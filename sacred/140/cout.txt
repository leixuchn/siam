INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "140"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch16
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(16, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(16, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(16, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(16, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(16, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(16, 72, 8, 8), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-07 10:14:00.841422: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:14:00.841462: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:14:00.841468: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:14:00.841472: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:14:00.841476: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:14:01.584006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 4.02GiB
2017-12-07 10:14:01.584040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-07 10:14:01.584046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-07 10:14:01.584054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 166250 steps
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(16, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(16, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(16, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(16, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(16, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(16, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(16, 15, 15), dtype=float32)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-07 10:14:16.306878: step 0, loss = 2.06, batch loss = 2.00 (1.4 examples/sec; 11.199 sec/batch; 517h:10m:25s remains)
2017-12-07 10:14:17.917580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2608027 -4.2344689 -4.2177386 -4.2209153 -4.2266965 -4.2281 -4.2228127 -4.2216725 -4.2052331 -4.1836905 -4.18497 -4.2038569 -4.2150126 -4.210288 -4.2232161][-4.23606 -4.2010913 -4.178987 -4.1927881 -4.2093911 -4.2129803 -4.206697 -4.1998224 -4.179018 -4.1625447 -4.1643834 -4.1836052 -4.1940265 -4.1904063 -4.2087541][-4.2224016 -4.18118 -4.150013 -4.1675382 -4.1947021 -4.2038784 -4.2004066 -4.1912708 -4.1740308 -4.1657467 -4.1682215 -4.1820531 -4.1899805 -4.1884055 -4.204535][-4.2067461 -4.1623111 -4.120801 -4.1352186 -4.170619 -4.1901913 -4.1922708 -4.1876106 -4.1808991 -4.1830111 -4.1875677 -4.1943135 -4.1992617 -4.1958184 -4.2053661][-4.1783433 -4.1348786 -4.09103 -4.0975852 -4.1331739 -4.1596646 -4.1671281 -4.1682763 -4.1669917 -4.1757078 -4.1888709 -4.1964312 -4.2024717 -4.1988688 -4.2040629][-4.1396441 -4.1067381 -4.0769715 -4.0825472 -4.1106429 -4.1358352 -4.1414022 -4.1340027 -4.1229477 -4.1298437 -4.1559505 -4.1700511 -4.1818547 -4.1823764 -4.1881371][-4.097055 -4.0831575 -4.0796361 -4.0931392 -4.1159239 -4.1298342 -4.1217289 -4.0965614 -4.0680728 -4.0708265 -4.1130037 -4.1420407 -4.162251 -4.1630197 -4.1688786][-4.0985146 -4.0995088 -4.1160388 -4.1363211 -4.1503906 -4.1492052 -4.1230078 -4.079968 -4.0344882 -4.0328631 -4.0873928 -4.1336894 -4.1615305 -4.1610684 -4.1647472][-4.1414957 -4.1412487 -4.1632996 -4.184196 -4.18958 -4.1775966 -4.1461363 -4.0989513 -4.0459838 -4.0351949 -4.088089 -4.1398163 -4.1698308 -4.1691437 -4.1723404][-4.2020526 -4.1937556 -4.2109694 -4.2296476 -4.2314019 -4.2171588 -4.1908865 -4.1504755 -4.0983858 -4.0767751 -4.1131415 -4.15732 -4.1832614 -4.1822696 -4.1889024][-4.2576747 -4.2431178 -4.2502031 -4.264204 -4.2687826 -4.2619538 -4.2467837 -4.2172117 -4.1702733 -4.1432695 -4.1588221 -4.1896787 -4.2043176 -4.1996202 -4.2083812][-4.2995577 -4.2848325 -4.284554 -4.2924943 -4.2996635 -4.3009853 -4.2954969 -4.2748222 -4.2322655 -4.2041159 -4.2048688 -4.2224026 -4.2273769 -4.2191944 -4.2265744][-4.3225417 -4.3148875 -4.3129463 -4.3173556 -4.323812 -4.329042 -4.3265371 -4.3071165 -4.26229 -4.233418 -4.2301493 -4.2434855 -4.2477889 -4.238667 -4.2428427][-4.3244991 -4.3239994 -4.3232369 -4.3263431 -4.3319221 -4.3376403 -4.3348322 -4.3170524 -4.2717047 -4.2441678 -4.240756 -4.2547054 -4.2612715 -4.2526436 -4.2558079][-4.3185582 -4.3208122 -4.3216724 -4.323885 -4.3283434 -4.3337393 -4.3323951 -4.3190761 -4.2780132 -4.2542982 -4.2521806 -4.2635288 -4.2679105 -4.2587562 -4.2609835]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch16/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch16/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 10:14:34.370148: step 10, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 1.532 sec/batch; 70h:43m:45s remains)
INFO - root - 2017-12-07 10:14:50.243193: step 20, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 1.522 sec/batch; 70h:16m:18s remains)
INFO - root - 2017-12-07 10:15:05.834194: step 30, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.525 sec/batch; 70h:23m:34s remains)
INFO - root - 2017-12-07 10:15:21.744879: step 40, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 1.490 sec/batch; 68h:46m:29s remains)
INFO - root - 2017-12-07 10:15:37.323475: step 50, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.574 sec/batch; 72h:39m:48s remains)
INFO - root - 2017-12-07 10:15:53.098464: step 60, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 1.509 sec/batch; 69h:40m:21s remains)
INFO - root - 2017-12-07 10:16:08.852995: step 70, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.554 sec/batch; 71h:44m:25s remains)
INFO - root - 2017-12-07 10:16:24.531651: step 80, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 1.513 sec/batch; 69h:50m:28s remains)
INFO - root - 2017-12-07 10:16:40.464849: step 90, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.535 sec/batch; 70h:51m:36s remains)
INFO - root - 2017-12-07 10:16:56.320905: step 100, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 1.623 sec/batch; 74h:54m:27s remains)
2017-12-07 10:16:57.729539: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1988435 -4.1881576 -4.175231 -4.177774 -4.1933417 -4.2010813 -4.1950388 -4.1808581 -4.1560254 -4.1398554 -4.1458821 -4.1585083 -4.1646805 -4.1706433 -4.1846504][-4.1885171 -4.1834149 -4.1764822 -4.1783566 -4.1871252 -4.1850877 -4.1727366 -4.1588759 -4.1373038 -4.1236172 -4.1338744 -4.1531363 -4.1628385 -4.1726413 -4.1904936][-4.1989079 -4.1982784 -4.199152 -4.1993122 -4.1965075 -4.1851707 -4.1686311 -4.1548371 -4.1375856 -4.1283474 -4.1402354 -4.1591487 -4.1700931 -4.1814108 -4.1985345][-4.2212429 -4.2261891 -4.2321267 -4.2296505 -4.2150416 -4.1944084 -4.1746182 -4.1632686 -4.1543517 -4.1580834 -4.1735649 -4.185328 -4.1876364 -4.1909218 -4.2030196][-4.2405057 -4.2467041 -4.2526293 -4.2457657 -4.2224374 -4.1959171 -4.1795831 -4.1779661 -4.1811824 -4.1940384 -4.2087846 -4.2114949 -4.2028589 -4.1968489 -4.2051497][-4.2485423 -4.2520342 -4.2561951 -4.2464209 -4.2180681 -4.184155 -4.1632104 -4.1655521 -4.1811838 -4.2016716 -4.2098389 -4.204381 -4.192214 -4.1810584 -4.1886559][-4.2542624 -4.25593 -4.2584648 -4.2476912 -4.2153382 -4.1684213 -4.1284814 -4.1212726 -4.1475177 -4.1818976 -4.1922083 -4.1813736 -4.1647911 -4.1467357 -4.1516986][-4.2623496 -4.2674565 -4.2682371 -4.257369 -4.2240071 -4.1670809 -4.1057959 -4.0785041 -4.1087389 -4.1558914 -4.174624 -4.1624179 -4.1426029 -4.1221795 -4.1243567][-4.2720113 -4.2809453 -4.2812686 -4.2710819 -4.24324 -4.1872387 -4.1159949 -4.0698833 -4.0838013 -4.1285033 -4.1552691 -4.1489506 -4.1318383 -4.1174006 -4.1259465][-4.2640128 -4.2751727 -4.2787867 -4.2746987 -4.2593064 -4.2202225 -4.1579843 -4.103651 -4.0895853 -4.11294 -4.1364255 -4.1364989 -4.1286616 -4.123477 -4.1337895][-4.2386894 -4.2505546 -4.2600141 -4.2675982 -4.2688227 -4.2504869 -4.2064815 -4.1591625 -4.1341333 -4.1402292 -4.1565609 -4.16314 -4.1602936 -4.1531525 -4.1504622][-4.2109485 -4.2204928 -4.2340269 -4.2517209 -4.2643094 -4.2617707 -4.2359276 -4.2066197 -4.1946468 -4.2037935 -4.2176542 -4.224442 -4.2187023 -4.1994429 -4.1762733][-4.1871257 -4.1898355 -4.2055044 -4.2299113 -4.2494941 -4.2600327 -4.2527652 -4.2411671 -4.2432408 -4.2587633 -4.2712717 -4.27428 -4.2630668 -4.2374997 -4.2058539][-4.1768551 -4.1707726 -4.1876016 -4.2159843 -4.2383204 -4.2554584 -4.2633691 -4.265264 -4.2743359 -4.2930307 -4.3053188 -4.3037195 -4.2901731 -4.2655869 -4.2400723][-4.186305 -4.1757011 -4.1925378 -4.223712 -4.2431951 -4.2588997 -4.2715693 -4.2777891 -4.29039 -4.311337 -4.3284626 -4.3291192 -4.3190084 -4.3019977 -4.2867994]]...]
INFO - root - 2017-12-07 10:17:13.903916: step 110, loss = 2.09, batch loss = 2.04 (10.0 examples/sec; 1.608 sec/batch; 74h:12m:10s remains)
INFO - root - 2017-12-07 10:17:30.208664: step 120, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 1.551 sec/batch; 71h:34m:28s remains)
INFO - root - 2017-12-07 10:17:46.441865: step 130, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.661 sec/batch; 76h:37m:32s remains)
INFO - root - 2017-12-07 10:18:02.650069: step 140, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.629 sec/batch; 75h:09m:34s remains)
INFO - root - 2017-12-07 10:18:19.128476: step 150, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.697 sec/batch; 78h:18m:43s remains)
INFO - root - 2017-12-07 10:18:35.377798: step 160, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.588 sec/batch; 73h:15m:22s remains)
INFO - root - 2017-12-07 10:18:51.908214: step 170, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.712 sec/batch; 78h:58m:32s remains)
INFO - root - 2017-12-07 10:19:08.144999: step 180, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.620 sec/batch; 74h:44m:11s remains)
INFO - root - 2017-12-07 10:19:24.431887: step 190, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.665 sec/batch; 76h:47m:02s remains)
INFO - root - 2017-12-07 10:19:40.538288: step 200, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.538 sec/batch; 70h:55m:08s remains)
2017-12-07 10:19:41.995440: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.267941 -4.2623749 -4.2436104 -4.2339973 -4.2451963 -4.2715144 -4.2897525 -4.2985725 -4.2953796 -4.2743249 -4.2428341 -4.2295461 -4.242764 -4.2659745 -4.2855959][-4.2544136 -4.2486749 -4.227128 -4.2138181 -4.2244534 -4.2521048 -4.2703109 -4.2798972 -4.2715321 -4.2404351 -4.201591 -4.1856861 -4.2024751 -4.2317014 -4.2599483][-4.2389255 -4.2344661 -4.2087212 -4.1860723 -4.1889868 -4.2165232 -4.2375927 -4.2479753 -4.2385039 -4.2044873 -4.1577063 -4.1324582 -4.1494169 -4.1867371 -4.2283711][-4.2281394 -4.2237768 -4.1881261 -4.1491385 -4.1400862 -4.163568 -4.1893172 -4.2049203 -4.1991339 -4.1659951 -4.11561 -4.0910344 -4.1102734 -4.1537585 -4.2068744][-4.2218852 -4.206831 -4.1584916 -4.1053281 -4.0866618 -4.1087637 -4.1420221 -4.1643405 -4.162571 -4.1302657 -4.0837736 -4.0619144 -4.0837693 -4.1344447 -4.1943855][-4.2277732 -4.1968241 -4.1347404 -4.0690961 -4.0426116 -4.0671964 -4.1110911 -4.139504 -4.136199 -4.1034842 -4.0663233 -4.0530481 -4.0772595 -4.1313272 -4.1931581][-4.2395844 -4.1997194 -4.130722 -4.0573177 -4.0197616 -4.0415449 -4.0897079 -4.1194367 -4.1184387 -4.0929937 -4.069562 -4.0652 -4.0899773 -4.1416306 -4.1997457][-4.2602067 -4.219049 -4.1511889 -4.0761752 -4.0279016 -4.0411134 -4.0853314 -4.1144838 -4.1178608 -4.1001768 -4.0852208 -4.08703 -4.1102481 -4.1538391 -4.2049026][-4.2611127 -4.2226167 -4.1600628 -4.0901389 -4.0460286 -4.0606565 -4.10235 -4.1297407 -4.1331582 -4.1165438 -4.099977 -4.1023808 -4.1226387 -4.162674 -4.20919][-4.229167 -4.1958513 -4.1382313 -4.0796633 -4.0621238 -4.0957303 -4.1399689 -4.1622391 -4.160079 -4.1384459 -4.1143503 -4.110486 -4.1305604 -4.1719227 -4.2181077][-4.1731362 -4.1434641 -4.0899167 -4.0500379 -4.0683002 -4.1267233 -4.1757975 -4.1925554 -4.1777163 -4.1471643 -4.1148081 -4.1087222 -4.1370969 -4.1833382 -4.2309995][-4.1228552 -4.1031713 -4.0659409 -4.0480485 -4.0853395 -4.1518512 -4.1983657 -4.208262 -4.1821494 -4.1375055 -4.0968552 -4.0969524 -4.1421809 -4.1998477 -4.247869][-4.1083508 -4.10871 -4.0938458 -4.0837812 -4.1174259 -4.1709404 -4.2072878 -4.2123761 -4.1832724 -4.1314735 -4.0857873 -4.0971656 -4.1612754 -4.2303362 -4.2743115][-4.1172562 -4.136198 -4.1399236 -4.1334243 -4.15042 -4.1842232 -4.2044349 -4.2061834 -4.1847205 -4.140419 -4.1044149 -4.1278129 -4.1979394 -4.2659445 -4.297843][-4.1425104 -4.1683888 -4.1792607 -4.1717725 -4.17816 -4.1963849 -4.2070227 -4.2086267 -4.201098 -4.1752353 -4.1555462 -4.1790652 -4.2373104 -4.2905602 -4.3073583]]...]
INFO - root - 2017-12-07 10:19:58.290455: step 210, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.615 sec/batch; 74h:29m:16s remains)
INFO - root - 2017-12-07 10:20:14.538607: step 220, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 1.729 sec/batch; 79h:44m:18s remains)
INFO - root - 2017-12-07 10:20:30.760062: step 230, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.542 sec/batch; 71h:07m:14s remains)
INFO - root - 2017-12-07 10:20:47.034755: step 240, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.620 sec/batch; 74h:41m:26s remains)
INFO - root - 2017-12-07 10:21:03.000356: step 250, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.597 sec/batch; 73h:37m:04s remains)
INFO - root - 2017-12-07 10:21:19.363450: step 260, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.677 sec/batch; 77h:18m:18s remains)
INFO - root - 2017-12-07 10:21:35.785162: step 270, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.688 sec/batch; 77h:50m:34s remains)
INFO - root - 2017-12-07 10:21:51.871112: step 280, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.549 sec/batch; 71h:24m:44s remains)
INFO - root - 2017-12-07 10:22:08.240102: step 290, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.745 sec/batch; 80h:26m:08s remains)
INFO - root - 2017-12-07 10:22:24.436829: step 300, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.601 sec/batch; 73h:47m:02s remains)
2017-12-07 10:22:25.719851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2281804 -4.233747 -4.2360325 -4.2321072 -4.2309766 -4.2341652 -4.2337289 -4.2317309 -4.2343807 -4.2429271 -4.2481685 -4.2583818 -4.2716336 -4.2771187 -4.2762346][-4.2471967 -4.2545466 -4.25727 -4.2490993 -4.2402563 -4.2412515 -4.2464156 -4.2524657 -4.2586641 -4.2674489 -4.2748265 -4.2860341 -4.2980194 -4.302247 -4.3003068][-4.2891235 -4.2961011 -4.2956214 -4.2811804 -4.2617736 -4.2551012 -4.2598433 -4.2742739 -4.2869282 -4.2944422 -4.3004441 -4.3077459 -4.3159037 -4.3180604 -4.3156137][-4.3109093 -4.3157678 -4.3098021 -4.2865491 -4.2550411 -4.2342777 -4.2308846 -4.25262 -4.277307 -4.2936368 -4.3048897 -4.3119421 -4.3165746 -4.3178616 -4.3139439][-4.2946253 -4.300561 -4.2900052 -4.2517471 -4.2006145 -4.155551 -4.1366582 -4.1691713 -4.2180529 -4.2526903 -4.2780643 -4.2949061 -4.3055806 -4.3118544 -4.30571][-4.2490835 -4.2611327 -4.2401257 -4.1773219 -4.1001453 -4.0249982 -3.9815133 -4.0284772 -4.1169586 -4.1852617 -4.2338977 -4.2682157 -4.2904229 -4.3039351 -4.2956181][-4.1907935 -4.2139211 -4.1783233 -4.0857282 -3.9791789 -3.8711407 -3.7963719 -3.8599441 -3.996145 -4.1059275 -4.1837277 -4.2363119 -4.27349 -4.2951117 -4.2854443][-4.1419907 -4.1856594 -4.1490579 -4.0432158 -3.9221251 -3.7901826 -3.6781948 -3.7376761 -3.9017916 -4.038033 -4.140986 -4.2135148 -4.2632742 -4.2887263 -4.2800736][-4.1561704 -4.2118869 -4.1901417 -4.10016 -3.999023 -3.8837862 -3.7727895 -3.8018227 -3.9382956 -4.0625296 -4.16105 -4.2321291 -4.2788734 -4.2962537 -4.2828779][-4.2126093 -4.2583227 -4.2506833 -4.1896977 -4.1211042 -4.0458865 -3.9689643 -3.9737086 -4.0600691 -4.1529021 -4.2242765 -4.272635 -4.3020368 -4.3046832 -4.2830877][-4.25804 -4.2849689 -4.2822304 -4.2429757 -4.2031941 -4.162746 -4.1191864 -4.1131115 -4.1590166 -4.2188911 -4.2629118 -4.2881866 -4.2991371 -4.2877855 -4.259831][-4.2664275 -4.2775583 -4.2713113 -4.2416186 -4.2190633 -4.1979923 -4.1775641 -4.1691155 -4.1907158 -4.2294393 -4.2572885 -4.2692847 -4.2681041 -4.248136 -4.2180133][-4.2379527 -4.2401361 -4.2297873 -4.2071342 -4.1970382 -4.189784 -4.1825194 -4.1738009 -4.1814637 -4.2067356 -4.2231116 -4.2246714 -4.2189441 -4.1975145 -4.167841][-4.2223172 -4.219018 -4.2055187 -4.1839657 -4.17291 -4.1689057 -4.1654882 -4.160903 -4.1698341 -4.1932745 -4.20517 -4.2017875 -4.196054 -4.17968 -4.1540608][-4.2189932 -4.2212062 -4.2125125 -4.1911259 -4.1721878 -4.1612253 -4.1562247 -4.1555333 -4.1679392 -4.1898136 -4.2011271 -4.200211 -4.1991911 -4.1932211 -4.1786966]]...]
INFO - root - 2017-12-07 10:22:41.785928: step 310, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.616 sec/batch; 74h:29m:50s remains)
INFO - root - 2017-12-07 10:22:58.127995: step 320, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.739 sec/batch; 80h:08m:18s remains)
INFO - root - 2017-12-07 10:23:14.146383: step 330, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.628 sec/batch; 75h:02m:04s remains)
INFO - root - 2017-12-07 10:23:30.331479: step 340, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 73h:17m:28s remains)
INFO - root - 2017-12-07 10:23:46.590777: step 350, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.647 sec/batch; 75h:52m:54s remains)
INFO - root - 2017-12-07 10:24:02.757763: step 360, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 1.486 sec/batch; 68h:27m:25s remains)
INFO - root - 2017-12-07 10:24:18.994091: step 370, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.635 sec/batch; 75h:19m:01s remains)
INFO - root - 2017-12-07 10:24:35.127284: step 380, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.614 sec/batch; 74h:21m:58s remains)
INFO - root - 2017-12-07 10:24:51.368097: step 390, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 1.623 sec/batch; 74h:46m:41s remains)
INFO - root - 2017-12-07 10:25:07.572036: step 400, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.585 sec/batch; 73h:02m:02s remains)
2017-12-07 10:25:09.094499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2366195 -4.2285042 -4.23168 -4.2483683 -4.2706785 -4.2876158 -4.2917328 -4.2843661 -4.2686887 -4.2415733 -4.2113037 -4.1871672 -4.1801944 -4.1953306 -4.2194152][-4.2057376 -4.2053986 -4.2200937 -4.2464666 -4.2756705 -4.29394 -4.2941313 -4.2838836 -4.2671866 -4.2381697 -4.2054849 -4.1817851 -4.1767459 -4.1891274 -4.203609][-4.1591954 -4.1716337 -4.20061 -4.2352777 -4.2616696 -4.2685862 -4.2531672 -4.2339964 -4.2189851 -4.1965208 -4.1729965 -4.1597161 -4.1616378 -4.17202 -4.1813703][-4.1231656 -4.147459 -4.1852155 -4.2150583 -4.2230258 -4.2026548 -4.1615129 -4.130199 -4.1242175 -4.1239805 -4.1220851 -4.12632 -4.1373773 -4.1450577 -4.1449475][-4.1177926 -4.1500392 -4.1875844 -4.2009234 -4.1777363 -4.1158514 -4.0347767 -3.9805143 -3.9908197 -4.0302863 -4.0645318 -4.0904007 -4.1101518 -4.1163268 -4.1114726][-4.1428895 -4.1756191 -4.203722 -4.1954536 -4.1368113 -4.0325723 -3.9070158 -3.8169079 -3.8490498 -3.9402454 -4.0174184 -4.0712209 -4.1024861 -4.1084528 -4.1007757][-4.1768808 -4.1993861 -4.2106137 -4.1852036 -4.1086507 -3.9956853 -3.8676004 -3.772052 -3.8179178 -3.9312718 -4.0236492 -4.0834427 -4.1133556 -4.1170878 -4.1090617][-4.1872959 -4.1981111 -4.1987662 -4.1738019 -4.1168985 -4.0453172 -3.9761977 -3.9325409 -3.9721608 -4.051342 -4.114182 -4.1510329 -4.16837 -4.1679997 -4.158967][-4.1887684 -4.1910257 -4.187459 -4.1691508 -4.1391511 -4.1107044 -4.0925236 -4.0880551 -4.1170778 -4.1614804 -4.1944513 -4.2088094 -4.2137375 -4.2129426 -4.2090287][-4.1955786 -4.1896911 -4.1877623 -4.1821775 -4.1784391 -4.1815429 -4.1923342 -4.2039609 -4.220232 -4.2393937 -4.2514234 -4.2533965 -4.2537041 -4.2547131 -4.2550516][-4.2095757 -4.2015977 -4.205152 -4.2126079 -4.2260418 -4.2411466 -4.2589049 -4.2715034 -4.27659 -4.2780523 -4.2770343 -4.2749152 -4.2768831 -4.2802405 -4.2820096][-4.2132006 -4.2136679 -4.2248406 -4.2396808 -4.2556872 -4.2697673 -4.2861614 -4.2969189 -4.2970743 -4.2916651 -4.2863503 -4.2832503 -4.285141 -4.2881675 -4.289608][-4.2186222 -4.2245088 -4.2360458 -4.2497544 -4.2626624 -4.2718773 -4.2804646 -4.2858195 -4.2845292 -4.2805958 -4.2804189 -4.2828393 -4.2868934 -4.2911162 -4.2931952][-4.2342916 -4.235466 -4.2368169 -4.24078 -4.2437897 -4.2452116 -4.2472305 -4.2481518 -4.246242 -4.2453237 -4.2503285 -4.2573614 -4.2643576 -4.2710767 -4.2765694][-4.25293 -4.2455125 -4.2357068 -4.23133 -4.2289619 -4.2284927 -4.2315483 -4.2330589 -4.2301207 -4.2285509 -4.2317886 -4.2358165 -4.2401509 -4.246603 -4.2554326]]...]
INFO - root - 2017-12-07 10:25:25.243297: step 410, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 73h:42m:43s remains)
INFO - root - 2017-12-07 10:25:41.443132: step 420, loss = 2.09, batch loss = 2.04 (9.9 examples/sec; 1.611 sec/batch; 74h:12m:10s remains)
INFO - root - 2017-12-07 10:25:57.418770: step 430, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.559 sec/batch; 71h:48m:09s remains)
INFO - root - 2017-12-07 10:26:13.689982: step 440, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.585 sec/batch; 73h:00m:15s remains)
INFO - root - 2017-12-07 10:26:30.059644: step 450, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.674 sec/batch; 77h:05m:56s remains)
INFO - root - 2017-12-07 10:26:46.238833: step 460, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.630 sec/batch; 75h:04m:48s remains)
INFO - root - 2017-12-07 10:27:02.658059: step 470, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.704 sec/batch; 78h:27m:10s remains)
INFO - root - 2017-12-07 10:27:18.933663: step 480, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.576 sec/batch; 72h:34m:08s remains)
INFO - root - 2017-12-07 10:27:34.916062: step 490, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.683 sec/batch; 77h:28m:22s remains)
INFO - root - 2017-12-07 10:27:51.118702: step 500, loss = 2.05, batch loss = 2.00 (10.3 examples/sec; 1.560 sec/batch; 71h:49m:59s remains)
2017-12-07 10:27:52.484117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3243632 -4.3232317 -4.3246593 -4.3242836 -4.3221545 -4.3200841 -4.3172259 -4.3156643 -4.3160534 -4.3195343 -4.3236051 -4.3245454 -4.3231754 -4.3208337 -4.3199606][-4.3127022 -4.3053961 -4.3005676 -4.2941475 -4.2882929 -4.2855225 -4.2832208 -4.2847629 -4.2891641 -4.2984514 -4.3086715 -4.313087 -4.3133764 -4.3121982 -4.31311][-4.2948 -4.2781186 -4.2648482 -4.2512388 -4.2414918 -4.2350783 -4.230413 -4.2331948 -4.2430167 -4.2608271 -4.279 -4.2872677 -4.2913947 -4.29523 -4.3013983][-4.2774124 -4.2540288 -4.2324486 -4.2094722 -4.1926064 -4.1771731 -4.1610575 -4.1563125 -4.1716905 -4.2010226 -4.2280765 -4.241106 -4.2507563 -4.2616091 -4.2766957][-4.2691011 -4.2484393 -4.2243805 -4.195199 -4.1683044 -4.1337595 -4.0907583 -4.0685568 -4.09108 -4.1332965 -4.1660924 -4.181201 -4.1961994 -4.2143297 -4.2404413][-4.2694178 -4.2569513 -4.2338381 -4.2010269 -4.1616096 -4.101944 -4.023592 -3.98203 -4.0220985 -4.0822287 -4.1210513 -4.13599 -4.1520653 -4.1724429 -4.2038631][-4.2756352 -4.2720852 -4.25095 -4.2177334 -4.1685891 -4.091455 -3.9914594 -3.9451258 -4.0029011 -4.071074 -4.10632 -4.114718 -4.1243978 -4.1385655 -4.1683006][-4.2844329 -4.2845931 -4.267642 -4.2385569 -4.192204 -4.1195226 -4.0299067 -3.9985988 -4.0510621 -4.1069374 -4.1321068 -4.1301584 -4.1256905 -4.1244555 -4.1405892][-4.2907109 -4.2915912 -4.2789512 -4.2562084 -4.221643 -4.1702914 -4.1119709 -4.0969954 -4.1302962 -4.1666446 -4.1802111 -4.1684623 -4.1506729 -4.1365633 -4.1364851][-4.2895508 -4.2874269 -4.2782869 -4.2623563 -4.2407374 -4.2144017 -4.1887031 -4.1851168 -4.2030358 -4.2239714 -4.2267594 -4.209796 -4.1891842 -4.1739645 -4.1643276][-4.2825952 -4.2748919 -4.2663951 -4.2535086 -4.2406282 -4.2351503 -4.2344847 -4.2417269 -4.2556906 -4.2703657 -4.2687111 -4.254765 -4.2406411 -4.2310834 -4.2185407][-4.2736373 -4.2595849 -4.2477894 -4.2325997 -4.2261853 -4.2376385 -4.254262 -4.2702146 -4.2857146 -4.2986784 -4.2981391 -4.2892909 -4.282084 -4.2775259 -4.2685957][-4.2655225 -4.2468157 -4.2289181 -4.2110825 -4.2132297 -4.236433 -4.2614594 -4.2825346 -4.3006182 -4.3128438 -4.3146157 -4.3093896 -4.3063879 -4.3053875 -4.3023953][-4.2642856 -4.2446051 -4.2222705 -4.2032828 -4.2114511 -4.2403297 -4.2682142 -4.2919693 -4.30874 -4.3167477 -4.3177085 -4.3142009 -4.3129435 -4.3138809 -4.3151984][-4.271708 -4.2556562 -4.2337952 -4.2114153 -4.2175632 -4.2476931 -4.2773128 -4.3022432 -4.31551 -4.3180385 -4.315515 -4.3119645 -4.3119326 -4.3149171 -4.3186116]]...]
INFO - root - 2017-12-07 10:28:08.558512: step 510, loss = 2.09, batch loss = 2.04 (9.9 examples/sec; 1.619 sec/batch; 74h:33m:33s remains)
INFO - root - 2017-12-07 10:28:24.952986: step 520, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.705 sec/batch; 78h:29m:44s remains)
INFO - root - 2017-12-07 10:28:41.299392: step 530, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.585 sec/batch; 72h:57m:39s remains)
INFO - root - 2017-12-07 10:28:57.279149: step 540, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.632 sec/batch; 75h:07m:55s remains)
INFO - root - 2017-12-07 10:29:13.378136: step 550, loss = 2.06, batch loss = 2.01 (10.2 examples/sec; 1.562 sec/batch; 71h:53m:57s remains)
INFO - root - 2017-12-07 10:29:29.819837: step 560, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.618 sec/batch; 74h:28m:04s remains)
INFO - root - 2017-12-07 10:29:46.119167: step 570, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.652 sec/batch; 76h:02m:44s remains)
INFO - root - 2017-12-07 10:30:02.378305: step 580, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.601 sec/batch; 73h:41m:38s remains)
INFO - root - 2017-12-07 10:30:18.718881: step 590, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 1.696 sec/batch; 78h:01m:17s remains)
INFO - root - 2017-12-07 10:30:34.609526: step 600, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.553 sec/batch; 71h:26m:42s remains)
2017-12-07 10:30:35.868162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3420415 -4.3364239 -4.3217697 -4.2927208 -4.2540083 -4.2302942 -4.2419195 -4.277154 -4.308301 -4.3311291 -4.3438926 -4.344244 -4.3410592 -4.341908 -4.3435073][-4.3410726 -4.3279705 -4.3006806 -4.253448 -4.1939597 -4.1572227 -4.17273 -4.2234178 -4.2707562 -4.3099751 -4.3366175 -4.3435946 -4.3418727 -4.3428755 -4.3443079][-4.3360705 -4.3166857 -4.27665 -4.2073441 -4.1188622 -4.0642958 -4.0793152 -4.14876 -4.2195916 -4.2784929 -4.3206234 -4.3381052 -4.3411155 -4.3424067 -4.3429723][-4.3283148 -4.3110747 -4.2673011 -4.1806216 -4.0620141 -3.9816206 -3.9873421 -4.0734057 -4.1698861 -4.2451091 -4.2989292 -4.3279872 -4.3386874 -4.34152 -4.3408194][-4.3180056 -4.3064427 -4.2646146 -4.1667013 -4.0216842 -3.9073048 -3.8904977 -3.986912 -4.1116514 -4.2034149 -4.2660751 -4.3086782 -4.3309488 -4.3392086 -4.3391361][-4.3052526 -4.3016558 -4.2675271 -4.1692276 -4.0088959 -3.8620911 -3.8086982 -3.8973756 -4.0428085 -4.155406 -4.2295184 -4.2859745 -4.3207073 -4.3350692 -4.3376374][-4.2851019 -4.2949581 -4.2754593 -4.1927967 -4.0399637 -3.882267 -3.8021045 -3.87021 -4.0165119 -4.1397858 -4.22059 -4.282289 -4.3204775 -4.3334551 -4.3357944][-4.2684579 -4.2926135 -4.289022 -4.2283792 -4.1011338 -3.9589307 -3.8825555 -3.9326801 -4.0549016 -4.1642303 -4.2379146 -4.2960396 -4.3286009 -4.3361087 -4.3352256][-4.2777629 -4.3048711 -4.3072314 -4.2637968 -4.1666231 -4.0494852 -3.9832137 -4.0176268 -4.1098886 -4.1965423 -4.2595363 -4.3111854 -4.3375459 -4.3404922 -4.3370314][-4.3018646 -4.32544 -4.3261514 -4.2947645 -4.2252965 -4.133822 -4.0756607 -4.0930219 -4.1589389 -4.2298422 -4.2846003 -4.3277612 -4.347681 -4.3469305 -4.3415504][-4.3244338 -4.3437419 -4.3433423 -4.32138 -4.27534 -4.2085214 -4.1596007 -4.16312 -4.2063866 -4.2636976 -4.3098063 -4.3441625 -4.3587742 -4.35503 -4.3469906][-4.3442411 -4.3575206 -4.3573866 -4.3422217 -4.3150163 -4.2721081 -4.2345047 -4.2293725 -4.2578011 -4.3021131 -4.3386993 -4.3622336 -4.3674536 -4.3595324 -4.3499146][-4.3639159 -4.3697681 -4.3664942 -4.3527 -4.3358746 -4.3105087 -4.2867775 -4.281517 -4.3004923 -4.3329158 -4.3584008 -4.3701715 -4.367969 -4.3587294 -4.3499556][-4.3747845 -4.3738403 -4.3667779 -4.3538375 -4.3434796 -4.3298693 -4.3166361 -4.314043 -4.3263021 -4.3497214 -4.3673711 -4.3711247 -4.36506 -4.3558216 -4.3484306][-4.3748274 -4.3692021 -4.3616958 -4.3511009 -4.3471642 -4.342207 -4.3347344 -4.3335629 -4.3403769 -4.3547115 -4.3656058 -4.3657403 -4.3599153 -4.3524928 -4.3472996]]...]
INFO - root - 2017-12-07 10:30:52.040385: step 610, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.650 sec/batch; 75h:56m:13s remains)
INFO - root - 2017-12-07 10:31:08.375603: step 620, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.608 sec/batch; 73h:58m:30s remains)
INFO - root - 2017-12-07 10:31:24.698095: step 630, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.694 sec/batch; 77h:55m:36s remains)
INFO - root - 2017-12-07 10:31:40.936217: step 640, loss = 2.10, batch loss = 2.04 (10.4 examples/sec; 1.539 sec/batch; 70h:47m:22s remains)
INFO - root - 2017-12-07 10:31:57.201664: step 650, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.707 sec/batch; 78h:31m:19s remains)
INFO - root - 2017-12-07 10:32:13.296633: step 660, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.622 sec/batch; 74h:36m:03s remains)
INFO - root - 2017-12-07 10:32:29.662372: step 670, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.668 sec/batch; 76h:42m:28s remains)
INFO - root - 2017-12-07 10:32:45.871152: step 680, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.567 sec/batch; 72h:04m:37s remains)
INFO - root - 2017-12-07 10:33:02.163234: step 690, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.680 sec/batch; 77h:14m:30s remains)
INFO - root - 2017-12-07 10:33:18.217329: step 700, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.605 sec/batch; 73h:48m:12s remains)
2017-12-07 10:33:19.642330: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3435054 -4.3309784 -4.3105192 -4.2897696 -4.2661514 -4.2515764 -4.2454052 -4.2422 -4.2400713 -4.2400584 -4.241478 -4.2420607 -4.2346563 -4.2257123 -4.2212954][-4.34135 -4.325913 -4.3030133 -4.2814689 -4.2597785 -4.2419114 -4.2270241 -4.2174845 -4.2167807 -4.22413 -4.2344122 -4.2391872 -4.2313795 -4.22123 -4.2160273][-4.341754 -4.3272357 -4.30349 -4.2768922 -4.2521267 -4.2276511 -4.2028484 -4.1846638 -4.1860633 -4.2022634 -4.2214775 -4.231452 -4.2265143 -4.2236285 -4.2254467][-4.3453989 -4.3331251 -4.3071728 -4.2708259 -4.2345734 -4.1998482 -4.1636682 -4.1397943 -4.1425047 -4.1683679 -4.1948347 -4.2104235 -4.2108979 -4.2186313 -4.2295718][-4.33338 -4.3198204 -4.2867627 -4.2394123 -4.18994 -4.1422334 -4.0955276 -4.0665441 -4.0740814 -4.1098132 -4.1420135 -4.1651673 -4.1773109 -4.1976972 -4.2163138][-4.3084083 -4.2877769 -4.2441282 -4.1847916 -4.1217537 -4.0584831 -3.99867 -3.9666386 -3.9853225 -4.0309925 -4.067174 -4.0984664 -4.1225247 -4.1536961 -4.1808796][-4.2846704 -4.2584352 -4.2064309 -4.1365314 -4.059238 -3.9760685 -3.9001396 -3.8666167 -3.8984575 -3.9545655 -3.9917326 -4.0248775 -4.0575781 -4.0964317 -4.1341295][-4.2784448 -4.2542133 -4.2044907 -4.1331706 -4.0493479 -3.9592695 -3.8836637 -3.8590612 -3.8995085 -3.9558039 -3.9850819 -4.0067687 -4.0311327 -4.0672331 -4.1137867][-4.3000751 -4.2883143 -4.2521973 -4.1934314 -4.1179461 -4.0374832 -3.9789805 -3.9678402 -4.0026364 -4.0454211 -4.0649576 -4.0719762 -4.07989 -4.1024661 -4.1448593][-4.3325076 -4.3352919 -4.3154554 -4.272398 -4.2152863 -4.1548419 -4.1165338 -4.1121445 -4.1335487 -4.1587505 -4.1702704 -4.1715531 -4.1701775 -4.1785226 -4.2071276][-4.3513446 -4.3626523 -4.3546886 -4.3268089 -4.2910609 -4.2526803 -4.2305012 -4.2304969 -4.2428813 -4.2555656 -4.262569 -4.2608824 -4.254724 -4.2562423 -4.2732067][-4.3532872 -4.3678379 -4.3684564 -4.3545094 -4.3351574 -4.3123789 -4.2991195 -4.3008986 -4.3073921 -4.3117423 -4.3155084 -4.3137484 -4.3077769 -4.3082275 -4.3203073][-4.3429227 -4.3556943 -4.3603559 -4.3552279 -4.3449512 -4.3329992 -4.3258939 -4.3272624 -4.3298454 -4.3306088 -4.3313026 -4.3304873 -4.3273482 -4.3282523 -4.3373227][-4.3316922 -4.3375263 -4.3403516 -4.3385544 -4.3346157 -4.3312654 -4.3297033 -4.3304682 -4.3321347 -4.3325486 -4.3335562 -4.3344412 -4.33356 -4.335042 -4.3406563][-4.3295231 -4.3298025 -4.329731 -4.3286657 -4.3270984 -4.3261628 -4.3261642 -4.326138 -4.3268762 -4.3279257 -4.3295665 -4.3315687 -4.3321834 -4.334218 -4.3375907]]...]
INFO - root - 2017-12-07 10:33:35.696117: step 710, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.548 sec/batch; 71h:10m:41s remains)
INFO - root - 2017-12-07 10:33:51.765480: step 720, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.702 sec/batch; 78h:15m:03s remains)
INFO - root - 2017-12-07 10:34:07.910042: step 730, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.644 sec/batch; 75h:36m:09s remains)
INFO - root - 2017-12-07 10:34:24.373950: step 740, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.613 sec/batch; 74h:08m:39s remains)
INFO - root - 2017-12-07 10:34:40.473924: step 750, loss = 2.10, batch loss = 2.05 (9.8 examples/sec; 1.627 sec/batch; 74h:48m:41s remains)
INFO - root - 2017-12-07 10:34:56.648551: step 760, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.630 sec/batch; 74h:56m:27s remains)
INFO - root - 2017-12-07 10:35:13.006988: step 770, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.665 sec/batch; 76h:31m:18s remains)
INFO - root - 2017-12-07 10:35:28.985080: step 780, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 73h:25m:54s remains)
INFO - root - 2017-12-07 10:35:45.462412: step 790, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.704 sec/batch; 78h:18m:01s remains)
INFO - root - 2017-12-07 10:36:01.791572: step 800, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.605 sec/batch; 73h:44m:43s remains)
2017-12-07 10:36:03.131911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2710528 -4.279479 -4.2909131 -4.284997 -4.2475295 -4.1913753 -4.1360359 -4.105094 -4.1226449 -4.1686983 -4.2202673 -4.2490711 -4.2574191 -4.2417421 -4.2095971][-4.2528224 -4.2604432 -4.2765455 -4.2816958 -4.2520146 -4.2010975 -4.1520624 -4.1280527 -4.1406517 -4.1753111 -4.2198372 -4.2387209 -4.2446718 -4.229403 -4.20354][-4.2265697 -4.2370982 -4.2650018 -4.280067 -4.256031 -4.2066164 -4.1627226 -4.1473393 -4.1551991 -4.182343 -4.2265339 -4.2424979 -4.2440143 -4.2180915 -4.1811624][-4.2032337 -4.2242641 -4.262722 -4.2825856 -4.2569456 -4.20615 -4.16112 -4.1423087 -4.1518459 -4.17859 -4.2215676 -4.2354827 -4.2249942 -4.1826706 -4.1311665][-4.171751 -4.2069297 -4.2501221 -4.2612338 -4.220624 -4.157969 -4.0970984 -4.0641947 -4.0880666 -4.1364737 -4.1791878 -4.1867046 -4.1599989 -4.1087465 -4.0552435][-4.1314077 -4.1740637 -4.218091 -4.2128587 -4.1410294 -4.0470028 -3.9541013 -3.9015648 -3.9589374 -4.0506797 -4.1073332 -4.1109085 -4.0775104 -4.0336609 -3.9919481][-4.1001897 -4.1367526 -4.1667209 -4.1394405 -4.0316806 -3.8915958 -3.7448983 -3.6663737 -3.7928982 -3.9455037 -4.025506 -4.0417819 -4.0210266 -3.9960041 -3.9662375][-4.0995936 -4.1193705 -4.1239142 -4.07685 -3.9518476 -3.7871521 -3.6069055 -3.520633 -3.7135518 -3.9043932 -3.996686 -4.0271115 -4.0213513 -4.0042124 -3.9844964][-4.1408815 -4.1514678 -4.1471343 -4.1027131 -3.9893057 -3.8427472 -3.7071452 -3.6786542 -3.8337865 -3.9813209 -4.0546613 -4.0812488 -4.0762911 -4.0601377 -4.04605][-4.1897335 -4.2054081 -4.2087522 -4.1803141 -4.0922222 -3.9827697 -3.9078484 -3.9159913 -4.0102239 -4.0902929 -4.1312642 -4.144659 -4.1409822 -4.1271334 -4.1137075][-4.2160997 -4.2434177 -4.2591062 -4.2448239 -4.1820631 -4.1090755 -4.0745249 -4.0897517 -4.1413345 -4.1841097 -4.2006269 -4.2072515 -4.2058191 -4.1965318 -4.184979][-4.2308292 -4.2635827 -4.2871213 -4.2826123 -4.2431154 -4.1996355 -4.1885524 -4.2029476 -4.2318044 -4.255177 -4.2618222 -4.2660441 -4.2671266 -4.2632961 -4.2546868][-4.2563481 -4.2822304 -4.3047714 -4.3079376 -4.2847581 -4.2591143 -4.257566 -4.2719088 -4.2900343 -4.3018436 -4.3053632 -4.3105464 -4.3154721 -4.3143349 -4.3068771][-4.284739 -4.3000417 -4.3169026 -4.3253694 -4.3119626 -4.2965388 -4.300889 -4.3121519 -4.3239012 -4.3318734 -4.3327808 -4.3361969 -4.3417792 -4.3414817 -4.3360014][-4.3142319 -4.3218474 -4.3312807 -4.3404303 -4.33448 -4.3270621 -4.332324 -4.3398647 -4.3468237 -4.3501196 -4.3488336 -4.3484163 -4.3503075 -4.3506236 -4.3482823]]...]
INFO - root - 2017-12-07 10:36:19.343286: step 810, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.540 sec/batch; 70h:44m:58s remains)
INFO - root - 2017-12-07 10:36:35.803135: step 820, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.679 sec/batch; 77h:09m:28s remains)
INFO - root - 2017-12-07 10:36:51.837709: step 830, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.578 sec/batch; 72h:30m:17s remains)
INFO - root - 2017-12-07 10:37:08.084073: step 840, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 1.620 sec/batch; 74h:26m:01s remains)
INFO - root - 2017-12-07 10:37:24.078602: step 850, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.587 sec/batch; 72h:54m:01s remains)
INFO - root - 2017-12-07 10:37:40.302568: step 860, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 73h:03m:35s remains)
INFO - root - 2017-12-07 10:37:56.445749: step 870, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.703 sec/batch; 78h:14m:15s remains)
INFO - root - 2017-12-07 10:38:12.737025: step 880, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.605 sec/batch; 73h:43m:20s remains)
INFO - root - 2017-12-07 10:38:28.716892: step 890, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.620 sec/batch; 74h:25m:45s remains)
INFO - root - 2017-12-07 10:38:44.862531: step 900, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.696 sec/batch; 77h:54m:30s remains)
2017-12-07 10:38:46.230853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2957296 -4.3033824 -4.3033814 -4.3001857 -4.2900805 -4.2766685 -4.2733369 -4.2799749 -4.2900858 -4.2992764 -4.304935 -4.3086834 -4.30961 -4.3117776 -4.3116307][-4.2785645 -4.2817154 -4.2786336 -4.2752647 -4.2628531 -4.2476854 -4.2461939 -4.2563357 -4.2687583 -4.2786589 -4.2867894 -4.2932744 -4.2969365 -4.3038578 -4.3085394][-4.2455297 -4.241281 -4.2370834 -4.2339158 -4.2210231 -4.2059665 -4.2061353 -4.2182174 -4.2327375 -4.2474208 -4.2620177 -4.2717266 -4.27911 -4.291996 -4.3025255][-4.219933 -4.207634 -4.2009606 -4.1949892 -4.1799207 -4.162674 -4.1602368 -4.1724677 -4.1949048 -4.2222762 -4.2465792 -4.2602267 -4.2697048 -4.2861452 -4.2994003][-4.2084179 -4.188334 -4.1758642 -4.1643081 -4.1446238 -4.1208673 -4.1085715 -4.1162553 -4.1487894 -4.1920466 -4.228931 -4.25096 -4.2642579 -4.2814856 -4.2943435][-4.1963606 -4.1682887 -4.1446 -4.1221442 -4.0964212 -4.0654182 -4.0368638 -4.0354233 -4.0793843 -4.1395679 -4.188283 -4.2228384 -4.2483678 -4.2683067 -4.2771664][-4.172164 -4.140934 -4.1117067 -4.0764351 -4.0427575 -3.9990125 -3.9403937 -3.91854 -3.9759765 -4.0584526 -4.1237068 -4.1739035 -4.213552 -4.2356772 -4.2428994][-4.1629148 -4.1338253 -4.1052375 -4.0649905 -4.0237794 -3.9599295 -3.8596566 -3.8038559 -3.8759382 -3.9849057 -4.0670028 -4.126349 -4.1724939 -4.1911731 -4.1951613][-4.1871405 -4.1604524 -4.1321011 -4.0964084 -4.0556893 -3.9832573 -3.8735242 -3.8106332 -3.8762851 -3.9788568 -4.0575566 -4.1140738 -4.1551723 -4.1680379 -4.1617312][-4.2282491 -4.2103062 -4.1865678 -4.159163 -4.1268573 -4.0660596 -3.9877055 -3.9544179 -3.9932349 -4.0556083 -4.1093111 -4.1489849 -4.1757288 -4.178988 -4.1635818][-4.26125 -4.2522254 -4.240602 -4.2304049 -4.2123213 -4.16872 -4.1199608 -4.1016521 -4.1168714 -4.1481094 -4.1768408 -4.1989889 -4.2143826 -4.2107763 -4.1883664][-4.2772288 -4.2758107 -4.276926 -4.2816982 -4.2782736 -4.2536979 -4.2227278 -4.2073231 -4.206141 -4.2193809 -4.2317991 -4.2438297 -4.2544284 -4.2499313 -4.2297025][-4.284368 -4.2887158 -4.2997203 -4.3139868 -4.3210068 -4.3095174 -4.2885966 -4.2756376 -4.2662063 -4.2674122 -4.2700448 -4.2773805 -4.2894144 -4.2898049 -4.278306][-4.2900219 -4.300705 -4.3159189 -4.3292084 -4.3389859 -4.334764 -4.3205152 -4.3101649 -4.3025031 -4.2993984 -4.2965255 -4.3000908 -4.3123765 -4.3177338 -4.3167753][-4.2972956 -4.3068871 -4.3166409 -4.3237233 -4.3315587 -4.3319697 -4.3249044 -4.3196759 -4.31727 -4.3138394 -4.3086762 -4.3100262 -4.3184853 -4.32334 -4.3267913]]...]
INFO - root - 2017-12-07 10:39:02.317570: step 910, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.605 sec/batch; 73h:43m:16s remains)
INFO - root - 2017-12-07 10:39:18.562753: step 920, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 1.600 sec/batch; 73h:29m:49s remains)
INFO - root - 2017-12-07 10:39:34.846965: step 930, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.697 sec/batch; 77h:56m:17s remains)
INFO - root - 2017-12-07 10:39:50.957752: step 940, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 1.590 sec/batch; 73h:01m:16s remains)
INFO - root - 2017-12-07 10:40:07.235981: step 950, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 1.639 sec/batch; 75h:16m:05s remains)
INFO - root - 2017-12-07 10:40:23.316698: step 960, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.668 sec/batch; 76h:35m:06s remains)
INFO - root - 2017-12-07 10:40:39.610692: step 970, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.723 sec/batch; 79h:05m:10s remains)
INFO - root - 2017-12-07 10:40:55.698039: step 980, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 1.487 sec/batch; 68h:14m:44s remains)
INFO - root - 2017-12-07 10:41:11.924494: step 990, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.583 sec/batch; 72h:41m:24s remains)
INFO - root - 2017-12-07 10:41:28.051325: step 1000, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 1.699 sec/batch; 77h:58m:09s remains)
2017-12-07 10:41:29.459429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3027058 -4.2912693 -4.2788978 -4.2730236 -4.2749219 -4.27021 -4.2596774 -4.2500753 -4.2378888 -4.2342272 -4.2387004 -4.2383275 -4.2391024 -4.23223 -4.2291894][-4.2866216 -4.2758336 -4.2610126 -4.255177 -4.2581525 -4.2571511 -4.2526565 -4.2511353 -4.2464275 -4.2435212 -4.2435656 -4.2375631 -4.2329431 -4.2241158 -4.2203436][-4.2648273 -4.2556062 -4.24258 -4.23805 -4.2400556 -4.2413659 -4.2409911 -4.2399058 -4.2381845 -4.2326059 -4.2281122 -4.2189775 -4.2142944 -4.2081046 -4.2102695][-4.2404428 -4.2344475 -4.2274303 -4.228137 -4.2289824 -4.22888 -4.2289591 -4.22951 -4.2300248 -4.2200155 -4.2126637 -4.2069373 -4.20792 -4.2067852 -4.2167444][-4.2132826 -4.21012 -4.2113991 -4.2199645 -4.221508 -4.2196388 -4.2173514 -4.2171583 -4.2228251 -4.2193313 -4.2107992 -4.2037268 -4.2068639 -4.2090888 -4.223671][-4.1918669 -4.1861672 -4.1794786 -4.180037 -4.1737676 -4.1672063 -4.1604176 -4.1564507 -4.1715622 -4.1886263 -4.1900239 -4.1829844 -4.1867843 -4.1993809 -4.2212081][-4.1756721 -4.1509371 -4.1201482 -4.0948853 -4.0709085 -4.0594025 -4.0477734 -4.0315261 -4.05335 -4.1077929 -4.1392751 -4.1444411 -4.1546187 -4.1790509 -4.2079144][-4.1684909 -4.125423 -4.0720043 -4.0272689 -3.9890282 -3.9754071 -3.9680557 -3.94824 -3.9675918 -4.0458012 -4.1000538 -4.1198363 -4.1379766 -4.1679745 -4.2001796][-4.2041588 -4.1599636 -4.10658 -4.06152 -4.0270271 -4.0214772 -4.0250044 -4.0196757 -4.03362 -4.0831676 -4.1151147 -4.1222076 -4.1306162 -4.1568542 -4.1880088][-4.2538691 -4.2300429 -4.1997705 -4.1703482 -4.1464887 -4.1437826 -4.1501126 -4.1501732 -4.1560655 -4.1739783 -4.17285 -4.1512871 -4.1363282 -4.1448426 -4.1642466][-4.2679224 -4.2619429 -4.2512507 -4.2424974 -4.2361383 -4.2379665 -4.2440038 -4.2458653 -4.2498264 -4.2538147 -4.2396946 -4.2064033 -4.1738234 -4.1654644 -4.1750283][-4.2554021 -4.2536273 -4.2555809 -4.2617712 -4.27251 -4.2823672 -4.2914209 -4.2942972 -4.2977653 -4.3021417 -4.2894506 -4.2605886 -4.2278829 -4.2113285 -4.2171249][-4.2422571 -4.238297 -4.240736 -4.2514892 -4.2697334 -4.2857995 -4.2993269 -4.3069267 -4.3130555 -4.3166537 -4.3074913 -4.2868867 -4.2634444 -4.2496185 -4.2523623][-4.2145891 -4.2127509 -4.2181778 -4.2325182 -4.2496076 -4.2679205 -4.2843781 -4.2920384 -4.2968111 -4.2975321 -4.2895818 -4.2787132 -4.2718616 -4.2651281 -4.2660551][-4.1678581 -4.1777568 -4.1936455 -4.2155318 -4.2325516 -4.2487416 -4.2631116 -4.2659283 -4.2643127 -4.2589145 -4.2485628 -4.2466316 -4.2520385 -4.2534413 -4.2574811]]...]
INFO - root - 2017-12-07 10:41:45.619120: step 1010, loss = 2.10, batch loss = 2.04 (10.3 examples/sec; 1.552 sec/batch; 71h:14m:32s remains)
INFO - root - 2017-12-07 10:42:02.005744: step 1020, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.579 sec/batch; 72h:29m:29s remains)
INFO - root - 2017-12-07 10:42:18.334276: step 1030, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.701 sec/batch; 78h:03m:34s remains)
INFO - root - 2017-12-07 10:42:34.737286: step 1040, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.544 sec/batch; 70h:50m:57s remains)
INFO - root - 2017-12-07 10:42:51.048817: step 1050, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.667 sec/batch; 76h:31m:05s remains)
INFO - root - 2017-12-07 10:43:07.307216: step 1060, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.577 sec/batch; 72h:21m:49s remains)
INFO - root - 2017-12-07 10:43:23.489963: step 1070, loss = 2.09, batch loss = 2.04 (9.0 examples/sec; 1.775 sec/batch; 81h:26m:48s remains)
INFO - root - 2017-12-07 10:43:39.579820: step 1080, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.591 sec/batch; 72h:58m:39s remains)
INFO - root - 2017-12-07 10:43:55.843720: step 1090, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.681 sec/batch; 77h:06m:41s remains)
INFO - root - 2017-12-07 10:44:11.835077: step 1100, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.593 sec/batch; 73h:03m:34s remains)
2017-12-07 10:44:13.297936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.062901 -4.0265031 -4.0072227 -4.0179029 -4.0479851 -4.0746861 -4.0793195 -4.0669212 -4.0593486 -4.0659447 -4.0854073 -4.0976043 -4.0884285 -4.0735178 -4.0845461][-4.0632629 -4.0434866 -4.0415387 -4.0631838 -4.0949287 -4.1157136 -4.1136837 -4.0979762 -4.0909891 -4.0990233 -4.1169395 -4.1267943 -4.1152906 -4.0980167 -4.1053872][-4.0967617 -4.09697 -4.1086187 -4.1327858 -4.1568866 -4.1673055 -4.1613851 -4.1481953 -4.1430726 -4.1495771 -4.162303 -4.1695518 -4.1552835 -4.1336813 -4.1373172][-4.1496177 -4.1599512 -4.1727767 -4.1921697 -4.20515 -4.2043242 -4.1899271 -4.1739526 -4.1678419 -4.1738062 -4.1834307 -4.191926 -4.1822305 -4.1592808 -4.1558595][-4.19413 -4.2062078 -4.2145448 -4.222353 -4.2206306 -4.20323 -4.1731548 -4.1468415 -4.1379986 -4.1492691 -4.1663046 -4.17894 -4.1761808 -4.1561885 -4.1468267][-4.2217917 -4.2288303 -4.2249804 -4.2120328 -4.1876621 -4.15067 -4.1028962 -4.0635486 -4.0520377 -4.0778227 -4.1161022 -4.142242 -4.1461549 -4.1269679 -4.1113391][-4.2342987 -4.22874 -4.206563 -4.1693554 -4.1212215 -4.06412 -4.0038238 -3.9540346 -3.9427927 -3.9928021 -4.0613966 -4.1064363 -4.114357 -4.0918941 -4.0686016][-4.2372236 -4.2156935 -4.1738977 -4.1169953 -4.0550637 -3.9888289 -3.9268632 -3.8784015 -3.8716061 -3.9351988 -4.0196638 -4.0788078 -4.0942044 -4.0775146 -4.0566888][-4.2274776 -4.1930828 -4.1397009 -4.0755816 -4.0140862 -3.9574833 -3.9124112 -3.881901 -3.8802409 -3.9293137 -4.0016108 -4.0631466 -4.0903606 -4.0900965 -4.0833783][-4.2098656 -4.1695356 -4.1134934 -4.0545321 -4.0081372 -3.9773288 -3.957293 -3.9436886 -3.9429519 -3.9673514 -4.0170908 -4.0714893 -4.1053357 -4.1174617 -4.122077][-4.193923 -4.1562338 -4.1066394 -4.0635715 -4.0397449 -4.0318017 -4.0283222 -4.0251412 -4.0266666 -4.0376921 -4.0678434 -4.1098728 -4.1397328 -4.1531272 -4.1562157][-4.1888032 -4.1603045 -4.1239662 -4.0997291 -4.0934763 -4.0971656 -4.1008444 -4.1039381 -4.1106224 -4.1198697 -4.1378808 -4.1632471 -4.180357 -4.1875696 -4.18479][-4.1964869 -4.1794977 -4.1565876 -4.1449103 -4.1461334 -4.1543784 -4.1643815 -4.1730537 -4.1831965 -4.1927567 -4.2028146 -4.2122641 -4.2155581 -4.21643 -4.2085433][-4.2149115 -4.2060938 -4.1912122 -4.1844478 -4.1866064 -4.195538 -4.2089252 -4.22087 -4.2316041 -4.2408948 -4.2457323 -4.2444949 -4.2394843 -4.2363725 -4.2259631][-4.2374997 -4.2326231 -4.2219276 -4.2168407 -4.2173624 -4.22375 -4.2349229 -4.2465887 -4.2570791 -4.2657661 -4.2680335 -4.263308 -4.2560239 -4.2508388 -4.241487]]...]
INFO - root - 2017-12-07 10:44:29.388324: step 1110, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.633 sec/batch; 74h:53m:32s remains)
INFO - root - 2017-12-07 10:44:45.639129: step 1120, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.660 sec/batch; 76h:07m:14s remains)
INFO - root - 2017-12-07 10:45:01.799225: step 1130, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.578 sec/batch; 72h:22m:52s remains)
INFO - root - 2017-12-07 10:45:17.982033: step 1140, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.567 sec/batch; 71h:52m:19s remains)
INFO - root - 2017-12-07 10:45:34.385829: step 1150, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.715 sec/batch; 78h:38m:18s remains)
INFO - root - 2017-12-07 10:45:50.330602: step 1160, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.582 sec/batch; 72h:32m:28s remains)
INFO - root - 2017-12-07 10:46:06.422547: step 1170, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.561 sec/batch; 71h:35m:46s remains)
INFO - root - 2017-12-07 10:46:22.571525: step 1180, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.703 sec/batch; 78h:04m:17s remains)
INFO - root - 2017-12-07 10:46:38.691054: step 1190, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.598 sec/batch; 73h:16m:53s remains)
INFO - root - 2017-12-07 10:46:54.736819: step 1200, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.624 sec/batch; 74h:27m:00s remains)
2017-12-07 10:46:56.048375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0924807 -3.9749131 -3.9153242 -3.9537818 -4.0530825 -4.1716409 -4.2563658 -4.298882 -4.2851458 -4.2233224 -4.1122336 -3.976517 -3.8735313 -3.8777137 -3.9755065][-4.1506371 -4.0463562 -3.9796209 -3.9865594 -4.0670424 -4.1825757 -4.2695975 -4.3169909 -4.3114514 -4.2578444 -4.160296 -4.0423622 -3.9385762 -3.918159 -3.9956853][-4.2486105 -4.1840568 -4.1319928 -4.1110592 -4.1452103 -4.22098 -4.2796459 -4.3143196 -4.3189087 -4.2889261 -4.2310629 -4.1602569 -4.0898204 -4.0606823 -4.1014919][-4.3182282 -4.2853765 -4.2457948 -4.2094612 -4.2071843 -4.2432623 -4.273109 -4.2958164 -4.3080325 -4.2999868 -4.2777915 -4.2496729 -4.2148838 -4.1957254 -4.21077][-4.3415108 -4.3296108 -4.2940555 -4.2518849 -4.2267942 -4.2319641 -4.2410417 -4.2610865 -4.2809825 -4.2895718 -4.289443 -4.2924709 -4.2842779 -4.2825603 -4.2902555][-4.3292413 -4.33505 -4.3066688 -4.2605915 -4.2117567 -4.1839437 -4.1712589 -4.1904597 -4.2226729 -4.248147 -4.2669883 -4.2900352 -4.3039184 -4.3159966 -4.3252525][-4.2977047 -4.3130531 -4.2888455 -4.2340946 -4.1562672 -4.0859809 -4.0433412 -4.06413 -4.1192789 -4.1778088 -4.2208529 -4.2618103 -4.2916975 -4.3123302 -4.3243375][-4.260272 -4.2793684 -4.2542739 -4.1926112 -4.0941467 -3.9885366 -3.9110935 -3.9231422 -3.9999712 -4.0980334 -4.1748748 -4.234014 -4.2708349 -4.2927122 -4.3080783][-4.2430463 -4.2664566 -4.2449365 -4.184607 -4.087081 -3.9772246 -3.8835275 -3.8720679 -3.9473264 -4.0654216 -4.1635733 -4.2340183 -4.27402 -4.2905984 -4.3025203][-4.2652454 -4.2851148 -4.2675829 -4.2164021 -4.1365552 -4.054709 -3.9837923 -3.9654684 -4.0154009 -4.1141624 -4.2020912 -4.2652745 -4.3017774 -4.312993 -4.3170266][-4.3016214 -4.3129568 -4.2991843 -4.2591224 -4.1996422 -4.1458035 -4.1051869 -4.096314 -4.1267729 -4.1956015 -4.2581005 -4.3037148 -4.3300004 -4.3358407 -4.33577][-4.332509 -4.3390517 -4.3338466 -4.3093271 -4.2687783 -4.2300177 -4.2054276 -4.2048259 -4.2249327 -4.2683239 -4.3053689 -4.33312 -4.3511853 -4.3543835 -4.3506312][-4.351275 -4.3560576 -4.359055 -4.3525391 -4.3328676 -4.307076 -4.2883253 -4.2892284 -4.3013315 -4.322114 -4.3376884 -4.3510871 -4.3618646 -4.3627591 -4.3582053][-4.3511529 -4.3553181 -4.3630757 -4.3674607 -4.3607297 -4.3451486 -4.3327632 -4.3325262 -4.3401232 -4.3487387 -4.3539882 -4.3583546 -4.3622422 -4.3615003 -4.3585944][-4.3403339 -4.3446956 -4.354003 -4.3616529 -4.3594646 -4.3510165 -4.3436041 -4.343894 -4.3492179 -4.3540354 -4.356308 -4.3577023 -4.3572221 -4.3552532 -4.3540254]]...]
INFO - root - 2017-12-07 10:47:12.291012: step 1210, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.690 sec/batch; 77h:27m:54s remains)
INFO - root - 2017-12-07 10:47:28.431560: step 1220, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.591 sec/batch; 72h:57m:08s remains)
INFO - root - 2017-12-07 10:47:44.634821: step 1230, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.680 sec/batch; 77h:00m:01s remains)
INFO - root - 2017-12-07 10:48:00.526581: step 1240, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.607 sec/batch; 73h:38m:44s remains)
INFO - root - 2017-12-07 10:48:16.772105: step 1250, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.708 sec/batch; 78h:17m:08s remains)
INFO - root - 2017-12-07 10:48:32.950959: step 1260, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 1.637 sec/batch; 75h:00m:22s remains)
INFO - root - 2017-12-07 10:48:49.125983: step 1270, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.677 sec/batch; 76h:50m:50s remains)
INFO - root - 2017-12-07 10:49:05.215214: step 1280, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 1.557 sec/batch; 71h:21m:27s remains)
INFO - root - 2017-12-07 10:49:21.523817: step 1290, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 1.543 sec/batch; 70h:41m:31s remains)
INFO - root - 2017-12-07 10:49:37.610321: step 1300, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.675 sec/batch; 76h:46m:02s remains)
2017-12-07 10:49:39.102568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1265025 -4.1189137 -4.1202621 -4.1161504 -4.1056271 -4.0838847 -4.0604429 -4.0508022 -4.0734897 -4.1085196 -4.0913148 -4.0604439 -4.0702939 -4.1158395 -4.1662951][-4.1545362 -4.15308 -4.1459785 -4.1291714 -4.1149192 -4.0913267 -4.0670462 -4.0595107 -4.0809402 -4.1145859 -4.10413 -4.0799432 -4.0879817 -4.1307549 -4.177021][-4.1755896 -4.1760182 -4.1650028 -4.1401763 -4.1212454 -4.0978484 -4.0736074 -4.0641255 -4.0849361 -4.1154847 -4.112855 -4.0975523 -4.109242 -4.1475058 -4.186398][-4.188345 -4.194726 -4.1845202 -4.1580005 -4.137897 -4.1105113 -4.0786824 -4.0602942 -4.0691695 -4.0946355 -4.0991397 -4.0923347 -4.1159878 -4.1576152 -4.1959162][-4.1785593 -4.1838403 -4.1791034 -4.15178 -4.1263762 -4.0916824 -4.055409 -4.0314827 -4.0395131 -4.0696254 -4.0800152 -4.078382 -4.1128821 -4.1620431 -4.2026315][-4.1369557 -4.1303754 -4.1200066 -4.0856347 -4.0504842 -4.0055871 -3.9597616 -3.9406781 -3.973911 -4.030107 -4.0513124 -4.057466 -4.0981059 -4.1574025 -4.2050843][-4.0824957 -4.0583968 -4.0320511 -3.9886596 -3.941633 -3.8746076 -3.7940257 -3.7703698 -3.8505359 -3.9415913 -3.9810081 -4.001514 -4.0547471 -4.1315804 -4.1973481][-4.0578847 -4.0201931 -3.980469 -3.9331892 -3.8798747 -3.7981105 -3.6848631 -3.6460195 -3.7603984 -3.8693516 -3.9165728 -3.9446566 -4.0096459 -4.0995779 -4.181819][-4.0951338 -4.0635748 -4.0314255 -3.9902923 -3.9387183 -3.8720708 -3.7810388 -3.74614 -3.833925 -3.9162993 -3.948668 -3.9708185 -4.0276394 -4.1111069 -4.18913][-4.1437235 -4.1171122 -4.0884008 -4.0550189 -4.0150151 -3.9763529 -3.929626 -3.9154866 -3.9701071 -4.0209913 -4.0378265 -4.0550804 -4.1028652 -4.1702166 -4.2271266][-4.169569 -4.1437464 -4.1163416 -4.0947356 -4.0732918 -4.0540948 -4.0393615 -4.0446033 -4.0806732 -4.1123767 -4.1246 -4.1391606 -4.1785841 -4.2272263 -4.2628155][-4.1783304 -4.1593909 -4.1395283 -4.1361156 -4.1353383 -4.1283135 -4.1258931 -4.1385245 -4.1651311 -4.1864142 -4.192513 -4.19868 -4.22519 -4.2584462 -4.2838235][-4.1670218 -4.1581006 -4.1507235 -4.1598954 -4.1751575 -4.1809959 -4.1797347 -4.183908 -4.19906 -4.207345 -4.2046914 -4.2070456 -4.2261429 -4.255053 -4.2816153][-4.1510229 -4.1440563 -4.139863 -4.1504016 -4.1682892 -4.1820693 -4.185132 -4.1845703 -4.1868534 -4.1865311 -4.1783719 -4.1795712 -4.197402 -4.2273192 -4.2609763][-4.1587691 -4.14904 -4.1443825 -4.1517382 -4.1640344 -4.174644 -4.1754751 -4.1707554 -4.16718 -4.1647673 -4.1597528 -4.1633549 -4.1811123 -4.2124224 -4.2499061]]...]
INFO - root - 2017-12-07 10:49:55.148728: step 1310, loss = 2.06, batch loss = 2.01 (10.4 examples/sec; 1.535 sec/batch; 70h:19m:49s remains)
INFO - root - 2017-12-07 10:50:11.295818: step 1320, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.546 sec/batch; 70h:49m:39s remains)
INFO - root - 2017-12-07 10:50:27.341575: step 1330, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 1.658 sec/batch; 75h:58m:14s remains)
INFO - root - 2017-12-07 10:50:43.664097: step 1340, loss = 2.09, batch loss = 2.04 (10.1 examples/sec; 1.578 sec/batch; 72h:15m:53s remains)
INFO - root - 2017-12-07 10:50:59.930997: step 1350, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.708 sec/batch; 78h:14m:00s remains)
INFO - root - 2017-12-07 10:51:15.796244: step 1360, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.597 sec/batch; 73h:08m:56s remains)
INFO - root - 2017-12-07 10:51:32.068936: step 1370, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 1.728 sec/batch; 79h:08m:37s remains)
INFO - root - 2017-12-07 10:51:48.183764: step 1380, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.590 sec/batch; 72h:48m:17s remains)
INFO - root - 2017-12-07 10:52:04.496878: step 1390, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.639 sec/batch; 75h:04m:22s remains)
INFO - root - 2017-12-07 10:52:20.524354: step 1400, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.549 sec/batch; 70h:55m:31s remains)
2017-12-07 10:52:21.962423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9418364 -3.9588649 -4.0427084 -4.1426468 -4.235755 -4.3045049 -4.3375587 -4.3395257 -4.3289218 -4.3192449 -4.3209915 -4.3255749 -4.3281031 -4.3286467 -4.3271513][-3.8256512 -3.8747389 -4.0012984 -4.1312771 -4.2338142 -4.2979541 -4.32189 -4.3171997 -4.3076811 -4.3058248 -4.3162255 -4.3280082 -4.3357677 -4.3375244 -4.3336048][-3.8149278 -3.8973484 -4.0392871 -4.1657228 -4.2469363 -4.2844172 -4.2854142 -4.2692971 -4.2617779 -4.2721033 -4.2960663 -4.3199658 -4.336659 -4.3421211 -4.3374944][-3.9396036 -4.0222898 -4.1315565 -4.2148728 -4.2517633 -4.2491856 -4.223361 -4.1976013 -4.1967659 -4.2245874 -4.268136 -4.3084173 -4.3357253 -4.3446393 -4.33966][-4.0902786 -4.1517911 -4.2124171 -4.240458 -4.2234583 -4.1753817 -4.1219544 -4.09188 -4.1076612 -4.1599121 -4.22571 -4.2840338 -4.3249168 -4.3392992 -4.3363442][-4.2046833 -4.2380853 -4.2505274 -4.2209363 -4.1473417 -4.0528827 -3.9734266 -3.9490862 -3.9973073 -4.0845547 -4.1755309 -4.2535295 -4.307363 -4.3273983 -4.3284888][-4.2757912 -4.2793751 -4.24898 -4.1711817 -4.0505104 -3.9156628 -3.815516 -3.8095465 -3.9032488 -4.0294528 -4.1441555 -4.234653 -4.2936134 -4.3167605 -4.3209214][-4.3015456 -4.2813072 -4.22473 -4.1222458 -3.9821973 -3.8424323 -3.7588215 -3.7869747 -3.9048193 -4.0435071 -4.16 -4.2440143 -4.296031 -4.3155217 -4.3186793][-4.302536 -4.2694812 -4.2062769 -4.1080794 -3.9921279 -3.8977637 -3.8622921 -3.9094167 -4.0115833 -4.1251421 -4.2176056 -4.2791433 -4.313477 -4.3237247 -4.3215709][-4.2996612 -4.2654085 -4.212678 -4.1396313 -4.0663342 -4.0220423 -4.0218763 -4.0691357 -4.143548 -4.2209377 -4.2801938 -4.3148427 -4.3301048 -4.32998 -4.3227572][-4.3045597 -4.2796116 -4.2457209 -4.2015319 -4.1627822 -4.1477523 -4.161377 -4.1989145 -4.2461405 -4.2903371 -4.31873 -4.330399 -4.3316574 -4.3257775 -4.3187118][-4.3136458 -4.30065 -4.2843127 -4.2621017 -4.2445359 -4.2416511 -4.2556229 -4.280086 -4.3040652 -4.3223467 -4.3294835 -4.3275194 -4.3224621 -4.3172092 -4.3144164][-4.3206625 -4.3160133 -4.3111515 -4.3024859 -4.2954121 -4.2954769 -4.3039079 -4.3152781 -4.3233356 -4.3268785 -4.3243461 -4.3187947 -4.314755 -4.3133354 -4.3140979][-4.323173 -4.3219156 -4.3219547 -4.3202519 -4.317636 -4.3166795 -4.3183684 -4.320322 -4.3207965 -4.3195024 -4.3166742 -4.3146052 -4.3146338 -4.3154421 -4.3160629][-4.3241591 -4.3225036 -4.3235126 -4.3241711 -4.3237433 -4.3232441 -4.3227272 -4.3225102 -4.3218 -4.3206587 -4.3197751 -4.3198695 -4.3198104 -4.3186169 -4.3168936]]...]
INFO - root - 2017-12-07 10:52:37.893356: step 1410, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.595 sec/batch; 73h:02m:58s remains)
INFO - root - 2017-12-07 10:52:54.222427: step 1420, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.714 sec/batch; 78h:28m:55s remains)
INFO - root - 2017-12-07 10:53:10.262161: step 1430, loss = 2.10, batch loss = 2.04 (10.3 examples/sec; 1.558 sec/batch; 71h:18m:34s remains)
INFO - root - 2017-12-07 10:53:26.633112: step 1440, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 72h:18m:20s remains)
INFO - root - 2017-12-07 10:53:42.744168: step 1450, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.691 sec/batch; 77h:25m:04s remains)
INFO - root - 2017-12-07 10:53:58.912743: step 1460, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.597 sec/batch; 73h:07m:29s remains)
INFO - root - 2017-12-07 10:54:14.975618: step 1470, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 1.552 sec/batch; 71h:02m:11s remains)
INFO - root - 2017-12-07 10:54:31.085882: step 1480, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.632 sec/batch; 74h:42m:32s remains)
INFO - root - 2017-12-07 10:54:47.339214: step 1490, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.579 sec/batch; 72h:15m:20s remains)
INFO - root - 2017-12-07 10:55:03.576928: step 1500, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 74h:35m:11s remains)
2017-12-07 10:55:04.875781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2230663 -4.2182136 -4.2167482 -4.2231045 -4.2277808 -4.228981 -4.2239814 -4.2138124 -4.2001948 -4.1871324 -4.1814036 -4.187067 -4.2036057 -4.2236557 -4.2406549][-4.1927395 -4.1898003 -4.1942225 -4.2054043 -4.2122779 -4.2165966 -4.214767 -4.2064619 -4.1953621 -4.1799459 -4.1675472 -4.1675878 -4.1800823 -4.195641 -4.2107768][-4.1756797 -4.1779528 -4.1891317 -4.2065697 -4.2139115 -4.214138 -4.2080736 -4.1990361 -4.1871514 -4.1648788 -4.1409755 -4.1350441 -4.1453905 -4.1617594 -4.1797218][-4.16145 -4.1677852 -4.181376 -4.1985245 -4.2008486 -4.1915054 -4.178885 -4.1682863 -4.1547155 -4.1267447 -4.0953355 -4.0910268 -4.106245 -4.1306362 -4.1566615][-4.1336231 -4.1390967 -4.1509786 -4.1615458 -4.1529722 -4.133203 -4.1205006 -4.1149549 -4.1033983 -4.0713263 -4.0394082 -4.0425115 -4.0676708 -4.1036777 -4.14326][-4.08654 -4.0846295 -4.0881853 -4.0864758 -4.0686464 -4.045393 -4.03995 -4.0495014 -4.05 -4.0193629 -3.9913836 -4.0034723 -4.0381937 -4.0795379 -4.1251163][-4.0330663 -4.0215797 -4.0160737 -4.0000215 -3.9739113 -3.9524193 -3.9589884 -3.9846084 -3.9994593 -3.9835572 -3.9633822 -3.9749012 -4.0098176 -4.0500789 -4.090425][-3.9846976 -3.9639781 -3.9486403 -3.9234469 -3.890862 -3.8723035 -3.8874774 -3.9263048 -3.9564235 -3.9615278 -3.9461792 -3.9444571 -3.967742 -4.0015445 -4.0377159][-3.949126 -3.9256473 -3.9061275 -3.8791456 -3.8477709 -3.8344305 -3.8551104 -3.89609 -3.9324989 -3.9488032 -3.9359612 -3.9251921 -3.9374683 -3.9643438 -3.9985046][-3.9533002 -3.9317369 -3.914187 -3.891659 -3.8697062 -3.8634264 -3.8811026 -3.912611 -3.9380987 -3.9519897 -3.9418006 -3.9321587 -3.9433172 -3.9702744 -4.0062828][-4.0126643 -3.9959686 -3.9825568 -3.966785 -3.9539061 -3.9517307 -3.9615819 -3.979058 -3.9901659 -3.9970865 -3.9916677 -3.988198 -4.002059 -4.0299196 -4.0655622][-4.1020436 -4.0946317 -4.0871229 -4.0753722 -4.0655527 -4.0619035 -4.0652084 -4.0743928 -4.0803294 -4.0850968 -4.0829482 -4.0835924 -4.0972157 -4.1202974 -4.1487556][-4.1878238 -4.1876965 -4.1859374 -4.1788073 -4.1719723 -4.1670823 -4.166749 -4.1721311 -4.1774321 -4.1818109 -4.1815257 -4.1822658 -4.1899838 -4.2045732 -4.2236028][-4.2566748 -4.2591372 -4.2595868 -4.2567539 -4.2535729 -4.2504454 -4.2478657 -4.2486076 -4.2510223 -4.2535295 -4.2542582 -4.2553492 -4.2595663 -4.2671757 -4.2788081][-4.2982216 -4.2999172 -4.3006577 -4.2997317 -4.2987804 -4.2974653 -4.2953544 -4.2943063 -4.2950397 -4.2965012 -4.2980409 -4.2995782 -4.3026886 -4.3076425 -4.3141227]]...]
INFO - root - 2017-12-07 10:55:21.091876: step 1510, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.615 sec/batch; 73h:53m:57s remains)
INFO - root - 2017-12-07 10:55:37.375787: step 1520, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.550 sec/batch; 70h:56m:08s remains)
INFO - root - 2017-12-07 10:55:53.688160: step 1530, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 1.763 sec/batch; 80h:39m:35s remains)
INFO - root - 2017-12-07 10:56:09.877024: step 1540, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.625 sec/batch; 74h:20m:26s remains)
INFO - root - 2017-12-07 10:56:26.184332: step 1550, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.626 sec/batch; 74h:22m:48s remains)
INFO - root - 2017-12-07 10:56:42.351746: step 1560, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.599 sec/batch; 73h:07m:42s remains)
INFO - root - 2017-12-07 10:56:58.614924: step 1570, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.680 sec/batch; 76h:49m:47s remains)
INFO - root - 2017-12-07 10:57:14.793397: step 1580, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.542 sec/batch; 70h:31m:03s remains)
INFO - root - 2017-12-07 10:57:31.046098: step 1590, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.556 sec/batch; 71h:09m:39s remains)
INFO - root - 2017-12-07 10:57:47.195586: step 1600, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.702 sec/batch; 77h:51m:27s remains)
2017-12-07 10:57:48.568892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3271 -4.3186 -4.3107867 -4.3077826 -4.3121538 -4.3191876 -4.3201432 -4.3167138 -4.3086824 -4.3034115 -4.2992511 -4.295918 -4.2999868 -4.3095894 -4.318222][-4.3093104 -4.2941608 -4.2800117 -4.2722006 -4.2761483 -4.2884688 -4.2903819 -4.2828312 -4.2720866 -4.2675991 -4.2664533 -4.2678719 -4.2796922 -4.2959676 -4.3064671][-4.2906771 -4.2696514 -4.2477636 -4.2335744 -4.2333145 -4.2455873 -4.2458162 -4.2336736 -4.2224183 -4.2237377 -4.2309871 -4.2408051 -4.2609396 -4.2827115 -4.295485][-4.271306 -4.2460842 -4.2172847 -4.1979451 -4.1910882 -4.1979012 -4.1893106 -4.1671968 -4.1587887 -4.16664 -4.1795163 -4.1967516 -4.2255192 -4.257699 -4.2766261][-4.253201 -4.2233968 -4.185432 -4.1561909 -4.1405363 -4.1384115 -4.1199226 -4.09265 -4.089561 -4.1018138 -4.1162682 -4.1394978 -4.1818018 -4.2294116 -4.2571492][-4.2367167 -4.2040052 -4.1559353 -4.1126828 -4.0838647 -4.0669079 -4.0401945 -4.0099406 -4.0054889 -4.0134187 -4.0248713 -4.0534015 -4.1150007 -4.1878595 -4.2344313][-4.2055531 -4.1668787 -4.1069741 -4.0493407 -4.0108342 -3.9875431 -3.9593079 -3.9206026 -3.9061997 -3.9044244 -3.9092798 -3.9460435 -4.0319085 -4.1364441 -4.2061768][-4.1675558 -4.1247125 -4.060575 -3.9990282 -3.9597034 -3.9372952 -3.9091053 -3.861413 -3.8385787 -3.8353283 -3.8441935 -3.8924191 -3.9957135 -4.1175046 -4.1980443][-4.1622853 -4.1243105 -4.0702248 -4.0190253 -3.9860377 -3.9655466 -3.9337645 -3.8839211 -3.8677969 -3.8788726 -3.901454 -3.9521284 -4.0442328 -4.1518188 -4.2244267][-4.1983409 -4.1751308 -4.1368046 -4.1025596 -4.0798087 -4.061595 -4.0333147 -3.9949663 -3.9893122 -4.0077357 -4.0328774 -4.0738168 -4.1400533 -4.2177582 -4.2711272][-4.2488456 -4.2380619 -4.2147465 -4.1975617 -4.1873808 -4.17502 -4.1550064 -4.1323829 -4.1329694 -4.14878 -4.1697679 -4.1979465 -4.2395945 -4.2877445 -4.3197465][-4.2921433 -4.2875323 -4.2756219 -4.2701273 -4.2669988 -4.2609534 -4.2505107 -4.2410274 -4.2451782 -4.2578425 -4.2738996 -4.2928109 -4.3163371 -4.3397765 -4.35285][-4.3163614 -4.3130846 -4.3084321 -4.3087568 -4.3103528 -4.3088818 -4.3047857 -4.3010359 -4.3054223 -4.3145356 -4.3249373 -4.337944 -4.3514123 -4.3605275 -4.3639464][-4.3337932 -4.3312225 -4.329813 -4.33256 -4.334651 -4.3341961 -4.3334532 -4.3315783 -4.3347349 -4.3407397 -4.3463521 -4.353374 -4.3605342 -4.3635697 -4.3640285][-4.3498969 -4.3484793 -4.3473825 -4.3486762 -4.3494277 -4.34956 -4.3495207 -4.3482389 -4.3497105 -4.35311 -4.35565 -4.3589706 -4.3624387 -4.3640885 -4.3646197]]...]
INFO - root - 2017-12-07 10:58:04.604583: step 1610, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.563 sec/batch; 71h:29m:39s remains)
INFO - root - 2017-12-07 10:58:21.028527: step 1620, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.614 sec/batch; 73h:48m:19s remains)
INFO - root - 2017-12-07 10:58:37.229487: step 1630, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.670 sec/batch; 76h:21m:36s remains)
INFO - root - 2017-12-07 10:58:53.440610: step 1640, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.579 sec/batch; 72h:11m:14s remains)
INFO - root - 2017-12-07 10:59:09.610321: step 1650, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.716 sec/batch; 78h:26m:23s remains)
INFO - root - 2017-12-07 10:59:25.784477: step 1660, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.614 sec/batch; 73h:46m:13s remains)
INFO - root - 2017-12-07 10:59:41.989797: step 1670, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.655 sec/batch; 75h:39m:58s remains)
INFO - root - 2017-12-07 10:59:58.250357: step 1680, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.552 sec/batch; 70h:57m:04s remains)
INFO - root - 2017-12-07 11:00:14.642597: step 1690, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.703 sec/batch; 77h:52m:04s remains)
INFO - root - 2017-12-07 11:00:30.815650: step 1700, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.574 sec/batch; 71h:57m:18s remains)
2017-12-07 11:00:32.287899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3603082 -4.3597884 -4.361515 -4.3632655 -4.3645816 -4.3630643 -4.3593283 -4.3526826 -4.3461747 -4.3422875 -4.3412151 -4.3417287 -4.34467 -4.3491049 -4.3540592][-4.3594012 -4.3596659 -4.3619943 -4.3637748 -4.3652382 -4.3615236 -4.3519669 -4.33736 -4.3241677 -4.3194985 -4.3237352 -4.3309402 -4.3391404 -4.3461432 -4.3527379][-4.3606782 -4.361546 -4.36269 -4.361506 -4.3586698 -4.3480825 -4.3277578 -4.3015623 -4.2818947 -4.2811265 -4.2977486 -4.3182025 -4.3350616 -4.3449116 -4.3510485][-4.3644123 -4.3658743 -4.3638139 -4.3546453 -4.3397503 -4.314518 -4.2767324 -4.2337871 -4.20681 -4.2157311 -4.2520165 -4.2918453 -4.3237171 -4.3405671 -4.34819][-4.3666062 -4.3667016 -4.3588638 -4.3393927 -4.3090229 -4.263864 -4.202126 -4.1379132 -4.1058412 -4.1336122 -4.1937718 -4.2538395 -4.30154 -4.3296642 -4.3443723][-4.3659329 -4.3628511 -4.3474813 -4.3171768 -4.2718534 -4.2073364 -4.1221123 -4.0395532 -4.0087585 -4.0594249 -4.1409416 -4.2148185 -4.2741928 -4.3145919 -4.3388367][-4.3606434 -4.3518314 -4.3278055 -4.2853317 -4.2268562 -4.148479 -4.0484147 -3.9576387 -3.9356441 -4.0060778 -4.1002474 -4.1793203 -4.2455153 -4.2954855 -4.3287234][-4.355463 -4.3403134 -4.3091235 -4.2584152 -4.1966038 -4.1213822 -4.0281754 -3.9446421 -3.9322705 -4.0025458 -4.0934095 -4.1703043 -4.2363758 -4.2880154 -4.3225741][-4.358398 -4.340971 -4.3075404 -4.2573795 -4.20476 -4.1477995 -4.0789304 -4.0238709 -4.0220318 -4.0726595 -4.1406517 -4.2022223 -4.2575378 -4.3017335 -4.3297219][-4.3632116 -4.348309 -4.3194547 -4.279129 -4.2383308 -4.1958656 -4.1503406 -4.1220045 -4.1305704 -4.1632824 -4.206718 -4.2490745 -4.2905641 -4.3244362 -4.3419409][-4.36368 -4.3518639 -4.3281827 -4.2963095 -4.2612038 -4.2254844 -4.1961212 -4.18639 -4.2023191 -4.227036 -4.2583833 -4.289547 -4.3206491 -4.3445077 -4.352849][-4.3625317 -4.3522177 -4.3305378 -4.2984114 -4.2618484 -4.2277479 -4.2098269 -4.2172003 -4.2415905 -4.2691283 -4.2987151 -4.3246446 -4.3479075 -4.362628 -4.3642344][-4.3578715 -4.3472023 -4.3225584 -4.2833729 -4.2413459 -4.2091966 -4.2074862 -4.2360654 -4.2745848 -4.3098226 -4.3388929 -4.3585296 -4.3711944 -4.3753009 -4.3698664][-4.3487921 -4.332974 -4.2994018 -4.2494173 -4.2022381 -4.1791186 -4.2004433 -4.2472858 -4.2960382 -4.3369412 -4.3632107 -4.3755131 -4.3776455 -4.3739729 -4.3657117][-4.3354082 -4.3089914 -4.2638073 -4.2053056 -4.1595345 -4.1544414 -4.1981955 -4.2562475 -4.307219 -4.3465872 -4.3669987 -4.3729577 -4.368741 -4.36232 -4.3562264]]...]
INFO - root - 2017-12-07 11:00:48.472445: step 1710, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 72h:11m:19s remains)
INFO - root - 2017-12-07 11:01:04.783802: step 1720, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.692 sec/batch; 77h:19m:02s remains)
INFO - root - 2017-12-07 11:01:21.036807: step 1730, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.700 sec/batch; 77h:41m:09s remains)
INFO - root - 2017-12-07 11:01:37.338384: step 1740, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.563 sec/batch; 71h:26m:16s remains)
INFO - root - 2017-12-07 11:01:53.617961: step 1750, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.564 sec/batch; 71h:29m:16s remains)
INFO - root - 2017-12-07 11:02:09.909409: step 1760, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.697 sec/batch; 77h:33m:26s remains)
INFO - root - 2017-12-07 11:02:26.254933: step 1770, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 1.563 sec/batch; 71h:23m:29s remains)
INFO - root - 2017-12-07 11:02:42.665380: step 1780, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.711 sec/batch; 78h:08m:52s remains)
INFO - root - 2017-12-07 11:02:58.857285: step 1790, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.543 sec/batch; 70h:29m:37s remains)
INFO - root - 2017-12-07 11:03:15.209760: step 1800, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.643 sec/batch; 75h:03m:02s remains)
2017-12-07 11:03:16.517699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.193418 -4.25868 -4.3161941 -4.3444037 -4.3500347 -4.3461461 -4.3420377 -4.3352547 -4.319509 -4.2993608 -4.2781706 -4.2576113 -4.237236 -4.2181559 -4.2089906][-4.1363306 -4.2153006 -4.2905774 -4.3328013 -4.3394041 -4.3268986 -4.31582 -4.3085084 -4.2993007 -4.2837996 -4.2692819 -4.2565689 -4.2510133 -4.2394428 -4.2250981][-4.1150103 -4.1935797 -4.2727404 -4.322248 -4.3290839 -4.3104873 -4.2913885 -4.2819576 -4.2809429 -4.2763987 -4.2718434 -4.265677 -4.26943 -4.2649956 -4.2493873][-4.1354337 -4.2006278 -4.26991 -4.3169737 -4.3235312 -4.3033242 -4.2777991 -4.26339 -4.2648277 -4.27279 -4.2791409 -4.2797518 -4.28845 -4.28573 -4.271492][-4.1845837 -4.2311091 -4.2824397 -4.3169632 -4.317688 -4.2904334 -4.2547503 -4.232511 -4.2353158 -4.2600932 -4.2826295 -4.2911592 -4.2990623 -4.2971244 -4.2854948][-4.230196 -4.2616744 -4.2941642 -4.312058 -4.2994452 -4.25645 -4.2037711 -4.1628885 -4.1598306 -4.2050495 -4.2552576 -4.2806315 -4.2897077 -4.2935052 -4.2859621][-4.2483554 -4.2729392 -4.2961192 -4.3027806 -4.2782788 -4.2211475 -4.1485572 -4.0805979 -4.0637612 -4.1254597 -4.2068067 -4.2520285 -4.2653618 -4.271215 -4.2657938][-4.2495284 -4.2704797 -4.2900767 -4.2900295 -4.2575932 -4.190649 -4.1060467 -4.0213256 -3.9873054 -4.0495644 -4.1458964 -4.2073584 -4.2267203 -4.2306237 -4.2238717][-4.2638187 -4.2779903 -4.2900162 -4.2845635 -4.2506409 -4.1878242 -4.1085653 -4.0277672 -3.9863625 -4.0260768 -4.1084728 -4.1707644 -4.1921973 -4.1890779 -4.1737256][-4.2879872 -4.2942228 -4.299129 -4.290267 -4.2570729 -4.2063279 -4.1465979 -4.0884681 -4.0561323 -4.0742269 -4.1280618 -4.1744094 -4.1885762 -4.175199 -4.1427956][-4.3122005 -4.3112803 -4.3104939 -4.3006859 -4.2707744 -4.2310119 -4.1938262 -4.1617351 -4.1428218 -4.14548 -4.1725039 -4.2000532 -4.2062407 -4.18855 -4.1514597][-4.320682 -4.3166008 -4.3134775 -4.3060207 -4.2847929 -4.2588072 -4.2411051 -4.228756 -4.2191191 -4.2145452 -4.2218246 -4.2326722 -4.2296176 -4.2118096 -4.1785293][-4.3047619 -4.2971444 -4.2933545 -4.2891288 -4.2804155 -4.2710347 -4.2680569 -4.2669978 -4.2639637 -4.2583041 -4.2559385 -4.2557836 -4.2465477 -4.2309527 -4.2052035][-4.287632 -4.2762771 -4.2689147 -4.2641191 -4.2611928 -4.2622623 -4.2650337 -4.2677574 -4.2710958 -4.2699723 -4.264729 -4.258637 -4.2488317 -4.2363663 -4.2212353][-4.2864842 -4.2774644 -4.2692232 -4.2619376 -4.2576013 -4.2577076 -4.25706 -4.2569232 -4.2608743 -4.2616205 -4.2551832 -4.24759 -4.2420053 -4.2351069 -4.2301221]]...]
INFO - root - 2017-12-07 11:03:32.737670: step 1810, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.688 sec/batch; 77h:05m:43s remains)
INFO - root - 2017-12-07 11:03:48.945479: step 1820, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.567 sec/batch; 71h:33m:48s remains)
INFO - root - 2017-12-07 11:04:05.292426: step 1830, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.712 sec/batch; 78h:11m:36s remains)
INFO - root - 2017-12-07 11:04:21.655730: step 1840, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.581 sec/batch; 72h:12m:41s remains)
INFO - root - 2017-12-07 11:04:38.069629: step 1850, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.607 sec/batch; 73h:22m:31s remains)
INFO - root - 2017-12-07 11:04:54.347942: step 1860, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 1.603 sec/batch; 73h:11m:58s remains)
INFO - root - 2017-12-07 11:05:10.685316: step 1870, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.539 sec/batch; 70h:15m:30s remains)
INFO - root - 2017-12-07 11:05:26.891287: step 1880, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.691 sec/batch; 77h:13m:29s remains)
INFO - root - 2017-12-07 11:05:43.045426: step 1890, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.593 sec/batch; 72h:44m:46s remains)
INFO - root - 2017-12-07 11:05:59.407579: step 1900, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.694 sec/batch; 77h:21m:28s remains)
2017-12-07 11:06:00.734573: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2736073 -4.2800446 -4.2881269 -4.2799058 -4.2460237 -4.2046666 -4.1736865 -4.174788 -4.1886134 -4.1880732 -4.1835628 -4.189178 -4.2109656 -4.2382436 -4.264873][-4.2366285 -4.2480316 -4.263803 -4.2584496 -4.2210875 -4.1800447 -4.1585584 -4.1640592 -4.1795883 -4.1875296 -4.1908407 -4.2005725 -4.2276678 -4.2587624 -4.28977][-4.2015471 -4.221045 -4.2464461 -4.248755 -4.2142191 -4.1736851 -4.157053 -4.1623077 -4.1741657 -4.1842914 -4.1936269 -4.2078228 -4.2318535 -4.2613015 -4.2933054][-4.2258673 -4.2496486 -4.2727885 -4.2741594 -4.2423449 -4.2026954 -4.1815882 -4.174994 -4.1793957 -4.1925817 -4.2048616 -4.2195911 -4.2379169 -4.2636442 -4.2943168][-4.2754512 -4.2923269 -4.3023739 -4.2944007 -4.2610159 -4.2192698 -4.1861725 -4.1665673 -4.1698689 -4.192997 -4.2075706 -4.2204194 -4.2389336 -4.2626567 -4.289937][-4.3087077 -4.3124094 -4.3048635 -4.2810454 -4.2366533 -4.1796823 -4.1173968 -4.0693073 -4.0750365 -4.1190391 -4.1495924 -4.1764684 -4.207705 -4.2395616 -4.2724576][-4.3104029 -4.299201 -4.2708545 -4.2236218 -4.1564517 -4.0730982 -3.9699886 -3.8753483 -3.8880191 -3.9788363 -4.0467257 -4.1023664 -4.1561608 -4.2042875 -4.2468772][-4.2837467 -4.2595782 -4.2154922 -4.1500983 -4.0648041 -3.9687035 -3.8421435 -3.7140491 -3.7340369 -3.8726821 -3.9773741 -4.061995 -4.135355 -4.1928163 -4.2394958][-4.265132 -4.2383204 -4.1954427 -4.135942 -4.0642424 -3.9930806 -3.9034648 -3.817549 -3.8343964 -3.9420841 -4.0317278 -4.1047416 -4.1660147 -4.214397 -4.2553782][-4.2739739 -4.2525377 -4.2227068 -4.1854668 -4.1430879 -4.1016464 -4.0522847 -4.0062785 -4.0098844 -4.0630407 -4.1216 -4.1732717 -4.2158365 -4.2523389 -4.2850137][-4.292767 -4.2780414 -4.2601061 -4.2360916 -4.2119856 -4.191627 -4.1668782 -4.1387215 -4.1278725 -4.1406941 -4.173305 -4.2092571 -4.243403 -4.2751479 -4.3025765][-4.3055954 -4.2951126 -4.2825308 -4.2684531 -4.2568927 -4.2482023 -4.2384534 -4.2196674 -4.1943278 -4.178813 -4.193429 -4.2206473 -4.2538543 -4.2857385 -4.3112216][-4.3139482 -4.3042264 -4.2905793 -4.2802196 -4.2766047 -4.2747803 -4.2689862 -4.2542419 -4.2256117 -4.2013211 -4.2086568 -4.2310605 -4.2622061 -4.292872 -4.3159847][-4.32177 -4.3143578 -4.3002658 -4.2910891 -4.2915974 -4.2938066 -4.2896991 -4.2764015 -4.2528758 -4.2313633 -4.2339468 -4.2508941 -4.27714 -4.3009534 -4.3186917][-4.321043 -4.3131356 -4.3012223 -4.2936516 -4.2967672 -4.3023758 -4.302649 -4.2950063 -4.2811012 -4.2655435 -4.2610812 -4.2702317 -4.2879844 -4.303659 -4.3171091]]...]
INFO - root - 2017-12-07 11:06:17.011902: step 1910, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.718 sec/batch; 78h:25m:22s remains)
INFO - root - 2017-12-07 11:06:33.157353: step 1920, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 1.602 sec/batch; 73h:08m:42s remains)
INFO - root - 2017-12-07 11:06:49.478332: step 1930, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.606 sec/batch; 73h:17m:14s remains)
INFO - root - 2017-12-07 11:07:05.815695: step 1940, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.630 sec/batch; 74h:22m:59s remains)
INFO - root - 2017-12-07 11:07:22.171117: step 1950, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.666 sec/batch; 76h:02m:21s remains)
INFO - root - 2017-12-07 11:07:38.403228: step 1960, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.614 sec/batch; 73h:39m:43s remains)
INFO - root - 2017-12-07 11:07:54.672277: step 1970, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.576 sec/batch; 71h:55m:46s remains)
INFO - root - 2017-12-07 11:08:11.107781: step 1980, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.680 sec/batch; 76h:38m:14s remains)
INFO - root - 2017-12-07 11:08:27.151855: step 1990, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.590 sec/batch; 72h:34m:12s remains)
INFO - root - 2017-12-07 11:08:43.484226: step 2000, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.692 sec/batch; 77h:10m:47s remains)
2017-12-07 11:08:44.834712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.065393 -4.0827951 -4.1264229 -4.1872091 -4.2441688 -4.2813849 -4.3005071 -4.3084168 -4.3054771 -4.2957606 -4.2870831 -4.2837596 -4.2840214 -4.2943258 -4.3142371][-4.0791707 -4.0791788 -4.1107588 -4.1655226 -4.2190909 -4.2512941 -4.2651153 -4.2764654 -4.2884655 -4.2981491 -4.2986197 -4.2950191 -4.2894745 -4.2960815 -4.3161526][-4.10232 -4.0960541 -4.1203966 -4.1625352 -4.1996737 -4.2118139 -4.2039366 -4.206964 -4.2345204 -4.2734356 -4.2943168 -4.295547 -4.28634 -4.2884111 -4.308897][-4.152792 -4.1466212 -4.1632252 -4.1855259 -4.1990161 -4.1786246 -4.1347051 -4.1192646 -4.1636934 -4.2347016 -4.2826843 -4.2921715 -4.28403 -4.2839184 -4.3028111][-4.195744 -4.1948538 -4.2079659 -4.2185121 -4.20671 -4.148396 -4.0541873 -4.0109076 -4.0752478 -4.1847758 -4.2650237 -4.2898216 -4.2841568 -4.2842727 -4.3007712][-4.206944 -4.2136035 -4.2265577 -4.2369165 -4.2158237 -4.12124 -3.9614911 -3.8619471 -3.9493437 -4.1126142 -4.2341619 -4.2847466 -4.2873034 -4.28479 -4.2970881][-4.2078505 -4.2139158 -4.221765 -4.2357492 -4.2189169 -4.109261 -3.8897345 -3.7153563 -3.8262749 -4.0449576 -4.1984529 -4.2679267 -4.2855048 -4.2858667 -4.2937832][-4.2216568 -4.21894 -4.2178845 -4.2287421 -4.218329 -4.1274195 -3.9275265 -3.7472959 -3.82403 -4.0221152 -4.1663628 -4.238678 -4.2694507 -4.2784042 -4.2885146][-4.2328014 -4.2247343 -4.2276344 -4.2419057 -4.2366824 -4.1798363 -4.0579805 -3.9447646 -3.9531155 -4.0485492 -4.1365938 -4.1923623 -4.2300386 -4.2512383 -4.2708435][-4.2331152 -4.2266021 -4.2347803 -4.2565088 -4.25828 -4.2259398 -4.1606164 -4.0983438 -4.072072 -4.0895367 -4.1260185 -4.1549215 -4.1825438 -4.2094517 -4.2403316][-4.2350779 -4.2308111 -4.2403193 -4.2598996 -4.2658429 -4.2489228 -4.2147207 -4.1794128 -4.1464577 -4.1356397 -4.149159 -4.1550961 -4.1568356 -4.1680684 -4.1946182][-4.2357912 -4.2363276 -4.2460318 -4.255208 -4.2588768 -4.2547789 -4.2447319 -4.2299442 -4.2087336 -4.1939487 -4.1920905 -4.1760235 -4.1507168 -4.1426029 -4.1555614][-4.227819 -4.2345734 -4.247541 -4.2582183 -4.2606869 -4.2594171 -4.2614508 -4.2607765 -4.2499194 -4.2311797 -4.2142239 -4.1820774 -4.14672 -4.13393 -4.138308][-4.2236032 -4.2449179 -4.26127 -4.2711911 -4.2647314 -4.2506013 -4.2515645 -4.2614641 -4.2646079 -4.2512593 -4.22952 -4.2007489 -4.1671295 -4.1506882 -4.1463141][-4.2204542 -4.2510343 -4.2721739 -4.27672 -4.2623954 -4.2390947 -4.2317533 -4.2416997 -4.25977 -4.2679968 -4.2641912 -4.2497954 -4.2235346 -4.1952615 -4.1803737]]...]
INFO - root - 2017-12-07 11:09:00.966291: step 2010, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.639 sec/batch; 74h:45m:25s remains)
INFO - root - 2017-12-07 11:09:17.412682: step 2020, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 1.688 sec/batch; 77h:01m:26s remains)
INFO - root - 2017-12-07 11:09:33.564291: step 2030, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.580 sec/batch; 72h:04m:31s remains)
INFO - root - 2017-12-07 11:09:50.023026: step 2040, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.674 sec/batch; 76h:20m:51s remains)
INFO - root - 2017-12-07 11:10:06.155849: step 2050, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.605 sec/batch; 73h:12m:40s remains)
INFO - root - 2017-12-07 11:10:22.464689: step 2060, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 1.760 sec/batch; 80h:15m:10s remains)
INFO - root - 2017-12-07 11:10:38.709622: step 2070, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.612 sec/batch; 73h:32m:03s remains)
INFO - root - 2017-12-07 11:10:55.107766: step 2080, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 1.641 sec/batch; 74h:50m:46s remains)
INFO - root - 2017-12-07 11:11:11.472635: step 2090, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.626 sec/batch; 74h:07m:25s remains)
INFO - root - 2017-12-07 11:11:27.820581: step 2100, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 72h:52m:27s remains)
2017-12-07 11:11:29.123085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2463408 -4.2365589 -4.2178822 -4.1988015 -4.1761017 -4.1398325 -4.1097474 -4.1051927 -4.1364083 -4.1705685 -4.1682677 -4.1498308 -4.1528287 -4.16228 -4.1926527][-4.2504425 -4.24443 -4.2227 -4.2031116 -4.185658 -4.1511865 -4.1210189 -4.1099763 -4.1261616 -4.154778 -4.1533136 -4.1388888 -4.1428561 -4.1532369 -4.1846981][-4.2478619 -4.2433748 -4.2193851 -4.1961913 -4.1761394 -4.140727 -4.1163387 -4.1099982 -4.1186953 -4.1468229 -4.1505909 -4.1411438 -4.1426129 -4.1533332 -4.1862049][-4.2544279 -4.2408442 -4.2071252 -4.1731305 -4.1409326 -4.0985403 -4.0816083 -4.0878959 -4.1027365 -4.13676 -4.1564994 -4.160758 -4.1645617 -4.1745648 -4.2054491][-4.2863183 -4.254796 -4.1999035 -4.1405425 -4.0811396 -4.018291 -4.0063858 -4.0366592 -4.0725255 -4.12277 -4.1566958 -4.1747231 -4.1875243 -4.201438 -4.2288203][-4.3211727 -4.2751579 -4.2013159 -4.1082458 -3.9997637 -3.8914075 -3.8771477 -3.9409349 -4.0169482 -4.092104 -4.1403828 -4.174942 -4.1998472 -4.2201257 -4.2487612][-4.3346896 -4.2934551 -4.2193675 -4.1118469 -3.9706502 -3.8219275 -3.7928879 -3.8765929 -3.9800985 -4.0719328 -4.1272278 -4.1694417 -4.2003541 -4.2250385 -4.2563634][-4.3338327 -4.3063178 -4.2517943 -4.1684151 -4.0549359 -3.9317453 -3.887773 -3.9267571 -4.0025949 -4.0849414 -4.1422391 -4.1826463 -4.2105079 -4.2305393 -4.2579517][-4.3256769 -4.3098073 -4.2745843 -4.2235217 -4.1590261 -4.0863481 -4.0397038 -4.02765 -4.0581975 -4.1179547 -4.1709971 -4.2058678 -4.2238417 -4.2362909 -4.2598028][-4.316947 -4.3075156 -4.2884312 -4.2634554 -4.235 -4.1997809 -4.1587648 -4.1227646 -4.1264882 -4.1658587 -4.2047958 -4.2266479 -4.2334409 -4.2383742 -4.2588139][-4.3157349 -4.3066096 -4.2930307 -4.2807183 -4.2676091 -4.2487226 -4.2160792 -4.1801581 -4.1792612 -4.2079248 -4.2314477 -4.2384963 -4.2351151 -4.2352419 -4.2536516][-4.3179288 -4.3055687 -4.2910333 -4.2772555 -4.2608156 -4.2412629 -4.2128191 -4.1888437 -4.1979532 -4.2264915 -4.2412982 -4.2379785 -4.2281609 -4.2254386 -4.2436528][-4.3245578 -4.3093634 -4.2901788 -4.2690983 -4.241993 -4.2148161 -4.1875162 -4.1761818 -4.1982365 -4.2298822 -4.2417583 -4.2327533 -4.2158689 -4.21039 -4.2302432][-4.3310709 -4.3141704 -4.292779 -4.26712 -4.2350459 -4.2021408 -4.1773844 -4.1770554 -4.2057095 -4.2355547 -4.2429638 -4.2275505 -4.2044458 -4.196959 -4.2192011][-4.3348451 -4.3184772 -4.2985291 -4.2749667 -4.2463951 -4.2172494 -4.1997623 -4.2071958 -4.2331395 -4.2536225 -4.2500362 -4.2289543 -4.2038198 -4.1951551 -4.21744]]...]
INFO - root - 2017-12-07 11:11:45.322824: step 2110, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 1.698 sec/batch; 77h:25m:29s remains)
INFO - root - 2017-12-07 11:12:01.615566: step 2120, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 1.562 sec/batch; 71h:12m:17s remains)
INFO - root - 2017-12-07 11:12:18.117280: step 2130, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.587 sec/batch; 72h:20m:47s remains)
INFO - root - 2017-12-07 11:12:34.300468: step 2140, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.676 sec/batch; 76h:24m:34s remains)
INFO - root - 2017-12-07 11:12:50.418582: step 2150, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.598 sec/batch; 72h:50m:23s remains)
INFO - root - 2017-12-07 11:13:06.698136: step 2160, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.685 sec/batch; 76h:49m:30s remains)
INFO - root - 2017-12-07 11:13:22.963695: step 2170, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 72h:54m:20s remains)
INFO - root - 2017-12-07 11:13:39.214588: step 2180, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.746 sec/batch; 79h:33m:46s remains)
INFO - root - 2017-12-07 11:13:55.471356: step 2190, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.592 sec/batch; 72h:32m:54s remains)
INFO - root - 2017-12-07 11:14:11.623462: step 2200, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.670 sec/batch; 76h:07m:02s remains)
2017-12-07 11:14:12.931139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3405318 -4.3318424 -4.3234019 -4.3184652 -4.3175616 -4.3208618 -4.3248191 -4.3287964 -4.3292637 -4.3278246 -4.3269734 -4.3269 -4.3289294 -4.3318615 -4.3376222][-4.3249784 -4.3106103 -4.29903 -4.2909679 -4.2888784 -4.2908406 -4.29696 -4.3050957 -4.309772 -4.3120823 -4.3150797 -4.3173447 -4.3209114 -4.3255954 -4.3338122][-4.3032064 -4.2834826 -4.2678461 -4.2553806 -4.2484336 -4.2451348 -4.2510505 -4.2628145 -4.2723742 -4.2804117 -4.2889471 -4.2960458 -4.3039608 -4.3129973 -4.3250189][-4.2758689 -4.2496238 -4.2291012 -4.2099447 -4.1921659 -4.1767716 -4.1781344 -4.1924806 -4.2098351 -4.2270141 -4.2442069 -4.2568979 -4.2684259 -4.2841725 -4.303515][-4.2533803 -4.21876 -4.1888571 -4.1602306 -4.1233792 -4.0858512 -4.0714731 -4.0810661 -4.1082745 -4.1393609 -4.1682005 -4.1903586 -4.2097154 -4.2363987 -4.2698374][-4.2433333 -4.2000909 -4.1565008 -4.1127067 -4.05117 -3.9800439 -3.9283996 -3.9169638 -3.9595466 -4.0186095 -4.0683017 -4.1056781 -4.1419945 -4.1887469 -4.2402964][-4.2474174 -4.194325 -4.130177 -4.0598063 -3.9719543 -3.8643651 -3.7584972 -3.7106862 -3.780056 -3.8827176 -3.9652042 -4.0275497 -4.08859 -4.1594286 -4.227344][-4.2635217 -4.2080603 -4.1300883 -4.0404592 -3.9393041 -3.8181777 -3.6817019 -3.6050093 -3.6866384 -3.8096249 -3.9097221 -3.9904897 -4.0684719 -4.15104 -4.226275][-4.28743 -4.2424922 -4.172894 -4.0897527 -4.0057988 -3.9137795 -3.806375 -3.7378826 -3.7847462 -3.8707442 -3.955482 -4.031733 -4.1027594 -4.1758113 -4.2424564][-4.31048 -4.2801027 -4.2317829 -4.1730556 -4.117094 -4.0623465 -3.9938807 -3.9452231 -3.9623337 -4.0097337 -4.0687695 -4.1256733 -4.1768923 -4.2295222 -4.2772527][-4.3328266 -4.315249 -4.2885709 -4.2564387 -4.2272816 -4.198987 -4.1579738 -4.1246638 -4.1272826 -4.1541734 -4.1937351 -4.2285733 -4.260489 -4.2925067 -4.3197036][-4.3490214 -4.3400135 -4.3278017 -4.3163829 -4.307796 -4.297442 -4.2760372 -4.2570219 -4.2544165 -4.2663527 -4.2888913 -4.3052349 -4.3232889 -4.3402457 -4.3528895][-4.3572783 -4.3521376 -4.3469248 -4.3444877 -4.3451328 -4.3432631 -4.333807 -4.3254042 -4.3230596 -4.3295951 -4.342309 -4.3479915 -4.3553238 -4.3621621 -4.3658495][-4.3632178 -4.3595881 -4.35683 -4.3564787 -4.359107 -4.3607864 -4.3580818 -4.3551412 -4.3526034 -4.3551221 -4.3603835 -4.3601522 -4.3611574 -4.3629746 -4.3640366][-4.3674917 -4.3655 -4.3631654 -4.3620582 -4.3634295 -4.36491 -4.3645716 -4.363771 -4.3613462 -4.3601685 -4.3606424 -4.35971 -4.3593349 -4.3600206 -4.36137]]...]
INFO - root - 2017-12-07 11:14:29.163123: step 2210, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 1.750 sec/batch; 79h:45m:09s remains)
INFO - root - 2017-12-07 11:14:45.225694: step 2220, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.605 sec/batch; 73h:08m:12s remains)
INFO - root - 2017-12-07 11:15:01.717679: step 2230, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.674 sec/batch; 76h:15m:45s remains)
INFO - root - 2017-12-07 11:15:17.985447: step 2240, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.639 sec/batch; 74h:39m:37s remains)
INFO - root - 2017-12-07 11:15:34.401510: step 2250, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.583 sec/batch; 72h:05m:51s remains)
INFO - root - 2017-12-07 11:15:50.613302: step 2260, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.707 sec/batch; 77h:45m:58s remains)
INFO - root - 2017-12-07 11:16:06.683096: step 2270, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 1.548 sec/batch; 70h:30m:23s remains)
INFO - root - 2017-12-07 11:16:22.942660: step 2280, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.665 sec/batch; 75h:49m:43s remains)
INFO - root - 2017-12-07 11:16:38.939258: step 2290, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.684 sec/batch; 76h:42m:39s remains)
INFO - root - 2017-12-07 11:16:55.116654: step 2300, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.625 sec/batch; 73h:59m:07s remains)
2017-12-07 11:16:56.451992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1354165 -4.155962 -4.1730924 -4.1711364 -4.1731558 -4.1787024 -4.17237 -4.1624112 -4.1555877 -4.1616883 -4.173346 -4.1885486 -4.2056122 -4.2084885 -4.1997886][-4.1061783 -4.129847 -4.1508665 -4.1480513 -4.1473484 -4.1466303 -4.1413188 -4.1405282 -4.1451192 -4.1615272 -4.1785216 -4.1920013 -4.2011414 -4.1973343 -4.1835151][-4.0940437 -4.1194153 -4.1460629 -4.1442385 -4.1428981 -4.138061 -4.1324434 -4.1357055 -4.1479397 -4.17052 -4.1892543 -4.1983027 -4.2028303 -4.1976819 -4.1813059][-4.1071577 -4.1287918 -4.156074 -4.1580682 -4.1613393 -4.1555753 -4.1427312 -4.1375089 -4.1465878 -4.16623 -4.1781883 -4.1813245 -4.1827855 -4.183259 -4.1753411][-4.1355925 -4.14696 -4.1588688 -4.1569171 -4.1628742 -4.1553564 -4.1373429 -4.1268554 -4.1310472 -4.1440692 -4.148253 -4.1471286 -4.1456256 -4.1510367 -4.15711][-4.1538458 -4.1476488 -4.1309657 -4.107945 -4.1029906 -4.0967517 -4.0832357 -4.0831079 -4.0975828 -4.1157694 -4.125453 -4.1267977 -4.1208405 -4.1235185 -4.1345773][-4.1515803 -4.1274438 -4.0783749 -4.0177178 -3.9829729 -3.9771457 -3.9868002 -4.0193105 -4.0619 -4.0922556 -4.1090474 -4.1139569 -4.1039963 -4.1009593 -4.1096449][-4.1466808 -4.1129236 -4.0417938 -3.9479468 -3.87533 -3.8616629 -3.8991861 -3.9673164 -4.034987 -4.0721393 -4.0902443 -4.0976892 -4.0864682 -4.0775418 -4.0791712][-4.1579041 -4.1332374 -4.0731082 -3.9927011 -3.923902 -3.8985374 -3.9232466 -3.9803014 -4.0377855 -4.067997 -4.0810809 -4.0927362 -4.0881062 -4.0785408 -4.0742188][-4.1843824 -4.1765132 -4.14559 -4.1040173 -4.0726428 -4.0607519 -4.0583668 -4.0728874 -4.0949788 -4.1056471 -4.1093063 -4.12221 -4.1271377 -4.1246986 -4.1191125][-4.2029247 -4.2093778 -4.202559 -4.1900978 -4.1854038 -4.191227 -4.1831894 -4.176405 -4.17536 -4.1699853 -4.1627631 -4.1658134 -4.1717978 -4.1709642 -4.1677246][-4.2035589 -4.2183328 -4.2284842 -4.2299852 -4.2345204 -4.2490644 -4.244926 -4.2375441 -4.2344947 -4.2252584 -4.2129793 -4.2093287 -4.2151055 -4.2160449 -4.2149615][-4.1982322 -4.2178907 -4.238091 -4.2467556 -4.2548337 -4.2741389 -4.2752514 -4.2715735 -4.2717624 -4.2620711 -4.2477803 -4.2426348 -4.248611 -4.2542109 -4.2579989][-4.2138505 -4.2346821 -4.2556186 -4.2635331 -4.2725673 -4.2936478 -4.2993522 -4.2960906 -4.2938213 -4.28066 -4.2643189 -4.2579408 -4.2623682 -4.2737365 -4.2843995][-4.2466316 -4.2649908 -4.2802339 -4.2830858 -4.289175 -4.3077645 -4.3158803 -4.31486 -4.311552 -4.2953758 -4.2752037 -4.2656221 -4.2674661 -4.2812996 -4.2969327]]...]
INFO - root - 2017-12-07 11:17:12.733492: step 2310, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.579 sec/batch; 71h:54m:28s remains)
INFO - root - 2017-12-07 11:17:29.164178: step 2320, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.693 sec/batch; 77h:04m:28s remains)
INFO - root - 2017-12-07 11:17:45.092198: step 2330, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.585 sec/batch; 72h:09m:00s remains)
INFO - root - 2017-12-07 11:18:01.334186: step 2340, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.692 sec/batch; 77h:02m:52s remains)
INFO - root - 2017-12-07 11:18:17.655567: step 2350, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.614 sec/batch; 73h:29m:05s remains)
INFO - root - 2017-12-07 11:18:34.021828: step 2360, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.703 sec/batch; 77h:31m:05s remains)
INFO - root - 2017-12-07 11:18:50.282641: step 2370, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.606 sec/batch; 73h:05m:27s remains)
INFO - root - 2017-12-07 11:19:06.512814: step 2380, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.628 sec/batch; 74h:06m:55s remains)
INFO - root - 2017-12-07 11:19:22.439338: step 2390, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.554 sec/batch; 70h:43m:29s remains)
INFO - root - 2017-12-07 11:19:38.610205: step 2400, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.583 sec/batch; 72h:03m:25s remains)
2017-12-07 11:19:39.916320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3071942 -4.3137083 -4.3177137 -4.3153648 -4.3022146 -4.2811551 -4.2635794 -4.2568083 -4.2586632 -4.2566462 -4.2564955 -4.2559543 -4.26091 -4.2810016 -4.3102016][-4.302094 -4.30623 -4.3088703 -4.3060222 -4.2880392 -4.2570152 -4.2294459 -4.2126236 -4.2132931 -4.2182059 -4.2290983 -4.2392597 -4.252677 -4.2794304 -4.31282][-4.2747645 -4.273221 -4.2728453 -4.271543 -4.2537756 -4.219902 -4.1884975 -4.1687884 -4.1725922 -4.1874804 -4.2096534 -4.2281036 -4.2492189 -4.2797976 -4.3140965][-4.2497597 -4.2441478 -4.2419496 -4.2410398 -4.2255716 -4.1939125 -4.1606379 -4.1379027 -4.1425867 -4.1649685 -4.1967044 -4.2226152 -4.2495613 -4.2814317 -4.314991][-4.2318926 -4.2242846 -4.2213717 -4.2204256 -4.2082033 -4.1786895 -4.143364 -4.1162229 -4.1180587 -4.1475382 -4.1860681 -4.2188654 -4.2510552 -4.283237 -4.3157063][-4.1800284 -4.1711974 -4.1711154 -4.1721363 -4.1679068 -4.1462269 -4.1151614 -4.0934429 -4.10167 -4.1378026 -4.1800046 -4.2158747 -4.2535081 -4.2869492 -4.3172679][-4.0992641 -4.092207 -4.1005092 -4.1090846 -4.112927 -4.0984182 -4.0727873 -4.0596809 -4.0827804 -4.1270251 -4.1687613 -4.2059379 -4.2508388 -4.2892447 -4.3182716][-4.0555272 -4.0535254 -4.0701346 -4.0860195 -4.0925632 -4.0758543 -4.0466743 -4.034368 -4.0676293 -4.1181345 -4.1600261 -4.2010188 -4.2504282 -4.2898984 -4.318109][-4.0895915 -4.092052 -4.1089144 -4.1249814 -4.126246 -4.1031427 -4.0720997 -4.0551219 -4.0834074 -4.1312709 -4.1719408 -4.2131815 -4.2578096 -4.2905331 -4.3164511][-4.1446133 -4.1495714 -4.1606212 -4.1700549 -4.1655631 -4.1418262 -4.1115766 -4.0871615 -4.100822 -4.1430779 -4.1834769 -4.222929 -4.2603369 -4.288321 -4.313417][-4.1718559 -4.1784916 -4.1840215 -4.1879282 -4.184309 -4.1673579 -4.1436214 -4.1176944 -4.1209393 -4.156251 -4.19654 -4.2332392 -4.2657771 -4.2890739 -4.3122091][-4.1901073 -4.189436 -4.189146 -4.1880217 -4.1829915 -4.1725917 -4.1544409 -4.1285949 -4.1288114 -4.1613479 -4.2035823 -4.2399287 -4.2699876 -4.2901969 -4.311409][-4.1832924 -4.1732955 -4.1718864 -4.16525 -4.1567421 -4.1525788 -4.1415682 -4.1193185 -4.1201482 -4.1515946 -4.1938753 -4.2340789 -4.2672682 -4.2898097 -4.3116646][-4.1523266 -4.1449227 -4.1480117 -4.1356487 -4.1239438 -4.1243787 -4.1189237 -4.1093106 -4.1214972 -4.1537776 -4.1918478 -4.2287059 -4.2631111 -4.2886868 -4.310997][-4.1611824 -4.1590929 -4.1601882 -4.1462646 -4.1314754 -4.1314077 -4.1320381 -4.1353645 -4.15702 -4.1880889 -4.2170534 -4.2416263 -4.2672386 -4.2884583 -4.3088336]]...]
INFO - root - 2017-12-07 11:19:56.149475: step 2410, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.658 sec/batch; 75h:27m:22s remains)
INFO - root - 2017-12-07 11:20:12.474403: step 2420, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.583 sec/batch; 72h:01m:43s remains)
INFO - root - 2017-12-07 11:20:28.858101: step 2430, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.615 sec/batch; 73h:28m:59s remains)
INFO - root - 2017-12-07 11:20:45.200628: step 2440, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.665 sec/batch; 75h:46m:31s remains)
INFO - root - 2017-12-07 11:21:01.229000: step 2450, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.552 sec/batch; 70h:38m:07s remains)
INFO - root - 2017-12-07 11:21:17.526892: step 2460, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.727 sec/batch; 78h:33m:59s remains)
INFO - root - 2017-12-07 11:21:33.763520: step 2470, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.642 sec/batch; 74h:41m:37s remains)
INFO - root - 2017-12-07 11:21:49.942445: step 2480, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.628 sec/batch; 74h:03m:48s remains)
INFO - root - 2017-12-07 11:22:06.110283: step 2490, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.628 sec/batch; 74h:02m:14s remains)
INFO - root - 2017-12-07 11:22:22.527409: step 2500, loss = 2.06, batch loss = 2.01 (10.5 examples/sec; 1.522 sec/batch; 69h:14m:35s remains)
2017-12-07 11:22:23.824845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2560749 -4.2522259 -4.2521639 -4.2527823 -4.2521977 -4.2499871 -4.2452993 -4.2406759 -4.2386575 -4.2391853 -4.2401676 -4.2404613 -4.2407651 -4.2414265 -4.2414527][-4.2139368 -4.2090058 -4.2095065 -4.2115788 -4.2117634 -4.2088203 -4.2032189 -4.1981692 -4.1965036 -4.1980615 -4.1996865 -4.2005229 -4.2011008 -4.2017016 -4.2007885][-4.1815028 -4.1760645 -4.1762996 -4.1786246 -4.179039 -4.176259 -4.1713805 -4.1684294 -4.1715465 -4.1774616 -4.1809978 -4.1829357 -4.1845264 -4.1869087 -4.1872678][-4.1762619 -4.1727662 -4.1752295 -4.1789751 -4.1806297 -4.1791425 -4.1764708 -4.1760812 -4.1812124 -4.1902475 -4.1963897 -4.1977706 -4.1970782 -4.1995764 -4.2015309][-4.1490726 -4.1512175 -4.1602492 -4.1671376 -4.1677389 -4.1633024 -4.1587529 -4.1550794 -4.1577535 -4.1676965 -4.1756186 -4.1797647 -4.1811671 -4.1881332 -4.1988387][-4.0948582 -4.1015224 -4.1148949 -4.1231618 -4.1176615 -4.1038752 -4.0947824 -4.0903749 -4.0972619 -4.1138606 -4.1278024 -4.1378627 -4.1440749 -4.1570244 -4.1753397][-4.018054 -4.0274487 -4.0431838 -4.0489106 -4.0367064 -4.0116196 -3.9931972 -3.9899735 -4.0100956 -4.0406456 -4.0607185 -4.0726614 -4.0796623 -4.0978246 -4.1282969][-3.9446323 -3.954443 -3.969878 -3.9692349 -3.9458451 -3.8992834 -3.8535271 -3.8329639 -3.8618736 -3.9036734 -3.9259908 -3.9365833 -3.9420936 -3.9733655 -4.0306106][-3.9834425 -3.9963036 -4.0071335 -3.998688 -3.9738328 -3.9282136 -3.8743474 -3.8367429 -3.8551469 -3.892031 -3.9098554 -3.9174156 -3.9193449 -3.9505343 -4.0126767][-4.0853329 -4.0948572 -4.0989838 -4.0905852 -4.0723085 -4.041079 -4.0078435 -3.9851437 -3.995647 -4.0169106 -4.0289831 -4.0385647 -4.0379896 -4.0551505 -4.0950327][-4.1710253 -4.1734357 -4.1712108 -4.1603832 -4.1407952 -4.1123672 -4.0862627 -4.0746145 -4.0869274 -4.1029158 -4.1160374 -4.1275945 -4.1292062 -4.138546 -4.1613984][-4.226717 -4.224287 -4.2213926 -4.2126822 -4.19657 -4.1770282 -4.1601362 -4.1562233 -4.1701064 -4.1852293 -4.1980281 -4.2078915 -4.208128 -4.2102928 -4.2183595][-4.2586651 -4.2522779 -4.2512536 -4.2484417 -4.240737 -4.2329173 -4.22711 -4.2284713 -4.2398558 -4.2512846 -4.2616134 -4.2686396 -4.268486 -4.2676582 -4.267096][-4.2833505 -4.2746959 -4.2707877 -4.2674356 -4.2629147 -4.2595372 -4.2585397 -4.2631845 -4.2731061 -4.28157 -4.2888393 -4.2942915 -4.2951374 -4.2951207 -4.2943411][-4.2977338 -4.2924213 -4.2893419 -4.2863398 -4.2819729 -4.2781987 -4.2769074 -4.2796855 -4.2860889 -4.291822 -4.297266 -4.3019547 -4.3046207 -4.3071928 -4.3086324]]...]
INFO - root - 2017-12-07 11:22:39.833560: step 2510, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.687 sec/batch; 76h:44m:54s remains)
INFO - root - 2017-12-07 11:22:56.152395: step 2520, loss = 2.09, batch loss = 2.04 (10.2 examples/sec; 1.567 sec/batch; 71h:14m:43s remains)
INFO - root - 2017-12-07 11:23:12.525572: step 2530, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.544 sec/batch; 70h:13m:57s remains)
INFO - root - 2017-12-07 11:23:28.705953: step 2540, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 72h:51m:46s remains)
INFO - root - 2017-12-07 11:23:44.914723: step 2550, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.543 sec/batch; 70h:09m:37s remains)
INFO - root - 2017-12-07 11:24:01.190518: step 2560, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.677 sec/batch; 76h:16m:07s remains)
INFO - root - 2017-12-07 11:24:16.982134: step 2570, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.680 sec/batch; 76h:22m:49s remains)
INFO - root - 2017-12-07 11:24:33.205001: step 2580, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.577 sec/batch; 71h:42m:18s remains)
INFO - root - 2017-12-07 11:24:49.562741: step 2590, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.675 sec/batch; 76h:08m:46s remains)
INFO - root - 2017-12-07 11:25:05.820163: step 2600, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.656 sec/batch; 75h:15m:32s remains)
2017-12-07 11:25:07.245467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3048477 -4.2969923 -4.2870731 -4.2780266 -4.2729936 -4.2595816 -4.2308464 -4.20258 -4.1990256 -4.2163849 -4.2291055 -4.2220111 -4.2208195 -4.2423458 -4.2746782][-4.296052 -4.279871 -4.2583451 -4.2392039 -4.2267518 -4.2091265 -4.1810436 -4.1579909 -4.1663275 -4.197412 -4.2194171 -4.2172585 -4.2196407 -4.2419987 -4.2746997][-4.2704539 -4.2486725 -4.218854 -4.1923685 -4.1745148 -4.1521735 -4.1209712 -4.103797 -4.1284862 -4.1778493 -4.2088203 -4.2110715 -4.2176461 -4.239892 -4.2701378][-4.2431703 -4.216836 -4.1834178 -4.158277 -4.138392 -4.1081 -4.0669527 -4.0511889 -4.0908036 -4.1572514 -4.1962171 -4.2035623 -4.2152734 -4.2381797 -4.2662258][-4.2291021 -4.2064714 -4.173861 -4.1488676 -4.1263 -4.0895143 -4.0357876 -4.0122743 -4.0577717 -4.1349916 -4.1799107 -4.1918015 -4.2079864 -4.232749 -4.2602282][-4.2326465 -4.2180853 -4.1881304 -4.1592536 -4.1308084 -4.0872207 -4.0175772 -3.9784765 -4.0235152 -4.1031461 -4.1509705 -4.1668563 -4.1866841 -4.2158513 -4.2459364][-4.2232113 -4.2165456 -4.1889253 -4.160809 -4.131258 -4.0778933 -3.9833989 -3.9270768 -3.9759872 -4.0601511 -4.1125321 -4.1301126 -4.1487961 -4.1820149 -4.2144694][-4.1929255 -4.1871147 -4.1610327 -4.1396317 -4.1162758 -4.0618768 -3.9530566 -3.8825095 -3.9293096 -4.0155835 -4.0716248 -4.0900197 -4.1052904 -4.1381192 -4.1693316][-4.1618648 -4.1526518 -4.1361294 -4.1311722 -4.1190929 -4.076776 -3.984942 -3.9173369 -3.9410388 -4.0076938 -4.0571494 -4.07195 -4.0795832 -4.1051512 -4.1325545][-4.1431222 -4.1327834 -4.130928 -4.1470723 -4.15022 -4.12484 -4.0621109 -4.0101609 -4.0123086 -4.0494905 -4.0822439 -4.087708 -4.0872574 -4.1047378 -4.1250348][-4.155622 -4.1440682 -4.1526451 -4.1820469 -4.1945028 -4.1809363 -4.141809 -4.10856 -4.1038938 -4.1240788 -4.1422467 -4.137764 -4.1285424 -4.1369414 -4.1502194][-4.1965504 -4.1809969 -4.1932116 -4.2239075 -4.2408552 -4.2375183 -4.2207584 -4.2074676 -4.2075229 -4.2208538 -4.2290826 -4.217319 -4.1995664 -4.1962452 -4.1990457][-4.246242 -4.2305942 -4.2426224 -4.267499 -4.2832184 -4.2855506 -4.2827077 -4.2816563 -4.2852397 -4.2943954 -4.2983923 -4.2873926 -4.26954 -4.2597566 -4.254981][-4.2891474 -4.2755222 -4.2822156 -4.2986064 -4.3096862 -4.3127284 -4.316092 -4.3187871 -4.3212552 -4.3258815 -4.3290381 -4.3235087 -4.3117194 -4.303896 -4.2981105][-4.3182712 -4.3085556 -4.3088636 -4.3155303 -4.32077 -4.3222613 -4.3249516 -4.3267884 -4.3281536 -4.3312674 -4.3354897 -4.335021 -4.3298831 -4.3250909 -4.3189158]]...]
INFO - root - 2017-12-07 11:25:23.453231: step 2610, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.614 sec/batch; 73h:22m:59s remains)
INFO - root - 2017-12-07 11:25:39.872124: step 2620, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.739 sec/batch; 79h:02m:04s remains)
INFO - root - 2017-12-07 11:25:55.788960: step 2630, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 71h:28m:46s remains)
INFO - root - 2017-12-07 11:26:11.913338: step 2640, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.616 sec/batch; 73h:26m:01s remains)
INFO - root - 2017-12-07 11:26:28.043869: step 2650, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.620 sec/batch; 73h:36m:47s remains)
INFO - root - 2017-12-07 11:26:44.593845: step 2660, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.612 sec/batch; 73h:14m:36s remains)
INFO - root - 2017-12-07 11:27:00.748519: step 2670, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.583 sec/batch; 71h:56m:28s remains)
INFO - root - 2017-12-07 11:27:17.006945: step 2680, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.588 sec/batch; 72h:08m:59s remains)
INFO - root - 2017-12-07 11:27:33.045679: step 2690, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.703 sec/batch; 77h:21m:54s remains)
INFO - root - 2017-12-07 11:27:49.315919: step 2700, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.567 sec/batch; 71h:11m:29s remains)
2017-12-07 11:27:50.756334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2745976 -4.2799277 -4.2867875 -4.2839293 -4.2693524 -4.2514386 -4.2347975 -4.2215915 -4.2184176 -4.2215748 -4.2260289 -4.2321043 -4.2332635 -4.2346234 -4.2491422][-4.2499595 -4.25535 -4.2628336 -4.2590718 -4.2427731 -4.2235909 -4.20708 -4.1967568 -4.1994514 -4.2069459 -4.2177143 -4.2314239 -4.2373714 -4.2406459 -4.2544632][-4.222332 -4.2271266 -4.2331662 -4.2299781 -4.2143507 -4.1916161 -4.1709013 -4.1603961 -4.1703053 -4.1853461 -4.2049556 -4.2248359 -4.2356634 -4.2442427 -4.2575932][-4.1876431 -4.1897173 -4.19412 -4.1905513 -4.1738257 -4.1475129 -4.1249452 -4.1143513 -4.1307898 -4.1588793 -4.1885018 -4.2126093 -4.2267828 -4.2394142 -4.2544327][-4.139174 -4.1345692 -4.137506 -4.1339388 -4.1159549 -4.0860262 -4.0603337 -4.05324 -4.0829215 -4.12748 -4.1693106 -4.1967859 -4.2132564 -4.2281003 -4.2467279][-4.0796223 -4.0688715 -4.070776 -4.069664 -4.0507 -4.0107627 -3.9653528 -3.9533978 -4.0064278 -4.076776 -4.1348915 -4.1710777 -4.1925159 -4.2114949 -4.2355075][-3.9976184 -3.9779992 -3.980901 -3.9881864 -3.9810116 -3.9341614 -3.8530724 -3.8108785 -3.8913496 -3.9985311 -4.0811143 -4.1329384 -4.1612377 -4.1844091 -4.2148018][-3.9729114 -3.9492674 -3.9581933 -3.9790456 -3.9878287 -3.9497161 -3.8620672 -3.7968359 -3.8658974 -3.9731662 -4.0588017 -4.1135044 -4.1399107 -4.1606479 -4.1909885][-4.0452137 -4.0291767 -4.0458245 -4.0734715 -4.0880504 -4.0643106 -4.0029774 -3.9563541 -3.9819429 -4.0418563 -4.0981808 -4.13068 -4.1433778 -4.1553836 -4.1792645][-4.130034 -4.1242657 -4.1404958 -4.1634822 -4.1773987 -4.1599555 -4.1218405 -4.0952592 -4.1013336 -4.1242104 -4.1530962 -4.1649079 -4.1654353 -4.1676817 -4.183732][-4.1934562 -4.1907239 -4.2011123 -4.216764 -4.2302856 -4.2155275 -4.1891494 -4.1742015 -4.1739287 -4.1802497 -4.1936555 -4.1986594 -4.1947713 -4.1914773 -4.2003455][-4.2356071 -4.2319741 -4.2369518 -4.2458863 -4.2547722 -4.2419195 -4.2233553 -4.2148752 -4.2121258 -4.2136183 -4.2193947 -4.2214675 -4.2170744 -4.2136168 -4.221025][-4.2721138 -4.2696433 -4.2737713 -4.2806883 -4.2854466 -4.2776308 -4.2650623 -4.2555819 -4.2485256 -4.2460108 -4.2479906 -4.2504754 -4.2480431 -4.2453685 -4.2502193][-4.3026247 -4.3006167 -4.304934 -4.3107324 -4.3141003 -4.3112435 -4.303268 -4.2950792 -4.2876773 -4.2831697 -4.2836018 -4.287014 -4.2868915 -4.2851582 -4.2860303][-4.3160548 -4.3137808 -4.3143206 -4.3156676 -4.3174653 -4.3178196 -4.3156738 -4.3119388 -4.3083324 -4.3052173 -4.3045921 -4.30812 -4.3109169 -4.3123 -4.3133621]]...]
INFO - root - 2017-12-07 11:28:07.077134: step 2710, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.539 sec/batch; 69h:55m:41s remains)
INFO - root - 2017-12-07 11:28:23.508423: step 2720, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.657 sec/batch; 75h:16m:58s remains)
INFO - root - 2017-12-07 11:28:39.819236: step 2730, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.572 sec/batch; 71h:22m:54s remains)
INFO - root - 2017-12-07 11:28:55.849712: step 2740, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 1.459 sec/batch; 66h:16m:17s remains)
INFO - root - 2017-12-07 11:29:11.857296: step 2750, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.572 sec/batch; 71h:22m:32s remains)
INFO - root - 2017-12-07 11:29:28.298458: step 2760, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.554 sec/batch; 70h:34m:14s remains)
INFO - root - 2017-12-07 11:29:44.447650: step 2770, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.645 sec/batch; 74h:40m:52s remains)
INFO - root - 2017-12-07 11:30:00.812088: step 2780, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.609 sec/batch; 73h:03m:06s remains)
INFO - root - 2017-12-07 11:30:17.073170: step 2790, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.711 sec/batch; 77h:40m:32s remains)
INFO - root - 2017-12-07 11:30:33.099762: step 2800, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.596 sec/batch; 72h:26m:56s remains)
2017-12-07 11:30:34.344375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3183122 -4.3161941 -4.30703 -4.2922468 -4.2708788 -4.2501822 -4.2328863 -4.231041 -4.2349868 -4.2317309 -4.2280831 -4.2330251 -4.2436018 -4.2495947 -4.2442622][-4.307579 -4.2993088 -4.2858462 -4.2663288 -4.2381639 -4.2119427 -4.1896367 -4.1884441 -4.1984291 -4.1963773 -4.1955376 -4.2086606 -4.2257414 -4.2270384 -4.2081375][-4.2902 -4.2779779 -4.2627449 -4.23941 -4.2074132 -4.176281 -4.1485887 -4.1496477 -4.1675453 -4.1633487 -4.1678815 -4.1935835 -4.2103548 -4.1993232 -4.1602635][-4.2719975 -4.2560725 -4.24091 -4.21898 -4.1868105 -4.1535797 -4.1146107 -4.1162677 -4.1392365 -4.1324778 -4.1408763 -4.1749096 -4.1878157 -4.1618686 -4.1087394][-4.2566681 -4.2379136 -4.2202768 -4.1990547 -4.1702285 -4.1355348 -4.0898609 -4.0980668 -4.1334124 -4.1346712 -4.1424942 -4.1709242 -4.1815615 -4.1549473 -4.1042018][-4.2484822 -4.2256327 -4.2000966 -4.1727524 -4.14288 -4.1062131 -4.0555367 -4.0707231 -4.11947 -4.1305666 -4.1404867 -4.1650434 -4.1808772 -4.1673851 -4.1316509][-4.2360554 -4.19925 -4.1590266 -4.1235018 -4.0864725 -4.0362763 -3.9746597 -3.99862 -4.062593 -4.088501 -4.1059208 -4.1385069 -4.1722608 -4.1788163 -4.1584654][-4.2173533 -4.1703525 -4.1191869 -4.0699034 -4.0099912 -3.9277825 -3.8553231 -3.9003963 -3.9977169 -4.0489197 -4.078855 -4.1197195 -4.1665077 -4.1894779 -4.1741781][-4.1976452 -4.1487141 -4.0982275 -4.03806 -3.9528093 -3.8410363 -3.7685554 -3.8338606 -3.9530234 -4.0181727 -4.0555696 -4.0965581 -4.1437826 -4.1762619 -4.1680889][-4.1807184 -4.134017 -4.0913239 -4.0439081 -3.974247 -3.8819859 -3.8231268 -3.8704238 -3.9711118 -4.0247478 -4.0572762 -4.0891514 -4.1293578 -4.1621227 -4.158298][-4.1851721 -4.1408129 -4.10427 -4.0723648 -4.0291023 -3.9687076 -3.9199018 -3.9442439 -4.0232024 -4.0641351 -4.0901847 -4.1144991 -4.1457868 -4.1722183 -4.1697569][-4.2095141 -4.1678977 -4.1330824 -4.1109047 -4.087492 -4.050066 -4.0148153 -4.032196 -4.091012 -4.1179771 -4.1323266 -4.1448941 -4.1679354 -4.1904354 -4.1923547][-4.2427754 -4.2051792 -4.1740294 -4.1601391 -4.1491551 -4.1286306 -4.1081772 -4.1224909 -4.1618733 -4.1760678 -4.1805334 -4.184339 -4.1976953 -4.2122025 -4.2162547][-4.2781968 -4.2478361 -4.2230458 -4.2148643 -4.2141185 -4.2071834 -4.1980619 -4.2069468 -4.2315168 -4.2380738 -4.2365355 -4.2352166 -4.2379236 -4.2417035 -4.2443409][-4.3051796 -4.2844472 -4.2688675 -4.2652745 -4.267345 -4.26492 -4.2618346 -4.2665839 -4.2775679 -4.2807908 -4.2802367 -4.2785959 -4.2772417 -4.2771111 -4.2780023]]...]
INFO - root - 2017-12-07 11:30:50.752601: step 2810, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.600 sec/batch; 72h:38m:33s remains)
INFO - root - 2017-12-07 11:31:07.114754: step 2820, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.701 sec/batch; 77h:12m:47s remains)
INFO - root - 2017-12-07 11:31:23.229801: step 2830, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.615 sec/batch; 73h:18m:36s remains)
INFO - root - 2017-12-07 11:31:39.606148: step 2840, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.680 sec/batch; 76h:16m:00s remains)
INFO - root - 2017-12-07 11:31:55.730647: step 2850, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.717 sec/batch; 77h:56m:12s remains)
INFO - root - 2017-12-07 11:32:11.719874: step 2860, loss = 2.06, batch loss = 2.01 (10.6 examples/sec; 1.510 sec/batch; 68h:33m:15s remains)
INFO - root - 2017-12-07 11:32:27.992083: step 2870, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.735 sec/batch; 78h:44m:10s remains)
INFO - root - 2017-12-07 11:32:44.112012: step 2880, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.613 sec/batch; 73h:11m:58s remains)
INFO - root - 2017-12-07 11:33:00.486637: step 2890, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.606 sec/batch; 72h:53m:00s remains)
INFO - root - 2017-12-07 11:33:16.505540: step 2900, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.637 sec/batch; 74h:18m:05s remains)
2017-12-07 11:33:18.101920: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3232641 -4.3197536 -4.3142233 -4.3059506 -4.2896585 -4.2644925 -4.2505655 -4.2577133 -4.2791905 -4.3047142 -4.3263283 -4.3347807 -4.3270984 -4.3032889 -4.2773275][-4.3255835 -4.3250346 -4.3178792 -4.3015265 -4.2730703 -4.2370739 -4.2183938 -4.2328558 -4.2658334 -4.2986794 -4.3246679 -4.3388686 -4.3380108 -4.321352 -4.2948413][-4.3212342 -4.3209291 -4.3111639 -4.2878642 -4.2474084 -4.1972146 -4.1714349 -4.1914873 -4.2395082 -4.281867 -4.3144889 -4.3367276 -4.3433456 -4.3335452 -4.3088427][-4.3147058 -4.3142586 -4.3026776 -4.2708888 -4.2167549 -4.1517339 -4.1164126 -4.1401196 -4.2062049 -4.2610974 -4.3002219 -4.3272996 -4.3409276 -4.3375335 -4.3176594][-4.3096781 -4.3105626 -4.2989955 -4.2578349 -4.187263 -4.1059566 -4.0557232 -4.079535 -4.1612349 -4.2326365 -4.2811007 -4.3128939 -4.3316321 -4.3336411 -4.3200827][-4.3100252 -4.3096337 -4.2969122 -4.2488403 -4.1635 -4.0619345 -3.9855013 -3.9979115 -4.0940623 -4.1890106 -4.251236 -4.289444 -4.3155069 -4.3262753 -4.3186674][-4.3125129 -4.3083692 -4.2955136 -4.2458391 -4.1512756 -4.0297527 -3.9204097 -3.9061675 -4.0124874 -4.137764 -4.2201486 -4.2671971 -4.2979689 -4.31703 -4.3173442][-4.3175249 -4.3129249 -4.30227 -4.2553163 -4.1585574 -4.0230627 -3.8821335 -3.8275757 -3.9362473 -4.0928349 -4.1979585 -4.2508931 -4.2816148 -4.3046002 -4.312479][-4.3211951 -4.320199 -4.3133731 -4.2743063 -4.1851354 -4.0519018 -3.9002943 -3.8095741 -3.8978434 -4.0656242 -4.1858716 -4.2433257 -4.2711983 -4.2946596 -4.3076272][-4.3260303 -4.3311791 -4.3276825 -4.2975464 -4.2229786 -4.10654 -3.9710209 -3.8690171 -3.9127936 -4.058394 -4.1788182 -4.238461 -4.2642722 -4.2881317 -4.3058863][-4.33418 -4.3446093 -4.3426495 -4.3156648 -4.2523122 -4.1516142 -4.0363493 -3.9374921 -3.9429936 -4.0546827 -4.1681423 -4.2309918 -4.2601027 -4.2872224 -4.3086586][-4.3411837 -4.3535013 -4.3523717 -4.3250308 -4.2726226 -4.1895561 -4.0942855 -4.0051217 -3.987658 -4.0638161 -4.1614113 -4.2237854 -4.2582684 -4.2891507 -4.3134046][-4.3438659 -4.3543248 -4.3517261 -4.3248205 -4.2828169 -4.2187033 -4.1407723 -4.0625777 -4.03409 -4.0808477 -4.1596613 -4.2191215 -4.2573705 -4.290422 -4.3144593][-4.3400455 -4.3473887 -4.3421416 -4.317862 -4.285521 -4.2385044 -4.1786904 -4.1165681 -4.0865788 -4.1080117 -4.163178 -4.2145443 -4.2531176 -4.2883062 -4.31283][-4.332943 -4.3369093 -4.3301849 -4.311162 -4.2868543 -4.2549348 -4.21234 -4.1652513 -4.13653 -4.1353188 -4.1648836 -4.2069154 -4.2467904 -4.2865238 -4.314528]]...]
INFO - root - 2017-12-07 11:33:34.224956: step 2910, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 1.509 sec/batch; 68h:27m:04s remains)
INFO - root - 2017-12-07 11:33:50.431409: step 2920, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.558 sec/batch; 70h:41m:23s remains)
INFO - root - 2017-12-07 11:34:06.622925: step 2930, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.661 sec/batch; 75h:21m:05s remains)
INFO - root - 2017-12-07 11:34:22.790339: step 2940, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.595 sec/batch; 72h:22m:08s remains)
INFO - root - 2017-12-07 11:34:39.065164: step 2950, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.650 sec/batch; 74h:51m:28s remains)
INFO - root - 2017-12-07 11:34:55.249131: step 2960, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.579 sec/batch; 71h:36m:29s remains)
INFO - root - 2017-12-07 11:35:11.664276: step 2970, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 1.705 sec/batch; 77h:18m:34s remains)
INFO - root - 2017-12-07 11:35:27.702780: step 2980, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.625 sec/batch; 73h:43m:06s remains)
INFO - root - 2017-12-07 11:35:43.917060: step 2990, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.662 sec/batch; 75h:21m:02s remains)
INFO - root - 2017-12-07 11:36:00.314354: step 3000, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 1.687 sec/batch; 76h:28m:47s remains)
2017-12-07 11:36:01.684107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1563473 -4.140245 -4.1110325 -4.0913153 -4.0939488 -4.10038 -4.0993342 -4.0892987 -4.0875 -4.0933857 -4.0856357 -4.0714874 -4.0876327 -4.1115966 -4.1193604][-4.1467018 -4.1368523 -4.1218271 -4.107492 -4.1040039 -4.1030688 -4.0988927 -4.0871825 -4.0907717 -4.1088843 -4.11294 -4.1017113 -4.111228 -4.1232796 -4.1149092][-4.1458378 -4.13682 -4.1278353 -4.1153493 -4.1090508 -4.0987434 -4.0809841 -4.0614004 -4.0713573 -4.111835 -4.1371579 -4.1347251 -4.13932 -4.142818 -4.1216035][-4.1546206 -4.1415443 -4.13212 -4.1196084 -4.1113095 -4.0896134 -4.0526195 -4.0166464 -4.0284157 -4.0961185 -4.1474233 -4.1574616 -4.1599741 -4.1581788 -4.1307878][-4.1666508 -4.1501203 -4.1379209 -4.1226749 -4.1128764 -4.0838804 -4.0300436 -3.9745517 -3.9821007 -4.0735631 -4.1492352 -4.1712112 -4.1728873 -4.1679978 -4.1402698][-4.1777859 -4.1607809 -4.1456838 -4.1291022 -4.1214285 -4.0903854 -4.0251946 -3.9522817 -3.949578 -4.04749 -4.1384773 -4.1742444 -4.1782436 -4.1717057 -4.1466718][-4.1754651 -4.1619167 -4.1509771 -4.1407905 -4.1382208 -4.1098108 -4.0438719 -3.9635856 -3.9470634 -4.0325818 -4.1254606 -4.1703167 -4.1781583 -4.1712112 -4.1495719][-4.1617289 -4.1540971 -4.1542339 -4.1555548 -4.1594481 -4.1361938 -4.0747437 -3.9905882 -3.9527824 -4.0129766 -4.0988331 -4.1504183 -4.1651583 -4.1618214 -4.1450129][-4.1451836 -4.1402955 -4.150044 -4.1613889 -4.1718721 -4.1555223 -4.1005039 -4.0158677 -3.9609702 -3.9932523 -4.0621581 -4.1118197 -4.1350212 -4.1418047 -4.1348538][-4.1295 -4.1234951 -4.1381454 -4.158534 -4.1766 -4.1681323 -4.1251869 -4.0557332 -4.0062604 -4.0196552 -4.0587554 -4.0868278 -4.1059089 -4.1197333 -4.1247692][-4.1288424 -4.1232948 -4.14099 -4.1655073 -4.1849442 -4.1796784 -4.1504421 -4.1049356 -4.0734296 -4.0791674 -4.0929127 -4.0968847 -4.1028872 -4.1149716 -4.1251225][-4.1492577 -4.1474338 -4.1661735 -4.1864891 -4.2000575 -4.1953092 -4.1805677 -4.1588612 -4.1454935 -4.1490769 -4.14788 -4.1332941 -4.1214905 -4.1229944 -4.1317234][-4.1703234 -4.1738157 -4.1924281 -4.2063241 -4.2125711 -4.2095833 -4.2061753 -4.2037377 -4.2027907 -4.20494 -4.1950307 -4.1705585 -4.147274 -4.140285 -4.1454663][-4.1863642 -4.1967587 -4.2141385 -4.2230768 -4.2260361 -4.2244711 -4.2267237 -4.234 -4.2383437 -4.2390976 -4.2267613 -4.1993985 -4.1728663 -4.1638455 -4.1669645][-4.2010512 -4.216558 -4.2318878 -4.2381215 -4.2412024 -4.2400165 -4.2431498 -4.2527957 -4.2568321 -4.255651 -4.2443733 -4.2180781 -4.1943216 -4.191823 -4.1962862]]...]
INFO - root - 2017-12-07 11:36:18.025955: step 3010, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 1.560 sec/batch; 70h:44m:59s remains)
INFO - root - 2017-12-07 11:36:34.366632: step 3020, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 71h:35m:41s remains)
INFO - root - 2017-12-07 11:36:50.533621: step 3030, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.531 sec/batch; 69h:23m:58s remains)
INFO - root - 2017-12-07 11:37:06.555399: step 3040, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.577 sec/batch; 71h:30m:35s remains)
INFO - root - 2017-12-07 11:37:23.028715: step 3050, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.727 sec/batch; 78h:16m:15s remains)
INFO - root - 2017-12-07 11:37:39.252108: step 3060, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.600 sec/batch; 72h:32m:25s remains)
INFO - root - 2017-12-07 11:37:55.659116: step 3070, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.713 sec/batch; 77h:39m:29s remains)
INFO - root - 2017-12-07 11:38:11.807995: step 3080, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.603 sec/batch; 72h:40m:32s remains)
INFO - root - 2017-12-07 11:38:28.012119: step 3090, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.685 sec/batch; 76h:21m:25s remains)
INFO - root - 2017-12-07 11:38:43.935919: step 3100, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.576 sec/batch; 71h:26m:07s remains)
2017-12-07 11:38:45.287317: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1035647 -4.0856123 -4.1882253 -4.2888684 -4.3365717 -4.3471274 -4.3239741 -4.2744279 -4.2162366 -4.1548886 -4.1135488 -4.1014252 -4.0974188 -4.1209879 -4.155642][-4.1031742 -4.06866 -4.1687775 -4.2846632 -4.3408837 -4.3467813 -4.3125916 -4.2512488 -4.189404 -4.1380978 -4.1037025 -4.0925436 -4.0883822 -4.1047106 -4.1328034][-4.1116071 -4.0656533 -4.1579752 -4.2805681 -4.3375463 -4.3321319 -4.2829108 -4.2121687 -4.1580329 -4.137723 -4.121593 -4.1122289 -4.1053019 -4.1117196 -4.1262894][-4.1270857 -4.0805278 -4.1628356 -4.2794747 -4.3295312 -4.3106031 -4.2399015 -4.159565 -4.1258459 -4.1506963 -4.1593733 -4.1503472 -4.1397309 -4.1396408 -4.1435552][-4.1431918 -4.101686 -4.1739697 -4.2768884 -4.3168945 -4.283504 -4.1848321 -4.0963855 -4.0968504 -4.1650786 -4.193944 -4.1813116 -4.1694765 -4.1672578 -4.1611438][-4.1504588 -4.1183276 -4.1840296 -4.2716937 -4.2994852 -4.2441936 -4.111515 -4.0166321 -4.0611458 -4.1686621 -4.2117596 -4.1946645 -4.1803164 -4.1793532 -4.1743159][-4.1553278 -4.1326466 -4.1912084 -4.2604375 -4.2695107 -4.1854877 -4.0147905 -3.9245427 -4.0248938 -4.1685987 -4.2240233 -4.2020555 -4.1739874 -4.1629114 -4.1588049][-4.1525774 -4.1341877 -4.1854911 -4.2385912 -4.2299638 -4.1217861 -3.9275045 -3.8622413 -4.018878 -4.1845083 -4.24543 -4.2204709 -4.1744075 -4.1412411 -4.1253591][-4.1477046 -4.1285615 -4.1721535 -4.2156444 -4.1934733 -4.0734043 -3.8897238 -3.8665245 -4.055892 -4.2225676 -4.2815285 -4.2507415 -4.1800966 -4.1162667 -4.0840273][-4.1454482 -4.1282058 -4.1653562 -4.2059731 -4.18079 -4.0659266 -3.9199431 -3.9367712 -4.1240954 -4.2719197 -4.3225641 -4.2857928 -4.1931844 -4.0985756 -4.0472727][-4.1480579 -4.1390891 -4.1764736 -4.2228231 -4.206934 -4.1056271 -3.9994886 -4.0345869 -4.1923413 -4.3131809 -4.35628 -4.3195286 -4.2194448 -4.1040893 -4.0291719][-4.1720095 -4.1702518 -4.2006106 -4.2465625 -4.2417026 -4.1631045 -4.0896716 -4.117775 -4.2281 -4.3252845 -4.3654633 -4.3397985 -4.2521482 -4.1345196 -4.0406437][-4.1952066 -4.1967673 -4.2194853 -4.2605138 -4.2657614 -4.210248 -4.1576037 -4.1645813 -4.2246552 -4.301753 -4.3487396 -4.3410988 -4.2796569 -4.175849 -4.0735455][-4.2040076 -4.203445 -4.2151623 -4.2431407 -4.2470884 -4.209702 -4.1770992 -4.1703048 -4.1943431 -4.2593985 -4.3193021 -4.3333817 -4.2990065 -4.21246 -4.1095963][-4.2057433 -4.2028103 -4.2040782 -4.2186422 -4.221611 -4.2022023 -4.1880941 -4.1749353 -4.1761537 -4.2300992 -4.3009157 -4.3347054 -4.3176193 -4.2421031 -4.1417127]]...]
INFO - root - 2017-12-07 11:39:01.613681: step 3110, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.576 sec/batch; 71h:24m:10s remains)
INFO - root - 2017-12-07 11:39:18.051947: step 3120, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.686 sec/batch; 76h:23m:49s remains)
INFO - root - 2017-12-07 11:39:34.268490: step 3130, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.688 sec/batch; 76h:28m:34s remains)
INFO - root - 2017-12-07 11:39:50.456443: step 3140, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.574 sec/batch; 71h:18m:00s remains)
INFO - root - 2017-12-07 11:40:06.602140: step 3150, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.657 sec/batch; 75h:04m:25s remains)
INFO - root - 2017-12-07 11:40:22.664966: step 3160, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.592 sec/batch; 72h:07m:16s remains)
INFO - root - 2017-12-07 11:40:38.955416: step 3170, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 1.551 sec/batch; 70h:15m:25s remains)
INFO - root - 2017-12-07 11:40:55.261235: step 3180, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.689 sec/batch; 76h:30m:16s remains)
INFO - root - 2017-12-07 11:41:11.520118: step 3190, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.648 sec/batch; 74h:38m:57s remains)
INFO - root - 2017-12-07 11:41:27.941165: step 3200, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.713 sec/batch; 77h:35m:23s remains)
2017-12-07 11:41:29.342032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2706919 -4.2771845 -4.2721906 -4.261497 -4.2472467 -4.2227654 -4.2029514 -4.2054467 -4.2216587 -4.2362256 -4.2405076 -4.2316937 -4.2288613 -4.2416258 -4.2576804][-4.2843966 -4.2867427 -4.2809644 -4.2691765 -4.2474375 -4.2152185 -4.19486 -4.2019477 -4.22051 -4.2365527 -4.2453351 -4.2462559 -4.254334 -4.2718091 -4.2897215][-4.2929044 -4.300921 -4.30112 -4.2905965 -4.2649655 -4.2295337 -4.2050014 -4.2096505 -4.2271266 -4.239646 -4.2517042 -4.2594447 -4.2748237 -4.29518 -4.314208][-4.3020959 -4.3132505 -4.31616 -4.3037848 -4.2762728 -4.2320509 -4.191494 -4.1892395 -4.2150178 -4.23532 -4.2502828 -4.2584877 -4.276402 -4.3007317 -4.3184938][-4.3092637 -4.3167014 -4.3143239 -4.2987728 -4.2717447 -4.2191744 -4.1604214 -4.14987 -4.1861095 -4.2189918 -4.2387991 -4.2498145 -4.269711 -4.2966557 -4.3122573][-4.3114953 -4.3175616 -4.31159 -4.2907782 -4.2587075 -4.1984687 -4.1161351 -4.0873227 -4.1364117 -4.1878529 -4.2200222 -4.239099 -4.2618961 -4.286561 -4.2919474][-4.3164258 -4.3222218 -4.3112183 -4.2789021 -4.23144 -4.154542 -4.0350804 -3.9730439 -4.0452337 -4.1332393 -4.1906743 -4.2237458 -4.2491903 -4.2630906 -4.2541537][-4.3262162 -4.3325663 -4.3146081 -4.268085 -4.1966686 -4.0908246 -3.9275322 -3.8268154 -3.9335759 -4.0741978 -4.1642456 -4.2153 -4.2403903 -4.2397437 -4.2154365][-4.34308 -4.3503675 -4.32422 -4.2628007 -4.1733952 -4.0537434 -3.8865809 -3.7862968 -3.9072459 -4.0693665 -4.1707573 -4.2236991 -4.2410879 -4.2270145 -4.1860862][-4.3577242 -4.3650403 -4.335052 -4.2678447 -4.1779113 -4.0736465 -3.9555645 -3.9029677 -3.9961102 -4.1180439 -4.1931787 -4.2258906 -4.2315035 -4.2059517 -4.1547642][-4.3638124 -4.37137 -4.3435078 -4.2773757 -4.1940756 -4.1110325 -4.0372066 -4.0206342 -4.0818386 -4.1540194 -4.2008057 -4.2165537 -4.2144504 -4.1866941 -4.1368237][-4.3637872 -4.3727531 -4.3510184 -4.2939224 -4.2251477 -4.1631346 -4.1146092 -4.107964 -4.1393151 -4.1728182 -4.1970043 -4.2032762 -4.2030454 -4.1900864 -4.1588778][-4.3613715 -4.368896 -4.3544312 -4.3155069 -4.265028 -4.2171965 -4.1816673 -4.177052 -4.1883755 -4.1954489 -4.2052178 -4.2109365 -4.2165322 -4.220459 -4.2115912][-4.3568788 -4.3620586 -4.355247 -4.3317451 -4.2995677 -4.2651863 -4.2422018 -4.2410765 -4.2411041 -4.23639 -4.2387934 -4.2418303 -4.2501593 -4.2632313 -4.2667079][-4.3474708 -4.35158 -4.3508606 -4.3397536 -4.3197389 -4.2961717 -4.2803259 -4.2788806 -4.2767487 -4.2708235 -4.2725673 -4.2744927 -4.2806468 -4.291842 -4.29772]]...]
INFO - root - 2017-12-07 11:41:45.366595: step 3210, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.636 sec/batch; 74h:04m:14s remains)
INFO - root - 2017-12-07 11:42:01.659798: step 3220, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.634 sec/batch; 74h:00m:53s remains)
INFO - root - 2017-12-07 11:42:17.988362: step 3230, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.664 sec/batch; 75h:21m:04s remains)
INFO - root - 2017-12-07 11:42:34.143760: step 3240, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.581 sec/batch; 71h:35m:25s remains)
INFO - root - 2017-12-07 11:42:50.538075: step 3250, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 1.750 sec/batch; 79h:14m:38s remains)
INFO - root - 2017-12-07 11:43:06.827462: step 3260, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.611 sec/batch; 72h:56m:20s remains)
INFO - root - 2017-12-07 11:43:22.933065: step 3270, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.663 sec/batch; 75h:17m:32s remains)
INFO - root - 2017-12-07 11:43:39.056464: step 3280, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.589 sec/batch; 71h:56m:18s remains)
INFO - root - 2017-12-07 11:43:55.485945: step 3290, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.560 sec/batch; 70h:36m:25s remains)
INFO - root - 2017-12-07 11:44:11.824547: step 3300, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.688 sec/batch; 76h:23m:41s remains)
2017-12-07 11:44:13.323606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2546287 -4.2216763 -4.1844378 -4.1545148 -4.139473 -4.1321073 -4.11812 -4.110321 -4.1411171 -4.2005606 -4.2470632 -4.2675657 -4.2534456 -4.2022123 -4.1404858][-4.255373 -4.2225795 -4.1889963 -4.1615739 -4.1443872 -4.1293693 -4.104651 -4.0910521 -4.123045 -4.184588 -4.2341509 -4.2584209 -4.2477441 -4.1883478 -4.1070056][-4.2593279 -4.23171 -4.2033215 -4.1784658 -4.1547618 -4.1273632 -4.087214 -4.0641627 -4.0956907 -4.15832 -4.2126989 -4.2457156 -4.2434168 -4.1875744 -4.0945463][-4.26045 -4.2413344 -4.2187624 -4.1946878 -4.1576385 -4.10949 -4.0463505 -4.0164566 -4.0607786 -4.1342263 -4.1972165 -4.241828 -4.2508945 -4.2073636 -4.1155324][-4.2547293 -4.2477317 -4.2324109 -4.2029352 -4.1423831 -4.063427 -3.9755976 -3.9466014 -4.021143 -4.1183767 -4.1946406 -4.2487235 -4.2654147 -4.2342858 -4.1517911][-4.2458205 -4.2488308 -4.2360888 -4.1972747 -4.1173072 -4.0139556 -3.9095643 -3.8862238 -3.9920397 -4.1121521 -4.1972532 -4.2565722 -4.2801533 -4.2597084 -4.1931753][-4.2272758 -4.233335 -4.2196341 -4.1746616 -4.0908518 -3.9850948 -3.8834858 -3.8685794 -3.9809854 -4.10263 -4.1895485 -4.2522249 -4.2829022 -4.271275 -4.2212462][-4.2020278 -4.2099152 -4.1968312 -4.1500416 -4.0722556 -3.9801216 -3.8983614 -3.8933258 -3.9888632 -4.0947056 -4.1737676 -4.2356949 -4.2716489 -4.2671275 -4.2313585][-4.1959324 -4.2032208 -4.1893153 -4.1433663 -4.074832 -4.0024424 -3.9452496 -3.9475632 -4.0219059 -4.1055479 -4.1725669 -4.2293363 -4.2653608 -4.2659154 -4.240057][-4.2109604 -4.2146053 -4.1991343 -4.1549711 -4.0960855 -4.0408182 -4.0049391 -4.0136185 -4.0695739 -4.134798 -4.19263 -4.245379 -4.2785773 -4.2802577 -4.2619729][-4.23471 -4.2345376 -4.2187076 -4.1767178 -4.1246581 -4.0812936 -4.0601587 -4.074357 -4.1190681 -4.1737046 -4.22621 -4.2742481 -4.3025122 -4.303978 -4.2920403][-4.2570243 -4.2555885 -4.2408414 -4.2026634 -4.1574178 -4.1206293 -4.1055007 -4.1238351 -4.1640072 -4.2127318 -4.2605009 -4.3011961 -4.3230681 -4.325233 -4.3200378][-4.2746854 -4.2742305 -4.2622814 -4.2318034 -4.1957793 -4.1649075 -4.1518064 -4.1704865 -4.2085009 -4.2513194 -4.2896533 -4.3201408 -4.3359995 -4.3397093 -4.3391032][-4.2877 -4.28986 -4.2825627 -4.2600713 -4.2336793 -4.2093143 -4.197679 -4.2139292 -4.2467995 -4.2809372 -4.3105 -4.3320537 -4.3426213 -4.346312 -4.34706][-4.3006873 -4.3042135 -4.3006635 -4.2858753 -4.2681136 -4.2508097 -4.24157 -4.251924 -4.2749786 -4.3007894 -4.3239479 -4.3383274 -4.3450179 -4.347261 -4.3474951]]...]
INFO - root - 2017-12-07 11:44:29.427523: step 3310, loss = 2.06, batch loss = 2.01 (10.3 examples/sec; 1.549 sec/batch; 70h:07m:28s remains)
INFO - root - 2017-12-07 11:44:45.579193: step 3320, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.554 sec/batch; 70h:20m:17s remains)
INFO - root - 2017-12-07 11:45:01.886036: step 3330, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.715 sec/batch; 77h:38m:04s remains)
INFO - root - 2017-12-07 11:45:17.901129: step 3340, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.527 sec/batch; 69h:06m:11s remains)
INFO - root - 2017-12-07 11:45:34.272049: step 3350, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.686 sec/batch; 76h:17m:49s remains)
INFO - root - 2017-12-07 11:45:50.407642: step 3360, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.571 sec/batch; 71h:04m:45s remains)
INFO - root - 2017-12-07 11:46:06.679871: step 3370, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.651 sec/batch; 74h:40m:40s remains)
INFO - root - 2017-12-07 11:46:22.774591: step 3380, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 71h:25m:56s remains)
INFO - root - 2017-12-07 11:46:39.102078: step 3390, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.689 sec/batch; 76h:24m:56s remains)
INFO - root - 2017-12-07 11:46:55.098334: step 3400, loss = 2.07, batch loss = 2.02 (10.4 examples/sec; 1.544 sec/batch; 69h:51m:09s remains)
2017-12-07 11:46:56.546639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1877775 -4.18914 -4.1907282 -4.1917057 -4.1922083 -4.1930504 -4.1948371 -4.1967931 -4.1984534 -4.1993537 -4.1996555 -4.1991105 -4.1974573 -4.1949091 -4.1925364][-4.1782742 -4.1794715 -4.1813726 -4.1831584 -4.184742 -4.1870542 -4.1906738 -4.1946054 -4.1982207 -4.2009244 -4.2023249 -4.2017555 -4.1988282 -4.1940012 -4.1892877][-4.16331 -4.1624823 -4.1624184 -4.1628871 -4.1640439 -4.1673951 -4.173636 -4.1815777 -4.19009 -4.1981826 -4.2040548 -4.2057786 -4.2026753 -4.1956387 -4.1877995][-4.1509223 -4.1450782 -4.1396394 -4.1351643 -4.1328096 -4.1352105 -4.1434975 -4.1564641 -4.1722689 -4.188858 -4.20287 -4.2103839 -4.2097163 -4.2025871 -4.1928492][-4.1453753 -4.13223 -4.118258 -4.1047373 -4.0943174 -4.0913539 -4.097578 -4.1124659 -4.1345453 -4.1610508 -4.1859269 -4.2028384 -4.2088127 -4.2059703 -4.1982613][-4.1468763 -4.12608 -4.1021485 -4.0774288 -4.0558958 -4.0434084 -4.0427537 -4.0542765 -4.0776877 -4.1104407 -4.1448708 -4.1725826 -4.1887383 -4.1944532 -4.1931581][-4.1518607 -4.1254783 -4.0937119 -4.0599971 -4.0292983 -4.0084414 -3.999893 -4.0043445 -4.0231919 -4.055501 -4.0934811 -4.1284 -4.1540995 -4.1699271 -4.1771884][-4.1595526 -4.1325264 -4.0995989 -4.06478 -4.0330405 -4.0104017 -3.9983625 -3.9962323 -4.005949 -4.0294151 -4.0614319 -4.0948 -4.1236262 -4.1460047 -4.1602888][-4.1695495 -4.1472111 -4.1203351 -4.0924411 -4.0676174 -4.04989 -4.0397267 -4.035059 -4.0368767 -4.0483046 -4.0675774 -4.0908923 -4.1144586 -4.1363459 -4.1527653][-4.1814852 -4.1669264 -4.1496463 -4.1325064 -4.1181355 -4.1082764 -4.1025205 -4.0984497 -4.0960674 -4.0979705 -4.1047525 -4.1154246 -4.12875 -4.1444087 -4.1580029][-4.1920438 -4.1852183 -4.1771502 -4.1697712 -4.1640091 -4.1601124 -4.1577225 -4.1550484 -4.1517358 -4.149157 -4.1487713 -4.1509185 -4.1553311 -4.1629491 -4.1707292][-4.1986809 -4.1970806 -4.1951342 -4.1938448 -4.1929293 -4.1921048 -4.1912527 -4.1895509 -4.1869068 -4.18406 -4.1819263 -4.1807642 -4.1807323 -4.1827369 -4.184906][-4.2001543 -4.2004433 -4.2008438 -4.2015805 -4.2021737 -4.2020435 -4.201396 -4.1999769 -4.1982293 -4.1964092 -4.1951332 -4.1942277 -4.1936517 -4.1937556 -4.1932521][-4.1991677 -4.1994567 -4.1999722 -4.2007113 -4.2013254 -4.2014618 -4.2011132 -4.2001681 -4.19908 -4.1980505 -4.1972146 -4.1965718 -4.1960912 -4.1957369 -4.194653][-4.1992197 -4.1990218 -4.1989546 -4.1990561 -4.1992788 -4.1995239 -4.1996584 -4.1994729 -4.1991482 -4.1986332 -4.1978488 -4.1969814 -4.1961188 -4.1952229 -4.1938276]]...]
INFO - root - 2017-12-07 11:47:12.880810: step 3410, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.721 sec/batch; 77h:49m:57s remains)
INFO - root - 2017-12-07 11:47:29.018964: step 3420, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.585 sec/batch; 71h:40m:28s remains)
INFO - root - 2017-12-07 11:47:45.462166: step 3430, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.637 sec/batch; 74h:01m:13s remains)
INFO - root - 2017-12-07 11:48:01.474944: step 3440, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.611 sec/batch; 72h:52m:00s remains)
INFO - root - 2017-12-07 11:48:17.816153: step 3450, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.609 sec/batch; 72h:44m:32s remains)
INFO - root - 2017-12-07 11:48:34.038445: step 3460, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.724 sec/batch; 77h:57m:29s remains)
INFO - root - 2017-12-07 11:48:50.232441: step 3470, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.612 sec/batch; 72h:54m:23s remains)
INFO - root - 2017-12-07 11:49:06.736940: step 3480, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 1.686 sec/batch; 76h:13m:40s remains)
INFO - root - 2017-12-07 11:49:22.875025: step 3490, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.607 sec/batch; 72h:39m:38s remains)
INFO - root - 2017-12-07 11:49:39.129572: step 3500, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.718 sec/batch; 77h:40m:51s remains)
2017-12-07 11:49:40.457882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3216209 -4.336359 -4.3449426 -4.34107 -4.3326597 -4.3224926 -4.3154449 -4.317667 -4.3294721 -4.3458858 -4.3598824 -4.3637161 -4.3476424 -4.3027296 -4.2366867][-4.317595 -4.3376455 -4.3483081 -4.3419747 -4.3271551 -4.308732 -4.2922611 -4.2912731 -4.3091516 -4.3301034 -4.3430738 -4.3478527 -4.3305945 -4.2810082 -4.2097716][-4.2923889 -4.3207912 -4.3356252 -4.3274555 -4.3051672 -4.27569 -4.2473035 -4.24189 -4.2712064 -4.3020587 -4.3203769 -4.3246651 -4.3043637 -4.2509046 -4.1797261][-4.2649755 -4.2976532 -4.3144131 -4.307188 -4.2770634 -4.2321396 -4.1886621 -4.1775188 -4.2210655 -4.2696919 -4.2969275 -4.3014503 -4.2777858 -4.2207155 -4.1581039][-4.2628431 -4.2929139 -4.30108 -4.2891722 -4.2504435 -4.1880407 -4.1239653 -4.0984874 -4.1526756 -4.2250996 -4.2707071 -4.2824645 -4.2660952 -4.2169161 -4.1686907][-4.2787647 -4.2976518 -4.2945452 -4.2734976 -4.2287068 -4.1536665 -4.0659719 -4.0112586 -4.0667105 -4.1692877 -4.2386312 -4.2660723 -4.266479 -4.2385893 -4.2079616][-4.3053269 -4.3086276 -4.2961636 -4.2690368 -4.2183914 -4.1336555 -4.0227165 -3.9270566 -3.97227 -4.105144 -4.203042 -4.2499127 -4.2701283 -4.2620659 -4.2443318][-4.3241115 -4.3135791 -4.2954483 -4.267158 -4.2140865 -4.1241312 -3.9965637 -3.8662963 -3.8949366 -4.052711 -4.1806159 -4.2492604 -4.2834816 -4.2841129 -4.2713661][-4.3251328 -4.3055077 -4.2856574 -4.2608452 -4.2136374 -4.1344967 -4.0179472 -3.8849165 -3.8980427 -4.0522003 -4.1907248 -4.2694383 -4.3055773 -4.3029509 -4.282825][-4.3105578 -4.2844191 -4.2669396 -4.2519703 -4.2176938 -4.1610503 -4.0770459 -3.9756103 -3.9804463 -4.103209 -4.2243862 -4.2936368 -4.319025 -4.3026333 -4.2613921][-4.2929287 -4.2660713 -4.2492986 -4.2427006 -4.2213907 -4.1857867 -4.137629 -4.078001 -4.0816793 -4.1705465 -4.2633362 -4.3103118 -4.3155017 -4.2826152 -4.2213721][-4.272975 -4.2490406 -4.2321157 -4.2305632 -4.220511 -4.2050118 -4.1841831 -4.154192 -4.1589079 -4.222713 -4.2933965 -4.32107 -4.310194 -4.265625 -4.1926918][-4.2476654 -4.2340212 -4.2198076 -4.2183409 -4.2149367 -4.2130241 -4.2094774 -4.1970377 -4.2036824 -4.2506127 -4.3040133 -4.3225031 -4.3087649 -4.2667794 -4.1984324][-4.2260852 -4.224946 -4.2181983 -4.2153172 -4.2141323 -4.2202983 -4.2247734 -4.2218437 -4.230207 -4.2646322 -4.3014321 -4.3163171 -4.3104138 -4.2826948 -4.2325387][-4.2211742 -4.2323184 -4.2319574 -4.2276707 -4.2249603 -4.2325635 -4.2405858 -4.2438755 -4.2531214 -4.2740583 -4.2961931 -4.3098073 -4.3156714 -4.3069 -4.2784977]]...]
INFO - root - 2017-12-07 11:49:56.899197: step 3510, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.685 sec/batch; 76h:10m:14s remains)
INFO - root - 2017-12-07 11:50:12.994800: step 3520, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.595 sec/batch; 72h:05m:42s remains)
INFO - root - 2017-12-07 11:50:29.174306: step 3530, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 1.702 sec/batch; 76h:55m:01s remains)
INFO - root - 2017-12-07 11:50:45.480251: step 3540, loss = 2.05, batch loss = 2.00 (9.9 examples/sec; 1.611 sec/batch; 72h:48m:54s remains)
INFO - root - 2017-12-07 11:51:01.726165: step 3550, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.652 sec/batch; 74h:40m:14s remains)
INFO - root - 2017-12-07 11:51:17.645680: step 3560, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.578 sec/batch; 71h:19m:49s remains)
INFO - root - 2017-12-07 11:51:33.872204: step 3570, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.580 sec/batch; 71h:22m:45s remains)
INFO - root - 2017-12-07 11:51:49.935512: step 3580, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.678 sec/batch; 75h:49m:26s remains)
INFO - root - 2017-12-07 11:52:06.204494: step 3590, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.608 sec/batch; 72h:39m:53s remains)
INFO - root - 2017-12-07 11:52:22.531218: step 3600, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 1.681 sec/batch; 75h:56m:23s remains)
2017-12-07 11:52:23.843557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1648188 -4.1780696 -4.2005177 -4.2211957 -4.2311006 -4.2321362 -4.2327194 -4.2289581 -4.2216568 -4.2180319 -4.2233129 -4.2319865 -4.23856 -4.2430406 -4.2457342][-4.1200404 -4.1349974 -4.1618934 -4.1881604 -4.2018924 -4.2064614 -4.2126293 -4.2172828 -4.2166576 -4.2174 -4.2249217 -4.2351155 -4.2416263 -4.244339 -4.24279][-4.1177554 -4.1326137 -4.1587029 -4.1853123 -4.1957488 -4.1975121 -4.2046475 -4.214282 -4.2197981 -4.2242479 -4.2337227 -4.2449937 -4.2522349 -4.2551608 -4.2534628][-4.1436133 -4.1585083 -4.1808357 -4.2018447 -4.2036791 -4.1985545 -4.2010365 -4.2102747 -4.2189851 -4.2255 -4.2352591 -4.2465706 -4.2551956 -4.2622004 -4.2645][-4.1553788 -4.1713896 -4.1896491 -4.2022 -4.1956639 -4.1842532 -4.1791539 -4.181314 -4.1879063 -4.194953 -4.2058034 -4.2195992 -4.2336588 -4.2484994 -4.2584033][-4.1670485 -4.1794996 -4.1878929 -4.1878858 -4.1712289 -4.1519713 -4.1390638 -4.1333494 -4.134387 -4.13937 -4.1520615 -4.1708026 -4.1920977 -4.2152143 -4.2329354][-4.1708074 -4.1768956 -4.1749382 -4.1617985 -4.1351295 -4.1096225 -4.0936193 -4.0879788 -4.0911241 -4.0986414 -4.114871 -4.1373258 -4.1619873 -4.1877861 -4.211071][-4.177042 -4.1788778 -4.16937 -4.1467519 -4.1146469 -4.0870342 -4.0714984 -4.0683894 -4.0744047 -4.0850544 -4.1053543 -4.1313319 -4.1563807 -4.1795559 -4.2039413][-4.1973023 -4.1928763 -4.1780629 -4.1535506 -4.1242518 -4.1002131 -4.089201 -4.0902267 -4.0981483 -4.1090803 -4.1286221 -4.1532078 -4.1734743 -4.1881213 -4.2053671][-4.2175164 -4.2107711 -4.197051 -4.1773105 -4.1567097 -4.1421361 -4.1396818 -4.1462 -4.1545782 -4.1614761 -4.1743283 -4.1910944 -4.2019539 -4.2043853 -4.208652][-4.2358809 -4.2304535 -4.2213058 -4.2079353 -4.1949639 -4.1875916 -4.1895289 -4.1983123 -4.2066264 -4.2106647 -4.216876 -4.2244539 -4.2259 -4.21836 -4.2099738][-4.2393737 -4.2354589 -4.2295842 -4.2208338 -4.2125125 -4.2092257 -4.2134714 -4.2226858 -4.2307882 -4.2343888 -4.2367587 -4.2364006 -4.2306371 -4.2177672 -4.2032824][-4.2059321 -4.2043762 -4.2023411 -4.1976862 -4.1928225 -4.1918473 -4.1963139 -4.20544 -4.2147236 -4.2200747 -4.2208457 -4.2154655 -4.20588 -4.1927166 -4.1783805][-4.1720448 -4.1751604 -4.1798573 -4.1818175 -4.1812515 -4.1816792 -4.1841764 -4.1898222 -4.1961155 -4.199636 -4.1970258 -4.1861591 -4.1691723 -4.1511836 -4.1358166][-4.1916828 -4.1990933 -4.2106133 -4.2201576 -4.2244663 -4.2257848 -4.2253346 -4.2246075 -4.2231369 -4.2191744 -4.2099838 -4.1933255 -4.1696053 -4.1469235 -4.1322622]]...]
INFO - root - 2017-12-07 11:52:39.876301: step 3610, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.725 sec/batch; 77h:55m:51s remains)
INFO - root - 2017-12-07 11:52:56.079312: step 3620, loss = 2.05, batch loss = 2.00 (10.2 examples/sec; 1.570 sec/batch; 70h:55m:51s remains)
INFO - root - 2017-12-07 11:53:12.390879: step 3630, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.702 sec/batch; 76h:53m:38s remains)
INFO - root - 2017-12-07 11:53:28.376898: step 3640, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.581 sec/batch; 71h:24m:33s remains)
INFO - root - 2017-12-07 11:53:44.743858: step 3650, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.727 sec/batch; 78h:00m:13s remains)
INFO - root - 2017-12-07 11:54:00.899683: step 3660, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.571 sec/batch; 70h:56m:29s remains)
INFO - root - 2017-12-07 11:54:17.081962: step 3670, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.671 sec/batch; 75h:28m:56s remains)
INFO - root - 2017-12-07 11:54:33.300150: step 3680, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.595 sec/batch; 72h:02m:43s remains)
INFO - root - 2017-12-07 11:54:49.331652: step 3690, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.557 sec/batch; 70h:18m:30s remains)
INFO - root - 2017-12-07 11:55:05.409152: step 3700, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.613 sec/batch; 72h:49m:31s remains)
2017-12-07 11:55:06.800785: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3386245 -4.3402586 -4.3404851 -4.3430157 -4.3471107 -4.350842 -4.3537 -4.3560209 -4.3574944 -4.356864 -4.3535957 -4.3492546 -4.3457208 -4.3448124 -4.3464389][-4.3335228 -4.3323488 -4.3321204 -4.3356824 -4.3410268 -4.3456783 -4.3494568 -4.352612 -4.3549981 -4.3549213 -4.3519478 -4.3488817 -4.348597 -4.3488741 -4.3481293][-4.3239393 -4.3198133 -4.3183985 -4.3236032 -4.330296 -4.3350158 -4.3387265 -4.34369 -4.34929 -4.351975 -4.3520923 -4.3530045 -4.35715 -4.3591747 -4.3536258][-4.268764 -4.26827 -4.26962 -4.2783942 -4.2881927 -4.2950416 -4.2984471 -4.3046818 -4.3132992 -4.3173409 -4.3194814 -4.3273258 -4.337533 -4.3426452 -4.3376923][-4.1902494 -4.1897483 -4.1924515 -4.2055984 -4.2197285 -4.2279959 -4.2320666 -4.2410622 -4.2522917 -4.2585254 -4.266582 -4.2815642 -4.294241 -4.2978067 -4.2903728][-4.122323 -4.113 -4.1109543 -4.121438 -4.1353884 -4.1406827 -4.1464748 -4.1651983 -4.1810875 -4.1962771 -4.2192512 -4.2436347 -4.2566547 -4.2546086 -4.2373266][-4.0739818 -4.0582452 -4.046545 -4.0501437 -4.0615807 -4.0590897 -4.0639439 -4.0959678 -4.1191711 -4.1442766 -4.1833844 -4.2139564 -4.2201538 -4.2025537 -4.1718273][-4.0740867 -4.0557313 -4.0390487 -4.0407495 -4.0492463 -4.0396795 -4.0419025 -4.078227 -4.1014724 -4.1257439 -4.1623416 -4.1825023 -4.1710114 -4.134325 -4.1027093][-4.1261058 -4.1151977 -4.1036286 -4.106185 -4.1104107 -4.0968556 -4.0927668 -4.1143508 -4.1268291 -4.1394534 -4.1518455 -4.14463 -4.1109848 -4.0678024 -4.0535979][-4.1757841 -4.1792035 -4.1782165 -4.1832647 -4.1825933 -4.1685576 -4.159153 -4.1632085 -4.1609173 -4.1555929 -4.1401229 -4.1079841 -4.0572147 -4.0169978 -4.0288382][-4.2109394 -4.2282052 -4.2387137 -4.2448864 -4.2399178 -4.2265983 -4.2159491 -4.2082567 -4.1917925 -4.168818 -4.1388144 -4.1040668 -4.0588803 -4.0272212 -4.0456018][-4.2239809 -4.2473664 -4.2634215 -4.2721052 -4.2684188 -4.2559786 -4.2376871 -4.2162609 -4.188911 -4.1579118 -4.1287594 -4.1068964 -4.0818443 -4.0656981 -4.0806866][-4.22966 -4.251677 -4.26759 -4.27881 -4.2768116 -4.2662997 -4.2441678 -4.2122378 -4.1738048 -4.139936 -4.1203361 -4.1123462 -4.1054358 -4.1031055 -4.1205196][-4.2278223 -4.2458529 -4.2630272 -4.277462 -4.2777724 -4.2646303 -4.2373371 -4.2027464 -4.1653032 -4.1387568 -4.1298151 -4.1270847 -4.1294785 -4.1407533 -4.1636667][-4.2208819 -4.2329264 -4.2497039 -4.2651887 -4.2709179 -4.2585664 -4.2301784 -4.1985564 -4.1680961 -4.1493688 -4.1459088 -4.1440487 -4.1545372 -4.1764188 -4.2010565]]...]
INFO - root - 2017-12-07 11:55:23.067189: step 3710, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 1.685 sec/batch; 76h:04m:20s remains)
INFO - root - 2017-12-07 11:55:39.261896: step 3720, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.623 sec/batch; 73h:16m:54s remains)
INFO - root - 2017-12-07 11:55:55.459378: step 3730, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.619 sec/batch; 73h:06m:18s remains)
INFO - root - 2017-12-07 11:56:11.788512: step 3740, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.682 sec/batch; 75h:55m:04s remains)
INFO - root - 2017-12-07 11:56:28.045101: step 3750, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.603 sec/batch; 72h:22m:46s remains)
INFO - root - 2017-12-07 11:56:44.070953: step 3760, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.701 sec/batch; 76h:47m:41s remains)
INFO - root - 2017-12-07 11:57:00.139823: step 3770, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.536 sec/batch; 69h:19m:27s remains)
INFO - root - 2017-12-07 11:57:16.589300: step 3780, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 1.756 sec/batch; 79h:15m:17s remains)
INFO - root - 2017-12-07 11:57:32.610438: step 3790, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.609 sec/batch; 72h:37m:45s remains)
INFO - root - 2017-12-07 11:57:48.999209: step 3800, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.673 sec/batch; 75h:30m:33s remains)
2017-12-07 11:57:50.399822: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3518357 -4.3361669 -4.3158269 -4.2957435 -4.2784495 -4.2591925 -4.2480011 -4.2509851 -4.2556076 -4.2599707 -4.2632251 -4.2676964 -4.2671127 -4.2550554 -4.2372046][-4.3428273 -4.32434 -4.3010907 -4.2763066 -4.2533722 -4.2274704 -4.2129693 -4.2138863 -4.2166905 -4.2187757 -4.2248192 -4.2328887 -4.2292309 -4.2105513 -4.185092][-4.3383923 -4.3200827 -4.294209 -4.2656183 -4.2373214 -4.2068367 -4.1851006 -4.1764312 -4.1748466 -4.176456 -4.189404 -4.20108 -4.1904788 -4.1646986 -4.1338606][-4.3345408 -4.3168845 -4.2894306 -4.2580705 -4.2267203 -4.1887612 -4.1532598 -4.13589 -4.1340532 -4.1342497 -4.1498609 -4.1625557 -4.1496391 -4.1194654 -4.082531][-4.32735 -4.3091688 -4.2779593 -4.2401233 -4.2060914 -4.1643486 -4.1197624 -4.095139 -4.0920496 -4.0895367 -4.1045437 -4.1216683 -4.113462 -4.0769029 -4.033875][-4.317976 -4.2958155 -4.2567406 -4.2116404 -4.1775527 -4.1375527 -4.092164 -4.0604391 -4.0484605 -4.0397844 -4.0543284 -4.0789409 -4.0762191 -4.0354848 -3.9909053][-4.3055224 -4.2776237 -4.2349043 -4.1904593 -4.1620383 -4.1318812 -4.088871 -4.0453405 -4.0197716 -4.0081367 -4.0200329 -4.0382457 -4.0330644 -3.9925203 -3.9532342][-4.2961292 -4.2649951 -4.2237263 -4.1890984 -4.1723671 -4.15277 -4.1127863 -4.0670543 -4.0381451 -4.0260797 -4.0264554 -4.0246754 -4.0112295 -3.9703097 -3.9350011][-4.293263 -4.263319 -4.2261438 -4.2001138 -4.1898937 -4.1766143 -4.1443539 -4.1118326 -4.0895324 -4.07917 -4.0709529 -4.0525837 -4.0315385 -3.99385 -3.9633603][-4.2958026 -4.2660818 -4.2323337 -4.212791 -4.2080407 -4.1984906 -4.1768551 -4.1635108 -4.1532564 -4.1458464 -4.1295495 -4.1017809 -4.0829773 -4.0570459 -4.0322504][-4.2963142 -4.2667584 -4.2362146 -4.2221484 -4.22108 -4.2180486 -4.2097592 -4.2099032 -4.2077665 -4.1995454 -4.1781783 -4.1491 -4.1330185 -4.1138239 -4.0891333][-4.3017073 -4.2749033 -4.251689 -4.243474 -4.2416196 -4.2420025 -4.2431545 -4.247952 -4.2455196 -4.2328396 -4.2089763 -4.1837668 -4.1707554 -4.1550932 -4.1325774][-4.3083334 -4.2868419 -4.27229 -4.2707438 -4.27201 -4.2759371 -4.2798581 -4.2827029 -4.2767067 -4.25745 -4.2328453 -4.21525 -4.2071853 -4.195478 -4.1797333][-4.3144073 -4.2970667 -4.2878175 -4.2884936 -4.2914038 -4.2963443 -4.3001308 -4.3009276 -4.29434 -4.2749047 -4.2541671 -4.2438283 -4.2408876 -4.2356472 -4.22694][-4.3245583 -4.3084769 -4.2981129 -4.2949123 -4.295248 -4.2967706 -4.2965646 -4.2959328 -4.2938194 -4.2832294 -4.2717957 -4.2661414 -4.2647161 -4.2626829 -4.2606692]]...]
INFO - root - 2017-12-07 11:58:06.637120: step 3810, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.691 sec/batch; 76h:17m:48s remains)
INFO - root - 2017-12-07 11:58:22.744359: step 3820, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.619 sec/batch; 73h:01m:37s remains)
INFO - root - 2017-12-07 11:58:39.090281: step 3830, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.656 sec/batch; 74h:41m:43s remains)
INFO - root - 2017-12-07 11:58:55.207143: step 3840, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.546 sec/batch; 69h:43m:55s remains)
INFO - root - 2017-12-07 11:59:11.310922: step 3850, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.580 sec/batch; 71h:17m:33s remains)
INFO - root - 2017-12-07 11:59:27.514989: step 3860, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.670 sec/batch; 75h:20m:52s remains)
INFO - root - 2017-12-07 11:59:43.605181: step 3870, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.568 sec/batch; 70h:44m:30s remains)
INFO - root - 2017-12-07 11:59:59.867548: step 3880, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 1.737 sec/batch; 78h:19m:39s remains)
INFO - root - 2017-12-07 12:00:15.885558: step 3890, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 1.607 sec/batch; 72h:28m:24s remains)
INFO - root - 2017-12-07 12:00:32.028290: step 3900, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.656 sec/batch; 74h:39m:41s remains)
2017-12-07 12:00:33.375561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1489768 -4.1210136 -4.1105576 -4.1241837 -4.1450386 -4.1613321 -4.1674967 -4.1730528 -4.1811886 -4.1943159 -4.2092133 -4.2083321 -4.1982012 -4.175858 -4.1675968][-4.1612639 -4.1329236 -4.11772 -4.1215086 -4.1382403 -4.1517096 -4.1535711 -4.1542211 -4.1624603 -4.17966 -4.2033739 -4.209322 -4.2033482 -4.1845741 -4.1817074][-4.1855583 -4.1660886 -4.1499505 -4.1445408 -4.1503468 -4.15508 -4.1438651 -4.13347 -4.14177 -4.1628952 -4.1930494 -4.2055449 -4.2024813 -4.1857419 -4.1815209][-4.19264 -4.1922193 -4.185432 -4.1737628 -4.1692638 -4.1643891 -4.1396546 -4.1181636 -4.1275544 -4.1525764 -4.1843357 -4.19903 -4.1953855 -4.1773181 -4.1667295][-4.1732807 -4.1877794 -4.1887164 -4.1749983 -4.1622181 -4.1467543 -4.1097236 -4.080575 -4.0960588 -4.1325855 -4.1650229 -4.17824 -4.1702514 -4.1501088 -4.1323705][-4.1240654 -4.15139 -4.1653075 -4.1575179 -4.1404037 -4.1093092 -4.0556855 -4.0105557 -4.0320792 -4.0898652 -4.1322241 -4.1478195 -4.1445627 -4.1254115 -4.1040463][-4.0729856 -4.1006703 -4.1320148 -4.1376543 -4.1185331 -4.0696177 -3.9938118 -3.9234929 -3.9483263 -4.0325928 -4.0943332 -4.1191282 -4.1215119 -4.1089497 -4.0898075][-4.0565844 -4.0740824 -4.1161461 -4.1373091 -4.1242952 -4.07263 -3.9964471 -3.9182 -3.9297605 -4.0114 -4.0705557 -4.0947905 -4.097537 -4.0906096 -4.0750418][-4.072298 -4.0876513 -4.1330123 -4.1616397 -4.1594524 -4.1248693 -4.0756459 -4.0203094 -4.0127249 -4.0523357 -4.0800023 -4.0885158 -4.0878305 -4.0854959 -4.0770125][-4.1040831 -4.115725 -4.1542492 -4.1798468 -4.183392 -4.1671348 -4.1454458 -4.116559 -4.1045537 -4.1155815 -4.1162629 -4.1083927 -4.1033525 -4.1063418 -4.1106129][-4.1498919 -4.152894 -4.1746168 -4.1914 -4.1976867 -4.1943889 -4.1897044 -4.1759658 -4.1649928 -4.16417 -4.1524596 -4.1421709 -4.1401792 -4.1478076 -4.1646433][-4.1884284 -4.1894312 -4.1995773 -4.2069354 -4.2108216 -4.2105637 -4.2097707 -4.202599 -4.19089 -4.1865263 -4.178071 -4.1773582 -4.18368 -4.1957793 -4.2201452][-4.21861 -4.2200651 -4.2230835 -4.2249346 -4.2252955 -4.2220221 -4.2156019 -4.2078333 -4.1979904 -4.1941762 -4.1917996 -4.2008343 -4.2133007 -4.227633 -4.2523303][-4.2448306 -4.2474918 -4.2478752 -4.2458868 -4.2441339 -4.2382054 -4.2289681 -4.2203326 -4.2120981 -4.2078791 -4.2098818 -4.2263694 -4.2457819 -4.2589588 -4.2735381][-4.2663803 -4.27075 -4.2725658 -4.26849 -4.2660937 -4.26321 -4.2585592 -4.2537284 -4.2452941 -4.2389779 -4.24028 -4.25676 -4.2756939 -4.2838945 -4.2861128]]...]
INFO - root - 2017-12-07 12:00:49.805456: step 3910, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 1.705 sec/batch; 76h:52m:26s remains)
INFO - root - 2017-12-07 12:01:05.894957: step 3920, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 1.560 sec/batch; 70h:21m:07s remains)
INFO - root - 2017-12-07 12:01:22.224323: step 3930, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 1.705 sec/batch; 76h:52m:58s remains)
INFO - root - 2017-12-07 12:01:38.413072: step 3940, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 1.607 sec/batch; 72h:27m:19s remains)
INFO - root - 2017-12-07 12:01:54.607691: step 3950, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.641 sec/batch; 73h:59m:01s remains)
INFO - root - 2017-12-07 12:02:10.738005: step 3960, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.665 sec/batch; 75h:04m:34s remains)
INFO - root - 2017-12-07 12:02:26.840149: step 3970, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.598 sec/batch; 72h:02m:32s remains)
INFO - root - 2017-12-07 12:02:43.096549: step 3980, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.589 sec/batch; 71h:37m:05s remains)
INFO - root - 2017-12-07 12:02:59.314740: step 3990, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.663 sec/batch; 74h:55m:57s remains)
INFO - root - 2017-12-07 12:03:15.404550: step 4000, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.602 sec/batch; 72h:10m:55s remains)
2017-12-07 12:03:16.714998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1916571 -4.1802187 -4.179153 -4.1825724 -4.1814885 -4.1708541 -4.1553397 -4.1486545 -4.1476226 -4.1423106 -4.1438546 -4.1591892 -4.1833882 -4.2080522 -4.2268071][-4.1794987 -4.1727018 -4.1768379 -4.1805019 -4.1770973 -4.1657419 -4.1482649 -4.1422553 -4.1450348 -4.1434474 -4.1465678 -4.1594224 -4.1805696 -4.2027316 -4.2197661][-4.1713486 -4.1720734 -4.1787243 -4.1827483 -4.1771026 -4.1611266 -4.1389022 -4.1349683 -4.145442 -4.1504831 -4.1564789 -4.1672778 -4.1837893 -4.19773 -4.2055664][-4.1734281 -4.1784544 -4.1878085 -4.1928983 -4.1860514 -4.1659484 -4.1380758 -4.1330161 -4.1511006 -4.1644979 -4.1741567 -4.1807995 -4.1868224 -4.1884513 -4.1855927][-4.1965485 -4.1974072 -4.2054796 -4.2113318 -4.2035532 -4.1800618 -4.14589 -4.1317873 -4.1492529 -4.1712017 -4.1886487 -4.1950459 -4.1916847 -4.180388 -4.1661353][-4.2229891 -4.2159634 -4.220015 -4.2229776 -4.2129788 -4.1855464 -4.1429238 -4.1150036 -4.1260071 -4.1578059 -4.186976 -4.1991949 -4.1958375 -4.1806812 -4.1584072][-4.2397232 -4.229485 -4.2303309 -4.2289114 -4.2136383 -4.1795473 -4.1237941 -4.0735292 -4.0750775 -4.1248121 -4.1717844 -4.1937661 -4.1998258 -4.19102 -4.16924][-4.2382631 -4.2270217 -4.225121 -4.2206964 -4.2032304 -4.1655827 -4.096982 -4.01889 -4.0080256 -4.07904 -4.1500092 -4.1861067 -4.2063689 -4.2073979 -4.1910405][-4.2250242 -4.216135 -4.2154741 -4.2125688 -4.1957641 -4.1610007 -4.0957355 -4.0087972 -3.9840021 -4.0582075 -4.1401205 -4.1866927 -4.2145686 -4.221293 -4.2094107][-4.2159138 -4.2148924 -4.2171674 -4.2161446 -4.1995516 -4.1726465 -4.1285887 -4.0658345 -4.0425043 -4.0951877 -4.1620922 -4.20412 -4.2298026 -4.234446 -4.2249155][-4.2149138 -4.2260656 -4.2320762 -4.2314663 -4.2167139 -4.1970158 -4.1730304 -4.1377816 -4.1189365 -4.1485963 -4.1939578 -4.2273984 -4.2481356 -4.251009 -4.2438521][-4.2174096 -4.2392578 -4.2518692 -4.256547 -4.2483974 -4.2348762 -4.2194214 -4.1973233 -4.1762123 -4.1855135 -4.210566 -4.2337351 -4.2493687 -4.25119 -4.2460852][-4.2174959 -4.240912 -4.2558942 -4.267076 -4.26912 -4.2629013 -4.2506652 -4.2316 -4.2081542 -4.2013469 -4.2083921 -4.2213488 -4.2334013 -4.2362857 -4.2337847][-4.2049603 -4.2235785 -4.2410507 -4.2591152 -4.2701316 -4.2726765 -4.2661238 -4.251348 -4.2297807 -4.2144694 -4.2094793 -4.2130022 -4.2207041 -4.2233725 -4.2194076][-4.1916838 -4.2031717 -4.2168546 -4.2337747 -4.2477961 -4.2588429 -4.263875 -4.2579041 -4.2425013 -4.2283125 -4.2201171 -4.2200294 -4.2230954 -4.2204762 -4.2122364]]...]
INFO - root - 2017-12-07 12:03:33.041831: step 4010, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.580 sec/batch; 71h:12m:30s remains)
INFO - root - 2017-12-07 12:03:49.035401: step 4020, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.645 sec/batch; 74h:08m:40s remains)
INFO - root - 2017-12-07 12:04:05.397825: step 4030, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.641 sec/batch; 73h:57m:41s remains)
INFO - root - 2017-12-07 12:04:21.752655: step 4040, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 1.727 sec/batch; 77h:50m:17s remains)
INFO - root - 2017-12-07 12:04:37.686895: step 4050, loss = 2.08, batch loss = 2.03 (10.8 examples/sec; 1.483 sec/batch; 66h:50m:21s remains)
INFO - root - 2017-12-07 12:04:54.089571: step 4060, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.712 sec/batch; 77h:07m:17s remains)
INFO - root - 2017-12-07 12:05:10.393823: step 4070, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.624 sec/batch; 73h:10m:04s remains)
INFO - root - 2017-12-07 12:05:26.641960: step 4080, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.659 sec/batch; 74h:45m:18s remains)
INFO - root - 2017-12-07 12:05:42.867905: step 4090, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.578 sec/batch; 71h:05m:56s remains)
INFO - root - 2017-12-07 12:05:59.283312: step 4100, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.635 sec/batch; 73h:37m:54s remains)
2017-12-07 12:06:00.652012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2438278 -4.2416091 -4.244925 -4.2460613 -4.2428241 -4.234261 -4.2242951 -4.2197218 -4.2201438 -4.222702 -4.2210307 -4.2174811 -4.2178731 -4.2257581 -4.23644][-4.2364731 -4.2343645 -4.2379375 -4.2386632 -4.2345972 -4.2238159 -4.2104011 -4.203 -4.2013049 -4.2034216 -4.2004175 -4.1925926 -4.1903939 -4.1974025 -4.2117333][-4.2307391 -4.2302942 -4.23449 -4.233541 -4.2264757 -4.2112074 -4.1925254 -4.1786904 -4.171586 -4.1723318 -4.1699486 -4.1590915 -4.1528678 -4.1591039 -4.1792846][-4.2338815 -4.2381592 -4.2449565 -4.244184 -4.2356339 -4.2178516 -4.1965876 -4.176928 -4.1617103 -4.1535258 -4.1467304 -4.1310344 -4.117661 -4.1194658 -4.14258][-4.2417331 -4.2513323 -4.2602816 -4.261241 -4.2521849 -4.2349515 -4.2157497 -4.1975121 -4.1803641 -4.1629844 -4.1486783 -4.1271491 -4.1058197 -4.0992627 -4.1173587][-4.2446809 -4.2574267 -4.2666926 -4.2684703 -4.2579775 -4.2411342 -4.2262907 -4.212079 -4.1969266 -4.1756935 -4.1547575 -4.1311593 -4.1078773 -4.096664 -4.1094213][-4.2451386 -4.2597647 -4.2717009 -4.2766838 -4.2668948 -4.2495155 -4.2371 -4.2266884 -4.213315 -4.1899815 -4.1617193 -4.1349268 -4.1126232 -4.1017342 -4.1119356][-4.2460432 -4.2622886 -4.2775192 -4.2865615 -4.2800264 -4.2624278 -4.2509203 -4.2438054 -4.231739 -4.2082715 -4.1746273 -4.1429129 -4.1203632 -4.1107488 -4.1186528][-4.2513428 -4.2686105 -4.286274 -4.2977581 -4.2952213 -4.2790771 -4.2678304 -4.2621717 -4.2497716 -4.2259049 -4.1906304 -4.1552019 -4.1315141 -4.1235538 -4.1303682][-4.258749 -4.2762218 -4.2945061 -4.3084326 -4.3103995 -4.2987003 -4.2886271 -4.282371 -4.2691836 -4.2457485 -4.2134557 -4.1795354 -4.1579328 -4.1528788 -4.1591158][-4.2682486 -4.284039 -4.3012395 -4.3164282 -4.3244185 -4.3202386 -4.3135548 -4.3075957 -4.2954378 -4.274828 -4.2479153 -4.2190447 -4.1999216 -4.1945925 -4.198647][-4.2797956 -4.2933335 -4.3092885 -4.3254852 -4.3380165 -4.3410177 -4.3383632 -4.334168 -4.3246994 -4.3083973 -4.2863159 -4.2612181 -4.2425928 -4.2345338 -4.2349486][-4.2881303 -4.2997012 -4.3135738 -4.3280787 -4.3408971 -4.3476162 -4.3483539 -4.3458195 -4.3387103 -4.3262434 -4.30901 -4.2884655 -4.2716284 -4.2625079 -4.2599859][-4.2904029 -4.2981439 -4.3074455 -4.3182778 -4.328403 -4.3348813 -4.3373365 -4.336915 -4.332963 -4.3243752 -4.3121767 -4.2975364 -4.2849832 -4.2768564 -4.2732611][-4.2879128 -4.2907772 -4.294591 -4.3003855 -4.3065 -4.3110771 -4.3136621 -4.314239 -4.3123612 -4.3074241 -4.3008604 -4.2933478 -4.2863803 -4.2814989 -4.2792907]]...]
INFO - root - 2017-12-07 12:06:16.985707: step 4110, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.583 sec/batch; 71h:16m:49s remains)
INFO - root - 2017-12-07 12:06:33.198497: step 4120, loss = 2.09, batch loss = 2.04 (9.8 examples/sec; 1.631 sec/batch; 73h:27m:27s remains)
INFO - root - 2017-12-07 12:06:49.476884: step 4130, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.561 sec/batch; 70h:17m:49s remains)
INFO - root - 2017-12-07 12:07:05.759429: step 4140, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.660 sec/batch; 74h:44m:30s remains)
INFO - root - 2017-12-07 12:07:21.881632: step 4150, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.646 sec/batch; 74h:05m:56s remains)
INFO - root - 2017-12-07 12:07:38.324150: step 4160, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 73h:23m:47s remains)
INFO - root - 2017-12-07 12:07:54.299830: step 4170, loss = 2.07, batch loss = 2.02 (10.5 examples/sec; 1.529 sec/batch; 68h:49m:53s remains)
INFO - root - 2017-12-07 12:08:10.655080: step 4180, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.689 sec/batch; 76h:02m:50s remains)
INFO - root - 2017-12-07 12:08:26.771023: step 4190, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.576 sec/batch; 70h:55m:44s remains)
INFO - root - 2017-12-07 12:08:43.103715: step 4200, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.658 sec/batch; 74h:37m:49s remains)
2017-12-07 12:08:44.420056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.16698 -4.173686 -4.1848154 -4.1931748 -4.1981139 -4.201858 -4.2031922 -4.2020588 -4.2004924 -4.19997 -4.201642 -4.1989322 -4.18569 -4.1589184 -4.1404142][-4.2136745 -4.215064 -4.223845 -4.2322426 -4.2367969 -4.2337 -4.2248549 -4.220078 -4.220233 -4.2208061 -4.2219052 -4.2190356 -4.2054744 -4.1769729 -4.1551294][-4.2750897 -4.2782393 -4.2866492 -4.2941484 -4.2971892 -4.2880712 -4.2727118 -4.2657547 -4.2682629 -4.2716494 -4.2736263 -4.270432 -4.259841 -4.2360497 -4.2141528][-4.3167615 -4.3212304 -4.3254142 -4.3264613 -4.3260503 -4.3163943 -4.2996759 -4.291235 -4.2984037 -4.3095379 -4.3168669 -4.316884 -4.3117914 -4.2953243 -4.2757292][-4.3174024 -4.3153882 -4.3117304 -4.3052907 -4.29977 -4.2877226 -4.2680025 -4.2574577 -4.2734127 -4.3004231 -4.3215079 -4.3319416 -4.3375788 -4.3317924 -4.3189049][-4.2982354 -4.2799358 -4.2634258 -4.2463403 -4.22953 -4.2069826 -4.1797724 -4.1649766 -4.1884551 -4.2352095 -4.2778664 -4.3042016 -4.3227482 -4.3280315 -4.32565][-4.2741652 -4.2397561 -4.2102585 -4.1804814 -4.1466379 -4.1034703 -4.0617819 -4.0377588 -4.0596228 -4.1233282 -4.194397 -4.2449021 -4.2787056 -4.2963762 -4.3069086][-4.2612338 -4.2160897 -4.1760168 -4.1398897 -4.0944605 -4.0337105 -3.9734516 -3.9320149 -3.9402354 -4.0036793 -4.0936112 -4.1707754 -4.22083 -4.2551789 -4.2829113][-4.26954 -4.2237973 -4.1840892 -4.1513352 -4.1101437 -4.0518651 -3.9898674 -3.9397039 -3.9264979 -3.9654615 -4.048552 -4.13259 -4.1850471 -4.2250133 -4.2634091][-4.2916751 -4.2609854 -4.2311759 -4.2094545 -4.1819468 -4.1421919 -4.1010036 -4.0646381 -4.0415726 -4.0469284 -4.0939269 -4.1504779 -4.181253 -4.2105947 -4.248672][-4.3096337 -4.2947855 -4.2787852 -4.2675052 -4.2526035 -4.2338858 -4.2182937 -4.2018185 -4.1792545 -4.1640334 -4.1756287 -4.1931558 -4.1967273 -4.2108183 -4.2439222][-4.3159227 -4.3086925 -4.3061748 -4.3033237 -4.2971396 -4.2919126 -4.2924109 -4.2889524 -4.2702851 -4.249783 -4.2431259 -4.2355356 -4.2214651 -4.2260575 -4.2535539][-4.3179989 -4.3130708 -4.3210068 -4.3266535 -4.3256803 -4.3241482 -4.3271885 -4.3276582 -4.3125625 -4.2952404 -4.2832513 -4.26403 -4.2415113 -4.2429838 -4.2686515][-4.3118367 -4.3084116 -4.323709 -4.3363481 -4.3397236 -4.3397412 -4.3405147 -4.3394856 -4.3269444 -4.3125892 -4.3015089 -4.2821145 -4.2578177 -4.25673 -4.2815347][-4.2963419 -4.2919717 -4.3088341 -4.3263664 -4.333251 -4.3344336 -4.3331094 -4.3314371 -4.3206983 -4.3080235 -4.2995048 -4.286181 -4.2673736 -4.2666597 -4.28885]]...]
INFO - root - 2017-12-07 12:09:00.658064: step 4210, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.675 sec/batch; 75h:22m:50s remains)
INFO - root - 2017-12-07 12:09:16.883223: step 4220, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.608 sec/batch; 72h:22m:01s remains)
INFO - root - 2017-12-07 12:09:33.342059: step 4230, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.649 sec/batch; 74h:11m:39s remains)
INFO - root - 2017-12-07 12:09:49.568752: step 4240, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.667 sec/batch; 75h:00m:23s remains)
INFO - root - 2017-12-07 12:10:05.566890: step 4250, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.600 sec/batch; 72h:00m:29s remains)
INFO - root - 2017-12-07 12:10:21.868113: step 4260, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 1.604 sec/batch; 72h:09m:51s remains)
INFO - root - 2017-12-07 12:10:38.077743: step 4270, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.637 sec/batch; 73h:40m:19s remains)
INFO - root - 2017-12-07 12:10:54.401637: step 4280, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.591 sec/batch; 71h:35m:02s remains)
INFO - root - 2017-12-07 12:11:10.533140: step 4290, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.693 sec/batch; 76h:08m:45s remains)
INFO - root - 2017-12-07 12:11:26.660261: step 4300, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.554 sec/batch; 69h:53m:16s remains)
2017-12-07 12:11:28.013617: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3287287 -4.3304176 -4.3308043 -4.3252907 -4.3188243 -4.310596 -4.304347 -4.3017917 -4.3012705 -4.3020487 -4.303257 -4.3064213 -4.3123426 -4.3191662 -4.3259506][-4.3317037 -4.3312807 -4.3298397 -4.3216343 -4.3107147 -4.2963004 -4.2857604 -4.2794147 -4.277555 -4.2805009 -4.2868662 -4.2950654 -4.3051324 -4.3150373 -4.3242092][-4.331882 -4.3275356 -4.3216615 -4.3091621 -4.2919383 -4.2712736 -4.2576928 -4.2489743 -4.244585 -4.2501984 -4.2640262 -4.2796421 -4.2941942 -4.3086786 -4.3210335][-4.3273931 -4.31879 -4.3103137 -4.2949109 -4.2705832 -4.2429838 -4.2268672 -4.2158685 -4.2090354 -4.2148814 -4.2346048 -4.2585764 -4.2806191 -4.3011413 -4.3165417][-4.3130646 -4.3016262 -4.2917562 -4.2750545 -4.2443814 -4.2059784 -4.1800237 -4.1632137 -4.1586337 -4.173728 -4.2020774 -4.2341037 -4.2641068 -4.2896004 -4.3082151][-4.29459 -4.2807665 -4.2718005 -4.2575684 -4.2200356 -4.1607766 -4.1054931 -4.0676365 -4.0667033 -4.1068411 -4.1552496 -4.2006392 -4.24238 -4.2783852 -4.3020568][-4.2744455 -4.2589607 -4.2497144 -4.2341189 -4.1918559 -4.1154985 -4.0152397 -3.9304345 -3.9187045 -3.9937923 -4.0814176 -4.1538496 -4.2168708 -4.2694812 -4.3005161][-4.2553587 -4.2426629 -4.2373948 -4.2222071 -4.1782837 -4.0896716 -3.9500129 -3.8039947 -3.7638233 -3.8759689 -4.0095906 -4.1166158 -4.20141 -4.2672424 -4.3057504][-4.24511 -4.2424884 -4.2468772 -4.2380953 -4.2028222 -4.1210608 -3.9863858 -3.8378692 -3.7818894 -3.8812466 -4.0109191 -4.120276 -4.2055116 -4.2696557 -4.3092842][-4.2321496 -4.2426472 -4.2577395 -4.2591329 -4.2418704 -4.1903267 -4.103292 -4.0148678 -3.9803839 -4.0291777 -4.0979781 -4.1690454 -4.2318983 -4.2815986 -4.3150759][-4.2264676 -4.245048 -4.2638025 -4.2725406 -4.2728062 -4.2491474 -4.2052875 -4.164588 -4.1467133 -4.1603184 -4.1791377 -4.2105231 -4.248879 -4.2888145 -4.3189626][-4.2376161 -4.2539907 -4.2680693 -4.2714448 -4.2751584 -4.270175 -4.255209 -4.2390504 -4.2249184 -4.2179232 -4.2103934 -4.2186112 -4.2432175 -4.2814994 -4.3145175][-4.2489305 -4.2600145 -4.2693634 -4.2657061 -4.2647214 -4.2668567 -4.2686515 -4.2645626 -4.2464833 -4.2187786 -4.1928978 -4.1909065 -4.2114983 -4.2529259 -4.2925696][-4.2543178 -4.2530441 -4.2550688 -4.2497768 -4.2446465 -4.2472963 -4.2569666 -4.257606 -4.2347832 -4.1930337 -4.1573176 -4.1515226 -4.173368 -4.2198381 -4.2662416][-4.257586 -4.2461667 -4.2410107 -4.2366076 -4.2344437 -4.239543 -4.2522078 -4.25035 -4.2229352 -4.1740193 -4.1324081 -4.1262021 -4.1538115 -4.2052908 -4.255816]]...]
INFO - root - 2017-12-07 12:11:44.116231: step 4310, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.581 sec/batch; 71h:07m:13s remains)
INFO - root - 2017-12-07 12:12:00.666454: step 4320, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.691 sec/batch; 76h:02m:56s remains)
INFO - root - 2017-12-07 12:12:16.880289: step 4330, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 1.560 sec/batch; 70h:09m:34s remains)
INFO - root - 2017-12-07 12:12:33.257786: step 4340, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 1.760 sec/batch; 79h:09m:37s remains)
INFO - root - 2017-12-07 12:12:49.473559: step 4350, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.660 sec/batch; 74h:39m:06s remains)
INFO - root - 2017-12-07 12:13:05.914976: step 4360, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.591 sec/batch; 71h:32m:15s remains)
INFO - root - 2017-12-07 12:13:22.208239: step 4370, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.699 sec/batch; 76h:25m:13s remains)
INFO - root - 2017-12-07 12:13:38.545991: step 4380, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 72h:00m:44s remains)
INFO - root - 2017-12-07 12:13:55.057773: step 4390, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.692 sec/batch; 76h:05m:33s remains)
INFO - root - 2017-12-07 12:14:11.351999: step 4400, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.596 sec/batch; 71h:45m:05s remains)
2017-12-07 12:14:12.770308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2703018 -4.2380362 -4.211256 -4.2030668 -4.2156382 -4.2301893 -4.2404475 -4.2433615 -4.2446909 -4.2486262 -4.2456055 -4.2293882 -4.2090349 -4.1961432 -4.2034678][-4.2661247 -4.2354407 -4.2102866 -4.2024522 -4.2138619 -4.2250428 -4.2275634 -4.2224631 -4.2250466 -4.2393861 -4.2459688 -4.2352896 -4.2167959 -4.2034407 -4.2078385][-4.2672749 -4.2408576 -4.2196536 -4.2120652 -4.2212443 -4.2278743 -4.2221479 -4.2040534 -4.19776 -4.2156882 -4.2322974 -4.2302084 -4.2158904 -4.2054629 -4.2109423][-4.2648983 -4.2435102 -4.2269745 -4.223949 -4.2329917 -4.2352266 -4.2194386 -4.1866269 -4.1628733 -4.1769376 -4.2063437 -4.2188268 -4.2110238 -4.1988316 -4.1987276][-4.2535124 -4.2318487 -4.218544 -4.2205729 -4.2286482 -4.22565 -4.1979585 -4.1496472 -4.1099224 -4.1183653 -4.16156 -4.1912031 -4.1925616 -4.1790423 -4.1705518][-4.2277541 -4.200913 -4.1859531 -4.1849251 -4.1893826 -4.1807508 -4.1448884 -4.0812488 -4.0270653 -4.033411 -4.0912576 -4.1394892 -4.1548648 -4.142581 -4.1281109][-4.1922016 -4.1573296 -4.1336756 -4.1192093 -4.1144972 -4.0968256 -4.0523057 -3.9789238 -3.9208584 -3.9390335 -4.0108891 -4.0701456 -4.0961 -4.0907259 -4.0834956][-4.1756821 -4.13768 -4.1066227 -4.079483 -4.0593953 -4.0286274 -3.977597 -3.9087934 -3.8653102 -3.8979816 -3.9711535 -4.025703 -4.0524731 -4.0620027 -4.0778217][-4.1986637 -4.16564 -4.1343989 -4.1051307 -4.0779662 -4.0434408 -3.9964049 -3.9485326 -3.9278922 -3.9624724 -4.0201497 -4.0589623 -4.0799627 -4.0971217 -4.1254854][-4.2514305 -4.2296915 -4.2060184 -4.1813855 -4.1590042 -4.13342 -4.1025276 -4.0754519 -4.0658197 -4.088418 -4.121994 -4.1430688 -4.1552553 -4.1721907 -4.198235][-4.3143935 -4.3044839 -4.2874174 -4.2682543 -4.2533722 -4.2391758 -4.2240105 -4.2102828 -4.2031345 -4.211576 -4.2242351 -4.2304916 -4.2377958 -4.2533965 -4.2751479][-4.3557172 -4.3548684 -4.344069 -4.331449 -4.3211527 -4.3132129 -4.3045707 -4.2964773 -4.2925267 -4.2945 -4.294868 -4.293087 -4.2981806 -4.3119941 -4.3297687][-4.3673925 -4.3703761 -4.3654032 -4.3575668 -4.3508816 -4.3440137 -4.3366861 -4.3311872 -4.3310738 -4.3328729 -4.330225 -4.3261085 -4.3289766 -4.3401084 -4.353107][-4.3578854 -4.3603568 -4.3581209 -4.3540916 -4.351243 -4.3464265 -4.3404446 -4.337718 -4.3393974 -4.3405156 -4.3375559 -4.3347511 -4.335547 -4.34265 -4.3512678][-4.3429518 -4.3439 -4.3421493 -4.3409266 -4.3409629 -4.3393631 -4.3359947 -4.3346343 -4.3356829 -4.3366847 -4.3355522 -4.33454 -4.3352332 -4.3387532 -4.3428321]]...]
INFO - root - 2017-12-07 12:14:28.832941: step 4410, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 72h:00m:48s remains)
INFO - root - 2017-12-07 12:14:45.007204: step 4420, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.658 sec/batch; 74h:30m:53s remains)
INFO - root - 2017-12-07 12:15:01.366310: step 4430, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.615 sec/batch; 72h:35m:12s remains)
INFO - root - 2017-12-07 12:15:17.574090: step 4440, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.612 sec/batch; 72h:26m:54s remains)
INFO - root - 2017-12-07 12:15:33.840587: step 4450, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.569 sec/batch; 70h:31m:21s remains)
INFO - root - 2017-12-07 12:15:50.287189: step 4460, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.563 sec/batch; 70h:14m:30s remains)
INFO - root - 2017-12-07 12:16:06.405169: step 4470, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.684 sec/batch; 75h:40m:51s remains)
INFO - root - 2017-12-07 12:16:22.595970: step 4480, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.619 sec/batch; 72h:45m:10s remains)
INFO - root - 2017-12-07 12:16:38.938295: step 4490, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.690 sec/batch; 75h:56m:45s remains)
INFO - root - 2017-12-07 12:16:55.302547: step 4500, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.628 sec/batch; 73h:09m:12s remains)
2017-12-07 12:16:56.793932: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2808132 -4.2827396 -4.2882748 -4.2919211 -4.2924495 -4.2906976 -4.2891126 -4.2857819 -4.2815433 -4.2822871 -4.2884545 -4.2961774 -4.3068986 -4.3167033 -4.3245387][-4.2414494 -4.2414312 -4.2493873 -4.2544017 -4.2555819 -4.2533245 -4.2487755 -4.2425456 -4.2344379 -4.2341814 -4.2453179 -4.261343 -4.2806497 -4.2966423 -4.3117771][-4.1826987 -4.1825891 -4.194613 -4.2038789 -4.2068138 -4.20532 -4.1968694 -4.1870785 -4.1755004 -4.1722646 -4.1866713 -4.2136121 -4.2461014 -4.2712517 -4.2946787][-4.1200423 -4.1220217 -4.1410718 -4.1567631 -4.1631427 -4.1632862 -4.1527214 -4.1412477 -4.1308227 -4.1242304 -4.1370726 -4.1709366 -4.2147474 -4.2494526 -4.2797327][-4.0791178 -4.0843725 -4.10877 -4.1288528 -4.1369619 -4.1375937 -4.1274571 -4.1149478 -4.1076918 -4.0984855 -4.1045132 -4.1380625 -4.1903067 -4.2342148 -4.2700953][-4.0729108 -4.0860176 -4.1075029 -4.1257563 -4.1304336 -4.130477 -4.1241741 -4.11449 -4.1066613 -4.0928736 -4.0917878 -4.1205044 -4.1753631 -4.2248273 -4.2645946][-4.103858 -4.1260977 -4.1412592 -4.1498265 -4.1475043 -4.1437182 -4.1424193 -4.138659 -4.1321516 -4.1129222 -4.1050234 -4.1281676 -4.1787767 -4.226367 -4.264956][-4.1590586 -4.1850538 -4.1951776 -4.1927228 -4.1818647 -4.1727777 -4.1723447 -4.1726551 -4.1669774 -4.1482878 -4.1394863 -4.1598959 -4.2033319 -4.2418761 -4.2727213][-4.2204704 -4.2420092 -4.2485118 -4.2373371 -4.2211094 -4.2094812 -4.2083626 -4.2094078 -4.2031217 -4.187634 -4.1819291 -4.2019343 -4.2357655 -4.2619 -4.2819114][-4.2671618 -4.2815509 -4.2836218 -4.2665005 -4.2450724 -4.2302828 -4.229579 -4.2329106 -4.23 -4.2227135 -4.2221813 -4.2405949 -4.2657971 -4.28074 -4.2906194][-4.2923541 -4.3004527 -4.2989316 -4.277998 -4.2508821 -4.2301393 -4.2299709 -4.2369533 -4.2397733 -4.239666 -4.2462511 -4.2660065 -4.2862492 -4.2942777 -4.2997546][-4.306849 -4.3114982 -4.3079491 -4.2867374 -4.25788 -4.2361345 -4.2376738 -4.2477202 -4.2529097 -4.2550597 -4.2628388 -4.2833943 -4.3020573 -4.3064957 -4.3097897][-4.3112564 -4.3143396 -4.3111577 -4.2944841 -4.2713976 -4.2536 -4.2563071 -4.267272 -4.273314 -4.2727256 -4.2741742 -4.2911787 -4.3082795 -4.312017 -4.3151884][-4.311595 -4.3158817 -4.3153944 -4.3056984 -4.2905154 -4.2759895 -4.2771139 -4.2869387 -4.2942824 -4.2939477 -4.291254 -4.3000708 -4.3116064 -4.3156514 -4.3201957][-4.315486 -4.3193259 -4.3200469 -4.3162465 -4.3074765 -4.2954392 -4.2937117 -4.3021059 -4.31095 -4.3122292 -4.308403 -4.3086672 -4.3137069 -4.3176594 -4.3243189]]...]
INFO - root - 2017-12-07 12:17:12.877768: step 4510, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.618 sec/batch; 72h:41m:24s remains)
INFO - root - 2017-12-07 12:17:29.284003: step 4520, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 1.683 sec/batch; 75h:35m:52s remains)
INFO - root - 2017-12-07 12:17:45.459650: step 4530, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.638 sec/batch; 73h:34m:50s remains)
INFO - root - 2017-12-07 12:18:01.436353: step 4540, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.541 sec/batch; 69h:13m:38s remains)
INFO - root - 2017-12-07 12:18:17.981442: step 4550, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.703 sec/batch; 76h:29m:46s remains)
INFO - root - 2017-12-07 12:18:34.186108: step 4560, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.564 sec/batch; 70h:13m:24s remains)
INFO - root - 2017-12-07 12:18:50.624906: step 4570, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.678 sec/batch; 75h:21m:43s remains)
INFO - root - 2017-12-07 12:19:06.784416: step 4580, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.607 sec/batch; 72h:09m:42s remains)
INFO - root - 2017-12-07 12:19:22.978310: step 4590, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.577 sec/batch; 70h:48m:30s remains)
INFO - root - 2017-12-07 12:19:39.108497: step 4600, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.680 sec/batch; 75h:26m:48s remains)
2017-12-07 12:19:40.440215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2183018 -4.180954 -4.1362362 -4.113019 -4.1273522 -4.17077 -4.2190208 -4.249372 -4.2699862 -4.288949 -4.2931604 -4.2894254 -4.2819328 -4.266716 -4.240283][-4.2091932 -4.1634774 -4.1110716 -4.0790768 -4.0847325 -4.1250014 -4.1809993 -4.227438 -4.2619858 -4.2885804 -4.2941575 -4.2840919 -4.2730293 -4.2546797 -4.2290239][-4.2191257 -4.1693554 -4.1119108 -4.06991 -4.060472 -4.0867057 -4.1371369 -4.1933961 -4.2405725 -4.276639 -4.2882009 -4.2772021 -4.2660818 -4.2449951 -4.2180815][-4.242445 -4.195363 -4.1369843 -4.0861292 -4.05791 -4.06126 -4.0956807 -4.1542459 -4.2125449 -4.2577391 -4.2747741 -4.2636137 -4.2508373 -4.227098 -4.1972446][-4.2609477 -4.2238111 -4.1724734 -4.1203656 -4.0764084 -4.0491753 -4.0561371 -4.1066403 -4.1729193 -4.226923 -4.2511134 -4.2434649 -4.2289343 -4.2039604 -4.1743107][-4.2687774 -4.241569 -4.2027812 -4.157742 -4.1043925 -4.0492229 -4.0209937 -4.0516987 -4.1243219 -4.1895595 -4.2234025 -4.2223783 -4.2085838 -4.1848254 -4.159245][-4.2727027 -4.2533431 -4.22517 -4.1871185 -4.1302409 -4.0577979 -4.0025597 -4.0100484 -4.0805731 -4.1532669 -4.1956768 -4.2076526 -4.2032623 -4.1849332 -4.1589432][-4.2725668 -4.2606211 -4.23746 -4.2029133 -4.149014 -4.0734086 -4.0049372 -3.9951839 -4.057755 -4.1294656 -4.1780548 -4.2021747 -4.2079086 -4.1946306 -4.1648469][-4.2618642 -4.2614994 -4.2412462 -4.2091994 -4.1602902 -4.0898633 -4.0195532 -4.00168 -4.0528288 -4.1205692 -4.1736226 -4.2037687 -4.2127051 -4.1997576 -4.1675534][-4.2417789 -4.2494693 -4.2345695 -4.2064476 -4.1642866 -4.1015134 -4.0365067 -4.016974 -4.0563426 -4.11712 -4.1690178 -4.1998739 -4.2095985 -4.1992464 -4.170507][-4.2189274 -4.2267032 -4.21489 -4.1899571 -4.15499 -4.1008253 -4.0464978 -4.0294194 -4.0585508 -4.110363 -4.1587725 -4.1888437 -4.1992326 -4.1924267 -4.1707506][-4.2012024 -4.2035322 -4.1904631 -4.1669378 -4.14001 -4.0995317 -4.0581236 -4.0412602 -4.0610514 -4.1005292 -4.1445408 -4.1786213 -4.1940274 -4.1877127 -4.1670704][-4.2063065 -4.2013736 -4.1827049 -4.1585689 -4.1365347 -4.1115646 -4.0851331 -4.0680728 -4.078764 -4.1055913 -4.1402946 -4.1713967 -4.1843238 -4.1743097 -4.1528831][-4.2282424 -4.2219467 -4.2034888 -4.1817327 -4.161715 -4.1463232 -4.128839 -4.1112828 -4.1145115 -4.1302938 -4.150353 -4.1695127 -4.1735587 -4.1567149 -4.1306372][-4.2496014 -4.2461214 -4.2322965 -4.2170429 -4.2029843 -4.1950917 -4.1831126 -4.1651907 -4.1623139 -4.1700854 -4.1789432 -4.1855822 -4.1800685 -4.1564856 -4.1281037]]...]
INFO - root - 2017-12-07 12:19:56.742870: step 4610, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.592 sec/batch; 71h:28m:10s remains)
INFO - root - 2017-12-07 12:20:13.141071: step 4620, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.543 sec/batch; 69h:16m:17s remains)
INFO - root - 2017-12-07 12:20:29.468539: step 4630, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.693 sec/batch; 75h:59m:29s remains)
INFO - root - 2017-12-07 12:20:45.761870: step 4640, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.608 sec/batch; 72h:11m:15s remains)
INFO - root - 2017-12-07 12:21:02.028684: step 4650, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 71h:43m:58s remains)
INFO - root - 2017-12-07 12:21:18.139445: step 4660, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.600 sec/batch; 71h:49m:49s remains)
INFO - root - 2017-12-07 12:21:34.481502: step 4670, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.661 sec/batch; 74h:32m:54s remains)
INFO - root - 2017-12-07 12:21:50.615072: step 4680, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.587 sec/batch; 71h:14m:11s remains)
INFO - root - 2017-12-07 12:22:07.020154: step 4690, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.651 sec/batch; 74h:06m:11s remains)
INFO - root - 2017-12-07 12:22:23.170599: step 4700, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 1.618 sec/batch; 72h:35m:52s remains)
2017-12-07 12:22:24.714962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3321056 -4.3174605 -4.2854362 -4.2231145 -4.1608291 -4.1273737 -4.1291165 -4.1571064 -4.1992397 -4.2380095 -4.2567778 -4.2672243 -4.2761006 -4.2870216 -4.2983789][-4.3170214 -4.2915335 -4.247591 -4.1807618 -4.1278124 -4.1175647 -4.1400185 -4.1695876 -4.2001309 -4.2238197 -4.2332592 -4.2400351 -4.2497716 -4.2685013 -4.2878141][-4.3030062 -4.270278 -4.2210407 -4.154695 -4.1105728 -4.1174846 -4.1531239 -4.1842418 -4.2086759 -4.2260647 -4.2319756 -4.2404 -4.2554274 -4.2777648 -4.2971535][-4.2921953 -4.2519946 -4.1994076 -4.1391206 -4.1067066 -4.1225953 -4.157764 -4.1871343 -4.2158189 -4.2395563 -4.2523375 -4.2695541 -4.290668 -4.3095908 -4.3200197][-4.2823496 -4.2321777 -4.1750345 -4.1254864 -4.1067548 -4.126338 -4.1581845 -4.1838012 -4.2098718 -4.239717 -4.2637739 -4.2922621 -4.3213611 -4.3337407 -4.3343978][-4.2783613 -4.2195768 -4.1562738 -4.1077704 -4.0884738 -4.0970507 -4.123044 -4.147994 -4.174799 -4.2085457 -4.243495 -4.2859306 -4.3253636 -4.3381443 -4.3373294][-4.2747045 -4.2109184 -4.1414647 -4.0844398 -4.0496745 -4.0363073 -4.0537791 -4.0832129 -4.1181703 -4.1580868 -4.2058239 -4.2613277 -4.3076735 -4.3281884 -4.3333106][-4.2563524 -4.1920977 -4.121449 -4.0582685 -4.0079665 -3.9719572 -3.976881 -4.0087013 -4.0547271 -4.1038356 -4.1614094 -4.2268858 -4.2815018 -4.3139992 -4.3266864][-4.2304416 -4.1730723 -4.1059341 -4.0425596 -3.9886265 -3.9472563 -3.9447694 -3.9759312 -4.0294504 -4.0857983 -4.1465459 -4.2142243 -4.2731061 -4.3119922 -4.3293176][-4.2136168 -4.1660161 -4.1066351 -4.0531425 -4.0064278 -3.9755523 -3.9770443 -4.0102649 -4.065907 -4.12237 -4.1777596 -4.2383323 -4.2922912 -4.3281302 -4.3404403][-4.2124414 -4.1747942 -4.1347418 -4.0990658 -4.0671329 -4.0503993 -4.0598054 -4.0940075 -4.1417317 -4.188437 -4.2331424 -4.2806969 -4.3231564 -4.3495164 -4.353673][-4.2157979 -4.19336 -4.178299 -4.1660018 -4.1498089 -4.1435165 -4.1582789 -4.1886129 -4.2230988 -4.254807 -4.2863903 -4.3186941 -4.3453393 -4.3605075 -4.3575835][-4.2145 -4.2110758 -4.2191706 -4.2270536 -4.2239213 -4.2243447 -4.240037 -4.2643538 -4.2874336 -4.3065486 -4.3255253 -4.3437738 -4.3562469 -4.3612356 -4.3544378][-4.2244644 -4.2357907 -4.2555909 -4.2708545 -4.2743092 -4.2769575 -4.2911849 -4.3112135 -4.3286762 -4.340076 -4.3485765 -4.355237 -4.3575945 -4.356514 -4.3504877][-4.251792 -4.2692084 -4.2887897 -4.3019838 -4.3067379 -4.3085318 -4.3186016 -4.3315492 -4.341713 -4.3473139 -4.3496995 -4.3503737 -4.3505254 -4.3514557 -4.3491549]]...]
INFO - root - 2017-12-07 12:22:40.792938: step 4710, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.581 sec/batch; 70h:56m:06s remains)
INFO - root - 2017-12-07 12:22:57.039246: step 4720, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.633 sec/batch; 73h:16m:27s remains)
INFO - root - 2017-12-07 12:23:13.105365: step 4730, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.572 sec/batch; 70h:32m:52s remains)
INFO - root - 2017-12-07 12:23:29.485145: step 4740, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.568 sec/batch; 70h:21m:56s remains)
INFO - root - 2017-12-07 12:23:45.846840: step 4750, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.688 sec/batch; 75h:42m:37s remains)
INFO - root - 2017-12-07 12:24:02.023078: step 4760, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.597 sec/batch; 71h:39m:10s remains)
INFO - root - 2017-12-07 12:24:18.257138: step 4770, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.664 sec/batch; 74h:39m:31s remains)
INFO - root - 2017-12-07 12:24:34.396877: step 4780, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.563 sec/batch; 70h:06m:04s remains)
INFO - root - 2017-12-07 12:24:50.600852: step 4790, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.672 sec/batch; 74h:58m:18s remains)
INFO - root - 2017-12-07 12:25:06.742627: step 4800, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 71h:51m:37s remains)
2017-12-07 12:25:08.145298: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2444067 -4.2379737 -4.2357268 -4.2286382 -4.2140026 -4.20123 -4.1901369 -4.1799021 -4.1700377 -4.161561 -4.1606827 -4.171885 -4.1939907 -4.2152615 -4.2240596][-4.2224836 -4.2140217 -4.2130671 -4.205318 -4.1877494 -4.1685 -4.1509686 -4.138803 -4.1313434 -4.1252408 -4.12733 -4.1428461 -4.1700253 -4.1950088 -4.2050366][-4.2078137 -4.2011542 -4.2024441 -4.1963677 -4.1792684 -4.1572862 -4.1391654 -4.1309023 -4.1307149 -4.1302776 -4.135004 -4.1506853 -4.1753569 -4.1982532 -4.2088904][-4.2163205 -4.2153764 -4.2189579 -4.2149124 -4.1987443 -4.1760564 -4.1568108 -4.1498079 -4.1569419 -4.1663904 -4.1767626 -4.1900034 -4.2057638 -4.2219868 -4.2304225][-4.240499 -4.2446289 -4.2491155 -4.2444291 -4.2258272 -4.1999841 -4.1776452 -4.1676941 -4.1754336 -4.191164 -4.207942 -4.2211733 -4.2310042 -4.2420983 -4.2508688][-4.2404413 -4.2411127 -4.2411833 -4.2291279 -4.2062292 -4.1764679 -4.1485915 -4.1316118 -4.1366477 -4.1581697 -4.1867905 -4.2105346 -4.2244706 -4.2359977 -4.2467213][-4.2164598 -4.2053347 -4.196805 -4.1791272 -4.1544032 -4.1243205 -4.0951171 -4.0742378 -4.0796695 -4.1093516 -4.1487236 -4.1799822 -4.1964231 -4.2061715 -4.2152548][-4.1915655 -4.1758771 -4.1696606 -4.1588345 -4.1427722 -4.1228313 -4.1012235 -4.0831103 -4.086422 -4.1124377 -4.147954 -4.1752181 -4.1852612 -4.187037 -4.189425][-4.198194 -4.18778 -4.1899176 -4.1884813 -4.1805034 -4.1686873 -4.1557531 -4.1442261 -4.147119 -4.1635323 -4.1873794 -4.2043505 -4.2079787 -4.2023406 -4.1951923][-4.2084141 -4.20087 -4.2047677 -4.2039523 -4.1956062 -4.184442 -4.1742578 -4.1663184 -4.16824 -4.1782622 -4.1945791 -4.208118 -4.2139158 -4.2097139 -4.200469][-4.2087283 -4.2030253 -4.2055044 -4.1995163 -4.1872549 -4.1742973 -4.1623816 -4.15298 -4.1523676 -4.1582789 -4.16938 -4.1784682 -4.1866617 -4.1878791 -4.1812286][-4.2004037 -4.1965766 -4.2000031 -4.1945558 -4.1817679 -4.1672506 -4.1529765 -4.1403565 -4.1360617 -4.1394129 -4.1468315 -4.1529536 -4.1609821 -4.1681643 -4.1658325][-4.2107291 -4.2111979 -4.2193995 -4.2191176 -4.2097297 -4.1929035 -4.1738677 -4.1574316 -4.1511431 -4.1551981 -4.1633744 -4.1718669 -4.1819568 -4.192647 -4.1937571][-4.223208 -4.2275906 -4.2377849 -4.24109 -4.2376738 -4.2232866 -4.2040296 -4.1879439 -4.184442 -4.1918936 -4.2026863 -4.212666 -4.2227755 -4.2317414 -4.2316823][-4.2204261 -4.2268581 -4.2358871 -4.2367921 -4.2331223 -4.2195973 -4.202661 -4.19207 -4.1946888 -4.2046881 -4.2143965 -4.2228518 -4.2296677 -4.2339692 -4.2297711]]...]
INFO - root - 2017-12-07 12:25:23.917170: step 4810, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.577 sec/batch; 70h:42m:02s remains)
INFO - root - 2017-12-07 12:25:40.125157: step 4820, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.668 sec/batch; 74h:47m:42s remains)
INFO - root - 2017-12-07 12:25:56.165670: step 4830, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.593 sec/batch; 71h:24m:32s remains)
INFO - root - 2017-12-07 12:26:12.648344: step 4840, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.643 sec/batch; 73h:40m:52s remains)
INFO - root - 2017-12-07 12:26:28.893721: step 4850, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.640 sec/batch; 73h:30m:50s remains)
INFO - root - 2017-12-07 12:26:45.300950: step 4860, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.655 sec/batch; 74h:12m:42s remains)
INFO - root - 2017-12-07 12:27:01.500799: step 4870, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.726 sec/batch; 77h:23m:31s remains)
INFO - root - 2017-12-07 12:27:17.702699: step 4880, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.571 sec/batch; 70h:26m:06s remains)
INFO - root - 2017-12-07 12:27:33.999554: step 4890, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.724 sec/batch; 77h:15m:50s remains)
INFO - root - 2017-12-07 12:27:50.404886: step 4900, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.565 sec/batch; 70h:07m:24s remains)
2017-12-07 12:27:51.853752: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.34324 -4.3358116 -4.3278651 -4.3145366 -4.3000178 -4.2870264 -4.2758508 -4.2673178 -4.2653942 -4.271493 -4.2849131 -4.3017769 -4.3174639 -4.3281736 -4.332036][-4.3340755 -4.3307161 -4.3248715 -4.3112092 -4.29366 -4.2750807 -4.259809 -4.2513151 -4.2521734 -4.2582641 -4.2713556 -4.2922311 -4.3128271 -4.328568 -4.3362713][-4.3229871 -4.3261614 -4.3228145 -4.3077087 -4.2858744 -4.2600718 -4.2388515 -4.230957 -4.237803 -4.2480817 -4.2605467 -4.28286 -4.3063078 -4.3265238 -4.339129][-4.3202686 -4.32923 -4.3254895 -4.3046851 -4.2738204 -4.2381787 -4.2078934 -4.2024784 -4.2191257 -4.2368426 -4.2502456 -4.2720661 -4.2964597 -4.3197503 -4.3371539][-4.3271065 -4.3371377 -4.3295851 -4.3012433 -4.2608709 -4.2147212 -4.1773281 -4.1746688 -4.2022104 -4.2276831 -4.24194 -4.2619648 -4.2862048 -4.3104219 -4.330862][-4.3351755 -4.3438139 -4.3322091 -4.2969375 -4.2470827 -4.1922464 -4.1545644 -4.1562433 -4.1897035 -4.2195492 -4.2345471 -4.2525411 -4.276813 -4.3020825 -4.3238392][-4.3386989 -4.3447251 -4.328701 -4.2873044 -4.2279186 -4.1690454 -4.1396646 -4.1489549 -4.1809173 -4.2062883 -4.2195878 -4.2382946 -4.2655845 -4.2949672 -4.3176537][-4.3351731 -4.3387513 -4.32064 -4.2761559 -4.2116017 -4.1532297 -4.1313086 -4.1431561 -4.1651697 -4.17946 -4.1923079 -4.2186055 -4.2528334 -4.2874951 -4.3121638][-4.3283782 -4.3313594 -4.3112526 -4.2678714 -4.2061157 -4.1511612 -4.1320763 -4.1380625 -4.149838 -4.1544957 -4.1676283 -4.2020493 -4.2417784 -4.280561 -4.3064437][-4.3283091 -4.3278995 -4.3025036 -4.2595286 -4.2038116 -4.1527052 -4.1309023 -4.1298242 -4.1365333 -4.1394348 -4.1576519 -4.197793 -4.238883 -4.27739 -4.3023872][-4.332777 -4.3261523 -4.2953634 -4.2513013 -4.2004919 -4.1512146 -4.1231441 -4.1147885 -4.1208138 -4.127306 -4.1567039 -4.2025504 -4.2438359 -4.2779126 -4.300015][-4.3365459 -4.3247643 -4.2912655 -4.2477994 -4.1995907 -4.149744 -4.116282 -4.1012492 -4.1077962 -4.1230464 -4.1630845 -4.2100263 -4.2480483 -4.2766848 -4.2950482][-4.3314047 -4.3165154 -4.2839775 -4.2468076 -4.2052436 -4.1568093 -4.1198897 -4.1030073 -4.1147871 -4.1418939 -4.1873507 -4.2288246 -4.2595458 -4.2808003 -4.293704][-4.3206306 -4.3060465 -4.278131 -4.250977 -4.2200618 -4.17703 -4.1402521 -4.12559 -4.1437798 -4.1783504 -4.2221327 -4.2541604 -4.2760725 -4.2913694 -4.299448][-4.3124266 -4.3037677 -4.28558 -4.2663922 -4.2427077 -4.2059884 -4.1739345 -4.1630979 -4.1828079 -4.216289 -4.2535272 -4.2766876 -4.2924 -4.3027763 -4.3074021]]...]
INFO - root - 2017-12-07 12:28:08.196173: step 4910, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.595 sec/batch; 71h:29m:59s remains)
INFO - root - 2017-12-07 12:28:24.528327: step 4920, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.732 sec/batch; 77h:36m:22s remains)
INFO - root - 2017-12-07 12:28:40.844165: step 4930, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.636 sec/batch; 73h:17m:22s remains)
INFO - root - 2017-12-07 12:28:57.035805: step 4940, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 1.546 sec/batch; 69h:16m:32s remains)
INFO - root - 2017-12-07 12:29:13.313752: step 4950, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 71h:42m:15s remains)
INFO - root - 2017-12-07 12:29:29.680390: step 4960, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.599 sec/batch; 71h:38m:34s remains)
INFO - root - 2017-12-07 12:29:46.112687: step 4970, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.745 sec/batch; 78h:11m:30s remains)
INFO - root - 2017-12-07 12:30:02.224196: step 4980, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.536 sec/batch; 68h:47m:37s remains)
INFO - root - 2017-12-07 12:30:18.611228: step 4990, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 1.660 sec/batch; 74h:20m:20s remains)
INFO - root - 2017-12-07 12:30:34.698371: step 5000, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.562 sec/batch; 69h:58m:54s remains)
2017-12-07 12:30:36.121273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3163247 -4.3095622 -4.3108044 -4.309495 -4.3074179 -4.3029456 -4.2995372 -4.2973857 -4.2959414 -4.291369 -4.2802997 -4.2715111 -4.2705803 -4.2680812 -4.2694435][-4.3174357 -4.31098 -4.3114486 -4.3088961 -4.3043461 -4.295063 -4.2876573 -4.2851696 -4.2846575 -4.2812939 -4.2729988 -4.2710447 -4.2759933 -4.2779508 -4.2817969][-4.2972383 -4.2952633 -4.2988296 -4.2959576 -4.2902813 -4.2786121 -4.2699089 -4.2694759 -4.27092 -4.2708077 -4.2666712 -4.2678795 -4.2732472 -4.2770457 -4.2826557][-4.2579336 -4.262557 -4.2715087 -4.2715678 -4.2660389 -4.25386 -4.2484245 -4.2541609 -4.2609668 -4.2644954 -4.2625146 -4.2609849 -4.2626767 -4.264473 -4.2704272][-4.2152634 -4.2265992 -4.2397718 -4.2419672 -4.2368774 -4.226398 -4.2257676 -4.2394962 -4.254622 -4.2627487 -4.2636609 -4.260982 -4.2619634 -4.2649379 -4.2722397][-4.1772618 -4.1863084 -4.1917343 -4.182847 -4.1689453 -4.1562734 -4.161447 -4.1895037 -4.2226124 -4.2418923 -4.2518134 -4.2555795 -4.2613621 -4.2695751 -4.2805529][-4.153605 -4.1486459 -4.1288824 -4.088779 -4.0448523 -4.0144315 -4.0227079 -4.0765605 -4.1431627 -4.1875443 -4.21653 -4.2338066 -4.2489047 -4.2637725 -4.2803764][-4.1522145 -4.1256046 -4.0780139 -4.0039845 -3.9226279 -3.8656416 -3.8732748 -3.9548843 -4.0562057 -4.1279349 -4.174006 -4.2025352 -4.22588 -4.2474031 -4.2685809][-4.1768546 -4.1446505 -4.0922809 -4.0134053 -3.928359 -3.8706498 -3.8766003 -3.9528813 -4.0495286 -4.1206079 -4.1666245 -4.1967926 -4.2198439 -4.2401466 -4.2615471][-4.2182331 -4.2001352 -4.1684771 -4.1174955 -4.0642633 -4.0303917 -4.031713 -4.0761127 -4.1354089 -4.1799288 -4.2062678 -4.2247267 -4.2392845 -4.2526522 -4.2689862][-4.2300067 -4.22795 -4.2198038 -4.1993794 -4.1749983 -4.1581678 -4.1575909 -4.1797166 -4.2116184 -4.2367845 -4.2477379 -4.252316 -4.2538261 -4.2585349 -4.268899][-4.2017779 -4.2142515 -4.223557 -4.2215571 -4.2128849 -4.2073226 -4.2075553 -4.218883 -4.2372203 -4.2522535 -4.2554841 -4.2501273 -4.2432218 -4.2429991 -4.2529507][-4.1617184 -4.1859341 -4.2060986 -4.2145 -4.2139778 -4.2127419 -4.2139564 -4.2213392 -4.2330747 -4.2413774 -4.2396736 -4.2282758 -4.2182016 -4.2177768 -4.2299128][-4.1524391 -4.1772919 -4.1989493 -4.2104497 -4.2133212 -4.2139921 -4.215981 -4.2212434 -4.2285061 -4.2317066 -4.2273474 -4.2163382 -4.2089348 -4.2124691 -4.2273612][-4.1850858 -4.2018962 -4.2175512 -4.227283 -4.23132 -4.2343392 -4.2378855 -4.242837 -4.24773 -4.2487717 -4.2445269 -4.2374725 -4.2356071 -4.2413511 -4.2549949]]...]
INFO - root - 2017-12-07 12:30:52.349834: step 5010, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.609 sec/batch; 72h:05m:12s remains)
INFO - root - 2017-12-07 12:31:08.625609: step 5020, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.682 sec/batch; 75h:20m:33s remains)
INFO - root - 2017-12-07 12:31:24.787184: step 5030, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.615 sec/batch; 72h:20m:13s remains)
INFO - root - 2017-12-07 12:31:41.018441: step 5040, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.680 sec/batch; 75h:14m:14s remains)
INFO - root - 2017-12-07 12:31:57.200210: step 5050, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.624 sec/batch; 72h:42m:05s remains)
INFO - root - 2017-12-07 12:32:13.570448: step 5060, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.638 sec/batch; 73h:21m:24s remains)
INFO - root - 2017-12-07 12:32:29.660349: step 5070, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.645 sec/batch; 73h:37m:51s remains)
INFO - root - 2017-12-07 12:32:46.089999: step 5080, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.587 sec/batch; 71h:01m:50s remains)
INFO - root - 2017-12-07 12:33:01.979142: step 5090, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.592 sec/batch; 71h:17m:18s remains)
INFO - root - 2017-12-07 12:33:18.378211: step 5100, loss = 2.10, batch loss = 2.04 (9.8 examples/sec; 1.626 sec/batch; 72h:46m:16s remains)
2017-12-07 12:33:19.769151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3662276 -4.3712797 -4.3735809 -4.3726935 -4.3690362 -4.3600955 -4.3433976 -4.3233395 -4.3044848 -4.3013549 -4.3165722 -4.3407464 -4.3669262 -4.3868904 -4.3981123][-4.372818 -4.3777881 -4.3789477 -4.3751779 -4.3658152 -4.3487954 -4.3248453 -4.2997046 -4.28036 -4.2811275 -4.3036938 -4.3339715 -4.365582 -4.3861694 -4.3944407][-4.3773355 -4.3842611 -4.3848391 -4.3739958 -4.3516846 -4.3198266 -4.2824345 -4.2528148 -4.2391043 -4.2460451 -4.2728362 -4.3077307 -4.3442726 -4.3694229 -4.3812184][-4.3773508 -4.3841915 -4.38114 -4.3599582 -4.3219395 -4.2710404 -4.2168446 -4.1836314 -4.1812954 -4.2005048 -4.233675 -4.2747774 -4.3174644 -4.3476205 -4.3650346][-4.3721237 -4.3765311 -4.3674498 -4.33371 -4.2773342 -4.204318 -4.1316066 -4.0937357 -4.1033869 -4.13862 -4.1843328 -4.2335529 -4.2841678 -4.3229351 -4.3482409][-4.3647881 -4.3650918 -4.3465905 -4.2964578 -4.2170968 -4.1181831 -4.021286 -3.9697146 -3.9883645 -4.0570068 -4.13261 -4.1956587 -4.256731 -4.3050609 -4.337481][-4.360445 -4.354455 -4.3218117 -4.2506423 -4.143651 -4.0138979 -3.8801637 -3.7974613 -3.8226173 -3.9433379 -4.0696077 -4.1582956 -4.2333875 -4.2913733 -4.3308554][-4.3568745 -4.3462281 -4.3029175 -4.2174783 -4.0924678 -3.9378958 -3.7679279 -3.6580944 -3.6950305 -3.8624678 -4.0320907 -4.1477957 -4.2326746 -4.29445 -4.3343439][-4.3524747 -4.3429418 -4.3023376 -4.2228394 -4.1084566 -3.9661789 -3.8086019 -3.7086942 -3.7452142 -3.9034042 -4.0668807 -4.1831408 -4.2618809 -4.3165169 -4.3491216][-4.3505278 -4.34468 -4.3163033 -4.2593231 -4.178915 -4.0833368 -3.9762561 -3.9059989 -3.9227576 -4.0274172 -4.1489644 -4.2428489 -4.3054323 -4.3451009 -4.3649492][-4.3510089 -4.3454671 -4.327035 -4.2908592 -4.2459278 -4.1958628 -4.1328115 -4.08762 -4.0921359 -4.1552048 -4.2360587 -4.3019252 -4.3443127 -4.3682575 -4.3755913][-4.3520541 -4.3464975 -4.3345113 -4.3151217 -4.293932 -4.2668481 -4.2277236 -4.2012305 -4.207706 -4.2479329 -4.2986951 -4.341476 -4.3676267 -4.3809996 -4.3793125][-4.35009 -4.3431492 -4.3329873 -4.3231411 -4.3124084 -4.2930951 -4.2661304 -4.2542567 -4.2674446 -4.2992659 -4.3346124 -4.3638616 -4.3790941 -4.3840609 -4.3769345][-4.354013 -4.345459 -4.3328347 -4.3196945 -4.3027763 -4.278583 -4.2548065 -4.2501321 -4.2695889 -4.3018174 -4.3364992 -4.3668704 -4.3794956 -4.378952 -4.3713417][-4.3575363 -4.3442245 -4.3249927 -4.3040538 -4.2756171 -4.2449651 -4.2261868 -4.2300811 -4.2547278 -4.2890291 -4.3253489 -4.3576703 -4.3713632 -4.3710709 -4.365449]]...]
INFO - root - 2017-12-07 12:33:36.063920: step 5110, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.583 sec/batch; 70h:52m:30s remains)
INFO - root - 2017-12-07 12:33:52.438544: step 5120, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.668 sec/batch; 74h:38m:49s remains)
INFO - root - 2017-12-07 12:34:08.786931: step 5130, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.685 sec/batch; 75h:25m:49s remains)
INFO - root - 2017-12-07 12:34:25.059673: step 5140, loss = 2.11, batch loss = 2.05 (10.4 examples/sec; 1.532 sec/batch; 68h:33m:32s remains)
INFO - root - 2017-12-07 12:34:41.393051: step 5150, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.669 sec/batch; 74h:40m:04s remains)
INFO - root - 2017-12-07 12:34:57.635025: step 5160, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.587 sec/batch; 71h:00m:24s remains)
INFO - root - 2017-12-07 12:35:13.789171: step 5170, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.705 sec/batch; 76h:16m:59s remains)
INFO - root - 2017-12-07 12:35:29.856276: step 5180, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.570 sec/batch; 70h:13m:38s remains)
INFO - root - 2017-12-07 12:35:46.062624: step 5190, loss = 2.11, batch loss = 2.05 (9.5 examples/sec; 1.688 sec/batch; 75h:30m:12s remains)
INFO - root - 2017-12-07 12:36:02.256586: step 5200, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.549 sec/batch; 69h:17m:53s remains)
2017-12-07 12:36:03.677465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2451468 -4.2303367 -4.2173858 -4.1994023 -4.1796455 -4.1728258 -4.1909437 -4.2181973 -4.2320213 -4.2315512 -4.2256804 -4.2194753 -4.21271 -4.203516 -4.1978807][-4.2378697 -4.2206945 -4.2061672 -4.18811 -4.1696978 -4.1618733 -4.1783762 -4.2043467 -4.2178082 -4.2171278 -4.2094526 -4.1992373 -4.1916771 -4.1855717 -4.1818895][-4.2237563 -4.2049942 -4.1895785 -4.177598 -4.165 -4.1581607 -4.1726317 -4.1949258 -4.2069936 -4.2053995 -4.19436 -4.1791935 -4.1707997 -4.1682644 -4.1690364][-4.2051458 -4.1853223 -4.1669774 -4.1568041 -4.1471415 -4.1421671 -4.1569443 -4.1793685 -4.1950278 -4.1953826 -4.1823096 -4.1627655 -4.1504951 -4.1481423 -4.1537447][-4.1928697 -4.1750579 -4.1544237 -4.1367645 -4.1165929 -4.1019535 -4.1124616 -4.1404724 -4.1690607 -4.1792817 -4.171793 -4.1548781 -4.1395168 -4.1316595 -4.1356053][-4.1843553 -4.1711822 -4.1528029 -4.1270561 -4.087862 -4.0499277 -4.0450473 -4.0818 -4.1312375 -4.1593289 -4.1636691 -4.1546988 -4.1389236 -4.12232 -4.1171651][-4.1768394 -4.1670227 -4.1544194 -4.1295195 -4.0796728 -4.015902 -3.9823565 -4.0187411 -4.0878925 -4.1353736 -4.1557426 -4.1580629 -4.1459541 -4.1234584 -4.1085358][-4.1717153 -4.1659522 -4.1616368 -4.1462784 -4.1026874 -4.03302 -3.9797361 -4.0006971 -4.0682969 -4.1232691 -4.1538863 -4.1653271 -4.1574554 -4.1337266 -4.1140809][-4.176796 -4.1724925 -4.1743565 -4.169683 -4.1423931 -4.09423 -4.0536923 -4.0574212 -4.0942168 -4.132894 -4.15872 -4.170712 -4.1650996 -4.1434069 -4.1240091][-4.1880031 -4.1815281 -4.1868911 -4.1900821 -4.17969 -4.1599946 -4.1420074 -4.1379704 -4.1462922 -4.1625357 -4.1753216 -4.1815786 -4.1772556 -4.1607542 -4.1455483][-4.2026591 -4.1924706 -4.1965933 -4.201292 -4.2004132 -4.1995115 -4.1984835 -4.1961093 -4.1939325 -4.1971388 -4.1997786 -4.1997352 -4.1958427 -4.1857734 -4.1765146][-4.2206297 -4.20809 -4.2094564 -4.21265 -4.2143579 -4.2209396 -4.2281909 -4.2308593 -4.229043 -4.2275467 -4.2243543 -4.2209659 -4.2170987 -4.2113514 -4.2074752][-4.24044 -4.2297072 -4.230248 -4.2316895 -4.2314887 -4.237433 -4.247107 -4.2540884 -4.2561345 -4.2548947 -4.2511706 -4.2472258 -4.2428045 -4.2386637 -4.2391663][-4.2628517 -4.2556472 -4.2552028 -4.2550559 -4.2532992 -4.2559686 -4.2639437 -4.2717867 -4.2758136 -4.2768297 -4.275558 -4.2730708 -4.2696266 -4.2678933 -4.2703829][-4.2865562 -4.2816586 -4.2801447 -4.2786241 -4.275775 -4.2749095 -4.2781134 -4.2829909 -4.28617 -4.2879739 -4.2884016 -4.28755 -4.2861876 -4.2867312 -4.2894382]]...]
INFO - root - 2017-12-07 12:36:19.899357: step 5210, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.626 sec/batch; 72h:44m:06s remains)
INFO - root - 2017-12-07 12:36:36.308816: step 5220, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.637 sec/batch; 73h:14m:21s remains)
INFO - root - 2017-12-07 12:36:52.397280: step 5230, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 1.420 sec/batch; 63h:30m:34s remains)
INFO - root - 2017-12-07 12:37:08.793919: step 5240, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.581 sec/batch; 70h:43m:14s remains)
INFO - root - 2017-12-07 12:37:24.921751: step 5250, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 1.672 sec/batch; 74h:45m:54s remains)
INFO - root - 2017-12-07 12:37:41.188548: step 5260, loss = 2.05, batch loss = 2.00 (10.3 examples/sec; 1.558 sec/batch; 69h:40m:26s remains)
INFO - root - 2017-12-07 12:37:57.622841: step 5270, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.673 sec/batch; 74h:48m:36s remains)
INFO - root - 2017-12-07 12:38:13.766046: step 5280, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.585 sec/batch; 70h:52m:50s remains)
INFO - root - 2017-12-07 12:38:29.895566: step 5290, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.641 sec/batch; 73h:22m:34s remains)
INFO - root - 2017-12-07 12:38:46.288956: step 5300, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.657 sec/batch; 74h:05m:43s remains)
2017-12-07 12:38:47.616989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1999459 -4.1848989 -4.1854444 -4.1976538 -4.2266154 -4.2631741 -4.2933073 -4.311842 -4.322938 -4.3311319 -4.3325725 -4.3320837 -4.3331804 -4.3330269 -4.3361793][-4.2010098 -4.1914887 -4.1950312 -4.2097993 -4.2376595 -4.2749929 -4.3065343 -4.3253665 -4.3333549 -4.3379617 -4.3380766 -4.3356118 -4.3334336 -4.3280807 -4.3242345][-4.2335672 -4.227067 -4.2290211 -4.2413311 -4.2624254 -4.2883329 -4.3083181 -4.3174887 -4.3187 -4.3211861 -4.3212028 -4.3171458 -4.3114996 -4.3029466 -4.2945633][-4.272922 -4.2685084 -4.26677 -4.2724905 -4.2814832 -4.2826028 -4.2762032 -4.2634225 -4.2584009 -4.267036 -4.2769504 -4.2794523 -4.2742672 -4.2662439 -4.2578549][-4.304522 -4.3005695 -4.29355 -4.2895436 -4.2781682 -4.2457418 -4.201755 -4.156673 -4.1444068 -4.1721659 -4.2070093 -4.2303858 -4.2372737 -4.2366157 -4.2329][-4.32024 -4.3197079 -4.3073354 -4.2881317 -4.2537746 -4.18737 -4.0963612 -4.0045133 -3.9771876 -4.0371089 -4.119081 -4.1808667 -4.2133694 -4.2278447 -4.2329187][-4.3221593 -4.3217373 -4.3031054 -4.2721033 -4.2219939 -4.133594 -4.0094032 -3.8746085 -3.8203297 -3.9104929 -4.043828 -4.1478481 -4.2091794 -4.2406383 -4.2567973][-4.3073554 -4.3086629 -4.2923059 -4.2661152 -4.219933 -4.1385932 -4.0281162 -3.9057169 -3.8416185 -3.9080892 -4.0365086 -4.1458173 -4.2171073 -4.2610168 -4.2881846][-4.2709236 -4.2761993 -4.2704873 -4.259161 -4.2299132 -4.17819 -4.1114569 -4.038362 -3.9992769 -4.0342684 -4.11179 -4.185802 -4.2404561 -4.2799468 -4.3071032][-4.2350993 -4.2448349 -4.2511067 -4.2514277 -4.2374091 -4.2098088 -4.1782126 -4.1452422 -4.1324296 -4.1539116 -4.1947384 -4.235786 -4.2694864 -4.2941155 -4.3119388][-4.231688 -4.2415142 -4.2571 -4.2665658 -4.2628808 -4.2467847 -4.2318282 -4.2193995 -4.2202139 -4.2377057 -4.2612772 -4.2807961 -4.2952547 -4.3056803 -4.3141952][-4.25316 -4.2546587 -4.2642636 -4.2780628 -4.2838283 -4.2778111 -4.2688346 -4.2633791 -4.2670736 -4.2824078 -4.2994146 -4.3092918 -4.3121314 -4.3125486 -4.3136907][-4.2873073 -4.2748661 -4.2711592 -4.2826262 -4.29762 -4.3075423 -4.3061509 -4.3022194 -4.3021946 -4.3100133 -4.3194675 -4.3223987 -4.3179889 -4.3123984 -4.3095832][-4.3094554 -4.284451 -4.268096 -4.2711544 -4.2876554 -4.3103232 -4.3226476 -4.3262267 -4.328393 -4.3333035 -4.3381557 -4.3365927 -4.3291678 -4.3197637 -4.3120785][-4.295115 -4.2542577 -4.2309103 -4.2318306 -4.2541637 -4.2888594 -4.3159781 -4.332231 -4.3409405 -4.3456068 -4.3475976 -4.3449178 -4.3383265 -4.3283238 -4.318264]]...]
INFO - root - 2017-12-07 12:39:03.527280: step 5310, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.556 sec/batch; 69h:34m:50s remains)
INFO - root - 2017-12-07 12:39:19.818122: step 5320, loss = 2.10, batch loss = 2.05 (9.5 examples/sec; 1.676 sec/batch; 74h:54m:31s remains)
INFO - root - 2017-12-07 12:39:36.072475: step 5330, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.581 sec/batch; 70h:41m:19s remains)
INFO - root - 2017-12-07 12:39:52.506572: step 5340, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.737 sec/batch; 77h:38m:40s remains)
INFO - root - 2017-12-07 12:40:08.616795: step 5350, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.603 sec/batch; 71h:38m:33s remains)
INFO - root - 2017-12-07 12:40:25.015206: step 5360, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.724 sec/batch; 77h:03m:21s remains)
INFO - root - 2017-12-07 12:40:40.947005: step 5370, loss = 2.08, batch loss = 2.03 (10.4 examples/sec; 1.544 sec/batch; 69h:00m:10s remains)
INFO - root - 2017-12-07 12:40:57.299476: step 5380, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.612 sec/batch; 72h:00m:49s remains)
INFO - root - 2017-12-07 12:41:13.665133: step 5390, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.607 sec/batch; 71h:48m:45s remains)
INFO - root - 2017-12-07 12:41:30.056193: step 5400, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.633 sec/batch; 72h:56m:33s remains)
2017-12-07 12:41:31.508595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3477721 -4.3384981 -4.3366823 -4.3339334 -4.3299751 -4.3270607 -4.3212938 -4.3078761 -4.2997117 -4.2944026 -4.2858429 -4.2780976 -4.2778692 -4.2855511 -4.2971182][-4.3472977 -4.3328118 -4.3270726 -4.3224893 -4.3167882 -4.3141832 -4.3124743 -4.3009272 -4.2906342 -4.2820196 -4.269773 -4.256659 -4.2513304 -4.2567673 -4.2727208][-4.3341489 -4.314703 -4.3017578 -4.289607 -4.2789369 -4.2721176 -4.2708526 -4.2660561 -4.2554178 -4.2476215 -4.2422881 -4.2314329 -4.2267447 -4.2323852 -4.2518177][-4.322206 -4.3015742 -4.2830491 -4.2584739 -4.2299395 -4.2066793 -4.2009716 -4.2026167 -4.2011876 -4.2028737 -4.212204 -4.2120824 -4.2106657 -4.2175431 -4.2395988][-4.3093605 -4.29177 -4.2655191 -4.2244616 -4.173418 -4.1270542 -4.1019011 -4.0988398 -4.1110497 -4.1357975 -4.1671171 -4.1835856 -4.1895342 -4.2039585 -4.233747][-4.294836 -4.2768993 -4.2401214 -4.1816063 -4.1051636 -4.028348 -3.9607556 -3.9327781 -3.9694712 -4.0415144 -4.1095028 -4.14931 -4.170527 -4.1978655 -4.2363272][-4.2663765 -4.2479143 -4.2049732 -4.1377773 -4.0465937 -3.9393253 -3.8127277 -3.7230701 -3.7777672 -3.9121022 -4.0293937 -4.10135 -4.1452618 -4.1900029 -4.238204][-4.224906 -4.1994591 -4.1578426 -4.0987806 -4.0204663 -3.9199963 -3.7844694 -3.6518176 -3.6815009 -3.830651 -3.9717023 -4.0669789 -4.1276078 -4.1821432 -4.2340302][-4.2206059 -4.1931825 -4.16655 -4.1313877 -4.0908527 -4.034276 -3.944628 -3.8352222 -3.8273826 -3.919251 -4.0171008 -4.0910153 -4.1429377 -4.18929 -4.2328744][-4.2382779 -4.2212253 -4.2099271 -4.1956983 -4.18445 -4.1632819 -4.1141405 -4.0425653 -4.0188718 -4.0586834 -4.1054668 -4.1445012 -4.1754532 -4.2067838 -4.2380304][-4.2580771 -4.2479744 -4.2494926 -4.2521453 -4.2552304 -4.24986 -4.2282481 -4.1842337 -4.152751 -4.1637216 -4.1831 -4.2012954 -4.2165284 -4.2340059 -4.2495379][-4.2763429 -4.2677865 -4.2707324 -4.2770739 -4.283432 -4.2868872 -4.2807217 -4.2561221 -4.2277226 -4.2241392 -4.2266045 -4.2291813 -4.2331791 -4.2464676 -4.2566638][-4.2843356 -4.2772336 -4.2774835 -4.2833114 -4.2891192 -4.2947941 -4.2965941 -4.2817883 -4.2584977 -4.2444143 -4.2345276 -4.2287011 -4.2317982 -4.2507677 -4.2671266][-4.2843571 -4.2828894 -4.2860179 -4.2903819 -4.2921782 -4.2927961 -4.2952137 -4.2864017 -4.2705483 -4.2557569 -4.2442632 -4.2402368 -4.246902 -4.2699695 -4.2892852][-4.2830672 -4.2869163 -4.2931757 -4.2974224 -4.2966814 -4.2945004 -4.2946663 -4.2870812 -4.2761936 -4.2680016 -4.2630134 -4.2661786 -4.2788043 -4.3004928 -4.3165727]]...]
INFO - root - 2017-12-07 12:41:47.644458: step 5410, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.596 sec/batch; 71h:19m:33s remains)
INFO - root - 2017-12-07 12:42:03.966029: step 5420, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 1.452 sec/batch; 64h:51m:24s remains)
INFO - root - 2017-12-07 12:42:20.177906: step 5430, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.650 sec/batch; 73h:43m:12s remains)
INFO - root - 2017-12-07 12:42:36.516566: step 5440, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.540 sec/batch; 68h:47m:40s remains)
INFO - root - 2017-12-07 12:42:52.972570: step 5450, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.707 sec/batch; 76h:14m:52s remains)
INFO - root - 2017-12-07 12:43:09.293172: step 5460, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.625 sec/batch; 72h:34m:57s remains)
INFO - root - 2017-12-07 12:43:25.634343: step 5470, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.738 sec/batch; 77h:37m:57s remains)
INFO - root - 2017-12-07 12:43:41.747424: step 5480, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.519 sec/batch; 67h:51m:24s remains)
INFO - root - 2017-12-07 12:43:58.235827: step 5490, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.640 sec/batch; 73h:14m:18s remains)
INFO - root - 2017-12-07 12:44:14.300165: step 5500, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.537 sec/batch; 68h:38m:53s remains)
2017-12-07 12:44:15.760865: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2221661 -4.2079806 -4.2179079 -4.2289538 -4.2271762 -4.2261906 -4.2250776 -4.2325478 -4.2426162 -4.2399035 -4.2354755 -4.2461376 -4.2599435 -4.2753153 -4.2859173][-4.2017059 -4.1798429 -4.1891742 -4.2036991 -4.2076116 -4.2117753 -4.2082257 -4.2090931 -4.2170472 -4.2179251 -4.22078 -4.2350903 -4.2464528 -4.256022 -4.2678428][-4.1788182 -4.1471338 -4.1523218 -4.1692915 -4.1766624 -4.1841598 -4.1807466 -4.175601 -4.1756544 -4.1746464 -4.1836658 -4.1995764 -4.205195 -4.2107444 -4.2294703][-4.1661825 -4.1289964 -4.1311693 -4.1518 -4.1644711 -4.170495 -4.1615005 -4.1447616 -4.1291585 -4.1213374 -4.12973 -4.1437593 -4.1504903 -4.1643152 -4.1940894][-4.1778727 -4.1429925 -4.1448359 -4.172895 -4.1900954 -4.1898074 -4.1697235 -4.1361003 -4.1035919 -4.0881481 -4.0930061 -4.1085324 -4.1269355 -4.1500378 -4.1790786][-4.2016273 -4.1738124 -4.1779127 -4.2086163 -4.2288895 -4.2246008 -4.1969972 -4.1524153 -4.1119795 -4.0972147 -4.1034822 -4.11997 -4.1461735 -4.1696715 -4.1873388][-4.2081151 -4.1888213 -4.1986003 -4.2313986 -4.2532382 -4.2478218 -4.2200427 -4.1751494 -4.1391892 -4.1305723 -4.1378908 -4.152112 -4.1776161 -4.2015829 -4.2144423][-4.1976228 -4.1892824 -4.2091136 -4.2445807 -4.2656488 -4.2598267 -4.2329655 -4.1975594 -4.1765084 -4.1754928 -4.1808152 -4.1888781 -4.2075291 -4.2267284 -4.2407732][-4.1826363 -4.1850338 -4.21202 -4.2467866 -4.2625461 -4.2569857 -4.23687 -4.2161713 -4.2080936 -4.21271 -4.2169523 -4.2222013 -4.2354312 -4.2474585 -4.2585697][-4.1721745 -4.1795273 -4.207149 -4.2353954 -4.2456045 -4.2422075 -4.2310028 -4.2212639 -4.2199078 -4.2279477 -4.2334394 -4.2406087 -4.2515512 -4.2589684 -4.265625][-4.1743207 -4.1741247 -4.1934733 -4.2151527 -4.2239385 -4.2221832 -4.2151065 -4.2104168 -4.2101579 -4.2188196 -4.2282662 -4.2360282 -4.2454228 -4.2523179 -4.2591662][-4.1809692 -4.1634722 -4.1714783 -4.1869 -4.1981711 -4.1999688 -4.1947594 -4.1912451 -4.1926556 -4.2011676 -4.2086883 -4.2122593 -4.2162495 -4.2233071 -4.2352791][-4.1812148 -4.1488171 -4.1493325 -4.1660156 -4.1845012 -4.1921458 -4.1875243 -4.1830788 -4.1866727 -4.1940246 -4.1928763 -4.1818538 -4.1761184 -4.1835084 -4.2053967][-4.1888604 -4.1512 -4.1509643 -4.1693454 -4.1896019 -4.1992173 -4.1952763 -4.1898117 -4.1932111 -4.1969843 -4.1835351 -4.15486 -4.1379352 -4.1460419 -4.1814914][-4.20677 -4.1748233 -4.17732 -4.1927257 -4.2046719 -4.2096024 -4.2064333 -4.2042737 -4.2078133 -4.2069554 -4.1849804 -4.1451454 -4.1182852 -4.1262937 -4.1720748]]...]
INFO - root - 2017-12-07 12:44:32.269262: step 5510, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.647 sec/batch; 73h:31m:27s remains)
INFO - root - 2017-12-07 12:44:48.458910: step 5520, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.663 sec/batch; 74h:16m:08s remains)
INFO - root - 2017-12-07 12:45:04.796689: step 5530, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 70h:59m:13s remains)
INFO - root - 2017-12-07 12:45:21.187825: step 5540, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.581 sec/batch; 70h:35m:37s remains)
INFO - root - 2017-12-07 12:45:37.388825: step 5550, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.694 sec/batch; 75h:37m:45s remains)
INFO - root - 2017-12-07 12:45:53.767776: step 5560, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.599 sec/batch; 71h:22m:28s remains)
INFO - root - 2017-12-07 12:46:10.292574: step 5570, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.677 sec/batch; 74h:50m:29s remains)
INFO - root - 2017-12-07 12:46:26.608710: step 5580, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.584 sec/batch; 70h:42m:45s remains)
INFO - root - 2017-12-07 12:46:42.785631: step 5590, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.681 sec/batch; 75h:00m:50s remains)
INFO - root - 2017-12-07 12:46:59.143940: step 5600, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.642 sec/batch; 73h:16m:04s remains)
2017-12-07 12:47:00.568894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3349204 -4.3389511 -4.3399925 -4.3398895 -4.3381062 -4.3356586 -4.3342123 -4.3334694 -4.3324761 -4.3316183 -4.3304839 -4.3307314 -4.3332734 -4.3363948 -4.3389449][-4.3417907 -4.3490815 -4.3521609 -4.3506246 -4.3426776 -4.33214 -4.3242526 -4.3196092 -4.31771 -4.31549 -4.3135896 -4.3153238 -4.3234549 -4.3347797 -4.3435421][-4.3479614 -4.35792 -4.3588138 -4.3486328 -4.3259358 -4.2991571 -4.2791057 -4.2680516 -4.2666097 -4.2673392 -4.2708087 -4.2798548 -4.2988648 -4.3217044 -4.3376751][-4.3556194 -4.3662438 -4.3585496 -4.3296442 -4.282032 -4.2292132 -4.187223 -4.164155 -4.1677356 -4.1829472 -4.2009096 -4.2268791 -4.2628074 -4.300365 -4.3231063][-4.3599625 -4.3667269 -4.3442049 -4.2883444 -4.205689 -4.1152229 -4.0384841 -3.9963508 -4.0142517 -4.0596743 -4.1034546 -4.1529231 -4.2101641 -4.2642055 -4.29241][-4.3572941 -4.3553743 -4.3154693 -4.2327642 -4.111331 -3.9716995 -3.8471694 -3.7828975 -3.8343778 -3.9295371 -4.0118756 -4.0910592 -4.1698861 -4.2361627 -4.2636538][-4.3472948 -4.3330812 -4.2774162 -4.1749678 -4.0233054 -3.8404872 -3.6716738 -3.5955133 -3.6953654 -3.8525655 -3.9785738 -4.0837226 -4.1741977 -4.2384562 -4.2556729][-4.3360786 -4.3128281 -4.250824 -4.1438856 -3.9932117 -3.8160069 -3.6648021 -3.6189389 -3.7390981 -3.9074101 -4.0368676 -4.1383739 -4.2178121 -4.2660737 -4.2722373][-4.326272 -4.3003173 -4.2449379 -4.1547103 -4.0406017 -3.9182582 -3.8350897 -3.8353097 -3.9303973 -4.0487537 -4.13994 -4.2125883 -4.26916 -4.2993608 -4.2987156][-4.3189731 -4.2982149 -4.26124 -4.2011118 -4.131465 -4.0650377 -4.03452 -4.0534906 -4.1114087 -4.1768274 -4.2284756 -4.2725577 -4.3074541 -4.3246822 -4.3237662][-4.3158875 -4.3045936 -4.28644 -4.2536507 -4.2176366 -4.1897278 -4.1844525 -4.2022815 -4.2314587 -4.2598691 -4.2832603 -4.3061557 -4.3258529 -4.3363261 -4.3370748][-4.3164563 -4.3112674 -4.3028831 -4.2859406 -4.2681623 -4.2599659 -4.2656965 -4.28128 -4.2959261 -4.3053665 -4.3134308 -4.3226471 -4.3306026 -4.3343124 -4.3354487][-4.3194122 -4.3171191 -4.31198 -4.3030033 -4.2945123 -4.2943926 -4.3039145 -4.3168607 -4.3241253 -4.3258471 -4.3263478 -4.3263693 -4.3249526 -4.3233094 -4.3242297][-4.32261 -4.3218622 -4.3182917 -4.31321 -4.3092427 -4.31203 -4.3203454 -4.3288846 -4.3319144 -4.3309388 -4.3282337 -4.322618 -4.3156266 -4.3108249 -4.3123207][-4.3245754 -4.3242831 -4.320528 -4.3166466 -4.314467 -4.3177519 -4.3244781 -4.3307438 -4.3328571 -4.3313794 -4.32738 -4.3204679 -4.3125215 -4.3076448 -4.3093457]]...]
INFO - root - 2017-12-07 12:47:16.938625: step 5610, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.581 sec/batch; 70h:34m:11s remains)
INFO - root - 2017-12-07 12:47:33.548749: step 5620, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 1.701 sec/batch; 75h:53m:24s remains)
INFO - root - 2017-12-07 12:47:49.828078: step 5630, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.541 sec/batch; 68h:46m:09s remains)
INFO - root - 2017-12-07 12:48:06.048941: step 5640, loss = 2.09, batch loss = 2.04 (10.0 examples/sec; 1.601 sec/batch; 71h:25m:42s remains)
INFO - root - 2017-12-07 12:48:22.120611: step 5650, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.605 sec/batch; 71h:35m:08s remains)
INFO - root - 2017-12-07 12:48:38.347091: step 5660, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.618 sec/batch; 72h:09m:44s remains)
INFO - root - 2017-12-07 12:48:54.749403: step 5670, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.583 sec/batch; 70h:37m:04s remains)
INFO - root - 2017-12-07 12:49:11.163875: step 5680, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.680 sec/batch; 74h:57m:03s remains)
INFO - root - 2017-12-07 12:49:27.505732: step 5690, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.585 sec/batch; 70h:42m:15s remains)
INFO - root - 2017-12-07 12:49:43.653999: step 5700, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.708 sec/batch; 76h:11m:14s remains)
2017-12-07 12:49:44.999039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3442726 -4.3372765 -4.3219342 -4.3042884 -4.2934861 -4.2893038 -4.2920742 -4.299603 -4.3068795 -4.3094878 -4.3066883 -4.3011894 -4.2945533 -4.2917404 -4.2922282][-4.3505187 -4.3433971 -4.3226442 -4.2973371 -4.28267 -4.2759376 -4.2799878 -4.295763 -4.3124723 -4.3218641 -4.3189659 -4.3074608 -4.292738 -4.2822695 -4.2766938][-4.3503532 -4.338841 -4.3109379 -4.2761416 -4.2518663 -4.2404947 -4.2478843 -4.2773719 -4.3103232 -4.3303714 -4.3307757 -4.3115234 -4.2842932 -4.2608247 -4.2443547][-4.3495822 -4.332655 -4.2951608 -4.2456565 -4.2046218 -4.184267 -4.1964612 -4.2439485 -4.2993617 -4.3343048 -4.3401155 -4.3128672 -4.2672186 -4.2229648 -4.1940002][-4.3505068 -4.3289943 -4.281878 -4.2145476 -4.1468868 -4.1100197 -4.1234903 -4.1867146 -4.2675638 -4.3214626 -4.3371272 -4.308125 -4.246099 -4.1799006 -4.1424932][-4.3567085 -4.3365 -4.2836785 -4.19732 -4.100625 -4.0375338 -4.0398808 -4.1113257 -4.2169237 -4.2940054 -4.3246388 -4.3019147 -4.2350721 -4.1586509 -4.1198416][-4.3664031 -4.3511972 -4.2979169 -4.1995592 -4.0801191 -3.9834363 -3.9497581 -4.0072927 -4.1347442 -4.2421956 -4.2988138 -4.2987752 -4.2466588 -4.1774759 -4.1392441][-4.3728042 -4.3615594 -4.310585 -4.2083511 -4.0754666 -3.9493954 -3.8623998 -3.8825338 -4.0246925 -4.1658068 -4.2568512 -4.2908015 -4.2684331 -4.2213039 -4.1852674][-4.3740611 -4.3644853 -4.317121 -4.2190132 -4.0882816 -3.9526429 -3.8317456 -3.8115711 -3.9438548 -4.0978637 -4.21106 -4.272965 -4.2798753 -4.2564211 -4.2267284][-4.3687539 -4.3588867 -4.3165512 -4.2314544 -4.1234126 -4.0075512 -3.8992245 -3.8682742 -3.9581664 -4.0827537 -4.1869297 -4.2580872 -4.2808743 -4.2723813 -4.2528644][-4.3594856 -4.3507061 -4.3164959 -4.2475095 -4.1654334 -4.0796642 -4.0040693 -3.9822023 -4.032649 -4.1130924 -4.1901507 -4.2526321 -4.2770009 -4.274004 -4.2606397][-4.3503194 -4.3453412 -4.3223925 -4.2718391 -4.2131467 -4.1560335 -4.1083908 -4.0959148 -4.1223364 -4.166501 -4.2143497 -4.2558889 -4.2721992 -4.2716174 -4.2636662][-4.3451872 -4.3439369 -4.3320193 -4.3001318 -4.2609715 -4.2231283 -4.1903644 -4.1806808 -4.1946173 -4.220222 -4.2482114 -4.2702947 -4.2770495 -4.2757154 -4.2699585][-4.3426223 -4.3436236 -4.3393426 -4.3207316 -4.2961922 -4.2733278 -4.2521343 -4.2455072 -4.2518721 -4.2678785 -4.2852221 -4.2930641 -4.2930951 -4.2892623 -4.2822452][-4.3422031 -4.3430309 -4.3428717 -4.332583 -4.3179989 -4.3056993 -4.2942405 -4.2885637 -4.2890244 -4.297431 -4.309206 -4.3116984 -4.3095307 -4.3064857 -4.298934]]...]
INFO - root - 2017-12-07 12:50:01.409924: step 5710, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.715 sec/batch; 76h:28m:47s remains)
INFO - root - 2017-12-07 12:50:17.454056: step 5720, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.601 sec/batch; 71h:22m:24s remains)
INFO - root - 2017-12-07 12:50:33.898377: step 5730, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.711 sec/batch; 76h:17m:48s remains)
INFO - root - 2017-12-07 12:50:50.425956: step 5740, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 71h:21m:24s remains)
INFO - root - 2017-12-07 12:51:06.869023: step 5750, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 1.743 sec/batch; 77h:42m:49s remains)
INFO - root - 2017-12-07 12:51:23.062156: step 5760, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.633 sec/batch; 72h:48m:29s remains)
INFO - root - 2017-12-07 12:51:39.367568: step 5770, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.599 sec/batch; 71h:17m:56s remains)
INFO - root - 2017-12-07 12:51:55.725384: step 5780, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.701 sec/batch; 75h:48m:47s remains)
INFO - root - 2017-12-07 12:52:12.112831: step 5790, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.621 sec/batch; 72h:14m:04s remains)
INFO - root - 2017-12-07 12:52:28.440079: step 5800, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.700 sec/batch; 75h:46m:54s remains)
2017-12-07 12:52:29.855280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2858334 -4.284668 -4.2802944 -4.2649503 -4.2453389 -4.2344108 -4.2318048 -4.2330637 -4.237433 -4.2443161 -4.2500954 -4.253953 -4.2613177 -4.2688284 -4.2655578][-4.2818055 -4.2809119 -4.2798839 -4.2673216 -4.24929 -4.2388997 -4.2383928 -4.2442594 -4.2535214 -4.261188 -4.2629848 -4.2596316 -4.2624288 -4.2677817 -4.2635026][-4.2817531 -4.2811632 -4.2821445 -4.2731919 -4.2593379 -4.2497859 -4.2500453 -4.2601862 -4.2753758 -4.2836866 -4.2813439 -4.2713575 -4.270051 -4.2743278 -4.2713432][-4.2809706 -4.2795148 -4.2789378 -4.2708154 -4.2586694 -4.2494082 -4.248807 -4.2609873 -4.2808933 -4.2904248 -4.28423 -4.26944 -4.266499 -4.2721291 -4.2745094][-4.2796888 -4.2751641 -4.2693114 -4.2583447 -4.2435827 -4.2307124 -4.2267156 -4.23964 -4.2633071 -4.2745862 -4.2657313 -4.2468171 -4.2430496 -4.2549372 -4.2653322][-4.2774396 -4.2714109 -4.2602711 -4.243382 -4.2215872 -4.1991005 -4.1885777 -4.2033167 -4.2280245 -4.2399282 -4.2322068 -4.2135754 -4.2113461 -4.2300239 -4.2485785][-4.2733765 -4.2669005 -4.2507982 -4.2240486 -4.1895485 -4.1568851 -4.1427536 -4.1591043 -4.1838655 -4.194325 -4.1890292 -4.1707449 -4.1684542 -4.1932425 -4.2219372][-4.2684226 -4.2617421 -4.2409267 -4.2052011 -4.1592765 -4.1210756 -4.1103034 -4.1330647 -4.1586933 -4.1670156 -4.1615882 -4.1424561 -4.1377311 -4.164392 -4.196404][-4.2646866 -4.2580361 -4.235249 -4.1968088 -4.1493387 -4.1117172 -4.1042652 -4.13371 -4.1637444 -4.1735353 -4.1721058 -4.1603789 -4.1571016 -4.1759062 -4.197721][-4.2729187 -4.2641611 -4.2421179 -4.206388 -4.1634216 -4.1307068 -4.1239476 -4.1543226 -4.189991 -4.2041993 -4.2089295 -4.2065706 -4.2048664 -4.2160459 -4.2266378][-4.288579 -4.2801518 -4.2610116 -4.2328219 -4.1986074 -4.1721163 -4.1637774 -4.190455 -4.2248011 -4.2395558 -4.2475071 -4.25178 -4.2556787 -4.2638254 -4.2687316][-4.3042631 -4.2967849 -4.2815347 -4.2615047 -4.2367606 -4.215867 -4.2048445 -4.220015 -4.2434926 -4.2576509 -4.2726388 -4.2858863 -4.2952442 -4.3020854 -4.3038836][-4.3115325 -4.3032179 -4.2887983 -4.2752185 -4.2598758 -4.2444196 -4.2323613 -4.2363448 -4.2475615 -4.2568564 -4.2753429 -4.298008 -4.3140659 -4.3208675 -4.3209257][-4.3035126 -4.2903757 -4.2733736 -4.264986 -4.2623029 -4.2599368 -4.2535553 -4.2502561 -4.2507653 -4.2538266 -4.2698526 -4.2948003 -4.3139277 -4.3216105 -4.3221636][-4.2880697 -4.2673588 -4.2456703 -4.2413354 -4.2538176 -4.2684755 -4.2708511 -4.2649751 -4.2601933 -4.2574587 -4.2640443 -4.2811279 -4.2992392 -4.3089104 -4.3101521]]...]
INFO - root - 2017-12-07 12:52:46.255177: step 5810, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.677 sec/batch; 74h:44m:59s remains)
INFO - root - 2017-12-07 12:53:02.384640: step 5820, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.584 sec/batch; 70h:36m:04s remains)
INFO - root - 2017-12-07 12:53:18.801640: step 5830, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.697 sec/batch; 75h:38m:29s remains)
INFO - root - 2017-12-07 12:53:34.966270: step 5840, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.632 sec/batch; 72h:43m:55s remains)
INFO - root - 2017-12-07 12:53:51.170352: step 5850, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.644 sec/batch; 73h:15m:22s remains)
INFO - root - 2017-12-07 12:54:07.571623: step 5860, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.576 sec/batch; 70h:11m:56s remains)
INFO - root - 2017-12-07 12:54:23.800939: step 5870, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.650 sec/batch; 73h:31m:11s remains)
INFO - root - 2017-12-07 12:54:40.030259: step 5880, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.590 sec/batch; 70h:49m:31s remains)
INFO - root - 2017-12-07 12:54:56.475167: step 5890, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.560 sec/batch; 69h:30m:16s remains)
INFO - root - 2017-12-07 12:55:12.878452: step 5900, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.714 sec/batch; 76h:20m:22s remains)
2017-12-07 12:55:14.145839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.175487 -4.1809897 -4.1873274 -4.1788082 -4.1527662 -4.1235275 -4.11057 -4.1275377 -4.151381 -4.1636562 -4.1607752 -4.1476278 -4.1255074 -4.1081796 -4.1175456][-4.1235662 -4.1386819 -4.1604509 -4.1639624 -4.1442676 -4.1189933 -4.1053553 -4.1156769 -4.1381831 -4.1527061 -4.1537938 -4.1497364 -4.1391497 -4.1247439 -4.128417][-4.07286 -4.0921764 -4.1243782 -4.1386337 -4.1296449 -4.1119986 -4.0928311 -4.08113 -4.0908103 -4.1134453 -4.1297421 -4.140018 -4.1412811 -4.1310692 -4.1275172][-4.0456152 -4.061337 -4.0955586 -4.1156321 -4.1109195 -4.0920053 -4.0535274 -4.0068369 -3.9970894 -4.0390925 -4.0904641 -4.1311183 -4.1508083 -4.1471033 -4.1405854][-4.026875 -4.0371661 -4.0706048 -4.0937629 -4.0885496 -4.0612869 -3.9960523 -3.9080534 -3.8729274 -3.9403946 -4.0369778 -4.112639 -4.1546702 -4.1635337 -4.1626849][-4.001205 -4.016356 -4.060833 -4.0906324 -4.083221 -4.0486135 -3.9669161 -3.8536186 -3.7968938 -3.8716536 -3.9888906 -4.0822473 -4.1391263 -4.1617346 -4.1692][-3.9824927 -4.0037122 -4.0627208 -4.105063 -4.1020761 -4.0697002 -3.9934998 -3.8822937 -3.8162675 -3.8664961 -3.9673221 -4.0579219 -4.1231637 -4.1581655 -4.17221][-4.0132728 -4.035182 -4.0924325 -4.1355944 -4.1365151 -4.1141677 -4.0563254 -3.9617333 -3.8945537 -3.9109886 -3.9772618 -4.0504723 -4.1171637 -4.1623793 -4.1847625][-4.0920868 -4.111928 -4.1538253 -4.1794648 -4.1749878 -4.1612134 -4.1256046 -4.0533047 -3.9932923 -3.9869606 -4.0188608 -4.0699954 -4.1305151 -4.1747751 -4.1984763][-4.1665406 -4.1862278 -4.2108135 -4.2198009 -4.2131672 -4.2070384 -4.1871285 -4.1320395 -4.082396 -4.0680766 -4.0792322 -4.1104689 -4.160171 -4.1974754 -4.2184224][-4.2181497 -4.2345943 -4.2463794 -4.2487111 -4.2436738 -4.2408195 -4.2309456 -4.1911707 -4.1520934 -4.1368952 -4.1403933 -4.1600628 -4.1973925 -4.2259221 -4.2408981][-4.2498431 -4.2620087 -4.2669377 -4.2666278 -4.2643514 -4.262557 -4.2598448 -4.2365365 -4.207902 -4.19301 -4.1943884 -4.2101927 -4.2350273 -4.2506266 -4.2584214][-4.2794132 -4.287642 -4.2879152 -4.2853036 -4.2840943 -4.2818789 -4.2814827 -4.270669 -4.2499318 -4.2358723 -4.2390933 -4.2540741 -4.2720509 -4.2781477 -4.28044][-4.3040113 -4.3083777 -4.3039427 -4.299408 -4.297473 -4.294539 -4.2956953 -4.2929072 -4.2806759 -4.2721324 -4.2795525 -4.2929149 -4.3038225 -4.3047748 -4.3042364][-4.312079 -4.3134532 -4.3078604 -4.3034754 -4.3023882 -4.2996831 -4.3011703 -4.3029695 -4.2991095 -4.2966318 -4.3044972 -4.3155742 -4.3218179 -4.3197203 -4.3172793]]...]
INFO - root - 2017-12-07 12:55:30.465490: step 5910, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.639 sec/batch; 72h:59m:45s remains)
INFO - root - 2017-12-07 12:55:46.706025: step 5920, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.575 sec/batch; 70h:07m:28s remains)
INFO - root - 2017-12-07 12:56:02.704909: step 5930, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.608 sec/batch; 71h:36m:21s remains)
INFO - root - 2017-12-07 12:56:19.035959: step 5940, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 1.757 sec/batch; 78h:13m:23s remains)
INFO - root - 2017-12-07 12:56:35.310131: step 5950, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.600 sec/batch; 71h:14m:35s remains)
INFO - root - 2017-12-07 12:56:51.628539: step 5960, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 1.696 sec/batch; 75h:30m:47s remains)
INFO - root - 2017-12-07 12:57:07.935163: step 5970, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 1.601 sec/batch; 71h:16m:09s remains)
INFO - root - 2017-12-07 12:57:24.441773: step 5980, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.707 sec/batch; 76h:00m:01s remains)
INFO - root - 2017-12-07 12:57:40.588396: step 5990, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.589 sec/batch; 70h:44m:32s remains)
INFO - root - 2017-12-07 12:57:57.074840: step 6000, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.615 sec/batch; 71h:52m:52s remains)
2017-12-07 12:57:58.404869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2423925 -4.2436275 -4.2281556 -4.1911774 -4.1448536 -4.1082253 -4.1174445 -4.1568646 -4.1774988 -4.1649423 -4.1407228 -4.1415462 -4.1710682 -4.216114 -4.2624197][-4.2330689 -4.244894 -4.2342052 -4.1981606 -4.1487966 -4.1093845 -4.1205773 -4.1601996 -4.17668 -4.1628737 -4.1450071 -4.1524243 -4.1823659 -4.2255988 -4.2677116][-4.2263584 -4.2489095 -4.2507334 -4.2239957 -4.1807404 -4.1428361 -4.1477618 -4.1750369 -4.1815906 -4.1656871 -4.1535292 -4.1658654 -4.1945491 -4.2334671 -4.2697639][-4.2233567 -4.2528639 -4.2673626 -4.2519727 -4.2205043 -4.184814 -4.1798191 -4.1890426 -4.1774263 -4.1552305 -4.1475534 -4.1650629 -4.1962614 -4.2345061 -4.2688427][-4.2241063 -4.2520347 -4.267015 -4.2570548 -4.2363586 -4.20041 -4.1843495 -4.1729221 -4.14111 -4.1169357 -4.11748 -4.1421232 -4.1815295 -4.224668 -4.261972][-4.22081 -4.2360926 -4.2460923 -4.2378716 -4.2202997 -4.1813321 -4.1564589 -4.1299648 -4.0876808 -4.0667338 -4.0735335 -4.1072445 -4.1579781 -4.2088237 -4.2521238][-4.2248797 -4.226419 -4.227427 -4.2168636 -4.195641 -4.1522541 -4.1201334 -4.0857224 -4.037353 -4.0180526 -4.0296888 -4.074604 -4.1368694 -4.1944981 -4.2414131][-4.2320156 -4.2243357 -4.2161522 -4.1986251 -4.1743536 -4.130372 -4.09385 -4.0550861 -4.0043521 -3.9831133 -3.9969275 -4.051259 -4.1223569 -4.1854596 -4.233676][-4.2428837 -4.2329721 -4.2179065 -4.1920271 -4.1654491 -4.1283178 -4.094255 -4.0570874 -4.0157652 -4.0015063 -4.0165987 -4.0696697 -4.1362643 -4.1953125 -4.2387524][-4.2572904 -4.2468591 -4.2300034 -4.2017422 -4.1776357 -4.1550217 -4.1319065 -4.1043696 -4.0740576 -4.0669951 -4.0760226 -4.1145682 -4.165998 -4.2152643 -4.2524157][-4.2668405 -4.256494 -4.2402096 -4.213769 -4.1906934 -4.1782 -4.1698694 -4.1574993 -4.136878 -4.130837 -4.1351757 -4.1608872 -4.1974411 -4.2356997 -4.2669573][-4.2731895 -4.2631054 -4.2489271 -4.228651 -4.2098308 -4.2008181 -4.198472 -4.1964903 -4.1815405 -4.1737156 -4.177588 -4.1973891 -4.2247787 -4.2541261 -4.2796912][-4.2832246 -4.2740746 -4.2614126 -4.2461095 -4.2324305 -4.2264762 -4.2280245 -4.2306418 -4.2194939 -4.2056332 -4.2043881 -4.2185097 -4.2397323 -4.2643642 -4.2875047][-4.2913857 -4.2844143 -4.2745895 -4.2649021 -4.2586989 -4.2586226 -4.2642541 -4.2680511 -4.2598124 -4.2433124 -4.234652 -4.2437 -4.2614279 -4.282299 -4.3017297][-4.3084168 -4.3027205 -4.2957363 -4.290359 -4.28896 -4.2911191 -4.2957487 -4.2979493 -4.2915993 -4.2786446 -4.2684865 -4.2708926 -4.2830052 -4.2996869 -4.3157554]]...]
INFO - root - 2017-12-07 12:58:14.745681: step 6010, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 1.629 sec/batch; 72h:29m:30s remains)
INFO - root - 2017-12-07 12:58:30.722179: step 6020, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.524 sec/batch; 67h:48m:36s remains)
INFO - root - 2017-12-07 12:58:46.943175: step 6030, loss = 2.07, batch loss = 2.02 (10.4 examples/sec; 1.538 sec/batch; 68h:27m:52s remains)
INFO - root - 2017-12-07 12:59:02.996396: step 6040, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.594 sec/batch; 70h:56m:24s remains)
INFO - root - 2017-12-07 12:59:19.110930: step 6050, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.574 sec/batch; 70h:01m:17s remains)
INFO - root - 2017-12-07 12:59:35.288434: step 6060, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.638 sec/batch; 72h:52m:15s remains)
INFO - root - 2017-12-07 12:59:51.392486: step 6070, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.538 sec/batch; 68h:25m:11s remains)
INFO - root - 2017-12-07 13:00:07.520700: step 6080, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.672 sec/batch; 74h:23m:04s remains)
INFO - root - 2017-12-07 13:00:23.730896: step 6090, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.586 sec/batch; 70h:34m:40s remains)
INFO - root - 2017-12-07 13:00:39.770338: step 6100, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.716 sec/batch; 76h:19m:49s remains)
2017-12-07 13:00:41.198647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2928791 -4.2907553 -4.2824845 -4.274272 -4.2733164 -4.2806573 -4.2879562 -4.2924566 -4.2989426 -4.309864 -4.3201957 -4.3168492 -4.3088741 -4.3052382 -4.3116474][-4.2572312 -4.252717 -4.2417927 -4.2295628 -4.2262869 -4.2343154 -4.2414417 -4.2460885 -4.2538691 -4.2707944 -4.2890587 -4.2904606 -4.285171 -4.2838073 -4.2930388][-4.20917 -4.1988273 -4.184598 -4.1722441 -4.1733928 -4.1851292 -4.19116 -4.195642 -4.202353 -4.2260442 -4.2546048 -4.2639279 -4.2619691 -4.26085 -4.270093][-4.1549692 -4.1333747 -4.1140122 -4.105052 -4.1159024 -4.1346054 -4.1408415 -4.1423559 -4.14676 -4.1776686 -4.2138252 -4.2303243 -4.23136 -4.2289038 -4.2372289][-4.1011147 -4.06573 -4.0453858 -4.0463786 -4.0677795 -4.0878119 -4.0858054 -4.0785213 -4.08001 -4.1184268 -4.1597648 -4.1812353 -4.1866961 -4.1890459 -4.2037115][-4.0635123 -4.0187964 -4.00031 -4.0067673 -4.0260248 -4.0367913 -4.0201521 -3.9994574 -4.0010395 -4.0513897 -4.0993819 -4.1261382 -4.140367 -4.1572051 -4.1861362][-4.0552149 -4.0116892 -3.9945989 -3.9956517 -4.0009141 -3.9917021 -3.9560809 -3.9200878 -3.9222028 -3.9838333 -4.0418839 -4.0816689 -4.1090922 -4.1414895 -4.1820464][-4.0761223 -4.0438967 -4.0302839 -4.0227981 -4.0110164 -3.9791858 -3.9262538 -3.883153 -3.8896155 -3.9515009 -4.0103374 -4.0600305 -4.0990577 -4.1410456 -4.1833916][-4.1072521 -4.0887566 -4.0794678 -4.0669723 -4.0439253 -3.9987302 -3.9433098 -3.9066806 -3.9173973 -3.964484 -4.0137386 -4.0664363 -4.1117549 -4.15359 -4.1879468][-4.1370988 -4.1238208 -4.1211176 -4.1111584 -4.0876446 -4.04332 -3.9961777 -3.9695706 -3.9775243 -4.0069995 -4.0430264 -4.08843 -4.1302834 -4.1670403 -4.1934466][-4.1631866 -4.1494794 -4.1491671 -4.1435227 -4.1264887 -4.0939584 -4.0602584 -4.0415173 -4.045001 -4.0592413 -4.0828376 -4.1193428 -4.1533 -4.1817074 -4.2024794][-4.1931949 -4.1786041 -4.1784091 -4.1768889 -4.16892 -4.1483631 -4.1258855 -4.1104069 -4.1074157 -4.1108966 -4.1244426 -4.1503458 -4.1763377 -4.1982312 -4.2159028][-4.2305956 -4.2181082 -4.2190371 -4.2206049 -4.2176528 -4.2057505 -4.1906886 -4.17607 -4.1668277 -4.1621218 -4.1669631 -4.1843224 -4.2055035 -4.223979 -4.2399445][-4.2702241 -4.2616034 -4.263195 -4.2646341 -4.2622 -4.2544103 -4.24387 -4.2319269 -4.2217393 -4.2149873 -4.2156305 -4.2276359 -4.2446032 -4.2593036 -4.272357][-4.3059578 -4.3008523 -4.3015385 -4.3022604 -4.3004766 -4.2954745 -4.2892704 -4.2812653 -4.2730565 -4.2677827 -4.2678227 -4.2757168 -4.2872386 -4.2972746 -4.3063159]]...]
INFO - root - 2017-12-07 13:00:57.366627: step 6110, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.637 sec/batch; 72h:49m:41s remains)
INFO - root - 2017-12-07 13:01:13.561312: step 6120, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.564 sec/batch; 69h:35m:12s remains)
INFO - root - 2017-12-07 13:01:29.798202: step 6130, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.700 sec/batch; 75h:35m:25s remains)
INFO - root - 2017-12-07 13:01:45.933380: step 6140, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.568 sec/batch; 69h:42m:54s remains)
INFO - root - 2017-12-07 13:02:02.216975: step 6150, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.672 sec/batch; 74h:20m:41s remains)
INFO - root - 2017-12-07 13:02:18.196704: step 6160, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.527 sec/batch; 67h:55m:33s remains)
INFO - root - 2017-12-07 13:02:34.409215: step 6170, loss = 2.06, batch loss = 2.01 (10.4 examples/sec; 1.532 sec/batch; 68h:08m:30s remains)
INFO - root - 2017-12-07 13:02:50.651730: step 6180, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.661 sec/batch; 73h:50m:25s remains)
INFO - root - 2017-12-07 13:03:06.874862: step 6190, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.599 sec/batch; 71h:04m:52s remains)
INFO - root - 2017-12-07 13:03:22.855114: step 6200, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.715 sec/batch; 76h:14m:20s remains)
2017-12-07 13:03:24.249223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2379889 -4.2404842 -4.2554727 -4.2745361 -4.2943125 -4.301239 -4.2767076 -4.2256045 -4.1654367 -4.1210003 -4.1074057 -4.1267967 -4.1659374 -4.1921697 -4.2035937][-4.2450495 -4.2542567 -4.2706532 -4.2797165 -4.283793 -4.2774215 -4.2494664 -4.2060623 -4.1597052 -4.1292462 -4.1213174 -4.1370912 -4.1706395 -4.1945252 -4.2051911][-4.227922 -4.2466865 -4.2649245 -4.2614031 -4.2542286 -4.2392373 -4.2096214 -4.177104 -4.151721 -4.1425433 -4.1461349 -4.1591411 -4.1801376 -4.1942372 -4.2001696][-4.2114143 -4.2363105 -4.2501478 -4.2370009 -4.218575 -4.18694 -4.1447382 -4.1226478 -4.1265821 -4.1445146 -4.1669025 -4.1817656 -4.192842 -4.1923504 -4.1880779][-4.2139831 -4.23411 -4.2375636 -4.2124667 -4.175756 -4.1125884 -4.0423989 -4.0305023 -4.0738935 -4.1288419 -4.1777387 -4.2052531 -4.2136436 -4.2046542 -4.1905603][-4.2391338 -4.2483878 -4.2391834 -4.1981993 -4.1336169 -4.0207562 -3.9022162 -3.8995 -3.9943461 -4.0909896 -4.1647849 -4.2086706 -4.2279491 -4.2251034 -4.2116923][-4.2656603 -4.2675004 -4.2479959 -4.1861014 -4.0880122 -3.9251838 -3.7553895 -3.7733307 -3.9259472 -4.0539227 -4.1408014 -4.1955791 -4.2290416 -4.2382927 -4.2352977][-4.2789454 -4.2798605 -4.2576761 -4.1850553 -4.0729008 -3.9094331 -3.7580476 -3.7888193 -3.9353266 -4.0526595 -4.1339569 -4.1902213 -4.2307515 -4.24767 -4.256392][-4.2855277 -4.2922659 -4.2720881 -4.203958 -4.1060233 -3.9902029 -3.9050369 -3.9210777 -4.0088344 -4.0864835 -4.149725 -4.2004709 -4.2404451 -4.2604 -4.2733264][-4.2829823 -4.2922339 -4.2768421 -4.2222776 -4.1536469 -4.0935802 -4.0564613 -4.0589771 -4.0973229 -4.140903 -4.1862984 -4.2267718 -4.2580409 -4.2748418 -4.2854872][-4.2794981 -4.2845268 -4.2715178 -4.2337103 -4.1950374 -4.1731391 -4.1614809 -4.1580381 -4.172471 -4.1978579 -4.2335658 -4.2635417 -4.2853637 -4.2949615 -4.2941446][-4.2960973 -4.2955117 -4.2811618 -4.2561312 -4.237586 -4.2328849 -4.2288113 -4.2226219 -4.2240105 -4.2376151 -4.2671771 -4.2938423 -4.3105307 -4.3149438 -4.3068457][-4.3083534 -4.3053842 -4.2927256 -4.2804656 -4.2764578 -4.2757525 -4.2689309 -4.2571144 -4.2482333 -4.2534256 -4.280508 -4.3070846 -4.3215914 -4.3246017 -4.3179336][-4.2943139 -4.2941265 -4.2902169 -4.2933912 -4.3019 -4.3058028 -4.296679 -4.2820091 -4.2660871 -4.2606077 -4.2785449 -4.29611 -4.3037333 -4.3083472 -4.3097863][-4.266706 -4.274487 -4.2833371 -4.2994967 -4.3204575 -4.3295941 -4.3204379 -4.3024187 -4.2767329 -4.255115 -4.257153 -4.2605038 -4.2611208 -4.269486 -4.2857084]]...]
INFO - root - 2017-12-07 13:03:40.232684: step 6210, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.573 sec/batch; 69h:54m:29s remains)
INFO - root - 2017-12-07 13:03:56.160526: step 6220, loss = 2.06, batch loss = 2.01 (10.2 examples/sec; 1.567 sec/batch; 69h:39m:39s remains)
INFO - root - 2017-12-07 13:04:12.552956: step 6230, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.592 sec/batch; 70h:45m:16s remains)
INFO - root - 2017-12-07 13:04:28.747202: step 6240, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.644 sec/batch; 73h:04m:19s remains)
INFO - root - 2017-12-07 13:04:45.024407: step 6250, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.600 sec/batch; 71h:06m:59s remains)
INFO - root - 2017-12-07 13:05:01.075787: step 6260, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.691 sec/batch; 75h:08m:46s remains)
INFO - root - 2017-12-07 13:05:17.320445: step 6270, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.606 sec/batch; 71h:23m:21s remains)
INFO - root - 2017-12-07 13:05:33.541473: step 6280, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.738 sec/batch; 77h:12m:49s remains)
INFO - root - 2017-12-07 13:05:49.680229: step 6290, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.587 sec/batch; 70h:30m:31s remains)
INFO - root - 2017-12-07 13:06:05.922440: step 6300, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.585 sec/batch; 70h:24m:30s remains)
2017-12-07 13:06:07.273662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2584791 -4.2439871 -4.2222085 -4.1977215 -4.1833959 -4.1899447 -4.2209125 -4.2478428 -4.2471542 -4.2271829 -4.2070985 -4.1985569 -4.1968107 -4.190196 -4.184638][-4.2504516 -4.2468104 -4.2380872 -4.2221584 -4.2077522 -4.2047009 -4.2210336 -4.2342024 -4.2348337 -4.2242026 -4.2102318 -4.203753 -4.1972632 -4.1881995 -4.1786056][-4.2597933 -4.2687306 -4.2735519 -4.2670259 -4.2531228 -4.2368608 -4.2268362 -4.2149663 -4.206986 -4.2004747 -4.196085 -4.1940088 -4.1819568 -4.1681423 -4.1532035][-4.2759805 -4.2939482 -4.3073554 -4.3052711 -4.2903786 -4.2614169 -4.2241578 -4.1872311 -4.1716337 -4.1735291 -4.1813912 -4.1858859 -4.1734719 -4.1610684 -4.1478915][-4.2931266 -4.3121338 -4.3207145 -4.3116894 -4.28734 -4.2451544 -4.1884875 -4.1339922 -4.1167874 -4.1334844 -4.1637692 -4.181601 -4.178905 -4.1809149 -4.1853952][-4.2988448 -4.3098874 -4.3057356 -4.2842565 -4.2519259 -4.2044611 -4.1390266 -4.06931 -4.0462584 -4.0809383 -4.1418014 -4.1810074 -4.196537 -4.2166181 -4.2378182][-4.292367 -4.2880206 -4.2686787 -4.2353268 -4.2011604 -4.1587486 -4.0939546 -4.0148168 -3.9798477 -4.0287476 -4.11818 -4.1813803 -4.2162466 -4.2465191 -4.2699275][-4.2787719 -4.2553759 -4.2174063 -4.1708188 -4.1384377 -4.1061797 -4.0513153 -3.9794726 -3.9422638 -3.9949806 -4.0946016 -4.1705503 -4.2172689 -4.2477169 -4.2624273][-4.2571692 -4.2218127 -4.1763997 -4.1305866 -4.1071382 -4.0900087 -4.0577693 -4.0133419 -3.9897528 -4.0335722 -4.110661 -4.1664696 -4.2024179 -4.2222424 -4.2219276][-4.2364554 -4.2037792 -4.1666131 -4.1359434 -4.1255741 -4.1197586 -4.1080446 -4.0923867 -4.0896025 -4.1231546 -4.1596332 -4.1751361 -4.1864872 -4.191082 -4.17881][-4.2283726 -4.2044563 -4.1800165 -4.1634364 -4.1588197 -4.1553984 -4.1553936 -4.1605563 -4.1737213 -4.1955905 -4.2019634 -4.1906676 -4.1847548 -4.1824789 -4.1696239][-4.2358384 -4.2187352 -4.1996331 -4.1850977 -4.1782026 -4.170114 -4.1699729 -4.1858006 -4.2104874 -4.2269363 -4.2200541 -4.2028084 -4.1953874 -4.1901941 -4.174408][-4.246387 -4.2329993 -4.2152495 -4.197166 -4.1845651 -4.1695213 -4.164331 -4.1807227 -4.2123156 -4.2328711 -4.2296438 -4.2202959 -4.2176733 -4.2106194 -4.19275][-4.2416759 -4.2295246 -4.2136068 -4.1948013 -4.1814632 -4.1676359 -4.1553397 -4.16528 -4.1992903 -4.2289186 -4.24128 -4.2476258 -4.2514787 -4.2452583 -4.23146][-4.23337 -4.2207131 -4.2047906 -4.1887951 -4.1816139 -4.1769228 -4.1654444 -4.1683884 -4.1973357 -4.2283597 -4.2512465 -4.2678037 -4.2741442 -4.2677641 -4.2574544]]...]
INFO - root - 2017-12-07 13:06:23.478164: step 6310, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.688 sec/batch; 75h:00m:20s remains)
INFO - root - 2017-12-07 13:06:39.701836: step 6320, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.619 sec/batch; 71h:55m:47s remains)
INFO - root - 2017-12-07 13:06:56.036034: step 6330, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 1.502 sec/batch; 66h:43m:34s remains)
INFO - root - 2017-12-07 13:07:12.224994: step 6340, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.586 sec/batch; 70h:26m:33s remains)
INFO - root - 2017-12-07 13:07:28.602778: step 6350, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.602 sec/batch; 71h:08m:54s remains)
INFO - root - 2017-12-07 13:07:45.025150: step 6360, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.712 sec/batch; 76h:01m:59s remains)
INFO - root - 2017-12-07 13:08:01.173634: step 6370, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.612 sec/batch; 71h:35m:09s remains)
INFO - root - 2017-12-07 13:08:17.509704: step 6380, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.686 sec/batch; 74h:52m:18s remains)
INFO - root - 2017-12-07 13:08:33.641291: step 6390, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.626 sec/batch; 72h:11m:11s remains)
INFO - root - 2017-12-07 13:08:49.995961: step 6400, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.652 sec/batch; 73h:22m:04s remains)
2017-12-07 13:08:51.417121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3161659 -4.3041091 -4.2944865 -4.29171 -4.29715 -4.306736 -4.3158979 -4.3138547 -4.3012724 -4.2915945 -4.2914848 -4.2992768 -4.3152752 -4.3298526 -4.3392878][-4.3003006 -4.2837381 -4.2684312 -4.2621679 -4.270813 -4.2823925 -4.2871661 -4.2767272 -4.2599854 -4.2589073 -4.2745166 -4.2922626 -4.3081861 -4.3214808 -4.3315458][-4.2957764 -4.2710109 -4.2493753 -4.2400012 -4.2483068 -4.2592263 -4.2574959 -4.2378087 -4.217504 -4.2257676 -4.2585478 -4.2862782 -4.3021941 -4.3152347 -4.3256311][-4.296731 -4.2626786 -4.2340188 -4.22131 -4.2311363 -4.2442679 -4.2408175 -4.2221112 -4.2008662 -4.2078838 -4.238287 -4.2669315 -4.2849689 -4.2999444 -4.3095188][-4.2906957 -4.2573891 -4.2276907 -4.210135 -4.2106886 -4.2196541 -4.2225723 -4.2184181 -4.2049446 -4.2049971 -4.2173233 -4.2366624 -4.2532544 -4.2697558 -4.2859678][-4.2789512 -4.2549825 -4.229845 -4.2043753 -4.178679 -4.1586514 -4.154171 -4.1718197 -4.186285 -4.1957235 -4.2027426 -4.2085843 -4.220325 -4.2418542 -4.2659283][-4.2648263 -4.2478652 -4.2310934 -4.1994476 -4.1439595 -4.0727177 -4.0389986 -4.0822043 -4.1447816 -4.180459 -4.1917725 -4.1886778 -4.1901627 -4.2179427 -4.2515779][-4.2367516 -4.2258115 -4.2192507 -4.1932287 -4.1240916 -4.0150189 -3.9426749 -3.9878585 -4.0863018 -4.1446462 -4.1693854 -4.1718335 -4.1755953 -4.2075973 -4.245872][-4.1998715 -4.19074 -4.1942286 -4.1851025 -4.13806 -4.0498848 -3.9805989 -3.9920754 -4.0598493 -4.1115732 -4.144927 -4.1641545 -4.1833615 -4.2166185 -4.2506061][-4.1927829 -4.1821227 -4.1925769 -4.2015586 -4.186223 -4.1399474 -4.0971379 -4.0836468 -4.0979958 -4.1177173 -4.141953 -4.1706119 -4.2018323 -4.2318335 -4.2587347][-4.2105279 -4.1978536 -4.210578 -4.2268443 -4.2335563 -4.2163186 -4.1971478 -4.1802006 -4.1682224 -4.1612039 -4.1689014 -4.1947331 -4.2285876 -4.2534537 -4.2733197][-4.2439342 -4.2299829 -4.2386761 -4.2533731 -4.2687569 -4.2683177 -4.2618723 -4.2488256 -4.2282047 -4.2065592 -4.2021694 -4.2216678 -4.2520843 -4.2732139 -4.2872043][-4.2881417 -4.2748823 -4.2779231 -4.2862053 -4.2987561 -4.3045716 -4.303731 -4.2958322 -4.2732763 -4.2443252 -4.2326555 -4.2458615 -4.2697372 -4.2868586 -4.29648][-4.3212156 -4.313417 -4.3145342 -4.3192072 -4.3269811 -4.3356295 -4.3389273 -4.33466 -4.3178558 -4.2900743 -4.2742348 -4.2808876 -4.2950692 -4.3026824 -4.3074584][-4.3408375 -4.3390508 -4.3392048 -4.3405938 -4.3443413 -4.35087 -4.3551421 -4.3532586 -4.3448367 -4.3277221 -4.3133831 -4.3137689 -4.3187838 -4.3186512 -4.3212547]]...]
INFO - root - 2017-12-07 13:09:07.795782: step 6410, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.731 sec/batch; 76h:52m:21s remains)
INFO - root - 2017-12-07 13:09:24.004563: step 6420, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.536 sec/batch; 68h:10m:59s remains)
INFO - root - 2017-12-07 13:09:40.181385: step 6430, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.558 sec/batch; 69h:09m:47s remains)
INFO - root - 2017-12-07 13:09:56.339745: step 6440, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.664 sec/batch; 73h:51m:56s remains)
INFO - root - 2017-12-07 13:10:12.348239: step 6450, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.599 sec/batch; 70h:59m:18s remains)
INFO - root - 2017-12-07 13:10:28.611841: step 6460, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.665 sec/batch; 73h:54m:12s remains)
INFO - root - 2017-12-07 13:10:44.922671: step 6470, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 70h:55m:05s remains)
INFO - root - 2017-12-07 13:11:01.189582: step 6480, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.647 sec/batch; 73h:05m:45s remains)
INFO - root - 2017-12-07 13:11:17.506491: step 6490, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.641 sec/batch; 72h:49m:57s remains)
INFO - root - 2017-12-07 13:11:33.660717: step 6500, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.610 sec/batch; 71h:27m:03s remains)
2017-12-07 13:11:34.986171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2927365 -4.2869182 -4.2669816 -4.2462931 -4.2355237 -4.2356358 -4.2390666 -4.2426443 -4.2399364 -4.2319241 -4.2092323 -4.1947594 -4.1900778 -4.1781726 -4.168417][-4.2870746 -4.2792344 -4.2563052 -4.2300668 -4.212359 -4.2080612 -4.2123551 -4.22133 -4.2303219 -4.2359819 -4.2222486 -4.2036824 -4.19529 -4.1808352 -4.169013][-4.2967463 -4.2859364 -4.2589631 -4.2243977 -4.1995444 -4.1905265 -4.1950769 -4.2077055 -4.223681 -4.2362866 -4.2314782 -4.2157717 -4.2062778 -4.1852417 -4.1633821][-4.3058372 -4.294281 -4.2630582 -4.2236023 -4.1927891 -4.1761465 -4.1753173 -4.1871881 -4.204093 -4.2215848 -4.2267723 -4.2176666 -4.213336 -4.1878686 -4.1551394][-4.3036141 -4.2933688 -4.261086 -4.2156248 -4.1736546 -4.146307 -4.1372671 -4.1481986 -4.1683722 -4.1905355 -4.2047296 -4.2063828 -4.2128134 -4.1922197 -4.1603842][-4.3049793 -4.2946949 -4.2652531 -4.2157679 -4.1620359 -4.1253204 -4.1123567 -4.1203408 -4.1422768 -4.1687512 -4.1886706 -4.2045937 -4.2250471 -4.2181673 -4.1923523][-4.3178177 -4.3133826 -4.2940412 -4.254961 -4.2026486 -4.1619 -4.1427279 -4.1406956 -4.1528482 -4.1743126 -4.1921806 -4.2128568 -4.240366 -4.2479415 -4.2340713][-4.3293772 -4.3366752 -4.3319249 -4.31133 -4.2748857 -4.235322 -4.2071357 -4.1915083 -4.1882935 -4.1959224 -4.202858 -4.2189822 -4.245646 -4.2646842 -4.2623191][-4.329586 -4.3454132 -4.352746 -4.3476439 -4.3305011 -4.3012919 -4.2688518 -4.243773 -4.2296124 -4.2224274 -4.2157016 -4.2266626 -4.2473183 -4.2691169 -4.2771864][-4.3143711 -4.3352275 -4.3499732 -4.3533616 -4.3487816 -4.3304272 -4.2988806 -4.2711887 -4.2539577 -4.2399735 -4.2257481 -4.233252 -4.2488937 -4.2683024 -4.2811842][-4.2923908 -4.3190041 -4.3387985 -4.3458056 -4.3431535 -4.3267894 -4.2980919 -4.2728639 -4.2563195 -4.2414775 -4.2279625 -4.2349486 -4.2519007 -4.2675381 -4.2781634][-4.2724934 -4.3044958 -4.3271561 -4.3335142 -4.3261952 -4.302249 -4.2736998 -4.2526226 -4.2347412 -4.2196236 -4.208962 -4.2177157 -4.2409587 -4.2568893 -4.2639241][-4.2616277 -4.2990584 -4.3240676 -4.329051 -4.3129125 -4.2766023 -4.2421904 -4.2223392 -4.2020793 -4.1838379 -4.1770134 -4.1925173 -4.224144 -4.2472925 -4.2542391][-4.2664566 -4.304718 -4.328413 -4.3292503 -4.3041191 -4.2565851 -4.2135181 -4.1948605 -4.1781683 -4.1560664 -4.1523318 -4.1762748 -4.21358 -4.2405438 -4.2486873][-4.2852521 -4.3200126 -4.3353982 -4.3255968 -4.29162 -4.2384892 -4.19045 -4.1725192 -4.1647577 -4.1483736 -4.1496482 -4.178915 -4.2144909 -4.2385349 -4.2487769]]...]
INFO - root - 2017-12-07 13:11:51.060779: step 6510, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.597 sec/batch; 70h:51m:58s remains)
INFO - root - 2017-12-07 13:12:07.453000: step 6520, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.701 sec/batch; 75h:27m:24s remains)
INFO - root - 2017-12-07 13:12:23.705436: step 6530, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.612 sec/batch; 71h:31m:59s remains)
INFO - root - 2017-12-07 13:12:39.998507: step 6540, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.651 sec/batch; 73h:13m:57s remains)
INFO - root - 2017-12-07 13:12:56.270193: step 6550, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.586 sec/batch; 70h:20m:06s remains)
INFO - root - 2017-12-07 13:13:12.596036: step 6560, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.567 sec/batch; 69h:29m:29s remains)
INFO - root - 2017-12-07 13:13:28.858225: step 6570, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.717 sec/batch; 76h:09m:27s remains)
INFO - root - 2017-12-07 13:13:44.953904: step 6580, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.592 sec/batch; 70h:37m:03s remains)
INFO - root - 2017-12-07 13:14:01.270975: step 6590, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.725 sec/batch; 76h:29m:22s remains)
INFO - root - 2017-12-07 13:14:17.365617: step 6600, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.608 sec/batch; 71h:19m:25s remains)
2017-12-07 13:14:18.740979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2313824 -4.1878371 -4.1615663 -4.1519985 -4.1513047 -4.1645312 -4.1875286 -4.2134666 -4.2353191 -4.2470431 -4.2496958 -4.2496057 -4.2494917 -4.2491717 -4.2483535][-4.2230535 -4.1711431 -4.1370387 -4.11831 -4.10922 -4.12032 -4.1470766 -4.1802692 -4.2093177 -4.2241306 -4.2262516 -4.2259221 -4.2246823 -4.2226944 -4.222034][-4.2412558 -4.1967587 -4.1689363 -4.1500936 -4.13547 -4.1387062 -4.156899 -4.1839333 -4.2101026 -4.2235103 -4.2267694 -4.2276492 -4.2256627 -4.2200089 -4.216157][-4.2522149 -4.2167282 -4.1990008 -4.1850734 -4.1714368 -4.1659126 -4.1691356 -4.1823826 -4.2016959 -4.2131062 -4.21923 -4.2232137 -4.2227464 -4.2142429 -4.2059717][-4.2542405 -4.2236452 -4.210228 -4.1972966 -4.1841464 -4.1720276 -4.1592646 -4.1579223 -4.1682243 -4.1781983 -4.1954322 -4.2132897 -4.2194896 -4.2112212 -4.19827][-4.2499204 -4.2168441 -4.1985145 -4.181438 -4.1661172 -4.1479287 -4.1207447 -4.1011691 -4.0969534 -4.1070142 -4.1446376 -4.1895776 -4.2139287 -4.2126217 -4.1982117][-4.240119 -4.2018819 -4.1745648 -4.1514387 -4.1295657 -4.1030297 -4.0664506 -4.0327878 -4.01684 -4.0311761 -4.0932703 -4.1664085 -4.2112741 -4.2250767 -4.2189627][-4.2354779 -4.1927257 -4.1580648 -4.132309 -4.1094613 -4.0760942 -4.0314579 -3.9881916 -3.9642596 -3.9805508 -4.0514679 -4.1342897 -4.1899981 -4.2194996 -4.2320781][-4.2486906 -4.2091789 -4.1774807 -4.1606417 -4.1450353 -4.1105566 -4.0619645 -4.0176926 -3.9924376 -4.0061827 -4.0651917 -4.1376333 -4.1903954 -4.2251606 -4.2464442][-4.2620049 -4.229414 -4.2068968 -4.2041078 -4.2049575 -4.1852431 -4.1486144 -4.1107631 -4.0879912 -4.0949254 -4.1289282 -4.1752939 -4.2146549 -4.2444415 -4.2627149][-4.2690597 -4.2386622 -4.2192259 -4.2208724 -4.2332249 -4.2346034 -4.222713 -4.2038479 -4.1882215 -4.1848207 -4.1946688 -4.2148 -4.2383957 -4.2572374 -4.267076][-4.2716246 -4.2420855 -4.2236137 -4.2219353 -4.2380476 -4.2549295 -4.2633948 -4.264977 -4.2627673 -4.2575421 -4.2532506 -4.2564106 -4.2643232 -4.2685394 -4.2672415][-4.2716713 -4.24123 -4.2212095 -4.2131195 -4.224802 -4.2450638 -4.2627397 -4.282156 -4.2976303 -4.3000507 -4.2935743 -4.2899113 -4.2879205 -4.2811713 -4.2715349][-4.2735624 -4.244442 -4.2209306 -4.20405 -4.2050343 -4.2189918 -4.2348905 -4.2595205 -4.2882609 -4.3027024 -4.3025475 -4.30047 -4.2972326 -4.2871656 -4.2724929][-4.2748933 -4.2479129 -4.2214575 -4.1917467 -4.1762991 -4.1760979 -4.1815019 -4.2058415 -4.2437148 -4.2712336 -4.2820716 -4.2862849 -4.2866774 -4.27712 -4.25839]]...]
INFO - root - 2017-12-07 13:14:34.767499: step 6610, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.578 sec/batch; 69h:58m:14s remains)
INFO - root - 2017-12-07 13:14:50.964459: step 6620, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 1.705 sec/batch; 75h:36m:57s remains)
INFO - root - 2017-12-07 13:15:07.110619: step 6630, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.534 sec/batch; 68h:00m:28s remains)
INFO - root - 2017-12-07 13:15:23.394992: step 6640, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.688 sec/batch; 74h:49m:54s remains)
INFO - root - 2017-12-07 13:15:39.620134: step 6650, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.576 sec/batch; 69h:52m:25s remains)
INFO - root - 2017-12-07 13:15:55.824731: step 6660, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.604 sec/batch; 71h:07m:04s remains)
INFO - root - 2017-12-07 13:16:11.836833: step 6670, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 70h:49m:55s remains)
INFO - root - 2017-12-07 13:16:28.020066: step 6680, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.557 sec/batch; 69h:01m:36s remains)
INFO - root - 2017-12-07 13:16:44.241244: step 6690, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.650 sec/batch; 73h:07m:52s remains)
INFO - root - 2017-12-07 13:17:00.444475: step 6700, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.612 sec/batch; 71h:27m:21s remains)
2017-12-07 13:17:01.758642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1489835 -4.1434426 -4.1351814 -4.1347446 -4.1333122 -4.1488976 -4.1668296 -4.1708198 -4.1527591 -4.1396084 -4.1316495 -4.1311903 -4.1859379 -4.2494783 -4.26932][-4.1390996 -4.1352382 -4.128706 -4.1331725 -4.1409597 -4.1607666 -4.1716228 -4.16438 -4.1368885 -4.1185451 -4.1095061 -4.1146669 -4.17999 -4.251615 -4.2738638][-4.1434741 -4.1466866 -4.1422338 -4.1463923 -4.1567531 -4.1757274 -4.1783409 -4.1607671 -4.1274681 -4.1056623 -4.0981441 -4.1118464 -4.183579 -4.2572584 -4.2761612][-4.1481252 -4.1628737 -4.1615934 -4.1592865 -4.1665249 -4.1838169 -4.1820211 -4.1570311 -4.1173205 -4.0943813 -4.0900359 -4.1123543 -4.1862535 -4.2557092 -4.2689371][-4.1402726 -4.1637673 -4.1657567 -4.1576185 -4.1612272 -4.1788487 -4.17161 -4.1384511 -4.0958328 -4.0789104 -4.0852733 -4.1159663 -4.1858091 -4.2447357 -4.2496548][-4.1395373 -4.1596556 -4.1578345 -4.1426449 -4.1401243 -4.1555257 -4.1389389 -4.0959435 -4.0518856 -4.0519438 -4.0811381 -4.1240044 -4.1877761 -4.2329564 -4.2276688][-4.1668887 -4.1779227 -4.1628838 -4.135509 -4.1275358 -4.1355753 -4.1071362 -4.056251 -4.0165067 -4.0364647 -4.08704 -4.1389613 -4.1940632 -4.2236857 -4.2099676][-4.2178159 -4.2209435 -4.1966767 -4.1616511 -4.1484447 -4.1422119 -4.1079054 -4.0626206 -4.0360155 -4.060884 -4.111937 -4.1628804 -4.2050772 -4.2209182 -4.2037573][-4.2786288 -4.2761416 -4.2469606 -4.2105284 -4.190948 -4.1680627 -4.13047 -4.0999537 -4.0881939 -4.109477 -4.1485548 -4.1898084 -4.2180219 -4.2212672 -4.19931][-4.3274875 -4.3209505 -4.2882581 -4.2517133 -4.2258449 -4.1941805 -4.1604042 -4.1447968 -4.1412082 -4.153904 -4.1801114 -4.209445 -4.22737 -4.2205906 -4.1943421][-4.3483162 -4.3415103 -4.3064256 -4.2700691 -4.2428031 -4.2143831 -4.1936669 -4.19084 -4.1880407 -4.18431 -4.197578 -4.2195868 -4.235261 -4.2245941 -4.1974368][-4.340179 -4.3286424 -4.2914624 -4.2562761 -4.2345724 -4.2169814 -4.2104707 -4.2166595 -4.2139821 -4.2025442 -4.2084246 -4.2256742 -4.2402158 -4.2289429 -4.2006836][-4.3123355 -4.2923174 -4.2544475 -4.2232604 -4.2099142 -4.2073541 -4.2122192 -4.2286353 -4.2357292 -4.2268462 -4.22817 -4.2392735 -4.2454753 -4.2295752 -4.19581][-4.2863741 -4.2611618 -4.2251749 -4.1968517 -4.1879086 -4.1972108 -4.2131267 -4.2393632 -4.2552338 -4.2476592 -4.2450538 -4.2480669 -4.2439094 -4.2205653 -4.1845865][-4.2784128 -4.2519112 -4.2175493 -4.1879683 -4.1758823 -4.1854377 -4.2067618 -4.2410769 -4.2647004 -4.2630963 -4.2540107 -4.2464132 -4.2370844 -4.2112851 -4.1768579]]...]
INFO - root - 2017-12-07 13:17:18.262265: step 6710, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.601 sec/batch; 70h:57m:47s remains)
INFO - root - 2017-12-07 13:17:34.542959: step 6720, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.684 sec/batch; 74h:36m:29s remains)
INFO - root - 2017-12-07 13:17:50.696841: step 6730, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.590 sec/batch; 70h:26m:27s remains)
INFO - root - 2017-12-07 13:18:06.928552: step 6740, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.637 sec/batch; 72h:31m:41s remains)
INFO - root - 2017-12-07 13:18:23.277714: step 6750, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.587 sec/batch; 70h:19m:46s remains)
INFO - root - 2017-12-07 13:18:39.657850: step 6760, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.720 sec/batch; 76h:11m:08s remains)
INFO - root - 2017-12-07 13:18:55.746982: step 6770, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.610 sec/batch; 71h:18m:44s remains)
INFO - root - 2017-12-07 13:19:11.887042: step 6780, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.660 sec/batch; 73h:32m:17s remains)
INFO - root - 2017-12-07 13:19:28.140046: step 6790, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.645 sec/batch; 72h:51m:26s remains)
INFO - root - 2017-12-07 13:19:44.272013: step 6800, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.596 sec/batch; 70h:42m:31s remains)
2017-12-07 13:19:45.691391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.281621 -4.2844267 -4.2785873 -4.2744913 -4.2731628 -4.2801414 -4.3025837 -4.3233209 -4.3205018 -4.2942119 -4.2529764 -4.2271338 -4.2370276 -4.2610569 -4.291616][-4.2424984 -4.2440977 -4.2345543 -4.2316809 -4.2398968 -4.2567229 -4.2860432 -4.3092122 -4.3051095 -4.2774973 -4.2310629 -4.200398 -4.2070575 -4.2327719 -4.2742515][-4.2014489 -4.2026477 -4.1935992 -4.19305 -4.209343 -4.2390471 -4.277441 -4.3068852 -4.3058662 -4.2795525 -4.2272854 -4.189589 -4.18808 -4.2085462 -4.2552457][-4.1782408 -4.1898384 -4.1868281 -4.1891789 -4.2040467 -4.2326059 -4.2715607 -4.303628 -4.3078461 -4.2874122 -4.2370181 -4.198401 -4.1900792 -4.2029705 -4.2466731][-4.1646843 -4.1907825 -4.1971211 -4.2022414 -4.2093444 -4.2279358 -4.2597923 -4.2836504 -4.2904372 -4.2790956 -4.2372341 -4.2042713 -4.1970963 -4.2052569 -4.2450247][-4.1410127 -4.1742253 -4.18799 -4.1964331 -4.1962547 -4.2036109 -4.2208676 -4.2319379 -4.2438984 -4.2476115 -4.2181 -4.1928325 -4.1918564 -4.2017856 -4.242506][-4.1163568 -4.1470037 -4.1640453 -4.1710892 -4.1616158 -4.1532683 -4.1482725 -4.143239 -4.160645 -4.1866217 -4.1808333 -4.1723132 -4.1857924 -4.2029281 -4.2419629][-4.1112533 -4.132091 -4.1391768 -4.1282597 -4.095592 -4.0674543 -4.0414548 -4.0316806 -4.0680389 -4.1237588 -4.1424751 -4.1494493 -4.1761446 -4.207088 -4.2473521][-4.12534 -4.1357017 -4.1326485 -4.1093497 -4.0653291 -4.030633 -3.9994094 -3.9904473 -4.0340037 -4.0990715 -4.127471 -4.1427321 -4.175961 -4.2130766 -4.2539954][-4.13472 -4.1429048 -4.1424112 -4.1298327 -4.1053882 -4.0906959 -4.0674129 -4.0532231 -4.0793881 -4.1272221 -4.1471114 -4.1581936 -4.1904387 -4.2256122 -4.2630434][-4.1471424 -4.158802 -4.1690278 -4.1758575 -4.1726565 -4.1728106 -4.153842 -4.1364574 -4.1479845 -4.1781907 -4.1877522 -4.1902561 -4.2175083 -4.2487707 -4.2799468][-4.1791267 -4.1927986 -4.208941 -4.2249689 -4.2295456 -4.2342486 -4.2188764 -4.1995854 -4.2014761 -4.2191525 -4.2237582 -4.2224178 -4.2448821 -4.2726808 -4.2989011][-4.2201958 -4.2330537 -4.2470417 -4.258256 -4.261631 -4.2717991 -4.2656889 -4.2502923 -4.2454839 -4.2511 -4.2512488 -4.2496815 -4.2679205 -4.2910671 -4.31355][-4.2483978 -4.2552681 -4.2646375 -4.2704191 -4.2753167 -4.2859082 -4.2882204 -4.2819505 -4.2788262 -4.2780828 -4.2743759 -4.272089 -4.2860079 -4.30404 -4.3218617][-4.2633324 -4.2651072 -4.2706881 -4.2749586 -4.2843542 -4.2968421 -4.3035021 -4.3019471 -4.2996087 -4.29731 -4.2928715 -4.290226 -4.2995424 -4.3125553 -4.3243294]]...]
INFO - root - 2017-12-07 13:20:01.931828: step 6810, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.601 sec/batch; 70h:53m:23s remains)
INFO - root - 2017-12-07 13:20:18.161905: step 6820, loss = 2.09, batch loss = 2.04 (9.7 examples/sec; 1.646 sec/batch; 72h:54m:21s remains)
INFO - root - 2017-12-07 13:20:34.341219: step 6830, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.631 sec/batch; 72h:12m:17s remains)
INFO - root - 2017-12-07 13:20:50.645626: step 6840, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 1.500 sec/batch; 66h:24m:20s remains)
INFO - root - 2017-12-07 13:21:06.878638: step 6850, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.641 sec/batch; 72h:38m:49s remains)
INFO - root - 2017-12-07 13:21:22.902907: step 6860, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.562 sec/batch; 69h:10m:22s remains)
INFO - root - 2017-12-07 13:21:39.282762: step 6870, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.714 sec/batch; 75h:52m:05s remains)
INFO - root - 2017-12-07 13:21:55.444037: step 6880, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.624 sec/batch; 71h:54m:20s remains)
INFO - root - 2017-12-07 13:22:11.702485: step 6890, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.719 sec/batch; 76h:05m:55s remains)
INFO - root - 2017-12-07 13:22:27.863746: step 6900, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.574 sec/batch; 69h:40m:44s remains)
2017-12-07 13:22:29.235638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2152462 -4.1868386 -4.1675858 -4.1472654 -4.1346312 -4.1393814 -4.1593041 -4.1695914 -4.1781483 -4.2025685 -4.2253561 -4.2391591 -4.2412648 -4.2352991 -4.2304492][-4.1998191 -4.1675296 -4.1484923 -4.1320391 -4.1209788 -4.1187973 -4.1291103 -4.1383162 -4.1538377 -4.1836448 -4.20701 -4.2227135 -4.2204514 -4.2055683 -4.1944408][-4.203877 -4.164958 -4.142765 -4.1302462 -4.12249 -4.111527 -4.1081572 -4.1158447 -4.1437626 -4.17711 -4.1954894 -4.2028604 -4.1906605 -4.1679969 -4.1558361][-4.2192659 -4.1725683 -4.1428032 -4.1327066 -4.1260209 -4.1108627 -4.0986271 -4.1007624 -4.1340532 -4.1657462 -4.1743412 -4.1677957 -4.146874 -4.1298394 -4.1284728][-4.2275891 -4.1632576 -4.1167688 -4.1028724 -4.0965323 -4.0825648 -4.0658722 -4.0673127 -4.1056156 -4.1356 -4.1337276 -4.1142073 -4.0906725 -4.0862007 -4.1030455][-4.225244 -4.1410456 -4.0760922 -4.0587697 -4.0573425 -4.0422745 -4.0177407 -4.0194674 -4.0665264 -4.0944953 -4.0780449 -4.0521531 -4.0423217 -4.0604496 -4.0966849][-4.215322 -4.1196394 -4.046659 -4.028017 -4.0315523 -4.0132546 -3.9743466 -3.9675565 -4.020617 -4.0470657 -4.0267434 -4.0043912 -4.0148735 -4.0602708 -4.1117978][-4.198483 -4.104517 -4.0381303 -4.0236239 -4.0240655 -3.9979045 -3.9485416 -3.9363549 -3.9914613 -4.0197258 -4.0096035 -4.0016627 -4.0268669 -4.0817547 -4.1273365][-4.1879926 -4.1120529 -4.06893 -4.0659661 -4.0667334 -4.0391917 -3.9910269 -3.9795816 -4.0248976 -4.0425391 -4.0337129 -4.0329576 -4.0546627 -4.0997591 -4.1329775][-4.1899018 -4.136766 -4.1182256 -4.1268544 -4.1332421 -4.1155853 -4.0787096 -4.0643506 -4.0855632 -4.0744848 -4.04495 -4.03503 -4.0507479 -4.0878134 -4.106513][-4.1912494 -4.1521273 -4.1509814 -4.1697016 -4.1804247 -4.1685934 -4.1380315 -4.1154256 -4.1080041 -4.0742331 -4.0267925 -4.0069 -4.0193186 -4.0522819 -4.0664339][-4.1817942 -4.1485367 -4.159411 -4.18418 -4.1986027 -4.18823 -4.1563683 -4.1263781 -4.1016331 -4.0677772 -4.0338259 -4.02043 -4.0251474 -4.0431318 -4.0543933][-4.1633067 -4.1322036 -4.1487689 -4.1762528 -4.1960011 -4.1874943 -4.1569443 -4.1289492 -4.1029782 -4.0851326 -4.0792274 -4.0795488 -4.0774651 -4.0805182 -4.0869894][-4.140152 -4.1104345 -4.1303549 -4.1614566 -4.1880536 -4.183548 -4.159543 -4.137692 -4.1148944 -4.1106095 -4.1230865 -4.1319704 -4.12957 -4.1342888 -4.1448469][-4.1284146 -4.1037264 -4.1247697 -4.15453 -4.1828742 -4.1851811 -4.1718144 -4.1556072 -4.1377807 -4.142087 -4.1652951 -4.1794419 -4.1804934 -4.1879411 -4.1990485]]...]
INFO - root - 2017-12-07 13:22:45.117251: step 6910, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.573 sec/batch; 69h:37m:24s remains)
INFO - root - 2017-12-07 13:23:01.548308: step 6920, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.687 sec/batch; 74h:41m:06s remains)
INFO - root - 2017-12-07 13:23:17.788826: step 6930, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.634 sec/batch; 72h:20m:02s remains)
INFO - root - 2017-12-07 13:23:34.058174: step 6940, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.642 sec/batch; 72h:39m:15s remains)
INFO - root - 2017-12-07 13:23:50.223062: step 6950, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.564 sec/batch; 69h:13m:04s remains)
INFO - root - 2017-12-07 13:24:06.476982: step 6960, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.608 sec/batch; 71h:08m:19s remains)
INFO - root - 2017-12-07 13:24:22.487147: step 6970, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.628 sec/batch; 72h:02m:06s remains)
INFO - root - 2017-12-07 13:24:38.787119: step 6980, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.612 sec/batch; 71h:19m:11s remains)
INFO - root - 2017-12-07 13:24:55.162006: step 6990, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 1.761 sec/batch; 77h:54m:48s remains)
INFO - root - 2017-12-07 13:25:11.391777: step 7000, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.580 sec/batch; 69h:53m:23s remains)
2017-12-07 13:25:12.861481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2657046 -4.2698236 -4.2901888 -4.306869 -4.3184843 -4.3263288 -4.3312988 -4.3265972 -4.3094707 -4.2923279 -4.2775025 -4.2670431 -4.2567945 -4.2498336 -4.2533717][-4.2495933 -4.2502708 -4.2741747 -4.2929029 -4.3036447 -4.3092241 -4.3119617 -4.3053927 -4.2835145 -4.2593985 -4.2399716 -4.2256103 -4.2118783 -4.2072458 -4.21832][-4.2271566 -4.22706 -4.2543507 -4.273243 -4.2807345 -4.2809238 -4.27854 -4.268621 -4.2440434 -4.216712 -4.1951618 -4.1770897 -4.1583486 -4.155654 -4.1759396][-4.2172594 -4.2141533 -4.234324 -4.2492881 -4.2529955 -4.2467341 -4.2344718 -4.2189164 -4.1955905 -4.1737928 -4.1572351 -4.1370926 -4.1133785 -4.1136317 -4.1437812][-4.2026024 -4.1954532 -4.2063942 -4.2121677 -4.2098269 -4.1941648 -4.1692452 -4.1483626 -4.1291609 -4.1175437 -4.1103792 -4.0949888 -4.0779982 -4.0912504 -4.12987][-4.1601458 -4.1555409 -4.1662045 -4.1673336 -4.1577196 -4.1267242 -4.07999 -4.0416403 -4.0218458 -4.0226159 -4.0318594 -4.0340757 -4.0432682 -4.08195 -4.1310339][-4.1046057 -4.1020923 -4.1071386 -4.1013179 -4.0787859 -4.0269933 -3.9445798 -3.8753176 -3.8551779 -3.8802021 -3.9227767 -3.9601812 -4.0083275 -4.0744562 -4.1328382][-4.0660968 -4.0591278 -4.0512638 -4.0288668 -3.9899082 -3.9235845 -3.8195176 -3.7309904 -3.72785 -3.7941797 -3.8759961 -3.9448919 -4.0153618 -4.0887723 -4.1476212][-4.0781064 -4.0671873 -4.0502958 -4.0167642 -3.9733193 -3.9213748 -3.840534 -3.7781591 -3.801748 -3.8776789 -3.9564583 -4.0227785 -4.0845428 -4.1420445 -4.1882467][-4.1296043 -4.1163034 -4.1020131 -4.0787916 -4.0530577 -4.0268154 -3.9873593 -3.9636123 -3.9890778 -4.0394959 -4.0871177 -4.1284332 -4.17061 -4.2115064 -4.2453737][-4.2041335 -4.1955266 -4.1901417 -4.1800594 -4.1686974 -4.1557655 -4.1377134 -4.1307073 -4.1493912 -4.1783366 -4.2037292 -4.2274418 -4.2569475 -4.2869978 -4.3091469][-4.2737794 -4.2731333 -4.275506 -4.2727675 -4.2669187 -4.2607594 -4.253304 -4.250669 -4.2618661 -4.2775679 -4.2923269 -4.3064165 -4.3251505 -4.343854 -4.3554411][-4.3156633 -4.3182721 -4.322403 -4.3243804 -4.3253808 -4.3245058 -4.3219028 -4.3221278 -4.3287926 -4.3363223 -4.343061 -4.3499212 -4.3594303 -4.3683996 -4.3737745][-4.3404279 -4.3440232 -4.3484249 -4.3528929 -4.3559828 -4.3578973 -4.3584137 -4.3601513 -4.3636527 -4.3648252 -4.3648677 -4.3659081 -4.3685369 -4.37096 -4.3725548][-4.3487425 -4.3512406 -4.353858 -4.3576326 -4.3607078 -4.36301 -4.3647251 -4.3662133 -4.3671455 -4.3664756 -4.3658152 -4.3647637 -4.3637757 -4.3631864 -4.3635015]]...]
INFO - root - 2017-12-07 13:25:29.070579: step 7010, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.566 sec/batch; 69h:16m:23s remains)
INFO - root - 2017-12-07 13:25:45.319520: step 7020, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.745 sec/batch; 77h:10m:39s remains)
INFO - root - 2017-12-07 13:26:01.353682: step 7030, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.632 sec/batch; 72h:10m:31s remains)
INFO - root - 2017-12-07 13:26:17.687022: step 7040, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.694 sec/batch; 74h:54m:43s remains)
INFO - root - 2017-12-07 13:26:33.985338: step 7050, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.617 sec/batch; 71h:29m:33s remains)
INFO - root - 2017-12-07 13:26:50.157628: step 7060, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.589 sec/batch; 70h:15m:29s remains)
INFO - root - 2017-12-07 13:27:06.489703: step 7070, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.674 sec/batch; 74h:00m:20s remains)
INFO - root - 2017-12-07 13:27:22.652520: step 7080, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.572 sec/batch; 69h:30m:21s remains)
INFO - root - 2017-12-07 13:27:38.694916: step 7090, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 1.633 sec/batch; 72h:11m:01s remains)
INFO - root - 2017-12-07 13:27:54.806367: step 7100, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.626 sec/batch; 71h:53m:49s remains)
2017-12-07 13:27:56.310266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3023114 -4.2799349 -4.2795568 -4.2894382 -4.2968636 -4.3039532 -4.3055758 -4.2925539 -4.2601366 -4.2244854 -4.2016349 -4.1987114 -4.2078419 -4.224287 -4.2464862][-4.3106866 -4.2902117 -4.2883406 -4.2951646 -4.3024154 -4.305553 -4.3035464 -4.2877445 -4.2552528 -4.2214289 -4.1989756 -4.197473 -4.2065744 -4.2193556 -4.2363229][-4.3131428 -4.2968698 -4.2948818 -4.2982073 -4.2993298 -4.3007708 -4.3002062 -4.2916379 -4.2708473 -4.2441187 -4.2238641 -4.2208862 -4.2270193 -4.2381415 -4.2487235][-4.3018641 -4.2912145 -4.29046 -4.2911844 -4.28886 -4.2875013 -4.2847309 -4.28095 -4.2782917 -4.2625833 -4.2474446 -4.2438064 -4.2505603 -4.2588048 -4.2634926][-4.287612 -4.2813263 -4.276104 -4.2697272 -4.2576451 -4.2389212 -4.2216687 -4.2206807 -4.2333188 -4.2371049 -4.2345357 -4.2348537 -4.24144 -4.2459855 -4.2461572][-4.2527418 -4.2421842 -4.2309322 -4.2171173 -4.1922712 -4.1497359 -4.1044931 -4.097868 -4.1311398 -4.1638136 -4.1788092 -4.1871891 -4.1979551 -4.203186 -4.2027917][-4.2047076 -4.1855989 -4.1648688 -4.1365438 -4.0853724 -4.0044837 -3.9166481 -3.8953276 -3.9619536 -4.0386982 -4.0822549 -4.1122546 -4.1401291 -4.1578441 -4.1683116][-4.1789403 -4.1496515 -4.1127195 -4.0646734 -3.9902327 -3.8780463 -3.7492061 -3.710499 -3.8141937 -3.9352069 -4.0132556 -4.0718384 -4.1245074 -4.1605577 -4.1794171][-4.207294 -4.1803465 -4.1453247 -4.1029844 -4.0449929 -3.963273 -3.8695014 -3.8465981 -3.9200428 -4.0113 -4.0768008 -4.1317143 -4.1802573 -4.2118411 -4.2213197][-4.2728872 -4.2635317 -4.245852 -4.2217212 -4.1917353 -4.1541939 -4.1099358 -4.0939541 -4.1164155 -4.1548376 -4.1892943 -4.2228971 -4.2498088 -4.2638168 -4.2578669][-4.3189149 -4.3195343 -4.3113112 -4.2934675 -4.2758927 -4.2596412 -4.241477 -4.2297459 -4.2301273 -4.2430992 -4.2605267 -4.2766495 -4.2883444 -4.2926373 -4.2770715][-4.3397045 -4.34335 -4.3331695 -4.3093238 -4.2910185 -4.2805176 -4.2760592 -4.2731614 -4.273953 -4.28079 -4.2856088 -4.2852311 -4.2838559 -4.2831087 -4.2674904][-4.3392935 -4.3389564 -4.3214183 -4.2870278 -4.2580442 -4.2434249 -4.2439528 -4.2528539 -4.263 -4.2703867 -4.2666211 -4.2561293 -4.2478828 -4.242661 -4.2344112][-4.3139548 -4.3068519 -4.2806935 -4.2373257 -4.1975279 -4.1746325 -4.1796865 -4.198441 -4.2171192 -4.2249079 -4.2181115 -4.2056756 -4.1933765 -4.1861539 -4.19352][-4.2809882 -4.2683406 -4.2400732 -4.1995964 -4.1625209 -4.1411319 -4.1468663 -4.1675291 -4.1862926 -4.19075 -4.181489 -4.1667414 -4.1525731 -4.1471663 -4.1663833]]...]
INFO - root - 2017-12-07 13:28:12.501064: step 7110, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.537 sec/batch; 67h:55m:55s remains)
INFO - root - 2017-12-07 13:28:28.813004: step 7120, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 1.522 sec/batch; 67h:17m:20s remains)
INFO - root - 2017-12-07 13:28:45.127110: step 7130, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.652 sec/batch; 72h:59m:52s remains)
INFO - root - 2017-12-07 13:29:01.343327: step 7140, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.577 sec/batch; 69h:41m:36s remains)
INFO - root - 2017-12-07 13:29:17.519139: step 7150, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.731 sec/batch; 76h:30m:33s remains)
INFO - root - 2017-12-07 13:29:33.750978: step 7160, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.550 sec/batch; 68h:29m:17s remains)
INFO - root - 2017-12-07 13:29:50.108653: step 7170, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 1.713 sec/batch; 75h:42m:03s remains)
INFO - root - 2017-12-07 13:30:06.407833: step 7180, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.646 sec/batch; 72h:43m:46s remains)
INFO - root - 2017-12-07 13:30:22.718192: step 7190, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.640 sec/batch; 72h:28m:20s remains)
INFO - root - 2017-12-07 13:30:38.671238: step 7200, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.589 sec/batch; 70h:11m:27s remains)
2017-12-07 13:30:40.115880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2360425 -4.22155 -4.2092843 -4.2086792 -4.2132039 -4.2175126 -4.2198672 -4.2089081 -4.2025185 -4.2150321 -4.2340984 -4.2545924 -4.2776928 -4.2881856 -4.27487][-4.2335548 -4.2154074 -4.206377 -4.2117 -4.2190194 -4.2192645 -4.20711 -4.1833544 -4.1714921 -4.1836519 -4.213058 -4.2473097 -4.2772069 -4.2841358 -4.2684321][-4.21311 -4.1936383 -4.1902122 -4.2088194 -4.233386 -4.2453151 -4.2307153 -4.1915069 -4.1577754 -4.1530852 -4.1778922 -4.2195449 -4.2524829 -4.2566891 -4.237359][-4.1706095 -4.1452856 -4.1450906 -4.1779823 -4.2245307 -4.2551126 -4.2511845 -4.2115865 -4.1607995 -4.1331167 -4.1440573 -4.1847925 -4.2211976 -4.2204614 -4.1912336][-4.1171665 -4.0825143 -4.0779824 -4.1258144 -4.1909523 -4.2317 -4.2361259 -4.2034764 -4.1472387 -4.1094375 -4.1175504 -4.1596985 -4.1943326 -4.1878366 -4.1429362][-4.082118 -4.0454268 -4.0312405 -4.0812135 -4.1491008 -4.1887112 -4.1901703 -4.160862 -4.1046915 -4.0687184 -4.0861416 -4.1408982 -4.1795006 -4.1692405 -4.110261][-4.1051712 -4.0842376 -4.0702639 -4.0996132 -4.1399221 -4.1586308 -4.1446214 -4.1139579 -4.0632877 -4.0300164 -4.055356 -4.1221242 -4.1701794 -4.1608253 -4.0965838][-4.1566844 -4.1629376 -4.1619587 -4.1757441 -4.1831775 -4.1704607 -4.134511 -4.0985503 -4.0564137 -4.0298619 -4.0571179 -4.1234412 -4.1769953 -4.1731524 -4.1121325][-4.1858573 -4.2130895 -4.2275796 -4.235497 -4.2291112 -4.2015853 -4.1557765 -4.1147909 -4.076592 -4.0596595 -4.0851641 -4.1411862 -4.1919041 -4.196311 -4.1488266][-4.19013 -4.2275844 -4.2518163 -4.2567868 -4.2497725 -4.2260423 -4.1818519 -4.1391811 -4.1037517 -4.0895586 -4.1065435 -4.1476121 -4.1905618 -4.2058086 -4.1796174][-4.1930985 -4.2326579 -4.2569556 -4.2605896 -4.2563071 -4.241991 -4.2031131 -4.1591754 -4.1241355 -4.1101623 -4.1239605 -4.1530542 -4.1839666 -4.20121 -4.1924219][-4.2048669 -4.239943 -4.258738 -4.2622976 -4.2605705 -4.2526422 -4.2223754 -4.1807909 -4.1468182 -4.1298409 -4.1389785 -4.1579695 -4.1729355 -4.1829023 -4.183197][-4.2146878 -4.2430444 -4.25651 -4.2597508 -4.25878 -4.2552633 -4.2366962 -4.2065129 -4.1791396 -4.1630239 -4.1631351 -4.1681561 -4.1681552 -4.1671586 -4.1706462][-4.2158546 -4.234756 -4.2422647 -4.2424884 -4.2405105 -4.243166 -4.2417216 -4.2269983 -4.2104716 -4.1976261 -4.1896496 -4.1816111 -4.1710072 -4.160696 -4.160964][-4.2099762 -4.2135816 -4.2107687 -4.2063961 -4.2063332 -4.2150435 -4.2266159 -4.2305584 -4.2329068 -4.2307463 -4.2178082 -4.1995349 -4.1807976 -4.1657672 -4.1590948]]...]
INFO - root - 2017-12-07 13:30:56.219018: step 7210, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.598 sec/batch; 70h:34m:47s remains)
INFO - root - 2017-12-07 13:31:12.631079: step 7220, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.712 sec/batch; 75h:36m:44s remains)
INFO - root - 2017-12-07 13:31:28.991764: step 7230, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.618 sec/batch; 71h:28m:04s remains)
INFO - root - 2017-12-07 13:31:45.381680: step 7240, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.586 sec/batch; 70h:03m:10s remains)
INFO - root - 2017-12-07 13:32:01.610445: step 7250, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.647 sec/batch; 72h:44m:46s remains)
INFO - root - 2017-12-07 13:32:17.725183: step 7260, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.613 sec/batch; 71h:13m:26s remains)
INFO - root - 2017-12-07 13:32:33.990082: step 7270, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.688 sec/batch; 74h:33m:16s remains)
INFO - root - 2017-12-07 13:32:50.276624: step 7280, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.579 sec/batch; 69h:43m:27s remains)
INFO - root - 2017-12-07 13:33:06.783178: step 7290, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.687 sec/batch; 74h:29m:58s remains)
INFO - root - 2017-12-07 13:33:22.848565: step 7300, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.582 sec/batch; 69h:51m:29s remains)
2017-12-07 13:33:24.316282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2767205 -4.2807384 -4.279387 -4.2739406 -4.2638364 -4.2530637 -4.2474184 -4.2504106 -4.2579021 -4.2644453 -4.2682571 -4.2704511 -4.2713628 -4.2717905 -4.273046][-4.2829428 -4.2920427 -4.2925816 -4.2818937 -4.2586117 -4.2314944 -4.215971 -4.2219939 -4.2431083 -4.2639732 -4.2764726 -4.282032 -4.2826176 -4.2800322 -4.2787685][-4.2832093 -4.2953229 -4.2943921 -4.2741566 -4.2306938 -4.1786561 -4.1478262 -4.1555138 -4.1940737 -4.2367578 -4.2670226 -4.2833238 -4.2882767 -4.2847905 -4.2807651][-4.2826262 -4.29679 -4.2939296 -4.2623477 -4.1935863 -4.1077495 -4.0508943 -4.0536523 -4.1096072 -4.1789751 -4.2353792 -4.2696176 -4.2844491 -4.283884 -4.2796392][-4.281054 -4.2965 -4.2920809 -4.2525682 -4.16428 -4.0464506 -3.9548235 -3.9381804 -4.0035377 -4.0988879 -4.1854706 -4.2430277 -4.2717423 -4.2781067 -4.2768292][-4.2805448 -4.2968311 -4.2911897 -4.2491083 -4.1521082 -4.0133538 -3.886481 -3.8369339 -3.8986425 -4.013114 -4.1280951 -4.209837 -4.2556167 -4.2711773 -4.274765][-4.2816 -4.2982802 -4.2930741 -4.252634 -4.1577864 -4.01431 -3.8688569 -3.7936387 -3.8433809 -3.9636052 -4.0918469 -4.1863952 -4.2413077 -4.2626815 -4.2703357][-4.2828646 -4.300488 -4.2991242 -4.2671361 -4.1885357 -4.0671287 -3.9401114 -3.8684669 -3.9006391 -3.9991515 -4.1095543 -4.1923242 -4.2389154 -4.2556896 -4.2619653][-4.2861047 -4.3039241 -4.3081069 -4.2897897 -4.2379212 -4.15751 -4.07156 -4.0208526 -4.0369763 -4.0979128 -4.170579 -4.2237725 -4.2497134 -4.2539253 -4.2540555][-4.2907243 -4.3068347 -4.3151646 -4.3098474 -4.2833033 -4.2399707 -4.1909952 -4.1599865 -4.1666341 -4.1978178 -4.2348847 -4.2595754 -4.2647824 -4.256124 -4.2499657][-4.2913089 -4.3051748 -4.3154173 -4.31887 -4.3093729 -4.2900081 -4.2655749 -4.2487016 -4.2513218 -4.2652259 -4.2799044 -4.2861633 -4.2789769 -4.2625136 -4.2525544][-4.2857647 -4.2988882 -4.3099775 -4.3174081 -4.3167415 -4.3102674 -4.2992387 -4.2895021 -4.28834 -4.2928672 -4.2973442 -4.2972145 -4.2876992 -4.2719183 -4.2619319][-4.2758517 -4.289279 -4.3003592 -4.3079133 -4.3099585 -4.3080492 -4.302072 -4.2953277 -4.2927675 -4.29392 -4.2957115 -4.2956686 -4.2902246 -4.2803221 -4.2733235][-4.2644539 -4.2785368 -4.2890606 -4.2952757 -4.2968154 -4.2951322 -4.2912941 -4.2870235 -4.2856178 -4.2875066 -4.2907228 -4.2936683 -4.2930779 -4.2886271 -4.2845573][-4.2566419 -4.2704325 -4.2810044 -4.2872262 -4.2894354 -4.2889819 -4.2863927 -4.2827516 -4.2814388 -4.2830682 -4.2864413 -4.2906666 -4.2929707 -4.2921195 -4.2902079]]...]
INFO - root - 2017-12-07 13:33:40.499253: step 7310, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.586 sec/batch; 70h:00m:49s remains)
INFO - root - 2017-12-07 13:33:56.866359: step 7320, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 1.647 sec/batch; 72h:43m:46s remains)
INFO - root - 2017-12-07 13:34:12.927081: step 7330, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.610 sec/batch; 71h:04m:11s remains)
INFO - root - 2017-12-07 13:34:29.077543: step 7340, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.518 sec/batch; 66h:59m:38s remains)
INFO - root - 2017-12-07 13:34:45.234449: step 7350, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.637 sec/batch; 72h:14m:37s remains)
INFO - root - 2017-12-07 13:35:01.447111: step 7360, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.599 sec/batch; 70h:35m:15s remains)
INFO - root - 2017-12-07 13:35:17.855409: step 7370, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.575 sec/batch; 69h:29m:24s remains)
INFO - root - 2017-12-07 13:35:34.049528: step 7380, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 1.676 sec/batch; 73h:56m:40s remains)
INFO - root - 2017-12-07 13:35:49.973957: step 7390, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.586 sec/batch; 69h:58m:11s remains)
INFO - root - 2017-12-07 13:36:06.389614: step 7400, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 1.690 sec/batch; 74h:33m:10s remains)
2017-12-07 13:36:07.782288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1401958 -4.1481643 -4.14301 -4.150703 -4.1804924 -4.1959577 -4.1936274 -4.18722 -4.17408 -4.177146 -4.1835203 -4.1949568 -4.2043252 -4.1975269 -4.1897435][-4.1299133 -4.1368365 -4.1414723 -4.1577911 -4.1829939 -4.1966915 -4.2016721 -4.2027273 -4.1951041 -4.1954961 -4.19486 -4.1979184 -4.1989865 -4.19046 -4.1867452][-4.1306295 -4.1385393 -4.1538477 -4.172226 -4.1849732 -4.1815343 -4.17999 -4.1831589 -4.1769571 -4.1723905 -4.1645236 -4.1621809 -4.1622953 -4.1590953 -4.1643314][-4.1532311 -4.1580524 -4.1701841 -4.1771917 -4.1695056 -4.1453815 -4.1316409 -4.1416273 -4.1513867 -4.1499634 -4.1348267 -4.1208019 -4.1155071 -4.1168323 -4.132494][-4.1776896 -4.1782231 -4.1788931 -4.1665158 -4.1366215 -4.0933795 -4.0688677 -4.0883317 -4.1166658 -4.1217484 -4.1056466 -4.08861 -4.0781379 -4.0824656 -4.1068296][-4.1845341 -4.1869426 -4.1829891 -4.1559567 -4.1044912 -4.029283 -3.9718261 -4.0001869 -4.0568309 -4.082099 -4.0812197 -4.0715508 -4.0603938 -4.0638466 -4.0886993][-4.1976824 -4.1988831 -4.1874442 -4.1461678 -4.0693135 -3.952672 -3.8381674 -3.8680744 -3.9703093 -4.0322528 -4.0584307 -4.0622 -4.0523047 -4.0499539 -4.066843][-4.22581 -4.2231708 -4.2020345 -4.1513085 -4.06574 -3.9322126 -3.7837455 -3.798193 -3.9153495 -3.9975927 -4.0384679 -4.0464287 -4.0338287 -4.025012 -4.0350862][-4.233098 -4.2356234 -4.2184882 -4.1822257 -4.1227036 -4.0279284 -3.9140925 -3.8975332 -3.9623885 -4.0175676 -4.0464759 -4.0453653 -4.0291796 -4.0178161 -4.0224323][-4.2140765 -4.2237973 -4.2234149 -4.2126245 -4.1853333 -4.1260538 -4.0495563 -4.0152793 -4.0327063 -4.0547905 -4.0714488 -4.0702906 -4.060318 -4.056252 -4.058002][-4.182025 -4.194222 -4.2098708 -4.2224607 -4.2216825 -4.1944466 -4.152051 -4.1168795 -4.1081452 -4.1059589 -4.1071348 -4.1021872 -4.0979171 -4.10003 -4.102128][-4.1585155 -4.1653938 -4.1872587 -4.2107224 -4.2297153 -4.2298465 -4.2198772 -4.2022929 -4.1889968 -4.1745896 -4.16208 -4.1516905 -4.147017 -4.1486692 -4.1506538][-4.1565647 -4.1539941 -4.1785107 -4.2030926 -4.2248807 -4.2351213 -4.2389045 -4.2354007 -4.2276497 -4.2178354 -4.2116003 -4.2087269 -4.2083454 -4.2081771 -4.2077079][-4.1660972 -4.1585321 -4.1806087 -4.2009888 -4.214016 -4.2240844 -4.2326226 -4.2360358 -4.2327065 -4.2297082 -4.2341919 -4.2444015 -4.2537928 -4.25504 -4.251864][-4.1882849 -4.1775746 -4.1879883 -4.1948724 -4.1926527 -4.2007232 -4.2153411 -4.2270679 -4.2285004 -4.2281651 -4.2347674 -4.2491183 -4.2639723 -4.272099 -4.2743721]]...]
INFO - root - 2017-12-07 13:36:24.104483: step 7410, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.697 sec/batch; 74h:52m:33s remains)
INFO - root - 2017-12-07 13:36:40.371475: step 7420, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.575 sec/batch; 69h:28m:11s remains)
INFO - root - 2017-12-07 13:36:56.425716: step 7430, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 1.560 sec/batch; 68h:48m:22s remains)
INFO - root - 2017-12-07 13:37:12.635052: step 7440, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.571 sec/batch; 69h:17m:03s remains)
INFO - root - 2017-12-07 13:37:28.804605: step 7450, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.644 sec/batch; 72h:30m:18s remains)
INFO - root - 2017-12-07 13:37:44.836726: step 7460, loss = 2.06, batch loss = 2.01 (10.4 examples/sec; 1.532 sec/batch; 67h:34m:29s remains)
INFO - root - 2017-12-07 13:38:01.122006: step 7470, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.541 sec/batch; 67h:57m:24s remains)
INFO - root - 2017-12-07 13:38:17.060739: step 7480, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.624 sec/batch; 71h:38m:38s remains)
INFO - root - 2017-12-07 13:38:33.230843: step 7490, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.585 sec/batch; 69h:53m:01s remains)
INFO - root - 2017-12-07 13:38:49.564045: step 7500, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.720 sec/batch; 75h:50m:36s remains)
2017-12-07 13:38:50.896900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2657204 -4.2060719 -4.136528 -4.0874944 -4.0952039 -4.1463084 -4.2019453 -4.2461672 -4.2711725 -4.2773242 -4.275135 -4.2748671 -4.2766361 -4.2779832 -4.2785053][-4.3007684 -4.25852 -4.1970062 -4.1287556 -4.0850253 -4.1028171 -4.1625376 -4.2193255 -4.252667 -4.2642341 -4.2683544 -4.2736559 -4.2785964 -4.28099 -4.2819328][-4.3208647 -4.295341 -4.2507958 -4.1913466 -4.1210895 -4.0881858 -4.1248198 -4.183291 -4.2188616 -4.2316794 -4.2407146 -4.2509379 -4.2612252 -4.2692666 -4.2754869][-4.3288007 -4.3194394 -4.2880936 -4.246727 -4.1788836 -4.1107874 -4.0967331 -4.1283569 -4.1556225 -4.1650453 -4.1850972 -4.2094631 -4.2318482 -4.2479744 -4.2633042][-4.3153925 -4.3247547 -4.3050413 -4.2759833 -4.2203455 -4.141644 -4.0826612 -4.0515161 -4.0454154 -4.0512953 -4.0948119 -4.1487179 -4.1925015 -4.2231083 -4.2493548][-4.2822433 -4.3040633 -4.2974033 -4.2780933 -4.23112 -4.1528096 -4.0672021 -3.96494 -3.8824174 -3.8833122 -3.9768834 -4.0761309 -4.1501803 -4.1991773 -4.236927][-4.2399654 -4.2625356 -4.2676315 -4.2635832 -4.2315583 -4.1593122 -4.0609131 -3.9114792 -3.7472205 -3.7423272 -3.8977222 -4.0430346 -4.1366544 -4.1923742 -4.2333794][-4.1916571 -4.2151427 -4.2317266 -4.242918 -4.231173 -4.1784334 -4.1018882 -3.9736292 -3.82885 -3.8126943 -3.9442589 -4.0717282 -4.151361 -4.19943 -4.2330403][-4.1497808 -4.1721506 -4.1956053 -4.2222362 -4.2299767 -4.2068272 -4.1694436 -4.1011138 -4.0295811 -4.0114937 -4.0627956 -4.1192026 -4.1607995 -4.1986322 -4.2319474][-4.1243005 -4.14359 -4.1728854 -4.2060456 -4.2278113 -4.2338734 -4.2344823 -4.2163105 -4.1933684 -4.1738462 -4.1623058 -4.1516838 -4.1546278 -4.183022 -4.2179103][-4.109755 -4.1207356 -4.1564789 -4.19255 -4.2217722 -4.2488642 -4.2766361 -4.2878351 -4.2848506 -4.2630873 -4.2247443 -4.17696 -4.14959 -4.1609921 -4.1890697][-4.1102753 -4.1120882 -4.1441278 -4.1805372 -4.2160277 -4.2485008 -4.2786245 -4.3019791 -4.3078356 -4.2916389 -4.2542906 -4.2027636 -4.1557393 -4.1381364 -4.1478729][-4.1313734 -4.126389 -4.146924 -4.1739821 -4.2075624 -4.2346559 -4.2579303 -4.2831655 -4.2968836 -4.2930841 -4.2667375 -4.2254343 -4.1752915 -4.1299329 -4.1051641][-4.1744571 -4.165926 -4.1676264 -4.1707916 -4.1899557 -4.2105994 -4.2307596 -4.2549176 -4.2763739 -4.2861981 -4.2752266 -4.2462807 -4.1996431 -4.1374693 -4.0734754][-4.2130952 -4.1974559 -4.1847448 -4.1657081 -4.1604633 -4.1736631 -4.1980815 -4.2228022 -4.2476659 -4.27385 -4.2799792 -4.2613673 -4.2232542 -4.1618223 -4.0825028]]...]
INFO - root - 2017-12-07 13:39:06.967828: step 7510, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.644 sec/batch; 72h:29m:27s remains)
INFO - root - 2017-12-07 13:39:23.230897: step 7520, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.574 sec/batch; 69h:23m:50s remains)
INFO - root - 2017-12-07 13:39:39.422494: step 7530, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.671 sec/batch; 73h:41m:02s remains)
INFO - root - 2017-12-07 13:39:55.739541: step 7540, loss = 2.11, batch loss = 2.05 (10.2 examples/sec; 1.571 sec/batch; 69h:14m:54s remains)
INFO - root - 2017-12-07 13:40:12.177975: step 7550, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.660 sec/batch; 73h:11m:47s remains)
INFO - root - 2017-12-07 13:40:28.456743: step 7560, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.589 sec/batch; 70h:02m:09s remains)
INFO - root - 2017-12-07 13:40:44.692512: step 7570, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.678 sec/batch; 73h:58m:17s remains)
INFO - root - 2017-12-07 13:41:00.979960: step 7580, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.611 sec/batch; 71h:00m:49s remains)
INFO - root - 2017-12-07 13:41:17.465805: step 7590, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.658 sec/batch; 73h:04m:53s remains)
INFO - root - 2017-12-07 13:41:33.526630: step 7600, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.562 sec/batch; 68h:49m:46s remains)
2017-12-07 13:41:34.950980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3358946 -4.3303509 -4.3159423 -4.2932935 -4.2784534 -4.2815757 -4.2928529 -4.3067837 -4.3147683 -4.310514 -4.2984629 -4.2866817 -4.2911592 -4.2964668 -4.2928791][-4.3247294 -4.3152437 -4.2934456 -4.26106 -4.2351184 -4.2335916 -4.2468529 -4.272387 -4.293252 -4.2936678 -4.27661 -4.2589555 -4.2661929 -4.2749 -4.2655358][-4.3175278 -4.3049283 -4.2789664 -4.2409039 -4.2044158 -4.191009 -4.1972408 -4.2314043 -4.2646451 -4.2727842 -4.25548 -4.2340093 -4.241293 -4.2488623 -4.2293644][-4.3098059 -4.2934833 -4.2654762 -4.2235012 -4.1751833 -4.1453109 -4.1406984 -4.1802449 -4.224669 -4.2429771 -4.2289 -4.2095337 -4.2172136 -4.2202544 -4.191288][-4.3019934 -4.28223 -4.2503004 -4.204711 -4.1460814 -4.0965662 -4.0786858 -4.11803 -4.1685219 -4.1978431 -4.1994925 -4.190608 -4.2001586 -4.2060986 -4.1736622][-4.2957621 -4.2752166 -4.2424059 -4.195765 -4.126657 -4.0599742 -4.0284824 -4.0603008 -4.1040397 -4.1375756 -4.163672 -4.1765943 -4.1892066 -4.20116 -4.1770034][-4.2910218 -4.2724824 -4.2445364 -4.2031059 -4.1319294 -4.0526762 -4.008626 -4.0266294 -4.0529513 -4.077198 -4.1219745 -4.1587362 -4.1817193 -4.2017069 -4.1929393][-4.2818518 -4.2658768 -4.2449574 -4.2150354 -4.1529875 -4.0708632 -4.0181851 -4.0246253 -4.0405459 -4.0548458 -4.0958495 -4.1380444 -4.1681852 -4.1962504 -4.2015877][-4.2697506 -4.255403 -4.2415433 -4.2226114 -4.1699529 -4.0915 -4.0357327 -4.02762 -4.0434456 -4.0676165 -4.1002221 -4.1274657 -4.1488981 -4.1786876 -4.1899495][-4.2586021 -4.2476659 -4.2390103 -4.2281957 -4.1794338 -4.1026492 -4.0508003 -4.0297451 -4.0377564 -4.0770826 -4.1146164 -4.1264563 -4.1333165 -4.1559792 -4.1666341][-4.246243 -4.2363482 -4.2319512 -4.2279491 -4.1853371 -4.1166444 -4.0752907 -4.0526891 -4.0533257 -4.0937977 -4.1326919 -4.1397104 -4.1351628 -4.14447 -4.1461482][-4.2338848 -4.2244773 -4.2214365 -4.2182617 -4.186542 -4.1402836 -4.1104321 -4.0879941 -4.0894814 -4.1190791 -4.1462421 -4.1533227 -4.1455765 -4.1420379 -4.1328092][-4.2285156 -4.2200265 -4.2175069 -4.2158461 -4.1988111 -4.175509 -4.1524563 -4.131216 -4.1362915 -4.1524272 -4.16092 -4.1672826 -4.1636419 -4.1500506 -4.1377811][-4.2369471 -4.2335939 -4.2356024 -4.2351928 -4.2275882 -4.2187867 -4.202528 -4.1856241 -4.1889877 -4.1927843 -4.1873169 -4.1929121 -4.1937761 -4.1790109 -4.1733108][-4.2541642 -4.2593179 -4.2642865 -4.2639065 -4.2598386 -4.2554049 -4.2459831 -4.2349143 -4.2325497 -4.2266488 -4.2177238 -4.2231212 -4.2237315 -4.212821 -4.21829]]...]
INFO - root - 2017-12-07 13:41:50.836738: step 7610, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.595 sec/batch; 70h:18m:07s remains)
INFO - root - 2017-12-07 13:42:06.919132: step 7620, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.571 sec/batch; 69h:13m:03s remains)
INFO - root - 2017-12-07 13:42:23.254938: step 7630, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.676 sec/batch; 73h:49m:47s remains)
INFO - root - 2017-12-07 13:42:39.317903: step 7640, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.591 sec/batch; 70h:05m:52s remains)
INFO - root - 2017-12-07 13:42:55.478549: step 7650, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.641 sec/batch; 72h:18m:09s remains)
INFO - root - 2017-12-07 13:43:11.798963: step 7660, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.571 sec/batch; 69h:11m:52s remains)
INFO - root - 2017-12-07 13:43:28.153073: step 7670, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.613 sec/batch; 71h:01m:52s remains)
INFO - root - 2017-12-07 13:43:44.485375: step 7680, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.661 sec/batch; 73h:10m:36s remains)
INFO - root - 2017-12-07 13:44:00.605650: step 7690, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.627 sec/batch; 71h:38m:59s remains)
INFO - root - 2017-12-07 13:44:16.872002: step 7700, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.671 sec/batch; 73h:35m:12s remains)
2017-12-07 13:44:18.203308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2533422 -4.2499981 -4.25775 -4.26744 -4.2719727 -4.268383 -4.2637773 -4.2620845 -4.264257 -4.269104 -4.2732487 -4.2756681 -4.2770281 -4.2839603 -4.2924862][-4.2415509 -4.2346382 -4.2445521 -4.2577276 -4.2630806 -4.255229 -4.2468052 -4.2458134 -4.2520862 -4.2589474 -4.2610321 -4.2594023 -4.2599192 -4.268743 -4.2778554][-4.225141 -4.2121034 -4.2205486 -4.2344007 -4.2387996 -4.2247849 -4.2101836 -4.21326 -4.2278008 -4.2388606 -4.2402444 -4.233026 -4.2312074 -4.242939 -4.2554884][-4.2103209 -4.1918445 -4.1971316 -4.2045908 -4.197628 -4.1709514 -4.1515164 -4.1624923 -4.1923485 -4.2162089 -4.2227564 -4.2104259 -4.2029586 -4.2136593 -4.2297153][-4.2025552 -4.1782675 -4.1747808 -4.1653423 -4.1312575 -4.07998 -4.0508928 -4.078289 -4.1411176 -4.1941857 -4.2152586 -4.2027197 -4.1906981 -4.196384 -4.2123804][-4.2055874 -4.1771903 -4.1578455 -4.1203861 -4.0489769 -3.9587219 -3.9075615 -3.9604082 -4.0732903 -4.16322 -4.2054086 -4.2019663 -4.19099 -4.1944194 -4.2073312][-4.2216997 -4.18858 -4.1499519 -4.08145 -3.9698944 -3.8268964 -3.7394028 -3.8274336 -4.0032396 -4.1300993 -4.1942835 -4.2054148 -4.2008543 -4.2059393 -4.2153831][-4.2370458 -4.2013087 -4.1506157 -4.0619974 -3.9214506 -3.7350264 -3.6132634 -3.7338808 -3.9560189 -4.1111012 -4.1877732 -4.2073917 -4.2097578 -4.2189012 -4.2261562][-4.24544 -4.2148957 -4.1715989 -4.090148 -3.9546835 -3.771806 -3.6543264 -3.7604663 -3.9683032 -4.1193724 -4.1906104 -4.2108464 -4.2145076 -4.225883 -4.2309651][-4.2520781 -4.2329388 -4.2080159 -4.1549487 -4.0580645 -3.92514 -3.8383498 -3.9014387 -4.0474367 -4.1582675 -4.2095284 -4.2234216 -4.2246537 -4.2350802 -4.2394309][-4.2551293 -4.2450805 -4.2367072 -4.212081 -4.1605554 -4.0813069 -4.0262856 -4.0596056 -4.1479025 -4.2119694 -4.2395635 -4.2448468 -4.2441573 -4.2537942 -4.2598062][-4.2564936 -4.2512255 -4.2504573 -4.2453327 -4.2274661 -4.1880789 -4.1558704 -4.1748657 -4.2260275 -4.2578173 -4.2662721 -4.2655435 -4.2668529 -4.2781973 -4.2851167][-4.2564173 -4.2558327 -4.2599077 -4.2637138 -4.2620916 -4.24505 -4.2265167 -4.2350254 -4.2623167 -4.2783813 -4.2803278 -4.2811613 -4.2856512 -4.2981243 -4.3054738][-4.257719 -4.2627287 -4.2705822 -4.276927 -4.277349 -4.2668824 -4.2538218 -4.2565274 -4.271873 -4.2837667 -4.2893724 -4.2938728 -4.2992034 -4.3095098 -4.3160286][-4.267612 -4.27556 -4.2852464 -4.2904563 -4.2893295 -4.2820287 -4.2721868 -4.2711954 -4.2790518 -4.2868786 -4.2927446 -4.2981992 -4.3042831 -4.3119 -4.3177919]]...]
INFO - root - 2017-12-07 13:44:34.497064: step 7710, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.645 sec/batch; 72h:27m:30s remains)
INFO - root - 2017-12-07 13:44:50.721505: step 7720, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 1.504 sec/batch; 66h:13m:32s remains)
INFO - root - 2017-12-07 13:45:07.029337: step 7730, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.624 sec/batch; 71h:31m:04s remains)
INFO - root - 2017-12-07 13:45:23.114807: step 7740, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 1.422 sec/batch; 62h:37m:29s remains)
INFO - root - 2017-12-07 13:45:39.494619: step 7750, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.658 sec/batch; 72h:59m:14s remains)
INFO - root - 2017-12-07 13:45:55.712068: step 7760, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.600 sec/batch; 70h:27m:29s remains)
INFO - root - 2017-12-07 13:46:12.102580: step 7770, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.692 sec/batch; 74h:27m:57s remains)
INFO - root - 2017-12-07 13:46:28.344092: step 7780, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 1.615 sec/batch; 71h:04m:54s remains)
INFO - root - 2017-12-07 13:46:44.692069: step 7790, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.591 sec/batch; 70h:02m:17s remains)
INFO - root - 2017-12-07 13:47:00.904463: step 7800, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.625 sec/batch; 71h:30m:31s remains)
2017-12-07 13:47:02.174957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2940297 -4.2875996 -4.2727137 -4.25649 -4.2382407 -4.2225924 -4.2110205 -4.2203836 -4.2455869 -4.2584329 -4.2621183 -4.2587948 -4.2541537 -4.2551274 -4.2517929][-4.3125043 -4.2963858 -4.2772059 -4.2553539 -4.2337513 -4.2169328 -4.2063146 -4.21936 -4.2553473 -4.2748456 -4.2806568 -4.2795167 -4.2752113 -4.2746043 -4.2726932][-4.3156085 -4.2938814 -4.2758546 -4.25772 -4.2318568 -4.2110481 -4.2035356 -4.2181458 -4.257864 -4.2802 -4.2829728 -4.2822008 -4.2781515 -4.2755623 -4.2772174][-4.3096328 -4.2849507 -4.2700977 -4.2549572 -4.2276893 -4.2036557 -4.1972265 -4.2105136 -4.2419634 -4.2597466 -4.2578444 -4.2584057 -4.2609844 -4.2610035 -4.2674069][-4.29985 -4.2807431 -4.2701378 -4.256072 -4.2280307 -4.2020874 -4.1871824 -4.1866403 -4.1998539 -4.2121043 -4.2085838 -4.2124453 -4.2236838 -4.2289028 -4.2377682][-4.2798734 -4.2638726 -4.2568159 -4.2439127 -4.2124686 -4.18155 -4.1520348 -4.1267095 -4.1218896 -4.1400409 -4.1507573 -4.1614118 -4.1771927 -4.1904559 -4.2049708][-4.2515368 -4.2341895 -4.2254162 -4.2080359 -4.1680055 -4.1208992 -4.0674586 -4.0188913 -4.0094447 -4.0488367 -4.0828376 -4.1077857 -4.1376719 -4.1663723 -4.1930251][-4.2334685 -4.2088766 -4.1911573 -4.1714721 -4.1308231 -4.0764265 -4.0093808 -3.9531577 -3.9481928 -4.0008283 -4.0503454 -4.0870323 -4.1288862 -4.1696286 -4.2025423][-4.2350864 -4.2075114 -4.1891608 -4.1797323 -4.1564302 -4.122035 -4.0769396 -4.0376053 -4.0311666 -4.0648551 -4.1009479 -4.1288962 -4.16191 -4.1969872 -4.2237978][-4.2448292 -4.2229137 -4.2079215 -4.2072148 -4.2004 -4.1892958 -4.1713848 -4.1496911 -4.1354361 -4.1451635 -4.162766 -4.1759996 -4.1953697 -4.2201314 -4.2372456][-4.2323093 -4.2218928 -4.2133679 -4.2188268 -4.2217731 -4.22033 -4.2141066 -4.2001109 -4.1829395 -4.180769 -4.1899409 -4.1930475 -4.1974058 -4.2128472 -4.2266431][-4.2118359 -4.205543 -4.2025814 -4.2133365 -4.2210512 -4.2229667 -4.2213535 -4.2141428 -4.2009773 -4.1952553 -4.201376 -4.2015 -4.1972628 -4.2055421 -4.2168274][-4.2171087 -4.2116637 -4.2106614 -4.2224197 -4.2319622 -4.2335825 -4.2347112 -4.2328377 -4.2237105 -4.2175345 -4.2218814 -4.223856 -4.2185831 -4.2230239 -4.22811][-4.2585759 -4.2541494 -4.2514668 -4.2581015 -4.2683558 -4.2699256 -4.2728457 -4.2726603 -4.2640457 -4.255177 -4.25749 -4.2611103 -4.25882 -4.2621984 -4.2615752][-4.307826 -4.3024006 -4.296598 -4.29775 -4.3055224 -4.3082743 -4.3099551 -4.3087978 -4.3009253 -4.2943196 -4.2958531 -4.2993035 -4.2977529 -4.2987871 -4.2958717]]...]
INFO - root - 2017-12-07 13:47:18.534418: step 7810, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.624 sec/batch; 71h:28m:58s remains)
INFO - root - 2017-12-07 13:47:34.923520: step 7820, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.588 sec/batch; 69h:53m:43s remains)
INFO - root - 2017-12-07 13:47:51.075724: step 7830, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.623 sec/batch; 71h:25m:00s remains)
INFO - root - 2017-12-07 13:48:07.308007: step 7840, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.603 sec/batch; 70h:33m:21s remains)
INFO - root - 2017-12-07 13:48:23.823175: step 7850, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.700 sec/batch; 74h:49m:14s remains)
INFO - root - 2017-12-07 13:48:40.034989: step 7860, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 1.433 sec/batch; 63h:04m:00s remains)
INFO - root - 2017-12-07 13:48:56.500095: step 7870, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.682 sec/batch; 74h:00m:18s remains)
INFO - root - 2017-12-07 13:49:12.890406: step 7880, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.595 sec/batch; 70h:10m:19s remains)
INFO - root - 2017-12-07 13:49:29.161188: step 7890, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.562 sec/batch; 68h:41m:40s remains)
INFO - root - 2017-12-07 13:49:45.160227: step 7900, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.718 sec/batch; 75h:34m:24s remains)
2017-12-07 13:49:46.504964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2303538 -4.2478595 -4.2441239 -4.2247043 -4.1941681 -4.1640754 -4.1459703 -4.1334639 -4.1437526 -4.1676726 -4.196279 -4.2114077 -4.2059546 -4.1987143 -4.2030611][-4.2468486 -4.2517052 -4.2415428 -4.21568 -4.180922 -4.154686 -4.1381221 -4.1299224 -4.1362576 -4.1502891 -4.1819859 -4.2068663 -4.2041316 -4.2001119 -4.2104445][-4.2509079 -4.2485652 -4.2336569 -4.1998544 -4.1649318 -4.14467 -4.136241 -4.1372886 -4.1438794 -4.1511149 -4.1805687 -4.2073584 -4.2079549 -4.2109575 -4.2251172][-4.254683 -4.2450032 -4.2262268 -4.1912208 -4.1603074 -4.1424284 -4.1379819 -4.1461616 -4.1528053 -4.1618114 -4.1926088 -4.2163353 -4.2160983 -4.2208362 -4.2289581][-4.2677093 -4.2521286 -4.2308121 -4.1992407 -4.1721778 -4.1514411 -4.1396761 -4.1478925 -4.1601758 -4.1753445 -4.2076159 -4.2259946 -4.2191663 -4.2133126 -4.2113581][-4.2730932 -4.2563529 -4.2360339 -4.2082744 -4.17758 -4.1483135 -4.12464 -4.1261292 -4.1432261 -4.16364 -4.194757 -4.2130432 -4.1998496 -4.1857038 -4.1836972][-4.2706308 -4.2523303 -4.2311506 -4.201118 -4.1608458 -4.1204138 -4.0797305 -4.0633984 -4.0817251 -4.1120253 -4.1495614 -4.1819263 -4.179142 -4.1706958 -4.176517][-4.2595525 -4.2364798 -4.2119012 -4.1765313 -4.1240444 -4.0580587 -3.9886866 -3.9568474 -3.9990733 -4.0643015 -4.1274376 -4.1817727 -4.1971622 -4.1947889 -4.1995864][-4.22401 -4.1957779 -4.1689014 -4.1341124 -4.0725641 -3.9868665 -3.9159231 -3.9213598 -4.012475 -4.1017194 -4.1719689 -4.2257929 -4.2438126 -4.2394357 -4.2332406][-4.1791792 -4.146297 -4.1227655 -4.0974131 -4.0556207 -4.0028486 -3.9802763 -4.0237775 -4.1120715 -4.1853223 -4.2333503 -4.265317 -4.2725182 -4.2581935 -4.2363734][-4.1458044 -4.1258125 -4.1209335 -4.118331 -4.1091442 -4.08607 -4.0842628 -4.1308022 -4.1979227 -4.2454581 -4.2712107 -4.28067 -4.2675786 -4.2395415 -4.2103391][-4.1451764 -4.1422682 -4.1507874 -4.1637731 -4.1671739 -4.1556449 -4.1552267 -4.191812 -4.2370019 -4.2614956 -4.2649984 -4.256794 -4.2365842 -4.2053304 -4.1769381][-4.1670737 -4.1680183 -4.174747 -4.1898422 -4.1919389 -4.1832232 -4.1849289 -4.2039795 -4.223073 -4.2328258 -4.2295537 -4.2174888 -4.2029457 -4.17975 -4.1582971][-4.1902661 -4.1881175 -4.1847558 -4.1931758 -4.1936646 -4.1874766 -4.1872568 -4.1860361 -4.1883497 -4.2003593 -4.1990833 -4.1905618 -4.1834345 -4.1692405 -4.157074][-4.2095222 -4.2093034 -4.205965 -4.2113943 -4.2115836 -4.20549 -4.1976962 -4.1808662 -4.1703477 -4.18621 -4.1921268 -4.1943893 -4.1978803 -4.1905875 -4.182981]]...]
INFO - root - 2017-12-07 13:50:02.838376: step 7910, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.638 sec/batch; 72h:03m:02s remains)
INFO - root - 2017-12-07 13:50:19.012830: step 7920, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.593 sec/batch; 70h:04m:09s remains)
INFO - root - 2017-12-07 13:50:35.330909: step 7930, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.709 sec/batch; 75h:10m:11s remains)
INFO - root - 2017-12-07 13:50:51.404581: step 7940, loss = 2.05, batch loss = 1.99 (10.2 examples/sec; 1.573 sec/batch; 69h:09m:41s remains)
INFO - root - 2017-12-07 13:51:07.752005: step 7950, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.567 sec/batch; 68h:54m:17s remains)
INFO - root - 2017-12-07 13:51:23.872463: step 7960, loss = 2.10, batch loss = 2.05 (9.4 examples/sec; 1.709 sec/batch; 75h:07m:33s remains)
INFO - root - 2017-12-07 13:51:40.080326: step 7970, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.595 sec/batch; 70h:07m:33s remains)
INFO - root - 2017-12-07 13:51:56.078740: step 7980, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.708 sec/batch; 75h:06m:31s remains)
INFO - root - 2017-12-07 13:52:12.053339: step 7990, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.576 sec/batch; 69h:15m:56s remains)
INFO - root - 2017-12-07 13:52:28.408656: step 8000, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.657 sec/batch; 72h:49m:31s remains)
2017-12-07 13:52:29.704526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2801867 -4.2261472 -4.1628489 -4.1284242 -4.1368303 -4.1681457 -4.2085471 -4.234179 -4.2239127 -4.1933737 -4.1694431 -4.1639581 -4.1717992 -4.1867523 -4.2033663][-4.2845459 -4.2493939 -4.2002687 -4.1685662 -4.1706076 -4.1902571 -4.2256756 -4.2571831 -4.2543297 -4.2246275 -4.1882377 -4.1673551 -4.164259 -4.179481 -4.2133074][-4.2931709 -4.278635 -4.2487063 -4.2169943 -4.20383 -4.2018924 -4.2198868 -4.2464294 -4.2511854 -4.2293839 -4.1937847 -4.1685781 -4.1596332 -4.1757274 -4.2198458][-4.2988634 -4.292439 -4.2753835 -4.2480774 -4.2273731 -4.207283 -4.2027664 -4.2188907 -4.2253733 -4.2157078 -4.1906924 -4.1660571 -4.1555934 -4.1715364 -4.217176][-4.3012652 -4.29454 -4.2830439 -4.26172 -4.2431717 -4.2100468 -4.1773133 -4.1760793 -4.1803355 -4.1791167 -4.1723938 -4.1623955 -4.1609087 -4.1758995 -4.2145739][-4.3012362 -4.2911625 -4.2775207 -4.2584376 -4.2382689 -4.1936526 -4.1316338 -4.11206 -4.1185179 -4.1297174 -4.1495738 -4.1693711 -4.1881986 -4.2042875 -4.2221661][-4.3010182 -4.2827587 -4.2635274 -4.23761 -4.2031012 -4.1403122 -4.0535846 -4.0252934 -4.0462523 -4.085741 -4.1384506 -4.18547 -4.217536 -4.2298489 -4.2203488][-4.3095374 -4.2787066 -4.2458649 -4.20258 -4.1533828 -4.0837483 -3.99446 -3.9710126 -4.006916 -4.0713611 -4.1476707 -4.2078557 -4.2360024 -4.2363033 -4.2064052][-4.3160839 -4.2782006 -4.2338676 -4.1794229 -4.131393 -4.0827737 -4.0258641 -4.0147648 -4.0481739 -4.1058993 -4.1758761 -4.2244897 -4.234705 -4.2221823 -4.1882434][-4.312367 -4.2723527 -4.2250371 -4.1760712 -4.1474094 -4.1321025 -4.111866 -4.113646 -4.136 -4.1683393 -4.2062016 -4.2255049 -4.2165866 -4.2010689 -4.180819][-4.3016219 -4.2644944 -4.2223487 -4.1880975 -4.182353 -4.1915584 -4.1956434 -4.2042642 -4.2127285 -4.2156358 -4.2177062 -4.2093873 -4.1957865 -4.1876397 -4.1866159][-4.2909217 -4.263175 -4.236742 -4.2210932 -4.2279072 -4.2464876 -4.2581124 -4.2618442 -4.2531819 -4.229238 -4.205636 -4.1865563 -4.1791768 -4.1848674 -4.2076445][-4.2844224 -4.2685957 -4.2574129 -4.2508793 -4.2554145 -4.2697105 -4.2791562 -4.2736392 -4.2521682 -4.2141328 -4.1785445 -4.1605554 -4.16674 -4.19498 -4.2392697][-4.2822704 -4.273088 -4.267941 -4.266561 -4.2686119 -4.2732062 -4.2707758 -4.2537475 -4.22406 -4.1860809 -4.1540518 -4.1425314 -4.1598487 -4.2079649 -4.2645726][-4.2879457 -4.2822208 -4.2785935 -4.2767186 -4.2758265 -4.2679977 -4.249444 -4.2213392 -4.1909938 -4.1649113 -4.1495185 -4.1465049 -4.1672006 -4.2194586 -4.2756448]]...]
INFO - root - 2017-12-07 13:52:45.893904: step 8010, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.677 sec/batch; 73h:43m:01s remains)
INFO - root - 2017-12-07 13:53:02.098416: step 8020, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.606 sec/batch; 70h:35m:01s remains)
INFO - root - 2017-12-07 13:53:18.425336: step 8030, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 1.675 sec/batch; 73h:35m:58s remains)
INFO - root - 2017-12-07 13:53:34.602907: step 8040, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.593 sec/batch; 70h:00m:03s remains)
INFO - root - 2017-12-07 13:53:50.748004: step 8050, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.692 sec/batch; 74h:20m:40s remains)
INFO - root - 2017-12-07 13:54:07.061267: step 8060, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 70h:22m:43s remains)
INFO - root - 2017-12-07 13:54:23.274665: step 8070, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.591 sec/batch; 69h:53m:45s remains)
INFO - root - 2017-12-07 13:54:39.741295: step 8080, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.693 sec/batch; 74h:23m:45s remains)
INFO - root - 2017-12-07 13:54:56.098777: step 8090, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.600 sec/batch; 70h:16m:21s remains)
INFO - root - 2017-12-07 13:55:12.292545: step 8100, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.698 sec/batch; 74h:36m:16s remains)
2017-12-07 13:55:13.637734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2287683 -4.2181091 -4.2034392 -4.1983051 -4.2118788 -4.2243977 -4.2298846 -4.2302628 -4.2310448 -4.2319479 -4.2255473 -4.2199316 -4.2096415 -4.2072611 -4.2167158][-4.229167 -4.2200956 -4.2012506 -4.1847358 -4.1982408 -4.2214546 -4.2313185 -4.2285743 -4.2258806 -4.2244644 -4.2186413 -4.2179408 -4.2139187 -4.2170711 -4.2298112][-4.2256503 -4.2227287 -4.2043552 -4.1834331 -4.1921568 -4.2148085 -4.2219663 -4.2154016 -4.2035971 -4.1980195 -4.1990619 -4.2062044 -4.2099056 -4.2176309 -4.2337041][-4.1969876 -4.2061019 -4.2036767 -4.1920052 -4.1969185 -4.2073011 -4.1986175 -4.1774554 -4.1633582 -4.1654468 -4.1796761 -4.1967411 -4.2065597 -4.2141147 -4.2291846][-4.138227 -4.1551695 -4.1773353 -4.187717 -4.1943951 -4.1854572 -4.1506805 -4.106576 -4.096725 -4.1295705 -4.1741734 -4.2080393 -4.2237754 -4.2292418 -4.2357645][-4.1032906 -4.1232457 -4.1631875 -4.1910663 -4.1942096 -4.157783 -4.0854769 -3.9997413 -3.9873936 -4.0649333 -4.1541586 -4.2123084 -4.2364826 -4.2399068 -4.2370095][-4.0991797 -4.1223712 -4.166935 -4.2010179 -4.2002268 -4.1466613 -4.0523195 -3.9373927 -3.9079435 -4.0127182 -4.1305985 -4.203198 -4.230226 -4.2318344 -4.2237582][-4.1009874 -4.130456 -4.1712494 -4.2027636 -4.1991282 -4.1486497 -4.0692773 -3.9781985 -3.9483862 -4.0196457 -4.1176128 -4.1802154 -4.2028093 -4.2040782 -4.1972528][-4.1029124 -4.129302 -4.1608 -4.183763 -4.1835876 -4.1517134 -4.106267 -4.059947 -4.0458694 -4.0764041 -4.1282444 -4.1657166 -4.177393 -4.1798596 -4.1774554][-4.1142945 -4.1256824 -4.139482 -4.1537266 -4.1595869 -4.1496463 -4.1363797 -4.1250367 -4.1295447 -4.1440158 -4.159955 -4.1693797 -4.170908 -4.1745086 -4.1760674][-4.1482511 -4.1418519 -4.133924 -4.1344337 -4.1445112 -4.1607757 -4.1751862 -4.1842132 -4.1964583 -4.2057457 -4.201592 -4.1914473 -4.188201 -4.1953335 -4.2007189][-4.2038097 -4.186687 -4.1658478 -4.1515322 -4.158679 -4.1909504 -4.2197952 -4.2349625 -4.2494597 -4.25446 -4.2398982 -4.2202926 -4.2158175 -4.2252703 -4.2344527][-4.2618551 -4.2468657 -4.2274303 -4.2089009 -4.2089653 -4.2352233 -4.2630696 -4.2785482 -4.2889762 -4.288239 -4.2720146 -4.2544723 -4.2508388 -4.2579651 -4.2667217][-4.3039513 -4.2937784 -4.2805924 -4.2697434 -4.2693334 -4.2830343 -4.3002791 -4.3098383 -4.3147039 -4.3123178 -4.3020873 -4.2929773 -4.290421 -4.2930465 -4.29766][-4.3297944 -4.3228617 -4.3149595 -4.3093562 -4.3096623 -4.3151035 -4.322248 -4.3269467 -4.3292766 -4.3292422 -4.32664 -4.3253088 -4.3231483 -4.3223696 -4.3235235]]...]
INFO - root - 2017-12-07 13:55:29.870056: step 8110, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.625 sec/batch; 71h:22m:57s remains)
INFO - root - 2017-12-07 13:55:46.165025: step 8120, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.557 sec/batch; 68h:22m:39s remains)
INFO - root - 2017-12-07 13:56:02.440449: step 8130, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.626 sec/batch; 71h:24m:34s remains)
INFO - root - 2017-12-07 13:56:18.799146: step 8140, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.606 sec/batch; 70h:30m:50s remains)
INFO - root - 2017-12-07 13:56:35.169304: step 8150, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.650 sec/batch; 72h:27m:19s remains)
INFO - root - 2017-12-07 13:56:51.406018: step 8160, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.583 sec/batch; 69h:30m:08s remains)
INFO - root - 2017-12-07 13:57:07.743342: step 8170, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.694 sec/batch; 74h:22m:01s remains)
INFO - root - 2017-12-07 13:57:23.985019: step 8180, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.657 sec/batch; 72h:46m:06s remains)
INFO - root - 2017-12-07 13:57:40.139122: step 8190, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.607 sec/batch; 70h:33m:26s remains)
INFO - root - 2017-12-07 13:57:56.549977: step 8200, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.633 sec/batch; 71h:42m:12s remains)
2017-12-07 13:57:57.833405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2218895 -4.2503366 -4.270977 -4.283833 -4.2926683 -4.2874212 -4.2780809 -4.2699366 -4.2634077 -4.2582946 -4.2510247 -4.2434611 -4.23839 -4.2209105 -4.1991687][-4.2227488 -4.2438149 -4.257803 -4.2609978 -4.2632394 -4.2526865 -4.2430325 -4.2368813 -4.2317824 -4.2297134 -4.2284865 -4.2318373 -4.2329183 -4.2133904 -4.1892457][-4.2286367 -4.2456241 -4.2545662 -4.2467046 -4.2355194 -4.2163897 -4.2032552 -4.2002244 -4.1994686 -4.19671 -4.2006712 -4.2166591 -4.228375 -4.2155709 -4.1902151][-4.2292466 -4.2422428 -4.2467012 -4.2316117 -4.2089806 -4.17943 -4.1633682 -4.1656384 -4.1675487 -4.169333 -4.1803236 -4.2036514 -4.2262282 -4.2245979 -4.200861][-4.2222934 -4.2344761 -4.2361226 -4.21874 -4.1870604 -4.1481123 -4.1232166 -4.1318369 -4.1508541 -4.16183 -4.1747823 -4.200192 -4.22723 -4.235343 -4.2172213][-4.2105112 -4.2253165 -4.2297368 -4.2155051 -4.1795888 -4.1303525 -4.0927162 -4.1024094 -4.1361938 -4.1587992 -4.17375 -4.1976328 -4.2250953 -4.2369876 -4.2271276][-4.1917343 -4.2116036 -4.2178268 -4.199523 -4.1608925 -4.1100416 -4.0594573 -4.058732 -4.1013036 -4.1342444 -4.1507154 -4.1690693 -4.1948538 -4.2127314 -4.2202568][-4.15849 -4.1851478 -4.1958184 -4.1779461 -4.1415577 -4.0932593 -4.0371504 -4.0179067 -4.0459137 -4.083663 -4.1047516 -4.1190658 -4.141037 -4.1609182 -4.1807132][-4.1174436 -4.154119 -4.1755338 -4.1669679 -4.1384349 -4.0955153 -4.0400691 -4.0018363 -4.005578 -4.0409827 -4.0684533 -4.0836425 -4.0983658 -4.1134381 -4.1336565][-4.1082211 -4.1452141 -4.1714916 -4.1728649 -4.1604085 -4.1311455 -4.0850072 -4.0387945 -4.0196176 -4.0439105 -4.0742865 -4.090776 -4.0996118 -4.1079268 -4.1217103][-4.1345167 -4.1603 -4.180696 -4.1851821 -4.1864896 -4.177949 -4.153707 -4.1163173 -4.0881619 -4.0971403 -4.1209559 -4.1344104 -4.1368461 -4.1380816 -4.1462445][-4.1915026 -4.2018585 -4.2112164 -4.2135105 -4.2202797 -4.2249804 -4.219512 -4.200088 -4.1812644 -4.1827641 -4.195744 -4.2054858 -4.2063155 -4.2044406 -4.2095647][-4.2525463 -4.2484646 -4.2474437 -4.2503686 -4.2581482 -4.2668128 -4.2685852 -4.2607226 -4.252593 -4.2529635 -4.2601266 -4.2690077 -4.2735481 -4.2744336 -4.2780728][-4.2998443 -4.2894173 -4.2807612 -4.2821379 -4.291388 -4.3006244 -4.3045983 -4.3011274 -4.298727 -4.3000779 -4.3046446 -4.310607 -4.3158193 -4.3173666 -4.3185949][-4.3200755 -4.3129663 -4.3040614 -4.302845 -4.3105521 -4.3193665 -4.3238754 -4.3227453 -4.3220325 -4.3230577 -4.3249235 -4.3278465 -4.3303838 -4.3297381 -4.3289437]]...]
INFO - root - 2017-12-07 13:58:14.360482: step 8210, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 1.764 sec/batch; 77h:25m:17s remains)
INFO - root - 2017-12-07 13:58:30.472204: step 8220, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.600 sec/batch; 70h:14m:21s remains)
INFO - root - 2017-12-07 13:58:46.738233: step 8230, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.663 sec/batch; 72h:59m:13s remains)
INFO - root - 2017-12-07 13:59:03.046636: step 8240, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 69h:17m:13s remains)
INFO - root - 2017-12-07 13:59:19.278061: step 8250, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.541 sec/batch; 67h:36m:59s remains)
INFO - root - 2017-12-07 13:59:35.518679: step 8260, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.680 sec/batch; 73h:43m:06s remains)
INFO - root - 2017-12-07 13:59:51.769436: step 8270, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 1.568 sec/batch; 68h:48m:35s remains)
INFO - root - 2017-12-07 14:00:08.148892: step 8280, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 1.732 sec/batch; 76h:00m:30s remains)
INFO - root - 2017-12-07 14:00:24.359852: step 8290, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.538 sec/batch; 67h:28m:23s remains)
INFO - root - 2017-12-07 14:00:40.584129: step 8300, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 1.753 sec/batch; 76h:54m:20s remains)
2017-12-07 14:00:41.969578: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1901922 -4.186327 -4.1878548 -4.1937413 -4.2005916 -4.2171865 -4.2281165 -4.2216334 -4.201653 -4.1778159 -4.1640182 -4.1691475 -4.18967 -4.2153988 -4.2396331][-4.1747408 -4.1697989 -4.1691828 -4.1741757 -4.18308 -4.2014718 -4.2138934 -4.2103453 -4.1894822 -4.1659074 -4.1570468 -4.1664214 -4.1909113 -4.221365 -4.2516389][-4.1708207 -4.1616197 -4.154695 -4.1490054 -4.1469183 -4.1608748 -4.1787314 -4.183557 -4.1707072 -4.1522923 -4.149827 -4.1661406 -4.1926637 -4.2224479 -4.2539635][-4.1875453 -4.1771727 -4.1625729 -4.1432595 -4.119885 -4.1155772 -4.1280465 -4.1359992 -4.129559 -4.1193447 -4.1276822 -4.1555767 -4.1849084 -4.2135367 -4.240818][-4.2095189 -4.1960144 -4.1747365 -4.1423097 -4.0974336 -4.0680642 -4.0670414 -4.0708275 -4.0631614 -4.0558577 -4.0750122 -4.1191792 -4.1541328 -4.1822705 -4.205905][-4.222538 -4.2050776 -4.180654 -4.1450372 -4.0902095 -4.0388527 -4.0162954 -4.008533 -3.99892 -3.9884131 -4.0167413 -4.078496 -4.12219 -4.1475911 -4.16778][-4.2199373 -4.2051063 -4.1902876 -4.1712465 -4.130744 -4.084444 -4.0524631 -4.0317097 -4.0153422 -3.994611 -4.0177741 -4.0770764 -4.1175756 -4.1299062 -4.1380677][-4.2078123 -4.1935482 -4.1859083 -4.1869025 -4.1721983 -4.14702 -4.1244826 -4.1039381 -4.0801792 -4.045857 -4.0555191 -4.0988679 -4.1303577 -4.130434 -4.1260061][-4.2025809 -4.1819749 -4.1732612 -4.1921649 -4.2025404 -4.1981316 -4.1853037 -4.1679997 -4.1444926 -4.1060338 -4.10472 -4.1338139 -4.1613455 -4.1620483 -4.1570768][-4.205183 -4.1809106 -4.173749 -4.2005053 -4.2244577 -4.2325859 -4.2235689 -4.2101336 -4.1934218 -4.1639471 -4.1610494 -4.1780028 -4.1989555 -4.2040954 -4.2061028][-4.2143674 -4.1960993 -4.1917686 -4.2136235 -4.2334876 -4.2397161 -4.2322078 -4.2237105 -4.2159333 -4.1983685 -4.1944351 -4.2031517 -4.2193756 -4.2315617 -4.2436934][-4.2234883 -4.21134 -4.2077494 -4.221271 -4.2327194 -4.2297788 -4.2168322 -4.2103815 -4.2093253 -4.201509 -4.19675 -4.2000818 -4.2173028 -4.2401981 -4.2613826][-4.2201667 -4.2136507 -4.209661 -4.215651 -4.2228546 -4.2119079 -4.1898975 -4.1805992 -4.1860986 -4.1861396 -4.1851039 -4.1867075 -4.2080793 -4.2426972 -4.2694597][-4.1983962 -4.1965566 -4.1935821 -4.1976457 -4.2060013 -4.1933537 -4.1701684 -4.1584392 -4.163754 -4.1673884 -4.1724257 -4.1772594 -4.2035551 -4.2500615 -4.2832613][-4.1816626 -4.1811104 -4.1818209 -4.1898289 -4.2004237 -4.1895556 -4.1688719 -4.1563759 -4.158155 -4.164434 -4.1737242 -4.1831751 -4.2115445 -4.2618742 -4.2945185]]...]
INFO - root - 2017-12-07 14:00:58.392127: step 8310, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.677 sec/batch; 73h:33m:17s remains)
INFO - root - 2017-12-07 14:01:14.591102: step 8320, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.599 sec/batch; 70h:07m:39s remains)
INFO - root - 2017-12-07 14:01:30.885274: step 8330, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 1.736 sec/batch; 76h:09m:45s remains)
INFO - root - 2017-12-07 14:01:46.983632: step 8340, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.582 sec/batch; 69h:22m:33s remains)
INFO - root - 2017-12-07 14:02:03.390240: step 8350, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.682 sec/batch; 73h:47m:03s remains)
INFO - root - 2017-12-07 14:02:19.512422: step 8360, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.575 sec/batch; 69h:04m:27s remains)
INFO - root - 2017-12-07 14:02:35.749572: step 8370, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.546 sec/batch; 67h:46m:48s remains)
INFO - root - 2017-12-07 14:02:51.920882: step 8380, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.648 sec/batch; 72h:17m:05s remains)
INFO - root - 2017-12-07 14:03:08.183054: step 8390, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.649 sec/batch; 72h:18m:30s remains)
INFO - root - 2017-12-07 14:03:24.456008: step 8400, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.741 sec/batch; 76h:19m:56s remains)
2017-12-07 14:03:25.805679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2067971 -4.1813107 -4.1559134 -4.1497874 -4.1685314 -4.1974106 -4.2168169 -4.216383 -4.2041316 -4.2151637 -4.2487721 -4.2723656 -4.26891 -4.2356844 -4.1958137][-4.2060418 -4.1917472 -4.1766939 -4.176487 -4.1885056 -4.2046885 -4.2090516 -4.1988463 -4.1807218 -4.1917768 -4.2243619 -4.2479229 -4.2437978 -4.2130041 -4.1803536][-4.208704 -4.20597 -4.2018518 -4.2095952 -4.2188458 -4.2228918 -4.2098322 -4.1840754 -4.157382 -4.1667004 -4.1996145 -4.2230639 -4.21959 -4.1952372 -4.1755629][-4.2239313 -4.2244697 -4.2262535 -4.2361994 -4.2416534 -4.2368464 -4.2113 -4.1742611 -4.1405525 -4.1505942 -4.1837292 -4.2095594 -4.2121038 -4.1970077 -4.1946144][-4.2428412 -4.2424622 -4.2432985 -4.2508264 -4.2527614 -4.2425117 -4.2097087 -4.1683187 -4.1372728 -4.1517425 -4.1852784 -4.2153869 -4.2266269 -4.2240949 -4.2364445][-4.2488084 -4.2422671 -4.2426429 -4.2548265 -4.2557764 -4.23948 -4.2022228 -4.16088 -4.1357875 -4.15563 -4.1914968 -4.224205 -4.2437692 -4.2530785 -4.272306][-4.2339149 -4.2206221 -4.2197886 -4.2390118 -4.2459869 -4.229198 -4.192318 -4.151803 -4.129673 -4.1525397 -4.1866755 -4.2199664 -4.2444534 -4.2631054 -4.2832918][-4.2147408 -4.1920066 -4.1840105 -4.21034 -4.2271767 -4.2119308 -4.1808333 -4.1466188 -4.1267281 -4.1452942 -4.1709495 -4.19971 -4.2224894 -4.2483406 -4.274786][-4.2016211 -4.1673131 -4.1495209 -4.178019 -4.2043085 -4.2015285 -4.1807728 -4.1574531 -4.1418419 -4.1492119 -4.1592436 -4.1736145 -4.1893125 -4.2185044 -4.2565718][-4.1893568 -4.1436396 -4.1162457 -4.1470613 -4.1864634 -4.1979451 -4.1900806 -4.1748457 -4.1609712 -4.1539164 -4.1466928 -4.1512232 -4.1618872 -4.1908493 -4.2350168][-4.1769028 -4.118258 -4.0793262 -4.1099772 -4.1666355 -4.1944795 -4.20231 -4.196136 -4.1851964 -4.1637964 -4.13689 -4.1321964 -4.1404181 -4.1649003 -4.2082634][-4.1773605 -4.1110678 -4.0665145 -4.099525 -4.163835 -4.1995821 -4.2218733 -4.2243171 -4.2106094 -4.1767368 -4.1313229 -4.1088471 -4.1114006 -4.1386771 -4.1910319][-4.1825323 -4.1145678 -4.0765347 -4.114325 -4.1777773 -4.2157822 -4.2426996 -4.2443194 -4.2248335 -4.1819477 -4.1265292 -4.0880804 -4.0865064 -4.1176553 -4.1791449][-4.1793003 -4.1130486 -4.0880051 -4.1336422 -4.1948552 -4.233428 -4.25688 -4.2539611 -4.2279558 -4.1812248 -4.1285782 -4.0866065 -4.079886 -4.1025348 -4.1613464][-4.1518145 -4.0942936 -4.08268 -4.134438 -4.1957688 -4.239666 -4.2624574 -4.254653 -4.2232456 -4.1740189 -4.1243877 -4.0846548 -4.068337 -4.081923 -4.1338906]]...]
INFO - root - 2017-12-07 14:03:41.967421: step 8410, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.662 sec/batch; 72h:52m:26s remains)
INFO - root - 2017-12-07 14:03:58.032441: step 8420, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.546 sec/batch; 67h:46m:51s remains)
INFO - root - 2017-12-07 14:04:14.270557: step 8430, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.667 sec/batch; 73h:04m:44s remains)
INFO - root - 2017-12-07 14:04:30.431765: step 8440, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.618 sec/batch; 70h:54m:49s remains)
INFO - root - 2017-12-07 14:04:46.811202: step 8450, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.664 sec/batch; 72h:55m:10s remains)
INFO - root - 2017-12-07 14:05:02.881468: step 8460, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.661 sec/batch; 72h:47m:25s remains)
INFO - root - 2017-12-07 14:05:19.217565: step 8470, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.592 sec/batch; 69h:45m:40s remains)
INFO - root - 2017-12-07 14:05:35.262804: step 8480, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.662 sec/batch; 72h:50m:15s remains)
INFO - root - 2017-12-07 14:05:51.518608: step 8490, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.570 sec/batch; 68h:47m:03s remains)
INFO - root - 2017-12-07 14:06:07.884470: step 8500, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.551 sec/batch; 67h:57m:38s remains)
2017-12-07 14:06:09.263847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3194489 -4.3197827 -4.3203726 -4.3223758 -4.3225713 -4.318851 -4.31155 -4.304862 -4.305274 -4.3130369 -4.3221159 -4.3292389 -4.3316779 -4.3295751 -4.3279276][-4.322628 -4.3220849 -4.3193827 -4.31776 -4.3127842 -4.3010955 -4.2840633 -4.2699447 -4.26989 -4.2833996 -4.302187 -4.3203325 -4.3317676 -4.3329296 -4.3308916][-4.3236241 -4.3206968 -4.3126111 -4.30347 -4.28845 -4.2637911 -4.2333026 -4.2107749 -4.2122374 -4.2359772 -4.2680569 -4.3006783 -4.3249569 -4.3328381 -4.3322086][-4.3199844 -4.3123751 -4.2958879 -4.2750793 -4.2461867 -4.2093086 -4.1685452 -4.1401405 -4.1433349 -4.1777482 -4.2222619 -4.2685037 -4.3073683 -4.32543 -4.3285594][-4.309185 -4.2950063 -4.2684035 -4.2329569 -4.1917458 -4.1482453 -4.1079707 -4.0845671 -4.0920024 -4.1304941 -4.1778088 -4.2311759 -4.2806358 -4.3087225 -4.3167958][-4.2950668 -4.2727404 -4.2333088 -4.1814446 -4.1267176 -4.0787163 -4.0441322 -4.0321174 -4.0452385 -4.0836229 -4.1277514 -4.18657 -4.2454815 -4.2826905 -4.2965322][-4.2846141 -4.2551117 -4.2041292 -4.136229 -4.0679259 -4.0159936 -3.9862182 -3.9828863 -4.00204 -4.039875 -4.0836673 -4.1508856 -4.2194858 -4.2629557 -4.2800827][-4.2833228 -4.2526178 -4.1989942 -4.1279454 -4.0539093 -3.9936497 -3.9583077 -3.95569 -3.9809611 -4.0263357 -4.07769 -4.1511536 -4.2225628 -4.2637897 -4.2778463][-4.2906704 -4.2655377 -4.2196126 -4.1590896 -4.0902581 -4.0229483 -3.9769831 -3.970505 -4.0027556 -4.0590491 -4.1204314 -4.1948624 -4.2575212 -4.287292 -4.2927957][-4.3023086 -4.2866259 -4.2539883 -4.2096257 -4.1508985 -4.0825748 -4.0305386 -4.0195074 -4.0526714 -4.1129003 -4.1802573 -4.2505126 -4.300251 -4.3176627 -4.31474][-4.3147984 -4.3087044 -4.2895827 -4.2601237 -4.2143064 -4.1549735 -4.1067367 -4.0933723 -4.1232433 -4.1814194 -4.2461843 -4.3040671 -4.3371267 -4.3437157 -4.3346314][-4.3246474 -4.3250113 -4.3176923 -4.3026443 -4.2734413 -4.2329793 -4.1990814 -4.1903272 -4.2147632 -4.2622485 -4.3110242 -4.3469949 -4.361177 -4.3569455 -4.3438072][-4.3298626 -4.3331022 -4.33291 -4.3294916 -4.3165531 -4.2968588 -4.2820764 -4.2810268 -4.2979155 -4.3268657 -4.3530059 -4.3670468 -4.3667994 -4.3554688 -4.3421493][-4.33089 -4.3336105 -4.3357558 -4.3370085 -4.3339219 -4.3269048 -4.3241124 -4.3279696 -4.3386941 -4.3527575 -4.3638215 -4.3662024 -4.3595767 -4.3468909 -4.3366413][-4.3303728 -4.3319573 -4.3339605 -4.3364406 -4.3380532 -4.337328 -4.3381543 -4.3419318 -4.347651 -4.3534455 -4.3573442 -4.3555284 -4.3484983 -4.3389564 -4.3325977]]...]
INFO - root - 2017-12-07 14:06:25.626308: step 8510, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.673 sec/batch; 73h:17m:35s remains)
INFO - root - 2017-12-07 14:06:41.664739: step 8520, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.629 sec/batch; 71h:22m:47s remains)
INFO - root - 2017-12-07 14:06:57.888214: step 8530, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 1.515 sec/batch; 66h:21m:39s remains)
INFO - root - 2017-12-07 14:07:14.283622: step 8540, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.694 sec/batch; 74h:13m:27s remains)
INFO - root - 2017-12-07 14:07:30.557766: step 8550, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.634 sec/batch; 71h:33m:40s remains)
INFO - root - 2017-12-07 14:07:46.855278: step 8560, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.676 sec/batch; 73h:23m:53s remains)
INFO - root - 2017-12-07 14:08:03.022863: step 8570, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.590 sec/batch; 69h:39m:15s remains)
INFO - root - 2017-12-07 14:08:19.156580: step 8580, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.685 sec/batch; 73h:48m:46s remains)
INFO - root - 2017-12-07 14:08:35.406372: step 8590, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.638 sec/batch; 71h:45m:03s remains)
INFO - root - 2017-12-07 14:08:51.735151: step 8600, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.634 sec/batch; 71h:32m:52s remains)
2017-12-07 14:08:53.063867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32548 -4.3262744 -4.3291354 -4.3330326 -4.3363152 -4.3378243 -4.3361387 -4.3244839 -4.3089361 -4.2997713 -4.2967362 -4.29196 -4.2919869 -4.2949181 -4.2918267][-4.3392935 -4.3389812 -4.3389978 -4.3389645 -4.3383341 -4.3360305 -4.3332734 -4.3265896 -4.321702 -4.3210263 -4.3209023 -4.3126912 -4.304368 -4.3019476 -4.2957215][-4.3392949 -4.3355684 -4.332058 -4.3277407 -4.3211751 -4.31258 -4.3068905 -4.3052421 -4.3106666 -4.3171811 -4.3209915 -4.3146996 -4.3060989 -4.3033452 -4.2966962][-4.3309507 -4.3236451 -4.3172464 -4.3051963 -4.2859349 -4.26465 -4.2502861 -4.249651 -4.2648125 -4.2818866 -4.2934971 -4.2952943 -4.2935467 -4.2953458 -4.2918763][-4.3152423 -4.3042746 -4.2924447 -4.2702842 -4.2351432 -4.1973114 -4.1695895 -4.1650543 -4.1852007 -4.2152314 -4.2405114 -4.2559772 -4.2670836 -4.2761707 -4.278204][-4.2916212 -4.2819533 -4.2680359 -4.2396374 -4.1957197 -4.1450624 -4.1001062 -4.0785403 -4.0928817 -4.134254 -4.1769991 -4.2065487 -4.2282505 -4.2444873 -4.2547684][-4.2421422 -4.23411 -4.2226677 -4.1979132 -4.155169 -4.0956826 -4.0274396 -3.9748583 -3.9719105 -4.0229235 -4.0914702 -4.1443954 -4.1794181 -4.200995 -4.217689][-4.1718831 -4.1645913 -4.1648951 -4.1599936 -4.1338577 -4.0797868 -3.9994853 -3.9144161 -3.8814178 -3.931407 -4.0219007 -4.0987434 -4.1461639 -4.1697845 -4.1855288][-4.1536303 -4.1425104 -4.1488552 -4.1636238 -4.163187 -4.1333485 -4.0716949 -3.9920006 -3.9428473 -3.9635088 -4.0315142 -4.09986 -4.1431956 -4.1610141 -4.170763][-4.1826239 -4.1656656 -4.1687288 -4.1896214 -4.2076364 -4.2047114 -4.1727595 -4.1195259 -4.0729785 -4.0632949 -4.0884695 -4.1263852 -4.1515388 -4.1629057 -4.1701655][-4.2113514 -4.1930094 -4.1905284 -4.2038679 -4.2240634 -4.2356977 -4.2253251 -4.1977906 -4.1663985 -4.146667 -4.1436472 -4.1540213 -4.1606326 -4.164691 -4.172627][-4.2220531 -4.2102404 -4.2053137 -4.2083225 -4.2234263 -4.2405496 -4.2434025 -4.23446 -4.2182984 -4.2003093 -4.1873488 -4.1816282 -4.1764245 -4.1758366 -4.1841555][-4.2075067 -4.2052574 -4.2016311 -4.1974034 -4.2071447 -4.2245493 -4.2309022 -4.2271256 -4.2210307 -4.2103696 -4.1987486 -4.1941948 -4.1919932 -4.1938729 -4.2015452][-4.1820507 -4.1872621 -4.1846204 -4.1794567 -4.1873555 -4.2031727 -4.2079 -4.2016115 -4.1971583 -4.1894555 -4.1834712 -4.1846848 -4.192688 -4.2008638 -4.2077227][-4.1534338 -4.1635938 -4.1641994 -4.1609864 -4.1641412 -4.1749654 -4.1787658 -4.1737113 -4.1680145 -4.1620913 -4.1638074 -4.1694837 -4.1820292 -4.1928988 -4.1994328]]...]
INFO - root - 2017-12-07 14:09:09.417081: step 8610, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 1.717 sec/batch; 75h:09m:57s remains)
INFO - root - 2017-12-07 14:09:25.681662: step 8620, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 1.577 sec/batch; 69h:03m:51s remains)
INFO - root - 2017-12-07 14:09:42.118996: step 8630, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.583 sec/batch; 69h:17m:54s remains)
INFO - root - 2017-12-07 14:09:58.088957: step 8640, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 68h:51m:55s remains)
INFO - root - 2017-12-07 14:10:14.279856: step 8650, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.624 sec/batch; 71h:04m:25s remains)
INFO - root - 2017-12-07 14:10:30.556560: step 8660, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.663 sec/batch; 72h:48m:47s remains)
INFO - root - 2017-12-07 14:10:46.781707: step 8670, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.574 sec/batch; 68h:55m:07s remains)
INFO - root - 2017-12-07 14:11:03.270115: step 8680, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.666 sec/batch; 72h:55m:32s remains)
INFO - root - 2017-12-07 14:11:19.523676: step 8690, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.555 sec/batch; 68h:03m:45s remains)
INFO - root - 2017-12-07 14:11:35.877895: step 8700, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.675 sec/batch; 73h:19m:34s remains)
2017-12-07 14:11:37.224765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.15767 -4.1780529 -4.21705 -4.2516055 -4.2705832 -4.2773538 -4.2637467 -4.2475243 -4.2487612 -4.2654161 -4.2909231 -4.3094783 -4.3166394 -4.3099341 -4.2995386][-4.2303948 -4.2356462 -4.253212 -4.2687359 -4.2749276 -4.2764916 -4.2634635 -4.250782 -4.259531 -4.2810588 -4.307487 -4.3258266 -4.3316908 -4.3223548 -4.3093896][-4.2891498 -4.2871423 -4.2881694 -4.2818828 -4.2682419 -4.2575021 -4.2464743 -4.2411575 -4.2597136 -4.285861 -4.3120971 -4.3290873 -4.33688 -4.3331265 -4.3208394][-4.31383 -4.3086042 -4.2996321 -4.2785482 -4.2450323 -4.2195458 -4.2092237 -4.2142496 -4.2433972 -4.2762556 -4.3046212 -4.3217683 -4.3332429 -4.3361225 -4.3274021][-4.3187418 -4.3089395 -4.287159 -4.246408 -4.1926031 -4.1460338 -4.1297731 -4.1459789 -4.1971068 -4.2492275 -4.2875795 -4.3089037 -4.3221164 -4.3284125 -4.3234186][-4.3004479 -4.2865276 -4.2564363 -4.1985526 -4.1209168 -4.0443673 -4.0043373 -4.0253482 -4.1127081 -4.2034597 -4.2630315 -4.2933407 -4.3080683 -4.3147144 -4.3097978][-4.2723875 -4.2593203 -4.2248044 -4.1615534 -4.0690875 -3.9571249 -3.8693228 -3.8763845 -4.0004082 -4.1340728 -4.2196465 -4.2650909 -4.2863092 -4.29427 -4.2856631][-4.2461967 -4.2356105 -4.2017217 -4.1451421 -4.0570865 -3.9335234 -3.8099756 -3.7863929 -3.9117293 -4.0598578 -4.159831 -4.2191324 -4.2551918 -4.2694187 -4.2590446][-4.2457757 -4.2361879 -4.2094221 -4.1667447 -4.0981441 -3.9969516 -3.89403 -3.8565295 -3.9233747 -4.0298538 -4.1183147 -4.1821747 -4.2277689 -4.2511721 -4.2459016][-4.2667508 -4.2600384 -4.2430348 -4.2154508 -4.1690459 -4.102891 -4.0373821 -4.0038328 -4.0188003 -4.0685439 -4.1254253 -4.1780415 -4.2225785 -4.2496839 -4.2546568][-4.295928 -4.2926488 -4.2836652 -4.2692895 -4.2418656 -4.2057915 -4.1683006 -4.142127 -4.1348081 -4.1474605 -4.1762724 -4.2093849 -4.2437277 -4.2693172 -4.2810793][-4.3197713 -4.3191352 -4.314064 -4.3083539 -4.2976394 -4.2823391 -4.2650242 -4.2504277 -4.2389503 -4.23504 -4.2435989 -4.2597342 -4.2812319 -4.2998571 -4.3117404][-4.3342977 -4.3359203 -4.3325558 -4.3316679 -4.3307853 -4.3287506 -4.3224773 -4.3161917 -4.3088708 -4.301435 -4.2980533 -4.3031082 -4.3168068 -4.3292317 -4.3367462][-4.3402958 -4.3450475 -4.3442149 -4.3440742 -4.3458796 -4.3471651 -4.3444715 -4.3400941 -4.33772 -4.3347573 -4.3296156 -4.33006 -4.3396111 -4.3483553 -4.3537226][-4.346993 -4.3519306 -4.3533106 -4.3548589 -4.3580718 -4.357728 -4.3528485 -4.3460293 -4.3449388 -4.3457565 -4.3444567 -4.3451881 -4.3509803 -4.3576536 -4.3633]]...]
INFO - root - 2017-12-07 14:11:53.463751: step 8710, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.646 sec/batch; 72h:01m:13s remains)
INFO - root - 2017-12-07 14:12:09.747791: step 8720, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.596 sec/batch; 69h:51m:31s remains)
INFO - root - 2017-12-07 14:12:26.137464: step 8730, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.748 sec/batch; 76h:28m:47s remains)
INFO - root - 2017-12-07 14:12:42.316124: step 8740, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.707 sec/batch; 74h:41m:18s remains)
INFO - root - 2017-12-07 14:12:58.673719: step 8750, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.625 sec/batch; 71h:04m:22s remains)
INFO - root - 2017-12-07 14:13:14.920142: step 8760, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.704 sec/batch; 74h:32m:51s remains)
INFO - root - 2017-12-07 14:13:31.090903: step 8770, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.646 sec/batch; 72h:01m:10s remains)
INFO - root - 2017-12-07 14:13:47.387731: step 8780, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.594 sec/batch; 69h:42m:15s remains)
INFO - root - 2017-12-07 14:14:03.804440: step 8790, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.708 sec/batch; 74h:42m:36s remains)
INFO - root - 2017-12-07 14:14:20.168480: step 8800, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.592 sec/batch; 69h:38m:33s remains)
2017-12-07 14:14:21.574677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.238955 -4.2463603 -4.2500758 -4.2473197 -4.243073 -4.2430897 -4.2467 -4.2365975 -4.209805 -4.1917777 -4.1965981 -4.2171245 -4.23429 -4.2398267 -4.2402039][-4.2258353 -4.2366004 -4.2481756 -4.2542434 -4.2510438 -4.2428236 -4.2370443 -4.2210941 -4.1932626 -4.178174 -4.1912394 -4.2162118 -4.2283673 -4.2272367 -4.2231321][-4.2338929 -4.2464409 -4.256743 -4.2617044 -4.2584023 -4.2455831 -4.2336617 -4.2176828 -4.1961727 -4.18863 -4.2046471 -4.2226748 -4.2211723 -4.2108727 -4.204515][-4.2487626 -4.2641926 -4.2712755 -4.2719245 -4.2692685 -4.25695 -4.2437739 -4.2295747 -4.2154889 -4.2139397 -4.2299633 -4.2360072 -4.2172656 -4.1924338 -4.17875][-4.2511649 -4.2717233 -4.281477 -4.2811894 -4.2765036 -4.2618623 -4.2426071 -4.2245841 -4.2127848 -4.2143373 -4.2306027 -4.2246704 -4.1877618 -4.1493692 -4.1290464][-4.2326083 -4.2618556 -4.27444 -4.270009 -4.2571731 -4.23199 -4.2001896 -4.16866 -4.1501417 -4.1575737 -4.1884589 -4.1851511 -4.1474142 -4.1079197 -4.0875072][-4.1891751 -4.2250075 -4.2412796 -4.2320108 -4.2095981 -4.1709828 -4.1223197 -4.0716834 -4.03895 -4.0571189 -4.13124 -4.1647849 -4.154242 -4.1373038 -4.129323][-4.153306 -4.1864729 -4.2013092 -4.1849961 -4.1554213 -4.1139197 -4.06383 -4.0077267 -3.967376 -3.9894438 -4.0961809 -4.1671991 -4.1903672 -4.1944451 -4.1907754][-4.1637368 -4.1825738 -4.1836581 -4.1578331 -4.130806 -4.1079183 -4.0855379 -4.0561256 -4.030272 -4.0385876 -4.1202712 -4.1861076 -4.2194004 -4.231596 -4.22412][-4.2094903 -4.2143941 -4.197084 -4.1615429 -4.1436 -4.1459002 -4.1558456 -4.1543803 -4.1474757 -4.1459713 -4.1847949 -4.2215576 -4.2423906 -4.2516565 -4.2419157][-4.2342563 -4.2343016 -4.2095108 -4.173882 -4.1647611 -4.1826024 -4.2052579 -4.2124763 -4.2165384 -4.2180014 -4.2389159 -4.2567215 -4.26308 -4.2607751 -4.2422848][-4.219377 -4.2208271 -4.1974645 -4.1669617 -4.1653605 -4.1901112 -4.215239 -4.2262716 -4.2376847 -4.2480025 -4.269197 -4.2845278 -4.2835436 -4.2703605 -4.2462506][-4.1938019 -4.2040267 -4.188345 -4.1687059 -4.1748772 -4.1996694 -4.2196865 -4.2281079 -4.2431841 -4.25892 -4.2805119 -4.2964997 -4.2959104 -4.2814112 -4.2599869][-4.1915994 -4.2055488 -4.1973906 -4.1880045 -4.1984191 -4.2168889 -4.2282705 -4.2305937 -4.2417736 -4.2564807 -4.2735949 -4.284904 -4.2840152 -4.2752938 -4.2628193][-4.2214212 -4.2285523 -4.2234106 -4.2190609 -4.2268419 -4.2369814 -4.2402558 -4.2364349 -4.2402039 -4.2478514 -4.2562828 -4.259408 -4.2559891 -4.2491875 -4.2455535]]...]
INFO - root - 2017-12-07 14:14:37.803414: step 8810, loss = 2.06, batch loss = 2.01 (10.2 examples/sec; 1.565 sec/batch; 68h:25m:33s remains)
INFO - root - 2017-12-07 14:14:53.906467: step 8820, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.572 sec/batch; 68h:45m:30s remains)
INFO - root - 2017-12-07 14:15:10.070967: step 8830, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.591 sec/batch; 69h:33m:26s remains)
INFO - root - 2017-12-07 14:15:26.308687: step 8840, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.710 sec/batch; 74h:47m:23s remains)
INFO - root - 2017-12-07 14:15:42.354841: step 8850, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.578 sec/batch; 68h:58m:56s remains)
INFO - root - 2017-12-07 14:15:58.602773: step 8860, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.705 sec/batch; 74h:32m:47s remains)
INFO - root - 2017-12-07 14:16:14.853272: step 8870, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 1.613 sec/batch; 70h:31m:43s remains)
INFO - root - 2017-12-07 14:16:31.016153: step 8880, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.658 sec/batch; 72h:29m:25s remains)
INFO - root - 2017-12-07 14:16:47.278784: step 8890, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.638 sec/batch; 71h:36m:00s remains)
INFO - root - 2017-12-07 14:17:03.503889: step 8900, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.557 sec/batch; 68h:02m:50s remains)
2017-12-07 14:17:04.872442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2711887 -4.2799716 -4.288928 -4.286242 -4.2741003 -4.2615466 -4.2573323 -4.258369 -4.2654433 -4.276814 -4.2869639 -4.2840528 -4.2664304 -4.2420659 -4.2181091][-4.2749729 -4.2778106 -4.28366 -4.2833309 -4.2746325 -4.264266 -4.2606378 -4.2591453 -4.2661543 -4.2809339 -4.2936797 -4.2897816 -4.2668548 -4.2394781 -4.2203851][-4.2613378 -4.2646079 -4.2731676 -4.2743745 -4.2642493 -4.2519817 -4.2481489 -4.2446404 -4.2493153 -4.2674885 -4.2806826 -4.2735872 -4.2457891 -4.2171144 -4.200335][-4.2359481 -4.242084 -4.2529154 -4.2538991 -4.2422142 -4.2306576 -4.2304015 -4.2320871 -4.237895 -4.255352 -4.268292 -4.2602487 -4.2351327 -4.2108359 -4.1971064][-4.2006617 -4.206418 -4.2177744 -4.217978 -4.2049685 -4.1983509 -4.2065306 -4.2154078 -4.2241282 -4.2429543 -4.2587948 -4.2553258 -4.2401934 -4.2246394 -4.2133679][-4.1756115 -4.1754704 -4.1795812 -4.1751518 -4.1636715 -4.1644654 -4.1789627 -4.185185 -4.1879 -4.2076449 -4.2322483 -4.2374 -4.2311945 -4.2224588 -4.2135305][-4.1713672 -4.160707 -4.1467085 -4.1343951 -4.1265192 -4.13698 -4.1550856 -4.1498213 -4.1394439 -4.1611657 -4.2019272 -4.2200518 -4.2205386 -4.2139897 -4.2068119][-4.1788082 -4.15742 -4.1221766 -4.0951538 -4.0867186 -4.1015568 -4.1208053 -4.1144652 -4.1041422 -4.1311512 -4.180758 -4.2078233 -4.2147045 -4.2116022 -4.2110877][-4.181159 -4.1560655 -4.110445 -4.0710011 -4.0596509 -4.0756326 -4.0957565 -4.101222 -4.10788 -4.1390967 -4.1833754 -4.2068005 -4.2153625 -4.2188439 -4.2302184][-4.1761003 -4.1570559 -4.1164694 -4.0802436 -4.068006 -4.078691 -4.09702 -4.1122437 -4.1308522 -4.1590471 -4.1936421 -4.2120361 -4.219871 -4.2280812 -4.2463531][-4.1973186 -4.1771722 -4.1427288 -4.1145482 -4.1064706 -4.1111035 -4.1230407 -4.1421909 -4.1647887 -4.1876249 -4.2127328 -4.2240086 -4.226831 -4.2351789 -4.2528296][-4.2257113 -4.2042031 -4.1757994 -4.1578984 -4.1567144 -4.1664028 -4.1798611 -4.1997008 -4.2148743 -4.2243309 -4.2362905 -4.2369442 -4.2329993 -4.23875 -4.2519546][-4.2320604 -4.2184811 -4.1996875 -4.188283 -4.189733 -4.2066441 -4.2305493 -4.2525024 -4.2602077 -4.2582121 -4.2589169 -4.256052 -4.2528181 -4.2561259 -4.2596159][-4.2283 -4.2229528 -4.210743 -4.1990037 -4.1980782 -4.2212644 -4.2559385 -4.2786427 -4.2832694 -4.2760854 -4.2688565 -4.2650256 -4.2653084 -4.2687516 -4.2658319][-4.219799 -4.2181349 -4.2102466 -4.2038364 -4.2068448 -4.233211 -4.265357 -4.2797022 -4.2769189 -4.2671103 -4.2609015 -4.2576289 -4.2594151 -4.2630358 -4.2551832]]...]
INFO - root - 2017-12-07 14:17:21.364577: step 8910, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 1.618 sec/batch; 70h:42m:28s remains)
INFO - root - 2017-12-07 14:17:37.577191: step 8920, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.593 sec/batch; 69h:37m:04s remains)
INFO - root - 2017-12-07 14:17:53.900987: step 8930, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.527 sec/batch; 66h:42m:57s remains)
INFO - root - 2017-12-07 14:18:10.293090: step 8940, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.733 sec/batch; 75h:44m:02s remains)
INFO - root - 2017-12-07 14:18:26.540727: step 8950, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.588 sec/batch; 69h:23m:22s remains)
INFO - root - 2017-12-07 14:18:42.825681: step 8960, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.665 sec/batch; 72h:43m:58s remains)
INFO - root - 2017-12-07 14:18:59.151956: step 8970, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.642 sec/batch; 71h:44m:45s remains)
INFO - root - 2017-12-07 14:19:15.494447: step 8980, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.652 sec/batch; 72h:11m:19s remains)
INFO - root - 2017-12-07 14:19:31.549890: step 8990, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 1.414 sec/batch; 61h:46m:44s remains)
INFO - root - 2017-12-07 14:19:47.696839: step 9000, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.603 sec/batch; 70h:01m:02s remains)
2017-12-07 14:19:49.076822: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2743082 -4.2773228 -4.2786269 -4.2794189 -4.2790356 -4.2745237 -4.2683067 -4.2627807 -4.2583623 -4.2583508 -4.264914 -4.2775278 -4.29004 -4.297358 -4.2988734][-4.2811556 -4.28443 -4.2856984 -4.285871 -4.2843685 -4.277225 -4.2673039 -4.2557163 -4.2443376 -4.239213 -4.24438 -4.2611036 -4.2813611 -4.2956553 -4.3014088][-4.2784991 -4.2809825 -4.2817712 -4.2815585 -4.2784219 -4.26718 -4.2503061 -4.2286367 -4.2076836 -4.1964827 -4.2002635 -4.2228975 -4.25498 -4.2810731 -4.2953849][-4.2664061 -4.2676072 -4.268877 -4.2696223 -4.265873 -4.2500496 -4.2215772 -4.1849217 -4.1526093 -4.1354995 -4.1384392 -4.1683545 -4.2153234 -4.2569714 -4.2824125][-4.2530909 -4.2527075 -4.2538495 -4.2536731 -4.2463155 -4.2227116 -4.1821709 -4.1340065 -4.0976911 -4.0834064 -4.0920868 -4.1302733 -4.186451 -4.2367396 -4.2680249][-4.2327495 -4.2287426 -4.2279577 -4.2247148 -4.2118587 -4.1817932 -4.1359735 -4.0868034 -4.0575538 -4.057776 -4.0793996 -4.1230245 -4.1788707 -4.22759 -4.2568297][-4.1951952 -4.1890907 -4.1888542 -4.1860833 -4.1745729 -4.1472335 -4.1070991 -4.0674524 -4.0507436 -4.0645885 -4.0959749 -4.1416912 -4.192637 -4.2328377 -4.2540674][-4.1472564 -4.149344 -4.1589 -4.1647921 -4.1622939 -4.1473126 -4.1231117 -4.09825 -4.0894413 -4.1052184 -4.1364546 -4.1786203 -4.2216816 -4.2515373 -4.2628865][-4.1044416 -4.1246643 -4.1516118 -4.1703277 -4.1793909 -4.1779361 -4.1699882 -4.1591978 -4.1537881 -4.1653972 -4.1904907 -4.2239766 -4.2536473 -4.2695265 -4.2693162][-4.0893054 -4.1247115 -4.1660023 -4.1948481 -4.2128577 -4.2187524 -4.21893 -4.2146959 -4.2090712 -4.2154818 -4.2333817 -4.2565503 -4.2714148 -4.272296 -4.2606039][-4.1166134 -4.1541018 -4.1955013 -4.2253733 -4.2457976 -4.2535629 -4.2539005 -4.2501707 -4.2424583 -4.2440734 -4.2553387 -4.2696362 -4.2743073 -4.2657104 -4.2476773][-4.1637969 -4.1942606 -4.2262611 -4.2506332 -4.2686925 -4.2741046 -4.2720113 -4.266871 -4.2578731 -4.2565479 -4.2633142 -4.2727256 -4.2731643 -4.2627416 -4.2462697][-4.2063656 -4.2285633 -4.2523885 -4.2713971 -4.2855177 -4.2882872 -4.2854309 -4.2804952 -4.2729564 -4.2716784 -4.2756877 -4.2812915 -4.2802887 -4.2718339 -4.2603116][-4.2388725 -4.2534 -4.2693605 -4.2821493 -4.2912583 -4.2911849 -4.28846 -4.285089 -4.2800555 -4.279881 -4.2825689 -4.2860179 -4.28509 -4.2799892 -4.2736874][-4.26231 -4.2709303 -4.2784886 -4.2835712 -4.2862697 -4.2831011 -4.2808652 -4.28024 -4.2781153 -4.2784781 -4.2794023 -4.280982 -4.2801828 -4.2780404 -4.2759218]]...]
INFO - root - 2017-12-07 14:20:05.252563: step 9010, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.668 sec/batch; 72h:50m:15s remains)
INFO - root - 2017-12-07 14:20:21.271422: step 9020, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.689 sec/batch; 73h:47m:10s remains)
INFO - root - 2017-12-07 14:20:37.452100: step 9030, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.619 sec/batch; 70h:42m:56s remains)
INFO - root - 2017-12-07 14:20:53.722276: step 9040, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.622 sec/batch; 70h:50m:09s remains)
INFO - root - 2017-12-07 14:21:09.861613: step 9050, loss = 2.09, batch loss = 2.04 (10.0 examples/sec; 1.606 sec/batch; 70h:08m:20s remains)
INFO - root - 2017-12-07 14:21:26.202451: step 9060, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 69h:58m:01s remains)
INFO - root - 2017-12-07 14:21:42.526583: step 9070, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 1.658 sec/batch; 72h:23m:44s remains)
INFO - root - 2017-12-07 14:21:58.615388: step 9080, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 1.547 sec/batch; 67h:31m:19s remains)
INFO - root - 2017-12-07 14:22:14.852192: step 9090, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.668 sec/batch; 72h:49m:10s remains)
INFO - root - 2017-12-07 14:22:31.025725: step 9100, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.569 sec/batch; 68h:28m:19s remains)
2017-12-07 14:22:32.359542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2615032 -4.2673979 -4.2794828 -4.2977653 -4.3228297 -4.3487387 -4.3587561 -4.3475327 -4.3172765 -4.2775912 -4.2409263 -4.2274408 -4.2285423 -4.2397337 -4.2575426][-4.251555 -4.2596326 -4.2796216 -4.3072557 -4.3368378 -4.3584285 -4.357944 -4.3344913 -4.2923346 -4.2444291 -4.2054982 -4.1990867 -4.2078657 -4.221086 -4.239152][-4.2453575 -4.2591057 -4.2865133 -4.3191 -4.3484383 -4.3611732 -4.34661 -4.3104563 -4.25914 -4.2043543 -4.1643758 -4.164928 -4.1784606 -4.1951652 -4.2179375][-4.2485046 -4.2650871 -4.2929444 -4.3230472 -4.344595 -4.3421469 -4.31131 -4.2620635 -4.2001529 -4.1367683 -4.1013107 -4.1135793 -4.1340494 -4.1582155 -4.194952][-4.2627606 -4.275847 -4.2947435 -4.311533 -4.31672 -4.2966189 -4.2501411 -4.1891909 -4.1177034 -4.0499015 -4.0299506 -4.060277 -4.092577 -4.1311073 -4.1845646][-4.2939663 -4.2954803 -4.2919874 -4.289042 -4.2748008 -4.2348895 -4.17313 -4.1012111 -4.02662 -3.9726005 -3.9839103 -4.0327215 -4.0782666 -4.1325264 -4.1970057][-4.3145294 -4.3033905 -4.2794933 -4.2559185 -4.223424 -4.1670861 -4.09658 -4.0214434 -3.9549818 -3.9299421 -3.97668 -4.0437875 -4.1036811 -4.1707039 -4.2391043][-4.3104091 -4.2867241 -4.2473164 -4.2084222 -4.1649828 -4.108501 -4.0477562 -3.9884837 -3.9437079 -3.9508598 -4.0196204 -4.0924425 -4.1553683 -4.2225857 -4.2858262][-4.288826 -4.25668 -4.2113585 -4.1671152 -4.1223049 -4.0756607 -4.0400662 -4.016582 -4.0063934 -4.0362849 -4.1022768 -4.1664119 -4.2208085 -4.2757673 -4.3243165][-4.2620997 -4.2281728 -4.1878123 -4.1507454 -4.1152272 -4.0878148 -4.0850358 -4.095921 -4.1118469 -4.1455126 -4.1944308 -4.2427907 -4.2824616 -4.3189545 -4.3496494][-4.2517595 -4.2261853 -4.2023373 -4.18043 -4.1591272 -4.1490707 -4.1661797 -4.19073 -4.2112212 -4.2366018 -4.2650518 -4.2943196 -4.3198786 -4.3418455 -4.3589811][-4.2680483 -4.2533169 -4.2437797 -4.2327552 -4.2197857 -4.2167344 -4.2369289 -4.2611861 -4.2774744 -4.2921076 -4.3022375 -4.3141451 -4.3276792 -4.3419538 -4.3522806][-4.2965379 -4.2870579 -4.2826653 -4.27436 -4.2630572 -4.2610359 -4.275969 -4.2953176 -4.3048773 -4.3080726 -4.3065128 -4.3078341 -4.3166018 -4.3281493 -4.3371158][-4.3074303 -4.301187 -4.2986569 -4.2894778 -4.27612 -4.2719502 -4.2801027 -4.2923932 -4.2957516 -4.2920971 -4.2854061 -4.281312 -4.2887616 -4.300313 -4.3104334][-4.2924714 -4.289505 -4.2889166 -4.2797589 -4.2641964 -4.2563672 -4.2588491 -4.2645845 -4.2614884 -4.2522807 -4.2436781 -4.236145 -4.2463188 -4.2636967 -4.2767787]]...]
INFO - root - 2017-12-07 14:22:48.383230: step 9110, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 1.461 sec/batch; 63h:46m:39s remains)
INFO - root - 2017-12-07 14:23:04.556708: step 9120, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.728 sec/batch; 75h:25m:50s remains)
INFO - root - 2017-12-07 14:23:20.781647: step 9130, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.587 sec/batch; 69h:15m:52s remains)
INFO - root - 2017-12-07 14:23:37.002545: step 9140, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 1.667 sec/batch; 72h:43m:56s remains)
INFO - root - 2017-12-07 14:23:53.224558: step 9150, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.606 sec/batch; 70h:05m:12s remains)
INFO - root - 2017-12-07 14:24:09.662347: step 9160, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.607 sec/batch; 70h:07m:19s remains)
INFO - root - 2017-12-07 14:24:25.875527: step 9170, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.704 sec/batch; 74h:20m:46s remains)
INFO - root - 2017-12-07 14:24:41.992818: step 9180, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 1.605 sec/batch; 70h:02m:30s remains)
INFO - root - 2017-12-07 14:24:58.326193: step 9190, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.741 sec/batch; 75h:58m:12s remains)
INFO - root - 2017-12-07 14:25:14.585124: step 9200, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.603 sec/batch; 69h:56m:36s remains)
2017-12-07 14:25:15.919410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3222523 -4.3225722 -4.3211236 -4.3191519 -4.3186784 -4.3213248 -4.3249068 -4.3273439 -4.328228 -4.3272943 -4.3248849 -4.3221364 -4.3202429 -4.3190241 -4.3188963][-4.3237658 -4.3233161 -4.3205757 -4.3155274 -4.31129 -4.3128567 -4.3188653 -4.324934 -4.3293133 -4.3308825 -4.3294525 -4.3262157 -4.3230333 -4.3193669 -4.3163915][-4.3230004 -4.3217378 -4.317873 -4.3094993 -4.2990308 -4.2955427 -4.3022203 -4.3133464 -4.3235574 -4.3300138 -4.331624 -4.3295436 -4.3260293 -4.3201146 -4.3132071][-4.3228521 -4.3209004 -4.3164954 -4.3059645 -4.288908 -4.2762375 -4.2786365 -4.2939296 -4.3119268 -4.3256574 -4.3325109 -4.3336024 -4.3311653 -4.3239169 -4.3131075][-4.3227687 -4.3203692 -4.3158631 -4.3048244 -4.2832565 -4.2603059 -4.2526312 -4.266788 -4.29215 -4.3150787 -4.3293777 -4.3356795 -4.3357778 -4.3285732 -4.3149843][-4.3218188 -4.3188787 -4.3146248 -4.3047285 -4.2822132 -4.2516708 -4.2306933 -4.2356482 -4.2633018 -4.2952771 -4.3191628 -4.332902 -4.3370581 -4.3315268 -4.3171811][-4.3206563 -4.31734 -4.3139153 -4.3068428 -4.2876892 -4.2555013 -4.2228413 -4.2119222 -4.2321491 -4.2685614 -4.3019767 -4.3249731 -4.3346553 -4.3320661 -4.3187175][-4.3189154 -4.3149958 -4.3121285 -4.3086596 -4.2959433 -4.2685661 -4.23088 -4.2034307 -4.2079191 -4.2404981 -4.2799711 -4.3121929 -4.3285217 -4.3296275 -4.3186255][-4.3172235 -4.3130045 -4.31081 -4.3106709 -4.3056812 -4.2876153 -4.2530622 -4.215589 -4.2023787 -4.2226262 -4.2610316 -4.2985568 -4.320488 -4.3251686 -4.3166184][-4.3168435 -4.3126769 -4.3108573 -4.3130627 -4.3148131 -4.3069434 -4.2809796 -4.2429056 -4.2174249 -4.2216883 -4.2515945 -4.2890048 -4.3137937 -4.3210635 -4.3145118][-4.317976 -4.3136511 -4.3109546 -4.3134179 -4.3189564 -4.3189878 -4.3033924 -4.2726159 -4.2445474 -4.2370481 -4.2551956 -4.2871375 -4.3108559 -4.3186212 -4.3132048][-4.3200488 -4.3159976 -4.3121233 -4.3132429 -4.3193679 -4.3235765 -4.316052 -4.2952151 -4.2720609 -4.2601452 -4.268168 -4.2915335 -4.3116264 -4.3185482 -4.3140249][-4.3220506 -4.3186631 -4.314023 -4.3133225 -4.3181038 -4.3232608 -4.3209996 -4.308671 -4.29324 -4.2830973 -4.2857332 -4.3004527 -4.3147445 -4.3198271 -4.3160744][-4.3238921 -4.3215404 -4.3172712 -4.3152423 -4.3181996 -4.3228269 -4.3233681 -4.3172393 -4.3088412 -4.302855 -4.3036132 -4.311738 -4.3199506 -4.322566 -4.3192635][-4.3250079 -4.3231959 -4.3193026 -4.316556 -4.317503 -4.3209543 -4.323338 -4.3220439 -4.3192205 -4.3171616 -4.3176551 -4.3214369 -4.324914 -4.3254352 -4.3227959]]...]
INFO - root - 2017-12-07 14:25:31.839132: step 9210, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.587 sec/batch; 69h:14m:24s remains)
INFO - root - 2017-12-07 14:25:48.206697: step 9220, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.689 sec/batch; 73h:40m:00s remains)
INFO - root - 2017-12-07 14:26:04.111222: step 9230, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.610 sec/batch; 70h:12m:54s remains)
INFO - root - 2017-12-07 14:26:20.169575: step 9240, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 1.651 sec/batch; 72h:00m:55s remains)
INFO - root - 2017-12-07 14:26:36.314133: step 9250, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.566 sec/batch; 68h:16m:46s remains)
INFO - root - 2017-12-07 14:26:52.681562: step 9260, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.569 sec/batch; 68h:26m:34s remains)
INFO - root - 2017-12-07 14:27:08.821112: step 9270, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.614 sec/batch; 70h:23m:00s remains)
INFO - root - 2017-12-07 14:27:24.908580: step 9280, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.610 sec/batch; 70h:11m:22s remains)
INFO - root - 2017-12-07 14:27:41.079878: step 9290, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 1.691 sec/batch; 73h:43m:04s remains)
INFO - root - 2017-12-07 14:27:57.184716: step 9300, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.704 sec/batch; 74h:17m:35s remains)
2017-12-07 14:27:58.535366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2882013 -4.2813597 -4.2743545 -4.2692447 -4.2712579 -4.2701826 -4.247057 -4.198688 -4.1936188 -4.2304492 -4.2516932 -4.2477779 -4.2378654 -4.2366047 -4.2354083][-4.279356 -4.2660637 -4.2538018 -4.244638 -4.2418547 -4.239233 -4.2175136 -4.173399 -4.172143 -4.20626 -4.21797 -4.2066708 -4.1977954 -4.1987004 -4.1969619][-4.2676015 -4.2455249 -4.2235165 -4.2080059 -4.2003169 -4.1932192 -4.1755042 -4.141665 -4.1453028 -4.1760635 -4.1754813 -4.1582828 -4.1560812 -4.1647773 -4.1669884][-4.2556505 -4.224544 -4.19016 -4.16132 -4.1462221 -4.1367054 -4.1227283 -4.1011982 -4.111702 -4.1431389 -4.1420178 -4.1298609 -4.1399703 -4.1535692 -4.1623368][-4.2515159 -4.2179122 -4.1783586 -4.1400013 -4.1146488 -4.0979114 -4.07803 -4.058012 -4.0718231 -4.1094456 -4.1198769 -4.1215019 -4.1367927 -4.1488638 -4.1605692][-4.2569895 -4.2292161 -4.1956692 -4.1624045 -4.1348715 -4.1068373 -4.0704579 -4.0335469 -4.0369635 -4.0789027 -4.0982766 -4.109549 -4.1246104 -4.1344118 -4.1439939][-4.2643814 -4.2410192 -4.2127609 -4.1866012 -4.1612592 -4.12729 -4.074213 -4.0106239 -3.9947529 -4.0387321 -4.0673466 -4.0901084 -4.1117311 -4.1194739 -4.1222224][-4.2734051 -4.2534943 -4.2257948 -4.2000651 -4.1746125 -4.1397729 -4.0821 -4.0033278 -3.970681 -4.0100961 -4.0453968 -4.0742421 -4.1054797 -4.1168451 -4.1191339][-4.2823653 -4.2646756 -4.23755 -4.2110329 -4.1875625 -4.1576791 -4.10702 -4.02757 -3.9918952 -4.033453 -4.0720038 -4.0993657 -4.1324873 -4.147315 -4.152215][-4.2862744 -4.2717972 -4.2478523 -4.223321 -4.2063789 -4.1867523 -4.1486959 -4.0759668 -4.0378842 -4.0836291 -4.1268225 -4.1495147 -4.1759644 -4.1900053 -4.1954][-4.2844877 -4.270359 -4.250351 -4.2323904 -4.2200809 -4.207325 -4.1796308 -4.1153841 -4.0752859 -4.1208496 -4.1648188 -4.1820931 -4.2015462 -4.2142806 -4.2209058][-4.2789054 -4.2625995 -4.2448997 -4.233397 -4.2270846 -4.2192364 -4.1978726 -4.1353507 -4.0935192 -4.1402874 -4.1871457 -4.1985025 -4.2118983 -4.2249193 -4.2330632][-4.2749748 -4.2573233 -4.2411065 -4.2305536 -4.2253456 -4.220386 -4.203352 -4.1428375 -4.1035328 -4.1542635 -4.2054224 -4.2129045 -4.2197881 -4.2330723 -4.2451067][-4.2737432 -4.2560468 -4.2401519 -4.2314582 -4.23111 -4.2288656 -4.2132521 -4.1591835 -4.1265607 -4.1758871 -4.2267461 -4.2347231 -4.2385063 -4.2483788 -4.25777][-4.2738447 -4.256444 -4.2409225 -4.2354455 -4.2417068 -4.2486291 -4.2379837 -4.1894259 -4.1606941 -4.2018056 -4.2416911 -4.2468414 -4.2478032 -4.2531295 -4.2582827]]...]
INFO - root - 2017-12-07 14:28:14.792437: step 9310, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.684 sec/batch; 73h:24m:33s remains)
INFO - root - 2017-12-07 14:28:30.910486: step 9320, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.567 sec/batch; 68h:17m:53s remains)
INFO - root - 2017-12-07 14:28:47.215097: step 9330, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.726 sec/batch; 75h:13m:40s remains)
INFO - root - 2017-12-07 14:29:03.491827: step 9340, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.582 sec/batch; 68h:56m:16s remains)
INFO - root - 2017-12-07 14:29:19.589032: step 9350, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.635 sec/batch; 71h:15m:35s remains)
INFO - root - 2017-12-07 14:29:35.836548: step 9360, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.604 sec/batch; 69h:53m:29s remains)
INFO - root - 2017-12-07 14:29:52.069091: step 9370, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 1.619 sec/batch; 70h:33m:36s remains)
INFO - root - 2017-12-07 14:30:08.255062: step 9380, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.563 sec/batch; 68h:05m:48s remains)
INFO - root - 2017-12-07 14:30:24.541025: step 9390, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.565 sec/batch; 68h:11m:57s remains)
INFO - root - 2017-12-07 14:30:40.581178: step 9400, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.555 sec/batch; 67h:43m:50s remains)
2017-12-07 14:30:41.944903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1470714 -4.1537943 -4.1716132 -4.1983633 -4.2334571 -4.2489219 -4.2427044 -4.2322564 -4.2204361 -4.2017603 -4.1791148 -4.1684365 -4.1735783 -4.1916509 -4.1945462][-4.1085167 -4.1161156 -4.1392326 -4.1714354 -4.2096834 -4.2177758 -4.2057977 -4.198595 -4.1954451 -4.1816664 -4.1620545 -4.1576214 -4.1721811 -4.1991391 -4.2108245][-4.0929861 -4.1021976 -4.1224432 -4.1494756 -4.1782632 -4.170063 -4.1481414 -4.1489096 -4.1632776 -4.1671634 -4.1563635 -4.1514111 -4.1708317 -4.2012253 -4.2169971][-4.1098185 -4.1205916 -4.1324053 -4.1419997 -4.1454744 -4.1157131 -4.0861096 -4.0999103 -4.1319366 -4.1530175 -4.1518183 -4.1466303 -4.1681871 -4.198154 -4.2154717][-4.1512837 -4.1609449 -4.1665068 -4.1535416 -4.1195164 -4.0596485 -4.0138569 -4.0379949 -4.0874863 -4.1283607 -4.1449347 -4.1484094 -4.1712894 -4.2006421 -4.2177696][-4.1990628 -4.2044826 -4.201014 -4.1644235 -4.0895739 -3.9893022 -3.9073215 -3.9368663 -4.0164566 -4.0895066 -4.1341186 -4.1517143 -4.1715879 -4.1956024 -4.2154441][-4.2171931 -4.2128043 -4.19462 -4.138833 -4.0328879 -3.887552 -3.7612858 -3.8088014 -3.937829 -4.0479331 -4.1117582 -4.1375995 -4.1597323 -4.1896706 -4.2187605][-4.200345 -4.1890945 -4.1643496 -4.1013532 -3.9874098 -3.8309155 -3.6997468 -3.7730608 -3.9311235 -4.05162 -4.1177168 -4.1494737 -4.1726975 -4.2021527 -4.2309642][-4.15447 -4.1517558 -4.1396027 -4.095552 -4.0164046 -3.9153545 -3.8416381 -3.9022396 -4.0186877 -4.112515 -4.1694241 -4.199275 -4.213089 -4.2278152 -4.2422509][-4.1329527 -4.1535964 -4.160809 -4.1442118 -4.1079593 -4.0620265 -4.0291653 -4.0641665 -4.1305904 -4.1953921 -4.2373552 -4.2562637 -4.258379 -4.2547603 -4.2526484][-4.1528111 -4.1831012 -4.1974926 -4.1966414 -4.185503 -4.167007 -4.1552839 -4.1758018 -4.2112865 -4.2520442 -4.2794142 -4.2893252 -4.2855144 -4.2705059 -4.2561197][-4.1856518 -4.209178 -4.2222281 -4.2295427 -4.2304225 -4.2237439 -4.2218227 -4.2370934 -4.2564449 -4.2784438 -4.2915049 -4.2923269 -4.2839947 -4.2623153 -4.2393894][-4.2113333 -4.2216492 -4.2343698 -4.2445331 -4.2514338 -4.2493925 -4.2513604 -4.2614617 -4.268837 -4.2763085 -4.2807527 -4.276917 -4.2650175 -4.2373705 -4.2098274][-4.2396894 -4.2416019 -4.2550659 -4.2658467 -4.2707748 -4.2688231 -4.2681055 -4.26858 -4.2636819 -4.2564077 -4.2499661 -4.2427344 -4.2325125 -4.2094707 -4.1876297][-4.2712975 -4.2744026 -4.2831841 -4.2867408 -4.286458 -4.2814708 -4.278532 -4.2726517 -4.2581081 -4.2398915 -4.2276554 -4.221818 -4.2203608 -4.2062359 -4.1954336]]...]
INFO - root - 2017-12-07 14:30:58.120841: step 9410, loss = 2.06, batch loss = 2.01 (10.3 examples/sec; 1.554 sec/batch; 67h:41m:06s remains)
INFO - root - 2017-12-07 14:31:14.337958: step 9420, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 68h:31m:17s remains)
INFO - root - 2017-12-07 14:31:30.611503: step 9430, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.707 sec/batch; 74h:22m:36s remains)
INFO - root - 2017-12-07 14:31:46.716060: step 9440, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.522 sec/batch; 66h:16m:37s remains)
INFO - root - 2017-12-07 14:32:02.932150: step 9450, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.634 sec/batch; 71h:10m:16s remains)
INFO - root - 2017-12-07 14:32:19.133436: step 9460, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 1.598 sec/batch; 69h:34m:38s remains)
INFO - root - 2017-12-07 14:32:35.216085: step 9470, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.679 sec/batch; 73h:07m:36s remains)
INFO - root - 2017-12-07 14:32:51.298834: step 9480, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.583 sec/batch; 68h:56m:25s remains)
INFO - root - 2017-12-07 14:33:07.528658: step 9490, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.653 sec/batch; 71h:57m:26s remains)
INFO - root - 2017-12-07 14:33:23.698883: step 9500, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.586 sec/batch; 69h:02m:14s remains)
2017-12-07 14:33:25.104361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1676221 -4.1633015 -4.1638064 -4.1784897 -4.1979117 -4.221559 -4.2312937 -4.22246 -4.2079287 -4.1885052 -4.1763535 -4.1890235 -4.2040544 -4.2024584 -4.1859431][-4.1631584 -4.1576762 -4.1432276 -4.1516786 -4.1806955 -4.2149739 -4.2331877 -4.2302451 -4.2180343 -4.1998897 -4.1879368 -4.201582 -4.214931 -4.2067308 -4.1843696][-4.174108 -4.1686254 -4.1487665 -4.1471744 -4.1739225 -4.2059803 -4.228487 -4.2272644 -4.2147908 -4.2018938 -4.1941967 -4.209022 -4.226378 -4.2173333 -4.1936746][-4.1853671 -4.1867065 -4.16639 -4.1485014 -4.1552076 -4.1736593 -4.1993732 -4.2048011 -4.1929765 -4.18478 -4.1870546 -4.2071929 -4.2264428 -4.2175083 -4.1946454][-4.1845407 -4.1933303 -4.1769376 -4.1421547 -4.1162653 -4.1192846 -4.1601534 -4.1838722 -4.1729832 -4.1563931 -4.1606236 -4.1848478 -4.203835 -4.1957293 -4.1764655][-4.1666636 -4.17648 -4.1568313 -4.1144586 -4.0551434 -4.0438457 -4.111659 -4.16289 -4.1589003 -4.1348619 -4.1354551 -4.1537628 -4.1651239 -4.1546879 -4.1403236][-4.1407146 -4.1486697 -4.1237569 -4.0709276 -3.9831316 -3.9582493 -4.0517597 -4.1296649 -4.1369057 -4.1131134 -4.1125689 -4.125114 -4.1345024 -4.1320682 -4.1269135][-4.1299706 -4.1366706 -4.1236939 -4.0804505 -3.9916346 -3.9588006 -4.0511303 -4.13385 -4.144979 -4.1227775 -4.1210957 -4.1255484 -4.128726 -4.1356225 -4.1377206][-4.1514349 -4.1586442 -4.1636629 -4.14455 -4.0922546 -4.0674305 -4.1217413 -4.1804457 -4.1915674 -4.1765318 -4.1718311 -4.1703172 -4.1652517 -4.1691093 -4.1723404][-4.1918855 -4.1973209 -4.2075543 -4.203217 -4.1769471 -4.1614466 -4.1868649 -4.2197008 -4.2302928 -4.2277479 -4.2275553 -4.2243605 -4.2173548 -4.2155309 -4.2143097][-4.2276864 -4.2357469 -4.2425818 -4.2423687 -4.227829 -4.2158766 -4.2227645 -4.2343917 -4.2399216 -4.2442722 -4.2481184 -4.2483354 -4.243372 -4.2402368 -4.2329659][-4.248096 -4.2573447 -4.263783 -4.2693915 -4.2637606 -4.2521262 -4.2432547 -4.2394767 -4.2394214 -4.2434831 -4.2520151 -4.2577271 -4.2544317 -4.2487946 -4.2375817][-4.25715 -4.2677937 -4.2728014 -4.2783585 -4.2766528 -4.2678051 -4.2557058 -4.2474122 -4.2424574 -4.241787 -4.2508759 -4.2619596 -4.261405 -4.2570806 -4.2481251][-4.2474294 -4.2569532 -4.2563396 -4.2610869 -4.2630162 -4.2593594 -4.2486377 -4.2390804 -4.2318478 -4.23306 -4.2444463 -4.259769 -4.2630568 -4.2615027 -4.2570004][-4.2259865 -4.2309365 -4.2277112 -4.2361083 -4.2454963 -4.2474813 -4.2365642 -4.2225771 -4.2195511 -4.2308836 -4.2477026 -4.26427 -4.2700491 -4.2679873 -4.2647643]]...]
INFO - root - 2017-12-07 14:33:41.427847: step 9510, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 1.595 sec/batch; 69h:27m:31s remains)
INFO - root - 2017-12-07 14:33:57.596851: step 9520, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.643 sec/batch; 71h:31m:10s remains)
INFO - root - 2017-12-07 14:34:13.698537: step 9530, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.592 sec/batch; 69h:19m:17s remains)
INFO - root - 2017-12-07 14:34:29.926224: step 9540, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.556 sec/batch; 67h:45m:18s remains)
INFO - root - 2017-12-07 14:34:46.200540: step 9550, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.705 sec/batch; 74h:14m:00s remains)
INFO - root - 2017-12-07 14:35:02.459268: step 9560, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.595 sec/batch; 69h:24m:08s remains)
INFO - root - 2017-12-07 14:35:18.877040: step 9570, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.706 sec/batch; 74h:14m:45s remains)
INFO - root - 2017-12-07 14:35:34.927942: step 9580, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.737 sec/batch; 75h:35m:32s remains)
INFO - root - 2017-12-07 14:35:50.959265: step 9590, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.579 sec/batch; 68h:43m:16s remains)
INFO - root - 2017-12-07 14:36:07.337842: step 9600, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.716 sec/batch; 74h:39m:03s remains)
2017-12-07 14:36:08.712234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2130527 -4.2036552 -4.2009511 -4.219244 -4.252358 -4.2843833 -4.2992806 -4.2998405 -4.2955232 -4.2907538 -4.2718267 -4.2240911 -4.1806827 -4.1695008 -4.1740003][-4.1899443 -4.1868033 -4.1916237 -4.2158914 -4.2501469 -4.2825093 -4.3040457 -4.3123 -4.3159 -4.3166356 -4.3061771 -4.2648191 -4.2135472 -4.1791215 -4.1584349][-4.1702404 -4.1603112 -4.1706395 -4.2023563 -4.2389412 -4.2701464 -4.296484 -4.3160253 -4.3302445 -4.3345284 -4.32847 -4.2940454 -4.2433019 -4.195775 -4.1555815][-4.1600928 -4.137413 -4.147016 -4.1842694 -4.22506 -4.2555795 -4.2852263 -4.3109555 -4.3292718 -4.3327932 -4.3298926 -4.3068285 -4.2683253 -4.2240982 -4.1742][-4.1482611 -4.1115103 -4.1172328 -4.153687 -4.1979971 -4.2336988 -4.2668304 -4.2938223 -4.3097773 -4.3112555 -4.3128405 -4.3031468 -4.2838087 -4.2542515 -4.2064695][-4.1380792 -4.0910249 -4.0865836 -4.1139493 -4.15172 -4.1921935 -4.2329326 -4.2625742 -4.2743998 -4.2735524 -4.2791042 -4.2815065 -4.2794108 -4.2632442 -4.2220273][-4.157187 -4.1040926 -4.0805583 -4.088419 -4.1106005 -4.146337 -4.1888537 -4.2190847 -4.22783 -4.2238164 -4.233253 -4.2451706 -4.256238 -4.2542248 -4.2230959][-4.2043834 -4.1578727 -4.1186991 -4.1034989 -4.103611 -4.1187577 -4.1483469 -4.1703482 -4.1708236 -4.1621614 -4.1786385 -4.1997266 -4.2179565 -4.2256522 -4.2082858][-4.246635 -4.2132773 -4.177515 -4.155231 -4.1382165 -4.128932 -4.132771 -4.1327677 -4.1149297 -4.0940361 -4.1118531 -4.1375003 -4.1605945 -4.1791468 -4.1787786][-4.2577848 -4.2405128 -4.2229767 -4.211143 -4.1938872 -4.1771979 -4.1640954 -4.1415358 -4.1014972 -4.06437 -4.068645 -4.0853634 -4.1095057 -4.1383896 -4.1551886][-4.2497149 -4.2461004 -4.2456651 -4.2476954 -4.2411494 -4.233161 -4.2202349 -4.1907368 -4.1442475 -4.1003361 -4.0859532 -4.087471 -4.103857 -4.132966 -4.1590958][-4.2474022 -4.2474651 -4.2540226 -4.259779 -4.2611747 -4.2648287 -4.2659912 -4.2512474 -4.21826 -4.1802115 -4.1534109 -4.1409235 -4.1470103 -4.1684027 -4.190855][-4.2536116 -4.2473154 -4.2479644 -4.2524815 -4.2583647 -4.2708917 -4.2865748 -4.2906675 -4.2781086 -4.2511759 -4.224668 -4.2068253 -4.2048855 -4.2156878 -4.2283049][-4.2447867 -4.2309313 -4.2283254 -4.236362 -4.250464 -4.2702847 -4.2917695 -4.302597 -4.3026738 -4.2893615 -4.2725964 -4.2589154 -4.255218 -4.2596531 -4.2647257][-4.2272573 -4.2165618 -4.2163725 -4.2280207 -4.2512608 -4.27329 -4.2913308 -4.3006253 -4.3082581 -4.3104863 -4.3061838 -4.3004236 -4.2988529 -4.3003626 -4.3002157]]...]
INFO - root - 2017-12-07 14:36:25.018597: step 9610, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.675 sec/batch; 72h:52m:19s remains)
INFO - root - 2017-12-07 14:36:41.121814: step 9620, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.570 sec/batch; 68h:17m:15s remains)
INFO - root - 2017-12-07 14:36:57.442868: step 9630, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 1.666 sec/batch; 72h:29m:24s remains)
INFO - root - 2017-12-07 14:37:13.517131: step 9640, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.591 sec/batch; 69h:12m:31s remains)
INFO - root - 2017-12-07 14:37:29.309813: step 9650, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.648 sec/batch; 71h:41m:31s remains)
INFO - root - 2017-12-07 14:37:45.629146: step 9660, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.555 sec/batch; 67h:36m:59s remains)
INFO - root - 2017-12-07 14:38:01.996371: step 9670, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.615 sec/batch; 70h:15m:13s remains)
INFO - root - 2017-12-07 14:38:18.251555: step 9680, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.662 sec/batch; 72h:18m:11s remains)
INFO - root - 2017-12-07 14:38:34.258058: step 9690, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 1.495 sec/batch; 65h:01m:05s remains)
INFO - root - 2017-12-07 14:38:50.567168: step 9700, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.700 sec/batch; 73h:55m:41s remains)
2017-12-07 14:38:51.856508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2107096 -4.232307 -4.2501478 -4.2460127 -4.2299981 -4.209281 -4.1880512 -4.1718507 -4.1666527 -4.1673265 -4.1710963 -4.1653757 -4.1597166 -4.1516414 -4.1466765][-4.20865 -4.2299318 -4.2428174 -4.2374392 -4.2235332 -4.2061377 -4.1884336 -4.1762795 -4.17267 -4.1723475 -4.1695981 -4.1578484 -4.1488023 -4.141243 -4.1391215][-4.1900997 -4.2144918 -4.2274756 -4.2258921 -4.2185359 -4.2078037 -4.197751 -4.1921415 -4.1920037 -4.194139 -4.1905942 -4.1763177 -4.1615281 -4.1524854 -4.1522236][-4.1494894 -4.1719027 -4.190228 -4.2002654 -4.203177 -4.1967278 -4.1899681 -4.1891136 -4.189784 -4.1962061 -4.1991162 -4.1922259 -4.1795955 -4.1746697 -4.1792073][-4.1264224 -4.1403394 -4.1577353 -4.1786842 -4.1940508 -4.1920309 -4.1828833 -4.1801443 -4.177578 -4.1834149 -4.1927896 -4.1956615 -4.191093 -4.19433 -4.2076535][-4.1302118 -4.1352506 -4.1480942 -4.1759653 -4.2023377 -4.2038093 -4.1904874 -4.1832232 -4.1801844 -4.1842165 -4.19668 -4.2066603 -4.2112422 -4.2233486 -4.2431569][-4.1349607 -4.1469069 -4.1589713 -4.1865096 -4.215188 -4.2130594 -4.1944647 -4.1806831 -4.181109 -4.1913567 -4.2090945 -4.2264481 -4.240849 -4.260428 -4.283607][-4.1429634 -4.1629252 -4.1804342 -4.20829 -4.2348065 -4.2261319 -4.1943197 -4.1689143 -4.1721306 -4.1925235 -4.2172871 -4.241569 -4.2625375 -4.2848892 -4.3077741][-4.1594362 -4.1779494 -4.1978383 -4.2221112 -4.2387018 -4.2210045 -4.1780882 -4.1470504 -4.1585407 -4.19377 -4.2280145 -4.2536254 -4.2699428 -4.2848592 -4.3003831][-4.1710777 -4.1856651 -4.2078867 -4.2256312 -4.230444 -4.2053494 -4.1553736 -4.1238332 -4.1401544 -4.1795554 -4.2138081 -4.2315116 -4.2360873 -4.2397418 -4.2488365][-4.2073116 -4.2150459 -4.228332 -4.2348266 -4.2279625 -4.2014022 -4.1589565 -4.137424 -4.1529 -4.1801634 -4.1959133 -4.1908917 -4.173306 -4.1593847 -4.1647429][-4.2485313 -4.2515054 -4.2537985 -4.2487216 -4.2363739 -4.217319 -4.1957707 -4.1919 -4.2068691 -4.2173629 -4.2069716 -4.1759262 -4.134635 -4.1056318 -4.110929][-4.2742558 -4.2740884 -4.2685108 -4.2568903 -4.2443433 -4.2343087 -4.2293429 -4.2383895 -4.2524238 -4.249342 -4.220612 -4.1740818 -4.1226573 -4.0843234 -4.088304][-4.2776313 -4.2759843 -4.2656078 -4.24811 -4.2346706 -4.2310724 -4.2370663 -4.2513642 -4.2627392 -4.2537088 -4.2197933 -4.1710935 -4.1194444 -4.0782256 -4.08124][-4.2682462 -4.2633529 -4.2477407 -4.2244453 -4.209403 -4.2104521 -4.2248788 -4.2447071 -4.255146 -4.2463207 -4.2148409 -4.1699247 -4.123415 -4.0883765 -4.0935564]]...]
INFO - root - 2017-12-07 14:39:07.947745: step 9710, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.618 sec/batch; 70h:20m:24s remains)
INFO - root - 2017-12-07 14:39:24.321316: step 9720, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.606 sec/batch; 69h:50m:16s remains)
INFO - root - 2017-12-07 14:39:40.525722: step 9730, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.698 sec/batch; 73h:49m:11s remains)
INFO - root - 2017-12-07 14:39:56.870718: step 9740, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.607 sec/batch; 69h:51m:23s remains)
INFO - root - 2017-12-07 14:40:13.103792: step 9750, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.716 sec/batch; 74h:36m:34s remains)
INFO - root - 2017-12-07 14:40:29.368801: step 9760, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.585 sec/batch; 68h:53m:47s remains)
INFO - root - 2017-12-07 14:40:45.571490: step 9770, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.684 sec/batch; 73h:12m:29s remains)
INFO - root - 2017-12-07 14:41:01.615187: step 9780, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.551 sec/batch; 67h:25m:45s remains)
INFO - root - 2017-12-07 14:41:17.999984: step 9790, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 70h:49m:44s remains)
INFO - root - 2017-12-07 14:41:34.195530: step 9800, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.648 sec/batch; 71h:36m:35s remains)
2017-12-07 14:41:35.700988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2042189 -4.1884418 -4.2179203 -4.2565441 -4.2677293 -4.2679882 -4.2689333 -4.270256 -4.2703047 -4.2735286 -4.2793927 -4.2822623 -4.2718587 -4.2607765 -4.2496872][-4.1846905 -4.1727171 -4.1989179 -4.2332125 -4.246233 -4.2529554 -4.2579165 -4.266305 -4.274662 -4.2805276 -4.2837286 -4.2805986 -4.260325 -4.2342677 -4.2104583][-4.1612515 -4.152607 -4.180768 -4.2080913 -4.2189965 -4.2274876 -4.2346139 -4.2506371 -4.2711082 -4.2876182 -4.294528 -4.2877936 -4.2588835 -4.214972 -4.1726584][-4.1530862 -4.1468611 -4.1744623 -4.194 -4.1984534 -4.2046323 -4.2078476 -4.226974 -4.2572131 -4.28617 -4.3019319 -4.297543 -4.2682405 -4.2169046 -4.1599431][-4.1675406 -4.1510878 -4.1625652 -4.1694689 -4.1624465 -4.1583834 -4.1599374 -4.1829004 -4.220654 -4.2649512 -4.2951603 -4.2981038 -4.2785869 -4.2367969 -4.1820087][-4.1904383 -4.1540956 -4.1354938 -4.1145496 -4.0894465 -4.0694661 -4.0602422 -4.0788226 -4.1263151 -4.1927433 -4.2407308 -4.2621212 -4.2636342 -4.2456131 -4.2121949][-4.2058535 -4.1499567 -4.0954266 -4.0416851 -3.991276 -3.9437525 -3.9011114 -3.895911 -3.9552579 -4.0509348 -4.1222291 -4.1708264 -4.2037854 -4.2185092 -4.2202034][-4.2117696 -4.1517105 -4.0796709 -4.0005293 -3.9163256 -3.8249023 -3.7222869 -3.6655955 -3.7316079 -3.8508537 -3.9400704 -4.0178146 -4.0891595 -4.1449561 -4.1861773][-4.2225552 -4.1765003 -4.1143532 -4.0387168 -3.9470453 -3.8351774 -3.7071319 -3.6192663 -3.6523917 -3.737978 -3.8023314 -3.8814163 -3.9731085 -4.0630946 -4.1378756][-4.2349472 -4.209744 -4.1704164 -4.1205049 -4.0553956 -3.972513 -3.882019 -3.8138552 -3.8017488 -3.8136325 -3.8193612 -3.862834 -3.9391277 -4.0299749 -4.1166286][-4.2453942 -4.2391691 -4.2217422 -4.1941113 -4.1544042 -4.1045976 -4.0561228 -4.0184484 -3.9999254 -3.979727 -3.9555495 -3.9717851 -4.0236769 -4.0920472 -4.1604857][-4.263308 -4.2696362 -4.2696228 -4.2611413 -4.2410469 -4.2117496 -4.1873479 -4.17006 -4.1580529 -4.1385236 -4.1122627 -4.112958 -4.1391048 -4.1795912 -4.2250142][-4.2914076 -4.3024788 -4.313127 -4.318357 -4.3115406 -4.2959771 -4.2844234 -4.2755232 -4.2658119 -4.2489662 -4.228931 -4.2248845 -4.2352939 -4.257165 -4.2854695][-4.3290515 -4.3384743 -4.3509035 -4.3616276 -4.3627319 -4.3571172 -4.3510756 -4.344461 -4.3364992 -4.3246794 -4.3118763 -4.3067136 -4.3092341 -4.3201504 -4.3373332][-4.3486509 -4.3549371 -4.3643751 -4.3745561 -4.3789763 -4.3795972 -4.3786173 -4.3749423 -4.3697925 -4.3643365 -4.3566527 -4.3508329 -4.3503175 -4.3556576 -4.3639832]]...]
INFO - root - 2017-12-07 14:41:51.766185: step 9810, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.558 sec/batch; 67h:43m:00s remains)
INFO - root - 2017-12-07 14:42:08.183924: step 9820, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 1.549 sec/batch; 67h:18m:29s remains)
INFO - root - 2017-12-07 14:42:24.368147: step 9830, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.650 sec/batch; 71h:41m:51s remains)
INFO - root - 2017-12-07 14:42:40.619668: step 9840, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.582 sec/batch; 68h:42m:44s remains)
INFO - root - 2017-12-07 14:42:56.972429: step 9850, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.693 sec/batch; 73h:31m:51s remains)
INFO - root - 2017-12-07 14:43:13.122734: step 9860, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.677 sec/batch; 72h:50m:22s remains)
INFO - root - 2017-12-07 14:43:29.251928: step 9870, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.604 sec/batch; 69h:41m:30s remains)
INFO - root - 2017-12-07 14:43:45.646984: step 9880, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.704 sec/batch; 74h:02m:08s remains)
INFO - root - 2017-12-07 14:44:01.853225: step 9890, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.608 sec/batch; 69h:49m:37s remains)
INFO - root - 2017-12-07 14:44:18.129644: step 9900, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.677 sec/batch; 72h:51m:01s remains)
2017-12-07 14:44:19.428566: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1923957 -4.1684093 -4.141737 -4.1413884 -4.1608567 -4.1784921 -4.1979241 -4.2177787 -4.22866 -4.2187181 -4.1953473 -4.1786332 -4.1698427 -4.1423883 -4.1100307][-4.1923995 -4.1563997 -4.1158776 -4.1030531 -4.1172457 -4.140708 -4.1744251 -4.2053623 -4.2296844 -4.2343645 -4.2162528 -4.198874 -4.1842885 -4.1540408 -4.1253667][-4.1897936 -4.144083 -4.0914145 -4.0722547 -4.0889211 -4.1226344 -4.1647258 -4.1957593 -4.2190371 -4.2309122 -4.2229838 -4.213933 -4.2031679 -4.1800113 -4.1616979][-4.1874413 -4.140934 -4.0874085 -4.0771923 -4.1021185 -4.1325388 -4.1633229 -4.1852717 -4.2044196 -4.2175727 -4.2196045 -4.2233233 -4.2223024 -4.2096643 -4.1999865][-4.1893797 -4.160089 -4.124094 -4.126276 -4.144146 -4.15006 -4.1538839 -4.1555777 -4.171699 -4.1920028 -4.2049794 -4.2212462 -4.2311573 -4.2320333 -4.2324829][-4.1910362 -4.177774 -4.1496534 -4.1416707 -4.1397066 -4.1171503 -4.085691 -4.063199 -4.0865946 -4.1265411 -4.1570134 -4.1849003 -4.2064753 -4.2287359 -4.2480569][-4.1718969 -4.1582379 -4.1226974 -4.1020317 -4.0915742 -4.0559678 -3.9989347 -3.9539735 -3.9854128 -4.0454674 -4.0896425 -4.1267014 -4.1592512 -4.2032037 -4.2408733][-4.1503897 -4.1331959 -4.100493 -4.083787 -4.0781813 -4.0493689 -3.9914227 -3.9465029 -3.9731474 -4.0266204 -4.0605497 -4.0906239 -4.1258817 -4.1833463 -4.2304735][-4.1534667 -4.1390347 -4.1209288 -4.1137109 -4.11123 -4.0889678 -4.0433159 -4.0066557 -4.0284104 -4.0614982 -4.071394 -4.0852594 -4.1225419 -4.1867003 -4.2315912][-4.1926637 -4.1728745 -4.1611061 -4.1555228 -4.1434493 -4.1164327 -4.0751247 -4.0415249 -4.0608463 -4.0809231 -4.0808926 -4.09081 -4.1313138 -4.1929092 -4.2313919][-4.2130446 -4.1838684 -4.1736674 -4.1688561 -4.1502895 -4.121191 -4.08073 -4.0519013 -4.0674553 -4.084033 -4.0940523 -4.1147194 -4.1555896 -4.2054782 -4.231647][-4.1907797 -4.1612883 -4.159761 -4.1651373 -4.1494589 -4.1265879 -4.095695 -4.0690255 -4.0749636 -4.0877943 -4.1098747 -4.1414213 -4.1793017 -4.2190204 -4.2370086][-4.1379242 -4.107924 -4.1146317 -4.1340256 -4.1354871 -4.1329327 -4.1142459 -4.0885911 -4.086319 -4.0976419 -4.1295638 -4.1690555 -4.2021279 -4.2296667 -4.2378755][-4.1031752 -4.0688567 -4.0728059 -4.0963206 -4.1089182 -4.1267695 -4.1224513 -4.1055098 -4.1038804 -4.1119208 -4.1445951 -4.1821213 -4.2071209 -4.2226 -4.2215676][-4.1066494 -4.0746779 -4.0737982 -4.0924621 -4.1048632 -4.1297836 -4.13288 -4.1192546 -4.1173954 -4.12107 -4.1464295 -4.1793942 -4.1981215 -4.2065983 -4.2017579]]...]
INFO - root - 2017-12-07 14:44:35.609745: step 9910, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.667 sec/batch; 72h:24m:21s remains)
INFO - root - 2017-12-07 14:44:51.840633: step 9920, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.585 sec/batch; 68h:50m:03s remains)
INFO - root - 2017-12-07 14:45:08.132319: step 9930, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.644 sec/batch; 71h:24m:22s remains)
INFO - root - 2017-12-07 14:45:24.140766: step 9940, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 1.572 sec/batch; 68h:16m:19s remains)
INFO - root - 2017-12-07 14:45:40.516720: step 9950, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.581 sec/batch; 68h:39m:42s remains)
INFO - root - 2017-12-07 14:45:56.818863: step 9960, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.722 sec/batch; 74h:45m:31s remains)
INFO - root - 2017-12-07 14:46:12.978511: step 9970, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.567 sec/batch; 68h:02m:18s remains)
INFO - root - 2017-12-07 14:46:29.216723: step 9980, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.676 sec/batch; 72h:46m:04s remains)
INFO - root - 2017-12-07 14:46:45.738909: step 9990, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.620 sec/batch; 70h:19m:17s remains)
INFO - root - 2017-12-07 14:47:02.030964: step 10000, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 1.710 sec/batch; 74h:13m:43s remains)
2017-12-07 14:47:03.454482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2972803 -4.2968025 -4.298337 -4.2988405 -4.2964749 -4.2926931 -4.289865 -4.2878151 -4.2874622 -4.2882929 -4.2901034 -4.2936506 -4.2970262 -4.2989721 -4.299325][-4.2854934 -4.2804818 -4.2781448 -4.2755375 -4.2697096 -4.2636318 -4.2609625 -4.2599068 -4.2615581 -4.2650771 -4.2691951 -4.2756004 -4.2822752 -4.2883749 -4.2925406][-4.264163 -4.2496796 -4.2368808 -4.2261891 -4.2140217 -4.2068739 -4.207243 -4.208704 -4.2146239 -4.2230635 -4.2323904 -4.2447076 -4.2574549 -4.269599 -4.2792182][-4.2431455 -4.2154007 -4.1868114 -4.1624947 -4.1410227 -4.1321893 -4.1357431 -4.1409092 -4.1545391 -4.1717229 -4.1893516 -4.2103777 -4.2308612 -4.2488503 -4.2639613][-4.2262349 -4.186357 -4.1416979 -4.1014638 -4.0668116 -4.0530295 -4.0570645 -4.0627918 -4.0834956 -4.1148577 -4.1439295 -4.1753197 -4.2037048 -4.2272062 -4.2475114][-4.2067914 -4.1603308 -4.1068358 -4.0568724 -4.0104465 -3.9837179 -3.9728112 -3.9649653 -3.9886348 -4.039814 -4.0857635 -4.1326265 -4.1730566 -4.2035594 -4.2292156][-4.1803937 -4.1289592 -4.0751209 -4.0290437 -3.9836857 -3.944953 -3.9033587 -3.8623731 -3.882962 -3.9563611 -4.019289 -4.0831614 -4.1375408 -4.1758552 -4.2075791][-4.1567488 -4.1007333 -4.0505052 -4.0179672 -3.9888492 -3.957525 -3.9078643 -3.8501749 -3.8574905 -3.927073 -3.9872017 -4.0540094 -4.1154966 -4.1584873 -4.1942434][-4.154963 -4.099093 -4.0545163 -4.0366478 -4.0264177 -4.0148597 -3.9873168 -3.9507186 -3.9518096 -3.9915268 -4.0237193 -4.0730386 -4.1268587 -4.1656966 -4.1988363][-4.1824293 -4.1364727 -4.1009874 -4.0925608 -4.0934792 -4.0965891 -4.0901709 -4.0760655 -4.0797958 -4.0994024 -4.1093493 -4.1362038 -4.1728034 -4.20102 -4.2247529][-4.2152176 -4.1823015 -4.1581059 -4.1563888 -4.163826 -4.1737556 -4.1786485 -4.1769829 -4.1833224 -4.1942468 -4.1966195 -4.2107711 -4.2323003 -4.248693 -4.2610455][-4.241941 -4.2220311 -4.209435 -4.2132087 -4.225379 -4.238883 -4.24714 -4.2487154 -4.254499 -4.26186 -4.2656994 -4.27477 -4.2859068 -4.2910666 -4.29273][-4.2633076 -4.2523556 -4.2472873 -4.2517853 -4.2616916 -4.27126 -4.2759519 -4.2769718 -4.2817807 -4.2893062 -4.2972589 -4.3070912 -4.3153934 -4.3169441 -4.3140678][-4.2808237 -4.2755384 -4.2733812 -4.2759123 -4.2812572 -4.2859535 -4.2879915 -4.2887249 -4.2921624 -4.29742 -4.3037434 -4.31143 -4.3179011 -4.3194432 -4.3170662][-4.2913637 -4.2893376 -4.2886453 -4.2892542 -4.2910042 -4.2926445 -4.2931314 -4.2932744 -4.2946062 -4.2973619 -4.3011022 -4.3056746 -4.3096137 -4.3111405 -4.3101468]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch16/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch16/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 14:47:20.327044: step 10010, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 1.754 sec/batch; 76h:06m:11s remains)
INFO - root - 2017-12-07 14:47:36.497955: step 10020, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.571 sec/batch; 68h:11m:45s remains)
INFO - root - 2017-12-07 14:47:52.953644: step 10030, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.604 sec/batch; 69h:35m:33s remains)
INFO - root - 2017-12-07 14:48:09.128131: step 10040, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.670 sec/batch; 72h:27m:39s remains)
INFO - root - 2017-12-07 14:48:25.455026: step 10050, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.551 sec/batch; 67h:16m:40s remains)
INFO - root - 2017-12-07 14:48:41.757491: step 10060, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.681 sec/batch; 72h:55m:49s remains)
INFO - root - 2017-12-07 14:48:57.981956: step 10070, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 69h:23m:52s remains)
INFO - root - 2017-12-07 14:49:14.332743: step 10080, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 1.717 sec/batch; 74h:28m:11s remains)
INFO - root - 2017-12-07 14:49:30.594387: step 10090, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 1.598 sec/batch; 69h:19m:10s remains)
INFO - root - 2017-12-07 14:49:46.884501: step 10100, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.650 sec/batch; 71h:35m:22s remains)
2017-12-07 14:49:48.214425: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3405185 -4.3309536 -4.3105278 -4.2914386 -4.281209 -4.2726092 -4.2577353 -4.2230945 -4.20071 -4.2048187 -4.2320781 -4.2630339 -4.2870135 -4.3009849 -4.3075547][-4.3268595 -4.3148913 -4.2928782 -4.2712259 -4.257534 -4.2406383 -4.2167835 -4.1763849 -4.1542673 -4.1637731 -4.193418 -4.2271028 -4.2562847 -4.2780981 -4.2945004][-4.2960081 -4.2827473 -4.2604713 -4.2355704 -4.2168317 -4.1940126 -4.1628504 -4.1156731 -4.0959516 -4.1164904 -4.1544909 -4.1923695 -4.223865 -4.2530346 -4.279985][-4.2518344 -4.2299967 -4.2016649 -4.1684446 -4.1417561 -4.1167846 -4.0809078 -4.0238523 -4.0070615 -4.0460863 -4.1046839 -4.1519856 -4.1922464 -4.2339387 -4.27031][-4.1952453 -4.1671505 -4.1329861 -4.0878358 -4.0529408 -4.0195527 -3.9692492 -3.8948407 -3.8840542 -3.95363 -4.0431314 -4.1076159 -4.162395 -4.2174816 -4.2637434][-4.1228914 -4.094346 -4.065115 -4.0224075 -3.9880607 -3.94692 -3.8756762 -3.7739968 -3.7602282 -3.8551447 -3.9750075 -4.0612469 -4.1328712 -4.1997027 -4.2559247][-4.08329 -4.0509524 -4.0299358 -4.0008249 -3.9825041 -3.9483397 -3.8742318 -3.7559052 -3.7155952 -3.796958 -3.9250288 -4.0254521 -4.1054678 -4.1800771 -4.2450414][-4.1183839 -4.0874534 -4.0749855 -4.0567746 -4.0520048 -4.0384564 -3.9855568 -3.8871837 -3.8308578 -3.8693748 -3.9628909 -4.0458837 -4.1112275 -4.1757569 -4.239326][-4.1892891 -4.1573553 -4.143456 -4.131268 -4.1347146 -4.1378641 -4.1074114 -4.0409975 -3.9941335 -4.0108275 -4.0674276 -4.1200709 -4.1610408 -4.2064838 -4.2549109][-4.2483711 -4.2158527 -4.1999974 -4.1938529 -4.2065258 -4.2189012 -4.2001114 -4.1549773 -4.1265254 -4.1359363 -4.1675658 -4.1962171 -4.21919 -4.2495 -4.2828255][-4.2762904 -4.2458806 -4.2349162 -4.2361083 -4.2548823 -4.2689409 -4.2613244 -4.2318406 -4.2149835 -4.2265377 -4.2490635 -4.264679 -4.2752604 -4.2920818 -4.3095932][-4.2954745 -4.2721977 -4.2655096 -4.2684422 -4.28423 -4.294426 -4.292768 -4.2747512 -4.266788 -4.2799249 -4.2946367 -4.3025455 -4.3091149 -4.3191 -4.3270707][-4.3154364 -4.3012877 -4.2952509 -4.2954807 -4.302494 -4.3055344 -4.3058586 -4.2954946 -4.2923222 -4.3018217 -4.311275 -4.3155046 -4.3196278 -4.3260469 -4.3307309][-4.3206134 -4.3127012 -4.3060341 -4.3034196 -4.3059344 -4.30592 -4.3045917 -4.2974653 -4.2932982 -4.2974176 -4.3013673 -4.3041615 -4.3103857 -4.3189988 -4.324615][-4.3144817 -4.3104711 -4.3057194 -4.3030925 -4.3040895 -4.3035107 -4.3016672 -4.2962713 -4.2922554 -4.2927527 -4.2940412 -4.2965 -4.3039546 -4.3143086 -4.321178]]...]
INFO - root - 2017-12-07 14:50:04.614877: step 10110, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.680 sec/batch; 72h:52m:54s remains)
INFO - root - 2017-12-07 14:50:20.869074: step 10120, loss = 2.08, batch loss = 2.03 (10.6 examples/sec; 1.504 sec/batch; 65h:12m:37s remains)
INFO - root - 2017-12-07 14:50:37.101628: step 10130, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 1.631 sec/batch; 70h:43m:07s remains)
INFO - root - 2017-12-07 14:50:53.154423: step 10140, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.694 sec/batch; 73h:26m:52s remains)
INFO - root - 2017-12-07 14:51:09.501012: step 10150, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.556 sec/batch; 67h:29m:01s remains)
INFO - root - 2017-12-07 14:51:25.693309: step 10160, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.601 sec/batch; 69h:25m:46s remains)
INFO - root - 2017-12-07 14:51:41.860842: step 10170, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.564 sec/batch; 67h:47m:53s remains)
INFO - root - 2017-12-07 14:51:58.109038: step 10180, loss = 2.10, batch loss = 2.04 (10.9 examples/sec; 1.462 sec/batch; 63h:22m:21s remains)
INFO - root - 2017-12-07 14:52:14.360824: step 10190, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.639 sec/batch; 71h:02m:56s remains)
INFO - root - 2017-12-07 14:52:30.660817: step 10200, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.589 sec/batch; 68h:51m:37s remains)
2017-12-07 14:52:32.040812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.263525 -4.2774878 -4.2951093 -4.3149824 -4.3272924 -4.3291416 -4.3184457 -4.289845 -4.2611365 -4.2466187 -4.2419128 -4.2427554 -4.2523561 -4.2633219 -4.2650704][-4.2196069 -4.2284527 -4.2504144 -4.2802067 -4.3003969 -4.3121061 -4.3141227 -4.2969828 -4.27391 -4.2626467 -4.2569928 -4.2548718 -4.2627563 -4.2704315 -4.2705865][-4.1800957 -4.1853676 -4.205873 -4.237545 -4.2611856 -4.2772045 -4.2896652 -4.2859426 -4.2769976 -4.2775359 -4.2761455 -4.272048 -4.2773471 -4.28224 -4.2793074][-4.1415672 -4.146019 -4.1659851 -4.19616 -4.22003 -4.2332439 -4.2468791 -4.2491288 -4.2527509 -4.2694993 -4.2775526 -4.2754674 -4.2800789 -4.2835116 -4.2812548][-4.1253061 -4.1232119 -4.1337924 -4.1546783 -4.1727943 -4.178391 -4.1858454 -4.1897578 -4.2024226 -4.2314234 -4.2529535 -4.2590928 -4.2660031 -4.270196 -4.2733574][-4.1411991 -4.1377077 -4.1364241 -4.1402264 -4.1407695 -4.1226397 -4.1078844 -4.1072955 -4.1270704 -4.1671448 -4.2034955 -4.2224059 -4.2329526 -4.2386985 -4.2487378][-4.1678548 -4.1751103 -4.1727514 -4.1638775 -4.1444106 -4.0994291 -4.053659 -4.0328145 -4.050251 -4.09618 -4.1443453 -4.177012 -4.1921353 -4.201561 -4.2154255][-4.191361 -4.2061191 -4.2111378 -4.2047811 -4.1841311 -4.1346149 -4.0739512 -4.0329609 -4.0322332 -4.0639787 -4.1026211 -4.1335449 -4.1516976 -4.1662507 -4.1834936][-4.228899 -4.2392497 -4.2467718 -4.2430449 -4.2284088 -4.1902571 -4.1404376 -4.1038055 -4.0946097 -4.1031461 -4.1130009 -4.1213202 -4.130569 -4.1430044 -4.16076][-4.2675495 -4.2722926 -4.2773404 -4.275104 -4.2668447 -4.2421207 -4.2091756 -4.184329 -4.1741996 -4.1691465 -4.1576018 -4.1471357 -4.1441574 -4.1470423 -4.1560607][-4.295188 -4.2928915 -4.294569 -4.2948089 -4.2938805 -4.2819357 -4.2647367 -4.2502952 -4.2429094 -4.2339311 -4.2161231 -4.1972055 -4.1846824 -4.1760182 -4.1725059][-4.3078756 -4.2978492 -4.2962012 -4.2984414 -4.3022976 -4.3005414 -4.295866 -4.2897472 -4.286633 -4.2799067 -4.265595 -4.2507067 -4.2356882 -4.2188478 -4.204411][-4.3048334 -4.2906437 -4.2854319 -4.2879152 -4.2938242 -4.2977638 -4.3006172 -4.3006039 -4.30303 -4.3029008 -4.2965689 -4.2885857 -4.2759171 -4.256516 -4.235518][-4.2959142 -4.27588 -4.2649789 -4.26509 -4.2709694 -4.2775145 -4.2843976 -4.2890158 -4.2946162 -4.3012266 -4.3034315 -4.3025036 -4.2940197 -4.2745562 -4.2482438][-4.2879715 -4.264607 -4.2480173 -4.24355 -4.2453184 -4.2513218 -4.2588043 -4.2660632 -4.2743373 -4.2858067 -4.2945652 -4.2968464 -4.2896428 -4.2679133 -4.238584]]...]
INFO - root - 2017-12-07 14:52:48.195328: step 10210, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 1.519 sec/batch; 65h:49m:46s remains)
INFO - root - 2017-12-07 14:53:04.528610: step 10220, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.744 sec/batch; 75h:35m:35s remains)
INFO - root - 2017-12-07 14:53:20.677714: step 10230, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.591 sec/batch; 68h:56m:05s remains)
INFO - root - 2017-12-07 14:53:37.116765: step 10240, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.654 sec/batch; 71h:41m:13s remains)
INFO - root - 2017-12-07 14:53:53.417288: step 10250, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.622 sec/batch; 70h:18m:17s remains)
INFO - root - 2017-12-07 14:54:09.709410: step 10260, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.646 sec/batch; 71h:18m:51s remains)
INFO - root - 2017-12-07 14:54:25.868808: step 10270, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 1.648 sec/batch; 71h:24m:22s remains)
INFO - root - 2017-12-07 14:54:42.281352: step 10280, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.618 sec/batch; 70h:06m:18s remains)
INFO - root - 2017-12-07 14:54:58.459361: step 10290, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.638 sec/batch; 70h:58m:33s remains)
INFO - root - 2017-12-07 14:55:14.571654: step 10300, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 1.459 sec/batch; 63h:12m:28s remains)
2017-12-07 14:55:15.904129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3124342 -4.3200793 -4.3319912 -4.3446336 -4.3456364 -4.3347239 -4.3173084 -4.3010168 -4.2932792 -4.29592 -4.3066921 -4.3199053 -4.334136 -4.3487115 -4.3594065][-4.3151417 -4.3243604 -4.3360758 -4.3468971 -4.3447332 -4.3290086 -4.3060026 -4.2856236 -4.2749929 -4.2765431 -4.2889562 -4.3041072 -4.3188834 -4.3348045 -4.3473125][-4.3065033 -4.3168106 -4.327394 -4.3352437 -4.3292365 -4.3082027 -4.2780542 -4.2512679 -4.238132 -4.241765 -4.2582188 -4.2762284 -4.293314 -4.3116736 -4.3263869][-4.2969589 -4.3105946 -4.3189025 -4.3209782 -4.3090544 -4.281199 -4.2416258 -4.20773 -4.1964722 -4.2101989 -4.23757 -4.26058 -4.2791128 -4.2963495 -4.3089371][-4.2773914 -4.2901392 -4.2957759 -4.293828 -4.2788396 -4.2465372 -4.1997223 -4.1625633 -4.1602359 -4.1920595 -4.2349057 -4.2637348 -4.2816162 -4.2942896 -4.2991557][-4.2400908 -4.2515931 -4.2557607 -4.2512431 -4.232317 -4.1936884 -4.1397123 -4.1012979 -4.1107025 -4.1586084 -4.2144785 -4.249598 -4.2691464 -4.2820435 -4.2847705][-4.2012506 -4.2051773 -4.1997209 -4.1853609 -4.1553521 -4.1043253 -4.040257 -4.0020189 -4.0264845 -4.090991 -4.158412 -4.202302 -4.2293391 -4.2503667 -4.25732][-4.1638327 -4.1578307 -4.1448956 -4.1240668 -4.0859551 -4.0230894 -3.9472995 -3.906899 -3.9438884 -4.0200038 -4.0938029 -4.1456404 -4.1802092 -4.2069416 -4.2154093][-4.1514816 -4.1480956 -4.1455016 -4.1380672 -4.1124196 -4.0614028 -3.9955029 -3.9557834 -3.9809771 -4.0374718 -4.0911417 -4.1308174 -4.1590409 -4.1788597 -4.1770272][-4.1882305 -4.1936517 -4.2059751 -4.2170353 -4.2119226 -4.1847444 -4.1406126 -4.105 -4.1058822 -4.125658 -4.1458926 -4.16029 -4.170085 -4.1729407 -4.1557288][-4.2305384 -4.2363334 -4.2494311 -4.2636433 -4.2683215 -4.2601328 -4.2403445 -4.2173367 -4.2094984 -4.2095165 -4.2097068 -4.2078662 -4.203506 -4.1921663 -4.1659117][-4.2387352 -4.2419477 -4.2503242 -4.2606711 -4.2674637 -4.2710156 -4.2695971 -4.2622838 -4.2591186 -4.2585216 -4.257288 -4.2523942 -4.2434068 -4.2292972 -4.2077303][-4.2156324 -4.2170835 -4.2219453 -4.2295113 -4.2382441 -4.249156 -4.2582822 -4.2618251 -4.266573 -4.272769 -4.2779117 -4.2771106 -4.2702813 -4.2601566 -4.2486787][-4.2083235 -4.2069879 -4.2066078 -4.210165 -4.2177582 -4.2306929 -4.2438169 -4.2536578 -4.2642021 -4.2752109 -4.2846971 -4.2878838 -4.2843008 -4.2797923 -4.2770281][-4.228004 -4.225317 -4.2222652 -4.2230072 -4.2299819 -4.2427711 -4.2545066 -4.2635465 -4.2731733 -4.2834949 -4.2931242 -4.2982674 -4.2982025 -4.2970586 -4.2972555]]...]
INFO - root - 2017-12-07 14:55:32.229464: step 10310, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 1.602 sec/batch; 69h:23m:44s remains)
INFO - root - 2017-12-07 14:55:48.458511: step 10320, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.636 sec/batch; 70h:50m:34s remains)
INFO - root - 2017-12-07 14:56:04.653748: step 10330, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.601 sec/batch; 69h:19m:25s remains)
INFO - root - 2017-12-07 14:56:21.025570: step 10340, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.622 sec/batch; 70h:14m:48s remains)
INFO - root - 2017-12-07 14:56:37.313375: step 10350, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.584 sec/batch; 68h:37m:03s remains)
INFO - root - 2017-12-07 14:56:53.417139: step 10360, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.706 sec/batch; 73h:51m:20s remains)
INFO - root - 2017-12-07 14:57:09.537234: step 10370, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.533 sec/batch; 66h:23m:38s remains)
INFO - root - 2017-12-07 14:57:25.822489: step 10380, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.673 sec/batch; 72h:27m:02s remains)
INFO - root - 2017-12-07 14:57:41.971794: step 10390, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.614 sec/batch; 69h:53m:33s remains)
INFO - root - 2017-12-07 14:57:58.309493: step 10400, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 68h:48m:43s remains)
2017-12-07 14:57:59.658497: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3321834 -4.3249874 -4.3183513 -4.3179493 -4.3222904 -4.3269925 -4.3261547 -4.322051 -4.3195419 -4.3184581 -4.3199224 -4.3247843 -4.3313603 -4.33944 -4.3435268][-4.3227019 -4.3096027 -4.2977624 -4.2978587 -4.3072453 -4.3160181 -4.3136034 -4.3039727 -4.2959342 -4.2915306 -4.2920761 -4.299686 -4.3128328 -4.3279924 -4.3378692][-4.3020005 -4.2854223 -4.2737374 -4.2775745 -4.2896819 -4.2997642 -4.2909784 -4.2706032 -4.2542686 -4.2477841 -4.2508693 -4.2632384 -4.2825809 -4.3052831 -4.3223553][-4.2848787 -4.2710648 -4.2653747 -4.273592 -4.2824459 -4.2821736 -4.2569704 -4.2217956 -4.1973805 -4.190165 -4.1980295 -4.2185292 -4.2456865 -4.2751055 -4.2996397][-4.2784128 -4.2688618 -4.26712 -4.2769346 -4.275043 -4.2523808 -4.2027626 -4.1514606 -4.1189003 -4.1129336 -4.1326222 -4.1649094 -4.2019596 -4.2427306 -4.2774744][-4.2660365 -4.2565465 -4.2547746 -4.2630954 -4.2471571 -4.1940951 -4.1124058 -4.0378342 -4.0096312 -4.0205464 -4.0614619 -4.1151409 -4.1686826 -4.2239084 -4.2693872][-4.2467747 -4.2343884 -4.2271376 -4.2249913 -4.1860881 -4.1008577 -3.990993 -3.9041748 -3.9047227 -3.9569359 -4.02873 -4.1026616 -4.1695166 -4.2294788 -4.2754359][-4.2270036 -4.2019782 -4.1846523 -4.1657476 -4.1066785 -4.0071678 -3.9031885 -3.8486261 -3.8942084 -3.975296 -4.0592823 -4.1378851 -4.20228 -4.2523365 -4.2903452][-4.2002435 -4.1656709 -4.1429892 -4.1179023 -4.0651779 -3.9872289 -3.9318564 -3.924814 -3.9854636 -4.0554481 -4.1269832 -4.1939468 -4.2446485 -4.2806292 -4.3076496][-4.185081 -4.1527514 -4.1314254 -4.1160264 -4.0931425 -4.0569992 -4.0433764 -4.0550513 -4.1004415 -4.1451464 -4.1971574 -4.2453208 -4.2790818 -4.3026147 -4.3186917][-4.2066364 -4.183723 -4.1671133 -4.1648636 -4.1641326 -4.1559448 -4.1619725 -4.175324 -4.2022805 -4.2275543 -4.259479 -4.2910395 -4.310195 -4.3207107 -4.3265004][-4.2432852 -4.2314172 -4.2200909 -4.222126 -4.2308183 -4.2367768 -4.2500935 -4.2613392 -4.2721257 -4.2854152 -4.3015537 -4.3219423 -4.3334885 -4.3354836 -4.332943][-4.283668 -4.2816143 -4.2788763 -4.286356 -4.3010073 -4.3091083 -4.318881 -4.3205366 -4.3184857 -4.3226619 -4.3268318 -4.3386464 -4.3433356 -4.3391933 -4.3337235][-4.3070469 -4.3121343 -4.3168979 -4.3283787 -4.3400693 -4.3431625 -4.3463082 -4.3406029 -4.3321218 -4.332963 -4.33292 -4.3398781 -4.3402128 -4.3342257 -4.3304677][-4.3164396 -4.3239846 -4.3320651 -4.3403783 -4.3443704 -4.3443193 -4.3457489 -4.3404593 -4.3326912 -4.3301253 -4.3269277 -4.329524 -4.3291554 -4.3260818 -4.3269506]]...]
INFO - root - 2017-12-07 14:58:15.951308: step 10410, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 1.452 sec/batch; 62h:50m:46s remains)
INFO - root - 2017-12-07 14:58:32.028132: step 10420, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 1.627 sec/batch; 70h:24m:33s remains)
INFO - root - 2017-12-07 14:58:48.086969: step 10430, loss = 2.06, batch loss = 2.01 (10.5 examples/sec; 1.524 sec/batch; 65h:58m:11s remains)
INFO - root - 2017-12-07 14:59:04.496989: step 10440, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.599 sec/batch; 69h:13m:33s remains)
INFO - root - 2017-12-07 14:59:20.630611: step 10450, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.647 sec/batch; 71h:17m:20s remains)
INFO - root - 2017-12-07 14:59:36.806269: step 10460, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.564 sec/batch; 67h:39m:48s remains)
INFO - root - 2017-12-07 14:59:53.217228: step 10470, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.722 sec/batch; 74h:30m:55s remains)
INFO - root - 2017-12-07 15:00:09.450424: step 10480, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.646 sec/batch; 71h:12m:24s remains)
INFO - root - 2017-12-07 15:00:25.736860: step 10490, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.684 sec/batch; 72h:52m:23s remains)
INFO - root - 2017-12-07 15:00:41.839204: step 10500, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 1.479 sec/batch; 64h:00m:00s remains)
2017-12-07 15:00:43.184930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.216733 -4.2034249 -4.2070975 -4.2086434 -4.212729 -4.2222714 -4.2291961 -4.2390995 -4.25069 -4.256639 -4.2526207 -4.242846 -4.2329149 -4.223762 -4.2144327][-4.1878271 -4.1821818 -4.1916795 -4.2045817 -4.2184329 -4.2270947 -4.2296925 -4.2378964 -4.2461782 -4.2509613 -4.2514215 -4.2408204 -4.2263055 -4.2129607 -4.201499][-4.1596313 -4.1598592 -4.1753297 -4.200439 -4.2197194 -4.2238 -4.22305 -4.2249846 -4.2262726 -4.2307391 -4.2355595 -4.2249522 -4.2074347 -4.1887422 -4.1738386][-4.1646695 -4.1659408 -4.1825681 -4.2084227 -4.2164035 -4.2122612 -4.2084832 -4.2034106 -4.2040439 -4.2155185 -4.2307348 -4.2222056 -4.2025313 -4.1821008 -4.1671824][-4.190846 -4.1939 -4.2111259 -4.229085 -4.2228804 -4.2084327 -4.1913433 -4.1716132 -4.1689186 -4.1900439 -4.2223778 -4.2260761 -4.2110748 -4.1928453 -4.1821437][-4.2074962 -4.2118363 -4.2267056 -4.2296891 -4.2063808 -4.1758618 -4.1390343 -4.1032825 -4.1001825 -4.1374874 -4.1921248 -4.2093372 -4.200871 -4.1847034 -4.1766343][-4.2031465 -4.2033644 -4.2077723 -4.1883621 -4.1416097 -4.0881252 -4.0296731 -3.9859676 -3.9968147 -4.0615859 -4.1390724 -4.1668854 -4.1563873 -4.1326265 -4.1261024][-4.2106342 -4.2076535 -4.2008619 -4.1645036 -4.1064768 -4.0377274 -3.9700258 -3.933481 -3.9564023 -4.0323353 -4.1048059 -4.1300917 -4.1137733 -4.0811663 -4.074193][-4.235889 -4.2353134 -4.2276516 -4.1894565 -4.1332183 -4.0626583 -3.9949906 -3.9708996 -4.0046849 -4.0748224 -4.1328254 -4.1577139 -4.1458483 -4.1124854 -4.0985003][-4.2516389 -4.2575126 -4.2517834 -4.2179337 -4.1641593 -4.0911469 -4.0287681 -4.0239706 -4.0707579 -4.136723 -4.1945024 -4.2275734 -4.2252254 -4.1938004 -4.1712627][-4.2413521 -4.2589211 -4.2634425 -4.2401605 -4.1913047 -4.1170797 -4.06174 -4.0682774 -4.1131654 -4.1701026 -4.2311606 -4.2723846 -4.2753077 -4.2464604 -4.2179422][-4.2271938 -4.2528734 -4.2623982 -4.2490373 -4.2042327 -4.1317682 -4.077858 -4.084703 -4.1236567 -4.1760545 -4.2395172 -4.2844777 -4.2912936 -4.2671628 -4.235642][-4.2410703 -4.2664289 -4.2775903 -4.2680731 -4.2283263 -4.167891 -4.1222482 -4.1275711 -4.1598878 -4.207943 -4.2622919 -4.2988405 -4.3052864 -4.2851987 -4.2509179][-4.2772036 -4.2969456 -4.3076816 -4.3036041 -4.2769341 -4.236268 -4.203424 -4.2041731 -4.2271776 -4.2643366 -4.3012218 -4.3249812 -4.3293991 -4.3130884 -4.2821636][-4.3121052 -4.3243 -4.3304772 -4.3287191 -4.3141704 -4.2913265 -4.2729673 -4.2732754 -4.2874408 -4.3103933 -4.3307791 -4.3416843 -4.3428197 -4.3325133 -4.3110695]]...]
INFO - root - 2017-12-07 15:00:59.449708: step 10510, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.543 sec/batch; 66h:45m:47s remains)
INFO - root - 2017-12-07 15:01:15.867324: step 10520, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.716 sec/batch; 74h:13m:39s remains)
INFO - root - 2017-12-07 15:01:32.251737: step 10530, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.569 sec/batch; 67h:51m:37s remains)
INFO - root - 2017-12-07 15:01:48.393550: step 10540, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.578 sec/batch; 68h:15m:21s remains)
INFO - root - 2017-12-07 15:02:04.571560: step 10550, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.566 sec/batch; 67h:42m:43s remains)
INFO - root - 2017-12-07 15:02:20.780687: step 10560, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.591 sec/batch; 68h:48m:07s remains)
INFO - root - 2017-12-07 15:02:36.964453: step 10570, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.591 sec/batch; 68h:46m:56s remains)
INFO - root - 2017-12-07 15:02:53.184493: step 10580, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.553 sec/batch; 67h:08m:14s remains)
INFO - root - 2017-12-07 15:03:09.483440: step 10590, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.724 sec/batch; 74h:32m:06s remains)
INFO - root - 2017-12-07 15:03:25.735016: step 10600, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.576 sec/batch; 68h:07m:40s remains)
2017-12-07 15:03:26.979826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3299093 -4.3291817 -4.3276334 -4.3238678 -4.3172851 -4.311872 -4.3091836 -4.3059692 -4.3003511 -4.2928977 -4.2812252 -4.272984 -4.2747383 -4.2827845 -4.2977734][-4.3444118 -4.3421454 -4.3386445 -4.3312578 -4.3177838 -4.30398 -4.2951393 -4.2888374 -4.2842917 -4.2791648 -4.2715979 -4.2696729 -4.2788777 -4.2925949 -4.308094][-4.3516655 -4.3453035 -4.3380256 -4.3245878 -4.305172 -4.2831211 -4.2623529 -4.2457337 -4.2388396 -4.2375488 -4.2369714 -4.2433043 -4.2622843 -4.2859664 -4.3079357][-4.3524456 -4.3397522 -4.3251534 -4.3036256 -4.2781906 -4.2489944 -4.2178845 -4.1901865 -4.1799707 -4.1855125 -4.1941166 -4.2067084 -4.234067 -4.2677383 -4.2986941][-4.3399887 -4.3195381 -4.296102 -4.2660818 -4.2341232 -4.2036 -4.1709609 -4.1359344 -4.1180091 -4.1256728 -4.1427917 -4.1577406 -4.1907682 -4.2312164 -4.2719417][-4.3141375 -4.2820091 -4.2473111 -4.2085795 -4.1682954 -4.13359 -4.0974627 -4.0517745 -4.0226583 -4.0316586 -4.0628772 -4.091156 -4.1339641 -4.1840773 -4.2364469][-4.2857342 -4.2387457 -4.1908774 -4.1443949 -4.0966897 -4.0472908 -3.9962451 -3.9264154 -3.8716266 -3.8807769 -3.9333751 -3.9909382 -4.0578823 -4.1285219 -4.1977916][-4.2776151 -4.2236886 -4.1698942 -4.1217184 -4.0718632 -4.0101905 -3.940969 -3.8473332 -3.7706225 -3.7793746 -3.8501523 -3.931917 -4.021678 -4.10938 -4.1865549][-4.2984 -4.2532554 -4.2060452 -4.1660409 -4.1272531 -4.0766649 -4.0173068 -3.937048 -3.8736835 -3.8813434 -3.935379 -4.0017614 -4.0818691 -4.1600718 -4.2222347][-4.33306 -4.30872 -4.2802172 -4.2518325 -4.2227764 -4.1875162 -4.1462097 -4.0933418 -4.0533056 -4.062799 -4.0987267 -4.1382432 -4.1869559 -4.2359395 -4.2737451][-4.3576918 -4.3455696 -4.3313417 -4.3159432 -4.2977433 -4.2758484 -4.2519279 -4.2189574 -4.1933851 -4.1980958 -4.2171631 -4.2389326 -4.2646379 -4.288763 -4.3079925][-4.37334 -4.3665776 -4.3597736 -4.3534074 -4.3445425 -4.3343172 -4.3245263 -4.3063235 -4.2897549 -4.2897434 -4.2953496 -4.3006287 -4.3094592 -4.317008 -4.3257475][-4.3779545 -4.3762321 -4.3742 -4.3727536 -4.3691025 -4.3643026 -4.3602352 -4.3494511 -4.3392396 -4.3383579 -4.3378744 -4.3340325 -4.330832 -4.3270411 -4.328948][-4.3710647 -4.3730345 -4.374805 -4.376164 -4.3750687 -4.3719783 -4.3680305 -4.3607278 -4.3569751 -4.3584423 -4.3555241 -4.34752 -4.3374681 -4.3269649 -4.3243241][-4.3645434 -4.36787 -4.3707032 -4.3732142 -4.3742967 -4.3734 -4.3699346 -4.3650756 -4.3655415 -4.3674479 -4.3608236 -4.3482413 -4.3331795 -4.3201509 -4.3173437]]...]
INFO - root - 2017-12-07 15:03:43.430977: step 10610, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.578 sec/batch; 68h:12m:07s remains)
INFO - root - 2017-12-07 15:03:59.631411: step 10620, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.661 sec/batch; 71h:48m:29s remains)
INFO - root - 2017-12-07 15:04:16.049306: step 10630, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.574 sec/batch; 68h:01m:32s remains)
INFO - root - 2017-12-07 15:04:32.323080: step 10640, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 1.670 sec/batch; 72h:11m:58s remains)
INFO - root - 2017-12-07 15:04:48.521847: step 10650, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.567 sec/batch; 67h:42m:39s remains)
INFO - root - 2017-12-07 15:05:04.721697: step 10660, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.666 sec/batch; 71h:59m:09s remains)
INFO - root - 2017-12-07 15:05:20.955541: step 10670, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.583 sec/batch; 68h:24m:02s remains)
INFO - root - 2017-12-07 15:05:37.295453: step 10680, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 1.607 sec/batch; 69h:27m:16s remains)
INFO - root - 2017-12-07 15:05:53.690733: step 10690, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.709 sec/batch; 73h:50m:23s remains)
INFO - root - 2017-12-07 15:06:09.919935: step 10700, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.643 sec/batch; 70h:59m:02s remains)
2017-12-07 15:06:11.396920: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31994 -4.3279724 -4.3318858 -4.3335638 -4.3336182 -4.332056 -4.3299851 -4.3285179 -4.3274212 -4.3266845 -4.3257685 -4.3244739 -4.3245206 -4.3265266 -4.3295503][-4.3129678 -4.3234749 -4.327867 -4.327467 -4.3242817 -4.31863 -4.3129344 -4.3086853 -4.3061004 -4.3050776 -4.3040295 -4.3021231 -4.3033752 -4.3088536 -4.3163443][-4.3084064 -4.3176589 -4.3198996 -4.3159232 -4.307395 -4.2949433 -4.2819252 -4.2710352 -4.2645659 -4.2635665 -4.2655182 -4.265727 -4.272 -4.284996 -4.30025][-4.2967639 -4.3028111 -4.3013868 -4.2915893 -4.2767639 -4.255446 -4.2316394 -4.2108197 -4.1997604 -4.2013741 -4.2098484 -4.2157178 -4.2289739 -4.2509604 -4.2756481][-4.269413 -4.269959 -4.2641435 -4.249742 -4.2297053 -4.1993585 -4.163815 -4.1312122 -4.1138597 -4.1185102 -4.1347675 -4.1473241 -4.1681671 -4.1987991 -4.2318082][-4.2234025 -4.219965 -4.2125134 -4.1966891 -4.1756215 -4.1414742 -4.0984783 -4.0534062 -4.0262003 -4.0314393 -4.052969 -4.0708933 -4.0973439 -4.135849 -4.1737223][-4.1721764 -4.166625 -4.1603346 -4.1469464 -4.1283655 -4.0932908 -4.0479097 -3.9981103 -3.9665265 -3.9717414 -3.9959044 -4.0154333 -4.0393858 -4.0760837 -4.1109042][-4.128191 -4.11841 -4.1112595 -4.0991063 -4.0834408 -4.051003 -4.0103912 -3.9677677 -3.9441543 -3.9547815 -3.9824564 -4.0021844 -4.0183992 -4.0434046 -4.0674052][-4.1147132 -4.0997906 -4.0930705 -4.0867014 -4.0802007 -4.0590639 -4.0318179 -4.0059414 -3.9950049 -4.0087233 -4.0331311 -4.0484624 -4.05503 -4.0637174 -4.070323][-4.1259751 -4.1150002 -4.1165338 -4.121407 -4.1265855 -4.1197762 -4.108305 -4.0965242 -4.0926151 -4.1012406 -4.1134329 -4.116066 -4.1099262 -4.1013784 -4.089787][-4.156086 -4.1535215 -4.1622143 -4.1733832 -4.1848841 -4.1872892 -4.18404 -4.1778717 -4.1743236 -4.1763039 -4.1788883 -4.1721215 -4.1549034 -4.1336813 -4.1110129][-4.205195 -4.20688 -4.21571 -4.2263403 -4.2376451 -4.2418933 -4.2404943 -4.2356153 -4.2305346 -4.2277894 -4.2253647 -4.2140718 -4.1935506 -4.1708808 -4.1511884][-4.2419519 -4.2455459 -4.2534842 -4.26067 -4.2677126 -4.2710762 -4.271246 -4.2690659 -4.26569 -4.2633848 -4.26096 -4.2504835 -4.2314277 -4.2115521 -4.1973925][-4.2550244 -4.2590094 -4.2657189 -4.2684093 -4.2719631 -4.27586 -4.2786651 -4.2806644 -4.2813606 -4.2818446 -4.2810249 -4.27409 -4.2604613 -4.2457843 -4.2360454][-4.269331 -4.2727623 -4.278089 -4.278204 -4.2800465 -4.2831597 -4.2860451 -4.2885752 -4.2901354 -4.2913914 -4.2916813 -4.2878547 -4.2800217 -4.2720995 -4.2671671]]...]
INFO - root - 2017-12-07 15:06:27.567069: step 10710, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.577 sec/batch; 68h:08m:23s remains)
INFO - root - 2017-12-07 15:06:43.847163: step 10720, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.624 sec/batch; 70h:09m:19s remains)
INFO - root - 2017-12-07 15:07:00.281834: step 10730, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.722 sec/batch; 74h:22m:55s remains)
INFO - root - 2017-12-07 15:07:16.494996: step 10740, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.589 sec/batch; 68h:38m:52s remains)
INFO - root - 2017-12-07 15:07:32.824050: step 10750, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.708 sec/batch; 73h:46m:38s remains)
INFO - root - 2017-12-07 15:07:49.213866: step 10760, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.630 sec/batch; 70h:23m:06s remains)
INFO - root - 2017-12-07 15:08:05.550563: step 10770, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.688 sec/batch; 72h:53m:54s remains)
INFO - root - 2017-12-07 15:08:21.560044: step 10780, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.561 sec/batch; 67h:25m:58s remains)
INFO - root - 2017-12-07 15:08:37.869442: step 10790, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.622 sec/batch; 70h:02m:50s remains)
INFO - root - 2017-12-07 15:08:54.128231: step 10800, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.690 sec/batch; 72h:57m:46s remains)
2017-12-07 15:08:55.510564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2405462 -4.2531714 -4.2796369 -4.305666 -4.3136406 -4.2970595 -4.2820754 -4.2784853 -4.2801428 -4.2840033 -4.280798 -4.2709217 -4.2596159 -4.2502556 -4.2419558][-4.2110596 -4.2260818 -4.2604651 -4.2904596 -4.29562 -4.2678914 -4.2441435 -4.2447071 -4.257627 -4.2713385 -4.2793274 -4.2755518 -4.2625141 -4.2517595 -4.2419496][-4.2002306 -4.2153687 -4.2506666 -4.2748432 -4.2682819 -4.22483 -4.1850085 -4.1888747 -4.2183161 -4.2472024 -4.2684751 -4.2703381 -4.2560506 -4.2427645 -4.2322316][-4.2288918 -4.2362428 -4.2592235 -4.2685537 -4.2405348 -4.1764326 -4.1201863 -4.1275816 -4.180872 -4.22826 -4.2614074 -4.267005 -4.2534833 -4.2416558 -4.2344141][-4.2741771 -4.2710114 -4.2730646 -4.2610435 -4.2134476 -4.1193848 -4.0283666 -4.035686 -4.1250548 -4.2047887 -4.2539911 -4.2638507 -4.2548909 -4.2490864 -4.2468486][-4.2975092 -4.28927 -4.2781453 -4.2490325 -4.1825342 -4.0581894 -3.9046702 -3.8899305 -4.0332222 -4.1677408 -4.2419052 -4.2630115 -4.2589121 -4.2544985 -4.2545209][-4.2928224 -4.2841783 -4.272006 -4.2365541 -4.159317 -4.0236921 -3.8280139 -3.7625446 -3.9365649 -4.1213155 -4.2233229 -4.2544861 -4.2574806 -4.2519612 -4.246347][-4.2867384 -4.2756567 -4.2602706 -4.2277083 -4.1650352 -4.0637364 -3.9030213 -3.8163428 -3.93694 -4.1076279 -4.2125211 -4.2467189 -4.2505331 -4.2437077 -4.2303076][-4.2860785 -4.27607 -4.2567225 -4.2279549 -4.1890316 -4.1351814 -4.036685 -3.9717958 -4.0231695 -4.1375046 -4.2167854 -4.2410722 -4.2383304 -4.22867 -4.2135329][-4.2824836 -4.2778845 -4.2584233 -4.2322731 -4.2074108 -4.1843767 -4.1350322 -4.0951915 -4.1143923 -4.1831574 -4.2315054 -4.2404819 -4.2279954 -4.2081351 -4.191699][-4.2728548 -4.2742772 -4.2597847 -4.2400489 -4.2274208 -4.2267179 -4.208344 -4.1878548 -4.1933117 -4.2347407 -4.25982 -4.2540841 -4.2319851 -4.2028046 -4.1814504][-4.2734413 -4.2743168 -4.26542 -4.2527628 -4.2500911 -4.2621193 -4.2590132 -4.2492805 -4.2511535 -4.274878 -4.2907534 -4.2831392 -4.2610221 -4.2299142 -4.2002048][-4.2892685 -4.2887626 -4.2809453 -4.2715559 -4.27362 -4.2894645 -4.2940078 -4.2907085 -4.2903781 -4.301024 -4.3113971 -4.3098531 -4.2959414 -4.2711983 -4.2425928][-4.306612 -4.3063436 -4.2997265 -4.2912235 -4.2944665 -4.3063555 -4.3128166 -4.3143673 -4.3137593 -4.3173547 -4.3228855 -4.3239322 -4.3190732 -4.3060222 -4.2907124][-4.3187885 -4.3161983 -4.3108621 -4.3047113 -4.3075728 -4.3142214 -4.31768 -4.3199768 -4.3209267 -4.3236585 -4.328052 -4.3296404 -4.3288274 -4.3244476 -4.3195524]]...]
INFO - root - 2017-12-07 15:09:11.716067: step 10810, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 67h:54m:36s remains)
INFO - root - 2017-12-07 15:09:27.916355: step 10820, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.609 sec/batch; 69h:27m:46s remains)
INFO - root - 2017-12-07 15:09:44.292587: step 10830, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.653 sec/batch; 71h:22m:19s remains)
INFO - root - 2017-12-07 15:10:00.521934: step 10840, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.556 sec/batch; 67h:10m:22s remains)
INFO - root - 2017-12-07 15:10:16.814994: step 10850, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.677 sec/batch; 72h:23m:19s remains)
INFO - root - 2017-12-07 15:10:32.964304: step 10860, loss = 2.09, batch loss = 2.04 (10.0 examples/sec; 1.593 sec/batch; 68h:45m:32s remains)
INFO - root - 2017-12-07 15:10:49.303860: step 10870, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.651 sec/batch; 71h:16m:15s remains)
INFO - root - 2017-12-07 15:11:05.505223: step 10880, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 67h:52m:55s remains)
INFO - root - 2017-12-07 15:11:21.920194: step 10890, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.698 sec/batch; 73h:15m:24s remains)
INFO - root - 2017-12-07 15:11:37.978092: step 10900, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.530 sec/batch; 66h:02m:40s remains)
2017-12-07 15:11:39.450757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3146977 -4.319 -4.3232465 -4.3212752 -4.3120527 -4.2980933 -4.2854438 -4.2807159 -4.2765069 -4.2782364 -4.2749581 -4.2639966 -4.2502146 -4.2564707 -4.2774539][-4.3183985 -4.3247728 -4.3320479 -4.3310366 -4.3197951 -4.3035579 -4.288353 -4.2790294 -4.2690525 -4.2728624 -4.2751784 -4.2673988 -4.2529893 -4.2587643 -4.2778921][-4.320931 -4.3309474 -4.342792 -4.3443503 -4.33118 -4.3127527 -4.29339 -4.2758865 -4.2580938 -4.2655687 -4.2771325 -4.2762041 -4.26408 -4.26729 -4.27951][-4.3240547 -4.3381958 -4.3552961 -4.3592219 -4.3447008 -4.3241367 -4.2996626 -4.2717848 -4.2456331 -4.2566676 -4.2766533 -4.2814531 -4.2708988 -4.2705154 -4.274838][-4.3269982 -4.344451 -4.3642211 -4.3677058 -4.3494797 -4.3235645 -4.2905087 -4.2544866 -4.2278748 -4.2470627 -4.2759409 -4.2863464 -4.2778759 -4.2733755 -4.2674112][-4.3300667 -4.3496037 -4.3695183 -4.3706474 -4.3458014 -4.3106241 -4.2670283 -4.2230983 -4.2005439 -4.2311873 -4.2689795 -4.2859192 -4.2816234 -4.275454 -4.261333][-4.3322725 -4.3520393 -4.3697338 -4.3668165 -4.3363361 -4.2953835 -4.2450747 -4.1961346 -4.1796827 -4.2197013 -4.2647047 -4.2872334 -4.2857218 -4.2767544 -4.2599688][-4.3324127 -4.3507848 -4.3654737 -4.3586855 -4.3256726 -4.2829871 -4.2296405 -4.1789269 -4.1694622 -4.2139869 -4.2612352 -4.2858677 -4.284471 -4.2726736 -4.2570753][-4.3298965 -4.3454266 -4.356637 -4.3475142 -4.3149214 -4.275187 -4.2256761 -4.1803851 -4.176352 -4.2209597 -4.2652774 -4.2879114 -4.2839365 -4.2700682 -4.2569895][-4.3258209 -4.3372688 -4.3434677 -4.3328681 -4.3020778 -4.2685618 -4.2281775 -4.1914692 -4.1918144 -4.2332993 -4.2707448 -4.2883563 -4.2832394 -4.2703733 -4.2603312][-4.32098 -4.3283362 -4.3310485 -4.3202662 -4.292151 -4.2641711 -4.232532 -4.2030454 -4.2054439 -4.2438021 -4.2760949 -4.28996 -4.2850714 -4.27327 -4.2657256][-4.315309 -4.3186817 -4.3189821 -4.3095179 -4.2860427 -4.26498 -4.2423329 -4.2208147 -4.2227273 -4.2550273 -4.2813148 -4.2916059 -4.2885318 -4.2789683 -4.2721906][-4.3088226 -4.307795 -4.3043847 -4.2961307 -4.27813 -4.2633767 -4.2493958 -4.2367744 -4.2387738 -4.2627811 -4.2819176 -4.2882471 -4.2877665 -4.282104 -4.2773471][-4.30446 -4.2992063 -4.2924767 -4.2850323 -4.2713494 -4.2601614 -4.2514358 -4.2447276 -4.2460518 -4.2639675 -4.2763138 -4.2809453 -4.2839737 -4.2839007 -4.2826838][-4.3029423 -4.2949386 -4.2870607 -4.2814994 -4.2720327 -4.2631297 -4.2578483 -4.2531729 -4.2513247 -4.2619872 -4.2686849 -4.27182 -4.2762957 -4.2796807 -4.2819567]]...]
INFO - root - 2017-12-07 15:11:55.575434: step 10910, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.579 sec/batch; 68h:07m:59s remains)
INFO - root - 2017-12-07 15:12:11.882137: step 10920, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.727 sec/batch; 74h:31m:08s remains)
INFO - root - 2017-12-07 15:12:28.146411: step 10930, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.615 sec/batch; 69h:41m:23s remains)
INFO - root - 2017-12-07 15:12:44.579562: step 10940, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.552 sec/batch; 66h:58m:05s remains)
INFO - root - 2017-12-07 15:13:00.984476: step 10950, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.681 sec/batch; 72h:32m:13s remains)
INFO - root - 2017-12-07 15:13:17.203960: step 10960, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.543 sec/batch; 66h:34m:41s remains)
INFO - root - 2017-12-07 15:13:33.459785: step 10970, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.668 sec/batch; 71h:56m:33s remains)
INFO - root - 2017-12-07 15:13:49.721170: step 10980, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 1.637 sec/batch; 70h:35m:19s remains)
INFO - root - 2017-12-07 15:14:06.094069: step 10990, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.611 sec/batch; 69h:29m:58s remains)
INFO - root - 2017-12-07 15:14:22.488908: step 11000, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.709 sec/batch; 73h:42m:55s remains)
2017-12-07 15:14:23.798666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2789612 -4.2766 -4.2862239 -4.2927337 -4.28673 -4.276906 -4.2733693 -4.2755733 -4.279645 -4.2768831 -4.2682285 -4.2538009 -4.2398272 -4.2399187 -4.24774][-4.2899446 -4.2874556 -4.3044944 -4.3185334 -4.3149652 -4.30131 -4.2908874 -4.2861915 -4.2844381 -4.2792535 -4.275815 -4.2709541 -4.2624454 -4.2564449 -4.2578812][-4.2619476 -4.2639227 -4.2967019 -4.3280907 -4.3312588 -4.3146796 -4.2937126 -4.272984 -4.2533488 -4.2444868 -4.2540426 -4.2651329 -4.2658191 -4.2601223 -4.2561245][-4.1910119 -4.2023559 -4.2583566 -4.3084569 -4.316256 -4.2902493 -4.2502565 -4.2042794 -4.1649189 -4.1592879 -4.1897225 -4.2237844 -4.2398252 -4.2406545 -4.2387605][-4.0944409 -4.1197066 -4.2022629 -4.265214 -4.2677536 -4.2239294 -4.1585712 -4.0827866 -4.0263953 -4.0357647 -4.0978522 -4.1586728 -4.1953964 -4.2104759 -4.2162786][-3.9940846 -4.034225 -4.1451192 -4.2201033 -4.2173815 -4.154521 -4.059411 -3.9503198 -3.8771677 -3.9072576 -4.0043492 -4.0979357 -4.1606793 -4.1937003 -4.210216][-3.9245811 -3.982347 -4.1148868 -4.1939745 -4.1863203 -4.1077123 -3.981698 -3.8432407 -3.7607853 -3.8160233 -3.9447112 -4.0694547 -4.1523361 -4.1958642 -4.2179184][-3.9201751 -3.993052 -4.1294675 -4.2008615 -4.1878395 -4.1013188 -3.9660351 -3.8254857 -3.7485518 -3.8122275 -3.9458909 -4.079484 -4.1721025 -4.2192955 -4.2369914][-4.0219107 -4.076859 -4.1804876 -4.2344775 -4.2245708 -4.1560903 -4.04826 -3.9390957 -3.8761477 -3.9123127 -4.0141077 -4.1283021 -4.2115936 -4.250421 -4.2562985][-4.1634183 -4.1830344 -4.2349911 -4.267859 -4.2691741 -4.2384253 -4.1822963 -4.1158967 -4.0666542 -4.0684128 -4.1233454 -4.2038703 -4.2666745 -4.2911749 -4.2833924][-4.2483692 -4.250248 -4.269372 -4.2904129 -4.29991 -4.2954683 -4.2776809 -4.24312 -4.20799 -4.1964254 -4.221509 -4.27306 -4.31669 -4.3307548 -4.3184915][-4.2817135 -4.2805619 -4.285738 -4.3003516 -4.3129964 -4.3187642 -4.3149991 -4.2948732 -4.2732296 -4.2664194 -4.2776504 -4.3062439 -4.3344326 -4.3468976 -4.3409219][-4.2850113 -4.2844019 -4.2850227 -4.2947173 -4.30789 -4.3172741 -4.3181677 -4.30665 -4.2959747 -4.29522 -4.3018188 -4.316185 -4.3324795 -4.3440189 -4.3458996][-4.2838488 -4.281692 -4.2792664 -4.2841048 -4.2930031 -4.3019447 -4.305449 -4.2977467 -4.2928405 -4.2956519 -4.299274 -4.3055177 -4.3145494 -4.3248386 -4.3316722][-4.2903972 -4.2873883 -4.2820187 -4.2799511 -4.28007 -4.2839427 -4.2876749 -4.2831554 -4.2821016 -4.2846966 -4.2839303 -4.2859864 -4.2925348 -4.3020844 -4.3111157]]...]
INFO - root - 2017-12-07 15:14:40.062355: step 11010, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.653 sec/batch; 71h:17m:35s remains)
INFO - root - 2017-12-07 15:14:56.165494: step 11020, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 1.517 sec/batch; 65h:23m:34s remains)
INFO - root - 2017-12-07 15:15:12.489518: step 11030, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.684 sec/batch; 72h:36m:58s remains)
INFO - root - 2017-12-07 15:15:28.680975: step 11040, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.628 sec/batch; 70h:10m:13s remains)
INFO - root - 2017-12-07 15:15:44.893982: step 11050, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.701 sec/batch; 73h:19m:03s remains)
INFO - root - 2017-12-07 15:16:01.113379: step 11060, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.580 sec/batch; 68h:06m:45s remains)
INFO - root - 2017-12-07 15:16:17.504044: step 11070, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.592 sec/batch; 68h:37m:10s remains)
INFO - root - 2017-12-07 15:16:33.485659: step 11080, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.636 sec/batch; 70h:31m:39s remains)
INFO - root - 2017-12-07 15:16:49.836907: step 11090, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.597 sec/batch; 68h:48m:58s remains)
INFO - root - 2017-12-07 15:17:06.125958: step 11100, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.742 sec/batch; 75h:04m:40s remains)
2017-12-07 15:17:07.460265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3309021 -4.3235135 -4.3179207 -4.3145127 -4.3110709 -4.3113265 -4.3167262 -4.3230767 -4.3300261 -4.3344865 -4.3351436 -4.3291259 -4.3210745 -4.3100686 -4.2914014][-4.3150921 -4.2995634 -4.2886672 -4.2826791 -4.2773728 -4.2780719 -4.2858052 -4.2962856 -4.3081059 -4.3174024 -4.3204741 -4.3159761 -4.3069596 -4.2899222 -4.2595439][-4.2941375 -4.2658567 -4.2430043 -4.2281189 -4.216104 -4.214385 -4.2249546 -4.2418594 -4.261023 -4.27757 -4.2871366 -4.2886887 -4.2808442 -4.2595725 -4.2227077][-4.2684383 -4.2268081 -4.1884623 -4.1595016 -4.1408873 -4.1354074 -4.1448 -4.1643734 -4.1914077 -4.218565 -4.2392426 -4.249095 -4.2437282 -4.2205734 -4.1857061][-4.2446418 -4.1912851 -4.14145 -4.1044035 -4.0817289 -4.0684829 -4.0639262 -4.0749936 -4.1084819 -4.1538744 -4.1873307 -4.2035022 -4.2009563 -4.1800671 -4.1480336][-4.2199683 -4.1566572 -4.1002564 -4.0574074 -4.0225906 -3.9857421 -3.9492085 -3.9319589 -3.9687552 -4.0402589 -4.0946722 -4.1220737 -4.1309805 -4.1182666 -4.0913224][-4.2065978 -4.1392846 -4.080162 -4.0353961 -3.9903321 -3.9302306 -3.855509 -3.7947209 -3.8205578 -3.9148722 -3.9924483 -4.0434089 -4.0735316 -4.0717154 -4.0468016][-4.2132859 -4.1529098 -4.1022739 -4.0708661 -4.0352535 -3.9833272 -3.9044664 -3.8218951 -3.8234231 -3.9001179 -3.973248 -4.031167 -4.0712228 -4.0782466 -4.0594144][-4.229116 -4.1769514 -4.1366768 -4.1190104 -4.1056519 -4.0848384 -4.0416842 -3.98198 -3.9607215 -3.9894161 -4.0237064 -4.0509753 -4.0718265 -4.0821466 -4.0924687][-4.2391329 -4.1931233 -4.1574082 -4.1426411 -4.1462483 -4.1576695 -4.1538649 -4.1252241 -4.0998735 -4.0956297 -4.09476 -4.0831289 -4.0711145 -4.0742927 -4.1060596][-4.2358146 -4.20089 -4.1747236 -4.1627765 -4.1745257 -4.2053442 -4.22328 -4.2188311 -4.2035451 -4.1889372 -4.1706057 -4.1359425 -4.1033 -4.0922132 -4.1234503][-4.2252545 -4.200994 -4.1825986 -4.1732183 -4.18941 -4.2284851 -4.2571888 -4.2675629 -4.2608404 -4.2466 -4.2274828 -4.1903372 -4.1508317 -4.12931 -4.148067][-4.21444 -4.19827 -4.1866231 -4.1808362 -4.1961918 -4.2329431 -4.265161 -4.2821832 -4.2779098 -4.2651129 -4.2502208 -4.2196665 -4.1828518 -4.1611481 -4.1678][-4.2194376 -4.2073593 -4.1970229 -4.187582 -4.1933365 -4.2173109 -4.2456746 -4.2628608 -4.2581077 -4.2467933 -4.2378922 -4.2221427 -4.1957135 -4.1749449 -4.1720195][-4.2366629 -4.2246656 -4.2108903 -4.1907043 -4.1836476 -4.1989088 -4.2224159 -4.2383533 -4.2321386 -4.2187619 -4.2134371 -4.2095032 -4.1965256 -4.1752205 -4.1652336]]...]
INFO - root - 2017-12-07 15:17:23.705885: step 11110, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.656 sec/batch; 71h:22m:40s remains)
INFO - root - 2017-12-07 15:17:39.981119: step 11120, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 68h:50m:35s remains)
INFO - root - 2017-12-07 15:17:56.461126: step 11130, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.720 sec/batch; 74h:06m:15s remains)
INFO - root - 2017-12-07 15:18:12.244084: step 11140, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.563 sec/batch; 67h:20m:27s remains)
INFO - root - 2017-12-07 15:18:28.640976: step 11150, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.668 sec/batch; 71h:51m:28s remains)
INFO - root - 2017-12-07 15:18:45.008965: step 11160, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 1.611 sec/batch; 69h:25m:12s remains)
INFO - root - 2017-12-07 15:19:01.289107: step 11170, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.691 sec/batch; 72h:51m:24s remains)
INFO - root - 2017-12-07 15:19:17.664892: step 11180, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.607 sec/batch; 69h:12m:14s remains)
INFO - root - 2017-12-07 15:19:34.088690: step 11190, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.548 sec/batch; 66h:39m:30s remains)
INFO - root - 2017-12-07 15:19:50.028848: step 11200, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.693 sec/batch; 72h:56m:14s remains)
2017-12-07 15:19:51.463622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3364363 -4.3281069 -4.3001323 -4.2547626 -4.2158613 -4.1922011 -4.1850357 -4.19594 -4.21799 -4.2376752 -4.248817 -4.2529006 -4.2562423 -4.2578368 -4.2534313][-4.3323317 -4.3217282 -4.2912774 -4.2387133 -4.1864986 -4.1428986 -4.1175909 -4.1239476 -4.15951 -4.2015967 -4.2337351 -4.25035 -4.2576904 -4.2569141 -4.2495675][-4.326529 -4.3108821 -4.2745228 -4.2139668 -4.148025 -4.0815272 -4.0338821 -4.03449 -4.0829997 -4.14811 -4.2049823 -4.2395697 -4.252553 -4.2492256 -4.2402835][-4.3203149 -4.2994723 -4.2577972 -4.1908288 -4.1130614 -4.0281138 -3.9616885 -3.9574409 -4.0142117 -4.0951095 -4.17022 -4.221478 -4.2399287 -4.2331281 -4.2218556][-4.3144779 -4.2899032 -4.242444 -4.1684957 -4.0829787 -3.9895027 -3.9125414 -3.9017091 -3.9631886 -4.0513496 -4.1355739 -4.1983042 -4.2216854 -4.2126794 -4.1974363][-4.3114319 -4.286593 -4.2359338 -4.1575794 -4.0668893 -3.9713452 -3.8923879 -3.877425 -3.9418015 -4.0334091 -4.1198444 -4.1873984 -4.2154942 -4.2082424 -4.1924882][-4.3121319 -4.2908444 -4.244133 -4.1707554 -4.0828567 -3.987464 -3.9095659 -3.8937824 -3.9575939 -4.0485163 -4.1315603 -4.1979351 -4.2310071 -4.2324662 -4.2234254][-4.3159533 -4.2995248 -4.2612481 -4.197464 -4.1203623 -4.03139 -3.9566188 -3.9410682 -3.9976707 -4.0797124 -4.1541519 -4.21899 -4.2588892 -4.2720928 -4.2725124][-4.3232441 -4.3122015 -4.283483 -4.2307968 -4.1620154 -4.0792236 -4.0073266 -3.9873619 -4.0304542 -4.09919 -4.1636443 -4.2280469 -4.2750459 -4.2986903 -4.3083243][-4.3323159 -4.3274736 -4.3057308 -4.260767 -4.1954751 -4.1147823 -4.0431128 -4.0144181 -4.0414877 -4.0950727 -4.1506348 -4.2125549 -4.2646961 -4.2961617 -4.31288][-4.3399968 -4.337646 -4.3169756 -4.2718163 -4.206388 -4.1273589 -4.0574555 -4.0213523 -4.030899 -4.0671263 -4.11441 -4.1751261 -4.2322912 -4.2710762 -4.2946677][-4.3443894 -4.3416743 -4.3193064 -4.270752 -4.2008305 -4.1206036 -4.0554838 -4.0180616 -4.0147028 -4.0362864 -4.0777488 -4.1385188 -4.1990833 -4.2447705 -4.2757883][-4.3451924 -4.3412042 -4.3186154 -4.2710719 -4.2006421 -4.1216984 -4.06106 -4.0267725 -4.0191693 -4.0335031 -4.0713873 -4.1295881 -4.1901293 -4.2400589 -4.2764249][-4.3458471 -4.3417592 -4.3221645 -4.2824049 -4.2223043 -4.1554503 -4.1058135 -4.0814548 -4.0786057 -4.0907059 -4.1223607 -4.1691866 -4.2183461 -4.2612782 -4.2938771][-4.3460817 -4.3431091 -4.3299146 -4.303515 -4.2622995 -4.2154064 -4.18376 -4.174109 -4.1805239 -4.1918645 -4.2116566 -4.2405052 -4.2717695 -4.29964 -4.3210106]]...]
INFO - root - 2017-12-07 15:20:07.555325: step 11210, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.552 sec/batch; 66h:49m:11s remains)
INFO - root - 2017-12-07 15:20:23.820985: step 11220, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.577 sec/batch; 67h:54m:33s remains)
INFO - root - 2017-12-07 15:20:39.993680: step 11230, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.673 sec/batch; 72h:03m:27s remains)
INFO - root - 2017-12-07 15:20:56.211700: step 11240, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.612 sec/batch; 69h:24m:27s remains)
INFO - root - 2017-12-07 15:21:12.432279: step 11250, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 1.695 sec/batch; 72h:57m:51s remains)
INFO - root - 2017-12-07 15:21:28.189964: step 11260, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.637 sec/batch; 70h:28m:28s remains)
INFO - root - 2017-12-07 15:21:44.524947: step 11270, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.610 sec/batch; 69h:19m:40s remains)
INFO - root - 2017-12-07 15:22:00.940165: step 11280, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.734 sec/batch; 74h:37m:58s remains)
INFO - root - 2017-12-07 15:22:17.281768: step 11290, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.615 sec/batch; 69h:32m:00s remains)
INFO - root - 2017-12-07 15:22:33.838592: step 11300, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 1.765 sec/batch; 75h:57m:28s remains)
2017-12-07 15:22:35.174412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1860571 -4.1947794 -4.2083573 -4.2249055 -4.2434525 -4.2675567 -4.2924871 -4.3045034 -4.2992854 -4.2816615 -4.255548 -4.230125 -4.2086515 -4.1979752 -4.1998682][-4.2394695 -4.2518315 -4.2675667 -4.2805114 -4.290411 -4.3037772 -4.317627 -4.3218231 -4.3141985 -4.2951751 -4.2698627 -4.2485332 -4.2333574 -4.226913 -4.2284617][-4.2932854 -4.3095 -4.3257618 -4.3330541 -4.3321977 -4.3296814 -4.32728 -4.321187 -4.3102946 -4.2934308 -4.2773476 -4.2701902 -4.2674847 -4.2655134 -4.2652121][-4.3246255 -4.3431959 -4.3592606 -4.3610988 -4.3475823 -4.3271093 -4.3072133 -4.2905788 -4.2820892 -4.2804093 -4.2858405 -4.2957168 -4.30277 -4.3032703 -4.2973356][-4.3255148 -4.34326 -4.3568187 -4.35088 -4.3225904 -4.2846794 -4.2503457 -4.2294478 -4.229691 -4.2479706 -4.2753119 -4.2994642 -4.3094926 -4.3083329 -4.2978954][-4.3006511 -4.3097997 -4.3144007 -4.2961531 -4.25195 -4.1991262 -4.1551194 -4.1347804 -4.1493855 -4.1877103 -4.234571 -4.2677941 -4.2772965 -4.2745066 -4.2654982][-4.2543726 -4.2493796 -4.2408051 -4.2101617 -4.1551609 -4.0919914 -4.0425572 -4.0300713 -4.0617619 -4.1202803 -4.1794186 -4.218317 -4.2297897 -4.230742 -4.2304182][-4.2016492 -4.1828876 -4.1597424 -4.120398 -4.0610824 -3.9931769 -3.9446225 -3.9450383 -3.9864087 -4.0506067 -4.1147561 -4.1618209 -4.1845393 -4.2007251 -4.2163014][-4.1732197 -4.1417513 -4.1075764 -4.0676894 -4.0179358 -3.9615619 -3.9208136 -3.9207 -3.9452407 -3.9925435 -4.0519609 -4.1034918 -4.1423054 -4.1822257 -4.2191806][-4.1765089 -4.1395621 -4.1071005 -4.0801358 -4.0527563 -4.018373 -3.9816236 -3.9589179 -3.9470611 -3.9646921 -4.0114222 -4.063765 -4.1157966 -4.177866 -4.23082][-4.1999059 -4.167109 -4.1441031 -4.1322646 -4.1225133 -4.1020837 -4.062809 -4.0204072 -3.9834356 -3.9785337 -4.0153656 -4.067431 -4.1261158 -4.1942377 -4.246851][-4.2303786 -4.2056522 -4.1886525 -4.1850424 -4.1815219 -4.1639509 -4.1273694 -4.0813017 -4.0413914 -4.0351477 -4.0705395 -4.1196489 -4.1711197 -4.2232108 -4.2627482][-4.2513232 -4.2363443 -4.2240195 -4.2219462 -4.2173758 -4.1998277 -4.1716528 -4.135705 -4.1087623 -4.1146331 -4.1510515 -4.1892114 -4.2215605 -4.249114 -4.2697711][-4.2701683 -4.2638693 -4.2527943 -4.2435846 -4.2310309 -4.2118492 -4.1934872 -4.1762919 -4.1687312 -4.1863384 -4.2173305 -4.2384191 -4.2503738 -4.25838 -4.2628689][-4.2786112 -4.2773123 -4.2674503 -4.2503567 -4.2294607 -4.2078543 -4.1980619 -4.1974082 -4.20443 -4.2292085 -4.2553124 -4.263433 -4.2615929 -4.2556033 -4.2483063]]...]
INFO - root - 2017-12-07 15:22:51.430318: step 11310, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.542 sec/batch; 66h:23m:14s remains)
INFO - root - 2017-12-07 15:23:07.634937: step 11320, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.596 sec/batch; 68h:41m:04s remains)
INFO - root - 2017-12-07 15:23:23.776685: step 11330, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.657 sec/batch; 71h:18m:39s remains)
INFO - root - 2017-12-07 15:23:40.014939: step 11340, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.635 sec/batch; 70h:22m:32s remains)
INFO - root - 2017-12-07 15:23:56.304073: step 11350, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.648 sec/batch; 70h:54m:35s remains)
INFO - root - 2017-12-07 15:24:12.596948: step 11360, loss = 2.09, batch loss = 2.04 (9.7 examples/sec; 1.642 sec/batch; 70h:39m:07s remains)
INFO - root - 2017-12-07 15:24:28.816335: step 11370, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.591 sec/batch; 68h:26m:05s remains)
INFO - root - 2017-12-07 15:24:45.020795: step 11380, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.719 sec/batch; 73h:56m:54s remains)
INFO - root - 2017-12-07 15:25:01.220965: step 11390, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 1.631 sec/batch; 70h:10m:17s remains)
INFO - root - 2017-12-07 15:25:17.451151: step 11400, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 1.694 sec/batch; 72h:53m:08s remains)
2017-12-07 15:25:18.728742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1967149 -4.1990194 -4.2130995 -4.2220759 -4.2187896 -4.2166338 -4.2141647 -4.207994 -4.2026725 -4.2082839 -4.2193804 -4.2248669 -4.2132554 -4.1907697 -4.1843095][-4.2060637 -4.2262468 -4.2447438 -4.250329 -4.2398653 -4.2310638 -4.2241821 -4.2183056 -4.2167892 -4.2261639 -4.2365551 -4.2350936 -4.2135506 -4.1834097 -4.1807475][-4.2236156 -4.250659 -4.2676797 -4.26655 -4.2476764 -4.2295203 -4.2161283 -4.2086077 -4.2104573 -4.2228661 -4.233129 -4.2263374 -4.1978 -4.163775 -4.1659794][-4.2348237 -4.2537131 -4.2624092 -4.2537379 -4.224225 -4.1913948 -4.1666646 -4.1571712 -4.1693377 -4.1940985 -4.2101054 -4.2025871 -4.1699853 -4.1326218 -4.1347923][-4.2183423 -4.2288146 -4.2348738 -4.2201815 -4.178504 -4.1279392 -4.0839133 -4.0660629 -4.0913234 -4.1346512 -4.1623507 -4.1571765 -4.1252961 -4.0884504 -4.094089][-4.1802487 -4.1930513 -4.2040009 -4.1882911 -4.1338906 -4.0538754 -3.9742324 -3.9407775 -3.9857304 -4.0548735 -4.1021481 -4.11012 -4.0846028 -4.051662 -4.0615768][-4.1373367 -4.1540289 -4.1629906 -4.1335692 -4.0519586 -3.9295132 -3.8073022 -3.7718379 -3.862474 -3.9757788 -4.0526567 -4.0836964 -4.0746078 -4.0490532 -4.0553408][-4.0845461 -4.1075587 -4.1162124 -4.0776753 -3.9860535 -3.853858 -3.7243354 -3.7013073 -3.8190389 -3.9533565 -4.0417995 -4.0820494 -4.0817609 -4.06063 -4.0602093][-4.0494719 -4.0874038 -4.1087279 -4.0824594 -4.0112553 -3.9113941 -3.8176808 -3.8027096 -3.8946347 -4.0006757 -4.0693083 -4.0953684 -4.0891628 -4.0673089 -4.0625963][-4.06011 -4.1083374 -4.1401119 -4.1271982 -4.0776534 -4.0139227 -3.9560161 -3.9453268 -4.0030165 -4.0748434 -4.1227942 -4.1327338 -4.1173997 -4.09513 -4.091001][-4.106884 -4.151185 -4.1842184 -4.1841884 -4.154428 -4.1152563 -4.0826588 -4.0754743 -4.1056571 -4.1506667 -4.1817265 -4.1843162 -4.1679087 -4.1516151 -4.1520815][-4.1713171 -4.2051296 -4.2332606 -4.2389107 -4.2242951 -4.2010436 -4.184391 -4.1821723 -4.1955333 -4.2180347 -4.2346087 -4.2356248 -4.225337 -4.2179461 -4.2208271][-4.22921 -4.2520394 -4.2708211 -4.2757688 -4.2698407 -4.2583489 -4.2539015 -4.256393 -4.2614379 -4.2699785 -4.2771 -4.2772727 -4.272645 -4.271071 -4.27373][-4.2677841 -4.2805839 -4.29309 -4.299902 -4.2994747 -4.294559 -4.2935395 -4.2942562 -4.2954874 -4.2983189 -4.3014779 -4.3006325 -4.2979522 -4.2972894 -4.297657][-4.284627 -4.2896142 -4.2979422 -4.3049989 -4.3081307 -4.3070631 -4.3052673 -4.3032608 -4.302186 -4.3038259 -4.3052173 -4.3038535 -4.30146 -4.2999783 -4.2990746]]...]
INFO - root - 2017-12-07 15:25:35.277001: step 11410, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 1.727 sec/batch; 74h:16m:53s remains)
INFO - root - 2017-12-07 15:25:51.436929: step 11420, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.580 sec/batch; 67h:56m:02s remains)
INFO - root - 2017-12-07 15:26:07.598926: step 11430, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 1.700 sec/batch; 73h:07m:23s remains)
INFO - root - 2017-12-07 15:26:23.831038: step 11440, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.596 sec/batch; 68h:36m:55s remains)
INFO - root - 2017-12-07 15:26:40.051144: step 11450, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.717 sec/batch; 73h:50m:47s remains)
INFO - root - 2017-12-07 15:26:56.307026: step 11460, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 1.531 sec/batch; 65h:49m:07s remains)
INFO - root - 2017-12-07 15:27:12.635563: step 11470, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.631 sec/batch; 70h:06m:13s remains)
INFO - root - 2017-12-07 15:27:28.917739: step 11480, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.652 sec/batch; 71h:01m:28s remains)
INFO - root - 2017-12-07 15:27:45.031321: step 11490, loss = 2.10, batch loss = 2.04 (10.3 examples/sec; 1.559 sec/batch; 67h:00m:26s remains)
INFO - root - 2017-12-07 15:28:01.446885: step 11500, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.713 sec/batch; 73h:37m:43s remains)
2017-12-07 15:28:02.862650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3259206 -4.3234138 -4.3248816 -4.32855 -4.3330808 -4.3377938 -4.34212 -4.3460264 -4.3486853 -4.3497725 -4.3494029 -4.34801 -4.34619 -4.3407793 -4.3224382][-4.2914557 -4.2852683 -4.2866344 -4.2928624 -4.3012886 -4.3097935 -4.3183 -4.3275714 -4.337379 -4.3454046 -4.3498888 -4.350337 -4.3483119 -4.34215 -4.3231812][-4.244175 -4.2329311 -4.23035 -4.2341323 -4.244236 -4.2573915 -4.2715087 -4.2886782 -4.3095813 -4.3300686 -4.344243 -4.3505039 -4.3508277 -4.3444824 -4.324645][-4.1950045 -4.1805353 -4.1741505 -4.1702137 -4.1744103 -4.1858296 -4.2043905 -4.2301717 -4.2625918 -4.297195 -4.324924 -4.3407769 -4.3474512 -4.3445182 -4.325738][-4.1629953 -4.1472859 -4.1413054 -4.1290407 -4.117466 -4.1149235 -4.1278625 -4.1578317 -4.1999745 -4.2463579 -4.2859077 -4.312417 -4.3284149 -4.3332677 -4.3196821][-4.1499691 -4.1303287 -4.1283712 -4.1175823 -4.0905881 -4.0621276 -4.0555329 -4.0815229 -4.1295819 -4.1836596 -4.2303452 -4.2653794 -4.2923446 -4.308547 -4.3051143][-4.1502938 -4.1226764 -4.1224141 -4.11855 -4.0862093 -4.033637 -3.9994581 -4.0151386 -4.0632038 -4.1169734 -4.162828 -4.2025666 -4.2411809 -4.2733335 -4.284729][-4.1772008 -4.1468182 -4.1383829 -4.135047 -4.10737 -4.0516572 -3.9992075 -3.9939594 -4.0268321 -4.0660138 -4.1019893 -4.1404562 -4.1888642 -4.2369804 -4.26349][-4.2267809 -4.2001724 -4.1826081 -4.1720948 -4.1534667 -4.1144452 -4.0689182 -4.0442381 -4.0459051 -4.0580688 -4.0772667 -4.108912 -4.1577973 -4.2124081 -4.247365][-4.2793345 -4.2610826 -4.2420011 -4.22585 -4.2108436 -4.1847892 -4.1502843 -4.1193147 -4.0989513 -4.086987 -4.0867615 -4.1058912 -4.1461349 -4.1970325 -4.2327523][-4.3174577 -4.3092489 -4.2967868 -4.2836833 -4.2694206 -4.2473598 -4.219655 -4.1914024 -4.1653771 -4.141232 -4.1253915 -4.1292691 -4.1538291 -4.1913342 -4.2184911][-4.3356338 -4.3343997 -4.3303046 -4.3242 -4.3145671 -4.297576 -4.275228 -4.25091 -4.227057 -4.20212 -4.1801658 -4.1700792 -4.1737309 -4.1896534 -4.2014909][-4.3404913 -4.3429236 -4.3447032 -4.3456697 -4.3431983 -4.3346868 -4.3181729 -4.2952285 -4.2717648 -4.2483149 -4.2261777 -4.2067881 -4.1902804 -4.1817675 -4.1747437][-4.341104 -4.3439126 -4.3467445 -4.350863 -4.35381 -4.3517003 -4.341157 -4.3228416 -4.3006954 -4.2769036 -4.2534418 -4.2287054 -4.200613 -4.1739869 -4.149384][-4.3423982 -4.3434229 -4.3439879 -4.34679 -4.3501782 -4.3508711 -4.3456779 -4.3337994 -4.3164244 -4.2949109 -4.2712007 -4.2451458 -4.2145853 -4.1815748 -4.1456146]]...]
INFO - root - 2017-12-07 15:28:19.141165: step 11510, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.709 sec/batch; 73h:28m:00s remains)
INFO - root - 2017-12-07 15:28:35.412305: step 11520, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 1.602 sec/batch; 68h:51m:21s remains)
INFO - root - 2017-12-07 15:28:51.700853: step 11530, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.683 sec/batch; 72h:21m:06s remains)
INFO - root - 2017-12-07 15:29:07.764110: step 11540, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.693 sec/batch; 72h:46m:13s remains)
INFO - root - 2017-12-07 15:29:23.593769: step 11550, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 1.416 sec/batch; 60h:51m:43s remains)
INFO - root - 2017-12-07 15:29:39.916170: step 11560, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.744 sec/batch; 74h:55m:53s remains)
INFO - root - 2017-12-07 15:29:56.124854: step 11570, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.633 sec/batch; 70h:08m:44s remains)
INFO - root - 2017-12-07 15:30:12.425647: step 11580, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.657 sec/batch; 71h:12m:23s remains)
INFO - root - 2017-12-07 15:30:28.612151: step 11590, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.568 sec/batch; 67h:22m:13s remains)
INFO - root - 2017-12-07 15:30:44.890894: step 11600, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.553 sec/batch; 66h:42m:31s remains)
2017-12-07 15:30:46.143522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3127956 -4.3004031 -4.2925677 -4.2901583 -4.2861457 -4.2813506 -4.2748961 -4.2682076 -4.2651949 -4.2719846 -4.272902 -4.2738056 -4.2765808 -4.280036 -4.2863755][-4.3138714 -4.3071661 -4.3015342 -4.3008223 -4.2939105 -4.283864 -4.2710457 -4.26045 -4.2565742 -4.263103 -4.2684064 -4.267827 -4.2649403 -4.2644825 -4.26888][-4.3062735 -4.3010354 -4.292666 -4.2893052 -4.2802405 -4.269969 -4.25436 -4.2410192 -4.2345161 -4.2393041 -4.2454166 -4.2454467 -4.2376742 -4.2316742 -4.2334318][-4.2942028 -4.2838154 -4.2673573 -4.2572784 -4.2447495 -4.2304459 -4.212038 -4.202312 -4.2019629 -4.20789 -4.2120275 -4.2080131 -4.190877 -4.1776705 -4.1757684][-4.2856908 -4.2731862 -4.2505622 -4.230011 -4.204978 -4.1788063 -4.1552234 -4.1472993 -4.1521935 -4.1646271 -4.1697297 -4.1629577 -4.1403747 -4.1199379 -4.108068][-4.2739449 -4.2569423 -4.2259164 -4.1870737 -4.1412926 -4.0935516 -4.0475283 -4.0259261 -4.0399613 -4.072988 -4.0932174 -4.1005063 -4.0912657 -4.0740652 -4.0545478][-4.2459283 -4.2184944 -4.172399 -4.115397 -4.0452313 -3.9735208 -3.8926218 -3.8518035 -3.8752513 -3.938714 -3.9870994 -4.0230551 -4.0387135 -4.0339341 -4.01762][-4.1972179 -4.1561785 -4.0964241 -4.0253277 -3.9364593 -3.8402634 -3.730999 -3.6796715 -3.729867 -3.8354115 -3.916482 -3.9774163 -4.0119462 -4.0200405 -4.01902][-4.1728988 -4.1314154 -4.0722508 -4.0032039 -3.9153304 -3.8193312 -3.72435 -3.6910348 -3.7481773 -3.8520837 -3.9332922 -3.995877 -4.0352936 -4.0514803 -4.0666356][-4.1843715 -4.1539812 -4.1126251 -4.0589528 -3.9857645 -3.9167624 -3.8657975 -3.8553662 -3.8949401 -3.9624422 -4.0161228 -4.0632958 -4.0984907 -4.1225772 -4.1464992][-4.2034326 -4.1854072 -4.1602993 -4.1215425 -4.0725031 -4.0391078 -4.0212283 -4.0215421 -4.0462837 -4.0827436 -4.1108017 -4.1390738 -4.1671286 -4.1921868 -4.2133923][-4.2291245 -4.2237315 -4.2163687 -4.1906433 -4.1588717 -4.1454186 -4.1435037 -4.1475358 -4.1651754 -4.184267 -4.1920805 -4.2010646 -4.2199478 -4.2403159 -4.2574911][-4.2577753 -4.2598858 -4.2614913 -4.2453094 -4.2256017 -4.2214727 -4.226028 -4.2325416 -4.2453909 -4.2506 -4.2442589 -4.2429295 -4.2534866 -4.2683277 -4.2793479][-4.28359 -4.2863812 -4.2900467 -4.2818322 -4.2724266 -4.2731886 -4.2773504 -4.2804265 -4.2844291 -4.2818365 -4.2731619 -4.2686915 -4.2727342 -4.2827744 -4.2912817][-4.303792 -4.3034282 -4.3058014 -4.30395 -4.302165 -4.3030543 -4.3030682 -4.3042336 -4.3044186 -4.3004055 -4.2932835 -4.288393 -4.2896609 -4.2955832 -4.30117]]...]
INFO - root - 2017-12-07 15:31:02.368781: step 11610, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.643 sec/batch; 70h:33m:37s remains)
INFO - root - 2017-12-07 15:31:18.678585: step 11620, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.723 sec/batch; 74h:01m:19s remains)
INFO - root - 2017-12-07 15:31:34.907189: step 11630, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 1.525 sec/batch; 65h:29m:47s remains)
INFO - root - 2017-12-07 15:31:51.181351: step 11640, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 1.720 sec/batch; 73h:53m:16s remains)
INFO - root - 2017-12-07 15:32:07.476468: step 11650, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.580 sec/batch; 67h:51m:56s remains)
INFO - root - 2017-12-07 15:32:23.720322: step 11660, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.714 sec/batch; 73h:35m:20s remains)
INFO - root - 2017-12-07 15:32:39.787943: step 11670, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.656 sec/batch; 71h:06m:52s remains)
INFO - root - 2017-12-07 15:32:56.009662: step 11680, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.622 sec/batch; 69h:38m:05s remains)
INFO - root - 2017-12-07 15:33:12.392511: step 11690, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.649 sec/batch; 70h:48m:41s remains)
INFO - root - 2017-12-07 15:33:28.494034: step 11700, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.584 sec/batch; 68h:00m:41s remains)
2017-12-07 15:33:29.763039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1874886 -4.1986542 -4.2083883 -4.2263618 -4.2467823 -4.2473946 -4.2426429 -4.2414556 -4.2442045 -4.2536221 -4.2657256 -4.2795386 -4.2959666 -4.3005247 -4.2918334][-4.2073841 -4.2280331 -4.24295 -4.2615876 -4.2758188 -4.26672 -4.2539043 -4.2470269 -4.2451034 -4.2537942 -4.2651582 -4.2825346 -4.3056946 -4.3185759 -4.3145747][-4.225687 -4.2513628 -4.2691927 -4.2860608 -4.2935987 -4.2772346 -4.2535028 -4.2404084 -4.238533 -4.2487679 -4.263032 -4.284874 -4.3123775 -4.3308377 -4.3312278][-4.2384539 -4.260747 -4.2771268 -4.2857008 -4.2862453 -4.2637739 -4.2316966 -4.217289 -4.2241907 -4.2408915 -4.2645378 -4.290185 -4.3182445 -4.3397751 -4.3458052][-4.2438579 -4.2501869 -4.2550149 -4.249733 -4.2442546 -4.2255073 -4.1984305 -4.1880479 -4.2038803 -4.2258444 -4.2516055 -4.2788849 -4.309989 -4.3370304 -4.3498597][-4.2407141 -4.2205944 -4.2004056 -4.1780839 -4.1681252 -4.1581779 -4.1487408 -4.1489573 -4.1712561 -4.1988397 -4.2262211 -4.2551889 -4.2874179 -4.3202453 -4.3423429][-4.23447 -4.1797609 -4.124032 -4.0749526 -4.0573072 -4.0638103 -4.0796871 -4.0938153 -4.1177111 -4.15263 -4.190176 -4.2258372 -4.2591166 -4.297821 -4.3311267][-4.2278614 -4.1440148 -4.052505 -3.9766293 -3.952143 -3.9762056 -4.0164104 -4.0449791 -4.06834 -4.1054878 -4.15481 -4.2007833 -4.2395759 -4.2833805 -4.3242712][-4.2273479 -4.1370988 -4.0446582 -3.9721429 -3.9427192 -3.9610658 -4.0048466 -4.0348415 -4.058641 -4.1005898 -4.1537132 -4.203516 -4.246 -4.2882071 -4.328804][-4.2386522 -4.1679382 -4.1042852 -4.0559721 -4.0270925 -4.0240746 -4.0460162 -4.06278 -4.080894 -4.1214991 -4.1734853 -4.2207789 -4.2607226 -4.3008313 -4.3387752][-4.2633352 -4.2208633 -4.1872725 -4.1625209 -4.1393609 -4.1224751 -4.1201782 -4.1156573 -4.1185083 -4.1443729 -4.1864743 -4.2293758 -4.2687187 -4.3083711 -4.3433056][-4.287735 -4.27222 -4.2627916 -4.2551618 -4.2407832 -4.2245026 -4.2120781 -4.1903729 -4.1730089 -4.1768484 -4.1965275 -4.2270689 -4.2636442 -4.3022318 -4.3345528][-4.2990117 -4.3023148 -4.3077922 -4.309495 -4.307919 -4.3024731 -4.2917356 -4.2659192 -4.2317386 -4.209384 -4.2032743 -4.2177687 -4.2481012 -4.2837238 -4.3156719][-4.2914248 -4.3072529 -4.32399 -4.33599 -4.3437057 -4.3471251 -4.3429246 -4.3230934 -4.2871957 -4.246223 -4.2169466 -4.2123218 -4.2301116 -4.25896 -4.2891641][-4.2618556 -4.2877355 -4.3154211 -4.33851 -4.3549733 -4.3650718 -4.3687005 -4.3597589 -4.3339944 -4.2908897 -4.2456408 -4.21919 -4.2169523 -4.2322717 -4.2554007]]...]
INFO - root - 2017-12-07 15:33:45.958046: step 11710, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.657 sec/batch; 71h:08m:40s remains)
INFO - root - 2017-12-07 15:34:01.901654: step 11720, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.584 sec/batch; 68h:00m:02s remains)
INFO - root - 2017-12-07 15:34:18.134123: step 11730, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.562 sec/batch; 67h:03m:27s remains)
INFO - root - 2017-12-07 15:34:34.372299: step 11740, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.673 sec/batch; 71h:49m:02s remains)
INFO - root - 2017-12-07 15:34:50.555972: step 11750, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.585 sec/batch; 68h:01m:22s remains)
INFO - root - 2017-12-07 15:35:06.727159: step 11760, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.640 sec/batch; 70h:22m:11s remains)
INFO - root - 2017-12-07 15:35:23.001088: step 11770, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.641 sec/batch; 70h:25m:33s remains)
INFO - root - 2017-12-07 15:35:39.214496: step 11780, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.676 sec/batch; 71h:55m:57s remains)
INFO - root - 2017-12-07 15:35:55.229317: step 11790, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.613 sec/batch; 69h:12m:22s remains)
INFO - root - 2017-12-07 15:36:11.693152: step 11800, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.684 sec/batch; 72h:15m:06s remains)
2017-12-07 15:36:12.974893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.192625 -4.1934142 -4.2040429 -4.212368 -4.2200623 -4.2285681 -4.2355547 -4.2409286 -4.2448082 -4.2486372 -4.2483792 -4.2461543 -4.2390137 -4.2333369 -4.2307267][-4.1847191 -4.1871705 -4.1989875 -4.2098994 -4.2188516 -4.2264352 -4.2284579 -4.2269764 -4.2253881 -4.2257156 -4.2236705 -4.224134 -4.2199087 -4.2159486 -4.2131166][-4.180552 -4.1842532 -4.1970382 -4.2091122 -4.21784 -4.2224135 -4.2201743 -4.2114191 -4.2018805 -4.1999345 -4.1995544 -4.2039733 -4.2030396 -4.1981268 -4.1903853][-4.1868558 -4.1951966 -4.210875 -4.2207141 -4.2230983 -4.214129 -4.1987152 -4.1771379 -4.1565542 -4.1558409 -4.16577 -4.1792555 -4.1804233 -4.1689744 -4.1529956][-4.1999922 -4.2111897 -4.2257814 -4.229713 -4.2217784 -4.1918573 -4.154398 -4.1155739 -4.0864916 -4.0956545 -4.1246271 -4.14782 -4.1484241 -4.1289492 -4.1061478][-4.2065516 -4.2132912 -4.2203631 -4.2157907 -4.1957278 -4.1468315 -4.0892162 -4.0340328 -4.0045567 -4.0330129 -4.0843897 -4.11415 -4.1111574 -4.0885034 -4.0682974][-4.2058735 -4.2066789 -4.2039137 -4.1904125 -4.1586571 -4.0930967 -4.0149908 -3.9454017 -3.9264612 -3.984308 -4.0558567 -4.0871725 -4.0827103 -4.0673594 -4.0631895][-4.205194 -4.1995082 -4.18948 -4.1703558 -4.1277018 -4.0465627 -3.9518883 -3.8776083 -3.8756442 -3.9560394 -4.0371671 -4.0714188 -4.07132 -4.072299 -4.0825233][-4.2098331 -4.2029767 -4.1921959 -4.1725736 -4.1244683 -4.0393672 -3.9448304 -3.882935 -3.8932717 -3.9726079 -4.0454574 -4.0799365 -4.0874515 -4.0957661 -4.1042528][-4.2225804 -4.2165861 -4.2063966 -4.1864471 -4.1406755 -4.0672464 -3.9909537 -3.9493566 -3.9653659 -4.0286126 -4.0837235 -4.1123352 -4.1175632 -4.11908 -4.1163692][-4.2383914 -4.2321696 -4.2215176 -4.2014422 -4.1617785 -4.1052351 -4.0524349 -4.0304484 -4.0493164 -4.09223 -4.1279974 -4.1459131 -4.1446157 -4.1395173 -4.1298194][-4.2531204 -4.2460823 -4.2333713 -4.21153 -4.1794834 -4.1407332 -4.1104245 -4.1034074 -4.1211276 -4.1495938 -4.1671209 -4.1699367 -4.1602349 -4.1536975 -4.145925][-4.2683492 -4.2591286 -4.2422891 -4.2202768 -4.1962214 -4.1740761 -4.1608796 -4.1619787 -4.1740379 -4.1915283 -4.1975489 -4.1881042 -4.1724644 -4.1670203 -4.16518][-4.2805295 -4.269012 -4.24924 -4.227706 -4.2098575 -4.1998205 -4.1979551 -4.2040529 -4.2121606 -4.2210188 -4.2198706 -4.2064762 -4.191422 -4.18646 -4.1886439][-4.2882032 -4.2754626 -4.2560539 -4.2362828 -4.2228818 -4.2198 -4.2223229 -4.22961 -4.2382317 -4.2440362 -4.2398796 -4.227881 -4.2173705 -4.2145166 -4.2166758]]...]
INFO - root - 2017-12-07 15:36:29.155245: step 11810, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.650 sec/batch; 70h:46m:06s remains)
INFO - root - 2017-12-07 15:36:45.333195: step 11820, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.699 sec/batch; 72h:52m:58s remains)
INFO - root - 2017-12-07 15:37:01.395572: step 11830, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.594 sec/batch; 68h:23m:17s remains)
INFO - root - 2017-12-07 15:37:17.463216: step 11840, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.651 sec/batch; 70h:48m:23s remains)
INFO - root - 2017-12-07 15:37:33.563064: step 11850, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.586 sec/batch; 68h:01m:38s remains)
INFO - root - 2017-12-07 15:37:49.692061: step 11860, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.691 sec/batch; 72h:31m:07s remains)
INFO - root - 2017-12-07 15:38:05.971917: step 11870, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.568 sec/batch; 67h:14m:34s remains)
INFO - root - 2017-12-07 15:38:22.187389: step 11880, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.565 sec/batch; 67h:06m:37s remains)
INFO - root - 2017-12-07 15:38:38.355851: step 11890, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.562 sec/batch; 66h:58m:15s remains)
INFO - root - 2017-12-07 15:38:54.635244: step 11900, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.642 sec/batch; 70h:24m:25s remains)
2017-12-07 15:38:55.990485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2468882 -4.2442555 -4.2527995 -4.2582455 -4.2600222 -4.2593365 -4.2358494 -4.2043662 -4.1776505 -4.1765194 -4.2008376 -4.2241869 -4.2331781 -4.2454405 -4.2658238][-4.2342734 -4.2307749 -4.2352805 -4.2399316 -4.2367258 -4.2263536 -4.1958561 -4.1632929 -4.1372356 -4.1449804 -4.179359 -4.2103744 -4.2243948 -4.2385197 -4.2608504][-4.2341876 -4.2336922 -4.2325225 -4.2307482 -4.22157 -4.1989965 -4.1642704 -4.13462 -4.1122279 -4.1267333 -4.1659856 -4.2002945 -4.2173538 -4.2317047 -4.2559967][-4.2355466 -4.240253 -4.2347822 -4.2256203 -4.2107153 -4.1783814 -4.1386323 -4.1121759 -4.0993319 -4.1203818 -4.1582193 -4.1894236 -4.2069025 -4.2226233 -4.2509828][-4.235477 -4.2469816 -4.2421288 -4.2282519 -4.2055416 -4.1567807 -4.0992332 -4.0698795 -4.0751176 -4.107666 -4.147336 -4.1791759 -4.1987457 -4.2163572 -4.24676][-4.2270446 -4.2450905 -4.2451859 -4.2316737 -4.2009411 -4.1343279 -4.0471282 -3.9999032 -4.0275106 -4.0804949 -4.1316772 -4.1714425 -4.196641 -4.217423 -4.2471776][-4.2029376 -4.225029 -4.2296619 -4.21804 -4.1832819 -4.107255 -3.9948242 -3.9252586 -3.9699106 -4.0440655 -4.1108379 -4.160646 -4.1920671 -4.2172871 -4.2476363][-4.17132 -4.1944585 -4.2049894 -4.1997252 -4.1688595 -4.0994096 -3.9895446 -3.9137866 -3.9536281 -4.0284486 -4.0994205 -4.1511159 -4.1838045 -4.213635 -4.2468109][-4.1506681 -4.1735725 -4.1898108 -4.192667 -4.17385 -4.1256542 -4.0412517 -3.976521 -3.9924555 -4.0439939 -4.1041164 -4.1488466 -4.1775951 -4.2084889 -4.2447968][-4.1526113 -4.1782184 -4.1975675 -4.2041197 -4.1948357 -4.1618581 -4.1035233 -4.0549045 -4.0503592 -4.0732388 -4.1132674 -4.1490355 -4.1721539 -4.2028179 -4.2428417][-4.1606421 -4.1890397 -4.2126064 -4.2200761 -4.2135386 -4.1900368 -4.1530628 -4.1212044 -4.1080437 -4.1106138 -4.1326766 -4.1562166 -4.1719484 -4.2001061 -4.2413421][-4.167088 -4.1951904 -4.2229929 -4.2362847 -4.2333622 -4.2137742 -4.1885419 -4.1684933 -4.1526451 -4.145206 -4.1570897 -4.1698351 -4.1772575 -4.2012739 -4.2430749][-4.1786232 -4.1997714 -4.2259526 -4.2472963 -4.2497396 -4.2333593 -4.2155657 -4.2026606 -4.1875339 -4.177412 -4.1833167 -4.1862984 -4.1889114 -4.2105546 -4.2522674][-4.2043285 -4.212101 -4.2323375 -4.2547035 -4.2642713 -4.25584 -4.243885 -4.2348781 -4.2202816 -4.20911 -4.210156 -4.2083263 -4.2099924 -4.2301822 -4.2692041][-4.2450047 -4.2416692 -4.2512765 -4.2672396 -4.2763357 -4.2747293 -4.2681136 -4.2595019 -4.2445359 -4.2338495 -4.2325888 -4.2318611 -4.2347736 -4.2532692 -4.2871361]]...]
INFO - root - 2017-12-07 15:39:12.219138: step 11910, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 1.526 sec/batch; 65h:25m:07s remains)
INFO - root - 2017-12-07 15:39:28.228645: step 11920, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.632 sec/batch; 69h:57m:59s remains)
INFO - root - 2017-12-07 15:39:44.524158: step 11930, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.565 sec/batch; 67h:04m:20s remains)
INFO - root - 2017-12-07 15:40:00.706541: step 11940, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.687 sec/batch; 72h:19m:15s remains)
INFO - root - 2017-12-07 15:40:16.703608: step 11950, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.615 sec/batch; 69h:13m:16s remains)
INFO - root - 2017-12-07 15:40:33.069190: step 11960, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.674 sec/batch; 71h:43m:49s remains)
INFO - root - 2017-12-07 15:40:49.180745: step 11970, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 67h:41m:14s remains)
INFO - root - 2017-12-07 15:41:05.345209: step 11980, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.682 sec/batch; 72h:04m:19s remains)
INFO - root - 2017-12-07 15:41:21.587230: step 11990, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.558 sec/batch; 66h:45m:09s remains)
INFO - root - 2017-12-07 15:41:38.055850: step 12000, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.598 sec/batch; 68h:27m:01s remains)
2017-12-07 15:41:39.395798: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2692637 -4.2454963 -4.2242756 -4.2147279 -4.2192836 -4.2342105 -4.2452712 -4.2499652 -4.2508745 -4.2524347 -4.2581782 -4.2684088 -4.2817049 -4.29682 -4.3008642][-4.2524714 -4.2168636 -4.1879511 -4.1787939 -4.1885252 -4.2068272 -4.2175179 -4.2245717 -4.2298431 -4.2344146 -4.2351327 -4.2372904 -4.249331 -4.2667289 -4.27458][-4.2274466 -4.1846533 -4.1529641 -4.1399326 -4.1516838 -4.168694 -4.1715064 -4.1747408 -4.1851611 -4.1959963 -4.1938252 -4.1877642 -4.1977906 -4.2182384 -4.2331672][-4.2084975 -4.1676869 -4.1333928 -4.1120949 -4.1156764 -4.1220345 -4.1190529 -4.1189761 -4.1404862 -4.1670895 -4.1728048 -4.1633377 -4.1680288 -4.1812043 -4.1979995][-4.196147 -4.1590872 -4.1260443 -4.1011147 -4.0898581 -4.0707645 -4.0478358 -4.0476575 -4.0898294 -4.1314263 -4.143609 -4.1441374 -4.1514497 -4.1619539 -4.1756382][-4.1858239 -4.15007 -4.1163483 -4.08498 -4.04839 -3.9820025 -3.9207995 -3.9245455 -3.9971366 -4.0600715 -4.0812321 -4.1005387 -4.1305151 -4.1548643 -4.1706972][-4.1828728 -4.1502237 -4.1155977 -4.0749559 -4.0126138 -3.893975 -3.7814503 -3.7875457 -3.9036152 -4.00425 -4.04286 -4.0785713 -4.1283116 -4.1639242 -4.1820631][-4.1925235 -4.1670294 -4.1381907 -4.0926561 -4.0163984 -3.8820813 -3.7594719 -3.7739685 -3.9037375 -4.0102215 -4.0593891 -4.0975575 -4.1459241 -4.1805105 -4.1984916][-4.2074013 -4.190908 -4.1671257 -4.1270189 -4.0684204 -3.9756663 -3.9010408 -3.9138851 -3.9911151 -4.0524192 -4.0842681 -4.1059504 -4.1323891 -4.1537561 -4.1804562][-4.2227283 -4.215867 -4.196558 -4.1703572 -4.1386776 -4.0932045 -4.0554714 -4.058742 -4.0857916 -4.1046381 -4.1125526 -4.1094112 -4.1056061 -4.1110396 -4.1483588][-4.2328329 -4.2310376 -4.2187028 -4.2082276 -4.197401 -4.1814952 -4.1642666 -4.1572266 -4.1522579 -4.14303 -4.13355 -4.1143737 -4.0854206 -4.0781288 -4.1231046][-4.2448769 -4.2450466 -4.2413788 -4.2430553 -4.2470026 -4.248251 -4.2377033 -4.2214608 -4.2036057 -4.181931 -4.1646905 -4.1444697 -4.1201048 -4.1139121 -4.1514993][-4.2707224 -4.2753029 -4.2810316 -4.2895465 -4.2970905 -4.2991133 -4.2900219 -4.2745519 -4.2551455 -4.23143 -4.2170916 -4.2042346 -4.1962848 -4.1970239 -4.2193151][-4.3013992 -4.3064914 -4.3140068 -4.3210626 -4.3241882 -4.3214788 -4.3125329 -4.3023 -4.29154 -4.2809157 -4.2764535 -4.2717981 -4.2734351 -4.2805772 -4.2903986][-4.3216152 -4.323977 -4.32701 -4.3288665 -4.3277092 -4.3230495 -4.3165293 -4.3147535 -4.3155746 -4.3153272 -4.3163528 -4.3172746 -4.3223071 -4.3297367 -4.3330135]]...]
INFO - root - 2017-12-07 15:41:55.527115: step 12010, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.683 sec/batch; 72h:06m:30s remains)
INFO - root - 2017-12-07 15:42:11.708687: step 12020, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 1.586 sec/batch; 67h:57m:13s remains)
INFO - root - 2017-12-07 15:42:28.145974: step 12030, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.633 sec/batch; 69h:56m:10s remains)
INFO - root - 2017-12-07 15:42:44.321695: step 12040, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.591 sec/batch; 68h:10m:11s remains)
INFO - root - 2017-12-07 15:43:00.511450: step 12050, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.552 sec/batch; 66h:27m:45s remains)
INFO - root - 2017-12-07 15:43:16.826006: step 12060, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.666 sec/batch; 71h:20m:31s remains)
INFO - root - 2017-12-07 15:43:32.897695: step 12070, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.579 sec/batch; 67h:36m:22s remains)
INFO - root - 2017-12-07 15:43:49.316311: step 12080, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.697 sec/batch; 72h:39m:51s remains)
INFO - root - 2017-12-07 15:44:05.371355: step 12090, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.606 sec/batch; 68h:45m:44s remains)
INFO - root - 2017-12-07 15:44:21.507478: step 12100, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.560 sec/batch; 66h:48m:55s remains)
2017-12-07 15:44:22.938494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2634974 -4.2993555 -4.3319616 -4.3450346 -4.3307347 -4.2973876 -4.2642217 -4.2519078 -4.2607174 -4.2776213 -4.2921238 -4.3043056 -4.31399 -4.3184605 -4.3198047][-4.2507625 -4.2909455 -4.3266048 -4.3386683 -4.3194628 -4.2815032 -4.241889 -4.2289371 -4.2455235 -4.2725492 -4.2929497 -4.3077631 -4.3151164 -4.3149252 -4.3097415][-4.24853 -4.2919035 -4.3278213 -4.3385844 -4.3165212 -4.2746906 -4.2259188 -4.2070174 -4.2292418 -4.2655911 -4.2905045 -4.3069882 -4.311214 -4.3067489 -4.2958074][-4.2599554 -4.3058157 -4.3409328 -4.34825 -4.3185191 -4.2657695 -4.2040977 -4.173954 -4.1996875 -4.2469707 -4.2782741 -4.2978048 -4.3010488 -4.2962685 -4.2850122][-4.2806249 -4.3259144 -4.3574557 -4.3535609 -4.3069181 -4.2330875 -4.1538939 -4.1112356 -4.13735 -4.1959372 -4.24026 -4.27049 -4.2817297 -4.2858267 -4.2808862][-4.2970753 -4.336679 -4.3614554 -4.3427153 -4.2728977 -4.1747909 -4.0764012 -4.0234933 -4.0474091 -4.1145935 -4.177093 -4.2274175 -4.2548552 -4.2708068 -4.2718821][-4.2961349 -4.329515 -4.3486962 -4.3188782 -4.23489 -4.1175017 -4.0007477 -3.9373069 -3.9595683 -4.032527 -4.1115117 -4.1820703 -4.225204 -4.2506542 -4.2561316][-4.2850718 -4.3116755 -4.3236275 -4.2899179 -4.2094145 -4.0926065 -3.9727111 -3.9006791 -3.9175868 -3.9876068 -4.07154 -4.1508846 -4.2025728 -4.2325664 -4.2378392][-4.2752628 -4.2890825 -4.2864823 -4.2473984 -4.1797266 -4.0826964 -3.9813371 -3.9160025 -3.9295743 -3.9905682 -4.0654812 -4.1405711 -4.190989 -4.2166085 -4.2159591][-4.2661648 -4.2699952 -4.2529092 -4.2079644 -4.1507759 -4.07626 -3.9989038 -3.9553723 -3.975363 -4.0296626 -4.090107 -4.15065 -4.1913018 -4.2067227 -4.1997662][-4.2599754 -4.258934 -4.2356887 -4.1910238 -4.1462693 -4.0962772 -4.0447345 -4.0252848 -4.0537243 -4.1018758 -4.1467271 -4.1875482 -4.2139282 -4.2199969 -4.21048][-4.2625189 -4.2611136 -4.2408738 -4.2067385 -4.1782141 -4.1490703 -4.1190124 -4.1160879 -4.1462479 -4.1837668 -4.213419 -4.2356067 -4.2516274 -4.2557468 -4.2502408][-4.282 -4.2820358 -4.27033 -4.2495818 -4.2338867 -4.21776 -4.2057796 -4.21348 -4.2376618 -4.2629313 -4.2777982 -4.2861567 -4.2959185 -4.3012691 -4.301249][-4.3116555 -4.3148584 -4.3121181 -4.3040242 -4.2974329 -4.2891169 -4.2869368 -4.2975273 -4.3136344 -4.3283334 -4.3336954 -4.3348451 -4.3400722 -4.3455276 -4.3478208][-4.331223 -4.3353128 -4.3368244 -4.335155 -4.3322239 -4.3281093 -4.3302441 -4.3401031 -4.3491521 -4.3567176 -4.3583632 -4.358242 -4.3623462 -4.366756 -4.367743]]...]
INFO - root - 2017-12-07 15:44:39.276374: step 12110, loss = 2.05, batch loss = 2.00 (10.1 examples/sec; 1.590 sec/batch; 68h:05m:53s remains)
INFO - root - 2017-12-07 15:44:55.810976: step 12120, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.742 sec/batch; 74h:34m:17s remains)
INFO - root - 2017-12-07 15:45:11.954192: step 12130, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 68h:24m:26s remains)
INFO - root - 2017-12-07 15:45:28.370277: step 12140, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.718 sec/batch; 73h:33m:26s remains)
INFO - root - 2017-12-07 15:45:44.446971: step 12150, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.616 sec/batch; 69h:10m:16s remains)
INFO - root - 2017-12-07 15:46:00.507694: step 12160, loss = 2.09, batch loss = 2.04 (9.8 examples/sec; 1.640 sec/batch; 70h:12m:35s remains)
INFO - root - 2017-12-07 15:46:16.806521: step 12170, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.587 sec/batch; 67h:55m:44s remains)
INFO - root - 2017-12-07 15:46:33.171970: step 12180, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.596 sec/batch; 68h:18m:36s remains)
INFO - root - 2017-12-07 15:46:49.696029: step 12190, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.725 sec/batch; 73h:48m:35s remains)
INFO - root - 2017-12-07 15:47:05.810026: step 12200, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.607 sec/batch; 68h:45m:02s remains)
2017-12-07 15:47:07.132298: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2504668 -4.2357125 -4.2141843 -4.207777 -4.2047949 -4.2183361 -4.2199621 -4.2162409 -4.2116175 -4.2021222 -4.2084413 -4.2016921 -4.1848259 -4.1747913 -4.1674867][-4.2596383 -4.2308254 -4.1927843 -4.1793137 -4.1785936 -4.1973662 -4.2080264 -4.2151127 -4.2172375 -4.2211752 -4.2303038 -4.2167554 -4.1934137 -4.1843553 -4.1744967][-4.2488146 -4.2043643 -4.1596375 -4.1455407 -4.1468177 -4.1618547 -4.1777768 -4.199194 -4.2067075 -4.2189016 -4.2280307 -4.2140117 -4.1881495 -4.1815324 -4.1710453][-4.2170992 -4.1660023 -4.1241212 -4.1099324 -4.1103363 -4.1230669 -4.1400337 -4.1680503 -4.1791825 -4.1911211 -4.1981988 -4.1921077 -4.1706629 -4.1703968 -4.165966][-4.1672621 -4.1206017 -4.0863786 -4.0783281 -4.0770054 -4.0824585 -4.0954647 -4.1226931 -4.1434011 -4.1572242 -4.16726 -4.16463 -4.1456485 -4.1498051 -4.1610165][-4.1058731 -4.0658393 -4.0413384 -4.0442786 -4.043241 -4.0332041 -4.0244746 -4.0388536 -4.0776258 -4.11891 -4.147686 -4.1572685 -4.1461277 -4.1495934 -4.1656022][-4.0670195 -4.0403328 -4.0262446 -4.0359335 -4.0300279 -3.9978337 -3.9453268 -3.9233983 -3.9768467 -4.0595059 -4.1203251 -4.1546321 -4.1563716 -4.1587462 -4.1639628][-4.0763283 -4.0736532 -4.0750928 -4.0779557 -4.0601068 -4.0188928 -3.9467247 -3.8870177 -3.9173176 -4.0025778 -4.0797405 -4.1279316 -4.1372366 -4.1404395 -4.1406503][-4.1065874 -4.118155 -4.1323133 -4.1331215 -4.1149879 -4.0938931 -4.0506449 -3.9997797 -3.9934506 -4.0303578 -4.0745711 -4.1036425 -4.1051407 -4.1053619 -4.1070876][-4.1347675 -4.1386414 -4.1578407 -4.1672854 -4.1657476 -4.168736 -4.1546059 -4.1282654 -4.1105046 -4.1105433 -4.114563 -4.1059456 -4.0841732 -4.0772786 -4.0809159][-4.1588097 -4.1540689 -4.1735969 -4.1942072 -4.2033186 -4.2111163 -4.2019787 -4.1850572 -4.1690512 -4.1634765 -4.1566033 -4.1332216 -4.0959306 -4.0798626 -4.0790606][-4.1938777 -4.1930509 -4.2135844 -4.2366266 -4.2474594 -4.2492557 -4.2323332 -4.2143569 -4.2067118 -4.2070966 -4.2022915 -4.1803803 -4.1464458 -4.1335888 -4.1291037][-4.2403011 -4.2430906 -4.2644863 -4.281342 -4.2877674 -4.2822595 -4.26288 -4.2457929 -4.2402925 -4.2430325 -4.2407026 -4.2261834 -4.199955 -4.192153 -4.1886353][-4.2849107 -4.287324 -4.302834 -4.308476 -4.3062153 -4.2983623 -4.2810125 -4.2673984 -4.2630234 -4.2661219 -4.2657042 -4.2537622 -4.2350483 -4.2292619 -4.2275333][-4.3085556 -4.3108072 -4.3206873 -4.3194394 -4.3146949 -4.3156209 -4.3059568 -4.2932196 -4.2864275 -4.2855558 -4.2775354 -4.2625637 -4.2501616 -4.2489409 -4.2481618]]...]
INFO - root - 2017-12-07 15:47:23.348290: step 12210, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.553 sec/batch; 66h:28m:01s remains)
INFO - root - 2017-12-07 15:47:39.819106: step 12220, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.733 sec/batch; 74h:08m:06s remains)
INFO - root - 2017-12-07 15:47:56.167157: step 12230, loss = 2.11, batch loss = 2.05 (10.1 examples/sec; 1.588 sec/batch; 67h:56m:44s remains)
INFO - root - 2017-12-07 15:48:12.524638: step 12240, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.724 sec/batch; 73h:46m:16s remains)
INFO - root - 2017-12-07 15:48:28.938505: step 12250, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.678 sec/batch; 71h:46m:14s remains)
INFO - root - 2017-12-07 15:48:45.120803: step 12260, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.652 sec/batch; 70h:39m:16s remains)
INFO - root - 2017-12-07 15:49:01.099898: step 12270, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.571 sec/batch; 67h:10m:52s remains)
INFO - root - 2017-12-07 15:49:17.338914: step 12280, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.578 sec/batch; 67h:30m:04s remains)
INFO - root - 2017-12-07 15:49:33.594416: step 12290, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.657 sec/batch; 70h:51m:14s remains)
INFO - root - 2017-12-07 15:49:49.936439: step 12300, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.565 sec/batch; 66h:56m:31s remains)
2017-12-07 15:49:51.244340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3325481 -4.3283567 -4.3228755 -4.3166137 -4.3109021 -4.3060513 -4.302134 -4.2992568 -4.296771 -4.2949624 -4.2939672 -4.2911892 -4.284647 -4.2773237 -4.2701459][-4.3163176 -4.3113508 -4.3053436 -4.2987323 -4.2919555 -4.2845659 -4.2772913 -4.271862 -4.2673931 -4.266046 -4.2686191 -4.2674685 -4.2584648 -4.2449436 -4.2303619][-4.2959905 -4.2928081 -4.2881927 -4.2801771 -4.2691393 -4.2554555 -4.2438936 -4.2365546 -4.2337818 -4.2387714 -4.2502251 -4.2549043 -4.2460246 -4.2268968 -4.2028108][-4.2701597 -4.2725453 -4.2678714 -4.2538052 -4.2342134 -4.2119665 -4.1949949 -4.1858649 -4.1888275 -4.2061882 -4.230391 -4.2433643 -4.2378039 -4.2162728 -4.1858993][-4.2372174 -4.2450304 -4.235724 -4.2107534 -4.1788669 -4.146512 -4.1211329 -4.109076 -4.1197891 -4.1547017 -4.1959443 -4.2189274 -4.2184339 -4.1985803 -4.1671886][-4.2104292 -4.2169523 -4.1970973 -4.1579704 -4.1113296 -4.0646715 -4.0231528 -4.0023904 -4.0214672 -4.0751686 -4.1310759 -4.1628585 -4.1688638 -4.1538763 -4.1261454][-4.2021589 -4.202682 -4.1715851 -4.1204824 -4.0597348 -3.9913182 -3.9232063 -3.8868918 -3.9149783 -3.9850745 -4.0509462 -4.0863771 -4.0945978 -4.0859418 -4.0665474][-4.2140689 -4.2103152 -4.1747022 -4.1206851 -4.054162 -3.9716718 -3.8822722 -3.8346522 -3.866636 -3.9358354 -3.9952629 -4.0261188 -4.0354195 -4.0362496 -4.0289845][-4.2345471 -4.2296119 -4.20042 -4.1576548 -4.103672 -4.0327153 -3.9537809 -3.9173412 -3.9453776 -3.9933667 -4.0299878 -4.0451732 -4.0501966 -4.0536737 -4.0511808][-4.2481956 -4.2480283 -4.2319837 -4.2067304 -4.1725173 -4.1262717 -4.0749583 -4.0563917 -4.0726876 -4.0951395 -4.1093597 -4.1090684 -4.1085448 -4.1093645 -4.105413][-4.2494273 -4.2549963 -4.2516356 -4.2417984 -4.2240248 -4.1989932 -4.1729889 -4.1675782 -4.1728764 -4.1763716 -4.1742067 -4.1650043 -4.1619797 -4.161664 -4.1568279][-4.2363334 -4.2451048 -4.2498765 -4.24955 -4.2422023 -4.2304635 -4.2200689 -4.2205424 -4.2194977 -4.2135386 -4.2047453 -4.1940823 -4.1917591 -4.1916389 -4.1882095][-4.228066 -4.2349129 -4.2413745 -4.2455392 -4.2442632 -4.2389269 -4.2355847 -4.2374649 -4.2358556 -4.2306914 -4.2237096 -4.2167616 -4.2157774 -4.2166605 -4.2161884][-4.2422161 -4.2457709 -4.2496991 -4.2532907 -4.2531056 -4.25031 -4.2486367 -4.2502146 -4.2500587 -4.2487903 -4.2464342 -4.2432952 -4.2435212 -4.2458396 -4.248003][-4.2761378 -4.2769203 -4.2777028 -4.2787 -4.2790756 -4.2788143 -4.2788968 -4.2789531 -4.2778907 -4.277678 -4.2775116 -4.2768555 -4.2778029 -4.2809386 -4.2844834]]...]
INFO - root - 2017-12-07 15:50:07.698597: step 12310, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.666 sec/batch; 71h:14m:10s remains)
INFO - root - 2017-12-07 15:50:24.065668: step 12320, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.607 sec/batch; 68h:42m:21s remains)
INFO - root - 2017-12-07 15:50:40.292431: step 12330, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 1.547 sec/batch; 66h:07m:20s remains)
INFO - root - 2017-12-07 15:50:56.519546: step 12340, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.722 sec/batch; 73h:37m:35s remains)
INFO - root - 2017-12-07 15:51:12.912847: step 12350, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.618 sec/batch; 69h:10m:40s remains)
INFO - root - 2017-12-07 15:51:29.216853: step 12360, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.653 sec/batch; 70h:38m:33s remains)
INFO - root - 2017-12-07 15:51:45.302938: step 12370, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.594 sec/batch; 68h:08m:11s remains)
INFO - root - 2017-12-07 15:52:01.383633: step 12380, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.626 sec/batch; 69h:30m:39s remains)
INFO - root - 2017-12-07 15:52:17.542421: step 12390, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.656 sec/batch; 70h:45m:19s remains)
INFO - root - 2017-12-07 15:52:33.857051: step 12400, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.586 sec/batch; 67h:47m:36s remains)
2017-12-07 15:52:35.255696: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2749367 -4.2609954 -4.2377586 -4.2095351 -4.1958117 -4.1984339 -4.2007241 -4.19251 -4.1903715 -4.1962729 -4.2120004 -4.2308846 -4.2517633 -4.2755685 -4.2977052][-4.279901 -4.2581663 -4.2290578 -4.1985507 -4.18899 -4.1953788 -4.2000918 -4.1931214 -4.1905003 -4.1946731 -4.2064834 -4.2223454 -4.2386055 -4.2615337 -4.2848558][-4.27447 -4.2525115 -4.2229123 -4.1935453 -4.1841536 -4.1879764 -4.19551 -4.1949153 -4.1960683 -4.2000713 -4.2081127 -4.2197547 -4.22855 -4.2463741 -4.2704735][-4.2605143 -4.2403917 -4.2107029 -4.1792293 -4.1614795 -4.1608458 -4.172718 -4.1845503 -4.1977849 -4.2034693 -4.2076216 -4.2147388 -4.2185454 -4.2303205 -4.25272][-4.23561 -4.2173896 -4.1853275 -4.145699 -4.11539 -4.1104097 -4.1327653 -4.1617556 -4.1889629 -4.1959767 -4.1927471 -4.1924047 -4.1925874 -4.2004104 -4.2208915][-4.1971941 -4.1788015 -4.1398215 -4.0854092 -4.0362806 -4.024559 -4.0618043 -4.1130075 -4.1580567 -4.1687675 -4.1584759 -4.1502323 -4.1442485 -4.1485372 -4.1669469][-4.1422329 -4.1239405 -4.0751376 -4.0002894 -3.924351 -3.8968527 -3.947844 -4.0249696 -4.0934548 -4.11493 -4.1042762 -4.0900412 -4.0749736 -4.0757909 -4.0903797][-4.0828876 -4.0642676 -4.0090165 -3.9173923 -3.8139975 -3.7631445 -3.8195817 -3.9182875 -4.0091863 -4.0444403 -4.0404406 -4.0214205 -3.9960361 -3.9909601 -4.0045948][-4.0717564 -4.0588293 -4.0141931 -3.9340296 -3.8371308 -3.7775977 -3.8165483 -3.9076731 -3.9971862 -4.035192 -4.0307112 -4.0068488 -3.9754384 -3.9666982 -3.9805686][-4.125185 -4.1164489 -4.0875559 -4.037631 -3.9752665 -3.9313924 -3.9484653 -4.0103378 -4.0760055 -4.1048284 -4.0957685 -4.0731721 -4.0467958 -4.0427866 -4.0587482][-4.2117 -4.2046542 -4.1861873 -4.1603308 -4.1292233 -4.1056533 -4.1141477 -4.1525187 -4.1949391 -4.2117124 -4.1993041 -4.1791348 -4.1598768 -4.1613955 -4.1794138][-4.2904139 -4.2858262 -4.2731895 -4.2587366 -4.244688 -4.2356586 -4.2414494 -4.2647724 -4.2914872 -4.3018594 -4.2914267 -4.2761784 -4.2643557 -4.2693982 -4.286366][-4.3384433 -4.3354454 -4.3277473 -4.3190708 -4.3124537 -4.3087888 -4.311583 -4.3247771 -4.3411274 -4.3487606 -4.3437133 -4.3351474 -4.32881 -4.3344793 -4.3462529][-4.3625054 -4.3609877 -4.3575091 -4.3538423 -4.3512964 -4.3496623 -4.3512058 -4.3584633 -4.3672895 -4.3719029 -4.370225 -4.3656969 -4.3623695 -4.3648515 -4.3704967][-4.3672566 -4.3684106 -4.3676243 -4.3670311 -4.3669052 -4.3669763 -4.3676162 -4.3702645 -4.3735428 -4.3757749 -4.375596 -4.373477 -4.3719239 -4.3723431 -4.3740549]]...]
INFO - root - 2017-12-07 15:52:51.585031: step 12410, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.560 sec/batch; 66h:41m:06s remains)
INFO - root - 2017-12-07 15:53:07.707659: step 12420, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.653 sec/batch; 70h:37m:52s remains)
INFO - root - 2017-12-07 15:53:23.905908: step 12430, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 68h:27m:36s remains)
INFO - root - 2017-12-07 15:53:40.085261: step 12440, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.632 sec/batch; 69h:43m:15s remains)
INFO - root - 2017-12-07 15:53:56.124903: step 12450, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.559 sec/batch; 66h:36m:56s remains)
INFO - root - 2017-12-07 15:54:12.212792: step 12460, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.655 sec/batch; 70h:41m:58s remains)
INFO - root - 2017-12-07 15:54:28.371403: step 12470, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.639 sec/batch; 69h:59m:30s remains)
INFO - root - 2017-12-07 15:54:44.444995: step 12480, loss = 2.11, batch loss = 2.05 (9.5 examples/sec; 1.692 sec/batch; 72h:15m:21s remains)
INFO - root - 2017-12-07 15:55:00.599374: step 12490, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.579 sec/batch; 67h:27m:12s remains)
INFO - root - 2017-12-07 15:55:16.911516: step 12500, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.539 sec/batch; 65h:42m:29s remains)
2017-12-07 15:55:18.319678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.252 -4.2475057 -4.249197 -4.2500229 -4.25258 -4.2605748 -4.2690673 -4.2773914 -4.2765455 -4.2619414 -4.2441797 -4.2311993 -4.2217369 -4.2203851 -4.2336607][-4.2198558 -4.2170415 -4.2165179 -4.21822 -4.2255068 -4.2428565 -4.2592583 -4.2735615 -4.2762341 -4.262866 -4.2441359 -4.2257504 -4.2083845 -4.2008042 -4.2092037][-4.1841383 -4.183176 -4.183053 -4.1883106 -4.2007308 -4.2247319 -4.2458863 -4.2632036 -4.2695651 -4.2601781 -4.2438383 -4.2208357 -4.1948233 -4.18269 -4.1888618][-4.1532555 -4.1589127 -4.162941 -4.170321 -4.18346 -4.2062979 -4.2248921 -4.2405844 -4.2488203 -4.2455521 -4.235404 -4.213325 -4.1845379 -4.1717005 -4.1793551][-4.1417069 -4.1527758 -4.1596651 -4.1654205 -4.1700296 -4.1822062 -4.1942172 -4.2044506 -4.21283 -4.2183642 -4.2182751 -4.2018609 -4.1769695 -4.1684356 -4.1810718][-4.155982 -4.1647992 -4.16599 -4.1608853 -4.147131 -4.1353517 -4.1297464 -4.1310883 -4.139719 -4.1543956 -4.1654406 -4.1607118 -4.1499071 -4.1544075 -4.1774521][-4.1800647 -4.1751909 -4.1628981 -4.1399961 -4.1000004 -4.0545249 -4.0195818 -4.0094581 -4.0270548 -4.0562773 -4.0815849 -4.0946097 -4.104516 -4.1284738 -4.1660724][-4.183218 -4.1639853 -4.1389265 -4.1015363 -4.0438986 -3.9763105 -3.9271135 -3.921406 -3.9534552 -3.9963961 -4.0339408 -4.0616293 -4.0876975 -4.1242943 -4.1694908][-4.1745143 -4.1491137 -4.1237926 -4.0861473 -4.0307126 -3.9736717 -3.948658 -3.9638326 -3.99481 -4.0283771 -4.0582252 -4.0869 -4.1170616 -4.1511526 -4.1902428][-4.1686625 -4.1470151 -4.131979 -4.1095781 -4.0770783 -4.052896 -4.0575881 -4.0796509 -4.1015792 -4.1190653 -4.1314459 -4.1479859 -4.1687117 -4.1878242 -4.2123384][-4.1675215 -4.1544743 -4.1557469 -4.1548953 -4.1519532 -4.1537452 -4.1683841 -4.1881409 -4.2018685 -4.2058496 -4.2025371 -4.2018204 -4.2060018 -4.2096229 -4.2225642][-4.1730337 -4.1744843 -4.1909127 -4.2063541 -4.2190542 -4.2298265 -4.2407808 -4.2521257 -4.2599835 -4.2560611 -4.2435226 -4.2307324 -4.2225432 -4.2172189 -4.2234974][-4.1814742 -4.1993461 -4.2254162 -4.2459617 -4.2610545 -4.2712822 -4.2753654 -4.2800579 -4.2832952 -4.2742863 -4.2572961 -4.2378807 -4.2219496 -4.2120562 -4.2167506][-4.1919422 -4.2172084 -4.2448025 -4.2633433 -4.2755728 -4.283186 -4.2840328 -4.285718 -4.2845168 -4.2720418 -4.2536712 -4.23149 -4.2111292 -4.1999192 -4.2068734][-4.2056179 -4.2264032 -4.2502522 -4.2647004 -4.2736588 -4.2782469 -4.2782321 -4.2785835 -4.2745237 -4.2620916 -4.2442322 -4.2226338 -4.2031446 -4.1960826 -4.208921]]...]
INFO - root - 2017-12-07 15:55:34.307767: step 12510, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.656 sec/batch; 70h:42m:52s remains)
INFO - root - 2017-12-07 15:55:50.496158: step 12520, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.565 sec/batch; 66h:49m:06s remains)
INFO - root - 2017-12-07 15:56:06.796347: step 12530, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.572 sec/batch; 67h:08m:07s remains)
INFO - root - 2017-12-07 15:56:23.058022: step 12540, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.671 sec/batch; 71h:20m:38s remains)
INFO - root - 2017-12-07 15:56:39.381066: step 12550, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.593 sec/batch; 68h:01m:26s remains)
INFO - root - 2017-12-07 15:56:55.796709: step 12560, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.657 sec/batch; 70h:44m:05s remains)
INFO - root - 2017-12-07 15:57:11.705810: step 12570, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.568 sec/batch; 66h:57m:14s remains)
INFO - root - 2017-12-07 15:57:28.106075: step 12580, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 1.732 sec/batch; 73h:55m:25s remains)
INFO - root - 2017-12-07 15:57:44.178175: step 12590, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.549 sec/batch; 66h:06m:27s remains)
INFO - root - 2017-12-07 15:58:00.434785: step 12600, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 1.678 sec/batch; 71h:36m:19s remains)
2017-12-07 15:58:01.816699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2201757 -4.2035742 -4.1948028 -4.1827645 -4.1578913 -4.1235752 -4.107389 -4.1114187 -4.1165953 -4.1159296 -4.1236429 -4.1409645 -4.1535568 -4.1594148 -4.1628571][-4.2057109 -4.1903753 -4.1861887 -4.1804132 -4.1602225 -4.1333618 -4.1178641 -4.1168289 -4.11653 -4.1074729 -4.1046443 -4.1156406 -4.1286597 -4.137949 -4.1454029][-4.21169 -4.2009325 -4.1998172 -4.1985254 -4.1847186 -4.1680746 -4.1558094 -4.1496925 -4.1432157 -4.127049 -4.1160975 -4.1178513 -4.1240988 -4.1298633 -4.1351013][-4.2215676 -4.215508 -4.2107725 -4.2049885 -4.1959949 -4.1884446 -4.1794348 -4.1743603 -4.17319 -4.1621823 -4.1538806 -4.1522045 -4.1510558 -4.1493106 -4.1484361][-4.219346 -4.2172675 -4.2064767 -4.1914349 -4.1794081 -4.17261 -4.1633177 -4.1630373 -4.1751966 -4.1830735 -4.1881828 -4.1909051 -4.1889434 -4.1839709 -4.1796041][-4.1989117 -4.2006674 -4.18858 -4.1687093 -4.1530542 -4.1425347 -4.132144 -4.1354728 -4.1593552 -4.1843262 -4.2010264 -4.2089229 -4.2100682 -4.2066946 -4.2038841][-4.1562347 -4.1623387 -4.1548109 -4.13772 -4.12195 -4.1074986 -4.0936055 -4.0956097 -4.1227412 -4.1543837 -4.1744852 -4.1827564 -4.1863894 -4.1886759 -4.1949539][-4.102829 -4.1035857 -4.0935612 -4.0791984 -4.0674028 -4.0533752 -4.0338941 -4.0283871 -4.0511055 -4.0848851 -4.1071196 -4.1144691 -4.1219125 -4.1358323 -4.1556482][-4.0680637 -4.0528197 -4.02884 -4.0119047 -4.0048051 -3.9946125 -3.9726305 -3.9611099 -3.9768593 -4.00912 -4.03037 -4.0306945 -4.0348935 -4.0572362 -4.0860958][-4.105021 -4.0744424 -4.0374045 -4.0143723 -4.0076084 -4.001369 -3.9838874 -3.9726021 -3.9805741 -4.0035615 -4.0197344 -4.0122266 -4.0059218 -4.0214634 -4.0454607][-4.1805248 -4.15156 -4.1201468 -4.1012335 -4.0963392 -4.0943632 -4.0843673 -4.0747638 -4.0736971 -4.0840344 -4.0929985 -4.0788069 -4.0603032 -4.0614882 -4.0715332][-4.2323866 -4.216691 -4.2000966 -4.1929779 -4.1910243 -4.1886959 -4.179729 -4.1702218 -4.1648984 -4.1643291 -4.1648216 -4.1486044 -4.1297622 -4.1272478 -4.1298656][-4.2389684 -4.2382278 -4.2373633 -4.2402873 -4.240941 -4.2379518 -4.2276549 -4.2171688 -4.208096 -4.2003083 -4.1944642 -4.1808505 -4.1725912 -4.17695 -4.17994][-4.2328467 -4.2356358 -4.2377405 -4.2389603 -4.2356958 -4.2291331 -4.2181544 -4.2080579 -4.1978993 -4.1871543 -4.1770506 -4.1666021 -4.1693673 -4.18373 -4.1935034][-4.2371988 -4.2326035 -4.2266212 -4.2179718 -4.2061749 -4.1954513 -4.1845355 -4.1759691 -4.1678405 -4.1581311 -4.1487527 -4.1428204 -4.1535378 -4.1719089 -4.1857047]]...]
INFO - root - 2017-12-07 15:58:18.030264: step 12610, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.725 sec/batch; 73h:37m:21s remains)
INFO - root - 2017-12-07 15:58:34.166524: step 12620, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.550 sec/batch; 66h:09m:47s remains)
INFO - root - 2017-12-07 15:58:50.213509: step 12630, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.716 sec/batch; 73h:12m:21s remains)
INFO - root - 2017-12-07 15:59:06.374976: step 12640, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.607 sec/batch; 68h:34m:50s remains)
INFO - root - 2017-12-07 15:59:22.741319: step 12650, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.631 sec/batch; 69h:36m:35s remains)
INFO - root - 2017-12-07 15:59:38.767445: step 12660, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.679 sec/batch; 71h:38m:10s remains)
INFO - root - 2017-12-07 15:59:54.830680: step 12670, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.536 sec/batch; 65h:30m:35s remains)
INFO - root - 2017-12-07 16:00:11.150277: step 12680, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.570 sec/batch; 66h:58m:24s remains)
INFO - root - 2017-12-07 16:00:27.345715: step 12690, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.672 sec/batch; 71h:19m:31s remains)
INFO - root - 2017-12-07 16:00:43.451662: step 12700, loss = 2.11, batch loss = 2.05 (10.3 examples/sec; 1.554 sec/batch; 66h:16m:13s remains)
2017-12-07 16:00:44.676853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2568092 -4.2504177 -4.260107 -4.2784925 -4.2809477 -4.2702346 -4.2629871 -4.253479 -4.2475877 -4.2534761 -4.2605934 -4.24565 -4.2216697 -4.21836 -4.2350216][-4.2540808 -4.247673 -4.25321 -4.267067 -4.2657566 -4.2481947 -4.2327032 -4.2189288 -4.214292 -4.2234325 -4.2352543 -4.2217255 -4.1997786 -4.2017198 -4.2255397][-4.2644873 -4.2594795 -4.25975 -4.26576 -4.2575016 -4.23159 -4.2027321 -4.1818676 -4.1784191 -4.19244 -4.2083287 -4.1982613 -4.182024 -4.1911011 -4.2216067][-4.2793717 -4.2778969 -4.2757092 -4.2725072 -4.252655 -4.2162995 -4.1776552 -4.1563773 -4.1599264 -4.1803479 -4.1987972 -4.1925397 -4.1788254 -4.1884427 -4.2249336][-4.2882576 -4.292367 -4.2890344 -4.2742324 -4.240181 -4.1918468 -4.147408 -4.1314368 -4.1513572 -4.1832814 -4.205133 -4.2038412 -4.1923895 -4.1965094 -4.2300053][-4.2874489 -4.2968864 -4.2940936 -4.2706585 -4.2232618 -4.15735 -4.0971966 -4.079823 -4.1218295 -4.1762114 -4.210011 -4.2162333 -4.2099252 -4.2075305 -4.2307944][-4.2842727 -4.296699 -4.2954507 -4.2668166 -4.2063451 -4.1142445 -4.0168924 -3.9873796 -4.0618038 -4.146955 -4.1955009 -4.212719 -4.2160459 -4.2148952 -4.2302876][-4.2840791 -4.299582 -4.3024445 -4.2732444 -4.207962 -4.0962639 -3.9581778 -3.897588 -3.9954944 -4.1051488 -4.1687722 -4.1966791 -4.2076378 -4.2098937 -4.2236719][-4.282618 -4.30005 -4.3100839 -4.2873721 -4.2315264 -4.1322017 -4.002841 -3.9350405 -4.003253 -4.0946989 -4.1558909 -4.1896653 -4.2038579 -4.2069335 -4.2205234][-4.281014 -4.2981825 -4.314332 -4.30007 -4.2603993 -4.1892242 -4.0973759 -4.0477538 -4.0766988 -4.126317 -4.1672897 -4.1971512 -4.2111845 -4.2156758 -4.2267008][-4.2772427 -4.2927413 -4.3089876 -4.3012424 -4.272922 -4.2202 -4.1597838 -4.128427 -4.1397963 -4.1643038 -4.1904922 -4.2163925 -4.2311983 -4.2384257 -4.2446947][-4.2703772 -4.2844963 -4.2983079 -4.2916961 -4.2653618 -4.2201629 -4.1826973 -4.1688447 -4.1753225 -4.1916947 -4.20839 -4.2282205 -4.2422943 -4.2487578 -4.2529531][-4.2605853 -4.27387 -4.2858825 -4.2802539 -4.2546353 -4.2149076 -4.1910634 -4.1832004 -4.1815939 -4.194231 -4.2058525 -4.2258973 -4.239831 -4.2411451 -4.2442961][-4.2576165 -4.2703362 -4.2815595 -4.2771568 -4.2553959 -4.2226758 -4.205297 -4.1949019 -4.1832447 -4.189003 -4.1970038 -4.2167096 -4.2293043 -4.2281785 -4.2324805][-4.2679515 -4.2753763 -4.2846131 -4.2807078 -4.2662215 -4.2456031 -4.2339759 -4.2211747 -4.20453 -4.204494 -4.20971 -4.2240486 -4.2297578 -4.2273307 -4.2287755]]...]
INFO - root - 2017-12-07 16:01:00.938013: step 12710, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.563 sec/batch; 66h:40m:46s remains)
INFO - root - 2017-12-07 16:01:17.054053: step 12720, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.649 sec/batch; 70h:20m:24s remains)
INFO - root - 2017-12-07 16:01:33.259454: step 12730, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 1.538 sec/batch; 65h:35m:21s remains)
INFO - root - 2017-12-07 16:01:49.567765: step 12740, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.690 sec/batch; 72h:04m:02s remains)
INFO - root - 2017-12-07 16:02:05.469491: step 12750, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.556 sec/batch; 66h:19m:43s remains)
INFO - root - 2017-12-07 16:02:21.669078: step 12760, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.721 sec/batch; 73h:22m:22s remains)
INFO - root - 2017-12-07 16:02:37.883492: step 12770, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.562 sec/batch; 66h:35m:20s remains)
INFO - root - 2017-12-07 16:02:54.156047: step 12780, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.647 sec/batch; 70h:12m:44s remains)
INFO - root - 2017-12-07 16:03:10.470750: step 12790, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.675 sec/batch; 71h:25m:21s remains)
INFO - root - 2017-12-07 16:03:26.674497: step 12800, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.606 sec/batch; 68h:27m:25s remains)
2017-12-07 16:03:27.961542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2066393 -4.2170682 -4.2283816 -4.230741 -4.234591 -4.2367334 -4.2376981 -4.2343526 -4.2272081 -4.2208 -4.2219586 -4.2225266 -4.2230248 -4.227499 -4.2328992][-4.1905203 -4.2129545 -4.2328134 -4.2308526 -4.2249851 -4.2176838 -4.2163324 -4.2204666 -4.2199287 -4.2157116 -4.2084236 -4.1971135 -4.1872959 -4.1872711 -4.1959009][-4.182137 -4.2106762 -4.2317119 -4.2243938 -4.2136869 -4.196444 -4.1925945 -4.2033415 -4.2167859 -4.2236133 -4.2106204 -4.1918435 -4.1692777 -4.1565838 -4.1582761][-4.1719894 -4.20487 -4.2201152 -4.2036638 -4.1851234 -4.1616769 -4.1514254 -4.1663508 -4.2002139 -4.2255363 -4.216362 -4.1942377 -4.1630774 -4.140976 -4.1379642][-4.1514525 -4.1849561 -4.1957693 -4.1684957 -4.133985 -4.0990133 -4.0753393 -4.0980005 -4.1599412 -4.2119584 -4.2158 -4.1964703 -4.1655211 -4.1401639 -4.1373248][-4.1322389 -4.153686 -4.1489863 -4.1098576 -4.0591941 -4.0039988 -3.9599414 -3.9906707 -4.0908432 -4.1826634 -4.211062 -4.2042923 -4.1778073 -4.15427 -4.1521239][-4.1555037 -4.1571236 -4.1279492 -4.0732713 -4.0025592 -3.9061837 -3.8140697 -3.8391459 -3.9874227 -4.1255555 -4.1853337 -4.201932 -4.1911988 -4.1773496 -4.172699][-4.204762 -4.1998229 -4.1667924 -4.112431 -4.0303097 -3.9006352 -3.7475119 -3.7292995 -3.9002726 -4.0674114 -4.15507 -4.1919079 -4.2003407 -4.1998034 -4.1973958][-4.2240124 -4.2225952 -4.2068906 -4.170969 -4.1130071 -4.0124235 -3.8747332 -3.8195109 -3.9260919 -4.0618939 -4.1519089 -4.1968865 -4.2128 -4.2192912 -4.2211742][-4.2181268 -4.2220707 -4.2235203 -4.206605 -4.1759052 -4.1163239 -4.0282493 -3.9764028 -4.0215168 -4.1105437 -4.18563 -4.2257919 -4.2351413 -4.237062 -4.2386165][-4.2105994 -4.2136011 -4.2205577 -4.2154875 -4.1986594 -4.1622748 -4.1125493 -4.0780067 -4.0993 -4.1609874 -4.2226915 -4.2537742 -4.2552514 -4.2519159 -4.2529745][-4.2033954 -4.1999707 -4.2037606 -4.2068672 -4.2045193 -4.1879659 -4.1628723 -4.1414962 -4.1555614 -4.202239 -4.254158 -4.2791004 -4.2754879 -4.2656384 -4.2610221][-4.1852517 -4.1766868 -4.1783824 -4.1911249 -4.2017641 -4.1976104 -4.1890869 -4.1810312 -4.1967134 -4.2376208 -4.2761478 -4.2929296 -4.2880507 -4.2734513 -4.2601171][-4.1752625 -4.1641665 -4.1637621 -4.1758556 -4.1882334 -4.1845732 -4.1835876 -4.1890326 -4.2122011 -4.2460837 -4.2693419 -4.2778792 -4.2747483 -4.2608938 -4.2406735][-4.1827979 -4.1677876 -4.166502 -4.1768575 -4.1856894 -4.1780114 -4.1743908 -4.1868243 -4.2122159 -4.23822 -4.2469587 -4.2455349 -4.2474957 -4.2428102 -4.2257533]]...]
INFO - root - 2017-12-07 16:03:44.218601: step 12810, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.627 sec/batch; 69h:20m:17s remains)
INFO - root - 2017-12-07 16:04:00.385020: step 12820, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.677 sec/batch; 71h:27m:18s remains)
INFO - root - 2017-12-07 16:04:16.746520: step 12830, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.603 sec/batch; 68h:17m:49s remains)
INFO - root - 2017-12-07 16:04:33.069534: step 12840, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.691 sec/batch; 72h:03m:20s remains)
INFO - root - 2017-12-07 16:04:49.167742: step 12850, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.605 sec/batch; 68h:24m:06s remains)
INFO - root - 2017-12-07 16:05:05.454040: step 12860, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.694 sec/batch; 72h:09m:30s remains)
INFO - root - 2017-12-07 16:05:21.694994: step 12870, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.681 sec/batch; 71h:38m:20s remains)
INFO - root - 2017-12-07 16:05:37.886862: step 12880, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.687 sec/batch; 71h:53m:22s remains)
INFO - root - 2017-12-07 16:05:54.075406: step 12890, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.578 sec/batch; 67h:12m:51s remains)
INFO - root - 2017-12-07 16:06:10.363860: step 12900, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.547 sec/batch; 65h:54m:57s remains)
2017-12-07 16:06:11.734476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2427173 -4.258038 -4.2864084 -4.3054733 -4.3081431 -4.30435 -4.3020077 -4.3021259 -4.3039908 -4.3036628 -4.2991977 -4.2902632 -4.2773881 -4.2669606 -4.2647295][-4.1924715 -4.2080789 -4.2394376 -4.2596827 -4.2588806 -4.2520428 -4.2483978 -4.2492085 -4.25216 -4.2544146 -4.2505217 -4.2379475 -4.2189107 -4.2064757 -4.2105312][-4.1437225 -4.1602178 -4.1940093 -4.2162738 -4.2129016 -4.2037978 -4.197392 -4.1932611 -4.1951151 -4.200397 -4.1975904 -4.1828532 -4.1586065 -4.1444631 -4.1537967][-4.1173124 -4.1310263 -4.1619081 -4.18519 -4.1825228 -4.1706343 -4.1591525 -4.1483707 -4.1479616 -4.1570058 -4.1560802 -4.1398315 -4.1141543 -4.1013212 -4.1144881][-4.1292233 -4.1331515 -4.15071 -4.1665363 -4.163064 -4.149313 -4.1340575 -4.1190014 -4.116354 -4.126967 -4.1272364 -4.1130767 -4.0909333 -4.0810385 -4.0949793][-4.1358871 -4.123847 -4.123611 -4.127924 -4.121798 -4.1057358 -4.0841746 -4.0581312 -4.0614157 -4.0881233 -4.10054 -4.0945807 -4.0805688 -4.0754933 -4.0896845][-4.1306286 -4.1043139 -4.0856032 -4.0721951 -4.0477686 -4.0110717 -3.9624259 -3.9058764 -3.9202042 -3.9841485 -4.023787 -4.0473695 -4.0597115 -4.0749073 -4.1010642][-4.1141715 -4.0881252 -4.0708556 -4.056325 -4.0202136 -3.9569252 -3.8660336 -3.7613871 -3.767818 -3.8552775 -3.9253092 -3.9896131 -4.0377011 -4.0803623 -4.1265597][-4.104455 -4.086555 -4.0836353 -4.0860696 -4.0695653 -4.0210056 -3.9425473 -3.8510647 -3.8366065 -3.88379 -3.9365659 -4.0058804 -4.0649509 -4.1149468 -4.1652436][-4.134737 -4.1244173 -4.130055 -4.1400323 -4.1377168 -4.1117473 -4.0672793 -4.0208015 -4.0041862 -4.0141411 -4.0403185 -4.0861049 -4.126936 -4.1665788 -4.2111683][-4.1984386 -4.1929083 -4.2019205 -4.2114596 -4.2093091 -4.193748 -4.1703343 -4.1506276 -4.139812 -4.139164 -4.1516757 -4.1742969 -4.1949992 -4.224339 -4.2626166][-4.2629876 -4.2603784 -4.2690043 -4.2764163 -4.2732496 -4.2623458 -4.2500048 -4.2414703 -4.2380371 -4.2383661 -4.2459316 -4.2559943 -4.26571 -4.2847443 -4.3121243][-4.306633 -4.3068123 -4.3132634 -4.3179188 -4.3154354 -4.308311 -4.3025088 -4.2994194 -4.2991338 -4.3012981 -4.3050408 -4.3115177 -4.3168459 -4.3275924 -4.3432212][-4.3292527 -4.3297381 -4.3341942 -4.3369985 -4.3359714 -4.33301 -4.33119 -4.3313 -4.3321962 -4.3332338 -4.334631 -4.3370795 -4.3385744 -4.3437009 -4.3518682][-4.3403296 -4.3402214 -4.3422441 -4.3438034 -4.3434324 -4.3427219 -4.3425808 -4.3432746 -4.3437657 -4.3444495 -4.3455138 -4.3467555 -4.3475819 -4.3491411 -4.3522673]]...]
INFO - root - 2017-12-07 16:06:28.012416: step 12910, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.664 sec/batch; 70h:51m:43s remains)
INFO - root - 2017-12-07 16:06:44.151153: step 12920, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 1.439 sec/batch; 61h:18m:03s remains)
INFO - root - 2017-12-07 16:07:00.624111: step 12930, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.606 sec/batch; 68h:23m:24s remains)
INFO - root - 2017-12-07 16:07:16.805417: step 12940, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.594 sec/batch; 67h:52m:19s remains)
INFO - root - 2017-12-07 16:07:33.106592: step 12950, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.623 sec/batch; 69h:07m:06s remains)
INFO - root - 2017-12-07 16:07:49.335152: step 12960, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.597 sec/batch; 67h:59m:59s remains)
INFO - root - 2017-12-07 16:08:05.734650: step 12970, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.703 sec/batch; 72h:30m:33s remains)
INFO - root - 2017-12-07 16:08:22.056032: step 12980, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 1.492 sec/batch; 63h:30m:57s remains)
INFO - root - 2017-12-07 16:08:38.348392: step 12990, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 1.512 sec/batch; 64h:22m:22s remains)
INFO - root - 2017-12-07 16:08:54.404079: step 13000, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.624 sec/batch; 69h:07m:10s remains)
2017-12-07 16:08:55.933544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3165274 -4.309135 -4.2854 -4.2569032 -4.2393465 -4.2485876 -4.2729197 -4.2935915 -4.3032913 -4.2926784 -4.2817583 -4.2917991 -4.3193707 -4.3361535 -4.336585][-4.3320308 -4.3261566 -4.3050027 -4.2752743 -4.2495518 -4.2500329 -4.27476 -4.298655 -4.3133388 -4.3116784 -4.305253 -4.3094616 -4.3275375 -4.3387661 -4.3372087][-4.3487864 -4.3464408 -4.3283772 -4.2988787 -4.2663836 -4.2564635 -4.2771573 -4.3025904 -4.3244228 -4.3343182 -4.3345909 -4.3348942 -4.3422365 -4.3466835 -4.3421974][-4.3524833 -4.3543062 -4.3385515 -4.3054633 -4.265584 -4.2474527 -4.2653384 -4.2953453 -4.3269534 -4.3479896 -4.3531089 -4.350729 -4.3519936 -4.3537831 -4.3510237][-4.3524618 -4.3558555 -4.3387609 -4.2980847 -4.2485218 -4.22326 -4.2390404 -4.2717628 -4.3091383 -4.3395519 -4.3510895 -4.3521571 -4.3532524 -4.3577209 -4.3569279][-4.3517003 -4.3546176 -4.3335094 -4.2861657 -4.2267857 -4.1927676 -4.2010689 -4.23321 -4.2721758 -4.3069911 -4.3253679 -4.3320289 -4.3382459 -4.3508348 -4.3557053][-4.3441772 -4.34573 -4.3238769 -4.2764978 -4.2125239 -4.1677871 -4.1637239 -4.189065 -4.2264924 -4.2589836 -4.279562 -4.2931738 -4.3065424 -4.33115 -4.3470864][-4.3398733 -4.3427777 -4.3232985 -4.2801185 -4.2184491 -4.1636391 -4.14396 -4.1556263 -4.1843524 -4.2061276 -4.2210226 -4.2390704 -4.2605562 -4.3001418 -4.332695][-4.34043 -4.3453083 -4.3307161 -4.2934489 -4.23526 -4.1761122 -4.1445708 -4.1446061 -4.1623716 -4.1677389 -4.1682558 -4.1825476 -4.211894 -4.2647643 -4.313828][-4.3405323 -4.3508644 -4.3415661 -4.3125849 -4.25979 -4.205534 -4.1735868 -4.1677427 -4.1754994 -4.1670394 -4.1504774 -4.1536098 -4.18287 -4.2394857 -4.2964907][-4.3375072 -4.3561373 -4.3499527 -4.3269672 -4.2861252 -4.2481933 -4.2265744 -4.2196636 -4.2159128 -4.1936588 -4.1621981 -4.1523304 -4.1756525 -4.2296104 -4.2830811][-4.3280444 -4.3497319 -4.344039 -4.3256617 -4.3004866 -4.283536 -4.2754054 -4.2715397 -4.2612834 -4.2319193 -4.1968117 -4.1802158 -4.1952209 -4.2373247 -4.2751989][-4.3056707 -4.3280535 -4.3254185 -4.3119783 -4.3009191 -4.3017526 -4.3057804 -4.305141 -4.2916217 -4.26297 -4.23734 -4.2251015 -4.2350268 -4.2610936 -4.2784705][-4.2796583 -4.3025789 -4.3037639 -4.2938504 -4.289557 -4.3019304 -4.3148394 -4.3174019 -4.3051186 -4.2809052 -4.26619 -4.2638631 -4.2769547 -4.2933283 -4.2950687][-4.2651372 -4.2818322 -4.2828355 -4.2714748 -4.26656 -4.2855382 -4.3068671 -4.3143854 -4.3034039 -4.283515 -4.2771349 -4.2855911 -4.3004045 -4.3118343 -4.3084035]]...]
INFO - root - 2017-12-07 16:09:12.217175: step 13010, loss = 2.09, batch loss = 2.04 (10.0 examples/sec; 1.594 sec/batch; 67h:51m:28s remains)
INFO - root - 2017-12-07 16:09:28.514547: step 13020, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.670 sec/batch; 71h:05m:50s remains)
INFO - root - 2017-12-07 16:09:44.937859: step 13030, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.612 sec/batch; 68h:36m:55s remains)
INFO - root - 2017-12-07 16:10:01.252932: step 13040, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.584 sec/batch; 67h:24m:47s remains)
INFO - root - 2017-12-07 16:10:17.579958: step 13050, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.645 sec/batch; 70h:00m:20s remains)
INFO - root - 2017-12-07 16:10:33.791315: step 13060, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 66h:56m:24s remains)
INFO - root - 2017-12-07 16:10:50.142104: step 13070, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.724 sec/batch; 73h:20m:06s remains)
INFO - root - 2017-12-07 16:11:06.417777: step 13080, loss = 2.06, batch loss = 2.01 (10.2 examples/sec; 1.567 sec/batch; 66h:41m:05s remains)
INFO - root - 2017-12-07 16:11:22.842044: step 13090, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.701 sec/batch; 72h:23m:01s remains)
INFO - root - 2017-12-07 16:11:38.950982: step 13100, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.615 sec/batch; 68h:43m:17s remains)
2017-12-07 16:11:40.296828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.27087 -4.2652249 -4.2512293 -4.2375517 -4.233192 -4.2344551 -4.236599 -4.2332215 -4.2257166 -4.2184315 -4.2148776 -4.2174196 -4.2211113 -4.2219205 -4.2176147][-4.2738109 -4.263762 -4.2414064 -4.218473 -4.2103763 -4.2124429 -4.2165461 -4.2134342 -4.205482 -4.1974072 -4.1911387 -4.1898618 -4.1924696 -4.1966577 -4.1964393][-4.2548594 -4.2421455 -4.2129188 -4.183279 -4.1752658 -4.1835938 -4.1924362 -4.1902151 -4.1811528 -4.172215 -4.1652589 -4.1633186 -4.171679 -4.187469 -4.1983795][-4.2281709 -4.2149739 -4.1836667 -4.1531744 -4.1494923 -4.1670384 -4.1812916 -4.1799684 -4.1691365 -4.16002 -4.1560655 -4.1594357 -4.1772852 -4.2052383 -4.2268171][-4.203208 -4.1943903 -4.1673918 -4.14062 -4.1418772 -4.1652737 -4.1834311 -4.18296 -4.1702728 -4.1625967 -4.166604 -4.1779938 -4.2002692 -4.2312355 -4.2572088][-4.185533 -4.1820707 -4.1598859 -4.1340375 -4.1331553 -4.1555204 -4.175859 -4.1768847 -4.16502 -4.1624556 -4.176538 -4.1944036 -4.2146978 -4.2417846 -4.2676048][-4.1657457 -4.1703367 -4.1541514 -4.1274824 -4.1203017 -4.1386456 -4.1600418 -4.1639466 -4.1540046 -4.1562119 -4.1753569 -4.193397 -4.2069683 -4.225297 -4.2452512][-4.1423364 -4.1542292 -4.1412554 -4.1132436 -4.10203 -4.1196313 -4.1448116 -4.1529846 -4.1449823 -4.1465096 -4.1618018 -4.1720777 -4.1744604 -4.18048 -4.1901903][-4.1466784 -4.1565313 -4.1392407 -4.1074357 -4.0938416 -4.1123424 -4.1392941 -4.1498466 -4.1428704 -4.14053 -4.14584 -4.1449165 -4.1362491 -4.12996 -4.1288362][-4.1725268 -4.1738992 -4.1484628 -4.1116557 -4.0978312 -4.1181846 -4.1476526 -4.1623969 -4.1588531 -4.1548662 -4.1527014 -4.1447463 -4.1301823 -4.1168337 -4.1109252][-4.2060728 -4.1990733 -4.1681609 -4.1299706 -4.1155081 -4.1352463 -4.1674695 -4.1886783 -4.1925879 -4.1915355 -4.1875582 -4.1772132 -4.1627011 -4.150033 -4.1466646][-4.2403293 -4.229146 -4.201108 -4.1686077 -4.1565394 -4.1747909 -4.2056279 -4.2294168 -4.2389731 -4.2402225 -4.2366519 -4.2263317 -4.2135959 -4.2042713 -4.2035666][-4.2619209 -4.2510643 -4.2317052 -4.2117553 -4.2065187 -4.2239189 -4.2510519 -4.2739754 -4.2854428 -4.2872009 -4.2837286 -4.2734561 -4.261538 -4.2541313 -4.2533669][-4.267055 -4.25811 -4.2469945 -4.2373357 -4.2357621 -4.2498159 -4.2737751 -4.2956228 -4.3071017 -4.3091574 -4.3060017 -4.2976437 -4.2889438 -4.2845063 -4.2839327][-4.2627506 -4.2562275 -4.2494378 -4.2439861 -4.2417479 -4.2521052 -4.273016 -4.2922277 -4.3011818 -4.3021388 -4.299809 -4.2956195 -4.2935109 -4.2936993 -4.2943745]]...]
INFO - root - 2017-12-07 16:11:56.557502: step 13110, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.584 sec/batch; 67h:23m:34s remains)
INFO - root - 2017-12-07 16:12:12.805331: step 13120, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.677 sec/batch; 71h:19m:52s remains)
INFO - root - 2017-12-07 16:12:28.986892: step 13130, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 67h:57m:49s remains)
INFO - root - 2017-12-07 16:12:45.334622: step 13140, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.581 sec/batch; 67h:14m:22s remains)
INFO - root - 2017-12-07 16:13:01.688482: step 13150, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 1.723 sec/batch; 73h:17m:21s remains)
INFO - root - 2017-12-07 16:13:17.912877: step 13160, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.635 sec/batch; 69h:30m:39s remains)
INFO - root - 2017-12-07 16:13:34.116260: step 13170, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.716 sec/batch; 72h:57m:48s remains)
INFO - root - 2017-12-07 16:13:50.326885: step 13180, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.605 sec/batch; 68h:15m:38s remains)
INFO - root - 2017-12-07 16:14:06.672440: step 13190, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.607 sec/batch; 68h:20m:16s remains)
INFO - root - 2017-12-07 16:14:22.915539: step 13200, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 67h:35m:22s remains)
2017-12-07 16:14:24.288786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2468462 -4.2611814 -4.2728724 -4.2770929 -4.2712889 -4.2591395 -4.24766 -4.2395792 -4.2358646 -4.2404642 -4.2498984 -4.2590561 -4.2640934 -4.2644944 -4.2588434][-4.2520142 -4.2512126 -4.2547965 -4.2570357 -4.253212 -4.2439361 -4.2357073 -4.2281361 -4.220571 -4.2214437 -4.2305636 -4.2425718 -4.2517605 -4.2543111 -4.2464628][-4.2532086 -4.2378993 -4.2329793 -4.231627 -4.2273593 -4.2182269 -4.2118735 -4.2052293 -4.1973858 -4.1989689 -4.2110643 -4.2263923 -4.2386036 -4.2426734 -4.2335739][-4.2412448 -4.2164097 -4.2053351 -4.2005868 -4.1945691 -4.1852136 -4.1803689 -4.17667 -4.1736178 -4.1804762 -4.1957488 -4.2135663 -4.2283034 -4.2331352 -4.2250533][-4.2060871 -4.1778131 -4.1642718 -4.1579933 -4.1506248 -4.139914 -4.135139 -4.1380839 -4.148253 -4.1659107 -4.1855927 -4.2051849 -4.2219129 -4.2285051 -4.2237959][-4.1553097 -4.1275096 -4.1127286 -4.1055169 -4.0956397 -4.080853 -4.0747237 -4.0884938 -4.1167917 -4.1482553 -4.1756363 -4.1977344 -4.2147593 -4.2194896 -4.2161937][-4.0996013 -4.0713949 -4.0539479 -4.0454655 -4.0333958 -4.0169616 -4.0161819 -4.0464559 -4.093709 -4.1403775 -4.1765947 -4.2001843 -4.2100582 -4.2059793 -4.1981573][-4.0593548 -4.0313234 -4.0125146 -4.0042887 -3.9964352 -3.9923866 -4.0091176 -4.054131 -4.1074271 -4.1564569 -4.1920977 -4.210197 -4.208354 -4.1919522 -4.1758938][-4.0710378 -4.0475049 -4.03604 -4.037035 -4.043108 -4.055274 -4.081306 -4.1223278 -4.1628623 -4.19818 -4.2229362 -4.23107 -4.2191176 -4.1935735 -4.1724133][-4.1289763 -4.1153216 -4.1144981 -4.12434 -4.1379871 -4.1556573 -4.179318 -4.2083015 -4.2326407 -4.2524047 -4.2666569 -4.2673106 -4.2512336 -4.2231531 -4.200953][-4.1942573 -4.1890073 -4.1951113 -4.207428 -4.2204361 -4.2350411 -4.2523971 -4.2708144 -4.2852707 -4.2959776 -4.3047514 -4.3031917 -4.2887788 -4.2648735 -4.2450848][-4.2439618 -4.2423639 -4.2492304 -4.2574768 -4.2656622 -4.2748737 -4.2867751 -4.2990737 -4.3092165 -4.3165846 -4.3221087 -4.3201904 -4.3103504 -4.2929425 -4.2760687][-4.270462 -4.2678528 -4.2710848 -4.2749243 -4.2786117 -4.2837811 -4.2928481 -4.3032012 -4.3113875 -4.3152142 -4.3167968 -4.3143682 -4.3084173 -4.2967672 -4.2829356][-4.2672939 -4.2597933 -4.2578363 -4.2579045 -4.2584023 -4.2626867 -4.2725587 -4.2837691 -4.2910542 -4.2923856 -4.291019 -4.2877965 -4.2833462 -4.2756381 -4.2642803][-4.23296 -4.2204442 -4.215745 -4.21506 -4.216116 -4.2219543 -4.2335982 -4.2459197 -4.2534566 -4.2548614 -4.2533631 -4.2505341 -4.2470908 -4.2428021 -4.2350092]]...]
INFO - root - 2017-12-07 16:14:40.503758: step 13210, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.587 sec/batch; 67h:27m:36s remains)
INFO - root - 2017-12-07 16:14:56.553572: step 13220, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.595 sec/batch; 67h:48m:23s remains)
INFO - root - 2017-12-07 16:15:12.892613: step 13230, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.704 sec/batch; 72h:25m:32s remains)
INFO - root - 2017-12-07 16:15:29.017545: step 13240, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.619 sec/batch; 68h:48m:10s remains)
INFO - root - 2017-12-07 16:15:45.306549: step 13250, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.629 sec/batch; 69h:15m:06s remains)
INFO - root - 2017-12-07 16:16:01.480414: step 13260, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.574 sec/batch; 66h:53m:14s remains)
INFO - root - 2017-12-07 16:16:17.728649: step 13270, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.567 sec/batch; 66h:35m:08s remains)
INFO - root - 2017-12-07 16:16:33.701808: step 13280, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.600 sec/batch; 67h:59m:27s remains)
INFO - root - 2017-12-07 16:16:49.723253: step 13290, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 1.555 sec/batch; 66h:04m:20s remains)
INFO - root - 2017-12-07 16:17:06.022388: step 13300, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.687 sec/batch; 71h:40m:30s remains)
2017-12-07 16:17:07.366473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.19749 -4.2032123 -4.2084641 -4.2096567 -4.2078724 -4.2067461 -4.2066488 -4.2075648 -4.207057 -4.2033825 -4.2002988 -4.200397 -4.2017369 -4.2017865 -4.1996651][-4.2025414 -4.2069163 -4.2150702 -4.2189951 -4.2186055 -4.2181005 -4.2169714 -4.2160153 -4.2136574 -4.2092433 -4.2056823 -4.2051821 -4.2056441 -4.2043829 -4.2005649][-4.2151508 -4.2178903 -4.2287025 -4.2346034 -4.2357388 -4.2362351 -4.2367043 -4.2362909 -4.234077 -4.230792 -4.2268176 -4.2250061 -4.223968 -4.2213683 -4.2175589][-4.2287297 -4.2286448 -4.2380075 -4.2423396 -4.2420344 -4.2401958 -4.2409782 -4.244586 -4.2476587 -4.2494855 -4.2480011 -4.247427 -4.2488394 -4.2482982 -4.245873][-4.2267022 -4.2233067 -4.2261057 -4.2224622 -4.2126813 -4.1988287 -4.19244 -4.199604 -4.2127938 -4.2241521 -4.2305512 -4.2391281 -4.2503028 -4.2567673 -4.2578874][-4.2082262 -4.2010026 -4.193696 -4.1776295 -4.151011 -4.116 -4.0976195 -4.1109729 -4.1367726 -4.1562576 -4.1705456 -4.1928277 -4.2194676 -4.2375135 -4.2453771][-4.1855431 -4.1761341 -4.1600718 -4.129818 -4.0779257 -4.0144577 -3.9847493 -4.0136843 -4.0575061 -4.0834856 -4.1039476 -4.1385345 -4.1774912 -4.2066703 -4.2229791][-4.17366 -4.1705856 -4.1511574 -4.1085873 -4.0324292 -3.9474363 -3.91565 -3.9618757 -4.0165663 -4.0413613 -4.0609975 -4.0982819 -4.1426525 -4.1794262 -4.20471][-4.1533227 -4.1574669 -4.1481142 -4.1172767 -4.0565076 -3.9907126 -3.9698966 -4.0101285 -4.049614 -4.0627632 -4.0745125 -4.107305 -4.1490746 -4.1833797 -4.2065678][-4.1253562 -4.1372843 -4.1426988 -4.1345387 -4.1076016 -4.074688 -4.063736 -4.0886717 -4.1147451 -4.1257238 -4.1333313 -4.157084 -4.1886482 -4.2131076 -4.2272792][-4.1269522 -4.13438 -4.1447964 -4.1497793 -4.1478872 -4.138813 -4.1359267 -4.151279 -4.1702619 -4.1799245 -4.1873488 -4.2038708 -4.2231331 -4.2382421 -4.2458653][-4.142664 -4.1456 -4.1584644 -4.1706252 -4.1831932 -4.1883492 -4.1888709 -4.1978846 -4.2095556 -4.2180748 -4.2279592 -4.238369 -4.2466464 -4.25359 -4.2562785][-4.158113 -4.1598396 -4.1706071 -4.1843743 -4.2029867 -4.21709 -4.2232833 -4.2312098 -4.2391224 -4.2462311 -4.2530107 -4.2574239 -4.2598271 -4.2619848 -4.262145][-4.1658044 -4.1708536 -4.1837597 -4.1994677 -4.2188349 -4.2332959 -4.2393546 -4.2447805 -4.2516961 -4.2572727 -4.2591763 -4.2594395 -4.2597928 -4.2611647 -4.2608905][-4.1694202 -4.1820745 -4.2019925 -4.221879 -4.2375736 -4.2461815 -4.2484455 -4.2506576 -4.2551465 -4.2583256 -4.2566137 -4.2550225 -4.2556834 -4.25694 -4.2565374]]...]
INFO - root - 2017-12-07 16:17:23.419015: step 13310, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 1.553 sec/batch; 65h:57m:19s remains)
INFO - root - 2017-12-07 16:17:39.671702: step 13320, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.572 sec/batch; 66h:47m:12s remains)
INFO - root - 2017-12-07 16:17:55.962933: step 13330, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.679 sec/batch; 71h:20m:21s remains)
INFO - root - 2017-12-07 16:18:11.931394: step 13340, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 1.608 sec/batch; 68h:19m:06s remains)
INFO - root - 2017-12-07 16:18:28.128657: step 13350, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.662 sec/batch; 70h:36m:15s remains)
INFO - root - 2017-12-07 16:18:44.275928: step 13360, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.555 sec/batch; 66h:01m:12s remains)
INFO - root - 2017-12-07 16:19:00.635174: step 13370, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.744 sec/batch; 74h:03m:51s remains)
INFO - root - 2017-12-07 16:19:16.874242: step 13380, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.633 sec/batch; 69h:21m:32s remains)
INFO - root - 2017-12-07 16:19:33.159356: step 13390, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.623 sec/batch; 68h:55m:52s remains)
INFO - root - 2017-12-07 16:19:49.091430: step 13400, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.591 sec/batch; 67h:33m:35s remains)
2017-12-07 16:19:50.520324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3295169 -4.2995925 -4.2453232 -4.1683593 -4.0996423 -4.0718184 -4.0947804 -4.1457653 -4.2010221 -4.2572231 -4.2966061 -4.3169265 -4.321012 -4.3159122 -4.3078184][-4.3281488 -4.2954135 -4.2394624 -4.1619029 -4.0867529 -4.0467944 -4.0592532 -4.1097083 -4.1721067 -4.2379436 -4.2874045 -4.3162546 -4.3270864 -4.3264551 -4.3190317][-4.3313861 -4.2999682 -4.2439919 -4.1649289 -4.0801258 -4.02051 -4.0151758 -4.0614643 -4.1359577 -4.2184386 -4.2818742 -4.3204818 -4.3363976 -4.33858 -4.3316402][-4.3380585 -4.3105073 -4.2558136 -4.1764383 -4.0833459 -4.0019836 -3.971972 -4.0083661 -4.0944052 -4.1943169 -4.2737145 -4.3247356 -4.3461967 -4.3498139 -4.3436532][-4.3435555 -4.3204823 -4.2694364 -4.1939459 -4.0995307 -4.0054855 -3.9535599 -3.9738724 -4.0626426 -4.1707873 -4.2617154 -4.3248515 -4.3525944 -4.3575683 -4.3526034][-4.345274 -4.3271785 -4.2831511 -4.2156634 -4.1257615 -4.0300193 -3.96691 -3.9684987 -4.0468631 -4.1520519 -4.2477207 -4.3186922 -4.3534865 -4.3606033 -4.3570209][-4.3438597 -4.330267 -4.2946053 -4.2375402 -4.157362 -4.0689063 -4.0030584 -3.9880066 -4.0464139 -4.1403608 -4.2351747 -4.3086271 -4.3489509 -4.359149 -4.3573375][-4.3420033 -4.3322287 -4.3036795 -4.2575469 -4.1907206 -4.1135979 -4.0497322 -4.0255351 -4.064055 -4.1413465 -4.2283764 -4.2987409 -4.3410687 -4.3544846 -4.3551717][-4.3405108 -4.3330121 -4.310585 -4.2751908 -4.2229133 -4.1613326 -4.1063075 -4.0794611 -4.0998039 -4.155901 -4.2282343 -4.2906842 -4.3320165 -4.3478436 -4.3514194][-4.3402915 -4.3338528 -4.3170643 -4.290957 -4.253541 -4.2092533 -4.1671553 -4.1420374 -4.1468468 -4.1809082 -4.2339392 -4.2846274 -4.3224783 -4.3399754 -4.3463945][-4.3432107 -4.3374677 -4.3246231 -4.3057351 -4.2806888 -4.2514038 -4.2208271 -4.1968126 -4.1905665 -4.2075238 -4.2433639 -4.2823234 -4.3153691 -4.3334432 -4.3420849][-4.3470764 -4.3424058 -4.3320909 -4.3188744 -4.3037467 -4.2860155 -4.2649069 -4.2438974 -4.2318687 -4.2374544 -4.2590218 -4.2864528 -4.3129559 -4.3298411 -4.3392982][-4.3501892 -4.3472142 -4.3392143 -4.330739 -4.3227277 -4.3136206 -4.3016438 -4.2874837 -4.2769365 -4.2762928 -4.2851419 -4.3001924 -4.3174405 -4.3301597 -4.33809][-4.352675 -4.3521385 -4.3474112 -4.3428063 -4.3396087 -4.3366451 -4.3322339 -4.3255663 -4.3190866 -4.3157063 -4.3160672 -4.320776 -4.3281012 -4.3342481 -4.3384619][-4.3550677 -4.3563476 -4.3548074 -4.3532515 -4.3525548 -4.3528395 -4.3527746 -4.3511572 -4.3484116 -4.3453212 -4.3425574 -4.3413754 -4.3413334 -4.341011 -4.3407664]]...]
INFO - root - 2017-12-07 16:20:06.830554: step 13410, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.632 sec/batch; 69h:18m:30s remains)
INFO - root - 2017-12-07 16:20:23.054278: step 13420, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.654 sec/batch; 70h:13m:39s remains)
INFO - root - 2017-12-07 16:20:39.165664: step 13430, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.628 sec/batch; 69h:05m:54s remains)
INFO - root - 2017-12-07 16:20:55.480106: step 13440, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 67h:54m:17s remains)
INFO - root - 2017-12-07 16:21:11.683782: step 13450, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.722 sec/batch; 73h:06m:24s remains)
INFO - root - 2017-12-07 16:21:27.701180: step 13460, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.615 sec/batch; 68h:31m:43s remains)
INFO - root - 2017-12-07 16:21:43.972806: step 13470, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.632 sec/batch; 69h:15m:46s remains)
INFO - root - 2017-12-07 16:22:00.074984: step 13480, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.603 sec/batch; 68h:02m:30s remains)
INFO - root - 2017-12-07 16:22:16.497868: step 13490, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.697 sec/batch; 72h:00m:05s remains)
INFO - root - 2017-12-07 16:22:32.527255: step 13500, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.701 sec/batch; 72h:11m:40s remains)
2017-12-07 16:22:33.895725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3132367 -4.2983475 -4.28093 -4.2601752 -4.2405429 -4.2356896 -4.2496986 -4.2675815 -4.2729659 -4.2805452 -4.2948132 -4.3038659 -4.304122 -4.3060312 -4.3141341][-4.3054523 -4.2825985 -4.2561412 -4.2261457 -4.1958637 -4.1875744 -4.2102866 -4.236186 -4.244133 -4.2527208 -4.2712317 -4.2875347 -4.287416 -4.2898273 -4.3012042][-4.30298 -4.2776675 -4.2479062 -4.208323 -4.1648521 -4.1469259 -4.1651568 -4.18442 -4.1915679 -4.2075229 -4.2329969 -4.2556009 -4.2557993 -4.2624025 -4.2797441][-4.3003736 -4.2753582 -4.2454524 -4.1932874 -4.13063 -4.092061 -4.089499 -4.0926366 -4.1072993 -4.1421528 -4.18413 -4.2152758 -4.2210112 -4.2337847 -4.255765][-4.2937212 -4.2684932 -4.2353215 -4.1683636 -4.0830245 -4.0222197 -3.9869063 -3.9559259 -3.98416 -4.0552559 -4.1255012 -4.1765642 -4.1945562 -4.2085233 -4.2305422][-4.2749443 -4.2487826 -4.2073946 -4.1290593 -4.0250082 -3.9404089 -3.8627872 -3.781743 -3.8166583 -3.9391627 -4.0453053 -4.1243005 -4.1594534 -4.1791353 -4.2047472][-4.2524414 -4.2225137 -4.1723137 -4.0897274 -3.9822798 -3.8839624 -3.7825079 -3.666642 -3.7012048 -3.8562922 -3.9868567 -4.0845633 -4.1348453 -4.1645074 -4.1963654][-4.2454886 -4.2178912 -4.173018 -4.1101637 -4.0364347 -3.9564223 -3.8749495 -3.7907121 -3.8161407 -3.9242148 -4.0214963 -4.1045547 -4.1517491 -4.1787095 -4.2119942][-4.2542911 -4.2363691 -4.208818 -4.1681824 -4.1220121 -4.0618453 -4.006155 -3.9554794 -3.9614589 -4.0123339 -4.0721407 -4.1397095 -4.1837978 -4.2101445 -4.2413216][-4.2702823 -4.2552509 -4.2389112 -4.2157412 -4.18508 -4.1448464 -4.1088457 -4.0725136 -4.0620661 -4.0778837 -4.1111155 -4.1648388 -4.2080641 -4.2343836 -4.2624292][-4.2916942 -4.278583 -4.2660675 -4.2486196 -4.2267613 -4.1985073 -4.1655459 -4.1215768 -4.0948682 -4.0969982 -4.1211262 -4.1695795 -4.2154675 -4.246521 -4.2757287][-4.30531 -4.2937031 -4.280302 -4.26324 -4.2485566 -4.2291622 -4.2029605 -4.1685271 -4.1479864 -4.15037 -4.1695051 -4.205595 -4.2450242 -4.272603 -4.2936459][-4.310976 -4.3020806 -4.2895794 -4.2777066 -4.2737217 -4.2682738 -4.2557445 -4.2408242 -4.2343488 -4.2365756 -4.2462034 -4.2636948 -4.2854276 -4.3025913 -4.3132348][-4.3131542 -4.3059106 -4.2968988 -4.2910113 -4.2920609 -4.29408 -4.2934251 -4.2913141 -4.291718 -4.2934036 -4.298192 -4.3048072 -4.3142843 -4.3212628 -4.3233752][-4.3144155 -4.3099365 -4.3050194 -4.3032808 -4.30693 -4.3112235 -4.3145995 -4.316155 -4.316967 -4.3180432 -4.3202581 -4.3228426 -4.3267651 -4.3286228 -4.3278742]]...]
INFO - root - 2017-12-07 16:22:50.291048: step 13510, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 1.751 sec/batch; 74h:18m:23s remains)
INFO - root - 2017-12-07 16:23:06.269019: step 13520, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.617 sec/batch; 68h:36m:01s remains)
INFO - root - 2017-12-07 16:23:22.713146: step 13530, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.674 sec/batch; 71h:02m:07s remains)
INFO - root - 2017-12-07 16:23:39.064241: step 13540, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.610 sec/batch; 68h:16m:45s remains)
INFO - root - 2017-12-07 16:23:55.286205: step 13550, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.661 sec/batch; 70h:27m:50s remains)
INFO - root - 2017-12-07 16:24:11.382546: step 13560, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.574 sec/batch; 66h:45m:30s remains)
INFO - root - 2017-12-07 16:24:27.724712: step 13570, loss = 2.08, batch loss = 2.03 (10.6 examples/sec; 1.506 sec/batch; 63h:53m:27s remains)
INFO - root - 2017-12-07 16:24:43.888083: step 13580, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 1.661 sec/batch; 70h:25m:09s remains)
INFO - root - 2017-12-07 16:25:00.019427: step 13590, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.578 sec/batch; 66h:55m:26s remains)
INFO - root - 2017-12-07 16:25:16.301867: step 13600, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.666 sec/batch; 70h:37m:42s remains)
2017-12-07 16:25:17.589474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3448153 -4.3438945 -4.3419461 -4.3410296 -4.34154 -4.3424158 -4.3427539 -4.3421531 -4.341558 -4.3417597 -4.3425245 -4.34375 -4.3452206 -4.346416 -4.3469415][-4.3519325 -4.3513603 -4.3485918 -4.347014 -4.3472333 -4.3484297 -4.3493838 -4.3491311 -4.3480024 -4.3472109 -4.3471742 -4.3476572 -4.3486 -4.3493733 -4.3494406][-4.3553796 -4.3532162 -4.3479552 -4.3433456 -4.3404989 -4.3402352 -4.3418269 -4.3433242 -4.3430429 -4.3411465 -4.3393016 -4.3384862 -4.3400226 -4.3428063 -4.3451877][-4.3481431 -4.3397541 -4.327569 -4.3159513 -4.3072109 -4.30333 -4.3038568 -4.3050656 -4.3041821 -4.3009295 -4.2972593 -4.2967372 -4.30363 -4.3145823 -4.3256378][-4.3299069 -4.31515 -4.2970881 -4.2784762 -4.2611728 -4.2492723 -4.2431469 -4.2382407 -4.2318335 -4.2243352 -4.2179842 -4.2209268 -4.2393494 -4.2656651 -4.292417][-4.3024673 -4.2800107 -4.2553658 -4.2268243 -4.1950355 -4.1667891 -4.1459885 -4.1280575 -4.1142864 -4.1068177 -4.1036496 -4.1182508 -4.1567864 -4.2055306 -4.252974][-4.2688785 -4.2392578 -4.2080674 -4.1678047 -4.1169109 -4.064786 -4.0236664 -3.9913623 -3.9758618 -3.9791269 -3.9895887 -4.0249367 -4.0877995 -4.1590667 -4.2242827][-4.2348709 -4.1989727 -4.1614485 -4.11244 -4.0475063 -3.9779558 -3.9230447 -3.8867989 -3.8890405 -3.9245455 -3.9653039 -4.0229335 -4.0951781 -4.1665893 -4.2287226][-4.2190804 -4.1839166 -4.148212 -4.1041322 -4.0477042 -3.9938419 -3.9606748 -3.9491007 -3.974535 -4.0247045 -4.0724173 -4.1234732 -4.176384 -4.2240071 -4.2640996][-4.2456961 -4.2265849 -4.208415 -4.184505 -4.1543465 -4.1295986 -4.1186724 -4.1194472 -4.142139 -4.1770153 -4.20717 -4.2342134 -4.2594242 -4.2807837 -4.2996788][-4.2957687 -4.2935171 -4.2918568 -4.2856822 -4.2736382 -4.2634015 -4.2583437 -4.2587152 -4.2691889 -4.2833409 -4.294034 -4.303123 -4.3124561 -4.3203926 -4.3272347][-4.3346629 -4.3400126 -4.3445644 -4.3451734 -4.3413405 -4.3368306 -4.3326321 -4.3301578 -4.3323088 -4.3364587 -4.3395667 -4.3418655 -4.3438067 -4.3448992 -4.3458576][-4.3553505 -4.3613625 -4.367434 -4.3709145 -4.3709579 -4.369278 -4.3661957 -4.3631358 -4.3615494 -4.3611994 -4.3604131 -4.3589063 -4.3571811 -4.3556914 -4.3546758][-4.3601093 -4.3640375 -4.3679223 -4.37076 -4.3713803 -4.3705592 -4.3678613 -4.3646584 -4.3619971 -4.360014 -4.3581352 -4.3557553 -4.3539462 -4.3530364 -4.3526444][-4.3526211 -4.3548965 -4.3562961 -4.357276 -4.3570795 -4.3560147 -4.3540254 -4.3514647 -4.3490567 -4.3473806 -4.3461337 -4.3446674 -4.3434839 -4.3431249 -4.3430839]]...]
INFO - root - 2017-12-07 16:25:34.051949: step 13610, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.698 sec/batch; 72h:00m:23s remains)
INFO - root - 2017-12-07 16:25:50.101954: step 13620, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.631 sec/batch; 69h:09m:32s remains)
INFO - root - 2017-12-07 16:26:06.325152: step 13630, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.659 sec/batch; 70h:20m:33s remains)
INFO - root - 2017-12-07 16:26:22.368481: step 13640, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.592 sec/batch; 67h:30m:19s remains)
INFO - root - 2017-12-07 16:26:38.618360: step 13650, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 1.660 sec/batch; 70h:21m:30s remains)
INFO - root - 2017-12-07 16:26:54.854966: step 13660, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.603 sec/batch; 67h:56m:32s remains)
INFO - root - 2017-12-07 16:27:11.189885: step 13670, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.685 sec/batch; 71h:24m:46s remains)
INFO - root - 2017-12-07 16:27:27.614741: step 13680, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 1.633 sec/batch; 69h:11m:40s remains)
INFO - root - 2017-12-07 16:27:43.652454: step 13690, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.524 sec/batch; 64h:34m:58s remains)
INFO - root - 2017-12-07 16:27:59.720169: step 13700, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.708 sec/batch; 72h:23m:40s remains)
2017-12-07 16:28:01.187052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2604933 -4.2560134 -4.252914 -4.2487612 -4.2442493 -4.2391672 -4.2344365 -4.2298856 -4.2280183 -4.2287154 -4.230679 -4.2355709 -4.2416954 -4.2476563 -4.2515192][-4.2751451 -4.2685456 -4.265348 -4.2638874 -4.2648821 -4.2629061 -4.2590065 -4.2544994 -4.2511749 -4.2501445 -4.2513947 -4.2544641 -4.2606826 -4.2685847 -4.2726388][-4.2796683 -4.2695055 -4.2657661 -4.2686553 -4.2778778 -4.2820625 -4.2807093 -4.2764425 -4.2719874 -4.26988 -4.27056 -4.2713213 -4.27441 -4.2790232 -4.2789283][-4.2601342 -4.2493539 -4.2465277 -4.2542686 -4.2719193 -4.282773 -4.2841568 -4.2799931 -4.2742066 -4.2724175 -4.2750721 -4.2751379 -4.2759686 -4.2792339 -4.2764478][-4.2191091 -4.2078276 -4.2061591 -4.216928 -4.2400632 -4.2555695 -4.2589498 -4.2545743 -4.2495584 -4.2506828 -4.2573686 -4.2605557 -4.2618833 -4.2651238 -4.2620854][-4.1756234 -4.1611986 -4.1548228 -4.16232 -4.1878357 -4.206543 -4.2117677 -4.2090735 -4.2061863 -4.2082548 -4.2187552 -4.2284288 -4.2333317 -4.2378592 -4.2351565][-4.1444139 -4.1234331 -4.1046038 -4.1019535 -4.1261663 -4.1459 -4.1520629 -4.1524906 -4.1548638 -4.1581044 -4.1707993 -4.187788 -4.1976714 -4.2047744 -4.2046337][-4.1257415 -4.0955405 -4.0604095 -4.0441256 -4.0650244 -4.0888958 -4.0994773 -4.1069098 -4.1167517 -4.1220875 -4.1343222 -4.1565981 -4.1711535 -4.1802373 -4.1831026][-4.1283927 -4.0909247 -4.0454845 -4.020874 -4.040771 -4.0725074 -4.0920467 -4.1059022 -4.1180568 -4.1208677 -4.127593 -4.1472793 -4.1616945 -4.1720262 -4.176918][-4.1612105 -4.1248274 -4.0841117 -4.0635509 -4.0797882 -4.1104383 -4.1310143 -4.1410952 -4.1460991 -4.1433825 -4.1422091 -4.1523967 -4.1644006 -4.1762466 -4.1823821][-4.2095017 -4.1805062 -4.1544991 -4.1419086 -4.1486397 -4.1658869 -4.1758051 -4.1740732 -4.1700096 -4.1644111 -4.1606922 -4.1660585 -4.1781545 -4.1929193 -4.201303][-4.2448416 -4.22203 -4.209415 -4.2015557 -4.199266 -4.2025905 -4.2025046 -4.1928964 -4.184659 -4.1777992 -4.1729918 -4.1767082 -4.1910439 -4.2104926 -4.2231569][-4.2506409 -4.2311034 -4.2268972 -4.2238488 -4.2182846 -4.218111 -4.21332 -4.1988649 -4.1877913 -4.1794295 -4.1736927 -4.17915 -4.1967397 -4.223042 -4.2422085][-4.2417464 -4.2249241 -4.2267432 -4.2275271 -4.2216234 -4.2223883 -4.2173018 -4.2023578 -4.1891346 -4.1783028 -4.1702147 -4.1764054 -4.1955891 -4.2255592 -4.2489233][-4.2277708 -4.2125473 -4.2197208 -4.2263017 -4.2237897 -4.2271152 -4.2238593 -4.2115784 -4.1983404 -4.1852684 -4.1740355 -4.1763077 -4.1934581 -4.2215953 -4.245553]]...]
INFO - root - 2017-12-07 16:28:17.411677: step 13710, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.578 sec/batch; 66h:50m:46s remains)
INFO - root - 2017-12-07 16:28:33.782719: step 13720, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.563 sec/batch; 66h:12m:31s remains)
INFO - root - 2017-12-07 16:28:50.075217: step 13730, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.695 sec/batch; 71h:47m:25s remains)
INFO - root - 2017-12-07 16:29:06.280373: step 13740, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.603 sec/batch; 67h:53m:46s remains)
INFO - root - 2017-12-07 16:29:22.476524: step 13750, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.685 sec/batch; 71h:22m:49s remains)
INFO - root - 2017-12-07 16:29:38.378672: step 13760, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.569 sec/batch; 66h:27m:27s remains)
INFO - root - 2017-12-07 16:29:54.617560: step 13770, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.686 sec/batch; 71h:23m:46s remains)
INFO - root - 2017-12-07 16:30:10.782544: step 13780, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.695 sec/batch; 71h:47m:06s remains)
INFO - root - 2017-12-07 16:30:27.122606: step 13790, loss = 2.09, batch loss = 2.04 (10.0 examples/sec; 1.604 sec/batch; 67h:55m:55s remains)
INFO - root - 2017-12-07 16:30:43.500077: step 13800, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.684 sec/batch; 71h:18m:21s remains)
2017-12-07 16:30:44.862367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1916995 -4.204248 -4.2264066 -4.2468748 -4.2482734 -4.2384496 -4.2139583 -4.1916208 -4.1804233 -4.1857891 -4.2095475 -4.2300777 -4.2332768 -4.2365003 -4.2442617][-4.2074471 -4.2233486 -4.2452641 -4.2603827 -4.2585125 -4.2487526 -4.2260919 -4.2054873 -4.1963654 -4.1974983 -4.2094917 -4.223177 -4.2281179 -4.2345147 -4.240509][-4.211967 -4.2322583 -4.2545862 -4.2644992 -4.2554116 -4.2434068 -4.2225294 -4.2095447 -4.2081714 -4.2092147 -4.213244 -4.219698 -4.2254572 -4.2324843 -4.2371593][-4.20199 -4.2245674 -4.2475262 -4.2522049 -4.2350411 -4.21687 -4.1929088 -4.1901135 -4.2004461 -4.2049408 -4.2088704 -4.2149262 -4.2220769 -4.2312775 -4.2390962][-4.189281 -4.2117615 -4.22903 -4.2240882 -4.1972103 -4.1661549 -4.139658 -4.153636 -4.1845894 -4.2006621 -4.2082076 -4.2168417 -4.2233763 -4.2331419 -4.2458034][-4.17489 -4.1942344 -4.1986523 -4.179009 -4.1376886 -4.0874977 -4.0588837 -4.097827 -4.1589713 -4.1958475 -4.2126274 -4.2245603 -4.2349658 -4.24718 -4.262908][-4.146584 -4.1623144 -4.15434 -4.1216421 -4.0642195 -3.996269 -3.9735589 -4.0428195 -4.1297398 -4.1855035 -4.2122536 -4.2283382 -4.2478962 -4.2663708 -4.2854614][-4.1019821 -4.110939 -4.0937953 -4.0505018 -3.9822242 -3.9085798 -3.9009442 -3.9924119 -4.0932484 -4.161252 -4.1962562 -4.2168179 -4.2422132 -4.2681947 -4.2916312][-4.0957823 -4.1039906 -4.0849729 -4.0401087 -3.9774029 -3.9183505 -3.9220207 -4.0027528 -4.0910358 -4.1516995 -4.1872835 -4.2086248 -4.2336917 -4.2631607 -4.2890449][-4.1386971 -4.1499572 -4.13271 -4.0942507 -4.0495205 -4.01258 -4.0178447 -4.0744133 -4.1417122 -4.1867895 -4.2115006 -4.2274137 -4.24485 -4.2697358 -4.29406][-4.1962695 -4.205956 -4.1882715 -4.1573133 -4.1274657 -4.1074324 -4.1144781 -4.1548629 -4.2022133 -4.2354493 -4.2523036 -4.2636623 -4.2746243 -4.2902365 -4.3064375][-4.2304358 -4.2366529 -4.2226582 -4.2051096 -4.1890268 -4.1819339 -4.1938038 -4.2249966 -4.2563462 -4.2777615 -4.2890139 -4.2964244 -4.3012247 -4.309279 -4.3171892][-4.2272658 -4.2339873 -4.2309217 -4.2284837 -4.2245259 -4.2257671 -4.2397385 -4.2654114 -4.2860909 -4.2991724 -4.3066349 -4.3106856 -4.3110638 -4.3170848 -4.3211279][-4.209609 -4.2169247 -4.2204952 -4.2256527 -4.2277365 -4.2297883 -4.2425094 -4.2643704 -4.281065 -4.2928691 -4.299839 -4.3031697 -4.3042803 -4.3116865 -4.3159447][-4.1956072 -4.2014027 -4.2088804 -4.2167177 -4.2187428 -4.2171693 -4.223495 -4.238616 -4.2518072 -4.2644815 -4.2743039 -4.2804565 -4.28319 -4.2921662 -4.2993908]]...]
INFO - root - 2017-12-07 16:31:01.007156: step 13810, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.675 sec/batch; 70h:56m:35s remains)
INFO - root - 2017-12-07 16:31:17.151828: step 13820, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.576 sec/batch; 66h:44m:38s remains)
INFO - root - 2017-12-07 16:31:33.423911: step 13830, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.704 sec/batch; 72h:08m:14s remains)
INFO - root - 2017-12-07 16:31:49.624566: step 13840, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.585 sec/batch; 67h:05m:14s remains)
INFO - root - 2017-12-07 16:32:06.020691: step 13850, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.582 sec/batch; 66h:57m:42s remains)
INFO - root - 2017-12-07 16:32:22.219885: step 13860, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.671 sec/batch; 70h:43m:42s remains)
INFO - root - 2017-12-07 16:32:38.534707: step 13870, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.619 sec/batch; 68h:31m:00s remains)
INFO - root - 2017-12-07 16:32:54.812738: step 13880, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.673 sec/batch; 70h:49m:30s remains)
INFO - root - 2017-12-07 16:33:10.954152: step 13890, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.557 sec/batch; 65h:53m:27s remains)
INFO - root - 2017-12-07 16:33:27.268491: step 13900, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.738 sec/batch; 73h:32m:44s remains)
2017-12-07 16:33:28.591848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2900724 -4.3027921 -4.3081889 -4.3098326 -4.3108482 -4.3126278 -4.3150105 -4.3192396 -4.32406 -4.3259125 -4.323287 -4.3180647 -4.3133159 -4.3090167 -4.3052855][-4.2547984 -4.2661328 -4.2670765 -4.2628613 -4.2583737 -4.2592177 -4.2650476 -4.2751083 -4.2900715 -4.3005753 -4.3031917 -4.3013644 -4.29832 -4.2938833 -4.2889118][-4.2153869 -4.2163081 -4.2043915 -4.1892052 -4.1750665 -4.1728382 -4.1828122 -4.2016168 -4.2316132 -4.2565017 -4.2689476 -4.2738948 -4.2731662 -4.2675033 -4.2587657][-4.189723 -4.1756434 -4.147367 -4.1186028 -4.0917387 -4.07912 -4.0838213 -4.1051559 -4.147718 -4.187458 -4.209475 -4.2215414 -4.2264347 -4.224391 -4.2188115][-4.1933174 -4.1669087 -4.1252432 -4.0832772 -4.0401134 -4.0075068 -3.9932837 -4.0050178 -4.0507026 -4.1004343 -4.1301789 -4.1503944 -4.1674018 -4.1792073 -4.1888151][-4.2134485 -4.18083 -4.1327238 -4.0819197 -4.0237093 -3.9685376 -3.9285202 -3.9211714 -3.9586265 -4.0115614 -4.0486012 -4.0770311 -4.1094365 -4.1397848 -4.1693][-4.2413597 -4.2068419 -4.1568022 -4.1003547 -4.033083 -3.9671025 -3.9133973 -3.8917036 -3.9118993 -3.9553554 -3.9917758 -4.02214 -4.0614343 -4.1043487 -4.1487079][-4.2598133 -4.2221947 -4.1745391 -4.1212711 -4.0618258 -4.0095077 -3.9731627 -3.9634118 -3.9730814 -3.9893575 -4.0038395 -4.0212379 -4.0472054 -4.0791483 -4.120924][-4.2496009 -4.2115259 -4.1708474 -4.1312742 -4.0946536 -4.0675106 -4.0556555 -4.0604744 -4.0654535 -4.0610795 -4.0550609 -4.0600739 -4.0698638 -4.0822439 -4.1073437][-4.2222509 -4.1884913 -4.1571064 -4.1328173 -4.1166582 -4.1143842 -4.1237431 -4.1341934 -4.1355982 -4.1221547 -4.1076679 -4.1042528 -4.1049891 -4.1064706 -4.1147461][-4.1990552 -4.1703434 -4.1471272 -4.1325016 -4.1286035 -4.1418757 -4.1620617 -4.1748314 -4.1773224 -4.1685667 -4.1614 -4.1613574 -4.1569576 -4.1475563 -4.1360016][-4.1813488 -4.1634455 -4.1463318 -4.1378446 -4.1389923 -4.157413 -4.1820397 -4.1975741 -4.2066035 -4.21066 -4.2172174 -4.2216182 -4.2139831 -4.197598 -4.1746855][-4.1766591 -4.1714134 -4.1618819 -4.1579828 -4.1611028 -4.1785727 -4.2034712 -4.2209916 -4.2361183 -4.248704 -4.2648425 -4.27115 -4.2595005 -4.238739 -4.2147985][-4.1868482 -4.1889191 -4.1809363 -4.176064 -4.1806111 -4.1973829 -4.221458 -4.2409849 -4.26104 -4.2767072 -4.2925739 -4.29993 -4.29001 -4.269084 -4.2496448][-4.199172 -4.2024589 -4.1946278 -4.1915188 -4.2015529 -4.2202125 -4.2417817 -4.2591276 -4.277853 -4.2913857 -4.302115 -4.3069191 -4.3012986 -4.2864065 -4.2750664]]...]
INFO - root - 2017-12-07 16:33:44.964697: step 13910, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.664 sec/batch; 70h:25m:39s remains)
INFO - root - 2017-12-07 16:34:01.053254: step 13920, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.605 sec/batch; 67h:53m:47s remains)
INFO - root - 2017-12-07 16:34:17.277977: step 13930, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 1.632 sec/batch; 69h:02m:28s remains)
INFO - root - 2017-12-07 16:34:33.576725: step 13940, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 68h:58m:51s remains)
INFO - root - 2017-12-07 16:34:49.829551: step 13950, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 68h:57m:28s remains)
INFO - root - 2017-12-07 16:35:06.000261: step 13960, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.600 sec/batch; 67h:41m:26s remains)
INFO - root - 2017-12-07 16:35:22.183770: step 13970, loss = 2.08, batch loss = 2.03 (10.4 examples/sec; 1.538 sec/batch; 65h:03m:00s remains)
INFO - root - 2017-12-07 16:35:38.186207: step 13980, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.660 sec/batch; 70h:13m:20s remains)
INFO - root - 2017-12-07 16:35:54.361034: step 13990, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.558 sec/batch; 65h:53m:07s remains)
INFO - root - 2017-12-07 16:36:10.676366: step 14000, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.734 sec/batch; 73h:20m:27s remains)
2017-12-07 16:36:12.044510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3040552 -4.2996178 -4.3018894 -4.3058715 -4.3083963 -4.3098559 -4.3113942 -4.31465 -4.3166995 -4.3205104 -4.3262691 -4.3315363 -4.3356476 -4.3379464 -4.3412471][-4.2632308 -4.2489185 -4.2500763 -4.2537713 -4.2539129 -4.2518411 -4.2515988 -4.2569404 -4.2635584 -4.2768507 -4.2946553 -4.3126669 -4.3249717 -4.33085 -4.3354912][-4.2182717 -4.1936536 -4.1936693 -4.1989403 -4.1981335 -4.1904221 -4.1864786 -4.1898708 -4.1980829 -4.2194085 -4.2500262 -4.2812185 -4.3051295 -4.3180127 -4.3264065][-4.1920738 -4.1594892 -4.155355 -4.1553707 -4.14668 -4.1286769 -4.1138682 -4.1098666 -4.114295 -4.143888 -4.1877317 -4.2322006 -4.2693543 -4.2928705 -4.3106694][-4.1914091 -4.1554275 -4.142664 -4.1296082 -4.1020079 -4.0605574 -4.0214729 -3.9980321 -3.9947259 -4.0443192 -4.1127911 -4.1739988 -4.2244272 -4.2603178 -4.291502][-4.2002845 -4.1694455 -4.1547689 -4.1342134 -4.0895448 -4.0187497 -3.9381051 -3.8707232 -3.8473225 -3.9259462 -4.0291905 -4.1130543 -4.1785097 -4.2275181 -4.2710752][-4.1893682 -4.16273 -4.1495986 -4.130435 -4.0818524 -3.9978037 -3.8823612 -3.7601743 -3.7048297 -3.8107281 -3.9457421 -4.0536761 -4.1386681 -4.2001171 -4.2528057][-4.1577806 -4.1286907 -4.115387 -4.1019135 -4.0657721 -3.9939654 -3.8856041 -3.7547972 -3.683656 -3.7736006 -3.9073162 -4.0190687 -4.1124282 -4.1815834 -4.2398429][-4.1454377 -4.117609 -4.1091251 -4.1060162 -4.0890379 -4.0426459 -3.9652939 -3.8647184 -3.802635 -3.8531129 -3.9486341 -4.037466 -4.1194859 -4.1881208 -4.24401][-4.1599622 -4.136291 -4.1338458 -4.1394997 -4.138813 -4.1146259 -4.064949 -3.9973588 -3.9513576 -3.9744334 -4.0341411 -4.094264 -4.157321 -4.2173052 -4.2644868][-4.2073045 -4.1900978 -4.1947665 -4.2038088 -4.2115974 -4.2032542 -4.1735787 -4.1308589 -4.0970306 -4.1062517 -4.139266 -4.1733975 -4.2140961 -4.2589674 -4.2938204][-4.2685914 -4.2565017 -4.2641053 -4.2730684 -4.2837639 -4.2827811 -4.2662592 -4.240561 -4.2169895 -4.2211876 -4.2378283 -4.2534451 -4.275166 -4.3040738 -4.3256431][-4.31349 -4.3046994 -4.308198 -4.3144827 -4.3244257 -4.3264761 -4.3180027 -4.3042407 -4.2900257 -4.2915854 -4.2983875 -4.3031015 -4.3137264 -4.3310781 -4.344449][-4.3312697 -4.3251681 -4.3250213 -4.3288989 -4.3348532 -4.3369346 -4.3343024 -4.3303914 -4.3239121 -4.3243957 -4.3267326 -4.3267837 -4.3309875 -4.3411202 -4.3508778][-4.3445287 -4.3412409 -4.339901 -4.3407564 -4.342557 -4.3431363 -4.3428044 -4.3426456 -4.3402538 -4.3402152 -4.3409958 -4.3399234 -4.3410172 -4.3459287 -4.3519769]]...]
INFO - root - 2017-12-07 16:36:28.292140: step 14010, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.676 sec/batch; 70h:52m:39s remains)
INFO - root - 2017-12-07 16:36:44.616381: step 14020, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.615 sec/batch; 68h:17m:02s remains)
INFO - root - 2017-12-07 16:37:00.935707: step 14030, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.621 sec/batch; 68h:33m:02s remains)
INFO - root - 2017-12-07 16:37:17.007045: step 14040, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 67h:13m:26s remains)
INFO - root - 2017-12-07 16:37:33.160546: step 14050, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.616 sec/batch; 68h:20m:04s remains)
INFO - root - 2017-12-07 16:37:49.280917: step 14060, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.714 sec/batch; 72h:27m:23s remains)
INFO - root - 2017-12-07 16:38:05.601630: step 14070, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.614 sec/batch; 68h:13m:53s remains)
INFO - root - 2017-12-07 16:38:21.717133: step 14080, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.612 sec/batch; 68h:07m:17s remains)
INFO - root - 2017-12-07 16:38:37.845035: step 14090, loss = 2.06, batch loss = 2.01 (10.4 examples/sec; 1.534 sec/batch; 64h:50m:38s remains)
INFO - root - 2017-12-07 16:38:54.124204: step 14100, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.577 sec/batch; 66h:39m:31s remains)
2017-12-07 16:38:55.454293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.181747 -4.1752138 -4.1783671 -4.1863503 -4.2069206 -4.2313046 -4.2474971 -4.256743 -4.2647762 -4.2678008 -4.2637515 -4.2571607 -4.2569032 -4.2640376 -4.2728519][-4.1944509 -4.173285 -4.1595964 -4.1549683 -4.1708441 -4.1971736 -4.2198215 -4.236012 -4.247798 -4.2489667 -4.2397289 -4.22554 -4.2165737 -4.2163291 -4.2233906][-4.2259 -4.1958013 -4.1676235 -4.1493521 -4.1557817 -4.176496 -4.2006197 -4.2227345 -4.2385755 -4.2425504 -4.2359862 -4.2236886 -4.2101531 -4.2002125 -4.1966276][-4.2464881 -4.214942 -4.1816888 -4.1551642 -4.1510024 -4.162334 -4.1825886 -4.2081418 -4.2255073 -4.2354074 -4.2404237 -4.2410946 -4.23202 -4.216197 -4.1992984][-4.2465234 -4.2243662 -4.1966152 -4.1693764 -4.1572871 -4.1576595 -4.1671176 -4.186522 -4.2032681 -4.2186222 -4.2358255 -4.2506146 -4.2493224 -4.229887 -4.2033916][-4.2343564 -4.2260323 -4.2087488 -4.1854196 -4.1685166 -4.1558237 -4.147089 -4.1520443 -4.1648083 -4.1848803 -4.2109752 -4.2370663 -4.2436609 -4.2224674 -4.1902504][-4.2134542 -4.2105532 -4.1979561 -4.1755424 -4.151022 -4.1221037 -4.0936847 -4.0804954 -4.0880132 -4.1147122 -4.1542287 -4.1925993 -4.2121539 -4.1987762 -4.168942][-4.1996865 -4.1923542 -4.1757841 -4.1483369 -4.1113653 -4.0628467 -4.0148606 -3.9839115 -3.9847939 -4.0181141 -4.071559 -4.1234808 -4.158071 -4.1622381 -4.1476693][-4.2080784 -4.1931014 -4.1676311 -4.1307034 -4.0800166 -4.0141292 -3.9490986 -3.9014208 -3.8941846 -3.9294589 -3.9883664 -4.0459485 -4.0921903 -4.11605 -4.123961][-4.2332664 -4.2158055 -4.1884017 -4.146184 -4.0897036 -4.0182304 -3.9479558 -3.8964112 -3.8830128 -3.9095316 -3.9570611 -4.0067625 -4.0529757 -4.0874376 -4.1105347][-4.2741947 -4.2610126 -4.2397704 -4.2033973 -4.1536875 -4.0940652 -4.0356951 -3.9931893 -3.9798753 -3.9930646 -4.0191021 -4.0481687 -4.0807772 -4.1096439 -4.1321597][-4.3118243 -4.3060122 -4.2953076 -4.272738 -4.2390351 -4.2004161 -4.1648359 -4.1394973 -4.1308646 -4.1352105 -4.14238 -4.1500063 -4.162981 -4.1778116 -4.1902933][-4.3357754 -4.3383608 -4.33753 -4.3276181 -4.3092637 -4.2883668 -4.2697067 -4.2566981 -4.2512255 -4.2504282 -4.2483292 -4.2457075 -4.248055 -4.2542729 -4.2588105][-4.3383107 -4.3491669 -4.3563766 -4.3553996 -4.3476319 -4.3384824 -4.3321252 -4.327457 -4.3239117 -4.31994 -4.3134413 -4.3069243 -4.30545 -4.3088088 -4.3111534][-4.316925 -4.3354197 -4.3493881 -4.3559084 -4.355474 -4.3523378 -4.3523679 -4.353344 -4.3524542 -4.3480368 -4.3414736 -4.335176 -4.3328404 -4.3351731 -4.3375664]]...]
INFO - root - 2017-12-07 16:39:11.732918: step 14110, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.648 sec/batch; 69h:39m:36s remains)
INFO - root - 2017-12-07 16:39:28.015364: step 14120, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.652 sec/batch; 69h:49m:21s remains)
INFO - root - 2017-12-07 16:39:44.272879: step 14130, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.549 sec/batch; 65h:27m:21s remains)
INFO - root - 2017-12-07 16:40:00.578904: step 14140, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.654 sec/batch; 69h:53m:04s remains)
INFO - root - 2017-12-07 16:40:16.648829: step 14150, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.604 sec/batch; 67h:45m:41s remains)
INFO - root - 2017-12-07 16:40:32.850683: step 14160, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 1.621 sec/batch; 68h:28m:07s remains)
INFO - root - 2017-12-07 16:40:48.868254: step 14170, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.578 sec/batch; 66h:40m:26s remains)
INFO - root - 2017-12-07 16:41:05.036415: step 14180, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.669 sec/batch; 70h:30m:21s remains)
INFO - root - 2017-12-07 16:41:21.105866: step 14190, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 67h:10m:45s remains)
INFO - root - 2017-12-07 16:41:37.478427: step 14200, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.652 sec/batch; 69h:47m:36s remains)
2017-12-07 16:41:38.842341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1744308 -4.1809382 -4.20674 -4.2370262 -4.2569985 -4.2638731 -4.2618232 -4.2622404 -4.2713838 -4.2837319 -4.287159 -4.2834582 -4.2813072 -4.2839861 -4.288497][-4.151732 -4.1668239 -4.1949916 -4.22213 -4.2366595 -4.2395382 -4.2380061 -4.242744 -4.2563677 -4.2730436 -4.2811127 -4.28212 -4.2828908 -4.2876072 -4.2903695][-4.1475868 -4.1618423 -4.1801219 -4.1931458 -4.1974297 -4.1979733 -4.2013783 -4.2131948 -4.2319155 -4.2527966 -4.2669153 -4.2750564 -4.2799039 -4.2845154 -4.2811794][-4.1642356 -4.1680012 -4.165925 -4.1549377 -4.1425743 -4.1394796 -4.1475334 -4.1653628 -4.1885257 -4.2147593 -4.23881 -4.2581677 -4.2693043 -4.2704782 -4.2545033][-4.1895485 -4.1775327 -4.151154 -4.1120663 -4.0779448 -4.0692072 -4.08116 -4.104084 -4.1322584 -4.1645007 -4.2008309 -4.232976 -4.2504148 -4.2446628 -4.2126646][-4.2053561 -4.1770353 -4.1290126 -4.0658865 -4.013104 -3.9983718 -4.0134892 -4.041935 -4.0740829 -4.1100655 -4.1577215 -4.20374 -4.2288356 -4.2168579 -4.1699443][-4.2073154 -4.16963 -4.1108055 -4.0366907 -3.9728835 -3.9511955 -3.9635546 -3.9917161 -4.0240054 -4.0631609 -4.1228223 -4.1837854 -4.2175708 -4.2022247 -4.1432438][-4.2051468 -4.1694674 -4.1154242 -4.0469084 -3.9835124 -3.9548011 -3.9594963 -3.9822674 -4.0130725 -4.056035 -4.1224742 -4.1909304 -4.2280884 -4.2113042 -4.1462278][-4.2091656 -4.1850114 -4.147016 -4.09532 -4.0425014 -4.0123653 -4.0093613 -4.0252814 -4.054 -4.0981741 -4.1617503 -4.2232385 -4.2547131 -4.2373037 -4.1772075][-4.2227955 -4.2133155 -4.1936893 -4.1617112 -4.1247349 -4.101202 -4.0966873 -4.1089835 -4.134531 -4.172617 -4.2208128 -4.2635694 -4.2831764 -4.2673459 -4.222888][-4.2400131 -4.2410488 -4.2350783 -4.2183309 -4.1948729 -4.1775951 -4.1748629 -4.187511 -4.2112117 -4.2412157 -4.2715926 -4.2949562 -4.30251 -4.2894187 -4.2631121][-4.2575564 -4.262753 -4.263308 -4.2550316 -4.2398272 -4.2268076 -4.2256017 -4.2384806 -4.2593589 -4.2814622 -4.3003254 -4.3107405 -4.3107514 -4.3013411 -4.289772][-4.2780223 -4.2822657 -4.2836003 -4.279304 -4.2702208 -4.2616949 -4.2621241 -4.2731671 -4.2882524 -4.302537 -4.3131633 -4.3163075 -4.3136516 -4.308898 -4.3068533][-4.29718 -4.2994943 -4.2992873 -4.2959828 -4.2905855 -4.285439 -4.286417 -4.2947173 -4.3041315 -4.3118029 -4.3164477 -4.3167777 -4.314961 -4.3144908 -4.3169594][-4.3123369 -4.3129525 -4.3111734 -4.3077836 -4.30416 -4.3012924 -4.3020191 -4.3069038 -4.3125749 -4.31719 -4.31969 -4.3198018 -4.3192654 -4.3203464 -4.3229442]]...]
INFO - root - 2017-12-07 16:41:55.183999: step 14210, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.666 sec/batch; 70h:20m:24s remains)
INFO - root - 2017-12-07 16:42:11.369318: step 14220, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.584 sec/batch; 66h:53m:00s remains)
INFO - root - 2017-12-07 16:42:27.614950: step 14230, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.671 sec/batch; 70h:34m:17s remains)
INFO - root - 2017-12-07 16:42:44.019200: step 14240, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.562 sec/batch; 65h:56m:50s remains)
INFO - root - 2017-12-07 16:43:00.340919: step 14250, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.616 sec/batch; 68h:14m:09s remains)
INFO - root - 2017-12-07 16:43:16.511980: step 14260, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 1.628 sec/batch; 68h:42m:45s remains)
INFO - root - 2017-12-07 16:43:32.435736: step 14270, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.609 sec/batch; 67h:54m:50s remains)
INFO - root - 2017-12-07 16:43:48.732116: step 14280, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.693 sec/batch; 71h:27m:30s remains)
INFO - root - 2017-12-07 16:44:04.723801: step 14290, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.613 sec/batch; 68h:05m:21s remains)
INFO - root - 2017-12-07 16:44:20.840670: step 14300, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.644 sec/batch; 69h:23m:17s remains)
2017-12-07 16:44:22.212606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.30028 -4.3025007 -4.303236 -4.3029737 -4.3010874 -4.2977848 -4.293797 -4.2911477 -4.2910562 -4.2946939 -4.2994318 -4.3013086 -4.3010006 -4.300478 -4.29909][-4.298697 -4.3000178 -4.2992315 -4.2958269 -4.2892237 -4.2792335 -4.2676039 -4.2618184 -4.2636271 -4.2741489 -4.2889252 -4.2985787 -4.3000774 -4.2974834 -4.2930865][-4.2933664 -4.29091 -4.2854071 -4.2740726 -4.2617788 -4.2433124 -4.22134 -4.2107153 -4.2170782 -4.2375007 -4.264461 -4.2857213 -4.295073 -4.2956004 -4.292172][-4.2764025 -4.2687221 -4.254703 -4.2323136 -4.2095289 -4.1797996 -4.1423798 -4.1249156 -4.13959 -4.1718473 -4.21121 -4.2459416 -4.2670903 -4.278789 -4.2829146][-4.2608094 -4.2479692 -4.222208 -4.1842151 -4.14607 -4.0978365 -4.0340047 -4.0045891 -4.0332317 -4.0849719 -4.1380186 -4.1867294 -4.2224736 -4.248395 -4.2627745][-4.2509112 -4.2322989 -4.1978703 -4.151751 -4.1001139 -4.0282283 -3.9336786 -3.884217 -3.9206026 -3.9901183 -4.0576754 -4.121881 -4.17715 -4.2195272 -4.2428293][-4.2427635 -4.2201304 -4.1871786 -4.1439052 -4.0869961 -4.0065665 -3.9087186 -3.855005 -3.8794839 -3.9415359 -4.0044131 -4.0691009 -4.137176 -4.1979489 -4.2311344][-4.2417593 -4.2214289 -4.198606 -4.1640711 -4.1064663 -4.0372891 -3.9742219 -3.9383512 -3.9436786 -3.9750128 -4.0097523 -4.057076 -4.1254797 -4.1919966 -4.2312956][-4.2531481 -4.2380657 -4.2247977 -4.1987638 -4.149869 -4.1007142 -4.074091 -4.0557466 -4.0463972 -4.0549073 -4.06601 -4.0930629 -4.1467133 -4.2039809 -4.2405281][-4.2628112 -4.2522058 -4.24705 -4.2315955 -4.1959357 -4.1662359 -4.1597691 -4.1509881 -4.1344614 -4.1354175 -4.1362157 -4.147203 -4.1793618 -4.2197943 -4.2478833][-4.2603335 -4.25432 -4.2537932 -4.2466397 -4.2239294 -4.2111373 -4.2175159 -4.2141151 -4.1987486 -4.196147 -4.196 -4.1999655 -4.215426 -4.2362175 -4.2540445][-4.254817 -4.2520342 -4.2544174 -4.2543597 -4.242671 -4.2400236 -4.251575 -4.2534342 -4.2375507 -4.2301416 -4.2315178 -4.2353768 -4.2415109 -4.2531624 -4.266396][-4.2593508 -4.2571282 -4.2589016 -4.260344 -4.2569952 -4.2615533 -4.2741723 -4.2755795 -4.2614322 -4.2536559 -4.2529864 -4.2545857 -4.2573476 -4.2642903 -4.2756834][-4.2742867 -4.2714367 -4.2727675 -4.273222 -4.2719278 -4.276217 -4.2855773 -4.28756 -4.2768297 -4.2700815 -4.2687082 -4.2682447 -4.2692413 -4.2733731 -4.2824688][-4.2894421 -4.2850809 -4.2850151 -4.2846017 -4.2823496 -4.2836461 -4.2882013 -4.2887545 -4.2821007 -4.2778082 -4.2784204 -4.2788081 -4.2793832 -4.2818084 -4.2881503]]...]
INFO - root - 2017-12-07 16:44:38.594905: step 14310, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 1.699 sec/batch; 71h:42m:31s remains)
INFO - root - 2017-12-07 16:44:54.709208: step 14320, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.625 sec/batch; 68h:33m:30s remains)
INFO - root - 2017-12-07 16:45:10.782325: step 14330, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.643 sec/batch; 69h:20m:55s remains)
INFO - root - 2017-12-07 16:45:26.917920: step 14340, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 1.692 sec/batch; 71h:23m:02s remains)
INFO - root - 2017-12-07 16:45:42.732758: step 14350, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.594 sec/batch; 67h:15m:48s remains)
INFO - root - 2017-12-07 16:45:59.061324: step 14360, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.723 sec/batch; 72h:41m:36s remains)
INFO - root - 2017-12-07 16:46:15.354766: step 14370, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.602 sec/batch; 67h:35m:14s remains)
INFO - root - 2017-12-07 16:46:31.375415: step 14380, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.647 sec/batch; 69h:29m:38s remains)
INFO - root - 2017-12-07 16:46:47.470294: step 14390, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.532 sec/batch; 64h:36m:35s remains)
INFO - root - 2017-12-07 16:47:03.755869: step 14400, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.632 sec/batch; 68h:51m:30s remains)
2017-12-07 16:47:05.064365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3257279 -4.3198857 -4.3148589 -4.308867 -4.3018188 -4.2924161 -4.2799158 -4.2660084 -4.2587228 -4.2617846 -4.2712622 -4.2846255 -4.2968235 -4.3069096 -4.3137951][-4.3106728 -4.2988925 -4.2916784 -4.2840366 -4.274601 -4.26241 -4.2451053 -4.2198997 -4.2046447 -4.2130132 -4.2307143 -4.2509627 -4.26883 -4.2825832 -4.2923217][-4.2858181 -4.2684164 -4.258677 -4.24624 -4.2328324 -4.2166734 -4.190064 -4.14851 -4.1253791 -4.1398869 -4.1660819 -4.1930289 -4.2186351 -4.2384 -4.2567835][-4.2618504 -4.2394514 -4.2256823 -4.2074976 -4.1877904 -4.1629276 -4.1220365 -4.0624342 -4.0394182 -4.0674233 -4.1006279 -4.132874 -4.1643605 -4.1892886 -4.2183867][-4.2512517 -4.227736 -4.20777 -4.1814289 -4.1560531 -4.1194797 -4.0556412 -3.9775753 -3.9601908 -4.0009956 -4.035779 -4.0732932 -4.1089773 -4.1400137 -4.1805363][-4.2480969 -4.227035 -4.2006874 -4.1652822 -4.1306973 -4.0786905 -3.99233 -3.9032118 -3.8978229 -3.9518044 -3.9903738 -4.0305815 -4.069098 -4.1062303 -4.1525874][-4.2467341 -4.2272124 -4.1959786 -4.1522279 -4.1107435 -4.0463815 -3.9416871 -3.8448858 -3.8452823 -3.9041436 -3.9479935 -3.9957542 -4.0414081 -4.0843396 -4.1345673][-4.248723 -4.231843 -4.19996 -4.1563611 -4.1154861 -4.0509076 -3.9453888 -3.8413222 -3.8322036 -3.8835549 -3.9311965 -3.9849734 -4.0358958 -4.0824118 -4.1339612][-4.2503772 -4.2341928 -4.2061272 -4.1706328 -4.1388855 -4.0867453 -3.9969165 -3.9019384 -3.8843613 -3.9176185 -3.9560502 -4.0055165 -4.0548716 -4.1010485 -4.149127][-4.2527952 -4.2333651 -4.2066956 -4.1793075 -4.1590962 -4.1186609 -4.0454893 -3.971648 -3.9552431 -3.9746361 -4.0012693 -4.0452285 -4.0879054 -4.12574 -4.1676769][-4.2656312 -4.2437043 -4.2191868 -4.1991544 -4.1844339 -4.1548333 -4.100637 -4.0460138 -4.0324392 -4.0397167 -4.0529032 -4.084856 -4.1142392 -4.1425071 -4.1804667][-4.2841177 -4.264729 -4.2450485 -4.2289124 -4.2144237 -4.1900482 -4.1495471 -4.1083455 -4.0948615 -4.0966005 -4.1030941 -4.1263123 -4.1476846 -4.1707268 -4.2027149][-4.2992229 -4.2848525 -4.2709332 -4.2582831 -4.24487 -4.2249746 -4.1959 -4.1679206 -4.1567721 -4.1576262 -4.1655374 -4.185863 -4.2012334 -4.2180529 -4.2408323][-4.3139057 -4.3048396 -4.2953005 -4.2864404 -4.2769346 -4.2641582 -4.2477789 -4.2334681 -4.2285953 -4.2310162 -4.2396827 -4.2527523 -4.2609568 -4.2689729 -4.2801867][-4.3247657 -4.32096 -4.316421 -4.3122139 -4.30642 -4.2996411 -4.292367 -4.2871542 -4.2864027 -4.288424 -4.2928839 -4.2971992 -4.298151 -4.3012094 -4.3073983]]...]
INFO - root - 2017-12-07 16:47:21.261629: step 14410, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.654 sec/batch; 69h:45m:09s remains)
INFO - root - 2017-12-07 16:47:37.549992: step 14420, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.602 sec/batch; 67h:32m:45s remains)
INFO - root - 2017-12-07 16:47:53.830715: step 14430, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.538 sec/batch; 64h:52m:40s remains)
INFO - root - 2017-12-07 16:48:09.806606: step 14440, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.602 sec/batch; 67h:33m:57s remains)
INFO - root - 2017-12-07 16:48:25.905263: step 14450, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.565 sec/batch; 66h:00m:06s remains)
INFO - root - 2017-12-07 16:48:41.957467: step 14460, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.679 sec/batch; 70h:46m:54s remains)
INFO - root - 2017-12-07 16:48:58.067011: step 14470, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.612 sec/batch; 67h:56m:41s remains)
INFO - root - 2017-12-07 16:49:14.293786: step 14480, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.681 sec/batch; 70h:50m:59s remains)
INFO - root - 2017-12-07 16:49:30.493968: step 14490, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.559 sec/batch; 65h:42m:50s remains)
INFO - root - 2017-12-07 16:49:46.708998: step 14500, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.660 sec/batch; 69h:59m:09s remains)
2017-12-07 16:49:48.176762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3604174 -4.3478613 -4.3307238 -4.3156753 -4.312099 -4.3191009 -4.332324 -4.3448076 -4.3521824 -4.3529058 -4.3498254 -4.3478837 -4.3477449 -4.3483014 -4.348331][-4.3743439 -4.3568845 -4.3323793 -4.3074164 -4.2962189 -4.3000426 -4.3176832 -4.3382335 -4.3534517 -4.3594079 -4.3571606 -4.3536344 -4.3516431 -4.351397 -4.3510127][-4.3676281 -4.3417888 -4.3077512 -4.2735758 -4.2545123 -4.2541008 -4.2760792 -4.3070359 -4.3348241 -4.3530154 -4.3589692 -4.3574872 -4.3536978 -4.35164 -4.3511229][-4.3446441 -4.3073597 -4.2608547 -4.2192292 -4.1958241 -4.191699 -4.2189474 -4.2594595 -4.2978311 -4.3274155 -4.3449516 -4.35252 -4.3532867 -4.3517747 -4.3508029][-4.3236427 -4.2770071 -4.214478 -4.157042 -4.1212983 -4.108933 -4.136529 -4.1856012 -4.2365241 -4.2788305 -4.3092232 -4.3305182 -4.3435125 -4.3491282 -4.3501391][-4.3236055 -4.2750649 -4.2032819 -4.1287403 -4.0670347 -4.0238385 -4.028801 -4.0775075 -4.1451254 -4.2049503 -4.2504911 -4.2883987 -4.3185225 -4.3364549 -4.344636][-4.3310833 -4.2865019 -4.2145886 -4.1332145 -4.0465946 -3.9593711 -3.9075758 -3.931591 -4.0151539 -4.1001697 -4.1661673 -4.2253604 -4.2765093 -4.3107734 -4.3295937][-4.3355985 -4.2969494 -4.2307634 -4.1513472 -4.0558853 -3.9347196 -3.8207181 -3.7889328 -3.8689489 -3.970145 -4.0549312 -4.1358876 -4.2123017 -4.2701492 -4.3052936][-4.346272 -4.3145094 -4.2585349 -4.1855917 -4.0987821 -3.9851432 -3.858937 -3.7809176 -3.8082342 -3.8825879 -3.96081 -4.0465751 -4.1395125 -4.2199731 -4.2753429][-4.3578973 -4.3364291 -4.2940793 -4.2345967 -4.1694679 -4.0913773 -4.0007043 -3.9268956 -3.9046693 -3.9184244 -3.9513924 -4.0100384 -4.0972695 -4.1864548 -4.2528567][-4.3631148 -4.3526206 -4.3272023 -4.2846904 -4.240624 -4.1961761 -4.1427851 -4.089736 -4.0521965 -4.03222 -4.0293584 -4.0525389 -4.1120796 -4.1871276 -4.2485108][-4.3599663 -4.3600039 -4.3510766 -4.3290877 -4.3033452 -4.28057 -4.2524619 -4.2180209 -4.1827483 -4.1540651 -4.1340551 -4.1361775 -4.1688747 -4.2191238 -4.2637062][-4.3466721 -4.3535376 -4.3552651 -4.3507195 -4.3430133 -4.3379173 -4.327445 -4.3061066 -4.2785082 -4.250391 -4.2250047 -4.2161565 -4.2307019 -4.2577515 -4.2844238][-4.3309937 -4.3411717 -4.3495412 -4.3559685 -4.3600173 -4.3656292 -4.3672123 -4.3571353 -4.33732 -4.3135653 -4.2886381 -4.2732897 -4.2769284 -4.2894926 -4.30338][-4.3243608 -4.3315959 -4.3403316 -4.3494868 -4.3589025 -4.3686843 -4.3764939 -4.3766751 -4.3672276 -4.3501654 -4.3274779 -4.30844 -4.3034086 -4.3076859 -4.3155956]]...]
INFO - root - 2017-12-07 16:50:04.408038: step 14510, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.669 sec/batch; 70h:21m:51s remains)
INFO - root - 2017-12-07 16:50:20.721901: step 14520, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.608 sec/batch; 67h:45m:29s remains)
INFO - root - 2017-12-07 16:50:36.815551: step 14530, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.649 sec/batch; 69h:29m:38s remains)
INFO - root - 2017-12-07 16:50:53.127804: step 14540, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.533 sec/batch; 64h:35m:14s remains)
INFO - root - 2017-12-07 16:51:09.392074: step 14550, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.556 sec/batch; 65h:34m:41s remains)
INFO - root - 2017-12-07 16:51:25.548413: step 14560, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.675 sec/batch; 70h:34m:25s remains)
INFO - root - 2017-12-07 16:51:41.658600: step 14570, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.611 sec/batch; 67h:51m:53s remains)
INFO - root - 2017-12-07 16:51:57.972198: step 14580, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.702 sec/batch; 71h:42m:22s remains)
INFO - root - 2017-12-07 16:52:13.915299: step 14590, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.584 sec/batch; 66h:43m:46s remains)
INFO - root - 2017-12-07 16:52:30.207745: step 14600, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.732 sec/batch; 72h:58m:32s remains)
2017-12-07 16:52:31.482000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3275414 -4.3153315 -4.3054109 -4.2994924 -4.3029575 -4.3109994 -4.3168058 -4.3188529 -4.3181748 -4.3177733 -4.3124604 -4.3015971 -4.2925076 -4.2962751 -4.3104429][-4.3326416 -4.3197393 -4.3070993 -4.2959251 -4.295929 -4.3012009 -4.3032875 -4.2993112 -4.2922144 -4.2873778 -4.2766204 -4.25836 -4.2428713 -4.2497139 -4.27349][-4.3110638 -4.2958794 -4.2818065 -4.2653222 -4.2622886 -4.26358 -4.26117 -4.2513437 -4.2405534 -4.2374358 -4.2281871 -4.2089262 -4.1909046 -4.1988249 -4.2275195][-4.282742 -4.260428 -4.2409811 -4.2190228 -4.2051468 -4.1954517 -4.1853228 -4.170599 -4.1624475 -4.1724157 -4.1738396 -4.15734 -4.1395826 -4.1514864 -4.1847763][-4.2587843 -4.2262669 -4.2002311 -4.1708555 -4.1430144 -4.1185231 -4.0946984 -4.0690923 -4.0660934 -4.0985246 -4.119832 -4.1115026 -4.0985179 -4.116806 -4.1539636][-4.2232194 -4.1826115 -4.1500559 -4.1157908 -4.0713115 -4.0232224 -3.9763582 -3.9325118 -3.9368784 -4.0034447 -4.0576296 -4.0755291 -4.0813971 -4.1066122 -4.1438274][-4.2151408 -4.1738534 -4.1371727 -4.0998549 -4.0430112 -3.970295 -3.8921072 -3.8174057 -3.8177547 -3.9094732 -3.9949789 -4.0440106 -4.076365 -4.1129618 -4.1551933][-4.2425818 -4.2117462 -4.1854682 -4.1604252 -4.1100273 -4.0367966 -3.9594488 -3.8805943 -3.8614001 -3.93509 -4.0196285 -4.0755463 -4.1144304 -4.15154 -4.191833][-4.2665906 -4.2500587 -4.2342114 -4.2188497 -4.1825442 -4.1270084 -4.0742979 -4.0181565 -3.9902112 -4.0340986 -4.1033821 -4.1537828 -4.1876707 -4.2157707 -4.2458253][-4.2863894 -4.2798414 -4.2712755 -4.2623363 -4.2396178 -4.2009645 -4.1664052 -4.1311517 -4.10357 -4.1256323 -4.1761651 -4.2176018 -4.2469177 -4.2694197 -4.2886233][-4.2980337 -4.294899 -4.2906747 -4.2866707 -4.2794514 -4.2574391 -4.238256 -4.2181377 -4.1966968 -4.2057829 -4.2360287 -4.2621784 -4.28447 -4.3015342 -4.311451][-4.3109283 -4.310401 -4.306952 -4.3009024 -4.297462 -4.2883477 -4.2797103 -4.2711887 -4.2591219 -4.2615776 -4.274785 -4.286828 -4.3013926 -4.3146267 -4.3200521][-4.3106284 -4.3119059 -4.3079743 -4.3012619 -4.2984519 -4.2975378 -4.2945175 -4.2921205 -4.2865162 -4.2889228 -4.2964416 -4.3026304 -4.3136969 -4.3234167 -4.3267817][-4.3167744 -4.316596 -4.31373 -4.3076262 -4.3028488 -4.3044763 -4.3061094 -4.3071251 -4.3056421 -4.3095093 -4.315764 -4.3210468 -4.3303328 -4.3370113 -4.3388867][-4.337851 -4.3378911 -4.3377371 -4.3334904 -4.3276649 -4.3263359 -4.3275633 -4.3290124 -4.3302541 -4.3330731 -4.3354316 -4.3386416 -4.3445926 -4.3483534 -4.3504658]]...]
INFO - root - 2017-12-07 16:52:47.722554: step 14610, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 1.740 sec/batch; 73h:16m:24s remains)
INFO - root - 2017-12-07 16:53:03.659735: step 14620, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.647 sec/batch; 69h:23m:15s remains)
INFO - root - 2017-12-07 16:53:19.994706: step 14630, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.596 sec/batch; 67h:14m:10s remains)
INFO - root - 2017-12-07 16:53:36.318941: step 14640, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.696 sec/batch; 71h:26m:20s remains)
INFO - root - 2017-12-07 16:53:52.409156: step 14650, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.560 sec/batch; 65h:41m:14s remains)
INFO - root - 2017-12-07 16:54:08.613712: step 14660, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 1.752 sec/batch; 73h:45m:37s remains)
INFO - root - 2017-12-07 16:54:24.685106: step 14670, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 1.482 sec/batch; 62h:25m:03s remains)
INFO - root - 2017-12-07 16:54:40.745803: step 14680, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 1.656 sec/batch; 69h:42m:52s remains)
INFO - root - 2017-12-07 16:54:56.844559: step 14690, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 1.512 sec/batch; 63h:40m:10s remains)
INFO - root - 2017-12-07 16:55:13.026073: step 14700, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.638 sec/batch; 68h:56m:08s remains)
2017-12-07 16:55:14.378056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2079368 -4.2383327 -4.2598114 -4.272018 -4.2780509 -4.2762041 -4.2541089 -4.2263651 -4.2008419 -4.1908336 -4.1988029 -4.2142577 -4.2370353 -4.2563267 -4.2638597][-4.1906834 -4.2176318 -4.2394133 -4.2569451 -4.2706294 -4.2730188 -4.2529564 -4.2269206 -4.2014141 -4.1869736 -4.186851 -4.20613 -4.2357454 -4.2558084 -4.2688107][-4.1882906 -4.2125854 -4.2333021 -4.2524939 -4.2720227 -4.28135 -4.2670617 -4.2383251 -4.208849 -4.188818 -4.1815333 -4.201088 -4.2296505 -4.2533369 -4.2745624][-4.2000055 -4.2257142 -4.2467871 -4.2620878 -4.2724056 -4.2765341 -4.256598 -4.2202845 -4.1903958 -4.1750236 -4.1718831 -4.1947079 -4.2217808 -4.2486434 -4.27366][-4.2114415 -4.24015 -4.260304 -4.2712 -4.2671342 -4.2569861 -4.2219853 -4.1721945 -4.1411014 -4.13723 -4.1486616 -4.1777925 -4.2080841 -4.2318273 -4.2519197][-4.2197847 -4.2519431 -4.2675905 -4.2682571 -4.2488022 -4.2190442 -4.1618757 -4.0949097 -4.0657706 -4.0770569 -4.1074266 -4.1482286 -4.1878123 -4.2108035 -4.2286391][-4.2249794 -4.26297 -4.2693191 -4.2549915 -4.2204738 -4.164825 -4.0795417 -3.9927874 -3.9704204 -4.0118508 -4.0714684 -4.1294537 -4.1733556 -4.1906109 -4.2066655][-4.2181325 -4.2579045 -4.2522354 -4.2227955 -4.1723967 -4.0971479 -3.9949088 -3.9024463 -3.8966055 -3.9715657 -4.05757 -4.1218934 -4.16015 -4.1680245 -4.1719403][-4.2111211 -4.2472544 -4.2362366 -4.1997256 -4.1410213 -4.0601196 -3.9675148 -3.8928046 -3.8982539 -3.9766133 -4.0638561 -4.1208754 -4.1471553 -4.1425056 -4.133388][-4.2063975 -4.2378321 -4.2294497 -4.1981435 -4.1442051 -4.073575 -3.9993629 -3.9462383 -3.9565308 -4.0202527 -4.0915165 -4.1350164 -4.14551 -4.1249743 -4.1047807][-4.2226114 -4.2509093 -4.2481995 -4.223093 -4.1766272 -4.1187682 -4.0642357 -4.0300422 -4.0410814 -4.0866232 -4.1369991 -4.1645813 -4.1612678 -4.1353254 -4.1136322][-4.2597322 -4.2852092 -4.2885017 -4.2708073 -4.2311993 -4.1850929 -4.1457238 -4.1222034 -4.1324625 -4.1624269 -4.1952677 -4.21167 -4.2051139 -4.1837678 -4.170167][-4.2875414 -4.3048224 -4.3088641 -4.2988653 -4.2711267 -4.2380652 -4.2100329 -4.1939273 -4.2036815 -4.2245069 -4.2463608 -4.2570252 -4.2526536 -4.2399158 -4.2344437][-4.3028326 -4.3113942 -4.3147936 -4.3126392 -4.2986097 -4.2785997 -4.2597284 -4.2477779 -4.2531796 -4.2663689 -4.2808619 -4.2903867 -4.2918744 -4.2887435 -4.2871056][-4.3173747 -4.3213549 -4.324245 -4.3260546 -4.3217716 -4.31114 -4.2985888 -4.2912211 -4.2944264 -4.3002477 -4.3059363 -4.3112478 -4.3152575 -4.3177581 -4.3188777]]...]
INFO - root - 2017-12-07 16:55:30.733958: step 14710, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.649 sec/batch; 69h:25m:48s remains)
INFO - root - 2017-12-07 16:55:47.120474: step 14720, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.587 sec/batch; 66h:47m:30s remains)
INFO - root - 2017-12-07 16:56:03.033238: step 14730, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 1.510 sec/batch; 63h:33m:21s remains)
INFO - root - 2017-12-07 16:56:19.228243: step 14740, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.691 sec/batch; 71h:09m:03s remains)
INFO - root - 2017-12-07 16:56:35.382181: step 14750, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.599 sec/batch; 67h:16m:42s remains)
INFO - root - 2017-12-07 16:56:51.661410: step 14760, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.674 sec/batch; 70h:26m:01s remains)
INFO - root - 2017-12-07 16:57:07.737260: step 14770, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.550 sec/batch; 65h:13m:40s remains)
INFO - root - 2017-12-07 16:57:23.944465: step 14780, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.720 sec/batch; 72h:21m:38s remains)
INFO - root - 2017-12-07 16:57:39.892594: step 14790, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.577 sec/batch; 66h:20m:53s remains)
INFO - root - 2017-12-07 16:57:56.254791: step 14800, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.661 sec/batch; 69h:51m:24s remains)
2017-12-07 16:57:57.585547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.257165 -4.2583418 -4.25802 -4.2606368 -4.2586923 -4.2567024 -4.2706122 -4.2927361 -4.314343 -4.3286729 -4.3336816 -4.3282013 -4.3155618 -4.3011928 -4.2903352][-4.2617011 -4.260819 -4.2575483 -4.2577252 -4.2507262 -4.2446852 -4.2614646 -4.2864795 -4.3064241 -4.3178086 -4.3243027 -4.3242521 -4.3180146 -4.312242 -4.3126874][-4.2689419 -4.2603526 -4.2554832 -4.25604 -4.2465253 -4.2339449 -4.2496123 -4.2788687 -4.2984266 -4.3057003 -4.307971 -4.3081551 -4.3058033 -4.305676 -4.3140268][-4.2823973 -4.2706695 -4.2677169 -4.2661943 -4.255156 -4.2372427 -4.246779 -4.2760515 -4.2944574 -4.2978492 -4.2962327 -4.2958078 -4.296741 -4.2992625 -4.3080993][-4.3025823 -4.2932239 -4.2907796 -4.2855248 -4.2731671 -4.2536259 -4.2549996 -4.2798333 -4.2940574 -4.2924318 -4.2872214 -4.2846627 -4.2862387 -4.2918196 -4.2990212][-4.3223948 -4.3186636 -4.3130774 -4.3003559 -4.28461 -4.2610288 -4.2561636 -4.2816358 -4.2973065 -4.293417 -4.2836142 -4.2729912 -4.2695441 -4.2762933 -4.2845359][-4.33199 -4.3339009 -4.3240833 -4.3019819 -4.2745757 -4.2408376 -4.2235274 -4.249908 -4.2787251 -4.2871585 -4.282342 -4.2679844 -4.2590847 -4.2611356 -4.2661171][-4.3267717 -4.3340387 -4.3239779 -4.2939773 -4.2527728 -4.2028322 -4.1664705 -4.1879025 -4.23218 -4.2632456 -4.2745891 -4.2662554 -4.2578297 -4.2562318 -4.2558][-4.3264532 -4.3343554 -4.3218956 -4.2841144 -4.2301159 -4.16375 -4.1058626 -4.1154165 -4.1689367 -4.2180786 -4.2489023 -4.2549648 -4.2553196 -4.2551465 -4.2520323][-4.33817 -4.3462529 -4.3330679 -4.2927613 -4.2324743 -4.1557722 -4.0824509 -4.0745907 -4.1210976 -4.1737185 -4.2154713 -4.2338133 -4.2445292 -4.2516837 -4.2536168][-4.3466845 -4.3559952 -4.3488355 -4.3171148 -4.2625203 -4.1860046 -4.1041603 -4.0760727 -4.1012053 -4.1412182 -4.180759 -4.2055211 -4.2257504 -4.2429276 -4.2548513][-4.3390355 -4.3516397 -4.353241 -4.3340092 -4.2934275 -4.2292247 -4.1535697 -4.1124411 -4.1132321 -4.1326418 -4.1621861 -4.186378 -4.2101183 -4.2354255 -4.256393][-4.3157125 -4.3333316 -4.3427424 -4.3374457 -4.3134966 -4.267755 -4.2099633 -4.1685772 -4.1531596 -4.1535754 -4.1699219 -4.1877871 -4.2079186 -4.2340426 -4.2579956][-4.2857146 -4.3078761 -4.3208437 -4.3234181 -4.3128524 -4.2848973 -4.2470064 -4.2172089 -4.1987867 -4.1911817 -4.197742 -4.2072515 -4.2208123 -4.2428226 -4.2638826][-4.2539091 -4.2760873 -4.2909966 -4.2994509 -4.2987142 -4.2850909 -4.264452 -4.2468758 -4.2323523 -4.2248993 -4.2277169 -4.2331743 -4.2413421 -4.2556329 -4.2696171]]...]
INFO - root - 2017-12-07 16:58:13.834208: step 14810, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.672 sec/batch; 70h:20m:30s remains)
INFO - root - 2017-12-07 16:58:30.106080: step 14820, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 67h:13m:55s remains)
INFO - root - 2017-12-07 16:58:46.317541: step 14830, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.552 sec/batch; 65h:16m:06s remains)
INFO - root - 2017-12-07 16:59:02.486302: step 14840, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.652 sec/batch; 69h:29m:23s remains)
INFO - root - 2017-12-07 16:59:18.604855: step 14850, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.603 sec/batch; 67h:24m:13s remains)
INFO - root - 2017-12-07 16:59:35.015651: step 14860, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.685 sec/batch; 70h:50m:38s remains)
INFO - root - 2017-12-07 16:59:51.069201: step 14870, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.518 sec/batch; 63h:49m:08s remains)
INFO - root - 2017-12-07 17:00:07.271683: step 14880, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.694 sec/batch; 71h:12m:45s remains)
INFO - root - 2017-12-07 17:00:23.336938: step 14890, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.625 sec/batch; 68h:19m:53s remains)
INFO - root - 2017-12-07 17:00:39.290039: step 14900, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 67h:21m:39s remains)
2017-12-07 17:00:40.696293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3296895 -4.3320503 -4.3325667 -4.3341637 -4.3354797 -4.3340368 -4.3310804 -4.3251309 -4.3172889 -4.3084536 -4.3007989 -4.296382 -4.3005052 -4.3081641 -4.310102][-4.3282323 -4.3302827 -4.3313928 -4.333571 -4.333427 -4.3283367 -4.3220625 -4.3137674 -4.3038144 -4.2885728 -4.2726417 -4.263453 -4.2684793 -4.2804961 -4.2853985][-4.3232026 -4.3251476 -4.3254089 -4.3243356 -4.318954 -4.3064384 -4.2953138 -4.2856727 -4.2782702 -4.2614655 -4.2427592 -4.2364206 -4.24369 -4.2568183 -4.2625113][-4.31844 -4.31907 -4.3137259 -4.3037715 -4.2886109 -4.2702622 -4.2547383 -4.245719 -4.2448473 -4.2331533 -4.2213268 -4.2260118 -4.2369428 -4.2455258 -4.2476563][-4.3200665 -4.3149858 -4.2960892 -4.2667537 -4.2349181 -4.213635 -4.2025037 -4.2039385 -4.2152867 -4.2144237 -4.2117414 -4.2265434 -4.2382889 -4.2382617 -4.2366676][-4.3220415 -4.3023763 -4.2573562 -4.1944032 -4.1371527 -4.1157355 -4.12467 -4.1501904 -4.1828022 -4.1980662 -4.2040448 -4.2259803 -4.2364635 -4.2280221 -4.2251678][-4.3164396 -4.2715411 -4.1924386 -4.0876145 -3.9967372 -3.9784751 -4.0220685 -4.0872078 -4.1501603 -4.1864519 -4.2028055 -4.2286711 -4.2328458 -4.2190008 -4.2187285][-4.2956758 -4.223515 -4.111958 -3.9660313 -3.8370554 -3.8262973 -3.917151 -4.0302649 -4.1241608 -4.1781507 -4.2026296 -4.2255297 -4.2245407 -4.213903 -4.2166615][-4.2665095 -4.1800685 -4.0581112 -3.8987291 -3.7595272 -3.7657127 -3.8871069 -4.022892 -4.1263819 -4.1852994 -4.2092443 -4.22082 -4.2144604 -4.2053914 -4.2075176][-4.2481022 -4.1678357 -4.0634665 -3.9327993 -3.8323283 -3.8579769 -3.9708202 -4.087585 -4.1724081 -4.222105 -4.2410474 -4.2396369 -4.2221174 -4.205956 -4.2018886][-4.2373648 -4.1746235 -4.1035309 -4.0221977 -3.9721856 -4.0037231 -4.0852823 -4.1655021 -4.223248 -4.2609444 -4.2757206 -4.264246 -4.2337313 -4.2055144 -4.1917052][-4.2235208 -4.1845293 -4.152154 -4.1184855 -4.1036887 -4.1291471 -4.17682 -4.2222471 -4.2567539 -4.2812281 -4.2946572 -4.2820597 -4.2494822 -4.2167616 -4.1964536][-4.2106018 -4.1965766 -4.194931 -4.1938782 -4.2005258 -4.2202134 -4.2442288 -4.2658978 -4.2812343 -4.2919288 -4.3028331 -4.2945404 -4.2659664 -4.23535 -4.2142344][-4.2062941 -4.2086124 -4.2215576 -4.2353725 -4.2515993 -4.2706661 -4.285841 -4.296783 -4.3002 -4.3014588 -4.3089123 -4.305953 -4.2868338 -4.263689 -4.2479577][-4.2143259 -4.2209582 -4.2361345 -4.2532229 -4.2730656 -4.2942863 -4.3061481 -4.3064466 -4.300355 -4.2958894 -4.3035841 -4.3105283 -4.3051958 -4.2945318 -4.2886572]]...]
INFO - root - 2017-12-07 17:00:56.876602: step 14910, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.565 sec/batch; 65h:48m:30s remains)
INFO - root - 2017-12-07 17:01:13.248998: step 14920, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.665 sec/batch; 69h:59m:51s remains)
INFO - root - 2017-12-07 17:01:29.371404: step 14930, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.595 sec/batch; 67h:03m:06s remains)
INFO - root - 2017-12-07 17:01:45.672412: step 14940, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.671 sec/batch; 70h:14m:05s remains)
INFO - root - 2017-12-07 17:02:01.623377: step 14950, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.581 sec/batch; 66h:27m:26s remains)
INFO - root - 2017-12-07 17:02:17.871118: step 14960, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.674 sec/batch; 70h:20m:09s remains)
INFO - root - 2017-12-07 17:02:34.271709: step 14970, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.569 sec/batch; 65h:56m:48s remains)
INFO - root - 2017-12-07 17:02:50.585622: step 14980, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.676 sec/batch; 70h:25m:48s remains)
INFO - root - 2017-12-07 17:03:06.829037: step 14990, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.551 sec/batch; 65h:11m:08s remains)
INFO - root - 2017-12-07 17:03:23.300167: step 15000, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.582 sec/batch; 66h:28m:43s remains)
2017-12-07 17:03:24.719589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.210669 -4.1956944 -4.181901 -4.1721106 -4.1806536 -4.2028131 -4.2095613 -4.2114453 -4.2153587 -4.2144313 -4.2133055 -4.2142553 -4.216383 -4.21747 -4.2166653][-4.2148852 -4.1988883 -4.1877546 -4.1772985 -4.1796303 -4.1973877 -4.2039385 -4.2059078 -4.2105069 -4.210515 -4.21081 -4.213675 -4.2168431 -4.2185583 -4.2187557][-4.2232895 -4.208776 -4.1994295 -4.1913404 -4.1899109 -4.205409 -4.2118783 -4.2114811 -4.2133727 -4.213429 -4.2151117 -4.21934 -4.2235546 -4.2254028 -4.2253256][-4.2300053 -4.2131038 -4.1977911 -4.1858406 -4.1838012 -4.1998386 -4.2062488 -4.2009292 -4.19551 -4.1948771 -4.2001371 -4.2095175 -4.2164783 -4.2181306 -4.2168822][-4.229835 -4.2069287 -4.1782022 -4.1534858 -4.1485386 -4.1665177 -4.1737142 -4.1624761 -4.1508274 -4.15177 -4.1680965 -4.1901078 -4.2026548 -4.2035131 -4.1999316][-4.2181029 -4.1815567 -4.129118 -4.0813689 -4.0717368 -4.0899916 -4.0955248 -4.0833097 -4.0664477 -4.0672245 -4.100956 -4.1436157 -4.1698666 -4.1766114 -4.1755681][-4.2052827 -4.1603103 -4.0916042 -4.0236187 -4.0099425 -4.0286627 -4.0311828 -4.0153756 -3.9916987 -3.9866126 -4.0332484 -4.0929079 -4.1308155 -4.1408734 -4.1396923][-4.208 -4.1721492 -4.1166711 -4.0538721 -4.0388751 -4.052834 -4.04743 -4.030992 -4.0112352 -4.0066895 -4.05039 -4.1054277 -4.13762 -4.1409445 -4.1328611][-4.2203369 -4.1969404 -4.1617036 -4.1176929 -4.1039019 -4.1133342 -4.1054387 -4.0908151 -4.0773516 -4.07436 -4.1084232 -4.1485081 -4.1692848 -4.1649718 -4.1525078][-4.23027 -4.2124619 -4.1895924 -4.1614981 -4.1561871 -4.1684093 -4.165771 -4.1553006 -4.1456008 -4.1429882 -4.1669 -4.1971536 -4.2142005 -4.2109442 -4.2003326][-4.239738 -4.2239208 -4.2091403 -4.194736 -4.19851 -4.2180405 -4.2256155 -4.2250667 -4.2206759 -4.2191815 -4.2326703 -4.2504511 -4.2614508 -4.2601094 -4.2520208][-4.250257 -4.2372308 -4.2252913 -4.2173109 -4.2285852 -4.2527809 -4.26285 -4.2652063 -4.2649989 -4.2667203 -4.2768021 -4.2863975 -4.2911954 -4.2893605 -4.28337][-4.2604628 -4.2484136 -4.2360897 -4.2258563 -4.2356811 -4.2633557 -4.2752929 -4.2783618 -4.2813025 -4.2841139 -4.2922635 -4.2991719 -4.3021817 -4.3019376 -4.3004146][-4.2691088 -4.2595496 -4.2488232 -4.2359676 -4.2396169 -4.26969 -4.2840562 -4.2890654 -4.2943311 -4.2976127 -4.3039865 -4.3091288 -4.3110461 -4.3113122 -4.3118877][-4.2723784 -4.2654877 -4.2584786 -4.249743 -4.2530847 -4.2825685 -4.2983522 -4.3034921 -4.309586 -4.3122735 -4.3149929 -4.3167171 -4.3164787 -4.316484 -4.3168569]]...]
INFO - root - 2017-12-07 17:03:40.890818: step 15010, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.707 sec/batch; 71h:43m:43s remains)
INFO - root - 2017-12-07 17:03:57.105562: step 15020, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.608 sec/batch; 67h:32m:10s remains)
INFO - root - 2017-12-07 17:04:13.492282: step 15030, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.541 sec/batch; 64h:43m:45s remains)
INFO - root - 2017-12-07 17:04:29.921237: step 15040, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.677 sec/batch; 70h:27m:11s remains)
INFO - root - 2017-12-07 17:04:46.350707: step 15050, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.616 sec/batch; 67h:53m:06s remains)
INFO - root - 2017-12-07 17:05:02.609335: step 15060, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.649 sec/batch; 69h:14m:08s remains)
INFO - root - 2017-12-07 17:05:18.934772: step 15070, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.568 sec/batch; 65h:50m:19s remains)
INFO - root - 2017-12-07 17:05:35.161800: step 15080, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.621 sec/batch; 68h:02m:51s remains)
INFO - root - 2017-12-07 17:05:51.412846: step 15090, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.542 sec/batch; 64h:44m:12s remains)
INFO - root - 2017-12-07 17:06:07.799833: step 15100, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.638 sec/batch; 68h:45m:32s remains)
2017-12-07 17:06:09.208493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1936297 -4.1988668 -4.1994038 -4.2008266 -4.2133651 -4.2177887 -4.2124672 -4.2055931 -4.1985493 -4.1821952 -4.151969 -4.1377878 -4.1597228 -4.18558 -4.1731076][-4.219687 -4.2275395 -4.2293191 -4.2263308 -4.22924 -4.22435 -4.2153544 -4.2087989 -4.2096233 -4.198946 -4.172349 -4.1584878 -4.1763215 -4.2010808 -4.1971874][-4.2002382 -4.2130976 -4.21898 -4.210885 -4.2057405 -4.1923661 -4.1840172 -4.1858315 -4.1990843 -4.1983633 -4.1852212 -4.1840959 -4.1979961 -4.2202611 -4.2306538][-4.1582551 -4.1783667 -4.1890697 -4.1730576 -4.1614404 -4.1452584 -4.1384144 -4.1455388 -4.1656446 -4.176641 -4.178082 -4.1934757 -4.2063069 -4.2203679 -4.2331414][-4.1338744 -4.1647582 -4.1810079 -4.1612396 -4.1375451 -4.1159234 -4.106122 -4.1155095 -4.1371217 -4.1528149 -4.1613278 -4.1805959 -4.1908064 -4.192575 -4.1906528][-4.0977139 -4.1391292 -4.1606641 -4.1358695 -4.0972805 -4.0695171 -4.0598073 -4.0779219 -4.1054983 -4.1252117 -4.131835 -4.1476417 -4.1531377 -4.144608 -4.1313376][-4.0564294 -4.094099 -4.1141944 -4.0882773 -4.0393505 -3.9941597 -3.9673772 -3.9961576 -4.0439377 -4.0718575 -4.079236 -4.098959 -4.1075315 -4.1015625 -4.1007352][-4.0923204 -4.1051221 -4.1089244 -4.0790033 -4.0229969 -3.9609203 -3.9133091 -3.9424415 -4.0054097 -4.0370054 -4.0465255 -4.0706415 -4.0819945 -4.0770593 -4.0918736][-4.1667466 -4.1607981 -4.1478214 -4.1135216 -4.0615368 -4.0109849 -3.9729052 -3.9962926 -4.0494285 -4.0775003 -4.0865297 -4.0988922 -4.0970569 -4.0824637 -4.0997496][-4.2182679 -4.2014918 -4.1812873 -4.1482987 -4.10935 -4.0781717 -4.0604997 -4.0804706 -4.1186867 -4.14403 -4.1506019 -4.1499271 -4.136374 -4.1132975 -4.1193852][-4.2404647 -4.2154508 -4.189527 -4.1588697 -4.1307058 -4.113554 -4.1115789 -4.1344328 -4.165503 -4.1879897 -4.1897769 -4.1828461 -4.1696658 -4.1444798 -4.1353927][-4.2477055 -4.2236209 -4.2004757 -4.1738987 -4.1501336 -4.1381974 -4.1424084 -4.1633477 -4.1886306 -4.2040863 -4.2017961 -4.1973257 -4.1885252 -4.1651459 -4.1519532][-4.2486062 -4.2316332 -4.2160878 -4.1971169 -4.1805658 -4.1712875 -4.1777773 -4.1927447 -4.2054577 -4.2070637 -4.1980243 -4.1918707 -4.1830292 -4.1627417 -4.1529169][-4.2505174 -4.2391319 -4.2295752 -4.2173328 -4.2066522 -4.1998959 -4.2036052 -4.2115536 -4.2140465 -4.2071075 -4.1921749 -4.1822052 -4.1719127 -4.1552644 -4.1513062][-4.2554622 -4.2486157 -4.2460542 -4.239995 -4.2338533 -4.2291188 -4.228107 -4.2266445 -4.218401 -4.2026129 -4.1835513 -4.1722856 -4.160955 -4.145926 -4.1485443]]...]
INFO - root - 2017-12-07 17:06:25.470564: step 15110, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.658 sec/batch; 69h:37m:33s remains)
INFO - root - 2017-12-07 17:06:41.642628: step 15120, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.623 sec/batch; 68h:08m:19s remains)
INFO - root - 2017-12-07 17:06:57.836938: step 15130, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.591 sec/batch; 66h:48m:16s remains)
INFO - root - 2017-12-07 17:07:13.923764: step 15140, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.680 sec/batch; 70h:30m:39s remains)
INFO - root - 2017-12-07 17:07:30.225154: step 15150, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.606 sec/batch; 67h:24m:34s remains)
INFO - root - 2017-12-07 17:07:46.672644: step 15160, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.687 sec/batch; 70h:47m:23s remains)
INFO - root - 2017-12-07 17:08:02.892222: step 15170, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.621 sec/batch; 68h:01m:58s remains)
INFO - root - 2017-12-07 17:08:18.965870: step 15180, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.640 sec/batch; 68h:49m:49s remains)
INFO - root - 2017-12-07 17:08:35.219110: step 15190, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.668 sec/batch; 69h:58m:46s remains)
INFO - root - 2017-12-07 17:08:51.279963: step 15200, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.595 sec/batch; 66h:55m:11s remains)
2017-12-07 17:08:52.641496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2575121 -4.242022 -4.2390785 -4.2380056 -4.22996 -4.2155867 -4.2015629 -4.1995039 -4.2090659 -4.2235584 -4.2344017 -4.2352161 -4.2290926 -4.222086 -4.2207413][-4.2747192 -4.2565689 -4.2477551 -4.2395859 -4.2265196 -4.2105303 -4.1953063 -4.1898489 -4.1968975 -4.2105255 -4.2225962 -4.2245383 -4.2209058 -4.2138762 -4.2108674][-4.2818756 -4.2639518 -4.2537146 -4.2435989 -4.2275562 -4.20791 -4.1879811 -4.1749034 -4.1751652 -4.1855092 -4.1997867 -4.2091103 -4.2140145 -4.2108369 -4.2082014][-4.2741709 -4.2569623 -4.246233 -4.2373972 -4.2224026 -4.2019429 -4.17812 -4.157793 -4.1511383 -4.1627059 -4.1846013 -4.2032523 -4.2130904 -4.2102108 -4.2073717][-4.2511883 -4.2354269 -4.2253895 -4.217598 -4.2063303 -4.1887541 -4.1628084 -4.1332941 -4.1178913 -4.1344142 -4.1683559 -4.1970162 -4.2075944 -4.1984448 -4.1901588][-4.2225614 -4.2070603 -4.1942916 -4.1836724 -4.1712012 -4.1531286 -4.1229591 -4.0817561 -4.0577593 -4.08452 -4.1397796 -4.1847119 -4.1997271 -4.1870522 -4.1730742][-4.1955485 -4.1810493 -4.1648474 -4.1502962 -4.1348944 -4.1126614 -4.0721626 -4.0157485 -3.9822547 -4.020031 -4.0962186 -4.1588821 -4.1808443 -4.165844 -4.1433182][-4.1886773 -4.1758065 -4.1570134 -4.139842 -4.1221285 -4.0977435 -4.0506372 -3.9863317 -3.9482367 -3.9861434 -4.0657058 -4.133359 -4.1576605 -4.1424322 -4.1140409][-4.2061338 -4.194418 -4.1744056 -4.1558657 -4.1390729 -4.1196833 -4.0796676 -4.0247192 -3.9886332 -4.0139928 -4.0778055 -4.1367655 -4.1632543 -4.1545172 -4.1292157][-4.2348022 -4.2249866 -4.2069092 -4.1904664 -4.1748314 -4.1574068 -4.1228638 -4.0775857 -4.0474787 -4.063139 -4.1100821 -4.1594996 -4.1889625 -4.1886096 -4.172174][-4.2629142 -4.2569318 -4.2448583 -4.2330213 -4.2186804 -4.2008038 -4.1689558 -4.1321945 -4.1122217 -4.1275754 -4.1626306 -4.20056 -4.2257357 -4.227128 -4.2128053][-4.2904725 -4.2864761 -4.280508 -4.27385 -4.2628384 -4.2480888 -4.2221103 -4.1941953 -4.1813083 -4.1961975 -4.2213674 -4.246726 -4.2642546 -4.263886 -4.2486186][-4.3091311 -4.3058538 -4.3035827 -4.3014421 -4.2950006 -4.2849936 -4.2680879 -4.2507906 -4.2441344 -4.2565637 -4.272717 -4.2854323 -4.2929912 -4.2906103 -4.2764516][-4.318337 -4.3154526 -4.313776 -4.3130507 -4.3098111 -4.3040433 -4.2944674 -4.2853966 -4.282115 -4.2905645 -4.2993021 -4.3033552 -4.3043528 -4.3017282 -4.2909493][-4.3183475 -4.3159127 -4.3137407 -4.3127294 -4.3112416 -4.308466 -4.3041363 -4.3005219 -4.299015 -4.3028421 -4.3065271 -4.3063555 -4.3030691 -4.2992349 -4.290071]]...]
INFO - root - 2017-12-07 17:09:08.869010: step 15210, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.587 sec/batch; 66h:35m:02s remains)
INFO - root - 2017-12-07 17:09:25.116339: step 15220, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.692 sec/batch; 70h:59m:46s remains)
INFO - root - 2017-12-07 17:09:41.185589: step 15230, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.575 sec/batch; 66h:04m:26s remains)
INFO - root - 2017-12-07 17:09:57.479848: step 15240, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 67h:02m:53s remains)
INFO - root - 2017-12-07 17:10:13.688764: step 15250, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.625 sec/batch; 68h:09m:02s remains)
INFO - root - 2017-12-07 17:10:30.018610: step 15260, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.667 sec/batch; 69h:55m:08s remains)
INFO - root - 2017-12-07 17:10:46.257702: step 15270, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.569 sec/batch; 65h:49m:12s remains)
INFO - root - 2017-12-07 17:11:02.681986: step 15280, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.595 sec/batch; 66h:52m:42s remains)
INFO - root - 2017-12-07 17:11:18.991121: step 15290, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.682 sec/batch; 70h:30m:59s remains)
INFO - root - 2017-12-07 17:11:35.149124: step 15300, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.539 sec/batch; 64h:32m:46s remains)
2017-12-07 17:11:36.426978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3176837 -4.3183193 -4.314816 -4.3049273 -4.3014812 -4.3032694 -4.3040366 -4.3045406 -4.2942853 -4.2892184 -4.2912946 -4.3023095 -4.3075643 -4.300323 -4.2800689][-4.2848911 -4.2896709 -4.2889166 -4.2765045 -4.2702966 -4.2690639 -4.2681804 -4.26607 -4.2505283 -4.2433372 -4.250495 -4.2721477 -4.2843428 -4.2765741 -4.24681][-4.24043 -4.2461205 -4.2527828 -4.2485433 -4.2511005 -4.252358 -4.2509351 -4.2424498 -4.2164412 -4.2007308 -4.210639 -4.2390671 -4.2587361 -4.2542009 -4.2169905][-4.1923242 -4.1932588 -4.2047257 -4.2114935 -4.2288613 -4.2377634 -4.2325888 -4.2138486 -4.179913 -4.1552763 -4.1661768 -4.1985312 -4.2237139 -4.222908 -4.1831489][-4.1584673 -4.1479793 -4.1606288 -4.1821423 -4.2145905 -4.2298326 -4.2152419 -4.1832252 -4.1430955 -4.1156864 -4.1303463 -4.1669774 -4.1962585 -4.1961346 -4.1566081][-4.1533661 -4.1311727 -4.1448355 -4.1778536 -4.2220221 -4.2425694 -4.2212224 -4.1797853 -4.13465 -4.10671 -4.1243916 -4.1628613 -4.1935773 -4.1931648 -4.1537042][-4.1476355 -4.1168828 -4.1375132 -4.1791997 -4.2298169 -4.2550249 -4.2314444 -4.1868234 -4.1410327 -4.1149907 -4.1329145 -4.1680403 -4.1971717 -4.1981406 -4.159276][-4.1131792 -4.0824971 -4.1175871 -4.1718616 -4.2259359 -4.2553272 -4.2348671 -4.1925387 -4.148787 -4.1215096 -4.1299415 -4.1565876 -4.1805773 -4.1800489 -4.1417112][-4.0685811 -4.0429316 -4.0882111 -4.1544228 -4.2108445 -4.2432318 -4.2326922 -4.1966724 -4.1576886 -4.1265426 -4.122859 -4.1392984 -4.15684 -4.1513119 -4.1147122][-4.0667439 -4.049644 -4.0918303 -4.1566825 -4.2140288 -4.2498569 -4.25119 -4.2274432 -4.1948619 -4.1605368 -4.1469054 -4.149353 -4.154561 -4.1428895 -4.1102314][-4.115766 -4.1025653 -4.1293888 -4.180356 -4.23004 -4.2674451 -4.276967 -4.2636585 -4.2359004 -4.2030377 -4.185123 -4.1793723 -4.1749377 -4.1548653 -4.123929][-4.1902618 -4.1793809 -4.19031 -4.2231092 -4.2609005 -4.290247 -4.2995338 -4.2910089 -4.2709994 -4.2482 -4.2370639 -4.2324367 -4.226378 -4.2057366 -4.1778951][-4.2466278 -4.2408037 -4.2445345 -4.2625833 -4.2850432 -4.3012571 -4.306788 -4.3006434 -4.2900419 -4.2820492 -4.2804661 -4.2812686 -4.278945 -4.2637305 -4.2415757][-4.2823648 -4.2808604 -4.2831364 -4.2943778 -4.3059168 -4.3105474 -4.3123856 -4.3087349 -4.3059826 -4.3076878 -4.3126378 -4.3158855 -4.3147836 -4.3031931 -4.285984][-4.3070722 -4.3074846 -4.30874 -4.315063 -4.3201084 -4.3198204 -4.3186955 -4.3163042 -4.3160677 -4.3207722 -4.3266678 -4.3285842 -4.3282986 -4.3204203 -4.3098545]]...]
INFO - root - 2017-12-07 17:11:52.700997: step 15310, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.560 sec/batch; 65h:23m:18s remains)
INFO - root - 2017-12-07 17:12:08.841930: step 15320, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.603 sec/batch; 67h:11m:56s remains)
INFO - root - 2017-12-07 17:12:25.273018: step 15330, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.588 sec/batch; 66h:35m:35s remains)
INFO - root - 2017-12-07 17:12:41.733287: step 15340, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.730 sec/batch; 72h:31m:12s remains)
INFO - root - 2017-12-07 17:12:58.172464: step 15350, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 1.625 sec/batch; 68h:07m:15s remains)
INFO - root - 2017-12-07 17:13:14.476735: step 15360, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.747 sec/batch; 73h:13m:11s remains)
INFO - root - 2017-12-07 17:13:30.455880: step 15370, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.585 sec/batch; 66h:26m:39s remains)
INFO - root - 2017-12-07 17:13:46.713613: step 15380, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.649 sec/batch; 69h:07m:36s remains)
INFO - root - 2017-12-07 17:14:03.085290: step 15390, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.628 sec/batch; 68h:14m:19s remains)
INFO - root - 2017-12-07 17:14:19.370873: step 15400, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.632 sec/batch; 68h:22m:06s remains)
2017-12-07 17:14:20.675427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3330755 -4.3290496 -4.3225236 -4.2939792 -4.2335539 -4.1769714 -4.1493759 -4.1543818 -4.1874828 -4.2321954 -4.2664466 -4.2768345 -4.2764449 -4.2580066 -4.2444062][-4.3316293 -4.3338423 -4.3281326 -4.2886963 -4.2093616 -4.1366172 -4.1059227 -4.1220183 -4.1695623 -4.22288 -4.2607079 -4.2752676 -4.2781215 -4.2628689 -4.2519755][-4.3329272 -4.3391871 -4.3313327 -4.2804074 -4.1916223 -4.1018338 -4.0627251 -4.0901365 -4.1579747 -4.2166376 -4.2549658 -4.275279 -4.2834167 -4.274086 -4.2657528][-4.3330088 -4.3417716 -4.3284211 -4.2666016 -4.1696978 -4.0715046 -4.0243993 -4.0667548 -4.153863 -4.2150116 -4.2486606 -4.2720752 -4.2842708 -4.2855463 -4.2827969][-4.3258276 -4.3313942 -4.308063 -4.2349863 -4.1316867 -4.0231738 -3.9651821 -4.0266132 -4.1339073 -4.200737 -4.2338653 -4.2621727 -4.2796984 -4.2920384 -4.2963419][-4.3057103 -4.2998247 -4.2639055 -4.1777096 -4.0591226 -3.9247649 -3.8449345 -3.9386945 -4.0771618 -4.1655083 -4.2115588 -4.2463861 -4.2699142 -4.2934017 -4.3065743][-4.2802749 -4.2605515 -4.2095776 -4.1086273 -3.9699755 -3.7955816 -3.6845286 -3.8132253 -3.9900992 -4.1087942 -4.1775012 -4.22459 -4.2604032 -4.2966485 -4.3171391][-4.2671742 -4.234673 -4.1719065 -4.0591407 -3.9031906 -3.6962905 -3.5721335 -3.712707 -3.9062929 -4.0444121 -4.1301818 -4.1905494 -4.2443781 -4.2949486 -4.32122][-4.2635179 -4.2212729 -4.1566944 -4.0489616 -3.905956 -3.7202914 -3.6151218 -3.7138658 -3.8690457 -3.9945822 -4.0804009 -4.1495805 -4.219058 -4.283 -4.3149357][-4.2734227 -4.2269521 -4.1690226 -4.0845561 -3.9781382 -3.8403225 -3.7592983 -3.8029454 -3.8999734 -3.9963839 -4.0734119 -4.1446109 -4.2170515 -4.2770934 -4.3075781][-4.2934418 -4.2510376 -4.2037535 -4.1440082 -4.0726986 -3.9805064 -3.9196558 -3.9352622 -3.9991639 -4.0700474 -4.1345782 -4.193717 -4.2489543 -4.2910314 -4.311389][-4.3105059 -4.2798748 -4.24654 -4.2078681 -4.166235 -4.1085849 -4.0635834 -4.066143 -4.1062226 -4.1558676 -4.20696 -4.2530532 -4.2922292 -4.3176813 -4.3271708][-4.3208108 -4.3027868 -4.2843933 -4.2608948 -4.2383842 -4.2033672 -4.1697774 -4.1636724 -4.1877565 -4.2213006 -4.2596464 -4.2970204 -4.3259568 -4.3400917 -4.3419914][-4.3188219 -4.3116479 -4.3032889 -4.2912498 -4.2833567 -4.2658234 -4.2441916 -4.2373877 -4.2504096 -4.2715063 -4.2961864 -4.3217125 -4.3390875 -4.3453741 -4.3425617][-4.3121881 -4.3086581 -4.3048935 -4.3000979 -4.2978616 -4.289968 -4.2784719 -4.2741294 -4.2810707 -4.2938786 -4.3097982 -4.3256941 -4.336298 -4.334568 -4.3215966]]...]
INFO - root - 2017-12-07 17:14:37.007079: step 15410, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.565 sec/batch; 65h:35m:15s remains)
INFO - root - 2017-12-07 17:14:53.186652: step 15420, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.631 sec/batch; 68h:20m:56s remains)
INFO - root - 2017-12-07 17:15:09.472049: step 15430, loss = 2.09, batch loss = 2.04 (9.8 examples/sec; 1.626 sec/batch; 68h:06m:00s remains)
INFO - root - 2017-12-07 17:15:25.806858: step 15440, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 1.678 sec/batch; 70h:16m:49s remains)
INFO - root - 2017-12-07 17:15:41.941539: step 15450, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.570 sec/batch; 65h:47m:04s remains)
INFO - root - 2017-12-07 17:15:58.150340: step 15460, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.586 sec/batch; 66h:25m:00s remains)
INFO - root - 2017-12-07 17:16:14.513059: step 15470, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 1.662 sec/batch; 69h:35m:39s remains)
INFO - root - 2017-12-07 17:16:30.511128: step 15480, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.579 sec/batch; 66h:07m:34s remains)
INFO - root - 2017-12-07 17:16:46.750688: step 15490, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.718 sec/batch; 71h:55m:33s remains)
INFO - root - 2017-12-07 17:17:02.951255: step 15500, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 1.560 sec/batch; 65h:18m:39s remains)
2017-12-07 17:17:04.400994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2838888 -4.2827554 -4.2848005 -4.2918639 -4.2972603 -4.3014951 -4.3041563 -4.2991457 -4.2908468 -4.2878685 -4.285902 -4.2760048 -4.2620988 -4.2543626 -4.2603807][-4.2899208 -4.2920232 -4.294559 -4.301753 -4.3067341 -4.3089247 -4.3080873 -4.2984152 -4.2890811 -4.2874303 -4.2849479 -4.271049 -4.2533922 -4.2436094 -4.248457][-4.2975612 -4.298738 -4.2986231 -4.3034196 -4.3056254 -4.3028631 -4.296576 -4.2841172 -4.2770662 -4.2804413 -4.2785416 -4.263751 -4.2454357 -4.2352223 -4.23755][-4.3006258 -4.2960777 -4.2890172 -4.2856469 -4.2782011 -4.2684846 -4.2552595 -4.2438455 -4.2502584 -4.2699456 -4.2779179 -4.270637 -4.2558374 -4.24386 -4.24139][-4.2910433 -4.2773671 -4.2592125 -4.2427354 -4.2221518 -4.1982141 -4.1689711 -4.1648188 -4.2008286 -4.2460356 -4.2715292 -4.2805624 -4.2787476 -4.2709365 -4.267684][-4.2823396 -4.2571564 -4.2228827 -4.1896219 -4.1497808 -4.0944958 -4.0270414 -4.0244756 -4.1052446 -4.1889796 -4.2375288 -4.2658472 -4.2786732 -4.2797441 -4.2831516][-4.2770724 -4.2501268 -4.2039375 -4.1531262 -4.0863185 -3.9806511 -3.8446233 -3.8249779 -3.9655015 -4.0993028 -4.172143 -4.2197828 -4.24893 -4.2652869 -4.2801332][-4.2685037 -4.2549925 -4.2211094 -4.167098 -4.0880446 -3.952508 -3.7614017 -3.708909 -3.8720276 -4.0291424 -4.1120224 -4.1682124 -4.2097464 -4.2415342 -4.2690549][-4.2534389 -4.2561212 -4.2481918 -4.2153134 -4.1577635 -4.0529804 -3.9034181 -3.8521821 -3.9581811 -4.0682669 -4.1196365 -4.1533036 -4.1868973 -4.2184606 -4.2446494][-4.2347503 -4.2436833 -4.2500596 -4.2385969 -4.2068558 -4.1475782 -4.0691023 -4.0500326 -4.1085749 -4.16054 -4.1736541 -4.1792717 -4.1982923 -4.2198186 -4.2304745][-4.2223926 -4.2366638 -4.2490506 -4.2496948 -4.2338257 -4.2033072 -4.1742511 -4.1793041 -4.2091684 -4.219985 -4.2123761 -4.2093854 -4.2266746 -4.2429767 -4.2405362][-4.1981831 -4.2178373 -4.2319131 -4.2391491 -4.2352715 -4.22092 -4.215086 -4.2296853 -4.2423248 -4.2308893 -4.2112584 -4.2075109 -4.2292571 -4.2447743 -4.2380805][-4.1718407 -4.1879334 -4.1949415 -4.2010241 -4.2023525 -4.194252 -4.1980519 -4.2155447 -4.2242908 -4.2085695 -4.1894794 -4.1910052 -4.2134347 -4.2234426 -4.2147837][-4.1673846 -4.1745796 -4.172267 -4.1711636 -4.1687217 -4.1618638 -4.1694317 -4.1880951 -4.1984811 -4.1892543 -4.1808367 -4.1894469 -4.2055731 -4.2059526 -4.1929398][-4.1921597 -4.1942663 -4.1870303 -4.1808648 -4.175962 -4.1710572 -4.1786227 -4.1947989 -4.2033973 -4.2000966 -4.2005754 -4.2110662 -4.2192488 -4.213809 -4.2010536]]...]
INFO - root - 2017-12-07 17:17:20.644794: step 15510, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.551 sec/batch; 64h:56m:27s remains)
INFO - root - 2017-12-07 17:17:36.912504: step 15520, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.696 sec/batch; 71h:00m:31s remains)
INFO - root - 2017-12-07 17:17:53.270648: step 15530, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 1.620 sec/batch; 67h:50m:03s remains)
INFO - root - 2017-12-07 17:18:09.319676: step 15540, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.671 sec/batch; 69h:57m:28s remains)
INFO - root - 2017-12-07 17:18:25.459320: step 15550, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.573 sec/batch; 65h:51m:22s remains)
INFO - root - 2017-12-07 17:18:41.728596: step 15560, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.557 sec/batch; 65h:10m:07s remains)
INFO - root - 2017-12-07 17:18:58.065123: step 15570, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 1.694 sec/batch; 70h:53m:36s remains)
INFO - root - 2017-12-07 17:19:14.235536: step 15580, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.592 sec/batch; 66h:37m:46s remains)
INFO - root - 2017-12-07 17:19:30.647807: step 15590, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.654 sec/batch; 69h:12m:07s remains)
INFO - root - 2017-12-07 17:19:46.572981: step 15600, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.562 sec/batch; 65h:22m:48s remains)
2017-12-07 17:19:48.035655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3050146 -4.28779 -4.2699366 -4.2546163 -4.2528443 -4.2600164 -4.2718248 -4.2807913 -4.2849755 -4.2825127 -4.2803278 -4.2853346 -4.2956791 -4.3093805 -4.3221602][-4.2936549 -4.2667494 -4.2417 -4.2208152 -4.2189565 -4.2267785 -4.2378836 -4.2469611 -4.2507071 -4.2473588 -4.2449932 -4.2514772 -4.2642813 -4.2849994 -4.3051114][-4.2738037 -4.2353425 -4.2010412 -4.1749487 -4.1706667 -4.1789193 -4.1914234 -4.2013283 -4.207366 -4.2048397 -4.2025485 -4.21273 -4.2285204 -4.2541075 -4.2825646][-4.2509131 -4.2077365 -4.1694794 -4.1345758 -4.1184287 -4.1186023 -4.1259727 -4.1381674 -4.1518788 -4.1581173 -4.158865 -4.17321 -4.1924033 -4.2213993 -4.2562075][-4.2174392 -4.168417 -4.1287308 -4.0895019 -4.0608954 -4.0400605 -4.0241351 -4.0256524 -4.0561857 -4.0956197 -4.1203003 -4.14561 -4.1682453 -4.1967597 -4.2353845][-4.1867814 -4.1301155 -4.0902066 -4.0545425 -4.0221033 -3.9797435 -3.9232571 -3.8840964 -3.91395 -3.9978113 -4.0677695 -4.1209507 -4.1571341 -4.1878061 -4.2262812][-4.1642818 -4.1000433 -4.054646 -4.0242567 -4.003334 -3.96406 -3.8941193 -3.8122566 -3.799335 -3.8975093 -3.9992533 -4.0859103 -4.1448789 -4.1866946 -4.2270346][-4.1606 -4.093297 -4.0393982 -4.0024638 -3.9867268 -3.9648314 -3.924772 -3.8656914 -3.8280513 -3.8839059 -3.9697466 -4.061017 -4.1315985 -4.1847334 -4.2296877][-4.1924162 -4.1364775 -4.0852919 -4.0434294 -4.0222769 -4.0089817 -3.9980974 -3.9740255 -3.9487333 -3.9742291 -4.0231829 -4.0884967 -4.1470594 -4.1959386 -4.238359][-4.23234 -4.1984725 -4.1638317 -4.1313381 -4.1117735 -4.1021957 -4.1020579 -4.09098 -4.0747643 -4.0833178 -4.1062775 -4.1438513 -4.183218 -4.220933 -4.2556682][-4.26154 -4.2427092 -4.2248983 -4.2052093 -4.1952438 -4.1952338 -4.2011261 -4.1982527 -4.1837997 -4.1802864 -4.1858139 -4.2031021 -4.2279282 -4.254005 -4.2793036][-4.2862968 -4.2776895 -4.2698059 -4.2612228 -4.2584329 -4.2639942 -4.27218 -4.2726235 -4.2624936 -4.25564 -4.2547746 -4.2594223 -4.2722859 -4.2885108 -4.3034377][-4.3026733 -4.3007069 -4.2988324 -4.295403 -4.2962303 -4.3011994 -4.3090825 -4.3115654 -4.3071094 -4.301034 -4.29999 -4.3016968 -4.3067341 -4.3153343 -4.3216887][-4.3115039 -4.3118773 -4.3138819 -4.3139238 -4.3152504 -4.3184524 -4.3226953 -4.3245878 -4.3237848 -4.3212042 -4.3203344 -4.3223066 -4.3243384 -4.3294768 -4.3326988][-4.3246484 -4.3254185 -4.3265505 -4.3269668 -4.3270793 -4.32803 -4.3296227 -4.33037 -4.3306065 -4.3302393 -4.3296294 -4.3305874 -4.3309627 -4.3330131 -4.3354521]]...]
INFO - root - 2017-12-07 17:20:04.324202: step 15610, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.642 sec/batch; 68h:42m:51s remains)
INFO - root - 2017-12-07 17:20:20.744261: step 15620, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.694 sec/batch; 70h:52m:44s remains)
INFO - root - 2017-12-07 17:20:37.066662: step 15630, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.596 sec/batch; 66h:45m:31s remains)
INFO - root - 2017-12-07 17:20:53.346998: step 15640, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.603 sec/batch; 67h:04m:25s remains)
INFO - root - 2017-12-07 17:21:09.502182: step 15650, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.606 sec/batch; 67h:11m:12s remains)
INFO - root - 2017-12-07 17:21:25.658680: step 15660, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.618 sec/batch; 67h:41m:58s remains)
INFO - root - 2017-12-07 17:21:41.948138: step 15670, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 1.683 sec/batch; 70h:22m:53s remains)
INFO - root - 2017-12-07 17:21:58.279059: step 15680, loss = 2.08, batch loss = 2.03 (10.4 examples/sec; 1.545 sec/batch; 64h:36m:23s remains)
INFO - root - 2017-12-07 17:22:14.711034: step 15690, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.700 sec/batch; 71h:07m:06s remains)
INFO - root - 2017-12-07 17:22:30.920941: step 15700, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.554 sec/batch; 64h:59m:22s remains)
2017-12-07 17:22:32.290395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1634521 -4.1548834 -4.1560893 -4.1641312 -4.1709614 -4.16204 -4.15083 -4.15763 -4.175446 -4.2132044 -4.2439523 -4.2555633 -4.2601271 -4.267324 -4.2720141][-4.1974053 -4.1903629 -4.1865973 -4.1886039 -4.1957679 -4.1912866 -4.1776347 -4.1791968 -4.1927834 -4.2268348 -4.2540278 -4.2600241 -4.259903 -4.2607694 -4.2641268][-4.2423315 -4.2380133 -4.2328639 -4.23547 -4.2433114 -4.2405744 -4.2245779 -4.2173128 -4.2263441 -4.2562079 -4.2807665 -4.2810903 -4.2756996 -4.2729111 -4.2738838][-4.265954 -4.2632146 -4.25665 -4.2565379 -4.26279 -4.2608571 -4.2425075 -4.2251797 -4.2296495 -4.2630415 -4.29536 -4.2997608 -4.2926254 -4.2904477 -4.2904339][-4.2506347 -4.2451077 -4.2336855 -4.2300129 -4.2368932 -4.2343349 -4.211206 -4.1864572 -4.1855516 -4.225729 -4.2725716 -4.290462 -4.2919369 -4.2949576 -4.2965198][-4.2048931 -4.1974754 -4.180975 -4.1727271 -4.1763358 -4.1656017 -4.1316223 -4.0948439 -4.0942926 -4.1559238 -4.2270465 -4.26197 -4.2758617 -4.2874 -4.292151][-4.1657848 -4.1532507 -4.1325617 -4.1216908 -4.1181746 -4.0967865 -4.05006 -3.9977388 -3.9908628 -4.0708752 -4.1674352 -4.219523 -4.2447205 -4.2678947 -4.2810278][-4.1595163 -4.1465459 -4.1306973 -4.124156 -4.1218286 -4.09754 -4.0550709 -4.000504 -3.977901 -4.0488887 -4.1426597 -4.1943817 -4.2229676 -4.2516813 -4.2721581][-4.1814432 -4.169436 -4.1596055 -4.1595349 -4.1628361 -4.1458097 -4.1192503 -4.0839982 -4.0651393 -4.1149125 -4.1810527 -4.213521 -4.2311893 -4.254076 -4.2735248][-4.1944466 -4.1851225 -4.1800728 -4.1845655 -4.1917028 -4.1838937 -4.1705894 -4.1545382 -4.1519408 -4.1925 -4.2399292 -4.2600541 -4.26809 -4.2789159 -4.2875323][-4.1823745 -4.17581 -4.1742153 -4.1816325 -4.1908135 -4.19093 -4.1848512 -4.1786246 -4.1859026 -4.2232656 -4.2654071 -4.2861714 -4.2931614 -4.2968664 -4.2959666][-4.1655083 -4.1589193 -4.156642 -4.1616559 -4.1683655 -4.1657782 -4.1557508 -4.1487489 -4.1622734 -4.2048903 -4.2479873 -4.2749491 -4.2878036 -4.2929792 -4.2907968][-4.1782618 -4.171495 -4.1671267 -4.1636362 -4.1566138 -4.1377344 -4.1133547 -4.1040039 -4.123652 -4.1717815 -4.2188358 -4.2522039 -4.268898 -4.2792063 -4.2806306][-4.2170062 -4.2101374 -4.2031875 -4.1901875 -4.1680579 -4.1367788 -4.1114578 -4.1095133 -4.1282029 -4.1701913 -4.20997 -4.2401109 -4.2561607 -4.2682686 -4.2722507][-4.2328949 -4.22913 -4.2211285 -4.2016706 -4.1726437 -4.1468711 -4.1379023 -4.1443167 -4.1616087 -4.1927047 -4.2235951 -4.2464452 -4.2598009 -4.2690439 -4.2753763]]...]
INFO - root - 2017-12-07 17:22:48.502753: step 15710, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.597 sec/batch; 66h:46m:08s remains)
INFO - root - 2017-12-07 17:23:04.741201: step 15720, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.717 sec/batch; 71h:48m:37s remains)
INFO - root - 2017-12-07 17:23:21.043219: step 15730, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.589 sec/batch; 66h:27m:19s remains)
INFO - root - 2017-12-07 17:23:37.184441: step 15740, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.586 sec/batch; 66h:18m:14s remains)
INFO - root - 2017-12-07 17:23:53.577814: step 15750, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 1.660 sec/batch; 69h:24m:53s remains)
INFO - root - 2017-12-07 17:24:09.906136: step 15760, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.621 sec/batch; 67h:46m:57s remains)
INFO - root - 2017-12-07 17:24:26.130889: step 15770, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.619 sec/batch; 67h:39m:21s remains)
INFO - root - 2017-12-07 17:24:42.119030: step 15780, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.623 sec/batch; 67h:50m:25s remains)
INFO - root - 2017-12-07 17:24:58.575518: step 15790, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.633 sec/batch; 68h:15m:18s remains)
INFO - root - 2017-12-07 17:25:14.930579: step 15800, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.713 sec/batch; 71h:36m:00s remains)
2017-12-07 17:25:16.275896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2730813 -4.2783031 -4.2707672 -4.2517118 -4.2282882 -4.199039 -4.171041 -4.16858 -4.1706567 -4.1424713 -4.1246443 -4.1339574 -4.1434937 -4.1451693 -4.1424832][-4.2709866 -4.2756076 -4.2646222 -4.2456884 -4.2236595 -4.2001386 -4.1774244 -4.1757908 -4.1765952 -4.1499219 -4.1270623 -4.1247573 -4.1219192 -4.1148348 -4.1151667][-4.2704163 -4.2754188 -4.2627578 -4.2422533 -4.2205572 -4.1977754 -4.1803207 -4.1728191 -4.1699119 -4.1537323 -4.1357632 -4.1220145 -4.1019368 -4.085042 -4.0888948][-4.2658234 -4.2681408 -4.2530484 -4.22876 -4.203311 -4.1772346 -4.1582127 -4.1482058 -4.1523275 -4.1546721 -4.150084 -4.1324286 -4.0993471 -4.0774693 -4.0863514][-4.2615705 -4.2612853 -4.2428503 -4.2152066 -4.1878133 -4.1590161 -4.1335988 -4.1180224 -4.1292148 -4.1485448 -4.156868 -4.1450291 -4.1123209 -4.0928726 -4.1028976][-4.2598763 -4.2590175 -4.2394857 -4.2094717 -4.1829429 -4.15041 -4.1144185 -4.0901523 -4.1035652 -4.1365857 -4.1611419 -4.161 -4.1407728 -4.1269765 -4.1322937][-4.2547355 -4.2528272 -4.2340384 -4.2031465 -4.1750822 -4.1351314 -4.0817804 -4.0417237 -4.0628662 -4.12076 -4.1687274 -4.1834569 -4.1747556 -4.1698713 -4.1710215][-4.2461104 -4.2431569 -4.22657 -4.195467 -4.1591215 -4.1051354 -4.0263348 -3.9659946 -4.0115609 -4.1096187 -4.179636 -4.2058449 -4.2112446 -4.2138419 -4.2159481][-4.2413836 -4.2370663 -4.2208614 -4.1888981 -4.145278 -4.0792837 -3.9838569 -3.915561 -3.9942806 -4.1146297 -4.18919 -4.2205091 -4.2365723 -4.2493148 -4.253428][-4.2320051 -4.2244883 -4.2075677 -4.1783695 -4.1398315 -4.0819783 -4.0095692 -3.9720833 -4.04685 -4.14264 -4.1946435 -4.2214832 -4.2421331 -4.2575445 -4.2641816][-4.2157669 -4.2038927 -4.1860752 -4.1653223 -4.1413927 -4.1086273 -4.0724735 -4.0650477 -4.1154985 -4.1692982 -4.1969762 -4.215373 -4.2321572 -4.2463322 -4.2539015][-4.1998997 -4.1868892 -4.1703348 -4.1567888 -4.146163 -4.1337328 -4.1253047 -4.133007 -4.1628304 -4.1890173 -4.2042656 -4.2137432 -4.2216754 -4.2302995 -4.2323079][-4.18762 -4.1793032 -4.1711512 -4.1631122 -4.1572695 -4.1550736 -4.1596842 -4.1654153 -4.1802936 -4.19575 -4.2087927 -4.2151022 -4.2117338 -4.2105618 -4.2064619][-4.1854191 -4.1847639 -4.1828413 -4.1746607 -4.1666603 -4.1666336 -4.1743469 -4.1746373 -4.180366 -4.1938825 -4.2134562 -4.2198811 -4.2099104 -4.1992226 -4.1879396][-4.1918721 -4.19466 -4.1929774 -4.1806221 -4.1723518 -4.1730118 -4.1796732 -4.1794019 -4.1858435 -4.2011228 -4.22432 -4.2294741 -4.2165184 -4.2013569 -4.1840687]]...]
INFO - root - 2017-12-07 17:25:32.431915: step 15810, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 66h:25m:55s remains)
INFO - root - 2017-12-07 17:25:48.810733: step 15820, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.550 sec/batch; 64h:46m:20s remains)
INFO - root - 2017-12-07 17:26:05.236095: step 15830, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.743 sec/batch; 72h:50m:00s remains)
INFO - root - 2017-12-07 17:26:21.299662: step 15840, loss = 2.08, batch loss = 2.03 (10.5 examples/sec; 1.522 sec/batch; 63h:35m:38s remains)
INFO - root - 2017-12-07 17:26:37.556705: step 15850, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.620 sec/batch; 67h:41m:36s remains)
INFO - root - 2017-12-07 17:26:53.773052: step 15860, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.597 sec/batch; 66h:43m:53s remains)
INFO - root - 2017-12-07 17:27:10.266500: step 15870, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.721 sec/batch; 71h:52m:45s remains)
INFO - root - 2017-12-07 17:27:26.474839: step 15880, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.598 sec/batch; 66h:45m:56s remains)
INFO - root - 2017-12-07 17:27:42.564815: step 15890, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.631 sec/batch; 68h:07m:16s remains)
INFO - root - 2017-12-07 17:27:58.514857: step 15900, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.537 sec/batch; 64h:10m:27s remains)
2017-12-07 17:27:59.968749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3100753 -4.3058157 -4.2976418 -4.2893906 -4.27677 -4.2685056 -4.274116 -4.2790809 -4.2787433 -4.2721047 -4.268651 -4.2664413 -4.2670488 -4.2725554 -4.2783933][-4.2931027 -4.2910438 -4.2866154 -4.2783589 -4.2567768 -4.2385979 -4.2366939 -4.2431469 -4.2488861 -4.243557 -4.2387481 -4.2396388 -4.2442265 -4.2542558 -4.2625265][-4.26128 -4.2604542 -4.2611356 -4.2547774 -4.2278066 -4.2009168 -4.1933751 -4.2020779 -4.2176423 -4.216877 -4.2095003 -4.2089095 -4.2144055 -4.2271395 -4.2341194][-4.213553 -4.2135167 -4.2207584 -4.2172737 -4.1866646 -4.1542449 -4.1402383 -4.1527696 -4.1794491 -4.1865177 -4.1803651 -4.1800866 -4.1853204 -4.1971269 -4.199729][-4.1580687 -4.1573195 -4.1729226 -4.1769171 -4.1424274 -4.0961809 -4.0651712 -4.0784411 -4.1213169 -4.1485186 -4.1531157 -4.1597176 -4.1679 -4.179575 -4.1812735][-4.1114192 -4.1095567 -4.1357007 -4.1455073 -4.106956 -4.0411448 -3.9853587 -3.9973724 -4.0637836 -4.1168656 -4.1332436 -4.1464472 -4.1590905 -4.1711864 -4.1726484][-4.1098623 -4.1026363 -4.1289611 -4.1393108 -4.1030545 -4.0314674 -3.9643793 -3.97105 -4.04291 -4.1066408 -4.1278162 -4.1369362 -4.1470089 -4.1571259 -4.1588912][-4.1298618 -4.1176224 -4.1347551 -4.1415415 -4.1151018 -4.0583744 -4.0047593 -4.0070095 -4.061018 -4.1166596 -4.1364818 -4.1405835 -4.1424775 -4.1447215 -4.1403956][-4.1535711 -4.1398425 -4.1451988 -4.147182 -4.1308689 -4.09381 -4.0633678 -4.0639005 -4.0965428 -4.1427836 -4.1665068 -4.1737533 -4.1770215 -4.1754656 -4.1641879][-4.1909986 -4.1791711 -4.1723394 -4.162189 -4.1480112 -4.123364 -4.1065912 -4.1078262 -4.1283917 -4.1659374 -4.19231 -4.2046895 -4.2106638 -4.2110782 -4.201324][-4.2228508 -4.2163634 -4.2071109 -4.1882267 -4.1697688 -4.1503043 -4.1402173 -4.1418619 -4.1502266 -4.1744018 -4.1993232 -4.2181587 -4.2261567 -4.2266974 -4.21673][-4.2232237 -4.2290974 -4.22806 -4.21091 -4.1918092 -4.1746831 -4.1677895 -4.1658731 -4.1630173 -4.1728487 -4.193799 -4.2153096 -4.2246537 -4.2233925 -4.2123594][-4.2011638 -4.21795 -4.227757 -4.2185817 -4.2054424 -4.1953821 -4.1903086 -4.1840158 -4.172029 -4.1694307 -4.18432 -4.2069678 -4.2176962 -4.2143974 -4.2026644][-4.1869454 -4.2046604 -4.2190251 -4.2172179 -4.2101259 -4.2063904 -4.1999164 -4.1906824 -4.1758351 -4.1680527 -4.1802883 -4.2014966 -4.21174 -4.2110319 -4.2054229][-4.2030358 -4.2159672 -4.2246709 -4.2226896 -4.2172294 -4.2169 -4.2121754 -4.2006512 -4.1849613 -4.1744833 -4.1825986 -4.2018094 -4.2135763 -4.2175512 -4.21765]]...]
INFO - root - 2017-12-07 17:28:16.273355: step 15910, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.664 sec/batch; 69h:29m:32s remains)
INFO - root - 2017-12-07 17:28:32.624296: step 15920, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.652 sec/batch; 68h:58m:48s remains)
INFO - root - 2017-12-07 17:28:48.972431: step 15930, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.565 sec/batch; 65h:19m:49s remains)
INFO - root - 2017-12-07 17:29:05.380288: step 15940, loss = 2.06, batch loss = 2.01 (10.3 examples/sec; 1.553 sec/batch; 64h:49m:50s remains)
INFO - root - 2017-12-07 17:29:21.521226: step 15950, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.729 sec/batch; 72h:11m:21s remains)
INFO - root - 2017-12-07 17:29:37.735579: step 15960, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.581 sec/batch; 65h:59m:49s remains)
INFO - root - 2017-12-07 17:29:53.974099: step 15970, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.655 sec/batch; 69h:04m:06s remains)
INFO - root - 2017-12-07 17:30:10.037748: step 15980, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.572 sec/batch; 65h:36m:04s remains)
INFO - root - 2017-12-07 17:30:26.400487: step 15990, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.743 sec/batch; 72h:44m:16s remains)
INFO - root - 2017-12-07 17:30:42.669991: step 16000, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.628 sec/batch; 67h:57m:31s remains)
2017-12-07 17:30:44.079906: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2604809 -4.27079 -4.2642388 -4.2439022 -4.2112813 -4.1921043 -4.194427 -4.211585 -4.2404947 -4.27108 -4.2973361 -4.3030877 -4.2881203 -4.2859755 -4.2833343][-4.265378 -4.2750683 -4.2742114 -4.2594256 -4.2266774 -4.2026658 -4.20409 -4.2199183 -4.2457662 -4.2751164 -4.3043194 -4.3138151 -4.2988915 -4.2911315 -4.2814565][-4.2581396 -4.2704186 -4.2778277 -4.27265 -4.24759 -4.22748 -4.2331538 -4.2483673 -4.2688656 -4.2930341 -4.3159137 -4.3216023 -4.3053942 -4.293726 -4.2791109][-4.2356033 -4.2504797 -4.2669778 -4.2747517 -4.2645206 -4.2545509 -4.2632809 -4.2734451 -4.2875972 -4.3076968 -4.3255858 -4.3277006 -4.312573 -4.3013754 -4.2894983][-4.2140207 -4.227221 -4.2492738 -4.2664237 -4.2674479 -4.2633295 -4.2675562 -4.2710152 -4.2805772 -4.3012419 -4.3198509 -4.3260989 -4.3218012 -4.316484 -4.3106737][-4.2053595 -4.2086787 -4.2264814 -4.2449741 -4.2520571 -4.2449284 -4.2341413 -4.2267318 -4.2346826 -4.2610655 -4.29009 -4.3105531 -4.3257155 -4.330924 -4.3287177][-4.2171588 -4.2089615 -4.2182927 -4.2302532 -4.2271304 -4.1994605 -4.1608534 -4.1434259 -4.1628613 -4.2082868 -4.25431 -4.2914257 -4.3223915 -4.3367243 -4.3366075][-4.2340345 -4.2186522 -4.2193074 -4.22008 -4.1996336 -4.1400137 -4.0602818 -4.0327506 -4.0782986 -4.1558867 -4.2280011 -4.28143 -4.3171506 -4.3350763 -4.3330874][-4.2445621 -4.2304425 -4.2257304 -4.2178783 -4.1828103 -4.0950122 -3.971828 -3.926929 -3.9993134 -4.1112962 -4.209137 -4.2765374 -4.3139391 -4.3311095 -4.322268][-4.246995 -4.2316704 -4.2240438 -4.2145071 -4.183136 -4.1025891 -3.9794869 -3.924655 -3.9932761 -4.1069484 -4.2089291 -4.2793527 -4.3124785 -4.3229756 -4.3077617][-4.2394409 -4.2225609 -4.208147 -4.1961613 -4.18341 -4.1465659 -4.0754309 -4.0368671 -4.0748339 -4.1518598 -4.2371378 -4.2961931 -4.3176913 -4.3162007 -4.2968712][-4.2223625 -4.1972241 -4.1686807 -4.1466012 -4.1514697 -4.1658325 -4.156877 -4.1478248 -4.1672845 -4.2091656 -4.2739406 -4.322176 -4.333343 -4.3228626 -4.30013][-4.1919074 -4.1603251 -4.12229 -4.0883188 -4.1003423 -4.1508036 -4.1921086 -4.2120595 -4.2283435 -4.2496457 -4.2944336 -4.3339915 -4.3412566 -4.3289561 -4.3126068][-4.1856461 -4.1580534 -4.1266117 -4.0906758 -4.0961146 -4.148531 -4.207098 -4.2438931 -4.2602611 -4.2683754 -4.2936916 -4.3243032 -4.33171 -4.323853 -4.3142748][-4.2097573 -4.1893215 -4.1707563 -4.1450496 -4.146915 -4.1853971 -4.233355 -4.2684188 -4.2835655 -4.2827497 -4.2905169 -4.3081174 -4.3150907 -4.31083 -4.3057342]]...]
INFO - root - 2017-12-07 17:31:00.120090: step 16010, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.543 sec/batch; 64h:24m:31s remains)
INFO - root - 2017-12-07 17:31:16.325361: step 16020, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.613 sec/batch; 67h:19m:53s remains)
INFO - root - 2017-12-07 17:31:32.643146: step 16030, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 1.731 sec/batch; 72h:14m:46s remains)
INFO - root - 2017-12-07 17:31:48.906850: step 16040, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.612 sec/batch; 67h:14m:33s remains)
INFO - root - 2017-12-07 17:32:05.291414: step 16050, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 1.771 sec/batch; 73h:54m:10s remains)
INFO - root - 2017-12-07 17:32:21.500930: step 16060, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.542 sec/batch; 64h:20m:31s remains)
INFO - root - 2017-12-07 17:32:37.800880: step 16070, loss = 2.06, batch loss = 2.01 (10.2 examples/sec; 1.573 sec/batch; 65h:35m:58s remains)
INFO - root - 2017-12-07 17:32:54.071615: step 16080, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.685 sec/batch; 70h:17m:22s remains)
INFO - root - 2017-12-07 17:33:10.380867: step 16090, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.584 sec/batch; 66h:05m:17s remains)
INFO - root - 2017-12-07 17:33:26.627699: step 16100, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.683 sec/batch; 70h:12m:33s remains)
2017-12-07 17:33:28.009139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2546391 -4.2548409 -4.2647972 -4.2748079 -4.2796683 -4.2839646 -4.2854357 -4.2938728 -4.305522 -4.3084517 -4.3051648 -4.2983289 -4.2949119 -4.2871094 -4.2891078][-4.2442913 -4.2466216 -4.2646761 -4.2789979 -4.2832608 -4.2812214 -4.2755418 -4.2859726 -4.3068442 -4.3148503 -4.3089018 -4.2986922 -4.2908783 -4.278892 -4.2834005][-4.2376676 -4.2401147 -4.2611184 -4.2766242 -4.2778044 -4.2663212 -4.2513514 -4.2638559 -4.2964005 -4.3144879 -4.31055 -4.2964725 -4.2790341 -4.2644997 -4.271203][-4.2071409 -4.206356 -4.2233768 -4.2416749 -4.2452836 -4.2270136 -4.2039037 -4.2217402 -4.2661438 -4.2966805 -4.2994967 -4.2859569 -4.265377 -4.2485938 -4.2506022][-4.1469994 -4.1458869 -4.1611214 -4.1886883 -4.200799 -4.1806097 -4.1484141 -4.165699 -4.2205844 -4.2654448 -4.2766147 -4.2690897 -4.2532291 -4.2353306 -4.2310214][-4.0996614 -4.0986814 -4.11569 -4.1499863 -4.1680312 -4.1471949 -4.0981622 -4.0944114 -4.1625657 -4.229713 -4.2573628 -4.2628751 -4.2574139 -4.2431335 -4.2306356][-4.0709357 -4.0754528 -4.0991797 -4.1307135 -4.1420875 -4.1129155 -4.0310163 -3.9743774 -4.0587406 -4.1711345 -4.2319355 -4.2567787 -4.2631803 -4.2533436 -4.2339797][-4.1036 -4.1058903 -4.1255383 -4.144011 -4.1395783 -4.0893826 -3.968354 -3.8484168 -3.9383996 -4.0956631 -4.1910505 -4.2381806 -4.2518854 -4.2425456 -4.2190332][-4.1813946 -4.17083 -4.1770973 -4.1841145 -4.1723452 -4.126308 -4.0206943 -3.9055908 -3.9617784 -4.0959959 -4.18721 -4.2369189 -4.2497025 -4.2312369 -4.1973863][-4.241293 -4.2228222 -4.2190485 -4.224143 -4.217761 -4.1929483 -4.1305485 -4.0576458 -4.0761619 -4.1504011 -4.2083893 -4.2411127 -4.2470403 -4.2165818 -4.16801][-4.2892056 -4.2691321 -4.2610683 -4.265152 -4.2656455 -4.2561049 -4.2269559 -4.187747 -4.1848016 -4.2083797 -4.2341142 -4.2502251 -4.2490072 -4.21496 -4.1622648][-4.3175106 -4.3025317 -4.295897 -4.2980752 -4.3024306 -4.2995024 -4.2883711 -4.2721691 -4.2629452 -4.2622433 -4.2710414 -4.2770658 -4.2686377 -4.2363849 -4.1896691][-4.3196039 -4.316772 -4.3113379 -4.3086686 -4.3120465 -4.3115211 -4.30959 -4.3035574 -4.2982407 -4.2970533 -4.3020329 -4.300653 -4.2882915 -4.2628088 -4.2308083][-4.2941623 -4.3028183 -4.3005733 -4.2940011 -4.2910843 -4.2894564 -4.287878 -4.2844071 -4.2868257 -4.2946687 -4.3003464 -4.2957392 -4.2844558 -4.268065 -4.2486916][-4.2571797 -4.270575 -4.2711892 -4.2641315 -4.2560706 -4.24869 -4.2402544 -4.2371874 -4.2497091 -4.2663627 -4.2714114 -4.2667704 -4.2602124 -4.2528491 -4.2445717]]...]
INFO - root - 2017-12-07 17:33:44.314694: step 16110, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.658 sec/batch; 69h:09m:22s remains)
INFO - root - 2017-12-07 17:34:00.465705: step 16120, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 65h:36m:21s remains)
INFO - root - 2017-12-07 17:34:16.847855: step 16130, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.680 sec/batch; 70h:02m:44s remains)
INFO - root - 2017-12-07 17:34:32.993806: step 16140, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.588 sec/batch; 66h:12m:54s remains)
INFO - root - 2017-12-07 17:34:49.308629: step 16150, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.720 sec/batch; 71h:43m:39s remains)
INFO - root - 2017-12-07 17:35:05.525935: step 16160, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.654 sec/batch; 68h:58m:08s remains)
INFO - root - 2017-12-07 17:35:21.892160: step 16170, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.714 sec/batch; 71h:26m:44s remains)
INFO - root - 2017-12-07 17:35:38.059528: step 16180, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.602 sec/batch; 66h:46m:10s remains)
INFO - root - 2017-12-07 17:35:54.381722: step 16190, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.597 sec/batch; 66h:34m:56s remains)
INFO - root - 2017-12-07 17:36:10.569688: step 16200, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 1.745 sec/batch; 72h:44m:42s remains)
2017-12-07 17:36:12.032989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1898537 -4.19142 -4.1680503 -4.1341906 -4.1159554 -4.1206079 -4.14166 -4.1763263 -4.2113814 -4.2345314 -4.2416377 -4.230557 -4.224144 -4.228034 -4.2320061][-4.2140608 -4.2210369 -4.2008595 -4.1684632 -4.150085 -4.1492481 -4.1597505 -4.1852007 -4.2117982 -4.2291141 -4.2338643 -4.2244978 -4.2202692 -4.2257824 -4.2306705][-4.2316961 -4.2412157 -4.2300658 -4.2117395 -4.2027221 -4.1983604 -4.1955075 -4.205142 -4.216095 -4.2235775 -4.2239037 -4.2136812 -4.2098794 -4.2182603 -4.2265987][-4.2380896 -4.2452321 -4.2394819 -4.234724 -4.23784 -4.2356663 -4.2256923 -4.2226405 -4.219357 -4.2151933 -4.2106409 -4.2000108 -4.1964221 -4.2051086 -4.2165432][-4.2362065 -4.2415895 -4.2361407 -4.2325959 -4.2378531 -4.2371049 -4.22841 -4.2195153 -4.2063289 -4.1919308 -4.1809855 -4.1721921 -4.1725526 -4.1849222 -4.1997943][-4.2237039 -4.231216 -4.2252107 -4.2161107 -4.21781 -4.2150879 -4.2104526 -4.2026463 -4.1847181 -4.1655164 -4.1487136 -4.1379795 -4.1412907 -4.1589241 -4.1781764][-4.2124496 -4.2263274 -4.2191277 -4.2003565 -4.1949077 -4.1860542 -4.1781573 -4.1690154 -4.1489725 -4.1280804 -4.1108961 -4.0993986 -4.1063027 -4.1300087 -4.1551838][-4.1975389 -4.2177868 -4.2111573 -4.1839457 -4.1622405 -4.1381421 -4.1205831 -4.1071892 -4.0824065 -4.0642676 -4.054419 -4.0502529 -4.0685697 -4.1010723 -4.1342969][-4.1912031 -4.2107887 -4.2022972 -4.169456 -4.1366782 -4.10003 -4.0745234 -4.0578446 -4.0299258 -4.0170579 -4.0205398 -4.032846 -4.0675182 -4.1029081 -4.1352925][-4.2108665 -4.2276607 -4.2185345 -4.1905222 -4.1589603 -4.1198149 -4.0962667 -4.0875869 -4.0669813 -4.0659623 -4.082397 -4.1020603 -4.1312151 -4.1471839 -4.1620812][-4.2429781 -4.2529268 -4.2440214 -4.2230992 -4.19833 -4.1660032 -4.1534238 -4.1556931 -4.1480646 -4.1516414 -4.1695156 -4.1857924 -4.2028837 -4.1979246 -4.1956391][-4.2619 -4.2642136 -4.2564831 -4.2431507 -4.2269282 -4.205955 -4.2020378 -4.2112746 -4.2099848 -4.2117858 -4.2220616 -4.231565 -4.2397571 -4.2277827 -4.2185683][-4.270936 -4.2694206 -4.262042 -4.2551155 -4.2476597 -4.2366457 -4.2352958 -4.2439933 -4.245903 -4.2464066 -4.2480254 -4.247437 -4.2502007 -4.237813 -4.2288289][-4.2715282 -4.2689548 -4.2643914 -4.2610226 -4.2561603 -4.2502565 -4.2526083 -4.2625213 -4.2670302 -4.2674217 -4.2604032 -4.2477589 -4.2441726 -4.2335987 -4.2281265][-4.2517252 -4.2500238 -4.2488589 -4.2480989 -4.2445197 -4.2450509 -4.2553935 -4.268425 -4.2753439 -4.2770381 -4.2666135 -4.2475734 -4.2343984 -4.2213521 -4.2144508]]...]
INFO - root - 2017-12-07 17:36:28.352952: step 16210, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.725 sec/batch; 71h:54m:40s remains)
INFO - root - 2017-12-07 17:36:44.739651: step 16220, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.641 sec/batch; 68h:24m:05s remains)
INFO - root - 2017-12-07 17:37:01.184226: step 16230, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.692 sec/batch; 70h:30m:17s remains)
INFO - root - 2017-12-07 17:37:17.342019: step 16240, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.562 sec/batch; 65h:05m:42s remains)
INFO - root - 2017-12-07 17:37:33.629904: step 16250, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.693 sec/batch; 70h:33m:18s remains)
INFO - root - 2017-12-07 17:37:49.728455: step 16260, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.556 sec/batch; 64h:49m:51s remains)
INFO - root - 2017-12-07 17:38:06.135565: step 16270, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.622 sec/batch; 67h:33m:32s remains)
INFO - root - 2017-12-07 17:38:22.424084: step 16280, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.697 sec/batch; 70h:42m:06s remains)
INFO - root - 2017-12-07 17:38:38.783704: step 16290, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.596 sec/batch; 66h:27m:44s remains)
INFO - root - 2017-12-07 17:38:54.713038: step 16300, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.577 sec/batch; 65h:41m:38s remains)
2017-12-07 17:38:56.074282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2855253 -4.2765026 -4.26929 -4.2656589 -4.262568 -4.2521529 -4.2368531 -4.214766 -4.1954947 -4.1815686 -4.1786385 -4.1937103 -4.1935382 -4.16445 -4.1504073][-4.274178 -4.2629356 -4.2536349 -4.247715 -4.2420917 -4.2285852 -4.212224 -4.1858673 -4.1642227 -4.1537476 -4.1568928 -4.1746507 -4.1724324 -4.136857 -4.1207995][-4.2603149 -4.2495551 -4.2418318 -4.2377605 -4.230423 -4.2118373 -4.1888409 -4.1592259 -4.1391206 -4.1362538 -4.149632 -4.1703863 -4.1649055 -4.1287417 -4.1146784][-4.2291465 -4.2203121 -4.2220206 -4.2279587 -4.2246566 -4.2020669 -4.1692729 -4.135735 -4.1233888 -4.1348505 -4.1571088 -4.1747546 -4.1651459 -4.1331806 -4.1238742][-4.1772609 -4.1723375 -4.1868129 -4.2066469 -4.2128563 -4.18895 -4.1490321 -4.1142516 -4.1130219 -4.1418772 -4.1699944 -4.1803288 -4.1645975 -4.1370463 -4.1338081][-4.129591 -4.1298027 -4.1546593 -4.1827874 -4.1939898 -4.1683383 -4.1209469 -4.08083 -4.0855622 -4.13434 -4.17347 -4.1792836 -4.156414 -4.1313615 -4.1331911][-4.1006746 -4.1080832 -4.1448689 -4.1764064 -4.1783385 -4.1363339 -4.0706716 -4.0085287 -4.0119362 -4.0925722 -4.1556449 -4.1653771 -4.1386256 -4.1189318 -4.1292295][-4.0736961 -4.0961504 -4.1524563 -4.18854 -4.1722493 -4.1031709 -4.0074553 -3.9026837 -3.8931923 -4.0158863 -4.1094136 -4.1277823 -4.1079092 -4.1011443 -4.1209445][-4.0318646 -4.0711522 -4.1459827 -4.18178 -4.1477842 -4.0603213 -3.947258 -3.8227963 -3.8086836 -3.9547606 -4.0639648 -4.0859017 -4.073554 -4.0805116 -4.1085367][-3.9998462 -4.0460873 -4.1253247 -4.1612692 -4.1264825 -4.0531797 -3.9624372 -3.8684721 -3.8561149 -3.9680743 -4.0569291 -4.06811 -4.0530553 -4.06668 -4.0962992][-4.0451536 -4.08779 -4.1491823 -4.1752691 -4.1451969 -4.0945678 -4.0392036 -3.9844928 -3.9732625 -4.03363 -4.0828757 -4.0749083 -4.0534358 -4.0647707 -4.0892377][-4.1166315 -4.1482577 -4.18488 -4.1952052 -4.1694188 -4.1364388 -4.1065478 -4.0760455 -4.0664039 -4.0961976 -4.116827 -4.1022711 -4.0852103 -4.0926766 -4.1102262][-4.1665707 -4.1867647 -4.2032342 -4.2052274 -4.1878929 -4.1682253 -4.1518497 -4.1358924 -4.1297736 -4.1435289 -4.1505837 -4.1377239 -4.1264915 -4.1283889 -4.1406956][-4.2109675 -4.2174797 -4.2201014 -4.2141705 -4.2017317 -4.191237 -4.1818838 -4.1737 -4.1700654 -4.1745658 -4.1761942 -4.1685505 -4.158649 -4.155766 -4.1649895][-4.2514596 -4.2456512 -4.23901 -4.229064 -4.2177234 -4.2115498 -4.2075372 -4.2049289 -4.2041965 -4.2070718 -4.2088728 -4.2083521 -4.2010179 -4.1930008 -4.1972685]]...]
INFO - root - 2017-12-07 17:39:12.388984: step 16310, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.681 sec/batch; 70h:00m:54s remains)
INFO - root - 2017-12-07 17:39:28.467937: step 16320, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.530 sec/batch; 63h:43m:58s remains)
INFO - root - 2017-12-07 17:39:44.692087: step 16330, loss = 2.09, batch loss = 2.04 (10.2 examples/sec; 1.564 sec/batch; 65h:08m:20s remains)
INFO - root - 2017-12-07 17:40:00.982668: step 16340, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 1.744 sec/batch; 72h:36m:20s remains)
INFO - root - 2017-12-07 17:40:17.041850: step 16350, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 1.473 sec/batch; 61h:20m:50s remains)
INFO - root - 2017-12-07 17:40:33.185309: step 16360, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.618 sec/batch; 67h:21m:07s remains)
INFO - root - 2017-12-07 17:40:49.539960: step 16370, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.572 sec/batch; 65h:27m:15s remains)
INFO - root - 2017-12-07 17:41:05.657170: step 16380, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 1.658 sec/batch; 69h:01m:08s remains)
INFO - root - 2017-12-07 17:41:21.782624: step 16390, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.561 sec/batch; 64h:58m:15s remains)
INFO - root - 2017-12-07 17:41:38.161802: step 16400, loss = 2.09, batch loss = 2.04 (10.0 examples/sec; 1.596 sec/batch; 66h:25m:24s remains)
2017-12-07 17:41:39.505784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2830744 -4.2971144 -4.302084 -4.3060465 -4.3050542 -4.2972565 -4.2839017 -4.2701592 -4.2597742 -4.255476 -4.2575936 -4.2657838 -4.282763 -4.3060174 -4.3233662][-4.3084888 -4.3264647 -4.3338003 -4.3340445 -4.3269567 -4.3145828 -4.2961383 -4.2785654 -4.2664547 -4.2628593 -4.266036 -4.2762332 -4.2973146 -4.3235006 -4.3417029][-4.3249464 -4.3438129 -4.3493161 -4.3434515 -4.330297 -4.3133173 -4.2913017 -4.2733445 -4.2647295 -4.2660627 -4.2729979 -4.285172 -4.3081141 -4.3355851 -4.3546844][-4.3278465 -4.3429179 -4.3432422 -4.3295703 -4.309813 -4.2881207 -4.2645059 -4.2492847 -4.2462158 -4.2531404 -4.2640038 -4.2777228 -4.3012018 -4.3304658 -4.3536768][-4.3186502 -4.32537 -4.3150778 -4.2910852 -4.2643614 -4.2364774 -4.210012 -4.1987944 -4.203598 -4.2167945 -4.2319789 -4.2478175 -4.2744179 -4.3086123 -4.3394194][-4.3017206 -4.2967696 -4.2691278 -4.2287655 -4.1888914 -4.1499577 -4.1160197 -4.108233 -4.1243763 -4.1474771 -4.1703849 -4.1917911 -4.2241955 -4.2674079 -4.310606][-4.28461 -4.266057 -4.2184463 -4.1569586 -4.0957823 -4.0345645 -3.982367 -3.9719908 -4.0030575 -4.0431213 -4.0822949 -4.118845 -4.1647096 -4.221838 -4.2784557][-4.2793493 -4.2531643 -4.1941805 -4.1176271 -4.0364108 -3.949883 -3.871124 -3.847827 -3.8915498 -3.9528065 -4.0145922 -4.0726252 -4.1337628 -4.2017016 -4.2650833][-4.289237 -4.2693181 -4.2173996 -4.1441731 -4.0617681 -3.9711318 -3.886245 -3.8568513 -3.8967035 -3.960865 -4.0316763 -4.0986342 -4.1620765 -4.2251329 -4.2798343][-4.3027515 -4.3006396 -4.2723832 -4.2204547 -4.1566334 -4.0839052 -4.0163574 -3.9914591 -4.0171051 -4.0647879 -4.1211486 -4.1759496 -4.2254033 -4.2719374 -4.3086181][-4.3051553 -4.3223677 -4.3195224 -4.2950611 -4.2554398 -4.2039509 -4.1534271 -4.1311932 -4.142313 -4.1709714 -4.2068343 -4.2429385 -4.2747378 -4.3039 -4.3244286][-4.2890444 -4.3156443 -4.3324909 -4.3338337 -4.3172011 -4.285934 -4.2505383 -4.2299037 -4.2303524 -4.2453818 -4.2657433 -4.2865257 -4.303966 -4.320806 -4.3310966][-4.2584524 -4.285768 -4.31222 -4.3301311 -4.3308392 -4.3164191 -4.2951026 -4.27927 -4.2743955 -4.2798223 -4.2894578 -4.3003769 -4.3107634 -4.3219986 -4.3272758][-4.2400794 -4.26449 -4.2924037 -4.3171668 -4.3271065 -4.3230009 -4.3113751 -4.300777 -4.294507 -4.2938213 -4.2968268 -4.30151 -4.30761 -4.3160176 -4.3184462][-4.2525558 -4.2696905 -4.2909322 -4.3132939 -4.3261456 -4.3277678 -4.3225031 -4.3160477 -4.310812 -4.3080778 -4.3070788 -4.3073435 -4.3097343 -4.3146539 -4.315062]]...]
INFO - root - 2017-12-07 17:41:55.725197: step 16410, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.640 sec/batch; 68h:16m:51s remains)
INFO - root - 2017-12-07 17:42:11.915586: step 16420, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.587 sec/batch; 66h:03m:05s remains)
INFO - root - 2017-12-07 17:42:28.220142: step 16430, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 1.599 sec/batch; 66h:31m:38s remains)
INFO - root - 2017-12-07 17:42:44.372152: step 16440, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.681 sec/batch; 69h:57m:07s remains)
INFO - root - 2017-12-07 17:43:00.605951: step 16450, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.606 sec/batch; 66h:50m:40s remains)
INFO - root - 2017-12-07 17:43:16.961489: step 16460, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.636 sec/batch; 68h:04m:05s remains)
INFO - root - 2017-12-07 17:43:33.004919: step 16470, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.607 sec/batch; 66h:51m:45s remains)
INFO - root - 2017-12-07 17:43:49.277317: step 16480, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.707 sec/batch; 71h:01m:08s remains)
INFO - root - 2017-12-07 17:44:05.450604: step 16490, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.578 sec/batch; 65h:38m:58s remains)
INFO - root - 2017-12-07 17:44:21.532209: step 16500, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.621 sec/batch; 67h:25m:30s remains)
2017-12-07 17:44:22.898761: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0918632 -4.12818 -4.1522512 -4.1429243 -4.1207695 -4.1175389 -4.128232 -4.155138 -4.1893215 -4.2106218 -4.2198038 -4.2303543 -4.2485008 -4.2625751 -4.2609968][-4.0990252 -4.1333537 -4.1579027 -4.1373754 -4.0928588 -4.0776253 -4.0945482 -4.1335082 -4.1811447 -4.2100005 -4.22258 -4.2298641 -4.2386684 -4.2470174 -4.245194][-4.1203775 -4.1391439 -4.15745 -4.132978 -4.0839381 -4.0667911 -4.0872488 -4.126421 -4.1726642 -4.2060547 -4.2238827 -4.2296567 -4.2328696 -4.2394438 -4.2374105][-4.1455512 -4.1459036 -4.1483951 -4.1185184 -4.0716228 -4.0482693 -4.0570626 -4.0911193 -4.1433539 -4.1858363 -4.2137046 -4.2276216 -4.2370253 -4.2435064 -4.2397823][-4.1429906 -4.1338196 -4.1283503 -4.102529 -4.0594773 -4.0252128 -4.0088792 -4.026546 -4.0860643 -4.1514139 -4.2025247 -4.2356849 -4.2559233 -4.2599249 -4.2527471][-4.0841064 -4.0927076 -4.1009936 -4.086185 -4.0526686 -4.0027514 -3.9477534 -3.9307852 -3.9981275 -4.0991688 -4.1787534 -4.2293358 -4.2629638 -4.2722759 -4.26455][-4.0180745 -4.0606127 -4.0948071 -4.0892644 -4.0561495 -3.9898868 -3.8850369 -3.8120289 -3.8864813 -4.0271597 -4.1340017 -4.2031851 -4.2504325 -4.2687149 -4.2640705][-4.0035796 -4.0569811 -4.1068444 -4.1040297 -4.0688014 -3.9955506 -3.866396 -3.7479873 -3.812469 -3.9697402 -4.0912328 -4.1730132 -4.2342048 -4.2622485 -4.2618728][-4.0236073 -4.0653863 -4.1109653 -4.10921 -4.0790486 -4.0257931 -3.931155 -3.837265 -3.8701091 -3.9852667 -4.0856004 -4.1625109 -4.2262697 -4.2576265 -4.2592473][-4.0685821 -4.0935497 -4.1228719 -4.1231589 -4.1044555 -4.0735536 -4.0189943 -3.9650469 -3.9846113 -4.0517182 -4.118505 -4.1774149 -4.2298865 -4.258996 -4.2619119][-4.1312432 -4.138773 -4.1510406 -4.1506314 -4.14393 -4.12731 -4.0983505 -4.0708838 -4.0897617 -4.1312375 -4.1755447 -4.2205663 -4.2585025 -4.2807474 -4.2827621][-4.1837311 -4.1754193 -4.1778517 -4.1788139 -4.1802096 -4.17467 -4.162818 -4.1532559 -4.1678486 -4.1939554 -4.2276764 -4.2654514 -4.2942629 -4.3077526 -4.3070087][-4.2173 -4.2032537 -4.202992 -4.2108841 -4.2202239 -4.22287 -4.2196712 -4.2148681 -4.2184625 -4.229424 -4.2535887 -4.28594 -4.3101811 -4.3190193 -4.3180914][-4.2363949 -4.2269287 -4.22976 -4.2405453 -4.2523746 -4.2602 -4.262177 -4.2615566 -4.26008 -4.259223 -4.2719932 -4.2974205 -4.3182869 -4.32445 -4.3229575][-4.247138 -4.2483497 -4.2557273 -4.2642479 -4.2715535 -4.2772617 -4.2797194 -4.2820678 -4.2843628 -4.2834258 -4.2912388 -4.3086395 -4.3240542 -4.3284106 -4.3265204]]...]
INFO - root - 2017-12-07 17:44:39.316746: step 16510, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.709 sec/batch; 71h:04m:49s remains)
INFO - root - 2017-12-07 17:44:55.647956: step 16520, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.608 sec/batch; 66h:52m:36s remains)
INFO - root - 2017-12-07 17:45:11.885054: step 16530, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.543 sec/batch; 64h:10m:28s remains)
INFO - root - 2017-12-07 17:45:28.311789: step 16540, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.694 sec/batch; 70h:26m:07s remains)
INFO - root - 2017-12-07 17:45:44.423508: step 16550, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 1.448 sec/batch; 60h:13m:50s remains)
INFO - root - 2017-12-07 17:46:00.879794: step 16560, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.707 sec/batch; 70h:59m:48s remains)
INFO - root - 2017-12-07 17:46:17.078297: step 16570, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.607 sec/batch; 66h:49m:02s remains)
INFO - root - 2017-12-07 17:46:33.398913: step 16580, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.574 sec/batch; 65h:26m:52s remains)
INFO - root - 2017-12-07 17:46:49.698354: step 16590, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.676 sec/batch; 69h:40m:52s remains)
INFO - root - 2017-12-07 17:47:05.965479: step 16600, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.634 sec/batch; 67h:55m:54s remains)
2017-12-07 17:47:07.301841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3549495 -4.355793 -4.3567572 -4.3583393 -4.3591256 -4.3586597 -4.3569775 -4.35643 -4.355844 -4.3555222 -4.3555756 -4.3551512 -4.3543596 -4.3549414 -4.356524][-4.3544865 -4.3513904 -4.3509135 -4.3533044 -4.3557262 -4.3564129 -4.3535442 -4.3536863 -4.3536983 -4.3511829 -4.3503089 -4.3510079 -4.3531165 -4.3563242 -4.3596821][-4.3356786 -4.3279824 -4.3247871 -4.3279915 -4.3338766 -4.3368239 -4.3316064 -4.3279595 -4.3250313 -4.3201809 -4.3204 -4.3253031 -4.33629 -4.3478928 -4.3560543][-4.2838345 -4.271698 -4.2639732 -4.2663436 -4.2778535 -4.2850614 -4.2719941 -4.2559767 -4.2448049 -4.2398787 -4.2458973 -4.2634969 -4.291059 -4.3133516 -4.3296957][-4.2138042 -4.199914 -4.1898661 -4.1879196 -4.19506 -4.1952038 -4.1647415 -4.1289825 -4.1045995 -4.1048164 -4.1220822 -4.1564364 -4.206563 -4.2457881 -4.2763343][-4.1802459 -4.1667705 -4.1547251 -4.1423554 -4.1285992 -4.1059313 -4.0585675 -4.0045714 -3.9674313 -3.9762905 -4.002934 -4.0509 -4.1197348 -4.1774173 -4.2226214][-4.20015 -4.1863933 -4.1676731 -4.1406622 -4.1002274 -4.0590148 -4.0093708 -3.9503531 -3.9096961 -3.9280529 -3.9651189 -4.0255623 -4.1021256 -4.1646214 -4.2088737][-4.2476811 -4.2334304 -4.2103882 -4.1784172 -4.13218 -4.0900769 -4.0508347 -3.9997773 -3.9630041 -3.9839642 -4.0239511 -4.0786772 -4.1426868 -4.1886053 -4.2216825][-4.2900057 -4.2775884 -4.2610025 -4.239006 -4.2048163 -4.1723495 -4.1462774 -4.1073656 -4.0777745 -4.0922923 -4.121624 -4.1600327 -4.2030797 -4.2323089 -4.2558875][-4.3132639 -4.3058887 -4.3009324 -4.2909131 -4.2689953 -4.2470751 -4.2308373 -4.2047949 -4.1883273 -4.2033434 -4.2245908 -4.245018 -4.2687283 -4.2820315 -4.2922258][-4.316463 -4.3130636 -4.3159089 -4.3147283 -4.3029957 -4.2931752 -4.2879038 -4.277318 -4.2742558 -4.2887535 -4.3053236 -4.3138194 -4.3217106 -4.3238225 -4.3252649][-4.3149881 -4.3127394 -4.3178835 -4.3236794 -4.3218722 -4.3211303 -4.3234429 -4.3252273 -4.3307776 -4.3432078 -4.3537593 -4.35673 -4.3583255 -4.3580666 -4.3589654][-4.3207045 -4.3168058 -4.3187051 -4.324326 -4.3264256 -4.3299308 -4.3368011 -4.3454027 -4.3564014 -4.3707318 -4.3803473 -4.3836308 -4.3829288 -4.382587 -4.3832955][-4.33155 -4.3260412 -4.322083 -4.3208842 -4.3196907 -4.3230853 -4.3308973 -4.3429208 -4.3591485 -4.3751988 -4.3861022 -4.3904362 -4.3903546 -4.3876915 -4.3846273][-4.3376031 -4.3317041 -4.3253703 -4.3187451 -4.3129945 -4.3119416 -4.3118033 -4.3185816 -4.3357949 -4.35338 -4.367425 -4.374908 -4.3778529 -4.3762126 -4.3725376]]...]
INFO - root - 2017-12-07 17:47:23.634902: step 16610, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.549 sec/batch; 64h:24m:02s remains)
INFO - root - 2017-12-07 17:47:40.021804: step 16620, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.690 sec/batch; 70h:14m:08s remains)
INFO - root - 2017-12-07 17:47:56.388479: step 16630, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.603 sec/batch; 66h:38m:29s remains)
INFO - root - 2017-12-07 17:48:12.611048: step 16640, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.669 sec/batch; 69h:22m:16s remains)
INFO - root - 2017-12-07 17:48:28.878515: step 16650, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.606 sec/batch; 66h:44m:56s remains)
INFO - root - 2017-12-07 17:48:45.156167: step 16660, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.559 sec/batch; 64h:47m:08s remains)
INFO - root - 2017-12-07 17:49:01.409275: step 16670, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.667 sec/batch; 69h:15m:09s remains)
INFO - root - 2017-12-07 17:49:17.604184: step 16680, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 1.539 sec/batch; 63h:57m:05s remains)
INFO - root - 2017-12-07 17:49:33.975765: step 16690, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.715 sec/batch; 71h:14m:12s remains)
INFO - root - 2017-12-07 17:49:50.178640: step 16700, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.573 sec/batch; 65h:20m:11s remains)
2017-12-07 17:49:51.606359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2744641 -4.2815313 -4.2772937 -4.2590809 -4.2318444 -4.202323 -4.1784415 -4.1745219 -4.16184 -4.12857 -4.1153522 -4.1268406 -4.1381259 -4.1443977 -4.14555][-4.2798362 -4.2895088 -4.2819338 -4.2564187 -4.2208128 -4.1873116 -4.1624608 -4.1542792 -4.1480641 -4.1297231 -4.1195889 -4.1203556 -4.1139464 -4.1081777 -4.112391][-4.2834978 -4.2942533 -4.2838693 -4.2510295 -4.2059197 -4.1644917 -4.1343393 -4.1207113 -4.1232963 -4.1255317 -4.1266885 -4.1212196 -4.0955219 -4.07766 -4.0853453][-4.2830606 -4.2910523 -4.2767363 -4.240088 -4.1864924 -4.1341414 -4.0962992 -4.0805831 -4.0977621 -4.1199727 -4.1336188 -4.12974 -4.0931649 -4.0704427 -4.0799694][-4.2832437 -4.2898884 -4.2759805 -4.240757 -4.1828003 -4.1205978 -4.069593 -4.0520682 -4.0817928 -4.1198316 -4.1405277 -4.1392965 -4.1047235 -4.0806952 -4.0903492][-4.2824793 -4.2880683 -4.2764997 -4.2460985 -4.1859674 -4.1091785 -4.0373445 -4.0170283 -4.064784 -4.1206331 -4.1510282 -4.1545324 -4.1291609 -4.1077189 -4.1119556][-4.2746239 -4.2774949 -4.267169 -4.2375078 -4.170496 -4.0731373 -3.9740579 -3.9486427 -4.0279112 -4.1118841 -4.156786 -4.1666055 -4.1537294 -4.1410832 -4.1445951][-4.2653208 -4.2656622 -4.2547731 -4.219914 -4.1418695 -4.0268059 -3.9012895 -3.8779869 -3.9959393 -4.105547 -4.1600347 -4.1784554 -4.1843653 -4.179965 -4.1837387][-4.2557693 -4.2518859 -4.2379956 -4.2021022 -4.1259179 -4.016664 -3.9052663 -3.9031024 -4.0203137 -4.1175056 -4.1630397 -4.18859 -4.2120447 -4.2192717 -4.2199073][-4.2386956 -4.2288537 -4.21088 -4.1831474 -4.1277246 -4.0514174 -3.986985 -3.999754 -4.0761175 -4.1376314 -4.1695824 -4.1978278 -4.2294722 -4.2468004 -4.2463508][-4.221519 -4.2042117 -4.1821733 -4.1662793 -4.1366115 -4.0954008 -4.06831 -4.0828304 -4.1228428 -4.1587453 -4.1839771 -4.210134 -4.2384772 -4.2579527 -4.253726][-4.2086296 -4.1896706 -4.1651778 -4.1563368 -4.143332 -4.1261625 -4.1205506 -4.1333709 -4.1548486 -4.1795344 -4.2050142 -4.2245746 -4.2410626 -4.2537169 -4.2453136][-4.1989694 -4.1868472 -4.1641459 -4.159318 -4.1565108 -4.1499014 -4.1519251 -4.1579046 -4.171114 -4.1928988 -4.2207141 -4.233274 -4.2381616 -4.2412572 -4.2282534][-4.1901445 -4.1841545 -4.1686292 -4.1693335 -4.1705847 -4.168107 -4.1683993 -4.1754174 -4.1862917 -4.2057071 -4.231432 -4.2365241 -4.2342162 -4.23055 -4.2145205][-4.1826849 -4.1881618 -4.1827593 -4.1807685 -4.1803184 -4.1780133 -4.1764736 -4.18424 -4.1917477 -4.2108688 -4.2311077 -4.2314296 -4.2279711 -4.2239952 -4.2129707]]...]
INFO - root - 2017-12-07 17:50:07.969029: step 16710, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.588 sec/batch; 65h:58m:08s remains)
INFO - root - 2017-12-07 17:50:24.308359: step 16720, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.618 sec/batch; 67h:11m:13s remains)
INFO - root - 2017-12-07 17:50:40.471236: step 16730, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.562 sec/batch; 64h:51m:28s remains)
INFO - root - 2017-12-07 17:50:56.726785: step 16740, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.714 sec/batch; 71h:12m:09s remains)
INFO - root - 2017-12-07 17:51:13.056439: step 16750, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.540 sec/batch; 63h:56m:21s remains)
INFO - root - 2017-12-07 17:51:29.327645: step 16760, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.678 sec/batch; 69h:39m:37s remains)
INFO - root - 2017-12-07 17:51:45.556663: step 16770, loss = 2.05, batch loss = 2.00 (9.9 examples/sec; 1.616 sec/batch; 67h:06m:48s remains)
INFO - root - 2017-12-07 17:52:01.904168: step 16780, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.557 sec/batch; 64h:37m:56s remains)
INFO - root - 2017-12-07 17:52:18.183334: step 16790, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.715 sec/batch; 71h:12m:43s remains)
INFO - root - 2017-12-07 17:52:34.313449: step 16800, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.603 sec/batch; 66h:32m:16s remains)
2017-12-07 17:52:35.604244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2028518 -4.2107553 -4.2265029 -4.2434068 -4.2577648 -4.2704248 -4.279954 -4.2855635 -4.2853742 -4.2786059 -4.2656956 -4.2471604 -4.225594 -4.2096624 -4.2033892][-4.2032285 -4.2104545 -4.2272773 -4.2462292 -4.26219 -4.2752595 -4.2840486 -4.2877774 -4.2847657 -4.2752247 -4.2611146 -4.2444921 -4.2280478 -4.2184377 -4.2174387][-4.2025356 -4.2114739 -4.2302518 -4.249208 -4.2629323 -4.2730923 -4.2796307 -4.2819085 -4.2770119 -4.2656312 -4.2505684 -4.2359152 -4.2247558 -4.2218013 -4.2262144][-4.2008476 -4.212492 -4.2313318 -4.2455006 -4.2511115 -4.2539263 -4.25737 -4.2607641 -4.2582774 -4.2488017 -4.2346983 -4.2226586 -4.2180996 -4.2231722 -4.23394][-4.1997681 -4.212285 -4.2262545 -4.2288842 -4.2197828 -4.2112617 -4.2120528 -4.2207322 -4.2281032 -4.2282248 -4.2206697 -4.2136359 -4.2159591 -4.2284074 -4.2443066][-4.1999159 -4.2091379 -4.2114086 -4.1950765 -4.1656795 -4.1438375 -4.1445408 -4.16498 -4.1908145 -4.2100778 -4.21787 -4.2204595 -4.2280307 -4.2427721 -4.2590852][-4.2050924 -4.2064447 -4.1913376 -4.15152 -4.0996633 -4.0655179 -4.0713091 -4.110642 -4.1606107 -4.2031007 -4.2305036 -4.2446432 -4.2535405 -4.2643442 -4.2757368][-4.2196469 -4.210722 -4.17742 -4.1166377 -4.0475688 -4.0082121 -4.0241776 -4.0823956 -4.1522207 -4.2115006 -4.2528415 -4.2737665 -4.2805743 -4.2849841 -4.2895527][-4.2428269 -4.2249413 -4.1802773 -4.1100111 -4.0367723 -4.0005522 -4.0254507 -4.0920811 -4.167232 -4.2294116 -4.2729807 -4.2938147 -4.2970462 -4.2962317 -4.2956338][-4.2683773 -4.2470593 -4.20227 -4.1378918 -4.0757861 -4.049623 -4.076818 -4.1370177 -4.201623 -4.2530303 -4.2872491 -4.3006239 -4.29863 -4.2946982 -4.2922378][-4.2885547 -4.2681947 -4.2309017 -4.1818752 -4.1389456 -4.1247859 -4.1498652 -4.19721 -4.2443905 -4.2779756 -4.2963967 -4.2989345 -4.2917213 -4.2862782 -4.2844439][-4.2952518 -4.2779918 -4.2502146 -4.2167764 -4.1912317 -4.1867127 -4.208612 -4.2439947 -4.2755122 -4.29346 -4.2987318 -4.2933745 -4.2833538 -4.2782383 -4.2777219][-4.2913566 -4.2776718 -4.2577009 -4.2351651 -4.2205324 -4.221446 -4.2399149 -4.2662306 -4.2871776 -4.2961411 -4.295001 -4.2873611 -4.2783775 -4.2747903 -4.2749276][-4.283874 -4.2728634 -4.2577076 -4.240912 -4.2311893 -4.2338614 -4.2490239 -4.26968 -4.2855058 -4.2914314 -4.289216 -4.2832031 -4.2773981 -4.275579 -4.2759228][-4.2774544 -4.26811 -4.2558656 -4.2425909 -4.2355638 -4.2383556 -4.2503462 -4.2670231 -4.2802196 -4.2855053 -4.2846103 -4.2814193 -4.2786674 -4.2782269 -4.2785835]]...]
INFO - root - 2017-12-07 17:52:52.020136: step 16810, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.566 sec/batch; 64h:59m:27s remains)
INFO - root - 2017-12-07 17:53:08.090416: step 16820, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.661 sec/batch; 68h:56m:31s remains)
INFO - root - 2017-12-07 17:53:24.273169: step 16830, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.534 sec/batch; 63h:39m:27s remains)
INFO - root - 2017-12-07 17:53:40.622209: step 16840, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.667 sec/batch; 69h:12m:03s remains)
INFO - root - 2017-12-07 17:53:56.694720: step 16850, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 65h:31m:56s remains)
INFO - root - 2017-12-07 17:54:12.663772: step 16860, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.569 sec/batch; 65h:06m:22s remains)
INFO - root - 2017-12-07 17:54:29.119648: step 16870, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.688 sec/batch; 70h:01m:36s remains)
INFO - root - 2017-12-07 17:54:45.228685: step 16880, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 1.529 sec/batch; 63h:25m:16s remains)
INFO - root - 2017-12-07 17:55:01.592547: step 16890, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.663 sec/batch; 69h:00m:26s remains)
INFO - root - 2017-12-07 17:55:17.726054: step 16900, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.604 sec/batch; 66h:33m:14s remains)
2017-12-07 17:55:19.129128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2937527 -4.2856283 -4.2729111 -4.263804 -4.2613382 -4.256516 -4.2584648 -4.2681956 -4.2793727 -4.2907171 -4.2939234 -4.2924261 -4.29454 -4.3036013 -4.3120046][-4.252799 -4.2439523 -4.2298875 -4.2177577 -4.2120881 -4.2024302 -4.2031937 -4.21906 -4.2395349 -4.2606044 -4.2686586 -4.2675953 -4.2702594 -4.2806644 -4.2874088][-4.226088 -4.2179317 -4.2047815 -4.1835761 -4.1663427 -4.1494904 -4.1461039 -4.1626596 -4.1942205 -4.2295966 -4.249197 -4.2545929 -4.2583466 -4.2657323 -4.2665377][-4.2110243 -4.2004604 -4.1856189 -4.1565714 -4.1268682 -4.0986729 -4.0844316 -4.0988288 -4.1416321 -4.1951909 -4.232142 -4.247241 -4.2528663 -4.2533321 -4.2467446][-4.1785312 -4.1614909 -4.1465569 -4.1165986 -4.081893 -4.0446825 -4.0156579 -4.0212865 -4.0705681 -4.1372485 -4.189868 -4.21803 -4.2278161 -4.2253556 -4.2157536][-4.1307197 -4.1234422 -4.1222553 -4.1032777 -4.0719137 -4.0285268 -3.9829471 -3.9637883 -3.9992409 -4.0639286 -4.1281261 -4.1698623 -4.1890192 -4.1908612 -4.1853952][-4.0871482 -4.1014519 -4.1185169 -4.1154456 -4.0971627 -4.0577507 -4.0086894 -3.97402 -3.9841619 -4.0284376 -4.0875535 -4.1371164 -4.1658783 -4.1742439 -4.1757016][-4.0774832 -4.100739 -4.1256771 -4.1370883 -4.1323328 -4.0996466 -4.052269 -4.012032 -4.00131 -4.0230303 -4.0694728 -4.1247993 -4.1633439 -4.1808095 -4.1893463][-4.0979848 -4.1142869 -4.135253 -4.1536469 -4.1597128 -4.1360517 -4.0974212 -4.0660648 -4.0494742 -4.0552616 -4.0867338 -4.1410704 -4.181601 -4.2001486 -4.2117653][-4.1139297 -4.1243706 -4.1373959 -4.1568737 -4.1726723 -4.1658382 -4.1409974 -4.1183176 -4.1060729 -4.1061592 -4.1268764 -4.1725125 -4.2056847 -4.2220497 -4.2322807][-4.112885 -4.1255965 -4.1319637 -4.1460795 -4.1652312 -4.168613 -4.1521335 -4.1417608 -4.1399412 -4.1415391 -4.1609282 -4.1997509 -4.2268744 -4.2414746 -4.245481][-4.0973563 -4.1198826 -4.1293435 -4.1422911 -4.1590514 -4.16399 -4.1546884 -4.1563263 -4.1685791 -4.1843104 -4.2103705 -4.2393141 -4.2555094 -4.263442 -4.2581458][-4.0937915 -4.1264772 -4.1480489 -4.164875 -4.1783018 -4.1827583 -4.1798058 -4.1868377 -4.2086167 -4.23815 -4.2665672 -4.2836561 -4.2844415 -4.28232 -4.2705317][-4.1319575 -4.1594462 -4.1827259 -4.1983171 -4.2090011 -4.2132711 -4.2160244 -4.2224159 -4.2449341 -4.2802644 -4.3075967 -4.3132696 -4.3049073 -4.2986703 -4.2857151][-4.18896 -4.2066088 -4.2239995 -4.2349563 -4.2426205 -4.2504272 -4.2553463 -4.2573547 -4.2752118 -4.3067536 -4.3262849 -4.3234572 -4.3126354 -4.306797 -4.2950921]]...]
INFO - root - 2017-12-07 17:55:35.249441: step 16910, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.596 sec/batch; 66h:12m:12s remains)
INFO - root - 2017-12-07 17:55:51.561732: step 16920, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.704 sec/batch; 70h:41m:21s remains)
INFO - root - 2017-12-07 17:56:07.724074: step 16930, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.611 sec/batch; 66h:48m:32s remains)
INFO - root - 2017-12-07 17:56:24.182209: step 16940, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.707 sec/batch; 70h:47m:57s remains)
INFO - root - 2017-12-07 17:56:40.469249: step 16950, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.599 sec/batch; 66h:19m:05s remains)
INFO - root - 2017-12-07 17:56:56.844109: step 16960, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.592 sec/batch; 66h:01m:30s remains)
INFO - root - 2017-12-07 17:57:13.134743: step 16970, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.725 sec/batch; 71h:32m:01s remains)
INFO - root - 2017-12-07 17:57:29.499950: step 16980, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.577 sec/batch; 65h:23m:36s remains)
INFO - root - 2017-12-07 17:57:45.965312: step 16990, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.702 sec/batch; 70h:34m:06s remains)
INFO - root - 2017-12-07 17:58:02.299081: step 17000, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.621 sec/batch; 67h:12m:51s remains)
2017-12-07 17:58:03.673416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2852859 -4.2849722 -4.2875457 -4.2903938 -4.2933979 -4.2966309 -4.2988186 -4.2993832 -4.2979403 -4.2958908 -4.29329 -4.2882247 -4.2788005 -4.2651162 -4.2501307][-4.3190327 -4.3198204 -4.3232679 -4.3267117 -4.3290319 -4.3299732 -4.3287926 -4.3261375 -4.3223825 -4.3188381 -4.3156524 -4.3118329 -4.3045497 -4.2965603 -4.2910461][-4.3432608 -4.345994 -4.3497858 -4.3519635 -4.3500462 -4.3447285 -4.336103 -4.3261995 -4.3176012 -4.3113713 -4.31059 -4.3138771 -4.3160424 -4.3168368 -4.3185339][-4.3432097 -4.3475242 -4.34853 -4.3436303 -4.3321824 -4.3174467 -4.2984457 -4.2805643 -4.2712092 -4.2662096 -4.2695351 -4.281899 -4.2945461 -4.3069172 -4.3180771][-4.3228626 -4.3251758 -4.318222 -4.3025727 -4.2788086 -4.2519875 -4.2196841 -4.1940436 -4.1887264 -4.19264 -4.2035713 -4.2227716 -4.2463603 -4.2713323 -4.2937608][-4.2806759 -4.2782536 -4.2631989 -4.2364211 -4.1981826 -4.1532454 -4.0981789 -4.0590839 -4.0647082 -4.0872793 -4.1126847 -4.1440558 -4.1807337 -4.2181425 -4.2513633][-4.2325039 -4.2210813 -4.1960626 -4.1587281 -4.1077261 -4.04185 -3.9578898 -3.9048045 -3.9323094 -3.9877131 -4.0367403 -4.0856376 -4.136003 -4.1811881 -4.2188725][-4.2043443 -4.1818008 -4.1479478 -4.1071758 -4.0584731 -3.9967737 -3.9222341 -3.8854151 -3.9283428 -3.9937868 -4.0471926 -4.0965667 -4.1446152 -4.1839395 -4.2141104][-4.2211428 -4.1952887 -4.1661558 -4.1406169 -4.1152649 -4.0845885 -4.051403 -4.0420103 -4.072629 -4.1138754 -4.14666 -4.1758623 -4.2041883 -4.2253451 -4.2392564][-4.2631369 -4.2423878 -4.2235584 -4.2122865 -4.2032228 -4.1948719 -4.1901813 -4.1962705 -4.2159939 -4.2399445 -4.256917 -4.2683187 -4.2762647 -4.277554 -4.2745352][-4.2956204 -4.2805815 -4.2703576 -4.2695475 -4.2712789 -4.27431 -4.2800517 -4.287632 -4.29683 -4.3074222 -4.3137708 -4.316956 -4.3162665 -4.3082147 -4.2983818][-4.3148885 -4.3035135 -4.2980561 -4.3037305 -4.3132863 -4.3228259 -4.3301878 -4.3323569 -4.3292851 -4.3252783 -4.321444 -4.3180704 -4.3165879 -4.3095713 -4.3020544][-4.325418 -4.31863 -4.3179431 -4.3254156 -4.3356848 -4.3445725 -4.3483062 -4.342978 -4.3290477 -4.3126459 -4.3000569 -4.293077 -4.2935624 -4.2916517 -4.290679][-4.3293033 -4.3241229 -4.3228192 -4.3247142 -4.3283691 -4.3331103 -4.3348041 -4.325243 -4.3024068 -4.2754636 -4.2573991 -4.2506609 -4.2535772 -4.2569 -4.2629733][-4.3391852 -4.3334284 -4.3243976 -4.3165174 -4.3112893 -4.3105669 -4.3089466 -4.2967033 -4.2699885 -4.2393565 -4.2220373 -4.2190881 -4.2235827 -4.2277722 -4.2321296]]...]
INFO - root - 2017-12-07 17:58:19.747891: step 17010, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.582 sec/batch; 65h:34m:24s remains)
INFO - root - 2017-12-07 17:58:36.067162: step 17020, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.680 sec/batch; 69h:38m:49s remains)
INFO - root - 2017-12-07 17:58:52.194059: step 17030, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.598 sec/batch; 66h:13m:29s remains)
INFO - root - 2017-12-07 17:59:08.605872: step 17040, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.674 sec/batch; 69h:22m:43s remains)
INFO - root - 2017-12-07 17:59:24.622147: step 17050, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.572 sec/batch; 65h:09m:27s remains)
INFO - root - 2017-12-07 17:59:40.978908: step 17060, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.591 sec/batch; 65h:55m:47s remains)
INFO - root - 2017-12-07 17:59:57.513256: step 17070, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.692 sec/batch; 70h:07m:05s remains)
INFO - root - 2017-12-07 18:00:13.735064: step 17080, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.586 sec/batch; 65h:42m:02s remains)
INFO - root - 2017-12-07 18:00:29.950779: step 17090, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.699 sec/batch; 70h:24m:26s remains)
INFO - root - 2017-12-07 18:00:46.126757: step 17100, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.616 sec/batch; 66h:57m:28s remains)
2017-12-07 18:00:47.594942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.212225 -4.2111735 -4.2150755 -4.220027 -4.2251964 -4.232688 -4.2413063 -4.2500482 -4.2575927 -4.2606697 -4.2590375 -4.2505236 -4.238081 -4.2261987 -4.2272916][-4.1891179 -4.1847291 -4.1884418 -4.1944294 -4.2008958 -4.2087421 -4.2190604 -4.2312179 -4.2441564 -4.2526708 -4.253932 -4.2479267 -4.2389536 -4.2281322 -4.2306833][-4.1730156 -4.1653061 -4.1684408 -4.1758862 -4.1851249 -4.1932526 -4.2007003 -4.2099171 -4.2225304 -4.2330208 -4.2343063 -4.2289343 -4.2222614 -4.2133989 -4.2199416][-4.1631885 -4.1526551 -4.1534181 -4.159636 -4.1707773 -4.1797466 -4.181294 -4.1817775 -4.1890984 -4.1981306 -4.1975269 -4.1915708 -4.1867638 -4.1834259 -4.1984873][-4.1591063 -4.1446681 -4.138628 -4.1377091 -4.146318 -4.1566124 -4.1576171 -4.1543255 -4.1575341 -4.1645422 -4.1620836 -4.1557212 -4.1542664 -4.1578565 -4.1810393][-4.1486945 -4.1285067 -4.1151757 -4.1078553 -4.1137986 -4.1270294 -4.1355677 -4.1353192 -4.1358113 -4.1382952 -4.132268 -4.1241579 -4.1249924 -4.1366644 -4.16756][-4.1378307 -4.115109 -4.1013207 -4.0947509 -4.10074 -4.117928 -4.1350131 -4.1378727 -4.1315646 -4.1226792 -4.1093049 -4.0948462 -4.0927496 -4.1098762 -4.1480956][-4.1419311 -4.1230545 -4.1165895 -4.1164579 -4.1226659 -4.1392817 -4.1577415 -4.1579738 -4.1407814 -4.1176991 -4.0943108 -4.0702343 -4.0612082 -4.0814147 -4.1266484][-4.1516924 -4.143218 -4.1492372 -4.1590881 -4.165472 -4.175787 -4.1867204 -4.1803579 -4.1537852 -4.119801 -4.0898771 -4.0618548 -4.0505362 -4.0731912 -4.1221089][-4.1564283 -4.1625719 -4.1824026 -4.2013779 -4.2081509 -4.2093344 -4.2102623 -4.2020535 -4.1792483 -4.1498933 -4.1230974 -4.0980616 -4.0875006 -4.1062679 -4.1472406][-4.1492362 -4.16702 -4.1934056 -4.2162881 -4.2233357 -4.2172875 -4.2120762 -4.2096295 -4.1996222 -4.1795645 -4.16041 -4.14255 -4.1359382 -4.146903 -4.1771045][-4.1261015 -4.1505222 -4.1766043 -4.1984973 -4.2071433 -4.2008233 -4.1944256 -4.1982121 -4.1994262 -4.1880651 -4.1741509 -4.1613913 -4.1561718 -4.1622763 -4.1867285][-4.1056366 -4.1309333 -4.1503296 -4.1652169 -4.1735215 -4.1710987 -4.1692472 -4.17591 -4.18168 -4.1776309 -4.1665258 -4.1556764 -4.1501045 -4.1557627 -4.18114][-4.1003737 -4.1217837 -4.130734 -4.135005 -4.1417236 -4.1481304 -4.1588368 -4.169837 -4.1771588 -4.1757646 -4.1646791 -4.1537709 -4.1459384 -4.15041 -4.1770248][-4.1114917 -4.1308923 -4.132525 -4.1261196 -4.12729 -4.1420293 -4.1661177 -4.1839476 -4.1902351 -4.1861944 -4.1735764 -4.1591377 -4.1480908 -4.1490631 -4.1740069]]...]
INFO - root - 2017-12-07 18:01:03.650000: step 17110, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.599 sec/batch; 66h:14m:07s remains)
INFO - root - 2017-12-07 18:01:19.979890: step 17120, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.666 sec/batch; 69h:00m:32s remains)
INFO - root - 2017-12-07 18:01:36.167347: step 17130, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.620 sec/batch; 67h:06m:23s remains)
INFO - root - 2017-12-07 18:01:52.452501: step 17140, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.601 sec/batch; 66h:19m:58s remains)
INFO - root - 2017-12-07 18:02:08.786455: step 17150, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.691 sec/batch; 70h:01m:39s remains)
INFO - root - 2017-12-07 18:02:24.914588: step 17160, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.536 sec/batch; 63h:36m:17s remains)
INFO - root - 2017-12-07 18:02:41.167866: step 17170, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.728 sec/batch; 71h:33m:05s remains)
INFO - root - 2017-12-07 18:02:57.400183: step 17180, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.547 sec/batch; 64h:04m:06s remains)
INFO - root - 2017-12-07 18:03:13.662376: step 17190, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 1.674 sec/batch; 69h:17m:47s remains)
INFO - root - 2017-12-07 18:03:29.956259: step 17200, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.579 sec/batch; 65h:23m:05s remains)
2017-12-07 18:03:31.417626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2779279 -4.2140937 -4.1486306 -4.1253471 -4.1491985 -4.18687 -4.2082462 -4.2248344 -4.228632 -4.2178607 -4.2263246 -4.2438188 -4.2711239 -4.2903581 -4.29207][-4.2921753 -4.2337217 -4.1714015 -4.1373811 -4.1386585 -4.1607957 -4.1822662 -4.2016215 -4.2089882 -4.2039495 -4.2221589 -4.2478347 -4.2756476 -4.2912388 -4.2883492][-4.3036737 -4.2507005 -4.1930666 -4.1519356 -4.1261439 -4.122436 -4.1463618 -4.1775379 -4.1948829 -4.2006836 -4.2284641 -4.2590003 -4.281136 -4.2896013 -4.2807407][-4.3103065 -4.2608271 -4.2050295 -4.1580963 -4.1039047 -4.0649486 -4.0895391 -4.1437726 -4.1811056 -4.2000756 -4.2364593 -4.2717767 -4.2892914 -4.2901793 -4.279254][-4.31089 -4.2655449 -4.2114558 -4.1586781 -4.076622 -3.992655 -4.0081906 -4.0974126 -4.16202 -4.1948972 -4.2369175 -4.2758713 -4.29123 -4.2906384 -4.2836118][-4.3050795 -4.2671828 -4.2193151 -4.1656461 -4.0630417 -3.9302051 -3.9191303 -4.0462432 -4.1405296 -4.1821675 -4.2239647 -4.2667727 -4.2857394 -4.2896647 -4.2903924][-4.2986636 -4.2693858 -4.2299 -4.1801329 -4.0785666 -3.9265387 -3.8884363 -4.02411 -4.1272659 -4.1658287 -4.2050915 -4.2527041 -4.2791409 -4.2898078 -4.2996478][-4.29536 -4.2756925 -4.2436495 -4.2031374 -4.123549 -3.9976707 -3.9528019 -4.0534325 -4.1335511 -4.1591139 -4.1913047 -4.2409992 -4.2724791 -4.2877779 -4.30392][-4.2861762 -4.2761545 -4.2544823 -4.2271633 -4.1762662 -4.0905666 -4.0534024 -4.1082458 -4.1550426 -4.1678762 -4.1915402 -4.2364988 -4.267417 -4.285059 -4.3052359][-4.2679482 -4.2673421 -4.2603736 -4.2486386 -4.2199044 -4.164053 -4.1351981 -4.1632042 -4.1869855 -4.1933751 -4.208631 -4.2442684 -4.2693429 -4.2853937 -4.3055005][-4.2524781 -4.2563586 -4.2625051 -4.2630944 -4.2468271 -4.2125368 -4.1906624 -4.2068648 -4.22093 -4.2250576 -4.2306442 -4.2572827 -4.2766113 -4.2884846 -4.3035502][-4.2566786 -4.2562237 -4.2660656 -4.2743139 -4.2619233 -4.2435875 -4.2312207 -4.2402787 -4.2451811 -4.2469568 -4.248415 -4.2677422 -4.2826467 -4.288774 -4.2968087][-4.274766 -4.2652383 -4.27074 -4.2762775 -4.26355 -4.2563891 -4.2563667 -4.2619214 -4.258678 -4.2542963 -4.2566829 -4.2714896 -4.2830758 -4.2821741 -4.2821116][-4.2925591 -4.2774291 -4.2738848 -4.2736783 -4.2606568 -4.2615047 -4.270916 -4.2772455 -4.2696776 -4.2617688 -4.2643437 -4.274313 -4.2803626 -4.2730269 -4.2680092][-4.3039808 -4.2899413 -4.2816839 -4.2775984 -4.2676907 -4.2705865 -4.2813759 -4.2855587 -4.2766008 -4.2689128 -4.2731762 -4.2820797 -4.2829361 -4.2721767 -4.2612309]]...]
INFO - root - 2017-12-07 18:03:47.501597: step 17210, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.586 sec/batch; 65h:40m:35s remains)
INFO - root - 2017-12-07 18:04:03.773986: step 17220, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 1.598 sec/batch; 66h:08m:12s remains)
INFO - root - 2017-12-07 18:04:20.092261: step 17230, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.668 sec/batch; 69h:03m:47s remains)
INFO - root - 2017-12-07 18:04:36.604272: step 17240, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.611 sec/batch; 66h:42m:08s remains)
INFO - root - 2017-12-07 18:04:52.965448: step 17250, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.691 sec/batch; 69h:59m:52s remains)
INFO - root - 2017-12-07 18:05:08.935863: step 17260, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.585 sec/batch; 65h:35m:37s remains)
INFO - root - 2017-12-07 18:05:25.161783: step 17270, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 1.717 sec/batch; 71h:03m:50s remains)
INFO - root - 2017-12-07 18:05:41.264570: step 17280, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.618 sec/batch; 66h:56m:42s remains)
INFO - root - 2017-12-07 18:05:57.598997: step 17290, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.653 sec/batch; 68h:24m:30s remains)
INFO - root - 2017-12-07 18:06:13.823939: step 17300, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.641 sec/batch; 67h:53m:59s remains)
2017-12-07 18:06:15.255710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1218352 -4.1209373 -4.1550603 -4.1880217 -4.2181973 -4.249579 -4.262177 -4.2409306 -4.1973491 -4.164753 -4.1473079 -4.1420951 -4.126112 -4.100462 -4.0716577][-4.1257696 -4.1226463 -4.1545134 -4.1825995 -4.20781 -4.2333121 -4.241282 -4.2197089 -4.1708779 -4.13334 -4.1155729 -4.1063094 -4.0867805 -4.0601063 -4.03877][-4.148376 -4.1374674 -4.1561737 -4.177649 -4.1941957 -4.2071176 -4.2074547 -4.1845684 -4.1382709 -4.1062608 -4.0857491 -4.0733428 -4.0522861 -4.0296869 -4.0266676][-4.1513166 -4.1394334 -4.1487818 -4.1672115 -4.1759887 -4.1740384 -4.1646137 -4.1456156 -4.111939 -4.0943394 -4.0786519 -4.061821 -4.0352635 -4.0090113 -4.0170631][-4.1452255 -4.1337152 -4.1399117 -4.1584382 -4.1604342 -4.1380444 -4.1138558 -4.0998263 -4.0857215 -4.0873971 -4.0843935 -4.0680208 -4.0425582 -4.0041881 -4.0048957][-4.1258759 -4.115087 -4.1208105 -4.1450391 -4.1460018 -4.1145148 -4.0844312 -4.0758467 -4.0790949 -4.0911403 -4.0945053 -4.0809879 -4.0607433 -4.0208464 -4.00915][-4.1217623 -4.1115136 -4.1127706 -4.1337838 -4.1331615 -4.104064 -4.0770516 -4.0730271 -4.0874634 -4.1028352 -4.1035194 -4.0946789 -4.0835481 -4.0504088 -4.0322475][-4.1434779 -4.1322722 -4.1291609 -4.1366959 -4.1317081 -4.1101332 -4.0959363 -4.0982761 -4.1161175 -4.1374278 -4.142478 -4.1428547 -4.1466122 -4.1249189 -4.105803][-4.175456 -4.1698403 -4.17401 -4.1685944 -4.1579609 -4.1464667 -4.1473517 -4.1552854 -4.1728077 -4.1953368 -4.2025461 -4.2077174 -4.2203059 -4.2123375 -4.1978641][-4.2162704 -4.2166367 -4.2247643 -4.2161889 -4.2044048 -4.2025642 -4.209764 -4.2196546 -4.2380958 -4.2591567 -4.2670183 -4.277617 -4.2915373 -4.290019 -4.277236][-4.2669883 -4.2701278 -4.278944 -4.2760386 -4.268837 -4.2709737 -4.2747226 -4.2783551 -4.2895722 -4.3039365 -4.311264 -4.3200526 -4.3287473 -4.3253365 -4.3104672][-4.2987509 -4.30144 -4.30456 -4.3060694 -4.3059206 -4.3079047 -4.306726 -4.3025374 -4.3049865 -4.3127193 -4.3169007 -4.3211861 -4.325449 -4.3193183 -4.30398][-4.2942066 -4.29762 -4.2953854 -4.2992105 -4.3038578 -4.3081784 -4.3074102 -4.3001771 -4.2972021 -4.3002353 -4.3018589 -4.3017406 -4.3024893 -4.2951541 -4.2811584][-4.2629266 -4.2669196 -4.2647166 -4.2708645 -4.2801127 -4.2854567 -4.284296 -4.2769461 -4.2738762 -4.278029 -4.2797327 -4.2777514 -4.2767277 -4.2704206 -4.2613535][-4.2345657 -4.2409873 -4.244545 -4.2549405 -4.2668395 -4.2728539 -4.2722726 -4.2674379 -4.2666507 -4.2722049 -4.2750807 -4.2739654 -4.2735977 -4.2696538 -4.26612]]...]
INFO - root - 2017-12-07 18:06:31.468166: step 17310, loss = 2.09, batch loss = 2.04 (10.0 examples/sec; 1.596 sec/batch; 66h:01m:54s remains)
INFO - root - 2017-12-07 18:06:47.804880: step 17320, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.666 sec/batch; 68h:54m:24s remains)
INFO - root - 2017-12-07 18:07:04.016288: step 17330, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 1.625 sec/batch; 67h:12m:44s remains)
INFO - root - 2017-12-07 18:07:20.206807: step 17340, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.588 sec/batch; 65h:40m:57s remains)
INFO - root - 2017-12-07 18:07:36.610948: step 17350, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 1.671 sec/batch; 69h:07m:12s remains)
INFO - root - 2017-12-07 18:07:52.816940: step 17360, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.530 sec/batch; 63h:16m:16s remains)
INFO - root - 2017-12-07 18:08:09.274962: step 17370, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.721 sec/batch; 71h:09m:57s remains)
INFO - root - 2017-12-07 18:08:25.533775: step 17380, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.579 sec/batch; 65h:17m:04s remains)
INFO - root - 2017-12-07 18:08:41.669371: step 17390, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.688 sec/batch; 69h:48m:06s remains)
INFO - root - 2017-12-07 18:08:57.814140: step 17400, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 65h:01m:14s remains)
2017-12-07 18:08:59.258107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3240952 -4.3197241 -4.3224454 -4.3316922 -4.3407741 -4.3451719 -4.3364105 -4.31901 -4.3079414 -4.2947335 -4.2827668 -4.2704563 -4.2545586 -4.2511916 -4.2653122][-4.3325915 -4.3331766 -4.335423 -4.3397164 -4.3425694 -4.3418036 -4.3311257 -4.3126674 -4.3004646 -4.2877855 -4.2805557 -4.2741184 -4.2618527 -4.2586403 -4.2706919][-4.3269539 -4.3320022 -4.3323841 -4.3286204 -4.3210077 -4.3119912 -4.3002124 -4.2830629 -4.2727242 -4.26256 -4.2580028 -4.255208 -4.2465143 -4.2469816 -4.2614136][-4.3113675 -4.3190904 -4.3178377 -4.306572 -4.2880597 -4.269177 -4.2553568 -4.2392087 -4.23075 -4.2227521 -4.2180305 -4.2160759 -4.2129464 -4.2225914 -4.244669][-4.2835984 -4.2889972 -4.2835884 -4.2631745 -4.2326474 -4.2064943 -4.1938934 -4.1819596 -4.177278 -4.1720047 -4.1661506 -4.1643677 -4.168077 -4.190414 -4.224144][-4.241437 -4.2413373 -4.2308822 -4.2019305 -4.1614828 -4.1293488 -4.1177115 -4.1115685 -4.1146584 -4.1160855 -4.1136708 -4.1157084 -4.1289997 -4.1649404 -4.2100163][-4.199327 -4.1929588 -4.1786761 -4.1463876 -4.1014166 -4.0643649 -4.0504375 -4.0475707 -4.0596647 -4.0727725 -4.0832891 -4.0947676 -4.117569 -4.1625314 -4.2141395][-4.158968 -4.1523018 -4.1429 -4.119513 -4.081882 -4.0476165 -4.0303683 -4.0256124 -4.04361 -4.0688848 -4.0932894 -4.1117773 -4.1386504 -4.1839995 -4.2336063][-4.1418686 -4.1384878 -4.1374383 -4.128798 -4.1113577 -4.0937595 -4.0805035 -4.0743876 -4.09513 -4.1244383 -4.1512375 -4.16803 -4.18857 -4.2227931 -4.2605891][-4.1429887 -4.1439519 -4.1492257 -4.1543069 -4.1560473 -4.1560774 -4.1521211 -4.1498108 -4.1699114 -4.19573 -4.2170434 -4.2281771 -4.2392535 -4.2607422 -4.2872047][-4.1462164 -4.1518598 -4.1634378 -4.1794 -4.19365 -4.2039042 -4.2110071 -4.2185173 -4.2385206 -4.2588248 -4.272882 -4.2768779 -4.2779789 -4.2888131 -4.3065286][-4.1578331 -4.1667619 -4.1817656 -4.2039447 -4.2253718 -4.2407384 -4.2522783 -4.2625289 -4.280026 -4.2950139 -4.3041444 -4.3043675 -4.298491 -4.3025441 -4.3139048][-4.1849537 -4.1949692 -4.2076559 -4.2282953 -4.2502704 -4.2668047 -4.2790828 -4.2882118 -4.300838 -4.3098946 -4.3150511 -4.3134871 -4.3038578 -4.3042331 -4.3125348][-4.2223349 -4.2338443 -4.2417011 -4.2558861 -4.2728643 -4.2868748 -4.2942066 -4.2979345 -4.3033266 -4.3071561 -4.3096447 -4.3080964 -4.2981782 -4.2981744 -4.3065748][-4.2541327 -4.2671542 -4.2686477 -4.2724686 -4.2802052 -4.2880421 -4.287159 -4.2807546 -4.2772317 -4.2770505 -4.2821617 -4.2854953 -4.2798715 -4.2826109 -4.2934866]]...]
INFO - root - 2017-12-07 18:09:15.706639: step 17410, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.627 sec/batch; 67h:15m:39s remains)
INFO - root - 2017-12-07 18:09:31.865361: step 17420, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.599 sec/batch; 66h:05m:52s remains)
INFO - root - 2017-12-07 18:09:48.082333: step 17430, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.647 sec/batch; 68h:05m:55s remains)
INFO - root - 2017-12-07 18:10:04.347567: step 17440, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.536 sec/batch; 63h:28m:25s remains)
INFO - root - 2017-12-07 18:10:20.387039: step 17450, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 1.461 sec/batch; 60h:24m:05s remains)
INFO - root - 2017-12-07 18:10:36.622317: step 17460, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.618 sec/batch; 66h:52m:15s remains)
INFO - root - 2017-12-07 18:10:52.838349: step 17470, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.594 sec/batch; 65h:51m:59s remains)
INFO - root - 2017-12-07 18:11:09.097671: step 17480, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 1.732 sec/batch; 71h:33m:18s remains)
INFO - root - 2017-12-07 18:11:25.191134: step 17490, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.582 sec/batch; 65h:22m:19s remains)
INFO - root - 2017-12-07 18:11:41.485114: step 17500, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 1.663 sec/batch; 68h:41m:53s remains)
2017-12-07 18:11:42.963974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2418361 -4.2364111 -4.2350307 -4.2408147 -4.2562623 -4.2773271 -4.2893367 -4.2868948 -4.2813411 -4.2812939 -4.28563 -4.2877731 -4.2873945 -4.286365 -4.2873054][-4.2405949 -4.2348142 -4.2337246 -4.2381907 -4.2496157 -4.2672539 -4.2807207 -4.2831049 -4.2825527 -4.2868543 -4.2934752 -4.2949786 -4.29216 -4.2881856 -4.2875381][-4.2417016 -4.2351031 -4.233048 -4.2345996 -4.239831 -4.2497654 -4.2597227 -4.2637248 -4.267746 -4.2787995 -4.2914176 -4.2962446 -4.2943254 -4.2894611 -4.2872825][-4.2516122 -4.2442527 -4.2371192 -4.2309022 -4.2249885 -4.2217326 -4.2223988 -4.2245936 -4.2341194 -4.2549129 -4.2778592 -4.2912979 -4.2948523 -4.2921476 -4.2894726][-4.2604752 -4.2556219 -4.2444887 -4.2282939 -4.2077932 -4.1893115 -4.1802068 -4.1808672 -4.1960034 -4.2262449 -4.2596884 -4.2831755 -4.2943726 -4.2961278 -4.2938032][-4.2502141 -4.2509713 -4.2409863 -4.2197781 -4.1899166 -4.163888 -4.1529307 -4.1577444 -4.1783619 -4.2126489 -4.2501636 -4.2791009 -4.2949076 -4.2999587 -4.2980976][-4.2181392 -4.2265043 -4.2217941 -4.2022076 -4.1706276 -4.1462307 -4.1413703 -4.1527586 -4.1778617 -4.2120438 -4.2480388 -4.2787008 -4.29603 -4.3023105 -4.3003163][-4.1827836 -4.1998482 -4.2028503 -4.188179 -4.1591754 -4.1387286 -4.138905 -4.1546674 -4.182302 -4.2163963 -4.2503867 -4.2799759 -4.296669 -4.302609 -4.3002834][-4.1642756 -4.1887212 -4.1972723 -4.1869321 -4.1615243 -4.1413832 -4.1399302 -4.155149 -4.1832271 -4.2170768 -4.2508035 -4.2797189 -4.2956996 -4.3010583 -4.298635][-4.1633563 -4.1901741 -4.20147 -4.1944709 -4.17276 -4.1527214 -4.1463475 -4.1579375 -4.184947 -4.217608 -4.2495503 -4.2769232 -4.2924328 -4.2973337 -4.2956514][-4.1701589 -4.1952858 -4.2083306 -4.20545 -4.1892672 -4.1719456 -4.1622281 -4.169003 -4.1917777 -4.2199893 -4.247077 -4.2708273 -4.2850685 -4.2902055 -4.2903337][-4.1766758 -4.2004948 -4.2165785 -4.2194357 -4.2104554 -4.197803 -4.1866097 -4.1882062 -4.2040625 -4.2251325 -4.2450781 -4.2631221 -4.2751427 -4.2814178 -4.2850289][-4.1854115 -4.2101312 -4.2287178 -4.235126 -4.232183 -4.2236319 -4.2125077 -4.2097287 -4.2174439 -4.2296662 -4.2412972 -4.2530761 -4.263648 -4.2726259 -4.2805805][-4.2011237 -4.2250257 -4.243844 -4.2514896 -4.2512355 -4.24596 -4.236536 -4.2297812 -4.2287197 -4.2314253 -4.2356815 -4.2431045 -4.2538381 -4.2658682 -4.2773676][-4.2217746 -4.2415581 -4.2578931 -4.2648244 -4.2649946 -4.2617369 -4.2548118 -4.2465954 -4.2393365 -4.2344885 -4.2326856 -4.2369242 -4.2478614 -4.2620063 -4.2757349]]...]
INFO - root - 2017-12-07 18:11:59.084221: step 17510, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.668 sec/batch; 68h:54m:59s remains)
INFO - root - 2017-12-07 18:12:15.321447: step 17520, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 65h:14m:32s remains)
INFO - root - 2017-12-07 18:12:31.542442: step 17530, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 1.757 sec/batch; 72h:33m:53s remains)
INFO - root - 2017-12-07 18:12:47.859816: step 17540, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.627 sec/batch; 67h:13m:16s remains)
INFO - root - 2017-12-07 18:13:04.250322: step 17550, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 1.634 sec/batch; 67h:28m:56s remains)
INFO - root - 2017-12-07 18:13:20.411189: step 17560, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 66h:05m:13s remains)
INFO - root - 2017-12-07 18:13:36.558868: step 17570, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.583 sec/batch; 65h:23m:22s remains)
INFO - root - 2017-12-07 18:13:52.782759: step 17580, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.676 sec/batch; 69h:13m:01s remains)
INFO - root - 2017-12-07 18:14:09.015865: step 17590, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 1.609 sec/batch; 66h:26m:03s remains)
INFO - root - 2017-12-07 18:14:25.351210: step 17600, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.741 sec/batch; 71h:53m:42s remains)
2017-12-07 18:14:26.740531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2956991 -4.3011184 -4.3078909 -4.3147111 -4.3224521 -4.3307786 -4.3359189 -4.3382845 -4.33919 -4.3394065 -4.3349848 -4.3211527 -4.2968693 -4.252532 -4.2087746][-4.2735896 -4.2835097 -4.292737 -4.296988 -4.301424 -4.3061309 -4.3081074 -4.3104649 -4.3123317 -4.3125119 -4.307219 -4.2932062 -4.2664976 -4.2191043 -4.1766114][-4.2673759 -4.2784967 -4.2851281 -4.2850075 -4.28553 -4.2860193 -4.2812171 -4.2797208 -4.281487 -4.2812519 -4.2754693 -4.2656913 -4.2462807 -4.2110257 -4.1820121][-4.2723188 -4.2794528 -4.2800736 -4.2741008 -4.2677255 -4.2604346 -4.247828 -4.2403975 -4.2388506 -4.2359042 -4.2281947 -4.2291188 -4.2309875 -4.2175741 -4.2071795][-4.2877955 -4.2838778 -4.2746987 -4.26155 -4.2492833 -4.2329345 -4.2120829 -4.1951742 -4.185854 -4.1753907 -4.1649942 -4.1793904 -4.209877 -4.2215419 -4.2305441][-4.2817068 -4.2633357 -4.2414923 -4.2151647 -4.1898212 -4.1581941 -4.1266794 -4.1125259 -4.1071625 -4.0951185 -4.085865 -4.1162739 -4.1739788 -4.2083712 -4.2340479][-4.2620826 -4.2296591 -4.1993642 -4.168262 -4.12896 -4.0807238 -4.0406952 -4.0424576 -4.0553126 -4.0535216 -4.0570583 -4.0995369 -4.1655364 -4.2053003 -4.2318716][-4.2465777 -4.2082748 -4.1769814 -4.1519032 -4.1081152 -4.0445304 -3.9952459 -4.0163093 -4.055222 -4.0674357 -4.0842142 -4.1279454 -4.183495 -4.2167492 -4.2391553][-4.2280555 -4.1927924 -4.1634879 -4.145648 -4.1083622 -4.0424538 -3.988214 -4.0107584 -4.071949 -4.1075668 -4.1367474 -4.1760569 -4.2122264 -4.2342639 -4.250308][-4.1955266 -4.1686425 -4.1459703 -4.1340957 -4.1096606 -4.0541854 -4.0004292 -4.0032506 -4.0576344 -4.110199 -4.1597457 -4.1995 -4.2252097 -4.2392778 -4.2416439][-4.1415858 -4.1236844 -4.1105571 -4.1045108 -4.0905104 -4.052927 -4.0059881 -3.9781814 -3.9933393 -4.044507 -4.1154494 -4.1705756 -4.1974874 -4.2114749 -4.2002659][-4.0882211 -4.0758743 -4.0666637 -4.0654349 -4.0571995 -4.0370522 -4.0055332 -3.960253 -3.9354398 -3.9707532 -4.0559731 -4.1314139 -4.1642737 -4.1758442 -4.152287][-4.0875449 -4.0801091 -4.0736456 -4.0785 -4.083097 -4.0847039 -4.0691829 -4.0240474 -3.9726667 -3.9769645 -4.0510736 -4.1294656 -4.1662264 -4.1774192 -4.1511478][-4.1272697 -4.121562 -4.116766 -4.1219788 -4.1367044 -4.156425 -4.1551943 -4.1233029 -4.0690708 -4.0530663 -4.1017361 -4.1655493 -4.1990528 -4.2110271 -4.1883974][-4.1736531 -4.1694832 -4.165832 -4.1711521 -4.1865115 -4.2135072 -4.2256222 -4.2119975 -4.172883 -4.1520276 -4.176867 -4.2208934 -4.2471542 -4.2555428 -4.2366633]]...]
INFO - root - 2017-12-07 18:14:43.074859: step 17610, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.627 sec/batch; 67h:10m:26s remains)
INFO - root - 2017-12-07 18:14:59.412644: step 17620, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.628 sec/batch; 67h:12m:16s remains)
INFO - root - 2017-12-07 18:15:15.491839: step 17630, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.668 sec/batch; 68h:51m:41s remains)
INFO - root - 2017-12-07 18:15:31.715905: step 17640, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.551 sec/batch; 64h:01m:55s remains)
INFO - root - 2017-12-07 18:15:48.048369: step 17650, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.569 sec/batch; 64h:44m:55s remains)
INFO - root - 2017-12-07 18:16:04.381052: step 17660, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.677 sec/batch; 69h:13m:27s remains)
INFO - root - 2017-12-07 18:16:20.703093: step 17670, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.618 sec/batch; 66h:45m:55s remains)
INFO - root - 2017-12-07 18:16:37.120857: step 17680, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.672 sec/batch; 69h:01m:02s remains)
INFO - root - 2017-12-07 18:16:53.297601: step 17690, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.595 sec/batch; 65h:48m:24s remains)
INFO - root - 2017-12-07 18:17:09.434227: step 17700, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.605 sec/batch; 66h:13m:45s remains)
2017-12-07 18:17:10.801200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3554683 -4.3527904 -4.3487639 -4.3450603 -4.3401027 -4.3356581 -4.3318014 -4.3293166 -4.3272614 -4.3251629 -4.3246531 -4.3239422 -4.3218031 -4.3171906 -4.3138828][-4.3688197 -4.3672476 -4.365128 -4.362462 -4.358377 -4.3526187 -4.3470135 -4.34405 -4.3412857 -4.3375626 -4.3361616 -4.3374996 -4.3377471 -4.3353462 -4.3338385][-4.3668623 -4.3664684 -4.3649273 -4.3610106 -4.352766 -4.3411121 -4.3299613 -4.3245959 -4.3227777 -4.3226395 -4.3269944 -4.3344932 -4.339623 -4.3411503 -4.3417439][-4.350471 -4.3485332 -4.3447533 -4.3345952 -4.3148394 -4.2900672 -4.2692189 -4.26083 -4.2638621 -4.2742739 -4.291934 -4.3106871 -4.3246236 -4.3342004 -4.3407493][-4.3218961 -4.3140907 -4.3029771 -4.2804222 -4.2394986 -4.1871929 -4.1463208 -4.1352649 -4.1531224 -4.1861091 -4.2262387 -4.26314 -4.2886128 -4.3062968 -4.3207216][-4.28954 -4.2731776 -4.2469544 -4.2036548 -4.13267 -4.0400357 -3.9669151 -3.9607971 -4.0137658 -4.0846386 -4.1509805 -4.2047505 -4.240294 -4.2643967 -4.283062][-4.2439671 -4.2157431 -4.1693625 -4.1051893 -4.009728 -3.8805904 -3.7757392 -3.7905049 -3.8976538 -4.0121288 -4.1004982 -4.1597257 -4.1977534 -4.2224474 -4.2424169][-4.1782813 -4.1432505 -4.0866055 -4.0154285 -3.9225826 -3.8043196 -3.7128386 -3.7525661 -3.8798041 -4.0004787 -4.0846491 -4.1372166 -4.1676741 -4.1867719 -4.2061968][-4.13107 -4.1030273 -4.0586271 -4.008862 -3.958003 -3.9006157 -3.8559902 -3.881907 -3.9650743 -4.0450172 -4.103828 -4.1430192 -4.164258 -4.1749434 -4.1896195][-4.1479411 -4.1373448 -4.116015 -4.0938148 -4.0773807 -4.06109 -4.0404944 -4.043035 -4.0768189 -4.1164484 -4.1518769 -4.181138 -4.1974316 -4.2009878 -4.2061896][-4.2129498 -4.2112312 -4.2018933 -4.1917329 -4.1859069 -4.181612 -4.1695242 -4.1616015 -4.169817 -4.1880703 -4.2095556 -4.2315626 -4.2464218 -4.2486358 -4.2467651][-4.2618504 -4.2593889 -4.2562737 -4.2549038 -4.2565107 -4.2567358 -4.2510009 -4.2453089 -4.2480226 -4.2584805 -4.2709846 -4.2854605 -4.2960396 -4.2965531 -4.2900796][-4.277338 -4.2720981 -4.2729788 -4.2799869 -4.2922049 -4.3014569 -4.3045073 -4.3036528 -4.3057904 -4.3106103 -4.3172607 -4.32647 -4.333005 -4.3308849 -4.321876][-4.2714849 -4.261342 -4.2620387 -4.2749639 -4.2947941 -4.3115058 -4.3226018 -4.3275981 -4.3292918 -4.3297281 -4.3317175 -4.3367081 -4.3404827 -4.3399696 -4.3354216][-4.2633481 -4.2462263 -4.24 -4.2489529 -4.2692008 -4.2904077 -4.3076715 -4.3199606 -4.3252721 -4.3225889 -4.3176517 -4.3175521 -4.3236041 -4.3310685 -4.335258]]...]
INFO - root - 2017-12-07 18:17:26.972039: step 17710, loss = 2.09, batch loss = 2.04 (10.1 examples/sec; 1.580 sec/batch; 65h:10m:20s remains)
INFO - root - 2017-12-07 18:17:43.367360: step 17720, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.661 sec/batch; 68h:31m:19s remains)
INFO - root - 2017-12-07 18:17:59.676655: step 17730, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 65h:07m:37s remains)
INFO - root - 2017-12-07 18:18:15.728424: step 17740, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.646 sec/batch; 67h:54m:49s remains)
INFO - root - 2017-12-07 18:18:31.884415: step 17750, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.559 sec/batch; 64h:18m:37s remains)
INFO - root - 2017-12-07 18:18:47.894921: step 17760, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.638 sec/batch; 67h:34m:32s remains)
INFO - root - 2017-12-07 18:19:03.957603: step 17770, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.574 sec/batch; 64h:55m:56s remains)
INFO - root - 2017-12-07 18:19:20.341124: step 17780, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.615 sec/batch; 66h:36m:49s remains)
INFO - root - 2017-12-07 18:19:36.505370: step 17790, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.692 sec/batch; 69h:47m:13s remains)
INFO - root - 2017-12-07 18:19:52.563574: step 17800, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 1.453 sec/batch; 59h:55m:03s remains)
2017-12-07 18:19:53.972406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0950794 -4.1377187 -4.1893611 -4.2199616 -4.2310715 -4.2326493 -4.2321424 -4.2306952 -4.2247324 -4.2091341 -4.1906104 -4.1955857 -4.2268944 -4.2608585 -4.2841253][-4.1067472 -4.158452 -4.209372 -4.2341485 -4.237886 -4.2313848 -4.2229443 -4.2174239 -4.2093582 -4.1950331 -4.1839905 -4.1945186 -4.2272139 -4.2596684 -4.282733][-4.131547 -4.1845202 -4.2297678 -4.2432594 -4.2374306 -4.2209234 -4.1992488 -4.186172 -4.179306 -4.1766191 -4.1817026 -4.2016263 -4.2336063 -4.2605448 -4.2767129][-4.1713481 -4.2238064 -4.2560225 -4.2494545 -4.2268291 -4.1954207 -4.15165 -4.12259 -4.117012 -4.1323538 -4.1593671 -4.1932735 -4.227386 -4.2513661 -4.2634578][-4.1891079 -4.2403803 -4.2618313 -4.2412038 -4.2034559 -4.1577392 -4.0925941 -4.0466719 -4.0486212 -4.0896044 -4.1417408 -4.1896095 -4.2222643 -4.2382584 -4.2461786][-4.1724458 -4.2130065 -4.2185316 -4.1837797 -4.1333923 -4.0786338 -4.0013456 -3.9445806 -3.9622059 -4.0375009 -4.1171613 -4.1783724 -4.212451 -4.2260904 -4.2316113][-4.162333 -4.1849661 -4.1713648 -4.1230431 -4.065392 -4.00926 -3.9314625 -3.8681054 -3.8945289 -3.9930956 -4.0920119 -4.16387 -4.1995287 -4.2137141 -4.2174544][-4.1753683 -4.1876078 -4.1700134 -4.1293254 -4.0888557 -4.053863 -4.0014043 -3.954668 -3.9759636 -4.0551476 -4.1367126 -4.1981406 -4.2219591 -4.227406 -4.2256613][-4.1842136 -4.1841741 -4.1685028 -4.1434665 -4.1243153 -4.1106648 -4.0829387 -4.057003 -4.0773249 -4.1341553 -4.1912756 -4.2359705 -4.2471409 -4.2450647 -4.2416849][-4.2043085 -4.1937108 -4.1779289 -4.1612725 -4.1530743 -4.1507516 -4.136251 -4.1231103 -4.1416245 -4.180429 -4.2177672 -4.2496343 -4.2530169 -4.2468162 -4.2456317][-4.2381167 -4.2262955 -4.2144713 -4.205956 -4.2052054 -4.2092228 -4.2036257 -4.1972303 -4.2114196 -4.2322965 -4.2473903 -4.2629251 -4.2627616 -4.25483 -4.2551661][-4.2561445 -4.2465234 -4.2392259 -4.2368741 -4.2398386 -4.2449665 -4.2447753 -4.2437639 -4.2582326 -4.2721238 -4.2764983 -4.2851305 -4.2863483 -4.280004 -4.2796803][-4.2686791 -4.2628818 -4.2579865 -4.2557507 -4.2565074 -4.25837 -4.2588 -4.2602854 -4.2733612 -4.2836723 -4.2850952 -4.2929978 -4.299686 -4.2984314 -4.2981076][-4.2844796 -4.2825313 -4.2807026 -4.2796068 -4.2790174 -4.2781172 -4.2780771 -4.2797775 -4.2883849 -4.2949409 -4.2946377 -4.2994246 -4.3065486 -4.3081098 -4.3075442][-4.3072534 -4.3064756 -4.3064265 -4.3062916 -4.3061271 -4.3055406 -4.3062057 -4.3079848 -4.3124132 -4.3162575 -4.3159761 -4.318254 -4.3224092 -4.3240485 -4.323143]]...]
INFO - root - 2017-12-07 18:20:10.210465: step 17810, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.567 sec/batch; 64h:37m:35s remains)
INFO - root - 2017-12-07 18:20:26.422139: step 17820, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 1.695 sec/batch; 69h:53m:57s remains)
INFO - root - 2017-12-07 18:20:42.807922: step 17830, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 65h:57m:23s remains)
INFO - root - 2017-12-07 18:20:59.113249: step 17840, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.642 sec/batch; 67h:42m:36s remains)
INFO - root - 2017-12-07 18:21:15.203889: step 17850, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 1.559 sec/batch; 64h:15m:10s remains)
INFO - root - 2017-12-07 18:21:31.393251: step 17860, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.716 sec/batch; 70h:43m:45s remains)
INFO - root - 2017-12-07 18:21:47.583606: step 17870, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.598 sec/batch; 65h:51m:06s remains)
INFO - root - 2017-12-07 18:22:03.956713: step 17880, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.576 sec/batch; 64h:57m:19s remains)
INFO - root - 2017-12-07 18:22:20.415668: step 17890, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.723 sec/batch; 71h:00m:03s remains)
INFO - root - 2017-12-07 18:22:36.698250: step 17900, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.625 sec/batch; 66h:58m:21s remains)
2017-12-07 18:22:38.178236: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3087435 -4.312099 -4.3139668 -4.3106036 -4.3041592 -4.2979913 -4.2981968 -4.2995434 -4.2996073 -4.3017125 -4.30751 -4.3087459 -4.3026228 -4.2956471 -4.2898655][-4.3009763 -4.307508 -4.3097486 -4.3037424 -4.2943034 -4.2823439 -4.278872 -4.2822609 -4.2883387 -4.2987924 -4.3104024 -4.3122 -4.3002362 -4.2839136 -4.2703142][-4.2806873 -4.2919459 -4.2964468 -4.289187 -4.27764 -4.2607059 -4.2479916 -4.2476282 -4.2624073 -4.2864666 -4.3073993 -4.3166418 -4.30087 -4.2698383 -4.2423439][-4.2537212 -4.2677422 -4.2762752 -4.268558 -4.2534013 -4.2304931 -4.2024302 -4.1928606 -4.2151566 -4.2584119 -4.2964621 -4.3171096 -4.3013129 -4.2527003 -4.2049861][-4.2283673 -4.2421813 -4.2510948 -4.2409506 -4.2214336 -4.1899486 -4.1429768 -4.1142039 -4.1373844 -4.20641 -4.2670803 -4.3012342 -4.2910452 -4.2325659 -4.1709232][-4.212811 -4.2231832 -4.2310085 -4.2212529 -4.1982803 -4.1562104 -4.0883889 -4.0206008 -4.0270905 -4.1305323 -4.2247605 -4.2770367 -4.2749577 -4.2190428 -4.1550198][-4.2145247 -4.2171288 -4.224946 -4.2190351 -4.1931448 -4.1408367 -4.0472364 -3.923481 -3.8900595 -4.0302024 -4.1726861 -4.2510586 -4.2598906 -4.2113519 -4.1549835][-4.2281933 -4.2209415 -4.2242727 -4.2241292 -4.2016292 -4.1501875 -4.0458508 -3.8816872 -3.7955358 -3.9442661 -4.1149168 -4.213913 -4.239614 -4.2062955 -4.1593618][-4.249033 -4.2370749 -4.2341738 -4.2349486 -4.2256312 -4.1917582 -4.1100283 -3.9664555 -3.8684888 -3.9638116 -4.1084895 -4.2056856 -4.2335172 -4.2079449 -4.168961][-4.2601914 -4.243804 -4.2375336 -4.2388277 -4.2410469 -4.2268238 -4.178863 -4.0878348 -4.0140624 -4.0549216 -4.1521349 -4.2302823 -4.2515693 -4.2241735 -4.1849489][-4.2678032 -4.2493992 -4.2433662 -4.2487774 -4.2579064 -4.2562985 -4.2331185 -4.1844611 -4.1346993 -4.1477551 -4.2071753 -4.2643337 -4.2756605 -4.2462177 -4.2047873][-4.2894106 -4.2677712 -4.2599216 -4.2697144 -4.284245 -4.2899752 -4.2798648 -4.2543211 -4.2199621 -4.2191858 -4.2531576 -4.2907367 -4.2965078 -4.270309 -4.2366996][-4.316247 -4.2931428 -4.2826123 -4.2919335 -4.3082385 -4.3152666 -4.3091183 -4.2954855 -4.27631 -4.27193 -4.290091 -4.3096147 -4.3093252 -4.2873735 -4.2633195][-4.3293724 -4.3112421 -4.3003526 -4.3070445 -4.3236175 -4.3312888 -4.32415 -4.3144579 -4.3054376 -4.3036251 -4.31366 -4.3213363 -4.317512 -4.3009367 -4.2834392][-4.3255172 -4.317102 -4.3122354 -4.3157873 -4.3274965 -4.3295393 -4.3197927 -4.3120756 -4.309967 -4.312294 -4.3172174 -4.3170547 -4.3122792 -4.3060408 -4.2998643]]...]
INFO - root - 2017-12-07 18:22:54.545264: step 17910, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 65h:30m:25s remains)
INFO - root - 2017-12-07 18:23:10.749392: step 17920, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.726 sec/batch; 71h:07m:54s remains)
INFO - root - 2017-12-07 18:23:26.758431: step 17930, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 1.538 sec/batch; 63h:20m:47s remains)
INFO - root - 2017-12-07 18:23:43.073650: step 17940, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.673 sec/batch; 68h:56m:30s remains)
INFO - root - 2017-12-07 18:23:59.281285: step 17950, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 1.518 sec/batch; 62h:30m:45s remains)
INFO - root - 2017-12-07 18:24:15.766290: step 17960, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.718 sec/batch; 70h:46m:50s remains)
INFO - root - 2017-12-07 18:24:31.941377: step 17970, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 1.606 sec/batch; 66h:08m:40s remains)
INFO - root - 2017-12-07 18:24:47.821073: step 17980, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 65h:58m:20s remains)
INFO - root - 2017-12-07 18:25:03.967311: step 17990, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 1.704 sec/batch; 70h:09m:52s remains)
INFO - root - 2017-12-07 18:25:20.220490: step 18000, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.634 sec/batch; 67h:18m:25s remains)
2017-12-07 18:25:21.708678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0809903 -4.0609646 -4.0470686 -4.0759912 -4.11709 -4.1380916 -4.1379623 -4.1334877 -4.12435 -4.1392403 -4.188777 -4.2352524 -4.2524762 -4.245882 -4.2444372][-4.0784235 -4.05874 -4.0533466 -4.0969257 -4.1490784 -4.16201 -4.1435404 -4.1238003 -4.104919 -4.1166034 -4.1688671 -4.2146745 -4.2251596 -4.216249 -4.2169647][-4.0856714 -4.0731592 -4.0831165 -4.1360221 -4.1870804 -4.1882372 -4.1526546 -4.1223845 -4.0954676 -4.1005163 -4.1418109 -4.1790314 -4.1833835 -4.1705251 -4.1724944][-4.0840464 -4.0867791 -4.1137919 -4.1664658 -4.2076306 -4.1978006 -4.1511388 -4.1149483 -4.0854764 -4.0842605 -4.1097183 -4.1347427 -4.1291046 -4.1072869 -4.1033297][-4.0730114 -4.0950584 -4.1375136 -4.1842365 -4.2100482 -4.1908126 -4.1406727 -4.1037717 -4.0723829 -4.0648913 -4.0763927 -4.091218 -4.08232 -4.0516028 -4.036448][-4.0624332 -4.1029925 -4.1563058 -4.193397 -4.1973362 -4.1651936 -4.1129484 -4.0754151 -4.0465903 -4.0342846 -4.0410171 -4.0545511 -4.0525026 -4.0209913 -3.9951437][-4.0683308 -4.1080709 -4.1469064 -4.1589465 -4.1410527 -4.10145 -4.044363 -4.0069876 -3.9864388 -3.9779937 -3.9925685 -4.0195704 -4.0357738 -4.0213637 -3.9945207][-4.0677915 -4.0825658 -4.0875 -4.0710511 -4.041791 -4.0031009 -3.9499164 -3.9183531 -3.9071581 -3.9060559 -3.9365709 -3.9874132 -4.0258508 -4.0352316 -4.0192914][-4.0633268 -4.044302 -4.0195045 -3.989856 -3.9682784 -3.9489827 -3.9150956 -3.8917863 -3.8849003 -3.885757 -3.9230969 -3.9854584 -4.0361881 -4.0597715 -4.0517879][-4.0807557 -4.0369577 -3.998528 -3.9747372 -3.9703763 -3.9745488 -3.9675958 -3.9597528 -3.9582033 -3.9578919 -3.982677 -4.0327253 -4.0801263 -4.1035767 -4.0971017][-4.1413341 -4.0893164 -4.052248 -4.0388408 -4.046391 -4.0631323 -4.0720639 -4.0741587 -4.0733438 -4.0681043 -4.0769887 -4.1074104 -4.1434975 -4.1607218 -4.1577082][-4.217217 -4.1755877 -4.149343 -4.1429467 -4.1519365 -4.1679544 -4.1777911 -4.1798968 -4.1771851 -4.170105 -4.1677504 -4.18362 -4.2081327 -4.2192698 -4.2207046][-4.2725449 -4.2470031 -4.2307224 -4.2273221 -4.2351809 -4.2475348 -4.2554617 -4.257513 -4.2553973 -4.2474113 -4.2395191 -4.2435069 -4.2555003 -4.2637291 -4.2688689][-4.2991705 -4.2857084 -4.2767649 -4.274375 -4.279984 -4.289916 -4.2980318 -4.30254 -4.3021708 -4.295671 -4.2865357 -4.2840142 -4.2872849 -4.29264 -4.2998376][-4.3140574 -4.3072953 -4.302619 -4.3020797 -4.3067818 -4.3142219 -4.3206825 -4.3246522 -4.3247638 -4.3206425 -4.3141322 -4.3104534 -4.3108225 -4.3153009 -4.3210554]]...]
INFO - root - 2017-12-07 18:25:37.982830: step 18010, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.581 sec/batch; 65h:06m:48s remains)
INFO - root - 2017-12-07 18:25:54.308541: step 18020, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 1.760 sec/batch; 72h:27m:39s remains)
INFO - root - 2017-12-07 18:26:10.536738: step 18030, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.604 sec/batch; 66h:03m:25s remains)
INFO - root - 2017-12-07 18:26:26.920431: step 18040, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.578 sec/batch; 64h:58m:11s remains)
INFO - root - 2017-12-07 18:26:43.198339: step 18050, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.687 sec/batch; 69h:27m:30s remains)
INFO - root - 2017-12-07 18:26:59.509950: step 18060, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.580 sec/batch; 65h:02m:34s remains)
INFO - root - 2017-12-07 18:27:15.765860: step 18070, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.715 sec/batch; 70h:35m:51s remains)
INFO - root - 2017-12-07 18:27:32.024173: step 18080, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.594 sec/batch; 65h:35m:18s remains)
INFO - root - 2017-12-07 18:27:48.265653: step 18090, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.663 sec/batch; 68h:27m:11s remains)
INFO - root - 2017-12-07 18:28:04.295518: step 18100, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.589 sec/batch; 65h:23m:59s remains)
2017-12-07 18:28:05.669239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1655631 -4.1522083 -4.150876 -4.1496282 -4.1414771 -4.1380267 -4.14691 -4.1651177 -4.1950436 -4.2320967 -4.2637539 -4.2740173 -4.2602372 -4.2398286 -4.2284789][-4.1869831 -4.179256 -4.1790562 -4.1701584 -4.1549959 -4.1504307 -4.1658211 -4.1936822 -4.2283072 -4.2633157 -4.289042 -4.2909775 -4.2715287 -4.2502055 -4.2368417][-4.208374 -4.2096171 -4.2132034 -4.2033105 -4.1863165 -4.183063 -4.1988368 -4.2244015 -4.2491293 -4.2705655 -4.2839551 -4.2772918 -4.2548985 -4.2346935 -4.2202258][-4.2295933 -4.2317586 -4.2387357 -4.2329526 -4.2205458 -4.2158265 -4.2228794 -4.2344594 -4.2438922 -4.2517409 -4.2546678 -4.2434778 -4.2257619 -4.2086253 -4.1904154][-4.2442536 -4.2408648 -4.2444997 -4.2433844 -4.2347283 -4.22375 -4.2136269 -4.2021637 -4.1995049 -4.2122893 -4.2246032 -4.2219038 -4.2112913 -4.1930103 -4.1678987][-4.2478971 -4.2366486 -4.2302518 -4.2287312 -4.2198148 -4.1977105 -4.160605 -4.1143627 -4.1022325 -4.1415172 -4.1883006 -4.2063303 -4.2040248 -4.1832395 -4.1509929][-4.2379518 -4.2197452 -4.2035794 -4.1971231 -4.1867032 -4.1511097 -4.0814366 -3.9958241 -3.9757333 -4.056922 -4.1459017 -4.1909451 -4.1988888 -4.1779089 -4.1432371][-4.2212896 -4.1998892 -4.1787963 -4.1668587 -4.1584263 -4.125627 -4.0566916 -3.9744496 -3.9600172 -4.0462618 -4.1391096 -4.1875238 -4.1986165 -4.18537 -4.157382][-4.2054582 -4.1840634 -4.1620836 -4.1483178 -4.1456919 -4.1340985 -4.1035724 -4.0695629 -4.072238 -4.1264834 -4.1857562 -4.2109628 -4.2091808 -4.2006726 -4.1853676][-4.1984663 -4.1830478 -4.1672945 -4.1546669 -4.1533766 -4.154099 -4.1522546 -4.1566429 -4.1726828 -4.203299 -4.2348161 -4.2371669 -4.2193642 -4.2057729 -4.1948018][-4.21997 -4.2124143 -4.2001595 -4.1858735 -4.1816716 -4.1854067 -4.1891422 -4.1997027 -4.2124577 -4.2218962 -4.2316437 -4.2246013 -4.2062635 -4.193574 -4.1825948][-4.26102 -4.2556572 -4.2402864 -4.223865 -4.2160454 -4.2156744 -4.2136641 -4.2104344 -4.2084646 -4.198802 -4.1943407 -4.1891932 -4.1821823 -4.1758609 -4.1635408][-4.286242 -4.2786813 -4.2616329 -4.2459741 -4.2356033 -4.2302117 -4.2229052 -4.2088742 -4.1951895 -4.1735749 -4.1615791 -4.1588678 -4.15903 -4.1591973 -4.1457791][-4.2912645 -4.2812028 -4.2649069 -4.2513604 -4.2419248 -4.2306142 -4.2149205 -4.1935673 -4.1779 -4.1594276 -4.1490755 -4.150125 -4.1562939 -4.161438 -4.152113][-4.2907991 -4.2772884 -4.2614737 -4.252686 -4.2463508 -4.2302217 -4.2089577 -4.1853929 -4.1722589 -4.1621022 -4.1577525 -4.1654682 -4.1796975 -4.1910343 -4.1864738]]...]
INFO - root - 2017-12-07 18:28:22.031748: step 18110, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.579 sec/batch; 64h:58m:45s remains)
INFO - root - 2017-12-07 18:28:38.406337: step 18120, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.699 sec/batch; 69h:55m:20s remains)
INFO - root - 2017-12-07 18:28:54.621421: step 18130, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.632 sec/batch; 67h:08m:11s remains)
INFO - root - 2017-12-07 18:29:10.943870: step 18140, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.596 sec/batch; 65h:40m:34s remains)
INFO - root - 2017-12-07 18:29:27.022450: step 18150, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.644 sec/batch; 67h:38m:50s remains)
INFO - root - 2017-12-07 18:29:43.191113: step 18160, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.586 sec/batch; 65h:13m:42s remains)
INFO - root - 2017-12-07 18:29:59.572830: step 18170, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.711 sec/batch; 70h:21m:47s remains)
INFO - root - 2017-12-07 18:30:15.817343: step 18180, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.552 sec/batch; 63h:49m:38s remains)
INFO - root - 2017-12-07 18:30:32.177536: step 18190, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.690 sec/batch; 69h:30m:57s remains)
INFO - root - 2017-12-07 18:30:48.313437: step 18200, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.605 sec/batch; 65h:59m:16s remains)
2017-12-07 18:30:49.814517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2359881 -4.2257962 -4.2325277 -4.2460556 -4.2535305 -4.2516737 -4.2429628 -4.2370176 -4.2438607 -4.256887 -4.2684011 -4.2734523 -4.268518 -4.2531571 -4.2512708][-4.2248225 -4.2204061 -4.2321873 -4.2471509 -4.2556934 -4.2590981 -4.2567277 -4.2536883 -4.2610793 -4.272 -4.2794147 -4.2817125 -4.2741137 -4.2551465 -4.2499352][-4.2084689 -4.2070513 -4.221209 -4.2366033 -4.2477593 -4.2556758 -4.2545958 -4.2523284 -4.2623081 -4.2752028 -4.2812948 -4.2821488 -4.2735915 -4.2536483 -4.2461262][-4.1801171 -4.1768279 -4.1928639 -4.2095437 -4.2242289 -4.2349792 -4.2299433 -4.2229743 -4.2344036 -4.2508783 -4.2596083 -4.2629724 -4.2571378 -4.2397356 -4.2332406][-4.1482744 -4.1378379 -4.150743 -4.1647973 -4.1772795 -4.1860628 -4.1735449 -4.1602798 -4.1765375 -4.2013416 -4.220726 -4.2323928 -4.2311234 -4.216507 -4.213151][-4.1218309 -4.1062946 -4.1111259 -4.1176467 -4.1198726 -4.114903 -4.0833955 -4.0595489 -4.0883269 -4.1310987 -4.1689711 -4.1931067 -4.1975479 -4.1876912 -4.191349][-4.0985217 -4.0791197 -4.0766773 -4.0726 -4.0586343 -4.0225077 -3.953706 -3.9194849 -3.9796994 -4.0591245 -4.1196747 -4.1528926 -4.1636653 -4.1632948 -4.1743279][-4.0871744 -4.0616841 -4.0527205 -4.0407672 -4.0133481 -3.9457958 -3.8285971 -3.7687788 -3.867162 -3.9917285 -4.0743771 -4.1132007 -4.1323395 -4.1446795 -4.1628737][-4.0732784 -4.0490623 -4.0454111 -4.0384579 -4.0153294 -3.9540319 -3.8476341 -3.7846289 -3.8724694 -3.9940591 -4.0718293 -4.106452 -4.1288738 -4.1442266 -4.1594567][-4.0547051 -4.0409608 -4.0546193 -4.0620213 -4.0592942 -4.0385642 -3.9915104 -3.9572616 -4.0018387 -4.0719419 -4.1144648 -4.1270638 -4.1398563 -4.150475 -4.1606679][-4.0567861 -4.0589137 -4.0909948 -4.1088533 -4.1237392 -4.1339755 -4.1271515 -4.1112976 -4.1258636 -4.155479 -4.16824 -4.1605091 -4.1577349 -4.1614089 -4.170187][-4.0964017 -4.111897 -4.1477761 -4.1640034 -4.1824088 -4.2038574 -4.2122984 -4.2005248 -4.1960025 -4.2047129 -4.2074957 -4.1956935 -4.1859393 -4.1851912 -4.1934128][-4.1524262 -4.1755481 -4.2011528 -4.2073984 -4.2204928 -4.2450218 -4.25816 -4.2488313 -4.2426825 -4.247169 -4.2490454 -4.2417665 -4.2312093 -4.2244387 -4.2274261][-4.1954513 -4.219243 -4.2338967 -4.2334104 -4.2430568 -4.2646217 -4.2762618 -4.2707067 -4.2709942 -4.2834344 -4.291913 -4.2897496 -4.2786222 -4.2637434 -4.2581353][-4.2218652 -4.2465706 -4.260767 -4.2593036 -4.2643595 -4.2799821 -4.2869186 -4.2846723 -4.29161 -4.3061428 -4.3169379 -4.31856 -4.3087378 -4.2891088 -4.2752681]]...]
INFO - root - 2017-12-07 18:31:05.956673: step 18210, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.555 sec/batch; 63h:55m:57s remains)
INFO - root - 2017-12-07 18:31:22.101500: step 18220, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.658 sec/batch; 68h:11m:31s remains)
INFO - root - 2017-12-07 18:31:38.277948: step 18230, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.584 sec/batch; 65h:07m:12s remains)
INFO - root - 2017-12-07 18:31:54.557851: step 18240, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.707 sec/batch; 70h:10m:11s remains)
INFO - root - 2017-12-07 18:32:10.675416: step 18250, loss = 2.05, batch loss = 2.00 (10.2 examples/sec; 1.575 sec/batch; 64h:43m:52s remains)
INFO - root - 2017-12-07 18:32:26.902638: step 18260, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.654 sec/batch; 67h:58m:39s remains)
INFO - root - 2017-12-07 18:32:43.091932: step 18270, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 1.639 sec/batch; 67h:21m:55s remains)
INFO - root - 2017-12-07 18:32:59.246410: step 18280, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.558 sec/batch; 64h:02m:32s remains)
INFO - root - 2017-12-07 18:33:15.533445: step 18290, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.594 sec/batch; 65h:29m:59s remains)
INFO - root - 2017-12-07 18:33:31.590850: step 18300, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.554 sec/batch; 63h:50m:58s remains)
2017-12-07 18:33:33.017146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2245097 -4.2171106 -4.2273765 -4.2500458 -4.2697921 -4.2738776 -4.2696648 -4.26447 -4.2586651 -4.2549443 -4.2506728 -4.2559571 -4.2691526 -4.2693267 -4.2587581][-4.2364159 -4.2234426 -4.223578 -4.2373309 -4.2560468 -4.2630515 -4.2643094 -4.2635274 -4.2536259 -4.2425151 -4.2327266 -4.2357664 -4.2452641 -4.2434168 -4.2292547][-4.2589941 -4.2371778 -4.2275271 -4.2319503 -4.2447567 -4.251442 -4.2564888 -4.2610836 -4.2514338 -4.2369785 -4.2228289 -4.2176328 -4.2166324 -4.2073421 -4.190968][-4.2782016 -4.2500772 -4.2345357 -4.2307386 -4.2334318 -4.23464 -4.239562 -4.246985 -4.2391706 -4.2264833 -4.2119584 -4.1962075 -4.1809525 -4.1613836 -4.1457109][-4.2846446 -4.2557988 -4.2390676 -4.2288308 -4.2211719 -4.2127762 -4.2104249 -4.217782 -4.2135034 -4.2067304 -4.1968665 -4.1737037 -4.1490483 -4.1259079 -4.1181493][-4.2788305 -4.25285 -4.2368131 -4.2224727 -4.2058725 -4.1891088 -4.17801 -4.1830235 -4.1830711 -4.1862164 -4.185935 -4.1666427 -4.1453404 -4.126771 -4.1293974][-4.2628036 -4.2374067 -4.2206349 -4.2046065 -4.1827641 -4.1609077 -4.1449056 -4.1471272 -4.1511087 -4.1623755 -4.17276 -4.1624765 -4.1512685 -4.1415658 -4.1514235][-4.2457166 -4.2198563 -4.2006688 -4.1838789 -4.1613059 -4.1392827 -4.1239204 -4.1261806 -4.1342239 -4.1482105 -4.1628075 -4.1610346 -4.1538463 -4.1506224 -4.167028][-4.2320662 -4.20299 -4.1811438 -4.164782 -4.1459155 -4.1281853 -4.1175656 -4.1242075 -4.1385832 -4.155015 -4.17346 -4.17527 -4.1693425 -4.166245 -4.1839705][-4.214231 -4.1820793 -4.1597037 -4.1469831 -4.1378393 -4.1316905 -4.132175 -4.1434622 -4.1600766 -4.177443 -4.1975789 -4.2026258 -4.196229 -4.1914563 -4.2038169][-4.1925244 -4.1584444 -4.1351228 -4.1298 -4.1388903 -4.1529679 -4.1657224 -4.1763611 -4.1868157 -4.1977305 -4.2115269 -4.2156148 -4.2107525 -4.2069359 -4.2164617][-4.1825833 -4.1508303 -4.1274991 -4.1282969 -4.1523514 -4.1832986 -4.205719 -4.2146521 -4.2138762 -4.2110314 -4.2091336 -4.2036419 -4.2002378 -4.2058005 -4.2195845][-4.191062 -4.1685004 -4.1494451 -4.1512365 -4.1786661 -4.2166481 -4.2428269 -4.2504296 -4.2403316 -4.2234712 -4.2040997 -4.1888795 -4.1875067 -4.2017488 -4.2214913][-4.2133017 -4.2015915 -4.1889157 -4.1890984 -4.2103381 -4.2437367 -4.2697945 -4.2778716 -4.2643995 -4.239007 -4.2096119 -4.1934118 -4.1978507 -4.2180972 -4.2385588][-4.2465844 -4.2412453 -4.2325363 -4.2307739 -4.2454147 -4.2707539 -4.2915735 -4.2975025 -4.2836938 -4.2549095 -4.225637 -4.2146673 -4.22383 -4.2428064 -4.258008]]...]
INFO - root - 2017-12-07 18:33:49.207575: step 18310, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.589 sec/batch; 65h:18m:33s remains)
INFO - root - 2017-12-07 18:34:05.230064: step 18320, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.637 sec/batch; 67h:16m:52s remains)
INFO - root - 2017-12-07 18:34:21.248034: step 18330, loss = 2.06, batch loss = 2.01 (10.2 examples/sec; 1.568 sec/batch; 64h:26m:27s remains)
INFO - root - 2017-12-07 18:34:37.591848: step 18340, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.578 sec/batch; 64h:49m:23s remains)
INFO - root - 2017-12-07 18:34:53.909724: step 18350, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.718 sec/batch; 70h:35m:03s remains)
INFO - root - 2017-12-07 18:35:10.186879: step 18360, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 1.591 sec/batch; 65h:20m:23s remains)
INFO - root - 2017-12-07 18:35:26.617975: step 18370, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.735 sec/batch; 71h:15m:46s remains)
INFO - root - 2017-12-07 18:35:42.760014: step 18380, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.594 sec/batch; 65h:28m:30s remains)
INFO - root - 2017-12-07 18:35:58.965035: step 18390, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.625 sec/batch; 66h:44m:31s remains)
INFO - root - 2017-12-07 18:36:15.055119: step 18400, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.637 sec/batch; 67h:14m:15s remains)
2017-12-07 18:36:16.544140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1951413 -4.187469 -4.17542 -4.1605325 -4.1511006 -4.1566868 -4.1659126 -4.1845779 -4.210186 -4.2311091 -4.2394619 -4.2254291 -4.2083611 -4.2033072 -4.2131677][-4.1777906 -4.1802182 -4.1785822 -4.16505 -4.1511688 -4.1534142 -4.1603165 -4.1732635 -4.1881776 -4.2070622 -4.2198071 -4.2117672 -4.1948671 -4.1890655 -4.1996746][-4.1622534 -4.1681905 -4.1732488 -4.1660285 -4.1598959 -4.1667738 -4.17708 -4.1890187 -4.1935339 -4.2036982 -4.2140474 -4.2060952 -4.1850982 -4.172698 -4.1770372][-4.1602325 -4.1640248 -4.1663322 -4.1579137 -4.1533613 -4.1625462 -4.1763659 -4.1886005 -4.1899881 -4.1981292 -4.20753 -4.2026067 -4.1855364 -4.1747975 -4.1755004][-4.1630063 -4.1585917 -4.1572342 -4.1448011 -4.1416135 -4.1510324 -4.1643844 -4.1758857 -4.1804938 -4.19483 -4.2084103 -4.2137594 -4.2086763 -4.2057028 -4.2061639][-4.1713805 -4.1574965 -4.154047 -4.1438432 -4.139636 -4.1382031 -4.1403842 -4.1468225 -4.1582689 -4.1885738 -4.214396 -4.2301965 -4.2401156 -4.2471447 -4.2471528][-4.1869183 -4.1603613 -4.1426096 -4.1192646 -4.1029406 -4.0809736 -4.05753 -4.0379171 -4.0414891 -4.0997887 -4.1617646 -4.2036982 -4.2318144 -4.2474055 -4.2447734][-4.1986032 -4.1715393 -4.1450958 -4.1113515 -4.0812454 -4.0406313 -3.9873309 -3.920656 -3.8854177 -3.965394 -4.0639672 -4.1341677 -4.1808643 -4.2051654 -4.19901][-4.2031722 -4.1912827 -4.174468 -4.1480074 -4.1226306 -4.0856676 -4.0397935 -3.9765253 -3.9268386 -3.9751532 -4.0475359 -4.100317 -4.1428404 -4.1697984 -4.1634741][-4.2149897 -4.213789 -4.2077732 -4.192132 -4.176311 -4.1522112 -4.1235094 -4.0916595 -4.0619078 -4.0797129 -4.1072478 -4.1221347 -4.1387 -4.1554589 -4.1510706][-4.2161355 -4.2203293 -4.2189956 -4.2109079 -4.20237 -4.1888847 -4.1738143 -4.1598239 -4.1436281 -4.1506109 -4.1566 -4.1484132 -4.1432848 -4.1448097 -4.1371341][-4.2117047 -4.2204051 -4.2230124 -4.2214451 -4.2177157 -4.2087255 -4.1996861 -4.1941857 -4.1879945 -4.1933408 -4.1920633 -4.1748142 -4.16188 -4.1545668 -4.1429338][-4.2235575 -4.2298326 -4.2329383 -4.2327418 -4.229847 -4.2228694 -4.21773 -4.2164359 -4.2158365 -4.2188559 -4.2162638 -4.205564 -4.2009869 -4.1949196 -4.183147][-4.2348528 -4.2346869 -4.2347221 -4.2333064 -4.230895 -4.225605 -4.2230353 -4.2238474 -4.2239742 -4.2229795 -4.2220664 -4.2192554 -4.2226272 -4.222959 -4.2137794][-4.2373977 -4.2373285 -4.238668 -4.2378325 -4.23616 -4.231925 -4.2280235 -4.2263031 -4.2222776 -4.2175369 -4.2164812 -4.2177758 -4.2237391 -4.2273893 -4.2230425]]...]
INFO - root - 2017-12-07 18:36:32.737601: step 18410, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.592 sec/batch; 65h:23m:34s remains)
INFO - root - 2017-12-07 18:36:49.046288: step 18420, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.625 sec/batch; 66h:44m:10s remains)
INFO - root - 2017-12-07 18:37:05.313253: step 18430, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.681 sec/batch; 69h:00m:34s remains)
INFO - root - 2017-12-07 18:37:21.385082: step 18440, loss = 2.06, batch loss = 2.01 (10.2 examples/sec; 1.571 sec/batch; 64h:31m:07s remains)
INFO - root - 2017-12-07 18:37:37.570130: step 18450, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 1.657 sec/batch; 68h:00m:45s remains)
INFO - root - 2017-12-07 18:37:53.534121: step 18460, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.526 sec/batch; 62h:38m:18s remains)
INFO - root - 2017-12-07 18:38:09.999280: step 18470, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.675 sec/batch; 68h:44m:28s remains)
INFO - root - 2017-12-07 18:38:26.259646: step 18480, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 1.535 sec/batch; 63h:00m:26s remains)
INFO - root - 2017-12-07 18:38:42.585285: step 18490, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.665 sec/batch; 68h:19m:43s remains)
INFO - root - 2017-12-07 18:38:58.718590: step 18500, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.608 sec/batch; 65h:59m:56s remains)
2017-12-07 18:39:00.090380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3325043 -4.3113389 -4.2714882 -4.2102289 -4.1416326 -4.0976357 -4.0647235 -4.0581403 -4.1125422 -4.1949644 -4.2512593 -4.2752728 -4.2760034 -4.2692451 -4.2632036][-4.3758349 -4.3589506 -4.3242369 -4.2643366 -4.18702 -4.1241388 -4.0637217 -4.0343509 -4.0812144 -4.1695457 -4.2348108 -4.2679715 -4.276638 -4.2756057 -4.2707043][-4.3949828 -4.3810658 -4.350739 -4.2943888 -4.2146454 -4.1343818 -4.0405717 -3.985687 -4.0262046 -4.125843 -4.2040114 -4.24849 -4.2667317 -4.271987 -4.2694774][-4.3996472 -4.3852949 -4.3543267 -4.2950211 -4.2104115 -4.1196342 -4.0042176 -3.9256668 -3.9542475 -4.0685215 -4.1663976 -4.225842 -4.2566533 -4.26968 -4.2712755][-4.3952179 -4.3761759 -4.3412871 -4.279345 -4.193737 -4.1033468 -3.9813612 -3.8771188 -3.8781011 -4.0002608 -4.123714 -4.204525 -4.2501163 -4.2675014 -4.2723207][-4.3803473 -4.3562517 -4.3204026 -4.2642951 -4.1865606 -4.1059208 -3.9874742 -3.8539863 -3.8106031 -3.9209015 -4.067646 -4.1716337 -4.2350073 -4.2611766 -4.2699108][-4.3586974 -4.3312559 -4.2998824 -4.2565222 -4.1958447 -4.1270218 -4.01383 -3.8525589 -3.7550488 -3.8284 -3.9920344 -4.122952 -4.2071314 -4.2485309 -4.264461][-4.3385391 -4.3142185 -4.291235 -4.2647481 -4.2234035 -4.1638455 -4.055696 -3.8808496 -3.7333686 -3.7464411 -3.9079905 -4.0649495 -4.1712871 -4.230257 -4.2555676][-4.3342495 -4.3216114 -4.310637 -4.2988477 -4.2732973 -4.2206187 -4.1198349 -3.9565127 -3.7877426 -3.7388 -3.8596482 -4.0162063 -4.1354718 -4.2092142 -4.2434759][-4.3424773 -4.3412371 -4.3393531 -4.3349762 -4.3208194 -4.2777724 -4.1948347 -4.058661 -3.904954 -3.8256791 -3.8846743 -4.004518 -4.1147795 -4.1927347 -4.23081][-4.3529463 -4.3521419 -4.349915 -4.3477373 -4.3435073 -4.3141813 -4.2487893 -4.1416659 -4.0187521 -3.9398203 -3.9525883 -4.0252614 -4.112371 -4.1829081 -4.2203813][-4.3616014 -4.3582363 -4.3493023 -4.3405557 -4.3349905 -4.3193822 -4.2759352 -4.1997981 -4.1144524 -4.0542536 -4.0419579 -4.0714693 -4.12697 -4.1828938 -4.2150807][-4.3643365 -4.3571367 -4.3405323 -4.3228331 -4.314045 -4.3116417 -4.29472 -4.2523265 -4.1962075 -4.1461358 -4.1174293 -4.1134481 -4.1400275 -4.181222 -4.20932][-4.3590317 -4.3504558 -4.3286824 -4.305119 -4.29655 -4.3077335 -4.3166389 -4.3026385 -4.2625036 -4.2110071 -4.1634197 -4.1326537 -4.1331983 -4.1599326 -4.1882591][-4.3428192 -4.3360782 -4.3161411 -4.2951932 -4.2896357 -4.3045039 -4.3224468 -4.3220673 -4.2926631 -4.2415366 -4.1825719 -4.1314206 -4.1122661 -4.12711 -4.1547441]]...]
INFO - root - 2017-12-07 18:39:16.169923: step 18510, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.574 sec/batch; 64h:34m:44s remains)
INFO - root - 2017-12-07 18:39:32.445743: step 18520, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.681 sec/batch; 68h:58m:06s remains)
INFO - root - 2017-12-07 18:39:48.819590: step 18530, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.667 sec/batch; 68h:23m:56s remains)
INFO - root - 2017-12-07 18:40:04.927620: step 18540, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.555 sec/batch; 63h:49m:04s remains)
INFO - root - 2017-12-07 18:40:21.051246: step 18550, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.541 sec/batch; 63h:13m:26s remains)
INFO - root - 2017-12-07 18:40:37.458184: step 18560, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.597 sec/batch; 65h:30m:50s remains)
INFO - root - 2017-12-07 18:40:53.928331: step 18570, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.606 sec/batch; 65h:51m:47s remains)
INFO - root - 2017-12-07 18:41:09.976804: step 18580, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.640 sec/batch; 67h:15m:06s remains)
INFO - root - 2017-12-07 18:41:26.313070: step 18590, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 1.572 sec/batch; 64h:28m:35s remains)
INFO - root - 2017-12-07 18:41:42.688937: step 18600, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.714 sec/batch; 70h:16m:48s remains)
2017-12-07 18:41:44.055343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1671863 -4.1912332 -4.1987348 -4.1850657 -4.1787934 -4.1895947 -4.2149687 -4.2534623 -4.2695217 -4.2529683 -4.2242241 -4.2029462 -4.1846409 -4.1778221 -4.1713057][-4.1150203 -4.1534142 -4.1736388 -4.1624045 -4.1544595 -4.1633263 -4.1901712 -4.2332649 -4.2544155 -4.2438121 -4.2198086 -4.1980853 -4.17231 -4.1625495 -4.1592298][-4.1187825 -4.1669364 -4.1924529 -4.1751914 -4.1574659 -4.1558828 -4.1787758 -4.2232723 -4.2490973 -4.2441082 -4.2258229 -4.2073665 -4.178545 -4.1700916 -4.174685][-4.1586294 -4.2005076 -4.2150917 -4.1872134 -4.1595259 -4.1464634 -4.1644545 -4.2057543 -4.2316232 -4.2362909 -4.2319713 -4.2217054 -4.1991668 -4.1945529 -4.2079282][-4.1938643 -4.2139745 -4.2073164 -4.1706357 -4.1342916 -4.1136379 -4.131197 -4.17245 -4.2035103 -4.2209849 -4.233284 -4.2401009 -4.2325773 -4.2356553 -4.2562456][-4.2193193 -4.2106013 -4.181488 -4.140223 -4.1107368 -4.1018047 -4.12462 -4.164052 -4.1947627 -4.21415 -4.233849 -4.2512045 -4.2543387 -4.2696218 -4.3024836][-4.2311363 -4.19194 -4.1413145 -4.1002908 -4.0869594 -4.0998507 -4.1320052 -4.1665316 -4.1925855 -4.2094965 -4.2268167 -4.243012 -4.2501884 -4.2756038 -4.3177705][-4.2296023 -4.1581354 -4.08574 -4.04536 -4.0494928 -4.0851078 -4.1293278 -4.1643195 -4.1867638 -4.1972051 -4.2057223 -4.2183828 -4.2296386 -4.2637968 -4.3129764][-4.2070103 -4.1129637 -4.0294919 -3.9947734 -4.0166087 -4.070447 -4.1280551 -4.1649466 -4.1811366 -4.1800532 -4.18076 -4.192287 -4.2089806 -4.2519541 -4.3030376][-4.1858397 -4.0909705 -4.0129056 -3.9881568 -4.0231934 -4.0848541 -4.1418233 -4.1746492 -4.1821675 -4.1693277 -4.1636653 -4.1773148 -4.2023387 -4.2496357 -4.2969246][-4.1900535 -4.1102929 -4.0485473 -4.0369864 -4.0780349 -4.1388769 -4.1891317 -4.21533 -4.21037 -4.1860156 -4.1786861 -4.1950607 -4.2231388 -4.2654805 -4.3040442][-4.2242489 -4.1674933 -4.1276312 -4.1322412 -4.17628 -4.2307963 -4.2711291 -4.2881641 -4.2710714 -4.2397785 -4.230804 -4.24396 -4.2683334 -4.2999086 -4.3260355][-4.2706676 -4.2379689 -4.2188916 -4.2323232 -4.2720952 -4.3144245 -4.34291 -4.3484535 -4.3264804 -4.2976284 -4.2859955 -4.293191 -4.3108683 -4.3306732 -4.3451915][-4.3148918 -4.298933 -4.2912817 -4.3028083 -4.3307004 -4.3573956 -4.3734751 -4.3722191 -4.3555069 -4.3359685 -4.3270588 -4.3311315 -4.3420339 -4.3531008 -4.3600807][-4.3484197 -4.3409576 -4.3367586 -4.34277 -4.3579197 -4.3726015 -4.380506 -4.3791771 -4.370048 -4.3597031 -4.3555703 -4.3586464 -4.3643947 -4.3699594 -4.3729887]]...]
INFO - root - 2017-12-07 18:41:59.887332: step 18610, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 1.511 sec/batch; 61h:58m:23s remains)
INFO - root - 2017-12-07 18:42:16.376635: step 18620, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.545 sec/batch; 63h:22m:02s remains)
INFO - root - 2017-12-07 18:42:32.671514: step 18630, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.721 sec/batch; 70h:33m:21s remains)
INFO - root - 2017-12-07 18:42:48.646141: step 18640, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.543 sec/batch; 63h:16m:01s remains)
INFO - root - 2017-12-07 18:43:05.050453: step 18650, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 1.687 sec/batch; 69h:09m:43s remains)
INFO - root - 2017-12-07 18:43:21.283725: step 18660, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.635 sec/batch; 67h:01m:30s remains)
INFO - root - 2017-12-07 18:43:37.520642: step 18670, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.701 sec/batch; 69h:43m:04s remains)
INFO - root - 2017-12-07 18:43:53.716121: step 18680, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.544 sec/batch; 63h:18m:00s remains)
INFO - root - 2017-12-07 18:44:10.207717: step 18690, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.585 sec/batch; 64h:57m:02s remains)
INFO - root - 2017-12-07 18:44:26.438371: step 18700, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.701 sec/batch; 69h:43m:46s remains)
2017-12-07 18:44:27.789899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2006965 -4.2015686 -4.2005892 -4.202507 -4.2161732 -4.2394848 -4.26061 -4.2732353 -4.2770514 -4.2779717 -4.2781248 -4.2779765 -4.2758174 -4.2687564 -4.2638936][-4.2237062 -4.2179275 -4.2088418 -4.1977262 -4.1993036 -4.21556 -4.2389736 -4.2598696 -4.2716155 -4.2771506 -4.2804041 -4.2816639 -4.2798305 -4.2723618 -4.2669663][-4.2344341 -4.223434 -4.2114353 -4.1923571 -4.1837568 -4.1901727 -4.2091069 -4.23232 -4.2504277 -4.2628264 -4.2716789 -4.2758789 -4.2764339 -4.2711287 -4.2673678][-4.2395091 -4.2240391 -4.2090073 -4.1877527 -4.1736884 -4.1702528 -4.1777887 -4.1965909 -4.2185493 -4.2381611 -4.2535763 -4.2612419 -4.2650418 -4.2638755 -4.2643733][-4.253603 -4.2338066 -4.2146807 -4.1919 -4.169229 -4.1521163 -4.1479216 -4.1569 -4.17717 -4.2020588 -4.2248716 -4.237606 -4.2454987 -4.2501926 -4.2569914][-4.2734237 -4.2517443 -4.23141 -4.2031808 -4.1703696 -4.1403427 -4.1239481 -4.12053 -4.13383 -4.1586485 -4.186583 -4.2068753 -4.22214 -4.2348475 -4.2486992][-4.2855625 -4.2704434 -4.2500582 -4.2156935 -4.1766677 -4.1455188 -4.1277452 -4.1176534 -4.1213183 -4.1361694 -4.1579838 -4.1811118 -4.2030597 -4.2242546 -4.2432709][-4.2751408 -4.2690392 -4.2561 -4.2234941 -4.1863766 -4.15979 -4.1492295 -4.1469412 -4.1503005 -4.1549635 -4.1639013 -4.1821265 -4.2036009 -4.2248278 -4.243432][-4.2475648 -4.248733 -4.2476034 -4.2279234 -4.2021732 -4.18212 -4.1813273 -4.1859856 -4.1895847 -4.19094 -4.1939111 -4.2053328 -4.2190938 -4.233417 -4.246419][-4.2192736 -4.2247005 -4.2370319 -4.2389879 -4.2318468 -4.2226195 -4.2271533 -4.2287669 -4.2272406 -4.225749 -4.2264614 -4.2307911 -4.2362108 -4.2417459 -4.2472153][-4.186903 -4.1953282 -4.2193255 -4.2411175 -4.2542171 -4.2573004 -4.2607446 -4.2570186 -4.25454 -4.2558289 -4.2527013 -4.2496181 -4.2477903 -4.2455769 -4.2450581][-4.1644783 -4.1744051 -4.2036781 -4.237184 -4.2638221 -4.2763352 -4.2789516 -4.2738571 -4.2752252 -4.2772713 -4.2677355 -4.2583776 -4.2524333 -4.2463918 -4.243145][-4.1614847 -4.1736 -4.205575 -4.2390261 -4.2657585 -4.2781491 -4.2817879 -4.2821331 -4.2846684 -4.2825937 -4.2685852 -4.2562284 -4.2476668 -4.2395077 -4.2359042][-4.1734562 -4.1923509 -4.2230968 -4.2474861 -4.2659864 -4.2728968 -4.2751322 -4.2741914 -4.2713819 -4.2605495 -4.243257 -4.2303047 -4.2192416 -4.2099171 -4.2070594][-4.1861796 -4.2072616 -4.2313952 -4.2439718 -4.25543 -4.2562165 -4.2529011 -4.2456975 -4.231154 -4.210434 -4.1920304 -4.1800642 -4.1706457 -4.1660762 -4.1698256]]...]
INFO - root - 2017-12-07 18:44:43.864167: step 18710, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.572 sec/batch; 64h:26m:06s remains)
INFO - root - 2017-12-07 18:45:00.269698: step 18720, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.599 sec/batch; 65h:31m:08s remains)
INFO - root - 2017-12-07 18:45:16.468075: step 18730, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.681 sec/batch; 68h:52m:04s remains)
INFO - root - 2017-12-07 18:45:32.683219: step 18740, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.584 sec/batch; 64h:55m:27s remains)
INFO - root - 2017-12-07 18:45:49.042842: step 18750, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 1.684 sec/batch; 68h:58m:59s remains)
INFO - root - 2017-12-07 18:46:05.260971: step 18760, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.636 sec/batch; 67h:01m:14s remains)
INFO - root - 2017-12-07 18:46:21.562402: step 18770, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.722 sec/batch; 70h:32m:12s remains)
INFO - root - 2017-12-07 18:46:37.819983: step 18780, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.571 sec/batch; 64h:21m:08s remains)
INFO - root - 2017-12-07 18:46:53.704119: step 18790, loss = 2.09, batch loss = 2.04 (9.7 examples/sec; 1.648 sec/batch; 67h:29m:39s remains)
INFO - root - 2017-12-07 18:47:09.992644: step 18800, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.594 sec/batch; 65h:16m:48s remains)
2017-12-07 18:47:11.441489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2562184 -4.2563195 -4.2552848 -4.2579136 -4.2643013 -4.2731929 -4.2778039 -4.2785339 -4.2759533 -4.2708564 -4.267272 -4.2646894 -4.2614512 -4.2541838 -4.2510247][-4.2611666 -4.2667665 -4.2654586 -4.2660828 -4.2692022 -4.2724776 -4.2737708 -4.2733703 -4.2705975 -4.2664366 -4.2624946 -4.2585135 -4.2520232 -4.2392588 -4.2303262][-4.2773943 -4.2828188 -4.2754278 -4.2684889 -4.2636662 -4.2577286 -4.25299 -4.2508435 -4.2503705 -4.2508359 -4.2510204 -4.2501287 -4.2453017 -4.2347207 -4.2268434][-4.2996197 -4.2994766 -4.2809238 -4.2600522 -4.2416 -4.2219548 -4.2036896 -4.1957412 -4.1986904 -4.20645 -4.2145853 -4.2220325 -4.2254939 -4.225368 -4.2262769][-4.3083029 -4.3012657 -4.270278 -4.2318025 -4.1911297 -4.1473565 -4.10774 -4.0924335 -4.1035094 -4.1222863 -4.1425281 -4.1630726 -4.1825576 -4.2010193 -4.2166309][-4.282465 -4.2737856 -4.2384839 -4.188282 -4.1252627 -4.0545506 -3.9902527 -3.9684153 -3.9969242 -4.0370955 -4.0737996 -4.1075492 -4.140244 -4.1769767 -4.2065153][-4.2470207 -4.24403 -4.216785 -4.1672678 -4.0938754 -4.0074463 -3.9302227 -3.9120853 -3.963172 -4.0232725 -4.0723896 -4.1122832 -4.1498723 -4.1960721 -4.2309318][-4.2180877 -4.2285671 -4.218195 -4.1826916 -4.121839 -4.0558686 -4.0027094 -3.9946582 -4.037766 -4.0872259 -4.1306114 -4.1652527 -4.1978979 -4.2378039 -4.2644877][-4.2048373 -4.2260718 -4.2312436 -4.2137656 -4.179944 -4.149724 -4.1307383 -4.1322532 -4.1565714 -4.1846871 -4.2117844 -4.2303286 -4.2481866 -4.2709956 -4.2819376][-4.2060785 -4.2248273 -4.2334709 -4.2281327 -4.2161961 -4.2130747 -4.2175555 -4.2287431 -4.2432981 -4.257566 -4.2722459 -4.2761812 -4.2782207 -4.2830358 -4.2806005][-4.2074604 -4.2161689 -4.2199121 -4.2180681 -4.2192941 -4.2342563 -4.2529569 -4.2673621 -4.2776303 -4.2867756 -4.29791 -4.2972026 -4.2927017 -4.2899137 -4.2821031][-4.2133555 -4.2114325 -4.2057791 -4.199554 -4.20479 -4.2265477 -4.2486172 -4.2605243 -4.2658076 -4.2728548 -4.2837949 -4.2869711 -4.2860689 -4.2870936 -4.2827845][-4.23325 -4.2247033 -4.2124 -4.2018776 -4.2034969 -4.2189693 -4.2337484 -4.23811 -4.2353168 -4.2355475 -4.2456279 -4.2559004 -4.2635522 -4.2737746 -4.2767253][-4.2721167 -4.2654023 -4.252955 -4.2389011 -4.2316389 -4.23559 -4.2400713 -4.2378368 -4.2259674 -4.2175913 -4.2216511 -4.2337537 -4.2447906 -4.2581644 -4.2661171][-4.2970982 -4.2961698 -4.2892876 -4.2775879 -4.2683887 -4.2656827 -4.26331 -4.256546 -4.2420387 -4.2305717 -4.2264571 -4.2321782 -4.2426968 -4.2557898 -4.266541]]...]
INFO - root - 2017-12-07 18:47:27.538908: step 18810, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.563 sec/batch; 64h:00m:23s remains)
INFO - root - 2017-12-07 18:47:43.532561: step 18820, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.638 sec/batch; 67h:05m:05s remains)
INFO - root - 2017-12-07 18:47:59.756979: step 18830, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.707 sec/batch; 69h:53m:23s remains)
INFO - root - 2017-12-07 18:48:16.164667: step 18840, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 1.494 sec/batch; 61h:11m:02s remains)
INFO - root - 2017-12-07 18:48:32.432218: step 18850, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.686 sec/batch; 69h:02m:48s remains)
INFO - root - 2017-12-07 18:48:48.612646: step 18860, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.558 sec/batch; 63h:46m:20s remains)
INFO - root - 2017-12-07 18:49:04.929577: step 18870, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 65h:30m:10s remains)
INFO - root - 2017-12-07 18:49:21.099044: step 18880, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.617 sec/batch; 66h:11m:31s remains)
INFO - root - 2017-12-07 18:49:37.393532: step 18890, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.579 sec/batch; 64h:38m:37s remains)
INFO - root - 2017-12-07 18:49:53.646844: step 18900, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.669 sec/batch; 68h:18m:05s remains)
2017-12-07 18:49:55.002982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2537622 -4.2610688 -4.2728734 -4.2805872 -4.2807317 -4.2758489 -4.2519112 -4.2138691 -4.1927953 -4.1979 -4.2083855 -4.2080841 -4.2013407 -4.1935029 -4.194334][-4.2553482 -4.2710304 -4.2891541 -4.2949009 -4.28627 -4.2671423 -4.2279572 -4.1768608 -4.1423035 -4.1381531 -4.1520996 -4.1614404 -4.1649647 -4.1596775 -4.1599317][-4.2560177 -4.2732964 -4.2891932 -4.2913003 -4.2748218 -4.2453613 -4.1972942 -4.1442904 -4.1116276 -4.1027117 -4.1147275 -4.1270347 -4.1350865 -4.1333141 -4.130888][-4.2557588 -4.2687044 -4.274941 -4.27109 -4.2464409 -4.212347 -4.1627669 -4.1177907 -4.1016631 -4.1036291 -4.1137395 -4.1229048 -4.1264167 -4.126 -4.1190286][-4.2469287 -4.2531815 -4.2489576 -4.2365856 -4.206152 -4.1639419 -4.1100097 -4.0716443 -4.0802431 -4.1092024 -4.1292534 -4.1332278 -4.1281719 -4.1233683 -4.1138144][-4.2369194 -4.2393379 -4.2241411 -4.1987529 -4.1585927 -4.1021132 -4.0291376 -3.976557 -4.0051866 -4.0698447 -4.1095314 -4.1162391 -4.1103687 -4.1067443 -4.1032529][-4.2377 -4.240839 -4.2206316 -4.1825347 -4.1224718 -4.0374641 -3.9218907 -3.8362546 -3.8815942 -3.9900086 -4.0582442 -4.0764117 -4.0771947 -4.083972 -4.0957451][-4.25116 -4.2566738 -4.2354612 -4.1926193 -4.1254835 -4.0191746 -3.8728628 -3.7653565 -3.8217747 -3.9558 -4.0438228 -4.0739574 -4.0832033 -4.0982738 -4.1212783][-4.2633262 -4.2743974 -4.2517772 -4.2123008 -4.1570477 -4.0648642 -3.9434245 -3.8594475 -3.9047661 -4.0170426 -4.0946784 -4.125967 -4.1374626 -4.1526613 -4.1748242][-4.2641907 -4.2809005 -4.261776 -4.2301016 -4.2015285 -4.1497955 -4.0821767 -4.0400381 -4.0645471 -4.1267805 -4.17283 -4.19367 -4.2017231 -4.2136688 -4.2297511][-4.2631478 -4.2829909 -4.2683845 -4.2463346 -4.2383671 -4.221961 -4.1961083 -4.1816196 -4.1948256 -4.2208915 -4.2385945 -4.2476468 -4.2539754 -4.2612824 -4.2697906][-4.2641826 -4.2847238 -4.2741437 -4.2605324 -4.2624135 -4.2640996 -4.2624354 -4.2628484 -4.2731423 -4.2773671 -4.2725773 -4.2714782 -4.2780271 -4.2844014 -4.2893696][-4.2760687 -4.2919874 -4.2842293 -4.276825 -4.2806311 -4.2880993 -4.2944379 -4.3014808 -4.3090849 -4.3053083 -4.292151 -4.2848263 -4.2883368 -4.2938881 -4.29703][-4.299243 -4.3100662 -4.3059926 -4.3010235 -4.3029423 -4.3081651 -4.3127184 -4.3168254 -4.3202639 -4.317306 -4.3080463 -4.3025188 -4.3050642 -4.3113103 -4.3139429][-4.3194213 -4.3244972 -4.32215 -4.319097 -4.3186121 -4.3199439 -4.3222446 -4.3253136 -4.3277769 -4.3274889 -4.324307 -4.3226337 -4.3246336 -4.3282981 -4.3298931]]...]
INFO - root - 2017-12-07 18:50:11.084563: step 18910, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.561 sec/batch; 63h:54m:17s remains)
INFO - root - 2017-12-07 18:50:27.436402: step 18920, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.591 sec/batch; 65h:05m:46s remains)
INFO - root - 2017-12-07 18:50:43.639709: step 18930, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.662 sec/batch; 68h:00m:03s remains)
INFO - root - 2017-12-07 18:50:59.792379: step 18940, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.634 sec/batch; 66h:50m:59s remains)
INFO - root - 2017-12-07 18:51:16.251584: step 18950, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.663 sec/batch; 68h:03m:20s remains)
INFO - root - 2017-12-07 18:51:32.370322: step 18960, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.576 sec/batch; 64h:30m:00s remains)
INFO - root - 2017-12-07 18:51:48.529091: step 18970, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.697 sec/batch; 69h:25m:00s remains)
INFO - root - 2017-12-07 18:52:04.890825: step 18980, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.622 sec/batch; 66h:21m:07s remains)
INFO - root - 2017-12-07 18:52:21.089329: step 18990, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 1.636 sec/batch; 66h:54m:32s remains)
INFO - root - 2017-12-07 18:52:37.327436: step 19000, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.566 sec/batch; 64h:02m:01s remains)
2017-12-07 18:52:38.744951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1030965 -4.0913081 -4.0819707 -4.0855718 -4.090652 -4.1002297 -4.1173959 -4.1504965 -4.1736536 -4.1728883 -4.1724396 -4.18596 -4.2084041 -4.23757 -4.254365][-4.1571503 -4.1377411 -4.1152396 -4.1018958 -4.094667 -4.0958333 -4.1094341 -4.136044 -4.1523609 -4.14885 -4.1491475 -4.1705065 -4.2030396 -4.2414932 -4.2661972][-4.2394342 -4.2269192 -4.2056727 -4.1869555 -4.176528 -4.1736341 -4.1830239 -4.1999745 -4.2068019 -4.1967826 -4.1897554 -4.205368 -4.233686 -4.26861 -4.292881][-4.3002839 -4.2981186 -4.2848306 -4.2696953 -4.26189 -4.2581482 -4.2621574 -4.271946 -4.2735424 -4.2622824 -4.252594 -4.2617245 -4.2817669 -4.3075457 -4.3267503][-4.3216081 -4.3245897 -4.3170214 -4.304688 -4.2970562 -4.292263 -4.2938538 -4.3005247 -4.2997675 -4.29094 -4.2837462 -4.2896805 -4.3033938 -4.3221178 -4.3387637][-4.288712 -4.2896605 -4.2826786 -4.2699909 -4.2594872 -4.2560844 -4.2605405 -4.2670417 -4.2665863 -4.26492 -4.2674322 -4.2749882 -4.2862864 -4.3027148 -4.3204784][-4.208106 -4.2010107 -4.1843286 -4.1613503 -4.1450219 -4.1483197 -4.1620197 -4.16916 -4.1666985 -4.1719446 -4.1890841 -4.2080779 -4.2270603 -4.2516413 -4.27825][-4.1173973 -4.1011171 -4.0703096 -4.0290976 -4.0046525 -4.0194492 -4.0498767 -4.0598984 -4.0510755 -4.056015 -4.0872445 -4.1220121 -4.1534252 -4.1882982 -4.2256541][-4.0967546 -4.0832877 -4.0522456 -4.0046706 -3.9746451 -3.991384 -4.0285811 -4.0388436 -4.0207539 -4.0159392 -4.048315 -4.0896606 -4.1264491 -4.162508 -4.2005463][-4.1467171 -4.1432953 -4.127213 -4.0936046 -4.0676322 -4.0755653 -4.1006393 -4.1039586 -4.0809922 -4.0672555 -4.0880332 -4.1240063 -4.1609774 -4.1928825 -4.2226744][-4.2144141 -4.2177062 -4.2151575 -4.1996036 -4.183435 -4.1831713 -4.1914296 -4.1872892 -4.1649017 -4.1438475 -4.1496234 -4.1774683 -4.2112885 -4.2390518 -4.2624125][-4.2547121 -4.2617145 -4.27013 -4.2713561 -4.2657418 -4.2605929 -4.2560692 -4.2472954 -4.2267432 -4.2030087 -4.1998363 -4.2213812 -4.2515841 -4.2775221 -4.2977681][-4.2509837 -4.2590408 -4.2718372 -4.2812567 -4.2809119 -4.2727594 -4.2618175 -4.2519646 -4.2351236 -4.2138147 -4.2087097 -4.2288337 -4.2595787 -4.2904038 -4.3143573][-4.2273097 -4.2343507 -4.245471 -4.2576795 -4.2600865 -4.2520781 -4.24381 -4.2381096 -4.2248254 -4.2069545 -4.2035089 -4.221014 -4.2499747 -4.2840443 -4.31336][-4.20041 -4.2071905 -4.2142982 -4.22598 -4.2311954 -4.2296414 -4.2302971 -4.2318187 -4.2262921 -4.2169123 -4.2193742 -4.2334347 -4.254108 -4.2822 -4.3099532]]...]
INFO - root - 2017-12-07 18:52:54.994986: step 19010, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.544 sec/batch; 63h:10m:04s remains)
INFO - root - 2017-12-07 18:53:11.345831: step 19020, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 1.721 sec/batch; 70h:22m:52s remains)
INFO - root - 2017-12-07 18:53:27.614409: step 19030, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.577 sec/batch; 64h:30m:32s remains)
INFO - root - 2017-12-07 18:53:44.081698: step 19040, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.641 sec/batch; 67h:06m:47s remains)
INFO - root - 2017-12-07 18:54:00.299978: step 19050, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.687 sec/batch; 68h:58m:08s remains)
INFO - root - 2017-12-07 18:54:16.619815: step 19060, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.547 sec/batch; 63h:14m:31s remains)
INFO - root - 2017-12-07 18:54:32.952868: step 19070, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.692 sec/batch; 69h:10m:08s remains)
INFO - root - 2017-12-07 18:54:49.066492: step 19080, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.589 sec/batch; 64h:57m:08s remains)
INFO - root - 2017-12-07 18:55:05.338933: step 19090, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.613 sec/batch; 65h:55m:34s remains)
INFO - root - 2017-12-07 18:55:21.813773: step 19100, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.688 sec/batch; 68h:59m:03s remains)
2017-12-07 18:55:23.280813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31436 -4.3068037 -4.3029704 -4.2957115 -4.2866526 -4.2785087 -4.2735729 -4.2676105 -4.252286 -4.2204556 -4.1789117 -4.1514835 -4.1537232 -4.1803651 -4.2221489][-4.3178148 -4.3157253 -4.3149176 -4.3121076 -4.3085914 -4.3061814 -4.3066435 -4.3100314 -4.3086529 -4.2929244 -4.2622685 -4.2335987 -4.2211576 -4.2265549 -4.2487135][-4.2941294 -4.2957253 -4.2955356 -4.2931237 -4.2938137 -4.2961168 -4.3022532 -4.3154554 -4.327538 -4.327065 -4.3114371 -4.2908015 -4.2747307 -4.2687068 -4.27698][-4.2721319 -4.2729659 -4.2672811 -4.2601876 -4.2593746 -4.2606277 -4.2666664 -4.2853646 -4.3089743 -4.3246484 -4.3249183 -4.3190246 -4.3101778 -4.3029127 -4.3033962][-4.2617955 -4.2576571 -4.241406 -4.2247944 -4.2163472 -4.2074156 -4.2019138 -4.2183151 -4.2535987 -4.2872853 -4.3065262 -4.3177867 -4.3230734 -4.3223581 -4.3222232][-4.2630229 -4.251915 -4.22693 -4.200551 -4.1805081 -4.1497812 -4.1155276 -4.118351 -4.163208 -4.2173328 -4.2565422 -4.2862935 -4.3100367 -4.3228889 -4.3290768][-4.2748179 -4.2556362 -4.2258248 -4.193687 -4.1617036 -4.1066918 -4.0359144 -4.0105567 -4.0544639 -4.1256981 -4.1860881 -4.23583 -4.2787371 -4.3088055 -4.3249903][-4.2893167 -4.2706938 -4.2414985 -4.2074008 -4.1704292 -4.104578 -4.0119429 -3.9528897 -3.9699416 -4.0339651 -4.1071029 -4.17413 -4.2355576 -4.2828507 -4.310041][-4.2951779 -4.2851243 -4.2655168 -4.2382822 -4.2087646 -4.15521 -4.0701804 -3.9941103 -3.9684463 -3.9951866 -4.0576549 -4.1281905 -4.1992655 -4.2588406 -4.2953973][-4.287982 -4.2878232 -4.2811689 -4.2679152 -4.2548695 -4.2239618 -4.164412 -4.0962658 -4.0475802 -4.0375314 -4.06914 -4.1225004 -4.1879916 -4.248735 -4.2880812][-4.2752719 -4.2783842 -4.2812848 -4.2830381 -4.2882028 -4.2809143 -4.2500134 -4.2035089 -4.15907 -4.1329417 -4.1355066 -4.1583786 -4.2032642 -4.2521749 -4.286622][-4.2633376 -4.2617788 -4.2636256 -4.27459 -4.2952747 -4.3095202 -4.3057785 -4.2844281 -4.2558265 -4.2281895 -4.2130022 -4.2117281 -4.2350826 -4.2676096 -4.292881][-4.2573481 -4.2443318 -4.23892 -4.2532148 -4.2856727 -4.3148885 -4.3269606 -4.3237185 -4.3100405 -4.2870073 -4.2654305 -4.253171 -4.2635193 -4.2836075 -4.3021364][-4.2510309 -4.2252588 -4.2140632 -4.2298374 -4.2655787 -4.2982149 -4.3139367 -4.3184266 -4.3154225 -4.300262 -4.2801228 -4.2669959 -4.2762613 -4.2937155 -4.3105779][-4.2459822 -4.2146945 -4.1996183 -4.2133818 -4.2435913 -4.2670212 -4.2756295 -4.28039 -4.2836657 -4.2770343 -4.2613993 -4.2538595 -4.26976 -4.2914271 -4.31165]]...]
INFO - root - 2017-12-07 18:55:39.722299: step 19110, loss = 2.06, batch loss = 2.01 (10.3 examples/sec; 1.552 sec/batch; 63h:26m:17s remains)
INFO - root - 2017-12-07 18:55:55.760440: step 19120, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.582 sec/batch; 64h:38m:12s remains)
INFO - root - 2017-12-07 18:56:11.843388: step 19130, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.577 sec/batch; 64h:26m:32s remains)
INFO - root - 2017-12-07 18:56:28.099331: step 19140, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.550 sec/batch; 63h:20m:48s remains)
INFO - root - 2017-12-07 18:56:44.402057: step 19150, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.696 sec/batch; 69h:18m:01s remains)
INFO - root - 2017-12-07 18:57:00.566773: step 19160, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 65h:21m:35s remains)
INFO - root - 2017-12-07 18:57:16.837693: step 19170, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.531 sec/batch; 62h:33m:39s remains)
INFO - root - 2017-12-07 18:57:33.121146: step 19180, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.619 sec/batch; 66h:07m:19s remains)
INFO - root - 2017-12-07 18:57:49.163059: step 19190, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.600 sec/batch; 65h:21m:04s remains)
INFO - root - 2017-12-07 18:58:05.576805: step 19200, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.731 sec/batch; 70h:42m:30s remains)
2017-12-07 18:58:06.933091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23929 -4.2511358 -4.2664976 -4.2788043 -4.2797494 -4.2796636 -4.28372 -4.2855673 -4.2793565 -4.2705884 -4.2645688 -4.2620568 -4.2612538 -4.2606068 -4.2597628][-4.184154 -4.1941767 -4.2125363 -4.2340302 -4.2485991 -4.2643042 -4.2822995 -4.2930098 -4.2911992 -4.2860408 -4.2830243 -4.2804375 -4.2743034 -4.2647815 -4.2569919][-4.1425157 -4.14009 -4.1445613 -4.1622429 -4.184751 -4.2121425 -4.2447147 -4.2674732 -4.2758884 -4.2817149 -4.2889609 -4.2914734 -4.283658 -4.2679863 -4.2526608][-4.1424522 -4.1220474 -4.10134 -4.1037297 -4.1235948 -4.153338 -4.1937022 -4.2271066 -4.2459912 -4.2629552 -4.2824535 -4.2934532 -4.2866912 -4.2679334 -4.2484708][-4.1799307 -4.1429858 -4.1021981 -4.0890012 -4.1018572 -4.1245494 -4.1593466 -4.1925826 -4.216702 -4.2404718 -4.2688808 -4.2883787 -4.2841105 -4.2640357 -4.2451072][-4.2356415 -4.2027097 -4.1635394 -4.142405 -4.1409683 -4.1467047 -4.163868 -4.1859465 -4.2069535 -4.2317572 -4.26477 -4.2902389 -4.2892127 -4.2713552 -4.2564111][-4.2738185 -4.2553406 -4.2261362 -4.20009 -4.1870041 -4.1820736 -4.1891441 -4.2062182 -4.2249041 -4.2465158 -4.2764006 -4.3016233 -4.3028278 -4.2891111 -4.2783][-4.2826991 -4.2783394 -4.2563891 -4.2296047 -4.2149596 -4.2101336 -4.2163277 -4.23557 -4.2544522 -4.2705412 -4.2919526 -4.3132515 -4.3173614 -4.3098311 -4.30391][-4.2601213 -4.267868 -4.2558413 -4.2387214 -4.2351727 -4.2380238 -4.2468495 -4.2686591 -4.2864943 -4.2968416 -4.3086257 -4.3228183 -4.3261447 -4.3198619 -4.3142481][-4.2122326 -4.2222061 -4.2178984 -4.2143593 -4.2296252 -4.2465954 -4.2605724 -4.28344 -4.3020492 -4.3114672 -4.3187423 -4.32733 -4.327507 -4.3193059 -4.3105111][-4.1542907 -4.1633439 -4.164443 -4.1700864 -4.2014952 -4.2358942 -4.2591453 -4.2826271 -4.3023534 -4.3129678 -4.3185329 -4.3230081 -4.3204269 -4.3106508 -4.2979703][-4.1082592 -4.1116 -4.1207657 -4.1341858 -4.1671371 -4.2049508 -4.2332406 -4.2598271 -4.2850156 -4.3013153 -4.3071437 -4.3103175 -4.307126 -4.2977228 -4.28319][-4.1247067 -4.1130605 -4.1178741 -4.1286545 -4.1510248 -4.18092 -4.2096066 -4.2385306 -4.2696881 -4.2927575 -4.3012905 -4.3057327 -4.3047862 -4.2967215 -4.2795186][-4.1792688 -4.1577425 -4.1536894 -4.1576314 -4.1695304 -4.1899953 -4.2113271 -4.2342372 -4.2619238 -4.2838244 -4.2920141 -4.2967553 -4.2979274 -4.2895494 -4.2684212][-4.245501 -4.2287269 -4.2228308 -4.2204533 -4.2245994 -4.235343 -4.2464104 -4.2561707 -4.2707825 -4.2841821 -4.2878737 -4.2904768 -4.2923317 -4.2845116 -4.2619]]...]
INFO - root - 2017-12-07 18:58:23.175813: step 19210, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.672 sec/batch; 68h:18m:08s remains)
INFO - root - 2017-12-07 18:58:39.582146: step 19220, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.588 sec/batch; 64h:50m:58s remains)
INFO - root - 2017-12-07 18:58:56.021804: step 19230, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.690 sec/batch; 69h:01m:33s remains)
INFO - root - 2017-12-07 18:59:12.237978: step 19240, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.577 sec/batch; 64h:23m:06s remains)
INFO - root - 2017-12-07 18:59:28.557873: step 19250, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.645 sec/batch; 67h:10m:49s remains)
INFO - root - 2017-12-07 18:59:44.842729: step 19260, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.585 sec/batch; 64h:42m:23s remains)
INFO - root - 2017-12-07 19:00:01.171554: step 19270, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 1.496 sec/batch; 61h:04m:53s remains)
INFO - root - 2017-12-07 19:00:17.280075: step 19280, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.661 sec/batch; 67h:47m:30s remains)
INFO - root - 2017-12-07 19:00:33.358816: step 19290, loss = 2.08, batch loss = 2.03 (10.5 examples/sec; 1.521 sec/batch; 62h:04m:23s remains)
INFO - root - 2017-12-07 19:00:49.714161: step 19300, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 1.657 sec/batch; 67h:38m:22s remains)
2017-12-07 19:00:51.072356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3292336 -4.330874 -4.3258305 -4.3200097 -4.3229461 -4.3296437 -4.3297772 -4.3220143 -4.3113894 -4.3004041 -4.2955313 -4.299365 -4.3096719 -4.3210073 -4.3293443][-4.3399391 -4.3383069 -4.3312025 -4.3241572 -4.3243604 -4.3269377 -4.3252554 -4.3156133 -4.2966743 -4.2750268 -4.2670517 -4.2778339 -4.2976656 -4.3150349 -4.3211536][-4.3461852 -4.3435764 -4.3347812 -4.3255825 -4.3219504 -4.3171053 -4.3080297 -4.2891717 -4.2575026 -4.2228866 -4.2160492 -4.2426677 -4.2781444 -4.3055458 -4.3115292][-4.3488369 -4.3470659 -4.3376956 -4.3238225 -4.3119879 -4.2966843 -4.2764215 -4.2458577 -4.207345 -4.1735144 -4.1763277 -4.2155604 -4.2586794 -4.2885871 -4.2951469][-4.3459148 -4.3413663 -4.3273025 -4.3072643 -4.2882795 -4.2638178 -4.2334762 -4.1974111 -4.1668253 -4.152595 -4.1725216 -4.2145743 -4.2506075 -4.2708764 -4.271245][-4.3353276 -4.3246574 -4.3006191 -4.2702436 -4.2366161 -4.1955175 -4.15292 -4.1264772 -4.1302667 -4.1512914 -4.1899333 -4.2311764 -4.257504 -4.2637739 -4.2504354][-4.324728 -4.3062463 -4.269248 -4.2228284 -4.1656733 -4.0983925 -4.0409646 -4.0380311 -4.0970874 -4.1625214 -4.2152214 -4.250073 -4.2701068 -4.2655048 -4.2408037][-4.3121181 -4.28822 -4.2425179 -4.1841021 -4.1109662 -4.0324383 -3.9739103 -4.000247 -4.1032858 -4.1921148 -4.2448754 -4.2671971 -4.2757525 -4.2587838 -4.2256584][-4.2915158 -4.2686071 -4.2247543 -4.172277 -4.110486 -4.0592365 -4.037262 -4.0777407 -4.1678915 -4.2370405 -4.2725997 -4.2762628 -4.26336 -4.2309771 -4.1873927][-4.2605681 -4.2419372 -4.2016034 -4.1646318 -4.1312065 -4.1235566 -4.1415558 -4.1840982 -4.2413712 -4.275929 -4.28782 -4.2717156 -4.2393346 -4.1924481 -4.1384907][-4.2246466 -4.2064638 -4.1595664 -4.1263909 -4.1230927 -4.1594148 -4.2064734 -4.2468872 -4.2833252 -4.29694 -4.2897134 -4.2608995 -4.2224178 -4.1696215 -4.1158795][-4.1990552 -4.1772232 -4.1246862 -4.0931163 -4.1120496 -4.1755319 -4.2375722 -4.2751575 -4.3018346 -4.3043218 -4.2829494 -4.2508259 -4.2164955 -4.1741624 -4.137742][-4.1974406 -4.181725 -4.1449261 -4.1216059 -4.1361661 -4.1909513 -4.2484603 -4.2832818 -4.3055234 -4.3025208 -4.2768135 -4.2476988 -4.2208433 -4.196857 -4.1791229][-4.2249417 -4.2232089 -4.2060852 -4.1858096 -4.1811934 -4.210216 -4.2496161 -4.2788906 -4.2997451 -4.2996211 -4.279695 -4.2594304 -4.2384071 -4.2247758 -4.2172174][-4.2613206 -4.2645483 -4.25367 -4.23216 -4.2171912 -4.2289863 -4.2531347 -4.276269 -4.2950225 -4.2991109 -4.2853475 -4.2725792 -4.256948 -4.2488952 -4.2450972]]...]
INFO - root - 2017-12-07 19:01:07.074513: step 19310, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.691 sec/batch; 69h:00m:06s remains)
INFO - root - 2017-12-07 19:01:23.359679: step 19320, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.608 sec/batch; 65h:36m:43s remains)
INFO - root - 2017-12-07 19:01:39.740868: step 19330, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.689 sec/batch; 68h:56m:09s remains)
INFO - root - 2017-12-07 19:01:55.989142: step 19340, loss = 2.10, batch loss = 2.05 (10.1 examples/sec; 1.585 sec/batch; 64h:40m:38s remains)
INFO - root - 2017-12-07 19:02:12.057328: step 19350, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 1.706 sec/batch; 69h:37m:51s remains)
INFO - root - 2017-12-07 19:02:28.196314: step 19360, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.601 sec/batch; 65h:19m:28s remains)
INFO - root - 2017-12-07 19:02:44.407913: step 19370, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 1.612 sec/batch; 65h:45m:35s remains)
INFO - root - 2017-12-07 19:03:00.627075: step 19380, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.638 sec/batch; 66h:49m:50s remains)
INFO - root - 2017-12-07 19:03:16.769476: step 19390, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.536 sec/batch; 62h:40m:48s remains)
INFO - root - 2017-12-07 19:03:33.075769: step 19400, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.557 sec/batch; 63h:30m:01s remains)
2017-12-07 19:03:34.484351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3023376 -4.3002143 -4.2912154 -4.2860327 -4.2884154 -4.2988272 -4.3046227 -4.30414 -4.3063927 -4.3101358 -4.31262 -4.3144088 -4.3157544 -4.31984 -4.3215466][-4.303391 -4.3009644 -4.2951035 -4.2933784 -4.2963033 -4.304069 -4.3063879 -4.3019438 -4.3017511 -4.3057394 -4.3102865 -4.3134546 -4.3155761 -4.3168554 -4.3162794][-4.2907834 -4.2879806 -4.2876716 -4.2924681 -4.300056 -4.3065739 -4.305285 -4.2981219 -4.2959766 -4.2984953 -4.3028092 -4.3055315 -4.3055787 -4.298944 -4.292737][-4.270277 -4.2670135 -4.2695932 -4.279778 -4.2923656 -4.2985716 -4.2935314 -4.2849736 -4.2828689 -4.2853765 -4.2907076 -4.2930121 -4.2885728 -4.2719245 -4.2574534][-4.2539396 -4.2446485 -4.2457404 -4.2559686 -4.2679343 -4.2714758 -4.2633462 -4.2557693 -4.2580414 -4.2638621 -4.2691617 -4.2705584 -4.2636395 -4.2423849 -4.2230964][-4.2397161 -4.22133 -4.2151217 -4.2193122 -4.2258596 -4.2242093 -4.2123785 -4.2081923 -4.2178078 -4.2284036 -4.235652 -4.2379251 -4.2326994 -4.2147446 -4.198935][-4.2274914 -4.1980648 -4.1819553 -4.176115 -4.174242 -4.16533 -4.1514916 -4.1533074 -4.1715622 -4.1887865 -4.1979508 -4.2002006 -4.1968522 -4.1855392 -4.1788278][-4.229393 -4.1929488 -4.1698751 -4.1561108 -4.1442156 -4.1276479 -4.1116819 -4.1135306 -4.13194 -4.1518888 -4.16172 -4.1619024 -4.1565776 -4.1504822 -4.1550918][-4.2481189 -4.2128778 -4.1890826 -4.1758485 -4.1648164 -4.151999 -4.1412029 -4.1362057 -4.1421275 -4.1538868 -4.158 -4.1501122 -4.1363983 -4.1310945 -4.1417742][-4.2693119 -4.2411942 -4.223084 -4.2145724 -4.2103076 -4.20463 -4.1974349 -4.1876311 -4.18343 -4.1898341 -4.1902194 -4.1757011 -4.157692 -4.1519952 -4.1599593][-4.2839541 -4.2664804 -4.2557058 -4.2504573 -4.25022 -4.248601 -4.2421808 -4.2294717 -4.2211175 -4.2269964 -4.2280245 -4.2127118 -4.1981735 -4.1961122 -4.1990814][-4.2904453 -4.2843137 -4.2811437 -4.2788324 -4.2784104 -4.2765913 -4.2691646 -4.2537374 -4.2429228 -4.2494745 -4.2510815 -4.2375693 -4.228982 -4.2308092 -4.2298903][-4.2967477 -4.2971191 -4.296638 -4.2939758 -4.2903185 -4.2870641 -4.2824616 -4.2690372 -4.2605057 -4.2673435 -4.2681022 -4.256309 -4.2520447 -4.2555108 -4.2524195][-4.3025527 -4.3044257 -4.302279 -4.2964892 -4.2898378 -4.2874269 -4.286623 -4.2773 -4.2724233 -4.2782645 -4.2781754 -4.2676587 -4.2651358 -4.2688203 -4.2652869][-4.3042846 -4.3050327 -4.2997961 -4.2918568 -4.2876563 -4.2904172 -4.29403 -4.2878752 -4.2847052 -4.285553 -4.2814708 -4.2716975 -4.2722154 -4.2774439 -4.2745495]]...]
INFO - root - 2017-12-07 19:03:50.494939: step 19410, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.629 sec/batch; 66h:27m:35s remains)
INFO - root - 2017-12-07 19:04:06.540851: step 19420, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 1.450 sec/batch; 59h:07m:59s remains)
INFO - root - 2017-12-07 19:04:22.801672: step 19430, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.578 sec/batch; 64h:20m:20s remains)
INFO - root - 2017-12-07 19:04:39.232606: step 19440, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.722 sec/batch; 70h:14m:24s remains)
INFO - root - 2017-12-07 19:04:55.518791: step 19450, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.655 sec/batch; 67h:29m:44s remains)
INFO - root - 2017-12-07 19:05:11.712127: step 19460, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.654 sec/batch; 67h:25m:32s remains)
INFO - root - 2017-12-07 19:05:28.008824: step 19470, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 1.639 sec/batch; 66h:50m:44s remains)
INFO - root - 2017-12-07 19:05:44.273123: step 19480, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.619 sec/batch; 66h:01m:18s remains)
INFO - root - 2017-12-07 19:06:00.281800: step 19490, loss = 2.05, batch loss = 2.00 (10.4 examples/sec; 1.532 sec/batch; 62h:26m:51s remains)
INFO - root - 2017-12-07 19:06:16.594997: step 19500, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.597 sec/batch; 65h:06m:34s remains)
2017-12-07 19:06:17.899768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.243856 -4.2421241 -4.2170649 -4.2045436 -4.2065115 -4.2171588 -4.2360344 -4.2609062 -4.2713408 -4.2366385 -4.1619091 -4.1017704 -4.104146 -4.1550608 -4.2167983][-4.2521415 -4.2387762 -4.2103014 -4.2010341 -4.2025237 -4.2031188 -4.2141304 -4.2353888 -4.24724 -4.2172513 -4.1566978 -4.10646 -4.1120987 -4.1583114 -4.2146578][-4.2558985 -4.2392249 -4.2115035 -4.1992903 -4.1919727 -4.1781187 -4.1768174 -4.1938014 -4.209795 -4.1930161 -4.1517038 -4.1186581 -4.1277928 -4.1657281 -4.211741][-4.2590709 -4.2462153 -4.2219853 -4.2046375 -4.1885824 -4.1611915 -4.1428385 -4.1542592 -4.1816134 -4.181313 -4.1562142 -4.1376863 -4.1468863 -4.1742368 -4.2106771][-4.2612109 -4.2534251 -4.2329669 -4.2141542 -4.194406 -4.1555281 -4.1157179 -4.1186738 -4.1627398 -4.1782031 -4.1622767 -4.1529818 -4.1626148 -4.1852837 -4.2159724][-4.2585258 -4.2518539 -4.2337604 -4.2156596 -4.1944966 -4.14258 -4.0778103 -4.0692134 -4.1312761 -4.1662145 -4.15859 -4.1544719 -4.166328 -4.19129 -4.2223129][-4.2528338 -4.2483897 -4.2327623 -4.2116251 -4.182168 -4.1105475 -4.0158815 -3.9899435 -4.0761728 -4.1401305 -4.139956 -4.1378775 -4.1522098 -4.1817174 -4.2156043][-4.2489533 -4.245542 -4.232748 -4.2108927 -4.1703534 -4.0736227 -3.9428809 -3.8865986 -3.9968433 -4.0969777 -4.1156354 -4.1206183 -4.1383467 -4.16981 -4.2048292][-4.2614379 -4.2565532 -4.2469716 -4.229537 -4.18892 -4.0887461 -3.9477835 -3.8687234 -3.9724813 -4.0853267 -4.1161766 -4.12727 -4.1477594 -4.1780939 -4.2099786][-4.2874804 -4.279974 -4.2709422 -4.2593594 -4.2312603 -4.1565437 -4.045588 -3.9727683 -4.0386267 -4.1271834 -4.1493826 -4.1554213 -4.1748443 -4.2031603 -4.2309685][-4.307426 -4.2985797 -4.2859526 -4.274579 -4.25604 -4.2091794 -4.1361961 -4.0797205 -4.1143522 -4.1739631 -4.176651 -4.1712818 -4.189249 -4.2194839 -4.2480593][-4.3174739 -4.3093719 -4.2937622 -4.2794685 -4.2654824 -4.237083 -4.1930523 -4.1532617 -4.170711 -4.2099504 -4.198051 -4.180131 -4.1931314 -4.222743 -4.25215][-4.3169432 -4.3124409 -4.2987761 -4.2845078 -4.2720428 -4.251359 -4.2222037 -4.1964607 -4.20695 -4.2350483 -4.216579 -4.1911511 -4.198307 -4.2245793 -4.2508407][-4.3174534 -4.3169737 -4.3101878 -4.2983637 -4.2845459 -4.2664471 -4.2438712 -4.2266488 -4.2365327 -4.2592239 -4.2402563 -4.2139926 -4.2138586 -4.2332897 -4.2538886][-4.3277764 -4.3316164 -4.329617 -4.3205962 -4.3064418 -4.2887635 -4.2710991 -4.2598715 -4.2694879 -4.2865381 -4.2698445 -4.2457318 -4.24016 -4.2517686 -4.2659969]]...]
INFO - root - 2017-12-07 19:06:34.200818: step 19510, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.673 sec/batch; 68h:11m:25s remains)
INFO - root - 2017-12-07 19:06:50.290912: step 19520, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 1.520 sec/batch; 61h:56m:55s remains)
INFO - root - 2017-12-07 19:07:06.437692: step 19530, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.558 sec/batch; 63h:29m:03s remains)
INFO - root - 2017-12-07 19:07:22.281130: step 19540, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.593 sec/batch; 64h:55m:59s remains)
INFO - root - 2017-12-07 19:07:38.618915: step 19550, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.568 sec/batch; 63h:53m:00s remains)
INFO - root - 2017-12-07 19:07:55.096838: step 19560, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.659 sec/batch; 67h:35m:48s remains)
INFO - root - 2017-12-07 19:08:11.303859: step 19570, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.572 sec/batch; 64h:04m:09s remains)
INFO - root - 2017-12-07 19:08:27.683993: step 19580, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.704 sec/batch; 69h:26m:30s remains)
INFO - root - 2017-12-07 19:08:43.720937: step 19590, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.572 sec/batch; 64h:02m:31s remains)
INFO - root - 2017-12-07 19:08:59.994698: step 19600, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.677 sec/batch; 68h:17m:49s remains)
2017-12-07 19:09:01.397720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.266614 -4.2555294 -4.2476969 -4.2449541 -4.2462287 -4.2500291 -4.2541213 -4.2556429 -4.2534542 -4.2429934 -4.2236738 -4.2041874 -4.1958947 -4.2078986 -4.2320628][-4.2395449 -4.2345605 -4.2340927 -4.2368112 -4.2418203 -4.2479606 -4.2527595 -4.2512617 -4.2454996 -4.2315955 -4.2064667 -4.1772852 -4.1609316 -4.1734581 -4.2049389][-4.2208414 -4.2248483 -4.2332668 -4.2412462 -4.2485695 -4.2554412 -4.25972 -4.2569742 -4.2503943 -4.2370615 -4.2104526 -4.175087 -4.1535854 -4.1653914 -4.2005033][-4.2164631 -4.2295671 -4.24357 -4.2519908 -4.2561665 -4.2592111 -4.2605867 -4.2565331 -4.250968 -4.243114 -4.2229056 -4.1892939 -4.1690397 -4.1804562 -4.2168937][-4.2189322 -4.2381783 -4.2538595 -4.2603321 -4.2597218 -4.2562943 -4.2506361 -4.2428322 -4.2380252 -4.2361159 -4.2240725 -4.1981783 -4.1840839 -4.1965103 -4.2302623][-4.2259398 -4.24846 -4.2650928 -4.2697277 -4.2633739 -4.2526021 -4.241991 -4.2328229 -4.2283964 -4.2279749 -4.2189131 -4.1997561 -4.1930814 -4.2062626 -4.2337565][-4.2310929 -4.2537341 -4.2704511 -4.2735267 -4.2640843 -4.2529688 -4.2449055 -4.2375979 -4.2325411 -4.2301769 -4.2207665 -4.2055221 -4.20315 -4.2157 -4.2368455][-4.2292914 -4.2500949 -4.2657609 -4.2688746 -4.2613297 -4.2540865 -4.2510185 -4.2463589 -4.2417588 -4.2390056 -4.2297978 -4.2168851 -4.2170587 -4.2272468 -4.2420249][-4.2257824 -4.2416005 -4.2558212 -4.2599363 -4.2556558 -4.2522812 -4.2537184 -4.2527013 -4.2497211 -4.2458696 -4.2355304 -4.2231703 -4.2229242 -4.23071 -4.2418666][-4.2247534 -4.2341423 -4.2464213 -4.2516189 -4.2494249 -4.2496767 -4.25546 -4.2594914 -4.2586546 -4.2526402 -4.2407951 -4.2266917 -4.2226596 -4.2283683 -4.2378969][-4.2215285 -4.2262397 -4.23847 -4.2458715 -4.2461333 -4.2475648 -4.255918 -4.2647171 -4.2677627 -4.2634673 -4.2515831 -4.2357879 -4.2278776 -4.2315955 -4.2406268][-4.222661 -4.2244635 -4.2343664 -4.2419062 -4.2429996 -4.243751 -4.2525868 -4.2643309 -4.2722588 -4.2739868 -4.2665234 -4.2504048 -4.2392612 -4.2398281 -4.2466507][-4.2385211 -4.236588 -4.2402134 -4.2438293 -4.2433062 -4.242991 -4.250453 -4.2624207 -4.2728186 -4.2794771 -4.2764835 -4.2617221 -4.2487698 -4.2453179 -4.2465868][-4.2568121 -4.2499785 -4.2459874 -4.2440991 -4.2422247 -4.2430377 -4.2505026 -4.2613482 -4.2719584 -4.2803731 -4.2797794 -4.267776 -4.2545581 -4.2471046 -4.2412281][-4.2611074 -4.2502823 -4.2418051 -4.2370439 -4.2363315 -4.2396479 -4.2472973 -4.2576213 -4.2688656 -4.2781296 -4.2792907 -4.2710958 -4.2592516 -4.2493877 -4.2395086]]...]
INFO - root - 2017-12-07 19:09:17.648424: step 19610, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.649 sec/batch; 67h:09m:31s remains)
INFO - root - 2017-12-07 19:09:33.668003: step 19620, loss = 2.09, batch loss = 2.03 (10.6 examples/sec; 1.510 sec/batch; 61h:30m:06s remains)
INFO - root - 2017-12-07 19:09:50.031821: step 19630, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.546 sec/batch; 62h:57m:53s remains)
INFO - root - 2017-12-07 19:10:06.247834: step 19640, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.706 sec/batch; 69h:29m:38s remains)
INFO - root - 2017-12-07 19:10:22.513923: step 19650, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.583 sec/batch; 64h:27m:21s remains)
INFO - root - 2017-12-07 19:10:38.668334: step 19660, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.610 sec/batch; 65h:33m:30s remains)
INFO - root - 2017-12-07 19:10:55.078116: step 19670, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.685 sec/batch; 68h:36m:13s remains)
INFO - root - 2017-12-07 19:11:11.374852: step 19680, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 64h:16m:50s remains)
INFO - root - 2017-12-07 19:11:27.688771: step 19690, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.676 sec/batch; 68h:14m:11s remains)
INFO - root - 2017-12-07 19:11:43.681606: step 19700, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.578 sec/batch; 64h:13m:15s remains)
2017-12-07 19:11:45.168585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3492088 -4.3509512 -4.3508415 -4.3506842 -4.3507409 -4.35057 -4.3503532 -4.3485861 -4.343843 -4.33717 -4.3303471 -4.32655 -4.3279433 -4.3347788 -4.3450804][-4.3564754 -4.3582368 -4.3566766 -4.3532162 -4.3487606 -4.3428378 -4.3372383 -4.3325825 -4.3270574 -4.3210907 -4.3155518 -4.3135934 -4.3175573 -4.3268313 -4.3412008][-4.3640528 -4.3642578 -4.3597012 -4.3518419 -4.3412404 -4.3267121 -4.3109288 -4.2971787 -4.2867322 -4.28074 -4.2784944 -4.2806382 -4.2897515 -4.3048191 -4.3277259][-4.3678827 -4.3650064 -4.3542886 -4.3376336 -4.3168311 -4.2900019 -4.2590265 -4.2313838 -4.2156267 -4.2139692 -4.220839 -4.2322931 -4.2495461 -4.2719603 -4.3039207][-4.3675442 -4.3611093 -4.3402019 -4.3088555 -4.2712712 -4.2263088 -4.1775036 -4.1345773 -4.1188025 -4.1343679 -4.1628804 -4.1897712 -4.2142673 -4.240921 -4.2782173][-4.3602438 -4.3462343 -4.311368 -4.2617774 -4.2035079 -4.1349554 -4.0624461 -3.9965367 -3.9860423 -4.0352983 -4.0984888 -4.1464205 -4.1766286 -4.2066092 -4.2507625][-4.3474603 -4.3210988 -4.2673225 -4.1929851 -4.1066628 -4.0037074 -3.8912063 -3.7855675 -3.7886462 -3.8880448 -3.9974878 -4.0768342 -4.125526 -4.1699505 -4.2254653][-4.3318472 -4.2943239 -4.221868 -4.1182494 -3.9937031 -3.8405919 -3.6683004 -3.5079241 -3.5382979 -3.7118511 -3.8914735 -4.0214915 -4.1038871 -4.1678209 -4.228415][-4.3211269 -4.27758 -4.1972184 -4.0765004 -3.9265511 -3.7420492 -3.5386367 -3.3620892 -3.4314828 -3.6632738 -3.8880353 -4.0484986 -4.1496229 -4.216289 -4.2688346][-4.3209629 -4.2794418 -4.2089376 -4.1031723 -3.9748726 -3.8272438 -3.6827075 -3.5814793 -3.6593213 -3.8490558 -4.0298038 -4.1616426 -4.2448387 -4.29185 -4.322917][-4.3296566 -4.2977834 -4.2465744 -4.1723537 -4.0865903 -3.997648 -3.9222927 -3.8859062 -3.9492295 -4.0683222 -4.1801491 -4.2659988 -4.3204412 -4.3479 -4.3605409][-4.3439069 -4.3249321 -4.2953358 -4.2531738 -4.2056971 -4.1599026 -4.1242809 -4.1150341 -4.1549673 -4.2223668 -4.2856641 -4.3368196 -4.3669424 -4.3789825 -4.3782578][-4.3570709 -4.3488078 -4.3355956 -4.3156166 -4.2915134 -4.2683392 -4.2529626 -4.2542396 -4.2796884 -4.3186936 -4.3521047 -4.3756046 -4.38516 -4.3837848 -4.3764615][-4.3658996 -4.362792 -4.3566384 -4.347352 -4.3334289 -4.3226819 -4.3204589 -4.3269958 -4.3437605 -4.3647008 -4.3793788 -4.386045 -4.3838382 -4.3764839 -4.3681617][-4.3682427 -4.3686132 -4.3665347 -4.3627539 -4.3552294 -4.3501515 -4.3491673 -4.3521948 -4.3592052 -4.3679814 -4.373982 -4.3761697 -4.3735819 -4.3680363 -4.3619738]]...]
INFO - root - 2017-12-07 19:12:01.184178: step 19710, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.600 sec/batch; 65h:07m:22s remains)
INFO - root - 2017-12-07 19:12:17.539567: step 19720, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.679 sec/batch; 68h:20m:14s remains)
INFO - root - 2017-12-07 19:12:33.910457: step 19730, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.560 sec/batch; 63h:29m:42s remains)
INFO - root - 2017-12-07 19:12:50.242897: step 19740, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.680 sec/batch; 68h:21m:52s remains)
INFO - root - 2017-12-07 19:13:06.582440: step 19750, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.601 sec/batch; 65h:09m:43s remains)
INFO - root - 2017-12-07 19:13:22.962804: step 19760, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.693 sec/batch; 68h:53m:48s remains)
INFO - root - 2017-12-07 19:13:39.014237: step 19770, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.640 sec/batch; 66h:43m:52s remains)
INFO - root - 2017-12-07 19:13:55.432368: step 19780, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.580 sec/batch; 64h:17m:56s remains)
INFO - root - 2017-12-07 19:14:11.808337: step 19790, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 1.723 sec/batch; 70h:06m:36s remains)
INFO - root - 2017-12-07 19:14:27.911588: step 19800, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.563 sec/batch; 63h:35m:21s remains)
2017-12-07 19:14:29.196157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2669072 -4.2499366 -4.2281814 -4.216228 -4.2142386 -4.2167845 -4.2298784 -4.2553005 -4.2741761 -4.2796 -4.2741938 -4.2632728 -4.2514992 -4.2457638 -4.2470326][-4.2688212 -4.251225 -4.2220922 -4.2017717 -4.196013 -4.1921148 -4.1997576 -4.2269325 -4.25808 -4.27538 -4.2723761 -4.2587738 -4.247025 -4.2414031 -4.2413511][-4.26615 -4.2461023 -4.2138314 -4.1857224 -4.1727719 -4.1624956 -4.1650419 -4.1937866 -4.2343769 -4.2622361 -4.2693768 -4.2634211 -4.2565703 -4.2534971 -4.2525921][-4.2600145 -4.2377639 -4.2047029 -4.1674051 -4.1404724 -4.1213078 -4.121779 -4.1564879 -4.2062759 -4.2428927 -4.2626309 -4.2704382 -4.2718725 -4.2721281 -4.2684484][-4.2617965 -4.2388439 -4.2044964 -4.1594028 -4.1161623 -4.0831919 -4.0854483 -4.1319323 -4.1886735 -4.2275143 -4.2521653 -4.269598 -4.2773013 -4.2776012 -4.2681885][-4.2721539 -4.2476721 -4.2103968 -4.1605062 -4.1078486 -4.0663047 -4.0717287 -4.1299257 -4.1875119 -4.2197518 -4.2400208 -4.2602839 -4.2700348 -4.2687407 -4.2584319][-4.2808909 -4.2534881 -4.2144833 -4.1662416 -4.1164103 -4.0793686 -4.0877976 -4.1460314 -4.1926084 -4.2136493 -4.2256784 -4.2442555 -4.2550492 -4.2555003 -4.2489157][-4.2875843 -4.2551589 -4.2179475 -4.1774497 -4.1385422 -4.1137705 -4.1238489 -4.167429 -4.1981292 -4.2081165 -4.212389 -4.2253585 -4.2354574 -4.2399111 -4.2419224][-4.2895489 -4.2549796 -4.2211895 -4.191287 -4.1656561 -4.1546812 -4.1665788 -4.1940122 -4.2091031 -4.2118144 -4.2118459 -4.217792 -4.2239504 -4.2292266 -4.2359672][-4.2857738 -4.25355 -4.2265692 -4.2065296 -4.1934962 -4.1930513 -4.206409 -4.2241778 -4.2293458 -4.2258916 -4.22173 -4.2187796 -4.21859 -4.2212119 -4.2270718][-4.2790451 -4.250792 -4.2317352 -4.2207904 -4.2171121 -4.2231836 -4.2379808 -4.2518172 -4.2542114 -4.2469726 -4.2380972 -4.2287931 -4.2228732 -4.2202573 -4.2241096][-4.2651687 -4.2413616 -4.2337232 -4.2328014 -4.23354 -4.2404284 -4.2533779 -4.2667189 -4.2712722 -4.2644882 -4.2503967 -4.2355819 -4.2272778 -4.2225337 -4.2239075][-4.2550597 -4.239027 -4.2415223 -4.2470779 -4.2487307 -4.2510338 -4.2617078 -4.2758327 -4.2827063 -4.2752056 -4.2588854 -4.2441735 -4.2379994 -4.2328353 -4.2318239][-4.2681689 -4.2590518 -4.2638316 -4.2719626 -4.2748318 -4.2757936 -4.2850952 -4.2948084 -4.2985439 -4.2888532 -4.2723923 -4.2620378 -4.2589688 -4.2531376 -4.2516861][-4.2890244 -4.286047 -4.2900195 -4.29807 -4.3035235 -4.3053994 -4.3104687 -4.3122368 -4.3104615 -4.2997384 -4.2858334 -4.2819419 -4.2813396 -4.2742844 -4.273241]]...]
INFO - root - 2017-12-07 19:14:45.435769: step 19810, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.573 sec/batch; 63h:59m:11s remains)
INFO - root - 2017-12-07 19:15:01.578312: step 19820, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.630 sec/batch; 66h:18m:54s remains)
INFO - root - 2017-12-07 19:15:17.806091: step 19830, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.630 sec/batch; 66h:18m:47s remains)
INFO - root - 2017-12-07 19:15:34.158550: step 19840, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 1.673 sec/batch; 68h:01m:16s remains)
INFO - root - 2017-12-07 19:15:50.489822: step 19850, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.595 sec/batch; 64h:51m:51s remains)
INFO - root - 2017-12-07 19:16:06.944831: step 19860, loss = 2.11, batch loss = 2.05 (9.8 examples/sec; 1.637 sec/batch; 66h:34m:19s remains)
INFO - root - 2017-12-07 19:16:23.399775: step 19870, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.625 sec/batch; 66h:03m:23s remains)
INFO - root - 2017-12-07 19:16:39.783948: step 19880, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.587 sec/batch; 64h:30m:52s remains)
INFO - root - 2017-12-07 19:16:55.641462: step 19890, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.578 sec/batch; 64h:09m:11s remains)
INFO - root - 2017-12-07 19:17:11.886256: step 19900, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.567 sec/batch; 63h:42m:38s remains)
2017-12-07 19:17:13.197212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1730704 -4.1544514 -4.1454744 -4.1599145 -4.1855783 -4.2013083 -4.2093253 -4.2171578 -4.226335 -4.23052 -4.228384 -4.222538 -4.2149587 -4.2067819 -4.1883836][-4.1481714 -4.1224318 -4.114666 -4.1387405 -4.1637192 -4.1723495 -4.1779804 -4.18572 -4.1970057 -4.204967 -4.2032666 -4.1960254 -4.18725 -4.1788239 -4.1535683][-4.1423359 -4.1162071 -4.1119246 -4.1370525 -4.1505113 -4.1522226 -4.159996 -4.1662316 -4.1751423 -4.1835041 -4.1830149 -4.1765089 -4.1682811 -4.1620188 -4.1367702][-4.1448255 -4.1197753 -4.1180224 -4.136621 -4.1385107 -4.1367626 -4.1421413 -4.1440759 -4.1471429 -4.1538486 -4.1523318 -4.1481681 -4.1466947 -4.146399 -4.1264229][-4.158617 -4.1318 -4.1283712 -4.1357689 -4.1271944 -4.1167841 -4.1149659 -4.11148 -4.1111107 -4.1184039 -4.1185575 -4.11504 -4.1152163 -4.1184421 -4.1069622][-4.1743894 -4.1479344 -4.1366458 -4.1289954 -4.1117835 -4.0960708 -4.0884318 -4.0791888 -4.0785861 -4.0931425 -4.09958 -4.0978217 -4.0980749 -4.1024733 -4.098989][-4.1633677 -4.1390023 -4.1240172 -4.1111021 -4.0970573 -4.0880404 -4.0816116 -4.0682812 -4.0701761 -4.0883074 -4.096426 -4.0968823 -4.0981183 -4.096787 -4.0865607][-4.1271009 -4.1038847 -4.098033 -4.101593 -4.1070218 -4.1128883 -4.1045604 -4.0836339 -4.0775657 -4.0854239 -4.0876851 -4.085731 -4.0840254 -4.0770245 -4.059792][-4.090868 -4.0712318 -4.0786591 -4.1030827 -4.1306143 -4.1466846 -4.1362791 -4.1039977 -4.0793662 -4.065074 -4.0594239 -4.0574627 -4.05658 -4.0499892 -4.0347896][-4.0831542 -4.0664196 -4.0712128 -4.100822 -4.13661 -4.1563425 -4.1459045 -4.1106634 -4.0741739 -4.04462 -4.0365295 -4.0384431 -4.041326 -4.0411048 -4.0295448][-4.1212091 -4.1042209 -4.0951214 -4.111753 -4.1431851 -4.1613903 -4.1498718 -4.121069 -4.0921364 -4.0659413 -4.0553594 -4.0539088 -4.0599117 -4.0597825 -4.0445523][-4.1806669 -4.1661434 -4.1474209 -4.1499519 -4.1703687 -4.1834106 -4.1696196 -4.1493158 -4.1348267 -4.1189294 -4.1055622 -4.1019039 -4.1093035 -4.1071715 -4.0853972][-4.2479825 -4.2383256 -4.2197833 -4.2164783 -4.226716 -4.2301865 -4.2160349 -4.2003689 -4.1947079 -4.1886592 -4.1822147 -4.1819091 -4.1862364 -4.1786251 -4.1535277][-4.3068051 -4.3021412 -4.2875757 -4.28131 -4.2821908 -4.2772079 -4.2660704 -4.2553592 -4.2548242 -4.257381 -4.2604136 -4.2621632 -4.2592416 -4.246736 -4.2266378][-4.3292737 -4.3288941 -4.3202224 -4.3135934 -4.3076539 -4.2972765 -4.2865748 -4.2791982 -4.2824583 -4.2924104 -4.3002357 -4.3020782 -4.298069 -4.2890086 -4.2763462]]...]
INFO - root - 2017-12-07 19:17:29.533515: step 19910, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 64h:57m:31s remains)
INFO - root - 2017-12-07 19:17:45.893732: step 19920, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.654 sec/batch; 67h:13m:38s remains)
INFO - root - 2017-12-07 19:18:02.265642: step 19930, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.634 sec/batch; 66h:24m:30s remains)
INFO - root - 2017-12-07 19:18:18.422625: step 19940, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.582 sec/batch; 64h:18m:51s remains)
INFO - root - 2017-12-07 19:18:34.447456: step 19950, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.710 sec/batch; 69h:30m:08s remains)
INFO - root - 2017-12-07 19:18:50.867441: step 19960, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.628 sec/batch; 66h:08m:49s remains)
INFO - root - 2017-12-07 19:19:07.184924: step 19970, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.645 sec/batch; 66h:51m:02s remains)
INFO - root - 2017-12-07 19:19:23.367839: step 19980, loss = 2.06, batch loss = 2.01 (10.4 examples/sec; 1.545 sec/batch; 62h:46m:26s remains)
INFO - root - 2017-12-07 19:19:39.619251: step 19990, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.623 sec/batch; 65h:56m:26s remains)
INFO - root - 2017-12-07 19:19:55.712017: step 20000, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.556 sec/batch; 63h:12m:31s remains)
2017-12-07 19:19:57.098795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3292575 -4.3280196 -4.32269 -4.3229389 -4.3265042 -4.3253994 -4.3226538 -4.3215141 -4.3229709 -4.3251157 -4.3285618 -4.3336716 -4.3384018 -4.3398724 -4.3408504][-4.3226004 -4.3201647 -4.3140769 -4.3139715 -4.3179445 -4.3167624 -4.3136039 -4.3121033 -4.3134522 -4.3148112 -4.3165383 -4.3201313 -4.3249269 -4.3285313 -4.3331027][-4.30462 -4.3018603 -4.2968559 -4.2987027 -4.3053603 -4.304893 -4.2997961 -4.2949066 -4.293664 -4.2932515 -4.2926979 -4.2925587 -4.2962155 -4.3020468 -4.3125148][-4.2744651 -4.2700891 -4.2650166 -4.2697387 -4.2817025 -4.283361 -4.2742114 -4.2619324 -4.2559404 -4.2535224 -4.250596 -4.2464089 -4.2494054 -4.2588005 -4.276773][-4.2376547 -4.2278705 -4.2174144 -4.2197018 -4.2344561 -4.2354374 -4.21932 -4.1991372 -4.1915293 -4.191947 -4.1916509 -4.1878042 -4.1919255 -4.2059169 -4.2315445][-4.2002211 -4.1820145 -4.1612592 -4.1545286 -4.1655426 -4.1585221 -4.1303425 -4.1046281 -4.1035075 -4.1182251 -4.1287279 -4.1307 -4.1388717 -4.1570439 -4.1866732][-4.1743069 -4.1453037 -4.1115313 -4.0929642 -4.0941963 -4.07096 -4.019978 -3.9897418 -4.0084653 -4.0514135 -4.0824366 -4.0942154 -4.1065488 -4.1238828 -4.14959][-4.1557369 -4.1148176 -4.0664196 -4.03648 -4.029655 -3.9928622 -3.9197078 -3.8880181 -3.9339459 -4.0122213 -4.0647573 -4.085351 -4.0980625 -4.1074805 -4.1221814][-4.1478291 -4.095027 -4.0330205 -3.9965315 -3.9913573 -3.9573436 -3.8817463 -3.8547697 -3.917697 -4.0132551 -4.0757861 -4.0969296 -4.1029263 -4.1030388 -4.1093698][-4.160202 -4.1023855 -4.0361214 -4.0029564 -4.0127978 -4.004746 -3.9552853 -3.9387369 -3.9867589 -4.0633206 -4.1134753 -4.1254506 -4.122159 -4.1162148 -4.1164279][-4.1957889 -4.144248 -4.0838041 -4.0605497 -4.0819845 -4.0939693 -4.069943 -4.0595784 -4.0822983 -4.1250391 -4.1526766 -4.1530905 -4.1414361 -4.1289372 -4.1227612][-4.2412267 -4.19962 -4.14412 -4.12225 -4.1393576 -4.1509933 -4.1387649 -4.1353869 -4.14767 -4.1682878 -4.17869 -4.1713467 -4.1503563 -4.1261606 -4.1101007][-4.2659736 -4.2299523 -4.1768985 -4.149694 -4.1571107 -4.1642361 -4.1585455 -4.1621041 -4.1729555 -4.1842022 -4.1885929 -4.1787729 -4.1483951 -4.1101451 -4.0835857][-4.2531819 -4.2139592 -4.15944 -4.1312385 -4.1389151 -4.1484613 -4.1508117 -4.1615324 -4.1728592 -4.1788592 -4.1830649 -4.1755018 -4.1413922 -4.0976214 -4.0711][-4.2179847 -4.1716123 -4.1162262 -4.0961146 -4.1158471 -4.1364808 -4.1460347 -4.159174 -4.16716 -4.1685772 -4.1743989 -4.1706862 -4.143723 -4.110858 -4.0994673]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch16/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch16/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 19:20:14.288506: step 20010, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.684 sec/batch; 68h:24m:41s remains)
INFO - root - 2017-12-07 19:20:30.490334: step 20020, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.594 sec/batch; 64h:45m:13s remains)
INFO - root - 2017-12-07 19:20:46.904363: step 20030, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.687 sec/batch; 68h:31m:49s remains)
INFO - root - 2017-12-07 19:21:03.037481: step 20040, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.598 sec/batch; 64h:54m:13s remains)
INFO - root - 2017-12-07 19:21:19.516311: step 20050, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 1.717 sec/batch; 69h:43m:44s remains)
INFO - root - 2017-12-07 19:21:35.810699: step 20060, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.650 sec/batch; 66h:59m:52s remains)
INFO - root - 2017-12-07 19:21:52.213194: step 20070, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.661 sec/batch; 67h:27m:22s remains)
INFO - root - 2017-12-07 19:22:08.588410: step 20080, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.618 sec/batch; 65h:42m:38s remains)
INFO - root - 2017-12-07 19:22:24.942646: step 20090, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.563 sec/batch; 63h:28m:09s remains)
INFO - root - 2017-12-07 19:22:41.407453: step 20100, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 1.736 sec/batch; 70h:27m:43s remains)
2017-12-07 19:22:42.720937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2746906 -4.2522407 -4.2310433 -4.2136984 -4.2135911 -4.2195644 -4.2218566 -4.2269354 -4.2286854 -4.2209649 -4.2013736 -4.182303 -4.1826425 -4.2154984 -4.2507844][-4.2765708 -4.2492242 -4.2211132 -4.1971545 -4.1917696 -4.1897492 -4.1853604 -4.1891112 -4.2030568 -4.2056956 -4.1833067 -4.1628928 -4.1714435 -4.2117319 -4.2536192][-4.2587748 -4.228528 -4.2001061 -4.1836128 -4.18617 -4.1841187 -4.1724367 -4.1734138 -4.1987982 -4.2120628 -4.1867352 -4.1643257 -4.1779957 -4.2193465 -4.2610207][-4.2325468 -4.2011805 -4.1801157 -4.1805682 -4.20014 -4.2057481 -4.1925235 -4.189302 -4.2182727 -4.2403421 -4.2167397 -4.192534 -4.2002473 -4.2308631 -4.2648][-4.2179961 -4.1915364 -4.1823077 -4.1898336 -4.2075405 -4.2102714 -4.1864867 -4.1676321 -4.1953788 -4.2342725 -4.2281628 -4.2111287 -4.2157688 -4.2351661 -4.2596784][-4.2312775 -4.2128239 -4.2068181 -4.2002382 -4.1952457 -4.1696076 -4.107985 -4.0510831 -4.0773926 -4.1591086 -4.1997428 -4.2090993 -4.22362 -4.2430153 -4.2610259][-4.2504225 -4.233798 -4.2135973 -4.1827173 -4.1529341 -4.0956225 -3.9905436 -3.889039 -3.9211228 -4.061336 -4.1555076 -4.1969047 -4.2299376 -4.2554746 -4.2691851][-4.2606921 -4.2408009 -4.2119012 -4.1719027 -4.1387358 -4.08642 -3.98637 -3.8855212 -3.9158916 -4.063941 -4.163034 -4.2064281 -4.2417712 -4.2677383 -4.2787485][-4.2611418 -4.2415209 -4.2133536 -4.1804914 -4.1681571 -4.1585503 -4.110498 -4.0453725 -4.05189 -4.1457205 -4.2098012 -4.2329564 -4.2577953 -4.2756624 -4.2833505][-4.2639508 -4.2490125 -4.2274532 -4.2081375 -4.2148509 -4.2320423 -4.2200494 -4.1773496 -4.1632648 -4.2116666 -4.2437634 -4.2469606 -4.2597265 -4.2722235 -4.2810364][-4.2771692 -4.2686553 -4.2584038 -4.2491016 -4.2587419 -4.2767997 -4.2704935 -4.2371311 -4.2198777 -4.2499127 -4.2675214 -4.260407 -4.2647114 -4.2774038 -4.286099][-4.2920051 -4.2870355 -4.2848997 -4.2785749 -4.2789564 -4.2842784 -4.2734323 -4.2463169 -4.2331905 -4.2570376 -4.2705884 -4.2629466 -4.2664046 -4.2840219 -4.2958355][-4.3022075 -4.2981319 -4.2995095 -4.2937503 -4.2861018 -4.282227 -4.2708735 -4.2502251 -4.2399321 -4.2575707 -4.2664919 -4.2625585 -4.26926 -4.2902532 -4.3047867][-4.30104 -4.2907124 -4.28676 -4.28088 -4.2765365 -4.2714872 -4.2602968 -4.2427893 -4.2320738 -4.2443137 -4.248004 -4.2475567 -4.261713 -4.2871861 -4.3063283][-4.2858491 -4.26808 -4.2587771 -4.2565417 -4.260438 -4.2601552 -4.2560234 -4.24508 -4.232583 -4.2345386 -4.2329583 -4.2304964 -4.24727 -4.2766385 -4.3008113]]...]
INFO - root - 2017-12-07 19:22:58.824976: step 20110, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.555 sec/batch; 63h:07m:15s remains)
INFO - root - 2017-12-07 19:23:15.147193: step 20120, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.584 sec/batch; 64h:17m:53s remains)
INFO - root - 2017-12-07 19:23:31.215845: step 20130, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.690 sec/batch; 68h:35m:39s remains)
INFO - root - 2017-12-07 19:23:47.528137: step 20140, loss = 2.07, batch loss = 2.02 (10.4 examples/sec; 1.544 sec/batch; 62h:39m:18s remains)
INFO - root - 2017-12-07 19:24:03.953158: step 20150, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 1.641 sec/batch; 66h:35m:41s remains)
INFO - root - 2017-12-07 19:24:20.048076: step 20160, loss = 2.10, batch loss = 2.04 (10.4 examples/sec; 1.537 sec/batch; 62h:21m:49s remains)
INFO - root - 2017-12-07 19:24:36.360736: step 20170, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.729 sec/batch; 70h:10m:13s remains)
INFO - root - 2017-12-07 19:24:52.545957: step 20180, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.618 sec/batch; 65h:39m:02s remains)
INFO - root - 2017-12-07 19:25:08.815038: step 20190, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.671 sec/batch; 67h:48m:57s remains)
INFO - root - 2017-12-07 19:25:25.002415: step 20200, loss = 2.09, batch loss = 2.04 (10.1 examples/sec; 1.583 sec/batch; 64h:14m:14s remains)
2017-12-07 19:25:26.450557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1847339 -4.198123 -4.2007165 -4.1928658 -4.1799836 -4.1611223 -4.1418467 -4.1340284 -4.142117 -4.1638722 -4.1888094 -4.2185597 -4.25031 -4.2791328 -4.2846131][-4.1565895 -4.172915 -4.1721172 -4.1602716 -4.1480589 -4.1289558 -4.1097517 -4.1041451 -4.1152592 -4.1418595 -4.1710081 -4.2032013 -4.2384424 -4.2704115 -4.2754097][-4.144516 -4.1633263 -4.1601892 -4.145926 -4.1365919 -4.121654 -4.1069479 -4.104403 -4.114357 -4.1409574 -4.1704235 -4.2006245 -4.232348 -4.26242 -4.2661257][-4.1600165 -4.1795297 -4.171701 -4.1539583 -4.14747 -4.1366262 -4.1256986 -4.1268587 -4.1377406 -4.1615434 -4.19091 -4.2193141 -4.2466803 -4.273005 -4.27354][-4.1935778 -4.2086296 -4.196249 -4.173234 -4.1670523 -4.1570544 -4.1424503 -4.1422973 -4.1551676 -4.1779113 -4.2063317 -4.2371531 -4.2687178 -4.2953234 -4.2957954][-4.2175922 -4.2251568 -4.2083368 -4.1802325 -4.1695962 -4.1510859 -4.1248608 -4.11517 -4.1275282 -4.1530042 -4.1874385 -4.2276573 -4.2713809 -4.3046536 -4.3072305][-4.2216673 -4.2216721 -4.1995182 -4.1675911 -4.1479049 -4.1156807 -4.0755262 -4.0514669 -4.0621653 -4.0994749 -4.1496515 -4.2022791 -4.2561154 -4.2952337 -4.3014231][-4.2071409 -4.2039013 -4.1781325 -4.1443586 -4.1208844 -4.0807176 -4.0292768 -3.9965961 -4.0106125 -4.0655541 -4.1283889 -4.1862659 -4.2412047 -4.27836 -4.2823839][-4.1755438 -4.1755857 -4.1519752 -4.1214161 -4.1022844 -4.0657363 -4.017478 -3.9892952 -4.0066719 -4.0675683 -4.1276832 -4.1790781 -4.2259321 -4.2555256 -4.25374][-4.1411347 -4.1524134 -4.1374841 -4.1136503 -4.1048727 -4.0835471 -4.0508428 -4.0306845 -4.0432668 -4.0914311 -4.1375151 -4.1765032 -4.2111845 -4.2303219 -4.2231107][-4.1333714 -4.15189 -4.1408195 -4.1195345 -4.1158223 -4.1058106 -4.0845566 -4.0664215 -4.067862 -4.0975318 -4.1301847 -4.1613922 -4.1882124 -4.2000895 -4.1910777][-4.1523471 -4.1690912 -4.1601853 -4.1425166 -4.1400928 -4.1339097 -4.1180806 -4.0978384 -4.0902352 -4.1051307 -4.1308441 -4.1602163 -4.182653 -4.1866918 -4.1760459][-4.1867895 -4.1968517 -4.1905141 -4.1770415 -4.17434 -4.1693273 -4.1583867 -4.1409235 -4.133153 -4.1436381 -4.1692896 -4.1966815 -4.2130318 -4.2073903 -4.194478][-4.23063 -4.2367997 -4.2339869 -4.2235007 -4.2201982 -4.214685 -4.2072191 -4.1953673 -4.1913242 -4.200305 -4.2216339 -4.244257 -4.2553759 -4.2440138 -4.2294388][-4.2744474 -4.2804861 -4.279829 -4.2715511 -4.2681975 -4.263999 -4.2604322 -4.2561731 -4.2563143 -4.261188 -4.2731423 -4.2882128 -4.2959623 -4.2843451 -4.2712011]]...]
INFO - root - 2017-12-07 19:25:42.876298: step 20210, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.642 sec/batch; 66h:36m:17s remains)
INFO - root - 2017-12-07 19:25:58.879922: step 20220, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.584 sec/batch; 64h:15m:23s remains)
INFO - root - 2017-12-07 19:26:15.191013: step 20230, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.582 sec/batch; 64h:09m:02s remains)
INFO - root - 2017-12-07 19:26:31.721837: step 20240, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.604 sec/batch; 65h:02m:27s remains)
INFO - root - 2017-12-07 19:26:48.012105: step 20250, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.626 sec/batch; 65h:57m:45s remains)
INFO - root - 2017-12-07 19:27:04.342242: step 20260, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 1.675 sec/batch; 67h:56m:00s remains)
INFO - root - 2017-12-07 19:27:20.649359: step 20270, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.574 sec/batch; 63h:49m:26s remains)
INFO - root - 2017-12-07 19:27:37.118182: step 20280, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 1.728 sec/batch; 70h:04m:14s remains)
INFO - root - 2017-12-07 19:27:53.213943: step 20290, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 1.559 sec/batch; 63h:11m:37s remains)
INFO - root - 2017-12-07 19:28:09.529674: step 20300, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.708 sec/batch; 69h:15m:29s remains)
2017-12-07 19:28:10.742625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.271915 -4.2634912 -4.2576256 -4.2590694 -4.2630119 -4.2718806 -4.2790709 -4.2782121 -4.269505 -4.2628846 -4.2576594 -4.2571592 -4.2560196 -4.2510972 -4.2510252][-4.2582231 -4.2481666 -4.244379 -4.2462234 -4.2512259 -4.2647347 -4.2776504 -4.2781892 -4.267345 -4.2587667 -4.251967 -4.2477403 -4.2388544 -4.2259274 -4.2221923][-4.2474909 -4.2331762 -4.2307405 -4.2331986 -4.2366242 -4.2516251 -4.2678938 -4.2683792 -4.2543707 -4.2418132 -4.2319407 -4.2238832 -4.210681 -4.1935329 -4.1903558][-4.2254596 -4.2044349 -4.1984749 -4.2007647 -4.2038279 -4.2184296 -4.2369909 -4.2395821 -4.2263956 -4.2106886 -4.19688 -4.1903257 -4.1766891 -4.1596141 -4.1589603][-4.198257 -4.1716838 -4.1570969 -4.1538076 -4.1474328 -4.153738 -4.1732016 -4.1769471 -4.1685057 -4.15597 -4.1449509 -4.1473746 -4.1399717 -4.1297603 -4.1343389][-4.1768103 -4.1477628 -4.1234431 -4.1108685 -4.0917892 -4.08317 -4.1004381 -4.1009288 -4.0920076 -4.0867252 -4.0837145 -4.1015773 -4.1116872 -4.1148543 -4.1268244][-4.1475797 -4.1127348 -4.0728755 -4.0354357 -3.9896498 -3.9607089 -3.9713843 -3.9619923 -3.9561825 -3.968513 -3.9829621 -4.0210705 -4.0551248 -4.0751281 -4.1008759][-4.1105824 -4.0640764 -4.008553 -3.947876 -3.8731093 -3.8225646 -3.8189416 -3.7935076 -3.7911067 -3.831953 -3.8697686 -3.9247561 -3.9867864 -4.0332365 -4.0769749][-4.0864863 -4.0377421 -3.9814382 -3.9125037 -3.8234577 -3.7656846 -3.755399 -3.7232418 -3.7292957 -3.7991004 -3.8647676 -3.9312975 -4.00283 -4.0603552 -4.1102066][-4.0826354 -4.0442853 -4.0052733 -3.9587827 -3.8940601 -3.8581188 -3.870378 -3.86392 -3.8769922 -3.9362688 -3.9940174 -4.050281 -4.1098652 -4.1605453 -4.2031617][-4.0845556 -4.0664582 -4.0514522 -4.0349164 -4.0074024 -3.9988258 -4.0250306 -4.0369196 -4.0491433 -4.095758 -4.1460066 -4.1928067 -4.2375932 -4.2748709 -4.3021197][-4.0971866 -4.0991073 -4.1058154 -4.1135292 -4.1153436 -4.1274285 -4.1600523 -4.1805859 -4.1928954 -4.2234721 -4.2588711 -4.291862 -4.3177562 -4.3388348 -4.352036][-4.1531138 -4.16998 -4.1886892 -4.2082825 -4.224175 -4.2412915 -4.2647119 -4.2780256 -4.2871032 -4.3026257 -4.3213081 -4.3391151 -4.3518219 -4.3608484 -4.3649888][-4.2341013 -4.2572227 -4.2778049 -4.2978005 -4.3130078 -4.3236685 -4.334939 -4.3390222 -4.3432178 -4.3485541 -4.3545809 -4.3607469 -4.3653984 -4.3673372 -4.3666925][-4.2970853 -4.3190556 -4.3347731 -4.3484879 -4.3577008 -4.3619146 -4.3653235 -4.364181 -4.36486 -4.3653784 -4.3659177 -4.3677111 -4.3687997 -4.3676672 -4.3653874]]...]
INFO - root - 2017-12-07 19:28:27.087546: step 20310, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.684 sec/batch; 68h:15m:16s remains)
INFO - root - 2017-12-07 19:28:43.473591: step 20320, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.619 sec/batch; 65h:36m:43s remains)
INFO - root - 2017-12-07 19:28:59.815699: step 20330, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.735 sec/batch; 70h:19m:07s remains)
INFO - root - 2017-12-07 19:29:16.272657: step 20340, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 1.608 sec/batch; 65h:10m:42s remains)
INFO - root - 2017-12-07 19:29:32.449957: step 20350, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.625 sec/batch; 65h:51m:28s remains)
INFO - root - 2017-12-07 19:29:48.666817: step 20360, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 1.496 sec/batch; 60h:36m:27s remains)
INFO - root - 2017-12-07 19:30:05.136683: step 20370, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.581 sec/batch; 64h:04m:16s remains)
INFO - root - 2017-12-07 19:30:21.445016: step 20380, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.734 sec/batch; 70h:14m:31s remains)
INFO - root - 2017-12-07 19:30:37.648042: step 20390, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.596 sec/batch; 64h:39m:41s remains)
INFO - root - 2017-12-07 19:30:54.113292: step 20400, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.704 sec/batch; 69h:01m:30s remains)
2017-12-07 19:30:55.501162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3196521 -4.3156476 -4.3133783 -4.3126316 -4.3114119 -4.3083806 -4.3060126 -4.30479 -4.3016577 -4.2988219 -4.3017488 -4.3097105 -4.31769 -4.3219514 -4.3274012][-4.3012171 -4.2932186 -4.2902131 -4.2887316 -4.2855363 -4.2803555 -4.2748041 -4.2668447 -4.2556572 -4.2509294 -4.2594419 -4.2731886 -4.2832036 -4.2884226 -4.29964][-4.27083 -4.2595854 -4.2551227 -4.2512431 -4.2449942 -4.2340775 -4.2188559 -4.2012935 -4.1856771 -4.1838765 -4.1982441 -4.2150979 -4.2262559 -4.2341938 -4.2504759][-4.2303572 -4.2138777 -4.208672 -4.2048731 -4.1963186 -4.1764965 -4.1442814 -4.1188736 -4.106029 -4.1108365 -4.1335545 -4.1573744 -4.1706796 -4.1794162 -4.1971221][-4.184577 -4.1639543 -4.1633644 -4.165556 -4.1522074 -4.1127086 -4.0608568 -4.0267649 -4.0188046 -4.0369568 -4.0780282 -4.1175137 -4.1337209 -4.14423 -4.1637988][-4.1493673 -4.1327486 -4.1354246 -4.1347189 -4.1082759 -4.0491328 -3.9746773 -3.9166107 -3.9130929 -3.9610555 -4.032752 -4.087842 -4.1052032 -4.1191854 -4.1451983][-4.1364121 -4.1273255 -4.1258063 -4.1119924 -4.0705423 -3.9943788 -3.8973711 -3.8171811 -3.8390794 -3.9317036 -4.0195122 -4.0751085 -4.0948648 -4.115952 -4.145226][-4.1385007 -4.1307025 -4.1219444 -4.1023803 -4.0548086 -3.9759433 -3.882957 -3.8231459 -3.87845 -3.9779065 -4.0514321 -4.0881104 -4.1057935 -4.12851 -4.1538782][-4.1432285 -4.1330428 -4.1236448 -4.1109848 -4.0749288 -4.016839 -3.9603148 -3.9369242 -3.9796641 -4.0456557 -4.0906181 -4.1111722 -4.1275053 -4.14882 -4.1702871][-4.1570587 -4.1465907 -4.1412973 -4.1392269 -4.1258178 -4.0957808 -4.0652418 -4.0563173 -4.0808539 -4.1184635 -4.1438031 -4.1554341 -4.1672616 -4.1827412 -4.1996455][-4.1764145 -4.165957 -4.1647954 -4.1731663 -4.1795707 -4.1726379 -4.1590424 -4.1569204 -4.1707115 -4.19432 -4.2096009 -4.2150693 -4.2166672 -4.2265444 -4.24086][-4.2020788 -4.1924005 -4.1934695 -4.2068305 -4.224339 -4.228857 -4.2232046 -4.2227468 -4.2346754 -4.2550259 -4.2626262 -4.2610154 -4.260963 -4.2711706 -4.2816286][-4.2331495 -4.2224941 -4.2234349 -4.2356477 -4.2550564 -4.2636256 -4.2604761 -4.2636228 -4.2771058 -4.2904649 -4.2911458 -4.2887454 -4.2907953 -4.3018188 -4.3090396][-4.2658825 -4.256474 -4.2575545 -4.268847 -4.287323 -4.2961931 -4.2949743 -4.2958755 -4.3016453 -4.3064828 -4.3054142 -4.3060474 -4.3123536 -4.3230429 -4.3295135][-4.2976551 -4.2898116 -4.2899351 -4.2998676 -4.3139157 -4.3192916 -4.3167777 -4.3147106 -4.3143272 -4.3152103 -4.3167295 -4.3228912 -4.3301134 -4.3385968 -4.3429213]]...]
INFO - root - 2017-12-07 19:31:11.768775: step 20410, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.699 sec/batch; 68h:48m:46s remains)
INFO - root - 2017-12-07 19:31:27.753973: step 20420, loss = 2.06, batch loss = 2.01 (10.4 examples/sec; 1.537 sec/batch; 62h:16m:14s remains)
INFO - root - 2017-12-07 19:31:44.063300: step 20430, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.678 sec/batch; 67h:58m:51s remains)
INFO - root - 2017-12-07 19:32:00.324421: step 20440, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.622 sec/batch; 65h:40m:39s remains)
INFO - root - 2017-12-07 19:32:16.728191: step 20450, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.551 sec/batch; 62h:48m:04s remains)
INFO - root - 2017-12-07 19:32:32.908528: step 20460, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.694 sec/batch; 68h:36m:12s remains)
INFO - root - 2017-12-07 19:32:49.094212: step 20470, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.576 sec/batch; 63h:50m:08s remains)
INFO - root - 2017-12-07 19:33:05.251066: step 20480, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.708 sec/batch; 69h:09m:34s remains)
INFO - root - 2017-12-07 19:33:21.461667: step 20490, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.627 sec/batch; 65h:53m:38s remains)
INFO - root - 2017-12-07 19:33:37.566569: step 20500, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.645 sec/batch; 66h:34m:53s remains)
2017-12-07 19:33:38.887063: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1902523 -4.1787124 -4.1760688 -4.1765323 -4.1758265 -4.1779871 -4.1789541 -4.1739855 -4.1662087 -4.1642637 -4.1652136 -4.1688528 -4.1729665 -4.175971 -4.1767631][-4.2277856 -4.2186666 -4.2171597 -4.2195945 -4.2225823 -4.2280331 -4.2306104 -4.2276711 -4.2235947 -4.2234492 -4.2237396 -4.2242742 -4.2229533 -4.218955 -4.2129397][-4.2466946 -4.2396765 -4.2404428 -4.2474909 -4.257318 -4.2681732 -4.2748837 -4.2782354 -4.2808723 -4.286325 -4.2901807 -4.2903428 -4.2835097 -4.2691722 -4.2504821][-4.2449489 -4.2372508 -4.2382112 -4.2486062 -4.2645154 -4.2808332 -4.2915897 -4.2982965 -4.3036633 -4.3113661 -4.317544 -4.3187656 -4.3091669 -4.2868538 -4.2570357][-4.2225261 -4.2096252 -4.2060337 -4.2136908 -4.2295675 -4.2467008 -4.2571411 -4.2624769 -4.2682095 -4.279295 -4.28987 -4.2947764 -4.2858276 -4.2618442 -4.2301335][-4.1823816 -4.1597342 -4.1479421 -4.1493025 -4.1611824 -4.1751852 -4.1815572 -4.1826434 -4.1907563 -4.2108345 -4.2324476 -4.2447577 -4.2390976 -4.218616 -4.1930256][-4.1455688 -4.1123796 -4.0913758 -4.0847926 -4.08916 -4.0950541 -4.0921435 -4.0853529 -4.0956535 -4.1274977 -4.1626148 -4.1838746 -4.1845675 -4.1729579 -4.1608381][-4.1498532 -4.1172156 -4.0943894 -4.0832171 -4.0796771 -4.0742288 -4.0585794 -4.0419459 -4.0502968 -4.0856028 -4.1240368 -4.1470475 -4.1516919 -4.1490207 -4.1498022][-4.1973815 -4.1754379 -4.15938 -4.1506996 -4.1457691 -4.1370444 -4.1204505 -4.1040225 -4.1061378 -4.1281796 -4.1519403 -4.1645422 -4.167367 -4.1680274 -4.1732807][-4.2511005 -4.2391205 -4.229949 -4.2250438 -4.2222414 -4.2159123 -4.2039447 -4.1915617 -4.1893363 -4.197485 -4.205472 -4.2082739 -4.2089372 -4.2099285 -4.2134719][-4.28918 -4.2823825 -4.2775421 -4.2761822 -4.2760081 -4.2732525 -4.2671852 -4.2608862 -4.2581611 -4.2589111 -4.2592692 -4.2574725 -4.2552557 -4.253273 -4.2523518][-4.313621 -4.3098445 -4.3077 -4.3078518 -4.308198 -4.3073049 -4.304287 -4.3006577 -4.2975588 -4.2955546 -4.2933397 -4.2898169 -4.2860641 -4.281847 -4.2788434][-4.3288274 -4.3268704 -4.3262215 -4.3266187 -4.3265862 -4.3258171 -4.3236289 -4.320662 -4.3176651 -4.3151288 -4.3125682 -4.3088574 -4.3050551 -4.30143 -4.2993293][-4.3380427 -4.336834 -4.3363662 -4.336576 -4.336575 -4.3361292 -4.3350191 -4.3331861 -4.3309417 -4.3287082 -4.32627 -4.3232923 -4.3207293 -4.31911 -4.31888][-4.3437519 -4.3433723 -4.3427849 -4.342443 -4.341938 -4.34122 -4.3403225 -4.339294 -4.3382149 -4.337285 -4.3362608 -4.3353119 -4.3347411 -4.3346643 -4.335063]]...]
INFO - root - 2017-12-07 19:33:55.269353: step 20510, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.605 sec/batch; 64h:57m:30s remains)
INFO - root - 2017-12-07 19:34:11.434633: step 20520, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.642 sec/batch; 66h:27m:26s remains)
INFO - root - 2017-12-07 19:34:27.612258: step 20530, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.604 sec/batch; 64h:56m:08s remains)
INFO - root - 2017-12-07 19:34:43.688618: step 20540, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.698 sec/batch; 68h:44m:28s remains)
INFO - root - 2017-12-07 19:34:59.720320: step 20550, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.593 sec/batch; 64h:28m:58s remains)
INFO - root - 2017-12-07 19:35:16.171938: step 20560, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.677 sec/batch; 67h:52m:00s remains)
INFO - root - 2017-12-07 19:35:32.519830: step 20570, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 1.634 sec/batch; 66h:06m:25s remains)
INFO - root - 2017-12-07 19:35:48.742159: step 20580, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 65h:56m:58s remains)
INFO - root - 2017-12-07 19:36:04.988971: step 20590, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.622 sec/batch; 65h:38m:30s remains)
INFO - root - 2017-12-07 19:36:21.409951: step 20600, loss = 2.11, batch loss = 2.05 (9.9 examples/sec; 1.609 sec/batch; 65h:06m:54s remains)
2017-12-07 19:36:22.817998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2771778 -4.26766 -4.2614188 -4.263 -4.2702494 -4.281951 -4.2906666 -4.3011589 -4.3071451 -4.2967615 -4.2637463 -4.2255349 -4.1884995 -4.1668739 -4.1555047][-4.2849154 -4.2771807 -4.2710714 -4.2700472 -4.2741413 -4.2843451 -4.2899256 -4.2971683 -4.3042531 -4.298902 -4.2714672 -4.2381783 -4.2058177 -4.1896296 -4.1836567][-4.2834744 -4.2821894 -4.27878 -4.2758269 -4.2751989 -4.2801309 -4.2790651 -4.2808514 -4.2870445 -4.2846808 -4.2648196 -4.23776 -4.2141166 -4.2083931 -4.2117434][-4.2832823 -4.2874389 -4.2852731 -4.2786846 -4.2709794 -4.2656503 -4.2536211 -4.2467761 -4.2511711 -4.2526317 -4.2394671 -4.2182188 -4.2048163 -4.2102737 -4.2234874][-4.280303 -4.2856307 -4.2825117 -4.269866 -4.25215 -4.2344418 -4.2072906 -4.1871867 -4.1903763 -4.2025509 -4.2009144 -4.1900234 -4.1883783 -4.2007151 -4.2173357][-4.2607012 -4.2630706 -4.2556763 -4.2328324 -4.2002754 -4.1666341 -4.1203856 -4.0860753 -4.0900168 -4.118433 -4.13411 -4.1420407 -4.15802 -4.1776991 -4.1952004][-4.2123189 -4.2076607 -4.1886969 -4.1500158 -4.0996737 -4.0478511 -3.9857638 -3.9449985 -3.9630141 -4.0143285 -4.0499182 -4.0781937 -4.1138582 -4.1420383 -4.1606584][-4.1392941 -4.1299434 -4.1008368 -4.0467539 -3.979598 -3.9148564 -3.8483922 -3.8166814 -3.8599045 -3.9318185 -3.9824729 -4.0229936 -4.0691042 -4.1028666 -4.121232][-4.0779457 -4.0734382 -4.0471129 -3.9952993 -3.9325421 -3.879174 -3.8335686 -3.8223219 -3.8755026 -3.9414337 -3.9843891 -4.016367 -4.0558305 -4.0871344 -4.1015086][-4.0650892 -4.0711565 -4.0616193 -4.0323544 -3.9951575 -3.9666321 -3.9459875 -3.9470761 -3.9899042 -4.0319452 -4.0548816 -4.0679903 -4.0910816 -4.1127729 -4.1218562][-4.099678 -4.1123319 -4.1179967 -4.1113443 -4.10058 -4.0938239 -4.08878 -4.0913529 -4.1161342 -4.1366124 -4.1421056 -4.1396713 -4.1469822 -4.1567054 -4.1611681][-4.1473393 -4.1592712 -4.1701369 -4.175168 -4.1789064 -4.1837745 -4.188715 -4.19342 -4.2077904 -4.2186847 -4.2159128 -4.205955 -4.2022896 -4.2015138 -4.199749][-4.1903915 -4.1988187 -4.2081304 -4.21524 -4.2214484 -4.227911 -4.235775 -4.2405844 -4.2480345 -4.2534842 -4.2493052 -4.2398672 -4.2325492 -4.2274189 -4.2234249][-4.2226739 -4.2294536 -4.2352781 -4.2394018 -4.2416534 -4.243155 -4.2474184 -4.2495456 -4.253 -4.2542439 -4.2496185 -4.2446847 -4.2404828 -4.2361045 -4.2300649][-4.2476039 -4.2544236 -4.2582355 -4.2598166 -4.2581959 -4.25504 -4.2538738 -4.2515154 -4.2513242 -4.2509832 -4.248167 -4.2471762 -4.2473583 -4.24505 -4.2401342]]...]
INFO - root - 2017-12-07 19:36:39.174793: step 20610, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.656 sec/batch; 67h:00m:22s remains)
INFO - root - 2017-12-07 19:36:55.341482: step 20620, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.622 sec/batch; 65h:37m:52s remains)
INFO - root - 2017-12-07 19:37:11.828909: step 20630, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.593 sec/batch; 64h:26m:09s remains)
INFO - root - 2017-12-07 19:37:28.033486: step 20640, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.648 sec/batch; 66h:40m:19s remains)
INFO - root - 2017-12-07 19:37:44.289595: step 20650, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.624 sec/batch; 65h:41m:43s remains)
INFO - root - 2017-12-07 19:38:00.562803: step 20660, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.646 sec/batch; 66h:33m:52s remains)
INFO - root - 2017-12-07 19:38:16.791976: step 20670, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.575 sec/batch; 63h:42m:11s remains)
INFO - root - 2017-12-07 19:38:33.238459: step 20680, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.695 sec/batch; 68h:32m:15s remains)
INFO - root - 2017-12-07 19:38:49.524302: step 20690, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.569 sec/batch; 63h:26m:15s remains)
INFO - root - 2017-12-07 19:39:05.719097: step 20700, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.655 sec/batch; 66h:55m:19s remains)
2017-12-07 19:39:07.102373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3675661 -4.3639331 -4.3626204 -4.3608475 -4.3565316 -4.3482966 -4.3406725 -4.3369608 -4.3375945 -4.3409686 -4.3447132 -4.3496327 -4.3566289 -4.3624845 -4.3668785][-4.3595772 -4.35612 -4.3571763 -4.3537922 -4.3420582 -4.3240833 -4.3083997 -4.3029556 -4.3075371 -4.31771 -4.3289475 -4.340024 -4.3509436 -4.3582745 -4.3645773][-4.3398786 -4.3371105 -4.3409295 -4.3335724 -4.3098593 -4.2769871 -4.2500696 -4.24329 -4.255846 -4.27635 -4.295557 -4.3126116 -4.3284841 -4.3389182 -4.3482137][-4.31542 -4.3110685 -4.3130913 -4.2955966 -4.2518086 -4.1953464 -4.1533031 -4.1456079 -4.1762962 -4.2141256 -4.2427459 -4.2680554 -4.2916141 -4.3079596 -4.3207569][-4.291769 -4.2821908 -4.2764921 -4.24476 -4.175045 -4.0923195 -4.0337281 -4.0303063 -4.0916743 -4.154191 -4.1939359 -4.2261963 -4.2556252 -4.2778835 -4.2934051][-4.2754431 -4.2579083 -4.2411718 -4.1888971 -4.090107 -3.975137 -3.8864183 -3.8880095 -3.9988656 -4.0962219 -4.1539645 -4.1931772 -4.2290778 -4.2545009 -4.2729459][-4.2713766 -4.2446623 -4.2114196 -4.1406941 -4.0214829 -3.8819022 -3.76496 -3.782227 -3.9422092 -4.0654535 -4.1338191 -4.1767006 -4.2140374 -4.2411728 -4.2617865][-4.2697916 -4.2342749 -4.1886039 -4.113606 -4.0010333 -3.8685074 -3.7635345 -3.8035643 -3.9589667 -4.0713987 -4.1339817 -4.1751924 -4.2072177 -4.229805 -4.2521591][-4.2701712 -4.236659 -4.1894593 -4.1334476 -4.0596466 -3.9670324 -3.902657 -3.9382098 -4.0371909 -4.1083708 -4.1473413 -4.1756105 -4.1999469 -4.2197437 -4.2435727][-4.2760119 -4.2519341 -4.2186441 -4.1943746 -4.1617832 -4.1069174 -4.0699978 -4.0840306 -4.124927 -4.1557717 -4.169395 -4.1789832 -4.1945734 -4.2113495 -4.2374716][-4.2752972 -4.2563143 -4.2377043 -4.2344079 -4.2237859 -4.1951075 -4.1781077 -4.1824818 -4.1914153 -4.1986365 -4.194809 -4.1904082 -4.1974392 -4.2131352 -4.241899][-4.2695217 -4.2514362 -4.2398448 -4.2437334 -4.2434692 -4.2341781 -4.2321472 -4.2360783 -4.2349625 -4.2334285 -4.2256732 -4.2184954 -4.2222624 -4.2372847 -4.2654195][-4.2632141 -4.2465329 -4.2420111 -4.2493711 -4.25838 -4.262794 -4.266911 -4.2716374 -4.2704453 -4.269486 -4.2642241 -4.2598891 -4.2642279 -4.2796459 -4.3034153][-4.2636347 -4.2520866 -4.2564721 -4.2678685 -4.28024 -4.2870092 -4.2916207 -4.2964911 -4.2983046 -4.2998552 -4.2983036 -4.2972054 -4.3022666 -4.3160057 -4.3330398][-4.2852931 -4.2794642 -4.2875624 -4.2980456 -4.3075662 -4.3106189 -4.3122139 -4.3138046 -4.3156128 -4.3181076 -4.3191757 -4.3207979 -4.3262844 -4.3361 -4.3457913]]...]
INFO - root - 2017-12-07 19:39:23.608065: step 20710, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.700 sec/batch; 68h:43m:56s remains)
INFO - root - 2017-12-07 19:39:39.838374: step 20720, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.559 sec/batch; 63h:00m:15s remains)
INFO - root - 2017-12-07 19:39:56.274960: step 20730, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.627 sec/batch; 65h:44m:58s remains)
INFO - root - 2017-12-07 19:40:12.541165: step 20740, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.634 sec/batch; 66h:01m:52s remains)
INFO - root - 2017-12-07 19:40:29.036287: step 20750, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.601 sec/batch; 64h:42m:52s remains)
INFO - root - 2017-12-07 19:40:45.236113: step 20760, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.655 sec/batch; 66h:53m:35s remains)
INFO - root - 2017-12-07 19:41:01.552593: step 20770, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.646 sec/batch; 66h:30m:27s remains)
INFO - root - 2017-12-07 19:41:17.735068: step 20780, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.612 sec/batch; 65h:09m:21s remains)
INFO - root - 2017-12-07 19:41:34.236564: step 20790, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.704 sec/batch; 68h:52m:08s remains)
INFO - root - 2017-12-07 19:41:50.589885: step 20800, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.592 sec/batch; 64h:18m:32s remains)
2017-12-07 19:41:52.010836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3183236 -4.3257251 -4.3201933 -4.2964973 -4.2626843 -4.2329659 -4.2198391 -4.2313628 -4.2568097 -4.2819724 -4.2908936 -4.2800765 -4.2554564 -4.2378354 -4.2449193][-4.3243661 -4.323781 -4.314651 -4.2954869 -4.2670989 -4.2420559 -4.2325754 -4.2424664 -4.2634788 -4.2834396 -4.2857981 -4.266645 -4.2406192 -4.2263627 -4.2381735][-4.3216076 -4.3192973 -4.3145828 -4.3035684 -4.2855043 -4.2661066 -4.2566466 -4.2591 -4.271606 -4.2821956 -4.2778149 -4.2564554 -4.2337093 -4.2237267 -4.233294][-4.2889409 -4.2947526 -4.3017373 -4.3037372 -4.2965837 -4.2802205 -4.2694416 -4.2673817 -4.2739153 -4.2801576 -4.273953 -4.2569079 -4.2405419 -4.2329197 -4.2347851][-4.24879 -4.2692251 -4.2899642 -4.3032937 -4.301168 -4.2827883 -4.268364 -4.2663145 -4.270473 -4.2744069 -4.2701454 -4.2599077 -4.2505088 -4.2470255 -4.2451181][-4.2206969 -4.2516866 -4.283783 -4.3018827 -4.29578 -4.2692256 -4.2507215 -4.2518878 -4.2593727 -4.2622342 -4.2612734 -4.2603564 -4.2605529 -4.2633853 -4.2634611][-4.208529 -4.2408934 -4.2710443 -4.284317 -4.271801 -4.2418227 -4.2249146 -4.2303104 -4.2427011 -4.2492366 -4.252727 -4.2607031 -4.2723341 -4.284647 -4.2900743][-4.2005792 -4.2251339 -4.247807 -4.2532153 -4.2366734 -4.2091503 -4.1979475 -4.2107329 -4.2339129 -4.2514467 -4.2637792 -4.2783942 -4.2959728 -4.3122759 -4.3181305][-4.1873312 -4.19932 -4.2090139 -4.2061653 -4.1890254 -4.1663609 -4.1632309 -4.1855621 -4.2196565 -4.2482452 -4.2678943 -4.2848382 -4.3016396 -4.3155684 -4.320107][-4.1824489 -4.1845517 -4.1807809 -4.1685772 -4.1529865 -4.1389632 -4.1421371 -4.1649818 -4.1981621 -4.22846 -4.250175 -4.2640414 -4.2746778 -4.2819705 -4.2823834][-4.216033 -4.2160158 -4.20927 -4.1970844 -4.1860881 -4.1795416 -4.1846924 -4.1993532 -4.2187304 -4.2375588 -4.2507272 -4.2538943 -4.2498245 -4.2429729 -4.2334476][-4.2604532 -4.2598472 -4.2516332 -4.2426057 -4.2391286 -4.2386346 -4.2433953 -4.2521305 -4.2617769 -4.2711763 -4.2771606 -4.275084 -4.2645955 -4.2485156 -4.2290063][-4.2870994 -4.2856984 -4.2769957 -4.2699828 -4.2700377 -4.272398 -4.2785411 -4.2867117 -4.29316 -4.2989545 -4.3021679 -4.3005157 -4.2932792 -4.2804866 -4.2614393][-4.3074055 -4.2997155 -4.2857385 -4.27768 -4.2788715 -4.282989 -4.2879362 -4.2953019 -4.3005867 -4.3056455 -4.3092494 -4.3088031 -4.3037305 -4.29437 -4.2802472][-4.3238592 -4.3080392 -4.2894487 -4.2797194 -4.2819586 -4.2878332 -4.29167 -4.2965193 -4.3017411 -4.3087907 -4.31554 -4.3154235 -4.3070216 -4.2947679 -4.282043]]...]
INFO - root - 2017-12-07 19:42:08.062035: step 20810, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.566 sec/batch; 63h:16m:30s remains)
INFO - root - 2017-12-07 19:42:24.484988: step 20820, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 65h:49m:52s remains)
INFO - root - 2017-12-07 19:42:40.655797: step 20830, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.623 sec/batch; 65h:34m:44s remains)
INFO - root - 2017-12-07 19:42:57.092182: step 20840, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.722 sec/batch; 69h:34m:04s remains)
INFO - root - 2017-12-07 19:43:13.451862: step 20850, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.611 sec/batch; 65h:03m:59s remains)
INFO - root - 2017-12-07 19:43:29.810416: step 20860, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.633 sec/batch; 65h:55m:53s remains)
INFO - root - 2017-12-07 19:43:45.986704: step 20870, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.627 sec/batch; 65h:42m:05s remains)
INFO - root - 2017-12-07 19:44:02.503794: step 20880, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.597 sec/batch; 64h:28m:38s remains)
INFO - root - 2017-12-07 19:44:18.911522: step 20890, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 1.702 sec/batch; 68h:42m:28s remains)
INFO - root - 2017-12-07 19:44:35.110386: step 20900, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.593 sec/batch; 64h:19m:03s remains)
2017-12-07 19:44:36.549871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3205757 -4.3279848 -4.3270206 -4.3171706 -4.3003469 -4.2842231 -4.2738638 -4.2684178 -4.2628279 -4.2626343 -4.2722325 -4.2872 -4.3016109 -4.3172445 -4.328702][-4.3181772 -4.3257318 -4.3217587 -4.3038139 -4.2741742 -4.2513809 -4.2411861 -4.23331 -4.2222581 -4.22101 -4.2343183 -4.254076 -4.2754993 -4.2982059 -4.3162665][-4.3195009 -4.3258514 -4.3153634 -4.2882457 -4.247159 -4.2201161 -4.2112637 -4.198482 -4.185657 -4.188386 -4.2065368 -4.2306752 -4.2574172 -4.2839446 -4.306572][-4.3207064 -4.32268 -4.3071852 -4.2737603 -4.2216334 -4.1847506 -4.1644387 -4.1378827 -4.1306014 -4.1459923 -4.1742163 -4.2055411 -4.2391195 -4.2695427 -4.2948976][-4.3175926 -4.312644 -4.2886186 -4.2475605 -4.18402 -4.1268454 -4.0789027 -4.0381513 -4.0411472 -4.0799241 -4.1282492 -4.1711874 -4.2150121 -4.2516775 -4.2801566][-4.3126965 -4.2988067 -4.2658668 -4.2156458 -4.140614 -4.057343 -3.97005 -3.9085588 -3.9294167 -4.0043902 -4.0784717 -4.1406016 -4.1975102 -4.2391682 -4.265872][-4.300035 -4.2809277 -4.2449946 -4.1910229 -4.1102209 -4.004909 -3.8740969 -3.7800405 -3.8158505 -3.9308496 -4.0278683 -4.1026015 -4.1716151 -4.2209473 -4.2500134][-4.2964258 -4.2824721 -4.2559738 -4.2106957 -4.1397676 -4.0391474 -3.9103885 -3.8159523 -3.8500836 -3.9612865 -4.0500269 -4.1148033 -4.1767731 -4.2231145 -4.2493606][-4.30795 -4.3014913 -4.2887864 -4.2616649 -4.2141519 -4.140399 -4.0495906 -3.9806869 -3.9923525 -4.0562572 -4.1098027 -4.1538968 -4.2006965 -4.2384787 -4.2609067][-4.3174539 -4.3156958 -4.3127837 -4.3006768 -4.2729549 -4.2243171 -4.17101 -4.1302156 -4.1272078 -4.1493912 -4.1732788 -4.1959419 -4.2270956 -4.2562885 -4.274579][-4.328454 -4.3289709 -4.3297729 -4.3237071 -4.3035026 -4.2749014 -4.2473812 -4.2286143 -4.2267308 -4.2291584 -4.2343431 -4.2439656 -4.2607055 -4.2785921 -4.2906418][-4.3344388 -4.335609 -4.3345532 -4.3277268 -4.3158331 -4.3035483 -4.2911124 -4.2850261 -4.2844 -4.2791824 -4.2748508 -4.2797728 -4.2892795 -4.3004436 -4.309433][-4.3342738 -4.3340816 -4.327095 -4.3175144 -4.3110294 -4.309514 -4.3071022 -4.3066859 -4.3054171 -4.2973137 -4.29278 -4.2964611 -4.3043804 -4.3141446 -4.3222566][-4.3303938 -4.3288512 -4.3207822 -4.3115182 -4.3076682 -4.3100429 -4.3119459 -4.3128481 -4.3123574 -4.3076491 -4.30473 -4.3074217 -4.3144321 -4.3233466 -4.3307815][-4.3312154 -4.3272896 -4.3210077 -4.314496 -4.3132062 -4.3166485 -4.3196959 -4.3215866 -4.3217759 -4.318882 -4.3163466 -4.3174 -4.3223925 -4.3286762 -4.3341417]]...]
INFO - root - 2017-12-07 19:44:52.905295: step 20910, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.616 sec/batch; 65h:14m:06s remains)
INFO - root - 2017-12-07 19:45:09.392695: step 20920, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.690 sec/batch; 68h:14m:08s remains)
INFO - root - 2017-12-07 19:45:25.560506: step 20930, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.575 sec/batch; 63h:34m:20s remains)
INFO - root - 2017-12-07 19:45:41.979894: step 20940, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.690 sec/batch; 68h:13m:53s remains)
INFO - root - 2017-12-07 19:45:58.336747: step 20950, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.629 sec/batch; 65h:44m:11s remains)
INFO - root - 2017-12-07 19:46:14.609342: step 20960, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.693 sec/batch; 68h:20m:21s remains)
INFO - root - 2017-12-07 19:46:30.911885: step 20970, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.649 sec/batch; 66h:32m:08s remains)
INFO - root - 2017-12-07 19:46:47.392570: step 20980, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 1.610 sec/batch; 64h:57m:36s remains)
INFO - root - 2017-12-07 19:47:03.694432: step 20990, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 1.726 sec/batch; 69h:37m:47s remains)
INFO - root - 2017-12-07 19:47:19.993049: step 21000, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.592 sec/batch; 64h:14m:43s remains)
2017-12-07 19:47:21.253735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3451972 -4.3384428 -4.3302279 -4.3242726 -4.3210411 -4.318049 -4.316855 -4.3187423 -4.321259 -4.3236752 -4.3261704 -4.3280754 -4.3282123 -4.3298521 -4.3289471][-4.3374162 -4.3230381 -4.3076811 -4.29777 -4.292644 -4.2884593 -4.2878966 -4.2900891 -4.2941809 -4.2986574 -4.3043189 -4.3091063 -4.3097949 -4.3106194 -4.3072958][-4.3204131 -4.29434 -4.2686229 -4.2519803 -4.2438688 -4.238759 -4.2411389 -4.2440743 -4.2471175 -4.253366 -4.2603235 -4.2668548 -4.2699661 -4.2731705 -4.2698059][-4.2980366 -4.2616515 -4.226069 -4.20022 -4.1850796 -4.1797237 -4.1875796 -4.195735 -4.1953096 -4.1968403 -4.1980324 -4.2006426 -4.2074776 -4.2170525 -4.2189851][-4.2787971 -4.2392182 -4.1981163 -4.1613488 -4.1362715 -4.12921 -4.1419649 -4.1523705 -4.1499233 -4.1449718 -4.1365166 -4.1245351 -4.1299758 -4.1498318 -4.1640835][-4.2680392 -4.2316971 -4.1907873 -4.1482186 -4.1170621 -4.1098766 -4.1189709 -4.1226459 -4.1135015 -4.0951219 -4.0717311 -4.0449519 -4.0498838 -4.0869966 -4.1200771][-4.2686234 -4.2367516 -4.202322 -4.1670361 -4.146008 -4.1435928 -4.1429629 -4.1313128 -4.1069775 -4.0714154 -4.0324087 -3.9944603 -3.998414 -4.0493293 -4.0974526][-4.2765594 -4.2488241 -4.2252526 -4.2089114 -4.2066131 -4.205719 -4.1904421 -4.1613331 -4.1267004 -4.0875125 -4.0501428 -4.0133109 -4.008378 -4.0525022 -4.0962887][-4.2844925 -4.2626104 -4.2519851 -4.25544 -4.2674704 -4.2656093 -4.2408891 -4.2041068 -4.1672826 -4.1361723 -4.1152506 -4.0863018 -4.0657926 -4.0846214 -4.1069913][-4.2899151 -4.2738247 -4.2732797 -4.2911291 -4.3102307 -4.3074617 -4.2804193 -4.2434893 -4.2094679 -4.1880531 -4.1783848 -4.1549234 -4.1247768 -4.116837 -4.1170788][-4.2906942 -4.2772546 -4.2830644 -4.3069353 -4.3269424 -4.3238091 -4.3003378 -4.2671342 -4.233326 -4.2160525 -4.2101 -4.1921377 -4.1595817 -4.1362247 -4.12392][-4.28828 -4.2759094 -4.2827606 -4.3046532 -4.3218455 -4.3211222 -4.3046837 -4.2749825 -4.241785 -4.224452 -4.2155046 -4.1974154 -4.16701 -4.1394806 -4.1239672][-4.2856946 -4.2730546 -4.2767496 -4.2934875 -4.3058424 -4.3052092 -4.2931066 -4.2678185 -4.2430682 -4.2321897 -4.2232461 -4.2063136 -4.177331 -4.1464214 -4.128571][-4.2896056 -4.2794647 -4.2801719 -4.2902241 -4.2982378 -4.2994537 -4.290894 -4.2711611 -4.2545643 -4.2529173 -4.2494683 -4.2366967 -4.2076139 -4.1735411 -4.1541119][-4.3040977 -4.29802 -4.2991958 -4.3050356 -4.3095441 -4.3097529 -4.3021412 -4.2859635 -4.2769895 -4.2816777 -4.2841253 -4.2745228 -4.2483239 -4.2170119 -4.2005439]]...]
INFO - root - 2017-12-07 19:47:37.513498: step 21010, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.587 sec/batch; 64h:01m:12s remains)
INFO - root - 2017-12-07 19:47:53.653008: step 21020, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.690 sec/batch; 68h:11m:19s remains)
INFO - root - 2017-12-07 19:48:10.000076: step 21030, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.610 sec/batch; 64h:56m:43s remains)
INFO - root - 2017-12-07 19:48:26.359540: step 21040, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.590 sec/batch; 64h:08m:26s remains)
INFO - root - 2017-12-07 19:48:42.620803: step 21050, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.592 sec/batch; 64h:12m:57s remains)
INFO - root - 2017-12-07 19:48:58.941907: step 21060, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.614 sec/batch; 65h:06m:12s remains)
INFO - root - 2017-12-07 19:49:15.322899: step 21070, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.680 sec/batch; 67h:46m:05s remains)
INFO - root - 2017-12-07 19:49:31.566648: step 21080, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.629 sec/batch; 65h:41m:05s remains)
INFO - root - 2017-12-07 19:49:47.844231: step 21090, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.655 sec/batch; 66h:44m:55s remains)
INFO - root - 2017-12-07 19:50:03.972238: step 21100, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.578 sec/batch; 63h:37m:32s remains)
2017-12-07 19:50:05.381113: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.26803 -4.2385445 -4.2210517 -4.2279158 -4.2489452 -4.2631187 -4.2757921 -4.2932181 -4.2985163 -4.295577 -4.2913523 -4.2746935 -4.258605 -4.251411 -4.2484732][-4.2530141 -4.2170529 -4.1933222 -4.2026339 -4.2292752 -4.2439671 -4.2525983 -4.2668524 -4.2732315 -4.2758212 -4.2757149 -4.2611465 -4.2436881 -4.2369061 -4.2356095][-4.2385988 -4.2029796 -4.1822314 -4.1948485 -4.217926 -4.2213464 -4.2186508 -4.2242422 -4.2334189 -4.2474136 -4.25801 -4.2522626 -4.2369113 -4.2297797 -4.2307825][-4.2243094 -4.1986771 -4.1906781 -4.2056642 -4.2149048 -4.1968465 -4.1706243 -4.1619968 -4.1763535 -4.2078676 -4.2319884 -4.2390528 -4.2323532 -4.223331 -4.223259][-4.2174067 -4.1984425 -4.199461 -4.2124414 -4.207696 -4.1649151 -4.1081114 -4.0788641 -4.0994797 -4.1548233 -4.1968684 -4.2158623 -4.22005 -4.2131362 -4.2146325][-4.2094555 -4.1895256 -4.1930919 -4.202507 -4.1841025 -4.1226549 -4.0383983 -3.9831672 -4.0141993 -4.1029315 -4.1695995 -4.20013 -4.2135072 -4.2095776 -4.2145634][-4.1901178 -4.1696959 -4.1789117 -4.1876426 -4.1593313 -4.0857592 -3.9761639 -3.8938041 -3.9449914 -4.0726857 -4.1636515 -4.2064967 -4.2266879 -4.22999 -4.2408218][-4.162746 -4.1537347 -4.1739578 -4.1872978 -4.1549392 -4.0766072 -3.9567127 -3.8623872 -3.9300518 -4.0740237 -4.1739635 -4.2255378 -4.25386 -4.2635784 -4.2744646][-4.1451983 -4.1544094 -4.1864376 -4.206409 -4.1790757 -4.1099362 -4.011508 -3.9375002 -3.9874175 -4.1032734 -4.1918368 -4.2441692 -4.275002 -4.2822971 -4.2867122][-4.1407542 -4.1595926 -4.1957574 -4.2194972 -4.2034693 -4.1557746 -4.088963 -4.0369658 -4.0606556 -4.1363282 -4.2030139 -4.2474446 -4.2752829 -4.2780843 -4.2768359][-4.1395183 -4.1568451 -4.1872249 -4.2085314 -4.20056 -4.1710935 -4.1313906 -4.0958676 -4.1032453 -4.14927 -4.1962261 -4.2321229 -4.2554483 -4.2541332 -4.2499714][-4.1494117 -4.1612434 -4.180594 -4.1930251 -4.1864834 -4.1673326 -4.1427259 -4.1211205 -4.1217475 -4.1497021 -4.1829233 -4.2118735 -4.22947 -4.224525 -4.2180948][-4.1707559 -4.1759486 -4.1859879 -4.1903381 -4.1847134 -4.1719236 -4.1559577 -4.1421881 -4.1410394 -4.1618958 -4.1863952 -4.2074666 -4.2178068 -4.2111406 -4.2000747][-4.1995521 -4.2011576 -4.2056437 -4.2075415 -4.2025018 -4.193419 -4.1823297 -4.1718521 -4.1705408 -4.1837196 -4.2011914 -4.2157178 -4.2203593 -4.2126055 -4.2011456][-4.2293792 -4.230473 -4.233974 -4.2348018 -4.2282662 -4.220295 -4.2118239 -4.2032042 -4.2018814 -4.2093186 -4.2207241 -4.2315555 -4.2360663 -4.2295504 -4.2210469]]...]
INFO - root - 2017-12-07 19:50:21.796962: step 21110, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.640 sec/batch; 66h:06m:21s remains)
INFO - root - 2017-12-07 19:50:38.010603: step 21120, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.643 sec/batch; 66h:13m:27s remains)
INFO - root - 2017-12-07 19:50:54.210341: step 21130, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.579 sec/batch; 63h:39m:24s remains)
INFO - root - 2017-12-07 19:51:10.396529: step 21140, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.614 sec/batch; 65h:04m:00s remains)
INFO - root - 2017-12-07 19:51:26.557661: step 21150, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.625 sec/batch; 65h:30m:40s remains)
INFO - root - 2017-12-07 19:51:42.683620: step 21160, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.545 sec/batch; 62h:16m:28s remains)
INFO - root - 2017-12-07 19:51:59.078001: step 21170, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.691 sec/batch; 68h:09m:56s remains)
INFO - root - 2017-12-07 19:52:15.399058: step 21180, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.647 sec/batch; 66h:22m:08s remains)
INFO - root - 2017-12-07 19:52:31.779613: step 21190, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.719 sec/batch; 69h:15m:23s remains)
INFO - root - 2017-12-07 19:52:47.946615: step 21200, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.634 sec/batch; 65h:51m:06s remains)
2017-12-07 19:52:49.350682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3339581 -4.3229117 -4.3153615 -4.3139634 -4.3142891 -4.3111005 -4.3052115 -4.2975168 -4.2921124 -4.2905087 -4.2875648 -4.2879076 -4.2845016 -4.2767015 -4.2693124][-4.3260908 -4.3121171 -4.3083143 -4.3115773 -4.3109641 -4.3039885 -4.2903533 -4.2740612 -4.2636962 -4.2589641 -4.2581425 -4.2621074 -4.258913 -4.2477546 -4.2397914][-4.3181019 -4.3014731 -4.2984471 -4.3078885 -4.30789 -4.2961121 -4.2722688 -4.2470846 -4.2330465 -4.2287893 -4.2320728 -4.2378287 -4.2334127 -4.2188253 -4.2121863][-4.3063364 -4.2883916 -4.2824082 -4.2878513 -4.2841072 -4.2680082 -4.2367086 -4.2061009 -4.1932659 -4.1938591 -4.2009034 -4.2092729 -4.2100153 -4.2002912 -4.1968341][-4.2954893 -4.2744122 -4.2610106 -4.25753 -4.2455344 -4.2227488 -4.1881695 -4.1611285 -4.157506 -4.1637306 -4.1739197 -4.1878676 -4.2001114 -4.2020116 -4.2002192][-4.2899704 -4.2630949 -4.2417884 -4.2285967 -4.2117004 -4.1837091 -4.1472874 -4.1247611 -4.1260719 -4.1332989 -4.1439691 -4.1657009 -4.192296 -4.2069759 -4.2065721][-4.2864165 -4.2493725 -4.2179537 -4.19365 -4.1719904 -4.1412926 -4.1031108 -4.0803218 -4.0839725 -4.0937624 -4.1049309 -4.1329589 -4.1735339 -4.1977892 -4.2014313][-4.281641 -4.2359376 -4.1961489 -4.1679854 -4.1451378 -4.1120996 -4.0720696 -4.0487418 -4.0570474 -4.072947 -4.0849304 -4.1141729 -4.1544404 -4.1774755 -4.1858439][-4.2742729 -4.2301874 -4.1979041 -4.1763873 -4.1591706 -4.1292992 -4.0889678 -4.0672021 -4.0802436 -4.0954976 -4.1029582 -4.1212683 -4.1469488 -4.1620917 -4.1757884][-4.2737546 -4.2396717 -4.2169652 -4.2045193 -4.1940975 -4.1737862 -4.1411009 -4.1221995 -4.1320777 -4.139112 -4.142025 -4.1514764 -4.1641426 -4.1689315 -4.1847291][-4.2821584 -4.2622867 -4.2492604 -4.2431388 -4.2391429 -4.2280769 -4.2028317 -4.1875396 -4.1912313 -4.1937056 -4.1946468 -4.1978879 -4.2012119 -4.2012668 -4.2138529][-4.2950087 -4.2909131 -4.2887754 -4.2867479 -4.2833347 -4.2767582 -4.2606521 -4.2492604 -4.2517323 -4.2569547 -4.2580323 -4.2522306 -4.2445588 -4.2406564 -4.2506089][-4.3047976 -4.3108816 -4.317697 -4.31775 -4.3115315 -4.3039722 -4.2954192 -4.2907758 -4.2964172 -4.3048258 -4.3057365 -4.2956738 -4.2822576 -4.2750783 -4.2805338][-4.3114953 -4.3205996 -4.3290815 -4.3291092 -4.3231077 -4.31598 -4.3107481 -4.3103189 -4.3173594 -4.3250837 -4.3250909 -4.3160839 -4.3044147 -4.2975535 -4.2984][-4.3181973 -4.3238506 -4.3269815 -4.3245325 -4.3204207 -4.3171654 -4.315239 -4.3156013 -4.3197174 -4.3241429 -4.3239508 -4.3195052 -4.3139806 -4.3102031 -4.3090358]]...]
INFO - root - 2017-12-07 19:53:05.701918: step 21210, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.605 sec/batch; 64h:38m:56s remains)
INFO - root - 2017-12-07 19:53:21.981440: step 21220, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.713 sec/batch; 69h:00m:17s remains)
INFO - root - 2017-12-07 19:53:38.438934: step 21230, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.602 sec/batch; 64h:33m:05s remains)
INFO - root - 2017-12-07 19:53:54.693111: step 21240, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.664 sec/batch; 67h:02m:45s remains)
INFO - root - 2017-12-07 19:54:11.036750: step 21250, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.631 sec/batch; 65h:41m:53s remains)
INFO - root - 2017-12-07 19:54:27.458597: step 21260, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.571 sec/batch; 63h:15m:51s remains)
INFO - root - 2017-12-07 19:54:43.874074: step 21270, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.678 sec/batch; 67h:34m:20s remains)
INFO - root - 2017-12-07 19:55:00.077977: step 21280, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.589 sec/batch; 64h:00m:14s remains)
INFO - root - 2017-12-07 19:55:16.400004: step 21290, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.688 sec/batch; 67h:59m:10s remains)
INFO - root - 2017-12-07 19:55:32.811433: step 21300, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.618 sec/batch; 65h:09m:24s remains)
2017-12-07 19:55:34.303145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3552284 -4.35135 -4.3484111 -4.3438878 -4.3364873 -4.3272495 -4.3174171 -4.3093367 -4.3025317 -4.2946959 -4.2888021 -4.2947474 -4.3091421 -4.322978 -4.3344331][-4.3479943 -4.3410745 -4.3321571 -4.3212948 -4.3087482 -4.2962933 -4.2849655 -4.273756 -4.2634873 -4.2520657 -4.24602 -4.2558794 -4.2776775 -4.2975059 -4.3131022][-4.3232341 -4.3123908 -4.2982144 -4.2804565 -4.2624412 -4.249229 -4.2392244 -4.2273746 -4.2104158 -4.1946187 -4.1903205 -4.2066908 -4.2367773 -4.2648439 -4.2871294][-4.279541 -4.26453 -4.2444592 -4.2156291 -4.1891365 -4.1807761 -4.1830158 -4.1730232 -4.1477523 -4.1314459 -4.1378756 -4.1650558 -4.2058368 -4.241188 -4.2681441][-4.2304754 -4.2110553 -4.177721 -4.1337242 -4.104928 -4.1121645 -4.1322446 -4.1242867 -4.0934672 -4.0794625 -4.0994663 -4.1391311 -4.1882749 -4.2261324 -4.2560225][-4.1865659 -4.1598945 -4.1119161 -4.0575294 -4.0379071 -4.0691919 -4.1018796 -4.0943589 -4.0563507 -4.0391545 -4.0655894 -4.1126404 -4.1657104 -4.2101374 -4.2506995][-4.1765289 -4.1351986 -4.0677943 -4.0083742 -4.0102229 -4.0611281 -4.0976911 -4.0869818 -4.0430083 -4.0189376 -4.041543 -4.0918612 -4.1514564 -4.2088051 -4.257916][-4.1888394 -4.1367006 -4.0625057 -4.0184321 -4.0442805 -4.1009879 -4.1281862 -4.1128426 -4.069335 -4.0409389 -4.0608549 -4.1144938 -4.1746392 -4.2307439 -4.2777996][-4.2062306 -4.1591015 -4.1077814 -4.0961328 -4.1334071 -4.1693621 -4.1707411 -4.1433778 -4.10142 -4.0808878 -4.1099215 -4.1665993 -4.2187591 -4.2630391 -4.301167][-4.2208886 -4.1949935 -4.177494 -4.1911683 -4.2216063 -4.227777 -4.2041292 -4.1629562 -4.1215048 -4.1135383 -4.1560011 -4.2133536 -4.2567825 -4.2909126 -4.3191223][-4.214879 -4.2081151 -4.2134094 -4.2328129 -4.2500114 -4.242259 -4.2114496 -4.1620975 -4.1183829 -4.1208348 -4.1725974 -4.229352 -4.2706017 -4.3020806 -4.323441][-4.1835732 -4.1932688 -4.2141118 -4.2322617 -4.2414641 -4.2331409 -4.2055392 -4.1599741 -4.1148229 -4.1193204 -4.1702809 -4.2251716 -4.269835 -4.30077 -4.3186479][-4.1632833 -4.1827974 -4.2128377 -4.2296815 -4.2342253 -4.2286458 -4.2124414 -4.1792808 -4.1384029 -4.1407204 -4.1833062 -4.233264 -4.2752838 -4.3007812 -4.3146353][-4.166656 -4.1890216 -4.2207346 -4.2363467 -4.2430854 -4.2464571 -4.2397728 -4.2116842 -4.1747613 -4.1752868 -4.2115264 -4.2530036 -4.2871728 -4.3060079 -4.3170428][-4.2105989 -4.2308149 -4.2537575 -4.265728 -4.2767572 -4.2861271 -4.2821803 -4.259841 -4.2330575 -4.2364421 -4.2616086 -4.2848806 -4.3041048 -4.3150554 -4.3230457]]...]
INFO - root - 2017-12-07 19:55:50.676933: step 21310, loss = 2.10, batch loss = 2.04 (9.8 examples/sec; 1.632 sec/batch; 65h:41m:10s remains)
INFO - root - 2017-12-07 19:56:06.955444: step 21320, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.728 sec/batch; 69h:33m:54s remains)
INFO - root - 2017-12-07 19:56:23.064783: step 21330, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 1.484 sec/batch; 59h:45m:21s remains)
INFO - root - 2017-12-07 19:56:39.234282: step 21340, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.667 sec/batch; 67h:05m:50s remains)
INFO - root - 2017-12-07 19:56:55.625430: step 21350, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.724 sec/batch; 69h:23m:22s remains)
INFO - root - 2017-12-07 19:57:11.833428: step 21360, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.583 sec/batch; 63h:42m:41s remains)
INFO - root - 2017-12-07 19:57:28.182487: step 21370, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.595 sec/batch; 64h:11m:19s remains)
INFO - root - 2017-12-07 19:57:44.257273: step 21380, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.592 sec/batch; 64h:03m:11s remains)
INFO - root - 2017-12-07 19:58:00.389802: step 21390, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.554 sec/batch; 62h:31m:24s remains)
INFO - root - 2017-12-07 19:58:16.431560: step 21400, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.608 sec/batch; 64h:41m:22s remains)
2017-12-07 19:58:17.828893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1386361 -4.141058 -4.1618891 -4.1755648 -4.1592951 -4.1418214 -4.1443048 -4.1643672 -4.1890326 -4.2137313 -4.2236719 -4.2174611 -4.2043262 -4.2063117 -4.2223067][-4.1369638 -4.1426854 -4.1665711 -4.1844988 -4.1569805 -4.1182075 -4.1077886 -4.1309061 -4.1655974 -4.1967745 -4.2072539 -4.194766 -4.1729736 -4.1681561 -4.1882892][-4.14439 -4.1518064 -4.1752534 -4.1927423 -4.1562057 -4.0958233 -4.0703335 -4.0950146 -4.1456852 -4.188252 -4.2004113 -4.1810517 -4.1533608 -4.1416583 -4.1629596][-4.1537628 -4.1568279 -4.1723804 -4.1829314 -4.1396585 -4.0649533 -4.0296941 -4.0616655 -4.1319 -4.1872373 -4.2009916 -4.1746726 -4.141715 -4.125711 -4.1435032][-4.1632953 -4.1595454 -4.167428 -4.1713495 -4.1312833 -4.0525002 -4.0110865 -4.0507784 -4.1336169 -4.1937633 -4.2065268 -4.1747026 -4.1358881 -4.1163116 -4.12877][-4.1774545 -4.1756773 -4.1807241 -4.179852 -4.1484737 -4.0780845 -4.0280957 -4.0601993 -4.1421442 -4.2012467 -4.2146416 -4.1825881 -4.1425538 -4.1234531 -4.1333575][-4.1854506 -4.1927066 -4.198379 -4.1936417 -4.1685023 -4.1097946 -4.0528288 -4.0638671 -4.1366534 -4.1918225 -4.2066107 -4.1781907 -4.1458182 -4.1363878 -4.148973][-4.1859674 -4.2051921 -4.2165694 -4.2122231 -4.1904445 -4.1433187 -4.0837016 -4.0717897 -4.1244144 -4.1685743 -4.1822753 -4.1607852 -4.1430078 -4.1487255 -4.1681433][-4.1759834 -4.2070451 -4.2280526 -4.2294021 -4.2130156 -4.179574 -4.1303124 -4.1044188 -4.1253085 -4.1487851 -4.1567245 -4.1401072 -4.1331019 -4.1526403 -4.1807594][-4.1569409 -4.1958752 -4.2302575 -4.240593 -4.2315388 -4.2093658 -4.1724343 -4.1375504 -4.1302118 -4.1320415 -4.129725 -4.1164131 -4.1174092 -4.1481767 -4.1854987][-4.1439471 -4.1822443 -4.2221389 -4.240653 -4.239831 -4.2254081 -4.1968527 -4.161777 -4.1395555 -4.1245942 -4.1144738 -4.106894 -4.1156678 -4.1497121 -4.1893272][-4.1421475 -4.1745715 -4.2099195 -4.2336636 -4.2381783 -4.2302518 -4.2100115 -4.1837339 -4.16452 -4.1440592 -4.1269588 -4.11894 -4.1301432 -4.161284 -4.1972671][-4.1496506 -4.174593 -4.2004914 -4.2221432 -4.233377 -4.2314224 -4.2178988 -4.2021027 -4.1967893 -4.1795106 -4.1524429 -4.1347284 -4.1407318 -4.1672068 -4.2036705][-4.1668496 -4.1882119 -4.2042241 -4.2159495 -4.2278576 -4.2324829 -4.2257566 -4.2163429 -4.2191563 -4.2074919 -4.1720319 -4.1416 -4.1397533 -4.165514 -4.2058806][-4.1844211 -4.2050476 -4.2138195 -4.2134366 -4.2193613 -4.2284303 -4.2255425 -4.2158065 -4.2194662 -4.21343 -4.17729 -4.1385627 -4.133121 -4.1620207 -4.2084656]]...]
INFO - root - 2017-12-07 19:58:33.972576: step 21410, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.540 sec/batch; 61h:58m:11s remains)
INFO - root - 2017-12-07 19:58:50.268891: step 21420, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.602 sec/batch; 64h:27m:34s remains)
INFO - root - 2017-12-07 19:59:06.393559: step 21430, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.669 sec/batch; 67h:08m:15s remains)
INFO - root - 2017-12-07 19:59:22.639254: step 21440, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.621 sec/batch; 65h:13m:18s remains)
INFO - root - 2017-12-07 19:59:38.826861: step 21450, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 1.693 sec/batch; 68h:06m:11s remains)
INFO - root - 2017-12-07 19:59:54.769832: step 21460, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 1.583 sec/batch; 63h:40m:49s remains)
INFO - root - 2017-12-07 20:00:11.065084: step 21470, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.668 sec/batch; 67h:06m:02s remains)
INFO - root - 2017-12-07 20:00:27.207957: step 21480, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.587 sec/batch; 63h:49m:31s remains)
INFO - root - 2017-12-07 20:00:43.247970: step 21490, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.614 sec/batch; 64h:53m:42s remains)
INFO - root - 2017-12-07 20:00:59.286696: step 21500, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.551 sec/batch; 62h:21m:42s remains)
2017-12-07 20:01:00.718157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1168385 -4.1932621 -4.2578592 -4.2916379 -4.2994 -4.2963762 -4.2915587 -4.2821827 -4.268208 -4.2467709 -4.2125983 -4.1754708 -4.1492357 -4.1399302 -4.1487565][-4.0981178 -4.1738658 -4.2391458 -4.2767587 -4.2854195 -4.278409 -4.2683825 -4.2553034 -4.2397523 -4.2216125 -4.1970639 -4.1739025 -4.1606469 -4.1594834 -4.1680379][-4.0823894 -4.1523194 -4.2131429 -4.2484789 -4.2567158 -4.2463784 -4.2272506 -4.2086077 -4.192997 -4.1802249 -4.1678534 -4.1560283 -4.1494789 -4.1525993 -4.1625471][-4.0737667 -4.1347919 -4.1923118 -4.2274923 -4.2355113 -4.2204046 -4.19277 -4.1711149 -4.1593828 -4.1559815 -4.1554337 -4.1497273 -4.1435828 -4.1442723 -4.1497221][-4.0829048 -4.1243076 -4.1756682 -4.2112136 -4.2161617 -4.1956687 -4.1643863 -4.1469851 -4.1456366 -4.1539073 -4.1643095 -4.1645737 -4.1572623 -4.1502461 -4.1484938][-4.10791 -4.1255679 -4.1617575 -4.185462 -4.1781006 -4.1458488 -4.110652 -4.1050978 -4.1248531 -4.1547875 -4.1820674 -4.192071 -4.1842766 -4.1691003 -4.1643786][-4.1285605 -4.1399179 -4.15953 -4.1590695 -4.1223831 -4.0645771 -4.0204387 -4.0283217 -4.0766091 -4.1346745 -4.18236 -4.2042747 -4.1977277 -4.1777077 -4.1718221][-4.156075 -4.1673369 -4.1718993 -4.1468153 -4.0780263 -3.9944041 -3.9487278 -3.9794214 -4.0563345 -4.1335731 -4.18697 -4.2083468 -4.1989613 -4.1740484 -4.1597371][-4.1671753 -4.187396 -4.1916714 -4.1662459 -4.1057248 -4.0377016 -4.0103354 -4.0522 -4.1230679 -4.1801114 -4.2118893 -4.2206655 -4.2052374 -4.1741171 -4.1464949][-4.1538677 -4.1778765 -4.1924758 -4.1886148 -4.155406 -4.11197 -4.0971947 -4.1297779 -4.1753926 -4.2026935 -4.2156086 -4.2228804 -4.21782 -4.1942563 -4.1646075][-4.1375756 -4.1520982 -4.1739531 -4.1894741 -4.1713271 -4.1423159 -4.1333909 -4.1531606 -4.1792755 -4.1882939 -4.1946373 -4.2145028 -4.2283249 -4.2188888 -4.1947656][-4.1342974 -4.1452975 -4.1681504 -4.1844034 -4.1713767 -4.1506958 -4.1438169 -4.150423 -4.1626024 -4.1608958 -4.1659756 -4.1982865 -4.22717 -4.2291412 -4.21509][-4.121336 -4.1371446 -4.1620851 -4.1787868 -4.16748 -4.1466942 -4.1327038 -4.1291909 -4.1332889 -4.1292176 -4.1377392 -4.1786661 -4.2157226 -4.2323141 -4.2372494][-4.0858769 -4.1206536 -4.1553969 -4.1714907 -4.15401 -4.1230149 -4.1042943 -4.1003814 -4.1016941 -4.1001444 -4.1230216 -4.1750231 -4.2148933 -4.2403154 -4.2610841][-4.074276 -4.1231685 -4.1637721 -4.1716294 -4.1451573 -4.1070418 -4.0873113 -4.0836372 -4.0825858 -4.0868087 -4.12511 -4.1812768 -4.2151432 -4.2436948 -4.2704287]]...]
INFO - root - 2017-12-07 20:01:16.589218: step 21510, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.569 sec/batch; 63h:03m:58s remains)
INFO - root - 2017-12-07 20:01:32.753897: step 21520, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.682 sec/batch; 67h:37m:55s remains)
INFO - root - 2017-12-07 20:01:48.952027: step 21530, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.564 sec/batch; 62h:53m:20s remains)
INFO - root - 2017-12-07 20:02:05.314956: step 21540, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.678 sec/batch; 67h:25m:56s remains)
INFO - root - 2017-12-07 20:02:21.434221: step 21550, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.572 sec/batch; 63h:10m:23s remains)
INFO - root - 2017-12-07 20:02:37.440760: step 21560, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.541 sec/batch; 61h:56m:35s remains)
INFO - root - 2017-12-07 20:02:53.502822: step 21570, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.619 sec/batch; 65h:04m:13s remains)
INFO - root - 2017-12-07 20:03:09.811960: step 21580, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.597 sec/batch; 64h:10m:03s remains)
INFO - root - 2017-12-07 20:03:26.155526: step 21590, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.726 sec/batch; 69h:20m:56s remains)
INFO - root - 2017-12-07 20:03:42.169803: step 21600, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.567 sec/batch; 62h:58m:48s remains)
2017-12-07 20:03:43.500911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2808857 -4.3013 -4.3214955 -4.3200107 -4.2962856 -4.2648721 -4.2485514 -4.2599716 -4.2816911 -4.2964835 -4.297977 -4.2752728 -4.2446012 -4.2402263 -4.2641988][-4.2885861 -4.3065338 -4.3259044 -4.3268723 -4.3005128 -4.2599216 -4.2363091 -4.2533731 -4.2858524 -4.3114834 -4.3178325 -4.2933993 -4.2576194 -4.2480812 -4.2693229][-4.2831278 -4.3026471 -4.323451 -4.3301625 -4.3059573 -4.2595119 -4.2301569 -4.2498446 -4.2927356 -4.3291235 -4.3394117 -4.3110685 -4.2652154 -4.2433753 -4.2567315][-4.2649918 -4.2907972 -4.3197184 -4.3333187 -4.3109245 -4.2593327 -4.2240438 -4.2496324 -4.3042574 -4.3490281 -4.355968 -4.3172183 -4.2509623 -4.2093358 -4.2173123][-4.2274518 -4.2697816 -4.3149643 -4.3345294 -4.308217 -4.2387695 -4.1861658 -4.22059 -4.2949066 -4.3520408 -4.3538709 -4.3008366 -4.2066312 -4.14693 -4.1591859][-4.1865635 -4.2487073 -4.3090382 -4.3257279 -4.2854338 -4.1824 -4.1004763 -4.151351 -4.2604222 -4.3341269 -4.3310213 -4.255156 -4.1312718 -4.0614157 -4.0886154][-4.1379528 -4.2187734 -4.2910428 -4.3017845 -4.2375612 -4.0859818 -3.9619794 -4.0369678 -4.1929903 -4.2877307 -4.2817283 -4.1832137 -4.0355735 -3.9652441 -4.0134244][-4.0888591 -4.1819639 -4.2620082 -4.2661319 -4.1721487 -3.9650517 -3.7877548 -3.8929737 -4.1042032 -4.2191663 -4.2060852 -4.0878091 -3.9299235 -3.8706949 -3.9398234][-4.0181575 -4.1164894 -4.2034669 -4.2119374 -4.1068392 -3.87585 -3.6659338 -3.7817843 -4.0207486 -4.1429543 -4.119771 -3.9952824 -3.843756 -3.8060997 -3.8873734][-3.9569943 -4.0594759 -4.1524868 -4.1793203 -4.1011147 -3.909193 -3.73364 -3.8160276 -4.0137 -4.1214519 -4.0976238 -3.9812138 -3.8513055 -3.8245046 -3.9018989][-3.9385073 -4.0421472 -4.1391058 -4.1848249 -4.1513367 -4.0273161 -3.9089429 -3.9500546 -4.0760436 -4.1511307 -4.1308842 -4.0361896 -3.9377303 -3.9158771 -3.9763114][-3.9766111 -4.0697632 -4.159029 -4.2121239 -4.2097988 -4.1423225 -4.0696154 -4.08767 -4.1584482 -4.2036567 -4.1924081 -4.1247978 -4.0566387 -4.0377207 -4.076901][-4.0734587 -4.1424294 -4.2120385 -4.2574358 -4.2691374 -4.2355351 -4.1915379 -4.1974268 -4.2341056 -4.2600923 -4.25601 -4.2107844 -4.1643796 -4.1507683 -4.1756568][-4.18499 -4.2305679 -4.2779541 -4.3108764 -4.3225074 -4.3045044 -4.2756724 -4.2743936 -4.2913051 -4.3061066 -4.309216 -4.284853 -4.2554455 -4.2475419 -4.26561][-4.2695603 -4.2963691 -4.324512 -4.3441963 -4.3490906 -4.336349 -4.3177156 -4.3130774 -4.3186388 -4.3256726 -4.3303022 -4.3197021 -4.3034158 -4.3001652 -4.3124309]]...]
INFO - root - 2017-12-07 20:03:59.845090: step 21610, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.586 sec/batch; 63h:42m:33s remains)
INFO - root - 2017-12-07 20:04:15.507843: step 21620, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.635 sec/batch; 65h:41m:11s remains)
INFO - root - 2017-12-07 20:04:31.627739: step 21630, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.596 sec/batch; 64h:06m:26s remains)
INFO - root - 2017-12-07 20:04:47.874154: step 21640, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.568 sec/batch; 63h:00m:13s remains)
INFO - root - 2017-12-07 20:05:04.296897: step 21650, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 1.765 sec/batch; 70h:54m:44s remains)
INFO - root - 2017-12-07 20:05:20.595576: step 21660, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 63h:10m:13s remains)
INFO - root - 2017-12-07 20:05:36.737882: step 21670, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 1.618 sec/batch; 64h:58m:20s remains)
INFO - root - 2017-12-07 20:05:52.803649: step 21680, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.632 sec/batch; 65h:31m:22s remains)
INFO - root - 2017-12-07 20:06:09.130643: step 21690, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 1.693 sec/batch; 67h:58m:15s remains)
INFO - root - 2017-12-07 20:06:25.313775: step 21700, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.553 sec/batch; 62h:21m:48s remains)
2017-12-07 20:06:26.774312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2846427 -4.2859912 -4.2913504 -4.2973928 -4.2991304 -4.2979021 -4.2960978 -4.2956533 -4.2982168 -4.3019443 -4.3059721 -4.3088408 -4.310317 -4.3116984 -4.3122697][-4.2705564 -4.2706041 -4.2773123 -4.2839994 -4.2842164 -4.2785983 -4.2718816 -4.2679739 -4.2702084 -4.2767181 -4.2838116 -4.2895889 -4.2927222 -4.294549 -4.2956686][-4.2510834 -4.2515435 -4.2595854 -4.2653556 -4.2603617 -4.2466445 -4.2314868 -4.2218838 -4.2225223 -4.2316594 -4.2437224 -4.2544332 -4.2614465 -4.26558 -4.2681742][-4.2380257 -4.2398825 -4.2471876 -4.2487097 -4.2365866 -4.2145023 -4.1891813 -4.1699462 -4.1681824 -4.1817737 -4.2032948 -4.2226391 -4.2348914 -4.2409153 -4.2448921][-4.2242641 -4.2289944 -4.2353382 -4.2316008 -4.2129407 -4.1818123 -4.1441445 -4.1159658 -4.1158285 -4.1392708 -4.172224 -4.1999631 -4.2146153 -4.2207761 -4.223731][-4.2068381 -4.21395 -4.2182722 -4.211175 -4.1865129 -4.1435313 -4.0896697 -4.0515857 -4.0597262 -4.1012092 -4.1480103 -4.1811585 -4.19402 -4.198442 -4.1991081][-4.1894255 -4.1951118 -4.1947136 -4.1819453 -4.1487675 -4.0924754 -4.0191383 -3.9656801 -3.9847002 -4.0526047 -4.1181073 -4.1578183 -4.17088 -4.1772718 -4.1810718][-4.1759052 -4.1797585 -4.1736512 -4.1532736 -4.1134233 -4.0467763 -3.9513247 -3.880003 -3.9154167 -4.0121088 -4.0944633 -4.14019 -4.1550608 -4.1620264 -4.1667914][-4.1705694 -4.1751504 -4.1670022 -4.1444855 -4.1078024 -4.0458846 -3.9528687 -3.8904309 -3.937799 -4.0377555 -4.116806 -4.1550522 -4.1641331 -4.1627011 -4.1608987][-4.1768155 -4.1841192 -4.1779323 -4.1597228 -4.135066 -4.093967 -4.0279331 -3.988101 -4.0289617 -4.1080451 -4.1700807 -4.195045 -4.1942177 -4.1814952 -4.1727967][-4.1898923 -4.2008257 -4.1974764 -4.1833153 -4.169239 -4.1484084 -4.1095743 -4.0848088 -4.1147323 -4.1714659 -4.2166476 -4.2312098 -4.2254629 -4.2062182 -4.193717][-4.2065692 -4.2204127 -4.2186475 -4.20686 -4.1976857 -4.1900568 -4.1692858 -4.1539946 -4.1749539 -4.2170916 -4.2519164 -4.2622995 -4.2554951 -4.2360635 -4.222754][-4.2192636 -4.2332282 -4.2326455 -4.2250919 -4.2225142 -4.2246284 -4.2156048 -4.2066159 -4.2204704 -4.2507973 -4.276473 -4.2835846 -4.2769833 -4.2601829 -4.247612][-4.2307048 -4.24083 -4.2385144 -4.23504 -4.2402391 -4.2513919 -4.2527595 -4.251461 -4.2613144 -4.2793179 -4.2943115 -4.2972026 -4.2886305 -4.2740865 -4.2642803][-4.244772 -4.2516007 -4.2480092 -4.2471108 -4.2559505 -4.2717991 -4.2806559 -4.2853212 -4.292007 -4.30024 -4.308424 -4.3082657 -4.2988086 -4.2877221 -4.2808518]]...]
INFO - root - 2017-12-07 20:06:43.056193: step 21710, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.611 sec/batch; 64h:41m:01s remains)
INFO - root - 2017-12-07 20:06:59.341707: step 21720, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.625 sec/batch; 65h:14m:03s remains)
INFO - root - 2017-12-07 20:07:15.339826: step 21730, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.554 sec/batch; 62h:22m:34s remains)
INFO - root - 2017-12-07 20:07:31.355753: step 21740, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.549 sec/batch; 62h:11m:19s remains)
INFO - root - 2017-12-07 20:07:47.522469: step 21750, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.651 sec/batch; 66h:16m:46s remains)
INFO - root - 2017-12-07 20:08:03.649276: step 21760, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.606 sec/batch; 64h:27m:56s remains)
INFO - root - 2017-12-07 20:08:19.934192: step 21770, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.679 sec/batch; 67h:23m:39s remains)
INFO - root - 2017-12-07 20:08:36.228842: step 21780, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.624 sec/batch; 65h:09m:20s remains)
INFO - root - 2017-12-07 20:08:52.521076: step 21790, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.644 sec/batch; 65h:57m:22s remains)
INFO - root - 2017-12-07 20:09:08.591475: step 21800, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.654 sec/batch; 66h:22m:12s remains)
2017-12-07 20:09:09.986531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2352376 -4.2333736 -4.2353616 -4.2324696 -4.2294831 -4.2314572 -4.2388353 -4.246418 -4.2549577 -4.2665091 -4.2804208 -4.2858849 -4.2843909 -4.2798967 -4.2707353][-4.2197471 -4.2183061 -4.2198024 -4.218864 -4.218627 -4.2232518 -4.2300539 -4.2336655 -4.2403164 -4.2516551 -4.2690477 -4.2798209 -4.2811928 -4.270987 -4.2529035][-4.2182379 -4.2158461 -4.2128491 -4.2095151 -4.2082949 -4.2083926 -4.207006 -4.2044458 -4.21035 -4.2235556 -4.243691 -4.2592411 -4.2635508 -4.2499776 -4.2281203][-4.2439938 -4.241425 -4.2340937 -4.2259846 -4.2208304 -4.2115722 -4.1984029 -4.19312 -4.2016106 -4.2189889 -4.2413588 -4.2625475 -4.2696409 -4.2562413 -4.2345777][-4.2716751 -4.26477 -4.2472258 -4.229372 -4.2106047 -4.1872029 -4.1658483 -4.1652055 -4.1875172 -4.2176619 -4.2458086 -4.2689037 -4.2804422 -4.2736964 -4.2567182][-4.2869139 -4.2717819 -4.2355576 -4.1947432 -4.1455855 -4.0933604 -4.05689 -4.0684409 -4.1251163 -4.1868482 -4.2326355 -4.2612925 -4.2764688 -4.2766109 -4.2659106][-4.2889886 -4.2650247 -4.2124705 -4.1456461 -4.0590672 -3.9640892 -3.89704 -3.9204407 -4.0175371 -4.1214347 -4.1928158 -4.2344775 -4.2578731 -4.2627492 -4.2567525][-4.2829151 -4.2629752 -4.20897 -4.1335444 -4.0283852 -3.9048908 -3.8079457 -3.8334928 -3.9492636 -4.0709729 -4.1558061 -4.2026858 -4.2265744 -4.2309127 -4.225553][-4.2698278 -4.2592106 -4.215415 -4.1515074 -4.0624862 -3.9533749 -3.8660736 -3.8833055 -3.9764719 -4.0766296 -4.1463571 -4.1808667 -4.1943665 -4.1946282 -4.1852307][-4.2660694 -4.2678118 -4.2400646 -4.1934195 -4.1279736 -4.0503016 -3.9894757 -3.99469 -4.048738 -4.1096339 -4.1532755 -4.1684608 -4.1675138 -4.1590219 -4.1459408][-4.2649689 -4.2749515 -4.2651386 -4.2347355 -4.1865058 -4.1307306 -4.0916381 -4.0910292 -4.1171165 -4.1473117 -4.1691303 -4.1719956 -4.1625395 -4.1494374 -4.13532][-4.2723546 -4.2850852 -4.2868094 -4.2714763 -4.2380815 -4.1982546 -4.1729 -4.1718855 -4.1843672 -4.1984282 -4.2115965 -4.2088833 -4.1960087 -4.1769962 -4.1566343][-4.283699 -4.2923288 -4.2972164 -4.2960768 -4.2804804 -4.25798 -4.2417154 -4.2427907 -4.2480621 -4.2504125 -4.2546182 -4.2515073 -4.2394762 -4.2172294 -4.1927676][-4.3005552 -4.30284 -4.3045468 -4.306406 -4.3025117 -4.2926087 -4.2827487 -4.2829661 -4.2836537 -4.2774706 -4.2746582 -4.2764978 -4.2741346 -4.2594905 -4.2375789][-4.3151731 -4.3108816 -4.3050647 -4.3032775 -4.3054333 -4.3058524 -4.3019943 -4.2993231 -4.2960343 -4.2888865 -4.28508 -4.2911105 -4.2983212 -4.2943621 -4.2792478]]...]
INFO - root - 2017-12-07 20:09:26.187135: step 21810, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.545 sec/batch; 61h:59m:14s remains)
INFO - root - 2017-12-07 20:09:42.379274: step 21820, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.635 sec/batch; 65h:35m:52s remains)
INFO - root - 2017-12-07 20:09:58.638992: step 21830, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.557 sec/batch; 62h:26m:54s remains)
INFO - root - 2017-12-07 20:10:14.898996: step 21840, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.601 sec/batch; 64h:14m:14s remains)
INFO - root - 2017-12-07 20:10:30.867013: step 21850, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.551 sec/batch; 62h:13m:45s remains)
INFO - root - 2017-12-07 20:10:47.061156: step 21860, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.592 sec/batch; 63h:52m:06s remains)
INFO - root - 2017-12-07 20:11:03.351505: step 21870, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.686 sec/batch; 67h:38m:16s remains)
INFO - root - 2017-12-07 20:11:19.553428: step 21880, loss = 2.11, batch loss = 2.05 (10.1 examples/sec; 1.577 sec/batch; 63h:15m:39s remains)
INFO - root - 2017-12-07 20:11:35.779725: step 21890, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.651 sec/batch; 66h:13m:24s remains)
INFO - root - 2017-12-07 20:11:51.855716: step 21900, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.689 sec/batch; 67h:44m:31s remains)
2017-12-07 20:11:53.224431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.217454 -4.2089028 -4.205935 -4.20438 -4.2030787 -4.2046967 -4.214726 -4.2354188 -4.2566485 -4.2722669 -4.279675 -4.2805638 -4.2735028 -4.2530274 -4.23291][-4.2158375 -4.2102566 -4.2137909 -4.2203765 -4.2253542 -4.2273927 -4.2321796 -4.2448521 -4.2606835 -4.270669 -4.2749815 -4.2791443 -4.2747054 -4.2568789 -4.2400866][-4.2231703 -4.217803 -4.2220335 -4.2321486 -4.2409282 -4.2403555 -4.2359571 -4.2391019 -4.2481837 -4.2535772 -4.256649 -4.2637744 -4.2607226 -4.2493076 -4.2418928][-4.2231073 -4.2180877 -4.2187824 -4.2243223 -4.2263145 -4.2196245 -4.208653 -4.204165 -4.2092533 -4.2173529 -4.2238464 -4.2300739 -4.2285471 -4.2282009 -4.2310824][-4.2158804 -4.2105975 -4.2075272 -4.2014952 -4.1823697 -4.1567922 -4.1379743 -4.1392517 -4.1580472 -4.1822114 -4.2014642 -4.2108335 -4.2094588 -4.2120981 -4.2182665][-4.2208276 -4.2124963 -4.199873 -4.1712909 -4.1186495 -4.059761 -4.0308414 -4.05265 -4.1056714 -4.1605296 -4.1991735 -4.2146816 -4.213726 -4.2145233 -4.2208958][-4.2421622 -4.2308483 -4.20123 -4.1436086 -4.0523672 -3.9450879 -3.8837864 -3.9306569 -4.0294695 -4.1224213 -4.1852112 -4.2121644 -4.2159114 -4.21753 -4.2245464][-4.2490983 -4.2342124 -4.1918855 -4.116436 -3.9973276 -3.8381493 -3.7168729 -3.7815809 -3.9335074 -4.0642385 -4.1453037 -4.1823778 -4.1939931 -4.1986327 -4.2114573][-4.2455177 -4.2285638 -4.1823163 -4.1124473 -4.0048213 -3.855124 -3.7279325 -3.7751923 -3.9135778 -4.0324006 -4.1045704 -4.1412129 -4.1599493 -4.1746531 -4.1961551][-4.2487774 -4.2292471 -4.1871223 -4.1350179 -4.0624728 -3.9704959 -3.8919525 -3.904917 -3.9736955 -4.03346 -4.0680494 -4.0878639 -4.1088634 -4.1351123 -4.1681986][-4.2533822 -4.2341313 -4.20437 -4.1689143 -4.1209855 -4.0705028 -4.0298848 -4.0253325 -4.0413609 -4.0500326 -4.0453143 -4.0438228 -4.0616932 -4.0947695 -4.1321216][-4.2601328 -4.2454562 -4.2292738 -4.2090673 -4.1773133 -4.1447039 -4.1179032 -4.1003709 -4.0886316 -4.0675955 -4.0372286 -4.021431 -4.0410461 -4.0799932 -4.1191225][-4.2673969 -4.2543736 -4.2464561 -4.2383633 -4.2230344 -4.2026849 -4.1804781 -4.1546278 -4.1271377 -4.090436 -4.0508671 -4.0334949 -4.0539007 -4.0961533 -4.1335654][-4.2785535 -4.2636061 -4.2557731 -4.2519093 -4.2453866 -4.2357073 -4.2227869 -4.2005696 -4.1697774 -4.1342626 -4.1041064 -4.0942125 -4.1114435 -4.1459146 -4.1751771][-4.2893729 -4.2718973 -4.2616034 -4.2561669 -4.24911 -4.2418346 -4.232151 -4.2172551 -4.1959386 -4.1751375 -4.1625323 -4.1637306 -4.1789041 -4.2025571 -4.2215905]]...]
INFO - root - 2017-12-07 20:12:08.990228: step 21910, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.638 sec/batch; 65h:40m:29s remains)
INFO - root - 2017-12-07 20:12:25.418708: step 21920, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.625 sec/batch; 65h:08m:56s remains)
INFO - root - 2017-12-07 20:12:41.608483: step 21930, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.676 sec/batch; 67h:12m:02s remains)
INFO - root - 2017-12-07 20:12:57.593909: step 21940, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 1.559 sec/batch; 62h:30m:08s remains)
INFO - root - 2017-12-07 20:13:13.898447: step 21950, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.635 sec/batch; 65h:31m:21s remains)
INFO - root - 2017-12-07 20:13:30.072465: step 21960, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.571 sec/batch; 62h:56m:48s remains)
INFO - root - 2017-12-07 20:13:45.948633: step 21970, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.689 sec/batch; 67h:41m:20s remains)
INFO - root - 2017-12-07 20:14:02.184540: step 21980, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.602 sec/batch; 64h:11m:44s remains)
INFO - root - 2017-12-07 20:14:18.660985: step 21990, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.663 sec/batch; 66h:38m:40s remains)
INFO - root - 2017-12-07 20:14:34.957387: step 22000, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.645 sec/batch; 65h:54m:06s remains)
2017-12-07 20:14:36.456325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2996774 -4.2710757 -4.239985 -4.2132492 -4.1965909 -4.2018552 -4.2157655 -4.2288475 -4.2310753 -4.2159991 -4.194015 -4.1754346 -4.1661925 -4.1662564 -4.1663656][-4.2862287 -4.2529564 -4.2179966 -4.1883378 -4.1682329 -4.1714563 -4.1892724 -4.2081652 -4.218163 -4.2121372 -4.1988077 -4.1900721 -4.18567 -4.1818771 -4.1779752][-4.26402 -4.2336578 -4.19621 -4.15723 -4.1297212 -4.1322985 -4.1568446 -4.1892366 -4.2172937 -4.2300048 -4.2292461 -4.2216625 -4.2112741 -4.1970992 -4.1819849][-4.2480192 -4.2245436 -4.1874094 -4.1363397 -4.0947247 -4.0892987 -4.1132312 -4.1532679 -4.1982088 -4.2331519 -4.2464814 -4.2395272 -4.2202272 -4.1939735 -4.163909][-4.2359905 -4.2183332 -4.1824889 -4.1238351 -4.0719008 -4.0468407 -4.0568733 -4.0925198 -4.1471863 -4.2034912 -4.233901 -4.2342997 -4.2140675 -4.1799855 -4.1380477][-4.225183 -4.2128754 -4.1852093 -4.1316705 -4.0764122 -4.0303841 -4.0088539 -4.0185633 -4.0702119 -4.1383653 -4.18365 -4.1979003 -4.1859951 -4.1564856 -4.1169925][-4.22229 -4.21584 -4.1968465 -4.1540985 -4.1015396 -4.0418139 -3.9794385 -3.9436655 -3.9780374 -4.0519805 -4.1108422 -4.1482477 -4.1617393 -4.1520219 -4.1268759][-4.226522 -4.2213497 -4.2080159 -4.1776619 -4.1329045 -4.0737877 -3.9885833 -3.9091692 -3.9187703 -3.9900432 -4.0545163 -4.1089458 -4.1479726 -4.1600108 -4.1513729][-4.226337 -4.21872 -4.206274 -4.1869349 -4.1577759 -4.1183462 -4.04779 -3.9624031 -3.9464557 -3.988018 -4.0385275 -4.0941544 -4.144474 -4.1671324 -4.1677961][-4.2295451 -4.2177014 -4.2076735 -4.1974325 -4.1828642 -4.1632481 -4.1193123 -4.0533724 -4.022027 -4.0278087 -4.0548105 -4.1050272 -4.1574793 -4.1830711 -4.1876688][-4.2438912 -4.2325873 -4.223012 -4.2154069 -4.2063727 -4.1952772 -4.16683 -4.1211014 -4.0890069 -4.0785995 -4.0945792 -4.13721 -4.1817346 -4.2041469 -4.2115459][-4.2609334 -4.2539225 -4.2442074 -4.232491 -4.2231574 -4.2151275 -4.1939721 -4.1627355 -4.1353168 -4.1214128 -4.1342187 -4.16968 -4.2054892 -4.2269464 -4.236557][-4.2695394 -4.2658105 -4.2603273 -4.2494912 -4.2412257 -4.2341013 -4.2143149 -4.1879168 -4.1618013 -4.1466031 -4.1552553 -4.18516 -4.2173705 -4.2418065 -4.2535419][-4.2732558 -4.27445 -4.2783937 -4.2760549 -4.2703829 -4.2625928 -4.2412124 -4.2128625 -4.184557 -4.1654735 -4.1686978 -4.1929817 -4.2203016 -4.2445354 -4.256804][-4.2716379 -4.2798114 -4.2914371 -4.296227 -4.2931485 -4.2850266 -4.2627382 -4.2322168 -4.20407 -4.1838403 -4.1820345 -4.2006245 -4.2217164 -4.2435946 -4.2581725]]...]
INFO - root - 2017-12-07 20:14:52.667802: step 22010, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.581 sec/batch; 63h:20m:15s remains)
INFO - root - 2017-12-07 20:15:08.967248: step 22020, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.616 sec/batch; 64h:45m:06s remains)
INFO - root - 2017-12-07 20:15:24.926730: step 22030, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.629 sec/batch; 65h:15m:56s remains)
INFO - root - 2017-12-07 20:15:41.409531: step 22040, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.562 sec/batch; 62h:35m:04s remains)
INFO - root - 2017-12-07 20:15:57.566543: step 22050, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.618 sec/batch; 64h:47m:27s remains)
INFO - root - 2017-12-07 20:16:13.827687: step 22060, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.614 sec/batch; 64h:38m:58s remains)
INFO - root - 2017-12-07 20:16:30.185216: step 22070, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 1.768 sec/batch; 70h:48m:56s remains)
INFO - root - 2017-12-07 20:16:46.422452: step 22080, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.580 sec/batch; 63h:16m:47s remains)
INFO - root - 2017-12-07 20:17:02.504762: step 22090, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 1.667 sec/batch; 66h:44m:36s remains)
INFO - root - 2017-12-07 20:17:18.882349: step 22100, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.583 sec/batch; 63h:22m:59s remains)
2017-12-07 20:17:20.278163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2785835 -4.2993789 -4.3051014 -4.3101106 -4.3107123 -4.3035164 -4.2819262 -4.259644 -4.2427783 -4.2454972 -4.2624531 -4.277483 -4.2805681 -4.27086 -4.2481685][-4.28077 -4.3064961 -4.3164606 -4.32462 -4.3283367 -4.3266234 -4.3047471 -4.2716928 -4.2418709 -4.2371335 -4.2517619 -4.2709608 -4.2776613 -4.2683339 -4.2401037][-4.2794623 -4.3107462 -4.3245535 -4.3320193 -4.3350611 -4.3373542 -4.3169732 -4.2776089 -4.2459145 -4.2373428 -4.2450643 -4.2630935 -4.2742357 -4.2665133 -4.2348037][-4.2797823 -4.315433 -4.3327003 -4.3380566 -4.3368378 -4.3347263 -4.312983 -4.2736745 -4.2460341 -4.24048 -4.2427034 -4.2542043 -4.2679572 -4.2661285 -4.2365355][-4.2803969 -4.316196 -4.3317137 -4.3320937 -4.32365 -4.3127537 -4.2876992 -4.248188 -4.2260985 -4.2296386 -4.2350688 -4.242898 -4.25727 -4.2620077 -4.241713][-4.2804561 -4.3135729 -4.3232841 -4.3135157 -4.2938008 -4.2728 -4.2442513 -4.2053504 -4.1850891 -4.1957922 -4.2128887 -4.2259674 -4.2435913 -4.2539129 -4.24427][-4.2800403 -4.3105073 -4.3149819 -4.2940321 -4.2595663 -4.2280245 -4.1958823 -4.156599 -4.1352849 -4.1474094 -4.1732717 -4.1983786 -4.224762 -4.2401042 -4.2350235][-4.2815795 -4.3138294 -4.3187051 -4.2925091 -4.2501121 -4.2137952 -4.1762705 -4.1300077 -4.1024742 -4.1120934 -4.1389203 -4.173357 -4.206368 -4.2237229 -4.2211432][-4.2835455 -4.3187547 -4.32867 -4.3066025 -4.2673559 -4.2334862 -4.1915822 -4.1409311 -4.1118 -4.1183405 -4.1355376 -4.165391 -4.1973543 -4.2140126 -4.2119646][-4.2832165 -4.318552 -4.3301458 -4.3106956 -4.2787738 -4.2492943 -4.2077804 -4.1627989 -4.1425867 -4.1488266 -4.1569228 -4.1755757 -4.1993833 -4.2106695 -4.2088008][-4.2850757 -4.3178759 -4.3255429 -4.3054514 -4.2806892 -4.257453 -4.2205992 -4.1864109 -4.1764655 -4.1850743 -4.1902642 -4.19932 -4.213006 -4.2158403 -4.2098336][-4.2900119 -4.3213911 -4.3257146 -4.3039579 -4.2828965 -4.2646036 -4.2373657 -4.2183943 -4.2186904 -4.2269297 -4.2273822 -4.2277937 -4.2296662 -4.2218947 -4.20892][-4.2933087 -4.3234944 -4.3253756 -4.3042 -4.2868147 -4.272181 -4.25401 -4.2506065 -4.2629085 -4.2724824 -4.2659473 -4.2556982 -4.2471633 -4.2289791 -4.2084732][-4.2953568 -4.3239279 -4.3242192 -4.3064542 -4.2919888 -4.2817183 -4.2703309 -4.2762046 -4.2952881 -4.3059425 -4.2954512 -4.2779541 -4.2628965 -4.2377405 -4.2103682][-4.2969785 -4.324357 -4.3235531 -4.3082762 -4.2966194 -4.2895527 -4.2821546 -4.287478 -4.3045597 -4.3142786 -4.3024 -4.2826934 -4.26734 -4.2424831 -4.2166138]]...]
INFO - root - 2017-12-07 20:17:36.712777: step 22110, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.616 sec/batch; 64h:43m:05s remains)
INFO - root - 2017-12-07 20:17:53.149650: step 22120, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.687 sec/batch; 67h:32m:04s remains)
INFO - root - 2017-12-07 20:18:09.426826: step 22130, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.624 sec/batch; 65h:01m:34s remains)
INFO - root - 2017-12-07 20:18:25.767582: step 22140, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.656 sec/batch; 66h:16m:33s remains)
INFO - root - 2017-12-07 20:18:41.654986: step 22150, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.585 sec/batch; 63h:26m:54s remains)
INFO - root - 2017-12-07 20:18:58.338033: step 22160, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.618 sec/batch; 64h:44m:50s remains)
INFO - root - 2017-12-07 20:19:14.744755: step 22170, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.668 sec/batch; 66h:46m:19s remains)
INFO - root - 2017-12-07 20:19:30.964050: step 22180, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.625 sec/batch; 65h:01m:47s remains)
INFO - root - 2017-12-07 20:19:47.409150: step 22190, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.598 sec/batch; 63h:57m:22s remains)
INFO - root - 2017-12-07 20:20:03.655907: step 22200, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.695 sec/batch; 67h:49m:46s remains)
2017-12-07 20:20:04.994785: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2688966 -4.2703056 -4.2724285 -4.2738132 -4.2750187 -4.2762218 -4.2774687 -4.27877 -4.280036 -4.2821064 -4.2839742 -4.2840972 -4.2823467 -4.2803092 -4.2792969][-4.2547207 -4.2576442 -4.2607546 -4.2612991 -4.2599897 -4.2590694 -4.2601619 -4.2638311 -4.2699547 -4.278501 -4.2864881 -4.2903008 -4.289628 -4.2865858 -4.2841883][-4.2404804 -4.2439971 -4.2458482 -4.2424688 -4.234549 -4.22676 -4.2233768 -4.2280278 -4.2411814 -4.2596965 -4.2775741 -4.28941 -4.2933931 -4.2910581 -4.2876287][-4.2247295 -4.227385 -4.2263017 -4.2166877 -4.197525 -4.1754546 -4.1608877 -4.163228 -4.1841025 -4.2156587 -4.2472115 -4.2716637 -4.2854762 -4.2886372 -4.2868109][-4.2048197 -4.2056837 -4.2007437 -4.18356 -4.150454 -4.1077728 -4.07548 -4.0720215 -4.1005607 -4.14692 -4.1935806 -4.232595 -4.260067 -4.2729549 -4.276268][-4.1824183 -4.1806865 -4.1732087 -4.15163 -4.1090884 -4.0498891 -3.9996967 -3.9865427 -4.0198984 -4.0769463 -4.1335983 -4.1819258 -4.2200074 -4.2423639 -4.251821][-4.1686068 -4.165401 -4.1561847 -4.1339736 -4.0911417 -4.0286179 -3.9709649 -3.948957 -3.9805474 -4.0380611 -4.0936432 -4.1408772 -4.1799145 -4.2048788 -4.2165155][-4.1759391 -4.1720343 -4.1626911 -4.1433244 -4.1068096 -4.0524807 -3.9996526 -3.9751952 -3.9991775 -4.0454011 -4.0883932 -4.124568 -4.1560421 -4.1757293 -4.1837869][-4.201601 -4.1991072 -4.1918192 -4.1778593 -4.1520286 -4.112855 -4.0725093 -4.0513449 -4.0660138 -4.095963 -4.1220236 -4.1422644 -4.1608396 -4.1704583 -4.1702747][-4.2360845 -4.2362347 -4.2324772 -4.2246652 -4.2099843 -4.187191 -4.1621785 -4.1466546 -4.1533346 -4.1684952 -4.1805806 -4.1868219 -4.1920795 -4.19121 -4.1818585][-4.269886 -4.2723546 -4.2722874 -4.2709451 -4.2667713 -4.2587514 -4.2478781 -4.239037 -4.2413211 -4.2460432 -4.2473035 -4.2441616 -4.2393637 -4.2298779 -4.2137084][-4.2872272 -4.2905922 -4.2934456 -4.2970304 -4.3009462 -4.3036013 -4.3027339 -4.29932 -4.299469 -4.2987404 -4.2940655 -4.2860823 -4.2771912 -4.2650757 -4.2480087][-4.284781 -4.2869377 -4.2907443 -4.2981105 -4.3075066 -4.3155103 -4.3191848 -4.3190966 -4.3190093 -4.3164954 -4.3102274 -4.3020029 -4.2934828 -4.2830529 -4.2703552][-4.270597 -4.2698383 -4.2714515 -4.2789965 -4.2909284 -4.3017712 -4.3076591 -4.3091192 -4.3092632 -4.3072782 -4.3024211 -4.2955985 -4.28816 -4.2809086 -4.2753191][-4.2527347 -4.2490749 -4.2470803 -4.2515388 -4.2620597 -4.2726893 -4.2783809 -4.2797961 -4.2804918 -4.28071 -4.2790136 -4.2747884 -4.2694983 -4.2661285 -4.2674322]]...]
INFO - root - 2017-12-07 20:20:21.034149: step 22210, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.629 sec/batch; 65h:10m:34s remains)
INFO - root - 2017-12-07 20:20:37.383677: step 22220, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.635 sec/batch; 65h:24m:19s remains)
INFO - root - 2017-12-07 20:20:53.879052: step 22230, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.690 sec/batch; 67h:35m:34s remains)
INFO - root - 2017-12-07 20:21:10.217237: step 22240, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.562 sec/batch; 62h:30m:02s remains)
INFO - root - 2017-12-07 20:21:26.688242: step 22250, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.726 sec/batch; 69h:01m:48s remains)
INFO - root - 2017-12-07 20:21:43.011545: step 22260, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.572 sec/batch; 62h:53m:09s remains)
INFO - root - 2017-12-07 20:21:59.303025: step 22270, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.636 sec/batch; 65h:25m:26s remains)
INFO - root - 2017-12-07 20:22:15.588829: step 22280, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.581 sec/batch; 63h:14m:12s remains)
INFO - root - 2017-12-07 20:22:31.848201: step 22290, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.555 sec/batch; 62h:11m:50s remains)
INFO - root - 2017-12-07 20:22:48.400375: step 22300, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.736 sec/batch; 69h:24m:40s remains)
2017-12-07 20:22:49.705527: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22233 -4.2484074 -4.2761679 -4.2904134 -4.2922473 -4.2782249 -4.2527943 -4.227911 -4.22483 -4.2439032 -4.275403 -4.3006339 -4.3066716 -4.3116441 -4.3174124][-4.1661711 -4.2123289 -4.2621555 -4.2913504 -4.2987428 -4.2828479 -4.2443824 -4.2042069 -4.1931229 -4.217988 -4.2621694 -4.2933197 -4.3017693 -4.3080635 -4.3145247][-4.0998912 -4.1677566 -4.2341871 -4.276391 -4.2915993 -4.2723188 -4.2157907 -4.1564822 -4.1365461 -4.1700239 -4.2303834 -4.2745595 -4.2906528 -4.3030996 -4.3116331][-4.05104 -4.1351871 -4.2115321 -4.2592454 -4.2769823 -4.247736 -4.1702251 -4.0886488 -4.0636196 -4.1074715 -4.1863184 -4.2473116 -4.2752767 -4.2973027 -4.3096638][-4.0398574 -4.1300406 -4.2096648 -4.2532835 -4.262496 -4.2169523 -4.113225 -3.999233 -3.9708264 -4.0343108 -4.1404872 -4.2204 -4.2608385 -4.292119 -4.3078828][-4.0614281 -4.1447954 -4.2221446 -4.2550311 -4.25177 -4.1880813 -4.0445862 -3.875643 -3.8429341 -3.95329 -4.0982404 -4.1965852 -4.2480259 -4.28915 -4.3080015][-4.0973034 -4.1633406 -4.2347226 -4.2565145 -4.2387638 -4.1588259 -3.9744635 -3.7372367 -3.7035525 -3.8877478 -4.0724406 -4.1828933 -4.2416229 -4.2883549 -4.3089433][-4.1253786 -4.1662688 -4.2307916 -4.2507563 -4.2234473 -4.1358995 -3.9448886 -3.6705773 -3.6394343 -3.8774953 -4.0755377 -4.1882253 -4.2489471 -4.2929 -4.3103557][-4.1417341 -4.1532111 -4.2069674 -4.2357 -4.2169504 -4.1427951 -3.9884045 -3.7691066 -3.7482505 -3.9473648 -4.1141338 -4.2141705 -4.2685552 -4.3031783 -4.3148155][-4.1561012 -4.1422882 -4.1770315 -4.2130103 -4.2171836 -4.1722679 -4.0724282 -3.9358082 -3.9166558 -4.0430384 -4.1637487 -4.2434912 -4.2870164 -4.3137264 -4.32088][-4.1822972 -4.158812 -4.1682916 -4.1945333 -4.2132535 -4.1953125 -4.1413317 -4.0660896 -4.0476537 -4.1221113 -4.2051225 -4.2648087 -4.2968197 -4.3194656 -4.3246384][-4.2093172 -4.1891236 -4.1776357 -4.1857853 -4.207129 -4.2038989 -4.1776595 -4.135376 -4.1177468 -4.1638479 -4.2230325 -4.2689815 -4.2951736 -4.3169179 -4.3234239][-4.2249589 -4.2156153 -4.1931782 -4.18222 -4.1967983 -4.1970658 -4.1804628 -4.1538544 -4.1394711 -4.1780443 -4.2298036 -4.2660379 -4.2884922 -4.3101215 -4.3188438][-4.2253551 -4.2278991 -4.2038965 -4.1829824 -4.1828542 -4.1816244 -4.1671252 -4.1436467 -4.1317997 -4.170929 -4.2208676 -4.2528162 -4.2765446 -4.3005719 -4.3136878][-4.2099624 -4.2198229 -4.2033343 -4.1821661 -4.173737 -4.1704321 -4.1565456 -4.1304417 -4.1162972 -4.1547785 -4.2029614 -4.2332273 -4.2595496 -4.291039 -4.3088946]]...]
INFO - root - 2017-12-07 20:23:06.014356: step 22310, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.654 sec/batch; 66h:07m:08s remains)
INFO - root - 2017-12-07 20:23:22.250942: step 22320, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 1.581 sec/batch; 63h:13m:08s remains)
INFO - root - 2017-12-07 20:23:38.458075: step 22330, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.659 sec/batch; 66h:20m:09s remains)
INFO - root - 2017-12-07 20:23:54.565825: step 22340, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.599 sec/batch; 63h:55m:55s remains)
INFO - root - 2017-12-07 20:24:10.947376: step 22350, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 1.777 sec/batch; 71h:01m:05s remains)
INFO - root - 2017-12-07 20:24:27.225269: step 22360, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.615 sec/batch; 64h:33m:54s remains)
INFO - root - 2017-12-07 20:24:43.587502: step 22370, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.729 sec/batch; 69h:05m:51s remains)
INFO - root - 2017-12-07 20:24:59.636482: step 22380, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.548 sec/batch; 61h:52m:46s remains)
INFO - root - 2017-12-07 20:25:15.880895: step 22390, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.605 sec/batch; 64h:08m:25s remains)
INFO - root - 2017-12-07 20:25:32.079156: step 22400, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.655 sec/batch; 66h:07m:17s remains)
2017-12-07 20:25:33.522079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2845397 -4.2854524 -4.2759562 -4.2598715 -4.2521482 -4.2495551 -4.2559066 -4.2638292 -4.2647495 -4.2609463 -4.250627 -4.2398624 -4.23711 -4.24535 -4.2559333][-4.2755723 -4.2806931 -4.2710133 -4.2513041 -4.2408347 -4.2373533 -4.2433605 -4.2539883 -4.2553596 -4.2485766 -4.2321582 -4.2193661 -4.21851 -4.2291131 -4.2446156][-4.2757726 -4.2846556 -4.2753439 -4.2514739 -4.2353249 -4.2293048 -4.2357817 -4.2520218 -4.2563405 -4.2461653 -4.2227912 -4.2049775 -4.2016821 -4.210144 -4.226799][-4.2884817 -4.2977843 -4.2868466 -4.2547693 -4.2256422 -4.2097855 -4.2158484 -4.2428684 -4.2586055 -4.2514377 -4.2249889 -4.2007689 -4.193943 -4.2006 -4.2183404][-4.3023152 -4.3115158 -4.2998962 -4.2610164 -4.2122865 -4.1756735 -4.1753621 -4.21662 -4.2523637 -4.2574892 -4.2348332 -4.2082233 -4.2024336 -4.211935 -4.2288527][-4.2975893 -4.3060942 -4.2976089 -4.2599254 -4.1978421 -4.1335382 -4.1112165 -4.1635089 -4.22879 -4.2579746 -4.2501607 -4.2263217 -4.2204351 -4.23164 -4.2478328][-4.2771358 -4.287704 -4.286859 -4.2564883 -4.1880484 -4.0941525 -4.0316982 -4.0764594 -4.1720767 -4.2378139 -4.2544146 -4.2398362 -4.2281938 -4.2348075 -4.2498913][-4.2479391 -4.2582679 -4.2658877 -4.2510247 -4.1938167 -4.0905762 -3.9896085 -3.9961219 -4.0979381 -4.1938286 -4.2394485 -4.2401438 -4.2234344 -4.2183933 -4.2235279][-4.2223687 -4.2232075 -4.2309709 -4.233748 -4.2025337 -4.1206951 -4.0183773 -3.9835341 -4.0475187 -4.1388259 -4.2026896 -4.2248931 -4.215848 -4.2051544 -4.2003808][-4.2189631 -4.208549 -4.2101159 -4.2250609 -4.2225142 -4.1773582 -4.1044407 -4.0558124 -4.0720868 -4.1314335 -4.1900277 -4.2227488 -4.225677 -4.2140479 -4.2037907][-4.2315464 -4.2155252 -4.2073584 -4.2219357 -4.2338123 -4.2197008 -4.1834474 -4.1464329 -4.1388679 -4.1628885 -4.1995335 -4.2269177 -4.2370825 -4.2309613 -4.2243853][-4.2470436 -4.2339244 -4.2234907 -4.2293086 -4.2385225 -4.2375979 -4.2261314 -4.2092333 -4.1995082 -4.2036214 -4.2178907 -4.2325754 -4.2419982 -4.24235 -4.2445221][-4.2594643 -4.2521877 -4.2454367 -4.24549 -4.2490082 -4.2527633 -4.2554908 -4.2544994 -4.2492628 -4.2457933 -4.2435193 -4.2433925 -4.2470903 -4.25162 -4.2584639][-4.2740273 -4.2707276 -4.2674465 -4.2648058 -4.2627172 -4.2653027 -4.2743845 -4.2830806 -4.28535 -4.2828135 -4.2745776 -4.2661662 -4.2637205 -4.2690649 -4.2766657][-4.293344 -4.2928586 -4.2922764 -4.2888308 -4.2832971 -4.2833815 -4.29111 -4.3011346 -4.3066645 -4.3066082 -4.3011322 -4.2931633 -4.2876911 -4.2900577 -4.295136]]...]
INFO - root - 2017-12-07 20:25:49.805956: step 22410, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.572 sec/batch; 62h:49m:27s remains)
INFO - root - 2017-12-07 20:26:06.225127: step 22420, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 1.602 sec/batch; 64h:00m:10s remains)
INFO - root - 2017-12-07 20:26:22.394447: step 22430, loss = 2.06, batch loss = 2.01 (10.5 examples/sec; 1.530 sec/batch; 61h:06m:18s remains)
INFO - root - 2017-12-07 20:26:38.524158: step 22440, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.614 sec/batch; 64h:28m:58s remains)
INFO - root - 2017-12-07 20:26:54.812970: step 22450, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.668 sec/batch; 66h:37m:59s remains)
INFO - root - 2017-12-07 20:27:10.848255: step 22460, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.682 sec/batch; 67h:11m:11s remains)
INFO - root - 2017-12-07 20:27:27.086833: step 22470, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.594 sec/batch; 63h:40m:06s remains)
INFO - root - 2017-12-07 20:27:43.340127: step 22480, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.729 sec/batch; 69h:02m:56s remains)
INFO - root - 2017-12-07 20:27:59.347314: step 22490, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.574 sec/batch; 62h:50m:19s remains)
INFO - root - 2017-12-07 20:28:15.708327: step 22500, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 1.705 sec/batch; 68h:03m:51s remains)
2017-12-07 20:28:17.115770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2849259 -4.2652254 -4.2461305 -4.2362947 -4.2324066 -4.2252631 -4.2081537 -4.1896248 -4.1887422 -4.2012095 -4.2225628 -4.2489777 -4.2704935 -4.2861571 -4.30075][-4.2587924 -4.2302289 -4.2065067 -4.1979613 -4.1969962 -4.1909575 -4.1709957 -4.1488581 -4.1475091 -4.1639662 -4.1938195 -4.2289362 -4.2586985 -4.2790413 -4.2967534][-4.238863 -4.2102361 -4.1893039 -4.1866446 -4.1873326 -4.1784172 -4.1522408 -4.1283021 -4.1231894 -4.1415696 -4.1757851 -4.2145061 -4.2500825 -4.2761364 -4.2971649][-4.2441511 -4.2285547 -4.2195368 -4.2231851 -4.2209458 -4.20427 -4.1688066 -4.1401048 -4.131391 -4.1447239 -4.1764879 -4.2115283 -4.2445965 -4.2742028 -4.3003745][-4.2581406 -4.2570572 -4.2569742 -4.2596674 -4.2485633 -4.2219129 -4.1765904 -4.1459827 -4.1410151 -4.1501408 -4.1746273 -4.2046924 -4.2351842 -4.2683668 -4.2999878][-4.2607803 -4.270421 -4.2733016 -4.27002 -4.2457442 -4.2074227 -4.14994 -4.1140351 -4.1165781 -4.130229 -4.1515756 -4.1810069 -4.2144504 -4.252862 -4.2908125][-4.2333155 -4.2518439 -4.2525649 -4.2352672 -4.1911135 -4.1309791 -4.0463257 -3.9907417 -4.0079112 -4.0471549 -4.0824466 -4.1282353 -4.1781678 -4.2296114 -4.2760887][-4.1772881 -4.1961918 -4.1902428 -4.159431 -4.0919852 -3.9994054 -3.8735423 -3.7952254 -3.8470025 -3.9282789 -3.9930468 -4.065958 -4.1401176 -4.2076411 -4.2626681][-4.143805 -4.1453533 -4.1272712 -4.0882859 -4.0109921 -3.9025512 -3.7633042 -3.6926904 -3.7805927 -3.8849216 -3.962976 -4.0503163 -4.1342087 -4.2040067 -4.2598014][-4.1555114 -4.1446819 -4.1243939 -4.0953674 -4.036716 -3.9575412 -3.8677337 -3.8276649 -3.8899839 -3.9632647 -4.019866 -4.0914068 -4.1616516 -4.2190275 -4.2674][-4.19685 -4.1847858 -4.1695075 -4.1560922 -4.1217904 -4.0786252 -4.0292792 -4.0018482 -4.03106 -4.0698438 -4.1015973 -4.15211 -4.2025824 -4.2434945 -4.2804756][-4.2514625 -4.2431908 -4.2321076 -4.2264843 -4.2073994 -4.1823435 -4.15091 -4.131691 -4.1430569 -4.1593513 -4.175231 -4.2091923 -4.2422891 -4.26958 -4.2964435][-4.303031 -4.2961669 -4.2862425 -4.2798033 -4.2668109 -4.2527905 -4.2350197 -4.224546 -4.2293429 -4.2312131 -4.2361403 -4.2552772 -4.2747965 -4.2906976 -4.30988][-4.3343749 -4.3315558 -4.3230205 -4.3148813 -4.3047061 -4.2983971 -4.2897334 -4.2843289 -4.2867622 -4.2812157 -4.2779164 -4.2864957 -4.2971897 -4.3068342 -4.3208442][-4.332159 -4.3338394 -4.3312054 -4.3273468 -4.3200979 -4.3143587 -4.3065057 -4.3021965 -4.3034563 -4.29626 -4.2912855 -4.2946124 -4.3008413 -4.3087254 -4.32232]]...]
INFO - root - 2017-12-07 20:28:33.289600: step 22510, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.727 sec/batch; 68h:56m:10s remains)
INFO - root - 2017-12-07 20:28:49.418954: step 22520, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.590 sec/batch; 63h:29m:25s remains)
INFO - root - 2017-12-07 20:29:05.708020: step 22530, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.648 sec/batch; 65h:47m:59s remains)
INFO - root - 2017-12-07 20:29:21.939322: step 22540, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.624 sec/batch; 64h:48m:52s remains)
INFO - root - 2017-12-07 20:29:37.938548: step 22550, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.715 sec/batch; 68h:28m:01s remains)
INFO - root - 2017-12-07 20:29:54.013084: step 22560, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.551 sec/batch; 61h:54m:00s remains)
INFO - root - 2017-12-07 20:30:10.259580: step 22570, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.687 sec/batch; 67h:20m:05s remains)
INFO - root - 2017-12-07 20:30:26.503237: step 22580, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.555 sec/batch; 62h:02m:19s remains)
INFO - root - 2017-12-07 20:30:42.901793: step 22590, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.589 sec/batch; 63h:23m:46s remains)
INFO - root - 2017-12-07 20:30:59.360901: step 22600, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.682 sec/batch; 67h:06m:56s remains)
2017-12-07 20:31:00.710837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2160397 -4.2139654 -4.2205982 -4.227272 -4.2098441 -4.1812344 -4.1522074 -4.1232777 -4.1019664 -4.1079993 -4.130403 -4.1437249 -4.1568651 -4.1683083 -4.1629443][-4.20697 -4.2031169 -4.2122321 -4.2168617 -4.1891294 -4.147388 -4.1137543 -4.0823097 -4.0589924 -4.0611205 -4.0780339 -4.082355 -4.08449 -4.0926375 -4.0938625][-4.213685 -4.2032604 -4.2043948 -4.1964335 -4.1548715 -4.1017966 -4.0678992 -4.0416503 -4.0258646 -4.0301766 -4.0426993 -4.0404034 -4.0332403 -4.0365887 -4.042335][-4.2198482 -4.1951785 -4.1823549 -4.1629324 -4.1145816 -4.0578003 -4.0291739 -4.0175 -4.0133047 -4.0215011 -4.0341473 -4.0332475 -4.0248218 -4.0250359 -4.0379014][-4.2141309 -4.1759105 -4.1499405 -4.128016 -4.0864224 -4.0382385 -4.0261831 -4.0378442 -4.0507565 -4.0601478 -4.0653424 -4.0586152 -4.0438766 -4.042809 -4.0650444][-4.2049618 -4.1596317 -4.1249719 -4.1080971 -4.0833135 -4.0500994 -4.0502238 -4.0765157 -4.0929174 -4.08984 -4.0806522 -4.0631256 -4.0377755 -4.033453 -4.0645847][-4.1991959 -4.1536126 -4.119432 -4.1067367 -4.0924735 -4.06588 -4.0704441 -4.1010623 -4.1120777 -4.0966578 -4.0733056 -4.0402064 -4.005631 -4.0029945 -4.0421152][-4.2002044 -4.1572294 -4.1251683 -4.1125388 -4.104526 -4.0842385 -4.09004 -4.1182885 -4.1282616 -4.109704 -4.0753078 -4.0306482 -3.9924235 -3.9893482 -4.0206165][-4.2032752 -4.1619997 -4.1303196 -4.1171336 -4.1109643 -4.0942936 -4.0970974 -4.1205678 -4.137629 -4.1294513 -4.1008706 -4.061708 -4.0304317 -4.0257668 -4.0441856][-4.2184343 -4.1846285 -4.1554546 -4.1415482 -4.1342115 -4.1216922 -4.1235733 -4.1445794 -4.1689191 -4.1731505 -4.1573772 -4.13319 -4.1137767 -4.1102138 -4.1191859][-4.2603 -4.2378831 -4.2150083 -4.2017369 -4.1949482 -4.1870923 -4.1870255 -4.20253 -4.2245693 -4.230319 -4.221921 -4.2102 -4.2021065 -4.2051163 -4.2136283][-4.3011665 -4.2901759 -4.2775965 -4.2701378 -4.2677407 -4.2608891 -4.2568712 -4.2638307 -4.2779388 -4.2792916 -4.2723532 -4.2669849 -4.2667818 -4.2743864 -4.2828226][-4.3240867 -4.3182664 -4.312427 -4.3101134 -4.3113074 -4.3084188 -4.304637 -4.307806 -4.315177 -4.3145323 -4.3100977 -4.3081794 -4.3102269 -4.3169026 -4.3237247][-4.3387203 -4.3336558 -4.3290634 -4.3281422 -4.3302975 -4.33053 -4.3298616 -4.3326979 -4.3364568 -4.3353171 -4.3333778 -4.3329229 -4.3341589 -4.3376312 -4.3416338][-4.35002 -4.3467126 -4.3435426 -4.3430638 -4.3444118 -4.3449273 -4.3454108 -4.3472066 -4.3485093 -4.3471532 -4.3454733 -4.3445888 -4.3443003 -4.3453393 -4.3471017]]...]
INFO - root - 2017-12-07 20:31:16.979237: step 22610, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.617 sec/batch; 64h:32m:12s remains)
INFO - root - 2017-12-07 20:31:33.268833: step 22620, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.611 sec/batch; 64h:16m:54s remains)
INFO - root - 2017-12-07 20:31:49.495082: step 22630, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.701 sec/batch; 67h:51m:02s remains)
INFO - root - 2017-12-07 20:32:05.670073: step 22640, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.527 sec/batch; 60h:54m:59s remains)
INFO - root - 2017-12-07 20:32:22.044518: step 22650, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.725 sec/batch; 68h:47m:29s remains)
INFO - root - 2017-12-07 20:32:38.325291: step 22660, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.720 sec/batch; 68h:36m:52s remains)
INFO - root - 2017-12-07 20:32:54.343395: step 22670, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.658 sec/batch; 66h:08m:25s remains)
INFO - root - 2017-12-07 20:33:10.641892: step 22680, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.567 sec/batch; 62h:30m:37s remains)
INFO - root - 2017-12-07 20:33:26.951506: step 22690, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.609 sec/batch; 64h:08m:42s remains)
INFO - root - 2017-12-07 20:33:43.204172: step 22700, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.688 sec/batch; 67h:18m:19s remains)
2017-12-07 20:33:44.537936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2504535 -4.2473421 -4.2430897 -4.2416086 -4.2399988 -4.2365646 -4.229557 -4.2210221 -4.218822 -4.2289896 -4.2444887 -4.2603979 -4.2691731 -4.2683721 -4.2620435][-4.25375 -4.2504888 -4.2439432 -4.2425489 -4.2413535 -4.2383504 -4.2346725 -4.2311597 -4.2306004 -4.2357697 -4.2432628 -4.2542295 -4.2644868 -4.266953 -4.2619081][-4.2492027 -4.2456722 -4.2360544 -4.2328391 -4.2314739 -4.2288003 -4.2253113 -4.2256784 -4.2282543 -4.2308927 -4.2296596 -4.232326 -4.2401886 -4.2501426 -4.2539935][-4.2413015 -4.2349739 -4.2195454 -4.2107735 -4.20846 -4.2101936 -4.2122664 -4.2177715 -4.2260966 -4.2323947 -4.224411 -4.216476 -4.2181044 -4.2307711 -4.2423558][-4.2338853 -4.2174516 -4.1899433 -4.1694183 -4.1584187 -4.1639462 -4.1797261 -4.1990771 -4.2201881 -4.2370405 -4.2318983 -4.2176371 -4.2142439 -4.22141 -4.2303805][-4.2271147 -4.1937337 -4.1469069 -4.1065407 -4.0753336 -4.0737991 -4.1009741 -4.1420879 -4.1839223 -4.2123551 -4.2155447 -4.20596 -4.2055435 -4.211658 -4.2165627][-4.2210512 -4.1690192 -4.1020069 -4.0398026 -3.9806628 -3.9498532 -3.970777 -4.035646 -4.114275 -4.1682143 -4.185626 -4.1875582 -4.1943979 -4.2024188 -4.2075295][-4.2177444 -4.1524658 -4.06998 -3.9854326 -3.8932521 -3.8232288 -3.8118758 -3.8873131 -4.0135818 -4.1088471 -4.1494408 -4.1652942 -4.182241 -4.1999583 -4.2066555][-4.226326 -4.1654668 -4.0876145 -3.9994321 -3.9022024 -3.8268232 -3.7941346 -3.8484941 -3.9811971 -4.0929737 -4.1415963 -4.1606512 -4.1813979 -4.2052178 -4.2130742][-4.2321734 -4.1863856 -4.1270084 -4.0620842 -3.9996448 -3.9597092 -3.9396467 -3.9604983 -4.0442328 -4.1295805 -4.1669559 -4.1778359 -4.1927485 -4.2137885 -4.2204041][-4.2243934 -4.1932364 -4.1528354 -4.11619 -4.0904164 -4.07581 -4.0651879 -4.0658865 -4.1074367 -4.1675453 -4.1983671 -4.2023115 -4.2078719 -4.2230787 -4.2287431][-4.2158623 -4.1982145 -4.1756005 -4.1571956 -4.1529431 -4.152071 -4.1441574 -4.1310234 -4.14877 -4.2000418 -4.23233 -4.233304 -4.2304611 -4.2415838 -4.2470527][-4.2042603 -4.1927648 -4.1809387 -4.1725073 -4.1785345 -4.1883535 -4.1854515 -4.1686492 -4.1800885 -4.2333641 -4.2702546 -4.2702074 -4.2632332 -4.2716675 -4.2745252][-4.2072992 -4.198329 -4.192687 -4.1932564 -4.2052927 -4.221128 -4.2232056 -4.2069216 -4.2142777 -4.2639184 -4.3015575 -4.3028326 -4.2954473 -4.2979817 -4.2967558][-4.2335277 -4.2282438 -4.2286515 -4.2323389 -4.2436953 -4.2568364 -4.2618456 -4.2494879 -4.2515879 -4.2908778 -4.3233995 -4.3279328 -4.3208685 -4.3169069 -4.31157]]...]
INFO - root - 2017-12-07 20:34:00.868773: step 22710, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.712 sec/batch; 68h:15m:45s remains)
INFO - root - 2017-12-07 20:34:16.949816: step 22720, loss = 2.06, batch loss = 2.01 (10.3 examples/sec; 1.548 sec/batch; 61h:41m:57s remains)
INFO - root - 2017-12-07 20:34:33.461917: step 22730, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.732 sec/batch; 69h:02m:53s remains)
INFO - root - 2017-12-07 20:34:49.466736: step 22740, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.656 sec/batch; 66h:00m:55s remains)
INFO - root - 2017-12-07 20:35:05.655552: step 22750, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.623 sec/batch; 64h:41m:11s remains)
INFO - root - 2017-12-07 20:35:22.005423: step 22760, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.670 sec/batch; 66h:34m:01s remains)
INFO - root - 2017-12-07 20:35:38.296358: step 22770, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.609 sec/batch; 64h:06m:59s remains)
INFO - root - 2017-12-07 20:35:54.614311: step 22780, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.686 sec/batch; 67h:11m:32s remains)
INFO - root - 2017-12-07 20:36:10.831745: step 22790, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.599 sec/batch; 63h:43m:14s remains)
INFO - root - 2017-12-07 20:36:27.159963: step 22800, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.651 sec/batch; 65h:46m:45s remains)
2017-12-07 20:36:28.542344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2249584 -4.2324381 -4.2538986 -4.272788 -4.2759151 -4.2753897 -4.2739687 -4.2666454 -4.2534924 -4.244554 -4.2430482 -4.246943 -4.2524571 -4.2612915 -4.2685719][-4.1977959 -4.2049789 -4.2295628 -4.2509427 -4.2567883 -4.2539868 -4.24921 -4.2395287 -4.225502 -4.2183971 -4.2180352 -4.2244968 -4.2299447 -4.2360544 -4.2441096][-4.1947589 -4.1987839 -4.2223945 -4.2423873 -4.2447758 -4.2384748 -4.2343407 -4.231102 -4.2241616 -4.22206 -4.221796 -4.221168 -4.21653 -4.2130365 -4.2160335][-4.2272964 -4.2252789 -4.2420435 -4.2569723 -4.2552676 -4.2452507 -4.24035 -4.2418265 -4.2432251 -4.2451167 -4.2450156 -4.23506 -4.2143421 -4.19472 -4.1904449][-4.271966 -4.264966 -4.2747183 -4.2814589 -4.2723527 -4.2543669 -4.239212 -4.2385182 -4.2448397 -4.2512426 -4.2450414 -4.2193279 -4.1792526 -4.1460342 -4.1403217][-4.3090591 -4.2991538 -4.301693 -4.2963057 -4.2745967 -4.24346 -4.2139821 -4.20737 -4.2205448 -4.2421737 -4.23474 -4.1957417 -4.1393251 -4.1007652 -4.1033058][-4.3224988 -4.3113904 -4.3069406 -4.2868452 -4.2502666 -4.2018595 -4.1545792 -4.1449809 -4.1795878 -4.22804 -4.2367015 -4.202672 -4.1500583 -4.1098852 -4.110816][-4.318306 -4.3119183 -4.3073225 -4.2859869 -4.2458944 -4.1860032 -4.1198964 -4.0982819 -4.1443424 -4.2181997 -4.2521787 -4.2401938 -4.2038536 -4.1651239 -4.1502137][-4.3213315 -4.32068 -4.3199115 -4.3054614 -4.2712708 -4.2173853 -4.1491094 -4.1138668 -4.1476088 -4.2220178 -4.27092 -4.2759857 -4.2536821 -4.2215457 -4.1965885][-4.3295274 -4.32906 -4.3313656 -4.3239069 -4.3007817 -4.2632508 -4.2100544 -4.1714025 -4.1845226 -4.2384338 -4.2835531 -4.2952476 -4.2806869 -4.2549858 -4.2287321][-4.337976 -4.3400779 -4.3435397 -4.3423414 -4.3292451 -4.3077273 -4.2697029 -4.2306385 -4.224587 -4.2537179 -4.2869253 -4.298213 -4.2872977 -4.2668991 -4.2461486][-4.3365159 -4.3419333 -4.3465114 -4.3500056 -4.3446097 -4.3336821 -4.3065362 -4.2711873 -4.25576 -4.2661996 -4.2866321 -4.2940903 -4.2862921 -4.2742739 -4.2632575][-4.3291116 -4.3357177 -4.341248 -4.3470769 -4.3476138 -4.3434525 -4.3254709 -4.2971773 -4.2809548 -4.2833166 -4.2946105 -4.2962584 -4.2896409 -4.2846704 -4.2800603][-4.3245521 -4.3295279 -4.3348165 -4.3411965 -4.3445315 -4.345623 -4.3364248 -4.3173184 -4.3034306 -4.3020186 -4.3079453 -4.3059983 -4.3005433 -4.2999945 -4.3015289][-4.3225827 -4.3243942 -4.3270187 -4.33076 -4.3337674 -4.3362484 -4.3339725 -4.3241768 -4.3144293 -4.3117008 -4.3146143 -4.3115282 -4.3067412 -4.3079858 -4.3120418]]...]
INFO - root - 2017-12-07 20:36:44.640548: step 22810, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.660 sec/batch; 66h:07m:26s remains)
INFO - root - 2017-12-07 20:37:00.840260: step 22820, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.625 sec/batch; 64h:45m:04s remains)
INFO - root - 2017-12-07 20:37:17.244435: step 22830, loss = 2.08, batch loss = 2.03 (10.5 examples/sec; 1.529 sec/batch; 60h:54m:16s remains)
INFO - root - 2017-12-07 20:37:33.296937: step 22840, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.556 sec/batch; 61h:59m:50s remains)
INFO - root - 2017-12-07 20:37:49.618105: step 22850, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.546 sec/batch; 61h:35m:57s remains)
INFO - root - 2017-12-07 20:38:06.007959: step 22860, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.723 sec/batch; 68h:37m:47s remains)
INFO - root - 2017-12-07 20:38:21.979023: step 22870, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.578 sec/batch; 62h:50m:29s remains)
INFO - root - 2017-12-07 20:38:38.379275: step 22880, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.711 sec/batch; 68h:09m:08s remains)
INFO - root - 2017-12-07 20:38:54.657091: step 22890, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.615 sec/batch; 64h:18m:44s remains)
INFO - root - 2017-12-07 20:39:10.677453: step 22900, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 1.707 sec/batch; 67h:58m:15s remains)
2017-12-07 20:39:12.016339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2428718 -4.2697415 -4.2877307 -4.2821684 -4.259624 -4.2435026 -4.2279568 -4.2147989 -4.2114534 -4.2244005 -4.2412329 -4.245172 -4.2438149 -4.2418046 -4.2374763][-4.2998934 -4.3191381 -4.3254333 -4.3064013 -4.2762733 -4.2540236 -4.2323585 -4.2132945 -4.21073 -4.2281675 -4.2496872 -4.2525678 -4.2462769 -4.2415481 -4.2416344][-4.363718 -4.3771496 -4.3754015 -4.3447471 -4.3015075 -4.2623529 -4.23106 -4.2055316 -4.1985688 -4.2211533 -4.2576675 -4.2705364 -4.2658081 -4.2609262 -4.2674003][-4.4118805 -4.4188046 -4.4099193 -4.3671389 -4.3092055 -4.2540755 -4.2169685 -4.1909947 -4.1828327 -4.2100425 -4.25719 -4.278842 -4.2803354 -4.2830563 -4.3008971][-4.43506 -4.435709 -4.4156413 -4.3642936 -4.2953424 -4.2257838 -4.1835008 -4.15931 -4.1576118 -4.185864 -4.2275057 -4.2507677 -4.2617836 -4.281219 -4.3130045][-4.4263115 -4.4096665 -4.3708963 -4.3112087 -4.2327337 -4.1497669 -4.0985422 -4.0769091 -4.0870275 -4.1179457 -4.1504107 -4.1713362 -4.1981034 -4.2409396 -4.2924681][-4.3785386 -4.3331261 -4.2696729 -4.1968017 -4.1027813 -4.0044827 -3.9406688 -3.9257841 -3.9528689 -3.9897132 -4.0155296 -4.0420909 -4.0945635 -4.1700315 -4.2465124][-4.3022718 -4.2252927 -4.1348958 -4.0433559 -3.9370322 -3.8342843 -3.7720227 -3.7734754 -3.8181863 -3.8573313 -3.8786182 -3.9183347 -3.9993911 -4.1037073 -4.2025118][-4.2291703 -4.1303444 -4.0228934 -3.9239073 -3.82893 -3.7518096 -3.7155559 -3.7347412 -3.7834735 -3.8153267 -3.8338225 -3.883455 -3.9776185 -4.0902929 -4.1925321][-4.1972389 -4.1010642 -4.0026011 -3.9211555 -3.856544 -3.8130929 -3.8011532 -3.8248761 -3.8633657 -3.8899517 -3.9131188 -3.9669507 -4.0529575 -4.1466722 -4.2264891][-4.2206254 -4.1522088 -4.0839334 -4.03054 -3.9906287 -3.9701381 -3.9721723 -3.9913113 -4.0161457 -4.0385509 -4.0642467 -4.1103559 -4.1723175 -4.2341084 -4.2830391][-4.27221 -4.2357159 -4.1979566 -4.169354 -4.1481004 -4.140882 -4.1480813 -4.16101 -4.1751609 -4.1908236 -4.2102551 -4.2390995 -4.2752953 -4.3079009 -4.3307633][-4.318501 -4.3053322 -4.2899823 -4.2780094 -4.2681522 -4.2668443 -4.2727485 -4.2798352 -4.2873368 -4.2953577 -4.3062644 -4.3201327 -4.3363972 -4.349103 -4.3561592][-4.3417373 -4.3419924 -4.3380327 -4.3335247 -4.3284397 -4.329412 -4.3332477 -4.3358755 -4.3390937 -4.3429451 -4.3468466 -4.3513217 -4.3572011 -4.3609495 -4.3614264][-4.3381543 -4.3441429 -4.3455596 -4.3444886 -4.3417983 -4.3430214 -4.3457675 -4.3474088 -4.3489556 -4.3507333 -4.3511095 -4.3514009 -4.3525567 -4.3531318 -4.3515315]]...]
INFO - root - 2017-12-07 20:39:28.149778: step 22910, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.643 sec/batch; 65h:23m:59s remains)
INFO - root - 2017-12-07 20:39:44.305607: step 22920, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.621 sec/batch; 64h:32m:19s remains)
INFO - root - 2017-12-07 20:40:00.520007: step 22930, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.697 sec/batch; 67h:32m:24s remains)
INFO - root - 2017-12-07 20:40:16.879829: step 22940, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.588 sec/batch; 63h:14m:07s remains)
INFO - root - 2017-12-07 20:40:33.083288: step 22950, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.668 sec/batch; 66h:22m:55s remains)
INFO - root - 2017-12-07 20:40:49.270418: step 22960, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.572 sec/batch; 62h:33m:49s remains)
INFO - root - 2017-12-07 20:41:05.601967: step 22970, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.584 sec/batch; 63h:03m:11s remains)
INFO - root - 2017-12-07 20:41:21.904851: step 22980, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.666 sec/batch; 66h:18m:11s remains)
INFO - root - 2017-12-07 20:41:38.041921: step 22990, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.601 sec/batch; 63h:43m:29s remains)
INFO - root - 2017-12-07 20:41:54.440053: step 23000, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.688 sec/batch; 67h:09m:54s remains)
2017-12-07 20:41:55.862169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2864542 -4.2906818 -4.2888823 -4.2760205 -4.2523961 -4.2348447 -4.2210822 -4.2167821 -4.2177238 -4.224772 -4.2421117 -4.2468266 -4.2532339 -4.2579041 -4.263701][-4.2689185 -4.2753115 -4.281661 -4.2784228 -4.2616334 -4.251049 -4.2443237 -4.2451391 -4.2446914 -4.2434993 -4.2516465 -4.248816 -4.2468262 -4.2424865 -4.23902][-4.2540479 -4.2612038 -4.2728996 -4.2785177 -4.2709002 -4.2667255 -4.266417 -4.2680445 -4.2634711 -4.2537928 -4.2535114 -4.2427187 -4.2313371 -4.220346 -4.2128482][-4.233048 -4.2422438 -4.2547483 -4.2636828 -4.2599883 -4.25712 -4.2590132 -4.2598605 -4.2531285 -4.2408175 -4.2367916 -4.2215471 -4.202199 -4.1871686 -4.1828794][-4.1879826 -4.1961055 -4.2067561 -4.2144465 -4.210237 -4.2059684 -4.2092218 -4.2114325 -4.2084846 -4.1996608 -4.1964149 -4.1807413 -4.1547389 -4.1378694 -4.1427932][-4.1203504 -4.1234307 -4.1278439 -4.132205 -4.1249166 -4.1165519 -4.1248121 -4.1377649 -4.1452389 -4.1441011 -4.1423678 -4.1276879 -4.098978 -4.079896 -4.0907083][-4.0420437 -4.0344787 -4.0267415 -4.0228167 -4.0089455 -3.9927802 -4.0041652 -4.0326724 -4.0592513 -4.071053 -4.0717 -4.05967 -4.036345 -4.01941 -4.0344887][-3.971307 -3.9524045 -3.9262424 -3.9019184 -3.8743639 -3.8482158 -3.8561244 -3.8984811 -3.953135 -3.98864 -4.000515 -3.9992323 -3.988889 -3.9817164 -4.0016193][-3.9529605 -3.930222 -3.8916466 -3.8503549 -3.8162215 -3.792002 -3.7968624 -3.8430257 -3.9157047 -3.9683473 -3.9894338 -3.9964485 -3.9959633 -3.9966269 -4.01865][-4.0267386 -4.0095553 -3.9777212 -3.9456725 -3.9280653 -3.9234533 -3.9295125 -3.9595747 -4.0120287 -4.0500503 -4.0642586 -4.0665336 -4.0663967 -4.070426 -4.0895452][-4.1486983 -4.1453428 -4.1316819 -4.1156158 -4.1099095 -4.11256 -4.1149235 -4.1250458 -4.1480894 -4.165751 -4.1692853 -4.1657538 -4.1624293 -4.1639585 -4.175302][-4.2489686 -4.2550049 -4.2532907 -4.2484274 -4.2470741 -4.2461514 -4.2417955 -4.2381177 -4.2432303 -4.249465 -4.2497187 -4.2448611 -4.2393575 -4.2362251 -4.2393837][-4.3012233 -4.3082628 -4.3098159 -4.3099546 -4.3105927 -4.3071504 -4.2988157 -4.2897596 -4.2869105 -4.2891345 -4.2908015 -4.2897005 -4.2863483 -4.2812243 -4.278615][-4.3146024 -4.3230309 -4.3256326 -4.3271918 -4.3275332 -4.3232608 -4.3164291 -4.3110619 -4.3095074 -4.3114996 -4.3146291 -4.3160758 -4.3143315 -4.3089714 -4.3031673][-4.3006806 -4.3125987 -4.3186388 -4.322402 -4.324471 -4.3220234 -4.3184409 -4.3190179 -4.3226037 -4.32652 -4.3300128 -4.3316956 -4.3302021 -4.325202 -4.3190207]]...]
INFO - root - 2017-12-07 20:42:11.986385: step 23010, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 1.716 sec/batch; 68h:17m:43s remains)
INFO - root - 2017-12-07 20:42:28.021908: step 23020, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.671 sec/batch; 66h:29m:06s remains)
INFO - root - 2017-12-07 20:42:44.247418: step 23030, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.587 sec/batch; 63h:08m:08s remains)
INFO - root - 2017-12-07 20:43:00.446652: step 23040, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.658 sec/batch; 65h:57m:21s remains)
INFO - root - 2017-12-07 20:43:16.536468: step 23050, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.576 sec/batch; 62h:42m:22s remains)
INFO - root - 2017-12-07 20:43:32.947515: step 23060, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.678 sec/batch; 66h:43m:27s remains)
INFO - root - 2017-12-07 20:43:48.976289: step 23070, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.556 sec/batch; 61h:51m:59s remains)
INFO - root - 2017-12-07 20:44:05.357080: step 23080, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.718 sec/batch; 68h:20m:35s remains)
INFO - root - 2017-12-07 20:44:21.707170: step 23090, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.543 sec/batch; 61h:21m:17s remains)
INFO - root - 2017-12-07 20:44:37.812031: step 23100, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 1.502 sec/batch; 59h:42m:37s remains)
2017-12-07 20:44:39.218223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0312076 -4.0272937 -4.0362935 -4.0522404 -4.07259 -4.0916839 -4.1049414 -4.1090641 -4.112298 -4.1183972 -4.136271 -4.15917 -4.1695228 -4.1618829 -4.1376529][-4.0467024 -4.0440111 -4.0604787 -4.0843778 -4.1106048 -4.1269732 -4.1269007 -4.1129818 -4.0998287 -4.0997629 -4.1232038 -4.1583204 -4.1792722 -4.1765318 -4.1531029][-4.0769243 -4.0791321 -4.0990119 -4.1236272 -4.1458988 -4.1547437 -4.1405272 -4.1129971 -4.0912638 -4.0915103 -4.1248388 -4.171278 -4.2018366 -4.2054892 -4.1866231][-4.11583 -4.127564 -4.1529603 -4.1766758 -4.1911359 -4.1837463 -4.1483583 -4.105361 -4.0808949 -4.0925665 -4.1433492 -4.2009706 -4.2385974 -4.2488971 -4.2348051][-4.1661825 -4.1842775 -4.2098985 -4.2257562 -4.221755 -4.1850414 -4.1204934 -4.0606956 -4.0419326 -4.0767961 -4.1511073 -4.22081 -4.2651987 -4.2812738 -4.2708707][-4.2071767 -4.2216139 -4.23692 -4.2372179 -4.2102437 -4.1474428 -4.0622034 -3.9977684 -3.9989433 -4.0641303 -4.1574087 -4.2324 -4.27565 -4.2903676 -4.2778955][-4.2220449 -4.2281804 -4.2347713 -4.226212 -4.1905813 -4.1244197 -4.043457 -3.99182 -4.0126209 -4.08918 -4.176929 -4.2385631 -4.2687192 -4.2715988 -4.251009][-4.216197 -4.2155552 -4.2180834 -4.2098589 -4.1800523 -4.129169 -4.0733223 -4.0442123 -4.0697608 -4.1322565 -4.196959 -4.237226 -4.2514424 -4.2403326 -4.2134957][-4.2150784 -4.2069407 -4.2076473 -4.2050757 -4.1878858 -4.1562924 -4.1254416 -4.1159062 -4.1399336 -4.180687 -4.2169886 -4.2352829 -4.2379451 -4.2179718 -4.1864328][-4.2109861 -4.1982503 -4.1983981 -4.2013578 -4.1948547 -4.176487 -4.16287 -4.1668911 -4.1875329 -4.2124562 -4.2285318 -4.2335415 -4.2293205 -4.2051034 -4.1707211][-4.2084761 -4.1918716 -4.1905632 -4.1942139 -4.19219 -4.1823573 -4.177845 -4.1848311 -4.1994705 -4.2139125 -4.2194567 -4.2178168 -4.2128496 -4.1913986 -4.1616373][-4.2006598 -4.186769 -4.1849437 -4.1852255 -4.1810226 -4.1730042 -4.1735406 -4.18316 -4.1903472 -4.1947684 -4.1953139 -4.1951461 -4.19727 -4.1868758 -4.1695809][-4.2088084 -4.2001462 -4.19864 -4.1929913 -4.1829805 -4.173182 -4.1733866 -4.1829967 -4.1844664 -4.1810026 -4.1794038 -4.182713 -4.1927886 -4.1921396 -4.1866989][-4.2335763 -4.2303038 -4.23057 -4.2251234 -4.2143817 -4.2043896 -4.2021832 -4.2085547 -4.2075028 -4.2005816 -4.1970429 -4.2005386 -4.2104692 -4.2125564 -4.2128716][-4.2714548 -4.2723966 -4.274435 -4.2706566 -4.2608023 -4.2511673 -4.247292 -4.2498388 -4.2472205 -4.2411237 -4.2372842 -4.2358313 -4.2379251 -4.2379212 -4.2400565]]...]
INFO - root - 2017-12-07 20:44:55.455862: step 23110, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.659 sec/batch; 65h:58m:26s remains)
INFO - root - 2017-12-07 20:45:11.670843: step 23120, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.540 sec/batch; 61h:13m:18s remains)
INFO - root - 2017-12-07 20:45:27.958944: step 23130, loss = 2.11, batch loss = 2.05 (9.9 examples/sec; 1.612 sec/batch; 64h:05m:08s remains)
INFO - root - 2017-12-07 20:45:44.198439: step 23140, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.612 sec/batch; 64h:03m:51s remains)
INFO - root - 2017-12-07 20:46:00.434055: step 23150, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.582 sec/batch; 62h:53m:47s remains)
INFO - root - 2017-12-07 20:46:16.609334: step 23160, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.616 sec/batch; 64h:12m:59s remains)
INFO - root - 2017-12-07 20:46:32.781834: step 23170, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.598 sec/batch; 63h:29m:46s remains)
INFO - root - 2017-12-07 20:46:49.202282: step 23180, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.722 sec/batch; 68h:25m:48s remains)
INFO - root - 2017-12-07 20:47:05.239128: step 23190, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.594 sec/batch; 63h:20m:52s remains)
INFO - root - 2017-12-07 20:47:21.669950: step 23200, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.632 sec/batch; 64h:50m:57s remains)
2017-12-07 20:47:23.169660: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2728491 -4.253984 -4.2280269 -4.2201481 -4.2223864 -4.2287188 -4.2460585 -4.2745075 -4.2854609 -4.2692981 -4.252367 -4.2439313 -4.2293787 -4.217124 -4.2136364][-4.2718363 -4.2496781 -4.2162781 -4.19539 -4.1906676 -4.1937661 -4.2072139 -4.2403321 -4.2581196 -4.245666 -4.2256312 -4.2160735 -4.2040362 -4.1919279 -4.1843619][-4.2790165 -4.2565417 -4.2186446 -4.182991 -4.1611967 -4.1527014 -4.1566391 -4.1917062 -4.2226219 -4.2266326 -4.2129931 -4.20295 -4.190115 -4.174736 -4.164712][-4.2860541 -4.2668514 -4.2277346 -4.1790094 -4.1353445 -4.1083889 -4.0971885 -4.128262 -4.17871 -4.2106338 -4.2125764 -4.1988053 -4.1719241 -4.1502786 -4.1420412][-4.2852912 -4.2708631 -4.2374191 -4.1835313 -4.1213822 -4.0703106 -4.0389628 -4.0591936 -4.1231313 -4.1810393 -4.2012882 -4.1859784 -4.1427922 -4.1073947 -4.0984392][-4.2794638 -4.2702365 -4.2426367 -4.1852317 -4.1046271 -4.0244355 -3.9689443 -3.9774773 -4.0549393 -4.1382656 -4.1778579 -4.1681318 -4.1136007 -4.0573711 -4.0396347][-4.2701035 -4.2632465 -4.2378817 -4.1809959 -4.0895276 -3.9866226 -3.9118626 -3.9090939 -3.9992137 -4.1046209 -4.1642222 -4.1630011 -4.1026831 -4.0287018 -3.9941781][-4.2676225 -4.2647805 -4.2411547 -4.187355 -4.0976133 -3.9918137 -3.911015 -3.8949151 -3.9811261 -4.0970559 -4.1709871 -4.1769695 -4.1198363 -4.0380187 -3.9867077][-4.2745066 -4.2775121 -4.2528992 -4.197053 -4.1107607 -4.0145693 -3.9435408 -3.9255657 -3.9995344 -4.1106448 -4.1889563 -4.2008648 -4.1525393 -4.0730133 -4.0128927][-4.2822471 -4.2858691 -4.259675 -4.1993036 -4.1120954 -4.0259051 -3.9757576 -3.973207 -4.0412579 -4.1392827 -4.212132 -4.2253852 -4.1864352 -4.1152172 -4.056169][-4.2891135 -4.2920165 -4.2679281 -4.2059331 -4.12157 -4.0509048 -4.0264993 -4.0377374 -4.0969472 -4.1786556 -4.2387481 -4.2488627 -4.2171645 -4.1562557 -4.1002407][-4.2994246 -4.3023596 -4.2841234 -4.2290945 -4.1566796 -4.1002049 -4.0909781 -4.1066642 -4.15239 -4.2182779 -4.2649131 -4.2716341 -4.2463708 -4.1970067 -4.14509][-4.3125319 -4.3172264 -4.3089995 -4.26786 -4.209722 -4.1619816 -4.1506476 -4.1628985 -4.1972508 -4.2516007 -4.2889471 -4.2950649 -4.2764716 -4.2364497 -4.1925955][-4.3264313 -4.3343611 -4.3351707 -4.3090053 -4.264873 -4.2223048 -4.2036891 -4.2106714 -4.2370243 -4.2804184 -4.3117661 -4.3186612 -4.3041158 -4.2722263 -4.2381272][-4.333621 -4.3428311 -4.3502536 -4.3366227 -4.3039389 -4.267117 -4.2473521 -4.2507787 -4.2693057 -4.3010979 -4.3253274 -4.3328056 -4.3228593 -4.299849 -4.2758183]]...]
INFO - root - 2017-12-07 20:47:39.365783: step 23210, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.681 sec/batch; 66h:48m:30s remains)
INFO - root - 2017-12-07 20:47:55.512968: step 23220, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 63h:33m:32s remains)
INFO - root - 2017-12-07 20:48:11.888778: step 23230, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.637 sec/batch; 65h:02m:31s remains)
INFO - root - 2017-12-07 20:48:28.117707: step 23240, loss = 2.08, batch loss = 2.03 (10.8 examples/sec; 1.480 sec/batch; 58h:46m:55s remains)
INFO - root - 2017-12-07 20:48:44.419051: step 23250, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.543 sec/batch; 61h:18m:36s remains)
INFO - root - 2017-12-07 20:49:00.740986: step 23260, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 1.752 sec/batch; 69h:35m:44s remains)
INFO - root - 2017-12-07 20:49:17.022294: step 23270, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.611 sec/batch; 63h:59m:38s remains)
INFO - root - 2017-12-07 20:49:33.249800: step 23280, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.648 sec/batch; 65h:25m:50s remains)
INFO - root - 2017-12-07 20:49:49.412906: step 23290, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.560 sec/batch; 61h:58m:08s remains)
INFO - root - 2017-12-07 20:50:05.223549: step 23300, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.572 sec/batch; 62h:25m:03s remains)
2017-12-07 20:50:06.599823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.195889 -4.1927533 -4.194664 -4.1960473 -4.181499 -4.1571178 -4.1384735 -4.1197577 -4.1081567 -4.0924115 -4.0859938 -4.1138225 -4.1548724 -4.18972 -4.2014151][-4.1788149 -4.1832614 -4.1927633 -4.201941 -4.1920214 -4.1703739 -4.1598883 -4.149816 -4.13827 -4.1275644 -4.1266227 -4.1511617 -4.1835475 -4.2076287 -4.2113342][-4.1600695 -4.1675787 -4.1878819 -4.208323 -4.2028122 -4.1837769 -4.1800222 -4.1740556 -4.1642818 -4.1666884 -4.176085 -4.195941 -4.2153883 -4.2266951 -4.2247696][-4.1601939 -4.1634307 -4.1872091 -4.2112656 -4.2052245 -4.1870728 -4.1836066 -4.1746225 -4.1694942 -4.1921773 -4.2149868 -4.2328691 -4.241437 -4.2446122 -4.2438464][-4.1802473 -4.1781316 -4.198041 -4.2169023 -4.20133 -4.1764474 -4.1667824 -4.1514168 -4.1536813 -4.1930985 -4.2287321 -4.2500043 -4.2559814 -4.2606306 -4.2670794][-4.1994796 -4.1935096 -4.2029433 -4.208508 -4.1798878 -4.1488304 -4.1372914 -4.1215892 -4.1282296 -4.1748786 -4.2211466 -4.2493076 -4.2594619 -4.2710423 -4.2849016][-4.2084618 -4.2009306 -4.2042031 -4.1952586 -4.1571374 -4.1283507 -4.1259441 -4.1223392 -4.1379466 -4.1855674 -4.2304358 -4.2563486 -4.2680554 -4.2809992 -4.29787][-4.2153864 -4.2077551 -4.2106552 -4.1913247 -4.149343 -4.1283121 -4.1379189 -4.14837 -4.1717753 -4.2139435 -4.2477007 -4.2659445 -4.2753992 -4.2835951 -4.2966895][-4.2202005 -4.2117996 -4.2130442 -4.1886854 -4.1493011 -4.1401572 -4.15994 -4.18086 -4.2070274 -4.2428169 -4.2675662 -4.2778931 -4.2799397 -4.2804766 -4.2846713][-4.2290368 -4.2239628 -4.2219648 -4.1986575 -4.1670947 -4.1687608 -4.1928358 -4.2174735 -4.2421913 -4.2727 -4.2899413 -4.2921166 -4.2827597 -4.2730541 -4.2712126][-4.2433381 -4.2407684 -4.2347336 -4.2174215 -4.2018795 -4.208673 -4.2310195 -4.2513967 -4.2689214 -4.2895241 -4.2980385 -4.2940149 -4.2779045 -4.2629933 -4.2594867][-4.249198 -4.2433438 -4.2394013 -4.235858 -4.2368493 -4.2478781 -4.2644439 -4.2773309 -4.2885847 -4.2987819 -4.2970643 -4.2866259 -4.2680578 -4.2529268 -4.2512126][-4.251442 -4.2448926 -4.2471752 -4.2530446 -4.2610965 -4.2705564 -4.2806034 -4.2839751 -4.286593 -4.2861462 -4.2786779 -4.2675409 -4.2534471 -4.2435594 -4.2466135][-4.2553096 -4.2463317 -4.250351 -4.2605367 -4.2710958 -4.2768636 -4.2806025 -4.2800474 -4.2779841 -4.2700005 -4.2610598 -4.2532072 -4.2453365 -4.2421689 -4.2498078][-4.2616854 -4.2523966 -4.255621 -4.2662697 -4.2760568 -4.2787609 -4.2801867 -4.2788777 -4.2744641 -4.2657466 -4.2572188 -4.2529006 -4.2521396 -4.2541218 -4.2626915]]...]
INFO - root - 2017-12-07 20:50:23.001257: step 23310, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.587 sec/batch; 63h:01m:53s remains)
INFO - root - 2017-12-07 20:50:39.398293: step 23320, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.682 sec/batch; 66h:47m:16s remains)
INFO - root - 2017-12-07 20:50:55.584569: step 23330, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.613 sec/batch; 64h:01m:32s remains)
INFO - root - 2017-12-07 20:51:11.957434: step 23340, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 1.762 sec/batch; 69h:56m:45s remains)
INFO - root - 2017-12-07 20:51:28.173419: step 23350, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.645 sec/batch; 65h:18m:13s remains)
INFO - root - 2017-12-07 20:51:44.363079: step 23360, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.642 sec/batch; 65h:09m:57s remains)
INFO - root - 2017-12-07 20:52:00.347220: step 23370, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.531 sec/batch; 60h:45m:39s remains)
INFO - root - 2017-12-07 20:52:16.678919: step 23380, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.637 sec/batch; 64h:59m:07s remains)
INFO - root - 2017-12-07 20:52:32.828059: step 23390, loss = 2.06, batch loss = 2.01 (10.2 examples/sec; 1.571 sec/batch; 62h:19m:21s remains)
INFO - root - 2017-12-07 20:52:49.011667: step 23400, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.613 sec/batch; 64h:00m:01s remains)
2017-12-07 20:52:50.413148: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2818217 -4.2806468 -4.2750936 -4.272552 -4.271255 -4.2657266 -4.2653451 -4.2746482 -4.2808352 -4.2822671 -4.2852621 -4.2861848 -4.2835536 -4.281033 -4.2829146][-4.2778406 -4.2783542 -4.276247 -4.2781181 -4.2825227 -4.2807722 -4.2803044 -4.285996 -4.2881274 -4.2863398 -4.288517 -4.2903357 -4.2865825 -4.2814159 -4.2828422][-4.259665 -4.2599111 -4.2610731 -4.2680697 -4.2773705 -4.2788677 -4.2775273 -4.2783594 -4.2754507 -4.2698627 -4.270844 -4.2730432 -4.26855 -4.262104 -4.2656689][-4.2174721 -4.2138081 -4.2159934 -4.2270813 -4.2436976 -4.2517285 -4.2527556 -4.2505155 -4.2412243 -4.23142 -4.2319136 -4.2337136 -4.2265105 -4.21992 -4.229475][-4.1480947 -4.1350327 -4.1330237 -4.1441936 -4.17088 -4.1894088 -4.1969295 -4.19668 -4.1849194 -4.1732821 -4.1740942 -4.1745982 -4.1648326 -4.1599808 -4.1782975][-4.0870619 -4.0567179 -4.0445819 -4.0559149 -4.098464 -4.1315746 -4.1496048 -4.1576262 -4.1489673 -4.1374497 -4.1363707 -4.1310844 -4.1153593 -4.1095428 -4.1321769][-4.073781 -4.0275078 -4.0046024 -4.0181079 -4.0781865 -4.1250706 -4.1485963 -4.1622405 -4.159513 -4.1518412 -4.1483083 -4.134984 -4.1107206 -4.099319 -4.1164522][-4.1130252 -4.0671349 -4.0427852 -4.0584011 -4.124928 -4.1753559 -4.1939344 -4.2028351 -4.2011557 -4.19425 -4.1870494 -4.1696606 -4.1418934 -4.1272149 -4.1360464][-4.1739664 -4.1396313 -4.1271729 -4.1455154 -4.2005854 -4.2380261 -4.2455792 -4.2457061 -4.2415185 -4.2342424 -4.2267742 -4.2110496 -4.1842647 -4.166666 -4.1672325][-4.2335734 -4.211051 -4.2099042 -4.229239 -4.2664833 -4.2886977 -4.2862973 -4.2802033 -4.2740068 -4.2659745 -4.2605505 -4.2493019 -4.2261138 -4.20783 -4.1998167][-4.2762613 -4.2628231 -4.2644343 -4.2780151 -4.3001251 -4.31033 -4.3027778 -4.2959223 -4.2907019 -4.2838411 -4.2813621 -4.2724633 -4.2499146 -4.2318811 -4.220386][-4.2924695 -4.2863026 -4.2883215 -4.2965789 -4.308341 -4.309948 -4.3007817 -4.2958302 -4.2935567 -4.2889953 -4.2885966 -4.2805052 -4.2563696 -4.2385197 -4.2275333][-4.2897325 -4.2885089 -4.2925887 -4.2990475 -4.3055167 -4.3041592 -4.298243 -4.2967663 -4.2967191 -4.2938437 -4.29333 -4.2842636 -4.2586026 -4.2402272 -4.2310009][-4.27508 -4.2755628 -4.2837796 -4.2925677 -4.2974777 -4.2968454 -4.2963915 -4.2986846 -4.3008142 -4.2993183 -4.2974992 -4.2870493 -4.2625351 -4.247498 -4.2436247][-4.2692018 -4.2699847 -4.2788429 -4.2891526 -4.29308 -4.2929668 -4.2972231 -4.3036757 -4.3077626 -4.3066459 -4.3022232 -4.2894368 -4.2675686 -4.2568464 -4.2588282]]...]
INFO - root - 2017-12-07 20:53:06.733006: step 23410, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.622 sec/batch; 64h:22m:28s remains)
INFO - root - 2017-12-07 20:53:22.905613: step 23420, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.639 sec/batch; 65h:00m:37s remains)
INFO - root - 2017-12-07 20:53:39.215446: step 23430, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.572 sec/batch; 62h:22m:15s remains)
INFO - root - 2017-12-07 20:53:55.484978: step 23440, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.698 sec/batch; 67h:20m:48s remains)
INFO - root - 2017-12-07 20:54:11.749028: step 23450, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.636 sec/batch; 64h:54m:27s remains)
INFO - root - 2017-12-07 20:54:27.960299: step 23460, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.660 sec/batch; 65h:50m:22s remains)
INFO - root - 2017-12-07 20:54:44.114629: step 23470, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.610 sec/batch; 63h:52m:10s remains)
INFO - root - 2017-12-07 20:55:00.391809: step 23480, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.657 sec/batch; 65h:42m:13s remains)
INFO - root - 2017-12-07 20:55:16.659016: step 23490, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.582 sec/batch; 62h:43m:52s remains)
INFO - root - 2017-12-07 20:55:33.138950: step 23500, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.633 sec/batch; 64h:45m:47s remains)
2017-12-07 20:55:34.593365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.228734 -4.2321467 -4.2335925 -4.2295089 -4.2219071 -4.2159019 -4.2191172 -4.22849 -4.2353506 -4.2323174 -4.2270288 -4.2198157 -4.2026939 -4.1786003 -4.1551256][-4.2395873 -4.2429194 -4.244628 -4.2378983 -4.2317529 -4.2310758 -4.2365575 -4.2452774 -4.2538619 -4.2561097 -4.2563553 -4.254858 -4.2449074 -4.2281289 -4.2095013][-4.2334123 -4.240252 -4.2449389 -4.2380056 -4.2327113 -4.2355933 -4.2407951 -4.2473183 -4.2572942 -4.2641978 -4.2692347 -4.2739863 -4.2737064 -4.2675834 -4.2568755][-4.2178526 -4.2329788 -4.2438726 -4.2381406 -4.2293735 -4.2270679 -4.2261486 -4.2298665 -4.2423396 -4.2558537 -4.26884 -4.2809973 -4.2892733 -4.2908812 -4.2850289][-4.2064528 -4.2257175 -4.2391882 -4.22924 -4.2109356 -4.1960459 -4.1854219 -4.1863465 -4.2043109 -4.228931 -4.2536616 -4.275485 -4.2923903 -4.3003311 -4.29688][-4.1878142 -4.205132 -4.2136464 -4.1942253 -4.1619792 -4.1325607 -4.1113687 -4.1081934 -4.1321182 -4.1726575 -4.2165837 -4.2546058 -4.281148 -4.2937803 -4.2912297][-4.1762505 -4.1844196 -4.1809769 -4.1505604 -4.1055379 -4.0607429 -4.0260544 -4.0125284 -4.03521 -4.0914593 -4.1580634 -4.2148581 -4.2531538 -4.272541 -4.2734985][-4.173646 -4.1702065 -4.1541448 -4.1177282 -4.0668583 -4.0107889 -3.9642446 -3.9375281 -3.9550934 -4.0228963 -4.1061125 -4.1756868 -4.22189 -4.2478776 -4.2543721][-4.1483407 -4.1402035 -4.1247869 -4.10016 -4.0675421 -4.0256052 -3.9861698 -3.9587307 -3.96796 -4.0253839 -4.1015415 -4.1678038 -4.2106152 -4.2352524 -4.2398033][-4.1337314 -4.1312246 -4.1277213 -4.1234331 -4.1155658 -4.1012831 -4.0822568 -4.0632653 -4.063345 -4.0929489 -4.142818 -4.1922069 -4.2238746 -4.2392836 -4.2368331][-4.1332736 -4.1390719 -4.146769 -4.156518 -4.1617441 -4.1614313 -4.1551142 -4.1424327 -4.1379795 -4.1504474 -4.1842551 -4.2245126 -4.2492576 -4.2551355 -4.2447634][-4.1453991 -4.1552067 -4.1677871 -4.1820354 -4.1876969 -4.1857667 -4.1793718 -4.1694279 -4.1638947 -4.1722641 -4.2041922 -4.2443633 -4.2686095 -4.2702684 -4.2541146][-4.175272 -4.1829476 -4.1947217 -4.2081656 -4.2098804 -4.2023277 -4.1936669 -4.1861906 -4.1818838 -4.188992 -4.2155051 -4.2513113 -4.2756391 -4.2813735 -4.26916][-4.2229872 -4.225543 -4.2349892 -4.2456145 -4.2432871 -4.2310524 -4.2194161 -4.2130709 -4.2096109 -4.2158237 -4.2354374 -4.2632422 -4.2850137 -4.2940874 -4.2862458][-4.27214 -4.2726154 -4.2794013 -4.2869306 -4.2841415 -4.2724648 -4.2602353 -4.2543354 -4.2533288 -4.2592373 -4.2728167 -4.2902331 -4.3036661 -4.3088865 -4.3033214]]...]
INFO - root - 2017-12-07 20:55:50.882415: step 23510, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.675 sec/batch; 66h:23m:43s remains)
INFO - root - 2017-12-07 20:56:06.837400: step 23520, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.573 sec/batch; 62h:20m:49s remains)
INFO - root - 2017-12-07 20:56:23.091743: step 23530, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 1.502 sec/batch; 59h:33m:53s remains)
INFO - root - 2017-12-07 20:56:39.433350: step 23540, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.690 sec/batch; 66h:59m:02s remains)
INFO - root - 2017-12-07 20:56:55.786102: step 23550, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.634 sec/batch; 64h:45m:54s remains)
INFO - root - 2017-12-07 20:57:12.044297: step 23560, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.690 sec/batch; 67h:00m:02s remains)
INFO - root - 2017-12-07 20:57:28.396590: step 23570, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.615 sec/batch; 64h:01m:18s remains)
INFO - root - 2017-12-07 20:57:44.368278: step 23580, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.565 sec/batch; 62h:02m:12s remains)
INFO - root - 2017-12-07 20:58:00.561129: step 23590, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.702 sec/batch; 67h:25m:42s remains)
INFO - root - 2017-12-07 20:58:16.861310: step 23600, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.606 sec/batch; 63h:37m:07s remains)
2017-12-07 20:58:18.234436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2703786 -4.2568655 -4.2523541 -4.2617512 -4.2735119 -4.2750621 -4.2681866 -4.254364 -4.2468548 -4.2536325 -4.273077 -4.2933354 -4.3076029 -4.3108077 -4.3053412][-4.2413783 -4.2258878 -4.219945 -4.2292552 -4.2475834 -4.2482791 -4.235323 -4.217144 -4.207819 -4.2182178 -4.2443018 -4.2696958 -4.2851081 -4.2875333 -4.2798324][-4.2216 -4.2064586 -4.1971407 -4.2020526 -4.2226419 -4.2240095 -4.2088761 -4.1912169 -4.1842027 -4.1978631 -4.2237158 -4.2458649 -4.2583518 -4.2600446 -4.2517428][-4.226088 -4.2126069 -4.1984024 -4.1915121 -4.2041554 -4.2027144 -4.1864619 -4.1677604 -4.164607 -4.1793995 -4.20504 -4.2241817 -4.2328439 -4.235106 -4.2286448][-4.2249551 -4.2126822 -4.1892204 -4.163691 -4.1612415 -4.1549749 -4.1375756 -4.123085 -4.1258035 -4.1440277 -4.168963 -4.1891756 -4.2004786 -4.2077975 -4.2068257][-4.2154131 -4.20495 -4.1770387 -4.1386285 -4.1170263 -4.0947328 -4.0648785 -4.0498843 -4.0594196 -4.0812783 -4.106812 -4.13313 -4.1553812 -4.1753149 -4.1844335][-4.1820264 -4.1698718 -4.13879 -4.0931339 -4.0504947 -4.0004525 -3.9502485 -3.9350045 -3.9589164 -3.9960589 -4.0354071 -4.075985 -4.110476 -4.1427388 -4.1627865][-4.1321545 -4.1140823 -4.0756197 -4.0168476 -3.9585965 -3.8860462 -3.8190017 -3.8192475 -3.8708441 -3.9299781 -3.9923196 -4.0440788 -4.084054 -4.1227317 -4.1491008][-4.1176605 -4.0939226 -4.0476794 -3.980752 -3.9152913 -3.8405602 -3.7814736 -3.8073094 -3.8796377 -3.9463587 -4.0092149 -4.055656 -4.0866995 -4.1195464 -4.1447248][-4.1540165 -4.1342354 -4.0958843 -4.0408044 -3.9833238 -3.9269609 -3.8882427 -3.9102705 -3.9655747 -4.0139289 -4.0587511 -4.0870857 -4.1029119 -4.12473 -4.14491][-4.2133093 -4.2094717 -4.1930361 -4.1553469 -4.1054649 -4.0542283 -4.0159745 -4.0183725 -4.0525227 -4.0851831 -4.1141872 -4.1283364 -4.1364894 -4.1517305 -4.1662636][-4.2633357 -4.2719011 -4.2679086 -4.2462678 -4.2047691 -4.1567535 -4.1160417 -4.1045766 -4.1252046 -4.1492181 -4.172483 -4.1846 -4.19374 -4.2038388 -4.2104011][-4.2920647 -4.3015518 -4.2984557 -4.286 -4.2564039 -4.2205534 -4.1845479 -4.1719084 -4.184432 -4.2056313 -4.2286448 -4.244576 -4.2543721 -4.2586007 -4.2605867][-4.3009949 -4.30749 -4.3047533 -4.2983265 -4.2809396 -4.2573967 -4.232317 -4.2252951 -4.235662 -4.2518787 -4.2706203 -4.2846313 -4.2911148 -4.2926307 -4.2937684][-4.2919326 -4.2917833 -4.2904954 -4.2901731 -4.2838278 -4.2732706 -4.2620358 -4.2646542 -4.273232 -4.2818027 -4.2914 -4.2993464 -4.3030725 -4.3051004 -4.3085051]]...]
INFO - root - 2017-12-07 20:58:34.473918: step 23610, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.553 sec/batch; 61h:31m:58s remains)
INFO - root - 2017-12-07 20:58:50.755530: step 23620, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.681 sec/batch; 66h:36m:52s remains)
INFO - root - 2017-12-07 20:59:07.093383: step 23630, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.595 sec/batch; 63h:10m:52s remains)
INFO - root - 2017-12-07 20:59:23.416249: step 23640, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.593 sec/batch; 63h:06m:57s remains)
INFO - root - 2017-12-07 20:59:39.663547: step 23650, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.699 sec/batch; 67h:18m:19s remains)
INFO - root - 2017-12-07 20:59:55.828677: step 23660, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.550 sec/batch; 61h:23m:44s remains)
INFO - root - 2017-12-07 21:00:12.211931: step 23670, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.714 sec/batch; 67h:53m:38s remains)
INFO - root - 2017-12-07 21:00:28.351568: step 23680, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.614 sec/batch; 63h:55m:15s remains)
INFO - root - 2017-12-07 21:00:44.711145: step 23690, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 1.750 sec/batch; 69h:18m:47s remains)
INFO - root - 2017-12-07 21:01:00.828447: step 23700, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.620 sec/batch; 64h:08m:57s remains)
2017-12-07 21:01:02.229237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3038545 -4.3037534 -4.3061976 -4.3078961 -4.3043342 -4.3022566 -4.29905 -4.2927003 -4.2886529 -4.2950792 -4.3056455 -4.317287 -4.3282809 -4.34014 -4.3485641][-4.2806921 -4.2780676 -4.2792315 -4.2781539 -4.271 -4.2675209 -4.2648487 -4.2584229 -4.2536063 -4.260798 -4.2762837 -4.293467 -4.308692 -4.3236418 -4.3333817][-4.2450876 -4.2407932 -4.2429285 -4.2403212 -4.22945 -4.2237892 -4.2201896 -4.2149434 -4.21365 -4.2224765 -4.2446728 -4.2708378 -4.2913275 -4.3084569 -4.3192372][-4.2091084 -4.2040138 -4.2067742 -4.2035904 -4.1901355 -4.1808095 -4.1742749 -4.1703844 -4.17488 -4.1883883 -4.2181787 -4.2532806 -4.277555 -4.2947736 -4.3050904][-4.1793647 -4.17476 -4.1788464 -4.1783924 -4.1623635 -4.1421976 -4.1246152 -4.1232662 -4.1389136 -4.1643224 -4.2044029 -4.2476206 -4.2746453 -4.2881308 -4.2938461][-4.144453 -4.1385269 -4.143281 -4.1491709 -4.1310916 -4.0901852 -4.04867 -4.0441532 -4.0825663 -4.1353688 -4.1899052 -4.242125 -4.274713 -4.286521 -4.2873197][-4.0695171 -4.0638185 -4.0718465 -4.0861058 -4.0681138 -4.0059004 -3.9235792 -3.8980711 -3.9690545 -4.0717921 -4.1548476 -4.2183003 -4.2593975 -4.2747874 -4.2755961][-3.980922 -3.9823928 -3.9962313 -4.0127621 -3.995249 -3.9166613 -3.7883949 -3.7202611 -3.8170507 -3.9781547 -4.1012607 -4.1814079 -4.2319279 -4.2551284 -4.2607851][-3.9759591 -3.9816508 -3.9918995 -4.0024896 -3.9886293 -3.9169459 -3.7843587 -3.6874986 -3.7610984 -3.9274969 -4.0659804 -4.1553473 -4.2103639 -4.2380233 -4.2489977][-4.0585022 -4.062603 -4.0669093 -4.0697041 -4.0614872 -4.017097 -3.9295948 -3.8488243 -3.8631482 -3.967375 -4.0800071 -4.159657 -4.2071147 -4.23145 -4.2449951][-4.1575594 -4.15823 -4.15765 -4.1552429 -4.1479287 -4.1237497 -4.0771384 -4.0241156 -4.0067477 -4.0546207 -4.1290879 -4.1881957 -4.22439 -4.2433171 -4.2563362][-4.242198 -4.2400241 -4.2359304 -4.2305689 -4.2225657 -4.2086096 -4.182426 -4.1475444 -4.1237125 -4.1420627 -4.1897211 -4.2336068 -4.2602324 -4.2727108 -4.2807846][-4.29726 -4.2946177 -4.2893753 -4.2834058 -4.2780347 -4.2710495 -4.2559743 -4.2338991 -4.2152457 -4.2229309 -4.2547154 -4.2881689 -4.3058434 -4.3096533 -4.3097873][-4.3201261 -4.3181806 -4.312768 -4.3074794 -4.3041449 -4.3002548 -4.2904415 -4.2767391 -4.2642937 -4.2676811 -4.2908263 -4.3171115 -4.3311267 -4.3329191 -4.3304429][-4.3239789 -4.3224306 -4.3172989 -4.3124294 -4.3097711 -4.3066726 -4.3003139 -4.2914858 -4.2835054 -4.2853851 -4.3016891 -4.3226733 -4.336988 -4.3415627 -4.3418007]]...]
INFO - root - 2017-12-07 21:01:18.426185: step 23710, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 1.641 sec/batch; 64h:57m:43s remains)
INFO - root - 2017-12-07 21:01:34.836931: step 23720, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.697 sec/batch; 67h:10m:39s remains)
INFO - root - 2017-12-07 21:01:51.089529: step 23730, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.573 sec/batch; 62h:16m:56s remains)
INFO - root - 2017-12-07 21:02:07.402479: step 23740, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.588 sec/batch; 62h:51m:00s remains)
INFO - root - 2017-12-07 21:02:23.655967: step 23750, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.632 sec/batch; 64h:35m:02s remains)
INFO - root - 2017-12-07 21:02:39.826645: step 23760, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.611 sec/batch; 63h:44m:41s remains)
INFO - root - 2017-12-07 21:02:55.977439: step 23770, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 1.668 sec/batch; 66h:00m:33s remains)
INFO - root - 2017-12-07 21:03:12.321212: step 23780, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.604 sec/batch; 63h:27m:36s remains)
INFO - root - 2017-12-07 21:03:28.638592: step 23790, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.637 sec/batch; 64h:45m:58s remains)
INFO - root - 2017-12-07 21:03:44.941255: step 23800, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.587 sec/batch; 62h:46m:56s remains)
2017-12-07 21:03:46.363075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2773743 -4.2595835 -4.2193656 -4.1747184 -4.1734848 -4.220736 -4.2687912 -4.2885714 -4.2782817 -4.2560821 -4.2372665 -4.2178473 -4.2016683 -4.1980062 -4.2080617][-4.257133 -4.2540627 -4.2333732 -4.19477 -4.1822395 -4.2195973 -4.2663956 -4.2875423 -4.2776809 -4.2552252 -4.2349744 -4.2160707 -4.2039084 -4.1986837 -4.2008243][-4.2518969 -4.2606759 -4.2602143 -4.2329221 -4.208034 -4.22038 -4.2547851 -4.2743349 -4.2696629 -4.2510672 -4.2319674 -4.215868 -4.2076035 -4.2019048 -4.19829][-4.2547083 -4.2661686 -4.2767529 -4.2608595 -4.2277317 -4.2114716 -4.2250919 -4.2445412 -4.2501159 -4.2402635 -4.2228413 -4.2040739 -4.193336 -4.1885982 -4.1853948][-4.2574482 -4.2661133 -4.2794981 -4.2690058 -4.2321258 -4.1924758 -4.1766858 -4.191844 -4.2140107 -4.2199478 -4.2071681 -4.1864986 -4.1721864 -4.1684842 -4.1721425][-4.2566018 -4.2619133 -4.2752666 -4.2665787 -4.228066 -4.1682806 -4.11433 -4.1133456 -4.1576195 -4.193481 -4.1992984 -4.1868811 -4.1762171 -4.17628 -4.1854463][-4.2562079 -4.2617908 -4.2722907 -4.2634645 -4.224905 -4.1523294 -4.0633445 -4.0303831 -4.0922112 -4.1672587 -4.2059765 -4.2128572 -4.2118545 -4.215364 -4.2252307][-4.2645507 -4.2719507 -4.2781129 -4.2680655 -4.2336955 -4.1642537 -4.0675378 -4.0117645 -4.0684695 -4.1631527 -4.2257276 -4.2501845 -4.2557473 -4.2586966 -4.2648149][-4.2719927 -4.2777395 -4.2813735 -4.272234 -4.2504144 -4.2049274 -4.1363869 -4.0876951 -4.1149516 -4.187079 -4.2457814 -4.2745185 -4.2824669 -4.2856264 -4.2899303][-4.2677178 -4.2694659 -4.2722993 -4.2682371 -4.2627149 -4.2488489 -4.2184482 -4.1859202 -4.1826019 -4.2091541 -4.2446179 -4.2731647 -4.2885885 -4.2947817 -4.2967038][-4.2472005 -4.2461815 -4.252028 -4.2555995 -4.2648697 -4.2753887 -4.2725115 -4.2520738 -4.2304263 -4.2172689 -4.22573 -4.2506394 -4.2751579 -4.2849836 -4.2823663][-4.2242208 -4.2226515 -4.2337151 -4.2459106 -4.2679539 -4.2943611 -4.3062215 -4.2931151 -4.2616258 -4.2231383 -4.2038035 -4.2146492 -4.242734 -4.257359 -4.2522349][-4.2159152 -4.2153153 -4.229146 -4.2452531 -4.272501 -4.3037133 -4.3208404 -4.3128767 -4.2828679 -4.2357483 -4.1959291 -4.1881447 -4.209897 -4.2285194 -4.2262769][-4.2337446 -4.23247 -4.2409968 -4.2518253 -4.2706409 -4.294384 -4.3111572 -4.311039 -4.2914581 -4.25199 -4.2092962 -4.1888952 -4.1988845 -4.2162638 -4.2208362][-4.2659235 -4.2629862 -4.2619429 -4.2592158 -4.2598209 -4.2666669 -4.2792411 -4.2877855 -4.2852926 -4.2645087 -4.2355075 -4.2141209 -4.2129197 -4.2247529 -4.2329149]]...]
INFO - root - 2017-12-07 21:04:02.568633: step 23810, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.599 sec/batch; 63h:15m:52s remains)
INFO - root - 2017-12-07 21:04:18.614201: step 23820, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.517 sec/batch; 60h:00m:30s remains)
INFO - root - 2017-12-07 21:04:34.793590: step 23830, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.654 sec/batch; 65h:26m:46s remains)
INFO - root - 2017-12-07 21:04:51.179859: step 23840, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.721 sec/batch; 68h:05m:08s remains)
INFO - root - 2017-12-07 21:05:07.389065: step 23850, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 63h:22m:11s remains)
INFO - root - 2017-12-07 21:05:23.648696: step 23860, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.622 sec/batch; 64h:08m:55s remains)
INFO - root - 2017-12-07 21:05:39.849780: step 23870, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.665 sec/batch; 65h:49m:51s remains)
INFO - root - 2017-12-07 21:05:55.875860: step 23880, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 62h:12m:03s remains)
INFO - root - 2017-12-07 21:06:12.284652: step 23890, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.587 sec/batch; 62h:44m:39s remains)
INFO - root - 2017-12-07 21:06:28.679729: step 23900, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.718 sec/batch; 67h:55m:17s remains)
2017-12-07 21:06:29.977818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0698228 -4.0585914 -4.0567937 -4.0690069 -4.0882063 -4.1075535 -4.1239986 -4.1301155 -4.1299257 -4.1366529 -4.1523414 -4.1692424 -4.1797214 -4.1817741 -4.177659][-4.1150618 -4.1100197 -4.1120543 -4.1230521 -4.1392293 -4.158154 -4.1786585 -4.1912141 -4.19648 -4.2043476 -4.2170711 -4.2283468 -4.2338648 -4.2343478 -4.2316008][-4.1451006 -4.1434145 -4.1459322 -4.154779 -4.167768 -4.1847758 -4.20546 -4.220592 -4.2305431 -4.2412777 -4.2540674 -4.2612767 -4.2612758 -4.2580004 -4.2538161][-4.1585016 -4.1656189 -4.1719155 -4.1780019 -4.18399 -4.1939497 -4.2091875 -4.2218819 -4.2320833 -4.2441688 -4.2584381 -4.2658052 -4.2636094 -4.2574186 -4.2510891][-4.1684546 -4.1861668 -4.1957273 -4.1977663 -4.1968136 -4.1986723 -4.204762 -4.2111721 -4.2180324 -4.2296095 -4.2457037 -4.2563353 -4.2557631 -4.2489905 -4.2418461][-4.1859074 -4.2081881 -4.2177649 -4.2145591 -4.2065244 -4.1986556 -4.1923537 -4.1888003 -4.1893892 -4.1986365 -4.2167263 -4.2338815 -4.2408519 -4.2379088 -4.2330346][-4.1883187 -4.2098851 -4.2196937 -4.2154856 -4.2020373 -4.1844058 -4.165328 -4.1518059 -4.1471663 -4.15453 -4.1746554 -4.1996813 -4.21802 -4.22377 -4.22511][-4.1645918 -4.1810713 -4.1920128 -4.1919084 -4.1784148 -4.1544404 -4.1252193 -4.103929 -4.099185 -4.109489 -4.1334925 -4.1669359 -4.1967335 -4.2119946 -4.2196059][-4.1345358 -4.1410522 -4.1508751 -4.1565366 -4.1473126 -4.1222138 -4.0887637 -4.065186 -4.0639582 -4.0800133 -4.1094222 -4.1494555 -4.186357 -4.2074733 -4.2178445][-4.1018476 -4.1002779 -4.1110845 -4.1234579 -4.1219764 -4.1041822 -4.0755215 -4.0558567 -4.0581946 -4.0775456 -4.1090975 -4.1494317 -4.1878014 -4.2104125 -4.2202082][-4.0591178 -4.0551877 -4.07375 -4.0975 -4.1115513 -4.1117449 -4.1002836 -4.0928674 -4.0989943 -4.1164093 -4.1410384 -4.172473 -4.2030158 -4.2210145 -4.22803][-4.0302882 -4.0280714 -4.0557451 -4.0908127 -4.1225896 -4.1424384 -4.149322 -4.1539435 -4.1630182 -4.1743841 -4.1864219 -4.20283 -4.2203221 -4.2312908 -4.2358847][-4.0329013 -4.0333757 -4.0660992 -4.1079855 -4.1484838 -4.1783485 -4.1936007 -4.202126 -4.2098103 -4.213758 -4.2144394 -4.2187243 -4.2259231 -4.2326374 -4.237525][-4.0746222 -4.0762239 -4.1050243 -4.1427069 -4.179213 -4.2065334 -4.2200322 -4.2269788 -4.2316527 -4.231266 -4.2267928 -4.224607 -4.2259059 -4.2303967 -4.2367625][-4.1497865 -4.1510606 -4.1688027 -4.1914229 -4.2136736 -4.2291508 -4.2347097 -4.2369986 -4.2396045 -4.2385826 -4.2333474 -4.2285 -4.2268739 -4.2300043 -4.2369981]]...]
INFO - root - 2017-12-07 21:06:46.064490: step 23910, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.589 sec/batch; 62h:49m:07s remains)
INFO - root - 2017-12-07 21:07:02.399313: step 23920, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 63h:09m:38s remains)
INFO - root - 2017-12-07 21:07:18.831089: step 23930, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.729 sec/batch; 68h:22m:14s remains)
INFO - root - 2017-12-07 21:07:34.835204: step 23940, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.612 sec/batch; 63h:44m:34s remains)
INFO - root - 2017-12-07 21:07:51.103470: step 23950, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.669 sec/batch; 65h:58m:13s remains)
INFO - root - 2017-12-07 21:08:07.227475: step 23960, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.559 sec/batch; 61h:37m:08s remains)
INFO - root - 2017-12-07 21:08:23.534157: step 23970, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.647 sec/batch; 65h:05m:51s remains)
INFO - root - 2017-12-07 21:08:39.816196: step 23980, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.519 sec/batch; 60h:01m:30s remains)
INFO - root - 2017-12-07 21:08:56.022086: step 23990, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.548 sec/batch; 61h:11m:07s remains)
INFO - root - 2017-12-07 21:09:12.123411: step 24000, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.670 sec/batch; 66h:00m:05s remains)
2017-12-07 21:09:13.609826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0849624 -4.1100426 -4.1577268 -4.19618 -4.2178693 -4.2214966 -4.2210288 -4.22966 -4.2473712 -4.2728906 -4.29556 -4.3133821 -4.3277431 -4.3378167 -4.3430429][-4.0834365 -4.10878 -4.1544733 -4.1902509 -4.2056618 -4.1996264 -4.189816 -4.1938338 -4.2101159 -4.23574 -4.2623858 -4.2886114 -4.3114457 -4.3260064 -4.3337121][-4.1029654 -4.121151 -4.1581893 -4.1860647 -4.1928988 -4.177454 -4.1563606 -4.1527967 -4.1664248 -4.18985 -4.2206707 -4.2590218 -4.2941995 -4.3144 -4.3242192][-4.1207738 -4.1330895 -4.1591539 -4.1757493 -4.1752658 -4.1532745 -4.1188745 -4.1003947 -4.1090684 -4.1381187 -4.1811461 -4.2363448 -4.2847013 -4.3096213 -4.3183427][-4.1248612 -4.1338291 -4.1502161 -4.1556754 -4.1504216 -4.1261992 -4.0795846 -4.0439949 -4.049283 -4.092051 -4.1545105 -4.2234783 -4.2798905 -4.3082342 -4.3150988][-4.1196642 -4.1202879 -4.128099 -4.1332369 -4.1300468 -4.1042562 -4.0485206 -4.003623 -4.0132074 -4.0689039 -4.1446676 -4.218977 -4.2793794 -4.30974 -4.3144031][-4.11239 -4.0995331 -4.1033359 -4.1186271 -4.1218085 -4.0986776 -4.0474491 -4.0064197 -4.020772 -4.0770788 -4.1496506 -4.2213912 -4.2805524 -4.3107328 -4.3137684][-4.1015544 -4.0895529 -4.1032324 -4.1270142 -4.1371412 -4.1237588 -4.0862122 -4.05267 -4.0627031 -4.1070738 -4.1664367 -4.230072 -4.28306 -4.3110671 -4.3134542][-4.0900917 -4.0881634 -4.1136045 -4.1464171 -4.1675096 -4.1699886 -4.14371 -4.112606 -4.111187 -4.1392469 -4.1863461 -4.2412519 -4.2867727 -4.3109627 -4.31324][-4.10308 -4.1006584 -4.1271768 -4.1649175 -4.1947923 -4.2107573 -4.1938233 -4.16218 -4.1514711 -4.1664047 -4.2026687 -4.2489157 -4.2875242 -4.3086438 -4.313663][-4.135251 -4.1261568 -4.1450353 -4.1807327 -4.2143788 -4.2372284 -4.2291555 -4.2039862 -4.1903558 -4.2003751 -4.2307544 -4.2669387 -4.2953243 -4.3131809 -4.3210506][-4.1712265 -4.1595159 -4.1705847 -4.1986408 -4.2272139 -4.248353 -4.2476315 -4.2330556 -4.2241678 -4.2341619 -4.2605782 -4.2880735 -4.3096528 -4.3247304 -4.3327975][-4.2084503 -4.203804 -4.2131953 -4.2338533 -4.2527075 -4.26313 -4.2606864 -4.2521663 -4.2479792 -4.258 -4.2785063 -4.2998905 -4.3176818 -4.3309393 -4.3390703][-4.2395635 -4.2431908 -4.2560859 -4.2726269 -4.2833047 -4.2859855 -4.2816367 -4.2763109 -4.27377 -4.2795463 -4.2927966 -4.3079338 -4.3223071 -4.3333058 -4.3400331][-4.2607288 -4.2691922 -4.2826605 -4.2942929 -4.3015051 -4.3048396 -4.3039312 -4.3035393 -4.3031511 -4.3061509 -4.31293 -4.3210583 -4.3304214 -4.3379283 -4.3424149]]...]
INFO - root - 2017-12-07 21:09:29.802749: step 24010, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.572 sec/batch; 62h:05m:58s remains)
INFO - root - 2017-12-07 21:09:46.139243: step 24020, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.602 sec/batch; 63h:17m:06s remains)
INFO - root - 2017-12-07 21:10:02.411652: step 24030, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.684 sec/batch; 66h:31m:17s remains)
INFO - root - 2017-12-07 21:10:18.701831: step 24040, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.596 sec/batch; 63h:02m:37s remains)
INFO - root - 2017-12-07 21:10:35.041671: step 24050, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 1.753 sec/batch; 69h:15m:43s remains)
INFO - root - 2017-12-07 21:10:51.020158: step 24060, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.589 sec/batch; 62h:46m:02s remains)
INFO - root - 2017-12-07 21:11:07.278637: step 24070, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.645 sec/batch; 64h:57m:19s remains)
INFO - root - 2017-12-07 21:11:23.483540: step 24080, loss = 2.08, batch loss = 2.03 (10.4 examples/sec; 1.535 sec/batch; 60h:36m:49s remains)
INFO - root - 2017-12-07 21:11:39.741724: step 24090, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.630 sec/batch; 64h:22m:36s remains)
INFO - root - 2017-12-07 21:11:55.902345: step 24100, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.590 sec/batch; 62h:47m:15s remains)
2017-12-07 21:11:57.410462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2726479 -4.2625809 -4.2730136 -4.2951832 -4.3212757 -4.3335433 -4.3145456 -4.2800126 -4.2550206 -4.25855 -4.27996 -4.3054137 -4.3244138 -4.3299665 -4.3165607][-4.2546778 -4.2449203 -4.2525282 -4.27184 -4.2973084 -4.310195 -4.2866731 -4.2426353 -4.2085152 -4.2196131 -4.2565413 -4.2942362 -4.3183393 -4.3253736 -4.3137355][-4.2525082 -4.2434239 -4.2466111 -4.2600217 -4.2786856 -4.2836185 -4.2451591 -4.1818829 -4.1456642 -4.1743717 -4.2345614 -4.2854266 -4.31426 -4.3247213 -4.3136854][-4.2685061 -4.2602606 -4.2593837 -4.2631927 -4.2686429 -4.2519436 -4.1809211 -4.092968 -4.0691557 -4.129096 -4.2134857 -4.2760816 -4.3098927 -4.3238192 -4.3162532][-4.2848983 -4.2782841 -4.2741718 -4.2688384 -4.2563634 -4.2039008 -4.0859051 -3.9711647 -3.9740174 -4.0728178 -4.1822567 -4.2569656 -4.2961874 -4.3142524 -4.3100128][-4.2959328 -4.2892847 -4.2828069 -4.2698231 -4.2339821 -4.13583 -3.9682515 -3.83168 -3.8758621 -4.0221181 -4.156426 -4.2409339 -4.2819538 -4.3029313 -4.3018742][-4.3027611 -4.2909646 -4.2770886 -4.2570467 -4.2040205 -4.0792646 -3.8867593 -3.7599895 -3.8466256 -4.0195947 -4.1618037 -4.2461214 -4.2844386 -4.307374 -4.3085718][-4.3005838 -4.2823596 -4.2624531 -4.2382073 -4.1825013 -4.057785 -3.8871789 -3.8109331 -3.9170351 -4.0752063 -4.1948624 -4.2629704 -4.2943549 -4.31113 -4.3078122][-4.2902374 -4.2674284 -4.2486935 -4.2299852 -4.1839185 -4.0785851 -3.9582262 -3.9381318 -4.0377097 -4.155364 -4.2335129 -4.2711945 -4.2874355 -4.2942805 -4.28849][-4.2895689 -4.266674 -4.2534809 -4.2426329 -4.209291 -4.127821 -4.0543933 -4.0705018 -4.1472068 -4.2191048 -4.2543592 -4.2622676 -4.2642832 -4.26541 -4.261766][-4.295845 -4.2797379 -4.2700434 -4.2642622 -4.2372861 -4.1763668 -4.1389689 -4.1668491 -4.2181697 -4.2514367 -4.2532482 -4.2436366 -4.2413292 -4.2449284 -4.2476606][-4.2920146 -4.2866225 -4.2816415 -4.2755837 -4.2544284 -4.2165585 -4.2060313 -4.23029 -4.2592263 -4.2643895 -4.245461 -4.2276578 -4.2260113 -4.235373 -4.2443762][-4.2980437 -4.30145 -4.2941566 -4.2815561 -4.2657995 -4.250556 -4.2572818 -4.27177 -4.2829938 -4.27404 -4.2496476 -4.232089 -4.23172 -4.242115 -4.2572927][-4.3139515 -4.3204422 -4.307497 -4.2871861 -4.2729745 -4.2726674 -4.2873964 -4.2962384 -4.297564 -4.2865682 -4.2649288 -4.2514076 -4.2545719 -4.2644567 -4.2840371][-4.3196063 -4.3231192 -4.30511 -4.2861905 -4.2797031 -4.2893362 -4.3083596 -4.3139482 -4.3107748 -4.3015471 -4.2820411 -4.2729239 -4.2804413 -4.2884545 -4.3020964]]...]
INFO - root - 2017-12-07 21:12:13.417827: step 24110, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.581 sec/batch; 62h:25m:46s remains)
INFO - root - 2017-12-07 21:12:29.833165: step 24120, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.570 sec/batch; 61h:59m:30s remains)
INFO - root - 2017-12-07 21:12:46.396531: step 24130, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 1.732 sec/batch; 68h:21m:27s remains)
INFO - root - 2017-12-07 21:13:02.524678: step 24140, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.690 sec/batch; 66h:43m:37s remains)
INFO - root - 2017-12-07 21:13:18.855351: step 24150, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.581 sec/batch; 62h:23m:49s remains)
INFO - root - 2017-12-07 21:13:35.246042: step 24160, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 1.648 sec/batch; 65h:03m:15s remains)
INFO - root - 2017-12-07 21:13:51.493689: step 24170, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.586 sec/batch; 62h:35m:25s remains)
INFO - root - 2017-12-07 21:14:07.682714: step 24180, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.712 sec/batch; 67h:33m:53s remains)
INFO - root - 2017-12-07 21:14:23.979886: step 24190, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.658 sec/batch; 65h:25m:14s remains)
INFO - root - 2017-12-07 21:14:40.337338: step 24200, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.636 sec/batch; 64h:32m:59s remains)
2017-12-07 21:14:41.672677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.275733 -4.2991033 -4.3193908 -4.3219228 -4.2902446 -4.2198448 -4.1291986 -4.0698628 -4.07934 -4.1520958 -4.237184 -4.2913294 -4.3017979 -4.2604151 -4.2000623][-4.2560458 -4.2817159 -4.3088984 -4.3173313 -4.2920637 -4.2251587 -4.1337805 -4.0744724 -4.0846977 -4.1548977 -4.2393122 -4.2942953 -4.3074069 -4.2695346 -4.2088771][-4.2423258 -4.2698092 -4.2997565 -4.309701 -4.2845292 -4.2198472 -4.138957 -4.0941448 -4.1135383 -4.1781192 -4.2504277 -4.2961316 -4.30932 -4.2793374 -4.2275281][-4.2452216 -4.2706609 -4.2956338 -4.2969255 -4.2641845 -4.200007 -4.1245956 -4.0867047 -4.1166711 -4.1844363 -4.2529755 -4.2978444 -4.3168044 -4.3016257 -4.2612052][-4.2522116 -4.2748904 -4.2869673 -4.2702646 -4.2263322 -4.1580215 -4.0742369 -4.037199 -4.0867267 -4.1669011 -4.2411394 -4.2907143 -4.3204675 -4.3228283 -4.2913814][-4.2764764 -4.2871294 -4.2761812 -4.2303047 -4.1649413 -4.0793028 -3.9711106 -3.9378891 -4.0198922 -4.1167636 -4.2032089 -4.2628765 -4.3048983 -4.3181777 -4.2897639][-4.3204322 -4.314033 -4.2779021 -4.201509 -4.1091805 -3.9992087 -3.868752 -3.8459778 -3.9580696 -4.0758176 -4.1787128 -4.2508316 -4.2982187 -4.3104739 -4.2766609][-4.356595 -4.342876 -4.2971468 -4.2164531 -4.1270318 -4.03385 -3.9415362 -3.9376888 -4.0279474 -4.1221576 -4.2087584 -4.2682719 -4.3040366 -4.3089552 -4.2707496][-4.3722873 -4.3620181 -4.32007 -4.2490005 -4.1758885 -4.110116 -4.0596671 -4.0649948 -4.1217308 -4.1820626 -4.242054 -4.2836432 -4.3100386 -4.3136148 -4.2786736][-4.3672938 -4.3633857 -4.3266811 -4.2629037 -4.1983895 -4.144752 -4.1153693 -4.1266656 -4.1702294 -4.214931 -4.261692 -4.2963014 -4.3180633 -4.3190393 -4.2856393][-4.3500309 -4.3501658 -4.3186059 -4.2601509 -4.1982527 -4.1493564 -4.1300249 -4.1419892 -4.1768508 -4.2094159 -4.2490754 -4.2851477 -4.3047452 -4.3036408 -4.2756085][-4.3336329 -4.3369155 -4.3116007 -4.2583294 -4.1983781 -4.1501141 -4.1307182 -4.1409345 -4.1645846 -4.1864352 -4.2208133 -4.2565737 -4.276567 -4.2801633 -4.2637796][-4.3301048 -4.3289924 -4.3052731 -4.255517 -4.1998754 -4.1558743 -4.139545 -4.149816 -4.171247 -4.1880417 -4.2158933 -4.247539 -4.2701683 -4.2822833 -4.2805715][-4.3309746 -4.3240323 -4.2987814 -4.252811 -4.2058206 -4.171196 -4.1598516 -4.1743183 -4.1982808 -4.2157245 -4.2395964 -4.26513 -4.289 -4.305954 -4.3142352][-4.3202581 -4.3108935 -4.2889462 -4.2505865 -4.2089233 -4.1794529 -4.1725512 -4.1944189 -4.2254143 -4.2500329 -4.2730803 -4.292417 -4.313498 -4.33109 -4.34182]]...]
INFO - root - 2017-12-07 21:14:57.993489: step 24210, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.717 sec/batch; 67h:45m:50s remains)
INFO - root - 2017-12-07 21:15:14.197428: step 24220, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.597 sec/batch; 63h:00m:26s remains)
INFO - root - 2017-12-07 21:15:30.371069: step 24230, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.668 sec/batch; 65h:48m:37s remains)
INFO - root - 2017-12-07 21:15:46.564156: step 24240, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.584 sec/batch; 62h:30m:14s remains)
INFO - root - 2017-12-07 21:16:02.935489: step 24250, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.559 sec/batch; 61h:30m:27s remains)
INFO - root - 2017-12-07 21:16:19.387414: step 24260, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.697 sec/batch; 66h:56m:12s remains)
INFO - root - 2017-12-07 21:16:35.581707: step 24270, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.614 sec/batch; 63h:38m:25s remains)
INFO - root - 2017-12-07 21:16:51.954831: step 24280, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.648 sec/batch; 64h:59m:30s remains)
INFO - root - 2017-12-07 21:17:08.082402: step 24290, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.586 sec/batch; 62h:32m:17s remains)
INFO - root - 2017-12-07 21:17:24.543908: step 24300, loss = 2.10, batch loss = 2.05 (9.3 examples/sec; 1.727 sec/batch; 68h:04m:52s remains)
2017-12-07 21:17:25.940126: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2677255 -4.2650104 -4.2600069 -4.2503624 -4.2424922 -4.2373533 -4.2454429 -4.2564473 -4.2669773 -4.2673593 -4.266377 -4.2690558 -4.2619429 -4.2355108 -4.2129984][-4.2688303 -4.2600183 -4.2517571 -4.2345767 -4.216629 -4.2055736 -4.2158189 -4.2320666 -4.2455249 -4.2461071 -4.2447162 -4.2479677 -4.2423725 -4.2191467 -4.201333][-4.2666912 -4.2507334 -4.2384448 -4.2135191 -4.1802125 -4.1564527 -4.1673808 -4.1969995 -4.2205725 -4.2234726 -4.2211246 -4.22372 -4.2199154 -4.2027359 -4.18851][-4.2658911 -4.244319 -4.2256927 -4.1929717 -4.1439843 -4.1006274 -4.1081161 -4.1547732 -4.1932621 -4.2010593 -4.1987448 -4.2015743 -4.2011766 -4.1875191 -4.1706285][-4.263237 -4.2439156 -4.2197533 -4.1765308 -4.109621 -4.0448332 -4.0444822 -4.1033716 -4.15483 -4.1710653 -4.1710157 -4.1775331 -4.1823387 -4.1702166 -4.1482987][-4.2639618 -4.2475681 -4.2191243 -4.1695113 -4.0902538 -4.006433 -3.9953671 -4.0602107 -4.1183405 -4.1389914 -4.1402783 -4.1499949 -4.157094 -4.1409407 -4.1114664][-4.2557287 -4.2432323 -4.2098246 -4.1529732 -4.0597548 -3.955152 -3.9239888 -3.9833519 -4.0418572 -4.0681868 -4.0800056 -4.1000462 -4.10982 -4.0919232 -4.0607677][-4.2324905 -4.2318382 -4.2009077 -4.1419139 -4.047133 -3.9331758 -3.879951 -3.9218991 -3.9755659 -4.0018148 -4.02012 -4.0458531 -4.0578108 -4.0423656 -4.019546][-4.2252584 -4.2374582 -4.2187896 -4.1779408 -4.1113057 -4.0217175 -3.9706795 -3.9943421 -4.0324807 -4.0455046 -4.0569005 -4.0751982 -4.0847392 -4.0726075 -4.0565391][-4.2495427 -4.2616596 -4.2472286 -4.2196832 -4.1801252 -4.12381 -4.0870266 -4.0999222 -4.126121 -4.131506 -4.1356783 -4.1449051 -4.1515613 -4.1462636 -4.1342478][-4.2833261 -4.2879524 -4.2757611 -4.2574272 -4.2363667 -4.2073765 -4.184556 -4.1929703 -4.2101641 -4.2122984 -4.2125025 -4.2168245 -4.2223215 -4.22072 -4.21122][-4.2804961 -4.2779875 -4.2727642 -4.2700157 -4.2689562 -4.2605476 -4.2510562 -4.2646174 -4.2831378 -4.2880754 -4.2898326 -4.2946 -4.2998171 -4.2989349 -4.2915425][-4.2334847 -4.2315726 -4.238266 -4.2447248 -4.2545028 -4.262434 -4.268074 -4.2906847 -4.3142371 -4.3260765 -4.3333654 -4.3386965 -4.3423772 -4.3408017 -4.334486][-4.1675911 -4.1711226 -4.1882124 -4.2030969 -4.2207842 -4.2406521 -4.258019 -4.2864175 -4.313201 -4.3298907 -4.3408718 -4.3460145 -4.3487511 -4.3481169 -4.3447084][-4.10711 -4.1167269 -4.1416993 -4.1621923 -4.1855116 -4.2117119 -4.2366886 -4.2686095 -4.2985067 -4.3185821 -4.3313918 -4.3367033 -4.3384309 -4.3391514 -4.3379879]]...]
INFO - root - 2017-12-07 21:17:42.380862: step 24310, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 1.667 sec/batch; 65h:43m:34s remains)
INFO - root - 2017-12-07 21:17:58.544477: step 24320, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.610 sec/batch; 63h:28m:44s remains)
INFO - root - 2017-12-07 21:18:14.890589: step 24330, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.631 sec/batch; 64h:17m:06s remains)
INFO - root - 2017-12-07 21:18:31.121322: step 24340, loss = 2.10, batch loss = 2.04 (10.3 examples/sec; 1.550 sec/batch; 61h:06m:30s remains)
INFO - root - 2017-12-07 21:18:47.328386: step 24350, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 1.516 sec/batch; 59h:44m:55s remains)
INFO - root - 2017-12-07 21:19:03.552140: step 24360, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.678 sec/batch; 66h:08m:15s remains)
INFO - root - 2017-12-07 21:19:19.878914: step 24370, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 1.651 sec/batch; 65h:03m:36s remains)
INFO - root - 2017-12-07 21:19:36.117840: step 24380, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.656 sec/batch; 65h:16m:35s remains)
INFO - root - 2017-12-07 21:19:52.165055: step 24390, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.592 sec/batch; 62h:44m:33s remains)
INFO - root - 2017-12-07 21:20:08.422138: step 24400, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 1.519 sec/batch; 59h:50m:17s remains)
2017-12-07 21:20:09.847311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2071886 -4.2037959 -4.2184339 -4.2431068 -4.2619991 -4.2601566 -4.2320747 -4.20905 -4.2214975 -4.2445254 -4.2611752 -4.2639709 -4.2588172 -4.2589512 -4.272593][-4.2147274 -4.2083073 -4.2237248 -4.249928 -4.268178 -4.26337 -4.2319374 -4.2063475 -4.2182417 -4.241889 -4.2588792 -4.2614651 -4.2568855 -4.2575722 -4.2721434][-4.2191949 -4.2133818 -4.2317705 -4.2588272 -4.2755332 -4.2674932 -4.2330222 -4.2051625 -4.2164435 -4.240459 -4.2580156 -4.2608395 -4.2562923 -4.2570629 -4.2717113][-4.220458 -4.217227 -4.2390857 -4.2673221 -4.2826314 -4.2718878 -4.2349715 -4.2051039 -4.2156625 -4.2399321 -4.257936 -4.2610049 -4.2562752 -4.2568722 -4.2714558][-4.2191176 -4.2185731 -4.2421231 -4.2708516 -4.2846289 -4.2721453 -4.2343664 -4.2033968 -4.213799 -4.2384877 -4.2571144 -4.2608824 -4.2564635 -4.2571363 -4.2714496][-4.21428 -4.214859 -4.2367425 -4.2627192 -4.2731714 -4.2596955 -4.2217174 -4.1899939 -4.2016172 -4.2276797 -4.2485147 -4.2554455 -4.2538195 -4.2562141 -4.270957][-4.2063756 -4.2073326 -4.2241821 -4.2435265 -4.2491889 -4.2349238 -4.1974907 -4.1653743 -4.1787472 -4.2062397 -4.2308292 -4.2437115 -4.2472162 -4.2530069 -4.2692971][-4.2115 -4.2116766 -4.2214274 -4.2321539 -4.2309413 -4.2134857 -4.1739006 -4.1384921 -4.1517859 -4.1802654 -4.2080779 -4.227809 -4.2371254 -4.2470055 -4.2660918][-4.2415366 -4.2415495 -4.2459249 -4.2491884 -4.2412415 -4.2191119 -4.1763186 -4.1374917 -4.149271 -4.1760354 -4.2031245 -4.2250476 -4.2356148 -4.2457185 -4.2647495][-4.2675829 -4.2673388 -4.2681527 -4.2672281 -4.2554779 -4.2317829 -4.1894007 -4.1516805 -4.1651769 -4.1906457 -4.2148838 -4.2357497 -4.2442031 -4.2515903 -4.2670875][-4.2799826 -4.2797179 -4.2792745 -4.277319 -4.265779 -4.2443156 -4.2046223 -4.169765 -4.1842217 -4.2071948 -4.2282734 -4.2477393 -4.253829 -4.2575583 -4.2689924][-4.2877178 -4.2873244 -4.2868576 -4.2854714 -4.2753453 -4.2569265 -4.2214355 -4.1898127 -4.2046132 -4.2245278 -4.2419968 -4.2592726 -4.2615933 -4.2611957 -4.2694635][-4.2861137 -4.2847581 -4.2841153 -4.2831388 -4.2741585 -4.2591233 -4.2287717 -4.201746 -4.2177439 -4.2356324 -4.2496576 -4.2636814 -4.2623291 -4.2590847 -4.2659326][-4.2886567 -4.2869234 -4.2858477 -4.2850175 -4.2772722 -4.2660823 -4.2422395 -4.2219148 -4.239615 -4.2553411 -4.2644053 -4.2726135 -4.2660241 -4.2584906 -4.2621131][-4.2965736 -4.2952738 -4.2944937 -4.2940474 -4.2870269 -4.2786145 -4.260675 -4.2462964 -4.2651091 -4.2779279 -4.2816553 -4.2842331 -4.27178 -4.2589464 -4.2588696]]...]
INFO - root - 2017-12-07 21:20:26.131152: step 24410, loss = 2.04, batch loss = 1.98 (10.0 examples/sec; 1.595 sec/batch; 62h:51m:44s remains)
INFO - root - 2017-12-07 21:20:42.280632: step 24420, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 1.680 sec/batch; 66h:11m:20s remains)
INFO - root - 2017-12-07 21:20:58.495687: step 24430, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.562 sec/batch; 61h:33m:10s remains)
INFO - root - 2017-12-07 21:21:14.971188: step 24440, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.658 sec/batch; 65h:18m:33s remains)
INFO - root - 2017-12-07 21:21:31.181835: step 24450, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.605 sec/batch; 63h:14m:07s remains)
INFO - root - 2017-12-07 21:21:47.318775: step 24460, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.679 sec/batch; 66h:07m:58s remains)
INFO - root - 2017-12-07 21:22:03.486731: step 24470, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 1.489 sec/batch; 58h:38m:52s remains)
INFO - root - 2017-12-07 21:22:19.818005: step 24480, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.674 sec/batch; 65h:54m:15s remains)
INFO - root - 2017-12-07 21:22:35.908948: step 24490, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.585 sec/batch; 62h:24m:43s remains)
INFO - root - 2017-12-07 21:22:52.261597: step 24500, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 1.554 sec/batch; 61h:10m:18s remains)
2017-12-07 21:22:53.662452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2725286 -4.2798319 -4.2959085 -4.3030257 -4.3061647 -4.302084 -4.2887316 -4.2659569 -4.2410679 -4.23252 -4.2479386 -4.2755251 -4.3040247 -4.3209224 -4.3259406][-4.2602682 -4.2689395 -4.2853913 -4.289742 -4.288806 -4.28226 -4.2661014 -4.2381105 -4.2093024 -4.2024384 -4.2204313 -4.251462 -4.2857203 -4.30967 -4.3193779][-4.242414 -4.2513938 -4.2670164 -4.2697282 -4.2642159 -4.2534733 -4.2315769 -4.1983271 -4.1677117 -4.1658964 -4.190793 -4.2280469 -4.2679615 -4.2975731 -4.311944][-4.2140756 -4.2237058 -4.2370014 -4.2400846 -4.2358761 -4.2228432 -4.1960411 -4.1568136 -4.1248093 -4.1269345 -4.1562414 -4.1998916 -4.2472482 -4.2828078 -4.3038116][-4.1797361 -4.187737 -4.2006092 -4.2036843 -4.1991024 -4.18421 -4.1519456 -4.1057911 -4.0732741 -4.0796967 -4.1110005 -4.1628861 -4.2243447 -4.2683296 -4.2964373][-4.1418729 -4.149683 -4.1610956 -4.1607137 -4.150579 -4.1268778 -4.0867577 -4.0332756 -3.99976 -4.0117321 -4.0510564 -4.1163845 -4.1952648 -4.2505174 -4.2871923][-4.1084089 -4.1164637 -4.1250563 -4.121232 -4.099606 -4.0597143 -4.0121326 -3.9545329 -3.92102 -3.9352796 -3.9848742 -4.0658946 -4.1606092 -4.226439 -4.2729712][-4.1068287 -4.1134214 -4.118783 -4.1115589 -4.0840974 -4.0383129 -3.9987113 -3.9532287 -3.9248774 -3.9360185 -3.9826543 -4.0626435 -4.1528196 -4.2174587 -4.2662582][-4.1541796 -4.1550145 -4.1523857 -4.1404757 -4.1183758 -4.0924749 -4.0778131 -4.0531249 -4.0301514 -4.0312266 -4.0618677 -4.1241527 -4.1929121 -4.2412262 -4.2792816][-4.2167473 -4.2126856 -4.2015772 -4.1886587 -4.1791244 -4.1743641 -4.1811104 -4.1702089 -4.1523175 -4.1475224 -4.1618214 -4.2039027 -4.25025 -4.2792749 -4.3017159][-4.2818208 -4.2726989 -4.2546687 -4.243084 -4.240715 -4.2490258 -4.2615862 -4.2526045 -4.2381158 -4.2371931 -4.2469416 -4.2721543 -4.3000607 -4.3125582 -4.3217993][-4.3332024 -4.3213344 -4.3039169 -4.2985291 -4.30084 -4.3112407 -4.3201222 -4.3095651 -4.2956181 -4.2949266 -4.3040948 -4.3215809 -4.3363962 -4.3379045 -4.3376522][-4.3447 -4.3379941 -4.3264418 -4.3271594 -4.3318281 -4.3399725 -4.3447537 -4.3322124 -4.319438 -4.3189931 -4.3279314 -4.3393822 -4.34747 -4.3461747 -4.3427181][-4.3228073 -4.3201461 -4.3146882 -4.315424 -4.3162093 -4.3193264 -4.3203735 -4.3088894 -4.2972107 -4.2961454 -4.3041615 -4.3173485 -4.3307824 -4.335835 -4.3367987][-4.2917194 -4.2902541 -4.2890615 -4.2897234 -4.2903371 -4.2928081 -4.2923656 -4.2834992 -4.2730889 -4.2708836 -4.2783923 -4.292501 -4.3094182 -4.320756 -4.32762]]...]
INFO - root - 2017-12-07 21:23:10.079027: step 24510, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.647 sec/batch; 64h:49m:48s remains)
INFO - root - 2017-12-07 21:23:26.226815: step 24520, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 1.699 sec/batch; 66h:53m:06s remains)
INFO - root - 2017-12-07 21:23:42.188461: step 24530, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 1.587 sec/batch; 62h:28m:09s remains)
INFO - root - 2017-12-07 21:23:58.608141: step 24540, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.678 sec/batch; 66h:03m:55s remains)
INFO - root - 2017-12-07 21:24:14.925784: step 24550, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.643 sec/batch; 64h:40m:20s remains)
INFO - root - 2017-12-07 21:24:31.143147: step 24560, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.653 sec/batch; 65h:04m:00s remains)
INFO - root - 2017-12-07 21:24:47.352044: step 24570, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.605 sec/batch; 63h:10m:37s remains)
INFO - root - 2017-12-07 21:25:03.279774: step 24580, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.599 sec/batch; 62h:55m:31s remains)
INFO - root - 2017-12-07 21:25:19.438152: step 24590, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.640 sec/batch; 64h:31m:06s remains)
INFO - root - 2017-12-07 21:25:35.674596: step 24600, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.601 sec/batch; 62h:58m:54s remains)
2017-12-07 21:25:37.016557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3300738 -4.3410487 -4.3487644 -4.3503065 -4.3469119 -4.3429232 -4.3405757 -4.3409243 -4.3445935 -4.3493843 -4.3523536 -4.35235 -4.3509789 -4.3506722 -4.3520794][-4.3114529 -4.3313394 -4.3421774 -4.3422055 -4.3352294 -4.3275871 -4.3224244 -4.3229971 -4.3298197 -4.3395872 -4.3473878 -4.3515 -4.352581 -4.3523116 -4.3524742][-4.2822471 -4.3067813 -4.3164244 -4.311039 -4.2965407 -4.2807379 -4.2705932 -4.2709155 -4.2832818 -4.3029795 -4.3222895 -4.3356652 -4.3430119 -4.3453712 -4.3456321][-4.2565293 -4.276372 -4.2780557 -4.2611613 -4.23229 -4.201663 -4.1809154 -4.1807275 -4.2041254 -4.2394032 -4.2749872 -4.3023806 -4.31922 -4.3273325 -4.3299041][-4.2441788 -4.2501564 -4.2388344 -4.2065506 -4.1573787 -4.1015053 -4.0588455 -4.0539932 -4.0925808 -4.1497154 -4.2052236 -4.2492485 -4.2786961 -4.2951431 -4.3026609][-4.2388749 -4.2327037 -4.2135563 -4.1736207 -4.11279 -4.037694 -3.9697757 -3.9483278 -3.9929276 -4.0616932 -4.1265736 -4.1819277 -4.223855 -4.2521949 -4.2684441][-4.2233458 -4.2124825 -4.1952543 -4.1627965 -4.1134453 -4.0499496 -3.9860237 -3.9526675 -3.9739413 -4.0201645 -4.0702477 -4.1231356 -4.1721282 -4.2119465 -4.2385969][-4.1953635 -4.1941018 -4.1916914 -4.1793947 -4.1576505 -4.1266589 -4.0906444 -4.0630603 -4.055253 -4.0596313 -4.0760369 -4.1104536 -4.15517 -4.198772 -4.2314758][-4.16255 -4.1829252 -4.2040029 -4.2149487 -4.2163291 -4.2081132 -4.1931505 -4.1742983 -4.1551704 -4.13808 -4.1322107 -4.1474876 -4.17992 -4.216701 -4.245687][-4.1388092 -4.1814642 -4.2232137 -4.2512083 -4.2662163 -4.2693667 -4.26424 -4.2513428 -4.2329731 -4.2136292 -4.201293 -4.2053356 -4.2243137 -4.2475405 -4.265265][-4.147202 -4.1972632 -4.2436085 -4.2739053 -4.2890463 -4.2932386 -4.2921815 -4.2845969 -4.2732062 -4.2596035 -4.246768 -4.2430692 -4.249763 -4.2595873 -4.2662249][-4.180759 -4.2215962 -4.2591066 -4.2820816 -4.289413 -4.2908354 -4.2932215 -4.2929544 -4.2889957 -4.2789741 -4.2656221 -4.2551513 -4.2499895 -4.2471271 -4.243825][-4.2252016 -4.2520013 -4.2752652 -4.2858553 -4.2833238 -4.2810197 -4.2869134 -4.294951 -4.2991543 -4.2934957 -4.2791967 -4.2617168 -4.2461834 -4.2335148 -4.2233334][-4.2629824 -4.2782631 -4.2911186 -4.2931929 -4.2857585 -4.2816682 -4.2893472 -4.3027959 -4.3122969 -4.3114004 -4.3005137 -4.2838073 -4.2671881 -4.2527952 -4.2401795][-4.2800155 -4.285625 -4.2906213 -4.2898655 -4.2847791 -4.2839274 -4.2935476 -4.3091578 -4.3216009 -4.3257589 -4.3209295 -4.3101234 -4.2989435 -4.2895346 -4.2799544]]...]
INFO - root - 2017-12-07 21:25:53.256918: step 24610, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.634 sec/batch; 64h:16m:25s remains)
INFO - root - 2017-12-07 21:26:09.672099: step 24620, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.678 sec/batch; 66h:00m:25s remains)
INFO - root - 2017-12-07 21:26:26.041055: step 24630, loss = 2.06, batch loss = 2.01 (10.2 examples/sec; 1.574 sec/batch; 61h:55m:29s remains)
INFO - root - 2017-12-07 21:26:42.262552: step 24640, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 1.722 sec/batch; 67h:45m:11s remains)
INFO - root - 2017-12-07 21:26:58.357156: step 24650, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.557 sec/batch; 61h:15m:19s remains)
INFO - root - 2017-12-07 21:27:14.847850: step 24660, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 1.754 sec/batch; 68h:58m:42s remains)
INFO - root - 2017-12-07 21:27:31.056811: step 24670, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.602 sec/batch; 63h:00m:11s remains)
INFO - root - 2017-12-07 21:27:47.338397: step 24680, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.629 sec/batch; 64h:02m:55s remains)
INFO - root - 2017-12-07 21:28:03.308768: step 24690, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 1.480 sec/batch; 58h:12m:48s remains)
INFO - root - 2017-12-07 21:28:19.496040: step 24700, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.615 sec/batch; 63h:30m:37s remains)
2017-12-07 21:28:20.944390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3198318 -4.3206525 -4.3228469 -4.3258448 -4.3280287 -4.3292494 -4.3287988 -4.32746 -4.3262262 -4.3260136 -4.3257461 -4.3242497 -4.3225379 -4.3210454 -4.3188438][-4.3052683 -4.3064256 -4.3112373 -4.3171048 -4.3208122 -4.3211408 -4.3194032 -4.3181853 -4.3179517 -4.3185496 -4.3197246 -4.3191094 -4.3167458 -4.313787 -4.3096633][-4.2819772 -4.2839684 -4.2905431 -4.2974124 -4.3003693 -4.2970195 -4.2901578 -4.28673 -4.288064 -4.2915435 -4.2967033 -4.299366 -4.2986159 -4.2956052 -4.290669][-4.2529941 -4.25668 -4.26397 -4.2700315 -4.2699189 -4.2591171 -4.2429023 -4.2345395 -4.2370572 -4.2439237 -4.2549911 -4.2645941 -4.2688103 -4.2683315 -4.2645369][-4.2311454 -4.2329116 -4.236414 -4.2372518 -4.229991 -4.2084689 -4.1796188 -4.1632409 -4.1664467 -4.1786966 -4.1979547 -4.2174234 -4.2305684 -4.2352009 -4.2332411][-4.2324405 -4.2282033 -4.2209215 -4.2064838 -4.1815524 -4.1442623 -4.09975 -4.0739307 -4.0790782 -4.103683 -4.1389618 -4.17395 -4.199717 -4.2131252 -4.214335][-4.249804 -4.2389631 -4.2147121 -4.1742253 -4.1231194 -4.0651689 -4.0044804 -3.967335 -3.974067 -4.0196319 -4.0790339 -4.1322746 -4.1730719 -4.2004752 -4.2103496][-4.2615952 -4.2484179 -4.2075763 -4.1397781 -4.0613379 -3.9847097 -3.910188 -3.8605027 -3.8682227 -3.9370623 -4.0203495 -4.0910873 -4.1468787 -4.1868234 -4.206759][-4.2563548 -4.2515469 -4.21118 -4.134665 -4.0479989 -3.9715035 -3.9014456 -3.8518591 -3.8551545 -3.9250522 -4.0103955 -4.0837393 -4.1447549 -4.1895337 -4.2150369][-4.2237535 -4.2369561 -4.2166748 -4.1608572 -4.0992279 -4.0496068 -4.006515 -3.9740412 -3.9748445 -4.0193481 -4.078021 -4.1329212 -4.1825423 -4.2205815 -4.2434244][-4.1841488 -4.2147932 -4.2182879 -4.1924787 -4.1633606 -4.1431413 -4.126307 -4.110384 -4.1080389 -4.1293087 -4.1622529 -4.1983838 -4.2341347 -4.2606721 -4.2745523][-4.1768703 -4.2117734 -4.2268491 -4.2221189 -4.2182913 -4.2216105 -4.2238503 -4.2208195 -4.2152815 -4.2198329 -4.2330356 -4.2534924 -4.2767181 -4.2941918 -4.3019066][-4.2092371 -4.2392282 -4.2538857 -4.2557836 -4.2614808 -4.2751908 -4.2867646 -4.2903247 -4.28412 -4.2784166 -4.279263 -4.2872119 -4.2990818 -4.3099375 -4.3150558][-4.2540116 -4.2771087 -4.2861114 -4.2850423 -4.2878079 -4.2992945 -4.31037 -4.315589 -4.3127594 -4.3062677 -4.3035469 -4.3061371 -4.3110037 -4.3171697 -4.3204856][-4.2908525 -4.304709 -4.3092813 -4.3063107 -4.3063087 -4.3139172 -4.3225527 -4.3274822 -4.3274159 -4.3241053 -4.321847 -4.3212047 -4.3205824 -4.3220763 -4.3229055]]...]
INFO - root - 2017-12-07 21:28:37.145192: step 24710, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.602 sec/batch; 62h:59m:40s remains)
INFO - root - 2017-12-07 21:28:53.419650: step 24720, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.636 sec/batch; 64h:19m:25s remains)
INFO - root - 2017-12-07 21:29:09.476620: step 24730, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.597 sec/batch; 62h:45m:50s remains)
INFO - root - 2017-12-07 21:29:25.843507: step 24740, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.642 sec/batch; 64h:33m:21s remains)
INFO - root - 2017-12-07 21:29:42.124305: step 24750, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.636 sec/batch; 64h:18m:22s remains)
INFO - root - 2017-12-07 21:29:58.342913: step 24760, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.557 sec/batch; 61h:10m:38s remains)
INFO - root - 2017-12-07 21:30:14.525233: step 24770, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.700 sec/batch; 66h:49m:36s remains)
INFO - root - 2017-12-07 21:30:30.579441: step 24780, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 1.606 sec/batch; 63h:07m:23s remains)
INFO - root - 2017-12-07 21:30:47.021276: step 24790, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.669 sec/batch; 65h:35m:51s remains)
INFO - root - 2017-12-07 21:31:03.169746: step 24800, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.552 sec/batch; 60h:58m:30s remains)
2017-12-07 21:31:04.649511: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2836266 -4.27515 -4.256588 -4.257822 -4.2729158 -4.2898426 -4.3051729 -4.3108358 -4.3010035 -4.2794266 -4.2558393 -4.2592735 -4.2924733 -4.3254533 -4.3359385][-4.2911677 -4.2829623 -4.2654576 -4.2723188 -4.2928853 -4.3128872 -4.3272066 -4.327261 -4.30756 -4.2747278 -4.2422242 -4.2440472 -4.2802997 -4.319912 -4.33035][-4.2918806 -4.2804122 -4.2655463 -4.2792349 -4.3060985 -4.3307343 -4.3418121 -4.3339882 -4.3060932 -4.2651043 -4.2291059 -4.22723 -4.2609677 -4.3042121 -4.317966][-4.28832 -4.2774172 -4.2685804 -4.2878056 -4.3191671 -4.3443375 -4.3475633 -4.3278027 -4.2895379 -4.2446413 -4.2113295 -4.2091756 -4.2389011 -4.2823577 -4.3025584][-4.2859492 -4.2841873 -4.2885962 -4.3151627 -4.3471713 -4.3608823 -4.3471107 -4.3099346 -4.2558365 -4.2036505 -4.1734395 -4.1752443 -4.2078481 -4.2561092 -4.2859459][-4.2902269 -4.3023267 -4.3239322 -4.3589611 -4.3846354 -4.3752937 -4.3351159 -4.2721057 -4.1965933 -4.1341643 -4.1071754 -4.1201749 -4.1693392 -4.2305098 -4.2695403][-4.3085065 -4.3336468 -4.3644462 -4.3957176 -4.4048748 -4.3695288 -4.3031688 -4.2144904 -4.12096 -4.0484891 -4.0287161 -4.0621595 -4.1366267 -4.2116609 -4.2534423][-4.3365211 -4.3684793 -4.3967814 -4.4148288 -4.405056 -4.3510242 -4.2686906 -4.1657081 -4.0644565 -3.9877071 -3.9744694 -4.029973 -4.123951 -4.2049117 -4.2424097][-4.3594637 -4.3896775 -4.4076347 -4.4101577 -4.3874888 -4.329886 -4.2482333 -4.1466293 -4.0496712 -3.9740243 -3.9690559 -4.0431209 -4.146811 -4.2209735 -4.2461157][-4.3638539 -4.3856497 -4.3915052 -4.3815084 -4.3511367 -4.30044 -4.2355795 -4.1550636 -4.0752487 -4.01573 -4.029037 -4.1150866 -4.2113986 -4.265317 -4.268682][-4.3429227 -4.3540907 -4.3506489 -4.3316922 -4.302134 -4.2662749 -4.2250419 -4.1720576 -4.1193018 -4.0924039 -4.131341 -4.216579 -4.2886086 -4.312747 -4.2914705][-4.3089104 -4.3135986 -4.3059983 -4.2861862 -4.2680745 -4.2549553 -4.2378521 -4.2062607 -4.1743107 -4.17661 -4.2322021 -4.3053079 -4.3464336 -4.3424821 -4.3031669][-4.2765851 -4.279398 -4.2725153 -4.2569952 -4.2531695 -4.2621117 -4.2652521 -4.250349 -4.23345 -4.2501121 -4.3075762 -4.3645644 -4.3806329 -4.35592 -4.304606][-4.2508616 -4.255352 -4.2534404 -4.247 -4.25781 -4.2812605 -4.2957377 -4.2919755 -4.2854242 -4.3031688 -4.3516912 -4.3907652 -4.3880658 -4.3460011 -4.2820396][-4.2362247 -4.2435255 -4.24843 -4.2545872 -4.2778649 -4.3070369 -4.3233171 -4.3238187 -4.322804 -4.33751 -4.3731966 -4.391655 -4.3683391 -4.301127 -4.2183666]]...]
INFO - root - 2017-12-07 21:31:20.535968: step 24810, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.557 sec/batch; 61h:10m:02s remains)
INFO - root - 2017-12-07 21:31:36.907126: step 24820, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.664 sec/batch; 65h:21m:16s remains)
INFO - root - 2017-12-07 21:31:52.956563: step 24830, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 62h:27m:21s remains)
INFO - root - 2017-12-07 21:32:09.282693: step 24840, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.708 sec/batch; 67h:04m:56s remains)
INFO - root - 2017-12-07 21:32:25.566937: step 24850, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.618 sec/batch; 63h:32m:32s remains)
INFO - root - 2017-12-07 21:32:41.831877: step 24860, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.660 sec/batch; 65h:12m:18s remains)
INFO - root - 2017-12-07 21:32:58.051500: step 24870, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.627 sec/batch; 63h:53m:57s remains)
INFO - root - 2017-12-07 21:33:14.456513: step 24880, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 1.627 sec/batch; 63h:53m:46s remains)
INFO - root - 2017-12-07 21:33:30.659223: step 24890, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 1.703 sec/batch; 66h:51m:05s remains)
INFO - root - 2017-12-07 21:33:46.863173: step 24900, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.552 sec/batch; 60h:56m:17s remains)
2017-12-07 21:33:48.232406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3238378 -4.323554 -4.3210983 -4.3197556 -4.3188605 -4.3188224 -4.3189778 -4.3204226 -4.3241124 -4.3299069 -4.3370605 -4.3430738 -4.3463178 -4.3463421 -4.3431463][-4.3118248 -4.3091841 -4.3055162 -4.3051252 -4.3055482 -4.3059087 -4.3052068 -4.3048224 -4.3094826 -4.3175821 -4.327342 -4.3349757 -4.338213 -4.3375449 -4.3349781][-4.2928495 -4.2836351 -4.2777505 -4.2789612 -4.2822027 -4.2833781 -4.2803669 -4.2775116 -4.2838044 -4.2970924 -4.3122153 -4.3218226 -4.3235044 -4.3204417 -4.3175554][-4.2548409 -4.2344265 -4.2239923 -4.2277064 -4.2385216 -4.2445869 -4.2402649 -4.2360673 -4.2426872 -4.2576752 -4.2741652 -4.287683 -4.2923684 -4.2896438 -4.28788][-4.2060685 -4.1691818 -4.1472368 -4.1487284 -4.1643434 -4.1785669 -4.1780386 -4.1779461 -4.1901684 -4.2099676 -4.229938 -4.2500777 -4.2596779 -4.2579575 -4.2568583][-4.1467705 -4.1012754 -4.0730062 -4.0748973 -4.0947938 -4.1149564 -4.1202374 -4.1244731 -4.1453419 -4.1732321 -4.2004609 -4.2267089 -4.2395215 -4.2378721 -4.2347655][-4.1052947 -4.0635791 -4.0365133 -4.0393829 -4.060411 -4.0819016 -4.0877619 -4.0883946 -4.1066504 -4.136271 -4.1656322 -4.1915722 -4.2063332 -4.2110219 -4.2131066][-4.0979853 -4.061553 -4.0323424 -4.0317116 -4.0493927 -4.0664196 -4.0666447 -4.057044 -4.0627174 -4.0857711 -4.1137242 -4.1376696 -4.1561189 -4.1742673 -4.1906071][-4.1198549 -4.092968 -4.0651989 -4.05904 -4.0684433 -4.0733919 -4.0627217 -4.0411077 -4.031055 -4.0444655 -4.0713305 -4.0957952 -4.1190257 -4.1481924 -4.1776814][-4.1643968 -4.1475854 -4.1249752 -4.1115365 -4.1106849 -4.1039777 -4.0827851 -4.051538 -4.03082 -4.0372515 -4.0633984 -4.088551 -4.112267 -4.1407909 -4.1730409][-4.2165828 -4.2082067 -4.1916318 -4.17536 -4.1698513 -4.1617804 -4.14009 -4.111865 -4.094286 -4.099154 -4.1199551 -4.1391921 -4.1519456 -4.1645107 -4.1854181][-4.2512975 -4.2480173 -4.2357864 -4.2226067 -4.2184653 -4.2156515 -4.2016029 -4.1833158 -4.176908 -4.1866665 -4.201004 -4.2088919 -4.2078066 -4.2050862 -4.21088][-4.2674413 -4.2623119 -4.2515631 -4.243031 -4.2429624 -4.2450042 -4.2397857 -4.2330604 -4.2356458 -4.2471848 -4.2565494 -4.2586107 -4.2517471 -4.2438674 -4.2429895][-4.2754469 -4.2699995 -4.2626638 -4.2585654 -4.2599869 -4.2620044 -4.2611489 -4.2609906 -4.26762 -4.2779346 -4.2831187 -4.2823257 -4.2760534 -4.2712293 -4.2719607][-4.290761 -4.2871952 -4.2834668 -4.2826324 -4.2843647 -4.2854319 -4.2847443 -4.2847223 -4.2895689 -4.296073 -4.2990279 -4.2983713 -4.295495 -4.2949605 -4.2974987]]...]
INFO - root - 2017-12-07 21:34:04.455324: step 24910, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.568 sec/batch; 61h:32m:56s remains)
INFO - root - 2017-12-07 21:34:20.941665: step 24920, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.724 sec/batch; 67h:41m:38s remains)
INFO - root - 2017-12-07 21:34:37.078594: step 24930, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.565 sec/batch; 61h:25m:59s remains)
INFO - root - 2017-12-07 21:34:53.382353: step 24940, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.732 sec/batch; 67h:59m:45s remains)
INFO - root - 2017-12-07 21:35:09.638921: step 24950, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.626 sec/batch; 63h:49m:52s remains)
INFO - root - 2017-12-07 21:35:25.968433: step 24960, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.649 sec/batch; 64h:42m:58s remains)
INFO - root - 2017-12-07 21:35:42.279110: step 24970, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.612 sec/batch; 63h:15m:35s remains)
INFO - root - 2017-12-07 21:35:58.436265: step 24980, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 62h:41m:31s remains)
INFO - root - 2017-12-07 21:36:14.825005: step 24990, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.597 sec/batch; 62h:40m:10s remains)
INFO - root - 2017-12-07 21:36:30.950679: step 25000, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.610 sec/batch; 63h:10m:21s remains)
2017-12-07 21:36:32.376861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3178391 -4.3153257 -4.3169303 -4.3144546 -4.3143606 -4.313241 -4.3121986 -4.3120794 -4.2996287 -4.2795844 -4.2655268 -4.2630925 -4.2698655 -4.2791452 -4.2937479][-4.3047156 -4.3021941 -4.3084588 -4.3111091 -4.3123217 -4.3073521 -4.302207 -4.301352 -4.2925839 -4.273984 -4.25801 -4.2549515 -4.2622986 -4.2710419 -4.2850337][-4.2688594 -4.2692623 -4.2811728 -4.2895741 -4.29218 -4.2833362 -4.2748985 -4.2746105 -4.2702427 -4.2543454 -4.2364674 -4.2299008 -4.2353721 -4.2413468 -4.2537837][-4.2119074 -4.2166505 -4.2336478 -4.2475128 -4.2507606 -4.238131 -4.223731 -4.221777 -4.2197523 -4.2074995 -4.1900969 -4.1780987 -4.1814947 -4.1881862 -4.2007985][-4.1517968 -4.1617641 -4.1844349 -4.2020292 -4.202981 -4.181994 -4.1570082 -4.149755 -4.149766 -4.1406813 -4.1255264 -4.1175704 -4.1226811 -4.134295 -4.1505094][-4.1128206 -4.127985 -4.1512432 -4.1629624 -4.1527562 -4.1141334 -4.0688338 -4.055244 -4.0669103 -4.0753112 -4.0770464 -4.0798383 -4.0928636 -4.1114788 -4.1298671][-4.1045551 -4.116117 -4.1270733 -4.124959 -4.1006804 -4.04382 -3.9714477 -3.9499342 -3.9882765 -4.0361972 -4.0689774 -4.0896397 -4.1096659 -4.1281934 -4.144721][-4.1357946 -4.1358252 -4.1325431 -4.118433 -4.0897613 -4.0373268 -3.9696457 -3.954555 -4.0044451 -4.0728254 -4.1246357 -4.1525588 -4.1670775 -4.1757236 -4.1893921][-4.1862922 -4.1784225 -4.1693811 -4.1576829 -4.1413174 -4.1144629 -4.0830026 -4.08178 -4.1141634 -4.1652446 -4.2075009 -4.2277918 -4.2322297 -4.2289243 -4.2335243][-4.2279949 -4.2214026 -4.2171664 -4.2139277 -4.2081361 -4.198216 -4.1902208 -4.1942172 -4.2105131 -4.23646 -4.2597694 -4.2696767 -4.2673206 -4.258317 -4.2590075][-4.2464323 -4.2464576 -4.2547746 -4.2619629 -4.262146 -4.2580366 -4.2569408 -4.2601323 -4.2651687 -4.2729115 -4.2814374 -4.2844558 -4.2806473 -4.2734504 -4.2698631][-4.2501655 -4.255331 -4.2719293 -4.2876868 -4.2928429 -4.289115 -4.2862258 -4.2851467 -4.2843719 -4.2852964 -4.2879653 -4.29045 -4.2909341 -4.2887425 -4.282135][-4.2584062 -4.26476 -4.2799325 -4.2943487 -4.2992258 -4.2947283 -4.2890754 -4.285677 -4.2853942 -4.287365 -4.2906604 -4.2953496 -4.2994642 -4.2984552 -4.2896638][-4.2648325 -4.2689023 -4.2787356 -4.2892294 -4.293045 -4.2900648 -4.2847409 -4.2837358 -4.2873955 -4.292932 -4.2985158 -4.3038096 -4.3073397 -4.3045311 -4.2927837][-4.2668591 -4.2671447 -4.2699556 -4.2755427 -4.2782388 -4.2783279 -4.2789612 -4.2838864 -4.2930737 -4.3009014 -4.3060594 -4.3077984 -4.3057714 -4.2964678 -4.2794151]]...]
INFO - root - 2017-12-07 21:36:48.593006: step 25010, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.606 sec/batch; 63h:01m:09s remains)
INFO - root - 2017-12-07 21:37:05.152083: step 25020, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.598 sec/batch; 62h:40m:29s remains)
INFO - root - 2017-12-07 21:37:21.379073: step 25030, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.676 sec/batch; 65h:45m:35s remains)
INFO - root - 2017-12-07 21:37:37.444566: step 25040, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.578 sec/batch; 61h:52m:48s remains)
INFO - root - 2017-12-07 21:37:53.848081: step 25050, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.709 sec/batch; 67h:02m:11s remains)
INFO - root - 2017-12-07 21:38:10.117751: step 25060, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.622 sec/batch; 63h:37m:38s remains)
INFO - root - 2017-12-07 21:38:26.329344: step 25070, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.653 sec/batch; 64h:50m:26s remains)
INFO - root - 2017-12-07 21:38:42.505408: step 25080, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.610 sec/batch; 63h:08m:27s remains)
INFO - root - 2017-12-07 21:38:58.895254: step 25090, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.599 sec/batch; 62h:42m:27s remains)
INFO - root - 2017-12-07 21:39:15.001568: step 25100, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.633 sec/batch; 64h:01m:56s remains)
2017-12-07 21:39:16.467339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3576055 -4.3522234 -4.3392467 -4.323679 -4.3136415 -4.3062 -4.301321 -4.3047247 -4.3107839 -4.3085113 -4.2988815 -4.2941303 -4.3033781 -4.3188138 -4.33264][-4.3592725 -4.355433 -4.3422389 -4.3247914 -4.3140855 -4.3051538 -4.2983313 -4.2975597 -4.2956352 -4.2831235 -4.2622309 -4.2514534 -4.2625012 -4.2844944 -4.3081651][-4.3637557 -4.3598709 -4.3427973 -4.3185148 -4.2999911 -4.2835145 -4.2735167 -4.268734 -4.2572227 -4.2338057 -4.2026176 -4.1864076 -4.2023768 -4.2332697 -4.2701473][-4.3668175 -4.3568397 -4.3271203 -4.2872839 -4.2525005 -4.2225957 -4.2111468 -4.2093391 -4.197639 -4.1763563 -4.1418891 -4.1194134 -4.139174 -4.1788044 -4.2283092][-4.3647308 -4.3432393 -4.2947187 -4.2316408 -4.1741223 -4.1255579 -4.1153922 -4.1281924 -4.1316233 -4.131115 -4.1122646 -4.0934072 -4.1125402 -4.15406 -4.2089777][-4.3569541 -4.3231463 -4.2547708 -4.1680574 -4.0867124 -4.0198069 -4.0117531 -4.0479302 -4.0792603 -4.1114874 -4.1215081 -4.1158781 -4.1329541 -4.1686735 -4.2188506][-4.3482623 -4.3074017 -4.2288961 -4.129818 -4.0401888 -3.9687479 -3.9653056 -4.0216956 -4.074616 -4.127737 -4.1595621 -4.1649766 -4.1768656 -4.2019272 -4.2393479][-4.3464961 -4.3076477 -4.235146 -4.1481981 -4.0755854 -4.0166273 -4.0106173 -4.0642095 -4.1188087 -4.1730046 -4.2109752 -4.2201467 -4.2251596 -4.2419586 -4.2654572][-4.3468356 -4.3129005 -4.2536097 -4.1900678 -4.1459045 -4.108439 -4.0988269 -4.1344738 -4.1740046 -4.2171807 -4.2529292 -4.2630024 -4.2683864 -4.2833433 -4.2999954][-4.3451185 -4.3124876 -4.2601852 -4.2115731 -4.1861868 -4.166935 -4.1612959 -4.187098 -4.2123384 -4.2432809 -4.2742224 -4.2869635 -4.2973719 -4.3152251 -4.3300252][-4.3425345 -4.3070488 -4.2529054 -4.2056932 -4.1856303 -4.1755853 -4.1807327 -4.2111306 -4.2341385 -4.2584333 -4.2810507 -4.2907829 -4.302002 -4.3222075 -4.3406205][-4.3433609 -4.3064318 -4.2504425 -4.2015648 -4.1794319 -4.1696482 -4.1850648 -4.2275853 -4.2561712 -4.2748036 -4.28368 -4.2846489 -4.2942133 -4.3149843 -4.3378549][-4.3453207 -4.3104329 -4.2584333 -4.2116585 -4.18863 -4.1805453 -4.2027664 -4.2493467 -4.276897 -4.2879782 -4.2836876 -4.2745252 -4.2802773 -4.3005128 -4.3264227][-4.3448467 -4.3126326 -4.2669072 -4.225656 -4.2064843 -4.2048144 -4.2305384 -4.2732511 -4.2951479 -4.2986445 -4.28568 -4.2697477 -4.2711582 -4.2893186 -4.3154922][-4.341311 -4.3080382 -4.2641597 -4.2256975 -4.2108746 -4.2177839 -4.2468948 -4.2841005 -4.303515 -4.3065825 -4.294373 -4.2778053 -4.2751989 -4.2881036 -4.3103814]]...]
INFO - root - 2017-12-07 21:39:32.639647: step 25110, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.580 sec/batch; 61h:57m:01s remains)
INFO - root - 2017-12-07 21:39:48.988775: step 25120, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.548 sec/batch; 60h:40m:46s remains)
INFO - root - 2017-12-07 21:40:05.191292: step 25130, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.663 sec/batch; 65h:11m:52s remains)
INFO - root - 2017-12-07 21:40:21.465897: step 25140, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.582 sec/batch; 62h:01m:20s remains)
INFO - root - 2017-12-07 21:40:37.733301: step 25150, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.685 sec/batch; 66h:02m:55s remains)
INFO - root - 2017-12-07 21:40:53.770828: step 25160, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.556 sec/batch; 60h:58m:04s remains)
INFO - root - 2017-12-07 21:41:09.879442: step 25170, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.701 sec/batch; 66h:38m:36s remains)
INFO - root - 2017-12-07 21:41:26.231311: step 25180, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.587 sec/batch; 62h:11m:21s remains)
INFO - root - 2017-12-07 21:41:42.535871: step 25190, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.660 sec/batch; 65h:02m:32s remains)
INFO - root - 2017-12-07 21:41:58.715834: step 25200, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.556 sec/batch; 60h:58m:18s remains)
2017-12-07 21:42:00.171693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3289547 -4.3285856 -4.3255286 -4.3217759 -4.3180823 -4.3161645 -4.3163066 -4.3153696 -4.3111219 -4.3068743 -4.3075972 -4.3128986 -4.3217177 -4.330122 -4.336586][-4.342628 -4.33608 -4.3267512 -4.3153095 -4.3037629 -4.2980375 -4.2987127 -4.2956915 -4.2839646 -4.2766576 -4.2802534 -4.2897549 -4.3043308 -4.3186917 -4.3292346][-4.3414555 -4.3304896 -4.3135872 -4.29307 -4.2737155 -4.2646017 -4.2627506 -4.2548041 -4.2365446 -4.2288442 -4.2395988 -4.2581415 -4.2829437 -4.3072734 -4.323864][-4.328269 -4.3168855 -4.2927895 -4.2597733 -4.22681 -4.2058187 -4.1926169 -4.1740546 -4.1431875 -4.1310744 -4.1536303 -4.19213 -4.2353258 -4.2766643 -4.3051648][-4.29699 -4.2859521 -4.2570324 -4.2115369 -4.1585774 -4.1148071 -4.0779839 -4.0411644 -3.9932482 -3.9728186 -4.0151563 -4.0863047 -4.1596761 -4.2239976 -4.2688][-4.2457857 -4.2335949 -4.1998863 -4.1430888 -4.0697384 -3.9979968 -3.930378 -3.8669653 -3.7900562 -3.7485385 -3.8141465 -3.9344752 -4.0555077 -4.1518908 -4.2199755][-4.2219667 -4.2030921 -4.1607733 -4.0926881 -4.0025916 -3.9043517 -3.8083372 -3.7134905 -3.5955374 -3.5220618 -3.6062849 -3.7802882 -3.9528806 -4.0810351 -4.1733212][-4.2457972 -4.2224212 -4.1774836 -4.1115327 -4.0268736 -3.9363585 -3.8544219 -3.7704928 -3.6535807 -3.5714469 -3.6392951 -3.7982013 -3.9587405 -4.0818768 -4.1738305][-4.2731061 -4.2627239 -4.2326965 -4.1884551 -4.128335 -4.0621877 -4.0077109 -3.9536657 -3.8667936 -3.7936783 -3.8282895 -3.9363732 -4.04731 -4.1347938 -4.2073545][-4.284574 -4.2947669 -4.2892795 -4.2714515 -4.2307048 -4.1818609 -4.1455827 -4.1109438 -4.047554 -3.9887784 -4.0033803 -4.0717449 -4.1421785 -4.1978197 -4.248343][-4.267076 -4.2896614 -4.3031263 -4.3012395 -4.2746067 -4.2443671 -4.2237277 -4.2062087 -4.1688275 -4.1312928 -4.1348295 -4.1734591 -4.2126174 -4.2441735 -4.2782664][-4.2278996 -4.2513471 -4.2746935 -4.2834816 -4.2692466 -4.2568874 -4.2513204 -4.2450933 -4.2307644 -4.2149005 -4.2154403 -4.2360821 -4.2560978 -4.2742076 -4.2974286][-4.1861959 -4.2010875 -4.2260966 -4.2456141 -4.2459521 -4.2476692 -4.2530212 -4.2539787 -4.2597671 -4.2646947 -4.2707052 -4.2828541 -4.2932324 -4.2998705 -4.3110585][-4.1634493 -4.1712317 -4.192368 -4.2177353 -4.2321291 -4.2437191 -4.2529416 -4.2531862 -4.2641025 -4.2785645 -4.2914009 -4.3034906 -4.3105316 -4.3121433 -4.3169117][-4.1614275 -4.1669116 -4.1806245 -4.2012439 -4.21769 -4.2341018 -4.2467079 -4.2461967 -4.2534552 -4.266078 -4.2811184 -4.2959051 -4.3068156 -4.3112245 -4.3165264]]...]
INFO - root - 2017-12-07 21:42:16.386891: step 25210, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.534 sec/batch; 60h:05m:03s remains)
INFO - root - 2017-12-07 21:42:32.630453: step 25220, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.656 sec/batch; 64h:53m:06s remains)
INFO - root - 2017-12-07 21:42:48.808666: step 25230, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.555 sec/batch; 60h:54m:34s remains)
INFO - root - 2017-12-07 21:43:05.372905: step 25240, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.577 sec/batch; 61h:46m:07s remains)
INFO - root - 2017-12-07 21:43:21.531759: step 25250, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.632 sec/batch; 63h:56m:21s remains)
INFO - root - 2017-12-07 21:43:37.339698: step 25260, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.551 sec/batch; 60h:45m:21s remains)
INFO - root - 2017-12-07 21:43:53.399890: step 25270, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.643 sec/batch; 64h:20m:57s remains)
INFO - root - 2017-12-07 21:44:09.697236: step 25280, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.672 sec/batch; 65h:29m:04s remains)
INFO - root - 2017-12-07 21:44:25.911908: step 25290, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.628 sec/batch; 63h:44m:51s remains)
INFO - root - 2017-12-07 21:44:42.210567: step 25300, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.721 sec/batch; 67h:21m:51s remains)
2017-12-07 21:44:43.585167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.253602 -4.2274971 -4.1994185 -4.1837754 -4.1887169 -4.204217 -4.2145991 -4.2035918 -4.1756525 -4.1654811 -4.189714 -4.2228651 -4.228477 -4.1949015 -4.1758404][-4.25247 -4.2291417 -4.2060494 -4.1987972 -4.2128639 -4.2346535 -4.2481589 -4.2394338 -4.2130437 -4.2027893 -4.2240362 -4.2523742 -4.2595959 -4.2330809 -4.2157764][-4.2672548 -4.2444286 -4.2237687 -4.2183557 -4.2355618 -4.2580681 -4.2691669 -4.2603059 -4.2384562 -4.2294025 -4.2462044 -4.2679148 -4.2718678 -4.2492242 -4.2293625][-4.2864451 -4.2673364 -4.2485704 -4.2388659 -4.2464337 -4.2587538 -4.2617621 -4.24691 -4.225245 -4.21366 -4.2223325 -4.2382321 -4.2430491 -4.2282391 -4.2071314][-4.2957954 -4.2813673 -4.2617893 -4.2444296 -4.2370319 -4.2350197 -4.2246761 -4.1998711 -4.1678939 -4.1443734 -4.146286 -4.1711421 -4.1934547 -4.1912022 -4.1721945][-4.2799153 -4.2633977 -4.2385349 -4.214613 -4.1953335 -4.1821256 -4.1595531 -4.1184244 -4.0682983 -4.033453 -4.0377688 -4.0846319 -4.1362286 -4.1522622 -4.1384554][-4.2237449 -4.20203 -4.1738949 -4.1508179 -4.1330042 -4.114193 -4.0838871 -4.0289588 -3.9575284 -3.9053576 -3.9125445 -3.9888477 -4.0759358 -4.1173048 -4.1194668][-4.1437407 -4.1126337 -4.0807776 -4.0653992 -4.0608134 -4.0519629 -4.0341353 -3.9890525 -3.9163718 -3.8505476 -3.8578391 -3.9533134 -4.0530448 -4.1046963 -4.1175275][-4.078218 -4.0390644 -4.0083308 -4.0065136 -4.0261645 -4.0463934 -4.0531006 -4.0349746 -3.9840841 -3.9254942 -3.9241219 -3.9990315 -4.0783329 -4.1170378 -4.1280355][-4.0786438 -4.0418396 -4.017838 -4.0285473 -4.0669985 -4.1065869 -4.1281323 -4.1248217 -4.0922933 -4.05077 -4.0425882 -4.084444 -4.1300545 -4.1470022 -4.1473184][-4.1546988 -4.13159 -4.1184416 -4.1309514 -4.1668849 -4.2036481 -4.2219949 -4.218751 -4.1966877 -4.17127 -4.1639552 -4.1815662 -4.1959777 -4.1897621 -4.1728148][-4.2550325 -4.2441444 -4.2370114 -4.2445111 -4.26744 -4.2903771 -4.3000059 -4.2933211 -4.2778664 -4.2616978 -4.2553248 -4.2586713 -4.2551556 -4.2356725 -4.2085447][-4.3291788 -4.3270049 -4.3233752 -4.3279657 -4.3404069 -4.3513513 -4.35266 -4.3433247 -4.3293552 -4.3167396 -4.3104143 -4.3094749 -4.3020787 -4.2832704 -4.2582569][-4.3659921 -4.3658123 -4.3642678 -4.3668351 -4.3719115 -4.3749738 -4.372963 -4.3647103 -4.3535686 -4.3436947 -4.33774 -4.3351846 -4.3302355 -4.3181 -4.3034048][-4.376792 -4.3775554 -4.3760142 -4.3749995 -4.3748064 -4.37526 -4.3746233 -4.371264 -4.3656359 -4.3597021 -4.3550582 -4.3520408 -4.3484273 -4.3430691 -4.3380637]]...]
INFO - root - 2017-12-07 21:44:59.809145: step 25310, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 1.621 sec/batch; 63h:28m:25s remains)
INFO - root - 2017-12-07 21:45:16.242381: step 25320, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.633 sec/batch; 63h:55m:03s remains)
INFO - root - 2017-12-07 21:45:32.650242: step 25330, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.673 sec/batch; 65h:28m:40s remains)
INFO - root - 2017-12-07 21:45:48.985005: step 25340, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.594 sec/batch; 62h:23m:49s remains)
INFO - root - 2017-12-07 21:46:05.251574: step 25350, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.709 sec/batch; 66h:53m:37s remains)
INFO - root - 2017-12-07 21:46:21.518924: step 25360, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.570 sec/batch; 61h:26m:22s remains)
INFO - root - 2017-12-07 21:46:37.867992: step 25370, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 63h:46m:07s remains)
INFO - root - 2017-12-07 21:46:54.129192: step 25380, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.638 sec/batch; 64h:06m:12s remains)
INFO - root - 2017-12-07 21:47:10.365674: step 25390, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.604 sec/batch; 62h:44m:31s remains)
INFO - root - 2017-12-07 21:47:26.728724: step 25400, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.685 sec/batch; 65h:54m:39s remains)
2017-12-07 21:47:28.021128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2391095 -4.2433181 -4.2367177 -4.2329979 -4.2488227 -4.2666578 -4.2888422 -4.2978978 -4.2803783 -4.2396216 -4.1999764 -4.1782455 -4.1659956 -4.1665311 -4.1840434][-4.2404728 -4.2525425 -4.249908 -4.2429743 -4.2508373 -4.2602739 -4.2791419 -4.2909031 -4.2736764 -4.2289019 -4.1868038 -4.1637769 -4.1503992 -4.1482868 -4.16717][-4.2404394 -4.257545 -4.2600722 -4.2510605 -4.2454624 -4.2421684 -4.2539783 -4.269309 -4.25667 -4.2147927 -4.1772909 -4.1589923 -4.1477213 -4.1440735 -4.1614923][-4.2419271 -4.2592454 -4.2670507 -4.2566724 -4.2400794 -4.2252088 -4.2267008 -4.2404256 -4.2328539 -4.201714 -4.1731071 -4.1626177 -4.1551318 -4.1524734 -4.1668835][-4.2446918 -4.2628293 -4.2749543 -4.2614489 -4.2347341 -4.210598 -4.2000613 -4.2070284 -4.2028456 -4.1882668 -4.174264 -4.1725154 -4.1730795 -4.175149 -4.1848149][-4.2436414 -4.2648015 -4.2798529 -4.2628126 -4.2267594 -4.1922145 -4.1667924 -4.1643443 -4.1627913 -4.1648655 -4.1690307 -4.1777663 -4.1888204 -4.1980028 -4.2063169][-4.2380462 -4.2629175 -4.2779088 -4.2588372 -4.2153935 -4.167809 -4.1241837 -4.1133108 -4.1200514 -4.1437092 -4.1692634 -4.1918383 -4.2126832 -4.2284861 -4.2354312][-4.2271419 -4.2557135 -4.2714992 -4.2542448 -4.2082763 -4.1492362 -4.0920362 -4.0752354 -4.0943866 -4.1376586 -4.1822248 -4.2154012 -4.2409139 -4.2590547 -4.2655048][-4.2221236 -4.2518859 -4.2687507 -4.256216 -4.2115707 -4.1471763 -4.0844169 -4.0653152 -4.0923781 -4.1457467 -4.2006793 -4.2399116 -4.264616 -4.2814317 -4.288044][-4.2290688 -4.26003 -4.2780728 -4.2703967 -4.2324085 -4.1736436 -4.1168575 -4.0986333 -4.1256566 -4.1748271 -4.226933 -4.2653995 -4.2862844 -4.2974467 -4.2995672][-4.2485051 -4.2780442 -4.2941 -4.2882223 -4.2579055 -4.2105279 -4.1673813 -4.15519 -4.180191 -4.2188773 -4.2592196 -4.2888885 -4.3034945 -4.307848 -4.3031282][-4.27479 -4.300046 -4.3121042 -4.3077602 -4.2856388 -4.2502728 -4.2198191 -4.2117114 -4.2313242 -4.2591357 -4.2865191 -4.305491 -4.3144884 -4.3146954 -4.3046288][-4.2995005 -4.3187385 -4.3272014 -4.325532 -4.3121905 -4.2887211 -4.267468 -4.2608256 -4.2726927 -4.2901607 -4.3069277 -4.3176203 -4.3215785 -4.3207259 -4.3091159][-4.3170462 -4.3299484 -4.3347516 -4.3348374 -4.3285646 -4.3135967 -4.2989388 -4.2930431 -4.3000331 -4.3112435 -4.3223886 -4.3282852 -4.3282132 -4.3270645 -4.3159919][-4.3267527 -4.3356538 -4.3392067 -4.3399615 -4.3377357 -4.3294797 -4.3201952 -4.3156071 -4.3186636 -4.3253164 -4.3337054 -4.3375597 -4.335412 -4.3341479 -4.3258276]]...]
INFO - root - 2017-12-07 21:47:44.383843: step 25410, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.685 sec/batch; 65h:54m:44s remains)
INFO - root - 2017-12-07 21:48:00.743339: step 25420, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.604 sec/batch; 62h:45m:05s remains)
INFO - root - 2017-12-07 21:48:17.029045: step 25430, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.696 sec/batch; 66h:20m:14s remains)
INFO - root - 2017-12-07 21:48:33.041374: step 25440, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 1.574 sec/batch; 61h:33m:41s remains)
INFO - root - 2017-12-07 21:48:49.249071: step 25450, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.671 sec/batch; 65h:22m:11s remains)
INFO - root - 2017-12-07 21:49:05.476051: step 25460, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.580 sec/batch; 61h:46m:39s remains)
INFO - root - 2017-12-07 21:49:21.708623: step 25470, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.646 sec/batch; 64h:22m:22s remains)
INFO - root - 2017-12-07 21:49:38.013037: step 25480, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.563 sec/batch; 61h:08m:00s remains)
INFO - root - 2017-12-07 21:49:54.191984: step 25490, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.593 sec/batch; 62h:16m:18s remains)
INFO - root - 2017-12-07 21:50:10.334702: step 25500, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.637 sec/batch; 64h:00m:35s remains)
2017-12-07 21:50:11.763039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3062291 -4.3082433 -4.3080158 -4.3066225 -4.3069468 -4.3087449 -4.3086348 -4.3062639 -4.302063 -4.2989154 -4.3005972 -4.3055682 -4.3102388 -4.3123517 -4.3122211][-4.2941704 -4.2948709 -4.2922549 -4.288806 -4.2898669 -4.2926373 -4.2922831 -4.28778 -4.2794614 -4.27579 -4.2810774 -4.2912025 -4.2985721 -4.3012819 -4.3012724][-4.275773 -4.2725854 -4.2640018 -4.2559586 -4.2570143 -4.2609839 -4.2607393 -4.2530093 -4.2391419 -4.2344646 -4.245934 -4.2643356 -4.2763586 -4.28021 -4.280498][-4.2572451 -4.2477283 -4.2320123 -4.2211947 -4.2245364 -4.2307563 -4.2312851 -4.2211494 -4.1998034 -4.1927133 -4.2120647 -4.2411432 -4.25799 -4.26359 -4.2641697][-4.2465458 -4.2300067 -4.2073121 -4.1958632 -4.2037148 -4.2122216 -4.2144556 -4.204752 -4.1779685 -4.1693592 -4.196805 -4.2355347 -4.2561274 -4.2613621 -4.2605462][-4.2330208 -4.2074738 -4.1792746 -4.1726928 -4.1886525 -4.2005143 -4.2056251 -4.1981711 -4.1676345 -4.1574135 -4.1910586 -4.23387 -4.2541442 -4.2572813 -4.2567978][-4.2229495 -4.1866317 -4.1529307 -4.1525121 -4.1732521 -4.1869631 -4.1948848 -4.1884866 -4.1557832 -4.1450253 -4.1824961 -4.22641 -4.2466373 -4.2498484 -4.2498546][-4.2265992 -4.1844234 -4.1477776 -4.1498628 -4.1699548 -4.1813483 -4.189362 -4.18397 -4.1513705 -4.139132 -4.1778059 -4.2231407 -4.2457414 -4.2503633 -4.2513633][-4.2282319 -4.1862054 -4.1547327 -4.1609483 -4.1794367 -4.1859837 -4.1931138 -4.18935 -4.1586685 -4.1426568 -4.179183 -4.22513 -4.2514324 -4.2607689 -4.2641792][-4.2465253 -4.2063761 -4.1783996 -4.1835051 -4.1969814 -4.1978602 -4.200892 -4.1971512 -4.1707883 -4.1535516 -4.1877394 -4.2348003 -4.2639031 -4.2751679 -4.2804976][-4.261198 -4.2259116 -4.20303 -4.20843 -4.2202244 -4.2179918 -4.2155681 -4.2099724 -4.1891518 -4.1724224 -4.20185 -4.2468309 -4.2768579 -4.2898474 -4.2952294][-4.246469 -4.2208405 -4.2054062 -4.2129092 -4.2268486 -4.2263694 -4.2214589 -4.2145653 -4.1981754 -4.1834955 -4.2088485 -4.253633 -4.2845874 -4.2967687 -4.2983179][-4.2239943 -4.208097 -4.200264 -4.210989 -4.2283216 -4.23198 -4.2285919 -4.2230577 -4.2072172 -4.1928654 -4.2169218 -4.2625394 -4.290391 -4.2945333 -4.288116][-4.2090378 -4.20012 -4.1974349 -4.2090344 -4.2274508 -4.2348657 -4.2317533 -4.2258682 -4.2079315 -4.1936121 -4.21874 -4.2646184 -4.286283 -4.281219 -4.2675738][-4.2093129 -4.2032757 -4.2016554 -4.2122059 -4.2296805 -4.2384505 -4.23253 -4.223175 -4.2039361 -4.19079 -4.2168264 -4.2592416 -4.2750545 -4.2642245 -4.2471938]]...]
INFO - root - 2017-12-07 21:50:28.148094: step 25510, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.580 sec/batch; 61h:46m:05s remains)
INFO - root - 2017-12-07 21:50:44.482909: step 25520, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.633 sec/batch; 63h:49m:07s remains)
INFO - root - 2017-12-07 21:51:00.973720: step 25530, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.708 sec/batch; 66h:45m:49s remains)
INFO - root - 2017-12-07 21:51:17.316075: step 25540, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.690 sec/batch; 66h:03m:53s remains)
INFO - root - 2017-12-07 21:51:33.427054: step 25550, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.560 sec/batch; 60h:58m:48s remains)
INFO - root - 2017-12-07 21:51:49.726432: step 25560, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.708 sec/batch; 66h:44m:00s remains)
INFO - root - 2017-12-07 21:52:06.005303: step 25570, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.594 sec/batch; 62h:17m:45s remains)
INFO - root - 2017-12-07 21:52:22.389832: step 25580, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.743 sec/batch; 68h:05m:41s remains)
INFO - root - 2017-12-07 21:52:38.664669: step 25590, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.594 sec/batch; 62h:15m:43s remains)
INFO - root - 2017-12-07 21:52:55.044023: step 25600, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.620 sec/batch; 63h:17m:41s remains)
2017-12-07 21:52:56.310029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2544351 -4.2548003 -4.2510042 -4.2430253 -4.2280564 -4.2164273 -4.2147522 -4.2175732 -4.2211943 -4.2309213 -4.2440596 -4.2625055 -4.2782516 -4.2760053 -4.2614589][-4.2634459 -4.2566571 -4.2396784 -4.2228751 -4.2059636 -4.1944914 -4.1931419 -4.1974926 -4.2027783 -4.2180548 -4.2388878 -4.2607617 -4.277812 -4.2760439 -4.2618213][-4.2848253 -4.2657309 -4.2329388 -4.2041616 -4.1834331 -4.1677289 -4.1580281 -4.1564164 -4.1645722 -4.1900167 -4.2235746 -4.2477045 -4.26146 -4.258069 -4.2450347][-4.2946424 -4.2645206 -4.222302 -4.1883535 -4.1607442 -4.1301689 -4.1027188 -4.0874319 -4.0935941 -4.1313915 -4.1790938 -4.20849 -4.2231827 -4.2230067 -4.2159128][-4.2778606 -4.24233 -4.2022676 -4.168941 -4.1333642 -4.0838695 -4.0333614 -3.996701 -3.9978366 -4.0513835 -4.1201239 -4.1647925 -4.1875935 -4.1945834 -4.193265][-4.2473435 -4.2121696 -4.1745362 -4.1389666 -4.0965476 -4.0287514 -3.9549282 -3.8981111 -3.8994575 -3.9762633 -4.0715747 -4.135941 -4.1709943 -4.1865215 -4.1884637][-4.2178955 -4.1863546 -4.1478343 -4.1062326 -4.0567088 -3.9800091 -3.9044044 -3.8501394 -3.8610282 -3.9514484 -4.0559454 -4.1276131 -4.168777 -4.1883135 -4.1959662][-4.2108903 -4.1879444 -4.15062 -4.1021166 -4.0458775 -3.9830785 -3.941319 -3.9246039 -3.9462795 -4.0141492 -4.09319 -4.1532836 -4.1890306 -4.2085762 -4.2224388][-4.2175837 -4.2065606 -4.1768923 -4.1299868 -4.0787458 -4.0416789 -4.0369759 -4.049686 -4.071806 -4.1083412 -4.153625 -4.1969953 -4.2229414 -4.2398272 -4.2573223][-4.2275691 -4.2229257 -4.2043839 -4.1659179 -4.1271324 -4.1083913 -4.1202283 -4.1409583 -4.1601024 -4.1817307 -4.2087116 -4.2393484 -4.2557974 -4.2658186 -4.2770076][-4.2282887 -4.2289853 -4.220016 -4.1904016 -4.1639414 -4.15358 -4.1668596 -4.1863461 -4.2038 -4.2223344 -4.2389841 -4.2575212 -4.2648263 -4.2681861 -4.2708278][-4.2222743 -4.2237363 -4.2195697 -4.2009425 -4.1858711 -4.18169 -4.1956754 -4.2166166 -4.2366552 -4.2577653 -4.2661314 -4.2713532 -4.2717495 -4.2720771 -4.2716126][-4.2221255 -4.2203312 -4.2128253 -4.201066 -4.196384 -4.1976724 -4.2145796 -4.2446342 -4.2711325 -4.28993 -4.2906985 -4.2830248 -4.2786036 -4.2799506 -4.2802482][-4.2317495 -4.2277241 -4.2188773 -4.2117248 -4.213419 -4.2215137 -4.2436557 -4.2791786 -4.3069148 -4.3170776 -4.3073921 -4.2874136 -4.2775397 -4.2830257 -4.2871943][-4.2547846 -4.2481937 -4.2386765 -4.2342544 -4.2426529 -4.2559547 -4.2772255 -4.3065472 -4.3284984 -4.3350968 -4.32308 -4.3003631 -4.2905498 -4.296833 -4.3015046]]...]
INFO - root - 2017-12-07 21:53:12.548540: step 25610, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.612 sec/batch; 62h:57m:42s remains)
INFO - root - 2017-12-07 21:53:28.769129: step 25620, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.542 sec/batch; 60h:13m:55s remains)
INFO - root - 2017-12-07 21:53:45.203076: step 25630, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.603 sec/batch; 62h:36m:59s remains)
INFO - root - 2017-12-07 21:54:01.506657: step 25640, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 1.644 sec/batch; 64h:12m:32s remains)
INFO - root - 2017-12-07 21:54:17.950425: step 25650, loss = 2.05, batch loss = 1.99 (10.2 examples/sec; 1.569 sec/batch; 61h:17m:39s remains)
INFO - root - 2017-12-07 21:54:34.234170: step 25660, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 61h:25m:24s remains)
INFO - root - 2017-12-07 21:54:50.520457: step 25670, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.633 sec/batch; 63h:45m:21s remains)
INFO - root - 2017-12-07 21:55:06.758274: step 25680, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.726 sec/batch; 67h:22m:52s remains)
INFO - root - 2017-12-07 21:55:23.073453: step 25690, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.558 sec/batch; 60h:49m:01s remains)
INFO - root - 2017-12-07 21:55:39.466229: step 25700, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 1.610 sec/batch; 62h:51m:17s remains)
2017-12-07 21:55:40.819213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0969028 -4.0942097 -4.1248775 -4.1696949 -4.2147179 -4.2571936 -4.2781944 -4.2686663 -4.2371283 -4.201252 -4.1839767 -4.1995578 -4.2445107 -4.2928758 -4.3248224][-4.171226 -4.1790538 -4.2085156 -4.243516 -4.2712026 -4.2890353 -4.2868185 -4.26557 -4.2366505 -4.2129226 -4.2101364 -4.2365713 -4.2817664 -4.3198233 -4.334229][-4.2349749 -4.2423968 -4.25992 -4.2756724 -4.2825222 -4.279448 -4.2646375 -4.2440019 -4.2300506 -4.2259607 -4.2380805 -4.2711086 -4.3127904 -4.3372273 -4.3348222][-4.2947145 -4.2961955 -4.2931857 -4.2819066 -4.2637758 -4.2439461 -4.221755 -4.2060194 -4.2107677 -4.2264466 -4.2515025 -4.287416 -4.3224735 -4.3362484 -4.3230071][-4.31954 -4.3122859 -4.289619 -4.2540092 -4.2139716 -4.1785512 -4.15039 -4.1424518 -4.166194 -4.2032928 -4.23803 -4.2756996 -4.3057146 -4.3138361 -4.29804][-4.3177671 -4.2950649 -4.2502446 -4.1917496 -4.1319857 -4.0833025 -4.0539885 -4.0651379 -4.1138787 -4.1720152 -4.2180624 -4.2587996 -4.2872243 -4.2930207 -4.2777386][-4.3122625 -4.2775879 -4.2150793 -4.1348872 -4.0536785 -3.9957349 -3.9685097 -3.9970191 -4.0657525 -4.1378326 -4.1926284 -4.2376785 -4.2692733 -4.27795 -4.2686243][-4.3129325 -4.278595 -4.2149897 -4.1310658 -4.04325 -3.9849186 -3.9620252 -3.9962597 -4.0666142 -4.1345906 -4.1870961 -4.2302423 -4.2595544 -4.270586 -4.268939][-4.3121495 -4.2845354 -4.2343879 -4.1697617 -4.1009092 -4.0548792 -4.0413465 -4.0738893 -4.1305714 -4.1797895 -4.2161813 -4.2431083 -4.2606311 -4.2690392 -4.2719588][-4.3150511 -4.2948189 -4.2625017 -4.2214694 -4.1781273 -4.1477027 -4.1421909 -4.166729 -4.201529 -4.2261219 -4.2337384 -4.23793 -4.2438192 -4.2504454 -4.2620134][-4.3223472 -4.311182 -4.2938294 -4.2692671 -4.242517 -4.2225733 -4.2165031 -4.2235942 -4.2316132 -4.2286291 -4.2098374 -4.1965208 -4.1983118 -4.21281 -4.23934][-4.3293214 -4.3276315 -4.3227053 -4.3116422 -4.2963595 -4.28011 -4.2635794 -4.2455792 -4.2189255 -4.1820908 -4.1376524 -4.1111903 -4.11811 -4.1535 -4.2016153][-4.3318186 -4.333786 -4.3345056 -4.3293376 -4.3187814 -4.3034935 -4.2792783 -4.2405491 -4.1779947 -4.1019344 -4.0321903 -3.999404 -4.020647 -4.0848165 -4.1581488][-4.3318963 -4.3345947 -4.3362088 -4.3334751 -4.3253741 -4.3115406 -4.281652 -4.2276611 -4.1407971 -4.0390449 -3.9546263 -3.9194021 -3.9602456 -4.0470066 -4.1343575][-4.3294582 -4.3318515 -4.3333917 -4.3334112 -4.3293595 -4.3173308 -4.2882938 -4.233602 -4.1474257 -4.0491791 -3.9745219 -3.9524734 -4.0037031 -4.0885282 -4.1633191]]...]
INFO - root - 2017-12-07 21:55:57.060697: step 25710, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.716 sec/batch; 66h:58m:37s remains)
INFO - root - 2017-12-07 21:56:13.113623: step 25720, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.594 sec/batch; 62h:13m:08s remains)
INFO - root - 2017-12-07 21:56:29.381502: step 25730, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.608 sec/batch; 62h:46m:31s remains)
INFO - root - 2017-12-07 21:56:45.587114: step 25740, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.634 sec/batch; 63h:46m:25s remains)
INFO - root - 2017-12-07 21:57:01.853512: step 25750, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 1.559 sec/batch; 60h:49m:40s remains)
INFO - root - 2017-12-07 21:57:18.136791: step 25760, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.670 sec/batch; 65h:09m:17s remains)
INFO - root - 2017-12-07 21:57:34.357555: step 25770, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 1.549 sec/batch; 60h:27m:03s remains)
INFO - root - 2017-12-07 21:57:50.427741: step 25780, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.689 sec/batch; 65h:54m:31s remains)
INFO - root - 2017-12-07 21:58:06.621975: step 25790, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.540 sec/batch; 60h:05m:28s remains)
INFO - root - 2017-12-07 21:58:22.905361: step 25800, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.659 sec/batch; 64h:43m:05s remains)
2017-12-07 21:58:24.243454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2497749 -4.234715 -4.2344117 -4.2457385 -4.2591157 -4.259438 -4.2553658 -4.2586231 -4.2739954 -4.2920847 -4.302372 -4.3020191 -4.2965531 -4.2855263 -4.2742085][-4.268084 -4.2557874 -4.2535334 -4.2616043 -4.2683969 -4.2638431 -4.2614818 -4.270833 -4.2941742 -4.3163266 -4.3276658 -4.3272891 -4.3213849 -4.3135405 -4.3072915][-4.2704811 -4.2617359 -4.2602544 -4.2614884 -4.2610011 -4.2485585 -4.2422209 -4.2586861 -4.2940803 -4.3244886 -4.339437 -4.3411884 -4.3380275 -4.3327589 -4.33275][-4.2752185 -4.2658844 -4.2575717 -4.2426038 -4.2279143 -4.1976118 -4.1736531 -4.1909819 -4.2397952 -4.2845397 -4.3124728 -4.3245749 -4.3325028 -4.3374195 -4.3457422][-4.2622008 -4.2506166 -4.2314887 -4.1947107 -4.1595116 -4.1037478 -4.04781 -4.0617 -4.1327281 -4.2030125 -4.2553248 -4.2868948 -4.3099303 -4.32788 -4.3462029][-4.2369471 -4.2237668 -4.1934333 -4.1352692 -4.0736918 -3.9838266 -3.8844311 -3.8929203 -3.9974272 -4.1052713 -4.1873164 -4.2401934 -4.2776275 -4.3035069 -4.3274765][-4.200304 -4.1855841 -4.1457419 -4.0780153 -3.9960089 -3.8750043 -3.7344327 -3.7423959 -3.8820641 -4.0202527 -4.1276793 -4.1989985 -4.2452636 -4.2713461 -4.2924113][-4.1628418 -4.157094 -4.1265879 -4.0725231 -3.9987488 -3.8896303 -3.7622275 -3.7634852 -3.8884819 -4.0134711 -4.1104536 -4.1764736 -4.2174864 -4.2341566 -4.2442307][-4.1443205 -4.1529479 -4.1438851 -4.1173682 -4.0722485 -4.0009074 -3.9179597 -3.9094844 -3.9851339 -4.0692925 -4.1367702 -4.1794639 -4.2008228 -4.2028818 -4.2002258][-4.1631703 -4.1818037 -4.186368 -4.1794991 -4.1554461 -4.1140981 -4.0590787 -4.0377741 -4.0670137 -4.1136217 -4.1583233 -4.1846371 -4.1915483 -4.1827927 -4.1698322][-4.1847162 -4.2085571 -4.2175846 -4.2148914 -4.199297 -4.1688375 -4.1272049 -4.0977793 -4.1001554 -4.1275773 -4.1627831 -4.1842003 -4.1857038 -4.1740217 -4.1569872][-4.1973796 -4.2216134 -4.2308021 -4.2266345 -4.2117524 -4.1824861 -4.1479878 -4.1164689 -4.1089034 -4.1312027 -4.1646585 -4.185 -4.1886649 -4.1819582 -4.1689043][-4.2066875 -4.2303834 -4.2415175 -4.2394619 -4.2265162 -4.2031426 -4.1772985 -4.1511588 -4.1407533 -4.1574988 -4.1829205 -4.198236 -4.2027135 -4.2004781 -4.1937342][-4.2200522 -4.2393394 -4.2487025 -4.2469797 -4.2404284 -4.2284422 -4.2130723 -4.196557 -4.1874042 -4.1970129 -4.21477 -4.2255845 -4.229712 -4.2285876 -4.2241979][-4.2382059 -4.2469964 -4.25047 -4.2459569 -4.2455835 -4.24523 -4.2405596 -4.2319932 -4.2281871 -4.2372012 -4.2516069 -4.2620182 -4.2678127 -4.2679996 -4.2644529]]...]
INFO - root - 2017-12-07 21:58:40.444884: step 25810, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.650 sec/batch; 64h:22m:25s remains)
INFO - root - 2017-12-07 21:58:56.434674: step 25820, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 1.718 sec/batch; 67h:00m:00s remains)
INFO - root - 2017-12-07 21:59:12.560375: step 25830, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.585 sec/batch; 61h:49m:14s remains)
INFO - root - 2017-12-07 21:59:28.756456: step 25840, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.619 sec/batch; 63h:09m:33s remains)
INFO - root - 2017-12-07 21:59:44.919225: step 25850, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.615 sec/batch; 62h:59m:20s remains)
INFO - root - 2017-12-07 22:00:01.340251: step 25860, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.714 sec/batch; 66h:50m:13s remains)
INFO - root - 2017-12-07 22:00:17.537981: step 25870, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 1.559 sec/batch; 60h:48m:30s remains)
INFO - root - 2017-12-07 22:00:33.901953: step 25880, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.605 sec/batch; 62h:35m:56s remains)
INFO - root - 2017-12-07 22:00:50.245558: step 25890, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.679 sec/batch; 65h:27m:05s remains)
INFO - root - 2017-12-07 22:01:06.447305: step 25900, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.538 sec/batch; 59h:56m:37s remains)
2017-12-07 22:01:07.775005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2106524 -4.229455 -4.2499247 -4.2734604 -4.2936831 -4.3049479 -4.3110547 -4.3154345 -4.3214364 -4.3275228 -4.3326406 -4.3421865 -4.3446412 -4.3246493 -4.2799969][-4.2844772 -4.2860546 -4.28192 -4.2829361 -4.2852488 -4.2829819 -4.2812853 -4.2878327 -4.3034306 -4.3139958 -4.3169961 -4.3249216 -4.3280663 -4.3109331 -4.2770061][-4.308476 -4.2884092 -4.2632833 -4.248826 -4.2378273 -4.2312832 -4.224514 -4.2320127 -4.2607136 -4.2817473 -4.2850533 -4.2899628 -4.29746 -4.2916045 -4.2762742][-4.2913494 -4.247786 -4.2054052 -4.1780543 -4.15826 -4.1487565 -4.1346407 -4.1446671 -4.1988478 -4.2402453 -4.2491055 -4.2535329 -4.2713122 -4.2795558 -4.2757053][-4.2707191 -4.206418 -4.1408272 -4.0933418 -4.062386 -4.040782 -4.0110922 -4.0258374 -4.1141934 -4.1870284 -4.2130737 -4.2264142 -4.2529836 -4.2675157 -4.2636495][-4.267971 -4.195837 -4.1045108 -4.0211506 -3.9620674 -3.9056745 -3.8433685 -3.8653464 -3.9977491 -4.1130691 -4.1686068 -4.1979079 -4.2290072 -4.2477369 -4.242569][-4.27931 -4.2224016 -4.1316009 -4.0277796 -3.9287271 -3.8079371 -3.6769729 -3.6868324 -3.8618402 -4.0238619 -4.1149712 -4.1665573 -4.2051492 -4.2279711 -4.2223454][-4.2993765 -4.2627759 -4.1986246 -4.1060119 -3.9945705 -3.8345704 -3.643975 -3.6058788 -3.7715456 -3.9542041 -4.0775819 -4.153573 -4.2015896 -4.2294674 -4.2208495][-4.3152051 -4.2953777 -4.2652483 -4.2077684 -4.1258879 -3.9906571 -3.8153663 -3.7416937 -3.8344655 -3.972188 -4.0893435 -4.168644 -4.217607 -4.2452722 -4.2362728][-4.3213305 -4.3113804 -4.3035922 -4.2816286 -4.2358084 -4.1469097 -4.0282612 -3.9681149 -4.0042095 -4.079052 -4.1570354 -4.21051 -4.2415895 -4.2552133 -4.243782][-4.3055582 -4.2976332 -4.3018489 -4.3059216 -4.2937074 -4.2430229 -4.1717134 -4.1353774 -4.155067 -4.1948876 -4.240766 -4.2700791 -4.2790837 -4.2728338 -4.2522025][-4.2760615 -4.2632103 -4.2711449 -4.2886043 -4.3004212 -4.2817492 -4.2427139 -4.224391 -4.2380385 -4.261929 -4.2877345 -4.3000917 -4.2962832 -4.2829342 -4.2590818][-4.2408462 -4.2227044 -4.230113 -4.250351 -4.272953 -4.2729988 -4.2542396 -4.2510633 -4.2689328 -4.2897029 -4.3051491 -4.3033128 -4.2900786 -4.2779841 -4.258707][-4.2076316 -4.1852207 -4.1910353 -4.215488 -4.2437477 -4.2496676 -4.2373009 -4.238699 -4.2570705 -4.2769041 -4.2888002 -4.2828975 -4.2724943 -4.2693515 -4.25827][-4.1905379 -4.1659007 -4.1704531 -4.1959605 -4.2224226 -4.2263947 -4.2175493 -4.221396 -4.2371311 -4.2526913 -4.2586379 -4.2545314 -4.2568393 -4.2668276 -4.2603192]]...]
INFO - root - 2017-12-07 22:01:23.880758: step 25910, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.590 sec/batch; 61h:58m:16s remains)
INFO - root - 2017-12-07 22:01:40.145434: step 25920, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.677 sec/batch; 65h:22m:52s remains)
INFO - root - 2017-12-07 22:01:56.240471: step 25930, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.583 sec/batch; 61h:41m:31s remains)
INFO - root - 2017-12-07 22:02:12.522618: step 25940, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.722 sec/batch; 67h:06m:03s remains)
INFO - root - 2017-12-07 22:02:28.884003: step 25950, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.565 sec/batch; 60h:58m:27s remains)
INFO - root - 2017-12-07 22:02:45.139728: step 25960, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.684 sec/batch; 65h:38m:06s remains)
INFO - root - 2017-12-07 22:03:01.247586: step 25970, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.568 sec/batch; 61h:05m:52s remains)
INFO - root - 2017-12-07 22:03:17.421564: step 25980, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.665 sec/batch; 64h:53m:05s remains)
INFO - root - 2017-12-07 22:03:33.614905: step 25990, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.604 sec/batch; 62h:29m:46s remains)
INFO - root - 2017-12-07 22:03:49.925448: step 26000, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.621 sec/batch; 63h:08m:14s remains)
2017-12-07 22:03:51.305855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1555443 -4.1626086 -4.1793108 -4.1933603 -4.1999841 -4.2026925 -4.2091079 -4.2213039 -4.2329679 -4.2420464 -4.2502527 -4.254005 -4.246521 -4.2338238 -4.228478][-4.125494 -4.1269917 -4.1465497 -4.1701045 -4.1884913 -4.2001767 -4.2151327 -4.2374187 -4.2565536 -4.2666163 -4.2704544 -4.2624884 -4.2413158 -4.2196393 -4.210052][-4.1215739 -4.1293569 -4.1554146 -4.1841173 -4.2042327 -4.2144 -4.2273579 -4.2489724 -4.2676497 -4.2754989 -4.2755966 -4.2605891 -4.2317924 -4.2069573 -4.1978245][-4.1579785 -4.1724763 -4.195313 -4.2154212 -4.2260227 -4.2264714 -4.2300386 -4.24205 -4.2531395 -4.2565804 -4.2551217 -4.2400827 -4.2137895 -4.1951861 -4.1919446][-4.196454 -4.2075009 -4.2181182 -4.2247477 -4.226469 -4.2186193 -4.2106309 -4.2091985 -4.211165 -4.21307 -4.2173376 -4.2133675 -4.2006621 -4.1932211 -4.1937003][-4.2062821 -4.2008572 -4.190289 -4.1840153 -4.18132 -4.1682396 -4.1473837 -4.1324005 -4.1272421 -4.1336427 -4.155858 -4.176085 -4.1843066 -4.1897745 -4.1937866][-4.1924038 -4.1646595 -4.1341124 -4.1181097 -4.1126113 -4.0944815 -4.0619707 -4.0333657 -4.0208282 -4.0379457 -4.0855703 -4.1354909 -4.1670737 -4.1853113 -4.1922321][-4.1855111 -4.1414714 -4.1013975 -4.0851507 -4.0824795 -4.0636539 -4.0231915 -3.9833167 -3.9655666 -3.9963746 -4.0656538 -4.134285 -4.1781297 -4.2021618 -4.2078247][-4.2015743 -4.1574531 -4.1221156 -4.1139169 -4.1169391 -4.1028471 -4.0663757 -4.0309653 -4.0187092 -4.0528622 -4.1187334 -4.1803308 -4.216536 -4.2353983 -4.2379913][-4.2246518 -4.1926279 -4.16979 -4.1709232 -4.1815343 -4.1778159 -4.15702 -4.1372852 -4.1307125 -4.1551638 -4.200284 -4.23954 -4.2589827 -4.2672153 -4.2645612][-4.2502327 -4.2304826 -4.217804 -4.2251787 -4.2416291 -4.2480249 -4.2420511 -4.233139 -4.2284656 -4.2409267 -4.2650867 -4.2859154 -4.2932048 -4.2920518 -4.2838755][-4.2755733 -4.2635283 -4.2569294 -4.2666721 -4.2829361 -4.2927432 -4.2931356 -4.2883887 -4.2845769 -4.2880282 -4.2995944 -4.3117785 -4.3146329 -4.3080544 -4.2953691][-4.2891054 -4.2775159 -4.2707663 -4.2775049 -4.2903972 -4.3003383 -4.3023205 -4.2960691 -4.2906713 -4.291935 -4.302474 -4.3166494 -4.3219953 -4.3148012 -4.3016706][-4.2917457 -4.2798543 -4.2701044 -4.2712903 -4.2800775 -4.2885437 -4.2920361 -4.2870712 -4.2837262 -4.2882876 -4.3022451 -4.3192921 -4.3265572 -4.3179555 -4.3027592][-4.284668 -4.2744026 -4.263442 -4.2609634 -4.26728 -4.278223 -4.2868714 -4.2869964 -4.287137 -4.2939405 -4.3073378 -4.3222532 -4.3256426 -4.310792 -4.28953]]...]
INFO - root - 2017-12-07 22:04:07.418686: step 26010, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 1.699 sec/batch; 66h:10m:02s remains)
INFO - root - 2017-12-07 22:04:23.444450: step 26020, loss = 2.06, batch loss = 2.01 (10.4 examples/sec; 1.537 sec/batch; 59h:51m:12s remains)
INFO - root - 2017-12-07 22:04:39.479696: step 26030, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.625 sec/batch; 63h:17m:31s remains)
INFO - root - 2017-12-07 22:04:55.674997: step 26040, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.618 sec/batch; 63h:00m:53s remains)
INFO - root - 2017-12-07 22:05:11.966352: step 26050, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.616 sec/batch; 62h:55m:55s remains)
INFO - root - 2017-12-07 22:05:28.173925: step 26060, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.566 sec/batch; 60h:59m:01s remains)
INFO - root - 2017-12-07 22:05:44.507769: step 26070, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 1.620 sec/batch; 63h:04m:02s remains)
INFO - root - 2017-12-07 22:06:00.570541: step 26080, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.594 sec/batch; 62h:03m:49s remains)
INFO - root - 2017-12-07 22:06:16.595836: step 26090, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.577 sec/batch; 61h:23m:34s remains)
INFO - root - 2017-12-07 22:06:32.860155: step 26100, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.582 sec/batch; 61h:34m:43s remains)
2017-12-07 22:06:34.200274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2598839 -4.2621074 -4.2675705 -4.2691593 -4.2712154 -4.2727728 -4.2661734 -4.261085 -4.264626 -4.2744403 -4.2856784 -4.2902818 -4.2816277 -4.2584734 -4.2341437][-4.2567525 -4.2550159 -4.2593832 -4.2603588 -4.2655568 -4.2684507 -4.2619314 -4.2529221 -4.2505722 -4.2597647 -4.2752352 -4.2833362 -4.2781625 -4.2569623 -4.2327943][-4.2503619 -4.2468157 -4.2482905 -4.2486424 -4.2551761 -4.2575788 -4.2508249 -4.2351847 -4.2246242 -4.23475 -4.25306 -4.2644105 -4.266469 -4.2529464 -4.2309132][-4.240078 -4.2389274 -4.2396936 -4.2390871 -4.2430296 -4.2369967 -4.2238622 -4.2025194 -4.1871758 -4.2013335 -4.225174 -4.2410855 -4.2507648 -4.2501826 -4.2349114][-4.2203431 -4.2228107 -4.2223396 -4.2201738 -4.2241826 -4.2130241 -4.1915803 -4.1591587 -4.1367979 -4.1575642 -4.1902933 -4.2147484 -4.2306428 -4.2408242 -4.2353268][-4.1887217 -4.1919947 -4.1894178 -4.186265 -4.1932139 -4.183722 -4.1503382 -4.0955873 -4.0617576 -4.0920272 -4.1371946 -4.1738558 -4.2002072 -4.2186403 -4.2214231][-4.154089 -4.153944 -4.1508112 -4.1490774 -4.1549268 -4.1436391 -4.0941463 -4.0128326 -3.9720349 -4.0157833 -4.0743728 -4.1199012 -4.1539793 -4.1769333 -4.1842093][-4.1200385 -4.1124072 -4.10958 -4.1097221 -4.1118989 -4.09152 -4.0235295 -3.9197285 -3.875649 -3.9332225 -4.006 -4.0608549 -4.1006355 -4.1282463 -4.1399174][-4.1093268 -4.0920529 -4.0838251 -4.0824451 -4.0804882 -4.0513992 -3.97221 -3.8644407 -3.8276253 -3.8972168 -3.9792926 -4.0405555 -4.0812521 -4.1098495 -4.127985][-4.1233454 -4.102231 -4.0904813 -4.0850964 -4.0762396 -4.043385 -3.9722986 -3.8921094 -3.8790424 -3.9467981 -4.019 -4.0705142 -4.099968 -4.1236944 -4.1446829][-4.1449218 -4.1278162 -4.1154232 -4.1047678 -4.0898714 -4.0587873 -4.0072255 -3.9620571 -3.9654715 -4.0172529 -4.0681071 -4.1016078 -4.1176839 -4.1352582 -4.1545711][-4.1553755 -4.1427917 -4.1323447 -4.12047 -4.1047773 -4.0795503 -4.0476637 -4.028203 -4.0363164 -4.0681705 -4.0988784 -4.1180582 -4.1261797 -4.1387267 -4.1548324][-4.1595278 -4.1491017 -4.1388264 -4.1278753 -4.1153889 -4.0983033 -4.0815916 -4.0761981 -4.0830684 -4.0994382 -4.1158695 -4.1251869 -4.1296849 -4.1387134 -4.1504183][-4.1460161 -4.1372933 -4.128799 -4.1213803 -4.113997 -4.103549 -4.0950103 -4.0940022 -4.0974584 -4.104991 -4.1136775 -4.1189432 -4.1228342 -4.1300721 -4.1376619][-4.1241722 -4.1177492 -4.1123047 -4.1081653 -4.1053739 -4.0993481 -4.0946078 -4.0943718 -4.09631 -4.101006 -4.1081452 -4.1143274 -4.1188264 -4.1248813 -4.1295972]]...]
INFO - root - 2017-12-07 22:06:50.535549: step 26110, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 62h:21m:23s remains)
INFO - root - 2017-12-07 22:07:06.671796: step 26120, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.652 sec/batch; 64h:17m:35s remains)
INFO - root - 2017-12-07 22:07:23.022250: step 26130, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 63h:26m:39s remains)
INFO - root - 2017-12-07 22:07:39.083815: step 26140, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.628 sec/batch; 63h:21m:20s remains)
INFO - root - 2017-12-07 22:07:55.268405: step 26150, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 1.584 sec/batch; 61h:39m:10s remains)
INFO - root - 2017-12-07 22:08:11.586442: step 26160, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.681 sec/batch; 65h:25m:33s remains)
INFO - root - 2017-12-07 22:08:27.823869: step 26170, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.581 sec/batch; 61h:31m:11s remains)
INFO - root - 2017-12-07 22:08:44.107481: step 26180, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.663 sec/batch; 64h:43m:25s remains)
INFO - root - 2017-12-07 22:09:00.418811: step 26190, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 63h:25m:20s remains)
INFO - root - 2017-12-07 22:09:16.515660: step 26200, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.574 sec/batch; 61h:14m:40s remains)
2017-12-07 22:09:17.839388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3390832 -4.3376775 -4.3353758 -4.3291087 -4.322309 -4.3183813 -4.3197422 -4.325542 -4.3339915 -4.3410511 -4.3410664 -4.3360825 -4.3264775 -4.3120828 -4.2974505][-4.3428464 -4.3429146 -4.3415213 -4.3328214 -4.3201289 -4.30766 -4.3011823 -4.3059316 -4.3176718 -4.3292041 -4.3349843 -4.3346982 -4.32875 -4.3169966 -4.3034739][-4.3444352 -4.3464031 -4.3445058 -4.3310862 -4.3090878 -4.2845287 -4.2668085 -4.2695012 -4.2880425 -4.3085332 -4.3244162 -4.3323917 -4.3302374 -4.3200107 -4.3057804][-4.3420205 -4.3445253 -4.3424606 -4.3246155 -4.2921987 -4.253139 -4.2197084 -4.2156782 -4.2426062 -4.2754173 -4.3044562 -4.3238258 -4.32703 -4.3198009 -4.30612][-4.3354244 -4.337327 -4.3354678 -4.3133554 -4.2701359 -4.2162843 -4.1651597 -4.1537104 -4.1909618 -4.2382512 -4.2797694 -4.30926 -4.3187885 -4.3148227 -4.3036509][-4.328835 -4.33129 -4.3289452 -4.3003974 -4.24332 -4.1754723 -4.1074252 -4.089685 -4.1395783 -4.2030964 -4.2554126 -4.2930212 -4.3083749 -4.3072248 -4.2990179][-4.3219743 -4.3250008 -4.3208 -4.2852454 -4.2162871 -4.1358843 -4.0521855 -4.0211549 -4.0756254 -4.1598568 -4.2284656 -4.2738266 -4.2950273 -4.2990704 -4.2945838][-4.3165045 -4.3171711 -4.3093758 -4.2715788 -4.198462 -4.1092882 -4.00991 -3.9560461 -4.0047584 -4.1079054 -4.1954913 -4.250423 -4.2797327 -4.2916121 -4.2931137][-4.3112226 -4.3117876 -4.3050365 -4.2726893 -4.2043996 -4.11384 -4.0021882 -3.9177635 -3.9465628 -4.0599122 -4.1631083 -4.2242522 -4.2599325 -4.2804432 -4.2905149][-4.3058228 -4.3083534 -4.3072748 -4.2865596 -4.2332139 -4.1535211 -4.0488791 -3.9497311 -3.9471955 -4.0440826 -4.1462097 -4.2065468 -4.2391214 -4.2643743 -4.2832937][-4.2981315 -4.3033528 -4.3080649 -4.3005333 -4.2679586 -4.2113814 -4.1275864 -4.034121 -3.998472 -4.0557327 -4.141942 -4.1965828 -4.2233143 -4.2489705 -4.2748351][-4.2865524 -4.2930465 -4.3013396 -4.3018942 -4.28476 -4.2511778 -4.19088 -4.1101117 -4.0537 -4.073235 -4.1392078 -4.1892786 -4.2143173 -4.2397785 -4.2683][-4.2730751 -4.2793341 -4.2894783 -4.2954059 -4.2899976 -4.2731061 -4.2341256 -4.1674557 -4.1013856 -4.0914454 -4.1365314 -4.182004 -4.20936 -4.2382197 -4.2677183][-4.2625585 -4.2695446 -4.2814207 -4.28836 -4.2876863 -4.2819462 -4.259912 -4.2081409 -4.1418762 -4.1103611 -4.1351619 -4.1724334 -4.2014465 -4.2343292 -4.2688866][-4.2579308 -4.2651763 -4.2762923 -4.2809563 -4.281271 -4.2806382 -4.2689009 -4.23047 -4.1719117 -4.12964 -4.1368375 -4.1659737 -4.1963978 -4.2342887 -4.2732878]]...]
INFO - root - 2017-12-07 22:09:34.135663: step 26210, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 63h:24m:59s remains)
INFO - root - 2017-12-07 22:09:50.168207: step 26220, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.601 sec/batch; 62h:16m:40s remains)
INFO - root - 2017-12-07 22:10:06.542037: step 26230, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.621 sec/batch; 63h:03m:29s remains)
INFO - root - 2017-12-07 22:10:22.921508: step 26240, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.699 sec/batch; 66h:04m:24s remains)
INFO - root - 2017-12-07 22:10:39.179909: step 26250, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.538 sec/batch; 59h:49m:04s remains)
INFO - root - 2017-12-07 22:10:55.400794: step 26260, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.696 sec/batch; 65h:57m:47s remains)
INFO - root - 2017-12-07 22:11:11.566883: step 26270, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.610 sec/batch; 62h:36m:28s remains)
INFO - root - 2017-12-07 22:11:27.816354: step 26280, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.703 sec/batch; 66h:12m:32s remains)
INFO - root - 2017-12-07 22:11:43.976117: step 26290, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.573 sec/batch; 61h:08m:53s remains)
INFO - root - 2017-12-07 22:12:00.311203: step 26300, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.574 sec/batch; 61h:11m:05s remains)
2017-12-07 22:12:01.604349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2244282 -4.2180853 -4.2008429 -4.1884909 -4.1776443 -4.1649475 -4.1603851 -4.1531558 -4.1366339 -4.1233878 -4.1276965 -4.1431441 -4.14806 -4.1483793 -4.1520672][-4.2344785 -4.2138939 -4.1805253 -4.1558762 -4.1349516 -4.118237 -4.1165748 -4.1180339 -4.1092086 -4.0990052 -4.1058717 -4.1219149 -4.1294165 -4.1348605 -4.1447568][-4.2383118 -4.2040987 -4.1580658 -4.1264768 -4.1036487 -4.089509 -4.0937176 -4.1052279 -4.1060691 -4.1010437 -4.1074271 -4.12043 -4.1309876 -4.1431155 -4.15679][-4.2266083 -4.1845756 -4.1375155 -4.1093903 -4.09429 -4.0878677 -4.0934567 -4.1052451 -4.1089005 -4.1109562 -4.1215596 -4.1353903 -4.1478457 -4.1624351 -4.1743631][-4.1966867 -4.1578336 -4.1232681 -4.1101756 -4.1061435 -4.1040115 -4.1010752 -4.0983977 -4.0936985 -4.098978 -4.1191678 -4.1396179 -4.1549578 -4.1683106 -4.1777334][-4.1688681 -4.1408448 -4.1260195 -4.1286774 -4.1320658 -4.1256437 -4.1067796 -4.0828304 -4.0631042 -4.0672083 -4.0935616 -4.1208048 -4.1402183 -4.1532254 -4.1635513][-4.1569738 -4.1420965 -4.1422682 -4.1515703 -4.1546164 -4.1411119 -4.1079407 -4.0654263 -4.0306849 -4.03111 -4.0566745 -4.0819888 -4.1039639 -4.1238675 -4.1463475][-4.1669273 -4.1592808 -4.16263 -4.16889 -4.1657166 -4.14453 -4.1036272 -4.0533261 -4.0084267 -3.9975767 -4.0112495 -4.0315132 -4.063592 -4.1034651 -4.1484246][-4.1767383 -4.1727843 -4.17515 -4.1771884 -4.1694531 -4.1459303 -4.1070719 -4.060936 -4.0161853 -3.9957542 -4.0000978 -4.0213308 -4.0672774 -4.1262608 -4.1854234][-4.1807451 -4.1799603 -4.18464 -4.189579 -4.1879644 -4.174202 -4.1490092 -4.1165395 -4.0810452 -4.0596466 -4.061996 -4.0854082 -4.1322803 -4.1893969 -4.2403007][-4.2035193 -4.2044926 -4.2102094 -4.2173734 -4.2216668 -4.2194662 -4.2086563 -4.1886473 -4.1647668 -4.1519656 -4.1579614 -4.178544 -4.2122369 -4.2492123 -4.2785125][-4.2442207 -4.2416582 -4.2416539 -4.2433209 -4.2458839 -4.248198 -4.2458858 -4.2354918 -4.2252688 -4.2233691 -4.2302189 -4.2408056 -4.2551322 -4.2709 -4.2831664][-4.2743778 -4.266695 -4.2602253 -4.2564707 -4.256289 -4.2606421 -4.2627678 -4.257143 -4.2521529 -4.2529464 -4.2564092 -4.2549644 -4.2530169 -4.2577357 -4.2641668][-4.283905 -4.2745814 -4.2685213 -4.2656913 -4.26609 -4.270699 -4.2717366 -4.2619548 -4.2510815 -4.2464738 -4.2446418 -4.2354579 -4.2257705 -4.2284427 -4.2373362][-4.2835441 -4.2724667 -4.2673931 -4.2672343 -4.2680783 -4.2697225 -4.2646751 -4.2456651 -4.22336 -4.2101336 -4.20297 -4.192359 -4.1859469 -4.1945624 -4.2119627]]...]
INFO - root - 2017-12-07 22:12:17.756960: step 26310, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 1.739 sec/batch; 67h:36m:50s remains)
INFO - root - 2017-12-07 22:12:33.721985: step 26320, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.562 sec/batch; 60h:42m:47s remains)
INFO - root - 2017-12-07 22:12:50.012861: step 26330, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.626 sec/batch; 63h:12m:45s remains)
INFO - root - 2017-12-07 22:13:06.110563: step 26340, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.575 sec/batch; 61h:12m:02s remains)
INFO - root - 2017-12-07 22:13:22.564367: step 26350, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.593 sec/batch; 61h:53m:59s remains)
INFO - root - 2017-12-07 22:13:38.916174: step 26360, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.683 sec/batch; 65h:24m:16s remains)
INFO - root - 2017-12-07 22:13:54.921526: step 26370, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.583 sec/batch; 61h:30m:15s remains)
INFO - root - 2017-12-07 22:14:11.106485: step 26380, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.590 sec/batch; 61h:47m:24s remains)
INFO - root - 2017-12-07 22:14:27.504571: step 26390, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.626 sec/batch; 63h:11m:06s remains)
INFO - root - 2017-12-07 22:14:43.862525: step 26400, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.610 sec/batch; 62h:32m:38s remains)
2017-12-07 22:14:45.238996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3053846 -4.300241 -4.2958612 -4.2968736 -4.3014631 -4.303647 -4.304606 -4.3003168 -4.2930579 -4.2896338 -4.2894931 -4.2941442 -4.3017135 -4.3133049 -4.3245058][-4.3055325 -4.2992916 -4.2939839 -4.295207 -4.3003087 -4.3043094 -4.3082156 -4.3037724 -4.294591 -4.2892923 -4.287127 -4.2897544 -4.2952833 -4.3054581 -4.3161554][-4.3059649 -4.3026042 -4.297895 -4.2985096 -4.3022389 -4.30567 -4.3074789 -4.301527 -4.2936006 -4.2910385 -4.2897363 -4.2915692 -4.2962766 -4.3038745 -4.3124747][-4.29561 -4.2904587 -4.2836761 -4.2832232 -4.2879338 -4.2915835 -4.2919173 -4.2842064 -4.2807064 -4.2862139 -4.2908063 -4.29325 -4.2949834 -4.2987032 -4.3047891][-4.2634482 -4.2466369 -4.2318492 -4.2263551 -4.226687 -4.2295151 -4.2324681 -4.2241931 -4.22764 -4.2423668 -4.2547841 -4.2621107 -4.2649031 -4.2702274 -4.2810416][-4.2114897 -4.1801329 -4.1589894 -4.1486683 -4.1433792 -4.145555 -4.1496229 -4.1397309 -4.146358 -4.1705704 -4.1907544 -4.2044268 -4.2160029 -4.229507 -4.2483077][-4.146162 -4.1070781 -4.0819917 -4.0674329 -4.0576315 -4.0607185 -4.0639863 -4.0526233 -4.056129 -4.087306 -4.1145973 -4.1372414 -4.1598535 -4.1795359 -4.2088685][-4.096118 -4.0542135 -4.0234551 -3.9991481 -3.9794993 -3.9761138 -3.9702606 -3.9528413 -3.9550612 -3.9886966 -4.0259748 -4.0656891 -4.1062207 -4.1358161 -4.1750264][-4.1019468 -4.0670094 -4.0363154 -4.005806 -3.9780235 -3.9639082 -3.9492447 -3.9305811 -3.9336283 -3.960124 -3.9958713 -4.042335 -4.0906754 -4.1246042 -4.1669941][-4.157372 -4.1422014 -4.1253633 -4.1056032 -4.0828948 -4.065906 -4.0520458 -4.040112 -4.0423169 -4.0541272 -4.07163 -4.10182 -4.1366038 -4.1620145 -4.1970124][-4.2314343 -4.229991 -4.22622 -4.2189727 -4.2052188 -4.1900373 -4.1794863 -4.1741228 -4.1752229 -4.1793795 -4.1855936 -4.1994905 -4.2184734 -4.231236 -4.2510557][-4.2944446 -4.300468 -4.3032842 -4.3012986 -4.2917118 -4.2793689 -4.2722664 -4.2697916 -4.2700505 -4.2722468 -4.2739944 -4.2805104 -4.2884874 -4.2900677 -4.2958908][-4.3248506 -4.3306808 -4.3344316 -4.3342261 -4.3280644 -4.3201385 -4.3163991 -4.3154197 -4.3162079 -4.3175054 -4.3173828 -4.3199873 -4.3225155 -4.3205781 -4.32066][-4.3360481 -4.3401661 -4.3429017 -4.342937 -4.3398261 -4.3355136 -4.333498 -4.3332419 -4.3338742 -4.3338733 -4.3324671 -4.3324375 -4.3332024 -4.3316751 -4.3320704][-4.3438659 -4.3463063 -4.3475356 -4.3478603 -4.3465848 -4.3443956 -4.3429861 -4.3429031 -4.3435855 -4.343091 -4.3415761 -4.340508 -4.3404565 -4.3402433 -4.3417983]]...]
INFO - root - 2017-12-07 22:15:01.247211: step 26410, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.571 sec/batch; 61h:00m:39s remains)
INFO - root - 2017-12-07 22:15:17.543834: step 26420, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.696 sec/batch; 65h:51m:57s remains)
INFO - root - 2017-12-07 22:15:33.782567: step 26430, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.554 sec/batch; 60h:21m:45s remains)
INFO - root - 2017-12-07 22:15:49.971966: step 26440, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 1.763 sec/batch; 68h:27m:46s remains)
INFO - root - 2017-12-07 22:16:06.304398: step 26450, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.621 sec/batch; 62h:55m:56s remains)
INFO - root - 2017-12-07 22:16:22.631633: step 26460, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.615 sec/batch; 62h:43m:11s remains)
INFO - root - 2017-12-07 22:16:38.783639: step 26470, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.605 sec/batch; 62h:19m:20s remains)
INFO - root - 2017-12-07 22:16:55.263520: step 26480, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.566 sec/batch; 60h:48m:21s remains)
INFO - root - 2017-12-07 22:17:11.550639: step 26490, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.700 sec/batch; 66h:00m:51s remains)
INFO - root - 2017-12-07 22:17:27.782281: step 26500, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.633 sec/batch; 63h:22m:53s remains)
2017-12-07 22:17:29.185141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2856636 -4.2939477 -4.3075881 -4.3172822 -4.31744 -4.3081226 -4.2949476 -4.2901421 -4.2938662 -4.2981734 -4.2996278 -4.2907634 -4.277473 -4.2685266 -4.2447515][-4.28617 -4.2961874 -4.3082671 -4.3147669 -4.3110833 -4.2964211 -4.2788615 -4.2734208 -4.2807713 -4.2886739 -4.287765 -4.2751064 -4.255918 -4.2492628 -4.2380633][-4.291678 -4.3006959 -4.3079662 -4.3078494 -4.2926073 -4.2633805 -4.2358871 -4.2250748 -4.2352562 -4.2494688 -4.2518048 -4.2401605 -4.2223778 -4.2262607 -4.2370129][-4.294282 -4.299706 -4.2995772 -4.2891865 -4.2564464 -4.2079325 -4.1657553 -4.1512251 -4.1704931 -4.1974578 -4.2100821 -4.1986651 -4.185009 -4.2036438 -4.2355642][-4.2893443 -4.2863812 -4.2747774 -4.249908 -4.1967506 -4.1252022 -4.0653496 -4.0565038 -4.1009073 -4.149827 -4.1728086 -4.1594634 -4.1481566 -4.1765079 -4.2218485][-4.2772837 -4.2647066 -4.2390952 -4.1926546 -4.11419 -4.0155082 -3.9372346 -3.9446943 -4.02129 -4.0976181 -4.1309462 -4.1185122 -4.1115108 -4.1454797 -4.1967006][-4.2574806 -4.2379761 -4.2019424 -4.1373482 -4.0395927 -3.9291871 -3.8554604 -3.8893659 -3.9823771 -4.0661664 -4.1008983 -4.0892115 -4.0858779 -4.1226764 -4.1743126][-4.2296944 -4.21061 -4.1777105 -4.1176991 -4.0289116 -3.9430041 -3.902195 -3.9425826 -4.0142632 -4.0718083 -4.0945454 -4.0871387 -4.0899754 -4.1253805 -4.1698537][-4.2040663 -4.1949735 -4.1870623 -4.1573114 -4.1019244 -4.0512676 -4.0303364 -4.0502987 -4.0758705 -4.0899096 -4.0973916 -4.1009994 -4.117147 -4.1538215 -4.1890445][-4.1925025 -4.1929665 -4.2115073 -4.2130003 -4.1920476 -4.1672487 -4.1491737 -4.1440907 -4.1251025 -4.100111 -4.0945935 -4.1116796 -4.1477952 -4.19343 -4.2231903][-4.1957359 -4.1954031 -4.2206345 -4.2347641 -4.2326436 -4.2251873 -4.2125316 -4.199863 -4.1627192 -4.1175766 -4.1040506 -4.1285028 -4.1749134 -4.2219119 -4.2472205][-4.2172513 -4.2070026 -4.2254186 -4.2370787 -4.2365851 -4.2358418 -4.231504 -4.2247353 -4.1962285 -4.1548591 -4.1417317 -4.1651831 -4.2055945 -4.2432623 -4.2651405][-4.2306471 -4.21443 -4.2252851 -4.2314811 -4.2349834 -4.2418928 -4.2449222 -4.24593 -4.2321606 -4.2052369 -4.198256 -4.2162743 -4.2433267 -4.2691674 -4.2901917][-4.2278333 -4.2140241 -4.2216334 -4.2270031 -4.2372994 -4.2522335 -4.2613344 -4.2668905 -4.2601709 -4.2427726 -4.2395854 -4.2523742 -4.2690797 -4.2900019 -4.3092966][-4.2466369 -4.2388406 -4.2427177 -4.2463512 -4.2577705 -4.274662 -4.2871709 -4.2954254 -4.291635 -4.2787 -4.2744346 -4.2801833 -4.2919354 -4.3116803 -4.3272791]]...]
INFO - root - 2017-12-07 22:17:45.578348: step 26510, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.620 sec/batch; 62h:53m:36s remains)
INFO - root - 2017-12-07 22:18:02.042115: step 26520, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.711 sec/batch; 66h:24m:43s remains)
INFO - root - 2017-12-07 22:18:18.234768: step 26530, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.589 sec/batch; 61h:40m:59s remains)
INFO - root - 2017-12-07 22:18:34.704083: step 26540, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.715 sec/batch; 66h:34m:25s remains)
INFO - root - 2017-12-07 22:18:50.878675: step 26550, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.695 sec/batch; 65h:47m:02s remains)
INFO - root - 2017-12-07 22:19:07.185731: step 26560, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.596 sec/batch; 61h:54m:47s remains)
INFO - root - 2017-12-07 22:19:23.306031: step 26570, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.588 sec/batch; 61h:35m:56s remains)
INFO - root - 2017-12-07 22:19:39.772994: step 26580, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.580 sec/batch; 61h:18m:21s remains)
INFO - root - 2017-12-07 22:19:56.126480: step 26590, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.686 sec/batch; 65h:23m:42s remains)
INFO - root - 2017-12-07 22:20:12.194728: step 26600, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.535 sec/batch; 59h:32m:33s remains)
2017-12-07 22:20:13.570014: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3480892 -4.3449135 -4.3321714 -4.3180132 -4.305759 -4.30176 -4.3037295 -4.3099279 -4.312583 -4.3078318 -4.3011436 -4.299243 -4.3053474 -4.3153481 -4.3265905][-4.3369923 -4.3283882 -4.3131137 -4.2949405 -4.2790036 -4.2717853 -4.2688336 -4.2729716 -4.2781591 -4.2733231 -4.2651935 -4.2651181 -4.2759438 -4.2905312 -4.3065176][-4.3179555 -4.2994571 -4.2760563 -4.2503376 -4.2281456 -4.2136421 -4.2054935 -4.2089334 -4.2208438 -4.2204542 -4.216784 -4.2234745 -4.2383485 -4.2563586 -4.2786403][-4.2868538 -4.2534051 -4.2162571 -4.1809378 -4.15137 -4.1241655 -4.11029 -4.1132736 -4.1352839 -4.1459646 -4.1575317 -4.1810594 -4.2031155 -4.2250071 -4.2543769][-4.2502475 -4.2019157 -4.1517 -4.1054688 -4.0641913 -4.021018 -4.0001087 -4.0074759 -4.0447168 -4.0737662 -4.1051655 -4.1458712 -4.174994 -4.2012286 -4.2361188][-4.2122397 -4.1519985 -4.0903177 -4.0367785 -3.9878144 -3.9340909 -3.9159188 -3.9359293 -3.9849875 -4.0218353 -4.0639095 -4.1168027 -4.1523156 -4.1833816 -4.2220025][-4.1763005 -4.1056314 -4.0360065 -3.9797196 -3.9205334 -3.8591723 -3.8566024 -3.8940148 -3.9442306 -3.9795365 -4.0231295 -4.0815997 -4.1255922 -4.1609745 -4.203259][-4.1421943 -4.0668573 -4.0032496 -3.9567473 -3.8949335 -3.8327231 -3.8401654 -3.8857789 -3.9361351 -3.9635096 -4.0040388 -4.065526 -4.1140256 -4.15167 -4.1957994][-4.1323981 -4.0667534 -4.0231509 -3.9973912 -3.9501483 -3.894196 -3.9051967 -3.946768 -3.9940631 -4.0143018 -4.0457697 -4.0998273 -4.1435919 -4.1762171 -4.2162004][-4.1516085 -4.107985 -4.0857749 -4.0768504 -4.0481009 -4.0069389 -4.0134978 -4.0461946 -4.0889931 -4.1066608 -4.1295357 -4.1724119 -4.2070074 -4.23169 -4.2583227][-4.1830182 -4.1625309 -4.1586342 -4.1643343 -4.1530566 -4.1284323 -4.1379075 -4.1641188 -4.1932607 -4.2024226 -4.2179689 -4.2478986 -4.2720013 -4.2871518 -4.2996945][-4.2261477 -4.2269669 -4.2372441 -4.2461181 -4.2404847 -4.2312346 -4.244596 -4.2656531 -4.2831926 -4.2870436 -4.2970366 -4.3131146 -4.327126 -4.3320012 -4.331028][-4.2711573 -4.2800922 -4.2916541 -4.2991214 -4.2986217 -4.299614 -4.3126206 -4.3270988 -4.3370891 -4.34002 -4.3474431 -4.3540936 -4.3590288 -4.3579679 -4.3513694][-4.3061776 -4.3144932 -4.3226819 -4.3280916 -4.331255 -4.335639 -4.3431406 -4.3493905 -4.3552165 -4.3601151 -4.3660855 -4.3694763 -4.3699651 -4.3673258 -4.3609424][-4.3315215 -4.3371634 -4.3416429 -4.344131 -4.3462114 -4.3492303 -4.3515091 -4.3532343 -4.3564739 -4.3599448 -4.3630114 -4.3649635 -4.3655968 -4.3643732 -4.3605618]]...]
INFO - root - 2017-12-07 22:20:29.769463: step 26610, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.597 sec/batch; 61h:56m:13s remains)
INFO - root - 2017-12-07 22:20:45.930009: step 26620, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.673 sec/batch; 64h:52m:53s remains)
INFO - root - 2017-12-07 22:21:02.216021: step 26630, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.588 sec/batch; 61h:34m:31s remains)
INFO - root - 2017-12-07 22:21:18.516216: step 26640, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 1.663 sec/batch; 64h:29m:15s remains)
INFO - root - 2017-12-07 22:21:34.856577: step 26650, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.598 sec/batch; 61h:56m:58s remains)
INFO - root - 2017-12-07 22:21:50.854958: step 26660, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.577 sec/batch; 61h:09m:53s remains)
INFO - root - 2017-12-07 22:22:07.220898: step 26670, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 1.682 sec/batch; 65h:13m:02s remains)
INFO - root - 2017-12-07 22:22:23.449215: step 26680, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.585 sec/batch; 61h:27m:30s remains)
INFO - root - 2017-12-07 22:22:39.836849: step 26690, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.583 sec/batch; 61h:23m:03s remains)
INFO - root - 2017-12-07 22:22:56.069409: step 26700, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.677 sec/batch; 65h:00m:24s remains)
2017-12-07 22:22:57.521616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0608058 -4.0583725 -4.0804877 -4.1014671 -4.1107483 -4.1150341 -4.122704 -4.1264596 -4.1237364 -4.1243024 -4.125289 -4.122663 -4.1193933 -4.1120238 -4.1003518][-3.994544 -3.9890208 -4.0153542 -4.041801 -4.0583725 -4.0689888 -4.0848632 -4.0969558 -4.0990477 -4.1016922 -4.1006036 -4.0893388 -4.0726047 -4.0579348 -4.0443954][-4.0318251 -4.0207129 -4.03414 -4.0522733 -4.0700722 -4.084651 -4.1089892 -4.1326323 -4.1378913 -4.1350183 -4.1224775 -4.0941 -4.05828 -4.0340462 -4.0251555][-4.1351118 -4.1249967 -4.118926 -4.1113396 -4.1111455 -4.1138778 -4.1342573 -4.1620641 -4.1749992 -4.1682191 -4.1433587 -4.0957437 -4.0403056 -4.0068655 -4.0049076][-4.2226057 -4.2217078 -4.2056193 -4.17897 -4.1601734 -4.1475549 -4.15054 -4.1665797 -4.1843 -4.1849813 -4.1609569 -4.1062536 -4.0421233 -4.0042615 -4.0080075][-4.2679477 -4.2803273 -4.2651649 -4.2315969 -4.1945639 -4.1633162 -4.148829 -4.155169 -4.1796637 -4.1937547 -4.1826739 -4.1408043 -4.0854259 -4.0533805 -4.0607376][-4.2599869 -4.2909212 -4.2906075 -4.2610331 -4.2135582 -4.1717925 -4.1507668 -4.1527047 -4.1788483 -4.2006388 -4.2042766 -4.1848741 -4.15144 -4.1328864 -4.1423659][-4.2245078 -4.2702045 -4.280117 -4.2545652 -4.2077365 -4.1694727 -4.1520581 -4.1529212 -4.1755357 -4.2066426 -4.2269797 -4.2277479 -4.215024 -4.2111845 -4.2209215][-4.1670313 -4.2107167 -4.222147 -4.1975164 -4.1514173 -4.1194077 -4.11024 -4.1178322 -4.1467738 -4.1978297 -4.2404823 -4.2548919 -4.2538304 -4.2562723 -4.2615795][-4.0967917 -4.1247535 -4.1285062 -4.1025081 -4.0584497 -4.03489 -4.0364413 -4.0549664 -4.0969272 -4.1684475 -4.2332153 -4.2607884 -4.2670412 -4.2704477 -4.2721705][-4.0518994 -4.0564909 -4.0488563 -4.0252309 -3.9924359 -3.978265 -3.9898727 -4.0169892 -4.0685682 -4.1492205 -4.2289014 -4.2702684 -4.284308 -4.2881904 -4.2872729][-4.0718555 -4.0575733 -4.0403948 -4.01753 -3.9991229 -3.9976592 -4.0175252 -4.0480289 -4.0997086 -4.1742458 -4.2492671 -4.2933464 -4.3093181 -4.3133421 -4.3115454][-4.1410146 -4.1272893 -4.1100969 -4.0923343 -4.0835404 -4.0877733 -4.1051111 -4.1284227 -4.1698604 -4.2269721 -4.2813091 -4.3133793 -4.3233037 -4.3238106 -4.3208694][-4.2098341 -4.2060533 -4.1938019 -4.1772113 -4.1693707 -4.1719761 -4.1856866 -4.2013154 -4.22775 -4.262444 -4.2923756 -4.3092675 -4.3098817 -4.3050141 -4.3027854][-4.2468963 -4.2487178 -4.2378116 -4.2214875 -4.2120514 -4.2139959 -4.227066 -4.237987 -4.2515645 -4.2645292 -4.2747145 -4.2789454 -4.2728968 -4.2661853 -4.2672596]]...]
INFO - root - 2017-12-07 22:23:13.647765: step 26710, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.598 sec/batch; 61h:56m:17s remains)
INFO - root - 2017-12-07 22:23:29.945067: step 26720, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.603 sec/batch; 62h:06m:58s remains)
INFO - root - 2017-12-07 22:23:46.168239: step 26730, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 61h:12m:27s remains)
INFO - root - 2017-12-07 22:24:02.256463: step 26740, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.558 sec/batch; 60h:23m:27s remains)
INFO - root - 2017-12-07 22:24:18.525108: step 26750, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.717 sec/batch; 66h:31m:05s remains)
INFO - root - 2017-12-07 22:24:34.856516: step 26760, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.555 sec/batch; 60h:15m:31s remains)
INFO - root - 2017-12-07 22:24:51.098537: step 26770, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.645 sec/batch; 63h:43m:23s remains)
INFO - root - 2017-12-07 22:25:07.368623: step 26780, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.599 sec/batch; 61h:57m:42s remains)
INFO - root - 2017-12-07 22:25:23.642181: step 26790, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.592 sec/batch; 61h:40m:10s remains)
INFO - root - 2017-12-07 22:25:39.780102: step 26800, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.596 sec/batch; 61h:48m:58s remains)
2017-12-07 22:25:41.228424: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.156168 -4.1387172 -4.1283255 -4.1252408 -4.131587 -4.1396532 -4.1426234 -4.14109 -4.1407976 -4.1447611 -4.1519003 -4.1623955 -4.1688051 -4.1701035 -4.1737423][-4.1568394 -4.1354203 -4.1211867 -4.1169477 -4.1214633 -4.1265626 -4.1282835 -4.1280031 -4.1291766 -4.1330628 -4.1390257 -4.1497803 -4.1578913 -4.159687 -4.1644444][-4.1636791 -4.1388521 -4.12 -4.1120625 -4.11265 -4.1134529 -4.1138196 -4.1155887 -4.1203575 -4.1264358 -4.1330895 -4.1444879 -4.1535373 -4.1563582 -4.1628289][-4.163836 -4.1368546 -4.1161008 -4.1066866 -4.1052785 -4.1053867 -4.10679 -4.1104245 -4.1166916 -4.1229258 -4.1296983 -4.1420112 -4.1523471 -4.1574249 -4.1665969][-4.1700487 -4.1444688 -4.125257 -4.1159935 -4.1138878 -4.1129794 -4.1134868 -4.1158519 -4.1191783 -4.121109 -4.1268797 -4.1401095 -4.1527243 -4.1609607 -4.1721153][-4.1632333 -4.1379981 -4.1208324 -4.1130095 -4.1110649 -4.110487 -4.1114459 -4.1130476 -4.1134348 -4.11226 -4.11846 -4.1334839 -4.1495323 -4.1607995 -4.1729188][-4.1630173 -4.1349754 -4.1151609 -4.1035075 -4.0979443 -4.09509 -4.0955715 -4.0974259 -4.0983181 -4.0996513 -4.110126 -4.1275172 -4.1454506 -4.1574492 -4.1678033][-4.1604161 -4.1296229 -4.1057272 -4.0882154 -4.0766521 -4.0698543 -4.0698013 -4.0748405 -4.0821137 -4.0921278 -4.1089678 -4.1277051 -4.1443481 -4.154798 -4.161231][-4.1675282 -4.1404042 -4.1170478 -4.0964193 -4.0799789 -4.0678196 -4.0643554 -4.070477 -4.0826211 -4.0984297 -4.1181245 -4.1359634 -4.1497054 -4.1567764 -4.15991][-4.1693411 -4.1474829 -4.1282835 -4.1111093 -4.0977359 -4.08696 -4.0831642 -4.0882649 -4.0999646 -4.114912 -4.1322103 -4.1467862 -4.1573415 -4.1624303 -4.163868][-4.1716523 -4.1570406 -4.1454735 -4.135591 -4.1286416 -4.1219368 -4.1181951 -4.119091 -4.124589 -4.1320062 -4.142333 -4.1533847 -4.1625919 -4.1667962 -4.1678748][-4.1758308 -4.1671357 -4.1602473 -4.1532006 -4.1486974 -4.1443472 -4.1409183 -4.1399007 -4.1414022 -4.142539 -4.1471629 -4.1566339 -4.1659522 -4.1702476 -4.1713386][-4.1746368 -4.1708107 -4.1675067 -4.1600347 -4.153748 -4.1488433 -4.1451864 -4.1436548 -4.14303 -4.1419992 -4.1453886 -4.155364 -4.1644874 -4.1679668 -4.1690359][-4.1708622 -4.1693282 -4.16719 -4.1582909 -4.1491852 -4.1415629 -4.1362305 -4.1338778 -4.1329126 -4.1327353 -4.138401 -4.1505289 -4.1600919 -4.1640992 -4.1661386][-4.1715021 -4.1733012 -4.174294 -4.1669445 -4.1573277 -4.1484056 -4.1409559 -4.1363144 -4.1345668 -4.1356997 -4.1427345 -4.154613 -4.1630955 -4.1667051 -4.1685042]]...]
INFO - root - 2017-12-07 22:25:57.472635: step 26810, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.611 sec/batch; 62h:24m:17s remains)
INFO - root - 2017-12-07 22:26:13.661353: step 26820, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.705 sec/batch; 66h:01m:52s remains)
INFO - root - 2017-12-07 22:26:29.986674: step 26830, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.557 sec/batch; 60h:18m:11s remains)
INFO - root - 2017-12-07 22:26:46.214826: step 26840, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.562 sec/batch; 60h:28m:17s remains)
INFO - root - 2017-12-07 22:27:02.481283: step 26850, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.643 sec/batch; 63h:37m:37s remains)
INFO - root - 2017-12-07 22:27:18.716290: step 26860, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.628 sec/batch; 63h:02m:38s remains)
INFO - root - 2017-12-07 22:27:35.084512: step 26870, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.701 sec/batch; 65h:50m:47s remains)
INFO - root - 2017-12-07 22:27:51.280691: step 26880, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.581 sec/batch; 61h:12m:25s remains)
INFO - root - 2017-12-07 22:28:07.546220: step 26890, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.601 sec/batch; 61h:58m:36s remains)
INFO - root - 2017-12-07 22:28:23.836655: step 26900, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.611 sec/batch; 62h:20m:30s remains)
2017-12-07 22:28:25.249689: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2439294 -4.2392416 -4.2380314 -4.2401609 -4.243691 -4.2565522 -4.2709293 -4.27166 -4.2603493 -4.2518649 -4.2479677 -4.2462964 -4.2471905 -4.249033 -4.2546568][-4.2331219 -4.2273517 -4.2213492 -4.2197123 -4.2256637 -4.2449059 -4.2644172 -4.2677088 -4.2547588 -4.2408566 -4.2269268 -4.2167664 -4.2199082 -4.2299323 -4.2425566][-4.2311187 -4.2200761 -4.2015166 -4.188179 -4.1910605 -4.2169342 -4.246871 -4.25768 -4.2515783 -4.2387938 -4.2171221 -4.1949735 -4.19468 -4.21409 -4.2349906][-4.22921 -4.2162952 -4.1872182 -4.1579819 -4.1513329 -4.1766458 -4.2153287 -4.2370648 -4.2465515 -4.2421184 -4.2162824 -4.1835175 -4.1762514 -4.20124 -4.2296338][-4.2207994 -4.2145605 -4.1867929 -4.1473274 -4.1220307 -4.1268368 -4.1622715 -4.2004724 -4.2343812 -4.2448664 -4.221324 -4.1837974 -4.1693735 -4.1924276 -4.224566][-4.2000232 -4.2043314 -4.1877742 -4.1467376 -4.0981588 -4.062839 -4.0743527 -4.12817 -4.1993823 -4.2403483 -4.23155 -4.2004514 -4.1850033 -4.1994715 -4.224956][-4.1701126 -4.1859026 -4.1848688 -4.1492796 -4.0810547 -3.9955966 -3.9613426 -4.020884 -4.1329012 -4.2129703 -4.2307625 -4.2134566 -4.20141 -4.2081428 -4.2227058][-4.1267362 -4.1526165 -4.16822 -4.1458249 -4.0726376 -3.9526191 -3.8610463 -3.9141645 -4.0577674 -4.167418 -4.2122765 -4.2113395 -4.20426 -4.2062225 -4.2133217][-4.077538 -4.1092978 -4.1379638 -4.1360164 -4.0837417 -3.9746103 -3.8681841 -3.8912585 -4.0167856 -4.1236 -4.1823254 -4.1998067 -4.1986275 -4.1982517 -4.2017026][-4.0577035 -4.0888991 -4.1220212 -4.1360393 -4.1135325 -4.0452595 -3.9685895 -3.9616909 -4.0262337 -4.0963364 -4.1530666 -4.1880465 -4.1991215 -4.20128 -4.2012205][-4.0788779 -4.1014218 -4.129374 -4.1511631 -4.1490397 -4.1141305 -4.0663495 -4.04457 -4.0569935 -4.0865226 -4.1301188 -4.1750913 -4.201498 -4.21289 -4.213551][-4.11301 -4.1216578 -4.1407728 -4.1647415 -4.1751342 -4.1613417 -4.1321754 -4.1055436 -4.0883904 -4.0854015 -4.1088424 -4.1522794 -4.1917281 -4.2169294 -4.227046][-4.136157 -4.1333895 -4.147018 -4.1707721 -4.186801 -4.1869388 -4.1715279 -4.1490026 -4.1218982 -4.1013536 -4.1013927 -4.1273923 -4.1657777 -4.2001815 -4.223208][-4.1667242 -4.1573892 -4.1662321 -4.1875696 -4.2034431 -4.2067285 -4.199337 -4.1849709 -4.1642594 -4.1454983 -4.1363096 -4.144505 -4.1693983 -4.198864 -4.2262053][-4.2093458 -4.201252 -4.2075381 -4.2216187 -4.231308 -4.232851 -4.2292128 -4.2206621 -4.2089806 -4.1988297 -4.1934547 -4.1973581 -4.2116766 -4.2299705 -4.2508225]]...]
INFO - root - 2017-12-07 22:28:41.182101: step 26910, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.536 sec/batch; 59h:27m:06s remains)
INFO - root - 2017-12-07 22:28:57.437855: step 26920, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.684 sec/batch; 65h:11m:28s remains)
INFO - root - 2017-12-07 22:29:13.735174: step 26930, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.637 sec/batch; 63h:20m:45s remains)
INFO - root - 2017-12-07 22:29:29.857665: step 26940, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.606 sec/batch; 62h:08m:20s remains)
INFO - root - 2017-12-07 22:29:46.034315: step 26950, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.713 sec/batch; 66h:17m:18s remains)
INFO - root - 2017-12-07 22:30:02.311503: step 26960, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.561 sec/batch; 60h:22m:53s remains)
INFO - root - 2017-12-07 22:30:18.500991: step 26970, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.553 sec/batch; 60h:05m:45s remains)
INFO - root - 2017-12-07 22:30:34.745881: step 26980, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.665 sec/batch; 64h:25m:35s remains)
INFO - root - 2017-12-07 22:30:50.983536: step 26990, loss = 2.11, batch loss = 2.05 (10.2 examples/sec; 1.568 sec/batch; 60h:40m:07s remains)
INFO - root - 2017-12-07 22:31:07.237171: step 27000, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.640 sec/batch; 63h:25m:02s remains)
2017-12-07 22:31:08.607639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1079926 -4.1050868 -4.1143045 -4.1296315 -4.1433268 -4.1585555 -4.1652989 -4.1609678 -4.1375818 -4.1048307 -4.0723028 -4.0500989 -4.0534773 -4.0756092 -4.0970488][-4.1047959 -4.1042686 -4.1132374 -4.1317925 -4.1469364 -4.1594911 -4.1599016 -4.1521235 -4.1308808 -4.1005197 -4.0694637 -4.048727 -4.050416 -4.063097 -4.0729408][-4.1160145 -4.1125789 -4.1165271 -4.133287 -4.145009 -4.1476355 -4.1404824 -4.1368885 -4.128397 -4.1067147 -4.0785336 -4.0584035 -4.0579858 -4.0631752 -4.0629897][-4.1286006 -4.1243024 -4.1247983 -4.1351919 -4.13567 -4.1209273 -4.1043849 -4.1086597 -4.1176195 -4.1072893 -4.0856924 -4.0720091 -4.0760293 -4.0794067 -4.0719686][-4.1381483 -4.1338239 -4.1295471 -4.1297131 -4.1150508 -4.0825191 -4.0582108 -4.0696316 -4.0935783 -4.0960779 -4.0887733 -4.0916491 -4.1056185 -4.1115971 -4.1033998][-4.1459136 -4.136755 -4.1236639 -4.1122475 -4.0825009 -4.0381503 -4.0114741 -4.0316472 -4.0682149 -4.0852866 -4.0938768 -4.111752 -4.1320734 -4.1396413 -4.1349816][-4.1492128 -4.1371427 -4.1148419 -4.0939856 -4.0541615 -4.0021744 -3.9755893 -4.0041037 -4.0512667 -4.0812049 -4.1014023 -4.1239266 -4.1435294 -4.152504 -4.1549897][-4.1417866 -4.131464 -4.1088705 -4.0864367 -4.045763 -3.9925339 -3.9645729 -3.9972446 -4.0481977 -4.0807738 -4.1026454 -4.1252217 -4.1447849 -4.1564674 -4.1629739][-4.1202888 -4.1156688 -4.1040888 -4.0933261 -4.0679007 -4.027123 -4.0015993 -4.0257115 -4.0637951 -4.0866308 -4.1040225 -4.1294818 -4.1524215 -4.1666231 -4.1763973][-4.1024289 -4.0973577 -4.0935893 -4.0972619 -4.0946279 -4.0761032 -4.0637083 -4.0795736 -4.0986567 -4.1057448 -4.1143332 -4.1381254 -4.1624188 -4.1800861 -4.1932278][-4.1075573 -4.1013961 -4.0990906 -4.1097007 -4.120347 -4.1155691 -4.110373 -4.1165705 -4.1192141 -4.1186242 -4.1274157 -4.149189 -4.1681442 -4.1829891 -4.1946859][-4.133172 -4.1340966 -4.1377835 -4.1500559 -4.1581283 -4.1515017 -4.1428108 -4.1363211 -4.1283035 -4.1266432 -4.1408744 -4.1631007 -4.1776052 -4.1900826 -4.1988378][-4.1719503 -4.1777744 -4.1850491 -4.1964736 -4.1991615 -4.1861138 -4.1684413 -4.1505585 -4.1336384 -4.1273079 -4.1396327 -4.16203 -4.1793761 -4.1946826 -4.2006326][-4.2060919 -4.21165 -4.22002 -4.2291055 -4.2260509 -4.2069578 -4.1807737 -4.1524429 -4.1256967 -4.11306 -4.1248188 -4.1522017 -4.1766634 -4.1929564 -4.195631][-4.2271938 -4.2304792 -4.2369561 -4.24156 -4.231648 -4.2074394 -4.1781936 -4.1486678 -4.1194224 -4.1064897 -4.12075 -4.1550794 -4.1862617 -4.1999588 -4.1973696]]...]
INFO - root - 2017-12-07 22:31:25.007481: step 27010, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.682 sec/batch; 65h:02m:45s remains)
INFO - root - 2017-12-07 22:31:41.069019: step 27020, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 1.550 sec/batch; 59h:56m:26s remains)
INFO - root - 2017-12-07 22:31:57.272633: step 27030, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.717 sec/batch; 66h:22m:53s remains)
INFO - root - 2017-12-07 22:32:13.435544: step 27040, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.634 sec/batch; 63h:11m:58s remains)
INFO - root - 2017-12-07 22:32:29.795191: step 27050, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.623 sec/batch; 62h:45m:26s remains)
INFO - root - 2017-12-07 22:32:45.932998: step 27060, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.578 sec/batch; 60h:59m:40s remains)
INFO - root - 2017-12-07 22:33:02.187021: step 27070, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.591 sec/batch; 61h:31m:20s remains)
INFO - root - 2017-12-07 22:33:18.420716: step 27080, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.670 sec/batch; 64h:32m:41s remains)
INFO - root - 2017-12-07 22:33:34.420768: step 27090, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.638 sec/batch; 63h:19m:14s remains)
INFO - root - 2017-12-07 22:33:50.761748: step 27100, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.641 sec/batch; 63h:24m:43s remains)
2017-12-07 22:33:52.056168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0972004 -4.0723209 -4.094923 -4.1263418 -4.1602964 -4.1941204 -4.2156138 -4.2279844 -4.2420483 -4.255887 -4.2650514 -4.2634106 -4.2626019 -4.2570186 -4.2299137][-4.098424 -4.0744424 -4.0959148 -4.1259089 -4.1555071 -4.1885543 -4.2080531 -4.2157054 -4.2238946 -4.2314315 -4.2401714 -4.2428665 -4.240624 -4.2328839 -4.2057567][-4.1657491 -4.1469321 -4.162972 -4.1832781 -4.1923761 -4.2007294 -4.2057719 -4.2079396 -4.2113609 -4.2150512 -4.2293468 -4.2408972 -4.2415075 -4.2314939 -4.2064281][-4.2288551 -4.2150497 -4.2266893 -4.2350993 -4.2209206 -4.2017713 -4.1927066 -4.1991191 -4.2075748 -4.2100015 -4.2318034 -4.2533469 -4.2612972 -4.251678 -4.2268219][-4.26736 -4.2575612 -4.2609882 -4.2537169 -4.2210183 -4.1783619 -4.1480942 -4.1598148 -4.1836953 -4.1945624 -4.2230487 -4.2535973 -4.2710271 -4.2658153 -4.2433081][-4.2709146 -4.2597041 -4.2547493 -4.2274761 -4.1732197 -4.1032162 -4.0383334 -4.0442929 -4.097847 -4.1325035 -4.1703968 -4.2115064 -4.244884 -4.2527552 -4.2411284][-4.2365713 -4.2180262 -4.1974535 -4.1512346 -4.0785174 -3.9795675 -3.8615537 -3.8389239 -3.9370558 -4.02362 -4.0824857 -4.1332951 -4.1809263 -4.212687 -4.2233896][-4.1963935 -4.1684928 -4.1343155 -4.0808568 -4.0054941 -3.8955946 -3.7408729 -3.6698704 -3.7972927 -3.9350054 -4.0173373 -4.0689254 -4.1197639 -4.1678329 -4.2034793][-4.1757007 -4.1485996 -4.1179094 -4.0729575 -4.0233278 -3.9467194 -3.83629 -3.7556171 -3.834703 -3.9570568 -4.0394607 -4.0829339 -4.1205778 -4.1631241 -4.2026749][-4.176249 -4.15691 -4.134635 -4.1046004 -4.08494 -4.0498519 -4.0000558 -3.9450378 -3.9669123 -4.0417633 -4.1076326 -4.1437783 -4.1701741 -4.1960688 -4.2205849][-4.2022085 -4.1916418 -4.1831751 -4.1686864 -4.1632152 -4.1488547 -4.1312418 -4.0972147 -4.0881824 -4.1211686 -4.1650362 -4.1913075 -4.2077942 -4.2218242 -4.2315865][-4.248251 -4.2417116 -4.2400908 -4.2341228 -4.2337046 -4.2291474 -4.2225394 -4.2002373 -4.1842203 -4.1919456 -4.2147694 -4.2316828 -4.2380724 -4.240149 -4.2424803][-4.2871089 -4.2825122 -4.279592 -4.2763362 -4.2780275 -4.2807245 -4.2793612 -4.2677565 -4.255403 -4.2526417 -4.2611871 -4.2683811 -4.2653751 -4.2562428 -4.251339][-4.3004522 -4.29539 -4.2880316 -4.2832007 -4.28642 -4.2938123 -4.2974544 -4.2933683 -4.2867818 -4.2827797 -4.28557 -4.2896233 -4.2865667 -4.2748671 -4.2639341][-4.3120456 -4.3085051 -4.298038 -4.2908559 -4.2930956 -4.2993894 -4.3028693 -4.3016133 -4.2972355 -4.2938194 -4.294261 -4.2977462 -4.2979569 -4.2921972 -4.2830858]]...]
INFO - root - 2017-12-07 22:34:08.174748: step 27110, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.678 sec/batch; 64h:51m:10s remains)
INFO - root - 2017-12-07 22:34:24.465046: step 27120, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.576 sec/batch; 60h:53m:30s remains)
INFO - root - 2017-12-07 22:34:40.729779: step 27130, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.623 sec/batch; 62h:42m:07s remains)
INFO - root - 2017-12-07 22:34:56.917154: step 27140, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.567 sec/batch; 60h:33m:22s remains)
INFO - root - 2017-12-07 22:35:13.149622: step 27150, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.685 sec/batch; 65h:06m:05s remains)
INFO - root - 2017-12-07 22:35:29.384597: step 27160, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.607 sec/batch; 62h:04m:51s remains)
INFO - root - 2017-12-07 22:35:45.869146: step 27170, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.543 sec/batch; 59h:35m:50s remains)
INFO - root - 2017-12-07 22:36:02.028247: step 27180, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.671 sec/batch; 64h:33m:26s remains)
INFO - root - 2017-12-07 22:36:18.197640: step 27190, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.560 sec/batch; 60h:16m:22s remains)
INFO - root - 2017-12-07 22:36:34.538794: step 27200, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.690 sec/batch; 65h:17m:04s remains)
2017-12-07 22:36:35.801230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3227258 -4.3072276 -4.2953768 -4.2896876 -4.2869077 -4.2797728 -4.267251 -4.2469635 -4.2300448 -4.2209253 -4.2228646 -4.2459488 -4.2647376 -4.2588458 -4.2443113][-4.320682 -4.3031969 -4.2926254 -4.2884755 -4.2848506 -4.2716494 -4.2523875 -4.2283006 -4.2062559 -4.1904016 -4.1924238 -4.2224469 -4.24158 -4.2314463 -4.2138114][-4.3159003 -4.2971759 -4.28741 -4.2843938 -4.2778883 -4.2583528 -4.2312889 -4.2007785 -4.1739511 -4.1556387 -4.1596894 -4.1940536 -4.2114272 -4.2002039 -4.182847][-4.3102632 -4.291337 -4.2826505 -4.2787962 -4.2676287 -4.242837 -4.210053 -4.1741562 -4.1471047 -4.1358848 -4.1465354 -4.182569 -4.2006979 -4.1937628 -4.1800885][-4.3059683 -4.2888594 -4.28125 -4.2751989 -4.2610145 -4.2344575 -4.1987324 -4.1619124 -4.1406012 -4.1459403 -4.1678934 -4.1990447 -4.2123761 -4.2081976 -4.2007132][-4.3058004 -4.2936316 -4.2895727 -4.28356 -4.2683482 -4.2401657 -4.2029352 -4.164937 -4.1472383 -4.1706572 -4.2002983 -4.2209878 -4.223074 -4.2164168 -4.2130837][-4.3088241 -4.3028836 -4.3044105 -4.3027549 -4.2880077 -4.2577434 -4.2185049 -4.1747465 -4.1552315 -4.1879568 -4.2166657 -4.2250719 -4.2176437 -4.2094188 -4.207706][-4.3113971 -4.3098621 -4.3166614 -4.32 -4.3046331 -4.2706847 -4.22913 -4.1801996 -4.1593671 -4.1948242 -4.2207193 -4.2202268 -4.2065344 -4.1973081 -4.195888][-4.3123064 -4.3134441 -4.3236876 -4.3298264 -4.3136129 -4.2795835 -4.2381368 -4.1872296 -4.1683035 -4.2058988 -4.2322993 -4.2266221 -4.2085018 -4.1980572 -4.1966047][-4.3117371 -4.3123307 -4.3224549 -4.3292365 -4.315125 -4.2863889 -4.2495966 -4.2031384 -4.1887422 -4.2255883 -4.2500954 -4.2414227 -4.2205467 -4.209166 -4.2081256][-4.3093028 -4.3065443 -4.3137436 -4.3188219 -4.3066225 -4.2826328 -4.2521496 -4.217628 -4.2124758 -4.2475476 -4.2673965 -4.2567687 -4.2375402 -4.226347 -4.22404][-4.303741 -4.2957973 -4.2976708 -4.3002534 -4.2894492 -4.2693176 -4.2440844 -4.2216749 -4.2270427 -4.2601042 -4.2749906 -4.2627416 -4.2463908 -4.2356486 -4.23106][-4.2996655 -4.2855277 -4.2798285 -4.2787828 -4.2695189 -4.2531576 -4.233336 -4.2193251 -4.2309475 -4.26188 -4.2716184 -4.2561407 -4.2393408 -4.2277031 -4.2233963][-4.3037724 -4.287096 -4.2771063 -4.2738729 -4.2673149 -4.254961 -4.2425685 -4.233026 -4.2418895 -4.2614446 -4.2625861 -4.244009 -4.2278585 -4.2164664 -4.2133312][-4.313498 -4.3004122 -4.2928567 -4.2883067 -4.2823153 -4.2734675 -4.2662978 -4.2587094 -4.2625246 -4.2710123 -4.266 -4.244771 -4.2299223 -4.2219996 -4.219667]]...]
INFO - root - 2017-12-07 22:36:51.796980: step 27210, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.613 sec/batch; 62h:18m:06s remains)
INFO - root - 2017-12-07 22:37:07.967035: step 27220, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.578 sec/batch; 60h:56m:04s remains)
INFO - root - 2017-12-07 22:37:24.302951: step 27230, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.604 sec/batch; 61h:56m:04s remains)
INFO - root - 2017-12-07 22:37:40.430577: step 27240, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.721 sec/batch; 66h:26m:56s remains)
INFO - root - 2017-12-07 22:37:56.644900: step 27250, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.535 sec/batch; 59h:15m:53s remains)
INFO - root - 2017-12-07 22:38:12.911640: step 27260, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.659 sec/batch; 64h:03m:41s remains)
INFO - root - 2017-12-07 22:38:28.849673: step 27270, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.561 sec/batch; 60h:16m:46s remains)
INFO - root - 2017-12-07 22:38:45.037752: step 27280, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.678 sec/batch; 64h:47m:27s remains)
INFO - root - 2017-12-07 22:39:01.367000: step 27290, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.609 sec/batch; 62h:06m:01s remains)
INFO - root - 2017-12-07 22:39:17.556047: step 27300, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.684 sec/batch; 64h:59m:31s remains)
2017-12-07 22:39:18.989142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2874703 -4.29243 -4.299181 -4.3084674 -4.3167315 -4.3199739 -4.3151627 -4.3056216 -4.2958388 -4.2901044 -4.290565 -4.2951512 -4.3001208 -4.303092 -4.3031507][-4.2863979 -4.2954512 -4.3076944 -4.319315 -4.3240223 -4.3174171 -4.2981639 -4.2746339 -4.2559409 -4.2505369 -4.2567725 -4.2689643 -4.2816448 -4.2907882 -4.2950521][-4.2775807 -4.2932544 -4.3124819 -4.3246112 -4.3206863 -4.299149 -4.2607884 -4.2189293 -4.1890955 -4.1853013 -4.1984034 -4.2195306 -4.24242 -4.2595353 -4.2713342][-4.2678866 -4.2924147 -4.3173275 -4.3258104 -4.3079228 -4.2660947 -4.2025847 -4.1372008 -4.09232 -4.0882587 -4.1113734 -4.1463594 -4.1824574 -4.2127509 -4.237978][-4.2644291 -4.2965641 -4.3227019 -4.3211079 -4.2845893 -4.218761 -4.1275797 -4.0348935 -3.9699533 -3.9622905 -4.0006485 -4.0563288 -4.1141644 -4.1666474 -4.2123427][-4.2647138 -4.2969012 -4.3152881 -4.2949448 -4.2345734 -4.1424012 -4.0187407 -3.8960214 -3.8144305 -3.8237085 -3.9020219 -3.9946582 -4.0782151 -4.1518464 -4.2122688][-4.2592793 -4.2795477 -4.2770991 -4.2318597 -4.1485815 -4.0332775 -3.8845174 -3.7419443 -3.6755314 -3.7462735 -3.8864672 -4.0114803 -4.1051373 -4.182559 -4.2403488][-4.2413 -4.2394466 -4.2092466 -4.1398468 -4.0411954 -3.9206138 -3.7779672 -3.6685216 -3.6708221 -3.80871 -3.9716825 -4.0904107 -4.1716733 -4.2329988 -4.2734008][-4.2093763 -4.1821003 -4.1280956 -4.0451307 -3.9512339 -3.8644228 -3.787081 -3.7608821 -3.8274074 -3.9678974 -4.0979133 -4.1822395 -4.2369413 -4.2738876 -4.2941055][-4.1784005 -4.1340504 -4.0720377 -3.9949791 -3.9310648 -3.9049108 -3.9011774 -3.9353862 -4.0149627 -4.116713 -4.1966214 -4.24334 -4.2701015 -4.2860684 -4.2930603][-4.1609988 -4.115099 -4.0634389 -4.011899 -3.9886615 -4.0054808 -4.0371327 -4.0835028 -4.1407371 -4.1967111 -4.2366486 -4.2545061 -4.2606945 -4.263701 -4.2656412][-4.1690674 -4.1354733 -4.1030426 -4.0784054 -4.07874 -4.1045389 -4.1371284 -4.17038 -4.19646 -4.2154088 -4.2247562 -4.223855 -4.221045 -4.222703 -4.2290478][-4.1884317 -4.1722093 -4.1567268 -4.1461105 -4.1491008 -4.1661758 -4.1874537 -4.2023683 -4.2054543 -4.1999683 -4.1883574 -4.1783247 -4.1770692 -4.185657 -4.2022605][-4.2078424 -4.2058935 -4.2003469 -4.192234 -4.1889939 -4.1948686 -4.2048211 -4.2077441 -4.1978784 -4.1788177 -4.1586118 -4.1493206 -4.1553893 -4.1741 -4.1999564][-4.2269673 -4.2336154 -4.2343206 -4.2300744 -4.2252536 -4.2247777 -4.225738 -4.2209721 -4.2068548 -4.1867528 -4.1689496 -4.163969 -4.1749129 -4.1969767 -4.22309]]...]
INFO - root - 2017-12-07 22:39:35.239157: step 27310, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.699 sec/batch; 65h:34m:20s remains)
INFO - root - 2017-12-07 22:39:51.549036: step 27320, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 1.587 sec/batch; 61h:15m:47s remains)
INFO - root - 2017-12-07 22:40:07.720894: step 27330, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.611 sec/batch; 62h:09m:01s remains)
INFO - root - 2017-12-07 22:40:23.862105: step 27340, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.577 sec/batch; 60h:51m:18s remains)
INFO - root - 2017-12-07 22:40:40.185961: step 27350, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.604 sec/batch; 61h:52m:26s remains)
INFO - root - 2017-12-07 22:40:56.273414: step 27360, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.683 sec/batch; 64h:55m:18s remains)
INFO - root - 2017-12-07 22:41:12.455542: step 27370, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.566 sec/batch; 60h:24m:37s remains)
INFO - root - 2017-12-07 22:41:28.759370: step 27380, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.644 sec/batch; 63h:24m:13s remains)
INFO - root - 2017-12-07 22:41:44.842651: step 27390, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.577 sec/batch; 60h:48m:48s remains)
INFO - root - 2017-12-07 22:42:01.217142: step 27400, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.662 sec/batch; 64h:06m:31s remains)
2017-12-07 22:42:02.705417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2572317 -4.2445321 -4.2449808 -4.2560487 -4.2838788 -4.3029795 -4.3027768 -4.3035555 -4.3082671 -4.3031487 -4.2909012 -4.2660384 -4.242291 -4.2181845 -4.2202492][-4.248148 -4.2319255 -4.2350187 -4.2466192 -4.2724066 -4.2825465 -4.2731829 -4.2699056 -4.2736058 -4.267067 -4.253952 -4.2252564 -4.2012548 -4.1788006 -4.1919279][-4.2528305 -4.2376251 -4.2384806 -4.239429 -4.2555809 -4.2548 -4.2369208 -4.2266073 -4.2263789 -4.2222714 -4.21185 -4.1831646 -4.1653962 -4.1479535 -4.1729779][-4.2730365 -4.2578011 -4.2499747 -4.236486 -4.2381182 -4.2266359 -4.2009277 -4.1799517 -4.1715693 -4.1728787 -4.1719418 -4.1513014 -4.1429515 -4.1311674 -4.1653957][-4.2976 -4.27299 -4.2481833 -4.2195749 -4.2133093 -4.1931658 -4.1563621 -4.1209741 -4.1071987 -4.1283727 -4.1476226 -4.1457024 -4.1497254 -4.143538 -4.1780677][-4.3004436 -4.2645836 -4.2284737 -4.191256 -4.1804194 -4.1519446 -4.101563 -4.048069 -4.0339146 -4.0885248 -4.13345 -4.1517649 -4.1662464 -4.1639047 -4.19481][-4.2785091 -4.2351747 -4.1938081 -4.15163 -4.1382661 -4.09986 -4.0332589 -3.9610879 -3.9526877 -4.0473824 -4.1114087 -4.1413708 -4.1634965 -4.1640267 -4.1913671][-4.2330513 -4.1785483 -4.1342278 -4.0871143 -4.0696716 -4.0289583 -3.9570243 -3.8756087 -3.876297 -3.9984248 -4.0670462 -4.0985694 -4.1259632 -4.1322532 -4.1560583][-4.19544 -4.1362386 -4.0915012 -4.0469718 -4.0377746 -4.0123682 -3.954386 -3.8903959 -3.8966088 -4.0015335 -4.0544181 -4.0801568 -4.1015282 -4.1014872 -4.1211023][-4.2056971 -4.1505718 -4.10893 -4.0728149 -4.0731006 -4.0611377 -4.0277066 -3.991996 -4.0015092 -4.0703063 -4.101449 -4.1132436 -4.1180143 -4.1057248 -4.1209764][-4.2436357 -4.1988196 -4.1630878 -4.1360483 -4.1419 -4.1380424 -4.1250358 -4.1136637 -4.1228566 -4.1620965 -4.1777406 -4.1750841 -4.1661019 -4.1473827 -4.1628232][-4.2908921 -4.2566419 -4.2288332 -4.2109118 -4.2177334 -4.2197804 -4.2205348 -4.2220716 -4.2295189 -4.2485633 -4.2529726 -4.2394891 -4.2214613 -4.2003174 -4.2147689][-4.3214507 -4.3003721 -4.2833691 -4.277441 -4.2843628 -4.2864513 -4.2891784 -4.2935619 -4.29986 -4.3091464 -4.3068833 -4.28905 -4.2696495 -4.2520971 -4.2639904][-4.3382845 -4.3246417 -4.3163109 -4.3160157 -4.3197727 -4.318666 -4.3182211 -4.3209605 -4.3259411 -4.3323679 -4.3291478 -4.3143659 -4.2989044 -4.288197 -4.2986636][-4.3393764 -4.333796 -4.3303533 -4.3300238 -4.3310294 -4.3286734 -4.3269186 -4.3283453 -4.332428 -4.3368344 -4.3339248 -4.324347 -4.3153367 -4.3111954 -4.319109]]...]
INFO - root - 2017-12-07 22:42:19.075011: step 27410, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.587 sec/batch; 61h:12m:10s remains)
INFO - root - 2017-12-07 22:42:35.390409: step 27420, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.592 sec/batch; 61h:24m:09s remains)
INFO - root - 2017-12-07 22:42:51.872045: step 27430, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.630 sec/batch; 62h:50m:41s remains)
INFO - root - 2017-12-07 22:43:08.060221: step 27440, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.569 sec/batch; 60h:30m:32s remains)
INFO - root - 2017-12-07 22:43:24.358941: step 27450, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.588 sec/batch; 61h:12m:49s remains)
INFO - root - 2017-12-07 22:43:40.635018: step 27460, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.700 sec/batch; 65h:32m:24s remains)
INFO - root - 2017-12-07 22:43:56.761414: step 27470, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.600 sec/batch; 61h:41m:19s remains)
INFO - root - 2017-12-07 22:44:13.053816: step 27480, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.648 sec/batch; 63h:32m:05s remains)
INFO - root - 2017-12-07 22:44:29.333051: step 27490, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 61h:16m:04s remains)
INFO - root - 2017-12-07 22:44:45.519973: step 27500, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.603 sec/batch; 61h:45m:51s remains)
2017-12-07 22:44:46.851372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3014507 -4.2944417 -4.2889643 -4.28926 -4.2907686 -4.2932377 -4.2910881 -4.2842197 -4.2779841 -4.2720151 -4.268178 -4.2663612 -4.261806 -4.2555285 -4.2415752][-4.291841 -4.2854276 -4.2814455 -4.2845483 -4.2898126 -4.2985892 -4.3016 -4.2965627 -4.288682 -4.2762623 -4.2646437 -4.2583871 -4.2525177 -4.2478561 -4.2333674][-4.2571025 -4.2444472 -4.240808 -4.2489705 -4.262898 -4.280128 -4.2906475 -4.2897816 -4.2828445 -4.2698979 -4.2556162 -4.2481713 -4.2433128 -4.2398119 -4.229495][-4.1904597 -4.1758327 -4.175633 -4.1895905 -4.2083421 -4.2312794 -4.2511458 -4.2608104 -4.2599716 -4.2535572 -4.2440491 -4.2369657 -4.2315555 -4.2281175 -4.2235055][-4.10341 -4.0936427 -4.0988336 -4.1149683 -4.1326427 -4.1551819 -4.1819053 -4.2016115 -4.2101789 -4.2143373 -4.2145534 -4.2124939 -4.2097774 -4.2087774 -4.208838][-4.069962 -4.0685806 -4.0770192 -4.0865211 -4.0893092 -4.0939021 -4.1107745 -4.13079 -4.145771 -4.1594763 -4.1701074 -4.1765347 -4.1810679 -4.1884713 -4.1941538][-4.120223 -4.1233521 -4.126739 -4.1189928 -4.0983253 -4.0730309 -4.0624146 -4.0693526 -4.0858722 -4.1097813 -4.1326776 -4.1459036 -4.1542897 -4.16928 -4.1830711][-4.1746349 -4.1778316 -4.1767907 -4.1583972 -4.1261272 -4.0839214 -4.0490804 -4.0347114 -4.044241 -4.0716958 -4.0980663 -4.1115828 -4.1190209 -4.1362424 -4.15821][-4.2018714 -4.2033854 -4.1970372 -4.1708212 -4.1418147 -4.1108236 -4.0858169 -4.0737376 -4.0772023 -4.0934725 -4.1054449 -4.1028562 -4.094717 -4.100605 -4.1240692][-4.2113323 -4.21246 -4.2052488 -4.1796327 -4.1594138 -4.1479521 -4.1451626 -4.1462588 -4.1476865 -4.15373 -4.154664 -4.1360755 -4.1072159 -4.099081 -4.1172767][-4.2323232 -4.2352657 -4.2328215 -4.2158575 -4.20408 -4.2031689 -4.2092404 -4.2135048 -4.2115211 -4.2121959 -4.2108245 -4.1891737 -4.1545148 -4.1395154 -4.1475096][-4.2508736 -4.2591534 -4.2651682 -4.2609072 -4.2575788 -4.2573543 -4.2581706 -4.2582774 -4.25387 -4.254262 -4.2547388 -4.2334633 -4.201726 -4.1842403 -4.1846385][-4.2558308 -4.2707176 -4.2841268 -4.289248 -4.2907743 -4.2866254 -4.2803912 -4.2778668 -4.2736673 -4.2769842 -4.2823048 -4.2651868 -4.2373719 -4.2193685 -4.21919][-4.2336745 -4.25787 -4.2769885 -4.2852879 -4.2886028 -4.282917 -4.2695756 -4.2645321 -4.2638083 -4.2741227 -4.2874165 -4.2813292 -4.26359 -4.2516236 -4.2559094][-4.1980157 -4.22478 -4.2477136 -4.2618184 -4.2688375 -4.2641978 -4.2482061 -4.2404571 -4.2431216 -4.258554 -4.2760248 -4.2823591 -4.2772589 -4.275918 -4.2860675]]...]
INFO - root - 2017-12-07 22:45:02.795288: step 27510, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 1.511 sec/batch; 58h:14m:55s remains)
INFO - root - 2017-12-07 22:45:19.206781: step 27520, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.699 sec/batch; 65h:27m:53s remains)
INFO - root - 2017-12-07 22:45:35.276572: step 27530, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.626 sec/batch; 62h:40m:24s remains)
INFO - root - 2017-12-07 22:45:51.626166: step 27540, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.667 sec/batch; 64h:14m:34s remains)
INFO - root - 2017-12-07 22:46:07.921994: step 27550, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.588 sec/batch; 61h:10m:44s remains)
INFO - root - 2017-12-07 22:46:24.253496: step 27560, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.642 sec/batch; 63h:14m:51s remains)
INFO - root - 2017-12-07 22:46:40.198653: step 27570, loss = 2.09, batch loss = 2.04 (10.7 examples/sec; 1.502 sec/batch; 57h:51m:47s remains)
INFO - root - 2017-12-07 22:46:56.458105: step 27580, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.569 sec/batch; 60h:26m:03s remains)
INFO - root - 2017-12-07 22:47:12.689957: step 27590, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.657 sec/batch; 63h:48m:54s remains)
INFO - root - 2017-12-07 22:47:29.029099: step 27600, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.613 sec/batch; 62h:08m:11s remains)
2017-12-07 22:47:30.320328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2969384 -4.3013053 -4.2968855 -4.2907395 -4.2866735 -4.2848835 -4.2724428 -4.2514772 -4.2279277 -4.2088552 -4.2056241 -4.2164583 -4.2320223 -4.2466736 -4.259377][-4.2854629 -4.2832203 -4.2700443 -4.2609162 -4.2576795 -4.2575545 -4.2467022 -4.2307172 -4.2175431 -4.2129035 -4.2257543 -4.2435956 -4.256958 -4.2672949 -4.2776394][-4.27048 -4.2508512 -4.2265654 -4.2125716 -4.2111983 -4.2170472 -4.2177153 -4.215394 -4.2182431 -4.2270885 -4.2475066 -4.2661667 -4.2780046 -4.2872558 -4.2952847][-4.2580185 -4.21773 -4.1814461 -4.1623287 -4.1663175 -4.18265 -4.1953197 -4.206676 -4.2244291 -4.2432041 -4.2649522 -4.282023 -4.291986 -4.3009024 -4.3070793][-4.237114 -4.1802754 -4.1366844 -4.1189837 -4.1281924 -4.1519308 -4.178648 -4.2043943 -4.2308736 -4.255918 -4.2771893 -4.2923484 -4.3009391 -4.307498 -4.3114629][-4.200666 -4.13043 -4.0831647 -4.0656123 -4.0786386 -4.11171 -4.1544318 -4.1925693 -4.2245188 -4.2534523 -4.2758408 -4.2920327 -4.3006306 -4.3057127 -4.308579][-4.1612368 -4.08504 -4.0376997 -4.0190744 -4.0283551 -4.0621238 -4.1118221 -4.1584363 -4.2000265 -4.2376585 -4.2650733 -4.2830205 -4.2935 -4.2981887 -4.3012853][-4.1425037 -4.0691328 -4.0218091 -3.9985573 -4.0012212 -4.0306263 -4.0826063 -4.1354675 -4.1852655 -4.2293253 -4.2584238 -4.2726955 -4.28061 -4.2834535 -4.2860436][-4.1603169 -4.0968494 -4.0531607 -4.0326271 -4.0356908 -4.0604491 -4.1040754 -4.1549039 -4.2010851 -4.239841 -4.26202 -4.2657924 -4.2636461 -4.2611732 -4.2618313][-4.1982317 -4.1543221 -4.1246142 -4.1139255 -4.1191406 -4.1382484 -4.1662841 -4.2053933 -4.2448335 -4.272131 -4.2797217 -4.2664971 -4.249207 -4.2371454 -4.2334991][-4.2201786 -4.1966224 -4.1859031 -4.1888037 -4.1982608 -4.2125545 -4.2277122 -4.25271 -4.2788048 -4.2905755 -4.2817707 -4.252861 -4.2229657 -4.2060871 -4.2032108][-4.2399354 -4.233098 -4.2387366 -4.2517786 -4.2610893 -4.2670188 -4.2708812 -4.2835517 -4.2951741 -4.2899961 -4.2622824 -4.2179666 -4.1764064 -4.1625662 -4.1703076][-4.2679234 -4.2703238 -4.2842832 -4.2991686 -4.304739 -4.3044486 -4.3045244 -4.3082671 -4.3051119 -4.2798438 -4.2277513 -4.1627393 -4.1101155 -4.1045408 -4.1353316][-4.2904058 -4.2936897 -4.3075156 -4.3208747 -4.3260136 -4.325285 -4.3262105 -4.3251696 -4.3083897 -4.2627044 -4.1841531 -4.0965495 -4.033103 -4.0331516 -4.0881476][-4.3062539 -4.3080993 -4.3177457 -4.3258772 -4.3286481 -4.3291192 -4.3326764 -4.3324103 -4.3115644 -4.257565 -4.1706481 -4.0748887 -4.0023489 -3.9985604 -4.0658431]]...]
INFO - root - 2017-12-07 22:47:46.693865: step 27610, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 1.608 sec/batch; 61h:56m:42s remains)
INFO - root - 2017-12-07 22:48:02.899459: step 27620, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.642 sec/batch; 63h:13m:14s remains)
INFO - root - 2017-12-07 22:48:19.145691: step 27630, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.566 sec/batch; 60h:16m:54s remains)
INFO - root - 2017-12-07 22:48:35.469277: step 27640, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.677 sec/batch; 64h:33m:23s remains)
INFO - root - 2017-12-07 22:48:51.580109: step 27650, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.538 sec/batch; 59h:12m:39s remains)
INFO - root - 2017-12-07 22:49:07.855747: step 27660, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.660 sec/batch; 63h:53m:30s remains)
INFO - root - 2017-12-07 22:49:24.229960: step 27670, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.635 sec/batch; 62h:56m:36s remains)
INFO - root - 2017-12-07 22:49:40.479378: step 27680, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.663 sec/batch; 64h:01m:18s remains)
INFO - root - 2017-12-07 22:49:56.635374: step 27690, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 60h:32m:20s remains)
INFO - root - 2017-12-07 22:50:13.021014: step 27700, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.606 sec/batch; 61h:49m:23s remains)
2017-12-07 22:50:14.285197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.15802 -4.1557832 -4.1406078 -4.1307878 -4.1258349 -4.1350751 -4.1645169 -4.1991034 -4.2392359 -4.2671671 -4.2801166 -4.287643 -4.2775364 -4.2468467 -4.2046909][-4.1511369 -4.1567149 -4.1478777 -4.1405921 -4.1308341 -4.1266832 -4.1410294 -4.16925 -4.2136803 -4.2523232 -4.2733364 -4.2805262 -4.2724504 -4.2366309 -4.1814985][-4.15852 -4.1677012 -4.1601043 -4.1521964 -4.1371007 -4.1175237 -4.1114311 -4.1287651 -4.1760731 -4.22461 -4.2573934 -4.269865 -4.2662439 -4.227942 -4.1606503][-4.1686149 -4.1741648 -4.1648417 -4.1576691 -4.1395 -4.1065731 -4.0767579 -4.0781288 -4.12727 -4.185904 -4.2325583 -4.2579288 -4.2645645 -4.2287273 -4.1518183][-4.1725812 -4.1672878 -4.1532369 -4.1429477 -4.1207561 -4.0762658 -4.0247393 -4.0112052 -4.0653138 -4.1377015 -4.2018771 -4.2434855 -4.2619686 -4.2347813 -4.1593246][-4.1735158 -4.1570587 -4.1384339 -4.1240807 -4.09456 -4.0346842 -3.9567723 -3.9274342 -3.9948919 -4.0909519 -4.17452 -4.229126 -4.2553554 -4.23752 -4.1715078][-4.1806965 -4.1546683 -4.132019 -4.1144819 -4.078218 -4.0007358 -3.8956394 -3.850076 -3.9308248 -4.0494971 -4.1493187 -4.2159586 -4.2490821 -4.2399273 -4.1853113][-4.1841731 -4.1526804 -4.1264305 -4.1079278 -4.0717711 -3.9903994 -3.8731952 -3.8159695 -3.8971839 -4.0254836 -4.1320815 -4.2051945 -4.2453942 -4.2453718 -4.2030253][-4.1731496 -4.1420088 -4.11886 -4.1052442 -4.0762167 -4.003006 -3.8907938 -3.8338881 -3.903044 -4.0250492 -4.1275821 -4.1997952 -4.2443395 -4.25348 -4.2279916][-4.1702828 -4.1433086 -4.1252346 -4.1177506 -4.0980754 -4.0374479 -3.9455557 -3.9002972 -3.9536719 -4.0583062 -4.1498785 -4.2144589 -4.2568951 -4.2704005 -4.257308][-4.1973686 -4.1762767 -4.1599283 -4.1531267 -4.1365104 -4.0859571 -4.018702 -3.9894032 -4.0306058 -4.1134944 -4.1915483 -4.2471757 -4.2841005 -4.2967772 -4.2892432][-4.2383785 -4.2232084 -4.2078891 -4.2014322 -4.1867518 -4.1480937 -4.1016254 -4.0845876 -4.1152897 -4.1746149 -4.2345157 -4.2812142 -4.3127174 -4.3224039 -4.3170991][-4.2676568 -4.2585621 -4.249043 -4.2463408 -4.2369013 -4.2119122 -4.1814122 -4.1670237 -4.1831832 -4.2219119 -4.2653904 -4.3038344 -4.3295527 -4.3376102 -4.3346658][-4.2846937 -4.2795 -4.2768364 -4.2783146 -4.2757292 -4.263742 -4.2471662 -4.235364 -4.2403812 -4.2621713 -4.2900581 -4.3165774 -4.3350315 -4.3407831 -4.338728][-4.299829 -4.2960477 -4.2973294 -4.3008513 -4.3021369 -4.3007789 -4.2965508 -4.2905936 -4.2906585 -4.2998023 -4.3142877 -4.3281894 -4.3383451 -4.34157 -4.3397555]]...]
INFO - root - 2017-12-07 22:50:30.681490: step 27710, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.653 sec/batch; 63h:36m:37s remains)
INFO - root - 2017-12-07 22:50:46.823140: step 27720, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.599 sec/batch; 61h:30m:53s remains)
INFO - root - 2017-12-07 22:51:03.147214: step 27730, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.569 sec/batch; 60h:22m:50s remains)
INFO - root - 2017-12-07 22:51:19.344641: step 27740, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.610 sec/batch; 61h:57m:04s remains)
INFO - root - 2017-12-07 22:51:35.654004: step 27750, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.571 sec/batch; 60h:26m:45s remains)
INFO - root - 2017-12-07 22:51:51.849113: step 27760, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.647 sec/batch; 63h:20m:29s remains)
INFO - root - 2017-12-07 22:52:08.150992: step 27770, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.593 sec/batch; 61h:15m:32s remains)
INFO - root - 2017-12-07 22:52:24.314458: step 27780, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.665 sec/batch; 64h:02m:57s remains)
INFO - root - 2017-12-07 22:52:40.564295: step 27790, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.666 sec/batch; 64h:04m:14s remains)
INFO - root - 2017-12-07 22:52:56.785752: step 27800, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.621 sec/batch; 62h:19m:32s remains)
2017-12-07 22:52:58.246162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1658416 -4.1710052 -4.1772041 -4.1785169 -4.1792736 -4.1840987 -4.1906753 -4.1985531 -4.2053294 -4.1967349 -4.1811972 -4.1890039 -4.209568 -4.2256246 -4.2392826][-4.1659112 -4.1748476 -4.1851821 -4.1890197 -4.1887178 -4.1893735 -4.1893463 -4.1904135 -4.1911206 -4.1771631 -4.1571207 -4.1624947 -4.1807446 -4.1991687 -4.2200747][-4.1768174 -4.1915255 -4.2070913 -4.213604 -4.2136149 -4.2101974 -4.2039328 -4.1978865 -4.1918731 -4.1714182 -4.1458964 -4.1461945 -4.1599693 -4.1796045 -4.2048907][-4.1889009 -4.20793 -4.2244473 -4.2300916 -4.2278309 -4.2214408 -4.2123933 -4.2032905 -4.195087 -4.1718874 -4.1426082 -4.1377778 -4.1477008 -4.1670284 -4.1942563][-4.1922107 -4.2091279 -4.218215 -4.2151966 -4.2054992 -4.1944451 -4.1865168 -4.1823092 -4.1790352 -4.1638827 -4.1394515 -4.13505 -4.1452093 -4.1648116 -4.1909828][-4.1821127 -4.1889114 -4.1832929 -4.1673937 -4.1465707 -4.1249051 -4.1151538 -4.1231985 -4.1353765 -4.13713 -4.128684 -4.1337981 -4.1503873 -4.1727529 -4.2006736][-4.1707134 -4.1613231 -4.1375294 -4.1065249 -4.0719552 -4.0330167 -4.0154233 -4.0362177 -4.0707369 -4.0963068 -4.1144805 -4.1383543 -4.167275 -4.1957874 -4.224442][-4.1711645 -4.1498537 -4.1153779 -4.0768914 -4.0349183 -3.9826493 -3.9561822 -3.9832015 -4.032949 -4.0762792 -4.110631 -4.1474214 -4.1867313 -4.2211409 -4.2513428][-4.1785297 -4.1544747 -4.1238165 -4.0927534 -4.055089 -4.0003638 -3.9710736 -3.9979854 -4.0457025 -4.0893893 -4.1237578 -4.1604486 -4.2008433 -4.235333 -4.2642493][-4.1846175 -4.1685381 -4.1527033 -4.1350141 -4.1084118 -4.0664392 -4.0459423 -4.0684643 -4.1047044 -4.1381578 -4.1644015 -4.1926217 -4.224504 -4.2498703 -4.2675204][-4.1836057 -4.1796532 -4.1794505 -4.1742177 -4.1593962 -4.1356936 -4.1281095 -4.1442795 -4.1678238 -4.1895108 -4.2060814 -4.2215452 -4.2422261 -4.256753 -4.263927][-4.168097 -4.1741047 -4.1858058 -4.1900015 -4.1848955 -4.1763511 -4.1773467 -4.1898055 -4.2058692 -4.2196779 -4.2276 -4.2357635 -4.2514391 -4.2607222 -4.262527][-4.1490474 -4.1649828 -4.1830206 -4.1930714 -4.1950746 -4.1937351 -4.1956592 -4.204515 -4.2154469 -4.2216792 -4.222671 -4.2277565 -4.2430716 -4.25323 -4.2573323][-4.1368337 -4.1576405 -4.1782565 -4.1910286 -4.1956143 -4.1966314 -4.1977253 -4.2027731 -4.2086787 -4.2075758 -4.2025347 -4.2067924 -4.2225003 -4.2332821 -4.2408309][-4.1579185 -4.1767249 -4.19401 -4.2047849 -4.2089267 -4.2096386 -4.2097287 -4.2118206 -4.2129645 -4.2039838 -4.1904874 -4.1909571 -4.20575 -4.2146235 -4.2201304]]...]
INFO - root - 2017-12-07 22:53:14.275995: step 27810, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 1.557 sec/batch; 59h:51m:27s remains)
INFO - root - 2017-12-07 22:53:30.556798: step 27820, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.702 sec/batch; 65h:26m:49s remains)
INFO - root - 2017-12-07 22:53:46.828227: step 27830, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.614 sec/batch; 62h:03m:01s remains)
INFO - root - 2017-12-07 22:54:03.054709: step 27840, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.640 sec/batch; 63h:03m:43s remains)
INFO - root - 2017-12-07 22:54:19.193578: step 27850, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.533 sec/batch; 58h:55m:26s remains)
INFO - root - 2017-12-07 22:54:35.518075: step 27860, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.560 sec/batch; 59h:57m:06s remains)
INFO - root - 2017-12-07 22:54:51.770284: step 27870, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 1.702 sec/batch; 65h:26m:20s remains)
INFO - root - 2017-12-07 22:55:07.799101: step 27880, loss = 2.10, batch loss = 2.04 (10.2 examples/sec; 1.566 sec/batch; 60h:10m:58s remains)
INFO - root - 2017-12-07 22:55:24.100921: step 27890, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.697 sec/batch; 65h:14m:12s remains)
INFO - root - 2017-12-07 22:55:40.375503: step 27900, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.606 sec/batch; 61h:42m:16s remains)
2017-12-07 22:55:41.816358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2530403 -4.2524581 -4.2526212 -4.2401257 -4.2170382 -4.1976027 -4.1899667 -4.19346 -4.1965303 -4.1990891 -4.2096682 -4.2193274 -4.2159877 -4.2078447 -4.2094378][-4.2455015 -4.2393622 -4.2325907 -4.2173386 -4.1949077 -4.1778092 -4.1718597 -4.1802049 -4.1883278 -4.1949944 -4.2026296 -4.2074165 -4.2018361 -4.1923347 -4.196969][-4.229135 -4.222198 -4.2125525 -4.1947312 -4.1740489 -4.1575537 -4.1505666 -4.1610403 -4.1761351 -4.1896996 -4.1980152 -4.2001224 -4.1944504 -4.1823292 -4.1872249][-4.1991539 -4.200551 -4.1948462 -4.180222 -4.1615248 -4.1417947 -4.1296225 -4.1399493 -4.161694 -4.1825805 -4.1918254 -4.1963797 -4.1930132 -4.1807613 -4.181715][-4.1559167 -4.1720228 -4.178854 -4.1738553 -4.1598053 -4.13542 -4.1178446 -4.1269 -4.153841 -4.1775231 -4.1854572 -4.1900048 -4.1903796 -4.1812592 -4.1799278][-4.1136236 -4.1442103 -4.1658945 -4.1747937 -4.1678128 -4.1462674 -4.1273832 -4.1299658 -4.1508851 -4.1684346 -4.1717381 -4.1757207 -4.1851954 -4.1860967 -4.1869907][-4.07064 -4.1089559 -4.1464963 -4.1721339 -4.1759639 -4.1607409 -4.1394253 -4.1302061 -4.1377506 -4.1473804 -4.1494088 -4.1575394 -4.1780849 -4.1882882 -4.1932597][-4.0433617 -4.0703783 -4.1104412 -4.1465411 -4.1618648 -4.154932 -4.1347661 -4.1185374 -4.1200404 -4.1275816 -4.1293015 -4.1387835 -4.1606522 -4.171133 -4.1770477][-4.0572977 -4.0547376 -4.0761824 -4.1017551 -4.1140938 -4.1130657 -4.1001925 -4.0870876 -4.0898004 -4.1030488 -4.1060662 -4.1113377 -4.1280842 -4.1327314 -4.1371174][-4.0670362 -4.0409651 -4.0374131 -4.0368924 -4.03152 -4.0269227 -4.0226121 -4.0190144 -4.0318651 -4.0567808 -4.06471 -4.0680556 -4.0804954 -4.0845666 -4.0886526][-4.030283 -3.9934733 -3.9768598 -3.9638805 -3.9527888 -3.9444187 -3.9402785 -3.9432731 -3.968019 -4.0015693 -4.0129275 -4.0179486 -4.0298762 -4.0391383 -4.0467229][-3.9726965 -3.9370959 -3.9213264 -3.9105418 -3.9050789 -3.8964329 -3.8821356 -3.8839827 -3.9242706 -3.969902 -3.9859586 -3.9934411 -4.0082717 -4.022562 -4.0340233][-3.9311349 -3.9037182 -3.8942513 -3.8924944 -3.8973832 -3.8899052 -3.8672237 -3.8659668 -3.9170825 -3.971302 -3.9893758 -3.9998329 -4.0147552 -4.0286326 -4.03725][-3.9597678 -3.9433839 -3.9428334 -3.9518237 -3.9613252 -3.9539275 -3.9315577 -3.9282603 -3.9755445 -4.0273418 -4.0410576 -4.0491281 -4.0627685 -4.0710244 -4.0704503][-4.0554223 -4.0463624 -4.0479746 -4.0596046 -4.0702186 -4.0632372 -4.0466032 -4.0445371 -4.0789351 -4.1160617 -4.1221738 -4.1258211 -4.1371078 -4.1411572 -4.1353765]]...]
INFO - root - 2017-12-07 22:55:57.989876: step 27910, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.584 sec/batch; 60h:51m:29s remains)
INFO - root - 2017-12-07 22:56:14.212558: step 27920, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.553 sec/batch; 59h:39m:27s remains)
INFO - root - 2017-12-07 22:56:30.598424: step 27930, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.595 sec/batch; 61h:16m:32s remains)
INFO - root - 2017-12-07 22:56:46.775816: step 27940, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.633 sec/batch; 62h:45m:16s remains)
INFO - root - 2017-12-07 22:57:03.105576: step 27950, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 1.663 sec/batch; 63h:53m:24s remains)
INFO - root - 2017-12-07 22:57:19.556189: step 27960, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.661 sec/batch; 63h:49m:22s remains)
INFO - root - 2017-12-07 22:57:35.744513: step 27970, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.675 sec/batch; 64h:20m:49s remains)
INFO - root - 2017-12-07 22:57:51.889060: step 27980, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.531 sec/batch; 58h:47m:16s remains)
INFO - root - 2017-12-07 22:58:08.221696: step 27990, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.549 sec/batch; 59h:28m:56s remains)
INFO - root - 2017-12-07 22:58:24.542548: step 28000, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.567 sec/batch; 60h:10m:35s remains)
2017-12-07 22:58:26.060249: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3737173 -4.3421564 -4.2648559 -4.1815233 -4.1485848 -4.1621866 -4.20089 -4.249012 -4.2870994 -4.3007412 -4.3001657 -4.2996507 -4.3066998 -4.3177648 -4.326756][-4.3811951 -4.3483057 -4.2685504 -4.1817241 -4.1386819 -4.1423159 -4.1814303 -4.2352939 -4.2791739 -4.2941709 -4.2910433 -4.2860742 -4.2901978 -4.3010759 -4.3101268][-4.3814988 -4.3446703 -4.2627735 -4.1736145 -4.1234622 -4.1195879 -4.1620312 -4.2237611 -4.2720995 -4.2878933 -4.2832446 -4.275547 -4.27474 -4.2820415 -4.2889776][-4.3790131 -4.3401232 -4.2555494 -4.1643271 -4.109498 -4.1028934 -4.1486659 -4.21656 -4.2692089 -4.2862635 -4.2801776 -4.27009 -4.2637181 -4.2673388 -4.2725625][-4.3772917 -4.3370695 -4.2513437 -4.1579008 -4.1006184 -4.0915256 -4.1367307 -4.2051883 -4.2611427 -4.2815437 -4.2774506 -4.2649217 -4.25136 -4.2489834 -4.25091][-4.3784456 -4.3399987 -4.2555985 -4.1600213 -4.0978975 -4.0821357 -4.119 -4.1761279 -4.2330589 -4.2641611 -4.2710314 -4.2634735 -4.2457085 -4.2363725 -4.2333536][-4.3779464 -4.3371186 -4.2481771 -4.1484914 -4.081707 -4.0611119 -4.0856352 -4.1238689 -4.180182 -4.2314262 -4.2573204 -4.2620754 -4.247129 -4.23388 -4.230083][-4.3757281 -4.3298283 -4.2327609 -4.1267996 -4.0569959 -4.0392308 -4.0561996 -4.0778441 -4.1314621 -4.2014108 -4.2455206 -4.2628226 -4.2571373 -4.2462897 -4.2438855][-4.3720231 -4.3226657 -4.2192588 -4.108232 -4.0429149 -4.0301862 -4.0433116 -4.0532279 -4.0985208 -4.1770973 -4.2327342 -4.259582 -4.2617722 -4.2556748 -4.255764][-4.3680453 -4.3177719 -4.2105303 -4.0972853 -4.0363393 -4.0279117 -4.0391073 -4.0448942 -4.0804982 -4.1587553 -4.2202511 -4.2528934 -4.2623296 -4.2626457 -4.2662749][-4.3678737 -4.3209386 -4.2184639 -4.1082792 -4.0519023 -4.0477705 -4.0605073 -4.0691676 -4.1010451 -4.1690726 -4.2239594 -4.2538218 -4.266902 -4.2724433 -4.2766495][-4.372581 -4.3344278 -4.2460084 -4.1493587 -4.1032958 -4.103858 -4.114151 -4.1236968 -4.1528611 -4.2059617 -4.248003 -4.2704554 -4.2824454 -4.29188 -4.2970982][-4.3753352 -4.3478947 -4.27809 -4.2029181 -4.17015 -4.1753597 -4.1855383 -4.196732 -4.2183857 -4.2532287 -4.2811446 -4.2976551 -4.3078465 -4.3173871 -4.3236313][-4.37199 -4.3555536 -4.3061991 -4.25089 -4.2275696 -4.2330775 -4.2434654 -4.2559175 -4.269897 -4.2886319 -4.3047886 -4.3160973 -4.3242116 -4.3314233 -4.3363662][-4.3658571 -4.3567524 -4.3236618 -4.2845945 -4.2675948 -4.2725158 -4.2827554 -4.2950964 -4.305531 -4.3138595 -4.3200264 -4.3261447 -4.3327589 -4.3390613 -4.3423858]]...]
INFO - root - 2017-12-07 22:58:42.226334: step 28010, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.612 sec/batch; 61h:53m:57s remains)
INFO - root - 2017-12-07 22:58:58.461326: step 28020, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.687 sec/batch; 64h:46m:11s remains)
INFO - root - 2017-12-07 22:59:14.655600: step 28030, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.581 sec/batch; 60h:41m:22s remains)
INFO - root - 2017-12-07 22:59:30.782694: step 28040, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.630 sec/batch; 62h:34m:39s remains)
INFO - root - 2017-12-07 22:59:47.062770: step 28050, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.677 sec/batch; 64h:22m:24s remains)
INFO - root - 2017-12-07 23:00:03.174803: step 28060, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.584 sec/batch; 60h:47m:37s remains)
INFO - root - 2017-12-07 23:00:19.527221: step 28070, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 1.626 sec/batch; 62h:24m:33s remains)
INFO - root - 2017-12-07 23:00:35.858890: step 28080, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.723 sec/batch; 66h:08m:54s remains)
INFO - root - 2017-12-07 23:00:52.168867: step 28090, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.583 sec/batch; 60h:45m:24s remains)
INFO - root - 2017-12-07 23:01:08.450736: step 28100, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.703 sec/batch; 65h:21m:22s remains)
2017-12-07 23:01:09.914426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2673383 -4.2774482 -4.2936721 -4.2981415 -4.2886095 -4.2736459 -4.2585068 -4.2393775 -4.2241087 -4.2046409 -4.1923666 -4.204536 -4.2401209 -4.2767973 -4.3045907][-4.2350841 -4.247283 -4.2638931 -4.2624702 -4.2460089 -4.2290778 -4.2106676 -4.184638 -4.1664829 -4.1459541 -4.14202 -4.1686077 -4.2185736 -4.263988 -4.2908521][-4.2172766 -4.2309809 -4.2444224 -4.2349768 -4.2146244 -4.2045913 -4.193717 -4.1676164 -4.148705 -4.1286383 -4.1278653 -4.1580796 -4.2114167 -4.2605824 -4.286232][-4.2052097 -4.2181025 -4.2236242 -4.2079406 -4.1876087 -4.1872597 -4.1901608 -4.170063 -4.1512437 -4.1343074 -4.1366167 -4.1663513 -4.2164664 -4.2617106 -4.2872982][-4.1936712 -4.2031269 -4.2011957 -4.1821642 -4.1636963 -4.1690364 -4.1851845 -4.1788678 -4.1692739 -4.1569114 -4.1590185 -4.184639 -4.2253046 -4.2592487 -4.2834067][-4.1844029 -4.188221 -4.1823449 -4.162271 -4.1440859 -4.1456304 -4.1641574 -4.1707559 -4.1776791 -4.1712432 -4.1651454 -4.182291 -4.21465 -4.2428164 -4.2696762][-4.1720552 -4.1742873 -4.1651959 -4.138886 -4.1092682 -4.1005473 -4.1158295 -4.1322961 -4.1543074 -4.1535454 -4.1407781 -4.1518526 -4.1835055 -4.2167344 -4.2531509][-4.1693683 -4.1716518 -4.1527863 -4.1119 -4.0625196 -4.0357976 -4.0432324 -4.0663853 -4.1025233 -4.1110563 -4.1039925 -4.1163139 -4.154778 -4.1995873 -4.2440372][-4.1755972 -4.1753173 -4.1455784 -4.0910797 -4.0271144 -3.9828076 -3.9761128 -3.9960222 -4.0386429 -4.0573392 -4.068738 -4.0947914 -4.1470051 -4.2056389 -4.2512617][-4.1880727 -4.1847496 -4.1494751 -4.0989461 -4.0459237 -4.0043364 -3.9921057 -4.0025 -4.0360613 -4.0581279 -4.079844 -4.114542 -4.1712685 -4.2297487 -4.2671432][-4.223763 -4.2164197 -4.1834559 -4.1472378 -4.1189256 -4.0951395 -4.0872626 -4.0918403 -4.1124363 -4.1260366 -4.1423793 -4.1717086 -4.2169652 -4.2593517 -4.2847366][-4.2739682 -4.2666655 -4.240869 -4.2160344 -4.2001796 -4.1900964 -4.1884093 -4.1919131 -4.2045183 -4.2112541 -4.2221403 -4.2404666 -4.2679367 -4.2901955 -4.3027134][-4.3090839 -4.3073087 -4.2933488 -4.2783055 -4.2669163 -4.2619257 -4.2606907 -4.260848 -4.2687707 -4.2759781 -4.2862654 -4.2963085 -4.3079104 -4.31531 -4.3208833][-4.3220043 -4.3220415 -4.3162742 -4.309196 -4.3018889 -4.2984262 -4.2944317 -4.2920408 -4.2943025 -4.30112 -4.3109803 -4.3176775 -4.3221903 -4.325726 -4.3308616][-4.328124 -4.3277063 -4.3241534 -4.3191628 -4.3149014 -4.3128223 -4.310638 -4.3083763 -4.3083024 -4.3135142 -4.3213167 -4.3254232 -4.3278217 -4.3328447 -4.3398309]]...]
INFO - root - 2017-12-07 23:01:26.162380: step 28110, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.728 sec/batch; 66h:19m:11s remains)
INFO - root - 2017-12-07 23:01:42.417467: step 28120, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.612 sec/batch; 61h:51m:04s remains)
INFO - root - 2017-12-07 23:01:58.731828: step 28130, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.661 sec/batch; 63h:43m:33s remains)
INFO - root - 2017-12-07 23:02:15.135577: step 28140, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.611 sec/batch; 61h:49m:15s remains)
INFO - root - 2017-12-07 23:02:31.535177: step 28150, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.672 sec/batch; 64h:09m:02s remains)
INFO - root - 2017-12-07 23:02:47.614695: step 28160, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.602 sec/batch; 61h:25m:51s remains)
INFO - root - 2017-12-07 23:03:03.871250: step 28170, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.554 sec/batch; 59h:35m:58s remains)
INFO - root - 2017-12-07 23:03:20.338932: step 28180, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.735 sec/batch; 66h:32m:29s remains)
INFO - root - 2017-12-07 23:03:36.420627: step 28190, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.602 sec/batch; 61h:26m:24s remains)
INFO - root - 2017-12-07 23:03:52.991406: step 28200, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.666 sec/batch; 63h:52m:28s remains)
2017-12-07 23:03:54.334599: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3333421 -4.3326406 -4.3286128 -4.3183851 -4.3092961 -4.30854 -4.3119411 -4.3177581 -4.3262606 -4.3306456 -4.3278 -4.3145423 -4.2998471 -4.287519 -4.2817583][-4.3320651 -4.3291168 -4.3234911 -4.3084521 -4.292263 -4.2867641 -4.288795 -4.2897105 -4.3008904 -4.3088841 -4.3026605 -4.2851262 -4.2692027 -4.2568135 -4.2538905][-4.32705 -4.3189378 -4.3112812 -4.2911625 -4.2647061 -4.2516737 -4.2522435 -4.2462 -4.2621913 -4.276382 -4.2670722 -4.25097 -4.2398105 -4.2309284 -4.2314296][-4.3202572 -4.3064556 -4.2945042 -4.270112 -4.2338648 -4.21324 -4.2082429 -4.1885715 -4.2100568 -4.2314019 -4.2238469 -4.210422 -4.2014961 -4.1955628 -4.2022915][-4.3030038 -4.2853141 -4.2693362 -4.2386017 -4.1885715 -4.1602955 -4.1472363 -4.1146197 -4.1408072 -4.1702247 -4.1694489 -4.1592464 -4.1509295 -4.1468954 -4.1652274][-4.2858434 -4.2647009 -4.2439194 -4.2074738 -4.1431813 -4.1038842 -4.07678 -4.0268154 -4.0627317 -4.1173086 -4.1303425 -4.1264086 -4.1208334 -4.1176596 -4.1446271][-4.2711973 -4.2481012 -4.2227778 -4.18192 -4.1041527 -4.0473104 -4.00361 -3.9395289 -3.9883697 -4.0758243 -4.1015172 -4.1068864 -4.1061883 -4.1022182 -4.1365848][-4.248806 -4.2276864 -4.2070079 -4.1649532 -4.083921 -4.017478 -3.9645352 -3.899514 -3.9600966 -4.0670366 -4.1003728 -4.1117082 -4.1135812 -4.107893 -4.1437182][-4.2375183 -4.2257142 -4.2214251 -4.19471 -4.1299086 -4.0664458 -4.0148029 -3.9582949 -4.0049319 -4.0905819 -4.11853 -4.1276283 -4.1272964 -4.119421 -4.1525064][-4.2300019 -4.2282467 -4.2357111 -4.2240777 -4.1759839 -4.1202669 -4.0672951 -4.0158129 -4.0432053 -4.1041579 -4.1324744 -4.1446056 -4.1471462 -4.1441488 -4.1703558][-4.2284641 -4.2372403 -4.2561111 -4.2583933 -4.2287755 -4.1854234 -4.1375546 -4.0928168 -4.1070747 -4.1454015 -4.1708555 -4.1849837 -4.1942863 -4.19668 -4.211132][-4.2479978 -4.2630515 -4.2868361 -4.2957978 -4.2772045 -4.2454886 -4.2082586 -4.1762552 -4.180913 -4.2031035 -4.2229791 -4.2358971 -4.2473536 -4.2521219 -4.2580018][-4.2718816 -4.2840061 -4.3029218 -4.3135176 -4.3054395 -4.2851543 -4.2582822 -4.2385578 -4.2426262 -4.2581339 -4.275331 -4.2873473 -4.2956924 -4.2972107 -4.2971258][-4.2908044 -4.2975283 -4.310441 -4.3208671 -4.3202558 -4.3106575 -4.2951279 -4.2859983 -4.291872 -4.3033 -4.32148 -4.3311667 -4.3342876 -4.3301048 -4.3243666][-4.3082018 -4.3093915 -4.3152723 -4.3211875 -4.32165 -4.3176723 -4.3087807 -4.3045692 -4.3086634 -4.3180318 -4.3347259 -4.3434277 -4.3448853 -4.3410406 -4.3365]]...]
INFO - root - 2017-12-07 23:04:10.577520: step 28210, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.670 sec/batch; 64h:02m:34s remains)
INFO - root - 2017-12-07 23:04:26.703998: step 28220, loss = 2.05, batch loss = 1.99 (10.2 examples/sec; 1.572 sec/batch; 60h:16m:36s remains)
INFO - root - 2017-12-07 23:04:42.989308: step 28230, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.729 sec/batch; 66h:18m:17s remains)
INFO - root - 2017-12-07 23:04:59.227625: step 28240, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 1.632 sec/batch; 62h:33m:56s remains)
INFO - root - 2017-12-07 23:05:15.590356: step 28250, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.736 sec/batch; 66h:33m:56s remains)
INFO - root - 2017-12-07 23:05:31.867585: step 28260, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.633 sec/batch; 62h:35m:43s remains)
INFO - root - 2017-12-07 23:05:48.086990: step 28270, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.680 sec/batch; 64h:23m:12s remains)
INFO - root - 2017-12-07 23:06:04.360128: step 28280, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 1.646 sec/batch; 63h:06m:04s remains)
INFO - root - 2017-12-07 23:06:20.634516: step 28290, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.577 sec/batch; 60h:25m:26s remains)
INFO - root - 2017-12-07 23:06:37.013441: step 28300, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.709 sec/batch; 65h:28m:29s remains)
2017-12-07 23:06:38.403343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3502188 -4.3393965 -4.3288631 -4.3183541 -4.310504 -4.3072176 -4.3073969 -4.3051639 -4.3032045 -4.3076949 -4.3159552 -4.3246179 -4.3351474 -4.3430939 -4.3489628][-4.3479071 -4.3298035 -4.3124952 -4.2927041 -4.2759519 -4.26748 -4.263216 -4.2529116 -4.2462535 -4.2558985 -4.2736835 -4.29074 -4.3084645 -4.3236394 -4.3361559][-4.3451076 -4.324326 -4.3017387 -4.2743473 -4.2487431 -4.2343593 -4.2234035 -4.1969867 -4.1810989 -4.1970696 -4.2264338 -4.2540507 -4.2787042 -4.3009214 -4.31845][-4.338623 -4.3187575 -4.2915897 -4.2596488 -4.2339277 -4.2180991 -4.2003517 -4.1588359 -4.1371732 -4.1611757 -4.2011404 -4.2378163 -4.267014 -4.2888827 -4.3043771][-4.3292947 -4.3109851 -4.2828932 -4.2489972 -4.225306 -4.202847 -4.1664853 -4.1080866 -4.0897636 -4.1300168 -4.1836281 -4.2304325 -4.2641063 -4.2834563 -4.2945108][-4.3181229 -4.2915573 -4.2550373 -4.2155886 -4.1890192 -4.1490793 -4.0774059 -3.9957502 -3.9901726 -4.0605145 -4.1336522 -4.1950769 -4.24095 -4.2675343 -4.2817335][-4.3012457 -4.2600956 -4.2081814 -4.1565323 -4.1144147 -4.0414114 -3.9235995 -3.8184564 -3.8409829 -3.9661839 -4.071023 -4.1467214 -4.2033825 -4.2407537 -4.2632737][-4.2869134 -4.2328625 -4.1696 -4.1035051 -4.03863 -3.9316278 -3.7808871 -3.6585109 -3.7021608 -3.8765364 -4.0149264 -4.1031389 -4.1706872 -4.2203546 -4.2507648][-4.2913923 -4.2408781 -4.1904807 -4.1374 -4.0852222 -3.9996672 -3.8798242 -3.7663379 -3.7868838 -3.9271395 -4.0425539 -4.1147165 -4.1764846 -4.2263975 -4.2569304][-4.3089828 -4.2708516 -4.2400365 -4.2121038 -4.1848836 -4.144125 -4.0779085 -3.9901175 -3.9803314 -4.0605145 -4.134511 -4.1793966 -4.2206497 -4.2580595 -4.2803268][-4.3242016 -4.2963347 -4.274035 -4.2522678 -4.2305179 -4.2121449 -4.1781859 -4.1152124 -4.0945377 -4.1403785 -4.1922436 -4.2264547 -4.2575092 -4.2842965 -4.301682][-4.3272319 -4.3006496 -4.2797046 -4.2613363 -4.2434969 -4.2393918 -4.2259488 -4.1878724 -4.170197 -4.1959767 -4.230423 -4.2573977 -4.2821345 -4.3015261 -4.3159733][-4.3259692 -4.2969313 -4.2768745 -4.2651143 -4.258976 -4.2676153 -4.2669096 -4.2475457 -4.2359271 -4.2502384 -4.270328 -4.2859945 -4.3000112 -4.31169 -4.3246574][-4.3288326 -4.3047247 -4.287643 -4.2794476 -4.2820005 -4.2974529 -4.3019552 -4.2916322 -4.2832265 -4.2900424 -4.3009663 -4.3080254 -4.3152113 -4.3233337 -4.3332477][-4.337873 -4.3221331 -4.3098464 -4.3045764 -4.3089361 -4.3208356 -4.3246541 -4.3206143 -4.3160143 -4.3184547 -4.3243437 -4.3280892 -4.3319669 -4.3377337 -4.3436465]]...]
INFO - root - 2017-12-07 23:06:54.654536: step 28310, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.644 sec/batch; 62h:59m:27s remains)
INFO - root - 2017-12-07 23:07:10.997457: step 28320, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.625 sec/batch; 62h:16m:02s remains)
INFO - root - 2017-12-07 23:07:27.477667: step 28330, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.718 sec/batch; 65h:48m:56s remains)
INFO - root - 2017-12-07 23:07:43.178777: step 28340, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.602 sec/batch; 61h:22m:59s remains)
INFO - root - 2017-12-07 23:07:59.609517: step 28350, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.622 sec/batch; 62h:07m:27s remains)
INFO - root - 2017-12-07 23:08:15.911632: step 28360, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.711 sec/batch; 65h:31m:13s remains)
INFO - root - 2017-12-07 23:08:32.038524: step 28370, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.609 sec/batch; 61h:37m:57s remains)
INFO - root - 2017-12-07 23:08:48.438534: step 28380, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 1.673 sec/batch; 64h:04m:28s remains)
INFO - root - 2017-12-07 23:09:04.513452: step 28390, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.583 sec/batch; 60h:38m:07s remains)
INFO - root - 2017-12-07 23:09:20.571184: step 28400, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.649 sec/batch; 63h:08m:06s remains)
2017-12-07 23:09:21.953196: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2943473 -4.3075809 -4.3185382 -4.3207588 -4.3196626 -4.3178844 -4.3161559 -4.3150587 -4.3136311 -4.309247 -4.3016057 -4.2951674 -4.2955227 -4.3023295 -4.3104858][-4.2732072 -4.2953486 -4.314436 -4.3212976 -4.3218818 -4.3206854 -4.3155656 -4.308435 -4.3008084 -4.2914591 -4.2825742 -4.2776804 -4.2820616 -4.2933874 -4.3039737][-4.2608604 -4.2889977 -4.3106403 -4.3152361 -4.3149629 -4.3107271 -4.2987332 -4.2860384 -4.2734513 -4.2606444 -4.2494259 -4.2473426 -4.2588687 -4.276031 -4.2902479][-4.2643328 -4.2982306 -4.314518 -4.3083553 -4.2979236 -4.2836046 -4.264123 -4.2481728 -4.2355843 -4.2229576 -4.2130709 -4.2171078 -4.2371039 -4.259841 -4.2767487][-4.2707005 -4.300797 -4.3082232 -4.29041 -4.2635989 -4.2368584 -4.2075486 -4.1905966 -4.1838713 -4.1822863 -4.1852059 -4.2018266 -4.2301555 -4.2573638 -4.2740221][-4.2673564 -4.284699 -4.2764006 -4.243557 -4.1993127 -4.1535287 -4.1139669 -4.1044488 -4.1174116 -4.1375794 -4.1657934 -4.199337 -4.2343607 -4.2633905 -4.2798429][-4.2415476 -4.2419562 -4.2146344 -4.1653957 -4.1010447 -4.031702 -3.9821353 -3.9844267 -4.0289803 -4.0818973 -4.1414638 -4.1953344 -4.2376924 -4.2668314 -4.2839365][-4.2124496 -4.2027984 -4.1658421 -4.111949 -4.0416489 -3.9668474 -3.9179473 -3.9280229 -3.9927661 -4.0675068 -4.1429448 -4.2032962 -4.2425079 -4.2656012 -4.2820029][-4.1988907 -4.1932626 -4.1639962 -4.1188354 -4.0622115 -4.0093784 -3.9820836 -3.9944024 -4.0488272 -4.1167974 -4.1808314 -4.2284808 -4.2545776 -4.2690692 -4.2808518][-4.2033525 -4.203826 -4.1870418 -4.1569757 -4.1211996 -4.0933948 -4.0793648 -4.0855007 -4.1223788 -4.1722045 -4.2170997 -4.2512879 -4.268517 -4.2769032 -4.2852206][-4.2241783 -4.2314277 -4.2260308 -4.2076564 -4.1875529 -4.17223 -4.1599765 -4.1589479 -4.1771464 -4.2102585 -4.2426648 -4.2690463 -4.2827148 -4.2898397 -4.2970438][-4.254673 -4.2671509 -4.2695041 -4.2576513 -4.2428446 -4.2311893 -4.2187223 -4.2145805 -4.2244716 -4.2450643 -4.2657413 -4.2854776 -4.2986245 -4.3078303 -4.3141165][-4.2794147 -4.2909985 -4.2928271 -4.2803802 -4.2657847 -4.2565393 -4.2472963 -4.243505 -4.250669 -4.264955 -4.280304 -4.2968588 -4.3104048 -4.3209543 -4.3270316][-4.2918491 -4.2992325 -4.2992978 -4.2873573 -4.2757974 -4.2691474 -4.2635922 -4.2614255 -4.2670164 -4.2773795 -4.2897887 -4.3036203 -4.3167825 -4.3278904 -4.3342719][-4.2988849 -4.3039837 -4.3039169 -4.2962823 -4.2896376 -4.2858014 -4.2822762 -4.2810993 -4.2851577 -4.2915783 -4.2993207 -4.3083777 -4.3179746 -4.3277779 -4.3348112]]...]
INFO - root - 2017-12-07 23:09:38.296058: step 28410, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.668 sec/batch; 63h:52m:17s remains)
INFO - root - 2017-12-07 23:09:54.606805: step 28420, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 1.628 sec/batch; 62h:19m:17s remains)
INFO - root - 2017-12-07 23:10:10.762546: step 28430, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.730 sec/batch; 66h:14m:15s remains)
INFO - root - 2017-12-07 23:10:27.023221: step 28440, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.541 sec/batch; 58h:59m:31s remains)
INFO - root - 2017-12-07 23:10:43.233376: step 28450, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.667 sec/batch; 63h:49m:31s remains)
INFO - root - 2017-12-07 23:10:59.093656: step 28460, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 1.580 sec/batch; 60h:28m:37s remains)
INFO - root - 2017-12-07 23:11:15.481608: step 28470, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.586 sec/batch; 60h:43m:04s remains)
INFO - root - 2017-12-07 23:11:31.765825: step 28480, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.671 sec/batch; 63h:57m:21s remains)
INFO - root - 2017-12-07 23:11:48.069748: step 28490, loss = 2.10, batch loss = 2.04 (9.8 examples/sec; 1.641 sec/batch; 62h:47m:00s remains)
INFO - root - 2017-12-07 23:12:04.388474: step 28500, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.715 sec/batch; 65h:37m:14s remains)
2017-12-07 23:12:05.806626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3073268 -4.2787986 -4.2406268 -4.2054925 -4.1864982 -4.1893582 -4.1964984 -4.2006278 -4.2089329 -4.2105832 -4.2048235 -4.2081704 -4.2282748 -4.2387981 -4.2283363][-4.3022604 -4.2751493 -4.240314 -4.2102723 -4.1920171 -4.1938958 -4.1991353 -4.2015896 -4.2071805 -4.2059708 -4.20137 -4.2075672 -4.2296786 -4.2382 -4.220994][-4.3029304 -4.278667 -4.2475343 -4.2189274 -4.2011204 -4.199079 -4.1969638 -4.1928034 -4.1904821 -4.1875453 -4.1880302 -4.2003565 -4.219584 -4.2188654 -4.1927462][-4.3081088 -4.2874079 -4.25893 -4.2286148 -4.2059145 -4.1917558 -4.1713181 -4.1512432 -4.1418829 -4.140708 -4.1509771 -4.17098 -4.1829824 -4.1686711 -4.1354995][-4.3126755 -4.2921748 -4.2615967 -4.2238011 -4.1857815 -4.1469016 -4.0976243 -4.054462 -4.0474076 -4.0689597 -4.1017222 -4.1320448 -4.13797 -4.1119142 -4.0759621][-4.3125477 -4.2882638 -4.2506042 -4.1997 -4.1400228 -4.0699906 -3.9830184 -3.9148362 -3.9308465 -3.99965 -4.0631437 -4.1015782 -4.1049047 -4.07807 -4.0463691][-4.3065114 -4.2753983 -4.2278261 -4.1626544 -4.0842347 -3.9878159 -3.8634458 -3.775373 -3.8318858 -3.9608326 -4.0521851 -4.0941424 -4.1009007 -4.0826769 -4.0549946][-4.29799 -4.2583866 -4.2011509 -4.1273537 -4.0409622 -3.936902 -3.8058941 -3.7320704 -3.8281744 -3.9825113 -4.0804768 -4.1245732 -4.1388016 -4.127429 -4.0920196][-4.2937727 -4.2497082 -4.1908646 -4.1195507 -4.0423064 -3.9621453 -3.8765295 -3.8524611 -3.9439631 -4.0660858 -4.1399465 -4.1763749 -4.1927657 -4.1798973 -4.1288805][-4.2944336 -4.2497249 -4.1930914 -4.1309743 -4.0731697 -4.0318866 -4.0029893 -4.0137749 -4.0775037 -4.1494288 -4.1882873 -4.2073874 -4.2149982 -4.1972589 -4.1394606][-4.2930403 -4.2487431 -4.1955791 -4.1463122 -4.1097941 -4.1020508 -4.1082087 -4.1353588 -4.1773281 -4.21133 -4.2228689 -4.2275023 -4.2237945 -4.20056 -4.1466546][-4.2929506 -4.251873 -4.2042847 -4.1670113 -4.1454096 -4.1496086 -4.1668468 -4.1985545 -4.2324772 -4.2500625 -4.2515574 -4.2504754 -4.2407784 -4.2169852 -4.1743374][-4.2960758 -4.2584705 -4.2158637 -4.1829009 -4.1630468 -4.1605611 -4.16911 -4.1978049 -4.2354221 -4.2543483 -4.2601662 -4.2596297 -4.2497139 -4.2331734 -4.209156][-4.3015494 -4.2666993 -4.2273908 -4.1936026 -4.1676145 -4.1466026 -4.1378632 -4.1650109 -4.2088571 -4.2324934 -4.2447729 -4.247685 -4.2437215 -4.2388268 -4.2335587][-4.3055973 -4.2716017 -4.2324357 -4.1963282 -4.1626406 -4.1231709 -4.0962996 -4.1138124 -4.1576881 -4.1846919 -4.2042322 -4.2141104 -4.2242284 -4.2336292 -4.2413955]]...]
INFO - root - 2017-12-07 23:12:21.999966: step 28510, loss = 2.09, batch loss = 2.04 (10.0 examples/sec; 1.597 sec/batch; 61h:06m:04s remains)
INFO - root - 2017-12-07 23:12:38.399139: step 28520, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.618 sec/batch; 61h:54m:23s remains)
INFO - root - 2017-12-07 23:12:54.779988: step 28530, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.673 sec/batch; 63h:59m:42s remains)
INFO - root - 2017-12-07 23:13:10.908880: step 28540, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.607 sec/batch; 61h:27m:12s remains)
INFO - root - 2017-12-07 23:13:27.149875: step 28550, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.667 sec/batch; 63h:45m:09s remains)
INFO - root - 2017-12-07 23:13:43.404602: step 28560, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.595 sec/batch; 61h:00m:04s remains)
INFO - root - 2017-12-07 23:13:59.496994: step 28570, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.669 sec/batch; 63h:49m:15s remains)
INFO - root - 2017-12-07 23:14:15.571666: step 28580, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.599 sec/batch; 61h:09m:55s remains)
INFO - root - 2017-12-07 23:14:31.939998: step 28590, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.618 sec/batch; 61h:52m:15s remains)
INFO - root - 2017-12-07 23:14:48.128963: step 28600, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.674 sec/batch; 64h:01m:01s remains)
2017-12-07 23:14:49.544525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2808781 -4.2960572 -4.3141236 -4.3228912 -4.322 -4.3136168 -4.300559 -4.2919154 -4.2938194 -4.2989798 -4.30219 -4.3012686 -4.2956996 -4.283628 -4.2639236][-4.2717175 -4.2916746 -4.3162208 -4.3278384 -4.3282142 -4.3199077 -4.3071 -4.2981625 -4.3006983 -4.3087392 -4.3143573 -4.3144832 -4.3067265 -4.2889128 -4.2626476][-4.2508092 -4.2717886 -4.2973933 -4.3110194 -4.3128047 -4.3059511 -4.2953205 -4.2893262 -4.2937827 -4.304204 -4.3114467 -4.3129082 -4.3047175 -4.2848978 -4.254528][-4.2250223 -4.242692 -4.2652459 -4.2786689 -4.2793427 -4.273479 -4.2667308 -4.2658482 -4.2746816 -4.28865 -4.2986913 -4.3017721 -4.2945919 -4.2765193 -4.2463341][-4.1984177 -4.2091351 -4.2253261 -4.2344632 -4.2327452 -4.2283282 -4.2287083 -4.235919 -4.2509818 -4.2705274 -4.2846804 -4.2886996 -4.2841773 -4.2705164 -4.24272][-4.1597171 -4.1708546 -4.1864014 -4.1942644 -4.1931791 -4.1913261 -4.1970134 -4.2121196 -4.23392 -4.2558031 -4.2712111 -4.2761869 -4.2747388 -4.2653756 -4.2446947][-4.1134973 -4.1367035 -4.158855 -4.1702023 -4.1720824 -4.1702962 -4.1754994 -4.1936779 -4.2192774 -4.2407889 -4.2564034 -4.2630944 -4.2626114 -4.2561159 -4.240634][-4.0582747 -4.092803 -4.1243448 -4.1439338 -4.1539006 -4.1554518 -4.1600609 -4.1745286 -4.1970191 -4.2146716 -4.229177 -4.2375827 -4.2389488 -4.2334518 -4.2201529][-4.0148 -4.0506868 -4.0862961 -4.1124682 -4.129693 -4.1362157 -4.1395316 -4.149 -4.1662159 -4.182158 -4.1975937 -4.2104955 -4.2165985 -4.2094526 -4.1947641][-4.0079608 -4.0321221 -4.0644069 -4.0920172 -4.1050148 -4.1048527 -4.102941 -4.1124249 -4.1347275 -4.1560469 -4.1738033 -4.1921916 -4.2056575 -4.2005172 -4.1855855][-4.0359182 -4.0439706 -4.0698638 -4.0934119 -4.0934434 -4.0765796 -4.0640144 -4.0729437 -4.1015782 -4.1318645 -4.1560373 -4.1783581 -4.1952109 -4.1913357 -4.1767454][-4.0852251 -4.0784187 -4.093297 -4.10866 -4.0994468 -4.0700164 -4.0427485 -4.0400629 -4.0636654 -4.0972443 -4.1268554 -4.15116 -4.1666117 -4.1622658 -4.1472511][-4.1532145 -4.1420012 -4.1485438 -4.1543922 -4.1376538 -4.0967665 -4.0497322 -4.022336 -4.0245872 -4.0499382 -4.0814095 -4.106071 -4.1163263 -4.106235 -4.0869851][-4.2185831 -4.2086811 -4.2104683 -4.2102728 -4.1898079 -4.1436777 -4.0818124 -4.02967 -4.0077758 -4.0189896 -4.0460291 -4.0692468 -4.0759139 -4.0596719 -4.033648][-4.2518196 -4.2474327 -4.2501216 -4.2497368 -4.2345872 -4.1972213 -4.1402688 -4.0845742 -4.0502367 -4.0445104 -4.0615978 -4.0794277 -4.0821881 -4.0641537 -4.0337429]]...]
INFO - root - 2017-12-07 23:15:05.662155: step 28610, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 1.556 sec/batch; 59h:28m:27s remains)
INFO - root - 2017-12-07 23:15:21.697216: step 28620, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.617 sec/batch; 61h:49m:23s remains)
INFO - root - 2017-12-07 23:15:37.986558: step 28630, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.684 sec/batch; 64h:22m:46s remains)
INFO - root - 2017-12-07 23:15:54.138576: step 28640, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.618 sec/batch; 61h:50m:24s remains)
INFO - root - 2017-12-07 23:16:10.590036: step 28650, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.559 sec/batch; 59h:36m:19s remains)
INFO - root - 2017-12-07 23:16:26.725268: step 28660, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.640 sec/batch; 62h:41m:55s remains)
INFO - root - 2017-12-07 23:16:43.059218: step 28670, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.541 sec/batch; 58h:53m:55s remains)
INFO - root - 2017-12-07 23:16:59.400394: step 28680, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.713 sec/batch; 65h:27m:50s remains)
INFO - root - 2017-12-07 23:17:15.771354: step 28690, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 1.760 sec/batch; 67h:14m:24s remains)
INFO - root - 2017-12-07 23:17:32.058148: step 28700, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.712 sec/batch; 65h:24m:05s remains)
2017-12-07 23:17:33.357981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3138914 -4.3082633 -4.3023372 -4.2904654 -4.2721028 -4.2551427 -4.245214 -4.2423434 -4.2493172 -4.2582383 -4.2552609 -4.2455568 -4.2307324 -4.2129416 -4.1979489][-4.3261757 -4.319171 -4.3118038 -4.2955704 -4.2664361 -4.2383204 -4.2204971 -4.2166133 -4.2279634 -4.2441764 -4.2509871 -4.2505522 -4.2400322 -4.221096 -4.2110653][-4.30681 -4.2947831 -4.2839103 -4.2633915 -4.2291255 -4.1984076 -4.1752214 -4.1640239 -4.1714325 -4.1937037 -4.2151046 -4.2311707 -4.2257624 -4.20482 -4.1913848][-4.2747965 -4.2549133 -4.2411876 -4.222024 -4.1935253 -4.1665716 -4.1443281 -4.1316285 -4.131402 -4.1490221 -4.1781259 -4.210341 -4.2195306 -4.2005734 -4.1807213][-4.2486162 -4.22449 -4.2104979 -4.1960688 -4.1696324 -4.1432419 -4.1228623 -4.1143804 -4.11026 -4.1156268 -4.1404266 -4.18253 -4.2104688 -4.198585 -4.1735268][-4.2277346 -4.210547 -4.1993251 -4.1836658 -4.1545119 -4.1260247 -4.1057358 -4.0930204 -4.0867376 -4.0892553 -4.1067934 -4.1454477 -4.1824403 -4.185657 -4.162735][-4.2056661 -4.1972542 -4.1892443 -4.1746988 -4.1549621 -4.1255827 -4.092998 -4.05678 -4.0406861 -4.0498772 -4.0699968 -4.1010942 -4.1451216 -4.16517 -4.1586018][-4.1839018 -4.1865072 -4.1839104 -4.1787233 -4.1690445 -4.1406493 -4.087358 -4.0091386 -3.9655633 -3.9877677 -4.0263438 -4.0652103 -4.1196218 -4.1566777 -4.1661129][-4.1776705 -4.1942635 -4.2007666 -4.2021174 -4.1983333 -4.1787014 -4.1236119 -4.0232391 -3.9492323 -3.9678121 -4.0166316 -4.0621924 -4.1227903 -4.165288 -4.1799612][-4.1763396 -4.2010288 -4.2189269 -4.2231088 -4.2266765 -4.2256966 -4.1985335 -4.1217527 -4.0522485 -4.0427217 -4.0605831 -4.088172 -4.140614 -4.1804857 -4.1907687][-4.1736012 -4.1969213 -4.2220688 -4.2341671 -4.2472839 -4.26359 -4.266469 -4.2232971 -4.1696348 -4.1398544 -4.1245184 -4.1274571 -4.1633782 -4.19619 -4.1983185][-4.1753063 -4.1919708 -4.2147541 -4.2369108 -4.2633018 -4.2914772 -4.3045163 -4.2813144 -4.24238 -4.2028165 -4.1726694 -4.1615691 -4.1744161 -4.1951342 -4.18785][-4.1716022 -4.1826878 -4.2060819 -4.2398548 -4.275126 -4.2984276 -4.3053741 -4.2913723 -4.2667623 -4.2297554 -4.1903987 -4.1678977 -4.16128 -4.1632819 -4.1487436][-4.1705246 -4.1799812 -4.2077713 -4.2467923 -4.2784705 -4.2909656 -4.2904391 -4.2804513 -4.2695146 -4.2402797 -4.1947789 -4.1594028 -4.1376705 -4.1265545 -4.1124692][-4.1899443 -4.1972303 -4.2211728 -4.2510381 -4.2702971 -4.2717633 -4.2633457 -4.2555394 -4.2562618 -4.23645 -4.1941495 -4.154068 -4.1171618 -4.0886211 -4.071003]]...]
INFO - root - 2017-12-07 23:17:49.677983: step 28710, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 1.698 sec/batch; 64h:52m:06s remains)
INFO - root - 2017-12-07 23:18:05.924871: step 28720, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.604 sec/batch; 61h:15m:57s remains)
INFO - root - 2017-12-07 23:18:22.200393: step 28730, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.653 sec/batch; 63h:08m:47s remains)
INFO - root - 2017-12-07 23:18:38.490471: step 28740, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 1.590 sec/batch; 60h:43m:55s remains)
INFO - root - 2017-12-07 23:18:54.837968: step 28750, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.561 sec/batch; 59h:36m:26s remains)
INFO - root - 2017-12-07 23:19:11.019273: step 28760, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.685 sec/batch; 64h:20m:20s remains)
INFO - root - 2017-12-07 23:19:27.315863: step 28770, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.638 sec/batch; 62h:32m:57s remains)
INFO - root - 2017-12-07 23:19:43.825676: step 28780, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.650 sec/batch; 63h:00m:01s remains)
INFO - root - 2017-12-07 23:20:00.040746: step 28790, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 1.506 sec/batch; 57h:30m:17s remains)
INFO - root - 2017-12-07 23:20:16.291996: step 28800, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.699 sec/batch; 64h:52m:55s remains)
2017-12-07 23:20:17.569869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.328743 -4.3272548 -4.3287578 -4.3310833 -4.3309636 -4.3278317 -4.3234448 -4.3187122 -4.3161173 -4.3172455 -4.3173018 -4.3169808 -4.3196034 -4.3240266 -4.3263049][-4.327755 -4.3238811 -4.322 -4.3219318 -4.3199677 -4.3147082 -4.30585 -4.295804 -4.2903781 -4.2920194 -4.2915225 -4.2903404 -4.2969413 -4.3052711 -4.3097811][-4.3280611 -4.3192515 -4.3127294 -4.3093114 -4.30489 -4.2963996 -4.2789946 -4.2593436 -4.2477489 -4.251718 -4.2545214 -4.2551069 -4.2657623 -4.2785215 -4.2863731][-4.3160782 -4.2993326 -4.2883706 -4.2822461 -4.2755761 -4.2591968 -4.2303686 -4.201129 -4.1850462 -4.1969962 -4.2087126 -4.2125506 -4.2259097 -4.245295 -4.2605076][-4.2862506 -4.2564507 -4.2368464 -4.2261372 -4.2145452 -4.1877937 -4.1515985 -4.1239643 -4.1146646 -4.1410322 -4.1639767 -4.1711326 -4.1839604 -4.2062416 -4.2297454][-4.251152 -4.204855 -4.1700544 -4.1480927 -4.1252689 -4.0848389 -4.0503573 -4.0414338 -4.0485382 -4.0899878 -4.124424 -4.1360526 -4.1445241 -4.1682954 -4.199964][-4.2392588 -4.1793385 -4.1259413 -4.0849662 -4.0476012 -3.9994259 -3.9747493 -3.9930465 -4.0222607 -4.07347 -4.1121178 -4.1266432 -4.1326246 -4.1559019 -4.1890321][-4.2501097 -4.1827841 -4.118907 -4.0736222 -4.0408549 -3.9996285 -3.9913969 -4.0268426 -4.0615563 -4.1034946 -4.1345286 -4.1461487 -4.1469769 -4.1632895 -4.1909437][-4.2721405 -4.2046494 -4.1392579 -4.1078467 -4.0966544 -4.071465 -4.0744309 -4.1084161 -4.1352754 -4.160563 -4.1790438 -4.1804748 -4.1652126 -4.1657476 -4.1858253][-4.2887053 -4.2260165 -4.1690645 -4.15256 -4.1594658 -4.1519179 -4.1636357 -4.1944556 -4.2129354 -4.2224469 -4.2235794 -4.2067647 -4.17128 -4.1550612 -4.1678281][-4.3067808 -4.2560921 -4.2140923 -4.205678 -4.2156043 -4.2144184 -4.227354 -4.2493725 -4.2582917 -4.256743 -4.2453461 -4.217155 -4.1674533 -4.1413312 -4.1485162][-4.3096943 -4.273159 -4.2419696 -4.2362223 -4.2424192 -4.2401333 -4.2475395 -4.2578392 -4.2571268 -4.2526617 -4.236918 -4.2041588 -4.1515174 -4.1266451 -4.1335707][-4.2957911 -4.2709732 -4.2472839 -4.2431035 -4.2461576 -4.2415075 -4.2435479 -4.2447262 -4.2378721 -4.2323256 -4.2145467 -4.1814718 -4.1354766 -4.1199331 -4.1322308][-4.2772565 -4.2588038 -4.2396336 -4.2355962 -4.2375197 -4.2342944 -4.2373219 -4.2367582 -4.2273712 -4.2217574 -4.2047529 -4.17622 -4.1407046 -4.1365738 -4.1549006][-4.2735629 -4.2590647 -4.2454805 -4.2442603 -4.2464809 -4.2433729 -4.2463984 -4.24442 -4.2348189 -4.2323284 -4.2202992 -4.2004542 -4.1791062 -4.1845746 -4.2000017]]...]
INFO - root - 2017-12-07 23:20:34.133884: step 28810, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.679 sec/batch; 64h:05m:53s remains)
INFO - root - 2017-12-07 23:20:50.274067: step 28820, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.562 sec/batch; 59h:37m:38s remains)
INFO - root - 2017-12-07 23:21:06.727966: step 28830, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.650 sec/batch; 62h:58m:01s remains)
INFO - root - 2017-12-07 23:21:23.097134: step 28840, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.650 sec/batch; 62h:58m:57s remains)
INFO - root - 2017-12-07 23:21:39.344227: step 28850, loss = 2.06, batch loss = 2.01 (10.3 examples/sec; 1.552 sec/batch; 59h:13m:19s remains)
INFO - root - 2017-12-07 23:21:55.629558: step 28860, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.692 sec/batch; 64h:35m:00s remains)
INFO - root - 2017-12-07 23:22:11.755864: step 28870, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.588 sec/batch; 60h:35m:23s remains)
INFO - root - 2017-12-07 23:22:27.945947: step 28880, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.717 sec/batch; 65h:31m:03s remains)
INFO - root - 2017-12-07 23:22:44.238690: step 28890, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 1.629 sec/batch; 62h:09m:21s remains)
INFO - root - 2017-12-07 23:23:00.345259: step 28900, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.560 sec/batch; 59h:30m:30s remains)
2017-12-07 23:23:01.739390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3190002 -4.3183331 -4.3154135 -4.3132267 -4.31217 -4.3089385 -4.3068075 -4.3058019 -4.3055816 -4.3057117 -4.3058648 -4.307261 -4.3085141 -4.3072567 -4.3034749][-4.3144822 -4.315402 -4.3134613 -4.3127956 -4.3133378 -4.309217 -4.3033381 -4.2978792 -4.2964849 -4.2992573 -4.301424 -4.3037429 -4.3049831 -4.3042274 -4.299139][-4.3067145 -4.3078337 -4.3057771 -4.3049183 -4.3032718 -4.2959766 -4.2865267 -4.2771621 -4.2761207 -4.2831798 -4.2889419 -4.2913895 -4.2907844 -4.2883439 -4.2796741][-4.291513 -4.2906518 -4.2861495 -4.2813544 -4.275775 -4.2618136 -4.2446885 -4.2311444 -4.2341909 -4.2499061 -4.2635565 -4.2676935 -4.2629161 -4.2569213 -4.24413][-4.2690287 -4.2667704 -4.2603164 -4.2490988 -4.2352343 -4.2118921 -4.1792541 -4.1534896 -4.1588669 -4.18911 -4.2209172 -4.2353649 -4.2312317 -4.2246819 -4.2093549][-4.247632 -4.2434788 -4.2320151 -4.2104545 -4.1808071 -4.1358023 -4.0767922 -4.0343213 -4.0499325 -4.1069565 -4.1643448 -4.1937551 -4.1964774 -4.1923776 -4.1800761][-4.2217307 -4.2142415 -4.1940517 -4.1588774 -4.109817 -4.0336323 -3.9348814 -3.8646774 -3.8979995 -3.9968431 -4.090529 -4.1423559 -4.1591935 -4.1652265 -4.161118][-4.2076769 -4.1927967 -4.1628771 -4.1160293 -4.0539923 -3.9555984 -3.8250649 -3.7189081 -3.7513545 -3.8790162 -4.0027514 -4.0784631 -4.1147571 -4.1386852 -4.1482205][-4.239841 -4.2230854 -4.1964192 -4.1551757 -4.1058044 -4.0305672 -3.9316974 -3.8419638 -3.8441026 -3.9267147 -4.0206943 -4.0864878 -4.123558 -4.150454 -4.1602287][-4.2759051 -4.2583742 -4.2388992 -4.2124872 -4.1835957 -4.1490426 -4.1064157 -4.0661917 -4.0605583 -4.0927067 -4.1395912 -4.1739345 -4.1894741 -4.2025132 -4.2019148][-4.2814088 -4.2681046 -4.2553306 -4.2407546 -4.2262936 -4.2150979 -4.2029438 -4.1943965 -4.1964855 -4.20675 -4.2202253 -4.2291164 -4.23255 -4.2393069 -4.2363119][-4.2583265 -4.2496452 -4.2412887 -4.2363429 -4.2364759 -4.2397056 -4.2378674 -4.2370529 -4.240705 -4.2406363 -4.2347178 -4.2260308 -4.2274661 -4.2358127 -4.234931][-4.2141604 -4.2023692 -4.1928992 -4.1939263 -4.2055 -4.2197795 -4.2244697 -4.2251644 -4.2285905 -4.2254691 -4.2148762 -4.1989145 -4.1952586 -4.2002592 -4.1991472][-4.1681137 -4.1472359 -4.1314826 -4.1330705 -4.1552062 -4.1772337 -4.1832724 -4.1845775 -4.1851854 -4.1809344 -4.1726727 -4.1573019 -4.14917 -4.1503415 -4.1531706][-4.1396165 -4.1100192 -4.087646 -4.0841045 -4.1055331 -4.1283321 -4.1359439 -4.1378016 -4.1399927 -4.1383128 -4.132566 -4.1200342 -4.11018 -4.1083765 -4.1127715]]...]
INFO - root - 2017-12-07 23:23:17.883812: step 28910, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.560 sec/batch; 59h:30m:14s remains)
INFO - root - 2017-12-07 23:23:34.248053: step 28920, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.691 sec/batch; 64h:29m:38s remains)
INFO - root - 2017-12-07 23:23:50.330403: step 28930, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.584 sec/batch; 60h:25m:40s remains)
INFO - root - 2017-12-07 23:24:06.660182: step 28940, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.748 sec/batch; 66h:40m:36s remains)
INFO - root - 2017-12-07 23:24:22.795994: step 28950, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.571 sec/batch; 59h:55m:16s remains)
INFO - root - 2017-12-07 23:24:38.997255: step 28960, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.675 sec/batch; 63h:51m:56s remains)
INFO - root - 2017-12-07 23:24:55.180067: step 28970, loss = 2.06, batch loss = 2.01 (10.3 examples/sec; 1.560 sec/batch; 59h:29m:18s remains)
INFO - root - 2017-12-07 23:25:11.382346: step 28980, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.624 sec/batch; 61h:55m:17s remains)
INFO - root - 2017-12-07 23:25:27.681050: step 28990, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.637 sec/batch; 62h:25m:39s remains)
INFO - root - 2017-12-07 23:25:43.883789: step 29000, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.606 sec/batch; 61h:14m:13s remains)
2017-12-07 23:25:45.195732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3245273 -4.3255792 -4.3279867 -4.331429 -4.3352909 -4.3377571 -4.3396769 -4.341063 -4.3414936 -4.3415232 -4.3417521 -4.3420506 -4.3424616 -4.3426971 -4.3428855][-4.2977057 -4.300962 -4.3084326 -4.3160806 -4.3229089 -4.3270955 -4.3305478 -4.3339119 -4.3354068 -4.3354635 -4.3360195 -4.3369827 -4.3378792 -4.3386736 -4.3399968][-4.2629237 -4.2703962 -4.2841811 -4.2944469 -4.303472 -4.3101029 -4.3165984 -4.3226757 -4.3256636 -4.3248978 -4.3247666 -4.3259444 -4.3273716 -4.32886 -4.3313074][-4.2151566 -4.2277565 -4.2460933 -4.2579131 -4.2694621 -4.281868 -4.2949052 -4.3063874 -4.3130593 -4.3134346 -4.3137112 -4.3155293 -4.3168249 -4.3181539 -4.3213596][-4.1639605 -4.1792274 -4.197186 -4.2064586 -4.2165027 -4.2336864 -4.2509427 -4.2618136 -4.2687235 -4.2726097 -4.2778244 -4.282639 -4.283906 -4.2848282 -4.2883892][-4.11648 -4.1352592 -4.1517138 -4.1554828 -4.1565657 -4.1694641 -4.1790557 -4.1741548 -4.17126 -4.1803246 -4.1964421 -4.2044272 -4.2040968 -4.20501 -4.2117953][-4.0763049 -4.1025486 -4.1176105 -4.1102734 -4.0934234 -4.0877976 -4.0758491 -4.040626 -4.0190468 -4.0398664 -4.0754623 -4.0870609 -4.0821123 -4.0826588 -4.0954852][-4.0626526 -4.0902033 -4.1000381 -4.0796871 -4.0449982 -4.0202131 -3.982378 -3.9095216 -3.8661299 -3.9081712 -3.9707448 -3.9871016 -3.97745 -3.9811089 -4.00013][-4.0878692 -4.1117334 -4.1183672 -4.0961881 -4.0603328 -4.0311413 -3.985023 -3.9025466 -3.8544576 -3.9085929 -3.9802706 -3.9926443 -3.9743826 -3.9719934 -3.9860575][-4.1399021 -4.1578689 -4.166966 -4.1544056 -4.1313858 -4.1147594 -4.0871696 -4.0340848 -4.0029659 -4.0373921 -4.0806894 -4.0747085 -4.0434132 -4.0305367 -4.0369873][-4.1990108 -4.2112565 -4.2206721 -4.215395 -4.2025065 -4.19563 -4.18366 -4.1589036 -4.1421914 -4.1549439 -4.1707416 -4.1550026 -4.1246047 -4.1094418 -4.1117353][-4.2441397 -4.2534385 -4.26214 -4.2627125 -4.25693 -4.2534146 -4.2499752 -4.2426596 -4.2354021 -4.2356739 -4.2363973 -4.2222767 -4.2017632 -4.19183 -4.190815][-4.26622 -4.269577 -4.2746444 -4.2781248 -4.2778587 -4.2776031 -4.2788916 -4.2805634 -4.2817807 -4.2795758 -4.2762208 -4.2678375 -4.2571664 -4.2532659 -4.2542033][-4.2797093 -4.2775178 -4.2778158 -4.2789707 -4.2798095 -4.280757 -4.2831912 -4.2879829 -4.2933888 -4.2946444 -4.2940865 -4.29125 -4.2885675 -4.2897491 -4.2919583][-4.2844687 -4.2797632 -4.2775183 -4.2763186 -4.2764649 -4.2772436 -4.2789207 -4.2833052 -4.2888889 -4.2914386 -4.2908158 -4.2899446 -4.2913671 -4.2951903 -4.2986312]]...]
INFO - root - 2017-12-07 23:26:01.667553: step 29010, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.572 sec/batch; 59h:54m:37s remains)
INFO - root - 2017-12-07 23:26:17.784963: step 29020, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.541 sec/batch; 58h:45m:32s remains)
INFO - root - 2017-12-07 23:26:34.037619: step 29030, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.576 sec/batch; 60h:05m:11s remains)
INFO - root - 2017-12-07 23:26:50.533254: step 29040, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 1.768 sec/batch; 67h:24m:09s remains)
INFO - root - 2017-12-07 23:27:06.749646: step 29050, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.601 sec/batch; 61h:00m:40s remains)
INFO - root - 2017-12-07 23:27:22.970030: step 29060, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.679 sec/batch; 63h:59m:44s remains)
INFO - root - 2017-12-07 23:27:38.982682: step 29070, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.603 sec/batch; 61h:05m:45s remains)
INFO - root - 2017-12-07 23:27:55.460302: step 29080, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.684 sec/batch; 64h:09m:45s remains)
INFO - root - 2017-12-07 23:28:11.650790: step 29090, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.614 sec/batch; 61h:30m:31s remains)
INFO - root - 2017-12-07 23:28:27.720325: step 29100, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 62h:05m:39s remains)
2017-12-07 23:28:29.030212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2161679 -4.2039223 -4.1988344 -4.1968694 -4.1942592 -4.1999054 -4.2114868 -4.21898 -4.22727 -4.23615 -4.2367325 -4.2355223 -4.2393837 -4.2496514 -4.2621717][-4.2191014 -4.2037778 -4.1954536 -4.18987 -4.1842647 -4.1867089 -4.1972423 -4.206327 -4.2162409 -4.2273426 -4.2292914 -4.2302527 -4.2342949 -4.2430396 -4.2551684][-4.2175512 -4.2027555 -4.1935644 -4.1863041 -4.178318 -4.1771669 -4.1850195 -4.1947703 -4.2068267 -4.2198205 -4.2228422 -4.2258887 -4.2300353 -4.2373605 -4.248632][-4.214839 -4.2031021 -4.1963291 -4.189074 -4.1800337 -4.1767268 -4.181385 -4.1902022 -4.2031903 -4.2165956 -4.2194786 -4.2230058 -4.22779 -4.2354746 -4.24628][-4.2085357 -4.2041931 -4.2028594 -4.199235 -4.1924214 -4.1891971 -4.1902933 -4.1963844 -4.2090259 -4.2206683 -4.2213745 -4.2236471 -4.229157 -4.2385387 -4.2490282][-4.2077165 -4.2105093 -4.2136707 -4.2125616 -4.2081079 -4.206974 -4.2067819 -4.21032 -4.2223406 -4.2315226 -4.22936 -4.2292986 -4.2342682 -4.2438407 -4.2539253][-4.2156324 -4.2189293 -4.2210164 -4.2178931 -4.2126756 -4.2126417 -4.2141819 -4.2200603 -4.233902 -4.2420721 -4.2396107 -4.2380586 -4.2414055 -4.2497311 -4.2593675][-4.2273712 -4.2273397 -4.225987 -4.219615 -4.2126207 -4.2147093 -4.2216725 -4.2340484 -4.2493696 -4.256134 -4.25224 -4.2487378 -4.2496181 -4.2556453 -4.264668][-4.2373548 -4.2341194 -4.2292495 -4.2203522 -4.2135892 -4.2201233 -4.2335072 -4.2487903 -4.261929 -4.2665019 -4.2612419 -4.2552075 -4.25419 -4.2600832 -4.2688546][-4.2465534 -4.2407188 -4.2330351 -4.2238832 -4.2200613 -4.2310457 -4.2484889 -4.2650223 -4.2750006 -4.2764349 -4.2690887 -4.261095 -4.2586818 -4.2632604 -4.2711329][-4.2589779 -4.2500734 -4.2405624 -4.2314758 -4.2294579 -4.2414865 -4.2591543 -4.2740726 -4.2819009 -4.2822061 -4.2736797 -4.2645936 -4.2607841 -4.2634869 -4.2709432][-4.2683949 -4.2595377 -4.2504888 -4.2426233 -4.2418904 -4.2511625 -4.2658377 -4.2784572 -4.2841849 -4.2837796 -4.2746954 -4.2658496 -4.2612543 -4.2631178 -4.27047][-4.2741866 -4.2677979 -4.2610517 -4.2552719 -4.2553983 -4.262495 -4.2742562 -4.2839265 -4.287425 -4.2856994 -4.2760673 -4.2676849 -4.2626352 -4.2642212 -4.2712216][-4.2803621 -4.2761583 -4.2713971 -4.2682228 -4.2702804 -4.2760353 -4.2848244 -4.2909975 -4.2914782 -4.2875466 -4.2774148 -4.269352 -4.2640533 -4.2656612 -4.2725482][-4.2862506 -4.2839165 -4.28118 -4.2805476 -4.284626 -4.2905288 -4.2965159 -4.2989745 -4.2962317 -4.2898235 -4.2788029 -4.2702026 -4.2648106 -4.2668381 -4.2739925]]...]
INFO - root - 2017-12-07 23:28:45.297486: step 29110, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.540 sec/batch; 58h:39m:14s remains)
INFO - root - 2017-12-07 23:29:01.615641: step 29120, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.597 sec/batch; 60h:49m:59s remains)
INFO - root - 2017-12-07 23:29:17.964262: step 29130, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.679 sec/batch; 63h:56m:03s remains)
INFO - root - 2017-12-07 23:29:34.091228: step 29140, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.564 sec/batch; 59h:33m:47s remains)
INFO - root - 2017-12-07 23:29:50.551750: step 29150, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.593 sec/batch; 60h:40m:23s remains)
INFO - root - 2017-12-07 23:30:06.826729: step 29160, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.698 sec/batch; 64h:40m:28s remains)
INFO - root - 2017-12-07 23:30:23.102724: step 29170, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 1.480 sec/batch; 56h:22m:18s remains)
INFO - root - 2017-12-07 23:30:39.221318: step 29180, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.546 sec/batch; 58h:52m:15s remains)
INFO - root - 2017-12-07 23:30:55.432328: step 29190, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.680 sec/batch; 63h:57m:14s remains)
INFO - root - 2017-12-07 23:31:11.803781: step 29200, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.622 sec/batch; 61h:45m:12s remains)
2017-12-07 23:31:13.204576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2554417 -4.2607994 -4.2659464 -4.26788 -4.26461 -4.2571559 -4.2472548 -4.2399583 -4.2352686 -4.2325435 -4.2300248 -4.2288222 -4.2305217 -4.2316504 -4.2324228][-4.2706866 -4.2760544 -4.2820845 -4.2855024 -4.2831035 -4.2787561 -4.2739539 -4.2702732 -4.2676005 -4.2658205 -4.2621579 -4.2584052 -4.2574782 -4.2549429 -4.2507324][-4.2761755 -4.2782297 -4.2833805 -4.2871366 -4.2851286 -4.283196 -4.2824278 -4.281714 -4.2813158 -4.2822795 -4.2803531 -4.2767282 -4.2738156 -4.2685575 -4.2604933][-4.27242 -4.2674193 -4.2667413 -4.2690511 -4.2669888 -4.2650938 -4.2663722 -4.2684855 -4.270401 -4.2743511 -4.2765651 -4.2753658 -4.2712593 -4.2645283 -4.2552142][-4.2548566 -4.2436228 -4.2361894 -4.2324772 -4.2251658 -4.2184753 -4.2163157 -4.2182145 -4.2235327 -4.2311997 -4.2402411 -4.2457056 -4.2446179 -4.2397165 -4.2338581][-4.2351708 -4.2192454 -4.2061119 -4.1938767 -4.1770854 -4.1588917 -4.1443892 -4.1372075 -4.1392431 -4.1510873 -4.1702375 -4.1861081 -4.1931052 -4.19514 -4.1983695][-4.2256641 -4.2069731 -4.1894708 -4.1691165 -4.1396217 -4.1045141 -4.0665131 -4.0337181 -4.0218782 -4.0401983 -4.0775208 -4.110518 -4.1313295 -4.145617 -4.1622057][-4.2240443 -4.2010221 -4.18035 -4.15687 -4.1215706 -4.0751715 -4.0134716 -3.9493837 -3.9171162 -3.940474 -3.99951 -4.0527306 -4.09 -4.11741 -4.1451993][-4.2244034 -4.2011733 -4.17914 -4.1548758 -4.1240873 -4.0849814 -4.0286694 -3.9636047 -3.9278445 -3.9455101 -3.9989707 -4.0512156 -4.0926189 -4.1245823 -4.1546888][-4.2121387 -4.1918764 -4.1741676 -4.1574917 -4.1403804 -4.1210628 -4.0908518 -4.0518026 -4.0297751 -4.038497 -4.0679307 -4.1019793 -4.1340904 -4.1614289 -4.185257][-4.1944413 -4.1776519 -4.1675196 -4.161499 -4.1574321 -4.1529474 -4.1410809 -4.1236882 -4.1158333 -4.1241803 -4.1409397 -4.1601 -4.1806602 -4.198729 -4.2127638][-4.1785126 -4.1660137 -4.1607962 -4.1616549 -4.1657243 -4.1688633 -4.1679 -4.1642 -4.1660323 -4.1744671 -4.1845241 -4.1950693 -4.2068257 -4.2168226 -4.2235541][-4.1808281 -4.174777 -4.1720452 -4.17502 -4.1797957 -4.1835489 -4.1850405 -4.18623 -4.1912627 -4.1994624 -4.2071648 -4.2119451 -4.2167292 -4.2200136 -4.2226849][-4.1941214 -4.1920071 -4.191421 -4.1942835 -4.19728 -4.1998515 -4.20067 -4.2008066 -4.204165 -4.2098484 -4.2153826 -4.2182713 -4.2198286 -4.2194467 -4.2200737][-4.2064638 -4.2046237 -4.2028513 -4.2030945 -4.2038918 -4.2060056 -4.2072597 -4.2069311 -4.2084575 -4.2115197 -4.215035 -4.2168226 -4.2176952 -4.2171273 -4.2178311]]...]
INFO - root - 2017-12-07 23:31:29.158920: step 29210, loss = 2.05, batch loss = 2.00 (10.3 examples/sec; 1.554 sec/batch; 59h:10m:25s remains)
INFO - root - 2017-12-07 23:31:45.520243: step 29220, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.735 sec/batch; 66h:03m:14s remains)
INFO - root - 2017-12-07 23:32:01.631756: step 29230, loss = 2.08, batch loss = 2.03 (10.7 examples/sec; 1.490 sec/batch; 56h:43m:22s remains)
INFO - root - 2017-12-07 23:32:18.004198: step 29240, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 1.681 sec/batch; 63h:59m:14s remains)
INFO - root - 2017-12-07 23:32:34.145441: step 29250, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.600 sec/batch; 60h:54m:08s remains)
INFO - root - 2017-12-07 23:32:50.320987: step 29260, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 1.700 sec/batch; 64h:41m:47s remains)
INFO - root - 2017-12-07 23:33:06.541007: step 29270, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.599 sec/batch; 60h:51m:36s remains)
INFO - root - 2017-12-07 23:33:22.871747: step 29280, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.608 sec/batch; 61h:11m:36s remains)
INFO - root - 2017-12-07 23:33:38.904896: step 29290, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 1.495 sec/batch; 56h:52m:46s remains)
INFO - root - 2017-12-07 23:33:55.195064: step 29300, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.592 sec/batch; 60h:34m:35s remains)
2017-12-07 23:33:56.618629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3124738 -4.3032823 -4.2805729 -4.2423873 -4.1873159 -4.1224952 -4.0512385 -3.9652164 -3.8771431 -3.8170016 -3.8515384 -3.9621377 -4.0566034 -4.1191583 -4.173059][-4.3183312 -4.3116741 -4.2928338 -4.2593222 -4.2106333 -4.1537533 -4.0956073 -4.029942 -3.9683828 -3.9339387 -3.961679 -4.0356374 -4.0978384 -4.1446915 -4.1913342][-4.3240623 -4.3199196 -4.3069019 -4.282598 -4.2471957 -4.2097678 -4.1791649 -4.1472664 -4.1143575 -4.0953336 -4.1094651 -4.1461186 -4.1771917 -4.2046371 -4.237186][-4.3277655 -4.3250961 -4.3162441 -4.2980304 -4.2725639 -4.2495904 -4.2427673 -4.2433996 -4.2387328 -4.2332053 -4.240098 -4.25608 -4.26776 -4.2784033 -4.2963729][-4.3253803 -4.3197904 -4.3068509 -4.2822266 -4.2529449 -4.2320428 -4.2381096 -4.2633853 -4.2873807 -4.3039651 -4.316195 -4.3284459 -4.3341703 -4.3357234 -4.3421769][-4.3176517 -4.3024158 -4.2751465 -4.2282529 -4.1761026 -4.1408758 -4.1556439 -4.2084122 -4.2671561 -4.31477 -4.3431315 -4.3610563 -4.3642788 -4.3596115 -4.3579068][-4.3108716 -4.283246 -4.2337251 -4.1495209 -4.0525064 -3.9802508 -4.0030093 -4.0943003 -4.1928253 -4.2725053 -4.3210745 -4.3517351 -4.358851 -4.35312 -4.3493695][-4.3055692 -4.2704024 -4.2041769 -4.0882959 -3.94532 -3.8241005 -3.8503175 -3.9795327 -4.1084867 -4.2084908 -4.2734866 -4.3186655 -4.33588 -4.3354788 -4.3345232][-4.30573 -4.2722154 -4.2075005 -4.0908017 -3.9453764 -3.8230145 -3.8521485 -3.9835579 -4.1057506 -4.1979408 -4.2606392 -4.3066463 -4.3266191 -4.3285303 -4.3280015][-4.3191819 -4.2954679 -4.249145 -4.1623893 -4.0590925 -3.9840128 -4.01017 -4.1005173 -4.180656 -4.2427006 -4.2847342 -4.3148708 -4.3290582 -4.3307791 -4.3309207][-4.3257713 -4.3098869 -4.2827525 -4.23089 -4.1702547 -4.1337161 -4.1523767 -4.2031846 -4.2456417 -4.2791438 -4.301002 -4.3149748 -4.3220649 -4.3246145 -4.3295217][-4.3085766 -4.2966666 -4.2795019 -4.2534194 -4.2247338 -4.210371 -4.22517 -4.2541261 -4.2740617 -4.2873077 -4.2926674 -4.2925687 -4.2930584 -4.2997184 -4.3146825][-4.2634196 -4.2519565 -4.2392421 -4.2286673 -4.222239 -4.223433 -4.2376323 -4.2548194 -4.2601581 -4.2549152 -4.2453794 -4.236114 -4.2350669 -4.251904 -4.2806978][-4.1979532 -4.18773 -4.1798005 -4.1781826 -4.1837945 -4.1975417 -4.2162957 -4.2288532 -4.2225633 -4.2009306 -4.1780205 -4.1641669 -4.1670761 -4.1944304 -4.2331352][-4.1601524 -4.1530366 -4.1491342 -4.1492996 -4.1544819 -4.1678662 -4.1853876 -4.1956229 -4.1848755 -4.1568251 -4.1281743 -4.111743 -4.1187062 -4.1531844 -4.1932015]]...]
INFO - root - 2017-12-07 23:34:13.041663: step 29310, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 1.632 sec/batch; 62h:04m:57s remains)
INFO - root - 2017-12-07 23:34:29.307196: step 29320, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.695 sec/batch; 64h:27m:27s remains)
INFO - root - 2017-12-07 23:34:45.481491: step 29330, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.551 sec/batch; 58h:59m:27s remains)
INFO - root - 2017-12-07 23:35:01.723810: step 29340, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.693 sec/batch; 64h:23m:27s remains)
INFO - root - 2017-12-07 23:35:17.924740: step 29350, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.652 sec/batch; 62h:50m:26s remains)
INFO - root - 2017-12-07 23:35:34.077397: step 29360, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.628 sec/batch; 61h:53m:14s remains)
INFO - root - 2017-12-07 23:35:50.319343: step 29370, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 60h:03m:10s remains)
INFO - root - 2017-12-07 23:36:06.726822: step 29380, loss = 2.09, batch loss = 2.04 (10.0 examples/sec; 1.603 sec/batch; 60h:57m:47s remains)
INFO - root - 2017-12-07 23:36:23.055179: step 29390, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.641 sec/batch; 62h:22m:45s remains)
INFO - root - 2017-12-07 23:36:39.457803: step 29400, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.666 sec/batch; 63h:18m:55s remains)
2017-12-07 23:36:40.757311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3050351 -4.2956862 -4.2899981 -4.2879858 -4.29006 -4.2987213 -4.2992606 -4.2859516 -4.26685 -4.2561083 -4.2613592 -4.2695231 -4.2755876 -4.2871137 -4.2924795][-4.2998552 -4.2871509 -4.2773952 -4.2734528 -4.2769437 -4.2867227 -4.2854486 -4.26864 -4.2506628 -4.244319 -4.2514396 -4.2605085 -4.2655029 -4.272192 -4.2732644][-4.2753983 -4.2606096 -4.247159 -4.2405152 -4.2449017 -4.2532024 -4.249557 -4.2313347 -4.2183132 -4.2202229 -4.2290182 -4.236268 -4.240191 -4.2447767 -4.2443738][-4.2362323 -4.2260261 -4.2140265 -4.2066889 -4.2107768 -4.2147617 -4.2044148 -4.1817865 -4.172102 -4.182291 -4.1941528 -4.2028837 -4.2087483 -4.2138872 -4.2147541][-4.1919289 -4.1869884 -4.1758676 -4.1636381 -4.1613932 -4.1561379 -4.1324382 -4.1024814 -4.1037726 -4.1309977 -4.152082 -4.1664495 -4.17853 -4.1896191 -4.1966429][-4.1624126 -4.1588583 -4.1414542 -4.1176887 -4.1009784 -4.0781312 -4.0320411 -3.9901886 -4.011795 -4.0724139 -4.1152973 -4.1380849 -4.1526618 -4.1663971 -4.1798511][-4.1562676 -4.1612754 -4.1447892 -4.1147342 -4.08688 -4.0470047 -3.9742804 -3.9124157 -3.9503293 -4.040503 -4.101759 -4.1302152 -4.1410832 -4.1519837 -4.1658611][-4.1565886 -4.1788058 -4.1705608 -4.1445174 -4.122169 -4.0927658 -4.0327096 -3.9799101 -4.0061045 -4.0798268 -4.13237 -4.1555028 -4.1600738 -4.1646857 -4.1764393][-4.1688457 -4.203104 -4.204514 -4.1875148 -4.1768641 -4.1659889 -4.1325917 -4.0990405 -4.1119251 -4.1581168 -4.189734 -4.2033792 -4.2033234 -4.2038431 -4.2118912][-4.1849504 -4.2180538 -4.2289023 -4.2275829 -4.2280674 -4.226809 -4.2097297 -4.1877303 -4.1932874 -4.2203445 -4.2365093 -4.2418032 -4.23948 -4.239109 -4.2454748][-4.2093396 -4.2274752 -4.2396388 -4.2494903 -4.2574544 -4.2628942 -4.2559123 -4.2430296 -4.2480779 -4.2649789 -4.2747793 -4.2765775 -4.2717991 -4.2690425 -4.2714634][-4.2413607 -4.2469621 -4.2546825 -4.2681527 -4.2771735 -4.2807341 -4.2759638 -4.2687607 -4.2749557 -4.2877631 -4.2953448 -4.2937927 -4.2864494 -4.2827687 -4.2834377][-4.2580094 -4.2596645 -4.2635522 -4.2769489 -4.2863755 -4.2857447 -4.2756224 -4.2657266 -4.268693 -4.2804041 -4.2899394 -4.2881479 -4.2799125 -4.2773294 -4.2777839][-4.2531376 -4.255198 -4.2571883 -4.2677541 -4.2772832 -4.2756505 -4.2587614 -4.2425747 -4.2421846 -4.252707 -4.2624149 -4.2620311 -4.2535429 -4.2531643 -4.2564278][-4.2402506 -4.2459373 -4.2504845 -4.262455 -4.2732196 -4.2705841 -4.2483153 -4.2265959 -4.2233806 -4.2296338 -4.2321506 -4.2271285 -4.2138972 -4.2143979 -4.2228303]]...]
INFO - root - 2017-12-07 23:36:57.049908: step 29410, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.742 sec/batch; 66h:12m:39s remains)
INFO - root - 2017-12-07 23:37:13.483935: step 29420, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.620 sec/batch; 61h:35m:18s remains)
INFO - root - 2017-12-07 23:37:29.766990: step 29430, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.630 sec/batch; 61h:56m:34s remains)
INFO - root - 2017-12-07 23:37:45.977902: step 29440, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.653 sec/batch; 62h:49m:56s remains)
INFO - root - 2017-12-07 23:38:02.326227: step 29450, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.557 sec/batch; 59h:09m:38s remains)
INFO - root - 2017-12-07 23:38:18.547587: step 29460, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 1.595 sec/batch; 60h:36m:39s remains)
INFO - root - 2017-12-07 23:38:34.711693: step 29470, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.622 sec/batch; 61h:37m:43s remains)
INFO - root - 2017-12-07 23:38:50.980723: step 29480, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.538 sec/batch; 58h:25m:27s remains)
INFO - root - 2017-12-07 23:39:07.482175: step 29490, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.675 sec/batch; 63h:38m:05s remains)
INFO - root - 2017-12-07 23:39:23.529514: step 29500, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.610 sec/batch; 61h:08m:50s remains)
2017-12-07 23:39:24.913208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3445854 -4.3469095 -4.346077 -4.3462229 -4.3470578 -4.3475356 -4.3476586 -4.3463688 -4.343853 -4.3410778 -4.3400631 -4.3399467 -4.340435 -4.3419094 -4.3438306][-4.3466444 -4.3464346 -4.3432083 -4.3432608 -4.3462052 -4.3505678 -4.3517895 -4.3491068 -4.3421993 -4.3343692 -4.33062 -4.3308048 -4.3327842 -4.3374591 -4.341126][-4.3379893 -4.3315821 -4.3220181 -4.320961 -4.3276706 -4.3385739 -4.3420806 -4.335279 -4.3207917 -4.307363 -4.3045964 -4.3105073 -4.3196449 -4.3278112 -4.3327208][-4.3129673 -4.2981505 -4.2802043 -4.2753673 -4.2851944 -4.3009262 -4.3049955 -4.292069 -4.267087 -4.2460275 -4.2475948 -4.2675123 -4.2919025 -4.3089585 -4.31597][-4.2802029 -4.2524939 -4.2240214 -4.2147145 -4.2237129 -4.24094 -4.2396564 -4.21014 -4.1667943 -4.1395435 -4.1501327 -4.1924043 -4.2371721 -4.2651329 -4.2810149][-4.2686634 -4.2274013 -4.1876616 -4.1743312 -4.1805282 -4.1911883 -4.1788392 -4.1256065 -4.0554152 -4.0228224 -4.0430589 -4.1054416 -4.1632833 -4.1936851 -4.2223392][-4.2787933 -4.23301 -4.1918192 -4.1772823 -4.17319 -4.162044 -4.1240139 -4.0450397 -3.9572248 -3.9301598 -3.9617209 -4.0340753 -4.0945492 -4.1214442 -4.162838][-4.29901 -4.2613163 -4.2307673 -4.2202063 -4.2031388 -4.1639161 -4.1032996 -4.0133963 -3.9243298 -3.9041355 -3.949923 -4.0232716 -4.0753169 -4.0892086 -4.1287632][-4.32471 -4.3036613 -4.2896433 -4.2858028 -4.2631364 -4.2046928 -4.1376829 -4.0688782 -4.0010691 -3.9768553 -4.0095677 -4.0624633 -4.0985007 -4.102252 -4.1358261][-4.3473816 -4.342402 -4.3402953 -4.3411965 -4.3221803 -4.2702756 -4.214766 -4.1698384 -4.1276879 -4.0940557 -4.1012087 -4.12697 -4.1493034 -4.1561127 -4.1861391][-4.3577156 -4.3634682 -4.3689594 -4.3698468 -4.3570004 -4.3248696 -4.2904654 -4.2682748 -4.248805 -4.2161627 -4.2047467 -4.2088866 -4.222281 -4.2333989 -4.2529831][-4.3585839 -4.370605 -4.3757782 -4.3758221 -4.3672972 -4.3505774 -4.3333359 -4.3237906 -4.3179708 -4.2978191 -4.2860203 -4.2817578 -4.2886696 -4.2985353 -4.3076196][-4.3506756 -4.3604031 -4.361455 -4.3641686 -4.3643146 -4.3561258 -4.3493919 -4.3464227 -4.34549 -4.337821 -4.3347459 -4.3340106 -4.3361392 -4.34126 -4.3448677][-4.349514 -4.3555803 -4.3533 -4.3557682 -4.3590803 -4.3538837 -4.3510518 -4.3522782 -4.3541665 -4.3542142 -4.3552556 -4.3542066 -4.3551297 -4.3595834 -4.3604226][-4.3528218 -4.3558674 -4.3514843 -4.3499756 -4.3491588 -4.3415694 -4.3367443 -4.3387146 -4.3432956 -4.3481493 -4.3518815 -4.3522859 -4.3526669 -4.3577394 -4.3602271]]...]
INFO - root - 2017-12-07 23:39:41.147284: step 29510, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.553 sec/batch; 58h:58m:28s remains)
INFO - root - 2017-12-07 23:39:57.507238: step 29520, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 1.692 sec/batch; 64h:15m:52s remains)
INFO - root - 2017-12-07 23:40:13.530319: step 29530, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.562 sec/batch; 59h:18m:26s remains)
INFO - root - 2017-12-07 23:40:29.900308: step 29540, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.660 sec/batch; 63h:02m:32s remains)
INFO - root - 2017-12-07 23:40:46.132019: step 29550, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.574 sec/batch; 59h:45m:52s remains)
INFO - root - 2017-12-07 23:41:02.263883: step 29560, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.668 sec/batch; 63h:19m:22s remains)
INFO - root - 2017-12-07 23:41:18.430692: step 29570, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.571 sec/batch; 59h:38m:54s remains)
INFO - root - 2017-12-07 23:41:34.907874: step 29580, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.559 sec/batch; 59h:12m:08s remains)
INFO - root - 2017-12-07 23:41:51.024515: step 29590, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.659 sec/batch; 62h:59m:12s remains)
INFO - root - 2017-12-07 23:42:07.319861: step 29600, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.571 sec/batch; 59h:37m:50s remains)
2017-12-07 23:42:08.721609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2346582 -4.2506638 -4.2641397 -4.2647219 -4.2659545 -4.2634726 -4.2597318 -4.2610221 -4.2686687 -4.2817588 -4.2958312 -4.3087039 -4.3143964 -4.31084 -4.3065929][-4.1697321 -4.214241 -4.250123 -4.2629185 -4.271194 -4.2758889 -4.2787676 -4.2848029 -4.2947769 -4.3048906 -4.3133121 -4.3222547 -4.3212562 -4.3114314 -4.3086562][-4.0991278 -4.1755939 -4.2363467 -4.2593555 -4.2679615 -4.27308 -4.2807045 -4.2922516 -4.30961 -4.3208551 -4.3243871 -4.3266187 -4.3146248 -4.2961187 -4.29593][-4.0595746 -4.1574121 -4.2284217 -4.2482805 -4.24815 -4.2406979 -4.2396564 -4.2540841 -4.2839565 -4.3037229 -4.3074203 -4.30347 -4.2808414 -4.25291 -4.255312][-4.0331841 -4.1356592 -4.2018251 -4.2079997 -4.1863132 -4.1550903 -4.1390805 -4.1603813 -4.2113543 -4.2444253 -4.2499642 -4.2381487 -4.19922 -4.1576538 -4.1624565][-4.0469966 -4.1200867 -4.1597834 -4.1426563 -4.0935955 -4.02738 -3.9810786 -4.0151739 -4.09922 -4.1480074 -4.1520958 -4.1316652 -4.0755792 -4.01955 -4.0243258][-4.0969496 -4.1266003 -4.1270161 -4.0778861 -3.9888384 -3.8737397 -3.7861652 -3.8346074 -3.9536977 -4.0149045 -4.0238147 -4.0094485 -3.9502554 -3.8874762 -3.8927529][-4.1274195 -4.12372 -4.0987835 -4.0319242 -3.9229336 -3.7827547 -3.6706934 -3.7201424 -3.8422656 -3.9003448 -3.9208307 -3.9273982 -3.887732 -3.8425379 -3.860739][-4.1548672 -4.1401315 -4.1078768 -4.0484643 -3.9585245 -3.8471131 -3.765646 -3.7992604 -3.8748543 -3.9029107 -3.9275284 -3.9504161 -3.9413884 -3.9323189 -3.9605312][-4.1819415 -4.1731577 -4.1460304 -4.1005764 -4.0386992 -3.9646559 -3.9171398 -3.9386106 -3.9776525 -3.9885881 -4.0055227 -4.0269294 -4.0358014 -4.047401 -4.0712838][-4.2051053 -4.2116609 -4.19739 -4.1684141 -4.1278162 -4.0783076 -4.0517712 -4.0704641 -4.0918412 -4.09553 -4.1053505 -4.1203837 -4.1303754 -4.1401448 -4.1515079][-4.2407436 -4.2570968 -4.2540884 -4.2372088 -4.2112112 -4.1778326 -4.166111 -4.1842866 -4.1992521 -4.1997242 -4.2023339 -4.2106619 -4.2159872 -4.2168832 -4.2169127][-4.2734537 -4.2934308 -4.2969823 -4.286706 -4.266551 -4.2453475 -4.2429919 -4.2576265 -4.2661524 -4.2625227 -4.261591 -4.2673144 -4.2697921 -4.2652283 -4.2580872][-4.2886243 -4.308917 -4.3146033 -4.3082089 -4.2926016 -4.2778463 -4.2781243 -4.289566 -4.2928267 -4.2852106 -4.2814174 -4.2849646 -4.2860928 -4.2810326 -4.2741771][-4.288456 -4.304657 -4.3094282 -4.3055105 -4.2949438 -4.285738 -4.2855682 -4.2915077 -4.2906957 -4.2834048 -4.2801361 -4.2830548 -4.2847481 -4.2825646 -4.2782416]]...]
INFO - root - 2017-12-07 23:42:25.066651: step 29610, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.572 sec/batch; 59h:40m:34s remains)
INFO - root - 2017-12-07 23:42:41.478083: step 29620, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.733 sec/batch; 65h:45m:50s remains)
INFO - root - 2017-12-07 23:42:57.615327: step 29630, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.620 sec/batch; 61h:28m:56s remains)
INFO - root - 2017-12-07 23:43:14.139899: step 29640, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.679 sec/batch; 63h:42m:41s remains)
INFO - root - 2017-12-07 23:43:30.480332: step 29650, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.644 sec/batch; 62h:22m:31s remains)
INFO - root - 2017-12-07 23:43:46.688429: step 29660, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.656 sec/batch; 62h:48m:56s remains)
INFO - root - 2017-12-07 23:44:02.811193: step 29670, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 1.473 sec/batch; 55h:52m:10s remains)
INFO - root - 2017-12-07 23:44:19.190733: step 29680, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.684 sec/batch; 63h:52m:30s remains)
INFO - root - 2017-12-07 23:44:35.345393: step 29690, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.584 sec/batch; 60h:05m:39s remains)
INFO - root - 2017-12-07 23:44:51.750269: step 29700, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.583 sec/batch; 60h:03m:37s remains)
2017-12-07 23:44:53.101227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2567763 -4.2035809 -4.1171479 -4.00374 -3.9106405 -3.8908212 -3.9513273 -4.0665541 -4.17444 -4.2331333 -4.2570291 -4.2591252 -4.2449679 -4.2134132 -4.1624408][-4.2583232 -4.2085228 -4.1236105 -4.0071607 -3.9035866 -3.8717134 -3.9297783 -4.0513043 -4.1650491 -4.2301583 -4.2565441 -4.2614031 -4.2527838 -4.232007 -4.1915932][-4.2665133 -4.2237983 -4.1468086 -4.0368052 -3.9315929 -3.8847609 -3.9257743 -4.0334549 -4.1466837 -4.2170753 -4.2468638 -4.2543554 -4.248847 -4.2356019 -4.2100077][-4.2734175 -4.2365479 -4.169735 -4.06922 -3.9619358 -3.89614 -3.9076574 -3.9949064 -4.1084938 -4.1915641 -4.2282176 -4.2414522 -4.2390451 -4.23136 -4.21779][-4.2888975 -4.2545724 -4.1947026 -4.1035957 -3.9950702 -3.9107449 -3.8850594 -3.9420102 -4.0534124 -4.1542253 -4.2050753 -4.2293015 -4.23605 -4.2378983 -4.2348685][-4.3087158 -4.27685 -4.2182293 -4.1295953 -4.0181913 -3.9114437 -3.8440218 -3.8665872 -3.98026 -4.1041603 -4.1756763 -4.2193117 -4.2408934 -4.252069 -4.2550755][-4.3245516 -4.2973318 -4.236629 -4.1414309 -4.0147243 -3.873347 -3.7563353 -3.7385168 -3.8652916 -4.0261793 -4.1303806 -4.1948051 -4.2335458 -4.2526708 -4.2615142][-4.336966 -4.3172669 -4.2597828 -4.1602569 -4.017961 -3.8390102 -3.6616793 -3.5895312 -3.7260756 -3.9234052 -4.0646687 -4.1521721 -4.2076707 -4.2392111 -4.2599587][-4.3424892 -4.3328853 -4.2867703 -4.1964211 -4.0599055 -3.8774526 -3.6777997 -3.5644948 -3.6636653 -3.8458328 -3.9975479 -4.1050944 -4.1800056 -4.2254882 -4.254055][-4.3431458 -4.3393016 -4.3061776 -4.2336478 -4.1209702 -3.9685709 -3.79745 -3.6900353 -3.7282162 -3.8458405 -3.9714341 -4.0820851 -4.1691742 -4.2215776 -4.253861][-4.3416281 -4.3382769 -4.3150592 -4.2595487 -4.1710358 -4.0509119 -3.9148936 -3.8239746 -3.82253 -3.8844457 -3.9724498 -4.0714703 -4.161972 -4.2204571 -4.2547064][-4.3405719 -4.3352227 -4.319694 -4.2804542 -4.2128792 -4.1162853 -4.0043797 -3.9231427 -3.8975811 -3.9220867 -3.9815593 -4.0637879 -4.1523619 -4.2233725 -4.2605662][-4.3406539 -4.3345327 -4.3251104 -4.2996368 -4.2489734 -4.1719289 -4.0796361 -4.0042248 -3.9672251 -3.9702029 -4.0086393 -4.0715342 -4.1474643 -4.220408 -4.2637191][-4.3437786 -4.3382635 -4.3314166 -4.3142891 -4.2776632 -4.2176213 -4.1424994 -4.0724325 -4.0302963 -4.0216727 -4.0447989 -4.086987 -4.1453595 -4.2106075 -4.2587962][-4.3473444 -4.3434272 -4.3377905 -4.3269997 -4.3015184 -4.2564354 -4.196178 -4.1337428 -4.0902381 -4.0723791 -4.0815625 -4.1053476 -4.1449366 -4.1981082 -4.2454495]]...]
INFO - root - 2017-12-07 23:45:09.278292: step 29710, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.698 sec/batch; 64h:23m:44s remains)
INFO - root - 2017-12-07 23:45:25.482374: step 29720, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.606 sec/batch; 60h:54m:44s remains)
INFO - root - 2017-12-07 23:45:41.636852: step 29730, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.631 sec/batch; 61h:51m:57s remains)
INFO - root - 2017-12-07 23:45:57.717777: step 29740, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.575 sec/batch; 59h:43m:33s remains)
INFO - root - 2017-12-07 23:46:14.061162: step 29750, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.600 sec/batch; 60h:40m:53s remains)
INFO - root - 2017-12-07 23:46:30.475905: step 29760, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.662 sec/batch; 63h:00m:08s remains)
INFO - root - 2017-12-07 23:46:46.681035: step 29770, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.609 sec/batch; 60h:59m:05s remains)
INFO - root - 2017-12-07 23:47:03.163921: step 29780, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 1.596 sec/batch; 60h:31m:05s remains)
INFO - root - 2017-12-07 23:47:19.402988: step 29790, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 1.699 sec/batch; 64h:24m:22s remains)
INFO - root - 2017-12-07 23:47:35.651473: step 29800, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 60h:16m:29s remains)
2017-12-07 23:47:37.085582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3347 -4.3344812 -4.3346605 -4.3352842 -4.3359547 -4.33505 -4.3342986 -4.3319478 -4.3284779 -4.3260431 -4.3258963 -4.32837 -4.3327246 -4.3361511 -4.3377986][-4.3357692 -4.3311791 -4.3277969 -4.3293009 -4.3325949 -4.3300362 -4.3208952 -4.3088446 -4.299881 -4.2967253 -4.2988009 -4.3078532 -4.3193264 -4.328474 -4.3351474][-4.3251038 -4.3128591 -4.3036613 -4.304987 -4.3103318 -4.3049245 -4.2844715 -4.260581 -4.2491031 -4.2511292 -4.2599225 -4.2783732 -4.2966075 -4.310493 -4.3205643][-4.3176875 -4.3022366 -4.2878981 -4.2828541 -4.2811227 -4.2667251 -4.2321992 -4.1988783 -4.1933875 -4.2079067 -4.2277994 -4.255631 -4.2803745 -4.2972164 -4.3082361][-4.3160849 -4.3006516 -4.2834091 -4.2673292 -4.2480836 -4.2142448 -4.1608019 -4.11892 -4.126997 -4.1625481 -4.19666 -4.2318368 -4.2637277 -4.2851644 -4.29647][-4.3177924 -4.3011751 -4.2768307 -4.2453423 -4.2054453 -4.1475186 -4.0768614 -4.0329957 -4.0597982 -4.120028 -4.1688023 -4.2097225 -4.2501616 -4.276298 -4.28674][-4.315176 -4.2866974 -4.2443137 -4.1939564 -4.1336446 -4.05699 -3.9834192 -3.9520359 -4.0016689 -4.0832772 -4.142035 -4.1883512 -4.2333317 -4.2621245 -4.2716484][-4.3132677 -4.2722468 -4.2133927 -4.148881 -4.0798321 -4.0056396 -3.9511247 -3.9461589 -4.0106273 -4.0970526 -4.1532044 -4.1934948 -4.2325826 -4.2574363 -4.267221][-4.3174887 -4.2682552 -4.20497 -4.1387844 -4.079628 -4.0294857 -4.0001459 -4.0073977 -4.065289 -4.138474 -4.1905584 -4.2255192 -4.25577 -4.27373 -4.27941][-4.3263612 -4.2751961 -4.2153134 -4.1607122 -4.1275711 -4.1077156 -4.0964727 -4.1040249 -4.1454635 -4.1994009 -4.2394028 -4.2649021 -4.2847314 -4.294117 -4.2913733][-4.3220677 -4.2786503 -4.2356114 -4.2068696 -4.2051206 -4.2101064 -4.21253 -4.218369 -4.2394543 -4.2686114 -4.2906737 -4.3023233 -4.3079967 -4.3077736 -4.2997851][-4.31232 -4.2785563 -4.2517357 -4.2459822 -4.2679515 -4.2899551 -4.30242 -4.3063688 -4.3117065 -4.3167424 -4.3160405 -4.3123035 -4.3076434 -4.3022661 -4.295259][-4.3094358 -4.2780304 -4.2552848 -4.2577596 -4.2878118 -4.3210797 -4.3393493 -4.3420649 -4.338253 -4.3302264 -4.3180437 -4.3076091 -4.3008718 -4.2966957 -4.2946682][-4.3171334 -4.2849364 -4.2620645 -4.2644224 -4.2966232 -4.3354511 -4.3559418 -4.3549833 -4.3448148 -4.3305717 -4.3156333 -4.3080544 -4.3088059 -4.3109975 -4.3146958][-4.3202229 -4.288259 -4.2684135 -4.2727022 -4.3051867 -4.3428841 -4.3593197 -4.3534455 -4.3383794 -4.3206925 -4.3069134 -4.3035445 -4.3113632 -4.3211489 -4.3305559]]...]
INFO - root - 2017-12-07 23:47:53.389394: step 29810, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.611 sec/batch; 61h:04m:08s remains)
INFO - root - 2017-12-07 23:48:09.675013: step 29820, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 1.651 sec/batch; 62h:34m:00s remains)
INFO - root - 2017-12-07 23:48:25.759530: step 29830, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.601 sec/batch; 60h:39m:02s remains)
INFO - root - 2017-12-07 23:48:41.945867: step 29840, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.666 sec/batch; 63h:07m:02s remains)
INFO - root - 2017-12-07 23:48:58.222315: step 29850, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.589 sec/batch; 60h:11m:59s remains)
INFO - root - 2017-12-07 23:49:14.589436: step 29860, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.640 sec/batch; 62h:09m:03s remains)
INFO - root - 2017-12-07 23:49:30.852145: step 29870, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.580 sec/batch; 59h:51m:24s remains)
INFO - root - 2017-12-07 23:49:47.082864: step 29880, loss = 2.09, batch loss = 2.04 (9.8 examples/sec; 1.638 sec/batch; 62h:02m:37s remains)
INFO - root - 2017-12-07 23:50:03.270963: step 29890, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.654 sec/batch; 62h:39m:53s remains)
INFO - root - 2017-12-07 23:50:19.686336: step 29900, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.588 sec/batch; 60h:09m:13s remains)
2017-12-07 23:50:21.004025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3480926 -4.3485446 -4.3412929 -4.3272018 -4.30515 -4.2832189 -4.2723093 -4.2729373 -4.2790909 -4.2740555 -4.2580395 -4.2315011 -4.2128973 -4.2114019 -4.2214394][-4.350409 -4.3563418 -4.347507 -4.3250771 -4.2900324 -4.2580056 -4.2465582 -4.2575378 -4.2706323 -4.26142 -4.2414126 -4.2110806 -4.1853566 -4.1766229 -4.1854987][-4.3470287 -4.3543386 -4.3399558 -4.3033824 -4.2487936 -4.197 -4.179265 -4.2016 -4.23103 -4.236001 -4.2269635 -4.2041612 -4.1776552 -4.1570005 -4.1590304][-4.3349981 -4.3420134 -4.32476 -4.2792244 -4.2092271 -4.1398911 -4.1106791 -4.1352134 -4.1820517 -4.2098265 -4.2210193 -4.2093334 -4.1857591 -4.1626139 -4.1607862][-4.3219585 -4.3317757 -4.318994 -4.2690368 -4.1879997 -4.1035461 -4.0524588 -4.0642962 -4.12072 -4.1695042 -4.1986032 -4.1992874 -4.1800461 -4.1630025 -4.1699524][-4.3027139 -4.3108668 -4.3028259 -4.2515244 -4.1590962 -4.0551748 -3.9707639 -3.9501324 -4.0161967 -4.0977058 -4.1526132 -4.1719451 -4.1628361 -4.1549664 -4.1747093][-4.2797866 -4.286324 -4.2824588 -4.2318559 -4.1290593 -4.0020132 -3.8682361 -3.7878 -3.8539526 -3.9773078 -4.0700145 -4.1166358 -4.1248512 -4.1333575 -4.1701889][-4.2643962 -4.2696142 -4.2751126 -4.2346735 -4.13569 -3.9945636 -3.8232441 -3.6908417 -3.7425642 -3.8885643 -4.0073605 -4.0785251 -4.1132021 -4.1433163 -4.1894855][-4.268662 -4.2749515 -4.29154 -4.2705235 -4.197217 -4.07706 -3.9284205 -3.8116279 -3.8463111 -3.9593892 -4.0579829 -4.12831 -4.1735325 -4.2076139 -4.2441497][-4.2910905 -4.2948117 -4.3110013 -4.298985 -4.2496181 -4.1569958 -4.0453649 -3.9700522 -3.9912367 -4.0676117 -4.1380906 -4.1968241 -4.2383585 -4.2680364 -4.2958555][-4.3171678 -4.31458 -4.3250613 -4.317699 -4.2835093 -4.2122574 -4.1311955 -4.0882788 -4.1028152 -4.1493888 -4.1995096 -4.2428555 -4.2746258 -4.2973871 -4.3207388][-4.3342857 -4.3277907 -4.3348732 -4.333343 -4.313427 -4.2645173 -4.20706 -4.1778235 -4.1818919 -4.2088823 -4.24128 -4.2674527 -4.2869749 -4.3044853 -4.3245921][-4.3265743 -4.3119483 -4.3139834 -4.3176684 -4.3117609 -4.2813458 -4.2413454 -4.2176023 -4.2165742 -4.2331161 -4.2582345 -4.2776523 -4.2928491 -4.3089175 -4.32805][-4.295166 -4.2748246 -4.2693281 -4.2702923 -4.2691507 -4.2537913 -4.2328916 -4.2204447 -4.2250195 -4.2445574 -4.2687 -4.2878304 -4.3022895 -4.3174438 -4.3356023][-4.2773442 -4.2559237 -4.2450318 -4.2397246 -4.2381306 -4.233851 -4.2313375 -4.2310238 -4.244319 -4.2678123 -4.2900825 -4.306025 -4.3186378 -4.3323464 -4.345602]]...]
INFO - root - 2017-12-07 23:50:37.336650: step 29910, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.633 sec/batch; 61h:51m:00s remains)
INFO - root - 2017-12-07 23:50:53.660061: step 29920, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.611 sec/batch; 61h:00m:52s remains)
INFO - root - 2017-12-07 23:51:10.105074: step 29930, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.612 sec/batch; 61h:02m:20s remains)
INFO - root - 2017-12-07 23:51:26.544833: step 29940, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.676 sec/batch; 63h:27m:14s remains)
INFO - root - 2017-12-07 23:51:42.536951: step 29950, loss = 2.05, batch loss = 2.00 (10.3 examples/sec; 1.549 sec/batch; 58h:38m:30s remains)
INFO - root - 2017-12-07 23:51:58.871443: step 29960, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 1.526 sec/batch; 57h:45m:40s remains)
INFO - root - 2017-12-07 23:52:15.196259: step 29970, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.636 sec/batch; 61h:55m:58s remains)
INFO - root - 2017-12-07 23:52:31.484939: step 29980, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.670 sec/batch; 63h:12m:59s remains)
INFO - root - 2017-12-07 23:52:47.661724: step 29990, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.534 sec/batch; 58h:03m:01s remains)
INFO - root - 2017-12-07 23:53:04.106062: step 30000, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 1.596 sec/batch; 60h:23m:34s remains)
2017-12-07 23:53:05.457133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2923026 -4.2903333 -4.2886248 -4.2881727 -4.2890882 -4.2896085 -4.2896395 -4.2902379 -4.2931457 -4.2962441 -4.2992573 -4.3028727 -4.3025727 -4.3003211 -4.2997947][-4.2887239 -4.2867131 -4.2838616 -4.2804856 -4.2789211 -4.2773638 -4.275219 -4.2754393 -4.2798295 -4.2848625 -4.2896895 -4.2959628 -4.2987103 -4.2989354 -4.3000855][-4.2846303 -4.2840166 -4.2814584 -4.277173 -4.2752075 -4.272892 -4.2698131 -4.2699232 -4.2747731 -4.2799287 -4.2846 -4.2916803 -4.2965426 -4.298574 -4.3000674][-4.271379 -4.2702532 -4.2672086 -4.2635036 -4.2636714 -4.2636609 -4.2632465 -4.2656927 -4.2716513 -4.2763391 -4.2790923 -4.2842989 -4.2885876 -4.2907715 -4.2918711][-4.2454023 -4.2427497 -4.2384386 -4.235548 -4.2381854 -4.2418194 -4.2454138 -4.2520609 -4.259841 -4.2645111 -4.265553 -4.2675524 -4.2692194 -4.2705731 -4.2720261][-4.2144737 -4.2088404 -4.2007241 -4.194211 -4.1942272 -4.1966448 -4.2021427 -4.2118936 -4.2205968 -4.2254114 -4.2269382 -4.2288151 -4.230587 -4.2343125 -4.2399664][-4.1844168 -4.173974 -4.1598806 -4.1455407 -4.1394439 -4.138639 -4.1454692 -4.1579714 -4.1686635 -4.1756959 -4.1798277 -4.1843605 -4.189168 -4.1975942 -4.2096863][-4.1870146 -4.1772089 -4.1611738 -4.1414213 -4.1283693 -4.1212339 -4.1251454 -4.1366329 -4.1477575 -4.1564713 -4.1629877 -4.1700559 -4.1776605 -4.1893249 -4.2055149][-4.2175832 -4.2145467 -4.2043743 -4.18812 -4.174674 -4.1657753 -4.1680484 -4.1771626 -4.1870928 -4.1945648 -4.1991181 -4.2029381 -4.2066426 -4.2134962 -4.224824][-4.2504692 -4.2543488 -4.2519565 -4.2433476 -4.2345495 -4.227448 -4.2287269 -4.2347279 -4.2415943 -4.2457771 -4.2456689 -4.243968 -4.2419548 -4.242568 -4.2474546][-4.279 -4.2878747 -4.2921867 -4.29052 -4.2869267 -4.2818952 -4.2817111 -4.2840328 -4.2861891 -4.2860541 -4.2815638 -4.275435 -4.269146 -4.2647505 -4.2644062][-4.2889986 -4.2986832 -4.3060937 -4.3090167 -4.3095164 -4.307344 -4.3060966 -4.3054614 -4.3042035 -4.3019376 -4.2961674 -4.2895875 -4.2841153 -4.2794247 -4.2772403][-4.2860765 -4.2924094 -4.297914 -4.3011789 -4.3030591 -4.3020711 -4.299952 -4.2978315 -4.2955122 -4.2940307 -4.2898045 -4.2857075 -4.2841983 -4.2823653 -4.2808375][-4.2733026 -4.2726045 -4.2719603 -4.2717791 -4.2721138 -4.2710533 -4.269515 -4.267695 -4.2668929 -4.2684307 -4.2680254 -4.2687783 -4.272943 -4.27575 -4.2766972][-4.2666812 -4.2588172 -4.2518921 -4.2480278 -4.2464662 -4.2453375 -4.245019 -4.2445793 -4.2462759 -4.2512169 -4.2547684 -4.2593756 -4.2678161 -4.2746353 -4.2781219]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch16/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch16/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 23:53:22.454892: step 30010, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.519 sec/batch; 57h:29m:38s remains)
INFO - root - 2017-12-07 23:53:38.673443: step 30020, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.613 sec/batch; 61h:03m:14s remains)
INFO - root - 2017-12-07 23:53:55.060688: step 30030, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 1.703 sec/batch; 64h:27m:22s remains)
INFO - root - 2017-12-07 23:54:11.249952: step 30040, loss = 2.08, batch loss = 2.03 (10.4 examples/sec; 1.534 sec/batch; 58h:01m:39s remains)
INFO - root - 2017-12-07 23:54:27.615613: step 30050, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.694 sec/batch; 64h:06m:27s remains)
INFO - root - 2017-12-07 23:54:44.017824: step 30060, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 1.620 sec/batch; 61h:17m:28s remains)
INFO - root - 2017-12-07 23:55:00.261039: step 30070, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.650 sec/batch; 62h:24m:41s remains)
INFO - root - 2017-12-07 23:55:16.468937: step 30080, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.582 sec/batch; 59h:50m:47s remains)
INFO - root - 2017-12-07 23:55:32.826189: step 30090, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 60h:31m:03s remains)
INFO - root - 2017-12-07 23:55:49.170402: step 30100, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.663 sec/batch; 62h:54m:45s remains)
2017-12-07 23:55:50.509948: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3804197 -4.3808722 -4.3794665 -4.376255 -4.3719025 -4.36838 -4.3649197 -4.3612108 -4.3565068 -4.35281 -4.3515229 -4.3538089 -4.3585482 -4.364265 -4.3690472][-4.38361 -4.384438 -4.3818674 -4.3747153 -4.3636513 -4.3513041 -4.3395295 -4.3281779 -4.31789 -4.3132787 -4.3149552 -4.3229666 -4.3360634 -4.3518753 -4.3650765][-4.3824444 -4.3819714 -4.3751097 -4.3595886 -4.3375883 -4.3130255 -4.2878637 -4.2627549 -4.2438474 -4.2370658 -4.2453356 -4.2663927 -4.2949548 -4.3265295 -4.3526411][-4.3726177 -4.3679166 -4.3534627 -4.3249135 -4.2879858 -4.246572 -4.2020154 -4.1567392 -4.1277342 -4.1215343 -4.1428747 -4.1831832 -4.2313695 -4.28409 -4.32794][-4.3581576 -4.3482904 -4.3238144 -4.2796807 -4.22592 -4.1635337 -4.0948305 -4.025496 -3.9832578 -3.9794044 -4.0205979 -4.0830293 -4.151371 -4.2264061 -4.2892771][-4.3517232 -4.3388338 -4.3074574 -4.25265 -4.1866455 -4.1058059 -4.0130162 -3.9209847 -3.8662219 -3.8645728 -3.9287944 -4.0138459 -4.09506 -4.1791153 -4.2484312][-4.3567324 -4.34585 -4.3175058 -4.2672577 -4.2039728 -4.1219544 -4.0245762 -3.9255385 -3.8631165 -3.8564231 -3.9251478 -4.01361 -4.0925159 -4.1687875 -4.226831][-4.3611622 -4.3533874 -4.3328705 -4.2952676 -4.2430282 -4.1773076 -4.1019845 -4.0268588 -3.97682 -3.9671788 -4.0204306 -4.0879989 -4.1496105 -4.2050414 -4.2407231][-4.3659353 -4.3596768 -4.3454475 -4.3199668 -4.2817526 -4.2357674 -4.1888638 -4.14302 -4.111773 -4.1051936 -4.1417637 -4.1870055 -4.2308807 -4.2693596 -4.2889085][-4.3733325 -4.3690028 -4.3608303 -4.3477764 -4.3262463 -4.299593 -4.2743917 -4.2491665 -4.2322836 -4.2276249 -4.2465339 -4.2717009 -4.300283 -4.3264971 -4.3384328][-4.3791375 -4.3766236 -4.3746486 -4.3720527 -4.3642392 -4.351367 -4.3395572 -4.3279915 -4.3188305 -4.3129559 -4.3176932 -4.3285089 -4.3448372 -4.3623981 -4.3716817][-4.3812103 -4.3807321 -4.3830595 -4.3868494 -4.3879085 -4.3847847 -4.3809409 -4.3767118 -4.3723741 -4.3671947 -4.3656111 -4.3683167 -4.3757567 -4.3858743 -4.3920722][-4.3786163 -4.378408 -4.3800225 -4.3827562 -4.3854117 -4.3880916 -4.3913059 -4.3938308 -4.3952875 -4.393929 -4.3915467 -4.3904338 -4.3920946 -4.3960538 -4.3988361][-4.3730178 -4.3714709 -4.3708992 -4.3706174 -4.3722854 -4.3755317 -4.3800893 -4.3842177 -4.3873959 -4.3886356 -4.3876429 -4.3868308 -4.3875542 -4.3899531 -4.392415][-4.3571553 -4.3513541 -4.3493109 -4.3492122 -4.3527594 -4.3612809 -4.3704829 -4.3766541 -4.380055 -4.3810449 -4.3799062 -4.3784871 -4.3785963 -4.3794241 -4.3809066]]...]
INFO - root - 2017-12-07 23:56:06.792770: step 30110, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.598 sec/batch; 60h:26m:36s remains)
INFO - root - 2017-12-07 23:56:23.167879: step 30120, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.600 sec/batch; 60h:30m:56s remains)
INFO - root - 2017-12-07 23:56:39.337985: step 30130, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.628 sec/batch; 61h:32m:44s remains)
INFO - root - 2017-12-07 23:56:55.691313: step 30140, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.557 sec/batch; 58h:52m:24s remains)
INFO - root - 2017-12-07 23:57:12.156284: step 30150, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 1.646 sec/batch; 62h:13m:38s remains)
INFO - root - 2017-12-07 23:57:28.561013: step 30160, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.614 sec/batch; 61h:00m:42s remains)
INFO - root - 2017-12-07 23:57:44.912024: step 30170, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.720 sec/batch; 65h:01m:56s remains)
INFO - root - 2017-12-07 23:58:01.116294: step 30180, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.576 sec/batch; 59h:34m:01s remains)
INFO - root - 2017-12-07 23:58:17.382823: step 30190, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.642 sec/batch; 62h:03m:57s remains)
INFO - root - 2017-12-07 23:58:33.438624: step 30200, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.593 sec/batch; 60h:11m:30s remains)
2017-12-07 23:58:35.030462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1528273 -4.154614 -4.1723061 -4.1931529 -4.2096019 -4.2178564 -4.2180004 -4.2063227 -4.1875935 -4.1843286 -4.2119908 -4.257545 -4.2995172 -4.3306 -4.3488159][-4.1285968 -4.1269765 -4.1509423 -4.1759014 -4.18609 -4.18498 -4.1832805 -4.179935 -4.1728039 -4.1763811 -4.200841 -4.24342 -4.28672 -4.3210807 -4.3428741][-4.116972 -4.1175623 -4.1461987 -4.1729727 -4.176827 -4.1617861 -4.1521273 -4.1529951 -4.1577878 -4.1676126 -4.1887889 -4.2265091 -4.2724543 -4.3110714 -4.3365517][-4.12197 -4.1275969 -4.1542621 -4.176126 -4.1754847 -4.14888 -4.1283894 -4.1245031 -4.1331429 -4.1444221 -4.1628866 -4.2005258 -4.2512145 -4.2973413 -4.3286743][-4.1411862 -4.148241 -4.1661749 -4.1791515 -4.1746182 -4.1443195 -4.1130848 -4.0997038 -4.1033435 -4.10758 -4.1268206 -4.1710911 -4.2269163 -4.2805305 -4.3195376][-4.1719918 -4.1739759 -4.1807823 -4.1833596 -4.1740294 -4.14394 -4.1113048 -4.0925035 -4.0914969 -4.0910625 -4.11301 -4.1627622 -4.2200809 -4.274508 -4.3161116][-4.2029853 -4.2026496 -4.2012811 -4.1941137 -4.1774983 -4.1495872 -4.1208439 -4.1033535 -4.1079311 -4.1136456 -4.1416731 -4.1896505 -4.2362266 -4.2822266 -4.3184309][-4.2202854 -4.22363 -4.2234206 -4.2126212 -4.1936951 -4.1687636 -4.1433678 -4.1289005 -4.1368494 -4.1515784 -4.1860719 -4.226234 -4.2575827 -4.2916064 -4.3211479][-4.21961 -4.2226372 -4.2250972 -4.21991 -4.2081981 -4.1901531 -4.1693892 -4.1544218 -4.1610036 -4.1789341 -4.2175117 -4.2529078 -4.2740059 -4.2989345 -4.323854][-4.2136879 -4.21347 -4.2144728 -4.2157536 -4.2160392 -4.208035 -4.1943889 -4.1821136 -4.1822052 -4.193871 -4.2291059 -4.2617426 -4.2803221 -4.3016777 -4.3249917][-4.2190309 -4.2154083 -4.2146654 -4.2189827 -4.229847 -4.2303753 -4.2248497 -4.2152758 -4.2092018 -4.212585 -4.2393403 -4.2673578 -4.2849631 -4.3037539 -4.3257327][-4.24272 -4.2402377 -4.2379022 -4.2407312 -4.2546186 -4.2592597 -4.2577171 -4.2526879 -4.2465754 -4.2456193 -4.2625432 -4.2836328 -4.2970929 -4.3119178 -4.32908][-4.2708669 -4.2703357 -4.2688894 -4.2701306 -4.27934 -4.2824879 -4.2821097 -4.2828326 -4.2799106 -4.2778411 -4.2878566 -4.3029647 -4.3141561 -4.3262463 -4.337894][-4.2955165 -4.2961612 -4.2957854 -4.2965312 -4.3016486 -4.3021379 -4.3028193 -4.3064055 -4.3066325 -4.3044963 -4.3094726 -4.3202553 -4.3301845 -4.3401928 -4.3475819][-4.3167248 -4.3178468 -4.3184805 -4.3189292 -4.3213568 -4.3204985 -4.3210177 -4.3244848 -4.3254981 -4.3235989 -4.3261123 -4.33319 -4.3400712 -4.3470035 -4.3519835]]...]
INFO - root - 2017-12-07 23:58:51.397237: step 30210, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.586 sec/batch; 59h:56m:29s remains)
INFO - root - 2017-12-07 23:59:07.698472: step 30220, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.622 sec/batch; 61h:17m:10s remains)
INFO - root - 2017-12-07 23:59:24.034509: step 30230, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.627 sec/batch; 61h:28m:22s remains)
INFO - root - 2017-12-07 23:59:40.307706: step 30240, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.580 sec/batch; 59h:41m:41s remains)
INFO - root - 2017-12-07 23:59:56.432727: step 30250, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.701 sec/batch; 64h:16m:35s remains)
INFO - root - 2017-12-08 00:00:12.721580: step 30260, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.566 sec/batch; 59h:08m:17s remains)
INFO - root - 2017-12-08 00:00:28.961216: step 30270, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.636 sec/batch; 61h:46m:56s remains)
INFO - root - 2017-12-08 00:00:45.165697: step 30280, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.606 sec/batch; 60h:40m:11s remains)
INFO - root - 2017-12-08 00:01:01.402533: step 30290, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.693 sec/batch; 63h:57m:08s remains)
INFO - root - 2017-12-08 00:01:17.554211: step 30300, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.584 sec/batch; 59h:50m:10s remains)
2017-12-08 00:01:18.949856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1965609 -4.1644945 -4.1426821 -4.1379719 -4.1446929 -4.1392722 -4.1521597 -4.1786308 -4.2141943 -4.2404051 -4.2486138 -4.2514367 -4.238934 -4.196382 -4.1290822][-4.195981 -4.1638789 -4.137238 -4.1370072 -4.1511626 -4.1500826 -4.1583385 -4.1758847 -4.212132 -4.2381325 -4.246871 -4.2513757 -4.2468262 -4.2102413 -4.151247][-4.2383771 -4.2231607 -4.2051468 -4.2062492 -4.2118106 -4.1997023 -4.1850033 -4.1789732 -4.2045808 -4.2288833 -4.2436819 -4.259469 -4.2736483 -4.2599421 -4.2259769][-4.2933745 -4.2943 -4.28452 -4.276412 -4.2622743 -4.2297058 -4.1848269 -4.14682 -4.160645 -4.2013659 -4.2431498 -4.2787161 -4.3129296 -4.3232727 -4.3108006][-4.3265791 -4.33503 -4.3318219 -4.3162432 -4.2827654 -4.2278395 -4.1497188 -4.0757155 -4.078692 -4.14713 -4.2283907 -4.2864122 -4.3373103 -4.3682156 -4.3668737][-4.3327208 -4.3411922 -4.3437109 -4.3241272 -4.2745304 -4.1971383 -4.0846939 -3.9714425 -3.9685311 -4.0705175 -4.19367 -4.276988 -4.341764 -4.3820324 -4.3814793][-4.3321595 -4.3353348 -4.3334332 -4.3036909 -4.2409654 -4.1514845 -4.0197811 -3.8865798 -3.8824818 -4.0045414 -4.1516328 -4.2527375 -4.3231678 -4.3645124 -4.3653874][-4.3144426 -4.3090053 -4.2948208 -4.2571144 -4.19201 -4.104126 -3.977211 -3.8590729 -3.8630919 -3.9811559 -4.1238308 -4.2238255 -4.2886887 -4.3218226 -4.31789][-4.2821493 -4.2648096 -4.241087 -4.2051682 -4.14874 -4.0743265 -3.9691303 -3.8725829 -3.8755405 -3.9761837 -4.1026187 -4.1895084 -4.2415071 -4.2659574 -4.2523608][-4.2563882 -4.2338982 -4.2096577 -4.1817923 -4.1434917 -4.0965633 -4.0228763 -3.9461884 -3.9404936 -4.012836 -4.1067939 -4.1683822 -4.2002826 -4.2154584 -4.1964803][-4.2602496 -4.2413359 -4.2245278 -4.2077579 -4.1863642 -4.1583328 -4.1094542 -4.0512829 -4.0410986 -4.086442 -4.1479082 -4.1818986 -4.1938672 -4.2003679 -4.1871619][-4.2817492 -4.26869 -4.2590008 -4.2500696 -4.2405434 -4.2254844 -4.1949239 -4.1562996 -4.147347 -4.1726747 -4.2085204 -4.2233186 -4.2242584 -4.2252111 -4.2197375][-4.2995615 -4.2915478 -4.286437 -4.2829871 -4.2810326 -4.2757282 -4.2610564 -4.2421031 -4.2377758 -4.2503452 -4.2717013 -4.2793255 -4.2757769 -4.2729893 -4.2716269][-4.3125563 -4.30967 -4.3088427 -4.3102407 -4.3125386 -4.311717 -4.3044434 -4.29545 -4.2936387 -4.2995849 -4.3128424 -4.3185005 -4.31499 -4.3122573 -4.3127093][-4.3205009 -4.3220339 -4.3252459 -4.3306785 -4.3358226 -4.3363161 -4.3323083 -4.3259945 -4.3212366 -4.3222857 -4.3306623 -4.3341436 -4.3307061 -4.329423 -4.3323674]]...]
INFO - root - 2017-12-08 00:01:35.003707: step 30310, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.685 sec/batch; 63h:37m:36s remains)
INFO - root - 2017-12-08 00:01:51.297179: step 30320, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.614 sec/batch; 60h:57m:22s remains)
INFO - root - 2017-12-08 00:02:07.582194: step 30330, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.713 sec/batch; 64h:39m:42s remains)
INFO - root - 2017-12-08 00:02:23.775260: step 30340, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.578 sec/batch; 59h:34m:35s remains)
INFO - root - 2017-12-08 00:02:39.961643: step 30350, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.646 sec/batch; 62h:07m:42s remains)
INFO - root - 2017-12-08 00:02:56.151245: step 30360, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.588 sec/batch; 59h:55m:49s remains)
INFO - root - 2017-12-08 00:03:11.989611: step 30370, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.574 sec/batch; 59h:24m:43s remains)
INFO - root - 2017-12-08 00:03:28.216048: step 30380, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.697 sec/batch; 64h:03m:34s remains)
INFO - root - 2017-12-08 00:03:44.487449: step 30390, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.594 sec/batch; 60h:10m:18s remains)
INFO - root - 2017-12-08 00:04:00.791583: step 30400, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.685 sec/batch; 63h:34m:55s remains)
2017-12-08 00:04:02.263843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.287322 -4.2856908 -4.2889452 -4.2914829 -4.2913766 -4.2875619 -4.2717371 -4.2651677 -4.2771239 -4.2894707 -4.3047128 -4.324419 -4.3312378 -4.3302574 -4.3255024][-4.2752218 -4.2706017 -4.2755117 -4.2821822 -4.2854795 -4.2825856 -4.2619967 -4.2558465 -4.2752175 -4.291605 -4.3086152 -4.3318748 -4.3389726 -4.3312178 -4.3180528][-4.2725973 -4.2684207 -4.2762303 -4.2856708 -4.2898464 -4.28413 -4.2600121 -4.2535534 -4.2781081 -4.29705 -4.3143587 -4.3361468 -4.3402796 -4.3274336 -4.30725][-4.2714672 -4.26834 -4.2788463 -4.2932649 -4.2964191 -4.2856159 -4.257452 -4.2515731 -4.2795038 -4.2999678 -4.3187885 -4.339046 -4.3406625 -4.32419 -4.301158][-4.2666874 -4.26094 -4.2693591 -4.2838526 -4.281189 -4.2638922 -4.2304664 -4.222332 -4.2582369 -4.2862659 -4.3099384 -4.3313675 -4.3320212 -4.3157487 -4.2964334][-4.2547407 -4.2430716 -4.2484441 -4.2571273 -4.2425513 -4.2113628 -4.1638789 -4.1442637 -4.1887493 -4.2364779 -4.2772865 -4.3110981 -4.3204618 -4.3131757 -4.2996106][-4.2442489 -4.2263255 -4.2249618 -4.2188988 -4.1863132 -4.1334343 -4.0568628 -4.0126448 -4.0668168 -4.1426134 -4.2122135 -4.2705617 -4.2985873 -4.30434 -4.2984848][-4.2319593 -4.2110338 -4.2034388 -4.1855392 -4.1412292 -4.0718622 -3.9661422 -3.8938379 -3.9504139 -4.0452771 -4.1378741 -4.2186608 -4.2631617 -4.2819977 -4.2861662][-4.22111 -4.2020383 -4.1968403 -4.185545 -4.1522341 -4.0919452 -3.9912281 -3.9193664 -3.9673829 -4.0491514 -4.1296439 -4.2033753 -4.2456579 -4.2684255 -4.27872][-4.2189136 -4.2077312 -4.2128344 -4.2149882 -4.1976838 -4.1545682 -4.0804777 -4.0305939 -4.0673389 -4.1205392 -4.1705155 -4.2187605 -4.2465668 -4.263917 -4.273067][-4.2224708 -4.2185311 -4.2314887 -4.2425203 -4.2340941 -4.2025785 -4.1511607 -4.12031 -4.1499991 -4.1864181 -4.2164822 -4.2458963 -4.2624211 -4.2736583 -4.2777147][-4.2302842 -4.2298422 -4.2446413 -4.2580609 -4.2538872 -4.2304406 -4.197475 -4.1828156 -4.2080221 -4.2350917 -4.2577562 -4.2781558 -4.2891541 -4.2952352 -4.2946534][-4.243485 -4.2424159 -4.2544155 -4.2680454 -4.2686267 -4.2536135 -4.2357845 -4.2329936 -4.252883 -4.2724 -4.2887011 -4.3021989 -4.3092322 -4.3116469 -4.3081193][-4.2572651 -4.2549667 -4.2637415 -4.2746344 -4.2748208 -4.2651162 -4.2552066 -4.2558308 -4.2687778 -4.2821736 -4.2947159 -4.3060894 -4.3138266 -4.316433 -4.3149981][-4.2700047 -4.265975 -4.2706857 -4.2774243 -4.2762446 -4.2691712 -4.261858 -4.2604437 -4.2658491 -4.2739792 -4.2843127 -4.2969546 -4.3088126 -4.315217 -4.3175473]]...]
INFO - root - 2017-12-08 00:04:18.693276: step 30410, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.701 sec/batch; 64h:11m:19s remains)
INFO - root - 2017-12-08 00:04:34.847117: step 30420, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.580 sec/batch; 59h:37m:27s remains)
INFO - root - 2017-12-08 00:04:51.015944: step 30430, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 1.712 sec/batch; 64h:34m:43s remains)
INFO - root - 2017-12-08 00:05:07.300889: step 30440, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.592 sec/batch; 60h:04m:20s remains)
INFO - root - 2017-12-08 00:05:23.497153: step 30450, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.683 sec/batch; 63h:28m:17s remains)
INFO - root - 2017-12-08 00:05:39.695918: step 30460, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.584 sec/batch; 59h:43m:43s remains)
INFO - root - 2017-12-08 00:05:56.072880: step 30470, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 1.579 sec/batch; 59h:32m:47s remains)
INFO - root - 2017-12-08 00:06:12.347679: step 30480, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.640 sec/batch; 61h:51m:49s remains)
INFO - root - 2017-12-08 00:06:28.248345: step 30490, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.589 sec/batch; 59h:54m:36s remains)
INFO - root - 2017-12-08 00:06:44.784504: step 30500, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 1.768 sec/batch; 66h:40m:23s remains)
2017-12-08 00:06:46.173104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2741222 -4.2816544 -4.2797036 -4.2775073 -4.2859573 -4.2869811 -4.2733622 -4.2528458 -4.236444 -4.2296042 -4.2383652 -4.2542152 -4.2673612 -4.277997 -4.2840958][-4.2733264 -4.2834606 -4.2782221 -4.2705369 -4.2804408 -4.2809062 -4.2643375 -4.2407861 -4.2256932 -4.2210402 -4.2315321 -4.2504172 -4.2625003 -4.2710509 -4.275629][-4.2699404 -4.2823858 -4.2749677 -4.2648 -4.2727437 -4.2678785 -4.2442794 -4.2137194 -4.2002039 -4.2023311 -4.2187791 -4.2437191 -4.2552609 -4.2649317 -4.2687073][-4.2677817 -4.2768555 -4.2673235 -4.2538075 -4.2529554 -4.2402458 -4.2114973 -4.176198 -4.1681275 -4.1821909 -4.207665 -4.2375684 -4.2503209 -4.2597227 -4.265583][-4.2552481 -4.2590585 -4.2459111 -4.2278719 -4.22046 -4.2049561 -4.1700864 -4.1269531 -4.1236734 -4.1548767 -4.1963806 -4.2333426 -4.2462115 -4.2547126 -4.2621374][-4.2279272 -4.2300353 -4.2162123 -4.1956868 -4.1855078 -4.1636057 -4.1077938 -4.0400858 -4.0402389 -4.1019578 -4.1701736 -4.2209225 -4.2393165 -4.2483649 -4.2551289][-4.1992769 -4.2024188 -4.1856556 -4.1604276 -4.1432209 -4.1053424 -4.0184083 -3.92194 -3.9296947 -4.026855 -4.1245689 -4.1906528 -4.2211294 -4.2348337 -4.2434983][-4.1750641 -4.1769423 -4.15881 -4.1291089 -4.104382 -4.05845 -3.9673305 -3.8775504 -3.8949394 -3.9987981 -4.1008482 -4.1701603 -4.2074003 -4.2267976 -4.2375][-4.175127 -4.18508 -4.1734319 -4.1418509 -4.1128306 -4.0720854 -4.0084305 -3.9543571 -3.9732106 -4.0502577 -4.1260567 -4.1799731 -4.2126727 -4.2311082 -4.2401586][-4.1856513 -4.2074151 -4.2070909 -4.1796761 -4.1494365 -4.114131 -4.0748553 -4.047956 -4.0659218 -4.1185865 -4.1700134 -4.2061439 -4.2300258 -4.2447166 -4.2510338][-4.2034492 -4.2318964 -4.2423515 -4.2225037 -4.1935158 -4.1632686 -4.136641 -4.1248412 -4.1404696 -4.1759639 -4.2114854 -4.2364616 -4.25355 -4.2648149 -4.2686119][-4.2229161 -4.2514138 -4.2636476 -4.2487707 -4.2236357 -4.1984076 -4.1797743 -4.1764803 -4.1914215 -4.2177391 -4.2434816 -4.2622104 -4.2754569 -4.2845836 -4.2881374][-4.2498527 -4.2730651 -4.2834249 -4.2738709 -4.2567449 -4.2399621 -4.2289891 -4.2293334 -4.2394075 -4.2565002 -4.2732706 -4.2875957 -4.2978582 -4.3055067 -4.3092246][-4.2880735 -4.3014793 -4.307013 -4.3023949 -4.2940598 -4.2856903 -4.28063 -4.2809806 -4.2858477 -4.2946076 -4.3033462 -4.3118286 -4.3179383 -4.3224268 -4.3255997][-4.3162966 -4.3207445 -4.3218575 -4.3202734 -4.3178821 -4.3143015 -4.31118 -4.3109779 -4.3131881 -4.3178139 -4.3226995 -4.3276544 -4.3314776 -4.334877 -4.338315]]...]
INFO - root - 2017-12-08 00:07:02.495867: step 30510, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.646 sec/batch; 62h:04m:21s remains)
INFO - root - 2017-12-08 00:07:18.940149: step 30520, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.597 sec/batch; 60h:13m:27s remains)
INFO - root - 2017-12-08 00:07:35.346927: step 30530, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 1.652 sec/batch; 62h:17m:11s remains)
INFO - root - 2017-12-08 00:07:51.217383: step 30540, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 1.488 sec/batch; 56h:06m:13s remains)
INFO - root - 2017-12-08 00:08:07.565279: step 30550, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 1.750 sec/batch; 65h:57m:13s remains)
INFO - root - 2017-12-08 00:08:23.830507: step 30560, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.633 sec/batch; 61h:32m:59s remains)
INFO - root - 2017-12-08 00:08:40.087568: step 30570, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.692 sec/batch; 63h:45m:40s remains)
INFO - root - 2017-12-08 00:08:56.333889: step 30580, loss = 2.08, batch loss = 2.03 (9.9 examples/sec; 1.611 sec/batch; 60h:43m:33s remains)
INFO - root - 2017-12-08 00:09:12.519126: step 30590, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.609 sec/batch; 60h:38m:07s remains)
INFO - root - 2017-12-08 00:09:28.496409: step 30600, loss = 2.07, batch loss = 2.02 (10.6 examples/sec; 1.507 sec/batch; 56h:46m:14s remains)
2017-12-08 00:09:29.802941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2828603 -4.2864032 -4.2884073 -4.280417 -4.2609963 -4.2414222 -4.2198267 -4.2033563 -4.2067204 -4.2250094 -4.252768 -4.2806778 -4.2968082 -4.3092279 -4.3140092][-4.2739139 -4.2792587 -4.2835464 -4.2754979 -4.2514052 -4.2243056 -4.1931243 -4.1694241 -4.16896 -4.1945028 -4.2346253 -4.2707486 -4.2923079 -4.3071771 -4.3111033][-4.2559886 -4.264565 -4.2739129 -4.2680969 -4.2443004 -4.2110662 -4.1691885 -4.1327844 -4.1261334 -4.1620812 -4.2142949 -4.2589746 -4.2887683 -4.3045516 -4.3071489][-4.2351751 -4.2520876 -4.2675514 -4.2646084 -4.2424183 -4.2062669 -4.1536117 -4.1038885 -4.0966487 -4.1468697 -4.2102575 -4.2588139 -4.2890577 -4.3026409 -4.3042564][-4.2174873 -4.2464061 -4.270884 -4.2660646 -4.236845 -4.1869755 -4.11614 -4.0561666 -4.0614157 -4.1293812 -4.2038221 -4.2542472 -4.2805715 -4.2912054 -4.2942963][-4.212719 -4.2525492 -4.2786341 -4.2645073 -4.2181077 -4.1425891 -4.0386181 -3.9648719 -3.9922931 -4.0880957 -4.1749821 -4.2291908 -4.2578897 -4.2687044 -4.2722774][-4.224462 -4.26812 -4.2847023 -4.2541089 -4.1823034 -4.0737042 -3.9209776 -3.8146818 -3.87985 -4.0244923 -4.1350121 -4.1982188 -4.231421 -4.2417808 -4.2443495][-4.2469163 -4.2801766 -4.2859921 -4.2423882 -4.1511631 -4.0159321 -3.8224461 -3.678169 -3.7867689 -3.9815016 -4.1205025 -4.1915526 -4.2245226 -4.2286468 -4.2273216][-4.2701778 -4.2880011 -4.2858944 -4.236558 -4.1455388 -4.0179625 -3.8518093 -3.7430632 -3.8451262 -4.0216441 -4.1488576 -4.211937 -4.2362571 -4.2325273 -4.2254348][-4.2872691 -4.2932439 -4.286305 -4.2442551 -4.171277 -4.0779152 -3.9740481 -3.9179447 -3.9857626 -4.1015406 -4.1871929 -4.2298126 -4.2439518 -4.2383075 -4.2323875][-4.2826695 -4.2839704 -4.280354 -4.256464 -4.2151875 -4.1563282 -4.0912442 -4.0581765 -4.0934734 -4.1605239 -4.2118883 -4.2373147 -4.2420626 -4.2346277 -4.2312264][-4.2642822 -4.2603164 -4.2620754 -4.2591047 -4.2452288 -4.210475 -4.1585374 -4.128655 -4.1459293 -4.1859975 -4.2170563 -4.2362037 -4.2366905 -4.22176 -4.2135253][-4.2487431 -4.2437592 -4.2504115 -4.2585268 -4.259737 -4.2375679 -4.1927557 -4.1672187 -4.1768208 -4.1983891 -4.2195559 -4.2391577 -4.2386303 -4.2180109 -4.200932][-4.24167 -4.2399669 -4.2495275 -4.26255 -4.2700882 -4.2576113 -4.2252426 -4.2061095 -4.2143593 -4.2277737 -4.2396622 -4.2564 -4.2544708 -4.2296958 -4.2049832][-4.2420855 -4.2446127 -4.2541647 -4.2692661 -4.2791004 -4.2738476 -4.2512441 -4.2379704 -4.2449775 -4.25911 -4.2693548 -4.2804084 -4.2731309 -4.2467074 -4.2184405]]...]
INFO - root - 2017-12-08 00:09:46.159698: step 30610, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 1.718 sec/batch; 64h:44m:47s remains)
INFO - root - 2017-12-08 00:10:02.495330: step 30620, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.571 sec/batch; 59h:11m:49s remains)
INFO - root - 2017-12-08 00:10:18.786430: step 30630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.699 sec/batch; 64h:00m:16s remains)
INFO - root - 2017-12-08 00:10:35.232639: step 30640, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.609 sec/batch; 60h:35m:42s remains)
INFO - root - 2017-12-08 00:10:51.660897: step 30650, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.611 sec/batch; 60h:41m:02s remains)
INFO - root - 2017-12-08 00:11:07.668613: step 30660, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.647 sec/batch; 62h:02m:01s remains)
INFO - root - 2017-12-08 00:11:24.113465: step 30670, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.613 sec/batch; 60h:44m:45s remains)
INFO - root - 2017-12-08 00:11:40.566427: step 30680, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.708 sec/batch; 64h:18m:49s remains)
INFO - root - 2017-12-08 00:11:56.794222: step 30690, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.581 sec/batch; 59h:32m:32s remains)
INFO - root - 2017-12-08 00:12:13.247650: step 30700, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.731 sec/batch; 65h:10m:18s remains)
2017-12-08 00:12:14.646352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2220306 -4.1987123 -4.1815281 -4.181262 -4.1886644 -4.2075787 -4.2173362 -4.2222939 -4.2410254 -4.2563982 -4.260088 -4.2576919 -4.2530575 -4.2493892 -4.2507706][-4.2069612 -4.1844525 -4.1644592 -4.1592941 -4.1666355 -4.1873512 -4.1989594 -4.2013583 -4.2163892 -4.2265596 -4.2271333 -4.2172103 -4.2059627 -4.2028704 -4.2082539][-4.1998672 -4.1858463 -4.1735964 -4.1722765 -4.1791239 -4.1950736 -4.2023277 -4.1974592 -4.2009597 -4.202868 -4.1994972 -4.1847434 -4.1695304 -4.1685734 -4.1777678][-4.1977344 -4.1913548 -4.1877589 -4.18942 -4.1959777 -4.20643 -4.2083793 -4.1956239 -4.1923466 -4.1961851 -4.1961837 -4.1813025 -4.1635985 -4.1622257 -4.1708164][-4.1802597 -4.1742115 -4.1722059 -4.1764808 -4.1859732 -4.1890764 -4.1803513 -4.1609421 -4.1572118 -4.1748276 -4.1879778 -4.1771593 -4.15773 -4.1523633 -4.1606345][-4.1567144 -4.1489983 -4.1414232 -4.1426773 -4.1484151 -4.1390266 -4.1160092 -4.088028 -4.0871506 -4.12036 -4.1468296 -4.1420288 -4.1234632 -4.1181936 -4.1312995][-4.1234722 -4.1168194 -4.1011682 -4.0917926 -4.0881987 -4.0640674 -4.0189414 -3.9703803 -3.972193 -4.0357718 -4.0873046 -4.0963969 -4.0875435 -4.0899768 -4.1128187][-4.1002355 -4.0978756 -4.0817904 -4.0676222 -4.0555515 -4.0189424 -3.9525263 -3.8862741 -3.8992071 -3.9990592 -4.0811181 -4.1111689 -4.116003 -4.12237 -4.14519][-4.1240029 -4.1233768 -4.1118093 -4.0994163 -4.08828 -4.0591879 -4.0050254 -3.9563706 -3.97886 -4.0705023 -4.147541 -4.1809497 -4.1864195 -4.1885929 -4.2044845][-4.17318 -4.1671562 -4.1554747 -4.1477652 -4.1456628 -4.137013 -4.1107311 -4.08722 -4.1054482 -4.1632609 -4.2131929 -4.2358971 -4.2373629 -4.2377615 -4.2515116][-4.2181582 -4.207407 -4.199111 -4.198967 -4.20544 -4.2094493 -4.2019348 -4.1936722 -4.2040172 -4.2317567 -4.2562294 -4.2687407 -4.270052 -4.2718105 -4.2835259][-4.2616339 -4.2485547 -4.2427545 -4.2473912 -4.2584152 -4.2660933 -4.2650528 -4.260931 -4.2634459 -4.2739911 -4.2859221 -4.2918367 -4.2940044 -4.2947183 -4.2998176][-4.2982526 -4.2826691 -4.2753291 -4.279695 -4.2914629 -4.2991352 -4.2986274 -4.2943983 -4.2916646 -4.2952037 -4.3030915 -4.3062024 -4.3051434 -4.30018 -4.2962413][-4.3141642 -4.2990222 -4.2917261 -4.2943592 -4.3037834 -4.3102636 -4.3088522 -4.3021955 -4.2950764 -4.2931652 -4.2973151 -4.2962341 -4.2884889 -4.277432 -4.2703991][-4.3090038 -4.2989726 -4.2930541 -4.2935872 -4.2989593 -4.301641 -4.2978663 -4.2888308 -4.2792158 -4.274507 -4.2749958 -4.2715125 -4.2616158 -4.2494569 -4.2458053]]...]
INFO - root - 2017-12-08 00:12:30.972018: step 30710, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.706 sec/batch; 64h:14m:55s remains)
INFO - root - 2017-12-08 00:12:46.824915: step 30720, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.549 sec/batch; 58h:18m:48s remains)
INFO - root - 2017-12-08 00:13:03.058608: step 30730, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.717 sec/batch; 64h:38m:47s remains)
INFO - root - 2017-12-08 00:13:19.213645: step 30740, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.629 sec/batch; 61h:18m:16s remains)
INFO - root - 2017-12-08 00:13:35.528453: step 30750, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.629 sec/batch; 61h:19m:13s remains)
INFO - root - 2017-12-08 00:13:51.879297: step 30760, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.621 sec/batch; 61h:01m:19s remains)
INFO - root - 2017-12-08 00:14:08.188250: step 30770, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 1.507 sec/batch; 56h:42m:22s remains)
INFO - root - 2017-12-08 00:14:24.500949: step 30780, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.745 sec/batch; 65h:40m:06s remains)
INFO - root - 2017-12-08 00:14:40.588948: step 30790, loss = 2.05, batch loss = 1.99 (10.2 examples/sec; 1.565 sec/batch; 58h:53m:06s remains)
INFO - root - 2017-12-08 00:14:56.748316: step 30800, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.638 sec/batch; 61h:37m:34s remains)
2017-12-08 00:14:58.099632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3209934 -4.3219314 -4.3169422 -4.3082495 -4.2964363 -4.2847538 -4.2781968 -4.2791104 -4.283597 -4.2866158 -4.2873182 -4.2876916 -4.2899752 -4.2923703 -4.2997961][-4.2991166 -4.3029404 -4.2994041 -4.2898903 -4.2767949 -4.2625422 -4.2517204 -4.2514863 -4.25753 -4.2620449 -4.2647886 -4.2666569 -4.2701683 -4.2740078 -4.2841711][-4.2849426 -4.2899537 -4.2874775 -4.2785177 -4.2663107 -4.2508254 -4.2364922 -4.2345867 -4.2406 -4.2488208 -4.2553954 -4.2587891 -4.2629709 -4.2680235 -4.2795897][-4.2702951 -4.2754016 -4.2727842 -4.2629137 -4.2488933 -4.2327027 -4.2161064 -4.2110443 -4.2169113 -4.2314372 -4.2447572 -4.2532177 -4.2603478 -4.2691169 -4.2800651][-4.2457085 -4.2485976 -4.2431297 -4.2275219 -4.20621 -4.182034 -4.1548233 -4.1385684 -4.1448555 -4.1742105 -4.2050719 -4.22643 -4.2438822 -4.2614155 -4.2763634][-4.2190251 -4.2225065 -4.2119126 -4.188004 -4.15388 -4.1097879 -4.0559678 -4.01151 -4.0105739 -4.064002 -4.1244078 -4.1662326 -4.1998272 -4.232482 -4.2573524][-4.1923928 -4.2042518 -4.1912379 -4.1587229 -4.1062446 -4.0364718 -3.9485707 -3.8587098 -3.8393998 -3.92314 -4.0234118 -4.092587 -4.1447897 -4.1916046 -4.2295065][-4.1708736 -4.1967139 -4.1902461 -4.1580997 -4.1014256 -4.0263033 -3.9313343 -3.8239408 -3.7827778 -3.8659489 -3.978174 -4.057035 -4.110785 -4.1607456 -4.2060513][-4.163826 -4.2009611 -4.2019186 -4.1778812 -4.1337767 -4.0794463 -4.0224223 -3.9623737 -3.9369516 -3.978699 -4.0442858 -4.0907764 -4.1224608 -4.158504 -4.1980505][-4.1725645 -4.2078695 -4.2095494 -4.192512 -4.1632686 -4.1283865 -4.099556 -4.0755954 -4.066762 -4.0885868 -4.1205854 -4.1391087 -4.151341 -4.172051 -4.2013][-4.1949849 -4.2201228 -4.2199788 -4.2058845 -4.1827559 -4.1561904 -4.1387696 -4.1326094 -4.1313658 -4.1408606 -4.1543021 -4.1603317 -4.1663504 -4.1814756 -4.2064533][-4.2167015 -4.2335658 -4.2355952 -4.224956 -4.2031546 -4.17618 -4.1605558 -4.1576042 -4.1578641 -4.1602197 -4.1651893 -4.1686583 -4.1754103 -4.1900387 -4.2131643][-4.2322249 -4.2468781 -4.2491708 -4.2377996 -4.212194 -4.1813068 -4.1653934 -4.1628537 -4.1682668 -4.17386 -4.1835346 -4.187017 -4.1885476 -4.1966128 -4.2159915][-4.24001 -4.2513552 -4.2525969 -4.2405257 -4.2149024 -4.1894217 -4.1799312 -4.1840725 -4.1972241 -4.2092113 -4.2210755 -4.21991 -4.2098026 -4.2036486 -4.2177176][-4.2448421 -4.2504106 -4.249722 -4.2375555 -4.2182021 -4.2066336 -4.2075653 -4.2196364 -4.2332697 -4.2417784 -4.249866 -4.2468324 -4.2288418 -4.2130585 -4.2209678]]...]
INFO - root - 2017-12-08 00:15:14.497192: step 30810, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.707 sec/batch; 64h:13m:53s remains)
INFO - root - 2017-12-08 00:15:30.657309: step 30820, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.612 sec/batch; 60h:38m:20s remains)
INFO - root - 2017-12-08 00:15:46.790250: step 30830, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.687 sec/batch; 63h:27m:57s remains)
INFO - root - 2017-12-08 00:16:02.991968: step 30840, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.600 sec/batch; 60h:09m:59s remains)
INFO - root - 2017-12-08 00:16:19.329407: step 30850, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.633 sec/batch; 61h:25m:38s remains)
INFO - root - 2017-12-08 00:16:35.193054: step 30860, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.531 sec/batch; 57h:33m:58s remains)
INFO - root - 2017-12-08 00:16:51.341653: step 30870, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.576 sec/batch; 59h:16m:48s remains)
INFO - root - 2017-12-08 00:17:07.756404: step 30880, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.671 sec/batch; 62h:50m:06s remains)
INFO - root - 2017-12-08 00:17:23.749123: step 30890, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 1.575 sec/batch; 59h:12m:55s remains)
INFO - root - 2017-12-08 00:17:39.922961: step 30900, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.614 sec/batch; 60h:40m:41s remains)
2017-12-08 00:17:41.165164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2615967 -4.2721748 -4.2878213 -4.2942381 -4.2819424 -4.2435451 -4.1976013 -4.1534739 -4.129539 -4.1490145 -4.1954379 -4.2389789 -4.2668595 -4.2810488 -4.2856646][-4.2586889 -4.26564 -4.2819872 -4.2899342 -4.275691 -4.2328739 -4.1776557 -4.1105843 -4.0559916 -4.0697055 -4.1290603 -4.18916 -4.2318697 -4.2577562 -4.2682762][-4.2579393 -4.2569704 -4.2671881 -4.2697144 -4.2545266 -4.2186017 -4.1721277 -4.1049738 -4.0344028 -4.0355682 -4.0926647 -4.1562567 -4.2097235 -4.2445431 -4.259922][-4.2563539 -4.2489905 -4.2469139 -4.23755 -4.2175984 -4.1941681 -4.1732988 -4.13368 -4.0846343 -4.0843382 -4.1238589 -4.1683655 -4.2119756 -4.2446985 -4.2640014][-4.2582293 -4.242537 -4.2224331 -4.1928821 -4.1593041 -4.1408896 -4.1497669 -4.1538296 -4.1483631 -4.1603985 -4.1834049 -4.2059422 -4.23184 -4.2565227 -4.2769856][-4.2577434 -4.2305512 -4.1883278 -4.1329389 -4.0767455 -4.0460434 -4.0697842 -4.1191316 -4.1647267 -4.198709 -4.2196827 -4.2305322 -4.2429423 -4.2610068 -4.2784986][-4.2564754 -4.22434 -4.1676574 -4.0846486 -3.9956846 -3.9230967 -3.92557 -4.0174842 -4.1174765 -4.1805124 -4.2136726 -4.2254276 -4.2304783 -4.2456431 -4.2621694][-4.2586994 -4.2275157 -4.1699972 -4.0806346 -3.9718447 -3.8513479 -3.8082924 -3.9216886 -4.0590949 -4.1444697 -4.1913676 -4.2149372 -4.2192225 -4.225307 -4.2366796][-4.2607512 -4.2429304 -4.2026768 -4.1339993 -4.0376692 -3.91843 -3.8561323 -3.9321964 -4.0471878 -4.1287508 -4.1818843 -4.2129622 -4.2192802 -4.2176757 -4.2208281][-4.2648339 -4.2614489 -4.2432661 -4.20517 -4.140295 -4.0561004 -3.9979513 -4.0125642 -4.0724859 -4.1326947 -4.1794281 -4.2121019 -4.2252889 -4.2256055 -4.225265][-4.2606826 -4.2653775 -4.2606406 -4.2440176 -4.2109261 -4.1640811 -4.1194148 -4.0976639 -4.1143646 -4.14916 -4.1828909 -4.2094646 -4.2261624 -4.233448 -4.2344384][-4.2464924 -4.2586393 -4.2567196 -4.2432718 -4.22687 -4.2109737 -4.1891131 -4.166357 -4.1643257 -4.1785741 -4.1937275 -4.2097754 -4.2244411 -4.2312622 -4.2311053][-4.2178173 -4.2335234 -4.2329731 -4.2199459 -4.2143631 -4.2188115 -4.2200036 -4.2121267 -4.2091351 -4.2107964 -4.2123017 -4.2177534 -4.2245255 -4.2227879 -4.2172723][-4.1721358 -4.1826143 -4.1866527 -4.1801085 -4.1817393 -4.1993027 -4.2173781 -4.231925 -4.24126 -4.242485 -4.2364902 -4.2277231 -4.2240205 -4.2156219 -4.2056561][-4.1266131 -4.1250777 -4.1337776 -4.1332436 -4.1347122 -4.1569319 -4.1859093 -4.2190847 -4.24597 -4.2557435 -4.2501011 -4.2308807 -4.2156281 -4.202199 -4.1923208]]...]
INFO - root - 2017-12-08 00:17:57.451264: step 30910, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.575 sec/batch; 59h:12m:03s remains)
INFO - root - 2017-12-08 00:18:13.731324: step 30920, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.668 sec/batch; 62h:42m:13s remains)
INFO - root - 2017-12-08 00:18:29.861626: step 30930, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.532 sec/batch; 57h:34m:44s remains)
INFO - root - 2017-12-08 00:18:46.043893: step 30940, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.683 sec/batch; 63h:14m:48s remains)
INFO - root - 2017-12-08 00:19:02.072140: step 30950, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.596 sec/batch; 59h:59m:23s remains)
INFO - root - 2017-12-08 00:19:18.153160: step 30960, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.679 sec/batch; 63h:06m:02s remains)
INFO - root - 2017-12-08 00:19:34.248111: step 30970, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.579 sec/batch; 59h:19m:31s remains)
INFO - root - 2017-12-08 00:19:50.523010: step 30980, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.721 sec/batch; 64h:40m:17s remains)
INFO - root - 2017-12-08 00:20:06.560302: step 30990, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 1.581 sec/batch; 59h:23m:16s remains)
INFO - root - 2017-12-08 00:20:22.678571: step 31000, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.679 sec/batch; 63h:05m:28s remains)
2017-12-08 00:20:24.072370: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3487196 -4.3436337 -4.3364067 -4.3333869 -4.33755 -4.3442326 -4.3481665 -4.3500643 -4.3509107 -4.3512764 -4.352129 -4.3536491 -4.3537936 -4.3533931 -4.35233][-4.34045 -4.3284936 -4.3137994 -4.3052969 -4.3071589 -4.3140535 -4.3188128 -4.3242888 -4.3297682 -4.33365 -4.3377733 -4.34409 -4.3487625 -4.3529434 -4.353682][-4.3189082 -4.3009324 -4.2761 -4.2561817 -4.2508059 -4.2541146 -4.2584987 -4.2665434 -4.2809024 -4.2936196 -4.3053246 -4.3193927 -4.3312192 -4.3425941 -4.3478117][-4.288105 -4.2630639 -4.2277331 -4.198452 -4.1887484 -4.1861186 -4.1806512 -4.1821151 -4.2022052 -4.2257657 -4.2468252 -4.2717028 -4.2959437 -4.3180518 -4.3305545][-4.2514319 -4.2163267 -4.1677589 -4.1294241 -4.1106558 -4.0919018 -4.062501 -4.0521779 -4.0867543 -4.1303115 -4.1651459 -4.2070513 -4.2487583 -4.2863331 -4.3075161][-4.2215228 -4.1720586 -4.1074066 -4.0567789 -4.0266442 -3.9806073 -3.9036405 -3.8619981 -3.9149606 -3.9935982 -4.0507627 -4.1100178 -4.1705494 -4.2288213 -4.2668867][-4.1996918 -4.1346903 -4.0530362 -3.9870255 -3.9400318 -3.85599 -3.7177773 -3.6294541 -3.7031293 -3.8299363 -3.9260492 -4.0116215 -4.09176 -4.1670051 -4.2167592][-4.2098932 -4.1413541 -4.058105 -3.9853942 -3.9311709 -3.8331852 -3.6730273 -3.5531623 -3.6168954 -3.755929 -3.8713355 -3.9688556 -4.0565739 -4.1353178 -4.1849403][-4.2491941 -4.1950564 -4.1292424 -4.0676546 -4.0209336 -3.940896 -3.8305852 -3.7469957 -3.7732368 -3.8565583 -3.9347644 -4.0099087 -4.0842228 -4.14898 -4.1879544][-4.2956095 -4.2628856 -4.2206068 -4.1789074 -4.147038 -4.0884295 -4.0233736 -3.9754729 -3.9785092 -4.01437 -4.0526495 -4.0980368 -4.1477962 -4.193625 -4.2187915][-4.3300371 -4.3145285 -4.2912388 -4.2661362 -4.2480092 -4.2097383 -4.1730871 -4.1461506 -4.1380105 -4.1502953 -4.168251 -4.1964121 -4.2283649 -4.2564855 -4.2691565][-4.3530951 -4.3470154 -4.3363051 -4.3238325 -4.3163123 -4.2967157 -4.2768064 -4.259933 -4.2487574 -4.2515721 -4.2603736 -4.2768464 -4.2949262 -4.3108115 -4.3155146][-4.3623452 -4.3626461 -4.3622408 -4.36012 -4.35761 -4.3479881 -4.3373408 -4.3293176 -4.321157 -4.3190155 -4.3194952 -4.3254867 -4.3338985 -4.3428459 -4.3451204][-4.359468 -4.3617887 -4.3646231 -4.3680477 -4.3702579 -4.3670168 -4.3615909 -4.3584023 -4.355938 -4.3530321 -4.3490076 -4.3474603 -4.3494906 -4.3544588 -4.3568106][-4.3562312 -4.3588529 -4.361249 -4.364603 -4.3674583 -4.3675523 -4.3658319 -4.36518 -4.36515 -4.3636842 -4.3600483 -4.3574367 -4.3573828 -4.359726 -4.3605204]]...]
INFO - root - 2017-12-08 00:20:40.295021: step 31010, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.687 sec/batch; 63h:22m:21s remains)
INFO - root - 2017-12-08 00:20:56.280987: step 31020, loss = 2.09, batch loss = 2.04 (10.1 examples/sec; 1.586 sec/batch; 59h:33m:31s remains)
INFO - root - 2017-12-08 00:21:12.516099: step 31030, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.657 sec/batch; 62h:13m:26s remains)
INFO - root - 2017-12-08 00:21:28.565492: step 31040, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.593 sec/batch; 59h:49m:38s remains)
INFO - root - 2017-12-08 00:21:44.918592: step 31050, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.599 sec/batch; 60h:02m:14s remains)
INFO - root - 2017-12-08 00:22:01.065018: step 31060, loss = 2.11, batch loss = 2.05 (9.7 examples/sec; 1.653 sec/batch; 62h:04m:17s remains)
INFO - root - 2017-12-08 00:22:17.158845: step 31070, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.526 sec/batch; 57h:18m:31s remains)
INFO - root - 2017-12-08 00:22:33.324435: step 31080, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.657 sec/batch; 62h:11m:55s remains)
INFO - root - 2017-12-08 00:22:49.474537: step 31090, loss = 2.07, batch loss = 2.02 (10.7 examples/sec; 1.499 sec/batch; 56h:16m:42s remains)
INFO - root - 2017-12-08 00:23:05.884060: step 31100, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.669 sec/batch; 62h:39m:37s remains)
2017-12-08 00:23:07.376987: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2702827 -4.2638373 -4.2525063 -4.2464414 -4.2435594 -4.2479362 -4.2491579 -4.239964 -4.2169867 -4.1938977 -4.1761103 -4.1647439 -4.1797833 -4.2017374 -4.2159529][-4.248003 -4.246604 -4.2390246 -4.2374454 -4.24086 -4.2463236 -4.2472672 -4.2378526 -4.2182441 -4.1915941 -4.1702852 -4.1568508 -4.1692543 -4.1919289 -4.2058349][-4.2416134 -4.2478809 -4.2437916 -4.242115 -4.2491908 -4.25476 -4.2501321 -4.2372446 -4.2207675 -4.1970358 -4.1769128 -4.1658287 -4.1747351 -4.1942406 -4.2050486][-4.242342 -4.2542248 -4.2532763 -4.2491632 -4.2519746 -4.2509017 -4.2394886 -4.2221746 -4.2069073 -4.1900434 -4.178472 -4.1767988 -4.1863031 -4.2028866 -4.2143369][-4.2359271 -4.2518783 -4.2510996 -4.2419968 -4.2365651 -4.2243114 -4.2028356 -4.1785331 -4.1627369 -4.1568403 -4.1622486 -4.1765656 -4.1953492 -4.2106614 -4.2212362][-4.2202549 -4.2348824 -4.2327204 -4.2213664 -4.20912 -4.1832376 -4.1472855 -4.1119833 -4.083662 -4.0895662 -4.1267481 -4.1690826 -4.2005887 -4.216826 -4.2239037][-4.2105112 -4.213706 -4.2031965 -4.1886125 -4.17051 -4.134573 -4.0838852 -4.0316133 -3.9774625 -3.9856026 -4.0598135 -4.1401415 -4.1953573 -4.2150555 -4.2114015][-4.2096596 -4.1973166 -4.1720872 -4.1469388 -4.12175 -4.0826182 -4.0288157 -3.9687226 -3.8854489 -3.8735871 -3.9733968 -4.0988545 -4.1855106 -4.2103658 -4.1986675][-4.2222672 -4.2013378 -4.165688 -4.1295009 -4.1026678 -4.0725613 -4.0320516 -3.9837761 -3.8982658 -3.86149 -3.9465199 -4.0835552 -4.1824136 -4.2029991 -4.1817846][-4.2338428 -4.2145147 -4.1782 -4.1401429 -4.1151028 -4.0982571 -4.0765791 -4.0495477 -3.9829307 -3.9330373 -3.9871609 -4.1052089 -4.19556 -4.2085724 -4.1837869][-4.2229934 -4.208446 -4.1849918 -4.1551456 -4.1321826 -4.1226144 -4.1130624 -4.0980268 -4.0519385 -4.0093727 -4.0407963 -4.1315584 -4.2026234 -4.2125592 -4.1904521][-4.2156339 -4.2074924 -4.1971521 -4.1787515 -4.1608014 -4.149775 -4.1406221 -4.1284008 -4.094687 -4.0598779 -4.0746579 -4.1387167 -4.1938286 -4.205337 -4.1923943][-4.2406635 -4.2391014 -4.238853 -4.231205 -4.2157927 -4.1978879 -4.1816154 -4.1643515 -4.1334057 -4.1027904 -4.1086488 -4.1538267 -4.193047 -4.2032304 -4.1957407][-4.2782412 -4.2800007 -4.2818494 -4.277607 -4.2635703 -4.242197 -4.2222128 -4.1988878 -4.1678634 -4.1420169 -4.1428485 -4.1710534 -4.1947675 -4.2014709 -4.1944852][-4.2919059 -4.2943211 -4.2966042 -4.2927918 -4.2789397 -4.2580862 -4.2388668 -4.215466 -4.1871905 -4.1689286 -4.1721144 -4.1908965 -4.2049551 -4.20567 -4.1989765]]...]
INFO - root - 2017-12-08 00:23:23.627778: step 31110, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.681 sec/batch; 63h:05m:42s remains)
INFO - root - 2017-12-08 00:23:39.639731: step 31120, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.571 sec/batch; 58h:58m:01s remains)
INFO - root - 2017-12-08 00:23:55.990826: step 31130, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.656 sec/batch; 62h:10m:05s remains)
INFO - root - 2017-12-08 00:24:11.861545: step 31140, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 1.419 sec/batch; 53h:16m:05s remains)
INFO - root - 2017-12-08 00:24:28.053365: step 31150, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.569 sec/batch; 58h:53m:09s remains)
INFO - root - 2017-12-08 00:24:44.354444: step 31160, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.667 sec/batch; 62h:32m:36s remains)
INFO - root - 2017-12-08 00:25:00.700120: step 31170, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.592 sec/batch; 59h:44m:16s remains)
INFO - root - 2017-12-08 00:25:16.856681: step 31180, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 1.675 sec/batch; 62h:50m:17s remains)
INFO - root - 2017-12-08 00:25:33.035854: step 31190, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.576 sec/batch; 59h:07m:39s remains)
INFO - root - 2017-12-08 00:25:49.359734: step 31200, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.577 sec/batch; 59h:10m:35s remains)
2017-12-08 00:25:50.761684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3399549 -4.3376966 -4.3366623 -4.3372316 -4.3377032 -4.33716 -4.335094 -4.3323479 -4.3313847 -4.3333745 -4.3377686 -4.3422928 -4.3455977 -4.345397 -4.341476][-4.3436027 -4.3426032 -4.341805 -4.3417959 -4.3409128 -4.3383994 -4.3335896 -4.3277688 -4.3243256 -4.3259296 -4.3324528 -4.3404026 -4.3458776 -4.3480449 -4.3460712][-4.3472567 -4.3469062 -4.3451314 -4.3425756 -4.3374076 -4.3291292 -4.3173747 -4.3049026 -4.2974558 -4.2998843 -4.3120494 -4.328135 -4.3415918 -4.3498106 -4.3511033][-4.3489966 -4.3484688 -4.3450074 -4.3379412 -4.3243179 -4.3044 -4.2783823 -4.252739 -4.2373939 -4.2419653 -4.2650652 -4.2971516 -4.3263226 -4.3450084 -4.3514571][-4.3477206 -4.3460803 -4.3394876 -4.323967 -4.2959652 -4.2560539 -4.2072506 -4.1621146 -4.1370349 -4.1474338 -4.1887789 -4.2441697 -4.2955055 -4.3294334 -4.3449][-4.3437867 -4.3397484 -4.3277583 -4.2990913 -4.24982 -4.182435 -4.1064138 -4.0415993 -4.0131526 -4.0361691 -4.1004696 -4.1818991 -4.2562914 -4.3067336 -4.33308][-4.3389759 -4.3308411 -4.310451 -4.2650924 -4.1923652 -4.0994258 -4.00261 -3.9269471 -3.9069281 -3.9504893 -4.0369453 -4.1382308 -4.2282362 -4.2894535 -4.3223562][-4.3345251 -4.3216419 -4.2919135 -4.2317986 -4.1425643 -4.0367932 -3.9356244 -3.866075 -3.8664284 -3.932266 -4.0325971 -4.139894 -4.2295275 -4.2875104 -4.3180785][-4.333015 -4.3168263 -4.2826166 -4.2197695 -4.1325669 -4.0380421 -3.9580016 -3.915997 -3.935678 -4.0034919 -4.0934544 -4.1856861 -4.2593679 -4.3023529 -4.322681][-4.3340878 -4.3192878 -4.2897038 -4.2391815 -4.1745262 -4.110589 -4.06359 -4.0464811 -4.0688591 -4.1185536 -4.1822467 -4.2469063 -4.2974143 -4.3228736 -4.3310246][-4.3367438 -4.327199 -4.3076468 -4.2767181 -4.2407331 -4.208272 -4.187789 -4.18441 -4.1999722 -4.22806 -4.2645812 -4.3019986 -4.3305368 -4.3411212 -4.3400836][-4.3411932 -4.3375077 -4.3290462 -4.3161197 -4.3023796 -4.2910995 -4.2849035 -4.2860107 -4.2934623 -4.3055954 -4.3233781 -4.341537 -4.3537054 -4.3539872 -4.3477][-4.3449726 -4.345418 -4.3442407 -4.3424573 -4.3407197 -4.3391647 -4.3385754 -4.3400011 -4.3427768 -4.3470154 -4.3535862 -4.3599091 -4.3622613 -4.3577814 -4.3507438][-4.3471751 -4.3492727 -4.351161 -4.3540206 -4.3568873 -4.3586712 -4.3591642 -4.3596916 -4.3601766 -4.360435 -4.361176 -4.3616 -4.359859 -4.3550773 -4.3503795][-4.3479023 -4.3496432 -4.3514071 -4.3539705 -4.3562379 -4.3568816 -4.3558083 -4.35473 -4.3545432 -4.354435 -4.3545942 -4.35448 -4.3528404 -4.3503013 -4.3481903]]...]
INFO - root - 2017-12-08 00:26:07.018219: step 31210, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.676 sec/batch; 62h:52m:40s remains)
INFO - root - 2017-12-08 00:26:23.096345: step 31220, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.528 sec/batch; 57h:17m:57s remains)
INFO - root - 2017-12-08 00:26:39.491820: step 31230, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.604 sec/batch; 60h:09m:52s remains)
INFO - root - 2017-12-08 00:26:55.806554: step 31240, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 1.716 sec/batch; 64h:21m:48s remains)
INFO - root - 2017-12-08 00:27:12.158630: step 31250, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 1.625 sec/batch; 60h:55m:10s remains)
INFO - root - 2017-12-08 00:27:28.402570: step 31260, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 1.726 sec/batch; 64h:42m:11s remains)
INFO - root - 2017-12-08 00:27:44.606591: step 31270, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 1.568 sec/batch; 58h:48m:35s remains)
INFO - root - 2017-12-08 00:28:00.866154: step 31280, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.692 sec/batch; 63h:25m:40s remains)
INFO - root - 2017-12-08 00:28:17.152633: step 31290, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.576 sec/batch; 59h:03m:56s remains)
INFO - root - 2017-12-08 00:28:33.291938: step 31300, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.668 sec/batch; 62h:32m:09s remains)
2017-12-08 00:28:34.666219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1652384 -4.1487403 -4.1511426 -4.1751986 -4.1856952 -4.1821532 -4.1895275 -4.2134113 -4.2282252 -4.2229276 -4.2178893 -4.21408 -4.2177019 -4.2273121 -4.2419162][-4.1983185 -4.1784143 -4.173192 -4.1799989 -4.1737781 -4.1604495 -4.17165 -4.2019367 -4.2196388 -4.2247467 -4.22573 -4.2223616 -4.222331 -4.226985 -4.2416544][-4.2404428 -4.2195897 -4.2061749 -4.1954007 -4.1706171 -4.1470284 -4.1566639 -4.1867781 -4.2108207 -4.2301388 -4.2395258 -4.2398491 -4.2346792 -4.2307816 -4.2415566][-4.2629185 -4.2384911 -4.2178211 -4.1968069 -4.1608167 -4.1347485 -4.1452851 -4.1775641 -4.2083397 -4.2403669 -4.257019 -4.2609987 -4.2545595 -4.2445216 -4.2487335][-4.2515531 -4.2236376 -4.2063804 -4.1896338 -4.1504259 -4.1164546 -4.1208472 -4.1608438 -4.204813 -4.2467494 -4.2706609 -4.2799654 -4.2735081 -4.2608461 -4.2571125][-4.2222242 -4.1972003 -4.1898751 -4.1825838 -4.1430488 -4.0963411 -4.0902433 -4.1325846 -4.1904745 -4.241612 -4.2726078 -4.2851453 -4.2770772 -4.2641864 -4.2557359][-4.2038231 -4.1836829 -4.1817746 -4.1787958 -4.1360369 -4.0810132 -4.0635757 -4.0976362 -4.1597176 -4.2122946 -4.2448311 -4.2553477 -4.2460508 -4.239687 -4.234611][-4.1847229 -4.1708875 -4.172924 -4.1744142 -4.1347933 -4.0872335 -4.0670342 -4.0883312 -4.136261 -4.1770382 -4.2046566 -4.20997 -4.2014222 -4.2044883 -4.2061267][-4.1523547 -4.1409755 -4.1542783 -4.167593 -4.1455665 -4.1197972 -4.1068053 -4.1180248 -4.14876 -4.1731749 -4.1842351 -4.1813631 -4.1771083 -4.1915264 -4.2030931][-4.1218376 -4.1077681 -4.1349621 -4.1583014 -4.1526093 -4.1473989 -4.145618 -4.1476641 -4.1610279 -4.1713424 -4.1736646 -4.1733332 -4.1822667 -4.2096314 -4.2281079][-4.115005 -4.0958219 -4.1298504 -4.1587195 -4.1576104 -4.1581917 -4.1582861 -4.1481524 -4.14015 -4.1405597 -4.1474686 -4.1584668 -4.1830492 -4.2198 -4.2412367][-4.1346722 -4.1085744 -4.1401038 -4.1672564 -4.1677074 -4.1700177 -4.16843 -4.1457605 -4.118742 -4.1053519 -4.11264 -4.1292939 -4.1599975 -4.2015095 -4.2297635][-4.1608534 -4.1284809 -4.1520495 -4.1691437 -4.165544 -4.1770754 -4.1922946 -4.1714277 -4.1329584 -4.1045213 -4.1000032 -4.1087613 -4.1343184 -4.1740403 -4.2121325][-4.1654148 -4.1290965 -4.1479616 -4.1611323 -4.1603479 -4.1817083 -4.2086821 -4.198885 -4.17048 -4.142909 -4.1227689 -4.118566 -4.1316657 -4.1651912 -4.2048416][-4.1610308 -4.1211443 -4.135901 -4.1505327 -4.1591167 -4.1877961 -4.2227139 -4.2259526 -4.2096887 -4.1854877 -4.1548014 -4.1403537 -4.1450214 -4.171771 -4.2073183]]...]
INFO - root - 2017-12-08 00:28:50.844069: step 31310, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 1.682 sec/batch; 63h:03m:51s remains)
INFO - root - 2017-12-08 00:29:06.924068: step 31320, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.633 sec/batch; 61h:12m:47s remains)
INFO - root - 2017-12-08 00:29:23.095762: step 31330, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.698 sec/batch; 63h:37m:25s remains)
INFO - root - 2017-12-08 00:29:39.356576: step 31340, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.586 sec/batch; 59h:24m:59s remains)
INFO - root - 2017-12-08 00:29:55.765534: step 31350, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.589 sec/batch; 59h:33m:34s remains)
INFO - root - 2017-12-08 00:30:12.040131: step 31360, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.661 sec/batch; 62h:13m:35s remains)
INFO - root - 2017-12-08 00:30:28.269834: step 31370, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.608 sec/batch; 60h:14m:30s remains)
INFO - root - 2017-12-08 00:30:44.510976: step 31380, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.630 sec/batch; 61h:04m:21s remains)
INFO - root - 2017-12-08 00:31:00.675702: step 31390, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.589 sec/batch; 59h:31m:57s remains)
INFO - root - 2017-12-08 00:31:17.107329: step 31400, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.747 sec/batch; 65h:26m:48s remains)
2017-12-08 00:31:18.511395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31266 -4.3196445 -4.3279805 -4.3332896 -4.3279042 -4.3070617 -4.2868881 -4.2828622 -4.2884307 -4.2964716 -4.3050437 -4.3040886 -4.2989593 -4.2878327 -4.2764163][-4.3081622 -4.318306 -4.3246341 -4.3207693 -4.3016996 -4.2648659 -4.2318 -4.2255077 -4.2422285 -4.2665782 -4.2906041 -4.3058825 -4.3103209 -4.3027864 -4.2911296][-4.3025274 -4.313405 -4.31353 -4.29709 -4.2623057 -4.2108192 -4.163518 -4.1498737 -4.1757736 -4.2179241 -4.2572136 -4.2791219 -4.2834244 -4.2767181 -4.2698245][-4.2977791 -4.3044815 -4.3009357 -4.2812243 -4.2385721 -4.1776652 -4.1170983 -4.0960908 -4.126657 -4.1832485 -4.2284908 -4.2481279 -4.248301 -4.2439666 -4.2420082][-4.2821097 -4.2855325 -4.2826819 -4.2620788 -4.2139554 -4.1364646 -4.0625029 -4.0416842 -4.0849819 -4.1564674 -4.2071347 -4.2276478 -4.2268271 -4.2192521 -4.2132249][-4.2485838 -4.2408 -4.2297459 -4.2055902 -4.1558867 -4.0762615 -4.0009651 -3.9913833 -4.0581946 -4.1453476 -4.1991534 -4.2178745 -4.2133522 -4.1994057 -4.1894226][-4.2026696 -4.1792445 -4.1534853 -4.126442 -4.0902395 -4.03417 -3.9783778 -3.9839704 -4.0617557 -4.1486716 -4.1975532 -4.2136469 -4.2104349 -4.1921639 -4.1794763][-4.1519361 -4.1259069 -4.1004725 -4.0832152 -4.0748463 -4.0500393 -4.0144143 -4.030241 -4.1016531 -4.17372 -4.2078142 -4.22073 -4.2168903 -4.1947842 -4.1732693][-4.1414762 -4.1240215 -4.104249 -4.0973134 -4.1097879 -4.1076279 -4.0899186 -4.1098475 -4.1680908 -4.22121 -4.2430658 -4.2459846 -4.2330527 -4.2038136 -4.1737542][-4.1669817 -4.1618576 -4.1478138 -4.1426973 -4.1635332 -4.1779766 -4.1781349 -4.1909466 -4.2305775 -4.2702012 -4.2858953 -4.2792726 -4.260056 -4.2325907 -4.2076178][-4.1954069 -4.1976261 -4.1923008 -4.1950994 -4.2193136 -4.2428141 -4.2519431 -4.2554231 -4.2755685 -4.3036704 -4.3186922 -4.3098283 -4.289485 -4.2676706 -4.2472262][-4.2362957 -4.2457523 -4.2488055 -4.2543731 -4.2735524 -4.2916012 -4.2960286 -4.2922654 -4.3012428 -4.323792 -4.3386369 -4.3319445 -4.313302 -4.2939625 -4.2693343][-4.2764363 -4.2883086 -4.295877 -4.2992592 -4.3099833 -4.3184686 -4.3187408 -4.3137126 -4.3192883 -4.3359165 -4.3461366 -4.338428 -4.3230667 -4.3083348 -4.2872252][-4.2980847 -4.3082733 -4.3156743 -4.3206964 -4.3276625 -4.3312616 -4.3271923 -4.3199549 -4.3239994 -4.3350673 -4.3416982 -4.3358541 -4.3264246 -4.3185825 -4.3039832][-4.3087211 -4.3114238 -4.3162441 -4.3223352 -4.3274369 -4.3281307 -4.3238077 -4.3190703 -4.3221827 -4.3288546 -4.3328757 -4.3278031 -4.3237834 -4.3213792 -4.3128753]]...]
INFO - root - 2017-12-08 00:31:34.965183: step 31410, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.663 sec/batch; 62h:16m:15s remains)
INFO - root - 2017-12-08 00:31:51.032745: step 31420, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.572 sec/batch; 58h:53m:39s remains)
INFO - root - 2017-12-08 00:32:07.306985: step 31430, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 59h:59m:13s remains)
INFO - root - 2017-12-08 00:32:23.461316: step 31440, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 1.710 sec/batch; 64h:01m:20s remains)
INFO - root - 2017-12-08 00:32:39.544330: step 31450, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.599 sec/batch; 59h:53m:24s remains)
INFO - root - 2017-12-08 00:32:55.880028: step 31460, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.621 sec/batch; 60h:41m:16s remains)
INFO - root - 2017-12-08 00:33:11.860786: step 31470, loss = 2.06, batch loss = 2.01 (10.2 examples/sec; 1.574 sec/batch; 58h:55m:25s remains)
INFO - root - 2017-12-08 00:33:28.162237: step 31480, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.535 sec/batch; 57h:27m:06s remains)
INFO - root - 2017-12-08 00:33:44.504992: step 31490, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.681 sec/batch; 62h:55m:50s remains)
INFO - root - 2017-12-08 00:34:00.738187: step 31500, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.596 sec/batch; 59h:44m:12s remains)
2017-12-08 00:34:02.084781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3054142 -4.3015418 -4.302145 -4.31007 -4.3210673 -4.3306479 -4.3372803 -4.3426671 -4.3510222 -4.36028 -4.3667755 -4.3666167 -4.36045 -4.3503942 -4.3397923][-4.2918944 -4.2921467 -4.2982106 -4.3088908 -4.3179131 -4.32443 -4.3309035 -4.3384423 -4.349041 -4.361948 -4.3720732 -4.3737235 -4.3680296 -4.3584633 -4.3491664][-4.26011 -4.2606478 -4.2686725 -4.2789021 -4.2839851 -4.2850642 -4.2869396 -4.293067 -4.3058648 -4.3240323 -4.3409915 -4.3499389 -4.35096 -4.3460259 -4.3401442][-4.2044959 -4.2041154 -4.214963 -4.2267141 -4.2310843 -4.2312732 -4.2323308 -4.2386446 -4.2543292 -4.2787657 -4.3033957 -4.3198848 -4.3267431 -4.3260565 -4.3232245][-4.123816 -4.1245661 -4.1405315 -4.156414 -4.1618919 -4.164094 -4.1691394 -4.1802034 -4.20279 -4.2354336 -4.2679925 -4.2916279 -4.3043261 -4.3075528 -4.3061576][-4.0384846 -4.0435138 -4.0689769 -4.096457 -4.1104813 -4.1166897 -4.1242352 -4.1383243 -4.16441 -4.1998334 -4.2359138 -4.2647681 -4.2823215 -4.2891746 -4.2873144][-3.9936533 -4.0022297 -4.0332856 -4.067482 -4.0866752 -4.0933452 -4.0979519 -4.1088514 -4.1320629 -4.1652389 -4.2008553 -4.2327061 -4.2555261 -4.2662048 -4.2651548][-4.0294633 -4.0386467 -4.0655584 -4.0940237 -4.1094642 -4.1127272 -4.1121125 -4.1145463 -4.1293516 -4.1548872 -4.1845741 -4.2143164 -4.2378774 -4.2512226 -4.2507977][-4.12158 -4.1271667 -4.1429586 -4.1588283 -4.1656575 -4.1642427 -4.16094 -4.1589508 -4.1663103 -4.1831045 -4.2059727 -4.2306519 -4.2498484 -4.2600894 -4.2577462][-4.2199464 -4.2219086 -4.2275457 -4.2310233 -4.2295504 -4.22459 -4.2207851 -4.2182522 -4.2221308 -4.2330308 -4.2503176 -4.2706738 -4.286406 -4.2927237 -4.2876978][-4.2897773 -4.2906561 -4.2911177 -4.2882223 -4.2840571 -4.280025 -4.2783566 -4.2772875 -4.2786622 -4.2841849 -4.2949653 -4.3089857 -4.3200312 -4.3238544 -4.3183861][-4.323843 -4.3251014 -4.323379 -4.3185091 -4.3150387 -4.3138804 -4.3149886 -4.3166642 -4.3186631 -4.3225565 -4.3296275 -4.3386817 -4.3453093 -4.3463249 -4.3408308][-4.3281674 -4.3278503 -4.3234787 -4.31695 -4.3127332 -4.3124723 -4.3160839 -4.3215003 -4.3279457 -4.3348722 -4.3423676 -4.3503289 -4.3558507 -4.356503 -4.3518867][-4.316803 -4.3141789 -4.3086653 -4.3023558 -4.2986674 -4.299499 -4.3039289 -4.3110652 -4.3199964 -4.3291326 -4.3366523 -4.342474 -4.3455696 -4.34506 -4.3417239][-4.3051538 -4.300344 -4.2949648 -4.2908421 -4.2892876 -4.291234 -4.295547 -4.3020282 -4.3114581 -4.3216276 -4.3286457 -4.331255 -4.3302813 -4.3270168 -4.3240089]]...]
INFO - root - 2017-12-08 00:34:18.417229: step 31510, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.570 sec/batch; 58h:45m:11s remains)
INFO - root - 2017-12-08 00:34:34.758573: step 31520, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.644 sec/batch; 61h:30m:53s remains)
INFO - root - 2017-12-08 00:34:50.821709: step 31530, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.596 sec/batch; 59h:44m:20s remains)
INFO - root - 2017-12-08 00:35:07.019494: step 31540, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.642 sec/batch; 61h:26m:58s remains)
INFO - root - 2017-12-08 00:35:23.438296: step 31550, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.615 sec/batch; 60h:25m:43s remains)
INFO - root - 2017-12-08 00:35:39.461267: step 31560, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.658 sec/batch; 62h:01m:51s remains)
INFO - root - 2017-12-08 00:35:55.695900: step 31570, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 59h:28m:14s remains)
INFO - root - 2017-12-08 00:36:12.019658: step 31580, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.675 sec/batch; 62h:40m:09s remains)
INFO - root - 2017-12-08 00:36:28.208563: step 31590, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.600 sec/batch; 59h:51m:17s remains)
INFO - root - 2017-12-08 00:36:44.497034: step 31600, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.555 sec/batch; 58h:10m:31s remains)
2017-12-08 00:36:45.851356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1872358 -4.2081828 -4.232511 -4.2537203 -4.2697325 -4.2701211 -4.2462845 -4.2132998 -4.1859593 -4.1729865 -4.1891994 -4.2181978 -4.234127 -4.226388 -4.1908379][-4.18398 -4.2030888 -4.2241726 -4.2413559 -4.2525835 -4.2520318 -4.2327247 -4.2113714 -4.1972771 -4.1904607 -4.1982527 -4.2144613 -4.2193785 -4.2031965 -4.1645474][-4.1820469 -4.1884341 -4.1975827 -4.2105126 -4.2270489 -4.2348537 -4.2292852 -4.2236924 -4.2215338 -4.2126713 -4.1979713 -4.1863728 -4.1766987 -4.1625872 -4.1362724][-4.1804743 -4.1745162 -4.1775913 -4.1916347 -4.2136369 -4.2328997 -4.2409759 -4.2426639 -4.2381463 -4.212688 -4.1843567 -4.167357 -4.160861 -4.1560736 -4.1470304][-4.1849418 -4.174685 -4.1716275 -4.1807389 -4.2050438 -4.2285676 -4.2380881 -4.2352538 -4.2208619 -4.1912789 -4.1757374 -4.181417 -4.1878161 -4.1880879 -4.1839094][-4.1870565 -4.1734958 -4.161757 -4.1670141 -4.1864042 -4.2041473 -4.210844 -4.2001758 -4.1794353 -4.1541047 -4.1455107 -4.1570678 -4.1696191 -4.1741796 -4.1756983][-4.1831064 -4.1674514 -4.148633 -4.1456933 -4.1549325 -4.1610289 -4.1578894 -4.1418381 -4.1205091 -4.1009789 -4.0958548 -4.1028214 -4.1199093 -4.1383858 -4.1589108][-4.1639223 -4.1481495 -4.1347151 -4.1388893 -4.1487837 -4.150599 -4.1409464 -4.1203752 -4.0904865 -4.0734415 -4.0735259 -4.0803165 -4.1030092 -4.1334767 -4.1639795][-4.1627045 -4.1430912 -4.1295037 -4.1322265 -4.1407166 -4.1430769 -4.1358981 -4.1196451 -4.0946293 -4.0853643 -4.0942965 -4.1095009 -4.1330791 -4.1594152 -4.1836448][-4.1739864 -4.1554689 -4.1391716 -4.1324706 -4.1332622 -4.1355352 -4.1333308 -4.1264434 -4.1185045 -4.1182451 -4.1296711 -4.147716 -4.1699038 -4.1863923 -4.1946774][-4.1997471 -4.1892953 -4.1760817 -4.1648784 -4.1594334 -4.1607161 -4.1649837 -4.1678581 -4.1676269 -4.1679993 -4.1772575 -4.1930895 -4.2071371 -4.2094336 -4.2032514][-4.2402034 -4.2348256 -4.2278218 -4.217062 -4.2081213 -4.2058439 -4.2105379 -4.2164369 -4.2197232 -4.218123 -4.2206726 -4.229104 -4.2346349 -4.2294612 -4.2173405][-4.284431 -4.2819476 -4.2793083 -4.2723165 -4.2638879 -4.2586613 -4.2584834 -4.2618132 -4.2634864 -4.2592125 -4.25369 -4.252738 -4.2503729 -4.2422218 -4.2301855][-4.3230987 -4.3215981 -4.3196197 -4.3154211 -4.30827 -4.3007393 -4.2956505 -4.295413 -4.2963538 -4.2929716 -4.28683 -4.2827721 -4.2776251 -4.26947 -4.2586193][-4.3543587 -4.3537297 -4.35334 -4.351994 -4.3487644 -4.3440528 -4.3400497 -4.3390989 -4.3391824 -4.3370118 -4.333025 -4.3292727 -4.3246832 -4.3183393 -4.3104224]]...]
INFO - root - 2017-12-08 00:37:02.287544: step 31610, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.641 sec/batch; 61h:21m:40s remains)
INFO - root - 2017-12-08 00:37:18.353784: step 31620, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.607 sec/batch; 60h:05m:47s remains)
INFO - root - 2017-12-08 00:37:34.704873: step 31630, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.567 sec/batch; 58h:36m:22s remains)
INFO - root - 2017-12-08 00:37:50.959628: step 31640, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.697 sec/batch; 63h:26m:32s remains)
INFO - root - 2017-12-08 00:38:07.047899: step 31650, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.580 sec/batch; 59h:05m:00s remains)
INFO - root - 2017-12-08 00:38:23.397261: step 31660, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.618 sec/batch; 60h:29m:20s remains)
INFO - root - 2017-12-08 00:38:39.588750: step 31670, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.599 sec/batch; 59h:45m:55s remains)
INFO - root - 2017-12-08 00:38:55.727748: step 31680, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.693 sec/batch; 63h:16m:18s remains)
INFO - root - 2017-12-08 00:39:12.025299: step 31690, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.623 sec/batch; 60h:39m:52s remains)
INFO - root - 2017-12-08 00:39:28.212015: step 31700, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 1.555 sec/batch; 58h:07m:50s remains)
2017-12-08 00:39:29.516392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2893467 -4.2922816 -4.2954197 -4.2983923 -4.3005686 -4.3035178 -4.3080158 -4.3117051 -4.3136048 -4.3137884 -4.312984 -4.3103175 -4.3056951 -4.3032432 -4.3019586][-4.2860479 -4.2885089 -4.2917457 -4.2936692 -4.2931919 -4.2935505 -4.29831 -4.3047771 -4.3105912 -4.3145189 -4.3164229 -4.3147144 -4.3089995 -4.3052044 -4.3030758][-4.2785759 -4.2789388 -4.27945 -4.2772427 -4.2708139 -4.2658606 -4.2693591 -4.277667 -4.2881985 -4.2984209 -4.3058343 -4.3088737 -4.3055487 -4.301755 -4.2991042][-4.2659631 -4.2613158 -4.2555513 -4.24644 -4.2336698 -4.2231684 -4.2232647 -4.2313871 -4.2450166 -4.2611089 -4.2761693 -4.2875686 -4.2916822 -4.2910223 -4.2895255][-4.2503481 -4.2380924 -4.22442 -4.207324 -4.1873078 -4.1695538 -4.1626105 -4.167222 -4.1823773 -4.2039366 -4.227562 -4.24984 -4.2655573 -4.2718029 -4.2740541][-4.233963 -4.21311 -4.1909819 -4.1636286 -4.13178 -4.1017146 -4.0826778 -4.0801172 -4.097218 -4.1283526 -4.16504 -4.2003851 -4.2294216 -4.2458105 -4.2545438][-4.2228975 -4.1962028 -4.1660867 -4.1267061 -4.0782709 -4.0306363 -3.9929566 -3.9749272 -3.9945686 -4.0431266 -4.0984249 -4.1477141 -4.1887579 -4.2165651 -4.2346292][-4.22023 -4.1930218 -4.1602836 -4.1147971 -4.05607 -3.9965343 -3.9417005 -3.9038749 -3.9220777 -3.9853573 -4.0537682 -4.1102753 -4.1575727 -4.1943779 -4.2206769][-4.2266521 -4.2041988 -4.1763234 -4.1351418 -4.0814414 -4.0274396 -3.9746866 -3.934942 -3.9474063 -4.0024753 -4.0603385 -4.1079783 -4.1511478 -4.1896996 -4.2195973][-4.2371826 -4.2227697 -4.2053556 -4.1775546 -4.1394038 -4.1018047 -4.0635891 -4.0316658 -4.0351548 -4.0689545 -4.10786 -4.1418886 -4.1752748 -4.2068558 -4.2317863][-4.2463408 -4.2392669 -4.2318764 -4.2173128 -4.1937752 -4.1674142 -4.1365967 -4.1076727 -4.1054716 -4.1288562 -4.1606078 -4.1884704 -4.2133136 -4.2344666 -4.2490563][-4.2521667 -4.2483211 -4.2460766 -4.24013 -4.2274303 -4.2092266 -4.1828718 -4.1565094 -4.1515722 -4.1703086 -4.1986217 -4.223207 -4.2423882 -4.2553749 -4.2617936][-4.2566943 -4.2516856 -4.2491827 -4.2466936 -4.2413678 -4.2313609 -4.2132421 -4.1932559 -4.18703 -4.1991735 -4.2199917 -4.2390995 -4.2538118 -4.2628856 -4.2665157][-4.26394 -4.2573805 -4.2534275 -4.2524309 -4.2519426 -4.2488861 -4.2399383 -4.2279029 -4.2221541 -4.2270913 -4.2388716 -4.2514453 -4.2617879 -4.2683206 -4.2711945][-4.2756786 -4.2701769 -4.2663193 -4.2657552 -4.26712 -4.267571 -4.2649512 -4.2599053 -4.2564197 -4.2572713 -4.2622781 -4.2685461 -4.2739329 -4.2777576 -4.2797871]]...]
INFO - root - 2017-12-08 00:39:45.740461: step 31710, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.580 sec/batch; 59h:03m:49s remains)
INFO - root - 2017-12-08 00:40:02.059349: step 31720, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.696 sec/batch; 63h:22m:12s remains)
INFO - root - 2017-12-08 00:40:18.208086: step 31730, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.538 sec/batch; 57h:29m:05s remains)
INFO - root - 2017-12-08 00:40:34.445784: step 31740, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.616 sec/batch; 60h:22m:59s remains)
INFO - root - 2017-12-08 00:40:50.643746: step 31750, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 59h:47m:39s remains)
INFO - root - 2017-12-08 00:41:06.918358: step 31760, loss = 2.07, batch loss = 2.02 (10.3 examples/sec; 1.553 sec/batch; 58h:00m:01s remains)
INFO - root - 2017-12-08 00:41:23.287649: step 31770, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.693 sec/batch; 63h:14m:57s remains)
INFO - root - 2017-12-08 00:41:39.421310: step 31780, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 1.557 sec/batch; 58h:09m:09s remains)
INFO - root - 2017-12-08 00:41:55.738102: step 31790, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.668 sec/batch; 62h:18m:40s remains)
INFO - root - 2017-12-08 00:42:11.912063: step 31800, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.583 sec/batch; 59h:08m:07s remains)
2017-12-08 00:42:13.319894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.302567 -4.2952719 -4.2890158 -4.281743 -4.2714353 -4.26017 -4.2502742 -4.2414885 -4.2388916 -4.231524 -4.2162833 -4.1996403 -4.1910167 -4.1797442 -4.1682][-4.2780313 -4.267139 -4.2583818 -4.2469239 -4.2315731 -4.2158327 -4.200511 -4.1897416 -4.19252 -4.1884308 -4.1728144 -4.1558046 -4.1485348 -4.1384387 -4.1278634][-4.2360215 -4.224802 -4.2157049 -4.20095 -4.1822362 -4.1627011 -4.145473 -4.1354141 -4.142447 -4.14221 -4.1306453 -4.121727 -4.1218567 -4.1155586 -4.1072125][-4.1992311 -4.1893706 -4.1821485 -4.166924 -4.1466012 -4.1252661 -4.1064787 -4.0955892 -4.1002488 -4.1002593 -4.095871 -4.103334 -4.1171374 -4.1246247 -4.1206212][-4.1800861 -4.1738405 -4.1706409 -4.1571274 -4.1358891 -4.1136708 -4.0933027 -4.0794344 -4.07668 -4.0745964 -4.0800333 -4.1029639 -4.1314311 -4.153038 -4.1542473][-4.1706343 -4.1658888 -4.1649981 -4.1520786 -4.1320915 -4.1105556 -4.0902543 -4.0760479 -4.071085 -4.0706277 -4.0829077 -4.11589 -4.1555543 -4.1862173 -4.1897][-4.1653104 -4.1614804 -4.1606188 -4.14856 -4.1314731 -4.1121044 -4.0922656 -4.0811071 -4.0810056 -4.0850291 -4.0980387 -4.1306009 -4.1750865 -4.20881 -4.2129679][-4.15487 -4.1545262 -4.1593103 -4.1547527 -4.1423717 -4.1220794 -4.0999589 -4.0879292 -4.0910516 -4.0984898 -4.1108742 -4.1390123 -4.1813574 -4.213336 -4.2197881][-4.1506529 -4.1477265 -4.1547465 -4.1578217 -4.1552496 -4.1395288 -4.1155519 -4.099751 -4.1028666 -4.1104655 -4.1209531 -4.1422973 -4.1753168 -4.2024989 -4.2120848][-4.147737 -4.1396537 -4.1462297 -4.1570053 -4.1660337 -4.1590567 -4.1378522 -4.1223774 -4.1251554 -4.1318355 -4.1397381 -4.1516132 -4.1717515 -4.190639 -4.198781][-4.1388025 -4.1273589 -4.1325588 -4.1506085 -4.1699381 -4.1734729 -4.1606512 -4.1500835 -4.1534982 -4.158936 -4.1616931 -4.1629109 -4.1713328 -4.18117 -4.1821942][-4.1227074 -4.1123424 -4.1184278 -4.1413913 -4.1698885 -4.1834903 -4.1784725 -4.1718473 -4.1757069 -4.1815095 -4.17905 -4.1723981 -4.1721363 -4.1733632 -4.1671472][-4.1153207 -4.1120844 -4.1204553 -4.1413503 -4.16854 -4.1846929 -4.1816878 -4.1776414 -4.1830363 -4.1884766 -4.1866474 -4.1805511 -4.1782236 -4.1765332 -4.1679974][-4.1407185 -4.1400981 -4.1457481 -4.1575141 -4.1740203 -4.1840463 -4.1795177 -4.1772618 -4.1825285 -4.1879492 -4.1921258 -4.1938653 -4.194521 -4.1956697 -4.19095][-4.1870222 -4.1835275 -4.1828079 -4.1843896 -4.1931891 -4.200315 -4.1963177 -4.1951962 -4.199616 -4.2047877 -4.2121873 -4.2179742 -4.2221613 -4.2303534 -4.2330284]]...]
INFO - root - 2017-12-08 00:42:29.536351: step 31810, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.602 sec/batch; 59h:48m:42s remains)
INFO - root - 2017-12-08 00:42:45.892182: step 31820, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.708 sec/batch; 63h:46m:44s remains)
INFO - root - 2017-12-08 00:43:02.137528: step 31830, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 1.613 sec/batch; 60h:13m:10s remains)
INFO - root - 2017-12-08 00:43:18.348474: step 31840, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.599 sec/batch; 59h:42m:19s remains)
INFO - root - 2017-12-08 00:43:34.369470: step 31850, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 1.449 sec/batch; 54h:05m:02s remains)
INFO - root - 2017-12-08 00:43:50.573531: step 31860, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.661 sec/batch; 62h:01m:02s remains)
INFO - root - 2017-12-08 00:44:06.720702: step 31870, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.557 sec/batch; 58h:07m:52s remains)
INFO - root - 2017-12-08 00:44:22.824919: step 31880, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.604 sec/batch; 59h:51m:20s remains)
INFO - root - 2017-12-08 00:44:39.247779: step 31890, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.678 sec/batch; 62h:38m:36s remains)
INFO - root - 2017-12-08 00:44:55.398454: step 31900, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.619 sec/batch; 60h:25m:43s remains)
2017-12-08 00:44:56.802822: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3185139 -4.3070064 -4.2959247 -4.2851877 -4.2718134 -4.24995 -4.228972 -4.2276106 -4.2474079 -4.2703681 -4.2854781 -4.2934256 -4.2875466 -4.2714934 -4.2474594][-4.311327 -4.2984562 -4.2873807 -4.2765183 -4.2572122 -4.2217989 -4.1887765 -4.1878147 -4.2196655 -4.2555294 -4.27618 -4.2856503 -4.2800965 -4.2620955 -4.2323775][-4.3110938 -4.3002744 -4.2881474 -4.2718949 -4.2419705 -4.1912346 -4.1448269 -4.1425791 -4.1896009 -4.2396369 -4.2671165 -4.2791605 -4.2756605 -4.2612166 -4.2246785][-4.316473 -4.3075156 -4.2924061 -4.2686429 -4.2319031 -4.1748223 -4.1170878 -4.1086106 -4.1670527 -4.2277112 -4.2602143 -4.2737541 -4.2717705 -4.2626724 -4.2257419][-4.3199635 -4.3126945 -4.2919221 -4.2591467 -4.2179322 -4.1595592 -4.0833788 -4.0534739 -4.1174664 -4.1940637 -4.2357888 -4.2540245 -4.2599058 -4.2587066 -4.2266269][-4.3202987 -4.3123083 -4.2875972 -4.2453508 -4.1942964 -4.1210604 -4.0067081 -3.9356461 -4.0111685 -4.1306586 -4.2040653 -4.2372875 -4.2559967 -4.2653751 -4.2403836][-4.3193493 -4.311913 -4.2862468 -4.23548 -4.1700172 -4.0651383 -3.890372 -3.7596436 -3.8663092 -4.054 -4.1721053 -4.2300162 -4.2648907 -4.2851338 -4.2703876][-4.3162403 -4.3123631 -4.2914052 -4.2380528 -4.1670809 -4.0502439 -3.8535695 -3.7033584 -3.8323333 -4.0359535 -4.1645188 -4.2291822 -4.2704778 -4.2953839 -4.2864289][-4.3142009 -4.3110437 -4.2951612 -4.2504196 -4.1959429 -4.11323 -3.9819889 -3.8911753 -3.974381 -4.108882 -4.1998177 -4.2442975 -4.2720704 -4.2879519 -4.279356][-4.3177552 -4.3124275 -4.2954521 -4.2619891 -4.2218032 -4.1678548 -4.0921459 -4.0374632 -4.0777588 -4.1641746 -4.2256179 -4.2548766 -4.2637091 -4.2605205 -4.2493682][-4.326335 -4.321857 -4.307497 -4.2824335 -4.2502065 -4.2123327 -4.1597939 -4.1107383 -4.1270437 -4.1851544 -4.2299895 -4.2534819 -4.254611 -4.2416253 -4.2282796][-4.3323565 -4.3313975 -4.3239255 -4.3067217 -4.2862296 -4.2646785 -4.2249045 -4.1809993 -4.1827497 -4.2188959 -4.2482839 -4.2631874 -4.2589779 -4.2413378 -4.2269568][-4.3345971 -4.3367991 -4.333035 -4.321733 -4.3132753 -4.3021035 -4.2764182 -4.2486649 -4.254251 -4.2788992 -4.2911849 -4.29402 -4.2830162 -4.2611 -4.2441568][-4.3347912 -4.33774 -4.3370266 -4.3316069 -4.3289042 -4.3233409 -4.3096557 -4.299336 -4.3113995 -4.3305955 -4.3348246 -4.3327122 -4.3196974 -4.2989993 -4.28178][-4.3367014 -4.3400555 -4.3448172 -4.3454375 -4.3443308 -4.3412838 -4.3342819 -4.3325009 -4.3416162 -4.3529072 -4.3536992 -4.3549366 -4.3489351 -4.3377457 -4.3265305]]...]
INFO - root - 2017-12-08 00:45:13.097015: step 31910, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.576 sec/batch; 58h:48m:35s remains)
INFO - root - 2017-12-08 00:45:29.487490: step 31920, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 1.683 sec/batch; 62h:48m:29s remains)
INFO - root - 2017-12-08 00:45:45.782010: step 31930, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.620 sec/batch; 60h:26m:43s remains)
INFO - root - 2017-12-08 00:46:02.264617: step 31940, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.729 sec/batch; 64h:31m:12s remains)
INFO - root - 2017-12-08 00:46:18.613440: step 31950, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.604 sec/batch; 59h:50m:05s remains)
INFO - root - 2017-12-08 00:46:34.976128: step 31960, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.690 sec/batch; 63h:02m:31s remains)
INFO - root - 2017-12-08 00:46:50.894532: step 31970, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.548 sec/batch; 57h:43m:53s remains)
INFO - root - 2017-12-08 00:47:07.195483: step 31980, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 1.494 sec/batch; 55h:44m:12s remains)
INFO - root - 2017-12-08 00:47:23.373186: step 31990, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.578 sec/batch; 58h:50m:17s remains)
INFO - root - 2017-12-08 00:47:39.569394: step 32000, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.654 sec/batch; 61h:40m:31s remains)
2017-12-08 00:47:41.067705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2295246 -4.2318859 -4.2318177 -4.2298231 -4.2307348 -4.2272038 -4.2235188 -4.2180243 -4.2146487 -4.2211981 -4.2259331 -4.2168784 -4.2003508 -4.1895761 -4.1893582][-4.21334 -4.2164674 -4.2180505 -4.2217703 -4.2314935 -4.2364864 -4.2402511 -4.242197 -4.2426338 -4.2470832 -4.2498021 -4.2399335 -4.2199883 -4.2042012 -4.1993775][-4.1939745 -4.1968679 -4.1976538 -4.2033763 -4.2154708 -4.2237244 -4.2307868 -4.2358551 -4.2402411 -4.2487144 -4.2564406 -4.251699 -4.2365274 -4.223125 -4.2177892][-4.190742 -4.1957383 -4.1965718 -4.1980743 -4.2027578 -4.2043066 -4.2058988 -4.2077432 -4.2143054 -4.2287216 -4.2427821 -4.2448273 -4.2383881 -4.2324824 -4.2280335][-4.2049785 -4.2077618 -4.2064481 -4.2031283 -4.1988187 -4.1887321 -4.1796317 -4.173676 -4.17865 -4.195076 -4.2098756 -4.2163997 -4.2187176 -4.2205396 -4.219501][-4.2335167 -4.2276497 -4.2197671 -4.2100492 -4.1969905 -4.1762362 -4.1565213 -4.1450233 -4.1458311 -4.1583261 -4.16887 -4.1766067 -4.1868749 -4.1945562 -4.1970811][-4.2556534 -4.249136 -4.2380805 -4.2254629 -4.2066545 -4.179749 -4.1500559 -4.1278191 -4.1177158 -4.1217818 -4.1329932 -4.1429152 -4.1577711 -4.1682816 -4.1740994][-4.2644958 -4.2649689 -4.2588043 -4.2486577 -4.2328711 -4.2059622 -4.1730227 -4.1435428 -4.11929 -4.1080265 -4.1141405 -4.1242638 -4.1410704 -4.1576562 -4.1689677][-4.2601161 -4.26654 -4.2680225 -4.2642975 -4.254982 -4.2350659 -4.208621 -4.18309 -4.1538229 -4.1289158 -4.126503 -4.1351743 -4.1521378 -4.1736646 -4.1870632][-4.2597117 -4.2652578 -4.2687588 -4.2696557 -4.2661228 -4.254972 -4.2386837 -4.2205334 -4.1957421 -4.17025 -4.1634011 -4.1692491 -4.1824446 -4.2025881 -4.215878][-4.2689819 -4.2702794 -4.2707491 -4.2700653 -4.2681751 -4.2643447 -4.2592278 -4.2518349 -4.2372627 -4.2176046 -4.2094183 -4.2111988 -4.2179608 -4.2306495 -4.2431293][-4.2770238 -4.2744856 -4.2710748 -4.2679853 -4.2665386 -4.26783 -4.2702031 -4.2714038 -4.26647 -4.2544589 -4.2473407 -4.2456675 -4.2456608 -4.2504311 -4.2589335][-4.28586 -4.2796335 -4.2720695 -4.2669935 -4.2658434 -4.2687 -4.27425 -4.2782393 -4.2793117 -4.2756052 -4.2703338 -4.2643137 -4.2571611 -4.25436 -4.2581997][-4.2967949 -4.2905807 -4.2815065 -4.2739825 -4.2696238 -4.2692552 -4.2731214 -4.277534 -4.2811246 -4.2825346 -4.2807112 -4.2744265 -4.265008 -4.2570868 -4.2540989][-4.3047223 -4.3023977 -4.2966714 -4.2902679 -4.2837291 -4.2784028 -4.2765074 -4.2773557 -4.2797766 -4.2832079 -4.2847748 -4.2816048 -4.2750883 -4.2677603 -4.2618546]]...]
INFO - root - 2017-12-08 00:47:57.261862: step 32010, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.606 sec/batch; 59h:52m:45s remains)
INFO - root - 2017-12-08 00:48:13.624992: step 32020, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.544 sec/batch; 57h:33m:09s remains)
INFO - root - 2017-12-08 00:48:29.689066: step 32030, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.667 sec/batch; 62h:10m:04s remains)
INFO - root - 2017-12-08 00:48:45.915635: step 32040, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.533 sec/batch; 57h:08m:12s remains)
INFO - root - 2017-12-08 00:49:02.060998: step 32050, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.681 sec/batch; 62h:40m:09s remains)
INFO - root - 2017-12-08 00:49:18.213678: step 32060, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.610 sec/batch; 60h:00m:35s remains)
INFO - root - 2017-12-08 00:49:34.736936: step 32070, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.713 sec/batch; 63h:50m:40s remains)
INFO - root - 2017-12-08 00:49:51.060218: step 32080, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.576 sec/batch; 58h:44m:23s remains)
INFO - root - 2017-12-08 00:50:07.291828: step 32090, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.613 sec/batch; 60h:06m:12s remains)
INFO - root - 2017-12-08 00:50:23.399191: step 32100, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.677 sec/batch; 62h:28m:52s remains)
2017-12-08 00:50:24.843379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.281754 -4.2838449 -4.2822423 -4.2805 -4.2779965 -4.2723236 -4.2727418 -4.2777023 -4.2801127 -4.2771206 -4.2688608 -4.2626071 -4.2610788 -4.2624736 -4.268364][-4.2637916 -4.2689428 -4.2654033 -4.2597589 -4.2557993 -4.24694 -4.2440529 -4.2487488 -4.2529426 -4.2518516 -4.244565 -4.2397103 -4.23732 -4.2396 -4.2477784][-4.2300997 -4.2394075 -4.2378087 -4.2291451 -4.2200065 -4.2032661 -4.1929908 -4.1917429 -4.19556 -4.200726 -4.2019024 -4.2037892 -4.2039261 -4.2091718 -4.2218962][-4.1939373 -4.2056222 -4.2098055 -4.1981173 -4.1810603 -4.1475134 -4.1219792 -4.1112318 -4.1178613 -4.1386428 -4.1565843 -4.1673322 -4.1696515 -4.1780376 -4.1972919][-4.1785913 -4.1903253 -4.1957941 -4.1798258 -4.148972 -4.0880942 -4.0311294 -3.9917369 -3.9948688 -4.0460777 -4.1005545 -4.1329093 -4.1455197 -4.158514 -4.1836591][-4.1676245 -4.1793962 -4.1801496 -4.1553936 -4.1082635 -4.0250812 -3.9268739 -3.8370073 -3.8220627 -3.9151955 -4.0241947 -4.0938292 -4.1279368 -4.1528482 -4.1832261][-4.1675816 -4.1825795 -4.1778913 -4.1441846 -4.0837874 -3.9795437 -3.8363314 -3.6849272 -3.6392934 -3.7711563 -3.9378867 -4.0445285 -4.1065354 -4.1527591 -4.1951237][-4.1935992 -4.2136736 -4.2133508 -4.1842685 -4.1269441 -4.0250463 -3.8759961 -3.7142115 -3.6599963 -3.7821696 -3.9463532 -4.05633 -4.1283345 -4.1815071 -4.2256589][-4.2081728 -4.2303691 -4.2385554 -4.2222528 -4.1825681 -4.1044559 -3.9858768 -3.8610957 -3.8203449 -3.9013181 -4.015872 -4.1028643 -4.1676083 -4.216063 -4.2549825][-4.2061124 -4.2222004 -4.232759 -4.2324438 -4.2194347 -4.177989 -4.1081953 -4.0328989 -4.0094705 -4.0468769 -4.1037197 -4.1556706 -4.200788 -4.2371764 -4.2695069][-4.2117667 -4.2182713 -4.2232547 -4.2306428 -4.2361183 -4.2223668 -4.1888933 -4.1534271 -4.1425366 -4.156671 -4.1769371 -4.2005982 -4.2260308 -4.2546139 -4.2845845][-4.2428994 -4.2396321 -4.23674 -4.2433133 -4.2553792 -4.25669 -4.245532 -4.2321334 -4.2299547 -4.2355418 -4.2411475 -4.249794 -4.2640419 -4.2862692 -4.3110352][-4.2858772 -4.2785783 -4.2692547 -4.2708459 -4.2802835 -4.284482 -4.2837296 -4.282104 -4.2833414 -4.2852659 -4.2847247 -4.2884851 -4.2981329 -4.3149123 -4.3321962][-4.3162313 -4.3098092 -4.3011794 -4.3017459 -4.3080335 -4.3109031 -4.3110127 -4.3111162 -4.31133 -4.3123331 -4.3106117 -4.3125443 -4.3202362 -4.3322859 -4.3429012][-4.3362813 -4.3351903 -4.3317828 -4.3332124 -4.3371067 -4.3381767 -4.3383403 -4.338222 -4.3379602 -4.3379393 -4.3353572 -4.3347025 -4.3384719 -4.3450952 -4.3501153]]...]
INFO - root - 2017-12-08 00:50:40.936986: step 32110, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.599 sec/batch; 59h:35m:31s remains)
INFO - root - 2017-12-08 00:50:57.311459: step 32120, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.691 sec/batch; 62h:59m:31s remains)
INFO - root - 2017-12-08 00:51:13.472517: step 32130, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.576 sec/batch; 58h:42m:42s remains)
INFO - root - 2017-12-08 00:51:29.754078: step 32140, loss = 2.07, batch loss = 2.02 (10.2 examples/sec; 1.568 sec/batch; 58h:23m:52s remains)
INFO - root - 2017-12-08 00:51:46.130575: step 32150, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 1.777 sec/batch; 66h:11m:52s remains)
INFO - root - 2017-12-08 00:52:02.155473: step 32160, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 1.480 sec/batch; 55h:07m:13s remains)
INFO - root - 2017-12-08 00:52:18.425820: step 32170, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.648 sec/batch; 61h:23m:14s remains)
INFO - root - 2017-12-08 00:52:34.584335: step 32180, loss = 2.08, batch loss = 2.03 (10.2 examples/sec; 1.570 sec/batch; 58h:27m:16s remains)
INFO - root - 2017-12-08 00:52:50.885225: step 32190, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.692 sec/batch; 63h:00m:04s remains)
INFO - root - 2017-12-08 00:53:07.304915: step 32200, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.627 sec/batch; 60h:35m:11s remains)
2017-12-08 00:53:08.729821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3276758 -4.3269849 -4.3233466 -4.3180456 -4.3111963 -4.3034458 -4.2971482 -4.2941766 -4.2939949 -4.2987614 -4.3067956 -4.3125911 -4.3169341 -4.3247805 -4.3319683][-4.33424 -4.3329964 -4.3282247 -4.3209186 -4.3118205 -4.3009729 -4.2897568 -4.2801642 -4.2742796 -4.2757716 -4.2847443 -4.2942276 -4.3032351 -4.3162589 -4.3281393][-4.3378735 -4.3363814 -4.3304496 -4.3211527 -4.3078337 -4.2884521 -4.2665353 -4.2468696 -4.2321305 -4.22789 -4.2409992 -4.2580867 -4.274044 -4.2943015 -4.3122506][-4.3414259 -4.3373117 -4.3272381 -4.3121276 -4.2900143 -4.2586536 -4.2233305 -4.1939263 -4.1718926 -4.1676207 -4.1908517 -4.2177715 -4.239285 -4.2665219 -4.2897491][-4.3451138 -4.3344908 -4.3154893 -4.2886329 -4.2526364 -4.2102051 -4.1666451 -4.1275148 -4.0995703 -4.1017394 -4.1397829 -4.1766958 -4.2054634 -4.2386818 -4.2690606][-4.3468928 -4.3273449 -4.2983065 -4.2579403 -4.2063279 -4.1534076 -4.0980949 -4.0354972 -3.9937696 -4.0120597 -4.0724788 -4.1309562 -4.1738305 -4.2167053 -4.2569642][-4.3418341 -4.318696 -4.2805171 -4.2231665 -4.1541648 -4.0820127 -3.9933128 -3.8837028 -3.8337405 -3.8955269 -4.0059576 -4.1016846 -4.1649237 -4.2167873 -4.2640066][-4.3294911 -4.3038664 -4.25523 -4.1837335 -4.1012673 -4.0059919 -3.8723485 -3.7130578 -3.6874919 -3.8274093 -3.9906051 -4.1120358 -4.18883 -4.24559 -4.2912078][-4.3118091 -4.2900438 -4.2375531 -4.1628881 -4.0783205 -3.9791877 -3.8353155 -3.6866813 -3.7184372 -3.8900251 -4.0465212 -4.1583591 -4.2303629 -4.2817841 -4.3170624][-4.293376 -4.2813559 -4.2391253 -4.1752119 -4.1084061 -4.0421019 -3.9544163 -3.8769391 -3.9108944 -4.019403 -4.1120839 -4.1904612 -4.25334 -4.2976694 -4.3232255][-4.2722425 -4.2696314 -4.2454309 -4.2046466 -4.1651669 -4.137424 -4.0999374 -4.0636458 -4.0711956 -4.1073871 -4.1422009 -4.1899276 -4.2418857 -4.2837038 -4.3082905][-4.2560425 -4.2598286 -4.2526183 -4.2368727 -4.2177725 -4.2052064 -4.1805878 -4.1490431 -4.13705 -4.1423631 -4.1529446 -4.1741123 -4.208941 -4.2490835 -4.2755666][-4.2586126 -4.2641225 -4.2653003 -4.2643929 -4.2574816 -4.2452769 -4.2180982 -4.1828294 -4.1579909 -4.1486859 -4.1474319 -4.149735 -4.1671453 -4.1998935 -4.2290568][-4.2797065 -4.2866611 -4.28784 -4.2897992 -4.2868824 -4.2750573 -4.2512255 -4.2142487 -4.1782336 -4.1539154 -4.13525 -4.1188884 -4.1232772 -4.150979 -4.1818452][-4.3091426 -4.3143816 -4.3114395 -4.3094974 -4.3063474 -4.2983222 -4.2843475 -4.2591558 -4.2213616 -4.180778 -4.1356206 -4.0923395 -4.0844069 -4.1140976 -4.1528673]]...]
INFO - root - 2017-12-08 00:53:24.617021: step 32210, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.629 sec/batch; 60h:39m:18s remains)
INFO - root - 2017-12-08 00:53:40.811280: step 32220, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 1.721 sec/batch; 64h:03m:19s remains)
INFO - root - 2017-12-08 00:53:57.109879: step 32230, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.609 sec/batch; 59h:54m:09s remains)
INFO - root - 2017-12-08 00:54:13.359277: step 32240, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.676 sec/batch; 62h:23m:01s remains)
INFO - root - 2017-12-08 00:54:29.415858: step 32250, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.555 sec/batch; 57h:51m:43s remains)
INFO - root - 2017-12-08 00:54:45.546679: step 32260, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.526 sec/batch; 56h:48m:51s remains)
INFO - root - 2017-12-08 00:55:01.547181: step 32270, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.592 sec/batch; 59h:14m:55s remains)
INFO - root - 2017-12-08 00:55:17.731981: step 32280, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 1.608 sec/batch; 59h:49m:45s remains)
INFO - root - 2017-12-08 00:55:34.097245: step 32290, loss = 2.09, batch loss = 2.03 (10.6 examples/sec; 1.512 sec/batch; 56h:14m:41s remains)
INFO - root - 2017-12-08 00:55:50.220619: step 32300, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.667 sec/batch; 62h:01m:39s remains)
2017-12-08 00:55:51.648737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.14355 -4.0976954 -4.0866389 -4.1069946 -4.1289897 -4.1323352 -4.1367221 -4.1642003 -4.1943779 -4.2077341 -4.2297316 -4.2540154 -4.259243 -4.2536068 -4.2377272][-4.1776 -4.1438608 -4.1345482 -4.1520329 -4.1713128 -4.1687541 -4.168961 -4.1892147 -4.2097187 -4.2217174 -4.2428751 -4.25851 -4.2524729 -4.2413368 -4.2286077][-4.2184696 -4.1998849 -4.1940084 -4.2038312 -4.2114592 -4.1956334 -4.1843858 -4.198566 -4.2138267 -4.220973 -4.236752 -4.2416515 -4.2233772 -4.2061934 -4.19938][-4.2347426 -4.2372112 -4.2346268 -4.231813 -4.2214293 -4.1893144 -4.1567249 -4.1623359 -4.1760397 -4.1818004 -4.1963081 -4.1963778 -4.1685877 -4.14452 -4.1453543][-4.2260342 -4.24344 -4.2392278 -4.2187142 -4.1855693 -4.1318679 -4.0701718 -4.0672431 -4.0991693 -4.116673 -4.1348438 -4.1298952 -4.0896764 -4.0614977 -4.0759611][-4.2007041 -4.2190309 -4.2111726 -4.1740408 -4.1186881 -4.038455 -3.9405365 -3.9321978 -3.9963775 -4.0374155 -4.0642114 -4.0567608 -4.0026555 -3.9797745 -4.0109515][-4.1659465 -4.1711507 -4.1514268 -4.1082335 -4.04265 -3.9392414 -3.8018279 -3.7982135 -3.9099 -3.9767194 -4.0100851 -4.0081267 -3.9590087 -3.9471502 -3.9820027][-4.125216 -4.1118445 -4.081378 -4.050066 -3.9998951 -3.9058743 -3.7783818 -3.7944775 -3.9232547 -3.9926016 -4.0242839 -4.0397744 -4.0182314 -4.0161924 -4.0358095][-4.1030083 -4.086133 -4.0594244 -4.0490046 -4.0293055 -3.9783475 -3.9057131 -3.9318051 -4.0253396 -4.0694795 -4.0888886 -4.1116443 -4.1078739 -4.1085129 -4.1182365][-4.1186543 -4.0982156 -4.0761523 -4.0773916 -4.0777473 -4.0616813 -4.0315642 -4.0543585 -4.1096435 -4.1301646 -4.1402264 -4.1631713 -4.1670408 -4.1658382 -4.1708951][-4.1458259 -4.1215825 -4.0995307 -4.100925 -4.1145239 -4.1197338 -4.1102123 -4.1214051 -4.1499619 -4.154985 -4.1580448 -4.1805964 -4.1939092 -4.1952224 -4.1965284][-4.1642694 -4.1383367 -4.1151171 -4.1128755 -4.1288319 -4.1425548 -4.1386366 -4.1403408 -4.153162 -4.1516733 -4.1569204 -4.1835976 -4.203084 -4.2053809 -4.2043228][-4.1824403 -4.1580763 -4.1380863 -4.1325035 -4.1471524 -4.1611753 -4.1544633 -4.1523991 -4.1625113 -4.1694722 -4.1885138 -4.2168655 -4.2317843 -4.2245617 -4.2148566][-4.1983786 -4.1891375 -4.1829987 -4.1804757 -4.192924 -4.2041082 -4.1960158 -4.1925135 -4.2014141 -4.2125726 -4.2338214 -4.2525606 -4.2550545 -4.2389097 -4.2281618][-4.2077446 -4.2180562 -4.2288928 -4.2344375 -4.246757 -4.2534561 -4.2449641 -4.2413793 -4.2455273 -4.2510805 -4.2629714 -4.2696328 -4.2650304 -4.251502 -4.2407932]]...]
INFO - root - 2017-12-08 00:56:07.823071: step 32310, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.611 sec/batch; 59h:55m:12s remains)
INFO - root - 2017-12-08 00:56:24.097523: step 32320, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.568 sec/batch; 58h:19m:13s remains)
INFO - root - 2017-12-08 00:56:40.005790: step 32330, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.669 sec/batch; 62h:05m:26s remains)
INFO - root - 2017-12-08 00:56:55.964492: step 32340, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.601 sec/batch; 59h:33m:59s remains)
INFO - root - 2017-12-08 00:57:12.286366: step 32350, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.676 sec/batch; 62h:20m:10s remains)
INFO - root - 2017-12-08 00:57:28.389098: step 32360, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.573 sec/batch; 58h:30m:11s remains)
INFO - root - 2017-12-08 00:57:44.719948: step 32370, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 1.708 sec/batch; 63h:30m:25s remains)
INFO - root - 2017-12-08 00:58:01.007050: step 32380, loss = 2.10, batch loss = 2.05 (9.7 examples/sec; 1.656 sec/batch; 61h:34m:01s remains)
INFO - root - 2017-12-08 00:58:17.050148: step 32390, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.620 sec/batch; 60h:14m:23s remains)
INFO - root - 2017-12-08 00:58:32.943444: step 32400, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 1.528 sec/batch; 56h:47m:47s remains)
2017-12-08 00:58:34.396723: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3239055 -4.3194547 -4.3173203 -4.3152056 -4.3136263 -4.311223 -4.310698 -4.3047647 -4.2986665 -4.3006 -4.299294 -4.2928643 -4.2900796 -4.2966275 -4.3071837][-4.3031774 -4.2938728 -4.2929959 -4.2945948 -4.2957921 -4.2954874 -4.2953577 -4.2863159 -4.271749 -4.2729449 -4.2738247 -4.2665763 -4.2623057 -4.2662468 -4.2781625][-4.281889 -4.2626419 -4.2584 -4.2638817 -4.2685337 -4.2685041 -4.2669992 -4.2575245 -4.2409458 -4.2451243 -4.2542744 -4.2524524 -4.245975 -4.2441187 -4.2597556][-4.2550349 -4.2247114 -4.21695 -4.2254767 -4.2316575 -4.2264132 -4.2192416 -4.2155657 -4.2102127 -4.2202544 -4.2386236 -4.2455826 -4.239253 -4.2303872 -4.2513843][-4.2169838 -4.1784344 -4.17094 -4.1836281 -4.1863146 -4.1700339 -4.1500192 -4.1511278 -4.1704507 -4.1980038 -4.227067 -4.236517 -4.2315083 -4.2198143 -4.247088][-4.1713648 -4.1317081 -4.127625 -4.1383677 -4.1246424 -4.079608 -4.0317726 -4.0364003 -4.0995512 -4.1573238 -4.19311 -4.2016988 -4.2005219 -4.1985621 -4.2330861][-4.12695 -4.090528 -4.0903935 -4.0950265 -4.0618219 -3.9742699 -3.8737254 -3.8710203 -3.9957929 -4.0948739 -4.1373229 -4.1484814 -4.1578779 -4.16839 -4.2047033][-4.0987711 -4.0638795 -4.06294 -4.0615215 -4.0107961 -3.8796184 -3.7062025 -3.68339 -3.8723071 -4.0141349 -4.0734806 -4.098556 -4.1171136 -4.13713 -4.1772938][-4.105381 -4.0686741 -4.0572414 -4.0439577 -3.9902666 -3.8512011 -3.6562209 -3.6283123 -3.8250511 -3.9650426 -4.0346632 -4.0738039 -4.1003451 -4.1287832 -4.1748023][-4.1507945 -4.125865 -4.1118197 -4.0915737 -4.0513496 -3.9532909 -3.8235538 -3.8051002 -3.9183841 -4.0021324 -4.055799 -4.0971265 -4.1271286 -4.1628852 -4.211308][-4.2181039 -4.2134037 -4.2082591 -4.1897864 -4.1589642 -4.0989814 -4.0223994 -4.0038095 -4.0509176 -4.0883436 -4.1200047 -4.151659 -4.1783395 -4.2146854 -4.2574544][-4.2748284 -4.2833366 -4.2870336 -4.2751541 -4.2573633 -4.2262197 -4.1853313 -4.1740003 -4.1917868 -4.2047997 -4.2209649 -4.2404528 -4.2597737 -4.2881241 -4.310945][-4.3092008 -4.3196769 -4.32647 -4.3229914 -4.3163705 -4.3040805 -4.2874465 -4.2859859 -4.2941132 -4.2970519 -4.3034353 -4.31522 -4.3255749 -4.3420095 -4.3509407][-4.3294511 -4.3354764 -4.3405209 -4.3409667 -4.3381038 -4.3349805 -4.3322492 -4.335815 -4.3393641 -4.3405252 -4.3442192 -4.3489566 -4.3555317 -4.3656826 -4.3686624][-4.3453212 -4.3488722 -4.3513503 -4.351377 -4.3483515 -4.3461547 -4.3464489 -4.3493109 -4.3506746 -4.3524604 -4.3559656 -4.3583412 -4.3621845 -4.3674231 -4.368319]]...]
INFO - root - 2017-12-08 00:58:50.593856: step 32410, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.606 sec/batch; 59h:41m:58s remains)
INFO - root - 2017-12-08 00:59:06.796680: step 32420, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.710 sec/batch; 63h:33m:56s remains)
INFO - root - 2017-12-08 00:59:22.966192: step 32430, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.529 sec/batch; 56h:50m:17s remains)
INFO - root - 2017-12-08 00:59:39.245871: step 32440, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.595 sec/batch; 59h:16m:20s remains)
INFO - root - 2017-12-08 00:59:55.587940: step 32450, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.589 sec/batch; 59h:03m:48s remains)
INFO - root - 2017-12-08 01:00:11.831534: step 32460, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.609 sec/batch; 59h:48m:27s remains)
INFO - root - 2017-12-08 01:00:28.149559: step 32470, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.653 sec/batch; 61h:25m:40s remains)
INFO - root - 2017-12-08 01:00:44.347182: step 32480, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.623 sec/batch; 60h:19m:18s remains)
INFO - root - 2017-12-08 01:01:00.581279: step 32490, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.670 sec/batch; 62h:03m:20s remains)
INFO - root - 2017-12-08 01:01:16.682070: step 32500, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.569 sec/batch; 58h:17m:18s remains)
2017-12-08 01:01:18.160620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2872381 -4.2851624 -4.2833352 -4.2810249 -4.278367 -4.2759562 -4.2744308 -4.2734184 -4.2725773 -4.2723455 -4.2721829 -4.2712789 -4.2700033 -4.2693558 -4.2691984][-4.2842927 -4.2858491 -4.28811 -4.2884068 -4.2874103 -4.2866158 -4.28675 -4.2868485 -4.2864356 -4.2860866 -4.2854056 -4.2836628 -4.2812395 -4.2790256 -4.27712][-4.2668653 -4.2735333 -4.2813826 -4.2859263 -4.2881117 -4.2904491 -4.2937393 -4.2964253 -4.2974725 -4.2976885 -4.297224 -4.2950959 -4.2914424 -4.2873731 -4.2832117][-4.231462 -4.2422 -4.2529178 -4.2608886 -4.2664618 -4.2722659 -4.2786918 -4.2838345 -4.2871585 -4.2902627 -4.2930794 -4.2934971 -4.291234 -4.288013 -4.2835464][-4.1733861 -4.1877208 -4.20046 -4.2112026 -4.2192588 -4.2267447 -4.2320628 -4.2358685 -4.2405839 -4.2477007 -4.25662 -4.263504 -4.2678113 -4.2705855 -4.2708306][-4.0986948 -4.1117239 -4.123909 -4.1353073 -4.1431322 -4.1480751 -4.1471095 -4.1451035 -4.1506281 -4.1647024 -4.1832151 -4.1995883 -4.2141166 -4.2285557 -4.2405677][-4.0562763 -4.0541463 -4.0554943 -4.0584378 -4.0581589 -4.053834 -4.0409336 -4.0290165 -4.0362763 -4.0605536 -4.0899711 -4.1153874 -4.1402159 -4.1684237 -4.1967082][-4.0914869 -4.0766025 -4.0647712 -4.0535164 -4.0374761 -4.0155711 -3.9821579 -3.9531865 -3.9567561 -3.9851365 -4.0190659 -4.0482764 -4.0786929 -4.1158543 -4.1567659][-4.1563158 -4.1420994 -4.1295552 -4.1159663 -4.0964632 -4.0695481 -4.02814 -3.988095 -3.9771495 -3.9902854 -4.012013 -4.0344996 -4.0626297 -4.1000037 -4.1426034][-4.209094 -4.1980805 -4.1883793 -4.1782284 -4.164031 -4.1440187 -4.1113582 -4.0768518 -4.0588679 -4.0557132 -4.0610137 -4.0716166 -4.0905166 -4.1195216 -4.1539788][-4.2444077 -4.2381268 -4.2331085 -4.2268534 -4.21816 -4.2056851 -4.1842546 -4.1597605 -4.1415315 -4.1297321 -4.1243005 -4.1248355 -4.1331162 -4.1507692 -4.1744823][-4.2548809 -4.2532053 -4.2530494 -4.2518134 -4.2485843 -4.2428041 -4.2312984 -4.2164416 -4.202354 -4.1899471 -4.1809163 -4.1757164 -4.1757112 -4.1826696 -4.1954269][-4.2542357 -4.2534804 -4.2541714 -4.2546163 -4.2542768 -4.2529292 -4.2490296 -4.2431955 -4.2357426 -4.22763 -4.2205925 -4.2145963 -4.2104211 -4.2103047 -4.2144437][-4.2556581 -4.2545862 -4.2548528 -4.25535 -4.2555475 -4.2555833 -4.2551785 -4.25439 -4.2522225 -4.2486725 -4.2445612 -4.2396693 -4.2346153 -4.2312374 -4.2299142][-4.2607117 -4.2594786 -4.2595773 -4.259954 -4.2600927 -4.2602158 -4.2604136 -4.2607789 -4.2608104 -4.2601094 -4.2583585 -4.2550206 -4.2507067 -4.24658 -4.2429552]]...]
INFO - root - 2017-12-08 01:01:34.337395: step 32510, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.613 sec/batch; 59h:55m:01s remains)
INFO - root - 2017-12-08 01:01:50.637291: step 32520, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 1.669 sec/batch; 62h:00m:02s remains)
INFO - root - 2017-12-08 01:02:06.915897: step 32530, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.532 sec/batch; 56h:54m:10s remains)
INFO - root - 2017-12-08 01:02:23.065311: step 32540, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 1.485 sec/batch; 55h:08m:26s remains)
INFO - root - 2017-12-08 01:02:39.328309: step 32550, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.588 sec/batch; 58h:58m:08s remains)
INFO - root - 2017-12-08 01:02:55.636045: step 32560, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.716 sec/batch; 63h:43m:52s remains)
INFO - root - 2017-12-08 01:03:11.764573: step 32570, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.601 sec/batch; 59h:25m:59s remains)
INFO - root - 2017-12-08 01:03:28.137404: step 32580, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.658 sec/batch; 61h:32m:59s remains)
INFO - root - 2017-12-08 01:03:44.476467: step 32590, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.628 sec/batch; 60h:26m:42s remains)
INFO - root - 2017-12-08 01:04:00.743363: step 32600, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.689 sec/batch; 62h:41m:43s remains)
2017-12-08 01:04:02.257199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2505894 -4.2480335 -4.244338 -4.2170544 -4.1604152 -4.1295967 -4.1441936 -4.1966639 -4.2413044 -4.251255 -4.2313209 -4.1949215 -4.1892662 -4.2042108 -4.2414637][-4.2139773 -4.2119246 -4.2176428 -4.1969938 -4.1459379 -4.12622 -4.1484532 -4.1968393 -4.2359986 -4.2446423 -4.2234869 -4.1851292 -4.1816716 -4.1998544 -4.2347651][-4.1911778 -4.1897135 -4.2046676 -4.2043686 -4.1793418 -4.1761646 -4.2010741 -4.2316175 -4.2483888 -4.2416778 -4.2153931 -4.1775331 -4.1757817 -4.1983094 -4.2305408][-4.2003655 -4.2055306 -4.2281604 -4.24246 -4.2357593 -4.2329817 -4.2456627 -4.2533836 -4.2483225 -4.2278352 -4.2026248 -4.1776438 -4.1781712 -4.1963959 -4.2255039][-4.2333937 -4.2445388 -4.2699285 -4.2868533 -4.2763195 -4.25698 -4.2450557 -4.223052 -4.197577 -4.1837873 -4.1795835 -4.1761818 -4.1831021 -4.1960783 -4.2204103][-4.2616076 -4.2721844 -4.2950397 -4.3061943 -4.2862682 -4.2456608 -4.1944103 -4.1248932 -4.0855107 -4.1027966 -4.138617 -4.1650062 -4.1836267 -4.1982079 -4.2186356][-4.2785087 -4.2771096 -4.2875643 -4.2860122 -4.2552023 -4.1892934 -4.09113 -3.9704998 -3.9329283 -4.0087252 -4.0937877 -4.1477113 -4.1775808 -4.1958385 -4.2082729][-4.28207 -4.2600822 -4.2538428 -4.240036 -4.2032485 -4.1296158 -4.0155039 -3.8816452 -3.8725033 -3.9982748 -4.1053405 -4.1632123 -4.1915751 -4.2011971 -4.1982889][-4.2703228 -4.2395325 -4.2307768 -4.2177658 -4.1900105 -4.139504 -4.0609641 -3.9782035 -3.994936 -4.0988364 -4.1753664 -4.2114363 -4.2244024 -4.2184825 -4.2005744][-4.2478228 -4.2233591 -4.2247143 -4.2233052 -4.2112279 -4.1883974 -4.1532388 -4.1218343 -4.1439934 -4.2009945 -4.2375774 -4.2500062 -4.2470126 -4.2279754 -4.2041969][-4.2294726 -4.2174015 -4.2302604 -4.241951 -4.2464781 -4.244545 -4.2344384 -4.2256026 -4.2351184 -4.2537379 -4.2622352 -4.2603626 -4.2527995 -4.2338305 -4.212204][-4.2278376 -4.2185359 -4.2300353 -4.2444725 -4.257936 -4.2661557 -4.2652445 -4.2549629 -4.2448764 -4.2420273 -4.2390766 -4.2378268 -4.2397194 -4.2310524 -4.2173719][-4.2339897 -4.2189941 -4.2185874 -4.2257276 -4.2348418 -4.239799 -4.2395005 -4.2220578 -4.199852 -4.1931663 -4.1960154 -4.2086606 -4.2227907 -4.2216363 -4.2108331][-4.2382364 -4.2149997 -4.2040486 -4.2081656 -4.213655 -4.2114182 -4.2066665 -4.1814203 -4.1497712 -4.1505303 -4.1742921 -4.2044635 -4.2244244 -4.2220292 -4.2068663][-4.243525 -4.2151766 -4.1998897 -4.20866 -4.21333 -4.2016506 -4.1849117 -4.1513734 -4.1211286 -4.134656 -4.1768475 -4.2204146 -4.2432308 -4.2404952 -4.2234159]]...]
INFO - root - 2017-12-08 01:04:18.512301: step 32610, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.728 sec/batch; 64h:07m:43s remains)
INFO - root - 2017-12-08 01:04:34.655866: step 32620, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.576 sec/batch; 58h:29m:14s remains)
INFO - root - 2017-12-08 01:04:50.592502: step 32630, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.645 sec/batch; 61h:03m:26s remains)
INFO - root - 2017-12-08 01:05:06.971790: step 32640, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.637 sec/batch; 60h:45m:09s remains)
INFO - root - 2017-12-08 01:05:23.252635: step 32650, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 1.536 sec/batch; 56h:59m:31s remains)
INFO - root - 2017-12-08 01:05:39.630553: step 32660, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.690 sec/batch; 62h:42m:58s remains)
INFO - root - 2017-12-08 01:05:55.927263: step 32670, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.618 sec/batch; 60h:02m:35s remains)
INFO - root - 2017-12-08 01:06:12.272147: step 32680, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.685 sec/batch; 62h:31m:50s remains)
INFO - root - 2017-12-08 01:06:28.417124: step 32690, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.636 sec/batch; 60h:40m:43s remains)
INFO - root - 2017-12-08 01:06:44.569377: step 32700, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 1.616 sec/batch; 59h:57m:51s remains)
2017-12-08 01:06:45.879537: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2399931 -4.2501163 -4.2414904 -4.2242131 -4.2062235 -4.1947169 -4.1978979 -4.2051158 -4.2147985 -4.2257624 -4.2288923 -4.2262869 -4.2127962 -4.1927161 -4.1834407][-4.2203884 -4.2324071 -4.2225275 -4.20451 -4.1902518 -4.1849861 -4.1972489 -4.208631 -4.216877 -4.2241321 -4.2255344 -4.22104 -4.2100897 -4.1892872 -4.1731644][-4.2068338 -4.2203808 -4.2107291 -4.1919341 -4.1773334 -4.1749773 -4.192008 -4.2053523 -4.214231 -4.2246075 -4.2295771 -4.2259464 -4.2174025 -4.1988764 -4.1784692][-4.2000775 -4.2149582 -4.2074728 -4.1844716 -4.163938 -4.1553831 -4.1677623 -4.1828728 -4.2020068 -4.225275 -4.2404766 -4.2426367 -4.2368526 -4.2214894 -4.1996307][-4.2015028 -4.2086554 -4.2025304 -4.1758518 -4.1425014 -4.1188197 -4.1111393 -4.1194148 -4.1517692 -4.1972332 -4.2330289 -4.25166 -4.2559094 -4.2506995 -4.2349205][-4.2066011 -4.2033482 -4.1978378 -4.1698337 -4.1242528 -4.080061 -4.0435324 -4.0365586 -4.0811925 -4.1531887 -4.216321 -4.254467 -4.2689724 -4.2716579 -4.2632985][-4.2169929 -4.2106414 -4.208374 -4.1808558 -4.127 -4.0620184 -3.9938498 -3.9636216 -4.0153365 -4.1086268 -4.1901464 -4.2424889 -4.2652078 -4.2748394 -4.2724376][-4.2384024 -4.2345662 -4.2349629 -4.208858 -4.1511397 -4.07622 -3.9904566 -3.9421315 -3.9910257 -4.0876231 -4.171041 -4.2260656 -4.2524743 -4.2655964 -4.2663651][-4.248724 -4.2482996 -4.2514729 -4.2313981 -4.1822257 -4.1156683 -4.0390115 -3.9894872 -4.0245495 -4.1034064 -4.1739945 -4.2204747 -4.2455316 -4.2575994 -4.2553468][-4.2439137 -4.2479072 -4.2541142 -4.2419562 -4.2021513 -4.147974 -4.09035 -4.0492444 -4.0673165 -4.1192255 -4.1773553 -4.22175 -4.2483749 -4.2612724 -4.2568307][-4.2383189 -4.2459631 -4.2556748 -4.2524128 -4.2208133 -4.1738386 -4.1268845 -4.0925264 -4.0952911 -4.1235676 -4.1672745 -4.2090316 -4.2394238 -4.2560658 -4.2532139][-4.2403574 -4.2518506 -4.2634349 -4.2681623 -4.2476115 -4.2089577 -4.1706686 -4.1407681 -4.1295776 -4.133286 -4.1571198 -4.1891875 -4.2158151 -4.2339134 -4.2344484][-4.2445865 -4.2572718 -4.2691793 -4.280036 -4.2752886 -4.2512436 -4.2224617 -4.1957645 -4.1776009 -4.1605864 -4.160069 -4.1751261 -4.1948562 -4.2077789 -4.2081594][-4.24723 -4.2548223 -4.266705 -4.2830119 -4.2884922 -4.2807755 -4.2680788 -4.25041 -4.2331386 -4.2076964 -4.1866217 -4.1809268 -4.1886749 -4.1917167 -4.1887631][-4.24196 -4.2457752 -4.2606463 -4.2804165 -4.2910738 -4.2952209 -4.2968369 -4.2934761 -4.2868562 -4.2678218 -4.2390537 -4.21808 -4.2101617 -4.2020016 -4.1939459]]...]
INFO - root - 2017-12-08 01:07:02.184831: step 32710, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.685 sec/batch; 62h:29m:37s remains)
INFO - root - 2017-12-08 01:07:18.513258: step 32720, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 1.637 sec/batch; 60h:44m:12s remains)
INFO - root - 2017-12-08 01:07:34.718327: step 32730, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.628 sec/batch; 60h:23m:26s remains)
INFO - root - 2017-12-08 01:07:50.718002: step 32740, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 1.512 sec/batch; 56h:05m:01s remains)
INFO - root - 2017-12-08 01:08:06.937401: step 32750, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.595 sec/batch; 59h:08m:39s remains)
INFO - root - 2017-12-08 01:08:23.238213: step 32760, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.660 sec/batch; 61h:33m:12s remains)
INFO - root - 2017-12-08 01:08:39.414349: step 32770, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.594 sec/batch; 59h:06m:31s remains)
INFO - root - 2017-12-08 01:08:55.775202: step 32780, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.700 sec/batch; 63h:01m:07s remains)
INFO - root - 2017-12-08 01:09:11.803664: step 32790, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.558 sec/batch; 57h:46m:01s remains)
INFO - root - 2017-12-08 01:09:27.867338: step 32800, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.684 sec/batch; 62h:25m:40s remains)
2017-12-08 01:09:29.367956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3187413 -4.3160133 -4.3146391 -4.3143859 -4.3145437 -4.3142467 -4.3115454 -4.3037677 -4.2898474 -4.2654743 -4.2313356 -4.1948485 -4.1652141 -4.142797 -4.1216345][-4.3106365 -4.3074689 -4.3073015 -4.3087168 -4.3115487 -4.313345 -4.3112369 -4.3007255 -4.2801805 -4.2473588 -4.20311 -4.1623955 -4.1346231 -4.1207294 -4.1060357][-4.3037376 -4.2982783 -4.2953396 -4.2960672 -4.29911 -4.3003087 -4.2968221 -4.2837086 -4.2579312 -4.2191048 -4.1702161 -4.1309772 -4.1051016 -4.0930381 -4.0827127][-4.2965651 -4.2867947 -4.2778492 -4.27315 -4.2735977 -4.2738171 -4.2692862 -4.2554307 -4.2261038 -4.182776 -4.1349592 -4.0983868 -4.0740156 -4.0629706 -4.05676][-4.2924423 -4.2771788 -4.2597003 -4.2466717 -4.2429304 -4.2426238 -4.2370143 -4.2200551 -4.1868153 -4.1432438 -4.1002531 -4.0696573 -4.0541577 -4.055274 -4.0541768][-4.2928696 -4.2703285 -4.2467556 -4.229218 -4.2236834 -4.2210975 -4.2136374 -4.1941061 -4.16008 -4.1209512 -4.0844526 -4.0643406 -4.0643926 -4.0781875 -4.075552][-4.2927084 -4.2633491 -4.2375717 -4.2202153 -4.2160697 -4.2135029 -4.2061152 -4.1854992 -4.154779 -4.1232529 -4.0955591 -4.0860949 -4.0960097 -4.1130533 -4.105514][-4.2852764 -4.2482615 -4.2181296 -4.2011008 -4.1986847 -4.19761 -4.1956887 -4.1811471 -4.1596985 -4.1383085 -4.1209726 -4.1162868 -4.1282926 -4.144475 -4.1390285][-4.2606759 -4.2142615 -4.1805553 -4.1691575 -4.1756396 -4.1826611 -4.1877766 -4.1827927 -4.1712546 -4.158565 -4.1482263 -4.147202 -4.1601171 -4.1783719 -4.1782041][-4.2329068 -4.1807847 -4.1451468 -4.1442981 -4.1662717 -4.1853542 -4.1958036 -4.1994224 -4.1975079 -4.1938148 -4.1882386 -4.1856241 -4.1974564 -4.215456 -4.2184429][-4.2064447 -4.153007 -4.1211572 -4.133482 -4.1696663 -4.194418 -4.207262 -4.2167521 -4.2214475 -4.2223635 -4.2190857 -4.2169561 -4.227736 -4.2418513 -4.2440581][-4.1894917 -4.1403542 -4.1122265 -4.1309819 -4.1704326 -4.1933842 -4.2081375 -4.2214808 -4.2307324 -4.2342978 -4.2327876 -4.2316256 -4.2398849 -4.2483163 -4.2494493][-4.1718864 -4.1264887 -4.1023297 -4.1225786 -4.1615672 -4.1822333 -4.1997328 -4.2182708 -4.2311912 -4.2369123 -4.2374945 -4.2360153 -4.2386189 -4.2398186 -4.2363667][-4.1479254 -4.1008282 -4.0775142 -4.1017394 -4.1415038 -4.1655293 -4.1892929 -4.2134609 -4.2305317 -4.2365255 -4.2357974 -4.2306261 -4.225338 -4.2186775 -4.2108245][-4.1147056 -4.0626268 -4.0386648 -4.0680842 -4.1098561 -4.143465 -4.1748571 -4.2021551 -4.2196913 -4.22674 -4.2243896 -4.2146873 -4.203784 -4.1921282 -4.1800385]]...]
INFO - root - 2017-12-08 01:09:45.452231: step 32810, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.708 sec/batch; 63h:18m:29s remains)
INFO - root - 2017-12-08 01:10:01.660857: step 32820, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 1.511 sec/batch; 56h:00m:10s remains)
INFO - root - 2017-12-08 01:10:17.674159: step 32830, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.548 sec/batch; 57h:22m:50s remains)
INFO - root - 2017-12-08 01:10:33.970100: step 32840, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.657 sec/batch; 61h:23m:17s remains)
INFO - root - 2017-12-08 01:10:49.998393: step 32850, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.613 sec/batch; 59h:45m:32s remains)
INFO - root - 2017-12-08 01:11:06.208489: step 32860, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.651 sec/batch; 61h:09m:30s remains)
INFO - root - 2017-12-08 01:11:22.399114: step 32870, loss = 2.09, batch loss = 2.04 (9.9 examples/sec; 1.624 sec/batch; 60h:10m:35s remains)
INFO - root - 2017-12-08 01:11:38.607202: step 32880, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 1.691 sec/batch; 62h:38m:50s remains)
INFO - root - 2017-12-08 01:11:54.711265: step 32890, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.600 sec/batch; 59h:16m:09s remains)
INFO - root - 2017-12-08 01:12:11.099825: step 32900, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.572 sec/batch; 58h:13m:54s remains)
2017-12-08 01:12:12.433037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.217659 -4.188961 -4.1864662 -4.19761 -4.2150397 -4.2209115 -4.2035031 -4.1702075 -4.1344256 -4.1214738 -4.16068 -4.2186441 -4.2701006 -4.3091521 -4.3352537][-4.2089891 -4.1830478 -4.1849384 -4.1989374 -4.2174177 -4.2168193 -4.189414 -4.1459785 -4.1056395 -4.0929375 -4.1390424 -4.2015276 -4.254643 -4.2966781 -4.3249516][-4.2039862 -4.1827278 -4.1884766 -4.2047415 -4.2252135 -4.2183948 -4.1819367 -4.1287045 -4.0856729 -4.0812206 -4.1384635 -4.2020435 -4.2511106 -4.2902489 -4.3179183][-4.1887283 -4.1699252 -4.17703 -4.1968241 -4.2182083 -4.206121 -4.1605144 -4.1007786 -4.0617509 -4.07463 -4.1458459 -4.2105646 -4.2565145 -4.2927046 -4.3174677][-4.1739888 -4.1552882 -4.1610942 -4.1814919 -4.2001543 -4.1817913 -4.1272149 -4.0637808 -4.0360427 -4.0737767 -4.1537309 -4.2183366 -4.2639127 -4.3020625 -4.3244014][-4.1769605 -4.1607962 -4.161798 -4.1749434 -4.1813607 -4.1544018 -4.0874672 -4.0159888 -4.0048304 -4.0684857 -4.1541772 -4.2203259 -4.2693138 -4.3113594 -4.3326139][-4.1966448 -4.1867723 -4.1834373 -4.1829619 -4.1690664 -4.1244984 -4.0340767 -3.9409044 -3.9496307 -4.0405293 -4.1344719 -4.2090044 -4.2675633 -4.3132482 -4.3367133][-4.2080059 -4.2051735 -4.2042012 -4.1939893 -4.1570592 -4.085103 -3.9625704 -3.8409805 -3.8733199 -3.9930043 -4.1012545 -4.1900153 -4.2592521 -4.3087568 -4.3360553][-4.2019815 -4.20692 -4.2164903 -4.2085013 -4.16096 -4.0754485 -3.94414 -3.8239498 -3.869884 -3.9894536 -4.0946317 -4.1840477 -4.2526217 -4.3005214 -4.3319139][-4.1845641 -4.1958952 -4.22068 -4.2265334 -4.1895804 -4.1167083 -4.0120492 -3.9265018 -3.9590054 -4.0440221 -4.1243653 -4.198391 -4.2560878 -4.296207 -4.3276644][-4.1649551 -4.1764827 -4.21289 -4.2341714 -4.2165513 -4.1636529 -4.0888386 -4.033792 -4.0543947 -4.1097989 -4.1677895 -4.2250953 -4.2685637 -4.2983408 -4.3261003][-4.1533084 -4.156476 -4.1914787 -4.2237377 -4.2224689 -4.1899428 -4.1443543 -4.1130323 -4.1329122 -4.171917 -4.2142172 -4.2572131 -4.2865534 -4.306201 -4.327857][-4.1534462 -4.1394615 -4.1625776 -4.1979909 -4.208745 -4.1949191 -4.179832 -4.1713905 -4.1895671 -4.2174678 -4.2481165 -4.2838178 -4.3069215 -4.3188796 -4.3335171][-4.161891 -4.1273332 -4.1341982 -4.1627584 -4.1803608 -4.1852961 -4.1948204 -4.2050929 -4.2206683 -4.24081 -4.2679777 -4.3013434 -4.3229933 -4.329751 -4.3392229][-4.1522446 -4.1067557 -4.1073956 -4.1330862 -4.1553774 -4.1731725 -4.1968346 -4.2170572 -4.2304358 -4.2490139 -4.27929 -4.3122654 -4.3333278 -4.3382525 -4.3446875]]...]
INFO - root - 2017-12-08 01:12:28.697901: step 32910, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.661 sec/batch; 61h:31m:53s remains)
INFO - root - 2017-12-08 01:12:44.699545: step 32920, loss = 2.10, batch loss = 2.04 (10.1 examples/sec; 1.585 sec/batch; 58h:42m:00s remains)
INFO - root - 2017-12-08 01:13:00.949955: step 32930, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.581 sec/batch; 58h:33m:36s remains)
INFO - root - 2017-12-08 01:13:17.227747: step 32940, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 1.717 sec/batch; 63h:35m:42s remains)
INFO - root - 2017-12-08 01:13:33.421519: step 32950, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.585 sec/batch; 58h:41m:34s remains)
INFO - root - 2017-12-08 01:13:49.634640: step 32960, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.624 sec/batch; 60h:08m:02s remains)
INFO - root - 2017-12-08 01:14:05.515909: step 32970, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.582 sec/batch; 58h:34m:22s remains)
INFO - root - 2017-12-08 01:14:21.624398: step 32980, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.590 sec/batch; 58h:50m:43s remains)
INFO - root - 2017-12-08 01:14:37.695950: step 32990, loss = 2.10, batch loss = 2.04 (10.4 examples/sec; 1.541 sec/batch; 57h:03m:14s remains)
INFO - root - 2017-12-08 01:14:53.962198: step 33000, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.682 sec/batch; 62h:15m:55s remains)
2017-12-08 01:14:55.382177: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3482594 -4.3430033 -4.340519 -4.3384805 -4.3375068 -4.3379536 -4.3401389 -4.3398213 -4.3363962 -4.3302612 -4.3229136 -4.3226166 -4.3298068 -4.3415728 -4.3522878][-4.3406835 -4.3317552 -4.3240805 -4.3166924 -4.3127651 -4.3135371 -4.319488 -4.3197174 -4.3120532 -4.2979746 -4.2840958 -4.2829323 -4.2919831 -4.30876 -4.3295641][-4.3253489 -4.3065381 -4.2893085 -4.2745657 -4.2666559 -4.2667909 -4.2787213 -4.2814097 -4.267415 -4.2410374 -4.2200265 -4.2211008 -4.2340379 -4.2579885 -4.29076][-4.2961693 -4.2609043 -4.22907 -4.2050147 -4.1885595 -4.186389 -4.2107744 -4.2236652 -4.2056808 -4.167459 -4.1419711 -4.1467361 -4.1653194 -4.1959867 -4.2387671][-4.2563252 -4.2038975 -4.1601868 -4.1269765 -4.0977631 -4.0884409 -4.1258793 -4.1535273 -4.1357455 -4.0906248 -4.0608497 -4.0642009 -4.0814133 -4.1146159 -4.1730866][-4.2263966 -4.1656809 -4.1167026 -4.0766978 -4.0344658 -4.0161619 -4.0629582 -4.1034479 -4.0878949 -4.0398507 -4.004909 -3.9982553 -4.0055227 -4.0384922 -4.1119204][-4.2202282 -4.1647406 -4.1214705 -4.0842204 -4.0349555 -4.0094914 -4.0602174 -4.111258 -4.1017113 -4.0623093 -4.0238891 -4.0026259 -3.9970715 -4.0252657 -4.0994964][-4.2315235 -4.1869903 -4.1541839 -4.1225567 -4.0720382 -4.0408516 -4.0878162 -4.1412039 -4.1427283 -4.1191344 -4.0893459 -4.0654769 -4.0555754 -4.0822387 -4.1400647][-4.2449703 -4.2098656 -4.1852422 -4.1570773 -4.1073146 -4.0734782 -4.1100626 -4.1592145 -4.1687045 -4.161356 -4.1509995 -4.1386666 -4.1335506 -4.1580229 -4.1959467][-4.260457 -4.2334666 -4.2149959 -4.1927176 -4.1502314 -4.1215897 -4.1482882 -4.1873903 -4.1955466 -4.19872 -4.2017875 -4.1960335 -4.193676 -4.2150011 -4.2388134][-4.2760744 -4.2528148 -4.2400589 -4.2241111 -4.1921668 -4.1730132 -4.1927156 -4.2183118 -4.2212267 -4.2230997 -4.2274532 -4.22384 -4.2221665 -4.2452631 -4.2656159][-4.289567 -4.2660718 -4.2523756 -4.2370768 -4.2164426 -4.2078633 -4.2235723 -4.2410917 -4.2409873 -4.2386365 -4.2398853 -4.2371674 -4.2380128 -4.2618523 -4.2815289][-4.3067083 -4.2873106 -4.272738 -4.2583003 -4.2454863 -4.2433357 -4.25478 -4.2639127 -4.2624006 -4.2596612 -4.2621202 -4.2615795 -4.2639389 -4.281281 -4.2966542][-4.3289924 -4.316062 -4.3037019 -4.2901864 -4.2807 -4.2797294 -4.2871528 -4.2902722 -4.2880068 -4.2883005 -4.2944822 -4.2987394 -4.3012896 -4.310451 -4.3197389][-4.34797 -4.3420105 -4.3353949 -4.3275681 -4.3221951 -4.3216481 -4.3243551 -4.3237524 -4.3219347 -4.3239036 -4.3300061 -4.3347611 -4.3379307 -4.3420429 -4.3457532]]...]
INFO - root - 2017-12-08 01:15:11.650207: step 33010, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.655 sec/batch; 61h:14m:42s remains)
INFO - root - 2017-12-08 01:15:27.827330: step 33020, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.600 sec/batch; 59h:12m:44s remains)
INFO - root - 2017-12-08 01:15:43.943779: step 33030, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.680 sec/batch; 62h:10m:28s remains)
INFO - root - 2017-12-08 01:16:00.189719: step 33040, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 1.506 sec/batch; 55h:43m:28s remains)
INFO - root - 2017-12-08 01:16:16.528628: step 33050, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.550 sec/batch; 57h:20m:38s remains)
INFO - root - 2017-12-08 01:16:32.709341: step 33060, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.647 sec/batch; 60h:55m:13s remains)
INFO - root - 2017-12-08 01:16:48.834557: step 33070, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.569 sec/batch; 58h:02m:06s remains)
INFO - root - 2017-12-08 01:17:05.185086: step 33080, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.725 sec/batch; 63h:47m:44s remains)
INFO - root - 2017-12-08 01:17:21.165804: step 33090, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.582 sec/batch; 58h:31m:01s remains)
INFO - root - 2017-12-08 01:17:37.463173: step 33100, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.675 sec/batch; 61h:56m:27s remains)
2017-12-08 01:17:38.719507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29246 -4.2924376 -4.2859058 -4.277842 -4.2728934 -4.2663584 -4.2652488 -4.2668877 -4.2650557 -4.2587895 -4.2573013 -4.2619066 -4.2633357 -4.2600985 -4.2620153][-4.2778735 -4.2745829 -4.2607474 -4.2428885 -4.2337341 -4.2258153 -4.2228513 -4.2184038 -4.2066779 -4.1954117 -4.1965141 -4.20608 -4.2096262 -4.2059183 -4.212008][-4.267189 -4.2605352 -4.2404652 -4.2137227 -4.1994081 -4.1895056 -4.1811152 -4.1677356 -4.1483049 -4.1358004 -4.14114 -4.1534147 -4.1553679 -4.150229 -4.161767][-4.255723 -4.24657 -4.2225347 -4.1903586 -4.1706 -4.1541295 -4.1331034 -4.1089077 -4.09179 -4.0905914 -4.1058254 -4.1211519 -4.1231823 -4.1188993 -4.1339135][-4.2514777 -4.2417607 -4.2179365 -4.1806622 -4.1530647 -4.1212029 -4.0757394 -4.0383139 -4.0382872 -4.0641308 -4.0972147 -4.118576 -4.1226807 -4.1200643 -4.1332812][-4.2695751 -4.25869 -4.2310066 -4.1848927 -4.139719 -4.0809503 -4.0020885 -3.9523075 -3.9837298 -4.0443296 -4.0972419 -4.125154 -4.130794 -4.1305838 -4.1409593][-4.2922587 -4.2743244 -4.2353783 -4.1749907 -4.1038356 -4.0165434 -3.9094086 -3.8518176 -3.9201207 -4.0113339 -4.0810795 -4.1193681 -4.1315031 -4.1368756 -4.1471195][-4.2978477 -4.2678504 -4.2141428 -4.1407461 -4.0560236 -3.963599 -3.8587041 -3.8125081 -3.8963985 -3.9896379 -4.0583277 -4.099196 -4.1157284 -4.129971 -4.144949][-4.28927 -4.2478647 -4.1862359 -4.1135974 -4.0419965 -3.9749913 -3.9098725 -3.8937798 -3.9555528 -4.0102258 -4.0530748 -4.0821757 -4.0961127 -4.1159797 -4.1371861][-4.269989 -4.22863 -4.1744986 -4.1211495 -4.0823803 -4.0581617 -4.0376549 -4.0395045 -4.0659194 -4.076448 -4.0834789 -4.0884595 -4.0907755 -4.1047144 -4.1251111][-4.245944 -4.2150025 -4.1780815 -4.1493044 -4.1470256 -4.1594734 -4.166141 -4.1713071 -4.1718941 -4.1559815 -4.1380606 -4.1217813 -4.1099567 -4.1101232 -4.1236968][-4.2236681 -4.20446 -4.1843548 -4.1771445 -4.1995635 -4.2288003 -4.2437148 -4.249959 -4.242291 -4.2205129 -4.1947622 -4.1687732 -4.1471376 -4.1369739 -4.1444459][-4.2242293 -4.2157297 -4.210022 -4.218277 -4.2503223 -4.2795506 -4.2927489 -4.2984195 -4.2921991 -4.2749 -4.2532969 -4.2303624 -4.208797 -4.195056 -4.1982384][-4.2613907 -4.2601161 -4.2612419 -4.2727661 -4.2994161 -4.3189812 -4.325808 -4.329267 -4.3259692 -4.3158884 -4.3031297 -4.2888751 -4.2748842 -4.2654161 -4.267695][-4.3032165 -4.3061519 -4.3089037 -4.3183331 -4.3346868 -4.3423433 -4.343812 -4.3439956 -4.3411245 -4.3357868 -4.3298244 -4.3241725 -4.3187556 -4.3151016 -4.31857]]...]
INFO - root - 2017-12-08 01:17:54.968966: step 33110, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.597 sec/batch; 59h:03m:17s remains)
INFO - root - 2017-12-08 01:18:11.079468: step 33120, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.646 sec/batch; 60h:51m:09s remains)
INFO - root - 2017-12-08 01:18:27.240076: step 33130, loss = 2.10, batch loss = 2.04 (10.0 examples/sec; 1.599 sec/batch; 59h:08m:35s remains)
INFO - root - 2017-12-08 01:18:43.712490: step 33140, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.698 sec/batch; 62h:45m:55s remains)
INFO - root - 2017-12-08 01:18:59.903903: step 33150, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.570 sec/batch; 58h:03m:39s remains)
INFO - root - 2017-12-08 01:19:16.224996: step 33160, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.660 sec/batch; 61h:22m:25s remains)
INFO - root - 2017-12-08 01:19:32.313873: step 33170, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.581 sec/batch; 58h:26m:28s remains)
INFO - root - 2017-12-08 01:19:48.665078: step 33180, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.642 sec/batch; 60h:42m:33s remains)
INFO - root - 2017-12-08 01:20:04.856017: step 33190, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 1.616 sec/batch; 59h:44m:18s remains)
INFO - root - 2017-12-08 01:20:21.118159: step 33200, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.588 sec/batch; 58h:40m:22s remains)
2017-12-08 01:20:22.493624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2285771 -4.2323647 -4.2508512 -4.2723508 -4.2920084 -4.2959151 -4.2846327 -4.2698612 -4.2582374 -4.2582455 -4.2690735 -4.2759519 -4.2676415 -4.2577934 -4.2688866][-4.2315593 -4.2470465 -4.2723036 -4.2943587 -4.3098016 -4.30407 -4.2796721 -4.2526054 -4.2379665 -4.2430749 -4.2580252 -4.2664528 -4.2555246 -4.2401466 -4.2477617][-4.2370105 -4.2603645 -4.2876682 -4.3028765 -4.3034005 -4.2794394 -4.2353826 -4.1949167 -4.1795106 -4.19857 -4.2284417 -4.2463245 -4.240294 -4.2264485 -4.2352462][-4.2495985 -4.2736921 -4.2944674 -4.2936935 -4.27138 -4.2193284 -4.1459174 -4.0892277 -4.0834274 -4.128047 -4.184196 -4.2204828 -4.226603 -4.2204895 -4.2348614][-4.2703514 -4.2883105 -4.2952309 -4.2730389 -4.2227859 -4.1383982 -4.03809 -3.9728506 -3.990658 -4.071825 -4.1556849 -4.2072988 -4.2229352 -4.2261047 -4.2446017][-4.2873583 -4.2948785 -4.2861018 -4.246408 -4.177824 -4.0814013 -3.9785123 -3.9218457 -3.9642224 -4.0700631 -4.1644049 -4.2181034 -4.235291 -4.2419114 -4.2591391][-4.2891316 -4.2867088 -4.269659 -4.2266288 -4.1632576 -4.0861673 -4.01002 -3.9733446 -4.0184555 -4.1134667 -4.1936078 -4.2364426 -4.2509179 -4.25674 -4.2694182][-4.2904358 -4.2841945 -4.2652149 -4.2300811 -4.1875362 -4.1435924 -4.0981731 -4.072494 -4.0956063 -4.1552134 -4.2130275 -4.2469096 -4.25925 -4.2629628 -4.2700062][-4.2918897 -4.2875171 -4.2735295 -4.2498817 -4.2288818 -4.2081227 -4.180994 -4.1580329 -4.1595421 -4.18489 -4.2187715 -4.2452264 -4.2562675 -4.25725 -4.2608414][-4.2860374 -4.2842851 -4.27744 -4.2652278 -4.2574539 -4.2461586 -4.2251234 -4.2001939 -4.1872697 -4.1900549 -4.2057481 -4.2291703 -4.2445755 -4.2485447 -4.2535157][-4.2733212 -4.2701344 -4.266326 -4.2606974 -4.2593603 -4.2544632 -4.2368016 -4.2087865 -4.1868987 -4.1809492 -4.1921053 -4.2161818 -4.2365818 -4.2463551 -4.2542815][-4.263309 -4.2597241 -4.2584395 -4.2584329 -4.2601147 -4.2593222 -4.2438254 -4.2093825 -4.1829877 -4.1822157 -4.2003808 -4.228168 -4.2503572 -4.2621779 -4.2710123][-4.2646828 -4.2639475 -4.2647281 -4.2674952 -4.2706571 -4.2704244 -4.2551937 -4.2200642 -4.2005477 -4.2112541 -4.2336392 -4.2593589 -4.2772784 -4.2853007 -4.2921777][-4.27729 -4.2794247 -4.2811689 -4.2839308 -4.2868605 -4.2866378 -4.2755208 -4.2511144 -4.24108 -4.2542119 -4.2715611 -4.2891593 -4.3008018 -4.3057089 -4.3115439][-4.2944531 -4.2956276 -4.2961845 -4.2974944 -4.2994251 -4.2988634 -4.2916613 -4.2769794 -4.2703438 -4.279429 -4.2917871 -4.3038259 -4.3133135 -4.3185439 -4.3251696]]...]
INFO - root - 2017-12-08 01:20:38.615717: step 33210, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.548 sec/batch; 57h:13m:17s remains)
INFO - root - 2017-12-08 01:20:54.931500: step 33220, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 1.758 sec/batch; 64h:57m:28s remains)
INFO - root - 2017-12-08 01:21:11.139278: step 33230, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.596 sec/batch; 58h:58m:19s remains)
INFO - root - 2017-12-08 01:21:27.443230: step 33240, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.659 sec/batch; 61h:18m:27s remains)
INFO - root - 2017-12-08 01:21:43.707630: step 33250, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.554 sec/batch; 57h:25m:31s remains)
INFO - root - 2017-12-08 01:21:59.900105: step 33260, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.603 sec/batch; 59h:12m:31s remains)
INFO - root - 2017-12-08 01:22:16.206100: step 33270, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.582 sec/batch; 58h:25m:38s remains)
INFO - root - 2017-12-08 01:22:32.458079: step 33280, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.660 sec/batch; 61h:19m:17s remains)
INFO - root - 2017-12-08 01:22:48.563071: step 33290, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.588 sec/batch; 58h:38m:09s remains)
INFO - root - 2017-12-08 01:23:05.003294: step 33300, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.595 sec/batch; 58h:54m:09s remains)
2017-12-08 01:23:06.385705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3416681 -4.33867 -4.3286195 -4.3060293 -4.2599144 -4.1999893 -4.1398177 -4.1020193 -4.1185484 -4.1641254 -4.2146254 -4.2611475 -4.2922521 -4.3109784 -4.3229637][-4.337873 -4.3372507 -4.3318295 -4.3111005 -4.2671385 -4.2066016 -4.1390438 -4.0909047 -4.1121159 -4.16635 -4.216877 -4.2609997 -4.2909436 -4.30713 -4.3201647][-4.3365169 -4.3397288 -4.3388419 -4.3238406 -4.2855711 -4.2260771 -4.1497273 -4.0931892 -4.1176391 -4.1767621 -4.2228017 -4.2614555 -4.2884712 -4.3017 -4.3146362][-4.3327494 -4.3388171 -4.3413086 -4.3323851 -4.3004045 -4.2414145 -4.1593628 -4.0973439 -4.1221132 -4.1846342 -4.2283859 -4.2611494 -4.2826767 -4.2924767 -4.3035784][-4.3275127 -4.3356247 -4.3395681 -4.3341918 -4.3086786 -4.254045 -4.1711903 -4.1050968 -4.1303964 -4.1940408 -4.2319846 -4.2562475 -4.2738981 -4.2807865 -4.2891474][-4.3222752 -4.3300128 -4.3323259 -4.3279767 -4.3062415 -4.2576857 -4.1813121 -4.1173429 -4.1453791 -4.2072186 -4.234539 -4.2472982 -4.2586222 -4.2629466 -4.2709785][-4.3084283 -4.3171639 -4.3166752 -4.3098278 -4.2868042 -4.2373743 -4.1665783 -4.1115522 -4.1509266 -4.2133722 -4.2338123 -4.2358823 -4.2385006 -4.23854 -4.2475042][-4.287045 -4.3004894 -4.3013577 -4.2923074 -4.2654715 -4.2136474 -4.1441364 -4.0954208 -4.140995 -4.2063031 -4.2227488 -4.2174582 -4.2138152 -4.2122922 -4.2246628][-4.26996 -4.287137 -4.2914667 -4.2839575 -4.260479 -4.2145624 -4.1517587 -4.1084819 -4.1432629 -4.1999121 -4.2097878 -4.1976185 -4.1884847 -4.1899261 -4.2108][-4.2579217 -4.2759895 -4.2845483 -4.2835336 -4.2690611 -4.2361135 -4.1870437 -4.1480889 -4.1629677 -4.2005019 -4.2022533 -4.1835337 -4.1707954 -4.1778936 -4.2066879][-4.2445431 -4.2607327 -4.2742362 -4.28229 -4.2780404 -4.2569551 -4.2212987 -4.18566 -4.1818924 -4.2019606 -4.200738 -4.1824207 -4.1719289 -4.1844969 -4.2140183][-4.2370152 -4.2489409 -4.2647038 -4.280839 -4.2844334 -4.2712946 -4.2439842 -4.2098169 -4.1931276 -4.2018428 -4.2039165 -4.1927452 -4.1885462 -4.2051363 -4.2312927][-4.2380176 -4.2485852 -4.2655811 -4.2857046 -4.29282 -4.2840061 -4.2610922 -4.2276812 -4.204978 -4.2104297 -4.2194633 -4.2162533 -4.2142344 -4.2275405 -4.2465167][-4.243751 -4.2551832 -4.2705946 -4.2860494 -4.2915 -4.284276 -4.2667527 -4.2404265 -4.2207751 -4.2267323 -4.2404442 -4.2422352 -4.2387424 -4.2421322 -4.2482262][-4.2620764 -4.274941 -4.2854757 -4.2931609 -4.2928567 -4.2829976 -4.265655 -4.2458558 -4.2331295 -4.2406759 -4.2557478 -4.2602406 -4.2545586 -4.2467146 -4.240519]]...]
INFO - root - 2017-12-08 01:23:22.784107: step 33310, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 1.523 sec/batch; 56h:14m:59s remains)
INFO - root - 2017-12-08 01:23:38.893530: step 33320, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.628 sec/batch; 60h:05m:55s remains)
INFO - root - 2017-12-08 01:23:55.288328: step 33330, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.606 sec/batch; 59h:18m:39s remains)
INFO - root - 2017-12-08 01:24:11.347171: step 33340, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 1.694 sec/batch; 62h:33m:05s remains)
INFO - root - 2017-12-08 01:24:27.520842: step 33350, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.625 sec/batch; 59h:58m:35s remains)
INFO - root - 2017-12-08 01:24:43.947020: step 33360, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 1.709 sec/batch; 63h:05m:10s remains)
INFO - root - 2017-12-08 01:25:00.145885: step 33370, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.605 sec/batch; 59h:15m:21s remains)
INFO - root - 2017-12-08 01:25:16.209773: step 33380, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.687 sec/batch; 62h:15m:03s remains)
INFO - root - 2017-12-08 01:25:32.341535: step 33390, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.676 sec/batch; 61h:51m:48s remains)
INFO - root - 2017-12-08 01:25:48.507517: step 33400, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.623 sec/batch; 59h:52m:43s remains)
2017-12-08 01:25:49.923630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2718868 -4.2542729 -4.2406597 -4.2464433 -4.269218 -4.2858958 -4.2935472 -4.2949924 -4.2973018 -4.29904 -4.2985468 -4.2970452 -4.2975178 -4.3004227 -4.3051991][-4.279233 -4.2589874 -4.237927 -4.2357721 -4.2517543 -4.2662973 -4.2720332 -4.2723718 -4.2736588 -4.2765045 -4.2787185 -4.2771912 -4.2750812 -4.2750382 -4.2820606][-4.2932286 -4.2730422 -4.2454491 -4.2300673 -4.2331586 -4.2390714 -4.2391467 -4.236969 -4.2384386 -4.2432213 -4.2484584 -4.2486758 -4.2448635 -4.2402353 -4.2483697][-4.3094945 -4.2919664 -4.2598748 -4.232018 -4.2220645 -4.2174573 -4.2102604 -4.2080822 -4.2159324 -4.2269697 -4.2349262 -4.2356277 -4.2299151 -4.221467 -4.2290182][-4.3008637 -4.2796693 -4.2426372 -4.2067785 -4.1876979 -4.1680746 -4.1484337 -4.1498747 -4.1720924 -4.1952834 -4.2119651 -4.2130241 -4.2050886 -4.1921105 -4.1975403][-4.2523065 -4.2201443 -4.1775532 -4.1411171 -4.1149473 -4.07347 -4.0299563 -4.0327907 -4.0773292 -4.1225185 -4.1502976 -4.1534805 -4.1461511 -4.1331344 -4.1353526][-4.1856904 -4.1398077 -4.08977 -4.0522523 -4.0181694 -3.9479713 -3.8632944 -3.8573253 -3.9371998 -4.0184603 -4.0618525 -4.0697412 -4.0665565 -4.0633588 -4.0679197][-4.1168079 -4.0580482 -4.0035615 -3.9684966 -3.9295871 -3.8323495 -3.7010159 -3.6757238 -3.7941089 -3.914403 -3.9753752 -3.9938934 -4.0002947 -4.0063915 -4.0100756][-4.0861168 -4.0321217 -3.9877558 -3.963815 -3.9316356 -3.8418767 -3.7142668 -3.6796393 -3.7897437 -3.9003015 -3.954988 -3.9763453 -3.9891763 -3.9965103 -3.988765][-4.1052389 -4.0696492 -4.0433769 -4.0326061 -4.0179453 -3.966974 -3.89139 -3.8697481 -3.9351583 -3.9989679 -4.0271811 -4.0383611 -4.0442452 -4.0442505 -4.0266848][-4.1334281 -4.1134148 -4.1021662 -4.1028819 -4.10806 -4.09394 -4.061348 -4.0541115 -4.0864692 -4.118187 -4.1272016 -4.1270142 -4.1264434 -4.1205759 -4.1018667][-4.1470022 -4.1288362 -4.122798 -4.1320577 -4.1521173 -4.1609111 -4.1553793 -4.1575913 -4.1771474 -4.1979232 -4.2006211 -4.1970458 -4.193893 -4.18896 -4.17577][-4.145884 -4.1232576 -4.1156421 -4.127141 -4.1550198 -4.1768842 -4.1864944 -4.1977515 -4.2170286 -4.2361131 -4.2398081 -4.2361279 -4.2332592 -4.2309213 -4.2229214][-4.1488523 -4.1222839 -4.1104517 -4.1198997 -4.1471515 -4.1758533 -4.1955843 -4.2122788 -4.2320261 -4.2496629 -4.2544675 -4.2509708 -4.2479177 -4.2471657 -4.2433763][-4.1842794 -4.1566296 -4.1414957 -4.1459279 -4.167809 -4.1952038 -4.2160769 -4.2326789 -4.24952 -4.2636423 -4.268816 -4.266541 -4.2633228 -4.2627034 -4.2610512]]...]
INFO - root - 2017-12-08 01:26:06.083583: step 33410, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 1.586 sec/batch; 58h:31m:02s remains)
INFO - root - 2017-12-08 01:26:22.463784: step 33420, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 1.650 sec/batch; 60h:52m:09s remains)
INFO - root - 2017-12-08 01:26:38.633147: step 33430, loss = 2.08, batch loss = 2.03 (10.0 examples/sec; 1.601 sec/batch; 59h:05m:01s remains)
INFO - root - 2017-12-08 01:26:54.856043: step 33440, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.627 sec/batch; 60h:01m:48s remains)
INFO - root - 2017-12-08 01:27:11.156055: step 33450, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.608 sec/batch; 59h:17m:59s remains)
INFO - root - 2017-12-08 01:27:27.219078: step 33460, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 1.634 sec/batch; 60h:16m:29s remains)
INFO - root - 2017-12-08 01:27:43.718066: step 33470, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.675 sec/batch; 61h:46m:17s remains)
INFO - root - 2017-12-08 01:28:00.027929: step 33480, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.571 sec/batch; 57h:56m:24s remains)
INFO - root - 2017-12-08 01:28:16.548924: step 33490, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.653 sec/batch; 60h:57m:13s remains)
INFO - root - 2017-12-08 01:28:32.543691: step 33500, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.586 sec/batch; 58h:29m:41s remains)
2017-12-08 01:28:33.985034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.182919 -4.1689677 -4.17561 -4.1795073 -4.1684237 -4.1428041 -4.1072078 -4.0818105 -4.0735297 -4.0769057 -4.0794625 -4.0740795 -4.0814981 -4.1015277 -4.1274233][-4.163269 -4.1481376 -4.1545453 -4.1656718 -4.1667366 -4.15227 -4.1244731 -4.1049242 -4.095881 -4.0916553 -4.0915523 -4.0840254 -4.0863724 -4.0943274 -4.1127114][-4.1371326 -4.1273661 -4.1362677 -4.1532788 -4.161592 -4.1532722 -4.1355748 -4.1249428 -4.1188078 -4.11006 -4.109982 -4.1071858 -4.10971 -4.1034188 -4.1042914][-4.10875 -4.10635 -4.1189456 -4.1373067 -4.1463814 -4.1380644 -4.1260939 -4.12442 -4.1211309 -4.1148624 -4.1195149 -4.1264238 -4.1319208 -4.1107154 -4.0904741][-4.086484 -4.0891371 -4.105824 -4.1220608 -4.1283903 -4.1155229 -4.1007624 -4.1041718 -4.1072845 -4.1084948 -4.1192956 -4.1325088 -4.142344 -4.1197305 -4.0957251][-4.0736184 -4.0827541 -4.1017365 -4.1132174 -4.1149955 -4.0975204 -4.0791292 -4.08495 -4.0934482 -4.0997157 -4.1137533 -4.1310344 -4.1464753 -4.1340628 -4.1211033][-4.0658178 -4.0877819 -4.1105781 -4.1112165 -4.0979538 -4.0755467 -4.0552549 -4.0580997 -4.063962 -4.0684896 -4.0801034 -4.1022186 -4.128613 -4.1299024 -4.1325588][-4.0602303 -4.0928636 -4.1177435 -4.1047978 -4.0802574 -4.0569224 -4.0360246 -4.0322666 -4.0314169 -4.0319414 -4.0450091 -4.0748515 -4.1116896 -4.1239204 -4.1355286][-4.0812988 -4.1109681 -4.1299486 -4.1102057 -4.0838218 -4.0625191 -4.0418496 -4.0283756 -4.0175862 -4.0171676 -4.0366035 -4.0717225 -4.1101565 -4.1242929 -4.1343403][-4.1216831 -4.1418839 -4.1487355 -4.1286469 -4.1109247 -4.0996013 -4.0831738 -4.0602574 -4.035182 -4.0329328 -4.0601845 -4.0958481 -4.1290507 -4.1394281 -4.1452065][-4.1452179 -4.1595078 -4.1594887 -4.1416373 -4.1299849 -4.1267796 -4.1139035 -4.0890975 -4.0614843 -4.0615573 -4.0935621 -4.1277285 -4.1527929 -4.158185 -4.1602573][-4.1583385 -4.1679292 -4.1673069 -4.1568241 -4.1515021 -4.1462193 -4.1331306 -4.1134582 -4.1000113 -4.1103663 -4.1370435 -4.1589 -4.1688495 -4.162899 -4.1604991][-4.1780257 -4.1821356 -4.1817226 -4.1780968 -4.1758461 -4.1700225 -4.1585751 -4.1439509 -4.1409307 -4.154613 -4.1710567 -4.1760473 -4.1709123 -4.1527824 -4.1461849][-4.2020512 -4.2010422 -4.1987767 -4.1973696 -4.1921005 -4.1829123 -4.1704168 -4.1549306 -4.1524291 -4.166327 -4.180624 -4.1802068 -4.1698728 -4.1499233 -4.1459222][-4.2172456 -4.2148471 -4.2119217 -4.2092271 -4.2002053 -4.186964 -4.172224 -4.15883 -4.1593432 -4.1777611 -4.1942148 -4.1949005 -4.1867809 -4.1719589 -4.1715107]]...]
INFO - root - 2017-12-08 01:28:50.446438: step 33510, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.573 sec/batch; 57h:59m:40s remains)
INFO - root - 2017-12-08 01:29:06.647282: step 33520, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 1.661 sec/batch; 61h:14m:02s remains)
INFO - root - 2017-12-08 01:29:22.874204: step 33530, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 1.551 sec/batch; 57h:11m:10s remains)
INFO - root - 2017-12-08 01:29:39.179611: step 33540, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 1.670 sec/batch; 61h:34m:25s remains)
INFO - root - 2017-12-08 01:29:55.345554: step 33550, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 1.492 sec/batch; 54h:59m:22s remains)
INFO - root - 2017-12-08 01:30:11.682377: step 33560, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.656 sec/batch; 61h:01m:41s remains)
INFO - root - 2017-12-08 01:30:28.005647: step 33570, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 1.614 sec/batch; 59h:29m:57s remains)
INFO - root - 2017-12-08 01:30:44.319686: step 33580, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 1.548 sec/batch; 57h:03m:57s remains)
INFO - root - 2017-12-08 01:31:00.677788: step 33590, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.742 sec/batch; 64h:11m:36s remains)
INFO - root - 2017-12-08 01:31:16.910176: step 33600, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 1.577 sec/batch; 58h:06m:09s remains)
2017-12-08 01:31:18.321162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3106318 -4.3111868 -4.307569 -4.3059068 -4.3063283 -4.3073363 -4.3065281 -4.3038712 -4.3028421 -4.2978239 -4.2959428 -4.2976542 -4.3019662 -4.3091407 -4.3131371][-4.3088646 -4.3093238 -4.3019509 -4.2945223 -4.2919288 -4.2941079 -4.2941642 -4.293581 -4.2951388 -4.2906351 -4.2869239 -4.2862821 -4.2919574 -4.2989197 -4.3037472][-4.2860012 -4.2862134 -4.2763038 -4.2632837 -4.2557487 -4.2591195 -4.2624478 -4.266809 -4.2737737 -4.2698765 -4.2655406 -4.2673411 -4.2763972 -4.2803235 -4.2851076][-4.2481794 -4.2526283 -4.2432842 -4.2283835 -4.2202458 -4.2227497 -4.2257667 -4.2309957 -4.2453985 -4.2507505 -4.2509313 -4.2561083 -4.2624674 -4.2608824 -4.2637711][-4.2073984 -4.2193909 -4.2155805 -4.1972094 -4.18056 -4.1721764 -4.1672173 -4.1741509 -4.2017083 -4.2199779 -4.2294555 -4.2392712 -4.2473168 -4.2408466 -4.2406344][-4.1740088 -4.1909885 -4.1863055 -4.1538639 -4.1171389 -4.095335 -4.0892711 -4.1032581 -4.141613 -4.1701 -4.1890755 -4.2069721 -4.2215123 -4.21651 -4.213685][-4.1477332 -4.1604247 -4.14604 -4.0919566 -4.0372853 -4.0174394 -4.0327177 -4.0636296 -4.1028595 -4.1300125 -4.1522779 -4.176312 -4.19785 -4.1942806 -4.1892409][-4.1328321 -4.1345606 -4.1096821 -4.0489354 -3.998085 -4.0022326 -4.0435896 -4.0807838 -4.1072774 -4.1252441 -4.1452565 -4.1718278 -4.1960068 -4.1965365 -4.1909323][-4.1449733 -4.1372466 -4.1138263 -4.0693851 -4.0399632 -4.0551567 -4.0940742 -4.1202497 -4.13004 -4.140203 -4.1586142 -4.1821966 -4.2036095 -4.207633 -4.2023973][-4.1722851 -4.1637969 -4.1493287 -4.1210732 -4.1044569 -4.1125407 -4.1300669 -4.1326675 -4.1266365 -4.1336627 -4.1534572 -4.1804233 -4.2056417 -4.2167978 -4.2161412][-4.2005 -4.1917148 -4.1817641 -4.1602459 -4.1462522 -4.1485567 -4.1512737 -4.1371093 -4.1238632 -4.1298513 -4.150403 -4.1810074 -4.2125878 -4.2274947 -4.2272849][-4.2335033 -4.2222137 -4.2099571 -4.191432 -4.1788945 -4.1798186 -4.1741133 -4.1572676 -4.15135 -4.1595831 -4.178071 -4.2055326 -4.2322168 -4.2432203 -4.2423687][-4.2609005 -4.2474332 -4.2330227 -4.216691 -4.2074461 -4.2084637 -4.201139 -4.190217 -4.1925573 -4.2020741 -4.2151852 -4.2347574 -4.2515688 -4.2575684 -4.2580256][-4.2739596 -4.2631884 -4.2508073 -4.2376027 -4.2335987 -4.236949 -4.2344675 -4.2307978 -4.2352042 -4.2387471 -4.2443962 -4.2570696 -4.2682071 -4.270689 -4.2723212][-4.290668 -4.2833986 -4.2749104 -4.265729 -4.262537 -4.2671041 -4.2699556 -4.2705021 -4.2724638 -4.2716579 -4.2728424 -4.2779646 -4.2838941 -4.2862096 -4.2902913]]...]
INFO - root - 2017-12-08 01:31:34.416198: step 33610, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 1.603 sec/batch; 59h:02m:56s remains)
INFO - root - 2017-12-08 01:31:50.830988: step 33620, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.702 sec/batch; 62h:42m:23s remains)
INFO - root - 2017-12-08 01:32:07.103720: step 33630, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.595 sec/batch; 58h:46m:29s remains)
INFO - root - 2017-12-08 01:32:23.371302: step 33640, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.747 sec/batch; 64h:21m:24s remains)
INFO - root - 2017-12-08 01:32:39.571934: step 33650, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.584 sec/batch; 58h:19m:33s remains)
INFO - root - 2017-12-08 01:32:56.022419: step 33660, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.632 sec/batch; 60h:07m:21s remains)
INFO - root - 2017-12-08 01:33:12.040962: step 33670, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.673 sec/batch; 61h:36m:37s remains)
INFO - root - 2017-12-08 01:33:28.405765: step 33680, loss = 2.09, batch loss = 2.04 (10.3 examples/sec; 1.555 sec/batch; 57h:14m:59s remains)
INFO - root - 2017-12-08 01:33:44.854806: step 33690, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.617 sec/batch; 59h:33m:32s remains)
INFO - root - 2017-12-08 01:34:00.993351: step 33700, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.618 sec/batch; 59h:35m:21s remains)
2017-12-08 01:34:02.462839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2914596 -4.2943912 -4.2923613 -4.2818618 -4.2663746 -4.2604308 -4.259057 -4.2586346 -4.25825 -4.2564316 -4.2578034 -4.2638664 -4.2734609 -4.2775235 -4.2817574][-4.2968493 -4.294816 -4.2861524 -4.2658896 -4.2455635 -4.2354531 -4.2326741 -4.2354131 -4.2409992 -4.2461228 -4.2515988 -4.2612534 -4.27412 -4.28299 -4.2886562][-4.2877803 -4.2749939 -4.255971 -4.2301211 -4.2094035 -4.1992483 -4.1910472 -4.1926346 -4.2075825 -4.2242212 -4.240685 -4.2550526 -4.2710872 -4.2837715 -4.2918725][-4.2816978 -4.2587667 -4.2391129 -4.2182302 -4.2029171 -4.1855931 -4.1648188 -4.1667128 -4.194849 -4.2196932 -4.2396049 -4.2550368 -4.2723079 -4.2842484 -4.291306][-4.2666974 -4.2400346 -4.2268219 -4.2147269 -4.19752 -4.1651492 -4.1236424 -4.1255131 -4.1685805 -4.2043743 -4.2286143 -4.2495646 -4.2696404 -4.2816591 -4.2885175][-4.2382956 -4.2121744 -4.2049541 -4.1961136 -4.1696548 -4.1141334 -4.0466404 -4.0484495 -4.1112061 -4.1643658 -4.202929 -4.2325 -4.2595448 -4.2753487 -4.2852349][-4.1919427 -4.1696358 -4.166688 -4.1562395 -4.1201005 -4.0406523 -3.9480653 -3.9559567 -4.0463409 -4.1266465 -4.1865354 -4.2297044 -4.2590046 -4.2747693 -4.2834306][-4.1371379 -4.1185751 -4.1201324 -4.1116176 -4.07028 -3.9810715 -3.8823485 -3.9069841 -4.0150123 -4.1118569 -4.1838303 -4.2369466 -4.2675209 -4.2790871 -4.282732][-4.1302319 -4.1143012 -4.120625 -4.1145573 -4.0788994 -4.0042558 -3.936168 -3.9663324 -4.0574546 -4.1413908 -4.2030272 -4.2466917 -4.2699876 -4.2773194 -4.2785873][-4.1515989 -4.1381893 -4.1498408 -4.1520691 -4.1287193 -4.0809131 -4.0473166 -4.0725965 -4.1325359 -4.1873078 -4.2258925 -4.2564764 -4.27068 -4.2731853 -4.2751169][-4.1814055 -4.1686373 -4.1823721 -4.19408 -4.1863956 -4.1614275 -4.1476841 -4.1612916 -4.192008 -4.2201557 -4.2432652 -4.2649727 -4.2714434 -4.2695427 -4.2727003][-4.2217951 -4.2080641 -4.2210875 -4.2350969 -4.2342854 -4.2219234 -4.2115827 -4.2120728 -4.2232742 -4.23837 -4.257966 -4.2740149 -4.2746272 -4.2697644 -4.2714343][-4.262341 -4.246747 -4.2557282 -4.2667656 -4.2651687 -4.2559152 -4.2462516 -4.2389417 -4.237617 -4.2455096 -4.2654223 -4.2821612 -4.2842603 -4.2809629 -4.2818079][-4.2998147 -4.2821827 -4.2877269 -4.2944989 -4.2906017 -4.2825646 -4.2761121 -4.2640014 -4.2518296 -4.2517724 -4.2724967 -4.289916 -4.2935467 -4.2939515 -4.2960396][-4.3303933 -4.3144989 -4.3160305 -4.3188715 -4.3148351 -4.3113279 -4.3082 -4.2968068 -4.2801118 -4.2778907 -4.2958179 -4.3085403 -4.3109603 -4.3096042 -4.3105092]]...]
INFO - root - 2017-12-08 01:34:18.664560: step 33710, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.600 sec/batch; 58h:53m:51s remains)
INFO - root - 2017-12-08 01:34:34.895982: step 33720, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 1.681 sec/batch; 61h:53m:34s remains)
INFO - root - 2017-12-08 01:34:51.141148: step 33730, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 1.588 sec/batch; 58h:28m:24s remains)
INFO - root - 2017-12-08 01:35:07.487432: step 33740, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.606 sec/batch; 59h:07m:13s remains)
INFO - root - 2017-12-08 01:35:24.025293: step 33750, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 1.768 sec/batch; 65h:05m:04s remains)
INFO - root - 2017-12-08 01:35:40.106300: step 33760, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.582 sec/batch; 58h:13m:29s remains)
INFO - root - 2017-12-08 01:35:56.497137: step 33770, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.677 sec/batch; 61h:43m:21s remains)
INFO - root - 2017-12-08 01:36:12.698172: step 33780, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.609 sec/batch; 59h:13m:01s remains)
INFO - root - 2017-12-08 01:36:28.887329: step 33790, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.661 sec/batch; 61h:06m:51s remains)
INFO - root - 2017-12-08 01:36:45.156326: step 33800, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.593 sec/batch; 58h:36m:13s remains)
2017-12-08 01:36:46.597976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2507434 -4.2781992 -4.2851372 -4.2749858 -4.2682772 -4.2646079 -4.2524948 -4.23089 -4.2089248 -4.2078323 -4.2233734 -4.2468405 -4.2588215 -4.2472715 -4.214355][-4.2843084 -4.30525 -4.3078518 -4.2964487 -4.286974 -4.275825 -4.2511191 -4.2158213 -4.1834517 -4.1844082 -4.215394 -4.2601323 -4.2956877 -4.3028011 -4.2836528][-4.3040047 -4.3212738 -4.3257537 -4.3178253 -4.3038278 -4.2808752 -4.2432013 -4.1953168 -4.1557684 -4.1596756 -4.1976357 -4.2504449 -4.3016667 -4.3271809 -4.3248019][-4.3192148 -4.3332982 -4.33631 -4.3303957 -4.3093176 -4.2745428 -4.2244892 -4.1651244 -4.116034 -4.1259003 -4.1733418 -4.2269111 -4.2815046 -4.3161306 -4.3251066][-4.3224049 -4.3326855 -4.3315988 -4.3215785 -4.2942753 -4.2505822 -4.1861892 -4.0998983 -4.0282025 -4.0517097 -4.127377 -4.1971154 -4.2596445 -4.3007436 -4.3154006][-4.326057 -4.3316274 -4.3266516 -4.3116336 -4.2738934 -4.2170119 -4.1284933 -3.9950871 -3.8849559 -3.9277174 -4.0512419 -4.1550441 -4.2364588 -4.28887 -4.3090577][-4.3310347 -4.3339949 -4.3243155 -4.3057108 -4.2586436 -4.1906643 -4.081418 -3.9046288 -3.7573106 -3.8147221 -3.9727311 -4.1015515 -4.200572 -4.2670922 -4.2960649][-4.3421059 -4.3424282 -4.3283362 -4.3061452 -4.2572565 -4.1936336 -4.0914426 -3.9206681 -3.7812824 -3.830678 -3.9704256 -4.0895252 -4.187212 -4.2574291 -4.2907152][-4.3708415 -4.3647404 -4.3407946 -4.3124938 -4.2723613 -4.2316809 -4.1628079 -4.044106 -3.943408 -3.9555204 -4.0350614 -4.1168647 -4.1978397 -4.2591939 -4.2897186][-4.3984814 -4.3862514 -4.354857 -4.32493 -4.2995577 -4.2820778 -4.2415547 -4.1660061 -4.0901437 -4.06762 -4.1008992 -4.1555276 -4.2183228 -4.2662311 -4.288219][-4.4030566 -4.3914208 -4.3612728 -4.3368378 -4.3249979 -4.3176818 -4.2887335 -4.2343836 -4.1697578 -4.1320267 -4.1449742 -4.1867366 -4.2387748 -4.2747984 -4.2858233][-4.3926044 -4.3864403 -4.3622856 -4.3442731 -4.3414888 -4.3388858 -4.3152823 -4.27167 -4.2166791 -4.1768947 -4.1809325 -4.2155528 -4.2598076 -4.2852077 -4.2879038][-4.3770828 -4.3781204 -4.3623118 -4.3507237 -4.3506584 -4.3511205 -4.333847 -4.2997775 -4.2564621 -4.2188363 -4.2152815 -4.2436209 -4.284471 -4.3072538 -4.3073721][-4.3657069 -4.3721423 -4.364346 -4.3563118 -4.3547373 -4.3540988 -4.3410215 -4.316186 -4.2847204 -4.254406 -4.249403 -4.2745514 -4.3095713 -4.3313651 -4.3311691][-4.3660522 -4.3747244 -4.3728547 -4.3668518 -4.3630104 -4.3610306 -4.3510842 -4.3342581 -4.3135796 -4.2910175 -4.286489 -4.3060937 -4.3359017 -4.3565083 -4.3571491]]...]
INFO - root - 2017-12-08 01:37:02.729792: step 33810, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.600 sec/batch; 58h:52m:00s remains)
INFO - root - 2017-12-08 01:37:18.767391: step 33820, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.644 sec/batch; 60h:28m:14s remains)
INFO - root - 2017-12-08 01:37:34.859191: step 33830, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.589 sec/batch; 58h:26m:17s remains)
INFO - root - 2017-12-08 01:37:51.113703: step 33840, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 1.620 sec/batch; 59h:35m:32s remains)
INFO - root - 2017-12-08 01:38:07.213122: step 33850, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 1.542 sec/batch; 56h:43m:38s remains)
INFO - root - 2017-12-08 01:38:23.603728: step 33860, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 1.554 sec/batch; 57h:08m:40s remains)
INFO - root - 2017-12-08 01:38:40.029240: step 33870, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.703 sec/batch; 62h:36m:57s remains)
INFO - root - 2017-12-08 01:38:56.254987: step 33880, loss = 2.04, batch loss = 1.99 (10.0 examples/sec; 1.608 sec/batch; 59h:06m:34s remains)
INFO - root - 2017-12-08 01:39:12.611830: step 33890, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 1.716 sec/batch; 63h:04m:41s remains)
INFO - root - 2017-12-08 01:39:28.758365: step 33900, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.599 sec/batch; 58h:46m:23s remains)
2017-12-08 01:39:30.139916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3317952 -4.3290854 -4.3266182 -4.322794 -4.3192034 -4.320488 -4.3255582 -4.3243308 -4.316566 -4.3103466 -4.304832 -4.3034143 -4.306982 -4.3126397 -4.3180022][-4.3177152 -4.3158617 -4.3139768 -4.3097367 -4.3050838 -4.3080783 -4.3176284 -4.3178177 -4.3071675 -4.2976918 -4.2875862 -4.2822571 -4.2866049 -4.2950511 -4.3019342][-4.2910008 -4.2901525 -4.2878022 -4.2841597 -4.2792397 -4.2833018 -4.2963996 -4.2989564 -4.286366 -4.2741075 -4.2624826 -4.2551112 -4.2608461 -4.27266 -4.2826109][-4.2593756 -4.2582889 -4.253758 -4.24928 -4.2427888 -4.2454185 -4.2584434 -4.2610378 -4.2473626 -4.2356815 -4.2280126 -4.2241745 -4.2311049 -4.2449174 -4.2585621][-4.2213483 -4.2160468 -4.2072525 -4.1979704 -4.1858859 -4.1827168 -4.1929445 -4.1996675 -4.1958184 -4.1948743 -4.1969519 -4.2004809 -4.2079449 -4.2212853 -4.2395668][-4.1836219 -4.1710811 -4.1557946 -4.1383882 -4.1138864 -4.094871 -4.0963712 -4.1119504 -4.1276689 -4.1449609 -4.1614342 -4.1756673 -4.1876974 -4.2026391 -4.2243295][-4.1371393 -4.1144805 -4.0894074 -4.0596027 -4.0157542 -3.9698737 -3.9559784 -3.9802251 -4.0195866 -4.0608411 -4.0973153 -4.1316433 -4.1604629 -4.1848087 -4.2109275][-4.0867095 -4.0553355 -4.02714 -3.9941306 -3.935843 -3.8547242 -3.8094735 -3.8329937 -3.8930278 -3.9558973 -4.0120044 -4.0654936 -4.1140213 -4.1551952 -4.1926842][-4.0506315 -4.02093 -4.004221 -3.9891591 -3.9475687 -3.8666205 -3.8012593 -3.8036647 -3.8554239 -3.9169793 -3.9700127 -4.025722 -4.0829067 -4.1321449 -4.1765709][-4.0666952 -4.0461755 -4.0436268 -4.0532384 -4.047863 -4.0028615 -3.9486508 -3.9254668 -3.9412494 -3.9761803 -4.0072966 -4.0503197 -4.098855 -4.1378269 -4.17403][-4.115788 -4.1016974 -4.1070595 -4.12801 -4.1434174 -4.129982 -4.0982704 -4.0699272 -4.0613847 -4.0720611 -4.0853815 -4.1161551 -4.1540341 -4.1800804 -4.2001982][-4.1789923 -4.1686363 -4.1758351 -4.1981516 -4.2190456 -4.2212586 -4.2079506 -4.1875615 -4.1734967 -4.1709385 -4.1719604 -4.1887903 -4.2165179 -4.2363935 -4.2471676][-4.2430415 -4.2349644 -4.2402363 -4.25839 -4.2787251 -4.2886715 -4.2854609 -4.2724619 -4.2608709 -4.2560954 -4.2516322 -4.2568793 -4.2737303 -4.2889132 -4.2956481][-4.2899504 -4.2857423 -4.289012 -4.3013196 -4.3159881 -4.3247609 -4.3253446 -4.3186612 -4.3107648 -4.306601 -4.30247 -4.3023992 -4.3104677 -4.321506 -4.326767][-4.3159275 -4.3127952 -4.3131738 -4.3187156 -4.3257046 -4.3308454 -4.3329086 -4.3310761 -4.3270421 -4.3242555 -4.320868 -4.3195138 -4.3238978 -4.3307314 -4.3348117]]...]
INFO - root - 2017-12-08 01:39:46.517986: step 33910, loss = 2.04, batch loss = 1.98 (10.1 examples/sec; 1.591 sec/batch; 58h:28m:12s remains)
INFO - root - 2017-12-08 01:40:02.898381: step 33920, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 1.640 sec/batch; 60h:17m:27s remains)
INFO - root - 2017-12-08 01:40:19.392374: step 33930, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.577 sec/batch; 57h:58m:46s remains)
INFO - root - 2017-12-08 01:40:35.717553: step 33940, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 1.698 sec/batch; 62h:24m:46s remains)
INFO - root - 2017-12-08 01:40:51.836460: step 33950, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 1.744 sec/batch; 64h:06m:07s remains)
INFO - root - 2017-12-08 01:41:08.184965: step 33960, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 1.585 sec/batch; 58h:14m:08s remains)
INFO - root - 2017-12-08 01:41:24.661949: step 33970, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.664 sec/batch; 61h:08m:08s remains)
INFO - root - 2017-12-08 01:41:40.955323: step 33980, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 1.583 sec/batch; 58h:10m:26s remains)
INFO - root - 2017-12-08 01:41:57.290962: step 33990, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 1.583 sec/batch; 58h:10m:29s remains)
INFO - root - 2017-12-08 01:42:13.565070: step 34000, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 1.726 sec/batch; 63h:24m:33s remains)
2017-12-08 01:42:14.839431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1417208 -4.1668205 -4.20218 -4.2078071 -4.1937652 -4.1731696 -4.1574235 -4.1526222 -4.1680121 -4.1901278 -4.2005644 -4.1935368 -4.1749744 -4.1627111 -4.1681285][-4.1338725 -4.1626425 -4.2058158 -4.218226 -4.2102337 -4.1928005 -4.1723361 -4.1609087 -4.1710186 -4.1914058 -4.2004724 -4.1903305 -4.1661696 -4.1567721 -4.1715775][-4.13548 -4.1655564 -4.2121758 -4.2297072 -4.2273316 -4.2149644 -4.1909142 -4.1747427 -4.182694 -4.20122 -4.2084436 -4.1936779 -4.1636043 -4.1596179 -4.182353][-4.1446381 -4.1744576 -4.214973 -4.2309518 -4.2324462 -4.22275 -4.1967421 -4.1787138 -4.1858134 -4.2033429 -4.2098885 -4.1932583 -4.1633072 -4.1628847 -4.1881247][-4.15008 -4.177927 -4.2098932 -4.2234387 -4.2238069 -4.2107639 -4.1813045 -4.163795 -4.1748805 -4.1964688 -4.2065964 -4.1918988 -4.1618052 -4.1584606 -4.1809583][-4.1494851 -4.1735468 -4.1982188 -4.2076011 -4.2007194 -4.1797352 -4.1434145 -4.1284065 -4.1518054 -4.1826577 -4.2003236 -4.1901264 -4.1617332 -4.1590858 -4.1816773][-4.1447134 -4.1610522 -4.1737075 -4.1751752 -4.1584644 -4.1289625 -4.0818982 -4.0640521 -4.1057177 -4.1539383 -4.1818018 -4.1785703 -4.1560545 -4.1545248 -4.1722174][-4.1399055 -4.1447396 -4.1432948 -4.1317534 -4.1012983 -4.0548124 -3.9860659 -3.9545105 -4.0219336 -4.101438 -4.145164 -4.1525216 -4.1359863 -4.1286888 -4.1381464][-4.1538148 -4.156395 -4.1468 -4.1200027 -4.0675278 -3.9920945 -3.8908293 -3.83224 -3.9202316 -4.0353889 -4.098433 -4.1185632 -4.1056776 -4.0893707 -4.0930228][-4.1914463 -4.1911874 -4.1775131 -4.144701 -4.0862522 -4.0070691 -3.9074798 -3.8428097 -3.9188015 -4.0287795 -4.0881491 -4.1047931 -4.0880795 -4.0677333 -4.0724907][-4.232832 -4.2266817 -4.2103539 -4.1812091 -4.1361141 -4.0841103 -4.0260806 -3.9862504 -4.0250726 -4.0897141 -4.1212134 -4.1232982 -4.1005979 -4.0809646 -4.088717][-4.2489228 -4.2415118 -4.2278004 -4.2090178 -4.1781116 -4.1445847 -4.1189423 -4.1049156 -4.1234107 -4.1552057 -4.1678085 -4.167542 -4.1501746 -4.1360712 -4.1419725][-4.2425132 -4.2361121 -4.2247715 -4.2103677 -4.1890135 -4.1643476 -4.1574521 -4.1630654 -4.1871362 -4.2124844 -4.2192683 -4.2208185 -4.2120996 -4.2056127 -4.2085409][-4.2281485 -4.2220798 -4.2064495 -4.19607 -4.1861029 -4.17301 -4.1763887 -4.1903877 -4.2188382 -4.2437572 -4.2504935 -4.2545185 -4.252346 -4.2492824 -4.2495079][-4.2057562 -4.2062778 -4.1906323 -4.1776628 -4.1713166 -4.1686969 -4.1824789 -4.2040291 -4.2331305 -4.2522621 -4.2563758 -4.261754 -4.2635441 -4.259017 -4.2560639]]...]
INFO - root - 2017-12-08 01:42:31.181115: step 34010, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 1.570 sec/batch; 57h:41m:03s remains)
INFO - root - 2017-12-08 01:42:47.527876: step 34020, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.596 sec/batch; 58h:37m:43s remains)
INFO - root - 2017-12-08 01:43:03.909055: step 34030, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 1.668 sec/batch; 61h:15m:40s remains)
INFO - root - 2017-12-08 01:43:20.015075: step 34040, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 1.570 sec/batch; 57h:38m:35s remains)
INFO - root - 2017-12-08 01:43:36.607691: step 34050, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 1.731 sec/batch; 63h:33m:03s remains)
INFO - root - 2017-12-08 01:43:52.791801: step 34060, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.613 sec/batch; 59h:12m:50s remains)
INFO - root - 2017-12-08 01:44:09.144070: step 34070, loss = 2.08, batch loss = 2.03 (10.5 examples/sec; 1.531 sec/batch; 56h:12m:22s remains)
INFO - root - 2017-12-08 01:44:25.202338: step 34080, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.579 sec/batch; 57h:57m:34s remains)
INFO - root - 2017-12-08 01:44:41.568487: step 34090, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 1.564 sec/batch; 57h:24m:54s remains)
INFO - root - 2017-12-08 01:44:57.779042: step 34100, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.690 sec/batch; 62h:02m:41s remains)
2017-12-08 01:44:59.190074: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3351278 -4.3271756 -4.3102345 -4.2901683 -4.2779584 -4.2762003 -4.2864704 -4.3038564 -4.3218265 -4.3358297 -4.3430777 -4.3453059 -4.3450732 -4.3443623 -4.3433614][-4.3405986 -4.3323908 -4.3103933 -4.2799997 -4.2549815 -4.2395329 -4.2423806 -4.2604547 -4.2885847 -4.3156161 -4.3348193 -4.344614 -4.3470025 -4.3456149 -4.3434391][-4.3312616 -4.3204088 -4.2930665 -4.2538681 -4.2171183 -4.191783 -4.1899858 -4.2083583 -4.2408624 -4.2781844 -4.3101196 -4.3315153 -4.3430848 -4.3455715 -4.3435135][-4.3229995 -4.3098922 -4.2754197 -4.2235041 -4.171751 -4.1319146 -4.1243281 -4.14524 -4.1836681 -4.2303476 -4.2740602 -4.3058319 -4.3285375 -4.3394918 -4.341095][-4.3272829 -4.3156061 -4.2782755 -4.2144527 -4.1432438 -4.0836987 -4.0587177 -4.0712619 -4.1118894 -4.169888 -4.2268658 -4.2676864 -4.2994876 -4.3221712 -4.3332171][-4.33537 -4.3271351 -4.29371 -4.2284346 -4.1415429 -4.0610361 -4.0059204 -3.9823484 -4.0089445 -4.0803432 -4.1607327 -4.2188163 -4.2611809 -4.2944956 -4.3187304][-4.3420663 -4.3389049 -4.3163152 -4.2633581 -4.1775322 -4.0804753 -3.9901745 -3.91142 -3.8935535 -3.9676137 -4.0759435 -4.1601996 -4.21958 -4.2653213 -4.3030796][-4.3460751 -4.3474517 -4.3357344 -4.3012104 -4.2308993 -4.13315 -4.0236683 -3.9096293 -3.8447671 -3.89022 -4.0041552 -4.1082463 -4.181459 -4.2375865 -4.2839112][-4.34825 -4.3518882 -4.3477726 -4.328917 -4.2850409 -4.2065067 -4.1012993 -3.9884746 -3.9109154 -3.9128532 -3.9868541 -4.0774446 -4.1495051 -4.2083282 -4.2567277][-4.3491254 -4.3544512 -4.3566957 -4.3494086 -4.3277736 -4.2782097 -4.1954389 -4.1011095 -4.030458 -4.0052867 -4.0278707 -4.0729985 -4.1183205 -4.1669059 -4.2145815][-4.3495579 -4.3537364 -4.3590755 -4.3609138 -4.353302 -4.3282375 -4.2758403 -4.2064881 -4.1461506 -4.1081262 -4.0932884 -4.0916858 -4.0976458 -4.1223016 -4.163878][-4.3476176 -4.3527908 -4.3571882 -4.3636622 -4.3660774 -4.3555989 -4.3277664 -4.2862439 -4.24334 -4.2021937 -4.1645875 -4.1240597 -4.0904651 -4.083333 -4.1092119][-4.3408322 -4.3479137 -4.3523893 -4.359139 -4.3663282 -4.364769 -4.3537779 -4.3340931 -4.3110085 -4.279767 -4.2367668 -4.1768456 -4.1109252 -4.0642 -4.0579615][-4.3324437 -4.3390112 -4.3446431 -4.3511729 -4.3592315 -4.362443 -4.3598857 -4.3535419 -4.3425045 -4.3241591 -4.2923026 -4.2373247 -4.1658225 -4.0933046 -4.0446224][-4.3244171 -4.3299475 -4.3346777 -4.340374 -4.3478022 -4.3540816 -4.3572211 -4.3568854 -4.3534575 -4.3451843 -4.3279266 -4.2916536 -4.2334895 -4.1604829 -4.0903077]]...]
INFO - root - 2017-12-08 01:45:15.365846: step 34110, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.561 sec/batch; 57h:18m:40s remains)
INFO - root - 2017-12-08 01:45:31.683518: step 34120, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.650 sec/batch; 60h:33m:20s remains)
INFO - root - 2017-12-08 01:45:47.911696: step 34130, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 1.696 sec/batch; 62h:13m:48s remains)
INFO - root - 2017-12-08 01:46:04.090988: step 34140, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.611 sec/batch; 59h:06m:17s remains)
INFO - root - 2017-12-08 01:46:20.473673: step 34150, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.658 sec/batch; 60h:50m:18s remains)
INFO - root - 2017-12-08 01:46:36.645981: step 34160, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 1.542 sec/batch; 56h:34m:29s remains)
INFO - root - 2017-12-08 01:46:52.731411: step 34170, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.602 sec/batch; 58h:45m:49s remains)
INFO - root - 2017-12-08 01:47:09.091964: step 34180, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 1.657 sec/batch; 60h:47m:24s remains)
INFO - root - 2017-12-08 01:47:25.390383: step 34190, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.673 sec/batch; 61h:21m:22s remains)
INFO - root - 2017-12-08 01:47:41.560316: step 34200, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.613 sec/batch; 59h:10m:25s remains)
2017-12-08 01:47:42.952486: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3093295 -4.3098826 -4.3152132 -4.3190475 -4.3176079 -4.3142219 -4.3155456 -4.3240385 -4.3287144 -4.328021 -4.3231306 -4.3150978 -4.3061247 -4.3023558 -4.3066125][-4.302465 -4.3077297 -4.3154488 -4.3179069 -4.3100252 -4.2958388 -4.2879963 -4.2952824 -4.3036537 -4.3066282 -4.3037915 -4.2956777 -4.2839136 -4.27953 -4.2865057][-4.302537 -4.3089089 -4.3128324 -4.308188 -4.2920122 -4.2696757 -4.2554417 -4.2608733 -4.2756863 -4.2848186 -4.2835913 -4.2737632 -4.2632594 -4.26077 -4.2699418][-4.301405 -4.3010259 -4.295465 -4.2818279 -4.2634978 -4.2430158 -4.2266154 -4.2305636 -4.2532153 -4.2663732 -4.2645597 -4.2547736 -4.251534 -4.2540212 -4.2634239][-4.2942142 -4.2857642 -4.2753267 -4.2606277 -4.2439575 -4.2213845 -4.1950245 -4.1928358 -4.2171011 -4.2323627 -4.2350578 -4.2321825 -4.2372689 -4.2428246 -4.2515965][-4.2905579 -4.2806878 -4.2708764 -4.2548509 -4.2287359 -4.1910706 -4.1448512 -4.1273007 -4.1524897 -4.1782837 -4.1979451 -4.2101345 -4.2218008 -4.230947 -4.242043][-4.2807856 -4.273747 -4.2656646 -4.2404013 -4.1909218 -4.1219172 -4.040606 -4.00614 -4.0493364 -4.1066504 -4.1567764 -4.1903563 -4.20954 -4.2194576 -4.2330809][-4.2556739 -4.2483835 -4.2379675 -4.1988354 -4.124311 -4.0240874 -3.9071658 -3.8645144 -3.9551551 -4.0610609 -4.1421666 -4.192328 -4.2161927 -4.2239704 -4.2354422][-4.2258449 -4.2112188 -4.1991558 -4.1587586 -4.0828633 -3.9839878 -3.8785355 -3.8572812 -3.9668972 -4.0802135 -4.1609206 -4.2100267 -4.2320843 -4.2386379 -4.2491503][-4.2121892 -4.1951847 -4.1876211 -4.1603589 -4.1071787 -4.0440378 -3.9894791 -3.9920864 -4.0630322 -4.1378231 -4.1934757 -4.2272992 -4.2428813 -4.2488551 -4.2582173][-4.2233558 -4.2069392 -4.2005844 -4.1833692 -4.1504946 -4.1171336 -4.0943956 -4.1020083 -4.1375265 -4.1787858 -4.2144217 -4.2385397 -4.2517223 -4.2591386 -4.266325][-4.2393303 -4.2205172 -4.2103057 -4.1966591 -4.1752505 -4.1562662 -4.1459913 -4.1530614 -4.1733851 -4.1999969 -4.226645 -4.2483563 -4.2616129 -4.2675877 -4.2705212][-4.2557144 -4.2335334 -4.218452 -4.2064447 -4.1937847 -4.1845012 -4.1810966 -4.188561 -4.2034163 -4.2254934 -4.2486615 -4.2672853 -4.2775726 -4.2788367 -4.2767811][-4.2787542 -4.2582884 -4.2465119 -4.2400885 -4.2351818 -4.233573 -4.2345271 -4.2399716 -4.2483253 -4.2627358 -4.2774196 -4.2867508 -4.29198 -4.2912445 -4.2882466][-4.3038678 -4.2902212 -4.2852416 -4.285285 -4.2874837 -4.2912841 -4.293889 -4.2958732 -4.2963572 -4.3003969 -4.3056593 -4.3093081 -4.31262 -4.3126588 -4.3102922]]...]
INFO - root - 2017-12-08 01:47:59.341508: step 34210, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 1.603 sec/batch; 58h:48m:21s remains)
INFO - root - 2017-12-08 01:48:15.665387: step 34220, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 1.637 sec/batch; 60h:02m:02s remains)
INFO - root - 2017-12-08 01:48:31.709195: step 34230, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 1.653 sec/batch; 60h:36m:23s remains)
INFO - root - 2017-12-08 01:48:47.936994: step 34240, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.625 sec/batch; 59h:34m:22s remains)
INFO - root - 2017-12-08 01:49:04.229895: step 34250, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.661 sec/batch; 60h:54m:33s remains)
INFO - root - 2017-12-08 01:49:20.368730: step 34260, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.605 sec/batch; 58h:50m:29s remains)
INFO - root - 2017-12-08 01:49:36.708277: step 34270, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 1.614 sec/batch; 59h:10m:37s remains)
INFO - root - 2017-12-08 01:49:52.938861: step 34280, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.731 sec/batch; 63h:28m:07s remains)
INFO - root - 2017-12-08 01:50:09.054000: step 34290, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 1.514 sec/batch; 55h:29m:26s remains)
INFO - root - 2017-12-08 01:50:25.360679: step 34300, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 1.744 sec/batch; 63h:54m:45s remains)
2017-12-08 01:50:26.625426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4065733 -4.3914857 -4.3439465 -4.2684922 -4.1839881 -4.0859952 -4.0095453 -4.0060077 -4.0605645 -4.1261549 -4.1836629 -4.2353482 -4.275701 -4.3053217 -4.31628][-4.3893342 -4.3699422 -4.3171754 -4.2400508 -4.1662822 -4.0890689 -4.0317025 -4.0266962 -4.061533 -4.1159668 -4.1720524 -4.226377 -4.2675543 -4.2966938 -4.3082757][-4.374907 -4.3557668 -4.3046517 -4.2322359 -4.1710348 -4.1111293 -4.0655117 -4.05112 -4.055491 -4.0910878 -4.1417422 -4.1995611 -4.2475019 -4.2832985 -4.2994633][-4.3701525 -4.3564219 -4.3127675 -4.2521544 -4.1983662 -4.1441441 -4.1013918 -4.0709524 -4.0438733 -4.0527124 -4.0960617 -4.1604567 -4.2197471 -4.2658215 -4.2911286][-4.3668265 -4.3635793 -4.3359408 -4.2915359 -4.241219 -4.1829967 -4.1283278 -4.0672727 -4.0088391 -3.9974487 -4.0450244 -4.124054 -4.1962891 -4.2526579 -4.288384][-4.3608742 -4.3645897 -4.3480287 -4.3164678 -4.2689285 -4.2041478 -4.1279073 -4.0303903 -3.9488277 -3.9392488 -4.0096745 -4.1067038 -4.1885157 -4.2503614 -4.2915468][-4.3571715 -4.3634644 -4.354022 -4.32914 -4.2820053 -4.2079449 -4.1076388 -3.9869428 -3.8974745 -3.9017265 -4.0004282 -4.1149292 -4.2057524 -4.2669287 -4.3044643][-4.353972 -4.3618808 -4.35555 -4.3304276 -4.2795115 -4.1964431 -4.0888786 -3.9744432 -3.8973 -3.9165318 -4.0256886 -4.1427431 -4.2331548 -4.2895889 -4.3212814][-4.3451395 -4.3513322 -4.3452811 -4.3193045 -4.2656388 -4.1769934 -4.0732236 -3.9876132 -3.9439743 -3.9780257 -4.0817342 -4.1867752 -4.2643728 -4.3107152 -4.3340144][-4.337873 -4.3395734 -4.3322325 -4.3066096 -4.2507696 -4.161706 -4.0661683 -4.0080357 -3.9974856 -4.0511913 -4.1479149 -4.2360511 -4.2945189 -4.327693 -4.3402495][-4.3417411 -4.3396144 -4.3285017 -4.3041668 -4.252017 -4.169467 -4.089467 -4.0528731 -4.0605259 -4.1235876 -4.2109294 -4.2810326 -4.3211575 -4.3413391 -4.346025][-4.3497915 -4.3445983 -4.3306079 -4.309412 -4.2676349 -4.203054 -4.1407032 -4.1151628 -4.1286964 -4.1881666 -4.2630539 -4.3179426 -4.3439245 -4.35412 -4.3554692][-4.3484235 -4.3429341 -4.3291759 -4.3128004 -4.2829762 -4.2385278 -4.1955562 -4.1781087 -4.1917949 -4.2410965 -4.3011317 -4.3433928 -4.3605037 -4.3660841 -4.3671679][-4.3384423 -4.3359752 -4.3259873 -4.3147764 -4.2945752 -4.2646914 -4.23884 -4.2295728 -4.2429309 -4.28145 -4.3258471 -4.3579926 -4.3721108 -4.3770852 -4.377933][-4.32336 -4.3239975 -4.3211017 -4.3178468 -4.3070073 -4.2887363 -4.2721758 -4.2633572 -4.2715349 -4.2988577 -4.3308425 -4.3576303 -4.3741412 -4.3806839 -4.3812981]]...]
INFO - root - 2017-12-08 01:50:42.872124: step 34310, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 1.745 sec/batch; 63h:57m:36s remains)
INFO - root - 2017-12-08 01:50:58.902528: step 34320, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 1.603 sec/batch; 58h:44m:21s remains)
INFO - root - 2017-12-08 01:51:15.280570: step 34330, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 1.689 sec/batch; 61h:53m:55s remains)
INFO - root - 2017-12-08 01:51:31.706157: step 34340, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 1.624 sec/batch; 59h:31m:18s remains)
INFO - root - 2017-12-08 01:51:47.974550: step 34350, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 1.706 sec/batch; 62h:29m:25s remains)
INFO - root - 2017-12-08 01:52:04.295155: step 34360, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 1.683 sec/batch; 61h:38m:24s remains)
INFO - root - 2017-12-08 01:52:20.669623: step 34370, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 1.553 sec/batch; 56h:52m:27s remains)
INFO - root - 2017-12-08 01:52:36.794218: step 34380, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 1.648 sec/batch; 60h:21m:58s remains)
INFO - root - 2017-12-08 01:52:53.069290: step 34390, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 1.621 sec/batch; 59h:22m:30s remains)
INFO - root - 2017-12-08 01:53:09.553845: step 34400, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 1.682 sec/batch; 61h:35m:54s remains)
2017-12-08 01:53:10.875989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1879067 -4.2098327 -4.2257891 -4.2335787 -4.2428803 -4.249465 -4.2433691 -4.2177191 -4.1962085 -4.1927071 -4.1937919 -4.1952291 -4.1803389 -4.1699057 -4.1834841][-4.1971703 -4.2118711 -4.2265682 -4.2348585 -4.2416682 -4.2456732 -4.2380538 -4.215744 -4.2031765 -4.2119212 -4.216836 -4.2108936 -4.1897535 -4.17653 -4.1887622][-4.2038517 -4.2141213 -4.2287 -4.23644 -4.2391357 -4.2417192 -4.2344532 -4.2177119 -4.2140179 -4.2296796 -4.2324467 -4.218997 -4.1989365 -4.1912808 -4.2023311][-4.1909733 -4.2023721 -4.22091 -4.2339468 -4.2361131 -4.2311206 -4.2166853 -4.2013659 -4.2037878 -4.220892 -4.2211657 -4.2060437 -4.1904988 -4.1887264 -4.2002106][-4.167666 -4.1824536 -4.2030711 -4.2204471 -4.219564 -4.2002096 -4.1717205 -4.156846 -4.169333 -4.189971 -4.19342 -4.1829662 -4.1766267 -4.177999 -4.187922][-4.1553741 -4.1685629 -4.1833973 -4.1959286 -4.1871934 -4.1503148 -4.102159 -4.086071 -4.1205125 -4.161449 -4.1769938 -4.1769543 -4.1783867 -4.1774688 -4.1802015][-4.15523 -4.157196 -4.1549487 -4.1514754 -4.1304049 -4.0722742 -3.9994118 -3.984592 -4.0521569 -4.1232462 -4.1531472 -4.1572332 -4.1628046 -4.1601615 -4.1575918][-4.1352797 -4.1204042 -4.0972528 -4.0789776 -4.0490942 -3.9713728 -3.876502 -3.8747573 -3.9828794 -4.0804033 -4.1245151 -4.1350422 -4.1404371 -4.1305342 -4.12366][-4.1043043 -4.0802326 -4.0544171 -4.0426354 -4.0283041 -3.9757974 -3.9077439 -3.9218974 -4.0181336 -4.0990543 -4.1376543 -4.1526723 -4.1551704 -4.1398377 -4.1278591][-4.1154523 -4.1006341 -4.0853128 -4.0837989 -4.088254 -4.0660186 -4.031765 -4.0463362 -4.1034894 -4.1534786 -4.1809564 -4.1962519 -4.1963649 -4.1814733 -4.1676073][-4.151844 -4.1442175 -4.1344485 -4.1351342 -4.1434278 -4.1344461 -4.1155777 -4.1224027 -4.1555505 -4.1846256 -4.2024527 -4.2185292 -4.2210469 -4.2083311 -4.1951165][-4.1771832 -4.1731462 -4.164567 -4.1641979 -4.1741934 -4.1745243 -4.1662507 -4.1719222 -4.1924696 -4.2080078 -4.2175779 -4.22993 -4.2318277 -4.21682 -4.1999512][-4.2148752 -4.2146635 -4.2073536 -4.2045527 -4.2148609 -4.2232294 -4.2238474 -4.229651 -4.2404556 -4.24634 -4.251327 -4.2583609 -4.2527924 -4.2280431 -4.205637][-4.2646565 -4.2669921 -4.2622809 -4.2591887 -4.2656364 -4.2738037 -4.2754607 -4.2755504 -4.2755427 -4.2708826 -4.2673311 -4.2648182 -4.2501855 -4.2222791 -4.2043624][-4.2939 -4.2970352 -4.292882 -4.2881021 -4.290431 -4.2958131 -4.296361 -4.2909093 -4.2802997 -4.2661934 -4.2553625 -4.2460723 -4.2287273 -4.2071981 -4.19903]]...]
INFO - root - 2017-12-08 01:53:26.904162: step 34410, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 1.688 sec/batch; 61h:49m:00s remains)
INFO - root - 2017-12-08 01:53:42.910764: step 34420, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 1.607 sec/batch; 58h:50m:34s remains)
INFO - root - 2017-12-08 01:53:59.422678: step 34430, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 1.673 sec/batch; 61h:14m:48s remains)
INFO - root - 2017-12-08 01:54:15.670437: step 34440, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.583 sec/batch; 57h:58m:20s remains)
INFO - root - 2017-12-08 01:54:32.074577: step 34450, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 1.734 sec/batch; 63h:29m:09s remains)
INFO - root - 2017-12-08 01:54:48.394973: step 34460, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 1.592 sec/batch; 58h:15m:58s remains)
INFO - root - 2017-12-08 01:55:04.596121: step 34470, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 1.583 sec/batch; 57h:57m:22s remains)
INFO - root - 2017-12-08 01:55:20.666169: step 34480, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 1.657 sec/batch; 60h:39m:06s remains)
INFO - root - 2017-12-08 01:55:36.960241: step 34490, loss = 2.07, batch loss = 2.02 (10.1 examples/sec; 1.592 sec/batch; 58h:16m:03s remains)
INFO - root - 2017-12-08 01:55:53.331534: step 34500, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 1.665 sec/batch; 60h:55m:18s remains)
2017-12-08 01:55:54.575935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3015924 -4.3034577 -4.3049035 -4.30591 -4.3042259 -4.3007569 -4.3002038 -4.296278 -4.2871618 -4.2753243 -4.2700105 -4.2749915 -4.2884068 -4.3054771 -4.3226533][-4.289916 -4.2908735 -4.2891726 -4.2842121 -4.2719908 -4.2602019 -4.2582407 -4.255003 -4.2440076 -4.2300754 -4.2253432 -4.233191 -4.2496877 -4.2736087 -4.2990856][-4.2699795 -4.2647815 -4.2562156 -4.2428894 -4.2193422 -4.1985445 -4.1965637 -4.1965947 -4.1882277 -4.1765976 -4.1758003 -4.1883917 -4.2070589 -4.2369995 -4.27087][-4.24079 -4.231595 -4.2200861 -4.2026286 -4.175396 -4.1507082 -4.1464453 -4.1458726 -4.1400256 -4.1314173 -4.1374664 -4.156601 -4.177516 -4.2109561 -4.24999][-4.2108221 -4.2001939 -4.1892729 -4.1713176 -4.1429515 -4.1162014 -4.10861 -4.1098204 -4.1125641 -4.1095262 -4.1199074 -4.1464739 -4.1733375 -4.2077227 -4.2450609][-4.182271 -4.1662951 -4.1542759 -4.1364374 -4.107028 -4.0823951 -4.0770621 -4.0881176 -4.1041374 -4.1106896 -4.1252346 -4.1588588 -4.1917152 -4.2247772 -4.2577167][-4.1494107 -4.1275988 -4.1136203 -4.095305 -4.0651846 -4.0462456 -4.0481396 -4.0711126 -4.0990734 -4.1150804 -4.1329131 -4.1703887 -4.2088175 -4.24265 -4.2727537][-4.1167674 -4.0893254 -4.0713916 -4.0480466 -4.0128684 -3.9956777 -4.0099921 -4.0484972 -4.0868521 -4.1125402 -4.1370416 -4.17584 -4.21884 -4.2572031 -4.2879586][-4.0989871 -4.0649495 -4.0397186 -4.0082784 -3.9684415 -3.9566913 -3.9845481 -4.0396137 -4.0879755 -4.1224074 -4.1527729 -4.19046 -4.2335539 -4.272912 -4.3008471][-4.1232653 -4.0908284 -4.0621047 -4.0293117 -3.9919765 -3.9848483 -4.0174379 -4.0733166 -4.1208081 -4.1545525 -4.1820173 -4.2123094 -4.2473526 -4.2819858 -4.3062077][-4.1720228 -4.1457853 -4.1218057 -4.0979671 -4.0728874 -4.0679622 -4.0969887 -4.14334 -4.1801014 -4.205205 -4.224082 -4.2425919 -4.2665944 -4.2920432 -4.310102][-4.2296863 -4.2129703 -4.1992631 -4.1878071 -4.1757793 -4.174901 -4.1962194 -4.2278728 -4.249476 -4.2619233 -4.2702627 -4.2791495 -4.291997 -4.3063383 -4.3173318][-4.2739615 -4.2670579 -4.2634153 -4.262558 -4.2598825 -4.2620339 -4.2755766 -4.2933655 -4.3023944 -4.3048759 -4.306005 -4.30887 -4.3140497 -4.3197641 -4.3248086][-4.3014936 -4.3012762 -4.3036914 -4.3082252 -4.3103766 -4.3121657 -4.3192053 -4.3277845 -4.32955 -4.3267145 -4.324131 -4.3239231 -4.3255057 -4.3272591 -4.3298221][-4.315341 -4.3178034 -4.3224 -4.3283191 -4.3318753 -4.3332658 -4.3360453 -4.3391943 -4.3378839 -4.3339515 -4.330904 -4.3298373 -4.330575 -4.3318539 -4.3339291]]...]
INFO - root - 2017-12-08 01:56:10.576441: step 34510, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 1.634 sec/batch; 59h:47m:51s remains)
INFO - root - 2017-12-08 01:56:26.802890: step 34520, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 1.575 sec/batch; 57h:38m:23s remains)
INFO - root - 2017-12-08 01:56:40.710767: step 34530, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.953 sec/batch; 34h:52m:07s remains)
