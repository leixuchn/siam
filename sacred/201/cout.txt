INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "201"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
aiaiaiaiaiaiaiiaaiia [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/offset1/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/offset2/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b1/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b2/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>]
2017-12-10 09:02:01.312388: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 09:02:01.312510: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 09:02:01.312516: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 09:02:01.312596: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 09:02:01.312748: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 09:02:13.011889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 6.38GiB
2017-12-10 09:02:13.011921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 09:02:13.011928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 09:02:13.011935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>]
kkkkkkkkkkkkkkkkkkkkkkk [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/weights/Momentum:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/BatchNorm/beta/Momentum:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/BatchNorm/gamma/Momentum:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/weights/Momentum:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/beta/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/gamma/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/weights/Momentum:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/beta/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/gamma/Momentum:0' INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/weights/Momentum:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/BatchNorm/beta/Momentum:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/BatchNorm/gamma/Momentum:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/weights/Momentum:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/beta/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/gamma/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/weights/Momentum:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/beta/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/gamma/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/offset1/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/offset2/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b1/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b2/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/detection/biases/Momentum:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv1/weights/Momentum:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv1/BatchNorm/beta/Momentum:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv1/BatchNorm/gamma/Momentum:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv2/b1/weights/Momentum:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv2/b1/BatchNorm/beta/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv2/b1/BatchNorm/gamma/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv2/b2/weights/Momentum:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv2/b2/BatchNorm/beta/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv2/b2/BatchNorm/gamma/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv3/weights/Momentum:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv3/BatchNorm/beta/Momentum:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv3/BatchNorm/gamma/Momentum:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv4/b1/weights/Momentum:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv4/b1/BatchNorm/beta/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv4/b1/BatchNorm/gamma/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv4/b2/weights/Momentum:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv4/b2/BatchNorm/beta/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/siamese_fc/conv4/b2/BatchNorm/gamma/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_1/detection/biases/Momentum:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_2/siamese_fc/conv5/def/offset1/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_2/siamese_fc/conv5/def/offset2/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_2/siamese_fc/conv5/def/b1/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss_2/siamese_fc/conv5/def/b2/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>]
INFO - root - 2017-12-10 09:02:34.301034: step 0, loss = 0.75, batch loss = 0.69 (0.5 examples/sec; 15.899 sec/batch; 1468h:29m:17s remains)
2017-12-10 09:02:35.499523: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00012974557 0.00013662338 0.00014208717 0.00015759436 0.00018258444 0.00021623228 0.00025435185 0.00029518167 0.0003212452 0.00031967543 0.00029318788 0.00025184458 0.00020861608 0.00016836499 0.00014120928][0.00013442902 0.00014695982 0.00016447432 0.00019637273 0.00024168975 0.0002966295 0.00035725036 0.00041716313 0.00044727142 0.00043045447 0.00038617 0.00032250103 0.00025875564 0.00020439578 0.00016299821][0.00014534649 0.00016144972 0.0001895641 0.00024390245 0.00031683981 0.0004031476 0.00049826817 0.00059067085 0.00063389388 0.00059769239 0.00052335585 0.00042232085 0.00032736032 0.00025100674 0.00019178401][0.00016213185 0.00017457666 0.00020624073 0.00028416901 0.00038991304 0.000517165 0.00066003879 0.00080405787 0.00086667313 0.0008007093 0.00067834079 0.00052532129 0.00039104262 0.00028956824 0.000215174][0.00018790302 0.00019089408 0.00022531493 0.00032861833 0.00047235467 0.00065143529 0.0008597725 0.0010769162 0.0011462594 0.0010251513 0.00083180325 0.00061318709 0.00043390869 0.00030952494 0.00022437272][0.00021961991 0.00021855481 0.00026329185 0.00039264592 0.00057444925 0.00081145688 0.0011023262 0.0013995164 0.0014330469 0.0012114958 0.00093395245 0.00066287059 0.00045404289 0.00031526465 0.0002229102][0.00025566595 0.00026147193 0.00031824995 0.00046086486 0.00066840509 0.00094304548 0.0012904725 0.0016106709 0.0015386003 0.0012195276 0.00090097956 0.00063056464 0.00042968744 0.00029619827 0.00020791752][0.00029228034 0.00031316932 0.00038052935 0.00051844696 0.00071072212 0.00095849723 0.0012567049 0.0014740701 0.0013165004 0.0010079857 0.00073529297 0.00052589219 0.000368958 0.00026054925 0.00018585478][0.00033040496 0.00036555444 0.00042628427 0.00053432764 0.00067278196 0.00084349071 0.0010292378 0.0011224585 0.00096404651 0.00073760789 0.00055106566 0.00041784992 0.00030975617 0.00022700087 0.00016749698][0.00036472394 0.0004053814 0.000453625 0.00052932312 0.0006148399 0.00070720125 0.00079559517 0.00081601192 0.000691575 0.00054763141 0.00043537756 0.00036019136 0.00028558198 0.00021459298 0.00016058842][0.00038583513 0.0004232541 0.00046127109 0.00050986867 0.00055979885 0.00059527589 0.00062389427 0.0006148353 0.00053092686 0.00044735262 0.00038229043 0.00033533075 0.00027381905 0.00020534296 0.0001553099][0.00038849647 0.00041339372 0.00043980137 0.00046746567 0.0004908194 0.00048799807 0.0004799843 0.00046338892 0.00041573602 0.00037698419 0.00034076269 0.00030393459 0.00024682167 0.00018281747 0.00014375862][0.00036647977 0.00037382133 0.00038637078 0.00039434416 0.00039609018 0.00037612749 0.00036441319 0.00036296184 0.00035273246 0.00034762741 0.00032533443 0.0002887896 0.00022840568 0.00016738816 0.0001380419][0.00032810253 0.00032195004 0.00032023128 0.0003152634 0.00031142181 0.00029876578 0.00030357754 0.00033009442 0.00035124685 0.00036314185 0.00034471776 0.00030165256 0.00023133538 0.00016972593 0.00014341193][0.0002994753 0.000287465 0.00027464912 0.0002631027 0.00026328387 0.00027043768 0.00030301797 0.00035463882 0.00038973018 0.00040302423 0.00038130954 0.00032876691 0.00024784455 0.00018462137 0.00015494005]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 09:02:46.152442: step 10, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 74h:02m:44s remains)
INFO - root - 2017-12-10 09:02:54.146620: step 20, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 74h:38m:03s remains)
INFO - root - 2017-12-10 09:03:02.122964: step 30, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 71h:48m:55s remains)
INFO - root - 2017-12-10 09:03:09.986973: step 40, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.772 sec/batch; 71h:16m:15s remains)
INFO - root - 2017-12-10 09:03:17.815429: step 50, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 73h:42m:15s remains)
INFO - root - 2017-12-10 09:03:25.593048: step 60, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.773 sec/batch; 71h:20m:57s remains)
INFO - root - 2017-12-10 09:03:33.437474: step 70, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.768 sec/batch; 70h:56m:50s remains)
INFO - root - 2017-12-10 09:03:41.419034: step 80, loss = 0.75, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 79h:27m:46s remains)
INFO - root - 2017-12-10 09:03:49.362911: step 90, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.796 sec/batch; 73h:29m:37s remains)
INFO - root - 2017-12-10 09:03:57.313669: step 100, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.792 sec/batch; 73h:09m:40s remains)
2017-12-10 09:03:58.106379: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00036875452 0.000367769 0.00037361629 0.00039067163 0.00041509981 0.00044471837 0.00047779421 0.00051401346 0.00052287185 0.00049239566 0.00044399948 0.00038843919 0.00035385526 0.00032962579 0.00033286837][0.00035108443 0.00035769414 0.00037390072 0.00040177931 0.00044100638 0.00047739327 0.00051056768 0.00054863264 0.00056612195 0.00054110814 0.00048022738 0.00039869404 0.00035278156 0.00033133524 0.0003312123][0.00033426328 0.00035764984 0.00039154102 0.00043186833 0.00047502702 0.00050627324 0.000524144 0.00054983771 0.00057368522 0.00056487467 0.00050781621 0.00041696973 0.00036442318 0.00034022445 0.00032722446][0.00033829 0.00038131568 0.00043408966 0.00048629672 0.00053460087 0.00055621268 0.000550294 0.00054814771 0.00055708532 0.0005518659 0.00050663739 0.00042223945 0.00037602373 0.00035432054 0.00033708487][0.00037191971 0.00043338229 0.00050027523 0.0005672354 0.00062573917 0.00064142613 0.00060579565 0.00056713272 0.00054397574 0.00051794626 0.00047343512 0.00040331637 0.00036838054 0.00035448969 0.00034339417][0.00042787738 0.00050236168 0.00058576 0.0006734549 0.00075254228 0.00076736382 0.00069693261 0.00060254254 0.0005285212 0.00046781558 0.0004226866 0.00037883362 0.00036170418 0.00035460788 0.00034790245][0.00047190703 0.00054729014 0.000640207 0.0007408947 0.000829991 0.00084964687 0.00076499546 0.00063181308 0.00052151538 0.00044130883 0.00039692028 0.00037525379 0.00037485725 0.00037285622 0.00036228806][0.00047019418 0.00053398614 0.00061878766 0.00071941339 0.00080680958 0.00083014788 0.00075785996 0.00062764395 0.000514339 0.00042826904 0.00038180992 0.00036788709 0.00037665883 0.00038138634 0.00037265962][0.00043360124 0.00048328168 0.00054840604 0.0006237296 0.00069385982 0.00071364292 0.00066937512 0.00057280436 0.00048464915 0.00041293635 0.00036952566 0.00035926554 0.00036958972 0.000380285 0.00038275259][0.00036998565 0.00041188838 0.00045969739 0.00051039393 0.00055832055 0.00057481148 0.00055685441 0.00049832492 0.00044567979 0.00039919175 0.00036116506 0.00034964914 0.00035747475 0.00036937764 0.00038139813][0.00028924443 0.0003298579 0.00037115454 0.00040857084 0.0004470901 0.00046641464 0.00046138847 0.00042643549 0.00039904343 0.00037188129 0.00033816334 0.0003237855 0.00033433043 0.00035489941 0.00037594844][0.0002377206 0.00027439924 0.0003108677 0.00033860208 0.000370169 0.000392291 0.00039273771 0.00037259163 0.00036161291 0.00034794403 0.00031904943 0.00030383992 0.00031738493 0.00034898415 0.00037746178][0.00022297783 0.0002532443 0.00028079032 0.00030067356 0.00032581695 0.00034824002 0.00035281127 0.00034148479 0.0003348926 0.000325833 0.00029854692 0.000285246 0.00030190544 0.00033974083 0.00037475978][0.00020769412 0.00023273804 0.00025485837 0.00026891788 0.00028485569 0.00029855443 0.00030216904 0.00029745567 0.00029236948 0.00028110936 0.00026144821 0.00025883812 0.00028236327 0.00032491566 0.00036395155][0.00018398157 0.00020082218 0.00021518808 0.0002239796 0.00023003112 0.00023287674 0.00023705515 0.00023902739 0.00023753761 0.00023050424 0.00022394891 0.00023611159 0.00026562222 0.00030963137 0.0003521319]]...]
INFO - root - 2017-12-10 09:04:06.074588: step 110, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 74h:32m:08s remains)
INFO - root - 2017-12-10 09:04:13.967303: step 120, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 75h:44m:48s remains)
INFO - root - 2017-12-10 09:04:21.722942: step 130, loss = 0.75, batch loss = 0.69 (11.5 examples/sec; 0.693 sec/batch; 63h:57m:41s remains)
INFO - root - 2017-12-10 09:04:29.515123: step 140, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 75h:28m:24s remains)
INFO - root - 2017-12-10 09:04:37.431876: step 150, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 74h:55m:55s remains)
INFO - root - 2017-12-10 09:04:45.309901: step 160, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.769 sec/batch; 70h:57m:01s remains)
INFO - root - 2017-12-10 09:04:53.293450: step 170, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.791 sec/batch; 73h:00m:19s remains)
INFO - root - 2017-12-10 09:05:01.302195: step 180, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.774 sec/batch; 71h:27m:34s remains)
INFO - root - 2017-12-10 09:05:09.180024: step 190, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.790 sec/batch; 72h:56m:50s remains)
INFO - root - 2017-12-10 09:05:17.154447: step 200, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.766 sec/batch; 70h:43m:38s remains)
2017-12-10 09:05:17.964436: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00043998071 0.00047542682 0.0004781977 0.00047742028 0.00047796642 0.00048145556 0.00048018989 0.00048180224 0.00048257454 0.00047656562 0.00045969561 0.0004428814 0.00044048706 0.00044083028 0.00043698528][0.00049326062 0.00052941241 0.00054146867 0.0005458942 0.00054917939 0.00055612938 0.00055359863 0.00055083621 0.00054656639 0.00053930772 0.0005193433 0.00049883785 0.00049889786 0.00050990458 0.00051077543][0.0005478104 0.00058157829 0.00059307716 0.00060773781 0.00062413112 0.00063531869 0.00062916282 0.0006146768 0.00060197193 0.00058446656 0.00056582369 0.00055052643 0.00055758137 0.00057839707 0.00058298366][0.0005887461 0.0006250217 0.00063538004 0.0006571628 0.00068382337 0.00070428697 0.00070159178 0.00067634217 0.00064788177 0.00061151508 0.00058150641 0.00057368912 0.00059526722 0.0006262266 0.00063842919][0.00060737459 0.00064296304 0.00065531267 0.00068826153 0.00073402 0.00077767315 0.00079848472 0.00076380192 0.00070331775 0.00063754356 0.00059235876 0.000583812 0.00060938339 0.00063856872 0.0006493089][0.00062938774 0.00066954346 0.00068402308 0.00071915676 0.00078503147 0.00087523519 0.00094715564 0.00090484432 0.00079528382 0.00068867247 0.0006235349 0.00061135914 0.00064116635 0.00066761917 0.00066502107][0.00063941366 0.00069208053 0.00070583762 0.00074359513 0.00084023591 0.00098808436 0.0011196571 0.0010750737 0.00090853538 0.00075230317 0.00066653168 0.00065981119 0.0006987956 0.00071958866 0.00069850369][0.0006578695 0.000721545 0.00073397928 0.00076884427 0.00087297458 0.0010210079 0.0011620417 0.0011345022 0.00096344016 0.00079698325 0.00069676829 0.00068532344 0.00072815351 0.00074361631 0.00071864872][0.00064741325 0.00071565964 0.0007296215 0.00076014933 0.00083991606 0.00093409541 0.0010272427 0.0010298843 0.00092161278 0.00079516415 0.00070480455 0.00068557286 0.00071791187 0.00073432986 0.00072358747][0.00063626852 0.00071050948 0.00072732719 0.00074888219 0.00079907116 0.00084767264 0.00090243714 0.0009205399 0.00087076554 0.00079153996 0.00072770449 0.00070422457 0.00071951881 0.00073156407 0.00073243049][0.00062992948 0.00069614832 0.00070815522 0.00072282139 0.00075030018 0.00078190304 0.00081966969 0.00084472657 0.00082242384 0.00077881373 0.00073556631 0.00071499927 0.00072186295 0.00072808919 0.000719791][0.00061944692 0.00067393837 0.00068373914 0.00068999367 0.000700628 0.00072704296 0.0007629352 0.00079342729 0.0007810854 0.00074214081 0.00071425847 0.00070096308 0.00069862197 0.00069938443 0.00068456045][0.00059103162 0.00064131088 0.00065290771 0.000655267 0.00065756525 0.00067805068 0.00070541649 0.00072864216 0.00072902412 0.000705159 0.00067976571 0.00066650886 0.00065612094 0.00064315816 0.00062040723][0.00054998131 0.00058832427 0.00059081794 0.00058974675 0.00059160742 0.00060537778 0.000627747 0.00064916268 0.00065395719 0.00064120127 0.00062388909 0.00061005738 0.00059734646 0.00058186526 0.00055858714][0.00047772159 0.00050916441 0.00050936878 0.00050488417 0.00050708576 0.00051354029 0.00053289463 0.00055325386 0.00056034396 0.00055218011 0.00053597486 0.00052114914 0.00050961663 0.00049521809 0.00047533924]]...]
INFO - root - 2017-12-10 09:05:25.880899: step 210, loss = 0.75, batch loss = 0.69 (11.6 examples/sec; 0.689 sec/batch; 63h:36m:34s remains)
INFO - root - 2017-12-10 09:05:33.521049: step 220, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.779 sec/batch; 71h:52m:35s remains)
INFO - root - 2017-12-10 09:05:41.486875: step 230, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 73h:56m:41s remains)
INFO - root - 2017-12-10 09:05:49.410802: step 240, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.775 sec/batch; 71h:30m:31s remains)
INFO - root - 2017-12-10 09:05:57.304569: step 250, loss = 0.75, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 76h:38m:47s remains)
INFO - root - 2017-12-10 09:06:05.228927: step 260, loss = 0.75, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 76h:35m:12s remains)
INFO - root - 2017-12-10 09:06:13.053155: step 270, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 71h:42m:11s remains)
INFO - root - 2017-12-10 09:06:20.927257: step 280, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.770 sec/batch; 71h:04m:44s remains)
INFO - root - 2017-12-10 09:06:28.930000: step 290, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 75h:11m:35s remains)
INFO - root - 2017-12-10 09:06:36.742486: step 300, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.800 sec/batch; 73h:49m:34s remains)
2017-12-10 09:06:37.693175: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0006572323 0.0006814645 0.00067743822 0.000656508 0.00060680846 0.0005439425 0.00050367 0.00049891038 0.00052465807 0.00055163715 0.00055985159 0.00054556964 0.000537002 0.00054380059 0.00055083731][0.00057653943 0.00061446475 0.00064688217 0.00066854776 0.000649399 0.00058741978 0.00052863895 0.00050477561 0.00050780061 0.00051564956 0.00051478826 0.00050699461 0.00051609357 0.000541923 0.00056751049][0.00050408649 0.0005558292 0.00062202994 0.00068244658 0.00069593708 0.0006510301 0.00058530964 0.00054008845 0.00051982526 0.00051441049 0.00051573681 0.00052336074 0.0005471168 0.00058259908 0.00060967449][0.00048658546 0.0005476877 0.0006337543 0.00072280772 0.00076669909 0.00074029487 0.00067428418 0.00061605743 0.00058320374 0.00057578861 0.00058397715 0.00061302469 0.00065406103 0.00068978925 0.000701964][0.00051829091 0.00059120666 0.00068255415 0.00078128354 0.00084168342 0.00083291146 0.00077816535 0.00072371413 0.00069331372 0.00069302565 0.0007095355 0.00075505726 0.00080504111 0.00082996086 0.00081363239][0.00055760017 0.00064361218 0.00073773303 0.00082953123 0.00089022046 0.00089849718 0.00086560805 0.00082968519 0.00081402669 0.00082563364 0.00085658388 0.00091173837 0.00095895666 0.00096703728 0.0009231529][0.00059454684 0.00068999315 0.00078629411 0.00086985988 0.00092559645 0.0009503843 0.00094060961 0.00092002063 0.00091591227 0.00094050256 0.00098463008 0.001036464 0.0010783421 0.0010765481 0.0010201026][0.00061450229 0.00070643483 0.00079488737 0.00086692156 0.00091509864 0.000958103 0.00097593031 0.00097518024 0.00097852841 0.0010045314 0.0010378585 0.0010754985 0.0011090733 0.0011047567 0.0010515382][0.00061422843 0.00069430342 0.00075892417 0.00081030425 0.00084692711 0.00089873018 0.00094176881 0.00096403569 0.00097383285 0.00099336216 0.001007183 0.0010234856 0.0010393491 0.0010303549 0.00098538876][0.000600934 0.00065750588 0.0006976595 0.00073309912 0.00076095003 0.00081322581 0.00087357778 0.00091648864 0.00093022053 0.00094216934 0.00094113324 0.0009368523 0.00093219453 0.00091558543 0.00088128349][0.00058367517 0.00061523705 0.00063111843 0.00065271818 0.00067477039 0.00073025172 0.00079755456 0.00085068104 0.0008751431 0.00088976946 0.00088012026 0.00085199345 0.00083045627 0.0008089738 0.00078616512][0.000585897 0.00059719471 0.000591777 0.00059166091 0.00060464919 0.0006583964 0.00072086009 0.000773119 0.00080699887 0.00083287852 0.00082927308 0.00079133932 0.00076161441 0.00073917775 0.000722371][0.00058011041 0.0005842059 0.00057115126 0.00055705133 0.00056106551 0.00059741427 0.0006446488 0.00069037382 0.00073437812 0.000771554 0.0007777292 0.00075067044 0.00071992021 0.00069189636 0.000673656][0.00054773013 0.00056400063 0.00055779604 0.00054474047 0.00054136588 0.00055471825 0.00057883293 0.00061181159 0.00066147163 0.00070776005 0.00072744529 0.00072068122 0.00069964462 0.0006738095 0.00065345614][0.00051836862 0.00054937258 0.00055508508 0.00055278087 0.00054882513 0.00054902327 0.00055177149 0.00057255087 0.00062143424 0.00067410211 0.00070559554 0.00070809777 0.00070016494 0.00068560295 0.00066793559]]...]
INFO - root - 2017-12-10 09:06:45.406853: step 310, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.787 sec/batch; 72h:39m:17s remains)
INFO - root - 2017-12-10 09:06:53.364120: step 320, loss = 0.75, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 76h:03m:53s remains)
INFO - root - 2017-12-10 09:07:01.309278: step 330, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 70h:30m:38s remains)
INFO - root - 2017-12-10 09:07:09.439823: step 340, loss = 0.75, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 75h:43m:45s remains)
INFO - root - 2017-12-10 09:07:17.487873: step 350, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.767 sec/batch; 70h:45m:48s remains)
INFO - root - 2017-12-10 09:07:25.415541: step 360, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.809 sec/batch; 74h:40m:37s remains)
INFO - root - 2017-12-10 09:07:33.166924: step 370, loss = 0.75, batch loss = 0.69 (11.2 examples/sec; 0.717 sec/batch; 66h:07m:12s remains)
INFO - root - 2017-12-10 09:07:41.040068: step 380, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.761 sec/batch; 70h:12m:11s remains)
INFO - root - 2017-12-10 09:07:48.980155: step 390, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.783 sec/batch; 72h:13m:13s remains)
INFO - root - 2017-12-10 09:07:56.728323: step 400, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.788 sec/batch; 72h:40m:03s remains)
2017-12-10 09:07:57.567957: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0005991456 0.00062663341 0.00062762632 0.00060829811 0.00057548383 0.00055461028 0.00056319346 0.00059547112 0.00062526215 0.00066535163 0.00069621444 0.00071390049 0.00069538003 0.0006527027 0.00059920247][0.0006649805 0.00070461008 0.00071498431 0.00069953932 0.00066839164 0.000650916 0.00066537 0.00070052315 0.00073219195 0.00078635028 0.00084191305 0.00088394753 0.00087717961 0.00082820392 0.00075010356][0.00069386803 0.00075825054 0.00079147943 0.00079370372 0.00077366119 0.0007728106 0.00079826044 0.00082846609 0.0008549823 0.00090880611 0.0009706156 0.0010267439 0.0010242304 0.00097343669 0.00087643816][0.00071223179 0.00081318984 0.00087992713 0.00090652565 0.00090475765 0.00091759372 0.00094718771 0.000973658 0.00099065853 0.0010329569 0.0010814562 0.0011301525 0.0011155234 0.0010550439 0.00094368559][0.00072188512 0.0008668463 0.00097675086 0.0010369206 0.0010544505 0.001079388 0.001108514 0.0011295076 0.0011297683 0.0011506432 0.0011800363 0.0012075272 0.0011730986 0.0010975013 0.00097591581][0.00071621209 0.00088779582 0.0010384474 0.001137465 0.0011801415 0.0012237265 0.0012655818 0.0012884006 0.0012757328 0.0012789384 0.0012907569 0.0012896466 0.0012320965 0.0011356424 0.0010043703][0.00074721116 0.00091088278 0.0010599684 0.0011592096 0.0012018175 0.0012630678 0.0013413085 0.0014004065 0.0014050816 0.0014053435 0.0014098111 0.0013830489 0.001298354 0.0011818436 0.0010389433][0.00086191314 0.0010014358 0.0011099351 0.0011581373 0.0011572874 0.001202781 0.0013031622 0.0014145924 0.0014708446 0.0015020903 0.0015098486 0.0014691525 0.0013688565 0.0012392073 0.0010833234][0.0010462537 0.0011730836 0.001233744 0.0012161779 0.0011496138 0.0011416588 0.001224369 0.0013512323 0.0014503011 0.001521993 0.0015505526 0.0015102007 0.0014077356 0.0012764803 0.0011170917][0.0012029217 0.0013197894 0.0013469453 0.0012792185 0.0011621212 0.0011084144 0.0011526503 0.0012564317 0.0013598703 0.0014517064 0.0015027825 0.0014786428 0.0013841542 0.0012543838 0.001097325][0.0012405101 0.0013519143 0.001367607 0.0012835196 0.0011488321 0.0010759514 0.0010915974 0.0011651092 0.0012475584 0.0013286612 0.0013799656 0.0013683434 0.0012916016 0.0011716186 0.0010276899][0.0011372529 0.0012416795 0.0012600854 0.00119377 0.0010732266 0.0010042689 0.0010057021 0.0010554671 0.0011170757 0.0011717435 0.0012042666 0.0011890345 0.0011287297 0.0010362261 0.00092279579][0.0009213161 0.0010105867 0.0010408687 0.0010159808 0.00094535906 0.00089692109 0.00089108665 0.00091691437 0.00095546653 0.00099380384 0.0010157621 0.00099301722 0.00094285677 0.00087925274 0.00080075423][0.00066847354 0.00073705008 0.00077344105 0.00078633812 0.00076526857 0.00075474585 0.00075476611 0.00076762296 0.00078738393 0.00081182283 0.00082775217 0.00081080204 0.00077224971 0.00072597043 0.00067395723][0.00046270777 0.00051659776 0.0005505064 0.00057687034 0.00058996881 0.00060974254 0.00062403874 0.00063600973 0.00064918649 0.00066228485 0.00067038171 0.00065361959 0.00062873919 0.00059743715 0.00056422711]]...]
INFO - root - 2017-12-10 09:08:05.484990: step 410, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.770 sec/batch; 70h:59m:35s remains)
INFO - root - 2017-12-10 09:08:13.463574: step 420, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 74h:58m:54s remains)
INFO - root - 2017-12-10 09:08:21.449837: step 430, loss = 0.75, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 76h:52m:19s remains)
INFO - root - 2017-12-10 09:08:29.456880: step 440, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.774 sec/batch; 71h:25m:53s remains)
INFO - root - 2017-12-10 09:08:37.341802: step 450, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 74h:03m:53s remains)
INFO - root - 2017-12-10 09:08:45.314387: step 460, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 75h:20m:48s remains)
INFO - root - 2017-12-10 09:08:53.335358: step 470, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.775 sec/batch; 71h:27m:24s remains)
INFO - root - 2017-12-10 09:09:01.233848: step 480, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 75h:12m:36s remains)
INFO - root - 2017-12-10 09:09:09.114736: step 490, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.780 sec/batch; 71h:56m:08s remains)
INFO - root - 2017-12-10 09:09:17.069454: step 500, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.796 sec/batch; 73h:26m:07s remains)
2017-12-10 09:09:18.000892: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00050142495 0.00053836807 0.00055167248 0.00055477512 0.00054978207 0.00053965393 0.00052851491 0.00052366604 0.00052709272 0.00054032978 0.00056142395 0.00058790977 0.00062055647 0.00065644225 0.00069581234][0.00050611666 0.0005563554 0.0005792515 0.000588595 0.000585925 0.00057330722 0.00055640354 0.0005440978 0.00054130662 0.00055266509 0.00057392765 0.00060519384 0.00064136233 0.00068065146 0.00072350632][0.00050335593 0.00056738179 0.00060678174 0.00063058082 0.0006346922 0.00061951746 0.00059252657 0.000564343 0.0005453967 0.00054483366 0.00055823626 0.00058355019 0.00061365939 0.00064870412 0.00068955374][0.00053537014 0.00062177738 0.00068770343 0.00073423557 0.00075066305 0.00073264842 0.00068946107 0.00063575193 0.00058945676 0.00056728208 0.00056407746 0.00057355408 0.00059065 0.00061316852 0.000642898][0.00059869164 0.00071316335 0.00081537245 0.00089802005 0.00093946769 0.00092774216 0.00087039237 0.00078663009 0.00070043246 0.00064045872 0.00060718658 0.00059528189 0.00059683342 0.00061026186 0.00063007214][0.00065766246 0.00079947122 0.00094071234 0.0010696364 0.0011551135 0.0011705831 0.0011153212 0.0010100564 0.00088414486 0.00077603309 0.00070635095 0.00067132548 0.000657284 0.000661585 0.00067168556][0.00067551172 0.00083107274 0.0010008925 0.001171192 0.0013071757 0.0013714334 0.0013455735 0.0012433698 0.0010951288 0.00095350656 0.00086252036 0.00081690791 0.00079680316 0.00079287955 0.00078694004][0.0006411642 0.00078870717 0.00096384413 0.0011572223 0.0013356715 0.0014563306 0.0014821434 0.0014091814 0.0012672332 0.0011171411 0.0010233034 0.00098378363 0.00097548525 0.00097508106 0.00095523574][0.00057025632 0.00069058395 0.00084222667 0.0010212283 0.0012014746 0.0013489476 0.0014248961 0.0014093322 0.0013141081 0.0011959169 0.0011280809 0.0011150842 0.0011338658 0.0011488259 0.0011262799][0.00049584854 0.00058144896 0.00069055596 0.00082813733 0.00097703864 0.0011122747 0.0012053455 0.0012345789 0.0012032815 0.0011486624 0.0011291495 0.0011528799 0.0012022177 0.0012410987 0.0012305818][0.00043880369 0.00049032009 0.00055682909 0.00064704166 0.00075135723 0.00085706572 0.0009446629 0.000995592 0.0010093418 0.0010073765 0.0010272 0.0010759211 0.0011458417 0.0012065981 0.0012228045][0.00042310945 0.00045211692 0.00048622148 0.00053540792 0.0005971861 0.00066859985 0.00073630229 0.000789615 0.00082435861 0.00084979209 0.00088478747 0.00093703315 0.0010086839 0.001076884 0.0011174722][0.00044067888 0.00046167339 0.00048015467 0.00050412497 0.00053521455 0.00057497894 0.00061902712 0.00066289591 0.0007013568 0.00073326065 0.00076537818 0.00080482574 0.00086226227 0.00092753972 0.000985108][0.0004642643 0.00048405657 0.00049619336 0.00050983683 0.00052490504 0.00054723013 0.00057733193 0.00061331253 0.00065038668 0.00067879917 0.00070157804 0.0007254894 0.00076890504 0.00082731515 0.00089755788][0.00048521551 0.00050854089 0.0005200801 0.00052968139 0.00053720345 0.00055200653 0.00057450525 0.00060445158 0.00064058171 0.00067024759 0.00068860012 0.00070276781 0.00073562068 0.00079014269 0.00086586183]]...]
INFO - root - 2017-12-10 09:09:25.923303: step 510, loss = 0.75, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 76h:12m:38s remains)
INFO - root - 2017-12-10 09:09:34.054409: step 520, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 73h:50m:23s remains)
INFO - root - 2017-12-10 09:09:41.877636: step 530, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.786 sec/batch; 72h:30m:47s remains)
INFO - root - 2017-12-10 09:09:49.720588: step 540, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 72h:06m:56s remains)
INFO - root - 2017-12-10 09:09:57.784571: step 550, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 74h:47m:27s remains)
INFO - root - 2017-12-10 09:10:05.738325: step 560, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 71h:38m:43s remains)
INFO - root - 2017-12-10 09:10:13.723606: step 570, loss = 0.75, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 75h:42m:04s remains)
INFO - root - 2017-12-10 09:10:21.502169: step 580, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 73h:53m:52s remains)
INFO - root - 2017-12-10 09:10:29.590942: step 590, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.790 sec/batch; 72h:50m:03s remains)
INFO - root - 2017-12-10 09:10:37.523730: step 600, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.794 sec/batch; 73h:13m:53s remains)
2017-12-10 09:10:38.353255: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0003430139 0.000428137 0.00050132565 0.00054371 0.00055467908 0.0005397246 0.00050613092 0.00047081895 0.00043982072 0.00041197223 0.00038578224 0.00035979942 0.00032920821 0.00029841324 0.00027506842][0.00049322931 0.00063049392 0.00074867706 0.000822547 0.00085125357 0.00083624833 0.00079353579 0.00073914294 0.0006864735 0.00063514628 0.00058565248 0.00053832523 0.00048131141 0.0004211567 0.00036779995][0.00067869417 0.00088776986 0.0010743313 0.0012050131 0.0012771738 0.0012825212 0.0012355244 0.0011598251 0.001072655 0.00097646058 0.00087691791 0.00078455941 0.00068789261 0.00058735017 0.00049331249][0.00088398159 0.0011959809 0.0014905458 0.0017182134 0.0018719333 0.0019273661 0.0018870622 0.0017763919 0.0016235798 0.0014497607 0.0012718084 0.0011045166 0.00094492442 0.00078812294 0.000641877][0.0010708354 0.0015013326 0.0019328551 0.0022911851 0.0025588381 0.0026989249 0.0026938068 0.0025505936 0.0023093524 0.0020267044 0.0017453864 0.0014797492 0.0012374483 0.0010073596 0.00079701055][0.001208019 0.0017479334 0.0023180367 0.0028173963 0.0032156045 0.0034634774 0.0035210832 0.0033712226 0.0030549818 0.00266933 0.0022755375 0.0018860252 0.0015340372 0.0012210207 0.00094895525][0.0012795618 0.0018892711 0.0025633064 0.0031859828 0.0037099712 0.004074249 0.0042129736 0.0040779947 0.0037098483 0.0032417905 0.0027505381 0.0022587471 0.0018120137 0.0014301357 0.0011074732][0.0012591968 0.0018808701 0.0025973374 0.0032887589 0.0039090971 0.0043876013 0.0046284986 0.00453991 0.0041656154 0.0036583731 0.0031042774 0.0025436459 0.0020279579 0.0015933247 0.0012344602][0.0011413129 0.0017165887 0.0023976981 0.0030769191 0.0037107822 0.0042497753 0.0045688506 0.0045514079 0.0042396882 0.0037869338 0.0032634833 0.0026975758 0.0021640302 0.0017067023 0.0013307459][0.00095250335 0.0014182549 0.0019864929 0.0025732932 0.0031370739 0.0036354291 0.0039637182 0.00402508 0.0038384825 0.0035178624 0.0031087042 0.0026208907 0.0021359564 0.0017109326 0.0013623221][0.00074391987 0.0010746442 0.0014853869 0.0019279859 0.0023624608 0.0027569868 0.0030533615 0.0031858748 0.0031332504 0.0029603597 0.0027011135 0.002349305 0.0019670222 0.0016156969 0.0013268789][0.00056114228 0.00077549793 0.0010383264 0.0013317121 0.0016286945 0.0019081946 0.002144505 0.0023073375 0.0023506738 0.0022996517 0.0021715588 0.0019646904 0.0017102745 0.0014516035 0.0012250422][0.00041753426 0.00054991071 0.00070590939 0.00088555744 0.0010740067 0.0012614905 0.0014358715 0.0015805643 0.0016711677 0.0016937118 0.0016550851 0.0015538974 0.0014102572 0.001238954 0.0010692686][0.00031737506 0.00040067435 0.00049478427 0.00060497248 0.00072497135 0.00084809744 0.00097113312 0.0010875567 0.0011796384 0.0012340468 0.0012460559 0.0012106865 0.0011361559 0.0010340052 0.0009138388][0.0002648638 0.00032504363 0.00038749029 0.00045848836 0.00053871918 0.00062434486 0.00071972609 0.00081599678 0.0008985824 0.00095727114 0.00098119606 0.000975874 0.00093500386 0.00086923363 0.00078789209]]...]
INFO - root - 2017-12-10 09:10:46.132512: step 610, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.789 sec/batch; 72h:41m:55s remains)
INFO - root - 2017-12-10 09:10:54.193965: step 620, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.767 sec/batch; 70h:40m:34s remains)
INFO - root - 2017-12-10 09:11:02.214522: step 630, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.800 sec/batch; 73h:46m:19s remains)
INFO - root - 2017-12-10 09:11:10.095716: step 640, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 73h:47m:58s remains)
INFO - root - 2017-12-10 09:11:17.978266: step 650, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.772 sec/batch; 71h:07m:50s remains)
INFO - root - 2017-12-10 09:11:25.754339: step 660, loss = 0.75, batch loss = 0.69 (13.2 examples/sec; 0.605 sec/batch; 55h:44m:56s remains)
INFO - root - 2017-12-10 09:11:33.663397: step 670, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.793 sec/batch; 73h:06m:30s remains)
INFO - root - 2017-12-10 09:11:41.660689: step 680, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.787 sec/batch; 72h:35m:00s remains)
INFO - root - 2017-12-10 09:11:49.493594: step 690, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.780 sec/batch; 71h:55m:25s remains)
INFO - root - 2017-12-10 09:11:57.574857: step 700, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 73h:50m:34s remains)
2017-12-10 09:11:58.387727: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.000943356 0.0009661254 0.00097543292 0.00098686665 0.0010030993 0.0010370652 0.0010844725 0.001138286 0.0011767596 0.0011962604 0.0011994767 0.0011835125 0.0011785657 0.0012131494 0.0012878333][0.00089861674 0.000932743 0.00095443253 0.00098253728 0.0010171315 0.0010711425 0.0011414029 0.0012173982 0.0012686122 0.0012925062 0.0012937378 0.001265954 0.0012320619 0.0012279471 0.0012673463][0.0008789571 0.00093091658 0.00097044115 0.00101489 0.0010666667 0.0011363075 0.0012185198 0.0013033977 0.0013550932 0.0013749736 0.0013671126 0.0013260563 0.0012640252 0.001223724 0.0012298211][0.00093059638 0.0010114653 0.0010787883 0.0011464321 0.0012152854 0.0012933846 0.0013752438 0.0014524857 0.001488974 0.001490837 0.0014630795 0.0014015174 0.0013139447 0.001243841 0.0012241715][0.0010327682 0.0011518822 0.0012523864 0.0013426826 0.00142782 0.0015161318 0.0015967565 0.0016598352 0.0016712012 0.00164076 0.0015788301 0.0014851639 0.0013710987 0.0012769762 0.0012409164][0.0011488517 0.0012969576 0.0014198283 0.0015257674 0.0016274438 0.001730924 0.0018227457 0.0018836771 0.0018753756 0.0018122185 0.0017084709 0.0015752262 0.0014325028 0.0013188681 0.00127567][0.0011914398 0.0013473971 0.001477023 0.0015940833 0.0017198134 0.0018511282 0.001967788 0.0020408088 0.0020255141 0.0019378468 0.0017994905 0.0016300386 0.0014606157 0.0013340231 0.0012901848][0.0011735733 0.0013138462 0.0014361065 0.0015596233 0.0017034406 0.0018584038 0.001996032 0.0020804929 0.0020677939 0.0019739731 0.0018221394 0.0016388035 0.0014614919 0.0013343632 0.0012951656][0.0011466001 0.001266408 0.001374652 0.0014963076 0.0016432356 0.0017992377 0.0019340194 0.0020157755 0.0020110165 0.0019284554 0.0017883183 0.001614187 0.0014476472 0.0013328376 0.0013052427][0.0011277581 0.0012381802 0.0013375612 0.0014566687 0.0015999703 0.0017424248 0.0018551542 0.0019182817 0.0019096773 0.0018365 0.0017136276 0.0015642594 0.0014305462 0.0013510173 0.0013532354][0.0011216606 0.0012362844 0.0013417506 0.0014669417 0.0016055721 0.0017323674 0.0018181633 0.001852738 0.0018279873 0.0017537145 0.0016450512 0.0015237465 0.0014287781 0.0013916255 0.0014288345][0.0011736682 0.0013011839 0.0014172102 0.0015443803 0.0016721445 0.001777455 0.0018355056 0.0018476713 0.0018126243 0.0017400278 0.0016483791 0.001553803 0.0014945732 0.0014946928 0.0015558816][0.0012461578 0.0013841148 0.0015048293 0.0016252816 0.0017328983 0.00181176 0.0018417063 0.001836625 0.0017973958 0.0017345529 0.0016644102 0.0016010368 0.0015795787 0.0016087848 0.0016826638][0.0012818845 0.0014219228 0.001538855 0.0016457165 0.0017300387 0.0017863134 0.0017974162 0.0017866776 0.0017572161 0.0017135022 0.0016678994 0.0016294458 0.0016373502 0.0016869574 0.0017647091][0.0012522406 0.0013861912 0.0014927312 0.0015875376 0.0016586997 0.0017010539 0.0017058696 0.0017007991 0.0016853765 0.0016646093 0.0016444314 0.0016274794 0.0016531344 0.0017083718 0.0017790735]]...]
INFO - root - 2017-12-10 09:12:06.301315: step 710, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 75h:14m:00s remains)
INFO - root - 2017-12-10 09:12:14.295435: step 720, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.792 sec/batch; 73h:01m:24s remains)
INFO - root - 2017-12-10 09:12:22.240563: step 730, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 75h:26m:12s remains)
INFO - root - 2017-12-10 09:12:30.122043: step 740, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 74h:19m:27s remains)
INFO - root - 2017-12-10 09:12:37.908066: step 750, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.805 sec/batch; 74h:12m:42s remains)
INFO - root - 2017-12-10 09:12:45.842326: step 760, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.794 sec/batch; 73h:11m:03s remains)
INFO - root - 2017-12-10 09:12:53.543327: step 770, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.796 sec/batch; 73h:21m:12s remains)
INFO - root - 2017-12-10 09:13:01.550601: step 780, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.805 sec/batch; 74h:12m:28s remains)
INFO - root - 2017-12-10 09:13:09.745925: step 790, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.785 sec/batch; 72h:20m:12s remains)
INFO - root - 2017-12-10 09:13:17.632722: step 800, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 71h:40m:20s remains)
2017-12-10 09:13:18.503342: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00099554192 0.0010949563 0.0012358201 0.0014478761 0.0017215693 0.0019869485 0.0021747418 0.0022520954 0.0022530889 0.0022352138 0.0022360208 0.0022936976 0.0024204822 0.0026101128 0.0028074607][0.0010356897 0.0011475555 0.0013006399 0.0015278271 0.0018160958 0.00210154 0.0023204356 0.002438829 0.002483998 0.0025156704 0.0025655723 0.0026739689 0.0028331967 0.0030344597 0.0032174194][0.0011585703 0.0013001536 0.001478521 0.0017225448 0.0020173788 0.0023094669 0.0025412079 0.0026825757 0.0027511334 0.0028048807 0.0028568739 0.0029521696 0.0030821618 0.0032410845 0.0033713125][0.0014425024 0.001633825 0.0018501792 0.0021119786 0.0024071287 0.0026943309 0.0029169573 0.0030572612 0.0031145168 0.0031468198 0.0031582569 0.0031938972 0.0032545635 0.0033350328 0.0033917672][0.0018265337 0.0020818654 0.00234135 0.0026265122 0.0029259173 0.0032092903 0.0034198419 0.0035378577 0.0035619403 0.003552797 0.0035139658 0.0034822158 0.0034654085 0.0034586354 0.0034313435][0.0022318054 0.0025508939 0.0028514939 0.0031594906 0.0034622571 0.0037392713 0.0039401031 0.004031939 0.004006966 0.0039339205 0.0038315102 0.0037301481 0.0036388449 0.0035522042 0.0034642788][0.0025372275 0.0028966586 0.0032328942 0.003574952 0.0039111571 0.0042176889 0.0044464217 0.0045384369 0.0044688312 0.0043224925 0.0041467515 0.003980678 0.0038355323 0.0037147542 0.0036171456][0.002649694 0.003011218 0.0033624235 0.0037367546 0.0041226922 0.0044876812 0.0047674337 0.0048786942 0.0047928542 0.0046066646 0.0043869573 0.0041926308 0.0040467246 0.0039510336 0.00390493][0.0025424969 0.0028707609 0.0032093457 0.0035824487 0.0039725914 0.0043389588 0.0046250508 0.0047488082 0.0046843863 0.0045197825 0.0043258397 0.0041764406 0.0040949835 0.0040827747 0.004128953][0.0022499114 0.0025236076 0.0028259493 0.0031725504 0.0035393273 0.0038847 0.0041606505 0.00429836 0.0042736339 0.0041550677 0.0040119924 0.0039335084 0.0039409632 0.0040234146 0.0041633588][0.0018270769 0.0020434868 0.0022974806 0.0026016503 0.0029277068 0.0032393893 0.0034946862 0.003640329 0.0036529077 0.003587472 0.0035023205 0.0034883465 0.0035678842 0.0037266719 0.0039406745][0.0013720088 0.0015353102 0.0017352503 0.0019872317 0.0022662268 0.0025385059 0.0027636753 0.00290724 0.0029514704 0.0029362661 0.0029097684 0.0029481037 0.0030775622 0.0032855698 0.0035343219][0.001015576 0.0011360981 0.0012855007 0.0014836817 0.0017132199 0.001940592 0.0021270576 0.0022599704 0.0023306333 0.0023629877 0.0023864568 0.0024575081 0.002604763 0.0028195179 0.003069893][0.00077751663 0.00087315752 0.00098357711 0.0011343921 0.0013087516 0.0014854722 0.0016316487 0.0017450349 0.001825738 0.00189243 0.001957437 0.0020578725 0.002210459 0.0024071389 0.0026347591][0.00064415118 0.00072477059 0.00080946955 0.00091898604 0.0010417795 0.0011703491 0.0012834588 0.0013828159 0.0014718294 0.00156209 0.0016564887 0.0017757739 0.0019312788 0.0021143553 0.0023271972]]...]
INFO - root - 2017-12-10 09:13:26.435898: step 810, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 73h:56m:54s remains)
INFO - root - 2017-12-10 09:13:34.498965: step 820, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.792 sec/batch; 72h:58m:38s remains)
INFO - root - 2017-12-10 09:13:42.505046: step 830, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.790 sec/batch; 72h:47m:20s remains)
INFO - root - 2017-12-10 09:13:50.196818: step 840, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.806 sec/batch; 74h:17m:47s remains)
INFO - root - 2017-12-10 09:13:57.892997: step 850, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 75h:29m:40s remains)
INFO - root - 2017-12-10 09:14:05.855462: step 860, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.792 sec/batch; 72h:59m:55s remains)
INFO - root - 2017-12-10 09:14:13.839558: step 870, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.784 sec/batch; 72h:12m:22s remains)
INFO - root - 2017-12-10 09:14:21.748874: step 880, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 73h:29m:48s remains)
INFO - root - 2017-12-10 09:14:29.655729: step 890, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.781 sec/batch; 71h:56m:44s remains)
INFO - root - 2017-12-10 09:14:37.578562: step 900, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 75h:27m:18s remains)
2017-12-10 09:14:38.440924: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0040742918 0.0044870637 0.0047894092 0.0050279284 0.0051753991 0.0051249703 0.0048771142 0.0046246848 0.0044972748 0.0044546672 0.0044518891 0.0044618524 0.0045524295 0.0047219805 0.0048686755][0.0048770285 0.00512445 0.0052241934 0.0053182403 0.0053806207 0.0052888542 0.0050013047 0.0046909871 0.004517029 0.0044613779 0.0044770613 0.0045243236 0.0046471693 0.0048285 0.0049585118][0.0055299369 0.005544893 0.0053567174 0.0052491077 0.0052228575 0.0051347692 0.0048842975 0.0046111052 0.0044480804 0.0043856017 0.0044033332 0.0044863275 0.0046429248 0.0048267934 0.0049421755][0.0058959257 0.0057231756 0.0053136703 0.0050516361 0.004968368 0.0049262438 0.004789534 0.0046333955 0.0045340848 0.0044946461 0.0045189732 0.0046242517 0.0047684014 0.0049190712 0.0049988548][0.0056787422 0.0054702964 0.0050394186 0.0047949734 0.0047784243 0.0048734532 0.0049217511 0.0049554016 0.0049658665 0.0049638529 0.0049853148 0.0050660479 0.0051486762 0.0052306615 0.0052453736][0.0050226622 0.0048952354 0.0046387184 0.00457802 0.0047455868 0.005049618 0.0053263581 0.0055655376 0.0056708916 0.0056696627 0.0056313984 0.0056326492 0.0056321062 0.0056412024 0.0055842442][0.0042299828 0.0042633992 0.0042882836 0.0045136148 0.0049334806 0.0054642041 0.0059440983 0.0063306326 0.0064580543 0.0063696159 0.0062150261 0.0060946192 0.0059874365 0.0059173424 0.0058194371][0.0034742476 0.0037187962 0.0040517221 0.0045558517 0.0052098734 0.005928685 0.0065566306 0.0070097116 0.0070646945 0.00682326 0.00654089 0.0063183852 0.0061340649 0.0060150465 0.0059202733][0.0028815411 0.0033105004 0.0038920972 0.0045853434 0.0053417124 0.0060852272 0.0066916011 0.0070620435 0.0070073279 0.0066852486 0.0063560703 0.006104216 0.0059171417 0.0058286781 0.0058068549][0.0025123279 0.0030213229 0.0036933988 0.0044264095 0.005142774 0.0057657473 0.006230202 0.0064614923 0.0063305474 0.0060306937 0.0057652658 0.0055873892 0.0054819188 0.0054595629 0.0055221962][0.0023189303 0.0027811769 0.0033740893 0.0039863652 0.0045557558 0.0050173467 0.0053261481 0.0054238359 0.0052668517 0.0050401785 0.0048780283 0.0048216041 0.0048516751 0.0049553653 0.00513701][0.0022415863 0.0025780739 0.0029931362 0.003422112 0.0038196137 0.0041301283 0.0043177013 0.004340142 0.0042072632 0.0040563578 0.0039914241 0.0040598009 0.0042357268 0.0044949879 0.0048120352][0.0022987155 0.002507312 0.0027316874 0.0029532977 0.0031650225 0.0033467971 0.0034427231 0.0034352238 0.0033622619 0.0032996696 0.0033366329 0.0035215192 0.0038400895 0.0042239833 0.0046384912][0.0024690416 0.0025797202 0.0026504847 0.0027133354 0.002773629 0.0028416321 0.002871237 0.0028661834 0.0028653026 0.0028954172 0.0030218922 0.0032766173 0.0036731488 0.0041215019 0.0045845322][0.0026971826 0.0027453029 0.0027194691 0.0026921716 0.0026703372 0.0026743661 0.0026694548 0.0026841334 0.0027369983 0.0028224692 0.002982195 0.0032606432 0.0036890337 0.0041614221 0.0046330579]]...]
INFO - root - 2017-12-10 09:14:46.535934: step 910, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.799 sec/batch; 73h:34m:41s remains)
INFO - root - 2017-12-10 09:14:54.448175: step 920, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.797 sec/batch; 73h:23m:21s remains)
INFO - root - 2017-12-10 09:15:01.953458: step 930, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 73h:30m:11s remains)
INFO - root - 2017-12-10 09:15:09.862486: step 940, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 71h:40m:35s remains)
INFO - root - 2017-12-10 09:15:17.797350: step 950, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.768 sec/batch; 70h:45m:23s remains)
INFO - root - 2017-12-10 09:15:25.754885: step 960, loss = 0.75, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 75h:46m:04s remains)
INFO - root - 2017-12-10 09:15:33.708171: step 970, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.786 sec/batch; 72h:25m:08s remains)
INFO - root - 2017-12-10 09:15:41.654934: step 980, loss = 0.75, batch loss = 0.69 (10.6 examples/sec; 0.753 sec/batch; 69h:23m:16s remains)
INFO - root - 2017-12-10 09:15:49.608835: step 990, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.812 sec/batch; 74h:48m:42s remains)
INFO - root - 2017-12-10 09:15:57.415392: step 1000, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.805 sec/batch; 74h:05m:22s remains)
2017-12-10 09:15:58.282993: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00445288 0.0044798357 0.0043528168 0.0041776779 0.0039535146 0.0038533255 0.0038904832 0.0040125279 0.0041112597 0.0041537588 0.004117487 0.0038886229 0.0034711263 0.0029199836 0.0023652173][0.0048000743 0.0047933497 0.0046478822 0.0044851028 0.0043002577 0.0042474577 0.0043098642 0.0044200672 0.0044493373 0.0043801488 0.0042234119 0.0038938094 0.0034107668 0.0028411443 0.0023048178][0.0047398396 0.0046911379 0.0045484025 0.0044536721 0.0043773786 0.0044388752 0.0045854114 0.0047196136 0.0047087739 0.0045431675 0.0042682337 0.0038447059 0.003301111 0.0027149536 0.0021956628][0.00451297 0.0044353306 0.0043225661 0.0043182676 0.0043831207 0.0045896349 0.0048581832 0.0050617647 0.0050599524 0.0048331902 0.0044697868 0.0039535412 0.0033276607 0.0026871585 0.0021354884][0.0040813414 0.0040171416 0.00398756 0.0041149678 0.0043404675 0.0047117341 0.0051228469 0.005397134 0.0054084808 0.0051406147 0.0047144359 0.004118558 0.0034110388 0.0027106586 0.0021131292][0.0034893895 0.0034894103 0.0035877705 0.0038676127 0.0042554121 0.0047630407 0.0052820584 0.0056144516 0.0056212973 0.0053170063 0.0048441687 0.0042174682 0.0034801192 0.0027456051 0.0021200606][0.0029932731 0.0031024204 0.0033308903 0.0037355486 0.0042417431 0.004835587 0.0054122317 0.0057592527 0.0057464945 0.005401399 0.00489065 0.0042592147 0.003524269 0.0027992215 0.0021721025][0.002772195 0.0029835291 0.0033046536 0.0037712571 0.0043200743 0.004925888 0.0054900614 0.0058094547 0.0057675526 0.0053947871 0.0048829406 0.0042629386 0.0035613598 0.0028631364 0.0022471664][0.0027965731 0.0030850633 0.0034523031 0.0039144736 0.0044238609 0.0049475012 0.00542579 0.0056828079 0.0056210207 0.005266232 0.0047939629 0.0042218454 0.0035672747 0.0029056084 0.0023105131][0.0029759684 0.003307519 0.0036770008 0.0040765116 0.0044753687 0.0048663127 0.0052234405 0.0054105008 0.0053577628 0.0050739134 0.0046702181 0.0041636182 0.0035547451 0.0029342924 0.002374517][0.0032908064 0.0036151998 0.0039328 0.0042310678 0.004498254 0.0047434266 0.0049890871 0.0051396857 0.0051192841 0.0048984624 0.0045615816 0.0041121077 0.0035464559 0.0029565345 0.0024238469][0.003574627 0.003868707 0.0041011493 0.004284116 0.0044279331 0.0045658913 0.0047379667 0.0048713936 0.0048914063 0.0047306558 0.0044475719 0.0040362035 0.0034951302 0.0029391004 0.0024366975][0.003683965 0.0039330781 0.0040909275 0.004177927 0.0042261868 0.004286563 0.0044050505 0.0045293849 0.0045675719 0.0044545336 0.0042210696 0.0038492584 0.0033553115 0.0028415329 0.0023919765][0.0035172459 0.0037236207 0.0038283949 0.0038601242 0.0038509509 0.0038585565 0.0039469842 0.0040595648 0.004101661 0.0040214886 0.0038297856 0.0035122021 0.0030850109 0.0026483247 0.0022707959][0.0030528542 0.0032183558 0.0032824297 0.0032907603 0.0032643548 0.0032569915 0.0033219517 0.0034248924 0.0034694141 0.0034181587 0.0032812506 0.0030351684 0.0027066546 0.0023785303 0.0020972462]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 09:16:06.039857: step 1010, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.791 sec/batch; 72h:47m:34s remains)
INFO - root - 2017-12-10 09:16:13.803289: step 1020, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 74h:45m:15s remains)
INFO - root - 2017-12-10 09:16:21.663883: step 1030, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 70h:12m:16s remains)
INFO - root - 2017-12-10 09:16:29.608563: step 1040, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 71h:35m:03s remains)
INFO - root - 2017-12-10 09:16:37.636519: step 1050, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 74h:22m:30s remains)
INFO - root - 2017-12-10 09:16:45.660344: step 1060, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 75h:25m:08s remains)
INFO - root - 2017-12-10 09:16:53.585478: step 1070, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 73h:57m:19s remains)
INFO - root - 2017-12-10 09:17:01.668136: step 1080, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.805 sec/batch; 74h:04m:54s remains)
INFO - root - 2017-12-10 09:17:09.383155: step 1090, loss = 0.75, batch loss = 0.69 (10.6 examples/sec; 0.754 sec/batch; 69h:24m:47s remains)
INFO - root - 2017-12-10 09:17:17.179201: step 1100, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 72h:00m:44s remains)
2017-12-10 09:17:18.042325: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0015723238 0.0015410753 0.0015382232 0.0015634883 0.0015871008 0.0016136237 0.0016400445 0.0016607299 0.0016590058 0.0016405153 0.0016115969 0.0015800238 0.0015640034 0.001557576 0.0016116819][0.0015279511 0.0015125338 0.0015256875 0.001570243 0.0016151002 0.001665961 0.0017175009 0.0017580499 0.0017697035 0.0017548398 0.0017198859 0.0016789936 0.0016506944 0.0016355946 0.0016778678][0.0014945607 0.001497109 0.0015287768 0.0016027918 0.0016852455 0.0017746886 0.0018657671 0.0019374712 0.00196285 0.001942755 0.0018865169 0.0018195648 0.0017653629 0.0017354588 0.001761415][0.0015643211 0.0015947986 0.0016592165 0.0017681411 0.0018879086 0.0020189453 0.0021537067 0.0022624861 0.0023091303 0.0022949944 0.0022316265 0.0021421472 0.0020527381 0.0019866128 0.0019715973][0.0017434144 0.0018313301 0.0019496627 0.0021017371 0.0022598172 0.0024272257 0.0025996543 0.00274882 0.0028240285 0.0028354055 0.0027834731 0.0026800616 0.0025491386 0.0024252241 0.0023386457][0.0020532794 0.0022422012 0.0024369904 0.0026419535 0.002835993 0.0030344496 0.0032483048 0.0034438418 0.0035591547 0.0036034842 0.0035677592 0.0034522249 0.0032674391 0.0030570547 0.0028683534][0.0024857502 0.0028107786 0.0031117422 0.0033887897 0.0036132643 0.0038292238 0.0040776031 0.0043203584 0.0044746171 0.0045405934 0.0045129578 0.0043739607 0.0041243727 0.0038110297 0.0034998991][0.0029831668 0.0034557502 0.0038650676 0.0041950266 0.0044227662 0.0046301661 0.0048901457 0.005161392 0.0053395848 0.005408226 0.0053714784 0.005208693 0.0048983069 0.0044943513 0.0040820888][0.0034216172 0.0040087504 0.0044866912 0.0048254845 0.0050221081 0.00519052 0.0054218792 0.0056816512 0.0058570476 0.0059173461 0.005877377 0.005714599 0.0053923586 0.004960556 0.0045081368][0.0036574882 0.0042849323 0.0047632679 0.0050737192 0.0052273162 0.0053304667 0.0054843249 0.0056760446 0.0058089239 0.0058544488 0.0058358475 0.0057199788 0.0054537561 0.0050768675 0.0046719303][0.003593537 0.0041795522 0.0045956192 0.0048497678 0.0049526924 0.00499056 0.0050653983 0.00517604 0.0052718185 0.0053116549 0.0053297025 0.0052911248 0.0051284051 0.0048617627 0.0045601884][0.0032948186 0.0037721156 0.0040903776 0.0042647049 0.0043063024 0.004291222 0.0043034661 0.0043553687 0.0044272989 0.0044827284 0.0045528263 0.0045983926 0.0045620594 0.0044432632 0.0042736651][0.0028623661 0.0032109062 0.0034222642 0.0035141623 0.0035044302 0.0034553625 0.0034291134 0.0034463403 0.0035061266 0.0035859768 0.0037050536 0.003827475 0.0039080749 0.0039294013 0.0038851111][0.0024171593 0.0026526914 0.0027741808 0.0028132624 0.0027796938 0.0027249027 0.0026987884 0.0027126472 0.0027786042 0.0028813372 0.0030390988 0.0032156284 0.0033749058 0.0034907197 0.0035320017][0.0020667582 0.0022206823 0.0022837259 0.0022957856 0.0022652145 0.0022296605 0.0022309823 0.0022731523 0.0023645242 0.0024859896 0.0026589518 0.002855306 0.0030426469 0.0031949687 0.0032847947]]...]
INFO - root - 2017-12-10 09:17:25.676570: step 1110, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 75h:14m:32s remains)
INFO - root - 2017-12-10 09:17:33.592508: step 1120, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 74h:31m:35s remains)
INFO - root - 2017-12-10 09:17:41.526445: step 1130, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 71h:57m:46s remains)
INFO - root - 2017-12-10 09:17:49.381148: step 1140, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.799 sec/batch; 73h:30m:48s remains)
INFO - root - 2017-12-10 09:17:57.318604: step 1150, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.793 sec/batch; 72h:58m:06s remains)
INFO - root - 2017-12-10 09:18:05.240441: step 1160, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.776 sec/batch; 71h:27m:39s remains)
INFO - root - 2017-12-10 09:18:12.914741: step 1170, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 70h:21m:40s remains)
INFO - root - 2017-12-10 09:18:20.682087: step 1180, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.765 sec/batch; 70h:25m:04s remains)
INFO - root - 2017-12-10 09:18:28.517924: step 1190, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 75h:24m:28s remains)
INFO - root - 2017-12-10 09:18:36.212078: step 1200, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.796 sec/batch; 73h:12m:51s remains)
2017-12-10 09:18:37.062142: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0060407817 0.0060035586 0.0056332839 0.0051070238 0.0046248389 0.0043547982 0.00429108 0.0042926506 0.0042862562 0.0042411713 0.0041201259 0.0039391858 0.0037366233 0.0036276605 0.003632975][0.007078934 0.0070180246 0.0065191728 0.0058137714 0.0051827831 0.0048393081 0.004749258 0.0047383006 0.004733956 0.004724504 0.0046599312 0.0045355787 0.004368145 0.0042528193 0.0042144447][0.0077412352 0.0076447329 0.0070646587 0.0062795561 0.0056174654 0.0052855676 0.0052308543 0.0052446132 0.0052486737 0.0052492772 0.0051941625 0.0050842897 0.0049310452 0.0048079975 0.0047335159][0.0079748482 0.0078749042 0.0073261168 0.0066340812 0.00610193 0.0058923373 0.0059328419 0.0059982673 0.0059820358 0.0059104711 0.005775372 0.0056013912 0.0054212306 0.005283731 0.0051899715][0.0077320402 0.0077135959 0.0073463041 0.0069350475 0.0066838064 0.006699793 0.0068820543 0.0069882129 0.006888302 0.0066631185 0.0063862666 0.0061002979 0.0058598551 0.0057060607 0.0056173112][0.0071983486 0.0073322654 0.0072490419 0.0072342688 0.0073660663 0.0076522087 0.0079603121 0.0080615561 0.0078023677 0.0073580989 0.0069095674 0.0065230886 0.006240793 0.0060929465 0.0060519692][0.0066418755 0.0069624581 0.0071781743 0.0075475937 0.0080268448 0.0085307118 0.0088982657 0.0089289816 0.0084697483 0.0078150835 0.0072335792 0.006804212 0.0065439628 0.0064528752 0.0064884773][0.0063456097 0.0068691429 0.0073329271 0.0079481769 0.0085954843 0.0091151958 0.0093553187 0.0092094829 0.0085818414 0.0078150481 0.007214468 0.0068501811 0.0066796965 0.0066785766 0.0067831082][0.0061926888 0.0068744421 0.0074977283 0.0082019623 0.0088118324 0.0091573987 0.0091036437 0.008712708 0.0080291694 0.0073161856 0.0068168705 0.0065810569 0.0065456573 0.0066631585 0.0068326383][0.0059264852 0.0066778441 0.0073478809 0.0080195023 0.0084931962 0.0086061647 0.0082815718 0.0077038561 0.0070353034 0.0064603845 0.0061247074 0.0060487539 0.0061571151 0.0063886372 0.006628904][0.0054291277 0.0061399364 0.00675908 0.0073223272 0.0076464848 0.0075862831 0.0071246568 0.0064968723 0.0059179664 0.0055150548 0.0053386521 0.0053959456 0.0056105466 0.0059250663 0.0062280362][0.0048179105 0.0054248003 0.0059344484 0.006357416 0.006566396 0.0064454358 0.0059950971 0.0054480471 0.0050199348 0.0047845631 0.0047375108 0.0048707351 0.0051332093 0.0054818531 0.0058204327][0.004249651 0.0047415746 0.005131932 0.0054501812 0.005634 0.0055722105 0.0052615218 0.0048807422 0.004620451 0.0045108194 0.0045302943 0.0046700756 0.0048971158 0.0052046957 0.0055276351][0.003751127 0.004115093 0.004394243 0.004661609 0.0048920866 0.0049631577 0.0048425389 0.0046395613 0.00450811 0.0044589327 0.0044952394 0.0046193223 0.0048049414 0.0050403769 0.0052882745][0.00333626 0.0035886986 0.0037917909 0.0040377895 0.0043154266 0.0045066713 0.0045440751 0.0044702468 0.0044100648 0.0043898528 0.0044389996 0.004552071 0.0047076861 0.0048616868 0.0050032865]]...]
INFO - root - 2017-12-10 09:18:44.944131: step 1210, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.766 sec/batch; 70h:30m:08s remains)
INFO - root - 2017-12-10 09:18:52.782755: step 1220, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.790 sec/batch; 72h:43m:56s remains)
INFO - root - 2017-12-10 09:19:00.632263: step 1230, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.781 sec/batch; 71h:49m:51s remains)
INFO - root - 2017-12-10 09:19:08.446117: step 1240, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 71h:33m:49s remains)
INFO - root - 2017-12-10 09:19:16.119853: step 1250, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.797 sec/batch; 73h:18m:02s remains)
INFO - root - 2017-12-10 09:19:23.972065: step 1260, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 71h:28m:28s remains)
INFO - root - 2017-12-10 09:19:31.859981: step 1270, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.805 sec/batch; 74h:02m:55s remains)
INFO - root - 2017-12-10 09:19:39.852436: step 1280, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 75h:00m:14s remains)
INFO - root - 2017-12-10 09:19:47.507651: step 1290, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.779 sec/batch; 71h:38m:49s remains)
INFO - root - 2017-12-10 09:19:55.389150: step 1300, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.788 sec/batch; 72h:27m:32s remains)
2017-12-10 09:19:56.153968: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0023230973 0.0022363397 0.0021708775 0.0021864057 0.0023774616 0.0027900664 0.0034637304 0.0043111998 0.0050286409 0.005268909 0.0049761543 0.0042870245 0.0034082511 0.0025189077 0.0017924407][0.0029647723 0.0028525111 0.0027642364 0.0027887623 0.0030290862 0.0035214031 0.0042634928 0.0051386161 0.0058390037 0.0060201054 0.0056239148 0.0047921692 0.003780551 0.0027925025 0.0019895534][0.0036621601 0.003546641 0.0034477229 0.0034781981 0.0037559802 0.0042941729 0.0050615319 0.0059170146 0.0065547209 0.0066532134 0.0061503281 0.00519755 0.0040745833 0.0030061537 0.002144811][0.0043801097 0.0042782277 0.0041922815 0.00422567 0.0045146863 0.0050593009 0.0058050491 0.0066038668 0.0071623884 0.0071677519 0.0065636691 0.0055089281 0.0043017236 0.0031749953 0.0022740208][0.0050403057 0.0049616322 0.0048946925 0.00493465 0.0052119135 0.0057257465 0.0064215777 0.0071505359 0.0076302541 0.0075601637 0.0068840403 0.0057588713 0.0044955541 0.0033316044 0.0023987642][0.0056952261 0.0056509417 0.0055955136 0.0056257788 0.0058553778 0.0062899925 0.006894548 0.0075402595 0.0079448456 0.0078076129 0.00708858 0.0059358841 0.0046509705 0.0034613234 0.0025079513][0.0062968666 0.006315371 0.0063006491 0.006331489 0.0065020216 0.0068254578 0.0073023611 0.0078361 0.008138556 0.0079159215 0.0071535055 0.0059994892 0.0047254697 0.0035337759 0.0025729754][0.0067534791 0.0068546957 0.0068998965 0.0069501069 0.0070709405 0.0072693047 0.0076089264 0.008028009 0.0082341107 0.0079213157 0.0071177343 0.0059865047 0.0047396221 0.0035616003 0.0026021258][0.0070594125 0.0072411406 0.0073551992 0.0074492046 0.007542247 0.0076411213 0.0078585958 0.0081677912 0.0082755452 0.0078905011 0.0070621972 0.00595195 0.0047372426 0.0035795779 0.0026275772][0.007123644 0.0073630395 0.0075360453 0.0077056452 0.0078354282 0.0079117883 0.0080734659 0.0083101848 0.0083288765 0.007875029 0.0070277131 0.0059316307 0.0047429991 0.0036088186 0.0026763421][0.0069263061 0.0072208368 0.0074533513 0.0077005359 0.0079172 0.0080642244 0.0082502514 0.0084357969 0.0083814794 0.0078653293 0.0069828331 0.005880211 0.0047090659 0.0036042724 0.0027081687][0.0064614322 0.0068024839 0.0070927567 0.007409981 0.0077259219 0.00797688 0.0082299132 0.0084099928 0.008317004 0.0077738403 0.00687486 0.0057791914 0.0046392665 0.0035815032 0.0027302105][0.0057569756 0.0061235013 0.0064504468 0.0068102754 0.0071932976 0.0075262492 0.0078357784 0.0080315657 0.0079354672 0.0074234707 0.00657737 0.0055386471 0.0044731372 0.0034877362 0.0027056725][0.0048781494 0.0052363961 0.00557062 0.0059423107 0.0063612736 0.0067484267 0.0070907404 0.00729421 0.0072107371 0.0067708483 0.0060303565 0.0051154792 0.0041842167 0.0033122713 0.0026310668][0.0038698956 0.0041862754 0.0044946573 0.0048397533 0.0052537541 0.0056578745 0.0060047875 0.006202722 0.00614575 0.00580248 0.0052140541 0.0044787358 0.0037260095 0.0030320704 0.0025033872]]...]
INFO - root - 2017-12-10 09:20:04.007820: step 1310, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.784 sec/batch; 72h:05m:22s remains)
INFO - root - 2017-12-10 09:20:11.905986: step 1320, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.793 sec/batch; 72h:57m:06s remains)
INFO - root - 2017-12-10 09:20:19.698513: step 1330, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.794 sec/batch; 73h:01m:08s remains)
INFO - root - 2017-12-10 09:20:27.545112: step 1340, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.780 sec/batch; 71h:46m:58s remains)
INFO - root - 2017-12-10 09:20:35.566960: step 1350, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.812 sec/batch; 74h:43m:08s remains)
INFO - root - 2017-12-10 09:20:43.439840: step 1360, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.794 sec/batch; 73h:02m:48s remains)
INFO - root - 2017-12-10 09:20:51.148793: step 1370, loss = 0.75, batch loss = 0.69 (12.2 examples/sec; 0.653 sec/batch; 60h:05m:01s remains)
INFO - root - 2017-12-10 09:20:59.017980: step 1380, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.793 sec/batch; 72h:58m:28s remains)
INFO - root - 2017-12-10 09:21:06.891436: step 1390, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.793 sec/batch; 72h:56m:36s remains)
INFO - root - 2017-12-10 09:21:14.839939: step 1400, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.793 sec/batch; 72h:54m:52s remains)
2017-12-10 09:21:15.654018: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010758786 0.01084884 0.01070551 0.010570521 0.010582711 0.010698636 0.010853419 0.010871217 0.010628632 0.010217028 0.0097434744 0.0093607558 0.0091797374 0.0092359874 0.0095123071][0.011592077 0.01176656 0.011610926 0.011410601 0.011343824 0.011423487 0.0115474 0.011526244 0.011231806 0.010756184 0.010197432 0.0097052138 0.0094243195 0.009447488 0.00980166][0.012017515 0.012240813 0.012065282 0.011832837 0.011747868 0.011862813 0.012039774 0.011996503 0.01157837 0.010925498 0.010170144 0.0094724391 0.0090056593 0.0089785522 0.0094689447][0.012174328 0.012353511 0.012154175 0.011969825 0.011989191 0.012280755 0.012618334 0.012597518 0.012027726 0.011091702 0.010022816 0.0090134488 0.0083186943 0.0082527176 0.0089324294][0.012019137 0.01215415 0.011986142 0.011936009 0.012147805 0.012668261 0.013163331 0.013145598 0.012389705 0.011145207 0.0097660832 0.0084905615 0.0076537491 0.0076276581 0.008522098][0.011671351 0.011768561 0.011659899 0.011770994 0.012185071 0.0129071 0.013517512 0.013461939 0.012506311 0.011003237 0.0094182817 0.0080269072 0.0071981195 0.0073051415 0.0084426356][0.011102061 0.011159729 0.011113888 0.011404333 0.012051056 0.012980899 0.013721775 0.013664721 0.012571065 0.010899048 0.0091904476 0.0077527412 0.0069762054 0.0072276453 0.0086160954][0.010244175 0.010274381 0.010312171 0.010811232 0.011723709 0.012877384 0.013763553 0.013770501 0.012677787 0.010940417 0.0091519132 0.0076959622 0.0069863871 0.0073837913 0.008970377][0.0092702685 0.0092862677 0.0094281891 0.010116635 0.011241765 0.012546606 0.013500579 0.013582113 0.012591136 0.010903636 0.0091299452 0.007734709 0.0071553737 0.0076765488 0.00935083][0.00834605 0.0083653349 0.0085794721 0.0093622468 0.010566262 0.011898448 0.012844506 0.012983311 0.012152498 0.01063183 0.00900423 0.0077546579 0.0073234104 0.0079139527 0.0095255971][0.0075051882 0.0075238431 0.0077302069 0.00847902 0.009624308 0.010867451 0.011750801 0.01193408 0.011277798 0.0099927373 0.0085924109 0.0075300173 0.0072042295 0.0077761137 0.0092119751][0.0065924926 0.0066153123 0.006782528 0.0074331956 0.0084385145 0.0095159244 0.010272365 0.010456931 0.0099579059 0.008920731 0.0077702412 0.00689809 0.0066369479 0.00711169 0.0082760276][0.0054852543 0.0055423579 0.0056908391 0.0062309387 0.0070482413 0.0078860465 0.008445574 0.0085672289 0.0081818029 0.00739351 0.0065167448 0.0058657904 0.0056904443 0.0060583926 0.0068976525][0.0043347422 0.0044344617 0.0045738085 0.0050023282 0.0056186868 0.0062009068 0.0065500196 0.0065983194 0.0063117375 0.0057482631 0.0051409286 0.0047146869 0.0046262164 0.0048859566 0.0054081343][0.0033049905 0.00343856 0.0035606511 0.0038597754 0.0042634741 0.00460838 0.0047973096 0.0048111947 0.0046297573 0.0042781639 0.0039197793 0.0036841061 0.0036527021 0.0038082518 0.0040908642]]...]
INFO - root - 2017-12-10 09:21:23.504890: step 1410, loss = 0.75, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 76h:30m:21s remains)
INFO - root - 2017-12-10 09:21:31.287919: step 1420, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 70h:04m:39s remains)
INFO - root - 2017-12-10 09:21:39.118635: step 1430, loss = 0.75, batch loss = 0.69 (10.6 examples/sec; 0.758 sec/batch; 69h:41m:12s remains)
INFO - root - 2017-12-10 09:21:47.118159: step 1440, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.796 sec/batch; 73h:13m:20s remains)
INFO - root - 2017-12-10 09:21:54.929147: step 1450, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 73h:40m:05s remains)
INFO - root - 2017-12-10 09:22:02.601466: step 1460, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 71h:32m:51s remains)
INFO - root - 2017-12-10 09:22:10.507466: step 1470, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.779 sec/batch; 71h:37m:35s remains)
INFO - root - 2017-12-10 09:22:18.386761: step 1480, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 74h:34m:04s remains)
INFO - root - 2017-12-10 09:22:26.378701: step 1490, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.761 sec/batch; 69h:57m:13s remains)
INFO - root - 2017-12-10 09:22:34.266614: step 1500, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 70h:08m:14s remains)
2017-12-10 09:22:35.076749: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012841013 0.012092037 0.01095737 0.010160003 0.010125138 0.010825519 0.01173502 0.012268038 0.012157607 0.01148855 0.010625395 0.0097901793 0.009164324 0.0087837419 0.0084091][0.014628533 0.01382963 0.012485298 0.011569611 0.011645476 0.012603282 0.013766564 0.014415476 0.014263794 0.013414001 0.012262797 0.011117223 0.010228214 0.0096896868 0.0092342859][0.01522604 0.014519265 0.013178231 0.012344477 0.012654246 0.013906291 0.015272079 0.015945965 0.015680859 0.014620807 0.013181719 0.011753302 0.010660195 0.010021427 0.009527876][0.01500278 0.014454698 0.013280062 0.012698726 0.013330621 0.014862411 0.016369259 0.017045693 0.016687443 0.015443728 0.013747104 0.012069519 0.010819703 0.010110598 0.00956254][0.014068586 0.013771635 0.012922551 0.012718006 0.013723987 0.01552253 0.017156187 0.0178597 0.017459275 0.016118027 0.014245841 0.012396961 0.011020172 0.010232336 0.0096035525][0.012648912 0.012694442 0.012318142 0.012604365 0.014016985 0.016095892 0.017893292 0.018668091 0.018275212 0.016919017 0.014984746 0.01305694 0.011555078 0.010616658 0.0098303109][0.011497927 0.011849689 0.011919758 0.012634858 0.014378591 0.016695676 0.018690474 0.019637525 0.019378778 0.018136602 0.016277917 0.014345423 0.012715826 0.011555581 0.010525415][0.011144618 0.011696783 0.011999558 0.012901538 0.014746094 0.017149208 0.01930055 0.020451613 0.020424256 0.019439679 0.017791206 0.015933676 0.014199997 0.012822094 0.011540668][0.011681563 0.012340819 0.012643781 0.013425073 0.015051805 0.017278956 0.019402841 0.020690549 0.020940812 0.020329669 0.019043252 0.01737144 0.015603098 0.014053605 0.012551048][0.012436817 0.013098948 0.013249575 0.013696538 0.014875595 0.016688455 0.018586526 0.019889677 0.02037782 0.020182889 0.0193448 0.017992215 0.016351674 0.014784614 0.01319842][0.012754655 0.013342598 0.013281121 0.013351537 0.014003602 0.015264529 0.016755139 0.017905453 0.018525071 0.018671338 0.018268021 0.017320246 0.015991563 0.014607508 0.01310101][0.012330475 0.012769667 0.01250437 0.012239655 0.012396863 0.013073222 0.014052321 0.014908673 0.015498183 0.015828732 0.015760358 0.015237416 0.014353339 0.013337499 0.012089421][0.010969393 0.011287273 0.010913818 0.010446302 0.010243277 0.01041652 0.010879263 0.01135947 0.01177166 0.012101744 0.012200204 0.012011203 0.011559555 0.010955462 0.010071433][0.0088957353 0.0091266 0.008751791 0.0082351649 0.0078561138 0.0077130613 0.0077964254 0.0079766447 0.0082077738 0.0084597208 0.0086092725 0.0086161084 0.0084603485 0.0081655709 0.007609799][0.0064671943 0.006600902 0.0062916493 0.0058723143 0.0055198567 0.0052945949 0.0052246507 0.0052782558 0.0054027503 0.0055514518 0.0056600915 0.0057207351 0.00570408 0.0055666515 0.0052535166]]...]
INFO - root - 2017-12-10 09:22:42.961675: step 1510, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 73h:22m:19s remains)
INFO - root - 2017-12-10 09:22:50.791472: step 1520, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 73h:37m:39s remains)
INFO - root - 2017-12-10 09:22:58.622773: step 1530, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.770 sec/batch; 70h:44m:46s remains)
INFO - root - 2017-12-10 09:23:06.465106: step 1540, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.773 sec/batch; 71h:05m:44s remains)
INFO - root - 2017-12-10 09:23:14.286983: step 1550, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.788 sec/batch; 72h:27m:45s remains)
INFO - root - 2017-12-10 09:23:22.265537: step 1560, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 73h:49m:16s remains)
INFO - root - 2017-12-10 09:23:30.006199: step 1570, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.788 sec/batch; 72h:23m:52s remains)
INFO - root - 2017-12-10 09:23:37.959406: step 1580, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.792 sec/batch; 72h:45m:32s remains)
INFO - root - 2017-12-10 09:23:45.887673: step 1590, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.786 sec/batch; 72h:16m:23s remains)
INFO - root - 2017-12-10 09:23:53.847583: step 1600, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.791 sec/batch; 72h:40m:52s remains)
2017-12-10 09:23:54.716374: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0039047443 0.00460703 0.0051727607 0.0055087754 0.0056036427 0.0055542053 0.0054577691 0.0053373151 0.005275792 0.0053111957 0.0054483456 0.0055811037 0.0056271749 0.0055313008 0.0051822853][0.0048297495 0.0058480613 0.0067290324 0.0073115923 0.007561537 0.0075615984 0.0074391942 0.0072457343 0.00713785 0.0071624704 0.0073086624 0.0074576214 0.0075126481 0.0073926104 0.0069123614][0.0056970241 0.0070330622 0.00823259 0.0091034221 0.0095696626 0.0096714692 0.0095445691 0.0092890793 0.0091218874 0.0091036269 0.0092105633 0.0093547283 0.0093943086 0.0092152609 0.0085845152][0.0063141119 0.0079308087 0.0094525358 0.010635661 0.011355058 0.011611779 0.011536755 0.011251912 0.011040199 0.010980192 0.011030146 0.011123355 0.011096835 0.01080955 0.0099796737][0.00650104 0.0082963984 0.010085468 0.011593729 0.012635637 0.013149409 0.013263919 0.013101092 0.01293195 0.012834551 0.01278625 0.012776301 0.012601849 0.012089266 0.010962259][0.0063992343 0.008280375 0.01024205 0.011983015 0.013294569 0.014068534 0.014457896 0.014514027 0.014452956 0.014341372 0.014195798 0.014080189 0.013735047 0.01298201 0.011575422][0.0062434981 0.0081382869 0.01014519 0.0119627 0.013379651 0.014292021 0.014898344 0.015177841 0.015266392 0.015203868 0.015043328 0.014875804 0.014402867 0.013474939 0.011882782][0.0061144033 0.0080214692 0.010027482 0.011809152 0.013167891 0.014073945 0.014774981 0.015196363 0.015415568 0.015480283 0.01544642 0.015324464 0.01479559 0.013763002 0.01205803][0.0060873856 0.0080239223 0.010028884 0.011731906 0.012962167 0.013769031 0.014428826 0.014813623 0.015092324 0.015329222 0.015516292 0.0155141 0.015010232 0.013967741 0.012226694][0.0062568192 0.0082356557 0.010225386 0.011831505 0.012913575 0.013572903 0.0140776 0.01429048 0.014488017 0.01477834 0.015123375 0.015254344 0.014858505 0.013920667 0.012270067][0.0065539689 0.0085595511 0.010487245 0.011944409 0.012830195 0.013284657 0.013540731 0.013491369 0.013509939 0.013730711 0.014112475 0.014319528 0.014060484 0.013307491 0.011881719][0.0066444613 0.0085682832 0.01033651 0.011603389 0.012287167 0.0125465 0.012556494 0.012281483 0.012117234 0.012204355 0.012519295 0.012716138 0.012556748 0.011995511 0.010840472][0.0062509738 0.0079754638 0.0095193116 0.010593245 0.011144541 0.011301113 0.011183687 0.010796714 0.010508034 0.010449003 0.010612351 0.010708239 0.010571703 0.010147982 0.0092444671][0.0054870509 0.0069104969 0.0081527652 0.0090054553 0.0094368858 0.00953586 0.00938323 0.0090085659 0.0087061673 0.0085540833 0.0085567432 0.008532241 0.0083826967 0.0080569657 0.0073910574][0.0045466758 0.0056129326 0.0064993571 0.0070796958 0.0073345685 0.0073425905 0.0071694446 0.0068689864 0.0066232667 0.0064625219 0.0063965856 0.0063176234 0.0061836247 0.00596439 0.0055379965]]...]
INFO - root - 2017-12-10 09:24:02.735451: step 1610, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.784 sec/batch; 72h:04m:40s remains)
INFO - root - 2017-12-10 09:24:10.509433: step 1620, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 70h:49m:10s remains)
INFO - root - 2017-12-10 09:24:18.353583: step 1630, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.794 sec/batch; 72h:57m:09s remains)
INFO - root - 2017-12-10 09:24:26.071441: step 1640, loss = 0.75, batch loss = 0.69 (10.9 examples/sec; 0.737 sec/batch; 67h:42m:31s remains)
INFO - root - 2017-12-10 09:24:33.792190: step 1650, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.804 sec/batch; 73h:52m:12s remains)
INFO - root - 2017-12-10 09:24:41.602219: step 1660, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.760 sec/batch; 69h:50m:29s remains)
INFO - root - 2017-12-10 09:24:49.515516: step 1670, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 74h:47m:17s remains)
INFO - root - 2017-12-10 09:24:57.319149: step 1680, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 70h:53m:11s remains)
INFO - root - 2017-12-10 09:25:05.234504: step 1690, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.784 sec/batch; 72h:02m:39s remains)
INFO - root - 2017-12-10 09:25:13.104444: step 1700, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.770 sec/batch; 70h:47m:32s remains)
2017-12-10 09:25:13.928142: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022129323 0.023195177 0.022652956 0.020880243 0.018219328 0.015358309 0.012857626 0.011012936 0.0097932341 0.0089693442 0.0083250068 0.0078387177 0.0076427669 0.0077409013 0.0082219206][0.022751097 0.02381107 0.023278406 0.02154704 0.019012397 0.016407829 0.014208178 0.01270456 0.011703488 0.01093486 0.010117095 0.0093256151 0.00889251 0.0089000277 0.0094224932][0.021539785 0.022641931 0.022320962 0.021004898 0.019097073 0.017239209 0.015783947 0.014902358 0.014240281 0.013489588 0.012405793 0.011203444 0.010435064 0.010266454 0.010755077][0.01949735 0.020623511 0.020628449 0.019966757 0.018979847 0.018160664 0.01768454 0.017556535 0.017276654 0.016501537 0.015099998 0.013454321 0.012307256 0.01191109 0.012251846][0.017223062 0.018314682 0.01866176 0.01877081 0.018868821 0.019222386 0.019775826 0.020361394 0.020404898 0.019547127 0.017808784 0.015752997 0.014243015 0.01358669 0.013662026][0.015163388 0.016189618 0.01687286 0.017751627 0.018942839 0.020446664 0.021960111 0.023087891 0.023275064 0.022240052 0.02016169 0.017768042 0.015942458 0.015042356 0.014801963][0.013596729 0.01458521 0.015554369 0.017085856 0.019148778 0.021506121 0.023674166 0.025058193 0.0251775 0.02389635 0.021555325 0.018937798 0.01687682 0.015740179 0.015178817][0.012729183 0.01361373 0.014727021 0.016634265 0.019138293 0.021830387 0.024180308 0.025474859 0.02535959 0.023870233 0.021444052 0.018812208 0.016676297 0.015403795 0.014615946][0.012457398 0.013166786 0.014228063 0.016180344 0.018671062 0.021198254 0.023270916 0.024194062 0.02380874 0.022270555 0.019965725 0.017522821 0.015492617 0.014232686 0.013383845][0.012353074 0.012917744 0.013819514 0.015530016 0.01765513 0.019693403 0.021212548 0.021638386 0.021010384 0.019546026 0.017511604 0.015409167 0.013671567 0.012614268 0.011909827][0.011739112 0.01218839 0.012889523 0.01422806 0.015819734 0.017257685 0.018209094 0.018234428 0.017463518 0.016138887 0.014454867 0.012769352 0.011430547 0.010684972 0.010265059][0.01034244 0.010705501 0.011214271 0.012162255 0.013217772 0.014081824 0.014535777 0.014307825 0.013535821 0.012434429 0.011150725 0.0099197356 0.0090076867 0.0085786339 0.0084396787][0.0083782719 0.00869882 0.0090611344 0.0096597178 0.010247256 0.010632678 0.010710924 0.010360041 0.0097145885 0.0089315083 0.0080981106 0.0073507293 0.0068698307 0.0067410655 0.00682392][0.0062905191 0.0066174073 0.006900257 0.0072476487 0.007512501 0.0075961659 0.0074649286 0.0071068918 0.0066542453 0.0061972858 0.005770199 0.0054274355 0.0052889739 0.0053639957 0.0055490136][0.004450175 0.0047558234 0.0049736416 0.0051630526 0.0052544228 0.0052054608 0.0050215144 0.0047564884 0.0045183385 0.0043252157 0.00419397 0.0041264514 0.004182647 0.0043331352 0.004514561]]...]
INFO - root - 2017-12-10 09:25:21.775952: step 1710, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 70h:01m:02s remains)
INFO - root - 2017-12-10 09:25:29.455099: step 1720, loss = 0.75, batch loss = 0.69 (13.5 examples/sec; 0.590 sec/batch; 54h:15m:14s remains)
INFO - root - 2017-12-10 09:25:37.234145: step 1730, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 75h:17m:02s remains)
INFO - root - 2017-12-10 09:25:45.106345: step 1740, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.787 sec/batch; 72h:18m:46s remains)
INFO - root - 2017-12-10 09:25:52.866462: step 1750, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.799 sec/batch; 73h:24m:35s remains)
INFO - root - 2017-12-10 09:26:00.692530: step 1760, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.795 sec/batch; 73h:01m:36s remains)
INFO - root - 2017-12-10 09:26:08.657039: step 1770, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.793 sec/batch; 72h:52m:00s remains)
INFO - root - 2017-12-10 09:26:16.468695: step 1780, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 71h:24m:13s remains)
INFO - root - 2017-12-10 09:26:24.297482: step 1790, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 70h:49m:07s remains)
INFO - root - 2017-12-10 09:26:32.174183: step 1800, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 74h:34m:20s remains)
2017-12-10 09:26:33.105853: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.018582167 0.019811872 0.020205522 0.019863136 0.018902451 0.01756699 0.016049813 0.014558245 0.013168339 0.012096758 0.011510637 0.011489445 0.011897543 0.012386002 0.012656663][0.020072652 0.021848649 0.022703094 0.022722406 0.021901544 0.02047687 0.01868009 0.01676219 0.014906983 0.013472192 0.012735053 0.012740308 0.013279818 0.013908144 0.014247944][0.020422665 0.02266256 0.024027307 0.02458103 0.024154313 0.022873066 0.020973459 0.018769998 0.016552739 0.014838962 0.014020848 0.01410629 0.014772063 0.015473405 0.015801309][0.020249352 0.022666652 0.024410063 0.025572522 0.025748223 0.024843827 0.0230864 0.020811494 0.018413674 0.016553733 0.015767785 0.01600977 0.016796302 0.017505562 0.01773319][0.01973661 0.022016229 0.023966795 0.02578936 0.026808044 0.026590982 0.025264401 0.023160107 0.020736972 0.018801121 0.018091012 0.018530628 0.019451344 0.020098019 0.020107625][0.019132825 0.021070713 0.023099553 0.025601909 0.027658135 0.028413823 0.027782254 0.026019905 0.023630194 0.021566976 0.020858293 0.021449288 0.022503648 0.023069963 0.022781422][0.019151442 0.020762758 0.022770567 0.025785921 0.028778294 0.03052181 0.030647855 0.029275106 0.026859064 0.024519915 0.023583524 0.024163432 0.025313077 0.025832124 0.02526292][0.019748002 0.021158164 0.023042921 0.026233971 0.029783102 0.032299567 0.033131976 0.032183435 0.029759761 0.027058724 0.025674298 0.026003351 0.027102202 0.027560914 0.026790323][0.020597914 0.021885684 0.023530031 0.026559971 0.03026098 0.033279084 0.03472095 0.03421345 0.031851139 0.02881078 0.026855059 0.026687192 0.027534319 0.027859129 0.026966631][0.021238707 0.022547869 0.023985144 0.026681133 0.030170422 0.033276126 0.034996718 0.034744896 0.032406516 0.029070649 0.026517525 0.02571319 0.026121499 0.0262091 0.025244541][0.021368599 0.022735786 0.023947733 0.026116429 0.028970465 0.031606063 0.033102762 0.032861147 0.030627241 0.027275437 0.024394935 0.023038799 0.022942048 0.022762753 0.021802403][0.020621723 0.022000233 0.022931993 0.024406116 0.026282925 0.027989795 0.028847482 0.028429614 0.026475377 0.023554815 0.020824289 0.019228661 0.018759131 0.018388305 0.017545171][0.018647434 0.019924009 0.020545745 0.021327881 0.022205042 0.022918446 0.023057805 0.022423029 0.020873392 0.0186904 0.016534463 0.015085801 0.014501882 0.014114607 0.013501953][0.016005456 0.017076336 0.017442176 0.017726146 0.01790718 0.017942173 0.017596632 0.016822085 0.015620017 0.014129483 0.012652453 0.011578377 0.011111787 0.01086682 0.010551434][0.013572939 0.014410977 0.014627684 0.014677457 0.0145426 0.014285893 0.013782093 0.013030109 0.012118262 0.011146226 0.010226544 0.009532975 0.0092288107 0.0091227023 0.0090214768]]...]
INFO - root - 2017-12-10 09:26:40.778177: step 1810, loss = 0.75, batch loss = 0.69 (10.7 examples/sec; 0.751 sec/batch; 68h:58m:54s remains)
INFO - root - 2017-12-10 09:26:48.801467: step 1820, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 75h:09m:40s remains)
2017-12-10 09:26:55.041810: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 490618 get requests, put_count=490617 evicted_count=1000 eviction_rate=0.00203825 and unsatisfied allocation rate=0.00224411
2017-12-10 09:26:55.041842: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
INFO - root - 2017-12-10 09:26:56.587352: step 1830, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.775 sec/batch; 71h:12m:06s remains)
INFO - root - 2017-12-10 09:27:04.364421: step 1840, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 71h:20m:48s remains)
INFO - root - 2017-12-10 09:27:12.321158: step 1850, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.795 sec/batch; 72h:59m:58s remains)
INFO - root - 2017-12-10 09:27:20.152568: step 1860, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 73h:31m:42s remains)
INFO - root - 2017-12-10 09:27:28.052119: step 1870, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 70h:49m:24s remains)
INFO - root - 2017-12-10 09:27:35.738796: step 1880, loss = 0.75, batch loss = 0.69 (12.8 examples/sec; 0.624 sec/batch; 57h:15m:58s remains)
INFO - root - 2017-12-10 09:27:43.565984: step 1890, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.788 sec/batch; 72h:24m:06s remains)
INFO - root - 2017-12-10 09:27:51.270919: step 1900, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.790 sec/batch; 72h:33m:14s remains)
2017-12-10 09:27:52.142406: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.023977198 0.024954623 0.024640108 0.023655312 0.022543756 0.021701949 0.021478178 0.021282839 0.020953894 0.020452568 0.020394696 0.020606855 0.021017168 0.021300845 0.021228375][0.026393395 0.027688775 0.027771875 0.027202014 0.0264493 0.025794264 0.025582483 0.025187101 0.024541792 0.023671633 0.023301894 0.023198688 0.023359373 0.023577372 0.023538729][0.027346872 0.028843503 0.029232858 0.029095054 0.028788609 0.028406098 0.028228663 0.027628267 0.026701272 0.025611008 0.025152786 0.024957495 0.025023377 0.0251947 0.025112061][0.027109278 0.028661642 0.029226068 0.029426781 0.02951964 0.029459847 0.029492967 0.028920488 0.027886072 0.026733208 0.026419885 0.026437072 0.026604943 0.026747875 0.026515294][0.025027489 0.026583938 0.027310373 0.027828587 0.028371289 0.028782208 0.029243665 0.029017717 0.028164126 0.027165771 0.027104776 0.027433513 0.027747996 0.027810173 0.027336648][0.021824604 0.023359243 0.024240367 0.025006533 0.025901414 0.026767604 0.02771643 0.028001955 0.027603805 0.027005427 0.027343411 0.027998136 0.028398229 0.028247114 0.027427811][0.019427394 0.020875592 0.021739608 0.022539912 0.023505745 0.024552157 0.0257781 0.026455987 0.026537351 0.026394375 0.027120369 0.02803045 0.028459195 0.028071137 0.026942154][0.018093815 0.019393429 0.020163083 0.020931143 0.021834923 0.022876747 0.024170587 0.025099106 0.025560357 0.025733471 0.026696267 0.02774206 0.028177377 0.027638698 0.026331728][0.016817812 0.017951291 0.018631708 0.019396629 0.020260723 0.021244677 0.022451064 0.023444347 0.024073498 0.024337994 0.025286444 0.026314439 0.02674822 0.026229979 0.024988187][0.015045977 0.01605363 0.016698135 0.017463738 0.018287899 0.019160978 0.020154247 0.021033119 0.02164329 0.021877354 0.022714471 0.023671627 0.024134934 0.023756046 0.022748033][0.013226201 0.014061318 0.014575358 0.015202624 0.01585329 0.016506601 0.017203439 0.0178425 0.018320905 0.01850887 0.019283414 0.020238305 0.020822404 0.02068832 0.020022124][0.011591174 0.012225741 0.012549771 0.012963432 0.013387048 0.013793016 0.014189751 0.01455072 0.014830901 0.014945088 0.015630249 0.016586376 0.017306713 0.017453313 0.017137881][0.0096238656 0.010089184 0.010271477 0.010521319 0.010776137 0.011018345 0.011226269 0.011412221 0.011561927 0.011644951 0.012233591 0.01314415 0.013925388 0.014268033 0.014194467][0.0073095094 0.0076500569 0.0077527496 0.007906924 0.0080689359 0.0082277171 0.0083633065 0.00850316 0.0086362744 0.0087447912 0.0092402082 0.010037837 0.010763632 0.011170966 0.011217604][0.005347501 0.0055868533 0.0056334403 0.0057214885 0.0058382521 0.0059560523 0.0060729794 0.0062171263 0.0063795522 0.0065407916 0.0069459327 0.007582529 0.0081647839 0.0085243182 0.0085956994]]...]
INFO - root - 2017-12-10 09:28:00.066264: step 1910, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.795 sec/batch; 73h:01m:00s remains)
INFO - root - 2017-12-10 09:28:07.929794: step 1920, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 74h:09m:34s remains)
INFO - root - 2017-12-10 09:28:15.742488: step 1930, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 70h:03m:18s remains)
INFO - root - 2017-12-10 09:28:23.694257: step 1940, loss = 0.75, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 76h:01m:26s remains)
INFO - root - 2017-12-10 09:28:31.460107: step 1950, loss = 0.75, batch loss = 0.69 (10.6 examples/sec; 0.757 sec/batch; 69h:31m:32s remains)
INFO - root - 2017-12-10 09:28:39.209364: step 1960, loss = 0.75, batch loss = 0.69 (12.7 examples/sec; 0.629 sec/batch; 57h:45m:52s remains)
INFO - root - 2017-12-10 09:28:47.124352: step 1970, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.797 sec/batch; 73h:12m:41s remains)
INFO - root - 2017-12-10 09:28:55.044649: step 1980, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.792 sec/batch; 72h:42m:30s remains)
INFO - root - 2017-12-10 09:29:02.690179: step 1990, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 70h:46m:13s remains)
INFO - root - 2017-12-10 09:29:10.552418: step 2000, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 71h:19m:08s remains)
2017-12-10 09:29:11.416179: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0058347317 0.0061186836 0.0062677623 0.0063277781 0.0062937154 0.0063057886 0.0064950776 0.0069056274 0.00742197 0.0078290524 0.0079594832 0.0076536569 0.0069407774 0.0060099843 0.005127063][0.0066754464 0.0073019592 0.0077234181 0.007994052 0.008149147 0.0084132673 0.0089895548 0.0099543473 0.011088572 0.011980018 0.012315946 0.011841357 0.01057143 0.0088193873 0.0070756725][0.0079749078 0.00914559 0.010013087 0.010633727 0.011069958 0.011657322 0.012713023 0.014354851 0.016223695 0.017670721 0.018213939 0.017488031 0.015469096 0.012612498 0.0097136311][0.0098132547 0.011741767 0.013254327 0.014405625 0.015256772 0.016174862 0.017640328 0.019860795 0.022390321 0.024301939 0.024933571 0.023833565 0.020929029 0.016829658 0.012616589][0.011962622 0.014814471 0.01718148 0.019115161 0.020602223 0.021948881 0.023697006 0.026178602 0.028970256 0.030941304 0.0313194 0.029631106 0.02580364 0.020555351 0.015162812][0.014127047 0.017988186 0.02135133 0.024277152 0.026588922 0.028374605 0.030099049 0.032249805 0.034564234 0.03590377 0.035544451 0.033119094 0.028565122 0.022609495 0.016550623][0.016268333 0.021196282 0.025612626 0.029518556 0.032516919 0.034406312 0.035544023 0.036647271 0.037761617 0.037929144 0.036594797 0.033558298 0.028725259 0.022682855 0.016599819][0.018037455 0.023855664 0.029087391 0.03358195 0.036758509 0.038216189 0.038284503 0.037945181 0.03760488 0.036585987 0.034499183 0.031249937 0.026655648 0.021096017 0.015537394][0.018799966 0.025014734 0.030526463 0.035046749 0.037909623 0.038614284 0.037511781 0.035796497 0.034198448 0.032326244 0.029902512 0.026858836 0.022958579 0.018372776 0.013789661][0.018072383 0.024008306 0.02913785 0.033088569 0.035272054 0.035232171 0.033301219 0.030695299 0.028336072 0.026099652 0.023774466 0.021266142 0.018313386 0.01494847 0.011592238][0.015878158 0.02091882 0.025130168 0.028125469 0.029496508 0.028911877 0.026671933 0.023849461 0.02139557 0.019324616 0.017464265 0.015672797 0.013709371 0.011543883 0.0093998993][0.012866344 0.016671399 0.019732064 0.021687834 0.022313539 0.021443719 0.019378575 0.016960641 0.014983928 0.013486522 0.012314154 0.011277723 0.010169121 0.0089359414 0.0077262321][0.0097512212 0.012231034 0.014131371 0.015199622 0.015332126 0.0144894 0.012948606 0.011279242 0.010020074 0.0091998177 0.008681545 0.0082582673 0.0077873417 0.0072203209 0.0066669621][0.0070340582 0.00839733 0.0093729943 0.0098726563 0.0098387264 0.0093007591 0.0084357932 0.0075382804 0.0069246497 0.0065917261 0.0064616636 0.0063990559 0.0063281022 0.0062129116 0.0061306017][0.0051484443 0.0057641449 0.0061569777 0.00636452 0.0063477312 0.0061377157 0.0058254893 0.0055189412 0.0053653172 0.0053143254 0.0053676181 0.0054407818 0.0055429577 0.0056759035 0.0058793118]]...]
INFO - root - 2017-12-10 09:29:19.247155: step 2010, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 70h:07m:15s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 09:29:27.140341: step 2020, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 74h:26m:47s remains)
INFO - root - 2017-12-10 09:29:34.987833: step 2030, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.783 sec/batch; 71h:51m:10s remains)
INFO - root - 2017-12-10 09:29:42.790676: step 2040, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.772 sec/batch; 70h:50m:23s remains)
INFO - root - 2017-12-10 09:29:50.733525: step 2050, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 70h:05m:41s remains)
INFO - root - 2017-12-10 09:29:58.704686: step 2060, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 74h:27m:25s remains)
INFO - root - 2017-12-10 09:30:06.603016: step 2070, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.790 sec/batch; 72h:30m:39s remains)
INFO - root - 2017-12-10 09:30:14.531607: step 2080, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 70h:45m:15s remains)
INFO - root - 2017-12-10 09:30:22.411398: step 2090, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.787 sec/batch; 72h:14m:25s remains)
INFO - root - 2017-12-10 09:30:30.404773: step 2100, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 73h:16m:29s remains)
2017-12-10 09:30:31.216335: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022499342 0.025228288 0.028247183 0.030737227 0.032354448 0.033343278 0.033918589 0.033791926 0.033161432 0.03247226 0.031436656 0.029583883 0.026921926 0.024042988 0.021584554][0.022598816 0.025600716 0.02865167 0.031025264 0.03243953 0.033021662 0.032981608 0.032200661 0.031004872 0.029959204 0.028876059 0.027347127 0.025231671 0.02295823 0.021104073][0.022217035 0.025399266 0.028359499 0.03046855 0.031611245 0.03187944 0.031400163 0.030148691 0.028581403 0.027315652 0.026201444 0.024878278 0.023119772 0.021260731 0.019873217][0.022545576 0.025671162 0.028292669 0.029907476 0.030642297 0.030641226 0.029980872 0.028640714 0.027111234 0.02589431 0.024819169 0.023596508 0.02193461 0.020125585 0.018806065][0.024122521 0.026889687 0.028803525 0.029623376 0.029710041 0.029379331 0.028709061 0.027596924 0.026406491 0.025420014 0.024398733 0.023142003 0.021383129 0.01938078 0.017821988][0.026465135 0.028616102 0.02958817 0.029484982 0.028881876 0.028247098 0.027694933 0.027035071 0.0263761 0.025746755 0.024797369 0.02342817 0.021474706 0.01913354 0.017149737][0.028564772 0.02989802 0.029878266 0.02903047 0.028048616 0.027427701 0.027232854 0.027162515 0.027065894 0.026706476 0.025702788 0.024117589 0.021962779 0.01940104 0.017136971][0.029657189 0.03001513 0.029132316 0.027937239 0.02704791 0.026832161 0.027202081 0.027773242 0.02819882 0.028036794 0.026943371 0.025169684 0.022932129 0.020363327 0.018073056][0.030008417 0.029470697 0.027926026 0.026709588 0.026280832 0.026765764 0.027857488 0.029069113 0.029894602 0.029790433 0.02854337 0.026612172 0.024398167 0.021975843 0.019853424][0.029838586 0.028658696 0.026712712 0.025685171 0.025875265 0.02716019 0.029004371 0.030788423 0.031891614 0.031749971 0.030344663 0.02828606 0.026152506 0.023939686 0.022033444][0.028846482 0.027385412 0.025371188 0.024689564 0.025484687 0.027468171 0.029931122 0.032135982 0.033371918 0.033101965 0.031550921 0.029481113 0.027610347 0.025796451 0.024224572][0.027249247 0.025758404 0.023901571 0.023579499 0.024791779 0.027188629 0.029970102 0.032316543 0.033490639 0.033016045 0.03134938 0.029423654 0.028062453 0.02697215 0.025993284][0.025905484 0.024435559 0.022638893 0.022371335 0.023608323 0.026032377 0.028802324 0.031064365 0.0320982 0.031509321 0.029890958 0.028299674 0.027626831 0.027433889 0.027215775][0.025503259 0.02391012 0.021874482 0.021205671 0.022021513 0.024128705 0.026679613 0.028789161 0.029760377 0.029254168 0.027849436 0.026637539 0.026532395 0.027071802 0.027502205][0.025843425 0.024036288 0.021624042 0.020338869 0.020470874 0.021977957 0.024063699 0.025877675 0.026790632 0.026482839 0.0254255 0.024608113 0.024901815 0.0259147 0.026783917]]...]
INFO - root - 2017-12-10 09:30:39.185136: step 2110, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.765 sec/batch; 70h:10m:41s remains)
INFO - root - 2017-12-10 09:30:46.859028: step 2120, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.791 sec/batch; 72h:34m:52s remains)
INFO - root - 2017-12-10 09:30:54.880417: step 2130, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.772 sec/batch; 70h:51m:39s remains)
INFO - root - 2017-12-10 09:31:02.764939: step 2140, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.806 sec/batch; 73h:57m:47s remains)
INFO - root - 2017-12-10 09:31:10.683489: step 2150, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.775 sec/batch; 71h:06m:29s remains)
INFO - root - 2017-12-10 09:31:18.561616: step 2160, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.773 sec/batch; 70h:55m:17s remains)
INFO - root - 2017-12-10 09:31:26.285705: step 2170, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.786 sec/batch; 72h:08m:05s remains)
INFO - root - 2017-12-10 09:31:34.246113: step 2180, loss = 0.75, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 74h:19m:30s remains)
INFO - root - 2017-12-10 09:31:42.121379: step 2190, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.787 sec/batch; 72h:11m:41s remains)
INFO - root - 2017-12-10 09:31:49.783751: step 2200, loss = 0.75, batch loss = 0.69 (11.0 examples/sec; 0.727 sec/batch; 66h:39m:55s remains)
2017-12-10 09:31:50.628754: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.03144278 0.0330619 0.031447213 0.027248302 0.021982752 0.017027184 0.013453375 0.011350058 0.010094197 0.0090838745 0.0080833156 0.0071431817 0.0064055487 0.0060657333 0.0060547716][0.036558893 0.038375165 0.036412019 0.031681851 0.025941759 0.020757612 0.017242081 0.015212649 0.01376042 0.012143532 0.010310342 0.0085559422 0.0071963877 0.0065152943 0.0063904552][0.039729808 0.041422408 0.039114039 0.03420848 0.028652638 0.024058744 0.021295043 0.019800508 0.018354792 0.016101705 0.013251348 0.010401096 0.0081673469 0.0069532436 0.0065766317][0.040826127 0.042154055 0.039568171 0.034905978 0.030204127 0.026922867 0.025539463 0.025000878 0.023825338 0.021040082 0.017143507 0.013063542 0.00974155 0.0077559846 0.0069092419][0.03956677 0.040485773 0.037889071 0.03395012 0.03067101 0.029255202 0.029622884 0.030283937 0.029511828 0.026318947 0.021474222 0.016216932 0.011793708 0.0089690471 0.0075479378][0.03684302 0.037491117 0.035200145 0.032309722 0.030701678 0.03128824 0.033420846 0.03516848 0.034678984 0.031131025 0.025476875 0.019188447 0.013801209 0.01023977 0.00826388][0.033860903 0.034458622 0.032710459 0.030901739 0.030808115 0.03306016 0.0365474 0.038997453 0.038561393 0.034707833 0.028488209 0.021546248 0.015550215 0.011460599 0.00902613][0.031362422 0.032181002 0.031078462 0.030181119 0.031140979 0.034342479 0.038421236 0.041032344 0.040437896 0.036376126 0.029956058 0.022810113 0.016589299 0.012252241 0.0095441872][0.030228006 0.031390343 0.030730788 0.03028712 0.031589 0.034852434 0.038714588 0.040899742 0.039948739 0.035797093 0.02950904 0.022625605 0.016619999 0.012369369 0.0096418876][0.031218717 0.032559287 0.03188945 0.031242175 0.032111354 0.034634165 0.037618864 0.038940668 0.037398461 0.033088621 0.027043454 0.020673607 0.015239081 0.011457752 0.0090560028][0.033648685 0.034807391 0.03366942 0.032343462 0.032370903 0.033887032 0.03588669 0.036327519 0.034205124 0.029705144 0.02385138 0.017961016 0.013148648 0.0099675292 0.0080532767][0.035991162 0.036692873 0.034823604 0.032698479 0.031921193 0.032645594 0.033976268 0.03396409 0.031615488 0.027070116 0.021378644 0.015827226 0.011445487 0.0086814845 0.0071365121][0.036969621 0.037175853 0.034737986 0.032151181 0.031010516 0.031564046 0.032877885 0.032950785 0.030728471 0.02623596 0.020599537 0.015097006 0.010815335 0.0081926556 0.0067969896][0.036311962 0.036208339 0.033624314 0.031162661 0.030400449 0.031569511 0.033535492 0.034140985 0.032147523 0.027586997 0.021740912 0.016032796 0.01167614 0.0090846187 0.007706834][0.034468029 0.034326252 0.032057628 0.030232605 0.030413015 0.032737359 0.03575854 0.037045747 0.0351989 0.030424412 0.024210915 0.018223062 0.013832646 0.011350885 0.0099793756]]...]
INFO - root - 2017-12-10 09:31:58.591740: step 2210, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.787 sec/batch; 72h:11m:39s remains)
INFO - root - 2017-12-10 09:32:06.464138: step 2220, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 71h:47m:04s remains)
INFO - root - 2017-12-10 09:32:14.342852: step 2230, loss = 0.75, batch loss = 0.69 (10.5 examples/sec; 0.765 sec/batch; 70h:10m:51s remains)
INFO - root - 2017-12-10 09:32:22.230104: step 2240, loss = 0.75, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 74h:54m:34s remains)
INFO - root - 2017-12-10 09:32:30.129313: step 2250, loss = 0.75, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 75h:19m:59s remains)
INFO - root - 2017-12-10 09:32:38.111192: step 2260, loss = 0.75, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 75h:51m:43s remains)
INFO - root - 2017-12-10 09:32:45.955803: step 2270, loss = 0.75, batch loss = 0.69 (10.6 examples/sec; 0.753 sec/batch; 69h:05m:43s remains)
INFO - root - 2017-12-10 09:32:53.639971: step 2280, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.784 sec/batch; 71h:53m:14s remains)
INFO - root - 2017-12-10 09:33:01.497412: step 2290, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 71h:20m:33s remains)
INFO - root - 2017-12-10 09:33:09.371690: step 2300, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.786 sec/batch; 72h:03m:16s remains)
2017-12-10 09:33:10.250339: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033318039 0.03529346 0.037575383 0.04037806 0.0437028 0.047823045 0.052545276 0.056336045 0.058339346 0.058673494 0.058162965 0.056522951 0.053434603 0.049969148 0.046674583][0.035361171 0.037975505 0.04089836 0.044171933 0.047799706 0.052091327 0.056958307 0.060590159 0.062146217 0.061806742 0.060372237 0.057621 0.053354383 0.048883993 0.044828836][0.034856241 0.038309075 0.042039186 0.045898385 0.049718577 0.05384589 0.058328625 0.061376598 0.062217012 0.061069515 0.058758028 0.055065066 0.049888544 0.044606257 0.039969184][0.033998955 0.03823157 0.042688731 0.047065448 0.050940312 0.054597259 0.058192272 0.060267709 0.060223285 0.058413167 0.0556317 0.051612061 0.046151452 0.040472254 0.035488911][0.034812391 0.03947651 0.0442528 0.048778269 0.052342877 0.055152096 0.05751111 0.058411624 0.05744952 0.055151783 0.052245509 0.048331454 0.043039456 0.037294026 0.03216286][0.038306993 0.042882387 0.04735823 0.051380407 0.054140687 0.055880584 0.057021059 0.05688218 0.055189289 0.052583866 0.049643982 0.045823038 0.040711444 0.034967039 0.029699033][0.04366843 0.04755833 0.050984859 0.053907059 0.055636484 0.056468546 0.056784809 0.056089856 0.054078959 0.051362634 0.048272442 0.044269163 0.039068196 0.033210628 0.02779757][0.048738193 0.051514566 0.053391062 0.054875605 0.055569135 0.055846427 0.0558338 0.055100635 0.053170078 0.050489869 0.047108654 0.04261139 0.03707673 0.031105669 0.025720552][0.052015573 0.053519905 0.053670436 0.053604618 0.053332873 0.053313959 0.053320706 0.052835856 0.051192962 0.048663203 0.045016948 0.040065248 0.034321338 0.028508684 0.023440935][0.052980307 0.053362712 0.052100182 0.050847739 0.049962245 0.049881574 0.05004473 0.049720589 0.048214786 0.045675319 0.041717798 0.036420003 0.030673875 0.025272891 0.020754931][0.051550347 0.051394518 0.049468424 0.047747903 0.046752956 0.046781026 0.046983123 0.046457347 0.044659898 0.041753519 0.037377894 0.031846084 0.026318079 0.021572499 0.017844029][0.048613679 0.0484776 0.046560787 0.044978078 0.044151917 0.04413804 0.044005617 0.04291695 0.0405535 0.037151583 0.032501817 0.027085472 0.02208798 0.018188661 0.015405181][0.045648973 0.045758408 0.044115908 0.042761307 0.041879289 0.041371133 0.040449183 0.038524613 0.035521187 0.031777352 0.027246334 0.022424038 0.018299183 0.015371891 0.013535854][0.043642908 0.0437573 0.042160157 0.040669404 0.039273966 0.03784534 0.035897523 0.0331934 0.02983279 0.026179112 0.022254327 0.018470833 0.015467465 0.013534741 0.012521176][0.041666389 0.041514017 0.039741389 0.037888259 0.035843812 0.033526544 0.030809637 0.027733635 0.024463562 0.021287674 0.018247105 0.015622983 0.0137197 0.012654479 0.012261114]]...]
INFO - root - 2017-12-10 09:33:18.195054: step 2310, loss = 0.75, batch loss = 0.69 (10.3 examples/sec; 0.773 sec/batch; 70h:54m:58s remains)
INFO - root - 2017-12-10 09:33:26.080755: step 2320, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.793 sec/batch; 72h:43m:06s remains)
INFO - root - 2017-12-10 09:33:33.957146: step 2330, loss = 0.74, batch loss = 0.69 (10.1 examples/sec; 0.789 sec/batch; 72h:20m:19s remains)
INFO - root - 2017-12-10 09:33:41.819637: step 2340, loss = 0.74, batch loss = 0.69 (9.9 examples/sec; 0.806 sec/batch; 73h:53m:26s remains)
INFO - root - 2017-12-10 09:33:49.518457: step 2350, loss = 0.75, batch loss = 0.69 (10.1 examples/sec; 0.795 sec/batch; 72h:56m:39s remains)
INFO - root - 2017-12-10 09:33:57.312410: step 2360, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 73h:32m:16s remains)
INFO - root - 2017-12-10 09:34:05.290916: step 2370, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.787 sec/batch; 72h:08m:09s remains)
INFO - root - 2017-12-10 09:34:13.168487: step 2380, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.787 sec/batch; 72h:11m:00s remains)
INFO - root - 2017-12-10 09:34:21.004769: step 2390, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.773 sec/batch; 70h:51m:13s remains)
INFO - root - 2017-12-10 09:34:28.913389: step 2400, loss = 0.74, batch loss = 0.68 (9.8 examples/sec; 0.813 sec/batch; 74h:35m:25s remains)
2017-12-10 09:34:29.715370: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.02099767 0.021881772 0.022430867 0.023121133 0.023732884 0.023847517 0.023501154 0.022951031 0.022647075 0.022423565 0.022017796 0.021486752 0.020727824 0.019196237 0.016628454][0.029556118 0.03119177 0.032208551 0.033270631 0.034090452 0.034070268 0.033264536 0.032214038 0.031568278 0.031161897 0.030512292 0.029685194 0.028621877 0.02644312 0.022728723][0.037429333 0.039823022 0.041431136 0.04307092 0.0443388 0.044399109 0.043271061 0.041600589 0.040384982 0.039563965 0.038523108 0.0373175 0.035913114 0.033130378 0.028346686][0.043211259 0.046094149 0.048139643 0.050290741 0.052108165 0.052560616 0.051466059 0.049371734 0.047554757 0.046238154 0.044787347 0.043202989 0.041435786 0.038161945 0.032610528][0.046305805 0.04934762 0.051484127 0.053835262 0.056132257 0.057259541 0.056628842 0.054529402 0.052338541 0.050595447 0.048773434 0.046798982 0.044600151 0.040884122 0.0348854][0.047793649 0.050492611 0.052314524 0.054562908 0.057223618 0.059239313 0.059568424 0.058081821 0.05600097 0.054085433 0.051994953 0.049578439 0.046711773 0.042325977 0.035805672][0.047780111 0.049670953 0.050787553 0.052663356 0.055567805 0.058548059 0.060237851 0.0599563 0.058546536 0.056853335 0.054728962 0.052033197 0.048473794 0.043251324 0.036087036][0.0469379 0.047705386 0.047747537 0.048870906 0.051624775 0.055246487 0.058241963 0.059315514 0.058897037 0.057761751 0.055934504 0.053300403 0.049354255 0.043538984 0.035896719][0.046492457 0.046321772 0.045241341 0.045380291 0.047583085 0.05132322 0.055107418 0.0572861 0.057905093 0.057548743 0.056233913 0.053860079 0.049834769 0.043674737 0.035713904][0.046389949 0.045775466 0.043989033 0.043381508 0.044936739 0.048290681 0.052070647 0.054644443 0.055875812 0.056156579 0.05528136 0.053160489 0.049250126 0.043085068 0.035075288][0.044555485 0.043998558 0.042192012 0.041407306 0.042435545 0.044985943 0.048034642 0.050250322 0.051540043 0.052145563 0.051611621 0.049802516 0.046279572 0.040601313 0.033052366][0.038893662 0.038656857 0.037279665 0.036611788 0.037096482 0.03852332 0.040359598 0.041722652 0.042654645 0.043267541 0.043011717 0.041703813 0.039006859 0.034476139 0.028234769][0.030696442 0.030779636 0.029880045 0.029289776 0.029182332 0.029471722 0.030087419 0.030592373 0.031054659 0.031524297 0.031493988 0.030760495 0.029066205 0.025985472 0.021543194][0.022168651 0.022419419 0.02183895 0.021290954 0.020822812 0.020490555 0.020451965 0.020569142 0.020849084 0.021239808 0.021358795 0.021015348 0.020027734 0.018065747 0.015182303][0.014524988 0.014727001 0.014310225 0.01385759 0.01340733 0.013021073 0.012888154 0.013003739 0.013301763 0.01366765 0.013814687 0.013609362 0.012967883 0.011742748 0.010031992]]...]
INFO - root - 2017-12-10 09:34:37.644488: step 2410, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.770 sec/batch; 70h:38m:11s remains)
INFO - root - 2017-12-10 09:34:45.461205: step 2420, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.768 sec/batch; 70h:23m:20s remains)
INFO - root - 2017-12-10 09:34:53.242456: step 2430, loss = 0.74, batch loss = 0.69 (12.1 examples/sec; 0.662 sec/batch; 60h:41m:43s remains)
INFO - root - 2017-12-10 09:35:00.932490: step 2440, loss = 0.74, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 74h:38m:59s remains)
INFO - root - 2017-12-10 09:35:08.813649: step 2450, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.788 sec/batch; 72h:13m:23s remains)
INFO - root - 2017-12-10 09:35:16.686000: step 2460, loss = 0.74, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 71h:43m:11s remains)
INFO - root - 2017-12-10 09:35:24.592214: step 2470, loss = 0.74, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 73h:08m:13s remains)
INFO - root - 2017-12-10 09:35:32.582128: step 2480, loss = 0.74, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 71h:43m:52s remains)
INFO - root - 2017-12-10 09:35:40.506069: step 2490, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 73h:09m:50s remains)
INFO - root - 2017-12-10 09:35:48.423439: step 2500, loss = 0.74, batch loss = 0.69 (10.2 examples/sec; 0.784 sec/batch; 71h:53m:07s remains)
2017-12-10 09:35:49.270568: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022797866 0.024227437 0.024819955 0.0247296 0.024181372 0.023527749 0.022915071 0.021978492 0.020594308 0.019381795 0.018614372 0.018061401 0.017667431 0.017383758 0.016893838][0.03245984 0.035269625 0.036966845 0.037546251 0.037175693 0.036362953 0.035465129 0.033952225 0.031537838 0.029434675 0.028129203 0.027310556 0.026786163 0.026446542 0.025736252][0.04410629 0.048658341 0.051719181 0.053184528 0.053208061 0.052510142 0.051681895 0.049776893 0.046238355 0.042907029 0.040794406 0.039612912 0.038855914 0.038297486 0.037160043][0.056014962 0.062317912 0.066743687 0.069140583 0.069699764 0.06938611 0.069051437 0.067143694 0.062685907 0.058009915 0.054840833 0.053052988 0.051750626 0.050599221 0.048748005][0.065616183 0.073579758 0.079291888 0.082620777 0.083740816 0.083866365 0.084083095 0.082317747 0.077176154 0.071232475 0.066883773 0.064278416 0.062208954 0.060155775 0.057375889][0.071503729 0.080833621 0.087594248 0.0918471 0.093682058 0.094348237 0.095088027 0.0935387 0.088043138 0.081051975 0.075448126 0.0717614 0.068674974 0.065545961 0.061763581][0.075151682 0.085956015 0.093748644 0.098934956 0.101532 0.10280798 0.10398722 0.1026414 0.0971048 0.089524478 0.083017565 0.078336112 0.0742139 0.06996163 0.065112486][0.077341221 0.089713775 0.098464854 0.10435012 0.10733165 0.10872278 0.10987405 0.1085472 0.10339902 0.0963164 0.090097338 0.085324511 0.080616064 0.0754125 0.069478579][0.078827083 0.092685439 0.10219762 0.10845198 0.11120698 0.11183161 0.11206321 0.11038492 0.10615236 0.1009709 0.096646525 0.092963152 0.0882841 0.08227744 0.075024873][0.079946876 0.094648585 0.10424995 0.1102692 0.11222526 0.11140195 0.11003897 0.10771815 0.10460512 0.10198676 0.10034528 0.098465428 0.0944258 0.088146672 0.079849586][0.079774164 0.094030134 0.10265129 0.10761862 0.10843767 0.10621502 0.10348408 0.10072234 0.098690771 0.098471239 0.099456765 0.0995675 0.096588768 0.090687923 0.082150549][0.077233247 0.089866787 0.0966905 0.099946052 0.099319369 0.095885135 0.092202626 0.0892204 0.088008083 0.0895723 0.09260349 0.094425328 0.092639349 0.087557346 0.079593465][0.069152594 0.079067931 0.0836857 0.085190177 0.083390944 0.079247668 0.075227983 0.072446875 0.07198064 0.074578963 0.078681149 0.081522085 0.080748677 0.07669612 0.069864541][0.056073617 0.062839791 0.06534633 0.065466858 0.06303817 0.058818813 0.054993961 0.052689549 0.052729364 0.05542117 0.059341058 0.062206049 0.062109951 0.059274647 0.054117829][0.041044503 0.044877034 0.045688219 0.044894937 0.042430643 0.038870081 0.035809942 0.034087367 0.034193184 0.036097251 0.038775936 0.040842675 0.041095838 0.039584327 0.03646417]]...]
INFO - root - 2017-12-10 09:35:57.150984: step 2510, loss = 0.74, batch loss = 0.69 (10.1 examples/sec; 0.793 sec/batch; 72h:41m:57s remains)
INFO - root - 2017-12-10 09:36:04.837089: step 2520, loss = 0.74, batch loss = 0.69 (10.0 examples/sec; 0.800 sec/batch; 73h:20m:55s remains)
INFO - root - 2017-12-10 09:36:12.728192: step 2530, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.788 sec/batch; 72h:11m:32s remains)
INFO - root - 2017-12-10 09:36:20.528009: step 2540, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.799 sec/batch; 73h:13m:51s remains)
INFO - root - 2017-12-10 09:36:28.451975: step 2550, loss = 0.74, batch loss = 0.68 (10.1 examples/sec; 0.792 sec/batch; 72h:35m:22s remains)
INFO - root - 2017-12-10 09:36:36.234425: step 2560, loss = 0.75, batch loss = 0.69 (10.6 examples/sec; 0.757 sec/batch; 69h:23m:57s remains)
INFO - root - 2017-12-10 09:36:44.092876: step 2570, loss = 0.74, batch loss = 0.68 (10.2 examples/sec; 0.786 sec/batch; 72h:02m:42s remains)
INFO - root - 2017-12-10 09:36:52.068267: step 2580, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.802 sec/batch; 73h:27m:48s remains)
INFO - root - 2017-12-10 09:36:59.982692: step 2590, loss = 0.74, batch loss = 0.69 (10.1 examples/sec; 0.789 sec/batch; 72h:18m:49s remains)
INFO - root - 2017-12-10 09:37:07.692948: step 2600, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.800 sec/batch; 73h:20m:22s remains)
2017-12-10 09:37:08.483941: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033756495 0.036905248 0.040039185 0.043129753 0.0455714 0.047420565 0.049037617 0.0507203 0.052318994 0.053486105 0.053698909 0.053150114 0.053032815 0.05364316 0.055317298][0.034853525 0.037254695 0.040017724 0.04335583 0.046603106 0.049536135 0.052314915 0.055073529 0.057224073 0.05836758 0.058124352 0.056963488 0.05627434 0.056432832 0.058079578][0.035521321 0.03715485 0.039541971 0.043272384 0.047641091 0.052078042 0.056188274 0.059707638 0.061737224 0.061932564 0.060435902 0.058159564 0.056650795 0.05627716 0.05766372][0.037743319 0.03968272 0.04257277 0.04728068 0.053007662 0.058877818 0.063832164 0.067192435 0.068013296 0.066240311 0.062728763 0.058813166 0.056098808 0.054918002 0.055813476][0.042212214 0.04571835 0.049935 0.055787761 0.062509894 0.069078512 0.073887073 0.076031484 0.074893147 0.0707442 0.064996846 0.059201427 0.055067014 0.053021122 0.053448714][0.047947016 0.05361075 0.059294574 0.065946668 0.072929412 0.079189487 0.082826965 0.083018541 0.079639539 0.073345363 0.065858431 0.058765471 0.053791743 0.051472615 0.051941458][0.053651635 0.061107863 0.067739181 0.074547261 0.081000052 0.086128928 0.088001147 0.086055607 0.080799408 0.073181421 0.064974308 0.057469226 0.052435618 0.050561797 0.051616468][0.058213767 0.066449516 0.073056474 0.079120032 0.084154971 0.0874773 0.08734934 0.083665028 0.077435747 0.069666356 0.061961588 0.055174664 0.051035691 0.050372846 0.052472476][0.060280539 0.068263546 0.0740398 0.078651249 0.0816274 0.082713827 0.080683142 0.075934686 0.069733828 0.0629276 0.056727029 0.051480159 0.048756987 0.049438424 0.052509665][0.058595639 0.065317154 0.069612704 0.072379306 0.073257484 0.072447218 0.069294989 0.0645282 0.059290294 0.054200504 0.050005402 0.046659343 0.045476392 0.047275882 0.05091165][0.052797075 0.057664145 0.060297802 0.061507542 0.061029445 0.059372332 0.056265451 0.052465517 0.048703674 0.045414768 0.0430721 0.041430734 0.041519068 0.043948777 0.047597975][0.044508085 0.0474445 0.048601069 0.048754726 0.047737222 0.046234265 0.044121403 0.0419014 0.039799049 0.038038529 0.036972709 0.036329377 0.036998719 0.039437838 0.042658571][0.035451472 0.036708631 0.036762577 0.03631353 0.035291526 0.034406237 0.033549488 0.032890573 0.032222994 0.03156703 0.031256832 0.031150445 0.032038882 0.03428179 0.037042864][0.026282093 0.026635591 0.026316453 0.02584832 0.025201656 0.024963232 0.02503507 0.025346704 0.025541225 0.025529675 0.025611494 0.025764616 0.026619058 0.028424174 0.03053241][0.018140864 0.018373128 0.018278161 0.018183114 0.018058445 0.018293902 0.01880982 0.019471433 0.019971287 0.020210763 0.02044552 0.02066992 0.021315336 0.022511087 0.02382594]]...]
INFO - root - 2017-12-10 09:37:16.183540: step 2610, loss = 0.74, batch loss = 0.68 (10.2 examples/sec; 0.786 sec/batch; 72h:00m:54s remains)
INFO - root - 2017-12-10 09:37:23.965224: step 2620, loss = 0.74, batch loss = 0.68 (10.5 examples/sec; 0.763 sec/batch; 69h:54m:10s remains)
INFO - root - 2017-12-10 09:37:31.954637: step 2630, loss = 0.74, batch loss = 0.69 (10.4 examples/sec; 0.766 sec/batch; 70h:10m:18s remains)
INFO - root - 2017-12-10 09:37:39.823069: step 2640, loss = 0.74, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 73h:29m:56s remains)
INFO - root - 2017-12-10 09:37:47.720257: step 2650, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.796 sec/batch; 72h:58m:34s remains)
INFO - root - 2017-12-10 09:37:55.569378: step 2660, loss = 0.75, batch loss = 0.69 (10.4 examples/sec; 0.766 sec/batch; 70h:10m:36s remains)
INFO - root - 2017-12-10 09:38:03.410098: step 2670, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 73h:28m:02s remains)
INFO - root - 2017-12-10 09:38:11.028990: step 2680, loss = 0.74, batch loss = 0.69 (10.1 examples/sec; 0.792 sec/batch; 72h:35m:47s remains)
INFO - root - 2017-12-10 09:38:18.920529: step 2690, loss = 0.74, batch loss = 0.68 (10.4 examples/sec; 0.768 sec/batch; 70h:22m:08s remains)
INFO - root - 2017-12-10 09:38:26.653431: step 2700, loss = 0.74, batch loss = 0.68 (10.2 examples/sec; 0.786 sec/batch; 72h:00m:01s remains)
2017-12-10 09:38:27.586264: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.050307333 0.044515025 0.038817238 0.034802586 0.032643974 0.031842936 0.031679172 0.031696919 0.031799011 0.031824984 0.031673033 0.031413477 0.031195527 0.031159943 0.03140508][0.050897047 0.044712704 0.038822528 0.0348537 0.033011056 0.032723159 0.033049889 0.033389259 0.033630352 0.033610146 0.033224776 0.032656297 0.03216508 0.03194198 0.032119136][0.049024556 0.043079168 0.037585851 0.034179583 0.033107623 0.033680171 0.034703895 0.035364371 0.0355037 0.035139486 0.034156319 0.032869585 0.031818047 0.031282112 0.031395767][0.047061738 0.041774884 0.037100341 0.034763765 0.035021991 0.03692269 0.038896952 0.039923333 0.039898708 0.038914032 0.036983881 0.034684628 0.03289666 0.031995755 0.031991106][0.046498965 0.042208258 0.038416747 0.037259784 0.039070055 0.042579103 0.0456835 0.047222529 0.047240827 0.045846887 0.043150827 0.039927334 0.037515074 0.036316969 0.03616447][0.048573092 0.045193516 0.041912362 0.041493788 0.04439329 0.049068093 0.052930124 0.054850575 0.055107109 0.0537355 0.050829738 0.047281608 0.044698689 0.043435235 0.043146245][0.052213121 0.049786456 0.046729922 0.046534751 0.049772814 0.054856177 0.058917619 0.060892358 0.061417706 0.060471881 0.058086421 0.05503615 0.052904367 0.051973492 0.051704809][0.056088537 0.054692097 0.051941469 0.05175101 0.054764722 0.059535954 0.06322208 0.064942233 0.065606877 0.065147512 0.063617371 0.061628334 0.060469396 0.060247887 0.060345776][0.05826493 0.0580319 0.0557691 0.055494376 0.057936024 0.061995119 0.065082431 0.066502549 0.067306243 0.067357272 0.066773824 0.065966666 0.065859519 0.066459768 0.067073815][0.058107231 0.058832735 0.057047214 0.056613766 0.058344685 0.061561264 0.064068913 0.065283693 0.066296361 0.066977143 0.067337155 0.067529716 0.068083957 0.069110833 0.069949955][0.056146052 0.057904869 0.056927785 0.056736354 0.058061816 0.06060553 0.06261123 0.063480906 0.064387329 0.065368548 0.066356264 0.067076728 0.0676046 0.068376638 0.0690024][0.053488221 0.056602802 0.057005886 0.057704132 0.05919455 0.061437041 0.063050449 0.063292406 0.063476533 0.06403093 0.064961337 0.065536745 0.065539993 0.065831758 0.066396095][0.051705275 0.055650733 0.057276316 0.058872469 0.060703173 0.062914491 0.064364217 0.064061418 0.063293375 0.062921576 0.063108534 0.062914371 0.062099587 0.061920539 0.062718019][0.05092537 0.055040818 0.057099845 0.058988281 0.060855549 0.063003145 0.064438924 0.063893571 0.062439639 0.061144281 0.060283717 0.058982462 0.05718606 0.056435656 0.057402242][0.050295413 0.053816173 0.055333294 0.056619048 0.057835627 0.059483353 0.06078881 0.060292225 0.058720689 0.057035569 0.055572994 0.053643748 0.051374033 0.050377496 0.051616821]]...]
INFO - root - 2017-12-10 09:38:35.492563: step 2710, loss = 0.75, batch loss = 0.69 (10.2 examples/sec; 0.787 sec/batch; 72h:07m:42s remains)
INFO - root - 2017-12-10 09:38:43.353774: step 2720, loss = 0.74, batch loss = 0.68 (10.1 examples/sec; 0.794 sec/batch; 72h:46m:15s remains)
INFO - root - 2017-12-10 09:38:51.209712: step 2730, loss = 0.74, batch loss = 0.68 (9.8 examples/sec; 0.815 sec/batch; 74h:39m:45s remains)
INFO - root - 2017-12-10 09:38:59.041133: step 2740, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.780 sec/batch; 71h:28m:39s remains)
INFO - root - 2017-12-10 09:39:06.870301: step 2750, loss = 0.74, batch loss = 0.68 (10.4 examples/sec; 0.770 sec/batch; 70h:31m:49s remains)
INFO - root - 2017-12-10 09:39:14.583638: step 2760, loss = 0.74, batch loss = 0.68 (10.1 examples/sec; 0.796 sec/batch; 72h:53m:07s remains)
INFO - root - 2017-12-10 09:39:22.499876: step 2770, loss = 0.74, batch loss = 0.68 (9.7 examples/sec; 0.825 sec/batch; 75h:31m:58s remains)
INFO - root - 2017-12-10 09:39:30.397785: step 2780, loss = 0.74, batch loss = 0.68 (10.2 examples/sec; 0.783 sec/batch; 71h:40m:25s remains)
INFO - root - 2017-12-10 09:39:38.092039: step 2790, loss = 0.74, batch loss = 0.68 (9.9 examples/sec; 0.805 sec/batch; 73h:45m:31s remains)
INFO - root - 2017-12-10 09:39:46.031806: step 2800, loss = 0.74, batch loss = 0.68 (10.1 examples/sec; 0.795 sec/batch; 72h:47m:50s remains)
2017-12-10 09:39:46.911167: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.03949495 0.042091034 0.046334986 0.052683719 0.059652764 0.066337876 0.072764806 0.077332735 0.078127518 0.075295016 0.070037134 0.062865719 0.053796716 0.043998618 0.035441112][0.041632306 0.043939456 0.047641255 0.05347224 0.060220446 0.066967018 0.073410437 0.077956229 0.078749083 0.076083392 0.070784166 0.063182734 0.053499505 0.043359693 0.034824692][0.045883238 0.047468074 0.049934074 0.054508302 0.060396343 0.066534296 0.072324708 0.076295979 0.076977052 0.074840806 0.070193231 0.063154615 0.054042254 0.044669166 0.037042528][0.054562654 0.055443354 0.056215607 0.059067708 0.063629486 0.068654545 0.07337679 0.076740712 0.077732652 0.076659493 0.073349319 0.067593627 0.059659626 0.051348675 0.044515431][0.065548167 0.065859064 0.065095432 0.066602804 0.070044026 0.073751986 0.07702782 0.079638973 0.080975823 0.08100275 0.079284064 0.075433567 0.069363773 0.06255313 0.056693628][0.075612344 0.075992174 0.07457485 0.075754441 0.078652762 0.081067838 0.082657591 0.084308036 0.0856563 0.086300038 0.085875921 0.08409822 0.080331266 0.075406104 0.070716679][0.082724035 0.083568804 0.082195349 0.083755128 0.086583473 0.087974019 0.088044927 0.088725664 0.089667529 0.090049505 0.089908496 0.089519188 0.087842286 0.084829979 0.081335433][0.08441101 0.085466631 0.084306352 0.086369283 0.089323781 0.09016823 0.089320973 0.089286834 0.089749545 0.089491673 0.089165017 0.089658514 0.089653082 0.088282995 0.085834853][0.079996139 0.081145674 0.080549978 0.083495468 0.0871203 0.088141762 0.087041907 0.086470231 0.086221382 0.084999517 0.084104821 0.0848924 0.085973695 0.085915111 0.084622681][0.070418343 0.071994394 0.072855689 0.077586614 0.082772814 0.084804006 0.083788037 0.082424335 0.080746278 0.077824757 0.07566388 0.0761565 0.078110762 0.079748034 0.080646306][0.059170239 0.061659213 0.064526267 0.071359038 0.078369983 0.081818655 0.081330881 0.079469271 0.076455295 0.071822695 0.06811256 0.067629844 0.069608927 0.07226789 0.075053126][0.049523655 0.052941646 0.05754124 0.065760151 0.0739135 0.078507848 0.078701295 0.076740526 0.073008761 0.067517661 0.062944964 0.061487343 0.062758222 0.06530121 0.068709046][0.043188579 0.048092213 0.054503664 0.063698359 0.072394349 0.077457771 0.077733494 0.075222261 0.070822723 0.065099381 0.060465124 0.058522128 0.05902236 0.060923174 0.064011067][0.040944926 0.047941372 0.056372479 0.066580117 0.075545907 0.080413483 0.079992548 0.076333806 0.071029224 0.06510514 0.060618564 0.05848217 0.058481257 0.059930071 0.062614672][0.041848663 0.050729088 0.060920093 0.072068393 0.081140086 0.085557237 0.084210075 0.079255618 0.072833911 0.066367432 0.061750945 0.059484422 0.059376016 0.060990129 0.063931435]]...]
INFO - root - 2017-12-10 09:39:54.827306: step 2810, loss = 0.74, batch loss = 0.68 (10.5 examples/sec; 0.765 sec/batch; 70h:03m:40s remains)
INFO - root - 2017-12-10 09:40:02.589221: step 2820, loss = 0.74, batch loss = 0.68 (10.6 examples/sec; 0.757 sec/batch; 69h:17m:36s remains)
INFO - root - 2017-12-10 09:40:10.443793: step 2830, loss = 0.74, batch loss = 0.68 (10.4 examples/sec; 0.768 sec/batch; 70h:17m:49s remains)
INFO - root - 2017-12-10 09:40:18.119287: step 2840, loss = 0.74, batch loss = 0.68 (10.1 examples/sec; 0.793 sec/batch; 72h:36m:42s remains)
INFO - root - 2017-12-10 09:40:25.951841: step 2850, loss = 0.74, batch loss = 0.68 (10.1 examples/sec; 0.793 sec/batch; 72h:38m:26s remains)
INFO - root - 2017-12-10 09:40:33.895566: step 2860, loss = 0.74, batch loss = 0.68 (10.1 examples/sec; 0.794 sec/batch; 72h:43m:33s remains)
INFO - root - 2017-12-10 09:40:41.864164: step 2870, loss = 0.74, batch loss = 0.68 (10.1 examples/sec; 0.791 sec/batch; 72h:23m:50s remains)
INFO - root - 2017-12-10 09:40:49.607442: step 2880, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.799 sec/batch; 73h:11m:36s remains)
INFO - root - 2017-12-10 09:40:57.598965: step 2890, loss = 0.74, batch loss = 0.69 (10.3 examples/sec; 0.774 sec/batch; 70h:49m:22s remains)
INFO - root - 2017-12-10 09:41:05.511033: step 2900, loss = 0.74, batch loss = 0.68 (10.4 examples/sec; 0.767 sec/batch; 70h:11m:08s remains)
2017-12-10 09:41:06.377230: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.016783265 0.0167007 0.01745419 0.01966182 0.022854546 0.026306609 0.029033532 0.030264562 0.029396841 0.026637195 0.023032995 0.019637564 0.017244929 0.01618368 0.016289409][0.017764959 0.018315258 0.019742386 0.022341173 0.025595393 0.028767949 0.030986737 0.031701833 0.030560637 0.027764816 0.024191786 0.020850372 0.018502228 0.017353475 0.017253011][0.019212073 0.02135795 0.024815436 0.029702313 0.035300981 0.040518548 0.044027567 0.045122325 0.043541551 0.039556958 0.034055151 0.02841866 0.023907617 0.020945335 0.01943752][0.021440236 0.025924422 0.03245528 0.04122363 0.051285818 0.060883686 0.067592837 0.070044257 0.0677799 0.0611654 0.0514854 0.040995959 0.032047343 0.025624029 0.021842886][0.02405866 0.031157518 0.041174084 0.054569416 0.070265673 0.085731424 0.096951865 0.10141285 0.098236449 0.088005729 0.072755516 0.055941585 0.041343618 0.030663062 0.024165338][0.026235577 0.03563825 0.048800383 0.0664381 0.087346055 0.10834073 0.12386423 0.13021024 0.12604919 0.11233971 0.091914736 0.069326892 0.049614742 0.035085514 0.026055787][0.027274327 0.037868164 0.052676849 0.072537631 0.096076265 0.11990015 0.13766028 0.14505424 0.14048639 0.12509482 0.10208924 0.076537654 0.054118268 0.037447236 0.02691528][0.027724221 0.037855417 0.051808637 0.070454925 0.092505068 0.1149646 0.13183613 0.13906772 0.13506122 0.12066812 0.098803349 0.074240342 0.05254608 0.036328685 0.026040604][0.030123232 0.038539018 0.049267888 0.063219234 0.079595104 0.096449159 0.10926122 0.11495245 0.11205833 0.10082863 0.0833154 0.0633058 0.045491435 0.032154825 0.023742817][0.038282935 0.045315858 0.052140482 0.059619743 0.067673966 0.0758327 0.081806228 0.084038906 0.081441365 0.073700935 0.061789975 0.048101764 0.03589135 0.02678062 0.021100489][0.053488132 0.060901783 0.064997271 0.066397034 0.065638147 0.063768335 0.061057679 0.057891935 0.053842478 0.048258558 0.041159686 0.03350383 0.026866382 0.021969266 0.018985944][0.07397791 0.083864614 0.0871077 0.083978795 0.075515412 0.064103834 0.052407473 0.042830218 0.035787228 0.030498475 0.026254578 0.022855431 0.020402258 0.018741013 0.017828925][0.095341682 0.10880625 0.11266199 0.10682796 0.092791006 0.073900208 0.054618444 0.039031327 0.028379977 0.021949798 0.018551251 0.017224558 0.01705901 0.017265823 0.017562965][0.1106574 0.12678917 0.1315712 0.12474413 0.10802746 0.0851435 0.061418042 0.041871004 0.028359637 0.02041146 0.016667902 0.015806543 0.016390502 0.017225388 0.017896011][0.11410987 0.13040081 0.13528363 0.1286364 0.11206025 0.089018852 0.064764336 0.04440809 0.03002857 0.021424457 0.017326549 0.016345875 0.016916215 0.017718198 0.018308394]]...]
INFO - root - 2017-12-10 09:41:14.146680: step 2910, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.776 sec/batch; 71h:01m:10s remains)
INFO - root - 2017-12-10 09:41:21.901520: step 2920, loss = 0.74, batch loss = 0.68 (9.9 examples/sec; 0.810 sec/batch; 74h:10m:41s remains)
INFO - root - 2017-12-10 09:41:29.764926: step 2930, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.800 sec/batch; 73h:13m:20s remains)
INFO - root - 2017-12-10 09:41:37.588968: step 2940, loss = 0.74, batch loss = 0.68 (10.5 examples/sec; 0.763 sec/batch; 69h:53m:28s remains)
INFO - root - 2017-12-10 09:41:45.484100: step 2950, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.779 sec/batch; 71h:19m:56s remains)
INFO - root - 2017-12-10 09:41:53.293460: step 2960, loss = 0.75, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 73h:04m:31s remains)
INFO - root - 2017-12-10 09:42:01.055136: step 2970, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.779 sec/batch; 71h:16m:42s remains)
INFO - root - 2017-12-10 09:42:08.809203: step 2980, loss = 0.74, batch loss = 0.68 (10.6 examples/sec; 0.754 sec/batch; 69h:01m:18s remains)
INFO - root - 2017-12-10 09:42:16.645274: step 2990, loss = 0.74, batch loss = 0.68 (10.4 examples/sec; 0.771 sec/batch; 70h:34m:06s remains)
INFO - root - 2017-12-10 09:42:24.210810: step 3000, loss = 0.74, batch loss = 0.68 (10.2 examples/sec; 0.785 sec/batch; 71h:48m:19s remains)
2017-12-10 09:42:25.016193: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10070991 0.092048712 0.080728091 0.073810071 0.076318793 0.088073246 0.1042382 0.11832075 0.12495612 0.12381106 0.11617415 0.10419035 0.091652617 0.081074707 0.073130831][0.10902251 0.10071065 0.088327892 0.079699315 0.080855921 0.092011832 0.10824598 0.12256471 0.12922636 0.12780076 0.11960979 0.10716362 0.094615579 0.084478445 0.077089682][0.11652268 0.10844812 0.095322788 0.085275434 0.084696583 0.093793809 0.1078096 0.12000623 0.12517703 0.12316361 0.11509561 0.10370363 0.092719384 0.084264129 0.078064762][0.12463205 0.11805572 0.1067228 0.09804488 0.097559392 0.10511404 0.11625956 0.12475292 0.12661228 0.12241376 0.11332083 0.10189357 0.091164716 0.08301314 0.076820761][0.13152444 0.12899445 0.12258469 0.11838059 0.12023998 0.12760429 0.13658059 0.14125693 0.13909519 0.13164696 0.12027457 0.10718117 0.094708115 0.084951289 0.0774513][0.13447139 0.13860081 0.14001697 0.14303058 0.14944054 0.15834758 0.16659124 0.16867413 0.16322669 0.15259336 0.13835414 0.12237597 0.10672179 0.094131 0.084735557][0.13130158 0.14222336 0.15168834 0.16211589 0.17351702 0.1847486 0.19330284 0.19418611 0.18685924 0.17410016 0.15756768 0.13912053 0.12088625 0.10637119 0.096221693][0.11932424 0.13476917 0.14978416 0.16522968 0.17990668 0.19262986 0.20138551 0.20197795 0.19440633 0.18153541 0.16515321 0.14707777 0.12962769 0.11671746 0.10902745][0.099523917 0.11673944 0.13405593 0.1512436 0.1664841 0.17880298 0.1868571 0.18761402 0.18159223 0.17135319 0.15855816 0.14461434 0.13183232 0.12413783 0.12197806][0.075567357 0.091493547 0.10756508 0.12312803 0.13635242 0.14672714 0.15371044 0.15539511 0.15258142 0.14735541 0.14122692 0.1349986 0.13048555 0.13141252 0.13742614][0.052371837 0.064912446 0.077592671 0.0896744 0.099704504 0.10771231 0.11385202 0.11724279 0.11858013 0.11944654 0.12120905 0.1240212 0.12878719 0.13876301 0.15245914][0.033969723 0.042351186 0.050763287 0.058647569 0.065092534 0.07062041 0.075948708 0.081002936 0.0862383 0.092884757 0.10215388 0.11349807 0.12662382 0.14392534 0.16282572][0.021589344 0.026311291 0.030867361 0.035021853 0.038326111 0.041657884 0.046103187 0.052251328 0.060451984 0.0716552 0.086693376 0.10421778 0.12286363 0.1440049 0.16467549][0.014586674 0.016881932 0.018869082 0.020599106 0.021893546 0.02370907 0.027327813 0.033822171 0.043603115 0.057380233 0.075362176 0.0955558 0.11580029 0.13666297 0.15555212][0.011532013 0.012481675 0.01308612 0.01358253 0.013913522 0.014882544 0.017733764 0.023697171 0.033293154 0.047032449 0.06470596 0.084186159 0.10301314 0.12111568 0.13642667]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 09:42:32.814238: step 3010, loss = 0.74, batch loss = 0.68 (10.1 examples/sec; 0.796 sec/batch; 72h:50m:45s remains)
INFO - root - 2017-12-10 09:42:40.663141: step 3020, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.776 sec/batch; 71h:00m:55s remains)
INFO - root - 2017-12-10 09:42:48.591136: step 3030, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.798 sec/batch; 72h:59m:47s remains)
INFO - root - 2017-12-10 09:42:56.497379: step 3040, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.775 sec/batch; 70h:57m:08s remains)
INFO - root - 2017-12-10 09:43:04.198964: step 3050, loss = 0.74, batch loss = 0.68 (11.6 examples/sec; 0.687 sec/batch; 62h:54m:25s remains)
INFO - root - 2017-12-10 09:43:12.002113: step 3060, loss = 0.74, batch loss = 0.68 (10.5 examples/sec; 0.761 sec/batch; 69h:37m:00s remains)
INFO - root - 2017-12-10 09:43:19.943401: step 3070, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.776 sec/batch; 71h:00m:27s remains)
INFO - root - 2017-12-10 09:43:27.661493: step 3080, loss = 0.74, batch loss = 0.69 (10.1 examples/sec; 0.793 sec/batch; 72h:32m:39s remains)
INFO - root - 2017-12-10 09:43:35.536244: step 3090, loss = 0.74, batch loss = 0.68 (10.6 examples/sec; 0.757 sec/batch; 69h:14m:55s remains)
INFO - root - 2017-12-10 09:43:43.359126: step 3100, loss = 0.74, batch loss = 0.68 (10.2 examples/sec; 0.783 sec/batch; 71h:41m:05s remains)
2017-12-10 09:43:44.210878: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.043458827 0.044591248 0.045550492 0.046791177 0.047852032 0.048636492 0.049005985 0.048943058 0.04969671 0.051347055 0.053690851 0.056129605 0.057549257 0.058110572 0.057154026][0.059490915 0.061926469 0.064247213 0.067101583 0.069470964 0.071166247 0.072106332 0.072341464 0.073775947 0.07655105 0.080851585 0.085762337 0.089095972 0.090745047 0.0897378][0.075743504 0.080271989 0.085299209 0.091203712 0.096051909 0.0996617 0.10197437 0.10267279 0.10431359 0.10728773 0.11312885 0.12067124 0.12613322 0.12877999 0.12759608][0.089455113 0.096928805 0.10643685 0.11731163 0.12636605 0.13341507 0.1382644 0.13969307 0.14036173 0.14144652 0.14669144 0.15478335 0.16046363 0.16249436 0.1604294][0.09843526 0.11002591 0.12550035 0.1426803 0.15719323 0.16882661 0.17690788 0.17880705 0.17700821 0.17400801 0.17585121 0.18114264 0.1838285 0.18288191 0.17897737][0.1021636 0.11870104 0.14076909 0.16433083 0.18418992 0.20009772 0.21063697 0.21195076 0.20624498 0.19760139 0.19368343 0.19318047 0.1901888 0.1844489 0.17798264][0.099262029 0.12092763 0.14900067 0.17772217 0.20170879 0.22056386 0.23212267 0.23182867 0.22178386 0.20740013 0.19669767 0.18861242 0.17836557 0.16683167 0.15724944][0.090185732 0.11535442 0.14724581 0.17880705 0.20495118 0.22502875 0.2363625 0.23430787 0.22097279 0.202487 0.18603991 0.17071015 0.15340629 0.1364361 0.12391564][0.077037804 0.10270511 0.13461064 0.16548933 0.19095388 0.21003671 0.21978185 0.21636726 0.20186704 0.1822993 0.16302826 0.14329514 0.12166018 0.10149664 0.087393753][0.062107258 0.085089341 0.11302123 0.13968302 0.16167751 0.17775221 0.18505274 0.18104523 0.16785872 0.15058292 0.13233715 0.11255223 0.090889916 0.0709888 0.057354435][0.047466822 0.065273941 0.086551674 0.10668992 0.12330713 0.13510434 0.1396915 0.13587779 0.12582901 0.11322799 0.099313796 0.0836051 0.0660452 0.049696855 0.038619965][0.034978114 0.046650991 0.060541455 0.073719561 0.084675193 0.092315078 0.094877586 0.091988407 0.085669965 0.078321993 0.070024155 0.060323346 0.048833683 0.037711643 0.030598238][0.026895925 0.032809075 0.040079121 0.047220819 0.05332651 0.057634767 0.058995605 0.057330921 0.0542059 0.051091377 0.047657091 0.043489214 0.037736461 0.031825937 0.029142721][0.023132803 0.024967384 0.027530473 0.030350102 0.032907557 0.034752641 0.035253625 0.03440021 0.033245638 0.03269767 0.032384142 0.032003336 0.030369356 0.028497832 0.029658487][0.022290502 0.022231614 0.022432663 0.022949474 0.023385061 0.023497017 0.023132522 0.022382028 0.021925727 0.022395691 0.023532404 0.025087988 0.025738934 0.026369711 0.030190786]]...]
INFO - root - 2017-12-10 09:43:52.064699: step 3110, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.775 sec/batch; 70h:56m:41s remains)
INFO - root - 2017-12-10 09:43:59.888922: step 3120, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.774 sec/batch; 70h:51m:42s remains)
INFO - root - 2017-12-10 09:44:07.774035: step 3130, loss = 0.74, batch loss = 0.68 (10.1 examples/sec; 0.791 sec/batch; 72h:23m:57s remains)
INFO - root - 2017-12-10 09:44:15.525291: step 3140, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.800 sec/batch; 73h:10m:19s remains)
INFO - root - 2017-12-10 09:44:23.406589: step 3150, loss = 0.73, batch loss = 0.68 (9.8 examples/sec; 0.819 sec/batch; 74h:56m:44s remains)
INFO - root - 2017-12-10 09:44:31.086336: step 3160, loss = 0.73, batch loss = 0.68 (10.0 examples/sec; 0.801 sec/batch; 73h:18m:34s remains)
INFO - root - 2017-12-10 09:44:38.942545: step 3170, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.778 sec/batch; 71h:09m:42s remains)
INFO - root - 2017-12-10 09:44:46.841006: step 3180, loss = 0.74, batch loss = 0.68 (9.8 examples/sec; 0.814 sec/batch; 74h:26m:10s remains)
INFO - root - 2017-12-10 09:44:54.760422: step 3190, loss = 0.74, batch loss = 0.68 (10.2 examples/sec; 0.786 sec/batch; 71h:54m:16s remains)
INFO - root - 2017-12-10 09:45:02.604200: step 3200, loss = 0.73, batch loss = 0.68 (9.8 examples/sec; 0.812 sec/batch; 74h:18m:31s remains)
2017-12-10 09:45:03.505660: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.038554437 0.043194786 0.045790628 0.046737295 0.046279412 0.045682587 0.046480384 0.047902614 0.049310137 0.05022452 0.050428279 0.048354495 0.042659752 0.034997806 0.026951054][0.059418086 0.067644291 0.072791263 0.075432114 0.075333044 0.074851051 0.076538488 0.078920551 0.080591112 0.081136219 0.08074262 0.076670066 0.066812865 0.053998943 0.041152392][0.083128929 0.096417248 0.10553227 0.11099528 0.11208266 0.11268275 0.11588401 0.11925549 0.12062766 0.1197555 0.11734369 0.10936895 0.093455568 0.073947422 0.055423573][0.10800458 0.12750851 0.14168321 0.15071735 0.15341307 0.15570547 0.16094172 0.16539338 0.16616794 0.16328567 0.15793686 0.14432871 0.12037974 0.0927567 0.068001218][0.13554265 0.16217713 0.18187165 0.19455922 0.19873379 0.20279944 0.21041198 0.21648751 0.21708283 0.21182267 0.20220158 0.18066496 0.14633329 0.10897703 0.077605143][0.16109362 0.19480111 0.21976486 0.23583171 0.24149792 0.24744882 0.25748226 0.26519254 0.26565591 0.25775743 0.24285823 0.21207683 0.16644481 0.11934493 0.08215604][0.17860484 0.21700872 0.24528544 0.26354524 0.27067858 0.27844849 0.29073048 0.30013436 0.30101967 0.29125258 0.2715984 0.23294041 0.17822577 0.1238587 0.082714163][0.18538524 0.22431983 0.25219247 0.26972279 0.27641755 0.2840589 0.29660171 0.30629641 0.30727798 0.29670784 0.27488202 0.23314628 0.17567486 0.11996662 0.078821979][0.18056975 0.21608365 0.23997696 0.2537452 0.25776407 0.26297548 0.27326715 0.28135768 0.28187826 0.27174628 0.25086936 0.21174763 0.15860704 0.10782298 0.07070075][0.16319205 0.192463 0.21021174 0.21873887 0.21933174 0.22131029 0.22807463 0.23349912 0.23318012 0.22431645 0.20646733 0.17373189 0.12982044 0.0883155 0.058129866][0.13287035 0.15410064 0.16515233 0.16877374 0.16674165 0.16605763 0.16937569 0.17200394 0.17082597 0.16374485 0.15022871 0.12611961 0.094128624 0.064282171 0.042660035][0.096271284 0.10943861 0.11475405 0.11480091 0.11129079 0.10882754 0.10925746 0.10954145 0.1078973 0.10306814 0.094485007 0.079521865 0.059721511 0.04143133 0.028138852][0.063605033 0.070521653 0.071881965 0.069699906 0.06533622 0.0616108 0.059728727 0.058257092 0.056447566 0.05372433 0.049469125 0.042126179 0.032290734 0.02325874 0.016618825][0.038089138 0.041122507 0.040567905 0.037744004 0.033661783 0.029952932 0.02731524 0.02529612 0.023771932 0.022511624 0.020999493 0.018233994 0.014485412 0.011155219 0.0087305391][0.01979154 0.020792035 0.019819278 0.017670624 0.014947554 0.012455026 0.010499209 0.00903258 0.0080969417 0.0076398244 0.0072838156 0.0064379652 0.0055026081 0.004830922 0.0045049218]]...]
INFO - root - 2017-12-10 09:45:11.328313: step 3210, loss = 0.73, batch loss = 0.68 (10.0 examples/sec; 0.798 sec/batch; 72h:59m:16s remains)
INFO - root - 2017-12-10 09:45:19.169445: step 3220, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.778 sec/batch; 71h:08m:30s remains)
INFO - root - 2017-12-10 09:45:26.829706: step 3230, loss = 0.73, batch loss = 0.67 (10.3 examples/sec; 0.777 sec/batch; 71h:04m:26s remains)
INFO - root - 2017-12-10 09:45:34.544583: step 3240, loss = 0.74, batch loss = 0.68 (10.2 examples/sec; 0.787 sec/batch; 71h:58m:05s remains)
INFO - root - 2017-12-10 09:45:42.323081: step 3250, loss = 0.74, batch loss = 0.68 (10.5 examples/sec; 0.765 sec/batch; 69h:58m:36s remains)
INFO - root - 2017-12-10 09:45:50.188046: step 3260, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.799 sec/batch; 73h:02m:59s remains)
INFO - root - 2017-12-10 09:45:57.997797: step 3270, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.779 sec/batch; 71h:12m:49s remains)
INFO - root - 2017-12-10 09:46:05.726577: step 3280, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.784 sec/batch; 71h:43m:44s remains)
INFO - root - 2017-12-10 09:46:13.511194: step 3290, loss = 0.73, batch loss = 0.68 (10.6 examples/sec; 0.755 sec/batch; 69h:05m:13s remains)
INFO - root - 2017-12-10 09:46:21.397939: step 3300, loss = 0.73, batch loss = 0.68 (10.0 examples/sec; 0.802 sec/batch; 73h:20m:23s remains)
2017-12-10 09:46:22.386731: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10425327 0.094520271 0.080813691 0.069019645 0.062891267 0.063234411 0.068206161 0.07510341 0.083357625 0.091675647 0.098304108 0.10257729 0.10576002 0.1086449 0.11063717][0.091394156 0.082073 0.069253772 0.058139756 0.05205936 0.051699854 0.055206288 0.060045589 0.065695129 0.071383 0.07652016 0.081026778 0.086034216 0.092091173 0.097671084][0.078291222 0.070031345 0.059230231 0.049988169 0.045022655 0.044793092 0.047475457 0.050605807 0.053393114 0.05555724 0.057680376 0.060619436 0.065733537 0.073534034 0.082131989][0.068634018 0.062310129 0.054719411 0.049120821 0.04751946 0.049901791 0.054052692 0.057031609 0.057627063 0.055755302 0.0530396 0.0516424 0.053826746 0.060627945 0.0703175][0.06211143 0.058422342 0.055363588 0.055426624 0.059903797 0.067899846 0.076188266 0.080774829 0.079795443 0.073256567 0.063803621 0.055315211 0.051631086 0.054719161 0.063320652][0.05757717 0.056973379 0.059126396 0.066060983 0.078191839 0.093361884 0.10693168 0.11361869 0.11112965 0.099705271 0.083036728 0.066815324 0.056526184 0.054987535 0.061194766][0.053786341 0.0562316 0.063519031 0.077293046 0.096765816 0.11837601 0.13619098 0.14394718 0.1394487 0.12351295 0.10085793 0.078548223 0.063064471 0.05756497 0.060957227][0.04939818 0.054475192 0.0660161 0.085136235 0.10969135 0.13493101 0.15417963 0.16099769 0.1539562 0.13471441 0.10871493 0.083535708 0.065696321 0.058017503 0.059034824][0.044491354 0.051575519 0.065698259 0.08724843 0.11299706 0.13772069 0.15502489 0.15934649 0.150355 0.13031451 0.10459781 0.080220118 0.062863559 0.054740869 0.054153323][0.040449776 0.049073834 0.06406451 0.084827363 0.10757279 0.12757996 0.13988276 0.14084873 0.13135646 0.11370165 0.092118017 0.071953386 0.057383619 0.050023545 0.048473377][0.038740665 0.048473772 0.062761873 0.0801075 0.096739873 0.10926127 0.11495215 0.11261554 0.10423087 0.091540352 0.076834321 0.063034393 0.052517988 0.046468675 0.044270873][0.040666956 0.050776582 0.063176647 0.075768247 0.085280143 0.089932486 0.08934141 0.084565528 0.078310542 0.07144428 0.0641674 0.0569056 0.050346371 0.04543541 0.042657997][0.045831125 0.055205677 0.064642049 0.072082087 0.075128496 0.073408738 0.068307295 0.062384915 0.05863655 0.057038639 0.056128602 0.054356586 0.051089879 0.047146585 0.04402595][0.051442947 0.058962293 0.0647513 0.067427218 0.065610707 0.059876028 0.05253629 0.0467314 0.045357533 0.047734737 0.051399224 0.053560328 0.052772172 0.049786478 0.046795398][0.054294188 0.0591746 0.06129748 0.0602756 0.055583362 0.048216518 0.040663257 0.035886612 0.036364719 0.041037817 0.047133423 0.051405918 0.052130856 0.050095189 0.047823913]]...]
INFO - root - 2017-12-10 09:46:30.254345: step 3310, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.797 sec/batch; 72h:51m:09s remains)
INFO - root - 2017-12-10 09:46:37.707171: step 3320, loss = 0.74, batch loss = 0.68 (10.2 examples/sec; 0.784 sec/batch; 71h:39m:54s remains)
INFO - root - 2017-12-10 09:46:45.487204: step 3330, loss = 0.74, batch loss = 0.68 (10.4 examples/sec; 0.768 sec/batch; 70h:12m:04s remains)
INFO - root - 2017-12-10 09:46:53.358925: step 3340, loss = 0.74, batch loss = 0.68 (10.1 examples/sec; 0.793 sec/batch; 72h:29m:21s remains)
INFO - root - 2017-12-10 09:47:01.194321: step 3350, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.801 sec/batch; 73h:11m:52s remains)
INFO - root - 2017-12-10 09:47:09.028788: step 3360, loss = 0.73, batch loss = 0.68 (10.0 examples/sec; 0.797 sec/batch; 72h:49m:50s remains)
INFO - root - 2017-12-10 09:47:16.897217: step 3370, loss = 0.73, batch loss = 0.68 (10.4 examples/sec; 0.769 sec/batch; 70h:18m:32s remains)
INFO - root - 2017-12-10 09:47:24.889435: step 3380, loss = 0.73, batch loss = 0.67 (9.9 examples/sec; 0.810 sec/batch; 74h:00m:58s remains)
INFO - root - 2017-12-10 09:47:32.878195: step 3390, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.798 sec/batch; 72h:58m:46s remains)
INFO - root - 2017-12-10 09:47:40.496586: step 3400, loss = 0.73, batch loss = 0.68 (10.2 examples/sec; 0.784 sec/batch; 71h:41m:50s remains)
2017-12-10 09:47:41.346524: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14251351 0.1427002 0.141954 0.14001517 0.13750498 0.13481045 0.13148911 0.12739125 0.12524477 0.12629668 0.12953353 0.1331782 0.13641953 0.14073673 0.14537385][0.14314774 0.14219524 0.14004809 0.13724613 0.13464227 0.13174772 0.12836303 0.12504093 0.12509634 0.12958658 0.13660657 0.14348707 0.14929086 0.15589029 0.16211198][0.13584639 0.13538107 0.13314301 0.13073498 0.12894233 0.12679881 0.12394459 0.12147804 0.12320559 0.13019523 0.13991486 0.14879361 0.15615284 0.16425891 0.17239182][0.13281518 0.13474463 0.13370065 0.1323707 0.13168865 0.13080244 0.1288534 0.12674414 0.12839182 0.13510691 0.1442643 0.15208666 0.15837643 0.1661157 0.17564312][0.14035691 0.1462101 0.14677092 0.14635837 0.14668147 0.14716667 0.14601071 0.14310405 0.14203364 0.14482535 0.14960688 0.153568 0.1570296 0.16329403 0.17341715][0.1534863 0.16283152 0.16426587 0.16432489 0.1655788 0.167576 0.16713956 0.16285652 0.15751363 0.15400757 0.15223271 0.15146403 0.15263723 0.15794873 0.16844739][0.16228391 0.17381878 0.17587698 0.17668101 0.17907172 0.18226558 0.18215615 0.17618784 0.16638865 0.15654776 0.14866522 0.14419754 0.1441929 0.14902863 0.15935102][0.16414391 0.17627919 0.17842451 0.17959864 0.18226999 0.18527775 0.18437324 0.17680562 0.16443552 0.15142204 0.14084257 0.1351791 0.13501218 0.13912128 0.14785238][0.16198908 0.17401241 0.17617093 0.17730348 0.17919634 0.18042585 0.17758909 0.16881438 0.156229 0.14350235 0.13369264 0.12930542 0.13018553 0.13392118 0.14011467][0.15651533 0.1689979 0.17221466 0.17385115 0.17469254 0.17353413 0.16854997 0.15916596 0.14772129 0.13710147 0.1298162 0.12797785 0.13104129 0.13547611 0.13984634][0.14766672 0.160944 0.16620076 0.16953129 0.17052433 0.16827404 0.16247852 0.15369807 0.14440095 0.13665983 0.13247269 0.13370752 0.13938856 0.14518778 0.14874493][0.14170086 0.15546893 0.16294602 0.16899462 0.17190224 0.17025207 0.16471614 0.15693028 0.14965053 0.14450674 0.142886 0.14642784 0.153814 0.16052893 0.16363338][0.14251639 0.1559093 0.16496547 0.17389534 0.17971778 0.17961016 0.17434402 0.16669314 0.16024177 0.15685734 0.15709122 0.16223878 0.17077906 0.17823301 0.18118414][0.14580773 0.15839048 0.16831635 0.17963007 0.18831296 0.18976966 0.18430768 0.17591128 0.16924939 0.16682689 0.16833219 0.1745245 0.18386756 0.19204772 0.19525892][0.1428455 0.15421447 0.16403663 0.17627712 0.18641007 0.18866901 0.18308887 0.17454101 0.16829333 0.16716786 0.16990227 0.17638411 0.18527256 0.1930286 0.1962257]]...]
INFO - root - 2017-12-10 09:47:49.106994: step 3410, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 71h:35m:57s remains)
INFO - root - 2017-12-10 09:47:56.973222: step 3420, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.778 sec/batch; 71h:09m:20s remains)
INFO - root - 2017-12-10 09:48:04.868323: step 3430, loss = 0.73, batch loss = 0.68 (10.3 examples/sec; 0.780 sec/batch; 71h:20m:32s remains)
INFO - root - 2017-12-10 09:48:12.708511: step 3440, loss = 0.74, batch loss = 0.68 (9.9 examples/sec; 0.808 sec/batch; 73h:53m:31s remains)
INFO - root - 2017-12-10 09:48:20.561326: step 3450, loss = 0.74, batch loss = 0.68 (10.2 examples/sec; 0.784 sec/batch; 71h:39m:19s remains)
INFO - root - 2017-12-10 09:48:28.449550: step 3460, loss = 0.73, batch loss = 0.67 (9.9 examples/sec; 0.812 sec/batch; 74h:10m:54s remains)
INFO - root - 2017-12-10 09:48:36.398260: step 3470, loss = 0.73, batch loss = 0.67 (10.3 examples/sec; 0.780 sec/batch; 71h:14m:42s remains)
INFO - root - 2017-12-10 09:48:44.067548: step 3480, loss = 0.73, batch loss = 0.67 (10.3 examples/sec; 0.774 sec/batch; 70h:46m:20s remains)
INFO - root - 2017-12-10 09:48:51.795935: step 3490, loss = 0.73, batch loss = 0.67 (10.5 examples/sec; 0.764 sec/batch; 69h:49m:06s remains)
INFO - root - 2017-12-10 09:48:59.521653: step 3500, loss = 0.73, batch loss = 0.67 (10.3 examples/sec; 0.776 sec/batch; 70h:57m:46s remains)
2017-12-10 09:49:00.366836: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19677238 0.19991669 0.19472642 0.18586735 0.17563689 0.16727479 0.16277528 0.15877879 0.1526995 0.14765738 0.14897791 0.15653521 0.16531196 0.17458723 0.18249935][0.2179504 0.2224786 0.21782173 0.2088339 0.19794215 0.18855634 0.18354233 0.17926069 0.17270529 0.16725846 0.16905367 0.178163 0.18910024 0.20092179 0.21117692][0.22500303 0.2305451 0.22627196 0.21722487 0.20595068 0.1959675 0.19077915 0.18675245 0.18057626 0.17582265 0.17826867 0.18825819 0.20043418 0.21363573 0.22491606][0.22489817 0.23088692 0.22667976 0.2174062 0.20580511 0.19557923 0.1904861 0.18692243 0.18147099 0.17770655 0.18079798 0.19140543 0.2044445 0.21819457 0.229688][0.22017112 0.22616148 0.22138414 0.21166493 0.19981578 0.19000399 0.18575843 0.18315706 0.1787836 0.17643774 0.18098411 0.19258583 0.20645155 0.22040123 0.23198664][0.21162407 0.2177504 0.21253377 0.2026117 0.19083281 0.18208955 0.17925723 0.17796054 0.17499454 0.17468481 0.18140031 0.19432563 0.20894289 0.22251929 0.23359294][0.20392817 0.21020085 0.20466042 0.19432563 0.18240048 0.17454402 0.17274973 0.17216192 0.17033374 0.17219202 0.181205 0.1953705 0.21049006 0.22362764 0.23407041][0.20187391 0.2085785 0.20231374 0.19062895 0.17752881 0.169471 0.16779193 0.16743895 0.1667459 0.17071076 0.18172722 0.19687037 0.21214178 0.22462776 0.2342089][0.20320657 0.20953088 0.20141377 0.18736155 0.17255133 0.16391011 0.16207498 0.16206172 0.16279332 0.16871858 0.18119913 0.19675194 0.21154419 0.22279622 0.23076335][0.19973585 0.204225 0.19307615 0.17624256 0.16001444 0.15131024 0.15000038 0.15118162 0.15402007 0.16198628 0.17538725 0.19052401 0.20394813 0.21321529 0.21858016][0.18954147 0.19077902 0.17612277 0.15675703 0.13969688 0.13129872 0.13075098 0.13339971 0.13833274 0.14761552 0.16047116 0.17363884 0.1843439 0.19057931 0.19248021][0.1800693 0.17847881 0.16128331 0.14027672 0.12267204 0.11390173 0.11290374 0.11559266 0.12085976 0.1293401 0.13936539 0.14856367 0.15521882 0.15813568 0.15739489][0.17304119 0.16982889 0.15167902 0.13048948 0.11312819 0.10400916 0.10204922 0.10381113 0.10799771 0.11407369 0.1199597 0.12431406 0.12654229 0.1262641 0.1237279][0.16634268 0.16262715 0.14518099 0.12526256 0.10901094 0.099951312 0.096930362 0.097091444 0.099126637 0.10194442 0.10362476 0.10362182 0.10223945 0.0997039 0.096450627][0.16244543 0.15969844 0.14355741 0.12477803 0.10933243 0.10049707 0.096959963 0.095876239 0.09580791 0.095523119 0.093632206 0.090283968 0.086472511 0.082926348 0.080224305]]...]
INFO - root - 2017-12-10 09:49:08.180238: step 3510, loss = 0.74, batch loss = 0.68 (10.3 examples/sec; 0.776 sec/batch; 70h:55m:51s remains)
INFO - root - 2017-12-10 09:49:16.000666: step 3520, loss = 0.73, batch loss = 0.68 (10.4 examples/sec; 0.772 sec/batch; 70h:30m:21s remains)
INFO - root - 2017-12-10 09:49:23.828427: step 3530, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.784 sec/batch; 71h:36m:41s remains)
INFO - root - 2017-12-10 09:49:31.625306: step 3540, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.797 sec/batch; 72h:50m:12s remains)
INFO - root - 2017-12-10 09:49:39.503961: step 3550, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.781 sec/batch; 71h:20m:04s remains)
INFO - root - 2017-12-10 09:49:47.263138: step 3560, loss = 0.73, batch loss = 0.67 (10.4 examples/sec; 0.771 sec/batch; 70h:26m:22s remains)
INFO - root - 2017-12-10 09:49:55.161194: step 3570, loss = 0.73, batch loss = 0.68 (10.2 examples/sec; 0.782 sec/batch; 71h:27m:55s remains)
INFO - root - 2017-12-10 09:50:02.973422: step 3580, loss = 0.73, batch loss = 0.67 (12.8 examples/sec; 0.625 sec/batch; 57h:04m:12s remains)
INFO - root - 2017-12-10 09:50:10.828455: step 3590, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.787 sec/batch; 71h:54m:32s remains)
INFO - root - 2017-12-10 09:50:18.705466: step 3600, loss = 0.73, batch loss = 0.67 (9.9 examples/sec; 0.811 sec/batch; 74h:05m:23s remains)
2017-12-10 09:50:19.469627: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.083616912 0.088159114 0.098278172 0.11473598 0.13291429 0.14641172 0.15325932 0.15731466 0.16199301 0.16467576 0.1623619 0.15605982 0.1490095 0.14014155 0.12986527][0.075651415 0.080326952 0.091196947 0.10980961 0.13106665 0.14829111 0.15799095 0.16281517 0.16569012 0.16487266 0.15877473 0.15001981 0.14268163 0.13555357 0.12858146][0.076141544 0.077590592 0.085772693 0.10338856 0.12545705 0.14545877 0.15829997 0.16439502 0.16504647 0.15945585 0.1481474 0.13565572 0.12684813 0.12071202 0.11697356][0.088848695 0.083651945 0.085771486 0.099503219 0.12083029 0.14345162 0.16031906 0.16878292 0.167843 0.15738785 0.13990583 0.12192543 0.10952564 0.10268091 0.10108426][0.10656749 0.094909064 0.089882486 0.098522693 0.11828443 0.14282098 0.16323194 0.17372707 0.17108701 0.1558574 0.13208322 0.1081951 0.091542818 0.083464116 0.083417468][0.12259386 0.10813589 0.098771229 0.103705 0.12194436 0.14710829 0.16916347 0.18011922 0.17524873 0.1557464 0.12677842 0.097966351 0.077507466 0.067759007 0.068024226][0.13123813 0.11925667 0.11019019 0.1143029 0.13197574 0.15695478 0.17894514 0.18923292 0.18234764 0.15995838 0.12788621 0.096336171 0.073749438 0.062957563 0.06301716][0.13101172 0.12533 0.12071966 0.12704726 0.1456378 0.17038392 0.19122383 0.20000301 0.19137456 0.16732208 0.13409713 0.10214303 0.079583272 0.069005452 0.069131367][0.12306365 0.12510635 0.12698086 0.1369971 0.15685642 0.18069047 0.19901121 0.20513682 0.19490218 0.17051442 0.13835661 0.10853289 0.08836139 0.079607137 0.080355488][0.11101847 0.11955659 0.12759791 0.14126097 0.16189101 0.18399568 0.19886564 0.201649 0.18996058 0.1664566 0.13734452 0.1116903 0.095642552 0.089740627 0.091346152][0.099376529 0.1112171 0.12335674 0.1394781 0.15999892 0.17987695 0.19136424 0.19117197 0.17860277 0.15706055 0.13244806 0.1122239 0.10097638 0.098026484 0.10021103][0.089315534 0.10121175 0.11417509 0.13045111 0.14954336 0.16696347 0.17599456 0.17424844 0.16206464 0.1434698 0.12378857 0.10873874 0.10145694 0.10072593 0.10320321][0.080686383 0.090479583 0.10144265 0.11530326 0.13108528 0.14520779 0.15227848 0.15030973 0.13992172 0.12519252 0.11044953 0.099676363 0.09494888 0.095196009 0.097467594][0.069699019 0.076629639 0.084341884 0.0943052 0.10549152 0.11549079 0.12057426 0.11903283 0.11132828 0.10096757 0.091039069 0.083932541 0.080895029 0.081288025 0.08296673][0.056121495 0.06042435 0.064938985 0.07091181 0.077484362 0.083438389 0.086576067 0.085495748 0.080441728 0.074082837 0.068348609 0.064299226 0.062518105 0.062822953 0.063973166]]...]
INFO - root - 2017-12-10 09:50:27.291675: step 3610, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.783 sec/batch; 71h:31m:57s remains)
INFO - root - 2017-12-10 09:50:35.124097: step 3620, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.782 sec/batch; 71h:27m:27s remains)
INFO - root - 2017-12-10 09:50:43.031250: step 3630, loss = 0.73, batch loss = 0.67 (10.0 examples/sec; 0.797 sec/batch; 72h:46m:27s remains)
INFO - root - 2017-12-10 09:50:50.680283: step 3640, loss = 0.73, batch loss = 0.67 (10.3 examples/sec; 0.775 sec/batch; 70h:47m:11s remains)
INFO - root - 2017-12-10 09:50:58.451846: step 3650, loss = 0.73, batch loss = 0.67 (10.4 examples/sec; 0.772 sec/batch; 70h:29m:09s remains)
INFO - root - 2017-12-10 09:51:06.372643: step 3660, loss = 0.74, batch loss = 0.68 (10.1 examples/sec; 0.794 sec/batch; 72h:33m:44s remains)
INFO - root - 2017-12-10 09:51:14.071538: step 3670, loss = 0.73, batch loss = 0.67 (10.0 examples/sec; 0.802 sec/batch; 73h:17m:37s remains)
INFO - root - 2017-12-10 09:51:21.986849: step 3680, loss = 0.73, batch loss = 0.67 (10.5 examples/sec; 0.763 sec/batch; 69h:41m:44s remains)
INFO - root - 2017-12-10 09:51:29.896026: step 3690, loss = 0.73, batch loss = 0.67 (9.9 examples/sec; 0.805 sec/batch; 73h:33m:58s remains)
INFO - root - 2017-12-10 09:51:37.834727: step 3700, loss = 0.72, batch loss = 0.67 (9.9 examples/sec; 0.811 sec/batch; 74h:01m:57s remains)
2017-12-10 09:51:38.723668: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11489868 0.12654951 0.15079199 0.18439919 0.21833491 0.24203044 0.25347826 0.25533831 0.24855897 0.23639335 0.22460753 0.21507454 0.20712766 0.20147806 0.19771181][0.15585688 0.16302499 0.1827254 0.21330869 0.24537206 0.26799646 0.28038046 0.28450412 0.27882192 0.26643646 0.25342533 0.24134091 0.22960682 0.22005954 0.2125676][0.19862662 0.19924803 0.20944102 0.23046184 0.2536431 0.26976588 0.27959341 0.28428483 0.27964136 0.26844 0.25612 0.24392673 0.2318176 0.22222424 0.21431892][0.23232649 0.22736688 0.22842954 0.23893991 0.25173241 0.25994453 0.26589563 0.27021438 0.26683143 0.25791651 0.2474333 0.23677357 0.22636952 0.21882923 0.21236821][0.24880786 0.24361928 0.24058084 0.24445973 0.24981558 0.25219443 0.25521886 0.25961781 0.25808075 0.25183302 0.24280103 0.23252116 0.22190195 0.21403362 0.20766357][0.24811611 0.24920708 0.24883959 0.2521697 0.25499672 0.25503013 0.25697312 0.26204959 0.26232064 0.25791907 0.24858414 0.23557951 0.22040534 0.20753187 0.19811167][0.23475005 0.24521022 0.2512489 0.25733906 0.26017633 0.25940627 0.26058009 0.26558247 0.26675627 0.26340717 0.25322154 0.23656544 0.21539198 0.19589615 0.18197988][0.20983765 0.22859855 0.24128193 0.25073177 0.25437284 0.25307944 0.25315094 0.25701803 0.25824457 0.25573137 0.24586171 0.22782044 0.20318955 0.17895076 0.16109827][0.17065372 0.19372699 0.21092302 0.22302125 0.22794865 0.22714975 0.22703405 0.23009925 0.23168577 0.23065846 0.22304001 0.20686567 0.18273537 0.15721525 0.13725266][0.12188757 0.14483076 0.16313575 0.17630683 0.18238844 0.18284567 0.18321292 0.18567818 0.18741612 0.18774509 0.18317492 0.17099158 0.15071805 0.12752765 0.10817455][0.079329334 0.098298147 0.11435079 0.12630267 0.13209254 0.13290372 0.13303459 0.13418534 0.13518591 0.13596931 0.13403483 0.12651142 0.11194343 0.093724489 0.077394441][0.050776791 0.064192437 0.075978883 0.084698312 0.088495441 0.088287435 0.087085716 0.086120531 0.085580111 0.086003952 0.085890286 0.082615778 0.074244022 0.062504679 0.051151738][0.037023216 0.045747265 0.053184275 0.058244087 0.059647229 0.05813688 0.055572771 0.052752376 0.050646409 0.050174356 0.050637159 0.049996965 0.046437319 0.040540323 0.034437746][0.035449542 0.041344196 0.04552212 0.047455598 0.04659459 0.04379461 0.040163264 0.036203485 0.033123218 0.031944264 0.032362193 0.032957915 0.032362774 0.030547712 0.028562974][0.044121642 0.049286325 0.051650554 0.051127817 0.048058152 0.043742288 0.038968354 0.034135569 0.030484803 0.028940644 0.029200777 0.030239366 0.031121973 0.031586796 0.03206725]]...]
INFO - root - 2017-12-10 09:51:46.606895: step 3710, loss = 0.73, batch loss = 0.67 (10.5 examples/sec; 0.765 sec/batch; 69h:53m:32s remains)
INFO - root - 2017-12-10 09:51:54.305709: step 3720, loss = 0.73, batch loss = 0.67 (9.9 examples/sec; 0.804 sec/batch; 73h:28m:12s remains)
INFO - root - 2017-12-10 09:52:02.264965: step 3730, loss = 0.73, batch loss = 0.67 (10.3 examples/sec; 0.777 sec/batch; 70h:56m:50s remains)
INFO - root - 2017-12-10 09:52:10.116448: step 3740, loss = 0.73, batch loss = 0.68 (10.3 examples/sec; 0.778 sec/batch; 71h:00m:55s remains)
INFO - root - 2017-12-10 09:52:17.979437: step 3750, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.785 sec/batch; 71h:43m:20s remains)
INFO - root - 2017-12-10 09:52:25.722067: step 3760, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.785 sec/batch; 71h:42m:35s remains)
INFO - root - 2017-12-10 09:52:33.604834: step 3770, loss = 0.72, batch loss = 0.67 (10.2 examples/sec; 0.783 sec/batch; 71h:28m:19s remains)
INFO - root - 2017-12-10 09:52:41.640414: step 3780, loss = 0.72, batch loss = 0.67 (9.9 examples/sec; 0.812 sec/batch; 74h:09m:14s remains)
INFO - root - 2017-12-10 09:52:49.484467: step 3790, loss = 0.72, batch loss = 0.67 (10.2 examples/sec; 0.788 sec/batch; 71h:56m:24s remains)
INFO - root - 2017-12-10 09:52:57.127914: step 3800, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.781 sec/batch; 71h:16m:57s remains)
2017-12-10 09:52:57.980578: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.051086437 0.0640969 0.074168183 0.079876877 0.081645757 0.08036194 0.076210834 0.07321655 0.072937675 0.0735174 0.073025919 0.070790745 0.067345113 0.060425274 0.049284458][0.074059173 0.096746296 0.11579827 0.12778132 0.13271934 0.13173169 0.12496093 0.11896463 0.11670589 0.11635257 0.11479655 0.11087911 0.10521554 0.094276175 0.077114575][0.096125655 0.1299658 0.1597075 0.17978105 0.18912345 0.18869865 0.17863947 0.16807523 0.16200459 0.15958986 0.15679739 0.15156126 0.1440306 0.12924759 0.1061376][0.11480035 0.15926881 0.19974728 0.22900255 0.24488056 0.24738514 0.23561861 0.22092414 0.20989218 0.2041316 0.19901182 0.19160414 0.18147096 0.1623932 0.13365337][0.12946062 0.18289746 0.23310944 0.27201062 0.29666972 0.30509421 0.29465494 0.277697 0.26163062 0.25114915 0.24170282 0.23000184 0.21508743 0.1904109 0.15612528][0.13934244 0.19902229 0.25637659 0.30430317 0.33921614 0.35645711 0.35133657 0.33558816 0.31638214 0.30049756 0.28487825 0.26631662 0.24369054 0.21142291 0.17139275][0.14499861 0.20707457 0.26688546 0.31956142 0.36204958 0.38716689 0.38879028 0.37723604 0.35846791 0.33936313 0.31809473 0.2920849 0.26035696 0.21985276 0.17497191][0.14840406 0.20880941 0.2656095 0.31640717 0.35946825 0.38647407 0.39144814 0.38375375 0.36812544 0.34972683 0.32651812 0.29607174 0.25793248 0.21215008 0.16560254][0.1451007 0.19942479 0.24898112 0.29321638 0.33135244 0.35501459 0.35966945 0.35452393 0.3434087 0.32886225 0.307393 0.27655771 0.23649977 0.18982165 0.14521088][0.1311921 0.1767152 0.21714598 0.25294581 0.28358877 0.30109578 0.30282864 0.29805019 0.29041633 0.27997091 0.26200843 0.23400883 0.19662 0.1538742 0.11481616][0.10606498 0.14110914 0.17155325 0.19812818 0.22026503 0.23093876 0.22926803 0.22412021 0.21902207 0.2121904 0.19868374 0.17622092 0.14573956 0.11127538 0.080757104][0.077663548 0.10234465 0.12308045 0.140169 0.15314838 0.15674582 0.15154386 0.14511822 0.14088501 0.13669184 0.1280922 0.11320376 0.092711367 0.069540866 0.049230628][0.052079331 0.067819975 0.080375262 0.089415394 0.094596118 0.092788942 0.085360788 0.077843234 0.073164567 0.069922425 0.064956143 0.056945127 0.046235941 0.034235481 0.023683406][0.029321244 0.037595477 0.043827068 0.047380019 0.048054036 0.044464413 0.037890002 0.031499054 0.027166428 0.024321325 0.021336667 0.017849477 0.013847693 0.0097199166 0.0061874967][0.012329482 0.015362098 0.017508142 0.018221432 0.017406998 0.014610548 0.010597079 0.00668797 0.003780988 0.0018607159 0.00029635074 -0.00090479449 -0.001521515 -0.0017147635 -0.0016412538]]...]
INFO - root - 2017-12-10 09:53:05.904688: step 3810, loss = 0.73, batch loss = 0.67 (10.3 examples/sec; 0.780 sec/batch; 71h:13m:32s remains)
INFO - root - 2017-12-10 09:53:13.698498: step 3820, loss = 0.72, batch loss = 0.67 (10.4 examples/sec; 0.773 sec/batch; 70h:33m:19s remains)
INFO - root - 2017-12-10 09:53:21.564116: step 3830, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.800 sec/batch; 73h:04m:41s remains)
INFO - root - 2017-12-10 09:53:29.406087: step 3840, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 71h:30m:02s remains)
INFO - root - 2017-12-10 09:53:37.168035: step 3850, loss = 0.73, batch loss = 0.67 (10.0 examples/sec; 0.802 sec/batch; 73h:13m:24s remains)
INFO - root - 2017-12-10 09:53:45.149402: step 3860, loss = 0.73, batch loss = 0.67 (10.1 examples/sec; 0.794 sec/batch; 72h:31m:29s remains)
INFO - root - 2017-12-10 09:53:53.098486: step 3870, loss = 0.73, batch loss = 0.67 (9.5 examples/sec; 0.840 sec/batch; 76h:39m:33s remains)
INFO - root - 2017-12-10 09:54:00.703870: step 3880, loss = 0.74, batch loss = 0.68 (10.0 examples/sec; 0.803 sec/batch; 73h:17m:51s remains)
INFO - root - 2017-12-10 09:54:08.501631: step 3890, loss = 0.73, batch loss = 0.67 (10.1 examples/sec; 0.790 sec/batch; 72h:04m:58s remains)
INFO - root - 2017-12-10 09:54:16.357929: step 3900, loss = 0.73, batch loss = 0.67 (10.8 examples/sec; 0.740 sec/batch; 67h:32m:57s remains)
2017-12-10 09:54:17.214352: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.046210445 0.04734271 0.048318133 0.050350364 0.053498864 0.057902444 0.06328249 0.06857276 0.072002582 0.073230796 0.07173758 0.0664428 0.058275972 0.050603081 0.046606362][0.04400963 0.045415655 0.04661477 0.048682015 0.051709179 0.055725977 0.06047928 0.065070957 0.067764893 0.068445608 0.066633791 0.06138989 0.05343483 0.045961808 0.042236529][0.04420644 0.049026612 0.053605791 0.058080569 0.062298641 0.066113912 0.069812074 0.073127091 0.074820817 0.074927576 0.07258068 0.06647341 0.05703371 0.047520526 0.041616496][0.050507061 0.061391149 0.072424449 0.082371339 0.090444334 0.096191116 0.10036661 0.10314883 0.10369562 0.10230552 0.097731344 0.088293485 0.074133612 0.059161607 0.047919679][0.064312272 0.082455166 0.10226014 0.12125476 0.13739668 0.14904356 0.15661062 0.16005668 0.1585422 0.15284654 0.14214589 0.12525386 0.10243908 0.078599826 0.059474539][0.086440057 0.11231794 0.14247411 0.17310667 0.20012571 0.21984281 0.23139112 0.23414454 0.227614 0.21369648 0.19286393 0.16519077 0.13157478 0.097803213 0.070581533][0.11648197 0.14841826 0.18778263 0.2294358 0.26646993 0.29265529 0.30555812 0.30423924 0.28905609 0.26374188 0.23080327 0.19185255 0.14864913 0.10733417 0.074879356][0.14900066 0.18275648 0.22688657 0.27546224 0.31842068 0.34700239 0.35726863 0.34848219 0.32224989 0.28443807 0.24001305 0.19228911 0.14395644 0.1004958 0.068071432][0.17111485 0.20156975 0.24464007 0.294404 0.3381941 0.36515486 0.37023339 0.35323775 0.31700209 0.26936489 0.21765675 0.16664487 0.11941192 0.079930387 0.052676372][0.17196876 0.19497567 0.23164792 0.27657965 0.31604239 0.33835265 0.338129 0.31577516 0.27490473 0.2244125 0.17290354 0.12558979 0.08529903 0.05417946 0.034889307][0.15048625 0.16442154 0.19115199 0.22640166 0.25734106 0.27314773 0.26902321 0.24586858 0.20738938 0.16225626 0.11874807 0.081293978 0.05190913 0.031187475 0.020259256][0.11538105 0.12054829 0.136086 0.15939592 0.18025059 0.18987876 0.18448794 0.16517803 0.13493477 0.10099572 0.070055351 0.045191225 0.027422696 0.016359845 0.012200985][0.077269018 0.076231219 0.082621783 0.095496438 0.10785519 0.11340905 0.10939454 0.096599735 0.076872185 0.055537246 0.037290711 0.023829885 0.015328969 0.011096903 0.010978877][0.044947505 0.041555017 0.043100327 0.049578037 0.056775898 0.060604479 0.059171695 0.052738536 0.042099822 0.03085559 0.022058407 0.01649289 0.013661165 0.012874349 0.014149741][0.026015528 0.023476962 0.023954583 0.027665582 0.032276627 0.035330787 0.035534043 0.032843035 0.02756012 0.022098256 0.018329633 0.016548071 0.016173894 0.016549274 0.017836221]]...]
INFO - root - 2017-12-10 09:54:25.086058: step 3910, loss = 0.73, batch loss = 0.68 (10.0 examples/sec; 0.796 sec/batch; 72h:41m:53s remains)
INFO - root - 2017-12-10 09:54:32.890746: step 3920, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.769 sec/batch; 70h:11m:22s remains)
INFO - root - 2017-12-10 09:54:40.718217: step 3930, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.785 sec/batch; 71h:39m:00s remains)
INFO - root - 2017-12-10 09:54:48.448500: step 3940, loss = 0.72, batch loss = 0.67 (10.1 examples/sec; 0.795 sec/batch; 72h:32m:26s remains)
INFO - root - 2017-12-10 09:54:56.302455: step 3950, loss = 0.73, batch loss = 0.67 (10.1 examples/sec; 0.794 sec/batch; 72h:28m:20s remains)
INFO - root - 2017-12-10 09:55:03.973583: step 3960, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.769 sec/batch; 70h:08m:43s remains)
INFO - root - 2017-12-10 09:55:11.781826: step 3970, loss = 0.73, batch loss = 0.67 (10.3 examples/sec; 0.778 sec/batch; 70h:59m:09s remains)
INFO - root - 2017-12-10 09:55:19.692347: step 3980, loss = 0.73, batch loss = 0.67 (10.1 examples/sec; 0.792 sec/batch; 72h:13m:56s remains)
INFO - root - 2017-12-10 09:55:27.501222: step 3990, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.762 sec/batch; 69h:32m:53s remains)
INFO - root - 2017-12-10 09:55:35.439481: step 4000, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.776 sec/batch; 70h:47m:02s remains)
2017-12-10 09:55:36.280198: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15467046 0.14993466 0.14021106 0.13067262 0.12224848 0.11256893 0.10245198 0.095355444 0.093006872 0.091106541 0.086172022 0.083418131 0.086230256 0.091791831 0.0948039][0.15093967 0.14671823 0.139092 0.13160917 0.12454226 0.1161492 0.10723069 0.1009766 0.099208169 0.0986417 0.096349128 0.097262293 0.10348598 0.11156381 0.11571892][0.13951139 0.13792337 0.13420917 0.1303108 0.12616554 0.12039597 0.11334447 0.10750412 0.10503931 0.10421316 0.10312387 0.10634101 0.11510286 0.12540716 0.13118255][0.13259028 0.13460098 0.1348554 0.13426743 0.13268168 0.12907183 0.12348095 0.11764222 0.11378635 0.11155584 0.11025039 0.11386118 0.12314577 0.13416499 0.1408523][0.13484187 0.14012702 0.14273092 0.14365695 0.14313501 0.14040357 0.13531812 0.12902853 0.12369627 0.11973304 0.11713588 0.1191099 0.12644064 0.13605961 0.14250787][0.14584659 0.15324032 0.15602705 0.15670551 0.15633103 0.15421472 0.14932844 0.14231287 0.13526377 0.12886973 0.12348569 0.12168461 0.12483637 0.13111129 0.13630889][0.16078937 0.16851845 0.16975257 0.16911076 0.16867959 0.16777879 0.16377777 0.15637888 0.14751609 0.13802506 0.12877229 0.12190132 0.1196578 0.12161696 0.12512921][0.17244892 0.17986736 0.17913799 0.17715253 0.17656349 0.17681532 0.17405927 0.16682518 0.15639263 0.14379197 0.1307317 0.11929056 0.11209393 0.11006905 0.11214225][0.17474046 0.18183421 0.18004438 0.17765516 0.17734815 0.17882444 0.17774974 0.17163645 0.16080604 0.14630909 0.13067158 0.11616801 0.10531902 0.10005192 0.10056608][0.16389805 0.17115228 0.17005908 0.1692787 0.17090653 0.17454557 0.17571034 0.17136732 0.16100584 0.14554064 0.12812299 0.11151099 0.098368987 0.091307923 0.091181114][0.14024457 0.14809714 0.15002917 0.15369256 0.1596677 0.1665951 0.17019413 0.16742048 0.15735936 0.14094989 0.12200458 0.10421108 0.09055046 0.083899096 0.084870189][0.10932789 0.11832318 0.12495007 0.13491337 0.14650188 0.15673667 0.16201395 0.16001666 0.1498256 0.13254015 0.11250478 0.094466574 0.081840262 0.077411518 0.080842443][0.080913924 0.090666257 0.10186436 0.11790099 0.13460134 0.14719006 0.15299402 0.15094239 0.1403975 0.12260216 0.10203022 0.084143408 0.073089667 0.071405441 0.077542186][0.062502272 0.072194323 0.086317942 0.10638376 0.12642729 0.14019537 0.14557545 0.14285558 0.13207605 0.11450586 0.094077811 0.076469056 0.066755712 0.067278586 0.07562118][0.056640413 0.066225931 0.081596971 0.10327117 0.12427286 0.13761601 0.14136481 0.13706873 0.12563439 0.10831702 0.088453487 0.071442246 0.063092425 0.065493368 0.076007672]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 09:55:44.072996: step 4010, loss = 0.73, batch loss = 0.67 (10.7 examples/sec; 0.750 sec/batch; 68h:28m:45s remains)
INFO - root - 2017-12-10 09:55:51.942742: step 4020, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.791 sec/batch; 72h:08m:01s remains)
INFO - root - 2017-12-10 09:55:59.736356: step 4030, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.789 sec/batch; 71h:59m:17s remains)
INFO - root - 2017-12-10 09:56:07.373422: step 4040, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.800 sec/batch; 72h:57m:12s remains)
INFO - root - 2017-12-10 09:56:15.258287: step 4050, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 71h:10m:47s remains)
INFO - root - 2017-12-10 09:56:23.147425: step 4060, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 72h:41m:57s remains)
INFO - root - 2017-12-10 09:56:30.859011: step 4070, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.770 sec/batch; 70h:12m:50s remains)
INFO - root - 2017-12-10 09:56:38.663657: step 4080, loss = 0.72, batch loss = 0.67 (10.6 examples/sec; 0.756 sec/batch; 68h:59m:51s remains)
INFO - root - 2017-12-10 09:56:46.513122: step 4090, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.792 sec/batch; 72h:17m:07s remains)
INFO - root - 2017-12-10 09:56:54.288844: step 4100, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.786 sec/batch; 71h:44m:22s remains)
2017-12-10 09:56:55.062419: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.047446359 0.045840006 0.041845527 0.037846059 0.03586879 0.036040638 0.03774238 0.040382277 0.043306865 0.046105511 0.049209245 0.052159566 0.054217592 0.054352161 0.051860835][0.083368637 0.081367433 0.075077146 0.068723343 0.065759644 0.066467173 0.069215715 0.072757587 0.076322839 0.07984972 0.08402025 0.087924615 0.090580583 0.090489872 0.08689893][0.12316079 0.1220997 0.11438131 0.10664491 0.10349394 0.10532882 0.10915519 0.11311785 0.11681496 0.12039445 0.12489615 0.12932189 0.13256104 0.13233787 0.12779273][0.15721855 0.15843146 0.15123041 0.14394923 0.14193235 0.14569013 0.15097776 0.15499534 0.15790732 0.16014859 0.1633313 0.16694665 0.17001766 0.16984957 0.16511211][0.17644645 0.18141963 0.17708755 0.17237711 0.17286737 0.17945999 0.1869811 0.19136687 0.19294015 0.19239932 0.1923207 0.19363996 0.19594471 0.19627331 0.19255556][0.18004999 0.19009364 0.190344 0.18939368 0.19291189 0.20211887 0.21154287 0.2162082 0.21627443 0.21252842 0.20836681 0.20694368 0.2084531 0.20987345 0.20845254][0.17437229 0.18886714 0.19330236 0.1955203 0.20111316 0.21119495 0.22064011 0.22435932 0.22256362 0.21613252 0.20852068 0.20507526 0.20638554 0.20984091 0.21160151][0.16728681 0.18254353 0.18813591 0.19125016 0.19685112 0.20544475 0.21273082 0.21458949 0.21140501 0.20384946 0.19483767 0.19143705 0.19435285 0.20114334 0.20636921][0.16404104 0.17518249 0.17798911 0.17919333 0.18265563 0.18816902 0.19252628 0.19283329 0.18920296 0.18199308 0.1737411 0.17261879 0.17903274 0.18997182 0.19838123][0.16986601 0.17498656 0.1727118 0.17033494 0.1705565 0.1727192 0.17439958 0.17380987 0.17035815 0.16411485 0.15765414 0.1601339 0.17153664 0.18663584 0.19692303][0.18409574 0.18429466 0.17731702 0.17187494 0.16978425 0.16992588 0.17000706 0.16886592 0.16522367 0.15929893 0.15393518 0.15904379 0.17447686 0.19215177 0.20221028][0.1974947 0.19502068 0.18572289 0.18038194 0.1798805 0.18173209 0.18275177 0.18165013 0.17698365 0.16988294 0.16349451 0.16829829 0.18473674 0.20264009 0.21080577][0.20373614 0.20064384 0.19217879 0.19070426 0.19605207 0.20346214 0.20791659 0.20746411 0.20114735 0.1917382 0.1827306 0.18485203 0.19978766 0.21664442 0.22289118][0.20138736 0.199924 0.19518575 0.19985056 0.21282755 0.22688645 0.23491836 0.23426756 0.22511797 0.21228409 0.19957188 0.19781387 0.21001579 0.22546339 0.23046489][0.18940751 0.19081537 0.19073915 0.20091107 0.219711 0.23823185 0.24767332 0.24518421 0.23245603 0.21616428 0.2003642 0.19543086 0.20520161 0.21961692 0.22432214]]...]
INFO - root - 2017-12-10 09:57:02.823263: step 4110, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.765 sec/batch; 69h:49m:23s remains)
INFO - root - 2017-12-10 09:57:10.237574: step 4120, loss = 0.73, batch loss = 0.67 (10.4 examples/sec; 0.770 sec/batch; 70h:13m:23s remains)
INFO - root - 2017-12-10 09:57:18.078239: step 4130, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 70h:59m:41s remains)
INFO - root - 2017-12-10 09:57:25.893268: step 4140, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 70h:55m:12s remains)
INFO - root - 2017-12-10 09:57:33.823815: step 4150, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.783 sec/batch; 71h:22m:37s remains)
INFO - root - 2017-12-10 09:57:41.702219: step 4160, loss = 0.72, batch loss = 0.67 (10.3 examples/sec; 0.777 sec/batch; 70h:49m:29s remains)
INFO - root - 2017-12-10 09:57:49.608279: step 4170, loss = 0.73, batch loss = 0.67 (9.8 examples/sec; 0.814 sec/batch; 74h:13m:10s remains)
INFO - root - 2017-12-10 09:57:57.568218: step 4180, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.809 sec/batch; 73h:48m:54s remains)
INFO - root - 2017-12-10 09:58:05.420693: step 4190, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.766 sec/batch; 69h:53m:48s remains)
INFO - root - 2017-12-10 09:58:13.034370: step 4200, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.807 sec/batch; 73h:36m:31s remains)
2017-12-10 09:58:13.777047: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.051057812 0.053384826 0.055386957 0.061154954 0.07431183 0.092246436 0.11160377 0.13112502 0.15101978 0.16929756 0.18136817 0.18352452 0.17461781 0.15393913 0.12307467][0.068156525 0.071746662 0.0771725 0.089267366 0.11230367 0.14207566 0.17238304 0.20003331 0.22465691 0.24375206 0.25214964 0.24709849 0.2296724 0.20035063 0.1611667][0.089800447 0.093755789 0.10260445 0.12133596 0.1543439 0.19574711 0.23602405 0.26916504 0.29394704 0.30814695 0.30689487 0.28957132 0.26060525 0.22296867 0.17897356][0.12076689 0.12305827 0.13351212 0.15659323 0.19604324 0.2448135 0.29010957 0.32314268 0.34223086 0.34635225 0.33173478 0.30028713 0.25992274 0.21637793 0.17203176][0.16358547 0.16177833 0.17006731 0.19298205 0.23290662 0.28188869 0.32477236 0.35121128 0.359863 0.35146889 0.32375678 0.28047368 0.23202769 0.18616992 0.14518796][0.21467321 0.20574993 0.2069696 0.22362728 0.2568914 0.29838648 0.33215716 0.34785107 0.34511176 0.32659605 0.29100719 0.24244645 0.19162819 0.14692442 0.11067441][0.26196516 0.24356923 0.23309535 0.23791569 0.25864425 0.28706744 0.30851471 0.31396583 0.30429566 0.28299096 0.24843678 0.20339546 0.15632398 0.11491271 0.081960067][0.28737283 0.26057428 0.23741792 0.22804402 0.2335401 0.24704911 0.25690821 0.25676692 0.24837874 0.23421751 0.21052983 0.1769972 0.13826044 0.10039422 0.067504339][0.28243667 0.25103393 0.21820834 0.19647205 0.18809316 0.18838999 0.18994996 0.18977167 0.18969449 0.18970796 0.18278809 0.16482805 0.13624439 0.10131821 0.066007659][0.24747534 0.2155306 0.17805539 0.14881968 0.1314051 0.12340286 0.1216426 0.12592863 0.13711147 0.15236011 0.16163605 0.1579528 0.13851929 0.10639109 0.069178961][0.19089447 0.16171144 0.12553135 0.095236123 0.075010993 0.064375982 0.063404217 0.073556863 0.094111316 0.12025061 0.14051794 0.14687558 0.13500684 0.10735247 0.072327293][0.12745795 0.10394166 0.074391253 0.049133088 0.031708274 0.022520777 0.023559034 0.037142012 0.0615367 0.0914119 0.11594431 0.12782329 0.12282626 0.10302562 0.0763563][0.073010191 0.056455187 0.035997592 0.018740049 0.0069005378 0.0008079629 0.0031435939 0.016619219 0.039313909 0.066631123 0.089936465 0.1040462 0.10602089 0.097563185 0.085174084][0.036115844 0.025504053 0.012969133 0.0029182245 -0.00355126 -0.0065754429 -0.0037049924 0.0079142107 0.026579326 0.048781425 0.068622418 0.083697759 0.092977986 0.0980065 0.10378941][0.0149423 0.0086477855 0.0015840284 -0.0037226926 -0.0066220136 -0.0073823915 -0.004070879 0.0061990819 0.022018485 0.040486563 0.057918523 0.074362792 0.090754837 0.10928095 0.13423468]]...]
INFO - root - 2017-12-10 09:58:21.673968: step 4210, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.776 sec/batch; 70h:43m:37s remains)
INFO - root - 2017-12-10 09:58:29.625442: step 4220, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.787 sec/batch; 71h:45m:59s remains)
INFO - root - 2017-12-10 09:58:37.504431: step 4230, loss = 0.72, batch loss = 0.67 (10.1 examples/sec; 0.792 sec/batch; 72h:10m:49s remains)
INFO - root - 2017-12-10 09:58:45.458492: step 4240, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.773 sec/batch; 70h:28m:35s remains)
INFO - root - 2017-12-10 09:58:53.351444: step 4250, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 71h:04m:13s remains)
INFO - root - 2017-12-10 09:59:01.343076: step 4260, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.782 sec/batch; 71h:17m:01s remains)
INFO - root - 2017-12-10 09:59:09.148368: step 4270, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 72h:38m:08s remains)
INFO - root - 2017-12-10 09:59:16.860012: step 4280, loss = 0.72, batch loss = 0.66 (9.7 examples/sec; 0.828 sec/batch; 75h:30m:52s remains)
INFO - root - 2017-12-10 09:59:24.752252: step 4290, loss = 0.73, batch loss = 0.67 (10.7 examples/sec; 0.751 sec/batch; 68h:28m:16s remains)
INFO - root - 2017-12-10 09:59:32.501471: step 4300, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 72h:24m:29s remains)
2017-12-10 09:59:33.362887: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26336566 0.24316394 0.22315192 0.21049415 0.20690583 0.2097396 0.20805627 0.20157066 0.18968265 0.17454535 0.16342665 0.16525233 0.181743 0.2028653 0.21603008][0.33212247 0.30737862 0.28349006 0.26995343 0.26841754 0.27618247 0.2777808 0.27021891 0.25285083 0.230516 0.2129302 0.20973001 0.22539708 0.24894245 0.26390815][0.3680903 0.34287152 0.320694 0.31158721 0.31642923 0.3330622 0.34223324 0.33707327 0.31610456 0.28682393 0.26110125 0.24980947 0.26067498 0.28285712 0.29713929][0.38549086 0.36349556 0.34614393 0.34261316 0.35325909 0.37729558 0.39436355 0.39379907 0.3722513 0.33857521 0.30581391 0.28652683 0.29105851 0.30992806 0.32224295][0.38780227 0.37190777 0.35931742 0.35934815 0.37261006 0.40090877 0.42514986 0.43171731 0.41480523 0.38268203 0.3483144 0.32498884 0.32442904 0.33877596 0.34693122][0.37262222 0.36135286 0.35131672 0.35369596 0.3699 0.403673 0.43737563 0.45455453 0.44721493 0.42143375 0.38996 0.36646298 0.3618992 0.36979756 0.37006551][0.35830298 0.34746462 0.33635741 0.33980489 0.35992447 0.39976507 0.442292 0.46914729 0.47059584 0.4504582 0.42260441 0.4012942 0.39424714 0.394925 0.38488546][0.35508233 0.34053564 0.32661051 0.33105484 0.35601521 0.40041381 0.44702205 0.47700998 0.4804931 0.46036819 0.43286055 0.4139297 0.4065558 0.40258753 0.38503316][0.35590076 0.33746141 0.32057557 0.32522762 0.35256967 0.39578971 0.43792242 0.46237269 0.46159759 0.4387747 0.41129526 0.39539662 0.38982749 0.38453415 0.3640753][0.35041162 0.3308526 0.31395245 0.31938842 0.3464103 0.38320249 0.41445082 0.42804506 0.41997576 0.39451224 0.3680284 0.35530561 0.35223624 0.34789351 0.3287369][0.33174405 0.31377953 0.29963833 0.30717835 0.33355361 0.3631894 0.38328841 0.38674149 0.37270302 0.34637454 0.32207128 0.31291389 0.31341761 0.31260404 0.29807407][0.30253705 0.28955147 0.28033108 0.2905148 0.31604689 0.33966395 0.35150406 0.34861952 0.3321397 0.30772972 0.28770292 0.28394368 0.29068884 0.29615703 0.28783244][0.26028922 0.25394303 0.25092742 0.2635386 0.2876167 0.30716902 0.31551781 0.31253481 0.29995036 0.28279257 0.27141646 0.27630213 0.29136238 0.30390385 0.30044127][0.21650572 0.21634051 0.21866864 0.23252738 0.25430909 0.27180392 0.28110421 0.28336966 0.27972957 0.27351162 0.27301618 0.28661269 0.30783266 0.32408071 0.32112408][0.18930867 0.19370404 0.19949546 0.21337132 0.23253612 0.24938792 0.26163056 0.27005959 0.27475947 0.27735934 0.28444165 0.30249268 0.32536691 0.342046 0.33786514]]...]
INFO - root - 2017-12-10 09:59:41.279999: step 4310, loss = 0.73, batch loss = 0.67 (10.1 examples/sec; 0.790 sec/batch; 71h:58m:31s remains)
INFO - root - 2017-12-10 09:59:49.218459: step 4320, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.784 sec/batch; 71h:30m:37s remains)
INFO - root - 2017-12-10 09:59:57.048664: step 4330, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 70h:21m:26s remains)
INFO - root - 2017-12-10 10:00:05.066776: step 4340, loss = 0.72, batch loss = 0.66 (9.7 examples/sec; 0.821 sec/batch; 74h:48m:22s remains)
INFO - root - 2017-12-10 10:00:12.915223: step 4350, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.802 sec/batch; 73h:06m:25s remains)
INFO - root - 2017-12-10 10:00:20.266164: step 4360, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.767 sec/batch; 69h:55m:33s remains)
INFO - root - 2017-12-10 10:00:28.174242: step 4370, loss = 0.72, batch loss = 0.67 (9.6 examples/sec; 0.829 sec/batch; 75h:35m:06s remains)
INFO - root - 2017-12-10 10:00:35.841373: step 4380, loss = 0.71, batch loss = 0.66 (10.9 examples/sec; 0.732 sec/batch; 66h:40m:52s remains)
INFO - root - 2017-12-10 10:00:43.813161: step 4390, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 72h:27m:35s remains)
INFO - root - 2017-12-10 10:00:51.700337: step 4400, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 71h:07m:02s remains)
2017-12-10 10:00:52.546084: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13597101 0.12725951 0.11073466 0.091760762 0.072897583 0.057977069 0.048694346 0.045044746 0.051391207 0.069118641 0.094258986 0.12059429 0.14252792 0.15357766 0.15022796][0.18783154 0.18113194 0.1630473 0.1403017 0.11558434 0.094066545 0.079621114 0.073281512 0.080388851 0.10284734 0.13502897 0.16910565 0.19705962 0.2103975 0.20392749][0.22509265 0.22442655 0.21000572 0.18892986 0.16331175 0.1388661 0.12136913 0.11247979 0.11757338 0.13926508 0.17276435 0.20983016 0.23954795 0.25177991 0.24085617][0.2456086 0.25559357 0.251378 0.23908022 0.21936558 0.19705647 0.17852415 0.16588508 0.16372019 0.17634672 0.20192942 0.23356184 0.25845656 0.26548511 0.24953642][0.24940343 0.27287692 0.28388304 0.28655562 0.27921247 0.26430503 0.2470948 0.22966652 0.21572518 0.2124602 0.22223328 0.24087943 0.25457335 0.25269467 0.23156004][0.24226071 0.27847269 0.30548573 0.32475191 0.33237246 0.32770389 0.31375214 0.29206347 0.26613292 0.24617743 0.23821515 0.24116164 0.24113402 0.22869895 0.20255023][0.23220813 0.27708355 0.3154242 0.34667364 0.36532918 0.36787331 0.35587215 0.3306365 0.29657072 0.26552078 0.24499765 0.23587474 0.22539786 0.20558143 0.17679825][0.23434874 0.28435376 0.32697418 0.36234191 0.384308 0.38747698 0.3736366 0.34562123 0.30938503 0.27555844 0.2509279 0.2370363 0.22300351 0.2016245 0.17295745][0.25493339 0.30774024 0.34911928 0.38113546 0.39805037 0.39495543 0.37625611 0.34710965 0.31449568 0.2861999 0.26617053 0.25478771 0.24320765 0.22501379 0.19858073][0.28732648 0.34188271 0.37880427 0.40228808 0.40836227 0.39517087 0.3713491 0.34403345 0.32040021 0.30454686 0.2964856 0.29319715 0.2868056 0.27250484 0.24772374][0.32235005 0.37777388 0.409204 0.42270547 0.41668832 0.39348802 0.36588567 0.3423107 0.32914758 0.32735664 0.33320832 0.33932033 0.33819735 0.32679641 0.30263674][0.343859 0.39890358 0.42559975 0.43170625 0.41736707 0.38805142 0.35875288 0.33865029 0.33278385 0.34027505 0.35541365 0.36748996 0.36906454 0.3588312 0.33538529][0.33819327 0.38982069 0.41267541 0.4152588 0.39846665 0.36907119 0.34144565 0.32420823 0.32086843 0.3304477 0.34791291 0.36110961 0.36243415 0.3516399 0.32921377][0.2926017 0.33563045 0.35386404 0.35496187 0.33993509 0.31549957 0.29327577 0.279617 0.27643764 0.28397891 0.29926544 0.3103368 0.30948615 0.2972582 0.27649891][0.21605387 0.24723552 0.2599436 0.26023746 0.24894629 0.2315681 0.21589613 0.20581092 0.20213512 0.20634224 0.21724109 0.22460929 0.22131138 0.20837188 0.1903353]]...]
INFO - root - 2017-12-10 10:01:00.603312: step 4410, loss = 0.73, batch loss = 0.67 (9.8 examples/sec; 0.813 sec/batch; 74h:06m:27s remains)
INFO - root - 2017-12-10 10:01:08.469802: step 4420, loss = 0.73, batch loss = 0.68 (10.1 examples/sec; 0.795 sec/batch; 72h:27m:20s remains)
INFO - root - 2017-12-10 10:01:16.351024: step 4430, loss = 0.72, batch loss = 0.67 (10.0 examples/sec; 0.804 sec/batch; 73h:14m:56s remains)
INFO - root - 2017-12-10 10:01:24.083224: step 4440, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.781 sec/batch; 71h:07m:45s remains)
INFO - root - 2017-12-10 10:01:31.971692: step 4450, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.794 sec/batch; 72h:21m:20s remains)
INFO - root - 2017-12-10 10:01:40.020269: step 4460, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.803 sec/batch; 73h:10m:49s remains)
INFO - root - 2017-12-10 10:01:47.848192: step 4470, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 71h:05m:26s remains)
INFO - root - 2017-12-10 10:01:55.675336: step 4480, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.763 sec/batch; 69h:31m:07s remains)
INFO - root - 2017-12-10 10:02:03.536087: step 4490, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.786 sec/batch; 71h:37m:38s remains)
INFO - root - 2017-12-10 10:02:11.348748: step 4500, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.773 sec/batch; 70h:24m:12s remains)
2017-12-10 10:02:12.177661: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.072187461 0.057856448 0.055542856 0.062312353 0.07507541 0.08935836 0.10025576 0.10523923 0.10503171 0.10082847 0.090633616 0.075980909 0.06173363 0.051861536 0.0468837][0.0863429 0.067740344 0.065557592 0.077775545 0.10076905 0.12807371 0.15126951 0.16405191 0.16485444 0.15586743 0.1354661 0.107338 0.079891309 0.060310017 0.049766198][0.096925311 0.0771594 0.077292755 0.097213753 0.1332431 0.17683323 0.21546744 0.23799615 0.23979458 0.2242391 0.19085605 0.14596455 0.10223806 0.070710272 0.05295559][0.10776824 0.088878773 0.09207239 0.11948603 0.16764536 0.22602113 0.27860397 0.30960694 0.31136721 0.28861439 0.2428779 0.1832006 0.1251521 0.082584828 0.058011051][0.12157556 0.1034471 0.1080743 0.14016075 0.19652708 0.26549786 0.32842946 0.36537433 0.365365 0.33460292 0.27783009 0.20709172 0.13957225 0.090109877 0.061677169][0.13784528 0.12079187 0.12530035 0.15927964 0.22011408 0.29555559 0.36512887 0.4050703 0.40148428 0.36160788 0.29429659 0.21523267 0.14270774 0.090873629 0.061985932][0.15474458 0.13969921 0.14301209 0.17614193 0.23716067 0.3136611 0.38448253 0.42370382 0.41614911 0.36878213 0.29438803 0.21133959 0.13810633 0.087330967 0.059718817][0.17203596 0.15856588 0.15915924 0.18847236 0.24506776 0.3161445 0.38110667 0.41423917 0.40184721 0.35015851 0.27437758 0.19427858 0.12678641 0.081455782 0.056929667][0.18722901 0.17258112 0.16904053 0.1937383 0.24464053 0.30739436 0.36183149 0.38480642 0.36634216 0.31255159 0.240048 0.16866638 0.11186118 0.075152017 0.055250861][0.20307554 0.18242125 0.17121083 0.19012882 0.23649885 0.29334432 0.33925447 0.3536244 0.330748 0.27725121 0.2104976 0.14987376 0.10557788 0.078807861 0.064138018][0.22614416 0.19608207 0.17308863 0.18271424 0.22261058 0.27440372 0.31567577 0.32733023 0.30631691 0.25932604 0.20296143 0.15548335 0.12467387 0.10838522 0.098975457][0.253631 0.21360716 0.17705064 0.17432067 0.20412062 0.24859898 0.28600502 0.29902557 0.28568459 0.251977 0.21197151 0.181677 0.16657978 0.16206731 0.15861779][0.27798229 0.22954139 0.18082793 0.16507392 0.1825482 0.21752672 0.25006756 0.2650035 0.26091978 0.24290285 0.22180247 0.21043351 0.21251687 0.22156055 0.22641864][0.28695634 0.23339432 0.17694578 0.15090984 0.15738942 0.18356398 0.211572 0.22822963 0.23227601 0.22767651 0.22248875 0.22663052 0.24327371 0.26485407 0.27905041][0.27313441 0.21934494 0.16061436 0.12814033 0.12600315 0.14497803 0.16962503 0.18812546 0.19895169 0.2046449 0.21056749 0.22442086 0.24995385 0.27957845 0.30047721]]...]
INFO - root - 2017-12-10 10:02:19.991951: step 4510, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 70h:06m:10s remains)
INFO - root - 2017-12-10 10:02:27.642732: step 4520, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.794 sec/batch; 72h:21m:41s remains)
INFO - root - 2017-12-10 10:02:35.541423: step 4530, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.796 sec/batch; 72h:31m:03s remains)
INFO - root - 2017-12-10 10:02:43.434163: step 4540, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 69h:35m:17s remains)
INFO - root - 2017-12-10 10:02:51.262990: step 4550, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.794 sec/batch; 72h:22m:04s remains)
INFO - root - 2017-12-10 10:02:58.886828: step 4560, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 70h:10m:10s remains)
INFO - root - 2017-12-10 10:03:06.732698: step 4570, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 69h:41m:56s remains)
INFO - root - 2017-12-10 10:03:14.554652: step 4580, loss = 0.72, batch loss = 0.67 (10.3 examples/sec; 0.775 sec/batch; 70h:33m:57s remains)
INFO - root - 2017-12-10 10:03:22.406380: step 4590, loss = 0.72, batch loss = 0.66 (10.7 examples/sec; 0.751 sec/batch; 68h:24m:07s remains)
INFO - root - 2017-12-10 10:03:30.035712: step 4600, loss = 0.73, batch loss = 0.67 (10.4 examples/sec; 0.768 sec/batch; 69h:59m:21s remains)
2017-12-10 10:03:30.813254: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.07435599 0.07445389 0.073473349 0.07304775 0.073876172 0.075420417 0.077081636 0.078576967 0.083243415 0.092737325 0.10102604 0.10472745 0.10420907 0.10191034 0.093612634][0.089118682 0.090538576 0.090459764 0.090938978 0.092586763 0.09462595 0.096394233 0.097919822 0.10371398 0.11562182 0.12621543 0.13103078 0.13041238 0.12771767 0.11781296][0.10032392 0.10494709 0.10725255 0.10924637 0.11135371 0.11269606 0.11302069 0.11267158 0.11653402 0.12662947 0.13565467 0.13924403 0.13729338 0.13327758 0.12203466][0.11748289 0.12715502 0.13320783 0.13727558 0.13963446 0.1396936 0.13794851 0.13476661 0.13417655 0.13798557 0.140525 0.13891961 0.13268679 0.12478093 0.11096498][0.15155691 0.16724643 0.17744619 0.18349274 0.18581085 0.18496065 0.18200748 0.17636798 0.17001657 0.16456211 0.15667868 0.14567152 0.1311969 0.11596276 0.097205862][0.19889441 0.21923478 0.23200579 0.23868102 0.24061179 0.24034713 0.23895974 0.23357463 0.22310327 0.20918776 0.19039844 0.16813624 0.14261317 0.11717834 0.091280237][0.24114859 0.26200813 0.27393636 0.27951863 0.28134498 0.28371909 0.28700769 0.28519306 0.27423492 0.25589758 0.22990784 0.19871028 0.16314183 0.12802827 0.095210962][0.25555989 0.27208689 0.279801 0.28299746 0.28532222 0.29142398 0.30061054 0.30429888 0.29646596 0.27898794 0.25193328 0.21789627 0.17775971 0.13781071 0.10166293][0.23307434 0.24276365 0.24503963 0.24579465 0.24895942 0.2579104 0.27101436 0.27893472 0.27534381 0.26231945 0.23961674 0.20934418 0.17200117 0.13431713 0.100358][0.18439728 0.18734798 0.18477714 0.183278 0.18652666 0.19590458 0.20934717 0.21863462 0.21849924 0.21134053 0.19605204 0.17395008 0.14497767 0.11523435 0.087917224][0.12877692 0.12613274 0.11984088 0.11628889 0.11846343 0.12602688 0.13712022 0.14574318 0.14857757 0.14858243 0.14313366 0.13207507 0.11434986 0.095245905 0.076088317][0.087398037 0.081317976 0.073059648 0.068195857 0.068586171 0.073048919 0.0804813 0.087378673 0.093097627 0.10130944 0.10710735 0.10790405 0.10107921 0.0916995 0.078785449][0.069442883 0.062994123 0.055298477 0.050576322 0.049373996 0.050703511 0.054536279 0.059520874 0.068232581 0.084805772 0.10145175 0.11241668 0.11327589 0.10972776 0.098914646][0.068923876 0.064798728 0.059761453 0.056726046 0.055005502 0.054430589 0.0559022 0.059423149 0.070607379 0.093699045 0.11830083 0.13581611 0.14039242 0.13874142 0.12667072][0.071270593 0.0703307 0.0686548 0.0679957 0.066693209 0.065503649 0.065919824 0.068595722 0.080985032 0.10708303 0.13521431 0.15537976 0.16105707 0.15941319 0.14544728]]...]
INFO - root - 2017-12-10 10:03:38.642506: step 4610, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.828 sec/batch; 75h:22m:59s remains)
INFO - root - 2017-12-10 10:03:46.623699: step 4620, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 73h:21m:53s remains)
INFO - root - 2017-12-10 10:03:54.624774: step 4630, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 71h:36m:41s remains)
INFO - root - 2017-12-10 10:04:02.527709: step 4640, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.773 sec/batch; 70h:23m:34s remains)
INFO - root - 2017-12-10 10:04:10.295946: step 4650, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.782 sec/batch; 71h:15m:35s remains)
INFO - root - 2017-12-10 10:04:18.134613: step 4660, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.801 sec/batch; 72h:54m:56s remains)
INFO - root - 2017-12-10 10:04:26.108184: step 4670, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 71h:52m:09s remains)
INFO - root - 2017-12-10 10:04:33.918191: step 4680, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 71h:53m:12s remains)
INFO - root - 2017-12-10 10:04:41.822029: step 4690, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 72h:33m:02s remains)
INFO - root - 2017-12-10 10:04:49.756172: step 4700, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 72h:48m:45s remains)
2017-12-10 10:04:50.587235: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.057132836 0.048069194 0.049280729 0.058531288 0.069715247 0.073902562 0.06774462 0.054228328 0.036661379 0.021187047 0.015563572 0.023049012 0.043094717 0.070568852 0.10101739][0.039576169 0.025318028 0.020328248 0.023211401 0.030112294 0.034447283 0.0327473 0.026009282 0.016041083 0.0077399132 0.0086028995 0.022147648 0.048565004 0.082789704 0.11907][0.034589306 0.018478997 0.010284867 0.009188747 0.013468234 0.018836629 0.022039512 0.022379639 0.020229492 0.019168718 0.025950039 0.044069033 0.07397756 0.11109149 0.14853629][0.045002341 0.031874247 0.025337523 0.025090454 0.030717267 0.039633211 0.048928078 0.056762908 0.061926551 0.066229358 0.075544514 0.093414329 0.12070028 0.15375409 0.18572211][0.066353917 0.06077024 0.060282834 0.065209046 0.075899236 0.090642355 0.10711965 0.12290668 0.13513878 0.14321341 0.15118913 0.16303398 0.1805367 0.201825 0.22132999][0.090907529 0.096365906 0.10538711 0.11845079 0.13622846 0.15727961 0.18022138 0.20277753 0.22092237 0.23145038 0.23565161 0.23785987 0.24111521 0.24570487 0.2485081][0.11358148 0.13048172 0.14969818 0.17128699 0.19585076 0.22168958 0.24869536 0.27519691 0.29691726 0.30871558 0.30870384 0.30108944 0.28999493 0.27743372 0.26338464][0.1301869 0.15509516 0.18128935 0.20828165 0.236477 0.26362982 0.29091254 0.31770185 0.3404707 0.35314137 0.35110474 0.33722988 0.31593028 0.29002345 0.26231182][0.13787502 0.16399407 0.19054545 0.21702351 0.24391693 0.26815739 0.29165119 0.31543666 0.33759531 0.35245845 0.3532925 0.34082448 0.31839165 0.28785855 0.25378066][0.13590075 0.15673269 0.17685789 0.19656178 0.21666653 0.23339193 0.24901994 0.266378 0.2857435 0.30325913 0.31152394 0.30920604 0.29726762 0.27490541 0.24593998][0.11970343 0.13153465 0.14225157 0.1531489 0.16516197 0.17412281 0.18181896 0.19206291 0.2067617 0.22485362 0.24098188 0.25317597 0.25969663 0.25566196 0.24186799][0.09175688 0.094189309 0.096362107 0.10071714 0.10813693 0.11393228 0.11794136 0.12340304 0.13266818 0.14780842 0.16784865 0.19224215 0.21748805 0.23445505 0.23988082][0.060503621 0.056488931 0.054599844 0.058041524 0.067260012 0.076422893 0.082169995 0.086078174 0.089861408 0.098339893 0.11571352 0.14436832 0.18055695 0.21250324 0.23402385][0.036540523 0.03064055 0.030337568 0.039281394 0.056869462 0.075196721 0.087612487 0.093675695 0.093707122 0.093893394 0.10250577 0.1253082 0.1597117 0.19411173 0.22211978][0.029056367 0.025432626 0.030679524 0.048834819 0.077817462 0.10729837 0.12829475 0.13876981 0.13712071 0.12914298 0.12524766 0.13420236 0.15531883 0.17905195 0.20116691]]...]
INFO - root - 2017-12-10 10:04:58.414004: step 4710, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.770 sec/batch; 70h:06m:23s remains)
INFO - root - 2017-12-10 10:05:06.338439: step 4720, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.774 sec/batch; 70h:29m:06s remains)
INFO - root - 2017-12-10 10:05:14.313605: step 4730, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 72h:59m:38s remains)
INFO - root - 2017-12-10 10:05:22.000735: step 4740, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.787 sec/batch; 71h:36m:45s remains)
INFO - root - 2017-12-10 10:05:29.940381: step 4750, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.770 sec/batch; 70h:04m:32s remains)
INFO - root - 2017-12-10 10:05:37.644473: step 4760, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 71h:35m:29s remains)
INFO - root - 2017-12-10 10:05:45.590558: step 4770, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 73h:58m:05s remains)
INFO - root - 2017-12-10 10:05:53.567723: step 4780, loss = 0.72, batch loss = 0.67 (9.0 examples/sec; 0.889 sec/batch; 80h:54m:16s remains)
INFO - root - 2017-12-10 10:06:01.422277: step 4790, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.761 sec/batch; 69h:14m:54s remains)
INFO - root - 2017-12-10 10:06:09.249726: step 4800, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 70h:23m:03s remains)
2017-12-10 10:06:10.106524: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13790607 0.13177271 0.126744 0.12177588 0.11496674 0.10692162 0.095344879 0.079603851 0.061753485 0.044851948 0.032046746 0.024065599 0.01917943 0.013493781 0.0055863918][0.19753034 0.19220749 0.18689406 0.18035832 0.17015724 0.15823112 0.14195357 0.11989394 0.094453938 0.070296831 0.051812176 0.039350551 0.030780762 0.022475522 0.013245935][0.24015073 0.24020402 0.24019271 0.23806226 0.23045108 0.21996583 0.20279543 0.17651136 0.14410962 0.11253199 0.08780501 0.0698022 0.056255449 0.044052098 0.032379955][0.25432411 0.26379183 0.27543813 0.28515097 0.28769493 0.28506991 0.27180812 0.24476469 0.20705649 0.16822556 0.13703713 0.11347878 0.094964296 0.078419954 0.063772619][0.23984903 0.26088053 0.28848067 0.31590411 0.33508155 0.34619683 0.34234214 0.31965345 0.28015867 0.23542087 0.19756061 0.16785423 0.14378965 0.12230093 0.1041845][0.20833081 0.24040675 0.28426832 0.33061433 0.36863124 0.39605236 0.40484685 0.390224 0.35289723 0.30528677 0.26183745 0.22568646 0.19585176 0.16987996 0.14866303][0.17283955 0.21021178 0.26440588 0.3256954 0.38091159 0.42437023 0.44682851 0.44299361 0.41205972 0.36537197 0.31775635 0.27519944 0.23996183 0.21081346 0.1880751][0.14247084 0.17663035 0.2318594 0.30018049 0.3668755 0.42240596 0.45591232 0.46183705 0.43842125 0.39492977 0.34497616 0.29747868 0.25867385 0.2292112 0.20855188][0.12546736 0.15052831 0.19734897 0.26179552 0.32937744 0.38771006 0.42484805 0.43536973 0.41738322 0.37787738 0.32846057 0.2802301 0.24272972 0.21815144 0.20520703][0.12012958 0.13485831 0.16778381 0.21906637 0.2767154 0.32757577 0.35973915 0.368241 0.35240409 0.31735593 0.27197793 0.22797331 0.19656986 0.18149424 0.18112391][0.12504768 0.13148381 0.14995158 0.18439655 0.22638837 0.26373622 0.28495356 0.2857843 0.26771832 0.23548402 0.19582237 0.15945311 0.13724613 0.13388631 0.14802012][0.14049158 0.14188343 0.14788982 0.16556135 0.19039349 0.21236885 0.22069788 0.211804 0.18904567 0.15785697 0.12383148 0.0959195 0.083100244 0.089961141 0.11624113][0.16389804 0.16462623 0.16231304 0.16589998 0.17503487 0.18273723 0.17939723 0.1620153 0.13489476 0.10413782 0.074789889 0.053705763 0.046916932 0.058493432 0.089952715][0.19434704 0.19844426 0.19146614 0.18384627 0.17939115 0.17470826 0.16228893 0.13996415 0.1115263 0.082760595 0.057835821 0.041286469 0.036177464 0.045779731 0.0736276][0.22646333 0.23568782 0.22706987 0.21181938 0.19704801 0.18308103 0.16535844 0.14229213 0.11609322 0.091381565 0.071108557 0.057198197 0.050245669 0.053169034 0.0711669]]...]
INFO - root - 2017-12-10 10:06:17.922155: step 4810, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 70h:17m:54s remains)
INFO - root - 2017-12-10 10:06:25.858872: step 4820, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.796 sec/batch; 72h:27m:51s remains)
INFO - root - 2017-12-10 10:06:33.751119: step 4830, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 70h:39m:52s remains)
INFO - root - 2017-12-10 10:06:41.461152: step 4840, loss = 0.72, batch loss = 0.66 (9.8 examples/sec; 0.815 sec/batch; 74h:12m:17s remains)
INFO - root - 2017-12-10 10:06:49.276397: step 4850, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 71h:12m:23s remains)
INFO - root - 2017-12-10 10:06:57.117919: step 4860, loss = 0.72, batch loss = 0.67 (10.4 examples/sec; 0.771 sec/batch; 70h:09m:22s remains)
INFO - root - 2017-12-10 10:07:04.888751: step 4870, loss = 0.71, batch loss = 0.66 (10.5 examples/sec; 0.763 sec/batch; 69h:27m:16s remains)
INFO - root - 2017-12-10 10:07:12.782703: step 4880, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.817 sec/batch; 74h:18m:42s remains)
INFO - root - 2017-12-10 10:07:20.727711: step 4890, loss = 0.73, batch loss = 0.67 (9.9 examples/sec; 0.806 sec/batch; 73h:22m:13s remains)
INFO - root - 2017-12-10 10:07:28.575602: step 4900, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 72h:04m:06s remains)
2017-12-10 10:07:29.409359: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19971929 0.17603222 0.1506688 0.14044571 0.15241361 0.17987579 0.20500189 0.21575643 0.21032842 0.19088596 0.16541773 0.14587165 0.13901475 0.14523029 0.15703629][0.20063008 0.17593071 0.14881654 0.13813309 0.15031838 0.17881727 0.20481065 0.21503907 0.2078626 0.1856785 0.15754709 0.13603128 0.13115235 0.14228831 0.15913954][0.19667527 0.17191535 0.14442463 0.1336538 0.14536072 0.17287357 0.19725673 0.20478737 0.19427839 0.16955844 0.14068158 0.12029181 0.11834139 0.13311586 0.15310884][0.20756844 0.18241854 0.15302171 0.14077692 0.15156229 0.17824458 0.20134591 0.20616183 0.19137128 0.16310439 0.13350482 0.11447435 0.11427254 0.13105424 0.15317246][0.222636 0.19828047 0.16720727 0.15323359 0.16324155 0.19009256 0.2137823 0.21784638 0.19980049 0.16789755 0.13668099 0.11735722 0.11722589 0.13515723 0.15978646][0.2248911 0.20254631 0.17235307 0.1589321 0.17037086 0.2000218 0.22736128 0.23393688 0.21530166 0.1808311 0.14673585 0.12460503 0.12242028 0.13988683 0.1664217][0.21234663 0.1943045 0.16799657 0.15770955 0.17246805 0.20636433 0.23861475 0.2492023 0.2315753 0.19537072 0.15717195 0.13018321 0.12420704 0.13980238 0.16719559][0.19294372 0.18229973 0.16411613 0.15978855 0.17864755 0.21578349 0.25073281 0.26302743 0.24499814 0.20543946 0.16072477 0.12572221 0.11266727 0.12357062 0.15002689][0.17880176 0.17897166 0.17243232 0.1766869 0.19980006 0.23734833 0.27030832 0.27929646 0.25733191 0.21227959 0.15968373 0.11521744 0.093377262 0.097553194 0.12071358][0.17153309 0.18328936 0.19030808 0.20458537 0.23089536 0.2645269 0.28946909 0.28935939 0.26009327 0.20976172 0.15295039 0.10430313 0.07774286 0.07667993 0.09515065][0.17050111 0.19175906 0.20946455 0.23063749 0.25656161 0.28221747 0.29549929 0.28426355 0.24832959 0.19686021 0.14316025 0.098719269 0.07387419 0.070752926 0.083772361][0.16845225 0.19380642 0.21526088 0.23687214 0.25779635 0.273185 0.27471161 0.25459111 0.21617629 0.16937757 0.12545675 0.091403477 0.072607711 0.069792755 0.077302627][0.14935349 0.17314951 0.19293417 0.21094346 0.22510923 0.23120625 0.22404847 0.1994265 0.16299164 0.12444449 0.092571266 0.070161454 0.058484133 0.057494868 0.061643109][0.11167859 0.13017491 0.14514744 0.1580222 0.16609992 0.16588081 0.15453547 0.13027939 0.099602923 0.071243845 0.051602822 0.040496495 0.036153883 0.037825089 0.040393181][0.06648685 0.077475943 0.085769378 0.092421353 0.095096886 0.091408 0.079999395 0.060352955 0.038323727 0.020948445 0.012522917 0.011200042 0.013483819 0.018043853 0.020467399]]...]
INFO - root - 2017-12-10 10:07:37.176156: step 4910, loss = 0.71, batch loss = 0.65 (12.8 examples/sec; 0.627 sec/batch; 57h:02m:09s remains)
INFO - root - 2017-12-10 10:07:44.743989: step 4920, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 71h:34m:34s remains)
INFO - root - 2017-12-10 10:07:52.526724: step 4930, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.779 sec/batch; 70h:50m:33s remains)
INFO - root - 2017-12-10 10:08:00.432825: step 4940, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.785 sec/batch; 71h:24m:38s remains)
INFO - root - 2017-12-10 10:08:08.470425: step 4950, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 71h:10m:34s remains)
INFO - root - 2017-12-10 10:08:16.332876: step 4960, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.776 sec/batch; 70h:36m:52s remains)
INFO - root - 2017-12-10 10:08:24.188340: step 4970, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 72h:54m:52s remains)
INFO - root - 2017-12-10 10:08:32.060722: step 4980, loss = 0.71, batch loss = 0.66 (10.5 examples/sec; 0.763 sec/batch; 69h:26m:47s remains)
INFO - root - 2017-12-10 10:08:39.884646: step 4990, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.747 sec/batch; 67h:55m:47s remains)
INFO - root - 2017-12-10 10:08:47.446702: step 5000, loss = 0.71, batch loss = 0.65 (13.0 examples/sec; 0.614 sec/batch; 55h:52m:06s remains)
2017-12-10 10:08:48.263703: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34157181 0.3144325 0.26869607 0.21091777 0.15298161 0.11189542 0.098681226 0.11084694 0.13651511 0.16410649 0.18571655 0.19848219 0.20122762 0.19430436 0.18075523][0.35469502 0.30509213 0.24299109 0.17962909 0.12712027 0.10049037 0.10667741 0.13848852 0.18065049 0.21891117 0.24621473 0.261315 0.26481849 0.2573567 0.24257502][0.29043052 0.22620618 0.1596771 0.10467368 0.071098417 0.069082476 0.099515952 0.15078957 0.20603953 0.25027233 0.27937421 0.29441744 0.29781038 0.29044959 0.27603602][0.17674039 0.11451419 0.061086603 0.028894121 0.023613146 0.049104076 0.10034344 0.16340561 0.22228068 0.26359859 0.28737405 0.29672852 0.29560357 0.2856482 0.27073404][0.062246379 0.018873312 -0.0074092639 -0.0080690123 0.017809041 0.067629591 0.13153699 0.19584167 0.24697118 0.27476838 0.28399712 0.28043634 0.26894137 0.2517052 0.23306075][-0.012797818 -0.029909361 -0.024511883 0.0073651052 0.062069576 0.1302072 0.1978932 0.25325307 0.28555086 0.28918132 0.27510706 0.25282094 0.2272772 0.1999574 0.17567062][-0.038526244 -0.032367613 -0.0006086693 0.057395641 0.13339062 0.21131895 0.27402189 0.31246254 0.31939328 0.29467255 0.25563714 0.21531783 0.17802306 0.14330755 0.11615244][-0.029188523 -0.0099185035 0.035360362 0.10589017 0.18932752 0.264833 0.31398439 0.33191919 0.31591552 0.27040228 0.21635914 0.16793735 0.12824439 0.0947934 0.072083026][-0.0025024912 0.016787222 0.059282277 0.12500842 0.19971389 0.26152235 0.2933524 0.2948592 0.26762247 0.2182155 0.16639064 0.12409495 0.093113482 0.069664687 0.057439167][0.029616177 0.0388792 0.06577564 0.11284433 0.1672892 0.21031535 0.2279955 0.22280699 0.19916956 0.16288486 0.12820935 0.10234509 0.086017385 0.075560182 0.074173294][0.064641781 0.059791688 0.065742053 0.08859016 0.12016784 0.14723834 0.1588379 0.15708463 0.14688084 0.13165514 0.11856539 0.11008227 0.10687785 0.10629314 0.11135441][0.099718772 0.082069434 0.069639064 0.071971066 0.086885087 0.10633238 0.12040917 0.1280667 0.13254632 0.1350915 0.13721693 0.13927959 0.14271952 0.14579441 0.15183027][0.12043233 0.095146477 0.072803043 0.06494195 0.073722795 0.093560718 0.11395014 0.1300377 0.14280859 0.15301445 0.15975061 0.16386949 0.16750096 0.16920847 0.17315018][0.11198624 0.085821576 0.063653946 0.05632522 0.066979952 0.089799322 0.11365113 0.13157125 0.14337815 0.15118957 0.15449445 0.15558149 0.15656932 0.1556751 0.15778132][0.072179474 0.050438493 0.0352156 0.033895489 0.047833014 0.070327044 0.091235831 0.10418114 0.10922804 0.11025274 0.10818011 0.1061202 0.10542344 0.10363352 0.1055963]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 10:08:55.974308: step 5010, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 71h:34m:05s remains)
INFO - root - 2017-12-10 10:09:03.797084: step 5020, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 69h:36m:30s remains)
INFO - root - 2017-12-10 10:09:11.833234: step 5030, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 69h:15m:38s remains)
INFO - root - 2017-12-10 10:09:19.575804: step 5040, loss = 0.71, batch loss = 0.66 (10.5 examples/sec; 0.765 sec/batch; 69h:35m:20s remains)
INFO - root - 2017-12-10 10:09:27.371145: step 5050, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 73h:23m:43s remains)
INFO - root - 2017-12-10 10:09:35.256885: step 5060, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.817 sec/batch; 74h:17m:50s remains)
INFO - root - 2017-12-10 10:09:43.080411: step 5070, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.808 sec/batch; 73h:29m:00s remains)
INFO - root - 2017-12-10 10:09:50.692583: step 5080, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 69h:14m:55s remains)
INFO - root - 2017-12-10 10:09:58.361606: step 5090, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.771 sec/batch; 70h:06m:18s remains)
INFO - root - 2017-12-10 10:10:06.148795: step 5100, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.766 sec/batch; 69h:39m:15s remains)
2017-12-10 10:10:07.053474: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.076995842 0.10568905 0.1347008 0.16069527 0.18074094 0.18979253 0.18142089 0.15917052 0.12827009 0.099748023 0.079639837 0.063208655 0.049524996 0.039395656 0.039826151][0.10334276 0.13428333 0.16547038 0.19254515 0.21268837 0.22054537 0.20987754 0.18416016 0.14993393 0.11930557 0.097301424 0.0787937 0.062168308 0.048550937 0.045854487][0.12559718 0.15198176 0.17750706 0.19834724 0.21388629 0.22078396 0.21375349 0.19341572 0.163894 0.13574573 0.11319087 0.092932872 0.074224733 0.058680486 0.054200903][0.14748777 0.16548121 0.18213929 0.19502166 0.20662595 0.21606459 0.21859096 0.20952782 0.18782687 0.16153222 0.13537458 0.11083086 0.089294933 0.072679631 0.06776569][0.16813309 0.17729503 0.18766323 0.19803354 0.21220216 0.2297944 0.2453004 0.24772935 0.23032966 0.200399 0.16550034 0.13368051 0.10858078 0.091396004 0.086571768][0.17791216 0.18263 0.19417083 0.21219884 0.23879975 0.27087814 0.300124 0.30964708 0.28893057 0.24813198 0.2000193 0.15919311 0.13064633 0.11344463 0.10911933][0.17699809 0.18075378 0.20028529 0.23531739 0.28261277 0.33315659 0.37446415 0.38488254 0.3531844 0.29535964 0.23118912 0.18107368 0.14983828 0.13339078 0.13042343][0.17384462 0.17871295 0.20674358 0.25810289 0.32307234 0.38648006 0.43283835 0.43881121 0.39501625 0.32183632 0.24482697 0.18821549 0.15605836 0.14149518 0.14097208][0.17110564 0.17857957 0.21165553 0.27006868 0.33909747 0.40008906 0.43853065 0.43468538 0.38342336 0.30519295 0.22621977 0.17008996 0.14013074 0.12851709 0.13048929][0.16686033 0.17886864 0.21163575 0.26372266 0.31868252 0.35955727 0.37724653 0.3604899 0.3095901 0.24101833 0.17489101 0.12857865 0.10436781 0.096151359 0.099356681][0.16392429 0.18111265 0.20803748 0.24187832 0.26860791 0.2776556 0.26789087 0.23850383 0.19467367 0.14657006 0.10362086 0.073834062 0.058143377 0.05349556 0.056699648][0.16715437 0.18820985 0.20725226 0.2202196 0.21729319 0.19569312 0.1618209 0.12264212 0.085888036 0.055899765 0.033212315 0.018339841 0.010713173 0.0093979817 0.012368866][0.18186456 0.20626009 0.21784404 0.21339147 0.1879752 0.14512676 0.096412249 0.051974133 0.019319242 -0.0014375764 -0.013930364 -0.020945558 -0.023773458 -0.022771314 -0.019660566][0.21081556 0.23872255 0.24318282 0.22374651 0.1815436 0.12550247 0.070607595 0.026355466 -0.0024587365 -0.019048329 -0.028084356 -0.032826614 -0.034291524 -0.032671344 -0.029370874][0.25065351 0.28132209 0.27683386 0.24148032 0.18362492 0.11820241 0.062756695 0.024022276 0.0020773851 -0.0097322008 -0.016964059 -0.022273222 -0.025006162 -0.024319213 -0.020849649]]...]
INFO - root - 2017-12-10 10:10:14.951554: step 5110, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.771 sec/batch; 70h:04m:58s remains)
INFO - root - 2017-12-10 10:10:22.732398: step 5120, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 71h:45m:17s remains)
INFO - root - 2017-12-10 10:10:30.499495: step 5130, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 72h:19m:27s remains)
INFO - root - 2017-12-10 10:10:38.432218: step 5140, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.804 sec/batch; 73h:06m:17s remains)
INFO - root - 2017-12-10 10:10:46.190649: step 5150, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 70h:05m:43s remains)
INFO - root - 2017-12-10 10:10:53.828304: step 5160, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.779 sec/batch; 70h:51m:47s remains)
INFO - root - 2017-12-10 10:11:01.692013: step 5170, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 73h:46m:27s remains)
INFO - root - 2017-12-10 10:11:09.397227: step 5180, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 71h:44m:39s remains)
INFO - root - 2017-12-10 10:11:17.295037: step 5190, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 70h:53m:02s remains)
INFO - root - 2017-12-10 10:11:25.190783: step 5200, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.799 sec/batch; 72h:37m:24s remains)
2017-12-10 10:11:26.031638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.014821257 -0.016454451 -0.015152364 -0.012686676 -0.011224808 -0.011880483 -0.014685791 -0.018475117 -0.021316353 -0.022797644 -0.022878621 -0.02180138 -0.019888045 -0.017706893 -0.015877834][0.013077902 0.0063824845 0.0057710172 0.009053098 0.0117256 0.010534924 0.0041407943 -0.0056382925 -0.014987776 -0.021668589 -0.0248214 -0.025012927 -0.023162918 -0.020613855 -0.018589139][0.064515777 0.05073379 0.047212932 0.052197278 0.058211941 0.058201216 0.048043698 0.029428178 0.0084784729 -0.0092110885 -0.020691074 -0.025870148 -0.026105737 -0.023821358 -0.021423943][0.13443604 0.11278938 0.10593907 0.11406194 0.12685654 0.13189176 0.12013589 0.091854282 0.055433124 0.020765455 -0.0055261119 -0.021103168 -0.027124397 -0.026997751 -0.024714531][0.20733301 0.17920561 0.17004023 0.1830616 0.20637162 0.22150627 0.21259005 0.17692234 0.12436821 0.0692753 0.022912484 -0.0084954649 -0.024366662 -0.028893854 -0.027791498][0.25882772 0.22783977 0.21877296 0.23816016 0.27407777 0.30323243 0.30270144 0.26541841 0.20135191 0.12819679 0.061360948 0.011829419 -0.0166622 -0.028064249 -0.029512506][0.26985279 0.24116954 0.23527157 0.26132703 0.30854768 0.351482 0.36229345 0.32937631 0.26161593 0.17796271 0.096347749 0.03191388 -0.0081096273 -0.026389604 -0.030705025][0.23913948 0.21790755 0.2182892 0.2492236 0.30179378 0.35206756 0.37178621 0.34688076 0.28381464 0.20016952 0.11404117 0.042820223 -0.0037751086 -0.02666234 -0.033131409][0.18210974 0.17318557 0.18264019 0.21588695 0.26602724 0.31395331 0.33611947 0.3194499 0.26734442 0.19341421 0.11320485 0.04383992 -0.0036809389 -0.028350202 -0.036071349][0.12332983 0.12937157 0.14914936 0.18258351 0.22475636 0.263293 0.28247011 0.27278692 0.23459165 0.17604332 0.10803951 0.045260478 -0.000995926 -0.027198507 -0.036912173][0.083378166 0.10349469 0.13153894 0.16282926 0.19491459 0.22246675 0.23787336 0.23581095 0.21335936 0.17256509 0.11864167 0.062584132 0.015585999 -0.015694927 -0.031128449][0.073010966 0.10241269 0.13348708 0.15986826 0.18257034 0.20231907 0.21765986 0.22639166 0.22144677 0.19848888 0.15700661 0.10453977 0.052557666 0.010812466 -0.015672985][0.08851102 0.12017591 0.14768845 0.16625571 0.181198 0.19807929 0.2189658 0.24268314 0.2574535 0.25246492 0.22150488 0.16949661 0.10899872 0.052607443 0.010499901][0.11429009 0.14197254 0.16042893 0.16900998 0.17728519 0.19447263 0.22405462 0.26379555 0.29783618 0.3097651 0.28842685 0.23676185 0.16866267 0.098714873 0.041270588][0.13517042 0.15507469 0.1620684 0.16019137 0.1618899 0.17865027 0.21434326 0.26482755 0.31194627 0.33626106 0.32367003 0.27530086 0.20524092 0.12872715 0.062368792]]...]
INFO - root - 2017-12-10 10:11:33.910731: step 5210, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.809 sec/batch; 73h:34m:00s remains)
INFO - root - 2017-12-10 10:11:41.748714: step 5220, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 72h:37m:18s remains)
INFO - root - 2017-12-10 10:11:49.554046: step 5230, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.758 sec/batch; 68h:55m:34s remains)
INFO - root - 2017-12-10 10:11:57.151701: step 5240, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.782 sec/batch; 71h:07m:43s remains)
INFO - root - 2017-12-10 10:12:04.933899: step 5250, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 71h:47m:42s remains)
INFO - root - 2017-12-10 10:12:12.741340: step 5260, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.765 sec/batch; 69h:32m:05s remains)
INFO - root - 2017-12-10 10:12:20.408291: step 5270, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.754 sec/batch; 68h:31m:16s remains)
INFO - root - 2017-12-10 10:12:28.242098: step 5280, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 69h:09m:14s remains)
INFO - root - 2017-12-10 10:12:36.195225: step 5290, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 70h:02m:06s remains)
INFO - root - 2017-12-10 10:12:44.035445: step 5300, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 70h:20m:43s remains)
2017-12-10 10:12:44.868229: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033942062 0.032862172 0.034103733 0.040702213 0.052044749 0.065182313 0.076723568 0.083792932 0.084287263 0.078762189 0.069798253 0.060504634 0.050905075 0.041089684 0.033442862][0.069183521 0.066190705 0.065720655 0.073261566 0.088548079 0.10778081 0.12576923 0.13746881 0.13910784 0.13141315 0.11822201 0.10426166 0.089637607 0.073992193 0.060288448][0.11361247 0.10694212 0.10219851 0.10821678 0.12567435 0.1502187 0.1747068 0.19133227 0.19412959 0.18374106 0.16550982 0.14654386 0.12759843 0.1076474 0.0893369][0.16173804 0.149878 0.13833827 0.1400983 0.15757267 0.18620662 0.21672791 0.23817906 0.24186578 0.22796142 0.20339836 0.17822613 0.15504159 0.13260481 0.11214975][0.20378645 0.18607605 0.16669323 0.16285676 0.17913046 0.21090639 0.24650663 0.27188441 0.27569145 0.25763112 0.22614762 0.19405673 0.1666013 0.14301521 0.1227578][0.23001581 0.20663624 0.17964715 0.16968855 0.18363325 0.2168619 0.25543329 0.28279513 0.28594515 0.26431388 0.22749294 0.19017889 0.16005634 0.1373684 0.11987798][0.2364613 0.20925629 0.17663768 0.16138501 0.17247428 0.20515627 0.24384797 0.27061391 0.27210736 0.24767946 0.20786858 0.16812706 0.137765 0.11803573 0.10546006][0.22517338 0.197566 0.16326569 0.14511627 0.15351997 0.18371272 0.21962099 0.24345453 0.24271268 0.21679413 0.17673826 0.13777477 0.10930973 0.09296152 0.084985107][0.20391661 0.17980774 0.14822794 0.13042188 0.13693491 0.16355126 0.19507614 0.21511982 0.21253243 0.18699931 0.14932935 0.11350366 0.087771669 0.073563389 0.067816153][0.17980655 0.16065776 0.13409877 0.11830344 0.12301109 0.14500794 0.17115216 0.1871587 0.18352251 0.160378 0.12769209 0.097421482 0.075897656 0.063803174 0.059009027][0.16082722 0.14646849 0.12483709 0.11079594 0.1126379 0.1285647 0.14837487 0.16019532 0.15620096 0.13669379 0.11040729 0.087046862 0.070898995 0.061779648 0.058217585][0.15084176 0.14007005 0.12235817 0.10964835 0.10838793 0.11795639 0.13116311 0.13849829 0.13384682 0.11757338 0.097082041 0.080082707 0.0692744 0.063841872 0.062537692][0.14742066 0.13827102 0.12292238 0.11114436 0.1072761 0.1112064 0.11831708 0.12130703 0.11593209 0.10274846 0.087837376 0.077110685 0.07198409 0.071123987 0.072994649][0.14379267 0.13445954 0.12006254 0.10917547 0.10396988 0.10400452 0.10659917 0.10667138 0.10167713 0.092401125 0.083749652 0.080038995 0.081498876 0.085939527 0.091074884][0.13296433 0.12267987 0.10894745 0.099607147 0.095111333 0.09423653 0.095346607 0.095379941 0.093024828 0.088983789 0.087007135 0.090115838 0.09759859 0.106193 0.11260485]]...]
INFO - root - 2017-12-10 10:12:52.731429: step 5310, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.793 sec/batch; 72h:05m:31s remains)
INFO - root - 2017-12-10 10:13:00.442915: step 5320, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.784 sec/batch; 71h:12m:53s remains)
INFO - root - 2017-12-10 10:13:08.319059: step 5330, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.787 sec/batch; 71h:33m:00s remains)
INFO - root - 2017-12-10 10:13:16.200953: step 5340, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 71h:27m:57s remains)
INFO - root - 2017-12-10 10:13:24.005062: step 5350, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 70h:52m:37s remains)
INFO - root - 2017-12-10 10:13:31.689722: step 5360, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.779 sec/batch; 70h:46m:47s remains)
INFO - root - 2017-12-10 10:13:39.547115: step 5370, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 69h:03m:38s remains)
INFO - root - 2017-12-10 10:13:47.369502: step 5380, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.794 sec/batch; 72h:08m:28s remains)
INFO - root - 2017-12-10 10:13:55.190228: step 5390, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.782 sec/batch; 71h:04m:49s remains)
INFO - root - 2017-12-10 10:14:02.816126: step 5400, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 69h:25m:21s remains)
2017-12-10 10:14:03.703146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.014768729 -0.018210417 -0.019022426 -0.016470565 -0.010898858 -0.0042258981 0.0021443358 0.0062775738 0.0065879584 0.0022278149 -0.0054785674 -0.013797803 -0.020135151 -0.022723543 -0.020836482][0.014308798 0.013071 0.017044699 0.02695174 0.041068025 0.056128658 0.069590226 0.078293189 0.0790491 0.07083378 0.055557992 0.037215918 0.020380713 0.0089510642 0.0051130564][0.0488642 0.053922098 0.067607358 0.090346269 0.11868248 0.14815253 0.17456272 0.19199355 0.19428238 0.17997912 0.15197141 0.1172183 0.083474629 0.057954825 0.044737123][0.075448878 0.090124972 0.11777781 0.15804392 0.20557316 0.25547689 0.30179548 0.33435428 0.34193143 0.3208465 0.27602175 0.21857716 0.16139656 0.11651224 0.091302119][0.094049476 0.11753452 0.15754052 0.21345514 0.27893475 0.34999108 0.4192231 0.47113264 0.48772296 0.46220267 0.401013 0.31963843 0.2370386 0.17107628 0.13330814][0.10903891 0.13643931 0.1814017 0.24444367 0.31974337 0.40509748 0.49277982 0.56245053 0.58908409 0.56272238 0.49046573 0.39112353 0.28833064 0.20532118 0.15752713][0.11862916 0.14395191 0.18391757 0.24185459 0.31516048 0.40413994 0.50178373 0.58385831 0.61986953 0.5969075 0.52146012 0.41442397 0.30258614 0.21247463 0.16135739][0.13617933 0.15546434 0.18163998 0.22184522 0.27885896 0.35670224 0.45053488 0.53470391 0.5761227 0.5592559 0.48919573 0.38681749 0.27965248 0.19448704 0.14799654][0.17763232 0.19050218 0.19723496 0.20874958 0.23465733 0.28414315 0.35661244 0.4290939 0.46917194 0.45970419 0.4032836 0.31831941 0.22964042 0.16116127 0.12701029][0.25477687 0.26493251 0.25228322 0.23036872 0.21493083 0.22083777 0.25387168 0.29807591 0.32616991 0.32165241 0.28339657 0.22508329 0.16580363 0.12385646 0.10827897][0.35898459 0.37349904 0.34827286 0.297302 0.23948886 0.19473851 0.17693473 0.18077421 0.18718031 0.18209369 0.16075322 0.13054983 0.10276893 0.088802814 0.09259861][0.4687179 0.49311465 0.46232292 0.38960066 0.29471809 0.20234437 0.13462064 0.097053066 0.078561977 0.068122536 0.058630556 0.051482823 0.050171953 0.059042875 0.078264236][0.56019181 0.59484375 0.56200528 0.47466275 0.35281503 0.22512706 0.12040681 0.052041385 0.013985757 -0.0032584534 -0.0077383122 -0.0020081559 0.011888009 0.033283554 0.060521733][0.60120028 0.6429776 0.60968083 0.51539093 0.38059935 0.23650044 0.11518689 0.033543747 -0.013212807 -0.035118442 -0.040425356 -0.032737542 -0.016067872 0.0072836536 0.035629563][0.5744912 0.61773783 0.58633852 0.49583107 0.36654234 0.22847475 0.11257727 0.034816295 -0.010592751 -0.034541324 -0.044744156 -0.043940563 -0.035014208 -0.018990465 0.0039002383]]...]
INFO - root - 2017-12-10 10:14:11.490760: step 5410, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 69h:06m:35s remains)
INFO - root - 2017-12-10 10:14:19.258475: step 5420, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.790 sec/batch; 71h:45m:16s remains)
INFO - root - 2017-12-10 10:14:27.145444: step 5430, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 72h:09m:18s remains)
INFO - root - 2017-12-10 10:14:35.061979: step 5440, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 71h:07m:29s remains)
INFO - root - 2017-12-10 10:14:42.840080: step 5450, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 72h:11m:55s remains)
INFO - root - 2017-12-10 10:14:50.784881: step 5460, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 71h:25m:14s remains)
INFO - root - 2017-12-10 10:14:58.660615: step 5470, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 71h:49m:14s remains)
INFO - root - 2017-12-10 10:15:06.362652: step 5480, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.770 sec/batch; 69h:57m:05s remains)
INFO - root - 2017-12-10 10:15:14.100912: step 5490, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.755 sec/batch; 68h:33m:14s remains)
INFO - root - 2017-12-10 10:15:21.894079: step 5500, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 72h:29m:34s remains)
2017-12-10 10:15:22.860357: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035036825 0.067465939 0.092635 0.10328208 0.096946456 0.078497015 0.054712877 0.031180894 0.012581492 -0.00050541596 -0.0086945649 -0.014793728 -0.020078618 -0.023872757 -0.025927512][0.067853987 0.11509352 0.15624245 0.1801388 0.18089369 0.16371998 0.13610546 0.10404997 0.074135922 0.050329618 0.033734377 0.01942114 0.0046224217 -0.0081794476 -0.016570127][0.10362563 0.17186019 0.23820427 0.28583997 0.30311558 0.29537979 0.26961488 0.22984068 0.18426774 0.14216428 0.10827454 0.076420136 0.043285452 0.014905636 -0.0038943712][0.13976689 0.23183702 0.32877168 0.40679243 0.44754666 0.45758778 0.442887 0.40244862 0.3437103 0.28070161 0.22317402 0.16438627 0.10215539 0.048057932 0.011535187][0.16907208 0.28421381 0.41237542 0.52251989 0.590006 0.62454408 0.63206762 0.603558 0.541577 0.46310249 0.38230076 0.29178759 0.19188607 0.10241023 0.039890412][0.18640959 0.31973332 0.47420278 0.61264455 0.70558351 0.76689333 0.80284619 0.794084 0.73551261 0.64592826 0.54428655 0.42310116 0.28594434 0.16141759 0.073158972][0.19005458 0.33362076 0.50679612 0.66911906 0.78768748 0.87845248 0.94483733 0.95602012 0.89933437 0.79624295 0.67173147 0.52103412 0.35238022 0.20116346 0.094938621][0.17987898 0.32376641 0.50375104 0.68018204 0.81851912 0.93371987 1.0250154 1.0522577 0.9970904 0.88306695 0.74013263 0.56750333 0.37759081 0.21079247 0.095787905][0.16094778 0.29344028 0.46318406 0.6349566 0.77513677 0.89682865 0.99612957 1.0289021 0.97523016 0.85946929 0.71371967 0.53882861 0.34905958 0.18591072 0.076263309][0.13783047 0.25015336 0.39537236 0.54459059 0.66747487 0.77490807 0.86222279 0.888102 0.83512712 0.72698683 0.59405786 0.43747616 0.27020577 0.13060123 0.040852603][0.10711777 0.19448203 0.30675784 0.42241052 0.51572764 0.59484488 0.65534121 0.66422868 0.60988575 0.51398695 0.40294355 0.27823696 0.15030733 0.050355043 -0.0066422122][0.065512195 0.12685145 0.20500928 0.28510574 0.34666651 0.39496076 0.42627871 0.418089 0.36513424 0.2854805 0.20048332 0.11198272 0.028330667 -0.027817369 -0.048780169][0.026474657 0.066008 0.11492579 0.16289015 0.19505763 0.21486281 0.22079228 0.20067313 0.15436921 0.095013104 0.037674233 -0.016098939 -0.059926484 -0.078585416 -0.06969022][0.00401366 0.029442323 0.057264913 0.079262041 0.08616858 0.081703663 0.067876928 0.041092731 0.0046001733 -0.033562165 -0.064681269 -0.088489443 -0.10009525 -0.089956336 -0.058889717][-0.0063018384 0.012251941 0.028455472 0.034791954 0.026164655 0.0068601663 -0.018116223 -0.046231173 -0.072502166 -0.091966055 -0.10075159 -0.099712871 -0.085613564 -0.052955702 -0.0040260334]]...]
INFO - root - 2017-12-10 10:15:30.628439: step 5510, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.767 sec/batch; 69h:37m:46s remains)
INFO - root - 2017-12-10 10:15:38.583234: step 5520, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 70h:42m:15s remains)
INFO - root - 2017-12-10 10:15:46.301851: step 5530, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.753 sec/batch; 68h:25m:13s remains)
INFO - root - 2017-12-10 10:15:53.955218: step 5540, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 70h:18m:47s remains)
INFO - root - 2017-12-10 10:16:01.762851: step 5550, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 70h:38m:30s remains)
INFO - root - 2017-12-10 10:16:09.430698: step 5560, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.767 sec/batch; 69h:39m:08s remains)
INFO - root - 2017-12-10 10:16:17.186960: step 5570, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 69h:33m:13s remains)
INFO - root - 2017-12-10 10:16:24.975384: step 5580, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 71h:41m:15s remains)
INFO - root - 2017-12-10 10:16:32.706630: step 5590, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.770 sec/batch; 69h:56m:56s remains)
INFO - root - 2017-12-10 10:16:40.583453: step 5600, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.771 sec/batch; 70h:01m:50s remains)
2017-12-10 10:16:41.464170: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022509564 0.036185008 0.0470876 0.05462718 0.060466189 0.062273849 0.058734249 0.049544267 0.035868011 0.019701771 0.00463101 -0.0050852462 -0.0092607541 -0.010973095 -0.014095324][0.0626679 0.085368708 0.10277154 0.11613228 0.12828672 0.13595337 0.13634257 0.1279977 0.10959858 0.082896687 0.054069895 0.032855906 0.022057706 0.01696457 0.010468068][0.09832938 0.12902372 0.15325786 0.1751937 0.19894822 0.22002676 0.2329445 0.23194543 0.21091954 0.17071721 0.12189811 0.082308948 0.059797976 0.048987839 0.038287252][0.12493666 0.16126594 0.19100805 0.22241573 0.26170942 0.3030279 0.33641243 0.34948123 0.32956919 0.2748667 0.20060892 0.13531888 0.095202558 0.075859107 0.060484741][0.15066181 0.19013125 0.22183849 0.25921711 0.31207088 0.37379169 0.42959264 0.46050495 0.44682863 0.38111463 0.28208587 0.18958943 0.12982309 0.10058169 0.079633847][0.18498771 0.22680771 0.25701776 0.29509729 0.35525638 0.43168661 0.507049 0.55782461 0.55657351 0.48749533 0.37114918 0.25728732 0.18096146 0.14142419 0.11200093][0.21863969 0.2625708 0.2901853 0.32493287 0.38505155 0.46714827 0.5548417 0.62207943 0.63513893 0.57035929 0.44775912 0.32304451 0.23683229 0.18946646 0.15096195][0.23717438 0.28334746 0.30938435 0.3401787 0.39572364 0.47563964 0.56649512 0.642551 0.667298 0.61105615 0.49180713 0.36613581 0.27644044 0.22397496 0.17809109][0.23268653 0.27982295 0.30546314 0.33252409 0.38040456 0.45124534 0.53533065 0.6099931 0.639584 0.59227788 0.48303372 0.36552736 0.280011 0.22794469 0.18040051][0.20205574 0.24661836 0.27069435 0.29272676 0.32992652 0.38638851 0.45636696 0.5218147 0.55107474 0.51362157 0.42107958 0.32047126 0.24676411 0.20070396 0.15706751][0.14839073 0.18605629 0.20680717 0.22345585 0.24964187 0.29050073 0.34400845 0.39672488 0.42237252 0.394881 0.32275337 0.24400494 0.18610325 0.14918119 0.11357289][0.082166508 0.108561 0.12338808 0.13406867 0.15008338 0.17636128 0.21297069 0.25031602 0.26852295 0.24865562 0.1967545 0.14050986 0.099566706 0.074095748 0.05067119][0.021064678 0.034744497 0.042357348 0.0473911 0.055787858 0.071450427 0.094642229 0.11853717 0.12947102 0.11535114 0.080302685 0.04263103 0.015593057 -2.5447847e-05 -0.011774507][-0.023584329 -0.020533575 -0.018762922 -0.017629907 -0.013740113 -0.0047213519 0.0091591934 0.023014016 0.028323557 0.018485716 -0.0040240204 -0.028144311 -0.045158289 -0.053610727 -0.056883313][-0.050958641 -0.054752577 -0.056770205 -0.058161091 -0.057239834 -0.05283498 -0.045612756 -0.038683195 -0.036736634 -0.042854 -0.055892374 -0.069931947 -0.079392888 -0.082559362 -0.080435961]]...]
INFO - root - 2017-12-10 10:16:49.300333: step 5610, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.775 sec/batch; 70h:21m:08s remains)
INFO - root - 2017-12-10 10:16:57.033748: step 5620, loss = 0.72, batch loss = 0.66 (11.5 examples/sec; 0.694 sec/batch; 62h:58m:37s remains)
INFO - root - 2017-12-10 10:17:04.658656: step 5630, loss = 0.72, batch loss = 0.66 (11.4 examples/sec; 0.700 sec/batch; 63h:32m:54s remains)
INFO - root - 2017-12-10 10:17:12.607010: step 5640, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 71h:30m:21s remains)
INFO - root - 2017-12-10 10:17:20.537395: step 5650, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 69h:36m:15s remains)
INFO - root - 2017-12-10 10:17:28.461443: step 5660, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 72h:31m:55s remains)
INFO - root - 2017-12-10 10:17:36.284720: step 5670, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 71h:14m:20s remains)
INFO - root - 2017-12-10 10:17:44.064315: step 5680, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.756 sec/batch; 68h:39m:40s remains)
INFO - root - 2017-12-10 10:17:51.860322: step 5690, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 70h:28m:18s remains)
INFO - root - 2017-12-10 10:17:59.665965: step 5700, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 69h:46m:04s remains)
2017-12-10 10:18:00.478976: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10573588 0.098829031 0.10520641 0.12436284 0.15100281 0.17963019 0.20188533 0.21883421 0.23267394 0.24365142 0.24941954 0.24461153 0.22814885 0.1997183 0.16602592][0.11363249 0.10207686 0.10311664 0.11788806 0.14314197 0.17224531 0.19406846 0.20754331 0.21590562 0.22161262 0.22367406 0.21609707 0.19855003 0.17110507 0.14065696][0.11395987 0.095927194 0.087672517 0.093000405 0.11163513 0.13710682 0.15704508 0.16729197 0.17019351 0.1700006 0.16728929 0.15796678 0.14211161 0.12066539 0.0991409][0.11548323 0.090828136 0.072919115 0.068315431 0.079384655 0.10013698 0.11768844 0.12472983 0.12218311 0.1157492 0.10764702 0.096393712 0.083145082 0.069256209 0.05765678][0.11638326 0.087596849 0.063407704 0.052644778 0.059220456 0.0776204 0.094372846 0.099714637 0.092976019 0.080756307 0.0670154 0.052440066 0.039675735 0.030396173 0.025146291][0.1146813 0.085176975 0.059106693 0.047052134 0.053838357 0.0740607 0.093436621 0.1002496 0.091844752 0.075403087 0.05596254 0.036107145 0.020123258 0.010261342 0.0062304079][0.11190727 0.084240377 0.059474666 0.050059523 0.061124127 0.086840577 0.11212803 0.12323382 0.11555897 0.095983282 0.070490092 0.043425586 0.020686181 0.005815323 -0.0012090378][0.10873334 0.083855055 0.062646493 0.058209483 0.075331211 0.10751654 0.13875751 0.15409902 0.14760977 0.1252306 0.093785614 0.059406284 0.029204952 0.0079690972 -0.0034302445][0.10548837 0.083815634 0.066321477 0.066529967 0.088390537 0.12474313 0.15897346 0.17602369 0.16961268 0.14496227 0.10951526 0.070529409 0.035929803 0.010726483 -0.003374798][0.10508416 0.0852981 0.06962727 0.071559668 0.094440468 0.13089839 0.16434015 0.1803975 0.17314896 0.14757021 0.11124904 0.071835347 0.037256617 0.012115414 -0.0018620187][0.10741708 0.088146582 0.072089419 0.072566211 0.092668235 0.12522587 0.1546104 0.16747865 0.15860263 0.13298026 0.098248318 0.062019855 0.031385373 0.0098467451 -0.0014241219][0.10969622 0.090197191 0.072677583 0.06995713 0.084893085 0.11109003 0.13452 0.14283192 0.13166773 0.10641825 0.074823864 0.043992285 0.019527597 0.0034487268 -0.0039592823][0.10909759 0.089445807 0.070332363 0.0636843 0.0721054 0.090574816 0.10697583 0.11026809 0.096905895 0.072990574 0.046206549 0.02255331 0.0053873579 -0.0049793627 -0.0087344479][0.10782607 0.088136867 0.067233279 0.056299817 0.057808504 0.068395205 0.07810808 0.077094883 0.062499218 0.041055154 0.020141674 0.0040733186 -0.0063181669 -0.011906503 -0.013020249][0.10987067 0.08948493 0.0662712 0.050729122 0.0457109 0.049545772 0.054131977 0.050478511 0.035985056 0.017543832 0.0020394011 -0.0078820977 -0.013362078 -0.015875107 -0.015544263]]...]
INFO - root - 2017-12-10 10:18:07.954368: step 5710, loss = 0.70, batch loss = 0.64 (13.2 examples/sec; 0.608 sec/batch; 55h:10m:28s remains)
INFO - root - 2017-12-10 10:18:15.851990: step 5720, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.793 sec/batch; 71h:59m:16s remains)
INFO - root - 2017-12-10 10:18:23.607441: step 5730, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 71h:15m:00s remains)
INFO - root - 2017-12-10 10:18:31.402562: step 5740, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 70h:29m:23s remains)
INFO - root - 2017-12-10 10:18:39.145118: step 5750, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.758 sec/batch; 68h:50m:14s remains)
INFO - root - 2017-12-10 10:18:46.889035: step 5760, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 71h:45m:46s remains)
INFO - root - 2017-12-10 10:18:54.662868: step 5770, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.790 sec/batch; 71h:40m:33s remains)
INFO - root - 2017-12-10 10:19:02.451809: step 5780, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.793 sec/batch; 71h:57m:16s remains)
INFO - root - 2017-12-10 10:19:10.347646: step 5790, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 70h:20m:13s remains)
INFO - root - 2017-12-10 10:19:17.990247: step 5800, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 71h:55m:14s remains)
2017-12-10 10:19:18.781851: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26277664 0.27136472 0.27416584 0.27680421 0.27082926 0.25730196 0.24026835 0.23193265 0.23435636 0.24181576 0.25249854 0.26212716 0.26928282 0.27222034 0.27823505][0.2968235 0.29486665 0.28675142 0.28233296 0.27306637 0.26014036 0.24471733 0.23665072 0.23691747 0.2417627 0.25319061 0.26400843 0.26809406 0.26423347 0.26222104][0.34037483 0.3359215 0.32505733 0.31967354 0.31080934 0.29749313 0.27823684 0.26193777 0.25318661 0.25177512 0.2629171 0.27619934 0.28085902 0.27518871 0.26735181][0.39770845 0.40725979 0.41097224 0.42047292 0.42626521 0.42029446 0.39532253 0.35998061 0.32749492 0.30558124 0.30206931 0.30643648 0.30660027 0.29950479 0.28815615][0.44057977 0.47807613 0.5127328 0.552304 0.58865631 0.60325468 0.57972151 0.52458805 0.46258414 0.41051623 0.37753713 0.35712221 0.34233692 0.33046457 0.31678584][0.4345215 0.50481522 0.57871628 0.65774417 0.73586196 0.78234965 0.77080047 0.70388234 0.61591589 0.53151625 0.45993656 0.40335274 0.3655754 0.34663835 0.33357996][0.37345919 0.46821755 0.57372063 0.68587905 0.80093986 0.87887079 0.88628763 0.82061738 0.7202028 0.61305314 0.50742865 0.41580698 0.35476288 0.32979864 0.32154775][0.26488122 0.3665401 0.48317176 0.60756421 0.73892397 0.83466357 0.85982621 0.80771762 0.71436793 0.60483265 0.48473862 0.37444961 0.30089033 0.27511996 0.27465895][0.13980447 0.22912595 0.33400306 0.44568586 0.56577128 0.65746146 0.69042164 0.65642303 0.58421046 0.49153402 0.38046011 0.27436593 0.20450562 0.18470182 0.19289045][0.027818941 0.090884648 0.16737495 0.24820815 0.33631989 0.40590927 0.43517703 0.41664943 0.36985737 0.30460042 0.22012129 0.13764228 0.085616276 0.076640435 0.091546096][-0.051812846 -0.018642955 0.024490789 0.069571674 0.11983103 0.16076943 0.17941764 0.17030813 0.1447728 0.10686768 0.054885831 0.0047167209 -0.023138434 -0.02037671 -0.0019672834][-0.088763177 -0.078605562 -0.061971821 -0.045506854 -0.026529374 -0.010861381 -0.004606978 -0.010724957 -0.023253817 -0.040913448 -0.065209553 -0.086651005 -0.093811311 -0.083085075 -0.06487596][-0.093431339 -0.0963982 -0.095074348 -0.094761148 -0.0936852 -0.092979692 -0.09476807 -0.10015423 -0.10608935 -0.11202706 -0.11900871 -0.12273673 -0.1185606 -0.105989 -0.09136495][-0.082815945 -0.090635031 -0.095048778 -0.10016081 -0.1050889 -0.10947283 -0.11374159 -0.11820266 -0.12131049 -0.12237711 -0.12193292 -0.11879345 -0.11195701 -0.10198428 -0.092174217][-0.068667054 -0.075919025 -0.080075525 -0.08443033 -0.088680863 -0.092406593 -0.095687047 -0.098776542 -0.10088564 -0.10115638 -0.0996594 -0.09644413 -0.091729015 -0.085916862 -0.080535732]]...]
INFO - root - 2017-12-10 10:19:26.611462: step 5810, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 71h:03m:47s remains)
INFO - root - 2017-12-10 10:19:34.350289: step 5820, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.768 sec/batch; 69h:40m:06s remains)
INFO - root - 2017-12-10 10:19:42.134653: step 5830, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.759 sec/batch; 68h:54m:33s remains)
INFO - root - 2017-12-10 10:19:49.961917: step 5840, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 70h:19m:54s remains)
INFO - root - 2017-12-10 10:19:57.740089: step 5850, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 70h:49m:26s remains)
INFO - root - 2017-12-10 10:20:05.600839: step 5860, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.812 sec/batch; 73h:38m:33s remains)
INFO - root - 2017-12-10 10:20:13.435926: step 5870, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 70h:44m:04s remains)
INFO - root - 2017-12-10 10:20:21.059703: step 5880, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 70h:09m:19s remains)
INFO - root - 2017-12-10 10:20:28.746866: step 5890, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.769 sec/batch; 69h:44m:42s remains)
INFO - root - 2017-12-10 10:20:36.632851: step 5900, loss = 0.73, batch loss = 0.67 (10.7 examples/sec; 0.750 sec/batch; 68h:04m:52s remains)
2017-12-10 10:20:37.448716: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11731274 0.12341636 0.12269443 0.11738793 0.10891152 0.10114121 0.0968973 0.0952477 0.092688769 0.089644536 0.0874768 0.086504236 0.090107322 0.10096544 0.11187302][0.14826298 0.15843217 0.1597123 0.15612546 0.14914408 0.14233296 0.13850085 0.13642089 0.13184251 0.12565991 0.11989118 0.11600109 0.1199049 0.13526511 0.15128006][0.17022307 0.18372297 0.18646125 0.18484455 0.18097125 0.17762864 0.17663142 0.17546767 0.16908194 0.15900257 0.14805879 0.13939331 0.1415596 0.15983035 0.18038765][0.18500455 0.19903341 0.20049585 0.198461 0.19666609 0.19749781 0.2012368 0.20281875 0.19605429 0.18276173 0.16688114 0.1533155 0.15281844 0.17235132 0.19666727][0.19167653 0.20334937 0.20083088 0.19556399 0.19369774 0.19789 0.20690894 0.21269356 0.20743006 0.19264372 0.17331818 0.15606269 0.15306064 0.17240888 0.19938345][0.19316941 0.20141731 0.19399916 0.18429449 0.18123133 0.18812487 0.20221581 0.21241292 0.20918876 0.19361241 0.17130266 0.15051365 0.14423469 0.16118491 0.18885508][0.19491501 0.19988234 0.18891154 0.17665227 0.17424031 0.18477593 0.20354341 0.21684265 0.21387364 0.19562833 0.1684355 0.14251316 0.13149053 0.14423443 0.17071702][0.1917804 0.19466518 0.1833847 0.17278917 0.17411754 0.18947187 0.21152854 0.22547792 0.22009955 0.19684099 0.16332416 0.13160428 0.11600626 0.1246914 0.14949284][0.18044239 0.18291093 0.17454755 0.16916567 0.17622824 0.19603862 0.21917774 0.23148502 0.22225794 0.19381788 0.15527619 0.11994755 0.10230783 0.10896393 0.13255332][0.16227992 0.1657389 0.16177167 0.16246381 0.17466727 0.19659467 0.21783216 0.22639921 0.21294439 0.18086131 0.14025569 0.10472744 0.087910861 0.094381675 0.11702538][0.13688517 0.14093812 0.14039157 0.14512688 0.1595684 0.18046112 0.19761007 0.20171759 0.18526912 0.15256739 0.11389382 0.081711538 0.067787476 0.074787445 0.095899716][0.10382566 0.1073053 0.10835353 0.11431254 0.12767631 0.14467542 0.15632313 0.15616374 0.13880995 0.10892396 0.075776212 0.049608093 0.039767388 0.047355171 0.066135526][0.063913532 0.0659109 0.06672994 0.071128972 0.080251664 0.091036074 0.096749686 0.093728624 0.0784991 0.055229038 0.031170031 0.013514027 0.0087669631 0.016866453 0.032679636][0.027440926 0.027530625 0.02690669 0.028098192 0.031726666 0.03589737 0.036750764 0.032788996 0.021913465 0.00707964 -0.0069266758 -0.015736161 -0.015684068 -0.0075551518 0.0048264582][0.0015699192 0.00015226936 -0.0019637686 -0.003890146 -0.0049638464 -0.0057769911 -0.0078257686 -0.011425005 -0.017891813 -0.025532814 -0.03171109 -0.034216341 -0.03152553 -0.024679534 -0.016107729]]...]
INFO - root - 2017-12-10 10:20:45.334261: step 5910, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 69h:56m:57s remains)
INFO - root - 2017-12-10 10:20:53.120824: step 5920, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.788 sec/batch; 71h:27m:01s remains)
INFO - root - 2017-12-10 10:21:00.938293: step 5930, loss = 0.72, batch loss = 0.66 (9.8 examples/sec; 0.812 sec/batch; 73h:41m:59s remains)
INFO - root - 2017-12-10 10:21:08.812505: step 5940, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 72h:48m:41s remains)
INFO - root - 2017-12-10 10:21:16.610721: step 5950, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 69h:49m:55s remains)
INFO - root - 2017-12-10 10:21:24.342617: step 5960, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 71h:47m:33s remains)
INFO - root - 2017-12-10 10:21:32.208955: step 5970, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 70h:19m:27s remains)
INFO - root - 2017-12-10 10:21:39.858341: step 5980, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 69h:26m:08s remains)
INFO - root - 2017-12-10 10:21:47.719960: step 5990, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 72h:24m:21s remains)
INFO - root - 2017-12-10 10:21:55.584447: step 6000, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 71h:03m:57s remains)
2017-12-10 10:21:56.436682: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24687888 0.25673795 0.23896745 0.20643176 0.17157508 0.14347886 0.12522145 0.11960702 0.12270769 0.13083445 0.14937297 0.18318143 0.23316635 0.28725582 0.32614586][0.33882859 0.34725845 0.31882256 0.27281067 0.22585621 0.18924651 0.16665214 0.15935838 0.16111515 0.16793643 0.18815391 0.22740945 0.28364691 0.34071717 0.37600851][0.39882156 0.40948963 0.37959439 0.33320874 0.28770423 0.25246027 0.2295911 0.21855859 0.21290873 0.21187837 0.22837302 0.26893497 0.32660651 0.38123241 0.40885219][0.41612414 0.42951795 0.40423933 0.36738613 0.33428681 0.30981612 0.29305449 0.28108761 0.26846474 0.25885627 0.2696366 0.3068653 0.35799477 0.40104395 0.41502795][0.39749393 0.41224757 0.39483088 0.37328368 0.35880697 0.35135934 0.34756288 0.3417663 0.328897 0.31559876 0.32132003 0.34915149 0.38219744 0.40189144 0.39577273][0.36205849 0.37799031 0.36790287 0.359753 0.36191571 0.37189198 0.38528293 0.39386377 0.39154309 0.38408706 0.38777182 0.39982924 0.40207052 0.38608462 0.35311902][0.32782838 0.34533736 0.3382645 0.33459064 0.3441208 0.36575815 0.39582038 0.42416781 0.44236752 0.45093802 0.45755625 0.452027 0.41758063 0.35981074 0.29632822][0.30933693 0.32514393 0.31255302 0.30041113 0.30330861 0.32570547 0.36699387 0.41605902 0.4610624 0.49430221 0.51238436 0.4969888 0.43362308 0.34097847 0.25033605][0.31086531 0.31797305 0.29047626 0.25820166 0.2414857 0.25338036 0.29871708 0.36636427 0.4405489 0.50447017 0.54280996 0.5294081 0.45249143 0.34001547 0.2317549][0.32770064 0.32074007 0.27257609 0.21528329 0.17455448 0.17130558 0.21520045 0.29586527 0.39405483 0.48504129 0.54397207 0.54000211 0.46467632 0.35013491 0.23756319][0.366738 0.34399539 0.27361581 0.19241624 0.13083841 0.11459797 0.15489756 0.24132693 0.35146603 0.45524108 0.52343911 0.52640504 0.46012843 0.35641706 0.25179136][0.43611282 0.39763772 0.30679163 0.20800824 0.13276267 0.10733376 0.14212869 0.22577254 0.3333219 0.43248761 0.49532491 0.49860048 0.44274503 0.35634482 0.26749191][0.51429021 0.46293721 0.35940871 0.25498569 0.1765206 0.14645967 0.17368831 0.24637605 0.33831975 0.41819009 0.46369064 0.46106556 0.41453335 0.34805289 0.27964568][0.5718528 0.51515907 0.41117623 0.31433371 0.24326167 0.2140028 0.23391166 0.29120418 0.35957357 0.41086027 0.43109187 0.41702279 0.37610471 0.32722786 0.27862355][0.59019488 0.53556323 0.44117442 0.3595525 0.30184773 0.27780679 0.2927143 0.33450583 0.37728694 0.39721623 0.38935679 0.36010665 0.31969622 0.2825543 0.24950954]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 10:22:04.297889: step 6010, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.788 sec/batch; 71h:29m:42s remains)
INFO - root - 2017-12-10 10:22:12.372356: step 6020, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 72h:27m:03s remains)
INFO - root - 2017-12-10 10:22:20.219747: step 6030, loss = 0.71, batch loss = 0.65 (11.3 examples/sec; 0.706 sec/batch; 64h:01m:24s remains)
INFO - root - 2017-12-10 10:22:28.113428: step 6040, loss = 0.70, batch loss = 0.65 (9.8 examples/sec; 0.814 sec/batch; 73h:48m:15s remains)
INFO - root - 2017-12-10 10:22:35.971442: step 6050, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 73h:02m:03s remains)
INFO - root - 2017-12-10 10:22:43.979043: step 6060, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.832 sec/batch; 75h:28m:25s remains)
INFO - root - 2017-12-10 10:22:51.724786: step 6070, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.792 sec/batch; 71h:51m:07s remains)
INFO - root - 2017-12-10 10:22:59.605133: step 6080, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.824 sec/batch; 74h:41m:10s remains)
INFO - root - 2017-12-10 10:23:07.508480: step 6090, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 72h:53m:52s remains)
INFO - root - 2017-12-10 10:23:15.315406: step 6100, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 70h:57m:22s remains)
2017-12-10 10:23:16.195460: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049450349 0.075244837 0.096814565 0.11389499 0.12403663 0.12588097 0.12021115 0.10934975 0.098964356 0.091635123 0.087306522 0.083898075 0.079419866 0.07798408 0.081953816][0.068928622 0.10340615 0.13287543 0.15870145 0.17875697 0.1899536 0.18902943 0.17690454 0.16366248 0.15399052 0.14663902 0.13903494 0.13044006 0.12725855 0.13044648][0.081850022 0.12190978 0.15727021 0.19178306 0.22353622 0.24678266 0.25252935 0.23930082 0.22160494 0.20756663 0.19519028 0.18252261 0.17033027 0.16633058 0.1690107][0.085548013 0.12752581 0.16614275 0.20783915 0.25072905 0.28629914 0.29903597 0.28452933 0.26111054 0.24125038 0.22361119 0.20647679 0.19116414 0.1864094 0.18862709][0.0868507 0.13066053 0.17238791 0.21954717 0.27055129 0.31484473 0.33163038 0.31433007 0.28516671 0.26111883 0.2415162 0.22329368 0.20685883 0.20146215 0.20271698][0.090014637 0.13638043 0.18124808 0.23145655 0.2854591 0.33208108 0.34852451 0.32765871 0.2944988 0.26891205 0.25071645 0.23482761 0.21967836 0.21439619 0.21508285][0.095569134 0.14468479 0.19285206 0.24399185 0.29587 0.33842355 0.35094786 0.32703793 0.2918011 0.26572523 0.24905723 0.23567738 0.2218353 0.21612774 0.21630681][0.099759392 0.15119436 0.20276277 0.25379211 0.3005923 0.33565909 0.34304318 0.31738412 0.28099385 0.25289026 0.23476857 0.22060966 0.20588352 0.19824134 0.19743757][0.10010473 0.15299234 0.20732166 0.2578482 0.29915941 0.32645318 0.32943094 0.30370429 0.26745349 0.23701826 0.21613035 0.19963208 0.18293566 0.17294341 0.17094634][0.097549222 0.14972211 0.20380937 0.251397 0.28588009 0.30468059 0.30301839 0.27771237 0.24295741 0.21206768 0.19013344 0.17320982 0.15706071 0.14744087 0.14599979][0.0869287 0.13386522 0.18240598 0.22318341 0.24890128 0.25881222 0.25270906 0.22887859 0.19778697 0.16946672 0.14912815 0.13427086 0.12135875 0.11449614 0.11473122][0.06551607 0.1034425 0.14304022 0.17542188 0.19303085 0.19622025 0.18780811 0.16717477 0.1414831 0.11784595 0.10062663 0.088648453 0.079358459 0.075198092 0.076380983][0.035790864 0.062524229 0.091390826 0.11506826 0.12639499 0.12602262 0.11791481 0.10177281 0.082177155 0.063637346 0.049774837 0.040409211 0.033953667 0.031547036 0.032807056][0.0063668652 0.021588467 0.039300594 0.054337192 0.060847994 0.059258536 0.053421505 0.042881455 0.030031774 0.017340215 0.0077260011 0.001619423 -0.0021443788 -0.0031485711 -0.0018152801][-0.014331435 -0.008138272 0.00024322845 0.0076825782 0.010168108 0.0080610067 0.0044523515 -0.0012675239 -0.0082114507 -0.015216828 -0.020230064 -0.022859871 -0.024068655 -0.023761576 -0.022165498]]...]
INFO - root - 2017-12-10 10:23:23.816994: step 6110, loss = 0.71, batch loss = 0.65 (12.9 examples/sec; 0.621 sec/batch; 56h:19m:34s remains)
INFO - root - 2017-12-10 10:23:31.613094: step 6120, loss = 0.72, batch loss = 0.66 (10.8 examples/sec; 0.742 sec/batch; 67h:17m:15s remains)
INFO - root - 2017-12-10 10:23:39.414197: step 6130, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 70h:43m:22s remains)
INFO - root - 2017-12-10 10:23:47.282048: step 6140, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 71h:49m:33s remains)
INFO - root - 2017-12-10 10:23:55.149879: step 6150, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.812 sec/batch; 73h:35m:54s remains)
INFO - root - 2017-12-10 10:24:02.954424: step 6160, loss = 0.72, batch loss = 0.66 (9.6 examples/sec; 0.835 sec/batch; 75h:39m:01s remains)
INFO - root - 2017-12-10 10:24:10.828648: step 6170, loss = 0.73, batch loss = 0.67 (9.9 examples/sec; 0.805 sec/batch; 73h:00m:40s remains)
INFO - root - 2017-12-10 10:24:18.837083: step 6180, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 69h:43m:10s remains)
INFO - root - 2017-12-10 10:24:26.681936: step 6190, loss = 0.72, batch loss = 0.66 (11.4 examples/sec; 0.703 sec/batch; 63h:44m:37s remains)
INFO - root - 2017-12-10 10:24:34.440106: step 6200, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.800 sec/batch; 72h:31m:19s remains)
2017-12-10 10:24:35.259374: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.38165745 0.36097136 0.32629889 0.30266 0.28635406 0.27352881 0.26963893 0.2745243 0.27584523 0.27326116 0.27462047 0.28382796 0.29714456 0.31366378 0.33356851][0.45669013 0.42881605 0.38344428 0.3539159 0.34083876 0.33868289 0.34912297 0.3711037 0.38899887 0.39807844 0.39993972 0.39842349 0.39252496 0.38501611 0.38282931][0.50958443 0.47790968 0.42517245 0.3910163 0.38145363 0.39152449 0.41849282 0.46048617 0.498947 0.52357453 0.526834 0.51085544 0.47774234 0.43615124 0.40210322][0.53404444 0.50793326 0.45774081 0.42540577 0.42014539 0.43790686 0.47321817 0.52515322 0.57569903 0.61134684 0.61606312 0.58779627 0.53078938 0.45948938 0.39663887][0.53057837 0.519601 0.48482651 0.46337429 0.46365464 0.48131964 0.51123387 0.55677241 0.60457379 0.64117515 0.64461172 0.60755873 0.53579384 0.44799057 0.36986917][0.50744879 0.5161463 0.5043053 0.5005219 0.50949383 0.52518815 0.54405969 0.57428223 0.60769671 0.63345987 0.62750542 0.57996261 0.4991143 0.40648848 0.32783133][0.47964039 0.50552231 0.51743221 0.53720832 0.56346804 0.58539057 0.59864861 0.6128118 0.62378925 0.62565595 0.59850013 0.53485614 0.44774172 0.35951278 0.29286438][0.45973897 0.49278334 0.52206534 0.56505632 0.6130209 0.64895213 0.66471046 0.66662526 0.65242726 0.62236941 0.56650823 0.48404843 0.39421472 0.31806678 0.27347991][0.44605482 0.47664529 0.51270682 0.571073 0.63626409 0.68534559 0.704475 0.69503081 0.6565066 0.59607559 0.51576507 0.42200184 0.3386701 0.28277197 0.26647303][0.42063528 0.43931338 0.46937719 0.52869445 0.59848976 0.65264297 0.67269158 0.65552408 0.60219473 0.5251298 0.43499517 0.342629 0.27364627 0.2405912 0.25065714][0.35574636 0.35961863 0.37586734 0.42425162 0.48685515 0.538032 0.5575071 0.53924048 0.48504415 0.40897012 0.32543632 0.24630576 0.19541551 0.18162943 0.20853715][0.25151035 0.24404614 0.24697056 0.27936077 0.32739088 0.36878565 0.38434377 0.36747164 0.32105839 0.258403 0.1936354 0.13636179 0.10509627 0.10537621 0.13845675][0.12920082 0.11600217 0.10979932 0.12562975 0.15497984 0.18140383 0.18966193 0.17496033 0.14112589 0.098941714 0.059222214 0.027322069 0.014598048 0.023461774 0.054053266][0.020055711 0.0062943366 -0.0033792821 0.00042025282 0.0137907 0.026493333 0.02868076 0.017929418 -0.0019861956 -0.023709688 -0.040766116 -0.051717672 -0.051680706 -0.040604 -0.018581528][-0.052323144 -0.063610323 -0.072096981 -0.07404241 -0.070479676 -0.066576287 -0.067062609 -0.072553746 -0.080365367 -0.086787015 -0.089280993 -0.088521458 -0.083440028 -0.074951477 -0.063006669]]...]
INFO - root - 2017-12-10 10:24:43.265137: step 6210, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 70h:08m:00s remains)
INFO - root - 2017-12-10 10:24:51.227300: step 6220, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.813 sec/batch; 73h:40m:09s remains)
INFO - root - 2017-12-10 10:24:59.121249: step 6230, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.810 sec/batch; 73h:27m:20s remains)
INFO - root - 2017-12-10 10:25:07.070058: step 6240, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 71h:46m:57s remains)
INFO - root - 2017-12-10 10:25:14.767970: step 6250, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 69h:10m:10s remains)
INFO - root - 2017-12-10 10:25:22.714084: step 6260, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 72h:05m:19s remains)
INFO - root - 2017-12-10 10:25:30.382190: step 6270, loss = 0.71, batch loss = 0.65 (12.9 examples/sec; 0.619 sec/batch; 56h:03m:13s remains)
INFO - root - 2017-12-10 10:25:38.305429: step 6280, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.792 sec/batch; 71h:45m:20s remains)
INFO - root - 2017-12-10 10:25:46.168945: step 6290, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 72h:16m:01s remains)
INFO - root - 2017-12-10 10:25:54.247024: step 6300, loss = 0.72, batch loss = 0.66 (9.6 examples/sec; 0.831 sec/batch; 75h:18m:38s remains)
2017-12-10 10:25:55.015752: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.056133684 0.038940921 0.032756839 0.036742248 0.047401797 0.065782458 0.086364992 0.10427766 0.12424421 0.14895326 0.18187611 0.22169365 0.25868514 0.28337324 0.2926687][0.049114868 0.031854518 0.022629702 0.023248788 0.032004703 0.050203867 0.073302649 0.095932364 0.11816071 0.13786513 0.15873921 0.18109189 0.20156562 0.21631388 0.22310242][0.080695689 0.067777425 0.059258018 0.058157336 0.064850107 0.082255162 0.10719916 0.13195249 0.1524013 0.1636444 0.16931218 0.17085212 0.17084126 0.17265098 0.17568669][0.14263706 0.13992408 0.14035694 0.14587811 0.15799621 0.18129042 0.21279423 0.24019076 0.25829479 0.26215389 0.25474539 0.23764762 0.21738072 0.20409472 0.19697607][0.21997055 0.23323505 0.252148 0.27411991 0.30016756 0.33584693 0.37708619 0.406596 0.42255014 0.42258638 0.40798941 0.37871847 0.34327033 0.31560439 0.29209921][0.30316922 0.33569908 0.37809604 0.42140108 0.46478334 0.51321924 0.56024158 0.58649725 0.59773678 0.59591395 0.578371 0.54238492 0.49821362 0.46064359 0.42125717][0.38463748 0.43580672 0.49885654 0.5601114 0.61664784 0.670981 0.71323627 0.72661334 0.72653836 0.72006679 0.69937611 0.65845841 0.61036396 0.5696497 0.52370143][0.44251883 0.50673246 0.58245748 0.65266252 0.712601 0.7617088 0.78762871 0.77950293 0.76104772 0.74451715 0.7183935 0.67384636 0.62599957 0.58975536 0.55228078][0.44882795 0.51422995 0.58916175 0.65558416 0.70753241 0.74180812 0.74618977 0.71641821 0.67945558 0.65122563 0.619849 0.57545048 0.533433 0.50986636 0.49625754][0.38824138 0.44175497 0.50215483 0.55300975 0.58823794 0.60351187 0.58887923 0.5448842 0.49634615 0.46028844 0.42791504 0.390622 0.36211747 0.35887873 0.37841228][0.27243203 0.30684468 0.34580022 0.3764562 0.39370462 0.39334267 0.36854649 0.32150874 0.27165443 0.23590712 0.21088234 0.19059117 0.184971 0.2077415 0.25960514][0.133125 0.14985143 0.16996787 0.1845413 0.18976836 0.18226583 0.15821962 0.11917208 0.078581423 0.05291428 0.044534549 0.050034739 0.07426507 0.12419029 0.19820386][0.014056207 0.019699074 0.028598268 0.034666426 0.035088886 0.027511407 0.010665158 -0.014840786 -0.040197629 -0.049376667 -0.035991557 -0.0011418791 0.055221315 0.13034837 0.21404645][-0.049925342 -0.048254825 -0.044159994 -0.04143421 -0.042257633 -0.047981173 -0.057877421 -0.071222946 -0.081836388 -0.074340358 -0.041156664 0.018276736 0.10116971 0.19554697 0.27942643][-0.061214875 -0.05974232 -0.056909945 -0.055254292 -0.056648638 -0.061592381 -0.067856766 -0.073566213 -0.07358449 -0.053708512 -0.0077025797 0.06556648 0.16325037 0.26824874 0.34897205]]...]
INFO - root - 2017-12-10 10:26:02.903852: step 6310, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 71h:07m:55s remains)
INFO - root - 2017-12-10 10:26:10.765534: step 6320, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 70h:04m:08s remains)
INFO - root - 2017-12-10 10:26:18.762215: step 6330, loss = 0.71, batch loss = 0.66 (9.9 examples/sec; 0.809 sec/batch; 73h:15m:51s remains)
INFO - root - 2017-12-10 10:26:26.548108: step 6340, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.790 sec/batch; 71h:33m:09s remains)
INFO - root - 2017-12-10 10:26:34.269769: step 6350, loss = 0.69, batch loss = 0.64 (10.9 examples/sec; 0.734 sec/batch; 66h:27m:43s remains)
INFO - root - 2017-12-10 10:26:42.162784: step 6360, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 72h:55m:27s remains)
INFO - root - 2017-12-10 10:26:50.120314: step 6370, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 71h:58m:04s remains)
INFO - root - 2017-12-10 10:26:58.076475: step 6380, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.761 sec/batch; 68h:58m:12s remains)
INFO - root - 2017-12-10 10:27:05.996686: step 6390, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 71h:34m:46s remains)
INFO - root - 2017-12-10 10:27:13.819466: step 6400, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 69h:31m:02s remains)
2017-12-10 10:27:14.830998: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1265457 0.11800052 0.10787875 0.10602917 0.11840224 0.14566602 0.1769017 0.20112358 0.2161023 0.22059575 0.21542026 0.20349351 0.19321291 0.18734819 0.18333687][0.12863587 0.1166597 0.10502031 0.10481172 0.12149956 0.1527721 0.18548799 0.20876038 0.22176526 0.2252795 0.22050244 0.21043408 0.20412777 0.2027027 0.20069763][0.12978292 0.11494078 0.10320169 0.10641804 0.12826005 0.16210625 0.19322178 0.21226749 0.22036603 0.22117597 0.21703057 0.2106721 0.21017492 0.2141891 0.21512829][0.14202969 0.12771204 0.11827824 0.12528498 0.1508476 0.18500361 0.21235819 0.22583199 0.22838959 0.22646156 0.22361046 0.22168641 0.2266309 0.23426518 0.23622465][0.16260843 0.15143658 0.14526008 0.15509979 0.18221283 0.21553549 0.24073491 0.25297037 0.25510517 0.25412911 0.25346366 0.25323939 0.25694075 0.25883064 0.25308993][0.18586032 0.17678432 0.17207606 0.18332578 0.2115798 0.24631312 0.27451554 0.29211414 0.29973751 0.30242372 0.30207184 0.29727122 0.2905876 0.2769857 0.25570023][0.21317378 0.20246577 0.19543628 0.20581123 0.23504423 0.27363735 0.30879232 0.33527291 0.35044679 0.35592014 0.35195291 0.3359572 0.31221205 0.27824873 0.23906747][0.22867598 0.21238546 0.20017275 0.20793685 0.23797107 0.2815766 0.32525069 0.36075139 0.3820402 0.38740742 0.37649912 0.34742543 0.30755833 0.25813386 0.20823179][0.22514321 0.20339733 0.1855109 0.18926601 0.21821035 0.26519209 0.31587964 0.35854688 0.38371402 0.38750288 0.3704648 0.3334192 0.28697559 0.23485698 0.18614435][0.1976489 0.17194447 0.14986528 0.14976858 0.17653842 0.2254803 0.282107 0.33173451 0.36142963 0.36535159 0.34539232 0.30678108 0.26305416 0.21923976 0.18137422][0.15083063 0.12432257 0.10166065 0.0994026 0.12309803 0.1704932 0.22795095 0.27964649 0.3113139 0.31645593 0.29861549 0.26757905 0.23838362 0.2151359 0.19840492][0.10089232 0.076910339 0.057309289 0.055163518 0.07583838 0.11837927 0.17038792 0.2169351 0.24512212 0.25045943 0.23906147 0.22350004 0.21756998 0.22191073 0.23142087][0.053489465 0.034460045 0.020522576 0.020795766 0.039862204 0.076789625 0.12010895 0.1568145 0.17701054 0.18051194 0.1768453 0.17950709 0.19880787 0.23031594 0.26445615][0.017681208 0.0050872806 -0.0018514328 0.0025377714 0.021563249 0.053551249 0.08766181 0.11314479 0.12353127 0.1231643 0.12466794 0.14077505 0.17814256 0.22803821 0.2783781][-0.0035474931 -0.010496834 -0.011868184 -0.0042295326 0.014480164 0.04178299 0.067498393 0.083251186 0.085537195 0.081410371 0.084338777 0.10511878 0.14766213 0.2016294 0.25488383]]...]
INFO - root - 2017-12-10 10:27:22.692393: step 6410, loss = 0.70, batch loss = 0.65 (9.8 examples/sec; 0.813 sec/batch; 73h:37m:24s remains)
INFO - root - 2017-12-10 10:27:30.597486: step 6420, loss = 0.73, batch loss = 0.67 (8.8 examples/sec; 0.912 sec/batch; 82h:37m:03s remains)
INFO - root - 2017-12-10 10:27:38.332878: step 6430, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 72h:20m:06s remains)
INFO - root - 2017-12-10 10:27:46.254473: step 6440, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 71h:42m:47s remains)
INFO - root - 2017-12-10 10:27:54.159387: step 6450, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.773 sec/batch; 70h:01m:14s remains)
INFO - root - 2017-12-10 10:28:02.170667: step 6460, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 73h:35m:59s remains)
INFO - root - 2017-12-10 10:28:10.081145: step 6470, loss = 0.71, batch loss = 0.65 (9.5 examples/sec; 0.841 sec/batch; 76h:11m:59s remains)
INFO - root - 2017-12-10 10:28:17.986082: step 6480, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 73h:12m:09s remains)
INFO - root - 2017-12-10 10:28:25.951480: step 6490, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 71h:51m:27s remains)
INFO - root - 2017-12-10 10:28:33.764060: step 6500, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 69h:11m:16s remains)
2017-12-10 10:28:34.533501: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0073687062 0.015202939 0.051558811 0.096011527 0.14056793 0.17567943 0.19394627 0.19010852 0.16787626 0.1389188 0.11563785 0.10470218 0.10397718 0.1053711 0.10156228][0.0012107086 0.03111003 0.07746128 0.13281471 0.18735512 0.22825602 0.2460552 0.23627724 0.20696628 0.17355861 0.14945792 0.14144318 0.14652766 0.15489537 0.15669437][0.010368592 0.048442796 0.10553234 0.17256287 0.23724993 0.28253809 0.2965464 0.27661568 0.23710407 0.19850309 0.17520146 0.17259178 0.18618433 0.20554158 0.21892335][0.017977044 0.064102367 0.1318928 0.21084356 0.28572187 0.3350997 0.34436238 0.31304684 0.26202905 0.21725604 0.19463921 0.19770493 0.22126479 0.25513744 0.28532431][0.024567446 0.078070194 0.1550604 0.2434542 0.32561707 0.37706053 0.38137227 0.34107441 0.28234205 0.23477684 0.21444203 0.22295414 0.25632954 0.30562317 0.35417238][0.031876117 0.093015447 0.17904896 0.27603176 0.36418378 0.41710684 0.41824862 0.37367374 0.3123675 0.2648668 0.24574663 0.25493851 0.29135492 0.34901479 0.41036007][0.040258411 0.10909574 0.20401908 0.30924398 0.40321985 0.45881122 0.46052945 0.41695407 0.35652041 0.30864358 0.28632614 0.28912386 0.31937289 0.37420115 0.43710047][0.050650578 0.12730782 0.23109566 0.34473237 0.44522828 0.50487453 0.50951558 0.46888375 0.4087151 0.35664639 0.32546416 0.31625503 0.33400726 0.37722731 0.43178207][0.060925022 0.14296241 0.252377 0.37088478 0.47472957 0.5356077 0.540457 0.49991453 0.43749776 0.37936243 0.33830136 0.31754878 0.32293507 0.35219455 0.39362007][0.0673953 0.14971626 0.25756177 0.37276587 0.4724063 0.52823716 0.52866912 0.48551765 0.42071778 0.35874796 0.31188485 0.28494224 0.28220877 0.29963234 0.32804281][0.068348773 0.14539322 0.24429917 0.34790725 0.43564886 0.48075527 0.47396436 0.42819673 0.36338985 0.30138987 0.25289023 0.22351378 0.21532826 0.2231185 0.24028727][0.0615348 0.12787558 0.21101509 0.29550284 0.3641136 0.3935062 0.37767458 0.33031181 0.26922333 0.21293263 0.16950956 0.14401557 0.13576303 0.13880064 0.1483285][0.047505535 0.099971712 0.16401048 0.22629246 0.27360573 0.28739259 0.26463056 0.21865505 0.16535029 0.11965244 0.086855 0.070426546 0.06729766 0.070750721 0.076969288][0.027306076 0.0641303 0.10818625 0.14878635 0.17693445 0.17950454 0.15560012 0.11655582 0.075394139 0.043185465 0.022765823 0.015776798 0.018025687 0.022911914 0.027562056][0.0047627841 0.025882442 0.051436886 0.073652 0.087483719 0.084845126 0.065923296 0.038596764 0.011863122 -0.0070685749 -0.017116975 -0.018092405 -0.013785591 -0.0092518963 -0.0061789164]]...]
INFO - root - 2017-12-10 10:28:42.107842: step 6510, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 70h:09m:49s remains)
INFO - root - 2017-12-10 10:28:50.009182: step 6520, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 72h:25m:02s remains)
INFO - root - 2017-12-10 10:28:57.924591: step 6530, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 70h:13m:18s remains)
INFO - root - 2017-12-10 10:29:05.746851: step 6540, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 69h:07m:29s remains)
INFO - root - 2017-12-10 10:29:13.578771: step 6550, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 71h:58m:05s remains)
INFO - root - 2017-12-10 10:29:21.467490: step 6560, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 73h:06m:23s remains)
INFO - root - 2017-12-10 10:29:29.488768: step 6570, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 70h:34m:40s remains)
INFO - root - 2017-12-10 10:29:37.335790: step 6580, loss = 0.71, batch loss = 0.66 (10.2 examples/sec; 0.787 sec/batch; 71h:14m:24s remains)
INFO - root - 2017-12-10 10:29:45.008326: step 6590, loss = 0.72, batch loss = 0.66 (10.7 examples/sec; 0.748 sec/batch; 67h:42m:09s remains)
INFO - root - 2017-12-10 10:29:52.856096: step 6600, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.819 sec/batch; 74h:09m:59s remains)
2017-12-10 10:29:53.693915: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10279102 0.1804115 0.26861131 0.35342947 0.41749322 0.460422 0.47995359 0.47409618 0.45216665 0.43666244 0.44255194 0.45392793 0.46582657 0.46981752 0.46403652][0.1133498 0.20109393 0.2996681 0.39209932 0.46197781 0.50950629 0.53259224 0.52814186 0.505158 0.48722628 0.48867318 0.49390385 0.50109249 0.50572085 0.50537169][0.11330003 0.20379737 0.30467039 0.39727822 0.4669604 0.51427859 0.5385676 0.53644645 0.515105 0.49657685 0.49276346 0.49029341 0.48988411 0.49186221 0.49360055][0.10782858 0.19560246 0.2934275 0.38190204 0.44773856 0.4912582 0.51421714 0.51411539 0.49650675 0.47989291 0.47263882 0.46361127 0.45477569 0.45098424 0.45000511][0.10076768 0.18402784 0.27834755 0.36315626 0.42526349 0.46416745 0.48452076 0.48487666 0.470207 0.45603696 0.44753292 0.43515262 0.41948751 0.40816519 0.40127394][0.095649265 0.17758872 0.27422091 0.36217263 0.42633218 0.46422654 0.48280373 0.48115596 0.46487963 0.44935706 0.43847823 0.4237856 0.40249625 0.3827472 0.36856672][0.095185034 0.1797882 0.28467354 0.38265869 0.45516333 0.49707979 0.51597804 0.5108254 0.48767921 0.46461287 0.44713113 0.42831123 0.40187857 0.37532303 0.35645005][0.097047642 0.18469019 0.2978633 0.40664244 0.48972327 0.53959364 0.562556 0.55575788 0.52518368 0.49266002 0.46719334 0.44427359 0.41569144 0.38774145 0.37058461][0.098271318 0.18582843 0.30207235 0.41655624 0.50768411 0.56604874 0.59574491 0.59091282 0.55723882 0.51944369 0.4894968 0.465562 0.43947127 0.41672307 0.40711534][0.0971015 0.17983842 0.29135689 0.40299207 0.49520314 0.55778491 0.59286553 0.5917241 0.56007582 0.52412117 0.49735945 0.47952154 0.46270838 0.45145795 0.45261398][0.094696037 0.16934693 0.27006483 0.37138706 0.45678946 0.51655012 0.55173016 0.55285776 0.52650368 0.49853054 0.48168391 0.47546309 0.47205338 0.47390482 0.48288307][0.085864186 0.14980273 0.23521893 0.32024524 0.39144787 0.44058034 0.46886256 0.4687967 0.44855049 0.4314031 0.42756593 0.43514279 0.44632289 0.46031046 0.47324952][0.065695226 0.11662446 0.18419376 0.25002715 0.30338857 0.33783242 0.3556906 0.35236642 0.33751106 0.33021083 0.33730903 0.35608125 0.37908596 0.40236121 0.4166722][0.03687929 0.074169025 0.12442919 0.17232253 0.20926511 0.23042959 0.23890899 0.23255208 0.22093815 0.21857256 0.22969823 0.25193313 0.27857739 0.30360994 0.31508228][0.0065811696 0.030922383 0.065563664 0.09817896 0.12195516 0.13367505 0.13594709 0.12793432 0.11769611 0.11589089 0.12523486 0.14391021 0.16642475 0.18654209 0.19241966]]...]
INFO - root - 2017-12-10 10:30:01.524248: step 6610, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 70h:34m:51s remains)
INFO - root - 2017-12-10 10:30:09.298456: step 6620, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 70h:24m:47s remains)
INFO - root - 2017-12-10 10:30:17.278984: step 6630, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 71h:29m:51s remains)
INFO - root - 2017-12-10 10:30:25.212717: step 6640, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 72h:20m:11s remains)
INFO - root - 2017-12-10 10:30:33.087370: step 6650, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 70h:49m:49s remains)
INFO - root - 2017-12-10 10:30:41.029644: step 6660, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 70h:04m:00s remains)
INFO - root - 2017-12-10 10:30:48.795002: step 6670, loss = 0.72, batch loss = 0.66 (9.8 examples/sec; 0.815 sec/batch; 73h:44m:40s remains)
INFO - root - 2017-12-10 10:30:56.607335: step 6680, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 71h:42m:27s remains)
INFO - root - 2017-12-10 10:31:04.378787: step 6690, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 70h:23m:59s remains)
INFO - root - 2017-12-10 10:31:12.276498: step 6700, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 72h:09m:29s remains)
2017-12-10 10:31:13.119325: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2891736 0.35371864 0.40893382 0.43467912 0.42902687 0.4122549 0.39060193 0.36295846 0.33204708 0.317665 0.32905006 0.34677508 0.35231948 0.33959737 0.30852997][0.31043279 0.37334102 0.42409104 0.44289285 0.430652 0.41427374 0.3984842 0.37766126 0.35270494 0.34187749 0.35103777 0.35726508 0.34831366 0.323157 0.28365457][0.30525813 0.35785496 0.39583948 0.40384319 0.38682553 0.37506062 0.36965555 0.36047962 0.34621233 0.34117341 0.34805623 0.34333223 0.321811 0.28781307 0.24437261][0.29126734 0.3333905 0.36037922 0.36180618 0.34659719 0.34422472 0.35080081 0.35196289 0.344024 0.33851832 0.33720115 0.32025346 0.28854764 0.24982202 0.20736359][0.28043413 0.31666556 0.33993235 0.34390631 0.33870056 0.34944418 0.36787969 0.3765085 0.36903706 0.35510287 0.33839944 0.3061924 0.26436841 0.22297132 0.18460496][0.2925674 0.32892275 0.35541335 0.36811683 0.37398419 0.39271444 0.41502723 0.42256892 0.40788236 0.37978676 0.34476945 0.29739669 0.24749787 0.20685539 0.17593652][0.3314234 0.37349442 0.40791136 0.43082258 0.44298625 0.4596338 0.47461617 0.47146198 0.44326937 0.40002295 0.35026053 0.29295209 0.23916733 0.20189975 0.17920807][0.37856904 0.42624623 0.46628866 0.49500331 0.50675553 0.51370889 0.51487136 0.496951 0.45489675 0.39994472 0.34153175 0.28051758 0.22679964 0.19425581 0.17951907][0.40259597 0.44943568 0.488717 0.51717627 0.52466041 0.52101904 0.50958258 0.48006392 0.42837214 0.36670566 0.30562979 0.24665442 0.19742033 0.17155381 0.16489421][0.38023877 0.41815716 0.44913885 0.47135922 0.4728871 0.46195972 0.44363505 0.41011688 0.35796621 0.29815418 0.24158096 0.19032556 0.14960614 0.13192256 0.13297343][0.30650824 0.32963145 0.34788033 0.3603892 0.35630348 0.34237614 0.32292891 0.29248297 0.24817289 0.19861108 0.15345155 0.11449787 0.085147314 0.075982012 0.083341688][0.19910349 0.20749995 0.21373659 0.21742617 0.21044195 0.19774598 0.1815591 0.15895581 0.12757155 0.09310431 0.062792882 0.037683785 0.019941563 0.017983453 0.029172353][0.091410868 0.088560782 0.086128756 0.084314458 0.078029424 0.069619894 0.059127755 0.045325354 0.026845345 0.00686792 -0.009854163 -0.022899445 -0.030571194 -0.027079556 -0.014243384][0.0084912423 0.001108842 -0.0040065693 -0.0071744835 -0.011129766 -0.01536056 -0.020768631 -0.027743807 -0.036749635 -0.046094313 -0.053078257 -0.057507973 -0.058084767 -0.051709756 -0.039611436][-0.038930796 -0.045654986 -0.049544178 -0.051996984 -0.054016437 -0.055713229 -0.05793402 -0.060827821 -0.06421119 -0.067228928 -0.068565406 -0.068036631 -0.064831235 -0.057889942 -0.048068691]]...]
INFO - root - 2017-12-10 10:31:20.853683: step 6710, loss = 0.70, batch loss = 0.65 (10.8 examples/sec; 0.741 sec/batch; 67h:01m:35s remains)
INFO - root - 2017-12-10 10:31:28.649896: step 6720, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 70h:58m:35s remains)
INFO - root - 2017-12-10 10:31:36.495959: step 6730, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.766 sec/batch; 69h:20m:58s remains)
INFO - root - 2017-12-10 10:31:44.339481: step 6740, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 70h:37m:23s remains)
INFO - root - 2017-12-10 10:31:52.160294: step 6750, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 72h:08m:14s remains)
INFO - root - 2017-12-10 10:31:59.945392: step 6760, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 69h:04m:24s remains)
INFO - root - 2017-12-10 10:32:07.766838: step 6770, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 70h:09m:39s remains)
INFO - root - 2017-12-10 10:32:15.571041: step 6780, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 72h:32m:48s remains)
INFO - root - 2017-12-10 10:32:23.429869: step 6790, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.806 sec/batch; 72h:54m:57s remains)
INFO - root - 2017-12-10 10:32:31.348705: step 6800, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.769 sec/batch; 69h:36m:41s remains)
2017-12-10 10:32:32.232909: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16931249 0.16960508 0.18422194 0.22094633 0.27601555 0.33425522 0.37048927 0.36936167 0.32730573 0.26150569 0.19902107 0.16233699 0.161619 0.18775785 0.22453076][0.20210178 0.20070326 0.21266989 0.24849775 0.30439442 0.36281562 0.39634371 0.38928318 0.3395597 0.26671284 0.20104635 0.16722198 0.17217143 0.20452419 0.2459586][0.22061838 0.21700808 0.22546898 0.25775763 0.31017771 0.36447304 0.39389548 0.38337997 0.33159065 0.25819555 0.19449809 0.16633971 0.17712741 0.21420838 0.25795868][0.2242804 0.21558434 0.21892662 0.246609 0.29567522 0.34781519 0.37775233 0.37050003 0.32408273 0.25585735 0.1957489 0.17062673 0.18231586 0.21856701 0.26019773][0.21753755 0.20596889 0.20618553 0.23140711 0.28008386 0.33366141 0.3679359 0.36780179 0.33053294 0.27017525 0.21335068 0.187188 0.19347645 0.22182624 0.255037][0.19703166 0.18773986 0.1892093 0.21513441 0.26559034 0.32227063 0.36207753 0.36981326 0.34204355 0.28950626 0.23472209 0.20405158 0.20021851 0.21568733 0.23644172][0.16007656 0.15610664 0.16291855 0.19341458 0.24913964 0.31234121 0.35973245 0.3747997 0.35337743 0.30369848 0.24527037 0.20419526 0.18610071 0.18677172 0.19529371][0.11253496 0.11462163 0.12846561 0.16572186 0.22930677 0.30178884 0.35829461 0.37973019 0.36098549 0.30839014 0.24038951 0.18401954 0.14947122 0.13561776 0.13381532][0.067993611 0.074220896 0.092747509 0.133961 0.20184861 0.27989507 0.34223324 0.36791402 0.35094813 0.29586518 0.22026628 0.15170811 0.10460931 0.080369249 0.072068706][0.03607117 0.043759316 0.062618919 0.10215376 0.16679353 0.24211872 0.30332685 0.32970363 0.31483153 0.26184967 0.1868888 0.11606342 0.065715238 0.038514305 0.028547417][0.018234402 0.023965798 0.039465915 0.073016115 0.12873179 0.19468687 0.24881962 0.27229518 0.25912085 0.2120582 0.14553168 0.082315281 0.037901863 0.015184659 0.0086755641][0.023815274 0.026193248 0.036394842 0.061873857 0.10645284 0.16054074 0.2052525 0.2238906 0.21127667 0.17082307 0.11560762 0.064305723 0.029992333 0.015568292 0.015949234][0.062301554 0.06253995 0.06681215 0.082974993 0.11553425 0.15761705 0.19370292 0.20864265 0.19781348 0.16529618 0.12267388 0.084175207 0.059929084 0.053449415 0.060643427][0.13143811 0.13271318 0.13170823 0.13679026 0.15423088 0.18195421 0.20946068 0.22289558 0.21762998 0.19658448 0.16826609 0.14126392 0.12306122 0.11944047 0.12891282][0.20501404 0.21125478 0.20788658 0.20328125 0.20572609 0.21893567 0.23890738 0.25253406 0.25476012 0.24553861 0.22967845 0.21025465 0.1931171 0.18743339 0.19506128]]...]
INFO - root - 2017-12-10 10:32:40.157504: step 6810, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 71h:23m:07s remains)
INFO - root - 2017-12-10 10:32:48.201135: step 6820, loss = 0.71, batch loss = 0.65 (8.7 examples/sec; 0.923 sec/batch; 83h:32m:22s remains)
INFO - root - 2017-12-10 10:32:55.967699: step 6830, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 69h:52m:48s remains)
INFO - root - 2017-12-10 10:33:03.930244: step 6840, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 72h:33m:13s remains)
INFO - root - 2017-12-10 10:33:11.785365: step 6850, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 69h:32m:52s remains)
INFO - root - 2017-12-10 10:33:19.690095: step 6860, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 72h:40m:57s remains)
INFO - root - 2017-12-10 10:33:27.490482: step 6870, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 72h:35m:34s remains)
INFO - root - 2017-12-10 10:33:35.450434: step 6880, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 72h:08m:14s remains)
INFO - root - 2017-12-10 10:33:43.294125: step 6890, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 68h:19m:49s remains)
INFO - root - 2017-12-10 10:33:51.094997: step 6900, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 69h:45m:30s remains)
2017-12-10 10:33:51.949691: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.097313181 0.13408709 0.16132118 0.17874974 0.18603486 0.18431577 0.17514591 0.16082557 0.14055024 0.11981453 0.11021235 0.12791128 0.17233616 0.2286614 0.27684844][0.141232 0.19391766 0.23908134 0.27503011 0.2982634 0.30659795 0.30065247 0.28251466 0.24981612 0.21365201 0.19312406 0.21163316 0.26609823 0.33547556 0.39380372][0.17058271 0.23864608 0.30409041 0.362597 0.406664 0.42992929 0.43068531 0.40909609 0.36013925 0.30126062 0.26131171 0.272095 0.32983741 0.40796396 0.47603762][0.17568499 0.25457424 0.33675453 0.41495341 0.47837234 0.51763415 0.5284391 0.50663984 0.44336477 0.36089835 0.2975857 0.29312077 0.34597054 0.42610916 0.50016314][0.16098966 0.2421556 0.32991254 0.41624054 0.49083015 0.54413956 0.5692144 0.55476516 0.48637947 0.3889361 0.30752817 0.28804782 0.33128834 0.40588924 0.47750324][0.13486029 0.20980036 0.29293227 0.37735724 0.45666835 0.52296221 0.56566507 0.56451654 0.49998263 0.3982507 0.30796239 0.27776915 0.30961868 0.37226388 0.43256068][0.10299303 0.16950552 0.24761763 0.33189476 0.41933766 0.50163418 0.56221354 0.57332182 0.51378238 0.41120741 0.31427249 0.27158284 0.28647745 0.3309797 0.3752889][0.074881695 0.13583948 0.21469334 0.30585793 0.40633205 0.50404769 0.57590687 0.59125495 0.53284377 0.43071866 0.33037966 0.27639174 0.27275008 0.29667354 0.32419071][0.064012691 0.12527837 0.20946339 0.30802706 0.41564235 0.51665163 0.58572721 0.59521925 0.53637332 0.44134712 0.34875363 0.29537496 0.28358844 0.29577649 0.31351343][0.070690267 0.1358358 0.22430226 0.32433936 0.42900905 0.52185279 0.57984185 0.58062392 0.52350152 0.44138625 0.36514688 0.32346454 0.31593055 0.32838538 0.34605753][0.080897525 0.14807563 0.23685771 0.33439094 0.43337393 0.51776993 0.56705886 0.56203252 0.5067029 0.4346512 0.37267345 0.34401441 0.34555751 0.36395034 0.38586226][0.08276917 0.14745688 0.23153162 0.32422012 0.41855514 0.4989742 0.54505533 0.53765142 0.48215851 0.41302225 0.35641858 0.33297077 0.33873764 0.36003309 0.38516268][0.0718283 0.12747273 0.19959885 0.28022119 0.36340466 0.43534577 0.47628072 0.46666029 0.41218281 0.34715346 0.29602808 0.27627149 0.28354609 0.30516535 0.33212686][0.052643985 0.096314594 0.15155835 0.2121764 0.27352008 0.32498547 0.35143197 0.33660212 0.28714964 0.23358153 0.19470942 0.18278135 0.19312446 0.21632414 0.245907][0.027199646 0.057721995 0.094371475 0.13220336 0.16834879 0.19585896 0.20583646 0.18817624 0.14962332 0.11245354 0.088108711 0.083530977 0.094803445 0.11690345 0.14558864]]...]
INFO - root - 2017-12-10 10:33:59.654578: step 6910, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 71h:46m:56s remains)
INFO - root - 2017-12-10 10:34:07.474059: step 6920, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 69h:50m:45s remains)
INFO - root - 2017-12-10 10:34:15.377427: step 6930, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.815 sec/batch; 73h:42m:03s remains)
INFO - root - 2017-12-10 10:34:23.262691: step 6940, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 69h:10m:10s remains)
INFO - root - 2017-12-10 10:34:30.902513: step 6950, loss = 0.72, batch loss = 0.66 (14.0 examples/sec; 0.571 sec/batch; 51h:38m:14s remains)
INFO - root - 2017-12-10 10:34:38.812270: step 6960, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.811 sec/batch; 73h:19m:22s remains)
INFO - root - 2017-12-10 10:34:46.664883: step 6970, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 70h:50m:29s remains)
INFO - root - 2017-12-10 10:34:54.559967: step 6980, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 70h:47m:31s remains)
INFO - root - 2017-12-10 10:35:02.251941: step 6990, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.811 sec/batch; 73h:19m:01s remains)
INFO - root - 2017-12-10 10:35:10.150029: step 7000, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 72h:26m:46s remains)
2017-12-10 10:35:11.005752: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.062658176 0.057022829 0.052200358 0.055709794 0.066101946 0.078891434 0.092615388 0.10881577 0.12317517 0.12771994 0.11850633 0.099431768 0.072295018 0.038528539 0.0031163904][0.10069342 0.0956099 0.093019538 0.10319906 0.12313452 0.14649126 0.17134789 0.1989581 0.22115797 0.22634555 0.21061327 0.18086156 0.1404019 0.090716176 0.038758013][0.13289537 0.12934248 0.13010775 0.14765731 0.17690296 0.20969874 0.24392682 0.28055882 0.30834559 0.31289655 0.29092652 0.25200358 0.20023172 0.13688496 0.070281923][0.1611855 0.16016543 0.16418993 0.18671063 0.22060938 0.25656068 0.29300228 0.33186895 0.36016098 0.36239487 0.33607152 0.29205588 0.23405984 0.16257258 0.0870492][0.1952979 0.19810203 0.20469637 0.22832043 0.26114169 0.29434797 0.32801637 0.36553964 0.39184639 0.39076 0.36037996 0.31176528 0.2484753 0.17075242 0.089944668][0.23877849 0.24979438 0.26030275 0.28348994 0.31315953 0.34302181 0.37463582 0.41111118 0.43557608 0.43065915 0.39414173 0.3368125 0.26399773 0.17768037 0.091253407][0.2767486 0.3008993 0.31836036 0.3414973 0.36913973 0.39765811 0.42844889 0.46344697 0.48540124 0.47714919 0.43382838 0.36623231 0.28302017 0.18869096 0.097620167][0.29210764 0.32994455 0.35510036 0.37814152 0.40444961 0.43228385 0.46093994 0.49118283 0.50878084 0.49772257 0.44996077 0.37624928 0.28850073 0.19301473 0.10249866][0.27494276 0.32070503 0.34934112 0.36957881 0.39236996 0.41721854 0.44011512 0.46193233 0.47374293 0.46252066 0.41738796 0.347566 0.26637062 0.17980136 0.097946167][0.22879001 0.27139911 0.29546109 0.30894166 0.32555935 0.34518278 0.36084396 0.37413046 0.38247204 0.37595561 0.3412759 0.28460518 0.21884632 0.14890333 0.0814678][0.15903838 0.1887024 0.20207886 0.20687753 0.21632768 0.22940046 0.23775224 0.24411622 0.25142315 0.25232577 0.23200351 0.19359134 0.14846164 0.10014096 0.051824413][0.079210147 0.093107745 0.095593639 0.09325736 0.096665978 0.10325696 0.10520261 0.1064046 0.11266372 0.11898579 0.1110322 0.090203218 0.065770708 0.039596818 0.011994168][0.012195644 0.013952456 0.0095476517 0.0036616193 0.0032115986 0.0049438057 0.0029891445 0.0015235511 0.006182217 0.013601268 0.012478165 0.0037762085 -0.0053749466 -0.014644452 -0.025514111][-0.027916694 -0.032664023 -0.039730027 -0.046497714 -0.049163744 -0.050479725 -0.053855244 -0.055713754 -0.052458528 -0.046413828 -0.044889528 -0.046965647 -0.047697879 -0.047737859 -0.04901205][-0.045047004 -0.052034 -0.059080288 -0.065255813 -0.068818696 -0.07153327 -0.074757971 -0.075986654 -0.073715672 -0.069704741 -0.067677766 -0.06674163 -0.064147957 -0.060909096 -0.058652997]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 10:35:18.826257: step 7010, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 69h:18m:41s remains)
INFO - root - 2017-12-10 10:35:26.666122: step 7020, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.784 sec/batch; 70h:51m:14s remains)
INFO - root - 2017-12-10 10:35:34.543752: step 7030, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 72h:28m:51s remains)
INFO - root - 2017-12-10 10:35:42.257014: step 7040, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 71h:48m:34s remains)
INFO - root - 2017-12-10 10:35:50.163071: step 7050, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 69h:43m:55s remains)
INFO - root - 2017-12-10 10:35:58.002138: step 7060, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 71h:01m:58s remains)
INFO - root - 2017-12-10 10:36:05.686542: step 7070, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.759 sec/batch; 68h:37m:24s remains)
INFO - root - 2017-12-10 10:36:13.597733: step 7080, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 70h:26m:53s remains)
INFO - root - 2017-12-10 10:36:21.465053: step 7090, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 70h:52m:14s remains)
INFO - root - 2017-12-10 10:36:29.353408: step 7100, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.751 sec/batch; 67h:54m:19s remains)
2017-12-10 10:36:30.188854: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11719783 0.12259141 0.13820021 0.16436426 0.19683985 0.22682641 0.24568562 0.24525091 0.21755323 0.17080748 0.12759167 0.10936228 0.12166163 0.14889802 0.17352059][0.086472325 0.099660978 0.1303931 0.17976256 0.23952115 0.29289705 0.32518911 0.32675421 0.28775641 0.21700169 0.14502195 0.10215912 0.099256039 0.12141351 0.14881338][0.058066126 0.08126539 0.12994413 0.2049423 0.29257897 0.36807248 0.41062191 0.40974417 0.3562873 0.26074544 0.1598722 0.092470929 0.074368924 0.091147006 0.12212401][0.046490062 0.082178622 0.15014948 0.24917969 0.36094224 0.45428622 0.50310928 0.49690598 0.42940024 0.312907 0.18721794 0.096910477 0.062799372 0.07198038 0.10274573][0.053550363 0.10403749 0.1902646 0.30817726 0.43642908 0.54111725 0.59232706 0.58057106 0.50430125 0.37803942 0.23924065 0.1312463 0.078377344 0.072658658 0.095360205][0.0821891 0.15080319 0.25589594 0.38832477 0.52490741 0.63282168 0.68090212 0.66123885 0.57940567 0.45391715 0.3149876 0.19635712 0.12290833 0.09521772 0.10219544][0.1194284 0.20726112 0.33074054 0.47304359 0.60921985 0.71081913 0.74956995 0.71963125 0.6344282 0.51633328 0.38555744 0.2619288 0.16938527 0.11957944 0.10953607][0.15355815 0.25734773 0.39353254 0.53758377 0.66384333 0.75174189 0.77937514 0.74246466 0.65834183 0.55016422 0.42954057 0.30500263 0.20025742 0.13644145 0.11543476][0.18677166 0.29687691 0.43243322 0.56449926 0.66910475 0.73650211 0.75361115 0.71599728 0.63888907 0.54280972 0.43468562 0.317621 0.21584447 0.15507399 0.13768212][0.23126723 0.33407411 0.45289198 0.55902332 0.63246816 0.67457706 0.68123758 0.64634937 0.57971448 0.49785972 0.40679884 0.30952191 0.22963993 0.1908147 0.19166186][0.27642998 0.36060837 0.45190808 0.52656311 0.56915528 0.58787507 0.58462632 0.55216783 0.4960779 0.4295947 0.35953823 0.29137391 0.24657547 0.2412601 0.26611853][0.28285497 0.34122157 0.39993203 0.44437155 0.46345741 0.46655715 0.45751607 0.42948368 0.38561895 0.3364279 0.28922719 0.25225639 0.24382435 0.27077442 0.31489095][0.24622209 0.2761192 0.30394012 0.32380837 0.32838073 0.32449508 0.31500137 0.29473045 0.26536372 0.23436585 0.208232 0.19713651 0.21612056 0.26366121 0.31537685][0.18313493 0.18891481 0.19304095 0.19519287 0.19101924 0.18442537 0.17681484 0.16524962 0.15041065 0.13650484 0.12787747 0.13446541 0.16750874 0.21995141 0.266618][0.11256339 0.10493698 0.09709882 0.090412691 0.082554609 0.075559117 0.0701044 0.065233342 0.060958829 0.058823396 0.061011028 0.0749584 0.10758118 0.14991955 0.18161547]]...]
INFO - root - 2017-12-10 10:36:38.216345: step 7110, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.777 sec/batch; 70h:16m:30s remains)
INFO - root - 2017-12-10 10:36:46.117742: step 7120, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 72h:38m:19s remains)
INFO - root - 2017-12-10 10:36:53.854497: step 7130, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.800 sec/batch; 72h:19m:04s remains)
INFO - root - 2017-12-10 10:37:01.851793: step 7140, loss = 0.72, batch loss = 0.67 (10.0 examples/sec; 0.802 sec/batch; 72h:31m:14s remains)
INFO - root - 2017-12-10 10:37:09.436821: step 7150, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 68h:50m:18s remains)
INFO - root - 2017-12-10 10:37:17.305334: step 7160, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 69h:37m:34s remains)
INFO - root - 2017-12-10 10:37:25.179833: step 7170, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 72h:41m:20s remains)
INFO - root - 2017-12-10 10:37:32.926002: step 7180, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.775 sec/batch; 70h:04m:11s remains)
INFO - root - 2017-12-10 10:37:40.754715: step 7190, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.770 sec/batch; 69h:33m:31s remains)
INFO - root - 2017-12-10 10:37:48.652620: step 7200, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 70h:35m:00s remains)
2017-12-10 10:37:49.540760: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30295604 0.30560407 0.30574438 0.29927993 0.28019902 0.25828075 0.24606587 0.24366802 0.24817339 0.26026592 0.28121322 0.30252919 0.3167035 0.32406747 0.32682902][0.24507836 0.26560816 0.28867519 0.30305952 0.29980865 0.28639349 0.27285495 0.26162812 0.25357461 0.25402409 0.26706162 0.28515589 0.30157876 0.31572175 0.32884267][0.18769448 0.22989489 0.28469238 0.33071288 0.35156339 0.34989548 0.33296353 0.30557451 0.27291664 0.24777791 0.24106035 0.24817608 0.26278737 0.28316793 0.30948621][0.16103496 0.21782243 0.30280402 0.385704 0.43817109 0.45477739 0.43874484 0.39464 0.33102387 0.27005237 0.23286904 0.2191741 0.22349952 0.24434157 0.28163338][0.16446854 0.23088773 0.34135383 0.46271873 0.55397838 0.59909683 0.59493154 0.54217362 0.45031124 0.34921786 0.2714842 0.22416578 0.2054926 0.21622032 0.25679252][0.17561375 0.25033146 0.3801156 0.53388286 0.66213751 0.73921585 0.75454128 0.70293725 0.59298915 0.45814162 0.33923542 0.25221455 0.20290627 0.19591333 0.23214358][0.17771752 0.26106447 0.40527734 0.58110344 0.73568261 0.83786428 0.87135786 0.8252908 0.70722425 0.55148244 0.40270936 0.28372538 0.208478 0.18579705 0.21651514][0.17067905 0.25873369 0.40866777 0.592625 0.75886452 0.87484133 0.9210043 0.8822695 0.76447487 0.60143667 0.43944052 0.30597195 0.22028886 0.19272052 0.22274242][0.15273347 0.23847914 0.37990341 0.55255687 0.70996636 0.82224613 0.87003875 0.8382265 0.73197132 0.5816111 0.43021256 0.3072843 0.23410438 0.21839611 0.25720277][0.12361872 0.2006146 0.3217043 0.46652791 0.59708852 0.688605 0.725023 0.69617033 0.60872269 0.48770115 0.36860517 0.28035015 0.24258561 0.25662255 0.31643674][0.075818181 0.13733289 0.2319155 0.34186283 0.43817008 0.50150645 0.52024329 0.49057323 0.42212978 0.33600149 0.2593157 0.21694054 0.22372097 0.27392757 0.35924727][0.020178514 0.0602074 0.12568001 0.20051059 0.26435271 0.30246139 0.30669588 0.27731487 0.22535102 0.1696905 0.13066791 0.12799174 0.17029692 0.24707136 0.34973794][-0.014335237 0.0023428651 0.037751198 0.079503216 0.11549147 0.13526429 0.13237849 0.10745304 0.069852017 0.035859659 0.021178197 0.041261565 0.099778362 0.18436946 0.28756073][-0.019367097 -0.020553911 -0.010175022 0.0047137761 0.0188961 0.026085729 0.020659547 0.0015883484 -0.024682634 -0.045506835 -0.04890538 -0.022736929 0.034215424 0.11039084 0.19941396][-0.0063464437 -0.014756787 -0.017854912 -0.017787289 -0.015334893 -0.01392948 -0.018871607 -0.03297516 -0.052603994 -0.068835817 -0.072585136 -0.054065265 -0.011877501 0.045328815 0.11185434]]...]
INFO - root - 2017-12-10 10:37:57.423265: step 7210, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.789 sec/batch; 71h:16m:16s remains)
INFO - root - 2017-12-10 10:38:05.051772: step 7220, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 70h:25m:10s remains)
INFO - root - 2017-12-10 10:38:12.788625: step 7230, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 71h:30m:11s remains)
INFO - root - 2017-12-10 10:38:20.691161: step 7240, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 69h:50m:28s remains)
INFO - root - 2017-12-10 10:38:28.667900: step 7250, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.828 sec/batch; 74h:48m:28s remains)
INFO - root - 2017-12-10 10:38:36.441713: step 7260, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 70h:56m:30s remains)
INFO - root - 2017-12-10 10:38:44.379342: step 7270, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 71h:00m:07s remains)
INFO - root - 2017-12-10 10:38:52.261542: step 7280, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 71h:03m:15s remains)
INFO - root - 2017-12-10 10:39:00.189428: step 7290, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 71h:55m:01s remains)
INFO - root - 2017-12-10 10:39:08.119509: step 7300, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 69h:19m:54s remains)
2017-12-10 10:39:08.864499: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18161991 0.14020036 0.10116787 0.079975352 0.085767113 0.11927801 0.17587504 0.249829 0.32281429 0.37520662 0.39040074 0.36926559 0.32062835 0.24968043 0.17459628][0.18062161 0.13935529 0.10270012 0.086798146 0.10051851 0.14425486 0.2116362 0.29436585 0.37425515 0.43268955 0.45165622 0.42976171 0.3787201 0.30838984 0.23749354][0.19079734 0.15314429 0.12177727 0.11380113 0.13578606 0.18629198 0.25629988 0.33450717 0.40665975 0.46073794 0.47997373 0.45978177 0.41162965 0.34892854 0.28965622][0.21993385 0.18745904 0.16168937 0.16174328 0.1912121 0.24430867 0.30770814 0.36755157 0.41608334 0.45228627 0.46532986 0.44812483 0.40828919 0.35965452 0.31853908][0.25225767 0.22688581 0.2080974 0.21737868 0.25399 0.30580309 0.35573471 0.38907668 0.40514082 0.41520247 0.41854134 0.40698811 0.38108149 0.35171261 0.33280709][0.28036514 0.26284337 0.25370023 0.27405763 0.317578 0.36496493 0.3976858 0.40328652 0.38709185 0.37106857 0.36413613 0.35885921 0.34787685 0.33672658 0.33689904][0.30142477 0.29212815 0.29321739 0.32385525 0.37259933 0.41411519 0.42986858 0.41288343 0.37309518 0.33751529 0.32083938 0.31822336 0.31797814 0.31948483 0.33166954][0.31181079 0.31087321 0.32076412 0.35880917 0.41036934 0.4463152 0.44790378 0.41487691 0.36142227 0.31451049 0.29018882 0.28646082 0.29206425 0.30181155 0.32130978][0.30901924 0.31362706 0.32679003 0.36545351 0.41458395 0.44475156 0.43694088 0.39679438 0.34155032 0.29427505 0.26889911 0.26493406 0.27435282 0.28975412 0.314123][0.2950516 0.2991766 0.30660951 0.33638105 0.3767758 0.40232626 0.39386922 0.35889193 0.31549203 0.28078237 0.26370037 0.26277447 0.27326438 0.28844991 0.31022415][0.28539658 0.28392953 0.27917975 0.29321674 0.32024702 0.34151116 0.33750466 0.31412309 0.2886689 0.27281013 0.26963022 0.274725 0.28490931 0.29476786 0.30666909][0.28487539 0.27839455 0.26137751 0.25850874 0.27040815 0.28507411 0.28380945 0.27065596 0.26150867 0.26343602 0.27450082 0.28696674 0.29735032 0.30131972 0.30284262][0.2890842 0.28317603 0.26047608 0.24597853 0.24503934 0.25073284 0.24758521 0.24001221 0.24198458 0.2564252 0.27673328 0.29318294 0.30185977 0.29968545 0.29320583][0.28856093 0.28813049 0.26795703 0.25061268 0.24388346 0.2435243 0.23763967 0.23350886 0.24306273 0.26507857 0.28804258 0.30219233 0.30483761 0.29504371 0.28242061][0.27260694 0.27784029 0.26430261 0.25044969 0.24581352 0.2469008 0.2433961 0.24555682 0.26267013 0.28985128 0.31099847 0.31701151 0.30818948 0.28684032 0.26607946]]...]
INFO - root - 2017-12-10 10:39:16.291362: step 7310, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.753 sec/batch; 68h:02m:00s remains)
INFO - root - 2017-12-10 10:39:24.176750: step 7320, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 71h:32m:00s remains)
INFO - root - 2017-12-10 10:39:31.945602: step 7330, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.749 sec/batch; 67h:40m:37s remains)
INFO - root - 2017-12-10 10:39:39.798652: step 7340, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 69h:07m:58s remains)
INFO - root - 2017-12-10 10:39:47.654133: step 7350, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 71h:32m:23s remains)
INFO - root - 2017-12-10 10:39:55.588642: step 7360, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 69h:28m:17s remains)
INFO - root - 2017-12-10 10:40:03.443114: step 7370, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 71h:00m:41s remains)
INFO - root - 2017-12-10 10:40:11.216107: step 7380, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.818 sec/batch; 73h:51m:19s remains)
INFO - root - 2017-12-10 10:40:18.901977: step 7390, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 72h:16m:33s remains)
INFO - root - 2017-12-10 10:40:26.400988: step 7400, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 68h:36m:44s remains)
2017-12-10 10:40:27.292890: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13945426 0.1172898 0.097741336 0.10708857 0.15948185 0.24461605 0.34346724 0.42522994 0.4562175 0.42158076 0.342072 0.24873982 0.1674149 0.12052922 0.1146626][0.1064887 0.096515693 0.092113666 0.11957285 0.19220458 0.2972475 0.41226545 0.50543016 0.53919655 0.49420577 0.39436486 0.2765359 0.17092589 0.10267752 0.082081161][0.077985078 0.078777917 0.089551836 0.13395011 0.22220162 0.33851722 0.45896903 0.55327046 0.5840885 0.53053826 0.41821358 0.28741267 0.16855855 0.085536346 0.049866464][0.056413013 0.066323087 0.090049013 0.14700374 0.24368818 0.3617115 0.47866946 0.567101 0.59241676 0.53427583 0.41742703 0.28257993 0.15792359 0.06550952 0.016767029][0.034464221 0.049350556 0.07947392 0.13969468 0.23376575 0.34376264 0.45101193 0.53129387 0.55360585 0.49925667 0.389236 0.26125044 0.14161384 0.050040256 -0.0028840944][0.017405182 0.033553224 0.065277532 0.12355038 0.21100824 0.31275284 0.41356534 0.49111435 0.51562518 0.46860066 0.36696845 0.24632792 0.1328906 0.044564392 -0.0090117417][0.012446885 0.0300683 0.064233385 0.12291466 0.20812996 0.30763596 0.4069348 0.48345745 0.50743771 0.46168804 0.36155313 0.24308 0.13323024 0.047613528 -0.0063791126][0.0095177237 0.029120753 0.0676932 0.13064973 0.21900198 0.3211683 0.42046151 0.49282339 0.51008946 0.4591808 0.35565782 0.2361318 0.12778962 0.044281811 -0.009513054][-0.0015771943 0.018886102 0.060434595 0.12619868 0.21596104 0.31785062 0.41283023 0.47670963 0.48587576 0.43253252 0.33146036 0.21623094 0.11245385 0.033076845 -0.017774355][-0.013596665 0.0056788255 0.045696598 0.10742725 0.19008254 0.28286549 0.366643 0.41928372 0.42312649 0.3742941 0.28475752 0.18212052 0.089280985 0.018291971 -0.025810692][-0.018351708 -0.0016734429 0.032922424 0.084692664 0.15327816 0.23076603 0.3004075 0.3427456 0.34409475 0.30211648 0.2262353 0.13898307 0.060264226 0.00036985398 -0.034914128][-0.01541401 -0.0035465586 0.022045858 0.059694771 0.10961219 0.16711976 0.21920142 0.24957393 0.24734364 0.21103038 0.14865071 0.079282865 0.019703511 -0.022943322 -0.044878706][-0.009374978 -0.0051792464 0.0081832772 0.029047497 0.057730462 0.092242785 0.12426712 0.14207619 0.13786002 0.11011687 0.064530954 0.016187642 -0.021669339 -0.044719875 -0.052173641][-0.0012070227 -0.0065410212 -0.0058643888 -0.00019180347 0.01016948 0.024693986 0.039323997 0.046949945 0.042832896 0.025421109 -0.002195708 -0.029814819 -0.048300169 -0.055296712 -0.051539287][0.013213698 -0.0028121721 -0.014129416 -0.020346047 -0.022825364 -0.022142461 -0.019970044 -0.020046109 -0.02442936 -0.033557534 -0.045794223 -0.055649165 -0.058425475 -0.053388018 -0.041328494]]...]
INFO - root - 2017-12-10 10:40:35.130364: step 7410, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.805 sec/batch; 72h:40m:17s remains)
INFO - root - 2017-12-10 10:40:43.097191: step 7420, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 72h:30m:05s remains)
INFO - root - 2017-12-10 10:40:50.976719: step 7430, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 68h:20m:40s remains)
INFO - root - 2017-12-10 10:40:58.871575: step 7440, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 72h:53m:58s remains)
INFO - root - 2017-12-10 10:41:06.811456: step 7450, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.808 sec/batch; 72h:55m:48s remains)
INFO - root - 2017-12-10 10:41:14.683086: step 7460, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 71h:00m:33s remains)
INFO - root - 2017-12-10 10:41:22.303265: step 7470, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 69h:14m:18s remains)
INFO - root - 2017-12-10 10:41:30.103189: step 7480, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 71h:30m:28s remains)
INFO - root - 2017-12-10 10:41:37.835422: step 7490, loss = 0.70, batch loss = 0.65 (10.6 examples/sec; 0.755 sec/batch; 68h:11m:43s remains)
INFO - root - 2017-12-10 10:41:45.715471: step 7500, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 70h:32m:51s remains)
2017-12-10 10:41:46.624116: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18670467 0.17041081 0.1614003 0.16995734 0.18796943 0.20540644 0.21801648 0.22099443 0.21198609 0.18943492 0.15315516 0.11583132 0.093538851 0.092178836 0.10789793][0.24814323 0.23174582 0.22199242 0.23174876 0.25138521 0.26847181 0.27784207 0.27204224 0.24757449 0.20546409 0.15034112 0.10013837 0.073683873 0.074126527 0.093142584][0.3010301 0.28677303 0.27487797 0.28149903 0.298666 0.31320569 0.32020497 0.30971473 0.27544007 0.22051865 0.15373252 0.096695833 0.070093907 0.073797017 0.095540777][0.33355162 0.32133341 0.30496633 0.30426753 0.31539637 0.3279759 0.33774969 0.33144993 0.29822919 0.24270518 0.1755881 0.11923884 0.095119834 0.10093352 0.12246686][0.33859071 0.32919967 0.30783367 0.29866379 0.30352667 0.31705338 0.33524427 0.34053758 0.31630871 0.26792523 0.2065862 0.15428646 0.1331917 0.13950384 0.15872119][0.32814643 0.32398602 0.3013247 0.28725865 0.28872925 0.30611506 0.33540013 0.35492688 0.34233779 0.30239955 0.24640022 0.1961153 0.17584221 0.18123734 0.1986316][0.33681938 0.33552071 0.31404573 0.29912007 0.30038702 0.32133928 0.35764796 0.38537368 0.37951779 0.34283724 0.287328 0.23537883 0.21427295 0.2205352 0.240256][0.37594867 0.37267447 0.34724936 0.32846743 0.326983 0.34649408 0.38111073 0.40821171 0.40480366 0.37119982 0.31913811 0.27030635 0.25440344 0.26851735 0.29602098][0.42121738 0.41174865 0.37733349 0.35222495 0.3467077 0.36190885 0.38817209 0.40756944 0.40360111 0.37435454 0.33002082 0.29002979 0.28567183 0.3133401 0.35142553][0.44887915 0.43380657 0.39004019 0.35905331 0.35044184 0.36172041 0.3796849 0.39243567 0.39055356 0.36969978 0.33663961 0.30701691 0.31300646 0.35037959 0.39416072][0.43117821 0.41752121 0.37158456 0.33922744 0.32968235 0.33858088 0.35155374 0.36160865 0.36433956 0.35273254 0.33013612 0.30825996 0.31807995 0.35549843 0.3964124][0.3614848 0.35471281 0.31292635 0.28122333 0.26968646 0.27548471 0.28623655 0.29681358 0.30284724 0.29474011 0.27478427 0.25427336 0.26098666 0.29114205 0.32484481][0.25939777 0.26005739 0.22468168 0.19304216 0.1767174 0.17713603 0.18573941 0.1967877 0.20241749 0.19130823 0.1682229 0.14763999 0.15358797 0.18102024 0.21311654][0.14318559 0.14633636 0.11806338 0.0892579 0.07134556 0.068403043 0.075602785 0.086184688 0.089978583 0.075676911 0.05100394 0.032886717 0.04166951 0.071318045 0.10611915][0.040379196 0.04092313 0.019459061 -0.0025971376 -0.016727833 -0.019364681 -0.012863725 -0.0032440978 0.00046531585 -0.010852543 -0.029465992 -0.040504232 -0.028748594 0.00047384598 0.033690754]]...]
INFO - root - 2017-12-10 10:41:54.514302: step 7510, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.813 sec/batch; 73h:24m:53s remains)
INFO - root - 2017-12-10 10:42:02.315471: step 7520, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 70h:20m:59s remains)
INFO - root - 2017-12-10 10:42:10.231255: step 7530, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 70h:54m:14s remains)
INFO - root - 2017-12-10 10:42:18.017222: step 7540, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 69h:42m:43s remains)
INFO - root - 2017-12-10 10:42:25.607764: step 7550, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 69h:37m:24s remains)
INFO - root - 2017-12-10 10:42:33.454925: step 7560, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 71h:46m:06s remains)
INFO - root - 2017-12-10 10:42:41.307098: step 7570, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.785 sec/batch; 70h:49m:55s remains)
INFO - root - 2017-12-10 10:42:48.961019: step 7580, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 69h:53m:22s remains)
INFO - root - 2017-12-10 10:42:56.793781: step 7590, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 69h:48m:27s remains)
INFO - root - 2017-12-10 10:43:04.576598: step 7600, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 70h:16m:31s remains)
2017-12-10 10:43:05.379996: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.038189698 0.10002475 0.17507784 0.24366651 0.29029432 0.30969715 0.299147 0.26739433 0.23163924 0.20735441 0.20174153 0.21172258 0.23329423 0.26219651 0.29756069][0.038318437 0.10356408 0.18471345 0.26236323 0.32093665 0.35360596 0.35585132 0.33568507 0.30965632 0.29096958 0.28312385 0.28350368 0.29213813 0.31093526 0.34163204][0.033954285 0.096921161 0.17785029 0.25931805 0.32605317 0.36999512 0.38529035 0.37744442 0.36131242 0.34779036 0.33759195 0.3282803 0.32274452 0.32963347 0.35440567][0.030077165 0.089952171 0.16978458 0.2538209 0.327105 0.38133878 0.40970191 0.41258693 0.40290776 0.39134479 0.37813181 0.35888702 0.33520296 0.3221803 0.33149365][0.026456498 0.085003093 0.16612439 0.25522566 0.33702981 0.40291098 0.44513661 0.45693219 0.44944143 0.43544033 0.41723496 0.38604 0.33818698 0.29424691 0.2746304][0.024051148 0.084322512 0.17032832 0.26848805 0.36208206 0.44094858 0.49622154 0.51382756 0.5046187 0.48502123 0.46055362 0.41604516 0.3405287 0.25980452 0.20319697][0.024595339 0.087128095 0.17763209 0.28373989 0.38719252 0.47568959 0.53927439 0.55913454 0.54771429 0.52455723 0.49710664 0.44258231 0.34358159 0.23103771 0.14081854][0.024719063 0.085486539 0.1732461 0.27748653 0.38013926 0.46812183 0.530544 0.54819614 0.53543013 0.51338196 0.48960277 0.43501928 0.32824489 0.20258637 0.096604079][0.018905625 0.073022395 0.15043774 0.24186344 0.33092019 0.40604219 0.45679811 0.46716806 0.45187387 0.43147936 0.41336203 0.36724696 0.27093282 0.15585251 0.057855893][0.0060125352 0.05116199 0.11494128 0.18756065 0.25439021 0.30697247 0.33746126 0.33545443 0.3146024 0.29321724 0.27870595 0.24499039 0.17215049 0.085784242 0.014211915][-0.0087732952 0.028541218 0.080856025 0.13649374 0.18152077 0.21023545 0.21857931 0.20219952 0.17364003 0.14850859 0.13449571 0.11251653 0.066594087 0.014748387 -0.024248807][-0.019384393 0.013169194 0.059165165 0.10472481 0.13543721 0.14664109 0.13760376 0.10918566 0.073703066 0.044569287 0.02935697 0.015618552 -0.008307253 -0.031641215 -0.043815985][-0.026848661 0.0017790402 0.043610785 0.084223896 0.1087347 0.11175089 0.094032824 0.059984628 0.0220214 -0.0080106342 -0.023414979 -0.032122687 -0.042232528 -0.048316102 -0.044957355][-0.03235174 -0.0080324337 0.029576298 0.0670782 0.089974582 0.091333158 0.071600541 0.037061349 0.00038893917 -0.027574241 -0.041547373 -0.047227982 -0.050532375 -0.049208827 -0.040507365][-0.03719414 -0.017522842 0.015067707 0.048936125 0.071069248 0.073420763 0.055420823 0.023973985 -0.0084506785 -0.031963795 -0.042929333 -0.046276987 -0.046898331 -0.044184078 -0.036601335]]...]
INFO - root - 2017-12-10 10:43:13.283841: step 7610, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 72h:06m:53s remains)
INFO - root - 2017-12-10 10:43:21.114514: step 7620, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 70h:18m:16s remains)
INFO - root - 2017-12-10 10:43:28.904954: step 7630, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 70h:36m:47s remains)
INFO - root - 2017-12-10 10:43:36.791666: step 7640, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 69h:10m:50s remains)
INFO - root - 2017-12-10 10:43:44.658917: step 7650, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.817 sec/batch; 73h:41m:07s remains)
INFO - root - 2017-12-10 10:43:52.378046: step 7660, loss = 0.70, batch loss = 0.65 (13.1 examples/sec; 0.612 sec/batch; 55h:11m:01s remains)
INFO - root - 2017-12-10 10:44:00.206919: step 7670, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.816 sec/batch; 73h:38m:28s remains)
INFO - root - 2017-12-10 10:44:08.222212: step 7680, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 70h:26m:06s remains)
INFO - root - 2017-12-10 10:44:16.024572: step 7690, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.752 sec/batch; 67h:50m:07s remains)
INFO - root - 2017-12-10 10:44:23.728596: step 7700, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.768 sec/batch; 69h:19m:46s remains)
2017-12-10 10:44:24.573234: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.193618 0.17408927 0.17401731 0.19441237 0.22661409 0.25491595 0.268897 0.26384327 0.23913731 0.20454584 0.17455451 0.16228096 0.17016618 0.19069675 0.21399219][0.19404671 0.18347013 0.19182824 0.21728978 0.24957535 0.27349621 0.28052121 0.26806912 0.23770744 0.19965027 0.17004667 0.16300648 0.17866221 0.20542821 0.23044932][0.1777859 0.17637065 0.19064799 0.21774246 0.24654004 0.26371077 0.26298755 0.24447764 0.2118957 0.1753521 0.15058072 0.15105286 0.17375326 0.20351434 0.22623248][0.16208963 0.16657151 0.18219517 0.20725095 0.23114391 0.24242191 0.23621075 0.21469773 0.18363358 0.15239465 0.134038 0.13980179 0.16479249 0.19328792 0.21137854][0.16365971 0.17096102 0.18393864 0.20449674 0.22356109 0.23114464 0.22226977 0.20008504 0.17198256 0.14640264 0.13270685 0.13829331 0.15795688 0.179567 0.19154285][0.17555633 0.18357222 0.19372444 0.21173161 0.22980872 0.23842326 0.23076771 0.20905988 0.182039 0.15814191 0.14387153 0.14262669 0.15049143 0.16117942 0.1668456][0.18004629 0.1891143 0.2000792 0.22020441 0.24205671 0.25534296 0.2510573 0.2296121 0.20034239 0.1726467 0.15248407 0.14155686 0.13714238 0.13813752 0.14008354][0.17326686 0.18531756 0.20172121 0.22792493 0.25555268 0.27327237 0.27075306 0.24754997 0.21310486 0.17836155 0.15068012 0.13168134 0.11935076 0.11503877 0.11684842][0.15823235 0.17433617 0.19843321 0.23180099 0.26415694 0.28385305 0.28107733 0.25553232 0.21652585 0.17631407 0.14412786 0.12150925 0.10614684 0.10008826 0.10321902][0.13945472 0.15993965 0.19158719 0.23095606 0.26575881 0.28455889 0.27944756 0.25134957 0.20941682 0.16624983 0.13275848 0.11063325 0.096595831 0.0917499 0.097296476][0.11909141 0.14230867 0.17903763 0.22183552 0.25724462 0.27448121 0.26812917 0.24049062 0.19957249 0.156989 0.12461039 0.1047904 0.09351255 0.090841375 0.09947896][0.10570525 0.12878537 0.1661493 0.20803821 0.2409621 0.25668383 0.25277236 0.23193775 0.19911359 0.16351084 0.13652591 0.12031416 0.11030234 0.10681521 0.1155165][0.10374596 0.12262581 0.15588313 0.19284216 0.22111446 0.23627198 0.23926029 0.23119724 0.21173969 0.18648812 0.16566178 0.15095167 0.13811411 0.12978077 0.13584016][0.1049215 0.11804197 0.14524348 0.17699714 0.20255555 0.22042814 0.23407269 0.24169022 0.23748563 0.22258379 0.20567545 0.18912525 0.17052601 0.15562488 0.15773906][0.098653048 0.1077202 0.13158354 0.1619558 0.18871103 0.21155898 0.23505051 0.25484672 0.26121241 0.25174561 0.2342696 0.21326947 0.18915266 0.16964389 0.16889213]]...]
INFO - root - 2017-12-10 10:44:32.248019: step 7710, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 70h:12m:27s remains)
INFO - root - 2017-12-10 10:44:40.112314: step 7720, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.776 sec/batch; 70h:00m:46s remains)
INFO - root - 2017-12-10 10:44:47.933785: step 7730, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 70h:09m:42s remains)
INFO - root - 2017-12-10 10:44:55.803862: step 7740, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 71h:01m:03s remains)
INFO - root - 2017-12-10 10:45:03.463697: step 7750, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 71h:29m:48s remains)
INFO - root - 2017-12-10 10:45:11.404013: step 7760, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.757 sec/batch; 68h:19m:18s remains)
INFO - root - 2017-12-10 10:45:19.240627: step 7770, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 69h:29m:03s remains)
INFO - root - 2017-12-10 10:45:27.059662: step 7780, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 71h:42m:42s remains)
INFO - root - 2017-12-10 10:45:34.730920: step 7790, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 70h:08m:58s remains)
INFO - root - 2017-12-10 10:45:42.587039: step 7800, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.789 sec/batch; 71h:12m:19s remains)
2017-12-10 10:45:43.434316: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33283344 0.36985815 0.365852 0.33470389 0.28632882 0.22364646 0.1598082 0.10784076 0.0749521 0.056964997 0.045595698 0.041043606 0.047475915 0.067699082 0.10301343][0.36551905 0.41059455 0.40775812 0.37143591 0.3138766 0.24116211 0.17072898 0.11600887 0.08294946 0.065853067 0.055368271 0.051488232 0.0576797 0.077057257 0.111439][0.3800306 0.43437716 0.43489054 0.39467311 0.32801595 0.24680156 0.17406812 0.12351289 0.09773618 0.086586691 0.078803077 0.074151039 0.076309778 0.0898155 0.11771861][0.38689613 0.45255336 0.45985514 0.41823715 0.34383628 0.25593477 0.18461995 0.14396088 0.13157053 0.12970532 0.12399239 0.11443932 0.10701679 0.10927529 0.1259][0.39640236 0.47045597 0.48433325 0.44369733 0.36643559 0.27772155 0.21302807 0.18590204 0.18858686 0.19557823 0.18970281 0.17162496 0.15102077 0.1396468 0.14495136][0.41020119 0.48287448 0.49758065 0.45900759 0.38562757 0.30414003 0.25071126 0.23717448 0.2525883 0.26661146 0.26006469 0.23380403 0.20057869 0.17694263 0.17452988][0.41436055 0.47799739 0.48996848 0.45628917 0.39393529 0.3257021 0.28347242 0.2778118 0.29899222 0.31659645 0.31071907 0.28070232 0.24000688 0.20844935 0.20230988][0.39023536 0.43906605 0.44657049 0.42078829 0.37548208 0.32621413 0.29638639 0.29467586 0.31602162 0.33469152 0.33130717 0.302824 0.26113728 0.22689289 0.21957399][0.33358768 0.36373627 0.36383259 0.34457037 0.3168771 0.28882608 0.2731142 0.27433014 0.29253665 0.31029987 0.31094185 0.28909329 0.25318927 0.22175011 0.21507008][0.25163624 0.26184249 0.25231829 0.23575605 0.22144806 0.21149951 0.20811586 0.21076272 0.22350764 0.23848316 0.24379501 0.23230369 0.20764466 0.18391933 0.17964937][0.15092549 0.14632481 0.13065669 0.11646076 0.11138902 0.1135589 0.11785377 0.12006371 0.12688906 0.13820586 0.14701359 0.14513458 0.13174802 0.11597327 0.11288814][0.046784692 0.036663011 0.023525858 0.015660627 0.017623097 0.025459183 0.031728055 0.031862348 0.033672303 0.040972605 0.050021634 0.053027283 0.046726894 0.036092758 0.031962611][-0.033317059 -0.042982046 -0.049608074 -0.050427377 -0.04434384 -0.035537127 -0.030135605 -0.031131921 -0.031484149 -0.027008424 -0.020007759 -0.016621385 -0.020236408 -0.028329156 -0.034002177][-0.07650163 -0.0848417 -0.087115638 -0.084538914 -0.077943675 -0.070886508 -0.067219809 -0.068192869 -0.068730138 -0.065988563 -0.061654255 -0.059791889 -0.0627177 -0.069185786 -0.075341746][-0.09252 -0.099386126 -0.099132843 -0.095322378 -0.089346915 -0.084032863 -0.081637926 -0.082326956 -0.082890019 -0.081727706 -0.079994969 -0.079990007 -0.083009593 -0.088499241 -0.094658174]]...]
INFO - root - 2017-12-10 10:45:51.306821: step 7810, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 70h:03m:47s remains)
INFO - root - 2017-12-10 10:45:59.180543: step 7820, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 68h:33m:34s remains)
INFO - root - 2017-12-10 10:46:07.043263: step 7830, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 71h:26m:44s remains)
INFO - root - 2017-12-10 10:46:14.712122: step 7840, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.758 sec/batch; 68h:22m:54s remains)
INFO - root - 2017-12-10 10:46:22.534103: step 7850, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 70h:15m:26s remains)
INFO - root - 2017-12-10 10:46:30.378114: step 7860, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 72h:05m:26s remains)
INFO - root - 2017-12-10 10:46:37.905253: step 7870, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 69h:30m:07s remains)
INFO - root - 2017-12-10 10:46:45.708430: step 7880, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 70h:13m:47s remains)
INFO - root - 2017-12-10 10:46:53.588814: step 7890, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.758 sec/batch; 68h:22m:43s remains)
INFO - root - 2017-12-10 10:47:01.302167: step 7900, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 69h:19m:38s remains)
2017-12-10 10:47:02.132067: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.06105262 0.068450183 0.0669636 0.058085449 0.045689341 0.034441385 0.030529238 0.032461762 0.036601465 0.039035097 0.037745569 0.034003116 0.030937433 0.031725593 0.0350701][0.1144947 0.12813304 0.12843437 0.11575241 0.09631554 0.078436784 0.072338276 0.0757825 0.082450919 0.086183563 0.083924994 0.077646978 0.072868332 0.074124694 0.079019293][0.16474873 0.18327178 0.18370697 0.16661848 0.14133348 0.11956768 0.11400857 0.12101285 0.13115823 0.13631968 0.13245539 0.12253213 0.11435576 0.11477037 0.12072007][0.19545238 0.21561012 0.21451375 0.19448976 0.16784851 0.14842744 0.14882047 0.16294935 0.17805803 0.18440495 0.17758551 0.16141659 0.14638737 0.14267597 0.14743996][0.19740468 0.21512517 0.21215825 0.19326335 0.17209904 0.16249178 0.17431025 0.19804104 0.2175988 0.22283207 0.21019825 0.18441489 0.15838654 0.1462553 0.14692172][0.17787138 0.19023116 0.18566816 0.17172028 0.1618126 0.16775824 0.19425398 0.2280063 0.2500481 0.25090355 0.22915031 0.19092421 0.15165086 0.12875617 0.12371473][0.1488001 0.15569608 0.15086937 0.14450157 0.14844179 0.17144111 0.21200341 0.25381508 0.27598575 0.270557 0.23892951 0.18921381 0.13898644 0.10690658 0.096691087][0.12638479 0.13100222 0.12794939 0.12885042 0.14362766 0.17857547 0.2274155 0.27222055 0.29183692 0.28075 0.24348076 0.18952738 0.13699481 0.10219144 0.089532875][0.1195841 0.12548219 0.12577394 0.1315936 0.15103039 0.18932141 0.2383676 0.28035963 0.29604506 0.28303829 0.24884291 0.20196949 0.15786763 0.12771052 0.11508727][0.12904739 0.13867636 0.14221437 0.14904585 0.16583391 0.19838196 0.23997083 0.27505994 0.28798342 0.27957192 0.25858882 0.2297163 0.2023336 0.18149 0.16958822][0.14829771 0.16297843 0.1688493 0.1730838 0.18161449 0.20217115 0.23150492 0.25824 0.27132827 0.27362508 0.27260724 0.2665123 0.25760219 0.24621058 0.23391697][0.16080032 0.17927603 0.18619767 0.18632422 0.18539543 0.19315177 0.21069959 0.23095846 0.24699588 0.2621733 0.28042015 0.29348737 0.29801396 0.29272923 0.27882838][0.15506278 0.17425498 0.18054414 0.17686695 0.16887833 0.16739447 0.17687407 0.19317597 0.21208297 0.23583528 0.26475579 0.28673542 0.29626626 0.29292664 0.27753827][0.12540802 0.14150733 0.14582407 0.14018482 0.12974469 0.12487219 0.1312978 0.14593714 0.16556253 0.19070856 0.21942224 0.23961779 0.24657582 0.24194641 0.22611804][0.0768286 0.086795807 0.088211931 0.082335688 0.073486619 0.070130296 0.077075258 0.091015019 0.10887703 0.12966374 0.15078178 0.16301817 0.16411078 0.15740131 0.1426985]]...]
INFO - root - 2017-12-10 10:47:09.883930: step 7910, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 70h:05m:03s remains)
INFO - root - 2017-12-10 10:47:17.665121: step 7920, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 71h:17m:00s remains)
INFO - root - 2017-12-10 10:47:25.482452: step 7930, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 70h:19m:54s remains)
INFO - root - 2017-12-10 10:47:33.324073: step 7940, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 71h:31m:22s remains)
INFO - root - 2017-12-10 10:47:40.958582: step 7950, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.787 sec/batch; 70h:58m:52s remains)
INFO - root - 2017-12-10 10:47:48.900369: step 7960, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.776 sec/batch; 69h:56m:55s remains)
INFO - root - 2017-12-10 10:47:56.677468: step 7970, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 68h:38m:58s remains)
INFO - root - 2017-12-10 10:48:04.499996: step 7980, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 71h:52m:32s remains)
INFO - root - 2017-12-10 10:48:12.294658: step 7990, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.801 sec/batch; 72h:12m:20s remains)
INFO - root - 2017-12-10 10:48:20.159879: step 8000, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 68h:32m:03s remains)
2017-12-10 10:48:20.955996: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.025809426 -0.0047633671 -0.024079584 -0.029042097 -0.020842347 -0.00065624242 0.027743233 0.0661695 0.11610276 0.17226148 0.22747901 0.27629623 0.31210804 0.32770455 0.31776688][0.06397634 0.024021123 -0.0037251969 -0.013113618 -0.0056364671 0.016965779 0.049336612 0.09171316 0.14464289 0.20290534 0.25876278 0.30461085 0.3342714 0.3420462 0.32560146][0.10378405 0.056770105 0.021292435 0.0074780239 0.014543572 0.039979741 0.076613992 0.12190139 0.17397431 0.22761036 0.27556035 0.30984327 0.32689694 0.32447866 0.30435696][0.14177316 0.0915768 0.050969623 0.03481194 0.044199109 0.075413227 0.11836809 0.16633183 0.21412238 0.2570245 0.28990492 0.30662268 0.30795988 0.2964876 0.27746159][0.17577468 0.12590633 0.083339676 0.067626484 0.082899891 0.12423688 0.17725156 0.22950621 0.27144563 0.299255 0.31110039 0.30525935 0.28849357 0.26864478 0.25344437][0.2052381 0.15732646 0.1143173 0.10009307 0.12324266 0.17830981 0.24545825 0.30509144 0.34235987 0.35352451 0.3408747 0.30959919 0.27394703 0.24637508 0.23509261][0.23456149 0.18850163 0.14416908 0.12899365 0.15651198 0.2220825 0.301563 0.3689687 0.40420344 0.40229097 0.36880663 0.31617966 0.2663838 0.23536588 0.22945169][0.26176456 0.21613777 0.16841879 0.14821242 0.17320089 0.24048038 0.32373539 0.39399329 0.42811781 0.41944697 0.3746694 0.31191865 0.25902793 0.23347089 0.23762608][0.27762172 0.23191032 0.17950459 0.15134811 0.16727588 0.22626883 0.30235285 0.36744103 0.39896005 0.38942763 0.34489223 0.28619373 0.24351159 0.23294145 0.25202706][0.26662704 0.22276486 0.16794731 0.13268016 0.13668226 0.18050796 0.24092063 0.29369888 0.319915 0.31274098 0.27729952 0.23401986 0.21128452 0.22148699 0.25787443][0.22126 0.18343376 0.1319568 0.094688252 0.088973105 0.11649671 0.15844595 0.1958572 0.21473759 0.21020024 0.18706407 0.16327704 0.16222931 0.19203073 0.24329311][0.14753787 0.11863007 0.076913044 0.044794016 0.03511712 0.049900185 0.075389966 0.0983132 0.1096377 0.1071905 0.0959632 0.090143368 0.10608603 0.14788075 0.20583408][0.062588938 0.04340804 0.015835362 -0.0054352856 -0.012757879 -0.0045340233 0.010119426 0.022529092 0.02755536 0.025535842 0.021979738 0.027313635 0.051072665 0.094183877 0.14831065][-0.0090706674 -0.0195366 -0.032720294 -0.041527346 -0.042834558 -0.035780586 -0.026014661 -0.019341111 -0.018603513 -0.021939598 -0.023470795 -0.015709382 0.0060674506 0.0412259 0.0833492][-0.05186056 -0.056712296 -0.05936699 -0.058436625 -0.053853527 -0.045517817 -0.037046097 -0.032584872 -0.03396723 -0.038923737 -0.041997954 -0.037943561 -0.024493355 -0.0022653104 0.024367932]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 10:48:28.900696: step 8010, loss = 0.71, batch loss = 0.66 (10.2 examples/sec; 0.782 sec/batch; 70h:28m:52s remains)
INFO - root - 2017-12-10 10:48:36.516956: step 8020, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 70h:00m:15s remains)
INFO - root - 2017-12-10 10:48:44.210923: step 8030, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 70h:37m:29s remains)
INFO - root - 2017-12-10 10:48:52.040591: step 8040, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 68h:50m:03s remains)
INFO - root - 2017-12-10 10:48:59.841351: step 8050, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 72h:44m:39s remains)
INFO - root - 2017-12-10 10:49:07.702733: step 8060, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.811 sec/batch; 73h:05m:58s remains)
INFO - root - 2017-12-10 10:49:15.574095: step 8070, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 69h:43m:02s remains)
INFO - root - 2017-12-10 10:49:23.408564: step 8080, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 69h:22m:27s remains)
INFO - root - 2017-12-10 10:49:31.395912: step 8090, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 71h:47m:40s remains)
INFO - root - 2017-12-10 10:49:39.275984: step 8100, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 70h:55m:42s remains)
2017-12-10 10:49:40.088904: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12272855 0.126127 0.11851191 0.10712697 0.097563244 0.089467339 0.084807806 0.085379124 0.092632242 0.10385446 0.11511004 0.12079748 0.11355007 0.0911426 0.056717128][0.15396571 0.16105 0.15320344 0.13924886 0.12700327 0.11697924 0.11050685 0.10846179 0.11123373 0.11681083 0.12200994 0.12242002 0.11191504 0.08835239 0.054305736][0.19223672 0.20124324 0.19237092 0.17637596 0.162892 0.15257496 0.14521568 0.14023781 0.13713795 0.13517109 0.13243873 0.12622692 0.11177678 0.087254763 0.054198153][0.23326951 0.24027495 0.22851703 0.21066408 0.19774245 0.18995796 0.18427809 0.17811388 0.17021218 0.16144793 0.15128919 0.13830258 0.11894897 0.091988616 0.058248654][0.26788649 0.26988977 0.25384665 0.23391441 0.22200902 0.21810821 0.21631743 0.21213032 0.20307247 0.19116399 0.1769654 0.15900663 0.13473091 0.10376186 0.067301117][0.2899906 0.28801459 0.26989663 0.24979064 0.24027258 0.24163909 0.24574907 0.24626157 0.23932755 0.22759275 0.21186452 0.19000065 0.1600261 0.12289199 0.080832124][0.29250544 0.29052332 0.27573767 0.26068193 0.25747749 0.26662296 0.27846193 0.28494367 0.2812745 0.27051339 0.253687 0.22793022 0.19141595 0.14639997 0.096389621][0.27467594 0.27506205 0.26734531 0.26117623 0.26694614 0.28457686 0.30343425 0.31463537 0.31377247 0.30383468 0.28640535 0.25782016 0.2160012 0.16399662 0.10622623][0.23997718 0.24336068 0.24342601 0.24652024 0.26029307 0.28353697 0.30564561 0.31867397 0.3199226 0.31181782 0.29594851 0.26767656 0.22474328 0.16992056 0.1080067][0.19856116 0.20400411 0.21016374 0.22055842 0.23945892 0.2638385 0.28413936 0.294879 0.29634139 0.29055575 0.27808765 0.25340739 0.21401946 0.16166969 0.10065667][0.16009513 0.16586225 0.17538464 0.19042008 0.21179125 0.23391542 0.24861458 0.25335324 0.25243109 0.24843581 0.24035779 0.22184412 0.18932286 0.14331779 0.0869741][0.13052014 0.1347575 0.14426386 0.16010515 0.18038151 0.19750941 0.20472932 0.20212722 0.19788302 0.19592156 0.19359989 0.18328682 0.15965946 0.12193577 0.072131358][0.11172679 0.11195232 0.1167466 0.12758857 0.14183472 0.15200418 0.152494 0.14487311 0.13937937 0.14131103 0.14630021 0.1452655 0.13104722 0.10204358 0.059500448][0.10612209 0.10060441 0.097488388 0.099487416 0.10476195 0.10773791 0.10400821 0.095090687 0.090695344 0.097024113 0.10840384 0.11495744 0.10845399 0.086989686 0.051629443][0.11296351 0.10260829 0.092810079 0.0875496 0.085957095 0.084449984 0.079546638 0.07172858 0.068646625 0.076720752 0.090097591 0.099270187 0.09597937 0.078628235 0.048558611]]...]
INFO - root - 2017-12-10 10:49:47.583924: step 8110, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 70h:04m:04s remains)
INFO - root - 2017-12-10 10:49:55.431544: step 8120, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 70h:57m:58s remains)
INFO - root - 2017-12-10 10:50:03.279698: step 8130, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 70h:11m:31s remains)
INFO - root - 2017-12-10 10:50:11.114632: step 8140, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 70h:34m:50s remains)
INFO - root - 2017-12-10 10:50:18.928074: step 8150, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 69h:17m:41s remains)
INFO - root - 2017-12-10 10:50:26.700541: step 8160, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.768 sec/batch; 69h:11m:29s remains)
INFO - root - 2017-12-10 10:50:34.511765: step 8170, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 70h:13m:57s remains)
INFO - root - 2017-12-10 10:50:42.326289: step 8180, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 70h:34m:37s remains)
INFO - root - 2017-12-10 10:50:49.977419: step 8190, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 70h:07m:44s remains)
INFO - root - 2017-12-10 10:50:57.565385: step 8200, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.769 sec/batch; 69h:18m:16s remains)
2017-12-10 10:50:58.349631: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054768 -0.021464139 0.028688604 0.095522463 0.1642777 0.21624519 0.23233615 0.21863727 0.19793522 0.17977349 0.16369739 0.14586402 0.12264011 0.092761345 0.058396816][-0.057634003 -0.025300363 0.025954872 0.095070273 0.16604407 0.21892539 0.2333675 0.21545312 0.18783911 0.16073383 0.13547774 0.1100694 0.082486548 0.051034383 0.017135343][-0.054686833 -0.0210516 0.034047768 0.10919617 0.1870302 0.24522565 0.2600328 0.23479789 0.1916364 0.14426652 0.099722371 0.060540225 0.027032264 -0.0034582482 -0.031579796][-0.050336648 -0.015696835 0.042699363 0.12407991 0.21064302 0.277992 0.29806817 0.26981318 0.21328875 0.14569727 0.08075653 0.025916398 -0.015549998 -0.046348032 -0.068859644][-0.046167586 -0.011212569 0.048578478 0.13437325 0.23052645 0.31227541 0.34652317 0.32579261 0.26555684 0.18472117 0.10238501 0.031428292 -0.020894162 -0.055628173 -0.076030768][-0.0423361 -0.0071731992 0.053191274 0.14366639 0.25365877 0.35910413 0.41977328 0.4183563 0.36375535 0.27518228 0.17680533 0.088098712 0.021471899 -0.022241982 -0.046588603][-0.039372478 -0.0041377489 0.056922629 0.15344191 0.2808843 0.41586071 0.50910586 0.53134888 0.48520976 0.39089867 0.27756622 0.17146772 0.090275332 0.036489505 0.0061095967][-0.038879946 -0.0049387822 0.055611093 0.15717059 0.2997908 0.46008402 0.58049005 0.62149793 0.58207238 0.48457235 0.36178133 0.24431126 0.15298429 0.09135443 0.055561405][-0.038147233 -0.0057881242 0.052528124 0.15374912 0.30005813 0.46846384 0.59817547 0.64575535 0.61027759 0.51566672 0.39431828 0.27645528 0.18308817 0.11855533 0.08017493][-0.035883546 -0.0037496188 0.051790826 0.14635225 0.28082594 0.4334726 0.548032 0.58622164 0.55122596 0.46632984 0.35874698 0.25335169 0.16866072 0.10940577 0.074265122][-0.035054889 -0.0026208193 0.050147943 0.13378526 0.24473636 0.36310419 0.44354561 0.45987466 0.42125404 0.34881103 0.26235709 0.17903714 0.1130431 0.068643421 0.044765819][-0.0375271 -0.0062844204 0.042612683 0.1129406 0.19625014 0.27514 0.31697458 0.30881879 0.26462197 0.20293002 0.13728145 0.078194261 0.035591591 0.011952076 0.0048206868][-0.040850174 -0.011778481 0.033311922 0.092158437 0.1532481 0.20153964 0.21471046 0.18930815 0.14148286 0.087328359 0.036697917 -0.0037630426 -0.027334817 -0.033899169 -0.027706899][-0.042038321 -0.014706126 0.027950695 0.0802242 0.13033518 0.16462742 0.16587636 0.1342207 0.085802175 0.035391349 -0.0081689479 -0.039154079 -0.052603334 -0.050782811 -0.039269939][-0.041710332 -0.01551905 0.025865745 0.076972552 0.12858634 0.16710962 0.17427729 0.14708015 0.099741951 0.047324374 0.00078602153 -0.031553984 -0.044382028 -0.041811034 -0.030694552]]...]
INFO - root - 2017-12-10 10:51:06.149509: step 8210, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.773 sec/batch; 69h:36m:37s remains)
INFO - root - 2017-12-10 10:51:14.003884: step 8220, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 70h:06m:32s remains)
INFO - root - 2017-12-10 10:51:21.699537: step 8230, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 71h:57m:19s remains)
INFO - root - 2017-12-10 10:51:29.520056: step 8240, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 69h:13m:24s remains)
INFO - root - 2017-12-10 10:51:37.342543: step 8250, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 69h:56m:15s remains)
INFO - root - 2017-12-10 10:51:45.111058: step 8260, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 70h:41m:47s remains)
INFO - root - 2017-12-10 10:51:52.878889: step 8270, loss = 0.72, batch loss = 0.66 (10.8 examples/sec; 0.740 sec/batch; 66h:38m:12s remains)
INFO - root - 2017-12-10 10:52:00.691492: step 8280, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 72h:42m:30s remains)
INFO - root - 2017-12-10 10:52:08.336763: step 8290, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 70h:20m:19s remains)
INFO - root - 2017-12-10 10:52:16.213309: step 8300, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 71h:53m:29s remains)
2017-12-10 10:52:17.073492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040975742 -0.04082023 -0.040698785 -0.042470232 -0.044749819 -0.045938212 -0.044960883 -0.042076591 -0.038871851 -0.036684275 -0.03620765 -0.03810763 -0.042857938 -0.050170954 -0.058193088][-0.019161046 -0.013913113 -0.010971007 -0.012140988 -0.015022332 -0.016326647 -0.013633101 -0.0069150208 0.00071903376 0.0066326242 0.0090853358 0.0060556582 -0.0042349882 -0.021652844 -0.041280504][0.020664359 0.036760572 0.046985291 0.049237803 0.046932671 0.045600072 0.050262015 0.061886333 0.076029807 0.0873958 0.0920386 0.085840836 0.064949967 0.03106848 -0.0066920645][0.071870409 0.10306358 0.12420022 0.13235769 0.13171549 0.13033241 0.13681978 0.15383323 0.17585278 0.19363163 0.19996764 0.18758321 0.15097341 0.094934762 0.034746744][0.12401918 0.17317612 0.20913813 0.22782651 0.23299707 0.23377737 0.24159855 0.26162258 0.2885437 0.30967104 0.31447732 0.29293862 0.23852916 0.15958188 0.077104375][0.17680441 0.24477385 0.297101 0.32832038 0.34088221 0.34398043 0.3505291 0.3678022 0.39166135 0.40914974 0.40847939 0.37846595 0.3116582 0.21692465 0.11787372][0.22268619 0.30552822 0.37076607 0.41199684 0.43062362 0.43524686 0.43846169 0.44782546 0.46036452 0.46667793 0.45700654 0.42128134 0.35125524 0.2526139 0.14739743][0.24903959 0.33981118 0.41179857 0.45832789 0.48006693 0.48479661 0.48373896 0.48295608 0.48101059 0.47365165 0.45505515 0.41743007 0.35187593 0.2598528 0.15873849][0.24784981 0.3369492 0.40750805 0.45294216 0.47348616 0.47613552 0.46971104 0.45931062 0.44536507 0.42891562 0.40716171 0.37397951 0.32024038 0.2429356 0.15374897][0.21851918 0.29663962 0.3583492 0.39708444 0.41256815 0.41066238 0.39785257 0.37968794 0.35916993 0.34111089 0.323753 0.30147737 0.26520282 0.20766075 0.13506061][0.16745004 0.22758551 0.27466214 0.30202919 0.3090288 0.30050775 0.28198218 0.25977126 0.23863026 0.22512962 0.21738349 0.20939918 0.19202019 0.1551391 0.10107864][0.1001372 0.1385081 0.16817932 0.18318944 0.18284416 0.17078158 0.15151866 0.13097058 0.11429331 0.10779921 0.10915434 0.11222748 0.10893386 0.089017108 0.052852549][0.033826508 0.052664187 0.066723347 0.071457215 0.066539675 0.054631308 0.039089836 0.024563156 0.015640067 0.016644571 0.024641965 0.033806741 0.037819445 0.028745232 0.0064391312][-0.017941982 -0.013232088 -0.0105094 -0.013093367 -0.020403007 -0.030375017 -0.040673442 -0.047887467 -0.048904397 -0.041961387 -0.030434094 -0.019311273 -0.012935663 -0.016223539 -0.0289945][-0.053959496 -0.057393704 -0.060444716 -0.066006154 -0.073145218 -0.080518775 -0.086541176 -0.088843592 -0.085770831 -0.077302761 -0.066331945 -0.056385379 -0.050084919 -0.050033618 -0.055582289]]...]
INFO - root - 2017-12-10 10:52:24.873194: step 8310, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 68h:54m:31s remains)
INFO - root - 2017-12-10 10:52:32.740540: step 8320, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.793 sec/batch; 71h:22m:42s remains)
INFO - root - 2017-12-10 10:52:40.574447: step 8330, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 69h:45m:48s remains)
INFO - root - 2017-12-10 10:52:48.411831: step 8340, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 68h:53m:50s remains)
INFO - root - 2017-12-10 10:52:56.066472: step 8350, loss = 0.73, batch loss = 0.67 (10.3 examples/sec; 0.779 sec/batch; 70h:10m:00s remains)
INFO - root - 2017-12-10 10:53:03.905240: step 8360, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 70h:11m:34s remains)
INFO - root - 2017-12-10 10:53:11.920008: step 8370, loss = 0.71, batch loss = 0.65 (8.7 examples/sec; 0.915 sec/batch; 82h:24m:10s remains)
INFO - root - 2017-12-10 10:53:19.768175: step 8380, loss = 0.71, batch loss = 0.66 (9.8 examples/sec; 0.820 sec/batch; 73h:49m:24s remains)
INFO - root - 2017-12-10 10:53:27.646718: step 8390, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 71h:03m:49s remains)
INFO - root - 2017-12-10 10:53:35.559331: step 8400, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 70h:38m:37s remains)
2017-12-10 10:53:36.349649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.013620957 0.022013389 0.090639405 0.18681061 0.29079342 0.37548786 0.41265279 0.39001203 0.31714818 0.21803211 0.12105413 0.049651176 0.015030549 0.0086348886 0.016225327][-0.029253343 0.0048277169 0.069231741 0.16044076 0.26202667 0.34997278 0.39745903 0.39206308 0.33912787 0.25461814 0.16141136 0.082540378 0.033148181 0.009844033 0.0028832857][-0.047621187 -0.0161111 0.042485632 0.12683961 0.22486414 0.31654721 0.37714547 0.39337507 0.36557949 0.30097476 0.21488914 0.12805289 0.059016164 0.012158715 -0.015433587][-0.062200479 -0.03268088 0.023043077 0.10516858 0.20582323 0.3081401 0.3876904 0.42890817 0.4267675 0.38019809 0.2976518 0.19759555 0.10280644 0.02623079 -0.026954774][-0.069432788 -0.040100809 0.016632553 0.10200141 0.21190751 0.33100089 0.4333818 0.49987721 0.519881 0.48584411 0.40111163 0.28367612 0.16106807 0.054620508 -0.023623597][-0.0686725 -0.036637429 0.02466459 0.11740083 0.23984984 0.37681 0.500237 0.58669996 0.62038261 0.58963335 0.49684468 0.36165181 0.21553355 0.086188465 -0.0093072969][-0.061201356 -0.023522554 0.045326646 0.14769389 0.28176734 0.43116176 0.56621158 0.66082066 0.69615132 0.65975636 0.55681527 0.40955606 0.251217 0.11219154 0.011716844][-0.050487734 -0.0053701326 0.072334781 0.18368906 0.32457873 0.4763256 0.60894722 0.69639075 0.721183 0.67416263 0.56471115 0.4164606 0.26115337 0.12832984 0.035709582][-0.040371645 0.011224724 0.095256716 0.21009178 0.34811109 0.48839876 0.60223252 0.667351 0.67249709 0.61447072 0.50615638 0.37038106 0.23475605 0.12412531 0.052096866][-0.036541987 0.016548043 0.099098757 0.20648219 0.32820746 0.44297135 0.52557826 0.56029725 0.54440552 0.48092923 0.38386127 0.27307686 0.17052953 0.0944852 0.053762872][-0.042140696 0.0042720074 0.074807964 0.16256674 0.25627488 0.33677572 0.38439873 0.39036685 0.35946882 0.29904485 0.2221117 0.14448527 0.082289368 0.04718034 0.044035487][-0.055265158 -0.023073101 0.026655925 0.086273074 0.14636515 0.19225736 0.21050858 0.19781293 0.16320099 0.11534107 0.064667664 0.023050642 0.0015503827 0.0065647205 0.039392393][-0.070070066 -0.054641273 -0.028026477 0.0028421727 0.03214452 0.050789181 0.050831661 0.032109603 0.0042570126 -0.024935784 -0.047982961 -0.0565048 -0.043460406 -0.0062895855 0.053004496][-0.08114256 -0.08001443 -0.072487935 -0.064085774 -0.056944743 -0.055121716 -0.062258206 -0.077113464 -0.091838457 -0.1004144 -0.097828947 -0.07882674 -0.03996402 0.018226335 0.091165774][-0.087353811 -0.095848106 -0.10062597 -0.10582641 -0.11072309 -0.11553665 -0.12054917 -0.12396567 -0.12093039 -0.10776462 -0.081901632 -0.041276276 0.013684292 0.079992853 0.15237211]]...]
INFO - root - 2017-12-10 10:53:44.277120: step 8410, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 71h:26m:17s remains)
INFO - root - 2017-12-10 10:53:52.137698: step 8420, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 70h:04m:12s remains)
INFO - root - 2017-12-10 10:53:59.800621: step 8430, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 69h:17m:56s remains)
INFO - root - 2017-12-10 10:54:07.680065: step 8440, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 73h:15m:47s remains)
INFO - root - 2017-12-10 10:54:15.593267: step 8450, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.812 sec/batch; 73h:07m:43s remains)
INFO - root - 2017-12-10 10:54:23.370577: step 8460, loss = 0.70, batch loss = 0.64 (11.8 examples/sec; 0.680 sec/batch; 61h:10m:58s remains)
INFO - root - 2017-12-10 10:54:31.329963: step 8470, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 70h:02m:08s remains)
INFO - root - 2017-12-10 10:54:39.180356: step 8480, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 70h:27m:44s remains)
INFO - root - 2017-12-10 10:54:47.086323: step 8490, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 71h:56m:00s remains)
INFO - root - 2017-12-10 10:54:55.014258: step 8500, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.765 sec/batch; 68h:51m:33s remains)
2017-12-10 10:54:55.849845: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12069272 0.098086432 0.078875139 0.0674476 0.065252915 0.0750217 0.08543209 0.085626714 0.073810652 0.060442746 0.056763 0.061477073 0.071060367 0.084428772 0.0986641][0.03118448 0.010576432 -0.0055604079 -0.012898966 -0.00992786 0.0021211186 0.011879507 0.011983335 0.0018914815 -0.00964459 -0.013769645 -0.0061843512 0.011735169 0.0342249 0.054226916][-0.01230259 -0.024839578 -0.033328082 -0.033010408 -0.022340925 -0.0053455103 0.0080138016 0.012398508 0.0061147818 -0.0046754032 -0.013406873 -0.0095559079 0.0078851106 0.030386638 0.047722723][0.018361386 0.018553851 0.019663798 0.029066158 0.04820659 0.071849182 0.092410885 0.10440401 0.10172942 0.087249413 0.066832341 0.056963649 0.061242081 0.070052966 0.072866291][0.11991901 0.13305759 0.14224976 0.15971738 0.18609522 0.21545389 0.24329863 0.26340926 0.26249695 0.23911795 0.20015162 0.16732587 0.1466054 0.12829356 0.10543464][0.27104223 0.29220033 0.30381739 0.32435459 0.35298973 0.38292181 0.41263393 0.43628004 0.43347806 0.3979513 0.33813655 0.2791042 0.2290895 0.17875059 0.12684949][0.43184486 0.4553569 0.46339723 0.47997665 0.50299561 0.52551574 0.54852635 0.56836063 0.56008118 0.51343971 0.43785644 0.35871333 0.285762 0.21053633 0.13737167][0.5547483 0.57611763 0.57635182 0.58257991 0.59262729 0.60140443 0.61177361 0.62300628 0.60987312 0.55948609 0.4802765 0.39493766 0.31398153 0.23057805 0.15230624][0.59791231 0.61414975 0.60645294 0.60061067 0.59520191 0.58905107 0.58653373 0.58941132 0.57574928 0.53153825 0.46245131 0.38632637 0.31395727 0.24085441 0.17513673][0.53794748 0.54736304 0.53511876 0.52158946 0.50621873 0.49165866 0.48293093 0.48254228 0.47352022 0.44258931 0.39210027 0.33476776 0.28182155 0.23160709 0.19053428][0.39381576 0.39720398 0.38472965 0.37005365 0.35374704 0.34041792 0.3339524 0.33558881 0.33377486 0.31763667 0.28697866 0.25026843 0.21895717 0.19413239 0.17896555][0.21235774 0.2116078 0.20176721 0.19080877 0.18060093 0.17548166 0.17696941 0.184167 0.18999538 0.1864887 0.17216313 0.15246558 0.13826752 0.1325316 0.13530624][0.04746728 0.043934513 0.037448209 0.031753775 0.029273221 0.03289121 0.041970957 0.054306149 0.065789521 0.070680924 0.067121878 0.058794964 0.054868747 0.058797665 0.068831258][-0.062731162 -0.06779135 -0.071476243 -0.0732123 -0.0710508 -0.063639186 -0.052483249 -0.039590891 -0.027235646 -0.019391324 -0.017675862 -0.019672738 -0.018918559 -0.012705681 -0.0031353743][-0.11062486 -0.11609626 -0.11828965 -0.11855451 -0.11564527 -0.10922343 -0.1007804 -0.091496751 -0.082415715 -0.075877629 -0.073055536 -0.07256934 -0.070768863 -0.066111051 -0.060204998]]...]
INFO - root - 2017-12-10 10:55:03.549624: step 8510, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.746 sec/batch; 67h:10m:37s remains)
INFO - root - 2017-12-10 10:55:11.310082: step 8520, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 68h:59m:21s remains)
INFO - root - 2017-12-10 10:55:19.250071: step 8530, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.804 sec/batch; 72h:21m:18s remains)
INFO - root - 2017-12-10 10:55:27.292348: step 8540, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 72h:00m:31s remains)
INFO - root - 2017-12-10 10:55:35.023094: step 8550, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 71h:18m:18s remains)
INFO - root - 2017-12-10 10:55:42.946343: step 8560, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 71h:16m:41s remains)
INFO - root - 2017-12-10 10:55:50.813665: step 8570, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 72h:43m:54s remains)
INFO - root - 2017-12-10 10:55:58.633513: step 8580, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 71h:33m:54s remains)
INFO - root - 2017-12-10 10:56:06.284930: step 8590, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.757 sec/batch; 68h:07m:36s remains)
INFO - root - 2017-12-10 10:56:14.087168: step 8600, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 69h:14m:51s remains)
2017-12-10 10:56:14.925006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.02739168 -0.022558812 -0.016294125 -0.011931956 -0.011185063 -0.01235859 -0.011732319 -0.0066746334 0.0072633042 0.034818254 0.077004068 0.12265177 0.15676486 0.17179848 0.16842532][-0.013047165 -0.00053484726 0.01446519 0.026294226 0.031813141 0.033092294 0.034066007 0.035877131 0.042164385 0.059317529 0.088901751 0.120405 0.14040008 0.1451478 0.13729121][0.009862408 0.03563451 0.065994054 0.091159135 0.10598261 0.11363626 0.11781137 0.1160483 0.11018375 0.10988162 0.11905018 0.12699282 0.12240508 0.10775559 0.090249024][0.043006387 0.090402849 0.14617744 0.19331214 0.22377589 0.24370015 0.25629017 0.25169837 0.22929761 0.20499423 0.18835399 0.16682084 0.12944211 0.086497486 0.05272156][0.086786538 0.16390777 0.25407225 0.32992002 0.37971827 0.41481316 0.43872112 0.43050241 0.38651907 0.33279774 0.28937453 0.24005653 0.16919376 0.094211183 0.040200204][0.13032892 0.23772776 0.36241186 0.46599942 0.53312981 0.58098626 0.61468536 0.60108691 0.53408915 0.45136651 0.38584623 0.31711087 0.22076435 0.119134 0.04726366][0.15961848 0.28876844 0.43832392 0.561111 0.63917649 0.69371825 0.73216313 0.71176511 0.62553072 0.52053905 0.43993181 0.36062521 0.25029737 0.1340282 0.053070225][0.16398911 0.29877743 0.45572984 0.58380985 0.66391057 0.71793932 0.75503993 0.72825074 0.63109183 0.51408428 0.42554617 0.34330148 0.2317912 0.11636779 0.038847994][0.14074726 0.26300955 0.40708888 0.52443129 0.59654188 0.64247841 0.67186463 0.64018875 0.54309857 0.42812237 0.34178188 0.26596093 0.16754892 0.069340974 0.007403763][0.093914934 0.18838608 0.30209181 0.39444134 0.44988903 0.48260564 0.50113273 0.46858972 0.38355142 0.28587741 0.2136758 0.15412468 0.081008337 0.012032517 -0.026949579][0.03630998 0.096653283 0.17251042 0.23388284 0.26934081 0.28797323 0.29614314 0.26691967 0.20162311 0.13058983 0.080459982 0.04283756 -0.0001288643 -0.036781792 -0.052482478][-0.014268494 0.016106598 0.058423977 0.0925727 0.1107603 0.11818766 0.11913972 0.096058741 0.051634807 0.0074759903 -0.020306436 -0.037241116 -0.0542773 -0.065059364 -0.063636422][-0.046958864 -0.037535526 -0.018876294 -0.003648675 0.0030766451 0.0039151693 0.0015604354 -0.014215995 -0.04048425 -0.0629817 -0.073136955 -0.074819461 -0.07475505 -0.070312947 -0.060593043][-0.061438847 -0.063905358 -0.059365209 -0.055419337 -0.054895952 -0.056814104 -0.059701197 -0.068646006 -0.081032731 -0.08828976 -0.086911708 -0.079764947 -0.071220994 -0.060862284 -0.049996439][-0.061833955 -0.06923794 -0.071317039 -0.072780065 -0.074803226 -0.077237032 -0.079434939 -0.0833485 -0.087103426 -0.086138017 -0.079793036 -0.0700555 -0.059761677 -0.049668711 -0.041519653]]...]
INFO - root - 2017-12-10 10:56:22.771156: step 8610, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 70h:35m:26s remains)
INFO - root - 2017-12-10 10:56:30.613639: step 8620, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 69h:58m:22s remains)
INFO - root - 2017-12-10 10:56:38.610531: step 8630, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 72h:01m:29s remains)
INFO - root - 2017-12-10 10:56:46.303314: step 8640, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 69h:11m:16s remains)
INFO - root - 2017-12-10 10:56:54.199954: step 8650, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 72h:05m:19s remains)
INFO - root - 2017-12-10 10:57:02.183827: step 8660, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 69h:25m:43s remains)
INFO - root - 2017-12-10 10:57:09.853999: step 8670, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 71h:14m:14s remains)
INFO - root - 2017-12-10 10:57:17.735363: step 8680, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 72h:17m:38s remains)
INFO - root - 2017-12-10 10:57:25.644257: step 8690, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 71h:58m:33s remains)
INFO - root - 2017-12-10 10:57:33.414137: step 8700, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 69h:48m:57s remains)
2017-12-10 10:57:34.326640: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0038564627 0.015123274 0.025070066 0.032926314 0.044270746 0.059734739 0.069902964 0.072668433 0.069302268 0.062163293 0.048561834 0.029228294 0.013705158 0.0068489574 0.0055021774][0.0376168 0.0595293 0.078755982 0.094125383 0.11486414 0.14159662 0.16058092 0.16821027 0.16749997 0.16093612 0.14036793 0.10689704 0.077655144 0.062153056 0.054906435][0.081118308 0.1182122 0.15200345 0.18050788 0.214968 0.254769 0.28281587 0.29441738 0.29566628 0.29026061 0.26407376 0.2175338 0.17256339 0.14356038 0.12413185][0.12263814 0.17767896 0.23133095 0.28068051 0.33482918 0.38934144 0.42505464 0.43618777 0.43279493 0.42174688 0.38838521 0.33085519 0.27070776 0.22600615 0.19087476][0.15105097 0.22387309 0.30143353 0.37974349 0.46176916 0.53665376 0.58319795 0.59273291 0.57871068 0.55297327 0.50656283 0.43680918 0.36116707 0.2992956 0.24648985][0.16625461 0.25697276 0.36204416 0.47552341 0.59100062 0.68998951 0.74772966 0.75132829 0.7180751 0.667344 0.59985805 0.5153445 0.42657536 0.35123965 0.28409961][0.18282868 0.29214418 0.42379624 0.56808829 0.71003723 0.82517028 0.88418692 0.87237787 0.81320959 0.73321176 0.64223552 0.54448146 0.44957826 0.36905658 0.29521334][0.18521781 0.30609131 0.45431477 0.61541837 0.76939231 0.88772917 0.93743688 0.90633 0.82489514 0.72455156 0.61938274 0.51779729 0.42699862 0.35052466 0.27823028][0.17152944 0.2946437 0.44589761 0.60611904 0.75394189 0.85965973 0.88942438 0.83796316 0.74274075 0.63680315 0.53228492 0.43897355 0.36235732 0.29913214 0.23663698][0.15587939 0.27079189 0.40843907 0.54736221 0.66940004 0.74773014 0.751699 0.68484354 0.5880962 0.49191847 0.40088946 0.3239055 0.26618993 0.22095038 0.17416587][0.13756135 0.23518956 0.34549543 0.44793379 0.53056306 0.57312965 0.55165148 0.47718716 0.38978127 0.31357619 0.2445787 0.18805629 0.14971015 0.12268405 0.093942419][0.10720477 0.18203031 0.2599369 0.32302907 0.36614114 0.37632784 0.33810315 0.26685661 0.19723874 0.14549316 0.10100661 0.064739227 0.04233807 0.029359048 0.015992219][0.058137048 0.10877284 0.15729117 0.18943602 0.20422605 0.19470951 0.15316239 0.095731646 0.048491333 0.020581625 -0.0016137467 -0.020430993 -0.031019842 -0.034860328 -0.037862234][0.014154468 0.043708537 0.069365561 0.081137255 0.079953976 0.063147791 0.028314041 -0.010994352 -0.037881646 -0.04867084 -0.056053031 -0.063237593 -0.066473804 -0.065109514 -0.062314291][-0.0090960106 0.0053013223 0.015830737 0.016558738 0.0090312753 -0.0070167924 -0.031125048 -0.054024942 -0.066600271 -0.06879279 -0.070004836 -0.072254337 -0.072557852 -0.069539934 -0.064778894]]...]
INFO - root - 2017-12-10 10:57:42.164370: step 8710, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 68h:46m:43s remains)
INFO - root - 2017-12-10 10:57:50.007726: step 8720, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 68h:37m:41s remains)
INFO - root - 2017-12-10 10:57:57.719859: step 8730, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 69h:01m:35s remains)
INFO - root - 2017-12-10 10:58:05.581137: step 8740, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 71h:22m:25s remains)
INFO - root - 2017-12-10 10:58:13.302759: step 8750, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 71h:52m:25s remains)
INFO - root - 2017-12-10 10:58:21.161591: step 8760, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 72h:54m:49s remains)
INFO - root - 2017-12-10 10:58:29.019146: step 8770, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.747 sec/batch; 67h:11m:32s remains)
INFO - root - 2017-12-10 10:58:36.823285: step 8780, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.753 sec/batch; 67h:44m:36s remains)
INFO - root - 2017-12-10 10:58:44.599890: step 8790, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.788 sec/batch; 70h:53m:51s remains)
INFO - root - 2017-12-10 10:58:52.609109: step 8800, loss = 0.71, batch loss = 0.65 (8.7 examples/sec; 0.924 sec/batch; 83h:03m:59s remains)
2017-12-10 10:58:53.367103: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.094519936 0.086556889 0.093700871 0.11773981 0.14809509 0.16131596 0.14448227 0.10723435 0.073040336 0.060028605 0.062245052 0.068666145 0.078275472 0.10222057 0.14622992][0.13639818 0.1093557 0.0939828 0.10098273 0.12396854 0.13902502 0.12899359 0.097906828 0.064476892 0.0450078 0.037925027 0.038033631 0.047718178 0.076747611 0.12624304][0.15458719 0.11510199 0.08827959 0.089209 0.11320028 0.13533555 0.13500071 0.11177506 0.080697529 0.055296622 0.037202548 0.028171545 0.035186008 0.06695959 0.11943767][0.13917331 0.098041609 0.07692726 0.088444218 0.12541312 0.16077732 0.17166193 0.15539333 0.12468332 0.090632342 0.058682673 0.038329426 0.041206259 0.074013874 0.1273969][0.10406587 0.07255736 0.070336625 0.10473139 0.16319638 0.21538673 0.23726441 0.22501518 0.18985008 0.14224997 0.0936206 0.06172435 0.061675835 0.095995836 0.14937577][0.066322342 0.052377626 0.076141678 0.13857119 0.22155437 0.29148951 0.32295638 0.310525 0.2651799 0.19991526 0.13409059 0.09214253 0.090388872 0.12543905 0.17654487][0.042545296 0.050675754 0.10017069 0.18738374 0.29139137 0.37606987 0.41356552 0.3963086 0.33678681 0.25389132 0.17454991 0.12630323 0.12309285 0.15536535 0.19891535][0.051015008 0.077956222 0.14161035 0.23954096 0.35360053 0.44790757 0.48980227 0.46665293 0.39263633 0.29442862 0.205547 0.15358296 0.1478786 0.1734658 0.20544438][0.091879465 0.12786928 0.18678996 0.27538964 0.38489529 0.48393294 0.53335243 0.51065862 0.42807394 0.31854987 0.22188814 0.16559474 0.15402399 0.16893496 0.18763083][0.14529669 0.18215781 0.2237917 0.28854635 0.38045517 0.47721973 0.53528547 0.51954985 0.43578526 0.3196345 0.21633309 0.15422931 0.134236 0.13768508 0.14618333][0.18256116 0.21612343 0.23923139 0.27710515 0.34390625 0.42821833 0.48805264 0.48001483 0.40234521 0.28839162 0.18429516 0.11919274 0.092927523 0.089907885 0.0965002][0.18659879 0.21661466 0.22783844 0.24490564 0.28596717 0.34891823 0.39939561 0.39417112 0.32657003 0.22412986 0.12858884 0.06799455 0.042725436 0.041501977 0.054383233][0.15901865 0.1866373 0.19501996 0.2022979 0.22440223 0.26408836 0.2978138 0.28995711 0.23289576 0.14702253 0.066439115 0.016717808 -0.00029693605 0.0074575925 0.031141473][0.11954708 0.14667743 0.15838853 0.16463009 0.1765103 0.19837704 0.21721686 0.20795451 0.16307268 0.094982229 0.029338447 -0.0095114559 -0.016780511 0.0014890099 0.034559987][0.092244618 0.11998196 0.13574345 0.14371294 0.15219657 0.16636945 0.18073365 0.17831813 0.14967574 0.098964766 0.0446845 0.011631688 0.0096431393 0.033975948 0.070229955]]...]
INFO - root - 2017-12-10 10:59:01.227324: step 8810, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.791 sec/batch; 71h:09m:00s remains)
INFO - root - 2017-12-10 10:59:08.990124: step 8820, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 71h:24m:31s remains)
INFO - root - 2017-12-10 10:59:16.583088: step 8830, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 70h:15m:26s remains)
INFO - root - 2017-12-10 10:59:24.441070: step 8840, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 71h:35m:13s remains)
INFO - root - 2017-12-10 10:59:32.326770: step 8850, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 71h:04m:31s remains)
INFO - root - 2017-12-10 10:59:40.186179: step 8860, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 68h:20m:01s remains)
INFO - root - 2017-12-10 10:59:48.050468: step 8870, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 71h:19m:45s remains)
INFO - root - 2017-12-10 10:59:56.106394: step 8880, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 72h:45m:38s remains)
INFO - root - 2017-12-10 11:00:03.933854: step 8890, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.774 sec/batch; 69h:32m:01s remains)
INFO - root - 2017-12-10 11:00:11.844345: step 8900, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 72h:19m:09s remains)
2017-12-10 11:00:12.658027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.018286046 -0.0038955137 0.014536346 0.030751374 0.03721299 0.028346002 0.0068096342 -0.018651208 -0.042208251 -0.060122494 -0.070077658 -0.070383973 -0.06363216 -0.04935734 -0.03280168][-0.00444105 0.024627287 0.059572209 0.091064364 0.10863213 0.10295342 0.075113147 0.0359835 -0.0051969714 -0.040996924 -0.066322975 -0.076320328 -0.072899543 -0.057559386 -0.038208216][0.01535411 0.064016819 0.12324996 0.17931096 0.21710667 0.22208764 0.19210494 0.13798425 0.073519006 0.012163802 -0.03646002 -0.063654013 -0.069244221 -0.056178924 -0.03617239][0.036118791 0.1061625 0.19428271 0.28208345 0.34857711 0.37301528 0.3481341 0.28263 0.1935863 0.1008352 0.020312553 -0.033182524 -0.055215366 -0.049551103 -0.030975854][0.053180803 0.14086345 0.25452358 0.37257063 0.46932888 0.51900518 0.50826472 0.44123483 0.33617616 0.21721923 0.10488851 0.019875871 -0.02691002 -0.03630697 -0.024521135][0.066265061 0.16356181 0.29295436 0.43318123 0.55662823 0.63423836 0.64622611 0.5903734 0.48308694 0.3497541 0.21245521 0.095799923 0.018250246 -0.01473996 -0.017355576][0.073301286 0.17204525 0.3066631 0.45947951 0.6035651 0.70800883 0.74698234 0.7106117 0.61079562 0.47185361 0.31642571 0.17231971 0.065400273 0.0081428224 -0.010540894][0.068609655 0.15887022 0.2866247 0.43979523 0.59388345 0.71737993 0.7809391 0.76828533 0.68440771 0.54957086 0.38638821 0.22512211 0.097350553 0.02179062 -0.0092397006][0.053078461 0.12270796 0.22914854 0.36725092 0.51633447 0.64561331 0.72580749 0.73796439 0.68034583 0.56554848 0.41293544 0.25219509 0.11740273 0.031923585 -0.0074457247][0.038156833 0.075625286 0.14765269 0.25591969 0.3844896 0.50481486 0.59012538 0.62211913 0.59420782 0.51037192 0.38469437 0.24336958 0.1187599 0.035408571 -0.00629039][0.031556237 0.031677574 0.063856758 0.13452914 0.23197788 0.3316136 0.41022316 0.45184812 0.44728991 0.39382526 0.30090156 0.1905525 0.09023457 0.021751717 -0.012955048][0.03458634 0.00086668017 -0.0027162209 0.031453192 0.093552582 0.16340363 0.22285315 0.26039743 0.26725551 0.23725021 0.17634141 0.10199608 0.034470078 -0.0095695555 -0.028966829][0.048296947 -0.010041678 -0.040676326 -0.036072154 -0.0062149451 0.032370012 0.066610053 0.090558864 0.098634221 0.084824629 0.052263796 0.012462735 -0.022680892 -0.041885529 -0.04539869][0.063072421 -0.00782062 -0.054189671 -0.069123156 -0.062469073 -0.047827233 -0.03482759 -0.025223533 -0.020667244 -0.024844738 -0.036974926 -0.051232222 -0.062529527 -0.063492432 -0.055700496][0.058306016 -0.009659539 -0.056050655 -0.076678082 -0.080877446 -0.079707876 -0.080420874 -0.082176886 -0.083427675 -0.084954627 -0.087095663 -0.087903835 -0.086231112 -0.076748595 -0.062117927]]...]
INFO - root - 2017-12-10 11:00:20.301295: step 8910, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 71h:08m:04s remains)
INFO - root - 2017-12-10 11:00:28.187411: step 8920, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 71h:22m:48s remains)
INFO - root - 2017-12-10 11:00:36.099551: step 8930, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 68h:41m:55s remains)
INFO - root - 2017-12-10 11:00:43.932890: step 8940, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 70h:20m:37s remains)
INFO - root - 2017-12-10 11:00:51.735033: step 8950, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 69h:01m:51s remains)
INFO - root - 2017-12-10 11:00:59.574988: step 8960, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.773 sec/batch; 69h:25m:56s remains)
INFO - root - 2017-12-10 11:01:07.390333: step 8970, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 68h:32m:24s remains)
INFO - root - 2017-12-10 11:01:15.189550: step 8980, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 70h:49m:02s remains)
INFO - root - 2017-12-10 11:01:22.943867: step 8990, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 71h:21m:35s remains)
INFO - root - 2017-12-10 11:01:30.642416: step 9000, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 70h:06m:09s remains)
2017-12-10 11:01:31.395368: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.059285525 0.066490367 0.074997887 0.081624821 0.0864677 0.089152515 0.088747837 0.082414187 0.070305936 0.058116458 0.050953619 0.056085978 0.074091054 0.10246596 0.13835609][0.062161978 0.064979613 0.071358949 0.078334168 0.084500767 0.088629723 0.088907793 0.081907675 0.067475893 0.05152357 0.040456217 0.04293948 0.061496351 0.094459578 0.13767277][0.072192833 0.072782107 0.080156252 0.090844 0.10105652 0.10828547 0.10925954 0.10042488 0.081766494 0.059778605 0.041927315 0.038760774 0.054906469 0.08922784 0.1359511][0.090483412 0.094020806 0.10817621 0.12739202 0.14510801 0.15756211 0.1601211 0.14901057 0.12494151 0.095008381 0.067813314 0.055794124 0.065219291 0.096386917 0.14203125][0.11748724 0.12785029 0.15226817 0.18189053 0.20738386 0.22509542 0.23006383 0.21761106 0.18891205 0.15136239 0.11420856 0.090498462 0.087984547 0.10898244 0.14652388][0.14749385 0.1678271 0.20488687 0.24551971 0.27820325 0.30127388 0.31092721 0.30053744 0.26993358 0.22612441 0.17864226 0.14061351 0.12102422 0.12510715 0.14771259][0.17396745 0.20296971 0.24868134 0.29538259 0.33078253 0.35658824 0.37156606 0.3670809 0.34000862 0.29533449 0.24233568 0.19421779 0.16069646 0.14941572 0.15705787][0.19836402 0.22977334 0.27574494 0.32010579 0.35132441 0.3741481 0.38961175 0.38864198 0.36664018 0.32597303 0.27434796 0.22385223 0.18432797 0.16426852 0.16159828][0.21199867 0.23951723 0.2779963 0.31348315 0.33625361 0.35238931 0.36411861 0.363569 0.34699094 0.315162 0.27288458 0.22906034 0.1921926 0.17030609 0.16240521][0.20613571 0.22647256 0.2535007 0.27710041 0.29029167 0.29911527 0.30591914 0.304706 0.29374641 0.27361971 0.24596232 0.21547489 0.18829371 0.17122644 0.16402893][0.17760472 0.19113161 0.2082752 0.22263484 0.22968234 0.23405129 0.23741738 0.23586729 0.22969092 0.22002222 0.20620406 0.18962321 0.17389563 0.16426927 0.16115366][0.13878334 0.14822909 0.15950671 0.16873002 0.17282631 0.17478363 0.1755562 0.17319582 0.16961789 0.16674186 0.16287622 0.15757306 0.1523677 0.15038316 0.15215135][0.10422137 0.11032005 0.11757834 0.12390302 0.12693578 0.12803935 0.12758076 0.12492751 0.12266387 0.12293041 0.12390245 0.12461256 0.12543164 0.12787621 0.13247134][0.085872784 0.088338219 0.091607131 0.095495336 0.097860813 0.098480687 0.097395405 0.094836392 0.09326636 0.094211571 0.09605407 0.098019853 0.10006349 0.1030252 0.10760279][0.084195949 0.082772948 0.0821745 0.084020168 0.085984439 0.086430214 0.085173585 0.083049089 0.081807956 0.0818262 0.081772037 0.081395678 0.080695234 0.080540441 0.082166068]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 11:01:39.258081: step 9010, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 71h:10m:05s remains)
INFO - root - 2017-12-10 11:01:47.079202: step 9020, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 70h:29m:47s remains)
INFO - root - 2017-12-10 11:01:54.896941: step 9030, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 68h:52m:08s remains)
INFO - root - 2017-12-10 11:02:02.786766: step 9040, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 70h:23m:52s remains)
INFO - root - 2017-12-10 11:02:10.660682: step 9050, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 73h:07m:55s remains)
INFO - root - 2017-12-10 11:02:18.562877: step 9060, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 69h:23m:48s remains)
INFO - root - 2017-12-10 11:02:26.222941: step 9070, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 68h:16m:51s remains)
INFO - root - 2017-12-10 11:02:34.069849: step 9080, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 70h:53m:31s remains)
INFO - root - 2017-12-10 11:02:41.788636: step 9090, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 71h:15m:08s remains)
INFO - root - 2017-12-10 11:02:49.731227: step 9100, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 72h:52m:17s remains)
2017-12-10 11:02:50.597142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.022766199 0.013286958 0.069240421 0.13743249 0.20357105 0.25239673 0.27841362 0.2819187 0.26441264 0.23899992 0.22257893 0.2237964 0.23652893 0.25114316 0.26204053][-0.023987878 0.010947946 0.065080918 0.12938912 0.1918873 0.23924506 0.26710486 0.27391994 0.25940797 0.23591609 0.2178317 0.21292862 0.21867207 0.23003852 0.24394746][-0.020496415 0.014860353 0.068082631 0.12811495 0.18382072 0.22393061 0.24608669 0.24917111 0.2341844 0.21441439 0.20041479 0.19623838 0.19890125 0.20654558 0.22018385][-0.017513132 0.018647633 0.071527258 0.12860492 0.17809303 0.20938165 0.22128405 0.21531056 0.19643191 0.17993422 0.17294249 0.17481036 0.1800085 0.18722409 0.20002034][-0.018900681 0.01512426 0.065204561 0.11919887 0.1647504 0.1908447 0.19520442 0.1808359 0.15671353 0.14080659 0.13946012 0.14891922 0.16132154 0.17358147 0.18822365][-0.023436997 0.0062071229 0.051681347 0.10369923 0.15055914 0.17977113 0.18510221 0.16772239 0.1380915 0.11623564 0.11168025 0.12314951 0.14220299 0.16215378 0.18145949][-0.027341504 -0.00014799881 0.043641329 0.097907551 0.15223856 0.19180089 0.20510109 0.18935813 0.15374543 0.11934819 0.10088528 0.10368174 0.12166318 0.14558868 0.16974543][-0.029997544 -0.0029836122 0.041819394 0.1000432 0.16315028 0.21461967 0.23900762 0.22913165 0.19111192 0.14497578 0.10850779 0.0942873 0.10141904 0.12129021 0.14638755][-0.030926077 -0.0015781022 0.046662875 0.10909373 0.17840333 0.23811494 0.27178743 0.26905158 0.23246565 0.179357 0.1277405 0.09462107 0.085030571 0.09454906 0.11725243][-0.031155881 0.0012855568 0.05302598 0.11762304 0.18758479 0.24858351 0.28674349 0.29075202 0.25996441 0.20775728 0.14985101 0.10379122 0.078527 0.075685732 0.09305539][-0.0309022 0.003729664 0.057403509 0.12186948 0.18879589 0.24626784 0.28436074 0.29318473 0.27058512 0.22572067 0.17013092 0.11930867 0.08380533 0.070341811 0.080576189][-0.029414091 0.0076927305 0.064480655 0.13161059 0.1989695 0.25448224 0.29027167 0.29910061 0.27988327 0.24013373 0.18833406 0.13776095 0.098957181 0.080861963 0.086586066][-0.026371194 0.013133503 0.074019305 0.14667232 0.21865091 0.27518964 0.30812067 0.31303126 0.29121414 0.25140893 0.20161656 0.15448055 0.11981726 0.10537382 0.112579][-0.02332277 0.017384011 0.081703156 0.16043171 0.23895827 0.29877865 0.33001992 0.3300364 0.30218416 0.25768948 0.20722145 0.16481654 0.13964899 0.13597593 0.15125109][-0.022571465 0.017695874 0.082812421 0.16432275 0.24644648 0.30810705 0.33782521 0.33417857 0.30147696 0.2526814 0.20198527 0.16538325 0.15160993 0.16078621 0.186229]]...]
INFO - root - 2017-12-10 11:02:58.500612: step 9110, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 69h:21m:27s remains)
INFO - root - 2017-12-10 11:03:06.448700: step 9120, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 71h:23m:01s remains)
INFO - root - 2017-12-10 11:03:14.347837: step 9130, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 68h:13m:12s remains)
INFO - root - 2017-12-10 11:03:22.152388: step 9140, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 70h:45m:51s remains)
INFO - root - 2017-12-10 11:03:29.865635: step 9150, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 72h:14m:09s remains)
INFO - root - 2017-12-10 11:03:37.689383: step 9160, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 70h:38m:53s remains)
INFO - root - 2017-12-10 11:03:45.501380: step 9170, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.745 sec/batch; 66h:55m:26s remains)
INFO - root - 2017-12-10 11:03:53.214766: step 9180, loss = 0.71, batch loss = 0.65 (9.6 examples/sec; 0.831 sec/batch; 74h:40m:29s remains)
INFO - root - 2017-12-10 11:04:01.077865: step 9190, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 70h:05m:33s remains)
INFO - root - 2017-12-10 11:04:09.000044: step 9200, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 71h:53m:18s remains)
2017-12-10 11:04:09.848017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.039944664 -0.014242589 0.021249332 0.0606202 0.10486897 0.14244288 0.16369252 0.17134644 0.16608521 0.15522514 0.13357946 0.10669457 0.075995214 0.040398303 0.0078251157][-0.022145722 0.018606003 0.076755196 0.14364237 0.21938188 0.2862272 0.32972404 0.34903187 0.34042662 0.31721455 0.27768871 0.23272425 0.18138267 0.12241589 0.068926647][-0.00027896883 0.056903139 0.14168827 0.24241079 0.35648036 0.45810518 0.52721483 0.55745292 0.53944635 0.49454 0.43025681 0.3647753 0.29267183 0.21168724 0.13937314][0.016670274 0.087560758 0.19667692 0.32997814 0.48092625 0.61585122 0.70831507 0.74470967 0.71214777 0.64119583 0.55238509 0.4704479 0.38486195 0.2903935 0.20563719][0.026923783 0.10714924 0.23483865 0.39488092 0.57634723 0.7386182 0.84858197 0.88559747 0.83713639 0.74191278 0.63403285 0.54173011 0.45070529 0.35205892 0.26132679][0.034046892 0.12196381 0.26415056 0.44558045 0.65007895 0.83084971 0.95027214 0.98252934 0.91902483 0.805157 0.68652332 0.59207296 0.50325125 0.40610096 0.31070217][0.0396828 0.13475743 0.2878691 0.48374528 0.70074415 0.88833368 1.0070415 1.027541 0.94931608 0.82356453 0.7047894 0.61946976 0.54230469 0.45256656 0.35403341][0.0412583 0.14132689 0.29899111 0.49849275 0.71365356 0.89465225 1.002942 1.0086061 0.92161369 0.79619342 0.69030768 0.62339675 0.56353223 0.48439345 0.3839514][0.040517811 0.14211783 0.29782087 0.49027786 0.69039834 0.85313022 0.94315428 0.93326479 0.845091 0.73271292 0.64919704 0.60316837 0.55942273 0.49101552 0.39119777][0.038087998 0.13809977 0.2864657 0.46368021 0.6393218 0.77437037 0.83930629 0.8135522 0.72856361 0.63593942 0.577809 0.55172336 0.52184337 0.4645842 0.37010103][0.030150484 0.12381709 0.25928566 0.41513026 0.56127858 0.66483843 0.70309824 0.66343766 0.58277822 0.50917983 0.47250631 0.46267658 0.44491762 0.39968377 0.31612328][0.012224892 0.091722257 0.20541708 0.33176905 0.44322553 0.513754 0.52751273 0.47913992 0.40529817 0.34810418 0.32717916 0.3293072 0.32353848 0.29396451 0.22924992][-0.013926258 0.044721212 0.12928423 0.22032988 0.29540363 0.33602804 0.3323656 0.283925 0.2220595 0.18014336 0.16980724 0.17828164 0.18148765 0.16736673 0.12456551][-0.038365543 -0.0010839882 0.054536823 0.11217317 0.15577461 0.17322157 0.15962395 0.11726226 0.069802888 0.041086487 0.036717676 0.046858318 0.05473648 0.052120425 0.028914995][-0.0571007 -0.037932862 -0.006496924 0.024616567 0.045068361 0.047523029 0.030492583 -0.0023082981 -0.035106346 -0.05290344 -0.05358234 -0.04380054 -0.03413064 -0.030165566 -0.039554927]]...]
INFO - root - 2017-12-10 11:04:17.688350: step 9210, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 72h:25m:10s remains)
INFO - root - 2017-12-10 11:04:25.524099: step 9220, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.812 sec/batch; 72h:54m:11s remains)
INFO - root - 2017-12-10 11:04:33.131323: step 9230, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 70h:58m:20s remains)
INFO - root - 2017-12-10 11:04:41.020494: step 9240, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.792 sec/batch; 71h:07m:34s remains)
INFO - root - 2017-12-10 11:04:48.744172: step 9250, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 68h:22m:05s remains)
INFO - root - 2017-12-10 11:04:56.461060: step 9260, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.757 sec/batch; 67h:57m:04s remains)
INFO - root - 2017-12-10 11:05:04.303413: step 9270, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.768 sec/batch; 68h:57m:00s remains)
INFO - root - 2017-12-10 11:05:12.161737: step 9280, loss = 0.72, batch loss = 0.67 (10.2 examples/sec; 0.787 sec/batch; 70h:38m:09s remains)
INFO - root - 2017-12-10 11:05:20.047286: step 9290, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.787 sec/batch; 70h:37m:21s remains)
INFO - root - 2017-12-10 11:05:27.871081: step 9300, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 69h:33m:14s remains)
2017-12-10 11:05:28.659601: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1782324 0.17877631 0.17125699 0.15991944 0.15366991 0.16406654 0.18317787 0.20124353 0.20745979 0.1963567 0.17194852 0.1345593 0.090708278 0.048072267 0.014851129][0.20737971 0.20922917 0.20213163 0.19004597 0.18274945 0.19251344 0.2106131 0.22608376 0.22732063 0.20986246 0.17853893 0.13517758 0.087097861 0.04238404 0.0087424014][0.21691279 0.21921842 0.21297902 0.20202132 0.19544739 0.2044654 0.22085212 0.23324949 0.23045598 0.2083797 0.17252116 0.1265074 0.078080721 0.034762274 0.0031136095][0.21542436 0.21705046 0.21174745 0.20381257 0.2000328 0.20915268 0.22519901 0.23687637 0.2322917 0.20726572 0.16821 0.12111301 0.073102064 0.030858587 0.00050107576][0.20741819 0.20750126 0.20204808 0.19734737 0.1969167 0.2067479 0.22424845 0.2378615 0.23438434 0.20908646 0.16908532 0.12206805 0.074441276 0.032264758 0.0018974458][0.20473383 0.20165029 0.19495058 0.19180442 0.19328474 0.20324542 0.22257552 0.23898564 0.23772141 0.21379381 0.17444322 0.12822007 0.080507815 0.037185915 0.0057282718][0.21995232 0.21329655 0.20321922 0.19823857 0.19867459 0.20695806 0.22667286 0.24431615 0.24386477 0.22049537 0.18139893 0.13555771 0.087353989 0.042619143 0.0099513177][0.24856205 0.2369774 0.22007379 0.20845664 0.20424232 0.20948713 0.22828196 0.24582723 0.2453084 0.22251637 0.18393353 0.13849467 0.0902715 0.045233529 0.012321153][0.27086723 0.25471172 0.23070548 0.21119224 0.20078829 0.20171469 0.2176225 0.2337198 0.23267995 0.21099946 0.17432618 0.13094684 0.084948994 0.042042229 0.010865853][0.26564437 0.2467618 0.21839331 0.19370094 0.17882417 0.17609864 0.18846139 0.20257667 0.20207456 0.18336602 0.15133998 0.11298118 0.072105952 0.033650644 0.0056978152][0.23361561 0.21421824 0.18592486 0.16087516 0.145353 0.14151418 0.15176158 0.16500801 0.16656025 0.15213405 0.1259093 0.093884744 0.058714777 0.024705606 -0.00027255251][0.18843724 0.17233613 0.14837779 0.12709288 0.11351563 0.10997684 0.11944231 0.13325758 0.13789231 0.12779291 0.10695238 0.080593511 0.049991492 0.01891657 -0.00449408][0.14368378 0.13312393 0.11600436 0.10124282 0.091780059 0.0905116 0.10184225 0.11864904 0.12743592 0.12099867 0.10382003 0.080546625 0.051212724 0.019846169 -0.0045460854][0.1109423 0.10761845 0.098706804 0.091918863 0.088247851 0.091499142 0.10756195 0.12918854 0.14160636 0.1362913 0.11946102 0.095495366 0.063568547 0.028388508 0.00048944092][0.093993396 0.096527115 0.094036862 0.09343566 0.094823889 0.10312144 0.12506814 0.15200837 0.16732264 0.1616814 0.14364941 0.11733011 0.081591316 0.041359484 0.0090075042]]...]
INFO - root - 2017-12-10 11:05:36.307416: step 9310, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.766 sec/batch; 68h:46m:54s remains)
INFO - root - 2017-12-10 11:05:44.161981: step 9320, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 71h:48m:26s remains)
INFO - root - 2017-12-10 11:05:52.101119: step 9330, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.807 sec/batch; 72h:28m:18s remains)
INFO - root - 2017-12-10 11:05:59.903123: step 9340, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 71h:16m:10s remains)
INFO - root - 2017-12-10 11:06:07.639940: step 9350, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 69h:50m:45s remains)
INFO - root - 2017-12-10 11:06:15.473608: step 9360, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 69h:46m:47s remains)
INFO - root - 2017-12-10 11:06:23.312013: step 9370, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 71h:41m:26s remains)
INFO - root - 2017-12-10 11:06:31.241878: step 9380, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 71h:18m:39s remains)
INFO - root - 2017-12-10 11:06:38.862319: step 9390, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 70h:49m:05s remains)
INFO - root - 2017-12-10 11:06:46.727861: step 9400, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.800 sec/batch; 71h:48m:09s remains)
2017-12-10 11:06:47.508756: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.55170935 0.52839029 0.49682185 0.47129011 0.4539921 0.4412185 0.4278135 0.42628053 0.440191 0.47118714 0.4988777 0.49431106 0.44101241 0.34190187 0.21185555][0.48332852 0.45323291 0.42320749 0.404611 0.39755994 0.39436257 0.38869283 0.38984042 0.3999863 0.42636907 0.45286581 0.45261556 0.40694541 0.3166481 0.1965005][0.39111805 0.36302677 0.344919 0.34213728 0.34907994 0.35480583 0.35354486 0.35182676 0.35311988 0.37113124 0.39614132 0.40272579 0.36824676 0.29079187 0.18377203][0.34213907 0.32880706 0.33402571 0.354761 0.3776893 0.38980594 0.38747525 0.37516516 0.36221936 0.36902189 0.39065057 0.39953402 0.36854941 0.29515547 0.19168611][0.36407313 0.37736019 0.41370082 0.46102163 0.49837914 0.51221424 0.50379694 0.47691685 0.44892406 0.44506374 0.46126619 0.46411234 0.42267707 0.33847365 0.22508647][0.45559573 0.50116557 0.56843919 0.63866884 0.68563592 0.69730443 0.6809113 0.64093214 0.60127479 0.58955193 0.59842128 0.5867216 0.52157778 0.4125666 0.27774483][0.56949383 0.64344019 0.73550814 0.82152849 0.8720336 0.87918633 0.85486621 0.80360878 0.75328434 0.73280889 0.73074615 0.69891632 0.60583335 0.46984279 0.31394863][0.63411468 0.71909755 0.8222422 0.91469574 0.96514082 0.97086114 0.94508451 0.88910806 0.83069891 0.79876179 0.78057265 0.72629762 0.60945451 0.45584351 0.2904115][0.59428692 0.66559613 0.7598964 0.84664041 0.89457434 0.90467107 0.88752389 0.83871841 0.78016567 0.73885393 0.70442671 0.63368624 0.50792772 0.35626993 0.20189086][0.45280856 0.49026728 0.55681169 0.62501919 0.66632122 0.68278337 0.67949831 0.64642835 0.59586966 0.55124295 0.50722933 0.4330005 0.31862465 0.19174902 0.070630252][0.25565714 0.25401476 0.28481567 0.32755971 0.35901237 0.38027623 0.39016145 0.37552768 0.33960769 0.30059755 0.25848958 0.19552743 0.10894497 0.020760132 -0.0569623][0.058166016 0.029246569 0.031599641 0.051287249 0.071946882 0.093159229 0.11015337 0.11048092 0.093034 0.070125304 0.044866826 0.0080907345 -0.040598605 -0.088058807 -0.12834653][-0.0873715 -0.12123751 -0.12870786 -0.1203201 -0.10633178 -0.088401586 -0.070989087 -0.062355567 -0.062890343 -0.065418683 -0.06676048 -0.072865412 -0.086243257 -0.10291613 -0.12217855][-0.16360354 -0.18385902 -0.18408692 -0.17421949 -0.16103557 -0.14659339 -0.13292038 -0.12286606 -0.11280461 -0.098150276 -0.077280246 -0.057133127 -0.04466708 -0.043310944 -0.056765605][-0.16137135 -0.16438133 -0.15291925 -0.13687243 -0.12164641 -0.10943444 -0.10049016 -0.093189776 -0.078466818 -0.051782396 -0.014008357 0.02405788 0.050224911 0.055759348 0.034349907]]...]
INFO - root - 2017-12-10 11:06:55.527955: step 9410, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 71h:03m:57s remains)
INFO - root - 2017-12-10 11:07:03.366231: step 9420, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 69h:53m:43s remains)
INFO - root - 2017-12-10 11:07:11.170325: step 9430, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 70h:36m:14s remains)
INFO - root - 2017-12-10 11:07:18.722706: step 9440, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 68h:39m:27s remains)
INFO - root - 2017-12-10 11:07:26.531353: step 9450, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 68h:36m:15s remains)
INFO - root - 2017-12-10 11:07:34.335514: step 9460, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 70h:35m:56s remains)
INFO - root - 2017-12-10 11:07:42.011152: step 9470, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 70h:16m:06s remains)
INFO - root - 2017-12-10 11:07:49.831495: step 9480, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.773 sec/batch; 69h:24m:02s remains)
INFO - root - 2017-12-10 11:07:57.792444: step 9490, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 69h:28m:03s remains)
INFO - root - 2017-12-10 11:08:05.546617: step 9500, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 69h:49m:03s remains)
2017-12-10 11:08:06.391193: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26182014 0.25959632 0.24799989 0.26130092 0.30937165 0.38397843 0.46294233 0.51484144 0.5140838 0.45690694 0.36771443 0.27743214 0.21404326 0.18882853 0.192536][0.25937819 0.25868431 0.24931899 0.2629481 0.31225857 0.39648494 0.4960115 0.57281846 0.59262854 0.54244745 0.44289565 0.32690224 0.22731806 0.16612312 0.14519832][0.24670854 0.25233406 0.24917363 0.26180726 0.30348688 0.38272372 0.4862175 0.57322109 0.60520822 0.56389958 0.4643755 0.33676037 0.21495049 0.12859058 0.089227892][0.24243242 0.26103064 0.272176 0.29274967 0.33290645 0.40638015 0.50511616 0.58789819 0.61737907 0.57641691 0.4776817 0.34594539 0.21184285 0.10869391 0.055239741][0.25218472 0.29006153 0.32493341 0.36595893 0.4168309 0.4915911 0.58506918 0.65653849 0.67412615 0.62740344 0.52914506 0.39666042 0.25504178 0.13948298 0.07282716][0.267969 0.32676715 0.38742045 0.45061162 0.51371109 0.59056312 0.67796248 0.7368561 0.74208635 0.69008589 0.59306353 0.4603 0.31382617 0.19030923 0.11280388][0.26622763 0.34053 0.42008612 0.49763831 0.56693608 0.64240605 0.72124189 0.76693368 0.76100636 0.705143 0.60987049 0.47932303 0.33635259 0.21702985 0.13936353][0.24227883 0.31760621 0.39957637 0.47569633 0.53907907 0.60484058 0.6697284 0.70151711 0.68737018 0.63329977 0.54762381 0.43090692 0.30627149 0.20560719 0.13953626][0.18424715 0.24608187 0.31504408 0.37718362 0.42727256 0.48059994 0.53286636 0.5561294 0.54195064 0.49827597 0.43080586 0.33746114 0.23994993 0.16460195 0.11534533][0.096780233 0.13794985 0.18598346 0.22825904 0.26249528 0.302828 0.34340274 0.36055276 0.35028976 0.32144496 0.27692741 0.21235111 0.14607111 0.097869925 0.066473514][0.01241482 0.0335051 0.058547646 0.077565737 0.091495872 0.11279662 0.13606824 0.14403296 0.13699979 0.12352309 0.10287308 0.068271808 0.033008762 0.010002445 -0.0048126681][-0.041153654 -0.034919683 -0.028015804 -0.027565744 -0.030397676 -0.026457762 -0.019484989 -0.020108432 -0.025527535 -0.02840055 -0.031825367 -0.043218896 -0.054295633 -0.058328617 -0.060271211][-0.068054043 -0.07041008 -0.073294438 -0.081841789 -0.092420936 -0.097675845 -0.099414177 -0.10332592 -0.10660592 -0.10382621 -0.098548621 -0.097137548 -0.095103741 -0.0897671 -0.08545845][-0.083813079 -0.090240054 -0.096265651 -0.10608495 -0.11685248 -0.12387167 -0.12772182 -0.13130787 -0.13287848 -0.12924156 -0.12287059 -0.11768631 -0.11130892 -0.10327543 -0.097118422][-0.091758892 -0.09903 -0.10460428 -0.11242454 -0.12096911 -0.12770113 -0.1323234 -0.1357411 -0.13702488 -0.13463974 -0.12968346 -0.12396865 -0.11669298 -0.10869391 -0.10266864]]...]
INFO - root - 2017-12-10 11:08:14.199437: step 9510, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 70h:43m:45s remains)
INFO - root - 2017-12-10 11:08:22.011137: step 9520, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 68h:48m:47s remains)
INFO - root - 2017-12-10 11:08:29.720717: step 9530, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.765 sec/batch; 68h:40m:01s remains)
INFO - root - 2017-12-10 11:08:37.575272: step 9540, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 72h:23m:52s remains)
INFO - root - 2017-12-10 11:08:45.244587: step 9550, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 71h:15m:49s remains)
INFO - root - 2017-12-10 11:08:53.154567: step 9560, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 70h:30m:40s remains)
INFO - root - 2017-12-10 11:09:00.912636: step 9570, loss = 0.69, batch loss = 0.64 (11.0 examples/sec; 0.728 sec/batch; 65h:16m:54s remains)
INFO - root - 2017-12-10 11:09:08.828161: step 9580, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 72h:23m:05s remains)
INFO - root - 2017-12-10 11:09:16.643033: step 9590, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 70h:45m:09s remains)
INFO - root - 2017-12-10 11:09:24.474953: step 9600, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.773 sec/batch; 69h:19m:12s remains)
2017-12-10 11:09:25.337844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.037625529 -0.027150003 0.0049275439 0.058319133 0.12790668 0.20490044 0.27625182 0.326271 0.34920517 0.34367025 0.31298497 0.27013764 0.22550428 0.1919093 0.16980205][-0.037814438 -0.036951747 -0.011637216 0.039056763 0.11074684 0.19517337 0.27701336 0.33681342 0.36666954 0.36341459 0.33149776 0.28567985 0.23862968 0.20499425 0.18651202][-0.006800686 -0.016176183 -0.00074269873 0.042288959 0.1093048 0.19369557 0.27819136 0.33985442 0.36895517 0.36196136 0.3250308 0.27427113 0.22379041 0.18896027 0.17317069][0.053625882 0.031542219 0.032396957 0.062154155 0.11829112 0.19691309 0.27894971 0.33865094 0.36382014 0.350234 0.3058725 0.2475529 0.19038117 0.15060292 0.13393578][0.12895398 0.091817416 0.074369431 0.086922541 0.12800188 0.19657786 0.27258888 0.32858029 0.34953535 0.32937393 0.27686071 0.2094785 0.14329672 0.095741428 0.07510338][0.21439268 0.1639519 0.12989648 0.12762675 0.15559502 0.21401234 0.28243062 0.33203614 0.345367 0.3143197 0.24945021 0.17071679 0.094930314 0.039591163 0.015282311][0.30102757 0.24758409 0.20707247 0.19882402 0.22067295 0.27255428 0.33446181 0.37652943 0.38024041 0.33567983 0.25503215 0.16132566 0.072517835 0.00697522 -0.022930436][0.3716341 0.3292475 0.29586971 0.29226738 0.3158581 0.36566374 0.42216909 0.45559451 0.44875979 0.39069086 0.29349196 0.18239011 0.077787034 8.7844855e-05 -0.03711931][0.4036046 0.38353792 0.36948377 0.38036957 0.41274118 0.46339989 0.51293337 0.5349865 0.51630068 0.44678327 0.33671474 0.21221107 0.096152507 0.00986586 -0.032858949][0.39727116 0.40152565 0.40953845 0.43586439 0.47706765 0.52725184 0.56804556 0.57904083 0.55131179 0.47654733 0.36179093 0.23234025 0.11312016 0.02486242 -0.019202577][0.35495907 0.37581533 0.39734769 0.43024889 0.47356659 0.51989245 0.55260056 0.55712759 0.52740061 0.45641115 0.34770155 0.22470213 0.1135876 0.03320688 -0.0050764848][0.27026039 0.29509187 0.31824571 0.3472698 0.38436985 0.42273226 0.44819334 0.45110098 0.42688057 0.36914018 0.27853742 0.17525846 0.084942274 0.023307588 -0.0011286774][0.16387662 0.18215027 0.19737791 0.2146562 0.23863444 0.2645753 0.28156537 0.28386354 0.26777998 0.22784773 0.16324091 0.090091564 0.031056948 -0.003044884 -0.0074232793][0.0707515 0.077605918 0.080997959 0.084822312 0.094843693 0.10850803 0.11820702 0.12054525 0.11217716 0.089242555 0.051151942 0.0099756606 -0.016437398 -0.022674434 -0.00823922][0.016368767 0.013242069 0.0066934549 0.00064784242 0.00089314987 0.0067622485 0.01356655 0.018531306 0.017739724 0.0082013533 -0.0095065385 -0.026303049 -0.028755143 -0.015108286 0.011075536]]...]
INFO - root - 2017-12-10 11:09:33.242555: step 9610, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 69h:48m:28s remains)
INFO - root - 2017-12-10 11:09:40.937958: step 9620, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 70h:02m:47s remains)
INFO - root - 2017-12-10 11:09:48.602086: step 9630, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 71h:52m:30s remains)
INFO - root - 2017-12-10 11:09:56.545443: step 9640, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 71h:08m:36s remains)
INFO - root - 2017-12-10 11:10:04.439237: step 9650, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 69h:22m:48s remains)
INFO - root - 2017-12-10 11:10:12.349739: step 9660, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 71h:49m:59s remains)
INFO - root - 2017-12-10 11:10:20.273820: step 9670, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 70h:48m:11s remains)
INFO - root - 2017-12-10 11:10:28.090548: step 9680, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 70h:47m:28s remains)
INFO - root - 2017-12-10 11:10:35.970935: step 9690, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 68h:42m:32s remains)
INFO - root - 2017-12-10 11:10:43.816305: step 9700, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 71h:24m:57s remains)
2017-12-10 11:10:44.596578: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013265504 0.014703242 0.028928228 0.054575022 0.082624458 0.10366572 0.11095248 0.10293273 0.077988543 0.042134371 0.0081460355 -0.01591612 -0.029901082 -0.036253251 -0.037127383][0.02367047 0.025750771 0.042596 0.072393894 0.10354084 0.12503989 0.13003373 0.11830694 0.08890757 0.048347585 0.010547213 -0.015959922 -0.031049538 -0.037888587 -0.038982026][0.03312793 0.035411511 0.05342152 0.085147358 0.11770509 0.13868995 0.14114784 0.12623687 0.093593165 0.050477587 0.011041336 -0.01625021 -0.031443592 -0.038389053 -0.039473996][0.041261453 0.042882953 0.060005736 0.091254793 0.12423514 0.14564642 0.14784665 0.13200054 0.097899966 0.053601779 0.013261093 -0.014660161 -0.030467575 -0.037930056 -0.0392119][0.048331603 0.048659328 0.063009821 0.091273256 0.12298033 0.14477803 0.14803971 0.13308111 0.0994987 0.055545736 0.015356526 -0.012756163 -0.029112525 -0.037136 -0.038599793][0.05651259 0.054886729 0.064576671 0.087135851 0.11503957 0.13582389 0.14013688 0.12718351 0.095789619 0.054102421 0.015449089 -0.012105048 -0.028539009 -0.036855195 -0.038435753][0.074783489 0.070036381 0.072551027 0.086357884 0.10723076 0.12433027 0.12830195 0.11714172 0.088654682 0.050298769 0.014163731 -0.012116035 -0.028200898 -0.036518678 -0.038043573][0.10016294 0.092402071 0.088078044 0.092673264 0.10519452 0.11659417 0.11823152 0.10720497 0.080775805 0.045551125 0.011970989 -0.01284129 -0.028433032 -0.036523972 -0.03776639][0.12564419 0.11713248 0.10852898 0.10611654 0.11107945 0.11595502 0.11329926 0.10026997 0.074351475 0.041012261 0.0093473895 -0.014161808 -0.029259957 -0.037046488 -0.037997007][0.15274537 0.14558107 0.13546732 0.12910558 0.12851058 0.12731649 0.11925483 0.10238048 0.074590944 0.040644556 0.0089662578 -0.014419366 -0.029611213 -0.037576843 -0.038649965][0.17190385 0.16796748 0.15882529 0.15179719 0.14849335 0.14309502 0.13043831 0.10984237 0.079421327 0.043504924 0.010370556 -0.013755661 -0.029332759 -0.037653748 -0.039106827][0.17859477 0.17850657 0.17272556 0.16820294 0.16510129 0.15762153 0.141597 0.11794429 0.084852479 0.046475317 0.011338772 -0.013774957 -0.029423794 -0.037635043 -0.039165296][0.16375825 0.16868974 0.16963196 0.17150281 0.17223713 0.16543786 0.1479318 0.12254249 0.087678254 0.047394853 0.010652577 -0.015134927 -0.030425668 -0.038080372 -0.039412115][0.12955707 0.13913539 0.14719246 0.15648533 0.16256441 0.1584928 0.14198299 0.1174195 0.083459266 0.043853179 0.0076994062 -0.017366583 -0.031658709 -0.038521864 -0.039555818][0.087108076 0.098242193 0.11046689 0.12487506 0.13545515 0.1351687 0.12200121 0.10084619 0.070559829 0.034613725 0.0017715608 -0.020747427 -0.033122949 -0.038949586 -0.039571047]]...]
INFO - root - 2017-12-10 11:10:51.947291: step 9710, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 72h:41m:41s remains)
INFO - root - 2017-12-10 11:10:59.803097: step 9720, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 71h:06m:54s remains)
INFO - root - 2017-12-10 11:11:07.534581: step 9730, loss = 0.69, batch loss = 0.64 (10.7 examples/sec; 0.751 sec/batch; 67h:20m:34s remains)
INFO - root - 2017-12-10 11:11:15.363645: step 9740, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 69h:12m:37s remains)
INFO - root - 2017-12-10 11:11:23.187884: step 9750, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 70h:25m:54s remains)
INFO - root - 2017-12-10 11:11:31.004719: step 9760, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 69h:35m:34s remains)
INFO - root - 2017-12-10 11:11:38.715373: step 9770, loss = 0.72, batch loss = 0.66 (10.7 examples/sec; 0.751 sec/batch; 67h:19m:55s remains)
INFO - root - 2017-12-10 11:11:46.430495: step 9780, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.755 sec/batch; 67h:40m:34s remains)
INFO - root - 2017-12-10 11:11:54.120676: step 9790, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 69h:40m:37s remains)
INFO - root - 2017-12-10 11:12:01.821348: step 9800, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 70h:47m:33s remains)
2017-12-10 11:12:02.670214: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.030331168 0.029468102 0.029236391 0.029527303 0.030318307 0.031319942 0.032067191 0.032498658 0.032663394 0.03254262 0.034938686 0.03930597 0.042192638 0.042867053 0.041544918][0.041520394 0.041648608 0.042428844 0.044088677 0.046326634 0.048242144 0.049187038 0.049369518 0.049100053 0.048408028 0.051199853 0.056551382 0.059968974 0.060771778 0.059405208][0.043632619 0.045762151 0.048744831 0.052659623 0.056842946 0.059653636 0.06040588 0.059840661 0.058631491 0.05697022 0.05956389 0.065340184 0.069267675 0.070705175 0.07020057][0.043979358 0.048743673 0.05442955 0.060924355 0.0672128 0.070905708 0.071351081 0.069718227 0.067355707 0.064760081 0.066773437 0.0721845 0.076027334 0.077491015 0.0768472][0.044595432 0.051718973 0.059670154 0.06790553 0.075490631 0.079548374 0.07953921 0.077020571 0.073742986 0.070861816 0.072527334 0.077400625 0.081148319 0.082343772 0.080747128][0.039195109 0.048508015 0.058103986 0.067400962 0.075675175 0.07971929 0.079311021 0.076356739 0.072799876 0.070041165 0.071101353 0.07496921 0.0786021 0.079862505 0.077981547][0.028750814 0.039895959 0.050851557 0.061051156 0.070065774 0.074931085 0.075395107 0.073207766 0.070382312 0.068186976 0.068350092 0.0703476 0.07301908 0.073855646 0.071790263][0.017761368 0.029517246 0.041239455 0.052204188 0.062250406 0.068882555 0.0715688 0.0715492 0.070521578 0.069148213 0.068331182 0.068205036 0.069143586 0.068477921 0.065389186][0.013564522 0.02458901 0.035651647 0.04623941 0.056341235 0.063986771 0.068644308 0.071211249 0.073069938 0.07406044 0.074225925 0.073724948 0.0737347 0.071503878 0.066590033][0.013120401 0.021975445 0.030793717 0.039647114 0.0484711 0.055644702 0.060988635 0.06580174 0.071203239 0.076014273 0.079319887 0.080895007 0.081915349 0.079343408 0.073255695][0.013454009 0.019941067 0.026390377 0.033477664 0.0408212 0.0469554 0.0520416 0.057879884 0.065644808 0.073568635 0.080050126 0.084322713 0.087183073 0.085490473 0.079589404][0.014560004 0.019301388 0.024246903 0.030442268 0.037066095 0.042772166 0.04781694 0.054050021 0.062600859 0.071667038 0.079628624 0.0854624 0.089594565 0.088756032 0.083314963][0.017158797 0.020557832 0.024378652 0.030012298 0.036329452 0.042054791 0.047372706 0.053714834 0.061645098 0.069675326 0.076790482 0.082343109 0.086610675 0.086253315 0.081640661][0.021182695 0.023498243 0.025921093 0.030011479 0.034727991 0.039315417 0.04414105 0.050180621 0.057202004 0.063954391 0.069991305 0.07516396 0.0796785 0.080236211 0.0771055][0.026504721 0.028098259 0.028995596 0.03088492 0.033108842 0.035714556 0.039445397 0.044964511 0.051222395 0.056992523 0.062322352 0.067410089 0.07248877 0.074406132 0.073344842]]...]
INFO - root - 2017-12-10 11:12:10.535874: step 9810, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 72h:07m:02s remains)
INFO - root - 2017-12-10 11:12:18.447245: step 9820, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 69h:24m:27s remains)
INFO - root - 2017-12-10 11:12:26.235349: step 9830, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 69h:48m:49s remains)
INFO - root - 2017-12-10 11:12:34.058726: step 9840, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 68h:56m:54s remains)
INFO - root - 2017-12-10 11:12:41.914264: step 9850, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.816 sec/batch; 73h:07m:31s remains)
INFO - root - 2017-12-10 11:12:49.746771: step 9860, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 69h:10m:11s remains)
INFO - root - 2017-12-10 11:12:57.408515: step 9870, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 71h:57m:02s remains)
INFO - root - 2017-12-10 11:13:05.342330: step 9880, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 72h:33m:53s remains)
INFO - root - 2017-12-10 11:13:13.062538: step 9890, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 71h:13m:12s remains)
INFO - root - 2017-12-10 11:13:20.800345: step 9900, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.751 sec/batch; 67h:16m:55s remains)
2017-12-10 11:13:21.649875: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.025835877 0.036556534 0.041282665 0.040547878 0.03645676 0.029473167 0.025803411 0.026418367 0.025785653 0.021043038 0.013147275 0.0060781096 0.0018817702 0.0018073802 0.0055339606][0.073969334 0.095422782 0.10721486 0.11114548 0.11022132 0.10353676 0.10039379 0.10169005 0.1005389 0.092473879 0.076735646 0.058942195 0.044180688 0.037230715 0.038908076][0.13089502 0.16458671 0.18457186 0.19348769 0.19590284 0.19010288 0.1880836 0.19060311 0.18975167 0.17817356 0.15308569 0.12189209 0.093039796 0.075203478 0.072174735][0.1829291 0.22786984 0.25595003 0.26981509 0.27523351 0.271109 0.27172872 0.27731481 0.27788255 0.26283041 0.22764421 0.18254152 0.1379334 0.10620833 0.095090747][0.22740376 0.28145659 0.31617159 0.33382094 0.34096676 0.33858833 0.34352964 0.35472623 0.3586131 0.34104574 0.29692727 0.23863082 0.17786042 0.12990205 0.1069619][0.26643771 0.32829633 0.36860281 0.38895655 0.39567909 0.39292493 0.40087059 0.41672912 0.42276713 0.40251023 0.35105154 0.28162611 0.20584123 0.14187843 0.10638624][0.29077503 0.35768026 0.4014498 0.42250991 0.42582294 0.41979793 0.42857417 0.44785362 0.45589781 0.43475002 0.38062045 0.30544367 0.21882562 0.14101101 0.093300462][0.30414113 0.37244332 0.41666135 0.43574503 0.43294895 0.42103645 0.42809555 0.44962269 0.46049768 0.44153836 0.38970134 0.31455997 0.22235046 0.13444556 0.075419113][0.30530739 0.37278375 0.41599056 0.43309671 0.42587569 0.40968972 0.4161272 0.44135311 0.45760712 0.44345883 0.3960461 0.32354817 0.22917567 0.13380033 0.064412661][0.29687858 0.36471066 0.40827963 0.42530343 0.41630489 0.39762759 0.40404919 0.43369484 0.45692179 0.44949654 0.40814784 0.34036371 0.24672785 0.14680538 0.069453984][0.27991363 0.34716719 0.3899141 0.40631625 0.39596716 0.37466177 0.37887862 0.410082 0.43870693 0.43858168 0.40529463 0.34504983 0.25657719 0.15825309 0.079715319][0.24825628 0.30973312 0.34740609 0.36021864 0.34720102 0.32291245 0.32277608 0.35182634 0.3836965 0.39210609 0.36989591 0.32053927 0.24235548 0.15326186 0.081613615][0.1960416 0.2465952 0.27643025 0.28425086 0.26904532 0.24350126 0.23962152 0.26466602 0.29762173 0.31359568 0.30202991 0.26413456 0.19964735 0.12606676 0.067985706][0.12179143 0.15875654 0.18050581 0.1849343 0.1715738 0.15029603 0.14757429 0.17051944 0.203388 0.22362848 0.21927075 0.19126233 0.1415156 0.086039618 0.044472795][0.055476833 0.077901691 0.089876026 0.089907825 0.078333437 0.062314615 0.061344121 0.08126577 0.11046763 0.13047355 0.13102557 0.11356397 0.080692008 0.044563439 0.019365424]]...]
INFO - root - 2017-12-10 11:13:29.549766: step 9910, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 72h:05m:18s remains)
INFO - root - 2017-12-10 11:13:37.399605: step 9920, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 69h:57m:36s remains)
INFO - root - 2017-12-10 11:13:45.257964: step 9930, loss = 0.73, batch loss = 0.67 (10.3 examples/sec; 0.780 sec/batch; 69h:51m:42s remains)
INFO - root - 2017-12-10 11:13:53.128438: step 9940, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 71h:33m:56s remains)
INFO - root - 2017-12-10 11:14:00.817290: step 9950, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 70h:37m:03s remains)
INFO - root - 2017-12-10 11:14:08.788458: step 9960, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 70h:10m:54s remains)
INFO - root - 2017-12-10 11:14:16.408450: step 9970, loss = 0.70, batch loss = 0.65 (13.7 examples/sec; 0.585 sec/batch; 52h:26m:07s remains)
INFO - root - 2017-12-10 11:14:24.098300: step 9980, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 69h:49m:09s remains)
INFO - root - 2017-12-10 11:14:31.870096: step 9990, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 69h:01m:33s remains)
INFO - root - 2017-12-10 11:14:39.748066: step 10000, loss = 0.72, batch loss = 0.66 (9.7 examples/sec; 0.821 sec/batch; 73h:32m:44s remains)
2017-12-10 11:14:40.667336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.045804974 -0.034578372 -0.02273716 -0.014537027 -0.012480965 -0.014912388 -0.018656243 -0.024557404 -0.034683466 -0.048279725 -0.062797889 -0.07597553 -0.087452963 -0.095728189 -0.098619126][-0.018903527 0.0032833731 0.028086696 0.049395625 0.061853051 0.065467745 0.064032093 0.05615858 0.039357804 0.01555148 -0.010483351 -0.035801731 -0.060464181 -0.08109612 -0.092578083][0.019435273 0.058454081 0.10368092 0.14602497 0.17599629 0.19150178 0.19712633 0.19111072 0.16970377 0.13392761 0.09097705 0.045161754 -0.0025736506 -0.044703625 -0.071547449][0.064605676 0.12616634 0.19863328 0.26881123 0.32389119 0.36035687 0.38217917 0.38522348 0.36187115 0.31005031 0.24035005 0.16131845 0.078629538 0.0057412339 -0.042568706][0.10162677 0.18434723 0.28290066 0.38122162 0.46528167 0.5301984 0.57499093 0.59069687 0.56417346 0.49115622 0.38902304 0.27339187 0.1561534 0.0548866 -0.012516358][0.12159295 0.21946582 0.33801094 0.46101186 0.57485145 0.67080474 0.73776269 0.76104558 0.72492719 0.62796313 0.49643436 0.35284689 0.21289207 0.094377078 0.014658875][0.13066882 0.23825867 0.37169203 0.51512063 0.65393615 0.77239084 0.84775078 0.86310822 0.80714339 0.68704832 0.53710049 0.38179684 0.23565498 0.1132127 0.029670626][0.12994008 0.2411727 0.3823669 0.53641719 0.68557191 0.80810869 0.87315434 0.86768746 0.79057807 0.65880048 0.507841 0.35766321 0.21983469 0.10550606 0.02721164][0.11917526 0.22708665 0.36551216 0.51416874 0.65187305 0.75655866 0.7971198 0.76871294 0.67991984 0.55353069 0.41922793 0.28914973 0.17245747 0.077317417 0.012682297][0.099543236 0.19641007 0.31890646 0.44321048 0.547501 0.61636388 0.62661964 0.58087808 0.49328965 0.38730097 0.28380281 0.18668997 0.1016923 0.033888552 -0.011494232][0.074414067 0.15470609 0.25261813 0.34284282 0.40607491 0.43717581 0.42362034 0.37097219 0.29385656 0.21286885 0.1420089 0.078930847 0.025640497 -0.015634544 -0.042112887][0.041674331 0.10172034 0.17157722 0.22831534 0.25746655 0.26286396 0.23930074 0.19067563 0.12874477 0.070230842 0.025091188 -0.012151383 -0.041488785 -0.061968755 -0.072209917][0.0016051293 0.03906367 0.081187636 0.11076601 0.11909679 0.11421978 0.093343861 0.0568648 0.013174656 -0.025374055 -0.051971216 -0.072437674 -0.086549737 -0.093279414 -0.092335224][-0.035008643 -0.016734255 0.0038003437 0.015645329 0.014412983 0.00799666 -0.005515255 -0.02798696 -0.054217461 -0.076456852 -0.090564035 -0.10100084 -0.10674626 -0.10643233 -0.10044725][-0.062816292 -0.05727084 -0.049565833 -0.046208333 -0.049397368 -0.053842749 -0.060624365 -0.071976013 -0.085524954 -0.097251818 -0.1046335 -0.10989064 -0.11161541 -0.10851359 -0.10139227]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 11:14:49.367783: step 10010, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 69h:39m:37s remains)
INFO - root - 2017-12-10 11:14:57.264878: step 10020, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 70h:54m:42s remains)
INFO - root - 2017-12-10 11:15:04.870684: step 10030, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 69h:09m:54s remains)
INFO - root - 2017-12-10 11:15:12.745223: step 10040, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 70h:15m:57s remains)
INFO - root - 2017-12-10 11:15:20.559612: step 10050, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 70h:51m:19s remains)
INFO - root - 2017-12-10 11:15:28.081654: step 10060, loss = 0.69, batch loss = 0.64 (10.7 examples/sec; 0.750 sec/batch; 67h:11m:42s remains)
INFO - root - 2017-12-10 11:15:35.908266: step 10070, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 69h:31m:12s remains)
INFO - root - 2017-12-10 11:15:43.778672: step 10080, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 71h:15m:33s remains)
INFO - root - 2017-12-10 11:15:51.625952: step 10090, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.769 sec/batch; 68h:53m:16s remains)
INFO - root - 2017-12-10 11:15:59.516378: step 10100, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 70h:53m:43s remains)
2017-12-10 11:16:00.290373: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19118509 0.18439004 0.1777436 0.17269221 0.16626658 0.15829714 0.14640155 0.12909077 0.11142432 0.098727711 0.091333009 0.08799015 0.091393553 0.1048852 0.12534384][0.20343809 0.2027204 0.19545472 0.18443523 0.17011815 0.15557741 0.1401829 0.12439745 0.11170231 0.10209536 0.093742482 0.0860331 0.084659681 0.095592268 0.11771668][0.20209789 0.2094387 0.20533639 0.19194445 0.17121513 0.14814813 0.12585832 0.10861253 0.0984466 0.091384239 0.083008505 0.072409928 0.066670515 0.073364258 0.09368708][0.19254224 0.20881324 0.21186443 0.20106593 0.17703596 0.14533901 0.11329073 0.089803807 0.077451266 0.070735455 0.062976278 0.051127497 0.041502021 0.042106848 0.055948392][0.17473491 0.1960762 0.20487961 0.19847758 0.17513575 0.13965492 0.10147935 0.072876059 0.058022793 0.051557016 0.045480996 0.034528159 0.023414338 0.019099286 0.024990045][0.15568519 0.17774117 0.18956654 0.18772532 0.16838983 0.13457465 0.095271684 0.064402469 0.047957562 0.041712582 0.03780983 0.030044172 0.021249266 0.016097069 0.016632702][0.1477042 0.16871209 0.18065573 0.18199219 0.16798671 0.1393141 0.10210887 0.070175804 0.051666442 0.044513702 0.042564586 0.039877303 0.037412826 0.037168022 0.038348321][0.15315279 0.17318305 0.18428348 0.18813166 0.18037619 0.15941022 0.12742373 0.096212223 0.075823776 0.0671821 0.065985329 0.067137353 0.070981644 0.077661231 0.083126836][0.16954613 0.18751323 0.19518374 0.19858265 0.19607307 0.18399389 0.1600474 0.13298808 0.1138185 0.10552495 0.10402471 0.10606154 0.11436731 0.12866341 0.14138108][0.18139619 0.19500071 0.19700916 0.1983636 0.20168883 0.20146888 0.19086286 0.17515767 0.16442116 0.1609073 0.15917723 0.15932925 0.16978574 0.19088051 0.21194963][0.18654457 0.19627778 0.19338925 0.19240513 0.19929481 0.20729107 0.20769902 0.20410091 0.20431885 0.20767292 0.20664021 0.20483759 0.21589983 0.24098469 0.26790664][0.1795909 0.18774745 0.18282765 0.18009274 0.18668783 0.19624473 0.20171811 0.20649098 0.21608931 0.22675879 0.22927238 0.22907725 0.24163157 0.26826829 0.2973803][0.14751558 0.15503222 0.15066335 0.1475122 0.15163732 0.15809284 0.1633262 0.17096011 0.1842957 0.19821371 0.20339888 0.20562574 0.21824314 0.24197856 0.26738676][0.089584306 0.094075508 0.090169311 0.08664795 0.087547071 0.089728944 0.092831008 0.10039368 0.11345682 0.12664582 0.13195243 0.1345906 0.14455937 0.16214837 0.18063602][0.020772623 0.022020962 0.019242108 0.016220132 0.015377413 0.015295117 0.017129613 0.023679517 0.03454712 0.044992723 0.048908133 0.05051234 0.056996662 0.068530723 0.080466025]]...]
INFO - root - 2017-12-10 11:16:08.011702: step 10110, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.831 sec/batch; 74h:23m:55s remains)
INFO - root - 2017-12-10 11:16:15.945154: step 10120, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 70h:46m:52s remains)
INFO - root - 2017-12-10 11:16:23.918616: step 10130, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.820 sec/batch; 73h:23m:59s remains)
INFO - root - 2017-12-10 11:16:31.766731: step 10140, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 71h:01m:30s remains)
INFO - root - 2017-12-10 11:16:39.497137: step 10150, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 70h:47m:42s remains)
INFO - root - 2017-12-10 11:16:47.318126: step 10160, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 69h:47m:44s remains)
INFO - root - 2017-12-10 11:16:55.217454: step 10170, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 71h:47m:17s remains)
INFO - root - 2017-12-10 11:17:03.200429: step 10180, loss = 0.72, batch loss = 0.66 (9.8 examples/sec; 0.815 sec/batch; 72h:56m:01s remains)
INFO - root - 2017-12-10 11:17:10.985502: step 10190, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 70h:10m:51s remains)
INFO - root - 2017-12-10 11:17:18.836306: step 10200, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 69h:08m:44s remains)
2017-12-10 11:17:19.653927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.036125526 -0.008466104 0.028924292 0.06750676 0.096052967 0.10483683 0.096651986 0.078049175 0.058334932 0.04701006 0.054263826 0.077963129 0.1030575 0.11419947 0.10341942][-0.022359898 0.018304281 0.073348276 0.13205969 0.17852454 0.19869901 0.19381413 0.17105263 0.14225692 0.12158663 0.12580977 0.15541445 0.19279505 0.21570951 0.20961452][-0.0074136662 0.04493127 0.11657363 0.19560006 0.2624355 0.29900667 0.30356672 0.28155088 0.24471436 0.21084109 0.20414655 0.23049253 0.27338147 0.30596766 0.30667067][0.0012892457 0.061331779 0.14513189 0.2408379 0.32783586 0.385435 0.40704378 0.39365479 0.35358959 0.30545124 0.27987584 0.29116747 0.3285045 0.36428922 0.37101728][0.0024200289 0.06630303 0.15731855 0.26381853 0.36706388 0.44602412 0.48834622 0.48886117 0.45066959 0.3914457 0.3450821 0.33416131 0.35657412 0.3877551 0.39657223][-0.00093339541 0.065490335 0.16249391 0.27882442 0.39851278 0.50034058 0.564942 0.57939464 0.54179394 0.46922934 0.39770398 0.3571589 0.35515657 0.37324038 0.37934607][-0.0060615465 0.061362207 0.16317518 0.28891939 0.42535853 0.54986119 0.6347338 0.65865606 0.61615235 0.52572978 0.42582339 0.35183656 0.31953475 0.31755471 0.31615919][-0.011796975 0.053750347 0.15553698 0.2844294 0.43019235 0.5684005 0.66459316 0.69068182 0.63987738 0.53383374 0.41269949 0.3138698 0.256572 0.23508677 0.2248126][-0.016562767 0.044668268 0.14082049 0.26316887 0.40395796 0.539285 0.63259643 0.65320909 0.5951736 0.48376304 0.35890639 0.25463229 0.18740399 0.15420912 0.13702506][-0.021994608 0.03308329 0.11912202 0.22663558 0.34852871 0.46414998 0.54057443 0.54928255 0.48677492 0.3801893 0.26727089 0.17484939 0.11317089 0.079292096 0.0609202][-0.030086236 0.016703926 0.089445241 0.17739442 0.27231544 0.35795146 0.40912202 0.40360722 0.34085158 0.24620025 0.15303838 0.0808588 0.033874433 0.0081023565 -0.0051236805][-0.04135894 -0.0068785329 0.047947373 0.11194968 0.17563732 0.22727577 0.2516357 0.23526871 0.17926893 0.10387608 0.0362754 -0.010493187 -0.037413459 -0.050017107 -0.05474972][-0.054130871 -0.034153618 8.9502337e-06 0.038485993 0.071963891 0.092931308 0.095642775 0.074973673 0.032253906 -0.018597551 -0.05764747 -0.077731267 -0.084115706 -0.08353626 -0.080525286][-0.065112934 -0.057645932 -0.041665506 -0.024542987 -0.013760479 -0.013486479 -0.023171568 -0.042224918 -0.069684252 -0.097435005 -0.11283419 -0.11323518 -0.1057161 -0.096824348 -0.089299224][-0.072629057 -0.07347665 -0.070144638 -0.067310289 -0.069404326 -0.078258976 -0.091372363 -0.10603992 -0.12131281 -0.13303594 -0.13465519 -0.12620369 -0.11373971 -0.10210773 -0.092819631]]...]
INFO - root - 2017-12-10 11:17:27.563642: step 10210, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 70h:44m:03s remains)
INFO - root - 2017-12-10 11:17:35.451088: step 10220, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.764 sec/batch; 68h:24m:09s remains)
INFO - root - 2017-12-10 11:17:43.359405: step 10230, loss = 0.72, batch loss = 0.66 (9.8 examples/sec; 0.813 sec/batch; 72h:44m:26s remains)
INFO - root - 2017-12-10 11:17:51.132252: step 10240, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.796 sec/batch; 71h:15m:34s remains)
INFO - root - 2017-12-10 11:17:59.106640: step 10250, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 70h:59m:50s remains)
INFO - root - 2017-12-10 11:18:06.921605: step 10260, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 69h:46m:00s remains)
INFO - root - 2017-12-10 11:18:14.726732: step 10270, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 69h:39m:51s remains)
INFO - root - 2017-12-10 11:18:22.703834: step 10280, loss = 0.68, batch loss = 0.62 (8.6 examples/sec; 0.935 sec/batch; 83h:39m:34s remains)
INFO - root - 2017-12-10 11:18:30.610580: step 10290, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 71h:15m:52s remains)
INFO - root - 2017-12-10 11:18:38.439721: step 10300, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 69h:37m:50s remains)
2017-12-10 11:18:39.233556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0110439 -0.016882073 -0.021461979 -0.023402227 -0.021839812 -0.017929178 -0.015030948 -0.014532656 -0.011597762 -0.0028533679 0.014552484 0.040682044 0.067919143 0.0878398 0.090131223][0.0070255529 -0.0022920074 -0.011132429 -0.015720898 -0.013657196 -0.0073180022 -0.0025841466 -0.0027765075 -0.0034505788 -0.00027480174 0.010787478 0.031512138 0.0557762 0.0755913 0.080642946][0.052699044 0.040298626 0.026333312 0.018105134 0.020224126 0.029897023 0.038122553 0.038394757 0.03410976 0.030229323 0.031577781 0.041390467 0.055272058 0.066743247 0.067156017][0.12895837 0.1159191 0.096998781 0.083637349 0.083831534 0.096424714 0.10993401 0.11375242 0.10881511 0.0993989 0.089922354 0.085045904 0.083143331 0.079808064 0.068020433][0.21951552 0.20858862 0.18567546 0.16699348 0.16436285 0.18001679 0.20087959 0.21143293 0.20869093 0.19465388 0.17378752 0.15253885 0.13232104 0.11111768 0.082917869][0.30339405 0.29633272 0.27055329 0.24793865 0.24362426 0.26329541 0.29317406 0.31254709 0.31345412 0.29465681 0.26198167 0.22441953 0.1865626 0.14771703 0.10260949][0.37078303 0.366027 0.33652151 0.31087539 0.30646896 0.33052418 0.36889064 0.39709407 0.40302381 0.38200897 0.34121674 0.29071268 0.23680821 0.18053339 0.11861536][0.41204029 0.40340066 0.36698383 0.33774981 0.33321068 0.35970286 0.40326321 0.43872285 0.45195949 0.43529648 0.39511478 0.33858907 0.27138287 0.1981034 0.12074412][0.41840011 0.4009138 0.35607424 0.3223812 0.31540138 0.33900681 0.38070258 0.41807392 0.43724391 0.4290016 0.39788961 0.34502137 0.27271083 0.18939158 0.10410026][0.39511433 0.36897928 0.31760702 0.27898616 0.265143 0.2780835 0.30920437 0.34127584 0.36219814 0.36277583 0.34579042 0.30567408 0.23983796 0.15868166 0.076869667][0.36229983 0.33180669 0.27706635 0.23226637 0.20604134 0.20043969 0.21287642 0.23307568 0.25173426 0.26064986 0.260118 0.23888093 0.18955792 0.12184546 0.052400958][0.34147453 0.31123966 0.25493124 0.20170143 0.15855381 0.12953514 0.11856899 0.123178 0.13752927 0.15493269 0.17207246 0.17293794 0.14588794 0.097711444 0.043088503][0.33757719 0.31054568 0.25308606 0.19157577 0.13307439 0.083582446 0.051609416 0.041324914 0.050827023 0.074934892 0.10656457 0.12647104 0.12024368 0.090478987 0.047413956][0.34062055 0.31693542 0.25994688 0.19530402 0.13008268 0.070160694 0.025423525 0.0042913058 0.0085733645 0.034411348 0.0726249 0.10311521 0.11056574 0.093275793 0.056553897][0.34170011 0.32123226 0.26639953 0.20384835 0.14022544 0.080107361 0.032971263 0.00853122 0.010669258 0.035763487 0.0743724 0.10734353 0.11971087 0.10665642 0.06972564]]...]
INFO - root - 2017-12-10 11:18:47.133663: step 10310, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 69h:22m:07s remains)
INFO - root - 2017-12-10 11:18:54.955701: step 10320, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 69h:48m:47s remains)
INFO - root - 2017-12-10 11:19:02.688640: step 10330, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 69h:56m:53s remains)
INFO - root - 2017-12-10 11:19:10.588255: step 10340, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 71h:36m:32s remains)
INFO - root - 2017-12-10 11:19:18.322594: step 10350, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 70h:30m:22s remains)
INFO - root - 2017-12-10 11:19:26.226998: step 10360, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.820 sec/batch; 73h:22m:26s remains)
INFO - root - 2017-12-10 11:19:34.146318: step 10370, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 72h:19m:55s remains)
INFO - root - 2017-12-10 11:19:41.961648: step 10380, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 71h:23m:13s remains)
INFO - root - 2017-12-10 11:19:49.837350: step 10390, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 69h:42m:46s remains)
INFO - root - 2017-12-10 11:19:57.674052: step 10400, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 69h:31m:32s remains)
2017-12-10 11:19:58.531586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.020986596 -0.016977366 -0.0059318244 0.012894846 0.036782533 0.057911646 0.066987649 0.060381882 0.041482069 0.01657044 -0.0058544925 -0.018851986 -0.021312678 -0.017432276 -0.012526515][-0.023459524 -0.018315982 -0.0052318508 0.016332161 0.042117506 0.063216135 0.069919549 0.059138972 0.035116971 0.00538855 -0.019725962 -0.032658186 -0.032453362 -0.02481859 -0.016459519][-0.021317108 -0.01248976 0.0063474425 0.036177922 0.071367845 0.10061166 0.11152382 0.099117339 0.06716001 0.024732953 -0.013568358 -0.036114365 -0.040170588 -0.032603845 -0.022291431][-0.0063249515 0.0093537252 0.038792841 0.083773971 0.13698605 0.18286532 0.20374171 0.19067971 0.14631048 0.082362555 0.019834222 -0.022941696 -0.03973658 -0.038055241 -0.029110517][0.031110063 0.056749698 0.10128094 0.16698377 0.24392231 0.31123847 0.3447395 0.33093092 0.27110815 0.18080893 0.087870486 0.017379571 -0.020363007 -0.032295108 -0.030376207][0.08696913 0.12306353 0.18372539 0.27105346 0.3721354 0.4611316 0.5079354 0.49530786 0.42264089 0.30889857 0.18668339 0.08599785 0.021303482 -0.011527474 -0.02252749][0.14816685 0.19381614 0.26779956 0.37184647 0.4904266 0.59510803 0.65286112 0.64462632 0.56751227 0.44077888 0.29783064 0.17043833 0.077108443 0.0191037 -0.0088526765][0.19497849 0.24632497 0.32667866 0.4366127 0.5592249 0.667303 0.72942364 0.72738284 0.65594822 0.53191549 0.38424972 0.24254453 0.12804632 0.0486747 0.0051898197][0.20815447 0.25707 0.33270791 0.43417796 0.54496938 0.64269781 0.7016713 0.7066502 0.65052092 0.54538351 0.41239652 0.2753728 0.15556486 0.066424504 0.014366792][0.18296088 0.22266008 0.28398952 0.36539912 0.45300817 0.53132254 0.58212179 0.59365261 0.55753285 0.48047525 0.37526762 0.25841266 0.14913152 0.0640975 0.013494034][0.1276572 0.15424986 0.1961305 0.25175557 0.31138831 0.36650184 0.40593982 0.42104316 0.40337473 0.35516354 0.28234437 0.19471943 0.10815097 0.03962037 0.00044724275][0.064617373 0.077628106 0.099509478 0.12934683 0.16168095 0.19355722 0.21952698 0.23353991 0.22826901 0.20340404 0.16064642 0.10484139 0.047643833 0.0033406527 -0.018279931][0.012222963 0.01437171 0.020467447 0.03007094 0.041017778 0.053736009 0.066692173 0.0761839 0.076862462 0.067651853 0.048338331 0.020674389 -0.00798485 -0.027807944 -0.032359511][-0.022789784 -0.027213007 -0.030407919 -0.032502729 -0.033827543 -0.03274541 -0.028917359 -0.024251228 -0.021841191 -0.02270611 -0.027400792 -0.035590485 -0.043279111 -0.045127507 -0.038141802][-0.038274966 -0.044478517 -0.0504055 -0.056181852 -0.061491493 -0.064971589 -0.065998033 -0.065327741 -0.064175285 -0.0629609 -0.061823778 -0.060614567 -0.057448998 -0.050045263 -0.037840236]]...]
INFO - root - 2017-12-10 11:20:06.468449: step 10410, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.837 sec/batch; 74h:54m:17s remains)
INFO - root - 2017-12-10 11:20:14.169347: step 10420, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 68h:31m:42s remains)
INFO - root - 2017-12-10 11:20:21.936280: step 10430, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 72h:05m:02s remains)
INFO - root - 2017-12-10 11:20:29.788989: step 10440, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 69h:41m:55s remains)
INFO - root - 2017-12-10 11:20:37.558365: step 10450, loss = 0.71, batch loss = 0.66 (10.5 examples/sec; 0.763 sec/batch; 68h:12m:58s remains)
INFO - root - 2017-12-10 11:20:45.366194: step 10460, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 68h:13m:36s remains)
INFO - root - 2017-12-10 11:20:53.234087: step 10470, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 71h:26m:10s remains)
INFO - root - 2017-12-10 11:21:01.056189: step 10480, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 70h:20m:23s remains)
INFO - root - 2017-12-10 11:21:08.947538: step 10490, loss = 0.71, batch loss = 0.66 (9.7 examples/sec; 0.824 sec/batch; 73h:41m:15s remains)
INFO - root - 2017-12-10 11:21:16.832842: step 10500, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.777 sec/batch; 69h:27m:32s remains)
2017-12-10 11:21:17.542981: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0242452 0.022873925 0.017620031 0.0071342033 -0.0061145 -0.017348563 -0.022447575 -0.019524077 -0.0086329924 0.0094269561 0.033394564 0.060564097 0.0850586 0.1025609 0.11128204][0.059732292 0.058686793 0.05071535 0.034439676 0.014159501 -0.0020997219 -0.0087636858 -0.0041224626 0.010157756 0.031291742 0.056851931 0.083368428 0.10499021 0.11759134 0.1201717][0.098349676 0.098041862 0.088075578 0.067284256 0.041635655 0.02174532 0.014121146 0.019894741 0.035748307 0.057412129 0.08115793 0.10330488 0.11872799 0.12408056 0.11946438][0.13172574 0.1317649 0.1205629 0.097854741 0.070872314 0.050754204 0.043614611 0.049736071 0.0647676 0.083758652 0.10215449 0.11689556 0.12434592 0.12210619 0.11097893][0.1538538 0.15239438 0.1401955 0.1181323 0.094052218 0.077489845 0.072787985 0.079152413 0.091779314 0.10564356 0.11631626 0.12217527 0.12116523 0.11175003 0.095380343][0.16228114 0.15949042 0.1472427 0.12826633 0.11001351 0.099212624 0.097712062 0.10402039 0.11340711 0.12108768 0.12353236 0.12083047 0.112244 0.097239383 0.077897422][0.1557298 0.15433739 0.14532578 0.13305137 0.12369993 0.12049859 0.12250109 0.12749138 0.13171782 0.13180338 0.12565593 0.11504879 0.10059946 0.082961991 0.064039819][0.1449794 0.14796056 0.14520991 0.14165592 0.14184876 0.14531302 0.14876355 0.14951314 0.14578153 0.1367739 0.12264796 0.10650833 0.08940164 0.072654612 0.057424974][0.13699879 0.14544079 0.14913221 0.15289427 0.15986393 0.16688307 0.16905716 0.16342564 0.15082881 0.13385367 0.11480267 0.096903466 0.080793642 0.068049885 0.058681097][0.13487908 0.14819854 0.15564625 0.16297257 0.17258134 0.18015862 0.17979579 0.16854496 0.1497943 0.12896293 0.10973646 0.094105318 0.0815005 0.073249057 0.068874054][0.13931032 0.15470618 0.16217719 0.16884591 0.17676042 0.18222438 0.17948727 0.16599506 0.14667663 0.12794854 0.11343069 0.10296872 0.09431316 0.0883795 0.085463241][0.1470125 0.16213639 0.1676916 0.17085955 0.17387046 0.17523883 0.17115794 0.1597275 0.14588794 0.13452826 0.12740347 0.122025 0.11484651 0.10688636 0.10037203][0.15374404 0.16711007 0.16985786 0.16807893 0.16442575 0.1609662 0.15725018 0.15146659 0.14689988 0.14484495 0.14438319 0.14147893 0.13211057 0.11785007 0.10366844][0.15340379 0.16354755 0.16292594 0.15621418 0.14644553 0.13938114 0.13758472 0.13884616 0.1435363 0.14896555 0.15213926 0.14881052 0.13533299 0.11363835 0.090970665][0.15026447 0.15686888 0.15302652 0.1428121 0.12968785 0.12174793 0.12319037 0.13068369 0.14146088 0.15028937 0.15346339 0.14749908 0.12955686 0.10128002 0.070773758]]...]
INFO - root - 2017-12-10 11:21:25.356961: step 10510, loss = 0.71, batch loss = 0.65 (8.7 examples/sec; 0.918 sec/batch; 82h:05m:38s remains)
INFO - root - 2017-12-10 11:21:33.219445: step 10520, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.799 sec/batch; 71h:30m:17s remains)
INFO - root - 2017-12-10 11:21:41.100416: step 10530, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 70h:26m:51s remains)
INFO - root - 2017-12-10 11:21:48.899735: step 10540, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 69h:17m:24s remains)
INFO - root - 2017-12-10 11:21:56.785313: step 10550, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 69h:10m:48s remains)
INFO - root - 2017-12-10 11:22:04.592982: step 10560, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 71h:27m:01s remains)
INFO - root - 2017-12-10 11:22:12.536261: step 10570, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.833 sec/batch; 74h:31m:23s remains)
INFO - root - 2017-12-10 11:22:20.500718: step 10580, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 69h:12m:42s remains)
INFO - root - 2017-12-10 11:22:28.134424: step 10590, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.758 sec/batch; 67h:46m:36s remains)
INFO - root - 2017-12-10 11:22:35.875961: step 10600, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 70h:30m:34s remains)
2017-12-10 11:22:36.659033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.018944988 -0.011473456 -0.00044121791 0.011531629 0.020511484 0.022604162 0.016658243 0.0058445064 -0.0050177663 -0.012605201 -0.016337033 -0.014792155 -0.0068652742 0.0024945526 0.0077717691][-0.0069430335 0.013059599 0.040563449 0.070406355 0.094332017 0.10438048 0.098393939 0.081722006 0.06182953 0.04478297 0.032946605 0.030187039 0.038703568 0.0499435 0.056071918][0.011705838 0.049086191 0.099461682 0.15436259 0.19965483 0.22231528 0.21894874 0.19742642 0.16741084 0.13748807 0.11356267 0.103626 0.11053623 0.12138962 0.12686808][0.030338395 0.085653536 0.16013859 0.24187627 0.31041622 0.34847069 0.35184878 0.33092117 0.29574916 0.25648627 0.22306311 0.20651132 0.20940736 0.21471256 0.21417587][0.041372187 0.10856194 0.19954191 0.30001312 0.385431 0.43751419 0.45325717 0.44437367 0.41803941 0.38263032 0.34963951 0.33009657 0.32536536 0.31723133 0.30207404][0.039945453 0.10829622 0.20177266 0.30596125 0.39662856 0.4589344 0.4931871 0.5099237 0.50935674 0.49331295 0.47099525 0.45105949 0.43392324 0.40354025 0.3643811][0.029022569 0.090152137 0.17594987 0.27356654 0.362835 0.43518129 0.49340877 0.54312491 0.574659 0.5822444 0.57112217 0.546867 0.51073784 0.45235154 0.38602316][0.016089853 0.068590492 0.14534488 0.23544401 0.32444087 0.40923336 0.49140987 0.56830311 0.62156308 0.64205021 0.63322 0.59869981 0.54164332 0.45841461 0.3720679][0.0087796254 0.056749828 0.12976053 0.21754038 0.31030536 0.40714082 0.50489819 0.59255636 0.64879054 0.66735715 0.65238822 0.60601521 0.53321695 0.43632057 0.34345484][0.010075745 0.060348902 0.13704634 0.2287458 0.32752192 0.43130904 0.53121668 0.61133611 0.65386695 0.65947425 0.63449866 0.57933241 0.4991473 0.40060884 0.31350213][0.017870659 0.074744776 0.15936139 0.25750825 0.36013338 0.4622367 0.55045331 0.60823351 0.625282 0.61095071 0.57552618 0.51746154 0.44029769 0.35230488 0.2816301][0.024403116 0.085682996 0.17421751 0.27344283 0.37226948 0.4629761 0.53018528 0.55981457 0.54997963 0.51720744 0.47588527 0.4232676 0.36004511 0.2932792 0.24568683][0.021243196 0.077981658 0.15838625 0.24617136 0.32995272 0.40137991 0.44589332 0.45312288 0.42717591 0.38698757 0.34885937 0.31010786 0.26849079 0.22757638 0.2020724][0.0080556916 0.051655557 0.11320132 0.17929459 0.24035053 0.28928572 0.3144629 0.3094677 0.28141505 0.24746732 0.22222835 0.20249043 0.18328315 0.16491792 0.15487748][-0.0089928405 0.018083295 0.057629772 0.10010698 0.13867301 0.16807005 0.17998457 0.17116269 0.14905512 0.12729026 0.11687287 0.11403738 0.11212567 0.10923366 0.1079426]]...]
INFO - root - 2017-12-10 11:22:44.548943: step 10610, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 69h:35m:23s remains)
INFO - root - 2017-12-10 11:22:52.319691: step 10620, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.796 sec/batch; 71h:11m:02s remains)
INFO - root - 2017-12-10 11:23:00.215576: step 10630, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.819 sec/batch; 73h:13m:08s remains)
INFO - root - 2017-12-10 11:23:08.177074: step 10640, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 70h:10m:35s remains)
INFO - root - 2017-12-10 11:23:16.019902: step 10650, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 69h:36m:22s remains)
INFO - root - 2017-12-10 11:23:23.765953: step 10660, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 70h:32m:54s remains)
INFO - root - 2017-12-10 11:23:31.545550: step 10670, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 70h:44m:38s remains)
INFO - root - 2017-12-10 11:23:39.412416: step 10680, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.810 sec/batch; 72h:23m:56s remains)
INFO - root - 2017-12-10 11:23:46.999655: step 10690, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 68h:44m:49s remains)
INFO - root - 2017-12-10 11:23:54.916565: step 10700, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 72h:13m:41s remains)
2017-12-10 11:23:55.744381: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10770745 0.087632984 0.06074255 0.036601853 0.018027546 0.0057275435 -0.0014263578 -0.0054008905 -0.0057107662 -0.0017575508 0.0060884315 0.019272029 0.036888614 0.054923333 0.067801826][0.10575047 0.08145611 0.052303653 0.028606236 0.012273807 0.0024757667 -0.002941177 -0.0064249956 -0.0068844203 -0.0037569546 0.0036508986 0.017577913 0.037595645 0.05964867 0.077344187][0.086637124 0.062117349 0.035773274 0.016805388 0.00515981 -0.0010236879 -0.0044684275 -0.0075493888 -0.0089626387 -0.00767931 -0.0023623705 0.0095638894 0.028607436 0.051682487 0.072392128][0.055458661 0.035316531 0.016484207 0.0051454818 -0.00072286226 -0.0033992247 -0.005451953 -0.0085480465 -0.011587004 -0.012902712 -0.010898304 -0.0032998677 0.011337511 0.031623147 0.052177917][0.019794656 0.00809253 -0.00044454957 -0.0033015648 -0.0037985293 -0.0039716084 -0.005527325 -0.0091717914 -0.013932156 -0.017886601 -0.019344078 -0.016907571 -0.0084945075 0.0060310927 0.0229007][-0.0093333088 -0.012468282 -0.01198494 -0.00851419 -0.0054468233 -0.0043738992 -0.005872882 -0.00978883 -0.015412699 -0.020913305 -0.024772998 -0.026174355 -0.023495678 -0.015884714 -0.0050770128][-0.025506968 -0.023908928 -0.018883446 -0.012926592 -0.0086153336 -0.0068224166 -0.0076280609 -0.010485647 -0.014748341 -0.019247156 -0.022929538 -0.025772208 -0.026794592 -0.025171475 -0.021315116][-0.028787263 -0.026889792 -0.021904521 -0.016721098 -0.012716023 -0.010362548 -0.0097247241 -0.010100657 -0.011180695 -0.012535216 -0.013903239 -0.015882833 -0.018236997 -0.020628026 -0.0225696][-0.025166906 -0.024793049 -0.021761131 -0.018239874 -0.014732085 -0.011715732 -0.0094138859 -0.0071368976 -0.0049809832 -0.0032394247 -0.0021739011 -0.0025589953 -0.0046304516 -0.0085146567 -0.013508967][-0.021012351 -0.021627324 -0.019785818 -0.017002596 -0.013488543 -0.0099351583 -0.0066194921 -0.00284906 0.00088883453 0.0039652488 0.0060068513 0.0065079564 0.0051744385 0.0015413581 -0.0037235024][-0.018928377 -0.019624101 -0.018001428 -0.015293406 -0.011724761 -0.0081563424 -0.0048224414 -0.0010258186 0.0025557557 0.0052041961 0.0067471997 0.0070559434 0.0060207215 0.0030662105 -0.00098939228][-0.018806098 -0.019569673 -0.018084142 -0.015639968 -0.01248474 -0.0095023708 -0.0068814061 -0.003954411 -0.0012661441 0.0003899298 0.00094240217 0.00064700271 -0.000429497 -0.0028227719 -0.0056446721][-0.020079397 -0.021202493 -0.020305052 -0.018698391 -0.016530098 -0.014585281 -0.013049223 -0.011344929 -0.0098301675 -0.0091952328 -0.0095967846 -0.010527521 -0.011817816 -0.013776158 -0.015580197][-0.022135936 -0.023830697 -0.023792978 -0.023334928 -0.022449028 -0.02173803 -0.021404916 -0.021019781 -0.020752041 -0.02106908 -0.022120308 -0.023412546 -0.024721272 -0.026152689 -0.027082704][-0.024278466 -0.026523162 -0.027207665 -0.027762027 -0.028058205 -0.028477078 -0.029230947 -0.030043675 -0.03089256 -0.031912323 -0.033226512 -0.034503892 -0.035591118 -0.036531061 -0.036944836]]...]
INFO - root - 2017-12-10 11:24:03.662272: step 10710, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.771 sec/batch; 68h:55m:15s remains)
INFO - root - 2017-12-10 11:24:11.525972: step 10720, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.799 sec/batch; 71h:26m:30s remains)
INFO - root - 2017-12-10 11:24:19.421591: step 10730, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 67h:45m:43s remains)
INFO - root - 2017-12-10 11:24:27.226123: step 10740, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 69h:43m:22s remains)
INFO - root - 2017-12-10 11:24:34.911513: step 10750, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 71h:37m:06s remains)
INFO - root - 2017-12-10 11:24:42.778683: step 10760, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 68h:02m:07s remains)
INFO - root - 2017-12-10 11:24:50.512027: step 10770, loss = 0.68, batch loss = 0.62 (11.3 examples/sec; 0.708 sec/batch; 63h:18m:22s remains)
INFO - root - 2017-12-10 11:24:58.383313: step 10780, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 69h:47m:20s remains)
INFO - root - 2017-12-10 11:25:06.156644: step 10790, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 68h:44m:12s remains)
INFO - root - 2017-12-10 11:25:14.145326: step 10800, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 70h:38m:59s remains)
2017-12-10 11:25:14.893864: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0099567873 0.018224945 0.028653551 0.042942263 0.061334278 0.079326615 0.094552055 0.10841138 0.12194119 0.13390923 0.14711455 0.170155 0.20884848 0.26519841 0.33102688][0.048910052 0.057220224 0.069759093 0.090765908 0.11903735 0.14751801 0.17036596 0.18656753 0.19548219 0.19711183 0.19978248 0.21675721 0.25524423 0.31400421 0.3809084][0.085018881 0.092353843 0.10745284 0.137641 0.17944309 0.22233686 0.25612268 0.27658418 0.27984503 0.26656044 0.25173044 0.25480717 0.28460708 0.33608142 0.39407015][0.10874639 0.11644667 0.13727665 0.18133579 0.24134418 0.30282235 0.35111597 0.37872902 0.37797555 0.34991559 0.31630161 0.30224895 0.31698039 0.35190171 0.39100686][0.1126652 0.12612456 0.15972373 0.22349177 0.30546686 0.3875778 0.45077041 0.4850972 0.48087537 0.44095054 0.39156091 0.35943091 0.35243121 0.36000612 0.36927089][0.10549354 0.13061377 0.18142098 0.26473919 0.36471933 0.46131521 0.53264195 0.56813723 0.56011534 0.51496965 0.45863739 0.41327551 0.38254529 0.35652804 0.32939497][0.10670339 0.14297931 0.20688987 0.30103755 0.40695077 0.50469333 0.57239962 0.60198444 0.59194559 0.55177861 0.50135976 0.45247969 0.40302485 0.34543708 0.28254378][0.12596636 0.16427802 0.22780235 0.31667894 0.41099843 0.49298036 0.544715 0.56459653 0.55910456 0.53739864 0.5070377 0.46653444 0.40893954 0.33014303 0.24043998][0.15358792 0.1818895 0.23148331 0.30168152 0.37169132 0.42712563 0.45749536 0.46920189 0.47450459 0.47790372 0.47299251 0.4470821 0.39171919 0.30624783 0.20496702][0.17938542 0.19134581 0.22044599 0.266333 0.30879462 0.33756998 0.34971574 0.35737732 0.37494588 0.40055734 0.41658911 0.40423587 0.356849 0.27644119 0.17778875][0.20341328 0.20049265 0.21049696 0.23444858 0.25409773 0.26273233 0.26292342 0.26883277 0.29411697 0.33186239 0.35931569 0.35669392 0.31913453 0.24939597 0.161146][0.22574155 0.21685915 0.21668944 0.22782369 0.23451062 0.23177928 0.22387785 0.22445942 0.24621569 0.2816225 0.30894369 0.30988839 0.27980494 0.22071275 0.14484026][0.25132483 0.2476376 0.24704878 0.252901 0.25239778 0.24104731 0.22263029 0.20972267 0.21532558 0.23546322 0.25291175 0.25139809 0.22527269 0.17564513 0.1131025][0.28914016 0.29718924 0.29942355 0.29996449 0.29032412 0.26711574 0.23316674 0.20035057 0.18302485 0.18162714 0.18364716 0.17492747 0.14965861 0.10885426 0.0609361][0.346623 0.36460873 0.36373013 0.35024008 0.32266608 0.28125167 0.22926188 0.1780218 0.14186539 0.12306695 0.11196279 0.096860506 0.073147796 0.042252876 0.00981347]]...]
INFO - root - 2017-12-10 11:25:22.869008: step 10810, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 68h:42m:40s remains)
INFO - root - 2017-12-10 11:25:30.676565: step 10820, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 71h:10m:30s remains)
INFO - root - 2017-12-10 11:25:38.336734: step 10830, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 70h:07m:27s remains)
INFO - root - 2017-12-10 11:25:46.242384: step 10840, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.813 sec/batch; 72h:36m:28s remains)
INFO - root - 2017-12-10 11:25:53.970246: step 10850, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.755 sec/batch; 67h:29m:53s remains)
INFO - root - 2017-12-10 11:26:01.654602: step 10860, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 69h:44m:21s remains)
INFO - root - 2017-12-10 11:26:09.549265: step 10870, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.784 sec/batch; 70h:00m:45s remains)
INFO - root - 2017-12-10 11:26:17.419874: step 10880, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 70h:21m:39s remains)
INFO - root - 2017-12-10 11:26:25.343659: step 10890, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 70h:45m:59s remains)
INFO - root - 2017-12-10 11:26:33.180397: step 10900, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 70h:17m:20s remains)
2017-12-10 11:26:33.976601: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.229827 0.23195 0.21867806 0.19132751 0.15539035 0.12115803 0.097888559 0.094824865 0.11418332 0.1537905 0.20467979 0.24754101 0.26191786 0.23773719 0.17985782][0.3016029 0.31084248 0.30571449 0.28495106 0.24972506 0.20657989 0.16547574 0.14052378 0.14201972 0.17430997 0.22717418 0.27576271 0.29257458 0.26525798 0.19943146][0.34695867 0.36455533 0.37472671 0.3733851 0.35438782 0.31526825 0.26326722 0.21558571 0.19161038 0.2051201 0.2487094 0.29513225 0.31271023 0.28583843 0.21782832][0.35974532 0.38323122 0.40895212 0.43109515 0.43638724 0.4123956 0.362066 0.30274379 0.25967568 0.25355759 0.28043005 0.31502521 0.32636142 0.29754809 0.22951867][0.35732883 0.381363 0.41331205 0.44887581 0.47259513 0.46723768 0.43287426 0.38324842 0.34178928 0.32770926 0.33722448 0.34965539 0.34089717 0.29919294 0.22650136][0.35011551 0.36835098 0.3961122 0.43302608 0.46632108 0.48035726 0.47392765 0.45292789 0.4306902 0.41744256 0.40791073 0.38827309 0.34720832 0.28371888 0.20293385][0.33124274 0.33974615 0.35727689 0.38883108 0.42633206 0.45958969 0.4865576 0.50264752 0.50605214 0.49480349 0.46355015 0.40784162 0.33182707 0.24675173 0.16077355][0.31564462 0.30887386 0.30895776 0.32669425 0.35982057 0.40431398 0.45752361 0.50515556 0.53119695 0.52227056 0.47420868 0.39203155 0.29270181 0.19602199 0.11123599][0.31858906 0.29291672 0.26847786 0.26244393 0.27917752 0.32069412 0.38446492 0.45012966 0.49175349 0.48770449 0.43435213 0.343507 0.2391566 0.14394964 0.06643983][0.33617467 0.29680085 0.25176951 0.22237657 0.21825962 0.24633235 0.30465356 0.37094831 0.41517484 0.41302133 0.36230817 0.27808982 0.18473332 0.10227223 0.0364698][0.34647426 0.30685657 0.25661281 0.21711159 0.19954555 0.21300916 0.25565079 0.30611318 0.33684146 0.32750016 0.28005779 0.21034111 0.13770409 0.074981406 0.023114488][0.34154382 0.31021914 0.26714608 0.23038867 0.20975377 0.21380158 0.24008617 0.26962239 0.28095284 0.26108736 0.21696006 0.16401303 0.11464463 0.072869092 0.033773102][0.33745492 0.31447068 0.28074703 0.25150716 0.23446351 0.23572725 0.25116703 0.26431084 0.25992164 0.23300761 0.19442557 0.15869577 0.13143447 0.10767122 0.076535963][0.34888405 0.33158895 0.30408716 0.28151649 0.26995638 0.27252704 0.28253967 0.28505036 0.26974955 0.23901698 0.20632894 0.18461433 0.17365839 0.16137879 0.13257594][0.36686817 0.35318521 0.32980818 0.31306696 0.30706319 0.31188309 0.31792235 0.31119251 0.28598073 0.25199997 0.2250987 0.21550925 0.21697263 0.21144499 0.18133281]]...]
INFO - root - 2017-12-10 11:26:41.710782: step 10910, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.758 sec/batch; 67h:45m:04s remains)
INFO - root - 2017-12-10 11:26:49.552088: step 10920, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 71h:46m:14s remains)
INFO - root - 2017-12-10 11:26:57.357715: step 10930, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 69h:38m:42s remains)
INFO - root - 2017-12-10 11:27:05.251044: step 10940, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 70h:15m:44s remains)
INFO - root - 2017-12-10 11:27:13.026595: step 10950, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 67h:38m:34s remains)
INFO - root - 2017-12-10 11:27:20.900054: step 10960, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 69h:35m:24s remains)
INFO - root - 2017-12-10 11:27:28.748390: step 10970, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 69h:23m:23s remains)
INFO - root - 2017-12-10 11:27:36.667218: step 10980, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.812 sec/batch; 72h:30m:02s remains)
INFO - root - 2017-12-10 11:27:44.345025: step 10990, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.766 sec/batch; 68h:23m:04s remains)
INFO - root - 2017-12-10 11:27:52.218107: step 11000, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 71h:37m:23s remains)
2017-12-10 11:27:53.027064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.039171316 -0.031711008 -0.028030762 -0.030997647 -0.038942728 -0.049503572 -0.05962301 -0.066642761 -0.068363 -0.064844638 -0.057314835 -0.048573349 -0.041802358 -0.040754382 -0.046903066][-0.00059944921 0.018484302 0.029213922 0.028934281 0.020169582 0.0062237759 -0.0071365647 -0.015548415 -0.016915007 -0.012299744 -0.0038938737 0.0048489929 0.010118661 0.0069222874 -0.0064952988][0.0578292 0.095240735 0.11950056 0.12828584 0.12334023 0.10826598 0.093251593 0.085678071 0.0852515 0.0888691 0.09387169 0.097975373 0.097273469 0.085569918 0.061396196][0.1374816 0.20411469 0.25192884 0.27736071 0.27969554 0.26413175 0.24690546 0.23950233 0.23759651 0.23476335 0.22833231 0.21905139 0.20503178 0.17965016 0.14154343][0.22517505 0.32824525 0.40727955 0.45613474 0.47250342 0.46732944 0.46111152 0.46373442 0.46101761 0.44217017 0.40818843 0.36590424 0.32080549 0.2689853 0.211139][0.29457858 0.42977914 0.53786719 0.61066085 0.6465975 0.66644841 0.68959844 0.71537709 0.7159732 0.67664546 0.60521978 0.516201 0.42586392 0.33767709 0.25639927][0.33326229 0.48912856 0.616745 0.7077027 0.76324636 0.81484336 0.87446856 0.92555386 0.92966837 0.8715058 0.76737404 0.63683265 0.50549573 0.38601246 0.28666669][0.33691305 0.49805251 0.6317637 0.72988808 0.79776543 0.87325776 0.95724148 1.0206062 1.0238138 0.95432341 0.83334744 0.6811564 0.52873319 0.39606526 0.29231593][0.30527356 0.45467469 0.57842469 0.66939884 0.73688662 0.81868821 0.90522325 0.96308213 0.9613173 0.89263654 0.77731186 0.631189 0.48464891 0.36152819 0.26917574][0.24367282 0.3671377 0.467889 0.54016727 0.5960297 0.66774547 0.73948109 0.78152055 0.77606922 0.72058451 0.62988567 0.5130133 0.39502728 0.29854286 0.22764935][0.16488865 0.2545889 0.32467517 0.37086079 0.40655097 0.45504928 0.49952176 0.51942021 0.51042491 0.47382894 0.41692242 0.34255159 0.26751754 0.20887247 0.16609196][0.086034261 0.14371797 0.18476366 0.20550235 0.21846128 0.2380257 0.25079772 0.24722683 0.23270491 0.21134394 0.18428473 0.15044807 0.11842469 0.097425312 0.083056539][0.01844896 0.050743867 0.070240937 0.073077828 0.06930881 0.066145092 0.055294778 0.035605494 0.016637364 0.0038383515 -0.0050587426 -0.012022063 -0.014256585 -0.0093734665 -0.0033146136][-0.03054527 -0.015485577 -0.00881598 -0.014481387 -0.025677029 -0.039616257 -0.060119 -0.084246829 -0.10325323 -0.11218015 -0.11389574 -0.10942406 -0.09860193 -0.082935818 -0.067648932][-0.057196818 -0.052026749 -0.051289827 -0.0588716 -0.070884489 -0.086358018 -0.10624556 -0.12694094 -0.14248726 -0.14917445 -0.14921156 -0.14283417 -0.13059099 -0.11501041 -0.09945827]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 11:28:00.904726: step 11010, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 71h:08m:44s remains)
INFO - root - 2017-12-10 11:28:08.879095: step 11020, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 70h:08m:33s remains)
INFO - root - 2017-12-10 11:28:16.775946: step 11030, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 69h:02m:10s remains)
INFO - root - 2017-12-10 11:28:24.414171: step 11040, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 68h:27m:16s remains)
INFO - root - 2017-12-10 11:28:32.197805: step 11050, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 69h:23m:24s remains)
INFO - root - 2017-12-10 11:28:40.096844: step 11060, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 68h:56m:07s remains)
INFO - root - 2017-12-10 11:28:47.780794: step 11070, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 71h:20m:38s remains)
INFO - root - 2017-12-10 11:28:55.658935: step 11080, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 69h:58m:48s remains)
INFO - root - 2017-12-10 11:29:03.546370: step 11090, loss = 0.72, batch loss = 0.66 (9.8 examples/sec; 0.814 sec/batch; 72h:42m:27s remains)
INFO - root - 2017-12-10 11:29:11.356168: step 11100, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 70h:20m:27s remains)
2017-12-10 11:29:12.197878: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.095392212 0.084672138 0.067710429 0.060424604 0.066014171 0.079978094 0.093158878 0.10407277 0.11337629 0.12046096 0.12659869 0.12605013 0.11763262 0.099820435 0.07285291][0.16778848 0.16092683 0.14709485 0.14754404 0.16545163 0.19245543 0.21444492 0.22950776 0.24000977 0.24643402 0.2507633 0.24656753 0.23357685 0.20933045 0.17255427][0.23295768 0.23017462 0.2217562 0.23412937 0.26961696 0.31503323 0.34910434 0.36803523 0.37552595 0.37442285 0.36948684 0.35653463 0.33745497 0.30811709 0.26536551][0.2832441 0.28647721 0.28708649 0.31578061 0.37345234 0.44151106 0.49005991 0.51097488 0.50872153 0.48932943 0.46443251 0.43616641 0.40982333 0.37907088 0.33652773][0.32159105 0.33355877 0.3463673 0.39252633 0.47098783 0.5586974 0.61881381 0.63831729 0.62113148 0.57694209 0.52717239 0.48291036 0.45347276 0.42769015 0.39061591][0.35761851 0.37839985 0.40085706 0.4583967 0.54901457 0.64817262 0.71498817 0.73133612 0.69826919 0.6293565 0.55696183 0.50191218 0.47604454 0.4610461 0.43198788][0.39338204 0.42002779 0.44588032 0.50633311 0.60078967 0.705336 0.7764411 0.79012948 0.743767 0.65556192 0.56797975 0.5099051 0.492847 0.4904612 0.46772015][0.42251843 0.44930789 0.47084427 0.525583 0.61669338 0.7222808 0.7971819 0.811519 0.75967872 0.66397125 0.57381 0.52194941 0.51659316 0.52484876 0.50365347][0.43782613 0.46029073 0.4712584 0.51204818 0.59087527 0.68851477 0.76152259 0.7777763 0.73063368 0.64472538 0.57099992 0.53951728 0.55099672 0.56751496 0.542017][0.42149293 0.43366328 0.42879343 0.4488433 0.50593793 0.58447343 0.64737493 0.66561049 0.63275516 0.57295287 0.53191519 0.53127223 0.56187958 0.58387733 0.55217558][0.3631402 0.35847041 0.33480486 0.33235395 0.36494404 0.42067066 0.47093165 0.49202371 0.4775447 0.44815993 0.44009051 0.46536228 0.50744355 0.52991933 0.49505162][0.25665012 0.23434006 0.19637012 0.17798212 0.19247063 0.23035057 0.27049145 0.29398462 0.29460964 0.28823593 0.30120498 0.33855063 0.38171166 0.4013747 0.36931816][0.12025768 0.086773664 0.044830035 0.020908669 0.025257131 0.049810346 0.079603538 0.10065578 0.10807849 0.11324081 0.13405572 0.17174815 0.20980486 0.2267092 0.20328186][-0.0031317854 -0.03765123 -0.07360924 -0.095174506 -0.096403047 -0.084204316 -0.0678384 -0.055685639 -0.050388057 -0.043887176 -0.024506982 0.0073487782 0.038583968 0.05435282 0.042820748][-0.09147092 -0.12034963 -0.14560303 -0.16134371 -0.16528934 -0.16244829 -0.15760636 -0.15466654 -0.15455468 -0.15175459 -0.13892378 -0.11660357 -0.093782365 -0.079616807 -0.080665089]]...]
INFO - root - 2017-12-10 11:29:19.990182: step 11110, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.773 sec/batch; 68h:59m:56s remains)
INFO - root - 2017-12-10 11:29:27.782566: step 11120, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 68h:14m:44s remains)
INFO - root - 2017-12-10 11:29:35.388425: step 11130, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 69h:54m:13s remains)
INFO - root - 2017-12-10 11:29:43.201542: step 11140, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.758 sec/batch; 67h:42m:29s remains)
INFO - root - 2017-12-10 11:29:50.904499: step 11150, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 71h:06m:06s remains)
INFO - root - 2017-12-10 11:29:58.772455: step 11160, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.757 sec/batch; 67h:35m:49s remains)
INFO - root - 2017-12-10 11:30:06.520095: step 11170, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.751 sec/batch; 67h:02m:12s remains)
INFO - root - 2017-12-10 11:30:14.321835: step 11180, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 69h:28m:19s remains)
INFO - root - 2017-12-10 11:30:22.286391: step 11190, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 71h:52m:27s remains)
INFO - root - 2017-12-10 11:30:30.105318: step 11200, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 70h:36m:48s remains)
2017-12-10 11:30:30.901420: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34267312 0.34770676 0.34137782 0.32121661 0.2978619 0.28885248 0.29146549 0.28753549 0.26758754 0.24082366 0.22142179 0.2116991 0.21508148 0.24462016 0.29168224][0.31820086 0.32445019 0.32165173 0.30799854 0.29219386 0.2894513 0.29418042 0.28900075 0.2664012 0.23926334 0.22363524 0.22089775 0.22970834 0.26062325 0.30763966][0.29808706 0.3026965 0.3002153 0.2902737 0.27992013 0.28298804 0.29280961 0.29313716 0.27664497 0.25629944 0.24767058 0.25161031 0.26284495 0.29045412 0.3315419][0.29666752 0.29619962 0.29068109 0.28270626 0.27925929 0.29369357 0.31795907 0.33317661 0.32900488 0.31532225 0.30744374 0.30809987 0.31184277 0.32887551 0.35848379][0.30040288 0.29519978 0.28787154 0.2853359 0.29398 0.32542476 0.36910415 0.40165412 0.40874532 0.39597121 0.38072938 0.369982 0.35972634 0.36207679 0.37749055][0.30482903 0.29766154 0.29336473 0.30211005 0.32641578 0.37375546 0.43160453 0.4744617 0.485853 0.46741384 0.43968979 0.41434866 0.38922635 0.37840036 0.38210037][0.31234086 0.30719149 0.309841 0.33300319 0.37207931 0.42882168 0.49029896 0.53222346 0.53899944 0.5097307 0.46776351 0.42925334 0.39382881 0.37703094 0.37680995][0.3297953 0.33094379 0.34203976 0.37664711 0.42252496 0.47655055 0.52816921 0.55802613 0.5546869 0.51524895 0.46256137 0.4153071 0.37514257 0.35953483 0.36299911][0.35455644 0.36374477 0.37930787 0.41435537 0.45318505 0.49159154 0.52353138 0.53703141 0.52526742 0.48263198 0.42695925 0.37654114 0.33594406 0.32599306 0.33843833][0.37436023 0.386924 0.39810458 0.42072546 0.44177943 0.45983693 0.47261396 0.47403908 0.45985597 0.42187893 0.37130815 0.32414046 0.28726947 0.28541717 0.30896929][0.38281018 0.39234802 0.39168897 0.39490864 0.39653841 0.39952207 0.40209723 0.3995235 0.387332 0.35640803 0.31265017 0.26922402 0.23500232 0.23895286 0.27150044][0.38840535 0.39295968 0.37921661 0.36409876 0.35146698 0.34827468 0.35020357 0.34950733 0.33960232 0.31196192 0.26970863 0.22418384 0.18728487 0.19191559 0.22909366][0.39602721 0.39790237 0.37583411 0.34920511 0.32920432 0.32525563 0.32916859 0.32948744 0.31763151 0.28714362 0.24117693 0.19036193 0.14944093 0.15278976 0.19124649][0.40316683 0.40770143 0.38476875 0.3547518 0.33136287 0.32480758 0.32515371 0.32069635 0.30234009 0.26624221 0.21700545 0.16479233 0.12530582 0.13049939 0.1696361][0.3994787 0.41085726 0.39390609 0.36801466 0.34557033 0.33585066 0.32976234 0.31802407 0.29250923 0.25210628 0.20286304 0.15446134 0.12179435 0.13156348 0.17073585]]...]
INFO - root - 2017-12-10 11:30:38.746039: step 11210, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 72h:15m:30s remains)
INFO - root - 2017-12-10 11:30:46.325838: step 11220, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 69h:49m:48s remains)
INFO - root - 2017-12-10 11:30:53.992348: step 11230, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 69h:19m:12s remains)
INFO - root - 2017-12-10 11:31:01.929002: step 11240, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 69h:49m:07s remains)
INFO - root - 2017-12-10 11:31:09.797569: step 11250, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 68h:41m:19s remains)
INFO - root - 2017-12-10 11:31:17.580007: step 11260, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 71h:33m:14s remains)
INFO - root - 2017-12-10 11:31:25.563045: step 11270, loss = 0.70, batch loss = 0.64 (8.7 examples/sec; 0.924 sec/batch; 82h:24m:27s remains)
INFO - root - 2017-12-10 11:31:33.410243: step 11280, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 69h:33m:09s remains)
INFO - root - 2017-12-10 11:31:41.303021: step 11290, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 70h:01m:35s remains)
INFO - root - 2017-12-10 11:31:49.176144: step 11300, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 70h:23m:16s remains)
2017-12-10 11:31:49.920115: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2528635 0.24918617 0.24965967 0.25458366 0.26665652 0.2848385 0.29932612 0.30949214 0.31227511 0.30513796 0.29037598 0.28331971 0.29486421 0.31970948 0.34687][0.28281289 0.26977935 0.26845136 0.28357694 0.31560811 0.35627335 0.38979736 0.40981382 0.40583497 0.37568903 0.3331708 0.30480307 0.3064248 0.33274022 0.3722783][0.27324891 0.25005394 0.24857198 0.27830359 0.3369956 0.40729904 0.466186 0.49920356 0.48904502 0.43539754 0.36348543 0.3104333 0.29740411 0.31975996 0.36699924][0.25459066 0.21778645 0.21302757 0.25268906 0.33172756 0.42721644 0.50991774 0.55659562 0.54537761 0.47824478 0.38744929 0.31533915 0.28686544 0.30042168 0.34868166][0.26301038 0.20782924 0.19615234 0.24183394 0.33554673 0.45178637 0.55492091 0.610781 0.59556681 0.51703739 0.41240096 0.32440197 0.27999264 0.28245437 0.32796118][0.30501223 0.23320763 0.21638913 0.27047122 0.38058743 0.51921713 0.64297819 0.70374811 0.67695093 0.57983851 0.45596254 0.34820005 0.28475872 0.27365005 0.31262276][0.35088739 0.27003884 0.25320405 0.31874496 0.44633028 0.606138 0.74742168 0.80866778 0.7656635 0.64578748 0.49972749 0.37348035 0.29616308 0.27658439 0.31031376][0.36549637 0.28376272 0.27219698 0.34829113 0.48678067 0.65532446 0.79989856 0.85364354 0.79588675 0.66066825 0.50264174 0.37082198 0.293419 0.27567774 0.30841264][0.33503714 0.260854 0.25754419 0.33926889 0.4765538 0.63533294 0.76398516 0.80138665 0.73383331 0.59750259 0.44558772 0.32515237 0.26009452 0.25016633 0.28244537][0.26520497 0.20416963 0.20956555 0.29137921 0.41897628 0.55673152 0.65786344 0.67393774 0.60021633 0.47276798 0.34000832 0.24258609 0.19685593 0.19649297 0.22659026][0.17009656 0.12537113 0.13797438 0.2136436 0.32446378 0.43559641 0.5063293 0.50229692 0.42715055 0.31493294 0.20755714 0.13686877 0.11111736 0.11944607 0.14656022][0.074715585 0.046507545 0.063868724 0.12803686 0.21625224 0.29734394 0.33840755 0.31819 0.24661872 0.15379007 0.073605217 0.02893614 0.021631178 0.039072856 0.066711992][0.0095540835 -0.0049277288 0.014661891 0.066477679 0.13194871 0.18495996 0.20150803 0.17092627 0.10653023 0.033471521 -0.02284137 -0.047587428 -0.0424508 -0.018472668 0.011910031][-0.016653344 -0.020811524 -0.0007560263 0.040647112 0.087491386 0.1187432 0.11878351 0.084065735 0.028781293 -0.026589055 -0.06411954 -0.0756941 -0.064670213 -0.039123587 -0.0080726212][-0.022924395 -0.020892568 -0.0017280312 0.030870331 0.063193873 0.0795706 0.071128331 0.038340352 -0.0058971 -0.045711849 -0.069025069 -0.0722049 -0.059038572 -0.035272852 -0.0068022637]]...]
INFO - root - 2017-12-10 11:31:57.506204: step 11310, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 67h:49m:17s remains)
INFO - root - 2017-12-10 11:32:05.431414: step 11320, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 70h:49m:17s remains)
INFO - root - 2017-12-10 11:32:13.250462: step 11330, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 70h:25m:32s remains)
INFO - root - 2017-12-10 11:32:21.084050: step 11340, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 69h:45m:03s remains)
INFO - root - 2017-12-10 11:32:28.868169: step 11350, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 67h:30m:15s remains)
INFO - root - 2017-12-10 11:32:36.603184: step 11360, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 69h:35m:48s remains)
INFO - root - 2017-12-10 11:32:44.406382: step 11370, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 70h:23m:36s remains)
INFO - root - 2017-12-10 11:32:52.212530: step 11380, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.810 sec/batch; 72h:13m:51s remains)
INFO - root - 2017-12-10 11:32:59.940061: step 11390, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 72h:41m:54s remains)
INFO - root - 2017-12-10 11:33:07.595430: step 11400, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 71h:08m:02s remains)
2017-12-10 11:33:08.532629: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.087964818 0.067801833 0.048647307 0.037304766 0.029103929 0.020000456 0.011731995 0.0065208962 0.0061792387 0.01220126 0.024993194 0.046565175 0.07256002 0.098415412 0.11784104][0.15398161 0.1293298 0.10624139 0.09464775 0.086721674 0.076591633 0.067648128 0.06256175 0.061168771 0.064549245 0.075402617 0.09887737 0.13142906 0.16525708 0.19119103][0.20475164 0.18277726 0.16476168 0.16387688 0.16719519 0.16426027 0.16050068 0.15898979 0.15455717 0.14761271 0.14687338 0.16349746 0.1930228 0.22464404 0.24938489][0.22684045 0.21328826 0.21059717 0.2328881 0.26061594 0.27629802 0.28794283 0.29754031 0.29000202 0.2647132 0.24141569 0.24090351 0.25420022 0.2679278 0.2791571][0.22319783 0.22241613 0.24353573 0.2997174 0.36439127 0.41111776 0.45011792 0.47894445 0.46774656 0.41579285 0.35911161 0.33071402 0.31405264 0.29445252 0.28089753][0.20569693 0.21965116 0.26785329 0.36017761 0.46449941 0.54565406 0.614313 0.66212052 0.64407724 0.56174785 0.46779886 0.40788907 0.35782564 0.30177101 0.26154089][0.18530394 0.2113329 0.28010741 0.39692733 0.52668548 0.6294679 0.71449089 0.76902 0.7393986 0.63310581 0.51413262 0.43485093 0.36540744 0.28888759 0.23519984][0.17242642 0.20062669 0.27203929 0.3891395 0.51789474 0.6192978 0.69875038 0.74205822 0.69824481 0.58340186 0.46311507 0.38659871 0.32262465 0.25496522 0.21316014][0.16531812 0.17935474 0.22909962 0.31830171 0.41956589 0.49968541 0.55868274 0.58279169 0.53266883 0.43154609 0.33674192 0.286338 0.25162607 0.21869792 0.20938966][0.15525952 0.14443593 0.15783308 0.2040929 0.26604453 0.31862929 0.35625765 0.36601335 0.32258883 0.25199327 0.19745766 0.18348192 0.18662417 0.19558452 0.22134465][0.1390679 0.10518289 0.083820365 0.090091527 0.11623098 0.14483102 0.16680919 0.17071773 0.14300504 0.1072318 0.091844194 0.10916363 0.14256835 0.18331961 0.23251142][0.11487547 0.068659887 0.026765985 0.0084476937 0.012343265 0.026075581 0.039852157 0.044415884 0.033642679 0.02493033 0.03516468 0.0684536 0.11471195 0.16772059 0.22277586][0.076302417 0.031826839 -0.011929261 -0.035983559 -0.038897593 -0.030317754 -0.018726239 -0.011224373 -0.010417599 -0.0053508817 0.013723264 0.046957973 0.08874853 0.13536422 0.18077153][0.018713864 -0.013495263 -0.043955386 -0.059645344 -0.059249353 -0.050574236 -0.039252412 -0.030401032 -0.025521336 -0.018576669 -0.0032921669 0.019467635 0.046303649 0.075530767 0.10286247][-0.049685836 -0.064900331 -0.07522662 -0.076660596 -0.069540821 -0.05976899 -0.049692176 -0.042504564 -0.039531115 -0.038044062 -0.033354845 -0.025908353 -0.017632261 -0.0084498674 0.00019627763]]...]
INFO - root - 2017-12-10 11:33:16.418141: step 11410, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 70h:52m:19s remains)
INFO - root - 2017-12-10 11:33:24.192339: step 11420, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 68h:17m:02s remains)
INFO - root - 2017-12-10 11:33:32.019217: step 11430, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 68h:35m:30s remains)
INFO - root - 2017-12-10 11:33:39.882649: step 11440, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 69h:51m:07s remains)
INFO - root - 2017-12-10 11:33:47.852125: step 11450, loss = 0.68, batch loss = 0.62 (9.7 examples/sec; 0.823 sec/batch; 73h:26m:01s remains)
INFO - root - 2017-12-10 11:33:55.696136: step 11460, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 68h:23m:08s remains)
INFO - root - 2017-12-10 11:34:03.392033: step 11470, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 71h:45m:14s remains)
INFO - root - 2017-12-10 11:34:11.172037: step 11480, loss = 0.70, batch loss = 0.64 (11.0 examples/sec; 0.724 sec/batch; 64h:35m:34s remains)
INFO - root - 2017-12-10 11:34:18.840929: step 11490, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 68h:10m:11s remains)
INFO - root - 2017-12-10 11:34:26.637931: step 11500, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 69h:15m:17s remains)
2017-12-10 11:34:27.513808: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.019143995 0.041841712 0.0951677 0.16436893 0.23181535 0.28040203 0.29810855 0.2797637 0.23149659 0.16806494 0.099236138 0.033769686 -0.018447973 -0.049521666 -0.063528717][0.081490695 0.08835876 0.13982891 0.22228266 0.31073824 0.37746632 0.40454 0.38621593 0.327711 0.24772759 0.16047391 0.076974586 0.00834882 -0.035213672 -0.057570573][0.16468863 0.14367828 0.17743175 0.25757453 0.35633472 0.437452 0.47614339 0.46375287 0.4036656 0.31570452 0.21709546 0.1199821 0.036851831 -0.0186075 -0.049293451][0.24295881 0.19470176 0.2034543 0.27044472 0.369591 0.46053132 0.51127356 0.50822604 0.45164561 0.36193904 0.2589134 0.15480152 0.061986897 -0.0026427309 -0.039960086][0.2987299 0.23302659 0.21946284 0.26889598 0.36055475 0.45513991 0.51517522 0.52087647 0.46933249 0.380498 0.27718455 0.17166291 0.075126238 0.0061816103 -0.034527682][0.34465083 0.27453122 0.24774383 0.28273529 0.36510578 0.45958057 0.52460861 0.53507131 0.48461637 0.39310542 0.28594509 0.17745297 0.078562796 0.0074352114 -0.034511782][0.42533919 0.3556968 0.31912044 0.34207091 0.41996059 0.52165079 0.5971368 0.61130548 0.55431694 0.44793844 0.32189807 0.19659792 0.086701609 0.0094981845 -0.035029892][0.53597778 0.4682965 0.42204911 0.43266603 0.50875187 0.62323773 0.71224225 0.72779667 0.65737408 0.52621 0.36962283 0.21719083 0.091587171 0.00803711 -0.037897952][0.6329025 0.57099 0.51688206 0.51362121 0.58365566 0.70554537 0.80432779 0.82069904 0.73929143 0.58600909 0.40088886 0.22341557 0.085159615 -0.0010801087 -0.045148686][0.66491461 0.61512744 0.55890077 0.54350781 0.600574 0.71474546 0.81224269 0.83090872 0.75200295 0.59524679 0.39964765 0.21157889 0.069772914 -0.014465119 -0.05452846][0.60949147 0.57779777 0.52883464 0.50798976 0.54926479 0.642677 0.72759163 0.7485922 0.6852957 0.54655397 0.36309406 0.18282887 0.048528969 -0.028525041 -0.06276647][0.47528204 0.46168995 0.42592341 0.40604112 0.43314037 0.50180674 0.56914258 0.5921635 0.5510419 0.44322261 0.288742 0.13243061 0.017144592 -0.046226472 -0.071444124][0.29002953 0.28745508 0.26450637 0.24814443 0.26267287 0.30642062 0.35333076 0.37424821 0.35261184 0.27928102 0.1659499 0.050052714 -0.032034785 -0.072466664 -0.083415657][0.10688997 0.10771106 0.0953099 0.085510075 0.09319602 0.11782929 0.14616573 0.16136205 0.15212849 0.10908522 0.03920462 -0.0309746 -0.076492243 -0.093347825 -0.091001719][-0.020943193 -0.021063954 -0.026646679 -0.0298482 -0.023892177 -0.010369893 0.0048658261 0.013732992 0.010202781 -0.011728337 -0.047422387 -0.081018806 -0.099041022 -0.10023567 -0.090775855]]...]
INFO - root - 2017-12-10 11:34:35.336359: step 11510, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 68h:59m:39s remains)
INFO - root - 2017-12-10 11:34:43.248288: step 11520, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 71h:28m:05s remains)
INFO - root - 2017-12-10 11:34:51.038249: step 11530, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 67h:54m:38s remains)
INFO - root - 2017-12-10 11:34:58.730711: step 11540, loss = 0.69, batch loss = 0.64 (12.0 examples/sec; 0.667 sec/batch; 59h:28m:26s remains)
INFO - root - 2017-12-10 11:35:06.407292: step 11550, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 68h:36m:20s remains)
INFO - root - 2017-12-10 11:35:14.243072: step 11560, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 69h:20m:01s remains)
INFO - root - 2017-12-10 11:35:22.039179: step 11570, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.814 sec/batch; 72h:36m:27s remains)
INFO - root - 2017-12-10 11:35:29.942381: step 11580, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 71h:07m:59s remains)
INFO - root - 2017-12-10 11:35:37.728240: step 11590, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 69h:00m:52s remains)
INFO - root - 2017-12-10 11:35:45.554529: step 11600, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 72h:18m:50s remains)
2017-12-10 11:35:46.395013: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16888361 0.15161231 0.12208947 0.10232874 0.1013785 0.106804 0.11942323 0.13496247 0.14893778 0.15216114 0.13061619 0.088558674 0.034284353 -0.018179642 -0.060206089][0.29656014 0.27977151 0.24422219 0.2213534 0.22560856 0.23737571 0.25743002 0.27782455 0.29108241 0.28464183 0.24436052 0.17936386 0.099995464 0.024806673 -0.035959344][0.40660033 0.39899042 0.36765555 0.34979469 0.36230084 0.3808614 0.40503016 0.42388681 0.42906508 0.40561917 0.34223175 0.25336453 0.15111218 0.057004038 -0.017739197][0.47596908 0.48617312 0.46815175 0.46129391 0.48214248 0.506964 0.53457606 0.55179381 0.54802907 0.50703067 0.42131555 0.30909097 0.185135 0.073940508 -0.011774354][0.51600981 0.54585856 0.54660189 0.55528927 0.58532214 0.61723781 0.65147841 0.672767 0.66608989 0.6112107 0.50496686 0.36937624 0.22202359 0.090713643 -0.0083323373][0.52324528 0.56679755 0.58671111 0.61421829 0.65607405 0.69766873 0.74227935 0.77307159 0.76815766 0.70301372 0.57991457 0.42477548 0.25670037 0.10567816 -0.006778351][0.49371818 0.53837556 0.57099491 0.61604911 0.67158473 0.72580177 0.78457081 0.82989991 0.83215839 0.76374644 0.63257056 0.4671168 0.28577879 0.11943441 -0.00391835][0.44200355 0.474685 0.50854671 0.562703 0.62744492 0.69181865 0.76307869 0.82247758 0.83594573 0.77385283 0.6470232 0.48328274 0.29886022 0.1253812 -0.0034348757][0.3760663 0.38835621 0.40966877 0.45804256 0.51995838 0.58460987 0.6594032 0.72657579 0.751965 0.70581537 0.59795219 0.45138395 0.27878591 0.11157236 -0.012882851][0.28989929 0.27952653 0.28167561 0.31487361 0.36565638 0.42277426 0.49163797 0.5565623 0.58764297 0.55808437 0.47578853 0.35815522 0.21332377 0.069317751 -0.037505761][0.17279311 0.14571054 0.1315743 0.14860603 0.18592903 0.23194806 0.28815454 0.3413642 0.36853573 0.34953213 0.29130971 0.20703509 0.10136244 -0.0041246838 -0.079742789][0.050300948 0.016422991 -0.0062473966 -0.0026443854 0.019771321 0.050517879 0.088486664 0.12410416 0.14211255 0.12964945 0.092813186 0.041385174 -0.022420464 -0.084456488 -0.12452886][-0.042750169 -0.074231461 -0.09712401 -0.10214898 -0.093699634 -0.07977239 -0.061700318 -0.044551753 -0.036544673 -0.044767421 -0.0646379 -0.089364111 -0.11833672 -0.14392111 -0.15492502][-0.10279125 -0.12629288 -0.14349125 -0.15108588 -0.15157688 -0.14986154 -0.14631067 -0.14246537 -0.14168501 -0.14719671 -0.15586932 -0.16337925 -0.16976829 -0.17186546 -0.16496311][-0.13244095 -0.14725107 -0.15679121 -0.16277893 -0.16646455 -0.17018548 -0.17359504 -0.1762989 -0.17902006 -0.18282481 -0.18575703 -0.18511255 -0.18089955 -0.17222895 -0.15804893]]...]
INFO - root - 2017-12-10 11:35:54.123923: step 11610, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 68h:38m:33s remains)
INFO - root - 2017-12-10 11:36:01.868804: step 11620, loss = 0.70, batch loss = 0.65 (13.8 examples/sec; 0.578 sec/batch; 51h:29m:06s remains)
INFO - root - 2017-12-10 11:36:09.789238: step 11630, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 70h:27m:39s remains)
INFO - root - 2017-12-10 11:36:17.677416: step 11640, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 70h:43m:07s remains)
INFO - root - 2017-12-10 11:36:25.496826: step 11650, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 68h:39m:16s remains)
INFO - root - 2017-12-10 11:36:33.136187: step 11660, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 67h:22m:39s remains)
INFO - root - 2017-12-10 11:36:41.113431: step 11670, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 70h:53m:21s remains)
INFO - root - 2017-12-10 11:36:49.173161: step 11680, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.761 sec/batch; 67h:49m:50s remains)
INFO - root - 2017-12-10 11:36:57.067896: step 11690, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 69h:27m:23s remains)
INFO - root - 2017-12-10 11:37:04.912602: step 11700, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.744 sec/batch; 66h:20m:33s remains)
2017-12-10 11:37:05.675063: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22174118 0.22988959 0.22369611 0.20407785 0.17953992 0.15646125 0.13482875 0.11206324 0.095609225 0.091391288 0.093729131 0.10566815 0.12357551 0.13594936 0.12976611][0.15771079 0.16124795 0.15454075 0.14185703 0.1304533 0.12073449 0.10955705 0.094592862 0.082376614 0.077198856 0.074237205 0.077281043 0.087469704 0.097510137 0.09561643][0.11860368 0.1156596 0.10682378 0.10151001 0.10610311 0.11327861 0.11390594 0.10539554 0.093394846 0.081375457 0.067163594 0.056330025 0.054062542 0.057125773 0.056435451][0.14289683 0.13493684 0.12276815 0.12162481 0.13893262 0.16138527 0.1719422 0.16626935 0.14976701 0.126034 0.096331023 0.067790315 0.048653446 0.039510936 0.034386124][0.2294108 0.21916501 0.20299518 0.20167127 0.2257174 0.25896353 0.27632895 0.27029225 0.24640363 0.20940776 0.16380253 0.11768135 0.080886014 0.056759477 0.042463984][0.34140974 0.33034995 0.30972841 0.30430725 0.32855058 0.36706126 0.38885841 0.38227627 0.35233119 0.30526948 0.24801601 0.18871258 0.1376593 0.099502429 0.073895216][0.41879514 0.40892825 0.38579935 0.37660667 0.39798537 0.43800509 0.46352494 0.45933077 0.42867827 0.37841582 0.31684902 0.25137818 0.19229807 0.14485019 0.11001371][0.42287615 0.4169288 0.39685306 0.38823622 0.40722328 0.44697183 0.476 0.47734159 0.45179927 0.40593711 0.34770411 0.28300366 0.222801 0.17315479 0.13479626][0.36196876 0.36448249 0.3537156 0.34999186 0.36771467 0.40497607 0.43463075 0.43984494 0.41975144 0.38101423 0.33004329 0.27187937 0.21844909 0.17636167 0.14382866][0.27280253 0.28763637 0.28976405 0.29304603 0.31073073 0.34481332 0.37239057 0.37710288 0.35799849 0.3232528 0.27838126 0.2294279 0.18871091 0.16207509 0.14316367][0.19169576 0.21921757 0.23356839 0.24302003 0.26128355 0.29301021 0.31778297 0.31891382 0.29628244 0.26083088 0.21938106 0.1809736 0.15661463 0.14955513 0.14777118][0.14288664 0.17655081 0.1973211 0.2097013 0.22812733 0.25844568 0.281501 0.279576 0.25247973 0.21319327 0.17216136 0.14368641 0.13655305 0.14962104 0.1648341][0.13185117 0.16359451 0.18379651 0.19531739 0.212131 0.24007829 0.26147774 0.25814348 0.22857696 0.18540928 0.14327063 0.12287192 0.13091451 0.16104843 0.18984279][0.14112115 0.16391358 0.17786843 0.18594185 0.19989985 0.22502184 0.24591672 0.24458285 0.21712551 0.17357634 0.13143699 0.11661038 0.13454165 0.17468014 0.21005669][0.15172911 0.16057684 0.16448295 0.16813472 0.17994627 0.20365766 0.22566989 0.22880903 0.2071642 0.16744027 0.12768039 0.11583447 0.13736826 0.17971931 0.21453285]]...]
INFO - root - 2017-12-10 11:37:13.510162: step 11710, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 69h:14m:01s remains)
INFO - root - 2017-12-10 11:37:21.529148: step 11720, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.810 sec/batch; 72h:10m:10s remains)
INFO - root - 2017-12-10 11:37:29.388830: step 11730, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.796 sec/batch; 70h:53m:12s remains)
INFO - root - 2017-12-10 11:37:37.201335: step 11740, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 68h:36m:53s remains)
INFO - root - 2017-12-10 11:37:44.892776: step 11750, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 70h:19m:37s remains)
INFO - root - 2017-12-10 11:37:52.830126: step 11760, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 70h:39m:31s remains)
INFO - root - 2017-12-10 11:38:00.794218: step 11770, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.830 sec/batch; 73h:58m:17s remains)
INFO - root - 2017-12-10 11:38:08.482888: step 11780, loss = 0.69, batch loss = 0.64 (12.9 examples/sec; 0.618 sec/batch; 55h:02m:37s remains)
INFO - root - 2017-12-10 11:38:16.298985: step 11790, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 71h:15m:35s remains)
INFO - root - 2017-12-10 11:38:24.212051: step 11800, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 70h:49m:06s remains)
2017-12-10 11:38:25.014802: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19800046 0.1894597 0.17036195 0.15620372 0.15338844 0.15619271 0.16706102 0.17632619 0.17528781 0.16146778 0.14234225 0.12138615 0.091722369 0.055285729 0.022020128][0.23244968 0.22823313 0.21100028 0.19780344 0.19562995 0.19741172 0.2045529 0.20719218 0.1987703 0.17805262 0.15253419 0.12633547 0.093407281 0.055818819 0.022173189][0.25884205 0.26127774 0.24877836 0.23783888 0.23617125 0.23633489 0.23850837 0.23260731 0.21508224 0.18736424 0.15636574 0.12544852 0.089776307 0.052541353 0.020735193][0.27568224 0.28683588 0.28131905 0.27367616 0.27251983 0.272017 0.27043024 0.25665218 0.23033643 0.19646363 0.16160294 0.12676248 0.087871782 0.049686674 0.018707879][0.29001638 0.31112477 0.31403834 0.30945638 0.30764803 0.30712396 0.30479047 0.28698933 0.25449443 0.21583475 0.17767224 0.13803281 0.092809148 0.049458954 0.016004533][0.30518615 0.3348355 0.34377468 0.33984089 0.33625698 0.33723676 0.33938876 0.32547742 0.29334739 0.25337312 0.21273701 0.16743021 0.11269599 0.059170984 0.018803926][0.32028258 0.35365915 0.36283529 0.35615388 0.35096318 0.35648805 0.36874387 0.36632419 0.34156018 0.30454245 0.2629599 0.21209067 0.14684594 0.080536745 0.03024357][0.32780066 0.35747924 0.36030927 0.34860244 0.34364283 0.35686728 0.38235483 0.39481947 0.38128647 0.35006112 0.30881357 0.25365216 0.17956983 0.10222302 0.042421848][0.31713989 0.33807826 0.33139849 0.31529573 0.31257373 0.33390003 0.3713254 0.39789113 0.39635307 0.37222305 0.33307311 0.27682418 0.19858684 0.1145862 0.048140217][0.28467393 0.29366386 0.27816725 0.26014245 0.26050934 0.28695586 0.33124897 0.36786541 0.37702367 0.360717 0.32578319 0.27265525 0.1967717 0.11361242 0.046714932][0.23376268 0.23120821 0.20830759 0.1892368 0.19094647 0.21745884 0.26193294 0.30313924 0.32096058 0.31322274 0.28529787 0.23999913 0.17372279 0.09966173 0.039223108][0.17002073 0.15898328 0.13220792 0.11245433 0.11235991 0.13341826 0.17105493 0.20963083 0.23131876 0.23083787 0.21173191 0.17803238 0.12722427 0.069429077 0.022031052][0.10473291 0.090693943 0.0654482 0.046954788 0.0437623 0.056314122 0.082366355 0.11190647 0.13141952 0.13482846 0.12417966 0.1031407 0.069664918 0.030572912 -0.0012090417][0.046907015 0.034658086 0.015636044 0.001378934 -0.0033452832 0.001360715 0.015101367 0.032687027 0.045685139 0.049315311 0.044911075 0.034848358 0.01716562 -0.0043302937 -0.021251772][0.0033639995 -0.0051890472 -0.016482808 -0.025059255 -0.028925272 -0.028576314 -0.023757093 -0.016482295 -0.010800552 -0.0093974639 -0.011193008 -0.014694826 -0.021249548 -0.029239377 -0.034444235]]...]
INFO - root - 2017-12-10 11:38:32.883316: step 11810, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 70h:38m:56s remains)
INFO - root - 2017-12-10 11:38:40.923847: step 11820, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.824 sec/batch; 73h:22m:06s remains)
INFO - root - 2017-12-10 11:38:48.829411: step 11830, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 70h:53m:12s remains)
INFO - root - 2017-12-10 11:38:56.668550: step 11840, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 68h:33m:09s remains)
INFO - root - 2017-12-10 11:39:04.517849: step 11850, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 69h:47m:19s remains)
INFO - root - 2017-12-10 11:39:12.274665: step 11860, loss = 0.68, batch loss = 0.62 (12.1 examples/sec; 0.663 sec/batch; 59h:01m:47s remains)
INFO - root - 2017-12-10 11:39:20.162941: step 11870, loss = 0.67, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 68h:33m:12s remains)
INFO - root - 2017-12-10 11:39:28.074130: step 11880, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 69h:14m:28s remains)
INFO - root - 2017-12-10 11:39:35.912749: step 11890, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.773 sec/batch; 68h:50m:09s remains)
INFO - root - 2017-12-10 11:39:43.753080: step 11900, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 69h:03m:55s remains)
2017-12-10 11:39:44.521641: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.016355105 0.06550619 0.11520264 0.15474334 0.17813188 0.17865458 0.15693748 0.1201572 0.078375116 0.040798612 0.012594731 -0.0048296168 -0.013728039 -0.016546078 -0.015734188][0.01102964 0.055776574 0.10052486 0.13508809 0.15343247 0.15006635 0.12646055 0.088986784 0.046807643 0.0090204859 -0.018640827 -0.033706725 -0.037732568 -0.033728298 -0.026096554][0.0072891545 0.049062755 0.092185296 0.12694176 0.14627028 0.14576969 0.12756112 0.0965561 0.060250882 0.026906086 0.0028666239 -0.0090557924 -0.010114238 -0.0037347758 0.0047090496][0.0080623608 0.050665449 0.097964145 0.13984476 0.16760789 0.1773506 0.17099206 0.15133655 0.12435295 0.097814061 0.078148335 0.067207664 0.0628058 0.062027391 0.061416775][0.013579777 0.060816113 0.11718807 0.17135566 0.21297343 0.23815754 0.24759364 0.24049886 0.22161064 0.19994566 0.18270087 0.16971526 0.15640238 0.1409373 0.12403367][0.023180302 0.078677617 0.14753981 0.21665959 0.27378529 0.31461811 0.337899 0.33867207 0.32130268 0.297762 0.27741891 0.25932881 0.2364924 0.2079207 0.17789042][0.033799715 0.097987466 0.17900127 0.26126197 0.330728 0.38249463 0.41383925 0.41585964 0.39299673 0.360712 0.33140334 0.30565742 0.2760185 0.24257854 0.21115676][0.041103944 0.11198147 0.20242386 0.29448426 0.37199488 0.42849493 0.4607304 0.45745283 0.42395681 0.37867448 0.33703071 0.30253035 0.26865894 0.23767453 0.21598187][0.042688441 0.11697011 0.21287073 0.31035873 0.3905375 0.4444584 0.46941885 0.45464632 0.40772209 0.34966424 0.2982541 0.2593517 0.22713855 0.20567594 0.20100486][0.036066279 0.10882217 0.2046085 0.30217457 0.3802942 0.4276205 0.44251874 0.41680562 0.36076578 0.29714504 0.24519618 0.21153581 0.18957256 0.18260637 0.19468106][0.021118578 0.087707706 0.17818668 0.27138788 0.34489211 0.38588348 0.39358315 0.36340621 0.30767041 0.24914308 0.20674615 0.18626578 0.1801517 0.1889254 0.21354005][0.0046330113 0.063584536 0.14616252 0.23152848 0.29715806 0.33092034 0.33395243 0.30667305 0.26249766 0.22156313 0.19833839 0.1953112 0.2050305 0.22525541 0.25413358][-0.0057129138 0.047319621 0.12115821 0.19555791 0.24953206 0.27435696 0.27391389 0.25410619 0.2290412 0.21434854 0.21657555 0.23275745 0.25586626 0.2812975 0.30541235][-0.0056346669 0.044795558 0.1111495 0.17346196 0.21355824 0.22778657 0.22272193 0.20832704 0.19956684 0.20912036 0.23617131 0.27167252 0.30726856 0.33553079 0.35166612][0.0044252169 0.056135111 0.11870299 0.17107669 0.1973054 0.19886588 0.18532526 0.16872379 0.16479002 0.18634765 0.22966935 0.28158614 0.32942209 0.36196554 0.37410188]]...]
INFO - root - 2017-12-10 11:39:52.438996: step 11910, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 71h:04m:26s remains)
INFO - root - 2017-12-10 11:40:00.321854: step 11920, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 68h:46m:27s remains)
INFO - root - 2017-12-10 11:40:07.980866: step 11930, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 69h:25m:46s remains)
INFO - root - 2017-12-10 11:40:15.623271: step 11940, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.574 sec/batch; 51h:05m:59s remains)
INFO - root - 2017-12-10 11:40:23.621940: step 11950, loss = 0.68, batch loss = 0.62 (9.7 examples/sec; 0.822 sec/batch; 73h:14m:12s remains)
INFO - root - 2017-12-10 11:40:32.279788: step 11960, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.022 sec/batch; 90h:57m:44s remains)
INFO - root - 2017-12-10 11:40:42.527519: step 11970, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.025 sec/batch; 91h:16m:36s remains)
INFO - root - 2017-12-10 11:40:52.686336: step 11980, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.017 sec/batch; 90h:31m:00s remains)
INFO - root - 2017-12-10 11:41:02.736963: step 11990, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.006 sec/batch; 89h:34m:22s remains)
INFO - root - 2017-12-10 11:41:12.893913: step 12000, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.009 sec/batch; 89h:47m:09s remains)
2017-12-10 11:41:13.906316: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.076564886 0.094746724 0.10220503 0.10534119 0.10928031 0.11871163 0.12568669 0.12095004 0.10213032 0.076505132 0.058719527 0.054708466 0.0622755 0.072313569 0.074029475][0.12416083 0.15133065 0.16224656 0.16512175 0.169624 0.18497115 0.20071532 0.20097974 0.17766109 0.13886309 0.10587119 0.091135763 0.0958192 0.10928079 0.11511207][0.16133709 0.19532807 0.20744373 0.20768932 0.2103021 0.22934866 0.25290036 0.25897095 0.23325257 0.18401335 0.13791528 0.11277922 0.11272161 0.12761021 0.13777713][0.1727462 0.21110997 0.22586612 0.22630255 0.22819336 0.24873313 0.27610177 0.2856338 0.25999379 0.20768046 0.15680699 0.12547745 0.11844981 0.1293712 0.14100087][0.15759873 0.19952416 0.22074227 0.22708504 0.23202662 0.25395021 0.2829299 0.29579347 0.27603984 0.23192409 0.1878535 0.15583825 0.13838175 0.13585362 0.14096817][0.13296598 0.17731526 0.20567748 0.21936952 0.22904694 0.25391281 0.28745636 0.30841333 0.30170259 0.27443838 0.24475919 0.21511109 0.18316694 0.15777756 0.14729127][0.11582674 0.16099437 0.19317918 0.21124476 0.22499354 0.25505841 0.29773548 0.33053645 0.33768418 0.325112 0.30679491 0.27723536 0.2290796 0.17909476 0.15113361][0.11428506 0.15964758 0.19131307 0.20801741 0.22108068 0.25409412 0.30518392 0.34791279 0.36286032 0.35513049 0.33856165 0.3056218 0.24597737 0.18035935 0.14146759][0.14061566 0.19273676 0.22568248 0.23580436 0.2357187 0.256185 0.30158564 0.34398654 0.3590017 0.34900537 0.32964912 0.29606366 0.23691332 0.17121761 0.13159105][0.19777349 0.26630643 0.30601111 0.30707967 0.281725 0.27086964 0.291175 0.31919295 0.32726404 0.31376871 0.29607955 0.27274513 0.23142162 0.18367061 0.15426837][0.26096633 0.34850025 0.39790389 0.39268285 0.34314588 0.29725608 0.28348657 0.28771868 0.28436244 0.26956791 0.26049927 0.25654685 0.24360694 0.22324027 0.20889653][0.30471811 0.40487245 0.46227869 0.45620498 0.39300144 0.32090107 0.27553546 0.25536662 0.24191417 0.23110323 0.23630717 0.25493243 0.27163222 0.27946839 0.2811175][0.30998185 0.40965062 0.4678334 0.46304134 0.39679256 0.31186011 0.24611835 0.20995516 0.19312868 0.19127162 0.21119106 0.24708188 0.28422505 0.31126347 0.32404214][0.26617479 0.3527422 0.40334275 0.39877394 0.33749208 0.25389946 0.18285404 0.14169019 0.12654865 0.13248193 0.1604898 0.202679 0.24695668 0.28109673 0.29819643][0.17797859 0.24355227 0.28098482 0.27495754 0.22443974 0.15502903 0.093243547 0.056346145 0.044454888 0.053563084 0.081998751 0.12181874 0.16445917 0.19798359 0.21478166]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 11:41:24.075990: step 12010, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 0.995 sec/batch; 88h:35m:36s remains)
INFO - root - 2017-12-10 11:41:34.114828: step 12020, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.042 sec/batch; 92h:44m:41s remains)
INFO - root - 2017-12-10 11:41:44.338040: step 12030, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.023 sec/batch; 91h:05m:49s remains)
INFO - root - 2017-12-10 11:41:54.590381: step 12040, loss = 0.70, batch loss = 0.65 (7.8 examples/sec; 1.028 sec/batch; 91h:28m:47s remains)
INFO - root - 2017-12-10 11:42:04.665274: step 12050, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 0.998 sec/batch; 88h:49m:12s remains)
INFO - root - 2017-12-10 11:42:14.905795: step 12060, loss = 0.70, batch loss = 0.64 (7.5 examples/sec; 1.066 sec/batch; 94h:50m:59s remains)
INFO - root - 2017-12-10 11:42:24.996368: step 12070, loss = 0.70, batch loss = 0.65 (7.8 examples/sec; 1.020 sec/batch; 90h:47m:19s remains)
INFO - root - 2017-12-10 11:42:35.104358: step 12080, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.004 sec/batch; 89h:22m:39s remains)
INFO - root - 2017-12-10 11:42:45.281191: step 12090, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.030 sec/batch; 91h:42m:16s remains)
INFO - root - 2017-12-10 11:42:55.247193: step 12100, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.042 sec/batch; 92h:44m:16s remains)
2017-12-10 11:42:56.182741: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30785447 0.30000988 0.28816617 0.28041646 0.26481393 0.22544676 0.16149522 0.098603964 0.063259222 0.059022289 0.071397461 0.08766181 0.098685592 0.097480029 0.086721711][0.34977597 0.34811965 0.33280379 0.3158302 0.29168501 0.24675518 0.17864069 0.11189825 0.071503751 0.059872661 0.061597168 0.065039843 0.06568338 0.061954979 0.05948865][0.3497982 0.35310176 0.33813533 0.31847185 0.29536572 0.25718573 0.19806077 0.13813907 0.098844014 0.082072563 0.073690385 0.0642925 0.053363565 0.044072274 0.042922381][0.31262094 0.32018161 0.31080842 0.29800916 0.2874977 0.26743957 0.22684589 0.17983916 0.14550376 0.12706979 0.11284263 0.094478354 0.073745795 0.056588892 0.04939317][0.2632173 0.27652484 0.2784147 0.28103191 0.29082558 0.29409045 0.27350527 0.23683713 0.20407505 0.18353428 0.16695307 0.14522684 0.11948928 0.096472107 0.08096572][0.21706432 0.24005947 0.2593126 0.28393465 0.31780508 0.3445887 0.34143695 0.30964211 0.27135336 0.24237287 0.22007087 0.19564725 0.16877091 0.14484423 0.12487028][0.17716318 0.21286638 0.25311393 0.30177811 0.35838991 0.40487167 0.41538936 0.38561508 0.33843753 0.29565144 0.26151186 0.2314751 0.20509885 0.18537995 0.16765682][0.14233741 0.18890263 0.2464793 0.3132216 0.38481331 0.44325033 0.46326968 0.43638554 0.38303566 0.32631662 0.27754128 0.23994938 0.21502236 0.203738 0.19498251][0.11099529 0.16067541 0.22583702 0.29931891 0.37435329 0.43559408 0.4614225 0.44100806 0.38914293 0.32562849 0.2664772 0.22232804 0.19810462 0.19397804 0.19508113][0.076939046 0.12004475 0.18168117 0.251698 0.32202554 0.38041037 0.41004691 0.39947766 0.35688758 0.29650244 0.23559681 0.18947403 0.16539624 0.16382216 0.17021918][0.043791909 0.073020056 0.1222882 0.18174124 0.24365047 0.29772896 0.33079311 0.33162892 0.30300167 0.2533536 0.19819278 0.15412505 0.12957078 0.12545438 0.13059743][0.023423027 0.037036847 0.069435813 0.113993 0.16593863 0.21651562 0.253658 0.26556444 0.25080174 0.21354163 0.16639243 0.12583858 0.10051519 0.091449678 0.0915352][0.026598806 0.029393682 0.046617266 0.074770942 0.11423679 0.15928568 0.19742365 0.21555679 0.20916063 0.18046753 0.14008164 0.10355294 0.078841984 0.066473685 0.06251014][0.050700903 0.05381554 0.062346157 0.075109147 0.0995517 0.1352649 0.16944908 0.18818435 0.18420884 0.15917121 0.12262253 0.089462593 0.066907689 0.054465242 0.049221195][0.084158286 0.09698379 0.10474803 0.10704549 0.11777066 0.14195223 0.16724278 0.18028614 0.17268233 0.14666231 0.1121495 0.083460756 0.066565461 0.058819249 0.055239867]]...]
INFO - root - 2017-12-10 11:43:06.150400: step 12110, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.036 sec/batch; 92h:10m:30s remains)
INFO - root - 2017-12-10 11:43:16.236622: step 12120, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.987 sec/batch; 87h:49m:49s remains)
INFO - root - 2017-12-10 11:43:26.514227: step 12130, loss = 0.69, batch loss = 0.63 (7.6 examples/sec; 1.057 sec/batch; 94h:03m:35s remains)
INFO - root - 2017-12-10 11:43:36.732004: step 12140, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.011 sec/batch; 89h:57m:10s remains)
INFO - root - 2017-12-10 11:43:46.958933: step 12150, loss = 0.69, batch loss = 0.64 (7.9 examples/sec; 1.017 sec/batch; 90h:28m:23s remains)
INFO - root - 2017-12-10 11:43:57.133795: step 12160, loss = 0.69, batch loss = 0.64 (7.9 examples/sec; 1.011 sec/batch; 89h:56m:08s remains)
INFO - root - 2017-12-10 11:44:07.264433: step 12170, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.036 sec/batch; 92h:10m:56s remains)
INFO - root - 2017-12-10 11:44:17.278425: step 12180, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.032 sec/batch; 91h:49m:06s remains)
INFO - root - 2017-12-10 11:44:27.309556: step 12190, loss = 0.68, batch loss = 0.63 (8.8 examples/sec; 0.905 sec/batch; 80h:29m:05s remains)
INFO - root - 2017-12-10 11:44:37.448991: step 12200, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 0.999 sec/batch; 88h:55m:08s remains)
2017-12-10 11:44:38.459669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.019417303 -0.021764351 -0.022284158 -0.021776369 -0.022059908 -0.023505898 -0.025202103 -0.025468713 -0.022517232 -0.018227762 -0.015485477 -0.015191781 -0.016968381 -0.01988698 -0.023116322][-0.0063061034 -0.0044631264 -0.00065735728 0.0027253071 0.0030494391 0.00033345772 -0.0033481545 -0.0056421659 -0.0033464085 0.00064068823 0.0022208949 0.0005593698 -0.0041947253 -0.011404825 -0.01996547][0.025615091 0.037133738 0.050083049 0.0591662 0.061063781 0.057064056 0.051020864 0.045584183 0.044798717 0.04550375 0.042508218 0.035700377 0.02496594 0.010391723 -0.0063544563][0.080036722 0.10730453 0.13353631 0.15068904 0.15539066 0.15056837 0.14229889 0.13364719 0.12893076 0.12434882 0.11374055 0.098345794 0.077791989 0.051476009 0.022042431][0.15672463 0.20422517 0.24489985 0.26969782 0.27661306 0.27079526 0.26137716 0.25186506 0.24587533 0.23717155 0.21790813 0.19090746 0.15595567 0.11267266 0.065436408][0.24087657 0.30983314 0.36334863 0.39352053 0.40084541 0.39382583 0.3855823 0.37942004 0.37691662 0.36748791 0.34064475 0.30016541 0.24700578 0.18255493 0.11421355][0.3087692 0.39622509 0.46001619 0.49308571 0.49877888 0.4901661 0.48532775 0.48741305 0.49429381 0.48994905 0.46024993 0.40838593 0.33668575 0.25013542 0.16019407][0.34437776 0.44435969 0.51420987 0.54670274 0.54722351 0.53452283 0.5321939 0.54455644 0.56598431 0.57396656 0.54941863 0.492733 0.40734166 0.30281249 0.19487678][0.34023818 0.44335598 0.51313812 0.54099894 0.53297055 0.5129419 0.50939643 0.52951455 0.5658555 0.59041864 0.57879597 0.52650172 0.43714145 0.32458916 0.20796736][0.29372522 0.38779494 0.45043418 0.47155952 0.45629224 0.43041772 0.4236775 0.445935 0.49026197 0.5275597 0.53058451 0.48973963 0.407696 0.30047578 0.18871665][0.21577731 0.289728 0.33894765 0.35331586 0.33614081 0.31004861 0.30065444 0.31890041 0.3606526 0.40105689 0.4144457 0.38828936 0.32349011 0.23486356 0.14160457][0.13039367 0.17978676 0.21273075 0.220747 0.20548131 0.18327194 0.17229843 0.18268211 0.21368255 0.2477293 0.26452386 0.25178745 0.20895979 0.14689656 0.0806347][0.058976665 0.086676687 0.10472826 0.10706288 0.094384581 0.076748446 0.065181382 0.067564167 0.0851964 0.10747896 0.12097852 0.11639298 0.092918493 0.057065174 0.018738648][0.0088982359 0.020667933 0.027792329 0.026296396 0.016602729 0.003939359 -0.005943595 -0.0081934761 -0.0013417044 0.0093518067 0.016641613 0.015216355 0.0047762236 -0.011391208 -0.027812818][-0.019890562 -0.017667439 -0.016575202 -0.019059386 -0.024864262 -0.031936046 -0.037971679 -0.040840287 -0.039690889 -0.036332645 -0.034114003 -0.035305515 -0.039483376 -0.044990104 -0.04918262]]...]
INFO - root - 2017-12-10 11:44:48.644601: step 12210, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.036 sec/batch; 92h:10m:04s remains)
INFO - root - 2017-12-10 11:44:58.819987: step 12220, loss = 0.69, batch loss = 0.64 (7.9 examples/sec; 1.009 sec/batch; 89h:45m:44s remains)
INFO - root - 2017-12-10 11:45:08.903916: step 12230, loss = 0.70, batch loss = 0.65 (7.6 examples/sec; 1.051 sec/batch; 93h:32m:08s remains)
INFO - root - 2017-12-10 11:45:18.944972: step 12240, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.023 sec/batch; 90h:59m:51s remains)
INFO - root - 2017-12-10 11:45:28.975967: step 12250, loss = 0.71, batch loss = 0.65 (8.2 examples/sec; 0.980 sec/batch; 87h:09m:22s remains)
INFO - root - 2017-12-10 11:45:39.074749: step 12260, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.038 sec/batch; 92h:19m:57s remains)
INFO - root - 2017-12-10 11:45:49.354682: step 12270, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.017 sec/batch; 90h:27m:13s remains)
INFO - root - 2017-12-10 11:45:59.525973: step 12280, loss = 0.69, batch loss = 0.63 (7.5 examples/sec; 1.063 sec/batch; 94h:35m:18s remains)
INFO - root - 2017-12-10 11:46:09.712673: step 12290, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.011 sec/batch; 89h:56m:42s remains)
INFO - root - 2017-12-10 11:46:19.878484: step 12300, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.022 sec/batch; 90h:55m:43s remains)
2017-12-10 11:46:20.885765: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.081407182 0.13674195 0.19552089 0.24128252 0.25955305 0.24084516 0.18692344 0.11984731 0.072390214 0.061685663 0.076886341 0.0993259 0.11843976 0.13229731 0.1356872][0.1104818 0.18427669 0.26238415 0.32447317 0.3533943 0.33770072 0.27797553 0.19818732 0.13371372 0.10605572 0.10311753 0.10389284 0.10154401 0.10040919 0.097824991][0.12993455 0.21764833 0.30975759 0.38444063 0.42491242 0.41963908 0.36717048 0.28908911 0.21744813 0.17286497 0.14383353 0.11235078 0.079599828 0.058353327 0.049259685][0.14262655 0.23961546 0.33972111 0.42181239 0.47188187 0.48070407 0.44562656 0.38311192 0.3168568 0.26264322 0.21055977 0.14734623 0.082481921 0.036868632 0.016212557][0.14125666 0.24130595 0.34355533 0.42799655 0.48358607 0.50536007 0.49003896 0.44764873 0.39139843 0.33320448 0.2663779 0.18234545 0.095537014 0.031071283 -0.0015913621][0.11726755 0.21478638 0.31954694 0.41280743 0.483094 0.52563417 0.53251237 0.50528044 0.44896907 0.37665737 0.29120257 0.19190624 0.095469452 0.024272233 -0.013540276][0.082188129 0.17084114 0.27719238 0.38598758 0.48248944 0.55480611 0.58525366 0.56604159 0.49750224 0.39797488 0.28636435 0.17436193 0.0781921 0.011536607 -0.023148164][0.05628252 0.13103598 0.23341718 0.35386795 0.47416621 0.57272196 0.62157273 0.60488218 0.52181649 0.39714843 0.26546344 0.14916332 0.061487682 0.0058681336 -0.022228563][0.046362583 0.10653473 0.20000221 0.32172498 0.45032805 0.55739641 0.6102106 0.58932424 0.49529749 0.35844153 0.22388746 0.11883368 0.0503168 0.011478211 -0.008333588][0.044555314 0.091781512 0.17429516 0.28850961 0.41007534 0.5081957 0.55064029 0.51931757 0.418881 0.2839399 0.16275196 0.081269979 0.039390143 0.021471864 0.012102525][0.041466363 0.076978594 0.14956425 0.25513223 0.3660863 0.45116639 0.4807108 0.44120753 0.34255958 0.22051069 0.12074339 0.064973384 0.046940722 0.044863377 0.0417506][0.035163786 0.057915732 0.11745322 0.20936866 0.30534455 0.37594447 0.396356 0.35838214 0.27484217 0.17824654 0.10724232 0.077466249 0.0774773 0.083273783 0.080786608][0.026087342 0.036876917 0.080563106 0.15310295 0.22902325 0.28335726 0.29770333 0.26820275 0.20843934 0.14519063 0.10687651 0.10200783 0.11592863 0.12604637 0.12371007][0.01357391 0.016237618 0.046419226 0.0999407 0.1555976 0.19442046 0.20459241 0.18603499 0.15205525 0.12266226 0.1148926 0.13023868 0.15278029 0.16359267 0.16082814][-0.00270179 -0.0046906094 0.016235143 0.055277411 0.095387526 0.12333223 0.13254975 0.12617424 0.11678109 0.11702764 0.13257557 0.15873241 0.1812391 0.1874954 0.1831979]]...]
INFO - root - 2017-12-10 11:46:31.031453: step 12310, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.034 sec/batch; 91h:58m:55s remains)
INFO - root - 2017-12-10 11:46:41.215762: step 12320, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.008 sec/batch; 89h:38m:03s remains)
INFO - root - 2017-12-10 11:46:51.480327: step 12330, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.036 sec/batch; 92h:07m:40s remains)
INFO - root - 2017-12-10 11:47:01.111186: step 12340, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.992 sec/batch; 88h:14m:19s remains)
INFO - root - 2017-12-10 11:47:11.378579: step 12350, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.022 sec/batch; 90h:52m:09s remains)
INFO - root - 2017-12-10 11:47:21.456930: step 12360, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.023 sec/batch; 91h:00m:19s remains)
INFO - root - 2017-12-10 11:47:31.404518: step 12370, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.021 sec/batch; 90h:48m:31s remains)
INFO - root - 2017-12-10 11:47:41.392243: step 12380, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.009 sec/batch; 89h:44m:27s remains)
INFO - root - 2017-12-10 11:47:51.556768: step 12390, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.008 sec/batch; 89h:38m:43s remains)
INFO - root - 2017-12-10 11:48:01.582194: step 12400, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.990 sec/batch; 88h:00m:32s remains)
2017-12-10 11:48:02.555302: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18580504 0.17311756 0.14644337 0.11338059 0.079941571 0.053984478 0.046810206 0.060609329 0.087762453 0.11524729 0.13397166 0.13954625 0.12635766 0.0969407 0.062956966][0.17103127 0.16177896 0.14129806 0.11635044 0.091083273 0.072077475 0.06936156 0.0846619 0.11049863 0.13402675 0.14715505 0.1470291 0.13008927 0.1001529 0.0682249][0.14394885 0.14039871 0.1277245 0.11188987 0.096918054 0.088139117 0.091186643 0.10533448 0.12402294 0.13680194 0.13877504 0.12979707 0.11073118 0.086138919 0.063611314][0.13484776 0.13591565 0.12769464 0.11745474 0.11168862 0.11574589 0.12801474 0.14178617 0.15057746 0.14780839 0.13418885 0.11442648 0.095913015 0.082620524 0.075960465][0.1547433 0.15580261 0.14714094 0.13933653 0.14206125 0.16103449 0.18635683 0.20308356 0.20350096 0.18495151 0.15504751 0.12562232 0.11125673 0.11407679 0.12686841][0.18783621 0.18466593 0.17340323 0.16874513 0.18142432 0.21573906 0.25410655 0.27457887 0.26884529 0.23806959 0.19618332 0.16264485 0.1568249 0.17777751 0.20917055][0.2096674 0.20322438 0.19231893 0.19413577 0.21693565 0.26140144 0.30518073 0.32481006 0.31351465 0.27586839 0.22967854 0.19910628 0.20436931 0.24052061 0.28424963][0.20877433 0.20287496 0.1976265 0.20839426 0.23785076 0.28227997 0.31938678 0.33057868 0.31269357 0.27254841 0.2281955 0.2039618 0.21724464 0.25957596 0.30529633][0.19333275 0.19295993 0.19817351 0.21856686 0.24986039 0.28493083 0.30572823 0.30233815 0.27598751 0.23441288 0.19304176 0.17230219 0.1844835 0.21938936 0.25457367][0.18325274 0.19244242 0.21016908 0.23831692 0.26695639 0.28805014 0.29011467 0.27204421 0.23827989 0.19586433 0.15575856 0.13251947 0.13321888 0.14786534 0.16091163][0.18666793 0.20558232 0.23345681 0.26538911 0.28859779 0.29695117 0.2859419 0.26088628 0.22699437 0.1887307 0.15126191 0.1224663 0.10563648 0.093361624 0.078151986][0.199099 0.222571 0.25299534 0.28255475 0.29925346 0.30060238 0.28745222 0.26758981 0.24443519 0.21870005 0.1897171 0.15860081 0.12576044 0.087642774 0.044718903][0.2107666 0.23122445 0.25524774 0.27650875 0.286672 0.28709584 0.28196335 0.27831441 0.27552113 0.26972681 0.25560114 0.22870839 0.18721172 0.13084982 0.066811845][0.21394019 0.22585869 0.23772959 0.24771324 0.25204006 0.25480983 0.26222119 0.27956694 0.30109715 0.31801134 0.32150519 0.30378312 0.26177171 0.19751559 0.12259201][0.20257004 0.20559931 0.20784639 0.21019252 0.21122211 0.21611273 0.23283324 0.26653478 0.30815148 0.34437129 0.36326241 0.35578227 0.31919894 0.25721473 0.1825444]]...]
INFO - root - 2017-12-10 11:48:12.679195: step 12410, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.043 sec/batch; 92h:46m:53s remains)
INFO - root - 2017-12-10 11:48:22.650922: step 12420, loss = 0.67, batch loss = 0.62 (8.1 examples/sec; 0.988 sec/batch; 87h:50m:16s remains)
INFO - root - 2017-12-10 11:48:32.869012: step 12430, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.026 sec/batch; 91h:14m:14s remains)
INFO - root - 2017-12-10 11:48:43.092119: step 12440, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.009 sec/batch; 89h:43m:16s remains)
INFO - root - 2017-12-10 11:48:53.303153: step 12450, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 0.999 sec/batch; 88h:49m:21s remains)
INFO - root - 2017-12-10 11:49:03.270088: step 12460, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.030 sec/batch; 91h:34m:16s remains)
INFO - root - 2017-12-10 11:49:13.384938: step 12470, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.024 sec/batch; 91h:04m:08s remains)
INFO - root - 2017-12-10 11:49:23.625142: step 12480, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.992 sec/batch; 88h:12m:08s remains)
INFO - root - 2017-12-10 11:49:33.769398: step 12490, loss = 0.70, batch loss = 0.64 (7.6 examples/sec; 1.049 sec/batch; 93h:14m:48s remains)
INFO - root - 2017-12-10 11:49:43.723930: step 12500, loss = 0.68, batch loss = 0.62 (7.8 examples/sec; 1.019 sec/batch; 90h:37m:10s remains)
2017-12-10 11:49:44.671019: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24339594 0.26833826 0.27951509 0.27482113 0.25793028 0.23916328 0.22696967 0.21391797 0.19558811 0.17512058 0.15303233 0.12408096 0.089440376 0.064191245 0.052619305][0.30339876 0.3268857 0.33353981 0.32512182 0.30509457 0.2830984 0.26828882 0.25265917 0.22993471 0.20222953 0.17192096 0.1364592 0.09768904 0.0691513 0.054850806][0.32483181 0.34606677 0.34960255 0.34011823 0.32051814 0.29899123 0.28417647 0.26768166 0.24211204 0.20874488 0.17304219 0.13605712 0.099060446 0.071473561 0.056292176][0.3089824 0.32752049 0.33267576 0.32888588 0.31648955 0.30107996 0.289907 0.27380165 0.24483256 0.20527901 0.16533922 0.12982649 0.0988152 0.076140158 0.062892094][0.2736662 0.29059705 0.30213076 0.31039318 0.31052408 0.30443645 0.29808986 0.28178865 0.24831107 0.20288475 0.1611508 0.13068402 0.10893588 0.0938526 0.0839876][0.23190784 0.246344 0.26437452 0.28590229 0.29951295 0.30305794 0.30187213 0.28673333 0.25111097 0.2028 0.16248213 0.13973002 0.12874818 0.12161371 0.11466824][0.18987785 0.20270789 0.22510193 0.25660005 0.28064722 0.29179743 0.29561561 0.28494069 0.25289473 0.20693299 0.17118299 0.15692325 0.15567239 0.1550799 0.15012968][0.15189709 0.16323109 0.18832944 0.22714594 0.26004004 0.27841568 0.28731132 0.28299654 0.25808337 0.21744712 0.18609734 0.17702557 0.18123978 0.18425319 0.18024413][0.13135746 0.1400349 0.16432676 0.2063985 0.24565569 0.27044228 0.28394419 0.28552333 0.26781493 0.23326784 0.20509148 0.19698519 0.20120332 0.20386018 0.19933048][0.1369098 0.14162946 0.16026887 0.19904728 0.2394028 0.26784438 0.28411165 0.28931472 0.27739862 0.2500267 0.22638929 0.21882378 0.22094339 0.22141868 0.21589255][0.15213175 0.15217923 0.16236927 0.19257049 0.22907804 0.25872725 0.27714002 0.2848984 0.27799627 0.25936893 0.24337265 0.23898125 0.23978633 0.2379493 0.23127736][0.17373335 0.1702352 0.17243683 0.19215336 0.22151202 0.24969317 0.26890481 0.27750623 0.27344921 0.26238057 0.25455979 0.254853 0.25558323 0.25207576 0.2440435][0.19716339 0.19059128 0.18503459 0.19411543 0.21509227 0.24007134 0.259259 0.2680307 0.26513869 0.25902149 0.25715229 0.26084828 0.26138344 0.2567701 0.24832848][0.20112501 0.19103323 0.17858091 0.17788951 0.19001688 0.21018735 0.22870252 0.23863234 0.23831424 0.23715331 0.2402167 0.2462611 0.24632688 0.2410114 0.23370685][0.17363302 0.15993394 0.14267789 0.13443196 0.13844927 0.15279819 0.16949101 0.18084532 0.18451768 0.18930836 0.19775902 0.20663248 0.207181 0.20192161 0.19624589]]...]
INFO - root - 2017-12-10 11:49:54.822985: step 12510, loss = 0.69, batch loss = 0.63 (7.5 examples/sec; 1.067 sec/batch; 94h:49m:51s remains)
INFO - root - 2017-12-10 11:50:04.999158: step 12520, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.024 sec/batch; 91h:03m:10s remains)
INFO - root - 2017-12-10 11:50:14.982361: step 12530, loss = 0.69, batch loss = 0.64 (8.2 examples/sec; 0.977 sec/batch; 86h:50m:16s remains)
INFO - root - 2017-12-10 11:50:25.034315: step 12540, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.987 sec/batch; 87h:44m:36s remains)
INFO - root - 2017-12-10 11:50:35.150462: step 12550, loss = 0.70, batch loss = 0.64 (7.6 examples/sec; 1.047 sec/batch; 93h:01m:42s remains)
INFO - root - 2017-12-10 11:50:45.165690: step 12560, loss = 0.70, batch loss = 0.65 (7.7 examples/sec; 1.038 sec/batch; 92h:16m:45s remains)
INFO - root - 2017-12-10 11:50:55.260279: step 12570, loss = 0.67, batch loss = 0.62 (8.1 examples/sec; 0.993 sec/batch; 88h:17m:17s remains)
INFO - root - 2017-12-10 11:51:05.354160: step 12580, loss = 0.70, batch loss = 0.65 (7.7 examples/sec; 1.041 sec/batch; 92h:31m:52s remains)
INFO - root - 2017-12-10 11:51:15.609680: step 12590, loss = 0.70, batch loss = 0.64 (7.4 examples/sec; 1.083 sec/batch; 96h:15m:59s remains)
INFO - root - 2017-12-10 11:51:25.765643: step 12600, loss = 0.70, batch loss = 0.64 (8.2 examples/sec; 0.975 sec/batch; 86h:37m:20s remains)
2017-12-10 11:51:26.728028: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.43665451 0.4120751 0.36689195 0.33659598 0.32904372 0.33234695 0.34190595 0.36101863 0.38213819 0.38702145 0.36764756 0.32878444 0.27767745 0.2107932 0.13361417][0.46866956 0.44901246 0.4076373 0.38154438 0.377815 0.38370511 0.39527777 0.41323438 0.42871371 0.42483377 0.39716494 0.35214373 0.29720289 0.22802025 0.14823672][0.46606559 0.44947121 0.41218367 0.39081034 0.39215365 0.40420359 0.42206147 0.4418146 0.453511 0.44359949 0.40980676 0.3591556 0.29924282 0.22771327 0.14812236][0.46292222 0.4423098 0.40165085 0.37966669 0.38576308 0.40892121 0.44002858 0.46823555 0.48182216 0.46944809 0.42970413 0.37005612 0.29929915 0.21993501 0.13844208][0.48351887 0.45297584 0.39911556 0.36732242 0.37506893 0.41319907 0.46499339 0.51037508 0.53366071 0.52255744 0.47489235 0.40017739 0.31111589 0.2168878 0.12912187][0.53855693 0.49494013 0.42110243 0.37230751 0.37759909 0.42962435 0.50217396 0.56662589 0.60329205 0.595882 0.54008365 0.44849905 0.33945137 0.22837496 0.13130939][0.61034691 0.55479753 0.461163 0.39557004 0.3973248 0.45859835 0.54411227 0.62118006 0.668697 0.66643912 0.60585248 0.50175196 0.3777234 0.25300983 0.1462663][0.6601004 0.59746265 0.49255753 0.41872114 0.42011598 0.485413 0.57267278 0.6515485 0.70375681 0.70724022 0.64807451 0.5407359 0.41176593 0.2806035 0.16665398][0.64367223 0.58150673 0.47893673 0.40876967 0.41291642 0.47565371 0.554065 0.62571317 0.67724681 0.68697715 0.63741142 0.54014575 0.42095962 0.29497966 0.18062292][0.55045515 0.4946686 0.40616038 0.3485496 0.35541126 0.40849051 0.47019705 0.52867728 0.575663 0.59139919 0.55698097 0.48092633 0.38497207 0.27705213 0.17270108][0.40294942 0.35663086 0.28813645 0.24696611 0.25471067 0.29297733 0.33394435 0.37622553 0.41539529 0.43424264 0.4152599 0.36437881 0.29803675 0.21713616 0.13334487][0.23886491 0.20363945 0.15566164 0.12986799 0.13692135 0.1604521 0.18306042 0.21045734 0.24025454 0.25812024 0.24951941 0.21896249 0.17877643 0.12567452 0.068021581][0.099304326 0.074548364 0.043586709 0.028672284 0.033688813 0.046161067 0.056821503 0.07327085 0.093756653 0.10703629 0.10287877 0.085232489 0.06330236 0.033023685 9.1934206e-05][0.0086378176 -0.0078026354 -0.026804391 -0.035894543 -0.03387785 -0.028551366 -0.023982439 -0.01461255 -0.0022986257 0.0050721983 0.0018947521 -0.0082829921 -0.019278143 -0.033928525 -0.048445135][-0.033168424 -0.043356564 -0.054005984 -0.05961784 -0.059613414 -0.057813693 -0.055799775 -0.051086925 -0.045262862 -0.043083258 -0.046739459 -0.053452481 -0.059521548 -0.065944724 -0.070206448]]...]
INFO - root - 2017-12-10 11:51:36.868203: step 12610, loss = 0.70, batch loss = 0.64 (7.4 examples/sec; 1.079 sec/batch; 95h:54m:32s remains)
INFO - root - 2017-12-10 11:51:47.092862: step 12620, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.031 sec/batch; 91h:39m:09s remains)
INFO - root - 2017-12-10 11:51:57.245072: step 12630, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.022 sec/batch; 90h:47m:24s remains)
INFO - root - 2017-12-10 11:52:07.196279: step 12640, loss = 0.68, batch loss = 0.63 (8.0 examples/sec; 1.005 sec/batch; 89h:17m:56s remains)
INFO - root - 2017-12-10 11:52:17.316129: step 12650, loss = 0.68, batch loss = 0.62 (7.8 examples/sec; 1.022 sec/batch; 90h:50m:09s remains)
INFO - root - 2017-12-10 11:52:27.287711: step 12660, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 0.996 sec/batch; 88h:30m:27s remains)
INFO - root - 2017-12-10 11:52:37.372553: step 12670, loss = 0.69, batch loss = 0.64 (7.7 examples/sec; 1.038 sec/batch; 92h:15m:11s remains)
INFO - root - 2017-12-10 11:52:47.565204: step 12680, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.008 sec/batch; 89h:31m:50s remains)
INFO - root - 2017-12-10 11:52:57.615358: step 12690, loss = 0.70, batch loss = 0.65 (8.3 examples/sec; 0.967 sec/batch; 85h:54m:23s remains)
INFO - root - 2017-12-10 11:53:07.800182: step 12700, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.043 sec/batch; 92h:38m:09s remains)
2017-12-10 11:53:08.871201: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32382968 0.3510983 0.33929363 0.31314585 0.27931103 0.24548736 0.21755125 0.1960782 0.18153113 0.18288185 0.20957084 0.24284379 0.27710015 0.31034592 0.33937466][0.33598769 0.369852 0.36339492 0.33908987 0.3015399 0.25834545 0.22120829 0.20210105 0.20005558 0.21719489 0.26053649 0.30847266 0.34979731 0.37932372 0.39997637][0.33154267 0.37051022 0.37023017 0.34993094 0.31252834 0.26473865 0.22157867 0.20583354 0.21534593 0.24464533 0.29813397 0.35437265 0.39866498 0.4235163 0.43594804][0.31920248 0.36051211 0.36551312 0.35016698 0.31646198 0.27024218 0.22675791 0.21549207 0.23374905 0.27054411 0.32807881 0.3863754 0.42972431 0.44976714 0.45474526][0.30800819 0.3472096 0.35366803 0.34199685 0.31532839 0.27853143 0.24365419 0.24168934 0.26993778 0.31357244 0.37268326 0.42858717 0.46764898 0.4833039 0.48157465][0.30095106 0.33772427 0.34479982 0.3373788 0.32044515 0.29774272 0.27622139 0.28489986 0.322128 0.37068349 0.42907661 0.47917184 0.51186794 0.52401406 0.51675004][0.32000527 0.36030421 0.37052298 0.36560142 0.35233864 0.33691183 0.32327169 0.33750093 0.3781758 0.42739505 0.48223484 0.52389348 0.54852396 0.55727935 0.54590809][0.36259267 0.41436312 0.43298817 0.4315607 0.41739973 0.40127864 0.38734645 0.3988204 0.43409461 0.47706661 0.52397829 0.55434966 0.56717557 0.56750864 0.54838443][0.41123536 0.47766867 0.504531 0.504211 0.48395592 0.45855823 0.43377441 0.43134132 0.45104086 0.48033437 0.51518518 0.534644 0.53788465 0.53090858 0.50566775][0.43686995 0.51070052 0.53786707 0.53233641 0.5015077 0.46213374 0.42151511 0.40111282 0.40269473 0.41661477 0.43859354 0.44902986 0.44670236 0.43631911 0.40961376][0.43210676 0.50020725 0.51790893 0.50193936 0.459673 0.40749282 0.35336038 0.31822726 0.30495924 0.30591071 0.31605616 0.31841522 0.31145281 0.29850134 0.2730535][0.40074971 0.45136175 0.45292172 0.4241991 0.37158012 0.30974793 0.24680953 0.20231152 0.1791126 0.17132455 0.1732741 0.17119385 0.16262481 0.149531 0.12833583][0.33488935 0.36395156 0.34960684 0.31181329 0.25537428 0.19198479 0.12951426 0.084711812 0.059666038 0.048853487 0.046569146 0.043091867 0.035599884 0.024882482 0.0093656238][0.25469956 0.26166457 0.23242988 0.18700826 0.12990232 0.0708834 0.017569516 -0.01796991 -0.036034279 -0.043471154 -0.045447621 -0.047404561 -0.051306814 -0.056972876 -0.065604068][0.17484909 0.1625457 0.12218069 0.073158316 0.020209523 -0.028108181 -0.065150619 -0.084397539 -0.090037555 -0.09036614 -0.089131705 -0.0885577 -0.089591086 -0.091860436 -0.096370436]]...]
INFO - root - 2017-12-10 11:53:19.036007: step 12710, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.036 sec/batch; 92h:00m:45s remains)
INFO - root - 2017-12-10 11:53:29.219756: step 12720, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 0.996 sec/batch; 88h:28m:32s remains)
INFO - root - 2017-12-10 11:53:39.137604: step 12730, loss = 0.71, batch loss = 0.66 (7.7 examples/sec; 1.036 sec/batch; 92h:02m:00s remains)
INFO - root - 2017-12-10 11:53:49.074094: step 12740, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.022 sec/batch; 90h:48m:06s remains)
INFO - root - 2017-12-10 11:53:59.120665: step 12750, loss = 0.70, batch loss = 0.65 (8.0 examples/sec; 1.004 sec/batch; 89h:11m:05s remains)
INFO - root - 2017-12-10 11:54:09.348326: step 12760, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.985 sec/batch; 87h:30m:32s remains)
INFO - root - 2017-12-10 11:54:19.598864: step 12770, loss = 0.72, batch loss = 0.66 (7.7 examples/sec; 1.040 sec/batch; 92h:22m:28s remains)
INFO - root - 2017-12-10 11:54:29.658544: step 12780, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.007 sec/batch; 89h:28m:21s remains)
INFO - root - 2017-12-10 11:54:39.755068: step 12790, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 0.994 sec/batch; 88h:16m:05s remains)
INFO - root - 2017-12-10 11:54:49.928840: step 12800, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.032 sec/batch; 91h:37m:39s remains)
2017-12-10 11:54:50.957897: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24040438 0.27130592 0.28641245 0.2784124 0.2463773 0.19828953 0.15087004 0.1193758 0.11256462 0.13243921 0.1695952 0.21506532 0.25295514 0.27411312 0.28240249][0.27758473 0.31601095 0.34244552 0.3483758 0.32914647 0.28890592 0.24202368 0.20100243 0.17636791 0.17723759 0.200246 0.23971006 0.27945229 0.3084386 0.32572812][0.2711252 0.31749475 0.36042866 0.39082351 0.39911056 0.3823902 0.34882161 0.30465016 0.26010358 0.23120433 0.22433656 0.24239631 0.27120814 0.30009943 0.32457545][0.22866908 0.2819306 0.34344029 0.40387222 0.44831556 0.46570113 0.45549032 0.41493061 0.35200685 0.28884533 0.24343973 0.22875932 0.23573869 0.25728953 0.28602642][0.16416386 0.22006185 0.29609594 0.38471535 0.46682021 0.52357727 0.5440141 0.51560897 0.44199023 0.34863856 0.264418 0.21291269 0.19221063 0.20155677 0.23172966][0.098069876 0.15075767 0.23338932 0.34187233 0.45554397 0.54916507 0.60185742 0.59258938 0.51918924 0.40678561 0.29154328 0.20727135 0.16007246 0.15526906 0.18318102][0.049464107 0.094146147 0.17385019 0.28936031 0.42142245 0.5409832 0.6201992 0.63170034 0.5677495 0.44953358 0.31633723 0.2099088 0.14305741 0.12656263 0.15144245][0.024098465 0.057069823 0.12435663 0.23156121 0.3632344 0.4905037 0.58281147 0.61040556 0.56178236 0.45175612 0.317956 0.20532377 0.13115472 0.10946143 0.1326247][0.022020714 0.0428621 0.0911647 0.17671293 0.28913867 0.40379879 0.49224833 0.52677631 0.49467605 0.40415272 0.28640467 0.18365489 0.11534603 0.096520342 0.12058478][0.032876827 0.044041023 0.072831579 0.13131724 0.21412288 0.30271885 0.37443191 0.40696028 0.38900873 0.322802 0.23077771 0.14849396 0.094518282 0.082679451 0.1079939][0.047872197 0.053825367 0.068514116 0.10322724 0.15606776 0.21473841 0.26381537 0.2876384 0.27824274 0.23416704 0.16960828 0.11129729 0.074380293 0.069842748 0.094365031][0.05892197 0.063281156 0.070087522 0.087920405 0.11637772 0.14833732 0.17517729 0.1873942 0.18096162 0.15350778 0.11335126 0.077909023 0.057070263 0.058428176 0.080414884][0.055325624 0.059156224 0.0624368 0.070406131 0.082475394 0.095029965 0.10466192 0.106806 0.10057437 0.084370248 0.0633063 0.046798728 0.03905987 0.044371713 0.063016415][0.037704859 0.04081656 0.042738628 0.046054758 0.049319316 0.05082773 0.05043821 0.04682228 0.040744204 0.031621113 0.023210114 0.01944181 0.01991009 0.026286587 0.040560868][0.0078889849 0.00957659 0.010921553 0.012221011 0.011699662 0.008824951 0.004844062 -0.00011942006 -0.0049072132 -0.0094950562 -0.010874712 -0.0082304841 -0.0045142309 0.0006573468 0.010315108]]...]
INFO - root - 2017-12-10 11:55:01.118137: step 12810, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 1.005 sec/batch; 89h:17m:11s remains)
INFO - root - 2017-12-10 11:55:10.902505: step 12820, loss = 0.72, batch loss = 0.66 (7.7 examples/sec; 1.039 sec/batch; 92h:14m:55s remains)
INFO - root - 2017-12-10 11:55:21.009204: step 12830, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.006 sec/batch; 89h:21m:07s remains)
INFO - root - 2017-12-10 11:55:31.101122: step 12840, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.034 sec/batch; 91h:48m:47s remains)
INFO - root - 2017-12-10 11:55:41.343760: step 12850, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.024 sec/batch; 90h:54m:11s remains)
INFO - root - 2017-12-10 11:55:51.492047: step 12860, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.014 sec/batch; 90h:04m:19s remains)
INFO - root - 2017-12-10 11:56:01.641602: step 12870, loss = 0.72, batch loss = 0.66 (8.0 examples/sec; 1.001 sec/batch; 88h:54m:48s remains)
INFO - root - 2017-12-10 11:56:11.787030: step 12880, loss = 0.69, batch loss = 0.64 (7.9 examples/sec; 1.010 sec/batch; 89h:39m:40s remains)
INFO - root - 2017-12-10 11:56:21.931631: step 12890, loss = 0.69, batch loss = 0.63 (7.6 examples/sec; 1.058 sec/batch; 93h:57m:42s remains)
INFO - root - 2017-12-10 11:56:31.912802: step 12900, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.013 sec/batch; 89h:54m:33s remains)
2017-12-10 11:56:32.842904: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012306063 -0.0045171836 -0.010231924 -0.0058259424 0.0027678881 0.012417832 0.021431632 0.02814538 0.033275872 0.038076032 0.044320133 0.052792385 0.062097818 0.06952358 0.071977518][0.044793338 0.018236371 0.0075081675 0.013637009 0.029464109 0.049219262 0.067456543 0.080690086 0.090507954 0.098927863 0.10823264 0.12018275 0.13338393 0.14355265 0.14532511][0.080474183 0.044671189 0.028999066 0.038192395 0.064424776 0.097950891 0.12796043 0.14842223 0.16171198 0.17090625 0.17975047 0.1925825 0.20852534 0.22166151 0.22417714][0.1131999 0.071067981 0.052658346 0.066722333 0.1052661 0.15393803 0.19616514 0.22298029 0.23743717 0.24373642 0.24777836 0.25741529 0.27312961 0.2879726 0.29192543][0.1390446 0.094427966 0.074953519 0.094549887 0.14478238 0.20672731 0.25914118 0.29037142 0.30377948 0.3045679 0.30120608 0.30534697 0.31941104 0.33600184 0.34289035][0.16130723 0.11744189 0.096915349 0.120433 0.1793973 0.25065118 0.30891317 0.34028602 0.34807435 0.33931807 0.32583839 0.32329166 0.3359642 0.35521558 0.36647329][0.18868741 0.14739643 0.12422154 0.1474427 0.20970546 0.28439179 0.34263271 0.36861318 0.36572102 0.34362751 0.31792426 0.30746302 0.31783643 0.33937073 0.35570747][0.22436452 0.18612388 0.15841486 0.1762533 0.23455204 0.30524996 0.35771671 0.37494862 0.36073831 0.32719374 0.29225075 0.27548876 0.28271583 0.3045328 0.32480311][0.25863203 0.22397579 0.19158003 0.20038097 0.24781117 0.30728498 0.34911111 0.35693535 0.33528659 0.29759371 0.26092437 0.242486 0.24715991 0.26708764 0.28851941][0.28439587 0.25471762 0.21830508 0.21519868 0.24589808 0.28799704 0.31548944 0.3149322 0.29105294 0.25681818 0.22595409 0.21160349 0.21680784 0.23523849 0.25665897][0.30029997 0.27954113 0.24308698 0.22858518 0.24035603 0.26240003 0.27443519 0.26717877 0.24451101 0.21705841 0.19501403 0.18761551 0.19575645 0.21391697 0.23503524][0.30568728 0.2975848 0.26580468 0.24239753 0.23571998 0.23795393 0.23651972 0.22608876 0.20865029 0.19066928 0.17884772 0.17884071 0.18946785 0.20621422 0.22519438][0.30377755 0.30953214 0.28549954 0.25629982 0.23318203 0.21727605 0.20491187 0.1942019 0.18468513 0.17775477 0.176521 0.18329407 0.19487299 0.20768176 0.22114316][0.30190226 0.32204086 0.30794844 0.27663496 0.24080236 0.2093993 0.18725833 0.17538105 0.1710526 0.17078236 0.17470232 0.18298899 0.19137076 0.19734685 0.20292202][0.30546454 0.33990082 0.33710212 0.3076503 0.26457873 0.2220293 0.19103028 0.17437141 0.16798468 0.16565625 0.16623479 0.16922943 0.17040461 0.16830692 0.16608228]]...]
INFO - root - 2017-12-10 11:56:42.874708: step 12910, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.036 sec/batch; 92h:00m:04s remains)
INFO - root - 2017-12-10 11:56:52.960753: step 12920, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.011 sec/batch; 89h:45m:20s remains)
INFO - root - 2017-12-10 11:57:02.980844: step 12930, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.041 sec/batch; 92h:26m:32s remains)
INFO - root - 2017-12-10 11:57:13.088907: step 12940, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.018 sec/batch; 90h:19m:47s remains)
INFO - root - 2017-12-10 11:57:23.130286: step 12950, loss = 0.72, batch loss = 0.66 (8.0 examples/sec; 0.994 sec/batch; 88h:13m:44s remains)
INFO - root - 2017-12-10 11:57:33.297818: step 12960, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.014 sec/batch; 89h:57m:34s remains)
INFO - root - 2017-12-10 11:57:43.619291: step 12970, loss = 0.70, batch loss = 0.65 (8.0 examples/sec; 1.003 sec/batch; 88h:59m:32s remains)
INFO - root - 2017-12-10 11:57:53.579332: step 12980, loss = 0.68, batch loss = 0.62 (7.8 examples/sec; 1.026 sec/batch; 91h:05m:57s remains)
INFO - root - 2017-12-10 11:58:03.730445: step 12990, loss = 0.68, batch loss = 0.62 (7.9 examples/sec; 1.011 sec/batch; 89h:44m:07s remains)
INFO - root - 2017-12-10 11:58:13.772065: step 13000, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.032 sec/batch; 91h:32m:57s remains)
2017-12-10 11:58:14.811795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.03737583 -0.03027737 -0.017854648 -0.0066686193 0.0005181313 0.0052441549 0.010527421 0.020889748 0.038682558 0.059649762 0.077716753 0.084099673 0.075893894 0.058691487 0.040662173][-0.026851781 -0.0183595 -0.00011920166 0.019423997 0.03557634 0.047721773 0.057315737 0.069912374 0.088742316 0.11032518 0.12735072 0.12955049 0.11519678 0.091539524 0.06841772][0.0025513154 0.010763192 0.034600254 0.064809404 0.095037319 0.11981937 0.13599682 0.14814892 0.16021201 0.17178854 0.176477 0.16546188 0.14059345 0.11010111 0.083961904][0.052467268 0.059066527 0.088916555 0.13327973 0.18427125 0.22829427 0.25406247 0.2626847 0.25811529 0.24568006 0.22364463 0.18888365 0.14843021 0.11108858 0.084743977][0.11805235 0.12264799 0.16062653 0.22464415 0.30410257 0.37427124 0.41327319 0.41595379 0.38574696 0.33640927 0.27476767 0.20692708 0.14654684 0.10216811 0.077507593][0.18701708 0.18949211 0.23620164 0.32253087 0.43360755 0.53231007 0.58608061 0.58262479 0.52509707 0.43575129 0.33148721 0.22856684 0.14692976 0.095061146 0.071843334][0.24337974 0.24288402 0.29496804 0.39901879 0.53562278 0.656772 0.7221579 0.71387738 0.63617623 0.51668817 0.380222 0.25085127 0.15274155 0.0946029 0.071552046][0.2717652 0.26797232 0.31904948 0.42840564 0.57329005 0.70034814 0.76741588 0.75524169 0.66983038 0.53933215 0.39203817 0.25489581 0.15221131 0.092826024 0.070459262][0.2725839 0.26698324 0.30973804 0.40839326 0.5395751 0.6524666 0.7094959 0.69405007 0.61312294 0.49097323 0.35458618 0.22879981 0.13483866 0.081047781 0.061641239][0.26423866 0.25902894 0.28763172 0.36181539 0.46100679 0.54392654 0.58166856 0.5627051 0.49377912 0.39259371 0.2808812 0.17801026 0.10177147 0.058992937 0.045432191][0.26194865 0.25892255 0.27148855 0.31407851 0.37129357 0.41545948 0.42797604 0.4035 0.34814987 0.27216956 0.189987 0.11444657 0.060273182 0.03237465 0.027593538][0.27054456 0.27111116 0.2702131 0.28260663 0.29793245 0.30243227 0.28769213 0.25450298 0.20888922 0.1550117 0.10038591 0.051495604 0.020184916 0.0088019874 0.014220193][0.28818884 0.29021516 0.27746347 0.26353291 0.2427891 0.21337424 0.17526488 0.13403563 0.095537342 0.059926756 0.029157408 0.0041016773 -0.0065037007 -0.0033546907 0.010239754][0.30912802 0.30748421 0.2809712 0.24366333 0.19529979 0.14296003 0.091958925 0.04947675 0.020122938 0.0011063691 -0.0095887994 -0.015350572 -0.01072424 0.0010874482 0.017129304][0.32889274 0.31944925 0.27815431 0.22120167 0.15389159 0.0893461 0.035618376 -0.0012044907 -0.018844014 -0.022431424 -0.01682956 -0.0085416455 0.004635239 0.018027779 0.029953485]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 11:58:25.018485: step 13010, loss = 0.66, batch loss = 0.61 (7.5 examples/sec; 1.064 sec/batch; 94h:26m:15s remains)
INFO - root - 2017-12-10 11:58:35.224200: step 13020, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.023 sec/batch; 90h:45m:16s remains)
INFO - root - 2017-12-10 11:58:45.401524: step 13030, loss = 0.69, batch loss = 0.63 (7.5 examples/sec; 1.069 sec/batch; 94h:51m:40s remains)
INFO - root - 2017-12-10 11:58:55.507051: step 13040, loss = 0.69, batch loss = 0.63 (7.6 examples/sec; 1.048 sec/batch; 92h:58m:08s remains)
INFO - root - 2017-12-10 11:59:05.531493: step 13050, loss = 0.71, batch loss = 0.65 (9.2 examples/sec; 0.873 sec/batch; 77h:29m:04s remains)
INFO - root - 2017-12-10 11:59:15.663211: step 13060, loss = 0.71, batch loss = 0.66 (8.1 examples/sec; 0.990 sec/batch; 87h:51m:26s remains)
INFO - root - 2017-12-10 11:59:25.783996: step 13070, loss = 0.70, batch loss = 0.64 (8.5 examples/sec; 0.945 sec/batch; 83h:52m:30s remains)
INFO - root - 2017-12-10 11:59:35.636983: step 13080, loss = 0.68, batch loss = 0.62 (9.1 examples/sec; 0.878 sec/batch; 77h:51m:34s remains)
INFO - root - 2017-12-10 11:59:45.831014: step 13090, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.028 sec/batch; 91h:15m:00s remains)
INFO - root - 2017-12-10 11:59:55.987982: step 13100, loss = 0.68, batch loss = 0.63 (8.0 examples/sec; 1.003 sec/batch; 88h:57m:04s remains)
2017-12-10 11:59:56.876195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.060678829 -0.046473417 -0.026544884 -0.005953582 0.0094687045 0.015022362 0.012335042 0.0051075965 0.00035745528 0.0040791277 0.015843583 0.027033519 0.032252155 0.030716836 0.017701494][-0.034209248 0.0005886021 0.0453266 0.090837605 0.12691842 0.14499618 0.14695247 0.1377542 0.13021506 0.14052409 0.16803312 0.19416425 0.20638394 0.20210539 0.17574561][-0.00050789648 0.060913973 0.13763435 0.21479999 0.27841562 0.31622195 0.32933974 0.32052383 0.30903304 0.32785055 0.37768173 0.42583549 0.44773862 0.44009969 0.39779332][0.033868134 0.12575537 0.23664671 0.34467965 0.43429115 0.49164626 0.5175088 0.50977451 0.49529973 0.52396864 0.5967443 0.66562688 0.69325453 0.67708194 0.61440825][0.069443941 0.19347033 0.33866227 0.4758994 0.5878157 0.66045737 0.69647431 0.69070852 0.67640269 0.71310389 0.79960567 0.87751693 0.89928675 0.86319548 0.77156633][0.093814045 0.24062869 0.41068733 0.5701704 0.698871 0.78530192 0.83579689 0.840646 0.83262753 0.87373918 0.96172923 1.0337776 1.036383 0.9703058 0.84681386][0.09574201 0.24964117 0.42968789 0.59967363 0.73827618 0.838491 0.90859431 0.93144697 0.93367976 0.97495586 1.0534053 1.1073115 1.0839468 0.98803347 0.84056979][0.082476854 0.2304894 0.4055351 0.56989145 0.70341861 0.80483544 0.88329923 0.91669929 0.92374521 0.9574312 1.0163977 1.0457503 0.99952155 0.88620865 0.73452032][0.062583633 0.19350517 0.3486248 0.4924247 0.60668027 0.69351655 0.76294059 0.79401857 0.79897606 0.81843203 0.85037929 0.85284597 0.79190975 0.67791009 0.54090345][0.032825336 0.13545628 0.25745505 0.36946225 0.45614964 0.52054042 0.57193124 0.59367096 0.59249735 0.5958339 0.60238177 0.58486086 0.52217311 0.4239172 0.3164905][-0.0089675141 0.058623064 0.14065543 0.21514063 0.27095821 0.31092846 0.34212837 0.35213256 0.34370592 0.33471125 0.32618874 0.3027035 0.25278056 0.18316887 0.11374874][-0.050706834 -0.015562017 0.029784305 0.069558844 0.097108185 0.11412093 0.12501188 0.12195037 0.10618416 0.089782216 0.075654931 0.05680988 0.027721213 -0.008995858 -0.041424561][-0.077378042 -0.064942628 -0.046206806 -0.032102644 -0.025962116 -0.027366677 -0.033609927 -0.047777139 -0.068139285 -0.086869814 -0.10061666 -0.11117145 -0.12106589 -0.13116951 -0.13673605][-0.090426877 -0.091431327 -0.089184858 -0.090835311 -0.097496666 -0.10944704 -0.12474807 -0.14309466 -0.16200998 -0.17735763 -0.18683137 -0.19024867 -0.18882145 -0.18467472 -0.17716947][-0.098367378 -0.10688746 -0.11291677 -0.12131378 -0.1320343 -0.14528131 -0.1598013 -0.17429002 -0.18670948 -0.19538715 -0.19914323 -0.1976887 -0.19181126 -0.18334912 -0.17271844]]...]
INFO - root - 2017-12-10 12:00:07.009751: step 13110, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.044 sec/batch; 92h:39m:47s remains)
INFO - root - 2017-12-10 12:00:17.092050: step 13120, loss = 0.71, batch loss = 0.66 (7.9 examples/sec; 1.013 sec/batch; 89h:52m:22s remains)
INFO - root - 2017-12-10 12:00:27.031279: step 13130, loss = 0.70, batch loss = 0.64 (9.3 examples/sec; 0.863 sec/batch; 76h:34m:23s remains)
INFO - root - 2017-12-10 12:00:37.211772: step 13140, loss = 0.71, batch loss = 0.66 (7.7 examples/sec; 1.034 sec/batch; 91h:44m:34s remains)
INFO - root - 2017-12-10 12:00:47.410500: step 13150, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.013 sec/batch; 89h:52m:58s remains)
INFO - root - 2017-12-10 12:00:57.712831: step 13160, loss = 0.71, batch loss = 0.65 (8.1 examples/sec; 0.986 sec/batch; 87h:30m:12s remains)
INFO - root - 2017-12-10 12:01:07.825522: step 13170, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.006 sec/batch; 89h:11m:56s remains)
INFO - root - 2017-12-10 12:01:18.012968: step 13180, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.016 sec/batch; 90h:08m:37s remains)
INFO - root - 2017-12-10 12:01:28.227070: step 13190, loss = 0.68, batch loss = 0.63 (7.9 examples/sec; 1.011 sec/batch; 89h:40m:47s remains)
INFO - root - 2017-12-10 12:01:38.300635: step 13200, loss = 0.71, batch loss = 0.65 (8.1 examples/sec; 0.990 sec/batch; 87h:48m:14s remains)
2017-12-10 12:01:39.318359: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0059831431 0.010066157 0.010491889 0.0076510734 0.0029716336 -0.0038458591 -0.0096865827 -0.012666911 -0.015946567 -0.020866051 -0.024920594 -0.019732747 -0.0077744517 0.0007312813 0.00082453614][0.0657065 0.07897222 0.084770255 0.085995734 0.083852768 0.076462984 0.068524837 0.063156269 0.055302221 0.043051355 0.0312411 0.036232896 0.054768223 0.068763383 0.068715461][0.14835837 0.17649852 0.19393328 0.20666076 0.21403179 0.21107961 0.20259044 0.19335535 0.17646714 0.14958727 0.12211543 0.12319425 0.1489585 0.17019084 0.17039698][0.23470417 0.28141433 0.31566727 0.34584329 0.36750942 0.37227926 0.363904 0.34942317 0.32073307 0.27505708 0.22744241 0.22025615 0.25134552 0.28134125 0.28393796][0.30723506 0.37232035 0.42455518 0.47392172 0.51113451 0.5243479 0.51597828 0.49584061 0.45635298 0.39369413 0.3277562 0.31091702 0.3438068 0.3798506 0.38372093][0.35315493 0.43029094 0.49359462 0.55583745 0.60491377 0.6259889 0.61902905 0.59640211 0.55288333 0.48301759 0.40706623 0.38117057 0.40912685 0.44281968 0.44308358][0.36846963 0.44951898 0.51559561 0.58333462 0.6398865 0.66821414 0.66609234 0.64798748 0.61001587 0.54437041 0.46835414 0.43652761 0.45538577 0.47983035 0.47173929][0.36492696 0.44542208 0.50944179 0.57624024 0.63457072 0.66662014 0.67011392 0.66080374 0.63468796 0.58025545 0.51042813 0.47684944 0.4874261 0.50161827 0.48358369][0.35458371 0.43534344 0.49528775 0.55346274 0.60396487 0.63240737 0.63914567 0.63997132 0.63002175 0.59235007 0.53483838 0.50443643 0.51016068 0.51681387 0.48986813][0.34727663 0.4307318 0.48643398 0.53117424 0.56761843 0.58782464 0.59481275 0.60290068 0.60799104 0.5879755 0.54405439 0.51842993 0.52260965 0.52799767 0.49888173][0.3379195 0.42090118 0.47026798 0.50030875 0.52163309 0.53297937 0.53981131 0.55312079 0.56911159 0.56281179 0.52962416 0.50736421 0.51195353 0.52383465 0.50510985][0.31524602 0.39493755 0.44011471 0.46112552 0.47236794 0.4775925 0.48430717 0.49923441 0.51785415 0.51589519 0.4874543 0.4665983 0.47313589 0.49579731 0.49561733][0.26870954 0.34141868 0.38406143 0.40168685 0.40827107 0.41009158 0.41509858 0.4260388 0.43870297 0.43239823 0.4029192 0.38163844 0.39056504 0.42417365 0.443515][0.19605289 0.25433323 0.28929108 0.30231169 0.30504921 0.30390218 0.30509734 0.30844894 0.31172103 0.29971904 0.27062657 0.25112119 0.26234877 0.3019512 0.33374888][0.11269478 0.14906748 0.16910934 0.17337283 0.17087696 0.16656666 0.16345075 0.15943217 0.15462638 0.14034978 0.11630393 0.10146458 0.11318611 0.15096138 0.1852534]]...]
INFO - root - 2017-12-10 12:01:49.316436: step 13210, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 1.002 sec/batch; 88h:53m:50s remains)
INFO - root - 2017-12-10 12:01:59.421336: step 13220, loss = 0.70, batch loss = 0.65 (8.1 examples/sec; 0.987 sec/batch; 87h:33m:44s remains)
INFO - root - 2017-12-10 12:02:09.676037: step 13230, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.032 sec/batch; 91h:30m:23s remains)
INFO - root - 2017-12-10 12:02:19.763254: step 13240, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.024 sec/batch; 90h:47m:14s remains)
INFO - root - 2017-12-10 12:02:29.858712: step 13250, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.007 sec/batch; 89h:20m:20s remains)
INFO - root - 2017-12-10 12:02:39.871856: step 13260, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.034 sec/batch; 91h:40m:13s remains)
INFO - root - 2017-12-10 12:02:50.156435: step 13270, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.019 sec/batch; 90h:23m:49s remains)
INFO - root - 2017-12-10 12:03:00.400119: step 13280, loss = 0.71, batch loss = 0.66 (8.2 examples/sec; 0.978 sec/batch; 86h:43m:14s remains)
INFO - root - 2017-12-10 12:03:10.375432: step 13290, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.004 sec/batch; 88h:59m:32s remains)
INFO - root - 2017-12-10 12:03:20.600531: step 13300, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.025 sec/batch; 90h:50m:25s remains)
2017-12-10 12:03:21.699302: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33626854 0.32724324 0.26519144 0.17433745 0.083237685 0.02711302 0.023065416 0.063093089 0.13138498 0.21271528 0.291659 0.34310445 0.34285653 0.29002917 0.20073085][0.30152351 0.29646251 0.24887919 0.18048677 0.1141164 0.076728411 0.081645 0.12612586 0.20428172 0.30692255 0.41438687 0.49257463 0.5087896 0.45531693 0.34401348][0.26247883 0.2671515 0.23953471 0.19668823 0.15611517 0.13681467 0.14871949 0.195101 0.28113407 0.403156 0.53517509 0.63448066 0.66352439 0.60807508 0.47508758][0.25563097 0.27132228 0.26073757 0.23705941 0.21497966 0.20856161 0.22521502 0.27204257 0.36161342 0.49306861 0.63507533 0.74040717 0.77131188 0.70928717 0.55696917][0.28293747 0.31403941 0.32308567 0.31974739 0.31639639 0.32396773 0.34701946 0.39226991 0.47450796 0.59400928 0.71925408 0.80585659 0.82150054 0.74577838 0.57765603][0.3348752 0.38750055 0.42171046 0.44526315 0.46658772 0.49318817 0.52595317 0.56737244 0.6295436 0.71251136 0.79297221 0.83692759 0.82063073 0.7244218 0.54555404][0.38794625 0.465355 0.52899587 0.58519548 0.63597804 0.68421549 0.72715783 0.76098585 0.79238164 0.82173413 0.84076595 0.83042461 0.77525753 0.65861183 0.47900724][0.41628665 0.51194066 0.59918237 0.68306392 0.75818181 0.821648 0.8683486 0.88958609 0.886594 0.86205596 0.8239522 0.768177 0.68381876 0.55771244 0.39173537][0.40028 0.497993 0.59175396 0.68645525 0.77100807 0.83782256 0.88042057 0.88841766 0.860553 0.8003763 0.72617513 0.64482671 0.54888183 0.42866397 0.28848651][0.32918391 0.41115302 0.49291754 0.58034325 0.65915787 0.71852535 0.75152826 0.74861908 0.70883733 0.63574892 0.55123305 0.46713847 0.37843165 0.27839616 0.17213669][0.20295848 0.25694177 0.31444389 0.38088769 0.441472 0.48422891 0.50368756 0.49281412 0.45213011 0.38500205 0.31190869 0.24457209 0.17878425 0.1097603 0.042180948][0.054278646 0.076327495 0.10512289 0.14375806 0.17953625 0.20176698 0.20787477 0.19386011 0.16089 0.11210387 0.063093051 0.023136036 -0.011723325 -0.045760624 -0.07575658][-0.067929506 -0.072986133 -0.069904141 -0.057691481 -0.044978458 -0.039699636 -0.042353176 -0.054542705 -0.074905209 -0.10126799 -0.12418263 -0.1376666 -0.14509864 -0.15057813 -0.15272771][-0.13879147 -0.15822908 -0.16934474 -0.17301773 -0.17400533 -0.17716509 -0.18260795 -0.19081059 -0.20021152 -0.20969062 -0.21462266 -0.21205635 -0.20432863 -0.19478638 -0.18339263][-0.16077971 -0.1816112 -0.1946723 -0.20280991 -0.20847066 -0.21389286 -0.21896248 -0.22369835 -0.22707002 -0.22840297 -0.22557506 -0.21746287 -0.20550448 -0.19186169 -0.1772121]]...]
INFO - root - 2017-12-10 12:03:31.796674: step 13310, loss = 0.69, batch loss = 0.64 (7.9 examples/sec; 1.009 sec/batch; 89h:27m:02s remains)
INFO - root - 2017-12-10 12:03:41.989265: step 13320, loss = 0.69, batch loss = 0.64 (7.6 examples/sec; 1.057 sec/batch; 93h:42m:40s remains)
INFO - root - 2017-12-10 12:03:52.049498: step 13330, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.989 sec/batch; 87h:42m:27s remains)
INFO - root - 2017-12-10 12:04:02.352397: step 13340, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.990 sec/batch; 87h:45m:53s remains)
INFO - root - 2017-12-10 12:04:12.270964: step 13350, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 0.995 sec/batch; 88h:14m:07s remains)
INFO - root - 2017-12-10 12:04:22.356457: step 13360, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.001 sec/batch; 88h:42m:12s remains)
INFO - root - 2017-12-10 12:04:32.425255: step 13370, loss = 0.71, batch loss = 0.66 (7.7 examples/sec; 1.038 sec/batch; 92h:02m:02s remains)
INFO - root - 2017-12-10 12:04:42.608839: step 13380, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.006 sec/batch; 89h:09m:21s remains)
INFO - root - 2017-12-10 12:04:52.694849: step 13390, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.038 sec/batch; 92h:01m:47s remains)
INFO - root - 2017-12-10 12:05:02.859985: step 13400, loss = 0.68, batch loss = 0.62 (7.8 examples/sec; 1.029 sec/batch; 91h:10m:54s remains)
2017-12-10 12:05:03.816613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.070105866 -0.058476947 -0.03013926 0.011058953 0.064015336 0.12911877 0.20479828 0.27625117 0.3335349 0.36999068 0.37550452 0.33844185 0.26040968 0.16298072 0.070572712][-0.069694817 -0.054301005 -0.019594759 0.029980272 0.091781482 0.16525638 0.2477846 0.32236123 0.37506881 0.39823678 0.38481456 0.32629105 0.22828929 0.11808254 0.024798948][-0.06560903 -0.045658793 -0.0040537873 0.054090776 0.12397736 0.20378865 0.29049066 0.36564335 0.4119519 0.42039222 0.38673836 0.30660588 0.1894583 0.068330772 -0.023918387][-0.058296856 -0.032904238 0.017072098 0.085815527 0.16567267 0.25237268 0.34245625 0.41672909 0.45593348 0.45022237 0.39837775 0.29967916 0.16644473 0.036078967 -0.056814019][-0.04910744 -0.017828874 0.042367198 0.12539627 0.22006388 0.31727204 0.41068932 0.47977036 0.50636256 0.48258576 0.41187277 0.29816732 0.15571283 0.022459108 -0.069321014][-0.03810396 0.00040721134 0.073483236 0.17531449 0.29085127 0.40394619 0.50170738 0.5611127 0.5679782 0.52019596 0.42817339 0.30254257 0.15831263 0.028902322 -0.05968371][-0.029418474 0.015414055 0.099978752 0.2194491 0.35644802 0.48845807 0.59423161 0.64644635 0.63449615 0.56237334 0.44843182 0.31141132 0.16794603 0.044807702 -0.039875254][-0.028564882 0.018893456 0.10939997 0.2393658 0.39068902 0.53640664 0.64817613 0.69502193 0.66843742 0.57683462 0.44478878 0.29884142 0.15863214 0.044861224 -0.031881791][-0.037184037 0.0067759939 0.09485618 0.22387931 0.37602383 0.52212942 0.63028443 0.6696189 0.63322365 0.53222865 0.39378232 0.24873561 0.11892902 0.020350449 -0.042166781][-0.052680865 -0.01784437 0.058652394 0.17378688 0.31152704 0.44324511 0.5368191 0.564187 0.52080107 0.42014289 0.28921577 0.15913017 0.051411096 -0.022736399 -0.063917652][-0.068258747 -0.045589726 0.012347703 0.10250032 0.2124733 0.31765866 0.38926765 0.40352279 0.35779589 0.26681367 0.15560481 0.051933244 -0.025651902 -0.070777424 -0.088708617][-0.082216546 -0.07350336 -0.039066195 0.018077061 0.090513967 0.16101909 0.20797379 0.21299274 0.17376694 0.10361501 0.023185443 -0.046408992 -0.091890365 -0.11057462 -0.10966477][-0.091724783 -0.095433 -0.082428172 -0.056036569 -0.019639026 0.016966512 0.040439192 0.038650561 0.009637801 -0.035854992 -0.082839444 -0.11842124 -0.13603051 -0.13547349 -0.122911][-0.093433522 -0.10430529 -0.10493925 -0.099226475 -0.088378 -0.076937325 -0.071448728 -0.078347012 -0.098390572 -0.1235136 -0.14412367 -0.15451804 -0.15364094 -0.14252958 -0.12520261][-0.089017786 -0.10210715 -0.10964926 -0.11529358 -0.1189843 -0.12213067 -0.1270667 -0.13627371 -0.14925919 -0.1605095 -0.16476847 -0.16066194 -0.14990695 -0.13422836 -0.11649095]]...]
INFO - root - 2017-12-10 12:05:13.937758: step 13410, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.016 sec/batch; 90h:02m:39s remains)
INFO - root - 2017-12-10 12:05:24.195642: step 13420, loss = 0.70, batch loss = 0.65 (7.8 examples/sec; 1.021 sec/batch; 90h:27m:44s remains)
INFO - root - 2017-12-10 12:05:34.404481: step 13430, loss = 0.69, batch loss = 0.64 (7.4 examples/sec; 1.080 sec/batch; 95h:42m:05s remains)
INFO - root - 2017-12-10 12:05:44.446360: step 13440, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.026 sec/batch; 90h:54m:21s remains)
INFO - root - 2017-12-10 12:05:54.474573: step 13450, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 0.997 sec/batch; 88h:22m:31s remains)
INFO - root - 2017-12-10 12:06:04.670787: step 13460, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.000 sec/batch; 88h:35m:21s remains)
INFO - root - 2017-12-10 12:06:14.881387: step 13470, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.011 sec/batch; 89h:37m:07s remains)
INFO - root - 2017-12-10 12:06:25.072164: step 13480, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.016 sec/batch; 90h:03m:35s remains)
INFO - root - 2017-12-10 12:06:35.281121: step 13490, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.991 sec/batch; 87h:48m:01s remains)
INFO - root - 2017-12-10 12:06:45.452972: step 13500, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.038 sec/batch; 91h:59m:56s remains)
2017-12-10 12:06:46.414505: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035364714 0.033088166 0.029251054 0.025387021 0.024327282 0.030862985 0.042618349 0.053382933 0.065091819 0.0817884 0.10042249 0.1076254 0.099608772 0.082344592 0.057639796][0.06608545 0.061046626 0.053772006 0.049059186 0.051074274 0.064404592 0.084237777 0.10137074 0.11820945 0.13992874 0.1635174 0.17274739 0.16178422 0.13730747 0.10215343][0.090403184 0.08144667 0.070151292 0.065105222 0.071658336 0.093086824 0.12168361 0.14502545 0.16590947 0.19008455 0.21518949 0.22444861 0.21045098 0.17995954 0.13623761][0.09774863 0.086219169 0.073209718 0.069444813 0.080959387 0.1094768 0.14509431 0.17272806 0.19557235 0.2193726 0.24283394 0.25033641 0.23358881 0.19924644 0.14996164][0.0963406 0.085522085 0.074407317 0.073654726 0.089122407 0.12128188 0.159594 0.18792492 0.20963968 0.23026715 0.25004867 0.25573859 0.23786178 0.20131214 0.14866754][0.098345742 0.090952463 0.083406948 0.084770493 0.10071587 0.13169897 0.16827287 0.19418529 0.21181242 0.22641551 0.2403672 0.243716 0.22574063 0.18873945 0.13522421][0.10709114 0.10394911 0.099389978 0.10086092 0.11402973 0.14038627 0.17239952 0.19412868 0.2056511 0.21195523 0.21797188 0.21769398 0.20003235 0.16457725 0.11336174][0.12154624 0.12206637 0.11927731 0.11915841 0.12676147 0.14494024 0.16897292 0.18458687 0.18930408 0.18761115 0.18617216 0.18223785 0.16552877 0.13357721 0.087587908][0.15041983 0.15501843 0.15320989 0.14970453 0.14885129 0.15511458 0.16734803 0.17455502 0.17295621 0.16660631 0.16167709 0.15643619 0.14189751 0.11410198 0.07324744][0.19146594 0.19877265 0.19698687 0.18902689 0.17894028 0.17231286 0.17120433 0.16848178 0.16106312 0.15341215 0.14923741 0.14584137 0.13515061 0.11186682 0.074747108][0.2254834 0.23308064 0.23077495 0.21956855 0.20345473 0.18808283 0.17704539 0.16627048 0.15510227 0.14875455 0.1473635 0.14629188 0.13852946 0.11862142 0.08328905][0.23234716 0.23734602 0.23447447 0.2231469 0.20683955 0.18987358 0.17530157 0.16082628 0.14928809 0.14636955 0.14879727 0.14998993 0.14397131 0.12601355 0.090991326][0.20541838 0.20673169 0.20356578 0.19384992 0.1803813 0.16595674 0.15240839 0.13838343 0.12949851 0.13109724 0.13754588 0.14115086 0.13669895 0.12058693 0.086487085][0.15596293 0.15441813 0.15132184 0.143097 0.13225961 0.1207409 0.10960453 0.098181933 0.0933694 0.099323109 0.10912473 0.11468434 0.11122817 0.096633352 0.064912647][0.087999694 0.0845049 0.08201956 0.0755763 0.067465119 0.059009988 0.0509475 0.043069743 0.0422207 0.05094333 0.06204043 0.06767413 0.0637368 0.050169419 0.023131704]]...]
INFO - root - 2017-12-10 12:06:56.553141: step 13510, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.042 sec/batch; 92h:21m:23s remains)
INFO - root - 2017-12-10 12:07:06.697485: step 13520, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.012 sec/batch; 89h:41m:33s remains)
INFO - root - 2017-12-10 12:07:16.550365: step 13530, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.021 sec/batch; 90h:28m:49s remains)
INFO - root - 2017-12-10 12:07:26.714626: step 13540, loss = 0.72, batch loss = 0.66 (7.9 examples/sec; 1.019 sec/batch; 90h:15m:40s remains)
INFO - root - 2017-12-10 12:07:36.920688: step 13550, loss = 0.69, batch loss = 0.64 (7.8 examples/sec; 1.026 sec/batch; 90h:53m:56s remains)
INFO - root - 2017-12-10 12:07:46.976028: step 13560, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.989 sec/batch; 87h:38m:02s remains)
INFO - root - 2017-12-10 12:07:57.134845: step 13570, loss = 0.68, batch loss = 0.63 (7.9 examples/sec; 1.018 sec/batch; 90h:09m:27s remains)
INFO - root - 2017-12-10 12:08:07.203705: step 13580, loss = 0.68, batch loss = 0.63 (7.8 examples/sec; 1.026 sec/batch; 90h:53m:12s remains)
INFO - root - 2017-12-10 12:08:17.377529: step 13590, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.006 sec/batch; 89h:06m:23s remains)
INFO - root - 2017-12-10 12:08:27.617080: step 13600, loss = 0.70, batch loss = 0.65 (7.8 examples/sec; 1.024 sec/batch; 90h:42m:34s remains)
2017-12-10 12:08:28.619950: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40740582 0.44741651 0.44901133 0.42829213 0.3929109 0.34995273 0.31512421 0.31018752 0.34368163 0.40215972 0.46397054 0.51363027 0.534812 0.50542343 0.42635542][0.38220444 0.42404389 0.43360585 0.42395228 0.40407443 0.37766719 0.35508624 0.35349241 0.37879789 0.4199287 0.45966405 0.486076 0.48929518 0.45463094 0.38578749][0.35606879 0.40566069 0.4292542 0.43553364 0.43370607 0.42703858 0.42111668 0.42436153 0.43784466 0.45429417 0.46363789 0.45854792 0.43702641 0.3969942 0.34527373][0.35871711 0.4268333 0.47187033 0.49694088 0.510631 0.52026081 0.53009206 0.53887773 0.54098463 0.53221494 0.5098682 0.46931958 0.41804811 0.37078634 0.33695871][0.39038125 0.48753768 0.56123954 0.60876578 0.63691568 0.65857083 0.67929065 0.69109815 0.68320906 0.65324956 0.6031962 0.52869529 0.44672745 0.38934276 0.3677159][0.42507175 0.55239248 0.65554541 0.72751629 0.77182412 0.80394691 0.83045512 0.84128875 0.82455975 0.77900517 0.7078445 0.60634083 0.49921042 0.4303205 0.41148439][0.4293271 0.57254827 0.69326943 0.78400654 0.84516853 0.88962775 0.92120707 0.92992437 0.90669477 0.85107 0.76551646 0.64702249 0.52575272 0.44892567 0.42701936][0.39133361 0.526823 0.64364952 0.73862386 0.81104714 0.86693805 0.9038617 0.9125182 0.88714629 0.82721037 0.73461562 0.61114722 0.48946273 0.4118768 0.38688833][0.32854372 0.43587807 0.52793872 0.60876518 0.67957497 0.73983294 0.78003496 0.79071528 0.76740623 0.7087326 0.61679363 0.49968308 0.38990027 0.32050467 0.29759052][0.26990432 0.33843189 0.39230093 0.44398409 0.49945408 0.55554432 0.59757924 0.61344856 0.59607506 0.54302579 0.4577572 0.3545602 0.26439929 0.21044411 0.19631881][0.24450596 0.27792358 0.29285577 0.30900937 0.33813375 0.37962702 0.41810071 0.43794668 0.42662564 0.38063303 0.30590156 0.22041652 0.15224455 0.11666789 0.11545519][0.266183 0.28134573 0.27011341 0.25676382 0.2575351 0.2769286 0.30311474 0.31911254 0.30755916 0.26443326 0.19866313 0.1293035 0.07996577 0.060516458 0.071844056][0.32117113 0.33888569 0.32249215 0.29775739 0.28180197 0.28152844 0.28980803 0.29198015 0.26951075 0.21912389 0.15340945 0.092381783 0.054340165 0.044809617 0.064162277][0.37861288 0.41292331 0.40899786 0.39172798 0.375983 0.36602566 0.35734725 0.34048721 0.30015382 0.23502336 0.16023619 0.098233476 0.0633096 0.056879856 0.078469381][0.41298473 0.46745637 0.48314363 0.48249459 0.47754517 0.46633139 0.44477302 0.40982124 0.35286689 0.27441585 0.18921791 0.12213324 0.086367704 0.07935071 0.099157967]]...]
INFO - root - 2017-12-10 12:08:38.572366: step 13610, loss = 0.71, batch loss = 0.66 (8.0 examples/sec; 1.001 sec/batch; 88h:39m:09s remains)
INFO - root - 2017-12-10 12:08:48.570556: step 13620, loss = 0.71, batch loss = 0.66 (7.7 examples/sec; 1.034 sec/batch; 91h:34m:35s remains)
INFO - root - 2017-12-10 12:08:58.846946: step 13630, loss = 0.69, batch loss = 0.63 (7.6 examples/sec; 1.049 sec/batch; 92h:55m:49s remains)
INFO - root - 2017-12-10 12:09:08.940459: step 13640, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 1.000 sec/batch; 88h:35m:39s remains)
INFO - root - 2017-12-10 12:09:19.106928: step 13650, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.027 sec/batch; 90h:58m:29s remains)
INFO - root - 2017-12-10 12:09:29.286418: step 13660, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.024 sec/batch; 90h:42m:23s remains)
INFO - root - 2017-12-10 12:09:39.545136: step 13670, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.018 sec/batch; 90h:11m:40s remains)
INFO - root - 2017-12-10 12:09:49.676443: step 13680, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.021 sec/batch; 90h:23m:39s remains)
INFO - root - 2017-12-10 12:09:59.572425: step 13690, loss = 0.68, batch loss = 0.62 (7.9 examples/sec; 1.013 sec/batch; 89h:44m:20s remains)
INFO - root - 2017-12-10 12:10:09.741011: step 13700, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.017 sec/batch; 90h:03m:16s remains)
2017-12-10 12:10:10.688819: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14764564 0.18105301 0.20561121 0.21649978 0.21372242 0.198807 0.17415769 0.14383121 0.12124586 0.11042098 0.10329941 0.095893539 0.0893255 0.096917927 0.11772397][0.1863188 0.22435972 0.24698427 0.25138432 0.24177817 0.22200921 0.19580218 0.16620414 0.14588621 0.13705897 0.13159628 0.12695509 0.12785771 0.15007222 0.18754253][0.23962183 0.27611992 0.28962073 0.28185561 0.2623105 0.23658913 0.20892175 0.18114249 0.16330139 0.15598483 0.15218742 0.15269828 0.16517814 0.20417213 0.25691023][0.29630682 0.32471162 0.32353926 0.30085042 0.27145812 0.24160255 0.21461937 0.19064049 0.17549695 0.16861258 0.16554667 0.17121659 0.1947967 0.24575199 0.30495167][0.33341658 0.3496123 0.33256483 0.29719347 0.26302791 0.23560184 0.21606977 0.20196849 0.19326206 0.18868786 0.18684188 0.19610994 0.22495455 0.27676007 0.32973579][0.33382526 0.33913746 0.31174731 0.27220553 0.24285628 0.2269499 0.22280417 0.22465095 0.22600855 0.22558171 0.2247335 0.23408195 0.25991225 0.30068144 0.33734906][0.30316249 0.30307874 0.27370942 0.23890851 0.22198734 0.22306217 0.23758075 0.25594145 0.26559243 0.26698393 0.26484382 0.27179331 0.29132155 0.318391 0.33991244][0.26032761 0.26089606 0.23561573 0.20909111 0.20441511 0.21938396 0.24730362 0.27503371 0.28589025 0.28399372 0.27830383 0.28388679 0.30141151 0.3225829 0.33955365][0.22048277 0.22502293 0.20556292 0.18508054 0.18595655 0.20545107 0.23612078 0.26244354 0.2668553 0.25788295 0.24828014 0.25540376 0.27804506 0.30435494 0.32790762][0.18932573 0.19642097 0.18042953 0.16122566 0.16001859 0.17503102 0.1993552 0.21703108 0.21257463 0.19725846 0.18516193 0.19499503 0.22535168 0.2619814 0.2962603][0.1673318 0.17266771 0.15657112 0.13535634 0.12801243 0.13423951 0.148774 0.15749043 0.14832482 0.13211873 0.12116025 0.1337169 0.16923815 0.212438 0.25166693][0.15320835 0.15419358 0.13598031 0.11267347 0.10008495 0.099055134 0.10623936 0.1104923 0.10355286 0.092722312 0.086160436 0.099858671 0.13458538 0.17552951 0.20937474][0.14390729 0.14035551 0.12065313 0.0986221 0.0858439 0.083205529 0.088522419 0.09391173 0.0946546 0.09286461 0.091903791 0.10390209 0.13089725 0.15973532 0.17848812][0.13600753 0.12912141 0.11076548 0.095621906 0.08970803 0.09242066 0.10155915 0.11206006 0.12184598 0.12851068 0.13178529 0.13973436 0.15496977 0.16658564 0.16664673][0.13034409 0.12203801 0.108741 0.10519927 0.11117623 0.12362893 0.13965939 0.15572162 0.17146061 0.18268616 0.18752387 0.19089624 0.19521695 0.19188154 0.17747702]]...]
INFO - root - 2017-12-10 12:10:20.768179: step 13710, loss = 0.68, batch loss = 0.62 (7.9 examples/sec; 1.017 sec/batch; 90h:04m:23s remains)
INFO - root - 2017-12-10 12:10:30.906101: step 13720, loss = 0.69, batch loss = 0.64 (7.8 examples/sec; 1.025 sec/batch; 90h:47m:51s remains)
INFO - root - 2017-12-10 12:10:40.979062: step 13730, loss = 0.70, batch loss = 0.65 (8.0 examples/sec; 1.003 sec/batch; 88h:48m:51s remains)
INFO - root - 2017-12-10 12:10:50.991011: step 13740, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.015 sec/batch; 89h:51m:15s remains)
INFO - root - 2017-12-10 12:11:01.263623: step 13750, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.034 sec/batch; 91h:35m:38s remains)
INFO - root - 2017-12-10 12:11:11.437868: step 13760, loss = 0.68, batch loss = 0.63 (8.1 examples/sec; 0.992 sec/batch; 87h:48m:03s remains)
INFO - root - 2017-12-10 12:11:21.294814: step 13770, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.009 sec/batch; 89h:17m:54s remains)
INFO - root - 2017-12-10 12:11:31.400688: step 13780, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.025 sec/batch; 90h:42m:39s remains)
INFO - root - 2017-12-10 12:11:41.536055: step 13790, loss = 0.69, batch loss = 0.64 (7.9 examples/sec; 1.014 sec/batch; 89h:44m:02s remains)
INFO - root - 2017-12-10 12:11:51.696357: step 13800, loss = 0.71, batch loss = 0.65 (8.2 examples/sec; 0.974 sec/batch; 86h:11m:10s remains)
2017-12-10 12:11:52.713859: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035502359 0.056051496 0.065649495 0.06643644 0.067626469 0.072151139 0.072790392 0.067740254 0.0573862 0.045625683 0.036712617 0.035971027 0.043343127 0.043295268 0.026217274][0.093347579 0.1282855 0.14455593 0.14971747 0.15532227 0.16320252 0.16272944 0.15050125 0.12557143 0.095524177 0.0703043 0.05992499 0.065459616 0.06611228 0.048420567][0.15483759 0.20538978 0.22907247 0.23948027 0.24993928 0.26067486 0.2591669 0.24069914 0.20160082 0.14988053 0.10179331 0.07484781 0.073974065 0.075868733 0.063316278][0.19303486 0.25762746 0.29127455 0.31088886 0.32886657 0.3423315 0.33873782 0.31483832 0.26471034 0.19370462 0.12136029 0.0742457 0.064283177 0.069197059 0.066928431][0.21503673 0.2960842 0.34674689 0.38119185 0.40779802 0.42122698 0.41269583 0.38207698 0.321773 0.23423392 0.14057091 0.074337475 0.055104814 0.062636986 0.07055562][0.23071691 0.33202168 0.40741232 0.46238869 0.49940559 0.51101977 0.49451673 0.453625 0.38096029 0.277381 0.16514583 0.082891911 0.055728372 0.065023594 0.079741046][0.22799739 0.34464064 0.44374973 0.51990747 0.56988478 0.58335108 0.56236207 0.51406717 0.43628508 0.32761171 0.20959753 0.12076147 0.088821433 0.096260011 0.10823964][0.19944917 0.31900349 0.43216214 0.52582645 0.59169662 0.6140179 0.594908 0.546959 0.47854808 0.38578638 0.283375 0.20294748 0.17074005 0.1688742 0.16237693][0.16030747 0.27262425 0.38966179 0.49527097 0.57517874 0.60578591 0.58689749 0.53966945 0.48616293 0.42287838 0.35436785 0.29976085 0.27666116 0.26225841 0.22536214][0.13514648 0.23924851 0.35466561 0.464519 0.54776311 0.57594109 0.54845351 0.49650529 0.45411977 0.42236531 0.39534485 0.376547 0.3688246 0.34080189 0.26941749][0.1130202 0.20520057 0.31107506 0.41366327 0.48662913 0.50077075 0.45968679 0.40178469 0.36762282 0.36372206 0.37674978 0.39617285 0.407272 0.37048876 0.27334419][0.074189216 0.15024562 0.24115402 0.32843605 0.38341591 0.38037598 0.32957375 0.27281716 0.25208572 0.27637571 0.32582664 0.3780303 0.40335035 0.35984534 0.24742289][0.027955242 0.08771988 0.16187818 0.23014347 0.26489976 0.24689317 0.19231367 0.14196387 0.13551877 0.18145271 0.25469896 0.32404268 0.35182443 0.30172092 0.18633345][-0.010016331 0.034231242 0.090029068 0.13659446 0.15086964 0.12123116 0.0666686 0.02303328 0.024123358 0.075907983 0.15194689 0.21813808 0.2384737 0.18852994 0.088819787][-0.031483129 -0.00049527362 0.037188128 0.0625241 0.059404965 0.023623433 -0.026362253 -0.062491946 -0.060492221 -0.017832035 0.043125339 0.093170591 0.10494693 0.065494649 -0.0049441904]]...]
INFO - root - 2017-12-10 12:12:02.866380: step 13810, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.021 sec/batch; 90h:22m:20s remains)
INFO - root - 2017-12-10 12:12:13.136365: step 13820, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.016 sec/batch; 89h:54m:00s remains)
INFO - root - 2017-12-10 12:12:23.214836: step 13830, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.017 sec/batch; 90h:03m:08s remains)
INFO - root - 2017-12-10 12:12:33.411104: step 13840, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.002 sec/batch; 88h:44m:02s remains)
INFO - root - 2017-12-10 12:12:43.423488: step 13850, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.007 sec/batch; 89h:09m:02s remains)
INFO - root - 2017-12-10 12:12:53.589914: step 13860, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.020 sec/batch; 90h:18m:39s remains)
INFO - root - 2017-12-10 12:13:03.770930: step 13870, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.022 sec/batch; 90h:26m:03s remains)
INFO - root - 2017-12-10 12:13:13.876967: step 13880, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.026 sec/batch; 90h:50m:18s remains)
INFO - root - 2017-12-10 12:13:23.957176: step 13890, loss = 0.72, batch loss = 0.67 (7.5 examples/sec; 1.061 sec/batch; 93h:52m:49s remains)
INFO - root - 2017-12-10 12:13:34.095247: step 13900, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.017 sec/batch; 89h:58m:55s remains)
2017-12-10 12:13:35.039136: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11350378 0.11687279 0.10233242 0.073146187 0.037615571 0.0058497279 -0.014558829 -0.022212613 -0.021327505 -0.015335707 -0.0084207058 -0.0046575926 -0.0061559384 -0.011326474 -0.015133142][0.12762745 0.13197757 0.11826773 0.089803077 0.055761568 0.026028592 0.0071629537 -0.00043229869 -0.00080025679 0.0023458616 0.0052507003 0.0054190257 0.00012108517 -0.008792 -0.014725666][0.14349529 0.15230179 0.14374873 0.12026417 0.091127776 0.065557458 0.047674142 0.036332536 0.029503873 0.025419492 0.021880629 0.017356945 0.0084487842 -0.0034677966 -0.011449143][0.15789048 0.17569283 0.17735083 0.16368186 0.14347884 0.12534863 0.10963303 0.092303015 0.074306063 0.058074556 0.044375051 0.032036051 0.017736867 0.003000828 -0.0065930942][0.16488276 0.19399564 0.20951991 0.2107536 0.20548195 0.20144303 0.19321671 0.17198625 0.14124058 0.1089571 0.080264837 0.054496083 0.030117005 0.01065545 0.00029824069][0.1584105 0.19646642 0.22541276 0.24406278 0.25855929 0.27502456 0.28135115 0.26253721 0.22300972 0.17566466 0.13003929 0.08659251 0.047487207 0.02026695 0.0092074247][0.13784647 0.17798701 0.21469024 0.24759305 0.28080615 0.31852615 0.34341848 0.33432493 0.29439467 0.23884028 0.17998265 0.11974476 0.065083161 0.028848626 0.016836053][0.11840299 0.15330374 0.18899058 0.22673008 0.26920041 0.32011306 0.36072597 0.36529025 0.33338127 0.27912918 0.21514228 0.14493424 0.079763994 0.037129693 0.024908876][0.11954544 0.14450544 0.17067996 0.20095195 0.238153 0.28794965 0.33465463 0.35057655 0.33028647 0.28472328 0.22492951 0.15571821 0.090286583 0.048033517 0.038066741][0.14709772 0.16247042 0.17509958 0.18862891 0.20795357 0.2428007 0.28366661 0.3035301 0.2922439 0.25765869 0.20895714 0.15125294 0.096447125 0.062329505 0.057456117][0.19197834 0.20276816 0.20358269 0.19762035 0.19307385 0.20497189 0.23112957 0.24746622 0.2406588 0.21639948 0.18207797 0.14190879 0.10419933 0.082723439 0.084211066][0.22604603 0.2368011 0.23033606 0.2088491 0.18351628 0.17359112 0.18316464 0.19257657 0.18734123 0.17165275 0.15096009 0.12795509 0.10707428 0.097525977 0.10390972][0.22036205 0.23198152 0.22274369 0.19386029 0.15768078 0.13446391 0.13185287 0.13507143 0.13062993 0.12160901 0.11151285 0.10148527 0.093039706 0.091495827 0.0995611][0.1649809 0.17482969 0.16524717 0.1370706 0.10132865 0.075583562 0.067864336 0.067949094 0.064945668 0.060925897 0.057759259 0.055438541 0.053903956 0.055741787 0.062365036][0.078451514 0.083730444 0.075135581 0.053138681 0.025724687 0.0053125182 -0.0019783704 -0.0025838129 -0.0039050637 -0.0047567314 -0.0043900339 -0.0033551361 -0.0022963781 -0.00024496269 0.0035103781]]...]
INFO - root - 2017-12-10 12:13:45.235849: step 13910, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.991 sec/batch; 87h:39m:44s remains)
INFO - root - 2017-12-10 12:13:55.331877: step 13920, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.042 sec/batch; 92h:13m:17s remains)
INFO - root - 2017-12-10 12:14:05.377916: step 13930, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.008 sec/batch; 89h:09m:39s remains)
INFO - root - 2017-12-10 12:14:15.433426: step 13940, loss = 0.69, batch loss = 0.64 (7.8 examples/sec; 1.023 sec/batch; 90h:31m:28s remains)
INFO - root - 2017-12-10 12:14:25.665263: step 13950, loss = 0.70, batch loss = 0.64 (7.6 examples/sec; 1.055 sec/batch; 93h:19m:23s remains)
INFO - root - 2017-12-10 12:14:35.836235: step 13960, loss = 0.72, batch loss = 0.66 (7.9 examples/sec; 1.016 sec/batch; 89h:54m:39s remains)
INFO - root - 2017-12-10 12:14:45.903615: step 13970, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.017 sec/batch; 89h:58m:33s remains)
INFO - root - 2017-12-10 12:14:55.878031: step 13980, loss = 0.70, batch loss = 0.65 (7.8 examples/sec; 1.022 sec/batch; 90h:27m:30s remains)
INFO - root - 2017-12-10 12:15:05.975046: step 13990, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.987 sec/batch; 87h:20m:43s remains)
INFO - root - 2017-12-10 12:15:16.058537: step 14000, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.011 sec/batch; 89h:26m:52s remains)
2017-12-10 12:15:16.925993: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18082997 0.23994114 0.26960865 0.2716665 0.26071575 0.24183694 0.22204335 0.22237849 0.2496428 0.28799948 0.31185511 0.31226274 0.29175383 0.25138983 0.20879285][0.1858028 0.25389028 0.29350939 0.302446 0.2937862 0.27257821 0.24606238 0.23806459 0.26016364 0.30111629 0.33261332 0.33908325 0.3198244 0.27587378 0.22458918][0.1807539 0.25113043 0.29354239 0.30386508 0.2939167 0.26904532 0.23644556 0.21955672 0.23372944 0.27304298 0.30954513 0.32263076 0.30864283 0.26841322 0.21704555][0.18304265 0.25466853 0.29782069 0.30816555 0.29666597 0.26986137 0.23502646 0.21340644 0.22180088 0.25765938 0.2946659 0.30914712 0.29593801 0.25833949 0.21087314][0.21251607 0.29210895 0.34159842 0.35631144 0.34689388 0.3207604 0.28522444 0.25934264 0.26006553 0.2878367 0.31752717 0.32551971 0.30738139 0.27203974 0.23436363][0.24473266 0.33285257 0.38974825 0.41254967 0.41201651 0.39390567 0.36364928 0.336172 0.3287 0.34367141 0.35841268 0.35345677 0.32818049 0.29774657 0.27474791][0.24350853 0.3314122 0.39071584 0.42325434 0.43935031 0.43955624 0.42326674 0.39920202 0.38541165 0.38534957 0.38195398 0.36346889 0.33498433 0.3134208 0.30557489][0.19971558 0.27704802 0.33180651 0.37078834 0.40270954 0.42198086 0.420837 0.40239024 0.38513514 0.37243995 0.35423347 0.32743934 0.30118719 0.28898567 0.29055527][0.13022994 0.18964353 0.23403837 0.27212268 0.31009632 0.33797419 0.34453878 0.33097726 0.3142668 0.29571962 0.26954231 0.23960468 0.21633327 0.20860767 0.21144432][0.05689067 0.096522421 0.12695958 0.15542927 0.18620911 0.20886859 0.21350478 0.20111084 0.18536651 0.16515149 0.1366045 0.10746183 0.087714352 0.083203368 0.087042712][-0.018232679 -0.00010006333 0.013508262 0.027544012 0.045083888 0.057578675 0.058311235 0.048237145 0.03723456 0.021672821 -0.0016067961 -0.024529431 -0.038654339 -0.039980266 -0.03491674][-0.075033128 -0.074919164 -0.074913137 -0.071612656 -0.06321083 -0.056373913 -0.055675961 -0.060792398 -0.065302856 -0.073659718 -0.088070072 -0.10205769 -0.10971649 -0.10887105 -0.1042663][-0.094639555 -0.10234063 -0.10715818 -0.10780843 -0.10369804 -0.099615455 -0.098479778 -0.10029268 -0.10125362 -0.1048444 -0.11235427 -0.11925571 -0.12240779 -0.1212379 -0.11871646][-0.091611035 -0.10017031 -0.10480353 -0.10641124 -0.10501606 -0.10349521 -0.10307843 -0.10391615 -0.10430215 -0.10635649 -0.1105011 -0.11380257 -0.1151034 -0.11455221 -0.11364939][-0.08515361 -0.092938833 -0.0967924 -0.09913785 -0.099972 -0.100581 -0.10114989 -0.10197417 -0.10255156 -0.10385126 -0.1058495 -0.10705901 -0.10730904 -0.10672601 -0.10591426]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 12:15:27.084943: step 14010, loss = 0.68, batch loss = 0.62 (7.8 examples/sec; 1.022 sec/batch; 90h:24m:30s remains)
INFO - root - 2017-12-10 12:15:37.177961: step 14020, loss = 0.70, batch loss = 0.65 (7.8 examples/sec; 1.023 sec/batch; 90h:28m:42s remains)
INFO - root - 2017-12-10 12:15:47.352209: step 14030, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.012 sec/batch; 89h:34m:00s remains)
INFO - root - 2017-12-10 12:15:57.507471: step 14040, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.024 sec/batch; 90h:32m:40s remains)
INFO - root - 2017-12-10 12:16:07.550580: step 14050, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.014 sec/batch; 89h:40m:51s remains)
INFO - root - 2017-12-10 12:16:17.513437: step 14060, loss = 0.71, batch loss = 0.65 (8.7 examples/sec; 0.921 sec/batch; 81h:25m:47s remains)
INFO - root - 2017-12-10 12:16:27.691225: step 14070, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.026 sec/batch; 90h:43m:40s remains)
INFO - root - 2017-12-10 12:16:37.727793: step 14080, loss = 0.68, batch loss = 0.62 (8.4 examples/sec; 0.951 sec/batch; 84h:09m:14s remains)
INFO - root - 2017-12-10 12:16:47.770043: step 14090, loss = 0.70, batch loss = 0.65 (7.8 examples/sec; 1.031 sec/batch; 91h:12m:19s remains)
INFO - root - 2017-12-10 12:16:57.894274: step 14100, loss = 0.69, batch loss = 0.64 (7.7 examples/sec; 1.036 sec/batch; 91h:39m:17s remains)
2017-12-10 12:16:58.826335: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.071483836 0.070112258 0.063466057 0.055132244 0.046841122 0.039209895 0.033761751 0.032999955 0.035275392 0.037773486 0.039119776 0.040030625 0.042172521 0.044226907 0.046520326][0.068419367 0.066799857 0.060534731 0.05338126 0.04717857 0.041754771 0.0373784 0.035936181 0.036402471 0.036798198 0.03667808 0.036597244 0.038017746 0.039581187 0.041301776][0.060617168 0.05937713 0.054364081 0.049194861 0.04571525 0.043346062 0.041244619 0.04027411 0.039686326 0.03844652 0.036725946 0.035116222 0.034819204 0.0348764 0.035180673][0.048798464 0.048447803 0.045330334 0.042545151 0.042008903 0.043193698 0.044400506 0.045166451 0.044653155 0.042477429 0.039103288 0.035298765 0.032326985 0.02997992 0.028420109][0.036489289 0.037077706 0.035881806 0.035310715 0.03739677 0.041931812 0.046576519 0.049498696 0.049629543 0.047169462 0.042634897 0.036751229 0.030909942 0.02593028 0.022645883][0.027715037 0.028830016 0.028908387 0.030049395 0.034380067 0.041949682 0.049978655 0.055257909 0.056355853 0.054098096 0.048935492 0.041137479 0.032199156 0.024230134 0.019240839][0.023619542 0.024890278 0.025642905 0.028278749 0.034971748 0.045767315 0.057308551 0.065058477 0.067104742 0.064828858 0.05871449 0.048583534 0.036047377 0.024661763 0.017871682][0.022526169 0.023465734 0.024525819 0.028646722 0.038020808 0.052393854 0.067554735 0.077841327 0.08068262 0.077943534 0.070543677 0.058079727 0.042327024 0.027848722 0.019578889][0.022701653 0.023046656 0.024125678 0.029406423 0.041102562 0.058441121 0.076405555 0.088533349 0.091916479 0.088814244 0.080886707 0.067395121 0.05002711 0.033950623 0.024917191][0.022792688 0.022559209 0.023620158 0.029526567 0.0423545 0.060772739 0.079369895 0.091574684 0.094726183 0.091629773 0.0845555 0.072406024 0.056271005 0.041092284 0.03255593][0.022435611 0.021795616 0.022678297 0.028283637 0.040415339 0.05734295 0.073948123 0.084400974 0.086752214 0.084116153 0.079089493 0.070359625 0.058018018 0.045854971 0.038648006][0.024657682 0.023571618 0.023419442 0.02709111 0.036180839 0.048982784 0.061421871 0.069098689 0.07080625 0.069326542 0.067093819 0.062717333 0.055238128 0.046861559 0.041092061][0.030649809 0.029034603 0.02693527 0.027346855 0.0319075 0.039293781 0.046871338 0.051880162 0.053625759 0.053889133 0.054339 0.053678229 0.050308563 0.044959422 0.040001307][0.038500909 0.036228426 0.031880211 0.028954998 0.029306967 0.032011591 0.035583232 0.03865771 0.040759839 0.042371377 0.04418759 0.045323528 0.044280313 0.04076644 0.036245294][0.044662 0.042066056 0.036386572 0.03164741 0.029878395 0.030329194 0.031924855 0.034101594 0.03630352 0.037927728 0.039083656 0.039750438 0.039262429 0.036796726 0.032940339]]...]
INFO - root - 2017-12-10 12:17:08.869557: step 14110, loss = 0.69, batch loss = 0.64 (7.9 examples/sec; 1.013 sec/batch; 89h:35m:56s remains)
INFO - root - 2017-12-10 12:17:18.980946: step 14120, loss = 0.69, batch loss = 0.64 (8.1 examples/sec; 0.990 sec/batch; 87h:32m:30s remains)
INFO - root - 2017-12-10 12:17:29.126868: step 14130, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.027 sec/batch; 90h:51m:43s remains)
INFO - root - 2017-12-10 12:17:39.258994: step 14140, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.024 sec/batch; 90h:35m:03s remains)
INFO - root - 2017-12-10 12:17:49.319909: step 14150, loss = 0.69, batch loss = 0.64 (7.8 examples/sec; 1.020 sec/batch; 90h:09m:46s remains)
INFO - root - 2017-12-10 12:17:59.326152: step 14160, loss = 0.72, batch loss = 0.66 (9.4 examples/sec; 0.847 sec/batch; 74h:56m:22s remains)
INFO - root - 2017-12-10 12:18:09.444393: step 14170, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.012 sec/batch; 89h:27m:52s remains)
INFO - root - 2017-12-10 12:18:19.584673: step 14180, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.039 sec/batch; 91h:53m:06s remains)
INFO - root - 2017-12-10 12:18:29.785833: step 14190, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.033 sec/batch; 91h:20m:40s remains)
INFO - root - 2017-12-10 12:18:39.830384: step 14200, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.008 sec/batch; 89h:08m:50s remains)
2017-12-10 12:18:40.854188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.011368943 0.0086196214 0.031361397 0.050804742 0.059234407 0.054391809 0.041221857 0.025058247 0.0098648025 -0.00094860251 -0.003387402 0.001541606 0.00830158 0.013778189 0.016261071][0.0039278059 0.0346571 0.070633411 0.10246847 0.11897104 0.1159625 0.09915863 0.074318647 0.046477504 0.022399295 0.010036835 0.010126075 0.015126117 0.020085111 0.02239844][0.019184442 0.060918115 0.11223017 0.15988584 0.1889194 0.19250615 0.17603818 0.14445591 0.10269023 0.061116748 0.032496434 0.020913545 0.018075382 0.018439114 0.018628815][0.029757291 0.080569811 0.14611699 0.21055511 0.25568348 0.27145413 0.2606186 0.22628935 0.17249991 0.11317397 0.065392934 0.037126381 0.021367643 0.013415864 0.0098847812][0.035412122 0.093913265 0.17195013 0.25229657 0.31513691 0.347593 0.34705734 0.31296644 0.2489312 0.1736511 0.10784028 0.06208203 0.031498361 0.01347345 0.0057424395][0.038934663 0.10487314 0.19422761 0.28837457 0.36751065 0.41759604 0.42990169 0.39826807 0.32633159 0.23833986 0.15869536 0.0989894 0.056362361 0.030843774 0.021064835][0.041310489 0.11351357 0.21198811 0.31604737 0.40648592 0.47039041 0.49540806 0.46986639 0.39656255 0.30393049 0.21930727 0.15360819 0.10573845 0.079182044 0.072618261][0.041077014 0.11615203 0.21926968 0.32850984 0.42493325 0.49684533 0.53160304 0.51521522 0.44928819 0.36253244 0.28261682 0.21978438 0.17510885 0.15517539 0.1579463][0.038435221 0.11210708 0.21437752 0.32429519 0.42298585 0.49862775 0.53923941 0.53164554 0.47781906 0.40452334 0.33659059 0.283236 0.24843116 0.241306 0.25810108][0.033824891 0.10170949 0.19704558 0.30189902 0.39844012 0.47419843 0.51829237 0.51911294 0.47959384 0.42401785 0.37286359 0.33357489 0.31123805 0.31649068 0.34516847][0.026301285 0.085089266 0.16835667 0.2621271 0.35101235 0.42286521 0.46937346 0.47988883 0.45710975 0.421744 0.38976339 0.36683962 0.35636219 0.36850137 0.40083387][0.016834505 0.064861283 0.13313365 0.21152186 0.28741333 0.35023543 0.39601031 0.41606632 0.411234 0.39618331 0.38196635 0.37406412 0.37328729 0.38705215 0.41557854][0.0062786257 0.043673221 0.096769847 0.15820397 0.21755442 0.26657304 0.30625793 0.33123893 0.34044051 0.34136158 0.33997634 0.34196436 0.34650815 0.35770491 0.37802151][-0.0052898028 0.022875948 0.062484764 0.10767358 0.14958082 0.18208513 0.21026103 0.23228425 0.24692851 0.25656733 0.26288441 0.27043769 0.27684918 0.28346425 0.29406056][-0.019716062 -0.000490345 0.026880076 0.05714383 0.082849272 0.099586412 0.11414705 0.12734139 0.13917789 0.15018015 0.15999076 0.17069088 0.17754591 0.17988294 0.18158334]]...]
INFO - root - 2017-12-10 12:18:50.984733: step 14210, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.006 sec/batch; 88h:57m:04s remains)
INFO - root - 2017-12-10 12:19:01.135708: step 14220, loss = 0.68, batch loss = 0.62 (8.0 examples/sec; 0.999 sec/batch; 88h:18m:45s remains)
INFO - root - 2017-12-10 12:19:11.195888: step 14230, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.039 sec/batch; 91h:49m:39s remains)
INFO - root - 2017-12-10 12:19:20.905230: step 14240, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.028 sec/batch; 90h:53m:20s remains)
INFO - root - 2017-12-10 12:19:31.101718: step 14250, loss = 0.68, batch loss = 0.63 (7.8 examples/sec; 1.029 sec/batch; 90h:59m:13s remains)
INFO - root - 2017-12-10 12:19:41.302170: step 14260, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.007 sec/batch; 88h:59m:41s remains)
INFO - root - 2017-12-10 12:19:51.443275: step 14270, loss = 0.68, batch loss = 0.62 (8.1 examples/sec; 0.986 sec/batch; 87h:10m:11s remains)
INFO - root - 2017-12-10 12:20:01.777488: step 14280, loss = 0.68, batch loss = 0.63 (8.3 examples/sec; 0.959 sec/batch; 84h:45m:09s remains)
INFO - root - 2017-12-10 12:20:11.851990: step 14290, loss = 0.71, batch loss = 0.65 (8.1 examples/sec; 0.991 sec/batch; 87h:35m:39s remains)
INFO - root - 2017-12-10 12:20:21.965481: step 14300, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.032 sec/batch; 91h:13m:40s remains)
2017-12-10 12:20:23.018122: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.006694016 0.033572413 0.054408584 0.069265738 0.071836725 0.053684946 0.02695829 0.011318326 0.018224869 0.052900936 0.11454202 0.19232541 0.26305923 0.31569743 0.3445693][0.0034485781 0.03399457 0.066679329 0.098862477 0.11772367 0.10986832 0.088388093 0.074916355 0.079884328 0.11162496 0.17302975 0.25292882 0.32373372 0.37274337 0.393583][0.0022987977 0.040036693 0.091631256 0.14898285 0.19067448 0.19853379 0.18648709 0.17679322 0.17578767 0.19377519 0.23840418 0.3024151 0.35787454 0.39158636 0.39764941][0.002499962 0.049769957 0.12242687 0.20559077 0.26921535 0.29250875 0.29045826 0.28495085 0.27555206 0.2731342 0.29056856 0.32695216 0.35702804 0.36835155 0.35641322][0.0020087282 0.056315638 0.14392331 0.24492688 0.32462022 0.36265066 0.37368628 0.37592313 0.3607569 0.33872867 0.32734877 0.33214077 0.33227593 0.31638303 0.28355789][-0.0011924896 0.054934457 0.14850739 0.25853682 0.35089773 0.40717879 0.43906215 0.45635518 0.44160023 0.40410885 0.36556306 0.33740017 0.30455545 0.25750372 0.20197062][-0.0066567613 0.046788935 0.1384545 0.24984601 0.35151574 0.42672628 0.48113009 0.51442611 0.50229329 0.45340821 0.3931725 0.33746028 0.27730706 0.20594816 0.13647541][-0.013548371 0.033800546 0.11754203 0.223326 0.32779548 0.41474354 0.48314932 0.524902 0.51331139 0.4570207 0.38471198 0.3146365 0.24323334 0.1662696 0.10070994][-0.020891039 0.017980553 0.088743962 0.18168494 0.28011933 0.36840492 0.44029352 0.48292041 0.47183257 0.41574281 0.34376454 0.27339906 0.20611168 0.14079602 0.0943548][-0.026101068 0.0045243916 0.061473925 0.13845022 0.22558933 0.30839387 0.37788463 0.41894773 0.4112719 0.36271319 0.2993013 0.2363358 0.17856431 0.12865627 0.10356901][-0.025431002 0.0028168489 0.053821068 0.12203998 0.20220424 0.2802977 0.34611028 0.38413155 0.37723574 0.332666 0.27356637 0.21399036 0.1607376 0.12108689 0.11318313][-0.018523026 0.014130559 0.069183834 0.13937339 0.22002287 0.29563838 0.35555232 0.38567451 0.37146205 0.32137898 0.25846907 0.19732775 0.1455521 0.11377127 0.1209871][-0.010879166 0.02824655 0.092045575 0.170903 0.25792432 0.33435312 0.38899636 0.40976813 0.38481107 0.3252075 0.2557672 0.19262059 0.14264797 0.1173514 0.13504985][-0.0049782945 0.039923239 0.1118428 0.19967344 0.2933515 0.3704797 0.42031521 0.43395966 0.40248746 0.3386479 0.26785916 0.20709303 0.16034684 0.13780658 0.156871][-0.0015370789 0.04634716 0.12141583 0.2122331 0.30652362 0.38011947 0.42438889 0.43423304 0.40236929 0.34081081 0.27393365 0.21907724 0.17585292 0.15179758 0.16365434]]...]
INFO - root - 2017-12-10 12:20:33.175822: step 14310, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.019 sec/batch; 90h:02m:06s remains)
INFO - root - 2017-12-10 12:20:43.207546: step 14320, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.025 sec/batch; 90h:37m:54s remains)
INFO - root - 2017-12-10 12:20:53.169355: step 14330, loss = 0.68, batch loss = 0.62 (7.8 examples/sec; 1.024 sec/batch; 90h:29m:51s remains)
INFO - root - 2017-12-10 12:21:03.389080: step 14340, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.004 sec/batch; 88h:46m:13s remains)
INFO - root - 2017-12-10 12:21:13.513138: step 14350, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.025 sec/batch; 90h:35m:45s remains)
INFO - root - 2017-12-10 12:21:23.672915: step 14360, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.028 sec/batch; 90h:52m:14s remains)
INFO - root - 2017-12-10 12:21:33.785409: step 14370, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.982 sec/batch; 86h:48m:52s remains)
INFO - root - 2017-12-10 12:21:43.906765: step 14380, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.014 sec/batch; 89h:37m:22s remains)
INFO - root - 2017-12-10 12:21:54.069931: step 14390, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 0.998 sec/batch; 88h:08m:50s remains)
INFO - root - 2017-12-10 12:22:04.028432: step 14400, loss = 0.69, batch loss = 0.64 (7.8 examples/sec; 1.022 sec/batch; 90h:17m:19s remains)
2017-12-10 12:22:05.075521: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.067208365 0.10411824 0.13099699 0.14164734 0.13244049 0.10723153 0.076267213 0.050158579 0.03543793 0.032473926 0.036799692 0.043849546 0.054533526 0.069881484 0.088106982][0.076637454 0.12126958 0.15796469 0.18090659 0.1847342 0.16919321 0.14194685 0.11393321 0.095035531 0.088038154 0.0909332 0.099716738 0.11506881 0.13681094 0.16137831][0.086854666 0.14153714 0.19120048 0.23015608 0.25172776 0.25082779 0.23132429 0.20406911 0.18163052 0.16897105 0.16713353 0.17431517 0.19105878 0.21656871 0.24558508][0.10014866 0.1656646 0.22883259 0.282972 0.32108775 0.33458313 0.32360995 0.29834658 0.27241024 0.25339982 0.2455759 0.24890828 0.26492947 0.29291862 0.32522029][0.10997351 0.18367285 0.25795168 0.32483512 0.37790138 0.40635782 0.40612951 0.38520566 0.35694888 0.33174053 0.31731045 0.31562051 0.33014289 0.36016098 0.39458185][0.10940093 0.18627675 0.26790205 0.34522963 0.4131237 0.4589977 0.47364426 0.46125576 0.43364474 0.40337843 0.38184413 0.37400246 0.3857424 0.41577443 0.45062762][0.096333124 0.17194092 0.25744164 0.34355482 0.42693833 0.49280757 0.52568948 0.52486914 0.49938154 0.46316859 0.4308067 0.41162267 0.41546479 0.44200015 0.47702107][0.074395768 0.14531395 0.23078912 0.32137784 0.4150787 0.4951694 0.54138517 0.5487591 0.52294379 0.47895035 0.43350223 0.40001503 0.39206558 0.41156167 0.44570863][0.052316859 0.11658726 0.19747452 0.28538442 0.37864879 0.45979196 0.50717616 0.51424849 0.48455739 0.43299329 0.37662011 0.33135563 0.3133232 0.32606152 0.35850373][0.032599892 0.08793018 0.15882272 0.23601636 0.31802085 0.38882476 0.42894253 0.43222067 0.40062582 0.34718722 0.28719944 0.23724104 0.21413454 0.22186066 0.25116253][0.012516167 0.055408139 0.11022304 0.16915403 0.23144819 0.28444514 0.31335655 0.31372717 0.28582394 0.23903373 0.18498191 0.13917333 0.11728405 0.12316082 0.14942354][-0.0011886674 0.029134108 0.065858066 0.10338631 0.14226575 0.17424174 0.18988971 0.18683922 0.16416928 0.12733528 0.083908573 0.047051657 0.029910695 0.035993714 0.059558518][0.00070375449 0.023517648 0.046435457 0.065230675 0.081793569 0.092585258 0.093627118 0.0849199 0.065372765 0.037454613 0.0052151731 -0.021678112 -0.03374818 -0.028247803 -0.0093772411][0.0098546259 0.030072341 0.045316115 0.051391009 0.05098046 0.044871498 0.033888537 0.020209439 0.0032051327 -0.016794778 -0.038474839 -0.05623617 -0.0643351 -0.061069936 -0.049216941][0.011213996 0.029371332 0.040099703 0.038619429 0.027828027 0.011486003 -0.006176142 -0.021527519 -0.034751475 -0.046953715 -0.058851313 -0.068470545 -0.073230609 -0.072300486 -0.067158788]]...]
INFO - root - 2017-12-10 12:22:15.199801: step 14410, loss = 0.68, batch loss = 0.62 (7.9 examples/sec; 1.008 sec/batch; 89h:03m:41s remains)
INFO - root - 2017-12-10 12:22:25.155934: step 14420, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.037 sec/batch; 91h:38m:39s remains)
INFO - root - 2017-12-10 12:22:35.438190: step 14430, loss = 0.67, batch loss = 0.62 (7.8 examples/sec; 1.030 sec/batch; 91h:01m:30s remains)
INFO - root - 2017-12-10 12:22:45.746346: step 14440, loss = 0.70, batch loss = 0.64 (7.6 examples/sec; 1.049 sec/batch; 92h:39m:31s remains)
INFO - root - 2017-12-10 12:22:55.914962: step 14450, loss = 0.70, batch loss = 0.65 (7.7 examples/sec; 1.044 sec/batch; 92h:12m:54s remains)
INFO - root - 2017-12-10 12:23:06.187863: step 14460, loss = 0.70, batch loss = 0.64 (7.6 examples/sec; 1.059 sec/batch; 93h:35m:24s remains)
INFO - root - 2017-12-10 12:23:16.335561: step 14470, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.010 sec/batch; 89h:15m:01s remains)
INFO - root - 2017-12-10 12:23:26.286656: step 14480, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.025 sec/batch; 90h:30m:48s remains)
INFO - root - 2017-12-10 12:23:36.428705: step 14490, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.008 sec/batch; 89h:04m:39s remains)
INFO - root - 2017-12-10 12:23:46.631705: step 14500, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 0.996 sec/batch; 88h:00m:14s remains)
2017-12-10 12:23:47.651522: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.061125416 0.074138604 0.10278777 0.15226826 0.22470447 0.30429685 0.36857939 0.39494285 0.37601927 0.32671747 0.2741982 0.24008454 0.23869908 0.27806258 0.3456288][0.029769074 0.049229585 0.086424626 0.14427988 0.22427247 0.31322739 0.39188588 0.43643752 0.43541172 0.4002482 0.35655123 0.32197103 0.31062409 0.33528697 0.38927022][0.0087860264 0.032205645 0.076585129 0.1434084 0.23178422 0.32831478 0.41502407 0.46842968 0.47589955 0.44726312 0.40800384 0.37390196 0.3560583 0.36689663 0.40314788][0.007093918 0.031086702 0.080903113 0.15792665 0.25656796 0.35958353 0.44795671 0.49864516 0.50161809 0.4687058 0.42807356 0.39553964 0.3766835 0.37793991 0.39527309][0.036580652 0.055557903 0.10530899 0.1888006 0.2951082 0.40074196 0.48364872 0.52214116 0.51164877 0.46945113 0.42808747 0.40291342 0.39132825 0.38950285 0.38986796][0.093396612 0.099289991 0.13840204 0.21882108 0.32679573 0.43255991 0.50899142 0.53508085 0.51205665 0.46363688 0.42671406 0.41540363 0.41816938 0.41849184 0.40401545][0.160585 0.14836077 0.16753182 0.23484068 0.33818427 0.44358113 0.51764882 0.53758788 0.50818014 0.45797011 0.42690903 0.42818549 0.44469917 0.44907728 0.42318252][0.22160143 0.19342002 0.19044188 0.23892772 0.33206391 0.43574357 0.51127589 0.53240287 0.50373137 0.45538479 0.42806697 0.43513149 0.45837641 0.46403933 0.42879072][0.25083566 0.21449186 0.19532841 0.22499132 0.30314 0.40040645 0.47800481 0.50753641 0.49027994 0.45345271 0.4342587 0.44518378 0.47008389 0.47392267 0.43023819][0.24080531 0.20867704 0.1852082 0.20100862 0.261161 0.34503505 0.42016703 0.45973456 0.46051174 0.4434121 0.43810076 0.45457619 0.48045194 0.48280519 0.43419909][0.20712584 0.19411419 0.18204676 0.19391081 0.23656699 0.29957604 0.36210284 0.40370551 0.41862306 0.42043364 0.42902443 0.45051679 0.47641709 0.47788942 0.42823598][0.16577487 0.18199623 0.19538999 0.21605112 0.24717021 0.28614137 0.32606387 0.35715342 0.3753559 0.38875329 0.40768525 0.43299112 0.45795351 0.45862672 0.4106847][0.12475412 0.16952007 0.2127642 0.25028139 0.277683 0.29593948 0.31085429 0.32467878 0.33866215 0.35775068 0.38445616 0.41393334 0.43838698 0.43802491 0.39232862][0.086669438 0.14922814 0.2150788 0.26869464 0.29759511 0.30267778 0.29821053 0.29648384 0.30540887 0.32851988 0.36238521 0.39712313 0.42285934 0.42225406 0.3788549][0.056712307 0.1233491 0.19896431 0.26256716 0.29612124 0.29794902 0.28488508 0.27494636 0.28148505 0.30820385 0.34773576 0.38683596 0.41349593 0.4116261 0.36862892]]...]
INFO - root - 2017-12-10 12:23:57.740629: step 14510, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.020 sec/batch; 90h:06m:42s remains)
INFO - root - 2017-12-10 12:24:07.997934: step 14520, loss = 0.68, batch loss = 0.62 (7.8 examples/sec; 1.030 sec/batch; 90h:59m:23s remains)
INFO - root - 2017-12-10 12:24:18.226731: step 14530, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.036 sec/batch; 91h:30m:19s remains)
INFO - root - 2017-12-10 12:24:28.470461: step 14540, loss = 0.69, batch loss = 0.64 (7.7 examples/sec; 1.039 sec/batch; 91h:44m:36s remains)
INFO - root - 2017-12-10 12:24:38.663492: step 14550, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.009 sec/batch; 89h:06m:44s remains)
INFO - root - 2017-12-10 12:24:48.746888: step 14560, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.027 sec/batch; 90h:42m:42s remains)
INFO - root - 2017-12-10 12:24:58.991986: step 14570, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.008 sec/batch; 88h:59m:11s remains)
INFO - root - 2017-12-10 12:25:09.240723: step 14580, loss = 0.72, batch loss = 0.66 (7.8 examples/sec; 1.022 sec/batch; 90h:13m:12s remains)
INFO - root - 2017-12-10 12:25:19.514673: step 14590, loss = 0.71, batch loss = 0.65 (7.5 examples/sec; 1.065 sec/batch; 94h:02m:15s remains)
INFO - root - 2017-12-10 12:25:29.560728: step 14600, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.029 sec/batch; 90h:50m:05s remains)
2017-12-10 12:25:30.552780: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28138408 0.26179484 0.26460415 0.29105091 0.32588333 0.35955018 0.38467744 0.4010458 0.41970888 0.46291587 0.53207207 0.59996343 0.63539875 0.62525117 0.56943738][0.24462046 0.23051219 0.23736915 0.26673871 0.30618221 0.34682861 0.37881631 0.40081808 0.42232996 0.4653489 0.534291 0.60674214 0.65239322 0.65476334 0.60879052][0.19382569 0.19392756 0.21556951 0.26028961 0.31690621 0.37305763 0.41234532 0.43054926 0.43639413 0.45238155 0.49058226 0.54066348 0.58031118 0.59236211 0.5664894][0.14181058 0.16463904 0.21318452 0.28568834 0.37067232 0.44977823 0.49855286 0.50788665 0.48682925 0.45912161 0.44533235 0.450121 0.46349552 0.47246897 0.46368703][0.068230875 0.12041334 0.20811418 0.32169864 0.44721919 0.56018519 0.62774092 0.6332438 0.58641309 0.51329529 0.44106838 0.38566485 0.35164326 0.3371233 0.33115104][-0.0081233522 0.072946504 0.20014359 0.35613516 0.52366722 0.67399907 0.76596123 0.77503568 0.71289951 0.60801566 0.48874149 0.37346888 0.28093287 0.22563054 0.20374696][-0.052464556 0.046495348 0.19782205 0.38147402 0.57843459 0.75844622 0.87208146 0.88867533 0.82300454 0.70578128 0.5612936 0.40208542 0.25742376 0.15797155 0.10944954][-0.026322342 0.065620638 0.20584336 0.38319308 0.58239424 0.77312773 0.89778984 0.92177409 0.86215317 0.75030118 0.6029439 0.4241012 0.2486448 0.11765967 0.044625293][0.077731483 0.1286349 0.21206868 0.33966535 0.50579643 0.68065959 0.79992229 0.82727164 0.779948 0.68711942 0.55687565 0.38630471 0.2111125 0.075141713 -0.0061773988][0.22795881 0.21434861 0.21106388 0.26114258 0.37017342 0.50813371 0.60752356 0.63309944 0.59917057 0.53020281 0.42825967 0.28718397 0.1400937 0.026702555 -0.040845279][0.36685675 0.2900711 0.20125453 0.17200837 0.21690682 0.30647767 0.37650552 0.39599451 0.37397963 0.32782048 0.25699291 0.1563893 0.054079816 -0.019405663 -0.058258869][0.423732 0.3060284 0.16246067 0.079006165 0.074194983 0.12050786 0.16334054 0.17742844 0.16593777 0.13892567 0.095836855 0.035333011 -0.020863198 -0.053186625 -0.0621814][0.38137484 0.2518363 0.094456419 -0.0074810567 -0.036899522 -0.018079011 0.0069816746 0.018852815 0.016546456 0.0048510842 -0.016173584 -0.044515718 -0.065028071 -0.06783326 -0.057622317][0.25847891 0.14332901 0.0084210876 -0.079995431 -0.11043548 -0.10291904 -0.08638715 -0.074606568 -0.070277788 -0.07157135 -0.078078523 -0.08661516 -0.087679505 -0.077277854 -0.060968034][0.11180808 0.026177168 -0.068095818 -0.12735274 -0.14647315 -0.14068699 -0.12792011 -0.11681138 -0.10976223 -0.10606217 -0.10506634 -0.10418401 -0.098303318 -0.086116835 -0.07168889]]...]
INFO - root - 2017-12-10 12:25:40.754540: step 14610, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.015 sec/batch; 89h:39m:35s remains)
INFO - root - 2017-12-10 12:25:50.839757: step 14620, loss = 0.68, batch loss = 0.62 (8.1 examples/sec; 0.990 sec/batch; 87h:24m:51s remains)
INFO - root - 2017-12-10 12:26:00.970205: step 14630, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.009 sec/batch; 89h:07m:57s remains)
INFO - root - 2017-12-10 12:26:10.962084: step 14640, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.044 sec/batch; 92h:13m:10s remains)
INFO - root - 2017-12-10 12:26:21.028348: step 14650, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.012 sec/batch; 89h:23m:39s remains)
INFO - root - 2017-12-10 12:26:31.155263: step 14660, loss = 0.70, batch loss = 0.65 (8.1 examples/sec; 0.991 sec/batch; 87h:28m:54s remains)
INFO - root - 2017-12-10 12:26:41.348335: step 14670, loss = 0.70, batch loss = 0.65 (7.7 examples/sec; 1.034 sec/batch; 91h:18m:36s remains)
INFO - root - 2017-12-10 12:26:51.504913: step 14680, loss = 0.70, batch loss = 0.65 (7.8 examples/sec; 1.032 sec/batch; 91h:05m:14s remains)
INFO - root - 2017-12-10 12:27:01.628582: step 14690, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.036 sec/batch; 91h:28m:43s remains)
INFO - root - 2017-12-10 12:27:11.861208: step 14700, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.021 sec/batch; 90h:07m:36s remains)
2017-12-10 12:27:12.779203: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29747829 0.2988832 0.26901788 0.22002992 0.16948763 0.12705207 0.094696932 0.070522889 0.056102592 0.054064412 0.074824058 0.13571595 0.22543882 0.31617922 0.37273][0.23702715 0.21966112 0.17761734 0.1233229 0.074043185 0.038412113 0.017536271 0.010626256 0.019615419 0.04381464 0.090675548 0.17727971 0.29161963 0.4033246 0.47439981][0.17540003 0.14353944 0.095745355 0.046232574 0.011260476 -0.0036258469 -0.0019542542 0.012963642 0.040591791 0.077430338 0.12940454 0.21504028 0.32589763 0.43462333 0.50609523][0.13539805 0.10064141 0.056798954 0.02148127 0.010408947 0.025076028 0.055106457 0.091617934 0.12915055 0.16233079 0.19577193 0.25138998 0.32804787 0.40727851 0.46253514][0.1325954 0.10704508 0.075916 0.059198737 0.073402084 0.11665075 0.17243296 0.2247418 0.26257685 0.28073469 0.28227109 0.29308775 0.32054526 0.35794735 0.38932863][0.16595918 0.15870717 0.14568429 0.1477502 0.18302384 0.24755184 0.32029879 0.37863907 0.40812698 0.40659449 0.37609011 0.34627849 0.331902 0.33505061 0.34694234][0.2108462 0.22501533 0.23156688 0.24968155 0.29912868 0.37483206 0.453088 0.507228 0.52251869 0.50229329 0.44913942 0.39606503 0.36172646 0.35043195 0.35536855][0.24203612 0.27391857 0.29628956 0.324736 0.37974739 0.4555226 0.527877 0.57048482 0.57097673 0.53772539 0.47437459 0.41701561 0.38501838 0.3789185 0.38920936][0.251079 0.29100958 0.32049659 0.35028186 0.39975503 0.46346125 0.5198878 0.54730093 0.53669953 0.49853039 0.43671277 0.38798654 0.37010378 0.37736258 0.39633834][0.22654366 0.26323867 0.28952894 0.31120649 0.34489504 0.38704476 0.42175049 0.43427196 0.41865438 0.38353327 0.3315098 0.29603374 0.29197869 0.30878353 0.33158833][0.16355945 0.18945596 0.20659259 0.21658544 0.23203805 0.25157434 0.26563874 0.26672667 0.25142261 0.22439627 0.18583812 0.16251005 0.16546769 0.18241097 0.20051914][0.0721366 0.084724665 0.091957644 0.092620149 0.09483666 0.0983384 0.09838035 0.093043581 0.080993943 0.063358888 0.03820594 0.024125149 0.028343797 0.039890323 0.049612138][-0.014146878 -0.012705967 -0.012824608 -0.0170978 -0.021092065 -0.025008241 -0.031108351 -0.038496308 -0.046839338 -0.05659382 -0.070613414 -0.078175575 -0.075035341 -0.068932578 -0.065432116][-0.068312019 -0.072759382 -0.075851671 -0.080745608 -0.085400514 -0.09022855 -0.0964072 -0.10271946 -0.1080607 -0.11276532 -0.11920917 -0.12262107 -0.12103579 -0.11865754 -0.11807977][-0.090340413 -0.096200541 -0.098875314 -0.10202254 -0.1047918 -0.10758526 -0.11119623 -0.11506386 -0.11818935 -0.12036524 -0.12287284 -0.12413911 -0.12345953 -0.12255405 -0.12243224]]...]
INFO - root - 2017-12-10 12:27:23.022793: step 14710, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.021 sec/batch; 90h:06m:43s remains)
INFO - root - 2017-12-10 12:27:33.047342: step 14720, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.018 sec/batch; 89h:53m:33s remains)
INFO - root - 2017-12-10 12:27:43.163427: step 14730, loss = 0.70, batch loss = 0.64 (8.2 examples/sec; 0.976 sec/batch; 86h:07m:36s remains)
INFO - root - 2017-12-10 12:27:53.327758: step 14740, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.028 sec/batch; 90h:45m:08s remains)
INFO - root - 2017-12-10 12:28:03.516393: step 14750, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.008 sec/batch; 88h:56m:05s remains)
INFO - root - 2017-12-10 12:28:13.571380: step 14760, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.039 sec/batch; 91h:42m:14s remains)
INFO - root - 2017-12-10 12:28:23.510348: step 14770, loss = 0.69, batch loss = 0.63 (9.4 examples/sec; 0.851 sec/batch; 75h:06m:52s remains)
INFO - root - 2017-12-10 12:28:33.718115: step 14780, loss = 0.68, batch loss = 0.63 (7.8 examples/sec; 1.031 sec/batch; 91h:02m:06s remains)
INFO - root - 2017-12-10 12:28:43.472919: step 14790, loss = 0.70, batch loss = 0.64 (8.3 examples/sec; 0.968 sec/batch; 85h:26m:38s remains)
INFO - root - 2017-12-10 12:28:51.016308: step 14800, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 69h:00m:20s remains)
2017-12-10 12:28:51.789752: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.4228887 0.30796874 0.19265212 0.1134462 0.071391374 0.048832785 0.033993151 0.02187269 0.0043844609 -0.014005139 -0.027639536 -0.03943998 -0.052345164 -0.065526612 -0.076645114][0.41448617 0.2942864 0.17560184 0.09772028 0.059876494 0.040049326 0.021724518 6.5330511e-05 -0.030050982 -0.0633554 -0.090793386 -0.10866293 -0.11729492 -0.11561929 -0.1014982][0.4038341 0.29054639 0.18060365 0.1133695 0.085549518 0.072754383 0.055591661 0.0299089 -0.0064760973 -0.049508173 -0.089147538 -0.11750631 -0.130037 -0.12266597 -0.090266272][0.43012145 0.33450359 0.24224439 0.19330162 0.18024932 0.17728309 0.1634644 0.13575274 0.094677649 0.042269528 -0.012369485 -0.058393169 -0.087041572 -0.089763649 -0.055828556][0.50650477 0.43370777 0.36404574 0.33797371 0.34295598 0.35096109 0.34075686 0.310149 0.26223677 0.19640107 0.12126993 0.049522966 -0.0068546911 -0.036001392 -0.021243561][0.59702909 0.5490995 0.50446826 0.5039227 0.530115 0.55311823 0.55148548 0.52262843 0.4701677 0.39011788 0.2916677 0.18939704 0.098201409 0.031233842 0.009258179][0.64025366 0.61203444 0.59036756 0.61549973 0.66567969 0.70944184 0.72422636 0.70648444 0.65822792 0.57223183 0.45866033 0.33409175 0.21524441 0.1133776 0.050297335][0.59535891 0.57448119 0.56908643 0.61607146 0.69070852 0.76043671 0.7997911 0.80394405 0.77174342 0.69349146 0.5802123 0.45066321 0.32225502 0.20172425 0.10905921][0.46399426 0.43945491 0.44294047 0.50602174 0.60295647 0.70028365 0.76930106 0.80250555 0.79407167 0.73345733 0.6330477 0.51367772 0.39346182 0.27439037 0.17305362][0.29163551 0.25622088 0.26190418 0.33232909 0.44191444 0.55732888 0.64911062 0.70717007 0.7204901 0.67976838 0.59884226 0.49990597 0.40093833 0.30082306 0.2123317][0.12749532 0.080543369 0.08360602 0.15160261 0.25945392 0.37504524 0.47132754 0.53744388 0.55998576 0.53377783 0.47387356 0.40128484 0.33180365 0.26226109 0.20197685][0.0047363285 -0.0491685 -0.050389033 0.0083778771 0.10345075 0.20447266 0.28754348 0.34205025 0.35622382 0.33171344 0.28544319 0.23601639 0.19581117 0.15994503 0.13429256][-0.066797368 -0.11874153 -0.12180144 -0.073657334 0.0066756052 0.091288134 0.15710981 0.19224291 0.1881721 0.15327184 0.1064766 0.065317392 0.040666908 0.028323457 0.031507935][-0.075207949 -0.12015469 -0.12475558 -0.086715586 -0.017058598 0.059478592 0.11712652 0.139224 0.11869104 0.068191528 0.0082662795 -0.041620698 -0.068353929 -0.072854154 -0.053956736][-0.016720839 -0.055264626 -0.06536784 -0.039556064 0.022458013 0.10030031 0.16327649 0.18628573 0.1597801 0.09651465 0.0176941 -0.052582812 -0.095233396 -0.10586902 -0.085027806]]...]
INFO - root - 2017-12-10 12:28:59.729224: step 14810, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 70h:00m:49s remains)
INFO - root - 2017-12-10 12:29:07.653097: step 14820, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 69h:13m:19s remains)
INFO - root - 2017-12-10 12:29:15.447321: step 14830, loss = 0.69, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 66h:47m:26s remains)
INFO - root - 2017-12-10 12:29:23.390484: step 14840, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 70h:09m:45s remains)
INFO - root - 2017-12-10 12:29:31.147698: step 14850, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 68h:04m:25s remains)
INFO - root - 2017-12-10 12:29:38.872735: step 14860, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 67h:57m:02s remains)
INFO - root - 2017-12-10 12:29:46.726698: step 14870, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 67h:31m:53s remains)
INFO - root - 2017-12-10 12:29:54.366424: step 14880, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 68h:36m:10s remains)
INFO - root - 2017-12-10 12:30:02.188223: step 14890, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 70h:30m:13s remains)
INFO - root - 2017-12-10 12:30:10.035870: step 14900, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 69h:49m:50s remains)
2017-12-10 12:30:10.820986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043250505 -0.043396242 -0.042754695 -0.042617213 -0.042796984 -0.042964075 -0.0428631 -0.04241756 -0.041650377 -0.04081025 -0.040236909 -0.0403343 -0.04139049 -0.043130152 -0.044937439][-0.042973161 -0.042569552 -0.041480608 -0.041239429 -0.041647471 -0.042231731 -0.042475224 -0.041972771 -0.040513523 -0.038426485 -0.0364287 -0.035621379 -0.036854237 -0.040019315 -0.044012126][-0.031898502 -0.029315874 -0.026610795 -0.025651574 -0.026446749 -0.028263411 -0.029960165 -0.03019256 -0.028048795 -0.023911919 -0.019244958 -0.01626719 -0.017048324 -0.022140829 -0.029733079][-0.0093087759 -0.0013471266 0.0056955409 0.0089271953 0.0078815036 0.0041160281 8.2159524e-05 -0.0011766152 0.0022136273 0.00973374 0.018617509 0.02474764 0.024290174 0.015971184 0.0029285762][0.032599129 0.048622616 0.061675914 0.068132348 0.066577733 0.059653491 0.051679637 0.048092116 0.051900286 0.062662937 0.076223746 0.085964076 0.086001426 0.074394092 0.055822421][0.094107978 0.11667113 0.13460438 0.14395417 0.14171031 0.13106772 0.11813997 0.11075163 0.11288281 0.12476128 0.14160638 0.15473026 0.15646777 0.14406307 0.12297872][0.15992425 0.18597622 0.2064521 0.217591 0.21463728 0.200833 0.18339424 0.17155075 0.16976039 0.17940614 0.19638163 0.21137007 0.21579628 0.20608479 0.18706769][0.20585036 0.23299815 0.25324684 0.2641944 0.25986212 0.24387525 0.22396192 0.20917107 0.20327905 0.20841859 0.22188689 0.2355511 0.24131803 0.2355434 0.22191875][0.22447948 0.24942799 0.26712996 0.27569154 0.26877242 0.25106123 0.23062474 0.21517327 0.20653781 0.20679478 0.21489215 0.22458619 0.22955996 0.22695555 0.21960677][0.22174358 0.24206808 0.25404814 0.25703833 0.24542969 0.22520891 0.20472565 0.18975313 0.17936897 0.17494133 0.1768202 0.18107745 0.18374535 0.18324943 0.18149565][0.19638425 0.20908764 0.21339975 0.20875643 0.19111606 0.1678831 0.14794588 0.13485008 0.12470631 0.11740156 0.11432324 0.11394957 0.11446586 0.11544164 0.11792536][0.15112896 0.15883566 0.15750335 0.145729 0.12142942 0.094161674 0.074201353 0.063474461 0.055614658 0.048091419 0.042475175 0.039272368 0.038290996 0.040012803 0.045091167][0.09918268 0.10480004 0.10036387 0.08355587 0.0542145 0.023890233 0.0042822696 -0.0032690384 -0.0068632741 -0.011731056 -0.017264649 -0.021736991 -0.024302959 -0.023708006 -0.018767036][0.058261942 0.061504897 0.05529093 0.035872228 0.0044920715 -0.026755016 -0.045675568 -0.050578974 -0.049854044 -0.050584912 -0.053882007 -0.057944752 -0.061502814 -0.062695071 -0.059766605][0.033304814 0.03467482 0.027844626 0.0083717918 -0.021945458 -0.052029368 -0.070272446 -0.074259035 -0.07122539 -0.068788685 -0.069748275 -0.073063612 -0.077477254 -0.080578282 -0.08023762]]...]
INFO - root - 2017-12-10 12:30:18.724757: step 14910, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 71h:17m:12s remains)
INFO - root - 2017-12-10 12:30:26.754542: step 14920, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 70h:51m:57s remains)
INFO - root - 2017-12-10 12:30:34.598948: step 14930, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.816 sec/batch; 72h:00m:14s remains)
INFO - root - 2017-12-10 12:30:42.429256: step 14940, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 67h:31m:01s remains)
INFO - root - 2017-12-10 12:30:50.233602: step 14950, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 71h:22m:28s remains)
INFO - root - 2017-12-10 12:30:57.823223: step 14960, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 67h:59m:12s remains)
INFO - root - 2017-12-10 12:31:05.603749: step 14970, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 68h:13m:37s remains)
INFO - root - 2017-12-10 12:31:13.494421: step 14980, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 70h:29m:25s remains)
INFO - root - 2017-12-10 12:31:21.366784: step 14990, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 70h:15m:07s remains)
INFO - root - 2017-12-10 12:31:29.314888: step 15000, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 68h:46m:44s remains)
2017-12-10 12:31:30.075929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.056581043 -0.060285058 -0.05722234 -0.045107033 -0.019712174 0.014740612 0.048923876 0.069542512 0.0699288 0.051659755 0.017052494 -0.022504186 -0.056799944 -0.076271288 -0.080173805][-0.062102854 -0.063111447 -0.054214448 -0.031683005 0.012327475 0.071286641 0.12931573 0.16651206 0.17324477 0.15064108 0.1001045 0.036505066 -0.022913037 -0.061963107 -0.078457169][-0.063697204 -0.059564725 -0.042514279 -0.0069236225 0.059139557 0.14631216 0.2316954 0.28816319 0.30360395 0.27794683 0.21139279 0.12012168 0.02904883 -0.035566147 -0.068529435][-0.057517603 -0.044395112 -0.015482774 0.035462946 0.12248875 0.2341823 0.34297106 0.41610423 0.43926677 0.41058278 0.32998231 0.21306196 0.090932749 -0.00041362765 -0.051851153][-0.030358903 -0.00084484869 0.046436619 0.11485513 0.21683225 0.34117594 0.46134394 0.54316884 0.57012647 0.53617591 0.44419867 0.30941555 0.16467167 0.050013825 -0.022147264][0.025400255 0.081242032 0.15445554 0.24199973 0.35129243 0.47388861 0.5902558 0.6695382 0.69319475 0.65014768 0.54968375 0.40682527 0.24933101 0.11504152 0.020377869][0.10153314 0.18991472 0.29228482 0.39750761 0.50614923 0.61453491 0.71335965 0.77855933 0.7908287 0.73664117 0.63324654 0.49328151 0.33298987 0.18381165 0.068259552][0.17621391 0.29323825 0.41798878 0.53151882 0.62896717 0.71168071 0.78056216 0.82156634 0.81802654 0.75729692 0.66084439 0.53575212 0.38402864 0.22979982 0.10220835][0.21737035 0.34619784 0.47587386 0.58257973 0.65841663 0.70824111 0.74109614 0.75463796 0.73811394 0.68088043 0.60243243 0.50221413 0.3705503 0.22617856 0.10244323][0.20289537 0.32045761 0.43382069 0.51866019 0.56653887 0.58383876 0.584825 0.57731736 0.55678022 0.51355571 0.45999503 0.38869587 0.28477666 0.1646108 0.061623994][0.13525498 0.22281495 0.30453902 0.35913104 0.37959892 0.37278932 0.35465845 0.33800703 0.3214623 0.29553187 0.26483503 0.21949796 0.14658961 0.060999997 -0.0089404145][0.042730622 0.091973409 0.13628337 0.15958463 0.15795539 0.13804877 0.11397884 0.09794011 0.088971913 0.078047581 0.06490168 0.041320894 -0.00042577364 -0.048089158 -0.081954226][-0.041061349 -0.02733813 -0.016492423 -0.018246071 -0.033204921 -0.055853207 -0.0764202 -0.086820059 -0.088863954 -0.089942619 -0.091712728 -0.098712362 -0.11393464 -0.12897639 -0.13316266][-0.096655324 -0.1069288 -0.11741984 -0.13220371 -0.14983305 -0.16696623 -0.17910999 -0.18297681 -0.18094483 -0.17770219 -0.17410079 -0.17140666 -0.16982478 -0.16468298 -0.15204439][-0.11644396 -0.1352102 -0.15185015 -0.1678393 -0.18133174 -0.19089186 -0.1957259 -0.19550255 -0.19228116 -0.18847917 -0.1841204 -0.17867765 -0.17116797 -0.15932249 -0.14264765]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 12:31:37.926038: step 15010, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 67h:12m:56s remains)
INFO - root - 2017-12-10 12:31:45.728134: step 15020, loss = 0.71, batch loss = 0.66 (10.6 examples/sec; 0.756 sec/batch; 66h:40m:25s remains)
INFO - root - 2017-12-10 12:31:53.481173: step 15030, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 68h:50m:38s remains)
INFO - root - 2017-12-10 12:32:00.974645: step 15040, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 70h:58m:20s remains)
INFO - root - 2017-12-10 12:32:08.799858: step 15050, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 70h:18m:44s remains)
INFO - root - 2017-12-10 12:32:16.731057: step 15060, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 70h:25m:23s remains)
INFO - root - 2017-12-10 12:32:24.509857: step 15070, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 67h:17m:43s remains)
INFO - root - 2017-12-10 12:32:32.302562: step 15080, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 68h:21m:43s remains)
INFO - root - 2017-12-10 12:32:40.161043: step 15090, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 68h:25m:34s remains)
INFO - root - 2017-12-10 12:32:47.880254: step 15100, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 68h:42m:15s remains)
2017-12-10 12:32:48.714455: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0057323305 0.017006882 0.0278171 0.039972275 0.056517605 0.0751775 0.091997169 0.10191962 0.095947914 0.073199525 0.0423078 0.020580668 0.017732609 0.028620241 0.043446075][0.029677834 0.048703577 0.068728715 0.092098832 0.12271167 0.15805952 0.19023894 0.20735405 0.19696699 0.16050155 0.11139745 0.073737569 0.062975384 0.075002596 0.095318][0.054900009 0.082319617 0.11273209 0.14862941 0.19385633 0.24539614 0.29199341 0.31588355 0.30027658 0.2491793 0.18260829 0.13145228 0.11400211 0.12612212 0.15107878][0.0769288 0.11075833 0.14842258 0.19212222 0.24592091 0.30804616 0.36575645 0.39665055 0.37906137 0.31842792 0.24061064 0.1808989 0.15899925 0.17053583 0.1979017][0.09737137 0.1363169 0.17922565 0.22812127 0.28812623 0.359808 0.42906874 0.46812356 0.4510642 0.38417661 0.29700679 0.22705549 0.19641374 0.20303611 0.22904842][0.12327476 0.1699958 0.22099181 0.27853423 0.34844673 0.4321548 0.51358813 0.55986261 0.54221916 0.4673517 0.36708462 0.28155789 0.23593837 0.23130964 0.24988823][0.15339307 0.20965473 0.26909298 0.33394942 0.40982136 0.49809366 0.58398616 0.63357162 0.61751103 0.54114443 0.43478298 0.33812875 0.27677032 0.25540504 0.25992316][0.17991984 0.24262172 0.30434063 0.36706063 0.43670833 0.51589334 0.59552574 0.64556438 0.63721305 0.57235807 0.47503734 0.37932277 0.30783543 0.26915061 0.25685415][0.19971302 0.26419225 0.32179523 0.37452358 0.42987478 0.49294069 0.56120014 0.61040276 0.61271626 0.56537193 0.48437795 0.39674819 0.32176149 0.27078626 0.24453828][0.20152573 0.26359305 0.31432158 0.35609958 0.39804721 0.44721496 0.50509107 0.55184609 0.56068635 0.52554029 0.45655087 0.37625831 0.30229074 0.24679136 0.21404134][0.1780784 0.23448749 0.27848998 0.31138924 0.34247693 0.37944809 0.42632335 0.46700644 0.47763014 0.45062259 0.39165005 0.31922954 0.24966745 0.19537091 0.16214254][0.12656432 0.17124367 0.20473078 0.22676374 0.24557814 0.268641 0.30199587 0.33420259 0.34601355 0.32952783 0.28604534 0.2282363 0.16927898 0.12176403 0.092984363][0.06277629 0.090057924 0.1079767 0.11571184 0.12056657 0.12965922 0.14948912 0.17295484 0.18560059 0.18052882 0.15623716 0.11897957 0.07742881 0.042807676 0.023258528][0.0070069525 0.017999845 0.022067303 0.018377384 0.013461949 0.013575526 0.023316871 0.038292546 0.048370268 0.048784912 0.037713315 0.017578719 -0.0061767264 -0.025625424 -0.034304302][-0.036526855 -0.036774926 -0.040442526 -0.048751011 -0.056457575 -0.059074342 -0.055061147 -0.047766421 -0.043253411 -0.0436595 -0.049801134 -0.060409784 -0.07149227 -0.078608617 -0.07867568]]...]
INFO - root - 2017-12-10 12:32:56.577513: step 15110, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 69h:22m:28s remains)
INFO - root - 2017-12-10 12:33:04.256473: step 15120, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 70h:10m:45s remains)
INFO - root - 2017-12-10 12:33:11.961490: step 15130, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 68h:45m:30s remains)
INFO - root - 2017-12-10 12:33:19.816611: step 15140, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 68h:09m:25s remains)
INFO - root - 2017-12-10 12:33:27.698775: step 15150, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.757 sec/batch; 66h:41m:24s remains)
INFO - root - 2017-12-10 12:33:35.549060: step 15160, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 69h:22m:38s remains)
INFO - root - 2017-12-10 12:33:43.415831: step 15170, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 70h:42m:57s remains)
INFO - root - 2017-12-10 12:33:51.294757: step 15180, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.771 sec/batch; 67h:56m:29s remains)
INFO - root - 2017-12-10 12:33:59.065534: step 15190, loss = 0.70, batch loss = 0.65 (10.6 examples/sec; 0.752 sec/batch; 66h:15m:25s remains)
INFO - root - 2017-12-10 12:34:06.763054: step 15200, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 70h:04m:06s remains)
2017-12-10 12:34:07.607689: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21491738 0.22486486 0.22700614 0.22634198 0.22252898 0.21443082 0.19816515 0.17415208 0.14855586 0.12486846 0.10433358 0.083934568 0.0647833 0.048739929 0.03528611][0.23547748 0.24004182 0.23898163 0.23914255 0.23903477 0.23518714 0.22229929 0.19901778 0.17156425 0.14518793 0.12277506 0.10298096 0.087025516 0.076497816 0.069026284][0.24270779 0.24006434 0.23348415 0.23179719 0.23363812 0.23412745 0.226667 0.20683737 0.17902607 0.14954346 0.12451595 0.10619713 0.096640542 0.09735021 0.10282121][0.2476474 0.23914665 0.22582483 0.2188381 0.21892031 0.22142915 0.21841206 0.20224109 0.17402743 0.13980311 0.10965721 0.090983659 0.087815478 0.10163918 0.12385238][0.2514106 0.23862812 0.21772428 0.20286809 0.19876283 0.20231718 0.20409971 0.19302189 0.16588016 0.12728965 0.090264328 0.067082085 0.065691076 0.0889177 0.12598228][0.24782543 0.23271932 0.20511924 0.18254454 0.17479496 0.18078834 0.18958396 0.18631205 0.16336493 0.12302248 0.079007618 0.047220156 0.040451128 0.065207347 0.11121342][0.2312032 0.21684247 0.18630435 0.15934396 0.15042315 0.16056779 0.17821953 0.18513587 0.17002326 0.13234204 0.083961815 0.042349655 0.024871992 0.042867422 0.089983582][0.20494756 0.19541045 0.1669649 0.13908057 0.12910455 0.14069413 0.16408451 0.18020849 0.17557514 0.14592452 0.099324837 0.052159373 0.024283215 0.031958066 0.073751248][0.17574902 0.17516689 0.1530557 0.12593694 0.11231098 0.11966049 0.14288765 0.16514093 0.17191969 0.15483433 0.11690105 0.071387962 0.038155407 0.036530618 0.069728486][0.14687708 0.15735354 0.1442956 0.11897027 0.099742331 0.098634295 0.11673463 0.14099082 0.15691584 0.15298061 0.12773341 0.090590365 0.059020016 0.052589078 0.076979853][0.12324367 0.1449562 0.14204799 0.11980233 0.095512606 0.08494062 0.094746985 0.1157466 0.1346003 0.13897209 0.12525837 0.099799387 0.07546436 0.069045857 0.08642298][0.1089571 0.14016883 0.1472138 0.13011594 0.10408141 0.0863842 0.0869249 0.099764347 0.11412435 0.11948594 0.11250694 0.097493321 0.081724614 0.077135548 0.088556387][0.10016382 0.13708182 0.1523941 0.14239812 0.11921779 0.099012405 0.092386141 0.095820494 0.10182 0.10324591 0.098164864 0.089443006 0.079363056 0.075126171 0.080411777][0.093859144 0.13127184 0.15070637 0.14708264 0.1293788 0.11067256 0.10054947 0.097432606 0.096883595 0.094576515 0.0892769 0.082447954 0.0730966 0.065684564 0.0637436][0.089562856 0.12378806 0.14325354 0.14325166 0.13035908 0.11427549 0.10324274 0.0974606 0.094619013 0.091641895 0.086679526 0.079802454 0.068122141 0.055139728 0.045126695]]...]
INFO - root - 2017-12-10 12:34:15.384289: step 15210, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 69h:05m:28s remains)
INFO - root - 2017-12-10 12:34:23.127645: step 15220, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 68h:33m:21s remains)
INFO - root - 2017-12-10 12:34:30.982082: step 15230, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 67h:46m:39s remains)
INFO - root - 2017-12-10 12:34:38.763313: step 15240, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 69h:37m:57s remains)
INFO - root - 2017-12-10 12:34:46.705819: step 15250, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 69h:03m:27s remains)
INFO - root - 2017-12-10 12:34:54.582340: step 15260, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 67h:28m:44s remains)
INFO - root - 2017-12-10 12:35:02.344207: step 15270, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.758 sec/batch; 66h:47m:59s remains)
INFO - root - 2017-12-10 12:35:10.075772: step 15280, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 70h:35m:14s remains)
INFO - root - 2017-12-10 12:35:17.876482: step 15290, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 71h:17m:20s remains)
INFO - root - 2017-12-10 12:35:25.828774: step 15300, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 68h:44m:26s remains)
2017-12-10 12:35:26.675940: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27635562 0.2664099 0.25207305 0.23554283 0.21151322 0.18553849 0.1647269 0.148396 0.14059888 0.15017127 0.19018069 0.25227588 0.3281337 0.40716621 0.47455284][0.18266355 0.18901798 0.20153132 0.21570596 0.21798922 0.20875798 0.19625099 0.18471567 0.1829707 0.20295316 0.26228231 0.34737068 0.43705928 0.50947553 0.55231535][0.094235271 0.12612979 0.17348395 0.22459151 0.25743306 0.26711172 0.26380128 0.25854462 0.26222628 0.28625661 0.35161537 0.4431797 0.5284887 0.57612485 0.57903385][0.047943842 0.10942745 0.19294718 0.2820884 0.34639269 0.3747744 0.37871724 0.37819949 0.38415441 0.40352333 0.45752093 0.53359294 0.59498376 0.60731268 0.567515][0.04523804 0.13233767 0.24593869 0.36848506 0.46243915 0.50933695 0.51988941 0.52381957 0.53126693 0.54287457 0.57452136 0.61698073 0.63765568 0.60806662 0.52908891][0.067009404 0.16959734 0.30173811 0.44819534 0.56787217 0.63481861 0.65445894 0.66256404 0.66927689 0.67212456 0.67738622 0.67748117 0.651397 0.58281446 0.47551477][0.091836475 0.20050751 0.34078461 0.50033265 0.6394037 0.72632611 0.75696063 0.76637495 0.767574 0.76163411 0.74627638 0.71309608 0.65310061 0.56121045 0.44157124][0.12011667 0.2286409 0.36752585 0.525327 0.66787761 0.76349211 0.79974514 0.8063311 0.80095649 0.79230821 0.77237314 0.72863615 0.65777504 0.56245869 0.44570354][0.15931877 0.26185653 0.38960078 0.52929634 0.65325445 0.73684877 0.76682156 0.76727682 0.76010042 0.76042169 0.75555086 0.72474712 0.66349506 0.58093226 0.48001137][0.21738176 0.30696058 0.41369107 0.52066612 0.60515058 0.65527761 0.66646928 0.66004068 0.65951061 0.68163741 0.70785767 0.70551783 0.66576713 0.6054489 0.52941585][0.28894606 0.35533854 0.42866939 0.49164852 0.5256654 0.5320037 0.5191887 0.50664794 0.51603484 0.56198466 0.62013513 0.64644033 0.62887746 0.59482676 0.55111808][0.35254487 0.38645655 0.41726637 0.43373415 0.4215301 0.39071852 0.36007336 0.34520718 0.3623625 0.422848 0.49927986 0.54196191 0.53918767 0.52889657 0.51835626][0.37323359 0.37338936 0.36334169 0.34170797 0.29866362 0.24756089 0.21032523 0.19659027 0.21552964 0.27639416 0.35277855 0.39719 0.40124393 0.40829548 0.42450863][0.3253457 0.30083349 0.26472864 0.22280149 0.169254 0.11582315 0.080938056 0.068769567 0.082921632 0.13191277 0.19422758 0.23143332 0.23931861 0.25806215 0.29066318][0.22482115 0.19182162 0.15094858 0.1089254 0.062433936 0.018832218 -0.0098902816 -0.022427278 -0.017903706 0.011387244 0.05130874 0.076211661 0.084739223 0.10708729 0.14289854]]...]
INFO - root - 2017-12-10 12:35:34.404907: step 15310, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 67h:47m:06s remains)
INFO - root - 2017-12-10 12:35:42.330216: step 15320, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 71h:01m:05s remains)
INFO - root - 2017-12-10 12:35:50.199835: step 15330, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 68h:35m:34s remains)
INFO - root - 2017-12-10 12:35:58.065725: step 15340, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 70h:07m:54s remains)
INFO - root - 2017-12-10 12:36:05.874319: step 15350, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 69h:01m:17s remains)
INFO - root - 2017-12-10 12:36:13.649549: step 15360, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.766 sec/batch; 67h:28m:14s remains)
INFO - root - 2017-12-10 12:36:21.632787: step 15370, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 69h:20m:00s remains)
INFO - root - 2017-12-10 12:36:29.435024: step 15380, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 67h:41m:29s remains)
INFO - root - 2017-12-10 12:36:37.401872: step 15390, loss = 0.69, batch loss = 0.64 (9.6 examples/sec; 0.837 sec/batch; 73h:44m:54s remains)
INFO - root - 2017-12-10 12:36:45.154819: step 15400, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.791 sec/batch; 69h:42m:35s remains)
2017-12-10 12:36:46.007859: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.057621785 0.044017557 0.029280446 0.018179677 0.014104172 0.018657152 0.029031985 0.03853438 0.040657628 0.034297127 0.023322526 0.010490659 -0.0045200665 -0.020885987 -0.037126627][0.15240298 0.13191079 0.10992427 0.095447481 0.094143286 0.10599485 0.12247394 0.13282397 0.12984227 0.1157075 0.097312331 0.078292057 0.057332128 0.033590209 0.0073974612][0.25285175 0.23026726 0.20865229 0.19847012 0.20565768 0.22619328 0.24547961 0.25028425 0.23573975 0.20988904 0.18303907 0.15945104 0.13478719 0.10541735 0.069637544][0.32974118 0.30968747 0.29356372 0.2911959 0.30715925 0.33332089 0.35126275 0.34689215 0.31889796 0.2823976 0.25107765 0.2285001 0.20572187 0.17586921 0.13452555][0.38274127 0.36433142 0.35109004 0.3533459 0.37527886 0.40682685 0.42736965 0.4201805 0.38490441 0.34232894 0.30938074 0.28883806 0.2682282 0.23860012 0.19324231][0.42823538 0.40777859 0.38973892 0.3903788 0.41662365 0.45949289 0.49494785 0.49864328 0.46642089 0.42182249 0.38530123 0.36022529 0.33424571 0.29901281 0.24657789][0.46232402 0.43967098 0.41524139 0.41362348 0.44456404 0.5009045 0.55497485 0.57287025 0.54449207 0.49489993 0.44984347 0.41530931 0.38050309 0.33882791 0.28159243][0.47596255 0.45675874 0.43308896 0.43578681 0.47341228 0.538835 0.6028567 0.62478703 0.59110653 0.52842325 0.46986046 0.42481858 0.38314703 0.33954474 0.28402308][0.47117734 0.46051931 0.44555873 0.45881733 0.50264841 0.56826931 0.62920827 0.64468747 0.60135978 0.52626896 0.45758781 0.40522459 0.35885456 0.31444705 0.2616632][0.46818778 0.47132403 0.47003508 0.49370077 0.5366928 0.5911867 0.63814437 0.64307731 0.59473574 0.51702005 0.44756809 0.39260548 0.34187448 0.29328662 0.2383257][0.47482878 0.49480867 0.5069896 0.53257757 0.56073785 0.58978337 0.612969 0.60676885 0.56226784 0.4958142 0.43873993 0.39045468 0.3396852 0.28617486 0.22511739][0.47841066 0.51444262 0.53661978 0.55667412 0.56271023 0.56076789 0.55718362 0.54031008 0.50376189 0.45588124 0.41945735 0.38639161 0.34182274 0.28609273 0.21871161][0.47633898 0.52010572 0.54591066 0.55835128 0.54636264 0.52106911 0.49793783 0.47491395 0.44909009 0.42315039 0.41102558 0.39781541 0.363216 0.30671003 0.23165855][0.4782494 0.51783961 0.54015321 0.54539222 0.52345544 0.4864459 0.454443 0.432613 0.42266503 0.4236061 0.4392966 0.44859225 0.42520991 0.36680159 0.2805005][0.49753755 0.52584147 0.53998387 0.54055017 0.5166288 0.47795603 0.44604561 0.43065259 0.43869656 0.46632144 0.50681394 0.53331506 0.51480734 0.44852406 0.34624571]]...]
INFO - root - 2017-12-10 12:36:53.981972: step 15410, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 69h:49m:51s remains)
INFO - root - 2017-12-10 12:37:01.879582: step 15420, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.790 sec/batch; 69h:37m:01s remains)
INFO - root - 2017-12-10 12:37:09.770007: step 15430, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 70h:49m:51s remains)
INFO - root - 2017-12-10 12:37:17.463841: step 15440, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 67h:32m:05s remains)
INFO - root - 2017-12-10 12:37:25.303806: step 15450, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 69h:35m:29s remains)
INFO - root - 2017-12-10 12:37:33.220186: step 15460, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 70h:05m:15s remains)
INFO - root - 2017-12-10 12:37:41.172228: step 15470, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 70h:09m:07s remains)
INFO - root - 2017-12-10 12:37:48.841508: step 15480, loss = 0.68, batch loss = 0.62 (12.4 examples/sec; 0.643 sec/batch; 56h:35m:44s remains)
INFO - root - 2017-12-10 12:37:56.738652: step 15490, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 69h:32m:03s remains)
INFO - root - 2017-12-10 12:38:04.588789: step 15500, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 69h:13m:49s remains)
2017-12-10 12:38:05.432689: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.44924381 0.47609571 0.48758468 0.48924479 0.47964686 0.45184231 0.39507625 0.32018271 0.2564947 0.20728862 0.17510001 0.17191972 0.19746324 0.23462325 0.26716161][0.549494 0.56861538 0.55897832 0.53166 0.49480665 0.4465512 0.37739438 0.299663 0.24428955 0.21116589 0.19679239 0.20762968 0.24007176 0.278689 0.31119397][0.56422025 0.56851941 0.53807235 0.489842 0.441193 0.39325109 0.33658537 0.27923262 0.24639265 0.23460002 0.23582207 0.25227612 0.27867341 0.3052972 0.32835215][0.49194577 0.48363438 0.44644985 0.40102133 0.37150845 0.35701346 0.34287754 0.32599917 0.31988317 0.32138869 0.32328954 0.32718897 0.32794926 0.32405886 0.32236773][0.36035821 0.35561305 0.33920813 0.32947761 0.35261947 0.39961746 0.44576964 0.47305852 0.4813824 0.47329056 0.44949797 0.41624153 0.37143031 0.32200909 0.28344762][0.21618597 0.23983218 0.27384865 0.32554787 0.41722712 0.52993822 0.630176 0.68746966 0.69008476 0.649495 0.57855046 0.49082962 0.38912359 0.28831261 0.20969328][0.1034392 0.17350891 0.27182448 0.38895091 0.53904033 0.69651514 0.82489407 0.88875055 0.8699683 0.78787851 0.66578823 0.52321059 0.36902669 0.22529116 0.11584421][0.051488664 0.16899513 0.32237473 0.48477182 0.66112322 0.82510114 0.94395947 0.98665947 0.93618381 0.81644952 0.65575027 0.47554532 0.29116198 0.12922308 0.012093781][0.061775364 0.21314895 0.39645159 0.57183033 0.73615253 0.86720026 0.94120044 0.93864667 0.85116744 0.70612055 0.53219414 0.34673795 0.16702951 0.019337419 -0.077983737][0.11967095 0.288007 0.4759692 0.63738286 0.76236749 0.83567458 0.84483153 0.78458154 0.660974 0.50682658 0.34932706 0.19646427 0.058514711 -0.04618071 -0.10460938][0.20568523 0.38122755 0.56092286 0.69595295 0.77114922 0.77953672 0.720875 0.605 0.45525211 0.31165814 0.19691807 0.10538175 0.032153703 -0.017218294 -0.035613131][0.30465218 0.48253822 0.64925176 0.75520962 0.7816751 0.73076946 0.61570233 0.46140963 0.30593374 0.19310318 0.1371692 0.11738816 0.10975354 0.10622797 0.1108781][0.39307815 0.56434184 0.71184862 0.7877602 0.77219683 0.67671281 0.52945095 0.36759666 0.2342782 0.16888286 0.17414206 0.21609369 0.25436169 0.271935 0.27549067][0.42759851 0.5772171 0.69712931 0.74439812 0.70093179 0.58527017 0.43747646 0.30063403 0.21199369 0.19927241 0.25325513 0.33177787 0.38647315 0.39872593 0.38183084][0.37510714 0.48799124 0.57331228 0.59619665 0.54322684 0.43469051 0.31409413 0.22190279 0.18322593 0.21225731 0.2915757 0.37829861 0.42585644 0.41954279 0.37852198]]...]
INFO - root - 2017-12-10 12:38:13.154775: step 15510, loss = 0.69, batch loss = 0.63 (12.7 examples/sec; 0.630 sec/batch; 55h:30m:19s remains)
INFO - root - 2017-12-10 12:38:21.063259: step 15520, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 68h:57m:40s remains)
INFO - root - 2017-12-10 12:38:28.966065: step 15530, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 69h:04m:50s remains)
INFO - root - 2017-12-10 12:38:36.926305: step 15540, loss = 0.71, batch loss = 0.66 (9.8 examples/sec; 0.817 sec/batch; 71h:56m:55s remains)
INFO - root - 2017-12-10 12:38:45.861841: step 15550, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.990 sec/batch; 87h:08m:23s remains)
INFO - root - 2017-12-10 12:38:55.938151: step 15560, loss = 0.71, batch loss = 0.65 (7.6 examples/sec; 1.053 sec/batch; 92h:42m:30s remains)
INFO - root - 2017-12-10 12:39:05.819285: step 15570, loss = 0.70, batch loss = 0.64 (8.3 examples/sec; 0.963 sec/batch; 84h:46m:55s remains)
INFO - root - 2017-12-10 12:39:15.878232: step 15580, loss = 0.72, batch loss = 0.66 (7.7 examples/sec; 1.039 sec/batch; 91h:27m:26s remains)
INFO - root - 2017-12-10 12:39:25.824515: step 15590, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.010 sec/batch; 88h:56m:26s remains)
INFO - root - 2017-12-10 12:39:35.651590: step 15600, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 1.005 sec/batch; 88h:28m:49s remains)
2017-12-10 12:39:36.648197: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.045847684 0.067394182 0.096138664 0.12035888 0.12686281 0.11186034 0.081962369 0.051336993 0.03563682 0.040328745 0.054980166 0.057591226 0.036549225 -0.00021942235 -0.039776314][0.0628175 0.089807421 0.12447368 0.1527718 0.15990031 0.1410408 0.10077018 0.055188198 0.022896359 0.015284543 0.025433712 0.030381627 0.01618572 -0.012039397 -0.04315782][0.066479959 0.10208604 0.14728758 0.18503052 0.1986687 0.18221125 0.13835661 0.082988761 0.037020769 0.016782938 0.017887345 0.018453576 0.0043118745 -0.020101419 -0.045779385][0.056606576 0.10277188 0.16237538 0.21507601 0.24178444 0.23478004 0.19530708 0.13716236 0.082028158 0.0490708 0.036705881 0.025604501 0.0034195061 -0.023844533 -0.048246529][0.038348805 0.0932105 0.16630536 0.2349748 0.27790955 0.28550923 0.25792435 0.20501427 0.14732538 0.10358226 0.075213552 0.047182955 0.010477162 -0.025416428 -0.052794967][0.015830159 0.075518042 0.15676902 0.23686016 0.29393429 0.31745577 0.30709752 0.26784053 0.21654075 0.16804466 0.12597847 0.080064 0.025600411 -0.023127161 -0.0573173][-0.0049510845 0.054452587 0.13624343 0.2191927 0.28300941 0.31822687 0.32487026 0.3038446 0.26685822 0.22246 0.17419073 0.11513262 0.045001015 -0.016876787 -0.059259225][-0.017935853 0.037695542 0.11331005 0.18982992 0.24993217 0.28714779 0.30381751 0.29889339 0.27918369 0.24634863 0.20115934 0.13778237 0.058901072 -0.012376126 -0.061391994][-0.019063359 0.031429768 0.097437643 0.16131425 0.20846108 0.23577586 0.24951363 0.25024638 0.24240598 0.22248605 0.1874032 0.13007654 0.053358644 -0.018729482 -0.069235094][-0.0094086919 0.036701348 0.0935359 0.14404316 0.17442314 0.18398817 0.18341127 0.17739271 0.17123069 0.16016026 0.13810682 0.095333271 0.031491015 -0.032529015 -0.079434469][0.0051646004 0.04959907 0.10130022 0.1427045 0.15889579 0.15012643 0.13067204 0.11021349 0.097223058 0.0895716 0.0800663 0.055531237 0.010034444 -0.041716661 -0.083345681][0.02160603 0.069495879 0.12297674 0.16248704 0.17067485 0.14751701 0.1097348 0.071518026 0.045887597 0.036010459 0.035301987 0.027046915 -0.0011939431 -0.041752908 -0.079456426][0.045146212 0.10266477 0.16505302 0.2094582 0.21579513 0.18299864 0.12924272 0.072749823 0.031738635 0.01533487 0.01787091 0.019548928 0.0023495066 -0.031751759 -0.068638526][0.07622467 0.14738157 0.22216508 0.27469906 0.2831578 0.2453348 0.17974965 0.10790811 0.053375006 0.029869847 0.03176697 0.036756106 0.023134351 -0.010473248 -0.050553691][0.10767072 0.19219682 0.27872679 0.33891231 0.35039157 0.31049725 0.237377 0.15535106 0.092495278 0.064585969 0.064935654 0.0694472 0.054610424 0.017327774 -0.029049395]]...]
INFO - root - 2017-12-10 12:39:46.766527: step 15610, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.989 sec/batch; 87h:02m:50s remains)
INFO - root - 2017-12-10 12:39:56.820198: step 15620, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.034 sec/batch; 91h:01m:01s remains)
INFO - root - 2017-12-10 12:40:06.844487: step 15630, loss = 0.70, batch loss = 0.65 (8.1 examples/sec; 0.985 sec/batch; 86h:42m:12s remains)
INFO - root - 2017-12-10 12:40:16.803558: step 15640, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.994 sec/batch; 87h:26m:41s remains)
INFO - root - 2017-12-10 12:40:26.804518: step 15650, loss = 0.70, batch loss = 0.65 (8.4 examples/sec; 0.954 sec/batch; 83h:55m:22s remains)
INFO - root - 2017-12-10 12:40:36.793631: step 15660, loss = 0.71, batch loss = 0.65 (8.2 examples/sec; 0.981 sec/batch; 86h:20m:50s remains)
INFO - root - 2017-12-10 12:40:46.692307: step 15670, loss = 0.72, batch loss = 0.66 (9.2 examples/sec; 0.871 sec/batch; 76h:41m:06s remains)
INFO - root - 2017-12-10 12:40:56.753257: step 15680, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.002 sec/batch; 88h:10m:03s remains)
INFO - root - 2017-12-10 12:41:06.790068: step 15690, loss = 0.71, batch loss = 0.65 (8.4 examples/sec; 0.955 sec/batch; 84h:02m:12s remains)
INFO - root - 2017-12-10 12:41:16.809050: step 15700, loss = 0.69, batch loss = 0.64 (8.1 examples/sec; 0.983 sec/batch; 86h:31m:35s remains)
2017-12-10 12:41:17.789415: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14559886 0.12008896 0.07580407 0.032522503 0.015305341 0.022235269 0.03975473 0.055467192 0.065418534 0.066457853 0.052378345 0.022470059 -0.012965908 -0.040097527 -0.051876146][0.16446765 0.14964174 0.11716199 0.084806345 0.077385388 0.09110041 0.10954913 0.11999213 0.12024672 0.10949958 0.082886465 0.041092537 -0.0028723527 -0.034343269 -0.047665324][0.18302278 0.181233 0.16482337 0.15078568 0.16147761 0.18871744 0.21308854 0.22134894 0.2121738 0.18635768 0.14159314 0.082362846 0.025617069 -0.014167412 -0.034722328][0.21367618 0.22445735 0.22272027 0.22768599 0.25823507 0.3000941 0.33263752 0.34305844 0.32890713 0.29149848 0.23139754 0.15737067 0.087978691 0.036036603 0.0005614853][0.26241124 0.28473842 0.29416537 0.31462288 0.36040983 0.41117406 0.44816855 0.4601734 0.44279408 0.39875948 0.3320291 0.25323084 0.17692126 0.11230467 0.057621945][0.31762931 0.35219204 0.37100288 0.40395749 0.46154228 0.51765871 0.55570316 0.56568128 0.54411817 0.49823093 0.43386161 0.3588002 0.27819911 0.19914706 0.12382472][0.35699856 0.40142056 0.42719728 0.47019705 0.539232 0.60198623 0.6412707 0.64757586 0.62316096 0.58114868 0.52583289 0.45837024 0.37432289 0.28226259 0.19147985][0.3756246 0.42217091 0.44842622 0.49566478 0.57231528 0.64005548 0.67955691 0.68247133 0.65861487 0.62506205 0.58191335 0.52333373 0.43871871 0.34081271 0.24681778][0.37500086 0.41454574 0.43430403 0.47766909 0.5520646 0.61685103 0.65190506 0.65172082 0.63180786 0.60934877 0.578628 0.52900553 0.44933155 0.3566466 0.27391157][0.35783163 0.38731503 0.39721975 0.42837802 0.48676807 0.53632647 0.55931544 0.554072 0.53863555 0.52706838 0.50796109 0.468116 0.400462 0.3237496 0.26179487][0.32429811 0.34647512 0.34881243 0.36559603 0.40113464 0.42849341 0.43531308 0.42363155 0.41137311 0.40749231 0.39675954 0.36645284 0.31504279 0.25898361 0.21841253][0.27570885 0.2925787 0.29065028 0.29575065 0.30897745 0.31284928 0.30163905 0.28078288 0.26680014 0.2646766 0.25973934 0.24156883 0.2122837 0.18186621 0.16306929][0.21272498 0.22541231 0.22269446 0.22170404 0.21879359 0.20347199 0.17534396 0.14312732 0.12261537 0.11829878 0.11892541 0.11531474 0.10954384 0.1036228 0.10290986][0.12736313 0.13724916 0.13729329 0.13559923 0.12398188 0.096615911 0.057034746 0.016611677 -0.00895713 -0.013903337 -0.0063363039 0.0035462934 0.01574659 0.026261296 0.03496689][0.030694121 0.038798977 0.042949993 0.042843964 0.02827373 -0.0032071448 -0.045616619 -0.086947441 -0.11247306 -0.11595491 -0.10326894 -0.08554244 -0.065235533 -0.049097944 -0.039960753]]...]
INFO - root - 2017-12-10 12:41:27.921765: step 15710, loss = 0.69, batch loss = 0.63 (7.6 examples/sec; 1.057 sec/batch; 92h:58m:49s remains)
INFO - root - 2017-12-10 12:41:37.983116: step 15720, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.008 sec/batch; 88h:43m:21s remains)
INFO - root - 2017-12-10 12:41:48.111093: step 15730, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.028 sec/batch; 90h:28m:50s remains)
INFO - root - 2017-12-10 12:41:58.134224: step 15740, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 0.995 sec/batch; 87h:30m:57s remains)
INFO - root - 2017-12-10 12:42:07.834871: step 15750, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 0.996 sec/batch; 87h:37m:10s remains)
INFO - root - 2017-12-10 12:42:17.902893: step 15760, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.009 sec/batch; 88h:47m:59s remains)
INFO - root - 2017-12-10 12:42:28.102471: step 15770, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.019 sec/batch; 89h:39m:09s remains)
INFO - root - 2017-12-10 12:42:38.101025: step 15780, loss = 0.70, batch loss = 0.64 (8.3 examples/sec; 0.969 sec/batch; 85h:15m:07s remains)
INFO - root - 2017-12-10 12:42:48.156667: step 15790, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 0.997 sec/batch; 87h:43m:08s remains)
INFO - root - 2017-12-10 12:42:58.203334: step 15800, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.032 sec/batch; 90h:48m:45s remains)
2017-12-10 12:42:59.136040: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.3289634 0.33273447 0.31205654 0.26962748 0.21405463 0.16756207 0.1480954 0.16275242 0.20144995 0.25253305 0.30384424 0.33867732 0.34553728 0.33210236 0.31305954][0.26039359 0.26289925 0.24205421 0.20131941 0.15200508 0.11831509 0.11695783 0.15129001 0.20723097 0.26567671 0.30652878 0.31519538 0.28857976 0.24457395 0.20555715][0.19921046 0.20154779 0.18253487 0.14860177 0.11377567 0.10334131 0.12948933 0.18907061 0.261537 0.32041034 0.339917 0.30946019 0.23888437 0.15897724 0.1012613][0.16688696 0.16910221 0.15272388 0.12914857 0.11493801 0.13451079 0.19362003 0.28099757 0.36705792 0.41892964 0.40912679 0.33414841 0.21820472 0.10494743 0.03382][0.15967387 0.16201144 0.14985661 0.14010972 0.14979169 0.2005077 0.29036132 0.39924434 0.48855931 0.522684 0.47875974 0.36414096 0.21522717 0.083687566 0.01125779][0.17731188 0.18027623 0.17426971 0.18024276 0.21277258 0.28812683 0.39647779 0.51009935 0.58580559 0.59082484 0.5132013 0.37302503 0.21329623 0.085356325 0.025706895][0.2194761 0.22534035 0.22724591 0.24791664 0.29605526 0.3803522 0.48509076 0.57929 0.6229291 0.592292 0.48984256 0.34514689 0.19952677 0.095660761 0.059276547][0.27525857 0.28572369 0.29544806 0.32511067 0.37604031 0.44911575 0.52683842 0.58153921 0.58440077 0.52402121 0.41496763 0.28863174 0.17697871 0.10950587 0.099086232][0.31990403 0.3373279 0.35383978 0.38529187 0.4264017 0.47217605 0.5080573 0.515701 0.48191732 0.40708458 0.31136465 0.21984953 0.15192628 0.12379487 0.13648686][0.33566609 0.35873494 0.37879252 0.40565145 0.43012193 0.44465533 0.44044423 0.41056451 0.35599977 0.28489894 0.21636336 0.16598473 0.14133747 0.14797869 0.17861041][0.31763521 0.34286764 0.36321172 0.38311455 0.39236757 0.38392624 0.3545785 0.3061446 0.24798653 0.19215788 0.15400361 0.14103436 0.15278803 0.18699747 0.23153663][0.26781556 0.29171789 0.31168187 0.32798886 0.33109689 0.31530926 0.28017318 0.23094627 0.17995201 0.14123863 0.12767003 0.14222577 0.18028285 0.23412085 0.28931531][0.19634867 0.21635576 0.23650406 0.25423667 0.26093462 0.25124961 0.22446676 0.18534809 0.14593609 0.12158539 0.12493636 0.15740614 0.21278422 0.2790094 0.34033376][0.1213617 0.13676526 0.15727812 0.17809373 0.19087335 0.18998682 0.17384055 0.14630821 0.11793777 0.10439187 0.11881787 0.16226131 0.22765641 0.2991876 0.36040205][0.056387242 0.067904666 0.087628283 0.10870004 0.12348765 0.12683092 0.11731005 0.098654442 0.079326637 0.073696382 0.094339684 0.1416925 0.20807831 0.27560073 0.32884726]]...]
INFO - root - 2017-12-10 12:43:09.203867: step 15810, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.010 sec/batch; 88h:50m:25s remains)
INFO - root - 2017-12-10 12:43:19.277112: step 15820, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.013 sec/batch; 89h:06m:55s remains)
INFO - root - 2017-12-10 12:43:29.103180: step 15830, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.986 sec/batch; 86h:45m:26s remains)
INFO - root - 2017-12-10 12:43:38.923825: step 15840, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.014 sec/batch; 89h:13m:25s remains)
INFO - root - 2017-12-10 12:43:48.915052: step 15850, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 0.995 sec/batch; 87h:30m:00s remains)
INFO - root - 2017-12-10 12:43:59.079723: step 15860, loss = 0.69, batch loss = 0.64 (7.9 examples/sec; 1.016 sec/batch; 89h:22m:55s remains)
INFO - root - 2017-12-10 12:44:09.211413: step 15870, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.986 sec/batch; 86h:42m:58s remains)
INFO - root - 2017-12-10 12:44:19.300308: step 15880, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.018 sec/batch; 89h:30m:17s remains)
INFO - root - 2017-12-10 12:44:29.374242: step 15890, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.988 sec/batch; 86h:54m:46s remains)
INFO - root - 2017-12-10 12:44:39.427817: step 15900, loss = 0.71, batch loss = 0.65 (7.5 examples/sec; 1.060 sec/batch; 93h:13m:36s remains)
2017-12-10 12:44:40.484310: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30735195 0.30861384 0.2960332 0.27872685 0.25455815 0.22550404 0.19229355 0.15679561 0.13042176 0.12705679 0.14342962 0.16297825 0.17246328 0.16237354 0.13169006][0.35511303 0.34947273 0.318355 0.27600333 0.22960103 0.1905963 0.1645195 0.14971882 0.15122727 0.17394425 0.20796086 0.23482142 0.24223834 0.2231352 0.17904446][0.34501314 0.33622745 0.29675713 0.24337302 0.19026905 0.15516272 0.14564021 0.15618768 0.1852759 0.23079139 0.27704573 0.30513927 0.3062008 0.27716815 0.22237949][0.27468213 0.267168 0.23407832 0.1911442 0.15442637 0.14126895 0.15646552 0.18990459 0.23608321 0.28884482 0.33142823 0.34885249 0.3373071 0.29829606 0.23784737][0.15841447 0.15980335 0.14944343 0.13932391 0.14337617 0.17133918 0.21883662 0.27029279 0.31787023 0.35532975 0.3716296 0.36154389 0.32891694 0.27873343 0.21703206][0.030841149 0.04427588 0.064751491 0.099595182 0.15820085 0.23816185 0.32244313 0.3885808 0.42503205 0.42825174 0.39910758 0.3471216 0.2852509 0.22212367 0.16316451][-0.052897844 -0.02940484 0.018908609 0.095468268 0.2039136 0.32956123 0.4441171 0.51797163 0.53483039 0.49571639 0.41542321 0.31792268 0.22419919 0.14742254 0.092967428][-0.050363969 -0.023117265 0.038126558 0.13593256 0.26983333 0.41758987 0.54497784 0.61670035 0.61200649 0.53690052 0.41696003 0.28568661 0.1693583 0.085476182 0.039592821][0.033960495 0.056447428 0.10925009 0.19724788 0.32028383 0.45640975 0.57301992 0.63294619 0.61253679 0.51967305 0.38480765 0.24425538 0.12567621 0.048271008 0.017019784][0.14463291 0.15655988 0.18523997 0.23824364 0.31976464 0.41610575 0.50337213 0.54697096 0.52120924 0.43244642 0.30929279 0.1854555 0.087023549 0.030943071 0.019607827][0.21304052 0.20970336 0.20987242 0.22170472 0.25298131 0.30083016 0.35340852 0.38190779 0.35970852 0.289224 0.19405502 0.10326326 0.038684543 0.011919121 0.021070374][0.20009528 0.18249285 0.16290468 0.14715002 0.14238034 0.15295796 0.1771577 0.19341472 0.17784601 0.12950791 0.066406973 0.012229535 -0.016507184 -0.014836556 0.010297677][0.11398467 0.08811719 0.063243672 0.040713325 0.024078203 0.018837532 0.027782282 0.035996061 0.025898442 -0.0035606881 -0.038557772 -0.060520388 -0.058771517 -0.0348675 -8.47702e-05][0.013902531 -0.01516669 -0.038672313 -0.057659704 -0.07129956 -0.076576896 -0.07161504 -0.0673536 -0.074486151 -0.091370642 -0.1064067 -0.10556815 -0.083745718 -0.047325265 -0.0097537721][-0.055039965 -0.083194733 -0.10293306 -0.11611619 -0.12280097 -0.12249103 -0.11580233 -0.11186422 -0.11695507 -0.12690303 -0.13134803 -0.11983754 -0.091092825 -0.0531455 -0.018846396]]...]
INFO - root - 2017-12-10 12:44:50.364728: step 15910, loss = 0.71, batch loss = 0.65 (8.2 examples/sec; 0.971 sec/batch; 85h:21m:53s remains)
INFO - root - 2017-12-10 12:45:00.442947: step 15920, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.011 sec/batch; 88h:54m:02s remains)
INFO - root - 2017-12-10 12:45:10.334618: step 15930, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.025 sec/batch; 90h:08m:33s remains)
INFO - root - 2017-12-10 12:45:20.431908: step 15940, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.009 sec/batch; 88h:44m:53s remains)
INFO - root - 2017-12-10 12:45:30.434762: step 15950, loss = 0.69, batch loss = 0.64 (8.2 examples/sec; 0.979 sec/batch; 86h:06m:51s remains)
INFO - root - 2017-12-10 12:45:40.477191: step 15960, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 0.999 sec/batch; 87h:51m:02s remains)
INFO - root - 2017-12-10 12:45:50.494325: step 15970, loss = 0.69, batch loss = 0.64 (7.8 examples/sec; 1.027 sec/batch; 90h:17m:42s remains)
INFO - root - 2017-12-10 12:46:00.656812: step 15980, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 0.997 sec/batch; 87h:37m:10s remains)
INFO - root - 2017-12-10 12:46:10.335323: step 15990, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.001 sec/batch; 88h:01m:21s remains)
INFO - root - 2017-12-10 12:46:20.435563: step 16000, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.041 sec/batch; 91h:33m:48s remains)
2017-12-10 12:46:21.445327: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22964762 0.19736317 0.16789444 0.14932443 0.14009282 0.13811558 0.14540988 0.1602848 0.18193327 0.22318517 0.27839258 0.33082429 0.36330423 0.38168621 0.39588052][0.263247 0.22860418 0.19546908 0.17475946 0.16334929 0.15720053 0.15806632 0.16695477 0.18665913 0.22810462 0.28491068 0.33811069 0.36767992 0.37954286 0.38625541][0.28110531 0.25170305 0.22509204 0.21297522 0.20816617 0.2042339 0.20161277 0.20390138 0.21641988 0.24829036 0.2950573 0.33858094 0.35904494 0.36196488 0.3611857][0.28643867 0.26519322 0.25053588 0.25279531 0.26113707 0.26821449 0.27142233 0.27369502 0.27915329 0.29565105 0.32327265 0.34819242 0.35448006 0.34806913 0.34251595][0.29269761 0.27866471 0.27300924 0.28621718 0.30725485 0.33069587 0.34964964 0.36175355 0.36507979 0.36571255 0.3698855 0.37083638 0.36006323 0.34589079 0.33990374][0.32064447 0.30807447 0.30057353 0.31361249 0.34046036 0.37955424 0.4194015 0.44870332 0.45629531 0.44597495 0.42949691 0.40844846 0.38307008 0.36488819 0.36158124][0.38529754 0.36548707 0.34150296 0.33938521 0.36083516 0.40828866 0.46579885 0.51215065 0.52815068 0.51519156 0.48717031 0.45238072 0.41825059 0.39900744 0.39818454][0.4779731 0.44355872 0.39285192 0.36578706 0.37361443 0.4202342 0.48549643 0.53986651 0.56075752 0.54833758 0.51595056 0.47549289 0.43822694 0.41910416 0.41845295][0.56674653 0.51601493 0.43868631 0.38727492 0.37997732 0.41846576 0.47915933 0.52797061 0.54366863 0.52709526 0.49116457 0.44912836 0.41254282 0.3938804 0.39120805][0.60985333 0.54840922 0.45381087 0.38640192 0.36622602 0.39158225 0.43764502 0.47038376 0.47257778 0.44676653 0.40651909 0.36523831 0.33226886 0.31513777 0.3103821][0.57955652 0.51321852 0.41566125 0.34554836 0.31920072 0.33175752 0.35942838 0.37227657 0.35870674 0.3231276 0.28011462 0.24202831 0.2149168 0.20139347 0.19748542][0.47877297 0.41715735 0.3319861 0.2728487 0.24910349 0.25317302 0.26376814 0.25796956 0.23107155 0.18967216 0.14853582 0.11744084 0.098762251 0.091789685 0.093370765][0.34142873 0.29440257 0.23333466 0.19431558 0.17929059 0.17875868 0.175633 0.15544017 0.12057227 0.080063477 0.047949154 0.030033853 0.024407381 0.028033594 0.03921444][0.21402751 0.18827729 0.15440039 0.13506882 0.12786976 0.12390306 0.11161838 0.084458366 0.04990381 0.018153135 0.0011516056 0.00054153445 0.010819684 0.02765687 0.050406076][0.13159423 0.12573783 0.11288083 0.1052611 0.10041192 0.09324044 0.077629276 0.052430417 0.026683627 0.0094008828 0.0090965386 0.02408844 0.047802489 0.076723821 0.11060306]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 12:46:31.594989: step 16010, loss = 0.70, batch loss = 0.64 (8.2 examples/sec; 0.978 sec/batch; 85h:56m:49s remains)
INFO - root - 2017-12-10 12:46:41.574337: step 16020, loss = 0.69, batch loss = 0.63 (7.6 examples/sec; 1.055 sec/batch; 92h:46m:40s remains)
INFO - root - 2017-12-10 12:46:51.758009: step 16030, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.985 sec/batch; 86h:36m:00s remains)
INFO - root - 2017-12-10 12:47:01.928891: step 16040, loss = 0.69, batch loss = 0.64 (7.9 examples/sec; 1.007 sec/batch; 88h:30m:47s remains)
INFO - root - 2017-12-10 12:47:11.960371: step 16050, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.001 sec/batch; 88h:02m:02s remains)
INFO - root - 2017-12-10 12:47:21.935413: step 16060, loss = 0.71, batch loss = 0.65 (8.2 examples/sec; 0.971 sec/batch; 85h:22m:09s remains)
INFO - root - 2017-12-10 12:47:31.854934: step 16070, loss = 0.72, batch loss = 0.66 (7.9 examples/sec; 1.015 sec/batch; 89h:10m:28s remains)
INFO - root - 2017-12-10 12:47:41.984641: step 16080, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.011 sec/batch; 88h:51m:57s remains)
INFO - root - 2017-12-10 12:47:52.078574: step 16090, loss = 0.71, batch loss = 0.65 (8.2 examples/sec; 0.979 sec/batch; 86h:00m:11s remains)
INFO - root - 2017-12-10 12:48:02.196383: step 16100, loss = 0.67, batch loss = 0.62 (7.7 examples/sec; 1.036 sec/batch; 91h:02m:46s remains)
2017-12-10 12:48:03.195483: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022005526 0.036538154 0.051487409 0.065987661 0.076201305 0.074331947 0.05507458 0.023168279 -0.0093655689 -0.033929344 -0.044970371 -0.040186625 -0.021591054 0.0048413146 0.033911023][0.064106464 0.091948509 0.12323453 0.15818906 0.18953168 0.20184936 0.18272611 0.13592188 0.078814588 0.026709596 -0.0073603177 -0.014738766 0.0060447869 0.047951203 0.10045885][0.10628815 0.14910163 0.19953088 0.26044646 0.32231429 0.36080498 0.35336158 0.29928827 0.21835776 0.13205372 0.062924944 0.030734582 0.045161769 0.10087212 0.18160239][0.13354504 0.18737577 0.25274703 0.33626047 0.42904377 0.50042224 0.51784182 0.47286156 0.38155219 0.26638147 0.15777887 0.089020178 0.083537571 0.14360538 0.24919561][0.1387331 0.19648573 0.26743332 0.36233535 0.47709855 0.58022255 0.63271022 0.61633039 0.53659254 0.40918684 0.26694366 0.15615369 0.11782271 0.16776825 0.28646588][0.11699094 0.17033677 0.23717082 0.33243674 0.45953375 0.59134054 0.6860435 0.71476752 0.66734433 0.54558569 0.37887844 0.2244781 0.14310142 0.16770378 0.28287509][0.081953742 0.12340286 0.1770045 0.2617017 0.38972163 0.54119354 0.6746577 0.75189424 0.74655312 0.64344156 0.46438497 0.2739062 0.14954163 0.14031233 0.2364355][0.058685824 0.085796557 0.12053971 0.18515576 0.30071858 0.45603675 0.61324722 0.72795254 0.76089907 0.68186933 0.50394195 0.29382613 0.13802257 0.09508305 0.16135502][0.063867815 0.078768313 0.093376942 0.13238549 0.22443593 0.36709034 0.52809745 0.66037971 0.71760988 0.66008526 0.49372262 0.28313187 0.11503384 0.04889496 0.084239937][0.093210272 0.10096335 0.098182544 0.11100583 0.17378004 0.29132685 0.43731126 0.56530029 0.62821478 0.58418351 0.43531618 0.24147435 0.082424596 0.010520867 0.024070673][0.1236058 0.12855698 0.11322274 0.10353724 0.13643928 0.22182895 0.33923659 0.44623047 0.50009078 0.46232894 0.33427212 0.16959076 0.036678903 -0.02422482 -0.019090092][0.13384597 0.13861473 0.11634462 0.089886822 0.095136449 0.14518425 0.22585091 0.30208546 0.33929977 0.30595407 0.20309128 0.076086588 -0.020721974 -0.060150571 -0.052714489][0.11206763 0.11710762 0.093580544 0.058605365 0.043355174 0.062125906 0.10704239 0.15242685 0.17355151 0.14756207 0.074506663 -0.011525217 -0.071095571 -0.087895989 -0.074318856][0.06353981 0.067533724 0.047284935 0.013015985 -0.011463762 -0.01303377 0.0054255887 0.027341768 0.03714576 0.02026101 -0.024223417 -0.074218921 -0.10393895 -0.10440443 -0.0867533][0.0063739894 0.0074850316 -0.0073562586 -0.034475397 -0.058195978 -0.06851586 -0.065473869 -0.058312591 -0.055474244 -0.064800754 -0.087313376 -0.1109481 -0.12111027 -0.11329985 -0.095488816]]...]
INFO - root - 2017-12-10 12:48:13.060888: step 16110, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.040 sec/batch; 91h:26m:20s remains)
INFO - root - 2017-12-10 12:48:23.048553: step 16120, loss = 0.68, batch loss = 0.63 (8.3 examples/sec; 0.963 sec/batch; 84h:38m:31s remains)
INFO - root - 2017-12-10 12:48:33.104579: step 16130, loss = 0.70, batch loss = 0.65 (8.2 examples/sec; 0.976 sec/batch; 85h:48m:11s remains)
INFO - root - 2017-12-10 12:48:43.072647: step 16140, loss = 0.71, batch loss = 0.65 (8.2 examples/sec; 0.974 sec/batch; 85h:36m:58s remains)
INFO - root - 2017-12-10 12:48:53.037995: step 16150, loss = 0.72, batch loss = 0.66 (7.6 examples/sec; 1.046 sec/batch; 91h:54m:41s remains)
INFO - root - 2017-12-10 12:49:03.116435: step 16160, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.031 sec/batch; 90h:37m:35s remains)
INFO - root - 2017-12-10 12:49:13.399053: step 16170, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.006 sec/batch; 88h:24m:55s remains)
INFO - root - 2017-12-10 12:49:23.443550: step 16180, loss = 0.68, batch loss = 0.62 (8.1 examples/sec; 0.992 sec/batch; 87h:09m:39s remains)
INFO - root - 2017-12-10 12:49:33.496764: step 16190, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.004 sec/batch; 88h:14m:44s remains)
INFO - root - 2017-12-10 12:49:43.417558: step 16200, loss = 0.68, batch loss = 0.62 (7.7 examples/sec; 1.039 sec/batch; 91h:15m:25s remains)
2017-12-10 12:49:44.437193: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.099998556 0.11919039 0.12731647 0.13029967 0.1333462 0.13978401 0.14846526 0.15415399 0.15006875 0.13821842 0.13292256 0.13189128 0.12578401 0.11274455 0.10152896][0.0083203092 0.026449488 0.043698244 0.063489728 0.09288612 0.13085902 0.16784422 0.18975087 0.1852902 0.16243626 0.13354515 0.0989284 0.060751315 0.028511688 0.011939072][-0.04813271 -0.026544472 0.0058006747 0.048710063 0.10840282 0.1809105 0.24835913 0.28714371 0.2822732 0.24717334 0.19091822 0.11815362 0.042326953 -0.015268357 -0.043239921][-0.055282969 -0.020365143 0.03629107 0.10957853 0.20098729 0.30305195 0.39411837 0.44578546 0.44011486 0.3915433 0.3073965 0.19812407 0.086468391 0.0018769456 -0.042666979][-0.023327775 0.032267716 0.11693998 0.21992633 0.33615083 0.45536417 0.55766261 0.61528438 0.60816383 0.54790115 0.44256857 0.30866969 0.17388606 0.069544718 0.0088431854][0.02947136 0.10696267 0.2164041 0.3419916 0.47313598 0.59815615 0.70118743 0.7580651 0.74738526 0.67721677 0.56016016 0.41866231 0.279108 0.16807184 0.097054742][0.086795121 0.18378811 0.31184876 0.45151377 0.58959693 0.71348661 0.80912775 0.856553 0.83641618 0.75613868 0.63548994 0.50123733 0.37337247 0.26958749 0.19814931][0.12497579 0.23497707 0.37314853 0.51776677 0.65556973 0.77248627 0.85423273 0.88512295 0.85224986 0.7660265 0.653261 0.54055643 0.43846574 0.35530648 0.29570997][0.12798434 0.23862822 0.37435555 0.51193464 0.63902628 0.740732 0.80324519 0.81583828 0.77393591 0.69265419 0.602326 0.52352262 0.45819342 0.40787551 0.37357968][0.09964443 0.19461912 0.31164491 0.42690194 0.52910149 0.60573345 0.64633 0.64480311 0.60231215 0.53670818 0.47828969 0.43956456 0.41616723 0.40586984 0.40640283][0.048349489 0.11290844 0.19741662 0.27963284 0.34917858 0.39793545 0.4203434 0.41293439 0.37809408 0.33273733 0.30518112 0.30266935 0.31816933 0.34707478 0.38318017][-0.010465882 0.015533036 0.061158154 0.1082727 0.1468313 0.17199515 0.18226399 0.1745875 0.15020841 0.12327711 0.11927999 0.14339085 0.18799257 0.24639271 0.30803582][-0.052586336 -0.062084019 -0.051521171 -0.0331817 -0.018333158 -0.010683314 -0.0091731455 -0.016995113 -0.033672456 -0.047496591 -0.036416553 0.0034147713 0.063425072 0.13471554 0.20447373][-0.060879856 -0.094510764 -0.10827601 -0.10889011 -0.10930136 -0.11279981 -0.11720036 -0.12575491 -0.13794295 -0.14402956 -0.12609975 -0.082277678 -0.022121429 0.044255938 0.1048288][-0.027524233 -0.07116048 -0.097297676 -0.10758782 -0.11558899 -0.12392735 -0.1306074 -0.13923484 -0.1494306 -0.15313163 -0.13575569 -0.097349934 -0.048729923 3.1087879e-05 0.040046144]]...]
INFO - root - 2017-12-10 12:49:54.544843: step 16210, loss = 0.70, batch loss = 0.65 (8.0 examples/sec; 1.005 sec/batch; 88h:20m:03s remains)
INFO - root - 2017-12-10 12:50:04.708912: step 16220, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.982 sec/batch; 86h:14m:36s remains)
INFO - root - 2017-12-10 12:50:14.621649: step 16230, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.022 sec/batch; 89h:44m:45s remains)
INFO - root - 2017-12-10 12:50:24.679110: step 16240, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.007 sec/batch; 88h:29m:11s remains)
INFO - root - 2017-12-10 12:50:34.764209: step 16250, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.011 sec/batch; 88h:50m:12s remains)
INFO - root - 2017-12-10 12:50:44.775685: step 16260, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.037 sec/batch; 91h:04m:30s remains)
INFO - root - 2017-12-10 12:50:54.823923: step 16270, loss = 0.68, batch loss = 0.63 (7.9 examples/sec; 1.016 sec/batch; 89h:14m:40s remains)
INFO - root - 2017-12-10 12:51:04.898863: step 16280, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.019 sec/batch; 89h:29m:36s remains)
INFO - root - 2017-12-10 12:51:14.781417: step 16290, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.020 sec/batch; 89h:33m:52s remains)
INFO - root - 2017-12-10 12:51:24.843339: step 16300, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.984 sec/batch; 86h:27m:10s remains)
2017-12-10 12:51:25.768137: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10502911 0.1184935 0.12064757 0.11963883 0.1163865 0.10909665 0.094985344 0.076144561 0.063273989 0.066470049 0.085614324 0.11042894 0.12719357 0.12518927 0.10113795][0.18742521 0.20745203 0.20905495 0.20790955 0.2073798 0.20489259 0.19408958 0.17325248 0.15460049 0.15302713 0.17154896 0.2003884 0.2230285 0.2239438 0.19466645][0.26344976 0.28836903 0.2891086 0.288759 0.29388955 0.30096117 0.2988719 0.28015247 0.25565785 0.24476911 0.2555337 0.28189802 0.30790481 0.31385586 0.28420985][0.30733213 0.33457869 0.33544728 0.33866695 0.35280848 0.37234968 0.38100463 0.36590889 0.33576864 0.31307718 0.31093976 0.32822156 0.35299802 0.36432216 0.33950943][0.32016563 0.34770966 0.35220447 0.36432728 0.39205134 0.42577177 0.44369644 0.42928964 0.39083028 0.35322383 0.33419403 0.33836958 0.35912305 0.37652597 0.36165676][0.32992977 0.35432872 0.36039338 0.38076115 0.42287889 0.47209725 0.49948862 0.48466733 0.43668625 0.38232809 0.34364125 0.33176255 0.34673697 0.37075734 0.37001598][0.34484926 0.36303949 0.36569518 0.38858315 0.44049755 0.50292933 0.538531 0.52253038 0.4654178 0.39510193 0.33707267 0.30839479 0.31713736 0.34890181 0.36661425][0.36051014 0.37428483 0.37128234 0.39015889 0.44302198 0.51072896 0.54995227 0.53231627 0.46979317 0.39005342 0.31996039 0.27925983 0.28313348 0.3222267 0.35773379][0.36920258 0.38132086 0.37304696 0.3841022 0.42994496 0.49321628 0.52939618 0.50902629 0.44555733 0.36529228 0.29442194 0.25119478 0.25401065 0.29821667 0.34585446][0.35409951 0.36481795 0.35166824 0.35310641 0.38630879 0.43727025 0.46484795 0.44163278 0.38196561 0.30999455 0.24997672 0.21559696 0.22330765 0.27049881 0.32332295][0.30168042 0.30854967 0.29058194 0.28208998 0.30051893 0.33630449 0.35385662 0.32973388 0.27782398 0.21930131 0.17531687 0.15399353 0.16781108 0.21394315 0.2648322][0.20752108 0.20877409 0.18840906 0.17371583 0.18051985 0.20324595 0.21317914 0.19146919 0.14987834 0.10561335 0.076046437 0.065313652 0.082234494 0.12348311 0.1679105][0.09769579 0.093505949 0.074556991 0.059550792 0.060615 0.074746981 0.0807869 0.064663753 0.034979805 0.0047206194 -0.012974333 -0.016584024 -0.00046843532 0.032170735 0.06625817][0.005523636 -0.0030239068 -0.018578194 -0.030637503 -0.031816039 -0.023664474 -0.020039225 -0.030223733 -0.048710898 -0.066503853 -0.074619554 -0.073139317 -0.059395362 -0.036357384 -0.013680538][-0.061686818 -0.072943911 -0.084712513 -0.093227454 -0.095172249 -0.091209725 -0.089295305 -0.09462101 -0.10414701 -0.11254296 -0.11445858 -0.11044209 -0.099937886 -0.085314564 -0.071969852]]...]
INFO - root - 2017-12-10 12:51:35.856742: step 16310, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.032 sec/batch; 90h:39m:27s remains)
INFO - root - 2017-12-10 12:51:46.087240: step 16320, loss = 0.69, batch loss = 0.64 (7.6 examples/sec; 1.050 sec/batch; 92h:12m:11s remains)
INFO - root - 2017-12-10 12:51:56.237500: step 16330, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.030 sec/batch; 90h:25m:51s remains)
INFO - root - 2017-12-10 12:52:06.308896: step 16340, loss = 0.69, batch loss = 0.64 (8.0 examples/sec; 1.004 sec/batch; 88h:09m:33s remains)
INFO - root - 2017-12-10 12:52:16.276776: step 16350, loss = 0.71, batch loss = 0.65 (8.1 examples/sec; 0.982 sec/batch; 86h:12m:55s remains)
INFO - root - 2017-12-10 12:52:26.434602: step 16360, loss = 0.72, batch loss = 0.66 (7.6 examples/sec; 1.047 sec/batch; 91h:59m:14s remains)
INFO - root - 2017-12-10 12:52:36.493948: step 16370, loss = 0.70, batch loss = 0.64 (8.4 examples/sec; 0.958 sec/batch; 84h:07m:21s remains)
INFO - root - 2017-12-10 12:52:46.443580: step 16380, loss = 0.68, batch loss = 0.62 (7.9 examples/sec; 1.015 sec/batch; 89h:09m:45s remains)
INFO - root - 2017-12-10 12:52:56.372109: step 16390, loss = 0.68, batch loss = 0.63 (7.8 examples/sec; 1.020 sec/batch; 89h:34m:01s remains)
INFO - root - 2017-12-10 12:53:06.427135: step 16400, loss = 0.68, batch loss = 0.63 (7.5 examples/sec; 1.070 sec/batch; 93h:55m:51s remains)
2017-12-10 12:53:07.335306: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10981093 0.14433017 0.20167789 0.2851963 0.38049188 0.45116556 0.47572964 0.44851729 0.37554854 0.27060586 0.1571397 0.061167363 -0.0079882471 -0.051064562 -0.076285854][0.10066622 0.12785828 0.17476752 0.25176102 0.35059884 0.43339577 0.47262391 0.4584448 0.39711559 0.29913819 0.18516226 0.083213329 0.0078840069 -0.039058954 -0.068316177][0.12564814 0.14334472 0.17126901 0.2281642 0.31747243 0.40473971 0.45869824 0.46220285 0.41803077 0.33113697 0.21784006 0.10863883 0.025095297 -0.026803285 -0.059725154][0.20453237 0.21298113 0.21617667 0.2423536 0.31057477 0.39636654 0.46481717 0.48635492 0.45681161 0.37727866 0.26048133 0.13932166 0.04152498 -0.020081956 -0.057227138][0.32412669 0.32543695 0.30570564 0.30223742 0.34894794 0.4318752 0.51170623 0.54634511 0.52409691 0.44459414 0.31921473 0.18211964 0.065071024 -0.011968285 -0.057146654][0.44187567 0.44275427 0.41564021 0.402622 0.44434506 0.52977395 0.61425519 0.64753163 0.61448878 0.520622 0.38073063 0.22766677 0.091769263 -0.0034028855 -0.059801616][0.52115893 0.5235551 0.50310987 0.50061005 0.55203909 0.64253223 0.72308517 0.73972017 0.67914921 0.56042188 0.40660718 0.24620348 0.10236743 -0.0028186343 -0.065868273][0.5370729 0.54013693 0.53349745 0.55150515 0.61731392 0.71127796 0.7840293 0.7832182 0.69771135 0.55847329 0.39755976 0.23961288 0.09925998 -0.0065360186 -0.0709388][0.48805416 0.49540189 0.5090676 0.55208486 0.63079786 0.7218526 0.78138596 0.76763493 0.67191827 0.52755409 0.36956683 0.22049126 0.089858793 -0.010412789 -0.072443731][0.40320131 0.41839603 0.45320487 0.51723683 0.6020717 0.68207228 0.72354585 0.70186687 0.61161071 0.47867209 0.33334076 0.19718596 0.078844771 -0.01325731 -0.071516521][0.33683524 0.35991719 0.40814152 0.48260385 0.56600249 0.63279319 0.6601454 0.63723797 0.56023014 0.44405818 0.31161818 0.18442489 0.073454268 -0.013640786 -0.069490872][0.31547749 0.33915937 0.38552073 0.45377308 0.52651489 0.58082145 0.60113978 0.583661 0.52345854 0.42477241 0.30239949 0.17910706 0.070768259 -0.0134758 -0.066959739][0.32871932 0.348347 0.3828975 0.43506733 0.49324995 0.53779107 0.55657768 0.54803103 0.50446242 0.42107022 0.30493239 0.18108822 0.071599431 -0.01167173 -0.063366562][0.34684914 0.3591671 0.37874818 0.41355914 0.45805898 0.49459067 0.51208186 0.50864679 0.47509426 0.40195486 0.29076377 0.16835336 0.061512262 -0.016281946 -0.062589921][0.33957008 0.33970082 0.34248391 0.36055028 0.39265433 0.42216781 0.43811551 0.43661803 0.40809461 0.34268036 0.24005851 0.12737815 0.0327434 -0.031814456 -0.067547485]]...]
INFO - root - 2017-12-10 12:53:17.302761: step 16410, loss = 0.69, batch loss = 0.64 (8.0 examples/sec; 1.001 sec/batch; 87h:51m:30s remains)
INFO - root - 2017-12-10 12:53:27.522082: step 16420, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.004 sec/batch; 88h:09m:09s remains)
INFO - root - 2017-12-10 12:53:37.579891: step 16430, loss = 0.70, batch loss = 0.65 (7.8 examples/sec; 1.027 sec/batch; 90h:08m:04s remains)
INFO - root - 2017-12-10 12:53:47.625597: step 16440, loss = 0.69, batch loss = 0.63 (8.2 examples/sec; 0.978 sec/batch; 85h:51m:54s remains)
INFO - root - 2017-12-10 12:53:57.667130: step 16450, loss = 0.72, batch loss = 0.66 (8.0 examples/sec; 0.997 sec/batch; 87h:32m:37s remains)
INFO - root - 2017-12-10 12:54:07.606757: step 16460, loss = 0.70, batch loss = 0.64 (8.5 examples/sec; 0.936 sec/batch; 82h:09m:00s remains)
INFO - root - 2017-12-10 12:54:17.501075: step 16470, loss = 0.68, batch loss = 0.63 (8.1 examples/sec; 0.985 sec/batch; 86h:28m:39s remains)
INFO - root - 2017-12-10 12:54:27.493834: step 16480, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.007 sec/batch; 88h:22m:57s remains)
INFO - root - 2017-12-10 12:54:37.620760: step 16490, loss = 0.69, batch loss = 0.64 (7.8 examples/sec; 1.024 sec/batch; 89h:53m:41s remains)
INFO - root - 2017-12-10 12:54:47.687321: step 16500, loss = 0.72, batch loss = 0.67 (7.9 examples/sec; 1.018 sec/batch; 89h:19m:36s remains)
2017-12-10 12:54:48.586144: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.021490742 0.027580854 0.02880704 0.026032886 0.022639131 0.020577597 0.019324128 0.018905375 0.020169137 0.022275593 0.023834415 0.026192114 0.031926394 0.042529494 0.058120724][0.032266337 0.039935965 0.043662287 0.045036595 0.046728961 0.049090587 0.049383767 0.048128679 0.047451176 0.046032868 0.042681288 0.039775636 0.041146487 0.047827121 0.059395105][0.051889289 0.060781322 0.066573471 0.072554067 0.080223441 0.087797686 0.090536319 0.089565873 0.088502035 0.084534839 0.076612234 0.0682098 0.0638451 0.064365372 0.069536582][0.081535056 0.091375388 0.098524593 0.10851523 0.12124634 0.1329495 0.13787168 0.13796702 0.13784708 0.13288738 0.12236904 0.11034046 0.10134126 0.096171834 0.095224857][0.12194026 0.13349868 0.14207731 0.15509793 0.1708526 0.18437333 0.18998876 0.1902107 0.19030628 0.18439738 0.17256077 0.15898691 0.14720307 0.13816184 0.1321699][0.167823 0.1813509 0.19062194 0.20429116 0.21960261 0.23139825 0.23497124 0.23281521 0.23074213 0.22332999 0.21197948 0.19997828 0.18836921 0.17834592 0.16926134][0.20728028 0.2235422 0.23342401 0.24608515 0.25845432 0.26570156 0.26455152 0.25701427 0.25005186 0.2408319 0.23222888 0.22520873 0.21687828 0.20841187 0.19835907][0.23278074 0.25300545 0.26441389 0.27587226 0.28413141 0.28482378 0.27629557 0.26032531 0.24565279 0.23422083 0.2305418 0.23150977 0.22937346 0.22483221 0.21593344][0.24977577 0.27355957 0.28670344 0.29736081 0.30178273 0.295678 0.27865455 0.25312966 0.22933656 0.21492703 0.21625157 0.22570771 0.23120242 0.23237716 0.22698793][0.25799897 0.28318384 0.29724792 0.30784488 0.31045014 0.30008224 0.27702767 0.24452275 0.21416759 0.19829811 0.20402847 0.22024751 0.23237416 0.23885322 0.23716578][0.24921784 0.27412081 0.28889161 0.3009665 0.3048006 0.29387912 0.26907912 0.23517413 0.20414126 0.19022468 0.20027861 0.22109134 0.23725867 0.24615005 0.24571247][0.22670856 0.25137281 0.26832962 0.28397229 0.29169172 0.28330272 0.26032454 0.22935139 0.20216018 0.19218765 0.20533681 0.22783592 0.24440062 0.25096825 0.24740776][0.2083649 0.23137371 0.24867192 0.26615292 0.27697897 0.27189484 0.25246969 0.22654828 0.20483208 0.19835337 0.21207657 0.23315035 0.24701175 0.24858719 0.23995325][0.20243436 0.22224982 0.23728704 0.25372332 0.26562685 0.26386082 0.24906379 0.22828743 0.2111114 0.20616546 0.2173232 0.23410523 0.24367246 0.24099153 0.22941339][0.20348166 0.22059152 0.23221216 0.24537538 0.25671792 0.25780502 0.24758357 0.23127869 0.21772473 0.21399201 0.22199382 0.23397972 0.2395712 0.23462641 0.22252481]]...]
INFO - root - 2017-12-10 12:54:58.512976: step 16510, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.989 sec/batch; 86h:50m:37s remains)
INFO - root - 2017-12-10 12:55:08.625201: step 16520, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.029 sec/batch; 90h:18m:35s remains)
INFO - root - 2017-12-10 12:55:18.853025: step 16530, loss = 0.70, batch loss = 0.65 (8.1 examples/sec; 0.982 sec/batch; 86h:10m:53s remains)
INFO - root - 2017-12-10 12:55:28.981937: step 16540, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 0.994 sec/batch; 87h:14m:23s remains)
INFO - root - 2017-12-10 12:55:38.799798: step 16550, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.991 sec/batch; 87h:00m:19s remains)
INFO - root - 2017-12-10 12:55:48.872391: step 16560, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.023 sec/batch; 89h:46m:02s remains)
INFO - root - 2017-12-10 12:55:58.947511: step 16570, loss = 0.68, batch loss = 0.62 (8.2 examples/sec; 0.977 sec/batch; 85h:42m:17s remains)
INFO - root - 2017-12-10 12:56:09.040764: step 16580, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.991 sec/batch; 86h:57m:48s remains)
INFO - root - 2017-12-10 12:56:19.001510: step 16590, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.023 sec/batch; 89h:45m:39s remains)
INFO - root - 2017-12-10 12:56:29.059180: step 16600, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.993 sec/batch; 87h:07m:31s remains)
2017-12-10 12:56:30.019392: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.46267593 0.53158462 0.557077 0.53817928 0.47837391 0.40363964 0.34824839 0.32387587 0.30749163 0.28958309 0.27420247 0.25945863 0.24672337 0.24924131 0.26681313][0.43566495 0.51612365 0.55566812 0.54883993 0.50074726 0.43686104 0.38821033 0.36201453 0.33421621 0.30178517 0.27097982 0.24192873 0.22260754 0.23233302 0.2721462][0.41750294 0.51039273 0.56278509 0.56715393 0.53460467 0.49118674 0.4585925 0.43475884 0.39583305 0.346513 0.2957767 0.24877375 0.22075991 0.2364804 0.30084684][0.41554379 0.51650095 0.57495242 0.584599 0.56655693 0.54913807 0.54212135 0.52956593 0.487629 0.42736739 0.36038464 0.29656157 0.25738904 0.27326578 0.3527602][0.41439903 0.52023929 0.58240491 0.59798634 0.59903008 0.61648214 0.64794022 0.66067362 0.62653857 0.55875272 0.4713439 0.37975502 0.31303805 0.30897239 0.37965527][0.42468482 0.53913736 0.61058211 0.6374681 0.65980595 0.71294993 0.78580743 0.82746017 0.80144048 0.72343165 0.60944104 0.47807017 0.36633706 0.32334834 0.36425051][0.4654294 0.59335303 0.67776805 0.71452004 0.74772394 0.82020879 0.91937011 0.9802497 0.9576441 0.8690778 0.73199075 0.56374967 0.40525782 0.31640682 0.31605348][0.51930922 0.6588456 0.751892 0.79040861 0.81880057 0.88692737 0.98940426 1.0577569 1.0408425 0.95433325 0.81371397 0.63099849 0.44437 0.31828004 0.27577284][0.57062125 0.71762562 0.81003422 0.83975929 0.84844989 0.88963431 0.96978372 1.0294187 1.0183529 0.94595736 0.82100111 0.64925683 0.46331191 0.32470092 0.25616783][0.61366838 0.76142961 0.84236807 0.8526026 0.82981741 0.82873553 0.8676281 0.90369624 0.893378 0.83958805 0.74241012 0.60181856 0.44337782 0.31941932 0.24846655][0.63944525 0.77797097 0.83743536 0.82145452 0.76432526 0.71942663 0.71366245 0.7211858 0.70747626 0.671417 0.60767913 0.51209551 0.40109569 0.31256714 0.2580564][0.6354782 0.75139385 0.78241485 0.74033433 0.65600294 0.57766289 0.53506643 0.51655519 0.49800968 0.47656789 0.44579634 0.39992496 0.34580705 0.30347344 0.27493495][0.57689482 0.65986526 0.66242105 0.60434169 0.51257765 0.42596486 0.37004367 0.34158725 0.32483089 0.31511092 0.30736732 0.29838288 0.29053095 0.28930569 0.28652143][0.4577907 0.50711286 0.49049422 0.43081024 0.3491025 0.27330866 0.22202486 0.19639124 0.18610939 0.18473686 0.18884163 0.1992193 0.21884449 0.2460659 0.26530877][0.292317 0.3140139 0.29001254 0.24016029 0.17870007 0.12403057 0.0883854 0.073486954 0.071086407 0.073968038 0.080500387 0.094526842 0.12134945 0.15861475 0.18964918]]...]
INFO - root - 2017-12-10 12:56:40.072688: step 16610, loss = 0.70, batch loss = 0.64 (8.3 examples/sec; 0.964 sec/batch; 84h:37m:51s remains)
INFO - root - 2017-12-10 12:56:50.071808: step 16620, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.030 sec/batch; 90h:20m:07s remains)
INFO - root - 2017-12-10 12:56:59.988183: step 16630, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 0.999 sec/batch; 87h:37m:23s remains)
INFO - root - 2017-12-10 12:57:09.836059: step 16640, loss = 0.71, batch loss = 0.65 (8.1 examples/sec; 0.988 sec/batch; 86h:39m:10s remains)
INFO - root - 2017-12-10 12:57:19.822440: step 16650, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.986 sec/batch; 86h:30m:57s remains)
INFO - root - 2017-12-10 12:57:29.790469: step 16660, loss = 0.68, batch loss = 0.62 (8.1 examples/sec; 0.986 sec/batch; 86h:30m:44s remains)
INFO - root - 2017-12-10 12:57:39.817975: step 16670, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 1.000 sec/batch; 87h:44m:52s remains)
INFO - root - 2017-12-10 12:57:49.941886: step 16680, loss = 0.69, batch loss = 0.63 (7.6 examples/sec; 1.052 sec/batch; 92h:17m:30s remains)
INFO - root - 2017-12-10 12:57:59.871570: step 16690, loss = 0.70, batch loss = 0.64 (8.2 examples/sec; 0.979 sec/batch; 85h:53m:59s remains)
INFO - root - 2017-12-10 12:58:09.940443: step 16700, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.992 sec/batch; 86h:58m:58s remains)
2017-12-10 12:58:10.874851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.018444868 -0.00054265215 0.012189864 0.016317705 0.013253724 0.0066632247 -0.00068458368 -0.00498654 -0.003375273 0.0021694719 0.01112493 0.028760159 0.054832827 0.085798971 0.11142777][-0.016581845 0.0021304132 0.013462452 0.013680203 0.0051061297 -0.0073619955 -0.019712338 -0.02753018 -0.028049177 -0.02359765 -0.015578303 0.0026706159 0.032817293 0.072037615 0.10884222][-0.01663292 0.0036783677 0.015573476 0.014972402 0.0041768351 -0.010225674 -0.023211015 -0.031330206 -0.033111345 -0.031258 -0.027419312 -0.014517195 0.011230908 0.049995728 0.090942539][-0.0091971457 0.018130707 0.039213773 0.047711663 0.043864693 0.036641739 0.031215722 0.026439069 0.021495545 0.015507079 0.0086947065 0.0074377218 0.016720289 0.041660842 0.074466921][0.0099795572 0.052058887 0.094651923 0.12609437 0.14248568 0.15497196 0.16744572 0.1700196 0.15888593 0.13852175 0.11409749 0.0916619 0.075775258 0.076108634 0.089038812][0.03750059 0.10242853 0.17876254 0.24711929 0.29803327 0.34252539 0.38046291 0.3910144 0.36811835 0.3252354 0.27673718 0.23047324 0.18882112 0.16235316 0.15035361][0.068958975 0.16013837 0.27585715 0.38793913 0.48084107 0.56104374 0.62290561 0.6357134 0.59324384 0.5203166 0.44495931 0.37910175 0.32034439 0.27434114 0.2402681][0.0935681 0.20406474 0.3488827 0.49317619 0.61506349 0.71546024 0.78588629 0.79139543 0.72823614 0.63140315 0.5422039 0.4744356 0.41785812 0.36767945 0.32014927][0.10108738 0.2153665 0.36588398 0.51592731 0.63902277 0.73220682 0.78948569 0.78020793 0.70479149 0.60213196 0.51998091 0.47048196 0.43495485 0.39673525 0.34846759][0.082145646 0.18166645 0.3119989 0.43996572 0.53855437 0.603797 0.63491774 0.610755 0.53568274 0.44508079 0.38410372 0.36102271 0.35231814 0.33388597 0.29625189][0.032220889 0.10157116 0.19377504 0.28322479 0.34657681 0.38039228 0.38789523 0.3569192 0.29362249 0.22539324 0.18816474 0.186352 0.19692451 0.19558793 0.17308427][-0.033636615 0.00046062411 0.05036328 0.099300474 0.1299908 0.13994347 0.13346787 0.10456362 0.058997571 0.014829398 -0.0033406608 0.0063689444 0.025562162 0.034775008 0.026566721][-0.08845371 -0.082579188 -0.066299841 -0.048922602 -0.041269112 -0.044705272 -0.055752851 -0.077022761 -0.10447097 -0.12832019 -0.13404115 -0.12085 -0.10107201 -0.087693013 -0.085794844][-0.11566913 -0.12437924 -0.12588859 -0.12525263 -0.12818991 -0.13522935 -0.14442863 -0.15707837 -0.17094788 -0.18167603 -0.18190235 -0.17112361 -0.15621428 -0.14423607 -0.13806339][-0.11891394 -0.13159123 -0.138501 -0.14303049 -0.14772236 -0.15282062 -0.15786496 -0.16387758 -0.1699433 -0.17423247 -0.17355901 -0.16754229 -0.15927128 -0.15130326 -0.14504515]]...]
INFO - root - 2017-12-10 12:58:20.670178: step 16710, loss = 0.70, batch loss = 0.64 (8.3 examples/sec; 0.966 sec/batch; 84h:45m:12s remains)
INFO - root - 2017-12-10 12:58:30.858828: step 16720, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.001 sec/batch; 87h:48m:39s remains)
INFO - root - 2017-12-10 12:58:40.730469: step 16730, loss = 0.68, batch loss = 0.62 (7.8 examples/sec; 1.032 sec/batch; 90h:32m:05s remains)
INFO - root - 2017-12-10 12:58:50.830020: step 16740, loss = 0.69, batch loss = 0.64 (7.8 examples/sec; 1.021 sec/batch; 89h:32m:04s remains)
INFO - root - 2017-12-10 12:59:00.894605: step 16750, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.019 sec/batch; 89h:20m:55s remains)
INFO - root - 2017-12-10 12:59:11.128526: step 16760, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.028 sec/batch; 90h:10m:26s remains)
INFO - root - 2017-12-10 12:59:21.118032: step 16770, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.988 sec/batch; 86h:39m:07s remains)
INFO - root - 2017-12-10 12:59:31.259455: step 16780, loss = 0.70, batch loss = 0.64 (7.1 examples/sec; 1.125 sec/batch; 98h:41m:31s remains)
INFO - root - 2017-12-10 12:59:41.035834: step 16790, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.002 sec/batch; 87h:53m:03s remains)
INFO - root - 2017-12-10 12:59:51.168897: step 16800, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.015 sec/batch; 88h:59m:08s remains)
2017-12-10 12:59:52.157951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0038505432 -0.01365929 -0.018521972 -0.015742751 -0.006208438 0.0073762718 0.022270635 0.035739258 0.04456608 0.044972263 0.037104446 0.02529316 0.013897919 0.0055559366 0.001989973][0.03476445 0.01729255 0.0045369589 0.00550503 0.020246329 0.045317903 0.075889058 0.10652335 0.13014048 0.13851784 0.1292551 0.10802766 0.083183721 0.062294144 0.050443981][0.075350434 0.0468833 0.02173119 0.016995622 0.035094343 0.072435349 0.12103789 0.17171848 0.21320038 0.23200798 0.22233775 0.19121748 0.15190665 0.11812329 0.098489419][0.13011365 0.091223568 0.051108561 0.036112692 0.052938648 0.09987925 0.16568878 0.23609187 0.29574439 0.32650366 0.31923932 0.28119966 0.22999236 0.18563066 0.1610565][0.20673576 0.16251406 0.10807533 0.078958347 0.088009119 0.13779011 0.21485609 0.29955679 0.37329549 0.41553864 0.41464505 0.37608814 0.31941408 0.269801 0.24364012][0.30648997 0.26403511 0.19570421 0.14680684 0.13946287 0.18286546 0.2628597 0.35430208 0.43604332 0.48779348 0.4964658 0.46345967 0.40738478 0.35646588 0.3296217][0.41140771 0.37944731 0.30283266 0.23362888 0.20349053 0.22832905 0.29571971 0.37849563 0.45582512 0.51075655 0.52952212 0.5072093 0.45818904 0.41084436 0.3845998][0.49023169 0.47680515 0.40184498 0.31837666 0.26429439 0.26095241 0.2994225 0.35605425 0.41409248 0.46186227 0.48634568 0.47664312 0.439891 0.400603 0.37653303][0.52632278 0.53626978 0.47354379 0.38581735 0.31208259 0.27691385 0.27698639 0.2964099 0.32513088 0.35767245 0.38276637 0.38449904 0.36165759 0.33185303 0.31045255][0.521433 0.55204582 0.50475633 0.41823915 0.32900074 0.26356667 0.22505845 0.20658381 0.20407116 0.21688662 0.23631279 0.24453774 0.23399718 0.21451057 0.19758537][0.50117671 0.54818618 0.51620334 0.43304127 0.3313877 0.24013688 0.16951837 0.12089948 0.0936745 0.089210972 0.10088795 0.11127955 0.10954951 0.099094748 0.087028116][0.49018106 0.55001527 0.53029776 0.45003122 0.33818373 0.226072 0.13114819 0.062981106 0.022033585 0.0078332908 0.014196412 0.025411677 0.030209055 0.027334252 0.0201111][0.51493824 0.58126 0.56610709 0.48275486 0.35845277 0.22791703 0.11555966 0.037383426 -0.0075999149 -0.024637392 -0.020710634 -0.0092801293 -0.00028061678 0.0022121011 -0.00093680579][0.56356996 0.6330834 0.61730129 0.5271771 0.39012748 0.24523829 0.12263315 0.041421715 -0.0028308565 -0.020769425 -0.020454118 -0.012547074 -0.0038129122 0.0012250443 0.0019754944][0.60315543 0.67338145 0.6552549 0.55901235 0.41364244 0.26271328 0.13869511 0.0598789 0.017872704 -0.0017445832 -0.0069741216 -0.0053709871 -0.0004210663 0.0042098849 0.0071888128]]...]
INFO - root - 2017-12-10 13:00:02.238538: step 16810, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.002 sec/batch; 87h:52m:39s remains)
INFO - root - 2017-12-10 13:00:12.278683: step 16820, loss = 0.70, batch loss = 0.64 (8.4 examples/sec; 0.954 sec/batch; 83h:41m:26s remains)
INFO - root - 2017-12-10 13:00:22.581224: step 16830, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.036 sec/batch; 90h:48m:25s remains)
INFO - root - 2017-12-10 13:00:32.578208: step 16840, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 0.998 sec/batch; 87h:29m:23s remains)
INFO - root - 2017-12-10 13:00:42.581182: step 16850, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.983 sec/batch; 86h:13m:59s remains)
INFO - root - 2017-12-10 13:00:52.511655: step 16860, loss = 0.69, batch loss = 0.63 (8.5 examples/sec; 0.946 sec/batch; 82h:58m:40s remains)
INFO - root - 2017-12-10 13:01:02.702666: step 16870, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.027 sec/batch; 90h:00m:48s remains)
INFO - root - 2017-12-10 13:01:12.697379: step 16880, loss = 0.72, batch loss = 0.66 (8.4 examples/sec; 0.949 sec/batch; 83h:13m:20s remains)
INFO - root - 2017-12-10 13:01:22.754064: step 16890, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.023 sec/batch; 89h:41m:27s remains)
INFO - root - 2017-12-10 13:01:32.912906: step 16900, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.025 sec/batch; 89h:51m:13s remains)
2017-12-10 13:01:33.957452: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.017702229 0.019828221 0.021907151 0.017016586 0.0040056612 -0.01023037 -0.017786216 -0.01722268 -0.009264851 0.0021762277 0.017067986 0.040661503 0.067969263 0.088456541 0.093828849][-0.075045295 -0.070903376 -0.061063394 -0.051335651 -0.044463161 -0.039299678 -0.035060409 -0.033384148 -0.031143121 -0.026649386 -0.015501128 0.013553981 0.050718032 0.076742709 0.078891747][-0.12163168 -0.11140566 -0.091794759 -0.0678453 -0.041955713 -0.016363675 0.0033688 0.010253254 0.0092561916 0.0064668572 0.011195654 0.040804993 0.083806738 0.11617119 0.11932704][-0.11720332 -0.096706688 -0.066013925 -0.028823571 0.014503531 0.060633823 0.097881749 0.11220223 0.10878203 0.097834565 0.092489026 0.11444026 0.15515806 0.19173393 0.200567][-0.079055719 -0.045367178 -0.0026204225 0.046559885 0.10388491 0.16623701 0.21781653 0.23883015 0.23340812 0.21472947 0.19788331 0.20574772 0.23575233 0.271736 0.28819758][-0.028118929 0.019649712 0.074028879 0.13226639 0.19610147 0.26341084 0.31914538 0.34257427 0.33601362 0.31255019 0.287127 0.28034866 0.2952134 0.32532081 0.34768525][0.020074967 0.07902769 0.14122352 0.20231006 0.26236683 0.32084349 0.36851546 0.38976026 0.38459018 0.36252064 0.33582443 0.32052237 0.32312429 0.34563839 0.37032142][0.05525339 0.11849204 0.18194881 0.23914844 0.28802258 0.33004788 0.36419296 0.383634 0.38505444 0.37135264 0.3507714 0.33438569 0.3302561 0.34490585 0.36615083][0.07229837 0.12945759 0.18510413 0.23232682 0.26805335 0.29522792 0.31954855 0.34191969 0.35504332 0.35431167 0.34451678 0.33312112 0.3282823 0.33709908 0.35051203][0.070141517 0.11030982 0.14927389 0.18226297 0.20674525 0.22586013 0.24756652 0.27694893 0.30355793 0.31725961 0.32088843 0.31921414 0.31926113 0.32599208 0.33131275][0.052481167 0.068459488 0.085491017 0.10264606 0.11913243 0.13722065 0.16298477 0.20152579 0.24030873 0.26684713 0.2830992 0.29192668 0.29835132 0.30494508 0.30398014][0.027391858 0.019461343 0.015426 0.018254178 0.029461652 0.050445579 0.08322043 0.12980334 0.17717411 0.2132712 0.23895563 0.25564912 0.26685956 0.27382025 0.26906589][0.002977211 -0.022321176 -0.04144517 -0.048709553 -0.040646411 -0.01670762 0.021198282 0.071246371 0.12206618 0.16325605 0.19375633 0.21410552 0.22789149 0.23606856 0.23088899][-0.016644955 -0.05068351 -0.077185079 -0.088702232 -0.080740362 -0.054529972 -0.015309806 0.032667276 0.081922583 0.12345506 0.1529647 0.17052118 0.18252338 0.19159697 0.18886662][-0.029171249 -0.063674144 -0.089511007 -0.098024741 -0.085511528 -0.056764163 -0.01958097 0.022013417 0.065616034 0.10256077 0.12451356 0.13193524 0.13628566 0.14369017 0.14408851]]...]
INFO - root - 2017-12-10 13:01:43.920576: step 16910, loss = 0.67, batch loss = 0.62 (7.9 examples/sec; 1.013 sec/batch; 88h:46m:14s remains)
INFO - root - 2017-12-10 13:01:53.951826: step 16920, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 1.002 sec/batch; 87h:50m:29s remains)
INFO - root - 2017-12-10 13:02:03.887493: step 16930, loss = 0.70, batch loss = 0.64 (8.2 examples/sec; 0.979 sec/batch; 85h:49m:09s remains)
INFO - root - 2017-12-10 13:02:13.894070: step 16940, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.036 sec/batch; 90h:48m:42s remains)
INFO - root - 2017-12-10 13:02:24.079174: step 16950, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.986 sec/batch; 86h:23m:45s remains)
INFO - root - 2017-12-10 13:02:34.087934: step 16960, loss = 0.70, batch loss = 0.65 (8.2 examples/sec; 0.973 sec/batch; 85h:19m:17s remains)
INFO - root - 2017-12-10 13:02:44.174179: step 16970, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.011 sec/batch; 88h:37m:20s remains)
INFO - root - 2017-12-10 13:02:54.330740: step 16980, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.023 sec/batch; 89h:42m:10s remains)
INFO - root - 2017-12-10 13:03:04.292311: step 16990, loss = 0.71, batch loss = 0.65 (8.3 examples/sec; 0.969 sec/batch; 84h:57m:01s remains)
INFO - root - 2017-12-10 13:03:14.266375: step 17000, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.018 sec/batch; 89h:10m:32s remains)
2017-12-10 13:03:15.259666: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.094220281 0.085569367 0.075753257 0.072642416 0.081491567 0.10096082 0.12267189 0.13647725 0.14704828 0.15807542 0.16628741 0.16628833 0.15579304 0.14052802 0.11584128][0.071400143 0.070881963 0.071921632 0.077155493 0.088380344 0.10382933 0.11793911 0.12298313 0.12502666 0.12983628 0.13643894 0.13811432 0.13040236 0.11718053 0.094987236][0.073252015 0.080696255 0.090832189 0.10068678 0.10854372 0.11409867 0.11571158 0.10913508 0.10175092 0.10120001 0.10773149 0.1124134 0.10831152 0.096671179 0.075343654][0.10993802 0.12354263 0.13926959 0.14972211 0.15057577 0.14461581 0.13378772 0.11645044 0.10061379 0.095548376 0.10223647 0.10929351 0.10769837 0.096154734 0.073166713][0.16660798 0.18504965 0.20378853 0.21277443 0.20655915 0.19160731 0.17241529 0.14839183 0.12661679 0.1177961 0.12412129 0.13244636 0.1320769 0.11907537 0.092118613][0.22323534 0.24655613 0.26819882 0.27631953 0.26591748 0.2463925 0.22388501 0.19750792 0.17211759 0.15957487 0.16321363 0.16932136 0.16622967 0.14906342 0.11666978][0.25668332 0.28512171 0.31059632 0.32013705 0.31078178 0.2929863 0.27320889 0.24911298 0.22282007 0.20692457 0.20511426 0.20457311 0.19417958 0.1703826 0.13264866][0.253949 0.28723174 0.31732339 0.33052596 0.32654324 0.31534493 0.30256769 0.28401637 0.25947732 0.24161561 0.23311967 0.22317217 0.20283359 0.17180003 0.13025773][0.21436049 0.2508291 0.28520498 0.30292141 0.30581215 0.30213243 0.29636258 0.28339711 0.26190355 0.24399947 0.23037253 0.21255106 0.18439905 0.14904712 0.10717875][0.14806584 0.18424492 0.21917172 0.23794511 0.24458168 0.24560972 0.24433252 0.23547545 0.21774602 0.20225754 0.18735233 0.16670991 0.136538 0.10249621 0.065619878][0.074453928 0.10499447 0.13483103 0.14985982 0.15575561 0.15771332 0.15797214 0.15152302 0.13808201 0.12733068 0.11550054 0.0977739 0.072218016 0.045307472 0.017955005][0.010443332 0.031126622 0.051129915 0.059003465 0.061087266 0.061633717 0.061489616 0.056261834 0.047198113 0.04241357 0.036882732 0.026857339 0.011578638 -0.0037066776 -0.018990487][-0.027458841 -0.018188473 -0.010051312 -0.010878793 -0.014485645 -0.017229909 -0.019523367 -0.024443381 -0.029377816 -0.02816245 -0.026212418 -0.026559774 -0.02976732 -0.033042572 -0.037619591][-0.038184706 -0.037787963 -0.038926538 -0.046161912 -0.054085352 -0.059576139 -0.063699283 -0.068139516 -0.069641173 -0.064281911 -0.057162207 -0.050856259 -0.046023186 -0.042068355 -0.0408478][-0.03303406 -0.037563212 -0.043765794 -0.053910654 -0.063628346 -0.070152231 -0.074632242 -0.077795856 -0.076600052 -0.0691619 -0.060110748 -0.051661495 -0.044359915 -0.038716648 -0.036528178]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 13:03:25.395568: step 17010, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.989 sec/batch; 86h:42m:02s remains)
INFO - root - 2017-12-10 13:03:35.416703: step 17020, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.025 sec/batch; 89h:50m:41s remains)
INFO - root - 2017-12-10 13:03:45.530058: step 17030, loss = 0.70, batch loss = 0.64 (8.4 examples/sec; 0.948 sec/batch; 83h:06m:55s remains)
INFO - root - 2017-12-10 13:03:55.534364: step 17040, loss = 0.68, batch loss = 0.62 (8.2 examples/sec; 0.972 sec/batch; 85h:11m:34s remains)
INFO - root - 2017-12-10 13:04:05.573255: step 17050, loss = 0.68, batch loss = 0.62 (7.8 examples/sec; 1.028 sec/batch; 90h:03m:22s remains)
INFO - root - 2017-12-10 13:04:15.574770: step 17060, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.004 sec/batch; 87h:57m:58s remains)
INFO - root - 2017-12-10 13:04:25.583338: step 17070, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.020 sec/batch; 89h:24m:48s remains)
INFO - root - 2017-12-10 13:04:35.604191: step 17080, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.010 sec/batch; 88h:27m:26s remains)
INFO - root - 2017-12-10 13:04:45.510424: step 17090, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 0.999 sec/batch; 87h:30m:27s remains)
INFO - root - 2017-12-10 13:04:55.366254: step 17100, loss = 0.69, batch loss = 0.64 (8.0 examples/sec; 0.997 sec/batch; 87h:21m:43s remains)
2017-12-10 13:04:56.392615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.049984414 -0.033872508 -0.00074055867 0.03030907 0.047979202 0.046233989 0.024228334 -0.010813967 -0.046196207 -0.070900917 -0.077731542 -0.069034077 -0.053223521 -0.0383727 -0.029901536][-0.057148531 -0.029989759 0.023126593 0.078101628 0.11659904 0.12619345 0.10223152 0.052744821 -0.00468258 -0.052645057 -0.07604298 -0.073526226 -0.055840425 -0.035958633 -0.023701351][-0.060266834 -0.019906295 0.057093762 0.14243062 0.21004093 0.23954329 0.22150588 0.16191554 0.0814356 0.0051775612 -0.041140322 -0.050062068 -0.033774722 -0.010702961 0.0036142771][-0.048245307 0.0013656712 0.09772142 0.21084152 0.30961835 0.3652443 0.36245835 0.30101967 0.20202696 0.099461928 0.032047443 0.014085419 0.031113382 0.057822481 0.072670996][-0.010901421 0.039245181 0.13981017 0.26611283 0.38849586 0.4718695 0.49306473 0.44136256 0.33402863 0.21361707 0.13272785 0.11307458 0.13741845 0.17134269 0.1877993][0.056453347 0.10021059 0.19002439 0.3116357 0.44229859 0.54706311 0.59543091 0.56313431 0.45924875 0.33208168 0.24478652 0.22665788 0.25992858 0.30219418 0.32093534][0.15250684 0.18549302 0.25348786 0.35347885 0.47186649 0.58171451 0.65002191 0.64109623 0.55298108 0.43298534 0.34607694 0.32770526 0.36483052 0.41305342 0.43479833][0.249043 0.26958978 0.31116855 0.37961683 0.46912923 0.56438577 0.63671011 0.64726233 0.58366382 0.48420987 0.40472341 0.38455287 0.41994473 0.47155926 0.49919596][0.32338202 0.33241305 0.34965321 0.38631007 0.43987021 0.50409806 0.56132638 0.5807693 0.54505521 0.47727582 0.41622731 0.40015596 0.434248 0.48789051 0.52266687][0.3741774 0.37425169 0.37263361 0.38361487 0.4031907 0.42920148 0.4574945 0.47198907 0.45867333 0.42598492 0.39303052 0.38982335 0.42638808 0.4805986 0.51970661][0.40798092 0.40599537 0.39470309 0.39017937 0.38588271 0.37950382 0.376723 0.37543356 0.37026158 0.36249098 0.35658121 0.36976418 0.41074976 0.46146786 0.49639949][0.43335921 0.43422732 0.41703475 0.40145707 0.38266373 0.35693485 0.33272314 0.31522724 0.30737403 0.31056148 0.32244894 0.34910947 0.39296222 0.43593794 0.45941818][0.43026328 0.43520254 0.41501376 0.3921805 0.3670336 0.33508176 0.30211514 0.27409694 0.25868422 0.26069385 0.27764952 0.31000698 0.35364869 0.389558 0.40393123][0.3762593 0.38498652 0.36477631 0.33900991 0.31313777 0.2832939 0.25126097 0.21974991 0.19813982 0.19462629 0.20942737 0.24111684 0.28156865 0.31283811 0.32428312][0.2695359 0.28206086 0.26731011 0.24533649 0.22302486 0.19847156 0.1716748 0.14164987 0.11791575 0.11022596 0.12054116 0.14684941 0.17963953 0.20439592 0.21352756]]...]
INFO - root - 2017-12-10 13:05:06.392132: step 17110, loss = 0.68, batch loss = 0.62 (8.0 examples/sec; 0.996 sec/batch; 87h:16m:38s remains)
INFO - root - 2017-12-10 13:05:16.408607: step 17120, loss = 0.70, batch loss = 0.65 (7.8 examples/sec; 1.030 sec/batch; 90h:12m:21s remains)
INFO - root - 2017-12-10 13:05:26.545482: step 17130, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.039 sec/batch; 91h:00m:06s remains)
INFO - root - 2017-12-10 13:05:36.644992: step 17140, loss = 0.71, batch loss = 0.66 (8.1 examples/sec; 0.988 sec/batch; 86h:35m:22s remains)
INFO - root - 2017-12-10 13:05:46.790327: step 17150, loss = 0.70, batch loss = 0.64 (8.2 examples/sec; 0.981 sec/batch; 85h:55m:16s remains)
INFO - root - 2017-12-10 13:05:56.868901: step 17160, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.000 sec/batch; 87h:33m:43s remains)
INFO - root - 2017-12-10 13:06:06.934054: step 17170, loss = 0.69, batch loss = 0.64 (7.6 examples/sec; 1.059 sec/batch; 92h:45m:13s remains)
INFO - root - 2017-12-10 13:06:16.683256: step 17180, loss = 0.68, batch loss = 0.62 (7.7 examples/sec; 1.035 sec/batch; 90h:37m:49s remains)
INFO - root - 2017-12-10 13:06:26.688426: step 17190, loss = 0.71, batch loss = 0.65 (8.1 examples/sec; 0.983 sec/batch; 86h:07m:01s remains)
INFO - root - 2017-12-10 13:06:36.837169: step 17200, loss = 0.71, batch loss = 0.65 (8.1 examples/sec; 0.989 sec/batch; 86h:38m:07s remains)
2017-12-10 13:06:37.848849: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0030173056 0.0031290636 0.0077169556 0.019570017 0.038975645 0.061465841 0.079456292 0.085460559 0.076631211 0.0558021 0.029587464 0.005583534 -0.011162071 -0.019176712 -0.020452211][0.0042875847 0.0064209141 0.014756582 0.033257652 0.0627515 0.097170882 0.1254563 0.13598146 0.12345786 0.09157563 0.050837472 0.013899315 -0.010809921 -0.021716792 -0.023002554][0.0051976857 0.0094554583 0.021493116 0.046319779 0.085377976 0.13141139 0.17034464 0.18660769 0.17204748 0.13030304 0.075230822 0.024944615 -0.00821807 -0.022446586 -0.024130115][0.006054081 0.012189033 0.027309015 0.057079926 0.10346992 0.15847723 0.2060287 0.22773965 0.21291834 0.16422562 0.097679354 0.036109228 -0.0045267832 -0.021903684 -0.02427464][0.0056716576 0.013511948 0.031107297 0.064213522 0.11497996 0.17506292 0.22752668 0.25264555 0.23839782 0.18605368 0.11243457 0.043562535 -0.0019601614 -0.02139293 -0.024323236][0.0034090693 0.012618853 0.032063745 0.067035131 0.11971323 0.18174072 0.23609284 0.26285034 0.24935073 0.19586636 0.11904342 0.046442978 -0.0017045633 -0.022189103 -0.025194611][-0.00022462846 0.0093142586 0.029316731 0.064334169 0.11662322 0.17848374 0.23356716 0.26217595 0.2511183 0.19936806 0.12258842 0.048708711 -0.0012242928 -0.023047997 -0.026495587][-0.003546139 0.0042734281 0.022651104 0.055338 0.10492336 0.16529144 0.22159575 0.25456029 0.2497801 0.20385301 0.13023262 0.055938534 0.0031580527 -0.02194972 -0.027269093][-0.0046566888 -0.0010534172 0.013232926 0.0410851 0.085644081 0.14335302 0.2019296 0.24292436 0.24974991 0.21490994 0.14777094 0.07327535 0.015239852 -0.016368497 -0.026235588][-0.0016081048 -0.0045165368 0.0035654795 0.024429372 0.061687924 0.11485718 0.17539361 0.22653334 0.24908963 0.23000288 0.17268631 0.098800756 0.034183 -0.0062899594 -0.02275995][0.0054166149 -0.0050661396 -0.0044005816 0.0081282007 0.036132973 0.081948459 0.14144796 0.20083854 0.23925318 0.23804787 0.19369659 0.12330802 0.053959772 0.0053948597 -0.017735144][0.013067651 -0.00369413 -0.0095625389 -0.0048030713 0.013365731 0.04897178 0.10191297 0.16260131 0.21097036 0.225076 0.19544408 0.13363649 0.065581843 0.013870486 -0.01316829][0.016709736 -0.0026034357 -0.011661473 -0.011786352 -0.0016846276 0.022899503 0.064529166 0.11811163 0.16702445 0.18953854 0.17282023 0.12313741 0.062950447 0.014598832 -0.011811555][0.013487898 -0.0035855295 -0.011352742 -0.012316157 -0.006837396 0.0083841784 0.036833622 0.077108495 0.1176047 0.14022136 0.13189 0.095006153 0.047048066 0.0074375067 -0.014192265][0.0041450271 -0.0071071857 -0.010352192 -0.0091657359 -0.0051289476 0.0036196767 0.020146295 0.045201644 0.072147742 0.088356152 0.08325202 0.057358149 0.022922678 -0.0051286034 -0.019380823]]...]
INFO - root - 2017-12-10 13:06:47.810528: step 17210, loss = 0.68, batch loss = 0.63 (8.1 examples/sec; 0.993 sec/batch; 86h:55m:45s remains)
INFO - root - 2017-12-10 13:06:57.901779: step 17220, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.988 sec/batch; 86h:30m:19s remains)
INFO - root - 2017-12-10 13:07:08.072035: step 17230, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.032 sec/batch; 90h:20m:28s remains)
INFO - root - 2017-12-10 13:07:18.243433: step 17240, loss = 0.68, batch loss = 0.63 (7.7 examples/sec; 1.045 sec/batch; 91h:29m:04s remains)
INFO - root - 2017-12-10 13:07:28.297354: step 17250, loss = 0.72, batch loss = 0.66 (7.9 examples/sec; 1.010 sec/batch; 88h:27m:56s remains)
INFO - root - 2017-12-10 13:07:38.293072: step 17260, loss = 0.68, batch loss = 0.62 (7.7 examples/sec; 1.034 sec/batch; 90h:32m:50s remains)
INFO - root - 2017-12-10 13:07:48.232836: step 17270, loss = 0.71, batch loss = 0.66 (7.7 examples/sec; 1.033 sec/batch; 90h:28m:34s remains)
INFO - root - 2017-12-10 13:07:58.322131: step 17280, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.011 sec/batch; 88h:32m:01s remains)
INFO - root - 2017-12-10 13:08:08.406242: step 17290, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.003 sec/batch; 87h:49m:41s remains)
INFO - root - 2017-12-10 13:08:18.300874: step 17300, loss = 0.68, batch loss = 0.62 (8.3 examples/sec; 0.963 sec/batch; 84h:18m:52s remains)
2017-12-10 13:08:19.348757: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20391892 0.20305261 0.19516243 0.18064444 0.15773639 0.12917741 0.098446749 0.068589039 0.044082616 0.038654946 0.054468066 0.083623447 0.11208769 0.12943183 0.13748991][0.19038999 0.2064908 0.20683549 0.19088788 0.16022749 0.12148493 0.082680151 0.052081317 0.034439988 0.038711876 0.063858137 0.10172114 0.13809206 0.15991873 0.16673832][0.19485044 0.23123935 0.24553686 0.23520616 0.20283222 0.15698619 0.10993474 0.074705109 0.056501895 0.061189234 0.086537741 0.12571436 0.1646167 0.18779363 0.19255254][0.2174097 0.27511686 0.307589 0.31017619 0.28356603 0.23587596 0.18229087 0.1390395 0.11247552 0.10692467 0.12167501 0.15264808 0.18655954 0.20647797 0.20845297][0.254401 0.33205408 0.38484684 0.40696639 0.39462891 0.35279098 0.2969785 0.24413647 0.20189656 0.17444046 0.16554825 0.17618504 0.1952942 0.20625889 0.20402698][0.30440444 0.39914709 0.47126749 0.51512128 0.52200723 0.49216691 0.43963492 0.37945595 0.31902114 0.26227775 0.21902053 0.19754274 0.19108178 0.18504497 0.17443854][0.35694489 0.46331316 0.5484013 0.60816568 0.63109469 0.6137687 0.56845838 0.50641346 0.43220559 0.34772176 0.26747927 0.20817146 0.16917664 0.13953923 0.11587767][0.39064956 0.49941647 0.58678895 0.65016949 0.67810476 0.66698986 0.62897193 0.57101649 0.49359131 0.39461923 0.28979719 0.20169018 0.1353929 0.084311984 0.047374759][0.39497888 0.49450567 0.57070953 0.62190497 0.6392411 0.62241238 0.5860374 0.53508294 0.4662568 0.37298426 0.26783547 0.17400257 0.099279627 0.039789744 -0.0035005496][0.3800846 0.46268514 0.51902187 0.54736936 0.54225624 0.508099 0.46431181 0.41748458 0.36297536 0.29117686 0.20848826 0.13383283 0.073172443 0.022481492 -0.016030321][0.3689931 0.4343031 0.46995398 0.47446445 0.4443436 0.38756821 0.32979521 0.28252304 0.24279167 0.1998032 0.15320578 0.11452284 0.083525844 0.053415623 0.027482988][0.39536294 0.4446117 0.45915043 0.44014019 0.38579458 0.30575508 0.23104794 0.179217 0.15028773 0.1340261 0.12524045 0.12760864 0.13236997 0.1286002 0.12040663][0.46667019 0.4985683 0.48860753 0.44500259 0.3692911 0.27158377 0.18409938 0.12761542 0.10430962 0.1055804 0.12447116 0.15927549 0.19412781 0.21463647 0.22537732][0.5557096 0.56771946 0.530031 0.461689 0.36951613 0.26389241 0.17304727 0.11654329 0.097120136 0.1071854 0.13929576 0.18894388 0.23769195 0.2710138 0.29438743][0.61095744 0.6021502 0.53837067 0.44982949 0.34851551 0.24548581 0.16194977 0.11333784 0.10206477 0.11965912 0.15652989 0.20544305 0.25001806 0.27978578 0.30216509]]...]
INFO - root - 2017-12-10 13:08:29.273637: step 17310, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.040 sec/batch; 91h:01m:53s remains)
INFO - root - 2017-12-10 13:08:39.301726: step 17320, loss = 0.68, batch loss = 0.62 (8.3 examples/sec; 0.969 sec/batch; 84h:52m:44s remains)
INFO - root - 2017-12-10 13:08:49.429146: step 17330, loss = 0.67, batch loss = 0.61 (7.8 examples/sec; 1.023 sec/batch; 89h:33m:38s remains)
INFO - root - 2017-12-10 13:08:59.354629: step 17340, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.015 sec/batch; 88h:53m:22s remains)
INFO - root - 2017-12-10 13:09:09.202263: step 17350, loss = 0.70, batch loss = 0.64 (9.5 examples/sec; 0.845 sec/batch; 73h:57m:05s remains)
INFO - root - 2017-12-10 13:09:19.322576: step 17360, loss = 0.69, batch loss = 0.63 (8.2 examples/sec; 0.979 sec/batch; 85h:40m:25s remains)
INFO - root - 2017-12-10 13:09:29.330483: step 17370, loss = 0.68, batch loss = 0.62 (8.0 examples/sec; 1.001 sec/batch; 87h:35m:46s remains)
INFO - root - 2017-12-10 13:09:39.445689: step 17380, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.045 sec/batch; 91h:29m:22s remains)
INFO - root - 2017-12-10 13:09:49.426910: step 17390, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.021 sec/batch; 89h:23m:31s remains)
INFO - root - 2017-12-10 13:09:59.489798: step 17400, loss = 0.69, batch loss = 0.64 (8.0 examples/sec; 1.002 sec/batch; 87h:44m:01s remains)
2017-12-10 13:10:00.518304: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31891909 0.35640988 0.39307791 0.41732779 0.41298965 0.37423962 0.30318558 0.21675335 0.13865408 0.080962092 0.04347191 0.02136419 0.014831368 0.02019367 0.028662339][0.25362828 0.2858735 0.32301596 0.35235286 0.3571139 0.33147532 0.27836218 0.20940083 0.14527534 0.099527016 0.070628017 0.050043497 0.038672015 0.038821571 0.043508563][0.17960568 0.20793392 0.2465959 0.28270826 0.30182734 0.2988818 0.27482662 0.23263384 0.18808384 0.15765291 0.13869615 0.11944948 0.10249721 0.097300746 0.097822428][0.12240741 0.148758 0.19043396 0.23526952 0.2723271 0.29747939 0.30711439 0.29457831 0.27025855 0.25302109 0.24172044 0.22412895 0.20386085 0.19557601 0.1929476][0.085122429 0.11424184 0.16333622 0.22179958 0.28101394 0.33647808 0.37891188 0.3941128 0.38767481 0.37907046 0.37175009 0.35623091 0.33654457 0.3274343 0.32070833][0.075059511 0.11560686 0.18108487 0.26102436 0.34483916 0.42622194 0.49168721 0.52375174 0.52545917 0.51510674 0.50462782 0.489697 0.47343 0.4647862 0.45191652][0.088813938 0.14551009 0.23182833 0.3353107 0.43967476 0.53657097 0.61132842 0.64768958 0.64818972 0.62834704 0.60907537 0.59184319 0.57862908 0.56962 0.54849][0.10096476 0.170948 0.27374282 0.39436692 0.510599 0.612306 0.6857295 0.71912259 0.71547931 0.68632591 0.65908217 0.63926 0.6267066 0.61348963 0.58190256][0.11713929 0.19083744 0.29490927 0.41485792 0.52635533 0.6185596 0.68022573 0.70535296 0.69854814 0.66717613 0.6396746 0.62171388 0.60972673 0.5906691 0.54921013][0.13622946 0.20361248 0.29352266 0.39478645 0.48556706 0.55600661 0.59831458 0.61183125 0.60280949 0.5754081 0.55412126 0.54200542 0.53161567 0.50838214 0.46197474][0.14897211 0.20304045 0.26942256 0.34100604 0.40119997 0.443174 0.46339425 0.46505874 0.45452386 0.43368185 0.42034864 0.41476232 0.4064914 0.38288906 0.33947012][0.14467484 0.18130845 0.22064005 0.25903183 0.28638783 0.30074909 0.302485 0.29628077 0.28718674 0.27477449 0.2694197 0.26989534 0.26568627 0.24821718 0.2171344][0.12307062 0.14242971 0.15705761 0.16651604 0.16764088 0.16340534 0.15602134 0.14903291 0.14513132 0.14113432 0.140591 0.14253138 0.14062187 0.13163212 0.11715178][0.10544419 0.10989969 0.10533083 0.095221885 0.081679106 0.070302784 0.062708594 0.059186198 0.059540432 0.058228668 0.054124754 0.048776049 0.043344654 0.04017305 0.04083015][0.093317725 0.08565288 0.068961941 0.051164735 0.03716461 0.031236058 0.031860847 0.034848612 0.03781046 0.033908922 0.01943427 -0.00036874297 -0.015969789 -0.019588437 -0.010248852]]...]
INFO - root - 2017-12-10 13:10:10.718243: step 17410, loss = 0.68, batch loss = 0.62 (7.8 examples/sec; 1.024 sec/batch; 89h:34m:57s remains)
INFO - root - 2017-12-10 13:10:20.640000: step 17420, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 0.998 sec/batch; 87h:21m:25s remains)
INFO - root - 2017-12-10 13:10:30.696211: step 17430, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.018 sec/batch; 89h:04m:36s remains)
INFO - root - 2017-12-10 13:10:40.572193: step 17440, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 0.996 sec/batch; 87h:10m:01s remains)
INFO - root - 2017-12-10 13:10:50.619379: step 17450, loss = 0.70, batch loss = 0.64 (7.7 examples/sec; 1.033 sec/batch; 90h:24m:13s remains)
INFO - root - 2017-12-10 13:11:00.697397: step 17460, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.993 sec/batch; 86h:56m:31s remains)
INFO - root - 2017-12-10 13:11:10.624432: step 17470, loss = 0.67, batch loss = 0.61 (8.1 examples/sec; 0.986 sec/batch; 86h:19m:03s remains)
INFO - root - 2017-12-10 13:11:20.736300: step 17480, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.007 sec/batch; 88h:05m:33s remains)
INFO - root - 2017-12-10 13:11:30.796552: step 17490, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.014 sec/batch; 88h:42m:01s remains)
INFO - root - 2017-12-10 13:11:40.638035: step 17500, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.000 sec/batch; 87h:28m:14s remains)
2017-12-10 13:11:41.708833: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.084663332 0.079729758 0.067100644 0.051850315 0.03482943 0.020688146 0.014830226 0.016861077 0.022530951 0.024813397 0.019730946 0.0085163424 -0.003910203 -0.012646132 -0.019230356][0.188059 0.18129636 0.16190627 0.14026219 0.11743315 0.098918639 0.091286063 0.093709931 0.099162132 0.096663117 0.080721423 0.055229068 0.030649723 0.015783083 0.0079713175][0.29932472 0.29185885 0.26631257 0.23907155 0.21231623 0.19242716 0.18606183 0.19088198 0.19594754 0.18632317 0.1556226 0.11108766 0.069948278 0.045253813 0.034539804][0.39376241 0.38903072 0.36272147 0.33493116 0.30916646 0.29320386 0.29350832 0.30530164 0.31202883 0.29537234 0.24963422 0.18455382 0.12313562 0.081864521 0.061729129][0.44404158 0.44373128 0.41853794 0.39220646 0.37062383 0.36430183 0.3787978 0.40501222 0.417814 0.39664358 0.3391116 0.25781751 0.17910393 0.12110028 0.088780463][0.44285977 0.44400638 0.41977328 0.39777565 0.3855519 0.39450771 0.427611 0.47033939 0.48777565 0.45972511 0.39047951 0.29810616 0.21055046 0.14380528 0.10430258][0.41340059 0.41208339 0.38896266 0.37540224 0.37838405 0.40613571 0.4569034 0.51144516 0.52683723 0.48611814 0.40265292 0.30208704 0.21299396 0.14643256 0.1072018][0.37382203 0.36908716 0.34987521 0.34886515 0.36862728 0.41123042 0.47055447 0.52734309 0.53562033 0.48235762 0.38835156 0.28502151 0.20007615 0.13943213 0.10474167][0.35136062 0.34028786 0.32195237 0.33096433 0.36359963 0.41273788 0.46853912 0.51628125 0.51476187 0.45531595 0.36128521 0.26482308 0.19082549 0.14115235 0.11256168][0.37622675 0.35344449 0.3269636 0.33549252 0.3715575 0.41778171 0.45962298 0.48785707 0.47299004 0.41254029 0.32808277 0.24747759 0.19149365 0.15821174 0.13757001][0.44174716 0.40898871 0.37146518 0.37092036 0.39939731 0.43272626 0.45287868 0.45523477 0.42398292 0.3650502 0.29618025 0.23692009 0.20153804 0.18405753 0.16884543][0.51605678 0.47669071 0.42977181 0.41661325 0.43020487 0.44465253 0.44192013 0.42068771 0.37661752 0.32323205 0.27419275 0.23852514 0.22139479 0.21408913 0.1988596][0.55632776 0.512022 0.45806861 0.4327634 0.43014473 0.42683318 0.40787646 0.37382111 0.32781783 0.28679177 0.25938502 0.24450158 0.23956224 0.23549217 0.21587531][0.53405935 0.48391688 0.42563966 0.39241233 0.37890345 0.36550957 0.34142348 0.3074542 0.26993507 0.24498132 0.2365208 0.23620857 0.23741278 0.23301882 0.20922574][0.43574724 0.3846139 0.32979059 0.29690406 0.28081617 0.2658039 0.24511936 0.21999191 0.19675764 0.18784156 0.19341154 0.20123197 0.20357475 0.1964483 0.17001209]]...]
INFO - root - 2017-12-10 13:11:51.690318: step 17510, loss = 0.68, batch loss = 0.62 (8.1 examples/sec; 0.984 sec/batch; 86h:03m:18s remains)
INFO - root - 2017-12-10 13:12:01.696916: step 17520, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.982 sec/batch; 85h:55m:18s remains)
INFO - root - 2017-12-10 13:12:11.764314: step 17530, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.001 sec/batch; 87h:35m:52s remains)
INFO - root - 2017-12-10 13:12:21.898445: step 17540, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.007 sec/batch; 88h:07m:52s remains)
INFO - root - 2017-12-10 13:12:31.958460: step 17550, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 0.997 sec/batch; 87h:12m:45s remains)
INFO - root - 2017-12-10 13:12:41.952866: step 17560, loss = 0.69, batch loss = 0.64 (8.0 examples/sec; 1.000 sec/batch; 87h:29m:58s remains)
INFO - root - 2017-12-10 13:12:52.038475: step 17570, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 1.004 sec/batch; 87h:50m:38s remains)
INFO - root - 2017-12-10 13:13:01.920531: step 17580, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.018 sec/batch; 89h:02m:54s remains)
INFO - root - 2017-12-10 13:13:11.898326: step 17590, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.011 sec/batch; 88h:25m:13s remains)
INFO - root - 2017-12-10 13:13:21.970751: step 17600, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.035 sec/batch; 90h:31m:51s remains)
2017-12-10 13:13:22.996989: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.082800254 0.0759529 0.0694984 0.0663189 0.063767694 0.058891375 0.053930346 0.047260854 0.036599077 0.025567546 0.015238936 0.0061130077 -0.0022222807 -0.0063346177 -0.0043654065][0.067995332 0.0640983 0.061212834 0.06081586 0.059222043 0.054358307 0.049157321 0.042848591 0.033800315 0.024444208 0.0148747 0.0060726437 -0.0016624918 -0.0060386574 -0.0046183914][0.052061945 0.058482066 0.06761089 0.077515319 0.083407141 0.084072262 0.082602575 0.078460179 0.069349796 0.056303147 0.040346444 0.02536552 0.01330572 0.0061072963 0.0076646162][0.053255077 0.079641804 0.10985164 0.13695207 0.15585679 0.16641639 0.1720286 0.17217955 0.16281387 0.14268784 0.11500364 0.087694511 0.064608388 0.048252769 0.046496302][0.081249706 0.13625003 0.1937549 0.24173488 0.27547103 0.29603848 0.30863163 0.31357911 0.30472109 0.27723005 0.23713639 0.19642597 0.15940933 0.1293302 0.12139615][0.13337101 0.21973448 0.30558619 0.37454352 0.42158133 0.44905075 0.46567488 0.47360465 0.46447104 0.43020302 0.38030124 0.3290447 0.2790809 0.23489647 0.2209838][0.19797811 0.309906 0.41729873 0.50049371 0.55354595 0.58077991 0.59510219 0.60078549 0.58842593 0.54877496 0.49397707 0.43815583 0.38074836 0.32714331 0.30891204][0.24696808 0.36811075 0.48114327 0.56516182 0.61391222 0.63380694 0.6406346 0.64027357 0.62347507 0.58179909 0.52763319 0.47334889 0.41523376 0.35831508 0.33680585][0.24925236 0.35916969 0.45929432 0.53007448 0.56570059 0.57370305 0.57006508 0.56180573 0.54150659 0.50210291 0.45384365 0.40638435 0.35420746 0.30075508 0.27753496][0.19632146 0.2785553 0.35213655 0.40098765 0.42008388 0.41655111 0.4035967 0.38837969 0.3659479 0.33098078 0.29094392 0.25314161 0.21178007 0.16843338 0.14799581][0.11178226 0.15943283 0.20229368 0.22856905 0.23397762 0.22361737 0.20605884 0.18749039 0.16512066 0.135392 0.1035835 0.075468495 0.046432879 0.016627759 0.0026050645][0.031803396 0.049690165 0.066825867 0.075571157 0.072686508 0.060645707 0.043600444 0.026021466 0.0065390132 -0.017141694 -0.041253213 -0.0610982 -0.079213157 -0.096206807 -0.10292568][-0.018062405 -0.018830869 -0.017734695 -0.018882867 -0.024571482 -0.03455041 -0.04750165 -0.060760114 -0.075089663 -0.092080675 -0.10902983 -0.1222619 -0.13233036 -0.13979462 -0.14052497][-0.024708888 -0.032176491 -0.037227456 -0.04118577 -0.045742173 -0.051865198 -0.059471488 -0.067311369 -0.076032691 -0.086776771 -0.097554721 -0.10563915 -0.11042523 -0.11201737 -0.10897882][0.005723984 -0.00055043225 -0.005616487 -0.0086515071 -0.010907444 -0.013503363 -0.016633218 -0.02003287 -0.024380613 -0.030277928 -0.036224015 -0.040287152 -0.041702114 -0.040298052 -0.035790965]]...]
INFO - root - 2017-12-10 13:13:33.153116: step 17610, loss = 0.71, batch loss = 0.65 (7.7 examples/sec; 1.041 sec/batch; 91h:03m:04s remains)
INFO - root - 2017-12-10 13:13:43.041388: step 17620, loss = 0.71, batch loss = 0.65 (8.1 examples/sec; 0.988 sec/batch; 86h:24m:50s remains)
INFO - root - 2017-12-10 13:13:53.109936: step 17630, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 1.004 sec/batch; 87h:51m:11s remains)
INFO - root - 2017-12-10 13:14:03.103751: step 17640, loss = 0.71, batch loss = 0.65 (8.2 examples/sec; 0.980 sec/batch; 85h:42m:03s remains)
INFO - root - 2017-12-10 13:14:13.171676: step 17650, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.001 sec/batch; 87h:32m:03s remains)
INFO - root - 2017-12-10 13:14:23.008775: step 17660, loss = 0.70, batch loss = 0.64 (8.2 examples/sec; 0.976 sec/batch; 85h:21m:29s remains)
INFO - root - 2017-12-10 13:14:33.257391: step 17670, loss = 0.71, batch loss = 0.65 (7.8 examples/sec; 1.019 sec/batch; 89h:07m:35s remains)
INFO - root - 2017-12-10 13:14:43.399469: step 17680, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 1.004 sec/batch; 87h:46m:32s remains)
INFO - root - 2017-12-10 13:14:53.541424: step 17690, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.004 sec/batch; 87h:48m:16s remains)
INFO - root - 2017-12-10 13:15:03.513492: step 17700, loss = 0.70, batch loss = 0.64 (8.2 examples/sec; 0.981 sec/batch; 85h:47m:00s remains)
2017-12-10 13:15:04.429876: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18827909 0.18807763 0.1906262 0.19893414 0.20771492 0.20830385 0.19483632 0.17305097 0.15591386 0.15285008 0.1567616 0.1522909 0.1370641 0.12035077 0.11373327][0.21598512 0.21596186 0.2198112 0.23030768 0.24001127 0.23977549 0.2254352 0.20372532 0.18607759 0.17932086 0.17681034 0.16644734 0.14927886 0.13490906 0.13255285][0.23055683 0.22860916 0.23139252 0.24186206 0.25145411 0.25170949 0.24028718 0.22353569 0.20907585 0.19967276 0.18923591 0.17105213 0.15104294 0.14027911 0.14541724][0.23547319 0.22729233 0.22500001 0.23172344 0.23980814 0.24193732 0.23631136 0.22781861 0.21974064 0.2113914 0.19721553 0.17510821 0.15439937 0.14719191 0.15823954][0.22621618 0.21564147 0.20913374 0.21034932 0.21406616 0.21558939 0.21402939 0.21281004 0.21203418 0.20854999 0.19821046 0.18027915 0.16363508 0.15863498 0.17014749][0.2029236 0.1967288 0.19164737 0.18984015 0.18826933 0.1860033 0.18500373 0.18830486 0.19466579 0.19873606 0.19729069 0.18838829 0.17714222 0.17119195 0.17714104][0.18174849 0.18394892 0.18300995 0.17907915 0.17150348 0.16367024 0.16066332 0.16595575 0.17804469 0.19033967 0.19885547 0.19908318 0.19213365 0.18398432 0.18329735][0.1740528 0.18398881 0.18544987 0.17732807 0.16312717 0.15044679 0.14512281 0.15035777 0.16494448 0.18286692 0.19920979 0.20738308 0.20548746 0.19805104 0.19460827][0.17273685 0.18637066 0.18721534 0.17439304 0.15500318 0.13926408 0.1320803 0.1354596 0.14916611 0.16912453 0.19165766 0.2091756 0.21683504 0.2151435 0.21129014][0.16015027 0.17388871 0.17441307 0.16096982 0.14079326 0.1242971 0.11590934 0.11709267 0.12905756 0.14914703 0.17544438 0.20038547 0.21717982 0.22185481 0.21775071][0.134607 0.14574151 0.14837076 0.14042415 0.12543905 0.11141542 0.10356048 0.10450274 0.11615042 0.13531907 0.16046372 0.18512547 0.20316096 0.2089781 0.20293997][0.11100326 0.11877041 0.12297027 0.12151862 0.11404163 0.10530214 0.10101743 0.10487401 0.11768981 0.13413043 0.15272295 0.16972551 0.18207273 0.18516453 0.17738843][0.1051804 0.10708884 0.10716128 0.10641667 0.10345683 0.10038183 0.10189901 0.11108637 0.12589079 0.13876715 0.14879587 0.15596507 0.16108403 0.16154636 0.15409276][0.11696297 0.11100803 0.10114594 0.094544575 0.0915057 0.092391647 0.10017809 0.11564368 0.1335829 0.14444371 0.14727223 0.14551218 0.14403671 0.14271027 0.1365864][0.13229999 0.11903906 0.099477313 0.086213626 0.081839532 0.086041227 0.0999884 0.12124479 0.14209086 0.15153031 0.14804117 0.13747361 0.12850547 0.12348853 0.11679752]]...]
INFO - root - 2017-12-10 13:15:14.319520: step 17710, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.001 sec/batch; 87h:32m:03s remains)
INFO - root - 2017-12-10 13:15:24.335566: step 17720, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.985 sec/batch; 86h:05m:17s remains)
INFO - root - 2017-12-10 13:15:34.145734: step 17730, loss = 0.70, batch loss = 0.64 (9.0 examples/sec; 0.884 sec/batch; 77h:18m:21s remains)
INFO - root - 2017-12-10 13:15:44.264834: step 17740, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.013 sec/batch; 88h:35m:29s remains)
INFO - root - 2017-12-10 13:15:54.262355: step 17750, loss = 0.72, batch loss = 0.66 (7.8 examples/sec; 1.021 sec/batch; 89h:14m:34s remains)
INFO - root - 2017-12-10 13:16:04.296521: step 17760, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.029 sec/batch; 89h:56m:13s remains)
INFO - root - 2017-12-10 13:16:14.425520: step 17770, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.045 sec/batch; 91h:21m:47s remains)
INFO - root - 2017-12-10 13:16:24.464538: step 17780, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.013 sec/batch; 88h:34m:27s remains)
INFO - root - 2017-12-10 13:16:34.540837: step 17790, loss = 0.70, batch loss = 0.65 (8.1 examples/sec; 0.988 sec/batch; 86h:21m:08s remains)
INFO - root - 2017-12-10 13:16:44.431179: step 17800, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.013 sec/batch; 88h:32m:48s remains)
2017-12-10 13:16:45.501073: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026885277 0.031376179 0.028421547 0.01946597 0.009529368 0.0015735484 -0.0048404392 -0.010824469 -0.015955187 -0.018348832 -0.018011866 -0.015071181 -0.0096886214 -0.0032150112 0.0022229778][0.05409677 0.060341362 0.055542026 0.042246256 0.027706083 0.016256519 0.0072755357 -0.00097047287 -0.0079427129 -0.011031811 -0.0096593155 -0.0037724683 0.0063155894 0.017625326 0.025778783][0.079827033 0.086936288 0.07921271 0.061155941 0.042253353 0.028088786 0.0180143 0.0093092853 0.0020190692 -0.00061723043 0.0026440497 0.011994612 0.026393956 0.041059222 0.049772494][0.097621828 0.10380416 0.092902668 0.07118424 0.049724631 0.034706209 0.025619704 0.01858581 0.012566834 0.010984171 0.015556284 0.026561191 0.041960847 0.056077491 0.06221747][0.10678938 0.11173685 0.098945789 0.075889796 0.053996213 0.039500173 0.032416057 0.028202316 0.02432364 0.023412224 0.027409559 0.036963966 0.04954876 0.059792437 0.061797891][0.10895934 0.11302906 0.10029115 0.078322604 0.057709068 0.044472106 0.0394814 0.038286645 0.0365378 0.035177186 0.036646068 0.042588837 0.050660137 0.056379747 0.055099856][0.10476346 0.10835125 0.0967709 0.077407263 0.059356283 0.0481029 0.045112789 0.046373278 0.045904025 0.042939082 0.040573731 0.042480778 0.04689458 0.050323106 0.049128477][0.0969792 0.1000296 0.089908771 0.073715486 0.058835637 0.049768057 0.048110165 0.050450526 0.049865503 0.044393953 0.037830837 0.036170617 0.038664971 0.042925473 0.0457553][0.091455095 0.093192644 0.083646737 0.069826558 0.057269786 0.049182434 0.047059294 0.048546497 0.046872154 0.039222069 0.02954313 0.025396189 0.027697839 0.035543658 0.045377254][0.094047077 0.093634948 0.083008736 0.069257252 0.05663795 0.04730463 0.042656351 0.041988838 0.039417882 0.031411529 0.021078618 0.016233971 0.019775221 0.032603573 0.050413921][0.10651878 0.1034395 0.089585453 0.072761096 0.057392526 0.045171343 0.036971759 0.033576243 0.030312231 0.023617158 0.015065401 0.011601735 0.017391965 0.034855347 0.059469476][0.12394599 0.118201 0.099343449 0.076796316 0.056943178 0.041948404 0.03142089 0.02618847 0.022452613 0.017333839 0.011090957 0.0095023271 0.017241931 0.0377904 0.067315593][0.14142333 0.13368897 0.10981689 0.080794469 0.055963468 0.0386979 0.027357103 0.021642495 0.017915258 0.013992175 0.0093789278 0.0086885542 0.016752739 0.037865516 0.069848835][0.14867732 0.13924113 0.11217406 0.078928679 0.051022541 0.032763891 0.0218449 0.016758347 0.01379946 0.011173987 0.0079066334 0.0073520513 0.014035737 0.032859754 0.063862823][0.13487881 0.12427169 0.097507589 0.064781725 0.0377052 0.020680055 0.011235805 0.0075082229 0.0060866359 0.0054373755 0.0040517887 0.0039076228 0.0086706532 0.023244267 0.049688525]]...]
INFO - root - 2017-12-10 13:16:55.220689: step 17810, loss = 0.70, batch loss = 0.64 (8.2 examples/sec; 0.975 sec/batch; 85h:11m:38s remains)
INFO - root - 2017-12-10 13:17:05.276137: step 17820, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 0.998 sec/batch; 87h:12m:39s remains)
INFO - root - 2017-12-10 13:17:15.329362: step 17830, loss = 0.71, batch loss = 0.65 (8.7 examples/sec; 0.919 sec/batch; 80h:19m:46s remains)
INFO - root - 2017-12-10 13:17:25.366286: step 17840, loss = 0.71, batch loss = 0.65 (8.1 examples/sec; 0.987 sec/batch; 86h:16m:11s remains)
INFO - root - 2017-12-10 13:17:35.475862: step 17850, loss = 0.72, batch loss = 0.66 (7.9 examples/sec; 1.011 sec/batch; 88h:23m:58s remains)
INFO - root - 2017-12-10 13:17:45.390660: step 17860, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 0.998 sec/batch; 87h:11m:45s remains)
INFO - root - 2017-12-10 13:17:55.346322: step 17870, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.001 sec/batch; 87h:26m:28s remains)
INFO - root - 2017-12-10 13:18:05.355237: step 17880, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.018 sec/batch; 88h:57m:23s remains)
INFO - root - 2017-12-10 13:18:15.231351: step 17890, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.021 sec/batch; 89h:12m:10s remains)
INFO - root - 2017-12-10 13:18:25.272536: step 17900, loss = 0.70, batch loss = 0.64 (8.2 examples/sec; 0.977 sec/batch; 85h:24m:08s remains)
2017-12-10 13:18:26.249070: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12439466 0.14320862 0.14127105 0.12028234 0.08582259 0.051224414 0.029916134 0.024373934 0.031628892 0.0434443 0.053839348 0.062981464 0.071878567 0.080247983 0.080593348][0.13566999 0.15531623 0.15322666 0.13140334 0.093618855 0.05186813 0.021074163 0.0066700005 0.005615517 0.01032864 0.017447075 0.027858874 0.041552205 0.054477826 0.059256162][0.14049688 0.1607253 0.16152653 0.14503706 0.11192584 0.070471972 0.034842759 0.012917846 0.0030259087 0.00012430573 0.004897356 0.019432437 0.041639674 0.062983625 0.075388983][0.13973637 0.16106205 0.16798446 0.16277039 0.142371 0.10990623 0.07669384 0.051775143 0.036049798 0.028333925 0.033746466 0.055309948 0.088727072 0.12137382 0.14341721][0.13424261 0.15552661 0.16997439 0.18042545 0.17984007 0.16492532 0.14220737 0.12002219 0.10232499 0.092337579 0.099303313 0.12727799 0.17009217 0.21217582 0.24204673][0.12817737 0.14817116 0.16898435 0.19563954 0.2174423 0.22408628 0.21615279 0.19966668 0.18178266 0.16978353 0.17563038 0.20471382 0.25038248 0.29606026 0.32921383][0.1293375 0.14781992 0.17230609 0.21082622 0.25005949 0.27450287 0.27947181 0.26816604 0.25053567 0.2362082 0.23777367 0.26155844 0.30210793 0.34406906 0.3748][0.13745183 0.1545112 0.17974226 0.22358021 0.27158642 0.30592254 0.31888646 0.31107661 0.29373348 0.27664989 0.27131617 0.28437355 0.31291643 0.3444654 0.36705223][0.14855085 0.16437323 0.18805312 0.23109245 0.27929437 0.3148714 0.32922873 0.32158849 0.3032234 0.28281954 0.26946834 0.26983851 0.28424791 0.30317312 0.31580362][0.15708256 0.1705983 0.19005355 0.2264677 0.26729995 0.29679018 0.30704993 0.29672584 0.276125 0.25261286 0.23280738 0.22293897 0.22596894 0.23471056 0.2397221][0.15547344 0.16559964 0.178778 0.20458959 0.23329017 0.25271583 0.25662926 0.24310024 0.22052883 0.19526304 0.17236979 0.15699939 0.15374345 0.15700568 0.1585937][0.13544643 0.14115359 0.14795798 0.1626434 0.17887887 0.18870066 0.18782465 0.17353949 0.15165883 0.12797005 0.10657606 0.091401644 0.087516256 0.090249635 0.092224509][0.099863991 0.10007587 0.10047406 0.1051019 0.11121657 0.11481068 0.11325213 0.10313897 0.087112881 0.070194371 0.055207424 0.044778589 0.043721788 0.048262689 0.051910982][0.062174447 0.056474231 0.050489519 0.046314958 0.044601563 0.045431674 0.047473542 0.046387073 0.041150209 0.035201028 0.029484278 0.024851669 0.025758822 0.030107616 0.033467948][0.031707559 0.022110196 0.011600275 0.0014160412 -0.0050532594 -0.0041617388 0.0038887023 0.013745446 0.020841002 0.026384633 0.029137708 0.027728086 0.02643932 0.025932347 0.025185343]]...]
INFO - root - 2017-12-10 13:18:36.282908: step 17910, loss = 0.69, batch loss = 0.64 (8.1 examples/sec; 0.991 sec/batch; 86h:34m:23s remains)
INFO - root - 2017-12-10 13:18:46.328300: step 17920, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 0.995 sec/batch; 86h:54m:28s remains)
INFO - root - 2017-12-10 13:18:56.297009: step 17930, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 0.994 sec/batch; 86h:53m:24s remains)
INFO - root - 2017-12-10 13:19:06.357550: step 17940, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.001 sec/batch; 87h:26m:50s remains)
INFO - root - 2017-12-10 13:19:16.459304: step 17950, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.994 sec/batch; 86h:48m:56s remains)
INFO - root - 2017-12-10 13:19:26.422024: step 17960, loss = 0.69, batch loss = 0.64 (8.0 examples/sec; 0.995 sec/batch; 86h:58m:31s remains)
INFO - root - 2017-12-10 13:19:36.475713: step 17970, loss = 0.70, batch loss = 0.64 (8.2 examples/sec; 0.981 sec/batch; 85h:42m:38s remains)
INFO - root - 2017-12-10 13:19:46.300232: step 17980, loss = 0.69, batch loss = 0.64 (8.3 examples/sec; 0.963 sec/batch; 84h:10m:28s remains)
INFO - root - 2017-12-10 13:19:56.411203: step 17990, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.005 sec/batch; 87h:50m:00s remains)
INFO - root - 2017-12-10 13:20:06.589613: step 18000, loss = 0.68, batch loss = 0.63 (7.9 examples/sec; 1.014 sec/batch; 88h:36m:05s remains)
2017-12-10 13:20:07.698837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.08741729 -0.069852561 -0.038608171 -0.00041853954 0.038319178 0.072184652 0.089997165 0.088586845 0.07522852 0.05738141 0.045225862 0.049590923 0.067712113 0.090779088 0.11172496][-0.087660074 -0.065509096 -0.026889706 0.02100743 0.071779504 0.11831925 0.14585425 0.1493239 0.13701545 0.1167333 0.098020189 0.092759587 0.10016372 0.11358389 0.12713158][-0.0853747 -0.059782896 -0.016496431 0.038723398 0.10019451 0.15902455 0.19711946 0.2070085 0.19731215 0.17475232 0.14728051 0.12796101 0.11948077 0.11869847 0.12164395][-0.083940387 -0.056590177 -0.010191437 0.051446669 0.12359128 0.19446035 0.24289107 0.25913888 0.25193667 0.226762 0.18937035 0.15407483 0.1278085 0.11076471 0.10089245][-0.082288884 -0.053543285 -0.0038460391 0.064369418 0.14697233 0.22849131 0.2855356 0.3064059 0.29938284 0.27056149 0.2234996 0.17372261 0.13184184 0.10051932 0.079298042][-0.080637872 -0.050553389 0.0026914864 0.078159407 0.17090338 0.26180139 0.32550687 0.34831369 0.3373957 0.30156526 0.24464788 0.18461333 0.13510421 0.099249788 0.075808719][-0.0793692 -0.04891 0.0075602611 0.089936882 0.19154951 0.29052463 0.36018091 0.38366061 0.36604556 0.31938958 0.2504501 0.18193826 0.12988688 0.0972419 0.079999655][-0.077533387 -0.045121305 0.017624116 0.11104261 0.22541927 0.33542749 0.41218081 0.43545333 0.40734991 0.34440449 0.25980672 0.18363722 0.13323468 0.10882676 0.10183676][-0.074618489 -0.038794555 0.032657459 0.13982816 0.26949507 0.3925074 0.47652155 0.49794665 0.45663077 0.37558582 0.27578822 0.1944351 0.14927024 0.13654681 0.14220884][-0.072646342 -0.034443993 0.042201262 0.15821673 0.29767933 0.42902249 0.51728994 0.53635305 0.48469388 0.39032003 0.28103068 0.19829887 0.15999006 0.15948023 0.17795621][-0.072061047 -0.034049615 0.041724827 0.15751173 0.296629 0.42746577 0.5145722 0.53093058 0.47488254 0.37537611 0.26436305 0.18415783 0.15274918 0.1624895 0.19116102][-0.07258147 -0.037115872 0.032633442 0.13895704 0.26646432 0.38541511 0.46328798 0.47467196 0.41910037 0.32300192 0.21863633 0.14562438 0.12030894 0.13582502 0.16874307][-0.074977741 -0.043602269 0.016727906 0.10731898 0.21556239 0.31469432 0.37680984 0.3806628 0.32829878 0.24237992 0.15163124 0.09029793 0.071837947 0.090006121 0.12204659][-0.081769928 -0.058668997 -0.013618401 0.054029036 0.1355166 0.20872986 0.25187546 0.24933007 0.20562294 0.13767542 0.067690969 0.022255242 0.011415448 0.02938519 0.056400638][-0.092062369 -0.08121901 -0.056687966 -0.017821955 0.031211287 0.07503397 0.099207446 0.093842961 0.0645323 0.020943547 -0.022792282 -0.049081679 -0.05158522 -0.035449572 -0.01565557]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 13:20:17.762198: step 18010, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.029 sec/batch; 89h:53m:46s remains)
INFO - root - 2017-12-10 13:20:27.777034: step 18020, loss = 0.70, batch loss = 0.64 (8.2 examples/sec; 0.978 sec/batch; 85h:24m:39s remains)
INFO - root - 2017-12-10 13:20:37.830638: step 18030, loss = 0.69, batch loss = 0.63 (8.1 examples/sec; 0.992 sec/batch; 86h:38m:21s remains)
INFO - root - 2017-12-10 13:20:47.957954: step 18040, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.010 sec/batch; 88h:15m:17s remains)
INFO - root - 2017-12-10 13:20:57.949618: step 18050, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.004 sec/batch; 87h:39m:32s remains)
INFO - root - 2017-12-10 13:21:07.934643: step 18060, loss = 0.70, batch loss = 0.64 (8.8 examples/sec; 0.908 sec/batch; 79h:20m:48s remains)
INFO - root - 2017-12-10 13:21:18.128210: step 18070, loss = 0.71, batch loss = 0.66 (8.1 examples/sec; 0.989 sec/batch; 86h:21m:56s remains)
INFO - root - 2017-12-10 13:21:28.213427: step 18080, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.021 sec/batch; 89h:12m:50s remains)
INFO - root - 2017-12-10 13:21:38.275511: step 18090, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.018 sec/batch; 88h:54m:32s remains)
INFO - root - 2017-12-10 13:21:48.309669: step 18100, loss = 0.71, batch loss = 0.65 (7.9 examples/sec; 1.007 sec/batch; 87h:54m:11s remains)
2017-12-10 13:21:49.248776: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.069994986 0.087176047 0.10171628 0.10947733 0.11039136 0.10631153 0.10111926 0.095430285 0.090461262 0.088699169 0.093867496 0.10539057 0.11738861 0.12776712 0.13959292][0.10555287 0.12748531 0.1468447 0.15937833 0.16448171 0.1628051 0.15908667 0.15514764 0.15253139 0.15300651 0.15947042 0.17060468 0.18050639 0.18757093 0.19597156][0.11865482 0.14334306 0.16551057 0.18252125 0.19397718 0.1982128 0.20022707 0.20423871 0.21234497 0.2230285 0.23384929 0.24119546 0.24160375 0.23571672 0.22864829][0.12275352 0.1492624 0.17259565 0.19360484 0.21255222 0.22421131 0.23368081 0.24961537 0.2759572 0.3047865 0.32280532 0.32347664 0.3086246 0.28264725 0.25054279][0.14235377 0.17250417 0.19795768 0.22392653 0.25080213 0.26876298 0.28313759 0.3093124 0.35514638 0.40424392 0.4288384 0.41995755 0.38677827 0.33867812 0.27938315][0.19254538 0.23101106 0.26354876 0.29743734 0.33228493 0.35342115 0.36647713 0.39436108 0.45125937 0.51412952 0.54169178 0.52383864 0.47602469 0.41164124 0.33235916][0.25757965 0.31192359 0.35919958 0.40497437 0.44758788 0.46879324 0.47423056 0.49229231 0.54437625 0.60645854 0.63073879 0.6076144 0.55500168 0.48699546 0.40222642][0.30259535 0.3746393 0.44019607 0.49906832 0.54787612 0.56700295 0.5617975 0.56220853 0.59477443 0.64107317 0.65639067 0.63352448 0.58804566 0.53043914 0.4558537][0.30122975 0.38484031 0.46411738 0.53162867 0.58232373 0.59821206 0.5842976 0.56812751 0.577064 0.60164016 0.60737514 0.58948064 0.55771577 0.51659024 0.45928797][0.25856337 0.34041008 0.42072445 0.48665956 0.53229916 0.54403955 0.52689457 0.50330055 0.49673021 0.50541657 0.50703412 0.49748263 0.4793753 0.45140523 0.40753978][0.20520024 0.2729511 0.34024373 0.39320779 0.42636159 0.43188998 0.41547903 0.39432827 0.38500422 0.3890008 0.39320365 0.39289761 0.38482648 0.36385062 0.326375][0.16849764 0.21632652 0.26206112 0.29519698 0.31132075 0.30809754 0.29253632 0.27840382 0.27491611 0.28205788 0.29140821 0.29768172 0.29514778 0.27774459 0.24403101][0.15478034 0.18442139 0.20899783 0.22249605 0.22225453 0.20989479 0.19334699 0.18483911 0.18824913 0.20008704 0.21285267 0.22128643 0.22028829 0.20552646 0.17638578][0.16044404 0.17989168 0.19089775 0.19123121 0.18071714 0.16313533 0.14530143 0.13806579 0.14294273 0.15449813 0.16521648 0.17036967 0.16700764 0.15385847 0.12995334][0.1775846 0.1925465 0.19689876 0.19134732 0.17890388 0.16363579 0.14816001 0.1404144 0.14130943 0.1466929 0.15039286 0.14863187 0.14026318 0.12685093 0.10747378]]...]
INFO - root - 2017-12-10 13:21:59.399183: step 18110, loss = 0.70, batch loss = 0.65 (8.0 examples/sec; 1.002 sec/batch; 87h:32m:05s remains)
INFO - root - 2017-12-10 13:22:09.513638: step 18120, loss = 0.72, batch loss = 0.66 (8.0 examples/sec; 0.997 sec/batch; 87h:02m:53s remains)
INFO - root - 2017-12-10 13:22:19.471738: step 18130, loss = 0.68, batch loss = 0.62 (8.1 examples/sec; 0.991 sec/batch; 86h:32m:37s remains)
INFO - root - 2017-12-10 13:22:29.490907: step 18140, loss = 0.69, batch loss = 0.64 (8.0 examples/sec; 0.994 sec/batch; 86h:47m:41s remains)
INFO - root - 2017-12-10 13:22:39.552794: step 18150, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.985 sec/batch; 85h:59m:02s remains)
INFO - root - 2017-12-10 13:22:49.731881: step 18160, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 0.996 sec/batch; 87h:00m:10s remains)
INFO - root - 2017-12-10 13:22:59.980775: step 18170, loss = 0.70, batch loss = 0.64 (8.1 examples/sec; 0.994 sec/batch; 86h:46m:14s remains)
INFO - root - 2017-12-10 13:23:10.087958: step 18180, loss = 0.68, batch loss = 0.62 (7.9 examples/sec; 1.013 sec/batch; 88h:28m:54s remains)
INFO - root - 2017-12-10 13:23:20.237668: step 18190, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.017 sec/batch; 88h:45m:44s remains)
INFO - root - 2017-12-10 13:23:30.290589: step 18200, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.004 sec/batch; 87h:38m:51s remains)
2017-12-10 13:23:31.297390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.037810735 -0.030007737 -0.018749658 -0.0066426834 0.0004090071 -0.0005195103 -0.00897462 -0.017843295 -0.026738575 -0.03759177 -0.048458237 -0.057644196 -0.062107228 -0.063486628 -0.062900692][0.026229737 0.040757615 0.061149355 0.08505106 0.10304704 0.11005982 0.10375243 0.093400925 0.078913592 0.058582578 0.035587057 0.013211492 -0.001437069 -0.010571268 -0.015290602][0.11331728 0.13716905 0.1696673 0.20939629 0.24305426 0.26266044 0.26269808 0.2539542 0.23402186 0.20131327 0.16170518 0.12124267 0.091043979 0.067809425 0.052423246][0.20294791 0.23761784 0.28474894 0.34372273 0.39750668 0.43442187 0.44575506 0.44178057 0.4169167 0.37180617 0.31559315 0.25542778 0.20547602 0.1629357 0.13215791][0.26992291 0.31645375 0.3808147 0.46163443 0.53914869 0.59638 0.62151212 0.62337285 0.59504247 0.54190385 0.47541228 0.40099907 0.33251503 0.26817617 0.21727003][0.30185053 0.36155459 0.44508469 0.54945558 0.65281141 0.73151678 0.77052432 0.77696747 0.74495465 0.68668807 0.61642689 0.53598815 0.45535776 0.37280643 0.30219406][0.29600245 0.36726367 0.46716896 0.59102076 0.71615094 0.81370628 0.86592907 0.87768549 0.84427512 0.78535891 0.71803385 0.64015394 0.55623788 0.46460542 0.38149172][0.26221365 0.34253404 0.45320162 0.58696014 0.72273773 0.83095235 0.89312589 0.91042942 0.87921977 0.82593828 0.77016968 0.70522469 0.62948126 0.54073805 0.45381746][0.22497781 0.31220609 0.42745841 0.5598138 0.69111234 0.79636395 0.85907513 0.87732887 0.84985012 0.80839086 0.77452189 0.73573732 0.682402 0.6093421 0.52776569][0.20651928 0.29859155 0.41228583 0.53304887 0.64576906 0.73375422 0.784725 0.79558033 0.76926124 0.74157715 0.73553127 0.7312957 0.70996463 0.66148919 0.5932976][0.21609698 0.31098714 0.41846961 0.52079242 0.60538352 0.66517919 0.69384283 0.688949 0.65859276 0.64161146 0.66132545 0.69024247 0.70139784 0.68042445 0.63089406][0.24222806 0.335909 0.43228018 0.51144224 0.56285524 0.58814806 0.58798808 0.56256479 0.52561539 0.516543 0.556276 0.61081457 0.64775527 0.65046728 0.61923927][0.25548488 0.340929 0.42147946 0.47713727 0.49837378 0.49193934 0.46383071 0.41817752 0.37437859 0.3699894 0.42138985 0.49081704 0.54327 0.56162894 0.54403651][0.23404336 0.3029902 0.36326197 0.39771014 0.39759755 0.3705548 0.32421833 0.26568678 0.21770179 0.21434276 0.26707643 0.33865309 0.39476782 0.41939777 0.40987724][0.17243628 0.21866216 0.2562449 0.27287194 0.26102504 0.22645891 0.17538361 0.11512844 0.067695692 0.061884977 0.10563781 0.16728857 0.21712591 0.24166557 0.23765682]]...]
INFO - root - 2017-12-10 13:23:41.313829: step 18210, loss = 0.71, batch loss = 0.65 (8.0 examples/sec; 1.001 sec/batch; 87h:22m:41s remains)
INFO - root - 2017-12-10 13:23:51.537693: step 18220, loss = 0.70, batch loss = 0.64 (7.8 examples/sec; 1.025 sec/batch; 89h:28m:32s remains)
INFO - root - 2017-12-10 13:24:01.586971: step 18230, loss = 0.67, batch loss = 0.61 (7.8 examples/sec; 1.025 sec/batch; 89h:27m:48s remains)
INFO - root - 2017-12-10 13:24:11.507320: step 18240, loss = 0.69, batch loss = 0.63 (8.2 examples/sec; 0.978 sec/batch; 85h:20m:16s remains)
INFO - root - 2017-12-10 13:24:21.605050: step 18250, loss = 0.70, batch loss = 0.64 (8.3 examples/sec; 0.966 sec/batch; 84h:20m:56s remains)
INFO - root - 2017-12-10 13:24:31.689452: step 18260, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 1.001 sec/batch; 87h:21m:31s remains)
INFO - root - 2017-12-10 13:24:41.791365: step 18270, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 1.005 sec/batch; 87h:43m:54s remains)
INFO - root - 2017-12-10 13:24:51.820725: step 18280, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.014 sec/batch; 88h:28m:03s remains)
INFO - root - 2017-12-10 13:25:01.707469: step 18290, loss = 0.69, batch loss = 0.64 (8.1 examples/sec; 0.988 sec/batch; 86h:12m:41s remains)
INFO - root - 2017-12-10 13:25:11.710515: step 18300, loss = 0.70, batch loss = 0.65 (8.0 examples/sec; 0.996 sec/batch; 86h:53m:57s remains)
2017-12-10 13:25:12.736881: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.095286272 0.091110937 0.074801117 0.051257946 0.02719491 0.0046270518 -0.012620847 -0.019896569 -0.02434979 -0.035708763 -0.057203755 -0.081343688 -0.094679728 -0.08615616 -0.051202919][0.07271868 0.063771524 0.0531197 0.045652226 0.045165248 0.046998814 0.04781555 0.048888274 0.042302568 0.019716812 -0.018944954 -0.06102398 -0.088374749 -0.088507459 -0.055714272][0.054376949 0.040969942 0.037196472 0.049321536 0.078339666 0.11169015 0.13762769 0.15213755 0.14492819 0.10795157 0.04524887 -0.022689281 -0.071151324 -0.085338451 -0.059310067][0.050088037 0.035582498 0.040711269 0.073380291 0.13278317 0.19993442 0.2546885 0.28600073 0.27964973 0.22693382 0.13567573 0.035015706 -0.041376892 -0.074628606 -0.058980413][0.057110317 0.047823269 0.065632358 0.12069149 0.21018872 0.31018192 0.39364961 0.44175404 0.43540379 0.36536849 0.24370621 0.10774117 5.2726748e-05 -0.055471309 -0.053064328][0.075025029 0.0759619 0.10969332 0.18873642 0.30720502 0.43676779 0.54491091 0.60560477 0.59506172 0.504956 0.35291231 0.18371806 0.046798944 -0.0299795 -0.041242938][0.09785378 0.11082217 0.16087548 0.26308411 0.40751496 0.56106341 0.686593 0.75256437 0.73222047 0.62019026 0.4411332 0.24604914 0.087627955 -0.0042826389 -0.026059914][0.12562297 0.14816064 0.20898362 0.32550934 0.48395917 0.64792866 0.77866638 0.84211254 0.81080961 0.683395 0.4900285 0.28407097 0.11691492 0.017691422 -0.01214264][0.16006659 0.18835758 0.25157556 0.36817989 0.521375 0.67504668 0.7933737 0.8447597 0.80482668 0.67622852 0.490723 0.29757684 0.14066517 0.044345081 0.0084017031][0.21044941 0.24083535 0.29675624 0.39680389 0.52315354 0.64450991 0.73309141 0.76487768 0.72165442 0.60923755 0.45693159 0.30369586 0.1792428 0.098281957 0.058653798][0.27377447 0.30292493 0.34210679 0.40996549 0.4914819 0.5645026 0.61267531 0.62230533 0.58339143 0.50326467 0.40458515 0.31187209 0.23768623 0.18428104 0.14566118][0.34060225 0.36750051 0.38570258 0.41482991 0.44523677 0.46589726 0.47290888 0.463542 0.4362056 0.39686152 0.35807833 0.32856846 0.30571175 0.28053433 0.2436028][0.40474162 0.4273819 0.42495814 0.41811073 0.40296504 0.378358 0.35095638 0.32778865 0.31485683 0.31494167 0.32946694 0.35253358 0.36996475 0.36509266 0.32765695][0.43642446 0.45429257 0.4365043 0.40514088 0.36066535 0.30721602 0.25753728 0.22566408 0.22396651 0.25223044 0.30277711 0.35972914 0.40226033 0.4102723 0.37427625][0.41584539 0.4284772 0.40356562 0.36323 0.31002384 0.24827647 0.19179709 0.15724427 0.16047013 0.1995419 0.2623899 0.32946849 0.37945712 0.39385888 0.36449173]]...]
INFO - root - 2017-12-10 13:25:22.764160: step 18310, loss = 0.70, batch loss = 0.64 (8.0 examples/sec; 0.995 sec/batch; 86h:49m:45s remains)
INFO - root - 2017-12-10 13:25:32.767893: step 18320, loss = 0.72, batch loss = 0.66 (7.7 examples/sec; 1.038 sec/batch; 90h:37m:50s remains)
INFO - root - 2017-12-10 13:25:42.672467: step 18330, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.030 sec/batch; 89h:50m:43s remains)
INFO - root - 2017-12-10 13:25:52.733260: step 18340, loss = 0.68, batch loss = 0.62 (8.0 examples/sec; 1.000 sec/batch; 87h:16m:47s remains)
INFO - root - 2017-12-10 13:26:01.810225: step 18350, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.751 sec/batch; 65h:33m:30s remains)
INFO - root - 2017-12-10 13:26:09.772778: step 18360, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 69h:05m:25s remains)
INFO - root - 2017-12-10 13:26:17.570792: step 18370, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 68h:48m:33s remains)
INFO - root - 2017-12-10 13:26:25.425312: step 18380, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 66h:28m:12s remains)
INFO - root - 2017-12-10 13:26:33.355690: step 18390, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.828 sec/batch; 72h:16m:40s remains)
INFO - root - 2017-12-10 13:26:41.255109: step 18400, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.801 sec/batch; 69h:53m:46s remains)
2017-12-10 13:26:42.039643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0353779 -0.033225767 -0.031886905 -0.031524539 -0.031271387 -0.029628813 -0.025058473 -0.020578871 -0.018529491 -0.020701744 -0.026383929 -0.035953343 -0.04856905 -0.059512764 -0.064986765][0.0074999509 0.01680851 0.021081991 0.021120327 0.018170141 0.016984517 0.020874936 0.025322346 0.0257443 0.02015857 0.010553939 -0.0059121088 -0.028754342 -0.050214931 -0.062359896][0.080941588 0.10250796 0.11249639 0.11196726 0.10230912 0.094145492 0.094151355 0.096221529 0.093062811 0.082208119 0.067290269 0.041522153 0.0045147175 -0.031478375 -0.053040706][0.18283135 0.22600761 0.24949971 0.25278997 0.23697834 0.22069456 0.2154018 0.21369784 0.2051052 0.18728077 0.16513816 0.12521407 0.065475456 0.0049021686 -0.033944745][0.29451436 0.36618674 0.41001192 0.4219659 0.40230298 0.3785657 0.36750904 0.3593508 0.34097394 0.31223658 0.28031176 0.22325106 0.13571551 0.045803547 -0.013173321][0.39778352 0.4964633 0.559998 0.58024371 0.55750203 0.5279727 0.5125677 0.49686682 0.46614438 0.42414466 0.38130596 0.30770561 0.19477755 0.079710864 0.0044046249][0.47359103 0.59002692 0.66606194 0.68992537 0.66351539 0.63036931 0.61519361 0.59590358 0.55561125 0.50184095 0.4490242 0.36318237 0.2333407 0.10280461 0.018174607][0.50298852 0.62401932 0.70228738 0.7237941 0.69293213 0.65860945 0.64828259 0.63155973 0.589028 0.53002137 0.47183847 0.38058096 0.24489993 0.1107365 0.024620965][0.47833043 0.58952469 0.6588493 0.67227584 0.63608563 0.6008448 0.59579331 0.58494854 0.54738122 0.49154881 0.43572977 0.34971687 0.222481 0.0977941 0.01853862][0.39853862 0.48472953 0.53458065 0.53655106 0.49753609 0.46384171 0.46341568 0.45958504 0.43306613 0.38851619 0.3432228 0.27327207 0.16816603 0.064651921 -0.0002515793][0.28509253 0.33952469 0.3668327 0.3586334 0.32113034 0.29170746 0.29468685 0.29755116 0.28431222 0.25498077 0.22443138 0.17607646 0.099831127 0.022933908 -0.024322862][0.16406995 0.190573 0.19952281 0.18581721 0.15472344 0.13282314 0.13902201 0.14737979 0.14518249 0.12891862 0.11084435 0.081214011 0.031164579 -0.020675236 -0.05073553][0.057889596 0.065748088 0.06388253 0.049744342 0.027444674 0.013796504 0.021832185 0.032717545 0.03690964 0.02874209 0.018167954 0.0012952205 -0.028499154 -0.059403658 -0.074815914][-0.01521437 -0.016160974 -0.021158941 -0.031866532 -0.045354821 -0.052581821 -0.045101359 -0.035175934 -0.029624257 -0.034080509 -0.040785309 -0.049692776 -0.065366074 -0.081002556 -0.086057171][-0.053312566 -0.056655902 -0.060129665 -0.065913774 -0.072200775 -0.074872687 -0.068932943 -0.061521579 -0.057312783 -0.060430735 -0.065112658 -0.0698691 -0.077172779 -0.08365903 -0.083355889]]...]
INFO - root - 2017-12-10 13:26:49.847968: step 18410, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 66h:26m:17s remains)
INFO - root - 2017-12-10 13:26:57.596787: step 18420, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.811 sec/batch; 70h:45m:31s remains)
INFO - root - 2017-12-10 13:27:05.489434: step 18430, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 69h:06m:10s remains)
INFO - root - 2017-12-10 13:27:13.350794: step 18440, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 68h:57m:30s remains)
INFO - root - 2017-12-10 13:27:21.035911: step 18450, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 68h:22m:01s remains)
INFO - root - 2017-12-10 13:27:28.888383: step 18460, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 68h:15m:07s remains)
INFO - root - 2017-12-10 13:27:36.735380: step 18470, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 69h:05m:26s remains)
INFO - root - 2017-12-10 13:27:44.581441: step 18480, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 67h:06m:14s remains)
INFO - root - 2017-12-10 13:27:52.453045: step 18490, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 67h:41m:08s remains)
INFO - root - 2017-12-10 13:28:00.353698: step 18500, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 67h:55m:23s remains)
2017-12-10 13:28:01.216769: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21555406 0.2022157 0.18538266 0.17791286 0.18123239 0.19109815 0.19968511 0.198054 0.18538755 0.15675877 0.11972369 0.089395031 0.0742999 0.074056551 0.078350373][0.25176537 0.23391199 0.2088784 0.19192252 0.18475097 0.18226337 0.17812522 0.16596037 0.1491628 0.12594552 0.099367052 0.07780844 0.06753844 0.069044575 0.075291023][0.24502108 0.2260775 0.19804199 0.17585924 0.15995036 0.14636809 0.13226956 0.11463118 0.099756077 0.087617971 0.076801486 0.069193743 0.069601946 0.079239815 0.092664249][0.20460242 0.188712 0.16474947 0.14481491 0.12735555 0.11058713 0.094460115 0.078224562 0.069599785 0.069506496 0.073617764 0.078763247 0.089734018 0.10813766 0.12780975][0.15831357 0.14792864 0.13318035 0.12198839 0.11069086 0.099453829 0.089101978 0.079294011 0.077925242 0.086463332 0.098832659 0.10839939 0.12179056 0.14208739 0.16206534][0.13948523 0.13793769 0.13452715 0.13361701 0.13050032 0.12767677 0.12588805 0.12215 0.12332169 0.13284089 0.14425288 0.14789619 0.15272903 0.16513078 0.17884153][0.15860552 0.16927311 0.17751852 0.18468699 0.18578711 0.18675958 0.19023468 0.18856429 0.18534409 0.18741027 0.18982349 0.18129665 0.17183125 0.17118779 0.17598411][0.20670573 0.22904444 0.24453032 0.25323766 0.25112659 0.24772209 0.2498568 0.24596407 0.23440932 0.225199 0.21615605 0.19660515 0.17608814 0.16538826 0.16347538][0.2629115 0.29048592 0.30439854 0.3067297 0.29580623 0.28288129 0.27894977 0.2710942 0.2520088 0.23336405 0.2156671 0.19109428 0.16834819 0.15613285 0.15312][0.30306023 0.32855317 0.33508515 0.32790029 0.30789232 0.28513128 0.27264932 0.25968504 0.23557661 0.21146034 0.18925749 0.16578001 0.14948946 0.14510724 0.14787754][0.30608216 0.32547152 0.32483217 0.3117314 0.28883842 0.26239544 0.24376509 0.22658771 0.20060898 0.17490625 0.15129343 0.1316542 0.12570663 0.13430673 0.14736977][0.26502508 0.27827913 0.27515632 0.26383552 0.24764888 0.22776696 0.21043761 0.19315681 0.16883095 0.14397617 0.11993316 0.10338918 0.10639994 0.12742347 0.15100618][0.2002928 0.20963404 0.20905018 0.20508455 0.20038532 0.19286928 0.18300098 0.1694326 0.14902022 0.12645817 0.10316502 0.088782154 0.096869692 0.12505929 0.15437043][0.14446883 0.15417434 0.15882543 0.16269159 0.16622548 0.16690092 0.16234066 0.15117411 0.13382983 0.11340341 0.091925658 0.079775929 0.089653641 0.1187015 0.14716874][0.11733542 0.13041522 0.13922447 0.14656225 0.14981897 0.14825132 0.14100279 0.12787116 0.11195973 0.094158769 0.076487809 0.067893147 0.077753976 0.10261904 0.12503059]]...]
INFO - root - 2017-12-10 13:28:09.067830: step 18510, loss = 0.70, batch loss = 0.65 (9.8 examples/sec; 0.818 sec/batch; 71h:23m:15s remains)
INFO - root - 2017-12-10 13:28:16.956304: step 18520, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 67h:45m:43s remains)
INFO - root - 2017-12-10 13:28:24.649433: step 18530, loss = 0.71, batch loss = 0.66 (10.2 examples/sec; 0.788 sec/batch; 68h:41m:00s remains)
INFO - root - 2017-12-10 13:28:32.454945: step 18540, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 67h:16m:27s remains)
INFO - root - 2017-12-10 13:28:40.299817: step 18550, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 69h:58m:20s remains)
INFO - root - 2017-12-10 13:28:48.126604: step 18560, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 67h:54m:11s remains)
INFO - root - 2017-12-10 13:28:56.027772: step 18570, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.826 sec/batch; 72h:04m:03s remains)
INFO - root - 2017-12-10 13:29:03.848904: step 18580, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 66h:36m:47s remains)
INFO - root - 2017-12-10 13:29:11.608305: step 18590, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 67h:09m:48s remains)
INFO - root - 2017-12-10 13:29:19.315481: step 18600, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 67h:04m:41s remains)
2017-12-10 13:29:20.124890: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14341179 0.13055615 0.1231294 0.13214529 0.16108461 0.20806064 0.26403731 0.31165779 0.3382948 0.34095517 0.32017469 0.28010041 0.21876274 0.13963707 0.056798391][0.15303394 0.15034264 0.14898102 0.15951471 0.185235 0.22555192 0.27395856 0.31541565 0.33918971 0.34402663 0.32921407 0.2972686 0.24165721 0.16461317 0.08069098][0.15976851 0.16539986 0.1692424 0.1818413 0.2053975 0.2393956 0.27831057 0.31020293 0.32699853 0.32964584 0.31965438 0.29823244 0.25426871 0.18499762 0.10334259][0.16584305 0.17471747 0.1810409 0.19739337 0.22437876 0.25925088 0.29514858 0.32155561 0.33112857 0.32752725 0.31695965 0.30203858 0.26800612 0.20487151 0.12374517][0.16563514 0.17224173 0.17843334 0.19918357 0.23374568 0.27644226 0.31821483 0.34678018 0.35134503 0.33779749 0.31923035 0.3028135 0.27241972 0.21214955 0.13150616][0.15999264 0.16132706 0.16467184 0.18721245 0.22855645 0.2813476 0.33475202 0.37214187 0.37597889 0.35208467 0.32016513 0.29413491 0.26016662 0.20011686 0.12204745][0.15972814 0.15433988 0.15305814 0.17472276 0.2206289 0.28270572 0.34823316 0.39660859 0.40363827 0.37261343 0.32613173 0.28530422 0.2425137 0.18116783 0.10773029][0.18246509 0.17285278 0.16709359 0.18482731 0.22895071 0.29181349 0.36101553 0.41541955 0.42767182 0.396309 0.34162739 0.28955409 0.24031498 0.18085855 0.11536027][0.21829019 0.20836641 0.19975664 0.21163075 0.24756432 0.30130929 0.36369783 0.41658577 0.43459114 0.41111982 0.35956705 0.30616164 0.25725642 0.20443836 0.14965558][0.25406247 0.24697697 0.23706147 0.2433055 0.26938662 0.31050587 0.361167 0.40741721 0.42947152 0.41722649 0.3758235 0.3282949 0.284941 0.24218418 0.20030612][0.27860874 0.27621204 0.266376 0.26854345 0.28674 0.31757405 0.35744563 0.39530769 0.41810369 0.415345 0.38465729 0.34478009 0.30856138 0.27653039 0.24735451][0.27300948 0.27622846 0.26970109 0.27061671 0.28315878 0.30664429 0.33808577 0.36708525 0.38643187 0.38847634 0.36640131 0.334556 0.30624723 0.28442681 0.26599517][0.22229894 0.23028976 0.22918864 0.23065381 0.23892158 0.25722206 0.28327307 0.30513889 0.31934372 0.32255179 0.30710661 0.28346875 0.2626614 0.24878782 0.23780276][0.12452048 0.13330595 0.13677521 0.13924481 0.14517322 0.16144517 0.18673255 0.206597 0.21830155 0.22253548 0.21310602 0.19710317 0.18131241 0.17052954 0.16206278][0.016049383 0.021010367 0.026298884 0.029158946 0.033895526 0.049171869 0.074507572 0.0945322 0.10527883 0.11018163 0.10528994 0.094954595 0.082097746 0.071726061 0.063737616]]...]
INFO - root - 2017-12-10 13:29:27.776153: step 18610, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 67h:44m:05s remains)
INFO - root - 2017-12-10 13:29:35.641605: step 18620, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 67h:39m:36s remains)
INFO - root - 2017-12-10 13:29:43.522141: step 18630, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 67h:52m:13s remains)
INFO - root - 2017-12-10 13:29:51.395706: step 18640, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 69h:18m:42s remains)
INFO - root - 2017-12-10 13:29:59.224710: step 18650, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 67h:53m:36s remains)
INFO - root - 2017-12-10 13:30:07.097387: step 18660, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 67h:23m:10s remains)
INFO - root - 2017-12-10 13:30:15.011394: step 18670, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 68h:05m:19s remains)
INFO - root - 2017-12-10 13:30:22.858756: step 18680, loss = 0.69, batch loss = 0.64 (9.7 examples/sec; 0.821 sec/batch; 71h:36m:26s remains)
INFO - root - 2017-12-10 13:30:30.478888: step 18690, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 70h:50m:11s remains)
INFO - root - 2017-12-10 13:30:38.342344: step 18700, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 67h:11m:43s remains)
2017-12-10 13:30:39.167472: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12281128 0.12592807 0.12469903 0.12413911 0.12336032 0.11993133 0.1120128 0.10592206 0.10452076 0.10382836 0.10409239 0.10443211 0.1040806 0.10383902 0.10379539][0.1282821 0.14181471 0.14903067 0.15300162 0.15276213 0.14641166 0.13321552 0.1215801 0.116389 0.11388826 0.11227065 0.10961011 0.10496915 0.099395625 0.095149308][0.12364726 0.14514704 0.15832895 0.16517287 0.16567777 0.15890115 0.14420089 0.13104554 0.12540762 0.12325156 0.12053213 0.11465304 0.10543454 0.094741747 0.086805239][0.11739092 0.141739 0.15637457 0.16356094 0.16497603 0.16041251 0.14827344 0.13772769 0.13454245 0.13394506 0.13041158 0.12179074 0.10965807 0.096589215 0.087737828][0.11554319 0.13872916 0.15133391 0.15709458 0.15941532 0.15856239 0.1512645 0.14535828 0.14557658 0.14652959 0.1423541 0.13179953 0.11864276 0.10595467 0.098441683][0.12267832 0.14276154 0.15208408 0.1559698 0.15878209 0.16105238 0.15834709 0.15686642 0.15990214 0.16174927 0.15668622 0.14443351 0.13009506 0.11729444 0.11058504][0.13589579 0.15355802 0.16087778 0.16369902 0.166314 0.16945368 0.16900651 0.16980906 0.17364381 0.17500253 0.1682917 0.15371533 0.13650495 0.12160387 0.1141015][0.1498667 0.16726331 0.17503932 0.17790462 0.17915867 0.1800929 0.17857948 0.17841862 0.17982151 0.1781095 0.16870907 0.15203449 0.13224545 0.11510639 0.10654407][0.16107164 0.17972888 0.18917374 0.19228645 0.19080037 0.18671507 0.18092161 0.17666446 0.17303115 0.16669105 0.15499924 0.13825302 0.11887258 0.10194811 0.093906395][0.16653956 0.18639982 0.19694972 0.19951864 0.19436543 0.18437931 0.17303857 0.16360265 0.15454781 0.14455092 0.13311832 0.11988552 0.10485218 0.091352418 0.085879438][0.16378865 0.18397538 0.19410673 0.19489972 0.18613237 0.17125534 0.15482216 0.14076698 0.12827803 0.1182631 0.11117751 0.10537846 0.09821257 0.0909668 0.089831084][0.15546817 0.17553289 0.18424053 0.18263571 0.17103316 0.15304649 0.13275149 0.11505167 0.10153098 0.095224753 0.095654435 0.09936022 0.1018471 0.10276876 0.10710178][0.14938989 0.16939908 0.17729682 0.17462422 0.16210948 0.14313824 0.12063392 0.10035625 0.087001547 0.085360646 0.093254052 0.10558052 0.11670274 0.12497912 0.13331641][0.14987588 0.17014277 0.17906547 0.17849341 0.16825132 0.1508352 0.1280922 0.10660063 0.093671374 0.094993517 0.1070829 0.12405842 0.13966377 0.15137222 0.15963921][0.15477175 0.17478025 0.18576047 0.18956435 0.18406793 0.17056154 0.150072 0.12947391 0.11744361 0.11950985 0.13141322 0.14738676 0.16117133 0.17027627 0.17349106]]...]
INFO - root - 2017-12-10 13:30:47.048490: step 18710, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 68h:20m:47s remains)
INFO - root - 2017-12-10 13:30:54.971959: step 18720, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 69h:41m:16s remains)
INFO - root - 2017-12-10 13:31:02.841117: step 18730, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 70h:41m:07s remains)
INFO - root - 2017-12-10 13:31:10.775804: step 18740, loss = 0.67, batch loss = 0.62 (10.0 examples/sec; 0.803 sec/batch; 69h:57m:11s remains)
INFO - root - 2017-12-10 13:31:18.671935: step 18750, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.826 sec/batch; 72h:00m:51s remains)
INFO - root - 2017-12-10 13:31:26.618466: step 18760, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 68h:09m:00s remains)
INFO - root - 2017-12-10 13:31:34.105718: step 18770, loss = 0.69, batch loss = 0.64 (12.3 examples/sec; 0.648 sec/batch; 56h:29m:47s remains)
INFO - root - 2017-12-10 13:31:41.926559: step 18780, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 67h:05m:56s remains)
INFO - root - 2017-12-10 13:31:49.885587: step 18790, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.825 sec/batch; 71h:55m:56s remains)
INFO - root - 2017-12-10 13:31:57.660601: step 18800, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 68h:41m:07s remains)
2017-12-10 13:31:58.496827: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10153899 0.061790943 0.026164133 0.014072167 0.025585312 0.052058108 0.090540029 0.13747621 0.18064106 0.21177137 0.23462641 0.2584084 0.27942032 0.28492841 0.274486][0.14094922 0.097938649 0.056249894 0.038489547 0.047567107 0.075987257 0.12048523 0.17445813 0.22268885 0.25428465 0.27345851 0.29351288 0.31303126 0.31954056 0.31063148][0.18729131 0.14235453 0.09418121 0.069457114 0.07544674 0.10571679 0.15487553 0.21197142 0.25948027 0.28581789 0.29644769 0.30941027 0.32745671 0.33835334 0.33651772][0.22517094 0.17996928 0.12915669 0.10249708 0.11039493 0.14627622 0.20069261 0.25764719 0.298653 0.31443083 0.31337002 0.31695578 0.33250955 0.34994888 0.35908589][0.24621898 0.20612361 0.16211462 0.14361966 0.16131751 0.20749512 0.26686129 0.31865874 0.34604374 0.34595248 0.33137515 0.32445264 0.33598611 0.35881263 0.37883967][0.24655113 0.21800889 0.18968439 0.18753116 0.21963376 0.27645674 0.33734438 0.37900943 0.38931271 0.37327296 0.34841487 0.33327347 0.339137 0.36283705 0.38904703][0.23823422 0.22559647 0.21807991 0.23594263 0.28230444 0.34563056 0.40176082 0.42929983 0.42320058 0.39527574 0.36537868 0.3443608 0.34160414 0.35834527 0.38173851][0.23763393 0.24116938 0.25364643 0.29043993 0.34838203 0.41367695 0.46163204 0.47571608 0.45667398 0.42066669 0.38789824 0.36161935 0.34716907 0.34880632 0.35841522][0.24283069 0.25878295 0.28438675 0.33385304 0.39771345 0.46079 0.50051755 0.50605339 0.48155567 0.44411287 0.41256008 0.38409108 0.35903475 0.34265617 0.33153245][0.25401014 0.27220678 0.30001947 0.35176441 0.41367531 0.47041333 0.50311571 0.50656265 0.48586518 0.45644349 0.43375412 0.41078615 0.38307121 0.35350087 0.32198128][0.2536065 0.26446497 0.28415921 0.32928553 0.38462698 0.43591446 0.46798682 0.47883615 0.47190773 0.45835176 0.44950271 0.43642324 0.41127437 0.37350795 0.32551008][0.23189513 0.22912215 0.23639823 0.27200451 0.32189295 0.37356147 0.414011 0.44079167 0.45355415 0.45751676 0.46028385 0.45451376 0.4322072 0.39080149 0.33410269][0.19916113 0.18301542 0.17816456 0.20405903 0.25057888 0.30754462 0.36186171 0.40732974 0.4381091 0.45348418 0.4596681 0.45483378 0.43366244 0.39271453 0.33547774][0.16249442 0.13993534 0.12838288 0.14707188 0.19034767 0.25138915 0.316679 0.3753247 0.41606736 0.43272352 0.43255195 0.42139578 0.39851061 0.36100477 0.31032744][0.14097631 0.12124696 0.10820654 0.1203944 0.15590169 0.21213803 0.27692935 0.33661774 0.37608293 0.38459134 0.37130556 0.34896713 0.32185754 0.28888825 0.24919993]]...]
INFO - root - 2017-12-10 13:32:06.382299: step 18810, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 67h:50m:39s remains)
INFO - root - 2017-12-10 13:32:14.205644: step 18820, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 68h:11m:40s remains)
INFO - root - 2017-12-10 13:32:22.005778: step 18830, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 68h:51m:45s remains)
INFO - root - 2017-12-10 13:32:29.816070: step 18840, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 68h:27m:22s remains)
INFO - root - 2017-12-10 13:32:37.370706: step 18850, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 68h:20m:47s remains)
INFO - root - 2017-12-10 13:32:44.670621: step 18860, loss = 0.69, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 49h:31m:00s remains)
INFO - root - 2017-12-10 13:32:52.501135: step 18870, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 66h:09m:08s remains)
INFO - root - 2017-12-10 13:33:00.369948: step 18880, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 70h:06m:56s remains)
INFO - root - 2017-12-10 13:33:08.251678: step 18890, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 70h:08m:03s remains)
INFO - root - 2017-12-10 13:33:16.035624: step 18900, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 68h:25m:25s remains)
2017-12-10 13:33:16.890712: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17175135 0.16325654 0.14929104 0.14098655 0.13798404 0.13866547 0.14724253 0.15711132 0.1571565 0.14609724 0.12888446 0.10582048 0.073126972 0.035147037 -0.000993065][0.25964421 0.25417918 0.2380579 0.23023622 0.23077849 0.23418055 0.2469514 0.25983608 0.25671992 0.23584667 0.20626745 0.17132288 0.1256396 0.075284593 0.028615365][0.30930057 0.30477223 0.28813821 0.28374031 0.29168868 0.30283579 0.32385769 0.34282812 0.3400133 0.3148275 0.27802804 0.23506884 0.18063444 0.12165154 0.066310748][0.32366604 0.31898445 0.30396187 0.30451486 0.32045302 0.34083071 0.37234303 0.40164378 0.4053095 0.38273066 0.34342343 0.29438081 0.2322877 0.16447178 0.10040393][0.30915424 0.308417 0.29782739 0.30469063 0.32994533 0.3627277 0.40803927 0.4491182 0.45882162 0.4363479 0.3914651 0.33264297 0.25964081 0.18269737 0.11327308][0.29124519 0.29578367 0.29124421 0.3041009 0.33834657 0.38526338 0.44578773 0.49614838 0.50590825 0.4759042 0.41918346 0.34689575 0.2620517 0.17801692 0.10792161][0.29720074 0.30654824 0.3068178 0.32148466 0.35914022 0.41632611 0.48806936 0.541046 0.543893 0.50103956 0.43108007 0.34596992 0.25232854 0.16522327 0.097899228][0.33833966 0.35358176 0.35787612 0.36894229 0.39988595 0.45577559 0.52902776 0.57850569 0.57275486 0.51956677 0.44240984 0.35112107 0.25361696 0.16576254 0.10033122][0.38763258 0.41322798 0.42367306 0.42832315 0.44227943 0.47984603 0.53752303 0.57410848 0.56061321 0.50383657 0.42832655 0.34071097 0.24758641 0.16455451 0.10288618][0.40288606 0.43960884 0.45737064 0.45684138 0.45200846 0.46419272 0.49771738 0.51726669 0.50061089 0.44901705 0.38356045 0.30715755 0.22474675 0.15184879 0.096677482][0.37425825 0.41496861 0.43537143 0.42926782 0.40807489 0.39682797 0.40711391 0.41351864 0.4037869 0.37023067 0.32529828 0.266058 0.19819498 0.13754924 0.089101441][0.30792442 0.3459304 0.36459115 0.3529385 0.32035029 0.29211181 0.28506982 0.28457946 0.28767136 0.27907264 0.25952718 0.21977185 0.16715327 0.11843058 0.076769613][0.20785911 0.2423514 0.2597805 0.24667276 0.21048628 0.17365488 0.15608504 0.15360411 0.16962257 0.18470667 0.18811989 0.1670596 0.12907091 0.091003411 0.055795651][0.093843251 0.12313394 0.1392483 0.1286988 0.096168995 0.058827274 0.03650355 0.032694437 0.055047069 0.083953485 0.10220878 0.096001551 0.071608968 0.04407635 0.016943818][-0.00026154329 0.020744564 0.03364199 0.027535459 0.0047449847 -0.023831951 -0.042641513 -0.045676731 -0.023482522 0.0077426289 0.030977162 0.033592638 0.019830719 0.0010086108 -0.018955408]]...]
INFO - root - 2017-12-10 13:33:24.767494: step 18910, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 70h:27m:37s remains)
INFO - root - 2017-12-10 13:33:32.597624: step 18920, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 66h:40m:04s remains)
INFO - root - 2017-12-10 13:33:40.292831: step 18930, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 67h:55m:32s remains)
INFO - root - 2017-12-10 13:33:48.081450: step 18940, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 67h:22m:20s remains)
INFO - root - 2017-12-10 13:33:55.757841: step 18950, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 67h:31m:09s remains)
INFO - root - 2017-12-10 13:34:03.598230: step 18960, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 69h:24m:50s remains)
INFO - root - 2017-12-10 13:34:11.463360: step 18970, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 67h:15m:24s remains)
INFO - root - 2017-12-10 13:34:19.240183: step 18980, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 66h:52m:31s remains)
INFO - root - 2017-12-10 13:34:27.023652: step 18990, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 66h:50m:47s remains)
INFO - root - 2017-12-10 13:34:34.948824: step 19000, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 69h:14m:10s remains)
2017-12-10 13:34:35.860572: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.075990088 0.13690402 0.20665883 0.25961089 0.28446761 0.28323445 0.27023798 0.24618891 0.20919681 0.169586 0.1356263 0.10643378 0.070119478 0.028817102 -0.0059167026][0.066162646 0.12822342 0.20156726 0.2612645 0.29459053 0.30114698 0.29231051 0.27094626 0.23682314 0.19882025 0.16387624 0.130845 0.089188516 0.041720923 0.00038036349][0.046315558 0.10486914 0.17715046 0.24176437 0.28528759 0.30397046 0.30442542 0.29099098 0.26386288 0.2284181 0.19033624 0.15006156 0.10123187 0.047863875 0.0017165528][0.033533145 0.091832757 0.16522092 0.23601466 0.29070136 0.32330924 0.33554628 0.33101228 0.30947432 0.27274051 0.22681421 0.17421672 0.11359961 0.051587984 -2.0141602e-05][0.036594383 0.10099738 0.18082608 0.25973415 0.32480827 0.37065285 0.39512193 0.39860323 0.37947172 0.33780384 0.28077242 0.21218486 0.13522166 0.060287103 0.00055389409][0.055667672 0.13119416 0.22136873 0.30833533 0.38052955 0.43521875 0.46849489 0.47646722 0.45576596 0.40677279 0.33827448 0.25386411 0.15958825 0.070300527 0.0016602555][0.080015071 0.16778271 0.26908475 0.362612 0.4380759 0.49659443 0.5344435 0.54303956 0.51769274 0.4613362 0.38396224 0.28740487 0.17866184 0.077174291 0.0015927507][0.099880807 0.19804697 0.30915791 0.40766716 0.48369411 0.54188895 0.57969826 0.58549649 0.55436492 0.49289912 0.41120505 0.30788282 0.18973948 0.079875566 -4.159546e-05][0.10849151 0.21267839 0.33009723 0.43147212 0.50599855 0.56026322 0.59373987 0.59473652 0.55896461 0.4962703 0.41546586 0.311834 0.19125614 0.078526773 -0.0025272218][0.10399917 0.20791887 0.32628366 0.42784646 0.500246 0.54991215 0.57881141 0.57720023 0.54039609 0.47937846 0.40166032 0.30155256 0.18377836 0.072915278 -0.006108948][0.088652812 0.18638766 0.30086872 0.40136892 0.47370461 0.5217315 0.54874057 0.54706687 0.51043022 0.44967324 0.37279615 0.27620557 0.16464573 0.060715023 -0.012074189][0.070295051 0.15902613 0.26715648 0.36584103 0.43883982 0.48612094 0.51113451 0.50778162 0.46857566 0.40468282 0.32655275 0.2343999 0.13299583 0.041269649 -0.020681497][0.053065639 0.13189088 0.23162869 0.32591435 0.39695203 0.44058168 0.45942572 0.44934967 0.40399435 0.33604309 0.25845337 0.17478834 0.0889969 0.015033593 -0.032230027][0.034220468 0.10117134 0.1882825 0.27217025 0.3348349 0.36914736 0.37715694 0.35749382 0.30710045 0.23980179 0.16980006 0.10174134 0.03752118 -0.014583321 -0.045174349][0.010053689 0.0601767 0.12718323 0.19202338 0.23892839 0.26024079 0.25794321 0.23338361 0.18642586 0.12971056 0.07582584 0.028733088 -0.011553025 -0.041670017 -0.056793422]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 13:34:43.603869: step 19010, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 70h:45m:17s remains)
INFO - root - 2017-12-10 13:34:51.463965: step 19020, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.753 sec/batch; 65h:32m:15s remains)
INFO - root - 2017-12-10 13:34:59.427544: step 19030, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 67h:04m:18s remains)
INFO - root - 2017-12-10 13:35:07.170131: step 19040, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 70h:12m:54s remains)
INFO - root - 2017-12-10 13:35:15.031640: step 19050, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.791 sec/batch; 68h:51m:56s remains)
INFO - root - 2017-12-10 13:35:22.822877: step 19060, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.769 sec/batch; 66h:57m:25s remains)
INFO - root - 2017-12-10 13:35:30.740153: step 19070, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 70h:32m:38s remains)
INFO - root - 2017-12-10 13:35:38.598635: step 19080, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.745 sec/batch; 64h:51m:05s remains)
INFO - root - 2017-12-10 13:35:46.292808: step 19090, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 69h:16m:19s remains)
INFO - root - 2017-12-10 13:35:54.081170: step 19100, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 65h:43m:13s remains)
2017-12-10 13:35:54.955020: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20726143 0.13699119 0.070999041 0.037431739 0.031397555 0.028632745 0.017679479 0.0077696689 0.0093685044 0.038261056 0.11603504 0.24413265 0.3800469 0.47464815 0.49830422][0.28365967 0.20918165 0.14083213 0.1126914 0.11399136 0.11119056 0.087544337 0.055280317 0.031008793 0.036571663 0.098713785 0.22134027 0.36330622 0.47405761 0.51587969][0.33870825 0.2747494 0.22128603 0.21278968 0.23205894 0.23543984 0.20105308 0.14370656 0.084941149 0.054686785 0.085503794 0.18550415 0.31778324 0.43216154 0.48502573][0.38255098 0.34444359 0.31758231 0.33471134 0.37410438 0.3854596 0.34368277 0.26435083 0.17228912 0.10525112 0.10065477 0.16958119 0.27981508 0.38362175 0.4357498][0.42672884 0.42463776 0.42923111 0.46744213 0.51956642 0.53642923 0.49148566 0.39757702 0.28141046 0.18675379 0.1528284 0.19165845 0.27431789 0.35662967 0.39636379][0.46905717 0.50658005 0.54002053 0.588282 0.642403 0.66484135 0.62855059 0.53744048 0.41469568 0.30764791 0.25483125 0.26901436 0.32461816 0.38081139 0.40105388][0.51002496 0.5834204 0.6397875 0.68785238 0.73211408 0.75512064 0.73287523 0.65839577 0.5465467 0.44356084 0.38500771 0.38362509 0.41813636 0.45212254 0.45391464][0.55347025 0.64702821 0.71218121 0.75055093 0.77636671 0.79217541 0.77946138 0.72542888 0.63690031 0.55344641 0.50247979 0.49301451 0.51050282 0.52608532 0.51260751][0.57636893 0.67312282 0.7354722 0.76261753 0.77002496 0.77276814 0.75960892 0.72091359 0.66003275 0.60584682 0.57231283 0.56038129 0.56383681 0.56457859 0.5404523][0.53932559 0.6243704 0.67847174 0.69800562 0.69306242 0.68118459 0.65909404 0.627558 0.59148794 0.56756872 0.55439484 0.54431581 0.53950226 0.53293395 0.50757617][0.43080515 0.49671978 0.54063517 0.55664831 0.54713327 0.52613276 0.49638826 0.46838784 0.449122 0.4453457 0.44562709 0.43760195 0.42933145 0.42227593 0.404196][0.27397656 0.32039455 0.35380855 0.36698666 0.35803449 0.33578572 0.30480257 0.27901027 0.26583633 0.26706305 0.27010122 0.26368335 0.257519 0.25573403 0.24892616][0.10560202 0.1335413 0.15490274 0.16255641 0.15473273 0.13663 0.11118136 0.089345053 0.076816879 0.074401557 0.074018262 0.068895571 0.067276359 0.071841508 0.07484892][-0.032355428 -0.020425808 -0.010466752 -0.0084608318 -0.014620083 -0.026102388 -0.042683862 -0.058945797 -0.071484759 -0.078761861 -0.083744437 -0.088661261 -0.087668188 -0.079664312 -0.070471667][-0.11845919 -0.1177622 -0.11566208 -0.11722905 -0.12160227 -0.12733172 -0.13575822 -0.14580122 -0.15596363 -0.16433325 -0.17104065 -0.17580223 -0.17492194 -0.1676327 -0.1568848]]...]
INFO - root - 2017-12-10 13:36:02.921709: step 19110, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 66h:57m:50s remains)
INFO - root - 2017-12-10 13:36:10.802452: step 19120, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 70h:29m:17s remains)
INFO - root - 2017-12-10 13:36:18.444602: step 19130, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 68h:11m:49s remains)
INFO - root - 2017-12-10 13:36:26.255518: step 19140, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 67h:34m:56s remains)
INFO - root - 2017-12-10 13:36:34.073726: step 19150, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 68h:54m:14s remains)
INFO - root - 2017-12-10 13:36:41.921461: step 19160, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 67h:27m:44s remains)
INFO - root - 2017-12-10 13:36:49.649920: step 19170, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 68h:02m:11s remains)
INFO - root - 2017-12-10 13:36:57.481344: step 19180, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 67h:05m:46s remains)
INFO - root - 2017-12-10 13:37:05.327259: step 19190, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 70h:15m:14s remains)
INFO - root - 2017-12-10 13:37:13.219148: step 19200, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 67h:31m:41s remains)
2017-12-10 13:37:14.012024: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.032345206 -0.031106908 -0.030792834 -0.032682955 -0.0362362 -0.0403597 -0.044365544 -0.045421988 -0.042494997 -0.039990079 -0.042369355 -0.051310435 -0.061079156 -0.066234015 -0.066612][-0.011612236 0.00061000825 0.010890951 0.014689322 0.013463241 0.010251177 0.0061048232 0.0045939838 0.0070309364 0.0078022829 0.00024369289 -0.019481968 -0.042648219 -0.060033072 -0.069004476][0.01927953 0.049219903 0.077900931 0.094962873 0.10140666 0.10316879 0.10182788 0.10033607 0.099521339 0.093194842 0.073809355 0.037243769 -0.0051628212 -0.040235851 -0.062598526][0.055871181 0.10726018 0.15951554 0.19515637 0.2126679 0.22173569 0.22633335 0.22802551 0.22385934 0.20648661 0.17018382 0.11319456 0.048398606 -0.0085256584 -0.048601881][0.092627361 0.16608009 0.2431927 0.30009663 0.33056673 0.34672672 0.35957184 0.37140408 0.37126407 0.34611943 0.29175898 0.21350539 0.12525265 0.043034434 -0.020570885][0.12285405 0.21678193 0.31804866 0.39681107 0.44093218 0.46243209 0.4835327 0.51201308 0.52647805 0.5018419 0.43267637 0.33520436 0.22480263 0.11542178 0.022834374][0.14076209 0.24983709 0.3709909 0.46993986 0.52655774 0.55198854 0.58038408 0.62793642 0.66152388 0.64136428 0.56074023 0.44820711 0.320186 0.18693043 0.067218006][0.14598471 0.26099318 0.39163947 0.50140506 0.5637871 0.59001541 0.62260211 0.68470585 0.73423088 0.72002643 0.63470566 0.51505661 0.37779438 0.23021255 0.093939625][0.13266954 0.23978318 0.36371464 0.46850157 0.52521807 0.5458588 0.57428324 0.63687724 0.69065905 0.68212444 0.60372573 0.4913938 0.36071923 0.21767457 0.085364521][0.097313471 0.18305035 0.28439912 0.36965916 0.41168454 0.4221946 0.43987611 0.490117 0.53698546 0.53327119 0.47193387 0.38090321 0.27216825 0.15160011 0.041975945][0.044107616 0.10175543 0.17252222 0.23136033 0.2559683 0.25668702 0.26217914 0.29370466 0.32646915 0.3255713 0.2850486 0.22209767 0.14435767 0.057639461 -0.0179326][-0.014496733 0.015790965 0.056606829 0.090271153 0.1006332 0.095736466 0.09285143 0.1069605 0.1239231 0.12262888 0.099193022 0.061807774 0.014972932 -0.035802189 -0.075334005][-0.0577561 -0.048287433 -0.03134926 -0.018019948 -0.018037373 -0.025816018 -0.033406071 -0.032319073 -0.028506642 -0.031834424 -0.043931022 -0.062306788 -0.084050037 -0.10452735 -0.11447404][-0.079581343 -0.082897954 -0.081656657 -0.081889875 -0.088205695 -0.097174443 -0.10635398 -0.11199253 -0.11528219 -0.11914791 -0.12392867 -0.12964234 -0.13433671 -0.1347186 -0.12667383][-0.084626757 -0.092895515 -0.098253407 -0.10377269 -0.11112817 -0.11863526 -0.12617522 -0.13226926 -0.13637272 -0.13871302 -0.13915561 -0.13871552 -0.13574636 -0.12818015 -0.11486788]]...]
INFO - root - 2017-12-10 13:37:21.893918: step 19210, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 66h:44m:01s remains)
INFO - root - 2017-12-10 13:37:29.566756: step 19220, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 70h:08m:37s remains)
INFO - root - 2017-12-10 13:37:37.366626: step 19230, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 67h:41m:23s remains)
INFO - root - 2017-12-10 13:37:45.342963: step 19240, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.812 sec/batch; 70h:39m:36s remains)
INFO - root - 2017-12-10 13:37:53.095652: step 19250, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 69h:52m:06s remains)
INFO - root - 2017-12-10 13:38:00.966459: step 19260, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 67h:02m:33s remains)
INFO - root - 2017-12-10 13:38:08.825833: step 19270, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 70h:33m:37s remains)
INFO - root - 2017-12-10 13:38:16.588553: step 19280, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 66h:33m:38s remains)
INFO - root - 2017-12-10 13:38:24.414315: step 19290, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 66h:12m:07s remains)
INFO - root - 2017-12-10 13:38:32.271434: step 19300, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 70h:54m:11s remains)
2017-12-10 13:38:33.133658: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0328458 0.05662173 0.075091124 0.08650139 0.090675928 0.091435112 0.087126449 0.078856736 0.063995861 0.04309357 0.025086019 0.026782747 0.049893111 0.082614549 0.10949245][0.084312759 0.12434445 0.15745679 0.18225755 0.19753852 0.20519504 0.19975984 0.18256776 0.15387931 0.11746582 0.087069966 0.088262163 0.12475404 0.17922631 0.22346681][0.13119708 0.1880257 0.23771858 0.28052962 0.3153249 0.33979377 0.34134352 0.31802768 0.27373186 0.21692206 0.16717878 0.15932688 0.20127892 0.27235457 0.33107764][0.15938842 0.22971009 0.29484078 0.35761213 0.41824949 0.46894887 0.487807 0.46593222 0.40844426 0.32838553 0.25139019 0.22118178 0.25306767 0.32711086 0.39230534][0.17813292 0.26105559 0.34049341 0.42098591 0.50512242 0.58035845 0.61728209 0.599408 0.53099513 0.42850834 0.32158545 0.26051992 0.27003172 0.33494216 0.40070346][0.1988281 0.29497653 0.3870832 0.47901812 0.576126 0.66399711 0.71062326 0.69467026 0.61870331 0.49985591 0.36844504 0.27877584 0.26372984 0.3137756 0.37696463][0.21983522 0.32830206 0.43040213 0.5266425 0.62423611 0.71108246 0.7576701 0.7432422 0.66758823 0.54467422 0.40153712 0.29345173 0.2590667 0.29211682 0.34787393][0.23920137 0.35835141 0.46742579 0.5596714 0.64261585 0.71108341 0.74387532 0.72700292 0.65917146 0.54967058 0.41578615 0.30889571 0.2685861 0.28979897 0.33534518][0.25422755 0.38058409 0.49194473 0.57205111 0.62712306 0.66157788 0.66713154 0.64164829 0.58617508 0.5048573 0.40140504 0.31809986 0.2903975 0.31165102 0.35073438][0.24961013 0.37463167 0.48136514 0.54535228 0.57029861 0.56806457 0.54342496 0.50728816 0.46559206 0.41835555 0.3562291 0.3100082 0.30677953 0.33621165 0.37265059][0.20213963 0.30990404 0.40182266 0.44999993 0.4553073 0.4316088 0.39106312 0.35282668 0.32671821 0.31060696 0.28722847 0.27608809 0.29461104 0.32919297 0.3603873][0.11521295 0.19075382 0.2565254 0.2874662 0.28347626 0.25634539 0.21895456 0.19099213 0.18280104 0.19139631 0.19693446 0.20967121 0.23929515 0.27155414 0.29276949][0.022252388 0.061149444 0.095435567 0.10702713 0.097075157 0.074536048 0.049830575 0.03781762 0.045964137 0.07051795 0.093332157 0.11855892 0.15038005 0.17638114 0.18808091][-0.050431162 -0.040947389 -0.033003591 -0.037041597 -0.050229654 -0.0661149 -0.077476606 -0.075361915 -0.056750856 -0.026379541 0.00213763 0.030145451 0.058569122 0.077547587 0.083004571][-0.097162083 -0.10521962 -0.11249783 -0.12442341 -0.13773654 -0.14815727 -0.15121207 -0.14250459 -0.12201425 -0.0945154 -0.068608694 -0.044084243 -0.021573525 -0.0079074651 -0.004446194]]...]
INFO - root - 2017-12-10 13:38:40.832701: step 19310, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 67h:49m:51s remains)
INFO - root - 2017-12-10 13:38:48.690426: step 19320, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 69h:10m:15s remains)
INFO - root - 2017-12-10 13:38:56.370576: step 19330, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 69h:05m:51s remains)
INFO - root - 2017-12-10 13:39:04.123960: step 19340, loss = 0.71, batch loss = 0.65 (10.8 examples/sec; 0.743 sec/batch; 64h:40m:13s remains)
INFO - root - 2017-12-10 13:39:12.019354: step 19350, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 67h:58m:21s remains)
INFO - root - 2017-12-10 13:39:19.810618: step 19360, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 67h:48m:12s remains)
INFO - root - 2017-12-10 13:39:27.636101: step 19370, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.773 sec/batch; 67h:12m:10s remains)
INFO - root - 2017-12-10 13:39:35.457206: step 19380, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 69h:25m:23s remains)
INFO - root - 2017-12-10 13:39:43.380397: step 19390, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 69h:03m:06s remains)
INFO - root - 2017-12-10 13:39:51.115149: step 19400, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.819 sec/batch; 71h:14m:31s remains)
2017-12-10 13:39:51.995779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.065226816 -0.067893036 -0.070496388 -0.0751613 -0.080915727 -0.085891746 -0.089616992 -0.091800123 -0.091742024 -0.089162536 -0.083402649 -0.074883759 -0.065990344 -0.059081744 -0.057051189][-0.063304968 -0.068808384 -0.073291652 -0.078400694 -0.082514904 -0.083730362 -0.082952842 -0.08120957 -0.078911394 -0.075985432 -0.070688374 -0.062088467 -0.051548757 -0.042246621 -0.039309114][-0.060973849 -0.067796782 -0.070750676 -0.071000129 -0.066981204 -0.05855773 -0.049609523 -0.043473475 -0.041591015 -0.043523125 -0.045657925 -0.043580621 -0.034931932 -0.022371156 -0.01441247][-0.047240406 -0.052761979 -0.0494539 -0.036537584 -0.013080982 0.015935628 0.039979104 0.050635152 0.045060571 0.026255688 0.002403328 -0.015278594 -0.017447602 -0.0045277295 0.011082863][-0.016709916 -0.0185705 -0.0041106129 0.033406243 0.093976058 0.16188011 0.2127707 0.22818971 0.20373896 0.14968607 0.084213354 0.029404283 0.0034599972 0.0085226977 0.028501261][0.026181649 0.030161561 0.060342915 0.13283966 0.24553737 0.36635819 0.45066783 0.46670341 0.41054526 0.30512434 0.18364999 0.081090257 0.022921627 0.012005662 0.029144028][0.066154011 0.079974063 0.13085726 0.24300638 0.40986457 0.58135587 0.69269758 0.70089054 0.60614079 0.44648632 0.27035016 0.12336239 0.03433435 0.0043444675 0.011975541][0.083951391 0.10986536 0.18039134 0.32266963 0.52512085 0.725494 0.8474465 0.843304 0.71842217 0.52154189 0.31072506 0.1371979 0.029298708 -0.014690995 -0.017490953][0.069742478 0.10370146 0.18395382 0.33515748 0.54237819 0.74111283 0.8558383 0.84162557 0.70694935 0.50230491 0.28719008 0.11280329 0.004643776 -0.041472919 -0.048712648][0.025920779 0.0581145 0.132288 0.26767913 0.44871697 0.61800003 0.71109611 0.69152552 0.56854749 0.3864505 0.19865142 0.050692268 -0.037119936 -0.07161516 -0.075583726][-0.025557648 -0.004490742 0.049510472 0.14940064 0.28191677 0.40330166 0.46616215 0.44520539 0.3488943 0.21104226 0.07347782 -0.028648723 -0.082074255 -0.095863082 -0.091123462][-0.064779617 -0.057291962 -0.028261973 0.029710691 0.1078117 0.17818159 0.21144252 0.19267753 0.12863371 0.041797221 -0.039747316 -0.092994712 -0.11163364 -0.10517871 -0.091445021][-0.083816312 -0.08738216 -0.080291435 -0.058908831 -0.026970673 0.0019992902 0.013675085 0.00041279604 -0.033656098 -0.076303743 -0.111855 -0.12772737 -0.12215197 -0.10298652 -0.083710223][-0.084027939 -0.093771674 -0.10064454 -0.1031299 -0.1010832 -0.098081946 -0.098514467 -0.10603199 -0.11915578 -0.13298377 -0.14054328 -0.13604328 -0.11954922 -0.096377134 -0.075758062][-0.070863053 -0.0808611 -0.092612922 -0.10597081 -0.11878201 -0.12901951 -0.1356056 -0.14008807 -0.14278339 -0.14281574 -0.1383075 -0.12712434 -0.10955104 -0.088631779 -0.070200026]]...]
INFO - root - 2017-12-10 13:39:59.593521: step 19410, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.790 sec/batch; 68h:42m:07s remains)
INFO - root - 2017-12-10 13:40:07.378786: step 19420, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 67h:04m:44s remains)
INFO - root - 2017-12-10 13:40:15.161575: step 19430, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 67h:50m:25s remains)
INFO - root - 2017-12-10 13:40:23.058926: step 19440, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 70h:04m:01s remains)
INFO - root - 2017-12-10 13:40:30.930234: step 19450, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 68h:17m:52s remains)
INFO - root - 2017-12-10 13:40:38.723779: step 19460, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 69h:43m:56s remains)
INFO - root - 2017-12-10 13:40:46.511647: step 19470, loss = 0.70, batch loss = 0.65 (10.6 examples/sec; 0.753 sec/batch; 65h:30m:11s remains)
INFO - root - 2017-12-10 13:40:54.295634: step 19480, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 69h:21m:16s remains)
INFO - root - 2017-12-10 13:41:01.709999: step 19490, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 66h:36m:20s remains)
INFO - root - 2017-12-10 13:41:09.736932: step 19500, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 69h:15m:15s remains)
2017-12-10 13:41:10.560976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.061401509 -0.064748324 -0.064152457 -0.061412953 -0.057783414 -0.054814197 -0.053356919 -0.053158086 -0.053616062 -0.054365639 -0.055005029 -0.055501714 -0.05580711 -0.0556877 -0.054765865][-0.062515378 -0.063007541 -0.059015505 -0.053330611 -0.048653483 -0.047113452 -0.049052879 -0.052741855 -0.056553334 -0.059429005 -0.060690623 -0.060749657 -0.060249217 -0.059597529 -0.058602322][-0.051417433 -0.045914974 -0.035613269 -0.024506774 -0.017161267 -0.016631493 -0.022877254 -0.032737814 -0.043571368 -0.053085811 -0.059383325 -0.062426325 -0.063218065 -0.0629892 -0.062154736][-0.019131772 -0.0057878182 0.012525922 0.031485546 0.044574037 0.047050774 0.038264673 0.02166687 0.00038488343 -0.021744352 -0.040009562 -0.052276786 -0.059038017 -0.062384147 -0.063491322][0.039785981 0.061656963 0.088232011 0.11676894 0.13903733 0.14771402 0.1400186 0.11816741 0.084881291 0.045412134 0.0086270319 -0.019624643 -0.038291167 -0.050004985 -0.056685079][0.12148269 0.15138552 0.18483961 0.22244725 0.254869 0.27209419 0.26860452 0.2446634 0.20147209 0.14474244 0.087314807 0.039104715 0.0036533969 -0.021473741 -0.038181383][0.21009155 0.2464782 0.28337595 0.32566768 0.36415827 0.38761118 0.38894585 0.3668631 0.31994358 0.25249362 0.17903641 0.11265337 0.059596933 0.018734941 -0.01083149][0.27993917 0.32023302 0.35617977 0.39571163 0.43121898 0.45304254 0.45543045 0.43717647 0.39413172 0.32716137 0.24921297 0.17395188 0.10944597 0.056352016 0.015502716][0.30160877 0.34243923 0.37366852 0.40361404 0.42694753 0.43782681 0.4345862 0.41824341 0.38328317 0.32706532 0.25842866 0.18875541 0.12575769 0.071099326 0.027119409][0.2572743 0.2936053 0.31737685 0.33454141 0.3419098 0.33788198 0.32562974 0.309134 0.28351703 0.24396232 0.19459428 0.14281708 0.09419699 0.050296459 0.013831452][0.1591626 0.18561876 0.20032771 0.20524183 0.19871558 0.18227111 0.16263773 0.14598094 0.12947831 0.10810111 0.081977695 0.054200508 0.027434731 0.0023648092 -0.019000841][0.041670304 0.056096788 0.062571004 0.059411142 0.0458196 0.024475904 0.0031231062 -0.011742785 -0.020638483 -0.027314078 -0.033676315 -0.039488852 -0.0447033 -0.049930856 -0.054753173][-0.053649098 -0.05022284 -0.049876612 -0.056473259 -0.070164353 -0.088417858 -0.10489176 -0.11470494 -0.11738618 -0.11516694 -0.11007934 -0.10279427 -0.094326451 -0.086094655 -0.0791371][-0.1006207 -0.10394198 -0.10689293 -0.11360468 -0.12343709 -0.13453339 -0.14307053 -0.14656141 -0.14482149 -0.1392605 -0.1311534 -0.12068126 -0.10850414 -0.095911659 -0.084600873][-0.10260958 -0.10806695 -0.11174808 -0.11706235 -0.12261248 -0.12712263 -0.12888478 -0.1272987 -0.12302298 -0.11742935 -0.11114225 -0.10354585 -0.094422437 -0.084387295 -0.074945331]]...]
INFO - root - 2017-12-10 13:41:18.440054: step 19510, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 69h:16m:50s remains)
INFO - root - 2017-12-10 13:41:26.245754: step 19520, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 67h:52m:00s remains)
INFO - root - 2017-12-10 13:41:34.154227: step 19530, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 68h:39m:45s remains)
INFO - root - 2017-12-10 13:41:42.008366: step 19540, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 66h:09m:12s remains)
INFO - root - 2017-12-10 13:41:49.968593: step 19550, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 69h:16m:14s remains)
INFO - root - 2017-12-10 13:41:57.809123: step 19560, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 67h:14m:56s remains)
INFO - root - 2017-12-10 13:42:05.327579: step 19570, loss = 0.69, batch loss = 0.63 (11.3 examples/sec; 0.706 sec/batch; 61h:21m:44s remains)
INFO - root - 2017-12-10 13:42:13.159023: step 19580, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 68h:18m:18s remains)
INFO - root - 2017-12-10 13:42:21.072750: step 19590, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 68h:40m:22s remains)
INFO - root - 2017-12-10 13:42:28.871707: step 19600, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.752 sec/batch; 65h:21m:29s remains)
2017-12-10 13:42:29.694755: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.032392252 0.065086991 0.086500525 0.087691478 0.07258939 0.050425448 0.028647473 0.013396576 0.0069176573 0.0075288396 0.00875885 0.0078129768 0.0060702497 0.004941931 0.0099975606][0.070605159 0.11744832 0.14989075 0.15527543 0.13830027 0.11103008 0.082333453 0.059936296 0.048103865 0.046678584 0.047441188 0.045935396 0.043754067 0.042332094 0.048102584][0.10760231 0.17100532 0.21800272 0.23238213 0.21912813 0.19182722 0.15950608 0.13057609 0.11185049 0.10568917 0.1038819 0.10098215 0.097950406 0.09527494 0.10051417][0.13615723 0.21545544 0.27762541 0.3039971 0.29963279 0.27800041 0.24644338 0.21269456 0.18613617 0.17280626 0.16698387 0.16303332 0.16057755 0.15892328 0.16638868][0.15805933 0.25133452 0.32730278 0.36603472 0.373073 0.36069912 0.33304065 0.29613256 0.26175243 0.23981611 0.22874616 0.22388786 0.22358274 0.22617118 0.24083069][0.17692173 0.28201529 0.36950693 0.41916713 0.4371179 0.43416837 0.41125509 0.37200549 0.329899 0.29799989 0.2794407 0.27178225 0.2733489 0.28349322 0.31258076][0.18901722 0.30194369 0.39766291 0.45669344 0.48476568 0.4912056 0.4745914 0.43625733 0.38995096 0.34959847 0.32293946 0.31065139 0.31189454 0.32950044 0.37516615][0.19296843 0.30801785 0.40750709 0.47362745 0.51090747 0.52669764 0.51790953 0.48407644 0.43826792 0.39404017 0.36272684 0.34750673 0.3484309 0.37218672 0.4307082][0.193749 0.30761191 0.40911707 0.48267949 0.52995592 0.55421817 0.55067384 0.51901412 0.47349539 0.42939064 0.40079978 0.39071459 0.39642295 0.42541698 0.48942557][0.18893073 0.30005088 0.40482095 0.48859635 0.54756367 0.57872647 0.57619011 0.5423218 0.49539992 0.45452192 0.43488148 0.43612489 0.44935676 0.47990903 0.53949976][0.16580957 0.2715075 0.38012606 0.4760955 0.54807156 0.58496779 0.58079731 0.54271668 0.49442717 0.45917273 0.4509736 0.46482173 0.48454452 0.51178288 0.55809313][0.12674563 0.22356986 0.33330292 0.43777302 0.51908076 0.55912077 0.55271292 0.51215142 0.46532419 0.43777728 0.43996418 0.46271688 0.48479959 0.50450313 0.53325939][0.083395213 0.16836788 0.27348775 0.377605 0.45942971 0.4971748 0.48685914 0.44475454 0.4004432 0.37999555 0.38900381 0.41604143 0.43809763 0.45065951 0.46467286][0.0381149 0.10751028 0.19976364 0.2920475 0.36350194 0.39327657 0.37859288 0.33734006 0.296884 0.28135547 0.2915284 0.31643179 0.33542037 0.34276852 0.34895962][-0.011296269 0.038045466 0.10894089 0.1793993 0.23217507 0.25037241 0.23221117 0.19461949 0.15968986 0.14708994 0.15387288 0.1710427 0.18312569 0.18548164 0.18820164]]...]
INFO - root - 2017-12-10 13:42:37.562861: step 19610, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 67h:21m:04s remains)
INFO - root - 2017-12-10 13:42:45.310670: step 19620, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 66h:23m:49s remains)
INFO - root - 2017-12-10 13:42:53.141373: step 19630, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 68h:35m:26s remains)
INFO - root - 2017-12-10 13:43:01.115959: step 19640, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 69h:41m:33s remains)
INFO - root - 2017-12-10 13:43:08.872737: step 19650, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 68h:11m:36s remains)
INFO - root - 2017-12-10 13:43:16.580009: step 19660, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 69h:44m:37s remains)
INFO - root - 2017-12-10 13:43:24.590242: step 19670, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.811 sec/batch; 70h:29m:12s remains)
INFO - root - 2017-12-10 13:43:32.483720: step 19680, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 67h:17m:41s remains)
INFO - root - 2017-12-10 13:43:40.368144: step 19690, loss = 0.67, batch loss = 0.62 (10.5 examples/sec; 0.760 sec/batch; 66h:04m:39s remains)
INFO - root - 2017-12-10 13:43:48.225153: step 19700, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 68h:59m:10s remains)
2017-12-10 13:43:48.958036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042444468 -0.042347103 -0.041273881 -0.040365085 -0.0401488 -0.040457558 -0.040835883 -0.040505428 -0.040420089 -0.040838148 -0.0422578 -0.044384103 -0.047501106 -0.051452294 -0.0545333][-0.040844325 -0.0390944 -0.035722233 -0.032502461 -0.030153582 -0.028481221 -0.027001536 -0.024507347 -0.02266765 -0.021392064 -0.02194513 -0.024772249 -0.030130578 -0.03869861 -0.047503892][-0.034857623 -0.029402914 -0.021204846 -0.013240797 -0.0062791761 4.024446e-05 0.0055768825 0.012029797 0.017163737 0.02149022 0.022441128 0.018303923 0.0096903611 -0.0049296562 -0.022103228][-0.025270887 -0.014522546 0.00062636571 0.01550678 0.02961245 0.0431513 0.054467846 0.0656344 0.074657612 0.082879081 0.08549998 0.079946257 0.067956969 0.046827555 0.019446239][-0.011791268 0.0060049822 0.030374385 0.054608554 0.078063414 0.10038251 0.11807667 0.13294883 0.14454816 0.15631 0.16178925 0.15677159 0.14356953 0.11759374 0.079717122][0.0069548688 0.03426294 0.07156238 0.10930439 0.14501889 0.17725924 0.20133887 0.21819541 0.22982289 0.24335478 0.252301 0.2500647 0.23773704 0.20873293 0.16062307][0.029334066 0.0677966 0.12058128 0.17450401 0.22414409 0.26706538 0.29845032 0.31712037 0.32734811 0.34103456 0.35245332 0.35224283 0.33944294 0.30771562 0.2502104][0.050915286 0.099772207 0.16663715 0.23419622 0.29329079 0.34176973 0.37744233 0.39644709 0.40444541 0.41767898 0.43205431 0.4345668 0.42067203 0.38618249 0.32148176][0.069377266 0.125992 0.20258528 0.27824226 0.34030145 0.38770437 0.42338759 0.44082996 0.44622827 0.45889321 0.47671109 0.48345971 0.46906605 0.43195218 0.36218649][0.081204653 0.14177354 0.22228988 0.29966813 0.35822982 0.39911774 0.4310284 0.44562644 0.4481045 0.45895895 0.47881973 0.4891938 0.47410285 0.43424886 0.36147431][0.078491993 0.137248 0.21462139 0.28726393 0.33728871 0.36822486 0.39382765 0.406025 0.40717843 0.41657031 0.4366363 0.44830945 0.43131119 0.38846144 0.31580356][0.0569125 0.10698083 0.17341596 0.23464157 0.27271363 0.29300183 0.31236881 0.324534 0.328738 0.339881 0.35989583 0.370287 0.3502413 0.3049213 0.2358325][0.024652436 0.060701393 0.10947131 0.15289792 0.17595889 0.18543483 0.19864637 0.21158808 0.22177435 0.23747312 0.25774133 0.26600879 0.24460664 0.20038225 0.13938703][-0.0066214125 0.014251086 0.04338444 0.067856632 0.077128746 0.078330658 0.085976727 0.098382324 0.11255234 0.13045642 0.14804044 0.1527472 0.13278684 0.094940834 0.046927534][-0.034899104 -0.027482871 -0.015128193 -0.0055114063 -0.0051432918 -0.0083379447 -0.0056040636 0.0032555989 0.015604352 0.029987544 0.041709378 0.04284583 0.027556999 0.00091233832 -0.029916868]]...]
INFO - root - 2017-12-10 13:43:56.871738: step 19710, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 67h:44m:55s remains)
INFO - root - 2017-12-10 13:44:04.734328: step 19720, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 68h:38m:10s remains)
INFO - root - 2017-12-10 13:44:12.443399: step 19730, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 69h:00m:46s remains)
INFO - root - 2017-12-10 13:44:20.413259: step 19740, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 67h:56m:40s remains)
INFO - root - 2017-12-10 13:44:28.129252: step 19750, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 68h:52m:16s remains)
INFO - root - 2017-12-10 13:44:36.034962: step 19760, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 68h:12m:26s remains)
INFO - root - 2017-12-10 13:44:43.846917: step 19770, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 69h:10m:33s remains)
INFO - root - 2017-12-10 13:44:51.788963: step 19780, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.776 sec/batch; 67h:25m:20s remains)
INFO - root - 2017-12-10 13:44:59.625938: step 19790, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 69h:24m:47s remains)
INFO - root - 2017-12-10 13:45:07.570226: step 19800, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 66h:59m:57s remains)
2017-12-10 13:45:08.362220: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20274483 0.18977378 0.14981267 0.095811091 0.038217261 -0.012235745 -0.051931188 -0.078574054 -0.086962484 -0.066976614 -0.0090316907 0.081735067 0.18394127 0.269107 0.3149156][0.2322437 0.225943 0.19154397 0.14014745 0.083078146 0.033028375 -0.0042442782 -0.02755592 -0.033576623 -0.012337307 0.048379052 0.14786646 0.26800278 0.37896737 0.45114285][0.22574459 0.22892679 0.2077886 0.17055281 0.12790297 0.092553504 0.06998539 0.058120791 0.055454034 0.0695626 0.11633816 0.20375945 0.32295668 0.44808969 0.54355669][0.1888268 0.20533274 0.20479408 0.19136299 0.17502412 0.16777414 0.17323276 0.1825505 0.18546465 0.18385477 0.1972468 0.24632283 0.33784017 0.4559316 0.56213892][0.14509086 0.17830864 0.20424238 0.22211479 0.24132152 0.27254555 0.31610122 0.35378113 0.36334863 0.338514 0.30141234 0.288107 0.32694283 0.41703427 0.51986521][0.10461009 0.15494265 0.20971373 0.26289597 0.32296085 0.39873356 0.48517624 0.55376786 0.56946647 0.516682 0.41890934 0.32928964 0.30127761 0.35050905 0.43993732][0.068340383 0.13209836 0.21191877 0.29792568 0.39769849 0.5178386 0.64647323 0.74467492 0.76537091 0.68522853 0.52877176 0.36550251 0.27257404 0.27972877 0.35111725][0.039133623 0.10949144 0.20407696 0.31126693 0.43871126 0.59127277 0.75058949 0.86995351 0.89368314 0.79398388 0.59679997 0.38300991 0.24499492 0.22037132 0.27438971][0.012934251 0.080113538 0.17563206 0.28804713 0.42510164 0.59041977 0.76090753 0.88626617 0.90869856 0.80124992 0.59110695 0.36224952 0.20987464 0.17193095 0.2150348][-0.007596741 0.045135997 0.1259699 0.22528987 0.35036021 0.5038166 0.66029561 0.77182645 0.78615355 0.68307859 0.48991811 0.28434095 0.15167895 0.12368687 0.16853227][-0.016231598 0.013748101 0.067090459 0.13778704 0.23244795 0.35356665 0.476541 0.56100553 0.56547576 0.47895107 0.3265675 0.17225824 0.083374009 0.081954971 0.13857511][-0.010909974 -0.0038843234 0.018358506 0.054497272 0.11067444 0.19002846 0.272501 0.3284598 0.32829639 0.26765546 0.16711184 0.073203646 0.033658568 0.061700206 0.13169108][0.0090750512 -0.0020565644 -0.0068751681 -0.0025009539 0.017119626 0.056593493 0.10269886 0.13606438 0.13681971 0.10286342 0.049597826 0.0078411875 0.0087821055 0.05990161 0.14033139][0.035152659 0.013341508 -0.0091706086 -0.026309907 -0.031822965 -0.019715572 0.0030579974 0.023862356 0.028413601 0.013740337 -0.0087812217 -0.018679434 0.0048224223 0.067419827 0.15356919][0.058766909 0.034655839 0.0057569849 -0.019800682 -0.035171743 -0.033678096 -0.018926004 -0.00074411777 0.0085035441 0.003984869 -0.0056332704 -0.0052157547 0.02139396 0.0814779 0.16562264]]...]
INFO - root - 2017-12-10 13:45:16.052528: step 19810, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 68h:00m:21s remains)
INFO - root - 2017-12-10 13:45:23.934591: step 19820, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 68h:38m:36s remains)
INFO - root - 2017-12-10 13:45:31.793939: step 19830, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 68h:24m:30s remains)
INFO - root - 2017-12-10 13:45:39.509444: step 19840, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 67h:23m:47s remains)
INFO - root - 2017-12-10 13:45:47.384495: step 19850, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 70h:11m:16s remains)
INFO - root - 2017-12-10 13:45:55.302561: step 19860, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.773 sec/batch; 67h:07m:05s remains)
INFO - root - 2017-12-10 13:46:03.121548: step 19870, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 69h:42m:01s remains)
INFO - root - 2017-12-10 13:46:11.027940: step 19880, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 68h:39m:10s remains)
INFO - root - 2017-12-10 13:46:18.803267: step 19890, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.779 sec/batch; 67h:38m:06s remains)
INFO - root - 2017-12-10 13:46:26.674191: step 19900, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 69h:58m:18s remains)
2017-12-10 13:46:27.566194: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31577829 0.36552581 0.38753781 0.37862459 0.35920179 0.34843317 0.35180947 0.34718254 0.32508972 0.3069804 0.29997188 0.30406135 0.31119132 0.31635612 0.32273567][0.32198215 0.37030235 0.39203086 0.38465452 0.36732492 0.3548637 0.35387361 0.34843019 0.3294518 0.31526968 0.31101668 0.31626591 0.32287368 0.32755771 0.33349228][0.30317149 0.3475869 0.36796415 0.36237806 0.34811762 0.33706325 0.33590731 0.33450764 0.32414505 0.31742537 0.31698418 0.32216412 0.32725629 0.33179274 0.33768928][0.29132032 0.33164743 0.34794605 0.34063554 0.32846907 0.32295266 0.3272596 0.33285606 0.33258003 0.33529159 0.34008414 0.34536973 0.35003519 0.35755384 0.36582613][0.28261966 0.31966722 0.33092815 0.32082948 0.31144196 0.31509706 0.32826638 0.34054396 0.34771717 0.3582516 0.36814669 0.37400353 0.37886274 0.39066324 0.40261638][0.27628461 0.31246907 0.32214406 0.31249464 0.3079797 0.3219988 0.34392685 0.36002016 0.36907381 0.3821547 0.39463046 0.40063035 0.40474349 0.41825131 0.43149281][0.28235281 0.32078373 0.33282045 0.32579866 0.32548967 0.34703213 0.37490332 0.39131698 0.39639363 0.40572017 0.41693476 0.42163515 0.4230426 0.43416008 0.44510725][0.30462244 0.34893382 0.36603031 0.361104 0.36009654 0.38158211 0.40964535 0.42324626 0.42094606 0.4224999 0.42931741 0.43092403 0.42777157 0.43292457 0.4387652][0.34105384 0.39174372 0.41262147 0.4064205 0.39872044 0.41216362 0.43448946 0.44348666 0.43403581 0.42752802 0.42926022 0.42727414 0.41891566 0.41562083 0.41435838][0.38338122 0.43538088 0.45370051 0.44136781 0.42302829 0.42359835 0.4363766 0.44093233 0.42804003 0.41682851 0.41563228 0.41210043 0.400242 0.38848147 0.38001755][0.41064417 0.4550232 0.46248049 0.43946341 0.40951878 0.39750639 0.4013851 0.40384108 0.39232498 0.38135797 0.38091916 0.37866032 0.36645326 0.35019398 0.33874598][0.39455539 0.42334053 0.41614786 0.38344961 0.34675038 0.32822597 0.32814628 0.33217898 0.32579455 0.31849307 0.3209469 0.3217217 0.31253058 0.29804215 0.28952754][0.31727168 0.32839802 0.31030345 0.27516323 0.24074556 0.22483335 0.22764185 0.23677769 0.23703042 0.23420225 0.23864277 0.24075864 0.2339876 0.22287656 0.21837397][0.19365913 0.1901128 0.16899134 0.14055668 0.11713112 0.11089773 0.12065478 0.13533939 0.14074744 0.14038539 0.14381266 0.14393041 0.13648555 0.12632908 0.12296744][0.061311863 0.04904104 0.031206293 0.013972977 0.0037326396 0.0070137247 0.02160698 0.038395267 0.045718145 0.045756545 0.046589211 0.043559961 0.034295794 0.023632098 0.019196764]]...]
INFO - root - 2017-12-10 13:46:35.543293: step 19910, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 68h:53m:43s remains)
INFO - root - 2017-12-10 13:46:43.354008: step 19920, loss = 0.67, batch loss = 0.61 (10.1 examples/sec; 0.791 sec/batch; 68h:40m:21s remains)
INFO - root - 2017-12-10 13:46:51.138857: step 19930, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 67h:31m:24s remains)
INFO - root - 2017-12-10 13:46:58.963713: step 19940, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 68h:01m:23s remains)
INFO - root - 2017-12-10 13:47:06.895608: step 19950, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.836 sec/batch; 72h:33m:53s remains)
INFO - root - 2017-12-10 13:47:14.857744: step 19960, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.771 sec/batch; 66h:57m:15s remains)
INFO - root - 2017-12-10 13:47:22.535458: step 19970, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 68h:43m:31s remains)
INFO - root - 2017-12-10 13:47:30.433087: step 19980, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 69h:49m:36s remains)
INFO - root - 2017-12-10 13:47:38.248007: step 19990, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 67h:08m:54s remains)
INFO - root - 2017-12-10 13:47:46.070057: step 20000, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 66h:45m:59s remains)
2017-12-10 13:47:46.847712: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.015023162 0.0069892504 -0.005929579 -0.016136825 -0.025619982 -0.0344334 -0.0377885 -0.028936207 -0.012739308 0.00030578187 0.0059751687 0.0034625623 -0.0089789 -0.033282015 -0.062740058][0.10797355 0.10381743 0.087739535 0.072201669 0.054650474 0.037180353 0.029102681 0.040002156 0.06276346 0.081015885 0.088430807 0.083011806 0.062473156 0.023475438 -0.025389213][0.22287554 0.22791055 0.21368645 0.19576374 0.17161442 0.14668424 0.13497892 0.14678554 0.17268936 0.19294171 0.19819304 0.1854939 0.15219197 0.094966345 0.023771841][0.32881156 0.3525666 0.35384741 0.34730491 0.32867837 0.30492049 0.29484484 0.30777302 0.33180305 0.34665334 0.34126681 0.31268379 0.2579897 0.17511487 0.076218709][0.4025901 0.45300546 0.48395425 0.50670034 0.51161265 0.50311404 0.50309056 0.52069449 0.54105914 0.54463905 0.5196647 0.46648678 0.38225174 0.26600003 0.13306068][0.44469258 0.52462566 0.59139431 0.65348464 0.692209 0.70630461 0.71858245 0.74142665 0.75706875 0.7455408 0.69489139 0.61410332 0.50174928 0.35516518 0.19090876][0.45962444 0.56407297 0.66039711 0.75823015 0.82947105 0.86586094 0.88945562 0.91846442 0.93308467 0.90858364 0.8327812 0.7268241 0.59356362 0.42644814 0.23973913][0.4418523 0.55591834 0.66391021 0.78008646 0.86993539 0.91886276 0.94715649 0.98056573 0.9990077 0.97035342 0.88074321 0.76223528 0.62328207 0.45243827 0.25905371][0.38489792 0.48615116 0.58042103 0.6885342 0.77520055 0.82148075 0.84504867 0.87830985 0.9039256 0.88196057 0.79659015 0.68587917 0.56332904 0.41226482 0.23518936][0.29386893 0.36429402 0.42549738 0.50394082 0.56941068 0.6015631 0.61477369 0.64364642 0.67555988 0.66611463 0.59879643 0.51138568 0.42050129 0.30633897 0.1652045][0.17469306 0.21009451 0.23518172 0.27693605 0.31347123 0.32677615 0.32763177 0.3478888 0.38030136 0.38226056 0.33827373 0.28027293 0.22521931 0.15377617 0.058597714][0.044139493 0.051670145 0.051019594 0.0635172 0.075804263 0.074298076 0.066465989 0.07753823 0.10465895 0.11251653 0.088518575 0.055981353 0.029743962 -0.0053896927 -0.056892041][-0.053310335 -0.061002504 -0.073132224 -0.076990038 -0.079125486 -0.088221274 -0.099514306 -0.095796913 -0.077784836 -0.07040298 -0.082066372 -0.09812934 -0.10742243 -0.11957219 -0.13949262][-0.1018566 -0.11362598 -0.12648742 -0.13539253 -0.14257178 -0.15260127 -0.16281855 -0.16353571 -0.15473679 -0.15113597 -0.15705189 -0.16458644 -0.16649577 -0.16757393 -0.16985582][-0.11417629 -0.12399768 -0.13233192 -0.13942893 -0.14572512 -0.15305386 -0.16030912 -0.16299991 -0.16078188 -0.1605252 -0.16407764 -0.1679669 -0.16805771 -0.16581504 -0.16132657]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 13:47:56.147321: step 20010, loss = 0.70, batch loss = 0.64 (13.0 examples/sec; 0.613 sec/batch; 53h:13m:55s remains)
INFO - root - 2017-12-10 13:48:03.946997: step 20020, loss = 0.68, batch loss = 0.63 (10.6 examples/sec; 0.755 sec/batch; 65h:32m:32s remains)
INFO - root - 2017-12-10 13:48:11.872268: step 20030, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 68h:18m:12s remains)
INFO - root - 2017-12-10 13:48:19.648912: step 20040, loss = 0.70, batch loss = 0.64 (10.9 examples/sec; 0.733 sec/batch; 63h:35m:56s remains)
INFO - root - 2017-12-10 13:48:27.354370: step 20050, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 68h:59m:52s remains)
INFO - root - 2017-12-10 13:48:35.215556: step 20060, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 69h:45m:09s remains)
INFO - root - 2017-12-10 13:48:43.109113: step 20070, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 66h:23m:56s remains)
INFO - root - 2017-12-10 13:48:50.976913: step 20080, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 68h:08m:47s remains)
INFO - root - 2017-12-10 13:48:58.837988: step 20090, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 69h:49m:28s remains)
INFO - root - 2017-12-10 13:49:06.596613: step 20100, loss = 0.71, batch loss = 0.65 (12.7 examples/sec; 0.628 sec/batch; 54h:30m:05s remains)
2017-12-10 13:49:07.447661: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035906076 0.030412031 0.025702706 0.024747277 0.027609324 0.032978352 0.037094109 0.040019013 0.037765544 0.028937558 0.014781924 -0.0030200258 -0.018403035 -0.02853393 -0.035716329][0.095903613 0.093901113 0.089405514 0.089275636 0.094922841 0.10318764 0.10730685 0.10761594 0.098758169 0.0798417 0.053803671 0.023723578 -0.0027795269 -0.020523313 -0.031664871][0.15775004 0.1593979 0.15595233 0.15814625 0.16873002 0.18126433 0.18490715 0.18027318 0.1618547 0.1305275 0.09120778 0.048374023 0.010183474 -0.015967803 -0.031186175][0.21506257 0.22104496 0.22048131 0.22675188 0.24397174 0.26171798 0.26402411 0.25115871 0.22013249 0.17500752 0.12212472 0.067106143 0.017853346 -0.016453534 -0.035614606][0.27537656 0.28217116 0.27935663 0.283553 0.30202937 0.32280633 0.32481268 0.30571264 0.26542836 0.21133313 0.14954486 0.084937014 0.025748018 -0.016321495 -0.040103007][0.334951 0.33793965 0.32858819 0.32511726 0.34018156 0.36293712 0.36804608 0.34662721 0.29999524 0.24004246 0.17222258 0.10030049 0.033491623 -0.014206376 -0.041487671][0.37926283 0.38488951 0.37556756 0.36821115 0.38076964 0.40568042 0.41490144 0.3906267 0.33499554 0.26594087 0.18998773 0.1102957 0.03706402 -0.01338231 -0.041330926][0.4055073 0.4231998 0.42522004 0.42230073 0.4356313 0.46251208 0.47357035 0.44291019 0.3743102 0.29286 0.20677845 0.11826926 0.038483843 -0.013747575 -0.040543489][0.40827402 0.44073483 0.45933694 0.46585408 0.48158628 0.5092563 0.51978391 0.48220858 0.40110797 0.30823824 0.21408686 0.11992842 0.036827639 -0.015462327 -0.040185794][0.3837271 0.42521548 0.45604524 0.46958828 0.48568246 0.51148081 0.5200699 0.47926766 0.39315987 0.29717946 0.20273833 0.11041839 0.03015925 -0.019522019 -0.041625794][0.33137372 0.37798837 0.41568837 0.43266612 0.445699 0.46441653 0.46662721 0.42433521 0.34099409 0.25157616 0.16613032 0.084588014 0.014622795 -0.0282848 -0.0463567][0.25447759 0.30356023 0.34522656 0.36437333 0.37310705 0.38127676 0.37315157 0.32920456 0.25338066 0.17691378 0.10700081 0.042492878 -0.011446938 -0.043314118 -0.055019092][0.15723369 0.20165356 0.24069795 0.25889856 0.26358175 0.26293522 0.24681379 0.20438746 0.14058061 0.081331775 0.031059872 -0.012458451 -0.046287224 -0.063388579 -0.06608633][0.051450972 0.082116537 0.10964802 0.12229504 0.12463901 0.12154548 0.10615108 0.0730855 0.027099287 -0.01220138 -0.042111456 -0.064925104 -0.079209067 -0.081750073 -0.075603649][-0.0326922 -0.01823209 -0.0055732979 -0.0013571358 -0.0012809163 -0.003035391 -0.011636419 -0.030477315 -0.056530897 -0.076864988 -0.0898126 -0.096910655 -0.09719158 -0.0902789 -0.079094581]]...]
INFO - root - 2017-12-10 13:49:15.226814: step 20110, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 67h:51m:10s remains)
INFO - root - 2017-12-10 13:49:22.910817: step 20120, loss = 0.71, batch loss = 0.65 (13.4 examples/sec; 0.597 sec/batch; 51h:45m:49s remains)
INFO - root - 2017-12-10 13:49:30.872260: step 20130, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 67h:35m:19s remains)
INFO - root - 2017-12-10 13:49:38.729122: step 20140, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 68h:15m:17s remains)
INFO - root - 2017-12-10 13:49:46.561234: step 20150, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 69h:57m:38s remains)
INFO - root - 2017-12-10 13:49:54.457807: step 20160, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 70h:14m:36s remains)
INFO - root - 2017-12-10 13:50:02.295378: step 20170, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 68h:10m:51s remains)
INFO - root - 2017-12-10 13:50:10.154528: step 20180, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 67h:59m:05s remains)
INFO - root - 2017-12-10 13:50:17.882068: step 20190, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 68h:04m:23s remains)
INFO - root - 2017-12-10 13:50:25.781774: step 20200, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 67h:54m:43s remains)
2017-12-10 13:50:26.569478: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.096499145 0.0808915 0.069861777 0.075354837 0.10063328 0.13663995 0.17085771 0.19540828 0.20354895 0.19272898 0.17048155 0.14549686 0.11972432 0.095272005 0.083498143][0.0881753 0.077636495 0.077981375 0.098597981 0.13818613 0.18296562 0.21774162 0.23488843 0.23149589 0.21103425 0.18502764 0.16277781 0.14310554 0.12405921 0.11522686][0.088028155 0.0819795 0.088640414 0.11572245 0.15995754 0.20542945 0.23569374 0.24406071 0.23046091 0.20221645 0.17310324 0.15240519 0.13703208 0.12254017 0.11696825][0.10835697 0.10362874 0.10930295 0.13347721 0.17307974 0.21211801 0.23451766 0.23464049 0.21344984 0.1790096 0.14514549 0.12104717 0.10470654 0.092526674 0.090806581][0.14091061 0.13715257 0.14096044 0.16237491 0.19874628 0.2335179 0.25110796 0.24630834 0.21980909 0.17804915 0.13436875 0.099661738 0.075471565 0.061216436 0.061623864][0.16846275 0.16804695 0.17456055 0.20035428 0.24188885 0.28087395 0.30106893 0.29691419 0.26838362 0.21898286 0.16279615 0.11455776 0.080162205 0.061010156 0.060328875][0.17239591 0.17876615 0.19521998 0.2351121 0.29239425 0.34597403 0.37734416 0.37927046 0.35142413 0.29607219 0.2305495 0.17400825 0.1355543 0.11704423 0.11842983][0.1446929 0.16014065 0.19079208 0.24897276 0.32622093 0.39925584 0.44739425 0.46162239 0.44119534 0.38855958 0.32407424 0.26977816 0.23677519 0.22620396 0.23373523][0.084964409 0.10935653 0.15379277 0.22757258 0.32098928 0.41049239 0.47534591 0.50674486 0.50342739 0.46784115 0.42028245 0.38222158 0.36465114 0.36748391 0.38245925][0.0072836 0.03546476 0.085868821 0.16362302 0.26040536 0.35558721 0.43135157 0.48099864 0.502482 0.49695796 0.48019809 0.46916768 0.47277719 0.48960787 0.50979555][-0.061741456 -0.03904314 0.0043744892 0.07044176 0.1544003 0.24144371 0.31825027 0.38017252 0.42378274 0.44770148 0.46098813 0.47431844 0.49447602 0.51962614 0.54068482][-0.10679261 -0.095088445 -0.0671572 -0.022868913 0.037117273 0.10469749 0.17140034 0.23389588 0.28750876 0.3281936 0.35871273 0.38401914 0.40952688 0.43442059 0.45208642][-0.12499442 -0.12305565 -0.11029609 -0.088107012 -0.054635055 -0.012749318 0.033436332 0.082374342 0.12992896 0.17123604 0.20488721 0.23153168 0.25461692 0.27391565 0.28538686][-0.12284794 -0.12686183 -0.12449086 -0.11832728 -0.10607489 -0.08794868 -0.065107755 -0.037291933 -0.0067559304 0.022770582 0.048619326 0.069254853 0.085998423 0.098172568 0.10352776][-0.11074122 -0.11736923 -0.12033214 -0.1226695 -0.12269535 -0.11981409 -0.11396945 -0.10429625 -0.091518104 -0.0776395 -0.064593822 -0.054064717 -0.045850452 -0.040829357 -0.040239856]]...]
INFO - root - 2017-12-10 13:50:34.330717: step 20210, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 66h:22m:38s remains)
INFO - root - 2017-12-10 13:50:42.255042: step 20220, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 68h:30m:34s remains)
INFO - root - 2017-12-10 13:50:50.238160: step 20230, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.822 sec/batch; 71h:15m:56s remains)
INFO - root - 2017-12-10 13:50:58.175554: step 20240, loss = 0.71, batch loss = 0.66 (10.5 examples/sec; 0.763 sec/batch; 66h:11m:13s remains)
INFO - root - 2017-12-10 13:51:06.060116: step 20250, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 66h:41m:11s remains)
INFO - root - 2017-12-10 13:51:13.928133: step 20260, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 67h:30m:04s remains)
INFO - root - 2017-12-10 13:51:21.750846: step 20270, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 66h:22m:35s remains)
INFO - root - 2017-12-10 13:51:29.311957: step 20280, loss = 0.71, batch loss = 0.65 (11.2 examples/sec; 0.714 sec/batch; 61h:56m:52s remains)
INFO - root - 2017-12-10 13:51:36.786720: step 20290, loss = 0.72, batch loss = 0.66 (10.7 examples/sec; 0.747 sec/batch; 64h:48m:41s remains)
INFO - root - 2017-12-10 13:51:44.632229: step 20300, loss = 0.70, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 65h:31m:44s remains)
2017-12-10 13:51:45.498659: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12094062 0.12115452 0.11456677 0.10603403 0.10009072 0.099389367 0.10067753 0.099322 0.095479958 0.091259308 0.087890461 0.082590461 0.069964156 0.052026104 0.034292258][0.13435091 0.13229819 0.12133596 0.10788208 0.096319251 0.09005335 0.088152073 0.087067895 0.085966907 0.084099412 0.081878334 0.077088505 0.066745333 0.054285068 0.043814179][0.13961577 0.13576308 0.12178501 0.10586648 0.091220476 0.081277989 0.076817729 0.07522656 0.075370692 0.074958824 0.074155964 0.072040319 0.066661969 0.061271533 0.058311459][0.14091857 0.13412373 0.11662151 0.098622262 0.082453236 0.071375445 0.066354468 0.064598918 0.0647479 0.063989609 0.063237362 0.06345012 0.063571841 0.065821953 0.070596695][0.14570743 0.13653392 0.11729281 0.099236436 0.083611116 0.073298879 0.06851314 0.065492436 0.062707737 0.057448912 0.052340213 0.051203322 0.054237127 0.062676251 0.074948139][0.14785179 0.1390264 0.12155381 0.1070296 0.095595844 0.089498691 0.087490007 0.084195927 0.077326454 0.064494066 0.050556764 0.043162469 0.044812907 0.056298733 0.074658938][0.13525316 0.12900719 0.11638059 0.10859241 0.1045218 0.10541713 0.10858203 0.10735216 0.098239787 0.079190373 0.056731749 0.041916888 0.039895572 0.051886611 0.074257359][0.11143904 0.10871971 0.10275067 0.10265373 0.10678452 0.11608062 0.12701115 0.1320598 0.12612034 0.10637429 0.07961189 0.058855224 0.051853489 0.061400283 0.083767407][0.081848316 0.082279585 0.082875431 0.089802682 0.10151403 0.11919606 0.13911729 0.15326063 0.15509644 0.14066161 0.1154886 0.092501834 0.081788227 0.088018015 0.10727613][0.056739978 0.059323315 0.064905323 0.076971747 0.09398707 0.11734221 0.14367914 0.16530912 0.17532744 0.16911295 0.15050831 0.13075687 0.12030457 0.12405746 0.13791551][0.037584316 0.041121691 0.049148779 0.063774332 0.083315074 0.10894097 0.13781093 0.16333039 0.17941932 0.18211479 0.17387874 0.1627515 0.15691245 0.16000925 0.16752076][0.025615735 0.029538784 0.037937246 0.052165307 0.070421129 0.093574055 0.11932179 0.14284071 0.16004451 0.16866107 0.17003931 0.16839162 0.16854069 0.17249715 0.17575926][0.018687084 0.022707293 0.030138757 0.041957729 0.056582816 0.074682422 0.0942996 0.1121297 0.12623842 0.13593502 0.14168586 0.14477363 0.14751926 0.15104902 0.15148576][0.00952674 0.012770665 0.018655788 0.027748 0.038714293 0.051923543 0.0657077 0.077548467 0.086997434 0.094435945 0.09985514 0.10294982 0.10441342 0.10498267 0.10207601][-0.0022549978 -0.00052593712 0.0034080241 0.0097047938 0.017020222 0.025336217 0.033757869 0.040481921 0.04583713 0.050568972 0.054377619 0.056387149 0.055960692 0.053306606 0.0473047]]...]
INFO - root - 2017-12-10 13:51:53.445135: step 20310, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 69h:53m:22s remains)
INFO - root - 2017-12-10 13:52:01.264052: step 20320, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 67h:25m:36s remains)
INFO - root - 2017-12-10 13:52:09.035313: step 20330, loss = 0.68, batch loss = 0.63 (10.6 examples/sec; 0.755 sec/batch; 65h:28m:44s remains)
INFO - root - 2017-12-10 13:52:16.964450: step 20340, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 67h:33m:32s remains)
INFO - root - 2017-12-10 13:52:24.859239: step 20350, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 66h:57m:45s remains)
INFO - root - 2017-12-10 13:52:32.798262: step 20360, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.821 sec/batch; 71h:09m:42s remains)
INFO - root - 2017-12-10 13:52:40.279360: step 20370, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 65h:59m:18s remains)
INFO - root - 2017-12-10 13:52:48.135780: step 20380, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 67h:20m:46s remains)
INFO - root - 2017-12-10 13:52:56.018392: step 20390, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 69h:41m:18s remains)
INFO - root - 2017-12-10 13:53:04.048065: step 20400, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.792 sec/batch; 68h:41m:02s remains)
2017-12-10 13:53:04.852860: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27757719 0.26633906 0.24539773 0.22399814 0.20697674 0.19702137 0.1901681 0.1823155 0.17631844 0.17269892 0.16716944 0.15940477 0.1552919 0.16080783 0.17631797][0.35061085 0.34148955 0.31842142 0.29387808 0.27392572 0.26134285 0.25330964 0.24809724 0.24825847 0.25076565 0.24476276 0.22650023 0.20161009 0.17824747 0.16429292][0.39628658 0.38989028 0.36525929 0.33816007 0.31649789 0.3029505 0.29635489 0.2975179 0.30859864 0.32265007 0.32185847 0.29857913 0.256853 0.20544602 0.1589][0.38677475 0.38167915 0.35572192 0.32765552 0.3081972 0.30014125 0.3030428 0.31727478 0.3420954 0.36705655 0.36990771 0.34186092 0.28686756 0.21438935 0.14293982][0.31976545 0.3137908 0.28852564 0.26322952 0.25117454 0.25581294 0.27686849 0.31105062 0.35140955 0.38405445 0.38542017 0.34906214 0.28159785 0.1950404 0.11011353][0.20393029 0.19661246 0.17622116 0.16077107 0.16463043 0.1914878 0.23872752 0.29684103 0.35082805 0.38344246 0.37429449 0.32198885 0.23944907 0.14376639 0.057188425][0.073397964 0.067660816 0.057862848 0.057922859 0.081342854 0.1307836 0.20137806 0.27770311 0.33796489 0.36369583 0.33940718 0.27056009 0.17610557 0.078348711 2.4032595e-06][-0.024893763 -0.027523713 -0.026214991 -0.012406689 0.024789905 0.086604461 0.16645022 0.24607009 0.301729 0.31605288 0.27973834 0.20302317 0.10784694 0.018927626 -0.043420319][-0.072267137 -0.073213071 -0.0652244 -0.044760924 -0.0048810476 0.054050054 0.12533021 0.19195087 0.23330967 0.23605843 0.19612435 0.12511791 0.043679368 -0.025902644 -0.067741282][-0.076120451 -0.078343086 -0.070433445 -0.052230038 -0.020589948 0.02313138 0.073640645 0.11832457 0.14262968 0.13837007 0.10478257 0.051345598 -0.0064242631 -0.052010205 -0.0743554][-0.054840546 -0.060896374 -0.058245417 -0.047796521 -0.028816802 -0.0032037774 0.025425227 0.049106624 0.059569333 0.053081956 0.030487975 -0.0024008953 -0.036273062 -0.06090378 -0.069423914][-0.024371162 -0.035492286 -0.040310029 -0.038900971 -0.031561051 -0.020501398 -0.0082776938 0.00067373039 0.0026362154 -0.0030652615 -0.01503826 -0.030759828 -0.046202082 -0.056521181 -0.057891559][0.0025557091 -0.013178453 -0.025059266 -0.030964237 -0.031333756 -0.028723005 -0.025443977 -0.023940446 -0.025307937 -0.028808413 -0.03316424 -0.038010255 -0.042615052 -0.045594964 -0.045073397][0.015778078 -0.0027280084 -0.018840928 -0.028448369 -0.031677149 -0.031375438 -0.030111736 -0.029641977 -0.030090291 -0.030552227 -0.030341029 -0.030188745 -0.030793324 -0.032341354 -0.033787232][0.013139422 -0.0051744389 -0.021807473 -0.031477962 -0.033599902 -0.030968092 -0.026579225 -0.022435075 -0.018987086 -0.016141266 -0.013951947 -0.013304608 -0.014893066 -0.019042684 -0.024776727]]...]
INFO - root - 2017-12-10 13:53:12.807665: step 20410, loss = 0.69, batch loss = 0.63 (9.3 examples/sec; 0.856 sec/batch; 74h:12m:54s remains)
INFO - root - 2017-12-10 13:53:20.719870: step 20420, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 67h:22m:29s remains)
INFO - root - 2017-12-10 13:53:28.522135: step 20430, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 66h:29m:26s remains)
INFO - root - 2017-12-10 13:53:36.379903: step 20440, loss = 0.67, batch loss = 0.62 (11.5 examples/sec; 0.696 sec/batch; 60h:19m:37s remains)
INFO - root - 2017-12-10 13:53:44.313728: step 20450, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 69h:34m:57s remains)
INFO - root - 2017-12-10 13:53:52.023539: step 20460, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 69h:53m:45s remains)
INFO - root - 2017-12-10 13:53:59.954802: step 20470, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 69h:09m:26s remains)
INFO - root - 2017-12-10 13:54:07.756762: step 20480, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.751 sec/batch; 65h:03m:49s remains)
INFO - root - 2017-12-10 13:54:15.476210: step 20490, loss = 0.67, batch loss = 0.61 (10.6 examples/sec; 0.755 sec/batch; 65h:25m:16s remains)
INFO - root - 2017-12-10 13:54:23.397547: step 20500, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 67h:21m:44s remains)
2017-12-10 13:54:24.209649: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33448872 0.3516517 0.35859662 0.35374421 0.3393876 0.32601798 0.32308614 0.32505929 0.32932669 0.33246583 0.32992819 0.32631728 0.31595987 0.30423561 0.30505145][0.37137643 0.3736909 0.36375552 0.34319776 0.31639704 0.29343218 0.28114119 0.27283368 0.26719955 0.2642436 0.26090759 0.260355 0.25622308 0.2540451 0.26616147][0.39165732 0.38043356 0.35637116 0.32557392 0.29407984 0.26997054 0.25516328 0.24271551 0.23306468 0.22948612 0.23029228 0.23566853 0.23703986 0.23784605 0.25002673][0.38941568 0.37017202 0.34198636 0.31486014 0.29253706 0.27825922 0.26722974 0.25365865 0.24256954 0.2409147 0.24769178 0.25841349 0.26303118 0.26142535 0.26405928][0.37285158 0.35594 0.33931506 0.33233908 0.33175519 0.33311653 0.32540688 0.305088 0.28562844 0.28038436 0.28928921 0.30405733 0.31272647 0.30880824 0.29911005][0.3420909 0.3327992 0.34124282 0.36923444 0.40278357 0.42784914 0.42832088 0.40123641 0.36767489 0.35029122 0.35266986 0.36647487 0.37762108 0.37281269 0.35430506][0.29463765 0.29509208 0.33062819 0.39619991 0.46770597 0.52245945 0.53987008 0.51419985 0.47006688 0.43982863 0.43334794 0.44522145 0.45925927 0.456608 0.43506527][0.23134376 0.24046047 0.29713362 0.39297408 0.4982712 0.58373696 0.6233803 0.60697645 0.55875266 0.51888907 0.50444537 0.51468372 0.53319913 0.53774929 0.521128][0.15348947 0.16380712 0.22588679 0.33238766 0.45460403 0.56154495 0.623733 0.62636632 0.58841568 0.55199134 0.53888392 0.55289263 0.57972074 0.59607679 0.59099495][0.086713426 0.087913431 0.13546392 0.22631195 0.33874092 0.44530246 0.51808494 0.5400784 0.5221141 0.50146437 0.49993417 0.52397716 0.56201565 0.59245038 0.60447532][0.06491553 0.047135897 0.061669964 0.11380538 0.19229358 0.27623689 0.3422935 0.37387776 0.37380403 0.37047574 0.38210759 0.41571316 0.46276432 0.50510418 0.53464645][0.10793845 0.072254628 0.048970491 0.053098157 0.0839089 0.12814435 0.16842155 0.19145563 0.19564793 0.20029844 0.21848686 0.25558174 0.30542335 0.35324079 0.39392528][0.19612674 0.15634118 0.1093272 0.0757778 0.062365208 0.062934361 0.066356294 0.063942209 0.054388762 0.051207267 0.063470922 0.093391553 0.13554123 0.17820136 0.21882118][0.27658346 0.25219706 0.20724204 0.16126943 0.1229736 0.091776371 0.061997965 0.028970469 -0.0057158857 -0.030123902 -0.035817347 -0.023336606 0.001343586 0.028175279 0.056272462][0.30843672 0.31167898 0.29112604 0.25998268 0.22390583 0.18406555 0.1369925 0.0810102 0.021330731 -0.02892418 -0.059286565 -0.070538439 -0.068424389 -0.062298764 -0.053961679]]...]
INFO - root - 2017-12-10 13:54:32.076441: step 20510, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.786 sec/batch; 68h:05m:18s remains)
INFO - root - 2017-12-10 13:54:39.813751: step 20520, loss = 0.71, batch loss = 0.65 (12.4 examples/sec; 0.647 sec/batch; 56h:06m:22s remains)
INFO - root - 2017-12-10 13:54:47.679999: step 20530, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.755 sec/batch; 65h:26m:15s remains)
INFO - root - 2017-12-10 13:54:55.473247: step 20540, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.756 sec/batch; 65h:30m:50s remains)
INFO - root - 2017-12-10 13:55:03.066345: step 20550, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 67h:18m:58s remains)
INFO - root - 2017-12-10 13:55:10.932793: step 20560, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 67h:40m:36s remains)
INFO - root - 2017-12-10 13:55:18.831120: step 20570, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 69h:03m:49s remains)
INFO - root - 2017-12-10 13:55:26.784941: step 20580, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 70h:29m:58s remains)
INFO - root - 2017-12-10 13:55:34.639422: step 20590, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 66h:15m:14s remains)
INFO - root - 2017-12-10 13:55:42.650894: step 20600, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.796 sec/batch; 68h:59m:20s remains)
2017-12-10 13:55:43.357312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0532737 -0.050606985 -0.047981892 -0.045883462 -0.044979386 -0.045552209 -0.047280628 -0.049530338 -0.051462322 -0.052977763 -0.054255337 -0.055468835 -0.05599425 -0.055478923 -0.053799402][-0.043159168 -0.034382157 -0.025047654 -0.017018583 -0.012772563 -0.012935034 -0.015823534 -0.019268984 -0.022196703 -0.025393011 -0.029117579 -0.033151731 -0.03626753 -0.03734668 -0.035423581][-0.026242364 -0.0078132 0.013691892 0.033334509 0.045260128 0.048363719 0.046519175 0.04368905 0.040446468 0.0347855 0.027178667 0.019007763 0.012166399 0.0088213012 0.011120035][-0.0083561447 0.022700764 0.062073231 0.10054416 0.12716132 0.13956778 0.14331162 0.142919 0.137906 0.12599179 0.11089611 0.096306995 0.084252715 0.077996038 0.081379086][0.009786332 0.057153136 0.12027264 0.18415692 0.23063655 0.25508532 0.26383713 0.26159397 0.24873966 0.22583438 0.20188206 0.18195674 0.16614379 0.15806815 0.16320761][0.027220728 0.092859231 0.18180336 0.27246398 0.33822438 0.37157592 0.37923959 0.36737528 0.340821 0.30587569 0.27595142 0.25440055 0.23728698 0.22850934 0.23529087][0.040681116 0.12261861 0.23380521 0.346092 0.42430207 0.45818427 0.45581636 0.4277 0.38613796 0.34340826 0.31371078 0.2959877 0.28191045 0.27497146 0.28290251][0.047352884 0.14009154 0.26490179 0.38800034 0.46741891 0.49209294 0.47401613 0.43032423 0.38052714 0.33957568 0.31779385 0.3088412 0.30166665 0.29835147 0.30497497][0.047717668 0.14460012 0.272587 0.39437813 0.46495458 0.47498262 0.44249344 0.39171645 0.34608397 0.317985 0.31037521 0.31285635 0.31382272 0.31308115 0.31367603][0.043762248 0.13927118 0.2626164 0.37561238 0.43414155 0.43211105 0.3932783 0.34677565 0.31560463 0.30597231 0.31344581 0.32623085 0.33340982 0.33259654 0.32408485][0.036149073 0.12542878 0.23864329 0.34020254 0.39028415 0.38568288 0.35249573 0.320214 0.30757797 0.31417271 0.3317813 0.35073867 0.36097354 0.35841402 0.34190804][0.025682054 0.10541574 0.20562387 0.29604676 0.3427994 0.34392482 0.32376271 0.30814779 0.3101573 0.3245306 0.34358487 0.36273196 0.37331405 0.37047863 0.35186541][0.014334161 0.082670927 0.16797742 0.24606867 0.28945839 0.2967352 0.2889336 0.28624386 0.29580367 0.30879152 0.32081696 0.33366334 0.34174728 0.341115 0.32776853][1.2054444e-06 0.054392 0.12164696 0.18353938 0.21915537 0.22753669 0.2260906 0.22959101 0.24059187 0.24840578 0.25129926 0.25499159 0.2580699 0.25998819 0.25590712][-0.021363389 0.0157723 0.061859652 0.10415296 0.12796225 0.1326993 0.13172816 0.13530011 0.143426 0.14584529 0.14176957 0.13739063 0.13522257 0.13893008 0.14426221]]...]
INFO - root - 2017-12-10 13:55:51.285299: step 20610, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.825 sec/batch; 71h:27m:29s remains)
INFO - root - 2017-12-10 13:55:59.320758: step 20620, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 69h:53m:07s remains)
INFO - root - 2017-12-10 13:56:07.222922: step 20630, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 68h:46m:52s remains)
INFO - root - 2017-12-10 13:56:14.891008: step 20640, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 65h:47m:55s remains)
INFO - root - 2017-12-10 13:56:22.712800: step 20650, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 69h:32m:03s remains)
INFO - root - 2017-12-10 13:56:30.556241: step 20660, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 66h:34m:30s remains)
INFO - root - 2017-12-10 13:56:38.501844: step 20670, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 67h:19m:53s remains)
INFO - root - 2017-12-10 13:56:46.288264: step 20680, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 67h:21m:14s remains)
INFO - root - 2017-12-10 13:56:54.004543: step 20690, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.826 sec/batch; 71h:30m:04s remains)
INFO - root - 2017-12-10 13:57:01.896744: step 20700, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 67h:38m:59s remains)
2017-12-10 13:57:02.767409: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10492093 -0.085700452 -0.058584291 -0.02737049 0.0050332188 0.033429239 0.052793466 0.060427595 0.054379046 0.038091969 0.020603441 0.0097838938 0.008718878 0.016800884 0.03233549][-0.057449427 -0.042486697 -0.017307755 0.01661391 0.053794641 0.086438067 0.10812047 0.11368798 0.098010033 0.064193435 0.026282866 -0.004401539 -0.022948392 -0.028797744 -0.023272645][0.026948335 0.039357226 0.061765157 0.096109748 0.13547379 0.17051096 0.19349132 0.1967186 0.17245331 0.12279914 0.065207236 0.013642167 -0.02353958 -0.044572428 -0.050574914][0.11621621 0.13033593 0.15326831 0.18840398 0.2289605 0.26658621 0.29227144 0.29533216 0.26581532 0.20580554 0.13450047 0.066147782 0.011618641 -0.024381135 -0.041396994][0.17738037 0.19929002 0.229184 0.26850736 0.3116799 0.35361496 0.38421017 0.38983816 0.35898936 0.29564345 0.21937983 0.14237833 0.076279029 0.029019807 0.0031969263][0.19051188 0.2243873 0.26634532 0.31422943 0.36422563 0.41446018 0.45289102 0.4629584 0.4333744 0.37176654 0.29743868 0.2202125 0.15183772 0.10185934 0.073089465][0.15660477 0.202091 0.25754225 0.31736335 0.3786799 0.44048855 0.48739916 0.50064909 0.47228718 0.41511184 0.34776118 0.27765113 0.21616031 0.17271516 0.14817771][0.089623377 0.13857912 0.20338188 0.27500156 0.3490167 0.4218224 0.47421 0.48754224 0.4598771 0.40891883 0.35158974 0.29327619 0.24519289 0.21569419 0.20283771][0.02078753 0.061049778 0.12555526 0.20231828 0.28306928 0.35952854 0.41009566 0.41869548 0.39055011 0.34673831 0.30139473 0.25812191 0.22763743 0.21607755 0.21869324][-0.024868341 -0.0011371995 0.052719492 0.12302652 0.19788148 0.2651991 0.30373943 0.30237356 0.27257136 0.23569746 0.20225754 0.17488332 0.16347265 0.17076634 0.18886659][-0.0353078 -0.02743973 0.010745526 0.064614095 0.1208957 0.16697429 0.18534565 0.1713313 0.14005369 0.10948939 0.086003833 0.071858972 0.075375386 0.096097767 0.12386589][-0.0096143708 -0.0080565494 0.017868208 0.053561274 0.086249009 0.1055037 0.099419951 0.070037358 0.034217864 0.0059639742 -0.011676989 -0.017819652 -0.0069674286 0.018300673 0.047520749][0.041548893 0.0498802 0.074305467 0.10014708 0.11456173 0.1098502 0.07977131 0.03225071 -0.015391693 -0.051031169 -0.0715257 -0.077525683 -0.067051783 -0.044448763 -0.019406948][0.10047585 0.12896 0.16756095 0.19946811 0.20830014 0.18787837 0.13730617 0.069413535 0.0011074963 -0.053974845 -0.08795289 -0.10170291 -0.097216129 -0.081440806 -0.063363895][0.14978263 0.20494646 0.26910758 0.32071808 0.33620816 0.30885661 0.24283484 0.15513226 0.06185412 -0.01917352 -0.0722746 -0.0972721 -0.10058863 -0.092457511 -0.081326246]]...]
INFO - root - 2017-12-10 13:57:10.586957: step 20710, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.773 sec/batch; 66h:55m:55s remains)
INFO - root - 2017-12-10 13:57:18.437001: step 20720, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 68h:19m:54s remains)
INFO - root - 2017-12-10 13:57:26.225706: step 20730, loss = 0.69, batch loss = 0.64 (9.0 examples/sec; 0.886 sec/batch; 76h:45m:43s remains)
INFO - root - 2017-12-10 13:57:34.053768: step 20740, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.791 sec/batch; 68h:30m:36s remains)
INFO - root - 2017-12-10 13:57:41.959050: step 20750, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 67h:12m:49s remains)
INFO - root - 2017-12-10 13:57:49.899521: step 20760, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.773 sec/batch; 66h:55m:24s remains)
INFO - root - 2017-12-10 13:57:57.442561: step 20770, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 67h:38m:40s remains)
INFO - root - 2017-12-10 13:58:05.319249: step 20780, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 66h:42m:07s remains)
INFO - root - 2017-12-10 13:58:13.174511: step 20790, loss = 0.73, batch loss = 0.67 (10.1 examples/sec; 0.788 sec/batch; 68h:15m:50s remains)
INFO - root - 2017-12-10 13:58:20.992390: step 20800, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 68h:59m:34s remains)
2017-12-10 13:58:21.824976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.051477812 -0.053204615 -0.052382443 -0.050885838 -0.050378256 -0.051924273 -0.055750519 -0.06088632 -0.065653153 -0.068288147 -0.067963921 -0.06589026 -0.063713096 -0.062252428 -0.061387394][-0.043475714 -0.045357041 -0.040201738 -0.030044664 -0.020023596 -0.015192539 -0.018617691 -0.029542329 -0.043761536 -0.055912785 -0.062125437 -0.062917776 -0.061037935 -0.059165761 -0.058421437][-0.021209445 -0.021396561 -0.0077880253 0.018062562 0.046071246 0.064189427 0.063611962 0.043411173 0.011242262 -0.02088993 -0.042776119 -0.05215643 -0.05286327 -0.05059164 -0.049132638][0.0093254074 0.012705934 0.038448788 0.08660192 0.14132589 0.18081337 0.18728095 0.15597004 0.098346338 0.035794847 -0.011675898 -0.036624055 -0.043301605 -0.041124687 -0.037989419][0.03819878 0.046417847 0.08577022 0.15896915 0.24495503 0.31107607 0.32870424 0.28764966 0.20241593 0.1046037 0.025775868 -0.019524736 -0.034965288 -0.033623781 -0.028134167][0.054217082 0.067334935 0.11787493 0.21148765 0.32400033 0.41399497 0.44359115 0.39726231 0.29088581 0.16381267 0.057577327 -0.0061646425 -0.029954122 -0.029542008 -0.02159475][0.050943621 0.06700322 0.12195115 0.22333941 0.34714147 0.44865125 0.48597348 0.44043806 0.32739747 0.18844913 0.069732651 -0.0028474771 -0.030582165 -0.030025933 -0.019942544][0.030469339 0.04585731 0.09601742 0.18856159 0.30298188 0.39827758 0.43547651 0.39619967 0.29383546 0.16568936 0.054969523 -0.012793679 -0.037994489 -0.035855547 -0.024457192][0.0018929597 0.012887792 0.050321478 0.1200702 0.20740601 0.28080019 0.31007892 0.28038257 0.20225434 0.10394274 0.019024964 -0.031817276 -0.048859574 -0.04428589 -0.032829549][-0.024376754 -0.020009162 0.0011893169 0.0425735 0.095491849 0.14005119 0.15727219 0.13745233 0.0880539 0.027012005 -0.024447668 -0.052961953 -0.059527539 -0.052629787 -0.042350344][-0.042147014 -0.044369906 -0.037956703 -0.021662576 0.00065301324 0.019302152 0.025163228 0.013435252 -0.011263617 -0.039703403 -0.061491016 -0.070408642 -0.068316184 -0.060349938 -0.052057534][-0.050874423 -0.057425614 -0.060636733 -0.060485207 -0.057913359 -0.055980157 -0.057387386 -0.063501962 -0.071961261 -0.079350665 -0.082334846 -0.079582304 -0.073223166 -0.0659626 -0.060070116][-0.053814579 -0.061243687 -0.0676107 -0.073554061 -0.078887492 -0.083689 -0.087717749 -0.090637393 -0.091448173 -0.0896563 -0.085210577 -0.078910738 -0.072538778 -0.067336716 -0.06378758][-0.054353677 -0.060160022 -0.065149575 -0.0704337 -0.0758001 -0.0807544 -0.084448226 -0.086123221 -0.08529333 -0.082220785 -0.077600785 -0.07252381 -0.068223506 -0.065264307 -0.0635015][-0.05431493 -0.05811673 -0.060662154 -0.063457 -0.066506 -0.069585226 -0.072166093 -0.073659584 -0.073673077 -0.07225655 -0.069769993 -0.066883393 -0.06440714 -0.062758096 -0.061815355]]...]
INFO - root - 2017-12-10 13:58:29.514348: step 20810, loss = 0.69, batch loss = 0.63 (12.2 examples/sec; 0.654 sec/batch; 56h:37m:14s remains)
INFO - root - 2017-12-10 13:58:37.475747: step 20820, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 67h:25m:13s remains)
INFO - root - 2017-12-10 13:58:45.334618: step 20830, loss = 0.67, batch loss = 0.62 (10.2 examples/sec; 0.787 sec/batch; 68h:08m:59s remains)
INFO - root - 2017-12-10 13:58:53.149085: step 20840, loss = 0.70, batch loss = 0.64 (10.8 examples/sec; 0.743 sec/batch; 64h:21m:19s remains)
INFO - root - 2017-12-10 13:59:00.914395: step 20850, loss = 0.68, batch loss = 0.62 (9.7 examples/sec; 0.823 sec/batch; 71h:15m:26s remains)
INFO - root - 2017-12-10 13:59:08.817779: step 20860, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 67h:24m:04s remains)
INFO - root - 2017-12-10 13:59:16.750155: step 20870, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 68h:05m:16s remains)
INFO - root - 2017-12-10 13:59:24.711283: step 20880, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 68h:48m:21s remains)
INFO - root - 2017-12-10 13:59:32.669422: step 20890, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.832 sec/batch; 71h:58m:30s remains)
INFO - root - 2017-12-10 13:59:40.455652: step 20900, loss = 0.69, batch loss = 0.63 (12.7 examples/sec; 0.627 sec/batch; 54h:18m:39s remains)
2017-12-10 13:59:41.225974: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.52394235 0.39640164 0.24215122 0.12420929 0.068269812 0.067831889 0.095370285 0.12743555 0.15345515 0.1720199 0.18401282 0.1942459 0.20531611 0.21549754 0.22506787][0.53199351 0.40316451 0.24388038 0.12029074 0.059504639 0.055817563 0.0812232 0.11026784 0.13170591 0.14496383 0.15303783 0.16273455 0.17675783 0.19194114 0.20584963][0.44418195 0.33286226 0.19636558 0.094651408 0.0518304 0.061649617 0.096697025 0.12906978 0.14624263 0.14976361 0.14706211 0.1480948 0.15822874 0.17482014 0.1926847][0.29980507 0.21997461 0.12686449 0.067067541 0.059672926 0.097770669 0.15347493 0.19585425 0.21053471 0.2008273 0.17900358 0.16023216 0.15577908 0.1665554 0.18541516][0.1475517 0.10518294 0.064763963 0.05739047 0.095954627 0.17233184 0.25698367 0.31459317 0.32728204 0.29939184 0.24746333 0.19425264 0.16068985 0.15536106 0.17063053][0.034393907 0.025039025 0.03191537 0.073299177 0.15673499 0.27220634 0.38762757 0.46314442 0.4757928 0.42870185 0.34142122 0.24464738 0.17098002 0.13969876 0.14546463][-0.022359574 -0.0076354146 0.03298375 0.10922768 0.22495094 0.36894685 0.50775838 0.59851205 0.61303616 0.55136955 0.43345156 0.29625362 0.18320346 0.12408719 0.11732827][-0.038891833 -0.0122743 0.043820642 0.13484293 0.26271942 0.41691384 0.56557292 0.6652422 0.68379682 0.61673206 0.4821409 0.31976596 0.18060511 0.10175138 0.085348949][-0.044254184 -0.018671624 0.034635849 0.11988603 0.23888767 0.383517 0.526006 0.6253233 0.64787674 0.58516 0.45224315 0.28812155 0.1458234 0.064169683 0.047498841][-0.049762256 -0.034392543 0.0034855043 0.068496875 0.163227 0.28248519 0.40396053 0.49226648 0.51598924 0.46470734 0.34993547 0.20635158 0.082603328 0.014198288 0.005783299][-0.054284275 -0.051602304 -0.033776678 0.0043828478 0.066598639 0.15057661 0.24005733 0.30764014 0.32796943 0.29125386 0.20558022 0.098078504 0.0074837776 -0.037973464 -0.034786869][-0.05571232 -0.062258273 -0.061067048 -0.047434557 -0.016792798 0.031143012 0.08609204 0.1291745 0.14248514 0.11880306 0.063691363 -0.0042289547 -0.058045071 -0.07886792 -0.06550546][-0.052358791 -0.062152822 -0.069992386 -0.07276696 -0.065744571 -0.046570558 -0.020209802 0.0018328109 0.0081191948 -0.0061679794 -0.037151627 -0.073135287 -0.097633913 -0.10011408 -0.081853122][-0.045568764 -0.053219687 -0.06198309 -0.070984483 -0.07645867 -0.075075813 -0.0676635 -0.05997343 -0.058718409 -0.066997439 -0.082366176 -0.098027818 -0.10487624 -0.0981393 -0.08062999][-0.040157985 -0.043715075 -0.048976123 -0.056991719 -0.065594576 -0.07175146 -0.074092053 -0.074415773 -0.075891882 -0.080375925 -0.086638793 -0.091342531 -0.090168767 -0.081526965 -0.068481848]]...]
INFO - root - 2017-12-10 13:59:49.035673: step 20910, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 66h:34m:12s remains)
INFO - root - 2017-12-10 13:59:56.737035: step 20920, loss = 0.70, batch loss = 0.64 (13.5 examples/sec; 0.592 sec/batch; 51h:16m:01s remains)
INFO - root - 2017-12-10 14:00:04.690586: step 20930, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 70h:07m:33s remains)
INFO - root - 2017-12-10 14:00:12.609910: step 20940, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 66h:04m:23s remains)
INFO - root - 2017-12-10 14:00:20.469171: step 20950, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.770 sec/batch; 66h:39m:11s remains)
INFO - root - 2017-12-10 14:00:28.376089: step 20960, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 70h:09m:27s remains)
INFO - root - 2017-12-10 14:00:36.255829: step 20970, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 66h:36m:13s remains)
INFO - root - 2017-12-10 14:00:44.181330: step 20980, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 67h:38m:30s remains)
INFO - root - 2017-12-10 14:00:51.879463: step 20990, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 67h:57m:28s remains)
INFO - root - 2017-12-10 14:00:59.596966: step 21000, loss = 0.67, batch loss = 0.61 (10.6 examples/sec; 0.753 sec/batch; 65h:07m:07s remains)
2017-12-10 14:01:00.436225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0067831939 -0.023654832 -0.031345535 -0.024924092 -0.0017349669 0.03627624 0.074791938 0.099521048 0.10339813 0.086453967 0.058126807 0.031223133 0.018070046 0.023845088 0.047501095][0.037090715 0.015536253 0.0010341817 0.0032580625 0.02877659 0.077154912 0.13041934 0.16853938 0.17869517 0.15796882 0.11754178 0.074251086 0.045290958 0.039566904 0.056445505][0.097423755 0.072802782 0.051058441 0.047820214 0.073793367 0.13030015 0.19661455 0.24715492 0.26279336 0.23720795 0.18267493 0.12007892 0.071533352 0.049523707 0.054131877][0.16377828 0.13946888 0.11290279 0.10559072 0.13076729 0.19068488 0.26327926 0.31936404 0.33535078 0.30280703 0.23545204 0.15699302 0.092105 0.054974675 0.045820314][0.21860187 0.19924101 0.1725527 0.1645124 0.18906973 0.24833062 0.32000619 0.37370846 0.38373497 0.34148321 0.26308954 0.17447464 0.10050955 0.054033611 0.034018762][0.24248098 0.232193 0.21169674 0.20814979 0.23438743 0.29140219 0.35745817 0.40268546 0.40191981 0.34852919 0.26112336 0.16744761 0.091671027 0.043424174 0.01910785][0.23870851 0.24046817 0.23100299 0.23523504 0.26339006 0.31513417 0.37052515 0.40219605 0.38839284 0.32569635 0.23431264 0.14274533 0.0728788 0.030306024 0.0087940758][0.21025488 0.22382639 0.22730251 0.2405363 0.2698023 0.313352 0.35418776 0.36960065 0.34377456 0.27735898 0.19099921 0.11138418 0.056176286 0.026448583 0.013483617][0.16439918 0.18722431 0.20312507 0.22592907 0.25756037 0.29444644 0.32283089 0.32491696 0.29116768 0.22676051 0.15247725 0.09181989 0.056767415 0.043682653 0.041823789][0.12623923 0.15514009 0.18084353 0.21236604 0.2487742 0.28480017 0.30920756 0.30722341 0.27314678 0.21515322 0.15412472 0.11127673 0.093367331 0.093319185 0.099219821][0.11523377 0.14726332 0.17792195 0.21391994 0.25406322 0.29228461 0.31943378 0.32215166 0.2957283 0.24888523 0.20164567 0.17352965 0.1671751 0.17376058 0.18129036][0.14533053 0.18093637 0.21183996 0.24505195 0.28198469 0.31811583 0.34729898 0.35794598 0.34442481 0.31361452 0.28190121 0.26564065 0.26422831 0.26861066 0.26916018][0.20859814 0.25019732 0.27944264 0.30499417 0.33238325 0.36066231 0.38776344 0.40449378 0.40340137 0.38816574 0.36933783 0.35846242 0.35287675 0.34500524 0.33007783][0.27879971 0.32569781 0.35144636 0.36716565 0.38277951 0.40128094 0.4239153 0.44331798 0.45051834 0.44598156 0.43418279 0.42125922 0.40399459 0.37747347 0.342707][0.31701314 0.36334687 0.38265008 0.38826558 0.39363655 0.404657 0.42391413 0.44488829 0.45771578 0.45980465 0.45020857 0.43078634 0.39974472 0.35563898 0.3043305]]...]
INFO - root - 2017-12-10 14:01:08.247908: step 21010, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 67h:11m:35s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 14:01:16.103855: step 21020, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 66h:40m:23s remains)
INFO - root - 2017-12-10 14:01:23.916820: step 21030, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 69h:38m:04s remains)
INFO - root - 2017-12-10 14:01:31.798069: step 21040, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 66h:11m:15s remains)
INFO - root - 2017-12-10 14:01:39.605901: step 21050, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 65h:59m:39s remains)
INFO - root - 2017-12-10 14:01:47.376298: step 21060, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 66h:45m:05s remains)
INFO - root - 2017-12-10 14:01:55.277564: step 21070, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 68h:26m:50s remains)
INFO - root - 2017-12-10 14:02:02.894681: step 21080, loss = 0.72, batch loss = 0.66 (10.9 examples/sec; 0.731 sec/batch; 63h:15m:26s remains)
INFO - root - 2017-12-10 14:02:10.647363: step 21090, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 68h:01m:12s remains)
INFO - root - 2017-12-10 14:02:18.480095: step 21100, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 67h:02m:20s remains)
2017-12-10 14:02:19.399817: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.055341937 0.047843203 0.04389748 0.046426408 0.05267784 0.058641151 0.063778728 0.065629914 0.0651778 0.065782122 0.068579622 0.070300817 0.06913843 0.066858836 0.064580448][0.097263165 0.090463661 0.088959806 0.095547274 0.10534422 0.11263978 0.11736615 0.11704705 0.1144142 0.11573487 0.12352499 0.13173378 0.13576581 0.13723935 0.13749006][0.12400534 0.12221129 0.12613629 0.13741523 0.14944084 0.15539655 0.15656193 0.15139945 0.14452885 0.14441578 0.15491414 0.16852923 0.17869392 0.18674181 0.19322026][0.13161311 0.13968563 0.15275334 0.17144285 0.18944179 0.19810535 0.19846642 0.18929482 0.1767185 0.16939573 0.17219387 0.18017966 0.18952881 0.2026592 0.21669413][0.12977552 0.15263563 0.18003991 0.21326509 0.24898987 0.27323461 0.28191212 0.2723884 0.25231358 0.22809327 0.20629245 0.1902345 0.18486963 0.19452101 0.21063519][0.13117558 0.17247026 0.22017834 0.27843806 0.34869313 0.4059698 0.43476924 0.42903554 0.3991971 0.34821814 0.28525 0.22660063 0.19062264 0.18379426 0.19102235][0.14102639 0.20126523 0.27238432 0.36271784 0.478146 0.578652 0.63375175 0.63349843 0.59188235 0.50966525 0.39934418 0.29135019 0.21698667 0.18376933 0.17166816][0.15348974 0.22738384 0.31777981 0.43544242 0.58857787 0.72404629 0.79936504 0.8022995 0.7494933 0.64229625 0.49502635 0.34832475 0.24154073 0.18251157 0.14981809][0.15887144 0.23597443 0.33300582 0.45970157 0.62337697 0.76669574 0.84443676 0.84537077 0.78600067 0.66972536 0.50999868 0.35016197 0.23029621 0.15822487 0.11461147][0.15598166 0.22502391 0.31173089 0.42221752 0.56089991 0.67801678 0.73652989 0.72957164 0.67070037 0.56472611 0.42115945 0.27778712 0.16860338 0.10169169 0.062783286][0.15389505 0.20785098 0.2710754 0.34629086 0.4350791 0.50351089 0.529797 0.51236171 0.46039382 0.37861222 0.27116323 0.16469488 0.083545685 0.036758196 0.016458573][0.16675666 0.2044947 0.23992355 0.2750085 0.31053963 0.32958865 0.32602826 0.30213368 0.26241189 0.20943835 0.14252323 0.076663323 0.027312478 0.0054122088 0.0091153262][0.19741526 0.22319554 0.2356969 0.2401437 0.2389449 0.22717613 0.20917188 0.18811095 0.16469665 0.13816731 0.10385036 0.068366021 0.042391293 0.039684109 0.062222853][0.2301158 0.24878895 0.2487113 0.23970051 0.2269419 0.21012653 0.19696973 0.18866435 0.18358561 0.17810965 0.16403022 0.14421986 0.12844494 0.13336244 0.16230044][0.23488416 0.24942513 0.24614723 0.2384015 0.23356456 0.23056987 0.23484665 0.24421158 0.25646216 0.26696715 0.26434046 0.25046688 0.23567723 0.23814367 0.26094112]]...]
INFO - root - 2017-12-10 14:02:27.100810: step 21110, loss = 0.70, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 65h:31m:10s remains)
INFO - root - 2017-12-10 14:02:34.923412: step 21120, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 67h:31m:15s remains)
INFO - root - 2017-12-10 14:02:42.837611: step 21130, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 68h:08m:56s remains)
INFO - root - 2017-12-10 14:02:50.793397: step 21140, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.824 sec/batch; 71h:17m:38s remains)
INFO - root - 2017-12-10 14:02:58.630094: step 21150, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 69h:28m:21s remains)
INFO - root - 2017-12-10 14:03:06.289674: step 21160, loss = 0.69, batch loss = 0.64 (12.8 examples/sec; 0.624 sec/batch; 53h:59m:07s remains)
INFO - root - 2017-12-10 14:03:14.010856: step 21170, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.758 sec/batch; 65h:33m:09s remains)
INFO - root - 2017-12-10 14:03:21.824210: step 21180, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 69h:59m:32s remains)
INFO - root - 2017-12-10 14:03:29.605566: step 21190, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 66h:57m:59s remains)
INFO - root - 2017-12-10 14:03:37.538967: step 21200, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 69h:51m:10s remains)
2017-12-10 14:03:38.367965: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.37531555 0.42871988 0.46398494 0.49054977 0.49650565 0.48755425 0.48550522 0.49319112 0.49510339 0.49046075 0.49748316 0.51755524 0.55275607 0.6051622 0.65446645][0.47099876 0.52176112 0.55115724 0.57482022 0.58059984 0.57193464 0.56694287 0.56773573 0.55907595 0.54220068 0.5367716 0.55019045 0.58851534 0.64872962 0.70258504][0.51451552 0.55416638 0.575271 0.59851974 0.61154854 0.611497 0.60644352 0.59693205 0.57293719 0.54205918 0.52531379 0.5332306 0.57166994 0.6335761 0.68426073][0.51706266 0.54673481 0.56408793 0.59222567 0.61816508 0.62998825 0.62371075 0.60120708 0.56364483 0.52727407 0.50982386 0.51811308 0.552229 0.60402983 0.63870639][0.49466568 0.52178431 0.54710478 0.58776492 0.62848651 0.64835548 0.63541192 0.59803861 0.551551 0.51903313 0.51009959 0.5216735 0.544773 0.573228 0.57961679][0.460948 0.49715617 0.54246366 0.60243505 0.65725833 0.67991775 0.65763038 0.60818535 0.55839634 0.53396362 0.53439641 0.54522961 0.55006093 0.54762453 0.52162331][0.42546234 0.47943074 0.54943687 0.627945 0.69073641 0.710023 0.67809659 0.62199527 0.57402176 0.55696583 0.56102663 0.56409717 0.54737073 0.51560545 0.46312311][0.37594628 0.44861367 0.53693968 0.62403578 0.68488669 0.69534636 0.65425718 0.5947566 0.54957485 0.53549844 0.53683412 0.52974182 0.49754623 0.4490765 0.38552213][0.2990652 0.37889236 0.46980026 0.55061984 0.599349 0.597532 0.54919326 0.48915035 0.44693872 0.43249965 0.42866832 0.41521546 0.37963447 0.33135197 0.27403259][0.18415584 0.25371021 0.3309817 0.39453807 0.42738819 0.41643271 0.36759788 0.311653 0.27236873 0.25480786 0.24560054 0.23136793 0.20389082 0.16953349 0.130667][0.046674863 0.094376087 0.14787644 0.19011876 0.20923601 0.19598222 0.15518369 0.10942827 0.075369366 0.055801261 0.043773036 0.033379331 0.02011992 0.0056359256 -0.01063691][-0.070598356 -0.047726359 -0.019508937 0.0022331849 0.010641962 -0.00052922155 -0.028513102 -0.059860308 -0.085040547 -0.10215469 -0.11279991 -0.11749528 -0.11841714 -0.11731888 -0.11624604][-0.1466189 -0.14394911 -0.13626005 -0.1308239 -0.1301769 -0.13805714 -0.15313345 -0.16939589 -0.18307151 -0.1932085 -0.19902629 -0.1989954 -0.19405964 -0.18611772 -0.1766614][-0.17228873 -0.18167827 -0.1860528 -0.19017483 -0.1941651 -0.19971736 -0.20599315 -0.21110405 -0.21452264 -0.21634622 -0.21586964 -0.21209714 -0.20568596 -0.1974373 -0.18766569][-0.15666218 -0.1686469 -0.17585541 -0.18198469 -0.18642445 -0.18969801 -0.19144969 -0.19149861 -0.19020508 -0.18788265 -0.18437046 -0.17960048 -0.17415608 -0.16816445 -0.16149946]]...]
INFO - root - 2017-12-10 14:03:46.294782: step 21210, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.820 sec/batch; 70h:53m:50s remains)
INFO - root - 2017-12-10 14:03:54.092165: step 21220, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 68h:06m:35s remains)
INFO - root - 2017-12-10 14:04:01.904550: step 21230, loss = 0.69, batch loss = 0.63 (10.7 examples/sec; 0.746 sec/batch; 64h:32m:27s remains)
INFO - root - 2017-12-10 14:04:09.544019: step 21240, loss = 0.71, batch loss = 0.65 (11.5 examples/sec; 0.695 sec/batch; 60h:05m:44s remains)
INFO - root - 2017-12-10 14:04:17.419783: step 21250, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.823 sec/batch; 71h:11m:35s remains)
INFO - root - 2017-12-10 14:04:25.042006: step 21260, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 67h:49m:21s remains)
INFO - root - 2017-12-10 14:04:32.745538: step 21270, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 67h:30m:09s remains)
INFO - root - 2017-12-10 14:04:40.675680: step 21280, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 69h:11m:45s remains)
INFO - root - 2017-12-10 14:04:48.425928: step 21290, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 67h:00m:47s remains)
INFO - root - 2017-12-10 14:04:56.267210: step 21300, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 65h:59m:58s remains)
2017-12-10 14:04:57.060779: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.074962273 0.10362249 0.13029091 0.14289866 0.13849287 0.12388452 0.10034319 0.075987205 0.054880042 0.039703719 0.028768789 0.020253804 0.021194287 0.038270392 0.061837945][0.07965187 0.10779976 0.13226432 0.1407388 0.131263 0.11447734 0.092254184 0.071761489 0.05557109 0.044762012 0.037241317 0.029283933 0.027951458 0.043837968 0.0686165][0.085811071 0.10828942 0.12669972 0.13054453 0.11859097 0.10385204 0.087217644 0.073672846 0.065180682 0.06084571 0.059349224 0.053660907 0.050422166 0.064773485 0.090155259][0.088088714 0.10006136 0.10954628 0.1092789 0.098247439 0.089601859 0.082141206 0.077936478 0.078579485 0.082104772 0.088564284 0.087230042 0.08390981 0.098075695 0.12427495][0.084247611 0.08444234 0.085318826 0.08330328 0.07684204 0.07686761 0.079965793 0.085347123 0.093822166 0.10333378 0.11571362 0.11805106 0.11503673 0.12853371 0.15379587][0.081791952 0.072269015 0.06779477 0.067816466 0.069058426 0.078501314 0.091503762 0.10452818 0.11685444 0.12726253 0.14002213 0.14303127 0.13939095 0.1504928 0.17121996][0.08156535 0.0655598 0.059509668 0.0649473 0.075922683 0.094614282 0.11624671 0.13544652 0.14929318 0.15730704 0.16597089 0.16736872 0.16295953 0.16985667 0.18150385][0.086487561 0.06809397 0.063638061 0.075515382 0.095078006 0.12044776 0.14777228 0.17165957 0.1867231 0.19171904 0.19373998 0.19145545 0.18575731 0.1862752 0.18405254][0.093814246 0.076858126 0.075512022 0.092363976 0.11712016 0.14569618 0.1764009 0.2043781 0.22181636 0.2254159 0.22114268 0.21437214 0.20643747 0.19905271 0.18139724][0.10205153 0.08964061 0.092092447 0.11147524 0.13690338 0.16467854 0.1966625 0.22854097 0.25016412 0.25503436 0.24700917 0.23622124 0.2245937 0.20837232 0.17663273][0.11388998 0.10721795 0.11250126 0.13149318 0.15398043 0.17817536 0.2095197 0.24332866 0.26816681 0.27476943 0.26530233 0.25210604 0.23682499 0.21329381 0.17201844][0.12174604 0.11956792 0.12541372 0.14133829 0.15899919 0.17880738 0.20778234 0.24005738 0.26497695 0.27227774 0.26368585 0.25126261 0.23505352 0.20816988 0.16348021][0.12272805 0.12386996 0.1286023 0.13933244 0.15044951 0.1645315 0.18823062 0.21429673 0.2343871 0.23958413 0.23289785 0.22434574 0.21105036 0.18572275 0.14390503][0.12103763 0.12371148 0.12548441 0.12953529 0.13320997 0.14081021 0.15722667 0.17436354 0.18677735 0.18821365 0.18355484 0.18022746 0.17238085 0.1526725 0.11912262][0.11978764 0.12182306 0.11974785 0.11777729 0.11594231 0.11900143 0.12887028 0.13779396 0.14374429 0.14259949 0.13969505 0.13996208 0.13646866 0.1230844 0.099548757]]...]
INFO - root - 2017-12-10 14:05:04.822874: step 21310, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 66h:24m:35s remains)
INFO - root - 2017-12-10 14:05:12.442277: step 21320, loss = 0.70, batch loss = 0.65 (10.8 examples/sec; 0.741 sec/batch; 64h:02m:55s remains)
INFO - root - 2017-12-10 14:05:20.301033: step 21330, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 68h:46m:54s remains)
INFO - root - 2017-12-10 14:05:28.123786: step 21340, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 67h:44m:48s remains)
INFO - root - 2017-12-10 14:05:35.816950: step 21350, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 68h:03m:45s remains)
INFO - root - 2017-12-10 14:05:43.694895: step 21360, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 67h:31m:02s remains)
INFO - root - 2017-12-10 14:05:51.506089: step 21370, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.807 sec/batch; 69h:47m:09s remains)
INFO - root - 2017-12-10 14:05:59.398329: step 21380, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 68h:31m:24s remains)
INFO - root - 2017-12-10 14:06:07.210849: step 21390, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 68h:14m:36s remains)
INFO - root - 2017-12-10 14:06:14.953571: step 21400, loss = 0.69, batch loss = 0.63 (12.1 examples/sec; 0.661 sec/batch; 57h:08m:05s remains)
2017-12-10 14:06:15.708925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030166551 -0.0091726771 -0.012425232 -0.0084725628 0.00066907384 0.011247152 0.014990073 0.0080762953 -0.0036856162 -0.012529597 -0.014404749 -0.01150512 -0.0068799844 -0.0059726317 -0.014036478][0.047486141 0.042025685 0.038461842 0.0464237 0.063764624 0.084358357 0.093937106 0.084635459 0.064374194 0.045158125 0.035918016 0.035799541 0.040712141 0.041697994 0.029931877][0.11486293 0.11351707 0.1111315 0.12278192 0.14754917 0.17881653 0.19738786 0.18983683 0.16246867 0.12989786 0.10690415 0.097658373 0.099061124 0.099297069 0.0849488][0.18224965 0.18800999 0.18782261 0.20126483 0.23085038 0.27273384 0.30458164 0.305986 0.2757538 0.22763695 0.18409862 0.15804785 0.15081814 0.14933117 0.1344776][0.25474352 0.26693276 0.26652318 0.27704513 0.30714503 0.35835484 0.40595394 0.4210712 0.39147151 0.32939789 0.26506859 0.22044282 0.20221812 0.19762391 0.18228307][0.32446218 0.34053528 0.33628932 0.33838981 0.36258018 0.41655269 0.4744947 0.50025719 0.47311604 0.40451178 0.32940042 0.27344152 0.24611692 0.23691742 0.21932042][0.3672359 0.38672611 0.38002497 0.37298715 0.38648787 0.43369135 0.49072805 0.51883835 0.4950971 0.43070692 0.35848767 0.30101565 0.26793209 0.25340241 0.23265651][0.37534872 0.40032741 0.39506385 0.38005954 0.37888917 0.40965998 0.45401791 0.47646621 0.45756316 0.40813923 0.35138729 0.3016119 0.26718086 0.24826233 0.22519916][0.35911176 0.39106947 0.38989767 0.36813104 0.34953785 0.3569451 0.38039207 0.39238095 0.37964916 0.35157368 0.31814572 0.28343433 0.25484312 0.23707487 0.21538033][0.32547596 0.36128432 0.3619104 0.33324337 0.29792348 0.28197187 0.2839765 0.28567341 0.27949739 0.27216089 0.26187518 0.24487254 0.22833909 0.21851277 0.20267512][0.25345355 0.28609121 0.2865974 0.25626725 0.21437272 0.18599099 0.17498322 0.17053132 0.16905543 0.17437617 0.178378 0.17395355 0.16816667 0.16658561 0.15743141][0.14327383 0.16605142 0.16637862 0.14255342 0.1084099 0.083354 0.0715793 0.06641034 0.066396184 0.073999576 0.08053606 0.079979628 0.079112306 0.081838004 0.077731878][0.029232275 0.038437635 0.036682516 0.021649389 0.0014708715 -0.012244402 -0.018295892 -0.021497043 -0.022179244 -0.017830389 -0.014391576 -0.01506535 -0.013983588 -0.009243113 -0.0091937948][-0.062184256 -0.065427646 -0.069824204 -0.077560104 -0.084968127 -0.087276414 -0.086625308 -0.087166131 -0.088734016 -0.088369526 -0.0884763 -0.089970872 -0.088439941 -0.083305411 -0.080170132][-0.1167537 -0.12626472 -0.13074957 -0.13305686 -0.13204663 -0.12702575 -0.12183452 -0.12011862 -0.12175281 -0.12406695 -0.12714449 -0.13053867 -0.13095267 -0.12772231 -0.12351261]]...]
INFO - root - 2017-12-10 14:06:23.537337: step 21410, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 67h:45m:35s remains)
INFO - root - 2017-12-10 14:06:31.316134: step 21420, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 65h:55m:29s remains)
INFO - root - 2017-12-10 14:06:39.031222: step 21430, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 69h:04m:25s remains)
INFO - root - 2017-12-10 14:06:46.730749: step 21440, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.799 sec/batch; 69h:02m:15s remains)
INFO - root - 2017-12-10 14:06:54.580599: step 21450, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.814 sec/batch; 70h:20m:45s remains)
INFO - root - 2017-12-10 14:07:02.480180: step 21460, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 65h:24m:28s remains)
INFO - root - 2017-12-10 14:07:10.323399: step 21470, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 67h:04m:46s remains)
INFO - root - 2017-12-10 14:07:18.018839: step 21480, loss = 0.70, batch loss = 0.64 (12.2 examples/sec; 0.654 sec/batch; 56h:27m:40s remains)
INFO - root - 2017-12-10 14:07:25.773835: step 21490, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 70h:25m:11s remains)
INFO - root - 2017-12-10 14:07:33.593280: step 21500, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 65h:46m:44s remains)
2017-12-10 14:07:34.387091: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.3031747 0.29501125 0.28954571 0.30410486 0.32901943 0.3518773 0.36477241 0.36376503 0.33725911 0.27959278 0.20085841 0.11834448 0.046696026 -0.0043852311 -0.035084352][0.3456718 0.34005439 0.33296153 0.34386417 0.36201257 0.37358877 0.37477192 0.36771768 0.33958924 0.27804297 0.1946561 0.10982236 0.039019633 -0.010750641 -0.0406962][0.35643366 0.35780817 0.3547242 0.365377 0.37702456 0.37630296 0.3652505 0.3516649 0.32252038 0.2613526 0.17917997 0.098076165 0.032220483 -0.01488942 -0.043331131][0.34359539 0.35479233 0.36083692 0.37616077 0.38556758 0.37709695 0.35742882 0.3376675 0.30598879 0.24557365 0.16729432 0.092380434 0.031484421 -0.013885705 -0.041486818][0.30309963 0.32500345 0.34238055 0.36677369 0.38052079 0.37300479 0.35279602 0.33095923 0.29710528 0.23693097 0.16259483 0.093636163 0.036601465 -0.0084109269 -0.036694225][0.24140629 0.27202621 0.30008271 0.33522376 0.35805836 0.35910806 0.34659222 0.32800475 0.29346412 0.23286022 0.16113816 0.095762365 0.040760964 -0.004582169 -0.034028094][0.16660815 0.20013683 0.23648237 0.28207368 0.31693617 0.33324623 0.33593446 0.325482 0.29205391 0.23075609 0.15909699 0.093617737 0.038156085 -0.007125672 -0.036472335][0.086476289 0.1151519 0.15361159 0.20479561 0.25050846 0.28667465 0.31113666 0.31500456 0.28702861 0.22720043 0.15525617 0.08736486 0.029314488 -0.015759438 -0.043076217][0.018834837 0.037676856 0.070904776 0.1198635 0.17136766 0.22668998 0.27523285 0.29828164 0.2813268 0.22739929 0.15760969 0.086606324 0.023966966 -0.0223507 -0.047956046][-0.011009636 -0.0030291674 0.01995397 0.059687927 0.10975223 0.17722364 0.24444428 0.28429309 0.27898139 0.23394266 0.16909583 0.09621454 0.028242994 -0.021656483 -0.047440689][0.0068403096 0.0063011823 0.017412888 0.04340183 0.085131593 0.15481715 0.23041317 0.27974832 0.28262487 0.24625626 0.18755093 0.11390394 0.040104795 -0.015629658 -0.04372704][0.0578926 0.052629203 0.052942805 0.064326592 0.094406314 0.15824394 0.23244286 0.28369829 0.290871 0.26156211 0.208161 0.13386191 0.054361511 -0.0077882828 -0.039450947][0.12363073 0.118135 0.11134016 0.11162737 0.13115308 0.18493626 0.25081119 0.29719013 0.30293679 0.27548009 0.22330196 0.14704309 0.062586248 -0.0045608561 -0.03893419][0.19232443 0.18958677 0.18031044 0.17575721 0.18876681 0.23159592 0.28484309 0.32032597 0.31816664 0.28601831 0.23008657 0.15043622 0.062437996 -0.0070655635 -0.04207027][0.25141516 0.25195491 0.24287707 0.23739335 0.24620883 0.27838939 0.31818661 0.34146544 0.32990867 0.29107365 0.22994378 0.14741535 0.0581749 -0.011212338 -0.045488648]]...]
INFO - root - 2017-12-10 14:07:42.204689: step 21510, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 66h:05m:41s remains)
INFO - root - 2017-12-10 14:07:50.019063: step 21520, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 68h:10m:14s remains)
INFO - root - 2017-12-10 14:07:57.739949: step 21530, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.824 sec/batch; 71h:10m:28s remains)
INFO - root - 2017-12-10 14:08:05.608360: step 21540, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 67h:15m:30s remains)
INFO - root - 2017-12-10 14:08:13.442845: step 21550, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 67h:46m:55s remains)
INFO - root - 2017-12-10 14:08:21.062715: step 21560, loss = 0.69, batch loss = 0.63 (12.4 examples/sec; 0.646 sec/batch; 55h:48m:42s remains)
INFO - root - 2017-12-10 14:08:28.904609: step 21570, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 67h:03m:44s remains)
INFO - root - 2017-12-10 14:08:36.765644: step 21580, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 68h:11m:12s remains)
INFO - root - 2017-12-10 14:08:44.582001: step 21590, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 67h:31m:03s remains)
INFO - root - 2017-12-10 14:08:52.445860: step 21600, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 68h:09m:42s remains)
2017-12-10 14:08:53.309431: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.052966204 0.044009183 0.040971432 0.049130961 0.071587078 0.10745221 0.14153168 0.15670899 0.14375488 0.10737669 0.061548244 0.015306294 -0.019882003 -0.039532375 -0.047054552][0.089901939 0.083535954 0.082063936 0.091660604 0.11736259 0.15700737 0.19170019 0.20329526 0.1834228 0.13835214 0.083701283 0.028006198 -0.014545676 -0.038343981 -0.047426119][0.16146939 0.15967007 0.15867893 0.16697185 0.19113667 0.2278675 0.25498918 0.25570059 0.22311106 0.16658296 0.10205466 0.0377923 -0.010453682 -0.036826372 -0.046327438][0.24157262 0.24409507 0.24180838 0.24631496 0.26572716 0.2954939 0.31138769 0.29828683 0.25165355 0.18396671 0.11091948 0.0402257 -0.011483113 -0.038756769 -0.047573198][0.30059963 0.30434754 0.29886964 0.30012673 0.31728157 0.3435601 0.35228252 0.32870528 0.27111641 0.19493951 0.11530605 0.039681364 -0.014745568 -0.042152055 -0.049763642][0.31930879 0.31876752 0.30863512 0.30915156 0.33053386 0.36253855 0.37490681 0.35017371 0.28841656 0.2078032 0.12302906 0.042641886 -0.015163812 -0.043738261 -0.051258195][0.31597304 0.30673525 0.28978923 0.29006356 0.3190622 0.36313242 0.3872444 0.37008932 0.31160244 0.23096296 0.14224564 0.0563087 -0.0066380315 -0.038920861 -0.04907136][0.3107408 0.29212287 0.26817775 0.26665235 0.30047396 0.35440266 0.39034566 0.38349035 0.33285889 0.2561737 0.16638348 0.07648132 0.0083584143 -0.029150346 -0.044037629][0.29242238 0.2682128 0.23959245 0.23493537 0.26795 0.32415566 0.36672181 0.36937857 0.32947636 0.26133907 0.17642155 0.088605717 0.019556627 -0.020964967 -0.039546732][0.24456732 0.22067401 0.19215904 0.18503007 0.21263961 0.26367864 0.30644235 0.31600338 0.28773242 0.23200066 0.15860017 0.081064977 0.018640121 -0.019338738 -0.037907984][0.16578616 0.14602827 0.12171505 0.11345498 0.13337891 0.17497747 0.2138997 0.22843774 0.21233717 0.1719867 0.11567073 0.055164974 0.0059777033 -0.024221098 -0.039150145][0.087874889 0.073269807 0.053947322 0.044361018 0.05503729 0.084577039 0.11628597 0.13253105 0.1262103 0.100551 0.062493037 0.02102921 -0.012554272 -0.032693762 -0.042107448][0.037293136 0.027293753 0.012761343 0.002255067 0.0041089552 0.02076616 0.042184751 0.054840684 0.052114908 0.035760127 0.011822075 -0.013593707 -0.033225589 -0.043579195 -0.046935897][0.019085977 0.013518795 0.0040247496 -0.0052877245 -0.0085907457 -0.0014660826 0.0099663092 0.015343625 0.0096038561 -0.0049242349 -0.022148561 -0.038564779 -0.049507335 -0.053047422 -0.0515752][0.02564149 0.023708578 0.019235246 0.013255218 0.0098805 0.014081887 0.020403767 0.019438112 0.0077496287 -0.010612801 -0.028949568 -0.044543494 -0.053944897 -0.055981793 -0.053076204]]...]
INFO - root - 2017-12-10 14:09:00.973807: step 21610, loss = 0.70, batch loss = 0.64 (12.6 examples/sec; 0.637 sec/batch; 55h:00m:06s remains)
INFO - root - 2017-12-10 14:09:08.757829: step 21620, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.812 sec/batch; 70h:05m:42s remains)
INFO - root - 2017-12-10 14:09:16.558189: step 21630, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 67h:25m:38s remains)
INFO - root - 2017-12-10 14:09:24.173429: step 21640, loss = 0.70, batch loss = 0.65 (10.7 examples/sec; 0.746 sec/batch; 64h:22m:43s remains)
INFO - root - 2017-12-10 14:09:31.916810: step 21650, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 67h:42m:01s remains)
INFO - root - 2017-12-10 14:09:39.812695: step 21660, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 68h:37m:33s remains)
INFO - root - 2017-12-10 14:09:47.602571: step 21670, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.749 sec/batch; 64h:40m:55s remains)
INFO - root - 2017-12-10 14:09:55.419955: step 21680, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 69h:12m:47s remains)
INFO - root - 2017-12-10 14:10:03.269288: step 21690, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 69h:24m:02s remains)
INFO - root - 2017-12-10 14:10:11.066939: step 21700, loss = 0.70, batch loss = 0.64 (12.7 examples/sec; 0.632 sec/batch; 54h:35m:03s remains)
2017-12-10 14:10:11.916758: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21107531 0.22053951 0.22030362 0.21516266 0.21819156 0.23195951 0.246535 0.25063932 0.23835835 0.21094909 0.16487315 0.10475393 0.044402875 -0.0040347558 -0.034579117][0.26954708 0.2911728 0.29777434 0.29435238 0.29781783 0.31421041 0.33149824 0.33590481 0.31987008 0.28398067 0.22402094 0.14564563 0.067323662 0.0049062958 -0.033832941][0.30768761 0.34286359 0.3566072 0.35488185 0.35831812 0.37767386 0.39904982 0.40626588 0.38901502 0.34701404 0.27507424 0.18058977 0.086543776 0.012132768 -0.032953545][0.32064533 0.36679164 0.3862623 0.38704339 0.3922129 0.41683307 0.4454971 0.45924863 0.44373885 0.39685667 0.31394145 0.20519839 0.097502783 0.013487007 -0.035675645][0.32630685 0.37645912 0.39624172 0.396461 0.40178606 0.43099952 0.46794686 0.49042785 0.47928694 0.43009761 0.33912924 0.21967617 0.10136196 0.010255753 -0.041315049][0.3369104 0.38073096 0.3921468 0.38588613 0.38685033 0.41625389 0.45816666 0.48856953 0.4839634 0.43662351 0.34382319 0.2208387 0.098515846 0.0046508638 -0.047566194][0.35610348 0.3854219 0.38232154 0.36584896 0.36082691 0.38815814 0.43225837 0.46827027 0.4702363 0.42722592 0.33660275 0.21513295 0.093693495 0.00049928285 -0.051177371][0.37704998 0.39190742 0.37542233 0.3509725 0.34290808 0.36980227 0.41613427 0.45611113 0.46302381 0.42353049 0.33431071 0.21319911 0.09175472 -0.001037735 -0.052842531][0.38972509 0.39426285 0.37045428 0.34496674 0.34002987 0.36840704 0.41547552 0.45594087 0.4636243 0.42334455 0.33226094 0.20997903 0.088449538 -0.003527588 -0.055252314][0.39425376 0.39601657 0.37232953 0.35251284 0.35553876 0.38589382 0.4305881 0.46674541 0.470149 0.4250198 0.32959732 0.20585023 0.085235544 -0.0050319214 -0.056530338][0.38037533 0.38615981 0.36893192 0.36056578 0.37627742 0.41192856 0.4555704 0.48735681 0.48531404 0.43343765 0.33191592 0.20551072 0.0853085 -0.0034739077 -0.0550612][0.34405625 0.3571789 0.35007095 0.35511494 0.38417983 0.42745173 0.47287196 0.5022279 0.49529424 0.43745553 0.33091432 0.20235364 0.082969323 -0.0035572054 -0.054042451][0.28327248 0.30155659 0.30305618 0.31749228 0.3534897 0.39984494 0.44508228 0.47241628 0.46296304 0.40448794 0.3007628 0.1779888 0.066193894 -0.013022599 -0.058341838][0.19388679 0.21072511 0.21537457 0.23168802 0.26492995 0.30572066 0.3444365 0.36711386 0.35734212 0.30584621 0.2174159 0.11493838 0.024219334 -0.03731098 -0.070092119][0.091027245 0.10048682 0.10202292 0.11219116 0.1341048 0.16238032 0.19007765 0.20694233 0.20018747 0.16315547 0.10020441 0.028836435 -0.031432372 -0.068659946 -0.08472985]]...]
INFO - root - 2017-12-10 14:10:19.711799: step 21710, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.765 sec/batch; 66h:02m:24s remains)
INFO - root - 2017-12-10 14:10:27.380668: step 21720, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 68h:15m:52s remains)
INFO - root - 2017-12-10 14:10:35.351783: step 21730, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 65h:59m:25s remains)
INFO - root - 2017-12-10 14:10:43.361508: step 21740, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 67h:26m:38s remains)
INFO - root - 2017-12-10 14:10:51.300015: step 21750, loss = 0.70, batch loss = 0.64 (9.0 examples/sec; 0.887 sec/batch; 76h:36m:08s remains)
INFO - root - 2017-12-10 14:10:59.081037: step 21760, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 68h:55m:50s remains)
INFO - root - 2017-12-10 14:11:06.910394: step 21770, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 68h:29m:20s remains)
INFO - root - 2017-12-10 14:11:14.792956: step 21780, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 66h:18m:53s remains)
INFO - root - 2017-12-10 14:11:22.482040: step 21790, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 69h:09m:45s remains)
INFO - root - 2017-12-10 14:11:30.121990: step 21800, loss = 0.69, batch loss = 0.63 (13.1 examples/sec; 0.612 sec/batch; 52h:51m:08s remains)
2017-12-10 14:11:31.017322: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.68301404 0.66711158 0.61277145 0.57980263 0.57882166 0.6175673 0.68122894 0.73134625 0.72403347 0.63326114 0.48050126 0.32505041 0.20701548 0.14022037 0.10749888][0.75901949 0.74655044 0.68806356 0.64996719 0.651419 0.69817388 0.76474774 0.80618364 0.78145778 0.66836989 0.48962528 0.3083986 0.17244063 0.097284138 0.061979923][0.75111723 0.73875469 0.68108684 0.64552927 0.65718204 0.71698761 0.78693163 0.81680006 0.77093107 0.63732809 0.44217962 0.25198245 0.11785431 0.052019365 0.028848009][0.66791278 0.65191996 0.60132474 0.57884866 0.61069703 0.68859953 0.76291645 0.77982086 0.7112729 0.559021 0.35648444 0.17257938 0.057454098 0.016328553 0.016604677][0.53471583 0.51444685 0.47573018 0.47311661 0.52931058 0.62523186 0.70397758 0.71058863 0.62451947 0.46187916 0.26496351 0.1021351 0.017432587 0.0072572939 0.0333904][0.38922489 0.36849371 0.34462026 0.36203822 0.43837705 0.54667872 0.62884194 0.63086748 0.53812832 0.3786968 0.20113255 0.068179682 0.014251351 0.02898211 0.073298231][0.27415657 0.26088336 0.25535613 0.29012245 0.3776775 0.48840335 0.5680669 0.56611228 0.4738065 0.32771367 0.17692816 0.073922992 0.042555086 0.069299512 0.11937023][0.2213733 0.22703464 0.24417081 0.29279885 0.38015616 0.47768611 0.54046977 0.52733272 0.43698034 0.3085281 0.18528044 0.10724999 0.0882706 0.1145447 0.15899226][0.23116927 0.26342183 0.30203235 0.35594016 0.42838773 0.4960835 0.5285641 0.49779248 0.41010609 0.30239242 0.20652054 0.14818031 0.132915 0.15012316 0.1820261][0.28197277 0.33982688 0.39078954 0.43700421 0.48076025 0.50812328 0.50460237 0.45504457 0.37141511 0.2846916 0.21317771 0.16973701 0.15417704 0.16068724 0.18040641][0.35178798 0.4272508 0.4790588 0.50603056 0.5121659 0.49478024 0.45370308 0.38494119 0.304079 0.23409846 0.18118301 0.14836836 0.13265559 0.1331533 0.14747901][0.42397338 0.50434124 0.54631168 0.54671049 0.51227748 0.45124146 0.374249 0.28638417 0.20505692 0.14541808 0.10516518 0.081294484 0.069726661 0.072504066 0.090482473][0.47432113 0.54692024 0.5704937 0.54222727 0.47230691 0.37830204 0.27773306 0.18016523 0.10137904 0.049445659 0.01784298 0.0014181596 -0.0038001558 0.0054999394 0.030819764][0.49017221 0.542128 0.5401094 0.48451778 0.38970816 0.28061086 0.17846556 0.090545744 0.025544969 -0.015656158 -0.040573604 -0.053372927 -0.056691334 -0.045759458 -0.018505143][0.47642592 0.50080937 0.47173935 0.39527789 0.28919113 0.18264522 0.097897865 0.035855509 -0.0050232243 -0.031523675 -0.050626293 -0.064080708 -0.073058039 -0.070602916 -0.052056629]]...]
INFO - root - 2017-12-10 14:11:38.947954: step 21810, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 69h:46m:25s remains)
INFO - root - 2017-12-10 14:11:46.941583: step 21820, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 69h:50m:28s remains)
INFO - root - 2017-12-10 14:11:54.854371: step 21830, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.819 sec/batch; 70h:40m:21s remains)
INFO - root - 2017-12-10 14:12:02.654941: step 21840, loss = 0.69, batch loss = 0.63 (10.9 examples/sec; 0.735 sec/batch; 63h:26m:13s remains)
INFO - root - 2017-12-10 14:12:10.457094: step 21850, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 67h:26m:00s remains)
INFO - root - 2017-12-10 14:12:18.467882: step 21860, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 69h:26m:13s remains)
INFO - root - 2017-12-10 14:12:26.243783: step 21870, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.810 sec/batch; 69h:52m:45s remains)
INFO - root - 2017-12-10 14:12:33.885786: step 21880, loss = 0.70, batch loss = 0.64 (12.3 examples/sec; 0.649 sec/batch; 55h:58m:57s remains)
INFO - root - 2017-12-10 14:12:41.741820: step 21890, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 68h:30m:14s remains)
INFO - root - 2017-12-10 14:12:49.710749: step 21900, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 69h:54m:13s remains)
2017-12-10 14:12:50.543218: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.035781968 0.002025364 0.07995493 0.17471647 0.251261 0.28061596 0.25129166 0.18489963 0.098762549 0.011997488 -0.054949868 -0.089470565 -0.09321624 -0.074571021 -0.040449139][0.025177179 0.067631409 0.16574644 0.29174504 0.39949617 0.44843644 0.42033216 0.33914718 0.22275682 0.099115118 -0.0015049592 -0.058627017 -0.073779784 -0.061304308 -0.03102939][0.10159757 0.14353168 0.25176269 0.39593235 0.52458489 0.59130818 0.57269973 0.48952058 0.35458043 0.2018522 0.071458317 -0.00647802 -0.030745653 -0.022670632 0.0031609498][0.16546357 0.20486543 0.31629205 0.468349 0.609511 0.69291312 0.69263417 0.62161595 0.48343179 0.31461403 0.1630138 0.068272732 0.036864962 0.041485813 0.062130969][0.2024496 0.24020664 0.35192579 0.50656396 0.65619379 0.75630045 0.77916664 0.72807908 0.59857744 0.42884427 0.27063343 0.1690387 0.13571866 0.13832188 0.15217513][0.20964882 0.24673377 0.35617274 0.50878155 0.66256231 0.77726626 0.82448041 0.7974413 0.68689358 0.53125376 0.38104379 0.28213468 0.25041959 0.25228053 0.26000232][0.18204261 0.2169936 0.32016498 0.46580148 0.61964971 0.7471171 0.81897283 0.81843203 0.73418564 0.60292065 0.46983188 0.37675965 0.34419003 0.34375843 0.34842381][0.12050599 0.14960344 0.24298877 0.37882069 0.53196579 0.67325807 0.77159882 0.79938042 0.74377805 0.63787872 0.52049845 0.42795968 0.38615021 0.37855133 0.38300294][0.044750664 0.06467694 0.14426278 0.26710469 0.41919431 0.575269 0.70067704 0.75666714 0.7272824 0.64074284 0.53092134 0.4319061 0.37563306 0.35938722 0.36719334][-0.022465883 -0.010853829 0.053184725 0.15926319 0.30436337 0.46727017 0.61091614 0.68846512 0.6800611 0.60756 0.50044906 0.39363813 0.32553035 0.30519173 0.3212786][-0.067306034 -0.059502512 -0.0097916033 0.075767644 0.20293611 0.35651875 0.50175977 0.5898903 0.59830773 0.54062819 0.43952292 0.32963589 0.25418025 0.23187363 0.25551346][-0.081208438 -0.073035657 -0.034297328 0.029805575 0.12957957 0.25815839 0.38835952 0.47648537 0.49848756 0.45932621 0.37207437 0.26815668 0.19172125 0.16702704 0.19108327][-0.064365283 -0.052631821 -0.020355867 0.02529363 0.095780626 0.19366086 0.30201554 0.38542116 0.41881081 0.39954495 0.33137235 0.23986623 0.16571842 0.13497514 0.14952068][-0.020145394 -0.0041720164 0.025046177 0.057597935 0.10366715 0.17401728 0.26105371 0.33743417 0.37739429 0.37346396 0.32317284 0.24520682 0.17414488 0.13530366 0.13521123][0.038227078 0.057367612 0.084501438 0.10877929 0.13796592 0.18743543 0.25533029 0.32111737 0.36016041 0.36283633 0.32424682 0.25635138 0.1871091 0.14008875 0.12549955]]...]
INFO - root - 2017-12-10 14:12:58.577750: step 21910, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.793 sec/batch; 68h:24m:19s remains)
INFO - root - 2017-12-10 14:13:06.499269: step 21920, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 69h:09m:54s remains)
INFO - root - 2017-12-10 14:13:14.317506: step 21930, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 66h:48m:43s remains)
INFO - root - 2017-12-10 14:13:22.137947: step 21940, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 67h:16m:10s remains)
INFO - root - 2017-12-10 14:13:30.215256: step 21950, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 69h:36m:01s remains)
INFO - root - 2017-12-10 14:13:37.916055: step 21960, loss = 0.70, batch loss = 0.64 (11.8 examples/sec; 0.677 sec/batch; 58h:25m:00s remains)
INFO - root - 2017-12-10 14:13:45.735379: step 21970, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.820 sec/batch; 70h:45m:54s remains)
INFO - root - 2017-12-10 14:13:53.674881: step 21980, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 66h:25m:45s remains)
INFO - root - 2017-12-10 14:14:01.561860: step 21990, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 68h:52m:22s remains)
INFO - root - 2017-12-10 14:14:09.448105: step 22000, loss = 0.67, batch loss = 0.61 (9.9 examples/sec; 0.805 sec/batch; 69h:24m:05s remains)
2017-12-10 14:14:10.333578: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.030919462 0.0026475298 0.046770342 0.091715209 0.12789042 0.14846598 0.14957868 0.13024098 0.094047315 0.05177306 0.015885871 -0.0020376665 0.0026752169 0.02708762 0.062028218][-0.016368615 0.027511595 0.086066358 0.14794794 0.20481575 0.24691492 0.26506174 0.25290462 0.21161397 0.15414727 0.095827304 0.057864886 0.053646341 0.082858674 0.13246259][-0.0025707246 0.049610268 0.12086178 0.1989819 0.27852449 0.34623143 0.38627505 0.38614124 0.34301102 0.27087936 0.18726231 0.12431313 0.10746781 0.13981313 0.20326442][0.0079817278 0.066442 0.14919166 0.2429104 0.34440932 0.4360908 0.4953694 0.50440347 0.45779541 0.37147072 0.26495126 0.17844759 0.14706112 0.17648205 0.24412687][0.013533486 0.0769868 0.1709988 0.28194264 0.40770248 0.52522206 0.60452187 0.62296432 0.57433975 0.47809729 0.35471332 0.24881412 0.20128912 0.21986982 0.27983788][0.014746629 0.081840023 0.18560857 0.31425336 0.4649061 0.60774463 0.70571035 0.73262459 0.68332618 0.58074611 0.447849 0.33225214 0.27509132 0.28184283 0.32667747][0.01475885 0.084061824 0.19483478 0.33843225 0.50881422 0.66973656 0.78021866 0.81370384 0.7667715 0.66382813 0.53085083 0.41643134 0.35641924 0.35022604 0.37275648][0.013742417 0.083654538 0.19786875 0.35052985 0.52960145 0.69391972 0.80386662 0.83834159 0.79677868 0.70127106 0.57899928 0.47667542 0.42058679 0.40189523 0.39897156][0.0092972107 0.076625951 0.18805131 0.3393476 0.51233476 0.66292828 0.75785196 0.78622127 0.75132179 0.66880572 0.56294572 0.47578022 0.42378193 0.39235872 0.36577767][-0.0009664612 0.059124492 0.1593906 0.29698923 0.45045674 0.5757243 0.64771855 0.667117 0.64119989 0.57725567 0.49285513 0.42317367 0.3773962 0.33936629 0.29963446][-0.019421373 0.02700692 0.10632716 0.21721117 0.3394084 0.43230674 0.4779186 0.48553342 0.46531761 0.41809779 0.35484162 0.30312625 0.26819083 0.23448649 0.19678025][-0.04341159 -0.0149679 0.037119769 0.11290871 0.19759163 0.25801945 0.28146 0.27989709 0.26328683 0.22918889 0.18401887 0.14818846 0.1252452 0.10244881 0.07698603][-0.066350833 -0.055015225 -0.028758232 0.012647559 0.061458271 0.094642662 0.1036199 0.098251224 0.085945927 0.063437834 0.034257181 0.012460729 0.00036559298 -0.011103562 -0.023345841][-0.083198383 -0.083757743 -0.075392276 -0.058830138 -0.036444779 -0.0218083 -0.020291382 -0.026157454 -0.035082255 -0.049257249 -0.066453949 -0.077842116 -0.082005285 -0.084456988 -0.085877672][-0.092430562 -0.099684484 -0.1006598 -0.097562581 -0.090348706 -0.085884586 -0.087324038 -0.092166722 -0.098155506 -0.10575744 -0.11371063 -0.11719292 -0.11549217 -0.11138843 -0.10543475]]...]
INFO - root - 2017-12-10 14:14:18.482204: step 22010, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.816 sec/batch; 70h:22m:58s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 14:14:26.273555: step 22020, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 67h:44m:23s remains)
INFO - root - 2017-12-10 14:14:34.249098: step 22030, loss = 0.69, batch loss = 0.64 (9.5 examples/sec; 0.839 sec/batch; 72h:21m:38s remains)
INFO - root - 2017-12-10 14:14:41.855109: step 22040, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 67h:33m:26s remains)
INFO - root - 2017-12-10 14:14:49.739554: step 22050, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 66h:51m:24s remains)
INFO - root - 2017-12-10 14:14:57.464508: step 22060, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 66h:17m:24s remains)
INFO - root - 2017-12-10 14:15:05.317284: step 22070, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 67h:20m:12s remains)
INFO - root - 2017-12-10 14:15:13.319679: step 22080, loss = 0.66, batch loss = 0.61 (9.7 examples/sec; 0.822 sec/batch; 70h:55m:01s remains)
INFO - root - 2017-12-10 14:15:21.225551: step 22090, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.765 sec/batch; 65h:59m:38s remains)
INFO - root - 2017-12-10 14:15:29.069137: step 22100, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.759 sec/batch; 65h:26m:33s remains)
2017-12-10 14:15:29.879569: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22620074 0.24196982 0.23170488 0.20191151 0.17440104 0.17271015 0.19676518 0.23272789 0.25990638 0.260784 0.21428059 0.12645212 0.032529779 -0.038488008 -0.0794776][0.36187893 0.366055 0.34008503 0.29828259 0.26865125 0.27746367 0.31737918 0.3655743 0.39512637 0.38685659 0.3180033 0.19870506 0.074039921 -0.019827263 -0.074382707][0.49999514 0.48538569 0.43957958 0.38681132 0.35888195 0.38204086 0.43908247 0.49666092 0.52185994 0.49632651 0.40051252 0.2510097 0.10111161 -0.0088459328 -0.071253754][0.60747105 0.57451493 0.51340944 0.45894003 0.44324213 0.48840353 0.56533462 0.63009864 0.64450383 0.59342861 0.46544316 0.28613245 0.11485559 -0.0069855046 -0.073923513][0.66880864 0.6189447 0.5484336 0.49980152 0.50366735 0.57588422 0.67582279 0.74990875 0.754231 0.67735821 0.51800758 0.31198347 0.12305345 -0.0081544043 -0.078169316][0.69927007 0.63799977 0.56360638 0.52246869 0.54326832 0.63588244 0.75246549 0.83348328 0.82903874 0.73039722 0.54647183 0.32148936 0.12146907 -0.014762727 -0.085234396][0.70466119 0.64429843 0.57523525 0.5440467 0.57633811 0.6761266 0.79417354 0.8710137 0.85390061 0.73692757 0.53811264 0.30620208 0.10564189 -0.028445894 -0.094877474][0.67581195 0.62547821 0.56951356 0.55233145 0.59438455 0.69322777 0.80152643 0.86514324 0.833164 0.70342851 0.50063711 0.27327606 0.081130631 -0.045258731 -0.10503291][0.61405814 0.57392412 0.5310353 0.52530819 0.5721519 0.66383463 0.7568568 0.80558914 0.76378477 0.63258719 0.43917671 0.22788616 0.051916093 -0.062491108 -0.11410015][0.51885122 0.48119095 0.44360447 0.44011343 0.48168397 0.55844986 0.63306093 0.66900247 0.62595224 0.50736445 0.33892837 0.15785293 0.0089548649 -0.085492119 -0.12441898][0.38300991 0.34361431 0.30773979 0.30277261 0.3351489 0.39492807 0.45178896 0.47791904 0.43977517 0.34252137 0.20866032 0.067184664 -0.046418276 -0.11450735 -0.13672529][0.19958623 0.16012408 0.12957524 0.1272437 0.15461661 0.20208567 0.2461766 0.26696727 0.23866685 0.16789171 0.073350146 -0.024124539 -0.09877073 -0.13857077 -0.14406809][0.0095118145 -0.027998827 -0.05097178 -0.048577137 -0.023225915 0.015864432 0.051562991 0.070537828 0.054977223 0.011947169 -0.043958168 -0.099180162 -0.13742919 -0.15220146 -0.14479463][-0.13392301 -0.16562757 -0.18128324 -0.17704158 -0.15644871 -0.12698568 -0.099458069 -0.082324095 -0.08636006 -0.10557719 -0.129723 -0.15092018 -0.16073477 -0.15678211 -0.14101619][-0.20422474 -0.22735231 -0.23628947 -0.2325432 -0.21910624 -0.20090204 -0.18338655 -0.17112999 -0.16913785 -0.17313893 -0.17673595 -0.17605315 -0.16820298 -0.15343288 -0.13413064]]...]
INFO - root - 2017-12-10 14:15:37.756023: step 22110, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 68h:53m:20s remains)
INFO - root - 2017-12-10 14:15:45.492885: step 22120, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 65h:56m:15s remains)
INFO - root - 2017-12-10 14:15:53.463866: step 22130, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 67h:33m:44s remains)
INFO - root - 2017-12-10 14:16:01.258244: step 22140, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 67h:04m:43s remains)
INFO - root - 2017-12-10 14:16:09.088307: step 22150, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.812 sec/batch; 70h:00m:03s remains)
INFO - root - 2017-12-10 14:16:17.102605: step 22160, loss = 0.73, batch loss = 0.67 (10.0 examples/sec; 0.797 sec/batch; 68h:42m:15s remains)
INFO - root - 2017-12-10 14:16:25.092823: step 22170, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 68h:01m:45s remains)
INFO - root - 2017-12-10 14:16:33.015764: step 22180, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 69h:13m:10s remains)
INFO - root - 2017-12-10 14:16:40.947819: step 22190, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 67h:41m:59s remains)
INFO - root - 2017-12-10 14:16:48.601471: step 22200, loss = 0.68, batch loss = 0.62 (14.0 examples/sec; 0.570 sec/batch; 49h:06m:31s remains)
2017-12-10 14:16:49.351666: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17658244 0.189601 0.19080104 0.1738549 0.13612451 0.092506215 0.059156708 0.042840067 0.044264164 0.071459204 0.13503046 0.21850996 0.29960239 0.36047566 0.38574263][0.2548686 0.27375397 0.27766311 0.26117739 0.21881895 0.16524906 0.11914996 0.091239065 0.085413009 0.1122387 0.17909534 0.26390675 0.34349796 0.40045753 0.4195022][0.31614766 0.34513128 0.35620937 0.34591568 0.307126 0.25062427 0.19472215 0.15501896 0.14074665 0.16314268 0.2217041 0.29235628 0.35642597 0.40004513 0.40992048][0.35916615 0.40182143 0.42114991 0.41870075 0.38931343 0.33896509 0.28329468 0.24242553 0.22918008 0.24967885 0.292601 0.33684212 0.37432459 0.39681223 0.394315][0.40846249 0.45957121 0.48126212 0.48180836 0.46138749 0.42318788 0.37928668 0.35136861 0.35019553 0.37197763 0.39486435 0.40492257 0.4073976 0.40151027 0.38293236][0.46791869 0.521501 0.53902781 0.53802913 0.5249694 0.50173479 0.4754872 0.46692124 0.48093507 0.502674 0.50342721 0.47795773 0.44347969 0.40705392 0.37162718][0.52483481 0.57421523 0.58161658 0.57393879 0.56301069 0.55002844 0.53659511 0.54080296 0.56174916 0.57729286 0.55858171 0.50760019 0.44762111 0.39131293 0.34788895][0.54743969 0.5854556 0.57903361 0.56046265 0.54525554 0.53473634 0.52604473 0.53448516 0.55472279 0.56231964 0.53266907 0.47150788 0.40188476 0.33895093 0.2965031][0.51206863 0.53371388 0.51332486 0.48413783 0.46227157 0.45078194 0.44526249 0.45697293 0.4763771 0.48045546 0.45040694 0.39154264 0.32286891 0.26049146 0.22176225][0.42529482 0.43009156 0.39963606 0.36302426 0.33440933 0.31937608 0.31460658 0.32684654 0.34373114 0.34649733 0.32238233 0.27423683 0.21531308 0.16131762 0.13061805][0.29865456 0.29321927 0.26081908 0.22384116 0.19381049 0.17815989 0.17505409 0.186749 0.20024732 0.20229527 0.18583806 0.15121585 0.10699388 0.067822434 0.049855456][0.16262965 0.15319872 0.12517992 0.093698137 0.0684787 0.057480905 0.059319288 0.072154254 0.083903857 0.086792819 0.077844858 0.055954516 0.026911611 0.0042812959 -0.00015312577][0.050999004 0.039847892 0.018068016 -0.00481812 -0.020512784 -0.022639208 -0.013577156 0.0019490634 0.014008378 0.019170765 0.016093945 0.0030765515 -0.015246891 -0.026373863 -0.02296772][-0.020924505 -0.033176903 -0.049092676 -0.063037075 -0.068806812 -0.062901668 -0.048588235 -0.031614985 -0.019210987 -0.012299149 -0.0117977 -0.019749433 -0.032255623 -0.038233493 -0.033262789][-0.060380634 -0.071920991 -0.081988312 -0.08841715 -0.087349124 -0.077765718 -0.063018709 -0.047910087 -0.036724214 -0.029281503 -0.026834603 -0.031778168 -0.040861346 -0.044760112 -0.040726237]]...]
INFO - root - 2017-12-10 14:16:57.296417: step 22210, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 68h:34m:57s remains)
INFO - root - 2017-12-10 14:17:05.284379: step 22220, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 69h:52m:23s remains)
INFO - root - 2017-12-10 14:17:13.172001: step 22230, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 69h:06m:35s remains)
INFO - root - 2017-12-10 14:17:20.816629: step 22240, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 66h:59m:45s remains)
INFO - root - 2017-12-10 14:17:28.720986: step 22250, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.766 sec/batch; 66h:00m:45s remains)
INFO - root - 2017-12-10 14:17:36.665105: step 22260, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 68h:39m:37s remains)
INFO - root - 2017-12-10 14:17:44.552002: step 22270, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 68h:53m:21s remains)
INFO - root - 2017-12-10 14:17:52.180210: step 22280, loss = 0.69, batch loss = 0.63 (13.0 examples/sec; 0.617 sec/batch; 53h:11m:29s remains)
INFO - root - 2017-12-10 14:18:00.010262: step 22290, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 67h:25m:41s remains)
INFO - root - 2017-12-10 14:18:07.951131: step 22300, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 69h:01m:06s remains)
2017-12-10 14:18:08.817583: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.066709444 0.099030383 0.12495142 0.13443168 0.13102332 0.13190196 0.1480214 0.1730781 0.19692925 0.212744 0.21720114 0.20437364 0.16220827 0.093838774 0.016609926][0.080874838 0.11951225 0.15606454 0.17787009 0.18894193 0.20856063 0.24481156 0.28624818 0.31817952 0.33547145 0.33926305 0.32539603 0.27513281 0.18555275 0.078553177][0.083737068 0.12650774 0.17335281 0.21099541 0.24236214 0.28342712 0.33570594 0.38455343 0.41346005 0.42224267 0.42113656 0.41048849 0.36291775 0.26374704 0.13653558][0.0781745 0.12280753 0.17915994 0.23395739 0.286821 0.34682092 0.40810847 0.45348123 0.46611354 0.45504424 0.4429242 0.43601179 0.39841598 0.3031038 0.17068288][0.070205472 0.11630908 0.18369487 0.258894 0.33583644 0.4147751 0.48201081 0.51667678 0.50191283 0.45978609 0.42868227 0.42073396 0.39347768 0.30836213 0.18119609][0.060063984 0.10842656 0.18866648 0.28671339 0.39003432 0.48940265 0.56311858 0.587276 0.5450027 0.47129065 0.42006996 0.4086988 0.3885456 0.3102155 0.18616581][0.049765825 0.10106999 0.19346425 0.31256357 0.44048873 0.561573 0.64640069 0.66818023 0.60757929 0.51100534 0.44508237 0.43031931 0.41202468 0.33212343 0.20296177][0.047673143 0.10292712 0.2036175 0.33490831 0.47677359 0.61195827 0.70694727 0.731466 0.66195852 0.55277735 0.47928721 0.46285397 0.44262031 0.35529378 0.21706678][0.059473436 0.11831395 0.22125205 0.35264859 0.49333933 0.63003993 0.73140359 0.76371354 0.69664937 0.5859651 0.510309 0.490615 0.46339804 0.36507571 0.21804029][0.077752352 0.1366096 0.23324828 0.3524853 0.47803298 0.60359043 0.70504 0.748386 0.69539052 0.59409952 0.52047622 0.49621934 0.4601796 0.35271245 0.20234412][0.089953311 0.14272992 0.22357307 0.31947517 0.41770846 0.51949453 0.61020935 0.66009051 0.625995 0.5427267 0.47738603 0.4514921 0.41120753 0.30487245 0.16388291][0.090566896 0.13383527 0.1933345 0.25872567 0.32178292 0.38995987 0.45867676 0.50546336 0.48707527 0.42409694 0.36986795 0.3442612 0.30454388 0.21080506 0.093514554][0.078042649 0.11069024 0.14866748 0.1833849 0.21108894 0.24277321 0.28189522 0.31403846 0.30154315 0.25377724 0.20897771 0.18455213 0.15076973 0.080043279 -0.0025987437][0.054009624 0.074983478 0.094000719 0.10439043 0.10561873 0.10879663 0.12135572 0.1361655 0.12435984 0.087810919 0.051323939 0.029734917 0.0054426426 -0.03923865 -0.087347142][0.027631257 0.037231591 0.0417806 0.037157886 0.024500504 0.012236234 0.0076226564 0.0086068707 -0.0036670764 -0.030926282 -0.058246281 -0.074239969 -0.088016532 -0.11053535 -0.13241053]]...]
INFO - root - 2017-12-10 14:18:16.704645: step 22310, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 70h:13m:03s remains)
INFO - root - 2017-12-10 14:18:24.497419: step 22320, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 66h:18m:19s remains)
INFO - root - 2017-12-10 14:18:32.233408: step 22330, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 69h:18m:18s remains)
INFO - root - 2017-12-10 14:18:40.244618: step 22340, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 68h:24m:07s remains)
INFO - root - 2017-12-10 14:18:48.166742: step 22350, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 68h:02m:39s remains)
INFO - root - 2017-12-10 14:18:55.982164: step 22360, loss = 0.69, batch loss = 0.64 (11.8 examples/sec; 0.675 sec/batch; 58h:10m:44s remains)
INFO - root - 2017-12-10 14:19:03.867904: step 22370, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 67h:59m:44s remains)
INFO - root - 2017-12-10 14:19:11.756537: step 22380, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 66h:52m:26s remains)
INFO - root - 2017-12-10 14:19:19.746609: step 22390, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 69h:44m:05s remains)
INFO - root - 2017-12-10 14:19:27.672702: step 22400, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.797 sec/batch; 68h:39m:28s remains)
2017-12-10 14:19:28.483613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.041391283 -0.038815286 -0.032146066 -0.021948235 -0.0098234005 -0.00010308505 0.0043474226 0.006281307 0.010363469 0.016470633 0.023202352 0.029781684 0.03398354 0.029057866 0.012994139][-0.029974718 -0.0222057 -0.008625092 0.010967187 0.033983137 0.053045224 0.062889218 0.067474961 0.074943483 0.083592691 0.091939062 0.10011671 0.1056242 0.09796349 0.073218048][-0.01501998 -0.0014618225 0.019500695 0.049432263 0.08489807 0.11541492 0.1334468 0.14314228 0.15622209 0.16828904 0.17779692 0.18572386 0.19012266 0.17716831 0.14060035][0.0083332146 0.030662717 0.061559878 0.10295434 0.15007557 0.19024998 0.2142982 0.2262516 0.24256693 0.25734097 0.26800758 0.274679 0.2759876 0.25634998 0.20663002][0.049936086 0.08423914 0.12638295 0.17767072 0.23175234 0.27523428 0.2983357 0.30597615 0.32035354 0.33639303 0.34926942 0.3549028 0.35277656 0.32597229 0.26282528][0.10841418 0.15552215 0.20577858 0.26018196 0.31209928 0.35021034 0.36654133 0.36766684 0.379963 0.39950138 0.41716841 0.42238244 0.41495523 0.37846151 0.30073872][0.17779842 0.23439309 0.28788549 0.33920854 0.38218221 0.40996149 0.41890991 0.4164134 0.42921826 0.45445862 0.47719264 0.47948375 0.46118614 0.4096542 0.31590435][0.25246575 0.31310764 0.36424473 0.4074156 0.43767488 0.4536871 0.45624202 0.45254281 0.46633303 0.49538484 0.51970172 0.51516044 0.48158926 0.41173258 0.30325806][0.33144879 0.39129478 0.43524951 0.46723598 0.48460123 0.4901219 0.48663372 0.48042503 0.49014035 0.51468909 0.53255051 0.517313 0.46778378 0.38094276 0.26259327][0.4028 0.45609105 0.48798752 0.50778568 0.51640564 0.51776165 0.51336318 0.507171 0.50939548 0.51878345 0.51784885 0.48530349 0.41943762 0.31878862 0.19651379][0.45766744 0.4997263 0.51646119 0.52418584 0.52722007 0.52834475 0.5263527 0.5220927 0.51628762 0.505299 0.47893789 0.42674085 0.34742931 0.23935524 0.12008189][0.4972477 0.52687222 0.52955508 0.52738625 0.52582479 0.52414459 0.51904809 0.510409 0.49319363 0.46095875 0.41046935 0.34312758 0.25908139 0.15466395 0.047597323][0.52394563 0.54584086 0.5389573 0.52768421 0.51600569 0.49969453 0.476209 0.44959378 0.4157021 0.36624441 0.301472 0.23117466 0.156221 0.069750018 -0.014477723][0.54590958 0.56851774 0.55883628 0.53919315 0.51066577 0.46776626 0.4125644 0.35720143 0.30317771 0.24195161 0.17366998 0.11119632 0.05481315 -0.0051880344 -0.061130282][0.56427556 0.59280592 0.58322644 0.55345017 0.50289083 0.4287872 0.33955714 0.25522846 0.18411079 0.11835632 0.055238627 0.00589827 -0.029986871 -0.063168138 -0.091921419]]...]
INFO - root - 2017-12-10 14:19:36.334953: step 22410, loss = 0.70, batch loss = 0.65 (12.5 examples/sec; 0.642 sec/batch; 55h:18m:00s remains)
INFO - root - 2017-12-10 14:19:44.192143: step 22420, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 66h:02m:20s remains)
INFO - root - 2017-12-10 14:19:52.140871: step 22430, loss = 0.67, batch loss = 0.61 (9.9 examples/sec; 0.806 sec/batch; 69h:25m:42s remains)
INFO - root - 2017-12-10 14:19:59.859928: step 22440, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 69h:04m:10s remains)
INFO - root - 2017-12-10 14:20:07.767178: step 22450, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 69h:18m:19s remains)
INFO - root - 2017-12-10 14:20:15.657458: step 22460, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 69h:08m:55s remains)
INFO - root - 2017-12-10 14:20:23.594719: step 22470, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.812 sec/batch; 69h:56m:39s remains)
INFO - root - 2017-12-10 14:20:31.475058: step 22480, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 67h:16m:39s remains)
INFO - root - 2017-12-10 14:20:39.423259: step 22490, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 68h:35m:22s remains)
INFO - root - 2017-12-10 14:20:47.122959: step 22500, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.799 sec/batch; 68h:47m:07s remains)
2017-12-10 14:20:47.930710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.025388891 -0.025965765 -0.025826627 -0.027067188 -0.030411767 -0.034563418 -0.035583466 -0.028214056 -0.0089201471 0.020462895 0.053280424 0.081931017 0.10216509 0.11588859 0.12840198][-0.042194743 -0.041592121 -0.03906811 -0.036628932 -0.035837207 -0.035219017 -0.031116229 -0.019069664 0.0037655223 0.034779768 0.067229316 0.09361615 0.11070203 0.12161752 0.13167408][-0.046834894 -0.044545151 -0.038693823 -0.031249352 -0.024562798 -0.017605295 -0.0078400541 0.0077355197 0.030424625 0.056788217 0.080560334 0.095661759 0.10241965 0.10737246 0.11648832][-0.044462696 -0.041151933 -0.032405261 -0.0200398 -0.0073685516 0.0058445171 0.020454183 0.037802957 0.057864808 0.076396644 0.087677583 0.08781112 0.081705585 0.080455847 0.091631941][-0.042476352 -0.038832255 -0.027827187 -0.011149233 0.00707336 0.025880769 0.04460457 0.063223749 0.080966368 0.093384117 0.095227249 0.083855867 0.0675144 0.060899716 0.073422633][-0.043415766 -0.039687909 -0.026938988 -0.0067416043 0.016326146 0.040131778 0.062704109 0.083063595 0.1008724 0.11208829 0.11136022 0.09570244 0.073908 0.062321216 0.071938865][-0.044668384 -0.040693142 -0.026055166 -0.0025660763 0.024996262 0.053496141 0.079698429 0.10191506 0.12130345 0.13492405 0.13697246 0.12262375 0.098964252 0.082156047 0.084324464][-0.045340944 -0.040405847 -0.023087772 0.0043980675 0.037492126 0.071769364 0.10269269 0.12769102 0.14941606 0.16656645 0.17265351 0.16084522 0.13563766 0.11222988 0.10341626][-0.045532998 -0.038424205 -0.016036106 0.019061498 0.0625381 0.10795555 0.14854085 0.17928497 0.2033544 0.22141175 0.22727314 0.21338111 0.18270756 0.14997067 0.12782764][-0.045753457 -0.035175227 -0.0053188251 0.040638275 0.098384157 0.15919276 0.21336976 0.25196087 0.27737007 0.29262635 0.29363322 0.27355096 0.23486692 0.19213885 0.1571839][-0.045478825 -0.031524803 0.0050504687 0.061149854 0.13308601 0.21080528 0.28126633 0.33058879 0.35860431 0.37024447 0.36491382 0.33714479 0.28960633 0.23708054 0.19059944][-0.043000154 -0.027442804 0.012538956 0.074628174 0.15641572 0.24766387 0.33263239 0.39294702 0.42489278 0.43438327 0.42463768 0.39184743 0.33881453 0.28009036 0.22534114][-0.035558581 -0.020079073 0.019274514 0.080838762 0.16359565 0.25885037 0.35064092 0.41810146 0.45409292 0.46440366 0.45501992 0.42367029 0.37278104 0.31540126 0.25882229][-0.018088143 -0.0036020738 0.031609498 0.085946746 0.15898252 0.24531497 0.33149457 0.39751357 0.43430448 0.44669229 0.44237757 0.42005795 0.38144532 0.33619082 0.28768861][0.0087695168 0.021196924 0.048289623 0.087892637 0.13976471 0.20328203 0.26975778 0.32357645 0.35612643 0.37108505 0.3764067 0.37065595 0.35462704 0.33319446 0.30448103]]...]
INFO - root - 2017-12-10 14:20:55.836184: step 22510, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 67h:29m:19s remains)
INFO - root - 2017-12-10 14:21:03.551266: step 22520, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 65h:41m:49s remains)
INFO - root - 2017-12-10 14:21:11.346294: step 22530, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 67h:53m:37s remains)
INFO - root - 2017-12-10 14:21:19.235410: step 22540, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 67h:58m:21s remains)
INFO - root - 2017-12-10 14:21:27.052135: step 22550, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 66h:11m:06s remains)
INFO - root - 2017-12-10 14:21:34.974474: step 22560, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 69h:33m:17s remains)
INFO - root - 2017-12-10 14:21:42.776144: step 22570, loss = 0.67, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 66h:48m:09s remains)
INFO - root - 2017-12-10 14:21:50.633426: step 22580, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 68h:07m:43s remains)
INFO - root - 2017-12-10 14:21:58.283312: step 22590, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.773 sec/batch; 66h:30m:06s remains)
INFO - root - 2017-12-10 14:22:06.159256: step 22600, loss = 0.70, batch loss = 0.64 (12.1 examples/sec; 0.662 sec/batch; 57h:01m:18s remains)
2017-12-10 14:22:06.997787: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11729769 0.14168815 0.19060075 0.26060763 0.33872908 0.40889132 0.44971022 0.44165614 0.38143751 0.28644875 0.18115956 0.089030534 0.02329989 -0.014092599 -0.029866815][0.15214509 0.16069008 0.17935996 0.21063295 0.25118282 0.29455084 0.32497695 0.32338566 0.28235793 0.21220192 0.1302665 0.055513948 0.0016467782 -0.027326303 -0.037070557][0.18983275 0.18339314 0.17442329 0.17087556 0.17695703 0.19257721 0.20657861 0.20247194 0.17213319 0.12078808 0.060013484 0.0042755776 -0.034062009 -0.050645072 -0.050963502][0.21104942 0.19667834 0.17664558 0.16200846 0.15885371 0.16632026 0.17300084 0.16384247 0.13195483 0.082105018 0.025113469 -0.025610017 -0.058266379 -0.069171071 -0.064148][0.20273329 0.18894713 0.17604657 0.17642127 0.1935911 0.22023976 0.23938881 0.23403019 0.1975836 0.13638341 0.064184912 -0.002100857 -0.047894109 -0.06804689 -0.067931674][0.1673235 0.15914458 0.16317654 0.19230656 0.24638453 0.31076756 0.36076638 0.37395388 0.34059209 0.26748124 0.17214768 0.07720913 0.002988518 -0.041046746 -0.057446331][0.12397134 0.12226795 0.14189388 0.19504878 0.27981281 0.37877858 0.46252951 0.50353467 0.48547336 0.41144642 0.30046692 0.17963856 0.075306445 0.0027859192 -0.035835061][0.096825764 0.1016551 0.12905118 0.18914241 0.28069019 0.38955477 0.48808485 0.5492599 0.55176955 0.4919925 0.38469446 0.25592661 0.13522367 0.042850092 -0.013675332][0.10035878 0.11259852 0.13992339 0.18921773 0.26127306 0.348788 0.4317531 0.49036467 0.50450695 0.46544376 0.3795563 0.26535851 0.15025294 0.056316279 -0.0049781268][0.13347453 0.15358435 0.17733054 0.20724368 0.24516036 0.29093215 0.3346695 0.36700037 0.37452736 0.34830385 0.28754568 0.20108832 0.10984662 0.032876093 -0.017839832][0.18264501 0.20850486 0.22927846 0.24377915 0.25254267 0.258452 0.25909713 0.2538715 0.23822562 0.20779823 0.15998019 0.097700715 0.034954421 -0.015736824 -0.046295904][0.23116918 0.2582643 0.27752471 0.28785262 0.28752455 0.27631354 0.25183013 0.21639858 0.17220743 0.12176226 0.066642836 0.010807122 -0.035105038 -0.064701818 -0.076394029][0.26545373 0.28747872 0.304806 0.32127419 0.33420783 0.33602402 0.31677192 0.27408618 0.21118109 0.13610812 0.057758868 -0.01316575 -0.063753381 -0.089488223 -0.093780972][0.27873281 0.2908642 0.30351138 0.32912087 0.36679354 0.40226117 0.41486293 0.39159906 0.32941949 0.23811375 0.13372904 0.035645928 -0.037090845 -0.077240966 -0.08901576][0.27472144 0.27544332 0.27999079 0.30926484 0.36638823 0.43499327 0.48695192 0.49844474 0.45543548 0.36414459 0.24453244 0.12318745 0.025851816 -0.035948716 -0.063952371]]...]
INFO - root - 2017-12-10 14:22:15.013498: step 22610, loss = 0.71, batch loss = 0.66 (9.8 examples/sec; 0.813 sec/batch; 70h:00m:29s remains)
INFO - root - 2017-12-10 14:22:22.974054: step 22620, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.825 sec/batch; 71h:00m:58s remains)
INFO - root - 2017-12-10 14:22:30.846696: step 22630, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 66h:26m:49s remains)
INFO - root - 2017-12-10 14:22:38.712616: step 22640, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 67h:23m:32s remains)
INFO - root - 2017-12-10 14:22:46.538997: step 22650, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 69h:59m:12s remains)
INFO - root - 2017-12-10 14:22:54.330933: step 22660, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 68h:33m:08s remains)
INFO - root - 2017-12-10 14:23:02.271623: step 22670, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 69h:09m:29s remains)
INFO - root - 2017-12-10 14:23:09.873995: step 22680, loss = 0.70, batch loss = 0.64 (12.2 examples/sec; 0.658 sec/batch; 56h:39m:40s remains)
INFO - root - 2017-12-10 14:23:17.751395: step 22690, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.804 sec/batch; 69h:09m:47s remains)
INFO - root - 2017-12-10 14:23:25.531641: step 22700, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.795 sec/batch; 68h:25m:09s remains)
2017-12-10 14:23:26.351882: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0074501233 0.039854158 0.077643581 0.11591041 0.15782398 0.20257615 0.24908711 0.29285926 0.32247269 0.32832998 0.30261251 0.25051296 0.19165473 0.15192996 0.15178621][0.027769327 0.071411327 0.12284586 0.17571741 0.23151672 0.29063579 0.3505663 0.40498206 0.43988425 0.44610611 0.4168129 0.3592549 0.29780096 0.2637881 0.27780274][0.046931308 0.10183109 0.16972804 0.2432346 0.32047841 0.39874935 0.47065431 0.52718168 0.55444109 0.5467236 0.50285506 0.43692219 0.378644 0.36126241 0.40009576][0.061848287 0.1280304 0.21404698 0.31275102 0.41806117 0.5207935 0.60549587 0.66018838 0.67208731 0.64061272 0.57273686 0.4920682 0.43421265 0.43037063 0.48889944][0.0754505 0.15508854 0.2622292 0.38971457 0.52612394 0.65521014 0.75427485 0.80802113 0.80423933 0.74758369 0.65409964 0.55594611 0.4918696 0.48907015 0.55105484][0.093433566 0.19091652 0.32185262 0.47622794 0.6372869 0.7849611 0.89472538 0.94868177 0.935731 0.86481786 0.75596011 0.64537847 0.5711624 0.55834144 0.60998136][0.11303595 0.22794591 0.37948647 0.552575 0.72667617 0.8818298 0.99652261 1.0524964 1.041761 0.975154 0.868663 0.75552338 0.67154408 0.643579 0.676605][0.12771963 0.25238708 0.41253889 0.58863944 0.75862396 0.90417093 1.0100976 1.062701 1.0591736 1.0099722 0.92240322 0.82269567 0.74121404 0.70689332 0.72709173][0.12929285 0.25090605 0.4023701 0.56146979 0.70664573 0.8225534 0.90289974 0.94187951 0.94268793 0.91548872 0.85902983 0.789501 0.72812921 0.70194119 0.71998316][0.11137726 0.21698821 0.34417298 0.47037569 0.57694042 0.65308571 0.70099932 0.72172439 0.72225809 0.71244621 0.68552053 0.6482662 0.61201108 0.59960532 0.62143189][0.074905276 0.15469983 0.2474882 0.33258983 0.39637265 0.43315911 0.45043263 0.45313582 0.44961438 0.44903392 0.44204876 0.42882925 0.4133774 0.41359156 0.43961921][0.027256524 0.075492427 0.1298824 0.17431231 0.20068449 0.20668516 0.2012223 0.19097918 0.18364368 0.18636091 0.18987915 0.19136363 0.19063398 0.19977951 0.22604661][-0.015410557 0.0050515025 0.027313152 0.040748864 0.041426364 0.028398706 0.0094017722 -0.0087555535 -0.019963132 -0.019266743 -0.013499406 -0.0055635837 0.0022878258 0.016334835 0.040620539][-0.042472854 -0.039645821 -0.037157346 -0.040999606 -0.052548531 -0.072198726 -0.09440174 -0.11360114 -0.12568283 -0.12754616 -0.12332084 -0.11535148 -0.10636234 -0.094192028 -0.076559827][-0.055601917 -0.061330982 -0.067926854 -0.078268006 -0.092162505 -0.10935929 -0.12680647 -0.14118998 -0.1504796 -0.15317045 -0.15129551 -0.14719777 -0.14256482 -0.13650128 -0.12779947]]...]
INFO - root - 2017-12-10 14:23:34.213486: step 22710, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 69h:55m:25s remains)
INFO - root - 2017-12-10 14:23:42.213270: step 22720, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 69h:14m:13s remains)
INFO - root - 2017-12-10 14:23:50.119604: step 22730, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 66h:16m:55s remains)
INFO - root - 2017-12-10 14:23:58.035192: step 22740, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 69h:18m:46s remains)
INFO - root - 2017-12-10 14:24:05.892192: step 22750, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 67h:06m:17s remains)
INFO - root - 2017-12-10 14:24:13.564133: step 22760, loss = 0.69, batch loss = 0.63 (10.9 examples/sec; 0.731 sec/batch; 62h:53m:53s remains)
INFO - root - 2017-12-10 14:24:21.273115: step 22770, loss = 0.71, batch loss = 0.66 (9.9 examples/sec; 0.805 sec/batch; 69h:16m:14s remains)
INFO - root - 2017-12-10 14:24:29.296015: step 22780, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 66h:50m:25s remains)
INFO - root - 2017-12-10 14:24:37.137792: step 22790, loss = 0.67, batch loss = 0.61 (10.5 examples/sec; 0.766 sec/batch; 65h:51m:24s remains)
INFO - root - 2017-12-10 14:24:44.917301: step 22800, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 66h:14m:46s remains)
2017-12-10 14:24:45.762003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.02090957 -0.034981158 -0.0378763 -0.027205866 -0.0056316108 0.016738655 0.028271444 0.027455335 0.01727844 0.0056700716 -0.0049472661 -0.013403905 -0.016389247 -0.012121224 -0.0021994109][-0.005376494 -0.014527785 -0.010303208 0.0078131733 0.033530556 0.053810131 0.05748488 0.04471615 0.023289884 0.004662462 -0.0076334141 -0.014385765 -0.013793639 -0.0049461289 0.0092794383][0.018384693 0.026737764 0.050146669 0.084960282 0.11992451 0.14031729 0.13702966 0.11078101 0.073629886 0.041353513 0.019613232 0.0061231637 0.0014576836 0.0084975762 0.024623834][0.05306074 0.091340646 0.14707386 0.20910913 0.25927058 0.28281212 0.27417374 0.23295251 0.17498696 0.12167765 0.081519634 0.050676089 0.02930636 0.024956793 0.037182935][0.099176422 0.17427182 0.26969367 0.36437824 0.43264568 0.46176642 0.45021853 0.39621621 0.31806493 0.24121393 0.17602408 0.11804245 0.068931445 0.042858616 0.043187104][0.15213761 0.2618857 0.39419892 0.51873142 0.6043483 0.64136183 0.631647 0.57214707 0.48133484 0.38448426 0.29182744 0.20043896 0.11615551 0.060651295 0.041891787][0.20166779 0.33672774 0.49436778 0.63860685 0.73632365 0.78132337 0.77763838 0.72090811 0.62680739 0.51665759 0.39985549 0.27659485 0.15916322 0.076002687 0.038431734][0.23175362 0.37594688 0.54063541 0.6893416 0.79028338 0.84047794 0.84431452 0.795793 0.70566732 0.59033936 0.45921752 0.31651217 0.18030731 0.082094133 0.033269167][0.22432441 0.35734755 0.50771207 0.64364916 0.73761308 0.78875113 0.79960269 0.7624976 0.68344194 0.57492596 0.44699621 0.30629981 0.17328146 0.0777769 0.028199114][0.17504047 0.278276 0.39552689 0.50319439 0.58006161 0.62645495 0.64251673 0.61969388 0.55990785 0.47193831 0.36552429 0.24744643 0.13766022 0.060573045 0.019731935][0.10674179 0.1715254 0.24724415 0.31921333 0.37297255 0.40938812 0.426725 0.41707167 0.37959737 0.32018733 0.24640593 0.16346391 0.088099025 0.037735637 0.010883995][0.047542203 0.078088462 0.11726389 0.15649742 0.18727885 0.21057864 0.22409703 0.22244914 0.20464022 0.17433296 0.13542378 0.090112284 0.050163019 0.025477758 0.011292153][0.010019391 0.020641664 0.037525598 0.054541886 0.067772016 0.078486726 0.085406557 0.086795613 0.08358036 0.077557661 0.067726135 0.052597277 0.039176669 0.031426255 0.023916367][-0.0036719821 0.0021629983 0.011762721 0.018295182 0.021224407 0.022610247 0.023360061 0.025310311 0.031532805 0.041956086 0.050566025 0.052827243 0.053242359 0.052717857 0.047049075][-0.0033647157 0.0076791244 0.020073071 0.025154579 0.02445798 0.021006972 0.01856797 0.021572372 0.03373022 0.052665491 0.0687734 0.076534666 0.080090895 0.079493009 0.072711766]]...]
INFO - root - 2017-12-10 14:24:53.621121: step 22810, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 68h:35m:49s remains)
INFO - root - 2017-12-10 14:25:01.526858: step 22820, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 67h:13m:37s remains)
INFO - root - 2017-12-10 14:25:09.389275: step 22830, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 66h:40m:14s remains)
INFO - root - 2017-12-10 14:25:17.136109: step 22840, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 68h:12m:59s remains)
INFO - root - 2017-12-10 14:25:25.036838: step 22850, loss = 0.68, batch loss = 0.63 (9.5 examples/sec; 0.840 sec/batch; 72h:13m:44s remains)
INFO - root - 2017-12-10 14:25:32.720665: step 22860, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.834 sec/batch; 71h:42m:48s remains)
INFO - root - 2017-12-10 14:25:40.641260: step 22870, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 66h:41m:20s remains)
INFO - root - 2017-12-10 14:25:48.559501: step 22880, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.799 sec/batch; 68h:45m:24s remains)
INFO - root - 2017-12-10 14:25:56.495454: step 22890, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 69h:44m:04s remains)
INFO - root - 2017-12-10 14:26:04.559066: step 22900, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 67h:55m:51s remains)
2017-12-10 14:26:05.371856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054128002 -0.048997492 -0.041992921 -0.035998143 -0.032181468 -0.033415947 -0.039691929 -0.04705102 -0.052667141 -0.055789523 -0.056087408 -0.053472079 -0.049150854 -0.0455527 -0.043590911][-0.043025639 -0.033348434 -0.02165257 -0.01035307 -0.00093482883 0.00068928557 -0.00698443 -0.019127972 -0.031702518 -0.043791309 -0.053887438 -0.058992829 -0.05806458 -0.053862005 -0.048376624][-0.011831987 0.004045072 0.022109143 0.041768663 0.061089441 0.070044346 0.064146578 0.047675345 0.02552324 -0.00017928243 -0.026937716 -0.046719443 -0.054761402 -0.053362831 -0.045572031][0.053622592 0.0818082 0.11088125 0.14272527 0.17550506 0.19549215 0.19418384 0.17357212 0.13913213 0.094181783 0.042926811 0.00032767106 -0.024018832 -0.030994188 -0.024991529][0.16051282 0.20991805 0.25451821 0.29955333 0.345666 0.3788746 0.38647738 0.36576131 0.32071179 0.25437513 0.17346036 0.099533193 0.048348695 0.022636514 0.018253403][0.29767796 0.37208083 0.43201986 0.48605672 0.53957057 0.58340216 0.60187954 0.58620286 0.53592432 0.4511627 0.34149581 0.23374781 0.15005708 0.097825952 0.075654231][0.42855904 0.52334011 0.59376413 0.65000749 0.70239228 0.7502892 0.7781077 0.77048939 0.72104871 0.62526566 0.49507186 0.35963783 0.24565324 0.16647814 0.12535967][0.50800067 0.61157119 0.68441349 0.73740482 0.7831831 0.82820427 0.85878283 0.85683572 0.81284112 0.71788967 0.58222836 0.43320021 0.3009527 0.20487539 0.15241818][0.49680871 0.59091985 0.65507448 0.69947463 0.73522359 0.77142626 0.79752553 0.79694021 0.760894 0.67869318 0.556238 0.41533682 0.28674525 0.19350064 0.1445128][0.40024251 0.46803582 0.512616 0.54343814 0.568265 0.59452444 0.61424929 0.61402071 0.58758235 0.52542269 0.42963925 0.31595033 0.21155509 0.13852139 0.10475671][0.26490724 0.29959342 0.31922707 0.33303842 0.34619612 0.36261398 0.37576169 0.37524673 0.35779878 0.31744009 0.25486717 0.18019308 0.11260204 0.069101118 0.055434745][0.15507691 0.16320494 0.16188994 0.16074595 0.1634879 0.17082115 0.17722034 0.17530942 0.1647366 0.14385322 0.11295571 0.077044822 0.045994084 0.030465215 0.033752549][0.10278285 0.098481171 0.086908288 0.077808656 0.074055746 0.074647114 0.075630739 0.072710112 0.067566939 0.061561409 0.054260194 0.046472296 0.040576894 0.0422069 0.053088591][0.097620264 0.092364036 0.080426432 0.070104122 0.063656412 0.060123514 0.05784383 0.055339877 0.054818492 0.058214549 0.0639975 0.07015457 0.074835666 0.0810268 0.090335496][0.10339679 0.10275489 0.09564963 0.087815374 0.081170782 0.07576815 0.071997993 0.07052815 0.073213719 0.081107169 0.091529734 0.10095093 0.10604461 0.10882903 0.11166134]]...]
INFO - root - 2017-12-10 14:26:13.310751: step 22910, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 68h:17m:13s remains)
INFO - root - 2017-12-10 14:26:20.967137: step 22920, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 66h:20m:47s remains)
INFO - root - 2017-12-10 14:26:28.859878: step 22930, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 67h:22m:26s remains)
INFO - root - 2017-12-10 14:26:36.654462: step 22940, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.776 sec/batch; 66h:43m:04s remains)
INFO - root - 2017-12-10 14:26:44.514473: step 22950, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 66h:53m:17s remains)
INFO - root - 2017-12-10 14:26:52.425180: step 22960, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 66h:16m:48s remains)
INFO - root - 2017-12-10 14:27:00.364746: step 22970, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 67h:46m:35s remains)
INFO - root - 2017-12-10 14:27:08.214180: step 22980, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 65h:25m:25s remains)
INFO - root - 2017-12-10 14:27:16.018252: step 22990, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 67h:18m:37s remains)
INFO - root - 2017-12-10 14:27:23.843060: step 23000, loss = 0.68, batch loss = 0.63 (9.7 examples/sec; 0.824 sec/batch; 70h:49m:13s remains)
2017-12-10 14:27:24.698868: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15438505 0.14140955 0.13797086 0.16785321 0.23106138 0.30383053 0.36671919 0.40692842 0.41119292 0.37525305 0.317588 0.26137823 0.20397285 0.1256042 0.032478254][0.10915975 0.10141998 0.10643876 0.14296146 0.20823334 0.28078648 0.34478036 0.38880727 0.39801428 0.36600497 0.30986226 0.25169724 0.18989038 0.10710877 0.013668016][0.070657335 0.065489717 0.074119113 0.11193895 0.17494264 0.24465102 0.30723026 0.35083231 0.36016479 0.32864931 0.27257711 0.2118516 0.14674355 0.065484621 -0.019679936][0.068488926 0.058967944 0.062844291 0.097435869 0.15863913 0.22903715 0.29281175 0.33456239 0.338907 0.3011581 0.2386428 0.17002194 0.098238505 0.018352956 -0.056324802][0.10478411 0.082009271 0.072220981 0.099771872 0.16091126 0.23641448 0.30473024 0.34516525 0.34315449 0.2971825 0.22663078 0.14911334 0.069936067 -0.010150319 -0.077797964][0.15625267 0.11776648 0.091470361 0.11123971 0.17334621 0.25545207 0.3288326 0.36872298 0.36285651 0.31265298 0.23922275 0.15799235 0.074515529 -0.0073986934 -0.074384123][0.19892436 0.14986065 0.113434 0.13064967 0.19673549 0.28555495 0.36360198 0.4046073 0.39813206 0.34667158 0.27248481 0.18996397 0.10381612 0.018962229 -0.052258313][0.21993732 0.16845134 0.13169651 0.15262644 0.2245492 0.31877893 0.39997891 0.44216719 0.43611604 0.38366392 0.30845577 0.2256723 0.13930823 0.053068079 -0.023087854][0.21803585 0.17230672 0.14327404 0.17162611 0.24801804 0.34275115 0.42174807 0.46171135 0.45418188 0.40061358 0.32601747 0.24748982 0.16716845 0.08457844 0.0060908357][0.20899187 0.17536815 0.15717167 0.19047405 0.26405 0.34932128 0.41600809 0.44686761 0.43580219 0.3837651 0.31532374 0.24764699 0.18012074 0.10656909 0.029290071][0.20513633 0.18596867 0.17580351 0.2061034 0.26617578 0.33021569 0.37425843 0.39076582 0.37752578 0.33407339 0.28042334 0.23019414 0.17933817 0.11719501 0.043449566][0.21758598 0.20988427 0.20227283 0.22134013 0.25924683 0.29383507 0.30919179 0.30926451 0.29673648 0.26890907 0.2370757 0.20749553 0.17223461 0.11920817 0.048697516][0.23406111 0.23274596 0.22258164 0.22521113 0.23714633 0.24056794 0.22868793 0.21668434 0.21139762 0.20652342 0.20093288 0.19092916 0.16591431 0.11648022 0.047227986][0.22948463 0.23053153 0.21631449 0.20442288 0.19582377 0.17745565 0.15057635 0.13801439 0.14756034 0.16794592 0.18549968 0.18776546 0.1643694 0.11197085 0.041848034][0.19219758 0.19385804 0.17898901 0.16159683 0.14589909 0.12272642 0.097987011 0.096066184 0.12252633 0.16155554 0.19077545 0.19342579 0.16240908 0.10269895 0.031596981]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 14:27:32.628557: step 23010, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 68h:45m:26s remains)
INFO - root - 2017-12-10 14:27:40.505798: step 23020, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 67h:48m:57s remains)
INFO - root - 2017-12-10 14:27:48.401539: step 23030, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 66h:53m:56s remains)
INFO - root - 2017-12-10 14:27:56.095220: step 23040, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 68h:12m:08s remains)
INFO - root - 2017-12-10 14:28:03.896370: step 23050, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 68h:50m:50s remains)
INFO - root - 2017-12-10 14:28:11.700077: step 23060, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 67h:36m:25s remains)
INFO - root - 2017-12-10 14:28:19.742174: step 23070, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 68h:07m:09s remains)
INFO - root - 2017-12-10 14:28:27.480091: step 23080, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 66h:48m:30s remains)
INFO - root - 2017-12-10 14:28:35.340156: step 23090, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.798 sec/batch; 68h:36m:36s remains)
INFO - root - 2017-12-10 14:28:43.210008: step 23100, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 67h:02m:15s remains)
2017-12-10 14:28:44.005771: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1333345 0.139613 0.14171661 0.13997562 0.1420655 0.15060435 0.16187187 0.17194541 0.17867287 0.18160008 0.17993003 0.17671925 0.17509022 0.1741278 0.1726516][0.12473061 0.13072446 0.13136023 0.12903012 0.13177295 0.14161064 0.15412754 0.1660963 0.17622267 0.18319324 0.18526106 0.18333997 0.18012251 0.17504194 0.16958492][0.11838137 0.12527221 0.1265108 0.12692282 0.13271153 0.14337918 0.15446103 0.16460991 0.17394258 0.18123256 0.18491633 0.18539646 0.18384942 0.17789723 0.16969381][0.12299123 0.13174051 0.13623181 0.14296354 0.15360536 0.16399321 0.17171982 0.17877182 0.18516523 0.18830472 0.1885417 0.1885753 0.18878938 0.18377066 0.17388491][0.13998188 0.14988545 0.15746827 0.17074226 0.18584087 0.19532175 0.19955984 0.20387204 0.20653036 0.20262195 0.19517457 0.19119681 0.19162537 0.1895355 0.18195334][0.16802539 0.17473468 0.18140595 0.19685332 0.21285349 0.22060889 0.2226906 0.22659825 0.22796501 0.21906067 0.2053887 0.19689225 0.19567627 0.19521575 0.19118495][0.18987718 0.18978059 0.1912024 0.2036553 0.21756335 0.22513384 0.22942658 0.23747775 0.24225521 0.23364444 0.21906888 0.20882283 0.20451464 0.20209342 0.19880325][0.19339064 0.18692894 0.18246521 0.18996909 0.20082246 0.21019116 0.22098356 0.23710217 0.24877384 0.24431586 0.23271343 0.22291563 0.21437839 0.2063469 0.19972321][0.1857136 0.17465839 0.16535573 0.16740978 0.17431836 0.18519586 0.20298761 0.22748016 0.24672943 0.24803464 0.24056056 0.23115714 0.21762113 0.20249523 0.18996969][0.17645457 0.16355339 0.15090467 0.1474966 0.14913693 0.1587169 0.17942484 0.20825233 0.232878 0.24020775 0.23730791 0.22851697 0.21151142 0.19172685 0.17474975][0.17230485 0.16021761 0.14564237 0.13682008 0.13250981 0.13854206 0.15787265 0.18647642 0.21342298 0.22587177 0.22785307 0.2211073 0.20379786 0.18309903 0.16449964][0.17435111 0.16601333 0.15189119 0.13955677 0.13039514 0.13261703 0.14855768 0.17398405 0.20023036 0.21489167 0.21971919 0.21475825 0.19890589 0.18028203 0.16286093][0.181756 0.1785398 0.16735889 0.15479581 0.14309511 0.14173383 0.15323254 0.17412992 0.19760795 0.2118127 0.21739739 0.21343096 0.19987507 0.18549597 0.17227435][0.18777889 0.19159321 0.18610826 0.17634611 0.16392328 0.15924688 0.16568251 0.18154855 0.20149566 0.21507928 0.2223808 0.22063759 0.21012051 0.20019737 0.19191003][0.18943797 0.20103712 0.20279422 0.19764148 0.18580915 0.17826132 0.17941363 0.1893498 0.20486659 0.21864437 0.23029056 0.23355436 0.22749074 0.22160377 0.21741548]]...]
INFO - root - 2017-12-10 14:28:51.838599: step 23110, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 67h:16m:04s remains)
INFO - root - 2017-12-10 14:28:59.622650: step 23120, loss = 0.70, batch loss = 0.65 (11.2 examples/sec; 0.717 sec/batch; 61h:38m:32s remains)
INFO - root - 2017-12-10 14:29:07.455743: step 23130, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 68h:01m:54s remains)
INFO - root - 2017-12-10 14:29:15.415649: step 23140, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.784 sec/batch; 67h:24m:06s remains)
INFO - root - 2017-12-10 14:29:23.352358: step 23150, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 68h:09m:43s remains)
INFO - root - 2017-12-10 14:29:31.152345: step 23160, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 68h:32m:09s remains)
INFO - root - 2017-12-10 14:29:39.019029: step 23170, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.816 sec/batch; 70h:09m:22s remains)
INFO - root - 2017-12-10 14:29:46.873793: step 23180, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 66h:34m:33s remains)
INFO - root - 2017-12-10 14:29:54.637484: step 23190, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 67h:07m:43s remains)
INFO - root - 2017-12-10 14:30:02.470445: step 23200, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 67h:54m:40s remains)
2017-12-10 14:30:03.311782: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.071838349 -0.067538455 -0.063467443 -0.061298519 -0.061824482 -0.065116167 -0.069852494 -0.074846424 -0.078054078 -0.078981161 -0.078207262 -0.076978877 -0.07590545 -0.075598545 -0.0761385][-0.077512659 -0.074210912 -0.070694059 -0.0683512 -0.068142094 -0.070692338 -0.075168952 -0.080699548 -0.084893018 -0.08657746 -0.08557155 -0.0833157 -0.080985591 -0.07960134 -0.079554595][-0.080408 -0.078394689 -0.075006582 -0.071086727 -0.068130709 -0.068123996 -0.071916617 -0.079274207 -0.087297 -0.093006626 -0.094217032 -0.091857553 -0.087782614 -0.084120534 -0.082270861][-0.076906025 -0.074794494 -0.068390518 -0.057781864 -0.045705356 -0.037140094 -0.036770105 -0.04640983 -0.062703468 -0.079537131 -0.090490736 -0.09400472 -0.091761962 -0.087327376 -0.08391811][-0.064232133 -0.059076425 -0.043897055 -0.01745585 0.0156576 0.044742189 0.057504639 0.047221776 0.016585663 -0.022981662 -0.057423931 -0.078820594 -0.086651385 -0.085889451 -0.082639575][-0.04437634 -0.032582819 -0.0020271903 0.05116621 0.12019648 0.1854471 0.22214724 0.21470183 0.16362739 0.087878592 0.013339773 -0.041107655 -0.069889061 -0.079184644 -0.07907889][-0.023907419 -0.0034505809 0.04669062 0.13442911 0.250764 0.36444104 0.43466675 0.43366164 0.35884291 0.23818699 0.1123216 0.014048527 -0.044020314 -0.068623379 -0.074436329][-0.010420762 0.017322831 0.0850712 0.20506553 0.36632088 0.52656436 0.6297487 0.6365239 0.54149854 0.38057053 0.20748508 0.067834891 -0.018664265 -0.05883437 -0.071092494][-0.0087755127 0.020699114 0.096014425 0.23241277 0.41764915 0.603182 0.72480154 0.73697609 0.63307536 0.45278206 0.25618935 0.09521772 -0.0063404543 -0.054897815 -0.070662186][-0.018604325 0.003998463 0.071356632 0.19942185 0.37618959 0.55424994 0.67178142 0.6850214 0.58813125 0.41866812 0.23348528 0.0817777 -0.013803159 -0.059051372 -0.073226541][-0.032517906 -0.024595587 0.020745263 0.11778796 0.25642511 0.39766702 0.49182457 0.50365245 0.42895964 0.29755104 0.15441178 0.03807747 -0.034010667 -0.0667286 -0.075826406][-0.041430086 -0.051599767 -0.034738909 0.020427002 0.10682712 0.1977787 0.26054204 0.27135742 0.22713135 0.14663193 0.058813147 -0.011843745 -0.0545319 -0.072437063 -0.076259062][-0.043325931 -0.069562905 -0.078192733 -0.060833246 -0.021162443 0.026043935 0.063333683 0.076580279 0.062008303 0.027576258 -0.012462348 -0.045590427 -0.065709382 -0.07352712 -0.074626513][-0.040244307 -0.076776341 -0.10174401 -0.10821328 -0.096610755 -0.07365679 -0.047648195 -0.028193753 -0.021405 -0.027041195 -0.040095478 -0.054526117 -0.065518439 -0.070946157 -0.07255666][-0.031106962 -0.071191579 -0.10257934 -0.11795904 -0.11504497 -0.097159244 -0.07022243 -0.04419269 -0.027925499 -0.024713624 -0.032556847 -0.046373695 -0.05991631 -0.068750478 -0.0727109]]...]
INFO - root - 2017-12-10 14:30:10.935613: step 23210, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 65h:33m:15s remains)
INFO - root - 2017-12-10 14:30:18.745918: step 23220, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 65h:10m:37s remains)
INFO - root - 2017-12-10 14:30:26.647053: step 23230, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 67h:48m:59s remains)
INFO - root - 2017-12-10 14:30:34.382996: step 23240, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 66h:52m:36s remains)
INFO - root - 2017-12-10 14:30:42.328438: step 23250, loss = 0.67, batch loss = 0.61 (10.5 examples/sec; 0.765 sec/batch; 65h:41m:57s remains)
INFO - root - 2017-12-10 14:30:50.224208: step 23260, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.795 sec/batch; 68h:19m:44s remains)
INFO - root - 2017-12-10 14:30:58.079719: step 23270, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 68h:33m:21s remains)
INFO - root - 2017-12-10 14:31:05.859564: step 23280, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 66h:34m:07s remains)
INFO - root - 2017-12-10 14:31:13.735611: step 23290, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 67h:27m:23s remains)
INFO - root - 2017-12-10 14:31:21.372228: step 23300, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.777 sec/batch; 66h:42m:59s remains)
2017-12-10 14:31:22.314831: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24177639 0.21201673 0.19126777 0.18697248 0.18578061 0.18835804 0.19741713 0.21405751 0.22751655 0.22032435 0.19946457 0.18154994 0.17638639 0.17384887 0.16202995][0.25277644 0.23787123 0.22832471 0.22559617 0.21776964 0.20869289 0.20541953 0.21097881 0.21841794 0.21455641 0.20309168 0.19500574 0.19681032 0.19712205 0.18346614][0.22918239 0.23493406 0.24328612 0.25009397 0.24503496 0.23254417 0.22200719 0.21902065 0.22028019 0.21660809 0.21002986 0.20751072 0.21219033 0.21158892 0.19450951][0.1865329 0.21341935 0.24255553 0.26590014 0.27408585 0.26953191 0.26097572 0.25517324 0.25049248 0.24124141 0.2300933 0.22361647 0.22339985 0.21647517 0.19418819][0.16104597 0.20355022 0.25134796 0.29392433 0.32162318 0.33292964 0.33453786 0.33179429 0.32190925 0.30111164 0.27600574 0.25704002 0.24527355 0.22850478 0.1996066][0.16679248 0.21856603 0.28073207 0.34174272 0.39053115 0.42098477 0.43767241 0.44317618 0.43077135 0.39677614 0.35358059 0.31825823 0.29184827 0.26274076 0.22515345][0.19250798 0.24923871 0.32199153 0.39878923 0.46684185 0.51541662 0.54720885 0.562365 0.54967183 0.50432819 0.44373012 0.3916426 0.34893319 0.30507734 0.25640488][0.22574805 0.28353524 0.36039558 0.44463161 0.523073 0.58200842 0.62308425 0.64585733 0.63595057 0.58659607 0.51709056 0.45514321 0.40021828 0.34310773 0.28319454][0.24036936 0.29175442 0.36150014 0.43915048 0.51296455 0.5693537 0.609939 0.63441873 0.62812734 0.58306271 0.5175181 0.45900375 0.40465364 0.34616071 0.28454858][0.20971341 0.24770024 0.300716 0.360255 0.41730684 0.46107161 0.49338979 0.51414895 0.51103961 0.47630337 0.42548275 0.38276827 0.34263062 0.2968854 0.24622478][0.13035545 0.1531895 0.18851513 0.22943904 0.26935178 0.30084717 0.32526264 0.34192556 0.34134537 0.31685403 0.28135222 0.25594309 0.23488408 0.21007198 0.179493][0.035454288 0.046313137 0.06887453 0.097091854 0.12560561 0.14972124 0.16976383 0.18335375 0.18308724 0.16462468 0.1396569 0.12710065 0.12305072 0.12026332 0.11305612][-0.038139232 -0.032810446 -0.0160008 0.0069273706 0.031032657 0.05271171 0.071023852 0.081718616 0.078916728 0.061902393 0.042967271 0.03942927 0.049329005 0.066129856 0.080413938][-0.073218547 -0.065795317 -0.047408313 -0.023307241 0.0015092647 0.023325089 0.040097889 0.04659985 0.03853805 0.019568317 0.0040738396 0.0079490971 0.029626872 0.061979912 0.093104117][-0.071709983 -0.055773266 -0.029315088 0.0012851487 0.030416414 0.053346194 0.067398518 0.068237767 0.053748168 0.031580824 0.018436218 0.028574459 0.058656868 0.099842779 0.13964859]]...]
INFO - root - 2017-12-10 14:31:30.103150: step 23310, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 65h:09m:25s remains)
INFO - root - 2017-12-10 14:31:37.801548: step 23320, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 68h:46m:35s remains)
INFO - root - 2017-12-10 14:31:45.694561: step 23330, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 69h:14m:16s remains)
INFO - root - 2017-12-10 14:31:53.769464: step 23340, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 66h:52m:32s remains)
INFO - root - 2017-12-10 14:32:01.699692: step 23350, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 66h:46m:36s remains)
INFO - root - 2017-12-10 14:32:09.573622: step 23360, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 67h:15m:08s remains)
INFO - root - 2017-12-10 14:32:17.351691: step 23370, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.790 sec/batch; 67h:48m:45s remains)
INFO - root - 2017-12-10 14:32:25.179329: step 23380, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 66h:57m:23s remains)
INFO - root - 2017-12-10 14:32:32.870831: step 23390, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 66h:23m:52s remains)
INFO - root - 2017-12-10 14:32:40.496044: step 23400, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 67h:35m:12s remains)
2017-12-10 14:32:41.314507: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19452159 0.15013985 0.12404382 0.13517047 0.19106002 0.27209076 0.34895998 0.3900837 0.38129464 0.33898813 0.28401959 0.23736823 0.20614353 0.19226134 0.19048271][0.18180855 0.14091401 0.1226839 0.14785939 0.21873361 0.3114123 0.392131 0.42997524 0.41237295 0.35617623 0.28339043 0.21714854 0.16806379 0.14071421 0.13164295][0.17530656 0.13559309 0.12195221 0.15589491 0.23509961 0.33226791 0.41092119 0.44364771 0.41940993 0.35430512 0.2687223 0.18753459 0.12466906 0.087557077 0.074544683][0.19166182 0.14600919 0.12818891 0.16097431 0.23964417 0.33522415 0.41026509 0.44059128 0.41456494 0.34500936 0.25076187 0.1594068 0.088833377 0.048336565 0.037166644][0.21859977 0.16791065 0.14438878 0.17201887 0.24516235 0.33455822 0.40446362 0.43324465 0.40808669 0.3369858 0.23730262 0.13991632 0.066355623 0.026932741 0.020608421][0.24444835 0.19662686 0.17491326 0.20157081 0.26931313 0.34995046 0.41213697 0.43670267 0.41013655 0.33659926 0.23318836 0.13193278 0.05560128 0.015922196 0.012797982][0.25661418 0.22155437 0.21159904 0.24417879 0.31030545 0.38384873 0.43895376 0.45830932 0.42811471 0.3509416 0.24456483 0.13902323 0.055407647 0.0086126709 0.0023367004][0.2529273 0.23932737 0.24848907 0.28990418 0.35447848 0.42040217 0.46819735 0.4821091 0.44875875 0.37042382 0.26496127 0.15751676 0.064893954 0.0060583497 -0.0088321008][0.24108543 0.25410804 0.28257373 0.32722533 0.38233665 0.43499264 0.47408381 0.48491663 0.45450374 0.38333938 0.28672743 0.18257169 0.083422258 0.012644112 -0.012505868][0.23418176 0.27422553 0.31636158 0.35389942 0.38778871 0.41841015 0.44563234 0.45584515 0.4354119 0.38081595 0.3014195 0.2073759 0.1077971 0.0296756 -0.004114815][0.24147861 0.30225769 0.34757894 0.36774507 0.37196067 0.37500763 0.38772476 0.3980583 0.39054382 0.3565087 0.29858345 0.22060442 0.12913874 0.051645219 0.013191575][0.2565352 0.32447952 0.3611542 0.35838503 0.33370575 0.31269488 0.31330025 0.32512361 0.33077845 0.31644714 0.27900156 0.21922493 0.14249305 0.072483115 0.032554619][0.26510987 0.32589856 0.34540555 0.32023463 0.27397278 0.23729287 0.23038107 0.24458563 0.26157424 0.26339683 0.24300657 0.20135921 0.14388086 0.087363869 0.04952231][0.24252623 0.28519392 0.28595617 0.24690652 0.19363347 0.15525895 0.14984086 0.17018174 0.19787768 0.21244524 0.20512843 0.17894019 0.14024815 0.098463707 0.063865848][0.17445207 0.19688118 0.18607321 0.1474871 0.10373511 0.077836625 0.083697349 0.11556217 0.15567887 0.1827593 0.18775554 0.17458372 0.15009663 0.11858034 0.084241427]]...]
INFO - root - 2017-12-10 14:32:49.114981: step 23410, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 68h:18m:26s remains)
INFO - root - 2017-12-10 14:32:57.004733: step 23420, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 68h:57m:11s remains)
INFO - root - 2017-12-10 14:33:04.990851: step 23430, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 68h:40m:01s remains)
INFO - root - 2017-12-10 14:33:12.852263: step 23440, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 67h:23m:35s remains)
INFO - root - 2017-12-10 14:33:20.818337: step 23450, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 67h:55m:37s remains)
INFO - root - 2017-12-10 14:33:28.779559: step 23460, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 67h:41m:52s remains)
INFO - root - 2017-12-10 14:33:36.616455: step 23470, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 66h:54m:37s remains)
INFO - root - 2017-12-10 14:33:44.260144: step 23480, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 68h:42m:42s remains)
INFO - root - 2017-12-10 14:33:52.133504: step 23490, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 68h:02m:17s remains)
INFO - root - 2017-12-10 14:34:00.065764: step 23500, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 67h:23m:05s remains)
2017-12-10 14:34:01.031950: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35109633 0.25086361 0.15569353 0.10686415 0.11350559 0.17016275 0.25508848 0.3401759 0.40654939 0.44328621 0.44544241 0.43054107 0.41468391 0.40618384 0.41624758][0.3167859 0.23254117 0.14980371 0.10262887 0.10181598 0.14724171 0.22023587 0.29320985 0.34663758 0.37215626 0.36897781 0.35243329 0.33615494 0.32651398 0.33179587][0.26683879 0.20409928 0.13861257 0.0964816 0.088759817 0.12002297 0.17587945 0.23245133 0.27057555 0.28441659 0.276934 0.26087081 0.24667241 0.23806012 0.23964247][0.24983361 0.20751904 0.15818861 0.12312666 0.11370417 0.13637985 0.17899317 0.22062036 0.24290082 0.24202579 0.22438625 0.20161393 0.18280634 0.17099854 0.16722481][0.27954414 0.25161207 0.21386983 0.18721113 0.1836389 0.20841536 0.24715349 0.27864698 0.28568342 0.26653334 0.23056594 0.19028795 0.15663594 0.13410184 0.12141386][0.34278077 0.32429883 0.29507259 0.27870947 0.28740931 0.32293 0.36537936 0.39072248 0.38412723 0.3459557 0.28769159 0.22449914 0.17082427 0.13387835 0.11138964][0.40466616 0.39664534 0.37809452 0.374013 0.39663774 0.44376633 0.4904702 0.51013345 0.4912813 0.4365229 0.35812026 0.27442315 0.20305906 0.15395394 0.12465417][0.40928829 0.41305208 0.4070597 0.41381735 0.44538236 0.49781358 0.54474515 0.55960965 0.53408396 0.47228098 0.38568202 0.29365125 0.21500236 0.16135459 0.13061386][0.33411831 0.34788576 0.35358226 0.36722425 0.39952675 0.44773579 0.48947468 0.50197768 0.47898012 0.42420375 0.34672731 0.26367661 0.19232275 0.14396363 0.11733761][0.20189296 0.21888669 0.23076984 0.24526536 0.27061206 0.30658334 0.33845234 0.35042426 0.33806998 0.30322 0.2508803 0.19312182 0.14317185 0.10998724 0.092830174][0.068777226 0.0823827 0.094018146 0.10438857 0.11912462 0.14016676 0.1611633 0.17441441 0.17712358 0.16834517 0.14799562 0.12188926 0.098308079 0.082940929 0.075618431][-0.017951841 -0.010668424 -0.00297314 0.0017679387 0.0066688238 0.014752047 0.026703585 0.04219659 0.059661012 0.075168177 0.0834233 0.0844107 0.081882328 0.07880231 0.076283768][-0.048378754 -0.046934605 -0.043227442 -0.042349346 -0.043502994 -0.043756869 -0.038068011 -0.021871496 0.0041579832 0.034113549 0.059218667 0.075561434 0.083591163 0.084532656 0.081301622][-0.03991266 -0.041946448 -0.040840179 -0.04163193 -0.044885505 -0.048629403 -0.046530582 -0.032147244 -0.005382251 0.027569056 0.056839306 0.077065587 0.087067232 0.087202087 0.0814475][-0.016520411 -0.019099798 -0.01877892 -0.019487502 -0.022175504 -0.026078358 -0.025864458 -0.015524534 0.0055901334 0.03258393 0.056755058 0.073390521 0.081003711 0.079442851 0.07235758]]...]
INFO - root - 2017-12-10 14:34:08.985879: step 23510, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 66h:44m:48s remains)
INFO - root - 2017-12-10 14:34:16.880303: step 23520, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.790 sec/batch; 67h:45m:59s remains)
INFO - root - 2017-12-10 14:34:24.749798: step 23530, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 68h:31m:36s remains)
INFO - root - 2017-12-10 14:34:32.616905: step 23540, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 66h:54m:45s remains)
INFO - root - 2017-12-10 14:34:40.435388: step 23550, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 66h:33m:50s remains)
INFO - root - 2017-12-10 14:34:48.086154: step 23560, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 65h:36m:43s remains)
INFO - root - 2017-12-10 14:34:55.934503: step 23570, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 68h:01m:40s remains)
INFO - root - 2017-12-10 14:35:03.819670: step 23580, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 68h:06m:18s remains)
INFO - root - 2017-12-10 14:35:11.606314: step 23590, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 66h:47m:19s remains)
INFO - root - 2017-12-10 14:35:19.418708: step 23600, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.766 sec/batch; 65h:42m:51s remains)
2017-12-10 14:35:20.200314: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035540797 0.033893056 0.030619347 0.026238104 0.023914164 0.023910707 0.024722943 0.023966461 0.024738489 0.029465603 0.033575248 0.03372886 0.03070844 0.026753513 0.021409951][0.056729693 0.05268525 0.046092555 0.039051432 0.035964031 0.036831804 0.039352238 0.040380847 0.043259893 0.051182806 0.058629539 0.060423784 0.056846559 0.050444458 0.041333191][0.074363567 0.068644613 0.058958117 0.0498202 0.046338215 0.048318949 0.052714061 0.055935375 0.06113077 0.071941927 0.082609288 0.08625222 0.082354985 0.073148757 0.05960799][0.084262818 0.07701 0.066097811 0.056672055 0.053923827 0.057317849 0.062874168 0.067222811 0.073497824 0.085773833 0.098576076 0.10385507 0.099938452 0.088199832 0.070458189][0.089246787 0.082089879 0.07192573 0.063598692 0.062113438 0.06662219 0.071982227 0.075733028 0.081664048 0.094263934 0.10832016 0.11496395 0.11116704 0.097202562 0.0756448][0.094472274 0.088318072 0.079247817 0.071635209 0.070554942 0.075303853 0.079896934 0.082778811 0.088049971 0.10033547 0.1147681 0.1219585 0.11787832 0.10204304 0.077535279][0.10249433 0.097590581 0.088719167 0.0806868 0.0787169 0.082608357 0.085995518 0.0879598 0.092200413 0.1031455 0.11625832 0.12225612 0.11717276 0.10021836 0.074546449][0.11247304 0.11013816 0.10168146 0.093011811 0.089144729 0.090627551 0.091695778 0.091995448 0.094486475 0.10323606 0.11384474 0.11767088 0.1114862 0.0945538 0.069588721][0.12716308 0.12745951 0.11987227 0.11052979 0.10405952 0.1017863 0.099468373 0.097277515 0.09757819 0.10388747 0.1115328 0.11296348 0.10594561 0.089772604 0.0662381][0.14514875 0.14704762 0.13977621 0.12957172 0.12074146 0.11527333 0.11034904 0.10634611 0.10513111 0.10931093 0.11388937 0.11250439 0.10412057 0.087671377 0.064130649][0.15293275 0.1546602 0.14786436 0.13807179 0.12901682 0.12282238 0.11742468 0.11322255 0.11159497 0.11426159 0.11632441 0.11249289 0.10276418 0.085472815 0.061039522][0.14064011 0.14089707 0.13503614 0.12728594 0.12030669 0.11540949 0.11071993 0.10695868 0.10542559 0.10728971 0.10811054 0.10335689 0.093502805 0.076582417 0.05265303][0.11437922 0.11275662 0.10804435 0.10277313 0.098549 0.09542153 0.0915979 0.08845447 0.087480612 0.089469016 0.0903756 0.085929394 0.0767919 0.061468367 0.040042598][0.083418369 0.080375463 0.076875046 0.073685609 0.071577057 0.069608822 0.066306382 0.06371969 0.063546717 0.065813392 0.066663258 0.062125027 0.053298235 0.040003091 0.02269621][0.05113459 0.04791585 0.0458576 0.044460852 0.043867305 0.042918965 0.040591065 0.039153177 0.040040161 0.042328972 0.042545564 0.037392955 0.028524728 0.017112933 0.0041501676]]...]
INFO - root - 2017-12-10 14:35:28.182181: step 23610, loss = 0.68, batch loss = 0.62 (9.7 examples/sec; 0.824 sec/batch; 70h:40m:44s remains)
INFO - root - 2017-12-10 14:35:36.022917: step 23620, loss = 0.67, batch loss = 0.61 (10.3 examples/sec; 0.776 sec/batch; 66h:33m:00s remains)
INFO - root - 2017-12-10 14:35:43.803502: step 23630, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.764 sec/batch; 65h:35m:16s remains)
INFO - root - 2017-12-10 14:35:51.471035: step 23640, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 67h:11m:55s remains)
INFO - root - 2017-12-10 14:35:59.309342: step 23650, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.828 sec/batch; 71h:00m:17s remains)
INFO - root - 2017-12-10 14:36:06.996684: step 23660, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 68h:13m:37s remains)
INFO - root - 2017-12-10 14:36:15.015264: step 23670, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 65h:18m:04s remains)
INFO - root - 2017-12-10 14:36:22.937737: step 23680, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 67h:58m:25s remains)
INFO - root - 2017-12-10 14:36:30.898181: step 23690, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 68h:03m:10s remains)
INFO - root - 2017-12-10 14:36:38.805497: step 23700, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 68h:45m:27s remains)
2017-12-10 14:36:39.597922: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32704055 0.33711082 0.35359138 0.36725044 0.36525431 0.34793478 0.31205824 0.26158443 0.210227 0.18955737 0.22155842 0.2972118 0.37911969 0.42698187 0.42592156][0.32928714 0.32883677 0.33147073 0.33196297 0.32132295 0.30001295 0.26649725 0.22454649 0.18460241 0.17037608 0.1978897 0.2578249 0.31744725 0.344289 0.33161512][0.29839897 0.28617835 0.27734548 0.27160415 0.26426026 0.25351384 0.23566245 0.21228492 0.1898607 0.18205586 0.19871937 0.23416084 0.26520705 0.2697956 0.24725533][0.24207728 0.22153547 0.20849361 0.2072289 0.21605521 0.22874257 0.23704469 0.23887771 0.23740491 0.23729365 0.24323283 0.25436649 0.25940043 0.24803939 0.22087294][0.17200406 0.15015139 0.14300275 0.15764368 0.19271415 0.23577867 0.27307338 0.29931715 0.31659323 0.32479873 0.32463029 0.31927308 0.30712998 0.28668967 0.25719482][0.10843789 0.093744189 0.10163993 0.1401379 0.20477067 0.27602816 0.33566749 0.37713715 0.40443817 0.41756564 0.41500026 0.40241635 0.38292184 0.35904157 0.32853779][0.066979721 0.065075912 0.091287717 0.15301386 0.2409474 0.3301568 0.39951429 0.44237745 0.46776816 0.47977406 0.47634387 0.46326607 0.44356823 0.4204396 0.39142546][0.052026231 0.062716149 0.10475209 0.18286093 0.28288966 0.37695944 0.44407767 0.48060459 0.49981081 0.509591 0.508588 0.50035971 0.4836759 0.46031857 0.43152386][0.061149508 0.081557259 0.13286665 0.21637768 0.3145 0.39981908 0.45470157 0.48130307 0.49544081 0.50738072 0.51389152 0.51345068 0.499402 0.47260934 0.44257423][0.093379147 0.1179222 0.16830827 0.2432342 0.32461312 0.38935706 0.42653444 0.44512948 0.46001691 0.48008895 0.49859443 0.50878316 0.49843615 0.46848336 0.43891239][0.13653918 0.15814196 0.19764991 0.25343764 0.31005567 0.35125509 0.37410882 0.39245743 0.41624233 0.44952211 0.48085314 0.49955264 0.49059787 0.45754007 0.43035159][0.17005937 0.18061148 0.20131795 0.23273702 0.26485544 0.28841683 0.30642337 0.33279604 0.37046909 0.41722104 0.45672482 0.47693536 0.46475422 0.42992464 0.40931758][0.1866495 0.17966425 0.17714682 0.18388626 0.19662207 0.21120024 0.23206899 0.26892269 0.31851491 0.37314576 0.41326863 0.42807081 0.40988746 0.37574536 0.36493295][0.19141276 0.16525669 0.14044935 0.12624113 0.1250999 0.13505645 0.15819623 0.19848977 0.24991073 0.30261406 0.33732295 0.344822 0.32265621 0.29203066 0.29026556][0.18552494 0.14426322 0.10292579 0.074372143 0.064146534 0.0703024 0.091093965 0.12559518 0.16855991 0.21193248 0.24023719 0.24463062 0.22462581 0.19989391 0.20094502]]...]
INFO - root - 2017-12-10 14:36:47.455061: step 23710, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 67h:07m:03s remains)
INFO - root - 2017-12-10 14:36:55.029114: step 23720, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.785 sec/batch; 67h:20m:24s remains)
INFO - root - 2017-12-10 14:37:02.980439: step 23730, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 68h:45m:05s remains)
INFO - root - 2017-12-10 14:37:10.770988: step 23740, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.790 sec/batch; 67h:43m:34s remains)
INFO - root - 2017-12-10 14:37:18.375108: step 23750, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 68h:01m:55s remains)
INFO - root - 2017-12-10 14:37:26.215988: step 23760, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 65h:20m:12s remains)
INFO - root - 2017-12-10 14:37:34.067087: step 23770, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 65h:52m:30s remains)
INFO - root - 2017-12-10 14:37:41.858439: step 23780, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 67h:47m:23s remains)
INFO - root - 2017-12-10 14:37:49.672692: step 23790, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 67h:52m:59s remains)
INFO - root - 2017-12-10 14:37:57.477016: step 23800, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 68h:18m:02s remains)
2017-12-10 14:37:58.358877: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.075165838 0.086909495 0.093174554 0.094158433 0.088789158 0.075290181 0.059798595 0.056261458 0.072415359 0.10802605 0.1580908 0.21346165 0.25586784 0.2733885 0.26717433][0.10529283 0.12717775 0.14539349 0.15899582 0.16395563 0.15301798 0.12921 0.10904415 0.10577453 0.12162925 0.15414168 0.19760323 0.23514961 0.25237557 0.24605781][0.1367054 0.16428542 0.19410264 0.22443236 0.24694069 0.24701533 0.22211246 0.18670578 0.15788309 0.14248127 0.1439036 0.16286817 0.18719234 0.20262758 0.20135544][0.16675906 0.19403253 0.23166932 0.27808455 0.32029083 0.33698666 0.31936884 0.27587694 0.22431837 0.1763262 0.14266364 0.13227135 0.13938035 0.15180318 0.1574416][0.21033096 0.23085783 0.26760265 0.32083777 0.37584981 0.40805411 0.40363055 0.36269113 0.29990724 0.22924189 0.16713819 0.13062339 0.12039535 0.12710732 0.13636521][0.287269 0.29339749 0.31415203 0.35518363 0.40640444 0.44596869 0.45619759 0.42842427 0.37073028 0.29466537 0.2170451 0.16012083 0.13140388 0.12620415 0.13057376][0.39017734 0.37831593 0.37236825 0.38646072 0.42076242 0.46020809 0.48374161 0.47527328 0.43566158 0.3699728 0.28976062 0.2180167 0.16928698 0.14538209 0.13552472][0.48292848 0.45388478 0.41997674 0.40445849 0.41859311 0.45410985 0.48692951 0.49642119 0.47778428 0.42819819 0.35254309 0.2716963 0.20582838 0.16215989 0.1343783][0.52095616 0.48073977 0.42752382 0.3899855 0.38673073 0.41447926 0.44881302 0.46885356 0.46671966 0.43312857 0.36808416 0.28849661 0.21561527 0.15867716 0.11658282][0.47302625 0.43233964 0.37446132 0.32876638 0.31578609 0.33521327 0.3651453 0.38770449 0.39492676 0.37465253 0.32491231 0.25810543 0.19259639 0.13691342 0.094673067][0.35526508 0.32076693 0.26991534 0.22788639 0.21346195 0.22770998 0.25245258 0.27403486 0.28661269 0.27848843 0.24764737 0.20188987 0.15504193 0.11462269 0.088103257][0.22145659 0.19441752 0.1552778 0.12338954 0.11351438 0.12634596 0.1476344 0.16779718 0.18369594 0.18639991 0.17523129 0.15389955 0.13074079 0.1118834 0.10803164][0.14532676 0.11909683 0.083555326 0.055586178 0.046681158 0.05674177 0.074570686 0.092518024 0.10873864 0.11741274 0.11971262 0.11678668 0.11257862 0.11223748 0.1284764][0.15167095 0.11832526 0.074022353 0.038225174 0.021743711 0.024540896 0.037242986 0.051632725 0.065324254 0.074597172 0.081929892 0.087526463 0.093129672 0.10352668 0.13152669][0.19947429 0.15617727 0.0992011 0.051431537 0.024290688 0.018541165 0.025640395 0.035869814 0.045540508 0.052390154 0.059474032 0.066715688 0.075103961 0.089232974 0.11990526]]...]
INFO - root - 2017-12-10 14:38:06.182723: step 23810, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.822 sec/batch; 70h:26m:50s remains)
INFO - root - 2017-12-10 14:38:13.981721: step 23820, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 65h:40m:31s remains)
INFO - root - 2017-12-10 14:38:21.598020: step 23830, loss = 0.70, batch loss = 0.65 (12.4 examples/sec; 0.647 sec/batch; 55h:27m:52s remains)
INFO - root - 2017-12-10 14:38:29.404570: step 23840, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 66h:36m:44s remains)
INFO - root - 2017-12-10 14:38:37.350343: step 23850, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 68h:59m:09s remains)
INFO - root - 2017-12-10 14:38:45.162350: step 23860, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 67h:17m:18s remains)
INFO - root - 2017-12-10 14:38:52.996477: step 23870, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 66h:29m:32s remains)
INFO - root - 2017-12-10 14:39:00.577810: step 23880, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 66h:40m:37s remains)
INFO - root - 2017-12-10 14:39:08.462563: step 23890, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 67h:26m:40s remains)
INFO - root - 2017-12-10 14:39:16.275700: step 23900, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 68h:09m:04s remains)
2017-12-10 14:39:17.153486: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.015445024 0.016322045 0.068684176 0.13300535 0.19248588 0.22491023 0.22453004 0.20285724 0.17084296 0.13722642 0.10882568 0.094092809 0.091867678 0.089524955 0.090586744][-0.0080748331 0.032205708 0.10127748 0.19010484 0.27439681 0.32232687 0.32505664 0.29576921 0.24920124 0.19867256 0.15554924 0.13101065 0.12491201 0.1212127 0.12411796][-0.0011880036 0.044472747 0.12705991 0.23869507 0.34869587 0.41458556 0.42345491 0.38876936 0.32832113 0.26102027 0.20391142 0.17044626 0.16002803 0.1556403 0.16240162][0.0042960052 0.054366566 0.14756939 0.2774834 0.40920573 0.49138302 0.5069105 0.46834213 0.3969501 0.31796655 0.25239438 0.21301988 0.19992374 0.19633195 0.2091276][0.0073272479 0.062517531 0.16430703 0.30558169 0.44899508 0.53881985 0.55595154 0.51178581 0.43151742 0.34621519 0.27811587 0.23696142 0.22486329 0.22760078 0.25149864][0.0056949849 0.064002991 0.17030883 0.31563768 0.46069527 0.54927427 0.56335914 0.51423067 0.42845792 0.3409394 0.2742902 0.23477323 0.22686875 0.23988193 0.27612433][0.0011812821 0.060257807 0.16807127 0.31227577 0.45332813 0.53941935 0.55485982 0.51025212 0.42719373 0.34071013 0.27580622 0.23756088 0.23184639 0.25173095 0.29582116][0.0031701357 0.064905427 0.17330073 0.31043351 0.43929222 0.51791996 0.53581095 0.50342864 0.43313089 0.35612777 0.30045217 0.26957169 0.26967525 0.29620719 0.34515452][0.010677064 0.076662555 0.18400599 0.30845931 0.41684109 0.481438 0.49927083 0.48200113 0.43315935 0.37748098 0.34178981 0.32481661 0.33359241 0.36499125 0.41422537][0.017397493 0.087452427 0.19304891 0.30543417 0.39315218 0.44100809 0.45349997 0.4475317 0.42193532 0.39196461 0.37953481 0.37883943 0.39662641 0.43121979 0.47684017][0.020236284 0.0927042 0.19698025 0.3027125 0.3767361 0.4098804 0.41235891 0.41004726 0.40148622 0.39195293 0.39788017 0.40999055 0.43323025 0.46465832 0.49834517][0.019385591 0.091511548 0.19334923 0.29532784 0.36221084 0.38591811 0.37954265 0.3764587 0.376241 0.37743235 0.39319465 0.41151914 0.43434748 0.45595747 0.47200552][0.012355393 0.0772064 0.16740748 0.25698921 0.31308162 0.32767567 0.31433767 0.30862245 0.3118791 0.32103732 0.34701619 0.37395284 0.39899454 0.41355169 0.41501856][-0.004689415 0.043284141 0.10915995 0.17426366 0.21383846 0.22068407 0.20593593 0.20115037 0.20884311 0.22604872 0.26041067 0.29398665 0.31946018 0.32764411 0.31922597][-0.024874086 0.0032243731 0.040478479 0.076944552 0.098279774 0.098319858 0.084650077 0.081302345 0.091387048 0.11185502 0.14635113 0.17912906 0.20133835 0.20571628 0.19562608]]...]
INFO - root - 2017-12-10 14:39:25.003298: step 23910, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 66h:39m:04s remains)
INFO - root - 2017-12-10 14:39:32.700123: step 23920, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 66h:42m:49s remains)
INFO - root - 2017-12-10 14:39:40.528103: step 23930, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 66h:36m:40s remains)
INFO - root - 2017-12-10 14:39:48.249960: step 23940, loss = 0.69, batch loss = 0.63 (10.7 examples/sec; 0.747 sec/batch; 64h:01m:04s remains)
INFO - root - 2017-12-10 14:39:56.083581: step 23950, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 66h:22m:46s remains)
INFO - root - 2017-12-10 14:40:03.751016: step 23960, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 65h:17m:03s remains)
INFO - root - 2017-12-10 14:40:11.585850: step 23970, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.804 sec/batch; 68h:55m:57s remains)
INFO - root - 2017-12-10 14:40:19.437850: step 23980, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 68h:52m:06s remains)
INFO - root - 2017-12-10 14:40:27.319179: step 23990, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 68h:33m:54s remains)
INFO - root - 2017-12-10 14:40:35.073910: step 24000, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 67h:36m:43s remains)
2017-12-10 14:40:35.897510: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19019331 0.18905351 0.1876974 0.1896302 0.19909629 0.21381724 0.23440355 0.2542952 0.27177602 0.27767459 0.25906265 0.21962224 0.16796151 0.10969173 0.052718226][0.20758933 0.20356856 0.19634807 0.18663043 0.18337135 0.18903857 0.20498225 0.22269246 0.23952177 0.24699573 0.23087318 0.1938242 0.14501843 0.090081304 0.036815789][0.21609443 0.21003161 0.1987365 0.18010588 0.16452932 0.1588216 0.16608781 0.17693534 0.18969165 0.19775061 0.18580064 0.15439117 0.11133865 0.062834404 0.015725853][0.21442054 0.2178233 0.21741699 0.20589206 0.18962948 0.1792393 0.18083234 0.18297029 0.18400116 0.18051374 0.16157666 0.12717248 0.083823718 0.038127277 -0.0039657671][0.19908717 0.2240811 0.25002933 0.26154989 0.25807923 0.25372952 0.25887275 0.2578271 0.24555144 0.22281346 0.18780094 0.14171763 0.088400945 0.035912704 -0.0083055766][0.16417597 0.21574812 0.2737163 0.31435078 0.33124191 0.34204933 0.36192837 0.36886054 0.35074684 0.3109737 0.25841555 0.19609828 0.1253493 0.057372298 0.0032137453][0.11863781 0.19055994 0.27184138 0.33429912 0.36912844 0.39801589 0.43998945 0.46365318 0.44894326 0.3997789 0.33458635 0.25743487 0.16818364 0.082987927 0.017482987][0.078539073 0.15303561 0.2367705 0.30296242 0.34424946 0.3849369 0.44416186 0.48144612 0.47185567 0.42031673 0.35238495 0.27172714 0.17568369 0.084155038 0.016132912][0.052727886 0.11214679 0.17892329 0.23233876 0.2679075 0.308575 0.36974466 0.40792349 0.39948812 0.3515051 0.29241705 0.22371154 0.13858783 0.056242831 -0.0032270281][0.02920514 0.066017672 0.10848746 0.14276919 0.16685744 0.19928305 0.25019905 0.28022221 0.27132019 0.23245613 0.19007128 0.14263877 0.078890473 0.014355267 -0.031074487][0.0082906382 0.025430428 0.046816155 0.064348258 0.077434435 0.09984047 0.13834532 0.16040176 0.1532519 0.12503132 0.097238235 0.06699796 0.021602476 -0.026285347 -0.058597192][0.012979685 0.017706107 0.024385678 0.029972641 0.034809131 0.051100649 0.084339492 0.10601813 0.10449664 0.084135666 0.0625389 0.036862448 -0.0028496715 -0.043839641 -0.069847509][0.042812854 0.041925792 0.039926555 0.03819691 0.037748702 0.05291998 0.088889234 0.11735294 0.12352891 0.10780478 0.08709348 0.058187526 0.014991398 -0.028563902 -0.056235079][0.088228516 0.087236337 0.082491808 0.078374244 0.075970873 0.09335383 0.13476221 0.16993654 0.18013309 0.16407251 0.14111042 0.1065701 0.056461204 0.005209845 -0.029037822][0.13094147 0.13230547 0.12802179 0.1241997 0.12161116 0.14089358 0.18425651 0.21989925 0.22774075 0.20784703 0.18262888 0.14506489 0.090311334 0.032633461 -0.007419575]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 14:40:43.589601: step 24010, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 68h:18m:32s remains)
INFO - root - 2017-12-10 14:40:51.421998: step 24020, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 69h:17m:02s remains)
INFO - root - 2017-12-10 14:40:59.280242: step 24030, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 67h:49m:31s remains)
INFO - root - 2017-12-10 14:41:06.874818: step 24040, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 67h:22m:34s remains)
INFO - root - 2017-12-10 14:41:14.736846: step 24050, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 66h:03m:39s remains)
INFO - root - 2017-12-10 14:41:22.539935: step 24060, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 66h:54m:42s remains)
INFO - root - 2017-12-10 14:41:30.416691: step 24070, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 66h:49m:40s remains)
INFO - root - 2017-12-10 14:41:38.258701: step 24080, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 68h:41m:31s remains)
INFO - root - 2017-12-10 14:41:46.025655: step 24090, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 66h:20m:52s remains)
INFO - root - 2017-12-10 14:41:53.754905: step 24100, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 67h:03m:04s remains)
2017-12-10 14:41:54.574248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042645577 -0.035077166 -0.02907761 -0.029301252 -0.030551976 -0.027042916 -0.01775424 -0.0064761653 0.0013757916 0.00027295115 -0.012819172 -0.031619094 -0.043023646 -0.037536357 -0.014673749][-0.050540086 -0.056894388 -0.060367521 -0.060532816 -0.049036443 -0.022734033 0.014338079 0.052470215 0.079989478 0.086209811 0.065608673 0.028012132 -0.0074779466 -0.026496686 -0.024434937][-0.027286153 -0.053873137 -0.0733926 -0.0783672 -0.056411631 -0.0066936165 0.062151462 0.13336809 0.1870648 0.2049794 0.17809974 0.11932664 0.054115452 0.0031566622 -0.022733917][0.025407823 -0.022149026 -0.059988409 -0.0727307 -0.044387378 0.024139097 0.12075476 0.22219199 0.29970491 0.32867795 0.29694888 0.21973081 0.12696423 0.045720261 -0.0072844927][0.10555571 0.042023316 -0.011998635 -0.033894315 -0.0062911496 0.068215691 0.17598875 0.29063064 0.37879941 0.41364533 0.38183776 0.29876584 0.19429889 0.097098343 0.026696648][0.18186 0.11112037 0.047674991 0.017905584 0.038152788 0.10463959 0.20416954 0.31191206 0.39662588 0.43420431 0.41191226 0.34164533 0.24720563 0.15342578 0.078236677][0.21087343 0.14463221 0.083041191 0.050791454 0.061379645 0.11139986 0.19003034 0.27771682 0.35057527 0.390235 0.3853758 0.34221339 0.27500635 0.20081623 0.13269733][0.18043692 0.12702231 0.076645494 0.047782294 0.050155483 0.081991866 0.13617136 0.20005049 0.25836843 0.29880756 0.31173208 0.29780161 0.2622416 0.21405125 0.16088143][0.1158557 0.077259205 0.041502178 0.019434487 0.016852597 0.033439774 0.065375216 0.10618927 0.14822084 0.18464357 0.20774643 0.21455826 0.20417611 0.17948289 0.14349724][0.051061124 0.024968807 0.001526436 -0.014557152 -0.020532195 -0.015940996 -0.002127161 0.018860837 0.045005105 0.0731581 0.097456127 0.11349662 0.11737787 0.10840059 0.086806275][-0.0027684136 -0.018190669 -0.032202955 -0.043353472 -0.050611164 -0.052937422 -0.050159805 -0.041809142 -0.026759591 -0.0064021186 0.014664156 0.031665504 0.039737441 0.0370281 0.023066977][-0.043219507 -0.050111283 -0.056711026 -0.063292786 -0.069386557 -0.073546939 -0.074813679 -0.071801856 -0.062422991 -0.047607761 -0.03115017 -0.01734044 -0.010562265 -0.012611802 -0.023600958][-0.066799872 -0.0689068 -0.071110457 -0.074492067 -0.078393526 -0.081243724 -0.0819081 -0.079266071 -0.071782917 -0.060666211 -0.049242828 -0.040694881 -0.037822794 -0.041544314 -0.05137286][-0.071738638 -0.073181592 -0.074385256 -0.07652878 -0.078199513 -0.07797008 -0.075351715 -0.0703443 -0.062441509 -0.053338062 -0.0462447 -0.043079872 -0.044812143 -0.051367536 -0.061684184][-0.061580792 -0.063798085 -0.065985419 -0.068485968 -0.068638936 -0.065146513 -0.058650393 -0.050601188 -0.041582696 -0.033860072 -0.030723475 -0.032805119 -0.039240226 -0.049100816 -0.060964286]]...]
INFO - root - 2017-12-10 14:42:02.523090: step 24110, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 66h:03m:13s remains)
INFO - root - 2017-12-10 14:42:10.201400: step 24120, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 67h:52m:14s remains)
INFO - root - 2017-12-10 14:42:18.040847: step 24130, loss = 0.67, batch loss = 0.61 (10.2 examples/sec; 0.783 sec/batch; 67h:06m:36s remains)
INFO - root - 2017-12-10 14:42:25.874236: step 24140, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 67h:24m:03s remains)
INFO - root - 2017-12-10 14:42:33.728494: step 24150, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.773 sec/batch; 66h:12m:09s remains)
INFO - root - 2017-12-10 14:42:41.442063: step 24160, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 66h:20m:18s remains)
INFO - root - 2017-12-10 14:42:49.367894: step 24170, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 67h:03m:56s remains)
INFO - root - 2017-12-10 14:42:57.250665: step 24180, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 67h:03m:35s remains)
INFO - root - 2017-12-10 14:43:04.966680: step 24190, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 68h:28m:00s remains)
INFO - root - 2017-12-10 14:43:12.570705: step 24200, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 67h:13m:31s remains)
2017-12-10 14:43:13.401687: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12861589 0.14881426 0.16249694 0.17085065 0.16304508 0.13626552 0.10118282 0.069004357 0.040194508 0.024214773 0.030569449 0.069771275 0.14304042 0.20631495 0.23784214][0.27194965 0.31195945 0.3447406 0.36946359 0.36405677 0.32542315 0.2725929 0.22222002 0.17680475 0.14951123 0.15389816 0.20622297 0.30463377 0.3909305 0.42892885][0.39647567 0.46075314 0.52214181 0.57487947 0.58134693 0.53683168 0.46883178 0.39968765 0.33233908 0.282151 0.27051768 0.32471704 0.43881595 0.54155642 0.58325344][0.46583322 0.55719048 0.65363759 0.74276358 0.77224833 0.73569757 0.6657 0.58542764 0.4936544 0.40716279 0.36032087 0.39561939 0.50649387 0.61359084 0.65770245][0.49057284 0.60864377 0.73872626 0.86063987 0.91574591 0.901535 0.85097396 0.77830118 0.66909522 0.54130489 0.44665468 0.4416925 0.5264703 0.62034243 0.66217434][0.49350631 0.62922657 0.77912372 0.91627604 0.99170256 1.0153477 1.0129648 0.97598064 0.86665851 0.70613664 0.562013 0.49854609 0.53002495 0.58413714 0.60861576][0.50199544 0.63738817 0.78351444 0.91121858 0.99817926 1.070641 1.1374669 1.1560941 1.0623302 0.88578689 0.70260257 0.57361752 0.53054714 0.52103543 0.51305681][0.52632046 0.643703 0.7620483 0.85796404 0.94350278 1.0568489 1.1851197 1.2551044 1.1873184 1.0215673 0.82750422 0.65091258 0.53832668 0.46471578 0.42224398][0.55729395 0.65062332 0.72900397 0.77923012 0.84280545 0.963028 1.1123897 1.2067388 1.1701865 1.0458914 0.87779433 0.68922049 0.53664482 0.42404434 0.36292911][0.5637995 0.63660979 0.67732078 0.68189 0.70700252 0.79556096 0.92054749 1.0105057 1.0071919 0.94337755 0.82594842 0.657317 0.49981564 0.38163584 0.32376668][0.50540841 0.56101978 0.57237136 0.54112148 0.52476096 0.56323987 0.64144349 0.712289 0.73671794 0.72800857 0.66075736 0.5262394 0.39006531 0.29409096 0.25813711][0.3728615 0.41315886 0.40729213 0.35788691 0.31219825 0.3057057 0.33706903 0.38184634 0.41439772 0.43107954 0.39377636 0.29560506 0.1993342 0.14535385 0.14308186][0.18669812 0.21061838 0.19827007 0.14965539 0.095564716 0.063842624 0.061880697 0.080455251 0.10236733 0.11681268 0.092538647 0.03104103 -0.016874325 -0.024462324 0.0052726632][0.0031358185 0.010628527 -0.0023206922 -0.038602095 -0.082902722 -0.11715187 -0.13260955 -0.13120295 -0.12420605 -0.12080394 -0.1374747 -0.16702044 -0.17575 -0.15238312 -0.10679413][-0.12642388 -0.13292015 -0.14517997 -0.16739658 -0.19413951 -0.21616599 -0.22821903 -0.23081672 -0.23125269 -0.23451109 -0.24511178 -0.25477061 -0.24475023 -0.21337928 -0.17091194]]...]
INFO - root - 2017-12-10 14:43:21.360356: step 24210, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 66h:23m:54s remains)
INFO - root - 2017-12-10 14:43:29.180347: step 24220, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 65h:09m:21s remains)
INFO - root - 2017-12-10 14:43:37.115066: step 24230, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 67h:40m:38s remains)
INFO - root - 2017-12-10 14:43:45.000775: step 24240, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 67h:13m:16s remains)
INFO - root - 2017-12-10 14:43:52.908922: step 24250, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 67h:51m:03s remains)
INFO - root - 2017-12-10 14:44:00.648928: step 24260, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 66h:05m:26s remains)
INFO - root - 2017-12-10 14:44:08.519851: step 24270, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 68h:43m:46s remains)
INFO - root - 2017-12-10 14:44:16.167998: step 24280, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 68h:14m:14s remains)
INFO - root - 2017-12-10 14:44:24.137327: step 24290, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 67h:01m:08s remains)
INFO - root - 2017-12-10 14:44:31.952826: step 24300, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 64h:50m:52s remains)
2017-12-10 14:44:32.905864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00096697238 0.051662855 0.13294831 0.22702634 0.30701163 0.35860708 0.38607678 0.38709164 0.36421525 0.33368489 0.31760293 0.31606239 0.31187043 0.30584845 0.31124458][0.00024579623 0.0571181 0.14363486 0.24210551 0.3263942 0.379875 0.40498522 0.40047157 0.37261996 0.33965504 0.32122567 0.31842372 0.31588832 0.31435481 0.32503235][-0.0017682115 0.05381668 0.13695574 0.23105912 0.31305465 0.36350822 0.38250521 0.37183905 0.343285 0.31489238 0.30045816 0.30041873 0.30277491 0.30706832 0.32100022][-0.0017193375 0.052749962 0.13371287 0.22562477 0.30733341 0.35525459 0.36786434 0.35140449 0.32318869 0.30088073 0.29233137 0.2977953 0.30822775 0.32039857 0.3370105][0.002006958 0.059270788 0.14537342 0.24430272 0.33316335 0.38287249 0.39145771 0.37116736 0.34295344 0.3244178 0.32015347 0.33145612 0.35101411 0.37216112 0.39218768][0.0097497255 0.075386219 0.17542458 0.29168287 0.39633834 0.45473748 0.4647027 0.44470039 0.41644391 0.39761588 0.39251289 0.40509346 0.42993391 0.4578934 0.48117959][0.021264039 0.10000361 0.22001792 0.35920447 0.48488006 0.55871093 0.57708538 0.56231362 0.53380853 0.5085687 0.49261281 0.49547055 0.51578969 0.5438174 0.56741178][0.032191318 0.12378094 0.26221281 0.42136803 0.56594193 0.65630251 0.68591446 0.67773026 0.64697111 0.60933548 0.57467264 0.55874163 0.56512904 0.58428627 0.60213512][0.037212022 0.13500717 0.28144783 0.44840717 0.60110044 0.70135587 0.7395674 0.73611587 0.70157975 0.65062445 0.59661549 0.55960548 0.54656631 0.54893214 0.55350733][0.035224419 0.12866351 0.26653206 0.42172119 0.56397319 0.660074 0.69971561 0.69857997 0.66250879 0.60481352 0.53935319 0.48703796 0.45567185 0.43842569 0.42549577][0.025661798 0.10417217 0.21706468 0.34126475 0.45426682 0.53040934 0.5618127 0.55960804 0.52578151 0.47104824 0.40676016 0.3506816 0.30952126 0.27841228 0.2520656][0.0082217185 0.066796683 0.14752989 0.23250411 0.30732864 0.35463929 0.37101549 0.36433479 0.33429939 0.28856757 0.23481794 0.18560027 0.14528874 0.11099005 0.08125928][-0.012319058 0.028099021 0.079899751 0.12928003 0.16813074 0.18642662 0.18597175 0.17299354 0.14693378 0.11299548 0.07504271 0.040085778 0.0093566785 -0.018493924 -0.0423358][-0.028430091 0.00015333177 0.032931224 0.058694795 0.073293738 0.07149303 0.058769237 0.041058961 0.019036641 -0.0041541024 -0.027807591 -0.04882272 -0.0679006 -0.085612558 -0.10002944][-0.036543135 -0.012410294 0.012232612 0.027610997 0.031752951 0.0214839 0.0031904967 -0.016729303 -0.035932917 -0.052891009 -0.068673559 -0.081759751 -0.092838943 -0.10232263 -0.10869449]]...]
INFO - root - 2017-12-10 14:44:40.826426: step 24310, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 69h:22m:47s remains)
INFO - root - 2017-12-10 14:44:48.875539: step 24320, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 68h:59m:37s remains)
INFO - root - 2017-12-10 14:44:56.699578: step 24330, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 66h:11m:17s remains)
INFO - root - 2017-12-10 14:45:04.667581: step 24340, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 68h:22m:43s remains)
INFO - root - 2017-12-10 14:45:12.669413: step 24350, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 67h:27m:12s remains)
INFO - root - 2017-12-10 14:45:20.397632: step 24360, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 66h:33m:40s remains)
INFO - root - 2017-12-10 14:45:28.126037: step 24370, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 67h:49m:12s remains)
INFO - root - 2017-12-10 14:45:36.029208: step 24380, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 66h:02m:56s remains)
INFO - root - 2017-12-10 14:45:43.978575: step 24390, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 66h:44m:15s remains)
INFO - root - 2017-12-10 14:45:51.990030: step 24400, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.788 sec/batch; 67h:27m:51s remains)
2017-12-10 14:45:52.855164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.012712192 -0.023658581 -0.03492678 -0.043279506 -0.047489982 -0.04466369 -0.035788991 -0.027955636 -0.026798755 -0.028769223 -0.028989976 -0.02736406 -0.024299825 -0.021393823 -0.019291461][0.046821631 0.039439164 0.027146062 0.01515752 0.0065486976 0.0049461396 0.010508565 0.016349018 0.015843242 0.013498203 0.014527426 0.018674687 0.024393674 0.029596174 0.035089556][0.12497982 0.12661766 0.11793614 0.10645399 0.097158648 0.092513241 0.092891529 0.092939757 0.086810306 0.080569051 0.079569936 0.083827808 0.090707518 0.097227514 0.10577987][0.20187138 0.21900788 0.2219127 0.21962617 0.217986 0.21624076 0.21252246 0.20305319 0.18501709 0.16749841 0.1563575 0.15329567 0.15579052 0.16044563 0.17025855][0.26270345 0.30031106 0.32354546 0.34050593 0.35601053 0.36470127 0.36006892 0.33866587 0.30228481 0.26464865 0.23323536 0.21320893 0.20383237 0.2027794 0.21170971][0.30039102 0.36067027 0.4092012 0.4512217 0.48893118 0.51210719 0.50892985 0.47538027 0.41805115 0.3554844 0.29759583 0.25428352 0.22821662 0.21929453 0.22587989][0.32366616 0.40111563 0.46831626 0.52768779 0.57946968 0.61152041 0.60892028 0.56658345 0.49406821 0.41267416 0.33419949 0.27253267 0.23377295 0.22043635 0.22628787][0.34137338 0.42393208 0.49386621 0.55237907 0.60058033 0.62873328 0.62288582 0.5775243 0.50174433 0.41631213 0.33302096 0.26748121 0.22755061 0.21710961 0.22586253][0.35782337 0.43182838 0.485727 0.52252376 0.54754615 0.55778414 0.54445064 0.50276351 0.43814209 0.36729622 0.29951546 0.24852431 0.22126189 0.22134764 0.236001][0.36182323 0.41574579 0.44034967 0.44246426 0.43332341 0.41799513 0.395124 0.36193392 0.3187969 0.2760857 0.2383963 0.215547 0.2109112 0.2257921 0.24755028][0.33120662 0.36124796 0.3563008 0.32668984 0.28797469 0.2514779 0.22231548 0.20005859 0.1805615 0.16802092 0.16169773 0.16682419 0.1834287 0.21027891 0.2367263][0.25171718 0.26140252 0.23897032 0.19469137 0.14397527 0.1004681 0.07271859 0.061309334 0.059682257 0.068502247 0.08318 0.10457358 0.13073553 0.16000509 0.18486288][0.12985906 0.12643912 0.10238488 0.064625695 0.023654392 -0.0097814873 -0.027819192 -0.030349504 -0.0241036 -0.0084621739 0.011159274 0.033104897 0.054950062 0.076042868 0.092699155][0.0023224126 -0.0077370191 -0.023990212 -0.044092957 -0.063887291 -0.078035794 -0.082360059 -0.078608178 -0.071910158 -0.060005132 -0.046626352 -0.034064103 -0.023959957 -0.015456649 -0.0095053][-0.094671294 -0.10651179 -0.11333222 -0.1169061 -0.11780988 -0.11520675 -0.10883413 -0.10217629 -0.098504096 -0.0941955 -0.090376928 -0.088742532 -0.089577883 -0.091150671 -0.093202673]]...]
INFO - root - 2017-12-10 14:46:00.828764: step 24410, loss = 0.71, batch loss = 0.65 (9.6 examples/sec; 0.830 sec/batch; 71h:04m:28s remains)
INFO - root - 2017-12-10 14:46:08.803166: step 24420, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 67h:01m:09s remains)
INFO - root - 2017-12-10 14:46:16.817437: step 24430, loss = 0.72, batch loss = 0.66 (9.7 examples/sec; 0.826 sec/batch; 70h:40m:31s remains)
INFO - root - 2017-12-10 14:46:24.662262: step 24440, loss = 0.68, batch loss = 0.62 (8.8 examples/sec; 0.913 sec/batch; 78h:07m:10s remains)
INFO - root - 2017-12-10 14:46:32.561902: step 24450, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.759 sec/batch; 64h:54m:48s remains)
INFO - root - 2017-12-10 14:46:40.351106: step 24460, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 66h:47m:20s remains)
INFO - root - 2017-12-10 14:46:48.244855: step 24470, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 66h:48m:55s remains)
INFO - root - 2017-12-10 14:46:56.189437: step 24480, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 66h:23m:24s remains)
INFO - root - 2017-12-10 14:47:03.958804: step 24490, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 66h:28m:58s remains)
INFO - root - 2017-12-10 14:47:11.921101: step 24500, loss = 0.70, batch loss = 0.65 (9.8 examples/sec; 0.814 sec/batch; 69h:39m:24s remains)
2017-12-10 14:47:12.766038: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13932666 0.12800394 0.10108329 0.071385518 0.050650761 0.045357026 0.04843596 0.050992828 0.0506254 0.045920156 0.038628444 0.031021958 0.02472998 0.019726483 0.015268479][0.22129911 0.19899072 0.15482233 0.10814019 0.075025856 0.062313616 0.062380083 0.064561054 0.067006551 0.067800269 0.0669734 0.064635657 0.060963098 0.055825382 0.049092855][0.28014761 0.24899906 0.19387953 0.13851276 0.10038508 0.084550522 0.083511263 0.086063586 0.09047658 0.094148733 0.095680341 0.094603285 0.091545932 0.086691789 0.079667784][0.30502304 0.2737408 0.22012831 0.16833538 0.1337519 0.11892812 0.11756787 0.12007915 0.12468982 0.1282313 0.12843095 0.12531281 0.12105568 0.11614316 0.1096895][0.2959815 0.27498475 0.23566067 0.1991533 0.176883 0.16855477 0.16917725 0.17204158 0.17510276 0.17516094 0.1703961 0.16215166 0.15458527 0.14851576 0.14190178][0.26672465 0.26445606 0.24890724 0.2357415 0.23105659 0.2317683 0.2336608 0.23380479 0.23124528 0.22411846 0.21241663 0.19867221 0.18814625 0.18128186 0.1744734][0.23981825 0.25782308 0.26682338 0.27673563 0.28773963 0.29481447 0.29375505 0.28578529 0.27296156 0.25637093 0.23848973 0.22224036 0.21206246 0.20675389 0.20106335][0.22611503 0.25916851 0.28611952 0.31148162 0.33029464 0.33710295 0.32755807 0.30710512 0.28210768 0.25697377 0.23612881 0.22135174 0.21453445 0.21226206 0.20851731][0.22280771 0.26493791 0.30105153 0.33208051 0.34978387 0.34964937 0.32813156 0.29460484 0.25965786 0.22974437 0.20932131 0.19804838 0.19508408 0.19497445 0.19187149][0.21330464 0.2567668 0.29412019 0.32359487 0.33571169 0.32689548 0.2951017 0.252125 0.21120036 0.1791573 0.15949477 0.15064418 0.15012079 0.15141194 0.14879622][0.18191478 0.21974912 0.2531718 0.27805105 0.28470826 0.27008614 0.23359631 0.18762599 0.14548902 0.11345909 0.094157182 0.086325273 0.087645285 0.091284461 0.091226794][0.12547773 0.15399042 0.18140098 0.20188628 0.20632838 0.19165303 0.15783708 0.11622887 0.078607932 0.050164692 0.032693624 0.025846623 0.028907945 0.035651658 0.039803069][0.056173157 0.074947543 0.096587889 0.11474004 0.12154702 0.11347041 0.089788154 0.059360813 0.031580772 0.01071405 -0.0023275539 -0.0072273752 -0.0030445254 0.0052565616 0.011765599][-0.00074260618 0.011004524 0.028728211 0.047002185 0.059235405 0.060983904 0.050129063 0.032590285 0.015480478 0.002787048 -0.0051003392 -0.0076396069 -0.0032022288 0.0040317317 0.0088844094][-0.028903965 -0.021653771 -0.0066518397 0.012223125 0.02895375 0.038505457 0.037606437 0.029824663 0.020660799 0.014703581 0.012470481 0.013737856 0.019331586 0.024898455 0.025869424]]...]
INFO - root - 2017-12-10 14:47:20.558676: step 24510, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 67h:19m:13s remains)
INFO - root - 2017-12-10 14:47:28.296057: step 24520, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 65h:14m:08s remains)
INFO - root - 2017-12-10 14:47:36.215786: step 24530, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.810 sec/batch; 69h:17m:28s remains)
INFO - root - 2017-12-10 14:47:44.035995: step 24540, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 66h:45m:23s remains)
INFO - root - 2017-12-10 14:47:51.870106: step 24550, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 67h:23m:35s remains)
INFO - root - 2017-12-10 14:47:59.633044: step 24560, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 66h:28m:21s remains)
INFO - root - 2017-12-10 14:48:07.464944: step 24570, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.773 sec/batch; 66h:06m:20s remains)
INFO - root - 2017-12-10 14:48:15.169808: step 24580, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 65h:28m:10s remains)
INFO - root - 2017-12-10 14:48:23.062901: step 24590, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 67h:41m:13s remains)
INFO - root - 2017-12-10 14:48:30.798270: step 24600, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 67h:48m:41s remains)
2017-12-10 14:48:31.596303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.06083864 -0.037024386 0.00040028288 0.040325057 0.078328446 0.10762429 0.11986078 0.1109243 0.083037861 0.045358248 0.0083990078 -0.019135688 -0.029990327 -0.022663163 0.001228653][-0.04605151 -0.0042112316 0.060653348 0.13380522 0.20733999 0.2688466 0.30408835 0.29981074 0.25650519 0.19093028 0.12310578 0.068243288 0.03934934 0.0429974 0.076039374][-0.028354524 0.032948423 0.12759227 0.23865584 0.35408053 0.45413685 0.51880437 0.52458262 0.46938479 0.37716785 0.27899846 0.19832984 0.14984481 0.14560518 0.18100719][-0.013365106 0.063475467 0.18281692 0.32662803 0.47682393 0.60781121 0.69756985 0.7164467 0.65888709 0.5544104 0.4433713 0.35268149 0.29294437 0.27822304 0.30385658][-0.003507599 0.083811231 0.22100848 0.38799942 0.55822241 0.70443839 0.80838871 0.84078294 0.79267913 0.69669044 0.59871137 0.52135718 0.46603474 0.44271046 0.44810548][0.0010075073 0.093476921 0.24148722 0.42279583 0.60272151 0.754501 0.86563361 0.91141903 0.88072485 0.80721927 0.73823673 0.68929935 0.65232891 0.62805104 0.61211014][0.0019355165 0.0939672 0.24459398 0.42983314 0.61098927 0.76168227 0.87327909 0.92747265 0.91250771 0.86271167 0.8242138 0.8074761 0.79853225 0.78455055 0.75696832][0.0021816713 0.088911667 0.23224515 0.40831953 0.57868314 0.71584952 0.81452167 0.86450523 0.8562277 0.82341009 0.80943573 0.82401454 0.85015458 0.859789 0.83876914][0.0018037721 0.08028359 0.20807917 0.36325905 0.51074475 0.62294894 0.69826835 0.73409861 0.72438419 0.70065922 0.701606 0.73701954 0.78985816 0.82323867 0.82107276][-0.0011673127 0.06772808 0.17535327 0.30259588 0.42032847 0.50357419 0.55425614 0.57424158 0.56004751 0.53892773 0.5423218 0.57921153 0.63606328 0.67804605 0.69281429][-0.010318436 0.047878053 0.13385394 0.23116 0.31777331 0.37346005 0.40237385 0.40867841 0.39066318 0.36948094 0.36722791 0.39139968 0.43415445 0.47055402 0.49481288][-0.025868226 0.02029963 0.085558921 0.15500468 0.21224493 0.24217002 0.25035053 0.24322855 0.22159159 0.20010099 0.19180965 0.20209497 0.22708096 0.25220078 0.27749571][-0.043580066 -0.0096198358 0.037143257 0.083201 0.11618181 0.12501164 0.11546694 0.096199043 0.071086615 0.049899515 0.039236359 0.041935511 0.054792941 0.069675475 0.089139476][-0.059348509 -0.036490839 -0.0045772861 0.024508519 0.041648675 0.03851562 0.019643964 -0.0056423876 -0.031280275 -0.050326537 -0.059728257 -0.059278026 -0.053358022 -0.04751486 -0.039186276][-0.073637754 -0.06099119 -0.040704332 -0.022654748 -0.013179298 -0.01867548 -0.036822304 -0.059756771 -0.081215419 -0.09597829 -0.10258662 -0.10210999 -0.099710695 -0.10028706 -0.1025403]]...]
INFO - root - 2017-12-10 14:48:39.453202: step 24610, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 66h:01m:36s remains)
INFO - root - 2017-12-10 14:48:47.373386: step 24620, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 67h:15m:34s remains)
INFO - root - 2017-12-10 14:48:55.111963: step 24630, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 66h:55m:19s remains)
INFO - root - 2017-12-10 14:49:03.047337: step 24640, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 65h:14m:15s remains)
INFO - root - 2017-12-10 14:49:10.910589: step 24650, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 67h:43m:58s remains)
INFO - root - 2017-12-10 14:49:18.745899: step 24660, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 68h:19m:07s remains)
INFO - root - 2017-12-10 14:49:26.599650: step 24670, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 67h:44m:08s remains)
INFO - root - 2017-12-10 14:49:34.261564: step 24680, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 66h:25m:44s remains)
INFO - root - 2017-12-10 14:49:42.103388: step 24690, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 67h:38m:00s remains)
INFO - root - 2017-12-10 14:49:50.040259: step 24700, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 69h:05m:08s remains)
2017-12-10 14:49:50.813829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.026583746 -0.031263325 -0.033731747 -0.033969216 -0.033493187 -0.03190656 -0.027994795 -0.023822943 -0.021881565 -0.023826661 -0.029123582 -0.036303315 -0.043616187 -0.048055943 -0.04827626][-0.041571073 -0.044665761 -0.044723012 -0.04258335 -0.039626528 -0.033683456 -0.023533529 -0.013000042 -0.0064850715 -0.0059215445 -0.011201814 -0.021262601 -0.033043481 -0.040512491 -0.040169757][-0.036861643 -0.032956574 -0.024699414 -0.014862961 -0.004598157 0.01191044 0.035537913 0.057738483 0.070513465 0.0728935 0.064840265 0.046839379 0.023948602 0.0080553666 0.0060415282][-0.011634274 0.0037113267 0.024961583 0.046743777 0.068868883 0.10278281 0.14725363 0.18530111 0.20355463 0.20487516 0.19009596 0.15864649 0.1181504 0.088338077 0.08014676][0.021416932 0.051733077 0.091222353 0.1309254 0.17175776 0.23005797 0.3008357 0.3556059 0.37532783 0.37094772 0.34613565 0.29903424 0.23887882 0.19328159 0.17699765][0.054004449 0.10257421 0.16513547 0.22766712 0.29079166 0.37383959 0.46764994 0.53249788 0.54541349 0.52800077 0.48953578 0.4271799 0.35069972 0.29352352 0.27236968][0.082182772 0.14824677 0.23218971 0.31386763 0.392361 0.48785794 0.58911026 0.64990979 0.64742076 0.61309159 0.56181848 0.4901011 0.40680826 0.34681588 0.32550821][0.098264225 0.17473295 0.26922265 0.35760322 0.43684906 0.52556568 0.61357993 0.65616041 0.63475472 0.58555919 0.52686459 0.45471558 0.37602517 0.32207879 0.30453017][0.0881717 0.16216457 0.25124732 0.33071968 0.39618388 0.46293074 0.52402031 0.54277295 0.50729764 0.45250237 0.39642256 0.33456752 0.27148268 0.23149684 0.22152679][0.050330043 0.11028887 0.1821425 0.24410641 0.29085371 0.332972 0.36681002 0.36654669 0.32677335 0.27614537 0.23002033 0.18456474 0.1425202 0.1201527 0.11981638][0.0064551453 0.047297366 0.096344069 0.1370153 0.16392066 0.18333437 0.19430721 0.18211518 0.14637618 0.10720252 0.075425096 0.048145488 0.026809812 0.020503702 0.027934849][-0.029226532 -0.0069973157 0.019501686 0.039886154 0.049954843 0.052707322 0.048838489 0.032573853 0.0065826075 -0.017267613 -0.03344208 -0.044479374 -0.05002068 -0.046211611 -0.035989933][-0.053960167 -0.045620248 -0.035208035 -0.028210217 -0.027177118 -0.03122576 -0.039541695 -0.052854143 -0.067613527 -0.0777832 -0.081707746 -0.081724122 -0.078223243 -0.07033693 -0.061007269][-0.064109229 -0.062499523 -0.058983713 -0.056617673 -0.056883063 -0.060127206 -0.065922968 -0.073339172 -0.079297528 -0.080466732 -0.077049688 -0.07107503 -0.0636268 -0.0552964 -0.04851044][-0.05653698 -0.054017317 -0.049492136 -0.045545381 -0.043608736 -0.044676203 -0.048261046 -0.05266859 -0.054990903 -0.052846834 -0.047071505 -0.039450206 -0.03168935 -0.02503424 -0.021448331]]...]
INFO - root - 2017-12-10 14:49:58.599563: step 24710, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 66h:11m:47s remains)
INFO - root - 2017-12-10 14:50:06.288190: step 24720, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 65h:02m:39s remains)
INFO - root - 2017-12-10 14:50:14.069929: step 24730, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.755 sec/batch; 64h:33m:44s remains)
INFO - root - 2017-12-10 14:50:21.895954: step 24740, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 67h:24m:36s remains)
INFO - root - 2017-12-10 14:50:29.800331: step 24750, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.801 sec/batch; 68h:27m:09s remains)
INFO - root - 2017-12-10 14:50:37.426770: step 24760, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.767 sec/batch; 65h:32m:24s remains)
INFO - root - 2017-12-10 14:50:45.251245: step 24770, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 67h:58m:02s remains)
INFO - root - 2017-12-10 14:50:53.198706: step 24780, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 68h:51m:14s remains)
INFO - root - 2017-12-10 14:51:01.154210: step 24790, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.824 sec/batch; 70h:25m:09s remains)
INFO - root - 2017-12-10 14:51:09.050917: step 24800, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 66h:52m:32s remains)
2017-12-10 14:51:09.883774: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12914126 0.13997497 0.15209278 0.15854798 0.15585099 0.15791331 0.16546558 0.17163341 0.17092374 0.1695817 0.17599112 0.18209568 0.18117253 0.17114757 0.15532576][0.16097862 0.17878062 0.19714347 0.20749114 0.20463544 0.20408958 0.20580354 0.20402342 0.19377698 0.18127033 0.17407221 0.16409412 0.14678998 0.1231159 0.098170705][0.20660062 0.22692643 0.24129237 0.24323292 0.23058167 0.22054017 0.2138698 0.20563133 0.19046138 0.17226636 0.15628603 0.13619067 0.1111851 0.084994614 0.062582344][0.25320479 0.27278352 0.27804843 0.26812342 0.24725665 0.2324497 0.22489044 0.21823505 0.20604974 0.18918055 0.16965964 0.14477956 0.11859886 0.097753823 0.085017189][0.28917703 0.30788258 0.3075954 0.29364869 0.27647683 0.26971456 0.27364552 0.27812532 0.27598524 0.26626158 0.24727225 0.22034524 0.19381282 0.1775727 0.17209148][0.3158963 0.3392913 0.34366551 0.33775783 0.33545235 0.34547338 0.36740857 0.3871178 0.39788762 0.39773884 0.38055235 0.35034719 0.31832096 0.29826522 0.29058328][0.3415699 0.37680405 0.39490724 0.40395409 0.41760403 0.44086063 0.47482222 0.503733 0.52212197 0.52726221 0.50916809 0.47229946 0.42903426 0.39696339 0.37888056][0.35679486 0.40857661 0.44449872 0.46844339 0.49079886 0.51575857 0.54790735 0.57392788 0.58937657 0.59063965 0.56578988 0.51822317 0.46045157 0.41249341 0.38119847][0.33864847 0.40654138 0.45939365 0.49509475 0.51879412 0.53523135 0.5534851 0.56573969 0.56816286 0.55623 0.51934314 0.46019673 0.39076325 0.33089718 0.29043549][0.28916615 0.36438584 0.42672622 0.46877643 0.48987558 0.49454 0.49513763 0.49007758 0.47638381 0.44903103 0.4010464 0.33608717 0.26549095 0.20600468 0.1670571][0.2386229 0.3092114 0.37292939 0.41946766 0.44220334 0.4422394 0.43061885 0.41035193 0.3812519 0.33978164 0.2844305 0.22212678 0.16328022 0.11858986 0.092524759][0.22247463 0.27750155 0.33719951 0.39009047 0.42356557 0.43139985 0.41796413 0.38726604 0.34433427 0.29021049 0.23086736 0.17850001 0.14143078 0.12159374 0.11510973][0.25028154 0.28461233 0.33835098 0.40009379 0.45088723 0.4748624 0.46747792 0.43143371 0.37853298 0.31581068 0.2569117 0.21935828 0.20828393 0.21608986 0.22983894][0.2966828 0.31518832 0.36389619 0.43343639 0.49896139 0.53620231 0.53453571 0.4963803 0.43984595 0.37619528 0.32372952 0.30249006 0.31379035 0.34252748 0.37003455][0.31497097 0.325463 0.3692649 0.43971419 0.508726 0.54765648 0.54544419 0.50706375 0.45440233 0.39911836 0.35935467 0.35300207 0.37750402 0.4144586 0.44465935]]...]
INFO - root - 2017-12-10 14:51:17.534266: step 24810, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 67h:04m:59s remains)
INFO - root - 2017-12-10 14:51:25.403255: step 24820, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 68h:01m:03s remains)
INFO - root - 2017-12-10 14:51:33.358701: step 24830, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 68h:36m:07s remains)
INFO - root - 2017-12-10 14:51:41.117546: step 24840, loss = 0.67, batch loss = 0.61 (10.2 examples/sec; 0.786 sec/batch; 67h:12m:26s remains)
INFO - root - 2017-12-10 14:51:48.955944: step 24850, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 65h:43m:40s remains)
INFO - root - 2017-12-10 14:51:56.891830: step 24860, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.766 sec/batch; 65h:25m:09s remains)
INFO - root - 2017-12-10 14:52:04.733577: step 24870, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 65h:41m:43s remains)
INFO - root - 2017-12-10 14:52:12.552575: step 24880, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 66h:53m:07s remains)
INFO - root - 2017-12-10 14:52:20.342436: step 24890, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 66h:51m:42s remains)
INFO - root - 2017-12-10 14:52:28.025038: step 24900, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.816 sec/batch; 69h:44m:40s remains)
2017-12-10 14:52:28.806891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0020456086 0.026861181 0.063257918 0.089445621 0.097112812 0.091807283 0.08609432 0.083629318 0.087897666 0.10244603 0.12918372 0.15899797 0.18063371 0.18903923 0.179115][-0.014506895 0.01017289 0.043633174 0.069421276 0.078479685 0.075143479 0.070387162 0.069384925 0.072875641 0.080202505 0.091293156 0.10270823 0.10797211 0.10187533 0.082182422][-0.022049665 0.0026661484 0.038958367 0.071215354 0.087821722 0.090767846 0.089123681 0.087936953 0.084730789 0.076552644 0.066465639 0.058133494 0.048650879 0.030791448 0.0049026338][-0.015334849 0.015795281 0.063573249 0.11176063 0.14472196 0.16111873 0.16734788 0.16608879 0.15117283 0.12120496 0.087151378 0.059541058 0.035177216 0.0035157853 -0.031399086][0.01001556 0.052131854 0.11700729 0.18737258 0.24305792 0.27874625 0.29644313 0.29449224 0.26466787 0.21146449 0.15670554 0.11508831 0.078729935 0.032842476 -0.014469407][0.050573718 0.10281016 0.18413372 0.27726442 0.35827371 0.41699374 0.44803768 0.44519463 0.40123528 0.32892939 0.2603218 0.21091713 0.16598237 0.10551959 0.041655954][0.089031436 0.14264973 0.22891755 0.33343649 0.43115065 0.5078389 0.55005926 0.54795575 0.49668357 0.41634503 0.34465706 0.29447842 0.24503367 0.17312834 0.094686642][0.10254793 0.14587504 0.22099152 0.31811991 0.41461614 0.49475223 0.5402422 0.53951705 0.49052176 0.41683009 0.35456643 0.31161049 0.26453346 0.1906071 0.10776611][0.082718946 0.10919122 0.16253327 0.23715568 0.31490475 0.38128147 0.41870326 0.41688457 0.37598017 0.31873432 0.27385071 0.24394937 0.20683686 0.14363132 0.071053945][0.037695635 0.046705119 0.075467817 0.12108751 0.17061986 0.21281986 0.23485957 0.22977364 0.19898808 0.16115174 0.13545349 0.12059224 0.099024966 0.057434831 0.0084804846][-0.0089423964 -0.013477151 -0.0050252182 0.014188428 0.03641399 0.054801695 0.062555164 0.056291111 0.037129167 0.017332874 0.0075691747 0.0049284375 -0.002021142 -0.021362165 -0.045139458][-0.035580896 -0.045895889 -0.04959264 -0.049041964 -0.047353007 -0.046691753 -0.048627984 -0.053944439 -0.062836759 -0.069534689 -0.069673881 -0.065986909 -0.064275235 -0.068582639 -0.07475429][-0.036957994 -0.047029674 -0.0554724 -0.065595016 -0.076604709 -0.086667649 -0.093870543 -0.09749604 -0.098943256 -0.097701393 -0.0937601 -0.089047767 -0.085330769 -0.083934352 -0.082919843][-0.018934816 -0.023670256 -0.031337507 -0.046059672 -0.063889734 -0.079798855 -0.0898674 -0.092528738 -0.0896742 -0.084398 -0.079961859 -0.077822633 -0.077276595 -0.0777843 -0.077841945][0.012660876 0.017082496 0.014394118 -0.00031857111 -0.021023083 -0.0400978 -0.05182064 -0.053628597 -0.047517754 -0.039246745 -0.035321549 -0.037379473 -0.043149278 -0.050418694 -0.056653243]]...]
INFO - root - 2017-12-10 14:52:36.552809: step 24910, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 67h:22m:05s remains)
INFO - root - 2017-12-10 14:52:44.249949: step 24920, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 66h:32m:33s remains)
INFO - root - 2017-12-10 14:52:52.162716: step 24930, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 68h:59m:46s remains)
INFO - root - 2017-12-10 14:53:00.047391: step 24940, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 65h:48m:35s remains)
INFO - root - 2017-12-10 14:53:08.030393: step 24950, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 65h:59m:06s remains)
INFO - root - 2017-12-10 14:53:15.835733: step 24960, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 67h:00m:41s remains)
INFO - root - 2017-12-10 14:53:23.807301: step 24970, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 66h:22m:56s remains)
INFO - root - 2017-12-10 14:53:31.699220: step 24980, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.773 sec/batch; 65h:59m:49s remains)
INFO - root - 2017-12-10 14:53:39.402601: step 24990, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 68h:01m:25s remains)
INFO - root - 2017-12-10 14:53:47.156918: step 25000, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 67h:24m:18s remains)
2017-12-10 14:53:47.980794: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0071348385 -0.045220532 -0.070358463 -0.0724057 -0.066611409 -0.043656383 0.0026246121 0.0628255 0.11304437 0.14533348 0.15557496 0.13995968 0.096828826 0.028853813 -0.04111857][-0.022667479 -0.039423387 -0.035659421 -0.022714743 -0.017726388 -0.0058692824 0.023144467 0.064258285 0.098675266 0.12305895 0.13366883 0.12103164 0.081425726 0.016081272 -0.050286163][-0.019514693 0.00654666 0.047777407 0.081371807 0.088740133 0.088651553 0.09423238 0.10587963 0.11101831 0.11394221 0.11396143 0.097500943 0.059330333 -0.00012015915 -0.054870546][0.022443522 0.087570272 0.1653088 0.22103968 0.23458961 0.22502694 0.20680803 0.18574984 0.15502219 0.12824059 0.11026755 0.0854879 0.047421575 -0.0037957688 -0.042108271][0.10244382 0.18834391 0.28838247 0.36182424 0.38623989 0.37665454 0.34714952 0.30618855 0.24774548 0.19300449 0.15332417 0.11554375 0.072420284 0.025205873 0.00074170687][0.20058228 0.28539327 0.38571259 0.46807453 0.50947607 0.51566106 0.49790323 0.46280286 0.39777818 0.32570437 0.26341227 0.20655032 0.15058611 0.10021031 0.083301209][0.29177576 0.35678655 0.4367705 0.51720512 0.57731241 0.60982162 0.62009794 0.60936576 0.55503577 0.4768182 0.39499977 0.3184092 0.24932455 0.19621012 0.18559857][0.35080531 0.38915578 0.44244596 0.51605546 0.59244984 0.65034473 0.68810529 0.70261794 0.66561073 0.5913257 0.50082529 0.41527969 0.34417364 0.29523835 0.29004976][0.35417616 0.37634981 0.40934682 0.47302654 0.5531348 0.619512 0.66698682 0.69366163 0.67313135 0.61338562 0.53210783 0.45645022 0.39945993 0.36338595 0.3630504][0.29706228 0.31270623 0.33170459 0.37991282 0.44736025 0.50271827 0.54179519 0.56800485 0.56136221 0.52319556 0.4644354 0.41203475 0.37879291 0.36138856 0.36830744][0.18439434 0.19941804 0.21028084 0.2403641 0.28589991 0.32179621 0.34679249 0.36809024 0.37222922 0.35505643 0.32045519 0.29153347 0.28003618 0.28124264 0.29868519][0.056187082 0.069271564 0.07389947 0.085981086 0.10858282 0.12612036 0.13874705 0.15327974 0.1613805 0.156451 0.13793464 0.12444603 0.1277142 0.14504595 0.17552802][-0.044184197 -0.034799915 -0.033196095 -0.033481691 -0.027685521 -0.022307148 -0.017438738 -0.0094816824 -0.0034289458 -0.0050708526 -0.015400789 -0.019465908 -0.0063723014 0.022418076 0.06203625][-0.10069247 -0.094064929 -0.091523021 -0.096499711 -0.099742025 -0.10108899 -0.10087441 -0.098246038 -0.096169554 -0.09826047 -0.10412151 -0.10208312 -0.083714306 -0.050571006 -0.009789479][-0.11893835 -0.11584615 -0.11213684 -0.11762161 -0.12434535 -0.12863873 -0.13091916 -0.13165751 -0.13230778 -0.1341223 -0.13540556 -0.12778459 -0.10641263 -0.07395032 -0.038066577]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 14:53:55.937713: step 25010, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 67h:57m:07s remains)
INFO - root - 2017-12-10 14:54:03.800107: step 25020, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 68h:13m:16s remains)
INFO - root - 2017-12-10 14:54:11.558637: step 25030, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.764 sec/batch; 65h:16m:39s remains)
INFO - root - 2017-12-10 14:54:19.397087: step 25040, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 65h:11m:59s remains)
INFO - root - 2017-12-10 14:54:27.258106: step 25050, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 66h:27m:16s remains)
INFO - root - 2017-12-10 14:54:35.219184: step 25060, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.797 sec/batch; 68h:05m:48s remains)
INFO - root - 2017-12-10 14:54:43.111839: step 25070, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 68h:18m:31s remains)
INFO - root - 2017-12-10 14:54:50.679827: step 25080, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 67h:53m:01s remains)
INFO - root - 2017-12-10 14:54:58.554161: step 25090, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 67h:41m:45s remains)
INFO - root - 2017-12-10 14:55:06.436164: step 25100, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.747 sec/batch; 63h:47m:21s remains)
2017-12-10 14:55:07.410114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029579 -0.0028510038 0.029230943 0.054012164 0.064347535 0.062951654 0.052679889 0.035829272 0.019362168 0.01191324 0.013250241 0.0099948263 -0.0041040862 -0.02665711 -0.051282946][9.9527366e-05 0.048036173 0.10500913 0.15300629 0.1812719 0.19257738 0.18717612 0.16752857 0.14584382 0.135792 0.13456412 0.12227488 0.091984086 0.048339818 0.0018938829][0.034992989 0.10775596 0.19355391 0.26818195 0.31774703 0.34672621 0.35384926 0.34093654 0.32257912 0.313839 0.30748123 0.27935651 0.22504741 0.15265644 0.076676562][0.073714405 0.17433073 0.29140115 0.3932645 0.46412557 0.51264477 0.53668326 0.53533506 0.52116764 0.51015991 0.49223539 0.44272873 0.36196056 0.26030084 0.15454446][0.10719637 0.2340598 0.38093403 0.50956219 0.60327435 0.67406571 0.71606231 0.72193646 0.70192534 0.67729646 0.63981414 0.5672887 0.46518704 0.34417874 0.218735][0.12824379 0.27685675 0.45047852 0.60739422 0.72815526 0.82141834 0.87272048 0.86759561 0.82006758 0.76498634 0.70214123 0.61257696 0.50282127 0.37935382 0.25032967][0.14454523 0.30856195 0.50342935 0.68480629 0.82711744 0.93143672 0.97575009 0.94281864 0.85636944 0.76737833 0.68453711 0.59000635 0.48619327 0.37301448 0.25204664][0.15474407 0.32581997 0.53095317 0.72140455 0.86350912 0.95346892 0.97191119 0.90631354 0.789438 0.68119222 0.59444708 0.51006663 0.42340803 0.32817769 0.22260128][0.14932548 0.31494373 0.51234072 0.68822175 0.80317795 0.85519928 0.83742815 0.7473886 0.62123328 0.5152759 0.44070682 0.37732154 0.314775 0.24278393 0.15904976][0.11419547 0.25618875 0.4231905 0.56274796 0.63650084 0.64768338 0.60264319 0.5068512 0.39316556 0.30519435 0.25011867 0.20958407 0.17203134 0.12630692 0.070009783][0.055178352 0.15681447 0.27532083 0.36746213 0.40327963 0.38963282 0.33861679 0.25732368 0.16997193 0.10558606 0.068912156 0.0467847 0.029146722 0.0061703725 -0.024210358][-0.0086364178 0.046280757 0.11100101 0.15697336 0.16600303 0.14537145 0.1057446 0.051989295 -0.0022212067 -0.041058809 -0.061195426 -0.07034611 -0.076062158 -0.0850878 -0.097125784][-0.063942261 -0.047417618 -0.025242195 -0.012419749 -0.017251821 -0.03457693 -0.058309983 -0.085973054 -0.11157863 -0.12866881 -0.13549289 -0.13608481 -0.13511924 -0.13590413 -0.13590387][-0.10163898 -0.10742669 -0.10851459 -0.11167356 -0.1196925 -0.13042939 -0.14208564 -0.15342359 -0.16176841 -0.16569629 -0.16508271 -0.16193269 -0.15782583 -0.15337087 -0.145865][-0.11516396 -0.12826017 -0.13607314 -0.14242603 -0.14818339 -0.15317273 -0.15767241 -0.1610759 -0.16196559 -0.16059463 -0.15787873 -0.15492076 -0.15086055 -0.14476743 -0.13527247]]...]
INFO - root - 2017-12-10 14:55:15.361347: step 25110, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 67h:40m:38s remains)
INFO - root - 2017-12-10 14:55:23.293210: step 25120, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 68h:39m:14s remains)
INFO - root - 2017-12-10 14:55:31.163393: step 25130, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 67h:06m:17s remains)
INFO - root - 2017-12-10 14:55:38.986742: step 25140, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 67h:05m:57s remains)
INFO - root - 2017-12-10 14:55:46.986412: step 25150, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 68h:44m:00s remains)
INFO - root - 2017-12-10 14:55:54.616005: step 25160, loss = 0.71, batch loss = 0.65 (11.9 examples/sec; 0.672 sec/batch; 57h:23m:54s remains)
INFO - root - 2017-12-10 14:56:02.385752: step 25170, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 66h:55m:41s remains)
INFO - root - 2017-12-10 14:56:10.329705: step 25180, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 66h:51m:22s remains)
INFO - root - 2017-12-10 14:56:18.289970: step 25190, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 69h:32m:20s remains)
INFO - root - 2017-12-10 14:56:26.155335: step 25200, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.788 sec/batch; 67h:17m:13s remains)
2017-12-10 14:56:26.928921: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0419024 0.0077449041 0.080301277 0.17124282 0.26895413 0.36070663 0.44803718 0.50805867 0.51560944 0.47054115 0.39210629 0.29538876 0.18087348 0.075193651 0.0066543506][-0.041054949 0.016056016 0.10142612 0.21027665 0.33031544 0.44997081 0.57193309 0.66525626 0.69437379 0.65713316 0.57329261 0.454753 0.30178076 0.15406656 0.05449586][-0.040296022 0.021027939 0.11409692 0.23375842 0.36878955 0.50843996 0.65451056 0.77210796 0.81814408 0.79003668 0.70921737 0.58437246 0.41275352 0.24158558 0.12717724][-0.04102942 0.024046464 0.12487357 0.25489745 0.40156713 0.55216813 0.70622474 0.8308062 0.88071007 0.85537225 0.77997154 0.66000026 0.49055704 0.32078767 0.2147367][-0.041089159 0.02880536 0.13919201 0.28004456 0.43369257 0.58438993 0.73064935 0.84575588 0.886548 0.85551333 0.78300154 0.67609119 0.52923363 0.38643795 0.31060478][-0.039392427 0.036643602 0.15698345 0.30721346 0.46241269 0.60453224 0.73365861 0.8315376 0.85855883 0.81944042 0.75043619 0.66343397 0.55207896 0.45104435 0.415719][-0.035286028 0.045948055 0.17311074 0.32762593 0.47998047 0.61213344 0.72545624 0.80804718 0.82389349 0.78042936 0.71955222 0.6565733 0.58281469 0.52150947 0.51963556][-0.031575594 0.052524447 0.18253335 0.33705738 0.48595521 0.61156768 0.71440345 0.78645009 0.79554397 0.75239575 0.70330924 0.66446012 0.62126833 0.58440936 0.59678578][-0.029941622 0.053578708 0.18014029 0.32733452 0.46774143 0.58572924 0.679428 0.74372983 0.75214052 0.71901113 0.68920761 0.67399055 0.65132046 0.62255692 0.63082325][-0.030882172 0.047735844 0.16242178 0.29134732 0.41261882 0.51353127 0.59069425 0.64432997 0.65701371 0.64400887 0.64011395 0.64750648 0.63936645 0.61225551 0.60993886][-0.034608293 0.034731593 0.13081998 0.23227485 0.324374 0.39858663 0.45148793 0.48986769 0.50643754 0.51483727 0.53474772 0.55913067 0.5613699 0.53692144 0.52734381][-0.038559604 0.020162279 0.096259885 0.16880107 0.23070747 0.27745748 0.30538461 0.32564351 0.33896789 0.35722971 0.38773727 0.4180851 0.42547917 0.40557018 0.39427039][-0.041714128 0.0087496117 0.06956879 0.119893 0.15823637 0.18276495 0.18853936 0.18748142 0.18595579 0.19696271 0.22259736 0.24934952 0.25960132 0.24907883 0.24494426][-0.043829095 0.00051770784 0.051208783 0.087484606 0.11060005 0.11976466 0.10851871 0.0875383 0.066599682 0.061719246 0.074981324 0.094692588 0.10834249 0.11196844 0.12376572][-0.045348395 -0.0059548132 0.038312033 0.067599095 0.08356107 0.084826544 0.063458264 0.0287227 -0.0083037177 -0.02849428 -0.027096597 -0.012672015 0.0055521242 0.024307694 0.055366293]]...]
INFO - root - 2017-12-10 14:56:34.699227: step 25210, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 67h:08m:32s remains)
INFO - root - 2017-12-10 14:56:42.720266: step 25220, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 65h:37m:53s remains)
INFO - root - 2017-12-10 14:56:50.672595: step 25230, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 66h:43m:59s remains)
INFO - root - 2017-12-10 14:56:58.345788: step 25240, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 65h:51m:29s remains)
INFO - root - 2017-12-10 14:57:06.103936: step 25250, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 66h:33m:57s remains)
INFO - root - 2017-12-10 14:57:14.001352: step 25260, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.788 sec/batch; 67h:16m:00s remains)
INFO - root - 2017-12-10 14:57:21.874186: step 25270, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 65h:01m:10s remains)
INFO - root - 2017-12-10 14:57:29.660226: step 25280, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 66h:23m:38s remains)
INFO - root - 2017-12-10 14:57:37.486506: step 25290, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.787 sec/batch; 67h:07m:52s remains)
INFO - root - 2017-12-10 14:57:45.402674: step 25300, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.828 sec/batch; 70h:41m:52s remains)
2017-12-10 14:57:46.179297: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13742109 0.15461309 0.15108292 0.13157567 0.1069793 0.085639842 0.0821126 0.10648747 0.14579627 0.17746997 0.18187393 0.15932912 0.11769045 0.062711179 0.0083797686][0.21947941 0.24665861 0.24395704 0.21852486 0.18579413 0.15691805 0.15245347 0.18566357 0.23712873 0.27661526 0.2782464 0.242983 0.18104388 0.1021819 0.027455874][0.28772071 0.32564142 0.32669625 0.29962793 0.26274329 0.22934723 0.22503369 0.26444268 0.32338575 0.36530104 0.36128649 0.31294534 0.23230451 0.13298237 0.0412626][0.33121595 0.38292134 0.39493442 0.375042 0.34202126 0.31060359 0.3085103 0.34963307 0.40752205 0.44360617 0.42925888 0.36698011 0.26936728 0.15325025 0.04909597][0.34762549 0.41603276 0.44607335 0.44209865 0.42140931 0.39902461 0.40075746 0.43750465 0.48404923 0.50577664 0.47793967 0.40273842 0.29142866 0.16325761 0.051242396][0.35395452 0.44268867 0.49669564 0.51495874 0.51150757 0.50083447 0.50399244 0.5286659 0.55494839 0.55680937 0.51500082 0.42997232 0.30984211 0.17418633 0.056885187][0.36027864 0.46868154 0.54650861 0.5867222 0.59989494 0.59943241 0.60105181 0.60916984 0.612163 0.59433681 0.54127336 0.45064071 0.32663539 0.18812375 0.067659266][0.36353812 0.48411092 0.57713181 0.63098991 0.65322769 0.65679765 0.65277016 0.64365065 0.62662822 0.59461576 0.53630751 0.44645002 0.32579184 0.19125913 0.072585076][0.35211179 0.47092232 0.56419009 0.61884522 0.64083856 0.64238125 0.631122 0.6093595 0.58023006 0.54257423 0.48640507 0.40489671 0.29649597 0.17505406 0.066139892][0.31103766 0.41141123 0.48817152 0.53001112 0.5426634 0.53748751 0.520024 0.49274629 0.46135873 0.42710733 0.38102335 0.31637222 0.23037401 0.13244973 0.042775363][0.23816685 0.30930355 0.35996041 0.38212857 0.38172233 0.36909789 0.34889475 0.32336774 0.29761434 0.27280518 0.24126512 0.19773658 0.13908845 0.070503578 0.0063668825][0.144048 0.18382053 0.20804033 0.21193886 0.20118082 0.18467203 0.16661607 0.14857072 0.13306339 0.11970787 0.10251403 0.078385063 0.044904761 0.0044071926 -0.033858474][0.052720461 0.06724222 0.072114289 0.064818338 0.050004017 0.034481883 0.021727066 0.012800124 0.0074047185 0.0034598561 -0.0032073536 -0.013449868 -0.028386507 -0.047362588 -0.064924225][-0.015033494 -0.015714666 -0.020278355 -0.030535813 -0.04317474 -0.0542749 -0.061050259 -0.06274844 -0.061393861 -0.059999574 -0.061140437 -0.063804127 -0.068154596 -0.074170351 -0.0787797][-0.052219585 -0.058306642 -0.064074375 -0.071710639 -0.079437546 -0.085387461 -0.0876761 -0.085824512 -0.081784122 -0.078899115 -0.078221589 -0.07820157 -0.078431852 -0.079026483 -0.078309208]]...]
INFO - root - 2017-12-10 14:57:53.989019: step 25310, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 67h:37m:58s remains)
INFO - root - 2017-12-10 14:58:01.814288: step 25320, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 67h:48m:35s remains)
INFO - root - 2017-12-10 14:58:09.651669: step 25330, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 65h:35m:08s remains)
INFO - root - 2017-12-10 14:58:17.329989: step 25340, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.816 sec/batch; 69h:38m:33s remains)
INFO - root - 2017-12-10 14:58:25.210564: step 25350, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 69h:11m:14s remains)
INFO - root - 2017-12-10 14:58:33.023297: step 25360, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 65h:22m:51s remains)
INFO - root - 2017-12-10 14:58:40.829627: step 25370, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 65h:50m:03s remains)
INFO - root - 2017-12-10 14:58:48.624335: step 25380, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 66h:45m:59s remains)
INFO - root - 2017-12-10 14:58:56.419363: step 25390, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 66h:35m:38s remains)
INFO - root - 2017-12-10 14:59:04.127465: step 25400, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 68h:16m:15s remains)
2017-12-10 14:59:05.003162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.014775055 -0.0067493822 0.00377405 0.015451952 0.027339704 0.037665065 0.045125395 0.048646696 0.04617738 0.033848558 0.0083234366 -0.026689552 -0.059675012 -0.0824884 -0.092298433][0.028434495 0.042962935 0.062344629 0.086218081 0.11158228 0.13352534 0.1494022 0.15701304 0.15274629 0.13217601 0.090562753 0.032661643 -0.025008792 -0.067905463 -0.090127707][0.097867191 0.11630996 0.14125779 0.17524031 0.21286088 0.24552961 0.26932043 0.280572 0.27389133 0.24433315 0.18627238 0.10497984 0.020935833 -0.044439517 -0.081526175][0.19355156 0.21138865 0.23696235 0.27592248 0.31981522 0.35724965 0.38364738 0.3945277 0.3829878 0.34374684 0.27047896 0.16909909 0.062776975 -0.021995774 -0.071893111][0.31285915 0.32561764 0.34460369 0.37973854 0.4206315 0.45539254 0.47968704 0.48840362 0.47306904 0.42625 0.34068102 0.22309898 0.098871 -0.0016929322 -0.062384866][0.42226067 0.42645732 0.4333747 0.45953983 0.49438551 0.526591 0.551461 0.561406 0.54573184 0.49376577 0.39760739 0.26656431 0.12853183 0.01597905 -0.053102329][0.49080038 0.48183015 0.47101521 0.48417231 0.51270241 0.5463711 0.57822573 0.59534514 0.58376151 0.52962685 0.4260256 0.28644148 0.14172597 0.02482049 -0.047080491][0.505086 0.4811255 0.45229238 0.45254117 0.47611997 0.5143739 0.55643666 0.58348149 0.57827836 0.5252443 0.41949457 0.27837503 0.13556464 0.022675386 -0.045993853][0.46672678 0.4343766 0.396302 0.38815147 0.4068594 0.44671956 0.49471152 0.52876347 0.52983427 0.48212558 0.3822892 0.24901813 0.11651183 0.014079377 -0.047426533][0.38733339 0.35642579 0.31790125 0.30518442 0.3186788 0.35626513 0.40508905 0.44223955 0.44844943 0.40903786 0.32147419 0.20324446 0.0870467 -0.00070039369 -0.052413203][0.27457306 0.25078723 0.2197891 0.20874035 0.22051404 0.2555154 0.30201802 0.33848661 0.3467325 0.31517226 0.24192744 0.14234044 0.045940626 -0.024237573 -0.063415974][0.15293045 0.13816632 0.11786643 0.11038667 0.11979825 0.14849858 0.18718573 0.21826176 0.2261457 0.20274954 0.14638084 0.068981141 -0.0044802 -0.054988042 -0.079783745][0.041639347 0.035443872 0.026742341 0.023294868 0.028678084 0.047318347 0.073752351 0.095888712 0.10185383 0.086648308 0.048097353 -0.0057508457 -0.05535477 -0.086138144 -0.096822262][-0.041852511 -0.04020264 -0.039422486 -0.040200703 -0.039699327 -0.032420523 -0.019530687 -0.0078026382 -0.0052769091 -0.014562881 -0.037924889 -0.070606187 -0.098578811 -0.111715 -0.10999951][-0.090008169 -0.085470304 -0.080620378 -0.0806151 -0.083247118 -0.083253086 -0.079699419 -0.0757076 -0.076093271 -0.081837147 -0.09484487 -0.11229486 -0.12472953 -0.12576346 -0.11621822]]...]
INFO - root - 2017-12-10 14:59:12.876982: step 25410, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 68h:11m:22s remains)
INFO - root - 2017-12-10 14:59:20.717759: step 25420, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 68h:48m:58s remains)
INFO - root - 2017-12-10 14:59:28.418070: step 25430, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 68h:41m:05s remains)
INFO - root - 2017-12-10 14:59:36.410365: step 25440, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 67h:38m:58s remains)
INFO - root - 2017-12-10 14:59:44.146883: step 25450, loss = 0.67, batch loss = 0.61 (10.6 examples/sec; 0.752 sec/batch; 64h:07m:20s remains)
INFO - root - 2017-12-10 14:59:52.141306: step 25460, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 65h:39m:49s remains)
INFO - root - 2017-12-10 15:00:00.096908: step 25470, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 66h:23m:30s remains)
INFO - root - 2017-12-10 15:00:07.738435: step 25480, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 68h:55m:38s remains)
INFO - root - 2017-12-10 15:00:15.596160: step 25490, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 65h:51m:59s remains)
INFO - root - 2017-12-10 15:00:23.475760: step 25500, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 65h:48m:51s remains)
2017-12-10 15:00:24.297993: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10786936 0.13226078 0.14181061 0.13859168 0.12396076 0.10028302 0.077021517 0.067702912 0.074265607 0.081650116 0.081775442 0.07639797 0.066583939 0.045852464 0.013040154][0.1732126 0.21379152 0.23030546 0.2260842 0.20417267 0.16898848 0.13316481 0.117406 0.12456803 0.13340741 0.13169938 0.11951149 0.10112935 0.071218722 0.029034387][0.2334864 0.29346523 0.32126921 0.32132417 0.29854086 0.25823334 0.21541019 0.19789766 0.20722538 0.2154703 0.20651366 0.1803139 0.1459754 0.10026346 0.04504757][0.27402914 0.35438558 0.39946 0.41278818 0.40187949 0.36941952 0.33010304 0.31853572 0.33377847 0.34057167 0.31870213 0.27149281 0.21414271 0.14491504 0.069333911][0.29303914 0.39392403 0.46164241 0.49661997 0.50949806 0.49591619 0.4674817 0.46570271 0.48792207 0.49194795 0.45307675 0.37990871 0.29542381 0.19785497 0.09682063][0.30010498 0.41947031 0.50966763 0.56773061 0.60771704 0.61548662 0.5984441 0.60439897 0.63135242 0.63053447 0.57288146 0.47389236 0.3642903 0.24161334 0.11764858][0.29645532 0.42619354 0.530255 0.60454905 0.66542351 0.69010419 0.68230903 0.69252568 0.7205857 0.71294421 0.63920712 0.52080494 0.39528087 0.25902909 0.12350947][0.28721657 0.41584998 0.51978755 0.59595758 0.66195172 0.6911093 0.68556243 0.69475007 0.72059828 0.7083711 0.62887025 0.5057407 0.38020825 0.24698505 0.1151817][0.26521459 0.37828133 0.46491611 0.52498966 0.57689226 0.59620214 0.58530879 0.58927053 0.61191726 0.60155809 0.53136146 0.42227772 0.31451619 0.20131074 0.0880474][0.2259994 0.31386402 0.37341836 0.40817297 0.43616572 0.43889698 0.41971692 0.41673288 0.4359045 0.43152449 0.38002595 0.2966482 0.21643859 0.13199465 0.045083415][0.17817868 0.23873839 0.27128595 0.28209525 0.2874991 0.27585402 0.25139832 0.24354683 0.26000515 0.26220754 0.23053662 0.17387995 0.11991227 0.062115464 0.00019849397][0.11793227 0.15312506 0.16334981 0.15610501 0.14527522 0.1251535 0.10025612 0.09088935 0.10488354 0.11169075 0.095842488 0.061080698 0.027853126 -0.0080710566 -0.047591928][0.04887332 0.06627889 0.064868093 0.050686184 0.033942681 0.012730033 -0.00876543 -0.017501805 -0.0069281268 0.00063444307 -0.0063417284 -0.026849832 -0.046484351 -0.066962875 -0.088995226][-0.010051217 -0.00270607 -0.0067816661 -0.019301582 -0.033871248 -0.05066476 -0.066296577 -0.073181935 -0.066460043 -0.06092808 -0.064463079 -0.076892518 -0.088541687 -0.099340029 -0.10990467][-0.048237588 -0.045868021 -0.048608698 -0.056214478 -0.065317191 -0.075701453 -0.08507923 -0.089721821 -0.086365737 -0.083619878 -0.086252108 -0.094090074 -0.10118271 -0.10663725 -0.11104367]]...]
INFO - root - 2017-12-10 15:00:32.068593: step 25510, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 66h:49m:06s remains)
INFO - root - 2017-12-10 15:00:39.772928: step 25520, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 65h:51m:06s remains)
INFO - root - 2017-12-10 15:00:47.756248: step 25530, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 68h:18m:36s remains)
INFO - root - 2017-12-10 15:00:55.438629: step 25540, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 65h:10m:22s remains)
INFO - root - 2017-12-10 15:01:03.243440: step 25550, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 66h:19m:03s remains)
INFO - root - 2017-12-10 15:01:10.893638: step 25560, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 67h:33m:01s remains)
INFO - root - 2017-12-10 15:01:18.663256: step 25570, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.752 sec/batch; 64h:07m:41s remains)
INFO - root - 2017-12-10 15:01:26.442930: step 25580, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.764 sec/batch; 65h:06m:40s remains)
INFO - root - 2017-12-10 15:01:34.281891: step 25590, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 68h:51m:10s remains)
INFO - root - 2017-12-10 15:01:42.263743: step 25600, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 67h:18m:15s remains)
2017-12-10 15:01:43.074783: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23617727 0.29672179 0.34429 0.38058323 0.40342838 0.40886438 0.3885884 0.33534518 0.24868728 0.1385549 0.032549411 -0.045419831 -0.085848562 -0.095992483 -0.088072039][0.26684889 0.33206427 0.3827737 0.4195506 0.44122842 0.44584167 0.42599884 0.37255305 0.28260446 0.16546839 0.050545689 -0.035559047 -0.081833385 -0.09527421 -0.088224329][0.25396642 0.32061121 0.37413034 0.41389585 0.43893585 0.44851184 0.43584272 0.38977632 0.30407795 0.18657818 0.067250781 -0.024534639 -0.075834714 -0.092058994 -0.085744604][0.20075062 0.266384 0.32369453 0.37119645 0.40734184 0.43019173 0.43180937 0.3987819 0.32183173 0.2068833 0.084157661 -0.013857095 -0.071102217 -0.090871058 -0.085651048][0.12058747 0.18035315 0.24013236 0.29776022 0.35067022 0.39333296 0.414246 0.39764553 0.33248249 0.22307198 0.099115774 -0.0039513856 -0.066870831 -0.090370446 -0.086414345][0.03610621 0.086228408 0.14499728 0.21036123 0.27907142 0.3413392 0.38096151 0.38019571 0.32759377 0.22638172 0.10492329 0.00020730591 -0.066069767 -0.092177883 -0.089294583][-0.032161582 0.005936747 0.058638908 0.1249989 0.20314066 0.28054652 0.33685952 0.35166568 0.31283224 0.22241403 0.10630304 0.002800293 -0.064957313 -0.093363859 -0.09176439][-0.0746306 -0.049585428 -0.0074993479 0.05325618 0.13456239 0.22401401 0.29784358 0.33093435 0.3083435 0.22942464 0.11763308 0.012909998 -0.058945421 -0.09142296 -0.092615344][-0.091031678 -0.077905729 -0.047826778 0.0039093113 0.083833642 0.18154983 0.2709181 0.32177386 0.31495386 0.2462187 0.13689196 0.028563246 -0.049203083 -0.086953692 -0.091828406][-0.086846873 -0.083066136 -0.063728809 -0.022410283 0.05130427 0.15040644 0.24874623 0.31291389 0.31971022 0.26105371 0.1554029 0.044569872 -0.03830649 -0.080717675 -0.089070067][-0.074672043 -0.075934313 -0.064215124 -0.03296563 0.030401526 0.12293166 0.22126701 0.29174659 0.308673 0.26056 0.16218805 0.053149018 -0.031335939 -0.075898044 -0.086248673][-0.063856408 -0.066783324 -0.05951114 -0.036879465 0.014048545 0.094025135 0.18412733 0.25342503 0.27596888 0.23852468 0.15187214 0.050836582 -0.02989665 -0.073052466 -0.083444707][-0.056526303 -0.0594222 -0.054682147 -0.039244127 -0.0013386069 0.062459163 0.1381357 0.19955632 0.22286308 0.19542469 0.12419952 0.037298489 -0.033962954 -0.072091453 -0.080811284][-0.050198685 -0.053270519 -0.050877146 -0.0421998 -0.017395079 0.028424578 0.085986577 0.13491334 0.15516569 0.13666368 0.083127059 0.015026059 -0.042033147 -0.072223365 -0.078130208][-0.044479113 -0.048739668 -0.049063411 -0.046527348 -0.033164091 -0.0034485522 0.037351385 0.074191496 0.090828188 0.080129571 0.04343215 -0.0058548325 -0.048539981 -0.071238041 -0.075044379]]...]
INFO - root - 2017-12-10 15:01:50.741882: step 25610, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 68h:11m:33s remains)
INFO - root - 2017-12-10 15:01:58.570893: step 25620, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.773 sec/batch; 65h:52m:56s remains)
INFO - root - 2017-12-10 15:02:06.375069: step 25630, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 67h:35m:31s remains)
INFO - root - 2017-12-10 15:02:13.954910: step 25640, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 67h:18m:26s remains)
INFO - root - 2017-12-10 15:02:21.778848: step 25650, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 67h:17m:12s remains)
INFO - root - 2017-12-10 15:02:29.639965: step 25660, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 66h:22m:33s remains)
INFO - root - 2017-12-10 15:02:37.376640: step 25670, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 66h:05m:19s remains)
INFO - root - 2017-12-10 15:02:45.212358: step 25680, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.773 sec/batch; 65h:52m:53s remains)
INFO - root - 2017-12-10 15:02:53.019067: step 25690, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 64h:42m:30s remains)
INFO - root - 2017-12-10 15:03:00.626145: step 25700, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 66h:50m:24s remains)
2017-12-10 15:03:01.476535: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.006603661 0.014223286 0.020301905 0.013792698 -0.00017242051 -0.013907922 -0.023541028 -0.018675094 0.0085826265 0.06073625 0.1356488 0.21719661 0.27916062 0.30037758 0.27681085][0.0091449516 0.02044235 0.034785334 0.040463556 0.037284985 0.023984995 0.0032039033 -0.0094913216 -0.00027408602 0.041282848 0.11716988 0.20740862 0.28084704 0.31079981 0.29044175][0.044341829 0.068119146 0.097628765 0.12213321 0.13567711 0.12668124 0.0935835 0.055045296 0.031997796 0.044358347 0.099398822 0.17897414 0.25158644 0.28807893 0.2779322][0.1092362 0.15307896 0.20252702 0.24919862 0.282945 0.28328657 0.24195378 0.17721875 0.11468398 0.082989156 0.096889347 0.14636403 0.20506814 0.24392653 0.24736117][0.18395589 0.25064442 0.31932509 0.38616621 0.43947619 0.45328608 0.41282058 0.3317931 0.2348423 0.15523824 0.11624699 0.12165296 0.15495801 0.1890883 0.20347169][0.2439566 0.32796943 0.40799898 0.48685306 0.55511832 0.58595246 0.559656 0.480564 0.3656933 0.24765456 0.15740299 0.11444186 0.11451953 0.13488749 0.15212919][0.26876 0.35996327 0.44132072 0.52168083 0.59657657 0.64309287 0.63946217 0.57820415 0.46502817 0.32728773 0.20081674 0.11777399 0.085091129 0.086466484 0.098993137][0.24510489 0.33220816 0.40653807 0.47948384 0.55155164 0.60621977 0.62354594 0.58722389 0.49244589 0.35789254 0.2191719 0.11408218 0.057833698 0.041508228 0.046283603][0.18519023 0.2569167 0.31603938 0.37364846 0.43341768 0.48567918 0.515423 0.50311685 0.43679413 0.3253378 0.19862442 0.092565082 0.026684014 -0.00065115362 -0.0019304353][0.11783228 0.1643903 0.20070545 0.23647071 0.27706861 0.31858185 0.3512837 0.35662055 0.3198249 0.24231774 0.14485058 0.055500042 -0.0061121066 -0.035685852 -0.038691629][0.073454447 0.092814073 0.10444396 0.11690748 0.13708949 0.16444212 0.19259253 0.20651926 0.19303232 0.14993766 0.087850593 0.024169005 -0.024266995 -0.049451869 -0.051361568][0.066631429 0.065196276 0.057243608 0.050922092 0.054466359 0.068474181 0.087962575 0.10283349 0.10352561 0.08706139 0.055365503 0.015887909 -0.018199082 -0.037114523 -0.038086519][0.085954532 0.074598588 0.056555055 0.039912127 0.033635996 0.038384821 0.049678177 0.062011506 0.069242865 0.068850525 0.057193283 0.034370027 0.010343137 -0.0043417416 -0.0066050817][0.10285116 0.090956345 0.072264716 0.054155696 0.045528933 0.046910781 0.053967886 0.064063147 0.073046863 0.078995839 0.07651905 0.061517116 0.041781239 0.027940748 0.022164833][0.0923107 0.08434774 0.0708012 0.057558652 0.052332889 0.055394907 0.06218002 0.071081378 0.07870739 0.083755739 0.082099363 0.068513244 0.049456879 0.034789696 0.025383415]]...]
INFO - root - 2017-12-10 15:03:09.312661: step 25710, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 67h:29m:17s remains)
INFO - root - 2017-12-10 15:03:16.997541: step 25720, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.771 sec/batch; 65h:40m:39s remains)
INFO - root - 2017-12-10 15:03:24.739111: step 25730, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 64h:56m:07s remains)
INFO - root - 2017-12-10 15:03:32.565864: step 25740, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 66h:07m:04s remains)
INFO - root - 2017-12-10 15:03:40.382330: step 25750, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 66h:57m:19s remains)
INFO - root - 2017-12-10 15:03:48.140156: step 25760, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 67h:26m:01s remains)
INFO - root - 2017-12-10 15:03:56.053977: step 25770, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 66h:43m:56s remains)
INFO - root - 2017-12-10 15:04:03.900006: step 25780, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 67h:41m:14s remains)
INFO - root - 2017-12-10 15:04:11.550556: step 25790, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 66h:27m:14s remains)
INFO - root - 2017-12-10 15:04:19.271195: step 25800, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 66h:32m:27s remains)
2017-12-10 15:04:20.008512: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30807737 0.32702357 0.31079915 0.2665059 0.20633207 0.15486094 0.13710383 0.15975052 0.20560005 0.2550855 0.30541369 0.35627297 0.39797348 0.42056412 0.41146317][0.36626783 0.391839 0.37446171 0.32682616 0.266176 0.21745579 0.20231749 0.22423279 0.26612395 0.3108 0.35845467 0.41013882 0.45497528 0.4783583 0.46424821][0.41600969 0.44767874 0.43015549 0.38393709 0.33003768 0.29023176 0.27938178 0.29738563 0.32981098 0.36324781 0.40001771 0.44517934 0.48921785 0.51387721 0.497986][0.460041 0.4950968 0.47967619 0.44105369 0.40232241 0.379355 0.37766176 0.39170021 0.4109607 0.42720833 0.44516712 0.47500536 0.51133978 0.53392684 0.51653653][0.48732674 0.52196908 0.51296926 0.49033794 0.47571847 0.47791877 0.49024987 0.49966493 0.49978963 0.49000904 0.4802967 0.48699519 0.50994724 0.52798015 0.511784][0.49002364 0.52172214 0.52368826 0.52376223 0.53819025 0.5683158 0.59471279 0.596694 0.57268983 0.53243327 0.49294448 0.47614035 0.4854646 0.49899536 0.48761719][0.4627493 0.49366611 0.51162887 0.53805166 0.58032185 0.63294232 0.66749007 0.658214 0.60818511 0.53810173 0.4728893 0.43880889 0.43945831 0.45264277 0.45199153][0.418782 0.45254916 0.49078232 0.5435093 0.60761219 0.67212945 0.70388556 0.67567837 0.59642428 0.49876186 0.41534054 0.37452146 0.37687218 0.40035483 0.4214716][0.38078728 0.42340544 0.48457956 0.55998027 0.63499767 0.69574124 0.7091378 0.65176845 0.54103506 0.4206908 0.32878047 0.2916083 0.30517885 0.34806928 0.39931917][0.35534206 0.41118091 0.49117774 0.57920855 0.65045565 0.69124675 0.67355412 0.58267486 0.44538847 0.31265873 0.22381504 0.20037369 0.23265071 0.29941103 0.38067845][0.33492836 0.40366682 0.49399775 0.58187139 0.63595438 0.64552039 0.59230441 0.47293815 0.32178143 0.19110647 0.11752054 0.11587527 0.17014031 0.25862607 0.36184797][0.29758894 0.37155312 0.46122673 0.53807032 0.56910509 0.54790568 0.46822962 0.33664852 0.19033855 0.076450095 0.025728777 0.046378113 0.11785472 0.21918087 0.33209243][0.2300304 0.29991049 0.378836 0.43782356 0.44680277 0.40451503 0.31554449 0.192283 0.068819657 -0.017602516 -0.044217605 -0.0085670631 0.068004988 0.16745374 0.2747187][0.145383 0.2006992 0.2594769 0.29646114 0.28823939 0.23807961 0.15707813 0.058794096 -0.029725626 -0.083603486 -0.089237973 -0.049156908 0.018015286 0.099609032 0.18570237][0.058166858 0.093521319 0.128624 0.14488412 0.12743081 0.082094073 0.021139489 -0.043945577 -0.094825447 -0.11839022 -0.10928942 -0.073079422 -0.023645055 0.031829357 0.088758983]]...]
INFO - root - 2017-12-10 15:04:27.803384: step 25810, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 64h:25m:34s remains)
INFO - root - 2017-12-10 15:04:35.706370: step 25820, loss = 0.68, batch loss = 0.63 (9.6 examples/sec; 0.834 sec/batch; 71h:04m:38s remains)
INFO - root - 2017-12-10 15:04:43.528991: step 25830, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.756 sec/batch; 64h:23m:17s remains)
INFO - root - 2017-12-10 15:04:51.464942: step 25840, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.820 sec/batch; 69h:52m:49s remains)
INFO - root - 2017-12-10 15:04:59.451257: step 25850, loss = 0.67, batch loss = 0.62 (10.1 examples/sec; 0.789 sec/batch; 67h:11m:58s remains)
INFO - root - 2017-12-10 15:05:07.259810: step 25860, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 67h:27m:18s remains)
INFO - root - 2017-12-10 15:05:15.122180: step 25870, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 68h:01m:32s remains)
INFO - root - 2017-12-10 15:05:22.462725: step 25880, loss = 0.69, batch loss = 0.63 (10.7 examples/sec; 0.751 sec/batch; 63h:57m:16s remains)
INFO - root - 2017-12-10 15:05:30.312436: step 25890, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.758 sec/batch; 64h:35m:28s remains)
INFO - root - 2017-12-10 15:05:38.270609: step 25900, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 66h:01m:24s remains)
2017-12-10 15:05:39.038426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043495718 -0.029947823 -0.014116389 -0.0013518687 0.0047216695 1.3044834e-05 -0.013913469 -0.031892203 -0.048686936 -0.061788365 -0.069129676 -0.07040628 -0.068258569 -0.066519335 -0.066395253][-0.011715399 0.018642956 0.050259925 0.07524024 0.08607021 0.076466858 0.049578197 0.016254323 -0.014388078 -0.038725413 -0.053961217 -0.059653185 -0.05939291 -0.060297612 -0.064008735][0.032425471 0.081013352 0.12992667 0.16900508 0.18606526 0.17301388 0.13525431 0.089665622 0.04775656 0.013131186 -0.010722389 -0.022487693 -0.027130887 -0.035137124 -0.047585838][0.069023 0.13001272 0.19170234 0.24249639 0.26685745 0.25618014 0.21723352 0.17074712 0.12791209 0.090465687 0.061467238 0.043152113 0.030442545 0.010319619 -0.016708734][0.086295031 0.15096119 0.21818812 0.27622497 0.30974922 0.31030586 0.28372046 0.25156078 0.22234519 0.19364993 0.16577256 0.14156687 0.11774043 0.080149263 0.031850372][0.088327721 0.15285289 0.22224024 0.28553429 0.33209804 0.35291341 0.34973109 0.34315529 0.33883205 0.32826009 0.30546668 0.27325654 0.23322514 0.17205244 0.095807835][0.09425398 0.16447777 0.23998821 0.31077045 0.3732298 0.41679212 0.43714249 0.4553743 0.47636232 0.48403326 0.46344748 0.4171688 0.35469764 0.26553756 0.15842795][0.12118082 0.20832786 0.29762512 0.379189 0.45515364 0.51281494 0.54384905 0.57342339 0.60788059 0.62385243 0.5980323 0.53356385 0.44759232 0.33251816 0.19910496][0.16179585 0.2696867 0.37389877 0.46367946 0.54406583 0.60083318 0.62550521 0.64876938 0.67891252 0.68850571 0.65104485 0.57104105 0.46993351 0.34146488 0.19762394][0.18545665 0.30159113 0.40820581 0.49382979 0.56427532 0.60564137 0.61262149 0.61756849 0.62982953 0.62297559 0.57416141 0.48913217 0.38921145 0.2688252 0.13948874][0.1597736 0.25929824 0.34648365 0.41103441 0.4586094 0.47760084 0.46692818 0.454916 0.44977096 0.4299134 0.38016677 0.30645505 0.22612375 0.134482 0.041272912][0.080415227 0.14188758 0.19311032 0.22618005 0.24656522 0.24635887 0.22743303 0.20949897 0.19761911 0.17732793 0.14048164 0.091613047 0.042466983 -0.009976265 -0.058677591][-0.017600732 0.002492283 0.017637758 0.022428477 0.021556424 0.010966177 -0.0068354113 -0.021166334 -0.030224912 -0.041645162 -0.059049096 -0.079894044 -0.097767815 -0.11408444 -0.12496513][-0.087326325 -0.094279453 -0.10091374 -0.11073165 -0.12078349 -0.13248888 -0.14436844 -0.15201955 -0.15524867 -0.15719944 -0.15925665 -0.16046913 -0.15790592 -0.15226808 -0.14251991][-0.10707603 -0.12242918 -0.13474891 -0.14650294 -0.15630876 -0.16450529 -0.17068478 -0.17357236 -0.17321773 -0.17069599 -0.16641925 -0.16021037 -0.15091217 -0.13909309 -0.125305]]...]
INFO - root - 2017-12-10 15:05:47.000011: step 25910, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 66h:30m:18s remains)
INFO - root - 2017-12-10 15:05:54.932772: step 25920, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 67h:20m:15s remains)
INFO - root - 2017-12-10 15:06:02.747999: step 25930, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 66h:06m:09s remains)
INFO - root - 2017-12-10 15:06:10.664079: step 25940, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 65h:54m:48s remains)
INFO - root - 2017-12-10 15:06:18.597106: step 25950, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.792 sec/batch; 67h:26m:26s remains)
INFO - root - 2017-12-10 15:06:26.326646: step 25960, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 68h:23m:08s remains)
INFO - root - 2017-12-10 15:06:34.126788: step 25970, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.825 sec/batch; 70h:12m:37s remains)
INFO - root - 2017-12-10 15:06:42.079469: step 25980, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.809 sec/batch; 68h:52m:04s remains)
INFO - root - 2017-12-10 15:06:50.070616: step 25990, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 67h:49m:08s remains)
INFO - root - 2017-12-10 15:06:57.943190: step 26000, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 66h:55m:57s remains)
2017-12-10 15:06:58.815163: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13923515 0.13102558 0.12711698 0.12885408 0.1289773 0.12354783 0.11479812 0.10429351 0.0910385 0.076546632 0.070082068 0.078594908 0.094088033 0.10521961 0.10410162][0.10677773 0.10471932 0.10775845 0.11631376 0.12329409 0.12354031 0.12011745 0.11363307 0.10218926 0.0858004 0.074056208 0.075086884 0.08244922 0.085975692 0.078856193][0.079092242 0.083669066 0.092257038 0.10523678 0.11695901 0.12310252 0.12662193 0.1262897 0.11837133 0.1008619 0.082759313 0.073941417 0.071120419 0.066671588 0.054853626][0.074921466 0.087542273 0.10096506 0.11579354 0.12867276 0.13799076 0.14685078 0.15186745 0.14720991 0.12880963 0.10438399 0.086103141 0.074055336 0.06337963 0.048766557][0.095367543 0.11857606 0.13810451 0.15458603 0.16675425 0.17623094 0.18672098 0.19414766 0.19128595 0.1723873 0.14335608 0.118758 0.10119834 0.087281615 0.071208134][0.12983653 0.16467582 0.19207147 0.21217623 0.22433436 0.23222896 0.24025594 0.24545105 0.24081963 0.22043726 0.18901771 0.16292708 0.14531806 0.13210119 0.11627775][0.15884523 0.20308408 0.2390513 0.26593575 0.28128743 0.28833196 0.29148358 0.29037428 0.28035775 0.25677806 0.22435966 0.20045096 0.18690974 0.17736743 0.1630124][0.17586784 0.22691216 0.27001423 0.30378106 0.32253152 0.32772791 0.3238796 0.31450769 0.29808179 0.27122867 0.23937993 0.22009014 0.21278818 0.20830151 0.195301][0.18141384 0.2368744 0.2844725 0.32271942 0.34273675 0.34381118 0.33170134 0.31463662 0.29389054 0.26566285 0.23631352 0.22321558 0.22290829 0.22377537 0.21206267][0.17662224 0.23255451 0.280738 0.31958669 0.33773854 0.33303356 0.31321377 0.29082528 0.26941931 0.2436817 0.22032005 0.21560386 0.2227048 0.22870147 0.21833105][0.16467059 0.21514921 0.25895986 0.29429391 0.30866823 0.29984331 0.27645797 0.25269806 0.23364672 0.21284804 0.19750245 0.201834 0.21517128 0.22468443 0.2150225][0.14505383 0.1845942 0.21943404 0.24729635 0.25672242 0.24702168 0.22580524 0.20547172 0.19132185 0.17641628 0.16848655 0.17929892 0.19522128 0.20496167 0.19493443][0.1122858 0.13913172 0.16210599 0.17877488 0.18184577 0.17379242 0.15996964 0.14777775 0.14034216 0.13096817 0.12791643 0.14096048 0.15557212 0.16278318 0.15219252][0.061391316 0.076389588 0.0885126 0.094569847 0.092158504 0.086840734 0.081736416 0.078611486 0.077487461 0.072369553 0.071555607 0.08341419 0.094657905 0.098727971 0.088707417][-0.002327563 0.0027962863 0.0071927803 0.0064359652 0.0010753091 -0.0017953716 -0.00070406654 0.0025127779 0.0057253488 0.0040575848 0.0045868065 0.014175671 0.022179998 0.024029039 0.01570932]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 15:07:06.718228: step 26010, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 67h:16m:08s remains)
INFO - root - 2017-12-10 15:07:14.741878: step 26020, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 68h:25m:47s remains)
INFO - root - 2017-12-10 15:07:22.661116: step 26030, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 64h:42m:43s remains)
INFO - root - 2017-12-10 15:07:30.477397: step 26040, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.810 sec/batch; 68h:58m:38s remains)
INFO - root - 2017-12-10 15:07:38.381142: step 26050, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.823 sec/batch; 70h:03m:32s remains)
INFO - root - 2017-12-10 15:07:46.093382: step 26060, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 66h:23m:16s remains)
INFO - root - 2017-12-10 15:07:54.016143: step 26070, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 68h:24m:06s remains)
INFO - root - 2017-12-10 15:08:01.888929: step 26080, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 66h:38m:38s remains)
INFO - root - 2017-12-10 15:08:09.627708: step 26090, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 66h:32m:35s remains)
INFO - root - 2017-12-10 15:08:17.349644: step 26100, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.765 sec/batch; 65h:07m:03s remains)
2017-12-10 15:08:18.112122: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12588838 0.15177794 0.1731855 0.18151847 0.17856672 0.17946979 0.19946703 0.23316585 0.25955322 0.25921687 0.23331019 0.18556201 0.12559111 0.077430837 0.060588043][0.11367825 0.14334476 0.16665366 0.17629465 0.17338672 0.17310901 0.1900287 0.21725518 0.23444507 0.22607368 0.19723737 0.15307476 0.10248172 0.066983856 0.062655658][0.097954288 0.12834872 0.15390863 0.16914462 0.17318092 0.17739615 0.19219492 0.20934011 0.2114408 0.18943679 0.1531243 0.10934803 0.065955758 0.03928306 0.040835556][0.0959061 0.1249153 0.15170687 0.1740333 0.18915813 0.20221333 0.21665694 0.22286443 0.20705892 0.16740236 0.11884779 0.071117692 0.031015223 0.0080464175 0.00894357][0.12124426 0.14603586 0.17032556 0.196494 0.22093852 0.24282563 0.25763327 0.25399414 0.22086576 0.16332357 0.10162865 0.049431238 0.01221072 -0.0082040979 -0.0098224115][0.17569678 0.19319811 0.20913424 0.23166713 0.25834772 0.28434247 0.29802418 0.28625521 0.23982494 0.16957505 0.10012228 0.04873585 0.018799242 0.0044086687 0.0032049257][0.2379387 0.24672361 0.25080192 0.26380244 0.28604719 0.31061524 0.32063147 0.30216572 0.2484325 0.17447397 0.10745438 0.066615723 0.051552247 0.04920914 0.053282805][0.26812586 0.27052712 0.26622617 0.27133361 0.28824741 0.30883697 0.31394204 0.29081532 0.23678619 0.16970982 0.11717897 0.097042553 0.1033598 0.11758032 0.130093][0.24432841 0.24530244 0.24192153 0.24795793 0.26516023 0.28398159 0.2856811 0.26053536 0.21165384 0.15879416 0.12790096 0.1321605 0.15969846 0.18876429 0.20790307][0.17740506 0.18068103 0.18666114 0.20379978 0.22952275 0.25182894 0.25389272 0.2298262 0.18872699 0.15135743 0.14066239 0.1635612 0.20374259 0.23932686 0.25918734][0.10113387 0.10773727 0.12628824 0.15926896 0.19764377 0.22680622 0.23224153 0.21134633 0.177598 0.15162207 0.15257694 0.18145148 0.22082163 0.252276 0.2672345][0.046188563 0.055475086 0.084467269 0.13085966 0.17963532 0.21470617 0.22379462 0.20644212 0.17774202 0.15664628 0.15820868 0.17992295 0.20754413 0.22874232 0.23874195][0.02940256 0.041588251 0.076946482 0.13029085 0.18369813 0.22210325 0.23504028 0.22224614 0.19729805 0.17536919 0.16775742 0.1725758 0.18333079 0.19477247 0.20533037][0.046010125 0.06247215 0.09981551 0.15219735 0.20326237 0.24188966 0.25894293 0.25220716 0.23133186 0.20594792 0.18484503 0.17105708 0.168592 0.17817515 0.19843519][0.07707575 0.098784991 0.13545099 0.18189862 0.2262107 0.26253414 0.28271005 0.28174543 0.26469594 0.23550965 0.20233399 0.17523295 0.16836002 0.18588395 0.22287276]]...]
INFO - root - 2017-12-10 15:08:25.985099: step 26110, loss = 0.70, batch loss = 0.65 (9.7 examples/sec; 0.822 sec/batch; 69h:56m:37s remains)
INFO - root - 2017-12-10 15:08:33.656045: step 26120, loss = 0.68, batch loss = 0.62 (10.7 examples/sec; 0.750 sec/batch; 63h:48m:42s remains)
INFO - root - 2017-12-10 15:08:41.571326: step 26130, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 67h:38m:15s remains)
INFO - root - 2017-12-10 15:08:49.247727: step 26140, loss = 0.70, batch loss = 0.64 (12.7 examples/sec; 0.632 sec/batch; 53h:46m:39s remains)
INFO - root - 2017-12-10 15:08:57.107600: step 26150, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 68h:22m:58s remains)
INFO - root - 2017-12-10 15:09:04.994506: step 26160, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 68h:23m:37s remains)
INFO - root - 2017-12-10 15:09:12.831195: step 26170, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 66h:37m:52s remains)
INFO - root - 2017-12-10 15:09:20.671644: step 26180, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 66h:40m:29s remains)
INFO - root - 2017-12-10 15:09:28.525268: step 26190, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 65h:26m:24s remains)
INFO - root - 2017-12-10 15:09:36.300329: step 26200, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 66h:39m:40s remains)
2017-12-10 15:09:37.126613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0743985 -0.070792496 -0.064528786 -0.060002428 -0.058302425 -0.060288686 -0.064510591 -0.068092749 -0.066865452 -0.062728964 -0.059767947 -0.061136052 -0.066515647 -0.075217307 -0.087075152][-0.059381723 -0.046769124 -0.030172665 -0.015595331 -0.0049866643 -0.00078845635 -0.00157629 -0.0026880251 0.0022593294 0.0095942235 0.012284538 0.0060100234 -0.0080216359 -0.029086065 -0.057456676][-0.034436893 -0.0070654871 0.0276882 0.061284684 0.090318091 0.10972124 0.11940395 0.12444729 0.13421735 0.14436878 0.1449047 0.13026087 0.10169993 0.061002992 0.007206114][-0.0021922456 0.046450667 0.10765635 0.16822547 0.22255962 0.26427186 0.29119176 0.30776405 0.32549441 0.34093717 0.34055012 0.31554651 0.266333 0.19658852 0.1064617][0.037795212 0.11459806 0.21080859 0.30487472 0.38776794 0.45461947 0.50234777 0.53157228 0.55376053 0.56952435 0.56469214 0.52577209 0.45058876 0.34553871 0.21440104][0.073765934 0.17905089 0.31117815 0.43811911 0.54670376 0.63631839 0.70426041 0.7434513 0.76109338 0.76563323 0.74814439 0.69198328 0.59092784 0.45342216 0.28919911][0.09865737 0.2260984 0.387055 0.54065806 0.66902882 0.77611285 0.85909414 0.90260971 0.90660232 0.88879836 0.85066974 0.77487957 0.6528886 0.49370715 0.31271106][0.11372945 0.25408438 0.43142062 0.59910548 0.7352128 0.84589809 0.92768097 0.96102953 0.94200647 0.89811128 0.84025878 0.75117546 0.62118292 0.4586454 0.28216487][0.10837876 0.24532419 0.41764477 0.57903272 0.70557421 0.8024351 0.86568105 0.87810951 0.83724391 0.77589726 0.70887882 0.61895525 0.49700302 0.35030881 0.19836575][0.075355686 0.19101737 0.33696729 0.47371382 0.5785917 0.65283245 0.69132191 0.68236721 0.62794924 0.56033891 0.49336198 0.41095668 0.30696943 0.18924394 0.07640741][0.023453478 0.10585888 0.21136603 0.31089282 0.38635814 0.43525186 0.45101935 0.42783633 0.37140387 0.30846694 0.24885598 0.18018733 0.10106903 0.019867906 -0.048250634][-0.030033242 0.017405313 0.080930322 0.14122532 0.18612702 0.21179228 0.21194334 0.18484871 0.13725483 0.087357365 0.040630493 -0.009863711 -0.061159421 -0.1061056 -0.13570136][-0.072188728 -0.053240106 -0.023627076 0.0044552712 0.023950219 0.031490073 0.022996968 -0.00088283641 -0.034495097 -0.06798058 -0.098932192 -0.12964833 -0.15555301 -0.17213731 -0.17619075][-0.098683067 -0.0995956 -0.093308695 -0.087463714 -0.085271306 -0.088647664 -0.099274017 -0.11582644 -0.13485752 -0.15249252 -0.16806804 -0.18175036 -0.19000313 -0.19042325 -0.1825297][-0.10745416 -0.11887628 -0.12485418 -0.13033879 -0.13594055 -0.14238736 -0.15036646 -0.15921061 -0.16739008 -0.17386429 -0.17873903 -0.18187302 -0.18159336 -0.17645848 -0.16630989]]...]
INFO - root - 2017-12-10 15:09:44.990599: step 26210, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 66h:48m:37s remains)
INFO - root - 2017-12-10 15:09:52.865397: step 26220, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.753 sec/batch; 64h:03m:59s remains)
INFO - root - 2017-12-10 15:10:00.558868: step 26230, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 66h:56m:46s remains)
INFO - root - 2017-12-10 15:10:08.501976: step 26240, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.816 sec/batch; 69h:26m:14s remains)
INFO - root - 2017-12-10 15:10:16.358689: step 26250, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 64h:48m:52s remains)
INFO - root - 2017-12-10 15:10:24.146926: step 26260, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 65h:17m:12s remains)
INFO - root - 2017-12-10 15:10:32.063189: step 26270, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.833 sec/batch; 70h:52m:52s remains)
INFO - root - 2017-12-10 15:10:39.747838: step 26280, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 68h:31m:00s remains)
INFO - root - 2017-12-10 15:10:47.605377: step 26290, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 65h:40m:39s remains)
INFO - root - 2017-12-10 15:10:55.458727: step 26300, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 66h:32m:48s remains)
2017-12-10 15:10:56.372951: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.54536915 0.5375 0.50284851 0.459816 0.42104515 0.40320179 0.421471 0.47670865 0.55658579 0.63625073 0.6972267 0.73455316 0.73712188 0.69734514 0.61236626][0.5910126 0.56094265 0.507598 0.45449594 0.41204396 0.39321041 0.41201156 0.4741351 0.56950784 0.66949058 0.74962962 0.79964387 0.80682325 0.76699054 0.67997169][0.56823772 0.5242244 0.46716309 0.41932827 0.38522464 0.37194392 0.38982087 0.44808668 0.54127926 0.6419608 0.7241416 0.775052 0.7828809 0.74723333 0.67192554][0.49883133 0.45834494 0.42030618 0.39954025 0.39264393 0.39918432 0.42320523 0.47343177 0.54837823 0.62583184 0.68466318 0.71533751 0.71143132 0.67889196 0.62425804][0.40023011 0.38407722 0.38855562 0.41757351 0.45887363 0.50316411 0.54607326 0.59090626 0.63794875 0.67216778 0.68377763 0.67334914 0.6451475 0.611372 0.57937777][0.27672005 0.29736122 0.35873836 0.45276177 0.55852836 0.65700561 0.73308879 0.78020352 0.79766273 0.77977079 0.73261994 0.67144996 0.61428291 0.57773381 0.56746805][0.13192357 0.18647765 0.30244264 0.46164805 0.63509524 0.79352236 0.91015863 0.96683913 0.96139675 0.90026653 0.80550176 0.70603037 0.62971789 0.59436852 0.60097992][-0.012581284 0.060767476 0.21313009 0.41925374 0.64428127 0.84967637 0.99978071 1.0683403 1.0543804 0.97615111 0.8652817 0.7571891 0.68126649 0.65202445 0.66407853][-0.10783803 -0.035391849 0.12252004 0.33938736 0.57821381 0.795965 0.95518953 1.0287905 1.0202388 0.95460588 0.864141 0.78005385 0.7239849 0.70362616 0.70790946][-0.12812752 -0.074947469 0.05612421 0.24272262 0.45052236 0.63965213 0.77803594 0.84432286 0.84741372 0.81287491 0.76511931 0.72344071 0.69749981 0.68604082 0.67491168][-0.085060619 -0.062286031 0.019176483 0.1451501 0.28933284 0.42208859 0.521299 0.573462 0.5900203 0.590947 0.5885278 0.58800125 0.58808178 0.58138114 0.55386305][-0.010171815 -0.018903732 0.0073453523 0.064066656 0.13601083 0.20710456 0.2655406 0.3042632 0.33288497 0.36161184 0.39117378 0.41646567 0.43012345 0.42244148 0.38178426][0.048739307 0.016449578 -0.00049579621 0.00072246935 0.014903638 0.037913844 0.065389752 0.094172671 0.12939653 0.17149022 0.21385515 0.24694791 0.26243606 0.2511147 0.20501861][0.051302288 0.0083371457 -0.030296117 -0.058596455 -0.075514145 -0.077899404 -0.06640058 -0.04262697 -0.0063033891 0.036657944 0.077713549 0.10736223 0.11906738 0.10538299 0.061699916][-0.0071386495 -0.048423182 -0.087141559 -0.11898673 -0.14097752 -0.14804237 -0.14034826 -0.11919919 -0.087164477 -0.051655214 -0.019498438 0.0022797319 0.0095136892 -0.0034986078 -0.039013814]]...]
INFO - root - 2017-12-10 15:11:04.131217: step 26310, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 66h:02m:46s remains)
INFO - root - 2017-12-10 15:11:11.872947: step 26320, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 68h:15m:23s remains)
INFO - root - 2017-12-10 15:11:19.614480: step 26330, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 65h:27m:29s remains)
INFO - root - 2017-12-10 15:11:27.468104: step 26340, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 66h:01m:50s remains)
INFO - root - 2017-12-10 15:11:35.219033: step 26350, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 67h:01m:35s remains)
INFO - root - 2017-12-10 15:11:42.810970: step 26360, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 64h:31m:08s remains)
INFO - root - 2017-12-10 15:11:50.810150: step 26370, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 66h:03m:00s remains)
INFO - root - 2017-12-10 15:11:58.630576: step 26380, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 65h:55m:13s remains)
INFO - root - 2017-12-10 15:12:06.485137: step 26390, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.817 sec/batch; 69h:28m:05s remains)
INFO - root - 2017-12-10 15:12:14.297971: step 26400, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 67h:46m:16s remains)
2017-12-10 15:12:15.049778: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.45218918 0.44354361 0.41876346 0.39307106 0.38221011 0.37965363 0.37191194 0.35954985 0.35089877 0.34764454 0.35079116 0.35781232 0.37241888 0.39767691 0.42329419][0.4101519 0.40959772 0.40055886 0.39650521 0.4095839 0.42638797 0.42847824 0.4156459 0.39801297 0.37878218 0.35955366 0.34140608 0.333354 0.34232888 0.35708755][0.31745932 0.33675936 0.35514107 0.3826139 0.42852083 0.47438303 0.49370036 0.48257765 0.45375404 0.41380441 0.3639966 0.3094908 0.2658805 0.24686158 0.24331237][0.23225199 0.27779916 0.32788372 0.38601571 0.46084625 0.5354318 0.57387733 0.56477565 0.52249 0.46009463 0.37811506 0.2823984 0.19510219 0.13931043 0.11274512][0.21663743 0.30129263 0.38835189 0.4711948 0.56120628 0.65207952 0.70178008 0.68848836 0.62813693 0.54386312 0.43580908 0.30256546 0.16915907 0.068933204 0.012032212][0.29648328 0.42664307 0.54771435 0.64247668 0.72813368 0.81623417 0.86544544 0.84219795 0.762631 0.65973842 0.53570455 0.37727615 0.20603575 0.063521944 -0.025717484][0.44217715 0.60304427 0.7403785 0.83061904 0.8964268 0.96900165 1.0139778 0.98511577 0.89464074 0.77968091 0.64688277 0.47414267 0.27799624 0.10462631 -0.010356034][0.57815051 0.74023813 0.86698407 0.93574566 0.97105378 1.0201597 1.0611501 1.0378138 0.95286822 0.83828664 0.70560145 0.52988636 0.32461205 0.13787077 0.011051072][0.61755568 0.75847012 0.85890603 0.90067816 0.90505874 0.92663163 0.95864445 0.94355291 0.87533915 0.77338225 0.65215087 0.48830158 0.29403403 0.11617304 -0.0045755007][0.53113818 0.6402775 0.71166116 0.73208582 0.71711236 0.71826029 0.74100053 0.73435438 0.68725652 0.60543925 0.50397027 0.36490107 0.19948457 0.050029468 -0.04864879][0.34895027 0.41590866 0.45439783 0.45732826 0.43535995 0.42936102 0.44898477 0.45278707 0.4269864 0.3677237 0.28986198 0.18482944 0.063441925 -0.039938 -0.10094045][0.14009295 0.16409382 0.17003541 0.15734245 0.13444355 0.13023512 0.15190129 0.16701676 0.16060957 0.12415405 0.071503013 0.0039731907 -0.066548668 -0.11606085 -0.13275303][-0.031415649 -0.036491379 -0.051897936 -0.075325243 -0.09887784 -0.10292934 -0.083279364 -0.062749535 -0.055028606 -0.069604762 -0.095066734 -0.12374341 -0.14362288 -0.14257887 -0.1204356][-0.13779649 -0.15389572 -0.17431386 -0.19893637 -0.22112837 -0.22825117 -0.21751687 -0.20158255 -0.18971059 -0.18918583 -0.1913825 -0.18564004 -0.16163459 -0.1197426 -0.069061875][-0.1755261 -0.19285932 -0.20974116 -0.22939284 -0.24722202 -0.25606447 -0.25416887 -0.24645613 -0.2375529 -0.23120725 -0.22032829 -0.19243281 -0.13913485 -0.070671968 -0.0024799576]]...]
INFO - root - 2017-12-10 15:12:23.002484: step 26410, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 66h:50m:22s remains)
INFO - root - 2017-12-10 15:12:30.822658: step 26420, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 67h:30m:57s remains)
INFO - root - 2017-12-10 15:12:38.672332: step 26430, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.764 sec/batch; 64h:58m:02s remains)
INFO - root - 2017-12-10 15:12:46.321150: step 26440, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 66h:28m:50s remains)
INFO - root - 2017-12-10 15:12:54.180094: step 26450, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 69h:08m:39s remains)
INFO - root - 2017-12-10 15:13:02.040025: step 26460, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 66h:49m:54s remains)
INFO - root - 2017-12-10 15:13:09.869449: step 26470, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 66h:06m:15s remains)
INFO - root - 2017-12-10 15:13:17.627674: step 26480, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 65h:39m:40s remains)
INFO - root - 2017-12-10 15:13:25.385207: step 26490, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 65h:02m:29s remains)
INFO - root - 2017-12-10 15:13:33.074225: step 26500, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 67h:04m:04s remains)
2017-12-10 15:13:33.981694: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.047601391 0.054246709 0.077671424 0.11234111 0.15026122 0.17822483 0.19163987 0.1948427 0.18884128 0.16902372 0.13243757 0.087636463 0.056035034 0.060246374 0.1003024][-0.0084041068 0.013012574 0.0624559 0.12452591 0.18172261 0.21519808 0.22074148 0.20787223 0.18338764 0.14614125 0.096369758 0.045577206 0.018166298 0.035853397 0.093673453][-0.032921884 0.0038854068 0.076692119 0.16148967 0.23309867 0.26879165 0.2659438 0.2405031 0.20520326 0.16001093 0.10499352 0.051945023 0.026334673 0.047813676 0.10728455][-0.029477945 0.017774018 0.10460971 0.20290065 0.28302667 0.32135564 0.31859237 0.29502541 0.26376173 0.22097462 0.1641544 0.10653287 0.075747252 0.090232231 0.13894054][-0.017779687 0.032888323 0.12314593 0.22500925 0.3089084 0.35342383 0.36222297 0.35502547 0.33867776 0.30279723 0.24248384 0.17510344 0.13368222 0.13652843 0.17122033][-0.0032860262 0.043470412 0.12603997 0.22021165 0.30194446 0.35518166 0.38413674 0.40079436 0.40120283 0.37028515 0.30428708 0.22721742 0.17737928 0.17198041 0.19620587][0.017332535 0.055175669 0.12112669 0.19817747 0.27191395 0.3329393 0.38272488 0.4200671 0.43056032 0.39946792 0.32921541 0.25018775 0.20132382 0.19486742 0.21305364][0.0502121 0.080487922 0.12768768 0.18414298 0.24575078 0.30828011 0.36816755 0.41157851 0.4188104 0.38130856 0.31081396 0.24086112 0.20408024 0.20425513 0.22172633][0.10275836 0.12889946 0.16038932 0.19744372 0.24326624 0.29674697 0.35038418 0.38378164 0.37817672 0.33380994 0.26985785 0.21682087 0.19628726 0.20396706 0.22186717][0.17268956 0.19834781 0.2192118 0.24061073 0.26950103 0.3061921 0.34204337 0.35594624 0.3343069 0.28557518 0.23162113 0.1951877 0.18587972 0.19486076 0.2091147][0.24665372 0.27299583 0.28737062 0.29776764 0.31278065 0.33329195 0.35072 0.3451817 0.30910677 0.2569674 0.2104734 0.18397465 0.17712243 0.18036081 0.18658508][0.29577389 0.32118642 0.33373511 0.34194297 0.35386321 0.36851045 0.37549695 0.3561523 0.30836588 0.25080141 0.20483635 0.17955968 0.16923633 0.16479683 0.16250294][0.29609713 0.31973135 0.33603272 0.35147315 0.37128338 0.3902947 0.39448652 0.36691409 0.31020862 0.24620254 0.19702849 0.1702204 0.15864667 0.15258114 0.14779274][0.25508955 0.27716896 0.29721916 0.31891841 0.345208 0.3689366 0.37458634 0.3476533 0.29264829 0.23106007 0.18480372 0.16165258 0.15500587 0.15401304 0.15157886][0.19636866 0.21878007 0.23854628 0.25695464 0.27777267 0.29684854 0.30203363 0.28232911 0.24135035 0.19606169 0.16493998 0.1551732 0.16033298 0.16826938 0.16908881]]...]
INFO - root - 2017-12-10 15:13:41.859692: step 26510, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 66h:05m:35s remains)
INFO - root - 2017-12-10 15:13:49.468209: step 26520, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 67h:24m:41s remains)
INFO - root - 2017-12-10 15:13:57.554307: step 26530, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 68h:06m:22s remains)
INFO - root - 2017-12-10 15:14:05.448715: step 26540, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 66h:03m:12s remains)
INFO - root - 2017-12-10 15:14:13.244861: step 26550, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.822 sec/batch; 69h:49m:24s remains)
INFO - root - 2017-12-10 15:14:21.094520: step 26560, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.790 sec/batch; 67h:09m:12s remains)
INFO - root - 2017-12-10 15:14:28.841123: step 26570, loss = 0.70, batch loss = 0.65 (10.6 examples/sec; 0.753 sec/batch; 63h:58m:33s remains)
INFO - root - 2017-12-10 15:14:36.650472: step 26580, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 66h:12m:05s remains)
INFO - root - 2017-12-10 15:14:44.362184: step 26590, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 66h:10m:00s remains)
INFO - root - 2017-12-10 15:14:52.061866: step 26600, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 67h:00m:56s remains)
2017-12-10 15:14:52.935544: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.092271931 0.088647358 0.077689774 0.063818313 0.054056287 0.05458913 0.068331331 0.090850331 0.11498464 0.13348231 0.14103605 0.13552806 0.11916755 0.10027208 0.090856589][0.063080192 0.0713256 0.07278841 0.068496495 0.0632514 0.062230535 0.07106185 0.088316336 0.10987812 0.12862812 0.13857222 0.13587037 0.12135087 0.10164151 0.089206167][0.0453989 0.070927925 0.09305346 0.10839838 0.11696186 0.12155193 0.12918489 0.14053003 0.15550315 0.16898043 0.17597917 0.17060551 0.15192157 0.12572032 0.10533363][0.048990835 0.098035745 0.14935668 0.19482261 0.22764258 0.24707468 0.25920388 0.26524708 0.27001411 0.27277014 0.27173182 0.25908065 0.23163423 0.19496812 0.1635963][0.061702654 0.13612103 0.22118109 0.30381122 0.36952186 0.41283354 0.43592155 0.43901178 0.4319084 0.42102724 0.41071609 0.39148885 0.3565287 0.31068489 0.26987898][0.072961569 0.16758579 0.28188619 0.39825392 0.495532 0.56377143 0.5995943 0.601637 0.58375943 0.56150329 0.54644775 0.52710575 0.4916423 0.44400883 0.40164271][0.085603878 0.18781787 0.315712 0.44910195 0.56307018 0.64478695 0.68668807 0.68664008 0.66131777 0.63441712 0.62283438 0.6122843 0.58518058 0.54466975 0.5093143][0.10191456 0.19131327 0.30766168 0.43263841 0.54098904 0.61858559 0.65638286 0.65300107 0.6255458 0.60129118 0.59882104 0.60160744 0.58821827 0.56093228 0.53825647][0.12174667 0.17598554 0.25402388 0.34564897 0.42856574 0.48738179 0.51352471 0.50732619 0.48354051 0.46693739 0.47426096 0.48868 0.48838988 0.47487935 0.46518233][0.14408317 0.15086894 0.17611568 0.22122206 0.26920405 0.30354831 0.31699815 0.310633 0.294047 0.28587824 0.29898152 0.3185991 0.32564229 0.32124788 0.32001215][0.16222285 0.12766045 0.10531055 0.10781343 0.12428941 0.1379931 0.14284623 0.13907519 0.13044642 0.12786779 0.14015666 0.15676495 0.16369431 0.16190164 0.16314089][0.15817368 0.1014507 0.053091418 0.031919573 0.03162691 0.035903368 0.038843408 0.038984008 0.035695821 0.033174135 0.03772499 0.044330485 0.045390774 0.041959789 0.042598665][0.11825143 0.062624104 0.015157541 -0.0064948383 -0.0068058264 -0.000855814 0.0049556145 0.0080549 0.0060870233 -0.00019734909 -0.0053428216 -0.0095799416 -0.014899262 -0.019971399 -0.01900718][0.058359645 0.020329144 -0.0075170132 -0.014528121 -0.0054972926 0.0066070743 0.015400814 0.01833239 0.013999103 0.0030249639 -0.0090601882 -0.019690216 -0.027350832 -0.030224534 -0.024817841][0.0070036235 -0.0079826117 -0.010929555 -0.00024600985 0.018073833 0.034274846 0.043123171 0.042958904 0.034483559 0.019960893 0.0052009 -0.0061254515 -0.011001424 -0.0075332476 0.0056150756]]...]
INFO - root - 2017-12-10 15:15:00.774176: step 26610, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 66h:39m:08s remains)
INFO - root - 2017-12-10 15:15:08.574823: step 26620, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 66h:26m:47s remains)
INFO - root - 2017-12-10 15:15:16.424331: step 26630, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 66h:06m:34s remains)
INFO - root - 2017-12-10 15:15:24.304096: step 26640, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 64h:15m:17s remains)
INFO - root - 2017-12-10 15:15:32.108407: step 26650, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 67h:31m:43s remains)
INFO - root - 2017-12-10 15:15:39.949651: step 26660, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 65h:14m:34s remains)
INFO - root - 2017-12-10 15:15:47.917684: step 26670, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 67h:16m:56s remains)
INFO - root - 2017-12-10 15:15:55.333671: step 26680, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.753 sec/batch; 63h:57m:12s remains)
INFO - root - 2017-12-10 15:16:03.207029: step 26690, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 66h:20m:44s remains)
INFO - root - 2017-12-10 15:16:11.078806: step 26700, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 67h:31m:37s remains)
2017-12-10 15:16:11.967179: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00581815 0.0021953864 0.0077527566 0.010343309 0.012263401 0.013452516 0.013387352 0.011811175 0.00937543 0.007482816 0.0071683819 0.0093351882 0.012334698 0.011336041 0.002728943][0.021089444 0.031237984 0.037661593 0.04161207 0.047450725 0.055208772 0.062822931 0.067885436 0.069514848 0.06916222 0.068153434 0.068584494 0.068634152 0.062550783 0.04649023][0.064846039 0.077686734 0.084729731 0.090036124 0.10084575 0.11801646 0.13776074 0.15358958 0.16127914 0.16174632 0.15741053 0.15236223 0.144895 0.12975986 0.10367928][0.11096527 0.12658177 0.13419327 0.14105043 0.15759681 0.18594627 0.21993409 0.24773695 0.26057553 0.25919116 0.24842073 0.23439956 0.21619581 0.19009061 0.15366358][0.15483733 0.17307246 0.18092768 0.18909954 0.21110599 0.24998601 0.2957879 0.33133802 0.34406924 0.3360475 0.31636894 0.29319403 0.26636976 0.2327746 0.18982701][0.20754103 0.23098792 0.23934418 0.24706987 0.27105922 0.31597605 0.36716738 0.40284008 0.40852684 0.389262 0.36000326 0.33080098 0.3019098 0.26872212 0.22572997][0.27294916 0.30721173 0.31923136 0.32654908 0.35006323 0.39758918 0.44998732 0.48077822 0.47427309 0.43887857 0.39576948 0.35810572 0.32834083 0.3002429 0.26247483][0.33369496 0.38214448 0.39997482 0.40655354 0.42616096 0.4706116 0.51852483 0.54056984 0.52032703 0.46869048 0.41129556 0.36459997 0.33491904 0.31463668 0.2854073][0.37157083 0.42950231 0.44981781 0.45321095 0.4652203 0.49951285 0.53488189 0.54360819 0.51167947 0.45100814 0.38810641 0.34085661 0.31816423 0.30981776 0.29097927][0.38282207 0.44037956 0.45608416 0.45243725 0.45438045 0.47443947 0.49210432 0.48427993 0.44333044 0.38169262 0.32416934 0.28785831 0.28080365 0.288722 0.28186098][0.35700664 0.40654063 0.41297206 0.40071583 0.39395157 0.40325254 0.40828374 0.3898049 0.34656593 0.29076275 0.2437475 0.22079283 0.22799258 0.24809846 0.24965501][0.28793365 0.32606873 0.32417831 0.30684978 0.29657125 0.30180851 0.3030211 0.28348073 0.24493338 0.19805832 0.16004424 0.1449798 0.15809646 0.18200983 0.18737708][0.18933828 0.21264963 0.20389163 0.18498465 0.17454322 0.17786641 0.17822577 0.16220139 0.1323169 0.097139649 0.069209643 0.060956616 0.07666672 0.10029862 0.1075183][0.08821509 0.095034868 0.081617832 0.064356267 0.055874553 0.057645116 0.057304285 0.045921322 0.026410639 0.0050212671 -0.010466109 -0.01091959 0.006273848 0.027652029 0.03561784][0.0019988634 -0.0037571336 -0.017888581 -0.030647742 -0.035694096 -0.03410396 -0.034057625 -0.040166311 -0.049760684 -0.059020534 -0.06394165 -0.059196774 -0.04385373 -0.027532423 -0.020684974]]...]
INFO - root - 2017-12-10 15:16:19.808591: step 26710, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 66h:15m:10s remains)
INFO - root - 2017-12-10 15:16:27.633255: step 26720, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 67h:29m:52s remains)
INFO - root - 2017-12-10 15:16:35.520499: step 26730, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 65h:58m:47s remains)
INFO - root - 2017-12-10 15:16:43.373816: step 26740, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 67h:52m:28s remains)
INFO - root - 2017-12-10 15:16:51.231948: step 26750, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 67h:29m:04s remains)
INFO - root - 2017-12-10 15:16:58.856718: step 26760, loss = 0.69, batch loss = 0.63 (12.6 examples/sec; 0.634 sec/batch; 53h:51m:24s remains)
INFO - root - 2017-12-10 15:17:06.909191: step 26770, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.827 sec/batch; 70h:13m:11s remains)
INFO - root - 2017-12-10 15:17:14.770064: step 26780, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 66h:50m:04s remains)
INFO - root - 2017-12-10 15:17:22.647683: step 26790, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 67h:13m:59s remains)
INFO - root - 2017-12-10 15:17:30.507154: step 26800, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 66h:37m:06s remains)
2017-12-10 15:17:31.328250: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.016701646 0.0667207 0.12902965 0.19183905 0.2402619 0.26572451 0.26574677 0.2463619 0.21754646 0.19579408 0.19585243 0.22571167 0.28519475 0.35888937 0.4313564][0.01480983 0.064911477 0.12911433 0.19740729 0.25559318 0.29346669 0.30730969 0.30112159 0.28230873 0.26716641 0.27166057 0.30399635 0.36069551 0.42404908 0.48299634][0.012758744 0.0629984 0.13047698 0.20755786 0.28053334 0.33611709 0.36768255 0.37601125 0.36466867 0.34925076 0.34758016 0.3705624 0.4142077 0.46080375 0.50381172][0.01275557 0.065445647 0.139284 0.22801493 0.31754553 0.39101729 0.43925223 0.46041286 0.45396808 0.43339497 0.41751027 0.42188138 0.44535792 0.47281131 0.50094044][0.013748536 0.070654653 0.15257274 0.25353602 0.35894388 0.44894943 0.51235914 0.54449213 0.54090518 0.51219326 0.47752243 0.45850432 0.45972824 0.47012219 0.48683083][0.013547333 0.075549126 0.16684774 0.28208789 0.40613034 0.51525331 0.59320706 0.629287 0.61730009 0.57007557 0.51140273 0.46930495 0.4530817 0.45287809 0.46216244][0.014124162 0.083167367 0.18799187 0.3236244 0.47336313 0.60692865 0.6972968 0.72418314 0.68352163 0.59913313 0.5061444 0.43940437 0.4107165 0.4072527 0.4146927][0.017977754 0.096694469 0.21977909 0.38070688 0.55875039 0.71493608 0.80932808 0.81321859 0.72878993 0.59633267 0.46482825 0.37365711 0.3337377 0.32705668 0.33190471][0.021335969 0.10903309 0.24945275 0.43196493 0.62902075 0.79394674 0.87811273 0.85084391 0.72359711 0.55159122 0.39339271 0.2882646 0.24360453 0.23741141 0.2438383][0.02226294 0.11545637 0.26743162 0.46124098 0.66101259 0.81607777 0.87720609 0.81953263 0.66361707 0.47185278 0.30441743 0.19670968 0.15317614 0.15124853 0.16466913][0.020254189 0.11285138 0.26569179 0.45537782 0.63962203 0.76896417 0.80073053 0.72038621 0.55435038 0.36422887 0.20596784 0.10984951 0.077210419 0.085469045 0.10953125][0.013391793 0.097596347 0.23718584 0.40513971 0.55846941 0.65435618 0.660642 0.57307768 0.41896006 0.25339636 0.12472039 0.057027988 0.046679605 0.069942586 0.10397767][0.0024607012 0.0731778 0.19068404 0.32833356 0.448001 0.51605982 0.51061219 0.4336108 0.30920351 0.18304153 0.094078451 0.0591373 0.069617867 0.10222588 0.13958025][-0.0092925113 0.047784664 0.14363042 0.25554782 0.35283118 0.40991348 0.41089594 0.35864475 0.27164587 0.18638447 0.13331048 0.12323099 0.14495039 0.17861836 0.21519104][-0.016535325 0.03231328 0.11681636 0.21957402 0.31579548 0.38321033 0.40583798 0.38367218 0.32866502 0.27099586 0.23646528 0.2340474 0.25286692 0.27787486 0.30854872]]...]
INFO - root - 2017-12-10 15:17:39.136925: step 26810, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.789 sec/batch; 67h:02m:18s remains)
INFO - root - 2017-12-10 15:17:46.924462: step 26820, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.758 sec/batch; 64h:23m:15s remains)
INFO - root - 2017-12-10 15:17:54.746368: step 26830, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 68h:20m:27s remains)
INFO - root - 2017-12-10 15:18:02.477441: step 26840, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 68h:06m:17s remains)
INFO - root - 2017-12-10 15:18:10.152018: step 26850, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 65h:28m:11s remains)
INFO - root - 2017-12-10 15:18:18.061205: step 26860, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 66h:27m:48s remains)
INFO - root - 2017-12-10 15:18:25.951213: step 26870, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 65h:48m:24s remains)
INFO - root - 2017-12-10 15:18:33.895792: step 26880, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 66h:04m:31s remains)
INFO - root - 2017-12-10 15:18:41.796092: step 26890, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 67h:38m:18s remains)
INFO - root - 2017-12-10 15:18:49.644026: step 26900, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 65h:54m:44s remains)
2017-12-10 15:18:50.494749: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35811278 0.44318295 0.48630586 0.49075532 0.47057351 0.46136054 0.48368311 0.5198766 0.537799 0.52567828 0.48072171 0.4076409 0.32593715 0.26661712 0.25109568][0.32521132 0.41425857 0.46704611 0.48239186 0.4685677 0.46043247 0.48339617 0.52564806 0.55863583 0.57030481 0.55094647 0.4921301 0.40879774 0.33765066 0.30980512][0.29737711 0.38416907 0.43843764 0.45894119 0.45135975 0.44550979 0.46688083 0.51137525 0.5563339 0.59096277 0.59828287 0.55806321 0.4775658 0.39663869 0.35298669][0.29743528 0.37858573 0.42623428 0.4430967 0.43778336 0.43527162 0.45646018 0.50219256 0.55437416 0.60311127 0.62546647 0.5956201 0.51654541 0.42793795 0.36975485][0.32359514 0.39746106 0.43381381 0.44117311 0.43549231 0.43913049 0.46487564 0.51130313 0.56227356 0.60772222 0.62298113 0.58630824 0.50205946 0.40706787 0.34005529][0.36470619 0.43258089 0.45865512 0.45843509 0.45525753 0.46941262 0.50331658 0.54743886 0.58490252 0.60534096 0.59067476 0.53196716 0.43944216 0.34426478 0.27802962][0.40801382 0.47176918 0.48900938 0.48333442 0.48322287 0.507416 0.54977238 0.59012932 0.60622776 0.58747303 0.52993351 0.44251668 0.34329191 0.25594759 0.19994578][0.4421556 0.50300664 0.51221162 0.50046217 0.50093365 0.53113818 0.57871109 0.61425465 0.60701114 0.54784548 0.45048973 0.34150642 0.24491479 0.17502858 0.13749497][0.463971 0.527595 0.532362 0.51249111 0.50603658 0.53112036 0.57417643 0.60141128 0.57427996 0.48624203 0.36841953 0.26033473 0.18645194 0.14934516 0.14057086][0.47980446 0.55400878 0.55970633 0.52825493 0.50167656 0.50247085 0.52474105 0.53840423 0.5016036 0.40878409 0.30298227 0.22692992 0.1974842 0.20500726 0.23022451][0.49180698 0.57940155 0.58943129 0.54482716 0.48899081 0.451926 0.44206074 0.43934086 0.40521058 0.3325415 0.26616961 0.24270171 0.26777956 0.32218051 0.37987107][0.49273661 0.59106839 0.60803396 0.55479127 0.4717409 0.39638987 0.35270226 0.33386648 0.31028828 0.27143228 0.25424331 0.28380603 0.35490683 0.4429433 0.52032584][0.46191934 0.56300926 0.5862295 0.5314737 0.43264857 0.33194482 0.2644994 0.23445325 0.22254179 0.21562254 0.23798914 0.30284664 0.39794052 0.49839723 0.5790062][0.37919125 0.4733589 0.50059432 0.4515622 0.35278708 0.24644347 0.17189957 0.13867363 0.13563594 0.14954495 0.19257261 0.26844898 0.36269391 0.45463887 0.52374023][0.25360006 0.33178318 0.35849434 0.31854743 0.23188917 0.13621965 0.068308182 0.037802294 0.038452614 0.059728596 0.10487381 0.17118095 0.24569447 0.31439412 0.36226326]]...]
INFO - root - 2017-12-10 15:18:58.349540: step 26910, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 67h:08m:44s remains)
INFO - root - 2017-12-10 15:19:06.215022: step 26920, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 67h:04m:24s remains)
INFO - root - 2017-12-10 15:19:14.091112: step 26930, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 68h:34m:27s remains)
INFO - root - 2017-12-10 15:19:21.647928: step 26940, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.754 sec/batch; 63h:59m:29s remains)
INFO - root - 2017-12-10 15:19:29.607032: step 26950, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 68h:13m:13s remains)
INFO - root - 2017-12-10 15:19:37.475744: step 26960, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 66h:10m:24s remains)
INFO - root - 2017-12-10 15:19:45.477926: step 26970, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 66h:21m:38s remains)
INFO - root - 2017-12-10 15:19:53.372766: step 26980, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.803 sec/batch; 68h:08m:49s remains)
INFO - root - 2017-12-10 15:20:01.446930: step 26990, loss = 0.69, batch loss = 0.64 (9.6 examples/sec; 0.830 sec/batch; 70h:27m:58s remains)
INFO - root - 2017-12-10 15:20:09.168612: step 27000, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 68h:05m:16s remains)
2017-12-10 15:20:09.976861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0093679968 0.013050656 0.031223336 0.041315503 0.042314537 0.03426211 0.01689944 0.0010249721 -0.0075924476 -0.01083418 -0.012316819 -0.015622598 -0.019030714 -0.028326573 -0.047388926][0.038857561 0.08235433 0.12468637 0.16048573 0.18101484 0.18035142 0.15768509 0.13009134 0.10601044 0.084081315 0.066561162 0.0525992 0.045614079 0.032110766 0.0013377381][0.099156074 0.16841568 0.2402178 0.30721748 0.3546069 0.36805737 0.34271628 0.30114135 0.25460848 0.20640425 0.16581962 0.13692367 0.12537716 0.10871348 0.068569377][0.15223509 0.24847917 0.35168546 0.4515101 0.5302965 0.56474984 0.542583 0.48839253 0.41656324 0.33810928 0.269865 0.22234651 0.20279041 0.18177576 0.13462333][0.18519664 0.30834523 0.44352809 0.57524484 0.68461806 0.74182659 0.72644311 0.66104239 0.56381434 0.45613518 0.36190948 0.29608706 0.26633576 0.2398532 0.18799494][0.20506474 0.3537406 0.51777345 0.67355376 0.80225641 0.87271696 0.8590557 0.78049928 0.66000086 0.52968729 0.417923 0.34077016 0.30455568 0.27554339 0.22287318][0.2177145 0.38493949 0.5690794 0.73851836 0.87518281 0.9496783 0.93502533 0.84919995 0.71856415 0.57983375 0.46037835 0.37530196 0.33104044 0.29759943 0.24448259][0.21544442 0.3913303 0.58597589 0.76142132 0.90000188 0.97195393 0.953049 0.86541951 0.74014604 0.61145461 0.49594596 0.40543464 0.34864655 0.30396336 0.24573763][0.19375436 0.36741298 0.56430727 0.74265957 0.88323879 0.95276403 0.93087208 0.84671867 0.7363897 0.6281603 0.52412039 0.43179366 0.36179727 0.30114728 0.23252863][0.16203701 0.32316139 0.51250356 0.68786144 0.82728451 0.89396709 0.87071526 0.79236859 0.69847828 0.61249334 0.5252533 0.43911096 0.3633973 0.2899828 0.21105032][0.12650181 0.26588556 0.4350841 0.59563428 0.72532403 0.78737319 0.76692814 0.69915551 0.62220353 0.55603558 0.48676729 0.41219327 0.3384499 0.25979105 0.17630412][0.08132191 0.19124326 0.32988086 0.46570066 0.57945025 0.63690072 0.62535626 0.57530725 0.51790333 0.46888271 0.414532 0.35152328 0.28374928 0.20663528 0.12556253][0.02461474 0.10063621 0.2025594 0.30627558 0.39739469 0.44642866 0.44277713 0.40962893 0.3685568 0.33243838 0.29020718 0.23963584 0.18354654 0.11790431 0.050426241][-0.029124292 0.014808644 0.07978446 0.14829053 0.21105179 0.24578924 0.24485622 0.22386768 0.19514756 0.16936003 0.13903181 0.1034468 0.064802051 0.018876137 -0.026664315][-0.065574162 -0.045674615 -0.011160906 0.025000235 0.057980362 0.074161671 0.06980297 0.055202268 0.036427885 0.020997891 0.0034874799 -0.016870305 -0.0381736 -0.063978173 -0.087746479]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 15:20:17.787855: step 27010, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 65h:36m:30s remains)
INFO - root - 2017-12-10 15:20:25.692485: step 27020, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.773 sec/batch; 65h:37m:12s remains)
INFO - root - 2017-12-10 15:20:33.350905: step 27030, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.772 sec/batch; 65h:29m:18s remains)
INFO - root - 2017-12-10 15:20:41.305017: step 27040, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 65h:26m:51s remains)
INFO - root - 2017-12-10 15:20:49.193994: step 27050, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 67h:04m:14s remains)
INFO - root - 2017-12-10 15:20:56.995903: step 27060, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 65h:21m:14s remains)
INFO - root - 2017-12-10 15:21:04.854432: step 27070, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 65h:39m:09s remains)
INFO - root - 2017-12-10 15:21:12.509498: step 27080, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.785 sec/batch; 66h:34m:38s remains)
INFO - root - 2017-12-10 15:21:20.468315: step 27090, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 68h:07m:05s remains)
INFO - root - 2017-12-10 15:21:28.317380: step 27100, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 65h:50m:06s remains)
2017-12-10 15:21:29.167764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.038957186 -0.041356191 -0.041573796 -0.040149987 -0.037908036 -0.0356207 -0.033939995 -0.0329143 -0.032502893 -0.032983609 -0.033707488 -0.034257408 -0.034874573 -0.035346173 -0.03566334][-0.017298318 -0.019934697 -0.019421577 -0.015641393 -0.0094290469 -0.0021518853 0.0040773116 0.0080834236 0.0098730531 0.0093523469 0.007657724 0.0053623128 0.0020678453 -0.001535565 -0.0052302838][0.0169967 0.016082028 0.018868795 0.026585864 0.038802672 0.054515276 0.069188528 0.07832779 0.081598468 0.080132447 0.075881258 0.068452358 0.057516865 0.046460651 0.036535066][0.0591856 0.061280917 0.067410372 0.080349162 0.10021296 0.12701365 0.15296324 0.16836061 0.17189693 0.16670831 0.15653482 0.13986608 0.11656943 0.094273031 0.076226048][0.10410724 0.109905 0.1198769 0.13808903 0.16476154 0.20111027 0.23618279 0.25524983 0.25617042 0.24450193 0.22593412 0.19833216 0.16247673 0.13010103 0.10599216][0.14217305 0.15173976 0.16546029 0.18828337 0.21920396 0.25977045 0.29719844 0.31428853 0.30875283 0.28832424 0.26078478 0.22462724 0.18133366 0.14470801 0.11973493][0.16799751 0.18065336 0.19644694 0.22153772 0.25317356 0.29194856 0.32407588 0.33308384 0.31788087 0.28801709 0.25304419 0.21258666 0.16879004 0.13544023 0.11640416][0.18107146 0.19658025 0.21237411 0.23666023 0.26521769 0.29703492 0.3179695 0.315086 0.28982326 0.25286 0.21444903 0.1748355 0.13696748 0.11352326 0.10545668][0.18266147 0.20152111 0.21707389 0.23900217 0.26296106 0.28646645 0.29531828 0.28160858 0.24940941 0.20963074 0.17255259 0.13885173 0.11127787 0.099213995 0.10035247][0.16877121 0.19187498 0.20906635 0.22992443 0.25069875 0.26795056 0.26840192 0.24836613 0.21381257 0.17521623 0.14293699 0.11804566 0.10143714 0.0978778 0.10282149][0.14923218 0.174458 0.19346601 0.21507582 0.23597598 0.2519151 0.25118336 0.23259404 0.20146304 0.16663581 0.13904326 0.12072376 0.11047468 0.10922655 0.11207635][0.13751498 0.16107905 0.17879383 0.20018601 0.22249357 0.24095027 0.24571872 0.23603386 0.21459596 0.18695304 0.16357663 0.14778742 0.13768929 0.13261257 0.12894125][0.13918608 0.15699467 0.16947414 0.18752739 0.20866458 0.22837302 0.24007869 0.24290468 0.23524475 0.21804981 0.20017134 0.18601663 0.17424335 0.16397522 0.15347387][0.15167075 0.16179933 0.16575924 0.17638636 0.19191907 0.20964254 0.22705762 0.24244834 0.24939638 0.24458665 0.23426302 0.222486 0.20845063 0.19301304 0.17705865][0.17661753 0.17970429 0.17276591 0.17158128 0.17578419 0.1858805 0.20407066 0.22814345 0.24706353 0.25402889 0.252693 0.24490741 0.22958894 0.21063912 0.19255044]]...]
INFO - root - 2017-12-10 15:21:36.974491: step 27110, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 67h:52m:06s remains)
INFO - root - 2017-12-10 15:21:44.759193: step 27120, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 68h:10m:47s remains)
INFO - root - 2017-12-10 15:21:52.549141: step 27130, loss = 0.69, batch loss = 0.63 (10.7 examples/sec; 0.747 sec/batch; 63h:23m:53s remains)
INFO - root - 2017-12-10 15:22:00.410803: step 27140, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 65h:14m:24s remains)
INFO - root - 2017-12-10 15:22:08.248926: step 27150, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.797 sec/batch; 67h:34m:46s remains)
INFO - root - 2017-12-10 15:22:16.133243: step 27160, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 66h:57m:43s remains)
INFO - root - 2017-12-10 15:22:23.975059: step 27170, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 68h:07m:07s remains)
INFO - root - 2017-12-10 15:22:31.988418: step 27180, loss = 0.70, batch loss = 0.64 (9.0 examples/sec; 0.894 sec/batch; 75h:48m:15s remains)
INFO - root - 2017-12-10 15:22:39.881593: step 27190, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 65h:25m:59s remains)
INFO - root - 2017-12-10 15:22:47.885059: step 27200, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 65h:42m:44s remains)
2017-12-10 15:22:48.735165: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0062256474 -0.0021181984 -0.0089318687 -0.01221974 -0.012792878 -0.012852469 -0.014490657 -0.017375946 -0.020156113 -0.021697717 -0.019741978 -0.013221025 -0.0031153455 0.0075633684 0.015706619][0.020124523 0.0068904525 -0.0018482009 -0.0037774574 -0.0019269825 -0.00051166688 -0.0037357728 -0.011727709 -0.021203535 -0.028279774 -0.028393693 -0.020104108 -0.0060794037 0.0083889989 0.018158387][0.030749036 0.017208802 0.01401371 0.021568695 0.032225281 0.038468026 0.033746839 0.01710804 -0.0052492372 -0.024790289 -0.032309413 -0.025288867 -0.0095455889 0.0064461567 0.016029285][0.035956647 0.028021574 0.038546 0.063979 0.090469077 0.1063852 0.10200924 0.07501331 0.035018615 -0.002945808 -0.023136465 -0.021437142 -0.0070499778 0.0077054659 0.014828557][0.036708303 0.039184794 0.069715068 0.1198528 0.16971272 0.20309576 0.20566091 0.17092364 0.11035976 0.047184594 0.0068580485 -0.0024724579 0.0067616543 0.017497502 0.020186402][0.034672536 0.048757773 0.099757023 0.1748127 0.25009021 0.30547535 0.32086378 0.28295785 0.20271824 0.1125246 0.048590217 0.024706101 0.026154248 0.031082407 0.029028993][0.032621741 0.056321278 0.12297513 0.21704668 0.31289762 0.38800052 0.41760784 0.38114014 0.28696761 0.17497712 0.090550557 0.053231794 0.047076177 0.046418726 0.039939653][0.033571176 0.065027937 0.1416733 0.24705857 0.35536835 0.442654 0.48141989 0.44707003 0.34602681 0.22393809 0.13155606 0.090788238 0.083543383 0.07889092 0.064587288][0.036582597 0.073392853 0.15415581 0.26308417 0.37526038 0.46542972 0.50558627 0.4723531 0.37317064 0.25646284 0.17318459 0.14294514 0.14364851 0.13735355 0.11093234][0.039579149 0.077716559 0.15579908 0.2605148 0.36911675 0.45544842 0.4936167 0.46575174 0.3816908 0.28769031 0.22959748 0.22097486 0.23552553 0.22854097 0.18623178][0.043800991 0.078869067 0.14747442 0.24051526 0.33899641 0.41697237 0.45272097 0.43621212 0.38003466 0.32367 0.30191213 0.31955618 0.34755182 0.33884218 0.28114209][0.053418078 0.0830981 0.137563 0.21324228 0.29601145 0.36246797 0.39637503 0.39542267 0.3741073 0.36156672 0.37797359 0.41879612 0.4555707 0.44526869 0.37818846][0.077302076 0.10307559 0.1422801 0.19688712 0.25930771 0.31179339 0.34344149 0.35696822 0.36726287 0.39305502 0.44054461 0.49698463 0.537497 0.52767283 0.46051577][0.104249 0.12821244 0.15284853 0.18487598 0.22409746 0.26028615 0.28742611 0.31021029 0.3422538 0.39296111 0.45731923 0.51817787 0.55661345 0.54902381 0.49080208][0.13037802 0.15377381 0.16563822 0.1752588 0.18980637 0.2075128 0.22665748 0.25095129 0.29165748 0.35148957 0.41807467 0.4733516 0.5052017 0.501274 0.45877877]]...]
INFO - root - 2017-12-10 15:22:56.357301: step 27210, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.767 sec/batch; 65h:01m:49s remains)
INFO - root - 2017-12-10 15:23:04.167147: step 27220, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 66h:03m:05s remains)
INFO - root - 2017-12-10 15:23:12.064650: step 27230, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.759 sec/batch; 64h:21m:58s remains)
INFO - root - 2017-12-10 15:23:19.672172: step 27240, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 65h:37m:34s remains)
INFO - root - 2017-12-10 15:23:27.509186: step 27250, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 64h:58m:51s remains)
INFO - root - 2017-12-10 15:23:35.364701: step 27260, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 67h:00m:08s remains)
INFO - root - 2017-12-10 15:23:43.201954: step 27270, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 67h:02m:18s remains)
INFO - root - 2017-12-10 15:23:51.054692: step 27280, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 64h:59m:58s remains)
INFO - root - 2017-12-10 15:23:58.960797: step 27290, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 68h:34m:36s remains)
INFO - root - 2017-12-10 15:24:06.636561: step 27300, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 65h:23m:03s remains)
2017-12-10 15:24:07.540849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0076272492 0.0070963483 0.028994966 0.058290415 0.083420947 0.10147498 0.11169607 0.11264107 0.099643856 0.073884152 0.044289164 0.015164714 0.00013600636 -0.0025159912 0.013785913][-0.024116738 -0.023239722 -0.01336098 0.0085507194 0.031427041 0.050977442 0.065227374 0.070086323 0.060056962 0.036806293 0.010619452 -0.012939159 -0.024253543 -0.024758028 -0.0060495171][0.017617855 0.0068608709 0.0036518632 0.015288821 0.031679168 0.048039939 0.063060425 0.070982106 0.064413965 0.044857178 0.0244728 0.0090699038 0.0028409874 0.0046796477 0.025466552][0.12627101 0.10983805 0.096083015 0.099324249 0.10957246 0.12182754 0.13750613 0.14949457 0.14606193 0.12655485 0.10677141 0.093815096 0.085361764 0.0815812 0.0987924][0.27943861 0.26402998 0.24655648 0.24823084 0.25742012 0.26856893 0.28690121 0.30338258 0.30002391 0.27270195 0.24340686 0.22215395 0.20039137 0.17968595 0.18454728][0.43556985 0.42730364 0.41249177 0.41784608 0.42984769 0.44258896 0.46487111 0.48565635 0.48028657 0.44002065 0.39255673 0.35307974 0.30888388 0.2622956 0.2464332][0.52917528 0.53166544 0.52299881 0.53503788 0.55358487 0.57112026 0.59729111 0.6208334 0.61328274 0.56141341 0.49501908 0.43523261 0.36821878 0.29611221 0.25724974][0.52916676 0.53847718 0.53456682 0.54967505 0.57096714 0.5902459 0.61560559 0.63683844 0.62672204 0.56924152 0.49162406 0.41825986 0.33730814 0.2508302 0.19757749][0.46673977 0.46836215 0.45607677 0.45874128 0.46857166 0.4795939 0.497416 0.51312965 0.50364619 0.45156416 0.37727931 0.30452481 0.22591114 0.14404099 0.093631856][0.40233481 0.38279378 0.34605506 0.31877542 0.3020823 0.2955865 0.3016769 0.312314 0.308644 0.27315927 0.21708742 0.16006908 0.099628523 0.038915645 0.0061207125][0.380574 0.33567205 0.26688617 0.20089549 0.14914088 0.11838865 0.10909988 0.11452147 0.11928457 0.10653453 0.07837227 0.047117956 0.013000367 -0.020117136 -0.029960847][0.40633851 0.34394422 0.24821608 0.14738089 0.061417073 0.0036568225 -0.024533326 -0.02728026 -0.017042214 -0.0096769417 -0.010810127 -0.016872944 -0.027525743 -0.03832059 -0.030464692][0.44891474 0.38661137 0.28087306 0.16080159 0.050778117 -0.032215852 -0.082863063 -0.099451162 -0.091320865 -0.073970929 -0.059037246 -0.049784064 -0.046888329 -0.046160821 -0.030706117][0.47067618 0.425133 0.32942024 0.21051207 0.092492759 -0.0074260868 -0.078920692 -0.11312081 -0.11566295 -0.10086407 -0.082933359 -0.068394013 -0.059483118 -0.05348552 -0.035311174][0.43176767 0.40950084 0.33851868 0.23885295 0.12994513 0.027192617 -0.054722663 -0.10200981 -0.11678626 -0.11018818 -0.094821081 -0.077731982 -0.063448161 -0.051249772 -0.028506974]]...]
INFO - root - 2017-12-10 15:24:15.283356: step 27310, loss = 0.70, batch loss = 0.64 (11.3 examples/sec; 0.709 sec/batch; 60h:06m:15s remains)
INFO - root - 2017-12-10 15:24:22.991517: step 27320, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 64h:05m:40s remains)
INFO - root - 2017-12-10 15:24:30.934956: step 27330, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 66h:00m:42s remains)
INFO - root - 2017-12-10 15:24:38.800047: step 27340, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 65h:39m:57s remains)
INFO - root - 2017-12-10 15:24:46.729328: step 27350, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 68h:26m:36s remains)
INFO - root - 2017-12-10 15:24:54.619811: step 27360, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 65h:24m:15s remains)
INFO - root - 2017-12-10 15:25:02.465664: step 27370, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 66h:46m:20s remains)
INFO - root - 2017-12-10 15:25:10.387691: step 27380, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 65h:42m:40s remains)
INFO - root - 2017-12-10 15:25:17.999504: step 27390, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.773 sec/batch; 65h:30m:55s remains)
INFO - root - 2017-12-10 15:25:25.681988: step 27400, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 66h:20m:55s remains)
2017-12-10 15:25:26.504125: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1555056 0.19614655 0.24128783 0.28476474 0.29611227 0.26609197 0.22208256 0.20040382 0.20251885 0.20584367 0.21168089 0.22940861 0.25281292 0.25500587 0.2390486][0.15948212 0.19991343 0.24897294 0.2995953 0.31798989 0.29523951 0.25460032 0.22712764 0.21540028 0.20192552 0.19293448 0.19554412 0.2062531 0.20518447 0.1941935][0.15624362 0.19105102 0.23858787 0.29487768 0.32716641 0.32368445 0.29834518 0.27391109 0.25361648 0.22583714 0.20075227 0.18291578 0.17537348 0.16953513 0.16638862][0.1559764 0.18151359 0.22383919 0.28528973 0.33725962 0.36142018 0.35978475 0.34676406 0.32485768 0.28632569 0.24375944 0.20282643 0.17515671 0.16365294 0.16854565][0.1603578 0.17488393 0.20843385 0.27200904 0.34347472 0.39704344 0.42362919 0.42908743 0.41264686 0.36670044 0.3057768 0.2422363 0.1960963 0.1792158 0.19070372][0.16643293 0.16884315 0.18917884 0.24813075 0.33227336 0.41157246 0.46907502 0.50077921 0.49912342 0.45255405 0.37672094 0.29387105 0.23166662 0.20858429 0.22286268][0.17011996 0.16231947 0.16733597 0.21453477 0.30070421 0.39683729 0.4822543 0.542524 0.56150663 0.522299 0.43979573 0.34572622 0.27252311 0.24300854 0.25496662][0.17346296 0.15988545 0.15163669 0.18387961 0.26241925 0.36355573 0.46612662 0.54796916 0.58611882 0.55971867 0.48162043 0.389104 0.31503853 0.28245309 0.2886014][0.17427237 0.15781242 0.14086463 0.16096988 0.22849086 0.32537392 0.43175632 0.52238214 0.57285768 0.56212473 0.50087786 0.42531314 0.36335251 0.33386254 0.33352968][0.16866468 0.14798817 0.12533724 0.13849287 0.19714546 0.28593773 0.38587821 0.47356021 0.52968132 0.53691971 0.50348759 0.45735067 0.41730165 0.39534494 0.38846564][0.16236457 0.13479991 0.10899344 0.12105169 0.17607355 0.25772238 0.34655294 0.42384973 0.47969759 0.50308329 0.49864662 0.48260295 0.46408129 0.44895428 0.4355666][0.15274835 0.12141871 0.098407164 0.11488365 0.16984427 0.24498469 0.32062069 0.38275924 0.430376 0.45997554 0.47397134 0.47877651 0.47561225 0.46550441 0.4486371][0.14458379 0.11793759 0.10452129 0.12708488 0.18075334 0.24795185 0.31060165 0.35746425 0.39201871 0.41742671 0.43661597 0.45011458 0.45416883 0.44666433 0.429237][0.14533061 0.13139695 0.13095112 0.15706937 0.20491645 0.26154372 0.3129319 0.34874654 0.37131917 0.38745242 0.40218204 0.41362369 0.41622433 0.40742388 0.38996646][0.15430182 0.15563339 0.16711684 0.19294292 0.23126228 0.27590102 0.31701034 0.34461093 0.35733339 0.36301088 0.36849183 0.37112734 0.36645362 0.35373849 0.33620888]]...]
INFO - root - 2017-12-10 15:25:34.558449: step 27410, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.781 sec/batch; 66h:13m:34s remains)
INFO - root - 2017-12-10 15:25:42.449188: step 27420, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 64h:57m:32s remains)
INFO - root - 2017-12-10 15:25:50.300998: step 27430, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 66h:38m:40s remains)
INFO - root - 2017-12-10 15:25:58.184041: step 27440, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 68h:32m:44s remains)
INFO - root - 2017-12-10 15:26:06.094451: step 27450, loss = 0.66, batch loss = 0.60 (10.4 examples/sec; 0.768 sec/batch; 65h:03m:48s remains)
INFO - root - 2017-12-10 15:26:13.917975: step 27460, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.809 sec/batch; 68h:32m:38s remains)
INFO - root - 2017-12-10 15:26:21.645831: step 27470, loss = 0.68, batch loss = 0.62 (11.4 examples/sec; 0.701 sec/batch; 59h:24m:56s remains)
INFO - root - 2017-12-10 15:26:29.299054: step 27480, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 66h:01m:42s remains)
INFO - root - 2017-12-10 15:26:37.191973: step 27490, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 67h:42m:50s remains)
INFO - root - 2017-12-10 15:26:45.121698: step 27500, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 66h:25m:57s remains)
2017-12-10 15:26:45.991176: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12045228 0.10696964 0.081057057 0.052041378 0.035814162 0.035157405 0.047319535 0.06335526 0.0818749 0.1024468 0.12172962 0.14276877 0.16627763 0.18589859 0.19364055][0.13648501 0.12022029 0.088375166 0.05124148 0.025400069 0.015797041 0.02023685 0.030458048 0.045827925 0.066842243 0.091132835 0.12021063 0.15109973 0.17604367 0.18440621][0.16635221 0.15125632 0.11812565 0.078220323 0.04750102 0.033950895 0.035650749 0.043558165 0.0576479 0.0796964 0.10689893 0.13718489 0.16518793 0.18454079 0.18356149][0.21162975 0.20241281 0.17271177 0.13443996 0.10319775 0.090879045 0.096166052 0.10780533 0.12523152 0.15021698 0.17767756 0.20028953 0.21203384 0.21078107 0.18876228][0.27121246 0.27525491 0.25515133 0.22257331 0.19333619 0.18352243 0.1936754 0.21091565 0.23269032 0.25823361 0.27891341 0.283997 0.26942694 0.23883419 0.19164163][0.33400905 0.35701138 0.35052949 0.32639575 0.30096003 0.29264626 0.30445936 0.3240881 0.34759852 0.37006056 0.37896693 0.36295912 0.32129255 0.26291871 0.19452624][0.38024288 0.42185286 0.42942575 0.41574219 0.39773428 0.39153731 0.40137616 0.41772971 0.43735585 0.45148021 0.44480735 0.40826613 0.34525979 0.2689026 0.19154677][0.39146298 0.44666988 0.46620673 0.46323153 0.45483315 0.45152852 0.45788789 0.46773359 0.47969621 0.48246458 0.45994431 0.406998 0.33171234 0.25055254 0.1793039][0.35352913 0.41762352 0.44765681 0.45515674 0.45637253 0.45567214 0.45773309 0.46037135 0.46464974 0.45874259 0.42677778 0.36742342 0.29271013 0.22042723 0.16652603][0.27147517 0.33756772 0.37434176 0.38973311 0.39806151 0.39931771 0.39889863 0.39749569 0.3982273 0.39063492 0.3597388 0.30659893 0.24499796 0.19174552 0.15989064][0.17494695 0.23458497 0.27183828 0.29089075 0.30307743 0.30645356 0.30756795 0.30797207 0.31097829 0.307543 0.28463972 0.24506547 0.20272529 0.17214069 0.16178763][0.11410483 0.1637737 0.1962104 0.21339183 0.22359666 0.2260976 0.22854276 0.23167479 0.23686878 0.23674266 0.22166131 0.19609541 0.17337437 0.16482724 0.17345151][0.10725446 0.14725965 0.17152981 0.18117648 0.18273588 0.17996468 0.18166254 0.18690237 0.1938026 0.19620165 0.18775861 0.17380793 0.16723149 0.17635624 0.1987562][0.14529023 0.17583373 0.1894218 0.18755607 0.17664593 0.16608815 0.16545522 0.1722714 0.1809946 0.18602102 0.18362211 0.17903858 0.18507618 0.20773152 0.23980875][0.21277946 0.23397191 0.23685077 0.2240797 0.20237038 0.18465501 0.18041557 0.18631598 0.1944401 0.19967741 0.20022151 0.20056278 0.21388619 0.24482945 0.28248122]]...]
INFO - root - 2017-12-10 15:26:54.015372: step 27510, loss = 0.71, batch loss = 0.65 (9.5 examples/sec; 0.844 sec/batch; 71h:30m:35s remains)
INFO - root - 2017-12-10 15:27:01.832199: step 27520, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 66h:56m:07s remains)
INFO - root - 2017-12-10 15:27:09.714081: step 27530, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 66h:17m:04s remains)
INFO - root - 2017-12-10 15:27:17.465605: step 27540, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 68h:15m:41s remains)
INFO - root - 2017-12-10 15:27:25.308726: step 27550, loss = 0.69, batch loss = 0.63 (11.0 examples/sec; 0.727 sec/batch; 61h:33m:50s remains)
INFO - root - 2017-12-10 15:27:32.895517: step 27560, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 66h:13m:51s remains)
INFO - root - 2017-12-10 15:27:40.793167: step 27570, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 65h:07m:56s remains)
INFO - root - 2017-12-10 15:27:48.622628: step 27580, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 65h:45m:13s remains)
INFO - root - 2017-12-10 15:27:56.450958: step 27590, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 65h:36m:56s remains)
INFO - root - 2017-12-10 15:28:04.340670: step 27600, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 68h:22m:12s remains)
2017-12-10 15:28:05.186058: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13309638 0.11431987 0.090484895 0.082439952 0.095068626 0.12183234 0.1481995 0.16739956 0.18533003 0.20886508 0.23517108 0.2491897 0.23472068 0.18796326 0.12177512][0.1723339 0.14870735 0.11749713 0.10890668 0.13290304 0.17914203 0.22237112 0.24702686 0.25764021 0.26722181 0.28132465 0.28880706 0.27107072 0.22005549 0.14704821][0.20659275 0.17667761 0.13620041 0.1256351 0.16148324 0.23042527 0.29523343 0.32825065 0.32837623 0.31490815 0.30576307 0.29947361 0.27815193 0.22783215 0.1546697][0.24097343 0.20460711 0.15373588 0.13847239 0.18134172 0.26843652 0.35309938 0.39435759 0.38259432 0.3410821 0.30262575 0.27919075 0.25618619 0.21299 0.14708279][0.27394029 0.23473097 0.17739917 0.15794069 0.20289452 0.29922822 0.39556962 0.44000402 0.41421568 0.34378 0.27521306 0.23481661 0.21277948 0.18182935 0.12864952][0.31326661 0.27550295 0.21754436 0.19672257 0.24045402 0.33679226 0.4342778 0.47590822 0.437224 0.34408033 0.25147483 0.19693603 0.17605758 0.15605134 0.11414702][0.37101173 0.33997348 0.28662235 0.26517245 0.30182496 0.38702846 0.47494212 0.51015896 0.46521047 0.36217323 0.25660005 0.19137855 0.16732223 0.15016614 0.11262389][0.43423203 0.41553885 0.37088779 0.34841347 0.37120044 0.43420619 0.50289387 0.52986538 0.48747543 0.39004183 0.28521472 0.21418239 0.18171522 0.15783061 0.11721227][0.47308472 0.46736911 0.43368486 0.41273171 0.42177969 0.46057886 0.50855458 0.53041363 0.50034791 0.42256278 0.33034322 0.2573221 0.2114969 0.17223908 0.12218839][0.468255 0.47114459 0.44784531 0.43162298 0.43265787 0.45340741 0.4852877 0.50614721 0.49487081 0.44614062 0.375829 0.30611914 0.24646986 0.187757 0.12411287][0.42325246 0.429316 0.41296977 0.40078267 0.39705184 0.40491775 0.42362717 0.44326216 0.45054364 0.43336454 0.3908982 0.33160871 0.26370752 0.18874046 0.11320514][0.34279189 0.34780481 0.33429444 0.32307664 0.31562391 0.31512061 0.32477137 0.34346303 0.36556053 0.37561694 0.35998017 0.31454316 0.24481831 0.16111894 0.080585971][0.23315679 0.23197368 0.2168256 0.20471197 0.19622362 0.19343275 0.19992787 0.21879451 0.24947257 0.27691895 0.28095403 0.24941719 0.18521275 0.10403143 0.029045152][0.11833837 0.10715748 0.087949626 0.075136811 0.068911821 0.069176115 0.0773793 0.097003788 0.12986943 0.16427009 0.17978074 0.16128366 0.10951787 0.041030668 -0.020349409][0.022301286 0.0036271117 -0.016826725 -0.028153745 -0.030482357 -0.026144816 -0.016600255 0.00012084294 0.026783539 0.057034146 0.075459078 0.068416812 0.034668185 -0.013132281 -0.055822108]]...]
INFO - root - 2017-12-10 15:28:12.947035: step 27610, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.804 sec/batch; 68h:04m:18s remains)
INFO - root - 2017-12-10 15:28:20.788273: step 27620, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 66h:13m:54s remains)
INFO - root - 2017-12-10 15:28:28.519963: step 27630, loss = 0.70, batch loss = 0.64 (12.7 examples/sec; 0.631 sec/batch; 53h:25m:00s remains)
INFO - root - 2017-12-10 15:28:36.374206: step 27640, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.803 sec/batch; 68h:00m:58s remains)
INFO - root - 2017-12-10 15:28:44.087714: step 27650, loss = 0.68, batch loss = 0.62 (9.6 examples/sec; 0.837 sec/batch; 70h:52m:39s remains)
INFO - root - 2017-12-10 15:28:51.907442: step 27660, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 68h:00m:51s remains)
INFO - root - 2017-12-10 15:28:59.775766: step 27670, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.755 sec/batch; 63h:55m:15s remains)
INFO - root - 2017-12-10 15:29:07.612747: step 27680, loss = 0.69, batch loss = 0.63 (10.7 examples/sec; 0.750 sec/batch; 63h:27m:48s remains)
INFO - root - 2017-12-10 15:29:15.514369: step 27690, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 67h:55m:13s remains)
INFO - root - 2017-12-10 15:29:23.386996: step 27700, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 65h:37m:31s remains)
2017-12-10 15:29:24.206841: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28262863 0.24217744 0.19176635 0.14536792 0.11798704 0.12208018 0.15908274 0.20999867 0.24857146 0.2536324 0.21596724 0.14061469 0.049905222 -0.0281987 -0.079014927][0.29201078 0.24277265 0.18166466 0.12672865 0.094435677 0.096950017 0.13317539 0.18325464 0.22179273 0.22943397 0.19635329 0.12626356 0.040933155 -0.032701652 -0.080723308][0.29277495 0.2415278 0.17868519 0.12452286 0.096698307 0.10428076 0.14066935 0.18435457 0.21318783 0.21354234 0.17739908 0.10908591 0.029655932 -0.03683906 -0.079524755][0.28775698 0.24602346 0.19410847 0.15268786 0.14007975 0.16003047 0.19859716 0.2321801 0.24299578 0.22502176 0.1739552 0.098257042 0.019357858 -0.041707858 -0.078479253][0.27588961 0.2531698 0.22341254 0.20444509 0.2142753 0.25065953 0.29310116 0.31589612 0.3055729 0.2632297 0.18885636 0.097083911 0.01164544 -0.04812951 -0.080396995][0.26359215 0.26597 0.26623482 0.27711487 0.31398493 0.36858675 0.4166151 0.43055314 0.39973798 0.33054084 0.22718541 0.1115545 0.01139058 -0.053741533 -0.085503243][0.24758798 0.27443829 0.30552322 0.34696114 0.40913051 0.47928628 0.53265935 0.540388 0.49346605 0.40094587 0.27087185 0.13128178 0.014003968 -0.059750803 -0.093665525][0.22145435 0.26287282 0.31421131 0.37697232 0.45552388 0.53424674 0.58993661 0.59380436 0.53768396 0.43158102 0.28677493 0.13461082 0.0084912879 -0.069620289 -0.10421128][0.19830595 0.24066304 0.2963309 0.36533481 0.44719204 0.52566314 0.57966453 0.58178371 0.52454358 0.41775349 0.2740621 0.12414688 9.1629036e-06 -0.077148855 -0.11122754][0.19197188 0.2241092 0.26897791 0.32868549 0.40163219 0.47344723 0.52434587 0.52843058 0.47795862 0.38111249 0.2497046 0.1114238 -0.0043693851 -0.077656254 -0.11103962][0.20222764 0.22105005 0.24757665 0.28810227 0.34416756 0.4054285 0.452038 0.45921323 0.41849872 0.33614412 0.22115497 0.097749211 -0.0067820363 -0.073739126 -0.10524068][0.22425345 0.23242082 0.24034955 0.25854427 0.29414898 0.34201509 0.38204452 0.39056143 0.35871112 0.29084766 0.19190535 0.082781956 -0.010275784 -0.069740541 -0.098140031][0.2397595 0.24210359 0.23734513 0.23825447 0.2572442 0.29397404 0.32880446 0.33890659 0.3149204 0.25926504 0.17254664 0.073155239 -0.012159482 -0.066241316 -0.092245638][0.22895224 0.23043467 0.22119376 0.21386278 0.22374487 0.25400731 0.28718725 0.30120561 0.2859461 0.24152538 0.16466768 0.071607493 -0.0099506229 -0.062063683 -0.087822452][0.1871236 0.18974181 0.18154222 0.1730883 0.17982548 0.20707074 0.24105348 0.26090187 0.25561741 0.22267039 0.15591431 0.069383919 -0.0090068821 -0.060384411 -0.086869434]]...]
INFO - root - 2017-12-10 15:29:31.921426: step 27710, loss = 0.70, batch loss = 0.64 (12.5 examples/sec; 0.637 sec/batch; 53h:58m:10s remains)
INFO - root - 2017-12-10 15:29:39.818738: step 27720, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 66h:17m:30s remains)
INFO - root - 2017-12-10 15:29:47.684267: step 27730, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 65h:52m:54s remains)
INFO - root - 2017-12-10 15:29:55.085185: step 27740, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 66h:08m:02s remains)
INFO - root - 2017-12-10 15:30:02.880460: step 27750, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 65h:56m:43s remains)
INFO - root - 2017-12-10 15:30:10.682095: step 27760, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.772 sec/batch; 65h:21m:36s remains)
INFO - root - 2017-12-10 15:30:18.504293: step 27770, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 66h:25m:47s remains)
INFO - root - 2017-12-10 15:30:26.367897: step 27780, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 66h:51m:21s remains)
INFO - root - 2017-12-10 15:30:34.383253: step 27790, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 67h:22m:07s remains)
INFO - root - 2017-12-10 15:30:42.062811: step 27800, loss = 0.70, batch loss = 0.64 (9.3 examples/sec; 0.858 sec/batch; 72h:36m:04s remains)
2017-12-10 15:30:42.911876: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22246552 0.20404252 0.17154731 0.14459226 0.13431661 0.14217763 0.16423923 0.18808094 0.20146677 0.19458435 0.1693809 0.13712166 0.11832247 0.12856531 0.16557163][0.31834823 0.29370159 0.24665259 0.20467751 0.17905737 0.17169361 0.18021148 0.19491822 0.20447154 0.19856746 0.17795634 0.15352537 0.14255394 0.15843098 0.19778025][0.41861743 0.3866412 0.32623568 0.27267241 0.23684578 0.22029296 0.21964268 0.22536308 0.22687845 0.21546869 0.19363764 0.17223701 0.16387093 0.18051395 0.2192869][0.49348074 0.45718887 0.39064321 0.33486617 0.30131125 0.28999379 0.29255703 0.29528913 0.28719038 0.26377967 0.23253313 0.20511417 0.19053328 0.20125997 0.23668633][0.50286514 0.47152466 0.41454318 0.37305582 0.35838667 0.36823481 0.386604 0.39179543 0.37219527 0.33144009 0.28541744 0.24705397 0.2219386 0.22354835 0.25418821][0.45081887 0.43388832 0.40022811 0.38421297 0.39684293 0.4341749 0.47336593 0.48322162 0.45228273 0.39404929 0.33487338 0.28908831 0.25811505 0.25453514 0.28193116][0.37489516 0.38179862 0.37855634 0.38844198 0.42142192 0.47481191 0.523847 0.53306836 0.49287558 0.42440352 0.36168274 0.31967551 0.29497349 0.29577431 0.3236883][0.30792308 0.34475112 0.37343514 0.40329111 0.44166452 0.48979586 0.52741981 0.52464235 0.47754481 0.41007218 0.35641962 0.32962915 0.3227247 0.33760846 0.37086931][0.26625824 0.33245108 0.38791382 0.42814058 0.45692557 0.47964641 0.48630697 0.46113822 0.40859145 0.35027155 0.31324935 0.30701125 0.32333249 0.35849917 0.40235776][0.24681056 0.33131334 0.40207741 0.443269 0.45441112 0.44362339 0.4119809 0.36094746 0.3032015 0.25539732 0.23406135 0.24540164 0.28246981 0.33807927 0.3960264][0.22445351 0.3110812 0.38338068 0.41946346 0.41467416 0.37693796 0.31481391 0.2439907 0.1833048 0.14464408 0.13480796 0.15701631 0.20709866 0.27707556 0.34753838][0.17467344 0.24914294 0.31294605 0.34355313 0.3328343 0.28566831 0.21237028 0.13579139 0.077698216 0.047242753 0.044467311 0.069903567 0.12259862 0.1956508 0.27061585][0.10106896 0.15641035 0.20757665 0.23573484 0.23039539 0.19143043 0.12682109 0.059557039 0.010728486 -0.011268546 -0.0094071394 0.015365708 0.064886704 0.13274442 0.20358765][0.037051804 0.0718555 0.10839976 0.1352392 0.1411269 0.12038268 0.076253571 0.028014705 -0.0063800355 -0.01830201 -0.011334073 0.013670067 0.060139384 0.12018537 0.18118401][0.016759668 0.031483274 0.05069517 0.072690837 0.0873266 0.084654324 0.062567323 0.035695255 0.017702481 0.016826971 0.029931111 0.057344161 0.10324281 0.15650198 0.20530775]]...]
INFO - root - 2017-12-10 15:30:50.716404: step 27810, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.797 sec/batch; 67h:28m:10s remains)
INFO - root - 2017-12-10 15:30:58.555512: step 27820, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.820 sec/batch; 69h:21m:50s remains)
INFO - root - 2017-12-10 15:31:06.310221: step 27830, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 68h:52m:23s remains)
INFO - root - 2017-12-10 15:31:14.171799: step 27840, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 67h:28m:03s remains)
INFO - root - 2017-12-10 15:31:22.051672: step 27850, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 66h:12m:49s remains)
INFO - root - 2017-12-10 15:31:29.981056: step 27860, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 67h:27m:23s remains)
INFO - root - 2017-12-10 15:31:37.676652: step 27870, loss = 0.71, batch loss = 0.65 (13.1 examples/sec; 0.612 sec/batch; 51h:48m:09s remains)
INFO - root - 2017-12-10 15:31:45.505591: step 27880, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 65h:59m:02s remains)
INFO - root - 2017-12-10 15:31:53.329883: step 27890, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 64h:26m:30s remains)
INFO - root - 2017-12-10 15:32:01.254216: step 27900, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 66h:07m:06s remains)
2017-12-10 15:32:02.100448: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29067251 0.33141315 0.36299121 0.37980443 0.37456304 0.34130639 0.28575802 0.22853372 0.1906841 0.18081175 0.19384336 0.21999229 0.25291467 0.28134835 0.29184029][0.29607216 0.33020997 0.35029629 0.35406831 0.33997774 0.30164972 0.24183051 0.17813322 0.13384594 0.12333887 0.14442417 0.18744792 0.24192947 0.29068062 0.31714076][0.30508396 0.33098334 0.33363223 0.3147212 0.28272787 0.23566471 0.17452292 0.11074539 0.065027438 0.054469507 0.0800386 0.13420355 0.20339318 0.2663523 0.30452839][0.31662664 0.33938447 0.32861042 0.28909516 0.23956847 0.18492833 0.12637229 0.068402469 0.025213113 0.013547547 0.036980376 0.090930849 0.16130762 0.22585316 0.26527485][0.31854028 0.34755275 0.33673427 0.29156256 0.2366059 0.18187177 0.12984784 0.080335744 0.040374018 0.02455206 0.038280617 0.079958424 0.13734156 0.1893028 0.21761212][0.3109327 0.35132581 0.35250241 0.3182874 0.2728543 0.22720405 0.18491526 0.14385508 0.10640668 0.084733307 0.085082993 0.10850559 0.1451965 0.17496207 0.18186216][0.30099338 0.34868619 0.36606935 0.35453168 0.33115351 0.30287793 0.2728081 0.2390102 0.20234539 0.17407064 0.16107963 0.16662803 0.18213212 0.18751332 0.16931364][0.30262089 0.34743482 0.37435734 0.38437355 0.38398919 0.37464991 0.35950083 0.33651191 0.305024 0.27514833 0.25392702 0.24608958 0.24261512 0.2246611 0.18260932][0.32100877 0.3522521 0.37588319 0.39562848 0.408095 0.41212729 0.41254276 0.40614718 0.38926792 0.36879426 0.35025173 0.3374491 0.31908616 0.2800934 0.21716566][0.34333578 0.35683003 0.36822444 0.38465005 0.39721873 0.40663633 0.4198465 0.4320356 0.43648967 0.43558344 0.43072328 0.42168051 0.39369982 0.33785748 0.2592544][0.34910214 0.34873047 0.34609327 0.35156393 0.35642114 0.36656114 0.39060149 0.42114869 0.44886288 0.47200972 0.48703361 0.48712084 0.45366648 0.38523826 0.29608238][0.33329481 0.32863313 0.31739533 0.31168827 0.3069559 0.31534734 0.34602502 0.38925898 0.434395 0.477519 0.51090044 0.52039933 0.4845846 0.40898806 0.31528461][0.31139392 0.31177929 0.29934341 0.2857931 0.271841 0.27484015 0.30398846 0.34943274 0.40091076 0.45372036 0.49742252 0.51212418 0.47565925 0.39839473 0.30571669][0.29163936 0.29967213 0.29052725 0.27395716 0.25490752 0.25207308 0.27393183 0.31238142 0.35932547 0.40979338 0.4525727 0.46553183 0.42877889 0.35462454 0.26712349][0.27405241 0.28951266 0.28735068 0.27476868 0.25758454 0.25136924 0.26364395 0.28900275 0.32251546 0.35969806 0.39064094 0.39512476 0.35735565 0.28951511 0.21042585]]...]
INFO - root - 2017-12-10 15:32:09.964382: step 27910, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.781 sec/batch; 66h:04m:28s remains)
INFO - root - 2017-12-10 15:32:17.720901: step 27920, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 65h:53m:35s remains)
INFO - root - 2017-12-10 15:32:25.553582: step 27930, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 67h:00m:07s remains)
INFO - root - 2017-12-10 15:32:33.472753: step 27940, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 67h:09m:44s remains)
INFO - root - 2017-12-10 15:32:41.152689: step 27950, loss = 0.69, batch loss = 0.63 (13.4 examples/sec; 0.599 sec/batch; 50h:38m:57s remains)
INFO - root - 2017-12-10 15:32:48.884561: step 27960, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 65h:14m:49s remains)
INFO - root - 2017-12-10 15:32:56.675137: step 27970, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.759 sec/batch; 64h:12m:27s remains)
INFO - root - 2017-12-10 15:33:04.619035: step 27980, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.776 sec/batch; 65h:38m:25s remains)
INFO - root - 2017-12-10 15:33:12.424138: step 27990, loss = 0.68, batch loss = 0.63 (10.6 examples/sec; 0.757 sec/batch; 64h:03m:42s remains)
INFO - root - 2017-12-10 15:33:20.266293: step 28000, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.817 sec/batch; 69h:07m:29s remains)
2017-12-10 15:33:21.120735: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24552286 0.21935663 0.19351894 0.17521408 0.15747744 0.14017296 0.13208617 0.14004749 0.16109842 0.17601056 0.16729678 0.13538054 0.09693484 0.069955289 0.071732119][0.27779886 0.25415152 0.22690001 0.2067657 0.19472665 0.1944982 0.20996566 0.23971191 0.27107477 0.27777603 0.24394822 0.18097068 0.12001563 0.087819293 0.10097993][0.28952062 0.27340242 0.24995188 0.23307973 0.2320357 0.25435263 0.29763809 0.34982038 0.387748 0.38105011 0.31858185 0.22406214 0.14274022 0.10971909 0.13744596][0.28429925 0.28024414 0.26608548 0.25671676 0.26740554 0.30856279 0.37177533 0.4352794 0.46902475 0.44410789 0.35663545 0.23984337 0.14791511 0.12077941 0.1649816][0.246632 0.25884229 0.26070222 0.26592398 0.291279 0.34743723 0.42098984 0.48333052 0.50277716 0.45740396 0.35242307 0.22575074 0.1342539 0.11745683 0.17637335][0.19226749 0.22086403 0.24130204 0.26522657 0.30789119 0.37728697 0.4551594 0.50896209 0.50946611 0.44431984 0.32829556 0.20082577 0.11661587 0.11070339 0.17840515][0.15460542 0.19297276 0.22591443 0.26431265 0.32153255 0.40090963 0.47959864 0.52329016 0.50728649 0.4280138 0.30763194 0.18527235 0.11144793 0.11378028 0.18152452][0.15203501 0.19113138 0.22491789 0.26639345 0.32711849 0.40649581 0.47926375 0.51280421 0.4879739 0.40641451 0.292107 0.18136622 0.11963497 0.12703937 0.18867452][0.18038276 0.21130596 0.23495592 0.26910132 0.32337567 0.39385226 0.45471719 0.47761098 0.44916379 0.37397838 0.27350318 0.17990667 0.13163067 0.1430546 0.19778672][0.22130448 0.2348215 0.24128838 0.26344195 0.30907136 0.37105212 0.42309511 0.44010434 0.4121637 0.34532404 0.25904456 0.18268867 0.14837168 0.16515224 0.21519542][0.27578425 0.26595691 0.24933286 0.25592005 0.29177314 0.34650093 0.39318272 0.40830687 0.383482 0.32575068 0.25379577 0.19543599 0.17654909 0.20104627 0.24852894][0.34647369 0.31387269 0.27366748 0.2626844 0.28512287 0.32732034 0.36418319 0.37512147 0.35382515 0.30774462 0.25414306 0.21696216 0.21528807 0.24912959 0.29603332][0.41125071 0.36687127 0.3120195 0.28672469 0.29317194 0.31640166 0.33766356 0.34199116 0.32622382 0.29585788 0.2637994 0.24684234 0.25780314 0.29524767 0.33859444][0.45610338 0.41138989 0.35250509 0.31817487 0.30963251 0.31340927 0.31872115 0.31691873 0.30747047 0.2922253 0.27741197 0.27200761 0.28381032 0.3120943 0.34278548][0.47035214 0.43346059 0.37811628 0.33898351 0.31822681 0.30743557 0.3029165 0.29981959 0.296982 0.29227358 0.28626886 0.28248763 0.284993 0.29437512 0.30465004]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 15:33:28.801774: step 28010, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 66h:13m:37s remains)
INFO - root - 2017-12-10 15:33:36.577651: step 28020, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 65h:03m:38s remains)
INFO - root - 2017-12-10 15:33:44.219104: step 28030, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 65h:28m:01s remains)
INFO - root - 2017-12-10 15:33:52.093650: step 28040, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.810 sec/batch; 68h:28m:58s remains)
INFO - root - 2017-12-10 15:33:59.874314: step 28050, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 67h:37m:57s remains)
INFO - root - 2017-12-10 15:34:07.778111: step 28060, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 66h:48m:43s remains)
INFO - root - 2017-12-10 15:34:15.704365: step 28070, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.829 sec/batch; 70h:06m:47s remains)
INFO - root - 2017-12-10 15:34:23.582637: step 28080, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 66h:31m:34s remains)
INFO - root - 2017-12-10 15:34:31.350332: step 28090, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 66h:15m:19s remains)
INFO - root - 2017-12-10 15:34:39.031878: step 28100, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 65h:36m:41s remains)
2017-12-10 15:34:39.886959: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11090389 0.12156226 0.12618558 0.12622987 0.12210081 0.12018006 0.13969034 0.18204154 0.22783798 0.25877649 0.26222292 0.23034263 0.16352549 0.080576472 0.011441031][0.10324249 0.10916273 0.11027626 0.10995223 0.10773432 0.10632178 0.12160157 0.15701672 0.19571598 0.22052348 0.22017969 0.18821235 0.12602666 0.050403025 -0.011187716][0.11444312 0.12216345 0.12534982 0.12862921 0.13092265 0.13129871 0.14162344 0.16829908 0.19780332 0.21364129 0.20453106 0.16655852 0.10356726 0.030515838 -0.027189387][0.16150172 0.18077055 0.19431373 0.2073817 0.21920228 0.22586997 0.23478138 0.25548685 0.2769061 0.27967519 0.25161946 0.19329047 0.11402831 0.029396528 -0.034763236][0.230488 0.26731259 0.29599079 0.32324886 0.34982458 0.36931792 0.38404182 0.40542808 0.422655 0.41036156 0.35530519 0.26523268 0.15761234 0.050581645 -0.029188577][0.29267094 0.34395033 0.3851316 0.42600915 0.46982968 0.50817442 0.53833824 0.57079673 0.59141159 0.56707495 0.48512137 0.362279 0.22411095 0.091234811 -0.0087192543][0.32061505 0.37777585 0.42448935 0.47445035 0.53317463 0.59065723 0.64071548 0.68981475 0.71891958 0.689056 0.59017557 0.44626161 0.28760311 0.13603774 0.019815514][0.31061837 0.36243236 0.40696189 0.46069273 0.52975559 0.60281819 0.67299247 0.74021119 0.7799722 0.75076455 0.64661759 0.49557391 0.32872114 0.16885775 0.044547267][0.27582613 0.31435022 0.34995791 0.4007549 0.47149903 0.55092019 0.63530445 0.71773058 0.76852757 0.74591631 0.64780349 0.50308162 0.34038952 0.18262932 0.058582462][0.21679407 0.24012749 0.26494396 0.30828181 0.37237778 0.4474813 0.535666 0.624633 0.68120694 0.66642827 0.58175904 0.45498994 0.30945149 0.1660706 0.052810274][0.14057383 0.15217616 0.16875376 0.20342961 0.25573263 0.31845409 0.39878708 0.48176232 0.53424329 0.52351773 0.45419163 0.35146686 0.23223472 0.11361498 0.021243772][0.062498838 0.06767305 0.079565443 0.10574659 0.14396562 0.18941483 0.25174773 0.31657106 0.35502392 0.34293172 0.2872107 0.20964305 0.12129533 0.034701496 -0.029110128][0.0016193219 0.0042686942 0.013083306 0.030848699 0.054951362 0.082722068 0.12350792 0.16567288 0.18695335 0.1714803 0.12692671 0.071170777 0.011462332 -0.043340396 -0.077863462][-0.033989884 -0.031511806 -0.024996223 -0.014505603 -0.002049058 0.011309287 0.032938622 0.055101946 0.062810421 0.046750028 0.013372594 -0.023959203 -0.060377534 -0.08962781 -0.10167914][-0.050911784 -0.048275065 -0.043483492 -0.037904486 -0.032753441 -0.028328476 -0.020105775 -0.012105432 -0.01263758 -0.026178198 -0.048394598 -0.070578344 -0.089435093 -0.10077211 -0.099197827]]...]
INFO - root - 2017-12-10 15:34:47.534441: step 28110, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 66h:42m:38s remains)
INFO - root - 2017-12-10 15:34:55.352967: step 28120, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 66h:06m:03s remains)
INFO - root - 2017-12-10 15:35:03.103860: step 28130, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.753 sec/batch; 63h:40m:38s remains)
INFO - root - 2017-12-10 15:35:10.968767: step 28140, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 67h:40m:13s remains)
INFO - root - 2017-12-10 15:35:18.879190: step 28150, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 66h:24m:14s remains)
INFO - root - 2017-12-10 15:35:26.738204: step 28160, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.817 sec/batch; 69h:03m:20s remains)
INFO - root - 2017-12-10 15:35:34.714439: step 28170, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 67h:37m:19s remains)
INFO - root - 2017-12-10 15:35:42.550979: step 28180, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 68h:13m:51s remains)
INFO - root - 2017-12-10 15:35:49.969161: step 28190, loss = 0.70, batch loss = 0.64 (12.8 examples/sec; 0.625 sec/batch; 52h:47m:55s remains)
INFO - root - 2017-12-10 15:35:57.733560: step 28200, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 64h:12m:26s remains)
2017-12-10 15:35:58.545164: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18259764 0.21069641 0.20552415 0.18081114 0.1504539 0.12338205 0.11109988 0.12208491 0.14562052 0.17610933 0.2043947 0.20958272 0.18945283 0.14732526 0.09698613][0.28243172 0.31530142 0.30222654 0.26366583 0.21873483 0.18019132 0.16417304 0.17999712 0.21114826 0.25235742 0.29383555 0.30679658 0.28543636 0.22956853 0.15917595][0.36910537 0.40063846 0.37518832 0.3220984 0.26548159 0.21972704 0.20302159 0.22435072 0.2624197 0.31301951 0.36598653 0.38746473 0.36835673 0.30411538 0.21857263][0.42769864 0.45190361 0.41161332 0.34608054 0.28428829 0.2401105 0.22981776 0.25882241 0.30250019 0.35827932 0.41709563 0.44397572 0.42767212 0.35942483 0.26489386][0.46892735 0.483694 0.42942569 0.35504052 0.29472768 0.2607834 0.26321474 0.29952389 0.34284076 0.39491922 0.4503428 0.47677505 0.46176806 0.39295366 0.29584733][0.49610829 0.50136894 0.43544716 0.35606459 0.30256966 0.28431812 0.30082309 0.33886945 0.37163776 0.40728065 0.44753715 0.46661362 0.45217237 0.3880254 0.29585314][0.51352555 0.51047647 0.43608359 0.35471767 0.30920264 0.30497578 0.33034137 0.362442 0.37732771 0.38916102 0.40818506 0.41707572 0.40320307 0.34741014 0.2655431][0.51899981 0.50958741 0.43142825 0.35122913 0.31098744 0.31304598 0.33850056 0.35995576 0.35809505 0.34974328 0.34973124 0.34820881 0.33304659 0.28564009 0.21645842][0.49182263 0.4777967 0.40214223 0.32785237 0.29011828 0.28925008 0.3057256 0.31479126 0.30316359 0.28507888 0.27413571 0.26436982 0.24629635 0.20551351 0.1494247][0.41971502 0.40361232 0.33710146 0.27300942 0.23609892 0.2257165 0.22658119 0.22232072 0.20634906 0.18787622 0.17465246 0.16248772 0.14517584 0.11349384 0.072636396][0.30568874 0.28924516 0.23555815 0.18340024 0.14730494 0.12684779 0.11233026 0.096693516 0.079276435 0.064591862 0.055825219 0.049351521 0.040409151 0.023000928 -0.00021784974][0.17599881 0.1602401 0.11998367 0.079992145 0.047305293 0.022659523 0.0012333213 -0.018111458 -0.033470519 -0.04321719 -0.0459315 -0.044575654 -0.043691821 -0.047669176 -0.055972893][0.054014347 0.039606322 0.01150174 -0.016577888 -0.041520197 -0.061534189 -0.0790382 -0.0936339 -0.10370696 -0.10888216 -0.10799953 -0.10265714 -0.096473575 -0.092511244 -0.09124326][-0.040473398 -0.053398255 -0.07163211 -0.089173011 -0.10447225 -0.1158084 -0.12463473 -0.13092606 -0.13426872 -0.13466989 -0.13135353 -0.12480253 -0.11724045 -0.11038519 -0.10462251][-0.089169756 -0.10010719 -0.11130907 -0.12118044 -0.12856385 -0.13245456 -0.13392414 -0.13349524 -0.13147584 -0.12807254 -0.1231458 -0.11677481 -0.10997644 -0.10337889 -0.097139344]]...]
INFO - root - 2017-12-10 15:36:06.359856: step 28210, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 64h:36m:00s remains)
INFO - root - 2017-12-10 15:36:14.138856: step 28220, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 67h:28m:51s remains)
INFO - root - 2017-12-10 15:36:22.022706: step 28230, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 67h:23m:04s remains)
INFO - root - 2017-12-10 15:36:29.895494: step 28240, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 66h:13m:19s remains)
INFO - root - 2017-12-10 15:36:37.893653: step 28250, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 67h:51m:32s remains)
INFO - root - 2017-12-10 15:36:45.728110: step 28260, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 65h:35m:47s remains)
INFO - root - 2017-12-10 15:36:53.091585: step 28270, loss = 0.71, batch loss = 0.66 (18.8 examples/sec; 0.425 sec/batch; 35h:53m:26s remains)
INFO - root - 2017-12-10 15:37:00.685170: step 28280, loss = 0.69, batch loss = 0.63 (10.7 examples/sec; 0.748 sec/batch; 63h:14m:15s remains)
INFO - root - 2017-12-10 15:37:08.492126: step 28290, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 65h:34m:06s remains)
INFO - root - 2017-12-10 15:37:16.268836: step 28300, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 65h:36m:27s remains)
2017-12-10 15:37:17.118230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.011435105 -0.010201531 0.015604359 0.069825143 0.14601362 0.23037367 0.30819759 0.36223823 0.3815439 0.36836722 0.33406484 0.29788122 0.27720937 0.28294057 0.31174007][0.010972459 0.018733369 0.050904438 0.11199534 0.19632712 0.289568 0.37623855 0.43724582 0.4578906 0.43953168 0.39492282 0.34828368 0.32386777 0.33732018 0.3839145][0.040933259 0.052859575 0.086633846 0.14823735 0.23343317 0.328176 0.41559115 0.47516903 0.48959354 0.45914769 0.39976296 0.34210128 0.3164821 0.34169909 0.40953442][0.071000367 0.086025596 0.11801137 0.17501283 0.25481519 0.34463179 0.42692855 0.48060459 0.48569459 0.44051394 0.36600253 0.29993641 0.27677304 0.3138653 0.39922002][0.10322616 0.11932169 0.14793311 0.19980411 0.27410895 0.35894814 0.4354789 0.48100948 0.47257197 0.40705073 0.31289458 0.23584609 0.21300879 0.25775391 0.35421696][0.13102324 0.14892383 0.17930591 0.23516363 0.31489423 0.40411824 0.47865531 0.51248562 0.48165712 0.38522896 0.26130417 0.16454023 0.13414615 0.17790999 0.27600947][0.13790566 0.16437523 0.20818709 0.28243944 0.38136771 0.48365089 0.55644321 0.57103431 0.50816751 0.37417182 0.21730497 0.099056423 0.057374109 0.09305951 0.18429798][0.1217702 0.16381781 0.23065086 0.3331863 0.45875934 0.57641149 0.64482695 0.63513225 0.53715825 0.36747321 0.18356812 0.0492652 -0.0032932817 0.019100679 0.094947569][0.093557127 0.15192594 0.24207826 0.37084121 0.51734978 0.6414628 0.69785315 0.66232973 0.53581941 0.34409013 0.1489145 0.010622963 -0.047210779 -0.037807573 0.01924254][0.058198176 0.12545265 0.2272761 0.36560848 0.51304394 0.62544191 0.66043168 0.60353518 0.46584183 0.27737173 0.095072255 -0.030869195 -0.0850789 -0.0829173 -0.041123673][0.019469026 0.084114812 0.18162106 0.30891657 0.43641838 0.52215981 0.5325585 0.4645279 0.33577359 0.17480867 0.026047368 -0.073633857 -0.11505034 -0.11254717 -0.079729319][-0.017385559 0.034789842 0.1140881 0.21443447 0.30880672 0.36214817 0.35216391 0.28488049 0.17993523 0.060491178 -0.04351284 -0.10803173 -0.129573 -0.12101022 -0.0927707][-0.04588921 -0.011187939 0.042867176 0.10981388 0.16891994 0.1945858 0.17458679 0.11903062 0.04493108 -0.031365715 -0.091732211 -0.12246624 -0.12488797 -0.11045638 -0.08658994][-0.060716234 -0.043572869 -0.014841181 0.01983266 0.047835328 0.054129943 0.033894449 -0.0033014528 -0.046010561 -0.08468245 -0.10965192 -0.11476321 -0.10395948 -0.086518832 -0.0672677][-0.067947783 -0.065293886 -0.056725297 -0.046534456 -0.039988756 -0.043282054 -0.057177141 -0.075403564 -0.092285417 -0.1035024 -0.10489564 -0.095044561 -0.078209139 -0.060758006 -0.045714509]]...]
INFO - root - 2017-12-10 15:37:24.957091: step 28310, loss = 0.69, batch loss = 0.63 (11.0 examples/sec; 0.730 sec/batch; 61h:39m:43s remains)
INFO - root - 2017-12-10 15:37:32.844235: step 28320, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.797 sec/batch; 67h:23m:01s remains)
INFO - root - 2017-12-10 15:37:40.747358: step 28330, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 67h:37m:58s remains)
INFO - root - 2017-12-10 15:37:48.559248: step 28340, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 65h:31m:17s remains)
INFO - root - 2017-12-10 15:37:56.407352: step 28350, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.781 sec/batch; 65h:57m:26s remains)
INFO - root - 2017-12-10 15:38:03.914430: step 28360, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 65h:44m:31s remains)
INFO - root - 2017-12-10 15:38:11.712878: step 28370, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 67h:46m:49s remains)
INFO - root - 2017-12-10 15:38:19.502978: step 28380, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 66h:49m:07s remains)
INFO - root - 2017-12-10 15:38:27.347806: step 28390, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 65h:29m:03s remains)
INFO - root - 2017-12-10 15:38:35.094512: step 28400, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 65h:58m:19s remains)
2017-12-10 15:38:35.920688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.010308623 0.011073928 0.034041997 0.049312729 0.052270763 0.047171831 0.04209869 0.041071754 0.044004265 0.045870867 0.041619446 0.030628627 0.012048898 -0.01289603 -0.038030472][0.020377822 0.057021923 0.095080912 0.1215945 0.13017768 0.12658183 0.12035526 0.11518325 0.11221419 0.10785098 0.096382789 0.077067263 0.048578121 0.012710623 -0.023419328][0.057215728 0.11076362 0.16675523 0.20965874 0.23090366 0.23540048 0.23114237 0.21984841 0.20451397 0.18573773 0.15937357 0.12558882 0.083441123 0.036576904 -0.0079456791][0.092330121 0.1640777 0.23955703 0.3018477 0.34040168 0.357492 0.35804659 0.3416979 0.31250659 0.27477419 0.22812611 0.17480889 0.11498065 0.054935336 0.0026638948][0.12077992 0.2122142 0.31006312 0.39607558 0.45813709 0.49556705 0.5103057 0.49777034 0.45995772 0.40256247 0.33085236 0.251074 0.16534379 0.0845912 0.019729089][0.13982554 0.25049058 0.37081087 0.48039907 0.56628054 0.62645864 0.66143709 0.65980995 0.61828512 0.54250491 0.4451949 0.33845949 0.22579716 0.12312693 0.04460315][0.14400662 0.267714 0.40397987 0.52988905 0.63189751 0.70898896 0.76260036 0.77455068 0.73454082 0.64758509 0.5325563 0.40773422 0.27704194 0.15966561 0.071467377][0.13834597 0.26727349 0.41064376 0.54341984 0.65020436 0.73147374 0.79245144 0.81236452 0.77609622 0.68741781 0.56730783 0.43829033 0.30390987 0.18322235 0.091578968][0.12376694 0.24577351 0.3823002 0.50813854 0.6055733 0.67599094 0.7301746 0.75061852 0.72046095 0.64192575 0.5335806 0.41831335 0.29702425 0.18584806 0.098896258][0.0998394 0.20305036 0.31866962 0.42432529 0.50152677 0.55196351 0.59048867 0.60557693 0.58173186 0.52040493 0.43525729 0.34588015 0.24954431 0.15864247 0.08526355][0.065773413 0.14275625 0.22862586 0.30570871 0.35734406 0.38447803 0.40234989 0.40615433 0.38505247 0.34080759 0.28180844 0.22232257 0.15696247 0.094083674 0.04295351][0.023440167 0.073007889 0.12857613 0.17762104 0.20678632 0.21557027 0.21643846 0.20967692 0.1898362 0.15926233 0.12271468 0.088976979 0.051784489 0.015995953 -0.011906159][-0.01471048 0.013077845 0.04527171 0.0733964 0.087684453 0.0864154 0.078114972 0.065696888 0.04776917 0.026860021 0.005890104 -0.01049502 -0.027870737 -0.043947581 -0.05459661][-0.040849455 -0.028089074 -0.011744414 0.0021963636 0.0074012857 0.0020793381 -0.0086345859 -0.021367766 -0.03554856 -0.049109988 -0.060166027 -0.066558592 -0.072333567 -0.076581895 -0.076837465][-0.0543138 -0.050695825 -0.043607384 -0.037648186 -0.036543019 -0.041724619 -0.050668918 -0.060663592 -0.070474841 -0.078607768 -0.084307134 -0.086901277 -0.088069066 -0.087245747 -0.083099633]]...]
INFO - root - 2017-12-10 15:38:43.754669: step 28410, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 64h:51m:17s remains)
INFO - root - 2017-12-10 15:38:51.651335: step 28420, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 68h:42m:26s remains)
INFO - root - 2017-12-10 15:38:59.482626: step 28430, loss = 0.72, batch loss = 0.67 (10.6 examples/sec; 0.752 sec/batch; 63h:28m:55s remains)
INFO - root - 2017-12-10 15:39:07.010718: step 28440, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 65h:40m:42s remains)
INFO - root - 2017-12-10 15:39:14.630922: step 28450, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 65h:50m:12s remains)
INFO - root - 2017-12-10 15:39:22.416775: step 28460, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 65h:18m:04s remains)
INFO - root - 2017-12-10 15:39:30.203583: step 28470, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 65h:55m:54s remains)
INFO - root - 2017-12-10 15:39:38.049557: step 28480, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 65h:47m:48s remains)
INFO - root - 2017-12-10 15:39:45.848508: step 28490, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 65h:12m:11s remains)
INFO - root - 2017-12-10 15:39:53.733397: step 28500, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 66h:48m:56s remains)
2017-12-10 15:39:54.539224: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.032864384 0.0253255 0.0098818708 -0.00949762 -0.021590985 -0.02017251 -0.0063817408 0.017232133 0.043878257 0.064528123 0.068307258 0.05413343 0.031721454 0.0041450816 -0.026140906][0.093574032 0.091251887 0.07837151 0.061086733 0.054706298 0.06659317 0.096437976 0.13760301 0.17439926 0.19400163 0.1850203 0.14845414 0.09704154 0.041575994 -0.008156063][0.16808511 0.17783949 0.17474248 0.16542032 0.17039649 0.19917575 0.25151357 0.31253707 0.35537228 0.36580428 0.3346692 0.2655012 0.17412986 0.081528828 0.0071267588][0.24603322 0.2756297 0.29142943 0.2981863 0.32073328 0.37156209 0.44978744 0.52847683 0.56919163 0.56152058 0.50300813 0.39805794 0.262595 0.12738852 0.023816166][0.31371015 0.37133798 0.41804093 0.45291305 0.50094652 0.57648218 0.67809731 0.76604885 0.79207522 0.75411206 0.66158044 0.52064478 0.34504414 0.17099135 0.04046718][0.35567415 0.44660109 0.53442192 0.60848254 0.68756831 0.78510785 0.90034211 0.98433959 0.98379678 0.906218 0.77484953 0.60035264 0.39541784 0.19660145 0.04969855][0.36802563 0.48949739 0.61800808 0.72997749 0.83215338 0.93597746 1.0432791 1.1053355 1.0711124 0.95613843 0.79355246 0.59832036 0.38423783 0.18422891 0.039280046][0.35121828 0.4879871 0.63849151 0.7678442 0.86947209 0.95382863 1.0285393 1.0554028 0.99199384 0.85664165 0.68441105 0.49208021 0.29532051 0.12114121 -0.00048915105][0.29702798 0.42419833 0.56629127 0.6845997 0.76302326 0.81075627 0.84193385 0.83457351 0.75800645 0.62823719 0.47466749 0.3121908 0.15682901 0.02871399 -0.054730427][0.2057602 0.29981273 0.40592086 0.49049574 0.53523767 0.54702932 0.54161704 0.51083159 0.43706447 0.33297089 0.21904939 0.10546274 0.0056648715 -0.067425832 -0.10746661][0.10254771 0.1518416 0.20774165 0.24770549 0.25790337 0.24353898 0.21792668 0.180822 0.12490752 0.059106551 -0.0056125186 -0.065130994 -0.11088394 -0.13616192 -0.14155617][0.010004567 0.01905328 0.02982894 0.031352587 0.016880326 -0.0090829013 -0.037290286 -0.065422043 -0.09583886 -0.12397776 -0.14643948 -0.16336118 -0.17042954 -0.16539244 -0.1516268][-0.06243537 -0.0786827 -0.094786011 -0.11389381 -0.1368393 -0.15995148 -0.17824586 -0.19060978 -0.19850917 -0.20117271 -0.19878139 -0.19245403 -0.18027039 -0.16273598 -0.14347962][-0.10441092 -0.12830673 -0.15096302 -0.17294799 -0.19235304 -0.20639004 -0.21303035 -0.21290852 -0.20784563 -0.19905661 -0.18752427 -0.17404738 -0.15801288 -0.14110245 -0.1258226][-0.11297351 -0.13226414 -0.14875297 -0.16380288 -0.17540681 -0.18184553 -0.18241456 -0.17812987 -0.17040388 -0.16072385 -0.15008996 -0.13928747 -0.1283278 -0.11839364 -0.11035772]]...]
INFO - root - 2017-12-10 15:40:02.307389: step 28510, loss = 0.70, batch loss = 0.64 (10.9 examples/sec; 0.737 sec/batch; 62h:14m:36s remains)
INFO - root - 2017-12-10 15:40:10.038608: step 28520, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 65h:12m:08s remains)
INFO - root - 2017-12-10 15:40:17.998123: step 28530, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 68h:05m:40s remains)
INFO - root - 2017-12-10 15:40:25.773291: step 28540, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 67h:33m:40s remains)
INFO - root - 2017-12-10 15:40:33.742053: step 28550, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 68h:03m:06s remains)
INFO - root - 2017-12-10 15:40:41.691702: step 28560, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 68h:09m:58s remains)
INFO - root - 2017-12-10 15:40:49.645795: step 28570, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 64h:34m:45s remains)
INFO - root - 2017-12-10 15:40:57.436131: step 28580, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 65h:58m:19s remains)
INFO - root - 2017-12-10 15:41:05.377607: step 28590, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 67h:43m:06s remains)
INFO - root - 2017-12-10 15:41:13.052074: step 28600, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.805 sec/batch; 67h:58m:23s remains)
2017-12-10 15:41:13.882555: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14947394 0.13952662 0.17306258 0.25279123 0.36209688 0.47896791 0.57490677 0.61508352 0.58379716 0.49742714 0.39105928 0.29735738 0.23049003 0.19442625 0.17813791][0.16766866 0.14872079 0.176269 0.25092727 0.35622039 0.47054395 0.56628895 0.60755229 0.57594889 0.4858844 0.37092933 0.26506543 0.18812966 0.14875016 0.13621724][0.17899039 0.15071495 0.16774397 0.23108475 0.32759291 0.43647334 0.52926725 0.57023597 0.54018044 0.45053342 0.33075783 0.21605201 0.13321072 0.09465345 0.088120468][0.1681598 0.13597806 0.14578834 0.20090964 0.29335895 0.40212166 0.49615091 0.53913856 0.51195031 0.42293122 0.29884475 0.17765866 0.092676841 0.058482379 0.059386164][0.11970883 0.09326151 0.10460067 0.16002421 0.25713179 0.3746767 0.478239 0.52951884 0.50855845 0.42157188 0.29485169 0.17164764 0.090769865 0.06677556 0.078609638][0.050137576 0.032927148 0.050030019 0.11021267 0.21573897 0.34575596 0.46267912 0.52601993 0.51472324 0.43437806 0.31196129 0.19633271 0.12950563 0.12303527 0.14975129][-0.023539841 -0.035426393 -0.01505114 0.048537888 0.16013305 0.30030271 0.43049967 0.509334 0.51475823 0.45136937 0.3457118 0.2493155 0.2032813 0.21460742 0.2524358][-0.074539237 -0.091201365 -0.074874975 -0.012043694 0.10309573 0.25212359 0.39682978 0.49519211 0.52339089 0.48360449 0.40051585 0.32502359 0.2946465 0.31318727 0.35071567][-0.0877938 -0.11749698 -0.11266411 -0.056295916 0.056289654 0.20728564 0.36117822 0.47683761 0.52841741 0.51411128 0.45412639 0.39492464 0.36902791 0.38082978 0.40666169][-0.067230858 -0.11363044 -0.12512444 -0.081245817 0.021368029 0.16585207 0.3209413 0.44787505 0.51904052 0.52762669 0.4875637 0.43787271 0.40610993 0.39995649 0.40526924][-0.0075168004 -0.074342854 -0.10906391 -0.086938038 -0.004549202 0.1226781 0.26802155 0.39533046 0.47589004 0.49894404 0.47269607 0.42699531 0.38457233 0.3567954 0.33919936][0.075189881 -0.014931237 -0.077320747 -0.082372792 -0.026468819 0.076243006 0.20245521 0.31872958 0.39692357 0.42454198 0.40589258 0.36153921 0.30905691 0.26234379 0.22510585][0.15342547 0.042153664 -0.046866059 -0.078573085 -0.049918875 0.025862388 0.12748675 0.22398664 0.28928632 0.31186137 0.29499462 0.25187102 0.19511028 0.13917401 0.092498243][0.20433341 0.078534968 -0.029322037 -0.081845105 -0.078051016 -0.030822527 0.040963572 0.11044358 0.15568675 0.16819148 0.15176155 0.11476322 0.0650623 0.015013428 -0.026520936][0.21709457 0.089166872 -0.02461032 -0.0871455 -0.1000457 -0.076576985 -0.033137377 0.00891943 0.033430949 0.035616573 0.020018883 -0.0076978193 -0.043143746 -0.0777493 -0.1053477]]...]
INFO - root - 2017-12-10 15:41:21.990425: step 28610, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 67h:11m:00s remains)
INFO - root - 2017-12-10 15:41:29.829701: step 28620, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 66h:05m:48s remains)
INFO - root - 2017-12-10 15:41:37.717462: step 28630, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 63h:49m:48s remains)
INFO - root - 2017-12-10 15:41:45.549235: step 28640, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 66h:30m:10s remains)
INFO - root - 2017-12-10 15:41:53.371156: step 28650, loss = 0.70, batch loss = 0.65 (10.6 examples/sec; 0.755 sec/batch; 63h:43m:36s remains)
INFO - root - 2017-12-10 15:42:01.326827: step 28660, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.820 sec/batch; 69h:12m:01s remains)
INFO - root - 2017-12-10 15:42:09.160848: step 28670, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.819 sec/batch; 69h:06m:38s remains)
INFO - root - 2017-12-10 15:42:16.801249: step 28680, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 67h:00m:37s remains)
INFO - root - 2017-12-10 15:42:24.676382: step 28690, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 65h:51m:33s remains)
INFO - root - 2017-12-10 15:42:32.551884: step 28700, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 67h:10m:11s remains)
2017-12-10 15:42:33.390144: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.37668177 0.31132692 0.24472775 0.20171732 0.19577131 0.21379976 0.24109077 0.276168 0.3256115 0.38740999 0.45016402 0.49764442 0.50931418 0.4716537 0.40272114][0.42940786 0.36250457 0.29649433 0.25608924 0.25220853 0.26845822 0.28743988 0.30377671 0.32350081 0.3508431 0.38512895 0.41812941 0.43251741 0.41287008 0.37022594][0.48098612 0.41292277 0.34778348 0.30997267 0.3080287 0.3244535 0.338105 0.33765253 0.3258042 0.31238595 0.30771118 0.3144311 0.32234773 0.31673744 0.30055332][0.52904165 0.45630461 0.38792557 0.34982234 0.34924769 0.36912 0.3837117 0.37409061 0.33815208 0.28945631 0.24834709 0.22710276 0.2224735 0.22171043 0.22088505][0.55088615 0.47159073 0.40023434 0.36588812 0.37370753 0.40598097 0.43041804 0.41908824 0.36468065 0.2845256 0.20815793 0.15641074 0.13274878 0.12681517 0.12975174][0.53469926 0.4500477 0.37904793 0.35383603 0.37724733 0.43021634 0.47368321 0.47003138 0.40637788 0.30208826 0.19416413 0.11174879 0.065753788 0.048843324 0.048971407][0.47389546 0.38622826 0.31872094 0.30635077 0.35081086 0.43105552 0.5017606 0.51548707 0.45371583 0.33573031 0.20328899 0.093967266 0.027717782 0.0012396851 -0.00030744934][0.36725625 0.28046224 0.22187288 0.2274674 0.29769692 0.40732393 0.50602919 0.53966445 0.48643884 0.36443204 0.21750979 0.089969054 0.0096842581 -0.021108476 -0.019379258][0.249529 0.16940516 0.12339137 0.14647743 0.23724169 0.36644256 0.48295802 0.53183764 0.49094903 0.37624344 0.22978108 0.098368816 0.014177567 -0.014830574 -0.005713074][0.16821499 0.10348896 0.072520465 0.10774544 0.20642062 0.33903572 0.45863625 0.51487005 0.48692253 0.38778946 0.2545014 0.13210361 0.053305041 0.030205758 0.047476612][0.14925559 0.1056905 0.089134939 0.1285769 0.22206314 0.34431416 0.45497915 0.5113523 0.49493602 0.41510573 0.30222693 0.1964201 0.1286352 0.11219888 0.13414255][0.18631372 0.16598926 0.16080125 0.19637942 0.27430305 0.37675539 0.47143179 0.52404225 0.51809007 0.45915747 0.37033281 0.28480995 0.22935273 0.21597067 0.23445722][0.25385237 0.25799891 0.26256734 0.28866726 0.34192139 0.41409072 0.48463842 0.52967376 0.53388506 0.49775571 0.43639815 0.37388387 0.32972118 0.31308606 0.31770775][0.32888702 0.35603446 0.36796275 0.38006818 0.40213266 0.43654981 0.4772945 0.51148474 0.525033 0.51278543 0.48072031 0.44239694 0.40844595 0.38475367 0.36916822][0.38757187 0.4333964 0.452093 0.45380482 0.4516376 0.45691231 0.47523662 0.50199354 0.52473992 0.5336858 0.52629018 0.50631285 0.47687241 0.44100258 0.40019912]]...]
INFO - root - 2017-12-10 15:42:41.306196: step 28710, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 65h:34m:10s remains)
INFO - root - 2017-12-10 15:42:49.060122: step 28720, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 68h:35m:18s remains)
INFO - root - 2017-12-10 15:42:56.966061: step 28730, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 66h:26m:13s remains)
INFO - root - 2017-12-10 15:43:04.861775: step 28740, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 65h:18m:57s remains)
INFO - root - 2017-12-10 15:43:12.846169: step 28750, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 67h:34m:19s remains)
INFO - root - 2017-12-10 15:43:20.555702: step 28760, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 65h:46m:48s remains)
INFO - root - 2017-12-10 15:43:28.432059: step 28770, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 67h:08m:24s remains)
INFO - root - 2017-12-10 15:43:36.414122: step 28780, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.812 sec/batch; 68h:28m:41s remains)
INFO - root - 2017-12-10 15:43:44.286763: step 28790, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 66h:06m:02s remains)
INFO - root - 2017-12-10 15:43:52.128184: step 28800, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 64h:55m:49s remains)
2017-12-10 15:43:52.965546: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30460128 0.33009949 0.34686103 0.35177189 0.33677259 0.30323046 0.25964171 0.21591322 0.17042872 0.11873945 0.066300407 0.019187981 -0.018829575 -0.046115614 -0.059719555][0.35173598 0.37340242 0.37683302 0.36213416 0.32557827 0.2740196 0.2218513 0.17892137 0.14140274 0.10012849 0.056631189 0.015084184 -0.020201609 -0.046511363 -0.061014995][0.37447846 0.39154935 0.38197932 0.35063544 0.29860142 0.23858406 0.18811145 0.15381722 0.12763694 0.097587213 0.061610896 0.022781793 -0.01296801 -0.040681794 -0.056944285][0.36822158 0.38463464 0.37003794 0.33342364 0.27855936 0.22320937 0.18506449 0.16449097 0.14802462 0.12272623 0.087220915 0.04425576 0.0017550584 -0.031780723 -0.051558368][0.32989517 0.34971356 0.34015641 0.31185928 0.267528 0.22809011 0.20908593 0.20316209 0.19186406 0.16391578 0.12194213 0.069529206 0.016605817 -0.025056886 -0.049059313][0.26181147 0.28705835 0.290182 0.28021118 0.2558575 0.23911515 0.24213324 0.25061291 0.24175465 0.20737529 0.15592521 0.092024751 0.027433602 -0.022667035 -0.050297957][0.17465298 0.20320097 0.2203503 0.23096544 0.22968033 0.23683968 0.26100105 0.28314894 0.27786061 0.23982075 0.18155693 0.10899469 0.034886964 -0.022639539 -0.053508949][0.087530889 0.11465171 0.14031523 0.16557236 0.18285359 0.21095793 0.25429559 0.28980348 0.29144338 0.25564107 0.1965718 0.12016504 0.039969642 -0.022657307 -0.05578744][0.025023473 0.046080086 0.071554437 0.10085193 0.12751561 0.17015494 0.22891644 0.27731067 0.28812274 0.25729689 0.20065281 0.12491084 0.043420829 -0.021477509 -0.056118984][0.008259098 0.021560574 0.040168393 0.064282537 0.0916211 0.14196476 0.21057191 0.26681679 0.28238562 0.25363657 0.19761018 0.12243161 0.041778874 -0.022377461 -0.056556486][0.041054867 0.047321636 0.05650422 0.071057178 0.094742231 0.14854926 0.22159687 0.27854279 0.29046312 0.25633952 0.19525994 0.11742054 0.036834642 -0.025750676 -0.058120351][0.11146028 0.11257768 0.11219137 0.11661033 0.13575286 0.18976861 0.26065841 0.30982041 0.30985194 0.26417363 0.19381535 0.11057367 0.028768277 -0.032237586 -0.06204858][0.19623871 0.19420102 0.18552186 0.18096614 0.19545287 0.24676622 0.31103334 0.34837407 0.33421457 0.2760033 0.19548561 0.10524573 0.020442499 -0.040095821 -0.0677222][0.26798493 0.26469132 0.24975267 0.23761082 0.2466632 0.29231879 0.34800935 0.37490818 0.35089815 0.28517997 0.1984868 0.10345406 0.015906021 -0.045214571 -0.071983166][0.30374941 0.300882 0.28224188 0.26375684 0.26603669 0.30397195 0.35208848 0.37397817 0.34808171 0.28321424 0.19770792 0.10334628 0.015924981 -0.045522965 -0.07257314]]...]
INFO - root - 2017-12-10 15:44:00.853435: step 28810, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.809 sec/batch; 68h:14m:29s remains)
INFO - root - 2017-12-10 15:44:08.843769: step 28820, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.812 sec/batch; 68h:28m:27s remains)
INFO - root - 2017-12-10 15:44:16.558412: step 28830, loss = 0.69, batch loss = 0.63 (11.6 examples/sec; 0.689 sec/batch; 58h:09m:04s remains)
INFO - root - 2017-12-10 15:44:24.396763: step 28840, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 65h:57m:35s remains)
INFO - root - 2017-12-10 15:44:32.485667: step 28850, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 66h:55m:44s remains)
INFO - root - 2017-12-10 15:44:40.454811: step 28860, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.816 sec/batch; 68h:48m:53s remains)
INFO - root - 2017-12-10 15:44:48.328756: step 28870, loss = 0.69, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 63h:42m:10s remains)
INFO - root - 2017-12-10 15:44:56.208641: step 28880, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 67h:57m:52s remains)
INFO - root - 2017-12-10 15:45:04.128272: step 28890, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 66h:20m:27s remains)
INFO - root - 2017-12-10 15:45:11.875972: step 28900, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 67h:19m:36s remains)
2017-12-10 15:45:12.671396: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.080377594 -0.08354874 -0.084574111 -0.084242709 -0.083139807 -0.081709825 -0.080517292 -0.0804741 -0.081297532 -0.083272956 -0.085013554 -0.08627364 -0.08619827 -0.084848531 -0.083023317][-0.083658285 -0.087588862 -0.088715747 -0.087321863 -0.083576605 -0.078058586 -0.072425082 -0.069162875 -0.069207862 -0.073250428 -0.079241566 -0.085467659 -0.089353055 -0.090074256 -0.088408031][-0.08230301 -0.084070019 -0.081358828 -0.073433757 -0.060189594 -0.043665126 -0.02806144 -0.018294044 -0.016877372 -0.025316279 -0.041211031 -0.060225684 -0.076002367 -0.085176975 -0.08808513][-0.075702153 -0.070400506 -0.056929912 -0.033430319 0.00026283896 0.038566507 0.07200411 0.091867805 0.093955584 0.075981341 0.041177634 -0.0018120075 -0.040525254 -0.067222379 -0.081259355][-0.063263305 -0.045642167 -0.014190366 0.034101874 0.099221364 0.17003581 0.22889642 0.26157373 0.26185369 0.22739421 0.16340189 0.084875993 0.012776929 -0.039098807 -0.069357686][-0.048261423 -0.014985535 0.038679041 0.11662534 0.21812983 0.32538408 0.41144687 0.4561857 0.4513227 0.3951318 0.29599994 0.17705479 0.068657294 -0.0098077552 -0.057010431][-0.036379687 0.011281861 0.085047506 0.18849821 0.31946719 0.45447537 0.5592438 0.60947329 0.595982 0.51831388 0.38855106 0.23725919 0.10205614 0.0056358338 -0.051902462][-0.034903 0.019845491 0.10345494 0.21771955 0.35875478 0.50064075 0.60632473 0.6510672 0.62677 0.53496242 0.39081135 0.22855157 0.088215746 -0.00794059 -0.062144771][-0.046190668 0.0031518862 0.079921119 0.18375897 0.30959749 0.43330395 0.52088428 0.551249 0.51915377 0.42843458 0.29477224 0.15023832 0.030713666 -0.045726746 -0.083610989][-0.057705533 -0.0259076 0.027625723 0.10186426 0.19198109 0.27885482 0.33591238 0.34844995 0.31495363 0.24079044 0.1393216 0.035270173 -0.044919379 -0.089725986 -0.10525955][-0.04864832 -0.04025111 -0.019069731 0.015422025 0.060353585 0.10343863 0.12733325 0.12395481 0.094724737 0.04519546 -0.015437453 -0.072160274 -0.10968249 -0.123242 -0.11920534][-0.0061824727 -0.018577628 -0.028539937 -0.032053582 -0.028896457 -0.024419796 -0.027734656 -0.042280596 -0.065477513 -0.0927688 -0.11938743 -0.13894929 -0.14523248 -0.13804252 -0.12299491][0.06184743 0.038575623 0.0082492428 -0.021140406 -0.046232373 -0.066995546 -0.0870844 -0.10779677 -0.12689805 -0.14130378 -0.14964448 -0.15104856 -0.14428172 -0.13093491 -0.11555094][0.12896386 0.10718163 0.071815729 0.033569042 -0.0021737739 -0.032420337 -0.0583473 -0.08094117 -0.098640554 -0.10925975 -0.11345404 -0.11282026 -0.10779963 -0.099947855 -0.092951775][0.16417246 0.15294972 0.12582892 0.093521595 0.061795056 0.034532372 0.010773644 -0.010938178 -0.02876607 -0.039982129 -0.045856334 -0.048862416 -0.049938805 -0.05045687 -0.054157075]]...]
INFO - root - 2017-12-10 15:45:20.433621: step 28910, loss = 0.69, batch loss = 0.63 (12.8 examples/sec; 0.627 sec/batch; 52h:52m:08s remains)
INFO - root - 2017-12-10 15:45:28.320751: step 28920, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 66h:43m:32s remains)
INFO - root - 2017-12-10 15:45:36.287934: step 28930, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.819 sec/batch; 69h:01m:35s remains)
INFO - root - 2017-12-10 15:45:44.223661: step 28940, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 67h:26m:34s remains)
INFO - root - 2017-12-10 15:45:52.166256: step 28950, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 65h:59m:02s remains)
INFO - root - 2017-12-10 15:46:00.062530: step 28960, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 66h:37m:11s remains)
INFO - root - 2017-12-10 15:46:08.010709: step 28970, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 66h:06m:00s remains)
INFO - root - 2017-12-10 15:46:15.883665: step 28980, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 67h:02m:07s remains)
INFO - root - 2017-12-10 15:46:23.691784: step 28990, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 67h:44m:26s remains)
INFO - root - 2017-12-10 15:46:31.403971: step 29000, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 66h:14m:14s remains)
2017-12-10 15:46:32.276936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053471241 -0.06105756 -0.068921857 -0.07655859 -0.0821289 -0.084806487 -0.083428919 -0.074555226 -0.051456682 -0.002769581 0.075161576 0.17372431 0.28330663 0.39112484 0.48105621][-0.059251253 -0.066432685 -0.072534122 -0.077547118 -0.079661109 -0.077671967 -0.0698136 -0.051796205 -0.015206467 0.054960433 0.16061303 0.28715241 0.42151117 0.5456183 0.63572842][-0.065922715 -0.069046423 -0.06819015 -0.064152867 -0.055993382 -0.043058809 -0.024477983 0.0029112855 0.047069736 0.12549223 0.23859116 0.36865279 0.50232106 0.61972409 0.69438577][-0.064857133 -0.057458512 -0.040985744 -0.018299699 0.0091463318 0.039920457 0.072176762 0.10504332 0.14349993 0.2067081 0.29534426 0.39319691 0.49121475 0.57445866 0.62109911][-0.047880482 -0.021426026 0.021013809 0.072505705 0.12744345 0.18106315 0.22789848 0.26059535 0.27884865 0.30289754 0.3367762 0.37132114 0.40576798 0.4362897 0.45226246][-0.013506752 0.038804218 0.11454573 0.20040825 0.2856141 0.3619172 0.42063412 0.447426 0.43587068 0.40697896 0.37088224 0.32984748 0.29394782 0.27337757 0.26955587][0.028834764 0.10779656 0.21582636 0.33214724 0.44120315 0.53228885 0.59479207 0.60925674 0.56513023 0.48427007 0.38429305 0.28029788 0.19367103 0.14593019 0.14567243][0.061716981 0.15842152 0.28566694 0.41630739 0.53230745 0.622291 0.676408 0.67464846 0.60533118 0.49102694 0.35574859 0.22375746 0.12470385 0.085745566 0.11519679][0.068982683 0.16465092 0.28710335 0.40688479 0.50729251 0.57881337 0.61531985 0.60069072 0.52530795 0.41097248 0.28238934 0.16626441 0.095383756 0.095853373 0.16975597][0.048898149 0.12269518 0.21655861 0.30441603 0.37429029 0.42046058 0.44171527 0.42818838 0.37352914 0.29494867 0.21077217 0.14270711 0.12031819 0.1645781 0.27032346][0.015160447 0.055165462 0.10877838 0.15779367 0.19625293 0.22335121 0.24231014 0.24842785 0.23687233 0.2162775 0.19401649 0.18125147 0.1994095 0.26453462 0.3672325][-0.017142197 -0.0099684531 0.0072719958 0.025654333 0.044639893 0.068330288 0.10281147 0.14484067 0.18766308 0.22907995 0.26529244 0.29480526 0.3281967 0.37853909 0.43853533][-0.037836123 -0.052149281 -0.056496561 -0.052073948 -0.035297792 0.002078125 0.066574588 0.15209879 0.2470126 0.33868563 0.41504538 0.46332338 0.48352182 0.48855421 0.47920376][-0.043423045 -0.064199992 -0.07328967 -0.06719584 -0.03985988 0.019510102 0.11585017 0.23772913 0.36781251 0.48555911 0.575013 0.61690044 0.6042034 0.55220658 0.47252822][-0.036796402 -0.053015824 -0.056488145 -0.042063341 -0.0028544159 0.072512887 0.18631712 0.32207972 0.45917252 0.57346785 0.64971662 0.668005 0.6198169 0.52355051 0.3989366]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 15:46:40.237819: step 29010, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 68h:44m:41s remains)
INFO - root - 2017-12-10 15:46:48.242883: step 29020, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 67h:05m:17s remains)
INFO - root - 2017-12-10 15:46:56.067641: step 29030, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 65h:49m:04s remains)
INFO - root - 2017-12-10 15:47:04.014065: step 29040, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 67h:43m:06s remains)
INFO - root - 2017-12-10 15:47:11.982574: step 29050, loss = 0.71, batch loss = 0.65 (9.3 examples/sec; 0.861 sec/batch; 72h:33m:08s remains)
INFO - root - 2017-12-10 15:47:19.881238: step 29060, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 67h:08m:41s remains)
INFO - root - 2017-12-10 15:47:27.504709: step 29070, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.515 sec/batch; 43h:25m:46s remains)
INFO - root - 2017-12-10 15:47:35.421470: step 29080, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 63h:38m:21s remains)
INFO - root - 2017-12-10 15:47:43.270142: step 29090, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.752 sec/batch; 63h:24m:00s remains)
INFO - root - 2017-12-10 15:47:51.148477: step 29100, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 66h:59m:47s remains)
2017-12-10 15:47:52.092764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.056704767 -0.054939035 -0.053561214 -0.052971307 -0.053831194 -0.056464955 -0.061182346 -0.065844283 -0.069332413 -0.071040392 -0.069222443 -0.062860936 -0.053990882 -0.0460806 -0.039587807][-0.038242098 -0.030838152 -0.024744648 -0.021300184 -0.021728504 -0.026291788 -0.034850463 -0.045254171 -0.0565681 -0.066647179 -0.071907565 -0.070034876 -0.062296327 -0.053029943 -0.043003414][-0.010191466 0.0074688266 0.023749391 0.036387313 0.041974176 0.038815569 0.027082585 0.0085066743 -0.016091997 -0.042734444 -0.064234853 -0.075322382 -0.075446524 -0.068516724 -0.056185238][0.025290957 0.055979304 0.0870586 0.11526966 0.13251519 0.1344022 0.1206872 0.092764974 0.0519639 0.0041674217 -0.039586138 -0.069804236 -0.083394416 -0.083160006 -0.071914755][0.067232937 0.11284313 0.16203663 0.21022892 0.24305476 0.25351456 0.24076979 0.20587391 0.15026703 0.081000879 0.013353491 -0.03853216 -0.068687424 -0.078615531 -0.073095664][0.11712932 0.17735197 0.24343456 0.30947277 0.35625711 0.37623486 0.36898041 0.33450451 0.27192485 0.18716936 0.099355273 0.028095147 -0.017563706 -0.038853992 -0.042947412][0.17485355 0.24599683 0.32202744 0.39702109 0.45133066 0.4800373 0.48327416 0.45897964 0.40136439 0.31200668 0.21223104 0.12750219 0.06952814 0.037067026 0.020851633][0.22787149 0.30206162 0.37711853 0.44892079 0.50268406 0.53886795 0.55694634 0.55170882 0.51073182 0.42960271 0.32977644 0.24087475 0.17613265 0.13403739 0.10534634][0.26785576 0.33876255 0.40409604 0.46197379 0.50575083 0.542485 0.57179421 0.58657461 0.56938416 0.510073 0.42506492 0.34280789 0.27706948 0.22765699 0.1884477][0.29239663 0.35621095 0.40741983 0.44638196 0.4741132 0.50281769 0.53463882 0.56428474 0.57203907 0.54254276 0.48269179 0.41389796 0.34959748 0.29373074 0.24695688][0.30318189 0.35727033 0.39450148 0.41666731 0.42875338 0.44475451 0.47163567 0.51006991 0.53969681 0.53979373 0.50564265 0.44917059 0.38327554 0.31820351 0.26373497][0.31233954 0.35646936 0.38291794 0.39380988 0.39398426 0.39633352 0.41334713 0.45438039 0.49883628 0.52043009 0.50458 0.45483473 0.38256168 0.30325285 0.2361604][0.32830459 0.365194 0.38620979 0.39242145 0.38568288 0.37580341 0.37977657 0.41517428 0.46218747 0.49131808 0.48354372 0.43737522 0.36066157 0.26905426 0.18846203][0.35114363 0.384347 0.4045195 0.41178256 0.40394151 0.38633084 0.37713414 0.39911506 0.43562061 0.45857063 0.44976676 0.40565494 0.32868519 0.23041138 0.13989951][0.37492967 0.4051632 0.42405111 0.43395668 0.43038386 0.41397154 0.39886782 0.40732437 0.42667183 0.43464279 0.41729489 0.37155634 0.29490662 0.19404511 0.098269477]]...]
INFO - root - 2017-12-10 15:47:59.875137: step 29110, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 65h:42m:31s remains)
INFO - root - 2017-12-10 15:48:07.647182: step 29120, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 65h:33m:11s remains)
INFO - root - 2017-12-10 15:48:15.480325: step 29130, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 67h:49m:28s remains)
INFO - root - 2017-12-10 15:48:23.492203: step 29140, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 66h:58m:01s remains)
INFO - root - 2017-12-10 15:48:31.295031: step 29150, loss = 0.69, batch loss = 0.63 (12.0 examples/sec; 0.664 sec/batch; 55h:58m:52s remains)
INFO - root - 2017-12-10 15:48:39.078480: step 29160, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.796 sec/batch; 67h:06m:40s remains)
INFO - root - 2017-12-10 15:48:47.088128: step 29170, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 66h:47m:50s remains)
INFO - root - 2017-12-10 15:48:55.014550: step 29180, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 65h:57m:21s remains)
INFO - root - 2017-12-10 15:49:02.919645: step 29190, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 65h:34m:20s remains)
INFO - root - 2017-12-10 15:49:10.849670: step 29200, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 65h:02m:35s remains)
2017-12-10 15:49:11.762318: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.061265849 0.12680623 0.1993833 0.25312734 0.28353631 0.30306044 0.30977517 0.2983354 0.27427217 0.25209424 0.24398328 0.25706 0.27725044 0.29561672 0.34486929][0.10163816 0.19982731 0.30937907 0.39394161 0.44505775 0.48183149 0.49722481 0.48038369 0.44285744 0.41089913 0.39710456 0.40929645 0.43461075 0.46712208 0.54922062][0.12786245 0.25460005 0.39447349 0.50427824 0.57450444 0.62879252 0.65195775 0.62867767 0.5796476 0.542803 0.52770752 0.53662223 0.55868393 0.59332776 0.68803227][0.13867854 0.28437909 0.4431864 0.56784576 0.64842451 0.711144 0.73579192 0.70571363 0.64959013 0.61422986 0.60505128 0.6125226 0.62405241 0.64273161 0.71662152][0.14840636 0.30813846 0.48277867 0.61963862 0.70556027 0.77008 0.79621744 0.76657593 0.71259022 0.68230104 0.68046159 0.68486118 0.6758526 0.6579988 0.67774057][0.16059968 0.33302388 0.52438205 0.67699236 0.76947528 0.83762515 0.87250125 0.85415834 0.81124663 0.78617728 0.7862193 0.77757925 0.73213762 0.65785819 0.60244244][0.16833547 0.35357535 0.56277066 0.73439664 0.83790946 0.91810977 0.97245407 0.97393775 0.946689 0.92503 0.92180485 0.8937934 0.80475307 0.66932142 0.5409258][0.17689013 0.37448159 0.59816617 0.78571391 0.899648 0.99546796 1.0716531 1.0903662 1.0719296 1.0478195 1.0397261 0.99457794 0.86780488 0.6835103 0.503857][0.18170844 0.37657836 0.59415179 0.77746463 0.89064735 0.99408537 1.0825243 1.1084499 1.0920174 1.0660468 1.0589089 1.0086887 0.866376 0.66519749 0.47229296][0.16756791 0.33981 0.52814275 0.68497962 0.78243864 0.87706363 0.96027005 0.98205459 0.96392304 0.94073856 0.94177306 0.90015244 0.76767045 0.582607 0.41047591][0.12135923 0.25602096 0.40100655 0.51909572 0.59085953 0.66193527 0.72335333 0.73366189 0.71485883 0.699171 0.71057826 0.68423784 0.5780468 0.42943653 0.29663363][0.045977395 0.13925505 0.24049152 0.3213526 0.36781636 0.41199666 0.44643298 0.44286108 0.42247811 0.41156417 0.42604539 0.41032448 0.33293268 0.22748233 0.14018135][-0.032295596 0.02260158 0.08512897 0.13412297 0.15945721 0.18085133 0.19284371 0.17885415 0.15708722 0.14609335 0.15472233 0.14214338 0.089485586 0.02497918 -0.0192814][-0.08693967 -0.062577926 -0.030825296 -0.007400204 0.001087185 0.0052434551 0.0023937884 -0.015777722 -0.036284823 -0.048078556 -0.047098394 -0.057703134 -0.088431321 -0.1189404 -0.13077098][-0.11345021 -0.1095674 -0.098604694 -0.092065707 -0.094195306 -0.10030121 -0.11078521 -0.12845002 -0.14570613 -0.15673827 -0.15973142 -0.16583326 -0.17829545 -0.18634605 -0.18172657]]...]
INFO - root - 2017-12-10 15:49:19.633210: step 29210, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 66h:14m:06s remains)
INFO - root - 2017-12-10 15:49:27.523716: step 29220, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 64h:31m:37s remains)
INFO - root - 2017-12-10 15:49:35.280152: step 29230, loss = 0.68, batch loss = 0.63 (11.0 examples/sec; 0.726 sec/batch; 61h:08m:05s remains)
INFO - root - 2017-12-10 15:49:43.175134: step 29240, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 67h:06m:18s remains)
INFO - root - 2017-12-10 15:49:50.898805: step 29250, loss = 0.67, batch loss = 0.61 (9.9 examples/sec; 0.807 sec/batch; 67h:59m:36s remains)
INFO - root - 2017-12-10 15:49:58.947183: step 29260, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 64h:35m:33s remains)
INFO - root - 2017-12-10 15:50:06.866822: step 29270, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 65h:41m:12s remains)
INFO - root - 2017-12-10 15:50:14.800960: step 29280, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 66h:24m:41s remains)
INFO - root - 2017-12-10 15:50:22.699802: step 29290, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.799 sec/batch; 67h:20m:00s remains)
INFO - root - 2017-12-10 15:50:30.499649: step 29300, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 65h:02m:51s remains)
2017-12-10 15:50:31.277090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.083696082 -0.077820733 -0.067342043 -0.057943687 -0.053026423 -0.05408664 -0.057190817 -0.061369382 -0.067241363 -0.074376389 -0.080774665 -0.085870668 -0.094004154 -0.10325578 -0.10927053][-0.065076582 -0.046967108 -0.021163635 0.0041693239 0.021030121 0.026159057 0.026951836 0.022816122 0.010541012 -0.0092756692 -0.028734136 -0.045107532 -0.065730631 -0.088027559 -0.10470883][-0.035925146 0.0020437166 0.054542851 0.11013396 0.15231869 0.17343824 0.18670088 0.18792295 0.16882275 0.12905176 0.086733274 0.050281137 0.0075035593 -0.037807509 -0.074921742][2.9708865e-05 0.064926118 0.15524791 0.25457424 0.33445793 0.3816916 0.41858047 0.4345113 0.41194546 0.34847018 0.27654764 0.21199748 0.13503237 0.052459832 -0.018170319][0.040222734 0.13553649 0.26947561 0.41942549 0.54213452 0.61999977 0.688588 0.72729445 0.70415 0.61366349 0.50694025 0.40770638 0.28716114 0.15762806 0.04633832][0.075359471 0.19870938 0.37336031 0.57100564 0.73241335 0.83849525 0.9396348 1.0022993 0.97735685 0.85760355 0.715481 0.58055669 0.41662136 0.24328023 0.097162269][0.09913227 0.24340944 0.44875404 0.68023 0.86558342 0.98797673 1.1095102 1.1833016 1.1481779 1.0008253 0.83069468 0.66922182 0.47572073 0.27676207 0.11429282][0.10946354 0.26456112 0.48446229 0.72668642 0.9113338 1.0280669 1.1454879 1.208491 1.1541892 0.9875446 0.80592728 0.637214 0.43787006 0.23888338 0.083194338][0.10707534 0.26082125 0.47519788 0.70099425 0.8577314 0.944616 1.0312344 1.0632606 0.98680115 0.81593519 0.64417535 0.4908464 0.31182998 0.13842155 0.010699769][0.086704291 0.22463563 0.41176754 0.59665751 0.70641285 0.74929315 0.79021966 0.78461951 0.69398266 0.53738356 0.39369148 0.27287814 0.13492221 0.0073217931 -0.0779081][0.040409006 0.14788428 0.28958118 0.41872248 0.47818378 0.48180425 0.48362881 0.45249075 0.3650673 0.24036095 0.13632832 0.055679843 -0.033290919 -0.10905699 -0.14999972][-0.024005964 0.045021135 0.13429227 0.20739466 0.22701474 0.20827325 0.19020857 0.15319276 0.0851822 0.0010842953 -0.062113177 -0.10521613 -0.1500583 -0.1818465 -0.18869059][-0.086155966 -0.053751945 -0.0099010114 0.020690152 0.017811818 -0.0056161024 -0.025559863 -0.05367117 -0.095529228 -0.14215577 -0.17263806 -0.18822667 -0.20198593 -0.20551233 -0.19386448][-0.129737 -0.12445424 -0.11078616 -0.10377861 -0.11257882 -0.128775 -0.14078811 -0.15524662 -0.17441051 -0.19389576 -0.20324735 -0.20300275 -0.19980517 -0.19039139 -0.1738999][-0.1475886 -0.15756561 -0.15937585 -0.16150568 -0.16725877 -0.17292841 -0.17522308 -0.17796116 -0.18208213 -0.18588978 -0.18485074 -0.17877035 -0.17026052 -0.15890656 -0.14551243]]...]
INFO - root - 2017-12-10 15:50:38.989245: step 29310, loss = 0.71, batch loss = 0.65 (11.5 examples/sec; 0.695 sec/batch; 58h:33m:01s remains)
INFO - root - 2017-12-10 15:50:46.900361: step 29320, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 67h:31m:19s remains)
INFO - root - 2017-12-10 15:50:54.787562: step 29330, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 65h:51m:32s remains)
INFO - root - 2017-12-10 15:51:02.484796: step 29340, loss = 0.71, batch loss = 0.66 (10.5 examples/sec; 0.762 sec/batch; 64h:08m:16s remains)
INFO - root - 2017-12-10 15:51:10.363793: step 29350, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 67h:58m:17s remains)
INFO - root - 2017-12-10 15:51:18.289732: step 29360, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 66h:39m:35s remains)
INFO - root - 2017-12-10 15:51:26.206510: step 29370, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 67h:40m:45s remains)
INFO - root - 2017-12-10 15:51:34.046733: step 29380, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 65h:56m:03s remains)
INFO - root - 2017-12-10 15:51:41.901964: step 29390, loss = 0.68, batch loss = 0.62 (11.1 examples/sec; 0.722 sec/batch; 60h:47m:23s remains)
INFO - root - 2017-12-10 15:51:49.629961: step 29400, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 65h:23m:01s remains)
2017-12-10 15:51:50.425795: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013807236 -0.000407959 -0.0043703425 0.012193722 0.038178094 0.05783543 0.065565251 0.064915188 0.056479473 0.03747474 0.0070364946 -0.025692811 -0.052757706 -0.0730389 -0.085871086][0.085635312 0.067038968 0.058111072 0.082418509 0.12328985 0.15718798 0.17470691 0.17810698 0.1656781 0.13479368 0.086954974 0.035201553 -0.0097358115 -0.047262348 -0.074409455][0.17063586 0.15268694 0.14476578 0.18172722 0.24022047 0.28873256 0.31374276 0.31767872 0.29893783 0.2557556 0.19072363 0.1183373 0.051875994 -0.0072370572 -0.052955415][0.24332565 0.2317144 0.23430762 0.28872722 0.36432046 0.42335191 0.45066196 0.4506475 0.42485726 0.37221506 0.294287 0.20334288 0.11397988 0.032582186 -0.03143261][0.28696969 0.286753 0.30595997 0.37787056 0.46684095 0.53230435 0.55934888 0.55446362 0.52328 0.4657447 0.37956977 0.27244857 0.16052617 0.058673527 -0.019568613][0.3175309 0.33484241 0.37338942 0.45987 0.55745924 0.6261797 0.65237021 0.642944 0.60474408 0.53895289 0.44115984 0.31711537 0.1852006 0.06743893 -0.019337326][0.34986475 0.38765144 0.44329441 0.53985405 0.64202285 0.71241432 0.73801178 0.72334146 0.67252737 0.58926517 0.47279096 0.33166474 0.1868588 0.062211841 -0.025507852][0.377855 0.43111163 0.49868158 0.60102689 0.70452142 0.77477205 0.7982924 0.77557826 0.70707792 0.60089415 0.46491098 0.31163546 0.16389395 0.043520663 -0.036548022][0.38252836 0.44366604 0.51869869 0.62371862 0.72479987 0.78994668 0.80599827 0.77174616 0.68595952 0.56147683 0.41518357 0.26189941 0.1230289 0.016720887 -0.049151033][0.34859163 0.40893385 0.48656636 0.58928126 0.68237758 0.73644644 0.73932976 0.69100589 0.59280872 0.46254888 0.32162118 0.18395509 0.066632666 -0.016872102 -0.063636638][0.27935916 0.33307546 0.40799987 0.50319123 0.58334696 0.62188208 0.60870856 0.54757679 0.44570848 0.323871 0.20264623 0.092748947 0.0054233628 -0.051212274 -0.078347348][0.18349124 0.22704665 0.29308105 0.37306818 0.43431398 0.45536551 0.42961806 0.36414659 0.27078843 0.17024161 0.078740247 0.0027067643 -0.05195364 -0.081900015 -0.091315635][0.076499052 0.10744796 0.15838774 0.21566905 0.2536588 0.25869823 0.22870384 0.17189829 0.10013258 0.029965475 -0.027770264 -0.070408858 -0.095968962 -0.10428225 -0.10060853][-0.015829129 0.0029703332 0.03578645 0.0683515 0.084715888 0.079001419 0.052740395 0.013129379 -0.03104143 -0.069266558 -0.095980212 -0.11141694 -0.11621208 -0.11160131 -0.10189579][-0.080931567 -0.072645895 -0.056315593 -0.0435516 -0.041686516 -0.051274646 -0.069324449 -0.090906657 -0.11092236 -0.12427718 -0.12932092 -0.12768336 -0.12076931 -0.11030619 -0.098861426]]...]
INFO - root - 2017-12-10 15:51:58.324886: step 29410, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 65h:08m:35s remains)
INFO - root - 2017-12-10 15:52:06.276220: step 29420, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.827 sec/batch; 69h:38m:12s remains)
INFO - root - 2017-12-10 15:52:13.956507: step 29430, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 64h:01m:11s remains)
INFO - root - 2017-12-10 15:52:21.721164: step 29440, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 65h:05m:22s remains)
INFO - root - 2017-12-10 15:52:29.659574: step 29450, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 67h:14m:09s remains)
INFO - root - 2017-12-10 15:52:37.504491: step 29460, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 65h:19m:49s remains)
INFO - root - 2017-12-10 15:52:45.229898: step 29470, loss = 0.69, batch loss = 0.63 (12.4 examples/sec; 0.644 sec/batch; 54h:10m:37s remains)
INFO - root - 2017-12-10 15:52:53.074182: step 29480, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 67h:36m:13s remains)
INFO - root - 2017-12-10 15:53:00.888993: step 29490, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 66h:58m:36s remains)
INFO - root - 2017-12-10 15:53:08.766851: step 29500, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 66h:25m:40s remains)
2017-12-10 15:53:09.602662: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31955332 0.36268929 0.39145553 0.40122548 0.40266463 0.40787518 0.3980363 0.36573005 0.3237657 0.29096133 0.27612862 0.295897 0.33945161 0.38964745 0.43066359][0.41282058 0.46422517 0.49630317 0.50752681 0.51215541 0.52267009 0.51934695 0.49191189 0.44994551 0.41236612 0.3885099 0.39708784 0.42934787 0.47214776 0.50829339][0.48986971 0.54213929 0.57065737 0.57883471 0.58539295 0.60204273 0.60788941 0.58780706 0.546345 0.50306505 0.46847972 0.46226665 0.47847593 0.50826091 0.53504682][0.52662778 0.57319593 0.592549 0.5954389 0.60359055 0.62750649 0.64488018 0.63424051 0.59484428 0.54773927 0.50564355 0.4875828 0.49019533 0.50818223 0.52568406][0.52690303 0.56588322 0.57660639 0.57615435 0.58587712 0.61410522 0.63860804 0.63366526 0.595352 0.5454635 0.50028628 0.47697091 0.47345272 0.48502645 0.49443296][0.54115236 0.57774049 0.58541846 0.58638591 0.59835112 0.62495184 0.64705217 0.63941216 0.5979625 0.5440945 0.4982504 0.47586954 0.47334731 0.48310652 0.48493868][0.58566046 0.62926424 0.64084321 0.64698511 0.65960085 0.67684549 0.685842 0.66726404 0.62008929 0.5652703 0.52585864 0.5142476 0.52138644 0.53333342 0.527272][0.6242699 0.67700803 0.693659 0.70356256 0.71417111 0.72049433 0.7157796 0.688267 0.64171487 0.59598881 0.57299089 0.58009666 0.60096282 0.61472917 0.5978511][0.63055795 0.68886763 0.70845497 0.71948344 0.7270093 0.72544277 0.712557 0.68169594 0.64050955 0.60773563 0.6033076 0.62846386 0.66004092 0.672497 0.64483696][0.59336245 0.64947093 0.66896671 0.67917377 0.68322158 0.67612314 0.65861231 0.62697709 0.59132326 0.56884551 0.57788235 0.61412114 0.65007561 0.65872765 0.62436771][0.49997061 0.54517388 0.56014377 0.56756139 0.56890494 0.5596807 0.54193819 0.51327711 0.48346251 0.4674679 0.481944 0.51998633 0.55390775 0.55929029 0.52589345][0.3400017 0.36667103 0.37264961 0.37514934 0.3745538 0.36647671 0.35302967 0.33213869 0.31121323 0.30162045 0.31741047 0.35176307 0.3809993 0.38566735 0.35978612][0.14957768 0.15539995 0.15051572 0.14641777 0.14364706 0.13853043 0.13203798 0.12146851 0.11053083 0.10611882 0.11943273 0.14583759 0.16838923 0.17335626 0.15784398][-0.0072371303 -0.016495768 -0.028702993 -0.037755966 -0.042737111 -0.046206262 -0.048216693 -0.052019052 -0.057116777 -0.059952077 -0.051683597 -0.034463611 -0.01844056 -0.012284614 -0.017188333][-0.10877138 -0.12448258 -0.138256 -0.14851303 -0.15438028 -0.15738687 -0.15810303 -0.15937123 -0.16212387 -0.16478023 -0.16087206 -0.15100193 -0.14035819 -0.13362168 -0.13139561]]...]
INFO - root - 2017-12-10 15:53:17.563940: step 29510, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 68h:05m:29s remains)
INFO - root - 2017-12-10 15:53:25.317660: step 29520, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.824 sec/batch; 69h:21m:07s remains)
INFO - root - 2017-12-10 15:53:33.083260: step 29530, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 63h:34m:29s remains)
INFO - root - 2017-12-10 15:53:40.971477: step 29540, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 65h:27m:03s remains)
INFO - root - 2017-12-10 15:53:48.682746: step 29550, loss = 0.70, batch loss = 0.64 (12.8 examples/sec; 0.627 sec/batch; 52h:44m:06s remains)
INFO - root - 2017-12-10 15:53:56.483401: step 29560, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 66h:48m:52s remains)
INFO - root - 2017-12-10 15:54:04.390616: step 29570, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 65h:44m:06s remains)
INFO - root - 2017-12-10 15:54:12.216886: step 29580, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 66h:00m:51s remains)
INFO - root - 2017-12-10 15:54:20.068669: step 29590, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 67h:54m:48s remains)
INFO - root - 2017-12-10 15:54:27.999005: step 29600, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.819 sec/batch; 68h:56m:13s remains)
2017-12-10 15:54:28.822722: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.38077128 0.3400771 0.30547321 0.28686371 0.28152105 0.28132778 0.27684924 0.26181641 0.24757902 0.2530939 0.28292266 0.32777488 0.37565488 0.41715652 0.43760365][0.3288376 0.28872097 0.26054692 0.25369832 0.26431674 0.27919558 0.28547707 0.27599984 0.26257622 0.26382026 0.28471693 0.31868806 0.3593232 0.40038818 0.4262622][0.24819762 0.21538578 0.20109028 0.21530738 0.25175327 0.29082984 0.31406033 0.31217673 0.29586819 0.28016102 0.27143452 0.27026811 0.28171358 0.30604255 0.32956406][0.19733581 0.17665143 0.18254505 0.22702622 0.29892689 0.37161109 0.41866928 0.42685869 0.40414608 0.35997632 0.30369279 0.24454023 0.20313124 0.1906046 0.19838782][0.21199134 0.21248233 0.24754798 0.33036524 0.44228461 0.55123967 0.6225841 0.63693845 0.59986895 0.51876074 0.40873179 0.28470886 0.18004142 0.11751821 0.096051544][0.27810115 0.30943626 0.38369241 0.51043618 0.66263473 0.80616027 0.89821231 0.91106707 0.85012656 0.72818655 0.56916362 0.38903132 0.22727671 0.11525139 0.059856005][0.33538464 0.40496793 0.52427495 0.69402385 0.87958807 1.049394 1.1552547 1.161054 1.0740079 0.91854995 0.72788429 0.51548445 0.31953582 0.17511189 0.09437754][0.33071497 0.43341732 0.58996242 0.78726912 0.98663974 1.1637628 1.2713822 1.2689303 1.1656834 0.99830794 0.80594242 0.59745675 0.40368208 0.25801888 0.17270586][0.25661886 0.37491 0.544788 0.73981154 0.9220112 1.0775077 1.1681014 1.1555349 1.0507234 0.89868194 0.74005288 0.57859731 0.43260843 0.3275677 0.26879993][0.14904258 0.261176 0.41677675 0.58265483 0.72571909 0.84110355 0.90329015 0.88070166 0.78484344 0.6637221 0.55753493 0.46840563 0.40097669 0.3672553 0.36046314][0.06327831 0.15177442 0.27347508 0.39732864 0.49793676 0.57462263 0.611972 0.58452553 0.50261432 0.41239941 0.35377586 0.33195612 0.34180915 0.38124606 0.43018189][0.040305469 0.098431863 0.1794997 0.26098597 0.32562375 0.37233576 0.39153928 0.36193958 0.2918078 0.22245762 0.19390269 0.21720448 0.28253 0.37610543 0.46736476][0.076414734 0.10808518 0.15429746 0.20231284 0.24142812 0.26841339 0.27603239 0.24547459 0.18274635 0.1229946 0.1054932 0.14711709 0.23845918 0.35916853 0.47323221][0.13119078 0.14741957 0.17279771 0.20217282 0.22887003 0.24817225 0.25308281 0.22611396 0.17135125 0.1164791 0.096976362 0.13449414 0.22470176 0.34702206 0.46463484][0.16248582 0.17019784 0.18544169 0.20741636 0.23197056 0.25408304 0.26558733 0.24981532 0.20961647 0.16408183 0.14104092 0.16519642 0.23978522 0.34903029 0.45784816]]...]
INFO - root - 2017-12-10 15:54:36.523286: step 29610, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 66h:10m:56s remains)
INFO - root - 2017-12-10 15:54:44.440283: step 29620, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.819 sec/batch; 68h:53m:04s remains)
INFO - root - 2017-12-10 15:54:52.298811: step 29630, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 66h:19m:59s remains)
INFO - root - 2017-12-10 15:55:00.135712: step 29640, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 64h:34m:39s remains)
INFO - root - 2017-12-10 15:55:07.999322: step 29650, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 65h:36m:11s remains)
INFO - root - 2017-12-10 15:55:15.889068: step 29660, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 66h:37m:42s remains)
INFO - root - 2017-12-10 15:55:23.817129: step 29670, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.820 sec/batch; 68h:57m:56s remains)
INFO - root - 2017-12-10 15:55:31.568063: step 29680, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 66h:02m:56s remains)
INFO - root - 2017-12-10 15:55:39.363600: step 29690, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.746 sec/batch; 62h:43m:23s remains)
INFO - root - 2017-12-10 15:55:47.136853: step 29700, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 64h:07m:11s remains)
2017-12-10 15:55:47.943427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.055746693 -0.050462108 -0.034043934 -0.0048191338 0.035945598 0.081943557 0.12205492 0.14845882 0.16712581 0.18957725 0.21900935 0.25013205 0.269288 0.26997012 0.25725216][-0.057943184 -0.051266823 -0.031862702 0.00265049 0.051773477 0.10933936 0.16176572 0.19725481 0.22052352 0.24517523 0.27570456 0.30645451 0.31978786 0.30797061 0.27836987][-0.057414059 -0.049599376 -0.028338365 0.0085433125 0.06120554 0.1244939 0.18390267 0.22491242 0.25065848 0.27542806 0.30468184 0.33234736 0.33853036 0.31428364 0.26902163][-0.056710396 -0.047374588 -0.024024598 0.014999733 0.069830909 0.13619211 0.19937998 0.24320997 0.26905513 0.28965497 0.31233042 0.332743 0.33149263 0.29882842 0.24374887][-0.055939965 -0.045553129 -0.020269116 0.020864075 0.077908784 0.14661077 0.21245645 0.25793752 0.28224972 0.295298 0.30687243 0.31682685 0.30907059 0.273208 0.21549717][-0.0551316 -0.044912983 -0.019343259 0.022625929 0.080821283 0.15041286 0.21712564 0.26241514 0.28321871 0.28692555 0.28666526 0.28780639 0.27809006 0.24564002 0.19299088][-0.054090593 -0.045088459 -0.021102369 0.019476678 0.076347329 0.14396511 0.20738289 0.24770306 0.26075667 0.2535446 0.24337606 0.24116139 0.23794013 0.21891961 0.18108197][-0.053025741 -0.045424838 -0.023173802 0.015994271 0.071765259 0.13777256 0.19706507 0.22962667 0.23045248 0.20857745 0.18706217 0.18302684 0.19124523 0.19346921 0.17981096][-0.052717075 -0.046238024 -0.024618143 0.014772074 0.071634874 0.13883156 0.19649464 0.22248106 0.21058069 0.1728936 0.13739191 0.12684016 0.14193638 0.16424482 0.17761992][-0.05322665 -0.047766406 -0.025912436 0.014490502 0.073101625 0.14203352 0.19951725 0.22166553 0.20103349 0.15215479 0.10489318 0.085082293 0.099423148 0.13338435 0.16800597][-0.054203354 -0.049574845 -0.026955694 0.014893475 0.075387515 0.14562061 0.2033235 0.22356755 0.19841163 0.14438771 0.091066621 0.064340048 0.074797153 0.11321719 0.16026352][-0.055356067 -0.050953325 -0.027271722 0.016324891 0.078902379 0.15043327 0.20880295 0.22817643 0.20073926 0.14477445 0.089447677 0.059981752 0.068531089 0.10974531 0.16404927][-0.0565191 -0.051757742 -0.027278474 0.017441422 0.081335463 0.15414879 0.21458577 0.23565018 0.20922334 0.1537303 0.097977027 0.066727236 0.0731492 0.11518229 0.17473097][-0.057498224 -0.052357357 -0.028331311 0.014924821 0.076519124 0.14731127 0.20803094 0.23166539 0.20947646 0.15799025 0.10453086 0.072950132 0.076908343 0.11691637 0.1786707][-0.057865854 -0.052581057 -0.030131251 0.0090070842 0.064157136 0.12782042 0.18392164 0.20784508 0.19116129 0.14730218 0.10040402 0.072400115 0.076103769 0.11282763 0.17271623]]...]
INFO - root - 2017-12-10 15:55:55.751985: step 29710, loss = 0.70, batch loss = 0.64 (9.0 examples/sec; 0.889 sec/batch; 74h:43m:50s remains)
INFO - root - 2017-12-10 15:56:03.633140: step 29720, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 65h:37m:16s remains)
INFO - root - 2017-12-10 15:56:11.613775: step 29730, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 66h:45m:05s remains)
INFO - root - 2017-12-10 15:56:19.501896: step 29740, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 67h:53m:18s remains)
INFO - root - 2017-12-10 15:56:27.356888: step 29750, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 67h:31m:39s remains)
INFO - root - 2017-12-10 15:56:35.475455: step 29760, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 65h:57m:11s remains)
INFO - root - 2017-12-10 15:56:43.250610: step 29770, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.766 sec/batch; 64h:24m:18s remains)
INFO - root - 2017-12-10 15:56:51.060448: step 29780, loss = 0.69, batch loss = 0.63 (10.8 examples/sec; 0.742 sec/batch; 62h:23m:45s remains)
INFO - root - 2017-12-10 15:56:58.668804: step 29790, loss = 0.70, batch loss = 0.64 (13.7 examples/sec; 0.586 sec/batch; 49h:14m:54s remains)
INFO - root - 2017-12-10 15:57:06.601331: step 29800, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 67h:45m:56s remains)
2017-12-10 15:57:07.478668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.074310474 -0.069746651 -0.062257174 -0.054327 -0.046278767 -0.039275311 -0.034600437 -0.032342479 -0.033292379 -0.03668046 -0.041003607 -0.044959031 -0.046806946 -0.046451546 -0.044473492][-0.080110572 -0.0758739 -0.06884253 -0.062224135 -0.055749636 -0.048604947 -0.040583454 -0.032264039 -0.026276391 -0.024206346 -0.025934521 -0.030967265 -0.036989514 -0.041713838 -0.043629356][-0.081351645 -0.077292286 -0.06862238 -0.059546992 -0.049871422 -0.037747651 -0.022356462 -0.0054527773 0.0083307838 0.014842323 0.012972034 0.0027831804 -0.012389719 -0.027188132 -0.037152853][-0.075007707 -0.069959156 -0.055474386 -0.036543198 -0.014121995 0.012478884 0.042338822 0.070976064 0.090899006 0.095764466 0.084521137 0.059056092 0.025644459 -0.0061797048 -0.02863078][-0.061539922 -0.0535317 -0.028332029 0.0092881182 0.056847267 0.11091088 0.16455896 0.20748243 0.22806391 0.21914694 0.18319276 0.12846525 0.0673686 0.013898862 -0.022080194][-0.040895853 -0.025674814 0.016162163 0.080667347 0.16331013 0.25351888 0.33398587 0.38616326 0.39484805 0.35599658 0.28030574 0.1865837 0.094985433 0.022112764 -0.023000782][-0.01258905 0.013900681 0.074948609 0.16709425 0.28310713 0.4042519 0.50219083 0.551227 0.53557837 0.45843947 0.34076214 0.21186018 0.097880557 0.015086022 -0.030999819][0.012421311 0.050487336 0.1265008 0.23584926 0.36867756 0.50091809 0.5981316 0.63214588 0.5900988 0.48315549 0.33965951 0.19429977 0.075015366 -0.0046404041 -0.043577936][0.018997407 0.064681657 0.14493266 0.25285867 0.37799677 0.49687743 0.57685429 0.59291458 0.53661495 0.42338037 0.28211659 0.145673 0.039259288 -0.026966462 -0.054858837][0.0086080786 0.055897344 0.12766123 0.21513891 0.31070778 0.39789665 0.452634 0.45649385 0.40406793 0.30945644 0.19557971 0.087990522 0.0064186975 -0.041854639 -0.059276111][-0.0046845246 0.038675759 0.09310317 0.14968815 0.20608333 0.25568676 0.28563511 0.28448266 0.24754034 0.18392594 0.10789347 0.036068525 -0.017910633 -0.048953008 -0.058742817][-0.013022688 0.022363432 0.056759059 0.082852341 0.10369044 0.12126559 0.13216828 0.13026559 0.11032554 0.075734 0.033191454 -0.00799677 -0.038933009 -0.056198962 -0.060731374][-0.016698036 0.0085509159 0.025129687 0.0280259 0.024191204 0.020751275 0.019865749 0.018622627 0.011066628 -0.003779087 -0.023925325 -0.044352245 -0.058817614 -0.065169469 -0.064112239][-0.012433965 0.0042915051 0.0087395385 -0.0021407472 -0.019607678 -0.034443066 -0.041660525 -0.041986436 -0.041838907 -0.045217358 -0.052734829 -0.061391219 -0.06575501 -0.063837029 -0.056140225][0.0050955717 0.019476501 0.020461591 0.0053152475 -0.017719995 -0.03877065 -0.050115716 -0.049633797 -0.043827407 -0.039628465 -0.040005542 -0.043322105 -0.043807592 -0.037729692 -0.024171727]]...]
INFO - root - 2017-12-10 15:57:15.267348: step 29810, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 64h:05m:08s remains)
INFO - root - 2017-12-10 15:57:23.017046: step 29820, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 67h:16m:29s remains)
INFO - root - 2017-12-10 15:57:30.872135: step 29830, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 67h:05m:18s remains)
INFO - root - 2017-12-10 15:57:38.603342: step 29840, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 65h:33m:17s remains)
INFO - root - 2017-12-10 15:57:46.498453: step 29850, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 66h:06m:23s remains)
INFO - root - 2017-12-10 15:57:54.353705: step 29860, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 65h:26m:04s remains)
INFO - root - 2017-12-10 15:58:01.889264: step 29870, loss = 0.70, batch loss = 0.64 (16.6 examples/sec; 0.482 sec/batch; 40h:30m:20s remains)
INFO - root - 2017-12-10 15:58:09.708466: step 29880, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.818 sec/batch; 68h:47m:18s remains)
INFO - root - 2017-12-10 15:58:17.558850: step 29890, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 67h:12m:11s remains)
INFO - root - 2017-12-10 15:58:25.412756: step 29900, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 67h:40m:00s remains)
2017-12-10 15:58:26.276858: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20944181 0.20838133 0.19691361 0.19280756 0.19297443 0.19326097 0.19998369 0.22966921 0.27998376 0.33831185 0.38923919 0.415243 0.41524008 0.4018 0.38703793][0.28697369 0.26925862 0.23427476 0.20637918 0.1860193 0.17301157 0.17416303 0.20477812 0.26394996 0.33902439 0.41257498 0.46039623 0.47486061 0.46663034 0.45255741][0.38566384 0.36206463 0.3113099 0.26540282 0.22764434 0.19979843 0.18633817 0.19965757 0.24517874 0.31395894 0.39147773 0.45143914 0.48130444 0.48583639 0.48031655][0.48585507 0.46736726 0.41180032 0.35809335 0.31335756 0.27883813 0.25432312 0.24873692 0.2730777 0.32057557 0.3803288 0.43184835 0.46399027 0.47619814 0.47805259][0.56075662 0.5677349 0.5244059 0.47674406 0.43736282 0.40682217 0.38188872 0.36645326 0.37256321 0.39249757 0.41683352 0.4347004 0.44365656 0.44553193 0.44601265][0.58899683 0.63430756 0.61857426 0.58916616 0.56339848 0.54422659 0.5280863 0.51306838 0.50595188 0.49777928 0.47905928 0.44740137 0.41172227 0.38373947 0.36983925][0.56515795 0.64974284 0.67000639 0.66743386 0.65984559 0.6548124 0.64918268 0.63557523 0.61422205 0.57824427 0.51910043 0.4377602 0.35243067 0.2877205 0.25423098][0.4877255 0.59642279 0.6465348 0.66702151 0.67129642 0.67171967 0.66825396 0.65200049 0.61931539 0.56720245 0.48716247 0.37711489 0.25899532 0.16776372 0.11819974][0.36245707 0.46807811 0.52847427 0.55766571 0.56337529 0.56128675 0.55649024 0.54051971 0.5075022 0.45787805 0.38175473 0.27111089 0.14714557 0.050024081 -0.0036269228][0.22181508 0.30228028 0.3543635 0.37959778 0.38085771 0.37443888 0.36938325 0.35691264 0.33000007 0.29232186 0.23457845 0.14420193 0.039474886 -0.040831085 -0.082086951][0.10167764 0.14680286 0.17857265 0.1921282 0.18710658 0.17753547 0.17331296 0.16548325 0.14701506 0.12408226 0.090349354 0.03211794 -0.0374766 -0.086789124 -0.10598978][0.029894924 0.043343063 0.053318109 0.05498457 0.045973491 0.036525156 0.034626048 0.031838622 0.022082478 0.013160689 0.0024104582 -0.023111405 -0.056828808 -0.0761307 -0.075801834][0.013486445 0.0070928968 0.0024274441 -0.0026197296 -0.011849242 -0.018758934 -0.017347025 -0.014607661 -0.016021905 -0.014421257 -0.010189646 -0.013993113 -0.024359267 -0.025885064 -0.016252166][0.015884524 0.00092887069 -0.0094443774 -0.015089841 -0.020450147 -0.022277165 -0.016687538 -0.0095125111 -0.0057143033 0.00030955698 0.0096438117 0.014737231 0.01455895 0.019569781 0.030763326][0.0045728 -0.010923097 -0.021148253 -0.024842942 -0.025954993 -0.023452546 -0.015726024 -0.0072653722 -0.0019148403 0.0042089969 0.012724941 0.018867552 0.021051528 0.02622758 0.034907777]]...]
INFO - root - 2017-12-10 15:58:34.170749: step 29910, loss = 0.73, batch loss = 0.67 (10.3 examples/sec; 0.777 sec/batch; 65h:18m:03s remains)
INFO - root - 2017-12-10 15:58:42.003729: step 29920, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 66h:29m:06s remains)
INFO - root - 2017-12-10 15:58:49.974500: step 29930, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.756 sec/batch; 63h:30m:27s remains)
INFO - root - 2017-12-10 15:58:57.768746: step 29940, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 66h:28m:52s remains)
INFO - root - 2017-12-10 15:59:05.451124: step 29950, loss = 0.71, batch loss = 0.65 (12.9 examples/sec; 0.618 sec/batch; 51h:55m:48s remains)
INFO - root - 2017-12-10 15:59:13.185975: step 29960, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 66h:57m:48s remains)
INFO - root - 2017-12-10 15:59:21.001305: step 29970, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 65h:34m:49s remains)
INFO - root - 2017-12-10 15:59:28.749629: step 29980, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 64h:49m:31s remains)
INFO - root - 2017-12-10 15:59:36.563812: step 29990, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 63h:59m:14s remains)
INFO - root - 2017-12-10 15:59:44.522992: step 30000, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 66h:44m:34s remains)
2017-12-10 15:59:45.417033: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32787511 0.38042092 0.41335556 0.4220016 0.42006594 0.41230693 0.40248334 0.39806271 0.39357698 0.39962515 0.40667444 0.40588912 0.39374244 0.36976913 0.326774][0.32291737 0.37713125 0.41203782 0.42001718 0.41292298 0.399025 0.3845987 0.374179 0.36169648 0.35738996 0.3559764 0.35029435 0.33622646 0.31355166 0.2756373][0.2790851 0.3260673 0.35769534 0.36232784 0.35100636 0.33579496 0.32307521 0.3118985 0.29425338 0.28178284 0.27482367 0.26805931 0.2549867 0.23567477 0.20567274][0.22940822 0.26320845 0.28752661 0.28848112 0.27761897 0.27081305 0.27120706 0.26964131 0.25508118 0.24101463 0.23400833 0.22847298 0.21439457 0.19454534 0.16846414][0.191748 0.21100986 0.22761093 0.22865154 0.22638816 0.23930646 0.26357082 0.28030971 0.275356 0.2640157 0.25794014 0.25023732 0.22948448 0.20343918 0.17585702][0.16849922 0.17614034 0.18780226 0.19407594 0.20788862 0.24731906 0.29881948 0.33510885 0.3406072 0.3317281 0.32289919 0.306773 0.27438983 0.23867691 0.20646888][0.15445526 0.15822333 0.17063898 0.18831366 0.22318599 0.28862202 0.3600361 0.40694442 0.41619924 0.40558833 0.38981929 0.36188963 0.31812239 0.27493998 0.23961671][0.16297816 0.17020966 0.18726645 0.21803215 0.27201292 0.35461417 0.43135774 0.47432944 0.47696832 0.45951298 0.43521011 0.39750728 0.34870684 0.30497879 0.27081811][0.19232124 0.21075357 0.23728724 0.28074592 0.34658355 0.43159997 0.49672219 0.52149737 0.50803989 0.47942564 0.44712827 0.4053908 0.36009634 0.32331431 0.29453716][0.23878515 0.27521434 0.31554091 0.36854166 0.43506965 0.50556087 0.54241812 0.53678668 0.50023663 0.45849237 0.42059779 0.38194844 0.348466 0.32501104 0.30496362][0.29127327 0.34711489 0.40094209 0.45735121 0.51272005 0.55524939 0.55443144 0.51431757 0.45374 0.40043718 0.36101153 0.33142269 0.31401587 0.30582717 0.2957328][0.32416758 0.39345124 0.45547983 0.50871211 0.54604387 0.55748332 0.52308196 0.45692316 0.38040954 0.32195759 0.2863678 0.26831102 0.26499358 0.26795331 0.26487038][0.30989695 0.3831476 0.4467912 0.49256951 0.51158416 0.49830797 0.44448826 0.36891109 0.290641 0.23631029 0.20940885 0.20256254 0.20735401 0.21299808 0.21007453][0.23671113 0.30416778 0.36291274 0.39882752 0.40341365 0.37646946 0.31968784 0.25167429 0.18558513 0.14355206 0.1277881 0.12955463 0.13659982 0.13815004 0.12999377][0.13312168 0.18562591 0.23257022 0.25601345 0.25021607 0.21991701 0.17212179 0.12211738 0.076500319 0.051018249 0.045869749 0.0520954 0.057334717 0.052761964 0.039024666]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 15:59:54.060238: step 30010, loss = 0.67, batch loss = 0.61 (10.6 examples/sec; 0.757 sec/batch; 63h:35m:17s remains)
INFO - root - 2017-12-10 16:00:01.924909: step 30020, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 66h:27m:27s remains)
INFO - root - 2017-12-10 16:00:09.635975: step 30030, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 65h:33m:02s remains)
INFO - root - 2017-12-10 16:00:17.444208: step 30040, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 64h:25m:26s remains)
INFO - root - 2017-12-10 16:00:25.169475: step 30050, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 65h:34m:30s remains)
INFO - root - 2017-12-10 16:00:32.929626: step 30060, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.759 sec/batch; 63h:45m:57s remains)
INFO - root - 2017-12-10 16:00:40.763094: step 30070, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 66h:42m:06s remains)
INFO - root - 2017-12-10 16:00:48.565570: step 30080, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 64h:35m:50s remains)
INFO - root - 2017-12-10 16:00:56.375082: step 30090, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.773 sec/batch; 64h:57m:14s remains)
INFO - root - 2017-12-10 16:01:04.269380: step 30100, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 65h:48m:50s remains)
2017-12-10 16:01:05.180760: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14465955 0.19281511 0.21754892 0.21539813 0.19225843 0.15085813 0.10053439 0.056972444 0.03396602 0.033851765 0.051976621 0.0802395 0.11034579 0.13546 0.15188596][0.17519468 0.23069769 0.25555316 0.24813582 0.21746837 0.16741337 0.10991149 0.059229665 0.028914968 0.022261364 0.034365766 0.059635211 0.091925524 0.12708052 0.15762907][0.19987494 0.26159394 0.28805175 0.28065535 0.25022253 0.20040821 0.14366545 0.0913822 0.054161377 0.034894459 0.032089405 0.045783039 0.073709153 0.11571348 0.16136366][0.21104771 0.27833277 0.3104232 0.3114529 0.29188222 0.2535536 0.20791118 0.16192655 0.12195147 0.090442024 0.069799013 0.06633871 0.082243629 0.12228642 0.17620791][0.21185063 0.28100616 0.31842873 0.33040389 0.3256337 0.30465826 0.27729943 0.24544035 0.20953721 0.17095286 0.13639876 0.11729021 0.11919452 0.15173548 0.20799357][0.20650269 0.2732271 0.31213439 0.33126926 0.33741018 0.33197165 0.32399356 0.30976057 0.28295615 0.24343663 0.20264006 0.17467932 0.16633084 0.19120854 0.24877757][0.18929847 0.250839 0.28771189 0.30886313 0.32005408 0.32474837 0.3315843 0.33280531 0.31790745 0.28442463 0.24700382 0.2180647 0.20406483 0.22281468 0.28137907][0.15980718 0.2152006 0.24979906 0.2714535 0.28498763 0.29455781 0.30845743 0.31738997 0.31072 0.28536892 0.25630856 0.23089476 0.21412729 0.22721365 0.28426868][0.12767437 0.17694964 0.2105848 0.23395897 0.25002795 0.26118341 0.27447274 0.28151748 0.27491474 0.25343862 0.23052229 0.20831493 0.19022198 0.19872923 0.25143841][0.11211489 0.16094999 0.19922419 0.22847715 0.24830866 0.25736502 0.26067159 0.25329691 0.23395585 0.20487843 0.17876989 0.15580618 0.13853565 0.14659342 0.19514191][0.11620215 0.17103581 0.21873842 0.25612143 0.27863893 0.2812863 0.26827624 0.23913662 0.19959509 0.15568857 0.12101462 0.096905671 0.084534124 0.096670352 0.14085841][0.13128702 0.19453475 0.25129658 0.29356912 0.31434265 0.30726698 0.27817073 0.23016536 0.1740595 0.1184122 0.078264378 0.057326388 0.053650178 0.070955083 0.1071746][0.14952165 0.22063005 0.28426611 0.32806271 0.34444031 0.32898247 0.29048634 0.23256929 0.16814949 0.10740929 0.067134082 0.05239068 0.0561171 0.073967107 0.096464671][0.16177464 0.23600718 0.30145684 0.343631 0.35579655 0.33712775 0.29816985 0.24041633 0.17650372 0.11761005 0.082583152 0.076203451 0.085879527 0.10159254 0.10976916][0.15884435 0.22938582 0.29045483 0.32799 0.33695662 0.31930074 0.28537679 0.23397698 0.17707391 0.12619667 0.10114841 0.10502911 0.12069164 0.13559197 0.13513236]]...]
INFO - root - 2017-12-10 16:01:12.920597: step 30110, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 66h:27m:07s remains)
INFO - root - 2017-12-10 16:01:20.788313: step 30120, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 64h:32m:35s remains)
INFO - root - 2017-12-10 16:01:28.773371: step 30130, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 67h:22m:11s remains)
INFO - root - 2017-12-10 16:01:36.347479: step 30140, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 65h:09m:47s remains)
INFO - root - 2017-12-10 16:01:44.238642: step 30150, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.824 sec/batch; 69h:10m:22s remains)
INFO - root - 2017-12-10 16:01:52.064816: step 30160, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 64h:17m:39s remains)
INFO - root - 2017-12-10 16:01:59.926317: step 30170, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.759 sec/batch; 63h:43m:22s remains)
INFO - root - 2017-12-10 16:02:07.790330: step 30180, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 66h:23m:35s remains)
INFO - root - 2017-12-10 16:02:15.417279: step 30190, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 65h:52m:57s remains)
INFO - root - 2017-12-10 16:02:23.238841: step 30200, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 66h:06m:14s remains)
2017-12-10 16:02:24.177353: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.053000133 0.063476428 0.066201575 0.0650251 0.061388712 0.056908846 0.053431854 0.051109727 0.048241593 0.043136243 0.037346512 0.032251384 0.02929244 0.028600991 0.029648639][0.073458053 0.081603155 0.081734411 0.079039775 0.07539399 0.072268128 0.070437551 0.069013342 0.065767609 0.058776475 0.049130738 0.038076174 0.028311063 0.021990558 0.019470597][0.085166492 0.091834873 0.092055395 0.091814704 0.093324088 0.097663164 0.10331078 0.10715966 0.10624821 0.098241016 0.083238713 0.061340321 0.037086528 0.01747651 0.0061522485][0.087319762 0.097288042 0.10395171 0.11283945 0.12703708 0.14712366 0.16794626 0.18217215 0.18526393 0.17485078 0.1508278 0.11184172 0.064600579 0.023243917 -0.0027876513][0.084608 0.10398179 0.1237123 0.14754879 0.18001109 0.22160879 0.26296723 0.29038087 0.29638854 0.27931234 0.24125725 0.18116049 0.10836737 0.043807991 0.0027360765][0.084301084 0.1178444 0.1549672 0.19637446 0.24745423 0.3098323 0.37079188 0.40950125 0.41482818 0.38628528 0.33076459 0.25049391 0.15804201 0.078527488 0.030257333][0.09262006 0.14283493 0.19881591 0.25652516 0.32097653 0.39533949 0.4655771 0.5064953 0.5050649 0.46241117 0.39115849 0.29847059 0.19944349 0.11975189 0.076100476][0.10763904 0.17343687 0.24525794 0.31325069 0.38041878 0.45046192 0.51055795 0.5383113 0.52367461 0.46952146 0.39236605 0.30267835 0.2155637 0.15311536 0.12639441][0.1182955 0.19331867 0.27260897 0.34153661 0.4007591 0.45270491 0.48718292 0.49127844 0.46289322 0.40666384 0.33829555 0.26772022 0.20781823 0.17437086 0.17114012][0.11646324 0.19097325 0.26689345 0.32787415 0.37335756 0.40352166 0.41116208 0.39487728 0.36136585 0.31607139 0.2689485 0.22584178 0.1968555 0.19139351 0.20692986][0.10540108 0.17313415 0.23946491 0.28940472 0.32285514 0.33827314 0.33066311 0.30713192 0.28016311 0.25406992 0.23057026 0.21089622 0.20379081 0.21388505 0.23509704][0.094683982 0.15565675 0.2131789 0.25481251 0.28211796 0.29368252 0.28548041 0.26812136 0.25554919 0.24862216 0.24116319 0.23189838 0.23071906 0.2405441 0.25472671][0.091798827 0.14836854 0.199637 0.23619288 0.26157916 0.27652547 0.27742317 0.27448183 0.27949172 0.28727871 0.2853618 0.27290413 0.26347587 0.26159361 0.26231238][0.099575624 0.15309934 0.19888502 0.23062651 0.25416625 0.27348605 0.28591561 0.30001745 0.3232823 0.34271994 0.34017435 0.31655684 0.29077515 0.27269322 0.26013887][0.11746103 0.1672281 0.20536286 0.22906229 0.24702087 0.26629108 0.28606591 0.3137427 0.35211492 0.3810387 0.37705413 0.34234422 0.30099136 0.27068806 0.25243762]]...]
INFO - root - 2017-12-10 16:02:32.097031: step 30210, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 66h:40m:13s remains)
INFO - root - 2017-12-10 16:02:39.896480: step 30220, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 65h:21m:12s remains)
INFO - root - 2017-12-10 16:02:47.479193: step 30230, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 65h:05m:21s remains)
INFO - root - 2017-12-10 16:02:55.301060: step 30240, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.771 sec/batch; 64h:45m:35s remains)
INFO - root - 2017-12-10 16:03:03.180174: step 30250, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.785 sec/batch; 65h:53m:16s remains)
INFO - root - 2017-12-10 16:03:11.026153: step 30260, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 66h:26m:28s remains)
INFO - root - 2017-12-10 16:03:18.707104: step 30270, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 65h:07m:46s remains)
INFO - root - 2017-12-10 16:03:26.626360: step 30280, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 66h:58m:45s remains)
INFO - root - 2017-12-10 16:03:34.492927: step 30290, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 67h:25m:54s remains)
INFO - root - 2017-12-10 16:03:42.310690: step 30300, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 66h:39m:03s remains)
2017-12-10 16:03:43.116157: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35910267 0.39008093 0.40183583 0.38099062 0.32070997 0.22637263 0.12355151 0.035722453 -0.023949055 -0.049920771 -0.039673433 -0.00038317111 0.051049028 0.090714574 0.10462593][0.41997263 0.44997412 0.45131397 0.41803822 0.34741202 0.2462859 0.13863052 0.046990953 -0.01590828 -0.046599574 -0.040309738 -0.0014219513 0.05468208 0.10106293 0.11997948][0.44747481 0.47842982 0.47272789 0.43537652 0.36915103 0.27971303 0.18447062 0.10050903 0.038090397 -0.0013375779 -0.0093603292 0.015703354 0.063183948 0.1054045 0.12334034][0.45072907 0.48883846 0.4860898 0.46061307 0.41916436 0.36176205 0.29390904 0.22436796 0.16043906 0.10262368 0.065576933 0.060539272 0.084683813 0.11137922 0.12083175][0.44345289 0.49521086 0.50649756 0.50807834 0.50771135 0.49573463 0.46251246 0.40734243 0.33586079 0.25057694 0.17421168 0.12966461 0.12295678 0.12831239 0.12467047][0.4469853 0.516277 0.55125922 0.58998549 0.6371184 0.67140985 0.6685124 0.61962539 0.53115833 0.41151908 0.2925275 0.20885111 0.17302856 0.158472 0.14191948][0.46983707 0.55446756 0.61411893 0.68766159 0.77402252 0.84148419 0.85503447 0.8011955 0.68889385 0.53524965 0.38144413 0.27075088 0.21824358 0.19389366 0.17054503][0.49961355 0.58999419 0.66376346 0.75588733 0.86048436 0.94060421 0.95522588 0.88997513 0.75695646 0.5818302 0.41326869 0.29745957 0.24740003 0.22777319 0.2068119][0.52086955 0.60394561 0.67454791 0.76234037 0.85987043 0.93145621 0.93551958 0.85994428 0.71905774 0.54387981 0.38616973 0.28835842 0.25731209 0.25263828 0.23969875][0.51479536 0.57940388 0.63195938 0.69628334 0.76666719 0.81473756 0.8035323 0.72442925 0.59156722 0.43838412 0.31418717 0.25151646 0.24842703 0.26168573 0.25689077][0.45524171 0.49593785 0.5240581 0.55777264 0.59500176 0.61792934 0.5972743 0.52699137 0.41842642 0.30344704 0.22399232 0.20010892 0.22174327 0.24729906 0.24607231][0.32762972 0.34441721 0.34980744 0.35715476 0.36850756 0.37567052 0.35738835 0.30878174 0.23673841 0.1675539 0.13242002 0.13997023 0.17564641 0.20383403 0.20092897][0.15815091 0.15710334 0.14844725 0.14125846 0.14005375 0.14306806 0.13586454 0.11390375 0.079800084 0.052348778 0.051394925 0.077291191 0.11491044 0.13776374 0.13089971][-0.0026273481 -0.012344598 -0.024206433 -0.03373044 -0.036322664 -0.030884748 -0.026299646 -0.026598534 -0.031933658 -0.030871313 -0.013289968 0.016799573 0.04691809 0.060708735 0.050974425][-0.11660308 -0.12803924 -0.13645677 -0.14201544 -0.14124671 -0.13234067 -0.1203353 -0.10874543 -0.098840795 -0.084796406 -0.062951334 -0.038089573 -0.018480262 -0.012117646 -0.021093972]]...]
INFO - root - 2017-12-10 16:03:50.792758: step 30310, loss = 0.69, batch loss = 0.63 (12.8 examples/sec; 0.627 sec/batch; 52h:36m:18s remains)
INFO - root - 2017-12-10 16:03:58.561914: step 30320, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 65h:33m:57s remains)
INFO - root - 2017-12-10 16:04:06.420860: step 30330, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 67h:35m:15s remains)
INFO - root - 2017-12-10 16:04:14.278687: step 30340, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 66h:02m:41s remains)
INFO - root - 2017-12-10 16:04:21.896972: step 30350, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 66h:29m:04s remains)
INFO - root - 2017-12-10 16:04:29.810252: step 30360, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 65h:01m:22s remains)
INFO - root - 2017-12-10 16:04:37.704421: step 30370, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 66h:36m:14s remains)
INFO - root - 2017-12-10 16:04:45.551156: step 30380, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 66h:35m:51s remains)
INFO - root - 2017-12-10 16:04:53.350843: step 30390, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 67h:10m:23s remains)
INFO - root - 2017-12-10 16:05:01.083148: step 30400, loss = 0.70, batch loss = 0.64 (11.1 examples/sec; 0.720 sec/batch; 60h:26m:51s remains)
2017-12-10 16:05:01.994766: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.023009105 0.016144618 0.0086645409 0.0028547968 -0.0011730455 -0.0040385029 -0.0067103049 -0.0094443606 -0.011621327 -0.012649157 -0.012275826 -0.010854748 -0.0082403356 -0.0049697077 -0.0021223994][0.013308224 0.0087200738 0.0034036029 -0.00054686773 -0.0031099578 -0.00517371 -0.0076844105 -0.010576222 -0.012800163 -0.013610485 -0.012590616 -0.010108324 -0.006325278 -0.0018993875 0.0020842783][-0.0056628543 -0.0071480316 -0.0083927326 -0.0082119862 -0.0070276465 -0.0065162689 -0.0079391971 -0.011297106 -0.01510529 -0.017880853 -0.018427063 -0.016630612 -0.013011696 -0.0083372882 -0.0037708746][-0.018549228 -0.016479544 -0.012390422 -0.0063157314 0.00058007531 0.005445139 0.0062695486 0.00240284 -0.0047592577 -0.012706477 -0.018590009 -0.020957299 -0.020300165 -0.017562874 -0.014115886][-0.011994761 -0.004990818 0.0063804607 0.020606423 0.035170969 0.045854218 0.049761672 0.045542751 0.03373925 0.017839864 0.0029961488 -0.0073655425 -0.013279095 -0.015742745 -0.016173951][0.014996273 0.029510628 0.050735734 0.075582437 0.099841826 0.11798598 0.12603439 0.12230062 0.10609639 0.081872113 0.057242546 0.037345223 0.022615744 0.011977931 0.0045908396][0.048711989 0.073361546 0.10628133 0.14281815 0.17771785 0.20454413 0.2181146 0.21659113 0.19832762 0.16878255 0.13751715 0.11061538 0.088391595 0.069186024 0.053411376][0.069998354 0.10470872 0.14810573 0.194061 0.23781863 0.27288789 0.292947 0.2957378 0.27935046 0.25017172 0.21851364 0.19068955 0.16599017 0.14145064 0.11896288][0.067395285 0.1075872 0.15620887 0.20576307 0.2530396 0.29249296 0.31743124 0.32520735 0.31450593 0.29214397 0.26757306 0.24658188 0.22659487 0.20288129 0.17833371][0.043128658 0.081144378 0.12736338 0.1735007 0.21756129 0.25522509 0.28037545 0.29072297 0.28653687 0.27450985 0.26183826 0.25318584 0.24381235 0.22735234 0.20682159][0.011620568 0.040404543 0.077231534 0.11419822 0.14964347 0.18031225 0.20095359 0.20994423 0.20960343 0.20655698 0.20556138 0.20981111 0.21274352 0.2069317 0.19446054][-0.010909242 0.0053914664 0.029448554 0.054721508 0.079485096 0.1017185 0.11685764 0.12297669 0.12337246 0.12481063 0.13084035 0.14355217 0.15561126 0.15924229 0.15467951][-0.015600295 -0.010596409 0.0019382668 0.01705003 0.033082739 0.049606957 0.062606171 0.06839031 0.0690436 0.07112854 0.078624442 0.093118116 0.10783486 0.11555283 0.11473136][-0.0050132447 -0.0080111185 -0.0033382876 0.0054536439 0.016978288 0.032483406 0.048252698 0.058166523 0.061432436 0.063496895 0.069479637 0.080711477 0.092092045 0.09787377 0.095728055][0.011767613 0.0036201421 0.0035328676 0.0093257679 0.019814475 0.037404947 0.058473848 0.075043313 0.083431289 0.087149881 0.092005372 0.098837376 0.10456702 0.10539778 0.099275991]]...]
INFO - root - 2017-12-10 16:05:09.804609: step 30410, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 65h:00m:52s remains)
INFO - root - 2017-12-10 16:05:17.714915: step 30420, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 66h:53m:23s remains)
INFO - root - 2017-12-10 16:05:25.331009: step 30430, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 64h:24m:38s remains)
INFO - root - 2017-12-10 16:05:33.068076: step 30440, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 64h:39m:57s remains)
INFO - root - 2017-12-10 16:05:40.922036: step 30450, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.817 sec/batch; 68h:31m:07s remains)
INFO - root - 2017-12-10 16:05:48.713572: step 30460, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 67h:08m:08s remains)
INFO - root - 2017-12-10 16:05:56.582264: step 30470, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 66h:16m:00s remains)
INFO - root - 2017-12-10 16:06:04.369079: step 30480, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.752 sec/batch; 63h:07m:29s remains)
INFO - root - 2017-12-10 16:06:12.016739: step 30490, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 65h:28m:02s remains)
INFO - root - 2017-12-10 16:06:19.806863: step 30500, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.773 sec/batch; 64h:48m:15s remains)
2017-12-10 16:06:20.628647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.049618218 -0.056009624 -0.060943898 -0.064560749 -0.066396683 -0.066782355 -0.066898413 -0.067387849 -0.068243541 -0.06858138 -0.067292206 -0.063905217 -0.058979228 -0.053894725 -0.050141502][-0.035987798 -0.042585947 -0.048143845 -0.0515669 -0.052502051 -0.052477486 -0.053968892 -0.057965115 -0.063598804 -0.068573855 -0.070111 -0.066598058 -0.058443684 -0.047929198 -0.038103014][-0.02065984 -0.022782134 -0.023076504 -0.019543171 -0.012785758 -0.0068914192 -0.0069361385 -0.014675309 -0.028729044 -0.045352869 -0.058795244 -0.06408833 -0.059438266 -0.047222625 -0.03198937][-0.0091092922 -0.0008808506 0.012142024 0.032622531 0.058885112 0.083237246 0.096262127 0.092803918 0.0722542 0.037661292 -0.0016547664 -0.033429086 -0.048655529 -0.046724185 -0.033437546][-0.0033350068 0.018755028 0.051303361 0.098476246 0.15786801 0.21740052 0.26138985 0.2767798 0.25622958 0.19879572 0.11838768 0.039743297 -0.014746064 -0.038191967 -0.038008004][0.0017994386 0.037731614 0.091617487 0.1706067 0.2716409 0.37814906 0.46631166 0.51207197 0.4967283 0.41321361 0.28212804 0.14458473 0.039948344 -0.017632753 -0.038132593][0.0084478 0.054905735 0.12638369 0.23260476 0.36990961 0.51840967 0.64762706 0.7236321 0.71612895 0.61184794 0.43767232 0.24903822 0.099983424 0.0108378 -0.03070347][0.012723085 0.064078078 0.14445998 0.26331431 0.41576803 0.5808512 0.72662193 0.81660587 0.81514049 0.70505583 0.51515174 0.30590102 0.13699855 0.031490646 -0.023100633][0.0063288426 0.053660776 0.12905452 0.23903385 0.37771451 0.52587736 0.65619123 0.73839229 0.73992872 0.64307624 0.47289279 0.28344482 0.1284446 0.029219881 -0.024397751][-0.014590219 0.018750397 0.074313082 0.15528432 0.25657326 0.36359969 0.45726034 0.51758379 0.5202477 0.4496417 0.324024 0.18393043 0.0696591 -0.0027009279 -0.04036754][-0.041986071 -0.027135426 0.0019313622 0.045808505 0.10179491 0.16117625 0.21355025 0.24858236 0.25119549 0.20950326 0.13481897 0.052755281 -0.011643862 -0.048767474 -0.063437968][-0.064182937 -0.065219648 -0.058826122 -0.045671336 -0.026179388 -0.0042879707 0.015615989 0.029728059 0.030620515 0.011087026 -0.022563798 -0.056742724 -0.079286389 -0.086478688 -0.081705488][-0.071055233 -0.0821021 -0.089774996 -0.094637312 -0.095704928 -0.094210677 -0.091889732 -0.089948662 -0.091249816 -0.098755784 -0.1086451 -0.11465909 -0.11276408 -0.1031138 -0.088441782][-0.058684755 -0.074355647 -0.088966481 -0.10184175 -0.1111102 -0.11693137 -0.1204465 -0.12271851 -0.12497035 -0.12778197 -0.1284765 -0.12423971 -0.11449061 -0.10070299 -0.085042588][-0.033806439 -0.050741464 -0.068313353 -0.083978221 -0.095241323 -0.10207473 -0.10594758 -0.10840833 -0.11055794 -0.11221384 -0.11147535 -0.10688923 -0.098639816 -0.087898754 -0.07608936]]...]
INFO - root - 2017-12-10 16:06:28.225527: step 30510, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 67h:35m:08s remains)
INFO - root - 2017-12-10 16:06:36.101618: step 30520, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 64h:10m:00s remains)
INFO - root - 2017-12-10 16:06:44.005663: step 30530, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 67h:15m:40s remains)
INFO - root - 2017-12-10 16:06:51.944664: step 30540, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 65h:15m:41s remains)
INFO - root - 2017-12-10 16:06:59.823900: step 30550, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 67h:45m:35s remains)
INFO - root - 2017-12-10 16:07:07.690257: step 30560, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 64h:49m:57s remains)
INFO - root - 2017-12-10 16:07:15.572082: step 30570, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 65h:46m:46s remains)
INFO - root - 2017-12-10 16:07:23.220902: step 30580, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.777 sec/batch; 65h:07m:52s remains)
INFO - root - 2017-12-10 16:07:30.776463: step 30590, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 65h:10m:37s remains)
INFO - root - 2017-12-10 16:07:38.616000: step 30600, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.787 sec/batch; 65h:58m:46s remains)
2017-12-10 16:07:39.418338: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12143409 0.11443816 0.10221692 0.088601716 0.074425519 0.062547974 0.052596156 0.043202158 0.037744123 0.038148172 0.042235941 0.045394275 0.046442576 0.050559569 0.05741626][0.12588565 0.12837951 0.12728822 0.12484683 0.11875623 0.10984795 0.096613377 0.080158725 0.066458292 0.060520496 0.062537126 0.067860857 0.075311586 0.086974442 0.098183095][0.12484407 0.1477962 0.16920246 0.18804292 0.19819297 0.19788048 0.18332632 0.15750946 0.13029368 0.11265983 0.10951618 0.11645333 0.13085343 0.15141174 0.16690487][0.130942 0.17934407 0.2282951 0.27245474 0.30250069 0.31481084 0.30283958 0.26998907 0.22847962 0.19653359 0.18622638 0.19240113 0.2090171 0.23227718 0.24749616][0.14668284 0.21874161 0.29398054 0.36252698 0.41323745 0.44238487 0.44100744 0.40924904 0.35786638 0.31183445 0.29032594 0.28633273 0.29114142 0.30277222 0.30924025][0.1598274 0.24869311 0.34469223 0.43417645 0.50502753 0.555108 0.57279986 0.55089337 0.49491957 0.43529448 0.39650211 0.37003097 0.34677982 0.3315179 0.32082528][0.14516842 0.23955007 0.34688228 0.45097035 0.53919482 0.61051333 0.65120095 0.64567983 0.59214306 0.52319121 0.46555507 0.41115585 0.35390341 0.30669287 0.277155][0.09575329 0.18379076 0.29202181 0.40254956 0.50271159 0.59080929 0.6517303 0.66379517 0.61827064 0.54618442 0.4738439 0.39599571 0.31146196 0.23983718 0.19782232][0.028847275 0.099391729 0.19565919 0.29998088 0.40044346 0.49333176 0.56286168 0.58607256 0.55102086 0.4851369 0.41117892 0.32700208 0.23524867 0.15752812 0.11443251][-0.029598482 0.015149529 0.087345786 0.17201133 0.25937268 0.34397903 0.41038927 0.43786043 0.41435924 0.36205721 0.29867151 0.22452626 0.14407307 0.076539032 0.041453034][-0.065271281 -0.0467089 -0.0027888147 0.055919658 0.12204354 0.18902595 0.24277006 0.26695883 0.25177449 0.21387872 0.16636854 0.11122338 0.053004 0.0059529156 -0.014920358][-0.072216161 -0.076053046 -0.058789253 -0.026134089 0.016528616 0.062087327 0.098798357 0.11537243 0.10526931 0.080167167 0.049495243 0.016011691 -0.01661752 -0.03939889 -0.0430977][-0.056486368 -0.076018751 -0.079198219 -0.066972807 -0.04332104 -0.015912827 0.0058080172 0.015045379 0.0086587574 -0.0055082878 -0.02092539 -0.034790851 -0.044793721 -0.046362355 -0.035300273][-0.031091878 -0.058168728 -0.073603526 -0.073857471 -0.061513286 -0.045007814 -0.032051448 -0.026144724 -0.027747465 -0.032300323 -0.035553932 -0.034557763 -0.027828561 -0.014284008 0.0059321579][-0.011430806 -0.039935242 -0.061107852 -0.067695685 -0.060339969 -0.047888931 -0.03660582 -0.027856695 -0.021230031 -0.015716093 -0.0092529943 0.0013469696 0.017322676 0.03794295 0.060636226]]...]
INFO - root - 2017-12-10 16:07:47.327988: step 30610, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 65h:34m:13s remains)
INFO - root - 2017-12-10 16:07:55.229018: step 30620, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 64h:19m:14s remains)
INFO - root - 2017-12-10 16:08:03.113542: step 30630, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 66h:00m:21s remains)
INFO - root - 2017-12-10 16:08:11.020884: step 30640, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 67h:37m:05s remains)
INFO - root - 2017-12-10 16:08:18.844700: step 30650, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 67h:11m:28s remains)
INFO - root - 2017-12-10 16:08:26.621862: step 30660, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 63h:20m:58s remains)
INFO - root - 2017-12-10 16:08:34.008326: step 30670, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 65h:49m:36s remains)
INFO - root - 2017-12-10 16:08:41.795626: step 30680, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 65h:20m:38s remains)
INFO - root - 2017-12-10 16:08:49.586992: step 30690, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.768 sec/batch; 64h:21m:11s remains)
INFO - root - 2017-12-10 16:08:57.339601: step 30700, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 64h:16m:58s remains)
2017-12-10 16:08:58.153596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016037336 0.0057909233 0.0049953838 -0.0024904453 -0.011032244 -0.018240388 -0.021108652 -0.017577536 -0.009052828 0.00015515751 0.0064764977 0.0096914452 0.0093048336 0.0038094795 -0.007337315][0.073171027 0.091578126 0.092210606 0.07969097 0.06285055 0.046304785 0.036698353 0.0387627 0.051295526 0.067303337 0.079989225 0.0876941 0.089363962 0.082402058 0.064180769][0.15395486 0.18286388 0.18354036 0.16583085 0.14197505 0.11871677 0.10606368 0.11044718 0.13015909 0.1545542 0.17415778 0.18589018 0.1871035 0.17574956 0.14755116][0.22797018 0.26531366 0.26491031 0.24234651 0.21355692 0.18785456 0.17722969 0.18790534 0.21742231 0.25242278 0.28092968 0.296611 0.29396579 0.27273983 0.22931522][0.28610975 0.32995689 0.32811946 0.30169353 0.27082068 0.24729018 0.242619 0.2606999 0.29871455 0.34252846 0.37860018 0.39649636 0.38749838 0.35328847 0.29276413][0.32310757 0.37393135 0.37347159 0.34696063 0.31767657 0.29920831 0.3006883 0.32240707 0.36188582 0.40571395 0.44224557 0.45914784 0.44431216 0.4005751 0.32875308][0.33385462 0.39438283 0.4018071 0.38199458 0.35836333 0.3453213 0.34980869 0.36926207 0.40146789 0.4345848 0.46224794 0.47370839 0.4550522 0.40889156 0.33653185][0.32101291 0.39211971 0.4128395 0.40497607 0.38831997 0.37749228 0.37994704 0.39239153 0.41250458 0.43001255 0.44507036 0.4509095 0.43236586 0.38969919 0.32347089][0.29656485 0.37428761 0.40746859 0.41156155 0.4013319 0.39146906 0.39069277 0.39609924 0.40446219 0.40714279 0.41025627 0.41184452 0.39623341 0.35969162 0.30114371][0.25962523 0.33517641 0.37323654 0.3839919 0.37839797 0.37020206 0.36773658 0.36796585 0.36685511 0.3576223 0.35136434 0.35083044 0.34056923 0.31272998 0.26336986][0.20984401 0.27592564 0.31150579 0.32371563 0.32100314 0.31424925 0.3097972 0.30466071 0.29528347 0.27729049 0.26458448 0.26279658 0.25769785 0.23873305 0.19943245][0.14436655 0.1950902 0.2229345 0.23296657 0.23152648 0.2252336 0.217994 0.20768379 0.19240004 0.17059772 0.15590435 0.15424912 0.15333487 0.14227301 0.11370157][0.06710016 0.098634 0.11616357 0.12310731 0.12309663 0.11824497 0.11031152 0.098440558 0.0825667 0.063036181 0.05085117 0.050330896 0.051233195 0.044671908 0.024749195][-0.0045130886 0.0080890581 0.01558578 0.019841468 0.022268379 0.021746157 0.017772526 0.0096859578 -0.0018411713 -0.015823634 -0.024706595 -0.026019664 -0.02666283 -0.032029785 -0.04562727][-0.059000377 -0.059995044 -0.059996162 -0.058536816 -0.055332705 -0.052768808 -0.052869946 -0.056698039 -0.063507512 -0.072202094 -0.078364268 -0.081118107 -0.083790258 -0.088291213 -0.095967144]]...]
INFO - root - 2017-12-10 16:09:06.178751: step 30710, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 65h:21m:20s remains)
INFO - root - 2017-12-10 16:09:14.068885: step 30720, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 66h:48m:41s remains)
INFO - root - 2017-12-10 16:09:21.895231: step 30730, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 65h:38m:36s remains)
INFO - root - 2017-12-10 16:09:29.718093: step 30740, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 65h:11m:21s remains)
INFO - root - 2017-12-10 16:09:37.384780: step 30750, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 65h:28m:44s remains)
INFO - root - 2017-12-10 16:09:45.204585: step 30760, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.788 sec/batch; 66h:05m:12s remains)
INFO - root - 2017-12-10 16:09:53.038403: step 30770, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 65h:17m:00s remains)
INFO - root - 2017-12-10 16:10:00.836933: step 30780, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 64h:38m:33s remains)
INFO - root - 2017-12-10 16:10:08.640040: step 30790, loss = 0.67, batch loss = 0.62 (10.3 examples/sec; 0.773 sec/batch; 64h:47m:57s remains)
INFO - root - 2017-12-10 16:10:16.460009: step 30800, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 68h:08m:54s remains)
2017-12-10 16:10:17.310572: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.016735369 -0.010270663 -0.0048123314 -0.0037651493 -0.0067894785 -0.00965985 -0.00938528 -0.0071430937 -0.0033944778 -0.00098207116 -0.00072729687 -0.0026016403 -0.00981574 -0.020763651 -0.031896479][0.0043051676 0.020336915 0.034678672 0.041017778 0.040351778 0.040077869 0.043520067 0.047746558 0.051242337 0.052199319 0.049828019 0.04301298 0.026618116 0.0040225489 -0.017715104][0.038618822 0.070625037 0.10032132 0.11644839 0.1210523 0.12547992 0.13297552 0.13780098 0.13710631 0.13234337 0.1237561 0.10807688 0.077677377 0.039326992 0.0044356785][0.076932043 0.1293487 0.17939223 0.20911281 0.2216157 0.23255756 0.24467136 0.24836887 0.23919089 0.22373238 0.20460759 0.17618199 0.1281361 0.071212515 0.022544703][0.11159253 0.18706022 0.26113406 0.30771136 0.33059719 0.34943023 0.36576784 0.36533052 0.34260696 0.3111566 0.27735779 0.23314054 0.16615883 0.091573589 0.031500842][0.14132518 0.24002875 0.33788717 0.40100318 0.43373519 0.45854777 0.47524959 0.46601981 0.42566177 0.37580696 0.32643333 0.26721507 0.18491685 0.098012261 0.031881679][0.16017054 0.27690265 0.39272389 0.46759531 0.50617135 0.53216112 0.54383123 0.52209181 0.46376559 0.39746764 0.33618408 0.26786855 0.17928797 0.089911766 0.025138628][0.16444489 0.29072815 0.41494584 0.49422705 0.53323609 0.55518854 0.55853587 0.52517951 0.45482194 0.37934163 0.313081 0.2437069 0.15793049 0.073921107 0.014873032][0.15711598 0.28129166 0.40151042 0.47611734 0.50912583 0.52226555 0.51632315 0.47701448 0.40615785 0.33327213 0.27149746 0.20964631 0.13509974 0.062243663 0.010298692][0.14311258 0.25415832 0.35886082 0.42065811 0.44285777 0.4444263 0.42939615 0.38970271 0.32847598 0.26876795 0.2198039 0.17237392 0.1149136 0.057003055 0.01282721][0.12160508 0.21161221 0.2941184 0.33967593 0.35072047 0.3423692 0.32132825 0.28600824 0.23989034 0.19789994 0.16457719 0.13356541 0.094043568 0.050946545 0.014324433][0.086777091 0.15136693 0.2096969 0.23992451 0.24321201 0.23032847 0.20942442 0.18350303 0.1548728 0.13040125 0.11113938 0.093999594 0.069044754 0.037871484 0.0079557961][0.045888957 0.085605375 0.12141869 0.13875687 0.13785978 0.12540056 0.10957681 0.095343895 0.082940325 0.072831526 0.064016193 0.056301363 0.041110579 0.018387731 -0.0055248034][0.010114062 0.028882559 0.045421712 0.051166505 0.046239946 0.034685288 0.023471849 0.01781347 0.016493207 0.016259927 0.014974494 0.013885441 0.0066750473 -0.0074806875 -0.023439243][-0.019155128 -0.015127579 -0.011991744 -0.01470962 -0.022610227 -0.033033885 -0.041238066 -0.043166582 -0.0399805 -0.035722885 -0.032865293 -0.030107781 -0.031038102 -0.036407862 -0.043069262]]...]
INFO - root - 2017-12-10 16:10:25.112115: step 30810, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 63h:51m:40s remains)
INFO - root - 2017-12-10 16:10:33.075576: step 30820, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 65h:13m:57s remains)
INFO - root - 2017-12-10 16:10:40.703276: step 30830, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 66h:29m:38s remains)
INFO - root - 2017-12-10 16:10:48.457761: step 30840, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 64h:29m:19s remains)
INFO - root - 2017-12-10 16:10:56.192712: step 30850, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.751 sec/batch; 62h:57m:11s remains)
INFO - root - 2017-12-10 16:11:03.963396: step 30860, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 65h:56m:58s remains)
INFO - root - 2017-12-10 16:11:11.796614: step 30870, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 67h:17m:48s remains)
INFO - root - 2017-12-10 16:11:19.677855: step 30880, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.777 sec/batch; 65h:08m:23s remains)
INFO - root - 2017-12-10 16:11:27.522209: step 30890, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 65h:29m:30s remains)
INFO - root - 2017-12-10 16:11:35.389856: step 30900, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 64h:54m:19s remains)
2017-12-10 16:11:36.271229: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0048456574 0.0003379822 0.012413903 0.055346064 0.13532245 0.24448746 0.35506117 0.42875969 0.45194376 0.42785269 0.38682106 0.3680248 0.39445534 0.45746008 0.53567958][-0.021064455 -0.033809967 -0.025619492 0.017682031 0.10520225 0.23314311 0.37269613 0.47662881 0.52784854 0.52771688 0.50605416 0.49768385 0.52307534 0.57400948 0.62848628][-0.017706711 -0.036126178 -0.034895197 -0.001173874 0.077181429 0.20253591 0.35019746 0.46957448 0.54050606 0.563161 0.56168103 0.56235856 0.58038163 0.60995 0.63601649][0.043530643 0.024172487 0.018553391 0.038078409 0.096501686 0.20264801 0.33930749 0.4563401 0.52965194 0.5590443 0.5635637 0.56133169 0.56262624 0.56745136 0.57219738][0.16122892 0.14659843 0.13923755 0.148455 0.18708043 0.26898232 0.38490951 0.48651683 0.54433268 0.55966139 0.54978359 0.527839 0.5023849 0.48244935 0.4757767][0.30197561 0.29520357 0.29252461 0.29982466 0.32651067 0.38892123 0.48434439 0.567812 0.60258436 0.59152 0.55400246 0.50187641 0.4454897 0.40482083 0.39700526][0.41598195 0.41970482 0.4271614 0.43969962 0.46235871 0.51137096 0.59008354 0.65975171 0.67669749 0.64175725 0.576784 0.49516174 0.4115788 0.35635871 0.3536087][0.46124867 0.47645918 0.49455252 0.51305956 0.5341121 0.57257593 0.63631874 0.695117 0.70322806 0.65798616 0.5813688 0.4866221 0.39025721 0.32812232 0.32879767][0.41799989 0.43933174 0.46150461 0.48147818 0.50067627 0.531336 0.58253884 0.63302875 0.64106894 0.60161048 0.53406471 0.44854367 0.3580845 0.29700246 0.2954835][0.29091188 0.31043831 0.32914469 0.34619224 0.36344215 0.3886174 0.42888036 0.47136709 0.48261359 0.45712489 0.40994853 0.34748578 0.27648887 0.22349143 0.21654603][0.1206612 0.1323531 0.14406541 0.15647608 0.17058186 0.18870847 0.21558349 0.24576327 0.25634414 0.24312358 0.21733576 0.18221827 0.13774213 0.0996964 0.091390386][-0.036285844 -0.031890653 -0.024153156 -0.014615436 -0.0042874711 0.0054505053 0.0181073 0.033465691 0.038780093 0.031893037 0.020811344 0.0071267788 -0.013331736 -0.034307692 -0.039572429][-0.14096557 -0.14180918 -0.13621642 -0.12932356 -0.12329444 -0.12026363 -0.1176823 -0.11360989 -0.11343964 -0.11789116 -0.12200981 -0.12474778 -0.13050967 -0.13870944 -0.14051063][-0.18410921 -0.18871614 -0.18549506 -0.18189374 -0.17977689 -0.18038724 -0.18211704 -0.18306686 -0.18479693 -0.18782051 -0.18966863 -0.18964431 -0.19036072 -0.19312391 -0.19407566][-0.17833318 -0.18420337 -0.18287525 -0.18179187 -0.18170433 -0.1830152 -0.18530919 -0.18760455 -0.1897822 -0.19204557 -0.1934817 -0.19397314 -0.19455445 -0.19634886 -0.19782148]]...]
INFO - root - 2017-12-10 16:11:43.915383: step 30910, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 65h:32m:53s remains)
INFO - root - 2017-12-10 16:11:51.722375: step 30920, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.788 sec/batch; 66h:02m:36s remains)
INFO - root - 2017-12-10 16:11:59.548625: step 30930, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 67h:40m:42s remains)
INFO - root - 2017-12-10 16:12:07.310361: step 30940, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 67h:28m:12s remains)
INFO - root - 2017-12-10 16:12:15.158590: step 30950, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.750 sec/batch; 62h:48m:07s remains)
INFO - root - 2017-12-10 16:12:23.046886: step 30960, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 65h:16m:05s remains)
INFO - root - 2017-12-10 16:12:30.832336: step 30970, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.756 sec/batch; 63h:17m:14s remains)
INFO - root - 2017-12-10 16:12:38.718395: step 30980, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 67h:27m:03s remains)
INFO - root - 2017-12-10 16:12:46.474458: step 30990, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 65h:00m:52s remains)
INFO - root - 2017-12-10 16:12:54.370429: step 31000, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 65h:43m:07s remains)
2017-12-10 16:12:55.186746: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.085060917 -0.084996738 -0.083439767 -0.084570482 -0.087376527 -0.090042263 -0.091114864 -0.089838751 -0.086462565 -0.082095467 -0.07804253 -0.07543578 -0.0740811 -0.072794624 -0.070465468][-0.082324527 -0.077973545 -0.073060617 -0.073064178 -0.076504827 -0.08077278 -0.083626442 -0.083391115 -0.079898983 -0.074705 -0.070369452 -0.068977989 -0.069900975 -0.070661351 -0.068966895][-0.042713996 -0.034511011 -0.025013249 -0.020110365 -0.01937988 -0.021270486 -0.024527002 -0.027346577 -0.028997792 -0.030137707 -0.033268277 -0.040147547 -0.049106773 -0.055877149 -0.057040822][0.044477526 0.053723305 0.0692943 0.086460359 0.10106196 0.10931507 0.10767956 0.096852 0.079360679 0.05864545 0.035362005 0.0099521969 -0.014368082 -0.031707287 -0.038171221][0.17075168 0.17565046 0.19751157 0.23521501 0.275412 0.30310488 0.30485144 0.27930373 0.23307812 0.17774558 0.1210707 0.067888752 0.023306089 -0.0062726596 -0.018119717][0.29869404 0.29312941 0.32116637 0.3844758 0.45689404 0.50801438 0.51181877 0.46558774 0.38239992 0.28501669 0.1903201 0.10833819 0.04570074 0.0076809009 -0.0060751881][0.37778914 0.36006111 0.39358497 0.47952512 0.57944357 0.64864373 0.65079266 0.58360183 0.46743521 0.33548185 0.21220727 0.11183552 0.041434642 0.0036600116 -0.0064336704][0.37211248 0.34678516 0.38385284 0.48101783 0.59303212 0.66776603 0.6651122 0.58511531 0.45341736 0.30852798 0.17808238 0.07772892 0.013716584 -0.014811684 -0.017112123][0.28710189 0.26088837 0.29724494 0.3899774 0.49502689 0.56209743 0.55473268 0.47587967 0.35208157 0.22022881 0.10598335 0.02329245 -0.023251725 -0.037626438 -0.0314765][0.15934297 0.13719091 0.16848078 0.24367617 0.32695588 0.37763363 0.36827743 0.30358204 0.20644034 0.10676333 0.024354795 -0.030658925 -0.055734225 -0.056295305 -0.043828335][0.034521997 0.019334763 0.043531518 0.095670491 0.15145881 0.18335798 0.17474417 0.13056095 0.0675174 0.0061133844 -0.041035637 -0.0681791 -0.0746319 -0.065542236 -0.050269034][-0.053538341 -0.061058883 -0.04351978 -0.011890471 0.020083036 0.036869809 0.030833185 0.0062163207 -0.026627751 -0.056062635 -0.075448096 -0.082413837 -0.077366345 -0.064154156 -0.049280394][-0.092907995 -0.094656765 -0.082049355 -0.064361237 -0.048359871 -0.04110685 -0.044527933 -0.0553052 -0.068103969 -0.077589795 -0.080859877 -0.077476315 -0.068154328 -0.055160757 -0.042493671][-0.094179571 -0.092511721 -0.082841955 -0.072791919 -0.065501772 -0.063282967 -0.065231964 -0.068870239 -0.071921617 -0.072522834 -0.069588475 -0.063430034 -0.05462373 -0.044140223 -0.034321237][-0.078608692 -0.075317115 -0.067140445 -0.060705695 -0.057509441 -0.057609268 -0.059127353 -0.060282655 -0.060165521 -0.058341183 -0.054504368 -0.049201857 -0.0427419 -0.035266135 -0.028274221]]...]
INFO - root - 2017-12-10 16:13:03.131416: step 31010, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 66h:47m:47s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 16:13:10.762225: step 31020, loss = 0.69, batch loss = 0.64 (13.0 examples/sec; 0.615 sec/batch; 51h:30m:22s remains)
INFO - root - 2017-12-10 16:13:18.608023: step 31030, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 66h:38m:30s remains)
INFO - root - 2017-12-10 16:13:26.426498: step 31040, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 63h:40m:09s remains)
INFO - root - 2017-12-10 16:13:34.246845: step 31050, loss = 0.68, batch loss = 0.63 (9.6 examples/sec; 0.831 sec/batch; 69h:32m:39s remains)
INFO - root - 2017-12-10 16:13:42.063451: step 31060, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 67h:39m:41s remains)
INFO - root - 2017-12-10 16:13:49.653352: step 31070, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 65h:30m:19s remains)
INFO - root - 2017-12-10 16:13:57.554949: step 31080, loss = 0.71, batch loss = 0.65 (10.8 examples/sec; 0.739 sec/batch; 61h:54m:39s remains)
INFO - root - 2017-12-10 16:14:05.366382: step 31090, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 63h:46m:06s remains)
INFO - root - 2017-12-10 16:14:13.178400: step 31100, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 67h:21m:09s remains)
2017-12-10 16:14:14.129979: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20639937 0.25374666 0.29935139 0.34239545 0.37540126 0.384754 0.3559663 0.28887838 0.20354408 0.13662362 0.11804084 0.15724343 0.24232106 0.33864394 0.40563297][0.24193706 0.2903097 0.33164454 0.36509815 0.38645333 0.38859594 0.36206454 0.30516994 0.23084122 0.16681409 0.13669959 0.15276104 0.21163759 0.28581062 0.33768648][0.26076394 0.3104662 0.34833887 0.37325063 0.3860721 0.38748524 0.37289751 0.33652318 0.28070495 0.22121722 0.17440268 0.15735997 0.17952433 0.2248321 0.26202479][0.27304998 0.33102751 0.37358645 0.39917919 0.41383445 0.42466757 0.43040431 0.41925046 0.38194558 0.32241815 0.25139159 0.19261032 0.17045988 0.18285723 0.20638831][0.28051022 0.35442495 0.4119176 0.44991836 0.47732326 0.50493938 0.53225529 0.54083371 0.51145405 0.44012204 0.33711439 0.23459199 0.1703628 0.15382338 0.16699784][0.28293797 0.37844113 0.458803 0.51772344 0.56333375 0.60692596 0.64744145 0.6622569 0.62647951 0.53370231 0.39921942 0.26401305 0.17389506 0.14303368 0.15416518][0.27801484 0.39428395 0.49761474 0.57747513 0.638595 0.69060194 0.73154032 0.73821658 0.6853528 0.56929612 0.41452226 0.26858741 0.17706755 0.15088795 0.16904674][0.27200031 0.40012214 0.51483005 0.60184318 0.66235077 0.70529819 0.73071462 0.71949089 0.64964128 0.52188611 0.36846054 0.23793718 0.16907807 0.16547273 0.20076324][0.27247208 0.39818248 0.504099 0.5754661 0.613499 0.62991923 0.62930143 0.5986762 0.519735 0.39674672 0.265664 0.17088297 0.140418 0.16910902 0.22565132][0.26642114 0.37888139 0.46298185 0.50501949 0.50798911 0.4884029 0.45715976 0.40901113 0.32993382 0.22605863 0.13259177 0.08591865 0.10053973 0.16190146 0.23510814][0.24942538 0.34870979 0.41306362 0.42814675 0.39672402 0.33960259 0.27598092 0.21134266 0.13814908 0.062603295 0.014139306 0.017418435 0.073409528 0.16075081 0.24430205][0.22817783 0.3142353 0.36219239 0.35692346 0.29938424 0.21240513 0.1227922 0.04671732 -0.016190849 -0.060979467 -0.067584947 -0.02378856 0.061509915 0.16504496 0.25449604][0.21042663 0.28050867 0.3113409 0.28963995 0.21590278 0.11331431 0.013130532 -0.061963167 -0.10847555 -0.12523445 -0.10292159 -0.038100004 0.05790953 0.16571398 0.25767994][0.18592741 0.2386287 0.25264978 0.21926083 0.14114279 0.040889435 -0.050964579 -0.1118386 -0.13903598 -0.13574238 -0.10065815 -0.033289086 0.0584781 0.16142578 0.25188154][0.15510818 0.19662906 0.20197025 0.16646934 0.09532024 0.0095619587 -0.063605443 -0.10520021 -0.1160286 -0.10297083 -0.068986468 -0.011962737 0.066562578 0.15896647 0.24351531]]...]
INFO - root - 2017-12-10 16:14:21.763853: step 31110, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 63h:23m:21s remains)
INFO - root - 2017-12-10 16:14:29.539203: step 31120, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.755 sec/batch; 63h:11m:08s remains)
INFO - root - 2017-12-10 16:14:37.400829: step 31130, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 65h:09m:57s remains)
INFO - root - 2017-12-10 16:14:45.216781: step 31140, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 66h:56m:58s remains)
INFO - root - 2017-12-10 16:14:52.851061: step 31150, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.756 sec/batch; 63h:14m:39s remains)
INFO - root - 2017-12-10 16:15:00.645720: step 31160, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 65h:32m:41s remains)
INFO - root - 2017-12-10 16:15:08.528221: step 31170, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 66h:11m:16s remains)
INFO - root - 2017-12-10 16:15:16.361226: step 31180, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 66h:47m:15s remains)
INFO - root - 2017-12-10 16:15:24.170439: step 31190, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 64h:23m:17s remains)
INFO - root - 2017-12-10 16:15:31.898199: step 31200, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 64h:35m:39s remains)
2017-12-10 16:15:32.734354: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.046433274 0.038635947 0.041043706 0.047476944 0.05129448 0.051988445 0.050408222 0.047663089 0.048946932 0.06463705 0.10071912 0.1513446 0.20597535 0.2543844 0.28318051][0.089447752 0.09172982 0.10449293 0.12072856 0.13029282 0.13167241 0.12539564 0.11279954 0.1001686 0.0999682 0.12244041 0.16591777 0.22207594 0.27927449 0.32002246][0.11964715 0.13325743 0.15865427 0.188577 0.21050645 0.2214299 0.2190063 0.2013483 0.17307863 0.14781952 0.14161676 0.16115543 0.20556699 0.26478073 0.31795508][0.14013076 0.16299735 0.19954708 0.24557808 0.28854066 0.32301119 0.3400358 0.32927814 0.2886374 0.23122385 0.18044597 0.156935 0.17414643 0.2273311 0.29220518][0.15201518 0.17731294 0.22170594 0.28726304 0.36456227 0.44321921 0.5020777 0.51509851 0.46757087 0.37080392 0.258707 0.17331801 0.14897455 0.18771961 0.25964332][0.15846103 0.17883164 0.22770223 0.31516019 0.43651947 0.57368082 0.68866563 0.73498809 0.6823588 0.5416559 0.36076882 0.2066195 0.13521942 0.15425298 0.22719799][0.16378196 0.17642881 0.22488074 0.32916754 0.48949555 0.678746 0.84194291 0.91470194 0.85491669 0.67618144 0.43984202 0.23302503 0.12626444 0.13010059 0.20280604][0.17336713 0.17552248 0.21916437 0.33156806 0.51564837 0.73601174 0.92535949 1.0093156 0.941544 0.73991615 0.47503895 0.244684 0.12378658 0.12260447 0.19621396][0.1916659 0.18017246 0.21206729 0.31968653 0.50645089 0.73139942 0.9215399 1.0015987 0.92807448 0.72250128 0.45889133 0.23629218 0.12576266 0.13339317 0.21143682][0.20904088 0.17804377 0.18919525 0.27795374 0.44621429 0.65095574 0.8202858 0.88563395 0.81147993 0.6215933 0.38708687 0.19991992 0.1206013 0.14841142 0.23381485][0.22077902 0.17006759 0.15730679 0.21790555 0.35087562 0.51532054 0.64734775 0.6914885 0.62329853 0.46594888 0.28116566 0.14638512 0.10760021 0.15781479 0.24917433][0.20978104 0.14737648 0.11658394 0.14930387 0.24159867 0.35822365 0.44779336 0.47050357 0.412913 0.29519424 0.16554178 0.082839526 0.078340814 0.14167273 0.23077671][0.16220723 0.098704532 0.061063204 0.073211126 0.1297525 0.20393884 0.25798431 0.26567513 0.22089796 0.13935266 0.055773579 0.011691771 0.026675455 0.089680977 0.16655143][0.08409325 0.029520553 -0.0057819979 -0.0066360687 0.022396477 0.0639242 0.092948645 0.093123153 0.061625026 0.0090373829 -0.041502938 -0.062456492 -0.042219561 0.0094978875 0.067239538][-0.0060557178 -0.046164986 -0.072790548 -0.079833493 -0.070178516 -0.052677903 -0.041112389 -0.044393264 -0.064152792 -0.094774738 -0.12285056 -0.13173869 -0.11498672 -0.079551622 -0.042051457]]...]
INFO - root - 2017-12-10 16:15:40.600813: step 31210, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 65h:51m:40s remains)
INFO - root - 2017-12-10 16:15:48.479040: step 31220, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 65h:32m:26s remains)
INFO - root - 2017-12-10 16:15:56.184108: step 31230, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 63h:57m:13s remains)
INFO - root - 2017-12-10 16:16:04.054350: step 31240, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.809 sec/batch; 67h:43m:23s remains)
INFO - root - 2017-12-10 16:16:11.953118: step 31250, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 65h:42m:28s remains)
INFO - root - 2017-12-10 16:16:19.751506: step 31260, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 64h:30m:30s remains)
INFO - root - 2017-12-10 16:16:27.552139: step 31270, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.820 sec/batch; 68h:35m:48s remains)
INFO - root - 2017-12-10 16:16:35.435664: step 31280, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 67h:31m:48s remains)
INFO - root - 2017-12-10 16:16:43.091411: step 31290, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 66h:29m:04s remains)
INFO - root - 2017-12-10 16:16:50.937098: step 31300, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 65h:36m:00s remains)
2017-12-10 16:16:51.824808: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.264981 0.26602808 0.26953676 0.2761423 0.27895802 0.271494 0.25351721 0.23564731 0.22865254 0.22582993 0.21975607 0.21845241 0.23417677 0.26726729 0.30480778][0.31491527 0.31742507 0.31954396 0.32320479 0.3198607 0.30209896 0.27386841 0.25036883 0.24148773 0.23793626 0.23170488 0.23182811 0.25130755 0.28956279 0.33138046][0.34007886 0.3468523 0.34875056 0.34760284 0.33579877 0.30828813 0.27376711 0.24897724 0.24035235 0.23612833 0.22825377 0.22639976 0.24423088 0.28144893 0.32235676][0.34627196 0.35842329 0.3617121 0.35725495 0.33951136 0.30648267 0.27068484 0.24819914 0.2417338 0.23752229 0.22621621 0.21806209 0.22768913 0.25711578 0.29218572][0.3344073 0.34882572 0.35294676 0.34767511 0.33003044 0.29986066 0.270646 0.25513834 0.25274494 0.24872667 0.23171028 0.21229625 0.2069217 0.22258554 0.24860421][0.30629939 0.31862408 0.32221389 0.31962004 0.31030518 0.29285878 0.27777407 0.27228314 0.27447358 0.2707192 0.24790297 0.21603024 0.19300491 0.19239804 0.2081553][0.27122146 0.27918738 0.28134817 0.28308162 0.28669575 0.28783944 0.29083994 0.29565921 0.30072755 0.29633835 0.26964247 0.22867231 0.19086966 0.17560525 0.18172902][0.24363749 0.24976753 0.25233707 0.25902873 0.27552131 0.29519048 0.31482881 0.3273218 0.33260605 0.3268401 0.2991482 0.25451291 0.20811872 0.18294425 0.18118864][0.23884612 0.24923813 0.25599384 0.26730275 0.29198304 0.32386541 0.35452956 0.371652 0.37536371 0.36749592 0.341053 0.29818052 0.25010392 0.21972995 0.21108127][0.25912187 0.27930823 0.29286048 0.30633137 0.33099428 0.36404273 0.39575195 0.412634 0.41404006 0.40536958 0.383859 0.34910932 0.30736461 0.27792841 0.26541525][0.28830573 0.32034698 0.34161109 0.35574484 0.37416559 0.39824918 0.42078769 0.43183827 0.42971814 0.42041188 0.40564498 0.38342032 0.35531369 0.33484802 0.32517409][0.30192548 0.34323642 0.37053403 0.384275 0.3944968 0.40617636 0.41602662 0.41954568 0.41370624 0.40297383 0.39322746 0.38263738 0.370136 0.36326748 0.36223635][0.28277752 0.32644084 0.35531703 0.36807558 0.3728531 0.37622756 0.37742555 0.37613919 0.36844838 0.35636044 0.34911373 0.34607923 0.34547696 0.34982398 0.35709327][0.23718587 0.27352905 0.2961736 0.30425757 0.30495423 0.30494788 0.30374986 0.30199334 0.29521295 0.2832084 0.27685389 0.27687794 0.28206396 0.29162931 0.30282184][0.18051144 0.20266847 0.21269511 0.21230781 0.20876111 0.20798565 0.20777965 0.20704427 0.20164222 0.19058344 0.184521 0.18512794 0.19209659 0.20290785 0.21466297]]...]
INFO - root - 2017-12-10 16:16:59.493456: step 31310, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 64h:18m:45s remains)
INFO - root - 2017-12-10 16:17:07.451093: step 31320, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 65h:21m:34s remains)
INFO - root - 2017-12-10 16:17:15.262808: step 31330, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 65h:59m:58s remains)
INFO - root - 2017-12-10 16:17:23.074290: step 31340, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 64h:52m:49s remains)
INFO - root - 2017-12-10 16:17:31.008829: step 31350, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 65h:03m:46s remains)
INFO - root - 2017-12-10 16:17:38.891434: step 31360, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 66h:54m:33s remains)
INFO - root - 2017-12-10 16:17:46.748855: step 31370, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 68h:11m:37s remains)
INFO - root - 2017-12-10 16:17:54.515120: step 31380, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 64h:19m:29s remains)
INFO - root - 2017-12-10 16:18:02.194541: step 31390, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 63h:50m:03s remains)
INFO - root - 2017-12-10 16:18:10.051235: step 31400, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 63h:45m:48s remains)
2017-12-10 16:18:10.880125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.010048775 0.02880449 0.073646553 0.11549534 0.14558424 0.15674888 0.15993105 0.1639846 0.16443732 0.16188806 0.16024348 0.16905616 0.19598192 0.24447376 0.30397505][-0.00824556 0.036139116 0.089023657 0.14105347 0.17994523 0.1931829 0.18837544 0.17428002 0.14936465 0.12215972 0.10593184 0.11824224 0.16988544 0.25724897 0.35827193][-0.0075375829 0.043554582 0.10828902 0.1761287 0.2312302 0.25355852 0.24428548 0.21030277 0.15501541 0.096867248 0.060458381 0.07155247 0.14415897 0.26668257 0.40579402][-0.0057276157 0.05546378 0.13851413 0.23070873 0.3129583 0.35634735 0.35263252 0.30522323 0.22186004 0.13098253 0.067645684 0.06547045 0.14335969 0.28512853 0.44858524][-8.4075928e-06 0.075598821 0.18280281 0.30584502 0.42332307 0.49745193 0.50886214 0.45589826 0.34849885 0.22416747 0.12606475 0.095665626 0.15841039 0.2987383 0.4699319][0.0085429084 0.09995763 0.23203731 0.38531163 0.53704321 0.64240456 0.67257136 0.62061453 0.49682564 0.34537032 0.2146019 0.15168992 0.18674588 0.3075383 0.46821645][0.014387269 0.11717246 0.26782376 0.44283631 0.61911285 0.74782485 0.79362512 0.74515396 0.6126858 0.44493431 0.29307348 0.20594734 0.21424487 0.30883843 0.44751394][0.014851441 0.12229403 0.28303397 0.46985847 0.65827703 0.79815787 0.85261774 0.80827177 0.67464262 0.5033834 0.34810525 0.25519094 0.24976191 0.32262611 0.43559489][0.012527192 0.11883651 0.28120798 0.46905866 0.65531093 0.79160774 0.84646416 0.80798489 0.68424809 0.52732861 0.39138052 0.316664 0.31695026 0.37900102 0.46812424][0.0098444065 0.10970274 0.26472351 0.44098338 0.60871953 0.72516119 0.77200526 0.74269283 0.6427691 0.52021182 0.42575094 0.39066839 0.41307172 0.47322631 0.54205579][0.0044892887 0.09258341 0.23186392 0.38644046 0.52498984 0.61284876 0.64818788 0.63261247 0.56818908 0.4929817 0.44983444 0.45918953 0.50467539 0.56113034 0.60789788][-0.0061828461 0.066446155 0.18431193 0.31211832 0.41824141 0.47590309 0.49770322 0.49408183 0.46422389 0.43345618 0.43426624 0.47705287 0.5354104 0.58206004 0.6058374][-0.020264162 0.0364402 0.13075355 0.23078306 0.30704647 0.3396396 0.34954298 0.35352084 0.34915596 0.35019264 0.37837088 0.43610567 0.49262604 0.52223575 0.52284223][-0.037946437 0.002107071 0.070145808 0.13966934 0.18734755 0.20037058 0.20204695 0.210364 0.22136049 0.23965934 0.27682331 0.329995 0.37110746 0.38014394 0.36209843][-0.061368082 -0.0387109 0.0023331167 0.041356586 0.062891781 0.060929548 0.056456447 0.064552829 0.080148451 0.10167389 0.13282013 0.16859944 0.18889216 0.18154639 0.15490404]]...]
INFO - root - 2017-12-10 16:18:18.760901: step 31410, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.785 sec/batch; 65h:36m:52s remains)
INFO - root - 2017-12-10 16:18:26.639178: step 31420, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 65h:31m:32s remains)
INFO - root - 2017-12-10 16:18:34.603731: step 31430, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 66h:07m:38s remains)
INFO - root - 2017-12-10 16:18:42.526833: step 31440, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 66h:52m:23s remains)
INFO - root - 2017-12-10 16:18:50.429449: step 31450, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 64h:18m:56s remains)
INFO - root - 2017-12-10 16:18:58.404587: step 31460, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 65h:04m:17s remains)
INFO - root - 2017-12-10 16:19:05.930396: step 31470, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 66h:55m:57s remains)
INFO - root - 2017-12-10 16:19:13.884085: step 31480, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.798 sec/batch; 66h:44m:04s remains)
INFO - root - 2017-12-10 16:19:21.703236: step 31490, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 65h:37m:08s remains)
INFO - root - 2017-12-10 16:19:29.565519: step 31500, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 65h:14m:21s remains)
2017-12-10 16:19:30.416448: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040853564 -0.028203061 -0.010312565 0.010377247 0.029540045 0.041010417 0.041518167 0.035125557 0.0247282 0.010625095 -0.0059041921 -0.023257473 -0.037076227 -0.043771703 -0.042327944][-0.04252018 -0.028421143 -0.0073686708 0.018175716 0.043252986 0.060207371 0.064605027 0.059495304 0.0460163 0.023810439 -0.0032743388 -0.028869873 -0.0455494 -0.050073203 -0.04176892][-0.041603711 -0.026291311 -0.0020915756 0.028905366 0.061343264 0.086117834 0.097324684 0.096085951 0.080513619 0.048930131 0.0088262139 -0.026571458 -0.046147633 -0.0474703 -0.030689903][-0.039338466 -0.021815218 0.0067750993 0.044694461 0.086275779 0.12127594 0.14204437 0.14687712 0.130048 0.088692777 0.034758534 -0.011607408 -0.035512805 -0.035023034 -0.011076066][-0.036254305 -0.015891515 0.018091904 0.064570069 0.11768546 0.16570124 0.19821122 0.20930636 0.18959418 0.13703591 0.068708114 0.011080901 -0.01771521 -0.016190335 0.014069553][-0.034169007 -0.011139622 0.028689943 0.085387394 0.15275492 0.21627575 0.26102942 0.275618 0.24842018 0.180929 0.097012274 0.028429503 -0.0051221428 -0.0026904335 0.033458352][-0.032980964 -0.0073681646 0.03868375 0.10602105 0.18751928 0.26508883 0.31868312 0.33211702 0.29402912 0.21138792 0.11407784 0.036501616 -0.0014115162 0.0010926971 0.041169442][-0.032365531 -0.0042817616 0.046953026 0.12183309 0.21150926 0.29504856 0.34945965 0.35718173 0.3095884 0.21886936 0.11718293 0.037306953 -0.0021844502 -0.00015280153 0.040921271][-0.032202687 -0.0027249528 0.050960973 0.12761991 0.21639794 0.29554042 0.34247518 0.34194663 0.28971112 0.20151857 0.10760728 0.034961108 -0.00073990255 0.0021190951 0.0419152][-0.033005763 -0.0037042887 0.049133264 0.12220963 0.20334849 0.27183679 0.30762476 0.29949152 0.24784462 0.17063683 0.093727611 0.036303297 0.0099975308 0.016223473 0.053701773][-0.033954263 -0.0057491991 0.044666409 0.11254425 0.18530896 0.24390785 0.27085042 0.25845075 0.21073531 0.146828 0.088682055 0.048587069 0.033761583 0.044182479 0.07769113][-0.033869334 -0.0066108247 0.042038925 0.10632997 0.17364834 0.22628887 0.24835926 0.23450767 0.19110087 0.13813901 0.095702052 0.07156942 0.068409584 0.0830057 0.1113982][-0.032601412 -0.0050797234 0.043308381 0.10589187 0.16954778 0.21730834 0.23460396 0.21842894 0.17751522 0.1331384 0.10497598 0.097539179 0.10794103 0.12826762 0.15201768][-0.030303689 -0.0019422951 0.046561632 0.10699856 0.16506934 0.20441221 0.21277858 0.190836 0.15062897 0.11498905 0.10305275 0.11551858 0.14272416 0.17155869 0.19311525][-0.028732331 -0.000674305 0.045633804 0.10069618 0.14957404 0.17728627 0.175013 0.14784525 0.11086737 0.087047808 0.0936644 0.12712759 0.17128392 0.20885508 0.22952884]]...]
INFO - root - 2017-12-10 16:19:38.276361: step 31510, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 64h:04m:45s remains)
INFO - root - 2017-12-10 16:19:46.114034: step 31520, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 66h:11m:29s remains)
INFO - root - 2017-12-10 16:19:54.154307: step 31530, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 66h:21m:17s remains)
INFO - root - 2017-12-10 16:20:02.189202: step 31540, loss = 0.68, batch loss = 0.62 (9.3 examples/sec; 0.858 sec/batch; 71h:42m:37s remains)
INFO - root - 2017-12-10 16:20:09.976617: step 31550, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 66h:29m:33s remains)
INFO - root - 2017-12-10 16:20:17.647272: step 31560, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.760 sec/batch; 63h:32m:15s remains)
INFO - root - 2017-12-10 16:20:25.411382: step 31570, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.758 sec/batch; 63h:21m:02s remains)
INFO - root - 2017-12-10 16:20:33.243922: step 31580, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 64h:51m:55s remains)
INFO - root - 2017-12-10 16:20:41.135632: step 31590, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 66h:41m:59s remains)
INFO - root - 2017-12-10 16:20:48.990753: step 31600, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 65h:24m:06s remains)
2017-12-10 16:20:49.864599: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.309832 0.37753946 0.42988649 0.46130159 0.4675878 0.44349062 0.39797965 0.35296261 0.31826118 0.28840938 0.25624889 0.22119215 0.1907602 0.17662071 0.18844266][0.33423811 0.40840781 0.4683603 0.50810349 0.52397132 0.51149857 0.47590467 0.435169 0.39696646 0.35820553 0.31384188 0.26493028 0.22324103 0.20546129 0.22472852][0.30843055 0.3850244 0.45156574 0.50088549 0.52773756 0.52840734 0.50597018 0.47273153 0.43290144 0.38571456 0.33136988 0.27388337 0.22641538 0.20913087 0.23813301][0.25191346 0.33008796 0.40461683 0.46548939 0.50465149 0.52019924 0.51367587 0.49104729 0.45300591 0.40055183 0.33918902 0.27473944 0.22071466 0.20043653 0.23280407][0.18846849 0.27149561 0.35927212 0.43678108 0.49205577 0.52569079 0.53833693 0.529174 0.49557689 0.43978712 0.37103093 0.29542798 0.22742771 0.19288667 0.21432424][0.14501192 0.23404448 0.33782455 0.43530795 0.50871414 0.55925983 0.58835548 0.58979315 0.55788338 0.4958621 0.41541824 0.32409626 0.23756777 0.18263881 0.18438117][0.1255765 0.21521859 0.32878631 0.44244161 0.5319103 0.59623611 0.63875049 0.64894396 0.61773735 0.54852837 0.4556959 0.34939778 0.24602409 0.17118742 0.15039603][0.11622909 0.19487108 0.3038027 0.42184845 0.52091908 0.59605247 0.65195692 0.67467326 0.6506266 0.58174253 0.48381078 0.36976108 0.25593239 0.16542198 0.12323724][0.10974517 0.16546141 0.25375143 0.36042285 0.45883757 0.54019558 0.60911185 0.6498245 0.64330196 0.58766061 0.49648091 0.38398963 0.26654509 0.16492459 0.10377791][0.089938521 0.11842015 0.17836237 0.26379141 0.35280073 0.43344411 0.51021808 0.56793231 0.58269906 0.54823792 0.47353506 0.37264156 0.26065525 0.15586071 0.082539022][0.05002863 0.052359194 0.082541488 0.14172339 0.21402705 0.28560004 0.36117628 0.42892677 0.46298277 0.45311421 0.40255171 0.32368666 0.22894107 0.13275202 0.05796174][8.6830143e-05 -0.019210892 -0.015740113 0.016251164 0.067857862 0.12593362 0.19548276 0.26885113 0.32060921 0.33695245 0.31491742 0.26339635 0.19171883 0.10990058 0.039952938][-0.031323079 -0.065737426 -0.0829937 -0.075458422 -0.045123346 -0.0021492206 0.058251653 0.13242471 0.1970342 0.23521984 0.238821 0.21288298 0.16388154 0.0969132 0.032838464][-0.023032064 -0.066812143 -0.097805753 -0.1081057 -0.095601834 -0.0673559 -0.019149093 0.048852276 0.1169433 0.16756515 0.19003734 0.18530734 0.15683953 0.10439842 0.045932751][0.026560962 -0.023617219 -0.063268252 -0.083768047 -0.082852229 -0.06644541 -0.031455245 0.02462313 0.08682894 0.13987723 0.17369978 0.18545942 0.17481729 0.13640521 0.0842321]]...]
INFO - root - 2017-12-10 16:20:57.888984: step 31610, loss = 0.71, batch loss = 0.65 (9.5 examples/sec; 0.839 sec/batch; 70h:05m:19s remains)
INFO - root - 2017-12-10 16:21:05.806399: step 31620, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 67h:09m:55s remains)
INFO - root - 2017-12-10 16:21:13.446844: step 31630, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 65h:47m:41s remains)
INFO - root - 2017-12-10 16:21:21.355043: step 31640, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 64h:21m:45s remains)
INFO - root - 2017-12-10 16:21:29.033148: step 31650, loss = 0.67, batch loss = 0.61 (10.1 examples/sec; 0.795 sec/batch; 66h:28m:11s remains)
INFO - root - 2017-12-10 16:21:36.908956: step 31660, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 67h:42m:47s remains)
INFO - root - 2017-12-10 16:21:44.734260: step 31670, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 63h:33m:51s remains)
INFO - root - 2017-12-10 16:21:52.705299: step 31680, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 67h:17m:52s remains)
INFO - root - 2017-12-10 16:22:00.528121: step 31690, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 64h:58m:43s remains)
INFO - root - 2017-12-10 16:22:08.551659: step 31700, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 67h:22m:05s remains)
2017-12-10 16:22:09.383026: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16054551 0.18705168 0.21904315 0.24133322 0.24435648 0.23656759 0.22860463 0.22035892 0.20125984 0.17867301 0.15809324 0.13513596 0.10525481 0.067705862 0.029977987][0.20871495 0.23924275 0.26731804 0.2805309 0.27293491 0.25641876 0.24636106 0.24179719 0.22943696 0.21338518 0.19927207 0.17904018 0.14551204 0.0991693 0.050809506][0.23418008 0.26571468 0.28612965 0.28658807 0.26661524 0.24192223 0.23358907 0.24008678 0.24164049 0.23714268 0.23073842 0.21382764 0.17699979 0.12238391 0.065358259][0.23873685 0.2665672 0.27825582 0.26717195 0.23679759 0.20535246 0.19894014 0.21688761 0.23263709 0.2394738 0.24013887 0.22614889 0.1883025 0.13026284 0.069240287][0.26191434 0.28784943 0.29410979 0.27536404 0.23717219 0.19796556 0.18603325 0.20310979 0.22124124 0.23276 0.23884957 0.2297477 0.19616225 0.14104605 0.080205262][0.31772369 0.34478763 0.34735635 0.32259139 0.2802411 0.23732513 0.21775351 0.22344975 0.23032951 0.23478767 0.23866321 0.23055027 0.20126069 0.15128073 0.093222372][0.36788213 0.39414975 0.39055276 0.35813403 0.315736 0.27908519 0.26143488 0.26038879 0.25684503 0.25319943 0.25169468 0.23991621 0.20980386 0.16089734 0.10355449][0.38500544 0.40751889 0.39720413 0.35897958 0.32102913 0.297247 0.28794804 0.28498945 0.2738443 0.26374626 0.25659779 0.23935047 0.20582373 0.1567162 0.10106955][0.35208017 0.37219295 0.36049342 0.32223409 0.29021725 0.27578232 0.2697826 0.26092458 0.24198306 0.2279375 0.21931976 0.20038062 0.16746935 0.12407924 0.076933764][0.26570383 0.28447723 0.27507466 0.24204895 0.21742323 0.20976594 0.20437537 0.18974406 0.1668216 0.15381575 0.14821924 0.13121796 0.10226381 0.068334088 0.034257691][0.15090953 0.16635782 0.16090353 0.13765553 0.12418812 0.12582774 0.12456585 0.11010439 0.089096934 0.079584032 0.075999483 0.05935194 0.033047844 0.0072975694 -0.01429884][0.044401806 0.055590481 0.0539003 0.04073124 0.038121637 0.047961988 0.051595975 0.04062479 0.024288289 0.018756114 0.016441811 0.0012947083 -0.020759316 -0.038044073 -0.048442978][-0.016256416 -0.010542559 -0.010695473 -0.017341847 -0.014762779 -0.0032360298 0.0018495885 -0.0051791957 -0.015781885 -0.017761303 -0.018805476 -0.030462075 -0.04667972 -0.056849994 -0.060157891][-0.044165369 -0.04257055 -0.041760404 -0.044292469 -0.040870883 -0.032338031 -0.028475061 -0.032242235 -0.037773721 -0.037946619 -0.038723171 -0.046777759 -0.057134632 -0.062243603 -0.062354907][-0.066764131 -0.067884021 -0.066122264 -0.065797128 -0.06264203 -0.057087794 -0.054169156 -0.054988161 -0.056691337 -0.05607748 -0.056623194 -0.060906496 -0.065576993 -0.066806436 -0.065335892]]...]
INFO - root - 2017-12-10 16:22:17.237803: step 31710, loss = 0.70, batch loss = 0.64 (8.7 examples/sec; 0.922 sec/batch; 77h:00m:40s remains)
INFO - root - 2017-12-10 16:22:25.113307: step 31720, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 66h:30m:38s remains)
INFO - root - 2017-12-10 16:22:32.981611: step 31730, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 65h:18m:21s remains)
INFO - root - 2017-12-10 16:22:40.836870: step 31740, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 66h:11m:24s remains)
INFO - root - 2017-12-10 16:22:48.804221: step 31750, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 66h:15m:07s remains)
INFO - root - 2017-12-10 16:22:56.640037: step 31760, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 64h:14m:51s remains)
INFO - root - 2017-12-10 16:23:04.558081: step 31770, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 67h:01m:30s remains)
INFO - root - 2017-12-10 16:23:12.475297: step 31780, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 66h:08m:03s remains)
INFO - root - 2017-12-10 16:23:20.159906: step 31790, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.767 sec/batch; 64h:05m:48s remains)
INFO - root - 2017-12-10 16:23:28.036313: step 31800, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 66h:26m:26s remains)
2017-12-10 16:23:28.878111: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035642445 0.069642715 0.11296498 0.16191246 0.20523193 0.22958837 0.22566113 0.19887759 0.16786619 0.1568017 0.18080768 0.23096466 0.27811581 0.29231167 0.26160565][0.070278838 0.098444343 0.13527106 0.17937945 0.22080274 0.24588157 0.24244514 0.21417946 0.1799262 0.16477503 0.18751127 0.2419202 0.29768091 0.3198587 0.29135767][0.11564347 0.13533671 0.16565202 0.20773989 0.25201842 0.28233114 0.28177026 0.25127929 0.20908083 0.18069373 0.18805872 0.23033673 0.28047645 0.30384964 0.28071007][0.16695185 0.17853205 0.20582512 0.25189364 0.30572414 0.34675783 0.35251674 0.32071695 0.26816982 0.22007363 0.20139661 0.21768306 0.24851391 0.2640053 0.24503924][0.23120616 0.23492444 0.26178807 0.31508237 0.38051429 0.43358442 0.44804597 0.41881153 0.35949498 0.29320571 0.24671827 0.23136036 0.23484083 0.234418 0.21466419][0.3013314 0.29601991 0.32332709 0.3849667 0.46151868 0.52587366 0.55060232 0.52802306 0.46685579 0.38672438 0.31570065 0.27050221 0.24714218 0.22969303 0.20676379][0.35704595 0.34159938 0.36772779 0.43561682 0.52071154 0.59442937 0.62902361 0.61450428 0.55463564 0.46646303 0.37916803 0.31395632 0.27319145 0.24577706 0.22242945][0.38040397 0.355184 0.37763569 0.44719565 0.53625488 0.61580265 0.65780556 0.65105009 0.59557462 0.50688285 0.41415134 0.34075847 0.292973 0.26314723 0.2426836][0.363022 0.33506921 0.3539916 0.419244 0.50390929 0.5807398 0.62322819 0.6207999 0.57188565 0.49059433 0.40369639 0.33306164 0.28655544 0.25975075 0.24515329][0.30982196 0.28771031 0.30588 0.36292583 0.43541244 0.50048107 0.53578508 0.53309023 0.49138266 0.42316344 0.35015431 0.28952235 0.24916066 0.22807306 0.2209256][0.23423113 0.22490741 0.24650706 0.29553697 0.35318226 0.40218142 0.42612869 0.41986403 0.38393587 0.329057 0.27146298 0.22319344 0.19155665 0.17806834 0.17928471][0.15624729 0.16332166 0.19099225 0.23316576 0.27553013 0.30675915 0.31795874 0.30767614 0.27782318 0.23692128 0.19612809 0.16278735 0.14258598 0.13789341 0.14670911][0.091443792 0.11250261 0.14661741 0.18449222 0.21449116 0.22959942 0.22801749 0.21258539 0.18730967 0.15934348 0.13577195 0.11977995 0.11434273 0.12096734 0.13801868][0.043935709 0.072577916 0.10934521 0.14374357 0.16519319 0.16871876 0.15707202 0.13692637 0.11513744 0.098124668 0.089729056 0.089687988 0.098125592 0.11559159 0.1406894][0.023209913 0.049544822 0.082191952 0.11152819 0.12734327 0.12494393 0.10833037 0.086093985 0.067399018 0.058324471 0.06079983 0.072325155 0.0905497 0.11512359 0.14507496]]...]
INFO - root - 2017-12-10 16:23:36.793553: step 31810, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.787 sec/batch; 65h:44m:09s remains)
INFO - root - 2017-12-10 16:23:44.640488: step 31820, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 64h:09m:02s remains)
INFO - root - 2017-12-10 16:23:52.337047: step 31830, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 66h:59m:03s remains)
INFO - root - 2017-12-10 16:24:00.170401: step 31840, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 64h:12m:41s remains)
INFO - root - 2017-12-10 16:24:07.985901: step 31850, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 64h:26m:33s remains)
INFO - root - 2017-12-10 16:24:15.871843: step 31860, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 65h:00m:10s remains)
INFO - root - 2017-12-10 16:24:23.528815: step 31870, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 64h:22m:59s remains)
INFO - root - 2017-12-10 16:24:31.468539: step 31880, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 67h:32m:38s remains)
INFO - root - 2017-12-10 16:24:39.392109: step 31890, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 65h:22m:05s remains)
INFO - root - 2017-12-10 16:24:47.235832: step 31900, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 67h:44m:03s remains)
2017-12-10 16:24:48.074370: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.081054382 0.13931702 0.18241931 0.19103162 0.17224714 0.14742008 0.13173966 0.13525525 0.15793578 0.18490855 0.19675975 0.18195717 0.14504056 0.094740696 0.045140933][0.039535739 0.0921431 0.1385249 0.15841837 0.15654624 0.15004121 0.14920586 0.15902191 0.17683595 0.18917763 0.17989916 0.14319947 0.090960808 0.03641092 -0.0082113519][-0.0038458789 0.043118063 0.095409386 0.13246061 0.15394771 0.17025068 0.18544382 0.19971511 0.2078492 0.19908062 0.1621753 0.099284418 0.029819446 -0.029723065 -0.068337575][-0.030007493 0.012989835 0.0725789 0.12915505 0.17531003 0.21366255 0.242603 0.25871047 0.25405377 0.22093959 0.15513369 0.067356251 -0.017260766 -0.079982847 -0.1113015][-0.025987856 0.020304032 0.093013875 0.17316617 0.24538045 0.30549395 0.34746727 0.36489102 0.34637967 0.28789094 0.19247749 0.078858964 -0.023061654 -0.0917293 -0.11788682][0.010083443 0.066022292 0.15506974 0.25824809 0.3527537 0.42927852 0.48007396 0.49698266 0.46525002 0.38489729 0.26454395 0.12842891 0.00947781 -0.067145266 -0.090950705][0.074640311 0.14078274 0.2419413 0.35983536 0.4667573 0.55027461 0.60308766 0.61670852 0.57415384 0.47953796 0.343958 0.19522576 0.065565839 -0.01726941 -0.041370157][0.15058561 0.222324 0.32656202 0.44694716 0.55505693 0.6372754 0.68759143 0.69766748 0.64975554 0.55069304 0.41230318 0.2629931 0.13146338 0.046259202 0.020072052][0.21947473 0.28937361 0.38494387 0.49478182 0.59393263 0.66905093 0.7149756 0.723236 0.67649353 0.58181131 0.451264 0.31228444 0.18905292 0.10811954 0.080970906][0.26612434 0.32785451 0.40666389 0.49638322 0.57820606 0.6408084 0.67971253 0.68684459 0.64664209 0.56451541 0.45194319 0.33376315 0.22918977 0.16021845 0.13578649][0.27337176 0.32054749 0.37557372 0.43713394 0.49321324 0.53573918 0.56177527 0.56530344 0.53442425 0.47204489 0.38782611 0.30202657 0.22832432 0.18214436 0.16850287][0.23641111 0.26495981 0.29401612 0.3258546 0.35462213 0.37556347 0.38724819 0.38634396 0.36529708 0.3249501 0.27270862 0.22380906 0.18683772 0.17015587 0.17456742][0.16083831 0.17310295 0.18231468 0.19185996 0.20033933 0.20608222 0.20869403 0.20671822 0.19591253 0.17566572 0.15155254 0.1342375 0.12886405 0.13843733 0.16104655][0.072165981 0.073620379 0.071660139 0.068652593 0.065646581 0.063396327 0.062411275 0.062370729 0.0610237 0.057140514 0.054667469 0.060463462 0.07736095 0.10523363 0.14093077][-0.0021674614 -0.0049387822 -0.0095586805 -0.015856121 -0.021942951 -0.025724925 -0.025912503 -0.022107394 -0.015703501 -0.0079742931 0.0038136598 0.023657015 0.052750319 0.089522451 0.13048938]]...]
INFO - root - 2017-12-10 16:24:55.807459: step 31910, loss = 0.72, batch loss = 0.66 (12.1 examples/sec; 0.660 sec/batch; 55h:05m:02s remains)
INFO - root - 2017-12-10 16:25:03.693916: step 31920, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 64h:45m:12s remains)
INFO - root - 2017-12-10 16:25:11.576866: step 31930, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 64h:36m:25s remains)
INFO - root - 2017-12-10 16:25:19.400092: step 31940, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 64h:29m:22s remains)
INFO - root - 2017-12-10 16:25:27.171790: step 31950, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 65h:56m:08s remains)
INFO - root - 2017-12-10 16:25:34.985477: step 31960, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 63h:43m:33s remains)
INFO - root - 2017-12-10 16:25:42.791485: step 31970, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 63h:31m:55s remains)
INFO - root - 2017-12-10 16:25:50.698141: step 31980, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 64h:15m:33s remains)
INFO - root - 2017-12-10 16:25:58.574822: step 31990, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 66h:10m:54s remains)
INFO - root - 2017-12-10 16:26:06.296932: step 32000, loss = 0.70, batch loss = 0.65 (12.9 examples/sec; 0.619 sec/batch; 51h:39m:27s remains)
2017-12-10 16:26:07.248731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.034979578 -0.012061421 0.018144676 0.058014054 0.11155648 0.17430794 0.23037583 0.2629182 0.26821688 0.25080091 0.2185214 0.18323596 0.1502381 0.11857118 0.078111887][-0.047773831 -0.026149293 0.006917702 0.05599644 0.1276802 0.21436933 0.29509652 0.34691295 0.36221585 0.34457424 0.30268958 0.2521421 0.20124164 0.15121035 0.093640864][-0.059406061 -0.038644336 -0.0029438019 0.053341966 0.13984017 0.24672689 0.34949824 0.41933766 0.44391498 0.42523855 0.3726356 0.3052519 0.23504896 0.16663767 0.094684713][-0.065439261 -0.040267985 0.00379937 0.071606465 0.1746802 0.30096018 0.42386711 0.50913107 0.53990626 0.51665479 0.44959095 0.36179805 0.26913556 0.18005943 0.093386956][-0.05603078 -0.017483613 0.047297228 0.13919374 0.26722598 0.41482264 0.55369765 0.6459344 0.67308092 0.63676625 0.54895329 0.43467623 0.31263405 0.19632758 0.09098193][-0.025295274 0.035666537 0.13375233 0.26289806 0.42299351 0.58965909 0.73295343 0.81560606 0.82261795 0.76167333 0.64727861 0.50307852 0.34900191 0.20389356 0.080841444][0.022418061 0.10697297 0.24055794 0.40849137 0.59645021 0.77029437 0.89963621 0.95372331 0.926185 0.83393747 0.69397962 0.5267607 0.35114586 0.18954691 0.05983318][0.076890476 0.17599104 0.33207479 0.52279639 0.71939754 0.87964183 0.97593921 0.98933208 0.92443085 0.80636227 0.65349728 0.48247293 0.3089762 0.15430567 0.034676392][0.12935376 0.22576374 0.37907255 0.56343263 0.74131536 0.868185 0.92246759 0.89794719 0.80771464 0.68119097 0.53711647 0.38793972 0.2440142 0.11992063 0.024368744][0.17214635 0.24906519 0.37390628 0.52209365 0.65579331 0.735229 0.74711466 0.6956172 0.59826368 0.48542812 0.37557417 0.27585298 0.18715267 0.11241043 0.049854677][0.19453517 0.2430833 0.3245573 0.42027915 0.49933866 0.53131592 0.51082712 0.44776106 0.36161581 0.28188893 0.2264639 0.19552918 0.17702717 0.15908282 0.12846711][0.18607481 0.20802607 0.24825647 0.29569197 0.32987368 0.33043557 0.29686356 0.2398871 0.1793537 0.14299805 0.14749515 0.18528828 0.23139359 0.26016113 0.2493857][0.15144685 0.15728083 0.17175412 0.19009 0.20122378 0.19264916 0.16552345 0.12830409 0.099203594 0.10267618 0.15353715 0.23850004 0.32425305 0.37631747 0.36857712][0.10149804 0.10178842 0.10745306 0.11706503 0.12548718 0.12521532 0.11696533 0.10536972 0.10442999 0.13406052 0.20720229 0.30890667 0.40449354 0.45710498 0.44220704][0.048030734 0.049032085 0.0562236 0.068840511 0.084102094 0.097278006 0.10798291 0.1173619 0.1341005 0.17301244 0.24590541 0.33999002 0.42503333 0.46822047 0.44943178]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 16:26:15.164387: step 32010, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 66h:32m:47s remains)
INFO - root - 2017-12-10 16:26:22.928124: step 32020, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.758 sec/batch; 63h:17m:19s remains)
INFO - root - 2017-12-10 16:26:30.592559: step 32030, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 67h:26m:16s remains)
INFO - root - 2017-12-10 16:26:38.550575: step 32040, loss = 0.72, batch loss = 0.66 (9.7 examples/sec; 0.824 sec/batch; 68h:44m:43s remains)
INFO - root - 2017-12-10 16:26:46.401468: step 32050, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 66h:41m:35s remains)
INFO - root - 2017-12-10 16:26:54.290195: step 32060, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 65h:21m:23s remains)
INFO - root - 2017-12-10 16:27:02.167351: step 32070, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 64h:16m:56s remains)
INFO - root - 2017-12-10 16:27:10.096436: step 32080, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 66h:54m:01s remains)
INFO - root - 2017-12-10 16:27:17.781876: step 32090, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 67h:38m:45s remains)
INFO - root - 2017-12-10 16:27:25.599309: step 32100, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 63h:36m:30s remains)
2017-12-10 16:27:26.438263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.050509844 -0.050442614 -0.047967475 -0.045222934 -0.04266962 -0.040811013 -0.037359953 -0.026688205 -0.0058983304 0.021676878 0.044204354 0.045604464 0.021141339 -0.015882811 -0.049822673][-0.050818652 -0.043505669 -0.032537445 -0.020754 -0.0087358132 0.0028887345 0.017571058 0.042409845 0.079227865 0.12215769 0.15428947 0.15143961 0.10790677 0.045820273 -0.011241877][-0.038017318 -0.019102586 0.0052653924 0.032110665 0.06029975 0.088326722 0.1202381 0.16300477 0.21692668 0.27341917 0.31115803 0.298611 0.22983637 0.13493787 0.04646707][-0.009378518 0.028625337 0.076038174 0.12830178 0.17969325 0.22525609 0.27042329 0.323793 0.38645291 0.446804 0.47957468 0.449225 0.35123447 0.22135203 0.10014548][0.040426362 0.10706326 0.18759377 0.27171314 0.34471062 0.39780024 0.44155392 0.49021706 0.54739833 0.59753615 0.61196804 0.55611038 0.42998549 0.27205679 0.12804389][0.10866535 0.20956846 0.32628572 0.43922198 0.52338773 0.56840348 0.593987 0.62336558 0.66173404 0.68810242 0.6718201 0.58538771 0.43755308 0.2675713 0.12057849][0.18384829 0.31796402 0.46430746 0.59362876 0.67518145 0.70128065 0.70076317 0.7037636 0.713994 0.70612723 0.65136296 0.53479886 0.37624356 0.21375419 0.086459495][0.25069478 0.408561 0.56791484 0.69383353 0.7580232 0.7594716 0.73113781 0.70481253 0.6816386 0.63696915 0.54856777 0.41490933 0.26432422 0.13171659 0.047185428][0.29220846 0.45619416 0.60738575 0.71079236 0.74724126 0.72388613 0.67189324 0.61881924 0.56382632 0.48870361 0.38200039 0.25368389 0.13526244 0.055521015 0.033310015][0.28841886 0.43561515 0.55967665 0.63111186 0.64155066 0.60357046 0.54138619 0.47389409 0.39919657 0.30842203 0.20234236 0.10003315 0.032045916 0.018832155 0.066362247][0.22649187 0.33643147 0.42271188 0.46447864 0.46079275 0.42378569 0.3676542 0.30187634 0.22507639 0.13806628 0.052146152 -0.0087609412 -0.01859366 0.034034181 0.14303562][0.12501203 0.19052625 0.23944125 0.25898927 0.25117925 0.22406468 0.18351562 0.13279581 0.072130717 0.00857693 -0.041601859 -0.054773886 -0.011455907 0.0918249 0.24006447][0.020618195 0.048974369 0.069055282 0.073747285 0.065625869 0.048647273 0.024260461 -0.0067225881 -0.042349536 -0.073354945 -0.083676979 -0.053814884 0.028646884 0.16090587 0.3221066][-0.057415362 -0.052034244 -0.048929505 -0.052283917 -0.059663139 -0.069125049 -0.080685228 -0.094123788 -0.1067659 -0.10986353 -0.090104423 -0.033081096 0.0678131 0.20562248 0.35671842][-0.099466972 -0.10493083 -0.10907921 -0.11516538 -0.12121833 -0.12567678 -0.12842819 -0.12942176 -0.12653336 -0.11342476 -0.080736212 -0.018681316 0.076028153 0.19577646 0.31858489]]...]
INFO - root - 2017-12-10 16:27:34.135220: step 32110, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 67h:02m:33s remains)
INFO - root - 2017-12-10 16:27:42.013045: step 32120, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 64h:52m:14s remains)
INFO - root - 2017-12-10 16:27:49.912631: step 32130, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 66h:21m:48s remains)
INFO - root - 2017-12-10 16:27:57.710643: step 32140, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 64h:52m:51s remains)
INFO - root - 2017-12-10 16:28:05.722795: step 32150, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 66h:04m:30s remains)
INFO - root - 2017-12-10 16:28:13.623873: step 32160, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 65h:53m:18s remains)
INFO - root - 2017-12-10 16:28:21.528138: step 32170, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 64h:41m:05s remains)
INFO - root - 2017-12-10 16:28:29.276124: step 32180, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 64h:23m:18s remains)
INFO - root - 2017-12-10 16:28:36.887799: step 32190, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 63h:35m:03s remains)
INFO - root - 2017-12-10 16:28:44.792489: step 32200, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.796 sec/batch; 66h:24m:50s remains)
2017-12-10 16:28:45.774440: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36394247 0.349554 0.31177983 0.25927854 0.20724791 0.17404892 0.16557698 0.16943863 0.17132431 0.16802818 0.16052248 0.15225233 0.14865549 0.15473658 0.17082456][0.34777054 0.33268335 0.29112709 0.23237565 0.172957 0.13342182 0.11947538 0.11856952 0.11597423 0.10938812 0.097803868 0.085170925 0.07833074 0.083659217 0.10277214][0.30829322 0.30387244 0.27320325 0.22326159 0.16931348 0.13202675 0.1164488 0.11055113 0.1006598 0.087582655 0.068266913 0.046341591 0.030924978 0.029777085 0.047451403][0.27113056 0.29101723 0.28877902 0.26460943 0.23132069 0.2095582 0.20445231 0.20163499 0.18630929 0.16380702 0.12923963 0.087216273 0.051055703 0.032187007 0.040275544][0.25054377 0.30317608 0.34087753 0.35465172 0.35504854 0.36397102 0.38727933 0.40223071 0.38739908 0.35258135 0.29331025 0.21703534 0.14438888 0.095278561 0.086607829][0.24485204 0.32791024 0.40407473 0.45651388 0.49522069 0.54379416 0.60671747 0.64825624 0.63774353 0.58889651 0.49982917 0.38149655 0.26394072 0.17877553 0.15151352][0.23426934 0.33427736 0.43435806 0.51396328 0.5838511 0.66754216 0.76438177 0.82723308 0.82015079 0.76090008 0.65025771 0.49925691 0.34509274 0.23098165 0.19023004][0.19998223 0.29749644 0.40095884 0.49008259 0.57558757 0.67798734 0.78930837 0.85822475 0.85137212 0.78992647 0.67618346 0.51741439 0.35093105 0.22622047 0.18186758][0.14019945 0.21816243 0.30630445 0.38805988 0.47184211 0.57243478 0.6756193 0.73494709 0.726192 0.67125064 0.57248777 0.43140969 0.27942708 0.16488795 0.12710384][0.074657895 0.12435933 0.18606724 0.24941655 0.31949148 0.40388909 0.48457515 0.52572012 0.51407033 0.46892604 0.39245236 0.28278047 0.163301 0.07430847 0.049882822][0.029965578 0.053556409 0.088236608 0.13035929 0.18261002 0.2463643 0.30254492 0.32599095 0.31151396 0.27368036 0.21579285 0.13749042 0.055450313 -0.0021614723 -0.011529332][0.020143269 0.027473092 0.042114619 0.066262826 0.10230742 0.14842273 0.18683328 0.19952863 0.18425691 0.15057789 0.10392869 0.048063088 -0.0045762444 -0.036599126 -0.035293743][0.035815462 0.036733627 0.039840844 0.051280938 0.074968494 0.10870416 0.13703018 0.14596131 0.13304344 0.10355719 0.064126357 0.021598745 -0.014088919 -0.03213013 -0.027742581][0.052040678 0.052129548 0.049953073 0.053093012 0.067582749 0.092265986 0.11405127 0.12201518 0.11369146 0.091027439 0.059183832 0.025483036 -0.0013196336 -0.013594344 -0.010862458][0.048562717 0.049458422 0.04565867 0.043579869 0.05028512 0.066391788 0.08159174 0.088100515 0.084453568 0.070685513 0.048704792 0.023911664 0.0038267537 -0.005563512 -0.005428093]]...]
INFO - root - 2017-12-10 16:28:53.704205: step 32210, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 64h:02m:34s remains)
INFO - root - 2017-12-10 16:29:01.485239: step 32220, loss = 0.67, batch loss = 0.61 (10.0 examples/sec; 0.798 sec/batch; 66h:32m:05s remains)
INFO - root - 2017-12-10 16:29:09.379823: step 32230, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.766 sec/batch; 63h:53m:51s remains)
INFO - root - 2017-12-10 16:29:17.122946: step 32240, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 65h:32m:55s remains)
INFO - root - 2017-12-10 16:29:24.974795: step 32250, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.816 sec/batch; 68h:01m:02s remains)
INFO - root - 2017-12-10 16:29:32.727865: step 32260, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 65h:20m:52s remains)
INFO - root - 2017-12-10 16:29:39.954231: step 32270, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 67h:38m:27s remains)
INFO - root - 2017-12-10 16:29:47.759694: step 32280, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 63h:54m:03s remains)
INFO - root - 2017-12-10 16:29:55.656891: step 32290, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 65h:42m:02s remains)
INFO - root - 2017-12-10 16:30:03.551503: step 32300, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 64h:39m:04s remains)
2017-12-10 16:30:04.452099: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10741915 0.054174 0.036966976 0.051261686 0.081938528 0.11460448 0.13589421 0.13434772 0.10800172 0.063367262 0.010812218 -0.03452713 -0.061097331 -0.061248232 -0.036418632][0.14848676 0.10225939 0.096757963 0.12464352 0.16968586 0.21594232 0.24694097 0.24936052 0.22029491 0.16352217 0.089203574 0.017406631 -0.032742672 -0.049275957 -0.033524696][0.18077086 0.15519486 0.16949946 0.21411818 0.27398363 0.33267787 0.37097678 0.37505496 0.34257904 0.2743459 0.17894313 0.080005288 0.00436311 -0.031255927 -0.027283907][0.21059255 0.21608162 0.25500831 0.31575558 0.386646 0.45240459 0.49203408 0.4925549 0.45361152 0.37412122 0.26066059 0.13855435 0.040066987 -0.013289025 -0.020532327][0.23548183 0.27771595 0.34295687 0.41745132 0.49312502 0.55782562 0.59171975 0.58337003 0.53429425 0.44285226 0.31520581 0.17761603 0.064474255 0.00025428011 -0.0131353][0.24796389 0.32625574 0.41502687 0.49934322 0.57413709 0.631753 0.65575057 0.6359601 0.57485169 0.47218141 0.33558679 0.1921241 0.075331137 0.010053681 -0.0021720428][0.24607262 0.35243118 0.45804688 0.54749507 0.61753505 0.66473496 0.67667156 0.6448068 0.57165724 0.45969686 0.31974816 0.18009523 0.071504109 0.016110078 0.013447785][0.23045141 0.35024026 0.46172324 0.55000842 0.61250925 0.64790362 0.64753711 0.60505438 0.52233058 0.40529928 0.26901814 0.14348504 0.056176 0.023596955 0.041270539][0.20229517 0.31862521 0.42354286 0.5036453 0.55525285 0.57735842 0.56486362 0.51434779 0.42759815 0.31386068 0.191772 0.092085473 0.039382234 0.042755008 0.091650836][0.1650084 0.26451606 0.3518407 0.41612658 0.45207596 0.45844376 0.43353981 0.37748015 0.293257 0.19249858 0.095115758 0.030817956 0.021937486 0.069990881 0.15687877][0.127116 0.20036843 0.26186818 0.30396259 0.32072651 0.31077179 0.27518195 0.2173592 0.14233454 0.062480457 -0.0036312782 -0.029807474 0.0046433411 0.095275708 0.21650656][0.099590153 0.14351654 0.17737216 0.19667915 0.19556406 0.1725992 0.1305926 0.075199679 0.013411324 -0.042936925 -0.078459352 -0.072155319 -0.0051833536 0.11469761 0.2568832][0.083229914 0.10236397 0.11505274 0.11891928 0.10846784 0.081791736 0.041433785 -0.0069356924 -0.0556326 -0.093406536 -0.1079717 -0.083145104 -0.0026411964 0.12540331 0.26973915][0.073878773 0.077952251 0.080709636 0.080830157 0.072815463 0.053625051 0.023620995 -0.013585839 -0.051499419 -0.079451539 -0.08726377 -0.061722733 0.010891302 0.1242389 0.25154355][0.073836915 0.07224188 0.074328966 0.079304039 0.081227511 0.075444624 0.059648614 0.034837328 0.0049374546 -0.020726735 -0.033431262 -0.020993719 0.028450849 0.11134041 0.20864308]]...]
INFO - root - 2017-12-10 16:30:12.289251: step 32310, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 64h:01m:22s remains)
INFO - root - 2017-12-10 16:30:20.270819: step 32320, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 63h:40m:59s remains)
INFO - root - 2017-12-10 16:30:28.177963: step 32330, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.822 sec/batch; 68h:30m:16s remains)
INFO - root - 2017-12-10 16:30:36.071753: step 32340, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.790 sec/batch; 65h:50m:20s remains)
INFO - root - 2017-12-10 16:30:43.637970: step 32350, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 66h:49m:43s remains)
INFO - root - 2017-12-10 16:30:51.250619: step 32360, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 64h:03m:36s remains)
INFO - root - 2017-12-10 16:30:59.082583: step 32370, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 65h:41m:42s remains)
INFO - root - 2017-12-10 16:31:06.977203: step 32380, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 64h:44m:01s remains)
INFO - root - 2017-12-10 16:31:14.840050: step 32390, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 67h:13m:21s remains)
INFO - root - 2017-12-10 16:31:22.652358: step 32400, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 64h:43m:57s remains)
2017-12-10 16:31:23.556692: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24168687 0.23019199 0.20468351 0.1815252 0.17250153 0.17592438 0.18302363 0.17925154 0.15501404 0.12290844 0.10335287 0.11009524 0.14389534 0.19204822 0.24062224][0.25566226 0.24173383 0.21593422 0.19537446 0.19149268 0.2021115 0.21511394 0.21286891 0.18688999 0.15092331 0.12620173 0.12723029 0.15823658 0.2117549 0.27420235][0.25924766 0.2511299 0.23328514 0.22170091 0.22793506 0.25022858 0.2722396 0.27214557 0.24284686 0.19893374 0.1622791 0.15082273 0.17580199 0.23698312 0.31987655][0.27301326 0.27780128 0.27473411 0.27779716 0.29957187 0.3382214 0.37203962 0.37355578 0.33703777 0.27865189 0.22162251 0.19165547 0.21041377 0.2837531 0.39277712][0.29389134 0.31482831 0.32929757 0.35029098 0.39164171 0.45000088 0.49788967 0.50186235 0.45673239 0.38071975 0.30001402 0.25084838 0.26667503 0.35572812 0.49154159][0.3195006 0.35662612 0.38914376 0.43041369 0.49434987 0.57442176 0.63683271 0.64146966 0.58229339 0.48242113 0.37536055 0.30855927 0.32486042 0.42970717 0.58619314][0.34215266 0.39227733 0.44145027 0.50251979 0.58734363 0.68519634 0.756857 0.75629836 0.67639124 0.5486939 0.41667733 0.33632705 0.35412514 0.46947041 0.63485461][0.35024238 0.40684602 0.46885881 0.54756719 0.64921963 0.75706142 0.8287093 0.81682986 0.71506345 0.56314564 0.41221884 0.32104805 0.33523473 0.44763917 0.60386622][0.34749141 0.40212214 0.46800312 0.55473286 0.6606729 0.76322734 0.82171094 0.79363012 0.67551237 0.511832 0.35581625 0.26138264 0.26842237 0.36459845 0.4954209][0.32994166 0.37185207 0.42674458 0.50297719 0.59248662 0.67092144 0.70494521 0.662094 0.54230481 0.38873655 0.24951658 0.16706493 0.1709079 0.24556249 0.34372458][0.30381849 0.32799211 0.36173347 0.41240162 0.46927643 0.51141268 0.51681358 0.46452296 0.35691932 0.23055169 0.12382203 0.065317094 0.072018415 0.12791695 0.19644196][0.25650147 0.26480526 0.27650607 0.29827175 0.32080996 0.32978073 0.31389281 0.26179883 0.17699064 0.086576432 0.017869141 -0.013008889 -0.00015153505 0.04167138 0.08679691][0.19022688 0.18665719 0.1813906 0.18039995 0.17809704 0.16676655 0.14205298 0.098923571 0.04116343 -0.013624211 -0.048245385 -0.055601485 -0.037344825 -0.0063941614 0.02092441][0.12067098 0.10810635 0.091634721 0.076560751 0.061033219 0.042009417 0.018519256 -0.0112438 -0.043919533 -0.069482014 -0.078692749 -0.070073552 -0.049206275 -0.02656495 -0.011301621][0.057103332 0.040240992 0.020052809 0.00056906987 -0.017450972 -0.034475658 -0.050697193 -0.066914693 -0.0806681 -0.086726688 -0.080991492 -0.064555243 -0.043875232 -0.026855029 -0.01806823]]...]
INFO - root - 2017-12-10 16:31:31.374520: step 32410, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 65h:10m:55s remains)
INFO - root - 2017-12-10 16:31:39.244116: step 32420, loss = 0.70, batch loss = 0.65 (9.7 examples/sec; 0.821 sec/batch; 68h:27m:29s remains)
INFO - root - 2017-12-10 16:31:46.878272: step 32430, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 64h:16m:04s remains)
INFO - root - 2017-12-10 16:31:54.707051: step 32440, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 63h:36m:34s remains)
INFO - root - 2017-12-10 16:32:02.418668: step 32450, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 65h:11m:34s remains)
INFO - root - 2017-12-10 16:32:10.290586: step 32460, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 66h:17m:45s remains)
INFO - root - 2017-12-10 16:32:18.157947: step 32470, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 65h:36m:58s remains)
INFO - root - 2017-12-10 16:32:26.050023: step 32480, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 65h:26m:57s remains)
INFO - root - 2017-12-10 16:32:33.881923: step 32490, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.759 sec/batch; 63h:12m:40s remains)
INFO - root - 2017-12-10 16:32:41.829779: step 32500, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 64h:00m:17s remains)
2017-12-10 16:32:42.586571: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.053323928 0.052536208 0.051524349 0.049492504 0.046673279 0.046296503 0.048982661 0.060710896 0.082076237 0.0999731 0.10520189 0.094713 0.073540017 0.044682864 0.012214688][0.050619792 0.058601271 0.064319551 0.065843455 0.06387569 0.063007109 0.065292224 0.0775676 0.10128765 0.12212945 0.12943447 0.11941877 0.096440859 0.063606806 0.025246922][0.059729077 0.07434769 0.085258052 0.089428842 0.088121682 0.087162584 0.090008855 0.10429828 0.1301612 0.15169472 0.15859307 0.14752863 0.12245919 0.085165575 0.040250178][0.080245614 0.098264515 0.1129799 0.11969896 0.11948479 0.11915258 0.1238002 0.1424444 0.17077318 0.19016652 0.19196674 0.17574199 0.14616865 0.10282961 0.050991181][0.10354302 0.1239898 0.14365357 0.15535213 0.15819384 0.15958492 0.16639405 0.18983322 0.22001615 0.23472877 0.2273754 0.20191431 0.16530937 0.11503915 0.056859352][0.12042724 0.14372529 0.17081472 0.19116096 0.19993111 0.20408338 0.21212462 0.23835002 0.26820979 0.27626052 0.25782466 0.22093686 0.1758801 0.11900838 0.056341019][0.12788278 0.15251029 0.18598683 0.21493429 0.22995912 0.23630537 0.24345939 0.26832661 0.29461452 0.2959584 0.26876727 0.2229971 0.17130172 0.1104478 0.046935938][0.12668194 0.14750507 0.18082899 0.21248022 0.22974366 0.23575395 0.23939385 0.25812039 0.27747515 0.27349547 0.2434625 0.19646153 0.14471467 0.086449623 0.028325574][0.11960593 0.13095939 0.15514433 0.1805262 0.1939183 0.19641441 0.19468939 0.20433399 0.21526851 0.20896375 0.18331221 0.14415465 0.10059238 0.052350529 0.0054378971][0.11159492 0.10916179 0.11751904 0.12963258 0.13472064 0.13242242 0.12642983 0.12830609 0.13271867 0.12748912 0.11082369 0.084561072 0.05362343 0.019014321 -0.014521527][0.10466027 0.087421894 0.078085467 0.075268015 0.071879596 0.066868827 0.061085619 0.060958117 0.063986339 0.062682241 0.055209454 0.040628061 0.020492787 -0.002954361 -0.02597424][0.10012398 0.06952104 0.045312408 0.030862836 0.02219164 0.018162034 0.01773607 0.022354204 0.029908316 0.034670968 0.034129512 0.026254416 0.011099458 -0.0075448537 -0.026233122][0.095722891 0.057262868 0.025444493 0.0065539763 -0.0019230534 -0.00092224125 0.0075015966 0.020631818 0.03523631 0.045340966 0.047676239 0.040531076 0.024086045 0.0037736513 -0.016685285][0.090159141 0.051562153 0.021391373 0.0064542792 0.0039524287 0.012912852 0.030661318 0.051522668 0.071060076 0.082734726 0.083222955 0.071908414 0.049900424 0.023894727 -0.0018841706][0.078928716 0.047337059 0.026258906 0.021234319 0.028162213 0.045500252 0.069915071 0.094245873 0.11412029 0.12305839 0.1185355 0.10120089 0.073151112 0.04149184 0.010617985]]...]
INFO - root - 2017-12-10 16:32:50.272042: step 32510, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.811 sec/batch; 67h:32m:45s remains)
INFO - root - 2017-12-10 16:32:58.090313: step 32520, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 66h:39m:40s remains)
INFO - root - 2017-12-10 16:33:05.884152: step 32530, loss = 0.69, batch loss = 0.64 (10.7 examples/sec; 0.749 sec/batch; 62h:26m:52s remains)
INFO - root - 2017-12-10 16:33:13.552556: step 32540, loss = 0.67, batch loss = 0.62 (10.8 examples/sec; 0.744 sec/batch; 61h:59m:27s remains)
INFO - root - 2017-12-10 16:33:21.511659: step 32550, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 65h:53m:42s remains)
INFO - root - 2017-12-10 16:33:29.396036: step 32560, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 66h:47m:39s remains)
INFO - root - 2017-12-10 16:33:37.421823: step 32570, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 65h:03m:16s remains)
INFO - root - 2017-12-10 16:33:45.307350: step 32580, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 65h:28m:00s remains)
INFO - root - 2017-12-10 16:33:52.952309: step 32590, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 62h:52m:31s remains)
INFO - root - 2017-12-10 16:34:00.820409: step 32600, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 65h:19m:36s remains)
2017-12-10 16:34:01.666524: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28532818 0.26857933 0.23915546 0.22442846 0.23667151 0.2776159 0.33617556 0.39189607 0.42227313 0.41633236 0.38038313 0.32353309 0.25277951 0.17828415 0.10998116][0.31273237 0.29676938 0.26501587 0.24847123 0.2611095 0.30323991 0.36370021 0.42436588 0.46384445 0.46883327 0.44101614 0.38275349 0.29861578 0.20269936 0.11644262][0.33182624 0.32333538 0.29442912 0.27717152 0.28701541 0.32464537 0.38150504 0.44323543 0.48900592 0.50253063 0.48016107 0.41850927 0.32124317 0.20868924 0.11296767][0.33973104 0.343789 0.32375258 0.3097848 0.31812122 0.34985518 0.40023544 0.45907012 0.50610662 0.52221614 0.49995682 0.43329826 0.32691836 0.20692219 0.11327752][0.34011158 0.3581081 0.35100555 0.34415418 0.35321736 0.37934217 0.42034042 0.46978822 0.51033163 0.52274793 0.49803659 0.43037203 0.32617292 0.2139582 0.13536631][0.33810085 0.36716944 0.37265888 0.37606314 0.39120913 0.41748178 0.45139638 0.48753843 0.51389319 0.51562637 0.48572195 0.42129987 0.33013794 0.23891558 0.18399662][0.32605827 0.35977224 0.37524432 0.39220712 0.42100057 0.45615351 0.488343 0.50991338 0.51571023 0.50044066 0.46349576 0.4066667 0.33751583 0.27664146 0.24947128][0.30378646 0.33443543 0.35481286 0.38358164 0.42655936 0.47225675 0.50470191 0.51385987 0.501913 0.474495 0.43749139 0.3957864 0.35384518 0.32378927 0.31918681][0.2777074 0.29873011 0.31575751 0.3474409 0.39579013 0.44555447 0.47623518 0.47694567 0.45619416 0.42794698 0.40084544 0.37960187 0.36446026 0.359943 0.36995259][0.25839892 0.26615325 0.27163735 0.29429802 0.33522433 0.37954035 0.40662235 0.40572122 0.38819379 0.37072548 0.35962057 0.35691226 0.35986665 0.36920041 0.38364986][0.24251579 0.2398259 0.23310277 0.24258558 0.27063993 0.30504042 0.32878473 0.33204922 0.32546338 0.32370234 0.32821119 0.33749908 0.34707662 0.35742542 0.36727956][0.22970392 0.22260135 0.20819005 0.20658495 0.22129594 0.24329612 0.26080847 0.26719928 0.27127582 0.28330106 0.29956385 0.31539279 0.32576805 0.33240247 0.33655354][0.22313496 0.21632366 0.19881153 0.18998386 0.19355525 0.20341139 0.21309754 0.22051325 0.23263603 0.25403526 0.27675369 0.29410371 0.30150163 0.30224004 0.30082655][0.2075426 0.20343541 0.18791108 0.1789636 0.1792724 0.18430252 0.19100998 0.20025007 0.2169974 0.2412404 0.26398534 0.2790145 0.28314602 0.28036577 0.276717][0.19046085 0.18737635 0.17497157 0.17037632 0.17409499 0.18138178 0.19085032 0.20393576 0.22255473 0.24435765 0.26329607 0.27623677 0.28106031 0.280326 0.27905008]]...]
INFO - root - 2017-12-10 16:34:09.435866: step 32610, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 65h:12m:39s remains)
INFO - root - 2017-12-10 16:34:17.130415: step 32620, loss = 0.70, batch loss = 0.64 (11.7 examples/sec; 0.685 sec/batch; 57h:02m:33s remains)
INFO - root - 2017-12-10 16:34:24.975128: step 32630, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 63h:37m:10s remains)
INFO - root - 2017-12-10 16:34:32.872784: step 32640, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 67h:12m:01s remains)
INFO - root - 2017-12-10 16:34:40.728594: step 32650, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 65h:12m:43s remains)
INFO - root - 2017-12-10 16:34:48.612487: step 32660, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 67h:15m:37s remains)
INFO - root - 2017-12-10 16:34:56.313631: step 32670, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 65h:56m:02s remains)
INFO - root - 2017-12-10 16:35:04.162938: step 32680, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 64h:13m:00s remains)
INFO - root - 2017-12-10 16:35:12.029787: step 32690, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.756 sec/batch; 62h:59m:53s remains)
INFO - root - 2017-12-10 16:35:19.842473: step 32700, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 66h:33m:34s remains)
2017-12-10 16:35:20.774801: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11544845 0.12402903 0.13210297 0.1372166 0.13876569 0.14067198 0.14508455 0.14921705 0.15225352 0.16196467 0.17970496 0.19891328 0.2157429 0.23213734 0.25125393][0.14494656 0.16117398 0.18180995 0.20434758 0.22482121 0.24508336 0.26583728 0.28343275 0.29726467 0.31520545 0.33884847 0.3628588 0.38799065 0.4171052 0.44661558][0.1627093 0.18362425 0.2148928 0.25400656 0.29419735 0.33615485 0.37954494 0.41889539 0.45092922 0.48101136 0.51067954 0.53676504 0.56658357 0.60426414 0.63775259][0.15991434 0.18572214 0.22691765 0.2806533 0.33926791 0.40351197 0.47202554 0.53589457 0.58594841 0.62297803 0.64980036 0.66835743 0.69428968 0.73329854 0.76579124][0.12868294 0.16105722 0.21379034 0.28378788 0.36376649 0.45307571 0.54663378 0.62976813 0.68696976 0.71677715 0.727765 0.72942758 0.74364763 0.77482706 0.7980181][0.087608188 0.12451635 0.18642005 0.27260512 0.37749141 0.49720711 0.61838931 0.71626145 0.76748848 0.77129459 0.74703479 0.7153067 0.7022875 0.70999432 0.71231467][0.0543436 0.091721058 0.15779017 0.25624785 0.38346297 0.53148037 0.67603081 0.78044707 0.81353945 0.77810979 0.70594263 0.63083643 0.58281171 0.56181753 0.54389232][0.038869355 0.074869789 0.14181134 0.24748255 0.38879427 0.55210936 0.70320028 0.79739022 0.79974723 0.71874672 0.59681922 0.48049453 0.40287942 0.36339712 0.34170729][0.035243973 0.071561694 0.1404539 0.2525211 0.40257046 0.56917518 0.710024 0.77749586 0.74138004 0.61797673 0.45556858 0.30922511 0.21336347 0.16874878 0.15731481][0.036106553 0.073001951 0.14411227 0.26040724 0.41296157 0.572839 0.69323123 0.72755909 0.65635443 0.50400835 0.32034126 0.16223888 0.061856668 0.020739228 0.021752626][0.038558092 0.073479146 0.14276123 0.25573757 0.40116829 0.54680741 0.64635849 0.65575695 0.56408304 0.40065619 0.21416605 0.059234694 -0.036220096 -0.070794024 -0.060858272][0.038278468 0.06851653 0.13068695 0.22980428 0.35464942 0.47488716 0.55096722 0.54470205 0.45033509 0.29762453 0.13074301 -0.0024313738 -0.0808384 -0.10531338 -0.0906804][0.035934571 0.059202231 0.10967209 0.18729384 0.28266478 0.37101358 0.42241409 0.40723798 0.3235774 0.19848773 0.068120591 -0.030599538 -0.085586838 -0.10002071 -0.085389562][0.032396439 0.047739375 0.084156044 0.13865434 0.20478188 0.26500148 0.29850027 0.28446993 0.22359897 0.13802186 0.052959476 -0.00858376 -0.043524325 -0.055879187 -0.051093359][0.02970165 0.038050737 0.061246224 0.095390387 0.13713859 0.176619 0.20090033 0.1978626 0.16974132 0.13231018 0.097494438 0.072582096 0.051461697 0.030388402 0.012795945]]...]
INFO - root - 2017-12-10 16:35:28.626179: step 32710, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 66h:42m:40s remains)
INFO - root - 2017-12-10 16:35:36.447323: step 32720, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 65h:19m:10s remains)
INFO - root - 2017-12-10 16:35:44.243637: step 32730, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 66h:54m:00s remains)
INFO - root - 2017-12-10 16:35:52.130148: step 32740, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 66h:31m:02s remains)
INFO - root - 2017-12-10 16:35:59.793421: step 32750, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 63h:28m:44s remains)
INFO - root - 2017-12-10 16:36:07.610133: step 32760, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 64h:10m:01s remains)
INFO - root - 2017-12-10 16:36:15.589055: step 32770, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 65h:56m:03s remains)
INFO - root - 2017-12-10 16:36:23.415509: step 32780, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 65h:39m:05s remains)
INFO - root - 2017-12-10 16:36:31.330446: step 32790, loss = 0.69, batch loss = 0.64 (10.6 examples/sec; 0.754 sec/batch; 62h:47m:52s remains)
INFO - root - 2017-12-10 16:36:39.120356: step 32800, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 65h:18m:41s remains)
2017-12-10 16:36:40.047563: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22205175 0.24361023 0.29917428 0.36553884 0.4042162 0.39264551 0.33623737 0.2410292 0.11844388 -0.002025303 -0.091517031 -0.13841617 -0.153438 -0.15228352 -0.14524677][0.33514428 0.36175552 0.42640164 0.5024392 0.54327983 0.52686292 0.46095821 0.35448635 0.2196831 0.087071307 -0.015048241 -0.076853976 -0.11127536 -0.13694996 -0.16120966][0.42908713 0.46129277 0.52937913 0.608874 0.65190774 0.64172 0.58630818 0.49413002 0.37502724 0.25397593 0.152696 0.078528993 0.019996636 -0.041139215 -0.10691403][0.49988875 0.535225 0.60171622 0.68230814 0.73366588 0.74605805 0.72137904 0.66033518 0.56899834 0.46768075 0.37206939 0.28529561 0.19993915 0.10207742 -0.0054350588][0.54232925 0.57188892 0.62998563 0.71199358 0.78142643 0.83046991 0.847325 0.81942922 0.75262755 0.66828191 0.57908058 0.48300955 0.37720919 0.25344464 0.11551926][0.5453698 0.55940861 0.60744113 0.697519 0.79659373 0.89035863 0.94957972 0.94829726 0.89592528 0.81843078 0.7295872 0.62372762 0.50162303 0.36091757 0.20602232][0.49379238 0.49762851 0.54097092 0.64296389 0.7706629 0.90029556 0.99044657 1.0055529 0.95938379 0.88180083 0.78929168 0.67526853 0.54205358 0.39376917 0.23696616][0.39475209 0.39395592 0.43463442 0.5400576 0.67852646 0.8205843 0.92223114 0.94413662 0.90241432 0.82690531 0.73555082 0.62262005 0.4907822 0.3501907 0.20912626][0.26542884 0.25964105 0.28984809 0.38125527 0.50670284 0.63649809 0.73120838 0.753349 0.71897489 0.65374553 0.57353306 0.47509903 0.36137739 0.2453807 0.13560751][0.1234993 0.10975421 0.12367095 0.18869708 0.28397182 0.38469592 0.46040365 0.47972488 0.45568496 0.40676191 0.34576163 0.2729291 0.19095692 0.11124227 0.04059099][-0.00060363393 -0.022865281 -0.025118653 0.010947001 0.071296774 0.13800663 0.1902338 0.2044659 0.19000703 0.15879676 0.12095023 0.078515947 0.032143995 -0.010780671 -0.045560028][-0.076411605 -0.1040252 -0.1174413 -0.10412044 -0.072963327 -0.035984676 -0.0070352261 -0.0017238716 -0.012481574 -0.030267458 -0.046957888 -0.061594278 -0.076795243 -0.089221254 -0.096773155][-0.0973194 -0.12727611 -0.14625116 -0.14666635 -0.13453989 -0.11814795 -0.10714023 -0.11046182 -0.12096894 -0.13038108 -0.13250412 -0.12853946 -0.12339556 -0.11651905 -0.10806981][-0.083712205 -0.11254354 -0.13224338 -0.13780443 -0.13435026 -0.12823771 -0.12666458 -0.13418593 -0.14408454 -0.1490678 -0.14457813 -0.13303129 -0.11976798 -0.10537656 -0.091007672][-0.055886429 -0.079777196 -0.0957946 -0.10033447 -0.098102085 -0.094531573 -0.095502704 -0.1031764 -0.11136968 -0.11449916 -0.1092819 -0.097862035 -0.085169323 -0.072063349 -0.059259251]]...]
INFO - root - 2017-12-10 16:36:48.022850: step 32810, loss = 0.70, batch loss = 0.64 (9.1 examples/sec; 0.883 sec/batch; 73h:31m:56s remains)
INFO - root - 2017-12-10 16:36:55.857144: step 32820, loss = 0.68, batch loss = 0.62 (9.7 examples/sec; 0.821 sec/batch; 68h:22m:23s remains)
INFO - root - 2017-12-10 16:37:03.538164: step 32830, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 65h:45m:16s remains)
INFO - root - 2017-12-10 16:37:11.386601: step 32840, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 64h:56m:20s remains)
INFO - root - 2017-12-10 16:37:19.347261: step 32850, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.818 sec/batch; 68h:04m:00s remains)
INFO - root - 2017-12-10 16:37:27.236068: step 32860, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 65h:33m:35s remains)
INFO - root - 2017-12-10 16:37:35.053948: step 32870, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 65h:38m:40s remains)
INFO - root - 2017-12-10 16:37:42.877480: step 32880, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 65h:35m:54s remains)
INFO - root - 2017-12-10 16:37:50.660992: step 32890, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 67h:07m:55s remains)
INFO - root - 2017-12-10 16:37:58.568156: step 32900, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 64h:56m:07s remains)
2017-12-10 16:37:59.423410: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0013316527 -0.00064006617 -0.00052990153 0.0026593166 0.010294217 0.021723298 0.034764033 0.04528074 0.050081097 0.046441447 0.035776611 0.023926107 0.016702197 0.016513042 0.025626421][-0.00031920054 -0.002431761 -0.0017575418 0.0027177355 0.01127165 0.022838863 0.035785541 0.046686564 0.052406497 0.05029868 0.042373154 0.0346833 0.032003257 0.036043771 0.049103007][0.0093150064 0.00919797 0.012463782 0.019857965 0.030341184 0.042931903 0.056942157 0.069636777 0.077382967 0.0770965 0.071269989 0.066330083 0.066360749 0.07237561 0.0866973][0.027252167 0.031004811 0.038504403 0.050352227 0.0645666 0.080321185 0.097700596 0.11394849 0.12429105 0.12442542 0.11776728 0.11216106 0.11201284 0.11815807 0.13261585][0.052995875 0.060415477 0.07102982 0.086216338 0.1039982 0.12341303 0.14474951 0.1643012 0.17577802 0.17385194 0.16338859 0.1544982 0.15285844 0.15893991 0.17373097][0.088226356 0.096828833 0.10690076 0.1225041 0.14260688 0.16519468 0.1894816 0.21010441 0.21979426 0.21330635 0.19708641 0.18355463 0.1794519 0.1847869 0.19870935][0.13287669 0.13925605 0.14480561 0.15783165 0.17785327 0.20124073 0.22531913 0.24306452 0.24743675 0.23485877 0.21288905 0.19462486 0.18677999 0.1891482 0.19894694][0.17912368 0.18203516 0.18237124 0.1920381 0.20980591 0.23025011 0.24949072 0.25990325 0.25573865 0.23594202 0.20918162 0.18736407 0.17544544 0.17294693 0.17625132][0.21478312 0.2140248 0.21073291 0.21783851 0.23238975 0.24771012 0.25974086 0.26111165 0.24808268 0.22291729 0.19427015 0.17073442 0.1546704 0.14574192 0.14116658][0.24111381 0.23592544 0.22872755 0.23201263 0.24164575 0.25065994 0.25559208 0.2495786 0.2302369 0.20300038 0.17495582 0.15037718 0.12936252 0.11225527 0.098803759][0.26274115 0.2545251 0.24255309 0.23975407 0.24211538 0.24425802 0.24421407 0.23544401 0.21540447 0.19036277 0.16463348 0.13850617 0.11098822 0.083908208 0.06005466][0.27723721 0.26717824 0.25015682 0.2399044 0.23370317 0.2296268 0.22826016 0.22326307 0.21000578 0.19258374 0.1718532 0.14529635 0.11145421 0.073623128 0.037503313][0.27641517 0.26442334 0.2433033 0.22702 0.21524782 0.20965905 0.21267277 0.2176287 0.21689598 0.21116924 0.19823091 0.17370462 0.13555519 0.087770738 0.038965676][0.2513856 0.23710395 0.21320546 0.1940778 0.18240862 0.18185191 0.19488981 0.21424344 0.22950791 0.2382091 0.23570055 0.21614982 0.17666505 0.1210905 0.061088491][0.20563681 0.18886988 0.16320072 0.14375587 0.13589133 0.14360011 0.16859902 0.20337059 0.23580159 0.260315 0.2700392 0.2580893 0.22064561 0.16075148 0.092894517]]...]
INFO - root - 2017-12-10 16:38:07.123717: step 32910, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 66h:41m:26s remains)
INFO - root - 2017-12-10 16:38:14.956018: step 32920, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 66h:52m:03s remains)
INFO - root - 2017-12-10 16:38:22.851791: step 32930, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 63h:52m:48s remains)
INFO - root - 2017-12-10 16:38:30.673567: step 32940, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 64h:13m:13s remains)
INFO - root - 2017-12-10 16:38:38.677428: step 32950, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 66h:04m:03s remains)
INFO - root - 2017-12-10 16:38:46.639041: step 32960, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 65h:06m:04s remains)
INFO - root - 2017-12-10 16:38:54.465955: step 32970, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 65h:44m:23s remains)
INFO - root - 2017-12-10 16:39:02.137250: step 32980, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 64h:21m:22s remains)
INFO - root - 2017-12-10 16:39:09.840216: step 32990, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 65h:31m:01s remains)
INFO - root - 2017-12-10 16:39:17.660380: step 33000, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 66h:29m:00s remains)
2017-12-10 16:39:18.518433: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22053263 0.21947888 0.21839894 0.21456602 0.19643803 0.16068231 0.11963421 0.085358538 0.060809735 0.044456609 0.029372312 0.00819981 -0.022816472 -0.055198193 -0.078255937][0.36275959 0.35910597 0.35688052 0.35670096 0.33931735 0.297141 0.24490823 0.20106834 0.16814005 0.14376551 0.12073349 0.0896098 0.042276319 -0.012060509 -0.055895213][0.46464676 0.45586607 0.45439324 0.46518293 0.45800576 0.41953626 0.36509129 0.31806713 0.27874634 0.24517262 0.21355332 0.17408317 0.11258627 0.037055306 -0.027954713][0.50179619 0.48656806 0.48852146 0.51651675 0.52908611 0.50777191 0.46568951 0.42766848 0.38901892 0.3478705 0.30715233 0.25989664 0.1866046 0.091613889 0.0059065097][0.49571437 0.47804397 0.48997879 0.54239309 0.58451611 0.59515679 0.58168423 0.56472439 0.53116548 0.47932437 0.42221498 0.35912624 0.2689144 0.15266627 0.045784533][0.50368667 0.48790237 0.5110544 0.58597541 0.65860057 0.7083655 0.73323077 0.74285746 0.7136265 0.64470339 0.56100166 0.46999556 0.35421512 0.21357347 0.086299516][0.5589301 0.54485905 0.57034558 0.65130204 0.73995721 0.82074469 0.88151509 0.9159506 0.89052153 0.80504912 0.69358611 0.56985146 0.42390454 0.25850165 0.11501946][0.64967394 0.63004339 0.6401332 0.70120215 0.78053242 0.87066668 0.95119154 1.0000302 0.97787654 0.8851155 0.75758219 0.6101976 0.44123003 0.26028666 0.11212476][0.7166425 0.68421465 0.66539836 0.68802553 0.73715764 0.81356758 0.89353037 0.94491005 0.92920315 0.84655511 0.72606373 0.577251 0.40295374 0.22091162 0.0795864][0.69582486 0.65110612 0.60432327 0.58734185 0.60089463 0.65345925 0.72449553 0.77653396 0.77499676 0.71777934 0.62315643 0.49310544 0.33096835 0.16131698 0.034510758][0.571845 0.52324784 0.4619979 0.4193663 0.40732932 0.44270393 0.51409733 0.57910717 0.603131 0.57810354 0.51384658 0.40922531 0.26581785 0.11287631 4.8843387e-05][0.38515303 0.34423408 0.28940642 0.24592426 0.23033662 0.26685077 0.35518265 0.44705793 0.50250536 0.50532281 0.46260685 0.37538406 0.24365151 0.10015126 -0.007208176][0.20972495 0.18637617 0.1539939 0.1293733 0.12766676 0.17780629 0.28870508 0.40741479 0.48808816 0.50754195 0.47401893 0.39152986 0.25991109 0.11569546 0.0062192157][0.10338765 0.099308722 0.091356121 0.089723654 0.10526331 0.16670591 0.2874701 0.41535625 0.50435984 0.528575 0.49665406 0.41227597 0.27609119 0.12858966 0.01758091][0.065578841 0.074147917 0.081037559 0.092890732 0.11597957 0.17404316 0.28021652 0.3900879 0.46536583 0.483233 0.45056143 0.36763453 0.23599055 0.097549081 -0.0018707352]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 16:39:26.371864: step 33010, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 67h:05m:42s remains)
INFO - root - 2017-12-10 16:39:34.212094: step 33020, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 65h:01m:22s remains)
INFO - root - 2017-12-10 16:39:42.133779: step 33030, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 66h:12m:12s remains)
INFO - root - 2017-12-10 16:39:50.074018: step 33040, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 65h:55m:36s remains)
INFO - root - 2017-12-10 16:39:57.997930: step 33050, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 67h:00m:01s remains)
INFO - root - 2017-12-10 16:40:05.836712: step 33060, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 65h:27m:02s remains)
INFO - root - 2017-12-10 16:40:13.496664: step 33070, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 65h:27m:57s remains)
INFO - root - 2017-12-10 16:40:21.351090: step 33080, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 64h:44m:31s remains)
INFO - root - 2017-12-10 16:40:29.202151: step 33090, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 64h:03m:04s remains)
INFO - root - 2017-12-10 16:40:37.033064: step 33100, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 65h:08m:42s remains)
2017-12-10 16:40:37.887270: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1646682 0.18704033 0.19101349 0.17815675 0.156698 0.14119874 0.14486964 0.16355957 0.18633525 0.20385069 0.20814884 0.19942884 0.18291177 0.16779424 0.15904194][0.23300926 0.26534638 0.2738491 0.26003009 0.23509003 0.22022532 0.23158383 0.26088563 0.29186788 0.313706 0.31763032 0.30420944 0.28215492 0.2632685 0.25086546][0.2820245 0.32249272 0.33653879 0.3258355 0.30373472 0.2952562 0.31632429 0.35394064 0.38782534 0.40831465 0.4081845 0.38928378 0.36362928 0.34456521 0.332027][0.30047294 0.345548 0.36503342 0.36152324 0.3489323 0.35192847 0.38271132 0.42407486 0.45416152 0.46698329 0.45916918 0.43475759 0.40847334 0.39344779 0.38506261][0.29339218 0.33855709 0.361649 0.3666839 0.36678573 0.38226485 0.41929981 0.45794833 0.47749677 0.47774082 0.46018922 0.43088213 0.40598178 0.39699948 0.39531523][0.28395861 0.32852978 0.35347992 0.36414632 0.37280273 0.39431337 0.42997861 0.45944929 0.46474659 0.45115712 0.4236545 0.38979682 0.36588952 0.36185598 0.366526][0.28932822 0.33542582 0.36040428 0.37027737 0.37769145 0.39405465 0.41940141 0.43527666 0.42680553 0.40216708 0.36694205 0.32993245 0.30707484 0.30759126 0.31907547][0.31343257 0.36359093 0.38621074 0.38757044 0.38247037 0.38290688 0.39199769 0.39449447 0.3775962 0.3483336 0.30984756 0.27180964 0.25006217 0.25436187 0.27229348][0.34625018 0.40104505 0.41932675 0.40824458 0.38556409 0.366879 0.36022982 0.35352275 0.33486477 0.3070589 0.26945245 0.23162779 0.21069573 0.21807799 0.24178925][0.37823626 0.43585736 0.44919673 0.42667836 0.38863972 0.35463312 0.33639735 0.32422537 0.30667609 0.28236997 0.24776162 0.21142527 0.19234534 0.20407373 0.23447178][0.40778422 0.46867606 0.48075092 0.45405197 0.40918165 0.36715931 0.34071335 0.32218814 0.3021186 0.27782375 0.24553639 0.21223579 0.1977234 0.21687034 0.25620738][0.43360448 0.49875426 0.51473916 0.4926517 0.45097846 0.40867791 0.37614846 0.34754291 0.31734216 0.28641015 0.25414219 0.22594255 0.22016646 0.25053686 0.30124432][0.45610762 0.52633828 0.54987174 0.53787267 0.50551838 0.46831918 0.43254551 0.3936685 0.3504087 0.30994159 0.27635622 0.25405687 0.25837457 0.29969338 0.3602199][0.47318026 0.54538882 0.57447964 0.57175654 0.54957068 0.52048028 0.48651364 0.44308335 0.39125821 0.34385103 0.30896512 0.29115242 0.30257887 0.35002017 0.41489348][0.47168085 0.54077262 0.56940722 0.56977284 0.55305094 0.53083086 0.50281245 0.46331444 0.41289926 0.36621949 0.33247951 0.31686556 0.32907274 0.37442759 0.43597823]]...]
INFO - root - 2017-12-10 16:40:45.830441: step 33110, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.796 sec/batch; 66h:13m:44s remains)
INFO - root - 2017-12-10 16:40:53.774090: step 33120, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 65h:16m:36s remains)
INFO - root - 2017-12-10 16:41:01.581491: step 33130, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 64h:31m:23s remains)
INFO - root - 2017-12-10 16:41:09.366967: step 33140, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 65h:58m:25s remains)
INFO - root - 2017-12-10 16:41:16.944826: step 33150, loss = 0.68, batch loss = 0.63 (12.4 examples/sec; 0.643 sec/batch; 53h:27m:21s remains)
INFO - root - 2017-12-10 16:41:24.779572: step 33160, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 65h:34m:03s remains)
INFO - root - 2017-12-10 16:41:32.577255: step 33170, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 64h:11m:38s remains)
INFO - root - 2017-12-10 16:41:40.494854: step 33180, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 66h:55m:04s remains)
INFO - root - 2017-12-10 16:41:48.366412: step 33190, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 67h:40m:22s remains)
INFO - root - 2017-12-10 16:41:56.201366: step 33200, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 64h:58m:01s remains)
2017-12-10 16:41:57.021768: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.054361243 0.042933237 0.030645365 0.019728197 0.010132873 0.00238651 -0.0054830154 -0.016268251 -0.026380733 -0.027398562 -0.013747388 0.012191612 0.045598838 0.076523259 0.089934185][0.078213841 0.081734896 0.083723731 0.082133323 0.075137869 0.065917425 0.053936608 0.035551071 0.015892517 0.0083984546 0.024199445 0.063896947 0.12205102 0.18265271 0.21798609][0.12233234 0.15250182 0.18169582 0.20042051 0.20360692 0.19775733 0.18359976 0.15517049 0.11904664 0.094963521 0.1015371 0.14528947 0.22344956 0.31508297 0.37758803][0.18065971 0.24889831 0.32100013 0.37845394 0.41094652 0.42763394 0.42554188 0.39055946 0.32910988 0.26935476 0.24056987 0.25964719 0.33326203 0.44294685 0.53408945][0.24038413 0.35087758 0.47425097 0.58409715 0.66456354 0.72519141 0.75201786 0.71674979 0.62466407 0.51512176 0.42975312 0.39714631 0.43891346 0.54694545 0.66186345][0.2885052 0.43186 0.59751183 0.75602341 0.88838434 1.0017998 1.0662869 1.0377092 0.91995353 0.76337069 0.61874038 0.52331752 0.51453596 0.60064334 0.72694212][0.31419832 0.46745598 0.64821571 0.83020818 0.99535346 1.1466743 1.2398492 1.2207164 1.0928899 0.91269535 0.73205149 0.589864 0.53444988 0.59013921 0.71365863][0.319492 0.45154196 0.60823518 0.77298278 0.93413711 1.0910658 1.1927147 1.1840323 1.0674726 0.89689851 0.71639979 0.55945951 0.4777275 0.50633734 0.61477906][0.3197898 0.40344429 0.49970189 0.60642111 0.7233777 0.84922183 0.93512177 0.93312615 0.84339815 0.70855922 0.55935478 0.420599 0.34042019 0.35530353 0.44416329][0.33094424 0.35710287 0.37758818 0.40528986 0.45285764 0.52129126 0.57085913 0.56629872 0.50586456 0.41638309 0.31430155 0.21556842 0.15877265 0.17309752 0.24314627][0.36324164 0.3428033 0.29894507 0.25673041 0.23890455 0.24980921 0.26004878 0.2471569 0.2088355 0.15767957 0.0988546 0.0410886 0.010816372 0.026583025 0.0778638][0.40441763 0.35924333 0.27996823 0.19805424 0.14076518 0.11464619 0.098283269 0.078373574 0.053360056 0.026226265 -0.00465342 -0.035231568 -0.049781848 -0.039023791 -0.0079724425][0.43594456 0.38964218 0.30666545 0.21865745 0.15193434 0.11349043 0.087603264 0.066379443 0.049149066 0.033745088 0.015449272 -0.0035385992 -0.014704858 -0.014471803 -0.002602051][0.45007268 0.41667914 0.34943253 0.27669483 0.2200688 0.18522774 0.16079004 0.14192469 0.12910557 0.11823035 0.10386974 0.088255033 0.076007374 0.068016157 0.067807972][0.44704974 0.42889133 0.38174865 0.33044755 0.29049537 0.2649684 0.24560435 0.22984076 0.21946484 0.21034694 0.19749703 0.18371458 0.17185755 0.16174912 0.15760362]]...]
INFO - root - 2017-12-10 16:42:04.913892: step 33210, loss = 0.67, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 64h:00m:21s remains)
INFO - root - 2017-12-10 16:42:12.668838: step 33220, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 63h:11m:21s remains)
INFO - root - 2017-12-10 16:42:20.450042: step 33230, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.814 sec/batch; 67h:37m:45s remains)
INFO - root - 2017-12-10 16:42:28.116165: step 33240, loss = 0.71, batch loss = 0.65 (9.6 examples/sec; 0.836 sec/batch; 69h:28m:14s remains)
INFO - root - 2017-12-10 16:42:35.882835: step 33250, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 64h:28m:26s remains)
INFO - root - 2017-12-10 16:42:43.767524: step 33260, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.747 sec/batch; 62h:03m:23s remains)
INFO - root - 2017-12-10 16:42:51.595500: step 33270, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.773 sec/batch; 64h:14m:25s remains)
INFO - root - 2017-12-10 16:42:59.472525: step 33280, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.805 sec/batch; 66h:52m:20s remains)
INFO - root - 2017-12-10 16:43:07.281472: step 33290, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 63h:38m:33s remains)
INFO - root - 2017-12-10 16:43:15.175042: step 33300, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 66h:44m:32s remains)
2017-12-10 16:43:16.073726: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19473441 0.16823743 0.13561632 0.12370607 0.14432971 0.18901072 0.2412807 0.28551692 0.30331939 0.27316678 0.19191813 0.092849471 0.01107916 -0.035502743 -0.054751463][0.22028577 0.18784568 0.14872013 0.1351787 0.16047187 0.21523966 0.28137326 0.33970445 0.36663502 0.33607161 0.24388644 0.12712927 0.027759429 -0.030327981 -0.054847181][0.21896766 0.18523115 0.14539835 0.133425 0.16239405 0.22279616 0.29617512 0.3610951 0.39143321 0.36087495 0.26596278 0.1432263 0.036014169 -0.028585812 -0.055865824][0.21398231 0.1839188 0.14772445 0.13785332 0.16743861 0.22818077 0.302621 0.36785787 0.39729273 0.3664676 0.27301675 0.15005794 0.039472323 -0.02963577 -0.059095211][0.20731327 0.18410905 0.15526059 0.14965817 0.17974669 0.23874821 0.3107779 0.37198269 0.3965635 0.36293977 0.27062276 0.14812659 0.035113588 -0.03718349 -0.067062624][0.1849501 0.17206363 0.15600413 0.16050395 0.19491762 0.252067 0.31742448 0.36846858 0.38262975 0.34358284 0.25310618 0.13412736 0.022233613 -0.050596073 -0.0791242][0.17367893 0.16667551 0.16166177 0.17922972 0.22369081 0.28304237 0.33993912 0.37356022 0.36787897 0.31618237 0.22545034 0.11346961 0.0083520357 -0.060976695 -0.087024383][0.19421087 0.18751173 0.18863386 0.21795812 0.27488175 0.3398152 0.38840622 0.40047178 0.36836427 0.29758808 0.20239991 0.096779041 0.00050580216 -0.063437521 -0.087587632][0.22531138 0.22047578 0.22666521 0.2638607 0.32773164 0.39388284 0.43298486 0.42640609 0.3742528 0.29110628 0.19444481 0.0938328 0.002821625 -0.058971126 -0.08378312][0.23572464 0.23656642 0.24959601 0.29075974 0.35334882 0.41351187 0.44245702 0.42540056 0.36732373 0.28499123 0.19281235 0.095982105 0.0065794759 -0.055355076 -0.080993019][0.21603486 0.2263578 0.24676888 0.28663659 0.33928847 0.38606989 0.40386966 0.38330522 0.3314639 0.26182419 0.18149693 0.091815315 0.0060124057 -0.054201663 -0.0790365][0.1824885 0.20237039 0.22964837 0.26549405 0.30426058 0.33430117 0.34029627 0.31783339 0.27487123 0.22048213 0.15492216 0.076304346 -0.0012993393 -0.055681802 -0.077163465][0.14334601 0.17123966 0.20434286 0.23678978 0.26460949 0.28204212 0.27951026 0.25482956 0.21637708 0.17091441 0.11577763 0.048192233 -0.018199753 -0.062798671 -0.077855743][0.11549699 0.14643486 0.18103766 0.20940025 0.22876179 0.23733354 0.22879493 0.20180455 0.1649882 0.12429228 0.07612174 0.017960992 -0.037629135 -0.072140291 -0.080027953][0.113161 0.14024617 0.16849893 0.18845777 0.19893736 0.20104332 0.18988225 0.163171 0.12776151 0.088824838 0.043477837 -0.0086107673 -0.055408884 -0.081088886 -0.082440436]]...]
INFO - root - 2017-12-10 16:43:23.731091: step 33310, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 64h:41m:43s remains)
INFO - root - 2017-12-10 16:43:31.538363: step 33320, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 65h:58m:43s remains)
INFO - root - 2017-12-10 16:43:39.103405: step 33330, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 65h:38m:18s remains)
INFO - root - 2017-12-10 16:43:46.956955: step 33340, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 65h:00m:08s remains)
INFO - root - 2017-12-10 16:43:54.713324: step 33350, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 62h:55m:54s remains)
INFO - root - 2017-12-10 16:44:02.525664: step 33360, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.757 sec/batch; 62h:53m:11s remains)
INFO - root - 2017-12-10 16:44:10.495499: step 33370, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 67h:25m:17s remains)
INFO - root - 2017-12-10 16:44:18.308564: step 33380, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 63h:59m:09s remains)
INFO - root - 2017-12-10 16:44:25.977768: step 33390, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 65h:30m:58s remains)
INFO - root - 2017-12-10 16:44:33.779234: step 33400, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 65h:02m:53s remains)
2017-12-10 16:44:34.621798: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.41734293 0.44074717 0.45286781 0.4501937 0.43029392 0.38551855 0.32542053 0.26979011 0.22235139 0.18558359 0.16962299 0.1819957 0.20909266 0.23943435 0.29106092][0.51518148 0.52845997 0.51877719 0.4913353 0.44927266 0.38833818 0.32053578 0.26778656 0.23051 0.20410052 0.19317448 0.20528689 0.23134691 0.26424757 0.32533586][0.52463949 0.52509683 0.49881023 0.460027 0.41639632 0.36499727 0.31457192 0.28247064 0.2642504 0.24774101 0.23295575 0.23052897 0.24107939 0.26502204 0.32285127][0.45320761 0.44910222 0.42571265 0.40429038 0.39370382 0.38630769 0.37893525 0.37742063 0.37471032 0.35562843 0.31960708 0.28321317 0.25949946 0.25749013 0.29330036][0.34137067 0.353092 0.36158583 0.38835457 0.439837 0.49864721 0.54422623 0.56740361 0.56258416 0.51936269 0.44205472 0.35442284 0.28129989 0.23855507 0.23653734][0.23502994 0.28498483 0.34891868 0.44004983 0.560338 0.68299013 0.77184284 0.804054 0.77844006 0.69507271 0.56584311 0.42039472 0.29290473 0.20444506 0.16112928][0.17050765 0.27114204 0.39565757 0.54414564 0.71378994 0.87292361 0.97778571 0.99928778 0.94238156 0.81729937 0.64156121 0.44821486 0.27725565 0.15338942 0.080530658][0.15996784 0.30438471 0.47207266 0.64963859 0.83027691 0.98291004 1.0669641 1.058217 0.968851 0.81499624 0.61484671 0.40133223 0.21470113 0.080487065 0.002054001][0.19254613 0.36257032 0.54543126 0.71842164 0.87128842 0.9771198 1.0077435 0.95368755 0.83615112 0.67379129 0.48023075 0.28265163 0.1147927 -0.00062498479 -0.059953343][0.25403592 0.43311203 0.60839587 0.75213277 0.8518548 0.88705629 0.84685969 0.7420128 0.60536 0.45643166 0.3000184 0.15122838 0.031332962 -0.043571305 -0.0698887][0.33821714 0.52000457 0.67714655 0.7795307 0.81511736 0.77259451 0.6611653 0.51332533 0.37300622 0.25821859 0.16132551 0.080174349 0.020333894 -0.009871399 -0.0057079014][0.44116214 0.62325662 0.75837153 0.8157177 0.78769606 0.67645615 0.51122689 0.34229836 0.2190319 0.15391722 0.12481652 0.10994077 0.10076096 0.10048742 0.1188046][0.52724928 0.70107973 0.81060672 0.82656479 0.74904239 0.5965659 0.41353604 0.25765747 0.17404373 0.16583639 0.1972356 0.22918995 0.24527872 0.24936928 0.26093403][0.54887342 0.70031387 0.78119946 0.76652962 0.66457462 0.50798529 0.34736636 0.2354477 0.20465492 0.24758302 0.31936091 0.37125781 0.38588405 0.37232217 0.36193135][0.47498748 0.58851576 0.638585 0.60706496 0.50796705 0.38010004 0.26952761 0.21490364 0.23293117 0.30837056 0.39530441 0.44499606 0.4435643 0.40556595 0.37115908]]...]
INFO - root - 2017-12-10 16:44:42.437302: step 33410, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.773 sec/batch; 64h:11m:49s remains)
INFO - root - 2017-12-10 16:44:50.008666: step 33420, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.753 sec/batch; 62h:35m:21s remains)
INFO - root - 2017-12-10 16:44:57.773112: step 33430, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.755 sec/batch; 62h:45m:05s remains)
INFO - root - 2017-12-10 16:45:05.565833: step 33440, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 64h:28m:30s remains)
INFO - root - 2017-12-10 16:45:13.491123: step 33450, loss = 0.71, batch loss = 0.65 (9.6 examples/sec; 0.835 sec/batch; 69h:24m:06s remains)
INFO - root - 2017-12-10 16:45:21.398576: step 33460, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.759 sec/batch; 63h:01m:25s remains)
INFO - root - 2017-12-10 16:45:29.017721: step 33470, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 65h:51m:30s remains)
INFO - root - 2017-12-10 16:45:36.894945: step 33480, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 63h:33m:58s remains)
INFO - root - 2017-12-10 16:45:44.928875: step 33490, loss = 0.69, batch loss = 0.64 (8.8 examples/sec; 0.906 sec/batch; 75h:16m:01s remains)
INFO - root - 2017-12-10 16:45:52.898884: step 33500, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.830 sec/batch; 68h:58m:25s remains)
2017-12-10 16:45:53.667887: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16187915 0.16663565 0.17567746 0.19002162 0.20404612 0.2177999 0.22729905 0.22146474 0.20141321 0.17658171 0.16414033 0.16896489 0.18520378 0.20020393 0.20528249][0.1676733 0.172247 0.18019958 0.19655693 0.21515809 0.23435009 0.24867201 0.24652602 0.2258262 0.1965131 0.17918803 0.18324114 0.20385858 0.22490025 0.23657694][0.16595365 0.16865128 0.17385872 0.19038655 0.21210563 0.23640364 0.25823054 0.26637685 0.25378796 0.22735286 0.20820497 0.209781 0.23028502 0.25310946 0.26907954][0.17424549 0.17185365 0.17068313 0.18335889 0.20458436 0.23156154 0.25983375 0.2791079 0.27788809 0.25812507 0.23958112 0.23705481 0.25260168 0.27120075 0.28703761][0.19517964 0.18296018 0.17027305 0.17388423 0.19063462 0.21816884 0.25207397 0.28211352 0.29310548 0.28249878 0.2675049 0.26188833 0.27028114 0.28037784 0.29087737][0.22293931 0.20150599 0.17598347 0.16750547 0.1765094 0.20240909 0.24104199 0.28120327 0.30464742 0.30526942 0.29713014 0.29166713 0.29381096 0.29349196 0.29363185][0.26118824 0.23242937 0.19585721 0.17427041 0.17252141 0.1932133 0.23396026 0.28193948 0.31636408 0.3283622 0.32852522 0.32551882 0.3226822 0.31142604 0.29736727][0.32056233 0.28446645 0.23632509 0.20005588 0.18388355 0.19419234 0.23113453 0.28094515 0.32192567 0.34345609 0.35241207 0.35433033 0.3500196 0.33130062 0.3040472][0.37904423 0.33678737 0.27765873 0.22801426 0.19796228 0.19651115 0.22518308 0.27081087 0.31348768 0.3425515 0.36062774 0.36969638 0.36766917 0.34599248 0.30880919][0.40556332 0.36110103 0.29669943 0.24041758 0.20282909 0.19367237 0.214188 0.25276086 0.29346886 0.3272796 0.35320443 0.36952084 0.37146524 0.35037667 0.30792806][0.3925474 0.3498421 0.28830582 0.23403549 0.19689295 0.18586257 0.20033799 0.23061708 0.2658349 0.30010194 0.33059126 0.35285476 0.35982975 0.34261239 0.30034465][0.34594911 0.305667 0.25080514 0.20270903 0.16989787 0.16006993 0.17034088 0.19279246 0.22138754 0.25314054 0.28489116 0.31096095 0.32340962 0.31281778 0.27627861][0.27091536 0.23187986 0.18277323 0.14020956 0.11135291 0.10269497 0.10988593 0.12667894 0.15050775 0.17943646 0.21041472 0.23782109 0.25470027 0.25161305 0.22474837][0.16407001 0.12812735 0.08654736 0.050994791 0.027050938 0.020076271 0.025687866 0.039181653 0.060070749 0.08639247 0.11485948 0.14012754 0.1568948 0.15757897 0.13937415][0.046495344 0.017153528 -0.012537512 -0.036967151 -0.05283501 -0.056625158 -0.05195963 -0.04201708 -0.025990155 -0.0049843951 0.017903881 0.037932619 0.051001873 0.052574512 0.041545451]]...]
INFO - root - 2017-12-10 16:46:01.382918: step 33510, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 64h:33m:35s remains)
INFO - root - 2017-12-10 16:46:09.168111: step 33520, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 63h:07m:39s remains)
INFO - root - 2017-12-10 16:46:16.910494: step 33530, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 64h:27m:51s remains)
INFO - root - 2017-12-10 16:46:24.747449: step 33540, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.817 sec/batch; 67h:52m:28s remains)
INFO - root - 2017-12-10 16:46:32.431830: step 33550, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 64h:00m:11s remains)
INFO - root - 2017-12-10 16:46:40.215967: step 33560, loss = 0.70, batch loss = 0.65 (10.7 examples/sec; 0.747 sec/batch; 62h:00m:47s remains)
INFO - root - 2017-12-10 16:46:48.144417: step 33570, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 66h:42m:38s remains)
INFO - root - 2017-12-10 16:46:56.209496: step 33580, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 66h:43m:11s remains)
INFO - root - 2017-12-10 16:47:04.051211: step 33590, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 65h:09m:50s remains)
INFO - root - 2017-12-10 16:47:11.845032: step 33600, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 65h:38m:01s remains)
2017-12-10 16:47:12.732153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029761789 -0.018342149 -0.0052524251 0.01048197 0.027007978 0.04111246 0.0493367 0.047690026 0.034818273 0.011654628 -0.013452948 -0.033582188 -0.046315562 -0.052650262 -0.056225147][-0.004602585 0.012428589 0.031358931 0.055271465 0.081798263 0.10509057 0.11915607 0.11804618 0.10074932 0.068949707 0.034632083 0.0068509858 -0.012250191 -0.024652287 -0.03534944][0.030875219 0.051797129 0.074607588 0.10611226 0.14336292 0.17709291 0.19845606 0.19971223 0.1806574 0.14323819 0.10153113 0.066491395 0.040123604 0.01950575 -0.0011096993][0.068272717 0.090711109 0.11579721 0.15447013 0.20275882 0.24752198 0.27682161 0.28175908 0.26323622 0.22285087 0.17512317 0.13294257 0.098873667 0.06939555 0.038600124][0.102588 0.12473813 0.1508148 0.19469896 0.25164336 0.30574653 0.3423354 0.35128954 0.33445108 0.29337838 0.24227695 0.19553286 0.1563075 0.12044656 0.082337536][0.13665861 0.15948027 0.18588784 0.23201761 0.29308632 0.35224321 0.39264983 0.40280408 0.38585159 0.34464535 0.2935499 0.24736732 0.20931089 0.17332332 0.13330796][0.17234923 0.19533381 0.22005631 0.26479551 0.3258476 0.38602468 0.42634797 0.4342407 0.41422614 0.37161323 0.3218542 0.27993813 0.24894166 0.21993153 0.18447652][0.20345183 0.22479187 0.24502137 0.28530148 0.34329867 0.40060994 0.43602291 0.43696633 0.4103083 0.36558169 0.31967834 0.28690717 0.26859042 0.25246787 0.22728719][0.23792793 0.25674519 0.27117243 0.30575198 0.35913679 0.4108049 0.4376069 0.42835391 0.39300406 0.34589967 0.30516213 0.28314906 0.27816853 0.27533442 0.26165488][0.27762085 0.29475209 0.3040925 0.33355927 0.38230559 0.42767403 0.44552484 0.42648163 0.38290003 0.33299074 0.29556635 0.28079811 0.28423959 0.28970644 0.28454393][0.31260523 0.32909793 0.336901 0.365178 0.41236135 0.45390829 0.46585116 0.43986702 0.38919148 0.33422121 0.29477882 0.28002515 0.28430349 0.29189977 0.29133964][0.32915953 0.34527731 0.35379985 0.38283926 0.429725 0.46961018 0.47935206 0.45111176 0.39690289 0.33726355 0.29218611 0.271351 0.26976678 0.27460295 0.27683616][0.31558707 0.33033475 0.33951479 0.36864018 0.41346136 0.45031753 0.45810398 0.43025446 0.37666693 0.31630483 0.26737112 0.24024472 0.23125853 0.23231459 0.23782723][0.27327698 0.28548071 0.29536331 0.3234427 0.36383423 0.39527676 0.39998811 0.373349 0.32333162 0.26607385 0.21691626 0.18611564 0.17145184 0.16956925 0.17836335][0.19786197 0.20681228 0.21692601 0.24229853 0.27628604 0.30127808 0.30331513 0.27937478 0.23578642 0.18502645 0.13959052 0.10873232 0.091917053 0.089521118 0.10161085]]...]
INFO - root - 2017-12-10 16:47:20.560624: step 33610, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.763 sec/batch; 63h:23m:18s remains)
INFO - root - 2017-12-10 16:47:28.403515: step 33620, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 64h:19m:24s remains)
INFO - root - 2017-12-10 16:47:36.221099: step 33630, loss = 0.70, batch loss = 0.64 (9.5 examples/sec; 0.839 sec/batch; 69h:38m:10s remains)
INFO - root - 2017-12-10 16:47:44.203986: step 33640, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 64h:59m:49s remains)
INFO - root - 2017-12-10 16:47:52.112627: step 33650, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 65h:52m:36s remains)
INFO - root - 2017-12-10 16:48:00.084510: step 33660, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 66h:44m:51s remains)
INFO - root - 2017-12-10 16:48:07.949476: step 33670, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 66h:00m:27s remains)
INFO - root - 2017-12-10 16:48:16.003289: step 33680, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 66h:08m:09s remains)
INFO - root - 2017-12-10 16:48:23.689020: step 33690, loss = 0.69, batch loss = 0.63 (10.7 examples/sec; 0.749 sec/batch; 62h:09m:16s remains)
INFO - root - 2017-12-10 16:48:31.610879: step 33700, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 66h:37m:38s remains)
2017-12-10 16:48:32.344647: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12312873 0.092078924 0.058931172 0.037488755 0.03080322 0.032941118 0.038863022 0.048920922 0.06338051 0.077638477 0.080551691 0.062169719 0.023910902 -0.022519482 -0.063761517][0.26063076 0.21373026 0.16308895 0.13337736 0.12702033 0.13251844 0.14063449 0.15197419 0.1673217 0.18105204 0.17833211 0.14692988 0.089155205 0.02081492 -0.038786065][0.39513198 0.32959422 0.26209837 0.2299426 0.2325197 0.24976164 0.26656219 0.28069928 0.29305556 0.29709241 0.27789974 0.22481559 0.14261755 0.051710948 -0.024457444][0.48978549 0.41383383 0.34098774 0.31686494 0.33596596 0.3693864 0.39627963 0.41076303 0.414362 0.40152064 0.36017531 0.28396028 0.18006505 0.071847096 -0.015208649][0.52665043 0.46149844 0.40460953 0.40250844 0.44425017 0.49504972 0.53228503 0.5474959 0.54409975 0.51738584 0.4571695 0.36050215 0.23560536 0.10893537 0.0083387764][0.52294469 0.48733211 0.46183828 0.48775256 0.55019337 0.61396027 0.66028386 0.67905992 0.67551959 0.64296645 0.57086378 0.45803878 0.31215811 0.16454604 0.045997851][0.48296335 0.48478109 0.495678 0.54749268 0.62489104 0.69729155 0.75361466 0.78211331 0.78712952 0.75696462 0.67850858 0.55198783 0.38549873 0.21717641 0.081393987][0.4110629 0.44557562 0.48772112 0.55821097 0.64343363 0.71989661 0.78534031 0.8267538 0.84584892 0.82454771 0.74633 0.61230165 0.431303 0.24818604 0.10004041][0.30852711 0.36065346 0.41922948 0.49481031 0.57637745 0.647984 0.71446747 0.76385367 0.79518169 0.78688127 0.72016412 0.59537494 0.42009524 0.24064294 0.094732031][0.18415689 0.23390213 0.29030502 0.35662276 0.42447144 0.48366767 0.54211611 0.590326 0.62618357 0.62943715 0.58172196 0.48164424 0.33377126 0.17984 0.054371532][0.058768924 0.090311319 0.12923108 0.17522343 0.2224222 0.26422065 0.30796266 0.34713337 0.37995479 0.39049226 0.36283121 0.29395345 0.18562523 0.071734756 -0.019285653][-0.043952748 -0.034402788 -0.0175441 0.0049375021 0.029322006 0.051273722 0.076076157 0.1005154 0.12405823 0.13637702 0.12507935 0.086222515 0.019867016 -0.049370717 -0.1003623][-0.10561267 -0.11168763 -0.11140693 -0.10702106 -0.10080362 -0.095643073 -0.0883494 -0.079443529 -0.067871995 -0.058736347 -0.060232155 -0.076092429 -0.10704656 -0.13776176 -0.15478022][-0.1280897 -0.14108337 -0.14904693 -0.15452835 -0.15866853 -0.16312277 -0.16621815 -0.16717307 -0.16434057 -0.15926731 -0.1565029 -0.15879594 -0.1668946 -0.17289162 -0.16978781][-0.12484445 -0.13862486 -0.14831361 -0.15681174 -0.16448036 -0.17209277 -0.17888078 -0.18393363 -0.18576418 -0.1842884 -0.18152626 -0.17894657 -0.1768512 -0.17186011 -0.16119076]]...]
INFO - root - 2017-12-10 16:48:39.998784: step 33710, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 64h:50m:32s remains)
INFO - root - 2017-12-10 16:48:47.827875: step 33720, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.792 sec/batch; 65h:46m:00s remains)
INFO - root - 2017-12-10 16:48:55.624441: step 33730, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 63h:53m:52s remains)
INFO - root - 2017-12-10 16:49:03.560973: step 33740, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 65h:07m:46s remains)
INFO - root - 2017-12-10 16:49:11.393573: step 33750, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 66h:41m:47s remains)
INFO - root - 2017-12-10 16:49:19.261858: step 33760, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 65h:09m:27s remains)
INFO - root - 2017-12-10 16:49:27.096589: step 33770, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 63h:39m:45s remains)
INFO - root - 2017-12-10 16:49:34.767776: step 33780, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.812 sec/batch; 67h:22m:39s remains)
INFO - root - 2017-12-10 16:49:42.527730: step 33790, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 65h:59m:19s remains)
INFO - root - 2017-12-10 16:49:50.439138: step 33800, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 65h:50m:32s remains)
2017-12-10 16:49:51.338281: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.067660205 0.081611365 0.10962197 0.15133634 0.20093302 0.24790913 0.2806755 0.29062998 0.27286902 0.22972478 0.17542689 0.12531765 0.08677388 0.059059285 0.038975615][0.088066287 0.10552777 0.14295295 0.20010279 0.26760542 0.32842186 0.3646763 0.36496428 0.32665133 0.25905237 0.18323295 0.1173301 0.069609433 0.03940561 0.021698868][0.10029664 0.12172205 0.16469786 0.23019923 0.31061926 0.38480142 0.42879745 0.42531532 0.3736653 0.28877592 0.19624256 0.11808301 0.063879073 0.033367679 0.01845086][0.10214006 0.12573193 0.17025667 0.23806433 0.32547209 0.4109427 0.46600607 0.46708274 0.41299281 0.32123432 0.22047141 0.13666935 0.080447316 0.051161617 0.037359629][0.09766978 0.12028435 0.16465232 0.23417488 0.32747728 0.42480266 0.49513382 0.50901049 0.46258375 0.37306112 0.27089357 0.18569084 0.12995504 0.10236533 0.088819474][0.096287139 0.11221419 0.15394947 0.22606808 0.32861653 0.44292867 0.53513193 0.56856304 0.53327578 0.44488662 0.336719 0.24410795 0.18486612 0.15899868 0.14939058][0.098715782 0.10298633 0.13831583 0.21224549 0.32621512 0.46098191 0.57760572 0.6310786 0.604185 0.51104885 0.3878243 0.27854884 0.20939742 0.18374948 0.18136579][0.10192703 0.092431217 0.11851919 0.19148222 0.31369019 0.46392366 0.59821463 0.66472131 0.64148945 0.54104489 0.4011845 0.27385163 0.1939593 0.16817129 0.17318746][0.097457543 0.0759542 0.092878588 0.1614158 0.28162059 0.43095094 0.56523633 0.63280684 0.61116785 0.51073641 0.36783686 0.23563196 0.15289432 0.12819575 0.13807341][0.080650195 0.050231211 0.058745228 0.11828502 0.22494613 0.35671094 0.47458851 0.53378177 0.51412529 0.42321455 0.29288033 0.17146227 0.095868655 0.074720308 0.087398671][0.055821087 0.017926918 0.015680349 0.0591552 0.14211808 0.24570948 0.33836931 0.38431111 0.36670372 0.29057074 0.18171166 0.0811536 0.020837683 0.0083382074 0.026143502][0.028136995 -0.012795277 -0.025698541 -0.0020613309 0.052248292 0.12349317 0.18852623 0.22074123 0.2068722 0.14945453 0.06740699 -0.0071906019 -0.048909035 -0.051804487 -0.030078001][0.0011182718 -0.035608049 -0.052815568 -0.045027215 -0.016906522 0.023201162 0.06073565 0.078916155 0.069107443 0.032155439 -0.020464033 -0.067111365 -0.090380184 -0.086248145 -0.065345392][-0.023008958 -0.050819773 -0.066257291 -0.067617178 -0.058421083 -0.04272693 -0.027663156 -0.02135969 -0.027802788 -0.046995815 -0.073522508 -0.095528021 -0.10338177 -0.09530814 -0.078276873][-0.039643317 -0.059143573 -0.0719789 -0.078955375 -0.081805088 -0.081451982 -0.080281511 -0.08127144 -0.085526705 -0.093306139 -0.10259807 -0.1081774 -0.10617561 -0.09675736 -0.083934471]]...]
INFO - root - 2017-12-10 16:49:59.258872: step 33810, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 64h:52m:40s remains)
INFO - root - 2017-12-10 16:50:07.062019: step 33820, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 66h:40m:58s remains)
INFO - root - 2017-12-10 16:50:14.896250: step 33830, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.781 sec/batch; 64h:48m:00s remains)
INFO - root - 2017-12-10 16:50:22.715274: step 33840, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 63h:17m:30s remains)
INFO - root - 2017-12-10 16:50:30.599908: step 33850, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.823 sec/batch; 68h:16m:37s remains)
INFO - root - 2017-12-10 16:50:38.276997: step 33860, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 65h:26m:54s remains)
INFO - root - 2017-12-10 16:50:45.961676: step 33870, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 64h:37m:11s remains)
INFO - root - 2017-12-10 16:50:53.788619: step 33880, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 63h:36m:15s remains)
INFO - root - 2017-12-10 16:51:01.617912: step 33890, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 65h:42m:59s remains)
INFO - root - 2017-12-10 16:51:09.425550: step 33900, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.787 sec/batch; 65h:14m:56s remains)
2017-12-10 16:51:10.274703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043040633 -0.039448325 -0.037429824 -0.037925653 -0.040398352 -0.044402439 -0.050008129 -0.055571154 -0.061298363 -0.067396857 -0.072966926 -0.078498721 -0.084151231 -0.089717276 -0.092979014][-0.027332162 -0.017813893 -0.011011538 -0.0067610955 -0.0037188437 -0.0010684844 -0.0013797503 -0.004129699 -0.011616612 -0.024264764 -0.037195962 -0.050852396 -0.064594135 -0.077783555 -0.087507509][-0.0028467809 0.015935967 0.030985419 0.044453248 0.060993433 0.082761057 0.1008843 0.11070744 0.1040284 0.08117184 0.053459115 0.022291237 -0.0084508657 -0.035814829 -0.05589015][0.032569222 0.065933973 0.095092826 0.12344767 0.16307758 0.21975687 0.274959 0.31198695 0.31144941 0.27379757 0.21907774 0.15417416 0.090893574 0.038523972 0.001611847][0.078602254 0.1354343 0.18843785 0.2388863 0.30908376 0.41048762 0.51471043 0.58810133 0.59587514 0.53676146 0.43962097 0.32223603 0.21093132 0.1246862 0.06779822][0.13047747 0.21806785 0.30386561 0.3823677 0.48353723 0.6245724 0.77190262 0.87777227 0.89244783 0.80905753 0.66180897 0.48354971 0.31999868 0.20009312 0.12527297][0.18493269 0.30901068 0.43551186 0.54561484 0.67006177 0.83075589 0.99689043 1.1168704 1.131422 1.0254141 0.83407384 0.60422868 0.398655 0.2522341 0.16307271][0.23702809 0.39706686 0.56370467 0.70159376 0.83378136 0.98465067 1.1340698 1.2402562 1.2451823 1.1257951 0.91229063 0.65716094 0.4308874 0.26973224 0.17093168][0.27402955 0.45883167 0.65303618 0.80609465 0.92748058 1.0399679 1.1389275 1.2040279 1.1918204 1.0756221 0.8734014 0.63029772 0.41111946 0.25033453 0.14905287][0.285951 0.47503462 0.67259669 0.81945872 0.91109186 0.96621817 0.99600667 1.0058346 0.97580838 0.88060039 0.721124 0.52434152 0.33882004 0.19506304 0.10102314][0.26428986 0.433896 0.60687029 0.72488892 0.77490687 0.77249396 0.74055094 0.70502055 0.66671956 0.60365713 0.50120562 0.3659623 0.22745673 0.11171715 0.0333221][0.19714023 0.32907873 0.45816803 0.535151 0.54583216 0.50472909 0.44013116 0.3860994 0.35318667 0.32214198 0.26986349 0.18933454 0.095739342 0.011352928 -0.04585496][0.1008999 0.18594135 0.26342988 0.29857472 0.28237987 0.22869983 0.16353604 0.11602672 0.095925666 0.087086566 0.066803284 0.024398621 -0.03287971 -0.086814314 -0.12029051][0.0062264064 0.047781322 0.080773726 0.084934756 0.057836015 0.011173369 -0.036328979 -0.066567495 -0.074939549 -0.073361285 -0.078052454 -0.097099222 -0.12701912 -0.1545939 -0.16653419][-0.0684807 -0.056779772 -0.050877273 -0.061008703 -0.085914455 -0.11746212 -0.14500295 -0.16000114 -0.16183275 -0.15778629 -0.15689237 -0.16334048 -0.17493276 -0.18301821 -0.17907718]]...]
INFO - root - 2017-12-10 16:51:18.142583: step 33910, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 66h:48m:29s remains)
INFO - root - 2017-12-10 16:51:26.041999: step 33920, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.788 sec/batch; 65h:20m:58s remains)
INFO - root - 2017-12-10 16:51:33.907992: step 33930, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 64h:20m:37s remains)
INFO - root - 2017-12-10 16:51:41.737994: step 33940, loss = 0.69, batch loss = 0.63 (11.0 examples/sec; 0.729 sec/batch; 60h:29m:41s remains)
INFO - root - 2017-12-10 16:51:49.299093: step 33950, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 64h:42m:58s remains)
INFO - root - 2017-12-10 16:51:57.082222: step 33960, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 64h:06m:36s remains)
INFO - root - 2017-12-10 16:52:05.072291: step 33970, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.766 sec/batch; 63h:28m:45s remains)
INFO - root - 2017-12-10 16:52:12.994386: step 33980, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 65h:02m:00s remains)
INFO - root - 2017-12-10 16:52:20.860560: step 33990, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 64h:21m:28s remains)
INFO - root - 2017-12-10 16:52:28.683146: step 34000, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 65h:05m:16s remains)
2017-12-10 16:52:29.498955: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0017383462 0.0078395577 0.0084840916 0.0035648358 -0.0049248831 -0.012234831 -0.018789539 -0.024016302 -0.023957917 -0.017145852 0.000853735 0.028642263 0.062792771 0.091016844 0.10365523][0.040217273 0.054927532 0.060510423 0.056425683 0.045268472 0.034646768 0.025248487 0.017455408 0.016533842 0.026256748 0.052658033 0.092320368 0.13910885 0.17725101 0.19357106][0.076009028 0.10231836 0.11689167 0.11761411 0.10654549 0.093951344 0.082086682 0.071042225 0.066884845 0.076150529 0.10780493 0.15588319 0.21063156 0.25436389 0.271714][0.10089907 0.13990046 0.16660094 0.17603102 0.16975531 0.15915105 0.14747493 0.13333179 0.12279297 0.12562691 0.1545593 0.20325872 0.25823286 0.30199313 0.31820747][0.11748665 0.17147011 0.21338159 0.23535456 0.23981884 0.23727857 0.22954792 0.21216084 0.19105966 0.18020707 0.19602653 0.23393418 0.27917495 0.31640798 0.32906887][0.13430572 0.20313886 0.25910842 0.29278702 0.31079417 0.32117578 0.32164469 0.30379006 0.27462092 0.25164959 0.25353116 0.27717832 0.30847049 0.33474952 0.33961007][0.15523851 0.23548453 0.29971284 0.33979094 0.36926436 0.39332068 0.40423465 0.3906596 0.36075696 0.33282709 0.32621497 0.33755392 0.35440931 0.36626956 0.35913625][0.17709501 0.26321515 0.32898489 0.36912605 0.40408173 0.43641293 0.45455506 0.44738877 0.42336774 0.39849317 0.39011806 0.3930721 0.39662692 0.39238462 0.37108243][0.19118248 0.27586013 0.33642402 0.37077919 0.40314233 0.43432042 0.45289129 0.45148024 0.43875983 0.42528358 0.42285782 0.42337465 0.41764888 0.40054604 0.36778307][0.18229288 0.25623566 0.30525747 0.32975695 0.35315436 0.37528828 0.38792616 0.389926 0.3899447 0.39171997 0.40003896 0.40401709 0.39580423 0.37355596 0.33695009][0.14117488 0.1967473 0.23042992 0.24387126 0.25579473 0.26550227 0.2693142 0.27188003 0.28123018 0.29554087 0.31411073 0.32472163 0.32030922 0.30121091 0.2694222][0.078069463 0.11342185 0.13286404 0.13758847 0.1398593 0.13911107 0.13536417 0.13598007 0.1469539 0.16420011 0.18486106 0.19810982 0.1981248 0.18634745 0.16527982][0.015310148 0.03367316 0.042999163 0.042741586 0.039321244 0.032567531 0.024499131 0.021957463 0.028498072 0.040480573 0.05567966 0.06681899 0.06968189 0.0657943 0.056727141][-0.03118485 -0.024763061 -0.022002738 -0.024939042 -0.030944033 -0.039611246 -0.048331611 -0.0525423 -0.05086707 -0.045638621 -0.037483234 -0.030206541 -0.026380369 -0.024684461 -0.024595074][-0.057971261 -0.058978014 -0.059835672 -0.063575692 -0.069387548 -0.07663364 -0.083338477 -0.087318577 -0.088490665 -0.088056713 -0.085580811 -0.082485154 -0.07982102 -0.07633207 -0.071853794]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 16:52:37.303947: step 34010, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 65h:22m:05s remains)
INFO - root - 2017-12-10 16:52:45.072816: step 34020, loss = 0.71, batch loss = 0.66 (11.8 examples/sec; 0.676 sec/batch; 56h:02m:22s remains)
INFO - root - 2017-12-10 16:52:52.793424: step 34030, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 63h:55m:03s remains)
INFO - root - 2017-12-10 16:53:00.499141: step 34040, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 66h:48m:14s remains)
INFO - root - 2017-12-10 16:53:08.416090: step 34050, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 65h:31m:38s remains)
INFO - root - 2017-12-10 16:53:16.146503: step 34060, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 63h:36m:13s remains)
INFO - root - 2017-12-10 16:53:23.985922: step 34070, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 63h:22m:44s remains)
INFO - root - 2017-12-10 16:53:31.776979: step 34080, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 66h:09m:55s remains)
INFO - root - 2017-12-10 16:53:39.576323: step 34090, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 63h:36m:38s remains)
INFO - root - 2017-12-10 16:53:47.423348: step 34100, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 64h:16m:36s remains)
2017-12-10 16:53:48.140023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.021755785 -0.022924531 -0.01653433 -0.00077723339 0.019718729 0.037754364 0.047695935 0.047211062 0.037815813 0.023399128 0.00850698 -0.0031274906 -0.0098236511 -0.012302007 -0.011133058][-0.033834804 -0.036298081 -0.031794388 -0.01860052 -0.00079295022 0.015100851 0.023627086 0.022698445 0.013743467 0.00029716446 -0.013202321 -0.022600334 -0.025336202 -0.021446554 -0.011451063][-0.040882654 -0.042445268 -0.037964135 -0.02598457 -0.00959189 0.0055873687 0.014141372 0.01377372 0.0051250076 -0.0091710472 -0.024152858 -0.033262238 -0.031287625 -0.01683764 0.0094173774][-0.034019861 -0.030381173 -0.020241689 -0.0019797345 0.022094656 0.045649491 0.061311938 0.064447686 0.05400715 0.031880748 0.0056752614 -0.012327229 -0.011254561 0.013098298 0.059941918][-0.01089325 0.002288521 0.025134686 0.060688164 0.1067369 0.15330522 0.18700267 0.19731146 0.18072194 0.13981794 0.088143133 0.047202054 0.03639359 0.064382404 0.12985562][0.020042283 0.045323133 0.086549468 0.14945887 0.230572 0.31278187 0.37271297 0.39107895 0.3622345 0.29213241 0.2033318 0.12808567 0.094900407 0.11749886 0.19276237][0.0445982 0.081233464 0.14174382 0.23412497 0.35224783 0.47053444 0.55479777 0.57722306 0.53191543 0.43067476 0.30525357 0.19651757 0.13914019 0.1499801 0.22244498][0.055134829 0.097705416 0.17048065 0.28158072 0.42182571 0.55917174 0.65318543 0.67208928 0.61200339 0.49049094 0.34511593 0.21994129 0.14985222 0.15088455 0.2117523][0.054145083 0.093087353 0.16375753 0.27202106 0.40652791 0.53432435 0.61688149 0.62528509 0.5587399 0.43762049 0.29929173 0.18478471 0.12292666 0.12461612 0.17384201][0.047457546 0.073254 0.12627992 0.20930766 0.31126368 0.40449664 0.45970222 0.45590347 0.39482862 0.29508621 0.18803422 0.10702083 0.071450308 0.084836461 0.1282986][0.038524996 0.046373863 0.072652645 0.11820855 0.1749429 0.2244686 0.2491831 0.2372807 0.19115342 0.12414562 0.058510963 0.018228903 0.013773919 0.042430557 0.085257173][0.029892089 0.022199376 0.023452654 0.0345438 0.05180062 0.066200078 0.069450215 0.056165356 0.027536441 -0.0091096824 -0.039916478 -0.049207538 -0.032350782 0.0050474228 0.046618029][0.023912827 0.0086322473 -0.0052879518 -0.015726656 -0.022071535 -0.026323797 -0.031636529 -0.040953141 -0.055376187 -0.072053522 -0.083037533 -0.078697495 -0.056740507 -0.022243844 0.012725028][0.023892175 0.0098795425 -0.0079149595 -0.026566789 -0.042170633 -0.052589186 -0.058588259 -0.062803566 -0.067950316 -0.073912084 -0.076743148 -0.071191952 -0.055905458 -0.033601522 -0.011379751][0.032387413 0.024579259 0.0093031134 -0.010483109 -0.028967835 -0.041548766 -0.047479723 -0.049135044 -0.049532358 -0.049883276 -0.049052134 -0.045260485 -0.038327642 -0.029033048 -0.020224081]]...]
INFO - root - 2017-12-10 16:53:56.025273: step 34110, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 66h:00m:27s remains)
INFO - root - 2017-12-10 16:54:03.840377: step 34120, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 65h:15m:11s remains)
INFO - root - 2017-12-10 16:54:11.560509: step 34130, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 64h:32m:10s remains)
INFO - root - 2017-12-10 16:54:19.428826: step 34140, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 63h:25m:09s remains)
INFO - root - 2017-12-10 16:54:27.267131: step 34150, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 66h:11m:26s remains)
INFO - root - 2017-12-10 16:54:35.088470: step 34160, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 66h:28m:35s remains)
INFO - root - 2017-12-10 16:54:42.953726: step 34170, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.773 sec/batch; 64h:04m:53s remains)
INFO - root - 2017-12-10 16:54:50.792494: step 34180, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 64h:26m:24s remains)
INFO - root - 2017-12-10 16:54:58.361591: step 34190, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 63h:55m:18s remains)
INFO - root - 2017-12-10 16:55:06.169413: step 34200, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 65h:26m:37s remains)
2017-12-10 16:55:07.071230: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.47880292 0.49267489 0.50918436 0.53325623 0.55300212 0.56145024 0.55021548 0.50975823 0.45783797 0.4178839 0.39331505 0.37557852 0.36313903 0.3587046 0.37606704][0.52435416 0.51942396 0.50964183 0.50656366 0.49921951 0.48329574 0.45868096 0.41709688 0.36931294 0.33345571 0.31161994 0.29678759 0.28678229 0.28369305 0.30143848][0.57141286 0.54652995 0.50896883 0.4778277 0.44404 0.40625942 0.37215066 0.33498544 0.2957516 0.26754588 0.25221252 0.24494366 0.24186952 0.24220218 0.26000357][0.61787355 0.57806987 0.52113634 0.47218835 0.42406985 0.37680659 0.34239918 0.31500551 0.28808552 0.27081117 0.26418886 0.26731139 0.27450174 0.28222394 0.30367744][0.63433087 0.5933075 0.53517342 0.48723656 0.44238675 0.40107846 0.37750953 0.36428925 0.35167775 0.34728849 0.35189274 0.36735028 0.38530156 0.40233076 0.43068877][0.62829649 0.61198246 0.57980216 0.55472928 0.5263344 0.4975217 0.48584312 0.48194656 0.47802609 0.4818925 0.4949244 0.5180198 0.53921014 0.5587557 0.58864743][0.61207151 0.64724535 0.66706473 0.68351895 0.67868829 0.66056997 0.65291947 0.64687562 0.64074796 0.64312768 0.65625811 0.67541736 0.68472987 0.69031924 0.70574969][0.58621383 0.68276834 0.76353639 0.82629788 0.84441262 0.83208025 0.81982917 0.80275446 0.78585744 0.77705085 0.77894521 0.77931982 0.75914651 0.73090458 0.71267474][0.54558015 0.68901 0.8156966 0.91116345 0.94391274 0.93359882 0.91503876 0.88827926 0.86162025 0.84039164 0.82646269 0.80106658 0.74382693 0.672862 0.61218888][0.47269633 0.62847453 0.76800591 0.87011909 0.90483731 0.895021 0.87499934 0.8472144 0.81836063 0.79037881 0.76494473 0.71988136 0.63575333 0.53389937 0.44265679][0.36317289 0.49306044 0.60968041 0.69229215 0.71773225 0.70877254 0.694145 0.67503953 0.65321547 0.62693238 0.5980137 0.54523993 0.45323521 0.34396008 0.24569075][0.21121988 0.29114065 0.3635906 0.41313985 0.42567569 0.42035225 0.41675314 0.41265416 0.40415359 0.38660875 0.36173365 0.31288117 0.23125319 0.13762847 0.056384441][0.039623179 0.069269739 0.09773951 0.11582079 0.11736763 0.11585554 0.12147395 0.12909864 0.13155618 0.12343306 0.10630974 0.0700791 0.012630047 -0.048438549 -0.095222831][-0.10022368 -0.10366026 -0.10331298 -0.10444994 -0.10830501 -0.10706703 -0.097688936 -0.086222976 -0.079731733 -0.08199048 -0.091353245 -0.1119361 -0.14127596 -0.16654997 -0.17660341][-0.16922352 -0.18493558 -0.19461726 -0.20196617 -0.20612366 -0.20374687 -0.19570895 -0.18677944 -0.18147567 -0.18139832 -0.18534562 -0.1936245 -0.20179093 -0.20207712 -0.1879182]]...]
INFO - root - 2017-12-10 16:55:15.006111: step 34210, loss = 0.70, batch loss = 0.64 (9.4 examples/sec; 0.847 sec/batch; 70h:11m:36s remains)
INFO - root - 2017-12-10 16:55:22.752207: step 34220, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 65h:37m:52s remains)
INFO - root - 2017-12-10 16:55:30.529040: step 34230, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 63h:31m:53s remains)
INFO - root - 2017-12-10 16:55:38.401789: step 34240, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 66h:35m:11s remains)
INFO - root - 2017-12-10 16:55:46.281754: step 34250, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.777 sec/batch; 64h:23m:45s remains)
INFO - root - 2017-12-10 16:55:54.095435: step 34260, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 64h:53m:56s remains)
INFO - root - 2017-12-10 16:56:01.703790: step 34270, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 65h:19m:46s remains)
INFO - root - 2017-12-10 16:56:09.671467: step 34280, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 64h:19m:05s remains)
INFO - root - 2017-12-10 16:56:17.497378: step 34290, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 63h:21m:42s remains)
INFO - root - 2017-12-10 16:56:25.327467: step 34300, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 64h:48m:31s remains)
2017-12-10 16:56:26.169362: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0091458783 0.025782263 0.042320006 0.058405772 0.073095277 0.082164414 0.083645634 0.077940241 0.064334512 0.042324845 0.020748673 0.01149284 0.016229806 0.031049591 0.058368534][0.040603831 0.066983439 0.092834152 0.11872341 0.14288667 0.15887742 0.16335288 0.15643951 0.13673022 0.10388723 0.07100296 0.05629319 0.063942514 0.088210762 0.12944703][0.070186242 0.10682615 0.14378361 0.18205909 0.21752132 0.24115479 0.24823824 0.23831205 0.20966566 0.16294175 0.11684109 0.096328162 0.10846002 0.14539418 0.20225629][0.091704957 0.13792422 0.18642591 0.23749293 0.28339678 0.31267178 0.31999689 0.30440986 0.26472268 0.2035901 0.14518641 0.11923035 0.13559145 0.18456526 0.25393376][0.10363528 0.15822516 0.21774614 0.28109026 0.33703929 0.37250489 0.38187924 0.36283961 0.31442416 0.24127546 0.17225233 0.13937037 0.15562296 0.21040983 0.28416872][0.11298032 0.17615537 0.2459448 0.31987068 0.38537553 0.42970932 0.44659519 0.43060982 0.3791821 0.29809809 0.21923035 0.17487381 0.18179035 0.23122577 0.29874861][0.12252869 0.19413885 0.27253 0.35411143 0.42817768 0.48397464 0.5135746 0.50718284 0.45955643 0.37623605 0.28968057 0.23095813 0.22070009 0.25366825 0.30429342][0.13228016 0.21057986 0.2939325 0.37747774 0.45526817 0.51947188 0.56093407 0.56615651 0.52762032 0.44974032 0.36180872 0.29128608 0.26109678 0.2704922 0.29719722][0.14040306 0.22246946 0.30654719 0.38627955 0.46128857 0.52670985 0.57336891 0.58704031 0.55920696 0.492551 0.409605 0.33247641 0.28489155 0.27091682 0.27489737][0.13756146 0.21796882 0.29752466 0.36871204 0.4353044 0.49447817 0.53811175 0.55457461 0.53623891 0.48381346 0.41142568 0.33541965 0.27841133 0.24838392 0.23732647][0.11390063 0.18458788 0.25251502 0.30971789 0.36235103 0.40893686 0.44326717 0.45793194 0.44724971 0.40985769 0.35146922 0.2834883 0.22609508 0.18980885 0.17281827][0.068435393 0.12190198 0.17207572 0.21129234 0.24641174 0.27694485 0.29890877 0.30886781 0.30334002 0.27974969 0.23730537 0.18344884 0.13472234 0.10196889 0.086951755][0.014109761 0.047718883 0.078549542 0.099664986 0.11709481 0.13104279 0.13963819 0.14249426 0.1381464 0.12418043 0.096615724 0.060060028 0.02642983 0.0046995147 -0.0023366434][-0.032271493 -0.01601989 -0.0016352001 0.0050470163 0.0084373439 0.0092192823 0.0069695218 0.0029492895 -0.0028983867 -0.012125859 -0.02859311 -0.049129244 -0.066394441 -0.074841172 -0.072500087][-0.064195648 -0.060921267 -0.058246031 -0.060762689 -0.065646321 -0.071956471 -0.07940615 -0.086452946 -0.092647389 -0.09897802 -0.10780466 -0.11664777 -0.12154998 -0.12015238 -0.11199006]]...]
INFO - root - 2017-12-10 16:56:33.839690: step 34310, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 64h:26m:24s remains)
INFO - root - 2017-12-10 16:56:41.745540: step 34320, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 64h:57m:51s remains)
INFO - root - 2017-12-10 16:56:49.490322: step 34330, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 62h:50m:50s remains)
INFO - root - 2017-12-10 16:56:57.183663: step 34340, loss = 0.69, batch loss = 0.63 (11.9 examples/sec; 0.672 sec/batch; 55h:38m:41s remains)
INFO - root - 2017-12-10 16:57:04.991117: step 34350, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 64h:45m:14s remains)
INFO - root - 2017-12-10 16:57:12.881927: step 34360, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 65h:16m:49s remains)
INFO - root - 2017-12-10 16:57:20.691253: step 34370, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 65h:58m:05s remains)
INFO - root - 2017-12-10 16:57:28.455739: step 34380, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 64h:26m:16s remains)
INFO - root - 2017-12-10 16:57:36.339302: step 34390, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 65h:51m:54s remains)
INFO - root - 2017-12-10 16:57:44.001022: step 34400, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 66h:06m:23s remains)
2017-12-10 16:57:44.930380: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033332005 0.030254465 0.028097283 0.025636228 0.022787489 0.019301392 0.01651524 0.015806848 0.017037049 0.017454954 0.014167265 0.0073611825 0.0013805495 0.0036284172 0.01710129][0.038765252 0.034617465 0.032315578 0.030558946 0.029997919 0.031453725 0.036103357 0.04566801 0.058572397 0.067545526 0.065861486 0.054199304 0.038791869 0.030318987 0.034583222][0.032573 0.032473892 0.036230795 0.042071603 0.050319254 0.062684588 0.079775915 0.10400475 0.13217814 0.15219723 0.15246256 0.1337471 0.10441863 0.07720983 0.062191904][0.0292875 0.038837112 0.054696292 0.0739878 0.095674559 0.12234043 0.15364525 0.19276191 0.23458363 0.26296723 0.26309219 0.23626322 0.19141892 0.14174289 0.10265143][0.040603563 0.066159889 0.10029339 0.137949 0.17561278 0.21691777 0.26064339 0.30939826 0.3565141 0.38418293 0.37793142 0.33949006 0.27767655 0.20616597 0.14482835][0.062078379 0.10707039 0.1627097 0.22074318 0.27400246 0.32712245 0.37820789 0.42721781 0.46677971 0.481451 0.46071205 0.40702376 0.32982239 0.24327216 0.16946125][0.085063018 0.14789774 0.22301739 0.29803082 0.36144617 0.41806203 0.46648067 0.50289035 0.52194393 0.51388383 0.47484207 0.40788454 0.3230783 0.23441073 0.16253911][0.10304458 0.17666894 0.26165387 0.34210616 0.40329239 0.44987577 0.48262241 0.49578071 0.48791817 0.45559847 0.40153629 0.32937148 0.24883023 0.17279421 0.11755899][0.10056896 0.17117311 0.24941908 0.31854573 0.36364236 0.38943502 0.39988434 0.38974074 0.36129436 0.31532273 0.25820112 0.19396138 0.13116294 0.080803841 0.05273141][0.06876494 0.12202947 0.17875169 0.22467756 0.2475158 0.25149715 0.24306032 0.21898383 0.18397081 0.14031695 0.094182946 0.049456619 0.013109017 -0.0068740621 -0.0065435949][0.025842573 0.054011468 0.083301179 0.10451354 0.1087966 0.098456264 0.080359228 0.053787541 0.024187082 -0.0065484424 -0.034623269 -0.056798171 -0.068254128 -0.063643076 -0.042259049][-0.00034932807 0.0038872091 0.0085850181 0.010453992 0.0040378822 -0.011183708 -0.030032314 -0.051206987 -0.070070371 -0.085862644 -0.09729781 -0.10204613 -0.097104609 -0.078411579 -0.046152446][-0.0026337185 -0.015144498 -0.0261643 -0.034800369 -0.043953318 -0.056768816 -0.070859425 -0.084173054 -0.093673848 -0.099113621 -0.10058795 -0.096471496 -0.084865443 -0.062095683 -0.028014084][0.0086271549 -0.010603046 -0.026473332 -0.036888264 -0.043744437 -0.051570479 -0.060076289 -0.067494005 -0.072174393 -0.073832646 -0.072686546 -0.0675006 -0.056933567 -0.036921073 -0.0066401581][0.017598728 0.00047810414 -0.012129673 -0.019079749 -0.022443185 -0.026725594 -0.032133095 -0.0372615 -0.04130701 -0.043786615 -0.044639692 -0.042911455 -0.037464071 -0.023251135 0.0013178712]]...]
INFO - root - 2017-12-10 16:57:52.704704: step 34410, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.784 sec/batch; 64h:54m:52s remains)
INFO - root - 2017-12-10 16:58:00.331568: step 34420, loss = 0.68, batch loss = 0.63 (13.5 examples/sec; 0.593 sec/batch; 49h:05m:15s remains)
INFO - root - 2017-12-10 16:58:08.189951: step 34430, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 64h:46m:31s remains)
INFO - root - 2017-12-10 16:58:16.048515: step 34440, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.773 sec/batch; 63h:59m:08s remains)
INFO - root - 2017-12-10 16:58:23.866970: step 34450, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 63h:53m:04s remains)
INFO - root - 2017-12-10 16:58:31.666103: step 34460, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 65h:45m:25s remains)
INFO - root - 2017-12-10 16:58:39.614133: step 34470, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 64h:07m:10s remains)
INFO - root - 2017-12-10 16:58:47.446757: step 34480, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 64h:46m:34s remains)
INFO - root - 2017-12-10 16:58:55.190030: step 34490, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 62h:32m:26s remains)
INFO - root - 2017-12-10 16:59:03.080816: step 34500, loss = 0.70, batch loss = 0.65 (10.9 examples/sec; 0.736 sec/batch; 60h:55m:52s remains)
2017-12-10 16:59:03.928807: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.070639618 0.084941991 0.09132804 0.084383033 0.06534107 0.042382043 0.028070096 0.039116818 0.078874461 0.14002885 0.20243235 0.24217862 0.24302158 0.20474432 0.14375223][0.03736987 0.047162965 0.049646348 0.041112389 0.023593672 0.0041400837 -0.0064645484 0.0060116043 0.043229327 0.097733133 0.15166867 0.18438557 0.182175 0.14619899 0.092617288][0.012297227 0.016202835 0.014940515 0.0073880865 -0.0038520051 -0.014474818 -0.018440155 -0.0069661438 0.019991307 0.057131626 0.092544161 0.11253098 0.1082757 0.081550114 0.044551477][-0.0034230859 -0.0026663449 -0.0030504076 -0.0032493116 -0.00036808779 0.0058400612 0.014123452 0.025472449 0.038170829 0.050485928 0.058823247 0.058895469 0.048628792 0.029889328 0.00910591][-0.0079102181 -0.0034395391 0.00566577 0.022187913 0.048791971 0.080851212 0.10799251 0.12158454 0.11771329 0.099137254 0.072004914 0.043229539 0.018787967 0.00071010023 -0.010707103][-0.0012338334 0.013803969 0.039601959 0.079192936 0.13436389 0.19553867 0.24333315 0.25943694 0.23793815 0.186601 0.12133957 0.060169276 0.016524164 -0.0075217518 -0.017141247][0.018942231 0.047447737 0.091549039 0.15360731 0.23392542 0.31865558 0.38146248 0.39664441 0.35745579 0.27676582 0.17879768 0.090291157 0.029572275 -0.0017189255 -0.01307441][0.046483919 0.084620036 0.140486 0.21564811 0.30897748 0.40386841 0.47064078 0.48043755 0.42720345 0.3278088 0.21144676 0.10916749 0.040593404 0.0063934023 -0.0055051576][0.08120361 0.12010268 0.17570734 0.24951011 0.33889347 0.42614809 0.48245698 0.481141 0.41886866 0.31494331 0.19940223 0.10207205 0.039615981 0.010621514 0.0022584458][0.1228046 0.15280797 0.19552116 0.25312337 0.32198632 0.38542464 0.41927132 0.40342739 0.33873248 0.24499835 0.14876217 0.07349427 0.029489804 0.012552277 0.010716301][0.15648179 0.17334114 0.1970249 0.23056442 0.27019051 0.30247834 0.31065795 0.28319839 0.22464821 0.15285933 0.087679736 0.043345526 0.022625474 0.018891038 0.022466348][0.16861206 0.17605747 0.18507627 0.19830126 0.21215038 0.21746348 0.2053505 0.17234358 0.12532675 0.07855504 0.044421066 0.028338319 0.026951319 0.032330927 0.03766254][0.153772 0.15880512 0.1622263 0.16490708 0.16325133 0.15213647 0.129258 0.0969787 0.06288141 0.037105981 0.025773432 0.028088132 0.037474759 0.046196539 0.050017864][0.12197801 0.12934802 0.13352169 0.13343091 0.12565336 0.10827888 0.083270587 0.055902071 0.032934748 0.020735601 0.021335235 0.031075057 0.043007188 0.050665796 0.051666789][0.080567978 0.0919145 0.099815048 0.1016139 0.094300352 0.078000866 0.056574833 0.035810091 0.020884659 0.015332896 0.019206855 0.028587393 0.037955116 0.042627908 0.041357439]]...]
INFO - root - 2017-12-10 16:59:11.721313: step 34510, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.751 sec/batch; 62h:09m:10s remains)
INFO - root - 2017-12-10 16:59:19.608619: step 34520, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 64h:38m:58s remains)
INFO - root - 2017-12-10 16:59:27.440142: step 34530, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 63h:11m:37s remains)
INFO - root - 2017-12-10 16:59:35.308427: step 34540, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 64h:11m:54s remains)
INFO - root - 2017-12-10 16:59:43.112287: step 34550, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 65h:29m:13s remains)
INFO - root - 2017-12-10 16:59:51.090464: step 34560, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 66h:40m:06s remains)
INFO - root - 2017-12-10 16:59:58.729674: step 34570, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 64h:00m:38s remains)
INFO - root - 2017-12-10 17:00:06.390785: step 34580, loss = 0.69, batch loss = 0.63 (13.0 examples/sec; 0.614 sec/batch; 50h:48m:58s remains)
INFO - root - 2017-12-10 17:00:14.204533: step 34590, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 66h:08m:37s remains)
INFO - root - 2017-12-10 17:00:22.079251: step 34600, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 65h:29m:28s remains)
2017-12-10 17:00:22.960982: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1301036 0.12284135 0.14007504 0.17971635 0.22928044 0.27241027 0.293182 0.27990398 0.23556453 0.17217053 0.10429244 0.046256088 0.0075471043 -0.014247144 -0.024383035][0.15613277 0.14420941 0.15804106 0.19777888 0.24918389 0.29342243 0.31381464 0.29814112 0.25046125 0.18313979 0.11128352 0.050243769 0.0096055306 -0.01340096 -0.025018003][0.18163443 0.16603623 0.17303672 0.2076477 0.25639907 0.29944915 0.31906772 0.30213472 0.252507 0.18324596 0.11040669 0.0495179 0.0095501291 -0.012938492 -0.024859849][0.20971057 0.19248131 0.19170894 0.21888886 0.26395342 0.30797035 0.33161268 0.31812721 0.26885775 0.19665809 0.11893447 0.053694472 0.010888878 -0.012645097 -0.024932686][0.23174126 0.21426576 0.2056707 0.22330637 0.2625353 0.30761403 0.33777484 0.33213174 0.28760445 0.21522403 0.13296364 0.061797764 0.013786896 -0.012416932 -0.0254891][0.24862055 0.23494969 0.22153841 0.23014072 0.26261079 0.3069087 0.34101564 0.34039873 0.29888135 0.2259071 0.13987105 0.064141512 0.012509766 -0.015120164 -0.028020669][0.27604169 0.27032661 0.25514218 0.25500685 0.2783052 0.31687269 0.34841767 0.34663272 0.30366319 0.22746147 0.13738587 0.058499351 0.0055789761 -0.021248784 -0.032175064][0.30338943 0.30764103 0.29369375 0.28587556 0.29787812 0.32560554 0.34863508 0.34185466 0.29705277 0.22002229 0.12918438 0.049980726 -0.0023804703 -0.027544169 -0.036298744][0.31290907 0.32564947 0.314459 0.30173025 0.30326095 0.31797159 0.32949373 0.3169055 0.27318913 0.20127943 0.11663759 0.042250529 -0.0073852544 -0.030896932 -0.038737733][0.30394387 0.32057449 0.3113718 0.29673126 0.29151458 0.29587862 0.29721069 0.2796413 0.23792766 0.17315999 0.0980943 0.032064468 -0.012126435 -0.032996226 -0.040205706][0.28552225 0.30217981 0.2954424 0.28278062 0.27623346 0.27570087 0.27149495 0.25173983 0.21265794 0.15453328 0.087952979 0.029029069 -0.011057682 -0.030861154 -0.038955104][0.26352391 0.2757875 0.26924694 0.25797918 0.25083268 0.24742799 0.24080327 0.2221902 0.188681 0.13991368 0.083677068 0.032382958 -0.0043173982 -0.024502035 -0.034834228][0.22615455 0.228198 0.21765992 0.20541984 0.19758536 0.1935567 0.18836749 0.17491494 0.15093642 0.11546237 0.073233806 0.032766644 0.0016050072 -0.017849561 -0.029951494][0.17404824 0.16356824 0.14752841 0.13474296 0.12892212 0.12828805 0.12844016 0.12247511 0.10925118 0.087859318 0.060542516 0.032037981 0.0074931034 -0.010579415 -0.024130259][0.12095711 0.09850847 0.076521531 0.064535461 0.064137079 0.071425125 0.080721132 0.08433947 0.0817602 0.071884878 0.055292383 0.035052069 0.014719147 -0.0028788149 -0.017898275]]...]
INFO - root - 2017-12-10 17:00:30.937824: step 34610, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 63h:46m:08s remains)
INFO - root - 2017-12-10 17:00:38.808285: step 34620, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 64h:36m:43s remains)
INFO - root - 2017-12-10 17:00:46.656798: step 34630, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 66h:09m:56s remains)
INFO - root - 2017-12-10 17:00:54.476632: step 34640, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 66h:40m:51s remains)
INFO - root - 2017-12-10 17:01:02.343078: step 34650, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 66h:39m:49s remains)
INFO - root - 2017-12-10 17:01:09.779509: step 34660, loss = 0.69, batch loss = 0.63 (11.1 examples/sec; 0.721 sec/batch; 59h:38m:12s remains)
INFO - root - 2017-12-10 17:01:17.573078: step 34670, loss = 0.67, batch loss = 0.62 (10.4 examples/sec; 0.771 sec/batch; 63h:49m:19s remains)
INFO - root - 2017-12-10 17:01:25.326635: step 34680, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 64h:50m:33s remains)
INFO - root - 2017-12-10 17:01:33.166170: step 34690, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 66h:57m:33s remains)
INFO - root - 2017-12-10 17:01:41.042108: step 34700, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 67h:26m:47s remains)
2017-12-10 17:01:41.868009: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0037535632 0.024177819 0.065083273 0.10951076 0.14849041 0.17913835 0.20359778 0.22208044 0.22983119 0.22882254 0.22460118 0.21983421 0.21243355 0.20058754 0.18681349][0.011759248 0.052558292 0.11253929 0.17934819 0.23963268 0.2856012 0.31837434 0.33767256 0.33749491 0.32478341 0.31142259 0.30397493 0.29783678 0.28988454 0.28476986][0.025441865 0.077965274 0.15530567 0.24257711 0.32282704 0.38281706 0.42193222 0.43900186 0.42755753 0.40053639 0.37681544 0.36704358 0.36373603 0.36489192 0.37901577][0.035414979 0.0979274 0.1895961 0.29314464 0.38878602 0.45989314 0.50476635 0.52136427 0.50256145 0.46618882 0.43607482 0.4257845 0.42616683 0.4379659 0.47441891][0.045502763 0.11863285 0.22443356 0.34274715 0.45174709 0.53415835 0.58785027 0.60961264 0.59021109 0.55025387 0.51612109 0.50319171 0.50345105 0.52068812 0.57318342][0.057510957 0.14133051 0.26058882 0.39232221 0.51366419 0.60950625 0.67733473 0.71158189 0.69882274 0.66006076 0.62336403 0.60490948 0.60065114 0.61626083 0.67402172][0.0686604 0.1610342 0.29073384 0.43204895 0.56211931 0.66998929 0.75180823 0.79893434 0.79511553 0.76164132 0.7262466 0.70282835 0.69132662 0.6975553 0.74672776][0.072822928 0.16641675 0.29622823 0.43596467 0.5642876 0.67506558 0.76343715 0.81943244 0.82630026 0.80432868 0.77752328 0.75434917 0.73623329 0.72809988 0.75848734][0.067635439 0.15401681 0.27232832 0.39798638 0.51177722 0.61117214 0.69202507 0.74648857 0.75993043 0.75054091 0.73594582 0.71787441 0.69711357 0.67737377 0.68918067][0.050553575 0.12281742 0.220805 0.32358494 0.41427693 0.49177408 0.55391032 0.597008 0.6102311 0.60878026 0.60410917 0.59274822 0.57399124 0.55077326 0.55227846][0.021229936 0.073188268 0.14474124 0.21962766 0.2840378 0.33692703 0.37754896 0.40576002 0.41474953 0.41623452 0.41659594 0.41002956 0.39539754 0.37489712 0.37269589][-0.013433374 0.016452326 0.061018594 0.10886164 0.14945132 0.1808397 0.20233017 0.21524388 0.2162728 0.21449549 0.21369828 0.20865856 0.1984264 0.18402001 0.18225251][-0.042109855 -0.031376511 -0.01016316 0.014707801 0.035958659 0.050853938 0.058397755 0.060129374 0.054771129 0.048930209 0.045303311 0.040709488 0.034618828 0.027099226 0.028257249][-0.0575433 -0.059184194 -0.053683192 -0.044599641 -0.036301751 -0.031684544 -0.031903379 -0.035945665 -0.044059351 -0.051841464 -0.057198122 -0.0613002 -0.06439542 -0.0669541 -0.063947067][-0.0611302 -0.068277784 -0.070642993 -0.069987424 -0.068393677 -0.068235323 -0.0708301 -0.07606937 -0.0838546 -0.091295823 -0.0969162 -0.10091455 -0.10289518 -0.10327404 -0.10023751]]...]
INFO - root - 2017-12-10 17:01:49.703097: step 34710, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 65h:55m:59s remains)
INFO - root - 2017-12-10 17:01:57.463879: step 34720, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 64h:51m:12s remains)
INFO - root - 2017-12-10 17:02:05.269116: step 34730, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 64h:24m:43s remains)
INFO - root - 2017-12-10 17:02:12.956997: step 34740, loss = 0.67, batch loss = 0.61 (10.4 examples/sec; 0.766 sec/batch; 63h:21m:13s remains)
INFO - root - 2017-12-10 17:02:20.703316: step 34750, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 66h:13m:52s remains)
INFO - root - 2017-12-10 17:02:28.589256: step 34760, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 64h:44m:33s remains)
INFO - root - 2017-12-10 17:02:36.453021: step 34770, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 65h:27m:31s remains)
INFO - root - 2017-12-10 17:02:44.306675: step 34780, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 66h:04m:35s remains)
INFO - root - 2017-12-10 17:02:52.111816: step 34790, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 64h:17m:00s remains)
INFO - root - 2017-12-10 17:02:59.951461: step 34800, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 65h:48m:07s remains)
2017-12-10 17:03:00.773831: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30225068 0.34713039 0.37045056 0.37350446 0.37207913 0.37395424 0.36476886 0.3440662 0.32142833 0.30913922 0.31044051 0.31834456 0.33660331 0.36287069 0.39103648][0.32048306 0.35303459 0.3644827 0.3665522 0.37611312 0.3940134 0.397284 0.38017178 0.35140306 0.32649654 0.31571719 0.31708705 0.33526686 0.36702964 0.40381795][0.30676022 0.32439885 0.32424638 0.32742813 0.35003608 0.38543642 0.4051486 0.39767534 0.36650357 0.32804865 0.30132237 0.29315576 0.30844215 0.34221438 0.3829473][0.2581802 0.26337713 0.25813755 0.26889572 0.308496 0.36472437 0.40802705 0.41965503 0.39473054 0.34695461 0.3031866 0.27894771 0.28107658 0.30598152 0.34055606][0.1815629 0.1814689 0.18056814 0.20506884 0.26366085 0.34316334 0.41751212 0.45966884 0.45292494 0.40406594 0.34405306 0.29508838 0.26740724 0.26399982 0.27721775][0.10537242 0.10737499 0.11543754 0.15037431 0.21812059 0.31266034 0.41602191 0.49295533 0.51307 0.47315449 0.40336087 0.32805631 0.26092449 0.2158857 0.19729443][0.050437342 0.059904445 0.079993382 0.12257497 0.19180445 0.29283804 0.41706914 0.52254421 0.56657028 0.53692657 0.46117845 0.36321619 0.25910488 0.1735364 0.12377708][0.022663666 0.042422969 0.075950913 0.12500238 0.19083863 0.28864193 0.41868636 0.53687167 0.59379667 0.57220811 0.49627936 0.38632706 0.25834888 0.14471173 0.07252901][0.019657036 0.048328172 0.091332033 0.14192683 0.19832076 0.28180355 0.39987513 0.51203829 0.569853 0.55481172 0.48589587 0.37707913 0.24303013 0.12017472 0.040379316][0.034769267 0.067686759 0.11246821 0.1576248 0.19938463 0.2597332 0.35020643 0.43901613 0.48593214 0.47454408 0.4173201 0.32157418 0.20028257 0.088823862 0.018027166][0.048930775 0.07952439 0.11796343 0.15208451 0.17709757 0.21182951 0.26831037 0.32509372 0.35373622 0.34264195 0.29901606 0.22458464 0.13007177 0.04544263 -0.004654442][0.040209819 0.062898777 0.091151662 0.11450996 0.12830073 0.1454661 0.17486165 0.20329185 0.21337773 0.19920276 0.16614375 0.11389273 0.050543927 -0.0023623696 -0.029015271][0.0081339385 0.021602612 0.039761506 0.054211061 0.060907979 0.066915467 0.077296086 0.085002758 0.081305273 0.064973943 0.0409871 0.0089931777 -0.025959454 -0.051142849 -0.058849521][-0.036975753 -0.032923687 -0.024512809 -0.018424733 -0.017332848 -0.018062938 -0.018343823 -0.021249458 -0.030127686 -0.04410271 -0.059327252 -0.075596027 -0.089925237 -0.096187383 -0.092187025][-0.076337948 -0.079870172 -0.0792343 -0.079323389 -0.081210606 -0.083771124 -0.086529575 -0.090907522 -0.098006792 -0.10679688 -0.11498491 -0.12174405 -0.12492542 -0.12204152 -0.11370247]]...]
INFO - root - 2017-12-10 17:03:08.631224: step 34810, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 64h:22m:34s remains)
INFO - root - 2017-12-10 17:03:16.268798: step 34820, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.788 sec/batch; 65h:11m:56s remains)
INFO - root - 2017-12-10 17:03:24.100014: step 34830, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 63h:20m:53s remains)
INFO - root - 2017-12-10 17:03:31.716467: step 34840, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.768 sec/batch; 63h:31m:04s remains)
INFO - root - 2017-12-10 17:03:39.568962: step 34850, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 63h:28m:42s remains)
INFO - root - 2017-12-10 17:03:47.513086: step 34860, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 65h:45m:41s remains)
INFO - root - 2017-12-10 17:03:55.307558: step 34870, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.804 sec/batch; 66h:26m:21s remains)
INFO - root - 2017-12-10 17:04:03.124846: step 34880, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 62h:47m:39s remains)
INFO - root - 2017-12-10 17:04:11.001673: step 34890, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 66h:28m:10s remains)
INFO - root - 2017-12-10 17:04:18.757339: step 34900, loss = 0.70, batch loss = 0.65 (12.6 examples/sec; 0.637 sec/batch; 52h:40m:01s remains)
2017-12-10 17:04:19.564023: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.075243689 0.065271519 0.063666783 0.077994481 0.096427269 0.10640035 0.10309689 0.087071635 0.068538241 0.055027664 0.043365031 0.031160383 0.014291926 -0.0067706476 -0.028126603][0.17152856 0.1600289 0.1591922 0.18326716 0.21269941 0.22636428 0.21732274 0.1883807 0.15440883 0.12463305 0.096359231 0.06925302 0.040618476 0.011169014 -0.016552186][0.30129072 0.29232746 0.28912938 0.31553707 0.34685865 0.35785073 0.34230891 0.30644423 0.26606551 0.2244108 0.17777471 0.13040493 0.08387845 0.040499978 0.0020510408][0.43521035 0.43381318 0.42698896 0.4473936 0.47038376 0.47485623 0.45862198 0.43014228 0.39859852 0.35369009 0.2894724 0.21662723 0.14490774 0.079624787 0.023586275][0.52965111 0.54143715 0.53618848 0.55072194 0.564007 0.564923 0.55798316 0.55201495 0.54357326 0.50463271 0.42632207 0.32638887 0.22504972 0.13123091 0.050956257][0.56808412 0.5949623 0.5963341 0.60900515 0.6144684 0.61359704 0.61767644 0.63814694 0.65929192 0.63637191 0.55555278 0.43802 0.31301269 0.19176112 0.085066773][0.55097926 0.58569378 0.59181958 0.60370678 0.60388756 0.60205066 0.61387992 0.65550053 0.70451295 0.70524693 0.63698024 0.51898688 0.38456136 0.24524817 0.11746118][0.47804809 0.51115817 0.51879579 0.53114873 0.53075731 0.53097904 0.5480414 0.60115409 0.66781557 0.69070637 0.643864 0.54037094 0.41195253 0.26950222 0.13348536][0.35394919 0.37576181 0.38099453 0.39409384 0.39730117 0.40253037 0.4233354 0.47890604 0.55033994 0.58718693 0.56267846 0.48321968 0.37583342 0.24865596 0.12231275][0.21229377 0.21825367 0.21853562 0.23137821 0.23832548 0.24741989 0.2682786 0.31733659 0.38162285 0.42147556 0.41304612 0.35922012 0.28143373 0.18424612 0.083403826][0.092771232 0.08424177 0.078577 0.087659046 0.093250431 0.10013433 0.11498889 0.15047501 0.19924536 0.2332367 0.23395331 0.20278941 0.15536179 0.093209386 0.025338419][0.0023872738 -0.015055784 -0.025057241 -0.022037111 -0.021311047 -0.02034591 -0.014098725 0.0061035953 0.036946215 0.061113194 0.066149183 0.052417878 0.029674169 -0.0017785559 -0.037892308][-0.060294446 -0.079556905 -0.090527847 -0.091881581 -0.094916075 -0.098533407 -0.09920232 -0.091433264 -0.07583724 -0.061465576 -0.055587627 -0.058743469 -0.065319575 -0.075202189 -0.087404646][-0.091629595 -0.10821638 -0.11763416 -0.12060763 -0.12445684 -0.12921119 -0.13284908 -0.13215128 -0.12627384 -0.11924969 -0.11478412 -0.11343284 -0.11203324 -0.11048245 -0.10963406][-0.099287458 -0.11050656 -0.11599867 -0.11790754 -0.12011351 -0.12300204 -0.12597606 -0.12756775 -0.12660132 -0.12455169 -0.12305749 -0.12203313 -0.11943009 -0.11508061 -0.11055233]]...]
INFO - root - 2017-12-10 17:04:27.429280: step 34910, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 63h:44m:27s remains)
INFO - root - 2017-12-10 17:04:35.281857: step 34920, loss = 0.71, batch loss = 0.66 (10.2 examples/sec; 0.781 sec/batch; 64h:35m:19s remains)
INFO - root - 2017-12-10 17:04:42.832758: step 34930, loss = 0.70, batch loss = 0.64 (10.8 examples/sec; 0.742 sec/batch; 61h:22m:23s remains)
INFO - root - 2017-12-10 17:04:50.648726: step 34940, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 63h:56m:49s remains)
INFO - root - 2017-12-10 17:04:58.463864: step 34950, loss = 0.71, batch loss = 0.66 (9.9 examples/sec; 0.808 sec/batch; 66h:49m:09s remains)
INFO - root - 2017-12-10 17:05:06.279623: step 34960, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 66h:15m:44s remains)
INFO - root - 2017-12-10 17:05:14.091869: step 34970, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 63h:13m:55s remains)
INFO - root - 2017-12-10 17:05:21.649269: step 34980, loss = 0.70, batch loss = 0.64 (12.7 examples/sec; 0.632 sec/batch; 52h:12m:34s remains)
INFO - root - 2017-12-10 17:05:29.514877: step 34990, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 65h:18m:26s remains)
INFO - root - 2017-12-10 17:05:37.326546: step 35000, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.804 sec/batch; 66h:28m:46s remains)
2017-12-10 17:05:38.122977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.091900066 -0.095549069 -0.097403213 -0.096904606 -0.09143772 -0.082047626 -0.070555329 -0.06065359 -0.056058526 -0.060529683 -0.072307453 -0.078516193 -0.064681031 -0.023817087 0.046242975][-0.095110506 -0.097888991 -0.097648472 -0.092177644 -0.0767455 -0.053391267 -0.025612779 -0.00062998396 0.0135985 0.0089666778 -0.013118853 -0.033833452 -0.029060893 0.013845147 0.099269889][-0.095969811 -0.094818443 -0.088156268 -0.071395323 -0.037651736 0.0090110861 0.06211004 0.10897694 0.13607147 0.13014695 0.092178449 0.047180142 0.030104447 0.061559483 0.14944419][-0.097027272 -0.087737456 -0.068375975 -0.032040011 0.029061597 0.10722121 0.19086942 0.26108089 0.29919979 0.28751361 0.22723997 0.14971676 0.10103185 0.11035622 0.18779197][-0.093680345 -0.072242662 -0.034201927 0.028880605 0.1232773 0.23595597 0.348248 0.43560722 0.47623238 0.45029902 0.36128554 0.24717994 0.16399544 0.14662723 0.20312144][-0.079753056 -0.044631761 0.014794602 0.10751747 0.23585494 0.380022 0.51285613 0.605928 0.63714153 0.58773178 0.46697915 0.32000005 0.20899071 0.16864122 0.1990405][-0.051736169 -0.0068583456 0.07116323 0.19155772 0.35034281 0.51924133 0.66118777 0.7456978 0.7537753 0.67326123 0.52378309 0.35679108 0.23414454 0.18187016 0.18684141][-0.0080747223 0.036881618 0.1244943 0.26425722 0.44376224 0.62497723 0.76091397 0.82123321 0.7950412 0.68273991 0.51691175 0.35154998 0.23921648 0.18978338 0.17572866][0.050642457 0.084277704 0.16849712 0.31239489 0.49424151 0.66685045 0.77716637 0.80025655 0.7380963 0.60583413 0.44612557 0.30826974 0.22665314 0.19196112 0.16741662][0.10980847 0.12324448 0.19079725 0.31963542 0.48006356 0.62081778 0.69171518 0.67880511 0.59689993 0.47064167 0.34338456 0.2512762 0.20735613 0.18806249 0.15728046][0.15058331 0.1414655 0.1838385 0.28110591 0.40011752 0.49442661 0.52631897 0.49473962 0.42109823 0.329057 0.25060642 0.20425038 0.18637449 0.17082042 0.13241223][0.16873564 0.14088227 0.15566979 0.21193351 0.27997664 0.32724446 0.33333838 0.30624154 0.26450694 0.22058751 0.18759669 0.16863959 0.15403621 0.126701 0.078161567][0.16670401 0.12992938 0.12242736 0.13959002 0.16166919 0.17387751 0.17146817 0.16479182 0.16221565 0.16021393 0.15412629 0.13941649 0.11022837 0.064196244 0.0084301913][0.1514457 0.11853125 0.10068729 0.09274254 0.087341815 0.084370509 0.088033453 0.1047226 0.12997882 0.14775005 0.14442176 0.11660111 0.067426518 0.0077675288 -0.046649925][0.13276464 0.11316019 0.097637393 0.083015442 0.071905404 0.071649343 0.087307975 0.11763307 0.14848581 0.15997128 0.13991235 0.091840506 0.028071966 -0.033567946 -0.077577531]]...]
INFO - root - 2017-12-10 17:05:46.017701: step 35010, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 65h:35m:16s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 17:05:53.730737: step 35020, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 63h:35m:25s remains)
INFO - root - 2017-12-10 17:06:01.555926: step 35030, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 66h:44m:05s remains)
INFO - root - 2017-12-10 17:06:09.344483: step 35040, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 67h:08m:30s remains)
INFO - root - 2017-12-10 17:06:17.195759: step 35050, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 63h:34m:14s remains)
INFO - root - 2017-12-10 17:06:24.842820: step 35060, loss = 0.71, batch loss = 0.65 (11.1 examples/sec; 0.720 sec/batch; 59h:31m:26s remains)
INFO - root - 2017-12-10 17:06:32.750261: step 35070, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 65h:35m:37s remains)
INFO - root - 2017-12-10 17:06:40.729468: step 35080, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 65h:01m:43s remains)
INFO - root - 2017-12-10 17:06:48.567496: step 35090, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.818 sec/batch; 67h:36m:30s remains)
INFO - root - 2017-12-10 17:06:56.355526: step 35100, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.754 sec/batch; 62h:16m:40s remains)
2017-12-10 17:06:57.081645: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.039226383 0.065237015 0.087308414 0.098210812 0.09280999 0.076608628 0.059186511 0.0452681 0.047484424 0.066221043 0.082791112 0.07845629 0.054001257 0.025248572 -0.0032626784][0.089451052 0.13373691 0.17006601 0.18604682 0.17462143 0.14453486 0.11164629 0.086301215 0.087280035 0.11343817 0.13589394 0.12888542 0.09479671 0.056235231 0.019388249][0.14681856 0.21156488 0.26068386 0.27728835 0.25601819 0.21060665 0.16299778 0.12920889 0.13103132 0.16353793 0.18897483 0.17755771 0.13293943 0.083372273 0.037294816][0.19204812 0.27302158 0.3283098 0.33881918 0.30516 0.24827799 0.19606827 0.16714512 0.1794375 0.21987285 0.24479625 0.22499822 0.16667844 0.1019078 0.042251963][0.21330692 0.30357623 0.35826856 0.35836849 0.31247744 0.25064996 0.20727792 0.20068762 0.23773286 0.29176787 0.31425485 0.28097549 0.20409675 0.11865192 0.040504079][0.20394696 0.29706049 0.35010085 0.34352461 0.29069886 0.23023428 0.20331948 0.22825395 0.29874176 0.37209749 0.39312491 0.34357667 0.24527872 0.1371049 0.039848566][0.17257242 0.26731905 0.32629329 0.32552522 0.27581874 0.2198289 0.20499396 0.25293326 0.35085091 0.44337526 0.46616358 0.40283778 0.28491095 0.1572421 0.045450639][0.13295221 0.23073944 0.30508119 0.32329926 0.28615403 0.23437452 0.22039041 0.27219349 0.38068932 0.48534963 0.51335794 0.44508994 0.31679824 0.17896689 0.060201518][0.1025438 0.20403257 0.29610568 0.33575037 0.31147698 0.25938606 0.23488343 0.27118745 0.37025955 0.47564805 0.51073825 0.44993019 0.32692263 0.19428308 0.079946123][0.091028288 0.193287 0.29518202 0.346913 0.32854164 0.27190688 0.23130453 0.24096324 0.31487006 0.40779 0.44680241 0.40190732 0.29870021 0.18652064 0.088801347][0.0871212 0.18155602 0.27748203 0.32619214 0.30654383 0.24708873 0.19551866 0.18095191 0.22439824 0.2945219 0.32988924 0.30090007 0.22379905 0.14046592 0.067922369][0.0704677 0.14558683 0.21991041 0.25382054 0.23110947 0.17603391 0.1249253 0.098011985 0.11665376 0.16199088 0.18840623 0.17127186 0.11917264 0.064639971 0.018330036][0.032394946 0.08050555 0.12539463 0.14069873 0.11755878 0.074683867 0.035226431 0.0098614506 0.014861003 0.040223766 0.056442995 0.046517964 0.014445017 -0.016557453 -0.040936276][-0.013224452 0.00976308 0.028796777 0.029364981 0.0088015385 -0.020050801 -0.044946667 -0.062090527 -0.062307924 -0.0503698 -0.042692684 -0.049055856 -0.067186676 -0.081912979 -0.09058705][-0.052087706 -0.045553263 -0.041488074 -0.047193851 -0.062343158 -0.079428695 -0.093023568 -0.10242112 -0.10401654 -0.099922232 -0.098055273 -0.10307881 -0.1131962 -0.11935757 -0.11963569]]...]
INFO - root - 2017-12-10 17:07:04.880735: step 35110, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 66h:43m:54s remains)
INFO - root - 2017-12-10 17:07:12.773508: step 35120, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 65h:34m:08s remains)
INFO - root - 2017-12-10 17:07:20.712297: step 35130, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 66h:15m:44s remains)
INFO - root - 2017-12-10 17:07:28.270003: step 35140, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 63h:38m:54s remains)
INFO - root - 2017-12-10 17:07:36.104483: step 35150, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 65h:37m:57s remains)
INFO - root - 2017-12-10 17:07:43.925218: step 35160, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 63h:08m:46s remains)
INFO - root - 2017-12-10 17:07:51.771275: step 35170, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 64h:05m:12s remains)
INFO - root - 2017-12-10 17:07:59.657564: step 35180, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 64h:57m:19s remains)
INFO - root - 2017-12-10 17:08:07.250554: step 35190, loss = 0.69, batch loss = 0.63 (12.6 examples/sec; 0.637 sec/batch; 52h:35m:07s remains)
INFO - root - 2017-12-10 17:08:14.990139: step 35200, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.782 sec/batch; 64h:34m:46s remains)
2017-12-10 17:08:15.798125: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11961207 0.10941892 0.089355193 0.068841785 0.050751716 0.044381745 0.049892686 0.064003512 0.084148347 0.10788536 0.13058786 0.14808843 0.16249862 0.17632572 0.18253846][0.10894057 0.096067354 0.074864447 0.055368815 0.040837307 0.040120561 0.050684277 0.067987889 0.090527177 0.11984085 0.15082128 0.1774261 0.19846074 0.21720041 0.2281598][0.092395425 0.078359485 0.057668675 0.040650632 0.031053549 0.036316983 0.05230565 0.073616929 0.099499457 0.1353009 0.17565964 0.21338826 0.24262528 0.26570976 0.28015742][0.078687638 0.064648531 0.046398271 0.033647057 0.030473772 0.042352285 0.064421766 0.090205684 0.11918474 0.15938745 0.20615995 0.25175697 0.28646034 0.31125772 0.32626522][0.07131955 0.060285009 0.047893941 0.042421632 0.047896452 0.067435965 0.095374487 0.12378638 0.15283042 0.19270158 0.23971309 0.28595346 0.31954452 0.34158227 0.35418049][0.0770586 0.074687622 0.072926 0.078430071 0.094653092 0.12242541 0.15485539 0.18225764 0.20619899 0.2383616 0.27627254 0.31252357 0.33596182 0.34937668 0.35512406][0.095164709 0.10601804 0.11940912 0.1396109 0.16811286 0.2033809 0.23729679 0.25892982 0.27167496 0.28796211 0.30720097 0.32372749 0.32959828 0.32961196 0.32526606][0.11973048 0.1447667 0.17418306 0.20847699 0.2459996 0.28299293 0.31098315 0.31996962 0.31540209 0.31048596 0.30672595 0.30108467 0.28932062 0.27691877 0.26330975][0.13975713 0.17565772 0.21647182 0.25907624 0.29846627 0.32938832 0.34414271 0.33590207 0.31213826 0.28683272 0.26329255 0.24055864 0.21716136 0.19753858 0.17899539][0.14288907 0.18041639 0.22146322 0.26174453 0.29458529 0.31480268 0.316238 0.29551235 0.26063636 0.2250402 0.19266325 0.16352482 0.13757513 0.11756378 0.0994989][0.12775967 0.15922719 0.191262 0.22131808 0.24294412 0.25220636 0.24502461 0.2207842 0.18611518 0.15200435 0.12215037 0.09690471 0.076836154 0.063217178 0.051680256][0.1059639 0.12846766 0.14852858 0.16655363 0.17717381 0.17767283 0.16642174 0.14526004 0.11875324 0.0946491 0.075948916 0.06287393 0.055584628 0.05339567 0.052147049][0.094775915 0.10939621 0.11874329 0.12627566 0.12816456 0.12303078 0.11162104 0.097385153 0.082229622 0.070766523 0.065551683 0.06656272 0.072690696 0.08094044 0.087973118][0.10372953 0.11482081 0.11751017 0.11802143 0.11389799 0.10563016 0.096251279 0.089988522 0.085545637 0.084466569 0.088995837 0.098833427 0.11170574 0.12310323 0.13164563][0.12764947 0.14037113 0.14087264 0.13757414 0.12886997 0.11742455 0.10834945 0.10641655 0.10771938 0.11157982 0.12010571 0.13271677 0.14626728 0.15547374 0.16055544]]...]
INFO - root - 2017-12-10 17:08:23.677662: step 35210, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 66h:04m:57s remains)
INFO - root - 2017-12-10 17:08:31.438466: step 35220, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 65h:51m:47s remains)
INFO - root - 2017-12-10 17:08:39.190268: step 35230, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 62h:23m:07s remains)
INFO - root - 2017-12-10 17:08:47.000349: step 35240, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 64h:42m:25s remains)
INFO - root - 2017-12-10 17:08:54.825174: step 35250, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 65h:55m:30s remains)
INFO - root - 2017-12-10 17:09:02.655206: step 35260, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.799 sec/batch; 65h:56m:49s remains)
INFO - root - 2017-12-10 17:09:10.498542: step 35270, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 63h:39m:42s remains)
INFO - root - 2017-12-10 17:09:18.240896: step 35280, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 64h:13m:45s remains)
INFO - root - 2017-12-10 17:09:25.977078: step 35290, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.773 sec/batch; 63h:46m:58s remains)
INFO - root - 2017-12-10 17:09:33.696311: step 35300, loss = 0.70, batch loss = 0.64 (11.6 examples/sec; 0.689 sec/batch; 56h:53m:12s remains)
2017-12-10 17:09:34.524247: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11210897 0.12158967 0.14186491 0.16294557 0.17866696 0.189737 0.20490628 0.23342435 0.26897326 0.29674038 0.29244411 0.24288562 0.15499225 0.048722032 -0.043224838][0.1308516 0.13810122 0.15493497 0.17353477 0.18856931 0.20106193 0.22209157 0.25923988 0.30206856 0.33046731 0.32079232 0.26240882 0.16483167 0.049929764 -0.047451861][0.15377972 0.16251113 0.17862445 0.19687253 0.21236372 0.22606812 0.25218317 0.2965835 0.34569803 0.37541282 0.36115485 0.29433417 0.1861988 0.061070122 -0.043750722][0.18911411 0.20361057 0.22336288 0.2450847 0.2630657 0.27739564 0.305347 0.35231435 0.40348902 0.43161654 0.41160634 0.33647221 0.21760736 0.081153087 -0.033003315][0.23224056 0.25868574 0.28907111 0.32071957 0.3452765 0.36039943 0.3856827 0.42765802 0.47275254 0.49338669 0.46506336 0.38193494 0.25409982 0.10787182 -0.015347611][0.27349353 0.31573758 0.36088502 0.40472415 0.43563923 0.44862336 0.46509638 0.49407688 0.52536076 0.53389686 0.49624145 0.40748739 0.2757023 0.12579288 -0.001476349][0.30279994 0.36053848 0.42022189 0.47350088 0.50568229 0.51049459 0.51184994 0.52199292 0.53487968 0.52922827 0.4831855 0.39265573 0.2635757 0.11834446 -0.0046667177][0.30695611 0.37267715 0.44071379 0.49782577 0.52605373 0.51885229 0.50313944 0.49429891 0.49021125 0.4734579 0.42425212 0.33857271 0.21964408 0.087172844 -0.023588082][0.28341341 0.34202591 0.40522707 0.45649049 0.47513589 0.45485818 0.42420143 0.40100411 0.38603693 0.36537647 0.32139108 0.24911125 0.14881687 0.037089348 -0.054500416][0.23683508 0.27082717 0.31382623 0.34976807 0.35687509 0.32842362 0.29093111 0.26199153 0.24348679 0.22489274 0.19100085 0.13622269 0.058955468 -0.027239049 -0.095397979][0.18192193 0.17881371 0.19011207 0.20516486 0.2033518 0.17624667 0.14392978 0.11976306 0.10436348 0.089959972 0.06508673 0.025194157 -0.031456996 -0.093613245 -0.13920358][0.13326041 0.094103977 0.074142374 0.070983142 0.066837929 0.05092888 0.034625724 0.024644021 0.018082665 0.0088568255 -0.010317562 -0.041792009 -0.08619234 -0.13336854 -0.16496038][0.092877313 0.032436892 -0.0058704359 -0.01743174 -0.01642862 -0.01545451 -0.01117711 -0.0028082568 0.0027673761 8.6668973e-05 -0.015086166 -0.043269612 -0.083940893 -0.12724964 -0.15689686][0.058291905 -0.0023606797 -0.04111645 -0.050673947 -0.041343596 -0.025580585 -0.0046967436 0.018778132 0.035353307 0.03955764 0.028526323 0.0019619027 -0.039296206 -0.085653335 -0.12167423][0.023804445 -0.020886829 -0.047173943 -0.049029078 -0.032689784 -0.0084087411 0.021254661 0.053114772 0.077095307 0.087721884 0.082110114 0.058715761 0.017655877 -0.032543823 -0.076893128]]...]
INFO - root - 2017-12-10 17:09:42.435522: step 35310, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 64h:30m:47s remains)
INFO - root - 2017-12-10 17:09:50.199931: step 35320, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 63h:40m:09s remains)
INFO - root - 2017-12-10 17:09:58.071724: step 35330, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 64h:00m:52s remains)
INFO - root - 2017-12-10 17:10:05.930602: step 35340, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 63h:49m:26s remains)
INFO - root - 2017-12-10 17:10:13.637369: step 35350, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.812 sec/batch; 67h:03m:09s remains)
INFO - root - 2017-12-10 17:10:21.496709: step 35360, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 64h:50m:09s remains)
INFO - root - 2017-12-10 17:10:29.159895: step 35370, loss = 0.67, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 63h:59m:26s remains)
INFO - root - 2017-12-10 17:10:36.897737: step 35380, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 63h:24m:56s remains)
INFO - root - 2017-12-10 17:10:44.784234: step 35390, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 64h:37m:37s remains)
INFO - root - 2017-12-10 17:10:52.645933: step 35400, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 64h:20m:23s remains)
2017-12-10 17:10:53.451961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.020527292 0.017070916 0.066283152 0.11777483 0.16073705 0.19294016 0.21632887 0.23120075 0.23663044 0.23992875 0.25028855 0.26523921 0.26663241 0.24131697 0.19521986][-0.0036743472 0.046226971 0.11234989 0.18264283 0.2431066 0.29024476 0.32399741 0.34250844 0.34587777 0.34524453 0.35147023 0.36297706 0.3616946 0.3336941 0.2818478][0.008834633 0.067364275 0.1469557 0.23310645 0.30950251 0.3704991 0.41376722 0.43569642 0.43770659 0.43315884 0.43168369 0.4327209 0.42217189 0.38989294 0.33706993][0.014217469 0.078015119 0.16809411 0.26820621 0.36095947 0.43765661 0.49231675 0.51762503 0.51555556 0.50193822 0.4868876 0.47277927 0.45137745 0.41616035 0.36577663][0.01473713 0.082253173 0.18131174 0.29455861 0.40494332 0.50016832 0.56759012 0.59270072 0.5781858 0.546084 0.51277107 0.485135 0.45834461 0.42613879 0.382365][0.012252442 0.083085105 0.18984452 0.31464076 0.44116929 0.5533874 0.62984055 0.64785194 0.61336857 0.55812943 0.50811887 0.47313938 0.44843757 0.4254064 0.3916662][0.0082577141 0.082628392 0.19666691 0.33161604 0.47033811 0.59246594 0.669081 0.67348427 0.6181531 0.54444683 0.48657328 0.45340121 0.43707949 0.42526764 0.40102857][0.0040705795 0.081755087 0.20145197 0.34241018 0.48420411 0.60246187 0.66652435 0.653855 0.58561939 0.50798458 0.45696828 0.43640724 0.43257144 0.42952242 0.40969363][0.0014371262 0.08058501 0.20121278 0.3391847 0.4699322 0.56845492 0.60943878 0.58106446 0.51217037 0.44671798 0.41551468 0.41494042 0.42508695 0.42789033 0.40917879][-0.0029034349 0.073350549 0.18622233 0.30855641 0.41429621 0.48286837 0.49857149 0.46055904 0.40086517 0.35642168 0.349492 0.36949331 0.39219645 0.40054762 0.38647634][-0.01021666 0.057728592 0.15490758 0.25325197 0.32878414 0.36769378 0.36361197 0.32269558 0.27584895 0.25173956 0.26463932 0.29899082 0.32993886 0.34484273 0.34195125][-0.016638933 0.041260365 0.12088934 0.19653651 0.24820143 0.26733857 0.25340554 0.21505174 0.17925932 0.16823873 0.19112568 0.23044297 0.26403868 0.28538844 0.29777655][-0.018693574 0.030476991 0.0955907 0.15500779 0.1926924 0.20330189 0.18860713 0.15633747 0.12814027 0.1216802 0.14388047 0.17884362 0.20965625 0.23593023 0.2639457][-0.016829727 0.027309503 0.084538326 0.13636476 0.16882996 0.17851003 0.16767545 0.14164838 0.11643039 0.10700817 0.12067705 0.14518408 0.16931093 0.19726542 0.23655225][-0.01407116 0.029030306 0.085255027 0.13704942 0.1704409 0.1830925 0.17707863 0.15492426 0.12824021 0.11132743 0.11349097 0.12579159 0.14105149 0.16568205 0.20708168]]...]
INFO - root - 2017-12-10 17:11:01.269287: step 35410, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.758 sec/batch; 62h:33m:45s remains)
INFO - root - 2017-12-10 17:11:09.077465: step 35420, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.773 sec/batch; 63h:48m:07s remains)
INFO - root - 2017-12-10 17:11:16.974578: step 35430, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 65h:04m:42s remains)
INFO - root - 2017-12-10 17:11:25.189154: step 35440, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.812 sec/batch; 67h:01m:44s remains)
INFO - root - 2017-12-10 17:11:33.086974: step 35450, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 66h:01m:09s remains)
INFO - root - 2017-12-10 17:11:40.637071: step 35460, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 66h:38m:06s remains)
INFO - root - 2017-12-10 17:11:48.507089: step 35470, loss = 0.69, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 62h:26m:05s remains)
INFO - root - 2017-12-10 17:11:56.368723: step 35480, loss = 0.69, batch loss = 0.64 (9.7 examples/sec; 0.827 sec/batch; 68h:14m:31s remains)
INFO - root - 2017-12-10 17:12:04.228574: step 35490, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 63h:57m:12s remains)
INFO - root - 2017-12-10 17:12:12.119942: step 35500, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 66h:01m:22s remains)
2017-12-10 17:12:12.962219: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19141792 0.082478866 0.013399697 -0.0067912028 -0.0015673887 0.012483971 0.025228178 0.028306946 0.019426862 0.0052775163 -0.0066142525 -0.011971268 -0.0040658955 0.020015946 0.054096561][0.20919305 0.076851696 -0.0095744459 -0.035370413 -0.024770882 0.005056004 0.039938308 0.064675063 0.071892969 0.065016329 0.051519766 0.038111646 0.035924364 0.051235385 0.080483064][0.18795653 0.054660503 -0.027890688 -0.043254483 -0.013946418 0.042459842 0.10701149 0.15704586 0.17930061 0.17460786 0.15154789 0.12170438 0.10050172 0.098753296 0.11718149][0.14194581 0.032558564 -0.024036966 -0.014074929 0.043244492 0.13162394 0.22823577 0.30218434 0.3331354 0.32140088 0.27843165 0.22127467 0.17149222 0.14639562 0.14989373][0.09840101 0.028695254 0.012433892 0.057644892 0.1483271 0.26893276 0.394276 0.48633829 0.51946497 0.4948141 0.42876187 0.34356454 0.26666448 0.21904442 0.20623185][0.074726306 0.047848169 0.075809695 0.15924326 0.28420389 0.4333187 0.57941681 0.68117273 0.71290821 0.67849123 0.59902567 0.50000477 0.41126907 0.35146919 0.32435456][0.064806126 0.076412939 0.14563958 0.26633415 0.42401254 0.59687489 0.75472939 0.85679638 0.88245255 0.84060562 0.75651896 0.65639687 0.57016981 0.50917065 0.47259653][0.064204291 0.10589077 0.20841767 0.35996592 0.543307 0.73215616 0.89293414 0.98863029 1.0053847 0.95923012 0.87756914 0.78432548 0.70796353 0.65192127 0.60940409][0.058807179 0.11937905 0.24284856 0.41174811 0.60559148 0.79466629 0.9438234 1.0231302 1.0267359 0.97752446 0.90207559 0.8201313 0.75720441 0.70905232 0.66375142][0.033419374 0.10155763 0.23056111 0.39804214 0.58133852 0.75026077 0.87237549 0.92739224 0.91796762 0.87191707 0.81146383 0.74922395 0.70354342 0.66408038 0.61709625][-0.017424179 0.047793504 0.16679926 0.31394637 0.46691242 0.59895569 0.6842261 0.71254957 0.69363797 0.65569532 0.61468947 0.57567704 0.54789728 0.51750058 0.47168541][-0.074773632 -0.021860901 0.072624967 0.18401051 0.29383066 0.38208762 0.43117383 0.4379122 0.41425383 0.38460591 0.35917982 0.337845 0.322793 0.30115792 0.26272565][-0.11308929 -0.076818079 -0.013072634 0.058148075 0.12423054 0.17267562 0.19342048 0.18709584 0.16461197 0.14429499 0.13117707 0.12202618 0.11517166 0.10153031 0.074000813][-0.12554385 -0.10485712 -0.068956219 -0.032157615 -0.0014490414 0.017012306 0.019086955 0.0070731957 -0.010084157 -0.020318715 -0.023049062 -0.02359594 -0.025485147 -0.033344366 -0.050692629][-0.11843254 -0.10908204 -0.093248218 -0.0803851 -0.073308863 -0.073849551 -0.082234778 -0.095099643 -0.10517128 -0.1056779 -0.0997255 -0.094017111 -0.092678271 -0.096993454 -0.10748133]]...]
INFO - root - 2017-12-10 17:12:20.794861: step 35510, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 65h:04m:45s remains)
INFO - root - 2017-12-10 17:12:28.592003: step 35520, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 63h:49m:32s remains)
INFO - root - 2017-12-10 17:12:36.453340: step 35530, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 66h:32m:48s remains)
INFO - root - 2017-12-10 17:12:44.176489: step 35540, loss = 0.70, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 62h:19m:17s remains)
INFO - root - 2017-12-10 17:12:51.938932: step 35550, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.789 sec/batch; 65h:03m:38s remains)
INFO - root - 2017-12-10 17:12:59.776514: step 35560, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.777 sec/batch; 64h:03m:22s remains)
INFO - root - 2017-12-10 17:13:07.880198: step 35570, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.801 sec/batch; 66h:02m:20s remains)
INFO - root - 2017-12-10 17:13:15.702279: step 35580, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 63h:39m:29s remains)
INFO - root - 2017-12-10 17:13:23.665120: step 35590, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.821 sec/batch; 67h:44m:05s remains)
INFO - root - 2017-12-10 17:13:31.536570: step 35600, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 66h:16m:55s remains)
2017-12-10 17:13:32.399059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.011417458 0.05487673 0.15906768 0.28448272 0.40403292 0.48717111 0.50772542 0.45789477 0.35024014 0.21455668 0.082852662 -0.015868681 -0.07319919 -0.0986974 -0.10252124][-0.0013947678 0.0773878 0.20196465 0.35181823 0.49450129 0.59511709 0.62313688 0.56638455 0.4384447 0.27781048 0.12287192 0.0064428332 -0.0618289 -0.093127318 -0.10051687][0.0020735932 0.087576635 0.22449723 0.39095211 0.55059046 0.66543329 0.70075315 0.63888615 0.49405989 0.31312764 0.14084855 0.012769273 -0.060449831 -0.091629155 -0.097473346][-2.1827698e-05 0.087264985 0.22915989 0.40535897 0.57748926 0.70448136 0.74754071 0.68335885 0.52619606 0.32937616 0.1436262 0.0071403733 -0.067676321 -0.094769061 -0.094979085][-0.00086022954 0.08623635 0.23035933 0.41416097 0.59860444 0.73865592 0.79115182 0.72809446 0.5634318 0.35462731 0.15715469 0.012716393 -0.0639247 -0.086962022 -0.080904178][0.00061929325 0.08687029 0.23301715 0.42618257 0.627178 0.78524309 0.85097575 0.79244959 0.62298155 0.40270865 0.19193763 0.037436403 -0.043032046 -0.0642056 -0.053925388][0.0032190857 0.085565537 0.22963974 0.42864737 0.6450215 0.82308781 0.90606415 0.8577134 0.68906468 0.4607242 0.23797709 0.074042857 -0.010012863 -0.029585419 -0.015872743][0.0078072515 0.081348322 0.21483463 0.408097 0.62846 0.81941396 0.91918659 0.88745427 0.73053688 0.5052104 0.27953255 0.11267251 0.029123345 0.013533921 0.031914078][0.016267456 0.079487346 0.19687285 0.372716 0.58101296 0.76917744 0.87635118 0.86121672 0.7249893 0.51636881 0.30160731 0.1422064 0.064999513 0.056335513 0.081444249][0.02623282 0.07906574 0.17723531 0.32647103 0.50680286 0.67304242 0.7717784 0.7659809 0.65390837 0.47397006 0.28549978 0.14646582 0.083323106 0.084934331 0.11862032][0.027684145 0.066922337 0.1404973 0.25397223 0.3930347 0.52203947 0.598739 0.59413457 0.50631022 0.36359182 0.21432282 0.10739606 0.065901153 0.081157111 0.12430631][0.018457368 0.039846569 0.083810933 0.15540436 0.24579716 0.32982722 0.3778199 0.37014803 0.30636105 0.20607528 0.10438083 0.03706133 0.021150682 0.049753658 0.099210165][0.0063071866 0.011110593 0.027555631 0.060610522 0.10629822 0.14905144 0.1704803 0.15931526 0.11722577 0.057045657 0.0012153302 -0.028204307 -0.021343842 0.015725376 0.064685561][-0.0049595549 -0.010247933 -0.011637488 -0.0047509759 0.0096842479 0.023332521 0.02623222 0.013087496 -0.012858452 -0.04312608 -0.065028831 -0.06752222 -0.047056563 -0.0093717445 0.032239228][-0.016203454 -0.025549443 -0.035455834 -0.042431 -0.045326494 -0.047579363 -0.053855028 -0.066195071 -0.081210069 -0.093247272 -0.095696568 -0.084311038 -0.059475131 -0.026493471 0.0050783074]]...]
INFO - root - 2017-12-10 17:13:40.296413: step 35610, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 65h:44m:48s remains)
INFO - root - 2017-12-10 17:13:47.965044: step 35620, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 65h:36m:33s remains)
INFO - root - 2017-12-10 17:13:55.864621: step 35630, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 65h:04m:42s remains)
INFO - root - 2017-12-10 17:14:03.743357: step 35640, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 64h:11m:15s remains)
INFO - root - 2017-12-10 17:14:11.622850: step 35650, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 63h:35m:11s remains)
INFO - root - 2017-12-10 17:14:19.546665: step 35660, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 63h:09m:29s remains)
INFO - root - 2017-12-10 17:14:27.419272: step 35670, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.803 sec/batch; 66h:14m:38s remains)
INFO - root - 2017-12-10 17:14:35.327515: step 35680, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 65h:55m:30s remains)
INFO - root - 2017-12-10 17:14:43.208946: step 35690, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 62h:32m:53s remains)
INFO - root - 2017-12-10 17:14:50.932618: step 35700, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 63h:45m:19s remains)
2017-12-10 17:14:51.786453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.021944761 -0.023062861 -0.025020845 -0.027251538 -0.029415289 -0.030871397 -0.031763945 -0.031732772 -0.031474181 -0.031743441 -0.031958327 -0.029658806 -0.025005313 -0.021915713 -0.023679946][0.016723294 0.014449542 0.0093830368 0.0042490689 -0.00027577294 -0.0032454475 -0.0049982197 -0.0039390931 -0.0013580782 5.2239422e-05 0.0011283914 0.0069910544 0.01729019 0.023920288 0.020991692][0.070988111 0.068318777 0.059420809 0.051186413 0.045007326 0.041468464 0.040924784 0.046754185 0.055431429 0.06075979 0.064119838 0.074335359 0.090797052 0.10036206 0.094195753][0.13067137 0.13263652 0.12499781 0.11906853 0.11633182 0.11591735 0.12041061 0.1353875 0.1527358 0.16134763 0.16373676 0.17404658 0.1917652 0.19950575 0.18612616][0.18732718 0.20184892 0.20452495 0.20883407 0.21607305 0.22302058 0.23551381 0.26128754 0.285542 0.29249182 0.28585169 0.28650203 0.29565281 0.2940357 0.26957676][0.23022498 0.26511589 0.28680497 0.30712426 0.32806012 0.34437898 0.36506593 0.3994382 0.42606285 0.42606527 0.403688 0.38641861 0.37897596 0.36276132 0.32602957][0.26270315 0.32269728 0.36768535 0.40462902 0.43765956 0.46095583 0.48609933 0.52332741 0.54578513 0.5337835 0.49161509 0.4525325 0.42552432 0.39427388 0.34791711][0.28163332 0.36329854 0.42794293 0.47596249 0.51403666 0.53817761 0.56186014 0.59534156 0.60834807 0.582188 0.52121347 0.46317798 0.42070565 0.37975937 0.33044869][0.29444775 0.38573608 0.45612848 0.50112176 0.53062546 0.54522824 0.55992436 0.58511245 0.58931589 0.55485225 0.48498258 0.41953865 0.37369925 0.33351818 0.28930238][0.30940276 0.39389 0.45242545 0.48004934 0.48956269 0.48742214 0.48920518 0.50591052 0.50674266 0.47378433 0.40888417 0.35154667 0.31697175 0.28857562 0.25566915][0.32202137 0.385793 0.42052087 0.42553979 0.41455191 0.39761612 0.38925192 0.40130702 0.40446889 0.37957472 0.32861608 0.28922066 0.27435136 0.2632255 0.24345957][0.32548672 0.36290705 0.37300578 0.36137855 0.33987153 0.31837046 0.30773827 0.32001886 0.32740879 0.31054017 0.2739355 0.25311068 0.25742549 0.261917 0.25230804][0.30839679 0.32200313 0.31474569 0.29779226 0.27907589 0.26477668 0.26011741 0.276161 0.28699359 0.27492318 0.24877635 0.24228708 0.26060393 0.27521229 0.27046427][0.26491439 0.26275545 0.24875295 0.23661949 0.2296146 0.22858697 0.23286891 0.25155371 0.26166254 0.24917546 0.22777839 0.22915389 0.25418264 0.27293178 0.26953688][0.19265264 0.18495494 0.17524537 0.17522532 0.1835337 0.1957555 0.2069777 0.224057 0.22873047 0.21181047 0.19033277 0.19279011 0.21642347 0.23315495 0.22851454]]...]
INFO - root - 2017-12-10 17:14:59.602802: step 35710, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 64h:28m:44s remains)
INFO - root - 2017-12-10 17:15:07.480373: step 35720, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 65h:26m:16s remains)
INFO - root - 2017-12-10 17:15:15.056418: step 35730, loss = 0.69, batch loss = 0.64 (10.7 examples/sec; 0.749 sec/batch; 61h:43m:50s remains)
INFO - root - 2017-12-10 17:15:22.903368: step 35740, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 62h:50m:29s remains)
INFO - root - 2017-12-10 17:15:30.864172: step 35750, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.818 sec/batch; 67h:25m:34s remains)
INFO - root - 2017-12-10 17:15:38.786822: step 35760, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 65h:06m:00s remains)
INFO - root - 2017-12-10 17:15:46.602178: step 35770, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 64h:27m:27s remains)
INFO - root - 2017-12-10 17:15:54.296539: step 35780, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 63h:45m:10s remains)
INFO - root - 2017-12-10 17:16:02.142836: step 35790, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 66h:14m:37s remains)
INFO - root - 2017-12-10 17:16:10.002642: step 35800, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 66h:39m:47s remains)
2017-12-10 17:16:10.785938: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1909076 0.17874481 0.17744617 0.20034978 0.24413484 0.29421702 0.33446735 0.35864067 0.37174 0.39426532 0.42046162 0.44347012 0.45105237 0.43084335 0.37890694][0.17084445 0.16760781 0.17769369 0.21132618 0.26281223 0.31495371 0.3527416 0.37486783 0.38851169 0.41421255 0.44170716 0.46523008 0.4731558 0.45158646 0.39571419][0.12149871 0.13041221 0.15513141 0.20263457 0.26408875 0.32008144 0.35707247 0.37727985 0.39041841 0.41719916 0.44666082 0.47426513 0.48516676 0.4624112 0.401556][0.069283783 0.089848563 0.13006556 0.19247772 0.26538888 0.32670474 0.36464036 0.38320175 0.393821 0.42029092 0.45344827 0.48795369 0.50280541 0.47729209 0.4078818][0.025549797 0.054045618 0.10653035 0.18146031 0.26502067 0.33201933 0.37193349 0.387592 0.39243874 0.41609994 0.45435086 0.50038248 0.52615178 0.50545603 0.43332106][-0.0054271547 0.027631128 0.087778039 0.17107429 0.26290265 0.3357482 0.37915081 0.39225069 0.38997123 0.40786025 0.44856894 0.50536543 0.54638922 0.538192 0.47125232][-0.021818269 0.012336678 0.076236248 0.16798395 0.27400672 0.36331484 0.42015347 0.43537936 0.42446974 0.42663088 0.45331314 0.5038588 0.54980958 0.55225539 0.49494913][-0.024771715 0.0073213582 0.074265376 0.1798857 0.31143257 0.42960194 0.50651211 0.52445585 0.49840897 0.47126472 0.46546975 0.49109757 0.52869421 0.53582978 0.48863131][-0.012605461 0.015280739 0.082892627 0.1991785 0.35109597 0.49071309 0.57981557 0.59563392 0.55281264 0.4962073 0.45445913 0.44838214 0.46962804 0.47641128 0.43901658][0.0042003253 0.026711458 0.089327022 0.20474091 0.35967204 0.50375086 0.59499174 0.60889256 0.55808282 0.4839001 0.41704422 0.38496619 0.38912368 0.39193144 0.36231008][0.020273201 0.037323281 0.090105906 0.19330874 0.33346033 0.4646948 0.54776138 0.56083632 0.51273382 0.43668136 0.361718 0.31594524 0.3064988 0.30189186 0.27464426][0.027666902 0.03818436 0.0780651 0.16176337 0.27603748 0.38324863 0.45110729 0.46344143 0.42444533 0.35833922 0.28983364 0.24368301 0.22786111 0.21721916 0.19066052][0.026539506 0.029024808 0.053864673 0.11293626 0.19440424 0.27043456 0.31768957 0.32596871 0.29618222 0.24615508 0.19424857 0.15920691 0.14700694 0.13799022 0.11661133][0.018929487 0.014612938 0.024692839 0.057267491 0.10316326 0.1453234 0.16959062 0.17045945 0.14835951 0.11673325 0.086604066 0.069035836 0.066472672 0.064359456 0.0516859][0.0061314311 -0.0033145905 -0.0058669285 0.0035230715 0.0186507 0.032439254 0.038267918 0.03397638 0.020462632 0.0070863767 -0.0013496175 -0.0015307318 0.0055045341 0.0096700154 0.0041672047]]...]
INFO - root - 2017-12-10 17:16:18.800050: step 35810, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.807 sec/batch; 66h:29m:23s remains)
INFO - root - 2017-12-10 17:16:26.595893: step 35820, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.811 sec/batch; 66h:51m:29s remains)
INFO - root - 2017-12-10 17:16:34.529865: step 35830, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 64h:58m:29s remains)
INFO - root - 2017-12-10 17:16:42.479622: step 35840, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.805 sec/batch; 66h:18m:46s remains)
INFO - root - 2017-12-10 17:16:50.521774: step 35850, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 67h:08m:56s remains)
INFO - root - 2017-12-10 17:16:58.286232: step 35860, loss = 0.70, batch loss = 0.65 (9.6 examples/sec; 0.834 sec/batch; 68h:43m:29s remains)
INFO - root - 2017-12-10 17:17:06.189381: step 35870, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 63h:56m:08s remains)
INFO - root - 2017-12-10 17:17:14.084317: step 35880, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.806 sec/batch; 66h:23m:14s remains)
INFO - root - 2017-12-10 17:17:21.917908: step 35890, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 65h:56m:00s remains)
INFO - root - 2017-12-10 17:17:29.712849: step 35900, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 63h:19m:03s remains)
2017-12-10 17:17:30.498594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.084573425 -0.084311441 -0.083035275 -0.081658855 -0.080361307 -0.079467334 -0.078873888 -0.078353077 -0.078193404 -0.07869149 -0.079493947 -0.0801898 -0.080970407 -0.08058539 -0.080552638][-0.084339105 -0.081885107 -0.078822024 -0.075963669 -0.072955444 -0.071006581 -0.070733063 -0.070805706 -0.071199328 -0.072399169 -0.074327439 -0.076267153 -0.078081489 -0.079279393 -0.08116322][-0.064642236 -0.057152826 -0.051169861 -0.046277232 -0.040772188 -0.037970759 -0.039660744 -0.042181224 -0.044617578 -0.047759861 -0.051881097 -0.055439591 -0.058250576 -0.061826564 -0.067072161][-0.016651219 -0.00020189572 0.011058155 0.019962419 0.029366644 0.032417033 0.025812242 0.017498234 0.01035503 0.0026337423 -0.0057836166 -0.011675294 -0.014854519 -0.020317579 -0.029594669][0.060645327 0.089251645 0.108 0.12295394 0.13741718 0.13974869 0.12480552 0.10681043 0.0927642 0.080267124 0.06880331 0.06245628 0.061645791 0.056754973 0.044560175][0.15306455 0.19277897 0.21796711 0.23825873 0.2572262 0.25785571 0.23286903 0.20320868 0.18241735 0.16940126 0.16151407 0.16091526 0.1667863 0.1659423 0.15391916][0.23013465 0.27398074 0.29997155 0.32163692 0.34280533 0.34247321 0.31109798 0.27412793 0.25214618 0.2475113 0.25334021 0.26670915 0.28287753 0.28781989 0.27782562][0.25865254 0.29410028 0.3112672 0.32817855 0.34917676 0.35181057 0.32423431 0.29138049 0.27807921 0.2915549 0.31997108 0.35227436 0.37824562 0.38672361 0.37832984][0.23240277 0.24614425 0.24545652 0.25249216 0.27224892 0.28341958 0.27210364 0.25676242 0.26101556 0.29679731 0.34820193 0.39613119 0.42584464 0.43185684 0.42192689][0.17192881 0.15677936 0.13421525 0.12973863 0.14904752 0.17349912 0.18744642 0.19877699 0.22358561 0.27613413 0.33924276 0.39066195 0.414253 0.41119733 0.39600807][0.10933668 0.0671901 0.025418924 0.012226277 0.033529911 0.073555447 0.11470555 0.15305537 0.19321553 0.24829334 0.303845 0.34169388 0.3485654 0.33143243 0.30914545][0.074280538 0.01530388 -0.037870828 -0.054745648 -0.029386148 0.024982668 0.08830861 0.14617054 0.19141538 0.23407079 0.26598865 0.27742618 0.26139054 0.22882283 0.19998233][0.083526909 0.02056841 -0.036396217 -0.054375768 -0.026093893 0.037293348 0.11336361 0.18025766 0.22135812 0.24270472 0.24326411 0.22339022 0.18347721 0.13678052 0.10308647][0.13791436 0.080651529 0.0226018 0.00032901 0.024720781 0.087843858 0.16635706 0.23422182 0.26844382 0.27016497 0.24283317 0.1952194 0.1346591 0.075404584 0.036475755][0.21871987 0.17049226 0.10976171 0.076989658 0.088837251 0.14251859 0.21618241 0.2817018 0.312424 0.30402005 0.26087442 0.1953944 0.11989383 0.048971057 0.0023926392]]...]
INFO - root - 2017-12-10 17:17:38.344324: step 35910, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 64h:22m:56s remains)
INFO - root - 2017-12-10 17:17:46.284958: step 35920, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 64h:58m:56s remains)
INFO - root - 2017-12-10 17:17:54.180462: step 35930, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 63h:17m:15s remains)
INFO - root - 2017-12-10 17:18:01.851534: step 35940, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 64h:37m:09s remains)
INFO - root - 2017-12-10 17:18:09.666629: step 35950, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 65h:05m:06s remains)
INFO - root - 2017-12-10 17:18:17.457791: step 35960, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 63h:50m:20s remains)
INFO - root - 2017-12-10 17:18:25.383335: step 35970, loss = 0.71, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 64h:27m:39s remains)
INFO - root - 2017-12-10 17:18:33.252455: step 35980, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 65h:46m:35s remains)
INFO - root - 2017-12-10 17:18:41.025356: step 35990, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.795 sec/batch; 65h:29m:55s remains)
INFO - root - 2017-12-10 17:18:48.901361: step 36000, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 65h:17m:23s remains)
2017-12-10 17:18:49.739244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.036477659 -0.038856909 -0.019362962 0.020917397 0.071060851 0.11302345 0.1381367 0.14220344 0.12733644 0.094529852 0.048230082 0.0013793765 -0.037143387 -0.05943941 -0.057776727][0.0022454187 -0.010320317 0.01016726 0.06602338 0.14219762 0.20770867 0.24201682 0.23654276 0.19631347 0.13095896 0.054926015 -0.010531943 -0.051850513 -0.066911317 -0.055415504][0.076905146 0.045330681 0.058810417 0.12707315 0.22988245 0.32161933 0.36817476 0.35436821 0.28733367 0.18575062 0.075194925 -0.012149151 -0.057573866 -0.065027811 -0.042610966][0.17818795 0.12128074 0.1211332 0.19755715 0.32499045 0.44430941 0.50769794 0.49238187 0.40488023 0.27081922 0.12618427 0.01387296 -0.042213388 -0.049129259 -0.020894296][0.27845234 0.19773023 0.1801413 0.25830522 0.40283132 0.54593313 0.62864012 0.62045437 0.52404392 0.36839128 0.19771785 0.062581904 -0.0086728372 -0.023453012 0.0026716921][0.36800861 0.27138117 0.23766111 0.31018236 0.45945957 0.61620903 0.715887 0.72088283 0.62914729 0.46922788 0.28839797 0.13848783 0.050504856 0.018926682 0.031748123][0.44388437 0.34479198 0.29912987 0.35820174 0.49648103 0.65011042 0.757573 0.77830023 0.706179 0.56446922 0.39524508 0.24371839 0.13976625 0.0826297 0.068985417][0.47548434 0.390996 0.34429604 0.3893272 0.50586087 0.64176708 0.74666631 0.78464431 0.74602091 0.6437493 0.50727242 0.36748481 0.24949093 0.15977493 0.10903066][0.46035111 0.39834729 0.35985732 0.39428076 0.48607883 0.5974769 0.69341862 0.74786031 0.74862945 0.69687313 0.60590279 0.48825765 0.36205584 0.24141364 0.15187469][0.42195985 0.38102439 0.35103324 0.37563428 0.44433987 0.53248197 0.6192134 0.68825412 0.726267 0.7236833 0.67785174 0.58500379 0.45714334 0.31465358 0.19346961][0.39096981 0.36289877 0.33672512 0.3497068 0.39743167 0.46626785 0.54647732 0.62746412 0.69432318 0.72901124 0.71762604 0.64582878 0.52104419 0.36781389 0.22676566][0.38198 0.35807398 0.3271336 0.32356107 0.34988466 0.40236759 0.47854525 0.56895357 0.65476918 0.71137023 0.7193886 0.66129506 0.54230762 0.38863391 0.24089555][0.38500014 0.35437611 0.31107423 0.288156 0.29511023 0.33556488 0.40999722 0.50650257 0.6003949 0.66246814 0.67348742 0.62039429 0.50978428 0.36614367 0.2260921][0.39189461 0.34901538 0.28772071 0.24450551 0.23520051 0.26758632 0.34117818 0.43902457 0.53071409 0.58510685 0.58700806 0.53205019 0.43080103 0.30473113 0.18400681][0.4073365 0.34983534 0.26830012 0.20448506 0.17926423 0.20174733 0.26924789 0.36130783 0.44424486 0.48654807 0.47670487 0.41777876 0.32488194 0.21815977 0.12209882]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 17:18:57.680730: step 36010, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 65h:28m:29s remains)
INFO - root - 2017-12-10 17:19:05.382409: step 36020, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 66h:44m:08s remains)
INFO - root - 2017-12-10 17:19:13.220320: step 36030, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 62h:36m:50s remains)
INFO - root - 2017-12-10 17:19:21.149203: step 36040, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.773 sec/batch; 63h:38m:53s remains)
INFO - root - 2017-12-10 17:19:29.192450: step 36050, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 63h:36m:01s remains)
INFO - root - 2017-12-10 17:19:36.941334: step 36060, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 63h:51m:16s remains)
INFO - root - 2017-12-10 17:19:44.812331: step 36070, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.795 sec/batch; 65h:27m:00s remains)
INFO - root - 2017-12-10 17:19:52.567987: step 36080, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 66h:25m:38s remains)
INFO - root - 2017-12-10 17:20:00.455668: step 36090, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 65h:26m:49s remains)
INFO - root - 2017-12-10 17:20:08.112999: step 36100, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 64h:27m:56s remains)
2017-12-10 17:20:08.906630: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0563635 0.040396553 0.025340531 0.014977873 0.010479912 0.010855908 0.013035714 0.01381255 0.010384542 0.0036903513 -0.0041008126 -0.011548306 -0.016946487 -0.018141415 -0.012262299][0.1676973 0.14556913 0.12073375 0.10180118 0.092013024 0.089093953 0.087707341 0.082831532 0.07023935 0.051558871 0.0312352 0.013379491 0.0015424138 -0.00052073289 0.011386559][0.28684792 0.26094443 0.22825797 0.20327 0.19045487 0.18498346 0.1787367 0.16551003 0.14072473 0.10616246 0.068380162 0.036305703 0.01719095 0.015283573 0.034468736][0.37538937 0.35076395 0.31687483 0.29299983 0.282488 0.27740961 0.2672123 0.24537727 0.20839478 0.1578036 0.1014946 0.053961478 0.026982408 0.025818437 0.05198855][0.4063099 0.38723689 0.36013272 0.34523794 0.34331936 0.343298 0.33251253 0.30433047 0.25691658 0.19267225 0.12051788 0.059313603 0.025329895 0.024621263 0.055302594][0.38269424 0.37399244 0.36105987 0.36138678 0.37276345 0.38171408 0.37432319 0.34348261 0.28822243 0.21327332 0.1291443 0.057504956 0.017441826 0.015032647 0.046036411][0.30961311 0.31555983 0.32281035 0.34286064 0.37086064 0.39246011 0.39318624 0.36497104 0.30692214 0.22621883 0.13561781 0.057676129 0.012532654 0.0056664278 0.031757843][0.20849876 0.22897667 0.25627574 0.29526857 0.3393251 0.37423849 0.38574556 0.36503261 0.31050202 0.23108707 0.14154142 0.063417085 0.015790155 0.0033939288 0.021164834][0.11517457 0.1448916 0.1853047 0.23688675 0.29272878 0.33924982 0.3622444 0.35187402 0.30560079 0.233592 0.15182914 0.078961812 0.0314574 0.01367138 0.022192864][0.060238428 0.09217985 0.13634285 0.19205435 0.25356388 0.30815437 0.34115541 0.34113571 0.30502796 0.24397281 0.17459388 0.11107648 0.066544272 0.044785652 0.044510365][0.054250468 0.082352221 0.12020246 0.16942103 0.22659785 0.28131935 0.31991279 0.32961351 0.30672923 0.26176703 0.21063551 0.1615878 0.12325677 0.099235006 0.09082558][0.08371795 0.10402244 0.12851365 0.16220292 0.20458676 0.24945679 0.28695315 0.30415833 0.29628491 0.271447 0.24311094 0.21290128 0.18370821 0.15926455 0.14355133][0.1362993 0.14890751 0.15851843 0.17338544 0.19587462 0.22472014 0.25518382 0.27503541 0.27843893 0.27079347 0.26259577 0.25041553 0.23080125 0.20721821 0.1854368][0.19510874 0.20250742 0.20114633 0.20246933 0.2109758 0.22810978 0.25221288 0.27093068 0.27792683 0.27685091 0.27673724 0.27288961 0.2575466 0.23341709 0.20647708][0.23548067 0.24090686 0.23559444 0.23374778 0.24024183 0.25517878 0.27694732 0.29231054 0.29530489 0.28929916 0.28450093 0.27824476 0.26106706 0.23445919 0.20322628]]...]
INFO - root - 2017-12-10 17:20:16.780757: step 36110, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 64h:12m:46s remains)
INFO - root - 2017-12-10 17:20:24.672606: step 36120, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 64h:34m:09s remains)
INFO - root - 2017-12-10 17:20:32.485443: step 36130, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 62h:45m:50s remains)
INFO - root - 2017-12-10 17:20:40.334184: step 36140, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 66h:26m:23s remains)
INFO - root - 2017-12-10 17:20:48.254384: step 36150, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 65h:42m:02s remains)
INFO - root - 2017-12-10 17:20:56.125584: step 36160, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.804 sec/batch; 66h:10m:49s remains)
INFO - root - 2017-12-10 17:21:03.788576: step 36170, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 65h:24m:23s remains)
INFO - root - 2017-12-10 17:21:11.472958: step 36180, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 65h:34m:14s remains)
INFO - root - 2017-12-10 17:21:19.380314: step 36190, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.812 sec/batch; 66h:50m:24s remains)
INFO - root - 2017-12-10 17:21:27.160126: step 36200, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 64h:30m:21s remains)
2017-12-10 17:21:27.989539: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21980919 0.18574157 0.15190692 0.13116291 0.12840897 0.14510618 0.17014301 0.189527 0.18521729 0.14666887 0.082531042 0.01321272 -0.039281089 -0.066444382 -0.071763687][0.19839472 0.16466376 0.13326603 0.118982 0.12731916 0.15897614 0.199518 0.23017764 0.22934766 0.18595207 0.11198943 0.030980831 -0.031214872 -0.064315967 -0.0723421][0.1640753 0.13476315 0.1100183 0.10451299 0.12392151 0.1680513 0.21986161 0.25741 0.25732625 0.20933107 0.12834775 0.040193245 -0.02711363 -0.062999584 -0.072044514][0.12098389 0.097956456 0.0816417 0.085150845 0.11373247 0.16590913 0.22349557 0.26289517 0.26093373 0.20926365 0.12511355 0.035310421 -0.031835411 -0.06606061 -0.073058248][0.0728396 0.055640474 0.047787845 0.060206249 0.096881807 0.15478693 0.21538796 0.25467482 0.2505548 0.1970451 0.11268184 0.024571482 -0.039785363 -0.070731379 -0.074791864][0.033931598 0.022191808 0.021731066 0.041679475 0.084558584 0.14574598 0.20671281 0.24403593 0.2371154 0.18242012 0.099139623 0.013992012 -0.046916995 -0.074757613 -0.076508611][0.019030483 0.011625786 0.015570382 0.03895022 0.083522238 0.14398298 0.20235057 0.23601234 0.2252236 0.16864167 0.086135782 0.0040199757 -0.053198658 -0.077861041 -0.077452257][0.023264337 0.017514192 0.022115756 0.045158107 0.088517629 0.14653897 0.20161045 0.23159501 0.2168669 0.15783522 0.075222641 -0.0048195613 -0.059018642 -0.080999121 -0.078704931][0.037929028 0.033070173 0.037578437 0.060321238 0.10376395 0.16193561 0.21676731 0.24543644 0.22717632 0.16371797 0.077341244 -0.0047967378 -0.059616935 -0.081647068 -0.079282366][0.06076768 0.056695025 0.060887989 0.083398223 0.12788564 0.18852924 0.24606733 0.27581114 0.25477532 0.18546039 0.092179023 0.0042748568 -0.054461915 -0.079267837 -0.078850739][0.0854987 0.081781484 0.084768407 0.10607265 0.15025337 0.21166801 0.27016291 0.29961345 0.27544478 0.20084311 0.10191805 0.010027899 -0.050871275 -0.0773327 -0.078291491][0.10483918 0.10101925 0.10240482 0.12155592 0.16314919 0.22185682 0.27773511 0.30456802 0.27766216 0.20061307 0.10013144 0.0082790209 -0.051610969 -0.07750833 -0.078609295][0.1147099 0.11070724 0.11119747 0.12880974 0.16779795 0.22287467 0.27488473 0.29844552 0.26975581 0.19252393 0.0931055 0.0033996545 -0.05426598 -0.07884787 -0.079456128][0.1251231 0.11971321 0.11899617 0.13590312 0.17395502 0.22701243 0.27605289 0.29646552 0.26550642 0.18730512 0.088333771 0.00042813111 -0.055414751 -0.079237506 -0.079812109][0.13542032 0.12765971 0.12553792 0.14202619 0.17903014 0.22880539 0.27264035 0.2877278 0.25336885 0.1748656 0.078729622 -0.0045216638 -0.056598023 -0.078923672 -0.079499163]]...]
INFO - root - 2017-12-10 17:21:35.889440: step 36210, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.823 sec/batch; 67h:45m:40s remains)
INFO - root - 2017-12-10 17:21:43.767287: step 36220, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 64h:52m:34s remains)
INFO - root - 2017-12-10 17:21:51.575630: step 36230, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 63h:59m:12s remains)
INFO - root - 2017-12-10 17:21:59.549843: step 36240, loss = 0.72, batch loss = 0.66 (9.7 examples/sec; 0.822 sec/batch; 67h:40m:05s remains)
INFO - root - 2017-12-10 17:22:07.424684: step 36250, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.781 sec/batch; 64h:15m:10s remains)
INFO - root - 2017-12-10 17:22:14.712724: step 36260, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 65h:21m:50s remains)
INFO - root - 2017-12-10 17:22:22.638321: step 36270, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.826 sec/batch; 67h:56m:20s remains)
INFO - root - 2017-12-10 17:22:30.431694: step 36280, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 63h:43m:50s remains)
INFO - root - 2017-12-10 17:22:38.394953: step 36290, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.759 sec/batch; 62h:27m:14s remains)
INFO - root - 2017-12-10 17:22:46.274060: step 36300, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 64h:50m:23s remains)
2017-12-10 17:22:47.179565: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.070996895 0.052119534 0.051180173 0.064319871 0.078563131 0.09073516 0.10300788 0.13323604 0.16836058 0.19409415 0.20254704 0.19783941 0.17524739 0.12187435 0.056672007][0.080493152 0.06753891 0.0672536 0.077952765 0.085530005 0.087072425 0.085244648 0.10177822 0.12707157 0.14784981 0.15680033 0.1575274 0.14426377 0.10023236 0.043111153][0.10936303 0.10976259 0.11897419 0.13535807 0.14252572 0.13627683 0.11872508 0.11703883 0.12674513 0.13550413 0.13783513 0.13741544 0.12828244 0.089398317 0.036255687][0.16072245 0.18310383 0.21260284 0.24599072 0.2634055 0.25748444 0.22832072 0.21045735 0.20478083 0.19832651 0.18668588 0.17490499 0.15863779 0.11204352 0.049540427][0.22983707 0.27911294 0.33630905 0.39481387 0.43102318 0.43326986 0.39903834 0.37131673 0.35603872 0.336261 0.3069109 0.2755973 0.24088982 0.17335524 0.087506026][0.29925191 0.37448275 0.45871925 0.54249662 0.59896231 0.61324489 0.57992291 0.55007815 0.53304666 0.50634295 0.4615688 0.40846369 0.34950656 0.25337091 0.13688055][0.34995425 0.43990439 0.53785479 0.6348567 0.70296085 0.72702503 0.70041239 0.67717594 0.66684783 0.6414026 0.58866554 0.51914775 0.438005 0.31515256 0.17159697][0.35923189 0.44386798 0.53691214 0.63032329 0.69693363 0.72396 0.70583427 0.69322133 0.69343108 0.67552716 0.62482291 0.55055583 0.45834184 0.32213375 0.16683495][0.31508234 0.37899771 0.45163974 0.52685297 0.57979614 0.601648 0.58984977 0.58674961 0.59657145 0.58938116 0.54991812 0.48404807 0.39569518 0.26475376 0.11842249][0.21933478 0.25584507 0.301914 0.35216877 0.38597298 0.39875069 0.39085573 0.3932519 0.40747541 0.40896472 0.38438821 0.33595839 0.26501322 0.15792859 0.040460926][0.099013828 0.11195017 0.13425086 0.16065079 0.17610656 0.17963693 0.1738065 0.1778443 0.19135775 0.19718537 0.18578173 0.15666011 0.10937402 0.036103766 -0.042108931][-0.007457329 -0.0093224812 -0.0030265427 0.0056634774 0.0077813775 0.004875767 0.00059140206 0.0045292419 0.014939087 0.021529311 0.018742859 0.0051654461 -0.020368505 -0.061092287 -0.10234776][-0.079182424 -0.086474121 -0.086618461 -0.086230353 -0.089593358 -0.094323218 -0.097117968 -0.094388343 -0.088165596 -0.083521 -0.082783833 -0.086885765 -0.096751548 -0.11323423 -0.12813605][-0.11227891 -0.11864561 -0.11887134 -0.11978687 -0.12297878 -0.12653098 -0.12816623 -0.1268241 -0.12386221 -0.12123986 -0.11962415 -0.11928729 -0.12058502 -0.12351193 -0.12457673][-0.11207142 -0.11641108 -0.11535372 -0.11526946 -0.11651897 -0.11811002 -0.11896008 -0.11872042 -0.1178281 -0.11672457 -0.11542848 -0.11388117 -0.11215139 -0.11018037 -0.10693777]]...]
INFO - root - 2017-12-10 17:22:55.042027: step 36310, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 65h:31m:36s remains)
INFO - root - 2017-12-10 17:23:02.850204: step 36320, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.759 sec/batch; 62h:28m:19s remains)
INFO - root - 2017-12-10 17:23:10.706371: step 36330, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 65h:53m:55s remains)
INFO - root - 2017-12-10 17:23:18.389450: step 36340, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 65h:34m:33s remains)
INFO - root - 2017-12-10 17:23:26.041374: step 36350, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 66h:34m:01s remains)
INFO - root - 2017-12-10 17:23:33.986969: step 36360, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 64h:54m:18s remains)
INFO - root - 2017-12-10 17:23:41.971399: step 36370, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.824 sec/batch; 67h:48m:00s remains)
INFO - root - 2017-12-10 17:23:49.801682: step 36380, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 64h:54m:45s remains)
INFO - root - 2017-12-10 17:23:57.665826: step 36390, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 65h:31m:49s remains)
INFO - root - 2017-12-10 17:24:05.521360: step 36400, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 64h:41m:48s remains)
2017-12-10 17:24:06.284740: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16855775 0.22594233 0.25161481 0.24636513 0.22491279 0.20793875 0.20936865 0.23384868 0.28094119 0.33453524 0.37430191 0.38587293 0.37607422 0.35905668 0.34044129][0.16462608 0.22555345 0.25633883 0.25405037 0.22895887 0.20076099 0.18665275 0.19715509 0.23511671 0.28782186 0.33756757 0.36868736 0.38033998 0.37792042 0.36399898][0.14315757 0.20281421 0.23962294 0.24809022 0.23250337 0.20644614 0.18456833 0.17843498 0.19298504 0.2236764 0.26185828 0.2961078 0.319615 0.3286036 0.32266548][0.11863469 0.17375486 0.21690746 0.24308246 0.25130776 0.24534807 0.23107715 0.21470854 0.20119584 0.19633636 0.20383973 0.22043514 0.23737092 0.24524504 0.24253947][0.10273845 0.15187153 0.19929831 0.24434844 0.28403285 0.31218374 0.32219964 0.30981922 0.27703059 0.23885927 0.21098976 0.19867679 0.19464356 0.18869825 0.17917237][0.096739054 0.13934806 0.18645276 0.2446588 0.31348446 0.38086879 0.42804486 0.43802685 0.40632722 0.35239029 0.30032313 0.26240432 0.23445964 0.2081769 0.18269862][0.092892 0.12879312 0.17040104 0.23137738 0.31705865 0.41559932 0.50041044 0.54394269 0.53345549 0.4863348 0.42987949 0.380624 0.33740267 0.29482946 0.25256783][0.08211752 0.11083908 0.14283997 0.19520664 0.27919665 0.38827491 0.49519834 0.56778228 0.58605993 0.56069976 0.51731706 0.47337136 0.42978057 0.38361117 0.33454147][0.063523911 0.084840357 0.10520353 0.141301 0.20806246 0.30536935 0.41077095 0.49413246 0.53244722 0.52884609 0.5039404 0.47327498 0.43995497 0.40317276 0.36202216][0.045818351 0.060151629 0.068847813 0.086424388 0.12905109 0.20100711 0.28620812 0.36066261 0.40265888 0.40993232 0.39744729 0.37775019 0.35588282 0.3331548 0.30838326][0.039013669 0.04890826 0.049137607 0.052428532 0.073911674 0.11930618 0.1779127 0.23269764 0.26524422 0.27046373 0.25937006 0.24242027 0.22580336 0.21246834 0.20183277][0.052420747 0.0629558 0.060483806 0.056763221 0.064266548 0.088477589 0.12319263 0.15789486 0.17744489 0.1746175 0.15795854 0.13736349 0.11951867 0.10795032 0.10354567][0.086217239 0.10265972 0.10335559 0.099443614 0.099527635 0.10873655 0.12592676 0.14693658 0.15922755 0.152168 0.13179331 0.10779886 0.086610384 0.071544386 0.065663405][0.13206267 0.15756944 0.1656087 0.16612302 0.1632991 0.16218048 0.16697867 0.17995551 0.19183809 0.18784119 0.17069657 0.14866619 0.12711678 0.10860924 0.09819901][0.17460063 0.20917933 0.22603996 0.2324402 0.22874817 0.22029814 0.21533191 0.22116171 0.23333479 0.23525091 0.2269125 0.21366191 0.19848341 0.18213496 0.17006391]]...]
INFO - root - 2017-12-10 17:24:14.246678: step 36410, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 63h:03m:14s remains)
INFO - root - 2017-12-10 17:24:22.119439: step 36420, loss = 0.68, batch loss = 0.62 (9.5 examples/sec; 0.840 sec/batch; 69h:06m:59s remains)
INFO - root - 2017-12-10 17:24:29.996346: step 36430, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.814 sec/batch; 66h:57m:43s remains)
INFO - root - 2017-12-10 17:24:37.678644: step 36440, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 66h:35m:20s remains)
INFO - root - 2017-12-10 17:24:45.587613: step 36450, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 62h:51m:41s remains)
INFO - root - 2017-12-10 17:24:53.412467: step 36460, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.804 sec/batch; 66h:08m:09s remains)
INFO - root - 2017-12-10 17:25:01.236938: step 36470, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 63h:56m:19s remains)
INFO - root - 2017-12-10 17:25:09.122342: step 36480, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.803 sec/batch; 65h:59m:38s remains)
INFO - root - 2017-12-10 17:25:17.002891: step 36490, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.785 sec/batch; 64h:32m:48s remains)
INFO - root - 2017-12-10 17:25:24.611806: step 36500, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.753 sec/batch; 61h:53m:51s remains)
2017-12-10 17:25:25.402038: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.042189591 -0.0036500075 -0.032883286 -0.046444248 -0.045967232 -0.035221048 -0.01810205 0.010194461 0.050933946 0.091799289 0.11727196 0.11286414 0.080088548 0.033258811 -0.01411451][0.065355711 0.017167445 -0.01859621 -0.040500626 -0.047144316 -0.040880311 -0.024464203 0.0055103074 0.047649782 0.087684877 0.1119889 0.10766517 0.075423434 0.028746672 -0.01845121][0.11318491 0.069458209 0.031564217 0.00267274 -0.012794169 -0.01360719 0.00035379603 0.030427862 0.071450181 0.107202 0.12572046 0.11673024 0.0808447 0.031207621 -0.017647393][0.18375033 0.15560298 0.12618503 0.097470477 0.075480863 0.067321122 0.077768587 0.10623093 0.14307673 0.16921829 0.17502521 0.15387291 0.10662757 0.046922062 -0.0086094514][0.26008636 0.26163343 0.25577649 0.2388673 0.21647035 0.20310315 0.20948346 0.23399259 0.26260689 0.27358094 0.26013306 0.21927929 0.15296784 0.075980656 0.0082650688][0.31612131 0.357816 0.38771573 0.39382789 0.37928745 0.3655079 0.36851221 0.38676783 0.40347677 0.39600307 0.36045039 0.29643348 0.20718721 0.10901867 0.026214693][0.33452457 0.41580006 0.48277897 0.51596665 0.51619679 0.50898373 0.51204664 0.52335954 0.52593368 0.49936384 0.44292253 0.35756552 0.24729665 0.13017438 0.034834094][0.3149997 0.42099422 0.51207209 0.56599289 0.58373046 0.59030682 0.59979177 0.6070835 0.5969699 0.55374432 0.47990933 0.37811145 0.25337443 0.12541658 0.025033792][0.26590133 0.37610286 0.4725419 0.53527361 0.56873965 0.59317034 0.61489576 0.625007 0.60965592 0.55741835 0.47272104 0.3608759 0.22954556 0.0994663 0.0014026185][0.21259986 0.30777249 0.39165178 0.44998893 0.49255374 0.53269422 0.56882304 0.58913028 0.57976919 0.53104371 0.44514295 0.33005843 0.19784431 0.07119114 -0.021505402][0.19614483 0.26527327 0.32471877 0.36825517 0.41044173 0.45817959 0.50514215 0.53795606 0.54119241 0.50366366 0.4226895 0.30884376 0.17830506 0.056065913 -0.032277338][0.23383467 0.27715296 0.31013042 0.33458614 0.36767584 0.41178614 0.46043548 0.50070864 0.51496106 0.48878291 0.41440713 0.30424207 0.17677309 0.058011804 -0.028700799][0.29848555 0.32443571 0.33798927 0.34629878 0.36607185 0.39753255 0.43777916 0.47666958 0.49522951 0.47613972 0.40840125 0.30524439 0.18420942 0.069480665 -0.016776284][0.34145665 0.36041155 0.36447933 0.36285532 0.36949819 0.384343 0.40892416 0.43754622 0.45287636 0.436906 0.37779239 0.28740335 0.17954692 0.074355632 -0.007477738][0.322019 0.33784518 0.33796045 0.33085927 0.32700792 0.32664803 0.33423942 0.34820059 0.3555198 0.34057778 0.29308456 0.22233364 0.13706426 0.051687881 -0.016234819]]...]
INFO - root - 2017-12-10 17:25:33.370299: step 36510, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 63h:10m:46s remains)
INFO - root - 2017-12-10 17:25:41.175023: step 36520, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.754 sec/batch; 61h:57m:19s remains)
INFO - root - 2017-12-10 17:25:49.081586: step 36530, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 63h:06m:24s remains)
INFO - root - 2017-12-10 17:25:56.952501: step 36540, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.754 sec/batch; 61h:57m:45s remains)
INFO - root - 2017-12-10 17:26:04.816774: step 36550, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 64h:18m:41s remains)
INFO - root - 2017-12-10 17:26:12.757014: step 36560, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 66h:10m:55s remains)
INFO - root - 2017-12-10 17:26:20.770917: step 36570, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 66h:47m:55s remains)
INFO - root - 2017-12-10 17:26:28.479582: step 36580, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 63h:44m:12s remains)
INFO - root - 2017-12-10 17:26:36.343426: step 36590, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 65h:31m:03s remains)
INFO - root - 2017-12-10 17:26:44.217007: step 36600, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.754 sec/batch; 61h:56m:02s remains)
2017-12-10 17:26:45.059407: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14143072 0.15367 0.17390318 0.19890413 0.2183467 0.22869653 0.23824166 0.25044644 0.26120085 0.27159503 0.28259659 0.28646806 0.28187612 0.27370489 0.2639167][0.16044809 0.16237685 0.1695416 0.18124689 0.19101661 0.19749881 0.20771272 0.2218501 0.23322715 0.24095628 0.24537459 0.24183837 0.2317616 0.22354631 0.22127801][0.17170262 0.16781354 0.1649839 0.16429023 0.16378987 0.16470225 0.17260741 0.18701506 0.20118506 0.21208851 0.21744283 0.21462227 0.2064382 0.20228072 0.20757036][0.17237099 0.1682115 0.16103269 0.15297835 0.14668632 0.14548497 0.15285847 0.16780055 0.18541606 0.20178577 0.21311139 0.2186389 0.22129777 0.2274311 0.23999822][0.16478044 0.16365342 0.1565022 0.14593174 0.13870662 0.14065552 0.15273601 0.17185432 0.1941447 0.21560962 0.23285808 0.24790651 0.26446071 0.28401873 0.30300963][0.16102576 0.16448855 0.1601714 0.15052848 0.14520773 0.15321921 0.17355573 0.19996686 0.22765101 0.25247777 0.27204615 0.29100448 0.31564802 0.34415826 0.36579916][0.17438526 0.18340282 0.18273923 0.17444031 0.16997804 0.18205464 0.20908985 0.24225089 0.27445927 0.3007634 0.31853691 0.33327925 0.35490742 0.38166535 0.39884758][0.21059574 0.22549841 0.22684413 0.21648356 0.20760761 0.21661754 0.24314754 0.27782935 0.31159702 0.33808297 0.35198978 0.35663235 0.36458468 0.37717697 0.38139558][0.26099104 0.28033575 0.27976117 0.2620528 0.24206679 0.23892403 0.25472867 0.28199887 0.31155741 0.33598635 0.34600818 0.34023753 0.33095112 0.32298645 0.30961952][0.30462441 0.32426038 0.31717181 0.28803736 0.25292662 0.23175709 0.22937347 0.24146535 0.260616 0.2796368 0.28645784 0.27500975 0.25404525 0.22991392 0.20248725][0.31559604 0.32969204 0.31278735 0.27187166 0.22324775 0.18564019 0.16561173 0.16151847 0.16826341 0.18013863 0.18492036 0.17396976 0.15111259 0.12180889 0.0899867][0.27964354 0.2836369 0.25674716 0.20803785 0.15293227 0.10737985 0.077744126 0.063306496 0.060702171 0.066455096 0.070790693 0.065187551 0.049415015 0.02660574 0.0013581086][0.20467806 0.19783135 0.16492735 0.1159583 0.064027473 0.021284141 -0.0076282751 -0.024047231 -0.030418126 -0.028072942 -0.02318584 -0.022297034 -0.02743805 -0.03759842 -0.04964003][0.12024627 0.10668322 0.074469551 0.032926504 -0.0083166109 -0.041029096 -0.06255579 -0.07527563 -0.081166156 -0.08004386 -0.074947178 -0.070131578 -0.067411862 -0.0666502 -0.066317476][0.067556232 0.054768343 0.029822344 -0.00043934633 -0.029932149 -0.053108767 -0.06862662 -0.079106309 -0.085459813 -0.086131826 -0.082306623 -0.077087045 -0.071523473 -0.065537363 -0.058474422]]...]
INFO - root - 2017-12-10 17:26:52.369453: step 36610, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 64h:04m:13s remains)
INFO - root - 2017-12-10 17:27:00.208955: step 36620, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 66h:47m:20s remains)
INFO - root - 2017-12-10 17:27:07.981395: step 36630, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.776 sec/batch; 63h:48m:35s remains)
INFO - root - 2017-12-10 17:27:15.770146: step 36640, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.780 sec/batch; 64h:08m:37s remains)
INFO - root - 2017-12-10 17:27:23.667147: step 36650, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 64h:45m:37s remains)
INFO - root - 2017-12-10 17:27:31.253619: step 36660, loss = 0.71, batch loss = 0.66 (10.5 examples/sec; 0.759 sec/batch; 62h:22m:31s remains)
INFO - root - 2017-12-10 17:27:39.078762: step 36670, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.753 sec/batch; 61h:53m:48s remains)
INFO - root - 2017-12-10 17:27:46.941453: step 36680, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.816 sec/batch; 67h:05m:30s remains)
INFO - root - 2017-12-10 17:27:54.782591: step 36690, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 63h:48m:07s remains)
INFO - root - 2017-12-10 17:28:02.615888: step 36700, loss = 0.67, batch loss = 0.62 (9.9 examples/sec; 0.807 sec/batch; 66h:20m:17s remains)
2017-12-10 17:28:03.434750: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15642951 0.19519426 0.20322546 0.18323821 0.14373267 0.10002886 0.070607364 0.061582059 0.062435761 0.059277155 0.0511638 0.043290727 0.037298854 0.036423281 0.042794969][0.28648219 0.34752467 0.36484805 0.3429701 0.29190791 0.23432228 0.19641881 0.18511826 0.18286942 0.17131874 0.15326646 0.13719788 0.12513843 0.12081186 0.12531872][0.419337 0.50069726 0.52819312 0.51140767 0.46192196 0.40722352 0.37533545 0.36910054 0.36203384 0.33355016 0.29576361 0.26419869 0.24037708 0.22711393 0.22313944][0.52784729 0.62295634 0.65994751 0.65624 0.62395746 0.59042293 0.578346 0.5820415 0.56673759 0.51349759 0.44961461 0.40018776 0.36410421 0.34094056 0.32460257][0.58490044 0.68409425 0.72947389 0.74543476 0.7414006 0.73865253 0.74938232 0.75882548 0.72915995 0.64771974 0.55875087 0.49633887 0.4543303 0.42597067 0.39827684][0.58106488 0.67704678 0.73352015 0.77755433 0.81050748 0.84035683 0.86580926 0.86681008 0.81162727 0.69889271 0.58713913 0.51623672 0.47363058 0.44416323 0.40812564][0.51574272 0.6070112 0.68128276 0.76193154 0.83711481 0.8962974 0.92469555 0.90262961 0.81320703 0.66902715 0.53683561 0.45816565 0.41543368 0.38570881 0.34503859][0.39669335 0.48512939 0.58083749 0.69691467 0.807185 0.88375407 0.90276688 0.85087347 0.72998393 0.56562346 0.42421073 0.34389389 0.3033976 0.27486116 0.23374121][0.26286224 0.34946534 0.46134949 0.59788859 0.72267872 0.79826987 0.79886538 0.72038651 0.58062053 0.414238 0.27973166 0.20658004 0.1713983 0.14640354 0.11106268][0.15139127 0.23571724 0.35296607 0.48975515 0.60577816 0.66355288 0.64272463 0.54858655 0.4076387 0.25682184 0.14199822 0.082089148 0.05393564 0.0348396 0.011288239][0.090095274 0.16983533 0.28017446 0.39966425 0.49041066 0.52219009 0.48453212 0.38939804 0.26386669 0.14062113 0.052316543 0.0084938742 -0.012587898 -0.026029779 -0.037944451][0.075361945 0.14491172 0.23587699 0.32566491 0.38366342 0.39029261 0.34544811 0.2635057 0.16619012 0.077778175 0.019624997 -0.0057273484 -0.01808889 -0.025814783 -0.029940469][0.0830768 0.13484843 0.19656721 0.25021744 0.27498513 0.26193509 0.21883531 0.15864231 0.095028475 0.042566545 0.013790385 0.0070902486 0.0051198048 0.0037935334 0.0039690593][0.087780096 0.11868621 0.14986597 0.17018045 0.16839801 0.14377919 0.10743218 0.068596467 0.03327591 0.0079752263 -0.00079723552 0.0044548153 0.010216836 0.014573834 0.017624898][0.062476292 0.074805759 0.082870618 0.0809204 0.064648516 0.03749042 0.0094708521 -0.014178612 -0.032329042 -0.042920817 -0.042663559 -0.033244785 -0.024227763 -0.017079454 -0.013046856]]...]
INFO - root - 2017-12-10 17:28:11.340050: step 36710, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 63h:55m:33s remains)
INFO - root - 2017-12-10 17:28:19.241079: step 36720, loss = 0.68, batch loss = 0.62 (10.7 examples/sec; 0.746 sec/batch; 61h:16m:01s remains)
INFO - root - 2017-12-10 17:28:27.089250: step 36730, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 65h:55m:10s remains)
INFO - root - 2017-12-10 17:28:34.762896: step 36740, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 66h:16m:47s remains)
INFO - root - 2017-12-10 17:28:42.660758: step 36750, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 65h:44m:23s remains)
INFO - root - 2017-12-10 17:28:50.545127: step 36760, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 63h:36m:17s remains)
INFO - root - 2017-12-10 17:28:58.395322: step 36770, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 64h:32m:44s remains)
INFO - root - 2017-12-10 17:29:06.246974: step 36780, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 65h:08m:39s remains)
INFO - root - 2017-12-10 17:29:13.847169: step 36790, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 63h:30m:03s remains)
INFO - root - 2017-12-10 17:29:21.740245: step 36800, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 65h:09m:54s remains)
2017-12-10 17:29:22.614739: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24593922 0.27544385 0.30108768 0.31622195 0.30783477 0.27291346 0.22560848 0.18405332 0.16128947 0.15844293 0.17434083 0.20485836 0.23939884 0.27962241 0.32095417][0.28387633 0.32195729 0.35635126 0.37753153 0.36662683 0.31823951 0.24779311 0.17925216 0.13171153 0.11500744 0.1317167 0.17460559 0.22723825 0.28399843 0.33645287][0.30154985 0.3460333 0.39140677 0.42767057 0.42951262 0.38429585 0.30271232 0.2111901 0.13416786 0.092086382 0.094589941 0.13581142 0.19806734 0.26942575 0.33401906][0.30163819 0.34736133 0.40236568 0.45962045 0.48948291 0.46771142 0.39344513 0.28962234 0.18407466 0.10839611 0.082017489 0.1053805 0.1657068 0.247861 0.32610813][0.29327631 0.33482563 0.39281425 0.46789491 0.53122461 0.54583156 0.49531767 0.3937878 0.26890019 0.16101027 0.099443428 0.094457842 0.14248246 0.22926114 0.32093424][0.28786588 0.31965524 0.36996746 0.44870314 0.53361052 0.58084548 0.561377 0.47743466 0.35103726 0.22552377 0.13746408 0.10639872 0.13790984 0.22142063 0.31997588][0.28664762 0.305573 0.33941782 0.40574291 0.49141231 0.55429256 0.56112856 0.50376421 0.39474049 0.27342239 0.17875725 0.13519989 0.15306117 0.22614823 0.32059082][0.29294685 0.3000502 0.31537732 0.35948074 0.42599851 0.48192751 0.49924472 0.46529511 0.38428143 0.28761274 0.21039423 0.17459528 0.18868977 0.2489105 0.32888514][0.30729365 0.30893281 0.31151158 0.33399791 0.37222379 0.40462562 0.41428968 0.39088228 0.33492965 0.270685 0.22519772 0.21181752 0.23122352 0.28013271 0.34087348][0.32325703 0.32760286 0.32705083 0.336633 0.35111666 0.35758078 0.35005683 0.3239274 0.2807762 0.24089693 0.22363475 0.23273823 0.25933933 0.29927504 0.34247854][0.33127147 0.34318769 0.34682426 0.35400534 0.35702124 0.34684092 0.32368514 0.28811887 0.2449903 0.21386649 0.21015742 0.23107135 0.26021153 0.29190725 0.3212845][0.32165137 0.34162191 0.35265103 0.36478266 0.36811349 0.35373145 0.32281077 0.27914998 0.23031159 0.19553953 0.18996006 0.20962906 0.23493019 0.25851086 0.27826461][0.29728743 0.32346648 0.34134519 0.3595269 0.36751136 0.35554317 0.32361811 0.27697209 0.22390205 0.18193461 0.16730233 0.1785395 0.19709376 0.21456337 0.23054895][0.27536976 0.30478182 0.32529759 0.34405643 0.35233027 0.34138435 0.31114283 0.26700282 0.21539807 0.17030194 0.14789408 0.15015168 0.1622106 0.17625652 0.19418697][0.28178492 0.30997685 0.32619929 0.3370502 0.33738744 0.32223189 0.29356754 0.255992 0.21156141 0.16897644 0.14237446 0.13683185 0.14295799 0.1560315 0.18096305]]...]
INFO - root - 2017-12-10 17:29:30.487766: step 36810, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 64h:08m:04s remains)
INFO - root - 2017-12-10 17:29:38.219167: step 36820, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 65h:18m:42s remains)
INFO - root - 2017-12-10 17:29:46.201998: step 36830, loss = 0.72, batch loss = 0.66 (9.5 examples/sec; 0.838 sec/batch; 68h:50m:50s remains)
INFO - root - 2017-12-10 17:29:54.056216: step 36840, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 64h:51m:38s remains)
INFO - root - 2017-12-10 17:30:01.938142: step 36850, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 65h:09m:40s remains)
INFO - root - 2017-12-10 17:30:09.786033: step 36860, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 66h:05m:10s remains)
INFO - root - 2017-12-10 17:30:17.683829: step 36870, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 63h:45m:06s remains)
INFO - root - 2017-12-10 17:30:25.316908: step 36880, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.791 sec/batch; 64h:56m:40s remains)
INFO - root - 2017-12-10 17:30:33.116123: step 36890, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 64h:07m:02s remains)
INFO - root - 2017-12-10 17:30:40.933451: step 36900, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.818 sec/batch; 67h:12m:10s remains)
2017-12-10 17:30:41.739472: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26567945 0.24669735 0.21145466 0.17285198 0.14605035 0.1345672 0.1425285 0.18834546 0.25647846 0.30554059 0.31249574 0.29228258 0.26719034 0.24157514 0.21376312][0.27628535 0.25231326 0.21342751 0.17556596 0.15386298 0.14914951 0.16373798 0.21762909 0.29577035 0.35341504 0.36259538 0.33840835 0.30724409 0.275204 0.23887765][0.28465042 0.25846398 0.22264868 0.19407155 0.18419306 0.18902902 0.20660114 0.25589174 0.32594478 0.37753415 0.38331315 0.35536537 0.32248369 0.28983572 0.25278232][0.30553257 0.27987012 0.2498554 0.23271333 0.23636046 0.25248119 0.2745035 0.31603682 0.36902669 0.40200686 0.39378884 0.356146 0.31855378 0.28423753 0.24862841][0.32937729 0.31080791 0.28799346 0.27963194 0.295677 0.3280879 0.3651199 0.40870339 0.44721982 0.45512691 0.42104727 0.36165711 0.30830196 0.26472408 0.22735581][0.36478686 0.36250317 0.34663692 0.3380585 0.35750178 0.4046469 0.46137425 0.51201046 0.538221 0.51896036 0.45432413 0.3674148 0.2917667 0.23449717 0.19485587][0.41619417 0.43665341 0.42614609 0.40737259 0.41697142 0.46633071 0.532516 0.583825 0.59821713 0.55844849 0.472996 0.36798427 0.27624142 0.20917229 0.17156251][0.47247323 0.51482117 0.50810176 0.47614449 0.46643159 0.49986681 0.55523109 0.59577358 0.60058719 0.55303752 0.46396136 0.35756502 0.26228219 0.19414608 0.16587506][0.50940216 0.56368405 0.55879009 0.51675987 0.48490882 0.48854458 0.51425636 0.53237152 0.52718705 0.48151612 0.40364191 0.3126961 0.23149747 0.17910548 0.17325279][0.49147385 0.54234087 0.53614968 0.49102992 0.44400772 0.41815469 0.40887773 0.39968091 0.3828322 0.34303579 0.28498518 0.22227116 0.17160949 0.15094906 0.17637709][0.40432861 0.44181275 0.43285421 0.39068162 0.33863881 0.29403552 0.25837561 0.22725387 0.20147628 0.17027122 0.13666394 0.10945173 0.098599151 0.11593942 0.16978113][0.26920149 0.28906202 0.27577406 0.23820403 0.18916789 0.14056259 0.096244574 0.05899949 0.034974452 0.017935315 0.011153004 0.020268833 0.04878192 0.099738233 0.171435][0.12471753 0.12716816 0.10970023 0.077725127 0.038223784 -0.0010373364 -0.036935985 -0.064343125 -0.075802766 -0.0743435 -0.0558352 -0.01689334 0.041622445 0.1147669 0.1926965][0.0021180727 -0.0087483525 -0.027251728 -0.05135411 -0.07728716 -0.10078054 -0.11999412 -0.12995791 -0.12403339 -0.10298946 -0.062615581 -0.0022893669 0.074758 0.1584684 0.23435716][-0.077838428 -0.094099186 -0.10856043 -0.12316987 -0.13652143 -0.14685465 -0.15213221 -0.1472747 -0.12634858 -0.088528752 -0.03068671 0.043647118 0.12923916 0.21356815 0.28103551]]...]
INFO - root - 2017-12-10 17:30:49.590306: step 36910, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 64h:24m:57s remains)
INFO - root - 2017-12-10 17:30:57.578139: step 36920, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 63h:32m:58s remains)
INFO - root - 2017-12-10 17:31:05.465090: step 36930, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 64h:43m:26s remains)
INFO - root - 2017-12-10 17:31:13.303423: step 36940, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 65h:44m:43s remains)
INFO - root - 2017-12-10 17:31:21.320499: step 36950, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 66h:21m:35s remains)
INFO - root - 2017-12-10 17:31:29.210866: step 36960, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 65h:30m:16s remains)
INFO - root - 2017-12-10 17:31:36.861519: step 36970, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.818 sec/batch; 67h:10m:33s remains)
INFO - root - 2017-12-10 17:31:44.572359: step 36980, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 63h:51m:36s remains)
INFO - root - 2017-12-10 17:31:52.401387: step 36990, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 62h:37m:10s remains)
INFO - root - 2017-12-10 17:32:00.227972: step 37000, loss = 0.67, batch loss = 0.61 (10.3 examples/sec; 0.779 sec/batch; 63h:55m:45s remains)
2017-12-10 17:32:01.071676: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32459295 0.42401949 0.48034132 0.4737964 0.40808085 0.31942213 0.24659781 0.21714424 0.21929069 0.22999631 0.23424512 0.22050492 0.19616285 0.18096216 0.18791296][0.32115251 0.41085678 0.45098233 0.42659706 0.34892586 0.26079336 0.20319988 0.19877015 0.22824679 0.2632091 0.28110242 0.26374772 0.22114524 0.18216816 0.17116609][0.31130269 0.38795149 0.40952969 0.36874026 0.28399679 0.20207429 0.16585301 0.19235645 0.25433505 0.31734309 0.351198 0.33189088 0.26839268 0.19807127 0.16003312][0.31053549 0.38270456 0.39631927 0.35065261 0.26756582 0.19469807 0.17705464 0.22905076 0.31840748 0.40542859 0.45356849 0.43306631 0.348812 0.24365403 0.17080213][0.33234346 0.41223451 0.43121862 0.39259848 0.31662226 0.24899369 0.23776856 0.30057669 0.40421158 0.50449604 0.5608024 0.53993994 0.44080305 0.30501631 0.19560431][0.36823311 0.46385723 0.49876803 0.47534496 0.40798432 0.339805 0.32292232 0.38158408 0.48414096 0.58456308 0.64209169 0.62397975 0.5219208 0.36881068 0.23030685][0.40052998 0.51408219 0.56974673 0.56367779 0.50274277 0.42878655 0.39874771 0.442774 0.5311563 0.6196233 0.67300487 0.66380459 0.57633853 0.42864659 0.27902821][0.41463742 0.53924662 0.60993576 0.61505055 0.55632854 0.47695583 0.43553928 0.46239686 0.52910632 0.59754425 0.64351678 0.64769548 0.58966595 0.4706175 0.33283693][0.40104002 0.5240407 0.59692127 0.60352284 0.544679 0.46500295 0.41965756 0.4329758 0.47737738 0.52428573 0.56303507 0.58308327 0.56212783 0.48665664 0.37855971][0.35941604 0.46920347 0.53208542 0.5322423 0.47277 0.39790639 0.35407704 0.35678923 0.38029641 0.40679023 0.43750778 0.46970326 0.48235828 0.4523848 0.38269275][0.28414491 0.3731899 0.4199667 0.41115811 0.35356563 0.28798896 0.25030223 0.24766737 0.25669581 0.26865461 0.29175204 0.328758 0.36265045 0.3674576 0.33320042][0.17953874 0.24550138 0.27628756 0.26128608 0.21021433 0.15782034 0.13007207 0.12838048 0.13207242 0.13760221 0.15446714 0.1878729 0.22679335 0.2488497 0.23873316][0.067926146 0.11234806 0.13077565 0.11458715 0.074214175 0.036709834 0.019854518 0.0220058 0.025756324 0.029236946 0.039778903 0.062483896 0.09258803 0.11507303 0.11587065][-0.024066575 0.002958294 0.014158275 0.0014272634 -0.026188569 -0.049660761 -0.05750902 -0.052369758 -0.048101321 -0.046305023 -0.042137582 -0.031367261 -0.014587851 0.00013692093 0.0029970228][-0.083740287 -0.0698095 -0.062072467 -0.069032066 -0.084453523 -0.096354477 -0.098232932 -0.092561036 -0.089141376 -0.089418739 -0.090406977 -0.088909835 -0.0834968 -0.077294022 -0.075698823]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 17:32:08.876262: step 37010, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 62h:53m:51s remains)
INFO - root - 2017-12-10 17:32:16.746406: step 37020, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 65h:08m:27s remains)
INFO - root - 2017-12-10 17:32:24.629357: step 37030, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 64h:51m:52s remains)
INFO - root - 2017-12-10 17:32:32.493830: step 37040, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 64h:21m:38s remains)
INFO - root - 2017-12-10 17:32:40.350939: step 37050, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 64h:01m:44s remains)
INFO - root - 2017-12-10 17:32:47.809727: step 37060, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 62h:15m:29s remains)
INFO - root - 2017-12-10 17:32:55.650225: step 37070, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 63h:02m:20s remains)
INFO - root - 2017-12-10 17:33:03.344393: step 37080, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.758 sec/batch; 62h:09m:40s remains)
INFO - root - 2017-12-10 17:33:11.267170: step 37090, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 63h:30m:05s remains)
INFO - root - 2017-12-10 17:33:19.094231: step 37100, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 63h:41m:02s remains)
2017-12-10 17:33:19.941939: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.069967046 0.064458296 0.062570363 0.058075119 0.051162433 0.040853295 0.030078834 0.021964015 0.018019864 0.018119153 0.021270461 0.029948717 0.044738457 0.064843766 0.089389518][0.09417557 0.094841219 0.097921006 0.094712526 0.084799 0.070473716 0.057939231 0.051826246 0.052104786 0.056598268 0.060981136 0.06718915 0.078024417 0.094890058 0.11909761][0.0953492 0.10245427 0.11108952 0.11085614 0.10195145 0.089950971 0.082400016 0.084078424 0.0929062 0.10455836 0.1102593 0.11100699 0.11188019 0.1174181 0.13157465][0.078150876 0.089278385 0.10239635 0.10659596 0.10336466 0.10040848 0.10486948 0.12115568 0.14529669 0.1695971 0.17972599 0.17451629 0.16113903 0.14695139 0.13952349][0.058723137 0.073548831 0.091606162 0.10222104 0.10921998 0.12195132 0.14654115 0.18587525 0.23299849 0.27524477 0.29125407 0.27764997 0.24431308 0.20268162 0.16523182][0.05162894 0.076119207 0.10410868 0.12616394 0.14959335 0.18461926 0.2357948 0.30109876 0.36852553 0.42102718 0.4328073 0.40084419 0.34013641 0.26758116 0.19925685][0.074878171 0.11527489 0.1583664 0.19629788 0.23998602 0.29860479 0.37303656 0.4536168 0.52439928 0.56693888 0.55681705 0.49618265 0.40703809 0.30983639 0.22015885][0.14114499 0.20227545 0.26251191 0.3161484 0.3761557 0.44872171 0.5302819 0.60367119 0.65264761 0.66210073 0.61533111 0.52290142 0.413102 0.30667141 0.2135558][0.22383112 0.30440831 0.37870729 0.44327059 0.51071286 0.58304149 0.65275168 0.69842327 0.70767486 0.67290479 0.5885852 0.47302914 0.35739547 0.25896677 0.17897296][0.27480093 0.36468124 0.44372395 0.51011473 0.57388163 0.63302737 0.67753983 0.68609577 0.65306926 0.5803327 0.47242993 0.35116091 0.2456702 0.16813408 0.11220583][0.26233834 0.34600741 0.417695 0.47592521 0.52691984 0.56573808 0.581685 0.55793315 0.49633667 0.40565535 0.29617092 0.18837819 0.1061114 0.056577336 0.029068956][0.19480208 0.25841895 0.31192538 0.35385215 0.38713714 0.40606782 0.40089291 0.3609679 0.29325604 0.20883891 0.11880548 0.039408091 -0.01192273 -0.032271288 -0.033964265][0.10695844 0.14495958 0.17557928 0.19804585 0.2136385 0.21783736 0.20327634 0.16488078 0.11054267 0.049588971 -0.00948933 -0.055763125 -0.077970631 -0.076355532 -0.062107962][0.028514994 0.043301705 0.053621545 0.059480622 0.062243775 0.059402067 0.046099488 0.020825785 -0.010677273 -0.043078609 -0.071818836 -0.090618394 -0.092968248 -0.080749691 -0.061830081][-0.019759728 -0.020105198 -0.022400018 -0.026163006 -0.029293926 -0.03273749 -0.039624125 -0.050191134 -0.061679866 -0.072184637 -0.080194376 -0.082290746 -0.075342968 -0.061466914 -0.044095311]]...]
INFO - root - 2017-12-10 17:33:27.814210: step 37110, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 64h:23m:27s remains)
INFO - root - 2017-12-10 17:33:35.669493: step 37120, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 66h:21m:59s remains)
INFO - root - 2017-12-10 17:33:43.491483: step 37130, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.796 sec/batch; 65h:19m:31s remains)
INFO - root - 2017-12-10 17:33:51.111916: step 37140, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 62h:31m:58s remains)
INFO - root - 2017-12-10 17:33:58.753463: step 37150, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 62h:41m:10s remains)
INFO - root - 2017-12-10 17:34:06.657176: step 37160, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 65h:17m:45s remains)
INFO - root - 2017-12-10 17:34:14.600055: step 37170, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 65h:44m:09s remains)
INFO - root - 2017-12-10 17:34:22.348343: step 37180, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 62h:53m:52s remains)
INFO - root - 2017-12-10 17:34:30.276062: step 37190, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.788 sec/batch; 64h:39m:19s remains)
INFO - root - 2017-12-10 17:34:38.154693: step 37200, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 65h:43m:55s remains)
2017-12-10 17:34:39.015745: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1228328 0.12008911 0.10242888 0.079744056 0.057605043 0.041984942 0.035080895 0.035536427 0.041882839 0.052204423 0.062659338 0.06933368 0.0702694 0.060218085 0.035824247][0.21578903 0.21805204 0.19713627 0.16815254 0.13989341 0.12110844 0.11434375 0.11637282 0.12254438 0.13106138 0.13898401 0.14197138 0.13802323 0.1197096 0.083056107][0.28282526 0.29210562 0.27301022 0.2446904 0.21789694 0.2026123 0.20076124 0.20672108 0.21182792 0.2142185 0.21377431 0.20814449 0.19595216 0.16780022 0.11967288][0.30664995 0.32345274 0.31156641 0.29183707 0.27482888 0.269262 0.27577633 0.28675851 0.28993887 0.28320771 0.27020067 0.25270566 0.23054278 0.19299534 0.13622099][0.30018404 0.32185778 0.31795517 0.3089458 0.30432808 0.31035131 0.32627863 0.34236729 0.34397537 0.32924929 0.30505195 0.27687195 0.24520008 0.19960222 0.13722633][0.2884599 0.31097844 0.311335 0.30862343 0.3119373 0.32656926 0.35043886 0.37098664 0.37278956 0.35501817 0.32633007 0.29243708 0.25330302 0.20070197 0.13428879][0.28722131 0.30771729 0.30778706 0.30424929 0.30732614 0.32421193 0.35237238 0.37560639 0.37917081 0.36443093 0.33999744 0.30721956 0.26384595 0.20558573 0.13599434][0.29352108 0.31005159 0.30556369 0.29511306 0.29086995 0.3038466 0.3317157 0.35514477 0.36151057 0.35458583 0.34120995 0.31546876 0.27196187 0.2112256 0.14080644][0.28582278 0.29748151 0.28653553 0.26745012 0.25422454 0.26102552 0.28557947 0.30724761 0.31692347 0.31967369 0.31906778 0.30256352 0.26300555 0.20503004 0.13828208][0.24216594 0.24804068 0.23146915 0.20653553 0.18797706 0.19060735 0.21178459 0.23183036 0.24560726 0.25742847 0.26688233 0.25826502 0.22526313 0.17533116 0.11749277][0.15977629 0.16037998 0.14183384 0.11658963 0.09850084 0.10033672 0.1192041 0.13833538 0.15579994 0.17370884 0.18831149 0.18492001 0.16001 0.12204787 0.077203788][0.061358817 0.058034677 0.041743722 0.021108376 0.007378588 0.010004865 0.02652834 0.044449765 0.063534029 0.083347812 0.09845113 0.098043062 0.081751458 0.057237864 0.027142903][-0.020503389 -0.026079513 -0.03763137 -0.051326375 -0.059503596 -0.056114063 -0.042694367 -0.027113881 -0.0089415079 0.0093772281 0.022227069 0.023092099 0.013876985 0.00056005293 -0.016821012][-0.066676952 -0.072832741 -0.079226896 -0.086327329 -0.08984296 -0.0865922 -0.077239126 -0.065732971 -0.051395845 -0.037041176 -0.027440071 -0.026035987 -0.03037734 -0.03635563 -0.044979148][-0.079296805 -0.085087486 -0.087526917 -0.090032674 -0.09071254 -0.088212021 -0.082819648 -0.075983435 -0.066964515 -0.057670366 -0.051342126 -0.04978054 -0.051248722 -0.053301487 -0.056838725]]...]
INFO - root - 2017-12-10 17:34:46.860270: step 37210, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 64h:19m:32s remains)
INFO - root - 2017-12-10 17:34:54.583157: step 37220, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 64h:44m:54s remains)
INFO - root - 2017-12-10 17:35:02.177535: step 37230, loss = 0.71, batch loss = 0.65 (13.1 examples/sec; 0.611 sec/batch; 50h:05m:17s remains)
INFO - root - 2017-12-10 17:35:09.944862: step 37240, loss = 0.71, batch loss = 0.66 (10.5 examples/sec; 0.765 sec/batch; 62h:42m:13s remains)
INFO - root - 2017-12-10 17:35:17.864125: step 37250, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 66h:04m:23s remains)
INFO - root - 2017-12-10 17:35:25.763041: step 37260, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 65h:24m:18s remains)
INFO - root - 2017-12-10 17:35:33.504382: step 37270, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 62h:58m:44s remains)
INFO - root - 2017-12-10 17:35:41.266000: step 37280, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 62h:00m:57s remains)
INFO - root - 2017-12-10 17:35:49.138054: step 37290, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 65h:31m:30s remains)
INFO - root - 2017-12-10 17:35:56.832712: step 37300, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 64h:19m:22s remains)
2017-12-10 17:35:57.689475: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25203949 0.25808915 0.25614673 0.26097468 0.27075759 0.28445461 0.30000621 0.30798927 0.30681553 0.301884 0.29675624 0.28774145 0.27296555 0.2608487 0.2551702][0.26341808 0.27181581 0.26825473 0.2694726 0.27584097 0.28963318 0.31004292 0.32548463 0.32936171 0.32433456 0.3128832 0.29367518 0.26894242 0.25129312 0.24531312][0.26265132 0.27061975 0.26486 0.26289609 0.26656803 0.27951351 0.30193391 0.32284054 0.33313453 0.33054468 0.31512418 0.28765222 0.25596884 0.23652029 0.23387209][0.26139793 0.26831242 0.26313636 0.26108623 0.26405525 0.27561256 0.29631549 0.3167538 0.32816285 0.32559994 0.30762067 0.27642635 0.24400422 0.2280602 0.23171891][0.26856226 0.27697319 0.27629879 0.27808902 0.28236267 0.29201213 0.30762234 0.32098916 0.32450756 0.31487128 0.29284182 0.26261073 0.23740108 0.23171824 0.24510312][0.27109987 0.28087461 0.28482577 0.29181775 0.2985616 0.30634958 0.31632924 0.32148904 0.31480452 0.29572073 0.26908284 0.24232389 0.22821765 0.23619832 0.26103181][0.24923195 0.25832573 0.26479328 0.27633759 0.28566921 0.291944 0.2977879 0.29838389 0.28729078 0.26528603 0.23883089 0.21708022 0.21194138 0.2295599 0.26171741][0.20089653 0.20806353 0.21618035 0.23152721 0.24302448 0.24752766 0.24920687 0.24653971 0.23484831 0.21448767 0.19122791 0.17353934 0.17245698 0.19287577 0.22582436][0.1307845 0.13558522 0.14488487 0.16286884 0.17611919 0.17952201 0.17790978 0.17269321 0.16138932 0.14391865 0.12507454 0.11146004 0.11264818 0.13254716 0.16247772][0.054916825 0.058427326 0.0689865 0.088479251 0.103523 0.10798065 0.10558712 0.099156484 0.087937385 0.072367966 0.057178378 0.047358684 0.050355129 0.068454742 0.093576647][-0.011724317 -0.0096741552 0.00065168767 0.018677937 0.033277258 0.038762514 0.037171938 0.031133505 0.020837609 0.0077036703 -0.0034268962 -0.0092165237 -0.0052940855 0.0089199189 0.027088838][-0.058768194 -0.058582272 -0.050367448 -0.036743023 -0.025527837 -0.020762296 -0.021616567 -0.026290445 -0.034109835 -0.042990953 -0.048775941 -0.0500589 -0.0457223 -0.036363307 -0.025987064][-0.080990605 -0.082305551 -0.076321661 -0.067191042 -0.059741929 -0.056364652 -0.05688351 -0.060344342 -0.06577985 -0.071099319 -0.073313557 -0.072075628 -0.0680116 -0.062184263 -0.057035234][-0.083719365 -0.086074844 -0.08219634 -0.07664381 -0.072161563 -0.069957539 -0.069976479 -0.071848579 -0.074849963 -0.077532336 -0.078103535 -0.076426506 -0.073239475 -0.069368474 -0.066331908][-0.074879691 -0.077177316 -0.074701242 -0.07143236 -0.068717457 -0.067083612 -0.066533677 -0.066970907 -0.068127871 -0.069331236 -0.0696917 -0.068978719 -0.067414969 -0.065322563 -0.063496359]]...]
INFO - root - 2017-12-10 17:36:05.587046: step 37310, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 64h:14m:29s remains)
INFO - root - 2017-12-10 17:36:13.193393: step 37320, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 63h:23m:28s remains)
INFO - root - 2017-12-10 17:36:21.015645: step 37330, loss = 0.72, batch loss = 0.66 (9.7 examples/sec; 0.821 sec/batch; 67h:18m:53s remains)
INFO - root - 2017-12-10 17:36:28.863853: step 37340, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.769 sec/batch; 63h:04m:53s remains)
INFO - root - 2017-12-10 17:36:36.708216: step 37350, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 66h:12m:15s remains)
INFO - root - 2017-12-10 17:36:44.554576: step 37360, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.798 sec/batch; 65h:27m:12s remains)
INFO - root - 2017-12-10 17:36:52.390877: step 37370, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.821 sec/batch; 67h:20m:07s remains)
INFO - root - 2017-12-10 17:37:00.088227: step 37380, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 62h:59m:51s remains)
INFO - root - 2017-12-10 17:37:07.940822: step 37390, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 64h:00m:34s remains)
INFO - root - 2017-12-10 17:37:15.753469: step 37400, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 62h:39m:30s remains)
2017-12-10 17:37:16.598189: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.074933305 0.047615331 0.021417985 0.0028194275 -0.0079418411 -0.015813831 -0.020694649 -0.015411343 0.011962746 0.054821707 0.10987648 0.17104478 0.23272771 0.27998993 0.30166715][0.031363145 0.01118686 -0.0084438864 -0.023336576 -0.033544183 -0.03978603 -0.038681608 -0.023836458 0.013612069 0.063939065 0.12281874 0.18071155 0.23322423 0.27175185 0.28784871][0.018155282 0.0096545108 0.00094301609 -0.006779816 -0.013288434 -0.015982095 -0.00928973 0.011788884 0.0508134 0.09645585 0.14388737 0.1844372 0.21710263 0.24039583 0.25148898][0.040122118 0.04627293 0.053058449 0.058982719 0.06405355 0.071162052 0.085495368 0.11086522 0.14505656 0.17568378 0.19940351 0.21317381 0.22085592 0.22587305 0.23185894][0.094688512 0.11877255 0.14534549 0.17160209 0.19606911 0.2182795 0.24045341 0.26699334 0.29132032 0.30086398 0.29552135 0.28012735 0.26362193 0.25126049 0.25130424][0.17083162 0.21636511 0.26632857 0.31586313 0.36153585 0.39839783 0.42510715 0.44779396 0.45687824 0.44269693 0.40903175 0.36915442 0.33531228 0.3115482 0.30860958][0.24926198 0.31405911 0.38282797 0.44951737 0.510278 0.55608523 0.58203578 0.59612107 0.58668214 0.55071735 0.49632809 0.44272998 0.40340537 0.38016328 0.38390598][0.30571303 0.37656438 0.44999364 0.52200681 0.58763051 0.63417685 0.65521586 0.659937 0.63503033 0.58430362 0.52054054 0.46523836 0.43182373 0.42028132 0.4384822][0.32443142 0.3860369 0.44925848 0.51374614 0.57258219 0.61133766 0.62531424 0.62334365 0.59048134 0.53426212 0.47093728 0.42106676 0.39693043 0.39789 0.42786512][0.30666739 0.34830308 0.39103708 0.43724191 0.47850338 0.50149608 0.50597835 0.49963009 0.46615964 0.41349068 0.35835749 0.31747755 0.30014607 0.30573419 0.33611584][0.25435388 0.27278072 0.29220432 0.31575921 0.33536598 0.34125927 0.33728626 0.32895491 0.30045441 0.25817555 0.2166317 0.18717238 0.17478338 0.17819495 0.19900203][0.181753 0.17908986 0.17808068 0.18209532 0.18477769 0.18064012 0.17486671 0.16920304 0.14995819 0.12162382 0.095588244 0.077988468 0.069665194 0.06859231 0.076358885][0.10693052 0.089817189 0.076008961 0.06873817 0.063738033 0.058557957 0.057201404 0.057535343 0.048948012 0.034476995 0.022512265 0.015665611 0.011595274 0.0071937717 0.0038183404][0.044619475 0.021575874 0.0031965873 -0.0078619281 -0.013468462 -0.014377394 -0.0090979151 -0.0026461764 -0.0024561253 -0.0062275021 -0.0078699514 -0.0068635354 -0.0076499046 -0.014021534 -0.024499796][0.00056888681 -0.022727242 -0.041153867 -0.052260309 -0.055504192 -0.050939109 -0.0398321 -0.028647935 -0.021932863 -0.017871739 -0.012378296 -0.0064007742 -0.0051079579 -0.012168458 -0.025638757]]...]
INFO - root - 2017-12-10 17:37:24.292651: step 37410, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 64h:55m:32s remains)
INFO - root - 2017-12-10 17:37:32.046298: step 37420, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 62h:55m:48s remains)
INFO - root - 2017-12-10 17:37:39.798651: step 37430, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 63h:07m:55s remains)
INFO - root - 2017-12-10 17:37:47.800105: step 37440, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.820 sec/batch; 67h:12m:03s remains)
INFO - root - 2017-12-10 17:37:55.575382: step 37450, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 64h:17m:31s remains)
INFO - root - 2017-12-10 17:38:03.236658: step 37460, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 64h:53m:54s remains)
INFO - root - 2017-12-10 17:38:11.142295: step 37470, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 64h:48m:06s remains)
INFO - root - 2017-12-10 17:38:18.968707: step 37480, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.767 sec/batch; 62h:53m:14s remains)
INFO - root - 2017-12-10 17:38:26.767226: step 37490, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 64h:04m:24s remains)
INFO - root - 2017-12-10 17:38:34.444367: step 37500, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 63h:52m:35s remains)
2017-12-10 17:38:35.260196: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.053955603 0.044093691 0.031511191 0.021263879 0.016519254 0.015983744 0.020635024 0.034460258 0.058011465 0.087499544 0.11465008 0.13219404 0.13586652 0.12241632 0.090191551][0.12747619 0.1254078 0.11921401 0.11370467 0.11369379 0.1177274 0.12667347 0.14635886 0.17894138 0.21898682 0.25588131 0.28031623 0.28390998 0.26011872 0.20752119][0.19425191 0.20633048 0.21483937 0.22230528 0.23313737 0.24482071 0.2582325 0.28112319 0.3168461 0.35959208 0.39811707 0.42252156 0.42051485 0.3837046 0.31233034][0.24174441 0.27127296 0.29886568 0.32417372 0.34880278 0.36934787 0.38749731 0.41181773 0.44534451 0.48200753 0.51177281 0.52592045 0.50987178 0.45660982 0.36941424][0.27617645 0.32227066 0.36686844 0.40864182 0.44568387 0.47339827 0.4945634 0.517637 0.54396659 0.56588089 0.575595 0.56781352 0.53012788 0.45921379 0.36146975][0.30553016 0.36444941 0.4188633 0.47060809 0.51470226 0.54471081 0.56553924 0.58523792 0.60238504 0.6059947 0.59050959 0.55612051 0.49556625 0.41058186 0.30972689][0.351639 0.41540286 0.46628487 0.51372266 0.55315232 0.57808524 0.59578037 0.61228406 0.62179118 0.60945088 0.57056934 0.5114702 0.43187922 0.33958811 0.24363489][0.42726973 0.48752996 0.52104568 0.54666591 0.56530631 0.57631838 0.58831018 0.60191125 0.60537404 0.58123755 0.52581531 0.44980261 0.35888308 0.26631996 0.18103966][0.50970072 0.55980068 0.56843334 0.56362844 0.55452883 0.54918724 0.55567288 0.56736892 0.56742448 0.53593481 0.47103316 0.38674819 0.29330513 0.20688272 0.13534053][0.55926359 0.594115 0.57963568 0.55055994 0.52228421 0.50753695 0.51110494 0.5204609 0.51652747 0.47893938 0.40905812 0.32463035 0.23852281 0.16574687 0.11037943][0.56184036 0.58121228 0.55217117 0.51283354 0.479465 0.46352595 0.46456692 0.46702242 0.45393327 0.40814853 0.33540192 0.25725403 0.18692653 0.13404487 0.096239172][0.55611086 0.56672645 0.53423792 0.496284 0.46595895 0.44890079 0.43892872 0.42233145 0.38899031 0.32888827 0.25224176 0.18248838 0.13072737 0.098524675 0.077159747][0.562086 0.57568812 0.55021197 0.52055627 0.49427214 0.46990508 0.43837702 0.3921828 0.33133155 0.25409383 0.17473982 0.11578929 0.083197683 0.069862656 0.06248058][0.5887059 0.61278772 0.59682667 0.57337147 0.54572088 0.50725687 0.450108 0.37438437 0.28932655 0.19979768 0.12271985 0.077253141 0.063143849 0.066403538 0.071038075][0.63970166 0.67104125 0.65781891 0.63022995 0.59036368 0.5310725 0.44955006 0.35276 0.25491524 0.1633819 0.095394179 0.065645158 0.068817586 0.086751416 0.10168719]]...]
INFO - root - 2017-12-10 17:38:43.087575: step 37510, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 65h:55m:40s remains)
INFO - root - 2017-12-10 17:38:50.929722: step 37520, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.766 sec/batch; 62h:46m:07s remains)
INFO - root - 2017-12-10 17:38:58.721358: step 37530, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 64h:59m:39s remains)
INFO - root - 2017-12-10 17:39:06.405211: step 37540, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 64h:04m:22s remains)
INFO - root - 2017-12-10 17:39:14.393878: step 37550, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 63h:24m:24s remains)
INFO - root - 2017-12-10 17:39:22.171757: step 37560, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 63h:39m:03s remains)
INFO - root - 2017-12-10 17:39:30.053479: step 37570, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.777 sec/batch; 63h:37m:18s remains)
INFO - root - 2017-12-10 17:39:37.923315: step 37580, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 64h:03m:27s remains)
INFO - root - 2017-12-10 17:39:45.560589: step 37590, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.764 sec/batch; 62h:37m:10s remains)
INFO - root - 2017-12-10 17:39:53.446596: step 37600, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 63h:30m:11s remains)
2017-12-10 17:39:54.280315: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12625697 0.14259531 0.17596427 0.21766545 0.24699129 0.24637786 0.20981298 0.14271206 0.061843134 -0.0091631263 -0.0554584 -0.075998008 -0.079854392 -0.073757313 -0.0632381][0.15490372 0.18115254 0.22521068 0.27577248 0.31091943 0.31207904 0.27129671 0.19588584 0.10498058 0.022791497 -0.0346527 -0.064682193 -0.0753385 -0.073310569 -0.065008089][0.16106941 0.19972275 0.25739017 0.32088393 0.36719164 0.37682453 0.33750027 0.25752053 0.15814747 0.064531155 -0.005583954 -0.046916742 -0.065690137 -0.068890065 -0.063713834][0.14212552 0.19115236 0.26218569 0.34193254 0.40549594 0.43020365 0.39807075 0.31718904 0.21042685 0.10477652 0.021248329 -0.0312603 -0.057432204 -0.065006785 -0.062109385][0.11026272 0.16288553 0.24018568 0.33197898 0.41209763 0.45312661 0.43130609 0.35361642 0.24339268 0.12914525 0.035648108 -0.024252221 -0.054359738 -0.063682146 -0.061261646][0.087674811 0.13502604 0.20718488 0.29924533 0.3856926 0.43552709 0.42170012 0.34980044 0.24213386 0.12709288 0.031624075 -0.028704606 -0.057711869 -0.065451518 -0.061577111][0.090396047 0.12641385 0.18393561 0.2643556 0.34351173 0.38941818 0.37666741 0.31050986 0.2103537 0.10223224 0.01296259 -0.04094024 -0.064568132 -0.06861639 -0.062457241][0.12372741 0.15093324 0.1928051 0.2553823 0.31614244 0.34608197 0.32500228 0.25983298 0.16698109 0.06897112 -0.0097810524 -0.054133508 -0.071035556 -0.071293384 -0.06330055][0.17993009 0.20547451 0.23797743 0.28438178 0.32343233 0.33131295 0.29442969 0.22344011 0.13293393 0.042694524 -0.026245916 -0.062301241 -0.07414753 -0.072112143 -0.063588552][0.24255683 0.27183929 0.30239943 0.33877131 0.36072424 0.3486931 0.29476494 0.21327297 0.11890361 0.030994676 -0.032304917 -0.0640007 -0.073840685 -0.071488269 -0.063604921][0.29689345 0.33058155 0.3627426 0.39355531 0.4043088 0.37813479 0.31080738 0.21861854 0.11712745 0.027260935 -0.034119438 -0.06415274 -0.07356929 -0.071285509 -0.063772887][0.3265644 0.3612482 0.39408007 0.42111391 0.4257327 0.39261281 0.31892282 0.22095756 0.11445027 0.022436677 -0.038059525 -0.066635378 -0.075155884 -0.072218329 -0.06414026][0.32375878 0.35261077 0.38139063 0.40422419 0.40702033 0.37515083 0.30501425 0.21034279 0.10545001 0.014605691 -0.04408443 -0.070889071 -0.078214459 -0.074271962 -0.065247156][0.29428115 0.31074131 0.33045876 0.34934136 0.35494566 0.33196449 0.27400008 0.1906908 0.093991667 0.00783162 -0.048263714 -0.07366278 -0.080350704 -0.075838536 -0.066148855][0.25882471 0.26080489 0.27017727 0.28848603 0.30246153 0.29377851 0.25141519 0.18131115 0.093533792 0.010999836 -0.044913676 -0.071478 -0.079514831 -0.075905584 -0.0665251]]...]
INFO - root - 2017-12-10 17:40:02.175848: step 37610, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 64h:02m:52s remains)
INFO - root - 2017-12-10 17:40:09.977071: step 37620, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 63h:37m:31s remains)
INFO - root - 2017-12-10 17:40:17.850592: step 37630, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.798 sec/batch; 65h:22m:24s remains)
INFO - root - 2017-12-10 17:40:25.712685: step 37640, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 64h:09m:34s remains)
INFO - root - 2017-12-10 17:40:33.565016: step 37650, loss = 0.73, batch loss = 0.67 (10.2 examples/sec; 0.787 sec/batch; 64h:27m:58s remains)
INFO - root - 2017-12-10 17:40:41.534376: step 37660, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.824 sec/batch; 67h:31m:03s remains)
INFO - root - 2017-12-10 17:40:49.449783: step 37670, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 64h:10m:47s remains)
INFO - root - 2017-12-10 17:40:57.294575: step 37680, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.772 sec/batch; 63h:11m:17s remains)
INFO - root - 2017-12-10 17:41:05.201504: step 37690, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.762 sec/batch; 62h:26m:31s remains)
INFO - root - 2017-12-10 17:41:12.902292: step 37700, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 64h:18m:45s remains)
2017-12-10 17:41:13.789778: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.067189686 0.06768398 0.0600546 0.049209591 0.039123971 0.032354135 0.026021367 0.016593449 0.007781256 0.0018877536 -0.00072310644 -0.001607051 -0.0013206464 -0.00015324783 0.00039231684][0.074718252 0.074570447 0.066299655 0.055690728 0.047181163 0.043416418 0.039660495 0.030842278 0.02127347 0.01345027 0.008112587 0.0029441321 -0.0018615285 -0.004826548 -0.0066946833][0.079359747 0.082933843 0.0801124 0.076625124 0.076476656 0.081739567 0.085157044 0.080179125 0.071649134 0.063276052 0.056345072 0.047347207 0.036250513 0.027437354 0.020316621][0.084555119 0.098146766 0.109997 0.1239687 0.14183234 0.16368191 0.1781894 0.17799015 0.16991095 0.16124299 0.15400748 0.14293975 0.12805504 0.11499244 0.10126974][0.094764873 0.12594959 0.16290873 0.20558774 0.25114655 0.29533857 0.32082918 0.32112354 0.3084054 0.29685095 0.28986046 0.28098115 0.26964957 0.25875622 0.24008061][0.11014654 0.16421142 0.23325586 0.31135151 0.38973281 0.45744222 0.49042 0.48466495 0.46014872 0.44032094 0.43105659 0.42614353 0.42429474 0.42235115 0.40349343][0.12319747 0.19791862 0.296405 0.40711293 0.51514089 0.60196882 0.63866448 0.62397242 0.58396196 0.55002141 0.53267688 0.52916294 0.538365 0.54932296 0.53663725][0.1249086 0.20924714 0.32352427 0.45268816 0.57758439 0.67435038 0.71327919 0.69393158 0.64237255 0.59345108 0.56431812 0.55982715 0.58023965 0.60577935 0.60379267][0.11831003 0.19704054 0.30655068 0.43236041 0.5551089 0.65015143 0.69151384 0.676963 0.6257692 0.57226932 0.54033244 0.54383063 0.58255094 0.62618041 0.63720715][0.1101089 0.16949826 0.2548424 0.35558927 0.45685917 0.53849357 0.58185768 0.58135051 0.545704 0.50562996 0.4894883 0.51709527 0.58379436 0.6491788 0.67505533][0.10953205 0.14395389 0.19479056 0.25709656 0.32367536 0.38272971 0.42484319 0.44120455 0.42985621 0.41668326 0.43081802 0.49234232 0.58997571 0.67689496 0.718449][0.12703061 0.14011563 0.15681311 0.17716382 0.20294428 0.23269565 0.26627308 0.29370841 0.30611023 0.32399502 0.3722617 0.46560282 0.58798105 0.692906 0.750434][0.15166491 0.15287745 0.14535761 0.13154395 0.12072762 0.12085627 0.13910148 0.16711 0.19417912 0.23498155 0.30576608 0.41406611 0.5450601 0.66013139 0.73325807][0.15987441 0.15862334 0.14023316 0.10734772 0.073549561 0.052398004 0.05466418 0.07485991 0.1041088 0.15148576 0.22406362 0.3248097 0.44562483 0.55955988 0.64263421][0.13444783 0.13564266 0.11737476 0.080194116 0.038102839 0.0063262712 -0.0037334063 0.0051090932 0.026358316 0.064346954 0.11996747 0.1954214 0.29088208 0.39090246 0.47287151]]...]
INFO - root - 2017-12-10 17:41:21.651363: step 37710, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 65h:07m:41s remains)
INFO - root - 2017-12-10 17:41:29.583183: step 37720, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.785 sec/batch; 64h:17m:46s remains)
INFO - root - 2017-12-10 17:41:37.518013: step 37730, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 63h:43m:50s remains)
INFO - root - 2017-12-10 17:41:45.362854: step 37740, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 64h:13m:05s remains)
INFO - root - 2017-12-10 17:41:53.266892: step 37750, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 64h:34m:57s remains)
INFO - root - 2017-12-10 17:42:01.149314: step 37760, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 63h:19m:16s remains)
INFO - root - 2017-12-10 17:42:08.859142: step 37770, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 64h:01m:12s remains)
INFO - root - 2017-12-10 17:42:16.584378: step 37780, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 64h:24m:15s remains)
INFO - root - 2017-12-10 17:42:24.416054: step 37790, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 65h:47m:19s remains)
INFO - root - 2017-12-10 17:42:32.374762: step 37800, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 62h:46m:55s remains)
2017-12-10 17:42:33.169688: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.083722048 0.072260447 0.064780124 0.071451187 0.096861333 0.1340986 0.16888422 0.19087562 0.19764656 0.19059546 0.17480555 0.15859702 0.15560889 0.17476225 0.20324863][0.10613007 0.094873674 0.082109839 0.080839269 0.098976493 0.13005663 0.16016352 0.17961775 0.18607263 0.18036531 0.16532682 0.14888693 0.14544708 0.1631753 0.19033541][0.12492181 0.11233884 0.09237361 0.080616042 0.088073432 0.10940842 0.13242702 0.15031344 0.16040087 0.16141264 0.15246342 0.13982463 0.13673545 0.15080422 0.17403173][0.14310621 0.12816231 0.10219221 0.081925936 0.08100456 0.095834188 0.11568657 0.13600506 0.15301521 0.1629266 0.16074055 0.14964625 0.14244781 0.1479563 0.16324627][0.15512532 0.13873899 0.11091893 0.087912969 0.085208148 0.10096139 0.12473429 0.15231657 0.17769793 0.19480968 0.19574571 0.18090977 0.1641164 0.15685689 0.16151373][0.15367676 0.1393421 0.11597554 0.097583145 0.10027048 0.12308405 0.15549585 0.1923452 0.22442295 0.24429487 0.24314387 0.2206509 0.19227555 0.17233747 0.16712275][0.1379199 0.13133585 0.11859241 0.10993905 0.12073107 0.15157044 0.19238728 0.23658931 0.27177969 0.28980848 0.2823728 0.250572 0.21140869 0.18176511 0.170325][0.11916964 0.12540554 0.126815 0.12883376 0.14546995 0.17986688 0.22387549 0.27027202 0.30398241 0.31606326 0.30007574 0.259668 0.2128737 0.17840879 0.16589156][0.11557735 0.13511737 0.14905688 0.15824381 0.17554115 0.20658849 0.24627912 0.28742513 0.31394961 0.31703436 0.29220095 0.24542005 0.19557652 0.16209595 0.15377076][0.1303795 0.15758868 0.17673324 0.18646455 0.19832814 0.21929401 0.24788007 0.27786538 0.2939747 0.28837308 0.25835481 0.21122345 0.16497788 0.13826989 0.13869347][0.15551934 0.18044344 0.19473462 0.19790983 0.20022802 0.20885737 0.22519398 0.24403347 0.25214341 0.24360114 0.21651566 0.17689024 0.13981757 0.12268606 0.13214134][0.18316905 0.19629665 0.19711801 0.18875851 0.18064618 0.17944199 0.18742278 0.19966099 0.20621984 0.2033931 0.18783933 0.16144811 0.13527687 0.12628806 0.14083283][0.20498316 0.19991837 0.18257345 0.16108544 0.14446844 0.13827471 0.14361341 0.15534662 0.16736238 0.17724745 0.17838326 0.16732757 0.15114954 0.14650661 0.16017309][0.21402037 0.1892883 0.15409333 0.12171888 0.1006677 0.0953597 0.10423545 0.12121662 0.14339039 0.16812727 0.18524431 0.18647346 0.17559323 0.16897358 0.17483476][0.20410487 0.16450536 0.11788831 0.080281012 0.059083164 0.058030527 0.073005468 0.09682063 0.12897858 0.16567127 0.19410044 0.20237136 0.19175865 0.17783308 0.17047174]]...]
INFO - root - 2017-12-10 17:42:41.080422: step 37810, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 65h:44m:07s remains)
INFO - root - 2017-12-10 17:42:48.939792: step 37820, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 65h:00m:21s remains)
INFO - root - 2017-12-10 17:42:56.739518: step 37830, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 62h:29m:10s remains)
INFO - root - 2017-12-10 17:43:04.629762: step 37840, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 65h:35m:59s remains)
INFO - root - 2017-12-10 17:43:12.427688: step 37850, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.768 sec/batch; 62h:53m:54s remains)
INFO - root - 2017-12-10 17:43:20.066252: step 37860, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 65h:09m:07s remains)
INFO - root - 2017-12-10 17:43:28.046125: step 37870, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 64h:07m:27s remains)
INFO - root - 2017-12-10 17:43:36.033867: step 37880, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 66h:22m:48s remains)
INFO - root - 2017-12-10 17:43:43.871878: step 37890, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 64h:33m:16s remains)
INFO - root - 2017-12-10 17:43:51.808652: step 37900, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 64h:38m:39s remains)
2017-12-10 17:43:52.706712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.033316605 0.017730866 0.10094759 0.20969512 0.31467173 0.38321528 0.41802329 0.43063974 0.43122303 0.44000545 0.46562991 0.49940112 0.51487046 0.48503751 0.41774666][-0.036124498 0.016137756 0.10300964 0.22047924 0.33765972 0.417754 0.46113333 0.47823 0.4788363 0.48215768 0.49775729 0.52296436 0.53320122 0.50240403 0.43448928][-0.040418025 0.010769722 0.097771131 0.21866299 0.34242165 0.43024069 0.47917774 0.49689823 0.49371547 0.48758969 0.4900547 0.50362378 0.50860518 0.48145792 0.4227908][-0.045044933 0.0060550082 0.094778806 0.21845998 0.34469697 0.43406078 0.4828113 0.4956786 0.48394459 0.46661073 0.45889306 0.4659819 0.47025648 0.45078334 0.40691286][-0.046604242 0.0070674364 0.0992198 0.2241544 0.34901166 0.43563142 0.48157415 0.48939365 0.47028238 0.44491318 0.43155798 0.43668392 0.44290185 0.43224448 0.40427747][-0.044403773 0.013503136 0.10865219 0.23375924 0.35720596 0.44342467 0.49179456 0.502915 0.48518702 0.460332 0.44797084 0.45574427 0.46426961 0.45793793 0.43808815][-0.042133212 0.022710709 0.12671633 0.2605868 0.39155862 0.48471171 0.53880614 0.5533039 0.53678578 0.51511431 0.50881904 0.52461171 0.538593 0.53382957 0.51187891][-0.0376915 0.039136324 0.16183716 0.31523004 0.46167603 0.56387115 0.61917144 0.62904239 0.60789037 0.59010065 0.59550381 0.62325025 0.64201933 0.63089055 0.59140509][-0.027414674 0.063364506 0.2070059 0.38121927 0.5417636 0.64773649 0.69472772 0.69021827 0.65714329 0.63934278 0.65442526 0.68976015 0.70714641 0.68251687 0.61789626][-0.020528032 0.076539114 0.2289881 0.40964624 0.57185936 0.67293447 0.70610881 0.68388319 0.63493907 0.60904419 0.62181818 0.65321028 0.66268241 0.62630332 0.54659945][-0.020726593 0.070881024 0.21435191 0.3816542 0.52907157 0.615507 0.6331321 0.5956431 0.53269875 0.49411172 0.49457195 0.51388371 0.51511461 0.47684106 0.40063217][-0.019768717 0.062732294 0.1894173 0.33214679 0.45292437 0.51516765 0.51269972 0.45876515 0.38208139 0.32910427 0.31446448 0.32192105 0.32157552 0.29496941 0.24059784][-0.012889672 0.062105853 0.17038532 0.28337008 0.37036622 0.40214571 0.37670809 0.30748522 0.22117066 0.15892091 0.13461086 0.13577315 0.13960047 0.13243085 0.11186128][-0.011351891 0.052500919 0.13812558 0.2185664 0.27115065 0.27521932 0.23324084 0.15905264 0.074605651 0.012892551 -0.013246012 -0.012181576 -3.2176973e-05 0.016063314 0.036336374][-0.020599078 0.027953338 0.08922141 0.14052203 0.16691221 0.15457526 0.10858826 0.041492376 -0.03054207 -0.083901234 -0.10665053 -0.10154337 -0.078425869 -0.038739137 0.019667793]]...]
INFO - root - 2017-12-10 17:44:00.529094: step 37910, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 62h:33m:40s remains)
INFO - root - 2017-12-10 17:44:08.378510: step 37920, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 66h:19m:10s remains)
INFO - root - 2017-12-10 17:44:16.266407: step 37930, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 66h:09m:10s remains)
INFO - root - 2017-12-10 17:44:23.911681: step 37940, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 63h:36m:40s remains)
INFO - root - 2017-12-10 17:44:31.708415: step 37950, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 65h:41m:16s remains)
INFO - root - 2017-12-10 17:44:39.574784: step 37960, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 64h:36m:39s remains)
INFO - root - 2017-12-10 17:44:47.485880: step 37970, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 66h:04m:10s remains)
INFO - root - 2017-12-10 17:44:55.324359: step 37980, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 64h:25m:59s remains)
INFO - root - 2017-12-10 17:45:03.207949: step 37990, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 63h:24m:54s remains)
INFO - root - 2017-12-10 17:45:11.025779: step 38000, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 63h:31m:59s remains)
2017-12-10 17:45:11.808607: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25638413 0.27630055 0.28843135 0.28745076 0.27337825 0.25224859 0.2324634 0.21842557 0.21163186 0.21250339 0.22264783 0.24168269 0.26841024 0.29552796 0.30990192][0.30401459 0.3304095 0.34879795 0.35277566 0.34018612 0.31619626 0.28909722 0.26549149 0.24906114 0.24069011 0.24209449 0.25363556 0.27494639 0.30008888 0.31500602][0.34415874 0.37159452 0.39026624 0.39392516 0.37996483 0.35316658 0.32101056 0.29120809 0.26919135 0.25503287 0.24948913 0.25379843 0.26869124 0.290089 0.30440864][0.36105245 0.38862807 0.40864077 0.41387615 0.40164909 0.37650612 0.34528497 0.31603143 0.29446197 0.27897933 0.2683017 0.26486257 0.2722213 0.28999153 0.3058238][0.33872154 0.37225622 0.4016085 0.41746598 0.41628537 0.40219563 0.38095772 0.35934252 0.34218398 0.32701394 0.31037384 0.2965115 0.29371703 0.30612013 0.32126191][0.30024779 0.3495121 0.40007362 0.43959761 0.46167663 0.46673569 0.45672208 0.4369612 0.41530928 0.39264861 0.36539766 0.3403348 0.3278957 0.33246768 0.34011412][0.26962879 0.33440042 0.40664539 0.4724161 0.52159488 0.54812968 0.54755139 0.52393764 0.49163029 0.45839548 0.42282534 0.39235783 0.37473577 0.37106356 0.36738515][0.23885977 0.30876127 0.39216641 0.47467667 0.54266065 0.58442962 0.59091753 0.56601685 0.52911013 0.49402755 0.46131837 0.43615896 0.42103425 0.41304609 0.40076846][0.18662578 0.25578889 0.3421526 0.43052772 0.50591093 0.55474538 0.56738824 0.5477792 0.5162217 0.48927766 0.467391 0.45197287 0.44189504 0.43274024 0.41640803][0.11598518 0.176425 0.25411066 0.33425534 0.40392029 0.45134848 0.46830952 0.45716223 0.43573019 0.41999573 0.40901315 0.40158358 0.3958801 0.38828471 0.373401][0.054540318 0.0993295 0.1601406 0.22428541 0.28126508 0.32076946 0.33530763 0.32576433 0.30795783 0.29682937 0.29161239 0.29045278 0.29060468 0.28841037 0.28074998][0.028640527 0.057562608 0.099616222 0.14554326 0.18642415 0.21347694 0.22028416 0.20806664 0.1909088 0.18168935 0.17979166 0.18265629 0.187262 0.19029762 0.19164902][0.041179 0.059796039 0.086630233 0.1151359 0.13757798 0.14766774 0.14287785 0.12731853 0.11364028 0.11033788 0.11377534 0.11951152 0.12565619 0.1308676 0.13788322][0.069323532 0.08492063 0.1036876 0.12032945 0.12765609 0.1218465 0.10532409 0.086720221 0.077891409 0.0819874 0.090350971 0.096489131 0.10057259 0.10471875 0.11371874][0.11216523 0.12887412 0.14528495 0.15762073 0.15797532 0.14309543 0.11877744 0.098098457 0.092224762 0.099835925 0.10850404 0.11039633 0.10739069 0.10537513 0.11086991]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 17:45:19.728129: step 38010, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 64h:47m:48s remains)
INFO - root - 2017-12-10 17:45:27.280408: step 38020, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 63h:18m:13s remains)
INFO - root - 2017-12-10 17:45:35.181273: step 38030, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 62h:55m:50s remains)
INFO - root - 2017-12-10 17:45:42.986375: step 38040, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 64h:03m:26s remains)
INFO - root - 2017-12-10 17:45:50.864750: step 38050, loss = 0.69, batch loss = 0.64 (9.7 examples/sec; 0.825 sec/batch; 67h:29m:50s remains)
INFO - root - 2017-12-10 17:45:58.789686: step 38060, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.821 sec/batch; 67h:08m:04s remains)
INFO - root - 2017-12-10 17:46:06.643425: step 38070, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 62h:11m:23s remains)
INFO - root - 2017-12-10 17:46:14.554702: step 38080, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 62h:23m:46s remains)
INFO - root - 2017-12-10 17:46:22.411107: step 38090, loss = 0.71, batch loss = 0.66 (9.9 examples/sec; 0.811 sec/batch; 66h:18m:22s remains)
INFO - root - 2017-12-10 17:46:30.116914: step 38100, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 64h:37m:14s remains)
2017-12-10 17:46:30.967605: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.467266 0.38767463 0.30370769 0.24310966 0.20488396 0.18516237 0.17966962 0.18433467 0.20153134 0.24283677 0.29770187 0.34786978 0.39502043 0.45375854 0.51708335][0.48966643 0.40541837 0.31820121 0.25962263 0.22878142 0.22120267 0.22938797 0.24271192 0.2573351 0.28253925 0.31330854 0.33924758 0.36682111 0.41216126 0.46721378][0.43332124 0.36405152 0.29731014 0.26279384 0.25908524 0.27853993 0.30788341 0.32964516 0.33459112 0.33052254 0.31879771 0.30207258 0.29365945 0.31050259 0.34459385][0.31864211 0.28068379 0.25682354 0.26842362 0.31092009 0.37057924 0.42684376 0.45617104 0.44750392 0.40709096 0.34403798 0.27601862 0.22187158 0.19923621 0.20124197][0.19758902 0.20052035 0.23117749 0.29919422 0.39468208 0.49726346 0.5805223 0.61682594 0.59568959 0.52372611 0.41616359 0.30225345 0.20410143 0.13887741 0.10219678][0.10985526 0.15138717 0.23127462 0.34880212 0.4885073 0.62559962 0.73048794 0.77329361 0.74526066 0.65444291 0.52008462 0.37818697 0.25097284 0.15303689 0.082460091][0.06508559 0.13309 0.2443506 0.39148027 0.55598819 0.71283722 0.83060747 0.87908816 0.85192877 0.76001257 0.62257695 0.47517073 0.33895767 0.22548689 0.13310525][0.055085681 0.13283122 0.25297531 0.40430343 0.56822658 0.72324491 0.839063 0.88866705 0.86794049 0.79112053 0.67480946 0.54643184 0.42293605 0.31218979 0.21251215][0.063189991 0.13510053 0.24298479 0.37507623 0.5150972 0.64727086 0.74534 0.788014 0.77408016 0.72000223 0.63883311 0.54576004 0.45152465 0.3600437 0.2691687][0.069430128 0.123604 0.20433883 0.30196503 0.4039095 0.50051212 0.57132632 0.60132635 0.59187782 0.56045288 0.51671392 0.46382207 0.40641415 0.34390864 0.27350038][0.058978558 0.089324266 0.13719334 0.19641918 0.25806123 0.317298 0.35975111 0.37589967 0.36836022 0.35302338 0.33698818 0.31578395 0.29002559 0.25610584 0.21098197][0.035878725 0.042869575 0.061566174 0.088634767 0.11746074 0.14612213 0.16581737 0.17153269 0.16575837 0.16078307 0.16159236 0.1610232 0.15775837 0.14699228 0.12553161][0.00921136 0.00013471318 -0.00023440077 0.0066031972 0.015092206 0.024354 0.030118158 0.030726817 0.028000642 0.03018493 0.040916126 0.054344255 0.067890137 0.075810038 0.074260347][-0.016435022 -0.032473277 -0.040393893 -0.041366018 -0.040197019 -0.037359528 -0.0345027 -0.03214509 -0.029605661 -0.022190873 -0.0065309741 0.01574463 0.042119451 0.065784052 0.080666237][-0.038546335 -0.0536598 -0.060512215 -0.061189488 -0.05913185 -0.053847965 -0.045880139 -0.036191206 -0.025424184 -0.011750126 0.008003653 0.036253609 0.071210325 0.10549973 0.13063431]]...]
INFO - root - 2017-12-10 17:46:38.968453: step 38110, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 64h:44m:22s remains)
INFO - root - 2017-12-10 17:46:46.635099: step 38120, loss = 0.69, batch loss = 0.63 (12.7 examples/sec; 0.628 sec/batch; 51h:19m:04s remains)
INFO - root - 2017-12-10 17:46:54.481233: step 38130, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 63h:24m:41s remains)
INFO - root - 2017-12-10 17:47:02.380314: step 38140, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 63h:27m:40s remains)
INFO - root - 2017-12-10 17:47:10.468615: step 38150, loss = 0.71, batch loss = 0.66 (9.9 examples/sec; 0.810 sec/batch; 66h:13m:30s remains)
INFO - root - 2017-12-10 17:47:18.318085: step 38160, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 63h:12m:20s remains)
INFO - root - 2017-12-10 17:47:26.209842: step 38170, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 63h:58m:05s remains)
INFO - root - 2017-12-10 17:47:34.064393: step 38180, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 66h:14m:30s remains)
INFO - root - 2017-12-10 17:47:41.932584: step 38190, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 62h:53m:49s remains)
INFO - root - 2017-12-10 17:47:49.737399: step 38200, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 63h:19m:20s remains)
2017-12-10 17:47:50.617876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.051271062 -0.033194881 -0.012872404 0.00196203 0.0066263913 0.0039913142 -0.00063239649 -0.0065972437 -0.013709481 -0.019787045 -0.02077432 -0.019905981 -0.024117133 -0.030372187 -0.0346626][-0.036899943 -0.007729419 0.023793556 0.048464831 0.059903283 0.062428255 0.063713677 0.061343458 0.052940127 0.041242503 0.034215607 0.028311705 0.014213363 -0.0015178166 -0.010389132][-0.010406903 0.04004347 0.095071137 0.14337723 0.17451057 0.19216377 0.20744932 0.21426886 0.20551395 0.18468171 0.16649155 0.14728796 0.11339413 0.074658222 0.048051964][0.026679894 0.11095417 0.20584188 0.2961289 0.36271143 0.40656808 0.44556883 0.47010255 0.4635182 0.429854 0.39476442 0.35541424 0.2903561 0.2114332 0.14865012][0.076934755 0.20650877 0.357004 0.50505507 0.61880022 0.6952939 0.763124 0.80972123 0.80419356 0.75265497 0.69435304 0.6282931 0.5239166 0.39507684 0.28516415][0.14013685 0.32103842 0.53442335 0.7453537 0.90709829 1.012529 1.1025542 1.1654968 1.1551919 1.0804046 0.99363512 0.89824331 0.75731838 0.58407485 0.43125156][0.2005977 0.4243879 0.69025677 0.95097333 1.1470773 1.2669532 1.362348 1.4269986 1.4041796 1.3067391 1.1981676 1.0867172 0.92966032 0.73537958 0.55842888][0.23820335 0.48052871 0.76922804 1.0485302 1.2530172 1.3659197 1.4445713 1.491817 1.4498184 1.3373309 1.2231917 1.118329 0.97430259 0.79001319 0.61525625][0.24039446 0.4702518 0.743407 1.0027696 1.1865289 1.2737395 1.3190066 1.3329657 1.2683623 1.1513517 1.0478314 0.96718657 0.85819095 0.71019596 0.56432211][0.19859335 0.38735181 0.60971856 0.81591034 0.95668536 1.0089707 1.0166297 0.99523538 0.915432 0.80752289 0.72655094 0.67782503 0.61310893 0.51512724 0.41507369][0.11294239 0.2414348 0.39178181 0.52786416 0.61672103 0.63537657 0.6138413 0.56973392 0.49142087 0.40678871 0.35480398 0.3370696 0.31317428 0.26339483 0.20941874][0.0082052294 0.073737457 0.15238193 0.22225058 0.26499215 0.26021838 0.22556251 0.17668457 0.11420085 0.058890771 0.03399653 0.038888037 0.043372452 0.029569693 0.010750699][-0.079839326 -0.063679777 -0.039050281 -0.016956752 -0.0051553003 -0.018842297 -0.05012726 -0.088600181 -0.12925886 -0.1592302 -0.16705604 -0.15352476 -0.13758352 -0.13222641 -0.13039427][-0.13300374 -0.14556104 -0.15145008 -0.1553148 -0.15854537 -0.17019178 -0.18881339 -0.21003644 -0.22969089 -0.24118519 -0.23998109 -0.22674012 -0.21129555 -0.20059824 -0.19157289][-0.1495579 -0.17270915 -0.18988267 -0.20314424 -0.21080415 -0.21724103 -0.22404599 -0.23037317 -0.23403464 -0.23259933 -0.22566029 -0.21435887 -0.20303836 -0.19421452 -0.18667424]]...]
INFO - root - 2017-12-10 17:47:58.465014: step 38210, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 64h:09m:04s remains)
INFO - root - 2017-12-10 17:48:06.300594: step 38220, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 64h:24m:04s remains)
INFO - root - 2017-12-10 17:48:14.201450: step 38230, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.819 sec/batch; 66h:55m:43s remains)
INFO - root - 2017-12-10 17:48:22.043246: step 38240, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 64h:55m:16s remains)
INFO - root - 2017-12-10 17:48:29.927019: step 38250, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.816 sec/batch; 66h:39m:53s remains)
INFO - root - 2017-12-10 17:48:37.650081: step 38260, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 65h:29m:41s remains)
INFO - root - 2017-12-10 17:48:45.487011: step 38270, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 63h:46m:30s remains)
INFO - root - 2017-12-10 17:48:53.308660: step 38280, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 66h:15m:39s remains)
INFO - root - 2017-12-10 17:49:01.079738: step 38290, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 65h:45m:38s remains)
INFO - root - 2017-12-10 17:49:08.720805: step 38300, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 63h:16m:51s remains)
2017-12-10 17:49:09.554327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.037067574 -0.011334648 0.033681016 0.075301073 0.10598287 0.12317256 0.13461238 0.15320818 0.19082323 0.23987447 0.28737798 0.32434198 0.35444772 0.38251945 0.40110239][-0.056963336 -0.031895608 0.013425546 0.053959873 0.081614964 0.094486229 0.10264359 0.12066716 0.15998669 0.20978515 0.25706562 0.29306448 0.32329139 0.3560288 0.38708553][-0.053082377 -0.031786632 0.010815469 0.051029071 0.080595948 0.096487306 0.10942026 0.13391145 0.17928541 0.23142536 0.2755594 0.30146813 0.31658384 0.33330995 0.35468763][-0.012056618 0.010780365 0.055329308 0.10103416 0.13905117 0.16392228 0.1862326 0.21956849 0.26997358 0.32001448 0.35236308 0.35575894 0.3397029 0.32014596 0.3094112][0.078808933 0.11096831 0.16110222 0.21313536 0.25868788 0.29106641 0.32173926 0.36258784 0.41416493 0.45504922 0.46560118 0.43523043 0.3771185 0.3105478 0.25726813][0.21341856 0.25801316 0.31185251 0.36513591 0.41177762 0.44553715 0.47933102 0.52182573 0.56543726 0.586054 0.56484121 0.49599442 0.3976177 0.29110694 0.20438215][0.35219061 0.40805909 0.45942736 0.50658464 0.54661918 0.57381475 0.60194 0.6364612 0.66239917 0.65604335 0.60332739 0.5060159 0.38592404 0.2627992 0.16429119][0.46160766 0.51633364 0.5543108 0.58718228 0.61404192 0.62808818 0.64282972 0.66335511 0.67116261 0.6459378 0.58000273 0.48024911 0.36674759 0.25229937 0.15893583][0.51056683 0.54709995 0.56245953 0.57705504 0.58938879 0.59021425 0.5911566 0.59941345 0.59836644 0.57345182 0.52166849 0.44802633 0.36446828 0.27465931 0.19304787][0.49543208 0.50909841 0.50377 0.50408715 0.50590968 0.49794972 0.48909965 0.49032924 0.49094772 0.48291251 0.46428221 0.43186343 0.38642314 0.32367867 0.25193861][0.44708028 0.44389698 0.42683876 0.42027476 0.41757166 0.4069269 0.39460456 0.39456734 0.40197194 0.41339642 0.42572695 0.42745051 0.41097531 0.36821902 0.30388194][0.38956663 0.38005593 0.36153898 0.3551057 0.35359025 0.34602523 0.33549359 0.33592451 0.3469522 0.36822402 0.3960439 0.41478691 0.41394737 0.38512886 0.33091584][0.35003218 0.34088817 0.32358977 0.31519502 0.31192511 0.306689 0.29977608 0.30121449 0.31257185 0.33481139 0.36417061 0.38655323 0.39441136 0.38161066 0.34847033][0.3331908 0.32735687 0.3109653 0.29751626 0.28908193 0.28438932 0.28166106 0.28479633 0.29501671 0.31303522 0.3355253 0.35487935 0.37072212 0.37946019 0.37778252][0.32683674 0.32573554 0.31196174 0.29504868 0.28262189 0.27913368 0.281377 0.287404 0.29658359 0.30848312 0.32067305 0.33473116 0.35883203 0.38993979 0.41958863]]...]
INFO - root - 2017-12-10 17:49:17.402714: step 38310, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 65h:01m:26s remains)
INFO - root - 2017-12-10 17:49:25.233903: step 38320, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 63h:42m:34s remains)
INFO - root - 2017-12-10 17:49:33.071620: step 38330, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 64h:24m:56s remains)
INFO - root - 2017-12-10 17:49:40.747312: step 38340, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.822 sec/batch; 67h:12m:26s remains)
INFO - root - 2017-12-10 17:49:48.629894: step 38350, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 63h:59m:57s remains)
INFO - root - 2017-12-10 17:49:56.471132: step 38360, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.781 sec/batch; 63h:48m:17s remains)
INFO - root - 2017-12-10 17:50:04.345345: step 38370, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 65h:00m:20s remains)
INFO - root - 2017-12-10 17:50:12.141837: step 38380, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 63h:05m:52s remains)
INFO - root - 2017-12-10 17:50:19.778123: step 38390, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 64h:55m:37s remains)
INFO - root - 2017-12-10 17:50:27.589428: step 38400, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 63h:27m:24s remains)
2017-12-10 17:50:28.482580: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21947128 0.18483731 0.14287977 0.10917832 0.096214235 0.11610574 0.1655103 0.21798442 0.25407919 0.26422736 0.2548621 0.22151279 0.15371357 0.069126055 -0.0086384509][0.20615971 0.17254981 0.13174111 0.10286501 0.097455226 0.12345948 0.17436095 0.22499095 0.2574001 0.26334304 0.2500326 0.2136233 0.14446829 0.059700325 -0.017093163][0.18842712 0.16680592 0.14074226 0.128271 0.13580595 0.16566661 0.20921612 0.24559972 0.26330158 0.2588554 0.23922825 0.19936097 0.13007675 0.047892582 -0.025218682][0.17531461 0.17786288 0.17991349 0.19348152 0.21762316 0.24908793 0.27958271 0.29442987 0.28933364 0.26630965 0.23453903 0.18793465 0.11718241 0.037000477 -0.031856332][0.15958671 0.19732438 0.24017836 0.2886346 0.33397561 0.36984047 0.38871646 0.38044471 0.34770298 0.30029669 0.25156263 0.194242 0.1181873 0.036257241 -0.031094331][0.14128946 0.21672477 0.30473679 0.39216316 0.46370479 0.51128864 0.5262025 0.49860618 0.43747297 0.36256617 0.2928901 0.21982692 0.13241614 0.044021953 -0.025350809][0.13519487 0.24166374 0.36799511 0.48913369 0.58496219 0.64686865 0.66348177 0.62133086 0.53387326 0.43288425 0.34268931 0.25210235 0.1496464 0.051216736 -0.021596987][0.15157206 0.26955464 0.41022861 0.54465079 0.651867 0.7237348 0.74504912 0.6980927 0.59884858 0.48671046 0.38546923 0.28085163 0.16400428 0.055668842 -0.020535035][0.18647727 0.28680369 0.40732229 0.52510118 0.62305021 0.69438392 0.72193426 0.68500149 0.59823817 0.49965668 0.40518141 0.29791474 0.17442606 0.060921937 -0.017485414][0.21952175 0.28199527 0.35678542 0.43560028 0.50870442 0.57176405 0.60826862 0.59471512 0.53990638 0.47341403 0.40012276 0.3018904 0.18094572 0.067999512 -0.010848099][0.23866466 0.26043817 0.28450233 0.31810793 0.35954195 0.40795159 0.45080763 0.46297461 0.44484389 0.41534421 0.36994484 0.29019141 0.18013404 0.07250651 -0.0047161714][0.23046882 0.22192322 0.2076872 0.2054875 0.21960387 0.25249961 0.29735488 0.33001003 0.34272113 0.34461653 0.32639557 0.26960903 0.17601487 0.076520465 0.0015109024][0.19158106 0.16684672 0.13255592 0.10973906 0.10597445 0.12552263 0.16732354 0.20992331 0.24101731 0.26291916 0.26550475 0.23087315 0.15669376 0.070190229 0.0014473038][0.12383571 0.095108569 0.056538261 0.026725678 0.013997427 0.023002138 0.057410125 0.099877521 0.13655929 0.16631769 0.18140467 0.16603096 0.11343127 0.045320157 -0.01139705][0.038979173 0.014586474 -0.016634231 -0.042680208 -0.056463525 -0.053579181 -0.028067827 0.007025498 0.038871646 0.06579949 0.084172413 0.081811666 0.049241103 0.0019422894 -0.038377509]]...]
INFO - root - 2017-12-10 17:50:36.255758: step 38410, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.768 sec/batch; 62h:41m:54s remains)
INFO - root - 2017-12-10 17:50:43.927151: step 38420, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 66h:10m:39s remains)
INFO - root - 2017-12-10 17:50:51.848481: step 38430, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.788 sec/batch; 64h:24m:31s remains)
INFO - root - 2017-12-10 17:50:59.657365: step 38440, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 62h:22m:55s remains)
INFO - root - 2017-12-10 17:51:07.499761: step 38450, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 62h:30m:13s remains)
INFO - root - 2017-12-10 17:51:15.341055: step 38460, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 63h:04m:40s remains)
INFO - root - 2017-12-10 17:51:23.206919: step 38470, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 63h:47m:14s remains)
INFO - root - 2017-12-10 17:51:30.837067: step 38480, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 64h:13m:35s remains)
INFO - root - 2017-12-10 17:51:38.685781: step 38490, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 64h:27m:23s remains)
INFO - root - 2017-12-10 17:51:46.363465: step 38500, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 63h:11m:15s remains)
2017-12-10 17:51:47.225688: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20962596 0.18342321 0.16138807 0.16356632 0.18206818 0.20321395 0.21884085 0.23111396 0.24244474 0.25026405 0.25507402 0.25910476 0.26313367 0.26642597 0.27328798][0.23570366 0.20479457 0.17278995 0.16115925 0.16532141 0.17394224 0.18062858 0.1890161 0.20563368 0.22849625 0.25359759 0.27659139 0.29436621 0.3075608 0.32170647][0.28546968 0.25105506 0.20798622 0.1802097 0.1693607 0.16666959 0.16453919 0.16631907 0.182288 0.21278803 0.25077868 0.28714845 0.31551868 0.3372899 0.35845938][0.35256577 0.32196283 0.27676317 0.24327967 0.22919652 0.22606407 0.22132379 0.21500221 0.22048156 0.24230313 0.27423754 0.30615851 0.33125666 0.35380471 0.37909141][0.40493008 0.39179733 0.363452 0.34482923 0.34671175 0.35880119 0.36080793 0.34724095 0.33368409 0.33072591 0.33540246 0.33980531 0.34199598 0.35364926 0.38030434][0.41508183 0.43407235 0.44281203 0.45978269 0.49493572 0.53645146 0.55540645 0.539288 0.50418848 0.46656588 0.42806783 0.38504994 0.34651327 0.33704638 0.36189172][0.36972514 0.42727482 0.48384836 0.54674757 0.62212473 0.69948113 0.74273735 0.73144352 0.68011725 0.60948616 0.52630007 0.43084767 0.34540957 0.30948246 0.32920149][0.27428937 0.36279133 0.46033394 0.56189471 0.66911489 0.77657807 0.84490043 0.84599763 0.79021657 0.69973356 0.58709371 0.45494759 0.33438668 0.27603659 0.28776094][0.15334992 0.25287026 0.36845857 0.48585495 0.60333848 0.72299224 0.80724043 0.82273269 0.77426225 0.68264627 0.5657686 0.42642105 0.29681051 0.23043077 0.23544331][0.036130037 0.12182076 0.22622465 0.33133814 0.4338578 0.54242033 0.62604594 0.65138614 0.6178329 0.54265273 0.44584414 0.32929733 0.21946427 0.16390018 0.16872706][-0.045410592 0.010053399 0.0826716 0.15600505 0.22655495 0.30534634 0.37089413 0.39617825 0.37773952 0.32761946 0.26378736 0.18653749 0.11405321 0.08287406 0.094564892][-0.077543423 -0.054570131 -0.019273927 0.017003497 0.051664315 0.093807243 0.13148995 0.14758928 0.13877383 0.11254756 0.081182949 0.043259345 0.0096758911 0.0053667245 0.027870839][-0.06996654 -0.070592672 -0.065022618 -0.058475681 -0.051911443 -0.039872613 -0.027344773 -0.022644106 -0.027229398 -0.036503337 -0.044790428 -0.054681659 -0.059705105 -0.044025339 -0.012661435][-0.040602762 -0.051652808 -0.06147607 -0.070720963 -0.078876622 -0.083412692 -0.085775673 -0.08896172 -0.092580952 -0.093704969 -0.090997733 -0.087082066 -0.077070594 -0.051218547 -0.015424252][-0.0034921227 -0.014089824 -0.026233129 -0.038219415 -0.048929583 -0.057293214 -0.063602753 -0.068712212 -0.071670458 -0.070806347 -0.065995395 -0.058782268 -0.043959845 -0.015328215 0.02166998]]...]
INFO - root - 2017-12-10 17:51:55.116633: step 38510, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.804 sec/batch; 65h:41m:02s remains)
INFO - root - 2017-12-10 17:52:02.955045: step 38520, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 65h:13m:28s remains)
INFO - root - 2017-12-10 17:52:10.792701: step 38530, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 61h:43m:56s remains)
INFO - root - 2017-12-10 17:52:18.641743: step 38540, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 62h:38m:55s remains)
INFO - root - 2017-12-10 17:52:26.687773: step 38550, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.818 sec/batch; 66h:46m:42s remains)
INFO - root - 2017-12-10 17:52:34.614840: step 38560, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 63h:11m:35s remains)
INFO - root - 2017-12-10 17:52:42.274240: step 38570, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 62h:56m:34s remains)
INFO - root - 2017-12-10 17:52:49.904388: step 38580, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 64h:12m:28s remains)
INFO - root - 2017-12-10 17:52:57.794206: step 38590, loss = 0.67, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 63h:23m:16s remains)
INFO - root - 2017-12-10 17:53:05.659180: step 38600, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 64h:55m:28s remains)
2017-12-10 17:53:06.554158: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.011113876 0.068647809 0.18226771 0.30355635 0.40362811 0.46089792 0.46172193 0.40436575 0.3134034 0.23038085 0.18377243 0.17648405 0.19310746 0.21625783 0.22967066][-0.011243882 0.065200984 0.17665695 0.29563734 0.39168444 0.44243386 0.43684 0.37646359 0.28477806 0.20181865 0.15555204 0.14849165 0.16533928 0.19241168 0.21632069][-0.013561348 0.049447168 0.14860979 0.25724256 0.34465492 0.38953778 0.38372236 0.33048445 0.2497244 0.17592964 0.13407998 0.12518832 0.13659303 0.16038373 0.18883695][-0.0047398685 0.041852385 0.12721016 0.22624896 0.30788654 0.35207307 0.35228023 0.31251961 0.24915653 0.1895185 0.15377922 0.14138804 0.14446978 0.16007978 0.18523423][0.022891764 0.052285869 0.12531573 0.21864559 0.29962647 0.34941873 0.36100227 0.33788356 0.29370216 0.24871522 0.2185286 0.20287357 0.2000991 0.20993781 0.23088521][0.06707795 0.079634771 0.14286673 0.23599078 0.32381868 0.38733038 0.41672531 0.41302192 0.38663051 0.35171661 0.3219839 0.30207202 0.2977438 0.30810022 0.32856497][0.11525778 0.11386849 0.17111689 0.26935497 0.37057057 0.45357546 0.50389493 0.51824969 0.50368887 0.47086215 0.43518332 0.40947956 0.40667287 0.42342451 0.44808161][0.1410622 0.13225335 0.18743975 0.2941362 0.41251498 0.51682031 0.5865097 0.61374879 0.60304976 0.563721 0.515257 0.48015112 0.47730526 0.49975768 0.527524][0.13597576 0.12354255 0.17759773 0.29157639 0.42545772 0.54726124 0.63047117 0.66382855 0.649818 0.59798878 0.53278035 0.48519769 0.47772649 0.49936306 0.52411133][0.10676707 0.092736125 0.14319521 0.2574999 0.39748925 0.52566445 0.61180985 0.64323157 0.62117118 0.55619234 0.4759672 0.41592646 0.3992092 0.41263717 0.42960113][0.06200584 0.048256334 0.091601036 0.19645295 0.32860535 0.44861269 0.52571023 0.54797173 0.51714444 0.4446578 0.3576304 0.28985876 0.26287204 0.26369369 0.27056149][0.010473286 -0.0017134019 0.03232241 0.1191586 0.23022422 0.32882583 0.38668457 0.39447767 0.35596335 0.28333145 0.20021775 0.13372621 0.10162772 0.093480468 0.094756804][-0.034927148 -0.045543041 -0.021942087 0.042278994 0.12489109 0.19531661 0.22995988 0.22296174 0.17998812 0.11334843 0.042266726 -0.013915728 -0.042214539 -0.049750842 -0.044495735][-0.060619924 -0.0714174 -0.058532104 -0.017029235 0.036945615 0.080215156 0.094514348 0.076943941 0.034094423 -0.022429304 -0.077137716 -0.11685071 -0.13328107 -0.13103648 -0.11392639][-0.065318108 -0.077901796 -0.075238727 -0.053694248 -0.02367436 -0.0013282776 -0.00039000704 -0.022287115 -0.06040775 -0.10415537 -0.14128168 -0.16281356 -0.16397637 -0.14816351 -0.11742561]]...]
INFO - root - 2017-12-10 17:53:14.483165: step 38610, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 63h:58m:13s remains)
INFO - root - 2017-12-10 17:53:22.388405: step 38620, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.827 sec/batch; 67h:32m:39s remains)
INFO - root - 2017-12-10 17:53:30.207485: step 38630, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 62h:10m:19s remains)
INFO - root - 2017-12-10 17:53:38.054652: step 38640, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 64h:43m:26s remains)
INFO - root - 2017-12-10 17:53:45.885285: step 38650, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 63h:53m:50s remains)
INFO - root - 2017-12-10 17:53:53.399184: step 38660, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 64h:04m:50s remains)
INFO - root - 2017-12-10 17:54:01.303310: step 38670, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 65h:56m:17s remains)
INFO - root - 2017-12-10 17:54:09.148537: step 38680, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.752 sec/batch; 61h:23m:51s remains)
INFO - root - 2017-12-10 17:54:16.989192: step 38690, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 63h:15m:40s remains)
INFO - root - 2017-12-10 17:54:24.959582: step 38700, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.825 sec/batch; 67h:21m:50s remains)
2017-12-10 17:54:25.795350: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28859997 0.29185849 0.27822137 0.26020071 0.24526268 0.24149315 0.25285068 0.27387026 0.2894206 0.28551432 0.25703913 0.20326328 0.13798386 0.080875769 0.043432262][0.2477532 0.25078276 0.24166355 0.23262276 0.23105432 0.24285191 0.26867065 0.29895675 0.31681633 0.30954623 0.27331153 0.20856446 0.13009712 0.062023796 0.018372865][0.17948097 0.18620341 0.18732463 0.19150823 0.20471475 0.23107871 0.26975068 0.30839351 0.32865754 0.31855592 0.27548859 0.20200364 0.11428955 0.038350977 -0.0098960083][0.11508216 0.13062595 0.14693519 0.16768035 0.19620764 0.23562239 0.28449202 0.32946983 0.35157016 0.33803338 0.28637317 0.20180519 0.10393196 0.020603197 -0.031967476][0.060276844 0.086996995 0.12177356 0.16357099 0.2120876 0.26798329 0.32720613 0.37482724 0.39268073 0.36815968 0.30035713 0.19925921 0.089847878 0.00085757452 -0.053542655][0.029801851 0.067215368 0.11944625 0.18205023 0.25105217 0.32339644 0.39003024 0.43373066 0.43856537 0.39510322 0.30629975 0.18776378 0.070312016 -0.019295862 -0.071278825][0.031335924 0.074755631 0.13780339 0.21553345 0.30046552 0.38509217 0.45426077 0.4885588 0.47446096 0.40754324 0.29718491 0.16610871 0.047843922 -0.035819504 -0.0810632][0.057452142 0.10151603 0.16704023 0.25123677 0.34334072 0.43184429 0.49680713 0.51866674 0.4857057 0.39786184 0.27131873 0.13554801 0.02420347 -0.04821676 -0.083859883][0.084741779 0.12253679 0.1823227 0.26476058 0.35577753 0.43968326 0.49425021 0.50217551 0.45454752 0.35489208 0.22391413 0.094585478 -0.0023796693 -0.058983829 -0.081950076][0.091453969 0.1156342 0.16252171 0.23611332 0.3202593 0.39485294 0.43680617 0.43248561 0.37689903 0.2767629 0.15483107 0.043345932 -0.032372389 -0.069873951 -0.078379348][0.076225005 0.084297307 0.11293271 0.16900963 0.23763271 0.29713365 0.32579592 0.31358695 0.2589862 0.17055205 0.070252344 -0.014118161 -0.064033605 -0.081218511 -0.075735986][0.042801421 0.03618481 0.04605405 0.08021047 0.12736037 0.16811946 0.18475346 0.17032664 0.12505509 0.057272967 -0.013976571 -0.067111462 -0.090708926 -0.089269973 -0.072678596][0.00392157 -0.012462473 -0.016850827 -0.0025954305 0.022885676 0.045110993 0.051876519 0.039039854 0.007324113 -0.036599938 -0.078168474 -0.10271367 -0.10495737 -0.090462424 -0.067882769][-0.028517587 -0.049249511 -0.061499022 -0.060896661 -0.051674716 -0.043141764 -0.042571418 -0.051984124 -0.070457555 -0.093286425 -0.11097725 -0.11528289 -0.10499375 -0.085122883 -0.062198285][-0.0513415 -0.071888126 -0.086448312 -0.092720695 -0.092589982 -0.09129642 -0.092938878 -0.098446533 -0.10691666 -0.11522762 -0.11802937 -0.11167059 -0.096884787 -0.077472232 -0.057893146]]...]
INFO - root - 2017-12-10 17:54:33.527950: step 38710, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 62h:05m:52s remains)
INFO - root - 2017-12-10 17:54:41.324705: step 38720, loss = 0.71, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 63h:55m:09s remains)
INFO - root - 2017-12-10 17:54:49.074177: step 38730, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 64h:30m:42s remains)
INFO - root - 2017-12-10 17:54:56.628625: step 38740, loss = 0.68, batch loss = 0.63 (13.2 examples/sec; 0.605 sec/batch; 49h:22m:15s remains)
INFO - root - 2017-12-10 17:55:04.478848: step 38750, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 63h:22m:30s remains)
INFO - root - 2017-12-10 17:55:12.422364: step 38760, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.817 sec/batch; 66h:40m:25s remains)
INFO - root - 2017-12-10 17:55:20.291504: step 38770, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 64h:18m:35s remains)
INFO - root - 2017-12-10 17:55:28.210592: step 38780, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 63h:45m:02s remains)
INFO - root - 2017-12-10 17:55:36.108592: step 38790, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 63h:14m:21s remains)
INFO - root - 2017-12-10 17:55:44.030830: step 38800, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 64h:40m:22s remains)
2017-12-10 17:55:44.865173: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.044770107 0.062204424 0.073831595 0.07499034 0.067884289 0.059720706 0.0515597 0.043507662 0.036897138 0.033533454 0.03330335 0.033911187 0.039172944 0.05106581 0.064704113][0.05737862 0.077144876 0.089770176 0.090329304 0.081907138 0.072376832 0.062901475 0.053523686 0.045513239 0.040882144 0.039795771 0.0401302 0.046318322 0.059903733 0.074966721][0.0609177 0.079505712 0.091073096 0.091340259 0.08402469 0.076079741 0.068088196 0.059430532 0.050727155 0.044061847 0.040358104 0.038564354 0.04326617 0.05571074 0.069762528][0.0566541 0.071097493 0.080079749 0.080771871 0.077268831 0.074203625 0.070928425 0.065476291 0.057042539 0.047531445 0.038912393 0.032323871 0.032349009 0.040478408 0.051467605][0.050181102 0.059622902 0.065772772 0.067664631 0.069767959 0.073872492 0.077599555 0.076928332 0.069324471 0.056337267 0.041095033 0.02741586 0.020154348 0.021923278 0.0291399][0.046363872 0.052459408 0.057057913 0.060976136 0.069210976 0.080958493 0.0919613 0.095928147 0.0888875 0.072111055 0.049777392 0.028269509 0.012909817 0.0079590688 0.011615855][0.048639774 0.054809552 0.060112491 0.066306084 0.07919161 0.096671425 0.11264026 0.11902474 0.11150722 0.091594942 0.064146988 0.036780566 0.015410661 0.0057715322 0.0069929529][0.056712411 0.065857321 0.0730243 0.080027573 0.093675189 0.1122899 0.12883782 0.13475204 0.12627028 0.10533603 0.076806277 0.047981158 0.025165597 0.014757998 0.015579856][0.06702847 0.080310278 0.089120232 0.094594061 0.10438959 0.11884073 0.13125572 0.13414869 0.12500757 0.10611878 0.08126995 0.055982642 0.037091624 0.030582812 0.033513766][0.07599318 0.093314961 0.10299853 0.10515908 0.10789187 0.1144442 0.11970893 0.11838203 0.10930715 0.09472879 0.076808862 0.058659047 0.047715571 0.048804037 0.056215007][0.0804264 0.10045359 0.110098 0.10847317 0.10358836 0.10158312 0.099682361 0.094925568 0.0870214 0.077731222 0.067633629 0.057687182 0.055683892 0.065034114 0.07749331][0.077273011 0.09773621 0.10674865 0.10273537 0.093003467 0.08531592 0.078959577 0.072884351 0.067195818 0.062926985 0.059554994 0.056432202 0.061267205 0.07629358 0.09204793][0.063986048 0.081946567 0.089608952 0.085158087 0.074634127 0.065624163 0.058437385 0.053161539 0.05000345 0.049408428 0.050327677 0.051250074 0.05898013 0.075108342 0.090787217][0.041509237 0.054568592 0.060410146 0.057134654 0.049101207 0.042241789 0.036833696 0.033333104 0.032010738 0.033114329 0.035610877 0.037785187 0.044862337 0.058121212 0.070713684][0.016017726 0.023518514 0.027471477 0.025922 0.021483984 0.017828656 0.014945226 0.01325577 0.012966676 0.014331708 0.016614346 0.018459283 0.02314385 0.031734057 0.039968129]]...]
INFO - root - 2017-12-10 17:55:52.706490: step 38810, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.765 sec/batch; 62h:26m:54s remains)
INFO - root - 2017-12-10 17:56:00.365527: step 38820, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 64h:46m:18s remains)
INFO - root - 2017-12-10 17:56:08.041360: step 38830, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 62h:02m:58s remains)
INFO - root - 2017-12-10 17:56:15.808422: step 38840, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 63h:31m:16s remains)
INFO - root - 2017-12-10 17:56:23.621153: step 38850, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.821 sec/batch; 66h:56m:20s remains)
INFO - root - 2017-12-10 17:56:31.450076: step 38860, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 62h:24m:35s remains)
INFO - root - 2017-12-10 17:56:39.391047: step 38870, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 64h:35m:06s remains)
INFO - root - 2017-12-10 17:56:47.211086: step 38880, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 62h:21m:41s remains)
INFO - root - 2017-12-10 17:56:55.143496: step 38890, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 62h:50m:48s remains)
INFO - root - 2017-12-10 17:57:02.827922: step 38900, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 64h:34m:28s remains)
2017-12-10 17:57:03.686278: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25187302 0.28352302 0.3169983 0.34142104 0.34960985 0.34222817 0.32528493 0.29718074 0.25627503 0.21779484 0.19530791 0.20201322 0.2259741 0.24499661 0.24283339][0.26248688 0.29254726 0.32177082 0.34207904 0.34842792 0.3414053 0.32616743 0.2987493 0.25372642 0.20613444 0.17121285 0.16321956 0.17250255 0.18193501 0.17958407][0.24793895 0.27789289 0.30193934 0.31614932 0.31967685 0.31376803 0.30151138 0.27637002 0.23002002 0.17526852 0.12726471 0.10092502 0.091329694 0.088641226 0.085593633][0.2273 0.2623474 0.28435278 0.29366645 0.29557785 0.29262373 0.28461373 0.26252705 0.21701016 0.15676877 0.095282137 0.049107194 0.020220345 0.006109341 0.0025369264][0.21571568 0.2625089 0.28774098 0.29615125 0.2999554 0.30319777 0.30112895 0.28237635 0.23803958 0.17247748 0.097349018 0.033297952 -0.010289929 -0.031523529 -0.035452571][0.2206935 0.28118482 0.3132599 0.32481593 0.33381823 0.34536618 0.35007262 0.33429953 0.29040679 0.2201771 0.13501817 0.061468448 0.013494454 -0.0082842717 -0.012485642][0.23902829 0.30990905 0.34806845 0.36377877 0.37923989 0.39959151 0.41136834 0.39844844 0.35424277 0.28138435 0.19371314 0.12354176 0.083874524 0.068694517 0.064719796][0.25906712 0.3315281 0.37033787 0.38755393 0.40739858 0.43402934 0.45131239 0.44129354 0.39772683 0.32742366 0.24888551 0.19688343 0.1772598 0.1738923 0.16996436][0.27541757 0.33674389 0.36736837 0.3814134 0.40167445 0.42964175 0.44890982 0.44141531 0.40092123 0.33970943 0.28135267 0.25792462 0.26421192 0.27439624 0.27078211][0.28789097 0.32575539 0.33925354 0.34429485 0.35934272 0.38173988 0.39750397 0.39088434 0.35604891 0.3094449 0.27786592 0.28594935 0.31693655 0.3386597 0.33531418][0.29506141 0.3028256 0.29313868 0.28351715 0.28701618 0.29753354 0.30484551 0.29714581 0.26938656 0.23944934 0.23442446 0.26975197 0.31960702 0.34959933 0.34712747][0.29180092 0.27127352 0.23885784 0.21322477 0.2029998 0.20010902 0.19766243 0.18771428 0.16620816 0.14991654 0.16339546 0.2145333 0.27381089 0.30839387 0.3085731][0.26760066 0.22764423 0.18004715 0.14312363 0.12212575 0.10912962 0.099583156 0.088390708 0.072232321 0.0646346 0.085411139 0.13821836 0.19574189 0.2297577 0.2331187][0.21380816 0.16842696 0.11848074 0.079510212 0.054855645 0.038292643 0.026853856 0.016947225 0.0056593572 0.0020584203 0.020167205 0.062855884 0.10939851 0.13802509 0.14343336][0.12854627 0.090490423 0.050709233 0.019815834 -8.7925917e-05 -0.01301449 -0.0210732 -0.027249185 -0.034167048 -0.036751788 -0.026268229 0.0012664146 0.032931812 0.053601861 0.059186492]]...]
INFO - root - 2017-12-10 17:57:11.731792: step 38910, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 65h:06m:07s remains)
INFO - root - 2017-12-10 17:57:19.273970: step 38920, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 63h:15m:58s remains)
INFO - root - 2017-12-10 17:57:27.193405: step 38930, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.816 sec/batch; 66h:34m:57s remains)
INFO - root - 2017-12-10 17:57:35.092044: step 38940, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 65h:43m:24s remains)
INFO - root - 2017-12-10 17:57:42.958079: step 38950, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 63h:28m:06s remains)
INFO - root - 2017-12-10 17:57:50.878158: step 38960, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 65h:57m:07s remains)
INFO - root - 2017-12-10 17:57:58.810800: step 38970, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 63h:41m:14s remains)
INFO - root - 2017-12-10 17:58:06.455062: step 38980, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 62h:55m:51s remains)
INFO - root - 2017-12-10 17:58:14.288434: step 38990, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.773 sec/batch; 62h:59m:13s remains)
INFO - root - 2017-12-10 17:58:22.058536: step 39000, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 63h:59m:59s remains)
2017-12-10 17:58:22.970939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.061947592 -0.057419635 -0.051190995 -0.045711335 -0.042370938 -0.042869717 -0.047138706 -0.053845592 -0.060508177 -0.065137029 -0.06658043 -0.064911261 -0.061540823 -0.057965241 -0.055336434][-0.058323834 -0.049730502 -0.036961406 -0.023520589 -0.013243336 -0.010141019 -0.015514539 -0.027226865 -0.041737858 -0.05568653 -0.065894172 -0.070704028 -0.070736729 -0.067887418 -0.064042516][-0.037130523 -0.027906336 -0.0090042194 0.016077729 0.039226923 0.051735666 0.048983164 0.0330815 0.0089024082 -0.018603547 -0.04318494 -0.060202874 -0.068883292 -0.071518905 -0.070172086][0.010548327 0.014732769 0.036495414 0.073925726 0.11319488 0.13954972 0.14415431 0.12825651 0.096755669 0.055327851 0.013161186 -0.02105133 -0.044344515 -0.058802687 -0.065789044][0.086993724 0.082881123 0.10153469 0.1451595 0.19571108 0.23450777 0.24996768 0.24132812 0.2110028 0.16162796 0.10351626 0.049586162 0.0062579294 -0.026837533 -0.048944086][0.18155934 0.16963774 0.17991011 0.2204878 0.2727527 0.31813294 0.3457 0.3517437 0.33296213 0.28474927 0.21646792 0.14380987 0.077450112 0.020505551 -0.022153489][0.27030641 0.25608116 0.25669086 0.28708351 0.33225965 0.3776463 0.41534773 0.43797415 0.4353779 0.39446083 0.32180238 0.23515561 0.14855859 0.069028959 0.006126923][0.3359488 0.32834816 0.32307261 0.34182772 0.37568393 0.41551393 0.45648706 0.4883211 0.49644217 0.46237463 0.390094 0.29690275 0.19766749 0.10270014 0.025235124][0.37646717 0.38505793 0.38257277 0.39397454 0.4161523 0.44462517 0.47645906 0.50112063 0.50520629 0.47036147 0.39889374 0.305587 0.20354109 0.10473906 0.023826554][0.39323977 0.42038226 0.42595541 0.43396884 0.44510406 0.45737717 0.46925935 0.47276098 0.45987862 0.41672769 0.34491321 0.25606242 0.16073747 0.070987456 -0.00057600404][0.39257166 0.43511823 0.45047 0.45735049 0.45777011 0.4508591 0.43701962 0.41279688 0.37684408 0.3211897 0.2476721 0.16605802 0.085143708 0.01489618 -0.037131615][0.36870486 0.42021582 0.44388562 0.45191979 0.44496724 0.42198187 0.38481349 0.33470139 0.27626967 0.20748238 0.13194089 0.059181873 -0.0027178421 -0.047586445 -0.074598141][0.32287019 0.3753626 0.40402725 0.41613093 0.409925 0.38197261 0.33263007 0.26618767 0.19143489 0.11217698 0.035508037 -0.028451677 -0.072675467 -0.094995171 -0.099741079][0.27911013 0.32501131 0.35368451 0.37067425 0.37285891 0.35325411 0.30787751 0.24025047 0.16008802 0.075986452 -0.0011319428 -0.060543 -0.096324354 -0.10817848 -0.10164081][0.26195624 0.2944186 0.3163721 0.3350774 0.34796479 0.34579054 0.31918123 0.26696053 0.19513722 0.11351462 0.035820808 -0.026137186 -0.065871485 -0.08163783 -0.076397441]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 17:58:30.673093: step 39010, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 63h:27m:24s remains)
INFO - root - 2017-12-10 17:58:38.462452: step 39020, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 64h:15m:07s remains)
INFO - root - 2017-12-10 17:58:46.293414: step 39030, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 63h:24m:57s remains)
INFO - root - 2017-12-10 17:58:54.130443: step 39040, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.758 sec/batch; 61h:49m:12s remains)
INFO - root - 2017-12-10 17:59:01.884244: step 39050, loss = 0.67, batch loss = 0.61 (10.5 examples/sec; 0.762 sec/batch; 62h:05m:58s remains)
INFO - root - 2017-12-10 17:59:09.539798: step 39060, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 65h:14m:11s remains)
INFO - root - 2017-12-10 17:59:17.462302: step 39070, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 66h:08m:31s remains)
INFO - root - 2017-12-10 17:59:25.324315: step 39080, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 63h:15m:09s remains)
INFO - root - 2017-12-10 17:59:33.131859: step 39090, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.771 sec/batch; 62h:50m:21s remains)
INFO - root - 2017-12-10 17:59:40.904726: step 39100, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.825 sec/batch; 67h:13m:44s remains)
2017-12-10 17:59:41.762270: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.091534644 0.17921774 0.27069032 0.35145408 0.41428608 0.45505407 0.4589034 0.43316457 0.39884987 0.37329438 0.36875263 0.37598562 0.3834556 0.38532832 0.38483194][0.082615145 0.17358235 0.27006561 0.35575706 0.42114463 0.46005249 0.45715114 0.42252964 0.38015327 0.34937936 0.33862779 0.33646572 0.33567941 0.33316004 0.33134791][0.068353169 0.15713055 0.25377277 0.34095839 0.40674984 0.44265139 0.43314812 0.38969076 0.33757997 0.29905182 0.28129914 0.27191639 0.26673537 0.26430097 0.26609066][0.048404466 0.13195468 0.22877431 0.32118142 0.39325297 0.43232962 0.42187107 0.37316537 0.31225148 0.26590237 0.24364662 0.23212151 0.22722819 0.22610033 0.22876526][0.037305567 0.11887859 0.21870135 0.31841049 0.39866349 0.44372725 0.43723062 0.38967702 0.32687014 0.27833936 0.25648615 0.24657905 0.2416466 0.23624603 0.23022693][0.041051339 0.12600581 0.23111986 0.33690381 0.42330763 0.47434822 0.47507393 0.43462065 0.37683764 0.33088219 0.310112 0.29911837 0.28833753 0.27183223 0.25115955][0.047956172 0.13626942 0.24437059 0.352217 0.44241196 0.50006956 0.51109934 0.48172408 0.43251437 0.39072573 0.36887076 0.35248047 0.33138251 0.30251637 0.27010176][0.050485492 0.1365651 0.24130322 0.34551972 0.43654764 0.50080276 0.52301973 0.50504678 0.46416315 0.42583695 0.4005855 0.37606463 0.34353596 0.3047784 0.26721716][0.049535938 0.12589355 0.21903059 0.31232387 0.39838746 0.46431479 0.49191707 0.47988296 0.4440206 0.40807858 0.37913367 0.3476325 0.30796584 0.26669881 0.23299727][0.048001446 0.11066719 0.18648633 0.26146191 0.33318067 0.38959426 0.41077596 0.39582962 0.36145866 0.32889938 0.29978952 0.26664233 0.22766478 0.19324198 0.17327999][0.045683049 0.094848976 0.15204462 0.20485459 0.254962 0.29229915 0.29815713 0.27461731 0.23949502 0.21085796 0.18514308 0.15676957 0.12719724 0.10808698 0.10876895][0.041865893 0.079366148 0.11901921 0.14940837 0.17512928 0.18950567 0.17856421 0.1481909 0.1154101 0.094210759 0.077349611 0.060578767 0.046794672 0.04621936 0.067234717][0.042090941 0.073008016 0.099942379 0.11194568 0.11515296 0.10736708 0.082346812 0.0498342 0.024206178 0.01490819 0.012221884 0.011732758 0.015580737 0.030659124 0.065007731][0.045310132 0.072691023 0.091342486 0.0905547 0.07652422 0.051227648 0.017225167 -0.012390277 -0.026445417 -0.019875096 -0.0039920532 0.01504189 0.036662523 0.0639813 0.10474448][0.051114634 0.0763186 0.09110602 0.085261106 0.063068248 0.028873386 -0.0076979469 -0.031189643 -0.0328424 -0.010708713 0.0235079 0.06106104 0.09723109 0.13177784 0.17148609]]...]
INFO - root - 2017-12-10 17:59:49.559066: step 39110, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 61h:57m:13s remains)
INFO - root - 2017-12-10 17:59:57.365154: step 39120, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 63h:21m:10s remains)
INFO - root - 2017-12-10 18:00:05.189396: step 39130, loss = 0.67, batch loss = 0.61 (10.2 examples/sec; 0.787 sec/batch; 64h:06m:25s remains)
INFO - root - 2017-12-10 18:00:12.768625: step 39140, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 64h:32m:10s remains)
INFO - root - 2017-12-10 18:00:20.564002: step 39150, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.773 sec/batch; 62h:57m:25s remains)
INFO - root - 2017-12-10 18:00:28.347017: step 39160, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.759 sec/batch; 61h:48m:27s remains)
INFO - root - 2017-12-10 18:00:36.171699: step 39170, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 62h:21m:07s remains)
INFO - root - 2017-12-10 18:00:44.099488: step 39180, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 65h:13m:59s remains)
INFO - root - 2017-12-10 18:00:51.754878: step 39190, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.766 sec/batch; 62h:26m:56s remains)
INFO - root - 2017-12-10 18:00:59.552664: step 39200, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.790 sec/batch; 64h:24m:11s remains)
2017-12-10 18:01:00.414561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.082777321 -0.096204586 -0.10754291 -0.11648813 -0.12049773 -0.11677535 -0.10627023 -0.093712419 -0.08404173 -0.078812085 -0.076853029 -0.078137837 -0.079932921 -0.082290709 -0.087403595][-0.019495921 -0.030408058 -0.043697242 -0.059286289 -0.07296773 -0.077433981 -0.070709839 -0.057944283 -0.044856094 -0.033328716 -0.023273667 -0.016422311 -0.009349443 -0.003627039 -0.0054160883][0.1060224 0.10744683 0.0969986 0.074336141 0.045707557 0.023690717 0.014919482 0.016942764 0.027333101 0.045364249 0.067936994 0.09066233 0.11518283 0.13655533 0.1420811][0.27801713 0.30625004 0.30717462 0.2813119 0.23797648 0.19307998 0.15779872 0.13551158 0.1326779 0.15303494 0.18894361 0.2307719 0.27558783 0.31441775 0.3274141][0.4577181 0.52698231 0.55085224 0.53083652 0.4795976 0.41358772 0.34643379 0.28916541 0.26143685 0.27418843 0.31603995 0.37122256 0.42995107 0.47880489 0.49311006][0.58531404 0.69833946 0.752695 0.75142288 0.70925063 0.63827443 0.54998255 0.461999 0.4065955 0.40276596 0.43822291 0.49410793 0.55383879 0.59913677 0.60371828][0.61862785 0.76506293 0.85046393 0.87984836 0.86533332 0.80999672 0.72087431 0.62056714 0.54884893 0.53121364 0.55708271 0.60761684 0.65930974 0.68886846 0.67101067][0.54412377 0.70264673 0.81184149 0.87651235 0.90058112 0.87595403 0.80549586 0.7153334 0.64712113 0.62856132 0.65172946 0.69775778 0.73779827 0.74477553 0.6960066][0.3868514 0.53317934 0.65126914 0.74368304 0.80330718 0.81306851 0.77368087 0.71259451 0.6680637 0.66515887 0.69746184 0.74492508 0.77698845 0.7652055 0.68980896][0.20825832 0.3197076 0.4244315 0.52200782 0.59759015 0.63190347 0.62639332 0.60598344 0.60028356 0.627401 0.68068534 0.73816139 0.77120352 0.75281513 0.66614288][0.067252249 0.13280718 0.20506138 0.28216779 0.34873003 0.39043337 0.41103756 0.42855173 0.46094945 0.51892436 0.5952118 0.6666584 0.70926613 0.6997754 0.62303424][-0.0046875309 0.018920381 0.052511979 0.09452232 0.13417424 0.16538626 0.19427796 0.23065138 0.28285834 0.35715 0.44687146 0.52928162 0.58524233 0.5947963 0.54301143][-0.014287568 -0.020810777 -0.021131298 -0.015767494 -0.0089865653 0.0006298676 0.020634195 0.054157916 0.10396146 0.17425543 0.25996086 0.34153464 0.40441665 0.43088606 0.40614772][0.0037834283 -0.017516682 -0.039294254 -0.060810614 -0.080744371 -0.092752129 -0.088713005 -0.069830328 -0.036072619 0.01588678 0.082522526 0.14971133 0.20709872 0.23936343 0.23354681][0.011864234 -0.011434858 -0.040345922 -0.074073151 -0.10775244 -0.13281043 -0.14139311 -0.13749215 -0.12236428 -0.092839479 -0.050919112 -0.0051464541 0.0377295 0.065426081 0.06811434]]...]
INFO - root - 2017-12-10 18:01:08.395627: step 39210, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.796 sec/batch; 64h:53m:24s remains)
INFO - root - 2017-12-10 18:01:16.020839: step 39220, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 62h:37m:36s remains)
INFO - root - 2017-12-10 18:01:23.799181: step 39230, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.755 sec/batch; 61h:32m:32s remains)
INFO - root - 2017-12-10 18:01:31.584192: step 39240, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.787 sec/batch; 64h:08m:15s remains)
INFO - root - 2017-12-10 18:01:39.544734: step 39250, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 64h:32m:18s remains)
INFO - root - 2017-12-10 18:01:47.416463: step 39260, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 62h:51m:47s remains)
INFO - root - 2017-12-10 18:01:55.335397: step 39270, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 62h:37m:12s remains)
INFO - root - 2017-12-10 18:02:03.091210: step 39280, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 62h:50m:45s remains)
INFO - root - 2017-12-10 18:02:10.947099: step 39290, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.819 sec/batch; 66h:43m:17s remains)
INFO - root - 2017-12-10 18:02:18.680509: step 39300, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 64h:37m:43s remains)
2017-12-10 18:02:19.541824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.072906263 -0.072397418 -0.070540294 -0.067868091 -0.063064329 -0.056277338 -0.048229579 -0.039365105 -0.031950351 -0.031921785 -0.042215511 -0.059311274 -0.07693629 -0.090185173 -0.095772207][-0.067612812 -0.061384406 -0.054733448 -0.047436219 -0.036622223 -0.022846419 -0.0073731979 0.0089801261 0.022143262 0.022355851 0.0049582189 -0.024715021 -0.056519289 -0.082319766 -0.095892832][-0.041075457 -0.02535571 -0.011498081 0.0021216134 0.020254945 0.042061586 0.065428793 0.08884459 0.10679223 0.106047 0.080047004 0.035629518 -0.012523621 -0.051812045 -0.072341338][0.0080578579 0.036532562 0.060888179 0.083300754 0.10954907 0.13916801 0.16927727 0.19715342 0.21658558 0.21240491 0.17647289 0.11720274 0.054022823 0.0039793071 -0.019747201][0.084877111 0.12876359 0.1656322 0.19688912 0.2278073 0.2598837 0.29077855 0.31646845 0.33043158 0.31829414 0.27237147 0.20278612 0.13139741 0.077738255 0.057102144][0.19505984 0.24957235 0.29319581 0.3264485 0.35215023 0.37576795 0.39786771 0.41375992 0.41670164 0.39580494 0.3459875 0.27771148 0.21068162 0.16366754 0.15275404][0.32862619 0.37784761 0.41318941 0.4356828 0.44468716 0.4500339 0.45713124 0.46206984 0.4572013 0.43546435 0.39447385 0.34300295 0.294321 0.26325756 0.26549986][0.457614 0.48248416 0.49158621 0.49110338 0.47829139 0.46591544 0.4632988 0.46641678 0.46465179 0.45303792 0.43162581 0.40713355 0.38476264 0.37300545 0.38575363][0.54492277 0.535636 0.5108965 0.48735121 0.46217576 0.44717231 0.45108804 0.46639389 0.47705472 0.47881737 0.475801 0.47526827 0.47544491 0.47729817 0.49216884][0.5657208 0.52562511 0.4762314 0.44491363 0.42870241 0.43259272 0.45943046 0.49554458 0.51913226 0.52689922 0.52976811 0.53977942 0.55012757 0.55384487 0.55915576][0.51269054 0.45941409 0.40862891 0.39371127 0.40859687 0.44594055 0.49990824 0.55223757 0.57952476 0.58114934 0.57541192 0.58049476 0.58648181 0.57968819 0.56575775][0.40253589 0.35827997 0.32988635 0.34966642 0.40604746 0.47769248 0.5507006 0.60649943 0.62553388 0.61119348 0.58679825 0.57521397 0.56563151 0.54087263 0.50501907][0.26467592 0.24625255 0.25316095 0.31024733 0.400692 0.49342269 0.570213 0.61590832 0.61828804 0.58505023 0.54061663 0.50889587 0.48051032 0.43876195 0.386281][0.13118531 0.14092486 0.17919569 0.26005861 0.36450067 0.45905173 0.5256592 0.55371493 0.53813577 0.48980251 0.43113026 0.38405141 0.34243608 0.29238313 0.23402956][0.029114595 0.054710384 0.1057393 0.18683198 0.28133515 0.36027607 0.40860936 0.41906312 0.39165151 0.33846238 0.27758047 0.22646782 0.18273027 0.13650055 0.084962279]]...]
INFO - root - 2017-12-10 18:02:27.530125: step 39310, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 63h:47m:27s remains)
INFO - root - 2017-12-10 18:02:35.407986: step 39320, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 63h:34m:40s remains)
INFO - root - 2017-12-10 18:02:43.304068: step 39330, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 65h:31m:16s remains)
INFO - root - 2017-12-10 18:02:51.154765: step 39340, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 64h:48m:58s remains)
INFO - root - 2017-12-10 18:02:58.957928: step 39350, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 61h:38m:26s remains)
INFO - root - 2017-12-10 18:03:06.915365: step 39360, loss = 0.70, batch loss = 0.64 (11.2 examples/sec; 0.717 sec/batch; 58h:24m:04s remains)
INFO - root - 2017-12-10 18:03:14.870294: step 39370, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 65h:32m:03s remains)
INFO - root - 2017-12-10 18:03:22.519443: step 39380, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 63h:02m:52s remains)
INFO - root - 2017-12-10 18:03:30.387447: step 39390, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.823 sec/batch; 67h:02m:40s remains)
INFO - root - 2017-12-10 18:03:38.205636: step 39400, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.818 sec/batch; 66h:33m:43s remains)
2017-12-10 18:03:39.068681: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.076178335 -0.067717686 -0.055286497 -0.044595513 -0.040087942 -0.043654189 -0.052985858 -0.063687243 -0.0724292 -0.077620178 -0.078498334 -0.076104939 -0.072537243 -0.069154561 -0.065805852][-0.053031553 -0.042696893 -0.020603096 0.0034903823 0.019074699 0.019799853 0.0066920752 -0.014430228 -0.037978005 -0.058879755 -0.072974421 -0.078899123 -0.079087406 -0.076750264 -0.073364943][-0.00092359167 0.0095607778 0.043581307 0.087897524 0.12387711 0.13798434 0.12663004 0.094843052 0.05078106 0.0047319182 -0.033287738 -0.056875106 -0.0670582 -0.06863381 -0.065402985][0.076015495 0.083983056 0.12941267 0.19749124 0.26144543 0.29891166 0.29818869 0.26030013 0.19514069 0.11911858 0.049493257 -0.00074699789 -0.02863762 -0.03854683 -0.0364345][0.16270865 0.16439356 0.21456084 0.30134574 0.39340943 0.46025357 0.47971243 0.44551748 0.36713305 0.26603982 0.16618553 0.0878143 0.040833734 0.023651125 0.028114412][0.23001425 0.22277199 0.26783589 0.36038202 0.47065079 0.56436288 0.61044282 0.5934363 0.51866746 0.40877187 0.29165411 0.1941276 0.13447955 0.11617728 0.12969412][0.24847025 0.23304461 0.26582336 0.34994954 0.46272895 0.57216758 0.64307719 0.65273607 0.59986305 0.50302231 0.38909572 0.28881168 0.2284677 0.21775882 0.24769092][0.20976841 0.19027519 0.20865455 0.27370381 0.3725065 0.48056182 0.56513369 0.60108966 0.58066493 0.51498389 0.42493525 0.34034562 0.2923871 0.29633805 0.34610888][0.13127173 0.11199206 0.11881193 0.16140629 0.23576905 0.32696706 0.410203 0.46309677 0.47382668 0.44535697 0.39012921 0.33304873 0.3057878 0.32720193 0.39524934][0.043982089 0.030160218 0.031777024 0.055942666 0.1043908 0.17067581 0.2398369 0.29590884 0.32651973 0.3284069 0.30581075 0.27662191 0.26909173 0.30340865 0.3819305][-0.012015168 -0.017241867 -0.015245781 -0.0029646226 0.023888139 0.0645877 0.11278564 0.16000229 0.1961768 0.21476401 0.21359517 0.20324016 0.20579243 0.24069428 0.31439608][-0.017094132 -0.014844361 -0.010591599 -0.0040902141 0.0088362815 0.029850297 0.058096528 0.091302432 0.12315975 0.14613166 0.15385495 0.15045713 0.151317 0.17349833 0.22609021][0.021574976 0.028706178 0.033872847 0.037917234 0.043799855 0.052953839 0.066664629 0.086728029 0.11085869 0.13171095 0.14013936 0.13522767 0.12677889 0.12877609 0.15095474][0.08239156 0.090124764 0.092832521 0.09454143 0.097089678 0.10028677 0.10518592 0.1155189 0.13225508 0.14854611 0.15388086 0.1444708 0.12623672 0.1099417 0.10410998][0.14100675 0.14586411 0.14315446 0.14138187 0.14175512 0.14219947 0.14224741 0.145952 0.15627675 0.16700476 0.16780369 0.15440112 0.13075133 0.10463234 0.082451463]]...]
INFO - root - 2017-12-10 18:03:47.038213: step 39410, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 65h:26m:30s remains)
INFO - root - 2017-12-10 18:03:54.926141: step 39420, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 63h:01m:02s remains)
INFO - root - 2017-12-10 18:04:02.704559: step 39430, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 62h:59m:32s remains)
INFO - root - 2017-12-10 18:04:10.589900: step 39440, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 66h:02m:02s remains)
INFO - root - 2017-12-10 18:04:18.348410: step 39450, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 64h:51m:01s remains)
INFO - root - 2017-12-10 18:04:26.151235: step 39460, loss = 0.67, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 65h:18m:58s remains)
INFO - root - 2017-12-10 18:04:34.144788: step 39470, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 63h:26m:05s remains)
INFO - root - 2017-12-10 18:04:42.016843: step 39480, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 64h:29m:58s remains)
INFO - root - 2017-12-10 18:04:49.887102: step 39490, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 65h:01m:39s remains)
INFO - root - 2017-12-10 18:04:57.740130: step 39500, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 63h:32m:32s remains)
2017-12-10 18:04:58.569249: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.39792576 0.41527894 0.38062319 0.32207766 0.27149189 0.2481913 0.24728873 0.25032625 0.24970452 0.25131032 0.26458716 0.29080561 0.32699296 0.37395605 0.41864088][0.48553184 0.4896042 0.43661186 0.36718976 0.3155826 0.29727328 0.30146235 0.30394611 0.29689276 0.29007459 0.29734498 0.32194605 0.3597284 0.40799206 0.45001474][0.50972462 0.50106239 0.44310775 0.3830106 0.35103244 0.3529135 0.36855948 0.36977613 0.35033461 0.32344097 0.30925205 0.31559235 0.34040204 0.37887496 0.41122195][0.464847 0.45369378 0.41075456 0.38185224 0.38721141 0.41978762 0.44943923 0.44657856 0.40823168 0.35200918 0.30529866 0.2814967 0.28326368 0.30553046 0.32647288][0.370234 0.36994368 0.36132929 0.38001376 0.431519 0.49564037 0.53454655 0.52144939 0.45922109 0.37144136 0.29218984 0.23860449 0.21803364 0.22628249 0.24071959][0.27010751 0.291792 0.32797372 0.39860305 0.49272585 0.57919192 0.61675137 0.58520854 0.49654379 0.3810311 0.27731973 0.20338948 0.16905858 0.17130791 0.1874097][0.19937688 0.24676356 0.32593745 0.44047067 0.56402069 0.65847141 0.68405318 0.62951779 0.51614529 0.38032997 0.26373085 0.18318225 0.14746644 0.15372239 0.17950282][0.17142619 0.23775904 0.34420118 0.48141944 0.61180067 0.69646919 0.70194072 0.626621 0.49798062 0.35583651 0.24196804 0.16986468 0.14393488 0.16072519 0.19986954][0.17023715 0.24068303 0.351331 0.48503494 0.59936696 0.6606425 0.64536053 0.56038481 0.4343935 0.30524832 0.20990659 0.15669376 0.14509082 0.1719121 0.22174247][0.16635656 0.22716081 0.32282308 0.43292305 0.51697171 0.55002385 0.51968962 0.43837211 0.3315571 0.22996669 0.16164166 0.12982132 0.13053286 0.16216138 0.21542905][0.13521951 0.17854749 0.24901356 0.326349 0.37706476 0.38575241 0.34960061 0.28248641 0.20376085 0.13458137 0.093360446 0.079789415 0.08803577 0.11801873 0.16563685][0.076521367 0.099785738 0.1429906 0.18758927 0.20978446 0.20266053 0.16976722 0.12314031 0.0745213 0.035765037 0.017129643 0.016625112 0.028228907 0.05244283 0.088661455][0.0066734985 0.012808532 0.032657482 0.050499287 0.052279573 0.037484851 0.012506086 -0.013805858 -0.036280159 -0.050359111 -0.052264981 -0.044742603 -0.032551937 -0.014904011 0.0094135478][-0.054304104 -0.058713183 -0.053664245 -0.051957179 -0.06010722 -0.075433396 -0.091419578 -0.1025243 -0.10773419 -0.10681854 -0.099844486 -0.089520887 -0.078740984 -0.066602014 -0.051604435][-0.093458563 -0.10194904 -0.10314179 -0.1073301 -0.11659399 -0.12787993 -0.13632292 -0.13897178 -0.13649948 -0.13013671 -0.12122607 -0.11192554 -0.10367766 -0.095838346 -0.087424837]]...]
INFO - root - 2017-12-10 18:05:06.365564: step 39510, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 63h:40m:37s remains)
INFO - root - 2017-12-10 18:05:14.204438: step 39520, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 65h:27m:53s remains)
INFO - root - 2017-12-10 18:05:22.133723: step 39530, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 65h:21m:46s remains)
INFO - root - 2017-12-10 18:05:29.626600: step 39540, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 61h:50m:51s remains)
INFO - root - 2017-12-10 18:05:37.455226: step 39550, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 63h:22m:40s remains)
INFO - root - 2017-12-10 18:05:45.258157: step 39560, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 64h:10m:23s remains)
INFO - root - 2017-12-10 18:05:53.280645: step 39570, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 66h:16m:47s remains)
INFO - root - 2017-12-10 18:06:01.159992: step 39580, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 62h:48m:29s remains)
INFO - root - 2017-12-10 18:06:09.038372: step 39590, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 62h:37m:13s remains)
INFO - root - 2017-12-10 18:06:16.865101: step 39600, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 64h:10m:09s remains)
2017-12-10 18:06:17.684710: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18363114 0.21185398 0.21749496 0.20337263 0.18111628 0.16278327 0.15621473 0.16317935 0.17924222 0.20307103 0.22431184 0.22907823 0.21450083 0.18996274 0.16330852][0.18401267 0.21132047 0.21826644 0.20692907 0.18905132 0.17724995 0.17847666 0.19244492 0.21238384 0.23801066 0.25935179 0.2609306 0.2399957 0.20885321 0.17913809][0.17310961 0.19990112 0.20946094 0.20256239 0.19057426 0.18628959 0.19547871 0.2160475 0.23880976 0.26374504 0.281585 0.27747518 0.24917281 0.21165964 0.18146043][0.17082553 0.19877428 0.21058849 0.206331 0.19725752 0.19666658 0.20961654 0.23256901 0.25473183 0.27676955 0.29064038 0.28183773 0.24874793 0.20879148 0.18235023][0.17244115 0.20162094 0.21436574 0.21072713 0.20218083 0.20139541 0.21243285 0.23159952 0.2482188 0.2645686 0.27493337 0.26603884 0.23618703 0.20200838 0.18552579][0.17264768 0.20325108 0.21666582 0.21322456 0.20435108 0.20089085 0.20554148 0.21518651 0.22120243 0.22882055 0.23547322 0.22968906 0.20939018 0.18779206 0.18472594][0.17995343 0.21511355 0.23137821 0.22888112 0.21916354 0.212075 0.20907997 0.20783259 0.20305967 0.20238772 0.20453908 0.19976115 0.18685377 0.17623408 0.1836739][0.19367261 0.2373545 0.25899351 0.25836182 0.24779007 0.23756485 0.22766215 0.21709496 0.20521298 0.20090485 0.20130873 0.19648072 0.18658423 0.18156517 0.19352168][0.20620961 0.2583589 0.28529236 0.2864719 0.27481759 0.26147041 0.24459265 0.22552247 0.20993733 0.20804758 0.2131616 0.2120789 0.20563866 0.20431593 0.21581322][0.21163377 0.26722911 0.29517865 0.29551589 0.28268 0.26828188 0.24899913 0.22794659 0.21576692 0.22213809 0.23605241 0.24079844 0.23725232 0.23603828 0.24064609][0.21552172 0.27120906 0.29695666 0.294345 0.27999485 0.26619932 0.2495724 0.23322728 0.22938178 0.24534886 0.26645496 0.27389461 0.26957986 0.26435569 0.25776064][0.21665819 0.27164981 0.29550681 0.29123151 0.27622268 0.26303744 0.24994947 0.23995729 0.24412318 0.26582325 0.28818229 0.29311043 0.28442907 0.27302 0.25674379][0.2080705 0.25927016 0.28047055 0.27558321 0.2605826 0.24660474 0.23431246 0.22762452 0.23602009 0.25897285 0.27932003 0.28159827 0.27050719 0.25607324 0.23609932][0.19287963 0.23454832 0.24946393 0.24266942 0.22782685 0.21326725 0.20105274 0.19576439 0.20536509 0.22636202 0.24302709 0.24331762 0.2319694 0.21770535 0.19851325][0.17061302 0.19913591 0.20439143 0.19329548 0.17827961 0.16515006 0.15630127 0.15532613 0.16681227 0.18465541 0.19574438 0.19234665 0.1795613 0.16488855 0.14714956]]...]
INFO - root - 2017-12-10 18:06:25.579664: step 39610, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 65h:17m:21s remains)
INFO - root - 2017-12-10 18:06:33.245854: step 39620, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 62h:24m:03s remains)
INFO - root - 2017-12-10 18:06:40.998205: step 39630, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 64h:37m:07s remains)
INFO - root - 2017-12-10 18:06:48.849555: step 39640, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 65h:43m:24s remains)
INFO - root - 2017-12-10 18:06:56.769031: step 39650, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 64h:16m:20s remains)
INFO - root - 2017-12-10 18:07:04.654312: step 39660, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.785 sec/batch; 63h:52m:00s remains)
INFO - root - 2017-12-10 18:07:12.549812: step 39670, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 62h:47m:34s remains)
INFO - root - 2017-12-10 18:07:20.313601: step 39680, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 64h:02m:31s remains)
INFO - root - 2017-12-10 18:07:28.164502: step 39690, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 63h:42m:18s remains)
INFO - root - 2017-12-10 18:07:35.934525: step 39700, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 65h:18m:55s remains)
2017-12-10 18:07:36.860205: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.097941652 0.093892053 0.098253317 0.10619675 0.10786918 0.095149808 0.067125782 0.031162664 -0.00230394 -0.025551813 -0.037318353 -0.040414628 -0.037339907 -0.029765382 -0.017397923][0.080356523 0.07970465 0.092176713 0.11154962 0.12630577 0.12620194 0.10713246 0.074120782 0.037072361 0.0051261159 -0.017292334 -0.029255714 -0.030004237 -0.020035479 0.00044179536][0.063586481 0.06294845 0.077300854 0.10142791 0.12483255 0.1369424 0.13063759 0.10700237 0.072421961 0.035314485 0.0028359424 -0.019318158 -0.025708914 -0.014532517 0.013124171][0.060709033 0.056704052 0.065853432 0.086481676 0.11167444 0.13264954 0.14040296 0.1309167 0.10459159 0.066343918 0.024539858 -0.009975926 -0.026129408 -0.018809108 0.010164673][0.073378608 0.065105364 0.065720923 0.078427896 0.10146751 0.12959769 0.15299337 0.16173978 0.14831772 0.11158511 0.059686542 0.0087377951 -0.023683691 -0.028473753 -0.00764922][0.0921375 0.081673294 0.076059036 0.083168149 0.10650279 0.14449194 0.18658656 0.21633717 0.21777838 0.18300559 0.11956085 0.048768371 -0.0051126042 -0.029189415 -0.025163664][0.10289805 0.094402514 0.089452714 0.099200457 0.13102542 0.18558922 0.25049952 0.30259302 0.31840217 0.28470013 0.20915949 0.11801251 0.041107036 -0.0055002938 -0.023034485][0.095073983 0.092451833 0.095271111 0.1161782 0.16458887 0.24129066 0.33090764 0.40427634 0.43252322 0.39977863 0.31471443 0.207128 0.11070859 0.043890405 0.0060137296][0.071099438 0.075204 0.088358909 0.12295469 0.18843833 0.28427291 0.392349 0.47982585 0.51592892 0.48522004 0.39777145 0.28446564 0.17967597 0.10243031 0.05262569][0.0469156 0.055222232 0.074987136 0.11723478 0.18983877 0.29076698 0.40138036 0.48919874 0.5257057 0.49837166 0.41813368 0.31383443 0.21646895 0.14342348 0.094641939][0.040766526 0.049141604 0.0681384 0.10687305 0.17095748 0.25771645 0.35121825 0.42390889 0.45299757 0.4299722 0.36521864 0.28258631 0.20639999 0.15021197 0.11361209][0.061966006 0.067055054 0.078484878 0.10358222 0.14572617 0.20290174 0.26459587 0.3115668 0.32864907 0.310972 0.26662427 0.21220206 0.16369474 0.12983426 0.10975791][0.1060058 0.10766829 0.1091628 0.11661024 0.13166623 0.15380244 0.1788152 0.19704619 0.20111036 0.18887219 0.16512461 0.13870023 0.11721925 0.1043362 0.098697208][0.1589518 0.15972684 0.15381642 0.14664853 0.13884577 0.13145307 0.12557049 0.11992683 0.11333022 0.10525614 0.097213641 0.091619223 0.089467868 0.090345539 0.092719033][0.20082618 0.20383093 0.19540915 0.18038054 0.15914354 0.13404292 0.10936999 0.089230731 0.07599593 0.069615491 0.069120653 0.0731107 0.079001211 0.084262632 0.087568082]]...]
INFO - root - 2017-12-10 18:07:44.776134: step 39710, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 63h:26m:12s remains)
INFO - root - 2017-12-10 18:07:52.413462: step 39720, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 64h:42m:41s remains)
INFO - root - 2017-12-10 18:08:00.348183: step 39730, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 64h:40m:26s remains)
INFO - root - 2017-12-10 18:08:08.234440: step 39740, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 64h:55m:00s remains)
INFO - root - 2017-12-10 18:08:16.095776: step 39750, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 65h:02m:20s remains)
INFO - root - 2017-12-10 18:08:23.932233: step 39760, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 63h:30m:39s remains)
INFO - root - 2017-12-10 18:08:31.718304: step 39770, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 63h:56m:24s remains)
INFO - root - 2017-12-10 18:08:39.425160: step 39780, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 63h:53m:41s remains)
INFO - root - 2017-12-10 18:08:47.385327: step 39790, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.754 sec/batch; 61h:17m:44s remains)
INFO - root - 2017-12-10 18:08:55.212505: step 39800, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.821 sec/batch; 66h:45m:11s remains)
2017-12-10 18:08:56.032839: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10275317 0.11958615 0.12658946 0.11990952 0.10535546 0.094549172 0.09793368 0.11765166 0.14667597 0.16997541 0.17471519 0.15493992 0.11899813 0.081514791 0.050978206][0.054213952 0.057790887 0.055149235 0.047269396 0.043800425 0.055682715 0.086116545 0.12731399 0.16509657 0.18243749 0.16950794 0.12899381 0.075834237 0.028410209 -0.0046866932][0.018164955 0.014589138 0.0093856584 0.0072865891 0.021502601 0.060137089 0.11825252 0.17815621 0.21819162 0.22097649 0.18368274 0.11984561 0.051797278 -0.000928794 -0.032832988][0.0010372238 -0.0021603929 -0.0018363915 0.0098218881 0.047886219 0.11605019 0.20080467 0.27530164 0.31126907 0.29176742 0.22305974 0.1315908 0.047476806 -0.0097772675 -0.039614413][0.0020541307 0.0045320778 0.016024102 0.046866011 0.11239813 0.21055378 0.31898096 0.40254954 0.42827177 0.38112637 0.27731407 0.15635246 0.055213198 -0.0073720594 -0.036279172][0.032049906 0.044812594 0.070301868 0.12128133 0.21213098 0.33438852 0.45680943 0.53669035 0.5400883 0.45856896 0.32008034 0.17515531 0.06392356 0.00098122412 -0.025179392][0.080438025 0.1059702 0.14668952 0.21727538 0.32907125 0.46649387 0.59000558 0.6513136 0.62016952 0.49988154 0.33235666 0.17498749 0.065691791 0.010401093 -0.0093988348][0.12957981 0.16809009 0.22500566 0.31518978 0.44332764 0.58576894 0.69680864 0.72753179 0.65672868 0.50224417 0.31807566 0.16172801 0.064443476 0.022331528 0.012188569][0.16088071 0.21048194 0.28202295 0.38779896 0.5230065 0.65678203 0.74175137 0.73517853 0.62960243 0.45664206 0.27514869 0.13510306 0.058029711 0.032541566 0.035080705][0.16559586 0.22089189 0.29969063 0.40898913 0.53469187 0.64287466 0.69108921 0.64926636 0.52461356 0.35675773 0.20010582 0.091850549 0.042650387 0.037294343 0.055859055][0.14201641 0.1942926 0.26833466 0.36568478 0.4661617 0.5380829 0.54902697 0.48405951 0.36099404 0.21926466 0.10229302 0.033707391 0.015305207 0.031466804 0.067448415][0.097758427 0.13741817 0.19403948 0.26604128 0.33309236 0.37029734 0.357038 0.29030129 0.19051042 0.088787355 0.014707546 -0.018581284 -0.013235097 0.018169114 0.065949552][0.058991317 0.081431761 0.11324951 0.15306012 0.18610203 0.19664602 0.1746099 0.12312536 0.058292434 -0.001250784 -0.038882669 -0.047843534 -0.030250367 0.0060805669 0.055751786][0.041575022 0.050698202 0.060290523 0.070606865 0.074802555 0.066337146 0.043357559 0.011231855 -0.020721689 -0.045770336 -0.05759766 -0.053169914 -0.03312777 -0.00041382507 0.043023333][0.052853495 0.057914704 0.054625232 0.044017076 0.026602875 0.0040308125 -0.01947552 -0.038097374 -0.048191492 -0.051450755 -0.048767313 -0.039681185 -0.023639239 0.00038659479 0.033053223]]...]
INFO - root - 2017-12-10 18:09:03.769888: step 39810, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 63h:37m:55s remains)
INFO - root - 2017-12-10 18:09:11.692731: step 39820, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 64h:59m:42s remains)
INFO - root - 2017-12-10 18:09:19.555071: step 39830, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 63h:12m:49s remains)
INFO - root - 2017-12-10 18:09:27.328852: step 39840, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 63h:08m:34s remains)
INFO - root - 2017-12-10 18:09:35.191300: step 39850, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 61h:57m:40s remains)
INFO - root - 2017-12-10 18:09:42.872929: step 39860, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.787 sec/batch; 63h:56m:50s remains)
INFO - root - 2017-12-10 18:09:50.783818: step 39870, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 63h:25m:39s remains)
INFO - root - 2017-12-10 18:09:58.659644: step 39880, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 64h:24m:40s remains)
INFO - root - 2017-12-10 18:10:06.677150: step 39890, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 64h:27m:48s remains)
INFO - root - 2017-12-10 18:10:14.381978: step 39900, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 65h:28m:56s remains)
2017-12-10 18:10:15.175131: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19524845 0.16679996 0.13973729 0.13064344 0.13633509 0.1540727 0.19291963 0.24252391 0.27673942 0.27678126 0.24486351 0.18302652 0.094790563 0.0044860155 -0.059200868][0.28851369 0.26938939 0.25071883 0.25067043 0.26136228 0.27524734 0.30137393 0.33116257 0.34066659 0.31589004 0.26421657 0.18857938 0.090458468 -0.0063328785 -0.07211376][0.39486063 0.38806069 0.37925139 0.3887538 0.40450221 0.41265455 0.41831177 0.41462967 0.38436404 0.32352713 0.24569449 0.15607432 0.054832879 -0.037982471 -0.096707188][0.48600918 0.48801962 0.48639175 0.50707334 0.53507334 0.54785269 0.54026383 0.504345 0.43076655 0.32790121 0.2177764 0.11117893 0.00942553 -0.0734762 -0.12057927][0.52709913 0.53032047 0.53572553 0.57478511 0.63033265 0.66776836 0.66543376 0.60942894 0.49709058 0.35016096 0.20174649 0.07284186 -0.030260911 -0.10122186 -0.13582639][0.49407157 0.50240171 0.523314 0.59224051 0.68931574 0.76681226 0.78489369 0.72104138 0.57808363 0.39082333 0.20441665 0.051729143 -0.054902453 -0.11630257 -0.14146586][0.39221722 0.41435882 0.46317655 0.57124466 0.71487731 0.83544183 0.87939292 0.81695622 0.65596539 0.44151694 0.22801332 0.057082377 -0.053825166 -0.1110114 -0.13284849][0.25496066 0.29411665 0.37426096 0.51896369 0.69932848 0.850413 0.91396767 0.85904533 0.69695979 0.47663316 0.25666717 0.081583507 -0.029730013 -0.086092547 -0.10946307][0.13891406 0.18871495 0.28793055 0.4496806 0.64078087 0.79716319 0.86645013 0.82267344 0.67801714 0.47647765 0.27323952 0.11006255 0.0041814805 -0.053128038 -0.080842763][0.083151937 0.1293931 0.22479099 0.37510511 0.54603177 0.68152624 0.74272704 0.711692 0.59843022 0.4352949 0.26745424 0.12901804 0.033805687 -0.023999406 -0.056528706][0.07672926 0.1072986 0.17791638 0.29255056 0.42059907 0.51869231 0.56286734 0.54217505 0.4631075 0.34578249 0.22357056 0.11962789 0.042206943 -0.01121798 -0.044738382][0.084248625 0.090763889 0.12494824 0.19229104 0.27027747 0.32926258 0.35638875 0.34441718 0.29546785 0.22044091 0.14275694 0.074907869 0.019225683 -0.024287568 -0.052641261][0.0705886 0.056881547 0.056851733 0.07961905 0.11278254 0.13893098 0.15204142 0.14614838 0.11975349 0.078048162 0.036470905 -0.00055841642 -0.0348257 -0.064170994 -0.081276074][0.02852953 0.0052951165 -0.015715977 -0.023548163 -0.021672321 -0.01839507 -0.015669055 -0.018313147 -0.029642083 -0.048018653 -0.064990878 -0.081203081 -0.099175423 -0.11407706 -0.1175041][-0.023911241 -0.046789695 -0.073034286 -0.093079939 -0.10563228 -0.11400547 -0.1181009 -0.12069014 -0.12473772 -0.13039802 -0.13425083 -0.13896295 -0.14603719 -0.14928645 -0.14192443]]...]
INFO - root - 2017-12-10 18:10:23.012285: step 39910, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.808 sec/batch; 65h:41m:08s remains)
INFO - root - 2017-12-10 18:10:30.859710: step 39920, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 65h:07m:43s remains)
INFO - root - 2017-12-10 18:10:38.788215: step 39930, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 63h:37m:14s remains)
INFO - root - 2017-12-10 18:10:46.493850: step 39940, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 61h:47m:19s remains)
INFO - root - 2017-12-10 18:10:54.343136: step 39950, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 62h:55m:41s remains)
INFO - root - 2017-12-10 18:11:02.407872: step 39960, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 65h:38m:22s remains)
INFO - root - 2017-12-10 18:11:10.271635: step 39970, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 63h:20m:22s remains)
INFO - root - 2017-12-10 18:11:18.095751: step 39980, loss = 0.70, batch loss = 0.65 (10.6 examples/sec; 0.755 sec/batch; 61h:18m:39s remains)
INFO - root - 2017-12-10 18:11:25.823582: step 39990, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 63h:07m:02s remains)
INFO - root - 2017-12-10 18:11:33.598811: step 40000, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 62h:21m:59s remains)
2017-12-10 18:11:34.448956: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12153849 0.17537805 0.21749693 0.24227017 0.25660071 0.2541773 0.23566477 0.21123047 0.19242252 0.17830378 0.16068165 0.14291316 0.1337443 0.13988547 0.1542059][0.11953076 0.17348404 0.21725836 0.2459041 0.26629749 0.27034584 0.26025558 0.24370715 0.23013715 0.21615967 0.19344023 0.1705365 0.15842298 0.16352068 0.17691752][0.11523855 0.1694662 0.21655384 0.25245133 0.28247336 0.29694307 0.29894513 0.29325345 0.28565598 0.27147835 0.24376117 0.2171638 0.20438573 0.21031377 0.224238][0.11696529 0.17463624 0.22938316 0.27760836 0.32066616 0.34679481 0.36020955 0.36384752 0.36080685 0.34609762 0.31587932 0.29116458 0.28461996 0.29739487 0.31459028][0.12221224 0.18604466 0.25213397 0.31619036 0.37344146 0.41024494 0.43258867 0.44261789 0.44257447 0.42812037 0.39980203 0.38357648 0.39000544 0.4147279 0.43590671][0.12974457 0.20171738 0.28129837 0.36198342 0.43133479 0.47534522 0.50243491 0.51500922 0.51587617 0.50198895 0.47888997 0.47441727 0.4949238 0.52950895 0.54910719][0.13360685 0.21347618 0.30594784 0.40035242 0.47681233 0.52285177 0.54866952 0.55701846 0.55352968 0.53915823 0.52411771 0.53238851 0.56363004 0.60181922 0.613928][0.12880515 0.21522716 0.3181023 0.4214243 0.50002116 0.5439077 0.56298327 0.56104523 0.54818147 0.53201789 0.524911 0.54182488 0.57503325 0.60639375 0.60449415][0.11605666 0.20590465 0.31326574 0.41776803 0.4924446 0.5295772 0.53694922 0.52044082 0.496154 0.47803292 0.47704861 0.49653804 0.5236851 0.54157591 0.52459913][0.096594088 0.18278389 0.283836 0.37795317 0.4405486 0.4655472 0.45867172 0.42829654 0.39502904 0.37595332 0.37784785 0.39458504 0.41182333 0.41638303 0.39017573][0.078223117 0.15373194 0.23882875 0.31308803 0.35723567 0.36604556 0.34486231 0.30519721 0.26879105 0.25072017 0.25198624 0.26234278 0.26953876 0.26503295 0.23768234][0.067115135 0.12945081 0.1944737 0.24561059 0.27088293 0.26486331 0.23322718 0.19072038 0.15800266 0.14433488 0.1441115 0.14741507 0.1469159 0.13899536 0.11765717][0.067894734 0.11895338 0.16508092 0.19543932 0.20661223 0.1923123 0.15757826 0.11883691 0.09496624 0.088154241 0.086489066 0.083188772 0.077061392 0.068519965 0.054291721][0.080048271 0.12439879 0.156733 0.17177171 0.17469569 0.15905339 0.12800169 0.097569481 0.084918462 0.086236745 0.084265336 0.075175665 0.063345723 0.052626081 0.04127821][0.0911541 0.13186622 0.15580387 0.16101234 0.159502 0.1463861 0.12288201 0.10274179 0.10060965 0.1086989 0.10613421 0.0911324 0.072074622 0.055897303 0.042711154]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 18:11:43.453327: step 40010, loss = 0.70, batch loss = 0.64 (12.4 examples/sec; 0.646 sec/batch; 52h:31m:20s remains)
INFO - root - 2017-12-10 18:11:51.269616: step 40020, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 62h:31m:13s remains)
INFO - root - 2017-12-10 18:11:59.319879: step 40030, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.812 sec/batch; 65h:56m:26s remains)
INFO - root - 2017-12-10 18:12:07.140755: step 40040, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 64h:37m:38s remains)
INFO - root - 2017-12-10 18:12:15.016272: step 40050, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 63h:13m:13s remains)
INFO - root - 2017-12-10 18:12:22.852680: step 40060, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 63h:03m:01s remains)
INFO - root - 2017-12-10 18:12:30.642949: step 40070, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 65h:18m:57s remains)
INFO - root - 2017-12-10 18:12:38.455198: step 40080, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 64h:45m:38s remains)
INFO - root - 2017-12-10 18:12:46.324790: step 40090, loss = 0.70, batch loss = 0.64 (11.0 examples/sec; 0.730 sec/batch; 59h:19m:28s remains)
INFO - root - 2017-12-10 18:12:54.011044: step 40100, loss = 0.68, batch loss = 0.63 (10.8 examples/sec; 0.739 sec/batch; 60h:01m:11s remains)
2017-12-10 18:12:54.846138: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.51367 0.51134795 0.48621604 0.45017922 0.42245811 0.39787671 0.36540833 0.32925984 0.30167 0.27969733 0.25616354 0.2359165 0.22232857 0.21075849 0.19513588][0.54342663 0.53709739 0.51346707 0.48274568 0.45949435 0.43673798 0.40243331 0.35938314 0.32110161 0.28907785 0.25932729 0.237495 0.22505924 0.21363156 0.19625539][0.49800941 0.49451777 0.48214129 0.46585014 0.45426643 0.43917081 0.40735257 0.36072028 0.31513426 0.27765009 0.24718794 0.22835766 0.22021723 0.21140791 0.19395053][0.40801534 0.41483235 0.42233992 0.4283424 0.434626 0.431988 0.40617719 0.35909304 0.30963039 0.27026653 0.2431809 0.23097286 0.23008946 0.2270706 0.2126638][0.31069654 0.33014816 0.35874262 0.38790473 0.41355062 0.42604512 0.41010982 0.36760554 0.31953546 0.2826207 0.26209584 0.25757641 0.26280573 0.26322475 0.25074929][0.23224306 0.259605 0.30121762 0.34575185 0.38507769 0.41102692 0.40800813 0.37525997 0.33229503 0.30010232 0.28760257 0.29086092 0.30042291 0.30186427 0.28988647][0.18875261 0.21895131 0.26323789 0.31263092 0.35667545 0.39015079 0.3990854 0.37880343 0.34374702 0.31811339 0.31492066 0.32659039 0.33972782 0.34042463 0.32753804][0.18568058 0.21175684 0.25034088 0.29739192 0.34086597 0.37728268 0.39497864 0.38661614 0.36046717 0.3417435 0.34721142 0.36692685 0.38352412 0.3831749 0.36817074][0.21272716 0.22935618 0.25654268 0.2985414 0.34138578 0.3797355 0.40268815 0.40275627 0.38425097 0.37082064 0.38061321 0.40355384 0.42016432 0.41634116 0.39610532][0.25165939 0.25511515 0.26757625 0.30223641 0.34409487 0.38376212 0.40869865 0.41296029 0.40022498 0.39057812 0.40022963 0.42062247 0.43305135 0.42362034 0.39644665][0.28203741 0.27469438 0.27460393 0.30190414 0.34210476 0.38208437 0.40675005 0.41237459 0.40495309 0.40063387 0.41118598 0.42907134 0.43736789 0.42324057 0.38967147][0.29377747 0.28020453 0.27543971 0.30038491 0.34138745 0.3835451 0.40919626 0.41583589 0.41432288 0.41898292 0.43628344 0.45654178 0.46454322 0.44957986 0.41221946][0.30497262 0.28648293 0.27963129 0.3039372 0.34586287 0.39025587 0.41779697 0.42569417 0.43020779 0.44581068 0.47346419 0.4981207 0.50696343 0.493039 0.45374903][0.32382026 0.29849625 0.28706253 0.30841932 0.34898666 0.39426228 0.42446336 0.43486777 0.44483918 0.46960127 0.50546587 0.53093761 0.53681672 0.52244872 0.48270041][0.3268376 0.29637003 0.28049916 0.29694387 0.33292943 0.37608784 0.40775406 0.42075083 0.4345699 0.46451291 0.50399762 0.52656883 0.52669543 0.5112648 0.47396281]]...]
INFO - root - 2017-12-10 18:13:02.654382: step 40110, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 61h:46m:53s remains)
INFO - root - 2017-12-10 18:13:10.440547: step 40120, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 64h:04m:53s remains)
INFO - root - 2017-12-10 18:13:18.361339: step 40130, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 64h:59m:21s remains)
INFO - root - 2017-12-10 18:13:26.211701: step 40140, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 63h:59m:06s remains)
INFO - root - 2017-12-10 18:13:34.074289: step 40150, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 65h:43m:04s remains)
INFO - root - 2017-12-10 18:13:41.822745: step 40160, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 63h:50m:02s remains)
INFO - root - 2017-12-10 18:13:49.448877: step 40170, loss = 0.69, batch loss = 0.63 (13.3 examples/sec; 0.600 sec/batch; 48h:41m:40s remains)
INFO - root - 2017-12-10 18:13:57.238921: step 40180, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.785 sec/batch; 63h:45m:39s remains)
INFO - root - 2017-12-10 18:14:05.166096: step 40190, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 65h:07m:13s remains)
INFO - root - 2017-12-10 18:14:12.979944: step 40200, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 64h:32m:17s remains)
2017-12-10 18:14:13.792728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.046943031 -0.044099744 -0.043941218 -0.049424663 -0.054359019 -0.052762508 -0.044342972 -0.033318695 -0.023604702 -0.018212914 -0.021740431 -0.035773151 -0.055298004 -0.072398812 -0.0824922][-0.0087851761 0.0009476624 0.0033173086 -0.00472631 -0.011372825 -0.0069966749 0.0074392296 0.0241438 0.038049251 0.045752123 0.038883064 0.013863774 -0.021012204 -0.052421641 -0.072932862][0.047987871 0.0696608 0.078622989 0.071944155 0.066702686 0.075762317 0.096320882 0.11715577 0.13195375 0.13757905 0.1228655 0.08271876 0.028187342 -0.021771176 -0.055902533][0.12917499 0.17090493 0.19365019 0.19398285 0.19144908 0.20245343 0.22513579 0.24643478 0.25813082 0.25660604 0.22985277 0.17141908 0.093494713 0.020131085 -0.032300409][0.24263555 0.31298268 0.35578471 0.36638138 0.363339 0.3693282 0.38845241 0.40729898 0.41330519 0.40110186 0.35994893 0.2809763 0.17602777 0.074192978 -0.0013965989][0.36555171 0.46731 0.53251213 0.55478996 0.54907441 0.5460158 0.55931157 0.5748359 0.57273406 0.54539174 0.48652795 0.38505629 0.25179541 0.12131698 0.024143701][0.46195093 0.58835381 0.67209971 0.70588648 0.70095575 0.69564444 0.710807 0.72818363 0.71839333 0.67270082 0.5923686 0.46638724 0.30565846 0.15058041 0.038029503][0.50597459 0.639653 0.72827983 0.76568133 0.76219988 0.76252753 0.78829509 0.81334609 0.79934466 0.7385872 0.64170915 0.49901515 0.32203734 0.15488301 0.037873294][0.47937843 0.60105526 0.67961407 0.71099097 0.70701861 0.71507496 0.75177687 0.78282046 0.76642251 0.69871157 0.5988301 0.45846105 0.2875528 0.12909682 0.022289872][0.37567893 0.46999764 0.52877116 0.54917979 0.54513526 0.56074566 0.6040591 0.63666457 0.6195609 0.55449396 0.46587572 0.34645346 0.20275529 0.07229019 -0.011016632][0.21507801 0.27357602 0.30924273 0.31937551 0.31825885 0.34057179 0.38508 0.41527054 0.40021676 0.34628561 0.27831012 0.1907371 0.086845458 -0.0040168995 -0.05604234][0.053526714 0.077746466 0.09233588 0.094407715 0.096040748 0.1198257 0.15837122 0.18288434 0.17291169 0.13636121 0.093318634 0.03998296 -0.022003321 -0.0723664 -0.094519056][-0.059792127 -0.059254378 -0.059489217 -0.063328356 -0.06272319 -0.045813531 -0.020382967 -0.0044127111 -0.0087122777 -0.02708395 -0.046867479 -0.070530929 -0.096986219 -0.11459696 -0.11523166][-0.11744576 -0.12795366 -0.13465622 -0.14052442 -0.14199437 -0.13454309 -0.12264948 -0.11471751 -0.11523588 -0.12096649 -0.12542285 -0.12997133 -0.13343945 -0.13074942 -0.1200082][-0.13168189 -0.14396223 -0.15054783 -0.15479356 -0.15625803 -0.15430471 -0.15044391 -0.14748126 -0.14702709 -0.14770477 -0.1464856 -0.14361671 -0.13753034 -0.12721598 -0.11390317]]...]
INFO - root - 2017-12-10 18:14:21.566707: step 40210, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 62h:08m:15s remains)
INFO - root - 2017-12-10 18:14:29.323400: step 40220, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 63h:01m:33s remains)
INFO - root - 2017-12-10 18:14:37.079137: step 40230, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 62h:07m:56s remains)
INFO - root - 2017-12-10 18:14:44.959449: step 40240, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 62h:56m:18s remains)
INFO - root - 2017-12-10 18:14:52.442719: step 40250, loss = 0.69, batch loss = 0.64 (13.8 examples/sec; 0.579 sec/batch; 47h:02m:02s remains)
INFO - root - 2017-12-10 18:15:00.246523: step 40260, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 64h:00m:22s remains)
INFO - root - 2017-12-10 18:15:08.059775: step 40270, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 62h:41m:54s remains)
INFO - root - 2017-12-10 18:15:15.863951: step 40280, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.758 sec/batch; 61h:31m:29s remains)
INFO - root - 2017-12-10 18:15:23.728415: step 40290, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 65h:20m:17s remains)
INFO - root - 2017-12-10 18:15:31.495446: step 40300, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 63h:58m:12s remains)
2017-12-10 18:15:32.317627: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.002516306 0.019405102 0.03822365 0.054680109 0.066843569 0.072882645 0.069417804 0.058465473 0.043256611 0.029762585 0.022304276 0.022726493 0.027724296 0.033251766 0.03649348][0.024234377 0.050969698 0.080922924 0.10805473 0.129849 0.14335169 0.14194883 0.12849934 0.10894725 0.091952868 0.082006969 0.080311947 0.082657278 0.084461279 0.082399033][0.045685843 0.082137346 0.12274232 0.15985194 0.19051445 0.21154033 0.21382725 0.20119645 0.18152604 0.16532928 0.15748307 0.15746301 0.15812677 0.15424107 0.14411643][0.064283177 0.10920297 0.15755494 0.19967036 0.23362221 0.25756556 0.26223519 0.2523973 0.23689339 0.22635952 0.22543389 0.23093854 0.23068315 0.21967021 0.20014116][0.080936238 0.13499629 0.19088352 0.23579508 0.26926288 0.29198009 0.295845 0.2868194 0.27414516 0.26861542 0.27533621 0.28836432 0.28926775 0.27273172 0.24493404][0.0990752 0.16455211 0.23051718 0.27979478 0.31298015 0.33357918 0.33516127 0.32428706 0.30947906 0.30357552 0.31444228 0.33502418 0.34056568 0.32302552 0.29009661][0.11517663 0.19162315 0.26671198 0.31975803 0.35306424 0.37263736 0.37349245 0.36165538 0.34396389 0.33562809 0.34830546 0.3752026 0.38703638 0.37192553 0.33675992][0.12915662 0.21525322 0.29647321 0.35043162 0.38208723 0.39904505 0.39756784 0.38319138 0.36252725 0.35166648 0.36487731 0.39645797 0.41489828 0.40476152 0.3705475][0.14194803 0.23477302 0.31875771 0.37060159 0.39824 0.41009986 0.40299723 0.38289949 0.35805342 0.34401643 0.35462928 0.385953 0.40705067 0.40072134 0.36808541][0.14708054 0.24063574 0.32173577 0.36790138 0.38932687 0.3947219 0.38050172 0.35349557 0.32337776 0.30483559 0.31001711 0.3370471 0.35745883 0.35363698 0.32357949][0.13575709 0.22117612 0.29178974 0.32822871 0.34248897 0.34268507 0.32423833 0.29419491 0.2622745 0.24128611 0.24100327 0.26136214 0.27876607 0.27718475 0.25200462][0.099605307 0.16746436 0.22089362 0.24514911 0.252877 0.25051826 0.23299441 0.20629926 0.17880234 0.15974785 0.15570001 0.16788679 0.17937995 0.17838138 0.15920824][0.050217554 0.096742652 0.13204622 0.14563191 0.14862062 0.14511235 0.13049962 0.1093348 0.088143304 0.072786488 0.066498138 0.071388453 0.076725312 0.075630009 0.062844679][0.0062727951 0.032307252 0.05150139 0.056711726 0.05609528 0.051696844 0.040426519 0.025477074 0.010842166 -0.00018870021 -0.0069130776 -0.0068197167 -0.0062094806 -0.0080718324 -0.015726374][-0.025237087 -0.014945903 -0.0077951271 -0.0083152587 -0.011515408 -0.016700223 -0.025213469 -0.034784943 -0.043487657 -0.050118554 -0.055097919 -0.056347881 -0.057250462 -0.058816906 -0.062296629]]...]
INFO - root - 2017-12-10 18:15:40.224512: step 40310, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 63h:13m:42s remains)
INFO - root - 2017-12-10 18:15:48.048379: step 40320, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 62h:10m:27s remains)
INFO - root - 2017-12-10 18:15:55.750074: step 40330, loss = 0.69, batch loss = 0.63 (11.6 examples/sec; 0.691 sec/batch; 56h:04m:13s remains)
INFO - root - 2017-12-10 18:16:03.297600: step 40340, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 62h:02m:04s remains)
INFO - root - 2017-12-10 18:16:11.113439: step 40350, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 64h:47m:32s remains)
INFO - root - 2017-12-10 18:16:18.994832: step 40360, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 63h:26m:17s remains)
INFO - root - 2017-12-10 18:16:26.907405: step 40370, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 65h:06m:05s remains)
INFO - root - 2017-12-10 18:16:34.865475: step 40380, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 65h:05m:01s remains)
INFO - root - 2017-12-10 18:16:42.888045: step 40390, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 62h:20m:11s remains)
INFO - root - 2017-12-10 18:16:50.782844: step 40400, loss = 0.67, batch loss = 0.62 (10.1 examples/sec; 0.789 sec/batch; 63h:58m:58s remains)
2017-12-10 18:16:51.691208: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14340161 0.14238109 0.13403651 0.11449831 0.090687916 0.073591821 0.06886211 0.072096713 0.076781332 0.081840642 0.086940631 0.091934606 0.098078631 0.11623181 0.14229402][0.18998317 0.19842865 0.19367057 0.1684802 0.13390985 0.10688183 0.096519783 0.097645454 0.10056569 0.10427045 0.10919694 0.1157184 0.12470338 0.14656186 0.17608728][0.19915162 0.2238593 0.23162521 0.21115336 0.17530012 0.14320107 0.12606238 0.11964593 0.11343101 0.10787982 0.10535767 0.10806797 0.11554993 0.13425159 0.16052565][0.17159052 0.21474536 0.24317582 0.24171647 0.22016104 0.19389181 0.1726898 0.15489548 0.13225101 0.10832424 0.088794865 0.07872846 0.076971188 0.085159138 0.1023235][0.12355898 0.18019246 0.23116799 0.25886217 0.26541352 0.258212 0.24134539 0.21507135 0.17490439 0.12824835 0.0857327 0.056562953 0.03980799 0.031927422 0.035446741][0.077387072 0.13621004 0.20145661 0.25639302 0.295545 0.31730318 0.31705391 0.29322383 0.24430923 0.18071857 0.11869673 0.071420908 0.038517389 0.012180619 -0.001496956][0.05710683 0.10498301 0.16774292 0.23422025 0.29650471 0.34689894 0.37236819 0.36518124 0.32305706 0.25777277 0.18986747 0.13426758 0.089909121 0.046790149 0.014976258][0.080256209 0.10849784 0.15022071 0.20360108 0.26589352 0.329397 0.37695235 0.39295653 0.3713263 0.32299697 0.26877558 0.22125123 0.17627293 0.12464757 0.080283739][0.15577619 0.16556932 0.17581627 0.19570293 0.23214614 0.2833755 0.33533108 0.36775103 0.37024513 0.35015291 0.3234092 0.29668844 0.26232609 0.21566212 0.17403568][0.26256225 0.26642108 0.251096 0.23373142 0.230907 0.25014952 0.28806013 0.32523811 0.34727377 0.35464743 0.35430202 0.34690464 0.3251155 0.29261184 0.26732862][0.35227039 0.36707452 0.34517673 0.30451608 0.26706788 0.25004652 0.26541638 0.29853559 0.33176756 0.35649249 0.36989766 0.36949769 0.35288715 0.33422664 0.32936829][0.37763396 0.41434616 0.40653646 0.36514613 0.31130618 0.26862293 0.26306203 0.28705323 0.32253218 0.3528688 0.3673909 0.36262685 0.34159514 0.32803768 0.33672681][0.33260387 0.38670576 0.40065467 0.37514588 0.32492682 0.27467111 0.25759742 0.27271602 0.30478486 0.33372232 0.34407949 0.33172303 0.30233282 0.285613 0.29688022][0.25403056 0.31017378 0.3377991 0.32964894 0.29347268 0.250865 0.23304474 0.24270725 0.26925552 0.29424459 0.30082181 0.28414595 0.24990471 0.23007092 0.23920222][0.18961476 0.23484565 0.26234695 0.26186058 0.23774907 0.20772587 0.19508146 0.20214492 0.22234254 0.24226034 0.24679366 0.2311777 0.20007285 0.18410173 0.1932779]]...]
INFO - root - 2017-12-10 18:16:59.366016: step 40410, loss = 0.71, batch loss = 0.65 (11.0 examples/sec; 0.725 sec/batch; 58h:49m:14s remains)
INFO - root - 2017-12-10 18:17:07.267566: step 40420, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 64h:33m:57s remains)
INFO - root - 2017-12-10 18:17:14.999083: step 40430, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 64h:48m:01s remains)
INFO - root - 2017-12-10 18:17:22.878415: step 40440, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 61h:50m:26s remains)
INFO - root - 2017-12-10 18:17:30.727693: step 40450, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 65h:20m:55s remains)
INFO - root - 2017-12-10 18:17:38.675807: step 40460, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 63h:06m:28s remains)
INFO - root - 2017-12-10 18:17:46.608445: step 40470, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 65h:25m:35s remains)
INFO - root - 2017-12-10 18:17:54.509477: step 40480, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 63h:27m:39s remains)
INFO - root - 2017-12-10 18:18:02.320597: step 40490, loss = 0.70, batch loss = 0.64 (13.2 examples/sec; 0.607 sec/batch; 49h:14m:05s remains)
INFO - root - 2017-12-10 18:18:10.183866: step 40500, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.813 sec/batch; 65h:55m:56s remains)
2017-12-10 18:18:11.084152: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29162541 0.30568361 0.30575266 0.27792603 0.21818629 0.14964436 0.098174244 0.079206482 0.09101212 0.11665648 0.13794047 0.14364025 0.13545382 0.12381739 0.12057408][0.32483596 0.33839643 0.33854061 0.31477663 0.26048306 0.1969375 0.1496073 0.13216475 0.14080641 0.15950401 0.17324153 0.17531919 0.16898316 0.15976109 0.15315853][0.32525724 0.33135891 0.33051687 0.31664255 0.27953109 0.23373213 0.1999127 0.18809032 0.19320908 0.2024437 0.20632088 0.20447959 0.20172086 0.1979612 0.19111025][0.29988971 0.29396859 0.2916919 0.29010254 0.27506715 0.25232318 0.2360218 0.23238523 0.23615529 0.23845924 0.23512812 0.2310985 0.23197109 0.23384415 0.22822416][0.25193906 0.23326036 0.229765 0.24070576 0.24917708 0.25181186 0.25499973 0.2612066 0.26599413 0.26508537 0.2593393 0.25755844 0.26522765 0.27570972 0.27532774][0.19183184 0.16330793 0.15908085 0.18056858 0.21012224 0.23710462 0.25989687 0.2767511 0.28394577 0.28175822 0.27566543 0.27760744 0.29326588 0.31427798 0.32290754][0.12959239 0.094449647 0.089523554 0.11800261 0.16433981 0.21365868 0.25670886 0.28585327 0.29669055 0.29284793 0.28400064 0.28552943 0.30458003 0.33266994 0.34934196][0.063088559 0.027470179 0.026104657 0.061704941 0.12175907 0.18988092 0.25071353 0.29050002 0.30366969 0.29633239 0.2816833 0.27811816 0.29447195 0.32341957 0.34359252][-0.0011621017 -0.028708605 -0.02085085 0.023116922 0.093114935 0.17303137 0.24431491 0.28916883 0.30153644 0.28974861 0.26938316 0.26034418 0.27172753 0.29671624 0.31441921][-0.05557435 -0.070454106 -0.051045515 0.00040378954 0.075427465 0.15920822 0.23249342 0.27655172 0.28664428 0.27369857 0.25450689 0.24733943 0.2577377 0.27647492 0.28428632][-0.08626157 -0.087114096 -0.058105096 -0.0043490566 0.067057163 0.14387986 0.20942466 0.24789189 0.25776997 0.25183311 0.24519777 0.2498332 0.2638227 0.27389437 0.26395771][-0.086553104 -0.075713187 -0.042395812 0.0063014985 0.064765811 0.12438356 0.17407814 0.20413664 0.21720695 0.2263688 0.242152 0.26665682 0.28840968 0.28994587 0.2591185][-0.0657904 -0.0477061 -0.015300881 0.023592485 0.064652711 0.10283588 0.13376459 0.15521218 0.17357121 0.20095155 0.2422478 0.2893675 0.32180905 0.31837305 0.26978615][-0.03604066 -0.016785039 0.0099872136 0.037041064 0.061399486 0.080572821 0.095703654 0.11107896 0.13509427 0.17780012 0.23897775 0.30299228 0.34377271 0.3381691 0.27833563][-0.0081744939 0.0069065173 0.025004836 0.040703073 0.052454866 0.059075285 0.064557895 0.076865 0.10424465 0.15401514 0.22247188 0.29096133 0.33264124 0.32503909 0.26127493]]...]
INFO - root - 2017-12-10 18:18:18.925600: step 40510, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 61h:19m:42s remains)
INFO - root - 2017-12-10 18:18:26.746242: step 40520, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 64h:37m:16s remains)
INFO - root - 2017-12-10 18:18:34.622112: step 40530, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 63h:28m:14s remains)
INFO - root - 2017-12-10 18:18:42.527630: step 40540, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 61h:48m:35s remains)
INFO - root - 2017-12-10 18:18:50.330476: step 40550, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 62h:30m:59s remains)
INFO - root - 2017-12-10 18:18:58.211342: step 40560, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 63h:12m:09s remains)
INFO - root - 2017-12-10 18:19:05.865646: step 40570, loss = 0.70, batch loss = 0.64 (11.2 examples/sec; 0.712 sec/batch; 57h:46m:25s remains)
INFO - root - 2017-12-10 18:19:13.848316: step 40580, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 64h:18m:53s remains)
INFO - root - 2017-12-10 18:19:21.821130: step 40590, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 63h:14m:01s remains)
INFO - root - 2017-12-10 18:19:29.492544: step 40600, loss = 0.69, batch loss = 0.63 (11.0 examples/sec; 0.729 sec/batch; 59h:04m:54s remains)
2017-12-10 18:19:30.385233: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026741259 0.035404451 0.0353127 0.036141116 0.048905298 0.074668586 0.11099679 0.15150455 0.17931147 0.18158238 0.15745421 0.11595522 0.062012181 0.0021143896 -0.048832417][0.0732905 0.086009435 0.088254116 0.096691169 0.12714158 0.18079075 0.2539233 0.33512059 0.39371854 0.40599832 0.36825973 0.29481965 0.19649056 0.087090783 -0.0060647088][0.11437564 0.13177323 0.13923237 0.15913433 0.20980148 0.29187632 0.39949495 0.51667362 0.60222709 0.62385416 0.57604641 0.47833991 0.34289324 0.18749969 0.051280018][0.13326496 0.15492618 0.1711098 0.20593488 0.27647194 0.37956613 0.50701612 0.64080876 0.73680758 0.76096731 0.70865548 0.6019038 0.4504374 0.26950824 0.10398793][0.1403643 0.16927615 0.19959933 0.25255424 0.33971286 0.45142454 0.57780278 0.70285815 0.785885 0.79899991 0.74126643 0.63535661 0.48706087 0.3056505 0.13343325][0.14619698 0.18752822 0.23810704 0.31310451 0.41418415 0.52425945 0.63181311 0.72613418 0.77678412 0.76959187 0.70923328 0.61439604 0.4851301 0.32223928 0.15920924][0.14174815 0.19584781 0.26903206 0.3685765 0.48748073 0.60091293 0.69178933 0.751826 0.76749289 0.74316972 0.68877864 0.61682594 0.51874787 0.38511753 0.23469138][0.13466819 0.2037885 0.30433252 0.43500033 0.58161187 0.71095413 0.79616225 0.8271749 0.80883193 0.76622093 0.7171005 0.66776514 0.60157943 0.49796844 0.35778964][0.14002723 0.22784267 0.3566305 0.51730949 0.68966138 0.83502936 0.91793156 0.92483485 0.8736003 0.80694008 0.75131589 0.70887953 0.65965933 0.57622415 0.44469938][0.15498644 0.26119962 0.41166893 0.59130764 0.77554303 0.92562932 1.0033661 0.99398565 0.92099148 0.83591622 0.77004933 0.72372741 0.67610365 0.59879947 0.4721981][0.1494045 0.26236463 0.41908506 0.59960359 0.77801919 0.91835088 0.98534781 0.96746695 0.89095229 0.80813754 0.74655396 0.70223504 0.65391994 0.57593346 0.45013553][0.11346886 0.21757413 0.36361605 0.52899808 0.688993 0.81028932 0.862847 0.84155315 0.77264065 0.70330644 0.65444791 0.6184513 0.57371336 0.49820974 0.37758991][0.063895248 0.14921398 0.27287832 0.41251555 0.54433113 0.6381191 0.67097294 0.6446839 0.58102345 0.51790184 0.47370586 0.44226056 0.40365693 0.3382054 0.23529902][0.0076919789 0.065813541 0.1546424 0.25527242 0.34693977 0.40519929 0.41683167 0.387917 0.33191842 0.27357739 0.22966908 0.19993763 0.16868559 0.11979472 0.046653084][-0.045436293 -0.015684361 0.034543481 0.091098659 0.13918839 0.16212504 0.15610662 0.1292571 0.088480413 0.045310985 0.010999698 -0.010770728 -0.030356381 -0.059279639 -0.10122968]]...]
INFO - root - 2017-12-10 18:19:38.202039: step 40610, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 63h:43m:10s remains)
INFO - root - 2017-12-10 18:19:46.090097: step 40620, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 64h:12m:08s remains)
INFO - root - 2017-12-10 18:19:53.920005: step 40630, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.798 sec/batch; 64h:40m:44s remains)
INFO - root - 2017-12-10 18:20:01.734585: step 40640, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 61h:36m:24s remains)
INFO - root - 2017-12-10 18:20:09.456201: step 40650, loss = 0.69, batch loss = 0.64 (10.6 examples/sec; 0.753 sec/batch; 61h:01m:37s remains)
INFO - root - 2017-12-10 18:20:17.313193: step 40660, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 65h:15m:17s remains)
INFO - root - 2017-12-10 18:20:25.247874: step 40670, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 65h:29m:59s remains)
INFO - root - 2017-12-10 18:20:33.144599: step 40680, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.766 sec/batch; 62h:03m:56s remains)
INFO - root - 2017-12-10 18:20:40.857495: step 40690, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 63h:09m:10s remains)
INFO - root - 2017-12-10 18:20:48.733364: step 40700, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.759 sec/batch; 61h:31m:32s remains)
2017-12-10 18:20:49.559380: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33215931 0.30004984 0.25475204 0.21581925 0.18807971 0.18072577 0.19188526 0.21312287 0.231773 0.23876844 0.23175955 0.20583594 0.16668288 0.12213735 0.076391526][0.30737993 0.28684255 0.25198269 0.22226433 0.20135736 0.19868171 0.21860786 0.25799477 0.2985096 0.32186592 0.31894735 0.28394461 0.22816066 0.16425401 0.100292][0.24294199 0.23617508 0.21767755 0.20254573 0.1911725 0.19234785 0.21668941 0.26718336 0.32252616 0.35860151 0.36249924 0.32607415 0.262933 0.18764013 0.1113676][0.16575992 0.16976933 0.16849187 0.17011423 0.16969503 0.17366047 0.19564535 0.24284078 0.29650825 0.3341985 0.34401965 0.31514749 0.25832281 0.18527974 0.10796946][0.0999226 0.11197017 0.12867695 0.15042157 0.16478734 0.17306963 0.18874453 0.22083342 0.25734055 0.28355396 0.29227334 0.27215213 0.2285623 0.16729388 0.097275667][0.060455792 0.077150121 0.11019043 0.1530191 0.18502921 0.20073804 0.21270369 0.22953826 0.24565575 0.25494644 0.25490022 0.23642386 0.20139718 0.15043116 0.087449819][0.052756079 0.070857473 0.11427543 0.17262049 0.2198066 0.24444507 0.25949746 0.27254829 0.28010273 0.27949509 0.26837283 0.24148853 0.20232187 0.14968143 0.084216833][0.07980109 0.091518775 0.1318337 0.19031443 0.24114545 0.27087441 0.29347137 0.31577808 0.33133712 0.33385205 0.31677631 0.27745736 0.22466569 0.15875992 0.0811455][0.13435586 0.13236673 0.15615727 0.19803333 0.23887856 0.26715329 0.29501063 0.32808131 0.35696304 0.37051055 0.35582483 0.30962825 0.24513413 0.16536827 0.075206161][0.18994631 0.17520796 0.18065913 0.20274709 0.23087752 0.25567406 0.28324586 0.31794277 0.35151035 0.37224421 0.36270523 0.31725231 0.24991283 0.16559935 0.071577825][0.21462731 0.19669029 0.1947017 0.20895852 0.23408644 0.26004782 0.28453952 0.31179202 0.33950618 0.359126 0.35134187 0.30935615 0.24630049 0.16795018 0.079843394][0.20023641 0.18792649 0.19066797 0.21062151 0.24391721 0.27701238 0.30067587 0.32159868 0.34468386 0.36253023 0.3545301 0.31526724 0.25773612 0.18849218 0.10843393][0.15078314 0.1453696 0.15630622 0.18625705 0.2303711 0.270824 0.29490575 0.31411928 0.33911666 0.36028379 0.35521385 0.32066342 0.27004614 0.21082693 0.13894789][0.081582889 0.078648545 0.092845313 0.12657198 0.17390727 0.21561332 0.23900551 0.25893536 0.28827253 0.3146154 0.3157523 0.29039705 0.25096071 0.20460512 0.1437505][0.0073038028 0.0039836811 0.015931714 0.044606354 0.084665909 0.12006574 0.14089274 0.16088183 0.19091041 0.2184799 0.22554481 0.21190424 0.18674086 0.15507501 0.1085031]]...]
INFO - root - 2017-12-10 18:20:57.336715: step 40710, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 64h:21m:44s remains)
INFO - root - 2017-12-10 18:21:05.299330: step 40720, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.821 sec/batch; 66h:33m:01s remains)
INFO - root - 2017-12-10 18:21:13.040160: step 40730, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.822 sec/batch; 66h:39m:05s remains)
INFO - root - 2017-12-10 18:21:20.998334: step 40740, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 62h:40m:09s remains)
INFO - root - 2017-12-10 18:21:28.848563: step 40750, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 64h:35m:27s remains)
INFO - root - 2017-12-10 18:21:36.721482: step 40760, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 64h:19m:34s remains)
INFO - root - 2017-12-10 18:21:44.596220: step 40770, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 64h:26m:30s remains)
INFO - root - 2017-12-10 18:21:52.314487: step 40780, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.759 sec/batch; 61h:32m:15s remains)
INFO - root - 2017-12-10 18:22:00.201285: step 40790, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 65h:36m:41s remains)
INFO - root - 2017-12-10 18:22:08.113200: step 40800, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 64h:37m:56s remains)
2017-12-10 18:22:08.988951: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16074726 0.22529553 0.28698272 0.3154833 0.30183789 0.266 0.23074986 0.2007141 0.17873807 0.18116111 0.20413464 0.23023728 0.24938689 0.27129439 0.30631661][0.22942992 0.29347092 0.3514148 0.37483448 0.35550833 0.31550753 0.27981091 0.2528435 0.24171898 0.25953761 0.29652625 0.32820478 0.34281576 0.35151014 0.3697997][0.28889915 0.34363073 0.38808653 0.40160152 0.37964845 0.3417275 0.31104523 0.29279765 0.29713821 0.33246112 0.3825731 0.41671351 0.42281818 0.41246706 0.40540275][0.33872747 0.38237816 0.41126606 0.41461024 0.39244202 0.35851991 0.33242336 0.32026196 0.33578441 0.38418224 0.44462362 0.4806923 0.48024258 0.45519507 0.42648703][0.36631516 0.40002263 0.41661739 0.41377273 0.395355 0.36866823 0.34798279 0.33913931 0.35850912 0.41176569 0.47621402 0.51109326 0.504432 0.47049269 0.4308815][0.35733885 0.38643771 0.399716 0.39891028 0.38958046 0.37518874 0.36399215 0.35815498 0.373487 0.4213475 0.48122898 0.50963783 0.4949052 0.45568955 0.41534707][0.32053134 0.35260963 0.37556756 0.38746962 0.39167053 0.39266878 0.39452815 0.39245516 0.39586174 0.42604947 0.47083449 0.48701614 0.46164539 0.41699272 0.38053179][0.2789911 0.32128888 0.36354452 0.394003 0.41060469 0.4245007 0.43918836 0.44110623 0.42832449 0.4292267 0.44624439 0.44219065 0.40360487 0.35345858 0.32192975][0.24327864 0.29808643 0.36105606 0.40882808 0.43318111 0.45446149 0.47827822 0.4831503 0.45393997 0.42264158 0.40707675 0.37905055 0.32832578 0.27655995 0.25130656][0.20775513 0.26914564 0.34456897 0.40285558 0.43100131 0.45618966 0.48705679 0.49708492 0.4597069 0.40487438 0.36373648 0.31991592 0.2657401 0.21550044 0.1933451][0.17334878 0.22913674 0.30103856 0.35788628 0.38533023 0.41191193 0.44812042 0.46650848 0.4353911 0.376657 0.32705671 0.27989286 0.23160864 0.18801595 0.16667393][0.14837378 0.18618284 0.23743337 0.28034189 0.30299622 0.32845411 0.36675364 0.39449531 0.383206 0.34264588 0.30381817 0.26568481 0.23093294 0.19935532 0.17821671][0.1367912 0.15074001 0.1735016 0.19744237 0.21465938 0.23872679 0.27748108 0.31495997 0.33015385 0.31934005 0.30126107 0.27762508 0.25724715 0.23717564 0.21483912][0.14052261 0.13371931 0.13295968 0.1425557 0.15722023 0.18123946 0.22009978 0.26573005 0.30335969 0.31842312 0.31822261 0.30618489 0.29476711 0.2806741 0.25494426][0.14927332 0.13060099 0.11834449 0.12382561 0.14199018 0.16978127 0.21051691 0.26205489 0.31191847 0.34035116 0.34810376 0.34033567 0.33085683 0.31690893 0.28774464]]...]
INFO - root - 2017-12-10 18:22:16.826200: step 40810, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 63h:45m:31s remains)
INFO - root - 2017-12-10 18:22:24.682702: step 40820, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 63h:36m:55s remains)
INFO - root - 2017-12-10 18:22:32.668423: step 40830, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 65h:58m:14s remains)
INFO - root - 2017-12-10 18:22:40.499533: step 40840, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 61h:55m:55s remains)
INFO - root - 2017-12-10 18:22:48.300807: step 40850, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 62h:06m:02s remains)
INFO - root - 2017-12-10 18:22:56.205528: step 40860, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.817 sec/batch; 66h:10m:29s remains)
INFO - root - 2017-12-10 18:23:03.822169: step 40870, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 63h:24m:09s remains)
INFO - root - 2017-12-10 18:23:11.717799: step 40880, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 64h:22m:05s remains)
INFO - root - 2017-12-10 18:23:19.451145: step 40890, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 64h:07m:56s remains)
INFO - root - 2017-12-10 18:23:27.215997: step 40900, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 63h:10m:38s remains)
2017-12-10 18:23:28.080136: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.018470773 0.0095174741 0.0016374589 -0.0027778435 -0.0032530003 0.0023342362 0.017283931 0.044315618 0.077747315 0.10341686 0.11578748 0.11687572 0.10550871 0.07907442 0.041405063][0.022112 0.013403363 0.0070952303 0.0073087122 0.014684666 0.029201368 0.054964244 0.096674547 0.14754008 0.18686499 0.20520575 0.20338188 0.17979038 0.13332607 0.0715217][0.020389738 0.014305307 0.013112992 0.02194359 0.041135363 0.067412615 0.10464114 0.15980314 0.2254791 0.2759082 0.29717937 0.2889514 0.25108919 0.18557273 0.10368139][0.01825295 0.019477349 0.02873069 0.050310068 0.083842747 0.12239973 0.16841795 0.23042457 0.30156058 0.35363692 0.36852843 0.34611687 0.29068857 0.21033145 0.11933621][0.021711869 0.034751389 0.058552608 0.094997309 0.14338642 0.19407551 0.24561673 0.30612832 0.37108633 0.41364461 0.41353723 0.37174886 0.29896837 0.21074952 0.12288747][0.036422081 0.063713446 0.10362399 0.15535477 0.21825559 0.28160629 0.33686233 0.38889223 0.4364993 0.4591504 0.43871093 0.37916091 0.29707825 0.21258555 0.13941716][0.065718 0.10615999 0.15992132 0.22427408 0.29873002 0.37247732 0.42813978 0.46575886 0.4878369 0.48389786 0.44297183 0.37451649 0.29823783 0.23302273 0.18552676][0.10074825 0.14988023 0.21287815 0.285984 0.36736441 0.44675851 0.49960586 0.52136844 0.51667905 0.48566696 0.42739481 0.35828957 0.29927498 0.26378563 0.24879093][0.13700621 0.1896271 0.25653839 0.33285341 0.41398746 0.49065053 0.53570741 0.54282975 0.51822764 0.47041127 0.40564004 0.34492409 0.30928373 0.3057659 0.32162422][0.16392848 0.21396561 0.2783055 0.35088277 0.42363948 0.48804915 0.51935834 0.51325256 0.47774759 0.42697242 0.36957026 0.32738721 0.31881773 0.34441987 0.38358045][0.16884065 0.21120539 0.26717809 0.33043137 0.39062682 0.43905422 0.45537427 0.4384003 0.39756307 0.3504869 0.30602637 0.2841385 0.298404 0.34368581 0.39529985][0.14458373 0.17552653 0.21804218 0.26590386 0.31005713 0.34201419 0.34639302 0.32367593 0.28245419 0.24078123 0.20799255 0.20106214 0.22790076 0.27985108 0.33285049][0.090776727 0.10848308 0.13478214 0.16358766 0.18981729 0.20720571 0.20534863 0.18470141 0.15025663 0.11655866 0.092226788 0.090900347 0.11746229 0.16358598 0.20974168][0.026018936 0.034809656 0.048807491 0.061319459 0.071030304 0.074980676 0.068377644 0.052287821 0.028525928 0.006017711 -0.0097567169 -0.009519035 0.010141023 0.043726224 0.078114718][-0.035255633 -0.0308546 -0.023367943 -0.020117344 -0.02099582 -0.026679128 -0.037252687 -0.050201558 -0.0648576 -0.077081554 -0.0846538 -0.08284162 -0.0698429 -0.048583623 -0.026210427]]...]
INFO - root - 2017-12-10 18:23:36.009103: step 40910, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 65h:43m:38s remains)
INFO - root - 2017-12-10 18:23:43.874331: step 40920, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 64h:50m:48s remains)
INFO - root - 2017-12-10 18:23:51.785585: step 40930, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 64h:58m:10s remains)
INFO - root - 2017-12-10 18:23:59.689526: step 40940, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 63h:02m:03s remains)
INFO - root - 2017-12-10 18:24:07.551097: step 40950, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 64h:34m:38s remains)
INFO - root - 2017-12-10 18:24:15.202701: step 40960, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 62h:38m:35s remains)
INFO - root - 2017-12-10 18:24:23.002348: step 40970, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 63h:52m:59s remains)
INFO - root - 2017-12-10 18:24:30.757728: step 40980, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 61h:34m:33s remains)
INFO - root - 2017-12-10 18:24:38.665337: step 40990, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 65h:29m:26s remains)
INFO - root - 2017-12-10 18:24:46.518109: step 41000, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.809 sec/batch; 65h:32m:41s remains)
2017-12-10 18:24:47.374397: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.42169335 0.42174831 0.36526316 0.27272305 0.18463187 0.13326816 0.12090109 0.12292614 0.11129884 0.071434706 0.0096158069 -0.050570019 -0.087628879 -0.099128284 -0.0924753][0.43901876 0.44548857 0.393616 0.30682471 0.22616439 0.18230477 0.17575401 0.17832929 0.16078846 0.11019275 0.0350412 -0.037592553 -0.084041066 -0.10062653 -0.095859565][0.40465322 0.41431397 0.37649888 0.31499243 0.26542574 0.24932466 0.26060107 0.267273 0.24140809 0.17409462 0.079060696 -0.01172287 -0.071698248 -0.09655378 -0.09609174][0.337201 0.34951022 0.33357143 0.3107844 0.30781603 0.33246589 0.36933213 0.38272506 0.34696516 0.25895229 0.13949308 0.026109658 -0.051217858 -0.087295711 -0.093464017][0.23569626 0.25606379 0.2723296 0.30011073 0.3537471 0.42423359 0.48623195 0.50239372 0.45223916 0.34106788 0.19775707 0.063859038 -0.028975828 -0.075688295 -0.089235768][0.12561195 0.16016175 0.21383244 0.2940121 0.4017823 0.51186353 0.5909186 0.60254407 0.53408313 0.40081427 0.23879521 0.090860069 -0.011765175 -0.06558 -0.08513394][0.037709307 0.087144688 0.17231008 0.2928009 0.43879592 0.57303667 0.65812355 0.66007477 0.57441264 0.42532364 0.25348705 0.10054955 -0.0045173112 -0.06042714 -0.082933955][-0.016101906 0.041001789 0.14219938 0.28251207 0.44564524 0.58822185 0.67156607 0.66450894 0.56909144 0.41445875 0.24283527 0.093480848 -0.0075176931 -0.061051838 -0.083351932][-0.045149386 0.010692032 0.11185293 0.25291666 0.41557649 0.5552215 0.63335979 0.62122679 0.52417934 0.37249887 0.20770316 0.067270465 -0.02496328 -0.07135722 -0.088573083][-0.061050404 -0.015177987 0.073659688 0.20224106 0.35357863 0.48560628 0.56044436 0.54957265 0.45809624 0.31431767 0.1585882 0.028709257 -0.052620482 -0.088853471 -0.09755709][-0.062386639 -0.033655297 0.034625035 0.14283173 0.27806759 0.40332347 0.48032421 0.47868451 0.39934823 0.26626283 0.11847063 -0.0039572758 -0.077199042 -0.10479316 -0.1053705][-0.043965843 -0.038197335 0.0035422728 0.086692907 0.20406556 0.32421559 0.40725362 0.41968355 0.35613257 0.23528469 0.094690084 -0.02297762 -0.091688953 -0.11430293 -0.10961664][0.0038830645 -0.014843003 -0.0028563195 0.051664509 0.14855665 0.26210779 0.35084596 0.37751719 0.32999983 0.22176963 0.088291921 -0.026148267 -0.093267381 -0.11488471 -0.1090109][0.076861188 0.038932912 0.024030328 0.050501082 0.12424261 0.22496866 0.31231558 0.34660512 0.31069782 0.21405214 0.089391753 -0.020106927 -0.0855933 -0.10801852 -0.10380365][0.15620834 0.1073053 0.072888315 0.076458819 0.12750384 0.20959572 0.28640011 0.3190304 0.28903913 0.2033671 0.090884261 -0.0096843895 -0.0716623 -0.095483996 -0.095004342]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 18:24:55.253990: step 41010, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 62h:43m:34s remains)
INFO - root - 2017-12-10 18:25:03.068804: step 41020, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 62h:21m:18s remains)
INFO - root - 2017-12-10 18:25:10.911407: step 41030, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 61h:48m:25s remains)
INFO - root - 2017-12-10 18:25:18.841781: step 41040, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.831 sec/batch; 67h:14m:23s remains)
INFO - root - 2017-12-10 18:25:26.340404: step 41050, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.792 sec/batch; 64h:05m:40s remains)
INFO - root - 2017-12-10 18:25:34.217000: step 41060, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 62h:48m:53s remains)
INFO - root - 2017-12-10 18:25:42.024062: step 41070, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 64h:31m:02s remains)
INFO - root - 2017-12-10 18:25:49.908521: step 41080, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.822 sec/batch; 66h:30m:48s remains)
INFO - root - 2017-12-10 18:25:57.786532: step 41090, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 61h:33m:53s remains)
INFO - root - 2017-12-10 18:26:05.656660: step 41100, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 63h:59m:26s remains)
2017-12-10 18:26:06.565628: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18499905 0.21593913 0.21935178 0.18912052 0.13799378 0.092861794 0.074872851 0.086668305 0.11699162 0.1503716 0.1762457 0.18706122 0.17814736 0.15051 0.11599086][0.22223088 0.25677788 0.26076141 0.22988182 0.17937541 0.13671678 0.12226301 0.13848051 0.17442431 0.21340846 0.24254531 0.25149241 0.23415568 0.1924018 0.14158516][0.23574665 0.27147156 0.27842578 0.25346929 0.21258338 0.18185054 0.17996688 0.20869181 0.25511664 0.29878396 0.32255122 0.31557539 0.27565849 0.21082957 0.1410751][0.22382782 0.25712916 0.26897818 0.25728169 0.23757322 0.23324537 0.2595613 0.31384811 0.37675381 0.42045555 0.423722 0.38134223 0.30244026 0.2065983 0.11828265][0.18116096 0.20995018 0.23059902 0.24191386 0.25843766 0.2996031 0.37299374 0.4639833 0.54160279 0.57038313 0.53050381 0.43190762 0.30238104 0.1753379 0.075679578][0.12142326 0.14512004 0.17785847 0.22061156 0.28650904 0.388323 0.51943916 0.64565653 0.72304678 0.71452528 0.61209911 0.44816256 0.27052137 0.12207517 0.022745019][0.075650834 0.09350989 0.1376529 0.21368907 0.33169231 0.49311686 0.67189366 0.8124876 0.86470157 0.8017835 0.63635308 0.42199886 0.21833487 0.068270005 -0.017370285][0.0679747 0.075820461 0.12469807 0.22527388 0.38175973 0.57999963 0.77454907 0.89629042 0.9014377 0.78470558 0.58019537 0.35125545 0.15576053 0.028294398 -0.030214632][0.097449847 0.089496374 0.13168034 0.23816673 0.40461835 0.60163581 0.77234513 0.84880668 0.80454659 0.6570195 0.45289803 0.2503621 0.094970495 0.0089305956 -0.014423761][0.14755483 0.1166143 0.13843885 0.22818018 0.37275058 0.53295738 0.6528303 0.6792115 0.6049701 0.46147674 0.29428694 0.14635362 0.047919266 0.009065751 0.018973496][0.18955015 0.1337401 0.12748536 0.18463375 0.28539667 0.38902813 0.45148483 0.4414084 0.36508539 0.25482774 0.14452909 0.0599593 0.017749917 0.018904451 0.051934786][0.1969218 0.12435499 0.09536998 0.11967248 0.17436716 0.2246331 0.24270941 0.21681178 0.15819749 0.091538161 0.036296885 0.0043164408 0.0031310809 0.029052636 0.072722651][0.16252173 0.089488491 0.052623425 0.056626543 0.079746336 0.096975043 0.093512766 0.068958476 0.033746231 0.0030645425 -0.015044967 -0.017216174 0.0005324097 0.034706369 0.079021782][0.10284537 0.045023106 0.015522963 0.014968011 0.025768274 0.032282609 0.026156111 0.010763212 -0.0056831059 -0.014877402 -0.015628394 -0.0087712808 0.010192594 0.041284166 0.080287777][0.039753221 0.004820229 -0.0085245976 -0.0031550047 0.0083424514 0.017360611 0.017911917 0.013078334 0.0094665559 0.011096426 0.015551953 0.020453144 0.032427598 0.0557695 0.086821452]]...]
INFO - root - 2017-12-10 18:26:14.470013: step 41110, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 64h:11m:50s remains)
INFO - root - 2017-12-10 18:26:22.379332: step 41120, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 64h:13m:04s remains)
INFO - root - 2017-12-10 18:26:29.883851: step 41130, loss = 0.69, batch loss = 0.63 (12.9 examples/sec; 0.621 sec/batch; 50h:15m:12s remains)
INFO - root - 2017-12-10 18:26:37.858865: step 41140, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 63h:28m:03s remains)
INFO - root - 2017-12-10 18:26:45.618952: step 41150, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 62h:47m:44s remains)
INFO - root - 2017-12-10 18:26:53.397618: step 41160, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 62h:01m:59s remains)
INFO - root - 2017-12-10 18:27:01.185130: step 41170, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 63h:09m:24s remains)
INFO - root - 2017-12-10 18:27:09.124730: step 41180, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 65h:25m:09s remains)
INFO - root - 2017-12-10 18:27:16.945000: step 41190, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 63h:09m:50s remains)
INFO - root - 2017-12-10 18:27:24.663870: step 41200, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 64h:41m:14s remains)
2017-12-10 18:27:25.591716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.063270062 -0.059177805 -0.056272287 -0.059546672 -0.067685053 -0.078360237 -0.088045433 -0.094576754 -0.09706331 -0.091711558 -0.080629781 -0.067098834 -0.058725238 -0.061237194 -0.072737895][-0.020445151 -0.0023693468 0.012637347 0.015990656 0.0091518294 -0.0049108639 -0.021015421 -0.036278583 -0.047659181 -0.045501146 -0.034010254 -0.01724536 -0.0060368581 -0.0089930464 -0.025944181][0.040638614 0.085939087 0.12850404 0.15349579 0.16071537 0.15123887 0.13028553 0.1007545 0.070585757 0.058602024 0.059558198 0.070078567 0.07668402 0.068720169 0.043180533][0.10318137 0.18633141 0.27230743 0.33726969 0.37686715 0.38670352 0.36967686 0.32723665 0.27350003 0.23789196 0.21490569 0.20422663 0.19299243 0.16966902 0.12710275][0.15083189 0.27240548 0.4075923 0.52376497 0.61094892 0.65614969 0.65784764 0.614492 0.54460609 0.48603779 0.43210152 0.38781917 0.34509456 0.2934002 0.22181866][0.16882272 0.31552571 0.48949003 0.65324056 0.79109609 0.88102543 0.91627306 0.89162582 0.8253122 0.7570501 0.6786176 0.60051095 0.52155012 0.43297988 0.3226012][0.15801908 0.30696884 0.49402353 0.68255395 0.854094 0.9814297 1.0548543 1.0659291 1.0265208 0.97071546 0.88842714 0.79281068 0.68694842 0.56385487 0.41463014][0.13205059 0.26108864 0.42991939 0.608531 0.78032279 0.920057 1.0193121 1.0686158 1.0702807 1.0477148 0.98796821 0.90167296 0.79071134 0.6490075 0.47487971][0.10037138 0.19614349 0.32350257 0.46147135 0.59914935 0.71961105 0.81946355 0.8914113 0.92924875 0.94515079 0.92360568 0.86904472 0.77784145 0.64337832 0.47063711][0.063525461 0.12429152 0.20319782 0.28704745 0.37147769 0.44993234 0.52491045 0.59386647 0.646938 0.6881215 0.701619 0.68543804 0.63011616 0.52744603 0.38535896][0.020844385 0.053214077 0.091235586 0.12619083 0.15824711 0.18926802 0.22614121 0.2713815 0.31636181 0.3612684 0.39292684 0.40561423 0.38694265 0.32729122 0.2333214][-0.02333769 -0.0098210862 0.0018736239 0.0044911122 -0.00016200638 -0.0060072672 -0.004615881 0.0099892747 0.032981087 0.064183868 0.094631523 0.11843782 0.12410265 0.10215367 0.056065638][-0.063323945 -0.0608043 -0.061617009 -0.07289356 -0.093266472 -0.11666042 -0.13594751 -0.14440498 -0.14304468 -0.13127267 -0.11358962 -0.093641333 -0.079244517 -0.0783228 -0.09044718][-0.092831418 -0.096553154 -0.1013443 -0.11405556 -0.13441645 -0.15848619 -0.18172866 -0.19886357 -0.20892091 -0.21072629 -0.20568678 -0.19529325 -0.18355577 -0.17578159 -0.17255224][-0.10663005 -0.11326748 -0.11807286 -0.12719497 -0.14097682 -0.15757871 -0.1746995 -0.18925688 -0.20010172 -0.20624541 -0.20776848 -0.2052577 -0.20029804 -0.19538689 -0.19040002]]...]
INFO - root - 2017-12-10 18:27:33.252243: step 41210, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 62h:42m:41s remains)
INFO - root - 2017-12-10 18:27:41.022640: step 41220, loss = 0.69, batch loss = 0.64 (9.5 examples/sec; 0.842 sec/batch; 68h:08m:59s remains)
INFO - root - 2017-12-10 18:27:48.859018: step 41230, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 62h:50m:48s remains)
INFO - root - 2017-12-10 18:27:56.717115: step 41240, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 63h:51m:30s remains)
INFO - root - 2017-12-10 18:28:04.516116: step 41250, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 63h:00m:52s remains)
INFO - root - 2017-12-10 18:28:12.309048: step 41260, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 62h:55m:01s remains)
INFO - root - 2017-12-10 18:28:20.256565: step 41270, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 63h:13m:53s remains)
INFO - root - 2017-12-10 18:28:28.052132: step 41280, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 62h:17m:39s remains)
INFO - root - 2017-12-10 18:28:35.790056: step 41290, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 63h:43m:57s remains)
INFO - root - 2017-12-10 18:28:43.639194: step 41300, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 64h:15m:56s remains)
2017-12-10 18:28:44.485361: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30989328 0.31138942 0.30085409 0.27268618 0.22931111 0.18378735 0.14770412 0.12949452 0.12799421 0.15072563 0.21118058 0.28835768 0.3437323 0.35198572 0.31506363][0.30002016 0.29681924 0.2902334 0.26996225 0.2345521 0.19771431 0.17004466 0.15553477 0.15201437 0.16957644 0.22505367 0.29651019 0.34745833 0.35507557 0.31947109][0.2741375 0.27228183 0.27571189 0.26947004 0.24840207 0.22478303 0.20698707 0.19391745 0.1840883 0.1898893 0.22887065 0.28298864 0.32205987 0.32755286 0.29568616][0.24047837 0.24572481 0.26438078 0.27686492 0.27514905 0.26732945 0.25795686 0.24220493 0.22184275 0.21187152 0.23001078 0.26609197 0.29635054 0.30353844 0.277][0.21297093 0.22981416 0.26756564 0.30122489 0.31958362 0.32607192 0.32148317 0.29947749 0.26612791 0.23974171 0.23971437 0.26532039 0.29588926 0.30974305 0.2884585][0.20339461 0.23322456 0.29013884 0.34299251 0.37745765 0.39339706 0.38931319 0.35912594 0.31290796 0.27260712 0.261065 0.28556189 0.32382905 0.34615555 0.32825336][0.21420142 0.25497484 0.32620344 0.39153993 0.43471998 0.45453408 0.44825992 0.41041404 0.35399181 0.30396327 0.28659126 0.31282198 0.35706055 0.38340202 0.36613953][0.23918103 0.28671807 0.36370775 0.43129525 0.47378743 0.49072385 0.47995767 0.43669826 0.37473428 0.31985268 0.2988677 0.32236597 0.36342263 0.38554159 0.365866][0.25356278 0.30126244 0.37227592 0.43061575 0.46401283 0.47387826 0.45954669 0.41660792 0.35731521 0.30474615 0.28183234 0.2965453 0.32486758 0.33570212 0.31169179][0.23998681 0.27873811 0.33209652 0.37238348 0.39191657 0.393164 0.37731704 0.34120309 0.29440308 0.25297955 0.2319268 0.23634015 0.24880835 0.24656169 0.21838552][0.20093468 0.22250378 0.25046057 0.26804239 0.27186811 0.26442152 0.24796605 0.2214098 0.19240363 0.16838215 0.15475169 0.1541822 0.15691407 0.14778434 0.11965766][0.16033302 0.16209635 0.16515046 0.16235118 0.15247206 0.13725246 0.11908013 0.0987768 0.083470277 0.074700847 0.070865326 0.072929777 0.07693886 0.072012246 0.051433828][0.14451437 0.13029116 0.11540111 0.0985832 0.078223072 0.056267109 0.034933854 0.015337206 0.0043231756 0.0024549447 0.0066311248 0.016446326 0.030139128 0.037803441 0.030268883][0.16640724 0.14335291 0.12009383 0.097065128 0.069954187 0.042438447 0.017021947 -0.0069307787 -0.023478284 -0.028675307 -0.021530077 -0.0043509807 0.020169511 0.041450895 0.047258433][0.2212757 0.19761163 0.17499058 0.15225281 0.12112536 0.088384107 0.057963591 0.026324274 -0.0026135675 -0.018949097 -0.014322198 0.0063596233 0.037882373 0.068492435 0.083824039]]...]
INFO - root - 2017-12-10 18:28:52.153850: step 41310, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.818 sec/batch; 66h:10m:14s remains)
INFO - root - 2017-12-10 18:28:59.928103: step 41320, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.771 sec/batch; 62h:21m:28s remains)
INFO - root - 2017-12-10 18:29:07.762690: step 41330, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 64h:00m:31s remains)
INFO - root - 2017-12-10 18:29:15.703846: step 41340, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 63h:09m:22s remains)
INFO - root - 2017-12-10 18:29:23.483928: step 41350, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 64h:02m:32s remains)
INFO - root - 2017-12-10 18:29:31.410045: step 41360, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 64h:31m:52s remains)
INFO - root - 2017-12-10 18:29:39.077294: step 41370, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 62h:41m:37s remains)
INFO - root - 2017-12-10 18:29:46.880468: step 41380, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 63h:24m:56s remains)
INFO - root - 2017-12-10 18:29:54.731048: step 41390, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.818 sec/batch; 66h:09m:25s remains)
INFO - root - 2017-12-10 18:30:02.479490: step 41400, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.820 sec/batch; 66h:16m:47s remains)
2017-12-10 18:30:03.310107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.05117863 -0.043351054 -0.033435307 -0.02680338 -0.02462988 -0.0266456 -0.032395635 -0.039632391 -0.044832926 -0.044424981 -0.039489474 -0.032960646 -0.027657116 -0.027380258 -0.03478995][-0.032795891 -0.015669895 0.0035381385 0.016692268 0.021414788 0.01814927 0.0075324229 -0.0063615954 -0.016437473 -0.015305085 -0.0048772083 0.0091074258 0.020781545 0.022732714 0.0093010394][-0.0024420635 0.029607628 0.064443968 0.090581305 0.10412394 0.10467375 0.091565371 0.070311353 0.052720994 0.0511165 0.0640163 0.083558448 0.099999271 0.10103786 0.0770525][0.036712948 0.090310857 0.14960447 0.19889298 0.23180777 0.2447401 0.23363274 0.20424445 0.17405869 0.16168609 0.16772935 0.18395343 0.19720288 0.19072628 0.15035981][0.076930486 0.15693024 0.24763475 0.3284404 0.38894612 0.42011783 0.4133254 0.37457702 0.32798421 0.29878992 0.29049486 0.2951169 0.29769513 0.27831843 0.21823077][0.11055582 0.21645634 0.3386752 0.45179412 0.54092944 0.59087819 0.58833027 0.540569 0.47829428 0.43305787 0.41024396 0.40158677 0.39193842 0.35990474 0.28269851][0.13312685 0.25856841 0.40422043 0.5412572 0.65102309 0.713773 0.71276319 0.65756023 0.58505714 0.5315299 0.50223762 0.48639259 0.46865603 0.42777696 0.33833456][0.13907193 0.27291143 0.42838117 0.57501394 0.69229764 0.7585696 0.75573581 0.69615686 0.62163723 0.571701 0.54886097 0.53751796 0.52055216 0.4774102 0.38190323][0.12969732 0.25925869 0.40849569 0.54714286 0.655296 0.71340388 0.70540917 0.6467604 0.58137351 0.54879552 0.54614615 0.55115384 0.54376316 0.50440997 0.4087235][0.10969026 0.22367954 0.35170278 0.465279 0.54782528 0.586158 0.56999695 0.518006 0.47132865 0.46447629 0.48741633 0.5118413 0.51580667 0.48267066 0.39330935][0.078766711 0.16893715 0.26631886 0.34565964 0.39516643 0.40952674 0.38557833 0.34355333 0.31792188 0.33446634 0.37737697 0.4155744 0.42737657 0.40056691 0.32298562][0.038628869 0.10189293 0.16728239 0.21386746 0.23441151 0.22960283 0.20193516 0.17048585 0.16114838 0.19036561 0.23994975 0.28057155 0.29304817 0.26991194 0.20605463][-0.0029575531 0.0352659 0.073173925 0.094717242 0.096254192 0.081272379 0.055180006 0.033379193 0.032807384 0.062115345 0.10479105 0.13732894 0.14512017 0.12460876 0.0759476][-0.037209049 -0.018567584 -0.00072125817 0.0047726305 -0.0034811355 -0.021364581 -0.042928383 -0.057333339 -0.055306293 -0.033746853 -0.0051088003 0.014551553 0.015994806 -0.0013540803 -0.03421858][-0.060971078 -0.055256959 -0.049438875 -0.051608607 -0.062181972 -0.077862248 -0.093734443 -0.10314277 -0.10203414 -0.090421945 -0.076347932 -0.068930991 -0.07269194 -0.086217165 -0.10528026]]...]
INFO - root - 2017-12-10 18:30:11.197359: step 41410, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 63h:50m:27s remains)
INFO - root - 2017-12-10 18:30:19.027191: step 41420, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 63h:18m:46s remains)
INFO - root - 2017-12-10 18:30:26.914894: step 41430, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 64h:08m:27s remains)
INFO - root - 2017-12-10 18:30:34.692568: step 41440, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 63h:16m:56s remains)
INFO - root - 2017-12-10 18:30:42.372773: step 41450, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 65h:48m:43s remains)
INFO - root - 2017-12-10 18:30:50.256324: step 41460, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 63h:48m:33s remains)
INFO - root - 2017-12-10 18:30:58.193297: step 41470, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 63h:19m:54s remains)
INFO - root - 2017-12-10 18:31:05.979777: step 41480, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 62h:17m:06s remains)
INFO - root - 2017-12-10 18:31:13.659733: step 41490, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 62h:10m:00s remains)
INFO - root - 2017-12-10 18:31:21.489606: step 41500, loss = 0.66, batch loss = 0.60 (10.5 examples/sec; 0.758 sec/batch; 61h:18m:13s remains)
2017-12-10 18:31:22.556139: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36387393 0.38790259 0.38623488 0.36252424 0.33225057 0.31551203 0.32237354 0.34254652 0.35850972 0.35437635 0.32844678 0.29914194 0.2876091 0.304646 0.34568107][0.3722499 0.391453 0.37841651 0.34284469 0.30561641 0.28873539 0.30152041 0.3316974 0.35760966 0.35908204 0.33341816 0.30067378 0.28290543 0.29257548 0.32631949][0.35842803 0.36522493 0.33797151 0.29272577 0.25454494 0.24427326 0.26762384 0.30996862 0.34677157 0.35418361 0.32833138 0.28818196 0.25612092 0.24688669 0.2602101][0.34738925 0.33868262 0.29761425 0.24705167 0.21450709 0.21712823 0.25466 0.30861244 0.35241026 0.36139843 0.33137798 0.27984655 0.22907929 0.1955823 0.18212055][0.33476803 0.31418514 0.2670615 0.22132811 0.20387945 0.22559032 0.27880457 0.34002125 0.38221639 0.38425741 0.34476033 0.2809951 0.2141118 0.16053204 0.12432967][0.31908247 0.29657125 0.25635773 0.2266122 0.23070273 0.27234784 0.33731025 0.39799368 0.42887971 0.41567111 0.36248013 0.28780445 0.21134196 0.14819126 0.10128467][0.30653775 0.29567379 0.27443 0.26673162 0.29046628 0.34391266 0.40984693 0.46015659 0.47238734 0.43956694 0.3717587 0.28960821 0.21166652 0.15059568 0.10601044][0.31195346 0.32413137 0.32849568 0.34116817 0.37396553 0.42282075 0.47216442 0.49864823 0.48489332 0.43020639 0.34970289 0.26595154 0.19577104 0.14768489 0.11565764][0.34462085 0.38476288 0.41209269 0.43503982 0.46128002 0.48823434 0.50517225 0.49738264 0.45434469 0.3800295 0.29256439 0.21424049 0.15903589 0.13004698 0.11504173][0.40225393 0.46398824 0.50325251 0.52362895 0.5317598 0.5286271 0.50875509 0.46635586 0.39795047 0.31078511 0.22292431 0.15339451 0.11302647 0.10011108 0.098444551][0.46704862 0.53415442 0.56813294 0.57414991 0.5605461 0.53167373 0.48422056 0.4181776 0.33516103 0.24367262 0.15993378 0.099006765 0.0694368 0.065887958 0.070891492][0.5011946 0.55379683 0.56932437 0.55785149 0.52912474 0.48765212 0.42882234 0.35486689 0.2707696 0.18514685 0.11156628 0.061286852 0.040358804 0.040925615 0.046406634][0.47162876 0.49714229 0.49231106 0.47026825 0.43931711 0.40106407 0.34766537 0.28118837 0.20862027 0.13860068 0.082186483 0.046764147 0.034782808 0.036313433 0.03713936][0.37059882 0.36645243 0.34650961 0.32349709 0.30233744 0.28061274 0.24711277 0.20183778 0.15173194 0.10533251 0.07178437 0.055034656 0.053079944 0.054483768 0.048248582][0.23061396 0.20405686 0.17770247 0.1617744 0.1571724 0.15742189 0.14896509 0.12955655 0.10490134 0.083272249 0.07277263 0.074407168 0.081889182 0.083157092 0.070778958]]...]
INFO - root - 2017-12-10 18:31:30.404112: step 41510, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 64h:50m:24s remains)
INFO - root - 2017-12-10 18:31:38.227744: step 41520, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 63h:38m:53s remains)
INFO - root - 2017-12-10 18:31:45.921902: step 41530, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.787 sec/batch; 63h:36m:06s remains)
INFO - root - 2017-12-10 18:31:53.748547: step 41540, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 64h:13m:22s remains)
INFO - root - 2017-12-10 18:32:01.606446: step 41550, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.798 sec/batch; 64h:27m:50s remains)
INFO - root - 2017-12-10 18:32:09.459427: step 41560, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 64h:03m:05s remains)
INFO - root - 2017-12-10 18:32:17.271302: step 41570, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 62h:25m:30s remains)
INFO - root - 2017-12-10 18:32:25.009310: step 41580, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 65h:01m:38s remains)
INFO - root - 2017-12-10 18:32:32.820546: step 41590, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 62h:00m:06s remains)
INFO - root - 2017-12-10 18:32:40.589581: step 41600, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 62h:35m:50s remains)
2017-12-10 18:32:41.588114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.02383586 -0.004229336 0.013945186 0.024462555 0.027486688 0.022738187 0.0097706849 -0.0024363995 -0.012738871 -0.02079444 -0.028499641 -0.034417588 -0.037847705 -0.04610252 -0.060884863][0.0080634784 0.044154361 0.082710959 0.11677485 0.13988706 0.145618 0.1327749 0.11720464 0.09695112 0.072377719 0.046972349 0.027467536 0.017352354 0.0017797643 -0.024944497][0.058901366 0.11553278 0.18039551 0.24535264 0.29549465 0.31506807 0.30226022 0.28020498 0.24355006 0.19485344 0.14462095 0.1072397 0.0895334 0.067464493 0.028821275][0.11382719 0.1931435 0.2875931 0.38676077 0.46760798 0.50287467 0.48881972 0.45457554 0.39524865 0.31799018 0.23970832 0.18245164 0.15539224 0.12726681 0.079423778][0.15671553 0.26213318 0.38977119 0.52584314 0.64033628 0.69367838 0.67680824 0.62321216 0.5353986 0.42918608 0.325201 0.24898377 0.20960586 0.17255129 0.11665076][0.19222642 0.32792276 0.48959404 0.65845221 0.80131155 0.86893374 0.84381986 0.76236081 0.641998 0.51149935 0.3911823 0.30312517 0.25226122 0.20373817 0.13938682][0.22836539 0.39255211 0.578748 0.76387691 0.91978532 0.99297118 0.95744216 0.84990871 0.7057454 0.56411672 0.43918851 0.34606954 0.28489065 0.22391349 0.15067072][0.25660282 0.44140884 0.63883281 0.82273722 0.97547072 1.0449994 1.0003284 0.87750334 0.7268638 0.59063464 0.47073823 0.37499845 0.30163011 0.22648776 0.14446421][0.2595045 0.45356843 0.65317404 0.82873338 0.97123343 1.0336211 0.98561829 0.86348093 0.72551876 0.60940003 0.50137705 0.40244028 0.31283754 0.21959198 0.12674329][0.2329614 0.42228115 0.61591434 0.78004593 0.90849048 0.96137112 0.91494423 0.80654931 0.69438916 0.6072157 0.51735 0.4196499 0.31735021 0.20818904 0.10570657][0.1904961 0.36103889 0.53704637 0.68130612 0.78788054 0.82727444 0.78590763 0.69991046 0.619897 0.564262 0.4966234 0.40716392 0.30118588 0.18492335 0.079530336][0.13450529 0.27288106 0.417545 0.53218132 0.6123572 0.64071757 0.61258829 0.55745023 0.51177305 0.48336184 0.43423235 0.35381061 0.25041983 0.13711768 0.038575608][0.062278651 0.15787004 0.26068863 0.34042108 0.39529282 0.41757229 0.40721413 0.38344142 0.3664771 0.35583937 0.31844166 0.24730708 0.15483905 0.0579468 -0.0205533][-0.0082554249 0.044107851 0.10439146 0.1510084 0.18421641 0.20133908 0.20319623 0.19964634 0.19847181 0.19549631 0.16690746 0.11071288 0.04052946 -0.027995931 -0.077993795][-0.059707146 -0.04003178 -0.012493733 0.0084551265 0.023525633 0.032088235 0.034474496 0.034845248 0.035188776 0.032423213 0.012885317 -0.022616906 -0.063586608 -0.099504322 -0.120703]]...]
INFO - root - 2017-12-10 18:32:49.453991: step 41610, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.771 sec/batch; 62h:17m:22s remains)
INFO - root - 2017-12-10 18:32:57.369241: step 41620, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 63h:27m:06s remains)
INFO - root - 2017-12-10 18:33:05.148442: step 41630, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 62h:24m:48s remains)
INFO - root - 2017-12-10 18:33:12.991832: step 41640, loss = 0.67, batch loss = 0.61 (10.4 examples/sec; 0.771 sec/batch; 62h:17m:21s remains)
INFO - root - 2017-12-10 18:33:20.775927: step 41650, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 63h:10m:35s remains)
INFO - root - 2017-12-10 18:33:28.572224: step 41660, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 63h:48m:39s remains)
INFO - root - 2017-12-10 18:33:36.265502: step 41670, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 64h:33m:31s remains)
INFO - root - 2017-12-10 18:33:44.180472: step 41680, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 62h:55m:00s remains)
INFO - root - 2017-12-10 18:33:51.774118: step 41690, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 62h:10m:58s remains)
INFO - root - 2017-12-10 18:33:59.610709: step 41700, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.792 sec/batch; 63h:57m:35s remains)
2017-12-10 18:34:00.409563: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1938884 0.2402589 0.29843745 0.35007876 0.37224403 0.35437116 0.30209762 0.23395531 0.17134751 0.13211207 0.11770389 0.115842 0.11940992 0.13002589 0.14480233][0.21807337 0.25726894 0.30248421 0.33886606 0.35031846 0.332453 0.29218185 0.24208717 0.19489793 0.16319744 0.14733157 0.13718046 0.12832367 0.12689433 0.13236026][0.24972485 0.27727193 0.30265546 0.31786591 0.31541431 0.29835352 0.27551302 0.25106913 0.2267352 0.20769267 0.19222304 0.17272098 0.14878553 0.13187818 0.12450932][0.28165025 0.29629305 0.30086261 0.29594216 0.28157917 0.26738918 0.26388544 0.26646528 0.2656123 0.25801525 0.24082363 0.20993263 0.17034984 0.13855109 0.12017825][0.29934961 0.30203018 0.29023468 0.2722109 0.25289017 0.24569832 0.26153675 0.28789392 0.30596751 0.30451751 0.28113404 0.23695146 0.18293265 0.13988812 0.11663024][0.28524524 0.27805364 0.25765023 0.2370629 0.22260602 0.22848982 0.26394826 0.30967373 0.3402001 0.33891207 0.30614719 0.24872802 0.18291599 0.13275288 0.10982166][0.23339328 0.22008614 0.19996884 0.18676633 0.18510191 0.2079698 0.26131466 0.32069868 0.35698959 0.3518852 0.31011319 0.24319433 0.17020437 0.11662688 0.096092068][0.15999855 0.14592636 0.13331912 0.13299407 0.14630979 0.18411326 0.24913126 0.31484437 0.35122252 0.34116206 0.29390004 0.22385959 0.14975816 0.095820621 0.076898351][0.0923885 0.081857532 0.078468904 0.089389339 0.11319085 0.15828295 0.22549607 0.28966093 0.32243827 0.30914387 0.26196551 0.1962942 0.12798706 0.077258274 0.058572043][0.053486086 0.04837868 0.050470881 0.064867631 0.089642689 0.13148588 0.19117981 0.24765891 0.27609804 0.26327386 0.22118279 0.16477357 0.1069904 0.062876679 0.044865884][0.05105266 0.053003006 0.056394391 0.066171311 0.082135819 0.11058148 0.15473236 0.19998401 0.22590317 0.21831124 0.18531483 0.140261 0.094017744 0.057227168 0.040339623][0.080730446 0.091195121 0.09333466 0.093206041 0.093570687 0.10173442 0.12573086 0.15903367 0.18509583 0.18641385 0.16462931 0.13063514 0.094048515 0.062992714 0.047339711][0.12825784 0.14842466 0.14941801 0.1388822 0.12251896 0.10945974 0.11324131 0.13527609 0.1618454 0.17142153 0.15900791 0.1333119 0.10383687 0.077314138 0.06360206][0.1833422 0.21322148 0.21472697 0.19718195 0.16812089 0.13801371 0.12413161 0.13412292 0.1571836 0.16942088 0.16140066 0.14097449 0.11849768 0.0979397 0.087301627][0.23078561 0.26748368 0.27055672 0.25103733 0.21671943 0.17776774 0.15164615 0.14955585 0.16445141 0.17316261 0.16489002 0.14798786 0.13400126 0.12225636 0.11603664]]...]
INFO - root - 2017-12-10 18:34:08.348033: step 41710, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 62h:17m:09s remains)
INFO - root - 2017-12-10 18:34:16.157290: step 41720, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 62h:17m:31s remains)
INFO - root - 2017-12-10 18:34:23.929920: step 41730, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 61h:44m:28s remains)
INFO - root - 2017-12-10 18:34:31.882094: step 41740, loss = 0.68, batch loss = 0.62 (8.9 examples/sec; 0.898 sec/batch; 72h:31m:27s remains)
INFO - root - 2017-12-10 18:34:39.682077: step 41750, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 61h:37m:48s remains)
INFO - root - 2017-12-10 18:34:47.341433: step 41760, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 62h:17m:06s remains)
INFO - root - 2017-12-10 18:34:55.022354: step 41770, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 61h:43m:41s remains)
INFO - root - 2017-12-10 18:35:02.950656: step 41780, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.820 sec/batch; 66h:11m:35s remains)
INFO - root - 2017-12-10 18:35:10.899119: step 41790, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 64h:08m:04s remains)
INFO - root - 2017-12-10 18:35:18.896980: step 41800, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 63h:18m:50s remains)
2017-12-10 18:35:19.829402: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22404356 0.20395353 0.16624412 0.12629715 0.098448776 0.089281008 0.092337735 0.097189508 0.10248728 0.11808327 0.14760102 0.17947142 0.20134279 0.20901532 0.2026353][0.28005746 0.25511524 0.21209072 0.17211552 0.1504633 0.1530828 0.16901155 0.18214314 0.18783458 0.19664027 0.21474908 0.23201494 0.23919708 0.23627427 0.2255355][0.31364423 0.2910226 0.25261441 0.22267181 0.21535881 0.23564442 0.26766822 0.28987116 0.29386628 0.28935006 0.28516173 0.27501759 0.2559191 0.23376471 0.2134839][0.32255268 0.31092134 0.28737846 0.27491912 0.28679815 0.32831308 0.37885854 0.41147926 0.41392437 0.39352944 0.36106232 0.31514174 0.26165855 0.21445656 0.18114509][0.31201375 0.31758595 0.31432113 0.3218959 0.35380465 0.41561902 0.48338228 0.52718067 0.53106141 0.49816751 0.43964779 0.3597016 0.27411354 0.20307255 0.15545851][0.29619002 0.32313511 0.34321961 0.37252617 0.42472687 0.50353426 0.58310795 0.63516879 0.64165545 0.60109407 0.52419543 0.42045355 0.31304994 0.22415708 0.16157362][0.28858143 0.33574849 0.37554866 0.42271644 0.49056682 0.57897347 0.66223347 0.71738958 0.72621852 0.68355131 0.59952885 0.48770705 0.37410328 0.27725032 0.20159738][0.29400289 0.35212526 0.40000117 0.45434484 0.52797991 0.61654323 0.69601673 0.74949241 0.75932044 0.7181465 0.63701981 0.5326072 0.42892879 0.33646992 0.2548148][0.31846058 0.37587124 0.41762215 0.46588826 0.53309745 0.61192596 0.68092656 0.72734195 0.73438406 0.69521755 0.62432903 0.53934026 0.45838252 0.38186678 0.30424324][0.35420948 0.39885283 0.42052585 0.4485887 0.4962717 0.5555492 0.60764438 0.64156121 0.64281434 0.6079964 0.55559784 0.50053442 0.45212698 0.40090486 0.33728418][0.39526245 0.42007059 0.41342387 0.41171715 0.43086538 0.46521822 0.49828491 0.51861346 0.51450175 0.48796433 0.4598707 0.43907529 0.4248246 0.400395 0.35458553][0.43133762 0.43540266 0.39931923 0.36515951 0.35325754 0.3629598 0.37982723 0.38968396 0.38336131 0.36865595 0.36590484 0.37611642 0.39005604 0.38767561 0.35830465][0.43481302 0.42082107 0.36225611 0.30284834 0.26649129 0.25757766 0.26305556 0.26610842 0.25997177 0.25709394 0.27492386 0.30730873 0.3398366 0.35252687 0.33650312][0.38969234 0.365022 0.29723048 0.22744112 0.18000847 0.16193773 0.16153139 0.1615634 0.15769091 0.16463959 0.19517945 0.23802622 0.27694464 0.29459086 0.28506362][0.29738921 0.26880074 0.20446911 0.13855955 0.0924188 0.073260009 0.071044087 0.07070072 0.069947064 0.083055682 0.11769328 0.16055794 0.19608587 0.21075016 0.2018812]]...]
INFO - root - 2017-12-10 18:35:27.626279: step 41810, loss = 0.69, batch loss = 0.63 (10.8 examples/sec; 0.741 sec/batch; 59h:50m:22s remains)
INFO - root - 2017-12-10 18:35:35.540332: step 41820, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 64h:25m:28s remains)
INFO - root - 2017-12-10 18:35:43.566922: step 41830, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 63h:34m:19s remains)
INFO - root - 2017-12-10 18:35:51.308158: step 41840, loss = 0.70, batch loss = 0.64 (12.8 examples/sec; 0.624 sec/batch; 50h:21m:01s remains)
INFO - root - 2017-12-10 18:35:59.090086: step 41850, loss = 0.69, batch loss = 0.64 (9.5 examples/sec; 0.845 sec/batch; 68h:11m:09s remains)
INFO - root - 2017-12-10 18:36:06.960414: step 41860, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 63h:03m:31s remains)
INFO - root - 2017-12-10 18:36:14.765693: step 41870, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 64h:11m:35s remains)
INFO - root - 2017-12-10 18:36:22.609809: step 41880, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 62h:55m:11s remains)
INFO - root - 2017-12-10 18:36:30.527987: step 41890, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 65h:02m:02s remains)
INFO - root - 2017-12-10 18:36:38.357897: step 41900, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 63h:04m:54s remains)
2017-12-10 18:36:39.179882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018765298 0.011729214 0.031730544 0.052290276 0.066975988 0.078880362 0.089771628 0.097774036 0.10196944 0.10229978 0.1032401 0.10763871 0.11583211 0.12504327 0.13252039][0.0024030611 0.020150682 0.044585455 0.067642167 0.0820932 0.091023117 0.097589433 0.10064448 0.099806622 0.097302526 0.09856084 0.10464158 0.11536679 0.12881847 0.1424198][0.009506424 0.032857105 0.061592296 0.085288383 0.096867964 0.10096513 0.10355518 0.10314535 0.098872945 0.094385371 0.095324926 0.10180492 0.11202689 0.12544811 0.14127879][0.016901262 0.046690669 0.080751136 0.10634805 0.11676311 0.11994065 0.12378362 0.12412545 0.11784482 0.10976896 0.10692524 0.10934119 0.1150147 0.12521408 0.14116761][0.022337819 0.059003618 0.099211141 0.12781584 0.13966893 0.14596832 0.15429649 0.15690835 0.14890216 0.13670422 0.12887698 0.12560397 0.12580369 0.13278794 0.14800791][0.025469339 0.068608508 0.11440117 0.1463836 0.16115904 0.17203172 0.18488137 0.18876787 0.17923081 0.16481102 0.15442732 0.1465665 0.14086369 0.14370041 0.15514486][0.028321523 0.074566409 0.12302311 0.15725428 0.17422733 0.188403 0.2037272 0.20686312 0.19581544 0.18188123 0.17353655 0.1647585 0.15438268 0.15170404 0.15567262][0.029774694 0.073043779 0.11938709 0.15395457 0.17271538 0.18891229 0.20479614 0.20629972 0.19468229 0.18297376 0.17950453 0.17426978 0.16438825 0.15873042 0.15556736][0.027388662 0.064218663 0.10626852 0.14121903 0.16350111 0.1827269 0.19936335 0.20113227 0.19126585 0.1827227 0.18406284 0.18466307 0.17967661 0.17345521 0.16423884][0.021321908 0.053062312 0.092196673 0.12803213 0.15366872 0.17586362 0.19423844 0.19924162 0.19302483 0.186943 0.19063182 0.19666274 0.19856291 0.19317234 0.17893913][0.016952401 0.047831334 0.08673998 0.12232152 0.14769733 0.1698755 0.18950792 0.19857912 0.19533353 0.18895403 0.19170862 0.2011693 0.20965165 0.20584604 0.18902826][0.015520806 0.048741087 0.088918857 0.12369525 0.14685111 0.16738044 0.18770398 0.19948716 0.19651292 0.18688308 0.18654901 0.19737737 0.20974424 0.2067996 0.1899911][0.014252465 0.048627768 0.088994689 0.12446523 0.14850245 0.17013326 0.19234464 0.20607448 0.20269814 0.18985653 0.18620984 0.19480778 0.20489335 0.19938333 0.18393794][0.0099681476 0.041406319 0.079656363 0.11775049 0.14692861 0.17225944 0.194789 0.20626064 0.20028947 0.18418927 0.17721042 0.18036374 0.18331425 0.17342789 0.1599201][0.0012546731 0.026766455 0.062241264 0.10399857 0.13901743 0.16681331 0.18590178 0.19077407 0.18026167 0.16163474 0.15269126 0.15055451 0.1468277 0.13473697 0.12391234]]...]
INFO - root - 2017-12-10 18:36:47.026766: step 41910, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 62h:19m:29s remains)
INFO - root - 2017-12-10 18:36:55.180665: step 41920, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.012 sec/batch; 81h:41m:52s remains)
INFO - root - 2017-12-10 18:37:02.834400: step 41930, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 64h:18m:24s remains)
INFO - root - 2017-12-10 18:37:10.759984: step 41940, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 65h:14m:01s remains)
INFO - root - 2017-12-10 18:37:18.592149: step 41950, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.825 sec/batch; 66h:36m:49s remains)
INFO - root - 2017-12-10 18:37:26.497127: step 41960, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.791 sec/batch; 63h:48m:46s remains)
INFO - root - 2017-12-10 18:37:34.377292: step 41970, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 63h:36m:14s remains)
INFO - root - 2017-12-10 18:37:42.307108: step 41980, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 61h:59m:56s remains)
INFO - root - 2017-12-10 18:37:50.174103: step 41990, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 62h:38m:04s remains)
INFO - root - 2017-12-10 18:37:58.083325: step 42000, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 62h:38m:11s remains)
2017-12-10 18:37:59.001139: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22088617 0.23080519 0.24100605 0.2609244 0.27701154 0.29586074 0.33480588 0.38976756 0.43564516 0.46811762 0.49536228 0.50352228 0.47222763 0.41279873 0.35283792][0.24432588 0.24319239 0.2335829 0.22926955 0.22315946 0.22926179 0.26645765 0.32817093 0.38643864 0.43476027 0.47955179 0.504503 0.48825976 0.4418259 0.38932][0.28611752 0.28082535 0.25595254 0.22767325 0.19669591 0.18363228 0.20808037 0.26347816 0.3241708 0.38453183 0.44915566 0.50038868 0.51397812 0.49572539 0.45974627][0.35161325 0.35708925 0.33206451 0.29243252 0.24495198 0.21492976 0.22062084 0.25840428 0.30978146 0.37307596 0.45238715 0.528787 0.574556 0.58626765 0.56483716][0.43338814 0.4673456 0.464685 0.43636745 0.39031631 0.35332662 0.34047663 0.35223067 0.37973115 0.42837051 0.50404465 0.5882144 0.65072876 0.67877734 0.66094667][0.5118652 0.59036452 0.63361776 0.64253873 0.62023538 0.59063739 0.56504834 0.54712892 0.53721267 0.5508827 0.5975033 0.66078061 0.71120638 0.73012823 0.69922221][0.57056403 0.70221514 0.80599844 0.86958027 0.8866154 0.87344611 0.83928651 0.79083735 0.73727858 0.70360631 0.70239419 0.72032905 0.73250419 0.71926731 0.66279876][0.59368527 0.77035517 0.92919505 1.044807 1.0996722 1.1001846 1.0570835 0.98128825 0.88990253 0.81279731 0.763037 0.72975391 0.69457978 0.64119875 0.55768764][0.56831789 0.76512218 0.95086879 1.0933223 1.167106 1.1701369 1.117136 1.0245186 0.91438591 0.81555092 0.73763615 0.67044753 0.601074 0.5183593 0.41923732][0.48833188 0.67182034 0.8466928 0.98108947 1.0483799 1.0418957 0.98058522 0.8854636 0.78001255 0.68706226 0.61054194 0.539375 0.46276039 0.37460098 0.28065756][0.36138508 0.50512046 0.63967723 0.73960507 0.78319716 0.76294512 0.69978648 0.61648315 0.5339784 0.46688747 0.41348565 0.36156595 0.30065495 0.22849657 0.15674005][0.20433299 0.2957491 0.37859157 0.43579891 0.45272079 0.4237161 0.3683846 0.30720535 0.2555908 0.22017092 0.19607081 0.17172837 0.1370535 0.091592751 0.048359644][0.046550948 0.088793993 0.12570709 0.14682257 0.14415132 0.11500315 0.074871294 0.038228106 0.01385974 0.0033722469 0.0015589849 -0.00083028222 -0.012756513 -0.033787265 -0.052951876][-0.078682028 -0.070616782 -0.06225542 -0.061176978 -0.070956267 -0.092292011 -0.11533712 -0.13247792 -0.14017399 -0.13825515 -0.13030888 -0.12273645 -0.12242589 -0.12873097 -0.13405527][-0.1526923 -0.16162197 -0.16471675 -0.16913857 -0.17657958 -0.18728572 -0.19658588 -0.2021337 -0.20311438 -0.19894642 -0.19107333 -0.18321219 -0.1798968 -0.18085118 -0.18181837]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 18:38:06.608590: step 42010, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.754 sec/batch; 60h:50m:03s remains)
INFO - root - 2017-12-10 18:38:14.286486: step 42020, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 62h:31m:41s remains)
INFO - root - 2017-12-10 18:38:22.120539: step 42030, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 64h:43m:21s remains)
INFO - root - 2017-12-10 18:38:30.041642: step 42040, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 65h:02m:03s remains)
INFO - root - 2017-12-10 18:38:37.821155: step 42050, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 61h:12m:06s remains)
INFO - root - 2017-12-10 18:38:45.678729: step 42060, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 63h:26m:05s remains)
INFO - root - 2017-12-10 18:38:53.579188: step 42070, loss = 0.67, batch loss = 0.61 (10.2 examples/sec; 0.787 sec/batch; 63h:30m:27s remains)
INFO - root - 2017-12-10 18:39:01.378414: step 42080, loss = 0.69, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 60h:55m:21s remains)
INFO - root - 2017-12-10 18:39:09.090620: step 42090, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 65h:36m:33s remains)
INFO - root - 2017-12-10 18:39:16.959495: step 42100, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 61h:16m:47s remains)
2017-12-10 18:39:17.839982: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26855969 0.30708611 0.33480573 0.34708387 0.34675169 0.33852449 0.32797512 0.32254556 0.33132929 0.35847518 0.39001229 0.39975604 0.37873039 0.3239648 0.24115697][0.30864725 0.34637165 0.37210652 0.38415816 0.38443652 0.37559384 0.36068216 0.34484687 0.33684912 0.34366006 0.35736141 0.35761997 0.33563381 0.28558326 0.20911902][0.33065939 0.36395234 0.38448951 0.39508709 0.39719906 0.38926664 0.36987984 0.34150094 0.31243798 0.29326755 0.2830914 0.27027795 0.24725729 0.2067771 0.14678489][0.33811855 0.36830077 0.38511777 0.39519894 0.39977992 0.39276174 0.36788926 0.32625723 0.27694982 0.23351681 0.20045286 0.17477567 0.15193537 0.12496483 0.086527191][0.3385196 0.36722896 0.38315436 0.39463526 0.4029589 0.3992759 0.37282479 0.32345888 0.2603949 0.19907407 0.1473278 0.11015002 0.087385409 0.072069615 0.052593995][0.34452841 0.3705377 0.38670269 0.40124109 0.41433135 0.41614881 0.39362454 0.34576982 0.27990034 0.21121669 0.14968587 0.10607962 0.083591968 0.074367821 0.064405829][0.35077065 0.37329477 0.38959411 0.40728724 0.42457113 0.43150923 0.415456 0.37602431 0.3173832 0.25310266 0.19416642 0.15350388 0.1343168 0.12609537 0.11531451][0.34429008 0.36418751 0.38267207 0.40557387 0.42723575 0.43669045 0.42382506 0.39167151 0.34287074 0.28954133 0.24393128 0.2170009 0.20749515 0.19957672 0.18211903][0.3199617 0.33848536 0.36049622 0.39062402 0.41783857 0.42901242 0.41718113 0.38885805 0.34737277 0.30542973 0.27745172 0.26954624 0.27305636 0.2658608 0.23969409][0.28473341 0.3011758 0.32591933 0.363368 0.39749306 0.41247982 0.40414515 0.38013774 0.34434727 0.31102467 0.29801887 0.30703628 0.32133844 0.31320789 0.27738079][0.25482973 0.26820922 0.29216516 0.332846 0.37116629 0.38960865 0.38658223 0.36983594 0.34287813 0.31993285 0.32117757 0.3446492 0.36679211 0.35608187 0.30968282][0.24155179 0.2519381 0.2706978 0.30649158 0.34054011 0.35671917 0.35710165 0.35069931 0.34039977 0.33733377 0.35847524 0.39634052 0.42305091 0.40695497 0.348108][0.24586444 0.25476509 0.26561704 0.28891045 0.3088924 0.31462055 0.31383431 0.31816202 0.32969365 0.35531253 0.40321475 0.45706233 0.48540914 0.46012819 0.38629702][0.25719813 0.26690987 0.27078065 0.27837873 0.27869087 0.26826972 0.26167849 0.27378973 0.30689096 0.36410272 0.44143191 0.51189142 0.53980947 0.50255823 0.41198251][0.25803846 0.27194455 0.27249676 0.26625055 0.2474696 0.22070162 0.20654631 0.22216357 0.27061567 0.35265666 0.45379 0.5378992 0.56608111 0.520804 0.41895649]]...]
INFO - root - 2017-12-10 18:39:25.496895: step 42110, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 65h:14m:25s remains)
INFO - root - 2017-12-10 18:39:33.428643: step 42120, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 64h:49m:40s remains)
INFO - root - 2017-12-10 18:39:41.312767: step 42130, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.758 sec/batch; 61h:09m:57s remains)
INFO - root - 2017-12-10 18:39:49.115355: step 42140, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 61h:03m:37s remains)
INFO - root - 2017-12-10 18:39:56.966905: step 42150, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 62h:50m:22s remains)
INFO - root - 2017-12-10 18:40:04.919203: step 42160, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 63h:04m:40s remains)
INFO - root - 2017-12-10 18:40:12.572408: step 42170, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 63h:22m:43s remains)
INFO - root - 2017-12-10 18:40:20.472030: step 42180, loss = 0.71, batch loss = 0.65 (9.5 examples/sec; 0.842 sec/batch; 67h:55m:21s remains)
INFO - root - 2017-12-10 18:40:28.317925: step 42190, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 63h:31m:58s remains)
INFO - root - 2017-12-10 18:40:35.971624: step 42200, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 64h:46m:58s remains)
2017-12-10 18:40:36.822900: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29606611 0.26881549 0.24667452 0.23403284 0.22952858 0.23048718 0.23541081 0.24003187 0.24550451 0.25075287 0.25428742 0.25688729 0.25467122 0.24684933 0.23800348][0.29317662 0.27527311 0.26287144 0.25919613 0.26159132 0.26633647 0.27361786 0.28151578 0.29064089 0.29775155 0.30274042 0.30595988 0.30099 0.28488433 0.26379603][0.27937412 0.27567622 0.27782443 0.28641096 0.29602808 0.30243808 0.30878994 0.3158344 0.32363898 0.32763419 0.33052808 0.33353105 0.32693985 0.30633962 0.27988929][0.2671825 0.27670166 0.292367 0.311462 0.32516006 0.33035409 0.33461487 0.34119931 0.34813997 0.34971538 0.35116708 0.35372835 0.34581381 0.32394597 0.2981427][0.25697365 0.27576095 0.29951248 0.3236205 0.33757117 0.34040245 0.34425312 0.35402516 0.36436328 0.36760134 0.36974967 0.37228781 0.36341158 0.34185132 0.319217][0.25017646 0.27176312 0.2963323 0.3201313 0.33359024 0.3376064 0.34635732 0.36422703 0.38090903 0.38639942 0.38763815 0.3879374 0.37746048 0.35749003 0.3393465][0.25081486 0.27226454 0.29401469 0.31540552 0.33009821 0.3394582 0.35599539 0.38110453 0.40001029 0.40204036 0.39643076 0.39022836 0.37735656 0.36046761 0.34789816][0.26693195 0.29011136 0.30856222 0.32667482 0.34222671 0.35599309 0.37661183 0.40185332 0.41459769 0.40601644 0.38847417 0.3728714 0.35676455 0.3429192 0.33610976][0.29578748 0.32129851 0.33579895 0.34873921 0.36127895 0.373884 0.39130649 0.40857854 0.40932068 0.38846573 0.36061609 0.33807924 0.32102442 0.31169671 0.31228402][0.338492 0.36235356 0.36888039 0.37194094 0.37460586 0.37831813 0.3852441 0.38898739 0.37638912 0.34623888 0.31333503 0.28982592 0.27720606 0.27668807 0.28844362][0.38360012 0.39800307 0.39146113 0.3815583 0.37184179 0.36463654 0.36068055 0.35293767 0.33179963 0.29846397 0.26623267 0.24669881 0.24228209 0.25310734 0.27738178][0.41141886 0.41188878 0.39260823 0.37400961 0.35864875 0.34773967 0.34034163 0.32912984 0.30764931 0.2779718 0.25085384 0.23704995 0.23948592 0.25764367 0.28797922][0.41238618 0.40066636 0.37421206 0.35546485 0.34519473 0.34123567 0.34037784 0.3343848 0.31942415 0.29776597 0.277761 0.26888087 0.27327621 0.2901935 0.31514198][0.38703334 0.37042573 0.34651923 0.33684036 0.34003988 0.35019559 0.36213046 0.36620092 0.35975626 0.34547833 0.33049703 0.32299069 0.32295477 0.32961759 0.33886474][0.33882484 0.32495743 0.31192404 0.3171263 0.33681732 0.36219123 0.3869164 0.40029883 0.3997443 0.38859257 0.373351 0.36118972 0.34997529 0.33944681 0.32732436]]...]
INFO - root - 2017-12-10 18:40:44.661456: step 42210, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.796 sec/batch; 64h:13m:02s remains)
INFO - root - 2017-12-10 18:40:52.508745: step 42220, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 62h:28m:05s remains)
INFO - root - 2017-12-10 18:41:00.422951: step 42230, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 63h:06m:27s remains)
INFO - root - 2017-12-10 18:41:08.265262: step 42240, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 62h:02m:30s remains)
INFO - root - 2017-12-10 18:41:15.954241: step 42250, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 64h:48m:38s remains)
INFO - root - 2017-12-10 18:41:23.906614: step 42260, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 63h:27m:21s remains)
INFO - root - 2017-12-10 18:41:31.669625: step 42270, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 61h:22m:49s remains)
INFO - root - 2017-12-10 18:41:39.536205: step 42280, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 63h:07m:41s remains)
INFO - root - 2017-12-10 18:41:47.255648: step 42290, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 64h:46m:27s remains)
INFO - root - 2017-12-10 18:41:55.261363: step 42300, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 64h:09m:30s remains)
2017-12-10 18:41:56.105653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0045446055 0.0063270964 0.020103848 0.035707466 0.046042826 0.049829882 0.053577084 0.06041437 0.06658034 0.063794509 0.051909026 0.034462795 0.010255296 -0.017875332 -0.042807087][0.033019077 0.050620843 0.07241942 0.097115919 0.11417167 0.12150086 0.12871553 0.13979188 0.14942969 0.14565384 0.12819585 0.10259507 0.066263 0.02228535 -0.018809946][0.075222708 0.10283293 0.13631779 0.17336677 0.19964042 0.21248287 0.22381322 0.23884138 0.25055 0.2445471 0.22025433 0.18534569 0.13569401 0.074290849 0.015316937][0.10923354 0.1499791 0.20060359 0.25628567 0.29658425 0.31726065 0.33265579 0.35006025 0.35969675 0.34558308 0.3086805 0.26075318 0.19656593 0.11839441 0.043553606][0.12624019 0.1820835 0.25538751 0.3368299 0.39674464 0.4279108 0.44892547 0.4689478 0.47299588 0.44415545 0.38704491 0.32095268 0.24023142 0.14641429 0.058790896][0.13008161 0.20090619 0.29894036 0.40981609 0.49357477 0.53939533 0.571226 0.59843546 0.59867579 0.55298609 0.47130239 0.38242298 0.28204122 0.17096409 0.069921955][0.12977637 0.21172874 0.3282778 0.46191579 0.56622159 0.62756389 0.6746636 0.7144593 0.716163 0.65775263 0.55444717 0.44443902 0.32503125 0.19640873 0.081196994][0.12788333 0.21280733 0.33425814 0.47448671 0.58747977 0.65900081 0.72139055 0.77684152 0.78675163 0.7254101 0.61169559 0.48940882 0.35612154 0.21303016 0.086087435][0.12486997 0.20260806 0.31268829 0.4406158 0.54757887 0.6207664 0.69339991 0.76238179 0.78500152 0.73271316 0.62455457 0.5037533 0.36670893 0.21663566 0.083822511][0.1248693 0.18895176 0.27536714 0.37446633 0.45924079 0.52119374 0.59226322 0.66597933 0.70079982 0.66716105 0.579939 0.47554567 0.34815893 0.20335422 0.074451037][0.12439591 0.17325217 0.23141682 0.29353717 0.34546962 0.38448051 0.43974653 0.50484836 0.54433072 0.53006226 0.4699623 0.39038205 0.28396907 0.15832089 0.046589974][0.11001361 0.14480662 0.17731009 0.20474406 0.22333798 0.2356506 0.26677227 0.31298012 0.34733936 0.34586275 0.30957794 0.25498465 0.17517072 0.079200633 -0.0036044237][0.076291151 0.099477917 0.11420685 0.11867455 0.11470633 0.10768749 0.11709607 0.14225742 0.16489759 0.16702086 0.14559959 0.1100268 0.055863895 -0.0079002539 -0.058669046][0.036101535 0.051876072 0.058728989 0.054486733 0.042203542 0.027425986 0.023475364 0.031322919 0.039888605 0.037726894 0.021370368 -0.0034998336 -0.038230076 -0.07522811 -0.099576376][0.00035026172 0.011950862 0.017544629 0.013954866 0.0030873644 -0.011031592 -0.020366188 -0.023629773 -0.026632836 -0.035158329 -0.051193833 -0.070774332 -0.093107469 -0.11240134 -0.12016222]]...]
INFO - root - 2017-12-10 18:42:04.105190: step 42310, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 63h:56m:09s remains)
INFO - root - 2017-12-10 18:42:11.965339: step 42320, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.781 sec/batch; 62h:54m:51s remains)
INFO - root - 2017-12-10 18:42:19.713814: step 42330, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 62h:14m:11s remains)
INFO - root - 2017-12-10 18:42:27.596285: step 42340, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 63h:16m:11s remains)
INFO - root - 2017-12-10 18:42:35.416350: step 42350, loss = 0.71, batch loss = 0.65 (9.6 examples/sec; 0.831 sec/batch; 66h:59m:33s remains)
INFO - root - 2017-12-10 18:42:43.306452: step 42360, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 62h:03m:27s remains)
INFO - root - 2017-12-10 18:42:51.103121: step 42370, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 64h:00m:01s remains)
INFO - root - 2017-12-10 18:42:58.856428: step 42380, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 64h:02m:32s remains)
INFO - root - 2017-12-10 18:43:06.845434: step 42390, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 64h:14m:27s remains)
INFO - root - 2017-12-10 18:43:14.640203: step 42400, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 61h:56m:33s remains)
2017-12-10 18:43:15.591570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.060451478 -0.05811156 -0.059276573 -0.064945556 -0.069069207 -0.06772878 -0.0630856 -0.060119953 -0.0602096 -0.064101294 -0.07152541 -0.08346308 -0.096769422 -0.10601173 -0.10937662][-0.019629713 -0.0046580238 0.0028631364 0.0027830903 0.00406573 0.013705817 0.025405385 0.029347656 0.024435345 0.011793866 -0.0075910175 -0.035889067 -0.067907847 -0.092650078 -0.10560705][0.048863247 0.08519046 0.10901566 0.12035066 0.13024896 0.1500899 0.16760331 0.16698483 0.14969967 0.12114361 0.082807258 0.030687883 -0.026503919 -0.07066045 -0.095081329][0.14894573 0.21531047 0.25945994 0.28223005 0.29706433 0.320722 0.33620742 0.3241401 0.28942177 0.24154881 0.18211225 0.10407827 0.0196026 -0.046155404 -0.083261207][0.26207298 0.36509418 0.43280405 0.46687189 0.48461488 0.5069769 0.51401883 0.4860037 0.43144518 0.36354527 0.28274462 0.17839943 0.066200376 -0.021317255 -0.07061182][0.35491848 0.49281892 0.58414859 0.631501 0.65595096 0.678517 0.67707866 0.63301641 0.55909085 0.471285 0.36892989 0.23878343 0.10089367 -0.0050410768 -0.062796615][0.41292712 0.57585859 0.68623006 0.74737966 0.781634 0.80560207 0.79645622 0.73908687 0.6495856 0.54439139 0.42269412 0.271251 0.11428235 -0.0034179536 -0.0641546][0.42612422 0.59794652 0.71680921 0.78619581 0.82611883 0.84677333 0.82762218 0.76094925 0.66560829 0.55505788 0.42692611 0.26856342 0.10639852 -0.012672058 -0.070598252][0.38389656 0.54344338 0.65627211 0.72513783 0.76482403 0.77953571 0.754018 0.68890995 0.60320336 0.50387138 0.3851386 0.23568885 0.082359254 -0.027923707 -0.078583255][0.28846079 0.41642398 0.50952274 0.56962234 0.60550177 0.61729473 0.59567082 0.54546326 0.48134771 0.40230504 0.30064619 0.16943336 0.036119387 -0.055849656 -0.094020024][0.16223188 0.24696673 0.30948952 0.35112038 0.37661043 0.38544303 0.37365308 0.34579471 0.30859467 0.2540445 0.17499638 0.070628062 -0.032137629 -0.097253919 -0.11807869][0.043171369 0.087916322 0.11956365 0.13843258 0.14768003 0.14899957 0.14274637 0.13190475 0.11635883 0.084980577 0.03260972 -0.03779272 -0.10359296 -0.13904183 -0.1425205][-0.045265123 -0.030023165 -0.022021718 -0.022674266 -0.029165411 -0.037585236 -0.044855647 -0.049040675 -0.053084131 -0.066824794 -0.093271621 -0.12924947 -0.15959927 -0.16934262 -0.15987882][-0.10301212 -0.10586041 -0.1113416 -0.12233773 -0.13631251 -0.14912885 -0.15724899 -0.15985981 -0.1600087 -0.16409175 -0.17309581 -0.18481563 -0.19111376 -0.18510778 -0.16890806][-0.13188727 -0.14290015 -0.15245676 -0.16434233 -0.17675076 -0.18681958 -0.19234329 -0.19373481 -0.1932871 -0.19422379 -0.19611692 -0.19720551 -0.19352381 -0.1824317 -0.16549686]]...]
INFO - root - 2017-12-10 18:43:23.198314: step 42410, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 64h:30m:43s remains)
INFO - root - 2017-12-10 18:43:31.044597: step 42420, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 63h:42m:50s remains)
INFO - root - 2017-12-10 18:43:38.909261: step 42430, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 61h:51m:51s remains)
INFO - root - 2017-12-10 18:43:46.762213: step 42440, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 64h:50m:35s remains)
INFO - root - 2017-12-10 18:43:54.600516: step 42450, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 62h:52m:58s remains)
INFO - root - 2017-12-10 18:44:02.523044: step 42460, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.760 sec/batch; 61h:13m:52s remains)
INFO - root - 2017-12-10 18:44:10.245978: step 42470, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 62h:23m:35s remains)
INFO - root - 2017-12-10 18:44:18.119452: step 42480, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 63h:15m:33s remains)
INFO - root - 2017-12-10 18:44:25.811940: step 42490, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 63h:45m:25s remains)
INFO - root - 2017-12-10 18:44:33.702333: step 42500, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.753 sec/batch; 60h:40m:59s remains)
2017-12-10 18:44:34.526900: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19741774 0.18272698 0.16257189 0.14457108 0.13149256 0.11672463 0.099394366 0.082539231 0.079327039 0.10039432 0.13465445 0.16147341 0.16449629 0.14378935 0.10872624][0.23991613 0.23135716 0.21450803 0.19925527 0.18756253 0.17201512 0.14842936 0.11857967 0.099579871 0.10923182 0.14330637 0.17828616 0.19098045 0.17548746 0.13984235][0.26333418 0.26252541 0.25186992 0.24327837 0.23885271 0.22930855 0.20557804 0.16578348 0.12944373 0.12189281 0.14833717 0.18606216 0.20669141 0.19795316 0.1650987][0.27137142 0.27591905 0.27166986 0.27255756 0.28088206 0.28435716 0.267607 0.22388221 0.17321149 0.14672178 0.1590503 0.19201136 0.21564405 0.21301912 0.18626149][0.25298917 0.26007047 0.26293153 0.27633366 0.30154362 0.32286379 0.31869903 0.27753705 0.21772265 0.17291187 0.16499639 0.18442446 0.20520511 0.20863548 0.19260606][0.21424901 0.22367527 0.23522499 0.26173761 0.30179828 0.33777088 0.34517351 0.30988637 0.24743186 0.18969761 0.16147092 0.16345306 0.17840044 0.18848671 0.18651126][0.17841469 0.18997271 0.20802395 0.24175106 0.28669336 0.32639742 0.33848122 0.30907384 0.25056759 0.18967161 0.14964664 0.13903324 0.15060166 0.16919635 0.18212205][0.16197041 0.17366578 0.19223787 0.224094 0.26272994 0.29540029 0.30473575 0.27946973 0.22935519 0.1743394 0.13339521 0.11843459 0.1303691 0.15752654 0.18301028][0.16830629 0.17588107 0.18773171 0.21015058 0.23625797 0.25707096 0.26021132 0.23733577 0.19612907 0.15005852 0.11355887 0.09885408 0.11241532 0.1463746 0.18119885][0.18272853 0.18540435 0.18824467 0.19950333 0.21346988 0.22376303 0.22107701 0.19891961 0.16319397 0.12280419 0.089177012 0.074039452 0.087745473 0.12584209 0.16784513][0.18320328 0.18456918 0.18258794 0.18758054 0.19565958 0.20123439 0.19556464 0.17282617 0.13808686 0.098099552 0.063810766 0.04673484 0.059207685 0.099383429 0.14710218][0.16343471 0.16664717 0.16477762 0.16966052 0.17948008 0.18781611 0.18410951 0.16176841 0.12583895 0.083636083 0.048118 0.030985842 0.04407683 0.0862033 0.13820992][0.1303813 0.13604377 0.13632847 0.14372069 0.15886231 0.17445149 0.17741114 0.15963121 0.12616032 0.08583121 0.053783514 0.041576251 0.058399912 0.10184611 0.15438071][0.10625747 0.11239938 0.11314923 0.12118132 0.1400485 0.16266827 0.17407593 0.16469976 0.13896132 0.10699186 0.084866889 0.083406448 0.10792971 0.15333325 0.2036248][0.10562886 0.10877745 0.10655345 0.11230789 0.13189231 0.15904111 0.17747226 0.17669524 0.16041602 0.14020416 0.13192829 0.14486735 0.18005487 0.22883812 0.276156]]...]
INFO - root - 2017-12-10 18:44:42.558802: step 42510, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 62h:43m:23s remains)
INFO - root - 2017-12-10 18:44:50.380827: step 42520, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 62h:24m:28s remains)
INFO - root - 2017-12-10 18:44:58.250572: step 42530, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 64h:03m:59s remains)
INFO - root - 2017-12-10 18:45:06.229606: step 42540, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 63h:00m:28s remains)
INFO - root - 2017-12-10 18:45:13.989437: step 42550, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.800 sec/batch; 64h:26m:06s remains)
INFO - root - 2017-12-10 18:45:21.826432: step 42560, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 62h:28m:20s remains)
INFO - root - 2017-12-10 18:45:29.459276: step 42570, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 63h:42m:33s remains)
INFO - root - 2017-12-10 18:45:37.378596: step 42580, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 65h:01m:43s remains)
INFO - root - 2017-12-10 18:45:45.221670: step 42590, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 64h:42m:06s remains)
INFO - root - 2017-12-10 18:45:53.172222: step 42600, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 63h:42m:12s remains)
2017-12-10 18:45:53.977714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042068258 -0.037750263 -0.031138731 -0.023011494 -0.013916089 -0.0053843861 -0.0018022337 -0.0036982356 -0.0088729542 -0.01661977 -0.023214143 -0.023682283 -0.018929448 -0.011877276 -0.0064181844][-0.038642675 -0.031700712 -0.02060337 -0.0055784974 0.011661308 0.02774925 0.035491213 0.034326363 0.028001977 0.016859854 0.0060877926 0.0036235326 0.0088903327 0.018326631 0.025493454][-0.033538643 -0.022905992 -0.0053689084 0.019500645 0.047532823 0.072936617 0.086025916 0.086807974 0.080151349 0.065370664 0.049367432 0.043808185 0.048324626 0.059135314 0.066948421][-0.028470824 -0.013699995 0.010962697 0.046346948 0.085125647 0.11942518 0.1380301 0.14195268 0.13669062 0.11995538 0.09971042 0.0900961 0.0923719 0.1026606 0.10898469][-0.025395548 -0.0076469691 0.022886839 0.067117676 0.11458499 0.15605739 0.18024503 0.188944 0.186773 0.17020924 0.14793281 0.13464089 0.13298832 0.1400076 0.14221646][-0.025611486 -0.0067696422 0.028011099 0.080044359 0.13576292 0.18460615 0.21531129 0.22983833 0.23127383 0.21517581 0.19146934 0.17413764 0.16691716 0.16796216 0.1642873][-0.02781288 -0.0083901864 0.030360719 0.090415545 0.15574877 0.21405725 0.25304592 0.27458075 0.2806997 0.26633471 0.24247426 0.22141346 0.20768571 0.20023878 0.18827158][-0.030350771 -0.010054005 0.032348748 0.099762931 0.17442088 0.24205334 0.28874373 0.31641522 0.32753918 0.31589118 0.29254487 0.26778951 0.24749155 0.23059599 0.20869005][-0.033232082 -0.01290197 0.031652778 0.10448723 0.18730611 0.2642287 0.3190746 0.35343921 0.37079969 0.36299691 0.34063259 0.31209636 0.2851952 0.25870934 0.22620739][-0.035724055 -0.015979569 0.028733812 0.10346967 0.19097185 0.27458185 0.33615839 0.376995 0.40153944 0.39923176 0.37909576 0.3485 0.31732577 0.28439748 0.24370967][-0.037512384 -0.019010644 0.023347337 0.094911747 0.18100061 0.26531646 0.32863924 0.37226212 0.40167478 0.4044216 0.3871797 0.35685909 0.32540223 0.29098824 0.24668205][-0.03939512 -0.023336144 0.013559449 0.075749293 0.15204401 0.22807632 0.28544685 0.32521272 0.35378683 0.35883522 0.34463173 0.3171216 0.28872392 0.2570146 0.21405613][-0.043471072 -0.032254536 -0.0049740719 0.041259002 0.099066719 0.15734932 0.20070262 0.22979203 0.25114453 0.25464088 0.24262737 0.21931785 0.19591571 0.17027491 0.13470846][-0.048825912 -0.044220984 -0.029046075 -0.0020492058 0.032969095 0.068634495 0.093698971 0.10880043 0.11990793 0.12024038 0.11040256 0.09272296 0.076054938 0.059114229 0.035769403][-0.052804861 -0.054042123 -0.049753118 -0.040158503 -0.026309608 -0.012046735 -0.0041997144 -0.0019496285 -0.00024724958 -0.0027552473 -0.010105264 -0.021295127 -0.030468768 -0.038371328 -0.049018763]]...]
INFO - root - 2017-12-10 18:46:01.750447: step 42610, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 61h:27m:42s remains)
INFO - root - 2017-12-10 18:46:09.689796: step 42620, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 65h:31m:42s remains)
INFO - root - 2017-12-10 18:46:17.483830: step 42630, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 64h:44m:36s remains)
INFO - root - 2017-12-10 18:46:25.272227: step 42640, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 62h:52m:41s remains)
INFO - root - 2017-12-10 18:46:32.958207: step 42650, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 63h:57m:53s remains)
INFO - root - 2017-12-10 18:46:40.811760: step 42660, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 64h:12m:59s remains)
INFO - root - 2017-12-10 18:46:48.668604: step 42670, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 63h:26m:35s remains)
INFO - root - 2017-12-10 18:46:56.584067: step 42680, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 62h:40m:01s remains)
INFO - root - 2017-12-10 18:47:04.503818: step 42690, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 65h:08m:30s remains)
INFO - root - 2017-12-10 18:47:12.415158: step 42700, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 64h:12m:46s remains)
2017-12-10 18:47:13.352405: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18668531 0.19520515 0.19899085 0.22042143 0.25294414 0.28557655 0.30879056 0.31654832 0.31505361 0.30996516 0.32874402 0.37708119 0.4244338 0.44010583 0.41248071][0.27594885 0.29037368 0.29884604 0.32824445 0.3697761 0.41205895 0.44210905 0.44784805 0.43405855 0.41136742 0.41100287 0.44136485 0.47255495 0.47443661 0.43418247][0.3677502 0.38008058 0.3866756 0.41889417 0.46583986 0.51601237 0.5529862 0.55713892 0.5286532 0.48307937 0.45397493 0.45686582 0.46713853 0.45682368 0.41035452][0.44481802 0.44463763 0.44261688 0.47482634 0.52936947 0.59282869 0.64287722 0.65095162 0.61228204 0.54634231 0.48724747 0.45938975 0.44681627 0.42521697 0.37615335][0.50266486 0.48511901 0.47477919 0.51071286 0.57934427 0.66219264 0.72907704 0.74406826 0.70073736 0.62159771 0.539626 0.48492441 0.44988564 0.41470772 0.35997239][0.53166384 0.49789068 0.48443466 0.52947748 0.61584622 0.71785045 0.79899347 0.82032889 0.77814019 0.6977241 0.61020869 0.54557061 0.49829328 0.45037374 0.38436225][0.52831119 0.48597571 0.4758887 0.5312317 0.63001353 0.74110907 0.82713878 0.85221475 0.8173939 0.7483328 0.67387694 0.61849684 0.571657 0.51409775 0.43296677][0.50190645 0.46120515 0.4603605 0.52286178 0.62288225 0.72862953 0.80759847 0.83177847 0.8067764 0.75543272 0.70305264 0.66602272 0.62641239 0.56302023 0.46873721][0.45327145 0.42277703 0.43175566 0.49349752 0.5810712 0.66681361 0.72651249 0.74258024 0.72416496 0.68932939 0.65791166 0.63777983 0.6064747 0.54331762 0.44639322][0.38543776 0.3651813 0.37652031 0.424148 0.483464 0.53410667 0.56192362 0.56022269 0.54049683 0.51583117 0.49975654 0.49344358 0.47350919 0.42125934 0.33787885][0.282037 0.26643166 0.27178818 0.29694656 0.3226963 0.33646658 0.33239043 0.31204274 0.28788534 0.26890308 0.26321509 0.26835662 0.2627292 0.22969319 0.17085221][0.15143305 0.1352241 0.13013877 0.1330543 0.13168262 0.11990102 0.096150607 0.065270163 0.038594581 0.022060266 0.020132788 0.030043002 0.035310309 0.022497201 -0.00798729][0.013527391 -0.00415918 -0.016080871 -0.025893338 -0.039518658 -0.059119463 -0.085353591 -0.11427201 -0.13796763 -0.1523992 -0.15512644 -0.14724627 -0.13843539 -0.13760282 -0.14566165][-0.10713321 -0.12378871 -0.13598016 -0.14752127 -0.16022371 -0.17465943 -0.19165935 -0.20956449 -0.22439723 -0.2338566 -0.23670724 -0.23271388 -0.22576086 -0.22021766 -0.21719109][-0.17240177 -0.1861371 -0.19460352 -0.2025537 -0.20995428 -0.21686113 -0.22390112 -0.23080093 -0.23622185 -0.23928252 -0.23967017 -0.23707718 -0.23256961 -0.22754091 -0.2223964]]...]
INFO - root - 2017-12-10 18:47:21.319175: step 42710, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.828 sec/batch; 66h:41m:08s remains)
INFO - root - 2017-12-10 18:47:29.222320: step 42720, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 61h:51m:27s remains)
INFO - root - 2017-12-10 18:47:36.872334: step 42730, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.762 sec/batch; 61h:19m:52s remains)
INFO - root - 2017-12-10 18:47:44.862139: step 42740, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 65h:36m:23s remains)
INFO - root - 2017-12-10 18:47:52.786276: step 42750, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 64h:36m:55s remains)
INFO - root - 2017-12-10 18:48:00.718607: step 42760, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 63h:59m:55s remains)
INFO - root - 2017-12-10 18:48:08.577789: step 42770, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 62h:47m:22s remains)
INFO - root - 2017-12-10 18:48:16.346044: step 42780, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 61h:42m:06s remains)
INFO - root - 2017-12-10 18:48:24.177788: step 42790, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 63h:30m:15s remains)
INFO - root - 2017-12-10 18:48:32.031991: step 42800, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 63h:38m:37s remains)
2017-12-10 18:48:32.894393: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26097909 0.24081363 0.21996783 0.19898741 0.16976693 0.13199115 0.090657525 0.054312028 0.033311658 0.038492762 0.069942094 0.1181684 0.16544516 0.20202138 0.22494707][0.28058955 0.24811558 0.21407491 0.18542279 0.15574394 0.12354977 0.091738008 0.066243529 0.054989245 0.06642618 0.099667229 0.14591579 0.18786328 0.21687284 0.2303714][0.27713144 0.23729378 0.19241507 0.1586374 0.13275932 0.11224183 0.096241109 0.085819118 0.086389273 0.10284582 0.1324662 0.16786024 0.19567785 0.21172899 0.21483842][0.24792334 0.2049052 0.15657139 0.1255472 0.11178067 0.11097375 0.11685162 0.12374664 0.13446359 0.15158862 0.17110711 0.1881562 0.19586973 0.19598362 0.1893587][0.19945803 0.15751234 0.11506162 0.095846072 0.10121021 0.12422276 0.15282281 0.17490658 0.19048031 0.2026508 0.20790178 0.20492373 0.19295186 0.17826295 0.163509][0.14605604 0.11068883 0.08207877 0.079915151 0.10526122 0.14887004 0.19483152 0.22627538 0.24012001 0.24230815 0.23153532 0.21090119 0.18454535 0.16090502 0.14234412][0.093479894 0.069529496 0.059215013 0.075496554 0.11746771 0.17405199 0.22835127 0.2618508 0.26970223 0.26064491 0.23636103 0.20425352 0.1719365 0.14768982 0.13137121][0.049014028 0.036644775 0.043543197 0.076645143 0.13106461 0.19333348 0.24712963 0.27568761 0.27525637 0.25699002 0.22527075 0.1895864 0.15900263 0.1401051 0.12951876][0.022566633 0.018391877 0.037297763 0.082677022 0.14475314 0.2070062 0.25376159 0.27220139 0.26236612 0.2378293 0.20523646 0.17338106 0.14958139 0.13777399 0.13229896][0.020797418 0.020057423 0.043421142 0.093609504 0.15711881 0.215542 0.25337255 0.26153028 0.24417649 0.21705417 0.18840885 0.16430224 0.14856759 0.14162059 0.13773122][0.035427473 0.036343426 0.060101476 0.10941985 0.16886385 0.2203134 0.24938451 0.25006124 0.2290601 0.20318629 0.18084134 0.16468982 0.15496367 0.14900675 0.14236906][0.051950075 0.054946277 0.079346217 0.1258624 0.1784929 0.22191918 0.2448913 0.24386759 0.22523099 0.20390195 0.18671736 0.17386585 0.16371706 0.15285756 0.13852024][0.060995568 0.066721931 0.092601508 0.1363364 0.18235825 0.21887346 0.23839879 0.23935711 0.22630967 0.2098183 0.19415896 0.17859884 0.1618177 0.14201236 0.1183411][0.05506511 0.06276454 0.089522742 0.1300983 0.17027973 0.20107839 0.21781772 0.22040266 0.21185651 0.19812758 0.18125729 0.16080011 0.13671236 0.1094308 0.079602182][0.033200432 0.041819673 0.067138389 0.10269529 0.1369741 0.16298388 0.17714985 0.17983676 0.17331561 0.16023055 0.14133199 0.11708831 0.088673219 0.058458127 0.028358432]]...]
INFO - root - 2017-12-10 18:48:40.770409: step 42810, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 65h:11m:40s remains)
INFO - root - 2017-12-10 18:48:48.512381: step 42820, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 64h:17m:59s remains)
INFO - root - 2017-12-10 18:48:56.334557: step 42830, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 62h:24m:06s remains)
INFO - root - 2017-12-10 18:49:04.229410: step 42840, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.767 sec/batch; 61h:44m:26s remains)
INFO - root - 2017-12-10 18:49:12.059884: step 42850, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.753 sec/batch; 60h:34m:16s remains)
INFO - root - 2017-12-10 18:49:19.962262: step 42860, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 63h:58m:59s remains)
INFO - root - 2017-12-10 18:49:27.822279: step 42870, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 64h:42m:38s remains)
INFO - root - 2017-12-10 18:49:35.704994: step 42880, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 64h:33m:56s remains)
INFO - root - 2017-12-10 18:49:43.343752: step 42890, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 62h:18m:48s remains)
INFO - root - 2017-12-10 18:49:51.212633: step 42900, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 61h:25m:12s remains)
2017-12-10 18:49:52.117170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.064806625 -0.070369951 -0.073305167 -0.0732858 -0.072621204 -0.073819034 -0.0760203 -0.075807206 -0.069971859 -0.058358885 -0.04423961 -0.036449954 -0.043600455 -0.063504308 -0.083942369][-0.020697102 -0.020230485 -0.017521175 -0.012619138 -0.0098205693 -0.013051228 -0.020580765 -0.024869531 -0.020014824 -0.0073133591 0.0088345939 0.017036641 0.0055489666 -0.02503223 -0.059055865][0.056827575 0.0685679 0.082611 0.097735509 0.10561165 0.099899694 0.083667926 0.06938529 0.067445025 0.07729578 0.093754545 0.10181986 0.085494451 0.042302728 -0.010227234][0.15543938 0.18463857 0.21652666 0.24768114 0.26493376 0.25843483 0.23137669 0.20062444 0.1834062 0.18299089 0.1940233 0.19845624 0.17528757 0.11844093 0.046053793][0.24869788 0.30133706 0.35680905 0.40918159 0.44165492 0.43976197 0.40419516 0.3544955 0.31522065 0.2957342 0.29106596 0.28156537 0.24459282 0.17248304 0.083095394][0.31986883 0.39390638 0.47331235 0.54787761 0.59813327 0.60535073 0.56771874 0.50571442 0.44814193 0.40879831 0.3830446 0.35190859 0.29445025 0.2036608 0.098420769][0.37370783 0.45960766 0.55381632 0.6447916 0.71061677 0.72961015 0.69739145 0.63286442 0.56414628 0.507858 0.46122381 0.40812686 0.33104458 0.22413841 0.10715353][0.40392885 0.48793131 0.58450335 0.6839807 0.76328313 0.79835546 0.77946407 0.72123456 0.64692348 0.575162 0.507671 0.43450156 0.34290394 0.22744319 0.10773197][0.40059578 0.4716523 0.55969745 0.6601277 0.75101018 0.80541104 0.80672872 0.7642526 0.69274443 0.6091485 0.52092546 0.4285489 0.32622981 0.20881361 0.094524264][0.36706558 0.41692764 0.48662621 0.57935756 0.6767534 0.749516 0.775006 0.75646776 0.69869959 0.61395293 0.51393104 0.40869281 0.29989868 0.18398887 0.077974707][0.31133938 0.33672145 0.38295057 0.45982763 0.5529626 0.63339943 0.6756404 0.67847335 0.63867462 0.5639168 0.46751451 0.36513919 0.26188278 0.15520854 0.061145373][0.23802897 0.24201424 0.265889 0.32324776 0.40333438 0.47983083 0.52717197 0.54199541 0.51701784 0.45657894 0.37379166 0.28628942 0.19941938 0.11046159 0.033050071][0.13566817 0.12796482 0.13880521 0.1809146 0.24708539 0.31448066 0.35969135 0.377993 0.361648 0.31318238 0.24465483 0.17367208 0.10542337 0.037246011 -0.02022706][0.017297508 0.0063357814 0.012885379 0.044564448 0.09726093 0.15171984 0.18950737 0.20547388 0.19378279 0.15561695 0.10114089 0.046394914 -0.0039148903 -0.051335484 -0.088037804][-0.0841224 -0.093229733 -0.085937083 -0.061737489 -0.023372637 0.014331143 0.039552692 0.048730716 0.038608834 0.010213391 -0.028788347 -0.066073455 -0.098894261 -0.12728119 -0.14517546]]...]
INFO - root - 2017-12-10 18:49:59.897334: step 42910, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 64h:05m:28s remains)
INFO - root - 2017-12-10 18:50:07.774536: step 42920, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.744 sec/batch; 59h:53m:07s remains)
INFO - root - 2017-12-10 18:50:15.618032: step 42930, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.815 sec/batch; 65h:34m:26s remains)
INFO - root - 2017-12-10 18:50:23.439533: step 42940, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 62h:27m:32s remains)
INFO - root - 2017-12-10 18:50:31.299159: step 42950, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 63h:47m:45s remains)
INFO - root - 2017-12-10 18:50:39.196244: step 42960, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.815 sec/batch; 65h:32m:49s remains)
INFO - root - 2017-12-10 18:50:46.919873: step 42970, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 64h:28m:32s remains)
INFO - root - 2017-12-10 18:50:54.601811: step 42980, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 62h:41m:26s remains)
INFO - root - 2017-12-10 18:51:02.377579: step 42990, loss = 0.69, batch loss = 0.63 (10.7 examples/sec; 0.746 sec/batch; 59h:57m:09s remains)
INFO - root - 2017-12-10 18:51:09.939404: step 43000, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.792 sec/batch; 63h:42m:33s remains)
2017-12-10 18:51:10.825531: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.46465242 0.46586588 0.4280726 0.36131662 0.28156632 0.20517777 0.14752141 0.11649127 0.11153001 0.12638426 0.15434234 0.19383927 0.23979309 0.29131904 0.34018928][0.39425004 0.39322051 0.35789075 0.29882595 0.23285866 0.1755224 0.1387943 0.12338799 0.12439405 0.13536438 0.15245755 0.17723678 0.20682746 0.24420449 0.28453237][0.27910468 0.27901074 0.25668484 0.21941008 0.18289635 0.16186866 0.16316813 0.17736605 0.19138886 0.1968285 0.19276372 0.1869126 0.18374327 0.19350529 0.21659343][0.15844008 0.16419616 0.1624513 0.15428877 0.15389179 0.17621267 0.22243318 0.27199948 0.3012096 0.29840603 0.26512882 0.21814688 0.17262626 0.1472766 0.14703645][0.075488523 0.091686018 0.11222175 0.13248573 0.16620626 0.22950916 0.31764627 0.39887559 0.4377076 0.42037043 0.35194373 0.25945723 0.16977231 0.10908519 0.08545915][0.055629779 0.08200679 0.11793608 0.15747161 0.21597828 0.31095195 0.43134558 0.53501558 0.57554406 0.538983 0.43620118 0.30350161 0.17756766 0.089737229 0.049321283][0.089734927 0.12071742 0.16026343 0.20533584 0.27391937 0.38398939 0.51908129 0.63016433 0.66411012 0.60891724 0.48233727 0.32665786 0.18382283 0.08648514 0.042547703][0.14232539 0.17105414 0.20423481 0.24331935 0.30689925 0.41151953 0.53828239 0.63857633 0.66015822 0.594286 0.46354178 0.310701 0.17648917 0.089997306 0.055982724][0.18135288 0.20367949 0.22623624 0.25389841 0.302729 0.38593593 0.48648086 0.56299758 0.57133394 0.5066517 0.39211738 0.26543558 0.15997928 0.098020636 0.081025764][0.19096434 0.20527774 0.21700123 0.23276111 0.26418984 0.320653 0.38953736 0.44005403 0.43966934 0.38635117 0.30007821 0.21029139 0.14052103 0.10545722 0.10474939][0.16880259 0.17608887 0.17910282 0.18462409 0.19990012 0.23132136 0.27163127 0.30111223 0.29886591 0.26336658 0.2086747 0.15523586 0.11718814 0.10303789 0.11235917][0.12272356 0.12498677 0.12280951 0.12153301 0.12476861 0.13686877 0.15536757 0.17026085 0.17008048 0.15263999 0.12538293 0.10064273 0.085294649 0.083707474 0.096132874][0.070083387 0.069178753 0.064985842 0.061139204 0.058919664 0.060973711 0.067391939 0.074406356 0.076531142 0.071421936 0.061853245 0.054222431 0.050930906 0.054107871 0.064579785][0.026668349 0.023809679 0.019210119 0.015443265 0.012856827 0.012536682 0.014818008 0.018562062 0.02130539 0.021374684 0.019612158 0.019055642 0.019988708 0.023387274 0.030090846][-0.0043980125 -0.0081073595 -0.012320824 -0.015253068 -0.016750218 -0.016616661 -0.015052642 -0.012554704 -0.010387672 -0.0094065517 -0.009009921 -0.00773315 -0.0060100118 -0.0036284779 -0.00010746789]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 18:51:18.712901: step 43010, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 62h:40m:49s remains)
INFO - root - 2017-12-10 18:51:26.569224: step 43020, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 61h:56m:10s remains)
INFO - root - 2017-12-10 18:51:34.405284: step 43030, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 63h:03m:12s remains)
INFO - root - 2017-12-10 18:51:42.215069: step 43040, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 62h:19m:46s remains)
INFO - root - 2017-12-10 18:51:49.875699: step 43050, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 64h:11m:33s remains)
INFO - root - 2017-12-10 18:51:57.695438: step 43060, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 63h:17m:18s remains)
INFO - root - 2017-12-10 18:52:05.409082: step 43070, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 62h:38m:26s remains)
INFO - root - 2017-12-10 18:52:13.116552: step 43080, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 61h:45m:38s remains)
INFO - root - 2017-12-10 18:52:20.767578: step 43090, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 61h:04m:24s remains)
INFO - root - 2017-12-10 18:52:28.593377: step 43100, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 61h:44m:35s remains)
2017-12-10 18:52:29.460080: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31353727 0.33887586 0.34364438 0.32819581 0.30083302 0.26589283 0.22460505 0.17912586 0.14059052 0.12138158 0.12559496 0.13937052 0.14904623 0.16017893 0.17057252][0.32113627 0.34336683 0.3458088 0.33179793 0.31148013 0.289168 0.26042348 0.21953046 0.17483354 0.14328456 0.13342221 0.13319056 0.1315366 0.13692762 0.14913598][0.3446714 0.357188 0.35128209 0.3341203 0.31845936 0.31013563 0.30035412 0.27541307 0.23633491 0.20071556 0.17821889 0.16127795 0.14364158 0.13873686 0.14724958][0.3853398 0.38681251 0.36958128 0.34470084 0.32728052 0.32647029 0.33312491 0.32769534 0.30187905 0.26962948 0.24029021 0.2108216 0.17940687 0.16322304 0.16368179][0.41757038 0.41489944 0.39016908 0.356834 0.33239809 0.3310104 0.34724459 0.35866934 0.34803796 0.32324976 0.29285473 0.25724769 0.21743262 0.19221969 0.18423606][0.4172751 0.42168808 0.40150276 0.36734229 0.336003 0.32693362 0.34109434 0.35850722 0.3572363 0.3390682 0.31096831 0.27481997 0.23312731 0.2035054 0.18999845][0.37669545 0.39343148 0.38858694 0.3655037 0.33490428 0.3182312 0.32249528 0.33405051 0.33307186 0.31760421 0.29265669 0.25981739 0.22148964 0.19183213 0.1760051][0.30790031 0.332665 0.3450799 0.3421357 0.32503793 0.31094459 0.30772713 0.30796888 0.2990095 0.27957836 0.25401986 0.22406642 0.19119714 0.16475126 0.1494011][0.23299184 0.25930268 0.28394765 0.30230528 0.30812836 0.30851188 0.3064431 0.29726452 0.27716377 0.24957809 0.21964391 0.1908046 0.16273664 0.13966742 0.12529415][0.1624492 0.18741184 0.2176436 0.25186348 0.28291097 0.30628455 0.31623042 0.30628091 0.27939469 0.24484599 0.20897548 0.1779965 0.15112823 0.12926623 0.11506712][0.098509595 0.12083138 0.15163362 0.19449875 0.24739461 0.296006 0.32514131 0.32403147 0.29909611 0.26351038 0.2234848 0.18802238 0.15856892 0.13540061 0.11979755][0.04167968 0.060355958 0.0893945 0.13544853 0.20289014 0.27141422 0.32034296 0.3353321 0.32235354 0.29366505 0.25408778 0.2148886 0.18132469 0.15563345 0.13756606][-0.0015259018 0.014003014 0.041540165 0.088073574 0.16142142 0.23972392 0.30209115 0.33371326 0.33734027 0.32022473 0.2851581 0.2453776 0.20916621 0.18149842 0.16065185][-0.022768842 -0.0081024747 0.019950742 0.065425023 0.13525346 0.21037503 0.2743288 0.31480289 0.33128411 0.32527655 0.29784253 0.26355183 0.23084036 0.20534767 0.18385468][-0.022206655 -0.0053568804 0.025073197 0.067635924 0.12547825 0.18567888 0.23925199 0.27840877 0.30016154 0.30145913 0.28283265 0.25997829 0.23870163 0.22135356 0.20283473]]...]
INFO - root - 2017-12-10 18:52:37.465972: step 43110, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 64h:28m:17s remains)
INFO - root - 2017-12-10 18:52:45.281113: step 43120, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 62h:37m:31s remains)
INFO - root - 2017-12-10 18:52:53.089053: step 43130, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 63h:50m:09s remains)
INFO - root - 2017-12-10 18:53:01.101551: step 43140, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 62h:08m:45s remains)
INFO - root - 2017-12-10 18:53:08.870689: step 43150, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 63h:16m:35s remains)
INFO - root - 2017-12-10 18:53:16.667380: step 43160, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 61h:10m:42s remains)
INFO - root - 2017-12-10 18:53:24.527923: step 43170, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 64h:34m:41s remains)
INFO - root - 2017-12-10 18:53:32.118473: step 43180, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 61h:09m:32s remains)
INFO - root - 2017-12-10 18:53:39.845652: step 43190, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 62h:34m:45s remains)
INFO - root - 2017-12-10 18:53:47.680473: step 43200, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.812 sec/batch; 65h:13m:44s remains)
2017-12-10 18:53:48.471105: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18981105 0.18141904 0.16171829 0.1352706 0.11086372 0.097177528 0.090293288 0.086743459 0.091030225 0.10616825 0.12622754 0.14013575 0.14985137 0.16045938 0.16907647][0.19748268 0.18705097 0.1626897 0.13375197 0.11042338 0.0972956 0.089179963 0.084872492 0.089850381 0.10410009 0.12023702 0.12947418 0.13519405 0.14209174 0.14810444][0.18919107 0.17956018 0.15632227 0.13160603 0.11461559 0.10574505 0.099329107 0.096426792 0.10201449 0.1123261 0.12020738 0.12130958 0.12063892 0.12118766 0.12159716][0.17628939 0.17036153 0.15257277 0.13545312 0.12590636 0.12163575 0.11825977 0.11865122 0.12520675 0.13046651 0.12838724 0.12034354 0.11260971 0.10557859 0.098675773][0.16666165 0.16384 0.15084122 0.1394354 0.13400443 0.1318682 0.13102232 0.13457084 0.14106622 0.14113368 0.13151503 0.11773284 0.10596151 0.094426364 0.083697788][0.16256098 0.16003723 0.14814737 0.13867652 0.13449702 0.13338624 0.13416736 0.13903208 0.14334348 0.13878764 0.12596099 0.11131606 0.099098362 0.085972704 0.074359335][0.16180333 0.15662955 0.14229813 0.131839 0.12769602 0.12780286 0.13029884 0.13542457 0.13704596 0.12996422 0.11843088 0.10683745 0.095982932 0.08228416 0.070470482][0.16534412 0.15583868 0.13701516 0.12373044 0.11924762 0.12124144 0.12611222 0.13243811 0.13414283 0.12824397 0.11990331 0.11098799 0.10002427 0.084550261 0.071569167][0.17171247 0.1581195 0.1344365 0.11826768 0.11409872 0.11897701 0.12706934 0.13620532 0.14089645 0.1382104 0.13092223 0.12041292 0.1066488 0.08946193 0.076894529][0.18255791 0.16659777 0.13962632 0.12078369 0.11568151 0.12120168 0.13075128 0.14207914 0.15003093 0.14977942 0.14083731 0.12623338 0.1101843 0.094598345 0.086322226][0.19678129 0.17988932 0.15141986 0.13102801 0.12360229 0.12640029 0.13429423 0.14526118 0.15346056 0.1513496 0.13716143 0.11718328 0.10035732 0.089085929 0.087613322][0.20039451 0.18425371 0.15798058 0.14062884 0.1339841 0.13546403 0.14227715 0.15189277 0.15675662 0.14758821 0.12457717 0.098070286 0.080356829 0.073580906 0.079341941][0.18720908 0.17340775 0.15344743 0.14444458 0.14372526 0.14855009 0.15747792 0.16601956 0.16492048 0.14505374 0.11073998 0.076172337 0.055488598 0.050946269 0.06200777][0.16805002 0.15610041 0.14241722 0.14155695 0.14757933 0.15780626 0.17046374 0.17808753 0.17055079 0.14067602 0.096974827 0.055966042 0.032263096 0.02867125 0.042509273][0.15131958 0.13987072 0.13075818 0.13565315 0.14710629 0.16224836 0.17779604 0.18395011 0.17120261 0.1359455 0.089660905 0.049255732 0.027512247 0.026081707 0.039560542]]...]
INFO - root - 2017-12-10 18:53:56.155886: step 43210, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 64h:29m:57s remains)
INFO - root - 2017-12-10 18:54:04.086381: step 43220, loss = 0.70, batch loss = 0.64 (9.5 examples/sec; 0.846 sec/batch; 67h:56m:43s remains)
INFO - root - 2017-12-10 18:54:12.026220: step 43230, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 62h:30m:04s remains)
INFO - root - 2017-12-10 18:54:19.861702: step 43240, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 61h:40m:17s remains)
INFO - root - 2017-12-10 18:54:27.909639: step 43250, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.798 sec/batch; 64h:04m:52s remains)
INFO - root - 2017-12-10 18:54:35.663306: step 43260, loss = 0.70, batch loss = 0.64 (12.2 examples/sec; 0.657 sec/batch; 52h:49m:15s remains)
INFO - root - 2017-12-10 18:54:43.456064: step 43270, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 62h:38m:51s remains)
INFO - root - 2017-12-10 18:54:51.392061: step 43280, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 62h:07m:33s remains)
INFO - root - 2017-12-10 18:54:59.044664: step 43290, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 62h:42m:21s remains)
INFO - root - 2017-12-10 18:55:06.974190: step 43300, loss = 0.70, batch loss = 0.65 (9.7 examples/sec; 0.822 sec/batch; 66h:00m:46s remains)
2017-12-10 18:55:07.790679: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.08651644 0.087289907 0.090826735 0.10066566 0.11393813 0.12596221 0.13347779 0.13665193 0.13299817 0.12392183 0.11236466 0.10148926 0.0968606 0.093415819 0.087952331][0.085071951 0.085666627 0.086549126 0.091947608 0.099749856 0.10614386 0.10987418 0.11274559 0.11272231 0.10954031 0.10398405 0.097603522 0.095972091 0.094446637 0.089838736][0.080295838 0.082742773 0.083376087 0.08557456 0.088058062 0.088680357 0.088326268 0.090343483 0.093435645 0.095846154 0.095777184 0.092998311 0.092941396 0.093135931 0.091240257][0.099324837 0.10796148 0.1124244 0.11477797 0.11440641 0.11098794 0.10723322 0.10799547 0.11264665 0.11777532 0.11947642 0.11626291 0.11422331 0.11516255 0.11849733][0.15424925 0.17218229 0.18276024 0.18731071 0.18617977 0.18121842 0.17599382 0.17598118 0.18089257 0.18580149 0.18575184 0.17834081 0.17207161 0.17483765 0.18732673][0.23350991 0.2611016 0.27711517 0.28418583 0.28421322 0.2808086 0.277098 0.27778187 0.28245813 0.28510222 0.28032064 0.2662167 0.25522441 0.2618565 0.28631893][0.30734113 0.34146923 0.36024916 0.36896187 0.37140003 0.3713699 0.37089291 0.3731564 0.37747315 0.37690613 0.36629513 0.34581941 0.33166865 0.34290341 0.37755606][0.33880955 0.37315077 0.39010128 0.39778817 0.40157759 0.40424824 0.40658486 0.41045725 0.41465175 0.41160333 0.39686537 0.37308443 0.35855106 0.37321076 0.41234073][0.30491456 0.33261445 0.34376657 0.34753415 0.35011253 0.35266584 0.35510683 0.35880995 0.36249653 0.35884726 0.34421748 0.32255909 0.31137794 0.32777131 0.36477211][0.21518286 0.23213285 0.23605175 0.23477577 0.23449825 0.23452769 0.23432103 0.23564592 0.23780937 0.23469277 0.22338542 0.20796613 0.20241836 0.21869551 0.2489069][0.1095273 0.11779764 0.11750988 0.11331012 0.11062194 0.10720164 0.102718 0.099925712 0.09897545 0.095245726 0.086639717 0.077052429 0.075764261 0.088988505 0.10933696][0.030645451 0.038424488 0.040222391 0.036561403 0.031656221 0.023241587 0.012245348 0.0029292365 -0.0035888825 -0.010636595 -0.019680483 -0.027533378 -0.029266352 -0.023034144 -0.01401082][0.0045327209 0.02113656 0.029969679 0.027780699 0.018031484 0.00067179871 -0.020662231 -0.039592154 -0.053892888 -0.066065341 -0.0775932 -0.08673209 -0.092016369 -0.09386716 -0.094478227][0.027420623 0.057039477 0.073532186 0.071151018 0.053567309 0.024404217 -0.0089340024 -0.038038265 -0.05984205 -0.076753065 -0.090746462 -0.10169216 -0.11058167 -0.11846834 -0.12505418][0.078153037 0.11798625 0.1377746 0.13152391 0.10399101 0.06271264 0.01870749 -0.018073019 -0.044281047 -0.06315475 -0.077574037 -0.088951744 -0.099746272 -0.11070049 -0.11992393]]...]
INFO - root - 2017-12-10 18:55:15.700843: step 43310, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 64h:17m:10s remains)
INFO - root - 2017-12-10 18:55:23.578131: step 43320, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 64h:38m:51s remains)
INFO - root - 2017-12-10 18:55:31.542839: step 43330, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 62h:09m:46s remains)
INFO - root - 2017-12-10 18:55:39.497237: step 43340, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 62h:59m:45s remains)
INFO - root - 2017-12-10 18:55:47.174214: step 43350, loss = 0.70, batch loss = 0.64 (12.6 examples/sec; 0.634 sec/batch; 50h:52m:58s remains)
INFO - root - 2017-12-10 18:55:55.105212: step 43360, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 62h:00m:34s remains)
INFO - root - 2017-12-10 18:56:02.850223: step 43370, loss = 0.73, batch loss = 0.67 (10.0 examples/sec; 0.802 sec/batch; 64h:23m:44s remains)
INFO - root - 2017-12-10 18:56:10.693849: step 43380, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 63h:55m:39s remains)
INFO - root - 2017-12-10 18:56:18.637770: step 43390, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 64h:46m:22s remains)
INFO - root - 2017-12-10 18:56:26.579300: step 43400, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.752 sec/batch; 60h:22m:39s remains)
2017-12-10 18:56:27.370500: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.099907659 0.12686045 0.16708 0.2170608 0.27634686 0.3383809 0.38098776 0.38007173 0.32914889 0.250857 0.16638227 0.090910129 0.039175525 0.020226564 0.034973763][0.1135809 0.14817943 0.19953345 0.26245144 0.33362654 0.40308443 0.44739255 0.44204414 0.38035902 0.28616658 0.1839326 0.0940883 0.034022275 0.012443596 0.027557245][0.12755308 0.1629398 0.22107421 0.2945751 0.37363845 0.44305789 0.48010784 0.46320581 0.38893557 0.28186065 0.16984786 0.076872535 0.018942282 0.0017353746 0.020274187][0.13439453 0.16645879 0.23078807 0.31679127 0.40478602 0.47175285 0.49531481 0.45952496 0.36857614 0.24990417 0.13451384 0.046929978 -0.00089866261 -0.0087994011 0.013884651][0.13282295 0.16011605 0.22917221 0.32727918 0.42509237 0.49072513 0.50119627 0.44642711 0.34039235 0.21464403 0.10186246 0.024993211 -0.0087599643 -0.005307625 0.021480462][0.12824018 0.15139766 0.22252296 0.3305127 0.43958139 0.50915205 0.51275843 0.44409394 0.32678676 0.19743407 0.08936841 0.023259621 0.0028046113 0.016667588 0.046572749][0.12657222 0.14710166 0.21690878 0.32910818 0.44614962 0.52194118 0.52528083 0.45088246 0.32866108 0.19934002 0.096221954 0.038301773 0.028090516 0.050532542 0.083620869][0.12840438 0.14766586 0.21199976 0.31989133 0.4372991 0.51718986 0.52453029 0.45270896 0.33347902 0.20901637 0.11206165 0.060472928 0.057236902 0.085995622 0.12259558][0.12495253 0.14311048 0.19974619 0.29787526 0.410582 0.4936842 0.509732 0.44838277 0.33911848 0.2234209 0.13342537 0.086750515 0.088402614 0.12263865 0.16442335][0.10610435 0.12228498 0.16974606 0.25476238 0.35855317 0.4427118 0.46941373 0.42380685 0.329767 0.22599317 0.14433973 0.10384009 0.11237844 0.15544964 0.20804682][0.0705425 0.082676828 0.11960984 0.18874763 0.27829671 0.35752407 0.39142063 0.36258918 0.28766653 0.19981608 0.12921165 0.096434526 0.11339355 0.16870044 0.23872609][0.023437615 0.03013912 0.056139927 0.10727513 0.17647791 0.24144319 0.27415594 0.25860134 0.20500328 0.13836785 0.083934709 0.062028605 0.088943489 0.15918116 0.25201622][-0.027112886 -0.025297029 -0.0089500239 0.024776278 0.071456134 0.11667965 0.14163478 0.13480023 0.10196596 0.058931887 0.023688145 0.014605272 0.051393289 0.13556111 0.24949273][-0.06870418 -0.070479296 -0.061940517 -0.043017145 -0.016572827 0.0097063081 0.025636693 0.024648644 0.0093281446 -0.012858077 -0.031000393 -0.029398909 0.013399972 0.10502826 0.23089115][-0.090924151 -0.094975956 -0.092790529 -0.086028568 -0.076235466 -0.065963626 -0.058832761 -0.057488751 -0.061449464 -0.069152176 -0.075120017 -0.067163594 -0.024706872 0.063517444 0.18641432]]...]
INFO - root - 2017-12-10 18:56:35.358029: step 43410, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 63h:17m:23s remains)
INFO - root - 2017-12-10 18:56:43.206772: step 43420, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 63h:54m:04s remains)
INFO - root - 2017-12-10 18:56:51.208959: step 43430, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.828 sec/batch; 66h:28m:01s remains)
INFO - root - 2017-12-10 18:56:58.943997: step 43440, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 63h:04m:32s remains)
INFO - root - 2017-12-10 18:57:06.769994: step 43450, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 62h:57m:00s remains)
INFO - root - 2017-12-10 18:57:14.657643: step 43460, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.794 sec/batch; 63h:46m:12s remains)
INFO - root - 2017-12-10 18:57:22.537381: step 43470, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 64h:39m:57s remains)
INFO - root - 2017-12-10 18:57:30.496877: step 43480, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 64h:49m:52s remains)
INFO - root - 2017-12-10 18:57:38.323318: step 43490, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.773 sec/batch; 62h:01m:06s remains)
INFO - root - 2017-12-10 18:57:46.194911: step 43500, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 63h:24m:04s remains)
2017-12-10 18:57:47.010612: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0074166264 0.00036129763 -0.0003036728 0.00460067 0.012870384 0.024833875 0.03852696 0.052660469 0.069249481 0.092657514 0.12501238 0.16713594 0.21542564 0.27149555 0.32570353][0.0062933047 0.0056675877 0.011781426 0.022670129 0.036917981 0.054640528 0.073037282 0.0906458 0.10934605 0.13386503 0.16632618 0.20847905 0.25672939 0.31136525 0.36219442][0.0077275089 0.016651863 0.032402758 0.051222023 0.072661325 0.096708186 0.11969879 0.13963501 0.15787852 0.17827223 0.20255969 0.23410808 0.27080977 0.31292316 0.35114473][0.013268013 0.033196948 0.06035731 0.08922907 0.1191945 0.14929959 0.17492777 0.19339731 0.20642412 0.21743757 0.22823054 0.24358732 0.26325822 0.28843385 0.31120276][0.023604974 0.054478478 0.092280239 0.13057135 0.16725303 0.19947834 0.22234036 0.23384778 0.23743223 0.23749025 0.23600176 0.2378716 0.24381351 0.25497854 0.26509506][0.039418336 0.078452036 0.12355961 0.16759896 0.20703188 0.23717582 0.25395614 0.25739926 0.25348029 0.24702814 0.23867363 0.23171085 0.22776964 0.22870314 0.22895975][0.059269585 0.10345412 0.1524211 0.19882376 0.23763902 0.26306793 0.27280632 0.26879835 0.25972664 0.25019494 0.23857261 0.22609636 0.21577467 0.21072322 0.20447694][0.079940721 0.12794048 0.17979378 0.22840036 0.26630616 0.28654578 0.28823963 0.27586469 0.26099426 0.24765657 0.23329341 0.21833928 0.2066053 0.20083609 0.19260968][0.095992781 0.14751698 0.20238139 0.25350496 0.29038075 0.30477759 0.29756463 0.27598798 0.25473049 0.23800629 0.22334385 0.21024165 0.20239049 0.20071757 0.19466104][0.1029952 0.15618773 0.21275373 0.26531538 0.30095181 0.31108889 0.29807961 0.27124926 0.24709016 0.2301397 0.2194293 0.21322925 0.21423253 0.22001114 0.21848655][0.11148745 0.16199249 0.21622533 0.26743606 0.30147842 0.30984622 0.29608613 0.27030909 0.24820054 0.23403618 0.22932878 0.2316168 0.24237609 0.25576246 0.25879812][0.13879476 0.18438219 0.23219381 0.2786181 0.30870008 0.3144905 0.30035219 0.27663231 0.2573171 0.2462703 0.24701057 0.25615239 0.27357081 0.29169026 0.29901814][0.17935596 0.21913397 0.25771147 0.29601696 0.31891671 0.3194232 0.30333591 0.28089198 0.26417792 0.25669327 0.26227075 0.27572712 0.29430032 0.31212991 0.32219845][0.22090022 0.25192118 0.27669537 0.30145961 0.31279439 0.3053959 0.28758693 0.26834476 0.2563968 0.25391778 0.26341218 0.27765131 0.29187292 0.3048377 0.31597275][0.24925093 0.26659861 0.27238128 0.27895382 0.27642867 0.26215467 0.24638072 0.235366 0.23219272 0.23648897 0.24870813 0.26076785 0.26810932 0.2747032 0.28599706]]...]
INFO - root - 2017-12-10 18:57:54.870589: step 43510, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 64h:37m:23s remains)
INFO - root - 2017-12-10 18:58:02.918654: step 43520, loss = 0.70, batch loss = 0.64 (8.8 examples/sec; 0.913 sec/batch; 73h:19m:31s remains)
INFO - root - 2017-12-10 18:58:10.545428: step 43530, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.827 sec/batch; 66h:21m:19s remains)
INFO - root - 2017-12-10 18:58:18.431169: step 43540, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 60h:57m:27s remains)
INFO - root - 2017-12-10 18:58:26.269582: step 43550, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 60h:39m:23s remains)
INFO - root - 2017-12-10 18:58:34.276592: step 43560, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 63h:27m:47s remains)
INFO - root - 2017-12-10 18:58:42.198323: step 43570, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 64h:30m:19s remains)
INFO - root - 2017-12-10 18:58:50.071616: step 43580, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.771 sec/batch; 61h:51m:40s remains)
INFO - root - 2017-12-10 18:58:57.878997: step 43590, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 63h:55m:25s remains)
INFO - root - 2017-12-10 18:59:05.596025: step 43600, loss = 0.69, batch loss = 0.64 (10.9 examples/sec; 0.733 sec/batch; 58h:49m:16s remains)
2017-12-10 18:59:06.504632: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.071790531 0.078998961 0.092943348 0.11085594 0.12212475 0.11901806 0.1014632 0.0740023 0.048471909 0.032689486 0.025136955 0.025218373 0.033794742 0.047571812 0.061623909][0.10147701 0.10641482 0.11068779 0.11518701 0.112126 0.096655995 0.070674583 0.040151928 0.016195478 0.00517726 0.0052964813 0.014091899 0.030388543 0.0508599 0.071775891][0.165796 0.16778654 0.15832104 0.14233831 0.11731716 0.08546944 0.0516843 0.021999095 0.0039490396 -0.00088099868 0.0052829823 0.019452743 0.040034749 0.064593866 0.0910232][0.25421739 0.25326413 0.22962309 0.19256388 0.14576048 0.098805211 0.060199961 0.035254076 0.025831483 0.027303644 0.036484104 0.051047314 0.0707009 0.095630243 0.12611164][0.33161023 0.32827753 0.29471821 0.24434516 0.18566325 0.1335113 0.098649874 0.084216475 0.085726537 0.093707755 0.10429863 0.11591804 0.12961493 0.14943172 0.17947601][0.372466 0.36904138 0.33532342 0.28684944 0.23276544 0.18883216 0.16532817 0.16269188 0.17221269 0.18403178 0.19504225 0.20291454 0.20882207 0.21989989 0.24418543][0.3848826 0.3829242 0.35617119 0.32043424 0.28259549 0.25551757 0.24635558 0.25261414 0.26401895 0.27461153 0.28355896 0.28718734 0.28545827 0.28704548 0.30222455][0.3642723 0.36292055 0.34454039 0.32363132 0.30448452 0.29555959 0.29938605 0.31053469 0.31926358 0.32475832 0.32918286 0.32832882 0.320818 0.31495178 0.3206884][0.30140248 0.29940146 0.28849447 0.27979779 0.27520442 0.27951095 0.29171893 0.303957 0.30789685 0.30732459 0.30716243 0.3030712 0.29273385 0.2828415 0.28134426][0.19900033 0.19717361 0.19427919 0.19497783 0.19924642 0.21007206 0.22510518 0.23554648 0.23472814 0.22949372 0.2258866 0.21956721 0.2079028 0.19574288 0.18920633][0.092847228 0.092084795 0.095035508 0.10045412 0.10716058 0.11790067 0.13060178 0.13757385 0.1338563 0.12696089 0.12175463 0.11379423 0.10138915 0.088317581 0.079478346][0.014166903 0.011526002 0.014699684 0.019362934 0.024074567 0.031098047 0.03902043 0.042637255 0.038923152 0.033732262 0.029433606 0.021833736 0.010446852 -0.0012554389 -0.0095550362][-0.035291441 -0.041438568 -0.041972648 -0.040705323 -0.038795657 -0.035171106 -0.030836763 -0.028601933 -0.029837249 -0.031488724 -0.033382114 -0.038807716 -0.047522362 -0.056447841 -0.062715955][-0.059946314 -0.069100454 -0.072880819 -0.074066855 -0.073389746 -0.070960253 -0.067691654 -0.065302752 -0.064439252 -0.064127274 -0.064894058 -0.06852755 -0.074631535 -0.080936342 -0.084846295][-0.0634995 -0.072070077 -0.075052157 -0.074845023 -0.072367013 -0.068905331 -0.065950714 -0.0649273 -0.065917365 -0.0680961 -0.071367286 -0.075796545 -0.080780469 -0.085127771 -0.086689211]]...]
INFO - root - 2017-12-10 18:59:14.157145: step 43610, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.799 sec/batch; 64h:05m:56s remains)
INFO - root - 2017-12-10 18:59:21.822978: step 43620, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 62h:48m:58s remains)
INFO - root - 2017-12-10 18:59:29.709277: step 43630, loss = 0.67, batch loss = 0.61 (10.1 examples/sec; 0.794 sec/batch; 63h:41m:03s remains)
INFO - root - 2017-12-10 18:59:37.658662: step 43640, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.818 sec/batch; 65h:38m:36s remains)
INFO - root - 2017-12-10 18:59:45.427313: step 43650, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.761 sec/batch; 61h:03m:51s remains)
INFO - root - 2017-12-10 18:59:53.309249: step 43660, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 61h:58m:20s remains)
INFO - root - 2017-12-10 19:00:01.167966: step 43670, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 62h:16m:28s remains)
INFO - root - 2017-12-10 19:00:09.055838: step 43680, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 64h:21m:57s remains)
INFO - root - 2017-12-10 19:00:16.808275: step 43690, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 62h:38m:44s remains)
INFO - root - 2017-12-10 19:00:24.701109: step 43700, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.773 sec/batch; 62h:00m:13s remains)
2017-12-10 19:00:25.499967: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.037872154 0.088628024 0.142568 0.19359972 0.2248777 0.22657809 0.20159864 0.16346063 0.12551054 0.094927311 0.087544136 0.10606443 0.14161921 0.18112284 0.20915458][0.035333209 0.089164063 0.14736962 0.20199051 0.23563592 0.23774619 0.21074739 0.16888651 0.12551095 0.0886678 0.07475365 0.088189363 0.12050018 0.15865278 0.18904921][0.030447198 0.086285114 0.14898926 0.20859304 0.24844117 0.25779223 0.23624574 0.19534309 0.14875022 0.10688642 0.0847788 0.088218711 0.11101472 0.14173004 0.16933243][0.025042333 0.082029775 0.14897855 0.21422468 0.26218337 0.28247476 0.27108845 0.23475866 0.1867042 0.14110282 0.11139214 0.1030165 0.11318452 0.13356313 0.15507464][0.023601217 0.082002833 0.15192512 0.22077985 0.27544081 0.30681142 0.30741856 0.27838486 0.23168522 0.18467814 0.14809842 0.12625913 0.12142226 0.12984882 0.14366156][0.025505137 0.085363664 0.15569028 0.22505641 0.28326753 0.32311431 0.33398604 0.31214327 0.26846194 0.22185948 0.17923497 0.14343211 0.12331562 0.1209413 0.12893057][0.026181893 0.0856737 0.15424925 0.223053 0.28433076 0.33148995 0.35164595 0.33782265 0.29928139 0.25422326 0.20612195 0.15689345 0.12203702 0.10991583 0.11315233][0.019168969 0.075829625 0.14157036 0.21080472 0.27703035 0.33195809 0.36123469 0.35621458 0.32441995 0.2814362 0.22864032 0.16799782 0.11991753 0.098301306 0.09633103][0.0075162817 0.060896773 0.12509942 0.19679905 0.2696324 0.33199242 0.36884975 0.3717767 0.34687844 0.30615425 0.24954979 0.18022019 0.12148786 0.0907557 0.082376294][0.0054000705 0.059886795 0.1269168 0.20258775 0.27936715 0.34452787 0.38600335 0.3972328 0.38135731 0.34590486 0.28920746 0.21467488 0.14544663 0.10089202 0.079300776][0.017119912 0.077930227 0.15197925 0.23155116 0.3073816 0.36979493 0.41298571 0.43224263 0.42635587 0.39884272 0.3468976 0.2711108 0.19148621 0.12913288 0.089045383][0.035183765 0.10518463 0.18843576 0.27160639 0.34385407 0.40158778 0.44511646 0.47019187 0.47183749 0.45232877 0.40893885 0.33678547 0.25021458 0.17181939 0.11371084][0.057673562 0.13927855 0.23327065 0.32056105 0.3889623 0.44118366 0.48209968 0.50660127 0.50848705 0.4930895 0.4594714 0.3960802 0.30999061 0.22442444 0.15597457][0.08245609 0.1768574 0.2820918 0.3743718 0.44031557 0.48657793 0.520173 0.535057 0.52682006 0.5071497 0.4791944 0.42674249 0.35014498 0.27152538 0.20740205][0.10014626 0.20471187 0.31951356 0.41775605 0.48409766 0.5255242 0.5481087 0.54519385 0.51677406 0.48293239 0.45312235 0.41055623 0.35161287 0.29482955 0.250976]]...]
INFO - root - 2017-12-10 19:00:33.232719: step 43710, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 65h:16m:20s remains)
INFO - root - 2017-12-10 19:00:41.087465: step 43720, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 61h:52m:16s remains)
INFO - root - 2017-12-10 19:00:48.952839: step 43730, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 63h:23m:20s remains)
INFO - root - 2017-12-10 19:00:56.712972: step 43740, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 62h:37m:27s remains)
INFO - root - 2017-12-10 19:01:04.605101: step 43750, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.773 sec/batch; 62h:01m:33s remains)
INFO - root - 2017-12-10 19:01:12.472304: step 43760, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 63h:56m:05s remains)
INFO - root - 2017-12-10 19:01:20.196705: step 43770, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 62h:16m:37s remains)
INFO - root - 2017-12-10 19:01:28.049803: step 43780, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 64h:11m:48s remains)
INFO - root - 2017-12-10 19:01:35.998338: step 43790, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 63h:27m:22s remains)
INFO - root - 2017-12-10 19:01:43.747517: step 43800, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 61h:37m:42s remains)
2017-12-10 19:01:44.542872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.05936487 -0.061528109 -0.059435856 -0.056855913 -0.052878495 -0.048148438 -0.043151192 -0.038127948 -0.030063882 -0.012326973 0.022755047 0.08019793 0.16543937 0.27554315 0.39195451][-0.062008671 -0.057298128 -0.048156861 -0.039101541 -0.02944153 -0.020117853 -0.010997093 -0.0032967455 0.0044673542 0.018449044 0.048191488 0.10016315 0.18087168 0.28928035 0.40749404][-0.031402741 -0.017052125 0.0017831498 0.020111699 0.037845168 0.0539515 0.068999916 0.07927037 0.083182514 0.085543722 0.097051293 0.12553842 0.18034947 0.2658132 0.36921412][0.032969546 0.063602179 0.098275058 0.1311447 0.16031577 0.18511726 0.20630625 0.21718958 0.2127746 0.19664638 0.18040521 0.17317693 0.18874668 0.23967806 0.32213196][0.12073661 0.17825662 0.23934653 0.29442051 0.33825153 0.37100476 0.39396235 0.399027 0.37899679 0.33805147 0.28963795 0.2441069 0.2192747 0.2360218 0.3011961][0.22018859 0.31079981 0.40407726 0.48335111 0.53944808 0.57403475 0.59051943 0.58226061 0.54127163 0.47461808 0.39815485 0.3232522 0.26950508 0.26438507 0.32353577][0.31161746 0.43215364 0.55383956 0.65194196 0.71336263 0.74181 0.74405104 0.71700406 0.653694 0.56428337 0.46737272 0.37473324 0.30640656 0.29321983 0.35504487][0.37175325 0.5090214 0.64444411 0.74767327 0.80333036 0.81716156 0.79881185 0.75067568 0.66874051 0.56527078 0.45899507 0.3608475 0.29086134 0.27889174 0.34436011][0.38044465 0.5118708 0.63877386 0.72985178 0.76969534 0.76492119 0.7266292 0.66284525 0.5731855 0.47017068 0.36926723 0.27884552 0.21660653 0.20855325 0.27049965][0.33013275 0.43361545 0.532588 0.59958893 0.62127334 0.60240668 0.552776 0.48327371 0.39715618 0.30589256 0.22058418 0.14683695 0.098402612 0.094827183 0.14618221][0.23650382 0.29990616 0.36035025 0.39814651 0.40359178 0.37912458 0.33097059 0.26893902 0.19809498 0.12748456 0.063794881 0.010271439 -0.023493066 -0.024827726 0.010851109][0.13290563 0.1566526 0.17944647 0.19025142 0.18351032 0.16066028 0.12415174 0.080198564 0.033247646 -0.011647469 -0.052107338 -0.08671312 -0.1088974 -0.11095053 -0.091622151][0.055301003 0.05054066 0.047287554 0.040533166 0.027648529 0.0095769586 -0.013564942 -0.0390869 -0.063786462 -0.08612179 -0.10707249 -0.12675847 -0.14090824 -0.14470045 -0.1376413][0.018779412 0.00097925 -0.013220796 -0.025026711 -0.036371835 -0.047427397 -0.059326593 -0.0716592 -0.081941187 -0.090237506 -0.098770417 -0.10843316 -0.11758737 -0.12335876 -0.12485136][0.017610479 -0.0016211587 -0.015997775 -0.024866806 -0.030578604 -0.034374949 -0.037754804 -0.041685551 -0.04446305 -0.046544142 -0.0494133 -0.053634174 -0.059860423 -0.066944554 -0.073508762]]...]
INFO - root - 2017-12-10 19:01:52.427786: step 43810, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.813 sec/batch; 65h:10m:25s remains)
INFO - root - 2017-12-10 19:02:00.333119: step 43820, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 63h:25m:35s remains)
INFO - root - 2017-12-10 19:02:08.279101: step 43830, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 61h:13m:46s remains)
INFO - root - 2017-12-10 19:02:16.212390: step 43840, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 64h:00m:32s remains)
INFO - root - 2017-12-10 19:02:24.001444: step 43850, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 63h:09m:05s remains)
INFO - root - 2017-12-10 19:02:31.858120: step 43860, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 64h:17m:49s remains)
INFO - root - 2017-12-10 19:02:39.834665: step 43870, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 64h:54m:30s remains)
INFO - root - 2017-12-10 19:02:47.636192: step 43880, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 63h:41m:47s remains)
INFO - root - 2017-12-10 19:02:55.364169: step 43890, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 62h:55m:55s remains)
INFO - root - 2017-12-10 19:03:03.239412: step 43900, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 63h:17m:41s remains)
2017-12-10 19:03:04.071378: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10842745 0.10715629 0.10233178 0.095659412 0.093441531 0.099576116 0.1126146 0.12375359 0.12694713 0.1200983 0.10188458 0.076368719 0.053057991 0.043093808 0.046663418][0.10441551 0.099967979 0.092460282 0.083325908 0.078904808 0.084704064 0.098524757 0.11114127 0.1149941 0.10843624 0.090773277 0.0668642 0.046919085 0.041637056 0.050820716][0.11555228 0.10550855 0.090607546 0.073969528 0.062851973 0.065220952 0.079174787 0.094192371 0.10053765 0.095849827 0.080011167 0.0587657 0.042854395 0.043114476 0.058883719][0.14315377 0.12651478 0.10065611 0.072308935 0.051375467 0.048764978 0.062911525 0.081629142 0.092593849 0.091470338 0.077928878 0.0581423 0.043832414 0.046814408 0.0670968][0.17708667 0.15512772 0.11807752 0.077343196 0.046046492 0.037850276 0.051891267 0.074679017 0.091285937 0.094551548 0.083118409 0.063337021 0.048186284 0.05100983 0.073189825][0.20705526 0.18299137 0.13780066 0.087501369 0.047806323 0.034389261 0.047481935 0.073155589 0.094482422 0.10151499 0.091344349 0.070371591 0.052826468 0.053581156 0.075421236][0.22264189 0.20048755 0.15282048 0.098510608 0.05451683 0.037272386 0.048231035 0.074307561 0.09791024 0.10708141 0.097278662 0.074634761 0.05415367 0.051887378 0.07195583][0.21527006 0.19794498 0.15395884 0.1022231 0.059175372 0.040736724 0.049503494 0.074298248 0.098103538 0.10799149 0.098286457 0.074319549 0.051072817 0.045194961 0.062336743][0.18204352 0.17072019 0.13509518 0.091510028 0.054303952 0.037809111 0.045592051 0.06864994 0.091573142 0.10160749 0.09254127 0.068584345 0.043802157 0.034785166 0.04846511][0.13469416 0.12885131 0.1030583 0.069749273 0.040475443 0.027576268 0.035251271 0.056424044 0.07771413 0.087672308 0.080013923 0.05783077 0.033502992 0.022818143 0.033519354][0.087330371 0.085456669 0.068521127 0.044722497 0.023047734 0.013978207 0.021809183 0.040887564 0.059947215 0.069386862 0.063310951 0.043783993 0.021067986 0.0095773526 0.017542103][0.050079476 0.050381128 0.039928369 0.023168238 0.0071888468 0.00099538337 0.00862677 0.025261892 0.041665621 0.050098047 0.045357566 0.028661447 0.0082020173 -0.0033444983 0.0022861157][0.026521711 0.028021935 0.02164891 0.0091660311 -0.0035346909 -0.0083980616 -0.0017422687 0.012035641 0.025512487 0.032558262 0.02872476 0.01465288 -0.0031000373 -0.013752271 -0.0096830167][0.015812377 0.018257661 0.014292214 0.0039227922 -0.0076493812 -0.012832786 -0.0081077367 0.0025004598 0.013047582 0.01858449 0.015231683 0.0033328016 -0.011691435 -0.020843165 -0.017610578][0.01470589 0.018181149 0.015702279 0.006172949 -0.0055985358 -0.012221325 -0.010159417 -0.0028441469 0.0049104523 0.0089532575 0.0057749161 -0.0044288803 -0.017032931 -0.024532896 -0.021655224]]...]
INFO - root - 2017-12-10 19:03:11.901928: step 43910, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 61h:46m:12s remains)
INFO - root - 2017-12-10 19:03:19.720415: step 43920, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 61h:27m:54s remains)
INFO - root - 2017-12-10 19:03:27.303017: step 43930, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 63h:55m:28s remains)
INFO - root - 2017-12-10 19:03:35.092139: step 43940, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 64h:27m:13s remains)
INFO - root - 2017-12-10 19:03:42.941864: step 43950, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 64h:06m:56s remains)
INFO - root - 2017-12-10 19:03:50.841539: step 43960, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 62h:26m:35s remains)
INFO - root - 2017-12-10 19:03:58.784494: step 43970, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 63h:01m:08s remains)
INFO - root - 2017-12-10 19:04:06.496339: step 43980, loss = 0.70, batch loss = 0.65 (9.7 examples/sec; 0.827 sec/batch; 66h:17m:39s remains)
INFO - root - 2017-12-10 19:04:14.400880: step 43990, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 63h:00m:42s remains)
INFO - root - 2017-12-10 19:04:22.264341: step 44000, loss = 0.67, batch loss = 0.61 (10.1 examples/sec; 0.790 sec/batch; 63h:18m:47s remains)
2017-12-10 19:04:23.181466: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30711263 0.39355397 0.472754 0.52961159 0.55663896 0.55951238 0.53229606 0.46200284 0.35547653 0.23825963 0.13167192 0.037652057 -0.038951281 -0.093442112 -0.12624237][0.33520246 0.43274429 0.52211851 0.58557242 0.61563247 0.61936492 0.59091532 0.51588041 0.40169039 0.27438679 0.15707934 0.052859981 -0.03185286 -0.091386832 -0.12624317][0.32273388 0.41892469 0.50779581 0.57151687 0.60411882 0.61455292 0.59507573 0.5291146 0.42188871 0.29684722 0.17689954 0.066808626 -0.024175173 -0.087783068 -0.12432392][0.28500965 0.37193537 0.45418629 0.51705462 0.5570569 0.58394641 0.58586073 0.54241753 0.45306394 0.33624092 0.21469441 0.095770583 -0.0063792192 -0.079012424 -0.1203064][0.23366356 0.30619541 0.3791908 0.44343591 0.49801853 0.55241174 0.5880596 0.57838506 0.51370752 0.40686995 0.28054926 0.14610112 0.025114816 -0.063369527 -0.11363454][0.18653785 0.24563837 0.31263968 0.38348788 0.45837042 0.54503447 0.6168704 0.64082968 0.59736329 0.49595076 0.36011535 0.20567496 0.061413746 -0.046138253 -0.10723827][0.15991853 0.21022429 0.27666041 0.35831723 0.45488271 0.56932116 0.66802186 0.71390891 0.68082875 0.57779348 0.42954072 0.25611353 0.0917892 -0.031764422 -0.101992][0.15702842 0.20458084 0.27435902 0.36573389 0.47630507 0.60356516 0.71050239 0.7602973 0.72535586 0.61559016 0.45700014 0.27308354 0.1001415 -0.029391481 -0.10259518][0.16986567 0.218207 0.28959474 0.38196182 0.49076283 0.60957718 0.70320648 0.74095416 0.69681251 0.58204508 0.422156 0.24241614 0.0773127 -0.044456225 -0.11175188][0.18309924 0.23169646 0.29936817 0.38132221 0.47186258 0.56380743 0.62915748 0.64630008 0.59330082 0.48081371 0.33196619 0.17143287 0.02862877 -0.074031673 -0.12835974][0.18341413 0.23091874 0.2914499 0.3567642 0.42040449 0.4774192 0.51076919 0.50826216 0.4525899 0.35182771 0.22450803 0.091887161 -0.023564087 -0.10447625 -0.14490785][0.17154492 0.2155216 0.26664707 0.3142024 0.35128421 0.37678549 0.38473535 0.37038681 0.32083449 0.24028832 0.1408903 0.037059549 -0.0551516 -0.12075804 -0.15285908][0.16627875 0.20858589 0.25250328 0.28593671 0.30159369 0.30332333 0.29479462 0.27636591 0.23821922 0.18082291 0.10955513 0.028899683 -0.051225673 -0.11417843 -0.14796971][0.18286708 0.22461729 0.26284876 0.28535464 0.28504688 0.27161452 0.25581992 0.24013905 0.21495439 0.17798387 0.12941207 0.063027784 -0.01640789 -0.087884575 -0.13188174][0.218695 0.26035634 0.29385433 0.30882809 0.29865539 0.27846485 0.26256916 0.25217471 0.23618504 0.2112581 0.17568095 0.11429319 0.028810102 -0.055143747 -0.11153062]]...]
INFO - root - 2017-12-10 19:04:30.747920: step 44010, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 61h:34m:14s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 19:04:38.571687: step 44020, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 61h:34m:02s remains)
INFO - root - 2017-12-10 19:04:46.472120: step 44030, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 65h:16m:40s remains)
INFO - root - 2017-12-10 19:04:54.424266: step 44040, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.799 sec/batch; 63h:58m:57s remains)
INFO - root - 2017-12-10 19:05:02.242108: step 44050, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 62h:25m:22s remains)
INFO - root - 2017-12-10 19:05:10.018173: step 44060, loss = 0.70, batch loss = 0.64 (12.2 examples/sec; 0.657 sec/batch; 52h:38m:44s remains)
INFO - root - 2017-12-10 19:05:17.847538: step 44070, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 62h:18m:09s remains)
INFO - root - 2017-12-10 19:05:25.647887: step 44080, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 64h:27m:03s remains)
INFO - root - 2017-12-10 19:05:33.220704: step 44090, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 63h:35m:35s remains)
INFO - root - 2017-12-10 19:05:41.119328: step 44100, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 61h:36m:33s remains)
2017-12-10 19:05:41.951562: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.001510378 -0.01666615 -0.026331577 -0.016324148 0.0094182137 0.034165088 0.044284638 0.03836466 0.027204297 0.020472974 0.019364905 0.01485099 0.00064202334 -0.021565484 -0.044762995][0.036405846 0.00876455 -0.0096865771 0.0025601417 0.0405678 0.079632409 0.099544227 0.096362457 0.083349347 0.074341178 0.071149223 0.0613983 0.03695865 0.0013145847 -0.035456292][0.0854554 0.04537937 0.014920154 0.028596241 0.079269581 0.1339523 0.16698903 0.17187682 0.16140293 0.15148686 0.14479165 0.12688795 0.088929892 0.03748814 -0.014936723][0.14852275 0.092410162 0.047489032 0.061409105 0.12373322 0.19377004 0.24189197 0.25900671 0.25504139 0.24511601 0.231742 0.19979273 0.14201733 0.071335591 0.0035051615][0.21642886 0.14507703 0.088587828 0.10487772 0.17937891 0.26507014 0.32965422 0.36117047 0.36530706 0.3542237 0.32930452 0.27581447 0.19126976 0.097666085 0.015203805][0.27245843 0.19112773 0.12926304 0.15055007 0.2361349 0.33564767 0.41644588 0.46309069 0.47557729 0.46225312 0.42350045 0.34654805 0.2351463 0.11951124 0.024196176][0.30492213 0.22251728 0.16404425 0.19323963 0.28838557 0.39909559 0.49428102 0.554005 0.57055277 0.54959708 0.49362013 0.3945823 0.26206326 0.13150489 0.029357385][0.31273812 0.24346541 0.19995835 0.23920161 0.33918852 0.45417646 0.55717093 0.62469393 0.64020514 0.60627931 0.52969742 0.40993017 0.26233628 0.12490166 0.023370469][0.30825415 0.26673451 0.24796163 0.29661277 0.39247042 0.49849004 0.59435636 0.65679592 0.66357327 0.61498332 0.52207255 0.38999686 0.23732187 0.10147524 0.0070933765][0.29192361 0.28730172 0.2979283 0.35408375 0.43814272 0.5226379 0.59497511 0.63694119 0.62739658 0.56633866 0.46785119 0.33846411 0.1952187 0.07109762 -0.0112771][0.26889655 0.29007676 0.3178542 0.3704446 0.43407616 0.48994657 0.53202564 0.54833269 0.52279544 0.45712662 0.36638048 0.25513333 0.13504343 0.032055873 -0.034237541][0.23451343 0.25931454 0.28367734 0.31864721 0.35601076 0.38437226 0.40164334 0.39971223 0.36652076 0.3051869 0.23011994 0.14425012 0.054369442 -0.02077868 -0.065867484][0.17313795 0.18430409 0.18976979 0.19777749 0.20647189 0.21169126 0.21449316 0.20871271 0.18338667 0.13982913 0.088781454 0.032254938 -0.025876451 -0.072266236 -0.0957614][0.082839027 0.077807575 0.065705642 0.052318729 0.039805796 0.029419774 0.024796108 0.021795619 0.011160153 -0.0083982749 -0.031757958 -0.058843013 -0.087546527 -0.10830873 -0.1133678][-0.012578126 -0.025326822 -0.041958779 -0.060528617 -0.078535818 -0.092999183 -0.099732228 -0.10065553 -0.10174537 -0.10531835 -0.11010685 -0.11697103 -0.12458578 -0.12674242 -0.11892331]]...]
INFO - root - 2017-12-10 19:05:49.731792: step 44110, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 62h:58m:09s remains)
INFO - root - 2017-12-10 19:05:57.598259: step 44120, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 64h:55m:29s remains)
INFO - root - 2017-12-10 19:06:05.437069: step 44130, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 64h:03m:48s remains)
INFO - root - 2017-12-10 19:06:13.292169: step 44140, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 64h:30m:18s remains)
INFO - root - 2017-12-10 19:06:20.905208: step 44150, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 62h:51m:23s remains)
INFO - root - 2017-12-10 19:06:28.845993: step 44160, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 61h:04m:09s remains)
INFO - root - 2017-12-10 19:06:36.414669: step 44170, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.747 sec/batch; 59h:49m:20s remains)
INFO - root - 2017-12-10 19:06:44.244369: step 44180, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 63h:50m:43s remains)
INFO - root - 2017-12-10 19:06:52.100833: step 44190, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 63h:28m:39s remains)
INFO - root - 2017-12-10 19:06:59.909977: step 44200, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 63h:54m:26s remains)
2017-12-10 19:07:00.690724: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18196094 0.21182276 0.22820024 0.22269538 0.19352767 0.1483734 0.12285344 0.12624744 0.15744084 0.19959839 0.22460222 0.21308193 0.16193682 0.092551485 0.026116472][0.24222282 0.27727374 0.29699153 0.29013062 0.25342804 0.19843899 0.16761713 0.16999735 0.19940811 0.23769239 0.25993207 0.24644867 0.18827923 0.10713052 0.03023288][0.30763215 0.34470832 0.36276224 0.34968963 0.30124307 0.23570558 0.20187742 0.206452 0.23693626 0.27239791 0.29143208 0.27489102 0.2100593 0.11940702 0.034019869][0.36524311 0.40208003 0.41801766 0.40020376 0.34242094 0.26984632 0.23679774 0.2465976 0.27958146 0.31035876 0.32117054 0.29635543 0.22422065 0.12681612 0.035732768][0.41229802 0.44538021 0.45868436 0.44187868 0.38465622 0.31566676 0.28976429 0.30614588 0.33842275 0.35740644 0.35102639 0.310503 0.22890304 0.12716977 0.033712648][0.44877291 0.47132447 0.47788143 0.4647373 0.4166036 0.35927695 0.3430827 0.36357075 0.389847 0.39106387 0.36246607 0.30394331 0.21447924 0.11294029 0.022745347][0.46325868 0.47296852 0.47134745 0.46536383 0.43439344 0.3940374 0.38696772 0.40689752 0.42269361 0.40382779 0.35252351 0.27776954 0.18369231 0.0864763 0.0035739138][0.45055282 0.44918653 0.44349763 0.45042846 0.44263524 0.4225218 0.42390877 0.43987361 0.44290411 0.4043248 0.33234397 0.24378766 0.14621136 0.054276522 -0.020189714][0.41430527 0.40539566 0.40187508 0.42595991 0.44313014 0.44348636 0.45311818 0.46559671 0.45758322 0.40320709 0.3150619 0.21567604 0.11533048 0.027245408 -0.040609147][0.35770682 0.34534344 0.3473202 0.38747504 0.42653435 0.44578397 0.46525416 0.47876534 0.46689072 0.40569374 0.30901033 0.20295653 0.099641144 0.012337624 -0.052645966][0.2923826 0.28081208 0.2884683 0.33789352 0.38963428 0.42121783 0.44821954 0.46489468 0.455027 0.39659512 0.30097297 0.19407713 0.08995492 0.0032266008 -0.05948415][0.2335283 0.22328749 0.23363423 0.28446928 0.34029409 0.37946704 0.41309303 0.4343859 0.42890009 0.37741393 0.28825366 0.18495812 0.083192661 -0.0015688936 -0.061609767][0.18305796 0.17324883 0.18401387 0.23136646 0.28716585 0.33313602 0.37507138 0.40237251 0.4005574 0.35538113 0.27478206 0.17874317 0.082100362 0.0002109604 -0.05753528][0.14566866 0.13847274 0.15048954 0.19505285 0.25201982 0.30567878 0.3556425 0.38642257 0.38357413 0.33892229 0.26340377 0.1740755 0.083129048 0.0045802384 -0.05098946][0.13713154 0.13262896 0.14560865 0.18825571 0.24756892 0.30887946 0.36578712 0.39830393 0.39152551 0.34169692 0.26411903 0.1752248 0.0857854 0.0086231194 -0.045859933]]...]
INFO - root - 2017-12-10 19:07:08.540223: step 44210, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 62h:45m:17s remains)
INFO - root - 2017-12-10 19:07:16.458583: step 44220, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 62h:04m:31s remains)
INFO - root - 2017-12-10 19:07:24.157791: step 44230, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.773 sec/batch; 61h:54m:10s remains)
INFO - root - 2017-12-10 19:07:31.749991: step 44240, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 64h:26m:24s remains)
INFO - root - 2017-12-10 19:07:38.961840: step 44250, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 62h:15m:59s remains)
INFO - root - 2017-12-10 19:07:46.822784: step 44260, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 61h:32m:14s remains)
INFO - root - 2017-12-10 19:07:54.596800: step 44270, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 62h:17m:18s remains)
INFO - root - 2017-12-10 19:08:02.496422: step 44280, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 64h:18m:18s remains)
INFO - root - 2017-12-10 19:08:10.275782: step 44290, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.777 sec/batch; 62h:13m:49s remains)
INFO - root - 2017-12-10 19:08:18.147170: step 44300, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.819 sec/batch; 65h:33m:07s remains)
2017-12-10 19:08:18.961103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.055663366 -0.05929435 -0.061348379 -0.063586175 -0.066031694 -0.069104329 -0.071562894 -0.071862862 -0.06927602 -0.065278217 -0.061686676 -0.059366021 -0.058902796 -0.059955049 -0.062584467][-0.042191435 -0.045992531 -0.049017575 -0.051563829 -0.054569568 -0.060419191 -0.06677188 -0.069270022 -0.065721 -0.058581598 -0.051248912 -0.045630138 -0.043427374 -0.045954753 -0.052802261][-0.0095828669 -0.012362728 -0.014877757 -0.014588011 -0.014997984 -0.022203341 -0.033444796 -0.040253405 -0.037130389 -0.026733922 -0.013903311 -0.0028763211 0.0013584631 -0.0060434425 -0.023056254][0.04871707 0.053865511 0.058732133 0.068771295 0.075645767 0.067763187 0.047830857 0.03039882 0.027526608 0.038580842 0.056653347 0.072737135 0.076479636 0.05771799 0.022269959][0.12946823 0.15123495 0.17220972 0.19952007 0.21887001 0.21268392 0.18175593 0.14754713 0.13129577 0.13667877 0.1543587 0.1696723 0.16720359 0.13128361 0.072216831][0.21684293 0.26065326 0.30293274 0.350848 0.38494357 0.38308448 0.34241861 0.29032949 0.25627348 0.24854894 0.2566075 0.26185986 0.24711418 0.19284059 0.11240979][0.289245 0.3534745 0.4128218 0.47346181 0.51547724 0.51744795 0.47278091 0.41015288 0.36170936 0.33827811 0.32991305 0.31811988 0.28874779 0.22138797 0.12967487][0.32681575 0.40263459 0.46719041 0.52519572 0.56235242 0.56420535 0.52181846 0.45916936 0.40478355 0.3696388 0.346354 0.31968239 0.2804755 0.21027373 0.12023449][0.31286696 0.38685605 0.4444634 0.48842421 0.51237869 0.51132405 0.47578126 0.42223442 0.3716462 0.33318049 0.30304715 0.27063519 0.23145838 0.17020677 0.093619369][0.24531336 0.30342263 0.34612036 0.3742587 0.38681412 0.38536742 0.36105523 0.32312384 0.28447115 0.25161314 0.22372971 0.19525754 0.16482028 0.12043577 0.064488634][0.15123437 0.18723196 0.2144248 0.23249783 0.24104942 0.24308671 0.23159894 0.21103755 0.18834794 0.16664627 0.14632051 0.12564191 0.10543201 0.078057155 0.042930365][0.068237409 0.085109077 0.10124953 0.1161508 0.12732163 0.13499762 0.1343026 0.12724474 0.11771985 0.10576676 0.091823585 0.077067129 0.064273626 0.049958415 0.031941984][0.023765463 0.030213596 0.041928165 0.058961015 0.0752525 0.086410709 0.089759462 0.087301455 0.082837038 0.075142585 0.064449288 0.053294696 0.044983562 0.039010622 0.032543693][0.02350761 0.028244294 0.039695177 0.059252176 0.077650912 0.086809568 0.08612334 0.078700721 0.070214555 0.060678989 0.050749332 0.042795978 0.038928293 0.039531641 0.041300703][0.049744975 0.055579904 0.066068351 0.084014349 0.098881461 0.10119802 0.092084251 0.075319536 0.058468953 0.044613168 0.035790253 0.0333043 0.036627147 0.045016371 0.054241251]]...]
INFO - root - 2017-12-10 19:08:26.872278: step 44310, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 63h:24m:22s remains)
INFO - root - 2017-12-10 19:08:34.665910: step 44320, loss = 0.67, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 62h:39m:23s remains)
INFO - root - 2017-12-10 19:08:42.179545: step 44330, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 63h:21m:38s remains)
INFO - root - 2017-12-10 19:08:50.007450: step 44340, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 61h:59m:29s remains)
INFO - root - 2017-12-10 19:08:57.946597: step 44350, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 63h:22m:23s remains)
INFO - root - 2017-12-10 19:09:05.712743: step 44360, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 63h:58m:12s remains)
INFO - root - 2017-12-10 19:09:13.615911: step 44370, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 64h:14m:40s remains)
INFO - root - 2017-12-10 19:09:21.435936: step 44380, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 62h:34m:48s remains)
INFO - root - 2017-12-10 19:09:29.295368: step 44390, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 64h:40m:42s remains)
INFO - root - 2017-12-10 19:09:37.260026: step 44400, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 62h:52m:15s remains)
2017-12-10 19:09:38.101039: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.047184426 0.04346985 0.035744354 0.028395761 0.023684515 0.021906761 0.022554293 0.022615978 0.020442402 0.016449219 0.012073014 0.0080143716 0.0039944956 7.5573924e-05 -0.0022496872][0.058727078 0.058725625 0.053525019 0.046917 0.040995385 0.036531582 0.034500346 0.032654718 0.02884488 0.023318473 0.01794688 0.013010232 0.0076949066 0.0019017746 -0.0022245846][0.063223444 0.06749054 0.065842435 0.060699984 0.053816784 0.046610497 0.041728463 0.038104959 0.033404443 0.027550023 0.022266125 0.017200463 0.010926449 0.0031389266 -0.0032545549][0.05927873 0.066955619 0.068944581 0.0661058 0.059495006 0.050892793 0.0444833 0.040561091 0.036893919 0.032348637 0.02813429 0.023454282 0.016448749 0.0067407135 -0.0025544225][0.049162198 0.058377713 0.062955186 0.062565491 0.057434898 0.049220033 0.043214839 0.04093454 0.040398382 0.039017305 0.037003245 0.033063393 0.025329178 0.0137416 0.0011791316][0.040003031 0.048887338 0.053953614 0.054570764 0.050565396 0.043490224 0.039085764 0.039605126 0.043426041 0.046689127 0.048037767 0.045372695 0.036998291 0.023253145 0.0071571618][0.04142436 0.048845064 0.052155346 0.051308863 0.046289664 0.038799219 0.034838982 0.037386522 0.044998851 0.052980952 0.058235835 0.05757574 0.048994906 0.033396166 0.013913861][0.0579126 0.064025514 0.064049579 0.059499312 0.050676491 0.039317437 0.032050181 0.032992907 0.041125908 0.051431756 0.059675086 0.061492864 0.054009128 0.038409255 0.017682511][0.087361686 0.093354695 0.089580081 0.079817384 0.064751193 0.045908395 0.031221716 0.026601216 0.031675875 0.041539267 0.051712044 0.056974366 0.053105067 0.040658616 0.022058757][0.12116987 0.12867035 0.12165449 0.10618696 0.083946474 0.056491725 0.032974984 0.020814987 0.020174101 0.027095294 0.037903987 0.047152545 0.049004208 0.0425291 0.029267352][0.14747171 0.15646406 0.14704227 0.12675749 0.098571509 0.064769447 0.035175033 0.017320938 0.010985252 0.013587443 0.023401933 0.035886806 0.044381816 0.046495624 0.041793][0.15741135 0.16544321 0.15356281 0.12986237 0.098309055 0.062298723 0.031737905 0.01293843 0.0040195012 0.0033858949 0.01184733 0.026631521 0.041881006 0.054244645 0.060503177][0.15061787 0.15451393 0.1393856 0.11334323 0.081208549 0.047295373 0.020859214 0.0061748452 -0.00082033541 -0.0015287667 0.0068219132 0.023436101 0.0446703 0.06711638 0.085007347][0.1343638 0.13226621 0.11334249 0.0855617 0.054866884 0.026114916 0.0071258112 -0.00024489785 -0.0018588456 0.00066590408 0.010825878 0.029385805 0.055079442 0.085720122 0.11417423][0.11392397 0.10566883 0.083328731 0.054681495 0.02681436 0.00474072 -0.0057277232 -0.0054117544 -0.0012186528 0.0051579927 0.017100232 0.036468219 0.06401787 0.10004877 0.13723668]]...]
INFO - root - 2017-12-10 19:09:45.687580: step 44410, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 62h:54m:36s remains)
INFO - root - 2017-12-10 19:09:53.385771: step 44420, loss = 0.70, batch loss = 0.64 (9.5 examples/sec; 0.841 sec/batch; 67h:17m:33s remains)
INFO - root - 2017-12-10 19:10:01.241060: step 44430, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 62h:08m:59s remains)
INFO - root - 2017-12-10 19:10:09.041558: step 44440, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 62h:37m:29s remains)
INFO - root - 2017-12-10 19:10:16.827572: step 44450, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 62h:39m:48s remains)
INFO - root - 2017-12-10 19:10:24.694161: step 44460, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.805 sec/batch; 64h:24m:40s remains)
INFO - root - 2017-12-10 19:10:32.630455: step 44470, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.826 sec/batch; 66h:05m:50s remains)
INFO - root - 2017-12-10 19:10:40.521420: step 44480, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 60h:51m:19s remains)
INFO - root - 2017-12-10 19:10:48.151908: step 44490, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 63h:16m:52s remains)
INFO - root - 2017-12-10 19:10:55.924361: step 44500, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 63h:38m:08s remains)
2017-12-10 19:10:56.805901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.091703638 -0.093095891 -0.091746867 -0.092575833 -0.0958965 -0.10052372 -0.10492613 -0.10859419 -0.11149044 -0.11360137 -0.11507479 -0.1164877 -0.11701509 -0.11548362 -0.11145374][-0.08996816 -0.092560261 -0.093752444 -0.098753974 -0.10621905 -0.11163206 -0.11255468 -0.10997111 -0.10603783 -0.10348154 -0.10359475 -0.10751408 -0.1129388 -0.115926 -0.11393382][-0.086458787 -0.089634873 -0.093809344 -0.10302662 -0.11265066 -0.11331885 -0.10012503 -0.0770826 -0.052371431 -0.034943428 -0.029503653 -0.038120884 -0.056327932 -0.076080769 -0.089488834][-0.077613629 -0.076507568 -0.077934869 -0.084989108 -0.089976445 -0.078576706 -0.040456593 0.018739175 0.081570871 0.1266779 0.14340046 0.12882932 0.089480512 0.036570173 -0.01361522][-0.051956397 -0.032347053 -0.013633751 -0.00031586457 0.013321084 0.043499243 0.10973433 0.21175334 0.32001111 0.39522341 0.41887563 0.39029151 0.31875589 0.21577592 0.10790794][-0.0015628128 0.056258425 0.11815064 0.17464617 0.22292721 0.27576703 0.36390895 0.50188959 0.64822817 0.74261725 0.75761831 0.70011306 0.58514506 0.42274812 0.24748237][0.071596853 0.18418579 0.30758375 0.42218471 0.51241404 0.58320439 0.67463237 0.82343435 0.981322 1.0712781 1.0573152 0.95807731 0.79862326 0.58671594 0.35775685][0.14810839 0.31722158 0.49975505 0.6628409 0.78041214 0.84884977 0.91392887 1.0364499 1.1706419 1.2330694 1.180825 1.0430477 0.85603213 0.62639236 0.38223681][0.19249785 0.39502183 0.60564089 0.78139395 0.89391017 0.93473333 0.95042211 1.0180624 1.1034573 1.1290289 1.048823 0.8954494 0.71248919 0.50644463 0.293963][0.17682455 0.37318653 0.56624973 0.71140337 0.78797597 0.78831738 0.75366628 0.76162905 0.79365122 0.78868324 0.70410132 0.56712258 0.42000479 0.2691792 0.12173282][0.10652786 0.26003462 0.39891934 0.48581833 0.51404417 0.48134768 0.41847602 0.38739932 0.38290241 0.36532268 0.3008047 0.20517673 0.11122213 0.024939606 -0.051384989][0.015502763 0.1102553 0.18438844 0.21257758 0.20094804 0.1538588 0.090513237 0.049300309 0.031887975 0.017815955 -0.015375714 -0.063132614 -0.10597502 -0.13906918 -0.16092844][-0.062420338 -0.020542901 0.0023367996 -0.0073373034 -0.037349254 -0.079687528 -0.12477791 -0.15562896 -0.16948152 -0.17430288 -0.18183726 -0.19392073 -0.20236434 -0.20402171 -0.19687809][-0.10849533 -0.10101564 -0.10512954 -0.12672742 -0.15479276 -0.18183878 -0.20461027 -0.21907777 -0.22400843 -0.22150014 -0.21620736 -0.21141636 -0.20518053 -0.19570827 -0.18100239][-0.12099876 -0.12790012 -0.13839635 -0.15539809 -0.17226735 -0.18425576 -0.19108114 -0.19359119 -0.1921338 -0.18698664 -0.17953952 -0.17195041 -0.16425022 -0.1555568 -0.14476226]]...]
INFO - root - 2017-12-10 19:11:04.402308: step 44510, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.754 sec/batch; 60h:20m:07s remains)
INFO - root - 2017-12-10 19:11:12.286573: step 44520, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 62h:57m:57s remains)
INFO - root - 2017-12-10 19:11:20.094137: step 44530, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 62h:32m:08s remains)
INFO - root - 2017-12-10 19:11:27.907833: step 44540, loss = 0.70, batch loss = 0.65 (10.7 examples/sec; 0.745 sec/batch; 59h:33m:21s remains)
INFO - root - 2017-12-10 19:11:35.686169: step 44550, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 62h:32m:45s remains)
INFO - root - 2017-12-10 19:11:43.523807: step 44560, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 63h:53m:57s remains)
INFO - root - 2017-12-10 19:11:51.266179: step 44570, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 62h:53m:54s remains)
INFO - root - 2017-12-10 19:11:59.241685: step 44580, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 61h:04m:51s remains)
INFO - root - 2017-12-10 19:12:07.100116: step 44590, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 62h:09m:38s remains)
INFO - root - 2017-12-10 19:12:14.884349: step 44600, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 62h:46m:55s remains)
2017-12-10 19:12:15.776621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.049500555 -0.049859986 -0.049893711 -0.05261125 -0.057727732 -0.063470222 -0.068850607 -0.073294379 -0.076190256 -0.07708215 -0.076951206 -0.07652206 -0.076709293 -0.078027293 -0.08094909][-0.020950735 -0.015215375 -0.0094593642 -0.0078507122 -0.010751616 -0.016998062 -0.025643555 -0.035714556 -0.044626176 -0.050999932 -0.055050664 -0.057196803 -0.059389628 -0.063960657 -0.07224416][0.025227409 0.041707538 0.058888666 0.071253024 0.076150753 0.072675027 0.06130556 0.043253351 0.023041112 0.0057596737 -0.0070616296 -0.014582849 -0.02076095 -0.030914899 -0.04758979][0.078078069 0.1088775 0.14245348 0.17191556 0.19087198 0.19472788 0.18255875 0.15551332 0.1197266 0.085487679 0.05881891 0.042704295 0.030602762 0.012770077 -0.015093777][0.12989911 0.17706193 0.22977024 0.27995673 0.3166523 0.33132708 0.32069752 0.28465343 0.2306973 0.17567793 0.13187809 0.10555917 0.086966708 0.060885582 0.020896852][0.1777104 0.24078743 0.31012192 0.3772372 0.42801034 0.45218173 0.4440344 0.40081793 0.33059746 0.25743556 0.20000923 0.16684489 0.14420196 0.11130419 0.060055781][0.21414012 0.28894871 0.36751962 0.4419969 0.4983398 0.52681047 0.52024549 0.4728885 0.39273262 0.3099657 0.24763261 0.21470551 0.1927397 0.15680201 0.097641475][0.2315376 0.31256771 0.39268172 0.46459395 0.51625454 0.54076 0.53113037 0.48090744 0.39867952 0.31749958 0.261646 0.23729289 0.22152606 0.18726468 0.12570202][0.2259919 0.30736032 0.3823193 0.443637 0.48143625 0.49281931 0.47363576 0.42090926 0.34585872 0.27883002 0.23992848 0.23014872 0.22467595 0.19744715 0.14032005][0.1971218 0.27189234 0.33550033 0.38055563 0.39927793 0.39273465 0.36166763 0.30922508 0.2495387 0.20564871 0.1887812 0.19447565 0.1997107 0.18328916 0.13735624][0.14445105 0.20486958 0.25183442 0.27820137 0.27856851 0.2565574 0.21790969 0.17143779 0.13173474 0.11208809 0.11433472 0.13067654 0.14323072 0.13804142 0.10783147][0.071564473 0.11153167 0.14007314 0.15098219 0.14100592 0.11412563 0.077927209 0.04291112 0.020918412 0.016955703 0.027416479 0.045194291 0.058976561 0.061609093 0.046971079][-0.0025583718 0.016814865 0.029710554 0.031217303 0.019649407 -0.0020744067 -0.0283681 -0.051143505 -0.0627042 -0.062098019 -0.05348403 -0.04128734 -0.030885732 -0.024909958 -0.02776411][-0.059120432 -0.055989016 -0.054351889 -0.058253236 -0.068297982 -0.082907595 -0.099219359 -0.1126644 -0.1191552 -0.11891992 -0.114824 -0.10869115 -0.10203692 -0.095251791 -0.091128349][-0.091696642 -0.098718911 -0.10367066 -0.11031397 -0.11864459 -0.12803252 -0.13740666 -0.14467271 -0.14804202 -0.14782429 -0.14587323 -0.14285249 -0.13867715 -0.13273446 -0.12634841]]...]
INFO - root - 2017-12-10 19:12:23.612580: step 44610, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 64h:21m:03s remains)
INFO - root - 2017-12-10 19:12:31.437444: step 44620, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 62h:38m:15s remains)
INFO - root - 2017-12-10 19:12:39.278425: step 44630, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 61h:55m:06s remains)
INFO - root - 2017-12-10 19:12:47.209736: step 44640, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 62h:40m:24s remains)
INFO - root - 2017-12-10 19:12:54.875283: step 44650, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 63h:44m:52s remains)
INFO - root - 2017-12-10 19:13:02.716751: step 44660, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 61h:22m:36s remains)
INFO - root - 2017-12-10 19:13:10.513155: step 44670, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 61h:24m:10s remains)
INFO - root - 2017-12-10 19:13:18.333001: step 44680, loss = 0.70, batch loss = 0.65 (9.7 examples/sec; 0.825 sec/batch; 65h:57m:48s remains)
INFO - root - 2017-12-10 19:13:25.969936: step 44690, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 62h:45m:59s remains)
INFO - root - 2017-12-10 19:13:33.818275: step 44700, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.810 sec/batch; 64h:46m:03s remains)
2017-12-10 19:13:34.649426: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.028620472 0.026667068 0.023307107 0.020012042 0.017697897 0.01746341 0.018964456 0.020631658 0.021888262 0.022247881 0.02171539 0.020828424 0.01990913 0.019782547 0.020437269][0.0069735073 0.0054563018 0.0026061938 -0.00022941375 -0.0024754722 -0.0027777327 -0.0010306881 0.0019853276 0.0055592158 0.0085428637 0.010510153 0.011530736 0.01113184 0.009949265 0.00871725][0.01266718 0.013139828 0.010978638 0.007260411 0.002423543 -0.0013030173 -0.0024669741 0.0003140035 0.005763446 0.011316401 0.016212422 0.020434435 0.022038881 0.01999896 0.015401335][0.049517184 0.05517216 0.055320606 0.050350521 0.040434368 0.029000364 0.020665502 0.02111041 0.02795054 0.03580144 0.043531146 0.0517102 0.056558833 0.053517796 0.043311916][0.11061304 0.12516232 0.1298209 0.12432465 0.10805286 0.085398838 0.065899707 0.06190655 0.06971845 0.080041215 0.09117458 0.104129 0.11262082 0.10697334 0.087675877][0.17380317 0.19842686 0.20814362 0.20296043 0.18130228 0.14781101 0.11695769 0.10800208 0.11624486 0.1287806 0.14342839 0.16110532 0.17268828 0.16307399 0.13293742][0.21050623 0.24178018 0.25434977 0.24950539 0.22524221 0.18586515 0.14844535 0.1365325 0.14512607 0.15916972 0.17621416 0.1965536 0.20921354 0.19559227 0.15697752][0.19988446 0.23100917 0.24285986 0.23792423 0.2145507 0.17645021 0.13993849 0.12859064 0.13779998 0.1524788 0.17001738 0.18978982 0.20100038 0.18515037 0.14449516][0.14324455 0.16796316 0.17673643 0.1720507 0.15306003 0.12272846 0.093721882 0.085454792 0.094481371 0.10795466 0.12343825 0.13988888 0.14860515 0.13440752 0.099999554][0.068004206 0.0832992 0.088533439 0.084859163 0.072271526 0.052952953 0.034648448 0.030295895 0.038078707 0.048787456 0.060465962 0.072390229 0.079004556 0.069938473 0.047412481][0.0071217921 0.013970839 0.01655856 0.014181136 0.0074187894 -0.0020479555 -0.010671627 -0.011737183 -0.0055474942 0.0023890072 0.010410147 0.018511415 0.024069864 0.021284711 0.011238179][-0.022982042 -0.021443013 -0.020288588 -0.02159569 -0.024569569 -0.027720446 -0.030045809 -0.029215695 -0.024454296 -0.01843683 -0.012895449 -0.0071681156 -0.0017989889 0.00055835204 -0.00026287843][-0.025469845 -0.026303152 -0.025943454 -0.026817853 -0.028004138 -0.028084958 -0.02714254 -0.025233584 -0.021134023 -0.015871946 -0.01155597 -0.0072880904 -0.0024325759 0.0021934453 0.0054825819][-0.015657166 -0.017168662 -0.017458718 -0.018493826 -0.019230848 -0.018306108 -0.016046418 -0.013435711 -0.00938888 -0.0042606243 -0.00052289286 0.0026259869 0.0064079 0.010920585 0.014428847][-0.00854963 -0.010050338 -0.010546009 -0.011626534 -0.012309409 -0.01130313 -0.0088291308 -0.0059381565 -0.0019339969 0.0030288135 0.006503318 0.0089151766 0.011538861 0.014853673 0.016826695]]...]
INFO - root - 2017-12-10 19:13:42.580422: step 44710, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.816 sec/batch; 65h:14m:21s remains)
INFO - root - 2017-12-10 19:13:50.356970: step 44720, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 60h:59m:03s remains)
INFO - root - 2017-12-10 19:13:57.979200: step 44730, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 62h:09m:51s remains)
INFO - root - 2017-12-10 19:14:05.743572: step 44740, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 63h:58m:25s remains)
INFO - root - 2017-12-10 19:14:13.537840: step 44750, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 63h:15m:00s remains)
INFO - root - 2017-12-10 19:14:21.300031: step 44760, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 61h:07m:25s remains)
INFO - root - 2017-12-10 19:14:29.041680: step 44770, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 61h:36m:52s remains)
INFO - root - 2017-12-10 19:14:36.759315: step 44780, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 61h:17m:19s remains)
INFO - root - 2017-12-10 19:14:44.613225: step 44790, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 62h:15m:01s remains)
INFO - root - 2017-12-10 19:14:52.450183: step 44800, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 63h:40m:53s remains)
2017-12-10 19:14:53.323234: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.469733 0.48154196 0.50052172 0.50097746 0.46788153 0.41782022 0.39353457 0.40517914 0.43596554 0.46365559 0.47957122 0.46931586 0.41976056 0.34664407 0.28102204][0.60766155 0.61494035 0.617689 0.59279335 0.53326404 0.46858361 0.44913942 0.48054403 0.53360361 0.57576972 0.59197682 0.56545264 0.48879963 0.38645911 0.29726571][0.68815982 0.69158256 0.68090284 0.64120793 0.57244104 0.51106864 0.50644684 0.55757815 0.62307048 0.66391438 0.66456193 0.61133587 0.50567442 0.37814972 0.27226594][0.68446833 0.69339919 0.68832445 0.66068542 0.60964763 0.56980425 0.58300126 0.64390725 0.7038185 0.72391617 0.69494313 0.61083311 0.48149866 0.33901149 0.22555877][0.59613568 0.62009865 0.64173049 0.65247816 0.64316332 0.63978004 0.67293912 0.73559147 0.77780831 0.7672022 0.704777 0.59300882 0.44683108 0.2968719 0.18128571][0.44433907 0.48934984 0.54956108 0.61046273 0.65206385 0.68896633 0.74048191 0.80145359 0.82610828 0.79065359 0.70344508 0.57403129 0.41962269 0.26921412 0.15718637][0.27462292 0.33875284 0.43240297 0.53410918 0.61739659 0.68875748 0.75851953 0.824054 0.84489024 0.80300486 0.70718384 0.57111537 0.41459775 0.26814705 0.16419528][0.13618749 0.20780565 0.3136341 0.43138039 0.53615862 0.63140965 0.72096962 0.80266374 0.836581 0.80490011 0.71263343 0.57667923 0.4219217 0.28324363 0.19122745][0.056537066 0.12185282 0.21781585 0.32778889 0.43523246 0.54314727 0.64982307 0.74864113 0.79696661 0.77687478 0.69033653 0.55818933 0.41212577 0.29019234 0.21723221][0.0397289 0.091837458 0.16722257 0.25747809 0.35566816 0.46367702 0.57491964 0.67632413 0.72404969 0.70361441 0.61935806 0.49621677 0.36895522 0.2726756 0.22324787][0.083891422 0.12668446 0.18211469 0.24864246 0.32560596 0.41269863 0.5016911 0.57731438 0.60148108 0.56760043 0.48516035 0.37885061 0.28006995 0.21569145 0.19139536][0.19355576 0.2329952 0.27050471 0.30954084 0.35198113 0.39650038 0.43738452 0.46278372 0.44697791 0.39287716 0.31361288 0.2280384 0.15866508 0.12207893 0.11647131][0.33997083 0.37664211 0.3971622 0.40681472 0.40725666 0.39834237 0.38045657 0.34915707 0.29314545 0.22202253 0.14819565 0.081796117 0.035063043 0.0160738 0.020130647][0.47160494 0.50534463 0.50878263 0.48972651 0.44896689 0.38971743 0.32000458 0.24384576 0.16161317 0.084811 0.021779787 -0.025342751 -0.053320941 -0.060035784 -0.049854234][0.55052972 0.574831 0.5596469 0.51468772 0.4409149 0.34590942 0.24412856 0.14646436 0.059586771 -0.0070171361 -0.050613847 -0.075355813 -0.084801272 -0.080492117 -0.064871907]]...]
INFO - root - 2017-12-10 19:15:01.032346: step 44810, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 63h:40m:01s remains)
INFO - root - 2017-12-10 19:15:08.806468: step 44820, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 62h:39m:29s remains)
INFO - root - 2017-12-10 19:15:16.692589: step 44830, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 64h:46m:22s remains)
INFO - root - 2017-12-10 19:15:24.561451: step 44840, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 61h:36m:27s remains)
INFO - root - 2017-12-10 19:15:32.367113: step 44850, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 63h:47m:41s remains)
INFO - root - 2017-12-10 19:15:39.897852: step 44860, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 44h:55m:09s remains)
INFO - root - 2017-12-10 19:15:47.871988: step 44870, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 65h:07m:00s remains)
INFO - root - 2017-12-10 19:15:55.723853: step 44880, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 62h:32m:59s remains)
INFO - root - 2017-12-10 19:16:03.396603: step 44890, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 61h:58m:04s remains)
INFO - root - 2017-12-10 19:16:11.265273: step 44900, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 63h:27m:53s remains)
2017-12-10 19:16:12.199620: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10119674 0.086077519 0.086843304 0.1112439 0.1526085 0.19983642 0.23964728 0.25935829 0.24984248 0.22051042 0.18068033 0.13811064 0.098334663 0.0687424 0.051592544][0.13600743 0.12785591 0.13208605 0.15603492 0.19170994 0.2337511 0.27567407 0.30401632 0.30437708 0.28354108 0.25109673 0.21293864 0.17232764 0.13700144 0.11174273][0.19337955 0.19426131 0.19962119 0.21687467 0.23781945 0.26551011 0.30391803 0.3407262 0.355262 0.34887454 0.3292875 0.30022773 0.26284987 0.22467515 0.193161][0.27897233 0.28786963 0.29094821 0.2975269 0.30080447 0.31392232 0.3507424 0.39940363 0.43076771 0.43828365 0.42779297 0.40223861 0.36249045 0.31754145 0.28002116][0.38780394 0.39969274 0.39713874 0.39366969 0.3858591 0.39663926 0.44495663 0.51415724 0.56249547 0.57534456 0.55924684 0.519449 0.46055681 0.39799318 0.35317108][0.48618141 0.49326304 0.48449045 0.47677767 0.47026238 0.49390742 0.5661661 0.6593948 0.71722043 0.72019994 0.68060881 0.61038208 0.52054989 0.43600351 0.38693821][0.54762727 0.54751569 0.53606063 0.53130913 0.53470135 0.57505935 0.66676223 0.77078533 0.82069141 0.79907095 0.727497 0.62604237 0.51187831 0.41604584 0.37231252][0.562065 0.55611515 0.54827923 0.55394936 0.57187241 0.62418348 0.7194224 0.81304353 0.83718872 0.78268981 0.67994994 0.55617356 0.43255281 0.34077641 0.31202489][0.5117951 0.50351703 0.50428516 0.52489078 0.55925906 0.61818933 0.703318 0.77183551 0.76375443 0.68002433 0.55637956 0.42438242 0.30667314 0.23118047 0.22194286][0.40565017 0.39632869 0.40609422 0.44125444 0.49073064 0.5526951 0.62230897 0.6639412 0.63093817 0.53228676 0.40364444 0.276919 0.17437635 0.11837534 0.12466969][0.27917022 0.26910934 0.28279224 0.32466584 0.38095981 0.44055873 0.49350631 0.51346 0.4687289 0.37275657 0.25652564 0.14680663 0.063449606 0.023619013 0.036758441][0.18357918 0.17468752 0.18695003 0.22565268 0.27708927 0.32735559 0.36465821 0.37070742 0.32570148 0.24414475 0.15080051 0.063470259 -0.0029306528 -0.034794282 -0.024667405][0.13958366 0.13369671 0.1419066 0.1715308 0.21107706 0.24897492 0.27458256 0.27523854 0.23719455 0.17293809 0.10185557 0.033736326 -0.022021569 -0.053761583 -0.053104367][0.13270463 0.12939389 0.13201933 0.14956619 0.1735995 0.19750841 0.21394618 0.21397693 0.1857692 0.13783471 0.085363455 0.03253226 -0.015847208 -0.049307395 -0.05808628][0.13568209 0.13268736 0.12843105 0.13305004 0.14107497 0.15114899 0.15963352 0.16019897 0.14143769 0.1078678 0.071174651 0.03236011 -0.0068723951 -0.037678558 -0.05024327]]...]
INFO - root - 2017-12-10 19:16:20.015880: step 44910, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 61h:56m:39s remains)
INFO - root - 2017-12-10 19:16:27.841050: step 44920, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 62h:22m:18s remains)
INFO - root - 2017-12-10 19:16:35.784319: step 44930, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 63h:15m:33s remains)
INFO - root - 2017-12-10 19:16:43.620509: step 44940, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 63h:53m:52s remains)
INFO - root - 2017-12-10 19:16:51.309784: step 44950, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 62h:47m:12s remains)
INFO - root - 2017-12-10 19:16:59.256633: step 44960, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.795 sec/batch; 63h:29m:15s remains)
INFO - root - 2017-12-10 19:17:06.861106: step 44970, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.756 sec/batch; 60h:21m:58s remains)
INFO - root - 2017-12-10 19:17:14.719424: step 44980, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 61h:39m:00s remains)
INFO - root - 2017-12-10 19:17:22.635906: step 44990, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 61h:52m:29s remains)
INFO - root - 2017-12-10 19:17:30.442393: step 45000, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 63h:47m:06s remains)
2017-12-10 19:17:31.286011: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23963921 0.2445744 0.24352816 0.23297773 0.21656939 0.20485349 0.19588427 0.19037674 0.180039 0.171721 0.16528916 0.15799937 0.1505722 0.14553577 0.14343379][0.29111809 0.29954484 0.30136117 0.29288062 0.27871937 0.27060458 0.26475081 0.26099446 0.25366673 0.25167355 0.2524544 0.25045705 0.24551867 0.24264105 0.24366677][0.30155078 0.31530678 0.32343718 0.32194191 0.31500661 0.31383613 0.31298894 0.31125155 0.30679032 0.3111825 0.320633 0.32738355 0.3289977 0.33139962 0.33835724][0.28226548 0.30617955 0.3283993 0.34308279 0.3515664 0.36232576 0.36801431 0.36628321 0.35919586 0.36255434 0.37476152 0.38799173 0.39756289 0.407838 0.42384067][0.25194785 0.290519 0.33428296 0.37342888 0.4038735 0.42925161 0.4404811 0.43477628 0.41806588 0.411381 0.41859749 0.43445027 0.45164716 0.47055826 0.49567324][0.2256099 0.27850091 0.34511411 0.41092104 0.46491706 0.50534207 0.52141154 0.51023072 0.47999802 0.45679438 0.45182088 0.46461257 0.48643771 0.51146191 0.54140252][0.20727712 0.26798928 0.35057279 0.43785289 0.51295227 0.56847012 0.5904758 0.57523161 0.53167242 0.48959324 0.46724191 0.47069564 0.49206996 0.51929027 0.54911107][0.19188455 0.24866533 0.33336282 0.42947313 0.51666713 0.58320713 0.6117366 0.59651536 0.54587269 0.49086925 0.45456082 0.44776228 0.46479177 0.4893584 0.51361454][0.17279547 0.21547969 0.28790075 0.3785159 0.46647307 0.5367946 0.57020134 0.55984581 0.51248497 0.45656446 0.41560072 0.40150288 0.41059598 0.42537451 0.43650717][0.15228824 0.17417039 0.22272605 0.29434371 0.37075925 0.43581516 0.47128978 0.4709346 0.43865022 0.39621195 0.36282954 0.34711906 0.34568104 0.3425602 0.33172134][0.13733588 0.14010547 0.16266711 0.20965658 0.2673561 0.32040632 0.35393232 0.36447319 0.35269871 0.33173981 0.31321946 0.29972121 0.28687978 0.26239145 0.22699563][0.14051214 0.13402216 0.13914737 0.16547239 0.20372592 0.24053155 0.26602334 0.28169209 0.28613 0.284243 0.27931643 0.26796761 0.24470761 0.20180173 0.14708139][0.16240555 0.15745771 0.15677857 0.17064439 0.19296509 0.21255311 0.22488378 0.23753542 0.24877784 0.25749522 0.26006743 0.24939908 0.21981697 0.16743568 0.10420263][0.18037976 0.18272181 0.18388301 0.19270903 0.20533803 0.21219063 0.21189363 0.21601303 0.22431938 0.23369859 0.23716065 0.22674492 0.19718471 0.14648484 0.086549021][0.16826293 0.17536148 0.17879142 0.18508998 0.19243437 0.19273701 0.18501168 0.18105282 0.1821937 0.18619093 0.1864364 0.17673247 0.153119 0.11388757 0.067679957]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 19:17:39.130640: step 45010, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 63h:04m:26s remains)
INFO - root - 2017-12-10 19:17:47.076926: step 45020, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.790 sec/batch; 63h:07m:31s remains)
INFO - root - 2017-12-10 19:17:54.985360: step 45030, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.816 sec/batch; 65h:10m:54s remains)
INFO - root - 2017-12-10 19:18:02.689127: step 45040, loss = 0.67, batch loss = 0.61 (10.0 examples/sec; 0.802 sec/batch; 64h:02m:28s remains)
INFO - root - 2017-12-10 19:18:10.423804: step 45050, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 64h:59m:17s remains)
INFO - root - 2017-12-10 19:18:18.308185: step 45060, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 61h:40m:47s remains)
INFO - root - 2017-12-10 19:18:26.261685: step 45070, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 63h:51m:44s remains)
INFO - root - 2017-12-10 19:18:34.132229: step 45080, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 62h:55m:15s remains)
INFO - root - 2017-12-10 19:18:42.018126: step 45090, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.809 sec/batch; 64h:34m:06s remains)
INFO - root - 2017-12-10 19:18:49.722611: step 45100, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 62h:06m:58s remains)
2017-12-10 19:18:50.598416: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.49737448 0.540748 0.53471637 0.48955458 0.42266855 0.3470113 0.27586141 0.22525093 0.210026 0.22659916 0.25788271 0.29059082 0.32435882 0.36468318 0.41550165][0.59218711 0.62393367 0.59658939 0.5209257 0.41873774 0.30956984 0.21486656 0.15936388 0.16067237 0.21221012 0.28867719 0.36881611 0.44269598 0.5054037 0.55266291][0.57369924 0.58965629 0.54951864 0.46408844 0.3575508 0.25185785 0.16999488 0.13649841 0.16572514 0.24571463 0.34800357 0.4492574 0.53565276 0.59689248 0.62157303][0.45538 0.46267954 0.43003657 0.36634347 0.29275656 0.22650996 0.18393852 0.1826269 0.23071696 0.31306106 0.40444374 0.48739618 0.552792 0.59073973 0.58542031][0.2903876 0.3046588 0.30339655 0.29004079 0.27539504 0.26580191 0.26679641 0.28715461 0.32952413 0.38081905 0.42530826 0.45717362 0.47659743 0.47907892 0.44876462][0.14686784 0.18475579 0.2317678 0.28188229 0.33080849 0.37246174 0.40189782 0.42194077 0.43315211 0.43005869 0.40943184 0.37652692 0.34146255 0.30714583 0.26168302][0.074708752 0.14621373 0.24401934 0.35007951 0.4453381 0.51464772 0.54869175 0.54763258 0.51353127 0.45131373 0.36893466 0.27967975 0.19965605 0.13711189 0.086307116][0.07367564 0.17728855 0.31192797 0.44768941 0.55723369 0.62514007 0.64359 0.61219573 0.53666186 0.43024316 0.30846676 0.18776251 0.084582224 0.0098149879 -0.038397036][0.10452517 0.22296648 0.36639345 0.49815682 0.59077704 0.63584042 0.63102573 0.576465 0.47962871 0.35744408 0.22762792 0.1056264 0.0049482426 -0.064450093 -0.10285217][0.11918616 0.22700676 0.34856164 0.44912502 0.50771242 0.52459097 0.5025115 0.44188997 0.35011184 0.24281007 0.13483103 0.037668116 -0.039511155 -0.08939293 -0.11273493][0.10022395 0.17688328 0.25688019 0.31470206 0.33841908 0.33403286 0.30609766 0.25474104 0.18668571 0.11323655 0.044162344 -0.01442025 -0.057645358 -0.0812641 -0.087641254][0.056798458 0.0948523 0.13087149 0.15087797 0.1504955 0.13678123 0.11313976 0.079516053 0.040963106 0.0039434168 -0.026616981 -0.048246861 -0.058998875 -0.057841878 -0.048889309][0.0063874382 0.013205978 0.017811039 0.015076572 0.0047099153 -0.0079402812 -0.021581788 -0.036630414 -0.050221935 -0.059590146 -0.063024595 -0.059377428 -0.048007496 -0.030112104 -0.010881593][-0.033464704 -0.043414455 -0.053347006 -0.064224996 -0.074806437 -0.081665561 -0.085025519 -0.085956305 -0.083729848 -0.077615865 -0.067447633 -0.05216831 -0.031260207 -0.0065496885 0.016658692][-0.0541603 -0.067808837 -0.079559572 -0.089573123 -0.0965467 -0.098132789 -0.094922706 -0.088482626 -0.079676025 -0.068758287 -0.055789858 -0.039353255 -0.018747123 0.0045228829 0.02587088]]...]
INFO - root - 2017-12-10 19:18:58.476981: step 45110, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 63h:59m:26s remains)
INFO - root - 2017-12-10 19:19:06.434140: step 45120, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.773 sec/batch; 61h:41m:13s remains)
INFO - root - 2017-12-10 19:19:14.041944: step 45130, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 61h:53m:07s remains)
INFO - root - 2017-12-10 19:19:21.960254: step 45140, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 61h:49m:37s remains)
INFO - root - 2017-12-10 19:19:29.816234: step 45150, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 63h:30m:47s remains)
INFO - root - 2017-12-10 19:19:37.759505: step 45160, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 63h:53m:27s remains)
INFO - root - 2017-12-10 19:19:45.591331: step 45170, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 62h:52m:59s remains)
INFO - root - 2017-12-10 19:19:53.600462: step 45180, loss = 0.68, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 65h:03m:21s remains)
INFO - root - 2017-12-10 19:20:01.445960: step 45190, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 62h:02m:37s remains)
INFO - root - 2017-12-10 19:20:09.330741: step 45200, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 60h:58m:35s remains)
2017-12-10 19:20:10.232346: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26878706 0.32968396 0.37056047 0.39010113 0.41438884 0.44706294 0.47540849 0.47599098 0.44034418 0.36664435 0.25633454 0.12757154 0.011741349 -0.061949704 -0.094703004][0.29678634 0.35406098 0.38842875 0.40522707 0.43101552 0.46410128 0.48932779 0.48669857 0.44705933 0.36635163 0.24752858 0.11345778 -0.0028951571 -0.075048678 -0.10558552][0.33515692 0.38099241 0.40196314 0.41304782 0.43649474 0.46541145 0.48316187 0.47376651 0.42807546 0.3406775 0.21702462 0.082807131 -0.028567141 -0.094447531 -0.11908831][0.38941976 0.420264 0.42588279 0.43097797 0.45036551 0.47361398 0.4826569 0.46514156 0.41216782 0.31722358 0.18869159 0.054670673 -0.052015811 -0.1122663 -0.13142978][0.45959622 0.47843695 0.47236612 0.4737834 0.48961091 0.50891149 0.5126211 0.48893854 0.42825761 0.32355177 0.18725441 0.048720218 -0.059586812 -0.12066044 -0.13937598][0.52379292 0.53866816 0.52998477 0.53291726 0.54882288 0.57008356 0.57606715 0.55080181 0.48264933 0.36587825 0.21779753 0.068063311 -0.0499239 -0.11874481 -0.14170116][0.55575043 0.57350993 0.56857437 0.57633513 0.59698486 0.62791568 0.64534158 0.62600994 0.55543351 0.42958131 0.26963216 0.10570671 -0.026507631 -0.10725451 -0.13802405][0.53614104 0.56074947 0.56613255 0.58381557 0.61549979 0.66236049 0.69712096 0.69024122 0.62453121 0.49679986 0.32937539 0.15259461 0.0055604405 -0.088398717 -0.12891947][0.47559515 0.50672364 0.5247696 0.55412942 0.59775734 0.65769464 0.70569921 0.71082318 0.65473568 0.53427261 0.36917269 0.18854587 0.033693772 -0.068791166 -0.11693101][0.38708615 0.41725704 0.44276208 0.4805547 0.53432339 0.60422927 0.66304469 0.6807636 0.63878578 0.53325546 0.3794145 0.2041584 0.04992567 -0.054252986 -0.10576232][0.27074966 0.29269251 0.31944254 0.36165762 0.42288384 0.49976733 0.56747836 0.59803689 0.57287526 0.48732859 0.35218972 0.192238 0.048982661 -0.048370205 -0.09777908][0.14427872 0.15516615 0.17734937 0.21737601 0.27859855 0.35497475 0.42475232 0.46321085 0.45293561 0.38930884 0.27956098 0.14583853 0.025559453 -0.055246875 -0.096103147][0.028509095 0.03094681 0.047337335 0.080935106 0.13484605 0.20217143 0.26501331 0.30320114 0.30165732 0.2569623 0.17430247 0.072884358 -0.016769966 -0.074457876 -0.10157479][-0.063323736 -0.06519258 -0.053549618 -0.0278306 0.014278549 0.066229761 0.11470852 0.14505577 0.14567836 0.11495467 0.057163171 -0.011634496 -0.068974361 -0.10140233 -0.112141][-0.12168109 -0.12626791 -0.11949358 -0.10328569 -0.07559415 -0.041406281 -0.0096002268 0.010274473 0.010431013 -0.0095718848 -0.046008669 -0.086324938 -0.11539784 -0.12584648 -0.12199595]]...]
INFO - root - 2017-12-10 19:20:17.934644: step 45210, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 62h:26m:11s remains)
INFO - root - 2017-12-10 19:20:25.768177: step 45220, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 64h:10m:18s remains)
INFO - root - 2017-12-10 19:20:33.654923: step 45230, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 62h:05m:40s remains)
INFO - root - 2017-12-10 19:20:41.456503: step 45240, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 61h:52m:10s remains)
INFO - root - 2017-12-10 19:20:49.331014: step 45250, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.795 sec/batch; 63h:24m:32s remains)
INFO - root - 2017-12-10 19:20:57.117233: step 45260, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 62h:18m:04s remains)
INFO - root - 2017-12-10 19:21:05.060715: step 45270, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 63h:51m:23s remains)
INFO - root - 2017-12-10 19:21:12.905295: step 45280, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 61h:19m:38s remains)
INFO - root - 2017-12-10 19:21:20.730477: step 45290, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 64h:14m:34s remains)
INFO - root - 2017-12-10 19:21:28.746889: step 45300, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 63h:59m:05s remains)
2017-12-10 19:21:29.646230: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26346117 0.23801669 0.1900117 0.15241222 0.13947012 0.1490168 0.16632606 0.17219931 0.15003824 0.093588933 0.025284166 -0.031903606 -0.066017456 -0.083698995 -0.091488257][0.4017826 0.37061018 0.30910563 0.26225558 0.24859574 0.26504225 0.28986567 0.29348591 0.2539455 0.16875145 0.071544208 -0.0083166277 -0.0569076 -0.082319118 -0.093710795][0.49244443 0.46580005 0.40667641 0.36509371 0.36064473 0.38587612 0.41191387 0.40413955 0.3404485 0.22484148 0.10066301 0.0020720826 -0.056394253 -0.085645042 -0.096830308][0.48879635 0.47356933 0.43172586 0.4104256 0.4259879 0.46376315 0.48927215 0.46730474 0.37998223 0.2393629 0.095127985 -0.014392297 -0.075840987 -0.10165589 -0.10579701][0.3987917 0.39470646 0.37762788 0.38605604 0.42794517 0.48161408 0.5089007 0.47679439 0.37429935 0.22128984 0.070028469 -0.040819313 -0.099625066 -0.11896496 -0.11460479][0.30192071 0.30642951 0.31330004 0.34941018 0.41266814 0.47647122 0.50210744 0.46075797 0.35004187 0.19634014 0.048871502 -0.057675164 -0.11285122 -0.12836049 -0.11991099][0.27088746 0.27699262 0.30068362 0.35913998 0.43916908 0.50905657 0.53069496 0.47994435 0.36198804 0.2077416 0.061144404 -0.0468268 -0.10556046 -0.12516998 -0.1197266][0.29316965 0.2997 0.33615193 0.41594711 0.5150674 0.596408 0.62059855 0.56629229 0.44253114 0.28238109 0.12478311 0.00025611115 -0.075711586 -0.10989872 -0.11422823][0.33611411 0.33700046 0.37751615 0.46817788 0.57747608 0.66575068 0.69401526 0.64256 0.5208596 0.36044478 0.19464515 0.054246143 -0.03960415 -0.089526527 -0.10536533][0.36729702 0.35999095 0.39291486 0.47593275 0.576874 0.65910071 0.687778 0.64399391 0.53505653 0.38735092 0.22758156 0.084947415 -0.015850114 -0.073946945 -0.097676][0.34661463 0.33268198 0.35331708 0.41896436 0.50239688 0.57278985 0.60094792 0.56879127 0.48019797 0.35431635 0.21198805 0.080473006 -0.014519685 -0.070617743 -0.095525473][0.26992697 0.25328866 0.26368666 0.31177187 0.37736911 0.4359183 0.46347621 0.44399911 0.37757042 0.2763885 0.15732323 0.045898549 -0.033716995 -0.0795744 -0.099124357][0.15138316 0.13560826 0.14059021 0.17403996 0.22282109 0.26920909 0.29446894 0.28522944 0.2391309 0.16354857 0.072487108 -0.011811715 -0.0689614 -0.09824878 -0.10709547][0.028495863 0.014466858 0.015375688 0.033297803 0.061295453 0.0890954 0.10507601 0.099778786 0.070784047 0.022160476 -0.035281334 -0.084912144 -0.11285707 -0.1203518 -0.11522613][-0.061534517 -0.072118334 -0.0746426 -0.070501268 -0.061812945 -0.052311372 -0.047313966 -0.052242495 -0.068241715 -0.093342312 -0.12048379 -0.13917859 -0.14229985 -0.13257062 -0.11759069]]...]
INFO - root - 2017-12-10 19:21:37.377817: step 45310, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 61h:24m:08s remains)
INFO - root - 2017-12-10 19:21:45.402668: step 45320, loss = 0.71, batch loss = 0.66 (9.9 examples/sec; 0.812 sec/batch; 64h:47m:15s remains)
INFO - root - 2017-12-10 19:21:53.424359: step 45330, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 63h:46m:33s remains)
INFO - root - 2017-12-10 19:22:01.374031: step 45340, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.822 sec/batch; 65h:34m:19s remains)
INFO - root - 2017-12-10 19:22:09.335206: step 45350, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 62h:30m:50s remains)
INFO - root - 2017-12-10 19:22:17.181385: step 45360, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 61h:06m:55s remains)
INFO - root - 2017-12-10 19:22:24.793433: step 45370, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 62h:33m:12s remains)
INFO - root - 2017-12-10 19:22:32.760713: step 45380, loss = 0.72, batch loss = 0.66 (9.8 examples/sec; 0.814 sec/batch; 64h:53m:22s remains)
INFO - root - 2017-12-10 19:22:40.705778: step 45390, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 63h:43m:24s remains)
INFO - root - 2017-12-10 19:22:48.367024: step 45400, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 62h:06m:29s remains)
2017-12-10 19:22:49.180420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.065422393 -0.066990949 -0.067158878 -0.066199839 -0.063583925 -0.060972441 -0.060469836 -0.062386684 -0.065513708 -0.067728847 -0.067601457 -0.063649267 -0.055862322 -0.044813395 -0.032955967][-0.060353503 -0.062167879 -0.062817521 -0.061676536 -0.058309224 -0.054966662 -0.05451506 -0.057337221 -0.062048793 -0.066261649 -0.068019688 -0.065316647 -0.058412332 -0.049540374 -0.042140204][-0.054240141 -0.051958047 -0.047115851 -0.039181277 -0.029148636 -0.02118789 -0.019272553 -0.023890337 -0.03329825 -0.044168655 -0.053225823 -0.057011295 -0.055088922 -0.050598003 -0.04751084][-0.045578815 -0.033832662 -0.016614661 0.0049177869 0.027296325 0.043928765 0.049284611 0.042421844 0.025549207 0.0030610324 -0.019914106 -0.037250623 -0.046246972 -0.049418651 -0.050807834][-0.031674217 -0.0074034482 0.026117409 0.065791443 0.10521001 0.1349442 0.14658074 0.13778837 0.11122332 0.073070362 0.030879131 -0.0057259011 -0.030490631 -0.044264682 -0.051173288][-0.013044499 0.024164863 0.075623862 0.13657108 0.19727671 0.24470975 0.26581869 0.25570881 0.21766703 0.1605995 0.0955222 0.035879366 -0.0075834733 -0.034026656 -0.04790907][0.012168061 0.061553363 0.13017535 0.21187915 0.29357821 0.35847297 0.38862729 0.37602273 0.32527381 0.24853322 0.16025172 0.077695116 0.015726544 -0.023365663 -0.044641826][0.0492857 0.10841438 0.18996742 0.28715265 0.38384572 0.45997059 0.49339709 0.473696 0.4076905 0.31145179 0.2028079 0.10204004 0.026561266 -0.02081582 -0.046438053][0.10163902 0.16502805 0.25062343 0.35245147 0.45247281 0.5282467 0.55537713 0.52275574 0.4404597 0.32826447 0.2069144 0.098275006 0.01972519 -0.027378961 -0.051496796][0.17082798 0.23397535 0.31429565 0.40826076 0.49764675 0.55968887 0.57084477 0.52150148 0.42499095 0.30330417 0.17854628 0.072539888 7.9597477e-05 -0.040149089 -0.058483172][0.24708277 0.30928385 0.37882233 0.45502681 0.52119178 0.55745804 0.54507971 0.47802708 0.37196809 0.24854618 0.12884082 0.032953914 -0.028163141 -0.057995945 -0.067974351][0.31649894 0.37524098 0.4277046 0.47662666 0.50906956 0.51172948 0.47364554 0.39313364 0.28630584 0.17145921 0.066287212 -0.012136704 -0.05641418 -0.071125887 -0.068038523][0.36069381 0.40957716 0.43913016 0.45646983 0.45406026 0.42667487 0.37106636 0.28880855 0.19396619 0.099075988 0.017363725 -0.037296183 -0.059801303 -0.054981302 -0.034907341][0.36322582 0.39592528 0.40219131 0.39361086 0.36804408 0.32694495 0.27230594 0.20647274 0.13884315 0.074360967 0.021322876 -0.0091894614 -0.012085557 0.0095184483 0.044217471][0.32793245 0.34412277 0.33520034 0.31503236 0.28407776 0.24702394 0.20831211 0.1698447 0.13585111 0.10421961 0.078768082 0.068444937 0.080334216 0.11436986 0.16062148]]...]
INFO - root - 2017-12-10 19:22:57.031019: step 45410, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 62h:22m:44s remains)
INFO - root - 2017-12-10 19:23:05.043251: step 45420, loss = 0.70, batch loss = 0.65 (9.6 examples/sec; 0.835 sec/batch; 66h:33m:35s remains)
INFO - root - 2017-12-10 19:23:12.953152: step 45430, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 62h:55m:53s remains)
INFO - root - 2017-12-10 19:23:20.800468: step 45440, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 63h:00m:06s remains)
INFO - root - 2017-12-10 19:23:28.572807: step 45450, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 60h:44m:35s remains)
INFO - root - 2017-12-10 19:23:36.369222: step 45460, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 62h:57m:39s remains)
INFO - root - 2017-12-10 19:23:44.203836: step 45470, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 63h:16m:12s remains)
INFO - root - 2017-12-10 19:23:52.108810: step 45480, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 63h:25m:17s remains)
INFO - root - 2017-12-10 19:23:59.489883: step 45490, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 62h:29m:58s remains)
INFO - root - 2017-12-10 19:24:07.480431: step 45500, loss = 0.67, batch loss = 0.62 (9.9 examples/sec; 0.809 sec/batch; 64h:29m:52s remains)
2017-12-10 19:24:08.301267: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10421032 0.14877889 0.1679174 0.15527311 0.12399799 0.10066793 0.10849573 0.15052025 0.20822647 0.25267163 0.26047838 0.22668266 0.16335577 0.091918372 0.030207423][0.040307984 0.080992781 0.10917365 0.11393175 0.10069676 0.088435762 0.09782508 0.13694476 0.19589858 0.25164631 0.27841046 0.26343837 0.21215223 0.14279102 0.07627552][-0.011186999 0.022681206 0.058094461 0.082770608 0.093809605 0.098874286 0.1110107 0.14025766 0.18695845 0.23935646 0.27601629 0.279136 0.24590629 0.18745954 0.12410363][-0.041186906 -0.01400536 0.026131745 0.068868853 0.10564052 0.13256754 0.15156752 0.16973226 0.19537577 0.23007664 0.26249775 0.27619091 0.26150918 0.22041667 0.16824876][-0.05327829 -0.031650916 0.0099969869 0.065458879 0.12456492 0.17492679 0.20658863 0.21912383 0.22328137 0.23239754 0.24785285 0.26030916 0.25852057 0.23617929 0.20071346][-0.054159086 -0.037045583 0.0025658915 0.063263848 0.13725938 0.20872825 0.2583687 0.27546036 0.266623 0.250784 0.2411162 0.23975635 0.24019587 0.23207951 0.21426035][-0.050639439 -0.038178697 -0.00433836 0.053445175 0.13252109 0.21845756 0.28760755 0.31941482 0.31127754 0.28017983 0.24603891 0.22331893 0.21551377 0.2141403 0.21124735][-0.048123866 -0.041186452 -0.016531648 0.031124827 0.10490098 0.19529484 0.27968931 0.33125955 0.33678073 0.30522367 0.25625902 0.2137346 0.19241688 0.1901996 0.19636562][-0.047570568 -0.046076633 -0.031693973 0.0019675219 0.062529482 0.1467191 0.23665595 0.30440009 0.3287707 0.30858856 0.25793341 0.20447789 0.17178263 0.16557713 0.17584956][-0.047008082 -0.049419794 -0.043684553 -0.024032021 0.019182691 0.088101476 0.17131332 0.24421909 0.28274366 0.27756628 0.23555881 0.1828137 0.14687626 0.13957255 0.15309179][-0.045293923 -0.049592957 -0.049663756 -0.0415416 -0.016162079 0.031893685 0.097442746 0.16236901 0.20468915 0.211278 0.18296146 0.14094965 0.111734 0.10972262 0.12877855][-0.042696714 -0.047307268 -0.050329536 -0.050027385 -0.039655771 -0.013024196 0.029218683 0.076415122 0.1121752 0.12395541 0.1092203 0.083127245 0.067664154 0.076378755 0.10367783][-0.039849974 -0.043813411 -0.047628313 -0.051345192 -0.050948646 -0.041402906 -0.021042608 0.0056056846 0.028781857 0.039387017 0.034246437 0.02318058 0.022599485 0.04322334 0.079416633][-0.037195511 -0.040359523 -0.0437887 -0.048618007 -0.052961033 -0.053747788 -0.048869342 -0.039148942 -0.0289089 -0.023041835 -0.02363633 -0.02446753 -0.013912441 0.01566454 0.058673985][-0.035344668 -0.037966073 -0.040743332 -0.045229547 -0.050704807 -0.05570709 -0.058708057 -0.059047014 -0.057861246 -0.056841563 -0.056474224 -0.052369684 -0.036290146 -0.0028050728 0.04312465]]...]
INFO - root - 2017-12-10 19:24:16.185707: step 45510, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 62h:06m:47s remains)
INFO - root - 2017-12-10 19:24:24.080784: step 45520, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 60h:14m:43s remains)
INFO - root - 2017-12-10 19:24:31.868809: step 45530, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 63h:19m:22s remains)
INFO - root - 2017-12-10 19:24:39.735703: step 45540, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 62h:58m:35s remains)
INFO - root - 2017-12-10 19:24:47.621783: step 45550, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 63h:08m:28s remains)
INFO - root - 2017-12-10 19:24:55.406097: step 45560, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 61h:32m:53s remains)
INFO - root - 2017-12-10 19:25:03.292374: step 45570, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 62h:45m:25s remains)
INFO - root - 2017-12-10 19:25:11.032045: step 45580, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 62h:22m:41s remains)
INFO - root - 2017-12-10 19:25:18.860642: step 45590, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 64h:08m:15s remains)
INFO - root - 2017-12-10 19:25:26.784143: step 45600, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 63h:46m:54s remains)
2017-12-10 19:25:27.670766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.03258707 -0.02407451 -0.019320669 -0.021836687 -0.027973264 -0.036447436 -0.042875245 -0.043143809 -0.038715594 -0.03374888 -0.033576772 -0.036852933 -0.040977068 -0.04567669 -0.048306212][0.003717825 0.023632605 0.035598453 0.033943895 0.024810711 0.010586522 -0.00061751012 -0.0022620556 0.0052755368 0.01643029 0.021196438 0.019914197 0.015950782 0.010052158 0.0067093517][0.048874605 0.084054753 0.10726691 0.11011612 0.10014815 0.080785327 0.064731874 0.060858767 0.070741825 0.087940983 0.098754823 0.10229655 0.10211952 0.098852031 0.097132035][0.099138848 0.15000099 0.18571685 0.19502833 0.18555915 0.16244462 0.14280367 0.13640369 0.14658302 0.1674547 0.18379502 0.19386005 0.20084667 0.20432484 0.20737119][0.14570804 0.20854025 0.25371543 0.26722193 0.25691134 0.23103271 0.21021852 0.20300186 0.21225469 0.23364542 0.2522887 0.26704434 0.28028414 0.28955665 0.29602879][0.18115595 0.2527979 0.30422741 0.31934571 0.3064363 0.27767786 0.25615206 0.2486544 0.25603721 0.27611849 0.29462227 0.31138721 0.32719588 0.33735046 0.34209916][0.20647919 0.28623623 0.34398711 0.36206064 0.3487483 0.31929103 0.29801038 0.29108179 0.29713807 0.31482089 0.33032122 0.34422466 0.35641 0.36083698 0.35753259][0.22138391 0.30786738 0.372169 0.39590627 0.38679433 0.36105838 0.34238654 0.33836883 0.34471214 0.35922229 0.3679629 0.37299722 0.37614462 0.37110707 0.35879305][0.22477336 0.31399041 0.38252413 0.41227004 0.41036972 0.39131519 0.37612593 0.37549794 0.3826108 0.39390585 0.39506763 0.39084688 0.38646892 0.37460139 0.35691309][0.21479109 0.30094838 0.36797348 0.39980319 0.40399441 0.39140233 0.37837741 0.3792778 0.38655928 0.39562121 0.39316 0.3868058 0.3830615 0.37160605 0.35386738][0.19298433 0.27175596 0.33149219 0.3598108 0.36575305 0.35592404 0.34105855 0.33862823 0.34357238 0.35137063 0.35059315 0.34936029 0.35273528 0.34683493 0.33124611][0.155015 0.22086722 0.26812062 0.28834432 0.29205689 0.28208059 0.26385847 0.25595194 0.25690389 0.26343098 0.26665804 0.27351278 0.28632516 0.28800261 0.27521339][0.096314013 0.14308141 0.17335451 0.18245758 0.18132062 0.1701082 0.15026782 0.1392131 0.13809966 0.14484079 0.15298867 0.16779292 0.18889436 0.19774626 0.1884384][0.029904043 0.055734657 0.068790123 0.06690488 0.060166333 0.047100466 0.027068071 0.015061181 0.014067249 0.022149295 0.034655396 0.054979168 0.080483831 0.094028682 0.088802293][-0.024008404 -0.014088284 -0.012829235 -0.021372972 -0.031580493 -0.045424663 -0.064213075 -0.076139234 -0.076916419 -0.06805516 -0.053153675 -0.031322114 -0.0065907026 0.0079094116 0.00635803]]...]
INFO - root - 2017-12-10 19:25:35.346094: step 45610, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 60h:59m:56s remains)
INFO - root - 2017-12-10 19:25:43.232522: step 45620, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 62h:57m:37s remains)
INFO - root - 2017-12-10 19:25:51.107440: step 45630, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 63h:38m:05s remains)
INFO - root - 2017-12-10 19:25:58.961548: step 45640, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 63h:11m:09s remains)
INFO - root - 2017-12-10 19:26:06.834431: step 45650, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 62h:19m:00s remains)
INFO - root - 2017-12-10 19:26:14.681272: step 45660, loss = 0.69, batch loss = 0.63 (11.0 examples/sec; 0.729 sec/batch; 58h:03m:20s remains)
INFO - root - 2017-12-10 19:26:22.455768: step 45670, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 62h:14m:58s remains)
INFO - root - 2017-12-10 19:26:30.379208: step 45680, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 62h:39m:16s remains)
INFO - root - 2017-12-10 19:26:38.090699: step 45690, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 62h:39m:34s remains)
INFO - root - 2017-12-10 19:26:46.121675: step 45700, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.829 sec/batch; 66h:02m:38s remains)
2017-12-10 19:26:46.935065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.045988008 -0.039562039 -0.029872669 -0.019102719 -0.010582029 -0.0072584013 -0.0083838422 -0.01092478 -0.012878654 -0.013443663 -0.012878582 -0.012481307 -0.013392181 -0.01485339 -0.015793249][-0.03777983 -0.023151184 -0.0023571306 0.020281646 0.038716875 0.047441553 0.046779547 0.041040331 0.033743527 0.027523298 0.023212755 0.020148342 0.017122006 0.014487764 0.012698371][-0.026650446 -0.0013403569 0.034769524 0.074622162 0.10871812 0.12791879 0.13025729 0.12025469 0.10354786 0.086316749 0.072752319 0.064118914 0.058837671 0.055867288 0.0540618][-0.015196599 0.02135101 0.073949814 0.13272025 0.18438268 0.21578294 0.22169225 0.20657831 0.1787028 0.14913158 0.12668759 0.11454857 0.11051951 0.11030763 0.10984484][-0.005123795 0.040719196 0.10729431 0.18231958 0.24942152 0.29195738 0.30146265 0.2822403 0.2447551 0.20517807 0.17720814 0.16604468 0.16958591 0.17960939 0.18728969][0.0027190782 0.054819711 0.13094233 0.21744239 0.29608595 0.34771663 0.36155191 0.3417027 0.29909799 0.253719 0.22400239 0.21827626 0.2357 0.26419505 0.28928262][0.0079058381 0.062608063 0.14384159 0.23675255 0.322543 0.38145 0.40135404 0.38499454 0.34227139 0.29599306 0.26833427 0.27081561 0.30461532 0.35485536 0.40143764][0.0088731693 0.063144855 0.14562272 0.24062979 0.32964441 0.39336014 0.41936 0.40819654 0.36824954 0.32427388 0.30116805 0.31261513 0.36009547 0.4266592 0.48879156][0.0037183382 0.052865785 0.12957825 0.21826854 0.30201548 0.36323905 0.39103946 0.38499904 0.35228896 0.31715292 0.3033545 0.32345897 0.37811232 0.44969711 0.51494884][-0.0057380372 0.035571277 0.10156778 0.17811751 0.25070781 0.30384743 0.32909986 0.32641593 0.30164325 0.27546418 0.26786062 0.28972119 0.34130159 0.40642196 0.46511021][-0.016231125 0.017913559 0.074291386 0.14044662 0.20349485 0.24930084 0.27108508 0.26929379 0.24872714 0.22640815 0.21867326 0.23485529 0.27444008 0.323376 0.36665761][-0.027461488 -0.00011781311 0.047482014 0.10453307 0.15937732 0.19893132 0.21738029 0.21537162 0.1969596 0.17620926 0.1663257 0.17450297 0.19825788 0.22591081 0.24812834][-0.038907167 -0.018853977 0.018701818 0.064984977 0.110315 0.14313053 0.15818116 0.15599321 0.13989671 0.12110163 0.10976111 0.11052062 0.11923769 0.12671903 0.12915552][-0.048164323 -0.03584554 -0.0096245725 0.023828331 0.057580028 0.082184941 0.093055665 0.090554796 0.077336907 0.061546091 0.050233904 0.046339989 0.045164406 0.040040363 0.031056343][-0.053319357 -0.04736371 -0.030913927 -0.0088796858 0.014199705 0.031085225 0.037899036 0.03471683 0.023555189 0.0099349786 -0.0010961853 -0.0073776697 -0.013010558 -0.022377891 -0.03388191]]...]
INFO - root - 2017-12-10 19:26:54.829701: step 45710, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 64h:47m:34s remains)
INFO - root - 2017-12-10 19:27:02.690403: step 45720, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 62h:22m:37s remains)
INFO - root - 2017-12-10 19:27:10.635787: step 45730, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 62h:59m:34s remains)
INFO - root - 2017-12-10 19:27:18.594399: step 45740, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 63h:44m:43s remains)
INFO - root - 2017-12-10 19:27:26.346387: step 45750, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.788 sec/batch; 62h:43m:43s remains)
INFO - root - 2017-12-10 19:27:34.240903: step 45760, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.835 sec/batch; 66h:28m:41s remains)
INFO - root - 2017-12-10 19:27:41.828777: step 45770, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 60h:05m:42s remains)
INFO - root - 2017-12-10 19:27:49.677123: step 45780, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.785 sec/batch; 62h:29m:09s remains)
INFO - root - 2017-12-10 19:27:57.602625: step 45790, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.812 sec/batch; 64h:42m:25s remains)
INFO - root - 2017-12-10 19:28:05.493163: step 45800, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 62h:56m:11s remains)
2017-12-10 19:28:06.322945: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12729998 0.22758707 0.33127046 0.41169754 0.44591829 0.42822298 0.3761982 0.31026557 0.26738068 0.24976628 0.2476124 0.24238893 0.22506312 0.19166264 0.13908127][0.1254172 0.22955091 0.33506998 0.41970569 0.46110037 0.45303139 0.40954447 0.34767962 0.30805659 0.29075769 0.28392354 0.27092445 0.24545757 0.20580836 0.14678247][0.12316591 0.22385935 0.32329273 0.40594748 0.45255893 0.45645866 0.42682943 0.37774897 0.35055274 0.33904359 0.32794791 0.30325511 0.26545408 0.21560802 0.14718495][0.13027121 0.22703685 0.31918231 0.3969965 0.44581971 0.45912093 0.44117239 0.40429193 0.38977924 0.38482258 0.3695339 0.33167285 0.28030115 0.21973436 0.1422281][0.14970945 0.24227525 0.32612598 0.39641947 0.44374567 0.46159622 0.45037922 0.42119944 0.41490403 0.41381836 0.39431235 0.346202 0.28572711 0.21968015 0.13811445][0.17584679 0.26287133 0.33597627 0.39599636 0.43919578 0.45825925 0.45073226 0.42821744 0.4303793 0.43399006 0.41214865 0.35761178 0.29148066 0.22182399 0.13715586][0.1942735 0.2739538 0.33476639 0.383311 0.42147556 0.44012475 0.4359282 0.42362678 0.44007713 0.45433572 0.435831 0.37956142 0.30951026 0.23390517 0.1426093][0.19995905 0.27315375 0.32442012 0.36442122 0.39908522 0.41713038 0.41554376 0.41423127 0.4458268 0.47092062 0.45602468 0.39772218 0.32308117 0.24040103 0.14330396][0.20388272 0.27454665 0.32178211 0.35702071 0.38798916 0.40222177 0.39873946 0.40291682 0.44301924 0.47221547 0.4564822 0.39364642 0.31429321 0.22693326 0.12921646][0.21205617 0.28571445 0.33457986 0.36650991 0.38995463 0.394533 0.38363045 0.38696671 0.42867169 0.45771793 0.44135013 0.37732035 0.29760739 0.20986508 0.11455807][0.21850815 0.30008003 0.35551584 0.38497347 0.39858332 0.39172393 0.37318712 0.37451547 0.4164722 0.44692898 0.4334169 0.37112153 0.29212505 0.20335671 0.10739284][0.22805029 0.31983066 0.38244966 0.408584 0.41205603 0.3963244 0.37402183 0.37555209 0.41957206 0.45506313 0.44813123 0.38850716 0.30793142 0.21454799 0.11307889][0.26241714 0.36656252 0.43410012 0.45437285 0.44666463 0.42274398 0.395975 0.39357606 0.43612486 0.47659951 0.47768888 0.42146894 0.33754984 0.23828803 0.12960252][0.32211977 0.43777451 0.5062449 0.51788133 0.49731553 0.46360084 0.42847872 0.41615367 0.45308122 0.49687254 0.50595796 0.45406666 0.36777654 0.26505312 0.15132327][0.37094545 0.49423972 0.56341773 0.57161808 0.545108 0.50576836 0.46174252 0.43728155 0.46852541 0.51573145 0.53261161 0.48556918 0.39859369 0.2950443 0.1783029]]...]
INFO - root - 2017-12-10 19:28:14.224530: step 45810, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 62h:27m:26s remains)
INFO - root - 2017-12-10 19:28:22.122219: step 45820, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 62h:53m:09s remains)
INFO - root - 2017-12-10 19:28:30.021423: step 45830, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 63h:30m:58s remains)
INFO - root - 2017-12-10 19:28:37.658562: step 45840, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 64h:33m:27s remains)
INFO - root - 2017-12-10 19:28:45.365462: step 45850, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.806 sec/batch; 64h:10m:50s remains)
INFO - root - 2017-12-10 19:28:53.316269: step 45860, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 63h:25m:16s remains)
INFO - root - 2017-12-10 19:29:01.141702: step 45870, loss = 0.67, batch loss = 0.62 (10.4 examples/sec; 0.767 sec/batch; 61h:03m:52s remains)
INFO - root - 2017-12-10 19:29:08.885557: step 45880, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 60h:46m:35s remains)
INFO - root - 2017-12-10 19:29:16.818035: step 45890, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 62h:00m:21s remains)
INFO - root - 2017-12-10 19:29:24.717875: step 45900, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 64h:50m:31s remains)
2017-12-10 19:29:25.562125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.016486634 -0.0196026 -0.028066996 -0.039428636 -0.048417132 -0.051159136 -0.042873066 -0.029785937 -0.021972127 -0.025224084 -0.038095973 -0.054299653 -0.070239142 -0.079383664 -0.078748129][0.022011094 0.026174959 0.018622912 0.0031520436 -0.012988309 -0.023477118 -0.015136336 0.0031544059 0.015413745 0.012525685 -0.005429538 -0.030239239 -0.057880908 -0.078329444 -0.085048921][0.090814687 0.11222071 0.11278608 0.096639015 0.072629385 0.051467445 0.060443081 0.08627861 0.10327511 0.098838381 0.071102127 0.031221613 -0.016040152 -0.056062311 -0.07724762][0.20097171 0.25209662 0.27157864 0.26230562 0.23341905 0.2000951 0.20795067 0.23937188 0.25682276 0.24409403 0.19792521 0.13288748 0.05549258 -0.013464768 -0.056925647][0.32755756 0.41747376 0.46765053 0.47934994 0.46018106 0.42634222 0.43960744 0.47973961 0.49699533 0.46940598 0.39225459 0.28708881 0.16553342 0.054855868 -0.021394791][0.43988791 0.57045954 0.65707523 0.70184296 0.70930755 0.69254655 0.72318178 0.77700055 0.79302543 0.74267447 0.62306362 0.46598005 0.29162213 0.13298778 0.019394623][0.52154678 0.68623561 0.80538815 0.88830882 0.93563175 0.95229179 1.0103874 1.0785769 1.0868716 1.0046028 0.83589929 0.62461638 0.39995328 0.19833633 0.052839052][0.56995636 0.75268918 0.88950115 1.000679 1.0843755 1.1365877 1.2230387 1.3022423 1.2986326 1.1843545 0.97381449 0.7202599 0.45988685 0.23099871 0.067398071][0.57980627 0.75707781 0.88787323 1.0041087 1.1041873 1.1778581 1.2798209 1.3621224 1.3492925 1.2162176 0.98701483 0.71858478 0.45050347 0.21984799 0.058094103][0.53446293 0.6854679 0.79082221 0.88959593 0.9821226 1.057188 1.1571941 1.2339712 1.2184604 1.0870832 0.86892861 0.617466 0.37149087 0.16491994 0.025238739][0.44470835 0.55736464 0.62672293 0.69326454 0.75838494 0.81323904 0.89030653 0.94746512 0.92821604 0.81252176 0.62940484 0.42268357 0.22513995 0.066828005 -0.031596176][0.32912835 0.40251553 0.43699545 0.46832365 0.4985939 0.52310443 0.5669477 0.5972349 0.5726794 0.4799943 0.34416804 0.19627586 0.059774425 -0.040617432 -0.092009924][0.20533155 0.24396758 0.25040174 0.25092098 0.2489381 0.24484286 0.25832716 0.26690358 0.2434379 0.18004869 0.094919853 0.0064567379 -0.0711661 -0.12006005 -0.13348739][0.096980043 0.10983579 0.099284224 0.080871508 0.058648683 0.036563396 0.029177804 0.02440886 0.0063757757 -0.029752264 -0.073067173 -0.1145509 -0.14700125 -0.15948407 -0.1494434][0.012057316 0.010374373 -0.0047202534 -0.026559133 -0.052085564 -0.076749429 -0.091277681 -0.10034908 -0.11219767 -0.12975754 -0.14784397 -0.16236331 -0.1696458 -0.16341224 -0.1438456]]...]
INFO - root - 2017-12-10 19:29:33.394669: step 45910, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 63h:42m:14s remains)
INFO - root - 2017-12-10 19:29:41.238178: step 45920, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 62h:58m:32s remains)
INFO - root - 2017-12-10 19:29:48.845349: step 45930, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 63h:22m:35s remains)
INFO - root - 2017-12-10 19:29:56.699104: step 45940, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 63h:48m:45s remains)
INFO - root - 2017-12-10 19:30:04.579980: step 45950, loss = 0.67, batch loss = 0.61 (10.2 examples/sec; 0.784 sec/batch; 62h:22m:54s remains)
INFO - root - 2017-12-10 19:30:12.481645: step 45960, loss = 0.73, batch loss = 0.67 (10.4 examples/sec; 0.772 sec/batch; 61h:28m:43s remains)
INFO - root - 2017-12-10 19:30:20.340117: step 45970, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 61h:16m:30s remains)
INFO - root - 2017-12-10 19:30:28.154425: step 45980, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 62h:29m:48s remains)
INFO - root - 2017-12-10 19:30:36.005614: step 45990, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 63h:40m:42s remains)
INFO - root - 2017-12-10 19:30:44.030621: step 46000, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.829 sec/batch; 66h:00m:06s remains)
2017-12-10 19:30:44.885235: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.49758634 0.47676039 0.46036777 0.4614 0.47838351 0.49611664 0.49797627 0.48177394 0.45901522 0.44182491 0.43004805 0.4291971 0.43898493 0.449999 0.44872254][0.48894036 0.46016353 0.43969923 0.44745889 0.48031068 0.51931673 0.53728861 0.52300239 0.4858467 0.44133598 0.39669949 0.36119783 0.33950758 0.32328618 0.29903534][0.48085898 0.44278264 0.4139547 0.42307594 0.46734837 0.524411 0.5588882 0.54871148 0.49765095 0.42137533 0.33541688 0.256344 0.19644225 0.15192243 0.11138389][0.46289277 0.41653708 0.38183534 0.39394391 0.4509944 0.52585989 0.57592207 0.56993484 0.50589591 0.39850906 0.2712729 0.1507023 0.057594042 -0.006347748 -0.051608723][0.40179765 0.35795975 0.33420345 0.36499006 0.44570374 0.54205608 0.60583806 0.60113335 0.52324766 0.38848597 0.22803122 0.077408358 -0.036496662 -0.10830195 -0.14838585][0.29611161 0.27323142 0.28414202 0.35310286 0.46809688 0.58731544 0.66068178 0.65302241 0.56084448 0.40482277 0.22214699 0.054259121 -0.069395766 -0.14180475 -0.17342983][0.17092922 0.18436866 0.2454156 0.36189732 0.51312023 0.6529274 0.73254752 0.720668 0.61701882 0.44747895 0.25205892 0.074683093 -0.0542307 -0.12614404 -0.15072027][0.063670278 0.11566851 0.22361591 0.37897572 0.55566412 0.70812613 0.79112148 0.7779246 0.66979784 0.49529138 0.29405704 0.11057994 -0.02287842 -0.094961323 -0.11351168][-0.0017730714 0.077882171 0.21311247 0.38474277 0.56733561 0.72086269 0.8046487 0.79471308 0.69053954 0.51929164 0.31836322 0.13286863 -0.0017267076 -0.07068868 -0.080442525][-0.027057085 0.061987467 0.19888569 0.36097559 0.528891 0.6709131 0.75109774 0.74617511 0.65184122 0.49196723 0.30103028 0.12416517 -0.00098840334 -0.057903092 -0.052738771][-0.032199968 0.047448114 0.16359134 0.29622364 0.43432277 0.554288 0.6244247 0.62195778 0.53992945 0.3993825 0.23218651 0.081032336 -0.018310495 -0.051063076 -0.024802523][-0.036507074 0.021086235 0.1039433 0.19795497 0.29901361 0.38957545 0.44252762 0.43755227 0.36861211 0.25534242 0.12665923 0.018706296 -0.039945249 -0.039777774 0.0091302572][-0.043988179 -0.011473657 0.036972977 0.093279354 0.15704438 0.2153047 0.24725004 0.23785087 0.18479991 0.10623683 0.026018584 -0.029850531 -0.0434017 -0.011527371 0.053656794][-0.049914129 -0.037413213 -0.015384238 0.012176403 0.045962617 0.077123359 0.092378207 0.0829346 0.049881313 0.0082498919 -0.024966167 -0.03453673 -0.012617723 0.039081659 0.10799139][-0.048629194 -0.047356129 -0.040283058 -0.028754931 -0.011835085 0.0053647216 0.015743436 0.014460469 0.0033475724 -0.0070319371 -0.0062830192 0.012622845 0.051151678 0.10599203 0.16574799]]...]
INFO - root - 2017-12-10 19:30:52.632389: step 46010, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.796 sec/batch; 63h:22m:21s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 19:31:00.358285: step 46020, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.824 sec/batch; 65h:36m:18s remains)
INFO - root - 2017-12-10 19:31:08.186250: step 46030, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 62h:06m:57s remains)
INFO - root - 2017-12-10 19:31:16.039957: step 46040, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 63h:19m:19s remains)
INFO - root - 2017-12-10 19:31:23.987170: step 46050, loss = 0.71, batch loss = 0.65 (9.1 examples/sec; 0.881 sec/batch; 70h:03m:45s remains)
INFO - root - 2017-12-10 19:31:31.948568: step 46060, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 62h:22m:49s remains)
INFO - root - 2017-12-10 19:31:39.797785: step 46070, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 62h:10m:02s remains)
INFO - root - 2017-12-10 19:31:47.797630: step 46080, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 61h:48m:29s remains)
INFO - root - 2017-12-10 19:31:55.541251: step 46090, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 63h:09m:45s remains)
INFO - root - 2017-12-10 19:32:03.470168: step 46100, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 62h:05m:36s remains)
2017-12-10 19:32:04.405990: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.045787908 0.10745922 0.18360463 0.24775058 0.28544319 0.29871786 0.28725711 0.2463818 0.18464702 0.11624021 0.06222913 0.03353063 0.028483121 0.037238017 0.052778136][0.046583358 0.11093574 0.19884616 0.28435567 0.34561983 0.37523422 0.3729068 0.33233431 0.25913671 0.16921128 0.089141607 0.038471553 0.018862016 0.020469407 0.033652171][0.051456705 0.11785536 0.214152 0.31638685 0.39895162 0.44801378 0.46045712 0.42665932 0.34959489 0.24723314 0.14836536 0.076592691 0.037056353 0.024079759 0.028158838][0.060871355 0.13025458 0.23181431 0.34339544 0.4416914 0.51304537 0.54910928 0.53304762 0.46337664 0.36072141 0.25219196 0.15898612 0.090994984 0.053607866 0.041300304][0.074281454 0.15124483 0.25748959 0.37157631 0.47761038 0.56890333 0.63039613 0.63502407 0.57632315 0.47913012 0.36724246 0.25553674 0.15888251 0.094899274 0.064165317][0.092195883 0.18477775 0.30002943 0.41549274 0.5233162 0.62576813 0.70213687 0.71703792 0.66109765 0.56462419 0.44998935 0.32624826 0.21087338 0.12904581 0.085587926][0.11559796 0.22848861 0.35640743 0.47327736 0.57587165 0.67381871 0.74432385 0.74976844 0.68186629 0.57636768 0.45668709 0.3286778 0.20953646 0.12532842 0.081354894][0.14080749 0.27068225 0.40662509 0.51843625 0.60383445 0.67693216 0.71783119 0.69550014 0.60796469 0.49334347 0.37420979 0.25337648 0.14592732 0.074083395 0.040891223][0.15541705 0.29182544 0.42531636 0.522605 0.58130139 0.61791563 0.61690891 0.55873322 0.45237356 0.33691195 0.23160498 0.13376689 0.053722888 0.0063249362 -0.007914017][0.15527241 0.28822652 0.41181967 0.49077091 0.52348709 0.52774537 0.4907361 0.403013 0.28564137 0.17946447 0.10007146 0.03784186 -0.0045543443 -0.021308618 -0.014614365][0.14814839 0.27117324 0.38111073 0.44191006 0.45388693 0.43719831 0.38104847 0.2816202 0.16752079 0.079535432 0.029771261 0.0027867737 -0.0060229036 0.0020656586 0.023035714][0.13902463 0.24635926 0.33894593 0.38113526 0.37555495 0.34628174 0.28620198 0.19384421 0.097660363 0.033663988 0.00991895 0.0080301138 0.019502152 0.040034596 0.065593414][0.12015472 0.20734031 0.28027198 0.30555654 0.28746772 0.25267923 0.1982336 0.12352043 0.052403271 0.012635674 0.0077499202 0.018952325 0.037795592 0.060241338 0.083278462][0.080566682 0.14308941 0.19529475 0.20902519 0.18844166 0.15766002 0.11538032 0.061624289 0.014920893 -0.0053377766 0.00046628382 0.015297921 0.032889511 0.050737809 0.067290477][0.021829467 0.058116678 0.0899711 0.096986853 0.082040973 0.062069353 0.035169635 0.0016767833 -0.025178578 -0.032889429 -0.023805616 -0.011236077 0.0010058975 0.011843799 0.020831952]]...]
INFO - root - 2017-12-10 19:32:12.143401: step 46110, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 62h:22m:08s remains)
INFO - root - 2017-12-10 19:32:19.948839: step 46120, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.812 sec/batch; 64h:33m:56s remains)
INFO - root - 2017-12-10 19:32:27.801037: step 46130, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 62h:18m:34s remains)
INFO - root - 2017-12-10 19:32:35.627056: step 46140, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 64h:24m:53s remains)
INFO - root - 2017-12-10 19:32:43.612086: step 46150, loss = 0.69, batch loss = 0.63 (10.7 examples/sec; 0.748 sec/batch; 59h:30m:45s remains)
INFO - root - 2017-12-10 19:32:51.436027: step 46160, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 63h:04m:23s remains)
INFO - root - 2017-12-10 19:32:59.113623: step 46170, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 62h:04m:54s remains)
INFO - root - 2017-12-10 19:33:06.968865: step 46180, loss = 0.72, batch loss = 0.66 (9.8 examples/sec; 0.818 sec/batch; 65h:03m:35s remains)
INFO - root - 2017-12-10 19:33:14.913113: step 46190, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 61h:12m:02s remains)
INFO - root - 2017-12-10 19:33:22.598075: step 46200, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 60h:27m:12s remains)
2017-12-10 19:33:23.483435: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.058904368 0.064336687 0.065399952 0.065320313 0.066066884 0.064018086 0.059510268 0.057349592 0.064965867 0.091425687 0.13792905 0.19941507 0.25712177 0.28986189 0.28553033][0.077268451 0.078789555 0.079684891 0.085168429 0.095603965 0.10207595 0.099382967 0.090183057 0.087772779 0.10791689 0.15408836 0.22054447 0.28410015 0.31980398 0.31159925][0.10196061 0.10424822 0.11007275 0.12533973 0.14872624 0.16633521 0.16703777 0.15029028 0.13471754 0.14236817 0.17868236 0.23914833 0.29952174 0.33300796 0.31981364][0.13623607 0.14403874 0.15751834 0.18253914 0.21620175 0.24248002 0.24624485 0.22365022 0.19611743 0.1894265 0.21031208 0.25664237 0.30637538 0.33331591 0.31537476][0.17859897 0.19376341 0.21374643 0.24400333 0.28119788 0.31090662 0.31690249 0.29158321 0.25611341 0.23641554 0.24033347 0.26850152 0.3026382 0.31889826 0.29583824][0.21721151 0.24073431 0.26447114 0.29525137 0.33039531 0.35972711 0.36803886 0.3441985 0.3063252 0.27825978 0.26824257 0.27861214 0.29480183 0.29649258 0.26612154][0.25303143 0.28134516 0.30364776 0.32929793 0.35759711 0.38377747 0.39432222 0.37518075 0.34005439 0.31055087 0.29407138 0.29202473 0.29267532 0.27959436 0.24097596][0.28625461 0.31423184 0.33081374 0.34806594 0.36655733 0.38629058 0.39615566 0.38125929 0.35107195 0.32543585 0.31051552 0.30417359 0.29583904 0.27165878 0.22584257][0.31209758 0.33841231 0.35031894 0.36133224 0.3706741 0.38064894 0.38314492 0.36618167 0.33770531 0.31669515 0.30787146 0.30426821 0.29451585 0.26502195 0.21499552][0.32403424 0.35081381 0.36367273 0.37514681 0.38014027 0.37991506 0.36959982 0.34318382 0.31055996 0.29125327 0.28882051 0.29178602 0.28658393 0.25832832 0.20919026][0.32301375 0.35225368 0.37069702 0.3887682 0.3956306 0.38748285 0.36254153 0.32274556 0.28266022 0.26374808 0.26801363 0.27937394 0.28201514 0.26028606 0.21786362][0.32062545 0.3495104 0.37221146 0.39681149 0.40771726 0.39450222 0.35769331 0.30708033 0.2628414 0.24756572 0.26030004 0.2807948 0.29229188 0.27912652 0.24566978][0.31749016 0.33997178 0.36041275 0.38749984 0.40237486 0.38839209 0.34726992 0.29525977 0.25681508 0.252585 0.27751884 0.30836526 0.32798189 0.32067335 0.29133558][0.30541804 0.31399721 0.32435763 0.34845886 0.36553809 0.354839 0.31896815 0.27771723 0.25727695 0.27272764 0.3139855 0.35615715 0.38171506 0.37513971 0.34170577][0.28476554 0.27274972 0.26560584 0.27949673 0.29384127 0.28710553 0.26362631 0.24379064 0.25186718 0.29441091 0.35529551 0.40846646 0.4364908 0.4250901 0.38056862]]...]
INFO - root - 2017-12-10 19:33:31.358071: step 46210, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 63h:37m:23s remains)
INFO - root - 2017-12-10 19:33:39.196747: step 46220, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 61h:15m:02s remains)
INFO - root - 2017-12-10 19:33:47.149337: step 46230, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.828 sec/batch; 65h:49m:12s remains)
INFO - root - 2017-12-10 19:33:55.107911: step 46240, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 62h:43m:34s remains)
INFO - root - 2017-12-10 19:34:02.807776: step 46250, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 63h:11m:58s remains)
INFO - root - 2017-12-10 19:34:10.638874: step 46260, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 64h:05m:56s remains)
INFO - root - 2017-12-10 19:34:18.505884: step 46270, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 62h:23m:36s remains)
INFO - root - 2017-12-10 19:34:26.328602: step 46280, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 64h:45m:20s remains)
INFO - root - 2017-12-10 19:34:34.082732: step 46290, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 63h:20m:19s remains)
INFO - root - 2017-12-10 19:34:41.921825: step 46300, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 62h:11m:52s remains)
2017-12-10 19:34:42.821246: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28195879 0.27971596 0.26718226 0.24722563 0.21392994 0.17786089 0.16811734 0.20608903 0.2598069 0.29016703 0.28069705 0.23972739 0.17435016 0.0925865 0.015492913][0.28383979 0.28871992 0.28144827 0.26281953 0.22838679 0.19155395 0.18070316 0.21401237 0.26282924 0.288175 0.27286389 0.223055 0.14895268 0.062189315 -0.013473916][0.28206557 0.2952444 0.29527602 0.28166223 0.25124142 0.21925977 0.20985892 0.23610045 0.27451906 0.28929672 0.26408929 0.2034051 0.12107098 0.031373192 -0.040864002][0.29704794 0.31964338 0.32959229 0.32569954 0.304261 0.28003666 0.27190548 0.28878349 0.31150207 0.3092784 0.26829982 0.1937657 0.10267413 0.010377457 -0.059378404][0.32987195 0.35960114 0.37826833 0.38521394 0.37427211 0.35685676 0.3482697 0.35672298 0.36397597 0.34371024 0.28555214 0.19878469 0.10180735 0.0076820836 -0.061961267][0.37948346 0.41149306 0.43496406 0.45027947 0.44742727 0.43287468 0.42231226 0.42709455 0.42555752 0.39180315 0.31892073 0.22334217 0.12292877 0.02611899 -0.04781305][0.42796817 0.45709619 0.48122653 0.50227475 0.50495315 0.49039254 0.47866476 0.48718017 0.48796067 0.45027411 0.36941871 0.26992965 0.1680284 0.066190489 -0.017807877][0.44296384 0.46452156 0.48635384 0.51169866 0.51942837 0.506046 0.49821913 0.5187344 0.53240716 0.49989974 0.41792622 0.3189972 0.21716179 0.10937721 0.013349976][0.39696321 0.40915293 0.42798746 0.45680562 0.47048849 0.4624114 0.46446967 0.50127715 0.53259605 0.51112044 0.43423909 0.34010485 0.2412241 0.13097729 0.028013276][0.31928149 0.32158163 0.33559948 0.36515895 0.38393098 0.38364491 0.39692387 0.44536409 0.48784432 0.47454134 0.404294 0.31708854 0.22405085 0.1176952 0.017512422][0.25403243 0.24723426 0.25322914 0.27774751 0.29693073 0.30256316 0.32235953 0.37172005 0.4130325 0.40086806 0.33600852 0.2557824 0.17034179 0.073739104 -0.014190697][0.22659425 0.21288936 0.20643957 0.21593466 0.22493772 0.2277977 0.2442245 0.28344154 0.314076 0.2994521 0.24082869 0.169032 0.093467169 0.011461259 -0.057645168][0.25074756 0.23238419 0.20813391 0.19107112 0.17653239 0.16427498 0.1666714 0.18851545 0.2051301 0.18896528 0.13929789 0.078101493 0.01428465 -0.050818171 -0.0993192][0.31218746 0.28847542 0.24174294 0.1901225 0.1423683 0.10574614 0.088800341 0.09414497 0.10235838 0.0914941 0.0559949 0.0085338214 -0.042354729 -0.091645479 -0.12347161][0.38396609 0.35227558 0.28317115 0.19986813 0.12232824 0.064485937 0.033211175 0.029298432 0.037124492 0.0366617 0.016480053 -0.018481664 -0.059474338 -0.098572589 -0.12200987]]...]
INFO - root - 2017-12-10 19:34:50.656604: step 46310, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 62h:46m:41s remains)
INFO - root - 2017-12-10 19:34:58.492944: step 46320, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 63h:09m:28s remains)
INFO - root - 2017-12-10 19:35:06.124988: step 46330, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 63h:41m:57s remains)
INFO - root - 2017-12-10 19:35:13.928451: step 46340, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 60h:46m:41s remains)
INFO - root - 2017-12-10 19:35:21.793677: step 46350, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 62h:05m:53s remains)
INFO - root - 2017-12-10 19:35:29.682197: step 46360, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 63h:04m:44s remains)
INFO - root - 2017-12-10 19:35:37.363671: step 46370, loss = 0.68, batch loss = 0.63 (12.4 examples/sec; 0.644 sec/batch; 51h:09m:06s remains)
INFO - root - 2017-12-10 19:35:45.231676: step 46380, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 63h:04m:16s remains)
INFO - root - 2017-12-10 19:35:53.076337: step 46390, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 60h:22m:57s remains)
INFO - root - 2017-12-10 19:36:00.831121: step 46400, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 61h:34m:41s remains)
2017-12-10 19:36:01.648984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.072171912 -0.066178955 -0.048851971 -0.026622267 -0.0059284861 0.0076629063 0.013864283 0.013680817 0.0058339676 -0.008151819 -0.021833682 -0.028583761 -0.027461328 -0.021726072 -0.014720217][-0.067100927 -0.053866889 -0.025268475 0.010937968 0.047002412 0.076371767 0.096898705 0.10655185 0.1004034 0.0813737 0.061987296 0.054190371 0.05947816 0.068946674 0.077030845][-0.054169971 -0.03058671 0.013251614 0.069116488 0.12764415 0.1805992 0.2224187 0.24589351 0.24125347 0.21442039 0.18725745 0.17729938 0.18511117 0.19511095 0.20080827][-0.04142997 -0.0073763812 0.052702568 0.13074538 0.21565878 0.29681885 0.36313388 0.40047634 0.39486325 0.35866326 0.32455006 0.3130253 0.32022077 0.32365388 0.31958547][-0.033425204 0.0083013466 0.083065942 0.18242784 0.29384354 0.4027079 0.49059352 0.53700906 0.52586049 0.47968444 0.439672 0.42693862 0.43047217 0.42147526 0.4025616][-0.03090613 0.017155679 0.10567701 0.2261927 0.36399809 0.49888986 0.60398841 0.65260953 0.62886912 0.56756169 0.51740664 0.497921 0.49038422 0.46339253 0.43006942][-0.032159928 0.020070221 0.11991812 0.25869626 0.41914704 0.57522649 0.69232231 0.73733121 0.694309 0.60939395 0.53954053 0.50307304 0.47627872 0.42968455 0.38775972][-0.035737671 0.0183396 0.12488274 0.27378234 0.44583127 0.61136764 0.73199767 0.76868153 0.70309788 0.59005338 0.49443012 0.43595928 0.38929105 0.32816055 0.2842592][-0.039943241 0.013325158 0.12000228 0.26716974 0.4352116 0.59399343 0.70632696 0.73070353 0.64634311 0.51212764 0.3957063 0.3211278 0.26454097 0.20183897 0.16424423][-0.044865359 0.0047126506 0.10479674 0.23979354 0.39057887 0.52871186 0.62169105 0.63057476 0.5349822 0.39202809 0.26686004 0.18839999 0.13686024 0.089520961 0.069264516][-0.051737238 -0.0083773155 0.079918712 0.19667894 0.32334739 0.43412396 0.50237882 0.49583825 0.39789325 0.25955021 0.14035788 0.070735388 0.035739381 0.013364426 0.015408273][-0.059156176 -0.025889488 0.045291789 0.13797468 0.23517582 0.31493515 0.35748732 0.33967397 0.25042117 0.13186601 0.034174852 -0.01595469 -0.029314371 -0.027175149 -0.0082401428][-0.064395607 -0.042225786 0.0082514621 0.071947351 0.13518663 0.18167743 0.19974118 0.17648996 0.10670302 0.021858832 -0.042147007 -0.067001484 -0.061148621 -0.043783251 -0.020382218][-0.06896098 -0.057957143 -0.028992577 0.0051958566 0.035670035 0.0527217 0.0523454 0.030463146 -0.01339214 -0.059208341 -0.08695899 -0.0880856 -0.070196964 -0.048435826 -0.030150292][-0.077329509 -0.076999955 -0.067119874 -0.057383813 -0.051912751 -0.054654256 -0.064930715 -0.081658542 -0.10279229 -0.118031 -0.11967279 -0.10665103 -0.085609637 -0.066606462 -0.055451561]]...]
INFO - root - 2017-12-10 19:36:09.271234: step 46410, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 61h:59m:58s remains)
INFO - root - 2017-12-10 19:36:17.113325: step 46420, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 62h:23m:42s remains)
INFO - root - 2017-12-10 19:36:24.996850: step 46430, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 62h:30m:49s remains)
INFO - root - 2017-12-10 19:36:32.881482: step 46440, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 62h:49m:21s remains)
INFO - root - 2017-12-10 19:36:40.706109: step 46450, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.771 sec/batch; 61h:17m:49s remains)
INFO - root - 2017-12-10 19:36:48.444794: step 46460, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 61h:19m:18s remains)
INFO - root - 2017-12-10 19:36:56.372666: step 46470, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 61h:42m:08s remains)
INFO - root - 2017-12-10 19:37:04.314123: step 46480, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 63h:19m:07s remains)
INFO - root - 2017-12-10 19:37:12.001407: step 46490, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 61h:15m:57s remains)
INFO - root - 2017-12-10 19:37:19.768323: step 46500, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.759 sec/batch; 60h:15m:42s remains)
2017-12-10 19:37:20.621056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.092619963 -0.090767443 -0.084176414 -0.075952232 -0.065781057 -0.055640195 -0.046905495 -0.043054469 -0.0438082 -0.048170846 -0.054172229 -0.059316624 -0.062532924 -0.062824436 -0.064212866][-0.079217575 -0.067675956 -0.050041515 -0.03063179 -0.0086181434 0.011668601 0.026551077 0.029548947 0.023308337 0.011989041 -0.00077340991 -0.010631167 -0.01619477 -0.016791597 -0.020303348][-0.051269814 -0.022182195 0.014906608 0.053484518 0.093609907 0.12665197 0.14588162 0.14168787 0.12132205 0.0943282 0.067441374 0.047248028 0.035834108 0.033955134 0.028767141][-0.010937363 0.045168124 0.11333225 0.18237165 0.2492713 0.29975045 0.32255185 0.30523035 0.26074934 0.20627661 0.15290599 0.11063552 0.084319256 0.075413465 0.066624232][0.033116587 0.12228479 0.22955751 0.33902338 0.44357163 0.52144885 0.55324733 0.52310538 0.44929007 0.35597527 0.2603963 0.17970744 0.12635714 0.10320778 0.088360749][0.0696475 0.19038025 0.33537966 0.48774943 0.6366421 0.75069749 0.79851776 0.76047593 0.65753531 0.51842278 0.36951721 0.2407198 0.15555035 0.11692754 0.09715493][0.090931721 0.23352633 0.40558237 0.59286463 0.782106 0.93177336 0.99705976 0.9557178 0.82880729 0.64729792 0.44823024 0.27634603 0.16580433 0.11669562 0.095478334][0.093429051 0.24371371 0.42631012 0.63222373 0.84678477 1.0195953 1.0957215 1.053673 0.91408503 0.70752382 0.4795354 0.28522274 0.16347972 0.10997324 0.088165179][0.075529695 0.21645018 0.38903898 0.59105462 0.80820304 0.98473239 1.0615884 1.0217311 0.88587016 0.681245 0.45616287 0.26705897 0.15120018 0.10014391 0.078222767][0.038447145 0.15300584 0.29499424 0.46842557 0.661417 0.81842124 0.8839035 0.84733 0.72988117 0.55360943 0.36259028 0.20607026 0.11431333 0.075828269 0.058611743][-0.010394447 0.066856407 0.16472568 0.2903263 0.43549874 0.552074 0.59614962 0.5637033 0.47491997 0.3457303 0.21041173 0.10555469 0.050608616 0.032751404 0.025458237][-0.057345632 -0.017693965 0.035598475 0.10831767 0.19621769 0.26407248 0.28410348 0.25716037 0.2002259 0.12317892 0.04813673 -0.0027988493 -0.021294001 -0.019375753 -0.016673306][-0.091553465 -0.08029294 -0.061136112 -0.031751174 0.0065976451 0.033069637 0.03446563 0.014146444 -0.015692567 -0.049756527 -0.076812364 -0.087281182 -0.0808792 -0.066933006 -0.058096729][-0.11266688 -0.11913463 -0.12106478 -0.11920276 -0.11279783 -0.11109237 -0.11853026 -0.13160728 -0.14304429 -0.15012848 -0.14906855 -0.13804966 -0.1204623 -0.10230668 -0.09074647][-0.12159726 -0.1365799 -0.14795014 -0.15842441 -0.1662765 -0.17410259 -0.18285915 -0.19012135 -0.19206239 -0.18714048 -0.17521213 -0.1575525 -0.13809329 -0.12050147 -0.10849531]]...]
INFO - root - 2017-12-10 19:37:28.512977: step 46510, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 63h:22m:21s remains)
INFO - root - 2017-12-10 19:37:36.229327: step 46520, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 62h:05m:31s remains)
INFO - root - 2017-12-10 19:37:44.063337: step 46530, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 62h:01m:59s remains)
INFO - root - 2017-12-10 19:37:51.842144: step 46540, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 64h:13m:32s remains)
INFO - root - 2017-12-10 19:37:59.539659: step 46550, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 60h:28m:11s remains)
INFO - root - 2017-12-10 19:38:07.282464: step 46560, loss = 0.67, batch loss = 0.61 (10.3 examples/sec; 0.775 sec/batch; 61h:34m:06s remains)
INFO - root - 2017-12-10 19:38:14.918513: step 46570, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 61h:44m:13s remains)
INFO - root - 2017-12-10 19:38:22.751121: step 46580, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 61h:12m:47s remains)
INFO - root - 2017-12-10 19:38:30.636606: step 46590, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 62h:34m:44s remains)
INFO - root - 2017-12-10 19:38:38.437969: step 46600, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 63h:43m:41s remains)
2017-12-10 19:38:39.252390: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0785837 0.10583334 0.15015665 0.21412644 0.29453889 0.36596692 0.40069664 0.38217539 0.30789959 0.20345993 0.10222705 0.035903752 0.012733632 0.021702303 0.042227939][0.087551691 0.11798937 0.1563963 0.20322877 0.2585313 0.30643159 0.32735002 0.30837509 0.24814567 0.16707622 0.093681127 0.052828297 0.047361474 0.064604618 0.085037395][0.11329766 0.15511452 0.19333939 0.22656453 0.25982898 0.29044178 0.30719021 0.29732516 0.25609663 0.19529609 0.14181101 0.11892478 0.1273292 0.15228301 0.17339638][0.15562254 0.21681297 0.26387927 0.29513478 0.32229865 0.35501671 0.3858166 0.39611849 0.37228775 0.31646448 0.26139742 0.23807839 0.25088957 0.28183717 0.30480465][0.21888526 0.30592158 0.37023085 0.41229537 0.45118448 0.50550222 0.566933 0.60544646 0.59303433 0.52284163 0.44070277 0.39685261 0.40480161 0.4420343 0.47102287][0.29503709 0.40960482 0.496704 0.55899638 0.6206637 0.70576429 0.80237478 0.86745453 0.85653985 0.75645453 0.62935472 0.55091345 0.54700935 0.59007567 0.62920469][0.36285481 0.49712455 0.60295665 0.68370879 0.76616031 0.87727678 1.0017864 1.0858574 1.0711949 0.94163531 0.77154875 0.656614 0.63314474 0.67344779 0.71894795][0.39793348 0.53270352 0.63900578 0.7213797 0.80998588 0.93442708 1.0762601 1.1731476 1.1594344 1.0175031 0.82290715 0.67784232 0.62711006 0.6501143 0.68940085][0.3762984 0.49085939 0.57762915 0.643235 0.7213766 0.84273475 0.98689044 1.0883113 1.0815552 0.94849771 0.75477618 0.59515619 0.518478 0.51485085 0.53520894][0.29822111 0.3814531 0.43943155 0.47889417 0.53405839 0.63502073 0.7626738 0.8565647 0.858209 0.75001568 0.58094251 0.42827523 0.339043 0.31249258 0.3116748][0.18658654 0.23640494 0.26561531 0.27865216 0.30623096 0.37565243 0.47243884 0.547625 0.55431777 0.47721925 0.34764081 0.22073418 0.13596843 0.098260477 0.083749011][0.072070107 0.093739405 0.1006684 0.093998939 0.097651526 0.13422556 0.19487421 0.24465489 0.25060102 0.20126548 0.11402498 0.022871815 -0.04328531 -0.078071386 -0.095312111][-0.020824514 -0.017335141 -0.022420179 -0.038142253 -0.048825495 -0.038035657 -0.00981836 0.014472367 0.015010984 -0.01489172 -0.066508532 -0.12170063 -0.16214244 -0.18369429 -0.19454265][-0.078695133 -0.082926214 -0.090201855 -0.10518551 -0.11980742 -0.12381249 -0.11801142 -0.11271317 -0.11760798 -0.13531058 -0.16171877 -0.18865083 -0.20634398 -0.21370701 -0.21587715][-0.0981939 -0.10327294 -0.10755567 -0.11688098 -0.12866659 -0.13758191 -0.14282882 -0.14757396 -0.15513587 -0.16548301 -0.17673995 -0.18610108 -0.189429 -0.1875747 -0.18378007]]...]
INFO - root - 2017-12-10 19:38:47.122413: step 46610, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 62h:38m:44s remains)
INFO - root - 2017-12-10 19:38:55.013924: step 46620, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 61h:30m:00s remains)
INFO - root - 2017-12-10 19:39:02.931053: step 46630, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 63h:27m:26s remains)
INFO - root - 2017-12-10 19:39:10.639223: step 46640, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 63h:29m:57s remains)
INFO - root - 2017-12-10 19:39:18.376170: step 46650, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.777 sec/batch; 61h:43m:26s remains)
INFO - root - 2017-12-10 19:39:26.177168: step 46660, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.772 sec/batch; 61h:16m:55s remains)
INFO - root - 2017-12-10 19:39:34.097255: step 46670, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 63h:41m:56s remains)
INFO - root - 2017-12-10 19:39:42.016628: step 46680, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 63h:32m:34s remains)
INFO - root - 2017-12-10 19:39:49.840719: step 46690, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.762 sec/batch; 60h:28m:55s remains)
INFO - root - 2017-12-10 19:39:57.599953: step 46700, loss = 0.67, batch loss = 0.62 (10.4 examples/sec; 0.772 sec/batch; 61h:16m:53s remains)
2017-12-10 19:39:58.457887: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0048933565 0.0064726872 0.028239312 0.081144683 0.16462263 0.25764722 0.33345553 0.37162867 0.36806402 0.33876345 0.30422053 0.28373304 0.28761315 0.31848761 0.36188138][-0.019531708 -0.010688874 0.021519357 0.086063594 0.18025576 0.28338027 0.36713263 0.4080961 0.39802238 0.353348 0.30031332 0.26097029 0.24719429 0.265107 0.30441526][-0.041704163 -0.028375527 0.0093169557 0.079011038 0.17746982 0.28575966 0.37539002 0.42052633 0.40872562 0.35366812 0.28421926 0.22495639 0.18936284 0.18641165 0.21268223][-0.053110141 -0.039140031 -0.0014468422 0.066907547 0.16361077 0.27253154 0.36559242 0.41541007 0.40595651 0.34586892 0.26359457 0.18489756 0.12600026 0.098809794 0.10623484][-0.052985307 -0.041508436 -0.0080156447 0.054403055 0.14575605 0.25318137 0.34948316 0.40584943 0.40237078 0.34203893 0.2501564 0.15372624 0.073225111 0.023354871 0.011552415][-0.048791554 -0.041205768 -0.014474549 0.03895798 0.12215929 0.2263571 0.32613429 0.39190993 0.39996636 0.34685269 0.25309938 0.14624612 0.051213168 -0.014848061 -0.04196975][-0.045040309 -0.041799482 -0.023102468 0.018732117 0.089809343 0.1861995 0.28611276 0.36119568 0.38545161 0.34929702 0.26696724 0.16419905 0.068046212 -0.0029057313 -0.038278237][-0.042627472 -0.04307919 -0.032147996 -0.0025860253 0.054063383 0.13845064 0.23408076 0.31604737 0.35847384 0.34746441 0.29003775 0.2077505 0.12586243 0.062008433 0.025358163][-0.040447276 -0.043220267 -0.0375397 -0.017089348 0.027846331 0.10135446 0.19251084 0.28086117 0.34217125 0.36054382 0.33622378 0.28517029 0.22825478 0.1797342 0.14640689][-0.038833629 -0.042681925 -0.0400863 -0.025447419 0.011275643 0.076351613 0.16424654 0.25943685 0.33955055 0.38711855 0.39674953 0.37883049 0.34873888 0.31681228 0.28791156][-0.038056567 -0.042375062 -0.041941315 -0.031585939 -0.0013879776 0.056200169 0.14055674 0.24121751 0.33718726 0.40868902 0.44574526 0.45366746 0.442801 0.42053241 0.39244008][-0.034373339 -0.038451862 -0.039034516 -0.031362008 -0.0061544115 0.044700526 0.12440266 0.22673737 0.33155453 0.41681117 0.46910551 0.48969528 0.48477724 0.46092543 0.42673624][-0.025630465 -0.028666735 -0.029133547 -0.022606226 -0.0010961533 0.043107636 0.1151867 0.21194507 0.31484568 0.40107679 0.45547739 0.47645235 0.46749514 0.43568027 0.39250818][-0.015291779 -0.017171873 -0.017265519 -0.011680626 0.0059285206 0.041988313 0.10196889 0.18468326 0.27424744 0.3492665 0.39500844 0.40887952 0.393188 0.35517809 0.30761257][-0.0065692407 -0.0077858814 -0.008271344 -0.0051272167 0.0062773288 0.030762471 0.073254369 0.13416202 0.2012226 0.25645185 0.28747103 0.29227859 0.27281761 0.23545812 0.191986]]...]
INFO - root - 2017-12-10 19:40:06.277603: step 46710, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 62h:03m:17s remains)
INFO - root - 2017-12-10 19:40:14.037855: step 46720, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 60h:31m:25s remains)
INFO - root - 2017-12-10 19:40:21.735366: step 46730, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 62h:34m:39s remains)
INFO - root - 2017-12-10 19:40:29.770846: step 46740, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 61h:38m:54s remains)
INFO - root - 2017-12-10 19:40:37.609395: step 46750, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 62h:58m:51s remains)
INFO - root - 2017-12-10 19:40:45.511462: step 46760, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 62h:03m:05s remains)
INFO - root - 2017-12-10 19:40:53.286231: step 46770, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 61h:39m:59s remains)
INFO - root - 2017-12-10 19:41:01.184539: step 46780, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 64h:07m:24s remains)
INFO - root - 2017-12-10 19:41:09.181716: step 46790, loss = 0.68, batch loss = 0.62 (8.8 examples/sec; 0.906 sec/batch; 71h:55m:52s remains)
INFO - root - 2017-12-10 19:41:17.046115: step 46800, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 63h:44m:34s remains)
2017-12-10 19:41:17.890562: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24846868 0.23974222 0.21402165 0.19387823 0.18720701 0.18785761 0.18543822 0.17656226 0.16410211 0.14254628 0.1204847 0.11302104 0.12727597 0.14928985 0.16601345][0.2498409 0.2515873 0.2428585 0.24226466 0.25385052 0.2664609 0.26559487 0.24922332 0.22701105 0.19909874 0.17244375 0.15959741 0.16940083 0.18911535 0.20248693][0.24183121 0.2647098 0.28235987 0.30724624 0.34111676 0.36975825 0.37244105 0.34559223 0.30727717 0.26569489 0.22792237 0.20402 0.20416556 0.21898434 0.22892433][0.23556544 0.28445214 0.32980722 0.37621737 0.425544 0.46488422 0.47060779 0.43696028 0.38680783 0.334103 0.28563941 0.25027168 0.24044208 0.25006115 0.25534698][0.23612937 0.3069495 0.37132969 0.42695916 0.47665539 0.51440293 0.51989919 0.48636991 0.43609115 0.38336524 0.33315542 0.29337958 0.27767676 0.28172746 0.28022689][0.24223527 0.32390624 0.39418843 0.44520748 0.48019552 0.50352794 0.50434351 0.47700027 0.4390744 0.40026218 0.3610428 0.32653883 0.30905318 0.30615136 0.29668745][0.24453884 0.32198322 0.38434845 0.42110202 0.43496674 0.43928242 0.43332368 0.41336781 0.39210573 0.37351143 0.353938 0.33339906 0.31990072 0.31291321 0.29897782][0.23067059 0.28796557 0.32812688 0.34367794 0.33751288 0.32587391 0.31423944 0.30176619 0.29765594 0.30026177 0.30247653 0.30104411 0.29901493 0.2965695 0.28673911][0.19829257 0.23002063 0.24380028 0.23912436 0.21874717 0.19733937 0.18295895 0.17864281 0.19067135 0.21172684 0.23133661 0.24526517 0.25560197 0.26333547 0.2642152][0.16706224 0.17711169 0.16971721 0.1513074 0.12362891 0.09775658 0.08201097 0.082904577 0.10507405 0.13827915 0.17091025 0.19875935 0.22324757 0.24485786 0.26042458][0.14894278 0.14493059 0.12606658 0.10251297 0.07422623 0.048240066 0.0319074 0.033371266 0.0567949 0.09513171 0.13917366 0.18331666 0.22577515 0.26428553 0.29481167][0.13754651 0.12854294 0.10866361 0.088322759 0.065126114 0.042788934 0.027053326 0.025555864 0.043331023 0.08096204 0.13412386 0.19437172 0.25469127 0.30799922 0.34878105][0.13512433 0.12490544 0.10641024 0.09034773 0.073712625 0.057876449 0.046319366 0.044340234 0.05728732 0.0928554 0.15073545 0.21935847 0.28762978 0.34536341 0.38686213][0.14505379 0.14040907 0.1271435 0.11694922 0.10734531 0.098592564 0.091881335 0.090302251 0.099394232 0.13039742 0.18507133 0.24947041 0.31060296 0.35783452 0.38750902][0.1709456 0.17689751 0.17049195 0.16409968 0.157332 0.1511519 0.14603242 0.144184 0.15056816 0.17591123 0.22254226 0.27517208 0.31996959 0.34692684 0.35635775]]...]
INFO - root - 2017-12-10 19:41:25.670736: step 46810, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 64h:06m:19s remains)
INFO - root - 2017-12-10 19:41:33.411495: step 46820, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 61h:42m:39s remains)
INFO - root - 2017-12-10 19:41:41.245518: step 46830, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 61h:59m:28s remains)
INFO - root - 2017-12-10 19:41:49.118936: step 46840, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 64h:29m:09s remains)
INFO - root - 2017-12-10 19:41:57.076114: step 46850, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 61h:13m:18s remains)
INFO - root - 2017-12-10 19:42:04.931440: step 46860, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 62h:29m:13s remains)
INFO - root - 2017-12-10 19:42:12.815036: step 46870, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 60h:23m:09s remains)
INFO - root - 2017-12-10 19:42:20.671938: step 46880, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.764 sec/batch; 60h:37m:09s remains)
INFO - root - 2017-12-10 19:42:28.354940: step 46890, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 63h:33m:07s remains)
INFO - root - 2017-12-10 19:42:36.261678: step 46900, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 63h:03m:12s remains)
2017-12-10 19:42:37.142965: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15217735 0.15795061 0.16905774 0.18243223 0.1943372 0.20833784 0.22243173 0.23328629 0.23198652 0.22022893 0.22006874 0.24533246 0.28544784 0.30789194 0.30847165][0.11469042 0.13089542 0.15369563 0.17465214 0.19152638 0.20832267 0.21969244 0.22035807 0.20755641 0.19038856 0.18880537 0.21338886 0.25811383 0.29249445 0.30935702][0.11391816 0.13846956 0.16657108 0.18452397 0.19372445 0.20290537 0.20694339 0.19730277 0.17480738 0.1525517 0.14758399 0.16810863 0.21187794 0.25251344 0.28229415][0.1585317 0.18548082 0.20973784 0.21281308 0.20222981 0.19647489 0.19416259 0.18113285 0.15704286 0.13410722 0.12596054 0.13815677 0.17197172 0.20782191 0.24085334][0.22726361 0.25339794 0.26940998 0.25335315 0.22015199 0.20059764 0.19883966 0.19318308 0.17697655 0.15868294 0.14866187 0.1499646 0.16601422 0.18514825 0.20897533][0.28666714 0.31355372 0.32334304 0.29383877 0.2464574 0.22200078 0.22906636 0.23833042 0.23500101 0.22449403 0.21482447 0.20598683 0.20129694 0.196819 0.2024736][0.30491829 0.33536917 0.34585446 0.31691736 0.27236432 0.25577873 0.27729383 0.30378649 0.31486055 0.31447437 0.30742851 0.2910009 0.26772344 0.24046904 0.22568046][0.27686328 0.31168643 0.32805818 0.31140614 0.2837806 0.28300098 0.31805885 0.35826668 0.38324392 0.395639 0.3946816 0.37665933 0.34348059 0.30274928 0.27307934][0.2187466 0.25616997 0.27935687 0.2791869 0.27251977 0.28614527 0.327247 0.37319097 0.40859753 0.4336201 0.44110385 0.42876962 0.39798385 0.35823631 0.32492116][0.15228571 0.1877785 0.21382853 0.22590573 0.23453966 0.25456795 0.29184103 0.33402088 0.37286445 0.40484282 0.41882032 0.41662839 0.39926395 0.37489223 0.35047665][0.0938391 0.12073565 0.14293283 0.15878756 0.17328078 0.19127052 0.21729408 0.24778132 0.28105137 0.31047451 0.32532057 0.33333576 0.33560374 0.33537385 0.32813406][0.060056996 0.073265076 0.086143211 0.098844 0.11257909 0.1256386 0.13993134 0.15611355 0.17609461 0.19276769 0.19999292 0.21121086 0.227767 0.24655354 0.25377825][0.053362668 0.053805619 0.057099048 0.065335833 0.077722967 0.088319249 0.095387124 0.099399872 0.10277741 0.10061943 0.093058422 0.096716933 0.11319267 0.13574888 0.14753753][0.069566682 0.058910098 0.053740382 0.059197113 0.073166959 0.086250141 0.093220659 0.092100434 0.0830891 0.064084947 0.040102609 0.028107239 0.030669218 0.042097971 0.048925053][0.10324563 0.079770967 0.06463483 0.067054018 0.083791494 0.10275026 0.11665747 0.12026469 0.10993723 0.08487381 0.051730357 0.025549605 0.010320088 0.004479046 0.0015870057]]...]
INFO - root - 2017-12-10 19:42:44.807244: step 46910, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 62h:27m:23s remains)
INFO - root - 2017-12-10 19:42:52.659736: step 46920, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 61h:27m:58s remains)
INFO - root - 2017-12-10 19:43:00.488741: step 46930, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 63h:20m:05s remains)
INFO - root - 2017-12-10 19:43:08.286638: step 46940, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 59h:57m:22s remains)
INFO - root - 2017-12-10 19:43:16.083685: step 46950, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 62h:35m:35s remains)
INFO - root - 2017-12-10 19:43:23.991179: step 46960, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.816 sec/batch; 64h:43m:27s remains)
INFO - root - 2017-12-10 19:43:31.667436: step 46970, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 60h:54m:46s remains)
INFO - root - 2017-12-10 19:43:39.538302: step 46980, loss = 0.68, batch loss = 0.63 (10.7 examples/sec; 0.749 sec/batch; 59h:25m:47s remains)
INFO - root - 2017-12-10 19:43:47.361877: step 46990, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 61h:09m:35s remains)
INFO - root - 2017-12-10 19:43:54.956169: step 47000, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 61h:53m:34s remains)
2017-12-10 19:43:55.755314: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1138815 0.10413286 0.083497263 0.059906919 0.038583636 0.019647129 0.0073432219 0.0072743227 0.024955424 0.054207429 0.082859226 0.10458285 0.11764133 0.11763934 0.10138505][0.088804826 0.081543617 0.066419587 0.048303623 0.030787155 0.01518698 0.0062605287 0.0083428053 0.027268566 0.0578932 0.087548748 0.10870188 0.11928911 0.11773902 0.10270443][0.072496064 0.070413187 0.06238611 0.050503045 0.038658019 0.030011328 0.028121576 0.033970714 0.053001363 0.081355609 0.10711651 0.12314811 0.1283436 0.12506273 0.11349212][0.06933327 0.071889877 0.069844753 0.064312465 0.060944956 0.063817233 0.073834263 0.086608894 0.1059617 0.12918264 0.1461301 0.15156855 0.14672749 0.13888867 0.12950036][0.077057734 0.082161516 0.084033124 0.085117891 0.092771515 0.1106946 0.13655232 0.15994731 0.18169914 0.19994086 0.20682201 0.19951667 0.18103254 0.16320191 0.14956199][0.10283248 0.10780181 0.10950717 0.11560769 0.13421321 0.16645788 0.20707402 0.24047984 0.26465145 0.27859181 0.27699929 0.25888863 0.22687714 0.19531059 0.17103735][0.14829475 0.15059784 0.14744273 0.15513463 0.1810715 0.22212823 0.27079779 0.30836859 0.33176014 0.34248531 0.33736727 0.31545851 0.2761921 0.23377189 0.19881634][0.20509084 0.20085338 0.18865333 0.19388469 0.22163376 0.26379266 0.31100842 0.34428376 0.36300796 0.3730287 0.3721604 0.3564828 0.32057065 0.27669111 0.23853831][0.25456765 0.24223748 0.221031 0.22247569 0.24735612 0.28352833 0.32076591 0.34296542 0.35493404 0.36756355 0.37809473 0.37722021 0.35444698 0.31955189 0.28761634][0.28036356 0.26090237 0.23472098 0.23361452 0.25364548 0.28068736 0.30454382 0.31402022 0.31991521 0.3368465 0.36057112 0.37651592 0.37097359 0.35116675 0.33049589][0.26817244 0.24844901 0.22566481 0.22563143 0.24161623 0.26053193 0.27300557 0.27235663 0.27348405 0.29245418 0.32393748 0.35089517 0.35946059 0.3541356 0.34350929][0.21830693 0.20624395 0.19506651 0.20076275 0.21653824 0.2324554 0.23959355 0.23390545 0.23117562 0.24753864 0.27870339 0.30905494 0.32654017 0.3317506 0.3262403][0.15800251 0.15677595 0.15849306 0.17067845 0.18901908 0.20642927 0.21316084 0.20535508 0.19807352 0.20771582 0.23330088 0.26301172 0.28600952 0.29715255 0.29077172][0.11779787 0.12472471 0.13467847 0.15032487 0.17119706 0.19162519 0.19904704 0.18828566 0.17423615 0.17560492 0.19576883 0.22636688 0.254588 0.26733533 0.25414702][0.11031599 0.12196305 0.1335324 0.147252 0.1677161 0.18913169 0.19515142 0.17901857 0.15743057 0.1531128 0.17280971 0.20886029 0.24329726 0.25509909 0.2330644]]...]
INFO - root - 2017-12-10 19:44:03.512674: step 47010, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 61h:33m:53s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 19:44:11.401523: step 47020, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 62h:34m:48s remains)
INFO - root - 2017-12-10 19:44:19.230234: step 47030, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 62h:53m:12s remains)
INFO - root - 2017-12-10 19:44:27.083661: step 47040, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 62h:41m:52s remains)
INFO - root - 2017-12-10 19:44:34.795933: step 47050, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.814 sec/batch; 64h:30m:43s remains)
INFO - root - 2017-12-10 19:44:42.663572: step 47060, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 61h:51m:49s remains)
INFO - root - 2017-12-10 19:44:50.516261: step 47070, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 62h:35m:43s remains)
INFO - root - 2017-12-10 19:44:58.363272: step 47080, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 62h:16m:49s remains)
INFO - root - 2017-12-10 19:45:06.027967: step 47090, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 60h:19m:10s remains)
INFO - root - 2017-12-10 19:45:13.851771: step 47100, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 60h:49m:27s remains)
2017-12-10 19:45:14.656494: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.039616343 0.029724592 0.024325002 0.023222234 0.02429709 0.027362294 0.031310651 0.034582194 0.035799049 0.034172986 0.030789243 0.026873058 0.023924956 0.034402374 0.058917437][0.013216549 0.0027105007 -0.0057216017 -0.0091193449 -0.0076802885 -0.0017301963 0.0071578403 0.015646743 0.020160509 0.018421071 0.011892167 0.0027663533 -0.0060501695 -0.0053268522 0.0060819923][0.029559884 0.017519848 0.0041111023 -0.0033776381 -0.002879153 0.0057320227 0.02086311 0.037151713 0.048214503 0.049241591 0.040464234 0.025268747 0.0074188444 -0.0054299938 -0.010677843][0.10590643 0.092531063 0.071023442 0.054450057 0.049742814 0.060081497 0.08436989 0.11470615 0.14049895 0.15137103 0.14299808 0.11892886 0.085077129 0.050511029 0.021850614][0.23003229 0.21647091 0.18345973 0.15203823 0.13757387 0.14922783 0.18634981 0.23877077 0.289419 0.31823149 0.31283647 0.276073 0.21889967 0.15440632 0.095481478][0.353698 0.3412818 0.29759157 0.25105765 0.22708187 0.24169423 0.29385549 0.371727 0.45048213 0.49910226 0.49660859 0.44544569 0.36355245 0.2685312 0.17975616][0.42197195 0.4115749 0.36415493 0.31095174 0.28442606 0.30479372 0.36932904 0.46522108 0.56207889 0.62194544 0.61933 0.55749577 0.45994449 0.34687114 0.24122295][0.39550233 0.38738373 0.34538552 0.297626 0.27715713 0.30341 0.37146804 0.46889052 0.56495976 0.621701 0.61515963 0.55015892 0.45254788 0.34225351 0.2409983][0.28556439 0.27847582 0.24775383 0.21428208 0.20485689 0.23340611 0.29355517 0.375584 0.45387337 0.49696004 0.48658705 0.42878327 0.34717584 0.25794905 0.17851713][0.14479154 0.13836011 0.1207364 0.1045409 0.10676747 0.13375339 0.17869918 0.23579033 0.28762403 0.31328535 0.30198646 0.25979602 0.2039047 0.145204 0.096161149][0.034804214 0.028343838 0.021681489 0.020386398 0.031424217 0.05446209 0.083214223 0.11518677 0.14120264 0.15086776 0.14036208 0.11503397 0.084796868 0.057106823 0.040511835][-0.0027160817 -0.011788636 -0.014067609 -0.0073693087 0.0070464388 0.025050871 0.041541398 0.056013938 0.064890705 0.064852051 0.056474827 0.044008113 0.031793252 0.028502248 0.040950332][0.033299189 0.020229382 0.015678495 0.022931281 0.035272341 0.048085287 0.057909105 0.064955473 0.0676639 0.065219887 0.059748713 0.054291688 0.050121076 0.061632581 0.0957515][0.10911265 0.09342365 0.084997661 0.089723542 0.098167457 0.10697285 0.11416954 0.11999366 0.12298814 0.12222138 0.11967728 0.11718041 0.11469001 0.13373323 0.1816078][0.17011012 0.15527873 0.14519897 0.14774998 0.15292157 0.15871988 0.16411899 0.16924629 0.17273819 0.17329386 0.17242168 0.17106745 0.16876774 0.19177094 0.2464986]]...]
INFO - root - 2017-12-10 19:45:22.458097: step 47110, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.762 sec/batch; 60h:24m:51s remains)
INFO - root - 2017-12-10 19:45:30.276725: step 47120, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 62h:37m:26s remains)
INFO - root - 2017-12-10 19:45:37.976354: step 47130, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 62h:36m:19s remains)
INFO - root - 2017-12-10 19:45:45.762001: step 47140, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 60h:01m:26s remains)
INFO - root - 2017-12-10 19:45:53.662514: step 47150, loss = 0.72, batch loss = 0.66 (9.7 examples/sec; 0.826 sec/batch; 65h:29m:58s remains)
INFO - root - 2017-12-10 19:46:01.447836: step 47160, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 63h:37m:36s remains)
INFO - root - 2017-12-10 19:46:09.149051: step 47170, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 62h:48m:48s remains)
INFO - root - 2017-12-10 19:46:16.977580: step 47180, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 59h:58m:27s remains)
INFO - root - 2017-12-10 19:46:24.819429: step 47190, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.776 sec/batch; 61h:31m:53s remains)
INFO - root - 2017-12-10 19:46:32.645481: step 47200, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 63h:57m:33s remains)
2017-12-10 19:46:33.472993: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.06369409 0.0730952 0.08613953 0.098300539 0.10257663 0.099531926 0.092289008 0.08733689 0.089293323 0.095884576 0.099585995 0.092657827 0.074798092 0.051744692 0.032813136][0.09608084 0.10605666 0.11873374 0.12961732 0.13128193 0.12535058 0.11691517 0.11457512 0.1227653 0.13644722 0.14466837 0.1372188 0.1131659 0.079197533 0.048112664][0.14250356 0.15193994 0.16286415 0.17247568 0.17270562 0.16496398 0.15616706 0.1568812 0.17105684 0.19059399 0.20188206 0.19393973 0.16543688 0.12366946 0.084222734][0.20695269 0.21906205 0.23317625 0.24775895 0.25291914 0.24755287 0.23945872 0.24118181 0.25637129 0.27417704 0.27982074 0.26451406 0.22862291 0.18053506 0.13795622][0.279043 0.29950577 0.3225981 0.34898937 0.36582625 0.36766389 0.36178628 0.36173138 0.37163264 0.379211 0.37036422 0.34027314 0.29266781 0.23830006 0.19629653][0.33924434 0.371881 0.40944153 0.45362595 0.48798782 0.50208306 0.50090307 0.49722359 0.49613973 0.4856185 0.45392168 0.4013415 0.33698991 0.27494574 0.23466977][0.36053663 0.40551683 0.45834848 0.52098972 0.57389736 0.602669 0.60922283 0.60374087 0.59186727 0.56277955 0.50813633 0.43313676 0.35238057 0.28327525 0.24381623][0.32481849 0.37482142 0.43480587 0.50717473 0.571973 0.61381739 0.63157254 0.6303609 0.61426246 0.57366163 0.50376648 0.41345352 0.32159626 0.24777354 0.20764472][0.24717753 0.29325637 0.34915009 0.41685346 0.47917756 0.52445936 0.55056459 0.5571028 0.54465455 0.50359547 0.43114644 0.33771646 0.24444175 0.17190583 0.13334188][0.15820381 0.19328791 0.23512642 0.28454542 0.32917312 0.36427405 0.38964286 0.40095338 0.39488873 0.36170834 0.29941902 0.21762003 0.13659833 0.076099321 0.04689613][0.10624179 0.13062349 0.15515211 0.1789885 0.19551887 0.2080956 0.22068761 0.22816254 0.22452776 0.20058466 0.15452088 0.093454808 0.034051917 -0.0067504658 -0.020839196][0.11445417 0.13212602 0.14112437 0.14023447 0.12843333 0.1159249 0.11128329 0.11076342 0.10637841 0.089175254 0.057061385 0.015040415 -0.023914907 -0.046168614 -0.046212751][0.14944027 0.16370711 0.16249079 0.14566214 0.11609031 0.087501809 0.0711702 0.065339588 0.060656119 0.048011653 0.025002753 -0.0040476383 -0.028443968 -0.037590578 -0.028181328][0.16592798 0.17805937 0.17347212 0.15299068 0.12065825 0.089631028 0.071158923 0.064986244 0.061578877 0.052444503 0.035595391 0.015743544 0.00180081 0.0013131638 0.015539914][0.13558182 0.14626682 0.14453742 0.13153677 0.1103407 0.09040916 0.080260731 0.079916984 0.080635443 0.075580753 0.0642305 0.051958103 0.045841437 0.050262578 0.06449654]]...]
INFO - root - 2017-12-10 19:46:41.214533: step 47210, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.812 sec/batch; 64h:19m:09s remains)
INFO - root - 2017-12-10 19:46:49.212227: step 47220, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 60h:55m:41s remains)
INFO - root - 2017-12-10 19:46:57.167030: step 47230, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.820 sec/batch; 64h:58m:13s remains)
INFO - root - 2017-12-10 19:47:05.141713: step 47240, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 61h:38m:45s remains)
INFO - root - 2017-12-10 19:47:13.061043: step 47250, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 61h:02m:06s remains)
INFO - root - 2017-12-10 19:47:20.704499: step 47260, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 60h:38m:56s remains)
INFO - root - 2017-12-10 19:47:28.584658: step 47270, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 61h:31m:00s remains)
INFO - root - 2017-12-10 19:47:36.412557: step 47280, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 62h:37m:52s remains)
INFO - root - 2017-12-10 19:47:44.019612: step 47290, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 61h:33m:20s remains)
INFO - root - 2017-12-10 19:47:51.884627: step 47300, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 63h:49m:41s remains)
2017-12-10 19:47:52.798693: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.042150773 0.041662484 0.037067417 0.027417498 0.013508134 0.0015619756 -0.0014391671 0.006976062 0.028174112 0.05787034 0.0900913 0.11502942 0.1243861 0.11789213 0.10047588][0.090568945 0.088201523 0.080773823 0.067837164 0.049182009 0.032392941 0.026917966 0.0370916 0.064263977 0.10129697 0.13909839 0.1667854 0.17602369 0.16695055 0.14448923][0.14803338 0.14380246 0.13402526 0.11887372 0.097157046 0.077618964 0.07160604 0.084372304 0.11524918 0.15304081 0.18588646 0.20489183 0.20513621 0.18917717 0.16163848][0.19633256 0.19133666 0.18122858 0.16746233 0.14793476 0.13203809 0.13140135 0.14958851 0.18183044 0.21277344 0.22949521 0.22801197 0.20963523 0.18106535 0.14748539][0.2183065 0.21677531 0.2120935 0.20646724 0.19659513 0.19161777 0.20138693 0.22511491 0.25373337 0.27041978 0.26415059 0.23732348 0.19734214 0.15507707 0.11657078][0.20285049 0.20889769 0.21614732 0.22602168 0.23358999 0.24601576 0.26890033 0.29530349 0.31362131 0.31002137 0.27988616 0.23217809 0.17762922 0.12835009 0.090441652][0.15310347 0.16914316 0.19183932 0.22234946 0.25419208 0.290138 0.32839534 0.35518768 0.35864857 0.33226523 0.28155154 0.22207294 0.1643319 0.11816634 0.087834656][0.085198909 0.1102977 0.14853616 0.20156868 0.26188949 0.32562062 0.38155824 0.40950131 0.39830434 0.35020021 0.28259498 0.21784009 0.16465725 0.12815577 0.10888427][0.015483994 0.046619598 0.09747044 0.16957435 0.25442895 0.34053433 0.40894929 0.43644825 0.41383868 0.3510257 0.27435902 0.210881 0.167964 0.14546072 0.13902348][-0.038122483 -0.0058519864 0.05216964 0.13572754 0.23420608 0.32896671 0.39723817 0.418074 0.38621426 0.31670997 0.23963037 0.18282232 0.15294239 0.1457393 0.15201214][-0.072800934 -0.044363234 0.015040875 0.10266965 0.20440228 0.29604384 0.35445666 0.36435911 0.32524934 0.25466603 0.18214417 0.13287775 0.11317836 0.11678103 0.13136384][-0.089364633 -0.068602987 -0.014200252 0.068919592 0.16342215 0.24286772 0.28672114 0.28555769 0.24243885 0.17542526 0.111001 0.069159292 0.05526128 0.063081592 0.079896718][-0.091327921 -0.08053638 -0.037623353 0.030769115 0.10661318 0.16619763 0.19413418 0.18542406 0.14377363 0.085941181 0.033653259 0.00086009409 -0.00884022 -0.00051342777 0.014855393][-0.087008744 -0.084975667 -0.056946751 -0.010439058 0.038700897 0.07342571 0.085185416 0.072565466 0.037786517 -0.00548782 -0.041686933 -0.062781781 -0.067464061 -0.059395105 -0.046573758][-0.08311145 -0.08680892 -0.072248332 -0.047092631 -0.023594925 -0.01136627 -0.012600807 -0.025900332 -0.050818343 -0.077927418 -0.097756229 -0.10728361 -0.1072416 -0.10000547 -0.090337694]]...]
INFO - root - 2017-12-10 19:48:00.788408: step 47310, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 61h:23m:58s remains)
INFO - root - 2017-12-10 19:48:08.692546: step 47320, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 64h:16m:06s remains)
INFO - root - 2017-12-10 19:48:16.618640: step 47330, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 61h:09m:28s remains)
INFO - root - 2017-12-10 19:48:24.524779: step 47340, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 63h:36m:33s remains)
INFO - root - 2017-12-10 19:48:32.226199: step 47350, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 60h:52m:08s remains)
INFO - root - 2017-12-10 19:48:40.103614: step 47360, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 62h:51m:00s remains)
INFO - root - 2017-12-10 19:48:47.892838: step 47370, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 62h:55m:37s remains)
INFO - root - 2017-12-10 19:48:55.777138: step 47380, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 61h:14m:21s remains)
INFO - root - 2017-12-10 19:49:03.751161: step 47390, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 62h:53m:26s remains)
INFO - root - 2017-12-10 19:49:11.584153: step 47400, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 61h:46m:40s remains)
2017-12-10 19:49:12.444749: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14672774 0.15859468 0.16029865 0.15194288 0.13745975 0.12341397 0.11791368 0.11985952 0.12741864 0.1384397 0.14935407 0.15651582 0.15883905 0.16116484 0.16359551][0.1719586 0.17972493 0.17426521 0.16028635 0.14548685 0.13635844 0.13702832 0.14188372 0.1474425 0.15243992 0.15636526 0.15810914 0.15770872 0.15856524 0.15752722][0.19391532 0.19491976 0.1818144 0.16400443 0.15099302 0.14737245 0.15215895 0.15666218 0.15710591 0.15453464 0.15197825 0.14987339 0.14822565 0.14804788 0.14363398][0.21001783 0.20234528 0.18293022 0.16357726 0.1534652 0.15473947 0.161762 0.16424371 0.15933521 0.15008327 0.14207219 0.13676718 0.13427413 0.13420175 0.13000441][0.21571516 0.20033555 0.177108 0.1594412 0.15432142 0.16079487 0.16956653 0.16991352 0.1604224 0.14533836 0.13121498 0.12094735 0.11646207 0.11757465 0.11788747][0.21152824 0.19241148 0.16862968 0.15525217 0.15670925 0.16871814 0.17936099 0.1786909 0.16632096 0.14701799 0.1271286 0.11165567 0.10497553 0.1079713 0.11497864][0.19842623 0.1801146 0.15876919 0.15070476 0.15878074 0.17640351 0.19019125 0.19059196 0.17740327 0.15589665 0.13184266 0.11212367 0.10277019 0.10646808 0.11897918][0.18284115 0.16768092 0.15146308 0.14937893 0.16349965 0.18538272 0.20170021 0.20335363 0.18986227 0.16724446 0.14119437 0.11933054 0.10786564 0.11062118 0.12543884][0.16816893 0.15781768 0.14873974 0.15370204 0.17334989 0.19801451 0.2159535 0.21862847 0.205125 0.18236168 0.15636003 0.1342987 0.12077286 0.12042127 0.13315627][0.15746057 0.15290174 0.15138324 0.16281438 0.18638234 0.2123961 0.23140363 0.2353328 0.22206065 0.19919221 0.17332694 0.15122278 0.13526282 0.13005063 0.13686284][0.15327206 0.15390782 0.1576785 0.17301889 0.19766179 0.22291121 0.24222802 0.24774435 0.23542647 0.21221752 0.18602225 0.16388531 0.14595674 0.13590875 0.13586926][0.15496159 0.15742412 0.16271646 0.17870004 0.2016276 0.22398716 0.24247813 0.24924774 0.23832341 0.21460164 0.1871839 0.164788 0.14726336 0.13674903 0.13452604][0.15899743 0.16065615 0.16431031 0.17846988 0.19794866 0.21558039 0.23117703 0.23749791 0.22776747 0.2044044 0.17656837 0.15582071 0.14318798 0.13869409 0.14036256][0.16291906 0.16175658 0.16247627 0.17497511 0.19258064 0.20720023 0.21923797 0.22253676 0.2112837 0.18752483 0.16046336 0.14356917 0.13887897 0.1439106 0.15219478][0.16278644 0.15777948 0.15623398 0.16931592 0.18906008 0.204337 0.21309258 0.21051036 0.19374359 0.16717902 0.14119026 0.12980717 0.13458076 0.14939612 0.16333604]]...]
INFO - root - 2017-12-10 19:49:20.274192: step 47410, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 60h:20m:06s remains)
INFO - root - 2017-12-10 19:49:28.102764: step 47420, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 61h:16m:27s remains)
INFO - root - 2017-12-10 19:49:35.928107: step 47430, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 63h:20m:14s remains)
INFO - root - 2017-12-10 19:49:43.614821: step 47440, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 63h:19m:52s remains)
INFO - root - 2017-12-10 19:49:51.411720: step 47450, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 61h:39m:09s remains)
INFO - root - 2017-12-10 19:49:59.279892: step 47460, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 62h:18m:52s remains)
INFO - root - 2017-12-10 19:50:07.205724: step 47470, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 64h:02m:30s remains)
INFO - root - 2017-12-10 19:50:15.066082: step 47480, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 62h:53m:51s remains)
INFO - root - 2017-12-10 19:50:22.891958: step 47490, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 63h:15m:52s remains)
INFO - root - 2017-12-10 19:50:30.707839: step 47500, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.754 sec/batch; 59h:42m:24s remains)
2017-12-10 19:50:31.578722: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12501366 0.15383416 0.1811606 0.19562459 0.19546272 0.19152299 0.190013 0.19163479 0.19660413 0.20755298 0.21885729 0.21471937 0.19247083 0.16417563 0.14432231][0.13825089 0.17590371 0.215452 0.24113469 0.24725975 0.24434298 0.24070378 0.23637305 0.23195405 0.23546544 0.24670076 0.24941306 0.236567 0.21681949 0.20090355][0.15174513 0.19944851 0.25137892 0.28891852 0.30293086 0.30234641 0.29780367 0.28934544 0.27744654 0.27394244 0.28295881 0.29031137 0.28692022 0.27805606 0.26965305][0.17246489 0.22513127 0.28301936 0.32759896 0.34815738 0.35142756 0.34928823 0.34139025 0.32653424 0.31730109 0.32074279 0.32719085 0.32893014 0.3291612 0.32880744][0.20867431 0.26124328 0.3160345 0.35911641 0.38152102 0.38803077 0.39052954 0.38714013 0.37276733 0.35739467 0.35031393 0.34946638 0.35190603 0.35908693 0.36696866][0.25026208 0.30032498 0.34665176 0.38306269 0.40461171 0.41447517 0.42331859 0.4266066 0.41434291 0.39204952 0.37108102 0.3588995 0.3587327 0.37027791 0.38431704][0.2640959 0.31129506 0.34956083 0.38073286 0.40443689 0.42161494 0.43988931 0.45168763 0.44361505 0.41660196 0.38273707 0.35848913 0.35358223 0.36717531 0.38508657][0.22861192 0.27067837 0.30356562 0.33464876 0.3666949 0.39717415 0.42844197 0.45030612 0.4488534 0.42170998 0.37983707 0.34603006 0.33499622 0.34716472 0.36549005][0.14409809 0.1779694 0.2084062 0.24464841 0.29028249 0.33813637 0.38361773 0.41470468 0.42054152 0.39786679 0.35484895 0.31659219 0.30023971 0.30891925 0.32561585][0.049227502 0.072376028 0.099263936 0.1385711 0.19400574 0.25491703 0.31058326 0.34798211 0.3610369 0.34749022 0.31174171 0.27696761 0.25962102 0.26572743 0.28087687][-0.024271585 -0.012562959 0.00684607 0.041029058 0.094656058 0.15697758 0.21464927 0.25480825 0.27600446 0.277239 0.25884187 0.23868775 0.22918677 0.23763932 0.25311759][-0.069334209 -0.067633107 -0.057352196 -0.034130074 0.007705586 0.06012493 0.11174757 0.15209413 0.18322703 0.20452791 0.21223271 0.2165308 0.22284617 0.23740031 0.25273436][-0.08741641 -0.092180379 -0.089335121 -0.077741764 -0.05213584 -0.016472165 0.023106324 0.060542595 0.10013101 0.1410412 0.17655441 0.20832953 0.2332979 0.25502178 0.26953223][-0.085150667 -0.092853911 -0.094827019 -0.092284612 -0.081475846 -0.063136011 -0.037584547 -0.0057573016 0.038076278 0.0931617 0.15138313 0.20648603 0.24733113 0.27458319 0.2878758][-0.072498448 -0.080610633 -0.084908418 -0.087790824 -0.087085992 -0.081574723 -0.0672482 -0.040570866 0.0050545922 0.068841249 0.14181305 0.21143821 0.26091683 0.28894678 0.29871449]]...]
INFO - root - 2017-12-10 19:50:39.754177: step 47510, loss = 0.70, batch loss = 0.64 (8.7 examples/sec; 0.920 sec/batch; 72h:47m:40s remains)
INFO - root - 2017-12-10 19:50:47.686737: step 47520, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.811 sec/batch; 64h:11m:09s remains)
INFO - root - 2017-12-10 19:50:55.148656: step 47530, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 62h:16m:48s remains)
INFO - root - 2017-12-10 19:51:03.003906: step 47540, loss = 0.71, batch loss = 0.66 (10.6 examples/sec; 0.755 sec/batch; 59h:44m:43s remains)
INFO - root - 2017-12-10 19:51:10.843239: step 47550, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 61h:36m:09s remains)
INFO - root - 2017-12-10 19:51:18.640207: step 47560, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 61h:35m:47s remains)
INFO - root - 2017-12-10 19:51:26.433285: step 47570, loss = 0.70, batch loss = 0.64 (10.8 examples/sec; 0.741 sec/batch; 58h:38m:55s remains)
INFO - root - 2017-12-10 19:51:34.361227: step 47580, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 61h:55m:46s remains)
INFO - root - 2017-12-10 19:51:42.219807: step 47590, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 61h:51m:18s remains)
INFO - root - 2017-12-10 19:51:50.092410: step 47600, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.772 sec/batch; 61h:06m:54s remains)
2017-12-10 19:51:51.031460: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23749226 0.23945618 0.2230237 0.20026051 0.17280051 0.13843971 0.098334081 0.06033802 0.036688682 0.035408136 0.052907851 0.07116586 0.080466233 0.082511686 0.080494538][0.26228634 0.25533202 0.22960389 0.2001276 0.16906141 0.13333271 0.094821125 0.060731925 0.042754006 0.050701328 0.079153821 0.10456584 0.11430287 0.11380406 0.10891166][0.25603974 0.23627035 0.20418137 0.17570597 0.15217888 0.12850679 0.10443673 0.083150163 0.074257493 0.089604534 0.12296133 0.14840776 0.15362754 0.14972232 0.14605615][0.22627668 0.19822669 0.1692611 0.15306993 0.14892487 0.15021238 0.15119345 0.1479841 0.14637147 0.16069175 0.18676618 0.20130312 0.19591399 0.18716933 0.1870694][0.17890601 0.15837382 0.1486733 0.15669367 0.18012153 0.2127205 0.24157634 0.2547085 0.25411204 0.257059 0.26433277 0.2593163 0.23958933 0.22533578 0.22937097][0.13257487 0.13935804 0.16470066 0.20391019 0.25446543 0.31324533 0.36166626 0.38160306 0.37374985 0.35802987 0.34137166 0.31551024 0.28406733 0.267526 0.27599734][0.1072676 0.15293284 0.21839634 0.28536257 0.35204488 0.42117408 0.47280952 0.48702443 0.46654636 0.43237856 0.39676324 0.35867465 0.32522377 0.31306124 0.32581332][0.10872545 0.19010954 0.28742039 0.36996791 0.43632841 0.49641472 0.5341801 0.53252393 0.49868611 0.45323959 0.411195 0.37525746 0.35200238 0.35124415 0.36780348][0.12234759 0.22374582 0.33520204 0.41789809 0.47044125 0.50831538 0.52246886 0.501024 0.45569018 0.40655422 0.36901891 0.34603786 0.34058338 0.35504046 0.37649533][0.12205353 0.22476354 0.33266774 0.40440202 0.43809336 0.45183656 0.443378 0.40553805 0.35226405 0.30343413 0.27404487 0.26518664 0.27534479 0.30173981 0.32755107][0.090311229 0.17929816 0.27253157 0.33056316 0.34979916 0.34732324 0.32511854 0.27876198 0.22148694 0.17348561 0.14989 0.15007553 0.16808866 0.19861503 0.22587247][0.033451516 0.10051941 0.17527275 0.22305641 0.23714231 0.23041262 0.20596138 0.16016734 0.10325711 0.055648845 0.033870045 0.0365597 0.054183092 0.080563605 0.10497803][-0.021913068 0.022816937 0.081102252 0.12444405 0.14222224 0.14118171 0.12311023 0.08388184 0.030119322 -0.017563295 -0.04074157 -0.040698223 -0.029448938 -0.013441968 0.0035790177][-0.05198903 -0.025816109 0.019728765 0.0625303 0.088685386 0.098530076 0.091488838 0.062144719 0.013800696 -0.033453587 -0.059779629 -0.065185718 -0.062694043 -0.058786172 -0.051059797][-0.048408039 -0.036255483 -0.0014754058 0.040035903 0.072865456 0.092833683 0.097297676 0.079276338 0.039021324 -0.0054429881 -0.0343342 -0.045367811 -0.050634775 -0.056387842 -0.056203287]]...]
INFO - root - 2017-12-10 19:51:58.743027: step 47610, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 60h:31m:13s remains)
INFO - root - 2017-12-10 19:52:06.503130: step 47620, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 61h:30m:31s remains)
INFO - root - 2017-12-10 19:52:14.550906: step 47630, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 62h:02m:41s remains)
INFO - root - 2017-12-10 19:52:22.410586: step 47640, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 63h:17m:36s remains)
INFO - root - 2017-12-10 19:52:30.325926: step 47650, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 63h:23m:53s remains)
INFO - root - 2017-12-10 19:52:38.156689: step 47660, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 62h:18m:40s remains)
INFO - root - 2017-12-10 19:52:46.074422: step 47670, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.802 sec/batch; 63h:28m:37s remains)
INFO - root - 2017-12-10 19:52:53.995774: step 47680, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.806 sec/batch; 63h:44m:38s remains)
INFO - root - 2017-12-10 19:53:01.665535: step 47690, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 63h:49m:31s remains)
INFO - root - 2017-12-10 19:53:09.623795: step 47700, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 61h:40m:18s remains)
2017-12-10 19:53:10.537016: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0004138985 0.012310004 0.02287323 0.026419913 0.02224748 0.0147944 0.0076874313 0.0046514086 0.0050442643 0.0050264942 0.004102625 -0.00049824815 -0.010085454 -0.023857199 -0.037794687][0.0389733 0.068779863 0.097069323 0.11332647 0.11598667 0.1109331 0.10194188 0.096751027 0.095052168 0.092307396 0.087511219 0.075168647 0.054171961 0.02583071 -0.0030045987][0.092314027 0.14668463 0.19887224 0.23244996 0.24503745 0.24387757 0.23330225 0.22370775 0.21674432 0.20798734 0.19592707 0.17138261 0.13436362 0.087728694 0.040928658][0.146006 0.22630529 0.30462003 0.35889649 0.38638449 0.39503163 0.38785085 0.37415609 0.35807982 0.33823553 0.31356722 0.27157047 0.21420759 0.1473567 0.083003283][0.18491404 0.2874988 0.391162 0.47006655 0.52113342 0.55018646 0.55610216 0.54359287 0.51922405 0.48640427 0.44533163 0.38210723 0.30105641 0.21176609 0.12863694][0.20391193 0.32081392 0.44295552 0.54363841 0.61988914 0.67374241 0.698521 0.69296438 0.66492486 0.62247294 0.56503153 0.47992694 0.37524632 0.26499471 0.16528773][0.20958047 0.33193746 0.46281669 0.57715064 0.67259818 0.74591875 0.78568959 0.78677678 0.75908512 0.71213174 0.64132911 0.53764427 0.41498581 0.2914432 0.18214872][0.20271353 0.32135916 0.44846442 0.562379 0.66260087 0.74082208 0.78371346 0.78590375 0.76083207 0.71699756 0.64295447 0.53282583 0.40579712 0.28328434 0.17622794][0.17375995 0.27816248 0.38949108 0.49057579 0.58266777 0.65399623 0.69111842 0.6921016 0.6721366 0.63676292 0.5683974 0.46456891 0.34755477 0.23877788 0.14458033][0.12186033 0.20313251 0.29031408 0.37087983 0.44664466 0.50400883 0.53073162 0.52970028 0.51558465 0.48998791 0.4325417 0.34494266 0.25008565 0.16554144 0.092545241][0.060318545 0.11417107 0.17346503 0.22944236 0.28340915 0.32186711 0.33485332 0.32955 0.31892177 0.30169135 0.25815305 0.19321394 0.1279653 0.073914245 0.027574472][0.0059699747 0.034905411 0.06859792 0.10093991 0.13279743 0.15231605 0.15224475 0.14199607 0.13264272 0.12178906 0.09300822 0.052308965 0.016727397 -0.0081195664 -0.029121913][-0.032545939 -0.021924308 -0.0072584366 0.0066779428 0.020442255 0.025595929 0.017714197 0.005553389 -0.00279132 -0.0092473291 -0.025075769 -0.045018174 -0.057643015 -0.061803959 -0.064916782][-0.056501042 -0.057254665 -0.054176293 -0.051505473 -0.049025394 -0.0511485 -0.059975557 -0.069604069 -0.075960465 -0.080340721 -0.088241853 -0.095272146 -0.095096335 -0.089250855 -0.083304606][-0.067544237 -0.074371479 -0.077431455 -0.080533423 -0.0836371 -0.088155873 -0.094591253 -0.10023218 -0.10407595 -0.10689647 -0.11006568 -0.11025207 -0.10472124 -0.095695578 -0.086697049]]...]
INFO - root - 2017-12-10 19:53:18.351764: step 47710, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.777 sec/batch; 61h:26m:50s remains)
INFO - root - 2017-12-10 19:53:26.250686: step 47720, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 61h:55m:33s remains)
INFO - root - 2017-12-10 19:53:34.144933: step 47730, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.791 sec/batch; 62h:32m:25s remains)
INFO - root - 2017-12-10 19:53:42.143437: step 47740, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 60h:37m:56s remains)
INFO - root - 2017-12-10 19:53:49.893941: step 47750, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 61h:05m:48s remains)
INFO - root - 2017-12-10 19:53:57.782064: step 47760, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 62h:14m:02s remains)
INFO - root - 2017-12-10 19:54:05.518508: step 47770, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 61h:47m:34s remains)
INFO - root - 2017-12-10 19:54:13.382691: step 47780, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 62h:36m:58s remains)
INFO - root - 2017-12-10 19:54:21.057664: step 47790, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 62h:15m:57s remains)
INFO - root - 2017-12-10 19:54:28.899104: step 47800, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 62h:34m:42s remains)
2017-12-10 19:54:29.659014: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.017558543 0.035163354 0.053312402 0.06917882 0.0816677 0.091156878 0.095958456 0.094135933 0.089149565 0.086577669 0.084526859 0.078551471 0.068397239 0.059168521 0.056235485][0.019442337 0.038109172 0.056665484 0.072447427 0.084690675 0.094882026 0.10246969 0.10382384 0.099706337 0.0941477 0.088141792 0.081367411 0.073748842 0.068305574 0.069219105][0.016792292 0.033530742 0.049557887 0.062467951 0.072106116 0.080782279 0.089412667 0.093026236 0.08972054 0.081875369 0.073847413 0.069308892 0.066850334 0.06702733 0.073199451][0.016841711 0.033396229 0.048999574 0.060196951 0.066431873 0.070993409 0.077625178 0.081577241 0.079483964 0.071551062 0.063403942 0.060844537 0.061416607 0.064860508 0.074265532][0.019255018 0.038183473 0.05606579 0.067483075 0.071082354 0.071365327 0.075017251 0.078574933 0.078211427 0.071879111 0.06418404 0.061248187 0.060369167 0.062093876 0.070408463][0.020662839 0.041854974 0.062467415 0.075013064 0.077218205 0.074580863 0.076359279 0.0806101 0.082975417 0.079380587 0.072863974 0.069024019 0.064533666 0.061479874 0.06598714][0.020352675 0.04243229 0.065002985 0.079496816 0.082749024 0.080386177 0.0825696 0.088432893 0.093863927 0.094432332 0.090850092 0.086818658 0.078439169 0.06964273 0.069057785][0.020729633 0.042664174 0.0663581 0.083471864 0.0901946 0.091265686 0.095817164 0.10335216 0.11088324 0.11553638 0.116073 0.11286053 0.10069921 0.085083149 0.077933937][0.021815928 0.043213747 0.067440577 0.087480262 0.099230953 0.10587929 0.11409226 0.12253531 0.12963118 0.13554908 0.13858046 0.13566169 0.12028785 0.098956682 0.086255811][0.022002898 0.042005625 0.0652644 0.086484827 0.10184435 0.11301097 0.12380094 0.13148719 0.13577054 0.14008823 0.14396885 0.14240326 0.12791742 0.10637399 0.092584051][0.02190138 0.039885316 0.060628276 0.080256693 0.095617175 0.10757099 0.11821659 0.12414048 0.12594797 0.12909888 0.13459285 0.13692731 0.12830085 0.11257146 0.10157633][0.021119339 0.036699127 0.053856578 0.069504119 0.081215359 0.089934394 0.097740248 0.1020546 0.10409857 0.10945511 0.11903283 0.12712735 0.12631531 0.11828266 0.11066506][0.019601014 0.033659689 0.048055932 0.059922472 0.067124918 0.070911914 0.074237324 0.07633806 0.079138435 0.087312534 0.10067128 0.11301398 0.11740784 0.11454345 0.10918575][0.016328707 0.02966786 0.042930759 0.052962087 0.057429075 0.057526674 0.057054553 0.056900594 0.059492372 0.06816718 0.081796154 0.094358265 0.099651486 0.098415814 0.094137579][0.010579338 0.02335955 0.036562297 0.04663451 0.050854258 0.050009754 0.047885537 0.046241991 0.047555462 0.053968184 0.063936837 0.072538361 0.07515157 0.073151633 0.069546133]]...]
INFO - root - 2017-12-10 19:54:37.591938: step 47810, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 63h:48m:55s remains)
INFO - root - 2017-12-10 19:54:45.550588: step 47820, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 62h:41m:18s remains)
INFO - root - 2017-12-10 19:54:53.312431: step 47830, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.818 sec/batch; 64h:41m:27s remains)
INFO - root - 2017-12-10 19:55:01.292922: step 47840, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 60h:09m:24s remains)
INFO - root - 2017-12-10 19:55:09.296947: step 47850, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 61h:42m:59s remains)
INFO - root - 2017-12-10 19:55:17.212952: step 47860, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 60h:58m:17s remains)
INFO - root - 2017-12-10 19:55:25.116503: step 47870, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 61h:24m:24s remains)
INFO - root - 2017-12-10 19:55:32.868058: step 47880, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 62h:43m:48s remains)
INFO - root - 2017-12-10 19:55:40.771173: step 47890, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.833 sec/batch; 65h:50m:42s remains)
INFO - root - 2017-12-10 19:55:48.592443: step 47900, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 60h:31m:36s remains)
2017-12-10 19:55:49.565123: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18815765 0.16033524 0.12599979 0.10134321 0.095416076 0.10564878 0.12216844 0.13729468 0.14412232 0.13923527 0.12679797 0.11621502 0.1217566 0.14808142 0.18754356][0.2223617 0.1975558 0.16331246 0.13466483 0.12014541 0.11747022 0.11672219 0.11065789 0.096556008 0.080668308 0.073932029 0.085405305 0.12083149 0.17367207 0.22995628][0.23764393 0.22274552 0.19929554 0.17896855 0.16745469 0.15981618 0.14571509 0.11916383 0.081918456 0.051838472 0.049230289 0.083301231 0.14766794 0.22171538 0.2852079][0.22533043 0.227994 0.2268234 0.22888976 0.23549372 0.2366689 0.220841 0.18177366 0.12432465 0.076983109 0.07017234 0.11467212 0.19292308 0.27400228 0.33375052][0.1878498 0.21682931 0.25068375 0.28952309 0.32925454 0.35370088 0.34881392 0.30605102 0.23117365 0.1607428 0.1342791 0.16607387 0.23330167 0.3018387 0.34818229][0.14110282 0.20429157 0.28316706 0.36868697 0.45061964 0.50781584 0.52236843 0.481327 0.38997295 0.28956288 0.22593257 0.2178652 0.24565962 0.28152603 0.30681935][0.10981041 0.20735982 0.33030257 0.4599902 0.58067113 0.6688711 0.70288384 0.66514653 0.55971819 0.42867306 0.31888807 0.25299788 0.22170603 0.21141773 0.21134126][0.1027296 0.22502628 0.37799245 0.53556168 0.67847282 0.78360039 0.82753408 0.79068059 0.67677623 0.52491236 0.376467 0.25526118 0.16513626 0.10926821 0.085338324][0.11037436 0.23973036 0.40024722 0.56206352 0.70408016 0.80552614 0.84481144 0.80481857 0.69042271 0.53466338 0.36894906 0.2149235 0.086815931 0.00084707647 -0.038614444][0.11238226 0.22868313 0.37247664 0.51423639 0.63325578 0.71233183 0.73498029 0.69009262 0.58494961 0.4449802 0.29011428 0.13693301 0.0038919528 -0.0873272 -0.12909408][0.088823 0.17530277 0.28371462 0.38867894 0.47198787 0.5200761 0.52292997 0.47655281 0.39059579 0.28273037 0.16258749 0.040404178 -0.067243777 -0.14049855 -0.17208564][0.037067462 0.086088188 0.1522925 0.21657726 0.26448587 0.28534308 0.27423707 0.23243828 0.17058012 0.10017512 0.024350049 -0.052155219 -0.11865715 -0.16168439 -0.17650257][-0.025678637 -0.010423386 0.0192033 0.050619304 0.073461458 0.07936728 0.065308064 0.033934914 -0.00525149 -0.044411469 -0.082546584 -0.11805078 -0.14634274 -0.16111991 -0.16059321][-0.076236218 -0.083695456 -0.078652717 -0.068562463 -0.059523091 -0.057533845 -0.06576556 -0.083288439 -0.10385224 -0.12208362 -0.13667054 -0.14676456 -0.15138559 -0.14904754 -0.14003673][-0.10321064 -0.12176951 -0.12937805 -0.13084698 -0.12838401 -0.12541351 -0.12541568 -0.12991676 -0.13657606 -0.14230967 -0.1454574 -0.1448977 -0.14052945 -0.1327056 -0.12253379]]...]
INFO - root - 2017-12-10 19:55:57.454592: step 47910, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 62h:54m:00s remains)
INFO - root - 2017-12-10 19:56:05.150285: step 47920, loss = 0.70, batch loss = 0.64 (13.4 examples/sec; 0.597 sec/batch; 47h:13m:10s remains)
INFO - root - 2017-12-10 19:56:13.083825: step 47930, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 62h:03m:39s remains)
INFO - root - 2017-12-10 19:56:20.989400: step 47940, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.774 sec/batch; 61h:11m:02s remains)
INFO - root - 2017-12-10 19:56:28.902368: step 47950, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 64h:01m:37s remains)
INFO - root - 2017-12-10 19:56:36.889322: step 47960, loss = 0.70, batch loss = 0.65 (9.0 examples/sec; 0.893 sec/batch; 70h:34m:19s remains)
INFO - root - 2017-12-10 19:56:44.614342: step 47970, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 61h:51m:24s remains)
INFO - root - 2017-12-10 19:56:52.391585: step 47980, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 61h:58m:39s remains)
INFO - root - 2017-12-10 19:57:00.338073: step 47990, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 62h:43m:33s remains)
INFO - root - 2017-12-10 19:57:08.261891: step 48000, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 59h:43m:26s remains)
2017-12-10 19:57:09.048981: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0097759785 0.016293561 0.03781582 0.066602893 0.091585971 0.10479409 0.10438485 0.092040889 0.072431967 0.047614511 0.019107766 -0.01228004 -0.043658946 -0.068450414 -0.080719687][0.084788121 0.09045396 0.11074588 0.14193769 0.17214683 0.19262023 0.19950499 0.19006871 0.16736455 0.13257746 0.089231178 0.039558973 -0.011623746 -0.054351423 -0.079025276][0.17233811 0.17472942 0.18900433 0.21847343 0.25243306 0.28202868 0.30089638 0.29900661 0.27565348 0.23047298 0.16985321 0.099010915 0.026008794 -0.035403308 -0.072987013][0.26325658 0.25559196 0.25843936 0.28444636 0.32362404 0.36565048 0.40073964 0.41113088 0.39141303 0.33874661 0.26167843 0.16919233 0.07314875 -0.0080430759 -0.059734654][0.36867085 0.33831868 0.3183412 0.33351716 0.37510711 0.43033692 0.48410493 0.51082671 0.49935275 0.44350785 0.35344261 0.2412764 0.12357902 0.023662515 -0.041466661][0.47704566 0.41998056 0.36961544 0.36563259 0.40408719 0.47050381 0.54322278 0.58727604 0.58392674 0.524182 0.42094442 0.29068658 0.15608869 0.044212818 -0.027989365][0.57585138 0.50068027 0.42257833 0.39524946 0.42384741 0.49531278 0.58205938 0.64053762 0.64359438 0.57918167 0.46231416 0.315349 0.16796298 0.05027315 -0.022805329][0.64310813 0.5613637 0.46500972 0.41562545 0.42844507 0.49564743 0.58753067 0.656159 0.66766834 0.60601389 0.48422939 0.32763332 0.17225134 0.051557057 -0.020882769][0.67919242 0.59737372 0.48955014 0.41852358 0.40776977 0.4567875 0.53880596 0.607651 0.62687868 0.57719576 0.46566334 0.31496418 0.16358064 0.046452563 -0.022755265][0.67146754 0.59588408 0.48413712 0.39479822 0.35665938 0.37754732 0.43689635 0.49473837 0.51644009 0.48239782 0.39266977 0.26314723 0.12960349 0.025798112 -0.034656357][0.59233731 0.53171635 0.42943263 0.33472449 0.27780712 0.27335387 0.3076703 0.34855092 0.36610106 0.34345376 0.27647695 0.17479731 0.068638429 -0.012734284 -0.057566456][0.45934922 0.41710976 0.33371517 0.24557242 0.18075053 0.15832114 0.17122725 0.19526613 0.20685045 0.19259587 0.14667377 0.074028976 -0.0017823983 -0.057842202 -0.085012011][0.29830256 0.27117965 0.20898958 0.13437849 0.07130637 0.040005937 0.038756121 0.050741073 0.058757331 0.052873433 0.026988424 -0.017330857 -0.063984431 -0.096345589 -0.10779642][0.1421883 0.12393914 0.080815747 0.024188885 -0.027787862 -0.057699833 -0.064711936 -0.059678245 -0.053788763 -0.053429514 -0.063580267 -0.084643275 -0.10725051 -0.12057304 -0.12016664][0.018826237 0.0048689083 -0.022749335 -0.059953675 -0.095330968 -0.11685076 -0.12336322 -0.12114548 -0.11656823 -0.11334442 -0.11471963 -0.12158719 -0.12888408 -0.13015842 -0.12279449]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 19:57:16.905650: step 48010, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 61h:36m:06s remains)
INFO - root - 2017-12-10 19:57:24.714623: step 48020, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.757 sec/batch; 59h:47m:14s remains)
INFO - root - 2017-12-10 19:57:32.730766: step 48030, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 61h:33m:32s remains)
INFO - root - 2017-12-10 19:57:40.593139: step 48040, loss = 0.66, batch loss = 0.61 (9.9 examples/sec; 0.810 sec/batch; 64h:01m:41s remains)
INFO - root - 2017-12-10 19:57:48.453420: step 48050, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 61h:16m:45s remains)
INFO - root - 2017-12-10 19:57:56.232460: step 48060, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 61h:12m:48s remains)
INFO - root - 2017-12-10 19:58:04.137565: step 48070, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 61h:53m:44s remains)
INFO - root - 2017-12-10 19:58:11.877452: step 48080, loss = 0.70, batch loss = 0.64 (12.3 examples/sec; 0.650 sec/batch; 51h:22m:07s remains)
INFO - root - 2017-12-10 19:58:19.704823: step 48090, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 60h:43m:25s remains)
INFO - root - 2017-12-10 19:58:27.637721: step 48100, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 62h:52m:18s remains)
2017-12-10 19:58:28.460481: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24843687 0.269971 0.27729028 0.28139967 0.29039082 0.30039012 0.30638704 0.30216733 0.28626907 0.25858864 0.22712165 0.2068947 0.19871028 0.20052452 0.20942128][0.31921002 0.35194379 0.37211698 0.38306519 0.39377242 0.40026581 0.39540511 0.37069175 0.32917237 0.27910957 0.23128438 0.19990343 0.18431361 0.18222086 0.18836229][0.39735371 0.43660775 0.46715268 0.48219332 0.4907302 0.49065319 0.47484657 0.43148395 0.36614409 0.29726028 0.23905964 0.19964054 0.17392501 0.16148192 0.15979476][0.46215677 0.49946553 0.5359816 0.5544818 0.5627898 0.56346709 0.5503301 0.50568533 0.43224844 0.35690534 0.29500055 0.24565002 0.20055796 0.16508105 0.14528205][0.49683645 0.53416115 0.57868081 0.60548186 0.6207639 0.63230491 0.63484365 0.60470754 0.53783584 0.46439621 0.39863375 0.33209243 0.25740644 0.18881288 0.14401469][0.49780059 0.53989553 0.59452331 0.63356841 0.66178823 0.69061965 0.71518791 0.70600516 0.65137905 0.58132118 0.50864422 0.42044997 0.31453657 0.2142904 0.1463452][0.4741095 0.52490276 0.58982939 0.64163065 0.68324941 0.72624367 0.76471466 0.76625311 0.71601522 0.64373159 0.56225586 0.45779276 0.33390629 0.21821885 0.1408367][0.43951052 0.49822104 0.5694744 0.62925333 0.67851043 0.72525555 0.7616632 0.75869232 0.704332 0.62739164 0.5417788 0.43621585 0.31745133 0.2105014 0.14295848][0.39191571 0.45678267 0.53037608 0.59344769 0.64369094 0.68432385 0.7065 0.6908325 0.63159537 0.55447096 0.47434762 0.3835195 0.28922006 0.20815969 0.16131635][0.34022686 0.40952379 0.48184127 0.54262942 0.58688611 0.61401165 0.61591983 0.58665979 0.52723855 0.45813325 0.39523518 0.33248276 0.27314302 0.22206944 0.19323093][0.31568423 0.37941483 0.43969294 0.48788774 0.5181576 0.52832514 0.51179957 0.47553116 0.42361814 0.37053734 0.33372796 0.30627814 0.28357545 0.25726423 0.23649767][0.32881328 0.37907785 0.42102268 0.45111376 0.46356446 0.45610738 0.42396048 0.38451669 0.34125212 0.30384523 0.29312947 0.3002249 0.31027091 0.30259916 0.28450897][0.36465898 0.3999835 0.4230583 0.43480819 0.430024 0.40713063 0.36354616 0.32270312 0.28546992 0.25895926 0.26911414 0.30474257 0.34096131 0.34893808 0.33481839][0.37079406 0.40225217 0.42218456 0.43058339 0.42056242 0.39103773 0.3414427 0.29714113 0.2575677 0.2316269 0.25145644 0.30448169 0.35754615 0.37844521 0.37152484][0.33941841 0.36911336 0.39199203 0.40382072 0.39535227 0.36563766 0.3149479 0.26790962 0.22445413 0.19716297 0.22217719 0.28551397 0.34914768 0.38154343 0.38419449]]...]
INFO - root - 2017-12-10 19:58:36.319424: step 48110, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 62h:12m:07s remains)
INFO - root - 2017-12-10 19:58:44.109744: step 48120, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.758 sec/batch; 59h:51m:31s remains)
INFO - root - 2017-12-10 19:58:52.053365: step 48130, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.818 sec/batch; 64h:34m:44s remains)
INFO - root - 2017-12-10 19:58:59.995875: step 48140, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 62h:44m:44s remains)
INFO - root - 2017-12-10 19:59:07.740166: step 48150, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 62h:58m:18s remains)
INFO - root - 2017-12-10 19:59:15.415189: step 48160, loss = 0.70, batch loss = 0.64 (11.1 examples/sec; 0.723 sec/batch; 57h:05m:52s remains)
INFO - root - 2017-12-10 19:59:23.290768: step 48170, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 61h:35m:54s remains)
INFO - root - 2017-12-10 19:59:31.094140: step 48180, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 60h:56m:25s remains)
INFO - root - 2017-12-10 19:59:38.992514: step 48190, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 61h:44m:31s remains)
INFO - root - 2017-12-10 19:59:46.922861: step 48200, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 63h:12m:03s remains)
2017-12-10 19:59:47.729656: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.047556616 0.078249626 0.10069565 0.10581678 0.095643453 0.080900587 0.065632924 0.055980358 0.05094552 0.0581797 0.084331132 0.11775199 0.14334723 0.14810139 0.13199639][0.1126809 0.16341594 0.19983378 0.20712109 0.18899769 0.16296044 0.13807996 0.12275611 0.11271479 0.11814121 0.14719546 0.1849712 0.21239136 0.21444133 0.19301432][0.19030924 0.262309 0.31126821 0.31878328 0.29304656 0.26024911 0.23367006 0.2207076 0.20990007 0.20970191 0.23119965 0.26052985 0.27842927 0.27041149 0.24120979][0.27219382 0.35779026 0.410334 0.41285825 0.38220376 0.35559377 0.34789854 0.35922262 0.3639549 0.36110365 0.36384743 0.36526814 0.35234421 0.3161515 0.2676135][0.34931424 0.435952 0.48093143 0.47448012 0.44404614 0.43728888 0.46807247 0.523531 0.5588848 0.556949 0.53294045 0.49164528 0.43151909 0.35311329 0.27576962][0.4024502 0.48109132 0.51546305 0.50539792 0.48534083 0.50715095 0.58087873 0.68022537 0.74235195 0.73480392 0.6768136 0.58961952 0.48417738 0.36802056 0.26610896][0.42284653 0.49241379 0.52248865 0.52107704 0.52117115 0.57006866 0.67317873 0.79563761 0.86543226 0.84069264 0.74695683 0.62290716 0.49013239 0.3570489 0.24659105][0.39899656 0.45846444 0.48946553 0.50388968 0.52588582 0.59097511 0.70075577 0.81993306 0.87873125 0.834299 0.71642977 0.57657731 0.44228137 0.31731382 0.21588555][0.33935553 0.38817737 0.41985261 0.44636744 0.48065108 0.54385453 0.63636512 0.73067325 0.76959354 0.71589 0.59789115 0.46955478 0.35838073 0.26077956 0.18090674][0.25407681 0.29178628 0.31974855 0.34770143 0.37995985 0.42489675 0.48319837 0.53939313 0.55584872 0.50542116 0.41055855 0.31712008 0.24656484 0.18777232 0.13709496][0.15352634 0.17897584 0.19857578 0.2183214 0.23827659 0.25873682 0.28039077 0.29863653 0.29586744 0.25672475 0.19652486 0.14648888 0.11855222 0.096841358 0.074480228][0.05792287 0.071384981 0.081395239 0.090345457 0.097470768 0.099830665 0.097615562 0.092259675 0.079979189 0.054597132 0.025897786 0.010217788 0.010747056 0.011428338 0.0065950034][-0.019033864 -0.015297993 -0.01252524 -0.011413142 -0.012428267 -0.018793328 -0.030408859 -0.043637518 -0.056060985 -0.06868393 -0.0754164 -0.070752136 -0.057415575 -0.048919525 -0.047491621][-0.064872317 -0.066350251 -0.066705391 -0.068338767 -0.071682118 -0.079347827 -0.091031969 -0.1029102 -0.11128702 -0.11562073 -0.11200401 -0.10030904 -0.0855755 -0.077682026 -0.076178268][-0.081938252 -0.084805258 -0.085519418 -0.086899355 -0.089474857 -0.095175311 -0.10360892 -0.11166872 -0.11635943 -0.11711167 -0.11155961 -0.10123532 -0.090826958 -0.086312637 -0.086085387]]...]
INFO - root - 2017-12-10 19:59:55.586672: step 48210, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 61h:00m:01s remains)
INFO - root - 2017-12-10 20:00:03.389304: step 48220, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 62h:50m:49s remains)
INFO - root - 2017-12-10 20:00:11.149381: step 48230, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 63h:25m:02s remains)
INFO - root - 2017-12-10 20:00:18.722861: step 48240, loss = 0.68, batch loss = 0.63 (10.9 examples/sec; 0.733 sec/batch; 57h:53m:00s remains)
INFO - root - 2017-12-10 20:00:26.711266: step 48250, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 63h:04m:55s remains)
INFO - root - 2017-12-10 20:00:34.570389: step 48260, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 61h:54m:40s remains)
INFO - root - 2017-12-10 20:00:42.368354: step 48270, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 61h:31m:54s remains)
INFO - root - 2017-12-10 20:00:50.197065: step 48280, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 62h:07m:14s remains)
INFO - root - 2017-12-10 20:00:58.113201: step 48290, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 60h:54m:28s remains)
INFO - root - 2017-12-10 20:01:05.910461: step 48300, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 61h:22m:04s remains)
2017-12-10 20:01:06.773049: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25396597 0.25828871 0.25783074 0.26558784 0.2853888 0.304391 0.31229725 0.31495351 0.3170042 0.30070254 0.25654617 0.1971273 0.13621745 0.078160554 0.023493569][0.28353554 0.2892049 0.28611785 0.29056633 0.30736116 0.32320318 0.32900888 0.33122417 0.33720279 0.32479724 0.28108439 0.21976043 0.15512355 0.092709087 0.032094628][0.29260311 0.29889902 0.29332057 0.295092 0.30824888 0.3192986 0.322873 0.325141 0.33393246 0.32590804 0.28645447 0.2276547 0.16307902 0.10001983 0.037464447][0.29189584 0.29815352 0.29274061 0.29527277 0.30724171 0.31715313 0.32362312 0.33019856 0.34192437 0.33663112 0.30013508 0.24311472 0.17706184 0.11081536 0.044576075][0.28522664 0.29212606 0.28974006 0.29559949 0.30796331 0.3194108 0.332834 0.34735841 0.36124903 0.35507178 0.31797504 0.26047337 0.19134647 0.11984504 0.049068581][0.28125903 0.29154477 0.29408437 0.30371541 0.31578425 0.32736018 0.3458105 0.36613581 0.37852708 0.36731032 0.32688537 0.26820564 0.19713005 0.12174841 0.048453309][0.2770611 0.29199594 0.29941988 0.31196842 0.32244691 0.33143911 0.34979677 0.37074918 0.37869498 0.36176452 0.31957179 0.26266068 0.19355045 0.11823062 0.045744639][0.26594529 0.28189275 0.2929357 0.30795375 0.31667852 0.32148886 0.33610916 0.35328841 0.35549405 0.33467054 0.29384303 0.24226931 0.17892909 0.10785209 0.039533976][0.25295773 0.26510593 0.27546519 0.2898545 0.29523075 0.29423329 0.30309114 0.31542006 0.31384245 0.29192206 0.25435454 0.20935753 0.15334928 0.088564575 0.026506409][0.25399825 0.25900364 0.26238447 0.26971546 0.26702479 0.257006 0.25779426 0.26432729 0.26024941 0.23917513 0.20569263 0.16731118 0.11948314 0.062593795 0.0084377294][0.27422529 0.27032274 0.26151767 0.25739402 0.24473411 0.22536017 0.21806084 0.21947181 0.21450499 0.19478348 0.16439432 0.13195397 0.0917274 0.041692913 -0.0061659245][0.29507712 0.28139773 0.26108071 0.24801303 0.23018782 0.2071493 0.19622248 0.1954727 0.1910982 0.17251033 0.14361386 0.11510316 0.079653367 0.033025179 -0.012297226][0.29513356 0.276002 0.25264007 0.24034913 0.22713792 0.20928156 0.20142479 0.20219699 0.19820045 0.17806537 0.147042 0.11819112 0.082454152 0.034416519 -0.012252328][0.26585686 0.25067291 0.2358499 0.2342699 0.23364933 0.22748299 0.22787772 0.232945 0.22868577 0.20439331 0.1680868 0.13502137 0.094586112 0.041553866 -0.00861998][0.21055055 0.20890592 0.21089981 0.22345935 0.23558213 0.24090533 0.2504656 0.26105276 0.25730562 0.22988255 0.18953465 0.15238437 0.10671969 0.048664719 -0.004278122]]...]
INFO - root - 2017-12-10 20:01:14.717550: step 48310, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 60h:06m:48s remains)
INFO - root - 2017-12-10 20:01:22.253746: step 48320, loss = 0.70, batch loss = 0.64 (15.7 examples/sec; 0.508 sec/batch; 40h:07m:14s remains)
INFO - root - 2017-12-10 20:01:30.147012: step 48330, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 62h:48m:06s remains)
INFO - root - 2017-12-10 20:01:37.961218: step 48340, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 60h:45m:44s remains)
INFO - root - 2017-12-10 20:01:45.893819: step 48350, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.824 sec/batch; 65h:02m:24s remains)
INFO - root - 2017-12-10 20:01:53.768096: step 48360, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 61h:47m:09s remains)
INFO - root - 2017-12-10 20:02:01.662329: step 48370, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 63h:03m:59s remains)
INFO - root - 2017-12-10 20:02:09.522844: step 48380, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 61h:49m:48s remains)
INFO - root - 2017-12-10 20:02:17.428185: step 48390, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 62h:02m:04s remains)
INFO - root - 2017-12-10 20:02:25.216645: step 48400, loss = 0.68, batch loss = 0.62 (12.7 examples/sec; 0.632 sec/batch; 49h:52m:36s remains)
2017-12-10 20:02:26.046518: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.037378296 -0.032038376 -0.017470505 0.0082844319 0.047209293 0.097182542 0.14923957 0.19203436 0.21540724 0.21853416 0.20935169 0.19692118 0.19083896 0.18804561 0.18601228][-0.038392637 -0.032784168 -0.016677219 0.01234121 0.056048181 0.1106529 0.16537096 0.2072054 0.22483225 0.21879326 0.19979712 0.18063773 0.17213511 0.16955824 0.16765617][-0.0380669 -0.032412931 -0.015757341 0.014509785 0.060068339 0.11605246 0.17087443 0.21076399 0.22406286 0.21210083 0.18698592 0.16403636 0.15427516 0.1512503 0.1473836][-0.03777802 -0.032420639 -0.01616835 0.01366642 0.058829561 0.11412572 0.16790782 0.20666102 0.21890205 0.20604397 0.18030161 0.1575857 0.14814781 0.14390846 0.13630012][-0.03757102 -0.032749765 -0.01738915 0.010996033 0.054230653 0.10711364 0.15838273 0.19526593 0.20707306 0.1957504 0.17282768 0.1535808 0.14673547 0.142876 0.13330072][-0.037490904 -0.033193946 -0.018857228 0.0078535313 0.048733834 0.098732047 0.14720747 0.18246561 0.19447944 0.18513395 0.16504669 0.14842109 0.14302401 0.13934036 0.12957743][-0.037576053 -0.033762086 -0.020243797 0.0050009312 0.043830644 0.0915153 0.13814874 0.17295222 0.18621854 0.17917512 0.16104311 0.14499964 0.13865192 0.13370831 0.12371985][-0.037874926 -0.034450751 -0.021461632 0.0027233278 0.040054645 0.08624053 0.13198732 0.16709471 0.18185024 0.17668538 0.15945819 0.14236267 0.13304606 0.12508705 0.11346176][-0.038129084 -0.034962323 -0.022141427 0.0015970861 0.038327683 0.084096 0.12975995 0.16509531 0.18003036 0.17441699 0.15553203 0.13543986 0.12211866 0.11080297 0.097490937][-0.038272642 -0.034954552 -0.021648457 0.0030857355 0.041566424 0.089932404 0.13843039 0.17560896 0.19027098 0.18165451 0.15739536 0.13095887 0.11163776 0.096803121 0.0828391][-0.038301706 -0.034436721 -0.019920843 0.0071663707 0.049487546 0.10287984 0.15626341 0.19636604 0.21087116 0.19850287 0.16727386 0.13226907 0.1048776 0.085347675 0.070737019][-0.038291849 -0.034138504 -0.018924138 0.0093633393 0.053942081 0.11062485 0.16754718 0.2103323 0.22647031 0.21382318 0.17940749 0.13850772 0.10448888 0.08092992 0.065991968][-0.038342234 -0.034264162 -0.01933212 0.0081388419 0.051837355 0.10817474 0.16539769 0.20934635 0.22804213 0.21831138 0.18503565 0.14216715 0.10454559 0.078902833 0.064770013][-0.038428895 -0.034547132 -0.020452008 0.0049673864 0.04544374 0.098279439 0.1523954 0.19464442 0.21408436 0.2070848 0.176491 0.13428049 0.096132584 0.071134768 0.059629139][-0.038867179 -0.035589494 -0.023051694 -0.00075921253 0.034833413 0.081956774 0.13049194 0.16861613 0.18670444 0.18139949 0.1542183 0.11510914 0.079571158 0.05751881 0.049504314]]...]
INFO - root - 2017-12-10 20:02:33.723049: step 48410, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 61h:39m:25s remains)
INFO - root - 2017-12-10 20:02:41.469421: step 48420, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 62h:45m:46s remains)
INFO - root - 2017-12-10 20:02:49.336955: step 48430, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.801 sec/batch; 63h:12m:48s remains)
INFO - root - 2017-12-10 20:02:57.175518: step 48440, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 60h:17m:40s remains)
INFO - root - 2017-12-10 20:03:05.076292: step 48450, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 63h:43m:56s remains)
INFO - root - 2017-12-10 20:03:12.991649: step 48460, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 62h:42m:26s remains)
INFO - root - 2017-12-10 20:03:20.665494: step 48470, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 60h:22m:10s remains)
INFO - root - 2017-12-10 20:03:28.309525: step 48480, loss = 0.69, batch loss = 0.63 (11.7 examples/sec; 0.687 sec/batch; 54h:09m:44s remains)
INFO - root - 2017-12-10 20:03:36.317162: step 48490, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 63h:35m:55s remains)
INFO - root - 2017-12-10 20:03:43.939282: step 48500, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 60h:40m:04s remains)
2017-12-10 20:03:44.708939: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.48394197 0.31861213 0.14960447 0.060188081 0.066748343 0.14772478 0.27211446 0.39784253 0.46963149 0.44868714 0.35539809 0.23369235 0.11199793 0.0062751775 -0.072119437][0.55143023 0.36739555 0.18342076 0.0979577 0.11890766 0.21641216 0.35163084 0.48323458 0.55296838 0.51674575 0.39732081 0.24371357 0.097417392 -0.021150498 -0.10232911][0.56292117 0.39119509 0.22531663 0.16237724 0.19825307 0.30232579 0.43499258 0.55778277 0.61449987 0.56357682 0.42988643 0.25941056 0.0985916 -0.029360352 -0.11401617][0.54154891 0.41729566 0.30476606 0.28355286 0.33665696 0.440679 0.56147105 0.66310638 0.69538653 0.6268298 0.48569521 0.3091926 0.13902028 -0.0011537629 -0.096128404][0.49593109 0.44323298 0.40965316 0.44506678 0.51980031 0.62182659 0.72575831 0.79766285 0.79670084 0.70710105 0.56213248 0.38562769 0.20874827 0.054742768 -0.055275608][0.43993258 0.46415463 0.51526225 0.60995483 0.70754439 0.8066085 0.89075404 0.92840016 0.8907842 0.77897781 0.63065463 0.4556492 0.27448079 0.10955106 -0.013538163][0.39303517 0.4767586 0.59325516 0.73272163 0.84838641 0.94401121 1.0082096 1.0126072 0.94263154 0.81258947 0.66123927 0.48823527 0.30725726 0.14054911 0.013500093][0.35238293 0.46636608 0.61571562 0.77664071 0.90029806 0.99008065 1.035035 1.0108106 0.91691035 0.776473 0.62646544 0.46096018 0.29039088 0.13556026 0.016615357][0.30944613 0.42557845 0.57749456 0.73905194 0.86139274 0.94182444 0.96683282 0.918832 0.80899793 0.6665079 0.52516884 0.37749293 0.23051855 0.1004995 -0.00028936769][0.26534337 0.36338708 0.49805823 0.64681929 0.76219177 0.83015686 0.83358234 0.76360071 0.64177543 0.50315177 0.3782928 0.25979146 0.14835732 0.052195817 -0.024958344][0.22700968 0.30173224 0.41309121 0.54398012 0.64886904 0.70065314 0.680087 0.58943778 0.4602128 0.33135244 0.22928526 0.14642656 0.074792348 0.012513093 -0.042819582][0.1936093 0.25342581 0.35060805 0.46997994 0.56583852 0.59998465 0.555641 0.4480108 0.31655514 0.20016138 0.12008699 0.068395868 0.029250093 -0.00839708 -0.049534615][0.1494942 0.20985246 0.30938983 0.42769849 0.51655912 0.53294164 0.46822405 0.3496666 0.22066955 0.1164627 0.052986149 0.021768617 0.0026345216 -0.021321846 -0.054492105][0.087141909 0.15923078 0.2713412 0.39257839 0.47355497 0.4743717 0.3968105 0.27577013 0.15370917 0.060415424 0.0074241068 -0.013212334 -0.02235711 -0.038932785 -0.065661833][0.011870693 0.091968454 0.20891325 0.32313493 0.38986611 0.37949783 0.30197757 0.19235596 0.086373277 0.0072804159 -0.036151528 -0.050408039 -0.054352611 -0.065559864 -0.084909692]]...]
INFO - root - 2017-12-10 20:03:52.584745: step 48510, loss = 0.70, batch loss = 0.64 (10.8 examples/sec; 0.739 sec/batch; 58h:18m:51s remains)
INFO - root - 2017-12-10 20:04:00.408604: step 48520, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 61h:27m:36s remains)
INFO - root - 2017-12-10 20:04:08.238287: step 48530, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 61h:00m:55s remains)
INFO - root - 2017-12-10 20:04:16.151902: step 48540, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.819 sec/batch; 64h:37m:58s remains)
INFO - root - 2017-12-10 20:04:23.952188: step 48550, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 63h:15m:03s remains)
INFO - root - 2017-12-10 20:04:31.531248: step 48560, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 59h:43m:37s remains)
INFO - root - 2017-12-10 20:04:39.340619: step 48570, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.774 sec/batch; 61h:02m:58s remains)
INFO - root - 2017-12-10 20:04:47.271583: step 48580, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 61h:50m:11s remains)
INFO - root - 2017-12-10 20:04:54.957006: step 48590, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 62h:02m:12s remains)
INFO - root - 2017-12-10 20:05:02.767784: step 48600, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 61h:30m:44s remains)
2017-12-10 20:05:03.590536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.11704651 -0.11255284 -0.10734802 -0.10412438 -0.10348833 -0.10499254 -0.10773235 -0.11055428 -0.11202236 -0.11169806 -0.10988373 -0.10796714 -0.10718773 -0.10757671 -0.10875001][-0.11019721 -0.10014915 -0.089597918 -0.082828596 -0.081180796 -0.083479971 -0.087723143 -0.092241704 -0.095089868 -0.094778739 -0.091314405 -0.088180616 -0.088239722 -0.092028923 -0.098102756][-0.071005993 -0.050076824 -0.0281324 -0.012378004 -0.0066194152 -0.008807851 -0.014832116 -0.02235998 -0.029376445 -0.032204334 -0.029608488 -0.027266491 -0.031314425 -0.042823866 -0.059295505][0.010116464 0.050487194 0.093605645 0.1265623 0.14072983 0.13929543 0.13043658 0.11741234 0.10128301 0.089171819 0.084583186 0.079970568 0.06518326 0.039197702 0.0062816986][0.13171832 0.19903956 0.27052379 0.32672727 0.35255343 0.35253549 0.34025735 0.31929615 0.2884607 0.25890431 0.23822859 0.21825942 0.18479785 0.13812366 0.084753528][0.27224672 0.36478725 0.46170974 0.53891033 0.575762 0.57788092 0.56379437 0.53586209 0.48867679 0.43671387 0.39289638 0.35027102 0.29279828 0.22359692 0.15090516][0.39656821 0.50031286 0.60736597 0.69397289 0.73751491 0.74353176 0.73224235 0.7019645 0.64195877 0.56836659 0.49992934 0.43301722 0.35328311 0.26674432 0.18199432][0.47197723 0.56899667 0.66757023 0.75028908 0.7957744 0.80630559 0.79899108 0.76719683 0.69767129 0.60739958 0.51984036 0.43525013 0.34313861 0.25126714 0.16644712][0.48618248 0.56098139 0.63638955 0.70414317 0.74608588 0.75834709 0.75066632 0.7140522 0.63856745 0.54209071 0.44892934 0.36098334 0.27112582 0.18669803 0.11193459][0.45738825 0.50481504 0.55223161 0.59944236 0.63205618 0.63886487 0.62164211 0.57396436 0.49656117 0.40858912 0.32890782 0.25610992 0.18332186 0.11519289 0.053164124][0.42231259 0.4517175 0.47827083 0.50601 0.52277237 0.51299137 0.47482231 0.4092308 0.33239025 0.26516554 0.21539113 0.1729383 0.12650853 0.077403463 0.024256945][0.41475391 0.44303718 0.4614163 0.47402316 0.46938387 0.43237239 0.36469311 0.27748835 0.20208646 0.16061629 0.14791223 0.14121668 0.12122355 0.086560585 0.03454639][0.44915834 0.48974159 0.50888419 0.50992113 0.48202759 0.4146747 0.31768626 0.21110168 0.13647836 0.11674746 0.1356445 0.15776414 0.15551454 0.12676316 0.069583938][0.51622051 0.5723179 0.59498209 0.58700764 0.53876895 0.44543898 0.325298 0.20427208 0.12860258 0.1213604 0.15878452 0.19693333 0.20206177 0.17281321 0.10893229][0.585995 0.65590459 0.68420446 0.67101043 0.60799527 0.49534431 0.35886478 0.22789067 0.14889926 0.14436212 0.18689686 0.22969759 0.23550664 0.20338553 0.13471101]]...]
INFO - root - 2017-12-10 20:05:11.436172: step 48610, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 61h:06m:40s remains)
INFO - root - 2017-12-10 20:05:19.329761: step 48620, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 61h:24m:11s remains)
INFO - root - 2017-12-10 20:05:27.183168: step 48630, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 61h:55m:36s remains)
INFO - root - 2017-12-10 20:05:34.762607: step 48640, loss = 0.71, batch loss = 0.66 (10.6 examples/sec; 0.754 sec/batch; 59h:25m:04s remains)
INFO - root - 2017-12-10 20:05:42.538508: step 48650, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 60h:24m:55s remains)
INFO - root - 2017-12-10 20:05:50.431754: step 48660, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 62h:53m:21s remains)
INFO - root - 2017-12-10 20:05:58.214401: step 48670, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 63h:40m:14s remains)
INFO - root - 2017-12-10 20:06:05.823954: step 48680, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 63h:39m:35s remains)
INFO - root - 2017-12-10 20:06:13.738730: step 48690, loss = 0.67, batch loss = 0.62 (10.0 examples/sec; 0.799 sec/batch; 63h:01m:30s remains)
INFO - root - 2017-12-10 20:06:21.498538: step 48700, loss = 0.69, batch loss = 0.63 (10.8 examples/sec; 0.744 sec/batch; 58h:37m:26s remains)
2017-12-10 20:06:22.421162: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32804516 0.32443947 0.3177194 0.31541914 0.31709182 0.31436193 0.29609084 0.25821894 0.21406232 0.18424518 0.18267037 0.21184051 0.25976589 0.31081483 0.34843656][0.32639095 0.33150837 0.33559164 0.3456991 0.36011702 0.36960065 0.36026931 0.32467452 0.27490357 0.23196833 0.21203449 0.22070748 0.25062719 0.29140443 0.32912][0.31427243 0.32154697 0.33149517 0.35136271 0.37855151 0.40367264 0.41036734 0.38612711 0.33869129 0.28632808 0.24723324 0.23120698 0.23636608 0.2582258 0.2872408][0.31797031 0.31845495 0.32327718 0.34132594 0.37152627 0.40607235 0.42741761 0.418501 0.38072047 0.3276796 0.27802527 0.24495007 0.23051064 0.23467982 0.25174093][0.33041233 0.31938875 0.30932453 0.31371883 0.33604077 0.37136349 0.40216756 0.40850496 0.38574582 0.34275359 0.29752243 0.26459736 0.24710888 0.2444777 0.25130787][0.34116268 0.31606188 0.28579664 0.27110422 0.28087607 0.31263784 0.34925658 0.3692826 0.36368674 0.33731928 0.30719715 0.28766102 0.28087971 0.28242564 0.28381136][0.36479944 0.32215634 0.27122462 0.2415695 0.24621245 0.28245527 0.33097643 0.36656004 0.37510851 0.3585701 0.33558181 0.32277921 0.323377 0.33002603 0.32977825][0.39686108 0.33449921 0.26547968 0.22832154 0.23804517 0.28877029 0.35588077 0.40769264 0.42390773 0.4046531 0.37329957 0.3527903 0.35103437 0.36010778 0.36258844][0.41223058 0.33413967 0.25329557 0.21511048 0.2352277 0.30337042 0.3890146 0.45437154 0.4730438 0.44384274 0.39546674 0.35907561 0.34962344 0.36032608 0.37016743][0.39259312 0.3103812 0.22884119 0.19612339 0.22729418 0.30826843 0.40496957 0.47687078 0.49416822 0.45522574 0.39267 0.34442317 0.33149365 0.34802836 0.37000093][0.333977 0.25823173 0.18586405 0.16310579 0.20268475 0.28832015 0.38622427 0.4579353 0.47403643 0.43240711 0.36574721 0.31410232 0.30198288 0.3256368 0.36052594][0.2659184 0.20287812 0.14424439 0.13077493 0.17250989 0.25422987 0.3462685 0.41460896 0.43200797 0.39503303 0.33280715 0.28339246 0.27317688 0.30212471 0.34757045][0.21864687 0.17107093 0.12674588 0.11905903 0.15605472 0.22627069 0.30616435 0.36721066 0.38497624 0.35421196 0.29866749 0.25229582 0.24205793 0.27194515 0.32292858][0.20748557 0.17645854 0.14408818 0.13726966 0.16335878 0.21562895 0.27693954 0.3242968 0.33655763 0.30782732 0.25661153 0.21192047 0.19971211 0.22714698 0.27995759][0.22720365 0.20956653 0.18398237 0.17262352 0.18292448 0.21240394 0.24953614 0.27728051 0.27931184 0.2506057 0.20506349 0.16571254 0.15554279 0.18326826 0.23892552]]...]
INFO - root - 2017-12-10 20:06:30.232659: step 48710, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 60h:51m:16s remains)
INFO - root - 2017-12-10 20:06:37.892403: step 48720, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 60h:11m:12s remains)
INFO - root - 2017-12-10 20:06:45.773744: step 48730, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 64h:07m:31s remains)
INFO - root - 2017-12-10 20:06:53.651561: step 48740, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 64h:16m:19s remains)
INFO - root - 2017-12-10 20:07:01.566658: step 48750, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 61h:22m:31s remains)
INFO - root - 2017-12-10 20:07:09.350428: step 48760, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 60h:25m:40s remains)
INFO - root - 2017-12-10 20:07:17.055361: step 48770, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.777 sec/batch; 61h:16m:11s remains)
INFO - root - 2017-12-10 20:07:24.908402: step 48780, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 63h:04m:12s remains)
INFO - root - 2017-12-10 20:07:32.721584: step 48790, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.752 sec/batch; 59h:17m:38s remains)
INFO - root - 2017-12-10 20:07:40.404935: step 48800, loss = 0.70, batch loss = 0.64 (12.1 examples/sec; 0.663 sec/batch; 52h:14m:23s remains)
2017-12-10 20:07:41.378081: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23003517 0.26635134 0.29604349 0.31052944 0.30361581 0.27271149 0.21195447 0.12833062 0.045024239 -0.014853703 -0.04696561 -0.058893908 -0.056590617 -0.047002066 -0.037698198][0.23796569 0.26053154 0.27162251 0.26789972 0.24761717 0.21169132 0.15569045 0.083674215 0.012727628 -0.03817391 -0.064961508 -0.073317312 -0.067772612 -0.054793149 -0.041481722][0.25038022 0.25464383 0.2430575 0.21924871 0.18624562 0.148211 0.10223601 0.04739425 -0.0078616105 -0.050306905 -0.075197957 -0.083983459 -0.079016313 -0.065543517 -0.050189137][0.26321736 0.25114119 0.22202581 0.18653704 0.15132545 0.12080029 0.090954162 0.055000409 0.013148862 -0.026583416 -0.0574085 -0.075366721 -0.078845657 -0.07138031 -0.058519464][0.2652961 0.24348436 0.20850208 0.17632203 0.15449403 0.14430918 0.13753051 0.1211498 0.088654846 0.045383364 0.00075483328 -0.03566594 -0.056825 -0.062718384 -0.058095161][0.24866155 0.22463144 0.1946698 0.17785032 0.18054983 0.19904111 0.21923554 0.22239165 0.19705594 0.14831002 0.087506443 0.028224492 -0.016730374 -0.042033382 -0.050433271][0.21406454 0.19306581 0.17396314 0.17659453 0.20577973 0.25286359 0.29903623 0.32092762 0.30369687 0.25295767 0.18063208 0.10179386 0.033213913 -0.013290578 -0.036642488][0.17181596 0.15601759 0.14763907 0.16584547 0.21440342 0.28139311 0.34571016 0.38223627 0.37422869 0.32701096 0.25171587 0.16309123 0.079161108 0.016789574 -0.019281944][0.13380449 0.12500079 0.12533334 0.15234269 0.20895761 0.28137225 0.34924406 0.39012003 0.38786006 0.34668013 0.27609542 0.19003543 0.10487084 0.0381502 -0.00342194][0.11017503 0.11027308 0.11847389 0.14950427 0.20436756 0.26786458 0.32287505 0.35307887 0.34628218 0.30658048 0.24295492 0.1686995 0.096127763 0.039264038 0.0037887727][0.10615488 0.1161347 0.13200867 0.16447067 0.21096876 0.25566724 0.28573269 0.29212075 0.26920888 0.22355969 0.16565692 0.10765179 0.056195606 0.018951105 -0.0015691224][0.11969556 0.13966057 0.16293377 0.19546068 0.2301757 0.25190413 0.25263473 0.23022334 0.18627614 0.13103884 0.077547982 0.036381681 0.0081550339 -0.0071720281 -0.011233524][0.13964291 0.16843882 0.19766322 0.2284943 0.24989173 0.24904098 0.2229899 0.1754228 0.11424039 0.051976856 0.0032576525 -0.023203278 -0.031309418 -0.02821829 -0.019636659][0.15238157 0.18588483 0.21764262 0.24443731 0.25285867 0.23406091 0.18966191 0.12723796 0.058362238 -0.0044717486 -0.046897937 -0.062000383 -0.05632126 -0.041753631 -0.026494604][0.15438929 0.18670481 0.21598516 0.23667118 0.23436329 0.20468748 0.15265936 0.08735019 0.020785071 -0.036249619 -0.07072527 -0.077579491 -0.064605586 -0.045568187 -0.029073976]]...]
INFO - root - 2017-12-10 20:07:49.182647: step 48810, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 60h:08m:16s remains)
INFO - root - 2017-12-10 20:07:57.133123: step 48820, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 63h:07m:33s remains)
INFO - root - 2017-12-10 20:08:05.068822: step 48830, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 62h:27m:30s remains)
INFO - root - 2017-12-10 20:08:12.812053: step 48840, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 61h:08m:11s remains)
INFO - root - 2017-12-10 20:08:20.577214: step 48850, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 59h:47m:57s remains)
INFO - root - 2017-12-10 20:08:28.262091: step 48860, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 61h:37m:23s remains)
INFO - root - 2017-12-10 20:08:36.081719: step 48870, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 60h:58m:35s remains)
INFO - root - 2017-12-10 20:08:43.763543: step 48880, loss = 0.71, batch loss = 0.66 (11.0 examples/sec; 0.729 sec/batch; 57h:24m:14s remains)
INFO - root - 2017-12-10 20:08:51.696395: step 48890, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 60h:12m:48s remains)
INFO - root - 2017-12-10 20:08:59.704536: step 48900, loss = 0.71, batch loss = 0.66 (10.6 examples/sec; 0.754 sec/batch; 59h:26m:05s remains)
2017-12-10 20:09:00.543957: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.41113186 0.39426002 0.34179127 0.26324439 0.18379386 0.13357814 0.12397633 0.1485579 0.19290206 0.24328299 0.28685203 0.3115572 0.31559184 0.31029442 0.30321544][0.33091941 0.31825203 0.27336371 0.20374338 0.13216162 0.086405046 0.076431729 0.094790526 0.12772675 0.16482383 0.19867128 0.2207673 0.22965303 0.23519194 0.24386574][0.25142437 0.24113381 0.20537817 0.14786628 0.087006345 0.046746321 0.035648312 0.04563731 0.064692825 0.085914396 0.10675843 0.12353251 0.13603377 0.15288532 0.17876045][0.19426911 0.18415304 0.15801622 0.11547334 0.069444276 0.039335832 0.032038506 0.038858075 0.048082802 0.054833539 0.060200833 0.066051215 0.075570896 0.098339424 0.13808256][0.16046081 0.14991592 0.13541359 0.11333832 0.09078984 0.082501538 0.092157289 0.10763434 0.11444782 0.10839027 0.092964455 0.075717367 0.067365482 0.08301653 0.12676489][0.14697166 0.13550349 0.13415116 0.13743606 0.14775737 0.17443846 0.21487996 0.24998353 0.25995412 0.24073176 0.19784817 0.14585358 0.10597444 0.10121597 0.13690354][0.14165793 0.12884608 0.13892344 0.16765763 0.21446429 0.28214881 0.35909909 0.41715083 0.43062645 0.39652255 0.32398844 0.23568083 0.16278891 0.13442956 0.15565231][0.13156106 0.11829431 0.1382053 0.19007081 0.27049035 0.37418121 0.47985718 0.55223185 0.56288904 0.51230204 0.41500154 0.30019224 0.20394684 0.15594147 0.15912271][0.10496355 0.093902975 0.12268614 0.19195619 0.29531953 0.41941264 0.5360564 0.6080097 0.60905409 0.54461 0.43408614 0.30956519 0.20620887 0.14880598 0.13719581][0.0676427 0.064015187 0.1024534 0.18197513 0.29269734 0.41542044 0.52094597 0.57723808 0.56573349 0.4964706 0.38952076 0.27438539 0.1799923 0.12335277 0.10204503][0.037324648 0.047128215 0.095416494 0.17714527 0.27905518 0.37970981 0.45510495 0.48426846 0.46010846 0.39593437 0.30824047 0.21905297 0.14745459 0.10180468 0.078188688][0.027262254 0.049309175 0.10271528 0.17870295 0.26105946 0.32897493 0.36656833 0.36609811 0.33302858 0.2824026 0.22487508 0.17191526 0.13156314 0.10398249 0.084207155][0.045741759 0.07227245 0.1224371 0.18525654 0.24327496 0.27817127 0.28202415 0.25858498 0.22371703 0.19212988 0.1675839 0.15066402 0.13993995 0.12938066 0.11381792][0.08051841 0.10213914 0.13982797 0.18263602 0.21464677 0.22203109 0.20322916 0.16910945 0.1409816 0.13088489 0.13620085 0.14843394 0.15987042 0.16105296 0.1478729][0.11089654 0.12334406 0.14342284 0.16338788 0.17138135 0.15922579 0.12965773 0.096251346 0.080828331 0.091301426 0.11948983 0.15037952 0.1731535 0.17812911 0.16408105]]...]
INFO - root - 2017-12-10 20:09:08.315567: step 48910, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 63h:44m:35s remains)
INFO - root - 2017-12-10 20:09:16.114345: step 48920, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.764 sec/batch; 60h:09m:29s remains)
INFO - root - 2017-12-10 20:09:23.898030: step 48930, loss = 0.70, batch loss = 0.64 (10.8 examples/sec; 0.741 sec/batch; 58h:22m:56s remains)
INFO - root - 2017-12-10 20:09:31.886849: step 48940, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 62h:46m:37s remains)
INFO - root - 2017-12-10 20:09:39.490826: step 48950, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 61h:59m:36s remains)
INFO - root - 2017-12-10 20:09:47.136592: step 48960, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 63h:08m:05s remains)
INFO - root - 2017-12-10 20:09:55.006309: step 48970, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.789 sec/batch; 62h:07m:06s remains)
INFO - root - 2017-12-10 20:10:02.941000: step 48980, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 62h:20m:12s remains)
INFO - root - 2017-12-10 20:10:10.748013: step 48990, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 60h:28m:02s remains)
INFO - root - 2017-12-10 20:10:18.571185: step 49000, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 62h:59m:26s remains)
2017-12-10 20:10:19.389895: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12549755 0.10929625 0.094814658 0.094378136 0.10859694 0.12787414 0.14206651 0.14335006 0.1277812 0.099104024 0.065718584 0.037957124 0.023549786 0.022122826 0.033167277][0.12945107 0.11693577 0.10351721 0.10346761 0.11735956 0.13585506 0.14844032 0.14599995 0.12603155 0.09302295 0.056512337 0.027284512 0.01433961 0.018191999 0.036318704][0.12808262 0.12177346 0.11100424 0.11026894 0.12235232 0.13915551 0.15052463 0.14667866 0.12614486 0.093114689 0.056584794 0.026923029 0.015053696 0.022736475 0.046144698][0.1212196 0.1223326 0.11494071 0.11343687 0.12427018 0.13995746 0.15041062 0.14613444 0.12806952 0.099521741 0.066754 0.038245354 0.026776049 0.036618 0.063244835][0.11210017 0.11911939 0.11435936 0.1121379 0.12218565 0.13711925 0.1466305 0.14220865 0.12805009 0.10698491 0.08154197 0.056980252 0.046274841 0.056812435 0.084591307][0.10851224 0.11886839 0.11504914 0.11119618 0.11942874 0.1330108 0.1417003 0.13769427 0.12817845 0.11593688 0.10003675 0.082036443 0.0730778 0.083184905 0.10960715][0.11504671 0.12798311 0.12492739 0.11858348 0.12317724 0.13415636 0.14236175 0.13914551 0.13364218 0.12928337 0.12257884 0.11234138 0.10602573 0.11579928 0.13996165][0.12909001 0.14582947 0.14606296 0.13802318 0.1373952 0.14374648 0.15084291 0.14834964 0.14559755 0.14776734 0.14958327 0.14791341 0.1444445 0.15323082 0.17488587][0.14394172 0.16717194 0.17355083 0.16582687 0.15915665 0.15815821 0.16146885 0.15810712 0.15660129 0.16414666 0.1743134 0.18188074 0.18132225 0.18762305 0.20522791][0.15325347 0.18151301 0.19396281 0.18787445 0.17600517 0.1675189 0.16551751 0.15964454 0.15762974 0.16846399 0.18564542 0.20136784 0.2039385 0.20738466 0.21942034][0.14807859 0.17718109 0.19320425 0.18962096 0.17541239 0.16186485 0.15536997 0.14751026 0.14470205 0.1560761 0.17655882 0.19705085 0.20242073 0.2037501 0.21038224][0.12309857 0.1484696 0.16536118 0.16485697 0.15203013 0.13756815 0.1297652 0.12226652 0.11938813 0.12903017 0.14856325 0.16964616 0.17677967 0.17706408 0.1792998][0.083739616 0.10338466 0.11933304 0.12226623 0.11396438 0.102671 0.096548468 0.090924926 0.087725557 0.093384132 0.10786673 0.12508549 0.13151573 0.13064699 0.12970175][0.040581781 0.053658545 0.065949939 0.069818467 0.065325826 0.058250554 0.055102624 0.052060504 0.049073767 0.050987527 0.059600651 0.071119182 0.075425521 0.073826723 0.071585223][0.0039267964 0.0095890425 0.016414579 0.018968467 0.016882513 0.013899262 0.014103171 0.014011879 0.012493853 0.012720696 0.017051877 0.023311118 0.025115239 0.022948023 0.020363521]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 20:10:27.232911: step 49010, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 61h:30m:19s remains)
INFO - root - 2017-12-10 20:10:35.067458: step 49020, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.807 sec/batch; 63h:33m:37s remains)
INFO - root - 2017-12-10 20:10:42.784991: step 49030, loss = 0.68, batch loss = 0.62 (12.6 examples/sec; 0.637 sec/batch; 50h:07m:33s remains)
INFO - root - 2017-12-10 20:10:50.398530: step 49040, loss = 0.69, batch loss = 0.63 (10.8 examples/sec; 0.742 sec/batch; 58h:24m:47s remains)
INFO - root - 2017-12-10 20:10:58.147654: step 49050, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 59h:37m:37s remains)
INFO - root - 2017-12-10 20:11:06.020046: step 49060, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 62h:21m:19s remains)
INFO - root - 2017-12-10 20:11:13.810718: step 49070, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.790 sec/batch; 62h:10m:38s remains)
INFO - root - 2017-12-10 20:11:21.766332: step 49080, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 61h:53m:17s remains)
INFO - root - 2017-12-10 20:11:29.548609: step 49090, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 60h:07m:55s remains)
INFO - root - 2017-12-10 20:11:37.382150: step 49100, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 61h:51m:43s remains)
2017-12-10 20:11:38.169556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0087881209 0.038766231 0.10334644 0.17826214 0.24524675 0.28892207 0.30382046 0.29253498 0.26058581 0.21450585 0.16198575 0.10639701 0.053903613 0.015023178 -0.0067067188][-0.00844606 0.04455347 0.12006427 0.20939218 0.29342163 0.35420209 0.38373989 0.38189086 0.35461578 0.31051779 0.25642416 0.19485898 0.13372056 0.086783379 0.05863509][-0.0024362947 0.059797395 0.14943936 0.2543433 0.351147 0.41990426 0.45363867 0.45424727 0.43265164 0.40059081 0.36242586 0.31317839 0.2565636 0.20671843 0.16967361][0.012552674 0.092047378 0.20279878 0.32518536 0.42965183 0.49492118 0.51888943 0.51070482 0.49137312 0.47741917 0.46928287 0.44938177 0.41104814 0.36373875 0.31483266][0.035682015 0.13901718 0.27780992 0.4225547 0.536398 0.59542078 0.60275489 0.57579154 0.54800171 0.5434463 0.560803 0.57047564 0.55326933 0.51141673 0.45287257][0.059205752 0.18765776 0.35749403 0.5298416 0.65964705 0.71841884 0.71206719 0.66455531 0.62047148 0.61253977 0.63977516 0.66397029 0.65640324 0.61372894 0.54453224][0.075165622 0.22148252 0.41518566 0.61210328 0.76215094 0.83154386 0.82338834 0.76259172 0.69978249 0.67458856 0.68861145 0.70183289 0.68438351 0.63258761 0.55600137][0.078797549 0.23060641 0.43250474 0.64031935 0.80430132 0.88726056 0.88486785 0.81753218 0.73624271 0.68526262 0.67059803 0.6565491 0.61854935 0.5551191 0.47624424][0.0637858 0.20582573 0.3961421 0.59412271 0.75572425 0.84461486 0.84890431 0.78208953 0.68989408 0.61736566 0.57455361 0.53385097 0.47805604 0.40798396 0.33329281][0.037125673 0.1593719 0.32345772 0.49364144 0.63448244 0.71545023 0.72124112 0.65886009 0.56449163 0.47957021 0.41643175 0.35767585 0.29351631 0.22690366 0.16543961][0.0087332157 0.10546409 0.23481573 0.36707821 0.47580087 0.5385685 0.54144025 0.48690993 0.40048066 0.31654915 0.24824232 0.18735628 0.12881355 0.075548932 0.031614739][-0.015097535 0.055545375 0.15132466 0.24926139 0.3291333 0.37348863 0.37128711 0.32309949 0.24787842 0.17395592 0.11504804 0.068137795 0.028166689 -0.0047765812 -0.030312937][-0.023754673 0.027925773 0.1003025 0.17622806 0.23849043 0.27012557 0.26186514 0.21511498 0.14817064 0.087293379 0.0463949 0.022373155 0.0060552792 -0.0067952005 -0.017995583][-0.015485505 0.028806925 0.092487022 0.16166736 0.21986793 0.24807756 0.2371331 0.19001862 0.12779973 0.077790558 0.053884871 0.050513033 0.053320609 0.0547124 0.052334238][0.0054329415 0.053193696 0.12056749 0.1935959 0.25544509 0.28572983 0.27530506 0.22748888 0.16569559 0.11949463 0.10366312 0.11084028 0.12315325 0.13170306 0.13422722]]...]
INFO - root - 2017-12-10 20:11:45.976788: step 49110, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 62h:07m:42s remains)
INFO - root - 2017-12-10 20:11:53.363193: step 49120, loss = 0.68, batch loss = 0.63 (9.7 examples/sec; 0.825 sec/batch; 64h:57m:14s remains)
INFO - root - 2017-12-10 20:12:01.122356: step 49130, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 60h:36m:32s remains)
INFO - root - 2017-12-10 20:12:08.884065: step 49140, loss = 0.70, batch loss = 0.65 (11.0 examples/sec; 0.728 sec/batch; 57h:20m:13s remains)
INFO - root - 2017-12-10 20:12:16.665439: step 49150, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 60h:53m:58s remains)
INFO - root - 2017-12-10 20:12:24.582422: step 49160, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.827 sec/batch; 65h:03m:03s remains)
INFO - root - 2017-12-10 20:12:32.518436: step 49170, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.788 sec/batch; 62h:02m:42s remains)
INFO - root - 2017-12-10 20:12:40.364819: step 49180, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 59h:56m:17s remains)
INFO - root - 2017-12-10 20:12:48.147847: step 49190, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.754 sec/batch; 59h:18m:23s remains)
INFO - root - 2017-12-10 20:12:55.810763: step 49200, loss = 0.72, batch loss = 0.66 (13.2 examples/sec; 0.608 sec/batch; 47h:49m:18s remains)
2017-12-10 20:12:56.585000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.07036557 -0.058876894 -0.036380768 -0.0095140114 0.013206801 0.02237929 0.013842959 -0.0085582985 -0.035251196 -0.057178635 -0.070099816 -0.073887944 -0.071764894 -0.068116508 -0.064717069][-0.078219272 -0.05772781 -0.020097028 0.025162324 0.065237306 0.0866303 0.082050532 0.054350786 0.015786987 -0.020377494 -0.047189996 -0.06293115 -0.070082791 -0.072520211 -0.0729221][-0.075527273 -0.04568468 0.0082793636 0.075074591 0.13841937 0.18090239 0.19046572 0.16562678 0.11857772 0.066484 0.019642215 -0.016884828 -0.042265505 -0.05769176 -0.066771932][-0.068949118 -0.031692978 0.036602885 0.12469735 0.21496581 0.28691438 0.32184333 0.31040862 0.26082617 0.1931262 0.1215421 0.05641374 0.0043569454 -0.030876137 -0.053053461][-0.063937642 -0.022891702 0.055695515 0.16272861 0.28168553 0.38931647 0.45852512 0.46889415 0.42121708 0.33785456 0.23803674 0.13941078 0.055912863 -0.0025414887 -0.039595246][-0.059669949 -0.017954133 0.066723637 0.18930712 0.33553043 0.4782798 0.58103853 0.61200196 0.56403309 0.46202826 0.3324472 0.20179729 0.090816326 0.01340284 -0.034631144][-0.056508213 -0.015864365 0.07163962 0.20526423 0.3721973 0.5401758 0.664083 0.70416284 0.64837378 0.52476233 0.36825362 0.21405692 0.088020116 0.003939705 -0.044567887][-0.05857892 -0.021726076 0.063861281 0.2009314 0.37676015 0.554311 0.682806 0.71974158 0.65194625 0.50988704 0.33557981 0.17111281 0.045311045 -0.031499308 -0.069107883][-0.0675968 -0.038264092 0.038506281 0.16755091 0.33570665 0.50390637 0.62101191 0.64736575 0.571803 0.42486268 0.2511209 0.095188722 -0.014897091 -0.074029289 -0.095218696][-0.07833457 -0.057450689 0.0054262318 0.11537517 0.25852132 0.39781949 0.4883078 0.49865055 0.42185813 0.28637952 0.13435166 0.0066640857 -0.074419573 -0.1098837 -0.1145137][-0.082469434 -0.067065544 -0.019027354 0.06463071 0.16994111 0.26627827 0.32027835 0.31211546 0.23970611 0.12770893 0.011592133 -0.076816335 -0.12364237 -0.13488851 -0.12527637][-0.074379452 -0.057473555 -0.019698048 0.037885971 0.10196624 0.1509183 0.16580573 0.13927925 0.073945887 -0.010363793 -0.0872551 -0.13656266 -0.1529232 -0.14510085 -0.12598288][-0.053335506 -0.025778275 0.011651738 0.052259851 0.083282337 0.091690816 0.072187632 0.029065242 -0.030565836 -0.091670856 -0.13731675 -0.1580987 -0.15562354 -0.13897727 -0.11792243][-0.021493662 0.026408007 0.077578582 0.11875786 0.13554713 0.1192857 0.073883519 0.01362506 -0.049167544 -0.10219709 -0.13495743 -0.14472504 -0.1377192 -0.12269296 -0.10636707][0.013537133 0.086621672 0.16292387 0.22269614 0.24704713 0.22593598 0.16586196 0.088419557 0.011656027 -0.051705584 -0.092913672 -0.11052664 -0.11203544 -0.10605411 -0.097712494]]...]
INFO - root - 2017-12-10 20:13:04.315328: step 49210, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 61h:38m:50s remains)
INFO - root - 2017-12-10 20:13:12.131590: step 49220, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 60h:40m:51s remains)
INFO - root - 2017-12-10 20:13:19.942272: step 49230, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 61h:32m:09s remains)
INFO - root - 2017-12-10 20:13:27.840740: step 49240, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 62h:53m:52s remains)
INFO - root - 2017-12-10 20:13:35.634897: step 49250, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 62h:30m:13s remains)
INFO - root - 2017-12-10 20:13:43.461887: step 49260, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 63h:05m:35s remains)
INFO - root - 2017-12-10 20:13:51.233588: step 49270, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 63h:02m:27s remains)
INFO - root - 2017-12-10 20:13:59.124208: step 49280, loss = 0.68, batch loss = 0.62 (11.0 examples/sec; 0.725 sec/batch; 57h:02m:03s remains)
INFO - root - 2017-12-10 20:14:06.987920: step 49290, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 61h:04m:54s remains)
INFO - root - 2017-12-10 20:14:14.685772: step 49300, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.749 sec/batch; 58h:57m:18s remains)
2017-12-10 20:14:15.454050: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.47779939 0.50885785 0.48329911 0.41880995 0.34843233 0.29748145 0.27081159 0.26164463 0.25706774 0.25455308 0.25769806 0.28797433 0.35840333 0.47127137 0.61232865][0.46441141 0.485628 0.4516345 0.38837186 0.33475482 0.3112455 0.31347516 0.3241885 0.32330927 0.3061212 0.28104407 0.28469166 0.34380129 0.46408334 0.62239188][0.41009283 0.42102191 0.38508031 0.3375929 0.32066891 0.34435147 0.38997489 0.42775992 0.42909402 0.38728341 0.31641632 0.27001366 0.29023123 0.38927427 0.5375315][0.34266165 0.34934321 0.32304791 0.30609384 0.33990654 0.42048615 0.51305151 0.57692569 0.57699537 0.50558007 0.38244337 0.27429613 0.23621382 0.28735566 0.39774173][0.26885116 0.27658421 0.26690814 0.28508052 0.36787054 0.49847129 0.62967581 0.71439797 0.71382928 0.61984658 0.45630324 0.29509494 0.19724104 0.18856844 0.24555945][0.19860594 0.20778604 0.21267261 0.25650463 0.36951235 0.52822065 0.6815182 0.78116375 0.78693479 0.69006044 0.514656 0.32672077 0.18475682 0.11957029 0.11993414][0.14310162 0.14952537 0.15946555 0.21032535 0.32594094 0.48403323 0.64027971 0.74920386 0.771158 0.69558537 0.5425455 0.3624734 0.2034502 0.099245317 0.052631121][0.11592294 0.11373472 0.116576 0.15481181 0.24825676 0.38181132 0.52424932 0.63558024 0.67782038 0.63802528 0.53017479 0.38532236 0.23683646 0.11876579 0.046691056][0.12681359 0.11303014 0.10001061 0.11342201 0.17221943 0.27045441 0.38990444 0.49605998 0.55320692 0.54716223 0.48523495 0.38283142 0.2602309 0.15048234 0.077060476][0.17197351 0.14884229 0.11862094 0.10563269 0.130186 0.19348158 0.28621551 0.37820667 0.43679243 0.44812232 0.41400596 0.34221986 0.24598472 0.15726636 0.10129306][0.226764 0.20223366 0.16328824 0.13367657 0.13435096 0.1691743 0.23307158 0.30063567 0.34430733 0.3521294 0.3236365 0.26600289 0.19051167 0.12605962 0.094761446][0.26061156 0.24243262 0.20562495 0.17197691 0.1606916 0.17453167 0.20932499 0.24601023 0.26463526 0.25713289 0.22369105 0.17317511 0.1159442 0.074693188 0.0659123][0.25688791 0.247937 0.21945626 0.18998241 0.17517938 0.17503737 0.18500453 0.19332111 0.18859434 0.16712122 0.13034321 0.087740131 0.047832154 0.025529107 0.03190162][0.22276372 0.22002436 0.19942072 0.17634372 0.1627349 0.15618913 0.15170629 0.14368078 0.12692209 0.10054381 0.066268921 0.032943133 0.0071319356 -0.0028501016 0.010702606][0.18565843 0.1838062 0.16564979 0.1451728 0.13231455 0.12407921 0.11529247 0.10367371 0.086585417 0.063620776 0.0359083 0.011243152 -0.005183592 -0.0090614129 0.0060526128]]...]
INFO - root - 2017-12-10 20:14:23.424197: step 49310, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 62h:26m:26s remains)
INFO - root - 2017-12-10 20:14:31.286631: step 49320, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 62h:11m:58s remains)
INFO - root - 2017-12-10 20:14:39.161976: step 49330, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 63h:16m:12s remains)
INFO - root - 2017-12-10 20:14:47.105110: step 49340, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 61h:24m:54s remains)
INFO - root - 2017-12-10 20:14:55.016274: step 49350, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 61h:29m:31s remains)
INFO - root - 2017-12-10 20:15:02.708931: step 49360, loss = 0.69, batch loss = 0.63 (10.9 examples/sec; 0.732 sec/batch; 57h:35m:15s remains)
INFO - root - 2017-12-10 20:15:10.673467: step 49370, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.776 sec/batch; 61h:03m:17s remains)
INFO - root - 2017-12-10 20:15:18.615410: step 49380, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.821 sec/batch; 64h:33m:56s remains)
INFO - root - 2017-12-10 20:15:26.438779: step 49390, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.806 sec/batch; 63h:25m:08s remains)
INFO - root - 2017-12-10 20:15:34.270320: step 49400, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 62h:03m:55s remains)
2017-12-10 20:15:35.150394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.039220076 -0.037317455 -0.033433549 -0.031095019 -0.030373907 -0.031051576 -0.033735223 -0.037380956 -0.040452663 -0.041810382 -0.041840907 -0.04116692 -0.038488984 -0.033396054 -0.026513889][-0.033519808 -0.026704513 -0.017400945 -0.010177239 -0.0050317897 -0.0023120716 -0.0036162324 -0.0078292526 -0.012311578 -0.015761204 -0.019424792 -0.023537809 -0.02436986 -0.019335771 -0.0094410731][-0.024885422 -0.010413534 0.00850359 0.026054179 0.041859254 0.054219414 0.0593821 0.057184573 0.050900973 0.042121209 0.028799463 0.012709076 0.0020630385 0.0030299914 0.014004713][-0.014335315 0.011063133 0.045120753 0.080265351 0.11484223 0.14465861 0.16224886 0.1641445 0.15376869 0.13319261 0.10122983 0.064010151 0.036292803 0.028525269 0.037087481][-0.00048585894 0.039112665 0.093466692 0.15280488 0.21309741 0.26644912 0.30025461 0.30631703 0.28762251 0.24779591 0.18885368 0.12369161 0.07427039 0.054276824 0.056821395][0.018301155 0.073629916 0.14984001 0.23469068 0.32122958 0.3977353 0.44585195 0.45239517 0.42031091 0.35628641 0.26783356 0.17476277 0.10483783 0.07268054 0.06721241][0.042052247 0.11148167 0.20549412 0.30925047 0.41336417 0.50319785 0.55614644 0.55604285 0.50717455 0.42063728 0.30985683 0.19894615 0.11746395 0.077994473 0.065796442][0.068364911 0.1488844 0.25346193 0.36477041 0.4715648 0.55839825 0.60179859 0.58694255 0.52179885 0.42211482 0.30496806 0.19335844 0.1136345 0.074402176 0.059075259][0.089788362 0.17635539 0.28251186 0.38873523 0.48291 0.55109835 0.57294679 0.53959996 0.46470782 0.36706117 0.26269883 0.16803743 0.1029027 0.071533963 0.057894129][0.096314482 0.18109904 0.27859974 0.36833656 0.43844986 0.47860086 0.47496963 0.427233 0.35396451 0.27346241 0.19649039 0.13028626 0.087312587 0.068524271 0.060486093][0.083842672 0.15891357 0.24025318 0.30769411 0.35065442 0.36334792 0.339497 0.28635907 0.22457816 0.16898689 0.12352011 0.086994462 0.065816291 0.059393037 0.0580724][0.054867029 0.11464328 0.1770141 0.22349964 0.24505858 0.23965243 0.20708966 0.15865014 0.11354677 0.081643052 0.061368685 0.046681214 0.040495276 0.042341277 0.046274874][0.017974069 0.060405914 0.10487098 0.13583985 0.14583504 0.13480455 0.10544448 0.068985082 0.040316042 0.024679409 0.017921545 0.013428822 0.013380801 0.01840025 0.025225259][-0.014625931 0.011685779 0.04103516 0.061573129 0.067540579 0.059072737 0.0390629 0.015935859 -0.00071692996 -0.0088600619 -0.012303236 -0.015241563 -0.014991246 -0.010029751 -0.0021894132][-0.035607059 -0.022340935 -0.0050392016 0.0079044262 0.012543894 0.0086148465 -0.002133149 -0.014722756 -0.024187906 -0.03006256 -0.034456942 -0.038741194 -0.039998654 -0.036573205 -0.029628461]]...]
INFO - root - 2017-12-10 20:15:43.050115: step 49410, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.789 sec/batch; 62h:03m:15s remains)
INFO - root - 2017-12-10 20:15:50.969168: step 49420, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 62h:23m:51s remains)
INFO - root - 2017-12-10 20:15:59.041399: step 49430, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 63h:29m:29s remains)
INFO - root - 2017-12-10 20:16:06.820535: step 49440, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 60h:19m:41s remains)
INFO - root - 2017-12-10 20:16:14.769619: step 49450, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 60h:05m:56s remains)
INFO - root - 2017-12-10 20:16:22.669980: step 49460, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 62h:15m:09s remains)
INFO - root - 2017-12-10 20:16:30.516336: step 49470, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 61h:26m:44s remains)
INFO - root - 2017-12-10 20:16:38.181619: step 49480, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 60h:27m:05s remains)
INFO - root - 2017-12-10 20:16:46.033476: step 49490, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 60h:50m:53s remains)
INFO - root - 2017-12-10 20:16:53.979242: step 49500, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 61h:44m:00s remains)
2017-12-10 20:16:54.803697: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28546229 0.26183581 0.235625 0.22049555 0.2269679 0.25282049 0.28973651 0.33652633 0.37887582 0.39509594 0.38277894 0.33913237 0.25670105 0.14581491 0.039882731][0.30525568 0.27836752 0.24592263 0.22348018 0.22419816 0.24649878 0.28338382 0.3327944 0.37688521 0.39369428 0.38099724 0.33918878 0.25928196 0.14895509 0.043662753][0.31157702 0.27983665 0.23973933 0.20880972 0.19953683 0.21010067 0.23711786 0.27868327 0.31694764 0.33349505 0.32439527 0.29188 0.225007 0.1281662 0.033907533][0.28862989 0.26311785 0.23009509 0.20482123 0.194868 0.19709201 0.21425666 0.24563649 0.27370864 0.28426746 0.27412471 0.24637134 0.18827869 0.10118444 0.015947213][0.23670258 0.23548107 0.23181958 0.23257361 0.23779057 0.24256694 0.25838044 0.28513893 0.30368227 0.30246493 0.28184345 0.24607107 0.18203163 0.0914128 0.0055592502][0.1592173 0.1944242 0.23606172 0.27851483 0.31382233 0.33522981 0.36335263 0.39658082 0.41041979 0.39414614 0.35402054 0.29732314 0.2123502 0.10450463 0.0077839815][0.081551261 0.14983216 0.23281163 0.31508762 0.3833133 0.42885453 0.47822237 0.52512586 0.53836405 0.50688475 0.44363976 0.36058727 0.24885589 0.11958776 0.0096732033][0.035527505 0.11476716 0.21246001 0.31033936 0.39428109 0.45541248 0.52067274 0.5778681 0.59108621 0.55002618 0.47255808 0.37384167 0.2469831 0.10793495 -0.0050135804][0.011558365 0.075922243 0.15793599 0.2424341 0.31826714 0.37807804 0.44431537 0.50110978 0.51394945 0.4753437 0.40377134 0.31300765 0.19510415 0.06704279 -0.03415519][0.000804184 0.036689337 0.086721957 0.141391 0.19358464 0.23855452 0.29298317 0.34156606 0.35525942 0.32938081 0.279167 0.21352024 0.12133183 0.017226465 -0.0647916][0.0080775646 0.018096177 0.037172146 0.061749239 0.087907985 0.1140238 0.15388095 0.19571544 0.21468514 0.20581678 0.17934772 0.1394268 0.071611837 -0.012100108 -0.079786867][0.041242369 0.037091061 0.03647539 0.040535387 0.047499955 0.059440523 0.09088923 0.13314998 0.16119678 0.16575658 0.15353175 0.12697025 0.070229784 -0.005872169 -0.069934376][0.10003105 0.093252689 0.085150324 0.079983205 0.077232547 0.082344167 0.11191721 0.15803717 0.1931504 0.20239557 0.191737 0.16495879 0.10696788 0.027893668 -0.040797908][0.14840147 0.14492679 0.1370748 0.13075031 0.12597908 0.12982044 0.15930632 0.20575075 0.24102069 0.24786793 0.23341891 0.20304738 0.14255184 0.060475998 -0.012231355][0.15243061 0.1525815 0.14764471 0.14334247 0.1398326 0.14367579 0.16986409 0.20946079 0.2380735 0.24073245 0.22538899 0.1967739 0.14056627 0.063256018 -0.00582827]]...]
INFO - root - 2017-12-10 20:17:02.699931: step 49510, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 61h:56m:57s remains)
INFO - root - 2017-12-10 20:17:10.426215: step 49520, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 60h:01m:44s remains)
INFO - root - 2017-12-10 20:17:18.338094: step 49530, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 62h:00m:13s remains)
INFO - root - 2017-12-10 20:17:26.233860: step 49540, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 60h:42m:23s remains)
INFO - root - 2017-12-10 20:17:34.131603: step 49550, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 61h:25m:52s remains)
INFO - root - 2017-12-10 20:17:41.938245: step 49560, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 60h:57m:55s remains)
INFO - root - 2017-12-10 20:17:49.670816: step 49570, loss = 0.70, batch loss = 0.65 (9.5 examples/sec; 0.838 sec/batch; 65h:51m:24s remains)
INFO - root - 2017-12-10 20:17:57.780590: step 49580, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 63h:57m:14s remains)
INFO - root - 2017-12-10 20:18:05.660359: step 49590, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 60h:47m:34s remains)
INFO - root - 2017-12-10 20:18:13.333704: step 49600, loss = 0.69, batch loss = 0.63 (12.6 examples/sec; 0.637 sec/batch; 50h:03m:45s remains)
2017-12-10 20:18:14.145744: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31038114 0.32728496 0.32261515 0.30115345 0.27590436 0.2707735 0.28634211 0.30933154 0.32302633 0.3227759 0.2960788 0.230979 0.14435853 0.058384359 -0.0073985751][0.27431592 0.296541 0.30082604 0.29023468 0.27493244 0.27469903 0.288071 0.30149126 0.29990056 0.28361812 0.24687614 0.18074293 0.10067306 0.026024079 -0.02737838][0.23687415 0.26139504 0.27172956 0.26996645 0.2628389 0.26434174 0.27082181 0.27060127 0.25174055 0.21966936 0.17512324 0.11237663 0.044444244 -0.01385138 -0.051369596][0.22678028 0.25189623 0.26275027 0.26254353 0.25685012 0.25629166 0.25553757 0.24465796 0.21437259 0.17252603 0.12283419 0.06196586 0.0021077425 -0.044332124 -0.069865376][0.25278836 0.27905047 0.28574035 0.27873373 0.26652873 0.26114309 0.25714192 0.24403627 0.21247315 0.16890231 0.11544424 0.050444726 -0.011222313 -0.055323385 -0.076293811][0.30566087 0.33609414 0.33842951 0.32185909 0.30128306 0.29325017 0.29276136 0.285428 0.25868398 0.21635206 0.1562881 0.078706592 0.0043721963 -0.047163922 -0.070865668][0.360616 0.39928284 0.40222964 0.3815594 0.35890052 0.35473129 0.36313906 0.363781 0.34053487 0.29653743 0.22591172 0.13095643 0.038226649 -0.026653085 -0.057837434][0.38376954 0.43314731 0.44497824 0.43141505 0.41704389 0.42258906 0.43972391 0.44319075 0.41681546 0.36670822 0.28516737 0.17584394 0.0686314 -0.0071780249 -0.045087695][0.35490149 0.41432086 0.44199368 0.44586992 0.44786307 0.46462062 0.48595288 0.48573548 0.45139742 0.39377463 0.30526626 0.19048411 0.079385042 0.0010248643 -0.03880623][0.28027955 0.34492937 0.38839945 0.41193295 0.43122372 0.45596865 0.47550905 0.467684 0.42566866 0.36440381 0.27703366 0.16846669 0.065926775 -0.0051417239 -0.040954415][0.18046403 0.24214594 0.29387942 0.33114272 0.36233488 0.38963214 0.40323287 0.38824588 0.34359249 0.285797 0.20888609 0.11711116 0.033204537 -0.023126695 -0.050296947][0.081617087 0.13081169 0.17889296 0.218725 0.25275248 0.27730727 0.28404188 0.26515028 0.22392629 0.17603584 0.11682679 0.049011681 -0.010324749 -0.04775355 -0.06359572][0.00218993 0.033432387 0.068330541 0.099820532 0.12742034 0.14542092 0.14697504 0.12876885 0.095717408 0.061045874 0.022014664 -0.020182205 -0.054316506 -0.072702467 -0.077068634][-0.047369637 -0.032744974 -0.013571122 0.0045497888 0.020958941 0.031134527 0.02961662 0.014952227 -0.0083412239 -0.030163463 -0.051403735 -0.0720313 -0.085881993 -0.08960434 -0.085454144][-0.069101728 -0.065912671 -0.058961228 -0.052550949 -0.046751216 -0.043684795 -0.0469554 -0.057399884 -0.071860157 -0.083723895 -0.09252692 -0.098616414 -0.099484749 -0.094653152 -0.08628384]]...]
INFO - root - 2017-12-10 20:18:22.008216: step 49610, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 63h:56m:29s remains)
INFO - root - 2017-12-10 20:18:29.896735: step 49620, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 61h:04m:59s remains)
INFO - root - 2017-12-10 20:18:37.800312: step 49630, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 61h:16m:03s remains)
INFO - root - 2017-12-10 20:18:45.645141: step 49640, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.779 sec/batch; 61h:13m:35s remains)
INFO - root - 2017-12-10 20:18:53.668261: step 49650, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 63h:40m:39s remains)
INFO - root - 2017-12-10 20:19:01.418130: step 49660, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 62h:31m:02s remains)
INFO - root - 2017-12-10 20:19:09.286885: step 49670, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.779 sec/batch; 61h:10m:54s remains)
INFO - root - 2017-12-10 20:19:16.993576: step 49680, loss = 0.67, batch loss = 0.61 (11.2 examples/sec; 0.713 sec/batch; 56h:00m:13s remains)
INFO - root - 2017-12-10 20:19:24.981086: step 49690, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 62h:04m:38s remains)
INFO - root - 2017-12-10 20:19:32.816934: step 49700, loss = 0.69, batch loss = 0.64 (9.6 examples/sec; 0.834 sec/batch; 65h:33m:07s remains)
2017-12-10 20:19:33.595527: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.086362489 0.0931303 0.0943525 0.097313583 0.10621378 0.12034598 0.1349548 0.14041775 0.12606719 0.09119387 0.04400656 -0.0010793495 -0.033399668 -0.050631527 -0.055157728][0.16912574 0.18514344 0.18727528 0.18696995 0.19182602 0.20261969 0.21487774 0.21686018 0.19627073 0.14959189 0.086007722 0.025507616 -0.018248295 -0.043389019 -0.052899472][0.25516719 0.27930257 0.28071073 0.27462813 0.27233264 0.27710229 0.28505921 0.28268349 0.25755689 0.20268388 0.12640715 0.052638061 -0.0013016243 -0.033644326 -0.048506483][0.29481387 0.32202804 0.32221022 0.31321022 0.30793419 0.31047341 0.31706685 0.31338778 0.28751406 0.23022142 0.14820234 0.0675757 0.0084145591 -0.027070733 -0.044315744][0.28149503 0.30795643 0.30923057 0.30303937 0.30079132 0.30499384 0.3113479 0.30683991 0.28183448 0.22597437 0.14479537 0.064437717 0.0059551587 -0.027534425 -0.043224618][0.24163833 0.26747036 0.27328315 0.27475989 0.27925283 0.28651193 0.29243091 0.28679168 0.26272041 0.21001932 0.13262403 0.055420265 -0.00054626848 -0.031337429 -0.045079522][0.18789473 0.21565922 0.23134655 0.24661055 0.26329392 0.27843946 0.28748187 0.28201166 0.25728908 0.20416734 0.12715894 0.050499793 -0.00493615 -0.034699555 -0.047517229][0.12810959 0.15619484 0.18268016 0.21399575 0.2463015 0.27339655 0.28905368 0.28538668 0.25881168 0.20308323 0.12529987 0.047919713 -0.0087530753 -0.038866561 -0.051210906][0.069644533 0.096280158 0.13088585 0.17521475 0.2218899 0.26137316 0.2845208 0.2836611 0.25585806 0.19799447 0.12001792 0.043080341 -0.013914102 -0.044216517 -0.055706047][0.037483152 0.06417165 0.10466625 0.1573429 0.21342929 0.26095405 0.287506 0.28587526 0.25425214 0.19260783 0.11360545 0.03737602 -0.018421249 -0.047697626 -0.058364738][0.037828427 0.066258 0.10972872 0.16446842 0.22262377 0.27138758 0.29711068 0.29329681 0.25812814 0.19327269 0.11285282 0.036348611 -0.01875131 -0.047426265 -0.057999156][0.054116495 0.083978623 0.12590463 0.17570218 0.22859752 0.27309337 0.29609114 0.29119375 0.25495192 0.18904795 0.10846584 0.032681942 -0.020674016 -0.047746472 -0.057356853][0.069251508 0.09799625 0.13384643 0.173409 0.21620613 0.25309354 0.27173093 0.26573548 0.23025185 0.16695558 0.090639316 0.020365754 -0.027487349 -0.050771073 -0.057983749][0.071338013 0.094563052 0.12020299 0.14707196 0.17842646 0.20745493 0.2223881 0.21672347 0.18516237 0.12910654 0.0619145 0.00147855 -0.037707087 -0.055309735 -0.059061084][0.060459521 0.07316862 0.085986868 0.10012979 0.12073253 0.14239982 0.15412882 0.1496999 0.12423003 0.078735523 0.024505429 -0.0228059 -0.050901376 -0.061035737 -0.060180187]]...]
INFO - root - 2017-12-10 20:19:41.450625: step 49710, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 59h:31m:24s remains)
INFO - root - 2017-12-10 20:19:49.291754: step 49720, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 61h:23m:26s remains)
INFO - root - 2017-12-10 20:19:57.218262: step 49730, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 63h:41m:38s remains)
INFO - root - 2017-12-10 20:20:05.102881: step 49740, loss = 0.73, batch loss = 0.67 (9.8 examples/sec; 0.817 sec/batch; 64h:10m:19s remains)
INFO - root - 2017-12-10 20:20:12.809654: step 49750, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 62h:19m:14s remains)
INFO - root - 2017-12-10 20:20:20.593985: step 49760, loss = 0.70, batch loss = 0.64 (12.1 examples/sec; 0.662 sec/batch; 51h:57m:14s remains)
INFO - root - 2017-12-10 20:20:28.664892: step 49770, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 61h:37m:12s remains)
INFO - root - 2017-12-10 20:20:36.609337: step 49780, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 61h:45m:05s remains)
INFO - root - 2017-12-10 20:20:44.516819: step 49790, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 63h:30m:25s remains)
INFO - root - 2017-12-10 20:20:52.377152: step 49800, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 60h:39m:42s remains)
2017-12-10 20:20:53.243133: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26944068 0.27358109 0.26792431 0.26035771 0.25836664 0.27955726 0.32512784 0.37666023 0.41390437 0.42925522 0.41635808 0.38417077 0.37321058 0.39722079 0.43532163][0.19383587 0.20753422 0.21442759 0.21636976 0.21783718 0.23428209 0.26990736 0.31422833 0.35078549 0.37094113 0.36596182 0.34175384 0.33650374 0.36210352 0.39820096][0.12258241 0.14489181 0.16825889 0.1867515 0.19851032 0.21074751 0.22805919 0.24893913 0.26514354 0.27189976 0.26235092 0.24073902 0.23845942 0.26404157 0.29931375][0.079608068 0.1112828 0.15471974 0.1963558 0.22649771 0.2403629 0.23879008 0.227722 0.20906974 0.18549265 0.15620832 0.12635686 0.11876785 0.13767143 0.16778433][0.077353418 0.12387995 0.19227473 0.26221436 0.31678253 0.33903775 0.32329836 0.27927396 0.21890897 0.15476695 0.094456896 0.045251809 0.0224269 0.026012674 0.043292567][0.11497962 0.18211171 0.2766484 0.37379941 0.45264149 0.48748353 0.46503276 0.39473873 0.29688215 0.1938194 0.10105067 0.027604386 -0.016426614 -0.034333728 -0.03686313][0.17302898 0.26240489 0.37790895 0.49381366 0.58948743 0.63567758 0.61315864 0.52863187 0.40762287 0.27899653 0.16264108 0.068113215 0.0035543139 -0.035486482 -0.058781892][0.22066896 0.32634518 0.45274177 0.57549965 0.67734951 0.73046255 0.71262378 0.62735093 0.50168705 0.36585554 0.23984444 0.1326175 0.053177357 -0.0013516999 -0.040663637][0.22963518 0.33909622 0.46307126 0.58005327 0.6773454 0.73178864 0.7219345 0.64853472 0.53577632 0.41021097 0.28930727 0.18156312 0.098090895 0.038606539 -0.0067546694][0.20133826 0.29898438 0.40494829 0.50242478 0.58423895 0.63439429 0.63485992 0.58335626 0.49684802 0.39465857 0.29065681 0.19397321 0.118416 0.066460483 0.027633287][0.148699 0.22026795 0.29540467 0.36355957 0.42371455 0.46721652 0.48013604 0.45646077 0.40312061 0.33090383 0.25083333 0.1741266 0.11689357 0.083256304 0.061580256][0.094653971 0.13350214 0.17292503 0.20931795 0.24696934 0.28325978 0.30767152 0.31098908 0.29100728 0.2511228 0.19941434 0.14837679 0.11528681 0.10463338 0.10376256][0.058539446 0.067087084 0.074707031 0.084074296 0.10286675 0.13248469 0.16492881 0.18958449 0.19804093 0.18833537 0.16504259 0.14008182 0.13046195 0.14016177 0.15680128][0.039632555 0.028363686 0.015123201 0.006219191 0.010790478 0.032778572 0.06695351 0.10376962 0.13285078 0.14894252 0.15215674 0.15107328 0.16088416 0.18498866 0.21293016][0.031903353 0.012574761 -0.00923779 -0.027092794 -0.032268148 -0.019356305 0.0094305957 0.048048597 0.088177264 0.1230069 0.14805152 0.16671494 0.19100323 0.22329964 0.25567731]]...]
INFO - root - 2017-12-10 20:21:01.208260: step 49810, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 61h:03m:56s remains)
INFO - root - 2017-12-10 20:21:09.155409: step 49820, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 62h:54m:40s remains)
INFO - root - 2017-12-10 20:21:16.976424: step 49830, loss = 0.70, batch loss = 0.64 (10.9 examples/sec; 0.731 sec/batch; 57h:23m:42s remains)
INFO - root - 2017-12-10 20:21:24.593384: step 49840, loss = 0.70, batch loss = 0.64 (11.9 examples/sec; 0.675 sec/batch; 52h:58m:54s remains)
INFO - root - 2017-12-10 20:21:32.476851: step 49850, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.798 sec/batch; 62h:39m:23s remains)
INFO - root - 2017-12-10 20:21:40.401564: step 49860, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 62h:10m:30s remains)
INFO - root - 2017-12-10 20:21:48.358037: step 49870, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 60h:23m:19s remains)
INFO - root - 2017-12-10 20:21:56.357997: step 49880, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.816 sec/batch; 64h:03m:15s remains)
INFO - root - 2017-12-10 20:22:04.245459: step 49890, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.781 sec/batch; 61h:16m:22s remains)
INFO - root - 2017-12-10 20:22:12.142585: step 49900, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 61h:46m:12s remains)
2017-12-10 20:22:13.020534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040290032 -0.039450347 -0.038306389 -0.036555033 -0.03397391 -0.031727742 -0.031184876 -0.032432359 -0.034396503 -0.035783682 -0.035561744 -0.033132672 -0.02909068 -0.025031926 -0.022380518][-0.035915229 -0.035626676 -0.034988098 -0.032972429 -0.029258726 -0.025457198 -0.023480505 -0.023389032 -0.023949681 -0.023847479 -0.022542832 -0.020152373 -0.018003918 -0.017702408 -0.019785775][-0.014978716 -0.014457437 -0.014407911 -0.012684039 -0.00868568 -0.0044070324 -0.0020261277 -0.0010555901 0.00024056931 0.0030862771 0.0070636519 0.010619797 0.010592271 0.0050545745 -0.0052439137][0.025990596 0.027814871 0.027317964 0.028259749 0.031624522 0.0350005 0.03628765 0.037091985 0.040283173 0.047149051 0.055601619 0.061529581 0.059038118 0.045450512 0.023052007][0.080511704 0.084529027 0.083386615 0.082862981 0.08467979 0.086226277 0.085775517 0.086158112 0.091586754 0.10336503 0.11755691 0.12721457 0.12334353 0.10197433 0.066399649][0.13209498 0.13811885 0.13580674 0.13291954 0.13239037 0.13193317 0.13050172 0.13148203 0.13981797 0.15670542 0.17714012 0.19225445 0.19031212 0.16568671 0.12127037][0.16389659 0.17096664 0.16718812 0.16196932 0.16001871 0.16007049 0.16150913 0.16691981 0.18065506 0.20372434 0.23164143 0.25511587 0.2601864 0.23903136 0.19231506][0.16606894 0.17306401 0.16852112 0.16229716 0.16093609 0.16479753 0.17354245 0.18736741 0.20848686 0.23779601 0.273475 0.30787423 0.32552302 0.31583211 0.27529863][0.14437518 0.15085828 0.14722766 0.14219499 0.14333905 0.15294074 0.17075777 0.19412069 0.22262333 0.25725618 0.29995582 0.34565851 0.37816694 0.38428834 0.35611355][0.11043676 0.11647734 0.11553232 0.11374679 0.11841457 0.13323203 0.15801071 0.18791771 0.22052449 0.25721073 0.30339307 0.35654366 0.4004384 0.4206883 0.40633145][0.074486949 0.0800931 0.082241349 0.083776034 0.090457052 0.10636792 0.13149531 0.1605562 0.19056891 0.22343089 0.26645175 0.3187784 0.36557788 0.39341155 0.3907856][0.040838886 0.046058428 0.0502251 0.053379044 0.059276529 0.07159441 0.090682112 0.11235804 0.13424525 0.158468 0.19206108 0.23477076 0.27469534 0.30134127 0.30517665][0.0098804329 0.014680066 0.019476933 0.022257488 0.025070531 0.030972242 0.040826052 0.052153349 0.063470252 0.076618083 0.096771128 0.12381586 0.1497632 0.16828533 0.17339589][-0.015753003 -0.01105509 -0.0065040602 -0.0050326358 -0.00543081 -0.0050810529 -0.0026774188 0.00057077745 0.0034096444 0.0068075792 0.013584988 0.023767667 0.033719104 0.04116033 0.043867096][-0.033095147 -0.028388852 -0.024009313 -0.023040354 -0.024183668 -0.024924787 -0.023918767 -0.022367584 -0.022514278 -0.024730744 -0.0282436 -0.032401089 -0.036640208 -0.03975711 -0.040954854]]...]
INFO - root - 2017-12-10 20:22:20.843888: step 49910, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 60h:31m:31s remains)
INFO - root - 2017-12-10 20:22:28.257563: step 49920, loss = 0.70, batch loss = 0.64 (11.3 examples/sec; 0.705 sec/batch; 55h:20m:54s remains)
INFO - root - 2017-12-10 20:22:36.136997: step 49930, loss = 0.70, batch loss = 0.65 (10.6 examples/sec; 0.751 sec/batch; 58h:59m:09s remains)
INFO - root - 2017-12-10 20:22:44.043626: step 49940, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 62h:51m:52s remains)
INFO - root - 2017-12-10 20:22:52.016629: step 49950, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 62h:36m:44s remains)
INFO - root - 2017-12-10 20:23:00.003307: step 49960, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 63h:07m:08s remains)
INFO - root - 2017-12-10 20:23:07.923412: step 49970, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 62h:19m:05s remains)
INFO - root - 2017-12-10 20:23:15.754089: step 49980, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.817 sec/batch; 64h:06m:50s remains)
INFO - root - 2017-12-10 20:23:23.607384: step 49990, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 63h:04m:20s remains)
INFO - root - 2017-12-10 20:23:31.471364: step 50000, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 60h:41m:54s remains)
2017-12-10 20:23:32.217281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.023691205 -0.026114065 -0.027561864 -0.027705401 -0.024438947 -0.018352529 -0.0097826784 0.00012826372 0.0087052183 0.01287814 0.0099072577 0.0020945931 -0.007500018 -0.017830316 -0.026961381][-0.012759934 -0.015501585 -0.018069558 -0.019438142 -0.015744889 -0.0074733854 0.0058687939 0.023577819 0.040141061 0.049659334 0.04719751 0.036606681 0.022278024 0.0053185895 -0.011502475][0.0099939965 0.010168501 0.0078404527 0.0043209535 0.0063404744 0.014445441 0.030454107 0.054070536 0.077982306 0.093468383 0.092336036 0.07931862 0.059904791 0.035793096 0.01010965][0.044944268 0.053882036 0.055614021 0.050919052 0.050290037 0.056277018 0.071710646 0.097505756 0.12611134 0.14635977 0.14652807 0.1308888 0.1057454 0.073281914 0.037040785][0.090641588 0.11403391 0.12359745 0.11923803 0.11559866 0.11810027 0.1301782 0.15480882 0.18535814 0.20848805 0.20890085 0.18943156 0.15761575 0.11557941 0.067369342][0.1445166 0.18470673 0.2027524 0.19827826 0.1900423 0.18736662 0.19426304 0.21682562 0.24824098 0.27205583 0.27038431 0.24495509 0.20558462 0.15376131 0.094054878][0.19319198 0.24600039 0.26840231 0.2616829 0.24865308 0.24172087 0.24491452 0.26677987 0.29884514 0.32068929 0.31362641 0.28026631 0.23377573 0.17478581 0.10784426][0.21953103 0.27577764 0.29638869 0.28627053 0.2708289 0.26304406 0.26576918 0.28837219 0.31958774 0.33600858 0.32091486 0.27959728 0.22869612 0.16814946 0.10176865][0.21169594 0.26123726 0.27519673 0.26244709 0.24831612 0.24308753 0.24734266 0.26956427 0.29651904 0.30477849 0.28149557 0.23556675 0.18565632 0.1312099 0.074762121][0.17302778 0.20862846 0.21372297 0.19982517 0.18930775 0.18759319 0.19263093 0.21145543 0.23147187 0.23225038 0.20480223 0.16033439 0.1174348 0.075568505 0.035359673][0.11856422 0.13828073 0.13514459 0.121494 0.11491023 0.11538122 0.11856241 0.13042024 0.14191172 0.1378631 0.11241327 0.076430969 0.045619126 0.019413697 -0.0034360487][0.060944226 0.067302115 0.059155207 0.04768296 0.04514217 0.047105268 0.047631908 0.051425591 0.054525651 0.04822956 0.029200498 0.00571834 -0.011510664 -0.023341168 -0.031987015][0.014598353 0.013191936 0.0042402679 -0.0039108847 -0.0034290687 -0.00062182813 -0.0017191621 -0.0036918356 -0.0064359205 -0.012922233 -0.024692014 -0.03671455 -0.043421231 -0.045701783 -0.045650467][-0.014122245 -0.018245561 -0.025397547 -0.030679708 -0.029284647 -0.02672979 -0.028142983 -0.032137085 -0.036803473 -0.04188 -0.04794912 -0.052482426 -0.053303879 -0.051021751 -0.047179345][-0.0271756 -0.031287432 -0.036078375 -0.039681621 -0.03920684 -0.03800685 -0.039360438 -0.042807754 -0.046432521 -0.0491546 -0.051397614 -0.052302226 -0.050831836 -0.047234654 -0.04273003]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 20:23:40.829778: step 50010, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 61h:35m:18s remains)
INFO - root - 2017-12-10 20:23:48.704522: step 50020, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 62h:12m:52s remains)
INFO - root - 2017-12-10 20:23:56.532343: step 50030, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 62h:02m:52s remains)
INFO - root - 2017-12-10 20:24:04.445500: step 50040, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 62h:48m:55s remains)
INFO - root - 2017-12-10 20:24:12.329492: step 50050, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 59h:45m:07s remains)
INFO - root - 2017-12-10 20:24:20.208774: step 50060, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 63h:35m:01s remains)
INFO - root - 2017-12-10 20:24:28.194573: step 50070, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 60h:23m:10s remains)
INFO - root - 2017-12-10 20:24:35.952716: step 50080, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.837 sec/batch; 65h:39m:37s remains)
INFO - root - 2017-12-10 20:24:43.856281: step 50090, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.823 sec/batch; 64h:34m:00s remains)
INFO - root - 2017-12-10 20:24:51.405420: step 50100, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 59h:46m:03s remains)
2017-12-10 20:24:52.242837: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28271565 0.31906533 0.30821803 0.25510725 0.18666483 0.13791659 0.12723301 0.15550476 0.2057149 0.24923326 0.27047712 0.27508241 0.27375355 0.27395266 0.27501276][0.27014351 0.29111865 0.26751474 0.21065149 0.1499325 0.11906611 0.13236064 0.18392479 0.25237483 0.30663592 0.33155227 0.33376405 0.32526302 0.31627876 0.31025079][0.24948406 0.24942347 0.21348417 0.1580984 0.11145455 0.10224929 0.13824217 0.20719613 0.28530574 0.34300888 0.36769307 0.36618653 0.35026991 0.33181453 0.31893274][0.25156206 0.23563349 0.19293919 0.14405064 0.1122512 0.12021954 0.16970451 0.24488582 0.32251692 0.3758688 0.39554739 0.38786814 0.3638674 0.33647537 0.31793728][0.27912506 0.2554616 0.21210478 0.17152122 0.15056095 0.16612788 0.21600752 0.28426033 0.35086069 0.39323154 0.405212 0.39141086 0.3621949 0.33099771 0.31216317][0.32043391 0.29702252 0.25749123 0.2244011 0.20831995 0.22147526 0.26056489 0.31272182 0.36234877 0.39148819 0.39731821 0.38271371 0.35605818 0.32861131 0.31436074][0.34722465 0.32937703 0.29668722 0.27131823 0.25929695 0.26983365 0.29963717 0.33890438 0.3748697 0.39264026 0.39275032 0.37775213 0.35448319 0.33071297 0.31845579][0.35292813 0.34180126 0.31547889 0.29644948 0.28984442 0.30336562 0.33256078 0.36821616 0.39714256 0.40510061 0.39574772 0.37428233 0.34809032 0.32228893 0.30647364][0.34082711 0.33857056 0.320238 0.30828854 0.30857027 0.32831123 0.36175352 0.39870012 0.42344314 0.42047134 0.39496022 0.35628185 0.31542388 0.27825487 0.25312528][0.33749303 0.34312907 0.33201429 0.32546395 0.33089352 0.35440066 0.38939193 0.42495024 0.44255957 0.42570335 0.37906197 0.31616756 0.25327536 0.19974597 0.16319361][0.36133584 0.37213546 0.36338282 0.35762027 0.36396715 0.38607636 0.41637918 0.44397259 0.44906986 0.41566506 0.34737492 0.26077276 0.17648031 0.10755585 0.061787996][0.40341884 0.41490689 0.40274116 0.39248383 0.39501017 0.4112756 0.43221363 0.44696072 0.43671927 0.38785294 0.30342835 0.20034809 0.1014962 0.022823533 -0.027560411][0.44911569 0.45477206 0.43247908 0.41136411 0.40420356 0.409586 0.41686282 0.41507825 0.38915211 0.33001429 0.24087912 0.13645829 0.037653558 -0.039246026 -0.086607508][0.4595702 0.45703426 0.42319423 0.38896573 0.36765853 0.35713065 0.34750855 0.32915384 0.2929002 0.23478968 0.15734766 0.070467241 -0.010701134 -0.072537385 -0.10867823][0.4019374 0.38910082 0.345704 0.30035061 0.26582178 0.241196 0.21950068 0.19287948 0.15774128 0.11384665 0.062305912 0.0076022074 -0.042723157 -0.079968415 -0.099901691]]...]
INFO - root - 2017-12-10 20:25:00.106039: step 50110, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 61h:18m:07s remains)
INFO - root - 2017-12-10 20:25:07.923157: step 50120, loss = 0.67, batch loss = 0.61 (10.1 examples/sec; 0.791 sec/batch; 62h:01m:29s remains)
INFO - root - 2017-12-10 20:25:15.788945: step 50130, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 61h:47m:22s remains)
INFO - root - 2017-12-10 20:25:23.664443: step 50140, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 62h:43m:55s remains)
INFO - root - 2017-12-10 20:25:31.545362: step 50150, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.789 sec/batch; 61h:53m:53s remains)
INFO - root - 2017-12-10 20:25:39.215757: step 50160, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 62h:43m:00s remains)
INFO - root - 2017-12-10 20:25:47.023885: step 50170, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 61h:27m:34s remains)
INFO - root - 2017-12-10 20:25:54.897349: step 50180, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.826 sec/batch; 64h:45m:16s remains)
INFO - root - 2017-12-10 20:26:02.659801: step 50190, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 60h:16m:33s remains)
INFO - root - 2017-12-10 20:26:10.545351: step 50200, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 59h:27m:10s remains)
2017-12-10 20:26:11.347275: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036369707 0.028192595 0.01915792 0.012925481 0.011573603 0.013328381 0.015861416 0.016702637 0.013379438 0.00522269 -0.0035448296 -0.0080236355 -0.0066275876 -0.00087987713 0.006324674][0.0605453 0.051836587 0.044801835 0.045383457 0.056244303 0.073549293 0.091796972 0.10394911 0.1036754 0.088814363 0.0663226 0.0467634 0.036611933 0.036726385 0.043632157][0.069499679 0.065685734 0.067492187 0.082791232 0.11478482 0.15734723 0.20067313 0.23109214 0.23603143 0.2104737 0.16394423 0.11511654 0.079805821 0.0655854 0.068674862][0.070921324 0.075663105 0.090939946 0.12640564 0.18607102 0.26172039 0.33857486 0.39490038 0.40972343 0.37374341 0.29812643 0.20981376 0.13646927 0.09578947 0.085878938][0.060313854 0.076615259 0.10885414 0.16720891 0.25715333 0.36928469 0.48381925 0.57013637 0.59895188 0.55616772 0.45289916 0.32252115 0.20549992 0.13180092 0.10243823][0.037448961 0.068647161 0.1222707 0.20709907 0.32920218 0.47779652 0.62721622 0.738627 0.77637875 0.72452158 0.59679019 0.43257651 0.2823953 0.18404149 0.13910891][0.0052860719 0.046974536 0.11833985 0.2265424 0.37733743 0.55910987 0.73966038 0.87191278 0.91612226 0.8577494 0.7164315 0.53722084 0.37470913 0.26633239 0.20981432][-0.026310289 0.015042603 0.090356544 0.20726146 0.37238443 0.57480627 0.77766335 0.92726839 0.98031819 0.92508012 0.78846329 0.62056983 0.47274554 0.37281874 0.31174013][-0.038644977 -0.0053185578 0.059667863 0.16631095 0.32365802 0.5225808 0.72578752 0.87839389 0.93661028 0.89317662 0.78098089 0.65179944 0.54675728 0.47750333 0.42545217][-0.024614107 -0.0044761812 0.037876237 0.11742768 0.24653237 0.41769817 0.59629589 0.73207313 0.78609234 0.75738692 0.68175977 0.60770768 0.5625791 0.53998661 0.51112908][0.028795168 0.033083484 0.04248311 0.079094626 0.16061208 0.28317288 0.41714543 0.5201481 0.561083 0.544777 0.50622958 0.48723429 0.50169575 0.52815568 0.52992749][0.12672587 0.11809967 0.089380018 0.068586975 0.082503177 0.13771471 0.21500413 0.28012025 0.30768508 0.30367702 0.29672602 0.31921408 0.37574872 0.43845186 0.46862096][0.27209565 0.25808918 0.19468552 0.11249037 0.048569035 0.025712056 0.040080957 0.0658161 0.080204956 0.085086718 0.098959237 0.14373398 0.21933967 0.29991207 0.35034114][0.4521957 0.44577435 0.36076179 0.22897223 0.094548449 -0.0026159449 -0.049567692 -0.064028516 -0.068042904 -0.065834045 -0.048201852 -0.0025643865 0.070107147 0.15052018 0.21052456][0.64200383 0.650345 0.55481458 0.39004436 0.20514557 0.054169323 -0.037761118 -0.084762037 -0.11072984 -0.12421019 -0.12118676 -0.094406314 -0.043040637 0.021883013 0.080293886]]...]
INFO - root - 2017-12-10 20:26:19.255025: step 50210, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 63h:01m:01s remains)
INFO - root - 2017-12-10 20:26:27.099933: step 50220, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 61h:45m:18s remains)
INFO - root - 2017-12-10 20:26:35.044198: step 50230, loss = 0.67, batch loss = 0.62 (10.0 examples/sec; 0.801 sec/batch; 62h:46m:03s remains)
INFO - root - 2017-12-10 20:26:42.817170: step 50240, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 60h:22m:34s remains)
INFO - root - 2017-12-10 20:26:50.721633: step 50250, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 62h:36m:00s remains)
INFO - root - 2017-12-10 20:26:58.685113: step 50260, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.807 sec/batch; 63h:14m:26s remains)
INFO - root - 2017-12-10 20:27:06.594833: step 50270, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 61h:48m:25s remains)
INFO - root - 2017-12-10 20:27:14.336257: step 50280, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.803 sec/batch; 62h:55m:25s remains)
INFO - root - 2017-12-10 20:27:22.229027: step 50290, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.816 sec/batch; 63h:58m:02s remains)
INFO - root - 2017-12-10 20:27:30.026367: step 50300, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 60h:58m:22s remains)
2017-12-10 20:27:30.941600: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14908175 0.17146985 0.17143381 0.15351366 0.12341793 0.093596086 0.079030655 0.088694081 0.11555375 0.14495133 0.16965267 0.18462126 0.18392719 0.1694162 0.14441739][0.11466772 0.12941715 0.12796037 0.11508945 0.0964353 0.081792131 0.080618337 0.095386304 0.11847446 0.13826974 0.15089811 0.15566696 0.15055075 0.13563499 0.11188466][0.082240209 0.08975938 0.0867999 0.0795261 0.074837983 0.080932543 0.0996454 0.12514128 0.14814612 0.16005123 0.16269828 0.16191044 0.15795551 0.14732841 0.12791891][0.071419083 0.074066475 0.070231147 0.06890735 0.078238942 0.10502845 0.14384945 0.18118973 0.20530626 0.21081422 0.20701951 0.20661208 0.2099321 0.20810935 0.19564041][0.087957188 0.089169085 0.087362193 0.0941362 0.1162661 0.15814427 0.20976608 0.25357476 0.27621588 0.27562636 0.26733831 0.26902649 0.28043067 0.289624 0.28739098][0.12778415 0.13189662 0.13557978 0.15138011 0.18302818 0.23181966 0.28648776 0.32930508 0.34721151 0.34118572 0.32867926 0.32905272 0.34323028 0.36103296 0.369899][0.17954436 0.18802287 0.19755137 0.22027561 0.25703755 0.30646852 0.35800648 0.3957203 0.40723392 0.39586276 0.37815484 0.37336078 0.38471034 0.40571272 0.42115504][0.22356676 0.23525877 0.24904 0.27582684 0.31424403 0.36164051 0.40890878 0.44092062 0.44482595 0.42583022 0.40104884 0.38902158 0.3945483 0.41321748 0.42790034][0.23742421 0.25038609 0.26653224 0.29518372 0.33424777 0.38147298 0.42852768 0.45822862 0.45558003 0.42799732 0.39577827 0.37700203 0.37602744 0.38806942 0.39553672][0.2029788 0.21462071 0.23164207 0.26177308 0.3032738 0.35462618 0.40578011 0.43648428 0.43017074 0.39656276 0.35953048 0.33690172 0.33132523 0.33624095 0.33501166][0.13857353 0.14698328 0.16263276 0.19248524 0.2363833 0.29294437 0.34883815 0.38108534 0.37373888 0.33817238 0.29936725 0.27443993 0.26520428 0.26457208 0.25801805][0.078146882 0.081463 0.092204809 0.11789568 0.16045766 0.21780847 0.27390289 0.30493063 0.29725876 0.26257586 0.22413607 0.1972875 0.18482466 0.18127784 0.17517357][0.04070567 0.039958678 0.044365928 0.063344575 0.10042292 0.15258753 0.20352311 0.23160616 0.22553916 0.19455561 0.15783262 0.12870489 0.11243355 0.10733579 0.10550022][0.040307973 0.03959199 0.039033923 0.050304171 0.077712096 0.11842263 0.15969948 0.18492754 0.18378495 0.16047502 0.12797494 0.0974285 0.0765065 0.068171039 0.068523534][0.0825436 0.08901763 0.088049948 0.092931785 0.10738848 0.13030604 0.15655036 0.1770348 0.18171582 0.16888805 0.14415324 0.11566338 0.091478996 0.078134924 0.0754748]]...]
INFO - root - 2017-12-10 20:27:38.843132: step 50310, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 61h:54m:17s remains)
INFO - root - 2017-12-10 20:27:46.519923: step 50320, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 60h:30m:55s remains)
INFO - root - 2017-12-10 20:27:54.437488: step 50330, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 61h:27m:56s remains)
INFO - root - 2017-12-10 20:28:02.339564: step 50340, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 62h:13m:29s remains)
INFO - root - 2017-12-10 20:28:10.321158: step 50350, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.785 sec/batch; 61h:33m:40s remains)
INFO - root - 2017-12-10 20:28:18.175154: step 50360, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.788 sec/batch; 61h:47m:14s remains)
INFO - root - 2017-12-10 20:28:25.820592: step 50370, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 62h:10m:36s remains)
INFO - root - 2017-12-10 20:28:33.752651: step 50380, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 60h:07m:34s remains)
INFO - root - 2017-12-10 20:28:41.669171: step 50390, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 60h:11m:51s remains)
INFO - root - 2017-12-10 20:28:49.408130: step 50400, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 60h:23m:33s remains)
2017-12-10 20:28:50.261048: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.45746651 0.48153287 0.47221956 0.45818043 0.43695781 0.41107666 0.39370993 0.39850318 0.41807872 0.43035689 0.43173769 0.43604761 0.44398671 0.44422 0.43967229][0.46311957 0.46041989 0.42550927 0.39657184 0.37218955 0.35010043 0.3388674 0.34888723 0.37113357 0.38842204 0.39941785 0.41469586 0.43242353 0.44231781 0.4491182][0.42827463 0.40752214 0.35850605 0.32247898 0.2997404 0.28494215 0.28030154 0.29045063 0.30629292 0.32120478 0.33624944 0.35354951 0.36902007 0.37803459 0.38934752][0.38098085 0.36487153 0.32656369 0.30137461 0.29026276 0.28952393 0.29509255 0.30217242 0.30261537 0.30501154 0.3153652 0.3226952 0.31970358 0.31199515 0.3159399][0.3438023 0.35882041 0.35855153 0.36361134 0.37423414 0.39440635 0.4153868 0.41890126 0.39774126 0.37887484 0.37743852 0.36767745 0.33627886 0.30089265 0.2884528][0.33092061 0.3932876 0.4458985 0.49015665 0.52475274 0.56495279 0.60038608 0.59934771 0.55556643 0.51305526 0.4986724 0.4747903 0.41943336 0.3591865 0.33029848][0.33988258 0.4462361 0.54496813 0.61953056 0.66749781 0.71428382 0.75226223 0.74282664 0.68128091 0.62217581 0.60134631 0.57604647 0.51609963 0.44956291 0.41541934][0.35382259 0.4812884 0.59841609 0.67795354 0.71816933 0.75039154 0.77306914 0.75064737 0.68104106 0.61950821 0.60345006 0.59260923 0.55274051 0.50507182 0.48205289][0.35352579 0.47323948 0.57546669 0.63270718 0.64644188 0.64811027 0.64281178 0.60630643 0.53969824 0.48989052 0.48704505 0.49895579 0.49325791 0.47993571 0.47822908][0.32360941 0.41447377 0.48057589 0.5016408 0.48405197 0.45477474 0.42294741 0.37797895 0.32363644 0.29353726 0.30556658 0.33681777 0.36161977 0.38002142 0.39761496][0.25727683 0.31176606 0.3388823 0.32775506 0.28911164 0.24331464 0.19949824 0.15675746 0.11967217 0.10879463 0.13041183 0.16924332 0.20831537 0.24271095 0.26914108][0.16274525 0.18525857 0.18450703 0.15711994 0.11537709 0.072943285 0.035778318 0.0059549022 -0.013282515 -0.010912678 0.012234021 0.045931138 0.081210591 0.11335608 0.13684635][0.0645573 0.065800615 0.052427612 0.025033867 -0.0059158397 -0.032354426 -0.052183565 -0.064924918 -0.069715478 -0.061129786 -0.042335071 -0.020474024 0.0011014453 0.020477746 0.034330647][-0.012350249 -0.022304682 -0.037662711 -0.056576312 -0.072367191 -0.081381872 -0.084332995 -0.083350256 -0.079443149 -0.069753587 -0.056901887 -0.045976084 -0.037420448 -0.030638875 -0.026173569][-0.05605787 -0.070691563 -0.084169053 -0.094551638 -0.099035755 -0.096991263 -0.090430118 -0.082806617 -0.075869977 -0.067686707 -0.059536029 -0.054781117 -0.053059284 -0.053002715 -0.053986225]]...]
INFO - root - 2017-12-10 20:28:58.162115: step 50410, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 61h:11m:40s remains)
INFO - root - 2017-12-10 20:29:06.023966: step 50420, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.747 sec/batch; 58h:32m:31s remains)
INFO - root - 2017-12-10 20:29:13.886327: step 50430, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 61h:44m:33s remains)
INFO - root - 2017-12-10 20:29:21.693074: step 50440, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 60h:48m:17s remains)
INFO - root - 2017-12-10 20:29:29.457855: step 50450, loss = 0.68, batch loss = 0.62 (12.0 examples/sec; 0.665 sec/batch; 52h:06m:16s remains)
INFO - root - 2017-12-10 20:29:37.420125: step 50460, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 59h:21m:57s remains)
INFO - root - 2017-12-10 20:29:45.236690: step 50470, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.755 sec/batch; 59h:07m:15s remains)
INFO - root - 2017-12-10 20:29:52.889840: step 50480, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 61h:25m:36s remains)
INFO - root - 2017-12-10 20:30:00.856072: step 50490, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 60h:39m:00s remains)
INFO - root - 2017-12-10 20:30:08.754460: step 50500, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 60h:22m:25s remains)
2017-12-10 20:30:09.638024: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18020083 0.16025075 0.1546016 0.16195378 0.1675871 0.17298618 0.1888527 0.20659257 0.21082737 0.20669459 0.21213225 0.23923127 0.26692557 0.28151551 0.28162295][0.18799841 0.15930542 0.14399746 0.1467488 0.15569723 0.16952205 0.19949587 0.23962456 0.2686339 0.2807804 0.29246497 0.31505409 0.32761586 0.317792 0.29050207][0.20775038 0.17505234 0.15300973 0.1532262 0.16717094 0.1899498 0.2315529 0.28993347 0.3423155 0.37363014 0.39988646 0.43107122 0.44263598 0.4179703 0.36680979][0.24997753 0.22270729 0.20057946 0.20078593 0.21856776 0.2468936 0.29338264 0.3592397 0.42327419 0.46708643 0.508453 0.55544353 0.5793134 0.55474019 0.49317178][0.3119227 0.30031276 0.28436404 0.2844657 0.30075613 0.32781896 0.371988 0.43437743 0.49484542 0.5379082 0.58603662 0.64635706 0.68563616 0.66937459 0.60970747][0.37770307 0.38717631 0.38019276 0.37679872 0.38442817 0.40310121 0.43883204 0.49083665 0.54032546 0.57742363 0.62713552 0.69398475 0.74228251 0.73199177 0.67690605][0.4287529 0.45698255 0.45647365 0.44546366 0.43790066 0.44070166 0.46131214 0.49855351 0.53642666 0.57010561 0.62228507 0.69167191 0.74188864 0.733175 0.68255365][0.452537 0.48724276 0.48482591 0.46310195 0.43725544 0.41887504 0.41879538 0.43868062 0.46674588 0.50187534 0.56030387 0.6328212 0.68300951 0.67562371 0.63135254][0.44738242 0.47858495 0.46956804 0.43816584 0.39470869 0.35114673 0.32417163 0.32221738 0.33795735 0.37297925 0.43538758 0.50981474 0.56041324 0.55801064 0.52451235][0.4308008 0.45285648 0.43700957 0.40006995 0.34376717 0.27718881 0.22207956 0.19436531 0.19102317 0.21648738 0.27260491 0.34200624 0.39139917 0.39654353 0.37696534][0.419706 0.43358785 0.41471425 0.3773666 0.3152093 0.23344557 0.15464589 0.099639878 0.069704339 0.07342 0.10955971 0.16279902 0.20556419 0.21747926 0.21214585][0.41594204 0.42760965 0.41132545 0.37687925 0.31410256 0.22600855 0.13167191 0.051626667 -0.0086080246 -0.035128146 -0.028913569 -0.0010204468 0.028344378 0.043188762 0.049664013][0.40582091 0.42254752 0.41341469 0.38326433 0.3229968 0.23680867 0.13722803 0.040455751 -0.043980736 -0.09716443 -0.11863133 -0.11614727 -0.10294124 -0.090337537 -0.07791096][0.37434584 0.40147021 0.40384847 0.38097009 0.32691431 0.24853639 0.1518725 0.049435429 -0.04600187 -0.11287769 -0.15012559 -0.16485356 -0.16519026 -0.1586072 -0.14737517][0.32683134 0.36681321 0.38420492 0.3735992 0.32997322 0.26035613 0.16773421 0.0648818 -0.032202251 -0.1017278 -0.14276484 -0.16345191 -0.16979736 -0.16781144 -0.16065894]]...]
INFO - root - 2017-12-10 20:30:17.476311: step 50510, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 60h:41m:33s remains)
INFO - root - 2017-12-10 20:30:25.336325: step 50520, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.781 sec/batch; 61h:11m:29s remains)
INFO - root - 2017-12-10 20:30:33.257103: step 50530, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 61h:27m:25s remains)
INFO - root - 2017-12-10 20:30:41.061012: step 50540, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 60h:54m:29s remains)
INFO - root - 2017-12-10 20:30:48.935901: step 50550, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 62h:01m:27s remains)
INFO - root - 2017-12-10 20:30:56.532131: step 50560, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 62h:23m:58s remains)
INFO - root - 2017-12-10 20:31:04.343636: step 50570, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 61h:06m:18s remains)
INFO - root - 2017-12-10 20:31:12.254932: step 50580, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.811 sec/batch; 63h:32m:20s remains)
INFO - root - 2017-12-10 20:31:20.125540: step 50590, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 61h:10m:54s remains)
INFO - root - 2017-12-10 20:31:27.914546: step 50600, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 60h:24m:52s remains)
2017-12-10 20:31:28.726818: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27230412 0.22817741 0.179854 0.15716566 0.16987665 0.20565623 0.24489455 0.26364797 0.2431255 0.19114415 0.1324649 0.092729755 0.077620625 0.080996469 0.10505764][0.23545781 0.18268694 0.12851122 0.10547965 0.1301517 0.19179252 0.26200545 0.30772921 0.30219945 0.24976602 0.17497692 0.10878767 0.069552064 0.057253662 0.074955009][0.17988628 0.12805876 0.080013588 0.068356745 0.11320757 0.20461777 0.30714464 0.37960109 0.38854724 0.33530259 0.24378407 0.1509929 0.08571513 0.05652101 0.065378845][0.13354382 0.093387976 0.064078711 0.074609205 0.14572018 0.26748708 0.39970559 0.49450913 0.51291323 0.4540945 0.34360054 0.22477822 0.13407306 0.0881628 0.086217456][0.11704769 0.095517315 0.091025621 0.1272846 0.22399558 0.3705312 0.5242579 0.63226932 0.65268815 0.58377379 0.45375687 0.31155831 0.19840589 0.13640253 0.12145669][0.11218302 0.11288977 0.13694383 0.20264207 0.32622609 0.4937135 0.6600585 0.76981866 0.78287435 0.698368 0.54795766 0.38545254 0.25418431 0.17757694 0.14915836][0.10679737 0.1296872 0.18211663 0.27922627 0.43018389 0.61432844 0.78370148 0.88358843 0.88055247 0.77697188 0.60799193 0.43065915 0.28739512 0.19952883 0.15961888][0.11476069 0.15466225 0.22828905 0.34910464 0.5176686 0.70595074 0.86523414 0.9466635 0.92621452 0.80891746 0.63037074 0.44688234 0.297461 0.19954337 0.14779831][0.13334173 0.18368711 0.26904213 0.40098277 0.57138908 0.74641556 0.8806138 0.93707222 0.90301418 0.78363723 0.610002 0.43141836 0.2821182 0.17592672 0.11274017][0.1535494 0.20820719 0.296114 0.42572492 0.58138889 0.72669923 0.8234942 0.85116589 0.80931944 0.70202357 0.55029619 0.39012811 0.250948 0.14365476 0.073650941][0.15772972 0.21199629 0.29553008 0.41284063 0.54367167 0.65242 0.7105059 0.71399188 0.67212528 0.58654839 0.46647787 0.33406526 0.21460348 0.11593895 0.046588488][0.1461904 0.19673231 0.27014223 0.3661387 0.46436313 0.53446656 0.5593217 0.54838014 0.51488411 0.45650887 0.3715682 0.27154213 0.17848589 0.097669654 0.037943751][0.12659894 0.17307344 0.23398778 0.30358779 0.36550733 0.39889821 0.3989647 0.38382509 0.36587694 0.33698562 0.28580743 0.21744515 0.15248401 0.095204175 0.053356096][0.11339951 0.15656072 0.20584825 0.24985407 0.27831647 0.28175995 0.26681221 0.25529462 0.256273 0.25512013 0.23289621 0.19225802 0.15349559 0.12083719 0.09977863][0.12297899 0.16430137 0.2036349 0.22545683 0.2255165 0.20533609 0.18128872 0.17625196 0.19505696 0.21712522 0.21837626 0.20155595 0.18642834 0.176486 0.17457466]]...]
INFO - root - 2017-12-10 20:31:36.546125: step 50610, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 63h:00m:26s remains)
INFO - root - 2017-12-10 20:31:44.414541: step 50620, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 62h:50m:01s remains)
INFO - root - 2017-12-10 20:31:52.077395: step 50630, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 63h:26m:47s remains)
INFO - root - 2017-12-10 20:31:59.746692: step 50640, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 63h:22m:19s remains)
INFO - root - 2017-12-10 20:32:07.673404: step 50650, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 62h:50m:50s remains)
INFO - root - 2017-12-10 20:32:15.534275: step 50660, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 62h:32m:11s remains)
INFO - root - 2017-12-10 20:32:23.338850: step 50670, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 60h:54m:52s remains)
INFO - root - 2017-12-10 20:32:31.207840: step 50680, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.822 sec/batch; 64h:18m:48s remains)
INFO - root - 2017-12-10 20:32:39.060115: step 50690, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 59h:46m:51s remains)
INFO - root - 2017-12-10 20:32:46.851569: step 50700, loss = 0.67, batch loss = 0.62 (10.5 examples/sec; 0.765 sec/batch; 59h:53m:15s remains)
2017-12-10 20:32:47.661838: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19446295 0.14981614 0.11615165 0.10362199 0.11045131 0.12628397 0.14087883 0.14373866 0.13412885 0.11786503 0.10628266 0.1119255 0.14239652 0.18906757 0.23009518][0.20685183 0.16395257 0.13669612 0.13502252 0.15370983 0.17730409 0.19261457 0.18945874 0.17017315 0.14465038 0.12500116 0.12463235 0.15241578 0.20013385 0.24479759][0.19219573 0.1569266 0.14210092 0.15618391 0.19041547 0.22353782 0.23943985 0.228739 0.19793452 0.1618046 0.13263474 0.12350108 0.143584 0.18548447 0.22798502][0.17047967 0.14405936 0.14157522 0.17075711 0.22042903 0.26401496 0.2822344 0.26612526 0.22574778 0.18051966 0.14268547 0.12448265 0.13474438 0.16782326 0.20529124][0.15874517 0.14091279 0.14762323 0.18774329 0.24985085 0.30353889 0.32639962 0.30862114 0.26206374 0.20961106 0.16418292 0.13755587 0.13831724 0.16246065 0.19432917][0.16372962 0.1517038 0.16090567 0.20352647 0.27100855 0.33137703 0.35974336 0.34449556 0.29687488 0.24075672 0.19057608 0.15874055 0.15391622 0.17274673 0.20075795][0.18037418 0.1716502 0.17670844 0.21266179 0.276121 0.33679262 0.36910871 0.35978913 0.31730026 0.26302075 0.21276279 0.1800205 0.17354752 0.18941173 0.21332364][0.20478833 0.19907971 0.19773631 0.22192822 0.27417836 0.32873294 0.36120418 0.35829434 0.32444033 0.27583846 0.22888328 0.19779228 0.19049972 0.20255969 0.22040114][0.23289585 0.23092304 0.22464998 0.23753412 0.27686942 0.32301542 0.35357618 0.35627493 0.3312209 0.28796858 0.24311079 0.21192998 0.20170322 0.20803306 0.21878269][0.25704098 0.25694627 0.24834464 0.25400046 0.28319687 0.3218587 0.3501589 0.35743189 0.34014124 0.30132839 0.2572431 0.22413708 0.20957693 0.20918663 0.21277456][0.26704916 0.26441327 0.25352868 0.2553511 0.27795094 0.31109273 0.337808 0.34968904 0.34118143 0.30946803 0.26885065 0.23559982 0.21820179 0.21255977 0.2105355][0.26679054 0.25736853 0.243192 0.24270061 0.260855 0.28951088 0.31467494 0.33042458 0.33080727 0.30842003 0.27407748 0.24300635 0.2250552 0.21681058 0.21182491][0.2672351 0.24720219 0.2279103 0.22455797 0.23875706 0.2631329 0.28602073 0.30323806 0.30926991 0.29437 0.2663345 0.2391012 0.2236419 0.21724848 0.21398497][0.27204046 0.24207813 0.21507609 0.2050257 0.21247709 0.2312751 0.2516478 0.26952896 0.27896163 0.2696172 0.24714592 0.22457169 0.21392778 0.2135098 0.21670122][0.280029 0.24609935 0.21219103 0.19235493 0.18862452 0.19767074 0.21240085 0.2287185 0.24003382 0.23616233 0.22034909 0.20415296 0.2000383 0.20771214 0.2198488]]...]
INFO - root - 2017-12-10 20:32:55.692959: step 50710, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 62h:41m:20s remains)
INFO - root - 2017-12-10 20:33:02.918141: step 50720, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 61h:48m:40s remains)
INFO - root - 2017-12-10 20:33:10.907490: step 50730, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.804 sec/batch; 62h:56m:49s remains)
INFO - root - 2017-12-10 20:33:18.909019: step 50740, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 62h:03m:09s remains)
INFO - root - 2017-12-10 20:33:26.752986: step 50750, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 62h:47m:04s remains)
INFO - root - 2017-12-10 20:33:34.622657: step 50760, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 60h:23m:40s remains)
INFO - root - 2017-12-10 20:33:42.590722: step 50770, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 60h:54m:52s remains)
INFO - root - 2017-12-10 20:33:50.435381: step 50780, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 59h:06m:21s remains)
INFO - root - 2017-12-10 20:33:58.283244: step 50790, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 60h:33m:40s remains)
INFO - root - 2017-12-10 20:34:06.010718: step 50800, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 62h:52m:12s remains)
2017-12-10 20:34:06.938269: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.080386229 0.094033949 0.086928949 0.061695565 0.02990447 0.0069377031 0.0011603318 0.0071207308 0.0126517 0.0099345595 0.00075360108 -0.0096892389 -0.016019495 -0.014974797 -0.00770046][0.15598714 0.18134421 0.1749614 0.13995518 0.094347261 0.062188603 0.055673067 0.064249195 0.067688592 0.054858293 0.030895948 0.0070737344 -0.0080322 -0.0099891 -0.00070167927][0.23467411 0.27304566 0.27054265 0.23123275 0.17848277 0.14421102 0.14421457 0.16096781 0.16358753 0.13635491 0.089615226 0.044324636 0.014186383 0.0049688015 0.013094151][0.29715711 0.3484264 0.35332063 0.31705961 0.2660771 0.23829697 0.25330043 0.28673413 0.29500508 0.25486106 0.18120377 0.10715976 0.054977588 0.032866292 0.034754869][0.35477048 0.42052162 0.4364228 0.40897512 0.36569718 0.34739357 0.37712857 0.42716536 0.44234639 0.38968992 0.28641894 0.17898618 0.1012432 0.064414933 0.058721025][0.40400064 0.48314267 0.51139951 0.49674147 0.4668225 0.45952725 0.49880269 0.55673516 0.57238066 0.50542629 0.37433097 0.23772036 0.14050452 0.095336124 0.086694047][0.41972554 0.50885922 0.55153573 0.55742031 0.55216277 0.56433511 0.61342525 0.67218816 0.68110549 0.59877938 0.44572383 0.28981027 0.18391505 0.13888112 0.13081276][0.39211947 0.4865239 0.54326844 0.57131988 0.59183145 0.62324435 0.67792445 0.73030776 0.7283439 0.63613981 0.47700492 0.32097408 0.22328298 0.18988824 0.18720615][0.33882263 0.43525082 0.50349772 0.54713392 0.58040023 0.61685437 0.66510147 0.70404458 0.69077092 0.59771746 0.45039481 0.31295419 0.23719409 0.22389933 0.23278676][0.29075947 0.38513559 0.4576731 0.50384611 0.5302965 0.55107182 0.57677782 0.593987 0.5688377 0.48257002 0.36181709 0.25877663 0.21595457 0.22869766 0.25502533][0.24162763 0.32455215 0.388474 0.42314228 0.4311021 0.42847407 0.4290801 0.42706263 0.39670518 0.32663259 0.24228874 0.18271296 0.17765355 0.21594684 0.2570287][0.177577 0.239783 0.28393275 0.29905209 0.28866294 0.27004158 0.25692278 0.24742322 0.22279462 0.17733674 0.13165033 0.11250967 0.13684529 0.18837862 0.2303672][0.10085192 0.13393158 0.15055314 0.14356139 0.11883941 0.092815705 0.077017151 0.07060232 0.060580339 0.0439759 0.033808909 0.044584259 0.08260297 0.13105933 0.16180696][0.02373502 0.0282735 0.021651659 0.0016502038 -0.025657197 -0.04801555 -0.057936508 -0.056485027 -0.052402951 -0.048240468 -0.038174782 -0.01702104 0.015761899 0.048058756 0.063572146][-0.040186625 -0.052695148 -0.069152743 -0.08954706 -0.10957709 -0.12266076 -0.12573874 -0.12035957 -0.11169505 -0.10176136 -0.0886685 -0.071809605 -0.052423473 -0.036992062 -0.032098651]]...]
INFO - root - 2017-12-10 20:34:14.623590: step 50810, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 62h:28m:34s remains)
INFO - root - 2017-12-10 20:34:22.465274: step 50820, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 62h:07m:24s remains)
INFO - root - 2017-12-10 20:34:30.341727: step 50830, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 62h:26m:20s remains)
INFO - root - 2017-12-10 20:34:38.232634: step 50840, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 60h:42m:18s remains)
INFO - root - 2017-12-10 20:34:46.102048: step 50850, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 60h:44m:47s remains)
INFO - root - 2017-12-10 20:34:53.989026: step 50860, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.812 sec/batch; 63h:29m:24s remains)
INFO - root - 2017-12-10 20:35:01.892528: step 50870, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 61h:21m:31s remains)
INFO - root - 2017-12-10 20:35:09.554431: step 50880, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.796 sec/batch; 62h:16m:34s remains)
INFO - root - 2017-12-10 20:35:17.410869: step 50890, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.819 sec/batch; 64h:01m:44s remains)
INFO - root - 2017-12-10 20:35:25.116395: step 50900, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 62h:20m:05s remains)
2017-12-10 20:35:25.899238: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2255404 0.24924287 0.25708351 0.25575665 0.24424466 0.23551235 0.23548234 0.25297928 0.28047627 0.30416241 0.33072379 0.35449746 0.36629525 0.36530453 0.35846809][0.26581243 0.27726912 0.26789677 0.25150838 0.23298955 0.22377248 0.22716361 0.24944967 0.2780135 0.300024 0.3246319 0.34926504 0.362185 0.36159393 0.35915577][0.27751631 0.27597573 0.25108424 0.223509 0.20444578 0.20214716 0.21512839 0.2440149 0.27213958 0.28970763 0.30867264 0.33011851 0.34033278 0.33652282 0.3351559][0.27180207 0.267996 0.24132754 0.21512848 0.20516841 0.21525252 0.239314 0.27137214 0.29384691 0.30118179 0.30783674 0.31797925 0.31777745 0.30514514 0.29945728][0.26578328 0.27685106 0.26703811 0.25720614 0.26500663 0.289123 0.3201257 0.34702468 0.35578421 0.34574348 0.33270684 0.32196122 0.3020708 0.27468824 0.26027453][0.27654749 0.31732911 0.33755538 0.35256112 0.37962407 0.41361189 0.44307214 0.45507047 0.44335654 0.41309416 0.37964925 0.34529224 0.30191639 0.25732356 0.23225269][0.31349227 0.38731018 0.43580636 0.46993408 0.5070684 0.541529 0.56055909 0.55223805 0.51998562 0.47468105 0.42876279 0.3773725 0.31493208 0.25598916 0.22176607][0.37100455 0.46690604 0.52674997 0.56126112 0.591273 0.61584753 0.62065154 0.59499443 0.54974663 0.50000089 0.45405281 0.39868972 0.32935059 0.26503694 0.22717516][0.42914405 0.52813864 0.57684857 0.58994192 0.59617138 0.60420763 0.59727776 0.56386048 0.51821232 0.47647017 0.4427422 0.39741004 0.33549029 0.27649412 0.24124959][0.45630842 0.53919744 0.56072462 0.54112357 0.51771319 0.51007146 0.49960747 0.47195777 0.43984079 0.41588429 0.40076748 0.37305897 0.32737479 0.28040737 0.25100565][0.43941835 0.49313235 0.48262662 0.43292648 0.38666719 0.37084651 0.36631972 0.35560969 0.3459402 0.34271842 0.34413636 0.33208314 0.302781 0.26865956 0.24463151][0.3879877 0.41127253 0.3758097 0.30919802 0.25381845 0.23828059 0.24446096 0.25250697 0.26322722 0.27486193 0.28447157 0.28077498 0.26344025 0.24011965 0.2204795][0.32123831 0.32268032 0.27665821 0.20989718 0.15963818 0.15094812 0.16789794 0.18899316 0.2103985 0.22648206 0.23507525 0.23279189 0.22235338 0.20681046 0.19102086][0.25727078 0.24954335 0.20676157 0.15262213 0.11640165 0.11682543 0.14093454 0.16676258 0.18823768 0.199931 0.20246589 0.19893046 0.19254526 0.18251067 0.17037189][0.21092494 0.20329805 0.17071462 0.13344885 0.11200388 0.11883984 0.14428635 0.16757536 0.18245082 0.18629201 0.18323441 0.17970408 0.17713094 0.17153721 0.16310312]]...]
INFO - root - 2017-12-10 20:35:33.749118: step 50910, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 60h:20m:23s remains)
INFO - root - 2017-12-10 20:35:41.589145: step 50920, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 59h:52m:15s remains)
INFO - root - 2017-12-10 20:35:49.524754: step 50930, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 62h:52m:54s remains)
INFO - root - 2017-12-10 20:35:57.542547: step 50940, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.800 sec/batch; 62h:36m:11s remains)
INFO - root - 2017-12-10 20:36:05.341605: step 50950, loss = 0.71, batch loss = 0.66 (9.9 examples/sec; 0.804 sec/batch; 62h:53m:59s remains)
INFO - root - 2017-12-10 20:36:13.031746: step 50960, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 59h:37m:55s remains)
INFO - root - 2017-12-10 20:36:20.776054: step 50970, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 59h:27m:39s remains)
INFO - root - 2017-12-10 20:36:28.669145: step 50980, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.810 sec/batch; 63h:18m:18s remains)
INFO - root - 2017-12-10 20:36:36.415408: step 50990, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 62h:20m:34s remains)
INFO - root - 2017-12-10 20:36:44.321614: step 51000, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 61h:14m:33s remains)
2017-12-10 20:36:45.157582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.01012476 -0.01660116 -0.022242112 -0.02515904 -0.022891987 -0.016880125 -0.011527753 -0.0098086717 -0.016496694 -0.032546312 -0.055841077 -0.078816071 -0.09327776 -0.098838925 -0.097142719][0.043727726 0.037721176 0.032685023 0.033906542 0.045862231 0.063052319 0.076319732 0.078546673 0.063921466 0.032715183 -0.011154117 -0.055171862 -0.085533217 -0.10016339 -0.10275697][0.11246724 0.10860907 0.10613561 0.11568614 0.14459851 0.18166889 0.21038429 0.21705204 0.19403321 0.14302079 0.070457213 -0.003941 -0.058514386 -0.088281132 -0.098869406][0.18433753 0.18458122 0.18669587 0.20766653 0.25792992 0.32027036 0.3697944 0.38397989 0.35232636 0.2792359 0.17423752 0.065119207 -0.018551927 -0.0676941 -0.08946836][0.26175892 0.26709571 0.27365419 0.30457529 0.37298074 0.45605373 0.52121568 0.53632522 0.49012756 0.39256763 0.25762624 0.11948167 0.012931344 -0.050130341 -0.079747759][0.35125628 0.36379808 0.37416342 0.41081646 0.49000129 0.58430648 0.65513349 0.66306466 0.5971154 0.47476596 0.314456 0.15552177 0.034297403 -0.036470775 -0.069902331][0.42856687 0.45227337 0.46871132 0.510518 0.59595084 0.69502723 0.76462245 0.76216686 0.67763931 0.53441262 0.35480881 0.1821612 0.052082337 -0.023558037 -0.059919666][0.4689447 0.5068568 0.53376824 0.57962781 0.66206318 0.75297475 0.80909723 0.79062927 0.68994856 0.53530604 0.35021847 0.17814539 0.050564494 -0.022666551 -0.057347585][0.475968 0.52739143 0.56476343 0.61068034 0.67896378 0.74592555 0.77427632 0.73331469 0.62059063 0.4665159 0.29274085 0.13788968 0.0262363 -0.035758715 -0.063502304][0.44477329 0.50422293 0.54750532 0.58896863 0.63811076 0.67580611 0.67328513 0.61190432 0.49506521 0.3517502 0.20023075 0.0724945 -0.014810227 -0.059414051 -0.075974278][0.36468789 0.42725465 0.4731043 0.50874192 0.540623 0.55496174 0.53145742 0.46218729 0.35358298 0.23016196 0.10601688 0.0069293827 -0.055959139 -0.083759107 -0.08942847][0.23842992 0.29707068 0.34114408 0.36997873 0.38776582 0.38684586 0.3551144 0.29142934 0.20290813 0.10804314 0.016724564 -0.051934786 -0.091082536 -0.10371908 -0.10034873][0.10252023 0.148032 0.18318269 0.20192957 0.20669423 0.19499379 0.16268034 0.11178867 0.049155429 -0.01273454 -0.067940325 -0.10451299 -0.12014582 -0.11877652 -0.10807833][-0.0076884367 0.019096399 0.039944135 0.047568239 0.04305971 0.026530728 -0.00065582281 -0.035491519 -0.072279148 -0.10432431 -0.12842731 -0.13841772 -0.13572915 -0.12463363 -0.11011373][-0.082069524 -0.073383689 -0.066589884 -0.067775778 -0.076460265 -0.09200637 -0.11111572 -0.13076276 -0.14686292 -0.15677741 -0.15903731 -0.15151973 -0.13731229 -0.12122463 -0.10595228]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 20:36:53.128738: step 51010, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 60h:40m:15s remains)
INFO - root - 2017-12-10 20:37:00.968759: step 51020, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 61h:57m:26s remains)
INFO - root - 2017-12-10 20:37:08.856753: step 51030, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 62h:16m:51s remains)
INFO - root - 2017-12-10 20:37:16.567211: step 51040, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.795 sec/batch; 62h:09m:53s remains)
INFO - root - 2017-12-10 20:37:24.545872: step 51050, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 60h:13m:30s remains)
INFO - root - 2017-12-10 20:37:32.477821: step 51060, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 59h:45m:17s remains)
INFO - root - 2017-12-10 20:37:40.285010: step 51070, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 60h:43m:31s remains)
INFO - root - 2017-12-10 20:37:48.008137: step 51080, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 61h:44m:56s remains)
INFO - root - 2017-12-10 20:37:55.901187: step 51090, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 60h:56m:51s remains)
INFO - root - 2017-12-10 20:38:03.727219: step 51100, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 60h:37m:30s remains)
2017-12-10 20:38:04.614943: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19162637 0.15512989 0.10376536 0.072046466 0.091597311 0.17489524 0.29947886 0.43084598 0.5418278 0.61464787 0.63107014 0.58857346 0.5042578 0.40133408 0.29955292][0.12452263 0.11117046 0.086362928 0.073979214 0.09986268 0.17512278 0.28005695 0.38278225 0.45989236 0.49862182 0.48787576 0.43124732 0.34771234 0.25990847 0.18473591][0.062377229 0.074780457 0.078513585 0.086421274 0.11774638 0.18226004 0.26463312 0.33701032 0.38164228 0.3912884 0.36030513 0.29398361 0.2106923 0.13267393 0.07608293][0.033422615 0.066459715 0.095309734 0.12277789 0.16148292 0.22067577 0.289982 0.34468094 0.37024617 0.3627021 0.31920853 0.2432249 0.15086633 0.065518163 0.0073304754][0.040234376 0.087052219 0.13768543 0.18724276 0.24097139 0.30668733 0.37788773 0.43080476 0.44913912 0.429805 0.37275341 0.28039181 0.1671641 0.058865961 -0.017658029][0.076038241 0.13278215 0.2048106 0.28162867 0.35956916 0.44167802 0.52253282 0.57769775 0.58705592 0.54901868 0.46905005 0.35243014 0.21340433 0.078808039 -0.019812874][0.12647131 0.19277191 0.28626579 0.39247474 0.49738437 0.59540385 0.67910427 0.72448981 0.71128875 0.64277935 0.53234953 0.39079872 0.23335084 0.084640048 -0.024951708][0.16909933 0.24386866 0.35522202 0.48511666 0.60893023 0.71068937 0.78013897 0.7982 0.75056833 0.64835352 0.5122388 0.3578541 0.19995061 0.057328295 -0.045670487][0.1848264 0.26271614 0.3809734 0.51886374 0.6436882 0.73147869 0.77122104 0.75272262 0.67215186 0.54861796 0.40650609 0.26208165 0.12639576 0.0099833682 -0.071249239][0.17520045 0.24776393 0.35691074 0.480658 0.58445728 0.64280581 0.6472227 0.59685743 0.498727 0.37738198 0.25597426 0.14615238 0.051935472 -0.025136888 -0.077837452][0.15381068 0.2130174 0.29665706 0.38462964 0.44884798 0.47040766 0.44629133 0.38182425 0.29072419 0.198322 0.1217576 0.063744336 0.019083703 -0.018418428 -0.048073396][0.1333535 0.17392954 0.22280824 0.26522633 0.28525037 0.27519232 0.23787208 0.18013398 0.11663862 0.069515586 0.048203047 0.045563642 0.04660162 0.039034095 0.019955678][0.129107 0.15159151 0.16739431 0.16932634 0.1552368 0.12797748 0.093912326 0.057267018 0.029860163 0.028891049 0.056963153 0.099095725 0.13216032 0.14002113 0.11847792][0.1511472 0.16094795 0.15462823 0.13256064 0.10395366 0.077410236 0.058731485 0.046905149 0.050156489 0.082158946 0.14163341 0.21020833 0.26012966 0.27317703 0.24369822][0.18881044 0.19346721 0.18031132 0.15466687 0.13174936 0.11977357 0.11975104 0.12538205 0.14302692 0.18630116 0.25464028 0.33215627 0.39029834 0.40800512 0.37519002]]...]
INFO - root - 2017-12-10 20:38:12.612387: step 51110, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 61h:31m:00s remains)
INFO - root - 2017-12-10 20:38:20.296296: step 51120, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 62h:17m:31s remains)
INFO - root - 2017-12-10 20:38:28.176576: step 51130, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 62h:42m:57s remains)
INFO - root - 2017-12-10 20:38:36.083360: step 51140, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 62h:54m:10s remains)
INFO - root - 2017-12-10 20:38:43.953513: step 51150, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 61h:16m:58s remains)
INFO - root - 2017-12-10 20:38:51.772987: step 51160, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 61h:04m:01s remains)
INFO - root - 2017-12-10 20:38:59.520067: step 51170, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 60h:48m:39s remains)
INFO - root - 2017-12-10 20:39:07.346096: step 51180, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 60h:36m:09s remains)
INFO - root - 2017-12-10 20:39:15.174246: step 51190, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 62h:35m:41s remains)
INFO - root - 2017-12-10 20:39:22.815511: step 51200, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 62h:38m:50s remains)
2017-12-10 20:39:23.608318: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013652063 0.010378415 0.0086058658 0.009171 0.011046196 0.012680032 0.012837226 0.010735651 0.0064049191 0.0010651893 -0.0030081638 -0.00451555 -0.0036022528 -0.0011765237 0.0016083036][0.039063714 0.035549864 0.034280453 0.03735473 0.043128055 0.04854361 0.050471932 0.047191769 0.038766131 0.027734794 0.018669976 0.014208115 0.014572125 0.018195434 0.022753099][0.069342487 0.066224985 0.066063464 0.072461732 0.0833191 0.094299108 0.099886052 0.096500061 0.083856329 0.066161469 0.050996032 0.042621642 0.042180773 0.04785325 0.055712327][0.096554205 0.09471748 0.097152986 0.10796042 0.12466718 0.14242996 0.15337421 0.15133967 0.13511832 0.11052004 0.088287 0.075042561 0.07352379 0.082086921 0.0947618][0.11220244 0.11565508 0.12498014 0.14297754 0.16648093 0.19108438 0.20750159 0.20716903 0.18787678 0.15685426 0.12790042 0.1103855 0.10861532 0.12056506 0.13831757][0.12034389 0.12884916 0.14606601 0.17166516 0.20157117 0.2324695 0.25481582 0.25736344 0.2365935 0.20103441 0.16720057 0.14671686 0.14499167 0.15952146 0.18179943][0.12621132 0.1381641 0.16229221 0.19570096 0.23212726 0.26849955 0.29587483 0.30055428 0.27774844 0.23774302 0.19950247 0.17599329 0.17339772 0.18976606 0.21693921][0.13752493 0.15089394 0.17911373 0.21862045 0.26029047 0.29969445 0.32890797 0.33277795 0.30600792 0.26054379 0.21654218 0.18783475 0.18187492 0.19865936 0.23078129][0.15928781 0.17111671 0.19768269 0.23842937 0.282042 0.32136691 0.34955543 0.35095346 0.31972295 0.26821688 0.21694514 0.18114887 0.16996145 0.18579805 0.2219398][0.19155015 0.1984496 0.21735854 0.2532185 0.29381043 0.32992089 0.35570425 0.35574421 0.32218751 0.26579186 0.20750928 0.16490646 0.14896069 0.1634108 0.2016338][0.23493646 0.23619176 0.24328777 0.26783931 0.29919544 0.3286953 0.35169867 0.3530682 0.32123256 0.26319718 0.20020023 0.15231487 0.13235436 0.14389893 0.18018191][0.27968264 0.27883542 0.27538973 0.28629395 0.30434564 0.32411507 0.3425971 0.34553859 0.3178311 0.26202706 0.19853041 0.14847347 0.12567642 0.13216311 0.16162919][0.31795403 0.3199062 0.31039229 0.30879611 0.31181124 0.31890982 0.32941875 0.33163187 0.30932468 0.26156098 0.20528319 0.15874937 0.13452224 0.13311785 0.15022969][0.33988166 0.34689155 0.33542684 0.32413167 0.31307447 0.30676869 0.30692711 0.30620766 0.29049546 0.25657302 0.21504839 0.17704917 0.1522727 0.14159992 0.14425594][0.33677563 0.34926632 0.3394255 0.32264057 0.30112258 0.28303981 0.27221382 0.26666987 0.25723597 0.23936115 0.21600538 0.18924323 0.1654145 0.14741246 0.13794972]]...]
INFO - root - 2017-12-10 20:39:31.498089: step 51210, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 61h:40m:03s remains)
INFO - root - 2017-12-10 20:39:39.439572: step 51220, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 61h:48m:39s remains)
INFO - root - 2017-12-10 20:39:47.297712: step 51230, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 60h:55m:39s remains)
INFO - root - 2017-12-10 20:39:55.148991: step 51240, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.768 sec/batch; 59h:59m:50s remains)
INFO - root - 2017-12-10 20:40:02.782583: step 51250, loss = 0.71, batch loss = 0.65 (11.9 examples/sec; 0.671 sec/batch; 52h:26m:44s remains)
INFO - root - 2017-12-10 20:40:10.669334: step 51260, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 61h:29m:31s remains)
INFO - root - 2017-12-10 20:40:18.499081: step 51270, loss = 0.67, batch loss = 0.61 (10.2 examples/sec; 0.784 sec/batch; 61h:13m:33s remains)
INFO - root - 2017-12-10 20:40:26.248409: step 51280, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 60h:34m:35s remains)
INFO - root - 2017-12-10 20:40:34.077809: step 51290, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 60h:12m:43s remains)
INFO - root - 2017-12-10 20:40:41.874841: step 51300, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 59h:34m:00s remains)
2017-12-10 20:40:42.718427: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12293117 0.16421439 0.2064968 0.24885939 0.28432813 0.30692807 0.31853095 0.33143881 0.34654298 0.35268456 0.34885755 0.336561 0.31902784 0.29658934 0.27574897][0.15625639 0.19763789 0.24095351 0.28542593 0.32040867 0.33806527 0.34190011 0.34620082 0.35240632 0.35024643 0.33788478 0.32048854 0.30320516 0.28716055 0.27615267][0.17541733 0.21098571 0.25149328 0.29766569 0.33537585 0.35318252 0.35477284 0.35434291 0.35369417 0.34338674 0.3208102 0.29480273 0.27436635 0.26288027 0.26091754][0.18980587 0.21335024 0.24814351 0.29649174 0.34092945 0.36554873 0.37227863 0.37225574 0.36796194 0.35193777 0.32117856 0.28658098 0.26083636 0.25089788 0.25563073][0.20677924 0.21668714 0.24549831 0.29591694 0.34752059 0.38131878 0.39565811 0.39660153 0.38820282 0.367406 0.3315658 0.29159683 0.26275831 0.25549534 0.26672316][0.23366688 0.23680657 0.26408276 0.31651497 0.37169686 0.40981057 0.42659095 0.42444855 0.41016403 0.38592935 0.34812388 0.30584872 0.27635175 0.27296108 0.28987932][0.26413429 0.27562484 0.31018531 0.36443412 0.41512111 0.44551107 0.45246786 0.43944174 0.41777217 0.39359176 0.35975343 0.32088998 0.29521197 0.29825965 0.32182583][0.28645909 0.31947497 0.36794692 0.42142072 0.45703593 0.46519643 0.44896585 0.41841012 0.38999024 0.37142414 0.35046095 0.3253186 0.31250581 0.32770875 0.36085832][0.29358256 0.35043874 0.41156125 0.45844209 0.47099221 0.44980705 0.40771142 0.3624852 0.33275047 0.32524788 0.3239283 0.32068196 0.32629323 0.35423803 0.39451575][0.28049937 0.35328183 0.41835478 0.45287558 0.44198528 0.39711806 0.341401 0.29711959 0.27854472 0.28701511 0.3052626 0.32174805 0.33988589 0.3703728 0.40744996][0.24067079 0.3141503 0.37183118 0.39151925 0.36528796 0.31348604 0.26634338 0.24430412 0.25151992 0.28032839 0.31399575 0.34054738 0.35791653 0.37660757 0.3998248][0.17766762 0.23658478 0.27771243 0.2844196 0.25576791 0.21704395 0.19980934 0.21655412 0.25925496 0.31064749 0.3541669 0.37846547 0.37968278 0.37336826 0.37533423][0.10665151 0.14389285 0.16661774 0.16575463 0.14560008 0.13195565 0.15310806 0.21155335 0.28869042 0.35947168 0.40607235 0.41774085 0.39228913 0.353673 0.33080482][0.045067612 0.063963652 0.074821316 0.073775828 0.066781931 0.076309159 0.12570846 0.21024491 0.30630103 0.38526943 0.427618 0.42259675 0.3700828 0.30113387 0.25436288][0.0055170865 0.01391507 0.020752098 0.023846574 0.027490327 0.048327845 0.10400531 0.18784431 0.27861816 0.34997442 0.38232365 0.36487025 0.29816937 0.21416837 0.15388611]]...]
INFO - root - 2017-12-10 20:40:50.745111: step 51310, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.835 sec/batch; 65h:15m:23s remains)
INFO - root - 2017-12-10 20:40:58.551756: step 51320, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 61h:34m:50s remains)
INFO - root - 2017-12-10 20:41:06.406200: step 51330, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 60h:25m:40s remains)
INFO - root - 2017-12-10 20:41:14.086901: step 51340, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 61h:18m:48s remains)
INFO - root - 2017-12-10 20:41:22.028537: step 51350, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.817 sec/batch; 63h:47m:21s remains)
INFO - root - 2017-12-10 20:41:29.775631: step 51360, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 61h:52m:38s remains)
INFO - root - 2017-12-10 20:41:37.640008: step 51370, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 61h:48m:52s remains)
INFO - root - 2017-12-10 20:41:45.451393: step 51380, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 62h:08m:09s remains)
INFO - root - 2017-12-10 20:41:53.289858: step 51390, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 61h:07m:53s remains)
INFO - root - 2017-12-10 20:42:01.074957: step 51400, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 61h:38m:00s remains)
2017-12-10 20:42:01.905462: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.050657589 0.033734672 0.023074346 0.022857988 0.033315014 0.048991311 0.061876927 0.065706924 0.056970056 0.035960212 0.0083133262 -0.01656574 -0.032069374 -0.034777034 -0.026877129][0.024430199 0.015305848 0.015248761 0.027331049 0.050856788 0.07852558 0.10013063 0.10717504 0.096162625 0.068660669 0.03188777 -0.0022159358 -0.025455583 -0.033797592 -0.029890571][0.0010587425 -0.00019962883 0.010566117 0.035713244 0.07458885 0.11787815 0.1521236 0.16533139 0.1529091 0.11767808 0.06880872 0.021750772 -0.012350474 -0.028134879 -0.029099742][-0.014594636 -0.010228575 0.0089563923 0.045677595 0.099854127 0.15988694 0.20816965 0.22892207 0.21622352 0.17363197 0.11217399 0.0504866 0.0039219917 -0.020472558 -0.026651403][-0.024639245 -0.017572889 0.0063445475 0.049764935 0.11337783 0.18445678 0.24249308 0.26967031 0.25910866 0.21508774 0.14842239 0.0779794 0.022445032 -0.00910857 -0.0199922][-0.028312711 -0.020548005 0.003851536 0.047437776 0.11210447 0.18623124 0.2492093 0.28286317 0.27915108 0.24162059 0.17826188 0.10554066 0.04408307 0.0060632708 -0.010035305][-0.020436715 -0.012588215 0.010014699 0.050093774 0.11174427 0.1857844 0.25257033 0.29356745 0.29933509 0.27184042 0.21482232 0.14160635 0.074398614 0.028803796 0.0054156194][-0.0027240526 0.006080803 0.02640941 0.06172822 0.11866152 0.19048916 0.25850782 0.30373472 0.31664491 0.29756984 0.24678329 0.17436378 0.10292789 0.050773423 0.020375116][0.015478829 0.02609231 0.044621889 0.074931644 0.12560154 0.19184165 0.25600779 0.29959974 0.31477457 0.30129334 0.25735918 0.18948257 0.11872187 0.064255789 0.03001312][0.02489293 0.037034944 0.054372009 0.080398858 0.12439893 0.18279874 0.23930043 0.27662095 0.28925937 0.27780604 0.23998888 0.17953646 0.11430162 0.062764689 0.029619038][0.023104722 0.035007942 0.050799757 0.072970018 0.11011375 0.15895006 0.20503297 0.23328066 0.24060498 0.22887053 0.19641365 0.14553331 0.09029831 0.047048975 0.019769216][0.013954557 0.023996864 0.03701333 0.054296412 0.082965873 0.1199825 0.15361553 0.17203267 0.17444125 0.16322958 0.13723874 0.09804821 0.056096628 0.024270434 0.0049763741][0.0036291773 0.011001356 0.019966608 0.030665265 0.048522189 0.07152243 0.0917394 0.10135521 0.10114765 0.093065217 0.075946532 0.05039718 0.023183202 0.0032004702 -0.0083116051][-0.0036121265 0.0020603973 0.0079066753 0.013034759 0.02111529 0.03167098 0.04075709 0.044298295 0.043390658 0.039039712 0.029763116 0.015295255 -0.00035207966 -0.011607272 -0.017569896][-0.0067220042 -0.0013016558 0.0035688984 0.0059314361 0.0075681745 0.00912134 0.0099228481 0.008918467 0.0070550567 0.0043658218 -0.00059127464 -0.008272782 -0.016538411 -0.022218585 -0.024618283]]...]
INFO - root - 2017-12-10 20:42:09.806025: step 51410, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 59h:51m:31s remains)
INFO - root - 2017-12-10 20:42:17.656119: step 51420, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 60h:25m:36s remains)
INFO - root - 2017-12-10 20:42:25.334542: step 51430, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 60h:36m:04s remains)
INFO - root - 2017-12-10 20:42:32.950962: step 51440, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 60h:42m:54s remains)
INFO - root - 2017-12-10 20:42:40.722315: step 51450, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 61h:18m:58s remains)
INFO - root - 2017-12-10 20:42:48.528111: step 51460, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 61h:57m:09s remains)
INFO - root - 2017-12-10 20:42:56.427647: step 51470, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 63h:14m:50s remains)
INFO - root - 2017-12-10 20:43:04.338587: step 51480, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 61h:52m:16s remains)
INFO - root - 2017-12-10 20:43:12.178677: step 51490, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 60h:13m:56s remains)
INFO - root - 2017-12-10 20:43:20.113999: step 51500, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 63h:04m:51s remains)
2017-12-10 20:43:20.962581: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29457337 0.28206304 0.30347353 0.34849909 0.38995409 0.40388986 0.37597346 0.31771225 0.25560409 0.20231596 0.16659676 0.14663066 0.14443468 0.16260849 0.19422884][0.36069593 0.33921006 0.34826142 0.385711 0.42751387 0.44500929 0.41752312 0.35270873 0.2749258 0.20046516 0.14100058 0.10146917 0.089303404 0.10456789 0.13957913][0.38525856 0.37396616 0.3870616 0.42789251 0.47455293 0.49652636 0.47057736 0.40140888 0.31039503 0.21835302 0.13959453 0.08384151 0.060687013 0.067696743 0.09668617][0.36955708 0.38770449 0.42443535 0.48088044 0.53531611 0.55946803 0.53370827 0.4631052 0.36469093 0.26072916 0.16750322 0.09883713 0.064584345 0.059577044 0.075671285][0.31731191 0.37397483 0.44645464 0.52720791 0.59192765 0.61851275 0.59622169 0.53324354 0.44059879 0.33554855 0.23408958 0.1545507 0.10818949 0.0893428 0.088865608][0.24513331 0.333569 0.43925673 0.54509926 0.62235975 0.65564513 0.6441375 0.59983343 0.52785331 0.4348242 0.3333399 0.24554704 0.18609671 0.15238972 0.13445045][0.17314656 0.2736119 0.39350209 0.51116675 0.59683508 0.64185023 0.65194827 0.6384213 0.60041761 0.53184825 0.43961751 0.34768441 0.27491131 0.22482589 0.18896511][0.11985449 0.2069701 0.3123531 0.41869253 0.50145489 0.55640262 0.59027284 0.61022794 0.6109215 0.57505721 0.504011 0.41961697 0.34175289 0.278815 0.22621721][0.097342305 0.1555507 0.22647765 0.30189481 0.36611831 0.41768005 0.46186197 0.5023061 0.5324946 0.52889162 0.48912972 0.42897311 0.36299151 0.299185 0.23674567][0.10827163 0.13787858 0.17071924 0.20593227 0.23717074 0.26675025 0.3001914 0.34103477 0.38428786 0.40561733 0.39921734 0.37315419 0.33314437 0.28269905 0.22252855][0.14611393 0.15657961 0.15942408 0.15776484 0.15284547 0.15085164 0.1586583 0.18227977 0.22042707 0.252393 0.26928127 0.27272889 0.26096085 0.23234071 0.18605435][0.19543433 0.19632308 0.18182835 0.15806174 0.13111015 0.10656406 0.089619294 0.088811159 0.10833646 0.13374875 0.15471724 0.16976994 0.1742944 0.16402696 0.13535397][0.23196712 0.22903563 0.21190444 0.18905427 0.16505677 0.13814186 0.1082966 0.085481204 0.079005271 0.082512774 0.088158973 0.095724285 0.10166867 0.10107373 0.0874419][0.24455671 0.24313372 0.2387657 0.2407607 0.2448274 0.23554768 0.20574503 0.16562229 0.12981851 0.10189391 0.079915039 0.066811889 0.061855234 0.060613617 0.054463267][0.24142742 0.24527067 0.26147255 0.29864624 0.3411999 0.35862419 0.3367562 0.28589395 0.22553058 0.16761513 0.11664951 0.079077385 0.057249878 0.047151629 0.039069582]]...]
INFO - root - 2017-12-10 20:43:28.767443: step 51510, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 62h:11m:57s remains)
INFO - root - 2017-12-10 20:43:36.319086: step 51520, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 61h:59m:37s remains)
INFO - root - 2017-12-10 20:43:44.248123: step 51530, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 61h:10m:54s remains)
INFO - root - 2017-12-10 20:43:52.156399: step 51540, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 59h:36m:25s remains)
INFO - root - 2017-12-10 20:44:00.026731: step 51550, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 60h:40m:00s remains)
INFO - root - 2017-12-10 20:44:07.980786: step 51560, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 61h:50m:03s remains)
INFO - root - 2017-12-10 20:44:15.838771: step 51570, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 60h:52m:50s remains)
INFO - root - 2017-12-10 20:44:23.662402: step 51580, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 61h:05m:38s remains)
INFO - root - 2017-12-10 20:44:31.513827: step 51590, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.777 sec/batch; 60h:38m:57s remains)
INFO - root - 2017-12-10 20:44:39.033844: step 51600, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 60h:03m:29s remains)
2017-12-10 20:44:39.911695: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1241698 0.073721386 0.047487382 0.056178294 0.095432177 0.14610071 0.18558778 0.20808043 0.21919832 0.22528389 0.22684696 0.22687908 0.231568 0.23703389 0.23660737][0.17884852 0.12173492 0.09051688 0.09930101 0.14422351 0.2030918 0.24737987 0.267521 0.271063 0.26908168 0.26682881 0.26839703 0.28053209 0.29486191 0.30192989][0.2306485 0.17293411 0.14196803 0.15311167 0.20317522 0.26825193 0.31593624 0.33142591 0.32333454 0.30843031 0.29718837 0.297366 0.31493044 0.33740968 0.35255924][0.25890338 0.209363 0.18693741 0.20598605 0.26417473 0.3367959 0.38842729 0.40035981 0.38101897 0.35044247 0.3248156 0.3147966 0.3278082 0.35164121 0.37266508][0.24230726 0.21195008 0.20967463 0.24591647 0.31812865 0.40235522 0.46183333 0.4758774 0.45017922 0.4052867 0.36048478 0.32978895 0.32324672 0.33345777 0.34998158][0.1758173 0.1722479 0.19941373 0.26214474 0.35582563 0.45823163 0.53356165 0.55980945 0.53791291 0.48501691 0.42083865 0.36037314 0.31776229 0.29439357 0.28839874][0.080070741 0.0986749 0.15351497 0.24413691 0.36259478 0.48782969 0.58611012 0.63320708 0.6245656 0.57180315 0.4921113 0.3999213 0.31323731 0.24407192 0.20117004][-0.0049134064 0.022343202 0.089758985 0.19566396 0.33050129 0.47457418 0.59639043 0.66879839 0.68114638 0.6377908 0.55106437 0.43451828 0.30995888 0.19865961 0.11800445][-0.048674107 -0.024904633 0.035918184 0.13594669 0.26763949 0.41454491 0.54835576 0.64085466 0.67581373 0.64959687 0.56856781 0.44467568 0.30231637 0.16764589 0.062435832][-0.036123406 -0.02147452 0.019135362 0.093369827 0.19844514 0.32360721 0.44666505 0.54184091 0.58965915 0.58046091 0.51563483 0.40505159 0.27201235 0.14149785 0.034534518][0.039202578 0.044721115 0.057971653 0.092949755 0.15257823 0.23284099 0.3203426 0.39592019 0.4407419 0.44156173 0.39657572 0.31287149 0.20970532 0.10644633 0.019401371][0.16564576 0.16537571 0.14996208 0.14012925 0.14359926 0.1632947 0.19640803 0.23400694 0.26116237 0.26322389 0.23593849 0.18359223 0.120004 0.056718979 0.0034269907][0.32184896 0.32021576 0.27975994 0.22780025 0.17545323 0.13156916 0.10366646 0.094533361 0.095197804 0.092129707 0.079099491 0.056869958 0.032775927 0.01030629 -0.0068185655][0.45821029 0.46118504 0.40569291 0.32413596 0.22965123 0.13497165 0.055414919 0.0044542393 -0.020770097 -0.031165751 -0.034105614 -0.032413621 -0.025007188 -0.014831681 -0.0025210571][0.534187 0.54440212 0.48443687 0.38881078 0.27152714 0.14710015 0.035711482 -0.041217387 -0.081586629 -0.0944473 -0.0887972 -0.070940286 -0.043946519 -0.014346681 0.014837617]]...]
INFO - root - 2017-12-10 20:44:47.639346: step 51610, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.812 sec/batch; 63h:21m:03s remains)
INFO - root - 2017-12-10 20:44:55.618159: step 51620, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 62h:06m:15s remains)
INFO - root - 2017-12-10 20:45:03.508074: step 51630, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 59h:47m:42s remains)
INFO - root - 2017-12-10 20:45:11.347999: step 51640, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.768 sec/batch; 59h:56m:30s remains)
INFO - root - 2017-12-10 20:45:19.102007: step 51650, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 60h:40m:20s remains)
INFO - root - 2017-12-10 20:45:26.983148: step 51660, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 63h:17m:03s remains)
INFO - root - 2017-12-10 20:45:34.870288: step 51670, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 61h:07m:26s remains)
INFO - root - 2017-12-10 20:45:42.491918: step 51680, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.784 sec/batch; 61h:08m:34s remains)
INFO - root - 2017-12-10 20:45:50.303725: step 51690, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 61h:03m:21s remains)
INFO - root - 2017-12-10 20:45:57.971141: step 51700, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.777 sec/batch; 60h:36m:13s remains)
2017-12-10 20:45:58.772725: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17835835 0.16673946 0.16436379 0.1714085 0.17725177 0.17119379 0.15707707 0.15121156 0.16153394 0.19077775 0.22722004 0.24728651 0.22571348 0.15724337 0.068782575][0.20124437 0.2004495 0.20849684 0.22287743 0.2301933 0.22126125 0.20373686 0.19513951 0.20202921 0.22463678 0.25097141 0.25910804 0.22720672 0.15108667 0.058725197][0.20966785 0.22197267 0.24285585 0.26683334 0.27799726 0.26889306 0.25092289 0.2412817 0.24386716 0.25580919 0.26632917 0.25877529 0.2155344 0.13412975 0.041477539][0.20622832 0.23129971 0.26493928 0.30012453 0.31944606 0.31609926 0.30254543 0.29402193 0.29169008 0.29091933 0.28458712 0.26211485 0.20944105 0.12378867 0.030458413][0.18927692 0.22604299 0.27192691 0.32026917 0.35398868 0.36494368 0.36230966 0.35706612 0.34774312 0.33105251 0.306332 0.27022573 0.21080859 0.12287226 0.028697656][0.16696799 0.21135974 0.2657595 0.32532436 0.37457898 0.40317121 0.41438732 0.41329214 0.396027 0.36274454 0.32106367 0.27483261 0.21250583 0.12485471 0.030663088][0.14625943 0.19223014 0.24878246 0.31493071 0.37736297 0.42306364 0.44800147 0.45039666 0.42497304 0.37653136 0.32132539 0.26929438 0.20828053 0.12446798 0.032314941][0.12759817 0.16886881 0.22295241 0.29333284 0.36904323 0.43397987 0.47565886 0.48464131 0.45402426 0.392943 0.32533786 0.26671237 0.20579109 0.12565289 0.035672855][0.11269362 0.14718619 0.19597001 0.26617813 0.34958065 0.42842206 0.48448661 0.50257063 0.47316721 0.40680659 0.33163562 0.2675159 0.20561662 0.12814115 0.040454775][0.09787064 0.12745926 0.1703659 0.23461583 0.31463173 0.39450473 0.45599982 0.48253605 0.46197104 0.40217796 0.32979059 0.26560867 0.20379283 0.12846924 0.04339686][0.081048489 0.10807133 0.14646652 0.20288475 0.27351525 0.34605241 0.40593284 0.43853071 0.4303475 0.38477209 0.3228032 0.2628977 0.2019881 0.12791789 0.045185052][0.058584508 0.083010823 0.1172931 0.16631685 0.22740798 0.29106337 0.34646621 0.38217366 0.3847079 0.3538658 0.30451375 0.2518467 0.19458279 0.12380303 0.0451937][0.032739472 0.0525037 0.081783354 0.1241491 0.17784223 0.23442507 0.28496826 0.32087994 0.32990572 0.31037471 0.2726703 0.22890009 0.17840677 0.11405918 0.042096034][0.016791467 0.03019161 0.052287124 0.085728705 0.12924205 0.17485982 0.21500008 0.24433547 0.25365561 0.24181528 0.21607184 0.18478392 0.14605284 0.092910267 0.03146835][0.013044071 0.020528389 0.034711465 0.057593234 0.087891094 0.11836683 0.14317799 0.160718 0.16575573 0.15854973 0.14393564 0.12625152 0.10133463 0.061755553 0.0132513]]...]
INFO - root - 2017-12-10 20:46:06.629703: step 51710, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 60h:42m:10s remains)
INFO - root - 2017-12-10 20:46:14.397073: step 51720, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 59h:47m:17s remains)
INFO - root - 2017-12-10 20:46:22.192644: step 51730, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 60h:49m:04s remains)
INFO - root - 2017-12-10 20:46:30.025611: step 51740, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 61h:20m:16s remains)
INFO - root - 2017-12-10 20:46:37.832330: step 51750, loss = 0.73, batch loss = 0.67 (10.0 examples/sec; 0.801 sec/batch; 62h:28m:40s remains)
INFO - root - 2017-12-10 20:46:45.559913: step 51760, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 61h:55m:46s remains)
INFO - root - 2017-12-10 20:46:53.409478: step 51770, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 61h:01m:51s remains)
INFO - root - 2017-12-10 20:47:01.300056: step 51780, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.836 sec/batch; 65h:10m:28s remains)
INFO - root - 2017-12-10 20:47:09.013936: step 51790, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 61h:57m:55s remains)
INFO - root - 2017-12-10 20:47:16.889053: step 51800, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 61h:49m:59s remains)
2017-12-10 20:47:17.677023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042782128 -0.038260885 -0.029601768 -0.016881451 0.00017015268 0.018737 0.035565905 0.04391066 0.040605973 0.035773229 0.040138513 0.05376726 0.069869459 0.080454543 0.082465492][-0.034865487 -0.026304817 -0.013555842 0.0046998141 0.031569768 0.064581662 0.097835071 0.11936368 0.12293954 0.11815957 0.11812556 0.12222321 0.12396419 0.11902278 0.10803133][-0.0085983323 0.0072718966 0.0247967 0.046610165 0.0802769 0.1257721 0.17454012 0.20929883 0.22173844 0.22038817 0.21843718 0.21382722 0.20021506 0.17702375 0.14832285][0.043563887 0.070801765 0.09516573 0.11871074 0.15211707 0.20024711 0.25526872 0.29781139 0.31903282 0.3246319 0.325243 0.31534883 0.28772432 0.24553962 0.19651832][0.1205673 0.15829624 0.18889172 0.21157593 0.23677045 0.27546224 0.32523379 0.36802769 0.39562723 0.4100762 0.41794977 0.40859768 0.37258559 0.31543198 0.24757336][0.20786142 0.24834429 0.28132007 0.3021141 0.315981 0.33900368 0.37730721 0.41629168 0.44743216 0.46844563 0.48278457 0.47601771 0.43695188 0.3714999 0.29144391][0.27746704 0.3065764 0.33441445 0.35356453 0.35930797 0.36933497 0.39868391 0.43604878 0.47027713 0.49419197 0.51100725 0.50542289 0.46584356 0.39847866 0.31531706][0.30850103 0.31418046 0.32756507 0.34237593 0.34382519 0.34803787 0.37481752 0.4136692 0.44877276 0.47090822 0.48708782 0.48320651 0.4471021 0.38454694 0.30739996][0.31126508 0.29300249 0.28827137 0.29475537 0.29154566 0.29154381 0.31493133 0.35053349 0.379565 0.39479992 0.40984255 0.41186446 0.38615823 0.3361344 0.272652][0.29772529 0.26512113 0.24485075 0.24059048 0.23068862 0.22433655 0.23883577 0.26292691 0.27838266 0.28311953 0.29659614 0.30710217 0.29671761 0.26495105 0.22063279][0.2782689 0.24017863 0.2089664 0.19415307 0.17766969 0.16497621 0.16914405 0.17963368 0.18089491 0.17529798 0.18493243 0.19985256 0.20115629 0.18489829 0.15801798][0.2587252 0.22422962 0.18927564 0.16745919 0.14742252 0.13207896 0.12999775 0.13090307 0.12243908 0.10890938 0.11154371 0.12369067 0.12929533 0.12233877 0.10686392][0.23720424 0.21477389 0.18590318 0.16551591 0.14954816 0.13870558 0.13572156 0.12997058 0.11381397 0.093021028 0.086025275 0.090189427 0.094263941 0.090958714 0.080649287][0.21363796 0.20852785 0.19346283 0.18169007 0.17549166 0.17460786 0.17609507 0.16796027 0.14716327 0.12148448 0.10685207 0.10332821 0.10309649 0.098252229 0.086560719][0.18193683 0.1919537 0.19097392 0.18965477 0.19413424 0.2046375 0.21528438 0.21268965 0.19582185 0.17470118 0.163487 0.16141209 0.15969416 0.14951935 0.12931091]]...]
INFO - root - 2017-12-10 20:47:25.529137: step 51810, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 61h:34m:07s remains)
INFO - root - 2017-12-10 20:47:33.314771: step 51820, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 61h:51m:03s remains)
INFO - root - 2017-12-10 20:47:41.190583: step 51830, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.758 sec/batch; 59h:05m:20s remains)
INFO - root - 2017-12-10 20:47:48.901933: step 51840, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 62h:03m:41s remains)
INFO - root - 2017-12-10 20:47:56.668185: step 51850, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 61h:29m:13s remains)
INFO - root - 2017-12-10 20:48:04.402670: step 51860, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 59h:37m:28s remains)
INFO - root - 2017-12-10 20:48:12.234762: step 51870, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 62h:25m:36s remains)
INFO - root - 2017-12-10 20:48:19.992126: step 51880, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 62h:04m:23s remains)
INFO - root - 2017-12-10 20:48:27.885141: step 51890, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 61h:37m:59s remains)
INFO - root - 2017-12-10 20:48:35.705387: step 51900, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.750 sec/batch; 58h:26m:48s remains)
2017-12-10 20:48:36.606205: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30477509 0.32967022 0.3252891 0.29385403 0.24975401 0.22153407 0.21509211 0.21350247 0.20326975 0.18704695 0.16583414 0.12981778 0.079163671 0.02806896 -0.0081176078][0.39176276 0.41605106 0.40033022 0.34882265 0.28341153 0.24065208 0.22748271 0.22139853 0.20493442 0.1786083 0.14700074 0.10287286 0.048619777 -0.0011153565 -0.031395875][0.46389627 0.48812494 0.4650909 0.40040186 0.32205567 0.27440661 0.26451096 0.26066041 0.23909259 0.19866247 0.14907801 0.08824762 0.022547517 -0.031225901 -0.058272503][0.5438081 0.56647575 0.53885084 0.46889061 0.38607752 0.34112111 0.34252337 0.34796008 0.32393152 0.26644728 0.19246507 0.10640495 0.01910698 -0.048377581 -0.079640277][0.64464748 0.664568 0.63721913 0.57384294 0.49918592 0.46601242 0.4866077 0.50809461 0.48369083 0.40521857 0.29885253 0.17665577 0.054044694 -0.041018024 -0.08766681][0.74645448 0.7617448 0.7387166 0.69264024 0.63907695 0.6274519 0.67348391 0.71437967 0.6896683 0.58685929 0.44358081 0.27946755 0.114734 -0.015137452 -0.083538167][0.80934906 0.82378554 0.81235492 0.79476297 0.77412444 0.79023147 0.85941142 0.9122656 0.87940568 0.74760574 0.5671913 0.36582118 0.16689955 0.010172028 -0.074955449][0.78811669 0.80739164 0.815519 0.83418655 0.85351938 0.89844871 0.98287767 1.0367163 0.99049747 0.8342858 0.62881434 0.40784198 0.19449669 0.028023683 -0.063981265][0.647968 0.67437565 0.70404792 0.75753886 0.8143521 0.8839606 0.97530246 1.0241449 0.96965075 0.80805212 0.60426319 0.39254156 0.19177942 0.035505466 -0.052778888][0.42370698 0.45427719 0.49726948 0.5689944 0.645274 0.72632629 0.81561482 0.858937 0.80948794 0.66976273 0.49943727 0.32554346 0.16018218 0.029739214 -0.045801729][0.18995515 0.21906516 0.26156545 0.3289474 0.40078881 0.47583082 0.5543735 0.59456027 0.56399435 0.46721599 0.35055923 0.22950734 0.10987503 0.012390786 -0.045125216][0.016584458 0.038386434 0.068901658 0.11483863 0.16360746 0.21809682 0.2790755 0.31704974 0.31033117 0.26050454 0.19797757 0.12821171 0.053074338 -0.011537049 -0.049915913][-0.074040875 -0.064903654 -0.052290205 -0.033721872 -0.013955156 0.016207211 0.059375223 0.095236994 0.10736795 0.092754945 0.069564044 0.037881907 -0.0019773371 -0.038926791 -0.060143765][-0.09763658 -0.10081275 -0.10467516 -0.1087386 -0.11234639 -0.10207549 -0.074561171 -0.04489043 -0.026905695 -0.025802301 -0.029955722 -0.040041957 -0.056089893 -0.071679384 -0.078004614][-0.087241672 -0.096702747 -0.1087217 -0.1236996 -0.13863267 -0.14038193 -0.12668735 -0.10888428 -0.097446531 -0.096520051 -0.097374395 -0.099098578 -0.10169645 -0.10270345 -0.097829528]]...]
INFO - root - 2017-12-10 20:48:44.400705: step 51910, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 60h:07m:10s remains)
INFO - root - 2017-12-10 20:48:51.974154: step 51920, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 60h:08m:54s remains)
INFO - root - 2017-12-10 20:48:59.812680: step 51930, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.796 sec/batch; 62h:00m:50s remains)
INFO - root - 2017-12-10 20:49:07.733276: step 51940, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 60h:09m:08s remains)
INFO - root - 2017-12-10 20:49:15.514721: step 51950, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 59h:37m:59s remains)
INFO - root - 2017-12-10 20:49:23.289248: step 51960, loss = 0.72, batch loss = 0.66 (11.0 examples/sec; 0.728 sec/batch; 56h:46m:12s remains)
INFO - root - 2017-12-10 20:49:31.019824: step 51970, loss = 0.71, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 61h:01m:30s remains)
INFO - root - 2017-12-10 20:49:38.789061: step 51980, loss = 0.69, batch loss = 0.64 (10.6 examples/sec; 0.754 sec/batch; 58h:43m:16s remains)
INFO - root - 2017-12-10 20:49:46.688440: step 51990, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 61h:03m:18s remains)
INFO - root - 2017-12-10 20:49:54.383181: step 52000, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 62h:09m:17s remains)
2017-12-10 20:49:55.185487: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043715093 -0.052039266 -0.057326097 -0.058592647 -0.057315715 -0.05646693 -0.056488886 -0.059333917 -0.065870792 -0.076067217 -0.08693102 -0.09351673 -0.092153937 -0.083224468 -0.069832012][-0.0036785756 -0.011170675 -0.010958311 -0.0024778377 0.0089433258 0.017430868 0.02221895 0.020272283 0.0085289711 -0.012540185 -0.036890473 -0.054949213 -0.060051586 -0.052843668 -0.037492685][0.044558495 0.042847507 0.056438502 0.085064024 0.11862723 0.14732513 0.16782236 0.17299262 0.15453079 0.11469352 0.0658187 0.026601674 0.0073862863 0.006261637 0.018423418][0.088130206 0.096051611 0.1287563 0.18612373 0.25429109 0.31961182 0.37339306 0.39894384 0.3768566 0.31008765 0.22250435 0.14843455 0.10408698 0.087020211 0.091495052][0.11574578 0.13356055 0.18382457 0.26900983 0.37516794 0.4869349 0.58852136 0.64775586 0.62890679 0.53397417 0.40076068 0.28258005 0.20450376 0.16596384 0.16087635][0.12815307 0.1497211 0.20691504 0.30617395 0.43764344 0.58833355 0.73716676 0.83469141 0.8281945 0.71621019 0.54677558 0.3882364 0.27639332 0.21633875 0.20278023][0.13438602 0.15091003 0.19842097 0.28886729 0.42029038 0.58580738 0.76312721 0.88985032 0.90130866 0.790959 0.60889196 0.42982066 0.29851916 0.22681573 0.21054408][0.15698782 0.16047946 0.18046956 0.23705313 0.33908069 0.48720273 0.66149026 0.79642087 0.82481712 0.73508757 0.57051086 0.40093037 0.27425924 0.20691422 0.19530171][0.2248895 0.21405867 0.1947796 0.19699369 0.24292575 0.34199896 0.47931215 0.59643424 0.63262379 0.57356131 0.45012844 0.31827241 0.22058044 0.17409129 0.17371602][0.35739589 0.34165129 0.28365451 0.22294998 0.19592145 0.22143885 0.29199308 0.36428171 0.39103892 0.358305 0.28389046 0.2060487 0.15519004 0.1426985 0.16001108][0.54675847 0.53958207 0.45059347 0.3290309 0.22470258 0.16575688 0.15361759 0.16455203 0.16711286 0.14920662 0.11763318 0.09360195 0.092387833 0.11676409 0.15440828][0.74439639 0.75336426 0.64355218 0.47325632 0.30311432 0.16926105 0.08189442 0.034264803 0.0076603396 -0.0078026582 -0.01211435 0.0027750474 0.038621295 0.090368174 0.14456654][0.88206923 0.9076491 0.78950649 0.59129018 0.38016024 0.19860646 0.06389758 -0.02150055 -0.070463873 -0.092625983 -0.090495378 -0.061992861 -0.011926804 0.050838914 0.1136333][0.90054345 0.93705863 0.8236438 0.6239236 0.40492111 0.21099433 0.063871354 -0.031797625 -0.088891193 -0.11901342 -0.12510711 -0.10491982 -0.062134348 -0.0044070436 0.057660557][0.78163719 0.82252079 0.72906506 0.55783355 0.36790606 0.1995341 0.074035667 -0.0071538547 -0.05910686 -0.094988465 -0.11739684 -0.12066365 -0.10239561 -0.064248472 -0.013454071]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 20:50:03.016785: step 52010, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 60h:58m:58s remains)
INFO - root - 2017-12-10 20:50:10.878279: step 52020, loss = 0.67, batch loss = 0.62 (10.4 examples/sec; 0.766 sec/batch; 59h:40m:51s remains)
INFO - root - 2017-12-10 20:50:18.699082: step 52030, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 60h:19m:29s remains)
INFO - root - 2017-12-10 20:50:26.648680: step 52040, loss = 0.67, batch loss = 0.61 (10.1 examples/sec; 0.792 sec/batch; 61h:43m:56s remains)
INFO - root - 2017-12-10 20:50:34.452385: step 52050, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 60h:45m:31s remains)
INFO - root - 2017-12-10 20:50:42.272760: step 52060, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 60h:40m:42s remains)
INFO - root - 2017-12-10 20:50:50.092154: step 52070, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 62h:01m:08s remains)
INFO - root - 2017-12-10 20:50:57.761444: step 52080, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 61h:17m:12s remains)
INFO - root - 2017-12-10 20:51:05.542817: step 52090, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.754 sec/batch; 58h:44m:28s remains)
INFO - root - 2017-12-10 20:51:13.391378: step 52100, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 59h:29m:36s remains)
2017-12-10 20:51:14.220800: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31419861 0.23899044 0.15373389 0.09708602 0.075520866 0.072088733 0.070234813 0.055981044 0.028314782 0.0049865879 0.00088451389 0.018165028 0.041499648 0.050726511 0.030681444][0.44168863 0.35542256 0.25887331 0.2024983 0.19284572 0.20567195 0.21520835 0.2001489 0.16105796 0.12535276 0.11497793 0.13262022 0.15711965 0.16013032 0.12068345][0.49450722 0.41269869 0.326451 0.29255268 0.31560275 0.36321536 0.39782965 0.38840866 0.3382014 0.2875149 0.26733547 0.28191623 0.30440572 0.29766566 0.23451567][0.44813052 0.38571846 0.32894328 0.33491966 0.406231 0.50308377 0.57549393 0.58186722 0.52609754 0.46038795 0.42672023 0.43284288 0.44897082 0.43095669 0.34471735][0.33367074 0.29400712 0.2731142 0.32394868 0.44723824 0.59741759 0.712841 0.74167877 0.68589741 0.60654128 0.55624956 0.54855645 0.55488569 0.52742457 0.42552122][0.20333055 0.1825185 0.19570453 0.29005542 0.46425238 0.66719073 0.82614106 0.87869316 0.82156545 0.72089565 0.63993704 0.60210186 0.58646852 0.54720151 0.43892178][0.10060184 0.0910437 0.12814911 0.25406963 0.466826 0.71131837 0.90507364 0.97595978 0.91346532 0.784418 0.66124636 0.58079261 0.53485948 0.48318532 0.3784464][0.0727703 0.062644213 0.10325468 0.23533644 0.45851967 0.71732819 0.9241479 1.0011963 0.93047988 0.77396 0.60950392 0.48712167 0.41259262 0.35340363 0.26254106][0.12847412 0.10681265 0.13011943 0.24068345 0.44144276 0.68098068 0.87360108 0.94203681 0.86325824 0.68996012 0.49900976 0.34889832 0.25614354 0.19616666 0.12586384][0.21384878 0.17523527 0.17007771 0.24219456 0.39711311 0.59080654 0.74707097 0.79595256 0.71335781 0.541157 0.34859246 0.1941196 0.099123552 0.045866534 -0.0017712098][0.25129503 0.201598 0.17350376 0.20820168 0.31158981 0.44927078 0.55988288 0.586335 0.50717491 0.35389557 0.18370143 0.047302958 -0.034943264 -0.07555861 -0.10219751][0.20864673 0.15729508 0.11773549 0.12362736 0.1796902 0.26254833 0.32869568 0.3371765 0.27157009 0.15380608 0.026284589 -0.073184609 -0.12956864 -0.15221946 -0.16090447][0.12245585 0.07695765 0.035433877 0.022069167 0.040546358 0.077849336 0.10800552 0.10577524 0.06017413 -0.015116835 -0.092741534 -0.14911002 -0.17598875 -0.18059804 -0.17619278][0.026607446 -0.0089046061 -0.044459123 -0.06534978 -0.069114156 -0.061404914 -0.054398034 -0.060777258 -0.086509213 -0.12403301 -0.15849763 -0.17856081 -0.18206355 -0.17457217 -0.16394198][-0.065582268 -0.090271473 -0.11473533 -0.13286629 -0.14365718 -0.1486966 -0.15160981 -0.15678412 -0.16659507 -0.17746052 -0.18322717 -0.18074699 -0.17136377 -0.15892279 -0.14723474]]...]
INFO - root - 2017-12-10 20:51:21.961134: step 52110, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 61h:52m:19s remains)
INFO - root - 2017-12-10 20:51:29.832878: step 52120, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 59h:59m:03s remains)
INFO - root - 2017-12-10 20:51:37.653229: step 52130, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 60h:22m:09s remains)
INFO - root - 2017-12-10 20:51:45.351291: step 52140, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 63h:04m:29s remains)
INFO - root - 2017-12-10 20:51:53.263995: step 52150, loss = 0.72, batch loss = 0.66 (9.7 examples/sec; 0.826 sec/batch; 64h:20m:53s remains)
INFO - root - 2017-12-10 20:52:00.805685: step 52160, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.757 sec/batch; 58h:56m:38s remains)
INFO - root - 2017-12-10 20:52:08.858261: step 52170, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 61h:04m:26s remains)
INFO - root - 2017-12-10 20:52:16.680955: step 52180, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.793 sec/batch; 61h:45m:30s remains)
INFO - root - 2017-12-10 20:52:24.571310: step 52190, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 61h:07m:44s remains)
INFO - root - 2017-12-10 20:52:32.509683: step 52200, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 61h:58m:26s remains)
2017-12-10 20:52:33.357204: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16952965 0.15968794 0.15137641 0.14781292 0.15008637 0.15722361 0.1649514 0.16562782 0.15882844 0.14955439 0.13724113 0.11880535 0.092106722 0.056469623 0.015772706][0.17830461 0.18206273 0.19108473 0.207835 0.2323382 0.25883839 0.27691457 0.27589223 0.25710383 0.23145729 0.20253605 0.17089055 0.13619526 0.096141011 0.049929507][0.17742121 0.19683863 0.22788493 0.27176985 0.32691437 0.381478 0.41676605 0.41695145 0.38442978 0.3363584 0.28273511 0.22933961 0.17961253 0.13167682 0.080447637][0.18959576 0.22853036 0.28470954 0.35652405 0.44141212 0.522157 0.57256007 0.57096738 0.52078962 0.44428334 0.35855111 0.27619392 0.20584425 0.14875294 0.09599302][0.2374367 0.29597303 0.37281263 0.46522698 0.57304084 0.67544413 0.73783535 0.73109233 0.6582588 0.5463571 0.41915178 0.29796344 0.20096894 0.13585871 0.0899524][0.30897817 0.3826243 0.47137895 0.57467604 0.69782263 0.81768936 0.89102471 0.87965167 0.78424138 0.63579041 0.463065 0.29804027 0.1723024 0.10294234 0.0734836][0.37051412 0.44987288 0.53966695 0.64446497 0.77556545 0.90755939 0.99030024 0.97755754 0.86647367 0.68987161 0.47966066 0.27824244 0.13082793 0.063875265 0.057207722][0.39343482 0.46733022 0.54733187 0.64354837 0.77113748 0.90383106 0.98998237 0.9799279 0.867325 0.68306661 0.45911673 0.24480744 0.094443135 0.040488236 0.059153277][0.361255 0.421345 0.48455262 0.5645622 0.67692178 0.79633284 0.87593055 0.87015057 0.771085 0.60407597 0.39762598 0.20165995 0.07092075 0.037901323 0.076551594][0.30387488 0.33925948 0.37459841 0.42782831 0.51399851 0.60994047 0.67622143 0.67537844 0.59977579 0.46777803 0.30265188 0.14903525 0.054356363 0.045522366 0.096516117][0.25286198 0.25454551 0.25314566 0.27190229 0.3251844 0.39326084 0.44376823 0.44783083 0.399368 0.31030259 0.19823261 0.097842947 0.044355892 0.05590716 0.10952958][0.24646208 0.21709588 0.17931435 0.16098872 0.17669329 0.21216621 0.24261156 0.24726072 0.22215427 0.17363915 0.11296388 0.062620305 0.04448545 0.068038821 0.11574356][0.28352538 0.23626894 0.17304942 0.1242606 0.10498364 0.10802057 0.11626859 0.11612146 0.10600873 0.08814095 0.066746339 0.052602779 0.057387833 0.084975883 0.12398958][0.33577237 0.28643325 0.21445517 0.14852479 0.10433684 0.0817466 0.070427343 0.062210266 0.057123531 0.054968547 0.054114215 0.057152439 0.070680268 0.096370764 0.12816502][0.3682237 0.32760346 0.25891054 0.18670925 0.12755974 0.087761983 0.062368535 0.046182077 0.039121654 0.03950268 0.042396277 0.047362491 0.060539436 0.083632968 0.11364306]]...]
INFO - root - 2017-12-10 20:52:41.187988: step 52210, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 61h:17m:32s remains)
INFO - root - 2017-12-10 20:52:49.089621: step 52220, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 60h:28m:51s remains)
INFO - root - 2017-12-10 20:52:56.842722: step 52230, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 59h:52m:23s remains)
INFO - root - 2017-12-10 20:53:04.264561: step 52240, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 63h:21m:26s remains)
INFO - root - 2017-12-10 20:53:12.239274: step 52250, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 60h:26m:36s remains)
INFO - root - 2017-12-10 20:53:20.203239: step 52260, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 62h:15m:07s remains)
INFO - root - 2017-12-10 20:53:28.001789: step 52270, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 59h:42m:33s remains)
INFO - root - 2017-12-10 20:53:35.890368: step 52280, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 62h:36m:50s remains)
INFO - root - 2017-12-10 20:53:43.843309: step 52290, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 62h:05m:45s remains)
INFO - root - 2017-12-10 20:53:51.750980: step 52300, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.787 sec/batch; 61h:14m:17s remains)
2017-12-10 20:53:52.618555: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1160526 0.089542605 0.06968236 0.070741892 0.091906868 0.12225234 0.15543905 0.18120579 0.19721812 0.20473272 0.1996755 0.19360721 0.19498631 0.19552968 0.18132699][0.17296495 0.13755251 0.11162906 0.11658116 0.15133443 0.19969206 0.25076905 0.29096088 0.31864589 0.33702174 0.33988753 0.33939666 0.343396 0.34083831 0.3162331][0.22867222 0.19013171 0.15962379 0.16700342 0.21042565 0.26944783 0.33042333 0.37891698 0.41326445 0.43683326 0.44315532 0.44472024 0.44676054 0.43690163 0.40263861][0.3085508 0.26446041 0.22608878 0.23242153 0.27968767 0.34268245 0.4052867 0.45343605 0.48556727 0.50441593 0.50759238 0.50581968 0.501645 0.48245767 0.44023159][0.42550454 0.37841031 0.33090195 0.3342129 0.38283953 0.44684324 0.50727016 0.54981387 0.57451713 0.58501738 0.5827018 0.57490325 0.56084532 0.52922612 0.47463417][0.54283768 0.49616405 0.44226298 0.4432582 0.49196535 0.55566418 0.6128841 0.64866459 0.665412 0.66756487 0.6585086 0.64149284 0.61283696 0.56496984 0.49298814][0.6073041 0.56474805 0.51282686 0.51698071 0.56925803 0.63733751 0.69780385 0.73286211 0.74462587 0.73868114 0.717668 0.68314052 0.63270837 0.5655359 0.47601515][0.58741117 0.549922 0.50681043 0.51946938 0.57863295 0.65363371 0.72057384 0.75820166 0.76627022 0.75001019 0.713119 0.65822619 0.58738357 0.50725389 0.41123536][0.48589846 0.45120052 0.41779131 0.43778211 0.5005641 0.57809073 0.64805532 0.68764043 0.69492024 0.67512554 0.63293064 0.57144213 0.49586537 0.41712356 0.32747075][0.33337459 0.30214679 0.27767548 0.30100349 0.3616749 0.43563825 0.50355327 0.54318458 0.5511834 0.53233629 0.49161184 0.43316367 0.36387953 0.29617715 0.22207157][0.16516264 0.13775244 0.12069847 0.14180018 0.19208153 0.25308517 0.30959919 0.34240469 0.34784219 0.33018637 0.29436091 0.24551009 0.19065067 0.14141981 0.090674244][0.018896325 -0.0044073337 -0.015973167 -0.00029646588 0.035758972 0.079955772 0.12141415 0.14495613 0.14725648 0.13185219 0.10304151 0.066186383 0.028032124 -0.0018775751 -0.029121129][-0.070969671 -0.090005346 -0.098692372 -0.090446815 -0.070042595 -0.044699542 -0.020864477 -0.0087698987 -0.010517383 -0.02350086 -0.044103332 -0.067747995 -0.08938019 -0.10243504 -0.1108113][-0.10810386 -0.12353693 -0.1312682 -0.12991315 -0.12231431 -0.11234827 -0.10308901 -0.10022016 -0.10430542 -0.11323642 -0.12452885 -0.1352988 -0.14305469 -0.1444515 -0.14201657][-0.10925941 -0.12092681 -0.12758267 -0.13051634 -0.13092946 -0.13051488 -0.13036445 -0.13304041 -0.13822095 -0.14423127 -0.14955728 -0.15272529 -0.15304072 -0.14925291 -0.14292362]]...]
INFO - root - 2017-12-10 20:54:00.532170: step 52310, loss = 0.71, batch loss = 0.66 (9.8 examples/sec; 0.817 sec/batch; 63h:37m:22s remains)
INFO - root - 2017-12-10 20:54:08.094590: step 52320, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 62h:38m:15s remains)
INFO - root - 2017-12-10 20:54:16.045779: step 52330, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.805 sec/batch; 62h:37m:38s remains)
INFO - root - 2017-12-10 20:54:23.897506: step 52340, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 58h:43m:15s remains)
INFO - root - 2017-12-10 20:54:31.778834: step 52350, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 61h:43m:19s remains)
INFO - root - 2017-12-10 20:54:39.678541: step 52360, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 61h:45m:56s remains)
INFO - root - 2017-12-10 20:54:47.556310: step 52370, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 61h:52m:20s remains)
INFO - root - 2017-12-10 20:54:55.548008: step 52380, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 63h:06m:42s remains)
INFO - root - 2017-12-10 20:55:03.433328: step 52390, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.812 sec/batch; 63h:12m:36s remains)
INFO - root - 2017-12-10 20:55:11.075158: step 52400, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 62h:14m:52s remains)
2017-12-10 20:55:11.873969: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13152909 0.19599763 0.24866854 0.27810103 0.28388122 0.27418581 0.25859967 0.24591656 0.25170907 0.27728805 0.31118226 0.33340311 0.32769802 0.30774489 0.28512827][0.15533887 0.23104146 0.29181308 0.32405537 0.32805568 0.31372368 0.29398653 0.28132373 0.29183581 0.32496339 0.36496761 0.3905229 0.38494477 0.36398977 0.34406942][0.16712129 0.25154075 0.31960028 0.35535702 0.35861194 0.33981454 0.31502503 0.30109453 0.31437191 0.35224238 0.39583552 0.42384169 0.41976014 0.39991084 0.38518929][0.17376593 0.26517797 0.34068745 0.38218239 0.38799044 0.36788249 0.34034577 0.32557705 0.33916926 0.37595332 0.41568923 0.43827835 0.42937568 0.40632585 0.39326328][0.17962019 0.27895945 0.36433649 0.41533273 0.42791316 0.41074476 0.38434473 0.37085396 0.38362807 0.41356912 0.44018656 0.4458068 0.42069998 0.38478512 0.36508173][0.18724631 0.29690233 0.39530575 0.458856 0.48056728 0.46855527 0.4448624 0.43335164 0.44451529 0.46500698 0.47369698 0.45455235 0.40437189 0.34761947 0.3149412][0.19169492 0.31067482 0.42202455 0.49938583 0.53259617 0.52927959 0.510705 0.5015831 0.50998962 0.51984119 0.509962 0.46532291 0.38945541 0.3122488 0.26749593][0.19056934 0.31211764 0.42915019 0.51497084 0.55788481 0.564976 0.55370218 0.547283 0.55142277 0.54981911 0.52361512 0.45967957 0.36720306 0.27951518 0.23064052][0.17642227 0.28866339 0.39745671 0.47902468 0.52325004 0.53723067 0.5332427 0.53049541 0.53186905 0.5223335 0.48659629 0.41491616 0.32075354 0.23740266 0.19509497][0.14260443 0.23510979 0.32337672 0.38860238 0.42445046 0.4381761 0.43708768 0.43602988 0.43563521 0.42334476 0.38732579 0.32168892 0.24131113 0.17555961 0.14866242][0.091645569 0.15828007 0.21957356 0.26190504 0.28310004 0.29026052 0.28688255 0.28412902 0.28186506 0.27068314 0.24175136 0.19187404 0.13501874 0.09438318 0.086814091][0.030511402 0.070707 0.10621848 0.12806873 0.13670962 0.13771869 0.13173839 0.12613118 0.12109847 0.11091202 0.090240404 0.058150738 0.025612134 0.0086477324 0.016912544][-0.023836378 -0.0059680697 0.0089733973 0.015520499 0.015215041 0.011609635 0.0038684227 -0.0039162268 -0.010824267 -0.019303339 -0.031694237 -0.047526356 -0.059636485 -0.058933325 -0.041838359][-0.060377583 -0.057993151 -0.056729015 -0.0597894 -0.065408029 -0.071918406 -0.080103457 -0.087915331 -0.094197527 -0.099485524 -0.10448127 -0.10834083 -0.10748024 -0.097937934 -0.079794116][-0.078448363 -0.083605371 -0.088080622 -0.094250374 -0.10098735 -0.10800172 -0.11559556 -0.12253074 -0.12765235 -0.13073765 -0.13175406 -0.13011032 -0.12469741 -0.1145712 -0.10101279]]...]
INFO - root - 2017-12-10 20:55:19.583550: step 52410, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 59h:53m:11s remains)
INFO - root - 2017-12-10 20:55:27.441365: step 52420, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 61h:29m:54s remains)
INFO - root - 2017-12-10 20:55:35.397839: step 52430, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 60h:50m:25s remains)
INFO - root - 2017-12-10 20:55:43.284194: step 52440, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 62h:48m:28s remains)
INFO - root - 2017-12-10 20:55:51.242752: step 52450, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 62h:32m:41s remains)
INFO - root - 2017-12-10 20:55:59.166549: step 52460, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 62h:59m:45s remains)
INFO - root - 2017-12-10 20:56:07.073412: step 52470, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.817 sec/batch; 63h:32m:01s remains)
INFO - root - 2017-12-10 20:56:14.684637: step 52480, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 62h:19m:05s remains)
INFO - root - 2017-12-10 20:56:22.522540: step 52490, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 61h:29m:45s remains)
INFO - root - 2017-12-10 20:56:30.260468: step 52500, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 61h:26m:53s remains)
2017-12-10 20:56:31.060552: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.018176369 0.012728863 0.0088160941 0.013155606 0.030012129 0.057548892 0.092465751 0.13235801 0.17146763 0.19909748 0.20920351 0.20357786 0.18718368 0.16675547 0.14792481][-0.0028916551 -0.008191864 -0.00444743 0.012759579 0.0450553 0.0863848 0.12894015 0.16827284 0.20122865 0.22203867 0.22834277 0.22529188 0.21803825 0.21093862 0.20524006][-0.020646112 -0.026245851 -0.015236985 0.013814797 0.059553131 0.11165128 0.15739964 0.1906302 0.21104139 0.21853581 0.21520938 0.21078178 0.21147187 0.21888763 0.22940654][-0.023991441 -0.031160586 -0.017081238 0.017845815 0.070514835 0.12769306 0.17258924 0.19775043 0.204622 0.19740982 0.18216571 0.17239137 0.17558186 0.192116 0.21444106][-0.0096756518 -0.018873086 -0.0056773876 0.029056057 0.082406588 0.14015634 0.18262376 0.20068583 0.19593431 0.17573805 0.14915016 0.13177961 0.13191669 0.15018988 0.17698741][0.017607117 0.0069706729 0.017346906 0.048840892 0.10010634 0.15684247 0.19770503 0.21191302 0.20001502 0.17109489 0.13578379 0.1105729 0.10472048 0.11975279 0.14565043][0.046523098 0.036189348 0.043892533 0.071660005 0.12016782 0.17540097 0.21613349 0.23073888 0.21850367 0.18797395 0.14909841 0.11874536 0.10750984 0.11808422 0.13996577][0.066718847 0.0594982 0.067769036 0.095517874 0.144195 0.19965521 0.2422751 0.26115689 0.25414109 0.22702123 0.18806349 0.15578833 0.1422437 0.14973314 0.16629028][0.075244866 0.075387843 0.089743122 0.12359767 0.17697679 0.23483098 0.28026143 0.30379951 0.30188197 0.27678242 0.23684181 0.20403565 0.19141614 0.19867188 0.21107386][0.072810367 0.081861272 0.10529751 0.14864507 0.20858671 0.26899421 0.31554422 0.33977979 0.33733049 0.30879679 0.26559514 0.23298103 0.2235097 0.23265983 0.24289285][0.062021472 0.077377476 0.10748312 0.15766577 0.22131532 0.28121462 0.3239336 0.34155363 0.33087215 0.29411477 0.24743219 0.21684349 0.21228346 0.22445759 0.23402894][0.049830776 0.066585794 0.096513681 0.1455819 0.20551933 0.25852278 0.29061696 0.29445466 0.27032337 0.22438847 0.17660487 0.15037882 0.1516172 0.1674099 0.17829478][0.04576692 0.06063262 0.085087344 0.12615027 0.1754452 0.2154399 0.23158881 0.21761537 0.17844169 0.12489446 0.078254551 0.057196777 0.064019196 0.084220678 0.099326164][0.057182398 0.069445819 0.086641 0.11643285 0.15119995 0.17504369 0.17383707 0.14297913 0.091326743 0.033449009 -0.0098268967 -0.024937345 -0.011568902 0.015692139 0.039301306][0.083844386 0.093468748 0.10273585 0.12050664 0.14075243 0.15014286 0.13581653 0.094508044 0.036820028 -0.020657891 -0.05846883 -0.066653006 -0.045359828 -0.0082904287 0.027197836]]...]
INFO - root - 2017-12-10 20:56:38.981273: step 52510, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.819 sec/batch; 63h:44m:01s remains)
INFO - root - 2017-12-10 20:56:46.895681: step 52520, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 61h:23m:28s remains)
INFO - root - 2017-12-10 20:56:54.727429: step 52530, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 59h:00m:00s remains)
INFO - root - 2017-12-10 20:57:02.773886: step 52540, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.818 sec/batch; 63h:35m:00s remains)
INFO - root - 2017-12-10 20:57:10.714817: step 52550, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 60h:20m:23s remains)
INFO - root - 2017-12-10 20:57:18.339863: step 52560, loss = 0.67, batch loss = 0.61 (10.3 examples/sec; 0.780 sec/batch; 60h:39m:20s remains)
INFO - root - 2017-12-10 20:57:26.220253: step 52570, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 59h:16m:35s remains)
INFO - root - 2017-12-10 20:57:34.151389: step 52580, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 61h:13m:54s remains)
INFO - root - 2017-12-10 20:57:41.940768: step 52590, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 61h:11m:47s remains)
INFO - root - 2017-12-10 20:57:49.901882: step 52600, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 62h:31m:46s remains)
2017-12-10 20:57:50.726962: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.025514871 0.046814058 0.075744025 0.099027909 0.10445841 0.089198522 0.059983674 0.029154047 0.0049498342 -0.0095173065 -0.014209395 -0.011699427 -0.0056198388 -0.0018589707 -0.0043897843][0.028932784 0.052765954 0.084090941 0.10899887 0.11500148 0.099284664 0.068655722 0.034768045 0.0061929589 -0.01212276 -0.018066367 -0.013333533 -0.0028562967 0.0052803862 0.0052994261][0.027286388 0.052500386 0.084943667 0.11069639 0.11786301 0.1040976 0.07546749 0.041820914 0.010668419 -0.010988945 -0.019205596 -0.014214421 -0.0013908654 0.010056182 0.012872322][0.023026701 0.047774773 0.080096193 0.10687691 0.11759137 0.11009255 0.088490129 0.058923803 0.026868924 0.0012950316 -0.01150914 -0.0096700806 0.0017364675 0.01352845 0.017661788][0.018472871 0.040366631 0.070758946 0.098098181 0.11439875 0.1173157 0.10790436 0.086897194 0.05706241 0.0283842 0.0091899112 0.0038504659 0.0089672226 0.01693025 0.020027421][0.0154849 0.033084206 0.059981607 0.08684653 0.1091847 0.12432276 0.12990919 0.12062477 0.096293233 0.066538565 0.04057467 0.025139121 0.019893546 0.020316882 0.020261357][0.016498659 0.029925236 0.05256499 0.077271596 0.10291948 0.1271777 0.14567856 0.14831388 0.13215008 0.10477872 0.074316546 0.048848987 0.031686883 0.022963163 0.0189666][0.020873334 0.032003503 0.050643474 0.071378686 0.095298745 0.12158667 0.14645237 0.15800297 0.15123515 0.1301724 0.099696226 0.067737333 0.040619723 0.023632556 0.016100479][0.025953341 0.036934108 0.052728523 0.068291388 0.085210145 0.10527571 0.12781152 0.14273068 0.14432359 0.13240232 0.10726929 0.075150274 0.043572627 0.021894336 0.012678282][0.02926909 0.04215217 0.057220034 0.0681408 0.075283848 0.083287694 0.095847405 0.10751693 0.11371053 0.11102591 0.095173508 0.069284625 0.039927632 0.018604001 0.010474116][0.030108189 0.045750462 0.061979562 0.070165157 0.068466105 0.063490331 0.062705882 0.066152476 0.072051749 0.07620278 0.071066104 0.055691775 0.033754576 0.016839076 0.011760226][0.028526064 0.046811372 0.064963154 0.072204091 0.064569555 0.049428578 0.036568936 0.030614443 0.032974243 0.040587738 0.044833645 0.040932819 0.028854841 0.018474674 0.017153744][0.025768563 0.045982052 0.06602671 0.073622748 0.0634353 0.042671252 0.022224596 0.0094705792 0.0080953836 0.016785851 0.027426187 0.032839376 0.029855657 0.025844591 0.027618222][0.02348618 0.04479583 0.066057377 0.074521013 0.064526461 0.042708993 0.01994456 0.0043472503 0.00076027587 0.0093863374 0.023387987 0.034735233 0.038328767 0.038901187 0.042105876][0.021071484 0.042578571 0.064114772 0.073529072 0.0656917 0.046266969 0.025101149 0.0097672436 0.0054938686 0.013602809 0.028690396 0.04277787 0.04983208 0.05231991 0.054791462]]...]
INFO - root - 2017-12-10 20:57:58.676025: step 52610, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 62h:55m:21s remains)
INFO - root - 2017-12-10 20:58:06.485859: step 52620, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 59h:17m:57s remains)
INFO - root - 2017-12-10 20:58:14.293847: step 52630, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 61h:11m:14s remains)
INFO - root - 2017-12-10 20:58:22.092434: step 52640, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 62h:51m:35s remains)
INFO - root - 2017-12-10 20:58:29.914071: step 52650, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 60h:50m:18s remains)
INFO - root - 2017-12-10 20:58:37.846997: step 52660, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 60h:29m:52s remains)
INFO - root - 2017-12-10 20:58:45.656732: step 52670, loss = 0.68, batch loss = 0.62 (11.4 examples/sec; 0.705 sec/batch; 54h:46m:44s remains)
INFO - root - 2017-12-10 20:58:53.382301: step 52680, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 58h:54m:25s remains)
INFO - root - 2017-12-10 20:59:01.203791: step 52690, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 59h:55m:19s remains)
INFO - root - 2017-12-10 20:59:09.021256: step 52700, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 60h:33m:49s remains)
2017-12-10 20:59:09.958636: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.077623762 0.10091162 0.11476135 0.11078261 0.088614143 0.056184474 0.02500144 0.0013890553 -0.016849758 -0.0261081 -0.019776417 0.0026850454 0.031637732 0.045603506 0.039225876][0.051447481 0.069995791 0.085161753 0.089033559 0.081222691 0.065595351 0.047036275 0.02643718 0.0025216776 -0.018257042 -0.027352484 -0.022045953 -0.0080471914 0.00019192888 -0.00287138][0.021834593 0.037200414 0.058226719 0.077459715 0.093075372 0.10276805 0.10176456 0.084707126 0.052069467 0.015500247 -0.014047827 -0.03121266 -0.036875635 -0.037881821 -0.039462686][0.0043497947 0.020816598 0.052225314 0.091714814 0.13500406 0.17219961 0.18864393 0.17313075 0.12906086 0.074805193 0.023682345 -0.016880898 -0.043318786 -0.055885702 -0.058866534][0.0048051071 0.026896736 0.071081959 0.13156223 0.20071876 0.2612263 0.29052034 0.2731652 0.21618171 0.14510548 0.075183824 0.01488959 -0.029099083 -0.052762892 -0.058798354][0.022147065 0.05608958 0.11665957 0.19835196 0.28906882 0.36506903 0.39901686 0.37422177 0.30186248 0.21476583 0.13033852 0.056764062 0.0015133477 -0.0306494 -0.040085513][0.048551563 0.098543823 0.17631766 0.275445 0.38025203 0.4618414 0.49101558 0.45411918 0.3667812 0.26759559 0.1751214 0.096026033 0.036403086 -0.00098020944 -0.013439423][0.074564621 0.14029233 0.23143665 0.33939978 0.44701883 0.52320194 0.5400424 0.48987648 0.39252949 0.28886226 0.19649105 0.11897402 0.059901025 0.01968696 0.0036602938][0.090173617 0.16782489 0.26546156 0.37103903 0.46897116 0.53004187 0.53077114 0.47086722 0.37317324 0.27585804 0.19244136 0.1223348 0.066893242 0.025228623 0.0047609713][0.085646749 0.16754356 0.26140049 0.3519879 0.4287526 0.46887109 0.45497262 0.3931427 0.30660146 0.22618297 0.15959963 0.10219883 0.054255135 0.014738823 -0.0081439214][0.058717605 0.13390318 0.21259384 0.27852234 0.32823458 0.34775934 0.32577571 0.27137068 0.20475736 0.14732774 0.10145174 0.060256228 0.0240626 -0.0076651461 -0.027832245][0.019712923 0.076902084 0.13140318 0.1686084 0.19167115 0.19513151 0.1725022 0.13216019 0.088892736 0.055004794 0.029481454 0.0056674923 -0.015825992 -0.035141751 -0.047764342][-0.016124474 0.018819634 0.047795668 0.059765689 0.061972007 0.055201828 0.036450356 0.011408946 -0.011174661 -0.025960872 -0.034961674 -0.043184239 -0.05037079 -0.056801997 -0.060795736][-0.03850643 -0.02281413 -0.013244919 -0.017114976 -0.026120404 -0.036627695 -0.049824633 -0.062803119 -0.071329534 -0.074049465 -0.072664358 -0.070203111 -0.066942811 -0.063974231 -0.061852619][-0.045830175 -0.043370713 -0.045299079 -0.055725191 -0.067655109 -0.077347688 -0.0855168 -0.09117841 -0.092663832 -0.090272717 -0.08482416 -0.0776621 -0.069291413 -0.061575126 -0.056464817]]...]
INFO - root - 2017-12-10 20:59:17.754638: step 52710, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.754 sec/batch; 58h:36m:34s remains)
INFO - root - 2017-12-10 20:59:25.384499: step 52720, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 60h:08m:12s remains)
INFO - root - 2017-12-10 20:59:33.219280: step 52730, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 59h:09m:07s remains)
INFO - root - 2017-12-10 20:59:41.130484: step 52740, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 62h:47m:17s remains)
INFO - root - 2017-12-10 20:59:49.070790: step 52750, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 62h:14m:46s remains)
INFO - root - 2017-12-10 20:59:56.790092: step 52760, loss = 0.70, batch loss = 0.64 (12.5 examples/sec; 0.640 sec/batch; 49h:42m:21s remains)
INFO - root - 2017-12-10 21:00:04.676498: step 52770, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 61h:49m:17s remains)
INFO - root - 2017-12-10 21:00:12.624333: step 52780, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 62h:15m:04s remains)
INFO - root - 2017-12-10 21:00:20.532869: step 52790, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 62h:19m:29s remains)
INFO - root - 2017-12-10 21:00:28.244640: step 52800, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 62h:12m:30s remains)
2017-12-10 21:00:29.099673: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036560487 0.026589762 0.02022659 0.017744193 0.017663695 0.017303405 0.016274637 0.014412404 0.01329185 0.014548894 0.016845543 0.019111911 0.020778174 0.021621808 0.020430293][0.036659978 0.029619047 0.025806297 0.024675971 0.024273017 0.022369567 0.01923205 0.014991381 0.011123158 0.009715911 0.010140517 0.011391077 0.013174916 0.014698016 0.014491467][0.035070803 0.030710896 0.028694063 0.028239995 0.027413407 0.024836643 0.020921724 0.016046753 0.0112511 0.0083614122 0.0074318624 0.0079142544 0.0093842736 0.010762841 0.010592554][0.049465876 0.045275889 0.04183102 0.040021289 0.038709857 0.037467957 0.035478991 0.032149322 0.028020479 0.024375167 0.021519935 0.019824686 0.019918391 0.021012051 0.020703172][0.088386767 0.081718825 0.072989747 0.0671993 0.06525702 0.067728944 0.071342275 0.072420247 0.070609652 0.066566117 0.06090273 0.055430811 0.054065034 0.056909405 0.058331035][0.14454007 0.1330846 0.11694069 0.10524654 0.10204612 0.10955072 0.12164177 0.1297833 0.1321384 0.12875998 0.12001137 0.11045342 0.10917681 0.11743876 0.12394531][0.1975241 0.18347107 0.16135496 0.14367157 0.13812695 0.14940441 0.16955644 0.18498042 0.19186261 0.18987963 0.17916901 0.16701072 0.16809885 0.18499134 0.20012569][0.22658306 0.21472819 0.19127473 0.16949919 0.15974382 0.1699073 0.19303653 0.21234255 0.22198273 0.22133583 0.21056284 0.19903678 0.20527864 0.23262699 0.25891697][0.22498746 0.21933424 0.19946735 0.17694548 0.1621889 0.16506572 0.18218237 0.19768266 0.20519346 0.20407851 0.19458374 0.1865938 0.19938341 0.23609741 0.27291188][0.20556511 0.20643979 0.19281743 0.17236237 0.1530865 0.14486609 0.14835843 0.15188646 0.15096046 0.14603598 0.1374256 0.1335208 0.15102167 0.19207285 0.23439036][0.18319972 0.18875301 0.1821162 0.16617328 0.14499357 0.12666953 0.11468572 0.10248232 0.088329673 0.075167954 0.064184584 0.061476458 0.078783818 0.116622 0.15658621][0.16808081 0.17511946 0.1747652 0.16579832 0.14759628 0.12467192 0.10143476 0.075996719 0.048587888 0.024449388 0.0062821889 -0.00091317564 0.0099899676 0.037747543 0.06818524][0.15983056 0.16621341 0.17046656 0.17013305 0.16019648 0.14015806 0.11351484 0.082040332 0.046646625 0.013273533 -0.014349 -0.03095202 -0.031691562 -0.018336598 -0.0015198326][0.15507065 0.15986817 0.16658714 0.17475276 0.17624915 0.16522419 0.14309654 0.11391489 0.078037918 0.04021731 0.0042254296 -0.023663662 -0.038688384 -0.041240312 -0.038652763][0.15460227 0.15734915 0.16376278 0.17771547 0.19025508 0.19049433 0.17746986 0.15617795 0.12605242 0.08900138 0.047854006 0.01000953 -0.018776052 -0.036199886 -0.046022113]]...]
INFO - root - 2017-12-10 21:00:36.911467: step 52810, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 62h:04m:34s remains)
INFO - root - 2017-12-10 21:00:44.874993: step 52820, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 59h:38m:56s remains)
INFO - root - 2017-12-10 21:00:52.727166: step 52830, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 60h:49m:02s remains)
INFO - root - 2017-12-10 21:01:00.669261: step 52840, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 60h:32m:16s remains)
INFO - root - 2017-12-10 21:01:08.535664: step 52850, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 61h:50m:20s remains)
INFO - root - 2017-12-10 21:01:16.439096: step 52860, loss = 0.66, batch loss = 0.60 (10.1 examples/sec; 0.792 sec/batch; 61h:31m:39s remains)
INFO - root - 2017-12-10 21:01:24.213883: step 52870, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 61h:21m:41s remains)
INFO - root - 2017-12-10 21:01:31.993073: step 52880, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.821 sec/batch; 63h:45m:06s remains)
INFO - root - 2017-12-10 21:01:39.881534: step 52890, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 60h:38m:34s remains)
INFO - root - 2017-12-10 21:01:47.741710: step 52900, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 60h:27m:07s remains)
2017-12-10 21:01:48.570770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.039227348 -0.030962393 -0.016077077 0.0044527813 0.027668077 0.046340298 0.054733213 0.052673608 0.043228272 0.031333011 0.023179192 0.021550773 0.022603273 0.023238942 0.024678554][-0.037909828 -0.024727611 -0.0015754586 0.030695893 0.067783974 0.099667363 0.11736983 0.1184511 0.10617373 0.087415263 0.071020126 0.061662897 0.05636894 0.052282669 0.050502658][-0.035284888 -0.017603941 0.012734847 0.055373184 0.10506111 0.14948575 0.17688783 0.18244502 0.16922043 0.1452648 0.12174878 0.10498105 0.092379235 0.081574939 0.073724851][-0.033283345 -0.012338139 0.022474272 0.071411081 0.128353 0.17994517 0.21298911 0.22131474 0.20821622 0.18187599 0.1546056 0.13321806 0.11535707 0.099196889 0.085802555][-0.031508911 -0.0079976162 0.029809568 0.082360245 0.14272648 0.19742097 0.23297901 0.24266157 0.23037964 0.20373322 0.17491293 0.15009598 0.12790301 0.10810149 0.091396883][-0.029508753 -0.0032383881 0.038650595 0.0968141 0.16338111 0.22434254 0.26591113 0.279953 0.26994538 0.24209361 0.20881948 0.177058 0.14707233 0.12066664 0.0990058][-0.028389951 -0.00039964297 0.045031141 0.10889575 0.18287438 0.25268093 0.30348012 0.32476941 0.31806618 0.28821045 0.24850817 0.20729199 0.16719995 0.13222335 0.10426369][-0.029008493 -0.0014446031 0.044354375 0.10981333 0.18709952 0.26242879 0.32007772 0.34785146 0.34440088 0.31357071 0.27029681 0.22386791 0.1774506 0.13619441 0.10393596][-0.029912645 -0.0042501413 0.039078888 0.10139622 0.17580694 0.24969144 0.30746982 0.33701205 0.33518875 0.30603102 0.26494515 0.22030766 0.17391026 0.13094366 0.09783341][-0.030270899 -0.0064766924 0.03423791 0.09280479 0.16266276 0.2320659 0.28595126 0.31365034 0.31133151 0.28378388 0.24661164 0.20657112 0.16328928 0.12128624 0.089271478][-0.030867521 -0.00881477 0.029841516 0.08548639 0.15110226 0.2151709 0.26292285 0.28568965 0.279425 0.24996842 0.21357441 0.17673439 0.13815595 0.10034549 0.072166443][-0.032999314 -0.013948582 0.020720312 0.070552237 0.12826173 0.18288462 0.22053987 0.23528941 0.2233742 0.19126472 0.15537882 0.1226901 0.092307359 0.064276293 0.045061208][-0.036205843 -0.021230409 0.0070549529 0.047172107 0.092544913 0.13349405 0.15827779 0.16432783 0.1487882 0.11753583 0.085282214 0.059169907 0.038912989 0.022780161 0.014334465][-0.038648713 -0.027409256 -0.0055730045 0.024309035 0.05674836 0.083487444 0.095253743 0.093012244 0.076055333 0.049596574 0.024604244 0.0066132518 -0.0045790505 -0.011386731 -0.011963246][-0.040184792 -0.031429019 -0.014371782 0.008242202 0.031781379 0.048854113 0.05144069 0.042840272 0.025295576 0.0035296697 -0.015143123 -0.027406514 -0.033154666 -0.034629125 -0.030641776]]...]
INFO - root - 2017-12-10 21:01:56.465674: step 52910, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 60h:09m:28s remains)
INFO - root - 2017-12-10 21:02:04.318791: step 52920, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.837 sec/batch; 65h:00m:45s remains)
INFO - root - 2017-12-10 21:02:12.370023: step 52930, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 62h:44m:46s remains)
INFO - root - 2017-12-10 21:02:20.055505: step 52940, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 61h:04m:42s remains)
INFO - root - 2017-12-10 21:02:27.990322: step 52950, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 59h:58m:31s remains)
INFO - root - 2017-12-10 21:02:35.642786: step 52960, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 60h:22m:41s remains)
INFO - root - 2017-12-10 21:02:43.488316: step 52970, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 61h:21m:50s remains)
INFO - root - 2017-12-10 21:02:51.438182: step 52980, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.820 sec/batch; 63h:41m:50s remains)
INFO - root - 2017-12-10 21:02:59.280316: step 52990, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 62h:00m:13s remains)
INFO - root - 2017-12-10 21:03:07.247347: step 53000, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.818 sec/batch; 63h:30m:01s remains)
2017-12-10 21:03:08.113314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033301222 -0.0055954936 -0.01009145 -0.014610212 -0.017232461 -0.016534185 -0.012179821 -0.0051830714 0.0021371217 0.0083297845 0.012302075 0.01142031 0.0035044244 -0.0096721388 -0.023340061][0.020207101 0.017462341 0.0091317669 7.0927621e-05 -0.0057643978 -0.0056816232 0.0015015929 0.015136516 0.030959433 0.04498582 0.054515056 0.055010028 0.04297094 0.020515149 -0.0043865326][0.050338767 0.049099889 0.038233776 0.024738297 0.01482544 0.013068699 0.022608887 0.043770891 0.070649311 0.095572166 0.11316212 0.11616611 0.098675266 0.0636874 0.023598149][0.082774192 0.085072815 0.073247634 0.055617448 0.040894955 0.036583789 0.048240714 0.077035181 0.11574496 0.15280451 0.17985924 0.1859449 0.16200861 0.11227988 0.054756034][0.11751647 0.12452742 0.11262694 0.090765126 0.070991196 0.064454131 0.079216629 0.1161413 0.16618405 0.21451576 0.24983759 0.25769511 0.22591823 0.1602256 0.084705986][0.1587356 0.17281079 0.16257887 0.13679859 0.11107551 0.10094233 0.11712294 0.15948644 0.21678367 0.2722238 0.31227612 0.31986699 0.2803348 0.20033607 0.10918641][0.19112308 0.2138373 0.20878065 0.18299976 0.15401122 0.14068827 0.15646531 0.19989549 0.25778681 0.31325752 0.35221437 0.35669875 0.31026742 0.22088489 0.12082826][0.19830024 0.23048991 0.23539189 0.21686748 0.19111329 0.17821339 0.19339754 0.23379003 0.28525254 0.33297211 0.36435157 0.36205319 0.3092179 0.21617053 0.11545549][0.18448798 0.22195806 0.2351974 0.22522449 0.20593789 0.19652787 0.21138188 0.24625134 0.28711164 0.32228547 0.34246123 0.3325389 0.27700925 0.18771745 0.095176995][0.16353382 0.1998487 0.21527235 0.20908254 0.19342338 0.18539655 0.19729811 0.22430208 0.25325936 0.27547732 0.28495347 0.26996657 0.21755148 0.13974105 0.062880404][0.1377259 0.16868338 0.18253574 0.17689973 0.16152023 0.1510374 0.15643933 0.1733242 0.18994808 0.2000872 0.2007772 0.1841014 0.13971198 0.078632787 0.021833623][0.096227117 0.11980639 0.13178581 0.12791522 0.11463622 0.10337465 0.10401248 0.11254951 0.11886073 0.11896644 0.11267397 0.09595596 0.060972735 0.017438328 -0.019196887][0.049214192 0.065656327 0.076345563 0.075985678 0.067295671 0.058375172 0.057127573 0.06029151 0.059834175 0.053596262 0.043527953 0.028386608 0.002770609 -0.025688825 -0.046337549][0.016060092 0.025325388 0.032142833 0.032055244 0.025634153 0.018068863 0.014754071 0.013756292 0.0098439567 0.0019298218 -0.0074519687 -0.018266367 -0.033860937 -0.049438324 -0.058293108][-0.0038891623 -0.00168639 -0.0010058918 -0.0047205794 -0.012452357 -0.021040253 -0.027128214 -0.031068612 -0.035472598 -0.041203789 -0.04673326 -0.051866852 -0.058287516 -0.063573785 -0.06415499]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 21:03:15.970476: step 53010, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 61h:12m:08s remains)
INFO - root - 2017-12-10 21:03:23.830036: step 53020, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 60h:04m:14s remains)
INFO - root - 2017-12-10 21:03:31.570205: step 53030, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 59h:47m:54s remains)
INFO - root - 2017-12-10 21:03:39.232691: step 53040, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 59h:45m:37s remains)
INFO - root - 2017-12-10 21:03:47.105137: step 53050, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 59h:36m:47s remains)
INFO - root - 2017-12-10 21:03:54.889593: step 53060, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 62h:02m:14s remains)
INFO - root - 2017-12-10 21:04:02.821189: step 53070, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 61h:25m:49s remains)
INFO - root - 2017-12-10 21:04:10.998243: step 53080, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 60h:42m:36s remains)
INFO - root - 2017-12-10 21:04:18.842454: step 53090, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 60h:00m:12s remains)
INFO - root - 2017-12-10 21:04:26.727059: step 53100, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 63h:13m:43s remains)
2017-12-10 21:04:27.663265: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31544703 0.21911168 0.12022705 0.04696925 0.00018237115 -0.029070284 -0.050914783 -0.056909423 -0.043870833 -0.024167852 -0.0089128865 -0.0083295675 -0.028398348 -0.065143973 -0.1033425][0.23504426 0.14072625 0.054761615 0.0020702581 -0.019300465 -0.023012349 -0.025611982 -0.027710363 -0.02899139 -0.036011439 -0.0510506 -0.075520784 -0.10708641 -0.13933596 -0.16256206][0.20099439 0.12046143 0.053650536 0.021454347 0.02108782 0.038009621 0.052858248 0.058209453 0.050199922 0.025589952 -0.012612202 -0.059675213 -0.10674544 -0.1433505 -0.16219884][0.22783476 0.16466318 0.11572808 0.099529788 0.11387581 0.14551337 0.17644322 0.19504398 0.19050181 0.15831745 0.1033999 0.03480646 -0.033410758 -0.085792504 -0.11268707][0.30529621 0.2552267 0.22095364 0.22006401 0.24768129 0.290742 0.33576486 0.37004232 0.37413648 0.33910686 0.26920551 0.17694372 0.08127825 0.0033017886 -0.041297443][0.41017863 0.36985037 0.35146335 0.37127766 0.41482291 0.46603432 0.51801759 0.56092161 0.56888723 0.52837259 0.44249 0.32589754 0.20137617 0.094633006 0.0289182][0.50707072 0.4778586 0.47950798 0.52471244 0.58481407 0.63909489 0.68938684 0.7308327 0.73436892 0.68305159 0.58036524 0.44224533 0.293936 0.16347244 0.08010488][0.58046633 0.56428164 0.58525431 0.65061647 0.71931934 0.76708716 0.805108 0.83430207 0.82647049 0.76214361 0.64603812 0.4947927 0.33510765 0.19445807 0.10372481][0.64624661 0.63851953 0.66572708 0.73216629 0.79110354 0.81800514 0.83170611 0.83922511 0.81649774 0.7430656 0.62407517 0.4742941 0.31935334 0.18429095 0.097896338][0.69453233 0.68758953 0.70393652 0.74956852 0.78073114 0.77617997 0.7587139 0.74026835 0.70339775 0.628781 0.52078056 0.38941249 0.25590834 0.14086884 0.0684564][0.69929039 0.68224037 0.67444211 0.68484133 0.68015617 0.64422143 0.59904337 0.558876 0.51361269 0.44714668 0.36257347 0.26406711 0.16550334 0.081263371 0.029369753][0.64320743 0.6060338 0.56711429 0.5388273 0.50209224 0.44575086 0.3864361 0.33734608 0.29417074 0.24511595 0.19117512 0.13136949 0.0719966 0.021147264 -0.0096898507][0.53157932 0.47141275 0.40518123 0.34756538 0.29151481 0.22994398 0.17265901 0.12961002 0.099054612 0.072500333 0.048634164 0.023271218 -0.0028040926 -0.026276384 -0.040874314][0.38844743 0.31154469 0.23130107 0.16236827 0.10374209 0.050362892 0.0064558261 -0.0211465 -0.033912025 -0.03882686 -0.039400287 -0.041158188 -0.045902424 -0.053029496 -0.059100863][0.22987907 0.14847083 0.072075531 0.011886826 -0.033044282 -0.068230428 -0.093608707 -0.10431662 -0.10161973 -0.09147732 -0.07867223 -0.068596251 -0.063573755 -0.064022742 -0.067213222]]...]
INFO - root - 2017-12-10 21:04:35.620258: step 53110, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 61h:41m:06s remains)
INFO - root - 2017-12-10 21:04:43.189099: step 53120, loss = 0.67, batch loss = 0.61 (9.9 examples/sec; 0.809 sec/batch; 62h:49m:08s remains)
INFO - root - 2017-12-10 21:04:51.196766: step 53130, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 60h:25m:25s remains)
INFO - root - 2017-12-10 21:04:59.046383: step 53140, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 60h:28m:00s remains)
INFO - root - 2017-12-10 21:05:06.826609: step 53150, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 60h:21m:27s remains)
INFO - root - 2017-12-10 21:05:14.788427: step 53160, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 62h:55m:22s remains)
INFO - root - 2017-12-10 21:05:22.691689: step 53170, loss = 0.67, batch loss = 0.62 (10.4 examples/sec; 0.769 sec/batch; 59h:41m:02s remains)
INFO - root - 2017-12-10 21:05:30.499805: step 53180, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 59h:41m:53s remains)
INFO - root - 2017-12-10 21:05:38.316474: step 53190, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.786 sec/batch; 61h:01m:05s remains)
INFO - root - 2017-12-10 21:05:46.096441: step 53200, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 62h:20m:00s remains)
2017-12-10 21:05:46.928517: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14414449 0.15411729 0.15991688 0.16451694 0.18040363 0.20106477 0.21782713 0.23236158 0.24419035 0.2519016 0.23648059 0.19579367 0.14891364 0.095743611 0.036937814][0.28758162 0.30392891 0.29362929 0.26313281 0.2326389 0.20787987 0.19117106 0.18553135 0.1876753 0.19143565 0.17666478 0.14003763 0.099903569 0.056040928 0.0081193009][0.42618328 0.44644108 0.41949007 0.35368273 0.27591553 0.20593689 0.15689915 0.13092536 0.12082553 0.11723521 0.10125975 0.070163377 0.039637759 0.0078984834 -0.02539061][0.50240558 0.52156925 0.48435059 0.39880702 0.29477847 0.20217627 0.14056672 0.10810649 0.091990374 0.080486178 0.058969 0.028865151 0.0022779314 -0.023612214 -0.047489505][0.48045975 0.49407151 0.45873427 0.38043839 0.28651166 0.20868947 0.16611069 0.150029 0.13950568 0.12061185 0.085402451 0.0446573 0.0093927085 -0.022736665 -0.047693986][0.37154022 0.37938604 0.36045834 0.31738225 0.26911944 0.23972343 0.24240851 0.26041242 0.26287258 0.23451085 0.1764393 0.11245485 0.056181408 0.0078213811 -0.02699022][0.23271656 0.24056028 0.24728876 0.25343546 0.2657679 0.295482 0.34847873 0.40048322 0.41234255 0.3702527 0.28456184 0.19243304 0.11304275 0.047880847 0.0016706086][0.1303592 0.14477731 0.17572774 0.22337326 0.28297791 0.3541553 0.43693841 0.50304025 0.51170182 0.4526841 0.34424868 0.23262084 0.14043894 0.068929471 0.019065194][0.099983849 0.12484661 0.1685427 0.23280439 0.30686066 0.38482603 0.46394679 0.51780117 0.51132685 0.43870345 0.32213473 0.2092033 0.12225847 0.059446674 0.016608117][0.13383454 0.16619274 0.20704195 0.259995 0.31550631 0.36957076 0.41985583 0.44524971 0.42114085 0.34572816 0.23958975 0.1438299 0.0766778 0.031944156 0.0021716156][0.18969758 0.22152044 0.24779069 0.27376896 0.295423 0.31444973 0.33086455 0.32988086 0.29746774 0.2332291 0.15188785 0.083433248 0.039387655 0.011366032 -0.0086964956][0.2272296 0.25129461 0.26181045 0.26442656 0.25955519 0.2534261 0.24996915 0.23959966 0.21444438 0.17165053 0.11818849 0.073260576 0.042471416 0.018548284 -0.0035935289][0.23423591 0.25015655 0.25408563 0.24986194 0.2379781 0.22567794 0.22252829 0.22201425 0.21663976 0.19744523 0.1638574 0.12805632 0.092788316 0.055597279 0.017542474][0.22240525 0.23598176 0.24450691 0.24939124 0.24716502 0.24259502 0.25216061 0.27175704 0.29024622 0.29014388 0.26427075 0.22231971 0.16776389 0.10541388 0.043856371][0.21137913 0.22944337 0.24747263 0.26575121 0.27670959 0.2814692 0.30310592 0.34061971 0.37861726 0.39070997 0.36427847 0.30916876 0.23189402 0.14417271 0.061269935]]...]
INFO - root - 2017-12-10 21:05:54.654616: step 53210, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 60h:38m:59s remains)
INFO - root - 2017-12-10 21:06:02.503282: step 53220, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 61h:55m:19s remains)
INFO - root - 2017-12-10 21:06:10.401718: step 53230, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 60h:04m:05s remains)
INFO - root - 2017-12-10 21:06:18.277394: step 53240, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 61h:57m:42s remains)
INFO - root - 2017-12-10 21:06:26.230094: step 53250, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 59h:46m:23s remains)
INFO - root - 2017-12-10 21:06:34.139652: step 53260, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 60h:27m:37s remains)
INFO - root - 2017-12-10 21:06:42.042456: step 53270, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 62h:24m:13s remains)
INFO - root - 2017-12-10 21:06:49.793592: step 53280, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 60h:01m:01s remains)
INFO - root - 2017-12-10 21:06:57.544320: step 53290, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 60h:41m:21s remains)
INFO - root - 2017-12-10 21:07:05.230536: step 53300, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 60h:26m:38s remains)
2017-12-10 21:07:06.033895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029819692 -0.023995422 -0.0161895 -0.0085541122 -0.0016494447 0.0019220806 -0.0010194984 -0.0072453897 -0.012192931 -0.016042072 -0.019455958 -0.019415723 -0.015072394 -0.010364781 -0.0063313907][0.035317436 0.045269471 0.056855511 0.067745835 0.077462144 0.082729936 0.077712633 0.06619104 0.056903627 0.051100239 0.047371324 0.050438836 0.060328688 0.069409043 0.075627826][0.1399491 0.15654281 0.17233245 0.18569289 0.19448714 0.19620228 0.18431884 0.16432421 0.15019996 0.14414616 0.1438047 0.15487593 0.17492494 0.18991747 0.19678965][0.26880741 0.2935428 0.31101564 0.32264748 0.32384497 0.31446379 0.29118407 0.262852 0.24752952 0.24594703 0.25396904 0.27753592 0.30954981 0.32857656 0.33211929][0.39319959 0.42667744 0.44204158 0.44692543 0.43568692 0.41184351 0.37800783 0.34682012 0.3375715 0.3459408 0.36536634 0.4012709 0.44137838 0.45829347 0.45320249][0.48267117 0.52380258 0.5339964 0.52939415 0.50599051 0.47069407 0.43230125 0.40708846 0.41209233 0.43486691 0.46555653 0.508985 0.55050373 0.56043845 0.54557681][0.51994926 0.56439996 0.56870878 0.55711842 0.52813858 0.49011517 0.45547175 0.4425821 0.4634673 0.49724936 0.53186709 0.57331824 0.609694 0.6132828 0.59468621][0.50643224 0.54784214 0.54821008 0.53676128 0.51374829 0.48519552 0.46286112 0.46284026 0.4908005 0.52230006 0.54720062 0.57567787 0.603052 0.60567939 0.59302449][0.46445271 0.49698514 0.49601623 0.49102572 0.48160321 0.46910086 0.46141377 0.46808347 0.48934034 0.50447732 0.50994372 0.52155274 0.54171646 0.549188 0.54751456][0.41833445 0.43569565 0.43250808 0.43534753 0.43980616 0.44137868 0.44392741 0.44977427 0.45600283 0.45007536 0.43639663 0.43505293 0.45239517 0.46725646 0.47541922][0.37287307 0.36984846 0.36058855 0.36786854 0.38109994 0.38961545 0.39574668 0.39802021 0.39231387 0.37302741 0.34949711 0.34318608 0.36058304 0.38158321 0.39606759][0.31499958 0.29307264 0.27643713 0.28453943 0.30106497 0.3115119 0.31994313 0.32426146 0.31902668 0.3003082 0.27792817 0.27160206 0.28651634 0.30781692 0.32495373][0.23674004 0.20240782 0.18112776 0.18753244 0.20330645 0.21577039 0.23304552 0.25267273 0.26475853 0.26102993 0.24835719 0.24317308 0.25051948 0.26574397 0.28320557][0.15884745 0.12383896 0.10350545 0.10725899 0.11919803 0.13435505 0.16618964 0.21047211 0.24973378 0.26771322 0.26739156 0.26273793 0.25997207 0.26583907 0.28126642][0.11244902 0.088636816 0.076001815 0.0789688 0.0865187 0.10399602 0.15058921 0.21923964 0.28337264 0.31881338 0.32588524 0.3182945 0.30375898 0.29696265 0.30443045]]...]
INFO - root - 2017-12-10 21:07:13.913517: step 53310, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 60h:45m:00s remains)
INFO - root - 2017-12-10 21:07:21.837136: step 53320, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.796 sec/batch; 61h:41m:35s remains)
INFO - root - 2017-12-10 21:07:29.639216: step 53330, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 59h:13m:29s remains)
INFO - root - 2017-12-10 21:07:37.414287: step 53340, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 58h:55m:56s remains)
INFO - root - 2017-12-10 21:07:45.297030: step 53350, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 60h:28m:56s remains)
INFO - root - 2017-12-10 21:07:53.049319: step 53360, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 60h:55m:57s remains)
INFO - root - 2017-12-10 21:08:00.878890: step 53370, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 61h:01m:07s remains)
INFO - root - 2017-12-10 21:08:08.751299: step 53380, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 60h:29m:51s remains)
INFO - root - 2017-12-10 21:08:16.429487: step 53390, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 59h:18m:38s remains)
INFO - root - 2017-12-10 21:08:24.369440: step 53400, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 60h:34m:11s remains)
2017-12-10 21:08:25.213780: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.065120682 0.10976284 0.14617792 0.16168931 0.15490113 0.13471229 0.10728456 0.078803122 0.056093618 0.045427844 0.047138017 0.059038792 0.078562789 0.10149708 0.12114181][0.060370632 0.10886621 0.15306391 0.17896876 0.18229592 0.16974601 0.14577 0.11606322 0.088261023 0.0704904 0.065902866 0.073556192 0.091061361 0.11212222 0.12933597][0.042518161 0.093774326 0.14668961 0.18751244 0.20851578 0.21255112 0.20058438 0.17624976 0.14695272 0.12168171 0.10860415 0.10866092 0.11935044 0.13277756 0.14124119][0.026233181 0.082830377 0.14741631 0.20627932 0.24885622 0.27445498 0.27993366 0.26628813 0.24001619 0.21050654 0.19130494 0.18436912 0.18698545 0.18920396 0.18353821][0.025448838 0.09384235 0.17554814 0.25514242 0.32086068 0.36986378 0.39511159 0.39517513 0.37563136 0.34640858 0.325198 0.31323811 0.30729949 0.29554817 0.27098793][0.043765858 0.13162363 0.23587157 0.33754578 0.42509919 0.49543458 0.53857815 0.55135661 0.53931695 0.5136233 0.49360365 0.47813296 0.46305209 0.43532979 0.38909066][0.075561419 0.18788694 0.31789392 0.44150865 0.54692531 0.63130414 0.68394154 0.70189983 0.69268245 0.668501 0.64881325 0.628953 0.60318995 0.55914313 0.49304888][0.10996987 0.24497567 0.397651 0.53753859 0.65139794 0.73690259 0.78495449 0.79630435 0.7826128 0.75681096 0.73600274 0.71078032 0.67486453 0.6180442 0.53919834][0.12970559 0.27583507 0.43752071 0.57960385 0.68666959 0.75717086 0.78596282 0.7803039 0.75715959 0.72950637 0.7096287 0.6832096 0.64384335 0.5839805 0.50434929][0.11620806 0.25334531 0.40299454 0.52935022 0.61625248 0.66264725 0.66795492 0.64551091 0.61592311 0.59086 0.57620853 0.55434787 0.51844341 0.46399403 0.39318147][0.062255487 0.16977413 0.28782395 0.38485715 0.44561225 0.46862814 0.456309 0.42463902 0.39424595 0.37447804 0.3665207 0.35257906 0.32544959 0.28283358 0.22765101][-0.010721776 0.057260253 0.13489974 0.19751753 0.23228316 0.23701732 0.21481931 0.18059516 0.15228178 0.13687538 0.13346176 0.12696733 0.11052604 0.082800493 0.046769273][-0.071850732 -0.040166341 -1.5821457e-05 0.031187063 0.044168279 0.036712579 0.01171282 -0.019062404 -0.042111434 -0.052617516 -0.052177988 -0.051948298 -0.057835307 -0.071076378 -0.089430079][-0.10868825 -0.1030633 -0.090233423 -0.081437737 -0.082213357 -0.094239742 -0.11523887 -0.13744125 -0.15239942 -0.15744422 -0.15400572 -0.14917935 -0.14752387 -0.15026209 -0.15618195][-0.12349571 -0.13264808 -0.13526234 -0.13833442 -0.14441282 -0.15467253 -0.1679032 -0.1801825 -0.18737993 -0.18851325 -0.18451306 -0.17908433 -0.17510933 -0.17334443 -0.17312065]]...]
INFO - root - 2017-12-10 21:08:33.059407: step 53410, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 61h:27m:10s remains)
INFO - root - 2017-12-10 21:08:40.991842: step 53420, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.818 sec/batch; 63h:23m:25s remains)
INFO - root - 2017-12-10 21:08:48.839479: step 53430, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 60h:25m:14s remains)
INFO - root - 2017-12-10 21:08:56.489944: step 53440, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 60h:25m:34s remains)
INFO - root - 2017-12-10 21:09:04.320708: step 53450, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 61h:49m:15s remains)
INFO - root - 2017-12-10 21:09:12.146071: step 53460, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 59h:50m:22s remains)
INFO - root - 2017-12-10 21:09:19.840663: step 53470, loss = 0.69, batch loss = 0.63 (13.2 examples/sec; 0.608 sec/batch; 47h:07m:43s remains)
INFO - root - 2017-12-10 21:09:27.738843: step 53480, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 61h:32m:49s remains)
INFO - root - 2017-12-10 21:09:35.641780: step 53490, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 59h:46m:31s remains)
INFO - root - 2017-12-10 21:09:43.442053: step 53500, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 59h:02m:58s remains)
2017-12-10 21:09:44.312543: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10326302 0.11211351 0.11901951 0.11601698 0.10366457 0.093486182 0.094309665 0.10419913 0.11669067 0.12524876 0.13091898 0.13739459 0.14714903 0.16360186 0.18855402][0.1015514 0.11395735 0.12111391 0.11605583 0.10085596 0.089208946 0.090326041 0.10143837 0.11428833 0.12075444 0.12080668 0.12114882 0.12942114 0.15235116 0.18931791][0.11292636 0.13068527 0.14026321 0.13568842 0.12049856 0.1094912 0.11084272 0.12078013 0.13007346 0.1299704 0.11998513 0.10980857 0.11320674 0.14012446 0.18727468][0.13614313 0.15694496 0.16817941 0.16573331 0.154248 0.14837345 0.15393287 0.16476893 0.17034987 0.16204469 0.13961992 0.11554427 0.10951224 0.13522676 0.18742356][0.16787404 0.18553801 0.19580398 0.19659831 0.19260196 0.19723704 0.21276358 0.22920072 0.23322526 0.21626943 0.17946997 0.13795686 0.11697254 0.13499248 0.18634361][0.19533853 0.20354848 0.21179162 0.21869653 0.22732233 0.248759 0.28107542 0.30949843 0.3161377 0.29187202 0.24020286 0.1794724 0.14005706 0.14555305 0.19196214][0.20473127 0.2013876 0.2070979 0.2211208 0.24382061 0.28340432 0.33464444 0.37836868 0.391291 0.36307228 0.30019996 0.22412235 0.16888589 0.16180597 0.2014087][0.1907309 0.1784661 0.18231122 0.20241202 0.23667201 0.29062179 0.357177 0.41447279 0.43481952 0.40691593 0.34000203 0.25713778 0.19282681 0.17539604 0.20576628][0.16021593 0.14449178 0.14839533 0.172426 0.21253262 0.27218178 0.34400856 0.40664625 0.43158069 0.40845367 0.34828493 0.27207249 0.2099074 0.18715751 0.20634109][0.13033232 0.11760677 0.12387986 0.14888392 0.18743275 0.24174874 0.30567861 0.36169991 0.38494927 0.36787221 0.32163104 0.26216608 0.21140355 0.18847229 0.19655408][0.10203115 0.096647516 0.10565473 0.12804322 0.15856797 0.19906437 0.24592897 0.28741121 0.30524072 0.29548404 0.26778716 0.23152724 0.19906501 0.18166439 0.18196027][0.086083472 0.089031771 0.098993063 0.11451733 0.13151872 0.15237769 0.1767657 0.1989632 0.20814541 0.20368552 0.19268163 0.17853208 0.16528459 0.15677698 0.15389533][0.089130431 0.099514768 0.10946609 0.11696894 0.11973432 0.12164089 0.12561233 0.13060713 0.13160105 0.12980293 0.12932251 0.12957413 0.12966722 0.12850489 0.12531289][0.10327563 0.11827791 0.12665312 0.12677871 0.1190116 0.10918706 0.1024733 0.099546134 0.097646348 0.097677276 0.10095543 0.10584989 0.1107472 0.11307937 0.11146699][0.12081325 0.13856584 0.14489746 0.14002061 0.12716189 0.11434303 0.10755664 0.10622545 0.10637882 0.10693049 0.10742895 0.1078473 0.10886727 0.10918311 0.10818257]]...]
INFO - root - 2017-12-10 21:09:52.174338: step 53510, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 59h:36m:38s remains)
INFO - root - 2017-12-10 21:09:59.877907: step 53520, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 59h:35m:25s remains)
INFO - root - 2017-12-10 21:10:07.704816: step 53530, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 58h:52m:10s remains)
INFO - root - 2017-12-10 21:10:15.472008: step 53540, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 61h:05m:13s remains)
INFO - root - 2017-12-10 21:10:23.450997: step 53550, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.790 sec/batch; 61h:10m:51s remains)
INFO - root - 2017-12-10 21:10:31.066521: step 53560, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 60h:33m:12s remains)
INFO - root - 2017-12-10 21:10:38.905986: step 53570, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 59h:56m:35s remains)
INFO - root - 2017-12-10 21:10:46.887401: step 53580, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.792 sec/batch; 61h:22m:54s remains)
INFO - root - 2017-12-10 21:10:54.723673: step 53590, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 60h:40m:12s remains)
INFO - root - 2017-12-10 21:11:02.427701: step 53600, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 60h:47m:56s remains)
2017-12-10 21:11:03.486021: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15340911 0.19339211 0.2234226 0.22982703 0.20462009 0.15508637 0.094489418 0.0389187 0.0011188936 -0.014409989 -0.011473198 0.00615918 0.039150987 0.088382423 0.14465797][0.23614873 0.28753853 0.32046291 0.32220477 0.28793845 0.22702736 0.15308653 0.083370544 0.0306783 -0.000587883 -0.014850148 -0.014336922 0.0050519719 0.046478275 0.10175593][0.31288424 0.3700147 0.40353346 0.40443662 0.37316009 0.31922922 0.2507636 0.17981 0.11732765 0.067081414 0.025814081 -0.0044525531 -0.012538995 0.0096773608 0.055006206][0.36791188 0.42249709 0.455331 0.46216035 0.44903994 0.42319906 0.38082245 0.32355002 0.25743264 0.18747661 0.11288993 0.042976137 -4.2556763e-05 -0.0012704468 0.033400688][0.39385507 0.44459268 0.48231775 0.50580943 0.52431446 0.54041964 0.53592056 0.500815 0.43498266 0.34566528 0.23664783 0.125816 0.047428463 0.024033625 0.051811878][0.3822237 0.43269667 0.48358822 0.53391021 0.59168035 0.65302688 0.68721491 0.67480916 0.60995656 0.50257725 0.36302564 0.21728386 0.1093017 0.0681004 0.093743049][0.32957256 0.38480434 0.45488405 0.53573984 0.63053942 0.72882056 0.79291183 0.79807132 0.7345345 0.61454993 0.45450857 0.2862336 0.15866035 0.10535247 0.13043462][0.25555092 0.31716293 0.40350848 0.50568759 0.62111765 0.73545855 0.8113693 0.82565188 0.76665092 0.64658135 0.48377827 0.31141585 0.17785136 0.11683735 0.13729911][0.18217526 0.24670546 0.33850726 0.44446212 0.557136 0.66247 0.73034531 0.74423468 0.69321364 0.58696127 0.44120389 0.28504354 0.16026062 0.097071946 0.10827804][0.11096667 0.17167555 0.25681275 0.35128152 0.44456333 0.52501357 0.57273388 0.57943106 0.53711087 0.45227459 0.33612293 0.21068831 0.10736375 0.0495332 0.0508857][0.04216871 0.088502586 0.15559824 0.22908869 0.29698268 0.34998891 0.3768529 0.37564933 0.34192348 0.28007552 0.19755363 0.10941032 0.035847809 -0.008256455 -0.012028107][-0.012650668 0.012056133 0.05443377 0.10328037 0.14690404 0.17790252 0.19002451 0.1840909 0.158338 0.11641184 0.064051062 0.011068921 -0.031430852 -0.056355711 -0.057934754][-0.042736758 -0.038619366 -0.019444419 0.0077336878 0.032597598 0.049436577 0.054326192 0.047890846 0.029355362 0.0020393159 -0.028330099 -0.055260431 -0.073442034 -0.080892093 -0.076565459][-0.045595385 -0.055763979 -0.053246416 -0.041097764 -0.027792919 -0.017596947 -0.01319974 -0.0159922 -0.027578749 -0.04501833 -0.061722331 -0.073618338 -0.078544356 -0.075867943 -0.06706617][-0.033720307 -0.050271973 -0.056353956 -0.051561777 -0.042096414 -0.031040082 -0.020676997 -0.014883824 -0.017635738 -0.027053429 -0.036257334 -0.042896651 -0.046113558 -0.043984223 -0.037067607]]...]
INFO - root - 2017-12-10 21:11:11.358994: step 53610, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 60h:51m:08s remains)
INFO - root - 2017-12-10 21:11:19.203285: step 53620, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 60h:08m:07s remains)
INFO - root - 2017-12-10 21:11:26.984292: step 53630, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 63h:00m:42s remains)
INFO - root - 2017-12-10 21:11:34.869098: step 53640, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.819 sec/batch; 63h:26m:38s remains)
INFO - root - 2017-12-10 21:11:42.657277: step 53650, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.820 sec/batch; 63h:32m:01s remains)
INFO - root - 2017-12-10 21:11:50.588638: step 53660, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 62h:05m:48s remains)
INFO - root - 2017-12-10 21:11:58.483334: step 53670, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 60h:13m:31s remains)
INFO - root - 2017-12-10 21:12:06.111655: step 53680, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 61h:42m:21s remains)
INFO - root - 2017-12-10 21:12:13.992154: step 53690, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.812 sec/batch; 62h:52m:39s remains)
INFO - root - 2017-12-10 21:12:21.833154: step 53700, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 60h:50m:01s remains)
2017-12-10 21:12:22.649802: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1715057 0.19481999 0.22382489 0.24855086 0.25521788 0.2374787 0.19896261 0.15784562 0.13965125 0.15580775 0.19832084 0.24135855 0.26149398 0.24855906 0.20844805][0.16367584 0.17765146 0.19479954 0.2073469 0.20922203 0.19963896 0.18194835 0.16775593 0.17358619 0.20446323 0.25009814 0.28686261 0.29601932 0.27555382 0.23569123][0.16297154 0.16240776 0.16414186 0.16494675 0.16427869 0.16405874 0.16622375 0.17568795 0.20005488 0.23871984 0.28107718 0.30652571 0.30249 0.27630118 0.24118783][0.17420411 0.16454682 0.15735121 0.15340646 0.15417372 0.16119614 0.17520159 0.19793423 0.23133054 0.2710124 0.30584374 0.31880417 0.30350429 0.27454847 0.24617922][0.18724658 0.17322315 0.16109498 0.15563914 0.15877463 0.17080566 0.19107036 0.21929002 0.25408769 0.2897974 0.31623489 0.32139936 0.30382472 0.2795575 0.26100332][0.20455371 0.18696231 0.16975769 0.16093419 0.16315222 0.1759133 0.19695686 0.22408617 0.254663 0.28422961 0.30607754 0.3134774 0.30636847 0.29579204 0.28934762][0.22091229 0.20201273 0.18147448 0.16959254 0.16884767 0.17831858 0.19416048 0.21352048 0.2349354 0.25715485 0.27840334 0.29565611 0.3062613 0.312117 0.31549695][0.21649179 0.20065528 0.18325372 0.17429394 0.17402089 0.18022269 0.18822846 0.19649665 0.206537 0.22096224 0.24203639 0.26795068 0.29210651 0.30883762 0.31719875][0.19575723 0.1856235 0.17445956 0.17015804 0.17118031 0.17509152 0.17792574 0.17998713 0.18461911 0.19592601 0.21711174 0.24525483 0.27139312 0.28742641 0.29289711][0.1713929 0.16618662 0.15933466 0.15616208 0.15539619 0.15611301 0.15654637 0.15860718 0.16545911 0.18009856 0.2034495 0.23027329 0.25109848 0.25910226 0.25713071][0.14710951 0.14570908 0.14155072 0.13762681 0.13351722 0.13029301 0.12864369 0.13184266 0.14271955 0.16292323 0.19016574 0.21670534 0.23317753 0.23506978 0.22804387][0.13284986 0.13275443 0.12860261 0.12244596 0.11427686 0.10645753 0.10166982 0.10442902 0.11821443 0.14445263 0.17777571 0.2072992 0.22215553 0.21930298 0.20692956][0.12850116 0.12892969 0.12451299 0.11605185 0.10351026 0.090071678 0.081041962 0.083025038 0.10091133 0.13565908 0.17798594 0.21270081 0.22601567 0.216042 0.19454783][0.14229439 0.14637721 0.14314002 0.13248423 0.11433635 0.0932995 0.078273185 0.078829981 0.10138959 0.14530461 0.19680721 0.23642179 0.24739084 0.22879283 0.19629152][0.16131693 0.17084904 0.16926938 0.1566381 0.13382094 0.10710874 0.088009313 0.087613285 0.11261907 0.16052464 0.21503654 0.25503638 0.26302052 0.23913106 0.2016167]]...]
INFO - root - 2017-12-10 21:12:30.668197: step 53710, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.821 sec/batch; 63h:35m:13s remains)
INFO - root - 2017-12-10 21:12:38.781540: step 53720, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 62h:32m:54s remains)
INFO - root - 2017-12-10 21:12:46.599034: step 53730, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 59h:24m:42s remains)
INFO - root - 2017-12-10 21:12:54.315202: step 53740, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 61h:56m:24s remains)
INFO - root - 2017-12-10 21:13:02.165029: step 53750, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 59h:37m:43s remains)
INFO - root - 2017-12-10 21:13:09.816571: step 53760, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 60h:56m:20s remains)
INFO - root - 2017-12-10 21:13:17.723720: step 53770, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 58h:56m:56s remains)
INFO - root - 2017-12-10 21:13:25.523699: step 53780, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 59h:59m:51s remains)
INFO - root - 2017-12-10 21:13:33.278084: step 53790, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 60h:44m:45s remains)
INFO - root - 2017-12-10 21:13:41.143562: step 53800, loss = 0.70, batch loss = 0.65 (9.8 examples/sec; 0.818 sec/batch; 63h:17m:34s remains)
2017-12-10 21:13:42.007293: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0097392648 -0.022921994 -0.03321521 -0.028676268 -0.021855058 -0.019039854 -0.021856129 -0.02728663 -0.032314021 -0.035767816 -0.037735704 -0.038755521 -0.039651219 -0.040739696 -0.042265322][0.028221695 -0.0047146445 -0.013062143 -0.0046723871 0.0049749254 0.0083301561 0.0037252533 -0.00420762 -0.01224611 -0.019234354 -0.024768637 -0.028882515 -0.032156203 -0.034854326 -0.0380157][0.049182374 0.019410126 0.016671175 0.032632638 0.049120177 0.056683175 0.053105496 0.043532982 0.031125566 0.017331006 0.0045506409 -0.0053811255 -0.011962994 -0.015687658 -0.019345082][0.075088911 0.052919116 0.060381189 0.090016834 0.12045278 0.13894916 0.14083177 0.13016264 0.11029273 0.083630048 0.056929171 0.036121804 0.024387553 0.020172665 0.017284375][0.114064 0.10135561 0.12150033 0.169473 0.22034921 0.25661838 0.26876697 0.25764519 0.22676679 0.18124372 0.1346745 0.098878831 0.079864904 0.07468468 0.073005][0.16218482 0.157739 0.1912898 0.2595714 0.33391151 0.39145729 0.41658497 0.4045983 0.35924289 0.29125869 0.22294767 0.17218876 0.14636795 0.14042474 0.14094642][0.21017356 0.21257257 0.2569764 0.34240085 0.43732893 0.51412344 0.5512557 0.53808028 0.47902602 0.39175647 0.30617407 0.24393031 0.21178155 0.20353809 0.20499617][0.24847712 0.25489452 0.3041622 0.39778575 0.50350553 0.59125966 0.63522816 0.62035859 0.55273831 0.45601889 0.363081 0.29553759 0.25872868 0.24735178 0.24871744][0.27009073 0.27682993 0.3240889 0.41424453 0.51671636 0.602596 0.64564735 0.62975746 0.56257826 0.47044492 0.38376057 0.3203623 0.283463 0.27001271 0.2709657][0.27059063 0.27630091 0.31613588 0.39203191 0.47796956 0.55024058 0.58638185 0.57223266 0.51586723 0.44232449 0.37478107 0.3245109 0.29281729 0.27971545 0.28021523][0.25225276 0.25741515 0.28740391 0.34402552 0.40770122 0.46234667 0.49161997 0.48452723 0.44765082 0.40091872 0.35825777 0.3245368 0.30069742 0.28967068 0.28924698][0.23341672 0.23851521 0.25867543 0.29666367 0.33990657 0.37948662 0.40471256 0.40825772 0.39427084 0.3753821 0.35636067 0.33793378 0.32247567 0.31438941 0.31204087][0.22933586 0.23430486 0.24534708 0.26795441 0.29619497 0.32599553 0.35033181 0.36500832 0.37151247 0.37532514 0.37410751 0.36698481 0.35847148 0.35289383 0.34756529][0.24189471 0.24716142 0.25094494 0.26254013 0.28111932 0.30523407 0.33004481 0.35353443 0.37484598 0.39346242 0.40229738 0.40065417 0.39487597 0.3887569 0.37830693][0.26173791 0.26726267 0.26535326 0.2684274 0.27903491 0.29710034 0.31937718 0.345727 0.37413409 0.39985037 0.4128114 0.41282052 0.40694252 0.39863229 0.38353923]]...]
INFO - root - 2017-12-10 21:13:49.863565: step 53810, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 60h:16m:17s remains)
INFO - root - 2017-12-10 21:13:57.735760: step 53820, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 61h:55m:48s remains)
INFO - root - 2017-12-10 21:14:05.475794: step 53830, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 61h:56m:50s remains)
INFO - root - 2017-12-10 21:14:13.295799: step 53840, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 59h:54m:43s remains)
INFO - root - 2017-12-10 21:14:21.137050: step 53850, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 60h:30m:15s remains)
INFO - root - 2017-12-10 21:14:28.937898: step 53860, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 60h:02m:36s remains)
INFO - root - 2017-12-10 21:14:36.761207: step 53870, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 60h:12m:50s remains)
INFO - root - 2017-12-10 21:14:44.584395: step 53880, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.765 sec/batch; 59h:11m:17s remains)
INFO - root - 2017-12-10 21:14:52.426918: step 53890, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 61h:14m:43s remains)
INFO - root - 2017-12-10 21:15:00.241648: step 53900, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 60h:45m:58s remains)
2017-12-10 21:15:01.073446: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26456192 0.28256509 0.29488134 0.2982721 0.2913273 0.2741273 0.25017217 0.22707613 0.21917954 0.24028836 0.29525757 0.3751891 0.44961062 0.48707712 0.48328021][0.25874698 0.28603458 0.30731562 0.31875715 0.32110789 0.31511852 0.29883847 0.27627966 0.26134852 0.26956126 0.30804703 0.37149018 0.43409017 0.46609589 0.46253791][0.23792817 0.26904938 0.2948108 0.3106443 0.32020184 0.3254745 0.32010731 0.30215669 0.28351387 0.27979994 0.29861411 0.33938357 0.38307834 0.40509737 0.40160447][0.20013182 0.23134111 0.26026046 0.28000736 0.29564694 0.31075293 0.31662357 0.30594248 0.28801563 0.27800319 0.28353852 0.3055934 0.33190739 0.34479615 0.34351039][0.1478025 0.18035963 0.21687098 0.24634188 0.27195761 0.29772511 0.31402647 0.30972815 0.29235482 0.27758282 0.274164 0.28406715 0.30059138 0.31144285 0.31675553][0.099841684 0.13612299 0.18329129 0.22733052 0.26715875 0.30480006 0.32986897 0.32850891 0.30811208 0.28617311 0.27415523 0.27787063 0.29650158 0.3199355 0.34104037][0.075980626 0.11740512 0.17195857 0.22639149 0.27664012 0.32150745 0.35018134 0.34690723 0.31983802 0.2888996 0.26926589 0.27374348 0.30718485 0.35728145 0.40201539][0.08987505 0.135664 0.18983293 0.24312031 0.29248261 0.33508217 0.36015061 0.35071492 0.31528908 0.27628347 0.25236788 0.26246822 0.31515846 0.39322805 0.4596225][0.146764 0.19374034 0.23941444 0.27991191 0.31657925 0.34756926 0.36279213 0.34470588 0.30061635 0.25375265 0.2249705 0.23741722 0.30250588 0.39875352 0.47869295][0.2278907 0.2715292 0.30588 0.3310529 0.35148287 0.367706 0.3709465 0.34388489 0.29133967 0.23570698 0.19886144 0.20668046 0.273039 0.37506276 0.46141714][0.30375326 0.33914018 0.36212957 0.37569094 0.38452908 0.38932872 0.38073602 0.34330994 0.28119844 0.21667747 0.17164396 0.17236944 0.23381731 0.3347449 0.42558542][0.35439986 0.38148132 0.39653575 0.40476656 0.40924734 0.40739596 0.38730571 0.336936 0.26219496 0.18708619 0.13411902 0.12697074 0.17932871 0.27348581 0.36647725][0.36651504 0.39077473 0.40563089 0.4169322 0.42482424 0.42165425 0.39230451 0.32883424 0.23982444 0.15215215 0.090400368 0.074546009 0.11545621 0.19892707 0.29038969][0.3495355 0.37510055 0.395431 0.4145973 0.42799675 0.4237369 0.38561288 0.31053713 0.20985599 0.11268651 0.045321468 0.023362657 0.05424184 0.12645362 0.21286187][0.31754518 0.34286261 0.36880568 0.39552322 0.41186085 0.40307486 0.35512146 0.2703414 0.16361338 0.065569676 0.0017011338 -0.018462563 0.0075656893 0.070272624 0.14796139]]...]
INFO - root - 2017-12-10 21:15:08.886803: step 53910, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 60h:45m:55s remains)
INFO - root - 2017-12-10 21:15:16.358191: step 53920, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 60h:44m:53s remains)
INFO - root - 2017-12-10 21:15:24.425723: step 53930, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 62h:23m:09s remains)
INFO - root - 2017-12-10 21:15:32.164889: step 53940, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 59h:42m:39s remains)
INFO - root - 2017-12-10 21:15:40.029287: step 53950, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 61h:54m:31s remains)
INFO - root - 2017-12-10 21:15:47.919512: step 53960, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 59h:45m:18s remains)
INFO - root - 2017-12-10 21:15:55.947711: step 53970, loss = 0.68, batch loss = 0.62 (8.6 examples/sec; 0.936 sec/batch; 72h:23m:15s remains)
INFO - root - 2017-12-10 21:16:03.808023: step 53980, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 59h:38m:20s remains)
INFO - root - 2017-12-10 21:16:11.638461: step 53990, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 60h:02m:41s remains)
INFO - root - 2017-12-10 21:16:19.295673: step 54000, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 60h:46m:45s remains)
2017-12-10 21:16:20.159287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.016889989 -0.017766707 -0.017911816 -0.017291116 -0.016126864 -0.014928892 -0.014036165 -0.013685718 -0.013883459 -0.014664942 -0.015850781 -0.017024728 -0.017965794 -0.018593812 -0.01902182][-0.017492587 -0.018331768 -0.018113634 -0.016658414 -0.014129178 -0.011181117 -0.008584301 -0.0069073834 -0.0066547291 -0.0079413485 -0.010376806 -0.013172561 -0.015590288 -0.017311938 -0.018296791][-0.017569607 -0.017303726 -0.01526948 -0.011471633 -0.00626006 -0.00068419817 0.0038556356 0.0063450169 0.0059656166 0.0028487016 -0.0022494534 -0.007939551 -0.012807627 -0.016257016 -0.018162303][-0.014182084 -0.011646925 -0.0064886413 0.00079835253 0.00949712 0.018184651 0.024850423 0.028017512 0.026634287 0.020979699 0.012092118 0.0019365457 -0.0070650489 -0.01377104 -0.017802166][-0.0064862827 -0.0010656076 0.00802879 0.019902691 0.033285685 0.046281844 0.056198474 0.060946722 0.059048276 0.050760578 0.037266795 0.021195352 0.0061466941 -0.0058147651 -0.013822647][0.0035318434 0.011675406 0.0246528 0.041212194 0.05946169 0.077016711 0.090557285 0.097434387 0.095668629 0.085504428 0.068288445 0.047260929 0.026798181 0.0096595883 -0.00274111][0.013750747 0.023819052 0.039358549 0.058823392 0.079914615 0.10011497 0.11605293 0.12489649 0.12424824 0.11427324 0.09639553 0.074018881 0.051536709 0.03181392 0.01667515][0.021537095 0.032504853 0.048476122 0.06743747 0.087107688 0.10560475 0.12056435 0.12960398 0.13038914 0.12307784 0.10881688 0.090450615 0.071322851 0.053868823 0.03991621][0.027619336 0.03937126 0.054417539 0.069982871 0.08407674 0.096306369 0.10623476 0.11238874 0.11321449 0.10922974 0.10095797 0.089994922 0.078029044 0.066825695 0.057813123][0.036376033 0.048496235 0.061325133 0.071583956 0.077767223 0.081523828 0.084448218 0.085768528 0.084810406 0.082644828 0.079166844 0.074682876 0.069485918 0.064912789 0.061907757][0.051947661 0.06378413 0.073340669 0.0774724 0.075548925 0.071338192 0.067898229 0.064500429 0.060738612 0.05817299 0.05620569 0.054308839 0.052139442 0.051034663 0.051767673][0.077312388 0.08717417 0.091367573 0.088563569 0.079798438 0.070637852 0.064627588 0.060104426 0.055527095 0.052015495 0.048577867 0.044649921 0.040333573 0.037607096 0.037651431][0.10322161 0.10819677 0.1045213 0.094728842 0.082217455 0.0735161 0.071451813 0.072451182 0.072091036 0.069631234 0.063719854 0.054497447 0.043606624 0.034600511 0.029904466][0.12089361 0.12024339 0.10880527 0.093554482 0.080221839 0.075573675 0.081400365 0.092432424 0.10077325 0.10273179 0.096322484 0.082246043 0.063774332 0.046651736 0.035335109][0.12553386 0.12151352 0.10607336 0.089008331 0.07728415 0.077249207 0.090160087 0.11037894 0.12760676 0.13554634 0.13150336 0.11595908 0.0928714 0.069829948 0.053384729]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 21:16:27.871183: step 54010, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 60h:21m:33s remains)
INFO - root - 2017-12-10 21:16:35.639223: step 54020, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 59h:08m:35s remains)
INFO - root - 2017-12-10 21:16:43.595292: step 54030, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 61h:08m:00s remains)
INFO - root - 2017-12-10 21:16:51.391283: step 54040, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.763 sec/batch; 59h:02m:43s remains)
INFO - root - 2017-12-10 21:16:59.258332: step 54050, loss = 0.67, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 60h:48m:34s remains)
INFO - root - 2017-12-10 21:17:07.135522: step 54060, loss = 0.67, batch loss = 0.61 (9.9 examples/sec; 0.810 sec/batch; 62h:37m:08s remains)
INFO - root - 2017-12-10 21:17:14.956028: step 54070, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 61h:45m:13s remains)
INFO - root - 2017-12-10 21:17:22.590692: step 54080, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 60h:41m:59s remains)
INFO - root - 2017-12-10 21:17:30.439196: step 54090, loss = 0.69, batch loss = 0.64 (10.7 examples/sec; 0.751 sec/batch; 58h:03m:04s remains)
INFO - root - 2017-12-10 21:17:38.117423: step 54100, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 59h:25m:49s remains)
2017-12-10 21:17:39.015971: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20441672 0.22092646 0.23032756 0.23273924 0.22593932 0.21159077 0.19472975 0.17602034 0.1562513 0.13910878 0.12375182 0.11009718 0.096240334 0.08432921 0.078494817][0.19479322 0.21425037 0.22100115 0.21439286 0.19412842 0.16572648 0.13796808 0.11511531 0.099206336 0.090917513 0.0861084 0.082290873 0.076769263 0.070933029 0.067794479][0.18070783 0.20500629 0.21110879 0.19830734 0.16803834 0.12911503 0.0940848 0.070712224 0.061493646 0.062910713 0.067800038 0.0717677 0.0716227 0.0687714 0.065665334][0.17934763 0.20966345 0.21829246 0.20464164 0.17090535 0.12786727 0.09083157 0.070718139 0.069946676 0.080773585 0.092480995 0.098892108 0.09735731 0.090814583 0.082895771][0.1865088 0.22323738 0.23808642 0.23020539 0.2008951 0.16099912 0.12706503 0.11266252 0.11993653 0.13764548 0.15134513 0.1535756 0.14388235 0.1282433 0.11246178][0.1920284 0.23575251 0.26134318 0.26604596 0.24798812 0.21624821 0.18762369 0.17800331 0.18961936 0.20922886 0.21982923 0.21390398 0.19462483 0.17159134 0.15184812][0.19246694 0.24092409 0.27756056 0.29695326 0.29273263 0.27111074 0.24825831 0.24140556 0.25285506 0.26901358 0.27284566 0.25842652 0.23338097 0.2104435 0.19547747][0.19116643 0.23806801 0.28001624 0.30982223 0.31713185 0.30482572 0.28705707 0.28081715 0.28862825 0.29819429 0.29505542 0.27584627 0.25217506 0.23800969 0.23556668][0.19304356 0.23045421 0.26814124 0.29894972 0.31046382 0.30286556 0.28762153 0.28006998 0.28327766 0.2872577 0.28154707 0.26468134 0.24991123 0.2496984 0.261474][0.20396611 0.22574611 0.25053176 0.27278042 0.28004467 0.27107254 0.25442809 0.2430207 0.2410441 0.24149857 0.23799366 0.22946326 0.22790034 0.24196205 0.2647506][0.22397232 0.23005497 0.2389449 0.24784 0.24544911 0.23049052 0.20935209 0.19289693 0.18651862 0.18600017 0.18785191 0.19016989 0.20152977 0.22578537 0.25378597][0.24418998 0.23962235 0.23496394 0.23062801 0.21722096 0.19506119 0.16962501 0.15042323 0.14303578 0.14489746 0.15342754 0.16528763 0.18511026 0.21256489 0.23887615][0.25763473 0.250414 0.23929498 0.2260929 0.20381397 0.17593874 0.14832707 0.12976874 0.12498664 0.13206597 0.14759681 0.16652393 0.19030286 0.21598634 0.23690259][0.2544077 0.25184557 0.24102457 0.22429706 0.19737947 0.16748099 0.14129789 0.12679562 0.12704183 0.14077467 0.16313486 0.18731312 0.21254773 0.23513001 0.25089344][0.22455938 0.22838624 0.22100544 0.20498383 0.17859304 0.15176411 0.13133048 0.12368995 0.12997404 0.14973781 0.17698707 0.2035806 0.2277645 0.24697271 0.25912645]]...]
INFO - root - 2017-12-10 21:17:46.927416: step 54110, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 62h:04m:29s remains)
INFO - root - 2017-12-10 21:17:54.727134: step 54120, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 60h:45m:49s remains)
INFO - root - 2017-12-10 21:18:02.583018: step 54130, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.820 sec/batch; 63h:22m:21s remains)
INFO - root - 2017-12-10 21:18:10.417572: step 54140, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 59h:38m:16s remains)
INFO - root - 2017-12-10 21:18:18.226584: step 54150, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 59h:16m:10s remains)
INFO - root - 2017-12-10 21:18:25.817597: step 54160, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 58h:26m:38s remains)
INFO - root - 2017-12-10 21:18:33.728178: step 54170, loss = 0.67, batch loss = 0.61 (10.2 examples/sec; 0.783 sec/batch; 60h:32m:09s remains)
INFO - root - 2017-12-10 21:18:41.441483: step 54180, loss = 0.70, batch loss = 0.64 (12.1 examples/sec; 0.661 sec/batch; 51h:05m:28s remains)
INFO - root - 2017-12-10 21:18:49.263307: step 54190, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 61h:49m:35s remains)
INFO - root - 2017-12-10 21:18:57.140479: step 54200, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 62h:22m:39s remains)
2017-12-10 21:18:58.034269: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.030935427 0.093704425 0.18223749 0.27971643 0.36734986 0.43172404 0.47208238 0.48841447 0.48953304 0.48754051 0.48476192 0.48695546 0.4929224 0.49708071 0.4835892][0.030753603 0.095232755 0.18948436 0.29362771 0.38736445 0.45631278 0.50200605 0.52712262 0.53569752 0.53588313 0.52982074 0.52649158 0.52369988 0.51354784 0.48087791][0.025884325 0.090069965 0.18593104 0.29079214 0.38346151 0.45021543 0.49671602 0.52743447 0.54009914 0.53995848 0.53131396 0.52585876 0.51691639 0.49453455 0.4466573][0.021604989 0.087002136 0.18596853 0.29100642 0.37942189 0.43981722 0.48336026 0.51500469 0.52609438 0.52144408 0.50945705 0.50198728 0.486568 0.45305085 0.39490059][0.02118261 0.09113986 0.19643956 0.30393621 0.38904318 0.44348651 0.48234662 0.5084486 0.50860357 0.4915621 0.4718563 0.45979431 0.43792704 0.39760542 0.33765805][0.022385446 0.097430475 0.20839845 0.3177413 0.40005159 0.45057297 0.48551449 0.50401628 0.4907271 0.459676 0.43116593 0.41383231 0.38783875 0.34604451 0.29110736][0.022498894 0.10055289 0.21321669 0.32059947 0.39832854 0.44585335 0.47749037 0.48867533 0.46500555 0.42483413 0.39161447 0.37219405 0.34709811 0.3103843 0.26623523][0.02022294 0.097131588 0.20488393 0.30412734 0.37334102 0.41591448 0.44359505 0.45026985 0.42469746 0.38676381 0.35887945 0.34375405 0.32253164 0.29253271 0.26014942][0.015689507 0.087440133 0.18433903 0.27048072 0.32853034 0.36431953 0.38811028 0.39440742 0.37529537 0.34873018 0.33330032 0.32622403 0.30957329 0.28582668 0.26496267][0.0083644111 0.071583271 0.15354629 0.2239224 0.26997685 0.29776052 0.31776056 0.32783103 0.32201973 0.31320888 0.31397396 0.31635293 0.30377761 0.28510505 0.27411842][0.00056327821 0.054054517 0.12039532 0.17516057 0.20937289 0.22746141 0.24194084 0.25625539 0.26516512 0.27471229 0.2906875 0.30174503 0.29270568 0.2772356 0.27202171][-0.003584099 0.042863946 0.09808246 0.14209464 0.16761674 0.17579572 0.18096428 0.1927761 0.20746037 0.22652158 0.2508496 0.26790217 0.26344714 0.25203347 0.2502524][-0.0027459336 0.041674457 0.093145952 0.13398726 0.15660034 0.15771832 0.15202983 0.1543057 0.16334979 0.17948648 0.20181052 0.21940297 0.21892552 0.21106379 0.2092444][0.0017374459 0.048737146 0.10337709 0.1486177 0.1750529 0.17449395 0.1598143 0.14889167 0.14353202 0.14583907 0.15618028 0.16771854 0.16839804 0.16215931 0.15755993][0.0056250347 0.056553114 0.11754677 0.17131361 0.20602635 0.20900252 0.1897862 0.16724856 0.14580545 0.13109314 0.12641212 0.12936497 0.12912126 0.1224228 0.11331472]]...]
INFO - root - 2017-12-10 21:19:05.938671: step 54210, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.820 sec/batch; 63h:24m:34s remains)
INFO - root - 2017-12-10 21:19:13.931852: step 54220, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 59h:58m:49s remains)
INFO - root - 2017-12-10 21:19:21.731606: step 54230, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 61h:45m:55s remains)
INFO - root - 2017-12-10 21:19:29.433559: step 54240, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 59h:40m:33s remains)
INFO - root - 2017-12-10 21:19:37.440173: step 54250, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 60h:53m:54s remains)
INFO - root - 2017-12-10 21:19:45.302470: step 54260, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.804 sec/batch; 62h:09m:12s remains)
INFO - root - 2017-12-10 21:19:53.037757: step 54270, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 61h:40m:40s remains)
INFO - root - 2017-12-10 21:20:00.817045: step 54280, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 61h:37m:07s remains)
INFO - root - 2017-12-10 21:20:08.571878: step 54290, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 59h:14m:23s remains)
INFO - root - 2017-12-10 21:20:16.408604: step 54300, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 59h:57m:41s remains)
2017-12-10 21:20:17.337105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043993067 -0.011943864 0.053812578 0.15789115 0.28625134 0.4072789 0.4884769 0.51145124 0.48133579 0.42537925 0.38198021 0.36841762 0.36952713 0.35935158 0.33296883][-0.047851365 -0.018117677 0.044128809 0.14525387 0.27429011 0.40113747 0.49174902 0.52305835 0.49576014 0.43382984 0.37444308 0.33896682 0.32105902 0.30419573 0.28570214][-0.048170429 -0.018620461 0.042702686 0.1426471 0.27267537 0.40481347 0.50398076 0.542505 0.5159322 0.44383323 0.36238244 0.30002251 0.26170415 0.24090734 0.23660222][-0.046438258 -0.014046677 0.051405437 0.15547319 0.29003945 0.4279626 0.53320509 0.57482582 0.54432774 0.45746535 0.3503167 0.25906795 0.20078361 0.17887555 0.19396341][-0.044130586 -0.0076675038 0.064669363 0.17615172 0.31766012 0.46210533 0.57292545 0.61675793 0.5814299 0.47914732 0.34621912 0.22656903 0.14950082 0.127473 0.16319981][-0.040336076 0.0016619645 0.083032347 0.20541143 0.35781148 0.51152813 0.62830794 0.672346 0.62799382 0.5062744 0.3470374 0.20324792 0.11398254 0.096433893 0.15353411][-0.035236239 0.012285935 0.10292085 0.23787831 0.40365031 0.5677911 0.68787068 0.725221 0.66320437 0.51642644 0.33355162 0.17642392 0.089179389 0.088042885 0.17294884][-0.030218324 0.021265931 0.11875455 0.26365891 0.43972421 0.609605 0.72577584 0.74787682 0.661981 0.49100792 0.29383051 0.13816682 0.067975961 0.094455674 0.21154432][-0.026462557 0.026053224 0.12492278 0.2711626 0.44582766 0.60765678 0.70728445 0.70759803 0.60155028 0.42064479 0.22871624 0.091997713 0.048931539 0.10416207 0.24615134][-0.024951341 0.025210205 0.11828364 0.25356388 0.41026625 0.546941 0.61816591 0.5956946 0.48129788 0.31088197 0.14493498 0.040508319 0.027305169 0.10432181 0.25677392][-0.027319444 0.017338442 0.098135158 0.21216586 0.33822548 0.43899933 0.477669 0.43686932 0.32530567 0.17878959 0.049783334 -0.01716353 -0.0026021157 0.087231539 0.2351588][-0.035031769 0.00040016175 0.063750833 0.15008163 0.23964034 0.3020722 0.31099856 0.25923568 0.15942849 0.043512531 -0.0464422 -0.078722842 -0.043721147 0.047778394 0.17723165][-0.047614839 -0.024236409 0.018863656 0.075811647 0.13016716 0.1595141 0.14721291 0.09398894 0.013329078 -0.06946709 -0.12397872 -0.13052283 -0.087512575 -0.0082282377 0.090123229][-0.06024231 -0.048835449 -0.02431019 0.0070989439 0.033133119 0.039098196 0.016332829 -0.029701814 -0.086799674 -0.13780116 -0.164444 -0.15794478 -0.12155236 -0.066861 -0.0058258288][-0.066495836 -0.061722942 -0.047908816 -0.031866793 -0.022884965 -0.029935583 -0.055823129 -0.0924509 -0.12957212 -0.15740831 -0.16797486 -0.16010544 -0.13817082 -0.10941668 -0.079842485]]...]
INFO - root - 2017-12-10 21:20:25.122117: step 54310, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 63h:00m:30s remains)
INFO - root - 2017-12-10 21:20:32.757143: step 54320, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 61h:31m:57s remains)
INFO - root - 2017-12-10 21:20:40.601447: step 54330, loss = 0.67, batch loss = 0.61 (10.4 examples/sec; 0.773 sec/batch; 59h:42m:45s remains)
INFO - root - 2017-12-10 21:20:48.446072: step 54340, loss = 0.70, batch loss = 0.64 (9.5 examples/sec; 0.846 sec/batch; 65h:20m:57s remains)
INFO - root - 2017-12-10 21:20:56.269086: step 54350, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 58h:31m:24s remains)
INFO - root - 2017-12-10 21:21:03.627872: step 54360, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 61h:45m:50s remains)
INFO - root - 2017-12-10 21:21:11.423235: step 54370, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 60h:23m:06s remains)
INFO - root - 2017-12-10 21:21:19.265144: step 54380, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 62h:05m:35s remains)
INFO - root - 2017-12-10 21:21:27.164823: step 54390, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 59h:07m:36s remains)
INFO - root - 2017-12-10 21:21:34.846222: step 54400, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 60h:40m:33s remains)
2017-12-10 21:21:35.710005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.033118527 -0.012795457 0.021951493 0.069857605 0.12837481 0.18715474 0.22642207 0.23815663 0.22059059 0.18204351 0.14178957 0.11291428 0.10301992 0.11376919 0.14567064][-0.0337113 -0.011012994 0.028652906 0.082817025 0.14844285 0.21343654 0.25838876 0.27481008 0.25970772 0.2223938 0.18154624 0.15064573 0.13499668 0.13575371 0.1551211][-0.033078715 -0.0090527916 0.034029447 0.09256497 0.16179447 0.22904633 0.27620313 0.29588491 0.28533438 0.2533375 0.21654537 0.18679138 0.16811119 0.16033655 0.16494107][-0.033119943 -0.008676487 0.036025237 0.096816912 0.16711515 0.23362049 0.28004724 0.30180898 0.29766434 0.27477124 0.24668793 0.22307746 0.20724815 0.19661485 0.19182585][-0.034157097 -0.010027035 0.034204327 0.094164371 0.16309446 0.22759572 0.27272114 0.29633296 0.29877278 0.28618592 0.2688382 0.25435328 0.24549206 0.23693779 0.22793208][-0.034642335 -0.0093087889 0.035857122 0.096628375 0.16685051 0.23223491 0.27791974 0.30218464 0.30677146 0.29790768 0.28453586 0.27487272 0.27195233 0.26674297 0.25591946][-0.034089755 -0.0059222607 0.043150164 0.10958892 0.1867909 0.25829038 0.30642697 0.32779616 0.32581314 0.30751857 0.28307459 0.2650719 0.26031598 0.25677425 0.24810328][-0.033276927 -0.0030708048 0.049644168 0.12308595 0.20930855 0.28918636 0.34059078 0.35728484 0.34397152 0.30785385 0.26181197 0.22565717 0.2123546 0.20895405 0.20580705][-0.033011485 -0.0025506937 0.051380649 0.12887235 0.22065364 0.30542788 0.35790014 0.36947635 0.34587446 0.29367059 0.22723667 0.17249236 0.14842033 0.14385352 0.14626373][-0.033148713 -0.0033991281 0.049562056 0.12668957 0.21713333 0.29857448 0.3457332 0.35015076 0.31925011 0.25826991 0.18082154 0.11494275 0.082726642 0.075993873 0.080415457][-0.033771414 -0.0055577871 0.045197815 0.11860075 0.20175338 0.27219233 0.30754972 0.30235979 0.26609233 0.20258412 0.12380783 0.05611008 0.021129528 0.012490143 0.016029233][-0.036836669 -0.012278674 0.033511989 0.099211857 0.17057894 0.22591755 0.24670148 0.23155133 0.19109859 0.12895595 0.056081522 -0.005092789 -0.036234707 -0.042970926 -0.038172983][-0.04322283 -0.02579274 0.0097720753 0.061782673 0.1171869 0.15712544 0.16678037 0.1472934 0.1086913 0.055050332 -0.0041373339 -0.051837396 -0.07435523 -0.07682126 -0.07021445][-0.0501758 -0.040892426 -0.01834601 0.016097521 0.052939426 0.07854607 0.082044832 0.065116182 0.03528022 -0.0036311371 -0.044369143 -0.075802356 -0.089285806 -0.08896251 -0.082900316][-0.05530832 -0.05188746 -0.040008347 -0.021299308 -0.0013310529 0.011944499 0.011356279 -0.0012006639 -0.020448716 -0.04342908 -0.065772288 -0.081815012 -0.087549329 -0.086081266 -0.082153194]]...]
INFO - root - 2017-12-10 21:21:43.605549: step 54410, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 61h:53m:32s remains)
INFO - root - 2017-12-10 21:21:51.493132: step 54420, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 59h:01m:18s remains)
INFO - root - 2017-12-10 21:21:59.266587: step 54430, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.746 sec/batch; 57h:38m:04s remains)
INFO - root - 2017-12-10 21:22:07.295948: step 54440, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 60h:56m:01s remains)
INFO - root - 2017-12-10 21:22:14.941916: step 54450, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 61h:13m:47s remains)
INFO - root - 2017-12-10 21:22:22.825371: step 54460, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 61h:06m:56s remains)
INFO - root - 2017-12-10 21:22:30.698448: step 54470, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.804 sec/batch; 62h:03m:57s remains)
INFO - root - 2017-12-10 21:22:38.298989: step 54480, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 59h:00m:05s remains)
INFO - root - 2017-12-10 21:22:46.139894: step 54490, loss = 0.73, batch loss = 0.67 (9.9 examples/sec; 0.806 sec/batch; 62h:12m:54s remains)
INFO - root - 2017-12-10 21:22:54.097331: step 54500, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 59h:18m:26s remains)
2017-12-10 21:22:55.038252: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19569321 0.21309882 0.2386262 0.26974773 0.29468039 0.30790731 0.30815688 0.29935408 0.28796041 0.275604 0.26431471 0.25954041 0.26140609 0.25871667 0.24048392][0.20376188 0.23069493 0.26792863 0.310582 0.34615797 0.36722323 0.37229252 0.3653191 0.35374403 0.34035459 0.32900208 0.32422885 0.32341054 0.31586111 0.29366809][0.17329775 0.20640962 0.25247833 0.30564016 0.35291466 0.3846468 0.39863279 0.39812753 0.38995978 0.37751326 0.36706284 0.36204574 0.3579933 0.34621161 0.32305893][0.14714612 0.17480931 0.2189911 0.27543727 0.33099446 0.37318704 0.39826351 0.40768075 0.40573734 0.39547208 0.38572508 0.38085473 0.37609267 0.36453584 0.34601966][0.1472287 0.15992558 0.1914448 0.24340354 0.30316213 0.35595706 0.39576551 0.42008248 0.42808786 0.42022958 0.40744093 0.39807594 0.39173031 0.38440195 0.37747189][0.19559394 0.18870616 0.20449051 0.24918219 0.30978918 0.37033111 0.4230513 0.4604651 0.47534737 0.46494469 0.44228327 0.42197692 0.41151386 0.409433 0.41682974][0.27892339 0.2598837 0.26441681 0.30248296 0.35929328 0.41867787 0.47335604 0.51287836 0.52525508 0.50564641 0.46910477 0.43643925 0.42282945 0.42660475 0.44661641][0.35771006 0.33821473 0.338602 0.37301058 0.42442745 0.47639588 0.52255172 0.55265677 0.5531289 0.51852906 0.465925 0.4220823 0.40733328 0.417051 0.44588223][0.39345694 0.37848946 0.37874439 0.4099935 0.45370358 0.49395639 0.52638918 0.54278213 0.5305087 0.48461726 0.42268813 0.37434688 0.36059907 0.37452066 0.407149][0.3632842 0.35349897 0.35430595 0.3798418 0.41254365 0.43907088 0.45798728 0.46387953 0.44636855 0.40109748 0.342979 0.29930195 0.28873983 0.30483004 0.33710524][0.27660629 0.27204931 0.2748062 0.29416978 0.31542006 0.32926723 0.33827034 0.34011433 0.32634944 0.29239154 0.2479315 0.21486823 0.20772175 0.2228864 0.25121275][0.15400557 0.15431547 0.15890156 0.17304692 0.18596348 0.19285035 0.20044716 0.20847584 0.20828776 0.19298221 0.1665778 0.14501284 0.13927275 0.14997512 0.1715408][0.036884561 0.03938223 0.044938959 0.056350961 0.06758409 0.077596746 0.095356233 0.11970738 0.13914362 0.14308393 0.13087302 0.11440431 0.10368587 0.1045992 0.11631987][-0.040154234 -0.039910242 -0.035683129 -0.025751444 -0.012058074 0.00686156 0.039763231 0.0825008 0.1194438 0.13710341 0.13246028 0.11520416 0.096122645 0.085450523 0.0872346][-0.076272622 -0.079289645 -0.076553494 -0.066135429 -0.047314752 -0.017568052 0.0295332 0.086225316 0.13351469 0.15706238 0.15349032 0.13166593 0.10386484 0.082998954 0.077025436]]...]
INFO - root - 2017-12-10 21:23:02.918790: step 54510, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 60h:56m:11s remains)
INFO - root - 2017-12-10 21:23:10.776698: step 54520, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 59h:28m:31s remains)
INFO - root - 2017-12-10 21:23:18.553301: step 54530, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.766 sec/batch; 59h:10m:06s remains)
INFO - root - 2017-12-10 21:23:26.275115: step 54540, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 60h:09m:35s remains)
INFO - root - 2017-12-10 21:23:34.097947: step 54550, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 60h:41m:24s remains)
INFO - root - 2017-12-10 21:23:41.847116: step 54560, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 61h:33m:56s remains)
INFO - root - 2017-12-10 21:23:49.702828: step 54570, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 60h:28m:56s remains)
INFO - root - 2017-12-10 21:23:57.597208: step 54580, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 60h:54m:02s remains)
INFO - root - 2017-12-10 21:24:05.398810: step 54590, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 59h:41m:07s remains)
INFO - root - 2017-12-10 21:24:13.214443: step 54600, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.793 sec/batch; 61h:13m:26s remains)
2017-12-10 21:24:14.066613: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0050909882 0.019560279 0.045688085 0.086071841 0.13773456 0.1887521 0.22247817 0.22553782 0.19379839 0.12938748 0.04576828 -0.031403236 -0.079764254 -0.097181566 -0.096065938][0.089625709 0.1131621 0.15124665 0.20955595 0.28096035 0.34775019 0.38818884 0.38539729 0.33448923 0.23861571 0.11966003 0.011740639 -0.057974122 -0.087623216 -0.093251035][0.19544007 0.23314658 0.28375483 0.35413215 0.434253 0.50433385 0.54175806 0.52863723 0.45941976 0.33774343 0.19046907 0.056859836 -0.032282703 -0.074829936 -0.087684385][0.30370563 0.35861239 0.41892073 0.49033695 0.56445265 0.62504977 0.652361 0.62818485 0.54745919 0.412383 0.24878187 0.096577756 -0.0095808338 -0.064067565 -0.083915412][0.38544053 0.46003672 0.52752918 0.59300476 0.65254009 0.69588828 0.70769292 0.67144978 0.58376157 0.44587198 0.27846712 0.11834111 0.0027526629 -0.058620378 -0.08238025][0.443448 0.53668648 0.61050647 0.6712203 0.7199384 0.74839777 0.74530494 0.69598955 0.59991091 0.4595243 0.29160988 0.12954789 0.010888184 -0.052695986 -0.078073412][0.50240242 0.60674524 0.684685 0.74499369 0.79305732 0.81780726 0.808671 0.74887985 0.64019626 0.48973227 0.31439731 0.14656653 0.023456391 -0.042932041 -0.070932925][0.54893047 0.65754241 0.73777235 0.80080819 0.85399753 0.88199413 0.8719418 0.8025766 0.67799139 0.51270437 0.32657272 0.15356648 0.028825274 -0.0375981 -0.066154681][0.56248486 0.669134 0.7518006 0.8198151 0.87896389 0.9093861 0.89618891 0.81576496 0.67591637 0.49869186 0.30795598 0.13815136 0.019796571 -0.041019853 -0.066569142][0.52257943 0.62244886 0.70488834 0.77479994 0.83387154 0.85955447 0.83807141 0.7492 0.60428077 0.42978328 0.25126138 0.099448569 -0.0022205964 -0.052311998 -0.071839526][0.4236916 0.51048017 0.58556867 0.64958936 0.69997734 0.71497732 0.68521637 0.59721476 0.46388203 0.31102154 0.16241635 0.041664872 -0.035597168 -0.070961341 -0.08150278][0.28626138 0.35248554 0.41062891 0.45852903 0.4920519 0.49459779 0.46188509 0.38627863 0.27940741 0.16301563 0.055946376 -0.026449341 -0.074934833 -0.092119373 -0.091194883][0.13914393 0.18095249 0.21635863 0.24203211 0.25519 0.24632859 0.21527845 0.1587549 0.085947767 0.012589565 -0.049374413 -0.091939256 -0.11056583 -0.10840331 -0.096220106][0.019971874 0.040586293 0.055782974 0.0624478 0.0599083 0.044887081 0.01968139 -0.016207762 -0.056918126 -0.093132459 -0.11876032 -0.13065186 -0.12735541 -0.11259818 -0.094888076][-0.05073652 -0.044395395 -0.042628009 -0.047510125 -0.057767611 -0.072493359 -0.088765226 -0.10648809 -0.1234237 -0.13525161 -0.13930857 -0.13488935 -0.12194472 -0.10450268 -0.088011608]]...]
INFO - root - 2017-12-10 21:24:21.909926: step 54610, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 61h:41m:43s remains)
INFO - root - 2017-12-10 21:24:29.714245: step 54620, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.766 sec/batch; 59h:05m:28s remains)
INFO - root - 2017-12-10 21:24:37.419212: step 54630, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 59h:43m:56s remains)
INFO - root - 2017-12-10 21:24:45.115986: step 54640, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 60h:38m:25s remains)
INFO - root - 2017-12-10 21:24:53.037258: step 54650, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 60h:37m:35s remains)
INFO - root - 2017-12-10 21:25:00.902803: step 54660, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 62h:49m:00s remains)
INFO - root - 2017-12-10 21:25:08.670517: step 54670, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 59h:25m:31s remains)
INFO - root - 2017-12-10 21:25:16.473615: step 54680, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 58h:25m:24s remains)
INFO - root - 2017-12-10 21:25:24.364791: step 54690, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 60h:27m:13s remains)
INFO - root - 2017-12-10 21:25:32.379664: step 54700, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 61h:32m:06s remains)
2017-12-10 21:25:33.198726: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16256733 0.18105814 0.19345883 0.19874173 0.19466358 0.17995515 0.16098922 0.14976117 0.15253706 0.16830763 0.19443798 0.22627565 0.25369143 0.26705268 0.25525254][0.14102603 0.15152624 0.16008531 0.16600071 0.16523214 0.15579756 0.1424983 0.13744596 0.14840597 0.17471227 0.21135102 0.25291783 0.28989619 0.31063065 0.30160275][0.12545004 0.12350277 0.12337412 0.12638879 0.12936805 0.13032866 0.13037983 0.13831671 0.16125707 0.19750744 0.24075799 0.287323 0.32952073 0.35445955 0.34645447][0.14029136 0.1251377 0.11253645 0.10765316 0.11013522 0.11948386 0.13345841 0.15665326 0.19389981 0.24116541 0.2901209 0.3390727 0.38221383 0.40626192 0.3942284][0.18282786 0.15657184 0.13046148 0.11428487 0.11216693 0.12583143 0.15126334 0.18909681 0.24053696 0.29821157 0.35073414 0.39811188 0.43689138 0.45549551 0.43682915][0.23266833 0.19792624 0.16015433 0.13489725 0.13055195 0.14961588 0.18613054 0.23665351 0.29839924 0.361468 0.41291696 0.45505995 0.48687872 0.49902534 0.47456613][0.26279396 0.21946706 0.17402545 0.1476637 0.15010411 0.18109141 0.230135 0.28856117 0.35011 0.40597945 0.44709525 0.48033133 0.50651461 0.51633072 0.49116462][0.26226556 0.21458027 0.17018679 0.15307841 0.16998415 0.21513727 0.27284747 0.32971248 0.37782478 0.41264719 0.4333171 0.45312557 0.47348243 0.48285326 0.45999652][0.23517741 0.19314763 0.1598324 0.15708332 0.18735009 0.24115969 0.30050552 0.34925321 0.37842897 0.38743985 0.38372755 0.38667592 0.39824367 0.40554696 0.38562077][0.19254968 0.16439606 0.1454819 0.15211231 0.18538313 0.23710383 0.29109979 0.33097467 0.34653851 0.33833936 0.31873453 0.31025213 0.31541249 0.32133767 0.30520436][0.14505443 0.131944 0.1255419 0.13516313 0.16171496 0.20064668 0.24045511 0.26847559 0.27623665 0.26467365 0.24657051 0.2420727 0.251594 0.26153713 0.25041714][0.11517208 0.11133415 0.11179619 0.11930756 0.13319016 0.15184166 0.16975781 0.1819813 0.18569252 0.18239947 0.18115731 0.194478 0.21763414 0.2352542 0.22806601][0.11950057 0.1212652 0.12482505 0.12857376 0.12958872 0.12841791 0.1259241 0.12570785 0.13047062 0.141025 0.16042668 0.19195256 0.22535768 0.24386796 0.23168418][0.13411832 0.13989019 0.1456826 0.14810559 0.14305988 0.13261795 0.12232136 0.12047342 0.13111331 0.15279205 0.18448006 0.2243218 0.2597717 0.27356946 0.25246677][0.1425439 0.14974208 0.15580414 0.15731502 0.15036744 0.13786331 0.1273403 0.12784135 0.14222181 0.1674021 0.20194297 0.24292105 0.27739429 0.2876296 0.26124018]]...]
INFO - root - 2017-12-10 21:25:41.151242: step 54710, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 59h:11m:39s remains)
INFO - root - 2017-12-10 21:25:48.808016: step 54720, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 60h:45m:17s remains)
INFO - root - 2017-12-10 21:25:56.641145: step 54730, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 59h:33m:01s remains)
INFO - root - 2017-12-10 21:26:04.474030: step 54740, loss = 0.69, batch loss = 0.64 (9.7 examples/sec; 0.822 sec/batch; 63h:25m:48s remains)
INFO - root - 2017-12-10 21:26:12.283479: step 54750, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.818 sec/batch; 63h:05m:10s remains)
INFO - root - 2017-12-10 21:26:20.191444: step 54760, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.758 sec/batch; 58h:28m:13s remains)
INFO - root - 2017-12-10 21:26:28.026901: step 54770, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.773 sec/batch; 59h:39m:03s remains)
INFO - root - 2017-12-10 21:26:35.886206: step 54780, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 60h:19m:35s remains)
INFO - root - 2017-12-10 21:26:43.769157: step 54790, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 59h:20m:21s remains)
INFO - root - 2017-12-10 21:26:51.553719: step 54800, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 59h:55m:26s remains)
2017-12-10 21:26:52.414973: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15700397 0.10844361 0.091554068 0.11158621 0.16100453 0.22284894 0.2756528 0.30005673 0.28503859 0.23882616 0.18657966 0.14941564 0.13376419 0.13445999 0.14237085][0.1889105 0.16524598 0.16680279 0.19769108 0.24943209 0.30502033 0.3490535 0.36642072 0.34758356 0.30197218 0.25309807 0.21892326 0.20040222 0.19190173 0.18919978][0.22887422 0.22851194 0.24754319 0.2888906 0.34273568 0.39288327 0.43145394 0.44686532 0.43097711 0.39504731 0.3595854 0.33652651 0.31923684 0.29995495 0.28229067][0.28946674 0.30097657 0.33348924 0.38675252 0.44712889 0.49742404 0.53513807 0.55105734 0.53893989 0.51553822 0.49922544 0.49426478 0.48296297 0.45372322 0.41753805][0.34754777 0.36226872 0.406199 0.47496754 0.548582 0.607112 0.64932424 0.66531742 0.65224314 0.63698035 0.63855082 0.65370876 0.6522997 0.61696488 0.56505352][0.37714711 0.39246029 0.44738492 0.53448451 0.62544948 0.69704276 0.74541336 0.75805396 0.7362 0.71824443 0.72750491 0.755796 0.76338965 0.72711176 0.66855031][0.35794589 0.37469563 0.44152951 0.54717082 0.65547234 0.73993373 0.79269081 0.79930753 0.76473647 0.73592752 0.74130726 0.7710076 0.7813977 0.74656153 0.69103253][0.28762484 0.30570126 0.38231421 0.50278074 0.62521541 0.72063309 0.77738649 0.78093117 0.73818988 0.69845188 0.69316632 0.71352565 0.71837831 0.68351531 0.63536352][0.19473234 0.21942906 0.30601177 0.4357394 0.56451881 0.66310167 0.7191658 0.72104913 0.67609966 0.62874126 0.61003226 0.61502552 0.610296 0.57723677 0.54154325][0.098857068 0.13549453 0.23094691 0.36315751 0.48955908 0.58311027 0.63459659 0.63708264 0.596642 0.546859 0.51596367 0.50471342 0.49186593 0.46527952 0.44736204][0.027538072 0.074343935 0.1702182 0.29233909 0.40505061 0.48680851 0.53360784 0.54318166 0.51680547 0.47238907 0.43325323 0.40703964 0.38753712 0.36886865 0.36777538][-0.0060404283 0.046775423 0.13407661 0.23403089 0.32304046 0.38847223 0.43239236 0.45532346 0.45223454 0.42473429 0.38880548 0.35757998 0.33764309 0.32775271 0.33812827][-0.015181428 0.039470185 0.11359291 0.18777281 0.25091463 0.30061185 0.34398255 0.38294804 0.40690237 0.40465567 0.38339892 0.3579897 0.34292555 0.34012043 0.35429788][-0.017498948 0.032516703 0.091639966 0.14348067 0.18558137 0.22340043 0.26702642 0.3188467 0.36527413 0.38850507 0.38774323 0.37485006 0.36798984 0.3722541 0.38859734][-0.021179901 0.012891663 0.051545907 0.082759552 0.10905644 0.13987327 0.18540961 0.2470286 0.31038657 0.35622385 0.3776089 0.37969053 0.38069656 0.38910177 0.40570173]]...]
INFO - root - 2017-12-10 21:27:00.015102: step 54810, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 62h:04m:45s remains)
INFO - root - 2017-12-10 21:27:07.818733: step 54820, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 61h:26m:23s remains)
INFO - root - 2017-12-10 21:27:15.699934: step 54830, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 60h:03m:04s remains)
INFO - root - 2017-12-10 21:27:23.515795: step 54840, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 60h:32m:27s remains)
INFO - root - 2017-12-10 21:27:31.357187: step 54850, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 60h:57m:55s remains)
INFO - root - 2017-12-10 21:27:39.129593: step 54860, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 62h:22m:03s remains)
INFO - root - 2017-12-10 21:27:47.083847: step 54870, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 59h:54m:06s remains)
INFO - root - 2017-12-10 21:27:54.775503: step 54880, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 60h:38m:29s remains)
INFO - root - 2017-12-10 21:28:02.679309: step 54890, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 60h:42m:42s remains)
INFO - root - 2017-12-10 21:28:10.303684: step 54900, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 60h:23m:39s remains)
2017-12-10 21:28:11.126457: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40906388 0.39381084 0.35989419 0.32592115 0.301675 0.30306637 0.32666963 0.35590413 0.37689152 0.39625949 0.42096993 0.44105986 0.45019746 0.45045987 0.43990985][0.36401767 0.3629306 0.34895885 0.33279228 0.3204692 0.32659921 0.35154828 0.38191751 0.40736908 0.43239173 0.45831454 0.47679383 0.484262 0.48569912 0.47987098][0.3128145 0.32468694 0.33421209 0.34223464 0.34822664 0.36423659 0.39433798 0.42711723 0.45623738 0.48515618 0.509865 0.52449512 0.52941114 0.53369623 0.53702551][0.30500659 0.32250544 0.3505998 0.38390025 0.41349411 0.44741863 0.48955035 0.52567655 0.55204016 0.57439446 0.58850372 0.5929656 0.59308219 0.60275823 0.62092304][0.35957509 0.3740494 0.41020074 0.46235567 0.51441127 0.56923807 0.62569779 0.66176105 0.6750105 0.6767869 0.66906571 0.65596414 0.648307 0.66521358 0.70259523][0.44725281 0.45582077 0.48983064 0.549503 0.61554044 0.68604445 0.75140339 0.78042716 0.77182978 0.74421716 0.70868522 0.67396307 0.65546626 0.67599684 0.72807586][0.51712358 0.52150923 0.54557985 0.59806317 0.66173571 0.73233891 0.79316348 0.80756754 0.77550745 0.72076052 0.661523 0.60849649 0.57893044 0.59628487 0.65120429][0.54820848 0.5467273 0.5522486 0.5803721 0.61949551 0.66765338 0.70646489 0.700137 0.64959049 0.579282 0.50945103 0.44906753 0.41346473 0.42338479 0.46980307][0.54993677 0.5372023 0.514927 0.50309438 0.49825415 0.50475192 0.50894183 0.48130855 0.42253226 0.35272956 0.28870663 0.23485036 0.20142524 0.20444034 0.23702446][0.53314573 0.50409287 0.44960928 0.39261985 0.33850369 0.29832536 0.266763 0.22357635 0.16809963 0.11241411 0.066053286 0.028387353 0.0041024555 0.0040459139 0.024448777][0.50795162 0.46112454 0.37829933 0.28397819 0.19082603 0.11461218 0.0576287 0.0085631339 -0.035236772 -0.070438594 -0.095659986 -0.11511583 -0.12804802 -0.12733731 -0.11370919][0.46023831 0.40019277 0.30128476 0.18855417 0.079090275 -0.010490237 -0.074332342 -0.11797649 -0.14571433 -0.16137697 -0.16941285 -0.17520438 -0.1792054 -0.17597151 -0.16408558][0.37156576 0.30748019 0.20918357 0.10060655 -0.00023553755 -0.079784892 -0.1329484 -0.16264014 -0.17402095 -0.17511638 -0.17272343 -0.17076539 -0.16887254 -0.16217476 -0.14848565][0.2371657 0.18023409 0.099069037 0.013856961 -0.059931215 -0.11403588 -0.14610389 -0.15796004 -0.15491246 -0.14621307 -0.13797389 -0.13117971 -0.12419136 -0.11288927 -0.095566414][0.077713288 0.034672603 -0.019200254 -0.070341393 -0.10873076 -0.13156773 -0.13954112 -0.13425608 -0.12054158 -0.1062153 -0.09461277 -0.083618507 -0.070671178 -0.053021953 -0.030261626]]...]
INFO - root - 2017-12-10 21:28:18.942879: step 54910, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 61h:22m:18s remains)
INFO - root - 2017-12-10 21:28:26.798810: step 54920, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 58h:53m:48s remains)
INFO - root - 2017-12-10 21:28:34.559476: step 54930, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 59h:37m:15s remains)
INFO - root - 2017-12-10 21:28:42.391635: step 54940, loss = 0.67, batch loss = 0.62 (9.9 examples/sec; 0.811 sec/batch; 62h:33m:46s remains)
INFO - root - 2017-12-10 21:28:50.195414: step 54950, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 59h:10m:12s remains)
INFO - root - 2017-12-10 21:28:57.891621: step 54960, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 60h:36m:30s remains)
INFO - root - 2017-12-10 21:29:05.781675: step 54970, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 60h:37m:00s remains)
INFO - root - 2017-12-10 21:29:13.593290: step 54980, loss = 0.68, batch loss = 0.62 (10.7 examples/sec; 0.745 sec/batch; 57h:24m:46s remains)
INFO - root - 2017-12-10 21:29:21.351232: step 54990, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 61h:09m:57s remains)
INFO - root - 2017-12-10 21:29:29.263579: step 55000, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 61h:34m:27s remains)
2017-12-10 21:29:30.092873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0083535081 0.028006952 0.084741838 0.15675426 0.23373264 0.29849148 0.33568275 0.34442148 0.33599147 0.3189781 0.29928449 0.27600238 0.2441269 0.1996692 0.14724222][-0.006778603 0.025164017 0.080892585 0.15690869 0.24122211 0.31278449 0.35371387 0.36287984 0.35369307 0.33679119 0.3188898 0.3002792 0.27461329 0.23510407 0.18345438][0.0018709222 0.024405176 0.071574286 0.14245224 0.22536343 0.29783693 0.34102884 0.35299787 0.34589243 0.33022013 0.31396395 0.30072626 0.2856324 0.25862634 0.2161783][0.013320412 0.026306763 0.06293343 0.12515533 0.20326869 0.27484566 0.32084015 0.33746582 0.333479 0.31818631 0.30176848 0.29273516 0.28824046 0.27540845 0.24510857][0.0253752 0.031947013 0.060581397 0.11458204 0.18635368 0.254536 0.30057129 0.31917116 0.31576747 0.29804784 0.27897683 0.27182832 0.276006 0.27675954 0.26040772][0.031918917 0.035234559 0.060186077 0.10943022 0.17673774 0.24270351 0.28985763 0.31160027 0.3091971 0.2870158 0.25935745 0.24467127 0.24619815 0.2504527 0.24279958][0.032629136 0.033803761 0.057862706 0.10612033 0.17270543 0.24062178 0.29366431 0.32315254 0.32463127 0.29741982 0.25505549 0.2207541 0.20427598 0.19837336 0.19139181][0.030576777 0.030158315 0.055122521 0.10489576 0.17231025 0.24236374 0.30071989 0.33719492 0.34232464 0.31050441 0.25331616 0.19686557 0.15674521 0.13372362 0.12214152][0.031657662 0.032125711 0.060508262 0.11462859 0.18414782 0.25407314 0.31166214 0.3474783 0.35116407 0.31527635 0.24901831 0.17709206 0.11734283 0.0770011 0.057302836][0.044405978 0.047629774 0.080327213 0.13941011 0.21048129 0.27672812 0.32624513 0.3522979 0.34760475 0.30669048 0.23693617 0.15828818 0.087079912 0.033638217 0.0049745259][0.072982073 0.07941521 0.11432497 0.17550041 0.24563563 0.30563593 0.34377423 0.35607669 0.33933967 0.29209676 0.22110905 0.14105542 0.064727269 0.0032633746 -0.032890618][0.11959299 0.1275271 0.15977758 0.2165038 0.279584 0.32844219 0.35151181 0.34697658 0.31545585 0.25989616 0.18813504 0.11063317 0.036327478 -0.024476144 -0.061122965][0.17705338 0.18415757 0.20789537 0.25182939 0.29963371 0.33107692 0.33517689 0.31160662 0.26413411 0.19971028 0.12895548 0.0600116 -0.002116478 -0.050505549 -0.077850714][0.2298512 0.23374957 0.24422733 0.26896518 0.29569983 0.30652907 0.29125062 0.25058493 0.19077131 0.12153877 0.055819403 0.00071241765 -0.042234592 -0.071135469 -0.084176868][0.25847641 0.25646424 0.25175053 0.25516865 0.25951022 0.25095931 0.2203805 0.16889554 0.10441432 0.037967961 -0.01770664 -0.057149172 -0.08136858 -0.092336357 -0.092847206]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 21:29:37.980243: step 55010, loss = 0.70, batch loss = 0.65 (9.6 examples/sec; 0.837 sec/batch; 64h:32m:04s remains)
INFO - root - 2017-12-10 21:29:45.799866: step 55020, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 59h:57m:30s remains)
INFO - root - 2017-12-10 21:29:53.670132: step 55030, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 61h:15m:02s remains)
INFO - root - 2017-12-10 21:30:01.356467: step 55040, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 59h:53m:04s remains)
INFO - root - 2017-12-10 21:30:09.259299: step 55050, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 60h:12m:22s remains)
INFO - root - 2017-12-10 21:30:17.147822: step 55060, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 60h:24m:12s remains)
INFO - root - 2017-12-10 21:30:24.815445: step 55070, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.788 sec/batch; 60h:44m:21s remains)
INFO - root - 2017-12-10 21:30:32.602073: step 55080, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 58h:52m:40s remains)
INFO - root - 2017-12-10 21:30:40.449644: step 55090, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 61h:35m:06s remains)
INFO - root - 2017-12-10 21:30:48.281751: step 55100, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 60h:45m:57s remains)
2017-12-10 21:30:49.137535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.06073546 -0.062422559 -0.063605934 -0.066352561 -0.068740562 -0.069302477 -0.068045884 -0.066286549 -0.06554912 -0.067147039 -0.0714385 -0.077713519 -0.08453466 -0.089833319 -0.090443343][-0.063172609 -0.066291876 -0.068130262 -0.068979837 -0.066157013 -0.058562841 -0.048551187 -0.039977808 -0.036699753 -0.041216824 -0.052763302 -0.067500338 -0.0808312 -0.090591192 -0.095769122][-0.064606115 -0.066460557 -0.064234257 -0.055184234 -0.03607497 -0.0079849251 0.02157153 0.043647263 0.049980089 0.036145426 0.0059640887 -0.030008452 -0.060421742 -0.080334023 -0.0902208][-0.063335329 -0.058802586 -0.044223052 -0.013285355 0.03643731 0.0989446 0.15825379 0.19803396 0.20417203 0.17053108 0.10711736 0.035005223 -0.024951534 -0.063327834 -0.081208818][-0.056434635 -0.039769754 -0.0042183078 0.059538573 0.15110883 0.25663313 0.34948182 0.40485445 0.40286642 0.33797303 0.22967322 0.11228984 0.016751504 -0.043837413 -0.071525924][-0.044336092 -0.011843437 0.049222108 0.14963117 0.28409097 0.42889071 0.54716551 0.60738927 0.58687526 0.48429754 0.3307983 0.17308696 0.048590448 -0.029318811 -0.064754173][-0.030431397 0.016687203 0.09977562 0.22855149 0.39157531 0.556262 0.67952716 0.72848052 0.68305945 0.54890734 0.36687747 0.19004174 0.05577952 -0.026196923 -0.062405366][-0.020548036 0.033767011 0.12634841 0.26325911 0.42786616 0.58337438 0.68732488 0.71200842 0.64462513 0.49763611 0.31558615 0.14925011 0.029571939 -0.039593935 -0.066747345][-0.020126069 0.029954305 0.11414003 0.23399396 0.3708849 0.49056742 0.55830973 0.55605733 0.47950527 0.34521011 0.19362441 0.065050572 -0.01977068 -0.062948525 -0.074158162][-0.0296005 0.0062018186 0.067289084 0.15185951 0.24331069 0.3153832 0.34487423 0.32384381 0.25442812 0.15300089 0.050080441 -0.02800446 -0.071202181 -0.086016126 -0.081558473][-0.044217538 -0.026623601 0.0063155349 0.051332697 0.096654341 0.12607828 0.12757316 0.1004568 0.04983205 -0.011421502 -0.06453748 -0.096494116 -0.10549907 -0.099444978 -0.08543288][-0.057914525 -0.055884644 -0.046424348 -0.032710813 -0.021019584 -0.018810388 -0.030290853 -0.052910447 -0.081740342 -0.10864707 -0.12482852 -0.12660606 -0.11635803 -0.10075743 -0.084372506][-0.06700933 -0.074092194 -0.078260079 -0.081929281 -0.087148257 -0.095851675 -0.10833427 -0.12186754 -0.13320532 -0.13840863 -0.13497654 -0.1237886 -0.10810661 -0.092275612 -0.078745134][-0.070798479 -0.080215447 -0.088157289 -0.096573278 -0.10534046 -0.11404207 -0.12184983 -0.12705763 -0.12816465 -0.12408243 -0.11512769 -0.10321902 -0.090754978 -0.079972848 -0.071691655][-0.070837393 -0.078310944 -0.0838996 -0.089774482 -0.095347874 -0.099923238 -0.1028621 -0.10345528 -0.10122896 -0.0963161 -0.089569874 -0.082474642 -0.076125368 -0.071147636 -0.067643791]]...]
INFO - root - 2017-12-10 21:30:57.087799: step 55110, loss = 0.68, batch loss = 0.62 (9.6 examples/sec; 0.837 sec/batch; 64h:28m:38s remains)
INFO - root - 2017-12-10 21:31:04.728348: step 55120, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 61h:36m:36s remains)
INFO - root - 2017-12-10 21:31:12.611408: step 55130, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 61h:01m:29s remains)
INFO - root - 2017-12-10 21:31:20.427332: step 55140, loss = 0.71, batch loss = 0.66 (9.9 examples/sec; 0.805 sec/batch; 62h:00m:58s remains)
INFO - root - 2017-12-10 21:31:28.242002: step 55150, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 59h:24m:02s remains)
INFO - root - 2017-12-10 21:31:35.950865: step 55160, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 58h:42m:18s remains)
INFO - root - 2017-12-10 21:31:43.740905: step 55170, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 60h:27m:16s remains)
INFO - root - 2017-12-10 21:31:51.669881: step 55180, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 61h:52m:47s remains)
INFO - root - 2017-12-10 21:31:59.484214: step 55190, loss = 0.71, batch loss = 0.66 (10.5 examples/sec; 0.764 sec/batch; 58h:52m:54s remains)
INFO - root - 2017-12-10 21:32:07.119077: step 55200, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.801 sec/batch; 61h:39m:53s remains)
2017-12-10 21:32:07.942627: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.077821031 0.13359377 0.17120704 0.17419218 0.14581889 0.097538792 0.04196541 -0.0042886706 -0.031404641 -0.041666429 -0.047111709 -0.05556418 -0.059658881 -0.049643293 -0.02292493][0.11990482 0.18998042 0.24105294 0.25515372 0.23656379 0.19819693 0.15142864 0.11115343 0.084532641 0.067365848 0.047991328 0.021473302 0.0011714669 0.0012330476 0.024195163][0.13898525 0.21757552 0.2810674 0.31365132 0.32187435 0.31817159 0.30639064 0.29080012 0.26941738 0.23704612 0.1899129 0.13326153 0.088399917 0.072363667 0.085896395][0.13039652 0.21077339 0.2856147 0.34528378 0.39792609 0.45088136 0.49335057 0.51166236 0.49119523 0.42988479 0.340285 0.24477054 0.17283081 0.14082083 0.14469914][0.11040787 0.19249527 0.28426161 0.38225827 0.49097827 0.60613614 0.69908428 0.7387104 0.70239639 0.59801883 0.45915681 0.32585463 0.23176987 0.18850087 0.18532945][0.10612831 0.20180398 0.32320955 0.46750447 0.6275093 0.78338522 0.89383388 0.92242533 0.85103387 0.70093095 0.52127856 0.36208615 0.25536534 0.20676225 0.20023765][0.12639272 0.24785987 0.40749204 0.59497821 0.78590333 0.94654715 1.0333252 1.0195252 0.90288031 0.716665 0.51523608 0.34757882 0.23965146 0.19172895 0.18464562][0.15498678 0.30029738 0.48714983 0.69406503 0.882089 1.0117741 1.0481297 0.98340178 0.83271575 0.6344716 0.43740138 0.2811515 0.1847502 0.14525452 0.14327866][0.17804791 0.33308768 0.52079093 0.71081895 0.8603555 0.93647951 0.91981304 0.81983548 0.66110718 0.47930086 0.31099018 0.18346298 0.11023974 0.087617844 0.097655267][0.17691913 0.32264039 0.48481122 0.62983268 0.72211671 0.74322575 0.68907964 0.57863575 0.43656138 0.29092178 0.16487937 0.074341036 0.029594285 0.027247377 0.051179308][0.13976398 0.257927 0.37727892 0.46698564 0.50411755 0.484592 0.41442746 0.31448072 0.20414005 0.10305045 0.023112943 -0.029021097 -0.045395397 -0.029607156 0.0037120592][0.070998237 0.1497898 0.22117478 0.2618379 0.26138416 0.22411349 0.1590863 0.083653189 0.010453198 -0.048140291 -0.088016853 -0.10871477 -0.1041532 -0.078997806 -0.046117742][-0.0086043095 0.030072374 0.060706545 0.068497546 0.051683165 0.016537789 -0.029835528 -0.075987108 -0.11513218 -0.14018187 -0.15131904 -0.15100111 -0.13506965 -0.10941096 -0.084382147][-0.069021 -0.058585186 -0.05271047 -0.059958063 -0.078406937 -0.10317679 -0.13024968 -0.15345563 -0.169586 -0.17477036 -0.17072573 -0.16046579 -0.14187497 -0.12121995 -0.10595209][-0.10240647 -0.10621715 -0.11063281 -0.12103971 -0.13503891 -0.14953837 -0.16288844 -0.17246947 -0.17678705 -0.17373306 -0.16517048 -0.15384679 -0.13918431 -0.12617183 -0.11936346]]...]
INFO - root - 2017-12-10 21:32:15.801666: step 55210, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 60h:43m:49s remains)
INFO - root - 2017-12-10 21:32:23.606451: step 55220, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 58h:42m:00s remains)
INFO - root - 2017-12-10 21:32:31.458930: step 55230, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 58h:57m:27s remains)
INFO - root - 2017-12-10 21:32:39.305413: step 55240, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 60h:01m:46s remains)
INFO - root - 2017-12-10 21:32:46.845448: step 55250, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.770 sec/batch; 59h:19m:31s remains)
INFO - root - 2017-12-10 21:32:54.624316: step 55260, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 59h:43m:34s remains)
INFO - root - 2017-12-10 21:33:02.497969: step 55270, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 60h:36m:43s remains)
INFO - root - 2017-12-10 21:33:10.190493: step 55280, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 60h:30m:55s remains)
INFO - root - 2017-12-10 21:33:18.103000: step 55290, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 60h:31m:15s remains)
INFO - root - 2017-12-10 21:33:25.886648: step 55300, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 60h:15m:36s remains)
2017-12-10 21:33:26.683027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0049486132 -0.0088505084 -0.0083702523 0.002324024 0.022632219 0.046200056 0.063266292 0.066386722 0.056502942 0.040453631 0.027058251 0.019953152 0.016244771 0.011092708 0.00082030112][0.01838734 0.012468178 0.012650313 0.027457891 0.056346729 0.090726376 0.11687348 0.12358634 0.11132423 0.089409709 0.070200965 0.059318483 0.053292178 0.045067057 0.029183518][0.053110056 0.043485783 0.040883821 0.057155229 0.092549779 0.13648365 0.17128278 0.18156275 0.16702759 0.13933662 0.11451063 0.10067797 0.093704611 0.08408007 0.063712552][0.096160151 0.081374347 0.073334 0.087854587 0.12652893 0.17743404 0.219353 0.23278104 0.2160897 0.18229899 0.15143073 0.1346081 0.127211 0.11772343 0.094726086][0.14266114 0.12241538 0.10718927 0.11747688 0.15677781 0.21221535 0.25929928 0.27482715 0.25544333 0.21496956 0.17716621 0.15624014 0.14777692 0.13883048 0.11505383][0.18639703 0.16106892 0.13751328 0.14110196 0.17827661 0.23581184 0.28641888 0.30361319 0.28184265 0.23522118 0.19029844 0.16440342 0.153826 0.14513056 0.12196926][0.22275306 0.19325101 0.16104999 0.15543994 0.18699642 0.24335988 0.29566616 0.31468228 0.29237086 0.24204445 0.19139287 0.16061567 0.14747337 0.13863859 0.11679167][0.24350303 0.21244101 0.17401393 0.15923797 0.18321134 0.23586243 0.28843933 0.31006727 0.29014474 0.23972632 0.18598585 0.15102974 0.13480504 0.12479074 0.10360887][0.24142474 0.21215761 0.17210434 0.15199958 0.17042631 0.22017395 0.27376863 0.29980662 0.28523341 0.23833549 0.18453941 0.14657667 0.12700534 0.11442427 0.092175722][0.22063617 0.19354087 0.1545549 0.13261509 0.14760569 0.19500084 0.24963731 0.28063038 0.27342525 0.23391213 0.18441309 0.14688727 0.12559655 0.11062215 0.086269058][0.19307117 0.16673738 0.12923096 0.10622361 0.11681371 0.15905668 0.2113928 0.24508482 0.2455489 0.21663754 0.17628452 0.14369917 0.12357089 0.10759249 0.081454776][0.16927159 0.14407849 0.10894763 0.08499749 0.08922226 0.12214155 0.16685075 0.19869211 0.20444196 0.18643233 0.15753381 0.13278154 0.11614598 0.10068099 0.073869489][0.1538332 0.13147314 0.10051308 0.077063479 0.075064227 0.096531153 0.12928908 0.1543521 0.16089128 0.15058565 0.13185979 0.11527682 0.10342094 0.089896657 0.064068213][0.14788629 0.12854508 0.10196119 0.080154389 0.073580973 0.084378883 0.10417929 0.11978979 0.1234576 0.11712909 0.1062875 0.09760274 0.09158276 0.0815705 0.058245774][0.14443426 0.12598915 0.10204067 0.081755191 0.072378054 0.075403094 0.084961124 0.092512034 0.093269564 0.089805976 0.086085156 0.085724935 0.0866793 0.081236072 0.060770411]]...]
INFO - root - 2017-12-10 21:33:34.522489: step 55310, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 60h:08m:58s remains)
INFO - root - 2017-12-10 21:33:42.372323: step 55320, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 60h:58m:53s remains)
INFO - root - 2017-12-10 21:33:50.210946: step 55330, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 59h:18m:18s remains)
INFO - root - 2017-12-10 21:33:57.890004: step 55340, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 62h:34m:30s remains)
INFO - root - 2017-12-10 21:34:05.771250: step 55350, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 60h:04m:51s remains)
INFO - root - 2017-12-10 21:34:13.299013: step 55360, loss = 0.71, batch loss = 0.65 (10.9 examples/sec; 0.731 sec/batch; 56h:14m:43s remains)
INFO - root - 2017-12-10 21:34:21.171402: step 55370, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.812 sec/batch; 62h:29m:41s remains)
INFO - root - 2017-12-10 21:34:29.109078: step 55380, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 61h:15m:53s remains)
INFO - root - 2017-12-10 21:34:36.969544: step 55390, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 61h:29m:33s remains)
INFO - root - 2017-12-10 21:34:44.805542: step 55400, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.766 sec/batch; 58h:57m:26s remains)
2017-12-10 21:34:45.596016: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2342698 0.21423559 0.16633956 0.10332154 0.048862673 0.029037565 0.0560097 0.12737626 0.22562842 0.32054844 0.38034895 0.38683081 0.34807843 0.28254378 0.2135637][0.20617938 0.16493861 0.11030461 0.056353625 0.017213799 0.0092180939 0.039176621 0.10412028 0.18923147 0.26872778 0.31555122 0.31466413 0.2738902 0.21022892 0.14455143][0.17860538 0.12263366 0.070536241 0.036166273 0.02205196 0.031142408 0.061890177 0.11103923 0.16940846 0.21957214 0.2443192 0.23611298 0.20213877 0.15446629 0.10648025][0.17313445 0.11408748 0.073536761 0.066558167 0.084432244 0.11525407 0.14665866 0.1745941 0.19713175 0.20811108 0.20308027 0.18497358 0.16150028 0.13811757 0.11811545][0.20251633 0.14777473 0.12232006 0.14471799 0.19770546 0.25486103 0.29095328 0.29862979 0.28242242 0.24908802 0.2093288 0.17852049 0.16584171 0.17033452 0.18404223][0.24420378 0.20029493 0.19376943 0.24553502 0.33237523 0.41605514 0.46036044 0.45262295 0.40197805 0.32816681 0.25660092 0.21281867 0.20718159 0.23373316 0.27462554][0.26291573 0.236888 0.25300622 0.33193305 0.4465262 0.55101627 0.60263991 0.58480006 0.51040614 0.40940779 0.31765652 0.26596376 0.26461381 0.3037951 0.35814324][0.23167498 0.22223106 0.2564449 0.35185412 0.48039711 0.59455764 0.65101761 0.63237786 0.55351788 0.44940451 0.36035815 0.31666756 0.32479677 0.37105086 0.42630678][0.14778514 0.146077 0.1863317 0.28121573 0.40533808 0.51610219 0.57557911 0.56804776 0.50738233 0.42799747 0.36839846 0.35261917 0.38026425 0.43355858 0.48350087][0.036190949 0.033244021 0.068140395 0.14739692 0.25142297 0.34765762 0.40806094 0.419781 0.39362031 0.35948402 0.34884605 0.37615073 0.43298149 0.49788168 0.54350281][-0.061980762 -0.0705465 -0.049159359 0.0040984577 0.076984257 0.14937262 0.20543052 0.23594606 0.24779484 0.26373494 0.30573097 0.3775574 0.46365908 0.54014969 0.58238012][-0.11284183 -0.12650748 -0.12054725 -0.0960334 -0.058646586 -0.016388413 0.026904894 0.066421591 0.10543153 0.15845561 0.23767173 0.33750692 0.4378702 0.51529962 0.55019265][-0.11751262 -0.13199717 -0.13613196 -0.13309114 -0.12444219 -0.10989045 -0.084784083 -0.050040912 -0.0050056195 0.058414169 0.14530557 0.2448546 0.33633277 0.40035975 0.42436123][-0.10131773 -0.11281208 -0.11975512 -0.12650889 -0.13338701 -0.13699156 -0.12956363 -0.10988302 -0.078215748 -0.029616598 0.037649177 0.11250487 0.17760444 0.22006607 0.23394036][-0.0817883 -0.088963434 -0.094360568 -0.10262509 -0.11448649 -0.12686166 -0.13257121 -0.12953506 -0.11770753 -0.093349062 -0.055500265 -0.012473264 0.023457242 0.045534372 0.053079337]]...]
INFO - root - 2017-12-10 21:34:53.384640: step 55410, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 60h:38m:25s remains)
INFO - root - 2017-12-10 21:35:01.227916: step 55420, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.773 sec/batch; 59h:29m:15s remains)
INFO - root - 2017-12-10 21:35:08.929113: step 55430, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 60h:14m:27s remains)
INFO - root - 2017-12-10 21:35:16.618234: step 55440, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.754 sec/batch; 57h:59m:29s remains)
INFO - root - 2017-12-10 21:35:24.428371: step 55450, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 60h:49m:13s remains)
INFO - root - 2017-12-10 21:35:32.261414: step 55460, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 60h:26m:34s remains)
INFO - root - 2017-12-10 21:35:40.033142: step 55470, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 60h:38m:58s remains)
INFO - root - 2017-12-10 21:35:47.857174: step 55480, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 61h:45m:32s remains)
INFO - root - 2017-12-10 21:35:55.661358: step 55490, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 59h:04m:31s remains)
INFO - root - 2017-12-10 21:36:03.457376: step 55500, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 59h:53m:21s remains)
2017-12-10 21:36:04.324661: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.39277607 0.37844539 0.34930623 0.33351952 0.32802761 0.32196906 0.30890024 0.2775439 0.21757291 0.14252199 0.074533314 0.028829766 0.0086653139 0.0045720753 0.01171595][0.39608556 0.37570643 0.34259731 0.32852802 0.33151698 0.33836147 0.33744064 0.31550622 0.26420748 0.19473496 0.12320561 0.064518712 0.027703669 0.010594803 0.011064438][0.35125095 0.32989377 0.30228484 0.29583761 0.30857214 0.32734457 0.3394435 0.33232656 0.29928809 0.2458986 0.17836525 0.10977937 0.0547866 0.021138612 0.01164579][0.28504321 0.27090633 0.25817525 0.26484832 0.28717068 0.31276003 0.33104414 0.33286127 0.31337592 0.27443466 0.21514644 0.14461043 0.078511678 0.032052893 0.013587167][0.21248046 0.2123065 0.21935624 0.24189189 0.27456883 0.30592677 0.32696992 0.3311328 0.31525442 0.28103223 0.2259637 0.15646622 0.0868751 0.034807254 0.01168065][0.15512361 0.17179349 0.19702257 0.23358241 0.27739656 0.31930903 0.34955943 0.35888147 0.34144574 0.29861566 0.23271041 0.15511814 0.081968039 0.028968027 0.00526511][0.1393057 0.17185618 0.20744297 0.24965532 0.29903615 0.35068351 0.39354265 0.41079068 0.389045 0.32823855 0.24140169 0.15070784 0.075468086 0.026013613 0.0056389165][0.16250286 0.20601687 0.244774 0.28430325 0.32879487 0.37830746 0.42293319 0.44030276 0.4107385 0.33408266 0.23404352 0.14021537 0.071213976 0.030986825 0.017496217][0.20897618 0.25613418 0.29174954 0.32292226 0.35444894 0.38934919 0.4207007 0.42632484 0.38601527 0.30206034 0.20352133 0.11971276 0.064611182 0.03745399 0.033287629][0.25150311 0.29342741 0.32051572 0.34180465 0.36007038 0.37745416 0.38957271 0.37946311 0.3313008 0.2502104 0.16391005 0.096240588 0.056437615 0.041958857 0.046760492][0.27597424 0.30413425 0.31619868 0.324992 0.33198011 0.33624122 0.33393943 0.31345227 0.2644999 0.1937086 0.12409118 0.073335417 0.047570612 0.044165153 0.056322765][0.26741806 0.2785196 0.27226749 0.26540813 0.26013762 0.25437492 0.24478382 0.2229954 0.18211326 0.12834409 0.079598665 0.04809754 0.037334196 0.044467743 0.062326502][0.21043006 0.20704028 0.18746611 0.16782029 0.15191941 0.13957493 0.12883216 0.1132288 0.087332927 0.055656895 0.030609526 0.019325543 0.022735531 0.038157806 0.058885325][0.12951504 0.1173941 0.093353443 0.068864778 0.048433669 0.034150314 0.025705641 0.018577214 0.0080329552 -0.0035515921 -0.0091336938 -0.00575855 0.0058304016 0.024063213 0.043773461][0.058083147 0.042429414 0.019945685 -0.0026574738 -0.021406153 -0.033560488 -0.038682535 -0.039279129 -0.039133348 -0.037607 -0.031910583 -0.021951707 -0.0090287914 0.0067733387 0.022377338]]...]
INFO - root - 2017-12-10 21:36:12.092748: step 55510, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 59h:47m:24s remains)
INFO - root - 2017-12-10 21:36:19.562574: step 55520, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 58h:44m:18s remains)
INFO - root - 2017-12-10 21:36:27.447553: step 55530, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 60h:10m:15s remains)
INFO - root - 2017-12-10 21:36:35.305135: step 55540, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 61h:28m:58s remains)
INFO - root - 2017-12-10 21:36:43.079141: step 55550, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.758 sec/batch; 58h:20m:47s remains)
INFO - root - 2017-12-10 21:36:50.898109: step 55560, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.810 sec/batch; 62h:16m:42s remains)
INFO - root - 2017-12-10 21:36:58.839709: step 55570, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.819 sec/batch; 62h:59m:40s remains)
INFO - root - 2017-12-10 21:37:06.750419: step 55580, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 60h:46m:30s remains)
INFO - root - 2017-12-10 21:37:14.496154: step 55590, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 58h:43m:26s remains)
INFO - root - 2017-12-10 21:37:22.236842: step 55600, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.789 sec/batch; 60h:40m:11s remains)
2017-12-10 21:37:23.149465: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34975657 0.36853591 0.37312663 0.37047109 0.35151121 0.3204253 0.28336877 0.24553199 0.21409298 0.19665992 0.19135341 0.1847249 0.16931111 0.15445915 0.14724803][0.33235416 0.34776372 0.35077175 0.35268411 0.34501746 0.32752416 0.2993674 0.26439604 0.23438317 0.21940345 0.21660735 0.21205111 0.20009574 0.19201823 0.19457875][0.28557608 0.29940894 0.3052541 0.31743449 0.32829109 0.33016023 0.31413087 0.28464058 0.25770953 0.24634854 0.24736045 0.24731216 0.24198726 0.24363334 0.25801706][0.24589415 0.26641878 0.28501788 0.31646261 0.35292825 0.37760597 0.37458083 0.35069394 0.32566017 0.31523135 0.3163788 0.3168219 0.31458595 0.32368293 0.34785333][0.23289259 0.27053863 0.31218106 0.36972484 0.434118 0.48050353 0.48909909 0.4692584 0.44307852 0.42886367 0.42358333 0.4155933 0.40601379 0.41212833 0.43833065][0.25942734 0.31719491 0.38358456 0.46593755 0.55318713 0.6158306 0.634357 0.61899227 0.59146708 0.57213873 0.55755472 0.53557169 0.50933808 0.49967042 0.51746631][0.31354713 0.38506719 0.46650019 0.56281048 0.66148436 0.73281032 0.76041758 0.75258273 0.72755742 0.70579726 0.68386722 0.64982671 0.60654336 0.57703227 0.581921][0.36119613 0.43551567 0.51821268 0.61371458 0.70971835 0.780788 0.81541693 0.81910944 0.80319393 0.78479958 0.7612555 0.72301096 0.67035949 0.62587076 0.61988562][0.37118354 0.43884963 0.51100796 0.59196538 0.67204589 0.73371655 0.77124274 0.78758556 0.78593743 0.77718914 0.75971669 0.72707582 0.675421 0.62535197 0.614455][0.34116408 0.39345869 0.44562775 0.50200611 0.55723113 0.60255581 0.63730973 0.66288829 0.67507231 0.67782056 0.67016989 0.64834887 0.60420609 0.55575252 0.54431826][0.27348205 0.30645505 0.33597207 0.36605453 0.39553633 0.42308804 0.45133379 0.48083863 0.50235754 0.5141086 0.51549321 0.50445384 0.46991456 0.42718318 0.41617221][0.17526643 0.18999256 0.20051661 0.20954219 0.21871836 0.23116529 0.25073853 0.27781683 0.30151406 0.31671941 0.32288733 0.31892672 0.29346117 0.25901002 0.24888039][0.064049877 0.0656547 0.06473764 0.061493304 0.058568858 0.060521822 0.07085596 0.090015538 0.10851038 0.12064556 0.12589128 0.12377606 0.10497005 0.078787066 0.06937708][-0.034881495 -0.039981119 -0.045451142 -0.053057864 -0.060455691 -0.064047195 -0.061588977 -0.052690621 -0.0435296 -0.038276691 -0.037330721 -0.040836882 -0.055093806 -0.074162044 -0.082625389][-0.099990562 -0.10789619 -0.11390281 -0.12129388 -0.12850036 -0.13364652 -0.13591267 -0.13507372 -0.13405225 -0.13489075 -0.13771494 -0.14244871 -0.15262564 -0.16533841 -0.17198704]]...]
INFO - root - 2017-12-10 21:37:30.812588: step 55610, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.753 sec/batch; 57h:57m:03s remains)
INFO - root - 2017-12-10 21:37:38.680849: step 55620, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 61h:03m:30s remains)
INFO - root - 2017-12-10 21:37:46.557063: step 55630, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 61h:50m:02s remains)
INFO - root - 2017-12-10 21:37:54.349879: step 55640, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 59h:16m:11s remains)
INFO - root - 2017-12-10 21:38:02.019982: step 55650, loss = 0.69, batch loss = 0.63 (10.7 examples/sec; 0.749 sec/batch; 57h:37m:40s remains)
INFO - root - 2017-12-10 21:38:09.794465: step 55660, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 58h:45m:36s remains)
INFO - root - 2017-12-10 21:38:17.662003: step 55670, loss = 0.67, batch loss = 0.61 (9.8 examples/sec; 0.817 sec/batch; 62h:49m:46s remains)
INFO - root - 2017-12-10 21:38:25.326463: step 55680, loss = 0.68, batch loss = 0.63 (10.7 examples/sec; 0.749 sec/batch; 57h:36m:08s remains)
INFO - root - 2017-12-10 21:38:33.269004: step 55690, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 60h:53m:14s remains)
INFO - root - 2017-12-10 21:38:40.966761: step 55700, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.759 sec/batch; 58h:19m:18s remains)
2017-12-10 21:38:41.789254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.079751268 -0.08226493 -0.0815365 -0.078852065 -0.075473346 -0.072389089 -0.070074514 -0.069269232 -0.070029907 -0.071416736 -0.073147632 -0.075112708 -0.07723099 -0.077718481 -0.077464685][-0.053802896 -0.05536649 -0.051279642 -0.04338659 -0.034137949 -0.025682168 -0.019621456 -0.017623421 -0.019258192 -0.022086503 -0.025189562 -0.029091215 -0.034404065 -0.039114166 -0.043221574][-0.0020088884 -0.00031121064 0.010511135 0.028044892 0.047685467 0.064787522 0.075726382 0.077153519 0.070901431 0.062691264 0.055028003 0.046826176 0.036090754 0.02509987 0.015193542][0.06562154 0.075698957 0.09923844 0.13234799 0.16668589 0.19372618 0.20754173 0.20351093 0.18596685 0.16586952 0.1487042 0.13324262 0.11569585 0.097809806 0.082241945][0.13665345 0.16194591 0.20480748 0.25789356 0.30707133 0.33957681 0.34926358 0.33248511 0.29681379 0.25897846 0.22833426 0.2043207 0.18116605 0.15904482 0.14097039][0.19339068 0.23681957 0.30083102 0.37371296 0.43382582 0.46482307 0.46348703 0.42969379 0.37401247 0.31730524 0.27256957 0.24104995 0.21580195 0.1943942 0.17867368][0.2223397 0.27943966 0.35766107 0.44232318 0.50519365 0.52792948 0.51164567 0.46180221 0.39147815 0.32287967 0.27054012 0.23763731 0.21641465 0.20124577 0.19246198][0.22043024 0.28000781 0.35736886 0.43911666 0.49447224 0.50524116 0.47555324 0.41680691 0.34299007 0.2749247 0.22590077 0.19980921 0.18844938 0.18352307 0.18457757][0.19490834 0.24528925 0.307163 0.37255073 0.41342476 0.41370949 0.37753454 0.32013887 0.25479084 0.19868965 0.16221668 0.14828 0.14872546 0.1540188 0.16508085][0.15541774 0.18881854 0.22745533 0.26991 0.29444203 0.28866509 0.25421417 0.20663749 0.15796399 0.12075191 0.10122492 0.10006565 0.10998116 0.12293199 0.14327505][0.11077067 0.12629212 0.14368206 0.16574165 0.17753495 0.16998389 0.14223182 0.10786375 0.0776016 0.059171084 0.054707661 0.062562943 0.077068374 0.094005063 0.12252957][0.073057175 0.077743657 0.082988456 0.092519425 0.0965746 0.088132404 0.065896079 0.0415368 0.024978459 0.019903062 0.024973741 0.036465149 0.050184924 0.0667692 0.10064267][0.051561005 0.055079147 0.05745542 0.061483916 0.060982667 0.050484844 0.029839596 0.0098161492 0.00015994263 0.0015852846 0.010277975 0.020847658 0.03014343 0.042897426 0.077880748][0.055370569 0.063617609 0.066353045 0.065653004 0.058831558 0.04293764 0.019714937 -0.00014445759 -0.0074672075 -0.003673414 0.0063309176 0.015860289 0.02165843 0.030525038 0.063366346][0.084853053 0.09761668 0.098291755 0.089485563 0.072502874 0.04799585 0.019416383 -0.0028569223 -0.010032604 -0.0045926771 0.0084181586 0.020845769 0.027423855 0.035056494 0.0642962]]...]
INFO - root - 2017-12-10 21:38:49.520217: step 55710, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 60h:15m:03s remains)
INFO - root - 2017-12-10 21:38:57.317537: step 55720, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 59h:07m:25s remains)
INFO - root - 2017-12-10 21:39:05.223980: step 55730, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.760 sec/batch; 58h:27m:21s remains)
INFO - root - 2017-12-10 21:39:13.082249: step 55740, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 61h:45m:57s remains)
INFO - root - 2017-12-10 21:39:20.928840: step 55750, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.777 sec/batch; 59h:45m:13s remains)
INFO - root - 2017-12-10 21:39:28.561808: step 55760, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 59h:50m:38s remains)
INFO - root - 2017-12-10 21:39:36.389146: step 55770, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 58h:32m:30s remains)
INFO - root - 2017-12-10 21:39:44.092870: step 55780, loss = 0.70, batch loss = 0.65 (11.6 examples/sec; 0.689 sec/batch; 52h:59m:58s remains)
INFO - root - 2017-12-10 21:39:51.872769: step 55790, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 62h:11m:58s remains)
INFO - root - 2017-12-10 21:39:59.782440: step 55800, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 60h:29m:23s remains)
2017-12-10 21:40:00.585648: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.46576589 0.44070566 0.42196333 0.4109934 0.40188918 0.38251507 0.35282347 0.32015231 0.30079532 0.31065047 0.35108662 0.40897742 0.468679 0.50810069 0.5206266][0.4702518 0.45866469 0.45264754 0.44628969 0.43121725 0.40039712 0.36023378 0.31807098 0.28682297 0.28536502 0.31780809 0.36975539 0.42266202 0.45846894 0.4739767][0.43032581 0.44035086 0.45690706 0.46547353 0.45400697 0.42106912 0.37736574 0.3280164 0.28230891 0.26174036 0.27322176 0.30384329 0.3365798 0.35919204 0.37229851][0.36812946 0.40262908 0.44580781 0.47743574 0.48280275 0.46370417 0.43026057 0.38305134 0.3271654 0.28385028 0.2635766 0.25842205 0.25710875 0.25515807 0.25673884][0.30683857 0.35941139 0.42121175 0.47224367 0.49940875 0.50664294 0.4986465 0.46815062 0.41367733 0.3525697 0.29756257 0.24787965 0.20240073 0.16695702 0.14946891][0.26657459 0.32732728 0.3942565 0.45262709 0.49494028 0.52784693 0.54976714 0.54516315 0.50396228 0.4366712 0.35562602 0.26637653 0.1786579 0.10871546 0.0687419][0.2845335 0.34477478 0.4026055 0.45091665 0.49136439 0.5343622 0.57590759 0.59392935 0.57042056 0.50852555 0.416939 0.30454329 0.18846448 0.093126178 0.034068216][0.35267609 0.40516335 0.44473636 0.4714036 0.49515358 0.530474 0.57466263 0.60255325 0.59189212 0.54083413 0.45356226 0.33851987 0.21474461 0.11007772 0.040920731][0.42713904 0.46944234 0.49041513 0.49542013 0.49898654 0.51730764 0.54981136 0.57190859 0.56089383 0.51440334 0.43506679 0.32991827 0.21554317 0.11770444 0.050348788][0.4789069 0.51492327 0.5248881 0.51598096 0.50177217 0.49862874 0.50860071 0.51090664 0.48564327 0.43468282 0.36261225 0.27512246 0.18291903 0.10501854 0.049857378][0.49322483 0.52918059 0.53910553 0.528433 0.50499785 0.48328358 0.46808705 0.44451934 0.39906648 0.33974648 0.27500093 0.20892009 0.14377885 0.08952152 0.048029102][0.46907222 0.5086748 0.52775192 0.52776736 0.50800794 0.47805923 0.44430646 0.39855617 0.3362712 0.27088669 0.21401434 0.16777694 0.12577738 0.089379281 0.054963008][0.41517049 0.46154779 0.49651521 0.51777107 0.51573539 0.49298152 0.45521966 0.4010261 0.33460414 0.27133033 0.22344927 0.19107905 0.16128312 0.12932622 0.089542143][0.34613582 0.40092942 0.45391759 0.50054914 0.5243746 0.52136713 0.4953073 0.44934833 0.39321145 0.340615 0.30159998 0.2743873 0.2429305 0.20055769 0.14387202][0.27945393 0.33990073 0.40548572 0.4712244 0.51691777 0.53344 0.52363992 0.49309221 0.45378637 0.41558096 0.38497037 0.35923707 0.32135329 0.26527891 0.19230689]]...]
INFO - root - 2017-12-10 21:40:08.323723: step 55810, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 59h:35m:07s remains)
INFO - root - 2017-12-10 21:40:16.186962: step 55820, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 61h:46m:00s remains)
INFO - root - 2017-12-10 21:40:23.984467: step 55830, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 60h:33m:15s remains)
INFO - root - 2017-12-10 21:40:31.606794: step 55840, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 59h:44m:13s remains)
INFO - root - 2017-12-10 21:40:39.356869: step 55850, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 58h:27m:02s remains)
INFO - root - 2017-12-10 21:40:47.149684: step 55860, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.767 sec/batch; 58h:58m:26s remains)
INFO - root - 2017-12-10 21:40:54.822264: step 55870, loss = 0.69, batch loss = 0.63 (11.2 examples/sec; 0.711 sec/batch; 54h:39m:52s remains)
INFO - root - 2017-12-10 21:41:02.698104: step 55880, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 61h:54m:54s remains)
INFO - root - 2017-12-10 21:41:10.496087: step 55890, loss = 0.70, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 58h:09m:01s remains)
INFO - root - 2017-12-10 21:41:18.263451: step 55900, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 61h:13m:46s remains)
2017-12-10 21:41:19.117010: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.087673455 0.15300207 0.212155 0.24769209 0.2562027 0.24459697 0.21592622 0.17968041 0.16059971 0.17441979 0.20754416 0.23588957 0.2473556 0.23774813 0.19773415][0.10563304 0.17761548 0.24216457 0.28131405 0.29340973 0.28648812 0.26255891 0.23130628 0.21940711 0.24377981 0.28665102 0.32134023 0.33518037 0.32393768 0.27634022][0.11373243 0.19102634 0.2623553 0.31038561 0.33447888 0.34190649 0.33244038 0.31303743 0.30621102 0.32685128 0.35887885 0.38093439 0.38360819 0.36447707 0.31259689][0.1198156 0.2074752 0.29395452 0.36191565 0.40912881 0.438961 0.44853145 0.44116753 0.43280849 0.43418387 0.43451008 0.42399129 0.39977157 0.36229327 0.30333185][0.12730691 0.22768815 0.33415195 0.43125966 0.51263809 0.57270193 0.60582715 0.61152893 0.59853846 0.57181352 0.5298506 0.47745243 0.41771182 0.35374981 0.2816968][0.14547545 0.25878507 0.38515773 0.51396567 0.63344342 0.72639203 0.78416616 0.80265081 0.78314078 0.72617346 0.64242005 0.55130404 0.45874795 0.36822525 0.2801649][0.16766138 0.2914224 0.43315539 0.58737588 0.7378273 0.85858327 0.93746811 0.96500045 0.93698746 0.85463214 0.74093848 0.62510508 0.512392 0.40575528 0.30676547][0.17813815 0.30759144 0.45725572 0.62250674 0.78538358 0.91804689 1.0046536 1.0305736 0.99204725 0.89744353 0.77688819 0.65991789 0.54784644 0.44286436 0.34341419][0.17457448 0.3017787 0.44711947 0.60259193 0.75107187 0.86960757 0.94144988 0.95107436 0.90207464 0.81179 0.70894915 0.61466068 0.52537638 0.44151646 0.35470545][0.15167506 0.26353049 0.38791376 0.51334584 0.62543213 0.70962453 0.75201982 0.74035794 0.68685293 0.614091 0.54324794 0.48393479 0.42926821 0.37818012 0.31553662][0.11542547 0.20257522 0.29574156 0.38222694 0.45123085 0.49619111 0.50821042 0.48036596 0.42965773 0.37817264 0.33790773 0.30968758 0.28606048 0.2649903 0.22820354][0.0682448 0.12838416 0.18981941 0.24063908 0.27328974 0.28653944 0.27663523 0.24195081 0.19922599 0.16609195 0.14722241 0.13884214 0.13442846 0.13161224 0.11352819][0.01492673 0.051377077 0.087847158 0.11385527 0.12434102 0.1206167 0.10235396 0.071027666 0.03969086 0.020300845 0.01317897 0.012987864 0.014793505 0.016881961 0.0073380265][-0.027339885 -0.0076732985 0.013128259 0.026267283 0.027974138 0.020071819 0.0039601685 -0.017893491 -0.03714674 -0.047125522 -0.049352426 -0.048870496 -0.048861392 -0.049723972 -0.057544231][-0.050806079 -0.042198114 -0.030779742 -0.023522679 -0.023296108 -0.028823905 -0.038445093 -0.050092541 -0.059450757 -0.06331706 -0.06373965 -0.064617574 -0.0675479 -0.071838073 -0.07946302]]...]
INFO - root - 2017-12-10 21:41:26.960437: step 55910, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 59h:33m:50s remains)
INFO - root - 2017-12-10 21:41:34.617343: step 55920, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 62h:15m:01s remains)
INFO - root - 2017-12-10 21:41:42.584429: step 55930, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 60h:28m:41s remains)
INFO - root - 2017-12-10 21:41:50.348079: step 55940, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 60h:24m:40s remains)
INFO - root - 2017-12-10 21:41:58.167602: step 55950, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 59h:12m:57s remains)
INFO - root - 2017-12-10 21:42:05.911175: step 55960, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 62h:17m:54s remains)
INFO - root - 2017-12-10 21:42:13.751824: step 55970, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 60h:20m:14s remains)
INFO - root - 2017-12-10 21:42:21.715818: step 55980, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 62h:01m:30s remains)
INFO - root - 2017-12-10 21:42:29.640670: step 55990, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 61h:45m:40s remains)
INFO - root - 2017-12-10 21:42:37.346533: step 56000, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 59h:50m:36s remains)
2017-12-10 21:42:38.211483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.025506899 -0.0093640182 0.01019389 0.033464625 0.059367824 0.088982314 0.11929203 0.1484029 0.17073512 0.17705704 0.17077796 0.15734486 0.14568242 0.14378251 0.15682161][-0.012181897 0.010916184 0.036032848 0.060203362 0.081833363 0.10179105 0.1193995 0.13642827 0.15130137 0.15676667 0.15297794 0.1420773 0.13184766 0.12982832 0.14220174][0.0035022355 0.035552118 0.069225289 0.098174885 0.11962962 0.1325784 0.13849433 0.14302288 0.14883804 0.15153217 0.1485272 0.13937016 0.13055257 0.1283762 0.13906594][0.01296045 0.053291414 0.096541196 0.13452329 0.16387531 0.18026863 0.18500565 0.18473996 0.18427356 0.18158969 0.17334397 0.15933704 0.14668989 0.14089383 0.14697838][0.015176889 0.061803076 0.11513915 0.16632761 0.21123567 0.24107777 0.25417393 0.25482154 0.24838017 0.23524965 0.21434148 0.18906389 0.16888246 0.15856816 0.15997285][0.01285125 0.06465666 0.12862508 0.19549976 0.25897306 0.3052651 0.32819256 0.32861793 0.31279886 0.28468907 0.24796736 0.21095178 0.18457913 0.17185639 0.17037678][0.0089796148 0.064845011 0.13846618 0.21982539 0.29939345 0.35893795 0.38774219 0.3843312 0.35735446 0.3150734 0.26583511 0.22116591 0.19181995 0.17874731 0.17595653][0.0075818179 0.068506807 0.15159607 0.24530961 0.33640608 0.40329275 0.43136504 0.41896713 0.37951016 0.32666221 0.27188167 0.22646959 0.19873543 0.18702118 0.18340655][0.009940438 0.076496728 0.16783191 0.27006313 0.36620429 0.43286428 0.45352316 0.42885002 0.3782087 0.32011288 0.26746625 0.22868507 0.20746256 0.19878346 0.19426902][0.014358117 0.085842639 0.182588 0.28784719 0.38143507 0.44012147 0.44831574 0.41121578 0.35354537 0.29699564 0.25429252 0.22916976 0.21889253 0.21469553 0.20961989][0.016336542 0.088665716 0.1850611 0.28637043 0.3705287 0.41633454 0.4113901 0.36635011 0.30924293 0.26210383 0.23538108 0.22725973 0.22847733 0.22737333 0.22000545][0.0097476356 0.075846255 0.16356617 0.25350717 0.32380307 0.35641587 0.34298179 0.29807803 0.24939582 0.21639186 0.20605712 0.21189897 0.22013938 0.21859051 0.20725235][-0.0070259096 0.045636091 0.1168167 0.18929452 0.24358664 0.26497597 0.24820039 0.20870417 0.17103258 0.15110712 0.15264748 0.16648906 0.177352 0.17401911 0.15948772][-0.029845001 0.0044607185 0.053531673 0.10399058 0.14068176 0.15263762 0.13671434 0.10624985 0.08033178 0.0705296 0.078104496 0.093686156 0.10368875 0.098669283 0.082871355][-0.051423047 -0.034945689 -0.0076172808 0.02127363 0.041518126 0.045978986 0.032981295 0.01230341 -0.0032738124 -0.0064826412 0.0025641392 0.016078554 0.02365098 0.017994029 0.0033466264]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 21:42:46.174698: step 56010, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 60h:35m:36s remains)
INFO - root - 2017-12-10 21:42:54.038338: step 56020, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 59h:08m:52s remains)
INFO - root - 2017-12-10 21:43:01.843949: step 56030, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.804 sec/batch; 61h:44m:50s remains)
INFO - root - 2017-12-10 21:43:09.785996: step 56040, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.816 sec/batch; 62h:38m:15s remains)
INFO - root - 2017-12-10 21:43:17.517143: step 56050, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 61h:14m:28s remains)
INFO - root - 2017-12-10 21:43:25.320296: step 56060, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 59h:00m:02s remains)
INFO - root - 2017-12-10 21:43:33.217653: step 56070, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 61h:08m:48s remains)
INFO - root - 2017-12-10 21:43:41.081338: step 56080, loss = 0.72, batch loss = 0.66 (9.5 examples/sec; 0.843 sec/batch; 64h:43m:57s remains)
INFO - root - 2017-12-10 21:43:49.206939: step 56090, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 60h:10m:04s remains)
INFO - root - 2017-12-10 21:43:57.158240: step 56100, loss = 0.72, batch loss = 0.66 (9.3 examples/sec; 0.860 sec/batch; 66h:01m:23s remains)
2017-12-10 21:43:58.048195: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25970435 0.26118931 0.2507579 0.23188788 0.21220395 0.2046417 0.21350572 0.22641937 0.24173829 0.25387147 0.26452914 0.27817363 0.29712832 0.31341261 0.32011437][0.24068223 0.23182003 0.21495417 0.19421797 0.17743295 0.17547773 0.19032744 0.20475595 0.21523328 0.21825081 0.21640739 0.21320646 0.2138761 0.21673201 0.21870387][0.19955942 0.19088985 0.17971218 0.16809021 0.16178294 0.16774318 0.18502763 0.19422269 0.19162248 0.17852958 0.16013178 0.14060292 0.12720381 0.12338112 0.12789416][0.15629983 0.16582914 0.17922789 0.19252631 0.20781861 0.22799321 0.24847722 0.24909075 0.22792229 0.19256972 0.15187873 0.11222991 0.083675541 0.073921844 0.081807755][0.12408147 0.16727321 0.2219355 0.27431643 0.32284907 0.3679665 0.40043989 0.39730191 0.35828727 0.29832935 0.22982752 0.16184126 0.10887364 0.084038012 0.086743936][0.11525583 0.19625261 0.2964831 0.39156973 0.47721028 0.55414861 0.60697013 0.60704625 0.5542255 0.47158629 0.37456411 0.27330416 0.187727 0.13774961 0.12559921][0.13184169 0.23845279 0.3711907 0.49870259 0.61500764 0.72248489 0.79877132 0.80796981 0.74813515 0.648743 0.5271247 0.39316279 0.27257997 0.19252856 0.15991838][0.15410188 0.26445076 0.40514088 0.54456449 0.67639774 0.80426258 0.89998513 0.92140734 0.86366022 0.75824636 0.62235278 0.46509677 0.31660059 0.21044829 0.15780917][0.15304045 0.2459525 0.36975577 0.49870712 0.62697697 0.75828773 0.86189592 0.89358228 0.84583986 0.74696618 0.61204493 0.449798 0.29207563 0.17501502 0.11174967][0.11628471 0.18065654 0.2732605 0.37680048 0.4860397 0.6034326 0.69967443 0.73421633 0.69830006 0.61341906 0.49233663 0.34482273 0.2009683 0.093693711 0.034941681][0.061225314 0.098611034 0.15908171 0.23326442 0.31638852 0.40873113 0.48577586 0.515451 0.48946425 0.42058384 0.31962955 0.19831946 0.082736269 -0.0015300598 -0.045681581][0.022804674 0.043993648 0.082050681 0.132737 0.19150843 0.25642377 0.3097629 0.3296425 0.30923432 0.25318038 0.17069981 0.075273193 -0.011765194 -0.072426364 -0.10103404][0.026044579 0.043911491 0.071857832 0.10779702 0.14721934 0.18707795 0.21680692 0.22377509 0.20263225 0.15285355 0.082808562 0.0065681669 -0.059058536 -0.10218996 -0.11893585][0.069491096 0.091318443 0.1163512 0.14299875 0.16697623 0.18549623 0.19415461 0.18707414 0.1597501 0.11029185 0.047262743 -0.016032359 -0.067144722 -0.098698907 -0.10737064][0.12670101 0.15113696 0.17225976 0.1893703 0.19895615 0.19963291 0.1914947 0.17291445 0.1398728 0.091423191 0.036230668 -0.014801598 -0.053651143 -0.076507889 -0.080210261]]...]
INFO - root - 2017-12-10 21:44:05.906924: step 56110, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 61h:36m:07s remains)
INFO - root - 2017-12-10 21:44:13.903132: step 56120, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 61h:36m:03s remains)
INFO - root - 2017-12-10 21:44:21.852895: step 56130, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 59h:05m:22s remains)
INFO - root - 2017-12-10 21:44:29.621584: step 56140, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 60h:30m:02s remains)
INFO - root - 2017-12-10 21:44:37.505596: step 56150, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.791 sec/batch; 60h:43m:34s remains)
INFO - root - 2017-12-10 21:44:45.244098: step 56160, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.789 sec/batch; 60h:32m:49s remains)
INFO - root - 2017-12-10 21:44:53.091657: step 56170, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 58h:20m:46s remains)
INFO - root - 2017-12-10 21:45:00.947911: step 56180, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 60h:18m:33s remains)
INFO - root - 2017-12-10 21:45:08.805239: step 56190, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.822 sec/batch; 63h:03m:28s remains)
INFO - root - 2017-12-10 21:45:16.866631: step 56200, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.797 sec/batch; 61h:09m:03s remains)
2017-12-10 21:45:17.718228: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1029919 0.1257337 0.14422406 0.15560873 0.16224857 0.16730839 0.17357792 0.18305258 0.19372261 0.20272395 0.20426933 0.19379026 0.17330812 0.1533308 0.14602543][0.12228339 0.147085 0.16727723 0.17902806 0.1845413 0.18848829 0.19591862 0.20969899 0.22648488 0.24198365 0.24777259 0.23542251 0.20643617 0.17381682 0.1524438][0.12485415 0.15344451 0.18047915 0.19923075 0.21006037 0.21762173 0.2266981 0.23921864 0.25114995 0.26044315 0.26082295 0.24348918 0.20945817 0.17023937 0.13996312][0.1152821 0.15144397 0.19440927 0.23304008 0.26346296 0.28740716 0.30449727 0.31200925 0.30586559 0.29137832 0.27172163 0.24277085 0.20520808 0.16556108 0.13325778][0.094017133 0.14064553 0.20682049 0.27651569 0.3396917 0.3925752 0.42537 0.42732576 0.39502352 0.34575328 0.29798859 0.25478977 0.21590911 0.18029796 0.14981313][0.06310413 0.11920137 0.2072137 0.30800813 0.40643892 0.49249887 0.54600435 0.54664588 0.4914256 0.41028109 0.3374888 0.28330892 0.24588273 0.21634743 0.18826686][0.031999718 0.091907412 0.19049895 0.30846235 0.42940062 0.5390535 0.61001945 0.61490273 0.55024457 0.45341739 0.36903584 0.31230661 0.27959791 0.25604948 0.22863658][0.0092965318 0.065203138 0.15841416 0.27200958 0.3925167 0.50486803 0.58064693 0.5912196 0.53188044 0.43934888 0.3601228 0.3110289 0.28653622 0.26831061 0.23990123][-0.0045568394 0.040431943 0.11495838 0.2057151 0.30408314 0.39762816 0.46276307 0.47556752 0.43011126 0.35539389 0.29227778 0.25665089 0.24187668 0.22911963 0.20190178][-0.011681008 0.017216427 0.066112384 0.12534744 0.19084483 0.25418353 0.29934517 0.31017411 0.28021497 0.22733542 0.18233012 0.15912412 0.15194926 0.144693 0.12379831][0.00030790712 0.011645608 0.0339039 0.062141627 0.095742635 0.12960589 0.15436029 0.16075346 0.14253633 0.1072086 0.075029835 0.058026325 0.053497784 0.050468408 0.039775819][0.041336428 0.037034824 0.035848651 0.039496679 0.05016695 0.064225651 0.075267196 0.077487975 0.064943574 0.03831277 0.010232335 -0.0080363182 -0.014849932 -0.015729899 -0.016014379][0.10459601 0.088356905 0.0682464 0.054424051 0.051971026 0.057194263 0.062921114 0.063186325 0.051755063 0.026096852 -0.0050594676 -0.029859869 -0.042180419 -0.042999405 -0.03534076][0.16218954 0.13922215 0.10651737 0.081689022 0.072861627 0.07608825 0.081962079 0.082643561 0.07108143 0.043607812 0.007346598 -0.024762226 -0.043073084 -0.045390911 -0.03369002][0.17586939 0.15170185 0.11477868 0.085866734 0.0751492 0.079068363 0.086997561 0.089673258 0.079306126 0.052043978 0.014261748 -0.020648342 -0.04147049 -0.044613857 -0.031890258]]...]
INFO - root - 2017-12-10 21:45:25.673560: step 56210, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 60h:56m:48s remains)
INFO - root - 2017-12-10 21:45:33.588444: step 56220, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 60h:45m:14s remains)
INFO - root - 2017-12-10 21:45:41.259406: step 56230, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.788 sec/batch; 60h:30m:11s remains)
INFO - root - 2017-12-10 21:45:49.089275: step 56240, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 60h:02m:24s remains)
INFO - root - 2017-12-10 21:45:56.971607: step 56250, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 59h:47m:16s remains)
INFO - root - 2017-12-10 21:46:04.782786: step 56260, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 60h:12m:41s remains)
INFO - root - 2017-12-10 21:46:12.716219: step 56270, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 58h:32m:32s remains)
INFO - root - 2017-12-10 21:46:20.525110: step 56280, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 59h:56m:07s remains)
INFO - root - 2017-12-10 21:46:28.497784: step 56290, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 60h:02m:32s remains)
INFO - root - 2017-12-10 21:46:36.461679: step 56300, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 59h:30m:46s remains)
2017-12-10 21:46:37.330868: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11944695 0.082689427 0.050966375 0.036075111 0.037751283 0.049742691 0.062332135 0.069644749 0.073157012 0.069872655 0.059507135 0.048165105 0.042140063 0.041386768 0.043824144][0.11685096 0.074112624 0.040510695 0.029134117 0.040037636 0.065724887 0.092466921 0.10831014 0.11088512 0.098099142 0.074481875 0.052407097 0.043735646 0.04899754 0.062343914][0.10791899 0.0638662 0.033140466 0.03103199 0.056896135 0.10018351 0.14181311 0.16383372 0.16096053 0.13434821 0.094692871 0.060817409 0.048860341 0.058729123 0.080508091][0.087925754 0.049477037 0.02897352 0.042020809 0.086003087 0.14643483 0.19894367 0.22159965 0.20887592 0.16736 0.11359795 0.07077948 0.055826955 0.0662607 0.089883685][0.062547542 0.034490187 0.029133569 0.059415013 0.11948097 0.19137776 0.24705254 0.26364344 0.2389975 0.18590979 0.12476081 0.079212613 0.0631165 0.071091279 0.09124437][0.054752871 0.039716814 0.051188938 0.097862326 0.17057906 0.24800111 0.29869723 0.30134028 0.260576 0.19673267 0.13277158 0.089257844 0.073675036 0.0773466 0.09100344][0.072387695 0.071685381 0.101105 0.1650694 0.25029376 0.33062679 0.37051266 0.35167775 0.28852788 0.21022843 0.14182095 0.099040322 0.082057126 0.0797009 0.086834505][0.10091462 0.11552043 0.16340449 0.24420853 0.33977193 0.41890895 0.44332463 0.39963961 0.3122223 0.21762437 0.14255516 0.098224051 0.079053991 0.072868735 0.078567289][0.11898775 0.148849 0.21405829 0.308264 0.40786508 0.47827196 0.4829891 0.41628847 0.31032941 0.20531569 0.12721805 0.082818069 0.063312389 0.057507381 0.067148089][0.11614774 0.16132973 0.24206303 0.34380305 0.43798384 0.49057853 0.4732241 0.38977733 0.27682963 0.17280124 0.099092685 0.057712391 0.039556056 0.036343042 0.051524125][0.10243595 0.15961836 0.24917372 0.34952536 0.42995963 0.46105734 0.425387 0.33555734 0.22843492 0.13595761 0.072337039 0.035716143 0.019192101 0.018627236 0.038210109][0.08721821 0.14743917 0.23318037 0.32050994 0.38089406 0.39190182 0.3460322 0.26103127 0.1701135 0.096436426 0.046699464 0.016776612 0.0031161881 0.00601001 0.029613726][0.078233965 0.13016593 0.19631959 0.25720406 0.29196829 0.28664723 0.24035428 0.17064892 0.10318935 0.051844776 0.017332722 -0.0045061419 -0.01346573 -0.0055201571 0.021975221][0.075149514 0.10936271 0.14556228 0.17329936 0.18259278 0.16771042 0.12999268 0.08211489 0.040275536 0.01027008 -0.010437187 -0.024263432 -0.027725618 -0.015024396 0.013802695][0.083577991 0.0961772 0.1021257 0.10029689 0.089922093 0.071116947 0.044733558 0.017636655 -0.0022522265 -0.015175407 -0.024931198 -0.032296576 -0.031795491 -0.0171536 0.0095838243]]...]
INFO - root - 2017-12-10 21:46:45.233652: step 56310, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 62h:09m:21s remains)
INFO - root - 2017-12-10 21:46:52.829709: step 56320, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 61h:10m:31s remains)
INFO - root - 2017-12-10 21:47:00.670481: step 56330, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 58h:41m:11s remains)
INFO - root - 2017-12-10 21:47:08.570250: step 56340, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 59h:53m:29s remains)
INFO - root - 2017-12-10 21:47:16.323761: step 56350, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.819 sec/batch; 62h:49m:28s remains)
INFO - root - 2017-12-10 21:47:24.261423: step 56360, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 61h:14m:28s remains)
INFO - root - 2017-12-10 21:47:32.086730: step 56370, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.752 sec/batch; 57h:39m:41s remains)
INFO - root - 2017-12-10 21:47:39.973327: step 56380, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 59h:43m:31s remains)
INFO - root - 2017-12-10 21:47:47.855880: step 56390, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 60h:18m:07s remains)
INFO - root - 2017-12-10 21:47:55.601971: step 56400, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 61h:50m:13s remains)
2017-12-10 21:47:56.558696: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.022860808 -0.016037185 -0.007580854 0.0055780667 0.021016335 0.032351594 0.032544289 0.021109376 0.0037099482 -0.015020883 -0.030875856 -0.04123459 -0.044915322 -0.044068623 -0.041002415][-0.012409662 0.00089229445 0.018051859 0.044310044 0.074612916 0.09767998 0.10136275 0.08417815 0.055857588 0.022893893 -0.007713763 -0.030341741 -0.040553831 -0.041097 -0.037788991][0.00045449831 0.021900358 0.050722841 0.094773836 0.14600354 0.18706791 0.19926023 0.17885527 0.13959815 0.090268269 0.041003507 0.0018136216 -0.017921222 -0.021313298 -0.018602662][0.015479158 0.045403354 0.086601518 0.14933342 0.22343846 0.28629681 0.31291649 0.2961469 0.25050148 0.1857188 0.11573631 0.057503164 0.026558274 0.019311784 0.020730291][0.030149911 0.066953905 0.1185196 0.19599195 0.2891286 0.37319893 0.41949224 0.41591784 0.37223205 0.29702875 0.20867613 0.13271022 0.0912831 0.080117717 0.080246851][0.043067157 0.084409319 0.14336179 0.22881868 0.33156833 0.42906019 0.49429396 0.51013571 0.47660628 0.39877492 0.29883423 0.21046278 0.1607829 0.14453837 0.14199556][0.055604771 0.10038944 0.16452928 0.25118417 0.35154328 0.44896761 0.52370727 0.55653024 0.53686774 0.46498245 0.36384249 0.27112874 0.2153071 0.19127952 0.18302597][0.071122788 0.11800838 0.184485 0.26597664 0.35251892 0.4349654 0.50394475 0.54240173 0.53328568 0.47292167 0.38209063 0.29556948 0.23773059 0.20466118 0.18794075][0.093770385 0.14140695 0.20780909 0.28014809 0.34585303 0.40301156 0.45231143 0.48149118 0.47355324 0.42466816 0.35198787 0.28104103 0.22754671 0.18839374 0.16347148][0.1276435 0.17500371 0.2389292 0.30081782 0.34517756 0.37652156 0.40212035 0.41310969 0.39624864 0.35215187 0.29604566 0.24224859 0.19741957 0.15873219 0.13163136][0.17084919 0.218999 0.27932084 0.33217674 0.36129364 0.37587678 0.38405079 0.37668049 0.34513044 0.29693982 0.24803561 0.20433381 0.16733885 0.13495758 0.1141114][0.21132661 0.26281711 0.3205862 0.36784634 0.3910628 0.40226004 0.40583187 0.38899225 0.34590125 0.28964093 0.23797572 0.19382516 0.15995444 0.13640901 0.12843576][0.23939477 0.29448777 0.35064632 0.39496204 0.41874731 0.43534768 0.4454717 0.43191233 0.38774091 0.32706681 0.26840082 0.21615718 0.17917866 0.16181485 0.16722897][0.25127652 0.30785012 0.36291897 0.40672562 0.43405136 0.45810425 0.4790895 0.47854629 0.44543067 0.39006019 0.32791835 0.26539275 0.21874061 0.19851887 0.20786908][0.24855497 0.30128175 0.35270748 0.39533481 0.42501506 0.45306161 0.48242012 0.4986406 0.48667055 0.44844481 0.39188233 0.32317165 0.263681 0.23015322 0.22840752]]...]
INFO - root - 2017-12-10 21:48:04.365469: step 56410, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.769 sec/batch; 58h:59m:47s remains)
INFO - root - 2017-12-10 21:48:12.259541: step 56420, loss = 0.71, batch loss = 0.66 (9.6 examples/sec; 0.837 sec/batch; 64h:09m:28s remains)
INFO - root - 2017-12-10 21:48:20.045758: step 56430, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.813 sec/batch; 62h:18m:38s remains)
INFO - root - 2017-12-10 21:48:27.896321: step 56440, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 61h:28m:24s remains)
INFO - root - 2017-12-10 21:48:35.812408: step 56450, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.820 sec/batch; 62h:52m:44s remains)
INFO - root - 2017-12-10 21:48:43.715607: step 56460, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 58h:21m:40s remains)
INFO - root - 2017-12-10 21:48:51.572073: step 56470, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 59h:50m:23s remains)
INFO - root - 2017-12-10 21:48:59.213001: step 56480, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 59h:38m:07s remains)
INFO - root - 2017-12-10 21:49:07.020159: step 56490, loss = 0.69, batch loss = 0.63 (10.8 examples/sec; 0.741 sec/batch; 56h:46m:46s remains)
INFO - root - 2017-12-10 21:49:14.725481: step 56500, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 59h:49m:40s remains)
2017-12-10 21:49:15.540227: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.061613794 0.06908489 0.091064274 0.11549237 0.13750446 0.15216722 0.1536732 0.14567363 0.127363 0.10591986 0.091072306 0.088006109 0.09476725 0.099819943 0.10415291][0.03780175 0.044423249 0.062145382 0.0844576 0.11148518 0.14074726 0.16474193 0.17644313 0.16833481 0.14774904 0.12491123 0.10879105 0.10318837 0.10110918 0.10443881][0.017085107 0.025492616 0.039286423 0.055518765 0.082451954 0.12258329 0.16843283 0.2023375 0.20893112 0.19395638 0.16624394 0.13883395 0.12154729 0.11072533 0.10996072][0.0090650218 0.022957623 0.035672523 0.045859788 0.069598883 0.11553091 0.17838722 0.23170772 0.25255048 0.24383022 0.21305454 0.17668745 0.14884406 0.12954928 0.1229621][0.021889554 0.045722809 0.060563657 0.063251466 0.076596119 0.11811814 0.1884256 0.25627783 0.2911008 0.28919786 0.25640351 0.21195881 0.17409502 0.14624204 0.13291343][0.053950064 0.092500828 0.11235838 0.10553218 0.10020227 0.12499788 0.18939044 0.26281002 0.30897659 0.31475934 0.282094 0.23146766 0.18453929 0.14917094 0.12948875][0.093796514 0.14732136 0.1732014 0.15712343 0.13046259 0.13253798 0.18206578 0.25330618 0.30656007 0.31991819 0.29001808 0.23717447 0.18472913 0.14411637 0.11952723][0.12727541 0.18992463 0.21893783 0.1967518 0.15591705 0.14091562 0.17545021 0.23896267 0.29310456 0.31032068 0.28413954 0.23330431 0.18087895 0.14053717 0.11532688][0.14751896 0.21065488 0.23845616 0.21438017 0.17080249 0.15064576 0.17479989 0.22501257 0.26864138 0.2809062 0.25767383 0.215132 0.17229763 0.14068182 0.11969595][0.15300441 0.20906068 0.23231712 0.21009079 0.17325996 0.15813541 0.1767296 0.21036237 0.23457953 0.23421437 0.21227211 0.18323797 0.1590414 0.14368074 0.13099112][0.14439934 0.18679133 0.20371513 0.1866498 0.16317534 0.15994123 0.17720114 0.19400077 0.1945373 0.17663886 0.15328093 0.13934208 0.13900509 0.14616795 0.14659189][0.12637857 0.1493607 0.15804163 0.14704485 0.13849704 0.14895868 0.16871272 0.17361557 0.15406986 0.1197482 0.093570523 0.092774384 0.11629812 0.148777 0.16633578][0.10849812 0.10923979 0.10793027 0.101154 0.10492463 0.12728469 0.15176295 0.15185897 0.12111388 0.076606371 0.048183769 0.055958819 0.097668573 0.15225545 0.18779518][0.095311947 0.076553613 0.066893756 0.0625064 0.072783493 0.1002558 0.12633669 0.12539561 0.092118293 0.045194171 0.016731802 0.028852275 0.080615126 0.14970107 0.19965947][0.083816178 0.051224902 0.035410356 0.031411361 0.042097669 0.066947289 0.089479342 0.088867471 0.06026879 0.019148482 -0.0051637958 0.0092061432 0.064090393 0.14038636 0.20079908]]...]
INFO - root - 2017-12-10 21:49:23.398337: step 56510, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 62h:23m:07s remains)
INFO - root - 2017-12-10 21:49:31.228711: step 56520, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.782 sec/batch; 59h:55m:23s remains)
INFO - root - 2017-12-10 21:49:39.041980: step 56530, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 59h:12m:42s remains)
INFO - root - 2017-12-10 21:49:46.881665: step 56540, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 59h:34m:37s remains)
INFO - root - 2017-12-10 21:49:54.695486: step 56550, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 60h:34m:55s remains)
INFO - root - 2017-12-10 21:50:02.472252: step 56560, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 59h:28m:31s remains)
INFO - root - 2017-12-10 21:50:10.378923: step 56570, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.823 sec/batch; 63h:04m:31s remains)
INFO - root - 2017-12-10 21:50:18.359064: step 56580, loss = 0.70, batch loss = 0.64 (12.5 examples/sec; 0.642 sec/batch; 49h:10m:22s remains)
INFO - root - 2017-12-10 21:50:26.332706: step 56590, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.768 sec/batch; 58h:52m:38s remains)
INFO - root - 2017-12-10 21:50:34.241190: step 56600, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 60h:40m:16s remains)
2017-12-10 21:50:35.053630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014054422 0.010625693 0.024592821 0.036928322 0.04334981 0.041728694 0.033461917 0.022707993 0.015237061 0.018611813 0.030742906 0.043104865 0.048527241 0.048028711 0.040799722][0.01669859 0.042203348 0.071381405 0.098392479 0.11622707 0.12013215 0.11071939 0.094245732 0.080170557 0.080436252 0.091981463 0.10333631 0.10529281 0.10042904 0.086322844][0.037624404 0.079806291 0.12929907 0.17816691 0.21492001 0.23099232 0.2256716 0.20681755 0.1860176 0.17795016 0.1806827 0.18219228 0.17249073 0.15546592 0.12869059][0.055158537 0.11562728 0.18942842 0.26573256 0.32645148 0.36047587 0.36527357 0.34812638 0.32033125 0.29866043 0.2849116 0.2683163 0.23902903 0.20215528 0.15700082][0.06701006 0.1463863 0.24747679 0.35393274 0.43929371 0.49307826 0.51241666 0.50121891 0.46734685 0.42981049 0.39757803 0.36078691 0.30846643 0.24581674 0.17690998][0.078012057 0.17586231 0.303261 0.43630439 0.54115438 0.61119145 0.645986 0.64415467 0.60862029 0.56060171 0.51678091 0.46571505 0.39261118 0.30252033 0.20660886][0.092397891 0.20547865 0.3527545 0.50292814 0.61874568 0.69801104 0.74363512 0.74950427 0.71374917 0.65986526 0.61120385 0.55401289 0.46819338 0.35820186 0.24215037][0.10657029 0.22851256 0.38509494 0.53996694 0.65697628 0.73714596 0.78638226 0.79530275 0.75998068 0.70646548 0.66156888 0.60792559 0.52019984 0.40290505 0.27852169][0.11343928 0.23352437 0.38484213 0.52969205 0.63737786 0.70985925 0.75492436 0.76271349 0.73043388 0.68531215 0.65280181 0.61186683 0.533469 0.42288077 0.30336437][0.10826269 0.21437381 0.34511805 0.46569431 0.55368966 0.61038727 0.64415079 0.64696145 0.61900574 0.58639091 0.56964296 0.54613137 0.48617518 0.39588463 0.29490688][0.093761049 0.17855036 0.27988854 0.36949244 0.43288967 0.46944454 0.48662227 0.48109633 0.45648411 0.43467391 0.42957839 0.42101339 0.38281894 0.32015967 0.24613246][0.068969004 0.13010989 0.20138301 0.26157221 0.30168739 0.31955692 0.32117561 0.30775481 0.28492343 0.26904634 0.26832557 0.26790404 0.24758171 0.21072148 0.16377322][0.036743652 0.075521655 0.12083077 0.15779155 0.18061432 0.18622856 0.17870048 0.16182199 0.14150508 0.12881118 0.12825681 0.13045688 0.12145542 0.10281806 0.076396935][0.0068762247 0.027536625 0.053164318 0.073927224 0.085811846 0.085455418 0.075225145 0.059514061 0.043146744 0.032851253 0.031105466 0.033305377 0.030154405 0.022057813 0.008678887][-0.012847885 -0.0039167157 0.0092257829 0.020248076 0.026017468 0.023688499 0.014358543 0.001759638 -0.010761701 -0.019357815 -0.022430746 -0.021762373 -0.0228261 -0.025721969 -0.031414278]]...]
INFO - root - 2017-12-10 21:50:42.862608: step 56610, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 58h:04m:56s remains)
INFO - root - 2017-12-10 21:50:50.771729: step 56620, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 61h:15m:52s remains)
INFO - root - 2017-12-10 21:50:58.599583: step 56630, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 61h:06m:05s remains)
INFO - root - 2017-12-10 21:51:06.316929: step 56640, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.790 sec/batch; 60h:34m:22s remains)
INFO - root - 2017-12-10 21:51:14.147643: step 56650, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 59h:37m:22s remains)
INFO - root - 2017-12-10 21:51:22.065539: step 56660, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 60h:12m:59s remains)
INFO - root - 2017-12-10 21:51:29.731676: step 56670, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 61h:54m:54s remains)
INFO - root - 2017-12-10 21:51:37.563802: step 56680, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 60h:13m:08s remains)
INFO - root - 2017-12-10 21:51:45.410857: step 56690, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 60h:00m:37s remains)
INFO - root - 2017-12-10 21:51:53.225413: step 56700, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.773 sec/batch; 59h:11m:24s remains)
2017-12-10 21:51:54.061716: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13573623 0.14823203 0.1637104 0.18527357 0.20495023 0.21208179 0.20235948 0.1729681 0.13684464 0.11195728 0.11129452 0.12968118 0.14783286 0.15529265 0.15103772][0.14927177 0.15680324 0.16112922 0.16818953 0.17586121 0.17749925 0.17145415 0.152655 0.12797146 0.10821813 0.10249968 0.10843772 0.11273128 0.11003001 0.10213199][0.17414598 0.17424272 0.16219433 0.14995569 0.14374627 0.14260708 0.14439736 0.13991493 0.12895291 0.11535838 0.10491838 0.098562568 0.0900804 0.078115039 0.067047976][0.2215139 0.2161486 0.18980663 0.1613991 0.14594661 0.14687344 0.15756248 0.16324873 0.15870279 0.14450265 0.12578945 0.10692614 0.08749222 0.067784578 0.053630862][0.27750215 0.26912907 0.23500904 0.20024772 0.18580134 0.1955047 0.21438849 0.22193919 0.21086679 0.18449162 0.15125471 0.11907607 0.091204561 0.066589259 0.050640758][0.31233791 0.30290949 0.27080855 0.24310979 0.24029776 0.2620247 0.28433156 0.28334263 0.25305775 0.2043301 0.15221201 0.10793307 0.075540341 0.050421238 0.035215516][0.29683602 0.28778034 0.26603997 0.25521681 0.26923904 0.30078945 0.32020122 0.304495 0.25137797 0.18033 0.1128094 0.061515994 0.029073862 0.0073071141 -0.0046264958][0.22617534 0.21739262 0.20901597 0.21646667 0.24424814 0.27906585 0.29118064 0.2624658 0.194851 0.11318786 0.041512258 -0.0084685022 -0.036485277 -0.052483264 -0.059839014][0.12717414 0.11837532 0.1201217 0.13897692 0.17144884 0.20203492 0.20622751 0.17225996 0.10500757 0.029036272 -0.033513986 -0.073930085 -0.093764685 -0.10240674 -0.10434673][0.035350367 0.026965126 0.033337358 0.054548107 0.083000459 0.10488387 0.10359721 0.072999492 0.018736631 -0.039092541 -0.083770245 -0.11009956 -0.12018636 -0.12152206 -0.11852929][-0.024814459 -0.03205594 -0.026001766 -0.0096871518 0.0094599575 0.021747012 0.018260367 -0.0034954245 -0.03869 -0.073813513 -0.0987976 -0.11106978 -0.11264661 -0.10865296 -0.10276502][-0.051017337 -0.057009339 -0.053716965 -0.044841036 -0.035155926 -0.029935624 -0.032861955 -0.044877339 -0.062660605 -0.078675769 -0.088165812 -0.090470314 -0.087094992 -0.0811239 -0.075159825][-0.057479613 -0.062312335 -0.061372668 -0.058186542 -0.054862004 -0.053510744 -0.055152223 -0.060144663 -0.066577144 -0.07098604 -0.071865648 -0.069453 -0.064760931 -0.059495945 -0.055261932][-0.061769661 -0.065397315 -0.065064937 -0.064282596 -0.063745253 -0.063956492 -0.064794064 -0.066319071 -0.067529626 -0.067152821 -0.065095857 -0.061814595 -0.057931002 -0.054379418 -0.052018944][-0.069850422 -0.072423562 -0.071528524 -0.0710802 -0.071274243 -0.071894214 -0.072430283 -0.072790436 -0.072461344 -0.071118489 -0.069012776 -0.066523843 -0.064009868 -0.061977528 -0.060807765]]...]
INFO - root - 2017-12-10 21:52:01.980047: step 56710, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 60h:25m:58s remains)
INFO - root - 2017-12-10 21:52:09.604416: step 56720, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 60h:26m:51s remains)
INFO - root - 2017-12-10 21:52:17.462454: step 56730, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 60h:02m:54s remains)
INFO - root - 2017-12-10 21:52:25.341884: step 56740, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 58h:55m:30s remains)
INFO - root - 2017-12-10 21:52:33.272553: step 56750, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.795 sec/batch; 60h:53m:32s remains)
INFO - root - 2017-12-10 21:52:41.009338: step 56760, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 62h:17m:42s remains)
INFO - root - 2017-12-10 21:52:48.911720: step 56770, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 61h:13m:11s remains)
INFO - root - 2017-12-10 21:52:56.698190: step 56780, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 59h:17m:33s remains)
INFO - root - 2017-12-10 21:53:04.537338: step 56790, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 61h:54m:34s remains)
INFO - root - 2017-12-10 21:53:12.249240: step 56800, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 60h:16m:10s remains)
2017-12-10 21:53:13.059275: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23677528 0.209839 0.17721227 0.1490497 0.12372602 0.098308586 0.074074924 0.053188957 0.037358459 0.032512024 0.038500108 0.048916973 0.058354236 0.061423469 0.054287691][0.22737667 0.21746852 0.19775507 0.17632328 0.15007767 0.11812898 0.083759181 0.053369552 0.031940114 0.02924915 0.046824541 0.075535893 0.10593794 0.12709408 0.12932004][0.20504113 0.21941458 0.22382841 0.21946621 0.19889879 0.16249126 0.11562665 0.069635563 0.035303246 0.028503137 0.056763928 0.11150327 0.17759763 0.23388991 0.26009443][0.19113529 0.23730955 0.27707282 0.30011654 0.29246014 0.25501075 0.19614412 0.1308919 0.07570377 0.053821992 0.082466416 0.15739359 0.26010391 0.35848206 0.42025545][0.19093554 0.27123791 0.35124919 0.40823507 0.42195717 0.3929784 0.33156112 0.25134853 0.1724799 0.12537421 0.13710724 0.21105058 0.33156037 0.46020663 0.55680382][0.20073894 0.30935869 0.42305502 0.51168162 0.553139 0.54752833 0.5020349 0.42043573 0.32191673 0.2428893 0.218236 0.26049745 0.36370084 0.49278045 0.60878021][0.20929758 0.32986969 0.45739779 0.562484 0.62908489 0.65840256 0.64822942 0.58539587 0.48168227 0.375051 0.30428645 0.29092592 0.3428804 0.43847591 0.54899907][0.20510159 0.316802 0.43304247 0.53260964 0.610929 0.67142779 0.70105779 0.67015517 0.57903343 0.46079752 0.35269392 0.28217989 0.2700547 0.31287265 0.39593861][0.18270271 0.268166 0.35324702 0.42802009 0.498274 0.56947911 0.62342769 0.623277 0.55941153 0.4539243 0.3350077 0.22984196 0.16702008 0.15938367 0.20455316][0.14679211 0.19725423 0.24234989 0.28189182 0.3273164 0.38466793 0.43770579 0.45396671 0.41983306 0.34500685 0.24363151 0.1362984 0.05286438 0.016102867 0.031257249][0.11229823 0.13057327 0.13972367 0.14505509 0.15931684 0.18778087 0.22046952 0.23605001 0.22293097 0.18166704 0.11312426 0.028748661 -0.046065137 -0.086951435 -0.085813448][0.090292618 0.086748309 0.071253195 0.051299985 0.038795937 0.037879225 0.045507982 0.051819846 0.049694233 0.035052694 0.00058895687 -0.050162271 -0.099450588 -0.12785789 -0.12783454][0.081982367 0.068521224 0.04154592 0.0089124339 -0.018763049 -0.038596246 -0.048880637 -0.051817849 -0.050826889 -0.050561547 -0.060400251 -0.081166618 -0.10301859 -0.11442337 -0.11000762][0.0826857 0.06797222 0.039361648 0.0044830143 -0.026850266 -0.052786838 -0.069873266 -0.077271655 -0.076502621 -0.07071 -0.067747436 -0.069669887 -0.072478428 -0.071463332 -0.0636039][0.086398505 0.074365243 0.0489921 0.017453603 -0.010806454 -0.034747686 -0.050966632 -0.058021389 -0.056918293 -0.0494198 -0.041765496 -0.035923105 -0.030746939 -0.025063394 -0.017296089]]...]
INFO - root - 2017-12-10 21:53:20.946302: step 56810, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.759 sec/batch; 58h:06m:15s remains)
INFO - root - 2017-12-10 21:53:28.845068: step 56820, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 61h:47m:46s remains)
INFO - root - 2017-12-10 21:53:36.674096: step 56830, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 61h:42m:10s remains)
INFO - root - 2017-12-10 21:53:44.509348: step 56840, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 59h:11m:18s remains)
INFO - root - 2017-12-10 21:53:52.210575: step 56850, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 61h:32m:36s remains)
INFO - root - 2017-12-10 21:54:00.074930: step 56860, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 58h:15m:46s remains)
INFO - root - 2017-12-10 21:54:07.927091: step 56870, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 60h:18m:59s remains)
INFO - root - 2017-12-10 21:54:15.664209: step 56880, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 60h:15m:19s remains)
INFO - root - 2017-12-10 21:54:23.581577: step 56890, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 59h:25m:12s remains)
INFO - root - 2017-12-10 21:54:31.411283: step 56900, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 58h:45m:12s remains)
2017-12-10 21:54:32.240909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.031376835 0.0067256559 0.053821724 0.100144 0.13103916 0.13422792 0.11101314 0.075836062 0.040612686 0.011636528 -0.004329565 -0.0075786156 -0.0046902145 -0.00026917076 0.0050062733][-0.030885046 0.0063258167 0.050618608 0.092094563 0.11686483 0.11587819 0.092985861 0.062984966 0.037855461 0.021586346 0.018175187 0.023993781 0.03034527 0.032157779 0.031599168][-0.033032533 0.001601885 0.04155628 0.0778749 0.098849371 0.097855963 0.079254016 0.056716762 0.042271528 0.038386829 0.046746537 0.061964314 0.073824115 0.07651221 0.074275471][-0.036135096 -0.0031338101 0.035123449 0.070427574 0.092533968 0.0951095 0.080859549 0.061974943 0.051140334 0.050905518 0.06330625 0.084381476 0.10372498 0.11346291 0.11778747][-0.036432277 -0.000536129 0.043754853 0.08743114 0.11898781 0.12871516 0.11570943 0.091603376 0.071663566 0.061551739 0.066397674 0.085436016 0.10903926 0.12663651 0.14065552][-0.032620545 0.011401804 0.070097119 0.13247867 0.18249992 0.20392703 0.19086345 0.15491815 0.11490498 0.081936128 0.066543855 0.07252679 0.091472231 0.1099437 0.12775688][-0.028498048 0.024056569 0.098517746 0.18215948 0.25375509 0.29050583 0.28120485 0.2362726 0.17665915 0.11841123 0.077836156 0.063351147 0.068229035 0.077244557 0.087804466][-0.029137902 0.026449397 0.10868079 0.20501502 0.29198 0.34364375 0.34451059 0.30181172 0.23491868 0.16169047 0.1025079 0.069705144 0.058939677 0.054553468 0.052088581][-0.033020832 0.02037867 0.10089125 0.19751404 0.28780225 0.34721634 0.35893133 0.32717127 0.26717356 0.19527186 0.13245307 0.092911556 0.074943885 0.063584968 0.052760676][-0.037227642 0.011418713 0.084272631 0.17190996 0.25448358 0.31159303 0.32841951 0.30862615 0.26369348 0.2069965 0.1566993 0.1257837 0.11372005 0.10671279 0.097608261][-0.041960914 0.0011985208 0.064488709 0.13952143 0.20883012 0.25625482 0.27064118 0.25774577 0.2274238 0.19078018 0.16193661 0.15013838 0.15369593 0.15948322 0.16024967][-0.045203082 -0.0053011896 0.052272134 0.11856839 0.17667219 0.21229365 0.21749587 0.20170486 0.17614049 0.15219782 0.1414398 0.1488227 0.16967751 0.19063033 0.20501924][-0.044279575 -0.0033088266 0.0552335 0.12057 0.17419827 0.20057456 0.19302002 0.16456027 0.12995072 0.10357857 0.097070746 0.11274768 0.14320911 0.174702 0.20147172][-0.039301496 0.0061690831 0.070937715 0.14144571 0.19632597 0.21709163 0.19680028 0.15075064 0.09738607 0.055256087 0.038228683 0.048286512 0.076864824 0.11022166 0.14334074][-0.034210715 0.015569657 0.086608313 0.16266409 0.21983641 0.23710905 0.20712461 0.14596762 0.074548766 0.014292188 -0.018847946 -0.022204325 -0.0041461834 0.022199422 0.0523784]]...]
INFO - root - 2017-12-10 21:54:40.039496: step 56910, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 58h:44m:53s remains)
INFO - root - 2017-12-10 21:54:47.920413: step 56920, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 60h:44m:38s remains)
INFO - root - 2017-12-10 21:54:55.835904: step 56930, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.827 sec/batch; 63h:18m:58s remains)
INFO - root - 2017-12-10 21:55:03.501298: step 56940, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 59h:02m:07s remains)
INFO - root - 2017-12-10 21:55:11.370789: step 56950, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 60h:19m:14s remains)
INFO - root - 2017-12-10 21:55:18.932384: step 56960, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 60h:01m:17s remains)
INFO - root - 2017-12-10 21:55:26.842764: step 56970, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 60h:41m:44s remains)
INFO - root - 2017-12-10 21:55:34.700961: step 56980, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.814 sec/batch; 62h:17m:38s remains)
INFO - root - 2017-12-10 21:55:42.701786: step 56990, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 62h:01m:07s remains)
INFO - root - 2017-12-10 21:55:50.544437: step 57000, loss = 0.72, batch loss = 0.66 (9.8 examples/sec; 0.817 sec/batch; 62h:33m:40s remains)
2017-12-10 21:55:51.356425: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.052922811 -0.02429782 -0.069997884 -0.080393374 -0.066225909 -0.045520939 -0.030788695 -0.030170487 -0.045681823 -0.071990907 -0.098874144 -0.11914361 -0.1309509 -0.13364825 -0.12068461][0.10983865 0.0090215476 -0.053404137 -0.065680929 -0.039203241 0.0020757027 0.036529575 0.04779464 0.030108767 -0.010195506 -0.0575787 -0.097972661 -0.1248867 -0.13549748 -0.12232871][0.16088191 0.05213568 -0.012894528 -0.016555339 0.028895756 0.093446657 0.14695016 0.16505623 0.13992575 0.080800891 0.0092878807 -0.053929806 -0.097670048 -0.11875599 -0.11064269][0.18747529 0.083310485 0.026329186 0.038863733 0.10749151 0.19660227 0.26828986 0.2915428 0.2573891 0.1783151 0.082009465 -0.0047143139 -0.066838212 -0.10140994 -0.10365281][0.19459049 0.099239595 0.052239023 0.080324546 0.16940562 0.27956751 0.36653095 0.39431691 0.35310447 0.25830439 0.14234909 0.036425654 -0.041548379 -0.088709332 -0.10256159][0.19238053 0.10537137 0.065444 0.10405172 0.20857023 0.33700374 0.43927127 0.47451133 0.43061209 0.3246198 0.19259144 0.069729105 -0.022707948 -0.081172004 -0.10559574][0.19505581 0.1124729 0.074041642 0.11690879 0.23212031 0.37709683 0.49634275 0.54302067 0.50049603 0.38594756 0.23859967 0.098471478 -0.008810929 -0.077934407 -0.11121493][0.21114627 0.12982401 0.088428378 0.12921284 0.2481852 0.40329987 0.53530085 0.5920409 0.55189908 0.43146348 0.27285206 0.12085123 0.0043834536 -0.070248507 -0.10818846][0.24036133 0.16317941 0.11961533 0.15427138 0.26795417 0.420906 0.5533728 0.61185259 0.57215494 0.45018893 0.28951168 0.13721469 0.022626802 -0.048954166 -0.085810564][0.28565508 0.21664205 0.17301899 0.19767265 0.29488215 0.42989522 0.54737431 0.59721911 0.5563544 0.44015154 0.29044271 0.15197788 0.051081821 -0.010213326 -0.042812992][0.33988649 0.28260541 0.24023391 0.25132695 0.32214338 0.42583898 0.51667666 0.55196083 0.51183724 0.40972528 0.28225222 0.16754277 0.086310923 0.036882326 0.0072413944][0.39205647 0.34907576 0.31026596 0.30758068 0.34642574 0.4089936 0.463638 0.47937 0.44135186 0.35983944 0.26326343 0.17972802 0.1221222 0.085093163 0.057215258][0.42308933 0.39561024 0.36707795 0.35739014 0.36752796 0.38801572 0.4028888 0.39580262 0.35843021 0.29860529 0.235805 0.18645783 0.15423851 0.13031931 0.1052573][0.41905582 0.40906683 0.39719591 0.39070332 0.38423255 0.37389743 0.35668886 0.32997486 0.29206362 0.24986902 0.21516767 0.19356132 0.18072636 0.16587643 0.14290409][0.3762674 0.38194042 0.38937092 0.3953355 0.38810021 0.36535004 0.33194947 0.29509664 0.25793964 0.22742718 0.21007758 0.20371883 0.19900081 0.18531762 0.16116059]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 21:55:59.214385: step 57010, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 60h:59m:44s remains)
INFO - root - 2017-12-10 21:56:07.104482: step 57020, loss = 0.69, batch loss = 0.64 (9.7 examples/sec; 0.829 sec/batch; 63h:25m:20s remains)
INFO - root - 2017-12-10 21:56:14.702886: step 57030, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 59h:17m:16s remains)
INFO - root - 2017-12-10 21:56:22.368374: step 57040, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 59h:00m:06s remains)
INFO - root - 2017-12-10 21:56:30.202670: step 57050, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 59h:22m:06s remains)
INFO - root - 2017-12-10 21:56:37.989992: step 57060, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 60h:10m:36s remains)
INFO - root - 2017-12-10 21:56:45.934025: step 57070, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 61h:09m:10s remains)
INFO - root - 2017-12-10 21:56:53.778941: step 57080, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 59h:15m:28s remains)
INFO - root - 2017-12-10 21:57:01.625474: step 57090, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.780 sec/batch; 59h:42m:34s remains)
INFO - root - 2017-12-10 21:57:09.465810: step 57100, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 59h:55m:57s remains)
2017-12-10 21:57:10.332559: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23045111 0.23161602 0.21295597 0.16701123 0.10420261 0.040502619 -0.0020593395 -0.016201593 -0.014533776 -0.0072668614 0.00025657655 0.0043093837 -0.00014209366 -0.013462674 -0.029675027][0.27651045 0.28147018 0.26347387 0.21297716 0.14170898 0.06875895 0.018424103 -0.0035122263 -0.010497742 -0.0096532749 -0.0055343094 -0.0038531267 -0.0098730363 -0.022503411 -0.036164988][0.29577374 0.30631155 0.29534495 0.25300604 0.19047871 0.12717368 0.086263634 0.069271825 0.060394164 0.054986838 0.048954021 0.035994556 0.013754689 -0.011328351 -0.031929247][0.28965944 0.30592385 0.30714545 0.28559279 0.24961235 0.21564831 0.2019286 0.2031448 0.1997797 0.18872663 0.16602887 0.12701355 0.07524617 0.025212899 -0.011616196][0.27717862 0.29866815 0.31257918 0.31608137 0.31404343 0.3175216 0.33754611 0.36217898 0.36774072 0.35095772 0.3084223 0.23911494 0.15375543 0.074415371 0.016877815][0.26303035 0.28853875 0.31259298 0.3372229 0.36390856 0.39814195 0.4442341 0.4855431 0.49586409 0.47156826 0.41132405 0.31831837 0.2086374 0.1084809 0.036452014][0.23380466 0.25999886 0.28940621 0.3256236 0.36729065 0.41503602 0.46864462 0.51082695 0.517251 0.48544368 0.41684267 0.31744429 0.20490269 0.10421821 0.033006325][0.16734409 0.18994865 0.21867789 0.25547567 0.2972818 0.34179375 0.3863149 0.41628218 0.41311893 0.37739828 0.31342962 0.22758508 0.1352749 0.0558766 0.0021371767][0.064548023 0.080109984 0.10350399 0.13320269 0.1656262 0.19747342 0.22568223 0.23992674 0.22858526 0.19514327 0.14584109 0.086094022 0.027162997 -0.018931413 -0.045916129][-0.040799025 -0.033693574 -0.0185761 1.9294739e-05 0.019284574 0.036508769 0.049574845 0.052561026 0.03945611 0.014565815 -0.015662055 -0.04711483 -0.072591253 -0.086757131 -0.088863179][-0.10796663 -0.10813427 -0.10055047 -0.091920473 -0.0839549 -0.0779245 -0.074382618 -0.07573 -0.084765151 -0.098383777 -0.11167856 -0.12182744 -0.12470258 -0.11915649 -0.1073501][-0.12234159 -0.12592292 -0.12313812 -0.12070009 -0.11940962 -0.11930192 -0.11981693 -0.12130337 -0.12500603 -0.12970312 -0.13265559 -0.13222247 -0.12616079 -0.11490852 -0.10098939][-0.10222378 -0.10671353 -0.10626438 -0.10654319 -0.10779241 -0.10955812 -0.11092217 -0.11147591 -0.11182564 -0.11198793 -0.11080535 -0.10745083 -0.10088263 -0.091887921 -0.082235076][-0.073690668 -0.077524491 -0.077654958 -0.078315556 -0.079531848 -0.080964424 -0.081967637 -0.082111135 -0.081592008 -0.080700777 -0.07909932 -0.076476276 -0.072518244 -0.067913942 -0.063615568][-0.054657083 -0.057659891 -0.057673618 -0.05791872 -0.058360536 -0.058876909 -0.059238225 -0.059262965 -0.058974504 -0.05851819 -0.057819284 -0.056803886 -0.055432878 -0.054076 -0.053122398]]...]
INFO - root - 2017-12-10 21:57:18.308362: step 57110, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 60h:29m:45s remains)
INFO - root - 2017-12-10 21:57:25.705445: step 57120, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 58h:44m:40s remains)
INFO - root - 2017-12-10 21:57:33.546396: step 57130, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 60h:58m:16s remains)
INFO - root - 2017-12-10 21:57:41.437303: step 57140, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 60h:58m:38s remains)
INFO - root - 2017-12-10 21:57:49.259806: step 57150, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 58h:23m:43s remains)
INFO - root - 2017-12-10 21:57:57.115085: step 57160, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 59h:42m:44s remains)
INFO - root - 2017-12-10 21:58:04.904337: step 57170, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 59h:27m:00s remains)
INFO - root - 2017-12-10 21:58:12.803224: step 57180, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 61h:35m:50s remains)
INFO - root - 2017-12-10 21:58:20.629814: step 57190, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 58h:52m:14s remains)
INFO - root - 2017-12-10 21:58:28.336181: step 57200, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 61h:00m:43s remains)
2017-12-10 21:58:29.215177: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.071624532 -0.067554638 -0.064428881 -0.064573117 -0.065601826 -0.067165591 -0.069949888 -0.073110126 -0.076086231 -0.07877516 -0.082282916 -0.087742515 -0.093137093 -0.0973431 -0.098937809][-0.020053908 -0.009467436 -0.0045007267 -0.0076915361 -0.013095093 -0.017937524 -0.022501551 -0.025107937 -0.027139328 -0.030329099 -0.037693821 -0.051495317 -0.067329742 -0.083174229 -0.094776779][0.076306626 0.092884317 0.096568421 0.085977085 0.072785966 0.063631594 0.059451669 0.062005963 0.066153824 0.065307789 0.051440936 0.023101456 -0.011753405 -0.048828527 -0.0785338][0.20427945 0.22593071 0.22508046 0.204159 0.18257841 0.17129236 0.17262512 0.18820068 0.20635824 0.2125261 0.19057195 0.14080982 0.0769734 0.0084178094 -0.048922814][0.33696526 0.36293945 0.35633448 0.32502946 0.29753351 0.28884625 0.30194119 0.33727741 0.376353 0.39405769 0.36483482 0.29024181 0.19145945 0.084854223 -0.0061309207][0.436735 0.46703032 0.45552656 0.41591561 0.38530096 0.38233528 0.41065675 0.46787924 0.52924091 0.56018984 0.52643555 0.42913803 0.29642504 0.15375346 0.032231729][0.47865555 0.51045287 0.49250838 0.44453049 0.41098765 0.41468844 0.45922178 0.53740907 0.61867058 0.66184866 0.62693042 0.51435268 0.35726681 0.19009714 0.049857881][0.45010841 0.48019966 0.45611 0.40214515 0.36720094 0.37724763 0.43497789 0.52858388 0.62317485 0.67423767 0.64008194 0.52156836 0.35545209 0.18221541 0.039830439][0.35471657 0.38328987 0.3615475 0.3128033 0.28347543 0.29965889 0.36285031 0.45859495 0.55214214 0.60041529 0.56592429 0.45122114 0.29340863 0.1337536 0.0068159793][0.22494853 0.255328 0.24860609 0.21917695 0.20266968 0.22174171 0.27798423 0.3571876 0.42972603 0.46010581 0.42089096 0.31733504 0.18295212 0.053989008 -0.042994048][0.11951982 0.15908815 0.17795266 0.17553511 0.17086177 0.18164831 0.21292171 0.25589311 0.28998017 0.29137084 0.24467845 0.15670991 0.054118887 -0.03548196 -0.095883533][0.072358921 0.12542793 0.17358287 0.19959055 0.20353524 0.19728966 0.19137675 0.1871071 0.17594892 0.14729163 0.095869429 0.026719224 -0.043165617 -0.096795626 -0.12690404][0.057622042 0.12115545 0.19663665 0.25285032 0.27059513 0.2533333 0.21509176 0.16745061 0.11558349 0.062943414 0.01145517 -0.038751755 -0.082179777 -0.11165591 -0.12533683][0.0413686 0.10079163 0.1909018 0.27395043 0.3144021 0.30394542 0.25369954 0.1808451 0.10261662 0.035782076 -0.012206792 -0.046641938 -0.072487615 -0.090082325 -0.099716619][0.02337772 0.058052212 0.13958873 0.23319773 0.29392371 0.3014712 0.25893992 0.18276203 0.097075231 0.028484035 -0.011542899 -0.032106534 -0.0455982 -0.057510957 -0.068576492]]...]
INFO - root - 2017-12-10 21:58:37.118562: step 57210, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 59h:01m:14s remains)
INFO - root - 2017-12-10 21:58:44.837062: step 57220, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 59h:00m:14s remains)
INFO - root - 2017-12-10 21:58:52.623635: step 57230, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 59h:23m:56s remains)
INFO - root - 2017-12-10 21:59:00.481574: step 57240, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 60h:48m:54s remains)
INFO - root - 2017-12-10 21:59:08.376860: step 57250, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.773 sec/batch; 59h:06m:56s remains)
INFO - root - 2017-12-10 21:59:16.143905: step 57260, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 59h:54m:14s remains)
INFO - root - 2017-12-10 21:59:23.965958: step 57270, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.790 sec/batch; 60h:24m:05s remains)
INFO - root - 2017-12-10 21:59:31.604021: step 57280, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 58h:25m:26s remains)
INFO - root - 2017-12-10 21:59:39.290812: step 57290, loss = 0.71, batch loss = 0.65 (13.0 examples/sec; 0.616 sec/batch; 47h:05m:46s remains)
INFO - root - 2017-12-10 21:59:47.207043: step 57300, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 59h:30m:14s remains)
2017-12-10 21:59:48.208358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.049433451 -0.051715128 -0.055994771 -0.062720276 -0.069376044 -0.073627263 -0.0753367 -0.075766623 -0.075372927 -0.072426885 -0.068136081 -0.065610535 -0.066987224 -0.071309134 -0.076134071][-0.022263227 -0.02038154 -0.023868619 -0.034240585 -0.04703515 -0.0567887 -0.061405323 -0.063310765 -0.063481323 -0.0580241 -0.049317438 -0.044607181 -0.048145533 -0.057314269 -0.067652278][0.027224034 0.040676072 0.044584077 0.035297364 0.018164359 0.0015282474 -0.0079427175 -0.013782213 -0.018164331 -0.014235711 -0.00431457 1.35355e-05 -0.0089969626 -0.0264044 -0.044736933][0.093158074 0.12732059 0.14863481 0.14969078 0.13477235 0.11374331 0.099796087 0.089210533 0.077496521 0.0749112 0.079981811 0.077825032 0.056702882 0.024811883 -0.0064066374][0.16621114 0.2301923 0.27973714 0.30162391 0.29808888 0.27891812 0.265481 0.25443032 0.23745467 0.22677699 0.22190835 0.20636332 0.16449766 0.10866851 0.055832263][0.2338623 0.332857 0.41714844 0.4665136 0.48168418 0.47126183 0.46550006 0.46043169 0.44289827 0.42402908 0.40358436 0.366244 0.29487014 0.20730296 0.12635739][0.29142836 0.42435843 0.54048395 0.61341393 0.64558822 0.6451568 0.64983082 0.65382695 0.6392504 0.61327618 0.57452524 0.51104683 0.40783066 0.28834927 0.18028654][0.331432 0.48838693 0.62249917 0.70412141 0.74138439 0.74475336 0.75585 0.76555848 0.7532829 0.7217887 0.66782576 0.58296084 0.45713407 0.31797853 0.19489688][0.33913609 0.50251347 0.63418865 0.70546329 0.73283648 0.73167455 0.741822 0.74981046 0.73609126 0.70165777 0.64121121 0.54829854 0.4185279 0.28090274 0.16235682][0.30368903 0.45349973 0.56324571 0.60934156 0.61600989 0.60378069 0.60528713 0.60379529 0.58424014 0.54957807 0.49329573 0.40894127 0.29602167 0.18134189 0.086385839][0.2230235 0.34175855 0.41746321 0.4341515 0.41921589 0.39544666 0.38451019 0.36992633 0.34321237 0.31146395 0.26778749 0.20441876 0.12256695 0.044386026 -0.014961098][0.11715703 0.197292 0.2385008 0.232146 0.20349321 0.17389338 0.15351398 0.12948304 0.099533975 0.07440567 0.047699593 0.0099883461 -0.037576567 -0.078576177 -0.10304729][0.016302934 0.060811736 0.076867342 0.060492374 0.030568516 0.0034771254 -0.018810676 -0.044367272 -0.071537293 -0.088817328 -0.10101596 -0.11734986 -0.13737744 -0.15047519 -0.15059626][-0.055167139 -0.036368202 -0.033776559 -0.049298894 -0.07126753 -0.090190336 -0.1075779 -0.12721848 -0.14638463 -0.15634781 -0.16027749 -0.16479006 -0.16943067 -0.1681461 -0.15748194][-0.086665869 -0.082465187 -0.084642828 -0.094580814 -0.10652789 -0.11668319 -0.12682755 -0.13799892 -0.14819838 -0.15294917 -0.15411189 -0.15483479 -0.15414694 -0.14892228 -0.13749436]]...]
INFO - root - 2017-12-10 21:59:56.024190: step 57310, loss = 0.67, batch loss = 0.61 (10.2 examples/sec; 0.784 sec/batch; 59h:57m:36s remains)
INFO - root - 2017-12-10 22:00:03.808215: step 57320, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 59h:48m:11s remains)
INFO - root - 2017-12-10 22:00:11.738994: step 57330, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 60h:50m:15s remains)
INFO - root - 2017-12-10 22:00:19.650134: step 57340, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 58h:15m:57s remains)
INFO - root - 2017-12-10 22:00:27.514463: step 57350, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 58h:45m:49s remains)
INFO - root - 2017-12-10 22:00:35.169882: step 57360, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 60h:10m:43s remains)
INFO - root - 2017-12-10 22:00:42.994530: step 57370, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 59h:50m:54s remains)
INFO - root - 2017-12-10 22:00:50.708088: step 57380, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 58h:55m:12s remains)
INFO - root - 2017-12-10 22:00:58.555695: step 57390, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 59h:25m:28s remains)
INFO - root - 2017-12-10 22:01:06.444464: step 57400, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 60h:52m:47s remains)
2017-12-10 22:01:07.279672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.048839893 -0.044872511 -0.028340615 -0.0011765747 0.033159714 0.072386332 0.11549333 0.15017924 0.17053968 0.18072776 0.18425666 0.18463056 0.18293349 0.18297049 0.18357551][-0.050481927 -0.046159409 -0.026755838 0.005407433 0.046716504 0.093894623 0.14452684 0.18570067 0.21254797 0.22924887 0.23755406 0.24150686 0.24321248 0.24559048 0.24539107][-0.050714485 -0.046776116 -0.02639018 0.0081160776 0.053237692 0.10414503 0.15700856 0.20056061 0.2324934 0.25613943 0.27005011 0.27827454 0.28305104 0.2859526 0.28255585][-0.051167712 -0.048297726 -0.028756566 0.0055801319 0.051252849 0.10215146 0.1531754 0.19686209 0.23390719 0.26557967 0.28675154 0.30061629 0.30885574 0.31098858 0.30138472][-0.05156216 -0.04984438 -0.032247428 4.3815617e-05 0.043515269 0.091530092 0.13877976 0.18221553 0.22432289 0.26353809 0.29196888 0.31155118 0.32355934 0.32497352 0.30780289][-0.051638339 -0.050577279 -0.034798257 -0.0048745121 0.035447977 0.07978759 0.12406864 0.1682982 0.21471302 0.25781131 0.28911075 0.31051031 0.32336617 0.32278964 0.29905409][-0.051254191 -0.050150048 -0.035032492 -0.0065698475 0.031096505 0.072854333 0.11636555 0.16241626 0.21214408 0.25625661 0.28681755 0.30602136 0.31643614 0.312764 0.28508413][-0.050597936 -0.048935175 -0.033237875 -0.00494664 0.0312565 0.07198669 0.11640694 0.16400853 0.21535566 0.25963184 0.28958181 0.3062641 0.31339687 0.30709192 0.2785067][-0.049891077 -0.047792036 -0.030988554 -0.0020756302 0.033504426 0.07409662 0.11947573 0.16677973 0.21670379 0.26022196 0.29125562 0.30816394 0.31485021 0.30807242 0.28061324][-0.049426556 -0.0472739 -0.029639425 -0.00029932405 0.034646876 0.07512223 0.12064986 0.16546656 0.20983244 0.24890497 0.27914277 0.29665384 0.30410352 0.29820308 0.27384913][-0.049270287 -0.047366083 -0.02970791 -0.00088418584 0.032924119 0.072990373 0.11816902 0.16014558 0.19770062 0.23042271 0.25742638 0.27354121 0.27982476 0.27323681 0.25182223][-0.049030155 -0.047660276 -0.030949723 -0.0038301242 0.028009592 0.066588752 0.11012687 0.14963806 0.18241352 0.21098609 0.23530793 0.24936897 0.25327402 0.24468833 0.22499695][-0.048460692 -0.047450449 -0.03212757 -0.007182797 0.02229744 0.05811419 0.0982859 0.13548857 0.16651167 0.19416703 0.21733858 0.22976667 0.23173931 0.22106634 0.20120585][-0.047471073 -0.046320543 -0.032419167 -0.0096030664 0.017523682 0.049771246 0.085568778 0.12075382 0.15222344 0.18064362 0.20325263 0.21444331 0.21519469 0.20231032 0.18009356][-0.04597266 -0.044290449 -0.031653903 -0.010877716 0.013652787 0.041513894 0.072189547 0.10504975 0.13727796 0.16638422 0.18829179 0.19887552 0.19939323 0.18523434 0.16056387]]...]
INFO - root - 2017-12-10 22:01:15.135382: step 57410, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 60h:30m:27s remains)
INFO - root - 2017-12-10 22:01:22.962572: step 57420, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 59h:52m:06s remains)
INFO - root - 2017-12-10 22:01:30.848383: step 57430, loss = 0.67, batch loss = 0.61 (9.7 examples/sec; 0.821 sec/batch; 62h:43m:02s remains)
INFO - root - 2017-12-10 22:01:38.531951: step 57440, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 59h:26m:29s remains)
INFO - root - 2017-12-10 22:01:46.452074: step 57450, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 59h:58m:22s remains)
INFO - root - 2017-12-10 22:01:54.341283: step 57460, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 59h:52m:36s remains)
INFO - root - 2017-12-10 22:02:01.863282: step 57470, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 58h:07m:31s remains)
INFO - root - 2017-12-10 22:02:09.634547: step 57480, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 59h:23m:38s remains)
INFO - root - 2017-12-10 22:02:17.391691: step 57490, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 60h:32m:32s remains)
INFO - root - 2017-12-10 22:02:25.235654: step 57500, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 58h:05m:59s remains)
2017-12-10 22:02:26.098722: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10205874 -0.052769378 0.033529263 0.16183721 0.31954584 0.45905995 0.53803009 0.543387 0.46757916 0.31794322 0.12761338 -0.044259131 -0.15622211 -0.20703416 -0.20977089][-0.0988799 -0.036225986 0.066677846 0.21146098 0.37813967 0.51776797 0.59146428 0.586675 0.49770638 0.33596188 0.13830353 -0.038291357 -0.15486893 -0.21042213 -0.21772686][-0.067279018 0.0053391573 0.11764175 0.2671631 0.42992562 0.56191504 0.6323697 0.62691361 0.54006225 0.38575307 0.19793116 0.024003271 -0.10195828 -0.17245138 -0.19272591][-0.029289925 0.053833805 0.17597355 0.32983753 0.48900989 0.6166321 0.69194597 0.69640863 0.62217689 0.48428774 0.3122969 0.14087625 -0.0023249283 -0.098236434 -0.13906704][0.0011253052 0.099979155 0.239895 0.40659827 0.57008255 0.69925672 0.78356338 0.79946095 0.73599696 0.60976249 0.45008436 0.28045222 0.12047081 -0.00028428651 -0.05985361][0.024933748 0.14407252 0.30767161 0.49320143 0.66504633 0.79524523 0.88355446 0.90489554 0.84604669 0.72533041 0.5758217 0.41350338 0.24915378 0.1166477 0.048038151][0.040232956 0.17662661 0.36017293 0.56062639 0.73595989 0.86005557 0.94137722 0.95982122 0.90220916 0.787172 0.65202409 0.50956428 0.36355361 0.24306013 0.18041976][0.034186035 0.17448242 0.36260384 0.56341624 0.73081106 0.83948809 0.90396953 0.9145624 0.86056542 0.75898576 0.64796722 0.54060537 0.43634248 0.35242012 0.31161976][0.002308426 0.1273369 0.29870263 0.48161706 0.6301403 0.71896046 0.76383424 0.76598769 0.71944773 0.63883948 0.55959111 0.49807489 0.45246997 0.42523319 0.42200997][-0.039784715 0.051836856 0.18716563 0.33836922 0.46486506 0.54016763 0.57416433 0.57361764 0.53669471 0.47582263 0.42320448 0.40273046 0.41602534 0.45179126 0.49582931][-0.064326085 -0.017320862 0.069417119 0.18295296 0.29143482 0.36605865 0.40534523 0.41277283 0.38702026 0.33789867 0.29630965 0.29705313 0.3501316 0.43646944 0.52445161][-0.040690754 -0.039353076 -0.0015996095 0.077992149 0.17578124 0.25839314 0.31154838 0.32902876 0.30679744 0.2543658 0.20435469 0.20358916 0.27156362 0.38719741 0.50826442][0.048351396 0.010531006 0.0065455553 0.059009705 0.14935862 0.23857284 0.30193946 0.32234338 0.29269564 0.22495726 0.15556873 0.13898063 0.20115429 0.32240114 0.45920527][0.18792854 0.13092646 0.10223529 0.13809122 0.22092821 0.30785063 0.36839247 0.37865877 0.33182979 0.24492259 0.15765579 0.12342265 0.16850773 0.27551594 0.40769076][0.34023792 0.28813952 0.25643331 0.28742632 0.36152369 0.43611005 0.48097146 0.47220987 0.40591684 0.30416059 0.20775877 0.162049 0.18702592 0.2669256 0.37702706]]...]
INFO - root - 2017-12-10 22:02:33.860008: step 57510, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 60h:27m:29s remains)
INFO - root - 2017-12-10 22:02:41.431295: step 57520, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 60h:04m:14s remains)
INFO - root - 2017-12-10 22:02:49.258331: step 57530, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 58h:53m:55s remains)
INFO - root - 2017-12-10 22:02:57.033463: step 57540, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 58h:49m:02s remains)
INFO - root - 2017-12-10 22:03:04.878213: step 57550, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.768 sec/batch; 58h:39m:10s remains)
INFO - root - 2017-12-10 22:03:12.511474: step 57560, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 60h:07m:56s remains)
INFO - root - 2017-12-10 22:03:20.294401: step 57570, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 59h:47m:04s remains)
INFO - root - 2017-12-10 22:03:28.071330: step 57580, loss = 0.70, batch loss = 0.64 (10.9 examples/sec; 0.734 sec/batch; 56h:03m:29s remains)
INFO - root - 2017-12-10 22:03:35.890537: step 57590, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 59h:45m:03s remains)
INFO - root - 2017-12-10 22:03:43.618372: step 57600, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.812 sec/batch; 61h:59m:28s remains)
2017-12-10 22:03:44.456337: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18637833 0.22213919 0.26788303 0.31228861 0.34680784 0.36425483 0.36214259 0.34103248 0.30670834 0.25853956 0.203623 0.15068135 0.11456856 0.10567315 0.12042058][0.14693937 0.17997536 0.22527665 0.27301535 0.31380188 0.33953297 0.34655818 0.3350935 0.30859691 0.2659905 0.21411292 0.16178077 0.12626325 0.11812853 0.13335493][0.10365489 0.13138975 0.17428294 0.22495911 0.27297005 0.30845851 0.32509285 0.32158738 0.29990524 0.260665 0.21226902 0.16394818 0.13431424 0.13275708 0.15303582][0.068500653 0.093541086 0.13689882 0.1925692 0.24824838 0.29243869 0.31598532 0.31567836 0.29299796 0.2516892 0.20413253 0.16121632 0.14105056 0.14988427 0.17824821][0.042173754 0.068566151 0.11541722 0.17666483 0.23825084 0.28815743 0.31549966 0.314663 0.2871449 0.24080233 0.19320035 0.15704697 0.14847398 0.16950673 0.20602934][0.02587465 0.055209577 0.10591469 0.17117378 0.23674665 0.29106861 0.32141432 0.31894687 0.28485772 0.23163627 0.18210083 0.15088417 0.15155281 0.18127523 0.22204137][0.019457597 0.049258288 0.10134778 0.16990358 0.24171397 0.30403772 0.3403081 0.33738831 0.29592228 0.232352 0.1746081 0.1406346 0.14249539 0.17359649 0.21361476][0.025371164 0.054370485 0.10719515 0.17957465 0.25898606 0.329759 0.37070283 0.365996 0.31631374 0.24034505 0.16981618 0.12677233 0.12348496 0.15123188 0.18827881][0.044418544 0.074378267 0.12753288 0.20156454 0.28424582 0.35785368 0.39895749 0.3912448 0.33475697 0.24825491 0.16492182 0.1107363 0.099783227 0.12282459 0.15749612][0.071214519 0.10314362 0.1557893 0.22833849 0.30918998 0.38057661 0.41928011 0.40912905 0.34864625 0.25511771 0.16148414 0.096830636 0.07781405 0.095525004 0.12761775][0.098857567 0.13294701 0.18413898 0.2520594 0.326201 0.39049864 0.42449856 0.41345507 0.35419056 0.26111931 0.16460346 0.094531044 0.068721212 0.079419032 0.1062305][0.12186144 0.15623975 0.20334472 0.26254743 0.32469538 0.37707084 0.40415862 0.39430851 0.34292847 0.26018554 0.17129222 0.10363424 0.074355736 0.077854469 0.097588889][0.1383068 0.17159532 0.21333247 0.26151168 0.30815712 0.34463522 0.36197859 0.35345039 0.31433392 0.25031018 0.17924407 0.12207003 0.093255572 0.090139821 0.10172211][0.15857199 0.18853609 0.22257978 0.25796306 0.28843856 0.30952108 0.31835774 0.31313503 0.28961971 0.24915205 0.20126219 0.15831654 0.13119456 0.12065952 0.12197499][0.19786969 0.22226046 0.24494243 0.26471937 0.2788702 0.2869364 0.28993317 0.2895411 0.28201294 0.26480642 0.23976 0.21099719 0.18550508 0.1671478 0.15657596]]...]
INFO - root - 2017-12-10 22:03:52.303548: step 57610, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 61h:20m:25s remains)
INFO - root - 2017-12-10 22:04:00.203168: step 57620, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 61h:25m:02s remains)
INFO - root - 2017-12-10 22:04:08.016070: step 57630, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 59h:50m:13s remains)
INFO - root - 2017-12-10 22:04:15.890261: step 57640, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 60h:40m:52s remains)
INFO - root - 2017-12-10 22:04:23.579973: step 57650, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 59h:35m:05s remains)
INFO - root - 2017-12-10 22:04:31.376733: step 57660, loss = 0.67, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 59h:27m:15s remains)
INFO - root - 2017-12-10 22:04:39.210411: step 57670, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 60h:43m:42s remains)
INFO - root - 2017-12-10 22:04:46.921706: step 57680, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.823 sec/batch; 62h:50m:10s remains)
INFO - root - 2017-12-10 22:04:54.743288: step 57690, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.805 sec/batch; 61h:28m:15s remains)
INFO - root - 2017-12-10 22:05:02.560748: step 57700, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 59h:30m:02s remains)
2017-12-10 22:05:03.497285: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13147339 0.10913771 0.0949207 0.0955746 0.10993229 0.13121207 0.15166482 0.16300867 0.16156118 0.14782287 0.12953174 0.11624294 0.11279781 0.11654102 0.12060149][0.14741078 0.12894472 0.11674165 0.11899313 0.13437533 0.15521991 0.17292105 0.17928758 0.17227784 0.15380527 0.13221446 0.11691552 0.11321476 0.11744223 0.12216424][0.16177732 0.14904496 0.13983424 0.14238037 0.15662755 0.17549543 0.19063282 0.19465083 0.1868358 0.16954067 0.14951669 0.13537949 0.13342653 0.13949811 0.14556773][0.18133351 0.17185807 0.16292723 0.16244504 0.17348914 0.19097939 0.20676662 0.2129094 0.20906891 0.19725791 0.18112999 0.16818428 0.1670327 0.17475064 0.18199994][0.20292404 0.1950887 0.18558049 0.1840941 0.19722609 0.22037992 0.24305296 0.25417048 0.25274372 0.24184649 0.22422703 0.20728065 0.20241389 0.20844737 0.21479394][0.22375599 0.21844059 0.21048471 0.21206535 0.23223707 0.26552916 0.29793686 0.314588 0.31290632 0.29798025 0.27348536 0.24754117 0.23373921 0.23280253 0.23373516][0.23598512 0.23381266 0.228703 0.23381437 0.25915378 0.29850045 0.33596858 0.35456082 0.35001564 0.3289955 0.29632264 0.2609545 0.23773612 0.2295984 0.22673374][0.24052353 0.24293453 0.24152841 0.24770895 0.27053359 0.30424169 0.33515653 0.34780392 0.33803168 0.31283125 0.27633229 0.23705317 0.20986313 0.20045887 0.20072885][0.24770302 0.25321954 0.2518459 0.25285825 0.26355544 0.28020582 0.29440027 0.2954573 0.28062046 0.25551814 0.22237901 0.18739361 0.16430996 0.160978 0.17067552][0.25253633 0.25733972 0.25127622 0.24294935 0.23767115 0.23386329 0.22901155 0.21848142 0.20115678 0.18046091 0.15710944 0.13473478 0.12377156 0.13091511 0.15017201][0.24037252 0.24119993 0.22953737 0.212347 0.19329835 0.17281935 0.15363465 0.13639022 0.12117621 0.10904904 0.09930595 0.093409277 0.097242758 0.11393121 0.13786472][0.20204264 0.20056777 0.18612543 0.16375695 0.13642874 0.10625156 0.079397365 0.05976361 0.048384737 0.045078021 0.048123997 0.05739795 0.0744742 0.098427147 0.12380744][0.14914609 0.14996652 0.13851203 0.11744183 0.089257263 0.056659028 0.027352583 0.0070865653 -0.0015716864 0.0014715577 0.013550432 0.032807756 0.05763771 0.084219664 0.10726732][0.11032937 0.11682457 0.11175601 0.095670104 0.07056497 0.039202362 0.0098047527 -0.01032755 -0.016814511 -0.0094876066 0.0072215884 0.029355813 0.053829852 0.0760215 0.092103355][0.092684925 0.10537678 0.10712887 0.096463993 0.074756674 0.045039941 0.016420024 -0.0020079538 -0.0041927937 0.00925965 0.03038534 0.051987581 0.07029172 0.082289338 0.087821782]]...]
INFO - root - 2017-12-10 22:05:11.346133: step 57710, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 59h:44m:23s remains)
INFO - root - 2017-12-10 22:05:19.130842: step 57720, loss = 0.70, batch loss = 0.65 (10.7 examples/sec; 0.749 sec/batch; 57h:11m:58s remains)
INFO - root - 2017-12-10 22:05:27.011143: step 57730, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 60h:57m:47s remains)
INFO - root - 2017-12-10 22:05:34.692330: step 57740, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 59h:53m:23s remains)
INFO - root - 2017-12-10 22:05:42.608460: step 57750, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 60h:34m:34s remains)
INFO - root - 2017-12-10 22:05:50.229485: step 57760, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 58h:37m:45s remains)
INFO - root - 2017-12-10 22:05:58.197115: step 57770, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 61h:31m:02s remains)
INFO - root - 2017-12-10 22:06:06.095758: step 57780, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 61h:06m:55s remains)
INFO - root - 2017-12-10 22:06:13.913800: step 57790, loss = 0.69, batch loss = 0.63 (10.8 examples/sec; 0.744 sec/batch; 56h:45m:33s remains)
INFO - root - 2017-12-10 22:06:21.825153: step 57800, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 59h:42m:38s remains)
2017-12-10 22:06:22.739505: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2808637 0.27114928 0.23112896 0.17674382 0.13209304 0.11287194 0.12010969 0.14740197 0.19302693 0.24938765 0.30235857 0.34426734 0.36818168 0.37185788 0.35755739][0.36591166 0.35649806 0.31588504 0.26095575 0.21436085 0.18950096 0.185437 0.19681495 0.22465019 0.26452661 0.30720606 0.34804606 0.379136 0.39201766 0.3842341][0.4117164 0.40118459 0.366202 0.3244938 0.29288545 0.27692479 0.26862696 0.26293167 0.26543215 0.27675334 0.29640359 0.32533866 0.35680264 0.37595552 0.3727071][0.42149794 0.41165796 0.388301 0.36988965 0.36595485 0.37165251 0.36926535 0.35267255 0.33235371 0.31371909 0.3052499 0.31427041 0.33674797 0.35362995 0.34770116][0.407178 0.40447012 0.40005848 0.41053438 0.43958095 0.4741897 0.48825973 0.47206023 0.43868881 0.39678198 0.36194417 0.3467069 0.34961417 0.35130152 0.3319124][0.37949312 0.39022309 0.40945268 0.44930246 0.50953257 0.57320338 0.60860783 0.60029644 0.56109452 0.50197077 0.4437952 0.40209135 0.37803602 0.35542622 0.31671581][0.34633246 0.37300569 0.4155038 0.48033607 0.56495667 0.6519869 0.70703316 0.70881349 0.66752446 0.59494084 0.51533717 0.44668093 0.39405909 0.347082 0.292422][0.31158963 0.35157612 0.4104805 0.48846608 0.58227921 0.677133 0.740093 0.747127 0.70501113 0.62543392 0.5336771 0.44950461 0.38136831 0.32421315 0.26773509][0.26080671 0.30871156 0.37230453 0.44658649 0.52932882 0.61128128 0.66602772 0.67187309 0.63311946 0.56035197 0.47582951 0.39793164 0.33649069 0.29012233 0.24955598][0.18048835 0.22750166 0.28537109 0.34572226 0.40727872 0.46612236 0.50417113 0.50535536 0.47284952 0.41570485 0.3508099 0.29312977 0.25249919 0.22989161 0.21644905][0.083028279 0.11940219 0.16334245 0.2059264 0.24626063 0.28293732 0.30425575 0.29982212 0.27229536 0.22946864 0.18386102 0.14746821 0.12982997 0.13292716 0.14807312][-0.0039324341 0.016869325 0.043137837 0.067972384 0.0910889 0.11142576 0.12094387 0.11318316 0.090121351 0.058226496 0.026956487 0.0062325615 0.0050744372 0.02561507 0.059784923][-0.064274125 -0.058160886 -0.048021689 -0.0384636 -0.028982664 -0.020456759 -0.018124092 -0.025919942 -0.042858094 -0.064317092 -0.083592914 -0.093162283 -0.08568462 -0.059443422 -0.020679982][-0.098973535 -0.10286853 -0.10359212 -0.103821 -0.1025199 -0.10020311 -0.099869743 -0.10409503 -0.11318906 -0.12491216 -0.13531089 -0.1392335 -0.13123807 -0.10975496 -0.078382634][-0.10503674 -0.11373848 -0.11942188 -0.12353201 -0.12469357 -0.12318166 -0.12094587 -0.12029614 -0.12288344 -0.12851764 -0.13530852 -0.13971204 -0.1375356 -0.12688613 -0.10907206]]...]
INFO - root - 2017-12-10 22:06:30.611840: step 57810, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 61h:29m:35s remains)
INFO - root - 2017-12-10 22:06:38.334861: step 57820, loss = 0.69, batch loss = 0.64 (12.9 examples/sec; 0.618 sec/batch; 47h:08m:38s remains)
INFO - root - 2017-12-10 22:06:46.115352: step 57830, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 59h:11m:59s remains)
INFO - root - 2017-12-10 22:06:53.874153: step 57840, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 60h:20m:24s remains)
INFO - root - 2017-12-10 22:07:01.743545: step 57850, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 60h:12m:08s remains)
INFO - root - 2017-12-10 22:07:09.598052: step 57860, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.827 sec/batch; 63h:04m:03s remains)
INFO - root - 2017-12-10 22:07:17.440636: step 57870, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 61h:02m:29s remains)
INFO - root - 2017-12-10 22:07:25.331536: step 57880, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 59h:57m:37s remains)
INFO - root - 2017-12-10 22:07:33.185140: step 57890, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 59h:59m:24s remains)
INFO - root - 2017-12-10 22:07:41.074333: step 57900, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 61h:32m:35s remains)
2017-12-10 22:07:41.994893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.019848572 -0.017405324 -0.014509917 -0.013298044 -0.01471413 -0.018394021 -0.022043442 -0.024815878 -0.026613137 -0.027482228 -0.027811479 -0.027916027 -0.028518066 -0.029365068 -0.029907823][-0.011475685 -0.0041402532 0.00377656 0.0083951456 0.00755152 0.0015975696 -0.0055677406 -0.011942515 -0.017001642 -0.020131927 -0.021879235 -0.02277749 -0.02433311 -0.026576685 -0.02855725][0.001075925 0.016074203 0.031978451 0.042242832 0.043202981 0.034907792 0.022885019 0.011043638 0.00064926792 -0.0061454396 -0.010072025 -0.012336714 -0.01529771 -0.019583618 -0.023742102][0.015923906 0.04150302 0.068733446 0.0872044 0.091481984 0.081065081 0.063000463 0.043850821 0.02583475 0.013531738 0.0061412756 0.0016518517 -0.0033170511 -0.010386198 -0.017412189][0.031078424 0.069224492 0.10988213 0.13799454 0.14637494 0.13396774 0.10921491 0.081715629 0.054809146 0.035892773 0.02439362 0.017133534 0.0097246841 -0.00062486081 -0.010906033][0.043472491 0.093573794 0.14669953 0.18376741 0.19629717 0.18250619 0.15208271 0.11748171 0.083027378 0.0584608 0.043294348 0.033219256 0.023025475 0.0090188794 -0.0048277476][0.050381884 0.10874215 0.17033866 0.21346754 0.22921543 0.21507113 0.18140331 0.14295731 0.10459957 0.077159211 0.0597165 0.047327846 0.034629676 0.017268071 4.7683719e-05][0.050776929 0.11185437 0.17619322 0.22120683 0.23840238 0.2247787 0.19099955 0.15321498 0.11609761 0.089756429 0.072219886 0.058769509 0.044381853 0.024352949 0.0039636041][0.044237122 0.10167415 0.16235825 0.20491041 0.22164537 0.20930745 0.17856264 0.14587423 0.11493624 0.093452215 0.078183033 0.065457515 0.050848495 0.029348882 0.0065827295][0.030662671 0.078500628 0.12933327 0.1650991 0.1792829 0.16868089 0.1434788 0.11919942 0.098164983 0.084471457 0.073659025 0.063451491 0.050360255 0.0294217 0.0060354695][0.011949531 0.046138354 0.08305715 0.10907036 0.1192136 0.11063185 0.09229929 0.0776588 0.067652993 0.062697358 0.057374865 0.050834846 0.040772822 0.022688378 0.0012009698][-0.0077042221 0.012169515 0.034620591 0.050383627 0.056136515 0.049606569 0.037807267 0.031234734 0.029819723 0.031483728 0.030771198 0.027888056 0.021730848 0.0085013853 -0.0083478969][-0.023656914 -0.015654054 -0.0051732971 0.0020983655 0.0042146822 -0.00039287945 -0.0068802419 -0.0080495 -0.0046556648 0.00026623614 0.002261101 0.0019817078 -0.00076959853 -0.0088187428 -0.019822283][-0.033550367 -0.033455119 -0.031088628 -0.029603265 -0.029931528 -0.032889422 -0.035574097 -0.033987153 -0.029242894 -0.024127891 -0.021448812 -0.020540467 -0.021083467 -0.024794828 -0.030205259][-0.037257884 -0.041201662 -0.043146677 -0.044843737 -0.046479844 -0.048262507 -0.04880935 -0.046636462 -0.042757224 -0.039111134 -0.036987491 -0.035901185 -0.035333786 -0.03592601 -0.03709022]]...]
INFO - root - 2017-12-10 22:07:49.707866: step 57910, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 61h:31m:19s remains)
INFO - root - 2017-12-10 22:07:57.448549: step 57920, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 58h:52m:24s remains)
INFO - root - 2017-12-10 22:08:05.322217: step 57930, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.823 sec/batch; 62h:47m:49s remains)
INFO - root - 2017-12-10 22:08:13.263975: step 57940, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.817 sec/batch; 62h:18m:55s remains)
INFO - root - 2017-12-10 22:08:21.166363: step 57950, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 62h:10m:24s remains)
INFO - root - 2017-12-10 22:08:29.183772: step 57960, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 60h:42m:06s remains)
INFO - root - 2017-12-10 22:08:37.031746: step 57970, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 59h:44m:05s remains)
INFO - root - 2017-12-10 22:08:45.015396: step 57980, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 58h:41m:23s remains)
INFO - root - 2017-12-10 22:08:52.835487: step 57990, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 60h:41m:26s remains)
INFO - root - 2017-12-10 22:09:00.417015: step 58000, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 59h:00m:33s remains)
2017-12-10 22:09:01.317011: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00838044 -0.021930082 -0.026523212 -0.030365018 -0.037735537 -0.047875788 -0.052751794 -0.04594313 -0.031276725 -0.019714314 -0.018978406 -0.029670179 -0.046146419 -0.06071455 -0.068281457][0.038208958 0.021752359 0.019217605 0.017267331 0.0061259284 -0.014944288 -0.032324016 -0.032317393 -0.01726565 -0.0014142086 0.0023288117 -0.0092981448 -0.031314187 -0.0535972 -0.067502223][0.096987836 0.086679079 0.095440172 0.10375057 0.092797 0.058830086 0.022000425 0.0057587684 0.011504334 0.024161734 0.026986359 0.013752853 -0.01284171 -0.042169623 -0.062575169][0.14528272 0.1520997 0.18396235 0.21423383 0.21289964 0.17293833 0.12045138 0.086311556 0.07445462 0.07154543 0.061777506 0.03918447 0.0049876939 -0.031525008 -0.057875142][0.16115187 0.19096087 0.2528863 0.31382155 0.33411667 0.30580604 0.25750509 0.21876442 0.19014491 0.16127311 0.12368388 0.077925295 0.02693665 -0.020997968 -0.054164786][0.13644755 0.18302037 0.26927257 0.35878572 0.40831783 0.41298282 0.39915258 0.38276094 0.35184121 0.29749066 0.22363155 0.14225867 0.062350642 -0.0056000063 -0.049789406][0.0866808 0.13325456 0.22423512 0.32597622 0.40045413 0.44951695 0.49386171 0.5270282 0.51722342 0.45284393 0.34988913 0.23203057 0.11612197 0.019778702 -0.041116182][0.041032162 0.06849888 0.13940562 0.22948757 0.3124963 0.39796594 0.50097162 0.59388351 0.62522584 0.57604349 0.46682212 0.32627776 0.17836531 0.051705349 -0.028940575][0.0216615 0.019246774 0.05274184 0.11132401 0.18187821 0.27886733 0.41481885 0.551677 0.62658483 0.61173153 0.52214557 0.38327357 0.22132465 0.075216807 -0.020024789][0.02582618 -0.0018014832 -0.0063920217 0.015015237 0.058979366 0.14029261 0.27147561 0.41724607 0.51601106 0.53520352 0.47930306 0.36487356 0.21440117 0.071110278 -0.024003735][0.029955568 -0.0073440555 -0.03488785 -0.040765002 -0.023894593 0.027084213 0.12458827 0.24401118 0.33662289 0.37240058 0.34738386 0.26899394 0.15164571 0.034242142 -0.043976814][0.014765853 -0.016462052 -0.047112253 -0.063289918 -0.063415959 -0.040879898 0.014403901 0.089357883 0.15312059 0.18482313 0.17778587 0.13340369 0.05793288 -0.020591293 -0.071753912][-0.017909288 -0.036927074 -0.057464555 -0.07029976 -0.075257249 -0.069801591 -0.0476181 -0.013825471 0.016497077 0.032861952 0.030941365 0.010159104 -0.028665815 -0.069817811 -0.094479941][-0.052529663 -0.06062375 -0.068389773 -0.07315322 -0.075883292 -0.076394014 -0.071766242 -0.063501693 -0.05690385 -0.054785788 -0.058220796 -0.067213446 -0.082571842 -0.097678013 -0.10365148][-0.0724227 -0.074346863 -0.073852524 -0.072919011 -0.07271263 -0.07369823 -0.074951313 -0.076748848 -0.080092095 -0.084592216 -0.089633867 -0.09434247 -0.098916538 -0.10126644 -0.09847077]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 22:09:09.196465: step 58010, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 60h:19m:52s remains)
INFO - root - 2017-12-10 22:09:17.019923: step 58020, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 60h:24m:17s remains)
INFO - root - 2017-12-10 22:09:24.916185: step 58030, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.803 sec/batch; 61h:12m:50s remains)
INFO - root - 2017-12-10 22:09:32.874226: step 58040, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 59h:05m:45s remains)
INFO - root - 2017-12-10 22:09:40.686350: step 58050, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 59h:49m:51s remains)
INFO - root - 2017-12-10 22:09:48.497130: step 58060, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.788 sec/batch; 60h:05m:01s remains)
INFO - root - 2017-12-10 22:09:56.354541: step 58070, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.795 sec/batch; 60h:37m:34s remains)
INFO - root - 2017-12-10 22:10:04.034048: step 58080, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 59h:50m:21s remains)
INFO - root - 2017-12-10 22:10:11.998716: step 58090, loss = 0.70, batch loss = 0.65 (9.8 examples/sec; 0.815 sec/batch; 62h:05m:46s remains)
INFO - root - 2017-12-10 22:10:19.903347: step 58100, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 58h:33m:15s remains)
2017-12-10 22:10:20.702341: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20763792 0.14631489 0.12605308 0.17420945 0.28416738 0.42255104 0.54328668 0.6011098 0.57137507 0.47812527 0.37879777 0.32354933 0.32943213 0.36929125 0.38515261][0.31809828 0.24147922 0.20275569 0.23667251 0.33677736 0.46965796 0.58767921 0.64529407 0.61640149 0.521703 0.41833562 0.36049947 0.37004367 0.41844648 0.44042933][0.4235445 0.34388673 0.29412597 0.31264338 0.39307839 0.50424808 0.60096258 0.6431486 0.60994124 0.51943815 0.42266279 0.37136152 0.38830179 0.44372073 0.46908957][0.50671029 0.4420273 0.39326268 0.40171912 0.46286863 0.54977578 0.62088543 0.64238 0.60085475 0.51357573 0.4245159 0.380302 0.40237468 0.4594073 0.48224208][0.56208938 0.52290791 0.48519215 0.4915404 0.54132158 0.61282676 0.66704214 0.673413 0.62409097 0.53777641 0.45328769 0.4120124 0.43310481 0.48386511 0.496367][0.61528927 0.59835845 0.569041 0.57375586 0.61813527 0.6846121 0.73413181 0.7353605 0.68241048 0.59686708 0.51582283 0.47629026 0.49348128 0.53219664 0.52660763][0.68412179 0.67833489 0.64737117 0.64439273 0.68343526 0.750239 0.80371773 0.808831 0.75766 0.67416066 0.59557962 0.55641997 0.56751192 0.590537 0.56268555][0.74946541 0.74538463 0.70657426 0.69145375 0.72181082 0.7866466 0.84462208 0.85758567 0.81316471 0.7347008 0.65955985 0.620189 0.62365639 0.62962049 0.58024842][0.76430345 0.76058888 0.71574163 0.68920678 0.70685363 0.76183867 0.81767559 0.83728456 0.80420625 0.73804176 0.67295969 0.637234 0.63434207 0.62438846 0.55781156][0.69878024 0.69209331 0.64535445 0.611648 0.61506534 0.65244788 0.69654828 0.716897 0.696339 0.649021 0.60220176 0.57737058 0.57440853 0.55632144 0.48371577][0.55924726 0.55036539 0.50725138 0.47140506 0.46110028 0.47596058 0.50001079 0.51338017 0.50215095 0.47515342 0.45039529 0.44143766 0.44500172 0.42861009 0.36297232][0.36276194 0.35374561 0.32131174 0.2921164 0.2759425 0.27328989 0.27799904 0.28115058 0.274506 0.26327524 0.25686079 0.26168215 0.27277747 0.26422024 0.21559294][0.1567798 0.14764567 0.1279045 0.11103542 0.097934663 0.088062756 0.081365436 0.077369347 0.073495917 0.072754093 0.078565866 0.092321634 0.1085685 0.10880429 0.079731643][0.0020471765 -0.0082088131 -0.018989105 -0.025998086 -0.033907685 -0.0445385 -0.055072032 -0.061672945 -0.063148826 -0.057655711 -0.045374665 -0.027628943 -0.0095257824 -0.0029173091 -0.01578987][-0.078310736 -0.090182126 -0.097146749 -0.10008187 -0.1046348 -0.11307195 -0.12264679 -0.12906812 -0.12970875 -0.12321898 -0.11090344 -0.09462522 -0.078147262 -0.068253785 -0.069626957]]...]
INFO - root - 2017-12-10 22:10:28.544998: step 58110, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 60h:08m:03s remains)
INFO - root - 2017-12-10 22:10:36.431685: step 58120, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.797 sec/batch; 60h:42m:27s remains)
INFO - root - 2017-12-10 22:10:44.230552: step 58130, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 59h:13m:05s remains)
INFO - root - 2017-12-10 22:10:52.078823: step 58140, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.764 sec/batch; 58h:11m:31s remains)
INFO - root - 2017-12-10 22:10:59.961800: step 58150, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 59h:41m:14s remains)
INFO - root - 2017-12-10 22:11:07.595010: step 58160, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 60h:28m:53s remains)
INFO - root - 2017-12-10 22:11:15.538685: step 58170, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 61h:49m:57s remains)
INFO - root - 2017-12-10 22:11:23.289720: step 58180, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 59h:11m:29s remains)
INFO - root - 2017-12-10 22:11:31.159017: step 58190, loss = 0.67, batch loss = 0.61 (10.0 examples/sec; 0.797 sec/batch; 60h:45m:00s remains)
INFO - root - 2017-12-10 22:11:39.049048: step 58200, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 58h:12m:25s remains)
2017-12-10 22:11:40.024814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.045647308 -0.033271424 -0.0170434 -0.001273572 0.011805189 0.018484769 0.017418088 0.0055291951 -0.014298069 -0.034779068 -0.049582005 -0.056797042 -0.054415062 -0.037862107 -0.011914426][-0.002150673 0.023915753 0.054130998 0.083542638 0.11023533 0.12830144 0.13373007 0.1216385 0.094299451 0.06029528 0.029832868 0.0097392276 0.0068088239 0.029537544 0.070670567][0.055128261 0.099418648 0.14696047 0.19227229 0.23392446 0.26464662 0.27788305 0.2680102 0.23630913 0.19109027 0.14446372 0.11027037 0.10229267 0.132338 0.1900337][0.11415797 0.17418684 0.23539688 0.29241344 0.34427521 0.38403669 0.40340143 0.39733008 0.36542484 0.31403914 0.25520536 0.20902692 0.19834763 0.2376115 0.31342968][0.16402839 0.23488341 0.30341288 0.36482188 0.41817164 0.45892739 0.48040348 0.47901812 0.45227158 0.40269166 0.33941117 0.28546718 0.27128431 0.31530812 0.40397316][0.20792018 0.28933248 0.36423096 0.42843875 0.47929007 0.5148077 0.53378761 0.53524023 0.51498944 0.47119004 0.40845647 0.34863722 0.32566634 0.36171505 0.44754294][0.2535961 0.3457011 0.42730176 0.49499917 0.54245877 0.56982523 0.58371258 0.58558023 0.56933892 0.52913892 0.46674708 0.39943483 0.36107337 0.37541658 0.44074544][0.29240707 0.38939139 0.473316 0.54214472 0.58496994 0.60323364 0.61142451 0.61206853 0.59658718 0.55592519 0.49122381 0.4149453 0.35729414 0.34320676 0.37964857][0.30851984 0.39849988 0.475871 0.54086381 0.57851863 0.59012383 0.59530956 0.59691006 0.58377492 0.544543 0.48023775 0.39761665 0.32108125 0.27696741 0.28146178][0.29766443 0.37004876 0.43440858 0.49343944 0.52928728 0.53986537 0.54576659 0.55032521 0.54207534 0.50865942 0.4505569 0.36906481 0.28023335 0.20977992 0.1818265][0.27335227 0.32134223 0.36634576 0.41497031 0.44936019 0.46277228 0.47159812 0.47958264 0.47692555 0.45206916 0.40478382 0.33271992 0.24388924 0.1593864 0.10657705][0.23099039 0.25134847 0.27082875 0.30146596 0.32978371 0.34571287 0.35727194 0.36762831 0.37051815 0.35716385 0.32582965 0.27220666 0.19812725 0.11812038 0.056619581][0.16486533 0.16069615 0.15557811 0.1658477 0.18497474 0.20181568 0.2153632 0.2273555 0.23552087 0.23525715 0.22282936 0.19287522 0.14330629 0.082242489 0.027597848][0.093874611 0.074032933 0.052371424 0.046785906 0.056702096 0.071973629 0.085196875 0.097060733 0.10861509 0.11924388 0.12371717 0.11562156 0.091037296 0.052432306 0.010939359][0.033482518 0.007694921 -0.020239234 -0.034321811 -0.032066852 -0.021694561 -0.012625821 -0.0043214182 0.0062870593 0.020912215 0.03426984 0.040430963 0.034839358 0.015957637 -0.01098342]]...]
INFO - root - 2017-12-10 22:11:47.970051: step 58210, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 57h:38m:22s remains)
INFO - root - 2017-12-10 22:11:55.745949: step 58220, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 58h:39m:42s remains)
INFO - root - 2017-12-10 22:12:03.686734: step 58230, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 61h:16m:13s remains)
INFO - root - 2017-12-10 22:12:11.404935: step 58240, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 61h:07m:06s remains)
INFO - root - 2017-12-10 22:12:19.203839: step 58250, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.750 sec/batch; 57h:08m:26s remains)
INFO - root - 2017-12-10 22:12:27.103507: step 58260, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 60h:31m:43s remains)
INFO - root - 2017-12-10 22:12:34.772098: step 58270, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 59h:29m:49s remains)
INFO - root - 2017-12-10 22:12:42.651270: step 58280, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 61h:33m:16s remains)
INFO - root - 2017-12-10 22:12:50.544438: step 58290, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 59h:11m:49s remains)
INFO - root - 2017-12-10 22:12:58.377432: step 58300, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 60h:15m:30s remains)
2017-12-10 22:12:59.261252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.023534592 -0.018124783 -0.017843762 -0.021061609 -0.020999555 -0.016916525 -0.014445771 -0.016724305 -0.022995131 -0.031061159 -0.038379192 -0.043305263 -0.046122719 -0.048274189 -0.052561335][0.021012656 0.031195695 0.032985233 0.030135283 0.034086619 0.045178339 0.053106152 0.051078718 0.040962115 0.027311105 0.01579853 0.008702782 0.0044000177 -0.00023017979 -0.010151244][0.091563493 0.10711754 0.1106632 0.10914444 0.11976201 0.14223365 0.15950029 0.15980159 0.14665723 0.12731412 0.11166304 0.10343107 0.098510094 0.089980945 0.070189483][0.17234039 0.19253907 0.19753422 0.19808464 0.21726857 0.25412288 0.28370586 0.28837654 0.27343804 0.24892433 0.22948781 0.22029683 0.214639 0.20138504 0.16930439][0.24408616 0.26889795 0.27599028 0.2798036 0.30758741 0.35804695 0.39994314 0.40968847 0.39361936 0.36458573 0.34175244 0.33094364 0.32339627 0.30446932 0.2598868][0.29943565 0.32838452 0.3378191 0.34491873 0.378704 0.43834111 0.48978156 0.50540626 0.49056298 0.45941761 0.43442383 0.42162511 0.41106308 0.38518569 0.32859647][0.33560055 0.37003174 0.38147509 0.38895187 0.42180946 0.48170456 0.53640485 0.55715555 0.54692215 0.51862979 0.49533513 0.48229975 0.46883076 0.43588632 0.36930993][0.34463248 0.38939381 0.40566003 0.41136774 0.43459016 0.48156771 0.52730989 0.54697931 0.54156959 0.5202238 0.50307196 0.49292105 0.47845197 0.4406493 0.36886871][0.32415041 0.38370126 0.41019893 0.41563448 0.4240635 0.4472689 0.47239292 0.48299545 0.47878584 0.4640187 0.45431361 0.44918582 0.43705913 0.39997411 0.33084449][0.28874248 0.36067823 0.39880997 0.40670058 0.40190583 0.40076041 0.40385818 0.40446702 0.40057662 0.39070961 0.38708767 0.38859043 0.3838467 0.35424608 0.29332784][0.2537767 0.32793215 0.37185889 0.38247359 0.37046719 0.35422719 0.34442842 0.3420752 0.34254616 0.33849618 0.3406949 0.35099661 0.35799173 0.33945274 0.28622815][0.22092262 0.28648934 0.32877156 0.34147853 0.32909402 0.30893487 0.2964263 0.29711887 0.30503541 0.309488 0.32064021 0.34286582 0.36298904 0.35413125 0.3041569][0.18455189 0.23276606 0.26721409 0.28091887 0.2734288 0.25740623 0.24770369 0.25271097 0.26798823 0.2830635 0.30694219 0.34309953 0.37498909 0.37309262 0.3243002][0.14837174 0.1749213 0.19698747 0.20868155 0.20717445 0.19973177 0.19707465 0.20792024 0.23055822 0.25687465 0.29329938 0.34022659 0.37908515 0.38137269 0.3345603][0.11922587 0.1262155 0.1351745 0.14211863 0.14489807 0.14741492 0.15505335 0.17403783 0.20387252 0.23933774 0.28376007 0.33439919 0.3728238 0.37426275 0.32871863]]...]
INFO - root - 2017-12-10 22:13:06.971056: step 58310, loss = 0.70, batch loss = 0.64 (11.8 examples/sec; 0.679 sec/batch; 51h:44m:19s remains)
INFO - root - 2017-12-10 22:13:14.703660: step 58320, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 59h:00m:11s remains)
INFO - root - 2017-12-10 22:13:22.574032: step 58330, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 59h:04m:39s remains)
INFO - root - 2017-12-10 22:13:30.452186: step 58340, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 59h:14m:55s remains)
INFO - root - 2017-12-10 22:13:38.235676: step 58350, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 59h:44m:30s remains)
INFO - root - 2017-12-10 22:13:45.921510: step 58360, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 60h:17m:30s remains)
INFO - root - 2017-12-10 22:13:53.698007: step 58370, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 58h:24m:04s remains)
INFO - root - 2017-12-10 22:14:01.569202: step 58380, loss = 0.69, batch loss = 0.64 (9.7 examples/sec; 0.823 sec/batch; 62h:40m:04s remains)
INFO - root - 2017-12-10 22:14:09.363373: step 58390, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 58h:42m:54s remains)
INFO - root - 2017-12-10 22:14:17.020240: step 58400, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.752 sec/batch; 57h:17m:20s remains)
2017-12-10 22:14:17.855429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.025940837 -0.027756184 -0.027448766 -0.025985017 -0.02351127 -0.020686323 -0.017128617 -0.012962809 -0.0088612409 -0.00512272 -0.0013564406 0.002496266 0.0049296203 0.0067311227 0.00921102][-0.020121515 -0.020420287 -0.017588651 -0.012354135 -0.005395079 0.0017891789 0.0097744726 0.018752005 0.02726192 0.034029238 0.039226975 0.042713854 0.042045951 0.03846065 0.035082582][-0.01146456 -0.0091820741 -0.0025706235 0.0077520241 0.02080209 0.034599647 0.0502969 0.067567058 0.082981594 0.093601562 0.098700188 0.09825103 0.089271262 0.074695714 0.060507063][-0.0023773958 0.0044428562 0.017313741 0.035531137 0.05777701 0.081586137 0.10830985 0.13581985 0.15801828 0.17027992 0.17158088 0.16251062 0.13990353 0.10941451 0.080382571][0.0099191247 0.025724813 0.050289888 0.081467554 0.1167689 0.15281971 0.19059086 0.22567599 0.2501021 0.25939009 0.25303325 0.23197107 0.19318528 0.14441043 0.098035589][0.028354211 0.057084177 0.09809038 0.14605922 0.19641961 0.24448036 0.29023409 0.32630256 0.34464112 0.343712 0.32486236 0.289921 0.23565544 0.17037515 0.10794644][0.052358005 0.095302813 0.15332177 0.21722266 0.28006506 0.33629483 0.38498327 0.41474822 0.41878077 0.40062609 0.36481842 0.31502822 0.24791905 0.17218329 0.1020221][0.078478806 0.13293217 0.20323762 0.27625254 0.34338838 0.40024868 0.44552743 0.46445066 0.45194322 0.4160279 0.36461183 0.30287224 0.22867179 0.15155093 0.085199818][0.098593615 0.1596233 0.23588997 0.31043696 0.37295657 0.42095351 0.45366678 0.45636827 0.42751259 0.37854892 0.3190372 0.25403038 0.1831971 0.11664147 0.06603615][0.10347883 0.16331358 0.23729496 0.30579957 0.35667971 0.38867232 0.40263742 0.38780209 0.34675783 0.2930283 0.23591456 0.1788758 0.12236701 0.075837635 0.048155762][0.09185984 0.142108 0.20416079 0.25904545 0.29398689 0.30841994 0.30538642 0.27927345 0.23496822 0.18551524 0.13888086 0.096760914 0.059430543 0.035113148 0.030371472][0.072138779 0.10709839 0.14950904 0.18454313 0.20154805 0.20153597 0.1894591 0.16311753 0.12644099 0.088911094 0.056151684 0.028853917 0.0080999229 0.0014001389 0.013735279][0.04995117 0.068770424 0.0899012 0.10451649 0.10637421 0.098915569 0.087568246 0.071255893 0.050334431 0.028541073 0.0084467242 -0.0086320406 -0.019555287 -0.017023096 0.0028038274][0.02503632 0.031773731 0.038218345 0.040811911 0.036793366 0.030178396 0.025330149 0.021609923 0.015757961 0.0065888194 -0.0052923462 -0.017036933 -0.023991089 -0.01972786 -0.00096380909][0.0067591951 0.0075655072 0.00813097 0.0084064389 0.0064999191 0.0048741619 0.0066417763 0.012116397 0.01610291 0.014064529 0.0060342136 -0.0042318297 -0.011840552 -0.011777028 -0.00044672145]]...]
INFO - root - 2017-12-10 22:14:25.876117: step 58410, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 61h:55m:54s remains)
INFO - root - 2017-12-10 22:14:33.633385: step 58420, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 58h:06m:59s remains)
INFO - root - 2017-12-10 22:14:41.501023: step 58430, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 60h:25m:15s remains)
INFO - root - 2017-12-10 22:14:49.338244: step 58440, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 58h:58m:02s remains)
INFO - root - 2017-12-10 22:14:57.004202: step 58450, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 60h:24m:49s remains)
INFO - root - 2017-12-10 22:15:04.921623: step 58460, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 61h:01m:28s remains)
INFO - root - 2017-12-10 22:15:12.711538: step 58470, loss = 0.69, batch loss = 0.63 (11.0 examples/sec; 0.726 sec/batch; 55h:14m:24s remains)
INFO - root - 2017-12-10 22:15:20.325352: step 58480, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 59h:51m:48s remains)
INFO - root - 2017-12-10 22:15:28.223839: step 58490, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 58h:21m:23s remains)
INFO - root - 2017-12-10 22:15:36.041054: step 58500, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.748 sec/batch; 56h:56m:10s remains)
2017-12-10 22:15:36.970386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.088540964 -0.095476292 -0.098402016 -0.09867774 -0.096516371 -0.093192346 -0.090632282 -0.091086462 -0.094888322 -0.10170239 -0.10837591 -0.11168238 -0.11011282 -0.10461179 -0.097434759][-0.0965412 -0.10378922 -0.10415944 -0.098429449 -0.087571673 -0.074879535 -0.065134846 -0.063103795 -0.06965889 -0.084071532 -0.10084856 -0.1137725 -0.11826797 -0.11489831 -0.10708541][-0.094490208 -0.099233977 -0.093191184 -0.075137638 -0.047385383 -0.016784791 0.007223763 0.0157327 0.0065133427 -0.020547688 -0.056309994 -0.08972542 -0.11023869 -0.11587173 -0.11076383][-0.081530124 -0.079394236 -0.061563723 -0.023377335 0.030559689 0.088127114 0.13319297 0.15111008 0.13732289 0.091055825 0.026844053 -0.0381357 -0.084862754 -0.1067843 -0.10840691][-0.057784405 -0.043090738 -0.0072706835 0.058249149 0.14610098 0.23812874 0.30980244 0.33922771 0.31910202 0.24862303 0.14858139 0.042892259 -0.038624454 -0.083502337 -0.096208788][-0.027708115 0.0053133625 0.064722754 0.16188279 0.28569403 0.41216713 0.50868988 0.54624504 0.5154323 0.41678089 0.27830672 0.13047758 0.01304641 -0.055420544 -0.079233527][0.0012062378 0.055786107 0.14123818 0.26829639 0.42095491 0.57074016 0.68005389 0.71527994 0.66772336 0.54082441 0.36998469 0.19008617 0.046460483 -0.038065232 -0.0680315][0.02242917 0.099920213 0.21169475 0.36173102 0.52800483 0.68069023 0.78179383 0.79987347 0.73086613 0.58272207 0.39565119 0.20409681 0.051975053 -0.037139725 -0.067365326][0.03121924 0.13234363 0.26950055 0.43257582 0.59347355 0.72492319 0.794699 0.78030568 0.68690407 0.52902544 0.34660116 0.16776198 0.028147103 -0.051518984 -0.075093538][0.030917177 0.15259789 0.30909804 0.47300479 0.61143792 0.70308316 0.72647089 0.67210734 0.55658227 0.40103123 0.24093054 0.094818212 -0.014560761 -0.07261008 -0.083247393][0.0363386 0.17217579 0.33775517 0.490246 0.59365809 0.63440287 0.60502589 0.51084816 0.37932774 0.23644318 0.11055259 0.009430184 -0.058477741 -0.086084113 -0.078773133][0.050829098 0.1903479 0.35182965 0.48227835 0.54597956 0.53789091 0.46398407 0.34202918 0.20740961 0.087343022 0.0010974694 -0.053042017 -0.078209132 -0.073868871 -0.047870524][0.074345216 0.20540684 0.34991536 0.45215949 0.48085687 0.43921703 0.34171703 0.21230848 0.089097947 -0.00313649 -0.053624477 -0.070027635 -0.061323114 -0.030978739 0.010200623][0.10200577 0.21144417 0.3271766 0.39854038 0.40186885 0.34589443 0.24774384 0.13094594 0.030748341 -0.033329111 -0.056527574 -0.048209354 -0.017863572 0.029218683 0.079052649][0.12020609 0.1977942 0.27733687 0.31856069 0.30634302 0.251768 0.17034107 0.080852956 0.011433278 -0.025793398 -0.02967203 -0.0088669052 0.029850045 0.081024632 0.12949628]]...]
INFO - root - 2017-12-10 22:15:44.810815: step 58510, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 58h:28m:55s remains)
INFO - root - 2017-12-10 22:15:52.589922: step 58520, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.752 sec/batch; 57h:12m:34s remains)
INFO - root - 2017-12-10 22:16:00.407255: step 58530, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.744 sec/batch; 56h:38m:17s remains)
INFO - root - 2017-12-10 22:16:08.045120: step 58540, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.752 sec/batch; 57h:15m:40s remains)
INFO - root - 2017-12-10 22:16:15.727977: step 58550, loss = 0.68, batch loss = 0.62 (12.9 examples/sec; 0.619 sec/batch; 47h:04m:29s remains)
INFO - root - 2017-12-10 22:16:23.453690: step 58560, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 58h:22m:54s remains)
INFO - root - 2017-12-10 22:16:31.356001: step 58570, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 60h:09m:18s remains)
INFO - root - 2017-12-10 22:16:39.198856: step 58580, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 59h:35m:43s remains)
INFO - root - 2017-12-10 22:16:47.060029: step 58590, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.759 sec/batch; 57h:46m:06s remains)
INFO - root - 2017-12-10 22:16:54.908362: step 58600, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 59h:30m:57s remains)
2017-12-10 22:16:55.796021: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.051830523 0.046527315 0.037164614 0.033006433 0.041330382 0.058855966 0.078960605 0.095333867 0.10481022 0.10448021 0.091667943 0.073084414 0.053773236 0.033439621 0.009790902][0.10466799 0.1021968 0.093806863 0.095033489 0.11663263 0.15150739 0.18709266 0.21231821 0.22248475 0.2127661 0.18185587 0.14334801 0.10744881 0.0748778 0.039624248][0.15577155 0.15861525 0.15450637 0.16507873 0.20434795 0.26070532 0.31387892 0.34735596 0.35482016 0.33023769 0.27500281 0.21128446 0.15667164 0.11277757 0.068624921][0.17946802 0.19066766 0.19689959 0.22242878 0.28162575 0.35826954 0.42636046 0.46512404 0.46710718 0.42599946 0.34661171 0.25922796 0.18778907 0.13516887 0.085809842][0.18942209 0.20975691 0.22721037 0.26503718 0.33605212 0.42260045 0.49685892 0.53566611 0.53062928 0.47626835 0.38084063 0.27875137 0.19724801 0.14059377 0.09028437][0.19492529 0.22176762 0.24626018 0.28945228 0.36235878 0.4484106 0.52060938 0.55476564 0.54222268 0.47911757 0.37695596 0.27056667 0.1873755 0.13177481 0.084491476][0.19246696 0.22463588 0.25540069 0.30299205 0.37657413 0.46047226 0.528353 0.5567643 0.53791934 0.47029647 0.36712167 0.26089984 0.17780854 0.12162303 0.075261958][0.19299433 0.23346746 0.27547786 0.33230868 0.4090142 0.49014059 0.55170977 0.57253909 0.54690713 0.47637892 0.37435341 0.26995802 0.18644889 0.12634355 0.076284431][0.21092039 0.2636542 0.31852171 0.38301289 0.45815262 0.53019768 0.58017069 0.59052604 0.55777711 0.48528558 0.38591763 0.28562251 0.20384552 0.14067037 0.086262383][0.2431967 0.30711532 0.36949396 0.43340144 0.4997398 0.55916977 0.59691471 0.59832829 0.56143892 0.48985866 0.39496934 0.30026633 0.22216962 0.1578971 0.10009281][0.26675671 0.33704409 0.40172824 0.46171007 0.51936936 0.56954265 0.59947425 0.59554785 0.5567944 0.4872078 0.39681488 0.30768672 0.23352131 0.16932075 0.10963494][0.2574394 0.32594931 0.38869405 0.4460229 0.50025982 0.547303 0.573964 0.56655937 0.52581775 0.45752609 0.37164852 0.28940076 0.22124477 0.160802 0.10374205][0.22168753 0.28259987 0.33958003 0.39299166 0.44421542 0.48832026 0.51078707 0.49858302 0.45508766 0.38797653 0.30769613 0.2345379 0.17594109 0.12490814 0.077125][0.16535023 0.21422239 0.260898 0.30521265 0.34796163 0.38409722 0.39907992 0.38187855 0.33844769 0.27723354 0.20834181 0.14912941 0.10440271 0.067644753 0.034342486][0.099074364 0.13216795 0.16427821 0.19387266 0.22184917 0.24504417 0.25158864 0.2335532 0.19767506 0.15148593 0.10251424 0.062652178 0.034215868 0.012196633 -0.0069958805]]...]
INFO - root - 2017-12-10 22:17:03.616108: step 58610, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 59h:06m:49s remains)
INFO - root - 2017-12-10 22:17:11.247266: step 58620, loss = 0.69, batch loss = 0.63 (12.9 examples/sec; 0.621 sec/batch; 47h:14m:31s remains)
INFO - root - 2017-12-10 22:17:18.924716: step 58630, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.581 sec/batch; 44h:10m:41s remains)
INFO - root - 2017-12-10 22:17:26.852822: step 58640, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 60h:03m:04s remains)
INFO - root - 2017-12-10 22:17:34.765609: step 58650, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 58h:53m:59s remains)
INFO - root - 2017-12-10 22:17:42.648994: step 58660, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 58h:11m:35s remains)
INFO - root - 2017-12-10 22:17:50.538286: step 58670, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 59h:14m:21s remains)
INFO - root - 2017-12-10 22:17:58.393170: step 58680, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 60h:20m:43s remains)
INFO - root - 2017-12-10 22:18:06.193245: step 58690, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 61h:28m:35s remains)
INFO - root - 2017-12-10 22:18:14.101761: step 58700, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 59h:27m:26s remains)
2017-12-10 22:18:14.941574: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0042913933 0.0046569332 0.00085136609 0.006464106 0.025842851 0.05265452 0.076315269 0.091058351 0.086372793 0.054685935 0.0090595018 -0.032051757 -0.06112729 -0.080003165 -0.090121061][0.075899288 0.077951312 0.071899891 0.075612769 0.094688825 0.12270771 0.14587568 0.15830833 0.14932533 0.10992781 0.052221775 -0.003483783 -0.044738345 -0.0728951 -0.090571657][0.15593819 0.15300803 0.14104727 0.14282855 0.16403736 0.19741024 0.22537281 0.24010716 0.23147684 0.18911554 0.12272495 0.052454975 -0.0053843022 -0.049221024 -0.080685794][0.23316486 0.22004111 0.20207804 0.20623009 0.23692326 0.28473055 0.32694739 0.35152465 0.34715292 0.30437422 0.23009169 0.14299452 0.061387695 -0.0077126352 -0.060397469][0.30063727 0.28011814 0.26553059 0.28471321 0.33777165 0.41102326 0.47507608 0.51231533 0.5095557 0.46196204 0.37557912 0.2667391 0.15418261 0.051237408 -0.028958322][0.35177365 0.33267125 0.33373553 0.37924373 0.46205577 0.5626387 0.646295 0.69107425 0.68206519 0.621417 0.51682168 0.38388065 0.24024333 0.10444756 -0.00085545355][0.37277812 0.36796525 0.39530611 0.47163162 0.58109319 0.69868648 0.78863037 0.82950383 0.80672014 0.72925758 0.6072495 0.45667124 0.29330564 0.13719089 0.016556855][0.35281935 0.37010616 0.42561167 0.52519906 0.64588195 0.76008254 0.8371864 0.86145794 0.82221633 0.73303908 0.6056022 0.45490846 0.29318881 0.13841473 0.018434418][0.30551258 0.34061593 0.40905309 0.50897217 0.6163063 0.70557874 0.75548375 0.75805652 0.70796597 0.62010437 0.50511086 0.37410995 0.234878 0.10185649 -0.0011497574][0.25141832 0.28557625 0.34030136 0.41305175 0.485248 0.53766942 0.55787766 0.54346818 0.49279025 0.41887417 0.32944369 0.23078135 0.12693292 0.02885708 -0.045380816][0.20465465 0.21789874 0.23582605 0.26190472 0.28721 0.30109295 0.29742068 0.27536741 0.23391232 0.18204436 0.12421393 0.062800258 -0.00072997669 -0.059206393 -0.10049322][0.17421794 0.15869515 0.13404445 0.11220089 0.094385259 0.07837873 0.061882809 0.042187355 0.016013542 -0.012354796 -0.04085324 -0.069408774 -0.098430663 -0.1241224 -0.13914922][0.16639259 0.12567391 0.067665391 0.011692622 -0.033658624 -0.0650958 -0.083860114 -0.095442921 -0.10638078 -0.11655983 -0.12548956 -0.13369556 -0.14210826 -0.14900896 -0.15016499][0.17498904 0.11990123 0.045948867 -0.023454843 -0.077643424 -0.11209097 -0.12855399 -0.13380595 -0.13570541 -0.13689815 -0.13819735 -0.1400452 -0.14257641 -0.14467502 -0.14330506][0.18160093 0.12744993 0.057588469 -0.0052761654 -0.053448945 -0.0840557 -0.099238768 -0.10456868 -0.10605767 -0.10700089 -0.10917281 -0.11316916 -0.11843561 -0.12395094 -0.12687065]]...]
INFO - root - 2017-12-10 22:18:22.544681: step 58710, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.577 sec/batch; 43h:53m:26s remains)
INFO - root - 2017-12-10 22:18:30.418828: step 58720, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 59h:18m:02s remains)
INFO - root - 2017-12-10 22:18:38.263208: step 58730, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 59h:09m:00s remains)
INFO - root - 2017-12-10 22:18:46.067127: step 58740, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 57h:49m:21s remains)
INFO - root - 2017-12-10 22:18:53.919772: step 58750, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 59h:34m:36s remains)
INFO - root - 2017-12-10 22:19:01.723758: step 58760, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 61h:06m:39s remains)
INFO - root - 2017-12-10 22:19:09.547419: step 58770, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 59h:06m:28s remains)
INFO - root - 2017-12-10 22:19:17.456619: step 58780, loss = 0.67, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 58h:34m:10s remains)
INFO - root - 2017-12-10 22:19:25.366066: step 58790, loss = 0.68, batch loss = 0.63 (11.0 examples/sec; 0.730 sec/batch; 55h:30m:43s remains)
INFO - root - 2017-12-10 22:19:32.884523: step 58800, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 58h:30m:53s remains)
2017-12-10 22:19:33.765674: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1455065 0.13999785 0.12242975 0.11062071 0.10683165 0.1062869 0.10512653 0.10237017 0.11833112 0.1498726 0.17313626 0.17787376 0.16357626 0.14705248 0.11908821][0.1749478 0.17183883 0.1533626 0.13956507 0.13465485 0.13727377 0.14195131 0.14369114 0.15927787 0.18619584 0.20249702 0.19728772 0.1684259 0.13726783 0.10136103][0.19579466 0.19804277 0.18418519 0.17136221 0.16574548 0.17457442 0.19186421 0.20364481 0.21561684 0.22953941 0.23225395 0.21260324 0.16446613 0.11180764 0.063687183][0.23587422 0.24399216 0.23622125 0.22318648 0.21384898 0.22961445 0.26438156 0.28971237 0.29524145 0.2906647 0.27642995 0.23969691 0.16936249 0.091135778 0.026418863][0.32542342 0.34201753 0.33937514 0.32316247 0.30566812 0.32559493 0.3786611 0.42053157 0.42075828 0.39494658 0.35982522 0.30311066 0.20879823 0.10145646 0.013178742][0.44238502 0.46477634 0.46396762 0.44060022 0.41046804 0.42799097 0.49468493 0.55475366 0.55547619 0.51230782 0.45649648 0.38103196 0.26745141 0.13462754 0.02229416][0.54207605 0.56842089 0.56758106 0.53628534 0.49262935 0.50140077 0.57259625 0.64827836 0.65631789 0.60389447 0.53081405 0.4391073 0.31347743 0.16493241 0.035645556][0.5849629 0.61159253 0.610983 0.57407403 0.52006376 0.51723808 0.58281159 0.66595274 0.68522036 0.63554984 0.55615115 0.45669553 0.32939669 0.17920934 0.045517243][0.54346138 0.56805068 0.56667286 0.52726054 0.46893829 0.45518774 0.50577378 0.5837577 0.6137284 0.57973307 0.51082814 0.41852316 0.30383769 0.16940378 0.047066987][0.42949888 0.44906524 0.44614625 0.40820968 0.35267305 0.3320924 0.36346278 0.42556956 0.46165657 0.45066422 0.40711138 0.33761007 0.24860673 0.1430867 0.043246523][0.26799616 0.27972841 0.2746172 0.24193776 0.19594763 0.1755189 0.1910347 0.23362397 0.27134243 0.28471088 0.27432927 0.23740681 0.18128914 0.11021763 0.0366942][0.11456986 0.11676142 0.10923041 0.084221959 0.051983126 0.03831936 0.045490719 0.0713137 0.10705638 0.13872275 0.15717104 0.15030129 0.12310676 0.081152067 0.029145086][0.0065238634 -0.00074631791 -0.010511496 -0.027551606 -0.045925081 -0.050566562 -0.0455468 -0.030540517 0.0017951063 0.042979877 0.078736246 0.091402151 0.082575336 0.058400083 0.019483788][-0.055784933 -0.070255138 -0.081399165 -0.092227459 -0.10013689 -0.098154441 -0.093286812 -0.084360093 -0.056479458 -0.014727858 0.024941219 0.044114307 0.042308424 0.025828423 -0.005419943][-0.088362969 -0.10672618 -0.11755718 -0.12358577 -0.12469646 -0.11945651 -0.11526955 -0.11036996 -0.089005329 -0.054532282 -0.021803811 -0.0062344419 -0.0084650749 -0.022320721 -0.04679301]]...]
INFO - root - 2017-12-10 22:19:41.563626: step 58810, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 59h:03m:48s remains)
INFO - root - 2017-12-10 22:19:49.376700: step 58820, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 60h:25m:46s remains)
INFO - root - 2017-12-10 22:19:57.166517: step 58830, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 59h:35m:47s remains)
INFO - root - 2017-12-10 22:20:05.104333: step 58840, loss = 0.69, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 57h:22m:03s remains)
INFO - root - 2017-12-10 22:20:12.994357: step 58850, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 59h:37m:07s remains)
INFO - root - 2017-12-10 22:20:20.963762: step 58860, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 60h:53m:18s remains)
INFO - root - 2017-12-10 22:20:28.563643: step 58870, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.578 sec/batch; 43h:54m:26s remains)
INFO - root - 2017-12-10 22:20:36.410540: step 58880, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 59h:48m:36s remains)
INFO - root - 2017-12-10 22:20:44.111309: step 58890, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 59h:38m:03s remains)
INFO - root - 2017-12-10 22:20:52.072768: step 58900, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 60h:03m:58s remains)
2017-12-10 22:20:52.905019: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.089281619 -0.082411774 -0.074170835 -0.066566318 -0.059506547 -0.052902538 -0.047944359 -0.04509683 -0.048159465 -0.059902195 -0.0784179 -0.098328069 -0.11319546 -0.12228744 -0.12578461][-0.066781744 -0.049481165 -0.029921547 -0.010642595 0.0086921407 0.028022731 0.044195134 0.053376537 0.050178889 0.030662537 -0.0017511202 -0.039281454 -0.071057834 -0.09400893 -0.1084741][-0.030997125 0.00074763875 0.036099304 0.072090387 0.10970291 0.14801437 0.18020415 0.19783063 0.19449303 0.16509572 0.11442962 0.054174662 0.00083494192 -0.039642531 -0.06832847][0.012018509 0.06111851 0.11558885 0.17133999 0.22955652 0.28759429 0.3344878 0.35739261 0.3501485 0.30726773 0.23540576 0.15168968 0.077957593 0.022394517 -0.019018006][0.053260736 0.12030715 0.19462147 0.27009261 0.34695438 0.4199515 0.4742074 0.49383602 0.475068 0.41419119 0.32148421 0.2189865 0.13124101 0.066129982 0.016335893][0.083875991 0.16748369 0.26109773 0.35587636 0.44887227 0.53057826 0.58183646 0.58536893 0.54400337 0.46033242 0.35066834 0.23910989 0.14761931 0.080632105 0.027994767][0.10208199 0.19958505 0.31269491 0.42928296 0.54055345 0.62978595 0.671275 0.64647174 0.56876934 0.45415431 0.32868227 0.21651256 0.13146412 0.071314611 0.0226606][0.11137582 0.21902663 0.34921676 0.48660403 0.61531866 0.70995706 0.73774111 0.67924434 0.55966651 0.41219953 0.27390945 0.16774623 0.096791863 0.049721621 0.009218445][0.11837692 0.23369972 0.377654 0.53130388 0.67161638 0.76567471 0.77568853 0.68387687 0.52625394 0.35195214 0.20645793 0.10991609 0.055257875 0.022078339 -0.0096160742][0.12943818 0.25195983 0.40775752 0.57391334 0.72038347 0.80866396 0.799239 0.67865241 0.49121833 0.29831538 0.15075395 0.064982072 0.025363831 0.0039265137 -0.020434739][0.14245756 0.26929459 0.43179575 0.6038422 0.75031644 0.8294059 0.80294496 0.66298658 0.45915321 0.25961608 0.11681072 0.043893375 0.018590363 0.0071620336 -0.012322418][0.15100832 0.27586189 0.43497497 0.60120231 0.7381857 0.80462456 0.76697475 0.622402 0.42066598 0.22886789 0.097718969 0.038169574 0.024804948 0.020798448 0.0037628023][0.14856215 0.26347932 0.40620449 0.5506776 0.66397411 0.71087468 0.66643471 0.53251284 0.35329673 0.18683816 0.077338181 0.033160266 0.029212037 0.029304612 0.012337449][0.12534541 0.22184892 0.33714131 0.44775921 0.52767634 0.55153614 0.50484741 0.3930054 0.25071672 0.12226467 0.041791894 0.014391595 0.017256729 0.018610978 0.0012309419][0.079667374 0.15140978 0.23363952 0.3065145 0.35228378 0.35600773 0.31248724 0.22859149 0.1287789 0.042691112 -0.006982693 -0.018989176 -0.012728562 -0.012461747 -0.029626513]]...]
INFO - root - 2017-12-10 22:21:00.752200: step 58910, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 59h:20m:16s remains)
INFO - root - 2017-12-10 22:21:08.573707: step 58920, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 58h:51m:03s remains)
INFO - root - 2017-12-10 22:21:16.426729: step 58930, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 60h:45m:30s remains)
INFO - root - 2017-12-10 22:21:24.376342: step 58940, loss = 0.67, batch loss = 0.61 (10.2 examples/sec; 0.782 sec/batch; 59h:23m:22s remains)
INFO - root - 2017-12-10 22:21:31.929695: step 58950, loss = 0.70, batch loss = 0.64 (11.3 examples/sec; 0.708 sec/batch; 53h:45m:57s remains)
INFO - root - 2017-12-10 22:21:39.697259: step 58960, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 61h:05m:19s remains)
INFO - root - 2017-12-10 22:21:47.489896: step 58970, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.826 sec/batch; 62h:44m:24s remains)
INFO - root - 2017-12-10 22:21:55.053569: step 58980, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 58h:57m:45s remains)
INFO - root - 2017-12-10 22:22:02.873887: step 58990, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 59h:26m:27s remains)
INFO - root - 2017-12-10 22:22:10.795691: step 59000, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 60h:30m:43s remains)
2017-12-10 22:22:11.816063: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19556822 0.1940795 0.22791633 0.27480695 0.30953786 0.31953427 0.30961838 0.29489198 0.29437056 0.3128601 0.33211789 0.33529469 0.31567132 0.27796802 0.22651674][0.18386078 0.20003876 0.2478521 0.30210641 0.33677959 0.34260488 0.328431 0.31340259 0.31875429 0.34624112 0.37191439 0.37553731 0.35155359 0.30775106 0.25013566][0.14741169 0.18185788 0.24039426 0.29783618 0.33085358 0.33404383 0.31917146 0.30718049 0.31988034 0.3563796 0.38889766 0.39524227 0.37070313 0.32501492 0.2668511][0.10319114 0.14848781 0.208631 0.26209965 0.29108071 0.29383907 0.28244498 0.27684149 0.29696158 0.34086111 0.37993568 0.39177445 0.37209791 0.3314499 0.27993867][0.079465382 0.12689434 0.17988335 0.22218816 0.24309805 0.24369237 0.23446363 0.23251222 0.25546122 0.30192053 0.34560588 0.365058 0.35576883 0.32772678 0.28983375][0.085191622 0.13238356 0.17802043 0.21007761 0.22338535 0.2202227 0.20841672 0.2026343 0.21992315 0.26134574 0.30528936 0.33190414 0.33505616 0.32122034 0.296602][0.11827425 0.16500404 0.20748875 0.23569874 0.24537487 0.23770311 0.21850647 0.2024723 0.20814742 0.24011064 0.28116137 0.31217843 0.32315817 0.31541309 0.29481402][0.1716547 0.21638575 0.25996852 0.2905575 0.30008608 0.28757977 0.25957763 0.2323859 0.22659723 0.24900794 0.284335 0.3127878 0.32057914 0.30647671 0.27958161][0.22993602 0.27099326 0.31532493 0.34813175 0.35632455 0.33819154 0.30363175 0.27017757 0.2583603 0.27483517 0.30445743 0.32710481 0.32750794 0.30451667 0.269732][0.26677144 0.30469915 0.34682703 0.37756217 0.38166517 0.359566 0.32497072 0.29359138 0.28268984 0.29745924 0.32415283 0.34395131 0.34162977 0.31605256 0.27761948][0.28008002 0.3174676 0.35433334 0.37712088 0.37286136 0.34694454 0.31584224 0.29044026 0.28271919 0.29716656 0.32393527 0.3463417 0.34873012 0.32806793 0.29046929][0.28575945 0.32328364 0.35245693 0.36250082 0.34546608 0.31255841 0.2818844 0.25951952 0.25237459 0.26524416 0.29312164 0.32080173 0.3311075 0.31856674 0.28489551][0.27767912 0.31759661 0.34205556 0.34186035 0.31351098 0.27270406 0.23872794 0.21526429 0.20601527 0.21588635 0.24371523 0.27580217 0.29495382 0.29446951 0.2721718][0.24950238 0.29085994 0.31269598 0.30713955 0.27389175 0.23104142 0.19766968 0.17601027 0.16693875 0.17456274 0.20071116 0.23544598 0.26479104 0.28191712 0.27958298][0.19941451 0.23417456 0.25044852 0.24261832 0.21342482 0.1811052 0.16127987 0.1525363 0.15163095 0.16074866 0.18396996 0.21717943 0.25274259 0.28487965 0.30160531]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 22:22:19.684322: step 59010, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 61h:19m:49s remains)
INFO - root - 2017-12-10 22:22:27.616495: step 59020, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 60h:54m:49s remains)
INFO - root - 2017-12-10 22:22:35.248824: step 59030, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.773 sec/batch; 58h:43m:15s remains)
INFO - root - 2017-12-10 22:22:43.101541: step 59040, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 58h:54m:05s remains)
INFO - root - 2017-12-10 22:22:50.945486: step 59050, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 60h:00m:36s remains)
INFO - root - 2017-12-10 22:22:58.803439: step 59060, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 59h:31m:23s remains)
INFO - root - 2017-12-10 22:23:06.460429: step 59070, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 61h:47m:13s remains)
INFO - root - 2017-12-10 22:23:14.323565: step 59080, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 61h:08m:51s remains)
INFO - root - 2017-12-10 22:23:22.315661: step 59090, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.773 sec/batch; 58h:41m:19s remains)
INFO - root - 2017-12-10 22:23:30.250663: step 59100, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 60h:45m:26s remains)
2017-12-10 22:23:31.106035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.08234071 -0.077168234 -0.067814976 -0.056758493 -0.047611386 -0.044396427 -0.048401035 -0.057999384 -0.07051938 -0.082900479 -0.092652455 -0.098177627 -0.099590667 -0.097662531 -0.093042009][-0.0781876 -0.065426871 -0.044884838 -0.021138409 -0.00026967432 0.011803021 0.01250777 0.0025369583 -0.016874826 -0.041814964 -0.06660749 -0.085854478 -0.097238913 -0.10149578 -0.09873268][-0.057525925 -0.0381724 -0.0054559852 0.03439362 0.072854146 0.10153179 0.11423877 0.10794336 0.080244556 0.036847312 -0.010742342 -0.051172212 -0.078636751 -0.094243117 -0.0970109][-0.011163121 0.013661719 0.058621958 0.11741032 0.17912714 0.23147325 0.26131856 0.25889668 0.21787046 0.14821513 0.070216507 0.0029056245 -0.0442986 -0.074066505 -0.084883593][0.057830982 0.086908512 0.14221473 0.21955672 0.30594045 0.38362917 0.43023723 0.42821884 0.36918277 0.26963323 0.15944467 0.065047629 -0.0010750504 -0.043536525 -0.061434984][0.13329943 0.16460721 0.22456099 0.31340787 0.41691086 0.5120393 0.56816888 0.56211221 0.4864049 0.36384293 0.23080245 0.11806276 0.039860688 -0.0094060218 -0.03041498][0.20080633 0.23021449 0.28562042 0.37255892 0.47762045 0.57574695 0.63210344 0.62134159 0.53825516 0.40828183 0.26857978 0.15019968 0.06864287 0.019569665 0.00035757449][0.23677723 0.26046968 0.30371365 0.37658668 0.46832624 0.5560801 0.60613251 0.59402376 0.51640981 0.39632156 0.26641825 0.15523995 0.079704218 0.038075052 0.025532564][0.2338105 0.24804369 0.27504814 0.32725033 0.39729664 0.46754903 0.50966179 0.50214207 0.44212404 0.34620264 0.23904124 0.14512867 0.082646333 0.052983586 0.049518388][0.20153904 0.20352775 0.21321647 0.24411088 0.29234028 0.34619778 0.38416159 0.38800874 0.35366151 0.28966829 0.21178226 0.13981503 0.092291109 0.073533721 0.07680653][0.16227205 0.15220968 0.14697114 0.16052391 0.19175053 0.23375197 0.27107662 0.28856656 0.28064889 0.24919133 0.20215987 0.15431842 0.12256385 0.11279306 0.11956067][0.13604406 0.1186755 0.10564321 0.11146107 0.13553178 0.17273143 0.21151148 0.2394361 0.25060079 0.24370396 0.22216511 0.19642811 0.17954835 0.17633659 0.18271075][0.13159055 0.11374731 0.10144181 0.1093036 0.13490741 0.17202067 0.21131669 0.24302016 0.26228321 0.26794547 0.26198962 0.25135848 0.24391788 0.24202904 0.24328865][0.1530948 0.13911341 0.1299108 0.14130063 0.16846533 0.20336589 0.2382707 0.26622939 0.28415883 0.29229787 0.29243025 0.28854805 0.28357717 0.2778151 0.27111545][0.18834357 0.17732568 0.16928554 0.18095942 0.20589201 0.23485997 0.26146874 0.28153828 0.29365364 0.29895565 0.2987577 0.294456 0.28561929 0.27193683 0.25603026]]...]
INFO - root - 2017-12-10 22:23:38.680258: step 59110, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 58h:22m:38s remains)
INFO - root - 2017-12-10 22:23:46.672316: step 59120, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.823 sec/batch; 62h:27m:54s remains)
INFO - root - 2017-12-10 22:23:54.566399: step 59130, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 60h:10m:33s remains)
INFO - root - 2017-12-10 22:24:02.429447: step 59140, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 59h:06m:02s remains)
INFO - root - 2017-12-10 22:24:10.347119: step 59150, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.814 sec/batch; 61h:48m:29s remains)
INFO - root - 2017-12-10 22:24:18.189456: step 59160, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.786 sec/batch; 59h:42m:37s remains)
INFO - root - 2017-12-10 22:24:26.261548: step 59170, loss = 0.69, batch loss = 0.63 (8.9 examples/sec; 0.900 sec/batch; 68h:21m:51s remains)
INFO - root - 2017-12-10 22:24:34.211203: step 59180, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 60h:00m:50s remains)
INFO - root - 2017-12-10 22:24:41.927194: step 59190, loss = 0.70, batch loss = 0.65 (13.1 examples/sec; 0.609 sec/batch; 46h:15m:30s remains)
INFO - root - 2017-12-10 22:24:49.845762: step 59200, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 61h:06m:52s remains)
2017-12-10 22:24:50.752058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0045817606 0.00072695548 0.016657304 0.036708206 0.054215185 0.064342268 0.063514 0.052343141 0.037897371 0.025301399 0.015342219 0.010690127 0.01803901 0.035333451 0.054509189][0.015573113 0.019464539 0.037893832 0.063998163 0.089848347 0.10852858 0.11376301 0.1043427 0.0873856 0.069027238 0.050909936 0.038123216 0.040710576 0.058356274 0.082093082][0.035081845 0.035821155 0.054473963 0.084940568 0.1191294 0.14869885 0.16404094 0.16054997 0.14338471 0.11884178 0.089952305 0.065209962 0.058635063 0.07246802 0.098157778][0.047686607 0.043591578 0.059844807 0.092325613 0.13397592 0.17578141 0.20464382 0.21114056 0.19646366 0.1663589 0.12567443 0.086534552 0.06724336 0.073817968 0.098989926][0.055068515 0.047026567 0.059901956 0.0923484 0.13922025 0.19174205 0.23426491 0.25222522 0.24172559 0.20803632 0.15781617 0.10572103 0.07296019 0.0698448 0.091416277][0.059206035 0.049873445 0.060508512 0.092501625 0.14261444 0.20299894 0.25673726 0.28469124 0.27783993 0.24180934 0.18518542 0.12351418 0.079029322 0.065458395 0.080743253][0.061350971 0.053134486 0.063563734 0.095906533 0.14774004 0.21221232 0.27246815 0.30611184 0.30036098 0.2621125 0.20212486 0.13563994 0.083876617 0.061789442 0.06968151][0.063924126 0.057031814 0.067539662 0.099794224 0.15109284 0.21490093 0.27525875 0.30859932 0.30079591 0.2607075 0.20079425 0.13505191 0.0821367 0.056494676 0.059257712][0.071686089 0.064744487 0.073763266 0.10411434 0.15213372 0.21088478 0.26594821 0.29475951 0.28405032 0.24479522 0.18984352 0.13057648 0.081965581 0.05689393 0.056534231][0.082583532 0.073954515 0.079791889 0.10645983 0.14934832 0.20072176 0.24772386 0.27048498 0.25825447 0.22329085 0.17725782 0.12821737 0.086952746 0.064141542 0.060888551][0.087879628 0.078143664 0.081538126 0.10492984 0.14304073 0.18736982 0.22611558 0.24311379 0.23155034 0.20383164 0.16910115 0.13176863 0.099257387 0.079883195 0.074017711][0.083184622 0.076034978 0.080285482 0.10218243 0.13631992 0.17436458 0.2054871 0.21796097 0.20957266 0.19213814 0.17134458 0.147552 0.12537262 0.11001653 0.10102675][0.076166175 0.075696111 0.084526 0.10699505 0.13781275 0.16989182 0.19395731 0.20292915 0.19903278 0.19326682 0.18770017 0.17818299 0.16663942 0.15453663 0.14097612][0.069919989 0.079368733 0.096691914 0.12272925 0.1519704 0.17890017 0.19637409 0.20224318 0.20272999 0.20794109 0.21639183 0.21959922 0.21716325 0.20664518 0.18683426][0.063239664 0.083751433 0.11112881 0.14206876 0.17017032 0.19119142 0.20076536 0.20198765 0.20486769 0.21892969 0.23954435 0.25432634 0.26040933 0.25169569 0.22631611]]...]
INFO - root - 2017-12-10 22:24:58.648943: step 59210, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 59h:45m:24s remains)
INFO - root - 2017-12-10 22:25:06.482648: step 59220, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 60h:18m:49s remains)
INFO - root - 2017-12-10 22:25:14.442367: step 59230, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 59h:05m:24s remains)
INFO - root - 2017-12-10 22:25:22.430538: step 59240, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 59h:40m:15s remains)
INFO - root - 2017-12-10 22:25:30.238183: step 59250, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.787 sec/batch; 59h:46m:05s remains)
INFO - root - 2017-12-10 22:25:38.215539: step 59260, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 58h:34m:20s remains)
INFO - root - 2017-12-10 22:25:45.904096: step 59270, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 58h:26m:52s remains)
INFO - root - 2017-12-10 22:25:53.758526: step 59280, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 58h:37m:24s remains)
INFO - root - 2017-12-10 22:26:01.594871: step 59290, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 59h:05m:03s remains)
INFO - root - 2017-12-10 22:26:09.414075: step 59300, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 57h:34m:06s remains)
2017-12-10 22:26:10.277949: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14958134 0.21750337 0.24743569 0.22777627 0.16482598 0.08183337 0.0052106041 -0.035936333 -0.026351362 0.019801613 0.071604483 0.10485409 0.11446003 0.097425349 0.055542041][0.14743067 0.21786681 0.24912235 0.23197465 0.17381376 0.099930257 0.036984615 0.014779605 0.048563756 0.11973772 0.19131596 0.23407182 0.24158818 0.21028017 0.14245957][0.13057557 0.20070378 0.23738793 0.2335289 0.19621365 0.14956899 0.11721295 0.12528932 0.1867363 0.27890253 0.36031374 0.39853689 0.38779134 0.32776296 0.22527519][0.10729696 0.17996696 0.23081733 0.25338742 0.25295335 0.24882007 0.25895748 0.30100524 0.38371164 0.48139352 0.55212659 0.5638904 0.5137763 0.41220361 0.274145][0.0906012 0.17147119 0.24527149 0.30343038 0.34834144 0.39173993 0.44445729 0.51192218 0.59861034 0.67932862 0.71626347 0.68243021 0.58191395 0.43893096 0.27649388][0.091740929 0.18669614 0.28820485 0.38416943 0.4722091 0.5548836 0.63562959 0.70706451 0.77137637 0.80967408 0.79468793 0.70954055 0.56647468 0.40035629 0.23722355][0.12099149 0.22963132 0.3552548 0.48043221 0.59666491 0.69803214 0.78254515 0.83311021 0.8527444 0.83340043 0.76264471 0.63620228 0.47281492 0.31157771 0.17466763][0.16880536 0.28376085 0.41948867 0.55416936 0.6730966 0.76555473 0.82807988 0.83997649 0.80818117 0.73631275 0.62621284 0.48313144 0.32794526 0.19747174 0.10538983][0.2120153 0.32136244 0.44930044 0.57154387 0.66851246 0.72908741 0.75186586 0.71943641 0.64449334 0.54063588 0.41891944 0.28814167 0.16685477 0.084879614 0.046003006][0.22716975 0.31911141 0.42378381 0.51689637 0.57678884 0.59581363 0.57678515 0.50990134 0.41211262 0.30301332 0.19610305 0.098951273 0.02601503 -0.0022673798 0.009687447][0.19397138 0.262294 0.33818647 0.3988055 0.42279178 0.40722254 0.35930365 0.27722639 0.1794402 0.086970508 0.011632302 -0.0424958 -0.066793859 -0.048754744 0.0029017031][0.12017363 0.16570817 0.21689139 0.2526677 0.25370243 0.22086161 0.1641532 0.0872533 0.0073134452 -0.056855481 -0.096750088 -0.11197633 -0.099244453 -0.051018421 0.023601197][0.032880969 0.060447559 0.094919272 0.11683893 0.10968124 0.076515622 0.028208362 -0.029419111 -0.082393616 -0.1158978 -0.12447606 -0.11086394 -0.076571569 -0.01714755 0.061069019][-0.037110973 -0.021555232 0.003937046 0.021045456 0.016681926 -0.0055022538 -0.035902634 -0.069183841 -0.094952658 -0.10298995 -0.090340495 -0.061399568 -0.02091785 0.033587735 0.099659488][-0.071279004 -0.062470041 -0.04192416 -0.0256535 -0.022751195 -0.029858235 -0.040154673 -0.050394922 -0.053695489 -0.044658985 -0.022689342 0.0080747269 0.041936915 0.080296583 0.12392456]]...]
INFO - root - 2017-12-10 22:26:18.088469: step 59310, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.796 sec/batch; 60h:24m:23s remains)
INFO - root - 2017-12-10 22:26:25.925250: step 59320, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 59h:31m:26s remains)
INFO - root - 2017-12-10 22:26:33.679800: step 59330, loss = 0.71, batch loss = 0.65 (11.5 examples/sec; 0.694 sec/batch; 52h:40m:21s remains)
INFO - root - 2017-12-10 22:26:41.588358: step 59340, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 60h:25m:51s remains)
INFO - root - 2017-12-10 22:26:49.250436: step 59350, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 58h:33m:46s remains)
INFO - root - 2017-12-10 22:26:57.139151: step 59360, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 57h:39m:33s remains)
INFO - root - 2017-12-10 22:27:04.951259: step 59370, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.800 sec/batch; 60h:42m:40s remains)
INFO - root - 2017-12-10 22:27:12.800430: step 59380, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.775 sec/batch; 58h:48m:14s remains)
INFO - root - 2017-12-10 22:27:20.612547: step 59390, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 61h:10m:03s remains)
INFO - root - 2017-12-10 22:27:28.505118: step 59400, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 58h:26m:51s remains)
2017-12-10 22:27:29.342701: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20188518 0.22471251 0.22780183 0.2235558 0.22210957 0.22446944 0.22702107 0.22473751 0.2133455 0.19489586 0.17373182 0.15490507 0.13768165 0.11641094 0.084576026][0.26295656 0.28929675 0.29112148 0.28516167 0.28613794 0.29684481 0.31219885 0.32290414 0.31823194 0.29626125 0.26438376 0.23638308 0.21680674 0.19537495 0.15747876][0.28731775 0.31464565 0.31778315 0.31624544 0.32778668 0.35607293 0.39247227 0.42039558 0.42135182 0.39086738 0.34258461 0.30153784 0.27866384 0.25881746 0.21865062][0.27902889 0.30441886 0.30991867 0.31808102 0.34894991 0.40457606 0.46797702 0.5118283 0.511851 0.46617678 0.39776009 0.34210378 0.31408915 0.29454309 0.25323254][0.2420167 0.26347712 0.27143896 0.2920672 0.34701455 0.43369371 0.52202016 0.57249963 0.56032073 0.49404615 0.40925324 0.34767482 0.32139656 0.30499354 0.26357585][0.19411492 0.21518159 0.22954841 0.265046 0.34233049 0.45231277 0.552544 0.595697 0.56251341 0.47642672 0.38499811 0.32983863 0.31336293 0.30167764 0.25816628][0.18426619 0.21315145 0.23882726 0.28851745 0.3800686 0.49843103 0.59403908 0.61786181 0.55953419 0.4565981 0.36520094 0.32160285 0.31544879 0.30472633 0.25413758][0.21833102 0.25733787 0.29430631 0.35476881 0.45119569 0.56302691 0.63931519 0.63542908 0.55233061 0.43813154 0.35174927 0.31912437 0.31786588 0.30270764 0.24331242][0.26856658 0.31442609 0.35900655 0.42452222 0.51527697 0.6055283 0.64916056 0.61317849 0.51204896 0.39888221 0.32601693 0.30416209 0.30245855 0.2798053 0.21477221][0.30744949 0.3557224 0.4025003 0.46613419 0.54214722 0.60150987 0.60788941 0.54439443 0.43722188 0.33714479 0.2824651 0.26900977 0.26342824 0.23408577 0.1696925][0.31494123 0.35873866 0.40068239 0.45572993 0.51252735 0.54138148 0.51748466 0.43890312 0.33799806 0.2572529 0.21933533 0.21046837 0.19973882 0.16658179 0.10841675][0.27280587 0.304719 0.33564129 0.37771285 0.41546106 0.42135549 0.38066354 0.30073497 0.21345115 0.15116878 0.12569164 0.12017959 0.1084078 0.078357689 0.032802109][0.17252909 0.18793996 0.20475473 0.23125209 0.25121951 0.24263979 0.19897056 0.13064696 0.063220821 0.018717796 0.0033263131 0.0023747599 -0.0044358256 -0.023818688 -0.051038653][0.054170463 0.055632204 0.060941931 0.073768936 0.08019077 0.0652717 0.027964588 -0.022190517 -0.068044238 -0.09625458 -0.10323843 -0.099275604 -0.098368272 -0.10440637 -0.1133647][-0.040469926 -0.046941329 -0.048171334 -0.044038508 -0.04462349 -0.058752138 -0.085089087 -0.11667757 -0.14351586 -0.1584408 -0.1593412 -0.15254948 -0.14653201 -0.14329386 -0.14028881]]...]
INFO - root - 2017-12-10 22:27:37.106878: step 59410, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 61h:31m:31s remains)
INFO - root - 2017-12-10 22:27:44.750865: step 59420, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 61h:20m:05s remains)
INFO - root - 2017-12-10 22:27:52.422302: step 59430, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 57h:40m:44s remains)
INFO - root - 2017-12-10 22:28:00.306047: step 59440, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 58h:27m:18s remains)
INFO - root - 2017-12-10 22:28:08.248880: step 59450, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 60h:59m:49s remains)
INFO - root - 2017-12-10 22:28:16.044536: step 59460, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.747 sec/batch; 56h:40m:50s remains)
INFO - root - 2017-12-10 22:28:23.929384: step 59470, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.812 sec/batch; 61h:34m:43s remains)
INFO - root - 2017-12-10 22:28:31.798565: step 59480, loss = 0.73, batch loss = 0.67 (10.1 examples/sec; 0.794 sec/batch; 60h:11m:16s remains)
INFO - root - 2017-12-10 22:28:39.777736: step 59490, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 59h:03m:54s remains)
INFO - root - 2017-12-10 22:28:47.762563: step 59500, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 60h:20m:06s remains)
2017-12-10 22:28:48.605179: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.091817982 0.090537183 0.084108546 0.086124025 0.095403 0.10197182 0.11101715 0.12216019 0.13413636 0.14785966 0.15945035 0.17442636 0.19030367 0.19706944 0.18986161][0.13796173 0.1465262 0.14621392 0.15178941 0.16498107 0.17004652 0.17119572 0.16887869 0.16713065 0.1678068 0.16795065 0.17381696 0.1859929 0.1944592 0.18934418][0.20065244 0.21646011 0.21957257 0.22676666 0.24359368 0.24819869 0.2412827 0.22380377 0.20575914 0.19021566 0.17348763 0.16560106 0.1717252 0.18179114 0.17988443][0.26668885 0.27933571 0.27649012 0.27831635 0.29407793 0.29830348 0.28651822 0.2594007 0.22985628 0.20073785 0.16884767 0.14841495 0.14889717 0.16030389 0.16287087][0.3158668 0.31271383 0.29400468 0.28336224 0.29227078 0.29546702 0.28353876 0.25552177 0.22278388 0.18753311 0.14788997 0.12055558 0.11798167 0.13102502 0.13974112][0.329133 0.30297238 0.26554996 0.24250458 0.24551557 0.25154474 0.2470399 0.22822237 0.20126331 0.16755149 0.12678239 0.097200386 0.092948794 0.10585079 0.11877263][0.30952862 0.26047462 0.20828834 0.17824054 0.18086492 0.19561884 0.2056216 0.20330591 0.18812171 0.16057053 0.1216498 0.09047696 0.082949482 0.092601091 0.10618959][0.28551283 0.22141747 0.164734 0.1366647 0.14545703 0.17205924 0.1972717 0.20999123 0.20350878 0.17816265 0.13688529 0.099712923 0.085045531 0.08854153 0.10034375][0.29205045 0.22053044 0.16762766 0.1481612 0.16719776 0.20462778 0.23985031 0.25969085 0.25322405 0.22129038 0.16940235 0.11974856 0.093986951 0.089566253 0.098522209][0.32666007 0.25605464 0.21105053 0.20423539 0.23669651 0.28377381 0.32278657 0.3404153 0.32482961 0.27823657 0.20895647 0.14255756 0.10433151 0.092433311 0.098667227][0.37265077 0.30667356 0.2702297 0.27530015 0.31897041 0.37126863 0.40743324 0.4159742 0.38612434 0.32208857 0.23539063 0.15485096 0.1078662 0.091881506 0.097494736][0.38027176 0.32161596 0.29265895 0.30540365 0.35395795 0.40581813 0.43610907 0.43604761 0.39628163 0.32262051 0.22929318 0.14629398 0.099952579 0.085984483 0.094194107][0.31533521 0.26815823 0.24755187 0.26430994 0.3119857 0.36033791 0.38747326 0.38701698 0.35003364 0.282066 0.19823234 0.12660114 0.089618534 0.081387043 0.092625916][0.19522676 0.15839532 0.14375354 0.1596712 0.20142828 0.24524757 0.27473494 0.28481 0.26514935 0.2187255 0.15895222 0.10904645 0.086159185 0.083968125 0.096134715][0.054523166 0.027563658 0.01849116 0.032062881 0.066212445 0.1054823 0.14030899 0.16613556 0.17059317 0.15292512 0.12325943 0.098557346 0.090300366 0.092841096 0.10369828]]...]
INFO - root - 2017-12-10 22:28:56.177535: step 59510, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 59h:17m:12s remains)
INFO - root - 2017-12-10 22:29:04.252503: step 59520, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 60h:32m:28s remains)
INFO - root - 2017-12-10 22:29:12.041474: step 59530, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 57h:18m:31s remains)
INFO - root - 2017-12-10 22:29:19.952786: step 59540, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 60h:05m:48s remains)
INFO - root - 2017-12-10 22:29:27.960008: step 59550, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 58h:53m:22s remains)
INFO - root - 2017-12-10 22:29:35.818159: step 59560, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 59h:03m:24s remains)
INFO - root - 2017-12-10 22:29:43.814661: step 59570, loss = 0.71, batch loss = 0.65 (9.3 examples/sec; 0.861 sec/batch; 65h:14m:40s remains)
INFO - root - 2017-12-10 22:29:51.684129: step 59580, loss = 0.67, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 59h:01m:07s remains)
INFO - root - 2017-12-10 22:29:59.420629: step 59590, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.773 sec/batch; 58h:35m:46s remains)
INFO - root - 2017-12-10 22:30:07.205323: step 59600, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 61h:17m:11s remains)
2017-12-10 22:30:08.079445: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.031044956 0.033777039 0.03126777 0.026396956 0.025187805 0.030288022 0.035932247 0.03755711 0.036770243 0.03452269 0.025809612 0.0085547091 -0.01131613 -0.026562076 -0.033403665][0.080990948 0.089600667 0.088233024 0.081533961 0.080078728 0.088915944 0.0998371 0.10437196 0.10248201 0.097210944 0.0825884 0.054854646 0.02141859 -0.0066455971 -0.022378787][0.16010208 0.17828272 0.17886934 0.16754182 0.15983222 0.16510469 0.17594294 0.18081705 0.17754561 0.17019 0.15162912 0.11495291 0.067375794 0.02435763 -0.0028221209][0.25637734 0.28999829 0.29812717 0.28479183 0.26779097 0.26093486 0.26120222 0.2570453 0.24543424 0.23274417 0.21149665 0.16959602 0.11089259 0.054033723 0.015328026][0.33488441 0.38746995 0.41198972 0.40766117 0.38904819 0.37185878 0.35772541 0.33704963 0.30788895 0.28201112 0.25397038 0.20725654 0.14053082 0.073397383 0.025834642][0.37832302 0.44892517 0.49587429 0.51149875 0.50326955 0.48571518 0.4628917 0.42700168 0.37797517 0.33428577 0.29388097 0.23829538 0.16308646 0.087326832 0.032695033][0.37841439 0.4595643 0.52534771 0.56213492 0.5707255 0.56367409 0.54456532 0.504357 0.44349581 0.38551375 0.3313413 0.26338881 0.17826383 0.095466249 0.036620066][0.34550759 0.42624196 0.49857396 0.54591852 0.56653017 0.5729298 0.56587917 0.53314126 0.47366029 0.41173038 0.34896353 0.26960066 0.1756932 0.089562006 0.031557832][0.30064431 0.3709245 0.43551528 0.47912928 0.50165743 0.51634008 0.52078426 0.500594 0.45214432 0.39656168 0.33385721 0.24957858 0.15254432 0.068606995 0.016540123][0.24632588 0.29961884 0.34696248 0.37738717 0.39385915 0.4096891 0.42056912 0.41177958 0.37803921 0.33545214 0.28160042 0.20302114 0.11284957 0.038513385 -0.0037342303][0.18093961 0.21591702 0.24405798 0.25861689 0.26509878 0.27637219 0.28751081 0.2852264 0.26407239 0.23573917 0.19661985 0.13413732 0.061372764 0.0035190927 -0.026015066][0.10418514 0.12374976 0.13640961 0.13876791 0.1372132 0.14306083 0.1511116 0.15056074 0.13721293 0.11960866 0.095035188 0.053010989 0.0035326234 -0.033788502 -0.048956931][0.033488106 0.042046342 0.045258928 0.041479342 0.036247529 0.038413964 0.04329754 0.042766687 0.034269616 0.02397057 0.010892809 -0.012696702 -0.0405262 -0.059426434 -0.062691249][-0.011433078 -0.0093125384 -0.01056148 -0.0157753 -0.021059021 -0.020799076 -0.018139085 -0.018555779 -0.023265382 -0.028362226 -0.033812389 -0.04433883 -0.056597315 -0.063007332 -0.059888531][-0.030867863 -0.031670749 -0.034073874 -0.038322609 -0.042219363 -0.042871349 -0.041668251 -0.041724194 -0.043672286 -0.045403216 -0.046930369 -0.050049823 -0.053030007 -0.052777357 -0.047809381]]...]
INFO - root - 2017-12-10 22:30:15.982820: step 59610, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 59h:10m:19s remains)
INFO - root - 2017-12-10 22:30:23.928401: step 59620, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 59h:18m:09s remains)
INFO - root - 2017-12-10 22:30:31.777825: step 59630, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 59h:56m:57s remains)
INFO - root - 2017-12-10 22:30:39.684184: step 59640, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 59h:58m:52s remains)
INFO - root - 2017-12-10 22:30:47.535418: step 59650, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 57h:41m:05s remains)
INFO - root - 2017-12-10 22:30:55.456674: step 59660, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.823 sec/batch; 62h:20m:51s remains)
INFO - root - 2017-12-10 22:31:03.201180: step 59670, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 60h:12m:07s remains)
INFO - root - 2017-12-10 22:31:11.095308: step 59680, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 60h:04m:20s remains)
INFO - root - 2017-12-10 22:31:18.792611: step 59690, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 59h:45m:29s remains)
INFO - root - 2017-12-10 22:31:26.724367: step 59700, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 61h:13m:23s remains)
2017-12-10 22:31:27.644181: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17041725 0.15374796 0.13945431 0.13468498 0.13730961 0.14020281 0.13736522 0.12515129 0.10706311 0.093047462 0.090115651 0.0912179 0.089970544 0.088332444 0.08873491][0.21678704 0.20236225 0.18763053 0.18400814 0.18806064 0.1918319 0.18983097 0.17624305 0.15375334 0.13232468 0.12160682 0.11682157 0.11211165 0.10976908 0.11301166][0.24335189 0.23701139 0.22619051 0.22641298 0.2338234 0.23991606 0.23939218 0.22451741 0.197764 0.16871458 0.14941649 0.13781834 0.12889162 0.12506437 0.13040388][0.23914851 0.24163923 0.23561884 0.23954833 0.25128394 0.26244566 0.267449 0.25590524 0.2297717 0.19763745 0.17329571 0.1567923 0.14443944 0.13864222 0.14425252][0.20573175 0.21497358 0.2128163 0.21924829 0.23487116 0.2521131 0.2660524 0.26418471 0.24617854 0.21850947 0.19515394 0.17748997 0.16389756 0.15713052 0.16165408][0.16867329 0.18186906 0.18172088 0.18870324 0.20582736 0.22723533 0.24917305 0.25910231 0.2541247 0.2374884 0.22073424 0.20576011 0.19432676 0.18914326 0.1933841][0.15084237 0.16526544 0.16508748 0.17083272 0.18771407 0.21071202 0.23620765 0.25378233 0.25996372 0.25585005 0.24859178 0.23973121 0.23455 0.23476328 0.23988505][0.15551776 0.1676074 0.16469775 0.16733742 0.18260841 0.20542605 0.23054551 0.24964598 0.26134714 0.26681551 0.26931271 0.26916793 0.27438882 0.28346094 0.28982326][0.17418659 0.17861782 0.1683197 0.1651857 0.17730217 0.19962025 0.22374484 0.24226411 0.25588563 0.26736316 0.27806816 0.28709558 0.30362988 0.32201758 0.3294827][0.1950236 0.1892067 0.16958454 0.15925242 0.16758704 0.1896086 0.21310057 0.23034547 0.24352585 0.25715551 0.27229294 0.287438 0.31131664 0.33571002 0.34444124][0.20416941 0.19107844 0.16486162 0.14860564 0.1534328 0.17504033 0.19815381 0.21425229 0.22621432 0.23925452 0.2544505 0.27072498 0.29557166 0.32048038 0.32923993][0.19185291 0.17595927 0.14704387 0.12678155 0.12788349 0.14770687 0.17003828 0.18531528 0.19633824 0.20779237 0.22046575 0.23404233 0.25465173 0.27507016 0.28123704][0.15974098 0.14434601 0.1159703 0.093461737 0.089959368 0.10527253 0.12459884 0.13825502 0.14852196 0.15878621 0.16897206 0.17879377 0.19309501 0.20676506 0.20903574][0.11758047 0.10430209 0.079068661 0.056652609 0.048857637 0.057359464 0.070425086 0.080006227 0.087951131 0.096460655 0.10448097 0.11092061 0.11886056 0.12557778 0.12442698][0.066995412 0.05592972 0.0362289 0.017276673 0.007881 0.010100969 0.015724387 0.019461561 0.023037519 0.028082956 0.033028059 0.036388498 0.039529607 0.0415283 0.039134171]]...]
INFO - root - 2017-12-10 22:31:35.529825: step 59710, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 58h:50m:26s remains)
INFO - root - 2017-12-10 22:31:43.349081: step 59720, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 58h:48m:12s remains)
INFO - root - 2017-12-10 22:31:51.351236: step 59730, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.817 sec/batch; 61h:56m:12s remains)
INFO - root - 2017-12-10 22:31:59.222448: step 59740, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 58h:28m:08s remains)
INFO - root - 2017-12-10 22:32:06.930966: step 59750, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 59h:44m:36s remains)
INFO - root - 2017-12-10 22:32:14.750737: step 59760, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 60h:07m:51s remains)
INFO - root - 2017-12-10 22:32:22.604379: step 59770, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 61h:26m:08s remains)
INFO - root - 2017-12-10 22:32:30.222245: step 59780, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 60h:22m:14s remains)
INFO - root - 2017-12-10 22:32:38.167083: step 59790, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 61h:40m:04s remains)
INFO - root - 2017-12-10 22:32:46.038515: step 59800, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 59h:45m:35s remains)
2017-12-10 22:32:46.889298: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.038983285 0.055900939 0.062923409 0.059268422 0.049804989 0.037172075 0.032011341 0.0383472 0.049092211 0.0538403 0.04940407 0.034080058 0.006877149 -0.027065977 -0.056786392][0.11196794 0.14894161 0.17270651 0.18046138 0.17470247 0.15680483 0.14543806 0.14911588 0.15703331 0.15565167 0.14220528 0.11531321 0.072048604 0.01775101 -0.032225296][0.20162217 0.26316932 0.30889666 0.33311355 0.33245143 0.30551949 0.28022623 0.27178273 0.26736596 0.25339794 0.22814308 0.1906393 0.13495952 0.064458869 -0.003307705][0.27770227 0.36488125 0.43694541 0.48261741 0.48947415 0.45288104 0.40988991 0.38362324 0.36144134 0.33185092 0.29549006 0.2515167 0.18960914 0.10875375 0.027111894][0.32382661 0.4338274 0.53397965 0.60567212 0.626433 0.58948183 0.53885716 0.50106293 0.46252853 0.41604728 0.3679952 0.31771711 0.24968679 0.15908505 0.0646729][0.33798829 0.46200335 0.58302212 0.67686766 0.71500379 0.69125885 0.65209889 0.61843753 0.57327324 0.51396763 0.45524973 0.39781997 0.3208279 0.21897639 0.11186602][0.32353514 0.44746968 0.57304877 0.67559659 0.72786933 0.72639441 0.71398246 0.69919419 0.66026878 0.597792 0.53346545 0.47039258 0.38380483 0.27185747 0.15551639][0.28112596 0.39116347 0.50382394 0.599182 0.65757012 0.67858231 0.69648159 0.70793945 0.68799895 0.63655984 0.5769825 0.51393396 0.42144576 0.30346954 0.18332534][0.2105362 0.299086 0.39007378 0.46979475 0.52768207 0.56513214 0.60619557 0.64075035 0.64440811 0.61417383 0.56953263 0.514422 0.42336309 0.30624533 0.18888801][0.12744167 0.19305038 0.2608006 0.32220474 0.37377009 0.41717166 0.46729961 0.51263148 0.532992 0.52267849 0.49442226 0.45049319 0.3686626 0.26264229 0.15820643][0.046221834 0.08896289 0.13455075 0.1774243 0.21751988 0.25558543 0.30013421 0.34225288 0.36742303 0.36835745 0.35147771 0.31897932 0.25440481 0.17106807 0.090900213][-0.01851584 0.0049932254 0.03260126 0.059676126 0.086700968 0.11342412 0.14476414 0.1757894 0.19649623 0.19923465 0.18675563 0.16311142 0.11890531 0.063512534 0.012123814][-0.056713983 -0.04665447 -0.031310547 -0.015365044 0.00093204406 0.016663274 0.03472947 0.053357344 0.065825887 0.065416142 0.053777315 0.036565516 0.01026305 -0.02062659 -0.047791831][-0.071742252 -0.069956556 -0.062603809 -0.054248761 -0.045675807 -0.037920695 -0.029479809 -0.020779423 -0.016074311 -0.01975492 -0.030269878 -0.042269714 -0.05590792 -0.06973432 -0.080485627][-0.070664234 -0.07312949 -0.070774786 -0.067614712 -0.064359762 -0.061654177 -0.058919162 -0.056344815 -0.056388468 -0.061012853 -0.069103271 -0.0771846 -0.083860137 -0.088704109 -0.090953976]]...]
INFO - root - 2017-12-10 22:32:54.790647: step 59810, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 59h:21m:30s remains)
INFO - root - 2017-12-10 22:33:02.685718: step 59820, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 59h:02m:32s remains)
INFO - root - 2017-12-10 22:33:10.407643: step 59830, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 59h:01m:19s remains)
INFO - root - 2017-12-10 22:33:18.331140: step 59840, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 59h:45m:15s remains)
INFO - root - 2017-12-10 22:33:26.282348: step 59850, loss = 0.69, batch loss = 0.63 (9.5 examples/sec; 0.842 sec/batch; 63h:47m:19s remains)
INFO - root - 2017-12-10 22:33:34.055525: step 59860, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.765 sec/batch; 57h:55m:40s remains)
INFO - root - 2017-12-10 22:33:41.781906: step 59870, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 59h:24m:13s remains)
INFO - root - 2017-12-10 22:33:49.600463: step 59880, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 57h:34m:17s remains)
INFO - root - 2017-12-10 22:33:57.513854: step 59890, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 57h:53m:05s remains)
INFO - root - 2017-12-10 22:34:05.373765: step 59900, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 60h:05m:30s remains)
2017-12-10 22:34:06.178364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.070910111 -0.068723112 -0.06652052 -0.06676805 -0.069712736 -0.073468477 -0.075331032 -0.07507126 -0.074290581 -0.073500469 -0.073194966 -0.074849829 -0.077546664 -0.079490922 -0.078645378][-0.054091197 -0.044504363 -0.035526421 -0.030944904 -0.032006964 -0.035210021 -0.0355896 -0.033315897 -0.031152735 -0.028218344 -0.025386587 -0.026328426 -0.029564435 -0.032771461 -0.032417551][-0.025229787 -0.0026494255 0.019188661 0.034084365 0.039166428 0.039922889 0.044258066 0.051182311 0.055724572 0.060380124 0.064824387 0.064271368 0.060641244 0.056108233 0.056948714][0.015296071 0.060381267 0.10579938 0.13978748 0.15583494 0.1613224 0.16857956 0.17643559 0.17902359 0.18171765 0.18567768 0.18480663 0.18066473 0.17560667 0.17917196][0.057772875 0.13226329 0.21171615 0.27511382 0.30876282 0.31952766 0.32505739 0.32486922 0.31569844 0.30882949 0.30914581 0.30924281 0.30706304 0.30350995 0.31105816][0.089575134 0.19356084 0.31051484 0.40879267 0.46563774 0.483773 0.48559573 0.47048864 0.44069907 0.41644907 0.40888992 0.41076332 0.41274339 0.41254884 0.42359468][0.10518961 0.23142911 0.379346 0.50836486 0.58768117 0.61368239 0.6114912 0.57941967 0.52672637 0.4827891 0.46532479 0.46872133 0.47519127 0.47873184 0.49135715][0.10158304 0.23600136 0.39808494 0.54308861 0.63642716 0.66854835 0.66444921 0.62178212 0.55606776 0.50205868 0.48045474 0.48703042 0.49783078 0.50391304 0.5148837][0.076657951 0.20111552 0.35435194 0.49385688 0.58684427 0.62066054 0.61833817 0.57815921 0.51884568 0.47363934 0.46046966 0.4752554 0.49199954 0.49987233 0.50635529][0.033067279 0.1308537 0.25400665 0.36726868 0.44451514 0.47410715 0.47653091 0.45205063 0.41790789 0.39896369 0.40524307 0.43200088 0.45559415 0.46516865 0.46799222][-0.017290849 0.045773651 0.12744322 0.20215699 0.25322807 0.27362609 0.28083187 0.27744889 0.27593213 0.28909215 0.31742725 0.35505587 0.38452247 0.39718693 0.4020353][-0.058961809 -0.028710892 0.012072018 0.047233641 0.069239885 0.07713785 0.08495938 0.096883729 0.12033079 0.15738511 0.20136715 0.2454295 0.27961031 0.29881173 0.31434983][-0.085759893 -0.079608925 -0.069437653 -0.064344 -0.065400556 -0.069215491 -0.065671206 -0.050064035 -0.018452821 0.025353009 0.072212055 0.11599423 0.15328641 0.18271179 0.2163882][-0.098015726 -0.10447232 -0.11011238 -0.12023639 -0.13301553 -0.14379239 -0.14629924 -0.13663423 -0.11250743 -0.07860399 -0.042081572 -0.0064528286 0.029512156 0.067675211 0.11938389][-0.097627565 -0.10709714 -0.11618082 -0.12911154 -0.14371946 -0.15607142 -0.16303091 -0.16220844 -0.15195468 -0.13515356 -0.11541864 -0.093231142 -0.06438981 -0.0239983 0.038049717]]...]
INFO - root - 2017-12-10 22:34:13.901870: step 59910, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.816 sec/batch; 61h:46m:41s remains)
INFO - root - 2017-12-10 22:34:21.843185: step 59920, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.817 sec/batch; 61h:51m:47s remains)
INFO - root - 2017-12-10 22:34:29.767403: step 59930, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 60h:01m:39s remains)
INFO - root - 2017-12-10 22:34:37.731663: step 59940, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.810 sec/batch; 61h:19m:35s remains)
INFO - root - 2017-12-10 22:34:45.693397: step 59950, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.817 sec/batch; 61h:52m:48s remains)
INFO - root - 2017-12-10 22:34:53.356510: step 59960, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 60h:22m:33s remains)
INFO - root - 2017-12-10 22:35:01.240503: step 59970, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 61h:42m:02s remains)
INFO - root - 2017-12-10 22:35:09.157854: step 59980, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 57h:30m:12s remains)
INFO - root - 2017-12-10 22:35:16.810981: step 59990, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.752 sec/batch; 56h:57m:34s remains)
INFO - root - 2017-12-10 22:35:24.647571: step 60000, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 60h:03m:30s remains)
2017-12-10 22:35:25.471617: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.019672208 0.014508749 0.014486396 0.018418936 0.02284454 0.025437515 0.025516285 0.022291994 0.0178387 0.017036734 0.019771867 0.022316705 0.024639308 0.026863337 0.025191737][0.068482414 0.065445177 0.068051942 0.075511552 0.08248575 0.085867204 0.084378228 0.075792156 0.062705062 0.054727502 0.054081086 0.055346489 0.057640731 0.062294047 0.064025156][0.12474795 0.12766291 0.13760044 0.15314811 0.16601221 0.17214791 0.16936611 0.15337409 0.12711835 0.1062818 0.097343847 0.094161846 0.094491333 0.10086778 0.10678139][0.1729276 0.18498474 0.20765904 0.23738167 0.26082531 0.27245283 0.26964864 0.2459217 0.20416142 0.16620329 0.14401169 0.13205354 0.12672482 0.1317337 0.14006995][0.20719418 0.23133524 0.27116227 0.3196719 0.35640207 0.37361079 0.3700265 0.33845779 0.281176 0.22454172 0.18605025 0.16228208 0.14817986 0.14810029 0.15558949][0.23340271 0.2701928 0.32584378 0.39065242 0.43724224 0.45604175 0.44946694 0.41182503 0.34429476 0.27405402 0.22187506 0.18677044 0.16274983 0.15428704 0.15682261][0.25732237 0.30383277 0.36797744 0.4403713 0.49022952 0.50705105 0.49763629 0.45879751 0.39004245 0.3155475 0.25612488 0.21220912 0.1775904 0.15711579 0.14935562][0.28729942 0.3368105 0.39808688 0.4673551 0.51510721 0.53010976 0.52139151 0.48771518 0.42630002 0.35614252 0.29633296 0.24759035 0.20308536 0.16812916 0.14533049][0.33177686 0.37675136 0.42412826 0.48066348 0.521804 0.53515363 0.52896917 0.50314194 0.45284736 0.3915948 0.33715317 0.28950542 0.2398535 0.19337964 0.15625736][0.38603204 0.4219335 0.449839 0.48811111 0.51942986 0.53048915 0.52626336 0.50688857 0.46659303 0.41535592 0.37032497 0.3295694 0.28122064 0.22959779 0.1830868][0.42261219 0.44884822 0.45890257 0.47981694 0.50124717 0.50964433 0.5057292 0.48943317 0.45552468 0.41331372 0.3793284 0.34849218 0.30602351 0.2554267 0.20626448][0.41226438 0.42943949 0.42871577 0.43941489 0.45521304 0.46213329 0.4571251 0.44040024 0.40879914 0.37330055 0.34898764 0.32733017 0.29221037 0.24684343 0.20154384][0.34576237 0.35521403 0.35070595 0.35874397 0.37336415 0.37972149 0.37272626 0.35388392 0.32298678 0.29280242 0.27598408 0.2616874 0.23460972 0.19786897 0.16189851][0.23650654 0.23893754 0.23389576 0.24185945 0.25601053 0.26136738 0.25260407 0.23281254 0.20434852 0.18010563 0.1693708 0.1610276 0.14248364 0.1163435 0.092089295][0.11301664 0.10945842 0.104468 0.11147018 0.12346283 0.12707792 0.11796223 0.10017457 0.077168234 0.059768673 0.053806026 0.05016144 0.039899174 0.024444189 0.011155472]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 22:35:34.627007: step 60010, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 59h:42m:40s remains)
INFO - root - 2017-12-10 22:35:42.660401: step 60020, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 57h:34m:29s remains)
INFO - root - 2017-12-10 22:35:50.590766: step 60030, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 59h:19m:26s remains)
INFO - root - 2017-12-10 22:35:58.317853: step 60040, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 58h:17m:16s remains)
INFO - root - 2017-12-10 22:36:06.217436: step 60050, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 59h:19m:10s remains)
INFO - root - 2017-12-10 22:36:14.150089: step 60060, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 60h:45m:02s remains)
INFO - root - 2017-12-10 22:36:21.782571: step 60070, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 57h:26m:01s remains)
INFO - root - 2017-12-10 22:36:29.642431: step 60080, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.767 sec/batch; 58h:04m:28s remains)
INFO - root - 2017-12-10 22:36:37.442978: step 60090, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 59h:04m:28s remains)
INFO - root - 2017-12-10 22:36:45.256814: step 60100, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 59h:59m:27s remains)
2017-12-10 22:36:46.109170: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12527464 0.14212748 0.14848682 0.15210475 0.15275849 0.15520616 0.1593388 0.17058764 0.1820441 0.18145032 0.16442859 0.13182236 0.090485625 0.043209568 -0.0016814003][0.18937036 0.21675552 0.22716044 0.22988282 0.2264196 0.22351961 0.2222206 0.23046641 0.24071209 0.23832501 0.2164164 0.17564201 0.12458272 0.06597326 0.0099487007][0.2420402 0.27951166 0.29404977 0.2957657 0.28835016 0.28070724 0.27567941 0.28216761 0.29264846 0.29102027 0.26720014 0.22145417 0.16263014 0.0942534 0.028059965][0.27291355 0.31676966 0.33421004 0.33498088 0.3250387 0.31515035 0.31006256 0.31766427 0.3300024 0.33013362 0.30600643 0.25703266 0.19130845 0.1140555 0.039267965][0.28381342 0.32928622 0.34792838 0.3489486 0.33968771 0.3321816 0.33273387 0.34570578 0.36174667 0.36325592 0.33794406 0.28482929 0.21172109 0.12649752 0.045179266][0.2820552 0.32555774 0.34439948 0.34756351 0.3425734 0.34178218 0.35158941 0.3714914 0.39070815 0.3917754 0.36359382 0.30582127 0.22651722 0.13592772 0.050604876][0.2657162 0.30360383 0.32109943 0.32757404 0.32961559 0.3385132 0.35890684 0.38555 0.40711981 0.40709394 0.3767612 0.31694025 0.23583247 0.14418125 0.05777289][0.23205513 0.26100111 0.27515757 0.28453237 0.29367727 0.31150249 0.34030512 0.37106618 0.39330855 0.39260232 0.36320961 0.30633637 0.22909367 0.14174251 0.058654081][0.18522401 0.20442605 0.21432209 0.2249105 0.23793678 0.25918522 0.2891883 0.31795061 0.33713213 0.33501381 0.30860442 0.25891924 0.19180812 0.11575396 0.042968407][0.12642889 0.13649431 0.14183436 0.15138093 0.16456647 0.18434218 0.21024916 0.23378021 0.24941005 0.24740368 0.22637236 0.18682985 0.13400978 0.074408479 0.017089879][0.062719904 0.066273 0.068258159 0.075405873 0.0861413 0.1015841 0.12073235 0.13769235 0.1495948 0.14850524 0.13350385 0.10503455 0.068067595 0.027281512 -0.012226044][0.0057718833 0.0045466213 0.0039879894 0.0083762491 0.015848644 0.026358571 0.038671885 0.049183581 0.056872569 0.055893373 0.045792956 0.027547376 0.0056414083 -0.017110117 -0.039363131][-0.034124423 -0.038077448 -0.040383134 -0.038783938 -0.034938235 -0.0296904 -0.023860088 -0.019346738 -0.016021525 -0.017452691 -0.023635728 -0.033301715 -0.043082394 -0.051760577 -0.060307451][-0.060195383 -0.065340824 -0.068346515 -0.068757564 -0.067695178 -0.066425614 -0.06533657 -0.065068714 -0.064829744 -0.066376835 -0.069495291 -0.073046088 -0.074884668 -0.0747787 -0.074419081][-0.075029641 -0.080786519 -0.083808213 -0.0852146 -0.085288942 -0.08508718 -0.085005865 -0.085193351 -0.084889047 -0.084969424 -0.085452422 -0.085644528 -0.084222965 -0.081077 -0.07744614]]...]
INFO - root - 2017-12-10 22:36:53.936981: step 60110, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 58h:53m:01s remains)
INFO - root - 2017-12-10 22:37:01.776547: step 60120, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 59h:56m:22s remains)
INFO - root - 2017-12-10 22:37:09.448010: step 60130, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 58h:17m:59s remains)
INFO - root - 2017-12-10 22:37:17.259089: step 60140, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 60h:33m:00s remains)
INFO - root - 2017-12-10 22:37:25.055280: step 60150, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.837 sec/batch; 63h:19m:00s remains)
INFO - root - 2017-12-10 22:37:32.958700: step 60160, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 58h:58m:48s remains)
INFO - root - 2017-12-10 22:37:40.697321: step 60170, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 58h:12m:30s remains)
INFO - root - 2017-12-10 22:37:48.514595: step 60180, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 59h:08m:18s remains)
INFO - root - 2017-12-10 22:37:56.356981: step 60190, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 57h:48m:49s remains)
INFO - root - 2017-12-10 22:38:04.175458: step 60200, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 61h:10m:22s remains)
2017-12-10 22:38:05.067037: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12357569 0.16758905 0.19415389 0.19777194 0.1867159 0.17077871 0.16855891 0.19361344 0.24194497 0.29618573 0.3351841 0.34783313 0.32940072 0.28487045 0.22952743][0.11765094 0.17173195 0.2082776 0.21953031 0.21288504 0.20016417 0.19879948 0.22166334 0.26386657 0.310557 0.34338316 0.35144159 0.32677498 0.27315655 0.20876864][0.10099554 0.16174132 0.20868306 0.23285334 0.23780997 0.23344538 0.23351255 0.2483566 0.273679 0.3011061 0.32007617 0.32291591 0.299886 0.24924749 0.18706238][0.086588711 0.15233764 0.2116133 0.25508583 0.28081316 0.29126918 0.29446527 0.29788673 0.29875508 0.29792273 0.29632941 0.2927494 0.27634656 0.23768215 0.18553345][0.079191744 0.14942896 0.22175792 0.28681135 0.33792579 0.36707312 0.37504697 0.3673889 0.34320667 0.31165242 0.28587988 0.27269587 0.2624042 0.23944883 0.20264348][0.074133933 0.1472152 0.22987466 0.31257373 0.38633436 0.43515319 0.45245492 0.44116184 0.4015719 0.34764186 0.30000877 0.27302095 0.26187295 0.25075397 0.23083171][0.072776541 0.14584315 0.2339455 0.32700217 0.41524348 0.47959989 0.50836647 0.50102252 0.45791528 0.39457119 0.33249009 0.29082918 0.27282289 0.2687732 0.26560059][0.078788526 0.15138844 0.24188907 0.34013906 0.43626869 0.51149577 0.55167639 0.55192685 0.51071131 0.44379374 0.37197852 0.31731561 0.29119572 0.29201528 0.30540675][0.09756276 0.16992475 0.2587631 0.35544544 0.45168751 0.53186268 0.58095008 0.58930212 0.55173784 0.48314413 0.40541986 0.34338006 0.31355622 0.31989774 0.34740645][0.13337873 0.20402767 0.28571057 0.37242648 0.45870134 0.53514153 0.58771473 0.60249662 0.569786 0.50243539 0.42440376 0.36165121 0.33274251 0.34372759 0.37932864][0.18459229 0.24943557 0.32055223 0.39366519 0.4632645 0.52522129 0.56990433 0.583101 0.5526799 0.48921835 0.41757637 0.36142433 0.33738461 0.35032526 0.38557076][0.24247901 0.29960415 0.35949776 0.42025304 0.47206643 0.51211637 0.53774196 0.53905737 0.50535542 0.44685811 0.38805857 0.34629 0.33139917 0.34405586 0.37039283][0.28860646 0.33695811 0.38558772 0.43491513 0.47012052 0.48776639 0.49187905 0.47794956 0.43962717 0.38844034 0.34735861 0.32501912 0.32122096 0.33120802 0.34256932][0.30737844 0.34710586 0.38440347 0.42067194 0.4390215 0.4369626 0.42376927 0.39907363 0.36167625 0.323406 0.30318439 0.30106544 0.3069225 0.31236354 0.30742538][0.28665873 0.32193956 0.35389808 0.38118115 0.38726279 0.37125582 0.34622157 0.31529135 0.28274763 0.2597535 0.25868616 0.27296129 0.28587747 0.28872719 0.2751303]]...]
INFO - root - 2017-12-10 22:38:13.016807: step 60210, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 60h:45m:36s remains)
INFO - root - 2017-12-10 22:38:20.737941: step 60220, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.810 sec/batch; 61h:14m:32s remains)
INFO - root - 2017-12-10 22:38:28.056859: step 60230, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 58h:46m:19s remains)
INFO - root - 2017-12-10 22:38:35.864068: step 60240, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 57h:40m:50s remains)
INFO - root - 2017-12-10 22:38:43.717643: step 60250, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 59h:21m:14s remains)
INFO - root - 2017-12-10 22:38:51.562702: step 60260, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 61h:03m:56s remains)
INFO - root - 2017-12-10 22:38:59.328501: step 60270, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 59h:16m:11s remains)
INFO - root - 2017-12-10 22:39:07.122288: step 60280, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 59h:24m:14s remains)
INFO - root - 2017-12-10 22:39:14.928103: step 60290, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 57h:35m:30s remains)
INFO - root - 2017-12-10 22:39:22.807421: step 60300, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 59h:24m:42s remains)
2017-12-10 22:39:23.746029: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26930463 0.28128925 0.27953675 0.26851496 0.24074507 0.2047783 0.17092405 0.14285129 0.12521738 0.1172272 0.11288628 0.099028185 0.068205155 0.021932619 -0.028498087][0.29903477 0.32970321 0.35249019 0.36269814 0.34441388 0.30885565 0.26895556 0.22976422 0.19767353 0.17744011 0.16676109 0.14826475 0.11136412 0.05536573 -0.0052198796][0.30252317 0.349152 0.3999626 0.43874687 0.43942478 0.41565156 0.38048518 0.33845904 0.2961576 0.26307455 0.24172828 0.2122972 0.16245863 0.091728091 0.01835827][0.29787454 0.36023456 0.43909273 0.50865346 0.533529 0.52977103 0.50988883 0.47624162 0.43126026 0.3882564 0.35318744 0.30485365 0.23102926 0.1352931 0.042873308][0.31287625 0.38837403 0.49484658 0.59362268 0.64246649 0.66036612 0.65983611 0.63898635 0.5944494 0.54105079 0.48873737 0.41555569 0.30969271 0.18250495 0.067416742][0.33333814 0.42291641 0.55505228 0.68020695 0.75236446 0.79131812 0.80827326 0.79623312 0.74721783 0.6783427 0.604558 0.504064 0.3663429 0.21168089 0.080071464][0.34075129 0.44357163 0.59240335 0.7348389 0.82533365 0.88194793 0.9132458 0.90672857 0.85230339 0.76953053 0.67757726 0.55516297 0.39443195 0.22327556 0.084032044][0.32353437 0.43023634 0.582223 0.72847897 0.82715374 0.89292753 0.93265456 0.9293853 0.874197 0.78789651 0.69062847 0.56134319 0.39389765 0.22081488 0.082466133][0.27044404 0.37104878 0.51130891 0.64462882 0.73513627 0.79455972 0.829972 0.82421523 0.77266753 0.69545537 0.60967016 0.4941695 0.34230995 0.18624385 0.06197916][0.17530626 0.25791529 0.37312454 0.47983071 0.55056977 0.59475684 0.61901969 0.60867971 0.56438822 0.503598 0.43892083 0.35058203 0.23126231 0.10899051 0.012794129][0.048043407 0.10648657 0.18942545 0.26398438 0.31194806 0.34061316 0.35443023 0.34210813 0.30857721 0.26743743 0.22614223 0.16816545 0.087779872 0.00743261 -0.052882411][-0.064022921 -0.030463116 0.019545652 0.061702732 0.086261556 0.09960486 0.10446009 0.093423784 0.073151618 0.052177634 0.033316478 0.0040211813 -0.039219059 -0.080317207 -0.10791462][-0.12783147 -0.11657515 -0.09400855 -0.0780367 -0.072331488 -0.0714451 -0.073412888 -0.082099006 -0.091455989 -0.097389974 -0.10002391 -0.10770652 -0.1227206 -0.13511401 -0.13981193][-0.15333802 -0.15614656 -0.15096131 -0.15067558 -0.15565704 -0.16177218 -0.16759434 -0.17432843 -0.17750657 -0.17578425 -0.17048283 -0.1664717 -0.16518527 -0.16128111 -0.15324177][-0.15520787 -0.16364188 -0.1656888 -0.17137453 -0.1794759 -0.18703344 -0.19326027 -0.19828425 -0.19926505 -0.19593152 -0.18958631 -0.18250887 -0.1750931 -0.16488262 -0.15234688]]...]
INFO - root - 2017-12-10 22:39:31.232846: step 60310, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 58h:34m:20s remains)
INFO - root - 2017-12-10 22:39:39.020241: step 60320, loss = 0.72, batch loss = 0.67 (10.1 examples/sec; 0.790 sec/batch; 59h:42m:51s remains)
INFO - root - 2017-12-10 22:39:46.902847: step 60330, loss = 0.70, batch loss = 0.64 (9.5 examples/sec; 0.841 sec/batch; 63h:36m:30s remains)
INFO - root - 2017-12-10 22:39:54.853749: step 60340, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 58h:51m:27s remains)
INFO - root - 2017-12-10 22:40:02.712511: step 60350, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 59h:02m:08s remains)
INFO - root - 2017-12-10 22:40:10.549697: step 60360, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 58h:04m:19s remains)
INFO - root - 2017-12-10 22:40:18.477146: step 60370, loss = 0.68, batch loss = 0.63 (9.7 examples/sec; 0.822 sec/batch; 62h:09m:29s remains)
INFO - root - 2017-12-10 22:40:26.191405: step 60380, loss = 0.68, batch loss = 0.62 (11.0 examples/sec; 0.729 sec/batch; 55h:08m:02s remains)
INFO - root - 2017-12-10 22:40:33.897602: step 60390, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 59h:22m:06s remains)
INFO - root - 2017-12-10 22:40:41.507316: step 60400, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 60h:23m:22s remains)
2017-12-10 22:40:42.391070: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31695864 0.33858597 0.32723805 0.29520673 0.26181966 0.24132463 0.23134312 0.22854398 0.22684242 0.22882237 0.2390514 0.25663412 0.27028614 0.26259017 0.22901979][0.23067158 0.25050619 0.24342042 0.22037928 0.19836892 0.18918796 0.18960474 0.19542985 0.20128745 0.21039785 0.22696088 0.25003988 0.26686 0.26055124 0.22690555][0.13238335 0.14684501 0.14419852 0.13365047 0.12852918 0.13610582 0.15106711 0.16817485 0.18342835 0.19897132 0.21702896 0.23770463 0.24984619 0.23944935 0.20424084][0.068643972 0.076170191 0.076278254 0.076819435 0.087595835 0.1109806 0.1389271 0.16413753 0.18447566 0.20027894 0.21241447 0.2224019 0.22291173 0.20472917 0.16742715][0.047212224 0.048223849 0.049757414 0.058524057 0.080947742 0.11568547 0.15279274 0.18193135 0.20118317 0.2097107 0.20890638 0.20225891 0.18718837 0.16077076 0.12376322][0.05663133 0.055387124 0.06059536 0.076682441 0.10737679 0.15010209 0.19435519 0.22671883 0.24279875 0.24063368 0.22276637 0.19552876 0.16198298 0.12532103 0.088232011][0.085860953 0.087850124 0.10000134 0.12295223 0.1586349 0.20605314 0.2555224 0.29044941 0.30214265 0.28814581 0.25264359 0.20498566 0.15380314 0.10785595 0.071902633][0.13395678 0.14450532 0.16469996 0.19114888 0.22515766 0.27011979 0.31893155 0.35314545 0.3584311 0.33300468 0.28333274 0.21990266 0.15534276 0.10205914 0.067151926][0.19466533 0.21716005 0.24395652 0.26799685 0.29175898 0.32406664 0.36258331 0.3901639 0.38827911 0.35608315 0.30076483 0.23142475 0.16155662 0.10407197 0.067608334][0.24844284 0.28472829 0.31870189 0.33930728 0.34965718 0.36187071 0.38014963 0.39311624 0.38241494 0.34852511 0.29772836 0.2354898 0.17133415 0.11477535 0.075277247][0.2724984 0.31856513 0.35792738 0.37608004 0.37473851 0.36561325 0.35918793 0.3530702 0.33421752 0.3055549 0.27089241 0.23002926 0.18453126 0.13740167 0.097158045][0.25855038 0.30477372 0.34416464 0.36051929 0.35216439 0.32745767 0.30020851 0.27673024 0.25268295 0.23490369 0.22504097 0.21601218 0.19975933 0.17090335 0.13453157][0.22331908 0.26055056 0.29338947 0.30667719 0.29605794 0.26597339 0.22929116 0.19730705 0.17322613 0.16762079 0.18141916 0.20220391 0.21463786 0.20565984 0.17573906][0.19422236 0.21752633 0.23874594 0.24627338 0.2354075 0.2077266 0.1726847 0.14220107 0.12301955 0.12714158 0.15586793 0.19529244 0.22701368 0.23370261 0.21089081][0.18219328 0.19784577 0.21113279 0.21443911 0.20363936 0.17910096 0.14773823 0.12065637 0.10623882 0.11556754 0.15031062 0.19770652 0.23923942 0.25598973 0.23895092]]...]
INFO - root - 2017-12-10 22:40:50.196696: step 60410, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 60h:14m:52s remains)
INFO - root - 2017-12-10 22:40:58.064042: step 60420, loss = 0.67, batch loss = 0.61 (10.1 examples/sec; 0.791 sec/batch; 59h:46m:50s remains)
INFO - root - 2017-12-10 22:41:05.900730: step 60430, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 57h:47m:16s remains)
INFO - root - 2017-12-10 22:41:13.692299: step 60440, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 57h:55m:33s remains)
INFO - root - 2017-12-10 22:41:21.528233: step 60450, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 60h:41m:21s remains)
INFO - root - 2017-12-10 22:41:29.305666: step 60460, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 58h:14m:03s remains)
INFO - root - 2017-12-10 22:41:36.921314: step 60470, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 58h:57m:31s remains)
INFO - root - 2017-12-10 22:41:44.848477: step 60480, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 60h:57m:56s remains)
INFO - root - 2017-12-10 22:41:52.521145: step 60490, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 60h:22m:30s remains)
INFO - root - 2017-12-10 22:42:00.302991: step 60500, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 58h:31m:58s remains)
2017-12-10 22:42:01.103605: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16497542 0.17125101 0.19305283 0.23526801 0.29888991 0.37258023 0.43782046 0.46943408 0.44676411 0.37180886 0.26622829 0.16043235 0.085779004 0.052879374 0.050941415][0.16279191 0.16832581 0.19298345 0.24357447 0.32064337 0.41006589 0.48902196 0.52931255 0.50929546 0.43052265 0.31446189 0.19296399 0.10108247 0.054971788 0.0466232][0.14697562 0.15708165 0.18968169 0.25129491 0.33879727 0.43552166 0.51793164 0.55722511 0.53440344 0.45289987 0.33320647 0.20619808 0.10618918 0.051313296 0.036591325][0.1155484 0.13437207 0.18056391 0.2590284 0.36090255 0.46611017 0.55003494 0.58400238 0.55277312 0.463978 0.34000635 0.21086447 0.10812057 0.048164684 0.027302049][0.076652437 0.10384651 0.16163349 0.25581747 0.3733649 0.48987544 0.5778178 0.60727465 0.56576759 0.46723208 0.33943415 0.21311012 0.11593255 0.058637753 0.036337342][0.050842762 0.0830337 0.14472103 0.24566945 0.37302876 0.49902761 0.59171885 0.61912578 0.57032263 0.46638691 0.34171256 0.22750387 0.14513776 0.09696126 0.075829804][0.051048234 0.086811192 0.14424884 0.23792741 0.36172354 0.49020389 0.58854151 0.62182307 0.57762569 0.48057565 0.369733 0.2757971 0.21260528 0.17402267 0.15136097][0.081554718 0.12469293 0.17510132 0.24950258 0.35394713 0.47278908 0.57301027 0.61769772 0.5897119 0.51092041 0.42080206 0.34876296 0.30335012 0.27243575 0.24633463][0.13405947 0.18872923 0.23252961 0.28058431 0.35209689 0.44599587 0.53745109 0.5927 0.58920169 0.53865784 0.47384703 0.42191809 0.38963667 0.36324644 0.33317751][0.1936544 0.25923574 0.29709047 0.31799152 0.35035849 0.40747193 0.47752413 0.53537637 0.55605668 0.53796858 0.50070268 0.46690255 0.44327736 0.41751835 0.38193482][0.23239262 0.30136508 0.33304742 0.33328658 0.33259627 0.35332826 0.39615187 0.44725204 0.48429739 0.4952473 0.4849028 0.46835855 0.4509272 0.42212588 0.37788558][0.23712029 0.29893392 0.32168153 0.30773017 0.28403062 0.27635017 0.2943095 0.33405191 0.37834519 0.40958023 0.41989484 0.41583693 0.40091202 0.36703598 0.31501469][0.22106388 0.26475039 0.27178144 0.24436873 0.20422231 0.17765047 0.17830423 0.20652147 0.24895924 0.28706348 0.30617759 0.30714351 0.29266816 0.25750893 0.20635715][0.22275969 0.23955363 0.22271724 0.17723002 0.12061746 0.079100318 0.067639634 0.08546342 0.1208237 0.15642124 0.17616619 0.17834817 0.16583489 0.13604009 0.09529043][0.26863891 0.25943547 0.21517 0.14691429 0.071635336 0.016018881 -0.0054940884 0.0026291239 0.027688459 0.054163303 0.068450995 0.069719292 0.062278587 0.045223147 0.022758203]]...]
INFO - root - 2017-12-10 22:42:08.907969: step 60510, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 58h:49m:49s remains)
INFO - root - 2017-12-10 22:42:16.790942: step 60520, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 58h:02m:57s remains)
INFO - root - 2017-12-10 22:42:24.625548: step 60530, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 60h:06m:25s remains)
INFO - root - 2017-12-10 22:42:32.410818: step 60540, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 60h:10m:13s remains)
INFO - root - 2017-12-10 22:42:40.007924: step 60550, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 59h:22m:24s remains)
INFO - root - 2017-12-10 22:42:47.722835: step 60560, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 58h:16m:40s remains)
INFO - root - 2017-12-10 22:42:55.520426: step 60570, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 57h:17m:56s remains)
INFO - root - 2017-12-10 22:43:03.248006: step 60580, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.796 sec/batch; 60h:08m:23s remains)
INFO - root - 2017-12-10 22:43:11.144652: step 60590, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 60h:30m:50s remains)
INFO - root - 2017-12-10 22:43:18.998854: step 60600, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 59h:39m:31s remains)
2017-12-10 22:43:19.793925: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12236103 0.0931242 0.088030204 0.10544462 0.13805573 0.177481 0.21097647 0.23125443 0.23699374 0.23007719 0.21991388 0.20769937 0.19147357 0.17106675 0.14893156][0.081129462 0.052503079 0.047908217 0.067245409 0.10200628 0.14385837 0.18151322 0.2084493 0.22153193 0.21959999 0.21174188 0.20075123 0.18620093 0.16673793 0.14342506][0.042200807 0.021689665 0.022549082 0.044457454 0.078334741 0.11592981 0.14866598 0.17319627 0.18776895 0.18888657 0.18372582 0.17592755 0.16622257 0.15137139 0.13050969][0.02638643 0.01769962 0.027546018 0.05421003 0.087407172 0.11872053 0.14230774 0.15862663 0.16899869 0.16882373 0.16409321 0.15893848 0.1545237 0.14595252 0.13065292][0.034267925 0.0382679 0.058218561 0.091160446 0.12558633 0.15313856 0.169447 0.17728819 0.18012147 0.17356649 0.16427439 0.15706132 0.15378903 0.14894439 0.13894609][0.065781116 0.082959861 0.11424067 0.15576814 0.19415034 0.22040828 0.23163603 0.23119694 0.22256699 0.20216708 0.17992923 0.16323134 0.15520848 0.150182 0.14357503][0.10894895 0.13657616 0.17757532 0.22863822 0.27274117 0.29945716 0.30818105 0.30162719 0.28058949 0.24255058 0.20265812 0.17223378 0.15590015 0.14758079 0.14210013][0.15853809 0.18843785 0.23078604 0.2846311 0.32980937 0.35508817 0.36273265 0.35440218 0.32565507 0.27424717 0.21977812 0.17668745 0.15113592 0.13782063 0.13184604][0.21878266 0.2430471 0.27652735 0.32281762 0.36170515 0.38268259 0.39066285 0.38560069 0.35613784 0.29890427 0.23538764 0.1823232 0.14739679 0.12740612 0.11911865][0.28125519 0.29510891 0.31341505 0.34603491 0.37712246 0.3973085 0.41115052 0.41453141 0.38897723 0.33005992 0.25976789 0.19724546 0.15186703 0.12365192 0.11114477][0.33126661 0.33679217 0.33924589 0.35644075 0.38036323 0.40309426 0.42525008 0.43706867 0.41553488 0.35671374 0.28288823 0.21452202 0.16197233 0.12770282 0.11157622][0.35075191 0.35143647 0.34124002 0.34303206 0.35832831 0.38212588 0.40999416 0.42746261 0.41106188 0.35854197 0.29055396 0.22591743 0.17438583 0.13965836 0.12255961][0.33292177 0.33150056 0.31304452 0.30178937 0.30806789 0.33049867 0.36115158 0.38291043 0.37511304 0.33694008 0.28410912 0.23104072 0.18603308 0.15460053 0.13850744][0.29122886 0.29116395 0.27037582 0.25135219 0.25076687 0.27079758 0.302935 0.32909146 0.33206978 0.31120571 0.27574784 0.23550309 0.19755617 0.16986395 0.1548253][0.23251982 0.23687172 0.22021466 0.20159158 0.20015727 0.22038548 0.25544959 0.28765878 0.30294493 0.29954436 0.28088909 0.25247023 0.22004621 0.19335812 0.17543565]]...]
INFO - root - 2017-12-10 22:43:27.625833: step 60610, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.804 sec/batch; 60h:41m:17s remains)
INFO - root - 2017-12-10 22:43:35.448567: step 60620, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 60h:50m:27s remains)
INFO - root - 2017-12-10 22:43:43.094336: step 60630, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 59h:58m:30s remains)
INFO - root - 2017-12-10 22:43:50.808191: step 60640, loss = 0.70, batch loss = 0.64 (11.2 examples/sec; 0.712 sec/batch; 53h:45m:15s remains)
INFO - root - 2017-12-10 22:43:58.609728: step 60650, loss = 0.67, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 58h:06m:49s remains)
INFO - root - 2017-12-10 22:44:06.516376: step 60660, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 61h:28m:27s remains)
INFO - root - 2017-12-10 22:44:14.242510: step 60670, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.826 sec/batch; 62h:21m:52s remains)
INFO - root - 2017-12-10 22:44:22.025626: step 60680, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 60h:14m:38s remains)
INFO - root - 2017-12-10 22:44:29.845742: step 60690, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 60h:43m:27s remains)
INFO - root - 2017-12-10 22:44:37.681579: step 60700, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.785 sec/batch; 59h:18m:17s remains)
2017-12-10 22:44:38.525199: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0067078536 -0.0071130735 -0.022075357 -0.034436472 -0.04319388 -0.051445514 -0.060558081 -0.068074949 -0.070469588 -0.06446892 -0.050732777 -0.032581277 -0.014592234 -0.0036024544 -0.0044164183][0.016358517 0.011268646 0.005368195 0.00023980142 -0.005156965 -0.014885384 -0.029440634 -0.04427807 -0.052638255 -0.046788935 -0.024743443 0.0095189307 0.04716675 0.074732207 0.081849553][0.038482092 0.050291393 0.06183647 0.070409276 0.072128184 0.061997276 0.040238325 0.013559018 -0.0081218379 -0.010402057 0.014258643 0.063812084 0.12533829 0.17634948 0.19875833][0.072584681 0.10941128 0.14858128 0.18096845 0.19838274 0.19460599 0.16923228 0.12990887 0.088434741 0.066509493 0.079889208 0.13242322 0.21081449 0.28561005 0.33110866][0.11511788 0.18237312 0.25741202 0.32396182 0.36827826 0.38270876 0.36419204 0.31806716 0.25596783 0.20481205 0.18916821 0.22204298 0.29747531 0.38727182 0.46164235][0.15527017 0.24985445 0.35905156 0.461803 0.53938258 0.58256811 0.58365089 0.54186463 0.46608344 0.38426027 0.3273719 0.3184562 0.36669919 0.45621386 0.55865008][0.18186109 0.28995553 0.41800746 0.54549819 0.65241611 0.72772026 0.75760329 0.73258072 0.65669042 0.55439532 0.45828724 0.39916748 0.40403071 0.476775 0.59616917][0.19150041 0.29076931 0.41184348 0.54158151 0.66307169 0.76253277 0.81976396 0.81695437 0.75314653 0.64712805 0.52829659 0.42993775 0.39222378 0.43847358 0.55669415][0.18483676 0.25091144 0.33671424 0.44167489 0.55496794 0.65969461 0.73111165 0.74694842 0.70238054 0.6101315 0.49268636 0.37944433 0.31463519 0.3330639 0.43065053][0.17240725 0.1894756 0.22123899 0.28023466 0.36238763 0.44920328 0.51506132 0.538433 0.51276857 0.44489706 0.34869581 0.24606355 0.1766125 0.17609218 0.24479237][0.16586778 0.13526264 0.1112844 0.11847179 0.15791777 0.21140522 0.25563103 0.27363017 0.25908837 0.21477985 0.14737917 0.071273774 0.0159322 0.009517231 0.053334378][0.16944587 0.10704596 0.042877119 0.008791104 0.0091510015 0.028213274 0.046776652 0.052822035 0.041438412 0.01361892 -0.02840204 -0.075947508 -0.11046805 -0.11399616 -0.085492752][0.17961612 0.10907395 0.028877627 -0.027032515 -0.052132305 -0.05781902 -0.058210887 -0.062112045 -0.073325604 -0.091373362 -0.11583365 -0.14171425 -0.15875202 -0.15741567 -0.13704404][0.18826875 0.12826325 0.053171419 -0.0058980105 -0.04139246 -0.059605129 -0.069341846 -0.077443212 -0.08721257 -0.098429158 -0.11135428 -0.12358955 -0.12995721 -0.12594613 -0.11092041][0.18492131 0.14462104 0.086704887 0.036679588 0.0021194345 -0.018887989 -0.030685082 -0.038489219 -0.045144085 -0.050660852 -0.055706117 -0.059940688 -0.060958873 -0.056552831 -0.046172868]]...]
INFO - root - 2017-12-10 22:44:46.119517: step 60710, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 58h:18m:27s remains)
INFO - root - 2017-12-10 22:44:54.001206: step 60720, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 59h:34m:46s remains)
INFO - root - 2017-12-10 22:45:01.821445: step 60730, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 59h:11m:17s remains)
INFO - root - 2017-12-10 22:45:09.722635: step 60740, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.800 sec/batch; 60h:24m:16s remains)
INFO - root - 2017-12-10 22:45:17.573765: step 60750, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 60h:22m:29s remains)
INFO - root - 2017-12-10 22:45:25.432341: step 60760, loss = 0.67, batch loss = 0.62 (10.6 examples/sec; 0.752 sec/batch; 56h:46m:14s remains)
INFO - root - 2017-12-10 22:45:33.263050: step 60770, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 58h:30m:35s remains)
INFO - root - 2017-12-10 22:45:41.163182: step 60780, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 60h:33m:37s remains)
INFO - root - 2017-12-10 22:45:48.842682: step 60790, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 59h:26m:29s remains)
INFO - root - 2017-12-10 22:45:56.796170: step 60800, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.827 sec/batch; 62h:25m:35s remains)
2017-12-10 22:45:57.602896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.088155158 -0.084129915 -0.077962257 -0.074401721 -0.075988181 -0.083406605 -0.0938527 -0.10346186 -0.11059127 -0.11392199 -0.11225507 -0.10700715 -0.10186916 -0.099743932 -0.10000244][-0.052379511 -0.039422229 -0.0273245 -0.022505268 -0.028158532 -0.044265281 -0.065518424 -0.084966816 -0.099492393 -0.10573205 -0.10187481 -0.091240361 -0.080272511 -0.075201482 -0.07574071][0.014518569 0.042229723 0.063673452 0.070694447 0.060081732 0.032414887 -0.0047135716 -0.040097304 -0.067437887 -0.080366492 -0.076025821 -0.059798971 -0.041321032 -0.031735536 -0.031339779][0.10557536 0.1564492 0.19629084 0.21416216 0.20614056 0.17120069 0.11719888 0.060088109 0.012072468 -0.015831856 -0.018815625 -0.0037837869 0.017268013 0.028098136 0.02734324][0.19901881 0.28136617 0.35154667 0.39478841 0.40514195 0.3765215 0.31400225 0.2366567 0.16436036 0.11374895 0.09229137 0.092849582 0.10281344 0.10313579 0.091678694][0.26346454 0.37615529 0.4805057 0.5589568 0.60340655 0.60077363 0.55097073 0.47116268 0.38584062 0.3147082 0.26752383 0.23923981 0.21902245 0.18953958 0.15090197][0.27517214 0.4068172 0.5386712 0.65321255 0.74049246 0.78100932 0.76917237 0.71402442 0.63717568 0.5557515 0.481111 0.4125776 0.34421921 0.26660353 0.18634523][0.2403495 0.37209421 0.51341873 0.65074748 0.77336222 0.858631 0.89673346 0.88434696 0.83207446 0.75008196 0.6518696 0.54347217 0.42579803 0.30016047 0.18092977][0.18700974 0.29945284 0.42566389 0.55928785 0.69111067 0.80154914 0.88005441 0.91099793 0.88940787 0.81605381 0.70672274 0.57455277 0.42751366 0.27591616 0.13845524][0.14324544 0.22481549 0.31604087 0.41873604 0.52769935 0.6322189 0.72517258 0.78358829 0.787855 0.72973651 0.62619328 0.49501169 0.34892958 0.2025644 0.074305527][0.12527598 0.17767484 0.22922477 0.28768903 0.35284278 0.42531967 0.5039869 0.56629544 0.5841952 0.54337859 0.46004066 0.35175645 0.23280416 0.1168164 0.018117402][0.12349976 0.15673469 0.17761828 0.1958185 0.21525791 0.2458176 0.29232687 0.33910576 0.35790122 0.33263355 0.27660018 0.20231825 0.12281962 0.047715861 -0.014918591][0.10901751 0.13091889 0.13391313 0.12573211 0.11221554 0.10757358 0.12045329 0.14434326 0.15686193 0.14379115 0.11397054 0.07287284 0.030758221 -0.0072805905 -0.038983479][0.063782856 0.076537296 0.071214348 0.051512979 0.022705805 -0.001576111 -0.011524466 -0.0063279076 -0.00042844631 -0.0046851216 -0.014790083 -0.03135572 -0.047132328 -0.060263332 -0.071836792][-0.0035445215 -0.00013320542 -0.0085317064 -0.029148703 -0.058798961 -0.087351844 -0.10620458 -0.11094355 -0.10940009 -0.10860103 -0.10737914 -0.10972665 -0.11129296 -0.11180845 -0.11293768]]...]
INFO - root - 2017-12-10 22:46:05.444819: step 60810, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 59h:04m:52s remains)
INFO - root - 2017-12-10 22:46:13.302803: step 60820, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 59h:33m:30s remains)
INFO - root - 2017-12-10 22:46:21.190217: step 60830, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 57h:08m:35s remains)
INFO - root - 2017-12-10 22:46:29.016591: step 60840, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 60h:37m:31s remains)
INFO - root - 2017-12-10 22:46:36.903957: step 60850, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.806 sec/batch; 60h:48m:25s remains)
INFO - root - 2017-12-10 22:46:44.847187: step 60860, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 59h:32m:29s remains)
INFO - root - 2017-12-10 22:46:52.358801: step 60870, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 59h:00m:10s remains)
INFO - root - 2017-12-10 22:47:00.278956: step 60880, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 59h:48m:04s remains)
INFO - root - 2017-12-10 22:47:08.293090: step 60890, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 61h:03m:03s remains)
INFO - root - 2017-12-10 22:47:16.165789: step 60900, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 59h:10m:22s remains)
2017-12-10 22:47:17.114466: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.41842714 0.36149108 0.27194893 0.17793 0.10660111 0.071940079 0.0682818 0.083790436 0.10590043 0.12395117 0.13799931 0.15092856 0.16753285 0.19370028 0.22265461][0.40787563 0.35048115 0.27187115 0.19584736 0.14151469 0.1169214 0.117399 0.13286372 0.15037681 0.1602819 0.16625014 0.17500363 0.19147058 0.21853317 0.24915057][0.37362856 0.32217929 0.26438609 0.22111796 0.20169428 0.20386705 0.2196576 0.23765618 0.24357912 0.23210408 0.21754153 0.21694867 0.23490161 0.26677218 0.3013097][0.33239931 0.29260561 0.26409382 0.26431173 0.29245284 0.33292258 0.37075809 0.39029431 0.37632972 0.33107254 0.28388357 0.26556352 0.28094643 0.31719521 0.35652745][0.2853635 0.26462135 0.27376875 0.32737681 0.41500404 0.50531483 0.57278138 0.59388369 0.55326515 0.46334156 0.37020546 0.31970155 0.318542 0.34864783 0.38655928][0.2331773 0.23763493 0.28919142 0.39967409 0.55095094 0.695496 0.79538757 0.81771743 0.74843627 0.61152357 0.46746284 0.37541169 0.34540424 0.35798585 0.38549584][0.18077184 0.21518081 0.3093105 0.47032326 0.67623591 0.86657768 0.99293768 1.0147768 0.92109948 0.7452547 0.55604351 0.42226261 0.3589265 0.34872368 0.36136568][0.13903549 0.19956902 0.32477129 0.5148769 0.74799991 0.95866275 1.0941197 1.1114986 1.0042275 0.81053591 0.5984152 0.43908411 0.35259786 0.32601398 0.32784522][0.11916377 0.19342482 0.32723477 0.51375419 0.73411793 0.92788327 1.0470973 1.0540364 0.94747728 0.76506567 0.56454849 0.41020858 0.32344377 0.29572648 0.29624632][0.12615426 0.19981658 0.32042456 0.47563234 0.650376 0.79684991 0.8795172 0.87248003 0.7797913 0.633444 0.47473562 0.35356715 0.28827024 0.27309948 0.28042176][0.15165752 0.21244407 0.30379406 0.41170931 0.52492046 0.61149538 0.65138274 0.63292348 0.563678 0.46700558 0.36679342 0.29512373 0.26376206 0.26777047 0.28467992][0.1832307 0.22414486 0.27978596 0.33807474 0.39179602 0.42435691 0.4295226 0.4069961 0.36473879 0.31678921 0.27278876 0.2489863 0.24986094 0.27025688 0.29363027][0.20013921 0.22381277 0.24966997 0.26916915 0.27831391 0.27226114 0.2547791 0.23291367 0.21314768 0.20160028 0.19863786 0.20902024 0.23112789 0.25941327 0.2832244][0.17602287 0.18833135 0.19667058 0.19494992 0.18106806 0.15666501 0.13020787 0.11160697 0.10538417 0.11267935 0.12979542 0.15528557 0.18426967 0.21155311 0.2313125][0.11411462 0.12011754 0.12235497 0.11563813 0.098608218 0.074363224 0.051072642 0.037756417 0.037266809 0.048736289 0.068156615 0.092251673 0.11598454 0.1355256 0.14852421]]...]
INFO - root - 2017-12-10 22:47:24.939200: step 60910, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 59h:38m:58s remains)
INFO - root - 2017-12-10 22:47:32.849533: step 60920, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 59h:09m:18s remains)
INFO - root - 2017-12-10 22:47:40.505062: step 60930, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 58h:52m:19s remains)
INFO - root - 2017-12-10 22:47:48.327225: step 60940, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 58h:21m:42s remains)
INFO - root - 2017-12-10 22:47:56.126660: step 60950, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 60h:02m:24s remains)
INFO - root - 2017-12-10 22:48:04.039240: step 60960, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 57h:20m:54s remains)
INFO - root - 2017-12-10 22:48:11.885860: step 60970, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 58h:46m:47s remains)
INFO - root - 2017-12-10 22:48:19.738652: step 60980, loss = 0.70, batch loss = 0.64 (10.8 examples/sec; 0.743 sec/batch; 56h:02m:23s remains)
INFO - root - 2017-12-10 22:48:27.562985: step 60990, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 59h:25m:14s remains)
INFO - root - 2017-12-10 22:48:35.375115: step 61000, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 60h:16m:23s remains)
2017-12-10 22:48:36.219551: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010022028 0.028299503 0.044538196 0.054224338 0.054060612 0.044671003 0.028145036 0.0096541317 -0.0045468309 -0.01216206 -0.01467825 -0.014781822 -0.014533589 -0.014554265 -0.014742437][-0.0022063246 0.012006554 0.026315289 0.037337922 0.042039424 0.039076548 0.028626412 0.014521197 0.0026040012 -0.0048524169 -0.008962173 -0.011076484 -0.012286746 -0.013443808 -0.014710822][-0.0065677222 0.0075280354 0.023557374 0.03814001 0.048259813 0.051186621 0.045466378 0.033396363 0.020689065 0.010394357 0.0021291962 -0.0041817976 -0.0086296964 -0.011785418 -0.014177305][-0.0065001356 0.0097936587 0.02988301 0.049733851 0.065950006 0.074611381 0.072554477 0.060710583 0.045009691 0.029703585 0.015719028 0.0040253382 -0.0043295748 -0.00972703 -0.013010729][-0.0053361924 0.013346312 0.037666157 0.0628839 0.084756382 0.098117784 0.098328955 0.085379779 0.065715551 0.04499558 0.025610037 0.0097391345 -0.00096050458 -0.007244694 -0.010413935][-0.0041300757 0.016189607 0.0432772 0.071922034 0.096935965 0.11228761 0.11268584 0.097898431 0.07498955 0.050609421 0.028380502 0.011308451 0.00090099528 -0.0043553775 -0.0063738576][-0.0039458619 0.016065333 0.043119296 0.071600609 0.095813416 0.10987186 0.1089758 0.093253516 0.069891989 0.045799494 0.024998507 0.010193906 0.0022612459 -0.00090055278 -0.0018336258][-0.0058904057 0.011341815 0.034845456 0.059069484 0.078804672 0.089236319 0.086870074 0.072352223 0.052225173 0.032505017 0.016531944 0.0061350749 0.0015331374 0.00034756662 -0.00020080186][-0.010126632 0.0021802215 0.019347765 0.036486004 0.049714167 0.055867262 0.052928295 0.041790932 0.027512487 0.014352513 0.004426349 -0.0013996835 -0.0032737418 -0.0033410359 -0.0041064476][-0.015418623 -0.0084811319 0.0017594091 0.011643824 0.018823696 0.021636544 0.019083332 0.011894315 0.0033942845 -0.0039043413 -0.0089067472 -0.011419358 -0.011758569 -0.011379174 -0.012231939][-0.019521672 -0.016766662 -0.011851712 -0.0073143379 -0.0043200133 -0.0035348157 -0.0054677948 -0.0096944226 -0.014290917 -0.017876098 -0.019902388 -0.020554425 -0.020221366 -0.01976138 -0.020426029][-0.021852264 -0.021587837 -0.019858843 -0.018394157 -0.017661331 -0.017816691 -0.019151742 -0.021467583 -0.023775591 -0.02531695 -0.025824646 -0.025586503 -0.025010131 -0.024579093 -0.024989583][-0.022619823 -0.02349609 -0.023257615 -0.023183828 -0.023393903 -0.023850705 -0.024707975 -0.02588092 -0.02689554 -0.027368968 -0.027206266 -0.026660778 -0.026090529 -0.025794625 -0.026065303][-0.022426831 -0.02372686 -0.023963813 -0.024305983 -0.024743952 -0.025188932 -0.025695523 -0.02622989 -0.02658822 -0.026614729 -0.026290335 -0.025734656 -0.025344171 -0.025215948 -0.025425695][-0.022026602 -0.023349237 -0.023586061 -0.023874005 -0.0241848 -0.024452 -0.024692548 -0.02490731 -0.02502002 -0.024976194 -0.024771478 -0.024445148 -0.024281159 -0.024182046 -0.024213936]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 22:48:44.041440: step 61010, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 57h:50m:42s remains)
INFO - root - 2017-12-10 22:48:51.869713: step 61020, loss = 0.71, batch loss = 0.66 (10.2 examples/sec; 0.786 sec/batch; 59h:15m:12s remains)
INFO - root - 2017-12-10 22:48:59.443453: step 61030, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 58h:14m:49s remains)
INFO - root - 2017-12-10 22:49:07.296047: step 61040, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 60h:34m:23s remains)
INFO - root - 2017-12-10 22:49:15.250481: step 61050, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.820 sec/batch; 61h:49m:20s remains)
INFO - root - 2017-12-10 22:49:23.101463: step 61060, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 58h:34m:53s remains)
INFO - root - 2017-12-10 22:49:30.938459: step 61070, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 58h:25m:22s remains)
INFO - root - 2017-12-10 22:49:38.833359: step 61080, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 59h:54m:23s remains)
INFO - root - 2017-12-10 22:49:46.802395: step 61090, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 60h:27m:20s remains)
INFO - root - 2017-12-10 22:49:54.632889: step 61100, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 58h:22m:36s remains)
2017-12-10 22:49:55.427273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.11953094 -0.11843649 -0.11330277 -0.10802347 -0.10438967 -0.10291674 -0.10291216 -0.10335761 -0.10259289 -0.099961676 -0.095193811 -0.08762905 -0.074856848 -0.055540424 -0.030961076][-0.11611393 -0.10870027 -0.09658993 -0.08703322 -0.083764471 -0.087395482 -0.095463552 -0.10476439 -0.11150275 -0.11309545 -0.10891691 -0.0999177 -0.085983574 -0.066731319 -0.043185819][-0.089291736 -0.072750837 -0.048720002 -0.029096613 -0.022245141 -0.030620161 -0.0503043 -0.074851751 -0.096981667 -0.110493 -0.11339112 -0.10786851 -0.096691914 -0.081222124 -0.06250722][-0.039284285 -0.0092924107 0.0331924 0.071496919 0.090154089 0.082239509 0.051864952 0.008895752 -0.035182446 -0.069470413 -0.088773832 -0.094702721 -0.0924119 -0.085180543 -0.074364237][0.035956264 0.080228783 0.14394587 0.20722902 0.24618021 0.24669972 0.21074541 0.15042035 0.081205577 0.020177014 -0.021911321 -0.043908216 -0.052763861 -0.054143749 -0.0505039][0.12776759 0.18165955 0.26304597 0.352499 0.41785038 0.435688 0.40267435 0.3307265 0.23821029 0.14943361 0.083022282 0.044452358 0.026245011 0.019988237 0.020614984][0.2191581 0.27115762 0.35774946 0.46418783 0.55306071 0.59208083 0.57080036 0.49816066 0.3930096 0.28572682 0.20215203 0.15246715 0.13070682 0.12605166 0.12924097][0.27648774 0.31468233 0.39042875 0.49679512 0.59610903 0.65152663 0.64641124 0.58507508 0.48424765 0.37683719 0.2922875 0.24360549 0.22740006 0.23126854 0.24124968][0.27131206 0.28861836 0.341961 0.4313992 0.5238483 0.58359981 0.59121054 0.54684019 0.46388394 0.37414718 0.30553728 0.27090192 0.26868653 0.28554091 0.30414915][0.19502872 0.19314072 0.22281323 0.28757754 0.36172852 0.41511336 0.42873162 0.39980704 0.33843964 0.27292493 0.2267573 0.21066961 0.22368047 0.25226423 0.27812862][0.081292622 0.068425424 0.082349613 0.1264898 0.18316588 0.22880402 0.24591868 0.22941929 0.1861818 0.13920131 0.10714791 0.099483691 0.11668654 0.1472372 0.17446604][-0.022321867 -0.034644268 -0.023405831 0.012978178 0.063080557 0.10826611 0.13172947 0.12568693 0.093790747 0.053297568 0.019758314 0.0042102463 0.011090138 0.033638667 0.058338273][-0.092323512 -0.093356788 -0.070117839 -0.02370915 0.037639603 0.096584246 0.1342804 0.13871045 0.11079001 0.063722342 0.012953055 -0.025498273 -0.040427435 -0.031898536 -0.010189704][-0.11750742 -0.10310053 -0.059753168 0.0097382553 0.098368473 0.18612222 0.24811623 0.26561946 0.23708238 0.17384422 0.093856409 0.021647073 -0.021833373 -0.029050957 -0.0080937119][-0.098139919 -0.072659038 -0.01370593 0.077197015 0.19476242 0.31483749 0.40431035 0.4369117 0.40854216 0.32946762 0.22050475 0.11572199 0.046211351 0.02647841 0.049486972]]...]
INFO - root - 2017-12-10 22:50:02.829062: step 61110, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 58h:05m:36s remains)
INFO - root - 2017-12-10 22:50:10.664505: step 61120, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 59h:44m:09s remains)
INFO - root - 2017-12-10 22:50:18.564886: step 61130, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 59h:55m:54s remains)
INFO - root - 2017-12-10 22:50:26.524910: step 61140, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 61h:02m:43s remains)
INFO - root - 2017-12-10 22:50:34.439519: step 61150, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 60h:16m:28s remains)
INFO - root - 2017-12-10 22:50:42.272681: step 61160, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 59h:53m:46s remains)
INFO - root - 2017-12-10 22:50:50.094880: step 61170, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 58h:04m:32s remains)
INFO - root - 2017-12-10 22:50:58.048986: step 61180, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 59h:16m:25s remains)
INFO - root - 2017-12-10 22:51:05.780381: step 61190, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 60h:14m:33s remains)
INFO - root - 2017-12-10 22:51:13.550403: step 61200, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.828 sec/batch; 62h:24m:11s remains)
2017-12-10 22:51:14.443158: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34839997 0.28475064 0.2571404 0.26440084 0.28831926 0.30623719 0.3115035 0.30381098 0.287953 0.26960731 0.25497761 0.25363892 0.26643881 0.2905086 0.31502852][0.34797111 0.28008282 0.25458685 0.27262354 0.31362203 0.35169497 0.37721333 0.38483539 0.37400404 0.34733373 0.31546351 0.29744583 0.30166969 0.32696465 0.35758638][0.27783319 0.22271425 0.21373218 0.25259206 0.31906313 0.38661075 0.44337189 0.47577846 0.4731001 0.43173957 0.36928195 0.3189764 0.30044943 0.3161017 0.34585178][0.16684806 0.13632762 0.15252413 0.21715254 0.31340212 0.41668144 0.51206678 0.57528609 0.58223206 0.52156752 0.41889015 0.32396826 0.27088988 0.2671203 0.2891489][0.054851543 0.056469515 0.10162798 0.19233367 0.31646556 0.45330539 0.58427507 0.67502028 0.68857419 0.60740447 0.46427688 0.322665 0.22939867 0.19812869 0.20584959][-0.019500352 0.013269578 0.083581083 0.1944757 0.33953181 0.50220859 0.66097903 0.7731967 0.79192346 0.69540727 0.520005 0.33655933 0.20095947 0.13457215 0.12075914][-0.042693209 0.0068505709 0.089059778 0.20655894 0.35847443 0.53261834 0.70647609 0.83256072 0.85936344 0.76095229 0.57039446 0.35767975 0.18462861 0.08367341 0.047054727][-0.011205948 0.045535173 0.12756884 0.23664622 0.37534845 0.53659379 0.70027471 0.82129586 0.85144997 0.76302809 0.57913637 0.36013776 0.16895287 0.047177464 -0.0034138949][0.064271122 0.12847556 0.20637806 0.29669124 0.40268758 0.52369362 0.64762944 0.74123251 0.76691771 0.69749629 0.54225516 0.34482375 0.16247407 0.040061213 -0.013061006][0.1597546 0.2406937 0.31954437 0.38871491 0.4497242 0.51073527 0.57445025 0.62654012 0.64266008 0.59650213 0.48353028 0.32806927 0.17619966 0.069996417 0.023096239][0.24885543 0.3510932 0.434868 0.4841949 0.49837443 0.49567294 0.49727547 0.50854576 0.51619691 0.49329904 0.42399019 0.31677788 0.20585375 0.12604885 0.089616247][0.30893579 0.42913505 0.51774514 0.55079889 0.5248037 0.46614829 0.4122209 0.38560084 0.38319251 0.37867031 0.34916237 0.29183796 0.22876039 0.18310966 0.16054913][0.34489655 0.47447631 0.5628112 0.58087867 0.52312577 0.42057869 0.32125691 0.26008523 0.24329294 0.24936326 0.25409877 0.24653624 0.23490009 0.22705948 0.22040838][0.36698383 0.4924601 0.56937504 0.56820291 0.4856002 0.3557148 0.23097731 0.15159512 0.12773663 0.14293242 0.17497672 0.20797499 0.23754127 0.25832009 0.26205155][0.37883207 0.48826617 0.54336077 0.51872367 0.41816598 0.27910182 0.15329714 0.078591168 0.0614079 0.086718611 0.13494071 0.18961979 0.23989771 0.27397421 0.28130513]]...]
INFO - root - 2017-12-10 22:51:22.365453: step 61210, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.818 sec/batch; 61h:37m:51s remains)
INFO - root - 2017-12-10 22:51:30.255968: step 61220, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 60h:44m:30s remains)
INFO - root - 2017-12-10 22:51:38.125858: step 61230, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 57h:47m:12s remains)
INFO - root - 2017-12-10 22:51:46.014449: step 61240, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.804 sec/batch; 60h:35m:23s remains)
INFO - root - 2017-12-10 22:51:53.920720: step 61250, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 59h:38m:47s remains)
INFO - root - 2017-12-10 22:52:01.777386: step 61260, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 58h:18m:34s remains)
INFO - root - 2017-12-10 22:52:09.590023: step 61270, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.818 sec/batch; 61h:38m:16s remains)
INFO - root - 2017-12-10 22:52:17.489618: step 61280, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 59h:59m:29s remains)
INFO - root - 2017-12-10 22:52:25.177346: step 61290, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 58h:25m:45s remains)
INFO - root - 2017-12-10 22:52:33.045145: step 61300, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 57h:14m:19s remains)
2017-12-10 22:52:33.910010: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23814087 0.2536366 0.23876977 0.20924447 0.18337333 0.17100598 0.17013714 0.1710621 0.17076302 0.167404 0.15943123 0.14499931 0.12174059 0.090431049 0.052304178][0.28236416 0.29539645 0.27408308 0.23989072 0.21347372 0.20423807 0.20814152 0.21359465 0.21729317 0.21581602 0.2070246 0.18865068 0.15840994 0.11750104 0.068452045][0.28205287 0.29194552 0.2710706 0.24336892 0.22712173 0.227504 0.23726074 0.24406451 0.24533138 0.23897621 0.22452347 0.2009545 0.16641371 0.12115688 0.067455143][0.24576879 0.25545266 0.242687 0.22970468 0.22902952 0.24002683 0.25262037 0.25523144 0.24811216 0.2333459 0.2132328 0.1882901 0.15559635 0.11309144 0.060996994][0.2023098 0.21756396 0.22017555 0.22835176 0.24699806 0.26909333 0.28238571 0.27643788 0.25529024 0.22751121 0.20010786 0.17493357 0.14740397 0.1113037 0.063327715][0.1790998 0.20137012 0.21883769 0.2469033 0.28406867 0.31797504 0.33374405 0.32125741 0.28681016 0.24470216 0.20726079 0.17878364 0.15397123 0.1225568 0.076910257][0.18288043 0.20861721 0.23364204 0.27409223 0.32513827 0.37083107 0.39274469 0.37989837 0.33874607 0.28675112 0.24079961 0.20878285 0.18567961 0.15654346 0.10895795][0.20791675 0.23194213 0.25644508 0.29998189 0.35814393 0.41337577 0.44357181 0.43563032 0.39389572 0.33653957 0.28411677 0.24889538 0.22690076 0.19820166 0.14684062][0.23786558 0.2576023 0.27544096 0.31369039 0.37115768 0.43089059 0.46876308 0.46931574 0.43322405 0.37608123 0.31985286 0.28091377 0.25713643 0.22570246 0.16932757][0.25170714 0.26559317 0.27419397 0.30270234 0.35386026 0.41268545 0.45507583 0.46362573 0.43535954 0.38176242 0.32477275 0.2828359 0.25619093 0.22175656 0.16349046][0.22778456 0.23408097 0.23251744 0.2483779 0.28732094 0.33786923 0.37830564 0.39093283 0.37012807 0.32385579 0.27232867 0.2329945 0.20740032 0.17507307 0.12300424][0.16449617 0.16315851 0.15353285 0.15736564 0.18156746 0.21873592 0.25139621 0.26380691 0.24950749 0.21376128 0.17312065 0.14176421 0.12177226 0.097018555 0.058027212][0.076487795 0.070325561 0.057652045 0.053929463 0.065327331 0.087838471 0.10933995 0.11795649 0.10796981 0.082659483 0.054029111 0.032285541 0.019686537 0.0050058234 -0.017913219][-0.0026165086 -0.010556756 -0.022363506 -0.02929369 -0.026790876 -0.016576817 -0.0056264605 -0.0013763467 -0.0081198011 -0.024300316 -0.042438824 -0.055766959 -0.062248837 -0.06865792 -0.078489356][-0.049298357 -0.056990493 -0.066270247 -0.073631108 -0.076099172 -0.073994383 -0.070515767 -0.069485068 -0.073492661 -0.082337514 -0.092138551 -0.099023305 -0.10157496 -0.1032098 -0.10548221]]...]
INFO - root - 2017-12-10 22:52:41.799721: step 61310, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 59h:07m:06s remains)
INFO - root - 2017-12-10 22:52:49.741895: step 61320, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 58h:43m:11s remains)
INFO - root - 2017-12-10 22:52:57.783652: step 61330, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 60h:44m:30s remains)
INFO - root - 2017-12-10 22:53:05.667797: step 61340, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.798 sec/batch; 60h:05m:50s remains)
INFO - root - 2017-12-10 22:53:13.415859: step 61350, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 59h:07m:11s remains)
INFO - root - 2017-12-10 22:53:21.177012: step 61360, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 57h:16m:51s remains)
INFO - root - 2017-12-10 22:53:29.040981: step 61370, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 59h:14m:42s remains)
INFO - root - 2017-12-10 22:53:36.889701: step 61380, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 60h:35m:56s remains)
INFO - root - 2017-12-10 22:53:44.818830: step 61390, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 59h:24m:16s remains)
INFO - root - 2017-12-10 22:53:52.711882: step 61400, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.803 sec/batch; 60h:27m:14s remains)
2017-12-10 22:53:53.537856: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.055679809 0.044785518 0.041458305 0.045146067 0.047226794 0.04534924 0.041710429 0.036745887 0.031586621 0.026736094 0.022165651 0.016314823 0.0088000968 0.0027957861 0.00015892983][0.08570765 0.071072385 0.066441439 0.071665809 0.074391186 0.070249833 0.061622087 0.050694317 0.040587846 0.03294437 0.027212698 0.020403858 0.012121495 0.0063545038 0.0042947084][0.12668104 0.10775072 0.10106289 0.10762172 0.11208251 0.10774323 0.096314788 0.081128471 0.066855304 0.056489713 0.048364349 0.038903214 0.027858311 0.020720018 0.019505978][0.16412859 0.14481051 0.13980103 0.1513658 0.16207829 0.16282627 0.15340765 0.13722904 0.11969782 0.10543139 0.092549518 0.077166215 0.060235638 0.050167993 0.049707279][0.17954774 0.16661949 0.17008354 0.1931981 0.21726757 0.23078358 0.2303278 0.21759337 0.19761722 0.17704269 0.15602626 0.13179664 0.1074674 0.094222918 0.095504552][0.16705045 0.16358961 0.17965606 0.21810541 0.26005673 0.29180703 0.30621326 0.30129981 0.28088152 0.25327334 0.22267149 0.18958485 0.15967415 0.14607567 0.15272884][0.12984864 0.13528989 0.1616836 0.2114466 0.26750493 0.31564584 0.34526515 0.35055965 0.33331656 0.3022745 0.26578912 0.22895765 0.20044012 0.19336237 0.2112138][0.087700792 0.095161065 0.12216435 0.17085911 0.22803749 0.281337 0.31931257 0.33400667 0.32473415 0.29844847 0.265073 0.23349077 0.21446623 0.2197374 0.25159758][0.055610437 0.058399715 0.077081069 0.11350138 0.15856141 0.20401968 0.24070579 0.26076171 0.26144814 0.24652363 0.22442973 0.20453162 0.19800106 0.21477212 0.25641102][0.044219729 0.038900048 0.044984683 0.064656809 0.091353677 0.12120709 0.148856 0.16793776 0.17485233 0.1705796 0.16082081 0.15262291 0.15547514 0.17731637 0.21953546][0.05235789 0.038095586 0.032590304 0.037641712 0.047449015 0.061143968 0.076737672 0.0896029 0.0963539 0.096669145 0.093781926 0.091902122 0.097970024 0.11782995 0.15288951][0.05835966 0.039742127 0.027765172 0.024244647 0.023281861 0.025526242 0.031015256 0.036527421 0.039383136 0.039267771 0.037854142 0.03725801 0.041806802 0.055296738 0.079107627][0.044648819 0.026296729 0.013073115 0.0065994258 0.00115833 -0.0020461513 -0.0020915624 -0.0012441755 -0.0015345613 -0.0028100454 -0.0042255358 -0.0049469508 -0.002899192 0.0040699369 0.017020239][0.0060326741 -0.00866997 -0.019308804 -0.024927545 -0.030377343 -0.034386612 -0.035735629 -0.036020536 -0.036853991 -0.037871581 -0.038643923 -0.039097361 -0.038577203 -0.035940062 -0.030410843][-0.036328387 -0.046671603 -0.053418018 -0.056820914 -0.060409389 -0.063126653 -0.063858211 -0.063707106 -0.063940056 -0.064260446 -0.064444751 -0.064616807 -0.064731568 -0.064156681 -0.062359]]...]
INFO - root - 2017-12-10 22:54:01.532603: step 61410, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.819 sec/batch; 61h:41m:53s remains)
INFO - root - 2017-12-10 22:54:09.488645: step 61420, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 60h:35m:47s remains)
INFO - root - 2017-12-10 22:54:17.134550: step 61430, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 58h:08m:54s remains)
INFO - root - 2017-12-10 22:54:25.039909: step 61440, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 59h:40m:00s remains)
INFO - root - 2017-12-10 22:54:32.977122: step 61450, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.809 sec/batch; 60h:55m:34s remains)
INFO - root - 2017-12-10 22:54:40.779184: step 61460, loss = 0.70, batch loss = 0.64 (13.1 examples/sec; 0.612 sec/batch; 46h:04m:14s remains)
INFO - root - 2017-12-10 22:54:48.792286: step 61470, loss = 0.72, batch loss = 0.66 (9.0 examples/sec; 0.891 sec/batch; 67h:06m:45s remains)
INFO - root - 2017-12-10 22:54:56.624584: step 61480, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 58h:02m:30s remains)
INFO - root - 2017-12-10 22:55:04.578893: step 61490, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 60h:55m:13s remains)
INFO - root - 2017-12-10 22:55:12.558682: step 61500, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 58h:57m:27s remains)
2017-12-10 22:55:13.411857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.039004367 -0.035469908 -0.03255574 -0.034263104 -0.03667184 -0.038974676 -0.043990016 -0.051315706 -0.057290547 -0.061684269 -0.066690966 -0.067381546 -0.063665777 -0.0581927 -0.054691561][-0.0072987066 0.010116265 0.023118718 0.025966119 0.026961705 0.028338179 0.021897001 0.0067683556 -0.0075634504 -0.017301198 -0.027980881 -0.0343384 -0.034254804 -0.028552789 -0.023416806][0.057675119 0.10659604 0.14335366 0.15936469 0.1683501 0.17402303 0.16028874 0.12516105 0.090002455 0.066456333 0.045247905 0.028878843 0.021394577 0.026999798 0.035517704][0.14556517 0.24018404 0.31102747 0.3460947 0.36635694 0.37655866 0.35053402 0.28748021 0.22485013 0.18434885 0.1538533 0.1285101 0.11342257 0.11776767 0.12828158][0.24253821 0.38937235 0.4983362 0.55484265 0.58826452 0.60397846 0.56584036 0.47626826 0.38960329 0.33662972 0.30249384 0.27270851 0.25270241 0.25653496 0.26743966][0.32144174 0.51110309 0.65293419 0.73104131 0.781107 0.80702382 0.76508635 0.65988457 0.55765373 0.49625203 0.46116602 0.42932358 0.40729237 0.41198108 0.42247084][0.36126614 0.57432789 0.73759127 0.83532465 0.90531838 0.94710463 0.9106403 0.80159676 0.69258952 0.62656671 0.5912925 0.55742258 0.53361428 0.53734267 0.54351223][0.35006812 0.56206059 0.73062432 0.83957595 0.92500091 0.98200947 0.95739996 0.85729718 0.75305706 0.68981683 0.65714896 0.622756 0.59623104 0.5940972 0.59140289][0.28524935 0.46984515 0.62317115 0.72816044 0.81543177 0.87957984 0.8708424 0.7931022 0.70697284 0.65511382 0.62836015 0.59572268 0.56691569 0.55751866 0.54696977][0.18149164 0.31977969 0.4408617 0.52807772 0.60404456 0.66570079 0.6720674 0.62263173 0.56112021 0.52368 0.50252432 0.47207692 0.44256502 0.42886829 0.41590515][0.066774376 0.15268345 0.23362988 0.29434821 0.34996414 0.40042183 0.41551667 0.38981459 0.34956747 0.3230904 0.30499277 0.27738759 0.25050232 0.23675017 0.22642924][-0.029155619 0.011619864 0.055050995 0.088330828 0.12090556 0.15474132 0.16927771 0.15689264 0.13112146 0.11252137 0.098158114 0.077072434 0.057341151 0.047451787 0.042333558][-0.089325696 -0.08016248 -0.064880475 -0.053297792 -0.040312663 -0.023077825 -0.014385083 -0.021264182 -0.038058322 -0.050741006 -0.05995059 -0.071549609 -0.081006624 -0.084209606 -0.083039142][-0.11364374 -0.12241767 -0.12480094 -0.12722717 -0.12757879 -0.12337753 -0.12137391 -0.12648624 -0.13690679 -0.1445497 -0.14858899 -0.15169668 -0.15226935 -0.14925188 -0.14315011][-0.11048682 -0.1257256 -0.13571836 -0.14422913 -0.1506684 -0.1537703 -0.15647361 -0.161084 -0.1670195 -0.17075197 -0.17133796 -0.16946775 -0.16480541 -0.1578922 -0.14911991]]...]
INFO - root - 2017-12-10 22:55:21.088606: step 61510, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 58h:05m:09s remains)
INFO - root - 2017-12-10 22:55:28.971633: step 61520, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 59h:31m:28s remains)
INFO - root - 2017-12-10 22:55:36.925690: step 61530, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 58h:25m:09s remains)
INFO - root - 2017-12-10 22:55:44.823858: step 61540, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 58h:47m:42s remains)
INFO - root - 2017-12-10 22:55:52.548726: step 61550, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.788 sec/batch; 59h:20m:07s remains)
INFO - root - 2017-12-10 22:56:00.501041: step 61560, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.812 sec/batch; 61h:08m:37s remains)
INFO - root - 2017-12-10 22:56:08.260058: step 61570, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 57h:15m:48s remains)
INFO - root - 2017-12-10 22:56:16.140466: step 61580, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 58h:41m:51s remains)
INFO - root - 2017-12-10 22:56:23.948613: step 61590, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.819 sec/batch; 61h:37m:38s remains)
INFO - root - 2017-12-10 22:56:31.852029: step 61600, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 58h:59m:40s remains)
2017-12-10 22:56:32.701290: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.030224714 -0.025661092 -0.061221134 -0.076592609 -0.078462519 -0.071710087 -0.063385472 -0.057750896 -0.053225476 -0.044765983 -0.025913494 0.0028803865 0.040365938 0.083517909 0.12954547][0.0052447324 -0.043214213 -0.073809594 -0.088372692 -0.090788841 -0.083754644 -0.07480675 -0.068793274 -0.065322287 -0.059852742 -0.046186041 -0.02428747 0.0053238203 0.039944906 0.078175329][-0.020334611 -0.046501841 -0.059637927 -0.0648754 -0.063831642 -0.057360917 -0.051550012 -0.050014656 -0.052375618 -0.054452382 -0.051272638 -0.042665742 -0.02886091 -0.011193943 0.01114055][-0.025193585 -0.021469986 -0.011183791 -0.0029237324 0.0029638703 0.0072731269 0.0066686925 2.6262762e-05 -0.011252133 -0.023105746 -0.031996526 -0.037922096 -0.041398179 -0.042469956 -0.038403612][0.0023972932 0.035174612 0.067814641 0.089331344 0.099695578 0.10059237 0.09180256 0.0750781 0.053734522 0.032088552 0.012185352 -0.0063560987 -0.024472075 -0.041279987 -0.053045481][0.056579486 0.11079169 0.15933259 0.1906186 0.20421137 0.20167448 0.18512525 0.15864269 0.12842068 0.099478319 0.071990252 0.044923194 0.016772008 -0.01088678 -0.033944648][0.11301466 0.17746468 0.23245507 0.26765767 0.28247827 0.2776683 0.25600424 0.2227744 0.18747807 0.15595497 0.12645644 0.096743263 0.064661287 0.032364469 0.0039528543][0.13741426 0.20014137 0.25186941 0.28447214 0.29801735 0.29202807 0.26864213 0.23381567 0.19925572 0.17099041 0.14568885 0.12022422 0.091755 0.061954182 0.0349251][0.11334609 0.16436477 0.20553234 0.23094954 0.24160264 0.23570667 0.21413432 0.18299979 0.15444477 0.13359733 0.1159242 0.098460183 0.078370139 0.056438118 0.036408562][0.054109666 0.087048531 0.11349088 0.12931739 0.13617913 0.13148108 0.1149679 0.091942817 0.072804481 0.060995556 0.051630545 0.0428168 0.032536674 0.020849528 0.010628005][-0.0099587 0.0040286565 0.015572335 0.021814538 0.024637235 0.021425301 0.011152687 -0.0024525628 -0.01217462 -0.016267586 -0.018724626 -0.019909075 -0.020968886 -0.022667633 -0.023371633][-0.053790037 -0.052984681 -0.051295415 -0.051037028 -0.050724778 -0.052512117 -0.05790941 -0.064783789 -0.068928942 -0.069483891 -0.068657093 -0.065409243 -0.060391691 -0.05539991 -0.050040744][-0.0687468 -0.072213009 -0.072869509 -0.0732083 -0.072638109 -0.072745718 -0.075290211 -0.079352625 -0.0825519 -0.08398401 -0.083877407 -0.08018671 -0.073574454 -0.066876486 -0.060579397][-0.059965786 -0.060591787 -0.058087394 -0.0553968 -0.052671257 -0.050957721 -0.052191403 -0.056065615 -0.060793933 -0.064989544 -0.067439206 -0.065638341 -0.060667668 -0.056169506 -0.052939173][-0.043106962 -0.038577929 -0.031728037 -0.025757328 -0.020912038 -0.01784296 -0.018787881 -0.023433257 -0.029721757 -0.03563929 -0.039426912 -0.038774308 -0.035427645 -0.033681031 -0.034233227]]...]
INFO - root - 2017-12-10 22:56:40.639519: step 61610, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 60h:53m:08s remains)
INFO - root - 2017-12-10 22:56:48.449686: step 61620, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 58h:37m:40s remains)
INFO - root - 2017-12-10 22:56:56.298629: step 61630, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 59h:42m:56s remains)
INFO - root - 2017-12-10 22:57:03.971028: step 61640, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 57h:13m:21s remains)
INFO - root - 2017-12-10 22:57:11.817888: step 61650, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 60h:28m:00s remains)
INFO - root - 2017-12-10 22:57:19.800056: step 61660, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 59h:54m:19s remains)
INFO - root - 2017-12-10 22:57:27.347138: step 61670, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 57h:22m:59s remains)
INFO - root - 2017-12-10 22:57:35.197878: step 61680, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 59h:28m:16s remains)
INFO - root - 2017-12-10 22:57:43.070734: step 61690, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 60h:05m:18s remains)
INFO - root - 2017-12-10 22:57:50.952367: step 61700, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 59h:09m:30s remains)
2017-12-10 22:57:51.902385: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23415138 0.32650715 0.42562696 0.50898284 0.56260711 0.57539755 0.54485929 0.47944003 0.40539595 0.34328634 0.29361641 0.26746547 0.27049649 0.29140243 0.31902841][0.27475369 0.36177757 0.455032 0.53853577 0.60022974 0.62727821 0.61360973 0.56558782 0.50585181 0.4542695 0.40878707 0.38108614 0.38339114 0.40818843 0.44310829][0.30859876 0.37937605 0.45592475 0.53036427 0.59206092 0.626804 0.62624544 0.59509516 0.55135661 0.51242912 0.47378653 0.44681233 0.44997436 0.48003969 0.5241394][0.34259552 0.39492574 0.45337865 0.51719528 0.57460403 0.6105482 0.61560512 0.5930056 0.55656964 0.52184379 0.48480222 0.45667538 0.45960507 0.49458867 0.54874343][0.3770161 0.41224229 0.4551017 0.5098905 0.56160659 0.59488875 0.59984142 0.57750833 0.53800917 0.49633375 0.45273763 0.41840157 0.41697207 0.45375526 0.51500696][0.4103685 0.428233 0.45543912 0.50107461 0.54764664 0.57954681 0.58572072 0.56207132 0.515158 0.45990604 0.40362233 0.35845912 0.34755939 0.37981954 0.44118267][0.43955305 0.43596077 0.44463673 0.47983319 0.52374941 0.55977106 0.57501125 0.55739826 0.50783789 0.44013271 0.36880875 0.30887622 0.28203467 0.30121815 0.35371906][0.44825166 0.42183706 0.41101637 0.43477419 0.47813368 0.52446193 0.55892545 0.559773 0.519352 0.44807661 0.36560246 0.29012838 0.24282932 0.24172552 0.27779838][0.41832638 0.37248391 0.34462059 0.35687417 0.39992568 0.45922464 0.51899111 0.54832876 0.52900553 0.46557447 0.37956434 0.29194558 0.22500364 0.20150276 0.21745372][0.34940502 0.29600212 0.26154232 0.26755232 0.31123146 0.38128042 0.46268168 0.51854616 0.52179325 0.47103092 0.38692728 0.29250202 0.2117798 0.17046353 0.16969292][0.25764552 0.21298105 0.18662556 0.19603965 0.24232592 0.31650984 0.40635112 0.47513694 0.49216452 0.45205563 0.37315527 0.27987167 0.1961837 0.14809406 0.14033972][0.16824292 0.14504792 0.13872541 0.16019903 0.21019921 0.27940059 0.35953382 0.42076704 0.43619448 0.40019327 0.32824495 0.24443437 0.17012385 0.12896457 0.12607622][0.10382606 0.1077674 0.12632208 0.16276911 0.21401881 0.26903945 0.32445136 0.36175781 0.36339512 0.32614309 0.26257357 0.19429307 0.13777481 0.11236647 0.12111554][0.065189317 0.092491038 0.13177449 0.17904404 0.22635934 0.26141408 0.28572926 0.2934925 0.2782135 0.23974641 0.18729435 0.13813815 0.1026771 0.095314533 0.11619668][0.035960127 0.076319277 0.12525102 0.17352553 0.21078329 0.22550063 0.22315702 0.20829637 0.18323228 0.14932638 0.11160499 0.081984088 0.0654078 0.0720572 0.10013528]]...]
INFO - root - 2017-12-10 22:57:59.979332: step 61710, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 59h:55m:20s remains)
INFO - root - 2017-12-10 22:58:07.861696: step 61720, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.769 sec/batch; 57h:50m:25s remains)
INFO - root - 2017-12-10 22:58:15.537953: step 61730, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 58h:35m:54s remains)
INFO - root - 2017-12-10 22:58:23.403619: step 61740, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.757 sec/batch; 56h:53m:57s remains)
INFO - root - 2017-12-10 22:58:31.059706: step 61750, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 60h:00m:34s remains)
INFO - root - 2017-12-10 22:58:38.997573: step 61760, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.818 sec/batch; 61h:29m:56s remains)
INFO - root - 2017-12-10 22:58:46.953426: step 61770, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 58h:27m:14s remains)
INFO - root - 2017-12-10 22:58:54.864445: step 61780, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 60h:32m:38s remains)
INFO - root - 2017-12-10 22:59:02.705563: step 61790, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 59h:04m:32s remains)
INFO - root - 2017-12-10 22:59:10.569783: step 61800, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 60h:19m:17s remains)
2017-12-10 22:59:11.396581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.019905824 -0.024606187 -0.028899053 -0.033097513 -0.036703371 -0.038802546 -0.039667644 -0.040475931 -0.041413844 -0.042285044 -0.042549912 -0.042141523 -0.040705651 -0.038753584 -0.037867915][-0.025636122 -0.031480834 -0.036689389 -0.041429833 -0.045058265 -0.046419308 -0.046166439 -0.046167251 -0.047249988 -0.049274705 -0.051274307 -0.052596368 -0.052389517 -0.050777208 -0.048938837][-0.029030075 -0.03539918 -0.040125582 -0.043002538 -0.04333555 -0.040127952 -0.034926254 -0.031022321 -0.030942934 -0.034948077 -0.041300271 -0.047923669 -0.052701026 -0.054644551 -0.054107454][-0.026977155 -0.03270692 -0.035168953 -0.033429589 -0.026656553 -0.014342637 0.00072015956 0.012966392 0.016776372 0.010593237 -0.002898186 -0.019379582 -0.034447838 -0.045106452 -0.050197564][-0.0176387 -0.021407356 -0.020198805 -0.011758216 0.0051624156 0.030532541 0.060065638 0.085121512 0.095693022 0.087659523 0.064678572 0.034064211 0.0035814147 -0.02088733 -0.036157694][-0.0029308703 -0.0035278418 0.0022132988 0.018236496 0.04634108 0.085892335 0.13100578 0.16996951 0.18817411 0.17834285 0.14506452 0.09898752 0.051265255 0.01085941 -0.016654484][0.013087765 0.016403176 0.026562624 0.048917361 0.085688181 0.13550872 0.19122931 0.23942083 0.26256016 0.25099859 0.20976165 0.15208128 0.091562323 0.039253972 0.0021305352][0.024497267 0.031633575 0.045156226 0.07072968 0.11049541 0.16199689 0.21762514 0.26487631 0.28691694 0.27366158 0.22977974 0.16934204 0.10638311 0.051883496 0.012490563][0.026013585 0.035811063 0.050854955 0.0758975 0.11230583 0.15611553 0.20010521 0.23524772 0.24925531 0.23391542 0.19326557 0.14024682 0.086900592 0.041569434 0.0088121267][0.015619035 0.026081452 0.040884063 0.062972732 0.092265919 0.12311476 0.14922079 0.16616276 0.16833696 0.15095344 0.11770824 0.0790627 0.043541126 0.015211278 -0.004741604][-0.0018783741 0.0069504809 0.019789131 0.037712324 0.059176739 0.077328511 0.086907662 0.087521568 0.07978417 0.062368732 0.038724087 0.016502263 0.00043255903 -0.0097509455 -0.016232982][-0.019623265 -0.014196633 -0.0048749326 0.0081934724 0.022860831 0.032319106 0.032324273 0.024653016 0.013369191 -0.00085690524 -0.014869986 -0.023086434 -0.023576772 -0.019709894 -0.016186226][-0.032771241 -0.031228766 -0.025936525 -0.017625095 -0.0080237351 -0.0026687076 -0.0047038705 -0.012011698 -0.020319868 -0.028799886 -0.034607414 -0.033212 -0.024212189 -0.012402959 -0.0036431972][-0.039536051 -0.04101371 -0.039056476 -0.034721263 -0.02875817 -0.024782216 -0.024689917 -0.026941255 -0.029031174 -0.031146422 -0.030741742 -0.02378758 -0.010544634 0.0040636933 0.01426954][-0.041978233 -0.044914838 -0.044727016 -0.04291489 -0.039359141 -0.035856958 -0.033121254 -0.030294405 -0.026938029 -0.02441429 -0.020456877 -0.011305132 0.0026991169 0.01713001 0.026960649]]...]
INFO - root - 2017-12-10 22:59:19.256561: step 61810, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 57h:45m:01s remains)
INFO - root - 2017-12-10 22:59:27.052342: step 61820, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 57h:27m:20s remains)
INFO - root - 2017-12-10 22:59:34.696444: step 61830, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 58h:36m:56s remains)
INFO - root - 2017-12-10 22:59:42.501700: step 61840, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.773 sec/batch; 58h:05m:24s remains)
INFO - root - 2017-12-10 22:59:50.427264: step 61850, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.795 sec/batch; 59h:44m:16s remains)
INFO - root - 2017-12-10 22:59:58.328058: step 61860, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 57h:25m:57s remains)
INFO - root - 2017-12-10 23:00:06.150109: step 61870, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 57h:53m:37s remains)
INFO - root - 2017-12-10 23:00:14.130498: step 61880, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 58h:59m:14s remains)
INFO - root - 2017-12-10 23:00:22.052634: step 61890, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 60h:38m:53s remains)
INFO - root - 2017-12-10 23:00:30.020453: step 61900, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.812 sec/batch; 61h:00m:30s remains)
2017-12-10 23:00:30.875280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0059387288 -0.0092896735 0.017374378 0.0786052 0.16232555 0.2435226 0.29305139 0.29503661 0.24768248 0.16337155 0.070099548 -0.00703562 -0.05422527 -0.07444983 -0.075739317][-0.010013962 -0.0075095771 0.028937157 0.10183416 0.19478953 0.2807999 0.33023751 0.32698688 0.27129763 0.1784925 0.0787161 -0.0031358681 -0.053715978 -0.076022513 -0.078044683][-0.005721272 0.00057368091 0.041852627 0.1201037 0.21719569 0.30631739 0.35786989 0.35478798 0.2965177 0.19864176 0.092659809 0.0047442666 -0.050410479 -0.075379252 -0.0782912][0.0049403766 0.013011462 0.055605717 0.13553801 0.23504594 0.32829586 0.38490522 0.3854436 0.32661122 0.22292122 0.1080457 0.011684148 -0.048772216 -0.075752415 -0.07870429][0.020618172 0.030169638 0.072281927 0.15154898 0.25159398 0.34741572 0.4071565 0.40898719 0.34709176 0.23602983 0.11230372 0.0095068533 -0.053235766 -0.079552747 -0.080833696][0.040831264 0.050655533 0.090076841 0.16530375 0.26184553 0.35544369 0.41351542 0.41294107 0.34671947 0.23031996 0.10222826 -0.0018294603 -0.062839225 -0.086178228 -0.084612407][0.068931736 0.077331789 0.11019436 0.17591403 0.26229346 0.34629449 0.39618954 0.3897239 0.32040474 0.20414177 0.078981042 -0.019938257 -0.07517197 -0.093654647 -0.088576958][0.10992005 0.11384155 0.13682742 0.18881294 0.25979096 0.32836267 0.36512172 0.35030735 0.27919117 0.16711472 0.049362589 -0.041047502 -0.088544145 -0.10108113 -0.092199154][0.15846582 0.15650763 0.16822703 0.20427358 0.25706515 0.30726233 0.32955652 0.30756611 0.23739989 0.13194169 0.022953516 -0.0586435 -0.098879039 -0.1063515 -0.094562873][0.20583427 0.19823906 0.1985825 0.22002742 0.25827652 0.29570958 0.31051081 0.28805241 0.22225893 0.12211147 0.017262124 -0.061221931 -0.099314705 -0.10588108 -0.0941445][0.24258815 0.23250645 0.22480965 0.23568995 0.2649104 0.29657355 0.31070384 0.29274184 0.23278302 0.13566366 0.030168775 -0.050715625 -0.091409534 -0.10067283 -0.091343239][0.26690447 0.26010302 0.2497216 0.25440267 0.27734369 0.30453187 0.31733435 0.30163723 0.24482043 0.14898208 0.042207 -0.041144375 -0.084364995 -0.096129149 -0.088802882][0.27476522 0.27481228 0.26731631 0.27164266 0.29225454 0.31644857 0.32648706 0.30946925 0.25230259 0.15628126 0.048874296 -0.035595194 -0.080341488 -0.093712285 -0.087529883][0.26882759 0.27542183 0.27216783 0.27763382 0.29641169 0.31708819 0.32305387 0.3033489 0.24609791 0.15270372 0.048845779 -0.033359587 -0.077887923 -0.092021465 -0.086566962][0.25646937 0.2670773 0.26649177 0.27184707 0.28722113 0.30282265 0.30363947 0.28078982 0.2245035 0.13720219 0.041681163 -0.033972677 -0.075606279 -0.089354284 -0.084696382]]...]
INFO - root - 2017-12-10 23:00:38.478502: step 61910, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 59h:37m:49s remains)
INFO - root - 2017-12-10 23:00:46.552024: step 61920, loss = 0.70, batch loss = 0.64 (8.7 examples/sec; 0.923 sec/batch; 69h:22m:12s remains)
INFO - root - 2017-12-10 23:00:54.381119: step 61930, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 60h:18m:06s remains)
INFO - root - 2017-12-10 23:01:02.320086: step 61940, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 59h:50m:36s remains)
INFO - root - 2017-12-10 23:01:10.499643: step 61950, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.808 sec/batch; 60h:42m:18s remains)
INFO - root - 2017-12-10 23:01:18.354433: step 61960, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 61h:09m:44s remains)
INFO - root - 2017-12-10 23:01:26.196651: step 61970, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 58h:33m:30s remains)
INFO - root - 2017-12-10 23:01:34.009816: step 61980, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 59h:46m:28s remains)
INFO - root - 2017-12-10 23:01:41.732214: step 61990, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 59h:59m:18s remains)
INFO - root - 2017-12-10 23:01:49.405956: step 62000, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 59h:52m:46s remains)
2017-12-10 23:01:50.184941: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.245662 0.23122923 0.20594056 0.17465614 0.15157323 0.1451342 0.1561055 0.1733 0.17719123 0.1473216 0.085662223 0.018477665 -0.034429267 -0.063201047 -0.070979908][0.27204916 0.26889691 0.25261542 0.2299564 0.21556133 0.21704185 0.23402673 0.25265536 0.25005344 0.20492265 0.12353917 0.040419355 -0.022925343 -0.0571709 -0.068523489][0.28812978 0.2924034 0.28106359 0.26662865 0.26360694 0.27609664 0.30084366 0.32137904 0.31274354 0.2546145 0.15693077 0.060235027 -0.011798996 -0.050261375 -0.064264312][0.30072346 0.30540827 0.29192427 0.28085139 0.28750882 0.31106672 0.34460646 0.36911103 0.35814264 0.29273972 0.18441691 0.076953776 -0.0034086648 -0.046254519 -0.062103245][0.30174738 0.302494 0.28338313 0.27342823 0.28890449 0.32441896 0.36835307 0.39863649 0.38778946 0.31892148 0.20449613 0.089186557 0.0018616716 -0.045052178 -0.062261861][0.28500393 0.28172606 0.25862247 0.25045177 0.27379078 0.31953713 0.37168738 0.40563357 0.39385641 0.32276765 0.20610543 0.087437078 -0.0026984217 -0.050650585 -0.066682033][0.25877607 0.25454742 0.23171248 0.22649504 0.25497249 0.30565602 0.3597962 0.39209688 0.37694344 0.30494794 0.19049549 0.073591724 -0.014887158 -0.0608257 -0.073735923][0.23014519 0.22844975 0.21025777 0.20945999 0.241477 0.29353046 0.34560433 0.37272781 0.35263881 0.28084382 0.17133421 0.059009712 -0.0260785 -0.070140816 -0.080783375][0.20158978 0.20497023 0.19435224 0.1990303 0.23348905 0.28543776 0.33528674 0.35877302 0.33610383 0.26676327 0.16314876 0.055216003 -0.02793677 -0.072307512 -0.083502024][0.18094136 0.190396 0.18658467 0.19453694 0.22836643 0.27757576 0.32402784 0.34464389 0.32176074 0.257001 0.16045289 0.057433408 -0.023690943 -0.06862089 -0.08169698][0.17748077 0.19334823 0.19522874 0.20599778 0.23923376 0.28432423 0.32332382 0.33540621 0.30840114 0.24669492 0.15756223 0.060382806 -0.018329998 -0.06324625 -0.077769749][0.19663522 0.22175717 0.23034556 0.24436195 0.27626258 0.31480411 0.34193069 0.33972231 0.30365378 0.24192332 0.15846199 0.066432424 -0.01062101 -0.056252588 -0.072706036][0.22868034 0.26183909 0.27406913 0.28831708 0.31710824 0.3487772 0.36461222 0.34810463 0.30183163 0.23806188 0.15823558 0.070749342 -0.0046792529 -0.051090252 -0.069279552][0.2552571 0.29044008 0.30167916 0.31344959 0.33816433 0.36410037 0.37266564 0.34813118 0.29656196 0.23226108 0.1549288 0.070882745 -0.0031202089 -0.049905963 -0.068784527][0.2668207 0.29814592 0.30459982 0.31241998 0.33252698 0.35347074 0.35847968 0.33288643 0.28270724 0.2214347 0.14744194 0.06619513 -0.0067436984 -0.053457644 -0.071558714]]...]
INFO - root - 2017-12-10 23:01:58.076847: step 62010, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 58h:53m:43s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 23:02:06.014877: step 62020, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 59h:16m:03s remains)
INFO - root - 2017-12-10 23:02:13.830043: step 62030, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.787 sec/batch; 59h:06m:45s remains)
INFO - root - 2017-12-10 23:02:21.751582: step 62040, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 59h:50m:59s remains)
INFO - root - 2017-12-10 23:02:29.620502: step 62050, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 58h:11m:35s remains)
INFO - root - 2017-12-10 23:02:37.402241: step 62060, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 58h:21m:48s remains)
INFO - root - 2017-12-10 23:02:45.138481: step 62070, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.798 sec/batch; 59h:56m:24s remains)
INFO - root - 2017-12-10 23:02:52.729701: step 62080, loss = 0.69, batch loss = 0.64 (12.9 examples/sec; 0.622 sec/batch; 46h:42m:21s remains)
INFO - root - 2017-12-10 23:03:00.580861: step 62090, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 57h:26m:07s remains)
INFO - root - 2017-12-10 23:03:08.429009: step 62100, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 60h:09m:55s remains)
2017-12-10 23:03:09.312829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.033869311 -0.033274729 -0.032908767 -0.0324034 -0.031962685 -0.03195671 -0.032182742 -0.0323637 -0.032373294 -0.032352496 -0.032566331 -0.032778841 -0.032858845 -0.032732621 -0.032543968][-0.032780454 -0.031526234 -0.030202802 -0.028627962 -0.027357912 -0.027474334 -0.028650822 -0.029864676 -0.030478967 -0.0306807 -0.031624369 -0.032744963 -0.033257842 -0.033232991 -0.033267375][-0.021736413 -0.0182567 -0.015197266 -0.01196132 -0.0094849244 -0.0099998163 -0.013158067 -0.016837075 -0.019249268 -0.020234434 -0.022259828 -0.02465382 -0.025763415 -0.026005358 -0.026732452][-0.00031561949 0.0070105982 0.012655429 0.018373543 0.022676928 0.021994134 0.016698144 0.010108459 0.0052273688 0.0026435123 -0.0010554442 -0.0047969371 -0.005601733 -0.0048921853 -0.0069406871][0.030991968 0.044045821 0.053607415 0.063428476 0.070879012 0.07062111 0.062881239 0.052877169 0.045408 0.041172512 0.035620388 0.030499501 0.030672913 0.033301141 0.02851541][0.070627347 0.0907763 0.10477312 0.11845216 0.12786977 0.12652639 0.11503756 0.10152214 0.093040474 0.089412525 0.084077 0.078920208 0.080819353 0.0854614 0.075753383][0.1115104 0.13829361 0.15614097 0.17182852 0.18095683 0.17781147 0.16358939 0.14797592 0.13984773 0.13787794 0.13401493 0.1293626 0.13196559 0.13707209 0.12136739][0.14147116 0.17423482 0.19579196 0.21233094 0.21982118 0.21588139 0.20315401 0.18974897 0.18332486 0.18108073 0.17607149 0.16953643 0.16982469 0.17273863 0.15130897][0.15492851 0.19155537 0.21766648 0.23608354 0.242405 0.23855732 0.23073579 0.2234661 0.2194795 0.21381302 0.2028833 0.1906406 0.1847899 0.18240303 0.15620857][0.15836106 0.19694619 0.22807349 0.24966121 0.25524482 0.24996339 0.24493766 0.24159382 0.23668522 0.22368085 0.20357411 0.18385711 0.17043062 0.16130027 0.13251507][0.15910819 0.19880036 0.23481321 0.26106867 0.26785064 0.26009619 0.25222823 0.2449352 0.23178664 0.20664591 0.1749403 0.14716938 0.12709077 0.11325779 0.086599357][0.159728 0.20071018 0.24136552 0.2743915 0.28718841 0.2796112 0.26405844 0.24266768 0.21130693 0.16862 0.12409706 0.089258976 0.0653486 0.050958317 0.031648748][0.16356312 0.20690858 0.25123751 0.29041755 0.31064823 0.30506355 0.28033009 0.2403466 0.18656734 0.12562963 0.070258059 0.031111963 0.006822411 -0.0046880078 -0.014584599][0.1739036 0.22012718 0.26485482 0.30416149 0.32577443 0.31894922 0.28519595 0.23027164 0.16071613 0.089699805 0.030250577 -0.0090125054 -0.030949006 -0.038379043 -0.040472027][0.18248917 0.22928205 0.26889402 0.30002588 0.31424925 0.30131233 0.25987649 0.1977531 0.12528543 0.057836946 0.0051019215 -0.02783574 -0.044152681 -0.047111303 -0.044662744]]...]
INFO - root - 2017-12-10 23:03:17.128778: step 62110, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 57h:10m:33s remains)
INFO - root - 2017-12-10 23:03:24.961245: step 62120, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 59h:30m:50s remains)
INFO - root - 2017-12-10 23:03:32.860330: step 62130, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 58h:52m:01s remains)
INFO - root - 2017-12-10 23:03:40.688652: step 62140, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 58h:40m:47s remains)
INFO - root - 2017-12-10 23:03:48.453002: step 62150, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 59h:38m:41s remains)
INFO - root - 2017-12-10 23:03:56.372106: step 62160, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 57h:26m:28s remains)
INFO - root - 2017-12-10 23:04:04.180408: step 62170, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.789 sec/batch; 59h:14m:32s remains)
INFO - root - 2017-12-10 23:04:12.024268: step 62180, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 59h:58m:18s remains)
INFO - root - 2017-12-10 23:04:19.932530: step 62190, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 59h:33m:22s remains)
INFO - root - 2017-12-10 23:04:27.737830: step 62200, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 57h:48m:28s remains)
2017-12-10 23:04:28.573715: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.074003592 0.090124942 0.11318758 0.14151579 0.17200676 0.19943941 0.21564817 0.21199785 0.18807016 0.15335475 0.12079294 0.099767014 0.090289772 0.0896578 0.092978686][0.063400753 0.082813792 0.11020788 0.14155447 0.17315927 0.20042145 0.21724659 0.21670742 0.19788967 0.16892168 0.1412569 0.12273791 0.11368514 0.11218191 0.11392491][0.052200418 0.072885215 0.10104974 0.13128613 0.15947853 0.18272009 0.19806518 0.20116472 0.19054292 0.17181891 0.15318531 0.14020039 0.13384989 0.13299936 0.13415788][0.050248522 0.069570348 0.094031125 0.11901473 0.1407678 0.15808539 0.17057019 0.17607023 0.17286451 0.16435276 0.15537277 0.14913277 0.14729422 0.14929722 0.15225698][0.054969884 0.071433291 0.090514623 0.10954941 0.12561838 0.13835807 0.14826645 0.15424904 0.15491089 0.15218535 0.14915733 0.14793524 0.15078501 0.15703574 0.16350482][0.062292915 0.075971723 0.090547442 0.1052121 0.11774511 0.12804542 0.1364343 0.14192025 0.14306958 0.14098723 0.13884729 0.13975491 0.14655919 0.15762427 0.16836369][0.066815294 0.0791634 0.091951579 0.10544703 0.11746099 0.12785316 0.13640675 0.14108051 0.13981901 0.13378128 0.1278436 0.12751108 0.13613658 0.15100248 0.16551185][0.066865847 0.078244753 0.090703696 0.10466569 0.11745978 0.12874897 0.13790259 0.14184389 0.13747631 0.12628043 0.1152844 0.11225577 0.1206771 0.13667569 0.15237767][0.062701963 0.072310418 0.084165536 0.098725833 0.11277462 0.12543675 0.13542771 0.13902539 0.13252665 0.11786458 0.10336898 0.097636923 0.10398851 0.11792275 0.13141069][0.053926464 0.061224777 0.072444625 0.087847963 0.1034046 0.11720742 0.12728032 0.13044709 0.12352624 0.10844868 0.0929765 0.084960587 0.087375581 0.096145943 0.10428495][0.044276476 0.048426487 0.057620365 0.0718894 0.086428061 0.098694719 0.10691216 0.10975888 0.10536211 0.094652884 0.082404852 0.074309126 0.072899289 0.075600572 0.077611826][0.035288207 0.036217265 0.042433184 0.053870663 0.065172017 0.073583774 0.07831537 0.080406606 0.079618886 0.075589687 0.069278412 0.0632857 0.059373043 0.057021294 0.054358628][0.027575586 0.026582105 0.030783955 0.039387591 0.046915729 0.050706483 0.051108353 0.051197276 0.052535981 0.05413508 0.053750057 0.050609093 0.04525847 0.039134912 0.033389337][0.019386731 0.018744744 0.022946743 0.030298447 0.035613745 0.036301874 0.033095073 0.030235132 0.031044066 0.035125904 0.038692772 0.038186345 0.0326031 0.024288895 0.016772212][0.010902533 0.011910374 0.017758422 0.025705682 0.030943509 0.030692948 0.025367448 0.019529775 0.017923633 0.021597933 0.026948582 0.028854225 0.024559345 0.016021905 0.0078200223]]...]
INFO - root - 2017-12-10 23:04:36.535404: step 62210, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 58h:12m:27s remains)
INFO - root - 2017-12-10 23:04:44.300480: step 62220, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 58h:36m:21s remains)
INFO - root - 2017-12-10 23:04:51.980910: step 62230, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 60h:22m:07s remains)
INFO - root - 2017-12-10 23:05:00.035306: step 62240, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 59h:23m:38s remains)
INFO - root - 2017-12-10 23:05:07.819772: step 62250, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 58h:04m:13s remains)
INFO - root - 2017-12-10 23:05:15.454364: step 62260, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.789 sec/batch; 59h:13m:39s remains)
INFO - root - 2017-12-10 23:05:23.286307: step 62270, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.748 sec/batch; 56h:08m:44s remains)
INFO - root - 2017-12-10 23:05:31.086587: step 62280, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 58h:52m:33s remains)
INFO - root - 2017-12-10 23:05:39.037630: step 62290, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 59h:12m:35s remains)
INFO - root - 2017-12-10 23:05:46.868823: step 62300, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 60h:17m:37s remains)
2017-12-10 23:05:47.652435: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033734962 0.033302564 0.039819695 0.054971814 0.069157884 0.074388891 0.070653327 0.060471967 0.043910835 0.023079854 0.0037699873 -0.0089572025 -0.01730202 -0.024193889 -0.02862305][0.043663848 0.049008217 0.060661424 0.080388919 0.097638614 0.10391474 0.099594243 0.088690996 0.071025245 0.04705115 0.022629438 0.0055801072 -0.0033310778 -0.00770751 -0.0085172392][0.05849411 0.070768364 0.089706853 0.11790358 0.14396223 0.15810795 0.15875328 0.14985387 0.13078909 0.10041536 0.0645472 0.035782259 0.020239519 0.015256097 0.018045025][0.077014878 0.098869041 0.12863196 0.17031163 0.2122665 0.24242179 0.25376835 0.24897768 0.22691792 0.18505558 0.13059525 0.081792839 0.051737584 0.041681975 0.047477882][0.094959646 0.12821423 0.17096248 0.22847658 0.29022723 0.3419638 0.36965883 0.3722766 0.3477262 0.29408425 0.22013313 0.14803968 0.097978324 0.077561177 0.083381288][0.11635852 0.16018575 0.21528675 0.28663033 0.36636192 0.43989161 0.48641244 0.49818435 0.47078288 0.40577289 0.31544727 0.22242585 0.15090661 0.11700229 0.12057786][0.14672364 0.19822291 0.26191074 0.34059054 0.43051797 0.51937151 0.5816713 0.6011309 0.56934083 0.49323463 0.38943556 0.27911872 0.18794702 0.14004996 0.14062279][0.17874958 0.23406154 0.30053824 0.3778899 0.4668279 0.55940586 0.62883782 0.65196407 0.61624265 0.53260368 0.42054647 0.29881731 0.19310157 0.13360828 0.13076155][0.19732817 0.24885868 0.31076968 0.37944826 0.4586083 0.54443914 0.61243862 0.63609236 0.59957319 0.51485866 0.40227729 0.27860677 0.1688004 0.10463871 0.098996051][0.20198072 0.24170239 0.29144937 0.34681478 0.41350326 0.48981416 0.55422407 0.57931131 0.54695833 0.46758744 0.36092755 0.24292786 0.1371664 0.072635114 0.062202435][0.19851102 0.22157329 0.25387919 0.29451561 0.35108125 0.42125073 0.48470917 0.51431781 0.49043295 0.42055863 0.32329249 0.21499792 0.11723804 0.053533375 0.035853706][0.19216907 0.19982895 0.21592207 0.24592082 0.29867285 0.36878413 0.43411154 0.46891275 0.45446128 0.395422 0.3083829 0.21014316 0.12025955 0.056372575 0.029439393][0.17925748 0.17672032 0.182998 0.20810656 0.26077405 0.33208498 0.39801842 0.43610924 0.43138149 0.38607979 0.31344891 0.22925641 0.15006974 0.088125356 0.052774318][0.16037615 0.15419564 0.15752774 0.18033564 0.23037742 0.2970725 0.35715872 0.39335293 0.39620751 0.366262 0.31254408 0.24892886 0.18748441 0.1350365 0.0984522][0.13846008 0.13659371 0.14347085 0.16446532 0.20568766 0.25849757 0.30450046 0.33311638 0.340928 0.32671303 0.2944586 0.25489134 0.21458256 0.17611969 0.14474328]]...]
INFO - root - 2017-12-10 23:05:55.342778: step 62310, loss = 0.71, batch loss = 0.66 (10.1 examples/sec; 0.794 sec/batch; 59h:34m:21s remains)
INFO - root - 2017-12-10 23:06:03.164490: step 62320, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.806 sec/batch; 60h:31m:02s remains)
INFO - root - 2017-12-10 23:06:11.012193: step 62330, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 59h:51m:28s remains)
INFO - root - 2017-12-10 23:06:18.786026: step 62340, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 57h:33m:35s remains)
INFO - root - 2017-12-10 23:06:26.396286: step 62350, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 57h:46m:38s remains)
INFO - root - 2017-12-10 23:06:34.285838: step 62360, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 60h:49m:18s remains)
INFO - root - 2017-12-10 23:06:42.164620: step 62370, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 59h:45m:33s remains)
INFO - root - 2017-12-10 23:06:49.987970: step 62380, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 59h:24m:41s remains)
INFO - root - 2017-12-10 23:06:57.719074: step 62390, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 58h:05m:24s remains)
INFO - root - 2017-12-10 23:07:05.607800: step 62400, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.818 sec/batch; 61h:22m:37s remains)
2017-12-10 23:07:06.417953: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32622576 0.30564195 0.2897501 0.28588372 0.2899991 0.30287349 0.32067251 0.3338224 0.33899403 0.33921584 0.338347 0.33943874 0.34203464 0.34656972 0.35555097][0.32100105 0.30206347 0.28787538 0.29075134 0.30608436 0.33050272 0.3560831 0.37327871 0.3809005 0.38269269 0.38071993 0.37972885 0.37879676 0.37267721 0.36328769][0.29798511 0.2823334 0.27763754 0.30187553 0.34694281 0.39785764 0.439715 0.46347451 0.47068635 0.46630287 0.45121127 0.43409774 0.41657004 0.39146689 0.36157787][0.29079616 0.28136671 0.29237702 0.34583747 0.4295024 0.51369172 0.57525611 0.606586 0.60951966 0.589022 0.54700452 0.49986684 0.45396665 0.40305158 0.35263544][0.31999239 0.32475692 0.35567498 0.43667188 0.55251795 0.66329056 0.74001682 0.77698761 0.77226377 0.727857 0.64878315 0.56141686 0.47940466 0.39900932 0.32975581][0.37753713 0.40170044 0.45079947 0.54643291 0.67275637 0.78925568 0.8689186 0.90871328 0.89981753 0.83753347 0.72856474 0.60731864 0.4935849 0.38614774 0.29851484][0.44226748 0.4855735 0.54630357 0.64125955 0.75624394 0.85767889 0.92802584 0.96742666 0.96009624 0.892893 0.77116323 0.63320965 0.50165582 0.37738502 0.27758771][0.49566782 0.54775649 0.60687894 0.68770278 0.77956665 0.8572852 0.91234308 0.94592446 0.93773746 0.87101823 0.75010711 0.6134308 0.4834975 0.36177528 0.26612642][0.5342561 0.58142418 0.62554485 0.68281519 0.74895048 0.80578214 0.84806639 0.87379676 0.86214536 0.7986424 0.68894434 0.56877726 0.4577913 0.35562181 0.27665347][0.55239516 0.58807755 0.60975057 0.63923436 0.68156248 0.72501838 0.76324338 0.78704959 0.77699935 0.72332352 0.63369215 0.5382688 0.45214155 0.37325439 0.31105152][0.52066433 0.54472995 0.5470171 0.55297315 0.57657516 0.61392432 0.65718788 0.68897074 0.68982255 0.65259987 0.58601356 0.51421344 0.44926578 0.38977623 0.34052888][0.4261499 0.44218972 0.434042 0.425371 0.43510211 0.46806049 0.51835644 0.56442881 0.5852493 0.57159418 0.52938223 0.47687563 0.42620584 0.37995687 0.34012452][0.28247133 0.2939789 0.28426966 0.27015272 0.27063173 0.2979337 0.3512508 0.40996644 0.45187613 0.46347329 0.44557804 0.41037855 0.37110206 0.33500484 0.30310944][0.12279928 0.13172776 0.12674947 0.11479127 0.11072234 0.13054053 0.17863029 0.23883396 0.29137984 0.32148573 0.32516754 0.30760837 0.28032568 0.25298625 0.22708157][-0.0098110428 -0.0029073984 -0.0021037464 -0.0092472387 -0.015155011 -0.004443537 0.029938601 0.077431567 0.12363769 0.15731691 0.17294773 0.16972966 0.15436888 0.13563614 0.11606972]]...]
INFO - root - 2017-12-10 23:07:14.326123: step 62410, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 59h:03m:04s remains)
INFO - root - 2017-12-10 23:07:22.148441: step 62420, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 61h:07m:54s remains)
INFO - root - 2017-12-10 23:07:30.079376: step 62430, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.817 sec/batch; 61h:16m:14s remains)
INFO - root - 2017-12-10 23:07:37.745784: step 62440, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 57h:55m:12s remains)
INFO - root - 2017-12-10 23:07:45.521744: step 62450, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 60h:04m:47s remains)
INFO - root - 2017-12-10 23:07:53.356618: step 62460, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 60h:23m:20s remains)
INFO - root - 2017-12-10 23:08:00.990733: step 62470, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 59h:31m:01s remains)
INFO - root - 2017-12-10 23:08:08.794887: step 62480, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 58h:04m:34s remains)
INFO - root - 2017-12-10 23:08:16.640156: step 62490, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 59h:28m:54s remains)
INFO - root - 2017-12-10 23:08:24.631394: step 62500, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.798 sec/batch; 59h:49m:30s remains)
2017-12-10 23:08:25.472414: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28700462 0.29456338 0.28624994 0.27740338 0.27275971 0.27669606 0.28817195 0.29284129 0.28426564 0.27293804 0.26938197 0.28187233 0.28972629 0.26929983 0.2251265][0.44136393 0.458193 0.44519636 0.42888308 0.41707245 0.41560936 0.42874813 0.43875259 0.4320336 0.414784 0.40196803 0.41151375 0.41845798 0.3919878 0.33834308][0.58051872 0.61502254 0.60004437 0.57252556 0.54549748 0.52930486 0.53561759 0.546967 0.5424456 0.51960933 0.49540982 0.4967739 0.49879605 0.46777853 0.41232741][0.68833315 0.74497575 0.7321558 0.69387078 0.65031809 0.61788774 0.616201 0.63030642 0.63342005 0.60966486 0.57312757 0.55948639 0.54732239 0.506263 0.44585067][0.77090812 0.84517717 0.83125114 0.77978826 0.7203126 0.67371744 0.66630316 0.68720984 0.70413721 0.6844455 0.6370225 0.60411477 0.56998944 0.51025677 0.43641946][0.84152913 0.92312735 0.90096974 0.8331421 0.75895119 0.70151162 0.69018275 0.71901083 0.75103253 0.73774636 0.68431276 0.63544619 0.58011121 0.4992488 0.40732774][0.90886784 0.98630536 0.94844741 0.86140782 0.77281785 0.70650786 0.69081819 0.72399455 0.767597 0.7633388 0.7152611 0.66393644 0.59827322 0.502334 0.39358723][0.957406 1.0191386 0.96182692 0.85741973 0.75733489 0.68488938 0.66549957 0.70017564 0.75246519 0.76183277 0.7317912 0.69382805 0.63059628 0.52775812 0.40539292][0.94961894 0.98844457 0.91413724 0.7982018 0.68931735 0.61116773 0.58756095 0.62215364 0.68161976 0.70824957 0.70504117 0.69283593 0.64456064 0.5459469 0.4188903][0.85246027 0.86787885 0.78480613 0.666277 0.5523687 0.46753532 0.43611097 0.46465993 0.52416247 0.56389213 0.58723307 0.60514629 0.58300054 0.50438195 0.39149889][0.66418 0.65919739 0.57612783 0.46469924 0.35418496 0.26886636 0.23174179 0.24967682 0.29994145 0.34332451 0.38444978 0.42687702 0.43358675 0.38657564 0.30463931][0.41446623 0.3962585 0.3228088 0.22848417 0.13300379 0.058502145 0.022741441 0.029284647 0.063162856 0.099742912 0.14538459 0.199213 0.22717381 0.21242718 0.16758084][0.1648683 0.1414011 0.085086279 0.015858734 -0.0539325 -0.1065905 -0.13231087 -0.13243696 -0.11506601 -0.091403045 -0.053912912 -0.0042840708 0.030753667 0.038047936 0.023771998][-0.028490454 -0.051477987 -0.088692494 -0.13125068 -0.17165315 -0.19862008 -0.21013726 -0.21075128 -0.20364527 -0.1913712 -0.16749878 -0.13240345 -0.10310191 -0.088065252 -0.085619494][-0.14223893 -0.16158317 -0.18222545 -0.20285001 -0.21925117 -0.22636531 -0.22671059 -0.224898 -0.22160868 -0.21626562 -0.20519686 -0.18721199 -0.16996567 -0.15777418 -0.15083338]]...]
INFO - root - 2017-12-10 23:08:33.308105: step 62510, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.798 sec/batch; 59h:50m:09s remains)
INFO - root - 2017-12-10 23:08:41.084283: step 62520, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 59h:58m:09s remains)
INFO - root - 2017-12-10 23:08:48.857090: step 62530, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 60h:58m:22s remains)
INFO - root - 2017-12-10 23:08:56.689148: step 62540, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 59h:27m:25s remains)
INFO - root - 2017-12-10 23:09:04.321323: step 62550, loss = 0.70, batch loss = 0.64 (9.5 examples/sec; 0.846 sec/batch; 63h:24m:13s remains)
INFO - root - 2017-12-10 23:09:12.146331: step 62560, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 58h:53m:40s remains)
INFO - root - 2017-12-10 23:09:20.044556: step 62570, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.779 sec/batch; 58h:23m:08s remains)
INFO - root - 2017-12-10 23:09:27.910770: step 62580, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 58h:22m:08s remains)
INFO - root - 2017-12-10 23:09:35.722990: step 62590, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.801 sec/batch; 60h:01m:24s remains)
INFO - root - 2017-12-10 23:09:43.560071: step 62600, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 57h:36m:53s remains)
2017-12-10 23:09:44.334189: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0023081761 0.0021726228 0.012190272 0.033613916 0.062291473 0.090791784 0.11036785 0.11500672 0.10068965 0.070774078 0.034486011 0.0013957787 -0.021537872 -0.033705276 -0.037159845][0.0078722555 0.0072655263 0.018574446 0.043088946 0.075390287 0.1068286 0.1275073 0.13125622 0.11432784 0.080847509 0.040717144 0.0040631494 -0.021312654 -0.034917809 -0.038972694][0.015469506 0.014483325 0.025789278 0.050829835 0.083687976 0.11556734 0.13621101 0.13915309 0.12068616 0.0854795 0.043852411 0.0060859942 -0.020190522 -0.034622494 -0.039192859][0.023843633 0.022990897 0.0332447 0.056501873 0.087574482 0.11864958 0.1396175 0.14316256 0.12482703 0.089264743 0.047083817 0.0087695708 -0.018229542 -0.033391871 -0.038593072][0.03380629 0.033294 0.041592062 0.060826257 0.087360956 0.11532991 0.13554282 0.13988981 0.12289824 0.088561907 0.047241375 0.0093651945 -0.017515989 -0.032716297 -0.037923906][0.049727142 0.048773311 0.053822439 0.067014374 0.08638937 0.10843631 0.125594 0.12985633 0.11492029 0.083675005 0.045033641 0.0085794358 -0.017763268 -0.032779224 -0.03791073][0.078379624 0.075895779 0.075906754 0.080890954 0.090710729 0.10418722 0.11549982 0.11762435 0.1041055 0.076413944 0.041469995 0.0073806662 -0.018027769 -0.032822222 -0.037836391][0.11009868 0.10718644 0.10377023 0.10197049 0.10329995 0.10828934 0.1123251 0.1097059 0.094925083 0.068800792 0.036535539 0.0047592968 -0.01932214 -0.033389237 -0.037874643][0.13286427 0.1316659 0.12817055 0.12317508 0.11906382 0.11777875 0.11536294 0.10719886 0.089345843 0.062633343 0.031285957 0.0010002537 -0.021818588 -0.034832347 -0.03848527][0.15126072 0.15348053 0.15148421 0.14520724 0.13780066 0.13235828 0.12551016 0.11309942 0.091995656 0.063212529 0.030633422 -0.00017448045 -0.023120286 -0.035994142 -0.039356809][0.16425362 0.17102371 0.17105064 0.16469157 0.15593937 0.14849374 0.13932689 0.1242512 0.10032266 0.068761669 0.033782788 0.0012693406 -0.022660811 -0.0360086 -0.039679032][0.16537046 0.17603162 0.17866965 0.17400023 0.16643418 0.159353 0.14944437 0.13261926 0.1063225 0.072188705 0.035149552 0.0014688759 -0.022820916 -0.0360394 -0.039668947][0.14410162 0.15825878 0.16604264 0.16720802 0.16502847 0.16128023 0.15204282 0.13418488 0.10633234 0.070793845 0.032948915 -0.00070687872 -0.024407657 -0.036826767 -0.039974328][0.1069319 0.12273295 0.13538855 0.14355931 0.14828323 0.14917368 0.141924 0.1248263 0.097926348 0.063852742 0.027705802 -0.0041877213 -0.026380997 -0.037701853 -0.0402628][0.06773781 0.082472496 0.096632242 0.10881247 0.11830203 0.1229338 0.11843819 0.10406363 0.080596805 0.0506366 0.018715024 -0.0094502242 -0.028840486 -0.03850295 -0.040336311]]...]
INFO - root - 2017-12-10 23:09:52.166306: step 62610, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 60h:21m:29s remains)
INFO - root - 2017-12-10 23:09:59.849700: step 62620, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 59h:19m:51s remains)
INFO - root - 2017-12-10 23:10:07.548249: step 62630, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 59h:23m:57s remains)
INFO - root - 2017-12-10 23:10:15.349027: step 62640, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 57h:05m:51s remains)
INFO - root - 2017-12-10 23:10:23.222779: step 62650, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 59h:33m:45s remains)
INFO - root - 2017-12-10 23:10:31.101883: step 62660, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 59h:03m:06s remains)
INFO - root - 2017-12-10 23:10:38.940569: step 62670, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 59h:25m:16s remains)
INFO - root - 2017-12-10 23:10:46.868884: step 62680, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 58h:03m:51s remains)
INFO - root - 2017-12-10 23:10:54.785865: step 62690, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 59h:39m:03s remains)
INFO - root - 2017-12-10 23:11:02.535995: step 62700, loss = 0.68, batch loss = 0.63 (10.6 examples/sec; 0.758 sec/batch; 56h:48m:04s remains)
2017-12-10 23:11:03.266174: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.49420038 0.49907675 0.48350951 0.45741853 0.42873496 0.40079787 0.37976989 0.37348679 0.37817749 0.37680003 0.36256424 0.34361148 0.33851141 0.34849435 0.36963138][0.56204462 0.56394154 0.536756 0.49767056 0.46210772 0.43299797 0.4120456 0.40377972 0.40469098 0.39999098 0.38298652 0.36630011 0.37651417 0.41239983 0.46022925][0.56841445 0.56962466 0.535851 0.49003768 0.45361185 0.42795148 0.40902969 0.39759377 0.39165005 0.38100454 0.36070755 0.34862128 0.3777037 0.44244236 0.51790768][0.52003551 0.51552671 0.48030332 0.43943581 0.41444331 0.40262192 0.39357722 0.38383582 0.37266043 0.35574129 0.33245528 0.32701468 0.37518081 0.46563539 0.56220454][0.4359878 0.42688295 0.40256831 0.3856127 0.3894352 0.404615 0.4161346 0.4160763 0.4030782 0.37827992 0.34721035 0.33990511 0.39381289 0.49419203 0.59697068][0.35567933 0.35213354 0.35407868 0.3781949 0.42374384 0.47310311 0.50867337 0.51742119 0.495824 0.45037416 0.39674565 0.36980212 0.40638191 0.4909232 0.57751369][0.29447833 0.31080705 0.35231176 0.42627302 0.5153228 0.59447587 0.64592111 0.65201336 0.60829484 0.52789032 0.43894589 0.37827227 0.37837332 0.42502195 0.47713944][0.23143049 0.27196929 0.35325807 0.47006083 0.59009886 0.68413466 0.73700565 0.72949684 0.65868992 0.54394424 0.42335483 0.33127654 0.291619 0.29314685 0.30466688][0.16632025 0.22075617 0.32178724 0.45421356 0.57866985 0.66721022 0.70965618 0.688788 0.60373318 0.476252 0.34723788 0.24480958 0.18427834 0.15847407 0.1464573][0.11585262 0.17009456 0.26618281 0.38540497 0.49038905 0.55830276 0.58427107 0.55565614 0.47327179 0.35661939 0.24228892 0.15017289 0.088761233 0.055812366 0.039709061][0.074853778 0.11648124 0.18934034 0.27742219 0.35168087 0.39547545 0.40690711 0.37771431 0.3094101 0.21693774 0.12877163 0.057857644 0.00857058 -0.017691689 -0.026486207][0.036581796 0.06166739 0.10767127 0.16342202 0.20949705 0.23430213 0.23669764 0.21154785 0.16089565 0.09561123 0.035531841 -0.01136245 -0.043415315 -0.058322165 -0.059025381][0.016122166 0.029153341 0.05437582 0.085121207 0.11002974 0.12178147 0.11962233 0.10105065 0.06811022 0.028035948 -0.0071371337 -0.032772433 -0.048676424 -0.053598106 -0.049718156][0.033442028 0.038534816 0.048572619 0.060923945 0.070160642 0.072736718 0.068660408 0.05721949 0.039877322 0.020506389 0.0050547142 -0.0040109647 -0.0067994045 -0.0032307273 0.0048076748][0.077827133 0.079220064 0.080260567 0.081873938 0.081913814 0.079385318 0.074904971 0.068646573 0.061452296 0.054820139 0.051019222 0.051110804 0.055014819 0.062392995 0.072007157]]...]
INFO - root - 2017-12-10 23:11:10.792352: step 62710, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 57h:43m:22s remains)
INFO - root - 2017-12-10 23:11:18.730544: step 62720, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 60h:09m:52s remains)
INFO - root - 2017-12-10 23:11:26.567022: step 62730, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 59h:09m:40s remains)
INFO - root - 2017-12-10 23:11:34.372937: step 62740, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 58h:39m:33s remains)
INFO - root - 2017-12-10 23:11:42.124978: step 62750, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 57h:31m:20s remains)
INFO - root - 2017-12-10 23:11:50.121524: step 62760, loss = 0.69, batch loss = 0.63 (9.3 examples/sec; 0.862 sec/batch; 64h:34m:14s remains)
INFO - root - 2017-12-10 23:11:57.914217: step 62770, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 57h:33m:33s remains)
INFO - root - 2017-12-10 23:12:05.797713: step 62780, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 57h:27m:31s remains)
INFO - root - 2017-12-10 23:12:13.411135: step 62790, loss = 0.71, batch loss = 0.65 (10.9 examples/sec; 0.732 sec/batch; 54h:49m:51s remains)
INFO - root - 2017-12-10 23:12:21.050050: step 62800, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 57h:20m:14s remains)
2017-12-10 23:12:22.002043: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20701875 0.20833945 0.20454761 0.20317639 0.20303629 0.2021749 0.19626635 0.17923148 0.15344419 0.13278095 0.12875842 0.14691082 0.17981593 0.20901157 0.22055531][0.14633015 0.15872179 0.17353828 0.19359 0.21354352 0.22895145 0.23326455 0.21922071 0.18870829 0.15569559 0.13586165 0.1391305 0.16268088 0.18820953 0.19869134][0.081542686 0.10023242 0.13079248 0.17222425 0.21557169 0.25223798 0.27178058 0.26474109 0.23162444 0.18595618 0.14739436 0.13194478 0.14318466 0.16466874 0.17653322][0.037079409 0.0541895 0.090265214 0.14436398 0.206686 0.26467246 0.30274156 0.30750418 0.27690706 0.22314253 0.16879097 0.13527629 0.13307402 0.14966597 0.1634813][0.029782815 0.037649788 0.064816914 0.11536055 0.18429321 0.258407 0.31638905 0.33831996 0.3171283 0.26274985 0.1991629 0.15188126 0.13634709 0.14485747 0.15664826][0.060160365 0.055894431 0.063966565 0.096569277 0.1578389 0.23880082 0.31414583 0.35598764 0.34968588 0.30181411 0.2362394 0.18136992 0.15515096 0.1535196 0.15842572][0.11238454 0.097503863 0.083506182 0.089982115 0.13075544 0.20539264 0.28947517 0.34905741 0.36197755 0.3297762 0.27354267 0.2218682 0.19250222 0.18381746 0.17984875][0.16812672 0.1474182 0.11546177 0.095672108 0.10874677 0.16436762 0.24470958 0.31492883 0.34756428 0.33810988 0.30113393 0.26230592 0.23787929 0.22719689 0.21561649][0.21522997 0.19646806 0.15563318 0.11609697 0.1011264 0.12980826 0.19514348 0.26742584 0.31769094 0.33463255 0.32333705 0.3032831 0.28871059 0.27994835 0.26327097][0.24365379 0.23570026 0.19844413 0.15145288 0.11551201 0.11523982 0.15621722 0.21966441 0.27965507 0.31946138 0.33437786 0.335662 0.334231 0.33093527 0.31244594][0.24830768 0.25676528 0.23377994 0.19211304 0.14607149 0.12060869 0.13261941 0.17748682 0.23651086 0.29051208 0.32741293 0.35015535 0.36458078 0.37014294 0.35239893][0.22787774 0.252185 0.24776131 0.21951242 0.17353417 0.13116267 0.11722544 0.14122792 0.19409072 0.25667426 0.31185311 0.35456589 0.38455608 0.39836887 0.37992379][0.19425957 0.22811581 0.23859583 0.22431502 0.18442096 0.13463041 0.10344721 0.11090244 0.15702441 0.22417654 0.29245064 0.35027862 0.39129102 0.40944245 0.3881363][0.16567613 0.19979498 0.21513872 0.20845054 0.1754867 0.1261878 0.087970488 0.086048596 0.12722175 0.19609132 0.27249873 0.34035963 0.38833472 0.40905032 0.38650411][0.14951195 0.17454943 0.18377557 0.17673674 0.1498137 0.10795307 0.073335946 0.070649862 0.11015004 0.17892207 0.25824738 0.32958463 0.37926307 0.40096185 0.37966263]]...]
INFO - root - 2017-12-10 23:12:29.789443: step 62810, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 56h:56m:14s remains)
INFO - root - 2017-12-10 23:12:37.619604: step 62820, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 60h:01m:20s remains)
INFO - root - 2017-12-10 23:12:45.563014: step 62830, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 59h:33m:00s remains)
INFO - root - 2017-12-10 23:12:53.462161: step 62840, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 57h:56m:36s remains)
INFO - root - 2017-12-10 23:13:01.335048: step 62850, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 58h:13m:17s remains)
INFO - root - 2017-12-10 23:13:09.127758: step 62860, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 59h:06m:13s remains)
INFO - root - 2017-12-10 23:13:16.768269: step 62870, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.787 sec/batch; 58h:57m:24s remains)
INFO - root - 2017-12-10 23:13:24.328666: step 62880, loss = 0.69, batch loss = 0.64 (11.4 examples/sec; 0.704 sec/batch; 52h:41m:44s remains)
INFO - root - 2017-12-10 23:13:32.169698: step 62890, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 58h:07m:38s remains)
INFO - root - 2017-12-10 23:13:40.087011: step 62900, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 59h:15m:25s remains)
2017-12-10 23:13:40.879105: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40800479 0.4848572 0.52232647 0.50812626 0.45077547 0.37412509 0.30612251 0.26462281 0.25060773 0.25087833 0.24973145 0.23918118 0.22616011 0.21413004 0.19859864][0.47319588 0.5505749 0.57612062 0.5463227 0.47932118 0.404587 0.34882545 0.32114014 0.30947292 0.29464069 0.2695969 0.24035232 0.22026937 0.21030121 0.20107518][0.49479702 0.56309026 0.57437128 0.537109 0.47738978 0.42488837 0.3992025 0.39540753 0.38674021 0.34943554 0.28907466 0.22964232 0.19374637 0.18283564 0.18309729][0.45444483 0.5077036 0.50936496 0.47806421 0.44376698 0.43096247 0.44795987 0.47465298 0.47139674 0.41168022 0.31292829 0.21692121 0.15750727 0.13959754 0.14756294][0.34652409 0.38360769 0.38537794 0.3758589 0.3837446 0.42390814 0.49187782 0.55419606 0.56233364 0.48831 0.35626465 0.22191408 0.13148245 0.098307088 0.10845445][0.20666018 0.23310123 0.24611518 0.27076089 0.33090553 0.43041953 0.5527339 0.65427893 0.68047804 0.600095 0.43823469 0.26170957 0.13165969 0.073550023 0.076804414][0.090871282 0.11293279 0.14178625 0.20121789 0.31070453 0.46556056 0.63898003 0.77891165 0.82516235 0.74113774 0.54973978 0.32875228 0.15577188 0.0680714 0.057050075][0.030459017 0.051180348 0.092673138 0.17691599 0.32091665 0.51501381 0.72538036 0.89333671 0.95376396 0.86510324 0.64860028 0.39171505 0.18486927 0.072487853 0.045518059][0.028956041 0.048537645 0.094255455 0.18678583 0.34210408 0.54885638 0.77027178 0.94472212 1.0055208 0.91073745 0.68263674 0.41291758 0.19522297 0.073420078 0.036315981][0.061455905 0.076650485 0.11585836 0.19905323 0.34143412 0.53177238 0.7338112 0.88868171 0.9354766 0.83840555 0.62183839 0.37201753 0.17332175 0.062308725 0.026079897][0.091717184 0.09855444 0.12387216 0.18608668 0.29861486 0.45116934 0.61096597 0.72748792 0.75189447 0.65934038 0.47554091 0.2733784 0.11952529 0.039176576 0.017723108][0.098113984 0.0954132 0.10500857 0.14173368 0.21589275 0.319057 0.42461094 0.49429828 0.4949294 0.41440573 0.27780995 0.13952851 0.044930715 0.0071392828 0.010506897][0.0790037 0.070932396 0.069865875 0.084645852 0.12141343 0.17425358 0.22524601 0.25077498 0.23368151 0.17241776 0.088389091 0.016444014 -0.018451879 -0.013283467 0.016192803][0.058326658 0.054646205 0.054502022 0.058502026 0.067935422 0.079322018 0.084004916 0.072987527 0.044022549 0.0036468124 -0.032376669 -0.047204509 -0.033087038 0.0052533438 0.052666172][0.065799467 0.076170065 0.087547466 0.0932937 0.090324983 0.076289371 0.049007639 0.010858917 -0.028518364 -0.056128457 -0.060618386 -0.038387325 0.0053159525 0.059672054 0.11031682]]...]
INFO - root - 2017-12-10 23:13:48.716262: step 62910, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 60h:43m:53s remains)
INFO - root - 2017-12-10 23:13:56.593337: step 62920, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 58h:30m:12s remains)
INFO - root - 2017-12-10 23:14:04.374392: step 62930, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 59h:19m:45s remains)
INFO - root - 2017-12-10 23:14:12.141902: step 62940, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 57h:40m:20s remains)
INFO - root - 2017-12-10 23:14:19.909243: step 62950, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 60h:35m:46s remains)
INFO - root - 2017-12-10 23:14:27.693258: step 62960, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 59h:17m:48s remains)
INFO - root - 2017-12-10 23:14:35.357715: step 62970, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 57h:32m:52s remains)
INFO - root - 2017-12-10 23:14:43.248935: step 62980, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 58h:04m:37s remains)
INFO - root - 2017-12-10 23:14:51.131268: step 62990, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.754 sec/batch; 56h:25m:49s remains)
INFO - root - 2017-12-10 23:14:58.950476: step 63000, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 57h:31m:46s remains)
2017-12-10 23:14:59.780440: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.52293867 0.51200432 0.479391 0.47492984 0.49088886 0.48372054 0.45069626 0.42247829 0.40093291 0.37754226 0.34856847 0.32650065 0.3216764 0.32186121 0.35188374][0.48041555 0.47303659 0.45333233 0.46577132 0.49830747 0.50594759 0.4810217 0.4500016 0.41612822 0.37989271 0.341558 0.3103779 0.3033812 0.30779806 0.34226722][0.42035535 0.42638519 0.43024179 0.46561891 0.51701194 0.54170775 0.52663136 0.49452162 0.45010361 0.40458974 0.36283249 0.32833046 0.32317328 0.33393162 0.37574708][0.39406398 0.42097625 0.45321226 0.51271617 0.5817858 0.62436175 0.62164426 0.59328926 0.54386657 0.49428669 0.45412481 0.41907597 0.41090932 0.41748744 0.45761573][0.38900483 0.44013181 0.501479 0.58498013 0.672455 0.73754078 0.75693464 0.74499077 0.70108122 0.65072477 0.60834408 0.56354022 0.53439659 0.51417452 0.53188306][0.37996772 0.45293337 0.54039317 0.64736414 0.75622416 0.85044181 0.9038136 0.920736 0.89095086 0.83904105 0.78375304 0.71247023 0.63977724 0.56926382 0.54307652][0.34973761 0.43782881 0.545335 0.6723631 0.80152088 0.9240604 1.0126311 1.0587004 1.0428984 0.985982 0.90946233 0.80223668 0.67700559 0.55011994 0.47509205][0.29916492 0.3920477 0.50846034 0.64623392 0.78705877 0.92520893 1.0340272 1.0942625 1.083235 1.0174049 0.91973174 0.78282976 0.61793303 0.45145765 0.34200948][0.24928173 0.33464077 0.44448566 0.57931632 0.71831912 0.85315287 0.95890456 1.0121162 0.99388939 0.91775984 0.80752969 0.65963089 0.48317769 0.30880252 0.19455929][0.23647682 0.3031587 0.39003015 0.50524229 0.62616444 0.73889047 0.82079941 0.85106146 0.81883061 0.73625726 0.62734616 0.49183428 0.33697042 0.19087946 0.10660613][0.27025554 0.31045958 0.36152577 0.44239333 0.53077227 0.60764104 0.65352345 0.6553399 0.61105537 0.53199834 0.4401466 0.33715343 0.22810295 0.13481162 0.10209791][0.31606111 0.32692 0.33798656 0.37882212 0.42946163 0.46823916 0.47953933 0.45903659 0.41095883 0.34569076 0.28104511 0.21876431 0.16161095 0.12353136 0.14056072][0.32433 0.30883852 0.28626418 0.29307333 0.3125788 0.32299215 0.31280363 0.28323317 0.24095246 0.19474809 0.15757786 0.13008796 0.11276079 0.11268827 0.15812626][0.26497909 0.23258282 0.19139996 0.17775947 0.17953317 0.17735273 0.16311771 0.13880187 0.11010781 0.083360933 0.06666562 0.059451696 0.06133233 0.076072514 0.12521408][0.15180759 0.11533087 0.072429009 0.051866792 0.046673208 0.042507716 0.034300018 0.0224776 0.009946431 -0.00031664182 -0.0046247467 -0.003916176 0.0019474336 0.015715791 0.052111208]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 23:15:07.595420: step 63010, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 57h:23m:05s remains)
INFO - root - 2017-12-10 23:15:15.432276: step 63020, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 58h:46m:15s remains)
INFO - root - 2017-12-10 23:15:23.103159: step 63030, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 58h:38m:23s remains)
INFO - root - 2017-12-10 23:15:30.886463: step 63040, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 57h:03m:13s remains)
INFO - root - 2017-12-10 23:15:38.719606: step 63050, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 57h:42m:37s remains)
INFO - root - 2017-12-10 23:15:46.433330: step 63060, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 59h:32m:44s remains)
INFO - root - 2017-12-10 23:15:54.296256: step 63070, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 59h:41m:15s remains)
INFO - root - 2017-12-10 23:16:02.178307: step 63080, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.830 sec/batch; 62h:07m:09s remains)
INFO - root - 2017-12-10 23:16:09.974475: step 63090, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 58h:46m:49s remains)
INFO - root - 2017-12-10 23:16:17.772959: step 63100, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 57h:56m:40s remains)
2017-12-10 23:16:18.682594: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26124275 0.28179663 0.28296015 0.27225155 0.25396445 0.23547226 0.23324269 0.2474874 0.26440829 0.26008141 0.2298467 0.17822523 0.10936765 0.040365916 -0.01237143][0.32996151 0.35299921 0.35281155 0.34065163 0.32126471 0.30199286 0.30079418 0.3186312 0.3398627 0.33512813 0.29912004 0.23696209 0.15367858 0.068649068 0.0021574097][0.36640632 0.38945746 0.38730106 0.37437674 0.356295 0.3393006 0.34055921 0.36051947 0.38316208 0.37698069 0.33666554 0.26777858 0.17591746 0.0818025 0.0076490175][0.37400344 0.39551297 0.39220423 0.37950006 0.36499131 0.35357597 0.35893393 0.37977645 0.40046731 0.39106235 0.34735772 0.27532503 0.18116856 0.085353069 0.0095354849][0.36301729 0.38136008 0.37688011 0.36442596 0.35318756 0.34817007 0.35794848 0.37894449 0.39604262 0.38363475 0.33853188 0.26665151 0.17505787 0.082995974 0.0098451236][0.33582795 0.34999502 0.34725502 0.33938888 0.33588985 0.34192652 0.35951373 0.38176084 0.39437991 0.37836623 0.33085114 0.25667897 0.1657255 0.077301271 0.00798124][0.29090413 0.30430534 0.30987233 0.31446812 0.32649985 0.34891641 0.37603539 0.39712569 0.4000068 0.37492818 0.31963754 0.23939718 0.14793976 0.064802542 0.002393883][0.23968577 0.25803345 0.27725258 0.29917735 0.330559 0.37102172 0.40692627 0.42426184 0.41393727 0.37607947 0.30959529 0.22164392 0.12952396 0.052209087 -0.0022797242][0.20007829 0.22636917 0.25926131 0.29638532 0.34296674 0.39613491 0.43653518 0.44921249 0.4273594 0.37877408 0.30352286 0.21006154 0.11785689 0.04486974 -0.0036849368][0.17211281 0.20501289 0.24590705 0.28998321 0.34086198 0.3952238 0.43245137 0.43900049 0.40997219 0.35691762 0.28038326 0.18868104 0.10130087 0.034271684 -0.0085858386][0.13715898 0.17345273 0.21711582 0.26178157 0.3097176 0.35744774 0.38616708 0.38604686 0.354625 0.30394387 0.23455419 0.1527739 0.076256625 0.018582193 -0.017414162][0.088633962 0.1242526 0.16535774 0.20574896 0.24705943 0.284896 0.30334133 0.29716641 0.26661617 0.22251147 0.16584201 0.10063631 0.04062609 -0.003825638 -0.030726716][0.04708511 0.077254161 0.11086559 0.14272459 0.17444666 0.20150627 0.21124548 0.20215541 0.17569897 0.14026743 0.097439781 0.049484417 0.0059671598 -0.025616631 -0.043856967][0.02455754 0.047540303 0.071968719 0.093241215 0.11263461 0.12725589 0.12884037 0.11862986 0.098307483 0.073151976 0.044413839 0.012446401 -0.016908057 -0.03837917 -0.050708968][0.014265071 0.031291574 0.048423156 0.060511984 0.067828745 0.069609307 0.06272611 0.05073515 0.036009353 0.020875379 0.0052794483 -0.012594328 -0.03011106 -0.043870013 -0.05257025]]...]
INFO - root - 2017-12-10 23:16:26.262696: step 63110, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 57h:46m:21s remains)
INFO - root - 2017-12-10 23:16:34.085497: step 63120, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.812 sec/batch; 60h:43m:50s remains)
INFO - root - 2017-12-10 23:16:41.923706: step 63130, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.816 sec/batch; 61h:05m:11s remains)
INFO - root - 2017-12-10 23:16:49.752126: step 63140, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 57h:38m:07s remains)
INFO - root - 2017-12-10 23:16:57.344149: step 63150, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 59h:06m:08s remains)
INFO - root - 2017-12-10 23:17:05.119506: step 63160, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 59h:26m:01s remains)
INFO - root - 2017-12-10 23:17:13.060386: step 63170, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 56h:41m:23s remains)
INFO - root - 2017-12-10 23:17:21.007310: step 63180, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 60h:53m:22s remains)
INFO - root - 2017-12-10 23:17:28.624050: step 63190, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 60h:13m:56s remains)
INFO - root - 2017-12-10 23:17:36.413792: step 63200, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 57h:46m:15s remains)
2017-12-10 23:17:37.314138: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.017919991 0.013665585 0.0072297673 0.00068818667 -0.00291906 0.0013810006 0.020937063 0.06125709 0.11403298 0.17081414 0.22452147 0.26449457 0.28031868 0.26798451 0.23710407][0.018883344 0.016711926 0.01187386 0.0064150929 0.0028703234 0.0069632875 0.027308922 0.069392949 0.1246658 0.18373762 0.23938078 0.27995446 0.29486996 0.2813628 0.24924167][0.025135694 0.02509976 0.021467622 0.016918192 0.013175759 0.016265716 0.034407549 0.072247736 0.12199249 0.17468125 0.22411546 0.259198 0.27020058 0.25639996 0.2271052][0.043349411 0.04504285 0.042099588 0.038819697 0.03624285 0.040435068 0.057039928 0.0889941 0.13006529 0.17230853 0.21120811 0.23669188 0.24071626 0.22550392 0.19929121][0.077447042 0.08082062 0.078699194 0.077616304 0.077895865 0.085329078 0.10159294 0.127981 0.1607464 0.19298844 0.2208074 0.2343068 0.22826588 0.20800796 0.18096389][0.12213068 0.12653463 0.12501305 0.12625752 0.13004108 0.14176655 0.15895334 0.1823065 0.21077541 0.23794514 0.25856483 0.26205248 0.24638897 0.21975727 0.18853745][0.16816229 0.17344557 0.17228901 0.1749773 0.18097956 0.1953551 0.21276453 0.23324278 0.25902683 0.2848841 0.30327138 0.3024753 0.28206763 0.25224125 0.21801083][0.20467615 0.21087442 0.20969272 0.21209431 0.21752851 0.23098607 0.24555929 0.26082405 0.28214455 0.30666253 0.32587209 0.3267425 0.30771229 0.28004476 0.2475976][0.22526576 0.23176655 0.22986677 0.2305185 0.23329121 0.24257094 0.25048178 0.25661042 0.26950985 0.28995305 0.31014523 0.31501496 0.30100036 0.27952987 0.25374579][0.22666499 0.2321876 0.22807492 0.2251469 0.22302787 0.22488944 0.22286524 0.2170268 0.2188433 0.23263738 0.25123921 0.25877434 0.25044718 0.23731163 0.22231705][0.21428253 0.21746309 0.21011582 0.20234923 0.19399449 0.18712187 0.17498361 0.1581244 0.14911689 0.15432902 0.16758753 0.17554791 0.17380953 0.17184173 0.1729615][0.20348649 0.20772938 0.20010567 0.18938741 0.17572397 0.16072884 0.13938835 0.11236054 0.0918621 0.085780829 0.090320818 0.096833177 0.10215998 0.1134226 0.13413243][0.20422946 0.213822 0.21004331 0.19998513 0.18409696 0.16385257 0.13539842 0.099240966 0.067531392 0.050190568 0.04656541 0.052431095 0.065051071 0.08932285 0.12815824][0.21501267 0.23141493 0.23336208 0.22691531 0.21237187 0.19045655 0.15768239 0.11518367 0.076191805 0.052894503 0.047260623 0.05650831 0.077090777 0.11218707 0.16320424][0.2239223 0.24437751 0.25052765 0.24798769 0.23622267 0.21514496 0.18193924 0.13911304 0.10069264 0.080549352 0.081533119 0.099751249 0.12969315 0.1730269 0.22972573]]...]
INFO - root - 2017-12-10 23:17:45.219509: step 63210, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 58h:04m:25s remains)
INFO - root - 2017-12-10 23:17:53.023922: step 63220, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.787 sec/batch; 58h:52m:31s remains)
INFO - root - 2017-12-10 23:18:00.963758: step 63230, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 60h:15m:28s remains)
INFO - root - 2017-12-10 23:18:08.051005: step 63240, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 59h:09m:00s remains)
INFO - root - 2017-12-10 23:18:15.900427: step 63250, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.804 sec/batch; 60h:06m:04s remains)
INFO - root - 2017-12-10 23:18:23.691614: step 63260, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.769 sec/batch; 57h:28m:45s remains)
INFO - root - 2017-12-10 23:18:31.315486: step 63270, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 58h:49m:46s remains)
INFO - root - 2017-12-10 23:18:39.189036: step 63280, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 58h:15m:43s remains)
INFO - root - 2017-12-10 23:18:47.018296: step 63290, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.820 sec/batch; 61h:18m:22s remains)
INFO - root - 2017-12-10 23:18:54.827541: step 63300, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 57h:21m:50s remains)
2017-12-10 23:18:55.610941: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.078506112 0.12313809 0.180102 0.24415848 0.30141151 0.34247279 0.36397934 0.3625285 0.3370271 0.2936902 0.2386598 0.17242277 0.098758228 0.024890626 -0.037286464][0.13015683 0.18022496 0.24178605 0.3086957 0.36565334 0.40251386 0.41480795 0.40054616 0.36176088 0.30674392 0.24153882 0.16714674 0.087915637 0.011194924 -0.050472461][0.19885272 0.24921508 0.30889979 0.37236512 0.42415839 0.45523831 0.46078438 0.439193 0.39291629 0.33039171 0.25776306 0.17491329 0.088462405 0.00707486 -0.055242769][0.27851978 0.32257357 0.37496889 0.43262446 0.48012027 0.50891453 0.51303148 0.48965806 0.44031832 0.37329659 0.29397887 0.20162261 0.10437974 0.01391726 -0.052976847][0.34067854 0.37422091 0.41764796 0.47240785 0.52266729 0.55814832 0.56936049 0.55048567 0.50160843 0.43106514 0.3443968 0.24147615 0.13176535 0.029746691 -0.044779003][0.37493792 0.39701307 0.43434158 0.49314296 0.55690223 0.61107272 0.63950223 0.63139576 0.58512926 0.50969952 0.41268256 0.29663038 0.17222399 0.056346096 -0.028597474][0.39599887 0.40278333 0.43037537 0.49295723 0.57389933 0.65204912 0.702368 0.70794 0.66553009 0.58490986 0.47693759 0.34901509 0.21347161 0.087586686 -0.0055897678][0.40408632 0.39072898 0.4040893 0.46559548 0.55917424 0.65711796 0.72635812 0.74471217 0.70716208 0.62335223 0.50655633 0.37096775 0.23143633 0.10422866 0.010286042][0.38887227 0.35446253 0.35110605 0.40569165 0.50232357 0.60837662 0.68714255 0.71482146 0.68557292 0.60672134 0.49081266 0.35771397 0.22444151 0.10517202 0.016705658][0.33847627 0.28998709 0.27072132 0.31252652 0.40023372 0.49989989 0.57670796 0.60897356 0.59063661 0.52491909 0.42177066 0.30364972 0.18827607 0.086529262 0.010908783][0.26114449 0.20652054 0.17678238 0.2035879 0.27335674 0.3551062 0.42040712 0.451592 0.44266036 0.39287782 0.30947447 0.21487525 0.12454277 0.045904305 -0.012010934][0.1655127 0.11231134 0.078283966 0.090571783 0.13772498 0.1949825 0.24253415 0.26729494 0.26366132 0.22857355 0.16733876 0.099667162 0.036639329 -0.017234124 -0.054948691][0.064502478 0.020429425 -0.0096188542 -0.0072277244 0.01865595 0.051257305 0.079402894 0.094713338 0.09242446 0.068941012 0.028057771 -0.015559434 -0.055711683 -0.08886832 -0.10816325][-0.016135465 -0.046619371 -0.0682351 -0.070193164 -0.059029497 -0.044841424 -0.032580871 -0.026782621 -0.030551389 -0.046592698 -0.072222769 -0.098236769 -0.12217151 -0.14049329 -0.14630798][-0.067848623 -0.085688211 -0.0981722 -0.10067687 -0.097600989 -0.094219349 -0.092063636 -0.092964694 -0.098314948 -0.10968537 -0.12525837 -0.13987236 -0.15284044 -0.16096167 -0.15844621]]...]
INFO - root - 2017-12-10 23:19:03.379849: step 63310, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 60h:58m:08s remains)
INFO - root - 2017-12-10 23:19:11.232095: step 63320, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 58h:51m:22s remains)
INFO - root - 2017-12-10 23:19:18.873479: step 63330, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 59h:22m:30s remains)
INFO - root - 2017-12-10 23:19:26.778199: step 63340, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 59h:05m:16s remains)
INFO - root - 2017-12-10 23:19:34.514998: step 63350, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 59h:42m:45s remains)
INFO - root - 2017-12-10 23:19:42.311244: step 63360, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 59h:13m:06s remains)
INFO - root - 2017-12-10 23:19:50.246499: step 63370, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 60h:28m:39s remains)
INFO - root - 2017-12-10 23:19:58.122119: step 63380, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 60h:06m:11s remains)
INFO - root - 2017-12-10 23:20:05.920717: step 63390, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 57h:04m:44s remains)
INFO - root - 2017-12-10 23:20:13.803270: step 63400, loss = 0.69, batch loss = 0.64 (9.8 examples/sec; 0.820 sec/batch; 61h:16m:22s remains)
2017-12-10 23:20:14.720350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0063951914 -0.095667936 -0.13527268 -0.1400138 -0.10680539 -0.031530451 0.053505819 0.1140639 0.13780937 0.12401313 0.073970191 0.003405411 -0.062056646 -0.10388254 -0.11829447][-0.029753331 -0.11373293 -0.15767044 -0.17503938 -0.16741236 -0.13401432 -0.0948944 -0.070273012 -0.062651843 -0.07184457 -0.097135529 -0.12832372 -0.15062442 -0.15683383 -0.14893372][-0.056564007 -0.12313311 -0.15657669 -0.17110725 -0.17242369 -0.1610748 -0.1488709 -0.14548908 -0.149297 -0.16048644 -0.17627491 -0.18863951 -0.1905977 -0.18206209 -0.16628198][-0.0788886 -0.11505998 -0.11820433 -0.10795297 -0.093467854 -0.072940812 -0.054860335 -0.048937328 -0.056854442 -0.0788754 -0.10764083 -0.13576381 -0.1560801 -0.16571167 -0.16374801][-0.089904793 -0.080095433 -0.030225923 0.030620145 0.088541359 0.14994797 0.20434675 0.23173985 0.22133492 0.17426841 0.1062578 0.026717544 -0.049432423 -0.10777 -0.13923749][-0.0875304 -0.018554406 0.10263116 0.23522097 0.35814953 0.48127085 0.58711332 0.63962394 0.61735874 0.52681154 0.39669541 0.24143155 0.087589987 -0.035253711 -0.10874216][-0.058925007 0.070312791 0.26698396 0.47779092 0.67205369 0.85925883 1.0113434 1.0755249 1.0245545 0.875262 0.67324722 0.43907079 0.20918486 0.026246324 -0.084023][0.0044561466 0.17990674 0.43403691 0.70504725 0.95347261 1.1848956 1.3594555 1.412709 1.3197752 1.1114339 0.84956884 0.55863261 0.27989855 0.06241855 -0.065178864][0.0915549 0.28763273 0.56424361 0.85715735 1.1218044 1.3583021 1.5200785 1.5414524 1.4058872 1.1573462 0.86831307 0.56296921 0.27977413 0.065988772 -0.051375583][0.17983457 0.3671234 0.62563932 0.89494109 1.1314174 1.3299468 1.4455633 1.4225616 1.2570615 1.0017864 0.72968137 0.45941037 0.21892613 0.046960667 -0.035129365][0.23933111 0.39511639 0.60325271 0.81187505 0.98486871 1.1160308 1.1698608 1.1072092 0.93709767 0.714898 0.50209785 0.30542636 0.13775465 0.027238848 -0.011272279][0.26326621 0.37710652 0.52127612 0.65463579 0.75306094 0.8155008 0.81881589 0.73844707 0.59183717 0.43148229 0.29894307 0.18531662 0.089786179 0.033352297 0.026667481][0.24303888 0.317141 0.40390131 0.4721196 0.50975674 0.52472037 0.50485933 0.4336656 0.32997507 0.23963128 0.18275125 0.13621281 0.091120034 0.066910617 0.073302656][0.18302377 0.22565955 0.27132544 0.29622 0.29761285 0.29151368 0.27289319 0.2277489 0.17129533 0.1401847 0.13714838 0.13119112 0.11291988 0.10388415 0.11380246][0.099968895 0.1203989 0.14111567 0.14354454 0.13043527 0.1218098 0.11573705 0.0987029 0.080783546 0.088005491 0.11170834 0.12143174 0.11415651 0.11423225 0.1297372]]...]
INFO - root - 2017-12-10 23:20:22.567456: step 63410, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 59h:28m:19s remains)
INFO - root - 2017-12-10 23:20:30.195559: step 63420, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 58h:10m:15s remains)
INFO - root - 2017-12-10 23:20:37.801387: step 63430, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 60h:16m:28s remains)
INFO - root - 2017-12-10 23:20:45.880363: step 63440, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 59h:11m:15s remains)
INFO - root - 2017-12-10 23:20:53.715628: step 63450, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 58h:02m:43s remains)
INFO - root - 2017-12-10 23:21:01.487658: step 63460, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.752 sec/batch; 56h:11m:44s remains)
INFO - root - 2017-12-10 23:21:09.308480: step 63470, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 59h:14m:42s remains)
INFO - root - 2017-12-10 23:21:17.101428: step 63480, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 59h:04m:21s remains)
INFO - root - 2017-12-10 23:21:25.005475: step 63490, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 57h:58m:54s remains)
INFO - root - 2017-12-10 23:21:32.797231: step 63500, loss = 0.69, batch loss = 0.64 (10.7 examples/sec; 0.747 sec/batch; 55h:49m:44s remains)
2017-12-10 23:21:33.724124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.065892406 -0.038281206 0.0051585049 0.059004005 0.11379316 0.15378672 0.16713005 0.15794131 0.13819255 0.12039804 0.12359414 0.16151692 0.22518873 0.28659913 0.31944796][-0.06734594 -0.044591639 0.00075367361 0.066511348 0.14203574 0.20445958 0.23600361 0.23691249 0.21833998 0.19585755 0.1961717 0.23636793 0.30587998 0.37275884 0.40746695][-0.024537737 -0.01498565 0.024574643 0.0972602 0.18967119 0.27022159 0.31386116 0.31753615 0.29255274 0.25699711 0.24393794 0.27267855 0.33341891 0.39428553 0.42634091][0.040526081 0.035256471 0.06626045 0.14172646 0.2437214 0.33309495 0.3796128 0.37860557 0.3423855 0.29063913 0.25840157 0.26524404 0.30482376 0.35056192 0.376798][0.1000295 0.083096243 0.10624543 0.18042272 0.28354627 0.37262008 0.41562483 0.40742964 0.36181489 0.29766503 0.2471046 0.22901823 0.24222025 0.26781934 0.28460222][0.15133329 0.13003933 0.14976804 0.22020322 0.3162173 0.39587849 0.42963156 0.41177934 0.35708344 0.28353518 0.21824238 0.17855066 0.16764407 0.1742608 0.18150628][0.20807432 0.19787446 0.22589536 0.29553431 0.37800974 0.436343 0.44803029 0.41001022 0.33966169 0.25619507 0.180926 0.12744999 0.099977896 0.092783891 0.093015157][0.27648896 0.2869834 0.32973182 0.3977021 0.45966965 0.48680115 0.46746171 0.4046106 0.31872407 0.22992283 0.15279721 0.094883673 0.058000073 0.040434085 0.034170128][0.34107319 0.37498823 0.43218637 0.49683931 0.53736711 0.53367323 0.48592341 0.40454206 0.31092092 0.2233891 0.15070164 0.094376385 0.052143853 0.024148759 0.00861094][0.38875091 0.43789873 0.50550735 0.56934011 0.59711224 0.57409477 0.50829685 0.41665664 0.32052648 0.23619771 0.16947736 0.11730357 0.072410263 0.033910479 0.0067907413][0.41212571 0.46296376 0.5340929 0.59996843 0.62599325 0.59697855 0.52285844 0.42595419 0.3286927 0.24729051 0.18713751 0.14224915 0.099587955 0.054412737 0.01719478][0.41382474 0.4554759 0.518619 0.58081311 0.60663855 0.578615 0.50523192 0.41175583 0.32027572 0.24640377 0.19634123 0.16331556 0.129168 0.083632007 0.040041108][0.40001678 0.42403969 0.46719497 0.51489139 0.53404111 0.50615346 0.43918395 0.35978439 0.28672713 0.23128666 0.19933143 0.18434538 0.16458078 0.12455355 0.078376047][0.39200339 0.399605 0.42021844 0.44667122 0.4493593 0.410518 0.34225255 0.27591822 0.22754136 0.20040315 0.1954022 0.20444451 0.20366362 0.17422102 0.12955919][0.40848643 0.40730852 0.40707919 0.4058668 0.37933666 0.31416914 0.23131219 0.17017612 0.14663264 0.15341046 0.18072829 0.21615031 0.23598555 0.22136469 0.18432993]]...]
INFO - root - 2017-12-10 23:21:41.188384: step 63510, loss = 0.67, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 59h:56m:07s remains)
INFO - root - 2017-12-10 23:21:48.984024: step 63520, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 57h:55m:11s remains)
INFO - root - 2017-12-10 23:21:56.836867: step 63530, loss = 0.70, batch loss = 0.65 (10.7 examples/sec; 0.746 sec/batch; 55h:42m:12s remains)
INFO - root - 2017-12-10 23:22:04.649821: step 63540, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 58h:42m:29s remains)
INFO - root - 2017-12-10 23:22:12.483794: step 63550, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 57h:10m:08s remains)
INFO - root - 2017-12-10 23:22:20.276588: step 63560, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 57h:47m:15s remains)
INFO - root - 2017-12-10 23:22:28.110782: step 63570, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 59h:23m:38s remains)
INFO - root - 2017-12-10 23:22:35.904517: step 63580, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 58h:00m:19s remains)
INFO - root - 2017-12-10 23:22:43.610612: step 63590, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 59h:14m:17s remains)
INFO - root - 2017-12-10 23:22:51.231321: step 63600, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 57h:54m:39s remains)
2017-12-10 23:22:52.144736: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23841216 0.20530958 0.18397802 0.19382027 0.23065738 0.27434528 0.30264485 0.31181 0.30172583 0.27941585 0.2538996 0.23446161 0.22743687 0.22606042 0.22769031][0.23267823 0.2050788 0.19261329 0.20918696 0.24618757 0.28315979 0.30075064 0.29607144 0.2727074 0.24082765 0.21259186 0.19639216 0.19431989 0.19729494 0.20141111][0.21052548 0.18972416 0.18787004 0.21333097 0.25264418 0.28584498 0.2963509 0.28172913 0.24686265 0.20524238 0.17232008 0.15660487 0.15653172 0.16110019 0.16644451][0.19293648 0.17697755 0.18249068 0.21479273 0.25760248 0.29253945 0.304364 0.28802153 0.24794653 0.19904926 0.15917304 0.13824184 0.13311945 0.13404107 0.13769649][0.19158962 0.177497 0.18495554 0.21834213 0.26116678 0.29827923 0.31620708 0.30528998 0.26649812 0.21362169 0.16656208 0.13760228 0.12346774 0.11740045 0.11658101][0.21353298 0.19894235 0.20248476 0.22911178 0.26431862 0.29834694 0.32156342 0.31960276 0.28717273 0.23484333 0.18401773 0.14868917 0.12525488 0.11046128 0.10287475][0.25887787 0.24269572 0.23845381 0.25252223 0.273405 0.29851025 0.3229529 0.328978 0.30440488 0.25626472 0.20686631 0.16956903 0.13973664 0.1169292 0.10157222][0.31028304 0.29233646 0.27961066 0.28111312 0.28739581 0.30218703 0.32507583 0.33608955 0.31763211 0.27432892 0.22943461 0.19397065 0.16147366 0.13331953 0.11114696][0.34564754 0.32262751 0.30220765 0.29544637 0.29284939 0.30068058 0.32144088 0.33498654 0.32055327 0.281923 0.24302223 0.21201275 0.18078525 0.15117978 0.12566806][0.35785508 0.326552 0.29878017 0.28759617 0.28287908 0.28955397 0.31035 0.32631364 0.3159419 0.2838549 0.25311607 0.22866307 0.20234115 0.17534135 0.14987142][0.35151595 0.31031522 0.27418351 0.25849453 0.25376567 0.26225016 0.28514802 0.30491281 0.30115414 0.28023866 0.26233223 0.24872032 0.23184744 0.21114844 0.18797246][0.32399 0.27550212 0.23219828 0.21266828 0.20932502 0.22077802 0.24586654 0.26905477 0.27292678 0.26583484 0.26444137 0.26626992 0.26416814 0.25497469 0.23801349][0.28930932 0.23814428 0.19145536 0.17022626 0.16959025 0.18322617 0.20734189 0.23045781 0.24032298 0.24702199 0.26298159 0.28208506 0.297093 0.30250996 0.29566345][0.266781 0.22059716 0.17589648 0.15415493 0.15408041 0.1655333 0.18327275 0.20225126 0.21652888 0.23664922 0.26915804 0.30406564 0.334319 0.35344431 0.35785976][0.26573095 0.23225629 0.19422314 0.17184483 0.16774227 0.17134778 0.17826065 0.19064444 0.20760883 0.2385558 0.28368989 0.32936928 0.36968306 0.39879203 0.41291326]]...]
INFO - root - 2017-12-10 23:22:59.993024: step 63610, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.769 sec/batch; 57h:28m:22s remains)
INFO - root - 2017-12-10 23:23:07.852152: step 63620, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 60h:26m:50s remains)
INFO - root - 2017-12-10 23:23:15.699369: step 63630, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.788 sec/batch; 58h:52m:37s remains)
INFO - root - 2017-12-10 23:23:23.505846: step 63640, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 57h:22m:21s remains)
INFO - root - 2017-12-10 23:23:31.352049: step 63650, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 57h:06m:06s remains)
INFO - root - 2017-12-10 23:23:39.212120: step 63660, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 58h:28m:23s remains)
INFO - root - 2017-12-10 23:23:46.887659: step 63670, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.749 sec/batch; 55h:57m:59s remains)
INFO - root - 2017-12-10 23:23:54.586957: step 63680, loss = 0.70, batch loss = 0.64 (12.1 examples/sec; 0.660 sec/batch; 49h:14m:51s remains)
INFO - root - 2017-12-10 23:24:02.435276: step 63690, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 58h:08m:06s remains)
INFO - root - 2017-12-10 23:24:10.169670: step 63700, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 57h:04m:38s remains)
2017-12-10 23:24:10.984190: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.014155812 0.027329858 0.077158578 0.12644693 0.16027881 0.16859172 0.15906373 0.14189196 0.13058171 0.13619339 0.15800016 0.19006434 0.21554692 0.21799827 0.1917429][-0.012551926 0.037487525 0.10006283 0.1639981 0.20948306 0.22315158 0.2126722 0.1905857 0.17499173 0.178499 0.20062383 0.23785239 0.27353528 0.28480116 0.26189584][-0.013933534 0.044387791 0.12035477 0.20090184 0.26160434 0.28561217 0.27929753 0.2567364 0.24028704 0.24203639 0.26189455 0.29971656 0.34050545 0.35646296 0.33529139][-0.018912995 0.046000235 0.1345731 0.2327403 0.31188083 0.35164467 0.35601491 0.339424 0.32716602 0.3296631 0.34719679 0.38189456 0.4211697 0.43450162 0.4096624][-0.025348604 0.043397777 0.14205211 0.25606886 0.35480177 0.41502455 0.437169 0.432554 0.42690814 0.43028346 0.44435671 0.47255942 0.50391805 0.506956 0.47160208][-0.028014284 0.042629365 0.14842466 0.27416992 0.38899392 0.46969262 0.51249635 0.52138668 0.51870596 0.51642334 0.52074307 0.53603458 0.55231309 0.54061085 0.49336842][-0.025692552 0.04634859 0.15677314 0.28856096 0.41233638 0.50768173 0.56736231 0.58706313 0.58401626 0.57333791 0.56557441 0.56383312 0.56025332 0.53269023 0.47675723][-0.022213761 0.050460618 0.16190049 0.29254889 0.41641468 0.5177967 0.58696806 0.61247522 0.607365 0.59213406 0.579812 0.56784219 0.54866844 0.50902808 0.44974062][-0.019990578 0.054163881 0.16587761 0.29207394 0.41055563 0.51056606 0.58014852 0.603324 0.59204859 0.57397169 0.56269252 0.54816604 0.522651 0.48119754 0.42869771][-0.02117479 0.055310335 0.16885303 0.29330513 0.40849447 0.50584012 0.56995928 0.5828687 0.559845 0.536485 0.526127 0.51217228 0.48737574 0.45365748 0.41814154][-0.02404744 0.053553682 0.1680326 0.29319653 0.40962717 0.50751853 0.5670802 0.56937355 0.53517818 0.50604945 0.4945007 0.48062325 0.45877317 0.43527925 0.41728365][-0.025021432 0.052439943 0.16587666 0.29151654 0.41081733 0.51185709 0.57301879 0.57470244 0.53908694 0.50773716 0.49226126 0.4752506 0.45313275 0.43391028 0.42345297][-0.023216264 0.052167505 0.16319929 0.28857777 0.41008857 0.51398027 0.58196867 0.59436637 0.56851214 0.53926933 0.51706523 0.49104446 0.46025807 0.43422097 0.41846949][-0.019335283 0.0513578 0.15836923 0.28277805 0.40576869 0.51182383 0.589688 0.61951458 0.60845232 0.5812692 0.54798794 0.50649548 0.45849624 0.41527361 0.38466609][-0.015327622 0.048182361 0.14748667 0.26592696 0.38526559 0.48964745 0.57701653 0.62679642 0.63277638 0.60950959 0.5667392 0.51032788 0.44299686 0.37818649 0.32900637]]...]
INFO - root - 2017-12-10 23:24:18.826213: step 63710, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 57h:47m:25s remains)
INFO - root - 2017-12-10 23:24:26.691962: step 63720, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 59h:07m:40s remains)
INFO - root - 2017-12-10 23:24:34.577898: step 63730, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 58h:58m:20s remains)
INFO - root - 2017-12-10 23:24:42.350915: step 63740, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 58h:07m:59s remains)
INFO - root - 2017-12-10 23:24:50.028155: step 63750, loss = 0.71, batch loss = 0.66 (9.7 examples/sec; 0.823 sec/batch; 61h:28m:09s remains)
INFO - root - 2017-12-10 23:24:57.852831: step 63760, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 59h:40m:04s remains)
INFO - root - 2017-12-10 23:25:05.580880: step 63770, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.812 sec/batch; 60h:38m:37s remains)
INFO - root - 2017-12-10 23:25:13.398466: step 63780, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 59h:49m:28s remains)
INFO - root - 2017-12-10 23:25:21.253817: step 63790, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 59h:07m:10s remains)
INFO - root - 2017-12-10 23:25:28.951864: step 63800, loss = 0.68, batch loss = 0.62 (10.8 examples/sec; 0.738 sec/batch; 55h:07m:02s remains)
2017-12-10 23:25:29.768341: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26933026 0.26277497 0.23582006 0.20048383 0.17151569 0.16186246 0.16489449 0.16782402 0.16918209 0.16854785 0.16635962 0.163446 0.16353133 0.1687161 0.17834659][0.2762211 0.27503446 0.24635719 0.2027197 0.16094705 0.13741569 0.12796599 0.12115987 0.1165849 0.11374521 0.11194109 0.11131436 0.11453214 0.12418074 0.14049578][0.27804571 0.2910307 0.2693288 0.22385295 0.17188062 0.13249175 0.10522123 0.082814693 0.06768395 0.060722232 0.058979943 0.059444189 0.06421259 0.077368125 0.1008742][0.28303206 0.31682041 0.31046295 0.27188268 0.21672419 0.16538608 0.12108275 0.0824694 0.055777911 0.04347102 0.039788496 0.037787292 0.040201414 0.053475007 0.08285547][0.296098 0.35664085 0.37466189 0.35412034 0.30749249 0.25653267 0.20691988 0.16062632 0.12621741 0.10782523 0.096932076 0.082655691 0.071397722 0.074421264 0.10161077][0.30907679 0.39711124 0.44484603 0.45187837 0.42759183 0.3948572 0.35858294 0.31802076 0.28101069 0.25418454 0.22814776 0.18938285 0.15216905 0.13528918 0.15316449][0.30616933 0.41470402 0.49024194 0.52845579 0.53712785 0.53821754 0.53082138 0.50603205 0.46899983 0.43078694 0.38363734 0.31342721 0.24368736 0.20250838 0.20719023][0.27253413 0.38717872 0.47998294 0.54413617 0.58653563 0.62505168 0.64984888 0.64274222 0.60613394 0.5566293 0.4906891 0.39537105 0.29957283 0.23879465 0.23131987][0.20740923 0.31279209 0.40796897 0.48498389 0.54970229 0.61396217 0.66039032 0.66532511 0.63032609 0.57640934 0.50363183 0.40004003 0.29430035 0.22546031 0.21106905][0.1253074 0.20986129 0.29427919 0.36993292 0.44066334 0.51232255 0.56406385 0.57267046 0.54107642 0.49153015 0.42500108 0.3295154 0.22960991 0.16409254 0.14936085][0.054962009 0.11326615 0.1779034 0.24096362 0.30408058 0.36750826 0.41064453 0.41525254 0.38651457 0.34438947 0.28945518 0.21134974 0.12946996 0.078087665 0.071265407][0.014363091 0.048795544 0.092061296 0.13803393 0.18653551 0.23335022 0.26093733 0.25794822 0.23093669 0.19519122 0.15147068 0.093654439 0.036639389 0.0058390889 0.01104234][0.0012422104 0.018332157 0.04322011 0.07248047 0.10530484 0.13484547 0.148007 0.13961418 0.11554679 0.085714094 0.051886972 0.012978757 -0.019601531 -0.0310762 -0.016712492][-0.00060022832 0.0057778191 0.017143765 0.033000756 0.053360403 0.070354283 0.074915044 0.066014551 0.047517266 0.024587933 -0.0005344266 -0.025078677 -0.040389631 -0.040547524 -0.024631066][-0.00561786 -0.0045668068 -0.0016218801 0.0049291821 0.016697669 0.026201753 0.027326347 0.021191483 0.0096108513 -0.0059116082 -0.023602022 -0.038750283 -0.044938389 -0.041527264 -0.029408583]]...]
INFO - root - 2017-12-10 23:25:37.543468: step 63810, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 58h:03m:35s remains)
INFO - root - 2017-12-10 23:25:45.411749: step 63820, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 57h:34m:54s remains)
INFO - root - 2017-12-10 23:25:53.102364: step 63830, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 58h:28m:37s remains)
INFO - root - 2017-12-10 23:26:00.876768: step 63840, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 59h:41m:56s remains)
INFO - root - 2017-12-10 23:26:08.829783: step 63850, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 59h:47m:27s remains)
INFO - root - 2017-12-10 23:26:16.481320: step 63860, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 57h:54m:25s remains)
INFO - root - 2017-12-10 23:26:24.242894: step 63870, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 58h:31m:11s remains)
INFO - root - 2017-12-10 23:26:32.169293: step 63880, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 60h:07m:14s remains)
INFO - root - 2017-12-10 23:26:40.008013: step 63890, loss = 0.69, batch loss = 0.63 (10.9 examples/sec; 0.737 sec/batch; 55h:00m:15s remains)
INFO - root - 2017-12-10 23:26:47.754813: step 63900, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 57h:05m:50s remains)
2017-12-10 23:26:48.590741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.024388155 -0.009785519 0.01159973 0.036357623 0.063048787 0.087384388 0.10742355 0.12465104 0.141025 0.15662357 0.1701536 0.17772596 0.17896324 0.17801331 0.17945197][-0.025478948 -0.010247643 0.012469353 0.039558973 0.070323221 0.10135778 0.1312872 0.1608606 0.18938603 0.21375383 0.23037246 0.2344978 0.22576334 0.2106116 0.19645131][-0.025069075 -0.00977504 0.013925435 0.043410704 0.079156265 0.118393 0.1593553 0.20077938 0.23863445 0.26712385 0.28188056 0.27868655 0.25873131 0.23048531 0.20283692][-0.024716916 -0.00944875 0.014770936 0.046045914 0.085629649 0.13110843 0.1797892 0.22834882 0.27007538 0.29741713 0.30712584 0.29640037 0.2681421 0.23198515 0.19699711][-0.024496354 -0.0094655268 0.015046319 0.04792909 0.091684759 0.14369737 0.19915602 0.25176525 0.29225805 0.31272066 0.31274074 0.2931515 0.25875118 0.21899964 0.18130967][-0.024058821 -0.0085949479 0.017974405 0.055985473 0.10911378 0.17315468 0.23932795 0.29608333 0.33132008 0.3379834 0.32089967 0.28729236 0.24472508 0.2016075 0.1619882][-0.02335315 -0.0064900364 0.023855995 0.069688447 0.13570964 0.21590175 0.29593927 0.35765436 0.38597453 0.37544668 0.336449 0.28366521 0.22931716 0.18125227 0.13990371][-0.023305032 -0.005397751 0.027243424 0.078170612 0.15299849 0.2449695 0.33499208 0.39965883 0.4212628 0.39563963 0.33747551 0.26772085 0.20373982 0.15300681 0.11353891][-0.023553476 -0.0049818042 0.028114114 0.079780586 0.15557492 0.2486718 0.33780622 0.39796394 0.41115662 0.37474373 0.30587205 0.22874036 0.16395383 0.11825699 0.087568693][-0.023473894 -0.0040084575 0.029232835 0.079110317 0.14964227 0.23357061 0.31046307 0.35805112 0.36122796 0.3194567 0.24993327 0.1764719 0.12025553 0.086706869 0.070076][-0.023641545 -0.0029645388 0.030902134 0.078435577 0.14075208 0.20950328 0.26690531 0.29640576 0.28829226 0.24477845 0.18174586 0.12002274 0.078649268 0.061278071 0.06157944][-0.026685255 -0.0073119337 0.02452147 0.067225009 0.119164 0.17119889 0.20834702 0.22021574 0.20282118 0.16130024 0.10973879 0.064708851 0.041725166 0.042731266 0.061403122][-0.03236223 -0.017477242 0.0082937814 0.0423987 0.081930287 0.11821301 0.13901114 0.13893765 0.11854891 0.084528491 0.048370738 0.022134343 0.017588142 0.035569239 0.070253506][-0.035760403 -0.025194891 -0.00579493 0.01879753 0.045222096 0.066485979 0.074038386 0.066752732 0.04844901 0.025816316 0.0067365458 -0.001771473 0.0081617506 0.037256226 0.081181586][-0.034126919 -0.024567056 -0.0087192459 0.0081745032 0.02245721 0.029743766 0.026119741 0.014596867 0.00090973667 -0.0096319607 -0.01344116 -0.0085720662 0.0094113825 0.041849732 0.087058067]]...]
INFO - root - 2017-12-10 23:26:56.238250: step 63910, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 59h:15m:36s remains)
INFO - root - 2017-12-10 23:27:04.096818: step 63920, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 59h:42m:32s remains)
INFO - root - 2017-12-10 23:27:11.882408: step 63930, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 57h:18m:32s remains)
INFO - root - 2017-12-10 23:27:19.665827: step 63940, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 57h:53m:29s remains)
INFO - root - 2017-12-10 23:27:27.354553: step 63950, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.788 sec/batch; 58h:48m:14s remains)
INFO - root - 2017-12-10 23:27:35.284523: step 63960, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 60h:09m:17s remains)
INFO - root - 2017-12-10 23:27:43.197619: step 63970, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 59h:32m:11s remains)
INFO - root - 2017-12-10 23:27:51.048456: step 63980, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 58h:17m:51s remains)
INFO - root - 2017-12-10 23:27:58.723535: step 63990, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 58h:41m:03s remains)
INFO - root - 2017-12-10 23:28:06.617949: step 64000, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.824 sec/batch; 61h:27m:23s remains)
2017-12-10 23:28:07.544920: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036860552 0.022454789 0.013215248 0.011827718 0.012465096 0.016114395 0.023309113 0.035204038 0.053005438 0.079023987 0.11223572 0.15247849 0.20746261 0.26928487 0.31868842][0.029429765 0.017461579 0.0126148 0.017246773 0.025161745 0.036787987 0.051627558 0.070563145 0.0953039 0.12928773 0.17105678 0.22011738 0.2835497 0.35265774 0.40633029][0.024276963 0.018503632 0.021935837 0.035328563 0.052619383 0.073674731 0.096784532 0.12214711 0.15150177 0.18903403 0.23347126 0.28379336 0.34526145 0.40918815 0.45651674][0.023360094 0.028224245 0.044284631 0.069705568 0.098469168 0.12967354 0.15950713 0.18723734 0.21528871 0.24814348 0.28572145 0.32765213 0.37776765 0.42771602 0.46152249][0.029533876 0.048362605 0.078829244 0.11717686 0.15706559 0.19646238 0.22892024 0.25299186 0.27301815 0.29394957 0.3177408 0.34495094 0.3785685 0.41051987 0.42842948][0.044888757 0.077951178 0.12157515 0.17025626 0.21702725 0.25856954 0.28729212 0.30217108 0.31094748 0.31854934 0.32807863 0.34020692 0.35783425 0.3735553 0.37750784][0.067501262 0.11205077 0.16524825 0.22032888 0.26912776 0.30677539 0.32706553 0.330999 0.32971764 0.32711598 0.32549834 0.32598981 0.33168134 0.33610603 0.32973069][0.0891272 0.14134897 0.20043425 0.25933877 0.30766749 0.33850306 0.34812191 0.34082592 0.3308852 0.32063976 0.31144643 0.30570778 0.30673233 0.30746418 0.2967785][0.10423807 0.16265839 0.2272865 0.29083928 0.33980727 0.36446717 0.36316231 0.34370968 0.32389367 0.30677569 0.29298207 0.28532961 0.28692475 0.28895417 0.27856851][0.11069339 0.17246124 0.24110775 0.30879402 0.35929328 0.3804045 0.37211746 0.34499282 0.31873035 0.29894173 0.28631985 0.28266102 0.28903165 0.29492378 0.28776544][0.1193447 0.1800364 0.24829297 0.31664479 0.36703134 0.38592032 0.37513307 0.34746248 0.32127321 0.30444691 0.29738289 0.30090711 0.31373012 0.32430905 0.32182893][0.14879066 0.20573622 0.26901039 0.33360955 0.38047695 0.39513379 0.38138157 0.35501724 0.33234626 0.32257026 0.32429141 0.33594644 0.35446554 0.3677817 0.36780703][0.19962065 0.25047415 0.30381033 0.35895747 0.39744747 0.40530866 0.38763595 0.36241627 0.34352106 0.34135798 0.35278592 0.37100652 0.39266169 0.40679067 0.408269][0.26420403 0.3057332 0.34300196 0.38148037 0.40598312 0.405242 0.38495681 0.36251882 0.34758455 0.35157582 0.36975715 0.38940772 0.40832874 0.41977212 0.42129871][0.32064387 0.34794533 0.3614662 0.3758285 0.3812696 0.37118721 0.35172415 0.33752564 0.331941 0.3442049 0.36712989 0.3840152 0.39622262 0.40320611 0.4041144]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 23:28:15.361723: step 64010, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 58h:02m:35s remains)
INFO - root - 2017-12-10 23:28:23.178944: step 64020, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 57h:28m:12s remains)
INFO - root - 2017-12-10 23:28:31.078035: step 64030, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 59h:17m:55s remains)
INFO - root - 2017-12-10 23:28:38.792185: step 64040, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 57h:25m:45s remains)
INFO - root - 2017-12-10 23:28:46.572724: step 64050, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 59h:35m:37s remains)
INFO - root - 2017-12-10 23:28:54.448389: step 64060, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 57h:47m:45s remains)
INFO - root - 2017-12-10 23:29:02.167454: step 64070, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 57h:31m:18s remains)
INFO - root - 2017-12-10 23:29:10.185838: step 64080, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 59h:25m:29s remains)
INFO - root - 2017-12-10 23:29:18.171800: step 64090, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 59h:41m:07s remains)
INFO - root - 2017-12-10 23:29:26.000171: step 64100, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.812 sec/batch; 60h:33m:52s remains)
2017-12-10 23:29:26.871297: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14673442 0.13787511 0.14093037 0.14667037 0.146717 0.14161652 0.13188659 0.11886916 0.11429191 0.13300692 0.16998006 0.21509571 0.25320917 0.2656554 0.24471785][0.1923141 0.18521747 0.18450487 0.18405692 0.17615525 0.16110243 0.13945921 0.11242349 0.095981173 0.10893836 0.14750226 0.19971797 0.24692182 0.26822504 0.25247371][0.24312605 0.24187559 0.23968877 0.23351741 0.21799412 0.19411054 0.16348948 0.12554994 0.097931452 0.10280202 0.13806744 0.1907333 0.24007668 0.26596394 0.25564757][0.30316138 0.31406793 0.31679988 0.30996463 0.29139724 0.26344246 0.22963105 0.18571723 0.1484323 0.14229478 0.16909839 0.21546236 0.25846455 0.28189492 0.27413777][0.36727324 0.395938 0.41007376 0.40884793 0.39367557 0.36907026 0.33954114 0.29468039 0.24876434 0.22950351 0.24410419 0.27855524 0.3081542 0.32299092 0.31454873][0.42631942 0.4752233 0.50509721 0.51334524 0.50564671 0.48982048 0.46968788 0.42762095 0.3746908 0.34268585 0.3433058 0.36221817 0.37439838 0.37704346 0.36547506][0.46447727 0.53431445 0.58199441 0.60197276 0.60401565 0.59923834 0.58907449 0.55018932 0.49219844 0.45045435 0.43908057 0.44327691 0.43953019 0.43138534 0.41687924][0.46486959 0.5496583 0.61422217 0.64945567 0.66579753 0.67441595 0.67308432 0.63726622 0.57772177 0.53121454 0.51167858 0.50393385 0.48815191 0.47261298 0.45609245][0.4179883 0.50507838 0.57901955 0.62938982 0.66279703 0.68628532 0.6934163 0.66284633 0.6078456 0.563976 0.54287386 0.52928668 0.50779194 0.48926681 0.47145498][0.34186441 0.41028616 0.47721192 0.53432643 0.58087969 0.61648661 0.63068897 0.6080519 0.5636797 0.52929103 0.51378733 0.50253087 0.48327094 0.46690878 0.44919065][0.2706964 0.29950222 0.33894557 0.38794744 0.43782321 0.47890592 0.49704507 0.48239404 0.44962251 0.42504144 0.4166564 0.41209579 0.40044942 0.39008197 0.37522107][0.22869858 0.20597504 0.20020364 0.22441882 0.26576132 0.30561393 0.32588902 0.31885108 0.29564992 0.27712509 0.27317932 0.27585584 0.27498734 0.27493837 0.268807][0.21472386 0.14306608 0.088534459 0.078725211 0.1017933 0.1336074 0.15325342 0.15242146 0.13675043 0.12216061 0.12089428 0.13009143 0.14032784 0.15218447 0.15889579][0.20506288 0.10690764 0.019779244 -0.019110898 -0.016995903 0.001257927 0.01449756 0.014701535 0.0034311677 -0.0078308871 -0.0067376285 0.0075226407 0.026644833 0.048976794 0.0696213][0.17128177 0.075444475 -0.016922239 -0.068312265 -0.081040025 -0.0768182 -0.073922835 -0.077414878 -0.086883672 -0.094981283 -0.0917324 -0.074554719 -0.050395038 -0.021524919 0.010518723]]...]
INFO - root - 2017-12-10 23:29:34.728009: step 64110, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 58h:56m:47s remains)
INFO - root - 2017-12-10 23:29:42.639547: step 64120, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 58h:21m:08s remains)
INFO - root - 2017-12-10 23:29:50.347713: step 64130, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 56h:18m:28s remains)
INFO - root - 2017-12-10 23:29:58.198813: step 64140, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 58h:00m:16s remains)
INFO - root - 2017-12-10 23:30:06.084120: step 64150, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 59h:08m:19s remains)
INFO - root - 2017-12-10 23:30:13.978651: step 64160, loss = 0.67, batch loss = 0.61 (10.3 examples/sec; 0.778 sec/batch; 57h:59m:58s remains)
INFO - root - 2017-12-10 23:30:21.891271: step 64170, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 59h:43m:00s remains)
INFO - root - 2017-12-10 23:30:29.816764: step 64180, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 59h:40m:00s remains)
INFO - root - 2017-12-10 23:30:37.730460: step 64190, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 58h:40m:06s remains)
INFO - root - 2017-12-10 23:30:45.547139: step 64200, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 57h:02m:26s remains)
2017-12-10 23:30:46.456966: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31054264 0.29057452 0.26454705 0.25299048 0.24570541 0.22855411 0.20183393 0.17656113 0.16383693 0.16573237 0.19145557 0.23585466 0.28111976 0.30427915 0.29314005][0.2653929 0.2402201 0.21434425 0.20858632 0.21210337 0.2107885 0.199316 0.1836201 0.17665498 0.18283555 0.21433733 0.27341482 0.33985552 0.38107386 0.37456843][0.2209453 0.18813667 0.15759666 0.1500987 0.15574479 0.16241287 0.16177231 0.155477 0.15536283 0.16554438 0.19916283 0.26580665 0.34652004 0.40339205 0.40671632][0.18902536 0.15124492 0.11633634 0.10389341 0.10544245 0.11160508 0.11427912 0.11290543 0.11598585 0.1252722 0.15301974 0.21440636 0.2947998 0.35780343 0.37229672][0.16581291 0.13039711 0.098349355 0.087066591 0.089298658 0.097412758 0.10378271 0.10576446 0.10770937 0.1084488 0.11967661 0.16021073 0.22228767 0.277423 0.29792872][0.14883859 0.12349238 0.10227137 0.10194021 0.11725375 0.14085189 0.16193584 0.17226972 0.17160556 0.15694067 0.14171594 0.14822447 0.17772239 0.21264157 0.22991847][0.14751357 0.13535653 0.12711656 0.14163867 0.17709768 0.22556187 0.26969814 0.29243591 0.28879637 0.25598314 0.21170869 0.18217796 0.17763928 0.18934827 0.19713685][0.1784068 0.17440026 0.17139125 0.19239594 0.23925605 0.30424353 0.3642652 0.39496651 0.38756749 0.34076265 0.27705464 0.22475223 0.19969706 0.19677515 0.19673193][0.24757221 0.24014314 0.22746165 0.23788273 0.2748996 0.3319599 0.38596079 0.4125165 0.40150246 0.35272107 0.29004326 0.23888905 0.21496823 0.21200803 0.21160553][0.33266953 0.30920339 0.27463663 0.26178581 0.27257916 0.30155692 0.33127588 0.34476003 0.33337715 0.29662195 0.25441685 0.22477597 0.21908659 0.22723705 0.23265541][0.39064118 0.34666139 0.28964955 0.25451881 0.24039923 0.24204107 0.24863148 0.25296864 0.24910074 0.234279 0.22001237 0.21771769 0.23411399 0.25442967 0.26371771][0.39202672 0.33404249 0.26542214 0.22021507 0.19623908 0.18801261 0.18885228 0.19763041 0.20983842 0.21911709 0.2275054 0.24579799 0.27781674 0.30374509 0.30833134][0.33839241 0.27799311 0.21145061 0.1696787 0.1516587 0.15161695 0.16415201 0.18822518 0.21851304 0.2454503 0.26412183 0.29064471 0.32811663 0.35234451 0.34656155][0.2514776 0.19712433 0.14135841 0.10986083 0.10398662 0.11807068 0.14667764 0.18551722 0.22742203 0.26134622 0.27947077 0.30409265 0.33866152 0.35724065 0.3431825][0.15333895 0.10919607 0.0675083 0.046667654 0.049563065 0.07118959 0.10651939 0.14884317 0.19046094 0.22138514 0.23434895 0.25299829 0.28082255 0.29431006 0.28006196]]...]
INFO - root - 2017-12-10 23:30:54.301672: step 64210, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 58h:27m:14s remains)
INFO - root - 2017-12-10 23:31:02.085592: step 64220, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 57h:24m:03s remains)
INFO - root - 2017-12-10 23:31:09.728409: step 64230, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 57h:08m:20s remains)
INFO - root - 2017-12-10 23:31:17.664256: step 64240, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 58h:07m:47s remains)
INFO - root - 2017-12-10 23:31:25.535572: step 64250, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.791 sec/batch; 58h:56m:46s remains)
INFO - root - 2017-12-10 23:31:33.347839: step 64260, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 57h:55m:45s remains)
INFO - root - 2017-12-10 23:31:41.254938: step 64270, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 59h:00m:31s remains)
INFO - root - 2017-12-10 23:31:49.046554: step 64280, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 56h:51m:15s remains)
INFO - root - 2017-12-10 23:31:56.938424: step 64290, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 59h:45m:37s remains)
INFO - root - 2017-12-10 23:32:04.829463: step 64300, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 59h:22m:41s remains)
2017-12-10 23:32:05.635356: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30925211 0.27708977 0.23182675 0.19509996 0.17606202 0.17412549 0.18018752 0.1792758 0.16909717 0.15131567 0.12594049 0.09571597 0.062122934 0.027723832 -0.0036506807][0.417951 0.3775605 0.32283968 0.27975389 0.25952768 0.26384896 0.27881217 0.27977803 0.26225397 0.22971945 0.18508704 0.1340885 0.08322008 0.037506282 -0.00027378846][0.47939456 0.43533066 0.37921953 0.33898783 0.32633686 0.34479895 0.37679935 0.38822126 0.36965472 0.32488966 0.25963461 0.18509924 0.1123167 0.051010575 0.0045914464][0.50284213 0.46185249 0.41313076 0.38441256 0.38620102 0.42315286 0.47685948 0.50699055 0.49627718 0.44524458 0.36202154 0.26337305 0.16380164 0.078925431 0.016460694][0.50245863 0.47580418 0.44475964 0.43539473 0.45519575 0.50912082 0.58260113 0.63425231 0.6362378 0.58289051 0.48465565 0.36332479 0.23471195 0.12074258 0.036501773][0.47943988 0.47689742 0.472215 0.48712221 0.52505535 0.59087062 0.67706 0.74571109 0.7596364 0.7059502 0.59679681 0.45718107 0.30279285 0.16163039 0.056184512][0.42459425 0.44690496 0.46839577 0.50437015 0.55485415 0.62519068 0.71590286 0.79461068 0.81825548 0.76849097 0.65744227 0.51097012 0.34330487 0.18616442 0.067677826][0.35373026 0.39307404 0.43031356 0.47481826 0.52500296 0.58806914 0.67205471 0.750917 0.78123665 0.74137086 0.641023 0.50332773 0.34047511 0.18388052 0.064439848][0.28282911 0.3290697 0.36977577 0.40954402 0.44689962 0.4915441 0.55789459 0.62700784 0.65960366 0.63308227 0.55284286 0.43745011 0.29576144 0.15503803 0.046315204][0.21121678 0.25482261 0.29026645 0.31842712 0.33845332 0.36145356 0.40593722 0.45926273 0.48889291 0.47399417 0.41613308 0.32916656 0.21809052 0.10401003 0.015278794][0.13647953 0.16949244 0.19419001 0.20908922 0.21419801 0.22030643 0.24641009 0.28458253 0.30922452 0.30289853 0.26465681 0.2046838 0.12516536 0.041080393 -0.023446169][0.056152582 0.075024195 0.087502033 0.090899773 0.086125448 0.082212754 0.095654421 0.12179108 0.14125049 0.14029646 0.11676291 0.0786811 0.027113987 -0.02765636 -0.066761024][-0.015744053 -0.0083445134 -0.0046733581 -0.0079823248 -0.017090965 -0.025635982 -0.021002961 -0.0055174972 0.0072175162 0.0069077741 -0.0085714264 -0.032097008 -0.062267415 -0.091963135 -0.10809758][-0.065990031 -0.065118559 -0.065339543 -0.069957554 -0.078087039 -0.086149111 -0.085871957 -0.078499608 -0.072956026 -0.076173663 -0.0889246 -0.10458802 -0.12092152 -0.13295627 -0.13299476][-0.094025165 -0.096049994 -0.096901983 -0.099770434 -0.10427345 -0.10895526 -0.10967619 -0.10729469 -0.10705326 -0.11282955 -0.12397397 -0.13495134 -0.14302555 -0.14487354 -0.13661674]]...]
INFO - root - 2017-12-10 23:32:13.299676: step 64310, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.797 sec/batch; 59h:23m:03s remains)
INFO - root - 2017-12-10 23:32:21.180937: step 64320, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.824 sec/batch; 61h:23m:29s remains)
INFO - root - 2017-12-10 23:32:28.979612: step 64330, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 58h:24m:36s remains)
INFO - root - 2017-12-10 23:32:36.824709: step 64340, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.801 sec/batch; 59h:38m:39s remains)
INFO - root - 2017-12-10 23:32:44.703805: step 64350, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 58h:56m:09s remains)
INFO - root - 2017-12-10 23:32:52.617005: step 64360, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 60h:15m:05s remains)
INFO - root - 2017-12-10 23:33:00.509933: step 64370, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 57h:00m:29s remains)
INFO - root - 2017-12-10 23:33:08.343003: step 64380, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 56h:48m:22s remains)
INFO - root - 2017-12-10 23:33:15.976406: step 64390, loss = 0.69, batch loss = 0.63 (10.9 examples/sec; 0.732 sec/batch; 54h:33m:07s remains)
INFO - root - 2017-12-10 23:33:23.760152: step 64400, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 57h:11m:40s remains)
2017-12-10 23:33:24.625599: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10854705 0.098714754 0.07852941 0.055869091 0.036312141 0.021248048 0.011073804 0.0032386496 -0.0033272181 -0.0069419043 -0.0036896479 0.010329499 0.033729538 0.058177974 0.07327652][0.16674802 0.15522909 0.12875706 0.098184653 0.071171224 0.050206214 0.0359312 0.024551813 0.013665808 0.0052247182 0.0049833665 0.018826298 0.0463259 0.0773728 0.098529316][0.19836271 0.19086735 0.166814 0.13839597 0.11353805 0.094399378 0.081148684 0.068808414 0.053872768 0.038836114 0.031566568 0.040002491 0.0642004 0.093646541 0.11333848][0.20505455 0.20903629 0.19828656 0.18433271 0.17305754 0.16503468 0.1598364 0.15083812 0.13275155 0.10853831 0.089725696 0.087147467 0.10036342 0.11941009 0.12963895][0.21936509 0.2424597 0.25276253 0.26038411 0.26869789 0.27674615 0.28363094 0.28007323 0.25797325 0.22058469 0.18564452 0.1684818 0.16700761 0.17059036 0.1659815][0.26754889 0.31199935 0.34307173 0.37057704 0.39710027 0.42150921 0.44162539 0.443881 0.41644129 0.36316663 0.31025481 0.2788558 0.26438567 0.25298548 0.23234345][0.34423894 0.40180796 0.4415549 0.47565982 0.50873035 0.54141569 0.56999367 0.5760321 0.54413056 0.47849697 0.41349715 0.3758049 0.35727212 0.33893025 0.30758893][0.41140109 0.46389768 0.49162325 0.51126724 0.5320276 0.55788225 0.58458668 0.59073281 0.55849457 0.49171376 0.42909476 0.39945173 0.390664 0.37863639 0.34834373][0.43603623 0.468473 0.46965712 0.46053189 0.45513535 0.46129778 0.47585568 0.47796804 0.44966495 0.3937971 0.3479034 0.3379212 0.34832057 0.35218281 0.3332766][0.3992759 0.40832168 0.38391152 0.34829187 0.31784797 0.30305624 0.30265036 0.2993426 0.27760553 0.23931022 0.21613903 0.22746535 0.25579971 0.274886 0.2692149][0.30133748 0.29263294 0.25442123 0.2062511 0.16336912 0.13648783 0.12609856 0.11991037 0.10616063 0.085878357 0.082123272 0.10675919 0.14282174 0.16827826 0.1704763][0.1661592 0.14971548 0.11230275 0.067202762 0.026096227 -0.0020537789 -0.015677003 -0.022112036 -0.028972745 -0.0358523 -0.028669417 -0.0019808093 0.030098844 0.051743664 0.054521788][0.032297634 0.017731681 -0.0083001712 -0.039772555 -0.069475971 -0.091115028 -0.10274684 -0.10779291 -0.11032519 -0.11018301 -0.10049588 -0.079949506 -0.058699965 -0.046522278 -0.047642428][-0.060686816 -0.06882973 -0.080733359 -0.096215658 -0.11198936 -0.12439162 -0.13159648 -0.13464759 -0.13530104 -0.13340327 -0.12653816 -0.11520138 -0.10564444 -0.10301059 -0.10862695][-0.1053772 -0.10839525 -0.11039295 -0.11442143 -0.1195891 -0.12423841 -0.12712793 -0.1283069 -0.12831892 -0.1269957 -0.1240311 -0.12022553 -0.11876969 -0.12188063 -0.12940916]]...]
INFO - root - 2017-12-10 23:33:32.452053: step 64410, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 57h:56m:02s remains)
INFO - root - 2017-12-10 23:33:40.241847: step 64420, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 59h:44m:27s remains)
INFO - root - 2017-12-10 23:33:48.305989: step 64430, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 59h:48m:35s remains)
INFO - root - 2017-12-10 23:33:56.199242: step 64440, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 59h:47m:24s remains)
INFO - root - 2017-12-10 23:34:04.111876: step 64450, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 60h:06m:23s remains)
INFO - root - 2017-12-10 23:34:12.109210: step 64460, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 59h:46m:58s remains)
INFO - root - 2017-12-10 23:34:19.777921: step 64470, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.791 sec/batch; 58h:51m:28s remains)
INFO - root - 2017-12-10 23:34:27.615203: step 64480, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 58h:00m:24s remains)
INFO - root - 2017-12-10 23:34:35.516114: step 64490, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 59h:08m:54s remains)
INFO - root - 2017-12-10 23:34:43.290803: step 64500, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 58h:24m:33s remains)
2017-12-10 23:34:44.148105: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14642307 0.15765858 0.16572358 0.17794541 0.19177827 0.19705977 0.18678276 0.1668514 0.1528395 0.15260267 0.1656107 0.18548252 0.19575293 0.1902945 0.17022245][0.14000027 0.14861347 0.15184881 0.15966348 0.17208943 0.18010302 0.17601039 0.16682744 0.16573343 0.17690499 0.1995925 0.22753027 0.2426967 0.23565792 0.21001366][0.12494335 0.1292177 0.12745394 0.13086855 0.14231642 0.15461664 0.15964806 0.1640173 0.17692152 0.19833849 0.22730562 0.25848511 0.27470845 0.26590639 0.23800227][0.1189024 0.11984377 0.114722 0.11576653 0.12794149 0.14593805 0.16049637 0.17669657 0.19935752 0.22571373 0.25427988 0.2810649 0.29263011 0.28123632 0.25489768][0.12508769 0.12485125 0.11839394 0.1188032 0.13243003 0.1555661 0.17730759 0.20076597 0.22663783 0.25073123 0.27185988 0.28740686 0.28950465 0.27481648 0.25285533][0.13482708 0.13578783 0.13030453 0.13202861 0.14786251 0.17473553 0.2002414 0.2252392 0.24726313 0.26235735 0.27048257 0.270718 0.26110193 0.24361394 0.22764049][0.14271951 0.14689547 0.14456604 0.14910729 0.16693377 0.19460927 0.21939538 0.24013807 0.25232068 0.25366762 0.24575327 0.22952467 0.20837098 0.18877076 0.17821091][0.14669952 0.15494315 0.15645373 0.16327931 0.18029124 0.2036075 0.22186489 0.2330063 0.23193419 0.21855262 0.1957569 0.16608609 0.13655198 0.11623137 0.10971293][0.13924903 0.15043448 0.15426239 0.16111031 0.1739637 0.18890584 0.19727537 0.19752356 0.18516672 0.16194446 0.13112369 0.095654264 0.0638878 0.044976033 0.041623686][0.11688561 0.12890369 0.13301158 0.13743758 0.14420672 0.14963865 0.14795884 0.13903521 0.1203398 0.094162084 0.063509308 0.030730078 0.0032611829 -0.011669448 -0.012412262][0.0799041 0.089902341 0.092453405 0.093737967 0.095389441 0.093920924 0.085742205 0.072369054 0.05349138 0.031160958 0.0071376404 -0.017047593 -0.035953049 -0.0452274 -0.04416341][0.036637712 0.041918788 0.041513503 0.039718274 0.037978847 0.033349436 0.023683393 0.011448113 -0.002370018 -0.016575558 -0.030705139 -0.044031732 -0.053311251 -0.056624454 -0.0540991][-0.0026566708 -0.002284074 -0.0054210932 -0.0093916683 -0.012724059 -0.017440744 -0.024836434 -0.03270511 -0.039689049 -0.04545293 -0.050365083 -0.054228917 -0.055628832 -0.053979192 -0.049979579][-0.0330277 -0.036177255 -0.040834658 -0.045532871 -0.048781641 -0.051724497 -0.055315141 -0.058278535 -0.059419498 -0.058959652 -0.057492889 -0.055209283 -0.051869035 -0.047297843 -0.042373065][-0.050672751 -0.055866707 -0.060722347 -0.0650258 -0.067492835 -0.0685609 -0.069045737 -0.068419978 -0.0660272 -0.062387254 -0.0580465 -0.053412445 -0.048668008 -0.043641914 -0.039121918]]...]
INFO - root - 2017-12-10 23:34:51.977537: step 64510, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 58h:03m:12s remains)
INFO - root - 2017-12-10 23:34:59.833993: step 64520, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 57h:42m:44s remains)
INFO - root - 2017-12-10 23:35:07.639831: step 64530, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 59h:57m:53s remains)
INFO - root - 2017-12-10 23:35:15.521754: step 64540, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 59h:45m:21s remains)
INFO - root - 2017-12-10 23:35:23.203264: step 64550, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 58h:31m:20s remains)
INFO - root - 2017-12-10 23:35:31.158274: step 64560, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 58h:03m:29s remains)
INFO - root - 2017-12-10 23:35:38.916002: step 64570, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 59h:48m:47s remains)
INFO - root - 2017-12-10 23:35:46.796872: step 64580, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.809 sec/batch; 60h:12m:29s remains)
INFO - root - 2017-12-10 23:35:54.632538: step 64590, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 58h:17m:08s remains)
INFO - root - 2017-12-10 23:36:02.575845: step 64600, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 57h:13m:33s remains)
2017-12-10 23:36:03.651839: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010995499 0.088056274 0.19166185 0.29627237 0.37509644 0.42229161 0.43718034 0.41705659 0.3710762 0.32630977 0.29905012 0.28738579 0.28868592 0.29537538 0.29548153][0.013628724 0.097282715 0.20843077 0.32152566 0.41036651 0.4697926 0.49532405 0.48083121 0.43426356 0.38627204 0.35455945 0.33772537 0.33343637 0.33529463 0.33404574][0.0087378239 0.09346395 0.20558652 0.3203907 0.41443625 0.48433346 0.52233279 0.517243 0.47463608 0.4265328 0.39185804 0.36938852 0.35585511 0.34810439 0.34492147][0.001410965 0.083990261 0.1941375 0.30801407 0.40542641 0.48545766 0.53907943 0.54884732 0.51572865 0.47030306 0.43325704 0.40367195 0.3758617 0.35278228 0.34664595][-0.0020811616 0.079327524 0.1899 0.304585 0.40531495 0.49543154 0.56715906 0.59459925 0.57100403 0.52477324 0.48123157 0.44144166 0.39604706 0.35522023 0.34798306][0.00080935674 0.086688675 0.20434207 0.32507038 0.430481 0.52949113 0.616953 0.65755135 0.63737583 0.58425081 0.5313195 0.48324293 0.42657557 0.37594363 0.37357205][0.0087006381 0.10494044 0.23649819 0.36949971 0.48252547 0.58851165 0.6853019 0.73007679 0.70513332 0.63988918 0.57778615 0.52811122 0.47287768 0.42515877 0.43332317][0.017218538 0.12499039 0.27147919 0.41765937 0.53773624 0.645334 0.7420662 0.78284621 0.75039738 0.67722142 0.61458457 0.57445806 0.53378516 0.49920911 0.518666][0.020810701 0.13459632 0.28883937 0.44125211 0.56215692 0.66328377 0.75084782 0.78419006 0.74984705 0.68164408 0.63096106 0.60875815 0.58797032 0.568306 0.59265941][0.020112718 0.13368836 0.2870397 0.43698981 0.55264592 0.64293069 0.71755242 0.74409819 0.71422178 0.66048628 0.62737012 0.6220634 0.61561918 0.60321695 0.62030083][0.014899293 0.12111513 0.26440802 0.40372303 0.51123226 0.59373832 0.66205972 0.68990028 0.6711973 0.6353209 0.6175946 0.62117094 0.61658281 0.5987283 0.5961774][0.0069266665 0.10061949 0.22836572 0.35366353 0.45511615 0.538981 0.61363333 0.65293 0.64715922 0.6215564 0.60487342 0.59942257 0.57765752 0.5393827 0.5104385][0.0034328615 0.085603029 0.19922304 0.31144288 0.40711373 0.49367988 0.5765335 0.62608725 0.62547344 0.59691274 0.566772 0.5391019 0.48970425 0.42711931 0.37783712][0.0039829486 0.078460433 0.18234742 0.28370765 0.36907503 0.44760048 0.5250991 0.57081705 0.56320357 0.52212048 0.47392142 0.425553 0.35671264 0.28252938 0.22855303][0.00080624392 0.0693713 0.16557048 0.256468 0.32643482 0.38569167 0.44359091 0.47197953 0.44997981 0.39601818 0.33638552 0.27977893 0.20969941 0.14217541 0.098223045]]...]
INFO - root - 2017-12-10 23:36:11.656716: step 64610, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 60h:01m:45s remains)
INFO - root - 2017-12-10 23:36:19.553652: step 64620, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 58h:18m:44s remains)
INFO - root - 2017-12-10 23:36:27.295495: step 64630, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 57h:55m:04s remains)
INFO - root - 2017-12-10 23:36:35.203432: step 64640, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 58h:19m:22s remains)
INFO - root - 2017-12-10 23:36:43.176777: step 64650, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 59h:01m:53s remains)
INFO - root - 2017-12-10 23:36:50.783174: step 64660, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 58h:14m:31s remains)
INFO - root - 2017-12-10 23:36:58.745099: step 64670, loss = 0.67, batch loss = 0.61 (9.8 examples/sec; 0.812 sec/batch; 60h:25m:57s remains)
INFO - root - 2017-12-10 23:37:06.641341: step 64680, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 57h:37m:01s remains)
INFO - root - 2017-12-10 23:37:14.440697: step 64690, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 56h:59m:32s remains)
INFO - root - 2017-12-10 23:37:22.377980: step 64700, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.790 sec/batch; 58h:46m:53s remains)
2017-12-10 23:37:23.214895: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.016666329 0.072702453 0.13294573 0.18893218 0.2350838 0.2696932 0.29141763 0.29485518 0.276631 0.23864436 0.1914258 0.15420905 0.14867549 0.17583311 0.22484066][0.091903023 0.14618683 0.21152966 0.28251153 0.34390023 0.38344693 0.39508939 0.37534136 0.32592627 0.25414106 0.18047303 0.12999023 0.12309411 0.15505886 0.20989226][0.17920974 0.21432173 0.26848334 0.34302369 0.4142375 0.45804146 0.46316385 0.42836794 0.35961077 0.26732352 0.1787768 0.12227746 0.11432564 0.14385298 0.19201387][0.25990534 0.26376456 0.29170895 0.3549363 0.42560434 0.47120821 0.47653079 0.44127128 0.37286323 0.28278711 0.20042424 0.15239573 0.14899406 0.17381766 0.20759286][0.33943281 0.30986694 0.30676329 0.35021666 0.41307759 0.45994014 0.47445631 0.45393336 0.40319303 0.3335467 0.27343842 0.24496971 0.25111449 0.2712239 0.28774983][0.40471047 0.35252336 0.32707924 0.35412678 0.40869424 0.45815793 0.48816079 0.49025637 0.46345472 0.4202511 0.38802406 0.38333839 0.40168813 0.41802716 0.4172684][0.4463937 0.38808975 0.3558341 0.37431887 0.42108914 0.47076327 0.51402712 0.5358628 0.52854711 0.50760955 0.49948969 0.51519215 0.54266834 0.553678 0.536003][0.44682631 0.39658129 0.37105232 0.38904786 0.42973343 0.47578573 0.52416712 0.55514908 0.55609536 0.54614604 0.55136657 0.57900476 0.61040336 0.61675966 0.5889008][0.39222857 0.35779914 0.34749705 0.37109688 0.40805081 0.44691089 0.48986539 0.51627564 0.51309365 0.50274396 0.51091671 0.54352492 0.57778329 0.58654696 0.56187731][0.29189205 0.27414382 0.27858922 0.30696645 0.33938631 0.36790541 0.39741462 0.41054398 0.39812696 0.38356611 0.39088404 0.42657191 0.46695834 0.48639351 0.476024][0.17387751 0.16673191 0.17745633 0.20299016 0.22629336 0.24173436 0.2542952 0.25249562 0.23319952 0.21843044 0.22916505 0.27292305 0.32691112 0.36504784 0.37550637][0.079515614 0.075802252 0.083949231 0.099394791 0.11001673 0.11193023 0.10905174 0.09626174 0.075516634 0.065931216 0.085386783 0.14262976 0.21590464 0.27708673 0.3106207][0.023241289 0.02064731 0.025570611 0.033396479 0.035918545 0.029453039 0.017023388 -0.0014848481 -0.021592423 -0.025696151 0.0027947351 0.074559964 0.168499 0.25306168 0.30830494][-0.0059990087 -0.0063613513 0.00070902257 0.0099512869 0.014404877 0.0087608192 -0.0041689151 -0.021823052 -0.039971583 -0.040271226 -0.0048652329 0.078591563 0.18877263 0.29048243 0.35984918][-0.024196779 -0.021682557 -0.0087646637 0.0088162487 0.023568792 0.027418664 0.02315844 0.013709534 0.0014543763 0.0062293056 0.048132796 0.13891348 0.25623855 0.36342424 0.43466896]]...]
INFO - root - 2017-12-10 23:37:30.855938: step 64710, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 57h:58m:48s remains)
INFO - root - 2017-12-10 23:37:38.873635: step 64720, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.834 sec/batch; 62h:00m:09s remains)
INFO - root - 2017-12-10 23:37:46.737293: step 64730, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.801 sec/batch; 59h:35m:06s remains)
INFO - root - 2017-12-10 23:37:54.542515: step 64740, loss = 0.69, batch loss = 0.63 (10.7 examples/sec; 0.747 sec/batch; 55h:33m:43s remains)
INFO - root - 2017-12-10 23:38:02.089133: step 64750, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 59h:00m:09s remains)
INFO - root - 2017-12-10 23:38:09.986848: step 64760, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 58h:39m:06s remains)
INFO - root - 2017-12-10 23:38:17.825556: step 64770, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 57h:10m:20s remains)
INFO - root - 2017-12-10 23:38:25.724953: step 64780, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 59h:28m:13s remains)
INFO - root - 2017-12-10 23:38:33.489057: step 64790, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 60h:27m:40s remains)
INFO - root - 2017-12-10 23:38:41.411053: step 64800, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 56h:48m:43s remains)
2017-12-10 23:38:42.311414: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11437438 0.14047785 0.15455298 0.15496223 0.14415325 0.13018699 0.11728589 0.10687859 0.095353015 0.078849882 0.057033475 0.028999643 -0.0011092587 -0.028415298 -0.047965236][0.13636234 0.17614372 0.20294577 0.21383467 0.20803803 0.19053258 0.16716883 0.14356688 0.11893705 0.091022521 0.06067583 0.027523011 -0.0046959156 -0.032677695 -0.051454823][0.14748068 0.20150666 0.2434148 0.26754567 0.26958081 0.25111249 0.21900341 0.18210953 0.14338264 0.10214464 0.061475419 0.022377403 -0.012014749 -0.039621137 -0.056417584][0.16039225 0.2292899 0.28709748 0.32507774 0.33600062 0.3200089 0.28423259 0.23834211 0.1872659 0.13273737 0.079982445 0.031109018 -0.010183487 -0.041435059 -0.058935221][0.17931114 0.2623505 0.33575636 0.387642 0.40930438 0.40227383 0.37207046 0.32543215 0.26690561 0.20025778 0.13294905 0.068133563 0.011615792 -0.030993486 -0.055025667][0.19393234 0.28677258 0.37390491 0.44029483 0.47645435 0.4860042 0.47033536 0.43035379 0.36807814 0.29031202 0.20683973 0.12159269 0.044006579 -0.015191567 -0.049299229][0.20317176 0.30263656 0.4016501 0.4825688 0.5347113 0.56342572 0.56463468 0.53205436 0.46545175 0.3760345 0.27640271 0.17082633 0.072024189 -0.0032221072 -0.046276323][0.21406278 0.31709817 0.42548898 0.5190717 0.58496463 0.63077468 0.6473583 0.62157464 0.5515545 0.45292884 0.34033364 0.21669145 0.098029226 0.0074084173 -0.044540439][0.22161569 0.32047594 0.429401 0.52764308 0.60036272 0.65828997 0.68900579 0.67300516 0.60604388 0.50631636 0.3888047 0.25389105 0.12067401 0.017557587 -0.042109836][0.21307468 0.2982851 0.39603868 0.488028 0.55893546 0.62248492 0.66483915 0.66154045 0.60507 0.51304048 0.40014872 0.26487881 0.12797114 0.020544754 -0.042136095][0.18738438 0.25384027 0.33221632 0.40909877 0.4702507 0.531033 0.5781154 0.58477503 0.54076642 0.46139991 0.36100265 0.2369792 0.10984918 0.0099976044 -0.04767219][0.14900173 0.19549006 0.25092372 0.30682889 0.35187069 0.4017204 0.44485006 0.4555189 0.42315876 0.36031467 0.27966452 0.17740721 0.071769595 -0.010356362 -0.056471888][0.10089085 0.12802418 0.16000009 0.1926984 0.21888573 0.25381306 0.28835475 0.30014938 0.28004017 0.23716347 0.1809193 0.10620534 0.027775666 -0.032542996 -0.065217823][0.045980472 0.055678211 0.066531852 0.077711783 0.086296976 0.10590461 0.13013653 0.141794 0.13363703 0.11101031 0.0792925 0.032704204 -0.017839678 -0.055912413 -0.074652359][-0.0041797259 -0.0060455459 -0.0088008214 -0.011905791 -0.015228287 -0.007757 0.0067566186 0.016534455 0.016307248 0.008519249 -0.0045856121 -0.028482476 -0.055973992 -0.075592868 -0.082623325]]...]
INFO - root - 2017-12-10 23:38:50.283109: step 64810, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 59h:05m:54s remains)
INFO - root - 2017-12-10 23:38:58.124697: step 64820, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 59h:52m:12s remains)
INFO - root - 2017-12-10 23:39:06.010106: step 64830, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 58h:31m:11s remains)
INFO - root - 2017-12-10 23:39:13.708468: step 64840, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.754 sec/batch; 56h:04m:52s remains)
INFO - root - 2017-12-10 23:39:21.482421: step 64850, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 57h:52m:21s remains)
INFO - root - 2017-12-10 23:39:29.273462: step 64860, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 59h:16m:01s remains)
INFO - root - 2017-12-10 23:39:37.016991: step 64870, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 58h:04m:00s remains)
INFO - root - 2017-12-10 23:39:44.848726: step 64880, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 58h:39m:57s remains)
INFO - root - 2017-12-10 23:39:52.694833: step 64890, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 56h:31m:04s remains)
INFO - root - 2017-12-10 23:40:00.637013: step 64900, loss = 0.68, batch loss = 0.63 (9.8 examples/sec; 0.817 sec/batch; 60h:45m:16s remains)
2017-12-10 23:40:01.506216: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.099434882 0.12417095 0.1315975 0.11389215 0.076990686 0.037532039 0.0063038943 -0.014337434 -0.014453717 0.018300984 0.083511181 0.15010247 0.1881772 0.19075653 0.18193507][0.11137835 0.15132435 0.16949716 0.1549661 0.1159144 0.071443073 0.03515821 0.008560624 -0.0017743016 0.019958064 0.080471553 0.14855339 0.1901938 0.19561014 0.18976764][0.11540376 0.16879304 0.20122135 0.19769566 0.16544029 0.12593177 0.094110765 0.068301819 0.048245862 0.051141914 0.092207737 0.14582597 0.17833461 0.17948292 0.17341997][0.1286211 0.19645801 0.2482677 0.26511759 0.24881245 0.22309192 0.20285054 0.18266785 0.15349261 0.13134928 0.13883772 0.1596528 0.16588193 0.15076704 0.13696146][0.16400713 0.24890363 0.32560739 0.37205109 0.37986714 0.37383577 0.36715892 0.35215983 0.3118445 0.25888959 0.2212932 0.19350366 0.1578628 0.1144106 0.084549695][0.22182915 0.32354525 0.42553672 0.50442231 0.53929138 0.55317467 0.55653507 0.54213691 0.4882479 0.40298244 0.31697935 0.23500921 0.15210728 0.076132514 0.027436838][0.27851996 0.39183772 0.51153177 0.61499137 0.67083073 0.69725162 0.7023415 0.68257147 0.61531585 0.504064 0.37910384 0.25433186 0.1367873 0.039211992 -0.020131852][0.30382174 0.41814846 0.54089653 0.65161985 0.71424693 0.74151748 0.74013919 0.71106476 0.63389128 0.50948864 0.36655498 0.22538948 0.10009862 0.0039897617 -0.048135653][0.28123584 0.38185608 0.48874056 0.58520716 0.63884145 0.65742022 0.64576346 0.60763985 0.52772963 0.40767169 0.27326465 0.14694776 0.044698153 -0.023204759 -0.046518404][0.21301962 0.28738958 0.36368325 0.42980257 0.46335682 0.46863386 0.44764209 0.40499702 0.33254153 0.23525688 0.13462 0.050224207 -0.0047881855 -0.02447078 -0.0032728808][0.12199781 0.16588914 0.20773666 0.23937534 0.24997339 0.24221337 0.21551231 0.17521207 0.1204917 0.059540126 0.0081373351 -0.020384489 -0.018301785 0.015476197 0.082623504][0.038268343 0.055974703 0.069920592 0.074413434 0.067486055 0.051749568 0.024843724 -0.0069004693 -0.038740817 -0.060072746 -0.060223512 -0.033552133 0.019835016 0.095710881 0.19435424][-0.016912973 -0.017095292 -0.019832384 -0.029935621 -0.044626996 -0.060863528 -0.082211375 -0.1016361 -0.11087347 -0.096885242 -0.05235203 0.019398846 0.10733393 0.20258644 0.30672136][-0.041503251 -0.052250408 -0.062329769 -0.076679625 -0.091191716 -0.10255568 -0.11494208 -0.12077319 -0.11008323 -0.068080552 0.0096238041 0.11027461 0.21130048 0.30105251 0.38352174][-0.04497131 -0.061742656 -0.074497 -0.088242464 -0.0994868 -0.10473603 -0.1082971 -0.10240638 -0.077332027 -0.018280163 0.076169334 0.18589534 0.28019428 0.3470633 0.39187127]]...]
INFO - root - 2017-12-10 23:40:09.424064: step 64910, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.789 sec/batch; 58h:40m:19s remains)
INFO - root - 2017-12-10 23:40:17.266030: step 64920, loss = 0.70, batch loss = 0.65 (10.8 examples/sec; 0.742 sec/batch; 55h:07m:04s remains)
INFO - root - 2017-12-10 23:40:25.020750: step 64930, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 57h:37m:18s remains)
INFO - root - 2017-12-10 23:40:32.828397: step 64940, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 56h:53m:25s remains)
INFO - root - 2017-12-10 23:40:40.431240: step 64950, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 57h:48m:25s remains)
INFO - root - 2017-12-10 23:40:48.357421: step 64960, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 59h:34m:14s remains)
INFO - root - 2017-12-10 23:40:56.162603: step 64970, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 57h:39m:57s remains)
INFO - root - 2017-12-10 23:41:04.005037: step 64980, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 57h:09m:32s remains)
INFO - root - 2017-12-10 23:41:11.950658: step 64990, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 58h:48m:46s remains)
INFO - root - 2017-12-10 23:41:19.854686: step 65000, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 57h:53m:00s remains)
2017-12-10 23:41:20.685888: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.057305105 0.05829481 0.056603927 0.054254953 0.056325946 0.065878287 0.0809497 0.091679156 0.092743181 0.085998364 0.077169135 0.072416522 0.074086688 0.081415527 0.089622706][0.072928652 0.080577567 0.086910665 0.094909549 0.10720165 0.12396569 0.14003913 0.14360075 0.13219678 0.11236042 0.093964212 0.085326582 0.088547707 0.10066393 0.1127266][0.085364707 0.10313603 0.1216139 0.14320004 0.16820036 0.19447413 0.21289554 0.20866023 0.18129505 0.14255977 0.10803788 0.090046026 0.091390111 0.1070283 0.12385871][0.090845093 0.1214263 0.15470123 0.19046535 0.22812898 0.26488245 0.28789371 0.27886635 0.23741585 0.17933252 0.12525655 0.091723129 0.083969414 0.097317129 0.11663734][0.0949728 0.13790631 0.18423563 0.23026511 0.27611136 0.31987819 0.34654352 0.33570749 0.28597036 0.21448034 0.14393066 0.093187258 0.071084544 0.076122493 0.094241545][0.095774919 0.14709283 0.20145069 0.25206497 0.29985449 0.3437337 0.3689408 0.35650545 0.30509436 0.2307727 0.15305248 0.09095908 0.056803476 0.053036571 0.068286724][0.092013158 0.14581741 0.20288959 0.25398725 0.29886904 0.33596215 0.35311055 0.33680019 0.28800344 0.220149 0.14643814 0.083698012 0.0458539 0.037895374 0.050420243][0.085477374 0.13519095 0.1901934 0.24043426 0.28250015 0.31216 0.31955421 0.29790729 0.25227132 0.19416007 0.13056011 0.07525339 0.041255333 0.033727076 0.044255923][0.087916292 0.12726022 0.17470182 0.22114179 0.25998932 0.28362268 0.28342015 0.2575978 0.21460357 0.1654022 0.11338967 0.069617875 0.044443063 0.040877026 0.050345629][0.10755111 0.1329876 0.16723073 0.20449983 0.23715156 0.25556269 0.25188941 0.22592185 0.18642978 0.14387433 0.10167244 0.069814809 0.055670962 0.059026863 0.06948822][0.14422671 0.15499698 0.17280498 0.19693574 0.22089541 0.23486276 0.23148942 0.20999993 0.1768589 0.14064354 0.10604688 0.083546221 0.078625217 0.08845073 0.099985063][0.19217309 0.1920778 0.19417015 0.20444685 0.21889615 0.22855413 0.22669251 0.21176918 0.18711488 0.15825769 0.13040318 0.11471829 0.11550001 0.12815705 0.13782535][0.24223699 0.23744446 0.22804621 0.2263722 0.23076113 0.2337653 0.23049811 0.220374 0.20456919 0.18468285 0.16425925 0.15417801 0.15795569 0.17030254 0.17672902][0.28238234 0.27821827 0.26280892 0.25251925 0.24733278 0.24187183 0.23359691 0.22411536 0.21376039 0.20108646 0.18751246 0.18282536 0.189718 0.20243068 0.20866118][0.298126 0.30005369 0.28565639 0.2724452 0.2612533 0.24893533 0.23529544 0.22380991 0.21432951 0.2040699 0.19385795 0.19307408 0.20371713 0.21921317 0.22951768]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 23:41:28.332992: step 65010, loss = 0.69, batch loss = 0.63 (13.2 examples/sec; 0.607 sec/batch; 45h:05m:24s remains)
INFO - root - 2017-12-10 23:41:36.181202: step 65020, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.768 sec/batch; 57h:04m:47s remains)
INFO - root - 2017-12-10 23:41:43.827394: step 65030, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 57h:18m:56s remains)
INFO - root - 2017-12-10 23:41:51.697987: step 65040, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 59h:01m:34s remains)
INFO - root - 2017-12-10 23:41:59.526954: step 65050, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 60h:05m:36s remains)
INFO - root - 2017-12-10 23:42:07.407773: step 65060, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 57h:01m:56s remains)
INFO - root - 2017-12-10 23:42:15.178810: step 65070, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 57h:42m:47s remains)
INFO - root - 2017-12-10 23:42:23.123347: step 65080, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 56h:23m:14s remains)
INFO - root - 2017-12-10 23:42:31.022629: step 65090, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 59h:51m:10s remains)
INFO - root - 2017-12-10 23:42:38.659407: step 65100, loss = 0.70, batch loss = 0.64 (13.0 examples/sec; 0.617 sec/batch; 45h:51m:10s remains)
2017-12-10 23:42:39.551343: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.081459157 0.15045439 0.21593662 0.26099476 0.28139311 0.28224093 0.2683979 0.25467527 0.25891373 0.28112537 0.30291903 0.30452204 0.28169411 0.23323423 0.16252744][0.10706881 0.19247483 0.27361995 0.33073419 0.35775453 0.35917756 0.3411988 0.32360747 0.32906556 0.357112 0.38580257 0.39061144 0.36480364 0.30618367 0.21997255][0.12327603 0.22069965 0.31413078 0.38238743 0.41714537 0.42108119 0.4011237 0.380464 0.3845517 0.41242823 0.44151205 0.44589663 0.41686904 0.35070986 0.25370896][0.13230571 0.23884343 0.34269214 0.42211264 0.46628255 0.47509879 0.45640111 0.43523502 0.437715 0.46226943 0.4879401 0.48892921 0.45446837 0.38027403 0.27404678][0.13137901 0.24372083 0.3578046 0.45144978 0.5102675 0.52991664 0.518716 0.50179136 0.5048089 0.52637696 0.54752213 0.54242313 0.49779391 0.41086483 0.29214436][0.12149535 0.23657033 0.36091411 0.47202602 0.55113089 0.58888149 0.59200668 0.58293873 0.58646828 0.60251749 0.61481911 0.5983178 0.53785086 0.43423316 0.30113629][0.11461461 0.23299952 0.36829826 0.49766403 0.59860438 0.65729731 0.67716503 0.67587072 0.67724389 0.68279731 0.68003517 0.646809 0.56854504 0.44992378 0.30555704][0.11656327 0.23883714 0.38299644 0.52519381 0.64100415 0.7140094 0.74490368 0.747479 0.74518424 0.74076933 0.72496009 0.67906082 0.5897631 0.46402258 0.31564406][0.12002968 0.2421973 0.38833788 0.53331071 0.652814 0.72991389 0.7635181 0.76546758 0.76062769 0.75410026 0.73658359 0.68928885 0.59870923 0.47340819 0.32622546][0.11698753 0.23157936 0.36845472 0.50239509 0.61181569 0.68144047 0.70844066 0.7037127 0.69490087 0.69080561 0.67979461 0.6405282 0.55907279 0.44482809 0.30881947][0.10475197 0.20223005 0.31631431 0.4242858 0.51015455 0.56205106 0.57540137 0.56036115 0.54608107 0.54536575 0.54476279 0.52102566 0.46025178 0.37063459 0.25940046][0.084227934 0.15890478 0.24276777 0.31712165 0.37296462 0.40291676 0.40206143 0.37945533 0.36256996 0.36676255 0.37800777 0.37343472 0.33944425 0.28143048 0.20140117][0.061540354 0.11457322 0.17038237 0.21348725 0.24075089 0.24998255 0.23816563 0.2134314 0.19971015 0.21182795 0.23456419 0.24554285 0.23190452 0.19751036 0.14141789][0.033230733 0.069738634 0.10648025 0.13001058 0.13957193 0.13632315 0.12001525 0.098612465 0.092643164 0.11397492 0.14521521 0.16343115 0.15671076 0.12917392 0.08121144][-0.0012482816 0.023478474 0.0498715 0.066432096 0.071777061 0.06756635 0.054967694 0.040908981 0.041740444 0.065847658 0.096113928 0.11116346 0.10109757 0.070837058 0.023692798]]...]
INFO - root - 2017-12-10 23:42:47.296089: step 65110, loss = 0.69, batch loss = 0.64 (9.0 examples/sec; 0.891 sec/batch; 66h:12m:00s remains)
INFO - root - 2017-12-10 23:42:55.229742: step 65120, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 58h:59m:04s remains)
INFO - root - 2017-12-10 23:43:03.079065: step 65130, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 57h:29m:41s remains)
INFO - root - 2017-12-10 23:43:11.107843: step 65140, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 58h:21m:15s remains)
INFO - root - 2017-12-10 23:43:19.001249: step 65150, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 56h:51m:56s remains)
INFO - root - 2017-12-10 23:43:26.876027: step 65160, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 60h:10m:59s remains)
INFO - root - 2017-12-10 23:43:34.740388: step 65170, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 58h:28m:40s remains)
INFO - root - 2017-12-10 23:43:42.742529: step 65180, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 58h:02m:29s remains)
INFO - root - 2017-12-10 23:43:50.135127: step 65190, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 56h:45m:53s remains)
INFO - root - 2017-12-10 23:43:58.097726: step 65200, loss = 0.68, batch loss = 0.63 (9.5 examples/sec; 0.838 sec/batch; 62h:14m:11s remains)
2017-12-10 23:43:58.975963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.080807343 -0.081472889 -0.077394873 -0.071606562 -0.065695852 -0.060991939 -0.057992239 -0.058603305 -0.063900657 -0.073035143 -0.083324052 -0.092267774 -0.098821245 -0.10142965 -0.09913709][-0.07289198 -0.068589382 -0.057173602 -0.043259878 -0.03047619 -0.020813288 -0.014252015 -0.01332364 -0.020954125 -0.036216248 -0.054785725 -0.072952271 -0.089079961 -0.10009214 -0.10287748][-0.05395345 -0.04166536 -0.018498059 0.0088530695 0.033664126 0.053205952 0.068433031 0.074585438 0.065009952 0.040717237 0.0089665521 -0.024394887 -0.057304177 -0.083804429 -0.097271487][-0.027240075 -0.0028980447 0.03752818 0.0844404 0.12691639 0.16235045 0.19275571 0.20907903 0.19895643 0.16282502 0.11311322 0.05780213 -0.00044127085 -0.051268894 -0.082345471][0.004623848 0.045509886 0.10860004 0.18056074 0.24591218 0.30354971 0.35528138 0.38553706 0.3755129 0.32563156 0.25409383 0.17028578 0.078658946 -0.0039096302 -0.05816086][0.036409996 0.097158976 0.18556215 0.28419918 0.37384537 0.45689711 0.53172266 0.57482666 0.5627448 0.49786493 0.40316835 0.28765944 0.15965049 0.044403233 -0.032652039][0.065737687 0.14835085 0.26209521 0.38480994 0.49471474 0.59903187 0.69048333 0.73849761 0.71989346 0.64125013 0.52761745 0.38412949 0.22446766 0.08323577 -0.010531983][0.091460772 0.19629176 0.33225587 0.47207221 0.59227115 0.70434135 0.79631168 0.83587784 0.80660665 0.72010511 0.59871709 0.43944424 0.26075226 0.1057873 0.004300171][0.11003859 0.23285452 0.384068 0.53194892 0.65154439 0.75641447 0.83187228 0.85070086 0.8085379 0.72143817 0.60463411 0.44453761 0.26305571 0.1085545 0.0089732213][0.10912223 0.23917793 0.39395443 0.53960121 0.65060395 0.73901707 0.78853536 0.78128505 0.72689861 0.64461088 0.54062676 0.39266774 0.22441116 0.084995776 -0.002180008][0.080693364 0.20273706 0.34694636 0.48040602 0.57787943 0.64652908 0.66970658 0.63968205 0.57784879 0.50492203 0.41962722 0.29489189 0.15346521 0.04033237 -0.027073091][0.0347446 0.13507144 0.2557891 0.36762327 0.447086 0.49564877 0.49852031 0.45580378 0.39483649 0.33555204 0.2728537 0.17902061 0.072370887 -0.010315308 -0.056743845][-0.015647192 0.054799333 0.14352666 0.22714607 0.28598672 0.31756833 0.31033772 0.2681554 0.21694361 0.1733855 0.13205619 0.069362827 -0.0022388 -0.05638653 -0.084496461][-0.059550624 -0.019654335 0.036401935 0.09173087 0.13157798 0.15194532 0.14467207 0.11354168 0.077721775 0.048808329 0.02343752 -0.014749993 -0.05806699 -0.089902416 -0.10443018][-0.089014269 -0.074091293 -0.045397073 -0.0140212 0.010399885 0.024046043 0.021473095 0.0046254396 -0.015422517 -0.032215495 -0.046798319 -0.068108179 -0.091472261 -0.1076255 -0.11302332]]...]
INFO - root - 2017-12-10 23:44:06.850309: step 65210, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 59h:56m:09s remains)
INFO - root - 2017-12-10 23:44:14.720676: step 65220, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 59h:51m:42s remains)
INFO - root - 2017-12-10 23:44:22.503038: step 65230, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.785 sec/batch; 58h:15m:28s remains)
INFO - root - 2017-12-10 23:44:30.245228: step 65240, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.759 sec/batch; 56h:22m:54s remains)
INFO - root - 2017-12-10 23:44:38.041413: step 65250, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 58h:49m:28s remains)
INFO - root - 2017-12-10 23:44:45.806991: step 65260, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 59h:14m:29s remains)
INFO - root - 2017-12-10 23:44:53.433094: step 65270, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 59h:53m:33s remains)
INFO - root - 2017-12-10 23:45:01.015034: step 65280, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 58h:40m:24s remains)
INFO - root - 2017-12-10 23:45:08.871910: step 65290, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 57h:18m:54s remains)
INFO - root - 2017-12-10 23:45:16.727549: step 65300, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.751 sec/batch; 55h:45m:13s remains)
2017-12-10 23:45:17.557296: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12300248 0.09741541 0.065555669 0.037683617 0.021342119 0.025021378 0.052891031 0.10509523 0.17213884 0.24377394 0.30359578 0.33464387 0.3326169 0.30368641 0.25920513][0.09289255 0.079908855 0.06297598 0.048460077 0.042238932 0.052442476 0.085117072 0.14288978 0.21528362 0.28947696 0.34731102 0.37072715 0.35466436 0.30986214 0.25250891][0.082958117 0.089272946 0.096677624 0.10627791 0.11659007 0.13097291 0.15709408 0.20351671 0.26250771 0.32105619 0.36303347 0.37313509 0.34631228 0.29394555 0.23579004][0.10153717 0.12707542 0.16082187 0.19924675 0.22946113 0.24672736 0.25886282 0.28125033 0.31169027 0.33956727 0.3544952 0.34653756 0.31201765 0.26152086 0.21415754][0.12763207 0.17108439 0.22740252 0.29294875 0.34438121 0.36658865 0.36453551 0.35929939 0.35593435 0.34672338 0.32791904 0.29753542 0.25526559 0.2104401 0.17828901][0.15411848 0.21329693 0.28617492 0.37265563 0.44306114 0.47257224 0.46091709 0.43230441 0.39821061 0.35373622 0.30143672 0.24806449 0.19753069 0.15673454 0.13582404][0.17639963 0.24729827 0.330155 0.42839566 0.51074326 0.54715288 0.53163493 0.48814696 0.43202347 0.36115488 0.28302222 0.21104063 0.1531768 0.11388858 0.098001555][0.18657087 0.26450688 0.34973019 0.44829512 0.53169107 0.57060063 0.55546921 0.50736457 0.44296569 0.3600657 0.26872313 0.18595348 0.123628 0.084750451 0.068677336][0.17605835 0.25342038 0.33280391 0.42091292 0.49486074 0.53027254 0.51686108 0.47202694 0.41129154 0.33047071 0.23959909 0.155959 0.093931414 0.056126069 0.038056385][0.15637626 0.22633812 0.29296869 0.36298454 0.42048866 0.44738412 0.43427566 0.39560464 0.3449097 0.27618948 0.19687742 0.12207785 0.06637527 0.032388546 0.014464028][0.13898069 0.19691686 0.24808793 0.29768044 0.33635458 0.35252452 0.33860388 0.30609772 0.26667896 0.21449427 0.15350105 0.094368696 0.04977636 0.022973282 0.009533341][0.12462122 0.16831876 0.2039158 0.23405449 0.25518948 0.26130873 0.24684241 0.22047047 0.19186018 0.15640266 0.11491503 0.073606193 0.0423945 0.025340371 0.019366438][0.1151214 0.14489633 0.16609447 0.1800355 0.1873714 0.18577601 0.17147249 0.15100624 0.13216554 0.1114103 0.08726348 0.0628792 0.045351259 0.038170442 0.038970161][0.10667044 0.12386749 0.13312577 0.13550206 0.13389124 0.12789194 0.11484525 0.099831276 0.0886576 0.078664646 0.067175426 0.055603635 0.048783768 0.049117219 0.054331943][0.093706444 0.099909753 0.099600561 0.094232582 0.087629326 0.080156043 0.069675438 0.059821557 0.054341227 0.051053777 0.047165025 0.043703936 0.043973964 0.049070388 0.056518231]]...]
INFO - root - 2017-12-10 23:45:25.411336: step 65310, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 56h:23m:05s remains)
INFO - root - 2017-12-10 23:45:33.237766: step 65320, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 60h:04m:31s remains)
INFO - root - 2017-12-10 23:45:41.050350: step 65330, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 58h:34m:00s remains)
INFO - root - 2017-12-10 23:45:48.964350: step 65340, loss = 0.71, batch loss = 0.66 (9.7 examples/sec; 0.824 sec/batch; 61h:09m:36s remains)
INFO - root - 2017-12-10 23:45:56.682091: step 65350, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 56h:37m:13s remains)
INFO - root - 2017-12-10 23:46:04.520813: step 65360, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.790 sec/batch; 58h:38m:18s remains)
INFO - root - 2017-12-10 23:46:12.164115: step 65370, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 58h:32m:06s remains)
INFO - root - 2017-12-10 23:46:19.974670: step 65380, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 58h:58m:56s remains)
INFO - root - 2017-12-10 23:46:27.924218: step 65390, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 58h:54m:36s remains)
INFO - root - 2017-12-10 23:46:35.637883: step 65400, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 57h:56m:34s remains)
2017-12-10 23:46:36.599411: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11586481 0.11507092 0.11154781 0.10837894 0.10618012 0.10509631 0.10458257 0.10373983 0.10084093 0.095851965 0.09045358 0.086034507 0.083755374 0.084744573 0.090063423][0.14755464 0.14584722 0.14091593 0.1365467 0.13309199 0.13073421 0.12927131 0.1272655 0.12123987 0.1119581 0.10226952 0.093738228 0.087740608 0.086702928 0.093178794][0.17659268 0.1754857 0.17111951 0.16870055 0.16758445 0.16744255 0.16783912 0.16600612 0.15621009 0.14016862 0.12295943 0.10707613 0.093686789 0.087027662 0.091718733][0.20722605 0.20962696 0.21095093 0.21716739 0.22556087 0.23460679 0.2425115 0.24412614 0.23146792 0.20705995 0.1789657 0.15069999 0.12313732 0.10318024 0.098927617][0.24595432 0.25800508 0.27348036 0.29723373 0.3237049 0.34961683 0.36987966 0.37600482 0.35933641 0.32401797 0.28093079 0.23312901 0.18216932 0.13957322 0.11796095][0.28425187 0.31393757 0.35325611 0.40243527 0.45309317 0.50106454 0.53650087 0.54684 0.52447128 0.47705176 0.41677266 0.34428179 0.26309058 0.19082816 0.14596452][0.3139523 0.36786789 0.43794832 0.51613611 0.59226835 0.66291517 0.71227103 0.72323883 0.69026124 0.62728179 0.547341 0.44829273 0.3369408 0.23744451 0.17360054][0.32236025 0.39925948 0.49698624 0.59888595 0.69430083 0.78182155 0.83998305 0.84745681 0.80103236 0.72208619 0.62445939 0.50402868 0.3718282 0.25667664 0.18532158][0.27942452 0.37030727 0.48582754 0.60133654 0.70552135 0.79888165 0.85646683 0.85535073 0.79623574 0.706983 0.60148078 0.4745177 0.34023261 0.22869442 0.16640437][0.19190183 0.28172609 0.39741415 0.51020449 0.60898614 0.69458306 0.74208659 0.73062062 0.66540968 0.57672185 0.47717991 0.36214143 0.24614671 0.15708375 0.1194065][0.10084297 0.17307121 0.26855794 0.36032569 0.44006079 0.50727946 0.53987455 0.52106 0.45953688 0.38322586 0.30201298 0.21246748 0.12762892 0.071353704 0.0652914][0.050956052 0.095659591 0.15750517 0.21690398 0.2700614 0.31445462 0.33197635 0.31002784 0.25948355 0.20292744 0.14597477 0.085352056 0.032097135 0.0061070179 0.025824098][0.061596408 0.077053688 0.10190149 0.12705606 0.15407145 0.17830025 0.18582283 0.16702598 0.13317211 0.099784963 0.066646881 0.029592862 -0.0020434572 -0.011048554 0.017828582][0.10849559 0.10128867 0.096150741 0.09484902 0.10393003 0.11681136 0.1223727 0.1126653 0.098090976 0.086989112 0.073218942 0.050954577 0.028789327 0.022025682 0.042575382][0.15247072 0.13208288 0.10931939 0.093998775 0.09565749 0.10669009 0.11660214 0.1180481 0.12008648 0.12660247 0.12702525 0.11357281 0.094427519 0.08303225 0.087670438]]...]
INFO - root - 2017-12-10 23:46:44.468243: step 65410, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.788 sec/batch; 58h:26m:07s remains)
INFO - root - 2017-12-10 23:46:52.407413: step 65420, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 58h:01m:28s remains)
INFO - root - 2017-12-10 23:47:00.047044: step 65430, loss = 0.72, batch loss = 0.66 (10.7 examples/sec; 0.747 sec/batch; 55h:23m:35s remains)
INFO - root - 2017-12-10 23:47:07.844424: step 65440, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 57h:54m:37s remains)
INFO - root - 2017-12-10 23:47:15.729258: step 65450, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.812 sec/batch; 60h:12m:34s remains)
INFO - root - 2017-12-10 23:47:23.351285: step 65460, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 58h:00m:18s remains)
INFO - root - 2017-12-10 23:47:31.188753: step 65470, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 58h:03m:55s remains)
INFO - root - 2017-12-10 23:47:39.061128: step 65480, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 59h:47m:15s remains)
INFO - root - 2017-12-10 23:47:46.866682: step 65490, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 57h:45m:48s remains)
INFO - root - 2017-12-10 23:47:54.714614: step 65500, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.777 sec/batch; 57h:36m:37s remains)
2017-12-10 23:47:55.553725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0413689 -0.028678197 -0.0061098691 0.025574835 0.063699841 0.10264708 0.13444544 0.15216838 0.15735571 0.15462144 0.1490919 0.14340961 0.13877437 0.13765998 0.13744755][-0.040693115 -0.025395414 0.00016565705 0.035155702 0.076519638 0.11881912 0.15444987 0.17718659 0.18805744 0.18971972 0.18654969 0.1797194 0.17066348 0.16508842 0.16348097][-0.038267214 -0.020735426 0.00669146 0.04332399 0.085849226 0.12830888 0.16281345 0.18409428 0.19383729 0.19483542 0.19091961 0.18191446 0.16984962 0.16285297 0.16318202][-0.035151675 -0.016269436 0.011971165 0.049426567 0.09275353 0.13415568 0.16453858 0.17978777 0.18331964 0.17897159 0.17088753 0.15918331 0.14621006 0.14069363 0.14431678][-0.032424495 -0.012682872 0.016403016 0.055178531 0.10038798 0.14219426 0.16974153 0.17956187 0.17656119 0.16576283 0.15196021 0.13631596 0.12174392 0.11628711 0.11947261][-0.031112324 -0.010685639 0.019472621 0.060109749 0.10834408 0.1523876 0.18034169 0.18894827 0.18339433 0.16902499 0.15028138 0.12980378 0.11103101 0.10057031 0.096608557][-0.030911805 -0.010416065 0.020814819 0.06333144 0.11467077 0.16189277 0.19268341 0.20331459 0.19879484 0.18369603 0.16161332 0.13731709 0.11532735 0.10058695 0.090054311][-0.03145434 -0.0107174 0.022119207 0.06707643 0.12193012 0.17257597 0.20664683 0.22013804 0.21769439 0.20242228 0.1773904 0.15029141 0.12762234 0.11238532 0.10050101][-0.032878228 -0.012747762 0.020991247 0.0675 0.12463813 0.1776502 0.21442543 0.23101063 0.23192662 0.21875124 0.19428229 0.1684761 0.14884849 0.13657469 0.12694268][-0.036510874 -0.019155443 0.012251153 0.056130998 0.11068323 0.1626229 0.20106627 0.22267739 0.2313188 0.22737335 0.21216694 0.19429807 0.17996007 0.16951884 0.16001725][-0.042071041 -0.030078653 -0.0054096072 0.030090742 0.075648941 0.1215413 0.15955235 0.18751125 0.20777564 0.21898845 0.21960671 0.21370266 0.204149 0.19215183 0.17913611][-0.046876304 -0.040041585 -0.022688234 0.0029402506 0.036794446 0.073066071 0.10654119 0.13688481 0.16531824 0.18938337 0.20475052 0.21019603 0.20465739 0.19077268 0.17450532][-0.048556846 -0.043115802 -0.028162867 -0.0071022608 0.01910682 0.046877779 0.072969116 0.098737553 0.12507069 0.15050797 0.17058095 0.18117018 0.1778494 0.16322996 0.14564496][-0.0477951 -0.040361024 -0.022212833 0.0017407895 0.027986515 0.052850079 0.073282935 0.090931684 0.1065492 0.12119124 0.13388769 0.14141339 0.13775063 0.12368871 0.10682482][-0.045796495 -0.03498489 -0.011117549 0.019849129 0.051648211 0.078966022 0.097431831 0.10747463 0.10928221 0.10651067 0.10417852 0.10306834 0.097756311 0.0861857 0.0730685]]...]
INFO - root - 2017-12-10 23:48:03.131087: step 65510, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.763 sec/batch; 56h:36m:03s remains)
INFO - root - 2017-12-10 23:48:10.997839: step 65520, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 60h:21m:57s remains)
INFO - root - 2017-12-10 23:48:18.952883: step 65530, loss = 0.70, batch loss = 0.65 (9.8 examples/sec; 0.819 sec/batch; 60h:44m:14s remains)
INFO - root - 2017-12-10 23:48:26.800382: step 65540, loss = 0.71, batch loss = 0.66 (10.2 examples/sec; 0.788 sec/batch; 58h:24m:31s remains)
INFO - root - 2017-12-10 23:48:34.387647: step 65550, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 58h:45m:28s remains)
INFO - root - 2017-12-10 23:48:42.142293: step 65560, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 57h:48m:19s remains)
INFO - root - 2017-12-10 23:48:50.003108: step 65570, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 56h:51m:12s remains)
INFO - root - 2017-12-10 23:48:57.863525: step 65580, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 57h:53m:44s remains)
INFO - root - 2017-12-10 23:49:05.433627: step 65590, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 56h:35m:30s remains)
INFO - root - 2017-12-10 23:49:13.244613: step 65600, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 57h:53m:59s remains)
2017-12-10 23:49:14.085325: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.086760364 0.082601912 0.081507161 0.10450866 0.16141707 0.23421343 0.29590949 0.32087347 0.30209225 0.23861898 0.15117237 0.079676405 0.05483824 0.080472834 0.13189851][0.0969452 0.095668919 0.10449859 0.14816093 0.23734123 0.34647217 0.4383409 0.47706038 0.45272586 0.36718875 0.24837181 0.14929861 0.11027534 0.14085722 0.21040991][0.099675857 0.10438715 0.12642069 0.19032082 0.30574971 0.4432081 0.55885381 0.60884291 0.58040589 0.47998202 0.34106189 0.22428578 0.17505305 0.208355 0.29073703][0.10061437 0.11119349 0.14485475 0.22331068 0.35392237 0.50775778 0.63816911 0.69616425 0.66674584 0.56123966 0.41621524 0.29372913 0.24060288 0.27586323 0.36394852][0.10434746 0.12265909 0.16764155 0.25617543 0.39337283 0.553351 0.68970913 0.7512455 0.72446746 0.62345362 0.48331058 0.36214784 0.30736715 0.34151563 0.4270947][0.13200328 0.16422608 0.22177133 0.31561875 0.4503032 0.60460615 0.73466241 0.79177105 0.76920485 0.68121928 0.55479151 0.44012588 0.38575014 0.41662762 0.49287185][0.20364851 0.25287679 0.31840837 0.40724671 0.52742243 0.66532791 0.77940142 0.82615292 0.80743444 0.73589748 0.62758023 0.52342457 0.47317958 0.50149941 0.56617773][0.30905622 0.37168714 0.43596652 0.50938815 0.60618776 0.72066486 0.81223309 0.84364468 0.82486457 0.76676738 0.675697 0.58576256 0.54628289 0.57677644 0.63215566][0.4092958 0.47688073 0.53089356 0.5822891 0.6516493 0.73969537 0.80730063 0.82355046 0.80241257 0.7529301 0.67622375 0.60280508 0.57943374 0.61679113 0.66825956][0.46312416 0.52729785 0.56818181 0.59920353 0.64295644 0.70401663 0.74782366 0.74952787 0.72245896 0.67411131 0.60607839 0.54768336 0.54189813 0.58972186 0.64441031][0.45627859 0.509987 0.53733033 0.55203021 0.57490736 0.61192971 0.6354211 0.62642139 0.594481 0.5450055 0.48166159 0.43356344 0.43947679 0.49413332 0.55319643][0.38672546 0.42495596 0.43841061 0.43944031 0.444792 0.46036157 0.46653369 0.45072168 0.41800666 0.37123725 0.31481239 0.27598998 0.28811726 0.34336293 0.40206441][0.2629441 0.28151441 0.28116289 0.27111587 0.26267359 0.26041412 0.25286576 0.23378348 0.20545809 0.16865619 0.12689327 0.10140789 0.11753067 0.16766474 0.21986808][0.11922541 0.12016728 0.11052886 0.095575519 0.081488907 0.070641205 0.057662174 0.040423311 0.01969135 -0.0048829922 -0.031003302 -0.044552829 -0.028521312 0.011917279 0.054106437][-0.0052581294 -0.013910756 -0.025180336 -0.037507739 -0.048128247 -0.056738619 -0.066589624 -0.078889772 -0.093425609 -0.10998371 -0.12666428 -0.13503417 -0.12485597 -0.098408423 -0.069269806]]...]
INFO - root - 2017-12-10 23:49:21.925906: step 65610, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.788 sec/batch; 58h:25m:07s remains)
INFO - root - 2017-12-10 23:49:29.825576: step 65620, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 58h:32m:49s remains)
INFO - root - 2017-12-10 23:49:37.609547: step 65630, loss = 0.70, batch loss = 0.64 (10.8 examples/sec; 0.742 sec/batch; 55h:00m:23s remains)
INFO - root - 2017-12-10 23:49:45.255675: step 65640, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 56h:46m:29s remains)
INFO - root - 2017-12-10 23:49:53.087220: step 65650, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 57h:40m:29s remains)
INFO - root - 2017-12-10 23:50:00.821599: step 65660, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 58h:47m:46s remains)
INFO - root - 2017-12-10 23:50:08.558992: step 65670, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 59h:03m:37s remains)
INFO - root - 2017-12-10 23:50:16.449880: step 65680, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 57h:24m:55s remains)
INFO - root - 2017-12-10 23:50:24.198790: step 65690, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.754 sec/batch; 55h:54m:03s remains)
INFO - root - 2017-12-10 23:50:32.132464: step 65700, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 59h:04m:33s remains)
2017-12-10 23:50:33.045878: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16628779 0.16076958 0.12570339 0.072838537 0.024393914 -0.001860325 -0.0002579384 0.029857403 0.090877138 0.17376257 0.25278148 0.30819923 0.32651931 0.30309451 0.23814912][0.20286967 0.20287348 0.17414358 0.13033991 0.092699818 0.0754227 0.0801352 0.10700654 0.16167343 0.23900276 0.31664029 0.3736625 0.39567173 0.37392351 0.30584842][0.23206504 0.23929591 0.22283049 0.1974445 0.17980848 0.17660312 0.18331608 0.20008631 0.23718193 0.29675293 0.36257479 0.41461459 0.43851027 0.4227806 0.36188796][0.24511077 0.25777265 0.25225049 0.24502513 0.2479139 0.25988123 0.27109462 0.28053996 0.30071479 0.33952251 0.38695869 0.42672768 0.447404 0.43794626 0.39057612][0.23859675 0.25471771 0.25794044 0.26592535 0.28740406 0.31551033 0.33655658 0.34599441 0.35439923 0.37237123 0.39536676 0.41418508 0.42437226 0.419688 0.38860869][0.23368953 0.25014251 0.25806844 0.27659449 0.31214407 0.35382912 0.38554031 0.39844713 0.39897129 0.3973645 0.39393613 0.38843384 0.38518855 0.38308388 0.36739096][0.2485839 0.26219955 0.26909941 0.29045424 0.33064851 0.37682709 0.41319308 0.42901891 0.42677552 0.41374817 0.39344528 0.37275851 0.36201021 0.36225015 0.35634047][0.29398406 0.30162892 0.30052418 0.31407821 0.34645534 0.38559508 0.41820121 0.43449843 0.4338356 0.41921639 0.3940073 0.3686291 0.35440934 0.35250515 0.34663421][0.35232282 0.3546702 0.34283894 0.34248117 0.36005616 0.38576594 0.40903777 0.42136732 0.42043692 0.40577096 0.3797552 0.35343915 0.3355906 0.32708743 0.31564179][0.41155598 0.41016322 0.387018 0.37007987 0.36924654 0.37701041 0.38445881 0.38458228 0.37579 0.35676706 0.33025283 0.30629548 0.28888032 0.27671736 0.26173934][0.44835725 0.44354141 0.40974751 0.37777361 0.35961139 0.3489441 0.33725271 0.31950098 0.2970733 0.27109772 0.24590544 0.22885354 0.21806964 0.20825757 0.19431019][0.43353507 0.42512637 0.38424763 0.34231555 0.31191355 0.28729147 0.26019564 0.22786647 0.19514413 0.16603073 0.14546466 0.13755192 0.13574915 0.13164815 0.12171074][0.35990736 0.34891579 0.30620062 0.26129657 0.22608079 0.19516037 0.16082674 0.12222248 0.086887687 0.060444932 0.046605553 0.046214994 0.050641444 0.051117461 0.045322221][0.23407616 0.22177732 0.18299763 0.14268902 0.11086086 0.08261653 0.051107798 0.016464233 -0.013028652 -0.031934369 -0.038551934 -0.03415902 -0.026709272 -0.0233098 -0.02519317][0.098988369 0.085263 0.052779246 0.019974608 -0.0051138215 -0.025972413 -0.048102856 -0.071396478 -0.089175761 -0.097732104 -0.097241916 -0.089964695 -0.081618115 -0.076823674 -0.075925864]]...]
INFO - root - 2017-12-10 23:50:40.984622: step 65710, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 59h:04m:22s remains)
INFO - root - 2017-12-10 23:50:48.885846: step 65720, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 59h:59m:53s remains)
INFO - root - 2017-12-10 23:50:56.538168: step 65730, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 58h:04m:26s remains)
INFO - root - 2017-12-10 23:51:04.266102: step 65740, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 57h:46m:50s remains)
INFO - root - 2017-12-10 23:51:12.202288: step 65750, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.812 sec/batch; 60h:07m:51s remains)
INFO - root - 2017-12-10 23:51:20.047307: step 65760, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 56h:58m:59s remains)
INFO - root - 2017-12-10 23:51:27.837272: step 65770, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 58h:02m:32s remains)
INFO - root - 2017-12-10 23:51:35.681546: step 65780, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.773 sec/batch; 57h:15m:28s remains)
INFO - root - 2017-12-10 23:51:43.520500: step 65790, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 57h:22m:42s remains)
INFO - root - 2017-12-10 23:51:51.230172: step 65800, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.752 sec/batch; 55h:42m:53s remains)
2017-12-10 23:51:52.057117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.020857088 -0.018946102 -0.016479179 -0.014689486 -0.014457978 -0.016635081 -0.019690981 -0.020805705 -0.018391114 -0.013049844 -0.0052186456 0.0032999585 0.0071123806 0.002875081 -0.0086243637][-0.029809736 -0.027659586 -0.024652068 -0.022789225 -0.023671616 -0.027631786 -0.031318352 -0.029979002 -0.022301972 -0.0085954266 0.0090245092 0.026542561 0.034856237 0.028947657 0.011037083][-0.030961454 -0.027881088 -0.023805365 -0.021308091 -0.022531297 -0.027328584 -0.030552438 -0.025740571 -0.011213858 0.013180133 0.043035027 0.07098534 0.083639443 0.075220443 0.048981227][-0.011787894 -0.0037210218 0.0055213929 0.012226159 0.013454393 0.0098528164 0.0082303453 0.017044907 0.03958844 0.077400789 0.12319136 0.1643848 0.18161263 0.16742657 0.12616888][0.025637858 0.043548007 0.062983826 0.078901634 0.087275885 0.088655926 0.089824148 0.10032722 0.12644994 0.17374678 0.23455457 0.29020339 0.31303364 0.29255459 0.23415761][0.072206773 0.10783952 0.14696439 0.18166971 0.2053436 0.21688829 0.22148778 0.22808447 0.24641575 0.28835231 0.34978566 0.40805787 0.42804548 0.39695716 0.32268155][0.11375317 0.16926169 0.23057662 0.28678694 0.32916859 0.35470808 0.36473775 0.36464381 0.36618921 0.38853446 0.43329686 0.47764689 0.48353183 0.43977255 0.3580243][0.13882317 0.2077162 0.28264996 0.35056037 0.40287411 0.43663552 0.45036379 0.44379348 0.42837816 0.42806453 0.45002243 0.47413966 0.46359968 0.41200525 0.33403659][0.13373624 0.20420338 0.27924854 0.34463921 0.39398888 0.42668137 0.44069847 0.43095687 0.40641174 0.39152724 0.39658993 0.40507677 0.3837325 0.33113167 0.26436004][0.094146952 0.15382574 0.21729097 0.27069795 0.30984467 0.33644202 0.34874362 0.33896413 0.31254843 0.2913155 0.28683966 0.28673667 0.26229519 0.21551585 0.16506557][0.0405444 0.082732171 0.12995729 0.16979603 0.19935362 0.22130342 0.23425595 0.22924364 0.207657 0.18608299 0.17547685 0.16848087 0.14268775 0.10238688 0.066877812][0.00064038282 0.024300033 0.053881317 0.079770923 0.10015057 0.11769124 0.13146734 0.13297623 0.12062034 0.104585 0.092690289 0.081671663 0.056779049 0.023743022 0.00031297305][-0.019950977 -0.013133998 -0.00095946126 0.010750945 0.0214674 0.033634987 0.047074258 0.05528941 0.055351682 0.051118668 0.046043582 0.038582284 0.020004278 -0.0042897137 -0.020169718][-0.022295276 -0.027455773 -0.028829778 -0.028378835 -0.025416646 -0.017832261 -0.0052708881 0.0087633841 0.021009644 0.030441407 0.036409684 0.037096087 0.02713757 0.010115637 -0.0034920601][-0.00857288 -0.018790027 -0.027876981 -0.0348175 -0.037587754 -0.0337838 -0.022925528 -0.0061221304 0.013413131 0.032115243 0.046967611 0.055222813 0.052772004 0.040661998 0.026566893]]...]
INFO - root - 2017-12-10 23:51:59.780967: step 65810, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 57h:28m:20s remains)
INFO - root - 2017-12-10 23:52:07.632044: step 65820, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 58h:00m:35s remains)
INFO - root - 2017-12-10 23:52:15.318829: step 65830, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 56h:27m:20s remains)
INFO - root - 2017-12-10 23:52:23.191988: step 65840, loss = 0.67, batch loss = 0.61 (10.2 examples/sec; 0.788 sec/batch; 58h:22m:18s remains)
INFO - root - 2017-12-10 23:52:31.108206: step 65850, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 56h:26m:16s remains)
INFO - root - 2017-12-10 23:52:38.955328: step 65860, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 56h:40m:24s remains)
INFO - root - 2017-12-10 23:52:46.864184: step 65870, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 59h:29m:27s remains)
INFO - root - 2017-12-10 23:52:54.815658: step 65880, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 58h:54m:29s remains)
INFO - root - 2017-12-10 23:53:02.737199: step 65890, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 56h:53m:20s remains)
INFO - root - 2017-12-10 23:53:10.479181: step 65900, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 57h:21m:52s remains)
2017-12-10 23:53:11.283322: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.075660951 0.10771837 0.12532501 0.12679884 0.11859658 0.10182828 0.088590339 0.092166409 0.12074357 0.16753691 0.2147249 0.24073569 0.23702963 0.22538294 0.22452672][0.079500154 0.11493795 0.13442905 0.13781929 0.13204578 0.11846391 0.11018615 0.11628545 0.14188932 0.17906162 0.21284771 0.22976993 0.22396225 0.21352375 0.21430668][0.09777607 0.13514654 0.15368542 0.15674859 0.15237309 0.1412887 0.13548349 0.14066938 0.15989973 0.18559068 0.20642066 0.21776524 0.21573262 0.21252893 0.21841154][0.1264486 0.16418582 0.17909677 0.17866574 0.17254356 0.16073345 0.1540999 0.15677363 0.17095402 0.18995808 0.20495798 0.21681707 0.22236931 0.22836262 0.23846512][0.15405092 0.18946373 0.19910569 0.19358981 0.18370512 0.16904218 0.16077214 0.16315958 0.17817616 0.19910879 0.21747953 0.2355198 0.24936262 0.26185712 0.27041751][0.17235912 0.20271027 0.206771 0.19690108 0.18337104 0.16561973 0.15623818 0.16148849 0.18363762 0.21419494 0.24193376 0.26706606 0.28536981 0.29864991 0.30041659][0.18065625 0.20543824 0.20683751 0.19689523 0.18215959 0.16157813 0.14971778 0.15643677 0.18527451 0.22453156 0.25860828 0.28498134 0.30071005 0.30974779 0.30479658][0.18130347 0.20434527 0.2107686 0.20966837 0.19901271 0.17644832 0.15807918 0.15879574 0.18505538 0.22244026 0.25245476 0.27102008 0.27803966 0.28110987 0.27462032][0.17525525 0.20221528 0.22309129 0.24084504 0.24117304 0.21922527 0.19118015 0.17693989 0.18827197 0.2106147 0.22603811 0.23044008 0.22716038 0.22680424 0.2254622][0.16760717 0.20410088 0.2456502 0.28732252 0.30253297 0.28301924 0.24332651 0.20767744 0.19508357 0.19363867 0.18929283 0.17953601 0.16962762 0.17048734 0.17825468][0.1590431 0.20729838 0.26833686 0.32953635 0.35646078 0.3385796 0.28803664 0.23161943 0.19508131 0.17033909 0.14900236 0.13066028 0.12057329 0.12688306 0.14354014][0.14749758 0.20465061 0.27661303 0.34518421 0.37508732 0.35655844 0.29937959 0.23003727 0.17777042 0.13785121 0.10746127 0.088236034 0.083884977 0.098038621 0.12084268][0.12635824 0.18483131 0.25477245 0.31666085 0.34110114 0.32133666 0.26489806 0.19488339 0.13943848 0.096239045 0.066335373 0.052577294 0.056387641 0.077174015 0.10128405][0.092273168 0.14208542 0.19705927 0.24121931 0.25550506 0.2366128 0.18943617 0.13113968 0.084603779 0.0488909 0.026681924 0.021193994 0.032022208 0.055879224 0.0770098][0.04846482 0.082558721 0.11571661 0.13847975 0.14226575 0.12647423 0.093392126 0.053591125 0.0226359 0.00011551286 -0.011336294 -0.0090808915 0.0055986694 0.02837857 0.044289112]]...]
INFO - root - 2017-12-10 23:53:18.965798: step 65910, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 59h:09m:27s remains)
INFO - root - 2017-12-10 23:53:26.734108: step 65920, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 56h:55m:24s remains)
INFO - root - 2017-12-10 23:53:34.488850: step 65930, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 57h:42m:24s remains)
INFO - root - 2017-12-10 23:53:42.391995: step 65940, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 59h:12m:34s remains)
INFO - root - 2017-12-10 23:53:50.166698: step 65950, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 58h:04m:00s remains)
INFO - root - 2017-12-10 23:53:58.052859: step 65960, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 57h:11m:09s remains)
INFO - root - 2017-12-10 23:54:05.934485: step 65970, loss = 0.71, batch loss = 0.66 (10.8 examples/sec; 0.742 sec/batch; 54h:57m:13s remains)
INFO - root - 2017-12-10 23:54:13.798779: step 65980, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 58h:28m:14s remains)
INFO - root - 2017-12-10 23:54:21.265246: step 65990, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 57h:43m:41s remains)
INFO - root - 2017-12-10 23:54:29.171739: step 66000, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 59h:22m:59s remains)
2017-12-10 23:54:30.018189: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20272771 0.20593843 0.24067686 0.28675884 0.31554672 0.31029621 0.27112269 0.21643622 0.16957682 0.1472284 0.14050177 0.13723779 0.13480137 0.13914807 0.14955749][0.26987511 0.25291723 0.25700206 0.27668744 0.29278058 0.29120645 0.26959845 0.24056177 0.22382931 0.23138548 0.24634932 0.25223485 0.24885064 0.24709105 0.24959204][0.36944094 0.33369166 0.30933976 0.30528671 0.31189609 0.31519532 0.31056836 0.30505392 0.31479374 0.3459093 0.37404841 0.38075772 0.37004438 0.3604902 0.35833579][0.485485 0.43991944 0.40038007 0.387126 0.39643264 0.41090664 0.42260888 0.43192351 0.45261922 0.48550314 0.50450134 0.49531466 0.46836376 0.45091444 0.44967541][0.59119523 0.54831904 0.50875461 0.5008021 0.525128 0.55917251 0.58888841 0.60379463 0.61525369 0.62221664 0.60571414 0.56339139 0.51436979 0.49162924 0.49791414][0.66487747 0.63625306 0.60747808 0.61298186 0.65741551 0.71365958 0.75916111 0.76915729 0.75273854 0.71261305 0.64488482 0.56479251 0.49874154 0.4780679 0.49705172][0.6983422 0.6882962 0.67319393 0.68987405 0.74635249 0.81410611 0.86326569 0.85722721 0.80503958 0.71861851 0.61058056 0.51045626 0.44434226 0.43368426 0.46418971][0.68298411 0.69122565 0.68710649 0.70500445 0.75427914 0.811091 0.84426016 0.81511438 0.7369163 0.63169914 0.52061868 0.43463257 0.38988471 0.3934004 0.42666647][0.60565716 0.62833905 0.6321013 0.64163262 0.66502863 0.68907976 0.69034135 0.64068192 0.56115842 0.47903851 0.41173068 0.37530896 0.36844912 0.3850123 0.40849012][0.46463925 0.49524111 0.50503141 0.50363094 0.49562502 0.48073512 0.44999167 0.39398777 0.34058443 0.31515524 0.32215884 0.35328817 0.38914135 0.41467074 0.41994411][0.27915061 0.30995926 0.32334182 0.31536561 0.28617841 0.24596427 0.20143524 0.16128875 0.15417264 0.1968126 0.27928579 0.37195656 0.44345969 0.47487792 0.46283585][0.091574177 0.11366927 0.1259491 0.11648638 0.082166381 0.038350571 0.0031903535 -0.0041443752 0.03914506 0.13854542 0.27571082 0.411318 0.51017421 0.55077451 0.53161973][-0.055040423 -0.044440355 -0.036215328 -0.043473575 -0.071020938 -0.10378457 -0.11931299 -0.096532375 -0.020139482 0.10817892 0.2713536 0.4336406 0.56022692 0.62241447 0.61175162][-0.13273777 -0.12985379 -0.12462592 -0.12801871 -0.14526539 -0.16537166 -0.1680546 -0.13553157 -0.056673378 0.068521328 0.23285577 0.41204119 0.57053477 0.66611183 0.67649126][-0.14256983 -0.14140743 -0.13645381 -0.13611509 -0.14507008 -0.15730801 -0.15912855 -0.13837335 -0.084060736 0.01307095 0.15912557 0.34310183 0.52762538 0.65657496 0.6921041]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-10 23:54:38.021249: step 66010, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.818 sec/batch; 60h:34m:04s remains)
INFO - root - 2017-12-10 23:54:45.883660: step 66020, loss = 0.67, batch loss = 0.61 (10.2 examples/sec; 0.788 sec/batch; 58h:18m:08s remains)
INFO - root - 2017-12-10 23:54:53.712060: step 66030, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 58h:28m:30s remains)
INFO - root - 2017-12-10 23:55:01.533931: step 66040, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 56h:26m:35s remains)
INFO - root - 2017-12-10 23:55:09.471048: step 66050, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 59h:40m:40s remains)
INFO - root - 2017-12-10 23:55:17.375675: step 66060, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 56h:55m:30s remains)
INFO - root - 2017-12-10 23:55:25.084580: step 66070, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 58h:12m:18s remains)
INFO - root - 2017-12-10 23:55:32.789729: step 66080, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.822 sec/batch; 60h:52m:08s remains)
INFO - root - 2017-12-10 23:55:40.642226: step 66090, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 58h:11m:04s remains)
INFO - root - 2017-12-10 23:55:48.499392: step 66100, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 56h:39m:12s remains)
2017-12-10 23:55:49.375916: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036826354 0.051918026 0.05952701 0.056874551 0.049369715 0.041861955 0.035008673 0.027374122 0.017275773 0.0052337097 -0.0068521723 -0.016069302 -0.021444157 -0.02338236 -0.022786008][0.028271891 0.037610389 0.044651009 0.047142077 0.04886055 0.051740505 0.05327427 0.049889591 0.039679926 0.023517579 0.0049899123 -0.010499113 -0.020253381 -0.024508107 -0.025186628][0.01939863 0.024325607 0.032774698 0.043269236 0.057806972 0.074473351 0.08689639 0.089391761 0.07991901 0.059365451 0.032374404 0.0072733527 -0.010238744 -0.019089282 -0.021725602][0.012802285 0.0165033 0.028921822 0.050098512 0.08125513 0.11633374 0.14423893 0.15583178 0.14843421 0.12273003 0.0843557 0.045084707 0.014925721 -0.0020493718 -0.0081301061][0.011571351 0.017668337 0.036235858 0.069746651 0.12003849 0.17732048 0.22504124 0.24844153 0.24309026 0.21030575 0.15779746 0.10128687 0.055386953 0.028269669 0.018625634][0.02264769 0.034506686 0.060241736 0.10513874 0.17253381 0.2506 0.31801167 0.352721 0.34726503 0.30468109 0.23676102 0.16394581 0.10440219 0.069895431 0.059878461][0.05084987 0.070845142 0.10296854 0.15480027 0.23085891 0.32030544 0.39971504 0.44057515 0.43189272 0.37862948 0.29792053 0.21501063 0.14995596 0.11603549 0.11197405][0.093119785 0.12108608 0.15606748 0.20666212 0.2777029 0.36197037 0.43829775 0.47620174 0.46226993 0.40224192 0.31685787 0.23471393 0.17628144 0.153944 0.16371723][0.13766839 0.16982062 0.20163412 0.24113782 0.29277757 0.35434452 0.41126773 0.43719596 0.41891903 0.36072171 0.28304541 0.21438321 0.17418745 0.1721832 0.20078495][0.1672204 0.19667606 0.21895923 0.24041247 0.26471016 0.29421765 0.32293615 0.33324564 0.31456709 0.2681751 0.20992284 0.16389728 0.1471229 0.16626762 0.21106626][0.16800025 0.18814309 0.19772291 0.20130552 0.20141825 0.20308952 0.20751551 0.2065164 0.19317387 0.16547197 0.13222407 0.10991045 0.11201085 0.14314643 0.1931316][0.14075567 0.14882106 0.1472369 0.13954635 0.12678027 0.11441404 0.10741507 0.10441445 0.1013342 0.093647666 0.083067477 0.077777646 0.087285727 0.11579145 0.1563506][0.098645419 0.095943123 0.088469476 0.079071537 0.066631138 0.055185214 0.050277673 0.05355395 0.061990183 0.069584273 0.073090732 0.074251346 0.079138674 0.09268517 0.11307][0.057422992 0.048500329 0.041463852 0.0381067 0.034584526 0.032714326 0.036793582 0.04876684 0.0656428 0.080942176 0.089264438 0.088684522 0.082856372 0.077628389 0.075933672][0.02834898 0.018699989 0.016137294 0.020831952 0.02685567 0.0343451 0.046118893 0.063612022 0.083984323 0.10109508 0.10931495 0.10528784 0.091202505 0.073118433 0.05637724]]...]
INFO - root - 2017-12-10 23:55:57.301157: step 66110, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 59h:10m:14s remains)
INFO - root - 2017-12-10 23:56:05.306739: step 66120, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 58h:49m:38s remains)
INFO - root - 2017-12-10 23:56:13.245213: step 66130, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 59h:53m:01s remains)
INFO - root - 2017-12-10 23:56:21.227882: step 66140, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.811 sec/batch; 59h:59m:52s remains)
INFO - root - 2017-12-10 23:56:28.930781: step 66150, loss = 0.67, batch loss = 0.61 (9.9 examples/sec; 0.807 sec/batch; 59h:43m:58s remains)
INFO - root - 2017-12-10 23:56:36.763207: step 66160, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 55h:56m:19s remains)
INFO - root - 2017-12-10 23:56:44.568070: step 66170, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 59h:17m:46s remains)
INFO - root - 2017-12-10 23:56:52.461194: step 66180, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 58h:38m:03s remains)
INFO - root - 2017-12-10 23:57:00.311616: step 66190, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 57h:38m:26s remains)
INFO - root - 2017-12-10 23:57:08.206669: step 66200, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 56h:14m:00s remains)
2017-12-10 23:57:09.104429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0062454492 -0.007047188 -0.010454461 -0.010233443 0.00073092943 0.02567582 0.062078968 0.10085363 0.12995036 0.1359932 0.11220965 0.063638382 0.0037162621 -0.05241859 -0.091225237][0.010272812 0.0049171261 -0.004482178 -0.0087818317 0.0019627018 0.033290457 0.083122775 0.14050385 0.18877055 0.20830388 0.18768165 0.1321739 0.057816852 -0.014697184 -0.067551896][0.03815813 0.029192574 0.014884789 0.0064649642 0.016398946 0.052434422 0.11226984 0.18229224 0.24304008 0.27048117 0.25044093 0.18784471 0.10115424 0.015497414 -0.048372496][0.079124235 0.068415716 0.05123388 0.040189147 0.049916122 0.089973688 0.15676257 0.23299938 0.29628843 0.320806 0.29267946 0.21830934 0.11902818 0.023832092 -0.045486219][0.12262087 0.11067728 0.093304515 0.084031 0.098475449 0.14625989 0.22079271 0.30033082 0.35949197 0.37157533 0.32593092 0.23303786 0.11900517 0.01658291 -0.053695656][0.14485219 0.13296925 0.11954044 0.11882135 0.14552914 0.20664456 0.29134795 0.37295094 0.42372558 0.41707295 0.34778494 0.23297054 0.10446514 -0.002420723 -0.070237353][0.12878537 0.11770985 0.11205491 0.12617002 0.17268671 0.25409529 0.35451436 0.44069093 0.48302659 0.45636398 0.36243433 0.22633912 0.085232414 -0.023847666 -0.087517232][0.077424571 0.068009511 0.071764931 0.10355797 0.17392485 0.28077173 0.40263337 0.49855974 0.536413 0.49315143 0.37697875 0.22127321 0.068957396 -0.041908875 -0.10167983][0.010744721 0.0031453324 0.014851373 0.061569583 0.15212227 0.28074867 0.42155379 0.52790672 0.56532508 0.51287651 0.38328406 0.21617158 0.058457348 -0.052017145 -0.10830355][-0.0547141 -0.06101089 -0.045178559 0.0092733894 0.11012425 0.24882321 0.3974703 0.507711 0.54534608 0.49180526 0.36192194 0.1978516 0.046705473 -0.056585863 -0.10757749][-0.10717635 -0.11303931 -0.097763725 -0.045144968 0.05252355 0.18575795 0.32733974 0.43148574 0.46692124 0.41821393 0.30025107 0.15378514 0.022064202 -0.065468639 -0.10707363][-0.13621061 -0.14208069 -0.13130473 -0.089526184 -0.008391791 0.10421651 0.22458561 0.31332937 0.34379709 0.30388331 0.206761 0.088159472 -0.01535096 -0.081193015 -0.10992227][-0.13657801 -0.14226733 -0.13779141 -0.11218187 -0.056987375 0.023009831 0.11023497 0.17507987 0.19702363 0.16717859 0.095841542 0.010894219 -0.059982464 -0.10150976 -0.11572269][-0.11739025 -0.12307349 -0.12451042 -0.11490259 -0.087144785 -0.043267023 0.0062064328 0.042822704 0.053489368 0.032662731 -0.012452275 -0.063144855 -0.1017642 -0.1199968 -0.12080667][-0.097102068 -0.10341234 -0.10909829 -0.11107122 -0.10433047 -0.089584112 -0.071695104 -0.059469104 -0.058925096 -0.072595984 -0.096060812 -0.11865079 -0.13130131 -0.13127089 -0.12207557]]...]
INFO - root - 2017-12-10 23:57:17.007770: step 66210, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 59h:34m:22s remains)
INFO - root - 2017-12-10 23:57:24.902889: step 66220, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 58h:29m:16s remains)
INFO - root - 2017-12-10 23:57:32.643184: step 66230, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 58h:37m:27s remains)
INFO - root - 2017-12-10 23:57:40.686655: step 66240, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 57h:42m:51s remains)
INFO - root - 2017-12-10 23:57:48.597339: step 66250, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 56h:46m:18s remains)
INFO - root - 2017-12-10 23:57:56.294562: step 66260, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 57h:07m:33s remains)
INFO - root - 2017-12-10 23:58:04.202939: step 66270, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 58h:05m:22s remains)
INFO - root - 2017-12-10 23:58:12.072488: step 66280, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.828 sec/batch; 61h:11m:43s remains)
INFO - root - 2017-12-10 23:58:20.006606: step 66290, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 58h:16m:11s remains)
INFO - root - 2017-12-10 23:58:27.853507: step 66300, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.764 sec/batch; 56h:29m:58s remains)
2017-12-10 23:58:28.797295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.03963612 -0.041131712 -0.036607016 -0.01999714 0.01137103 0.054068659 0.10304035 0.15836476 0.22274023 0.28217098 0.31118307 0.29765204 0.2541194 0.20175892 0.15061711][-0.038451578 -0.037973333 -0.032052025 -0.016027601 0.010016133 0.041260827 0.073858678 0.10815663 0.14556119 0.17641418 0.18483947 0.1645916 0.12679473 0.087023638 0.052101128][-0.031138809 -0.023058023 -0.009283348 0.01148445 0.03556506 0.057712276 0.075628 0.0888818 0.097371355 0.0964173 0.079623796 0.04670085 0.0087177036 -0.022313274 -0.042658728][-0.017918559 0.002012104 0.028934231 0.060624693 0.091087177 0.11596186 0.13285963 0.13690858 0.12463581 0.095183253 0.050873648 -0.0023009949 -0.050597414 -0.081962444 -0.095345333][0.00075984577 0.031722195 0.0713033 0.11589252 0.16069436 0.20310777 0.2366942 0.24721125 0.22563179 0.17419931 0.10392074 0.027546259 -0.036672663 -0.074467048 -0.087242082][0.023426339 0.059002228 0.1041176 0.15721019 0.218214 0.28615266 0.34670052 0.37337276 0.35155898 0.28690663 0.19809657 0.10403643 0.027171627 -0.016171357 -0.029905129][0.053991403 0.086046547 0.12629673 0.17804208 0.24709356 0.33338973 0.41459951 0.45454213 0.43561527 0.36734876 0.27304298 0.17504118 0.097256906 0.056026332 0.045207653][0.10282367 0.12638816 0.1519296 0.18862745 0.24735643 0.32955465 0.4103331 0.45089075 0.43421638 0.37177607 0.28615993 0.19882172 0.13237776 0.10216445 0.1007368][0.18208611 0.19686647 0.20004158 0.207266 0.2335414 0.28405342 0.34013939 0.36857802 0.35411498 0.30518019 0.23826037 0.17057659 0.12143613 0.10519147 0.11556213][0.2855795 0.293138 0.26978377 0.2383229 0.2175746 0.2194774 0.23796016 0.25021529 0.24101573 0.2104383 0.16546097 0.1180298 0.083714023 0.076331265 0.09346775][0.38548672 0.38751185 0.33851129 0.27013034 0.20409876 0.15993777 0.14504613 0.1468638 0.14793913 0.13735719 0.11254528 0.08015348 0.052139733 0.041856941 0.052599534][0.44270837 0.44048253 0.37494615 0.28475741 0.19266154 0.12308948 0.093139 0.09766531 0.11643624 0.12777184 0.12035627 0.095650047 0.062777095 0.034926537 0.022180254][0.42399678 0.42016062 0.35446426 0.26681852 0.17774735 0.11142205 0.08940009 0.11158562 0.15605606 0.19146113 0.19803596 0.17242445 0.12294798 0.065157063 0.017097887][0.33868897 0.33697608 0.28605732 0.22089835 0.15720859 0.11465495 0.11519877 0.1635958 0.23744081 0.29697317 0.31381434 0.28146982 0.21072158 0.12131647 0.039318878][0.22987579 0.23495921 0.206888 0.17127952 0.13877869 0.1237831 0.14632718 0.21615456 0.3119922 0.38889864 0.41150075 0.37199911 0.2858347 0.17682396 0.076396339]]...]
INFO - root - 2017-12-10 23:58:36.417266: step 66310, loss = 0.71, batch loss = 0.66 (10.5 examples/sec; 0.762 sec/batch; 56h:20m:38s remains)
INFO - root - 2017-12-10 23:58:44.319453: step 66320, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 56h:51m:11s remains)
INFO - root - 2017-12-10 23:58:52.087207: step 66330, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.751 sec/batch; 55h:33m:18s remains)
INFO - root - 2017-12-10 23:59:00.035747: step 66340, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.796 sec/batch; 58h:51m:51s remains)
INFO - root - 2017-12-10 23:59:07.763345: step 66350, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 59h:41m:13s remains)
INFO - root - 2017-12-10 23:59:15.649989: step 66360, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 59h:15m:27s remains)
INFO - root - 2017-12-10 23:59:23.516549: step 66370, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 57h:19m:07s remains)
INFO - root - 2017-12-10 23:59:31.388047: step 66380, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 59h:35m:40s remains)
INFO - root - 2017-12-10 23:59:39.176659: step 66390, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 58h:05m:40s remains)
INFO - root - 2017-12-10 23:59:46.926993: step 66400, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.758 sec/batch; 55h:59m:56s remains)
2017-12-10 23:59:47.812064: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1267336 0.12323337 0.1240883 0.12269416 0.11318509 0.095872059 0.07326214 0.047832489 0.024367819 0.0098677166 0.0094985319 0.01998961 0.035793174 0.05254519 0.065966755][0.18340769 0.16983336 0.15951669 0.14961867 0.13434723 0.11344898 0.088465415 0.060750093 0.034747433 0.017424643 0.014940718 0.025012325 0.043980692 0.067121267 0.087351494][0.24084224 0.21653321 0.19355272 0.17484571 0.15610923 0.13550599 0.112169 0.085476287 0.059331287 0.039791536 0.033727877 0.041007947 0.060760442 0.087678991 0.11311365][0.29120493 0.2615279 0.23009841 0.20543818 0.18631013 0.16928394 0.14993906 0.12605087 0.1015965 0.080577686 0.070467032 0.073309049 0.091567315 0.12019073 0.14861][0.31986839 0.29139605 0.25840905 0.23328029 0.21783306 0.20705101 0.19346364 0.17460737 0.15481773 0.13509016 0.12194594 0.11938317 0.13452254 0.16337836 0.19462523][0.31562451 0.29093683 0.26197147 0.24329068 0.23802924 0.2382683 0.23375107 0.2228874 0.20990849 0.19325697 0.17797191 0.17045662 0.18240125 0.21127026 0.24563175][0.28598642 0.26295209 0.23936738 0.2316875 0.24259117 0.25933346 0.26817697 0.26745263 0.26073751 0.2454945 0.22598673 0.21170656 0.21869655 0.24606846 0.282944][0.24119349 0.21706696 0.19781497 0.20121683 0.22915092 0.26436561 0.28836009 0.297307 0.29408216 0.27710339 0.25077927 0.22706917 0.22573471 0.24793151 0.2833845][0.18783735 0.16498931 0.15100148 0.16358414 0.20428538 0.25440577 0.2908625 0.3064402 0.30313855 0.28252491 0.24928755 0.21532933 0.20197113 0.21309806 0.24058166][0.13558829 0.11830071 0.11220473 0.1319579 0.17904189 0.23700276 0.28076637 0.29968604 0.29429281 0.26962116 0.23013651 0.18639584 0.15915541 0.15488708 0.16962042][0.092925392 0.081976339 0.082847141 0.10630498 0.15350243 0.21224146 0.25884363 0.27987111 0.27373639 0.24747807 0.20510933 0.1554596 0.11713453 0.098476149 0.09956158][0.069915459 0.061246842 0.063217573 0.084322892 0.12572406 0.1795789 0.22611696 0.25021538 0.24759299 0.22447187 0.18420686 0.13397028 0.090075165 0.061889205 0.052278675][0.0672589 0.055987425 0.052445576 0.065423645 0.096578605 0.14146845 0.18574794 0.21465026 0.22042294 0.20526484 0.17153354 0.12602596 0.082692824 0.0511083 0.035898685][0.086278521 0.070496887 0.057274967 0.057830676 0.074805789 0.1063152 0.14515369 0.1795803 0.19770767 0.19454367 0.17023256 0.13241054 0.092571661 0.059765913 0.041229524][0.12795138 0.10684671 0.082135342 0.068396062 0.069877729 0.085495658 0.11501402 0.15203249 0.18157734 0.19132511 0.17762305 0.14846152 0.11224837 0.076780982 0.052831579]]...]
INFO - root - 2017-12-10 23:59:55.712511: step 66410, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.825 sec/batch; 60h:57m:33s remains)
INFO - root - 2017-12-11 00:00:03.637705: step 66420, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 56h:19m:42s remains)
INFO - root - 2017-12-11 00:00:11.378147: step 66430, loss = 0.71, batch loss = 0.65 (12.3 examples/sec; 0.651 sec/batch; 48h:07m:42s remains)
INFO - root - 2017-12-11 00:00:19.219222: step 66440, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 58h:24m:19s remains)
INFO - root - 2017-12-11 00:00:27.304956: step 66450, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.822 sec/batch; 60h:43m:17s remains)
INFO - root - 2017-12-11 00:00:35.151962: step 66460, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.799 sec/batch; 59h:01m:33s remains)
INFO - root - 2017-12-11 00:00:42.925646: step 66470, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 59h:49m:57s remains)
INFO - root - 2017-12-11 00:00:50.829402: step 66480, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.789 sec/batch; 58h:17m:43s remains)
INFO - root - 2017-12-11 00:00:58.816735: step 66490, loss = 0.67, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 59h:05m:17s remains)
INFO - root - 2017-12-11 00:01:06.704718: step 66500, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 58h:01m:27s remains)
2017-12-11 00:01:07.565423: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.08898598 0.13676652 0.18807289 0.22500712 0.23946583 0.23753539 0.23474154 0.2286275 0.20984665 0.1851368 0.18315235 0.20431812 0.22383001 0.22267741 0.19954854][0.11154021 0.17280662 0.2417638 0.29598835 0.3228209 0.3256965 0.32397446 0.31629565 0.29394087 0.26245832 0.25126308 0.263477 0.27299592 0.25987956 0.22434628][0.11976305 0.19114169 0.27360108 0.34286508 0.3832463 0.39666378 0.40542108 0.40739235 0.39170414 0.35971919 0.33756581 0.33117098 0.31810981 0.28200537 0.22743262][0.1142749 0.19226897 0.28514895 0.36821187 0.42398635 0.45501924 0.48439619 0.50657094 0.50613743 0.47891605 0.44524059 0.4129585 0.36795655 0.30105981 0.22213714][0.10522074 0.1919045 0.29806641 0.39738837 0.4705798 0.52165312 0.57210988 0.61301041 0.62508047 0.6009739 0.55473661 0.49536404 0.41895181 0.32310951 0.22143552][0.10544782 0.20627883 0.33076885 0.44743454 0.53440332 0.59765822 0.6556204 0.6988371 0.70799839 0.67842931 0.62044775 0.54184562 0.44592622 0.33428523 0.22174935][0.12078499 0.24150147 0.3871586 0.51794314 0.60933429 0.66897929 0.71184242 0.733175 0.71999717 0.67664164 0.61272329 0.53128809 0.43579221 0.32765847 0.21982072][0.15134934 0.29177216 0.45236215 0.58554655 0.66618264 0.70324636 0.7099123 0.69067031 0.64400595 0.58519763 0.52512074 0.4592281 0.38425744 0.29750794 0.20786503][0.18129307 0.33024898 0.48897231 0.60703868 0.66194206 0.66441494 0.62741488 0.56498164 0.48749688 0.41869551 0.37055051 0.33093685 0.28846034 0.23408006 0.17100748][0.19043379 0.3307322 0.46759313 0.55453444 0.57502717 0.542073 0.469668 0.37764961 0.28385121 0.21500798 0.18288268 0.17094108 0.16160695 0.14051245 0.10555282][0.15882191 0.27117267 0.3694939 0.416974 0.40460107 0.34703204 0.25942355 0.16204959 0.073700823 0.017844789 0.0039952626 0.014923844 0.03057744 0.034467679 0.0221196][0.084413245 0.15643823 0.21075773 0.2226229 0.18987378 0.12678799 0.047575694 -0.031736556 -0.0959835 -0.12896101 -0.12531731 -0.1005293 -0.0727755 -0.056739189 -0.056173243][-0.0042991028 0.02840679 0.046504155 0.03604088 -0.0010318185 -0.053178024 -0.10937818 -0.15953739 -0.19414359 -0.20488872 -0.1904151 -0.16273431 -0.13491258 -0.11732182 -0.11168765][-0.0777123 -0.073326439 -0.076378673 -0.093957052 -0.12329933 -0.15713377 -0.18851033 -0.21227074 -0.22385326 -0.22051851 -0.20339954 -0.18036076 -0.15910985 -0.14533727 -0.1389856][-0.12209519 -0.13236481 -0.14282873 -0.15757865 -0.17493781 -0.19163184 -0.20409216 -0.21029587 -0.20891319 -0.20005049 -0.18552083 -0.16959678 -0.15615287 -0.14740023 -0.14225799]]...]
INFO - root - 2017-12-11 00:01:15.438065: step 66510, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 58h:55m:28s remains)
INFO - root - 2017-12-11 00:01:23.132620: step 66520, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 60h:14m:04s remains)
INFO - root - 2017-12-11 00:01:31.058686: step 66530, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 57h:20m:18s remains)
INFO - root - 2017-12-11 00:01:38.910171: step 66540, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 59h:19m:44s remains)
INFO - root - 2017-12-11 00:01:46.627677: step 66550, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.831 sec/batch; 61h:22m:45s remains)
INFO - root - 2017-12-11 00:01:54.664592: step 66560, loss = 0.69, batch loss = 0.63 (9.5 examples/sec; 0.842 sec/batch; 62h:13m:02s remains)
INFO - root - 2017-12-11 00:02:02.584178: step 66570, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 58h:53m:31s remains)
INFO - root - 2017-12-11 00:02:10.473156: step 66580, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 58h:02m:18s remains)
INFO - root - 2017-12-11 00:02:18.359590: step 66590, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 57h:23m:46s remains)
INFO - root - 2017-12-11 00:02:26.200358: step 66600, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 57h:54m:50s remains)
2017-12-11 00:02:27.020740: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22454385 0.19323125 0.14466463 0.095561609 0.062046733 0.047346279 0.045162741 0.047605146 0.054610781 0.071707815 0.099667177 0.1328914 0.16366145 0.18219599 0.18366596][0.28841069 0.24756055 0.18475267 0.12353973 0.084876217 0.0727179 0.07825844 0.089177452 0.10115049 0.11830417 0.1442986 0.17779788 0.213702 0.24086419 0.24984497][0.313891 0.2692436 0.200848 0.13940293 0.10907546 0.1125671 0.13561793 0.15822236 0.17014231 0.17509083 0.18472089 0.2074329 0.2430907 0.27792561 0.29508433][0.30042404 0.25843012 0.1948802 0.14652921 0.13866861 0.17027129 0.21886772 0.2542316 0.25983205 0.24155885 0.22354148 0.2272245 0.25824329 0.29864153 0.32212543][0.26287356 0.22977068 0.18036832 0.15519172 0.17846063 0.244056 0.31993163 0.36561325 0.35947847 0.31251624 0.26348513 0.24679238 0.27162862 0.31366432 0.33784622][0.22834609 0.20585227 0.17223109 0.17048885 0.22257833 0.3176617 0.41498634 0.46649674 0.44859371 0.37889624 0.30806923 0.27818638 0.297509 0.33522815 0.35142049][0.21434997 0.1991919 0.17557673 0.18774296 0.25569314 0.36569935 0.47159278 0.52302104 0.49795467 0.41937375 0.34318829 0.3113808 0.32694542 0.35486856 0.35609612][0.21414854 0.20329207 0.18336223 0.19752018 0.26441342 0.37067896 0.47036633 0.51582462 0.48882669 0.41428304 0.34574172 0.31868941 0.32985371 0.34447482 0.32965627][0.21624178 0.20863713 0.18859656 0.19566852 0.24808353 0.33573788 0.41799539 0.45319232 0.42704064 0.36308724 0.30740047 0.28607917 0.29143777 0.29310384 0.267453][0.21855707 0.2139498 0.19213526 0.18840504 0.22171281 0.28656685 0.34890726 0.37282062 0.34622708 0.289731 0.24132149 0.22009517 0.21753909 0.20968074 0.18128212][0.21934773 0.21697891 0.19334635 0.18020996 0.19771445 0.24402626 0.29010639 0.30322394 0.27186677 0.21406835 0.1628148 0.13464536 0.12318306 0.11069652 0.087185875][0.21727656 0.21651527 0.19249885 0.17444859 0.18271171 0.21659879 0.24961433 0.25085133 0.20963383 0.14365242 0.084640354 0.049403537 0.033565037 0.022959555 0.0094364816][0.21168651 0.21265233 0.19038156 0.1717128 0.17568529 0.20093361 0.22265831 0.21253137 0.16190381 0.089593329 0.026856961 -0.0096497918 -0.023867015 -0.028680071 -0.031918433][0.20331497 0.20671269 0.18762203 0.17016365 0.17163669 0.19023515 0.20387048 0.18786533 0.13552646 0.065471657 0.0063637393 -0.026666589 -0.037551694 -0.038026068 -0.035372317][0.19583087 0.20220989 0.1863066 0.16908596 0.16665187 0.17862986 0.1874385 0.17234176 0.1270804 0.067003906 0.015995655 -0.012917336 -0.022916274 -0.023659764 -0.021098832]]...]
INFO - root - 2017-12-11 00:02:34.728562: step 66610, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.824 sec/batch; 60h:50m:57s remains)
INFO - root - 2017-12-11 00:02:42.497994: step 66620, loss = 0.71, batch loss = 0.65 (10.8 examples/sec; 0.739 sec/batch; 54h:34m:40s remains)
INFO - root - 2017-12-11 00:02:50.260802: step 66630, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 57h:56m:46s remains)
INFO - root - 2017-12-11 00:02:58.071473: step 66640, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 58h:17m:42s remains)
INFO - root - 2017-12-11 00:03:05.886744: step 66650, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 57h:00m:05s remains)
INFO - root - 2017-12-11 00:03:13.832046: step 66660, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 58h:28m:00s remains)
INFO - root - 2017-12-11 00:03:21.656972: step 66670, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 58h:06m:14s remains)
INFO - root - 2017-12-11 00:03:29.623618: step 66680, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 57h:29m:58s remains)
INFO - root - 2017-12-11 00:03:37.556758: step 66690, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 56h:19m:03s remains)
INFO - root - 2017-12-11 00:03:45.243273: step 66700, loss = 0.67, batch loss = 0.61 (10.6 examples/sec; 0.753 sec/batch; 55h:37m:08s remains)
2017-12-11 00:03:46.009351: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17491421 0.17916518 0.17622238 0.1749246 0.17779508 0.18468879 0.19390398 0.20588094 0.21659032 0.22400185 0.22825249 0.23607111 0.25371781 0.2724773 0.28206542][0.18027808 0.19054286 0.19215742 0.19458589 0.1986208 0.20355457 0.21081856 0.22165751 0.22802055 0.22557563 0.21658225 0.21260038 0.22152005 0.23453835 0.24404351][0.2042504 0.21670356 0.21899468 0.22120997 0.22275807 0.22176896 0.22291794 0.22936396 0.23179108 0.22353333 0.20647891 0.1943012 0.19510929 0.20120119 0.20737687][0.2413087 0.25239775 0.25138646 0.25003976 0.24799506 0.24244301 0.23966086 0.24332058 0.24467695 0.23476328 0.21365699 0.19483738 0.18634085 0.18305787 0.18206254][0.27399558 0.28280273 0.27768978 0.27327651 0.27101198 0.26770729 0.26801428 0.2737295 0.27633914 0.26522282 0.2390687 0.2108224 0.18996757 0.17515595 0.16546369][0.2830624 0.28921595 0.28166655 0.27783108 0.28084254 0.28716847 0.29726058 0.30909804 0.31352025 0.2998423 0.26698965 0.227295 0.19239135 0.16517109 0.14776665][0.25195625 0.25523537 0.24840775 0.24878721 0.26001003 0.27908611 0.3024714 0.32365549 0.33167574 0.31669894 0.27963686 0.23197062 0.1863758 0.14931177 0.12705481][0.18806486 0.18794297 0.18215112 0.18543202 0.20140284 0.22926277 0.26394787 0.29573023 0.31113216 0.3003462 0.26572713 0.21755366 0.16794276 0.12588041 0.10193157][0.11709303 0.11311388 0.10636152 0.10757466 0.1210133 0.14995547 0.19081603 0.23211323 0.2575334 0.25627324 0.23050924 0.18856467 0.14099838 0.098186292 0.073965423][0.066567361 0.060125232 0.051426753 0.047344096 0.053435382 0.077768616 0.11960212 0.16678141 0.20045671 0.20887931 0.19308931 0.15861976 0.11480338 0.072523504 0.046961404][0.064157762 0.057041276 0.044980157 0.033197314 0.028619865 0.043515392 0.080226742 0.12722081 0.16388988 0.17810044 0.16940327 0.14014582 0.098790228 0.056123815 0.027083911][0.11486672 0.10705387 0.089677833 0.068906531 0.053055894 0.05678251 0.084515132 0.12617518 0.16054942 0.17567077 0.16930246 0.14107825 0.099193916 0.054469157 0.020467356][0.19535975 0.18554261 0.16192472 0.1337501 0.10985268 0.10431136 0.12195393 0.1547718 0.18262202 0.19405158 0.18491124 0.15366602 0.10913167 0.062327944 0.025003377][0.26335037 0.25186849 0.22438829 0.19420165 0.16836944 0.1573053 0.16543762 0.18797123 0.20718339 0.21270345 0.1994466 0.16677244 0.12379334 0.080076858 0.045325153][0.28675315 0.27727538 0.25248402 0.22752871 0.20612378 0.19369528 0.19450657 0.20857142 0.22181405 0.22460686 0.21184608 0.18480667 0.15158005 0.11719213 0.088938951]]...]
INFO - root - 2017-12-11 00:03:53.778787: step 66710, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.769 sec/batch; 56h:46m:02s remains)
INFO - root - 2017-12-11 00:04:01.602155: step 66720, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 57h:37m:14s remains)
INFO - root - 2017-12-11 00:04:09.380212: step 66730, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 58h:33m:58s remains)
INFO - root - 2017-12-11 00:04:17.306547: step 66740, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 58h:12m:49s remains)
INFO - root - 2017-12-11 00:04:25.166534: step 66750, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 58h:04m:23s remains)
INFO - root - 2017-12-11 00:04:33.062897: step 66760, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.812 sec/batch; 59h:57m:15s remains)
INFO - root - 2017-12-11 00:04:40.932447: step 66770, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.820 sec/batch; 60h:29m:47s remains)
INFO - root - 2017-12-11 00:04:48.818325: step 66780, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 58h:12m:37s remains)
INFO - root - 2017-12-11 00:04:56.350226: step 66790, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 58h:58m:48s remains)
INFO - root - 2017-12-11 00:05:04.204357: step 66800, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.812 sec/batch; 59h:53m:51s remains)
2017-12-11 00:05:05.023757: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1392971 0.15567039 0.1651758 0.16937156 0.17391613 0.18087676 0.18508358 0.19030717 0.1907497 0.18140335 0.14924277 0.099331886 0.049150653 0.011522789 -0.0154688][0.17677462 0.19984427 0.21339732 0.2171136 0.21998756 0.22527617 0.22790462 0.23062308 0.2280955 0.21784617 0.18347193 0.1282894 0.06959267 0.023700284 -0.008196339][0.20513639 0.2331432 0.2490865 0.24968408 0.24675916 0.24654122 0.24550644 0.24407417 0.23798658 0.22833061 0.197227 0.14378718 0.081963822 0.031450067 -0.0031782724][0.23161988 0.26411277 0.28167558 0.27848816 0.26799172 0.26099214 0.2569018 0.2529076 0.24404527 0.23485394 0.20969312 0.16222888 0.10028738 0.044429626 0.0043588029][0.25146958 0.28649348 0.30391714 0.296816 0.27923506 0.26720864 0.26370326 0.26071924 0.25034592 0.24002676 0.22042683 0.18038027 0.11982244 0.058642875 0.012012909][0.25940523 0.29575002 0.31325802 0.30425802 0.28190327 0.2675375 0.26609987 0.26416993 0.25020924 0.23470969 0.21718827 0.18280725 0.12469048 0.061061803 0.011387826][0.25891736 0.29534906 0.3134551 0.30392745 0.27762875 0.26078162 0.25968233 0.25711715 0.23834758 0.21634679 0.19837807 0.1677704 0.1134593 0.051958542 0.0044163973][0.24412769 0.27825978 0.2953614 0.28560719 0.25612873 0.2353019 0.23081608 0.22556615 0.20374398 0.17803425 0.16032331 0.1338311 0.086365223 0.032572117 -0.0080594253][0.20347565 0.23192233 0.24639697 0.23778427 0.20919903 0.18654664 0.17827027 0.17089498 0.14952177 0.12455188 0.10892627 0.087549075 0.049378727 0.006529198 -0.024745073][0.13686138 0.15532871 0.16483384 0.1575835 0.13357091 0.11313463 0.10348431 0.09534575 0.076580524 0.055375479 0.043424781 0.028439542 0.0014837723 -0.027873486 -0.047445454][0.0657249 0.073120221 0.0768015 0.070405126 0.052593414 0.037571397 0.029779581 0.022863401 0.0082414746 -0.0077412827 -0.015915016 -0.025378278 -0.042151567 -0.0589382 -0.067554444][0.013700432 0.013019646 0.012154797 0.0063159261 -0.0058518518 -0.015820768 -0.021314086 -0.026435975 -0.036295511 -0.046577774 -0.051322803 -0.056667075 -0.065836094 -0.07349094 -0.0750495][-0.019496288 -0.023879712 -0.027344521 -0.032958552 -0.041098043 -0.04777037 -0.051976454 -0.055716474 -0.061377332 -0.066585332 -0.0682267 -0.07008002 -0.0735268 -0.075287975 -0.073421359][-0.044319086 -0.049996428 -0.053911943 -0.058500763 -0.063281707 -0.06693235 -0.069222093 -0.071096331 -0.073510423 -0.075277261 -0.074985586 -0.074451283 -0.07413882 -0.0725913 -0.06939391][-0.05845328 -0.06454163 -0.067755513 -0.070820145 -0.073075704 -0.074285492 -0.074683629 -0.074913785 -0.0753774 -0.075508378 -0.074602224 -0.073063679 -0.071026839 -0.068217874 -0.064888641]]...]
INFO - root - 2017-12-11 00:05:12.812836: step 66810, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 56h:05m:17s remains)
INFO - root - 2017-12-11 00:05:20.600552: step 66820, loss = 0.67, batch loss = 0.61 (10.6 examples/sec; 0.755 sec/batch; 55h:44m:16s remains)
INFO - root - 2017-12-11 00:05:28.483974: step 66830, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 59h:42m:49s remains)
INFO - root - 2017-12-11 00:05:36.368502: step 66840, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 58h:55m:43s remains)
INFO - root - 2017-12-11 00:05:44.209746: step 66850, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 56h:25m:58s remains)
INFO - root - 2017-12-11 00:05:52.081804: step 66860, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 57h:42m:10s remains)
INFO - root - 2017-12-11 00:05:59.751511: step 66870, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 58h:08m:22s remains)
INFO - root - 2017-12-11 00:06:07.424265: step 66880, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 57h:38m:03s remains)
INFO - root - 2017-12-11 00:06:15.352798: step 66890, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.824 sec/batch; 60h:47m:42s remains)
INFO - root - 2017-12-11 00:06:23.133525: step 66900, loss = 0.70, batch loss = 0.65 (9.8 examples/sec; 0.820 sec/batch; 60h:28m:03s remains)
2017-12-11 00:06:24.012021: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.47519484 0.49938121 0.49146059 0.46972275 0.43656361 0.39491972 0.35138589 0.31743306 0.29079989 0.26065713 0.22764631 0.19309777 0.15309331 0.10223155 0.051678788][0.50874811 0.54061097 0.5389424 0.52495646 0.49734819 0.45546114 0.40756071 0.36536634 0.32966232 0.28756115 0.24315788 0.20204131 0.15842937 0.10524116 0.052383747][0.48098189 0.51454347 0.51778442 0.51429832 0.4994626 0.46648937 0.42248255 0.37859547 0.33807686 0.28927803 0.23675711 0.19050366 0.14640053 0.095086478 0.044891588][0.4297435 0.45865127 0.46220389 0.46501884 0.46099663 0.44009927 0.40745842 0.37197506 0.33680862 0.28899133 0.23322923 0.18397309 0.1391523 0.089119829 0.040647935][0.38991529 0.41022322 0.40936384 0.41148955 0.41133514 0.4001188 0.38218704 0.36131489 0.33675456 0.29353535 0.23702708 0.18469268 0.13633581 0.084173664 0.03495631][0.37569997 0.388012 0.38333863 0.38253066 0.38266644 0.37824163 0.37624907 0.37373665 0.3620497 0.32337373 0.26402944 0.20465516 0.14613295 0.085145079 0.030145815][0.37521076 0.38510704 0.38015822 0.37789762 0.37933648 0.38297066 0.4009847 0.4222008 0.42730641 0.39591628 0.33415097 0.26487368 0.1891569 0.11022302 0.041124057][0.37568617 0.38630614 0.38108778 0.37703046 0.37918973 0.39085168 0.43041635 0.47835758 0.50345 0.48161104 0.41901132 0.33992353 0.24529566 0.14518122 0.058366563][0.36588115 0.37755576 0.37055162 0.36242655 0.36144131 0.377685 0.434733 0.5072909 0.55356741 0.54299724 0.48265776 0.39577854 0.28465363 0.16551714 0.063147195][0.32684085 0.3370558 0.33024514 0.32109731 0.31859604 0.33760589 0.40251896 0.4872292 0.54577762 0.54430646 0.48945156 0.4009164 0.28288665 0.15601386 0.048871037][0.25047654 0.25827554 0.25480342 0.25017527 0.25098816 0.27219507 0.3353937 0.41871676 0.48039475 0.48775384 0.44419414 0.3635394 0.25046217 0.12831388 0.026879685][0.14521711 0.15149979 0.15247764 0.15332045 0.15736948 0.17748548 0.23185149 0.30501905 0.36392304 0.3794015 0.35119084 0.28626791 0.18869163 0.082636632 -0.0035701601][0.035252258 0.037926368 0.040026676 0.041664433 0.044066388 0.058107723 0.098137952 0.15485363 0.2046324 0.22332442 0.20847687 0.1621213 0.088443041 0.0094509432 -0.051391687][-0.049838819 -0.05230708 -0.052276947 -0.053160265 -0.054230317 -0.046673466 -0.020805828 0.018231729 0.054766916 0.071153864 0.06502445 0.036096372 -0.011461594 -0.06070016 -0.094765492][-0.098366007 -0.10465864 -0.10673716 -0.10942584 -0.11173628 -0.10764905 -0.092432827 -0.069282375 -0.047632206 -0.037874598 -0.04079733 -0.056658883 -0.081922889 -0.10576285 -0.11826255]]...]
INFO - root - 2017-12-11 00:06:31.936400: step 66910, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 56h:51m:53s remains)
INFO - root - 2017-12-11 00:06:39.686071: step 66920, loss = 0.69, batch loss = 0.64 (10.6 examples/sec; 0.754 sec/batch; 55h:37m:52s remains)
INFO - root - 2017-12-11 00:06:47.544530: step 66930, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 58h:54m:41s remains)
INFO - root - 2017-12-11 00:06:55.359034: step 66940, loss = 0.70, batch loss = 0.64 (10.8 examples/sec; 0.741 sec/batch; 54h:41m:23s remains)
INFO - root - 2017-12-11 00:07:03.172197: step 66950, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 58h:00m:35s remains)
INFO - root - 2017-12-11 00:07:11.017333: step 66960, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.801 sec/batch; 59h:04m:00s remains)
INFO - root - 2017-12-11 00:07:18.801149: step 66970, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 56h:35m:11s remains)
INFO - root - 2017-12-11 00:07:26.689513: step 66980, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.747 sec/batch; 55h:06m:05s remains)
INFO - root - 2017-12-11 00:07:34.527110: step 66990, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 57h:12m:01s remains)
INFO - root - 2017-12-11 00:07:42.474381: step 67000, loss = 0.67, batch loss = 0.61 (9.8 examples/sec; 0.812 sec/batch; 59h:54m:47s remains)
2017-12-11 00:07:43.256589: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033535272 0.032342058 0.036148731 0.044403486 0.051332694 0.051411584 0.045975748 0.035903763 0.023608834 0.013497948 0.0083212247 0.0081516476 0.013716743 0.025993306 0.040819991][0.034262404 0.03138198 0.03302826 0.040732052 0.048167408 0.049494036 0.04580703 0.037490346 0.025484236 0.012988341 0.0036517205 -0.0018058025 -0.0010253687 0.008084815 0.022700265][0.052049711 0.050203729 0.050827056 0.057568509 0.064871065 0.0675598 0.066255711 0.060855068 0.051729519 0.040367343 0.029980358 0.021025103 0.016986089 0.020850407 0.031039841][0.080001667 0.08516454 0.090272442 0.10032447 0.1110834 0.11786009 0.12015209 0.11838916 0.11494274 0.10898886 0.10060333 0.089074358 0.079466283 0.075159781 0.0744675][0.12330065 0.14225738 0.15900446 0.17866687 0.19813612 0.21221559 0.21877418 0.22048177 0.22397496 0.22483568 0.217667 0.20065083 0.18180183 0.16406004 0.144668][0.19170125 0.22948839 0.26278183 0.29627308 0.3270514 0.34851119 0.35712415 0.3599596 0.36893553 0.37490627 0.36551577 0.33889583 0.3069014 0.27055174 0.22579901][0.28521088 0.33968079 0.3868362 0.43096367 0.46848634 0.49140298 0.49610266 0.49545038 0.50579017 0.51350635 0.49978226 0.46313584 0.41843772 0.363853 0.29512173][0.3781673 0.43991527 0.49088317 0.53552651 0.56946075 0.58442634 0.57880735 0.57048297 0.57709306 0.58238673 0.56426042 0.52167237 0.4699825 0.40478566 0.32378352][0.42724606 0.48340237 0.525243 0.55804378 0.57712263 0.57595515 0.55697733 0.540203 0.54022306 0.54033 0.51973003 0.47765571 0.42753056 0.36401695 0.28864047][0.40822047 0.44947928 0.47460616 0.48931569 0.49028179 0.47414935 0.4469586 0.42674944 0.42180812 0.4168722 0.39635062 0.36013079 0.31835532 0.26665246 0.21062817][0.32897371 0.35340285 0.36213785 0.36108735 0.3498359 0.32840204 0.30373242 0.28821051 0.28301162 0.27591103 0.25813118 0.23063184 0.20037392 0.16518117 0.13314576][0.22286849 0.23393482 0.23181771 0.22221966 0.20809089 0.19186231 0.17825831 0.17286262 0.17158283 0.16581464 0.15269481 0.13427162 0.11501504 0.095216259 0.083499394][0.13005674 0.13491905 0.12954575 0.11931482 0.10975611 0.10402944 0.10307612 0.10719647 0.11044984 0.10756717 0.099071555 0.087373666 0.075154848 0.064718172 0.063976489][0.081166022 0.086299226 0.083140463 0.076529637 0.073341012 0.076401785 0.083380833 0.092040554 0.097593956 0.097546376 0.093220323 0.086158946 0.077603795 0.070924632 0.072650947][0.083176881 0.091896437 0.092480645 0.089443929 0.089638352 0.095585451 0.10372089 0.11169898 0.11699894 0.11905457 0.11863459 0.11567641 0.11000013 0.10481279 0.10501195]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 00:07:51.146428: step 67010, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 57h:03m:21s remains)
INFO - root - 2017-12-11 00:07:58.822803: step 67020, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 41h:45m:40s remains)
INFO - root - 2017-12-11 00:08:06.685397: step 67030, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 58h:10m:08s remains)
INFO - root - 2017-12-11 00:08:14.522980: step 67040, loss = 0.69, batch loss = 0.64 (10.6 examples/sec; 0.756 sec/batch; 55h:43m:18s remains)
INFO - root - 2017-12-11 00:08:22.358709: step 67050, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 58h:21m:22s remains)
INFO - root - 2017-12-11 00:08:30.194149: step 67060, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.823 sec/batch; 60h:39m:34s remains)
INFO - root - 2017-12-11 00:08:38.055373: step 67070, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 56h:15m:40s remains)
INFO - root - 2017-12-11 00:08:45.995091: step 67080, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 58h:41m:09s remains)
INFO - root - 2017-12-11 00:08:53.886608: step 67090, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 56h:07m:00s remains)
INFO - root - 2017-12-11 00:09:01.732790: step 67100, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.770 sec/batch; 56h:47m:36s remains)
2017-12-11 00:09:02.696667: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.3573274 0.40803733 0.43269929 0.43490168 0.41340771 0.37187296 0.32707039 0.29866552 0.29607624 0.31473571 0.34437209 0.36842188 0.37527511 0.36874026 0.36193791][0.38117704 0.42724746 0.44910985 0.45336837 0.4354836 0.39596555 0.34915146 0.31482169 0.30451295 0.31602138 0.34184527 0.36714634 0.38067716 0.38376364 0.38474613][0.38130298 0.41889235 0.43858048 0.44981575 0.442775 0.41227496 0.3678318 0.32756722 0.30463403 0.29987782 0.31208894 0.33159357 0.34853911 0.36039093 0.36923778][0.37432796 0.40061456 0.41602182 0.43410638 0.44133228 0.42660427 0.3917256 0.34993517 0.3138631 0.28795117 0.27885035 0.28356063 0.29550892 0.308925 0.32000625][0.37370008 0.38427755 0.39018306 0.41143656 0.43486091 0.443184 0.42689097 0.38977602 0.34103006 0.28994137 0.25274277 0.23444171 0.23303664 0.24032749 0.24774964][0.38493779 0.37830159 0.37119159 0.39231506 0.43252629 0.46792376 0.47558826 0.44697347 0.38600865 0.30665764 0.23546004 0.18720862 0.16551334 0.16148978 0.16327283][0.39762351 0.3746438 0.35396364 0.37212557 0.42564151 0.48514625 0.51568604 0.49736109 0.42858961 0.32638437 0.22522709 0.14910044 0.10713669 0.091242976 0.088401079][0.39574474 0.35887906 0.32546017 0.33740658 0.39592898 0.4687683 0.51396185 0.50444221 0.43560633 0.32512039 0.21046719 0.1204893 0.066998355 0.043794412 0.0387314][0.36462831 0.31998488 0.27887806 0.28338349 0.33743086 0.40942 0.45675093 0.45175856 0.38954166 0.28678781 0.17848748 0.092279613 0.03983641 0.016731767 0.012631241][0.2918472 0.24698439 0.20539212 0.20438448 0.24883941 0.31036687 0.35050943 0.34607077 0.29417351 0.20955318 0.12099919 0.051493522 0.010741971 -0.005196312 -0.005090897][0.18660465 0.14822944 0.11239531 0.10790944 0.13962646 0.18467997 0.21280777 0.20771311 0.16870703 0.10747776 0.04553147 -0.00024237443 -0.02345331 -0.028346712 -0.022225015][0.080202237 0.051907752 0.024494514 0.017116543 0.034012254 0.05963527 0.074105933 0.068466768 0.043241236 0.006600461 -0.027072243 -0.047420762 -0.052434281 -0.047187425 -0.036614612][-0.0036747381 -0.022525517 -0.041311029 -0.049834926 -0.045197569 -0.035823278 -0.032077335 -0.037417725 -0.050903555 -0.067734957 -0.078934163 -0.079690486 -0.071801595 -0.060751386 -0.049018264][-0.059290569 -0.070667818 -0.0812746 -0.08815559 -0.090185843 -0.09029191 -0.092171505 -0.096347339 -0.10228872 -0.10718998 -0.10580266 -0.097000659 -0.084290855 -0.07236968 -0.062074237][-0.087974027 -0.094856575 -0.099684238 -0.10398355 -0.10789591 -0.111525 -0.11505425 -0.11771418 -0.11936005 -0.11848687 -0.11253467 -0.10212784 -0.090425581 -0.08054509 -0.072750688]]...]
INFO - root - 2017-12-11 00:09:10.567188: step 67110, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 57h:02m:53s remains)
INFO - root - 2017-12-11 00:09:18.493259: step 67120, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 58h:21m:31s remains)
INFO - root - 2017-12-11 00:09:26.465778: step 67130, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.773 sec/batch; 56h:58m:00s remains)
INFO - root - 2017-12-11 00:09:34.093748: step 67140, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.581 sec/batch; 42h:50m:15s remains)
INFO - root - 2017-12-11 00:09:42.057541: step 67150, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.775 sec/batch; 57h:08m:19s remains)
INFO - root - 2017-12-11 00:09:49.945555: step 67160, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.812 sec/batch; 59h:51m:35s remains)
INFO - root - 2017-12-11 00:09:57.840537: step 67170, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 58h:42m:36s remains)
INFO - root - 2017-12-11 00:10:05.504815: step 67180, loss = 0.69, batch loss = 0.63 (12.7 examples/sec; 0.632 sec/batch; 46h:32m:34s remains)
INFO - root - 2017-12-11 00:10:13.371038: step 67190, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 58h:00m:52s remains)
INFO - root - 2017-12-11 00:10:21.257155: step 67200, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 57h:34m:44s remains)
2017-12-11 00:10:22.104936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.046260308 -0.048642132 -0.047492098 -0.044455491 -0.040118203 -0.034884021 -0.029840566 -0.026690342 -0.02666009 -0.030492578 -0.038029049 -0.047776688 -0.05649171 -0.061062805 -0.059162747][-0.044856571 -0.045113239 -0.041941449 -0.036864843 -0.029707149 -0.020173471 -0.0098811807 -0.002417512 -0.00097802759 -0.0075322716 -0.021657251 -0.039915275 -0.055701058 -0.063652158 -0.061624575][-0.045116115 -0.041845713 -0.034093272 -0.022386605 -0.0057979864 0.015646933 0.037802447 0.053068824 0.054770604 0.039994169 0.011494382 -0.022955824 -0.051617954 -0.066175461 -0.065601915][-0.050762463 -0.042798541 -0.027353894 -0.0029307462 0.03197683 0.07548783 0.1186857 0.14764637 0.14969714 0.12101253 0.069105372 0.0095311534 -0.039080165 -0.065517791 -0.0699455][-0.056660362 -0.0431063 -0.016686864 0.026233658 0.087344773 0.16170533 0.233916 0.28183633 0.28445351 0.23697203 0.15417527 0.0624683 -0.011247403 -0.053347148 -0.065563574][-0.045418855 -0.026326466 0.011897649 0.074800655 0.16358614 0.26952615 0.37060082 0.43659621 0.43805462 0.3693147 0.25288916 0.12746789 0.028596103 -0.028967664 -0.048869636][-0.0068294224 0.016010925 0.063170217 0.14132687 0.25002623 0.37741596 0.49687698 0.57278186 0.5700292 0.48271254 0.33903858 0.18759532 0.069922961 0.00069937139 -0.025348825][0.049607959 0.073636733 0.12434433 0.20843586 0.3224653 0.45295957 0.5721786 0.64376187 0.63250822 0.53395438 0.37854531 0.21808466 0.094704583 0.021871492 -0.0061936877][0.10211814 0.12371006 0.1705514 0.24808688 0.34963819 0.46265823 0.56240982 0.61661017 0.59514767 0.49641696 0.34948054 0.20158644 0.089358024 0.023792535 -0.00021268464][0.13719401 0.15250792 0.18719517 0.24479501 0.31701618 0.39545521 0.46231917 0.49269021 0.46413392 0.37761256 0.25735819 0.14016493 0.053008553 0.003559395 -0.012025041][0.14774041 0.15351349 0.16992502 0.19899833 0.23275356 0.26890162 0.29927474 0.30791 0.27968535 0.21610494 0.13394019 0.05712653 0.0016715471 -0.028285557 -0.03462667][0.13622646 0.13237692 0.13000305 0.13054052 0.12797205 0.12621868 0.12653862 0.12136007 0.10088478 0.065172017 0.023012269 -0.013756852 -0.038803205 -0.050600238 -0.049016118][0.11847658 0.10806527 0.091082215 0.069259904 0.04026024 0.013014643 -0.0046043037 -0.014472692 -0.023504624 -0.034236815 -0.044374492 -0.050636292 -0.052675188 -0.050416481 -0.042861056][0.10702453 0.094356671 0.069669887 0.034993067 -0.0075190617 -0.045463108 -0.0680452 -0.075029 -0.072612368 -0.065625213 -0.055965997 -0.044530246 -0.0331587 -0.022485552 -0.01231498][0.10049639 0.090758838 0.067787662 0.032837525 -0.0085279169 -0.042601846 -0.059132122 -0.059214596 -0.04924586 -0.034466453 -0.017900113 -0.0010972004 0.014545641 0.028132275 0.037739839]]...]
INFO - root - 2017-12-11 00:10:29.874461: step 67210, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 57h:03m:28s remains)
INFO - root - 2017-12-11 00:10:37.784325: step 67220, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 59h:11m:31s remains)
INFO - root - 2017-12-11 00:10:45.552106: step 67230, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 58h:55m:48s remains)
INFO - root - 2017-12-11 00:10:53.361462: step 67240, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 57h:51m:59s remains)
INFO - root - 2017-12-11 00:11:01.140916: step 67250, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.773 sec/batch; 56h:56m:02s remains)
INFO - root - 2017-12-11 00:11:08.802209: step 67260, loss = 0.70, batch loss = 0.65 (12.7 examples/sec; 0.628 sec/batch; 46h:14m:51s remains)
INFO - root - 2017-12-11 00:11:16.669197: step 67270, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 57h:32m:56s remains)
INFO - root - 2017-12-11 00:11:24.588919: step 67280, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.785 sec/batch; 57h:49m:40s remains)
INFO - root - 2017-12-11 00:11:32.584445: step 67290, loss = 0.69, batch loss = 0.64 (9.5 examples/sec; 0.845 sec/batch; 62h:14m:13s remains)
INFO - root - 2017-12-11 00:11:40.536221: step 67300, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 59h:09m:58s remains)
2017-12-11 00:11:41.336102: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21055423 0.19395125 0.16715427 0.13595864 0.11591266 0.11933035 0.12843916 0.12555154 0.11169792 0.090022393 0.056371447 0.01072847 -0.030207796 -0.056264605 -0.073240146][0.19219549 0.17418188 0.15229081 0.13386321 0.13525699 0.16338697 0.19321792 0.2018162 0.1877024 0.15622987 0.10609857 0.039062962 -0.021308964 -0.058201008 -0.078521922][0.16214564 0.14232612 0.1257461 0.12188354 0.14706437 0.20227638 0.25562635 0.27941382 0.26861894 0.23019129 0.16561121 0.078036986 -0.0026993447 -0.052904442 -0.0784249][0.14138365 0.11834771 0.10378014 0.11091001 0.15596151 0.23378776 0.3067058 0.3433176 0.33653909 0.29354602 0.21804447 0.11395247 0.0157528 -0.046263505 -0.076785259][0.14426388 0.11838049 0.10249457 0.11452925 0.17158909 0.2641606 0.34939483 0.39332038 0.38755575 0.34028903 0.25622129 0.13977949 0.028238539 -0.0431702 -0.077203035][0.17540196 0.14826998 0.12910591 0.14075644 0.20351377 0.30509883 0.39705586 0.44232076 0.43312204 0.38045895 0.28731123 0.15852806 0.035029031 -0.043695722 -0.079561934][0.23259877 0.20743749 0.18479462 0.19202183 0.25488287 0.35894516 0.45004353 0.48951557 0.47290397 0.41398311 0.3105652 0.16900459 0.035443865 -0.047140222 -0.082607791][0.29370177 0.27555364 0.25211596 0.25295222 0.31032121 0.40848911 0.48932481 0.51657814 0.49152777 0.42850119 0.31685171 0.16573735 0.027814379 -0.0538082 -0.086767875][0.33908576 0.33491206 0.31635633 0.31191429 0.35850033 0.44148457 0.50413227 0.51622844 0.48394075 0.41990405 0.30457708 0.15018585 0.014109578 -0.062816858 -0.092172086][0.37122574 0.38428873 0.37447158 0.36605304 0.39632896 0.45382017 0.49029693 0.48420629 0.44394284 0.37959164 0.26622725 0.11852545 -0.0072370912 -0.075491153 -0.099818617][0.39776072 0.42875105 0.43025491 0.420687 0.43321058 0.45917323 0.46309444 0.43299061 0.38045961 0.31374446 0.20703684 0.075217418 -0.032384098 -0.088102765 -0.10643153][0.42221323 0.46847731 0.4798035 0.46916971 0.46318227 0.45462304 0.42150789 0.3631272 0.29616731 0.22778852 0.13381197 0.026562585 -0.056274004 -0.096863367 -0.10912312][0.44525403 0.49795204 0.51030457 0.49387583 0.46865016 0.42710212 0.35925752 0.2754474 0.19806778 0.13245685 0.056941941 -0.020385515 -0.075323224 -0.10015286 -0.10716872][0.46502396 0.50958681 0.510151 0.4823094 0.43997872 0.37288222 0.28034779 0.18183251 0.10294925 0.045547936 -0.0095003434 -0.057699122 -0.0867886 -0.097831368 -0.10119139][0.465803 0.48996645 0.47098127 0.43062294 0.37819737 0.29911447 0.19780535 0.098769642 0.027344026 -0.01819027 -0.054798983 -0.079970062 -0.089871377 -0.091520093 -0.093311258]]...]
INFO - root - 2017-12-11 00:11:49.180847: step 67310, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.812 sec/batch; 59h:47m:59s remains)
INFO - root - 2017-12-11 00:11:56.950441: step 67320, loss = 0.65, batch loss = 0.60 (10.0 examples/sec; 0.800 sec/batch; 58h:56m:04s remains)
INFO - root - 2017-12-11 00:12:04.911425: step 67330, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 57h:08m:25s remains)
INFO - root - 2017-12-11 00:12:12.747186: step 67340, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.804 sec/batch; 59h:15m:18s remains)
INFO - root - 2017-12-11 00:12:20.697767: step 67350, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 59h:56m:11s remains)
INFO - root - 2017-12-11 00:12:28.482237: step 67360, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.755 sec/batch; 55h:34m:54s remains)
INFO - root - 2017-12-11 00:12:36.301385: step 67370, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 57h:47m:38s remains)
INFO - root - 2017-12-11 00:12:44.109119: step 67380, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 58h:34m:53s remains)
INFO - root - 2017-12-11 00:12:52.018878: step 67390, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 57h:26m:53s remains)
INFO - root - 2017-12-11 00:12:59.865551: step 67400, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 58h:21m:31s remains)
2017-12-11 00:13:00.739422: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15083575 0.16397294 0.175695 0.17829096 0.17163195 0.15340367 0.12932822 0.11094888 0.098697387 0.091476023 0.0886589 0.091604926 0.097763076 0.10354492 0.11035941][0.15736726 0.18102981 0.20396104 0.21432762 0.21073376 0.19004425 0.16097519 0.13845295 0.12208322 0.11250919 0.11009029 0.11568787 0.12422776 0.13033858 0.13679287][0.14427409 0.17597982 0.20928578 0.22810869 0.22971721 0.21080703 0.18197486 0.15977587 0.14279597 0.13411491 0.13511021 0.14590973 0.1595258 0.16843197 0.17579255][0.13632472 0.16959725 0.20694102 0.22986503 0.23570466 0.22229846 0.20063166 0.18601246 0.17518306 0.17325591 0.18126041 0.1988081 0.21896482 0.23179317 0.24012858][0.15480225 0.18181628 0.21495447 0.23648079 0.2453946 0.24195494 0.23607972 0.23858079 0.24270926 0.25392354 0.27109212 0.29436669 0.31888336 0.33299041 0.33783075][0.20901684 0.22612108 0.25054172 0.26817873 0.28012869 0.28878856 0.30255795 0.3260363 0.34820423 0.37307829 0.39661816 0.42207819 0.44840312 0.46148697 0.4596338][0.29494855 0.30511007 0.32093349 0.33517537 0.35010457 0.36907351 0.39822966 0.43722013 0.47177804 0.5037657 0.52720684 0.55092126 0.57794166 0.58983719 0.5813325][0.39499465 0.40152246 0.41032252 0.421873 0.43845987 0.46252728 0.49734625 0.54007733 0.57561123 0.60441923 0.62137115 0.64183021 0.67082536 0.68388444 0.67238456][0.48040676 0.48722535 0.49159092 0.50052083 0.51503694 0.53531706 0.56201333 0.593136 0.61612159 0.63159007 0.63775587 0.65528637 0.68818033 0.70566958 0.69706988][0.52465564 0.53474867 0.5370186 0.54150832 0.54833961 0.5552913 0.56070316 0.56628847 0.56569517 0.56118262 0.55508059 0.57052356 0.60864812 0.63358331 0.63355213][0.51859754 0.53266561 0.53274071 0.52913117 0.52248758 0.50949723 0.4868755 0.46110103 0.43361929 0.40920255 0.39334169 0.40743539 0.44924241 0.4807269 0.49051642][0.47074875 0.48915896 0.48742059 0.47483084 0.4540759 0.42250165 0.37597892 0.32442641 0.27592725 0.23756708 0.21588776 0.22721274 0.26648408 0.29885408 0.31443793][0.4006674 0.42214537 0.42110807 0.40441757 0.37668002 0.33609986 0.27878031 0.21582866 0.1588679 0.11612864 0.093184069 0.10030944 0.130964 0.15785512 0.1733821][0.32618675 0.34761849 0.3485986 0.33349043 0.30780166 0.27019656 0.21729866 0.15963183 0.10854951 0.071344964 0.051528465 0.054560527 0.074477896 0.093123406 0.10504798][0.25664675 0.27349758 0.27539268 0.26485512 0.24696442 0.22047825 0.18259495 0.14188187 0.10665154 0.081667423 0.068646669 0.070062175 0.0817199 0.093260683 0.1007845]]...]
INFO - root - 2017-12-11 00:13:08.520562: step 67410, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 57h:57m:49s remains)
INFO - root - 2017-12-11 00:13:16.187419: step 67420, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 57h:25m:33s remains)
INFO - root - 2017-12-11 00:13:24.022225: step 67430, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 58h:15m:05s remains)
INFO - root - 2017-12-11 00:13:31.847628: step 67440, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 57h:26m:22s remains)
INFO - root - 2017-12-11 00:13:39.755457: step 67450, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 56h:49m:17s remains)
INFO - root - 2017-12-11 00:13:47.547961: step 67460, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.790 sec/batch; 58h:10m:47s remains)
INFO - root - 2017-12-11 00:13:55.275491: step 67470, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 55h:40m:08s remains)
INFO - root - 2017-12-11 00:14:03.224194: step 67480, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.825 sec/batch; 60h:42m:19s remains)
INFO - root - 2017-12-11 00:14:11.039409: step 67490, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 56h:25m:52s remains)
INFO - root - 2017-12-11 00:14:18.588973: step 67500, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.773 sec/batch; 56h:53m:21s remains)
2017-12-11 00:14:19.528527: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.029518137 0.060932286 0.09604346 0.13502905 0.1763038 0.21494319 0.2354295 0.22121929 0.17302392 0.10387933 0.028810689 -0.034350082 -0.074339792 -0.090191357 -0.090146005][0.027230775 0.057762232 0.092423528 0.13309507 0.18000968 0.22961292 0.26557538 0.26817164 0.23131292 0.16275403 0.0772881 -0.0026353896 -0.059416335 -0.087751567 -0.093604825][0.024217514 0.052929275 0.085938789 0.12608398 0.1753978 0.23197389 0.28094861 0.30110493 0.28003162 0.21848151 0.1285816 0.035717111 -0.037106551 -0.079748385 -0.094494022][0.020453285 0.047999535 0.079808414 0.1187865 0.16795485 0.2265843 0.28259334 0.31582966 0.31002027 0.25892237 0.17087141 0.071258292 -0.013629159 -0.069354095 -0.094253488][0.016197667 0.04244243 0.072738953 0.10921572 0.15509354 0.21064894 0.2676053 0.30913016 0.31724137 0.27957997 0.19973321 0.1005904 0.0091466187 -0.056827374 -0.091504551][0.011023068 0.035814077 0.064549841 0.098031566 0.13922517 0.18971711 0.24533904 0.29325792 0.31511548 0.29273018 0.22424147 0.12901975 0.033364024 -0.041674212 -0.085969426][0.00576305 0.029007295 0.056285232 0.086708926 0.12261388 0.16716132 0.22049424 0.27401045 0.3093878 0.30252296 0.24566472 0.15452191 0.054731511 -0.029322622 -0.083689414][0.0016632958 0.023893595 0.049768854 0.076418169 0.10499089 0.14031807 0.1874503 0.24253754 0.2880978 0.2952401 0.25089711 0.16668226 0.066855781 -0.022188729 -0.083534382][0.0011718789 0.024423486 0.050721344 0.0746774 0.095900409 0.12052333 0.15892293 0.21249449 0.26564369 0.28621876 0.25626093 0.18292809 0.088503256 0.00032045748 -0.06283202][0.0051948549 0.031867675 0.061527591 0.085927933 0.10322978 0.12037696 0.15222883 0.20476097 0.26480123 0.29845443 0.28330812 0.22215165 0.13592276 0.053044986 -0.0061803628][0.023332985 0.056707233 0.092910923 0.12101997 0.13793813 0.15115163 0.17843717 0.22886933 0.29237264 0.33518019 0.33134109 0.28021023 0.20274048 0.12898222 0.079727754][0.054808654 0.098742649 0.14546107 0.18084882 0.19987912 0.21045959 0.23159133 0.27427563 0.33302259 0.37773785 0.38117144 0.33985671 0.274652 0.21621612 0.18334433][0.09118478 0.14335853 0.19791813 0.23834486 0.25849542 0.26671878 0.28085339 0.31174439 0.35889855 0.39927864 0.40693814 0.3765257 0.32725367 0.28835672 0.27469605][0.12482792 0.17753023 0.2307685 0.26827484 0.2851181 0.29056293 0.29907691 0.31850404 0.35091257 0.38130251 0.3877947 0.36476606 0.3287552 0.30602416 0.30698785][0.13612585 0.1818231 0.2243683 0.25077662 0.25927204 0.26030287 0.26400536 0.27316523 0.28980312 0.30608943 0.30609491 0.28512153 0.25662783 0.24331422 0.25176969]]...]
INFO - root - 2017-12-11 00:14:27.399689: step 67510, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 57h:13m:07s remains)
INFO - root - 2017-12-11 00:14:35.239996: step 67520, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 58h:32m:10s remains)
INFO - root - 2017-12-11 00:14:43.102163: step 67530, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.796 sec/batch; 58h:36m:27s remains)
INFO - root - 2017-12-11 00:14:50.961294: step 67540, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 55h:47m:48s remains)
INFO - root - 2017-12-11 00:14:58.797374: step 67550, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 57h:17m:43s remains)
INFO - root - 2017-12-11 00:15:06.663634: step 67560, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.812 sec/batch; 59h:43m:28s remains)
INFO - root - 2017-12-11 00:15:14.510573: step 67570, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 58h:14m:00s remains)
INFO - root - 2017-12-11 00:15:22.202013: step 67580, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 57h:32m:18s remains)
INFO - root - 2017-12-11 00:15:29.935162: step 67590, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 56h:27m:07s remains)
INFO - root - 2017-12-11 00:15:37.756700: step 67600, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.826 sec/batch; 60h:46m:18s remains)
2017-12-11 00:15:38.699158: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15889952 0.17323273 0.16308665 0.13960105 0.12117087 0.11818118 0.12623897 0.12746385 0.10581198 0.057847247 0.0019825213 -0.041720845 -0.06638328 -0.074510366 -0.072865739][0.14859289 0.16279265 0.15289664 0.13162379 0.11725382 0.11963953 0.13443814 0.14289263 0.1261497 0.077054374 0.014084054 -0.03923066 -0.07255663 -0.085748404 -0.084649406][0.098305404 0.11003872 0.10501535 0.09489762 0.094334051 0.10936671 0.13447888 0.15172866 0.14288765 0.09838903 0.034036145 -0.02574111 -0.067642443 -0.087741196 -0.089594558][0.045012809 0.053420305 0.056223482 0.063452788 0.084285758 0.11802703 0.15559687 0.18008107 0.17686442 0.1369655 0.072486162 0.0066740508 -0.044734605 -0.073778294 -0.081506766][0.018827328 0.022158841 0.03131536 0.056655906 0.10268883 0.16045451 0.21393205 0.2451081 0.24410442 0.20578323 0.13920619 0.065380566 0.0022502749 -0.038066279 -0.054333568][0.030126428 0.026742486 0.03738755 0.075066395 0.14244545 0.22406593 0.29561725 0.33495906 0.33515918 0.29613331 0.22528374 0.14143254 0.064816333 0.011608285 -0.015275449][0.080080368 0.069213852 0.07293205 0.11001822 0.18563414 0.28290716 0.37129062 0.42205098 0.42653391 0.38723141 0.31102261 0.21598369 0.12521335 0.0589321 0.021695329][0.15563546 0.13688987 0.12567931 0.14883894 0.21719421 0.31789234 0.41808876 0.48209766 0.49448243 0.45581856 0.3735041 0.2672278 0.16378422 0.086967729 0.042636089][0.24138422 0.21643327 0.18685283 0.18753147 0.23571886 0.3261188 0.42783102 0.50032949 0.51947993 0.48100168 0.39395273 0.28125727 0.17271592 0.093538642 0.0496355][0.32170361 0.29430637 0.24924989 0.22691883 0.24909602 0.31872606 0.40987617 0.48066616 0.49977219 0.45849398 0.36917135 0.25913373 0.15910043 0.092247531 0.061828997][0.384532 0.35909516 0.3064006 0.26869413 0.2680718 0.31269473 0.38255677 0.43869892 0.44689748 0.39733946 0.30774984 0.21063039 0.13491194 0.097388431 0.094993062][0.41020194 0.39131775 0.34209 0.30355787 0.29351908 0.31985217 0.36639076 0.39970607 0.38780817 0.32434651 0.23446718 0.15614195 0.11449583 0.11688745 0.15172948][0.38032916 0.37132972 0.33747521 0.31472382 0.31319478 0.33491367 0.36385357 0.3730346 0.33766854 0.25867012 0.16859663 0.10964961 0.10215157 0.1439824 0.21523954][0.3024399 0.30613485 0.29680949 0.30150911 0.32146421 0.34974581 0.36801669 0.35589463 0.29910526 0.20752555 0.11879401 0.076126888 0.094932631 0.16584274 0.26381257][0.21176313 0.22918014 0.24741952 0.28259671 0.32724029 0.36546195 0.37649688 0.34734243 0.27554414 0.1786164 0.094524451 0.063023254 0.094905779 0.17765513 0.28654379]]...]
INFO - root - 2017-12-11 00:15:46.555437: step 67610, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 57h:17m:23s remains)
INFO - root - 2017-12-11 00:15:54.427864: step 67620, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 57h:18m:16s remains)
INFO - root - 2017-12-11 00:16:02.289688: step 67630, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 56h:18m:26s remains)
INFO - root - 2017-12-11 00:16:10.039242: step 67640, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 59h:28m:31s remains)
INFO - root - 2017-12-11 00:16:17.971566: step 67650, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 58h:34m:22s remains)
INFO - root - 2017-12-11 00:16:25.706020: step 67660, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.809 sec/batch; 59h:30m:05s remains)
INFO - root - 2017-12-11 00:16:33.572640: step 67670, loss = 0.69, batch loss = 0.63 (10.7 examples/sec; 0.750 sec/batch; 55h:12m:08s remains)
INFO - root - 2017-12-11 00:16:41.200630: step 67680, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 56h:37m:35s remains)
INFO - root - 2017-12-11 00:16:48.975975: step 67690, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 58h:29m:41s remains)
INFO - root - 2017-12-11 00:16:56.824189: step 67700, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 59h:03m:05s remains)
2017-12-11 00:16:57.644338: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13542047 0.13833237 0.13372333 0.12665072 0.12251068 0.12389765 0.13999593 0.16868585 0.19786887 0.21395606 0.21141301 0.1932725 0.16761826 0.14587839 0.13290836][0.14658536 0.14327334 0.13390329 0.12361122 0.1163129 0.11390032 0.12533301 0.15013584 0.17780589 0.19671161 0.20136188 0.19256087 0.17582844 0.16082576 0.15303071][0.17546941 0.16492386 0.14663184 0.12643695 0.10800734 0.094471134 0.0955617 0.11306403 0.13908918 0.16369459 0.18061164 0.18817097 0.18797407 0.18447919 0.18053575][0.22618474 0.20996797 0.18204521 0.14994597 0.11851581 0.093111433 0.084162556 0.094876304 0.11981959 0.14938864 0.17640831 0.19677046 0.20869379 0.21097468 0.20258318][0.27856258 0.26089919 0.22886533 0.19200599 0.15640537 0.12955621 0.12054434 0.13087708 0.15585448 0.18623862 0.21410291 0.23366819 0.24160591 0.23479843 0.21108462][0.30184186 0.28887314 0.26492891 0.23946306 0.2171237 0.20476124 0.20812201 0.22479036 0.24898727 0.2724942 0.28819722 0.29111686 0.27882916 0.25037423 0.20634988][0.28549248 0.2823346 0.27853638 0.28015569 0.28671169 0.30106083 0.32537502 0.35280758 0.37405249 0.38179189 0.37249953 0.3462331 0.30358869 0.2484176 0.18621533][0.23960456 0.24785134 0.26820558 0.30126905 0.3393209 0.37969694 0.42299417 0.45967731 0.47538453 0.462477 0.42382136 0.36776209 0.29853174 0.22412418 0.15372336][0.18268739 0.19907802 0.23685966 0.29128227 0.3477411 0.39946413 0.44896194 0.48703939 0.49463075 0.46353257 0.40336123 0.33013263 0.25023788 0.17239037 0.10674658][0.14289188 0.15837872 0.19600984 0.24901707 0.3005048 0.34335119 0.38363332 0.41391635 0.41281712 0.37238011 0.30597398 0.23361762 0.16151856 0.095601112 0.045064952][0.13529018 0.13972227 0.15782534 0.1857103 0.21064787 0.22898456 0.24918152 0.2653313 0.25756165 0.21845447 0.16114609 0.1047511 0.05345396 0.0093720024 -0.020716481][0.14910135 0.13592999 0.12385371 0.11491229 0.10509342 0.096098147 0.095790595 0.098609932 0.08911024 0.060492359 0.022064259 -0.011657332 -0.038810667 -0.059979253 -0.071286589][0.16320202 0.13231669 0.091234043 0.049292184 0.011723453 -0.015967099 -0.028795082 -0.032969683 -0.038908113 -0.053166371 -0.071454778 -0.084498778 -0.091825 -0.0952376 -0.093608491][0.1706001 0.12565945 0.0640542 0.0012095909 -0.050770313 -0.085337043 -0.10091905 -0.10526197 -0.1053171 -0.10739677 -0.1105998 -0.10937998 -0.10415024 -0.097144611 -0.089151636][0.17077033 0.11692681 0.044647217 -0.026030293 -0.0795119 -0.11084814 -0.12159609 -0.12140186 -0.11580817 -0.1105132 -0.10555915 -0.097416542 -0.087197363 -0.077191256 -0.068996705]]...]
INFO - root - 2017-12-11 00:17:05.511333: step 67710, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 58h:04m:34s remains)
INFO - root - 2017-12-11 00:17:13.538488: step 67720, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 57h:09m:30s remains)
INFO - root - 2017-12-11 00:17:21.365658: step 67730, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 56h:13m:59s remains)
INFO - root - 2017-12-11 00:17:28.950392: step 67740, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 57h:19m:44s remains)
INFO - root - 2017-12-11 00:17:36.687251: step 67750, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.757 sec/batch; 55h:42m:11s remains)
INFO - root - 2017-12-11 00:17:44.389121: step 67760, loss = 0.69, batch loss = 0.63 (11.6 examples/sec; 0.687 sec/batch; 50h:32m:48s remains)
INFO - root - 2017-12-11 00:17:52.076425: step 67770, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 57h:43m:39s remains)
INFO - root - 2017-12-11 00:17:59.999527: step 67780, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 59h:26m:53s remains)
INFO - root - 2017-12-11 00:18:07.865742: step 67790, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 56h:39m:27s remains)
INFO - root - 2017-12-11 00:18:15.690815: step 67800, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 56h:56m:46s remains)
2017-12-11 00:18:16.562251: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29439962 0.28568929 0.26892918 0.24392882 0.22277732 0.22926912 0.26812479 0.3251932 0.37760422 0.40879849 0.40885356 0.3731696 0.3220101 0.27515393 0.2333567][0.25567263 0.2409666 0.22062576 0.19397958 0.17305481 0.18155436 0.22316895 0.28365174 0.34188393 0.37876439 0.38093168 0.34220734 0.28524148 0.2337244 0.19081347][0.21462291 0.19827858 0.18000993 0.15831453 0.14299703 0.1521579 0.18730393 0.23684114 0.28687951 0.32019281 0.32240546 0.2865541 0.23386317 0.18699649 0.14878963][0.1818428 0.17147455 0.16175416 0.15159807 0.14834251 0.16166896 0.18933041 0.2224272 0.25626522 0.27795497 0.27413177 0.23941222 0.19285457 0.15337858 0.12103132][0.16568734 0.16574799 0.16781099 0.17209312 0.18488613 0.20826973 0.23490719 0.25695816 0.2774924 0.28726655 0.27406251 0.23556677 0.18992984 0.15273368 0.12040583][0.17163165 0.18056858 0.19192618 0.20789912 0.23404606 0.26752993 0.29733047 0.31699821 0.33524016 0.34320349 0.32715425 0.28522944 0.23500212 0.19134003 0.14906624][0.20556593 0.21674229 0.22954603 0.24884751 0.27812836 0.31292793 0.34267044 0.36475289 0.39161757 0.4113088 0.40535286 0.36714691 0.31192085 0.2558282 0.19550331][0.25322735 0.26069203 0.26832485 0.283403 0.30532113 0.33020547 0.35294494 0.37777242 0.41929933 0.4605647 0.47701824 0.45224738 0.39623907 0.32641143 0.24494076][0.29822224 0.301151 0.30105349 0.306846 0.31442568 0.32175234 0.3317636 0.35545412 0.40763068 0.46759194 0.50701225 0.50039911 0.45064849 0.37398097 0.27845412][0.32902086 0.33013514 0.32325882 0.31841934 0.31089905 0.30105945 0.29852271 0.31778419 0.37147072 0.43792757 0.48942494 0.49690652 0.45773312 0.38446981 0.28803435][0.33863834 0.34198898 0.33242768 0.31989846 0.30175617 0.28069741 0.26974311 0.28325564 0.32998016 0.38967988 0.43788114 0.44859344 0.41812551 0.35521102 0.27006152][0.33742973 0.34581167 0.33721578 0.32029262 0.295795 0.26938996 0.2546733 0.26338184 0.29929745 0.34444672 0.37799081 0.38099164 0.35301882 0.30092114 0.2324269][0.33435297 0.34854832 0.34319553 0.32539016 0.29871693 0.27221522 0.2586661 0.26645193 0.294409 0.32585457 0.34225634 0.3314769 0.29840115 0.25094622 0.19503263][0.33077466 0.35068867 0.34949207 0.33335653 0.30750233 0.28432906 0.27542683 0.2856369 0.31024948 0.33277908 0.33583823 0.31140009 0.26912403 0.21989021 0.16905881][0.32955089 0.35292727 0.35390517 0.3394233 0.31608143 0.29827246 0.29579908 0.30936372 0.33338064 0.35238579 0.34943563 0.31742379 0.26804736 0.21478646 0.16267262]]...]
INFO - root - 2017-12-11 00:18:24.381997: step 67810, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.772 sec/batch; 56h:45m:30s remains)
INFO - root - 2017-12-11 00:18:31.998039: step 67820, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 58h:12m:28s remains)
INFO - root - 2017-12-11 00:18:39.798545: step 67830, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 57h:46m:59s remains)
INFO - root - 2017-12-11 00:18:47.633571: step 67840, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 56h:14m:21s remains)
INFO - root - 2017-12-11 00:18:55.215145: step 67850, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 58h:20m:50s remains)
INFO - root - 2017-12-11 00:19:03.029101: step 67860, loss = 0.69, batch loss = 0.64 (10.6 examples/sec; 0.752 sec/batch; 55h:18m:44s remains)
INFO - root - 2017-12-11 00:19:10.879849: step 67870, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 56h:53m:39s remains)
INFO - root - 2017-12-11 00:19:18.799104: step 67880, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 57h:27m:39s remains)
INFO - root - 2017-12-11 00:19:26.586290: step 67890, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 58h:23m:37s remains)
INFO - root - 2017-12-11 00:19:34.275602: step 67900, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 56h:06m:42s remains)
2017-12-11 00:19:35.089772: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25981963 0.25888687 0.24754222 0.2414712 0.24371378 0.24732096 0.25349861 0.259849 0.26157331 0.26457545 0.281547 0.31200686 0.3369759 0.34389046 0.34093556][0.31937212 0.3160356 0.30327198 0.29889911 0.30515668 0.31260133 0.32289264 0.3327131 0.33492592 0.33728456 0.35174268 0.37574306 0.38902482 0.38430896 0.37514472][0.37340021 0.36797041 0.35157645 0.34600529 0.35512879 0.36836246 0.38558963 0.400631 0.40292951 0.40265021 0.41121164 0.42432487 0.42044008 0.39880916 0.38044107][0.42382124 0.42202482 0.40561378 0.39844078 0.40644455 0.42189476 0.44480702 0.4653585 0.46903095 0.46663204 0.47052145 0.47517982 0.45648625 0.4190869 0.39209414][0.46328121 0.47276893 0.46338657 0.45716316 0.46067575 0.47216696 0.49436492 0.514939 0.5176335 0.51343572 0.51539207 0.51609272 0.4883185 0.44031924 0.40812847][0.47876874 0.49918556 0.49860519 0.49451417 0.49324173 0.49900284 0.51626247 0.53115845 0.52813333 0.51948321 0.51784706 0.51383364 0.48012432 0.42849386 0.39870977][0.47687003 0.50312912 0.50717348 0.5036937 0.49896336 0.49819496 0.50412083 0.5040639 0.48653924 0.46614635 0.45523226 0.44467136 0.40998554 0.36384493 0.34432715][0.47054014 0.49940535 0.50379843 0.497719 0.4881216 0.479621 0.47099715 0.45118496 0.41416541 0.37668863 0.35188338 0.33348989 0.30150324 0.26662895 0.26017132][0.47025603 0.50315654 0.50801933 0.50005627 0.48705178 0.4724769 0.450127 0.40981638 0.35126451 0.29425427 0.25536484 0.23198716 0.20688683 0.18514405 0.1888404][0.48941937 0.52359754 0.52749085 0.51919293 0.50701261 0.49261689 0.46352658 0.4086377 0.33345297 0.26171795 0.21397217 0.19083366 0.17735881 0.1707994 0.18271397][0.52016819 0.55219364 0.5537979 0.54684132 0.53953171 0.53073758 0.5016427 0.43973961 0.35507998 0.27421582 0.2208831 0.20085572 0.20276488 0.21511173 0.23697603][0.52420175 0.5575009 0.56190574 0.56234103 0.56482911 0.56494361 0.53924972 0.47503585 0.38581148 0.29948837 0.24213959 0.22605065 0.24435438 0.27773112 0.3127062][0.46813369 0.50063491 0.50834316 0.51700443 0.52988374 0.5408501 0.52424818 0.46735892 0.38466638 0.30282104 0.24848829 0.23859651 0.27056947 0.32131603 0.36899906][0.36551449 0.39041695 0.39499271 0.40312898 0.41775477 0.43488187 0.43039632 0.39103657 0.328881 0.2656956 0.22562647 0.22491771 0.26324064 0.31873849 0.36878219][0.24741338 0.26209411 0.25952649 0.25926545 0.26562655 0.27903944 0.28050673 0.25798231 0.22002095 0.18148381 0.16095375 0.1704683 0.2089195 0.25799853 0.29971144]]...]
INFO - root - 2017-12-11 00:19:42.868182: step 67910, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 58h:29m:44s remains)
INFO - root - 2017-12-11 00:19:50.679123: step 67920, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 57h:47m:49s remains)
INFO - root - 2017-12-11 00:19:58.563956: step 67930, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 58h:20m:05s remains)
INFO - root - 2017-12-11 00:20:06.264438: step 67940, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 59h:30m:43s remains)
INFO - root - 2017-12-11 00:20:14.164411: step 67950, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 57h:28m:46s remains)
INFO - root - 2017-12-11 00:20:22.038910: step 67960, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.822 sec/batch; 60h:23m:55s remains)
INFO - root - 2017-12-11 00:20:29.892585: step 67970, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 56h:21m:49s remains)
INFO - root - 2017-12-11 00:20:37.540168: step 67980, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 56h:42m:27s remains)
INFO - root - 2017-12-11 00:20:45.390447: step 67990, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 57h:38m:11s remains)
INFO - root - 2017-12-11 00:20:53.254413: step 68000, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 57h:29m:57s remains)
2017-12-11 00:20:54.058190: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23111919 0.22323407 0.19263983 0.16340901 0.14161555 0.12987162 0.12630436 0.13149893 0.13841748 0.1375109 0.14001814 0.16121317 0.194939 0.22722752 0.25431028][0.26838759 0.27348545 0.24693953 0.20998015 0.1712162 0.13823563 0.11308786 0.10147121 0.10166527 0.10292709 0.11187038 0.14088751 0.18703885 0.23378512 0.27150047][0.3050586 0.3254402 0.30910414 0.27194536 0.22323628 0.17425591 0.12975459 0.09998633 0.089597546 0.087795369 0.095512174 0.1199363 0.16280295 0.21052523 0.2515412][0.32210964 0.35448474 0.35108155 0.32218853 0.27630275 0.22430687 0.17163348 0.13030396 0.10955019 0.10024027 0.099737585 0.11095231 0.13939823 0.17897877 0.21891248][0.31693494 0.35451022 0.36025745 0.34265286 0.30970529 0.26928616 0.22495756 0.18552525 0.16001323 0.14182292 0.12902007 0.12514177 0.13820884 0.16800126 0.20888285][0.29372168 0.3283329 0.33541641 0.3265143 0.30995736 0.29080552 0.269125 0.24646193 0.22615717 0.20366897 0.1808628 0.16497302 0.1662209 0.18786407 0.23148452][0.2794047 0.30759445 0.30979103 0.30348495 0.29833695 0.29872689 0.30202508 0.30323416 0.29758248 0.28046644 0.25691706 0.23766832 0.23279963 0.2458674 0.28624314][0.3120091 0.33248109 0.32329664 0.3087728 0.30134919 0.30769813 0.32524112 0.34608984 0.35778934 0.3529574 0.33749691 0.32356873 0.31877571 0.32324681 0.35191917][0.37796581 0.39060822 0.3674159 0.33748493 0.31620833 0.31399816 0.33134094 0.36164656 0.38778275 0.39822981 0.39817008 0.39901444 0.40333426 0.40521368 0.42058823][0.45151934 0.45863196 0.42412084 0.37849751 0.33972535 0.32082286 0.32621786 0.35445553 0.38605368 0.40636182 0.4200547 0.43788028 0.45726615 0.46468794 0.47383025][0.47896689 0.48275772 0.44317171 0.38934046 0.33989421 0.30735084 0.29933462 0.31888276 0.34582168 0.36485907 0.38155138 0.4069244 0.43520448 0.44809338 0.45597652][0.41172409 0.41317171 0.37611964 0.32545128 0.2772806 0.24146257 0.226928 0.23837748 0.25656939 0.26744524 0.2774258 0.29769477 0.32234222 0.33351296 0.33924603][0.2697587 0.26816878 0.23860587 0.19852559 0.15961042 0.128686 0.11404896 0.11980084 0.12927969 0.13129331 0.13194792 0.1416395 0.15636389 0.16200727 0.16432066][0.10124492 0.097114749 0.077351719 0.051074143 0.025321307 0.0038421918 -0.0074122157 -0.0055456315 -0.0030713559 -0.0073752673 -0.013685477 -0.01317487 -0.00757012 -0.0066716657 -0.0069719278][-0.02961814 -0.035369847 -0.046287786 -0.059942648 -0.073015369 -0.08423578 -0.090723932 -0.0906498 -0.091838762 -0.09823107 -0.10690677 -0.11140431 -0.11192524 -0.11432707 -0.11656488]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 00:21:01.894761: step 68010, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 58h:22m:57s remains)
INFO - root - 2017-12-11 00:21:09.808320: step 68020, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 58h:17m:27s remains)
INFO - root - 2017-12-11 00:21:17.502162: step 68030, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 57h:56m:32s remains)
INFO - root - 2017-12-11 00:21:25.338864: step 68040, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 56h:38m:22s remains)
INFO - root - 2017-12-11 00:21:33.210609: step 68050, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 58h:22m:51s remains)
INFO - root - 2017-12-11 00:21:41.000590: step 68060, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 57h:12m:31s remains)
INFO - root - 2017-12-11 00:21:48.871764: step 68070, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.827 sec/batch; 60h:45m:47s remains)
INFO - root - 2017-12-11 00:21:56.770627: step 68080, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 57h:31m:27s remains)
INFO - root - 2017-12-11 00:22:04.543679: step 68090, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 57h:45m:16s remains)
INFO - root - 2017-12-11 00:22:12.441238: step 68100, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 57h:05m:08s remains)
2017-12-11 00:22:13.327749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.016826376 -0.016980959 -0.017246896 -0.017789802 -0.017931618 -0.017271802 -0.015598526 -0.01337239 -0.011557155 -0.010719153 -0.010192857 -0.0093005626 -0.0088926414 -0.010236674 -0.013397121][-0.018222408 -0.018585632 -0.018673878 -0.018020295 -0.015633265 -0.011304011 -0.0057649235 -0.0002253647 0.0039169518 0.00608191 0.0074147019 0.0086430535 0.00828212 0.0041922014 -0.0035549218][-0.014332375 -0.014639053 -0.013810116 -0.010585446 -0.0037498542 0.0063389982 0.017426984 0.027310688 0.034169383 0.037732095 0.03967993 0.040738851 0.038620017 0.029927498 0.014936575][-0.0049770013 -0.0049866508 -0.0023714334 0.004972605 0.018157914 0.035759218 0.053513229 0.068063572 0.07728298 0.081489377 0.083100587 0.082830258 0.077477418 0.062387805 0.038204003][0.0066973004 0.0078492034 0.013542977 0.026399583 0.047096688 0.072778031 0.097043812 0.11537002 0.12551787 0.12858844 0.12791342 0.12450681 0.11458462 0.092589185 0.059524089][0.013937145 0.016310712 0.02570371 0.04471099 0.073141769 0.10647306 0.13619579 0.15682811 0.16618031 0.16626598 0.16147016 0.15344679 0.13864498 0.11115123 0.07220006][0.014791073 0.017234856 0.028912695 0.052355293 0.086600587 0.12547262 0.15872261 0.17991477 0.18684685 0.18257938 0.17258516 0.15960163 0.1409266 0.11097548 0.070938654][0.013055592 0.013017621 0.023481881 0.047139056 0.08263123 0.12277414 0.15643647 0.17636982 0.1801822 0.17162287 0.15703772 0.14041263 0.12035262 0.092095196 0.056259219][0.01755837 0.010576419 0.014541241 0.033052485 0.064323246 0.10068145 0.13122983 0.14829513 0.14919516 0.13777527 0.12080655 0.10306611 0.08445058 0.061352998 0.033543039][0.034437075 0.01545712 0.0078632263 0.016964704 0.039902661 0.06844838 0.092698976 0.10527273 0.10347197 0.090964161 0.074366085 0.05843813 0.043828823 0.027886502 0.0096345376][0.062299848 0.028360156 0.0066919411 0.0046442882 0.017444128 0.036028758 0.051889475 0.058793511 0.054552387 0.042589869 0.028868262 0.017226469 0.00817604 -0.0003564911 -0.0098087927][0.091995314 0.045595959 0.012489432 0.0010030866 0.0043073869 0.012820499 0.019866312 0.020974142 0.014751376 0.0042298473 -0.0056569986 -0.01240411 -0.016149182 -0.018655008 -0.021582097][0.10968397 0.058299314 0.020603146 0.00441918 0.0013973389 0.0021663667 0.0019777298 -0.0016406289 -0.0091933766 -0.018105153 -0.024540169 -0.027346488 -0.0274107 -0.026290746 -0.025617197][0.10560846 0.057208445 0.022182569 0.0070157037 0.0027370483 0.00060694508 -0.0029564076 -0.0091219088 -0.017547432 -0.025670407 -0.030225527 -0.030979814 -0.029403707 -0.026965104 -0.025104268][0.082211517 0.041948311 0.01427835 0.0041796761 0.0030663717 0.0028651077 5.648041e-05 -0.0063756639 -0.015400571 -0.023876918 -0.028348291 -0.028838281 -0.027106903 -0.024741342 -0.023015473]]...]
INFO - root - 2017-12-11 00:22:21.114177: step 68110, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 55h:49m:46s remains)
INFO - root - 2017-12-11 00:22:28.679761: step 68120, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 57h:58m:11s remains)
INFO - root - 2017-12-11 00:22:36.489113: step 68130, loss = 0.67, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 57h:11m:28s remains)
INFO - root - 2017-12-11 00:22:44.150571: step 68140, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 58h:40m:40s remains)
INFO - root - 2017-12-11 00:22:52.059549: step 68150, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 57h:05m:39s remains)
INFO - root - 2017-12-11 00:22:59.941427: step 68160, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.748 sec/batch; 54h:56m:11s remains)
INFO - root - 2017-12-11 00:23:07.733762: step 68170, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.781 sec/batch; 57h:20m:07s remains)
INFO - root - 2017-12-11 00:23:15.449106: step 68180, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 57h:28m:18s remains)
INFO - root - 2017-12-11 00:23:23.388277: step 68190, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.823 sec/batch; 60h:25m:58s remains)
INFO - root - 2017-12-11 00:23:31.174651: step 68200, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 56h:53m:00s remains)
2017-12-11 00:23:32.026309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.063652746 -0.057504863 -0.052945919 -0.0538702 -0.058721267 -0.06270501 -0.060787685 -0.052054394 -0.039227441 -0.02663376 -0.018434111 -0.015795715 -0.015643498 -0.012996939 -0.00395212][-0.066897683 -0.055284295 -0.044810914 -0.039512187 -0.038254291 -0.037127323 -0.032468949 -0.023902597 -0.013924811 -0.0062578754 -0.0025611168 -0.0025729353 -0.0017224513 0.0056910994 0.022081865][-0.050302684 -0.0371974 -0.023372285 -0.012196857 -0.0024163714 0.00862004 0.022037573 0.036456674 0.048324641 0.054085668 0.053426944 0.048078645 0.045271356 0.051929176 0.069469959][-0.016719181 -0.0070980685 0.0067877183 0.022863684 0.042585704 0.067241028 0.094927572 0.12084611 0.13882621 0.14391492 0.13663788 0.12122549 0.10849002 0.10772856 0.11983091][0.024363229 0.029967295 0.045019951 0.068275407 0.10029308 0.14044486 0.18344527 0.22005366 0.24127489 0.24145244 0.22293268 0.19325449 0.16671604 0.15433082 0.15579195][0.073583841 0.078361191 0.0978302 0.13055559 0.1744287 0.22649485 0.27889156 0.31857279 0.33478186 0.32283628 0.28835016 0.24256609 0.2017999 0.17699404 0.16653408][0.13194636 0.14217527 0.16847596 0.20926522 0.25920066 0.31317633 0.36280873 0.39350039 0.39506218 0.36536288 0.31377217 0.25442934 0.20348637 0.16992797 0.15082304][0.18955652 0.20929319 0.24164551 0.28476405 0.3305788 0.37352911 0.40768653 0.42044213 0.40396622 0.35849357 0.29572657 0.23075892 0.17733321 0.14119236 0.11864817][0.22737607 0.25551754 0.28961715 0.32725719 0.35938165 0.38153857 0.39257106 0.38488397 0.35403773 0.30141002 0.23845962 0.17877528 0.13217796 0.10079255 0.080408223][0.24327679 0.27320173 0.30122668 0.325637 0.3379471 0.33609211 0.32382062 0.29940015 0.26145008 0.21138792 0.1583261 0.1123488 0.079153456 0.057681352 0.043698784][0.24523373 0.2660121 0.27846462 0.28363094 0.27535942 0.2541897 0.22685887 0.19521722 0.15918463 0.11915839 0.081223525 0.05130595 0.031694561 0.019782396 0.012288728][0.23754665 0.2402681 0.23228194 0.21875311 0.19561365 0.16474274 0.13307172 0.10322075 0.074742265 0.047267538 0.024477609 0.0084649092 -0.0011036263 -0.0069029657 -0.010564866][0.22377427 0.20558989 0.17901304 0.15302713 0.1240112 0.093122825 0.065617524 0.043242861 0.024983071 0.010184056 0.0005358315 -0.0051459181 -0.0089460528 -0.012681718 -0.016150087][0.20042066 0.16617653 0.12820327 0.098348804 0.072366558 0.049235981 0.031747088 0.020225918 0.013406953 0.010408869 0.011227374 0.012184392 0.010249085 0.0047687683 -0.0021890947][0.16772483 0.12671348 0.086674668 0.060486127 0.043141872 0.03143407 0.025969328 0.026043309 0.030542051 0.03808444 0.047056034 0.052002709 0.049558923 0.040177293 0.027639773]]...]
INFO - root - 2017-12-11 00:23:39.636559: step 68210, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 58h:09m:54s remains)
INFO - root - 2017-12-11 00:23:46.906928: step 68220, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.780 sec/batch; 57h:17m:38s remains)
INFO - root - 2017-12-11 00:23:54.719641: step 68230, loss = 0.67, batch loss = 0.61 (10.4 examples/sec; 0.766 sec/batch; 56h:13m:35s remains)
INFO - root - 2017-12-11 00:24:02.633977: step 68240, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 57h:43m:32s remains)
INFO - root - 2017-12-11 00:24:10.591797: step 68250, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 57h:47m:10s remains)
INFO - root - 2017-12-11 00:24:18.446591: step 68260, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 58h:21m:37s remains)
INFO - root - 2017-12-11 00:24:26.168249: step 68270, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 56h:37m:19s remains)
INFO - root - 2017-12-11 00:24:33.903959: step 68280, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.765 sec/batch; 56h:07m:20s remains)
INFO - root - 2017-12-11 00:24:41.725848: step 68290, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.756 sec/batch; 55h:31m:01s remains)
INFO - root - 2017-12-11 00:24:49.145385: step 68300, loss = 0.69, batch loss = 0.63 (13.6 examples/sec; 0.587 sec/batch; 43h:03m:22s remains)
2017-12-11 00:24:50.035240: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.087677084 0.073307984 0.051860973 0.033270892 0.033413164 0.051364426 0.07635431 0.09758807 0.11612427 0.13508964 0.15536551 0.17739628 0.19528189 0.21087123 0.21928273][0.19678035 0.18001722 0.14244641 0.099873058 0.077006534 0.074910156 0.085055619 0.097961672 0.11455536 0.1359759 0.1605953 0.18772425 0.21038359 0.23057564 0.24531819][0.33287293 0.3202278 0.27154717 0.20808996 0.16191016 0.13540031 0.12401756 0.12171531 0.12948172 0.14698564 0.17076281 0.20006251 0.22665788 0.25044855 0.27197337][0.45579198 0.45457876 0.40437362 0.33113497 0.27140021 0.22669834 0.19574888 0.17681497 0.17091799 0.17805482 0.19497906 0.22084817 0.24727543 0.27106521 0.29750031][0.52012163 0.53555739 0.49660805 0.43104085 0.37557793 0.32749313 0.28593689 0.25391224 0.23353757 0.228286 0.23624599 0.25586671 0.2790412 0.29982182 0.32785994][0.5225203 0.55654603 0.53869766 0.49507064 0.45832938 0.42019349 0.37992573 0.34415928 0.31618217 0.30521551 0.30955034 0.32561976 0.34391066 0.35723189 0.37855539][0.48550323 0.53613263 0.54327559 0.52657831 0.51342046 0.49201736 0.46204185 0.43201751 0.4062005 0.40095261 0.41088662 0.42867526 0.44167721 0.44304904 0.44805586][0.42449588 0.487739 0.51727748 0.5248099 0.53321707 0.52921426 0.51204872 0.49141848 0.47535938 0.48631102 0.51115167 0.53695005 0.54757404 0.53727508 0.52115679][0.36433339 0.42748642 0.46605384 0.4868758 0.50882584 0.51783985 0.51086408 0.49763697 0.49308303 0.5230369 0.56582254 0.60283989 0.61495137 0.59798366 0.56352842][0.33789369 0.38157398 0.405498 0.41970327 0.4400149 0.45106015 0.44535983 0.43204296 0.43229473 0.47224087 0.52572256 0.57129043 0.58803409 0.57209826 0.52961004][0.33800116 0.34722084 0.33727354 0.32860559 0.33516964 0.33910728 0.32813129 0.31000891 0.309074 0.3487227 0.40310928 0.4509989 0.47243392 0.46306264 0.42374033][0.34395063 0.3113496 0.25883853 0.22057243 0.20865893 0.20247345 0.18590398 0.1647305 0.16194056 0.19535343 0.24319245 0.28734738 0.31178132 0.31130287 0.28271377][0.34138027 0.26906753 0.17746787 0.11355242 0.086324759 0.073249541 0.056325678 0.038755994 0.038362227 0.065785579 0.10443648 0.14106965 0.16506344 0.17150864 0.15522151][0.31481391 0.2176948 0.10398993 0.028786032 -0.0031080572 -0.014623929 -0.023597406 -0.02994249 -0.023007913 0.0013621522 0.031860489 0.060679417 0.082115665 0.0920397 0.085348241][0.27379045 0.1718584 0.057861455 -0.012614708 -0.038313311 -0.040747009 -0.036190234 -0.028045194 -0.011939747 0.012336195 0.037547227 0.05969353 0.076381966 0.084635854 0.081183381]]...]
INFO - root - 2017-12-11 00:24:57.824027: step 68310, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 57h:20m:55s remains)
INFO - root - 2017-12-11 00:25:05.729131: step 68320, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 59h:17m:00s remains)
INFO - root - 2017-12-11 00:25:13.642843: step 68330, loss = 0.67, batch loss = 0.62 (10.2 examples/sec; 0.785 sec/batch; 57h:35m:38s remains)
INFO - root - 2017-12-11 00:25:21.512063: step 68340, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.757 sec/batch; 55h:32m:04s remains)
INFO - root - 2017-12-11 00:25:29.485651: step 68350, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 57h:24m:18s remains)
INFO - root - 2017-12-11 00:25:37.347089: step 68360, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 57h:42m:52s remains)
INFO - root - 2017-12-11 00:25:45.291509: step 68370, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 58h:59m:43s remains)
INFO - root - 2017-12-11 00:25:53.097404: step 68380, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 57h:05m:15s remains)
INFO - root - 2017-12-11 00:26:00.837302: step 68390, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 57h:00m:55s remains)
INFO - root - 2017-12-11 00:26:08.728733: step 68400, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 59h:00m:54s remains)
2017-12-11 00:26:09.577057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053842563 -0.045431823 -0.038430639 -0.037907731 -0.042514361 -0.046406738 -0.04094214 -0.022880213 0.010259815 0.048561562 0.07931786 0.088644162 0.069661975 0.030611288 -0.01854226][-0.040978845 -0.021330906 -0.0022182085 0.0077225193 0.0066786748 -0.0012183046 -0.005973014 -0.0033707744 0.00854423 0.023863882 0.035443146 0.033736475 0.013788557 -0.01751267 -0.05286333][-0.0078313183 0.030017335 0.071042046 0.099666961 0.10756303 0.097388536 0.080997854 0.065010428 0.051756494 0.040443614 0.028119711 0.0085106511 -0.020967724 -0.052734084 -0.079529472][0.043734953 0.10601312 0.18114094 0.24247539 0.2710683 0.26858452 0.24681288 0.21431522 0.17534105 0.13485648 0.093681291 0.046697125 -0.00676458 -0.055265013 -0.08807303][0.11689287 0.19947034 0.31184226 0.41613406 0.48055103 0.50543565 0.49601224 0.45658422 0.39316249 0.3180871 0.23835017 0.1502905 0.056653813 -0.024248552 -0.076711446][0.20490769 0.29019973 0.42594671 0.56869572 0.67550242 0.74350679 0.76433605 0.73045903 0.6487003 0.539234 0.41768482 0.28287902 0.14281066 0.024103273 -0.0532988][0.28525981 0.35158077 0.48547009 0.64748144 0.78803289 0.89887434 0.95558953 0.93613386 0.84726644 0.715918 0.56468731 0.393709 0.21694143 0.068330251 -0.02931305][0.32597506 0.35958958 0.46970922 0.62800139 0.78280735 0.91982532 1.0032156 1.0017929 0.92133605 0.79129571 0.6352123 0.45091733 0.25813681 0.095805958 -0.011926102][0.31034568 0.30818292 0.37965956 0.51027989 0.65176558 0.78666735 0.87874436 0.89666384 0.84292406 0.74223793 0.61066407 0.44094735 0.25681379 0.099973761 -0.0055245366][0.24367051 0.21524979 0.24663854 0.3351073 0.43964869 0.54388869 0.62263972 0.65327638 0.63433111 0.5796203 0.49291822 0.3622717 0.21112466 0.079407319 -0.011216767][0.15559678 0.11998135 0.12507051 0.1733388 0.23243605 0.29005975 0.33826381 0.36839023 0.376681 0.3655647 0.32869065 0.25181487 0.15079163 0.0561366 -0.014541893][0.0821057 0.056562077 0.053288162 0.074172236 0.096050009 0.11117759 0.12690762 0.14723441 0.16845247 0.18533546 0.18731673 0.16100298 0.10993606 0.050504111 -0.00392729][0.049653474 0.041113913 0.04152121 0.049738906 0.051082451 0.041016243 0.035369579 0.045300368 0.068443969 0.096294269 0.11856936 0.12517875 0.10824895 0.071151994 0.023761695][0.067153566 0.071151353 0.075921863 0.079337962 0.072625078 0.052820947 0.038139459 0.041712154 0.061890475 0.089688122 0.1164731 0.13605307 0.13353838 0.10462517 0.056964111][0.10996044 0.11769516 0.12223373 0.12369691 0.11631959 0.097189248 0.082384482 0.083333947 0.097696647 0.11794566 0.13817023 0.15578175 0.15387709 0.125238 0.076485552]]...]
INFO - root - 2017-12-11 00:26:17.510050: step 68410, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 57h:40m:11s remains)
INFO - root - 2017-12-11 00:26:25.428332: step 68420, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 59h:44m:07s remains)
INFO - root - 2017-12-11 00:26:33.305993: step 68430, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 59h:40m:27s remains)
INFO - root - 2017-12-11 00:26:41.183742: step 68440, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 56h:53m:56s remains)
INFO - root - 2017-12-11 00:26:49.050667: step 68450, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 56h:30m:49s remains)
INFO - root - 2017-12-11 00:26:56.812573: step 68460, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 58h:14m:12s remains)
INFO - root - 2017-12-11 00:27:04.738138: step 68470, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 56h:31m:18s remains)
INFO - root - 2017-12-11 00:27:12.377090: step 68480, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 56h:12m:35s remains)
INFO - root - 2017-12-11 00:27:20.285119: step 68490, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 58h:24m:34s remains)
INFO - root - 2017-12-11 00:27:28.286195: step 68500, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 56h:48m:12s remains)
2017-12-11 00:27:29.241225: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.065976836 0.062207427 0.052070688 0.044791032 0.047195829 0.058438417 0.072648138 0.084212974 0.08790227 0.077294469 0.053483333 0.025657453 0.0047556767 -0.0059712897 -0.011108168][0.12429243 0.11652903 0.10122678 0.0938642 0.10443109 0.13055629 0.16045423 0.18244684 0.18710589 0.1661832 0.12389776 0.075639516 0.038421802 0.017226275 0.0051819803][0.18193717 0.1694589 0.1498509 0.14372703 0.16326961 0.20514199 0.25187123 0.28507808 0.29041621 0.25804907 0.1956943 0.12539686 0.070794024 0.038750816 0.019956453][0.22377992 0.20777333 0.18735185 0.18519744 0.21316504 0.26732433 0.32628822 0.36617044 0.36912358 0.32552826 0.24686517 0.16000254 0.092750795 0.053067669 0.029901911][0.24815099 0.23502576 0.22008735 0.22543049 0.26067346 0.32163772 0.38473019 0.42301479 0.41755942 0.36160666 0.27094477 0.17429762 0.10012482 0.056609362 0.032202415][0.25693133 0.25458759 0.25115231 0.26596954 0.30659118 0.36928102 0.42958125 0.45989966 0.44241786 0.37441134 0.27536592 0.17396058 0.097538389 0.054764606 0.03443497][0.25119436 0.26367182 0.27490526 0.30024856 0.34491339 0.40517023 0.45726252 0.47616875 0.44755435 0.37183669 0.26973182 0.1686649 0.094730631 0.057723515 0.046587862][0.24076377 0.26637071 0.29027441 0.32452431 0.37164989 0.42636445 0.46654242 0.47226807 0.4347291 0.35727817 0.25860384 0.16341476 0.096422076 0.069253676 0.0703842][0.24193388 0.27367005 0.30224141 0.33993062 0.38679865 0.43436679 0.46233141 0.45596176 0.41176552 0.33619696 0.24471302 0.15876102 0.10106927 0.084208369 0.0959361][0.26465803 0.29097623 0.31227735 0.34522885 0.38885787 0.43068069 0.45123506 0.43819737 0.39048892 0.31728753 0.23253661 0.15605654 0.10829107 0.10032589 0.11876337][0.30189481 0.30974144 0.31301454 0.33421534 0.37192795 0.41020027 0.42988753 0.41818014 0.37267458 0.30422682 0.22757182 0.16288377 0.12815763 0.1307466 0.15565182][0.34203035 0.32299864 0.30168039 0.30770087 0.33818287 0.373728 0.3950384 0.38843989 0.34958827 0.29011416 0.2253757 0.17562217 0.1564813 0.17134516 0.20413658][0.38460761 0.34089863 0.29508832 0.28456864 0.30516732 0.335513 0.3564176 0.35403603 0.32325917 0.275686 0.2255245 0.19078298 0.18481562 0.20878656 0.24784791][0.43478629 0.38001403 0.31847858 0.29220396 0.2979688 0.31636089 0.33102983 0.32971537 0.30784214 0.27480751 0.24067594 0.21826881 0.21796232 0.24142076 0.2795127][0.48131895 0.42955178 0.36309102 0.32442233 0.31287697 0.31463286 0.31881228 0.31638339 0.30376643 0.28619316 0.26722208 0.25282782 0.25038168 0.26555762 0.29873183]]...]
INFO - root - 2017-12-11 00:27:37.132991: step 68510, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 59h:00m:06s remains)
INFO - root - 2017-12-11 00:27:45.165538: step 68520, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.807 sec/batch; 59h:10m:20s remains)
INFO - root - 2017-12-11 00:27:53.067518: step 68530, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 55h:56m:03s remains)
INFO - root - 2017-12-11 00:28:00.745457: step 68540, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 57h:42m:37s remains)
INFO - root - 2017-12-11 00:28:08.651240: step 68550, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.787 sec/batch; 57h:43m:42s remains)
INFO - root - 2017-12-11 00:28:16.329110: step 68560, loss = 0.69, batch loss = 0.63 (11.1 examples/sec; 0.719 sec/batch; 52h:43m:22s remains)
INFO - root - 2017-12-11 00:28:24.153589: step 68570, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 57h:07m:17s remains)
INFO - root - 2017-12-11 00:28:32.000230: step 68580, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 57h:20m:25s remains)
INFO - root - 2017-12-11 00:28:40.019459: step 68590, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 58h:47m:34s remains)
INFO - root - 2017-12-11 00:28:47.904959: step 68600, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 57h:02m:59s remains)
2017-12-11 00:28:48.781422: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.072547458 0.062543228 0.049985815 0.039411224 0.033811215 0.033223137 0.035556216 0.037010074 0.034256671 0.026595967 0.017237684 0.01150059 0.014158593 0.027330382 0.049721789][0.072332844 0.068998083 0.0617263 0.055134438 0.052846257 0.055960521 0.062917218 0.068747714 0.06803517 0.059394617 0.046699893 0.037356876 0.037899304 0.051736843 0.0780776][0.067885593 0.07347092 0.075496919 0.0776706 0.083798222 0.095379338 0.11041641 0.12206332 0.12314933 0.11232922 0.094928764 0.080669217 0.077192694 0.08847066 0.11381572][0.079051919 0.091223992 0.10159518 0.11318459 0.13030955 0.15398963 0.17970806 0.19703065 0.19733615 0.18066368 0.1551262 0.13312928 0.12286274 0.12792933 0.1472363][0.1174048 0.13059886 0.14344759 0.16080864 0.18835674 0.22548799 0.26243508 0.28321949 0.27849579 0.25131997 0.21395962 0.18140037 0.16174623 0.15738718 0.16633824][0.17155939 0.18020712 0.19074515 0.21217442 0.25092253 0.30220833 0.34834218 0.36767519 0.35251766 0.31113026 0.26053229 0.21695797 0.18758254 0.17284974 0.16924588][0.21544841 0.21714434 0.22566238 0.2542356 0.30695704 0.37138677 0.42121077 0.43217736 0.4021731 0.34638876 0.28531551 0.2340472 0.19843034 0.176693 0.16306898][0.22887535 0.22489808 0.23508494 0.27458611 0.34160829 0.41450119 0.46096018 0.45926413 0.41506869 0.34939092 0.28335682 0.22948451 0.19301112 0.1706904 0.15466185][0.20596537 0.19921058 0.21432447 0.26564991 0.34368744 0.41896188 0.45759553 0.44405824 0.39091164 0.3224338 0.2580528 0.20723717 0.17534728 0.15880027 0.14906916][0.15038508 0.14451349 0.16658592 0.22731584 0.31057325 0.38300845 0.4133085 0.3929207 0.33867788 0.27576888 0.22061954 0.1794263 0.156749 0.14972936 0.15220062][0.084252104 0.082320258 0.11037336 0.17402999 0.25487974 0.32115668 0.34674233 0.32843253 0.28365412 0.23553726 0.19566362 0.16663715 0.1516196 0.1508238 0.16422172][0.04354291 0.045493558 0.073997974 0.13058873 0.19979014 0.25636131 0.28106502 0.27417091 0.25016719 0.2256161 0.20365849 0.18297014 0.166883 0.16225605 0.17844564][0.045428295 0.048020516 0.069769673 0.11074832 0.16137424 0.2055376 0.23187247 0.24276042 0.24651802 0.24880627 0.24219795 0.2223517 0.19517823 0.17603269 0.18435962][0.086905114 0.086094864 0.096403472 0.11919019 0.15102381 0.18461391 0.21546073 0.2462364 0.2772297 0.30179211 0.30327436 0.27648237 0.23170426 0.19153789 0.18315372][0.14221676 0.13545367 0.13413624 0.14222401 0.16073479 0.18889715 0.22668847 0.27502036 0.32588947 0.36238784 0.36313608 0.32477516 0.26224449 0.20225112 0.17551818]]...]
INFO - root - 2017-12-11 00:28:56.617927: step 68610, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 57h:10m:33s remains)
INFO - root - 2017-12-11 00:29:04.189258: step 68620, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 56h:53m:13s remains)
INFO - root - 2017-12-11 00:29:12.039607: step 68630, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 58h:35m:14s remains)
INFO - root - 2017-12-11 00:29:19.869204: step 68640, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 56h:48m:37s remains)
INFO - root - 2017-12-11 00:29:27.578136: step 68650, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 57h:51m:25s remains)
INFO - root - 2017-12-11 00:29:35.497230: step 68660, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 58h:40m:21s remains)
INFO - root - 2017-12-11 00:29:43.336221: step 68670, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 58h:12m:58s remains)
INFO - root - 2017-12-11 00:29:51.262724: step 68680, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 58h:19m:11s remains)
INFO - root - 2017-12-11 00:29:59.100159: step 68690, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 57h:37m:17s remains)
INFO - root - 2017-12-11 00:30:06.887991: step 68700, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.822 sec/batch; 60h:12m:23s remains)
2017-12-11 00:30:07.786182: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.031230737 0.085843772 0.13932478 0.18185695 0.21139967 0.22038266 0.20694548 0.17769337 0.14429648 0.12128867 0.11832185 0.13925891 0.17525095 0.20772791 0.21862836][0.079405792 0.15936941 0.24294622 0.31426051 0.36618775 0.38535568 0.36555088 0.31372389 0.24772178 0.19333614 0.16807236 0.18005808 0.21951704 0.26107532 0.27982265][0.11274507 0.21487948 0.32827783 0.43114564 0.51040906 0.54648173 0.52616823 0.45433432 0.35312691 0.25928819 0.19997273 0.19018263 0.22253595 0.26759851 0.293972][0.12108512 0.23763146 0.3743102 0.50521296 0.61281025 0.67239761 0.66222489 0.58210444 0.45466566 0.32390347 0.22554435 0.1832356 0.19685763 0.23785621 0.26949468][0.11398874 0.23961745 0.39356682 0.54766285 0.68207926 0.76896334 0.77852356 0.70289469 0.5609268 0.40057984 0.26415372 0.18460445 0.17275102 0.20581608 0.24364634][0.096745364 0.22682288 0.39294934 0.56656325 0.72757065 0.84613073 0.88465464 0.82497245 0.6797384 0.49910656 0.33130145 0.21798876 0.18060483 0.20451207 0.24752954][0.073211 0.20402037 0.37788686 0.56671107 0.75082719 0.898382 0.965979 0.92649466 0.78684437 0.59909332 0.41587877 0.2845104 0.23361297 0.2550801 0.30363825][0.047196109 0.17341639 0.34721291 0.541635 0.73786074 0.90216482 0.98906 0.96670622 0.83927089 0.66011453 0.482915 0.35575348 0.30885047 0.33667621 0.39090523][0.02923343 0.14674628 0.31195819 0.49882296 0.68903208 0.84912676 0.93646759 0.9201979 0.80652893 0.6494692 0.49935731 0.39818171 0.37217531 0.41450223 0.47450414][0.024530657 0.13283405 0.28287882 0.44965377 0.6150077 0.74826431 0.81393158 0.78865194 0.68505865 0.557002 0.4482635 0.39001006 0.39957595 0.46455368 0.53337157][0.029938616 0.127543 0.25497988 0.38889116 0.51342571 0.60436332 0.63625497 0.59678692 0.50558895 0.41303974 0.35338527 0.34580281 0.39658406 0.48513359 0.56087029][0.047989473 0.1361469 0.23764102 0.33066809 0.40476313 0.44627231 0.44213179 0.39198703 0.31987679 0.268146 0.25893837 0.3005707 0.38920516 0.49682221 0.57678103][0.075229712 0.15868078 0.23857422 0.29363757 0.3197917 0.31608829 0.28250504 0.22853205 0.17903492 0.16555044 0.19806772 0.27684143 0.39097995 0.50804627 0.58736235][0.093508735 0.17459194 0.24042346 0.26975405 0.26286861 0.22891673 0.17901205 0.12873825 0.10045332 0.11567593 0.17531553 0.27471495 0.39841822 0.51239973 0.58241153][0.084072806 0.15759505 0.21187863 0.22686474 0.20450273 0.15861021 0.10693953 0.067202583 0.057102762 0.090312868 0.16217776 0.2658875 0.38338596 0.4820742 0.53456718]]...]
INFO - root - 2017-12-11 00:30:15.857594: step 68710, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 59h:33m:44s remains)
INFO - root - 2017-12-11 00:30:23.844648: step 68720, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 59h:09m:27s remains)
INFO - root - 2017-12-11 00:30:31.684578: step 68730, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 55h:44m:46s remains)
INFO - root - 2017-12-11 00:30:39.345385: step 68740, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 57h:06m:55s remains)
INFO - root - 2017-12-11 00:30:47.167146: step 68750, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 56h:09m:47s remains)
INFO - root - 2017-12-11 00:30:55.198141: step 68760, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 57h:31m:50s remains)
INFO - root - 2017-12-11 00:31:03.086321: step 68770, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 58h:50m:39s remains)
INFO - root - 2017-12-11 00:31:10.902121: step 68780, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 59h:43m:18s remains)
INFO - root - 2017-12-11 00:31:18.823096: step 68790, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 58h:05m:54s remains)
INFO - root - 2017-12-11 00:31:26.754721: step 68800, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 57h:23m:56s remains)
2017-12-11 00:31:27.598500: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18321154 0.17566374 0.17478482 0.21062844 0.27140212 0.32902947 0.35557404 0.33419153 0.2713148 0.18829438 0.13416491 0.12330119 0.14501253 0.17364137 0.19220158][0.20797059 0.20040305 0.18996651 0.20302802 0.23147245 0.25519982 0.25534692 0.22240895 0.16394351 0.096399926 0.05415079 0.04293726 0.052101973 0.0610871 0.060238663][0.23056681 0.21999229 0.19684559 0.18505958 0.18132813 0.1740918 0.15422063 0.12042048 0.079875968 0.041463945 0.019173218 0.01010443 0.0058247605 -0.0070783389 -0.030590395][0.25509405 0.24226634 0.21085092 0.18350746 0.16054539 0.13600868 0.10871428 0.085372657 0.072600663 0.069211118 0.06906087 0.062081438 0.043461915 0.0069103395 -0.042088907][0.28306404 0.27198759 0.24086092 0.21202664 0.1864648 0.16052192 0.13767993 0.13103245 0.14757502 0.1759275 0.19346659 0.18459038 0.14986043 0.089004859 0.01364798][0.32382223 0.31494185 0.28977415 0.26916003 0.2523393 0.23554604 0.22332262 0.23215069 0.27028495 0.31913051 0.34490475 0.32849538 0.27678153 0.19475369 0.097807631][0.37814322 0.37160909 0.35452205 0.34568036 0.34137556 0.33650675 0.33495972 0.35285929 0.39948571 0.45291916 0.47401869 0.44336343 0.37414718 0.27611762 0.16651383][0.43780202 0.43361428 0.42235285 0.42155847 0.42422333 0.42516279 0.42730793 0.44355783 0.4830721 0.52419639 0.52885264 0.48063105 0.39775151 0.29270422 0.18258838][0.47762918 0.47024894 0.45713323 0.4546459 0.45392752 0.45092392 0.44754556 0.45164946 0.47183096 0.49043825 0.47610757 0.4169566 0.33315516 0.23626673 0.13999581][0.46118629 0.4440107 0.42265296 0.41129535 0.40142974 0.39042664 0.37865067 0.36861506 0.36745429 0.36418027 0.33695087 0.2789951 0.2084794 0.1332459 0.061993044][0.37648726 0.34884852 0.31844172 0.29698092 0.27824247 0.26052028 0.24262737 0.22332783 0.20742875 0.19026349 0.15939783 0.11295491 0.0644645 0.017237473 -0.024418367][0.2428297 0.20990543 0.17551267 0.14872487 0.12608948 0.10610882 0.086408816 0.064687461 0.043607917 0.023144262 -0.0021296197 -0.030732797 -0.054495953 -0.074117526 -0.08816231][0.10906122 0.076252043 0.043920428 0.018512147 -0.0011902309 -0.01717796 -0.033125687 -0.050582983 -0.068076976 -0.082804829 -0.096292377 -0.1061562 -0.10884066 -0.10753439 -0.10198404][0.011949171 -0.017044082 -0.043553676 -0.062683746 -0.074233048 -0.0813109 -0.08882241 -0.097562812 -0.10646801 -0.11198584 -0.11431159 -0.11145554 -0.10209342 -0.090067074 -0.0751792][-0.032362998 -0.055074193 -0.07441856 -0.085704915 -0.087923512 -0.085408449 -0.083893664 -0.08392328 -0.0844656 -0.082520358 -0.078281909 -0.070516326 -0.057919882 -0.04411551 -0.028530881]]...]
INFO - root - 2017-12-11 00:31:35.497419: step 68810, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 57h:43m:25s remains)
INFO - root - 2017-12-11 00:31:43.436651: step 68820, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 58h:29m:23s remains)
INFO - root - 2017-12-11 00:31:51.184691: step 68830, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.768 sec/batch; 56h:13m:18s remains)
INFO - root - 2017-12-11 00:31:59.120184: step 68840, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 57h:10m:55s remains)
INFO - root - 2017-12-11 00:32:06.965227: step 68850, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 57h:12m:07s remains)
INFO - root - 2017-12-11 00:32:14.854636: step 68860, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 59h:42m:16s remains)
INFO - root - 2017-12-11 00:32:22.770702: step 68870, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 57h:31m:02s remains)
INFO - root - 2017-12-11 00:32:30.680574: step 68880, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 57h:14m:14s remains)
INFO - root - 2017-12-11 00:32:38.487516: step 68890, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 57h:36m:00s remains)
INFO - root - 2017-12-11 00:32:46.373204: step 68900, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 56h:20m:11s remains)
2017-12-11 00:32:47.154227: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18044432 0.20239243 0.21272828 0.22013108 0.24691704 0.30696577 0.39173341 0.469692 0.50129086 0.46606839 0.37422734 0.2678881 0.19100563 0.1632922 0.1732367][0.13164991 0.15130079 0.16979237 0.19533513 0.24779405 0.3359617 0.44476873 0.53750503 0.57209587 0.52836025 0.42028934 0.29575309 0.20498845 0.17318374 0.19025786][0.0668921 0.082435563 0.10709058 0.1483815 0.22269396 0.33346084 0.46047688 0.56414515 0.6018644 0.55411536 0.436342 0.29899007 0.19568305 0.15620872 0.17430164][0.0017105103 0.012245866 0.039485537 0.090431951 0.17843959 0.30413562 0.44487405 0.5600633 0.60658 0.56354976 0.44433066 0.29748657 0.17873387 0.12326633 0.13173124][-0.049691126 -0.046579011 -0.021144623 0.03214258 0.12370191 0.25397861 0.40176356 0.52794534 0.58918303 0.56111211 0.45111996 0.30134195 0.16745766 0.090155907 0.078957886][-0.069986939 -0.07571058 -0.056587975 -0.0080042388 0.077662051 0.20177762 0.34710228 0.47807938 0.55227566 0.541845 0.44717714 0.30168924 0.15837623 0.061511844 0.028101327][-0.052953564 -0.06849353 -0.059193049 -0.02010108 0.054351266 0.16665426 0.30359474 0.43378556 0.51654696 0.52128404 0.44226667 0.30483145 0.15763101 0.045988735 -0.0075913891][-0.004538422 -0.032312617 -0.036880609 -0.011565476 0.047834959 0.14463486 0.26889664 0.39354774 0.48013496 0.49649879 0.43214151 0.30577731 0.1610423 0.041258171 -0.027983446][0.052445542 0.011679108 -0.0089438027 0.00014639283 0.042346053 0.12128902 0.229552 0.3441174 0.42952532 0.45418033 0.40499476 0.29518706 0.16251887 0.0458371 -0.029949969][0.10625487 0.051868845 0.013836289 0.0052030068 0.028528504 0.0865789 0.17349231 0.2708081 0.34829989 0.37780488 0.34630215 0.26297709 0.15823482 0.062851265 -0.003894646][0.14651142 0.084416009 0.034391582 0.012930564 0.02188764 0.06220784 0.12786908 0.20429543 0.26811486 0.2971825 0.28139535 0.22793458 0.15990408 0.097752556 0.051199086][0.1762374 0.11475109 0.062651 0.037916802 0.042405576 0.075511 0.12961568 0.1915991 0.24329869 0.26819772 0.2604441 0.22796708 0.18932498 0.15586054 0.12768905][0.20387897 0.14837216 0.0996541 0.07772208 0.085098974 0.12022594 0.17288853 0.22889931 0.27205423 0.28969547 0.28072751 0.25631678 0.23426972 0.21999569 0.20519206][0.22118424 0.17487806 0.13443132 0.12096722 0.13682549 0.17944434 0.23612617 0.29069141 0.32698709 0.33461323 0.31712371 0.29008812 0.2729651 0.26837486 0.26193544][0.21976173 0.1821713 0.15404429 0.15494627 0.18470474 0.23818749 0.30041572 0.35345981 0.38155288 0.37682632 0.34725863 0.31214276 0.29230818 0.28955856 0.28627264]]...]
INFO - root - 2017-12-11 00:32:54.979258: step 68910, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 58h:43m:20s remains)
INFO - root - 2017-12-11 00:33:02.693584: step 68920, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.817 sec/batch; 59h:48m:01s remains)
INFO - root - 2017-12-11 00:33:10.539082: step 68930, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 59h:07m:00s remains)
INFO - root - 2017-12-11 00:33:18.104677: step 68940, loss = 0.67, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 57h:05m:16s remains)
INFO - root - 2017-12-11 00:33:26.000581: step 68950, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 57h:13m:30s remains)
INFO - root - 2017-12-11 00:33:33.895641: step 68960, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 56h:54m:18s remains)
INFO - root - 2017-12-11 00:33:41.663938: step 68970, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.788 sec/batch; 57h:41m:59s remains)
INFO - root - 2017-12-11 00:33:49.551996: step 68980, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.758 sec/batch; 55h:27m:16s remains)
INFO - root - 2017-12-11 00:33:57.403425: step 68990, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 55h:41m:47s remains)
INFO - root - 2017-12-11 00:34:05.222138: step 69000, loss = 0.67, batch loss = 0.62 (10.2 examples/sec; 0.781 sec/batch; 57h:12m:00s remains)
2017-12-11 00:34:06.067442: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1093426 0.15353337 0.1976524 0.22838786 0.24329384 0.24881217 0.25875321 0.27522951 0.28709677 0.2796717 0.24094929 0.17435175 0.099569894 0.036996875 0.0043429108][0.115047 0.17097232 0.23117891 0.27802125 0.30438855 0.31350252 0.31879985 0.32525289 0.32514626 0.30706549 0.25868067 0.18122298 0.095905855 0.026312158 -0.0098175583][0.11435375 0.17855784 0.25195432 0.31318852 0.35052693 0.36334616 0.3632654 0.35839885 0.34562531 0.31716469 0.260477 0.17594527 0.0847693 0.012447831 -0.024065973][0.11660267 0.1846627 0.26560134 0.3359372 0.38054341 0.39514977 0.38922024 0.37375534 0.34976166 0.31233674 0.24979737 0.16278309 0.072297245 0.0032572558 -0.02929513][0.1211431 0.18831946 0.26988381 0.34351847 0.392147 0.40822676 0.398066 0.37398902 0.34014374 0.29449379 0.22838378 0.14399946 0.061480358 0.0030111773 -0.019504922][0.12611994 0.18924503 0.26605365 0.33802521 0.38871938 0.40885597 0.40055606 0.37409821 0.33418885 0.28116462 0.2110116 0.12944131 0.056501664 0.011497903 0.0022459566][0.12509656 0.18124378 0.24870835 0.3148315 0.36755586 0.39799869 0.40287507 0.38603184 0.34822023 0.28973931 0.21255177 0.12904389 0.061404698 0.027377283 0.030344278][0.11038698 0.1580236 0.21431032 0.27258995 0.32711557 0.37075728 0.39484018 0.39498627 0.36551744 0.30497366 0.22153485 0.13555078 0.07230112 0.048069552 0.0626837][0.077017851 0.11313681 0.156503 0.20576864 0.26087666 0.31677595 0.360535 0.38023967 0.36417776 0.30906761 0.22762403 0.14567731 0.090255305 0.075732961 0.10017514][0.031125292 0.054503459 0.085434072 0.12623709 0.18076435 0.2453793 0.30477741 0.34193414 0.34139815 0.29846859 0.22724721 0.1547934 0.10783824 0.099598035 0.12883946][-0.00691919 0.0058186497 0.025198346 0.055215448 0.10291129 0.16721202 0.23378251 0.28342298 0.298123 0.27106681 0.21486245 0.1549603 0.11667679 0.11245264 0.1427743][-0.028090861 -0.023748506 -0.015600133 0.00066987425 0.034922667 0.089252241 0.15246046 0.20610988 0.23120478 0.21902139 0.1794423 0.13488281 0.10880911 0.11260753 0.14760455][-0.037169091 -0.038821291 -0.039355312 -0.03532755 -0.016237941 0.02205356 0.071997263 0.11854449 0.14475514 0.14249982 0.11904 0.092235208 0.08206892 0.098108284 0.14156586][-0.04199934 -0.046534855 -0.051075224 -0.053826086 -0.046160586 -0.02385335 0.0087610213 0.04151094 0.062126096 0.0650933 0.055561818 0.045604333 0.049556993 0.074325174 0.12087392][-0.044205692 -0.048585307 -0.052616268 -0.056645241 -0.054848827 -0.043908108 -0.025792535 -0.0062039704 0.0073475973 0.011664368 0.010036507 0.0096441079 0.019458834 0.044161759 0.085041612]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 00:34:13.871372: step 69010, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.812 sec/batch; 59h:27m:33s remains)
INFO - root - 2017-12-11 00:34:21.485827: step 69020, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 56h:20m:42s remains)
INFO - root - 2017-12-11 00:34:29.404464: step 69030, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 57h:51m:49s remains)
INFO - root - 2017-12-11 00:34:37.246838: step 69040, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 56h:03m:00s remains)
INFO - root - 2017-12-11 00:34:45.146028: step 69050, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 56h:52m:58s remains)
INFO - root - 2017-12-11 00:34:53.006881: step 69060, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 58h:22m:18s remains)
INFO - root - 2017-12-11 00:35:00.790920: step 69070, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 57h:34m:47s remains)
INFO - root - 2017-12-11 00:35:08.610233: step 69080, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 56h:42m:43s remains)
INFO - root - 2017-12-11 00:35:16.479761: step 69090, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.766 sec/batch; 56h:01m:55s remains)
INFO - root - 2017-12-11 00:35:24.016800: step 69100, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 56h:53m:40s remains)
2017-12-11 00:35:24.941956: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17835948 0.22276358 0.2584236 0.27889076 0.27596766 0.24825676 0.20747602 0.1679427 0.13719411 0.11298456 0.093817979 0.082304142 0.076421015 0.071643025 0.064925775][0.2058446 0.24108818 0.27081507 0.2913706 0.29437155 0.27533752 0.23959132 0.19856377 0.16070251 0.12760994 0.10306525 0.091821685 0.089886159 0.088362642 0.081482135][0.23633423 0.25864229 0.2793816 0.29842657 0.30767453 0.30006623 0.27312091 0.23345964 0.18996003 0.14941584 0.1193073 0.1065114 0.10660543 0.10721461 0.10009243][0.27384406 0.28400686 0.29353455 0.30732855 0.31806198 0.31714714 0.29635593 0.25869831 0.21329141 0.16950317 0.13595779 0.12029931 0.11950945 0.12123612 0.1153735][0.31226879 0.31112844 0.30565181 0.3073841 0.31119406 0.3086957 0.28939503 0.25463048 0.21278901 0.17241327 0.14063166 0.12395823 0.12225235 0.12551884 0.12311027][0.33191392 0.32256681 0.30473828 0.29538643 0.29107884 0.28501856 0.26729554 0.23862357 0.20444493 0.17037797 0.14145887 0.12332564 0.11911715 0.12226123 0.12295735][0.32678419 0.31577158 0.2944665 0.28193891 0.27514943 0.26967555 0.25766695 0.23883446 0.21409129 0.18547125 0.15621157 0.13241935 0.12113956 0.11991965 0.12071543][0.30256748 0.29384509 0.27878448 0.27264214 0.27091867 0.27076429 0.26670882 0.25719026 0.23988299 0.21458431 0.1832833 0.15251279 0.13170815 0.12373617 0.12263683][0.26531488 0.25984111 0.25461531 0.25957096 0.26761818 0.27582288 0.28010726 0.2775597 0.2642101 0.23985924 0.20705096 0.17167765 0.14332587 0.12921919 0.12612624][0.22729902 0.22329935 0.22432384 0.2379418 0.25627524 0.27543178 0.2911559 0.2978301 0.28935713 0.2659604 0.23150077 0.19152313 0.15551981 0.13425143 0.12784681][0.1944228 0.19006911 0.19147548 0.20821129 0.23468736 0.266928 0.29937765 0.32130677 0.32235956 0.30198964 0.26510462 0.21814893 0.17173472 0.1399634 0.12664667][0.18793769 0.18028413 0.17369542 0.18284681 0.20790143 0.2468397 0.29341525 0.33182681 0.34621918 0.33304647 0.29636195 0.24403602 0.18907714 0.14769134 0.12628165][0.20462339 0.19132075 0.17286862 0.16871634 0.18401062 0.21906111 0.26838893 0.31429332 0.33763626 0.332208 0.30084449 0.25169495 0.19827686 0.15546097 0.13042253][0.22669017 0.20783436 0.17980677 0.16293949 0.16374564 0.18475142 0.22388089 0.26623628 0.29322714 0.29695714 0.27871764 0.24403007 0.20315702 0.1672523 0.14340858][0.25068036 0.23139612 0.19937724 0.17312375 0.15829316 0.16057515 0.183518 0.21711808 0.24576652 0.26124153 0.26202273 0.24759714 0.22176844 0.19295429 0.16916648]]...]
INFO - root - 2017-12-11 00:35:32.822786: step 69110, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 56h:11m:30s remains)
INFO - root - 2017-12-11 00:35:40.723028: step 69120, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.770 sec/batch; 56h:18m:22s remains)
INFO - root - 2017-12-11 00:35:48.529277: step 69130, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 56h:36m:13s remains)
INFO - root - 2017-12-11 00:35:56.363452: step 69140, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 57h:31m:42s remains)
INFO - root - 2017-12-11 00:36:04.206351: step 69150, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 56h:23m:42s remains)
INFO - root - 2017-12-11 00:36:12.054848: step 69160, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 58h:58m:47s remains)
INFO - root - 2017-12-11 00:36:19.911823: step 69170, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 58h:59m:30s remains)
INFO - root - 2017-12-11 00:36:27.643461: step 69180, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.807 sec/batch; 59h:03m:22s remains)
INFO - root - 2017-12-11 00:36:35.387351: step 69190, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 56h:41m:25s remains)
INFO - root - 2017-12-11 00:36:43.255766: step 69200, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 56h:11m:02s remains)
2017-12-11 00:36:44.064926: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04088245 0.048964493 0.050230533 0.04559188 0.038075093 0.031409103 0.029946977 0.03197195 0.03465037 0.034572374 0.047813527 0.079386063 0.11320306 0.13470796 0.13512693][0.0572467 0.065283559 0.062424634 0.049296975 0.03191651 0.016244857 0.0074913315 0.0047781044 0.0053850547 0.0051379721 0.017316179 0.04652337 0.080995746 0.10646123 0.11338764][0.099287458 0.10494951 0.096112877 0.074069954 0.047958851 0.02567157 0.012193733 0.0058699581 0.0034180882 0.0012641296 0.0098332372 0.033055086 0.0630975 0.087026365 0.095921248][0.15659891 0.15814884 0.14328471 0.11511394 0.086028077 0.065505505 0.056837417 0.054531816 0.052155055 0.046588782 0.047825824 0.06029721 0.078732051 0.091945142 0.093937963][0.20689468 0.2046403 0.18619029 0.15699013 0.13296674 0.12541233 0.13417774 0.14662646 0.14981009 0.14071798 0.13150096 0.12850314 0.12781587 0.121062 0.10758618][0.22614412 0.22443655 0.2086096 0.18530561 0.17372413 0.18748656 0.22199214 0.25617567 0.26868817 0.25677979 0.23534773 0.21322156 0.18799427 0.1557121 0.12256771][0.20170616 0.2060148 0.19959536 0.18773845 0.19144146 0.22674561 0.28495437 0.33879238 0.36047605 0.34727925 0.31489852 0.27407911 0.22525389 0.16981223 0.11971191][0.14298479 0.15545699 0.16075002 0.16067515 0.17558652 0.2231089 0.29251802 0.35509664 0.38150308 0.36930156 0.33195913 0.27992377 0.21665613 0.14825362 0.089792594][0.084588028 0.1006758 0.11144636 0.11556543 0.13083817 0.17502797 0.23783812 0.29402533 0.31824097 0.30874452 0.27437806 0.22251475 0.15876795 0.092121981 0.037332248][0.067925818 0.079786263 0.084565878 0.080790877 0.083105609 0.10729352 0.14593214 0.18084517 0.19452769 0.18612207 0.15929292 0.11741033 0.066735223 0.016285902 -0.023022437][0.11682152 0.11918888 0.10799026 0.085660972 0.0645761 0.057038683 0.059995361 0.064119838 0.060893558 0.049844392 0.030515134 0.0030847418 -0.027124856 -0.05373913 -0.071744815][0.21338974 0.20629819 0.17607869 0.13219628 0.085734881 0.044973627 0.011670654 -0.014939004 -0.035975862 -0.052229639 -0.066773929 -0.081262656 -0.092434138 -0.097382359 -0.096603945][0.30610725 0.29355055 0.24933478 0.18998715 0.12583017 0.06152685 0.0026941262 -0.0450121 -0.078257479 -0.098812796 -0.11106998 -0.117194 -0.11610239 -0.10761727 -0.095807239][0.33804387 0.32502869 0.27739313 0.21518449 0.14770879 0.076861516 0.010128292 -0.043654341 -0.079630181 -0.10057931 -0.11114635 -0.11304137 -0.10664029 -0.093011573 -0.077785753][0.2831167 0.2726711 0.23206489 0.17982273 0.12365083 0.063702472 0.0070324023 -0.037779916 -0.066531695 -0.082421206 -0.089646295 -0.08916004 -0.08196573 -0.068699256 -0.054835811]]...]
INFO - root - 2017-12-11 00:36:51.898247: step 69210, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 58h:33m:43s remains)
INFO - root - 2017-12-11 00:36:59.696873: step 69220, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 55h:13m:23s remains)
INFO - root - 2017-12-11 00:37:07.601002: step 69230, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 58h:13m:00s remains)
INFO - root - 2017-12-11 00:37:15.468538: step 69240, loss = 0.67, batch loss = 0.62 (9.6 examples/sec; 0.830 sec/batch; 60h:41m:09s remains)
INFO - root - 2017-12-11 00:37:23.294737: step 69250, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.785 sec/batch; 57h:24m:59s remains)
INFO - root - 2017-12-11 00:37:31.060156: step 69260, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 57h:09m:35s remains)
INFO - root - 2017-12-11 00:37:38.772188: step 69270, loss = 0.70, batch loss = 0.64 (12.2 examples/sec; 0.656 sec/batch; 47h:57m:57s remains)
INFO - root - 2017-12-11 00:37:46.656957: step 69280, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.774 sec/batch; 56h:34m:20s remains)
INFO - root - 2017-12-11 00:37:54.476707: step 69290, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 56h:08m:32s remains)
INFO - root - 2017-12-11 00:38:02.349757: step 69300, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.808 sec/batch; 59h:05m:00s remains)
2017-12-11 00:38:03.192674: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28147203 0.27674797 0.26465774 0.2478727 0.22983949 0.21344842 0.20109767 0.19315664 0.19295332 0.19746262 0.20092405 0.20212668 0.2026232 0.19851166 0.18758409][0.2452952 0.26190567 0.27057144 0.26828751 0.25352973 0.22995955 0.20450006 0.17976587 0.1631612 0.15620527 0.15418454 0.1533418 0.15129186 0.14536244 0.13687961][0.20750526 0.24833944 0.27794826 0.28788644 0.27325228 0.23958765 0.19986705 0.16032094 0.13226207 0.11980376 0.11686975 0.11607461 0.11198252 0.10358207 0.0978897][0.19016746 0.25215667 0.29669875 0.31216848 0.29428127 0.25225 0.2039651 0.15811874 0.12719774 0.11523538 0.11395819 0.11341259 0.10675675 0.09489236 0.090731271][0.18431945 0.26152161 0.31463161 0.33263317 0.31561935 0.27640483 0.23361655 0.19480892 0.17075481 0.16461951 0.16658963 0.16494478 0.15265699 0.13316144 0.12447634][0.18020578 0.26512995 0.32281929 0.34608468 0.34116587 0.32006887 0.29611295 0.27221566 0.25733277 0.256151 0.25856262 0.25247344 0.23174705 0.20312209 0.18752922][0.18498395 0.26864031 0.32766291 0.35998186 0.37645131 0.38177371 0.37921503 0.36627963 0.35345405 0.35010138 0.3464818 0.33201581 0.3032963 0.27116165 0.25484443][0.20109202 0.27582979 0.33279753 0.37461561 0.41183585 0.43898273 0.44824234 0.43418646 0.41266495 0.39910668 0.38533112 0.36371896 0.33495691 0.31226078 0.3076407][0.22689575 0.2891995 0.34213811 0.389806 0.43715081 0.47025967 0.47398794 0.4445124 0.40449104 0.37452656 0.35004127 0.32772663 0.31179419 0.31315744 0.33208412][0.24819221 0.29600069 0.34168753 0.38790178 0.43144536 0.45460024 0.44096109 0.39055037 0.33060727 0.2846866 0.25313416 0.23720156 0.2415646 0.2705906 0.3130514][0.24383634 0.27662474 0.31199518 0.35042465 0.38096249 0.38707072 0.35603726 0.29157814 0.2207626 0.166807 0.13409254 0.12660508 0.14651567 0.19127117 0.24396686][0.20607395 0.22615187 0.25378728 0.2860682 0.30575123 0.30016518 0.26176918 0.19577613 0.12480465 0.070041947 0.038929343 0.035573628 0.0583469 0.10016208 0.14534605][0.14565334 0.15946996 0.18430744 0.21528319 0.23182496 0.22492342 0.19109882 0.13480088 0.072128236 0.022185806 -0.005537861 -0.0089788325 0.0068971752 0.033517722 0.061037749][0.091298178 0.10223906 0.12439179 0.15409932 0.17117496 0.1690225 0.14661616 0.1060395 0.056991238 0.016783576 -0.0042764074 -0.0060915225 0.0040041353 0.017500408 0.030270735][0.073175907 0.079936966 0.09485627 0.11877578 0.13512219 0.13854071 0.12830825 0.10295006 0.067165516 0.0366207 0.022150716 0.023731738 0.03321087 0.042240672 0.0491876]]...]
INFO - root - 2017-12-11 00:38:11.081201: step 69310, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.798 sec/batch; 58h:19m:23s remains)
INFO - root - 2017-12-11 00:38:19.061248: step 69320, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.813 sec/batch; 59h:24m:30s remains)
INFO - root - 2017-12-11 00:38:26.944671: step 69330, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 58h:06m:13s remains)
INFO - root - 2017-12-11 00:38:34.621894: step 69340, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 57h:31m:20s remains)
INFO - root - 2017-12-11 00:38:42.496821: step 69350, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 58h:29m:41s remains)
INFO - root - 2017-12-11 00:38:50.145298: step 69360, loss = 0.66, batch loss = 0.60 (10.4 examples/sec; 0.771 sec/batch; 56h:21m:08s remains)
INFO - root - 2017-12-11 00:38:57.956110: step 69370, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 57h:34m:47s remains)
INFO - root - 2017-12-11 00:39:05.760837: step 69380, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 56h:45m:58s remains)
INFO - root - 2017-12-11 00:39:13.551794: step 69390, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 57h:17m:13s remains)
INFO - root - 2017-12-11 00:39:21.438066: step 69400, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.773 sec/batch; 56h:30m:43s remains)
2017-12-11 00:39:22.355821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0023582841 0.010971165 0.039683919 0.07850197 0.1202116 0.15798169 0.18677868 0.20495562 0.21795356 0.23324713 0.24363634 0.23087223 0.19321878 0.1480287 0.11550646][-0.0076961904 0.0035944749 0.032470427 0.075305417 0.1269801 0.1791514 0.22460775 0.25910559 0.28304914 0.3027091 0.30976543 0.28553382 0.22889005 0.16202605 0.11149359][0.0065585407 0.013566994 0.037498094 0.078515 0.1351362 0.19778246 0.25762337 0.30825168 0.343215 0.36514747 0.36469653 0.32692251 0.25293213 0.16839597 0.10396501][0.033848435 0.036812272 0.055080958 0.093909092 0.15515463 0.22723542 0.29960072 0.36402622 0.40722552 0.42558113 0.40991151 0.35409555 0.26466092 0.1699893 0.10181928][0.072094873 0.072565615 0.0870407 0.12573875 0.19264545 0.27434352 0.35829395 0.43411887 0.48283279 0.493567 0.45848507 0.3839272 0.28490841 0.19017252 0.12959397][0.11693311 0.11645442 0.12993968 0.17157282 0.2443853 0.33380735 0.42607382 0.50822371 0.5572198 0.55725259 0.50349259 0.41536367 0.31566784 0.23007594 0.18341528][0.15250544 0.15156953 0.16538748 0.21013384 0.28583121 0.37800395 0.47343892 0.55596644 0.60033697 0.58982635 0.52441865 0.43371195 0.34435207 0.27687231 0.24758603][0.16331734 0.16306295 0.17823309 0.22548348 0.30200917 0.39500344 0.49269435 0.5748294 0.61495525 0.59820849 0.52953959 0.44431245 0.37005576 0.32269877 0.30923098][0.14462563 0.14504933 0.1620297 0.21065959 0.28604785 0.37785533 0.47617191 0.55747074 0.5952341 0.578616 0.51636928 0.44418845 0.38726479 0.35743779 0.35440457][0.10647751 0.10726379 0.12582739 0.17445458 0.24586602 0.33141512 0.42329329 0.49795157 0.53246111 0.521573 0.47602659 0.42552102 0.38785073 0.37058589 0.36966512][0.06072681 0.061116304 0.080216594 0.12633421 0.18987152 0.2621831 0.33736622 0.39555514 0.42115143 0.41573671 0.39137423 0.36721104 0.3486332 0.33770868 0.33087978][0.024335351 0.022868229 0.039103147 0.076923005 0.12580302 0.17737146 0.22786781 0.26362264 0.27796707 0.27858466 0.27719009 0.28114396 0.28164646 0.27484491 0.25972596][0.009356034 0.0060242829 0.017073357 0.043758027 0.076012745 0.10591432 0.13162328 0.14582215 0.1497086 0.1553022 0.1724342 0.19834313 0.21253677 0.20671162 0.18318696][0.013696795 0.010013041 0.015500968 0.031257991 0.049203277 0.06182985 0.068050623 0.06548591 0.060659707 0.0675334 0.093650468 0.13001218 0.15030956 0.14429145 0.11678714][0.029983934 0.02661836 0.026015021 0.031042878 0.037073854 0.037799783 0.032702103 0.022011679 0.013422458 0.0197352 0.045822054 0.081122138 0.10033505 0.0946853 0.070250817]]...]
INFO - root - 2017-12-11 00:39:30.135043: step 69410, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 58h:15m:23s remains)
INFO - root - 2017-12-11 00:39:37.842605: step 69420, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 57h:44m:52s remains)
INFO - root - 2017-12-11 00:39:45.564305: step 69430, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.773 sec/batch; 56h:29m:14s remains)
INFO - root - 2017-12-11 00:39:53.464390: step 69440, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.812 sec/batch; 59h:19m:55s remains)
INFO - root - 2017-12-11 00:40:01.229150: step 69450, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 57h:39m:27s remains)
INFO - root - 2017-12-11 00:40:09.054353: step 69460, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 55h:13m:04s remains)
INFO - root - 2017-12-11 00:40:16.845763: step 69470, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 57h:10m:51s remains)
INFO - root - 2017-12-11 00:40:24.661173: step 69480, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 57h:58m:51s remains)
INFO - root - 2017-12-11 00:40:32.591291: step 69490, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 57h:12m:21s remains)
INFO - root - 2017-12-11 00:40:40.181869: step 69500, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.796 sec/batch; 58h:11m:11s remains)
2017-12-11 00:40:41.085255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.061661471 -0.060284577 -0.055533014 -0.050047096 -0.04604597 -0.045672562 -0.04868342 -0.053466242 -0.059236176 -0.06437052 -0.068898305 -0.072150476 -0.0734986 -0.072964475 -0.070725963][-0.053405005 -0.044777941 -0.03136282 -0.017240925 -0.0060991892 -0.0024391338 -0.0066660484 -0.01684496 -0.030625321 -0.044891968 -0.058633663 -0.069964238 -0.077398583 -0.0805792 -0.080067419][-0.03652155 -0.017712509 0.00787513 0.035740294 0.061249088 0.075688422 0.075846039 0.061940134 0.037726946 0.0091877542 -0.020387115 -0.046063315 -0.064974241 -0.076184712 -0.080587208][-0.012202687 0.021675436 0.065936275 0.11482887 0.16286296 0.19590098 0.20588556 0.18901598 0.15057649 0.10127919 0.048359193 0.00074372295 -0.03633748 -0.060662337 -0.07422515][0.018554231 0.0744309 0.14575541 0.22337951 0.30067554 0.35751787 0.37982526 0.35924795 0.30257839 0.22715083 0.14512205 0.069452681 0.0088136755 -0.032799311 -0.059003465][0.051403377 0.13166703 0.2310055 0.33641011 0.44076398 0.51996225 0.55461484 0.5325464 0.46084127 0.36239779 0.25367889 0.1515605 0.068062678 0.0086933747 -0.031759378][0.075673096 0.17320901 0.29115623 0.41367728 0.535383 0.63165188 0.67979264 0.6645425 0.59165835 0.48558334 0.36306232 0.24344365 0.14206181 0.066406347 0.010711442][0.079667985 0.18195282 0.3048152 0.43084738 0.55796683 0.66424596 0.72670692 0.72796249 0.67101836 0.57703632 0.45861083 0.33472323 0.22324748 0.13406451 0.063341476][0.062383991 0.15771584 0.27376774 0.39300296 0.51515752 0.62199605 0.692235 0.70905447 0.67380226 0.60302746 0.50365686 0.39102888 0.28339106 0.19106953 0.11241286][0.030369401 0.10959673 0.20834485 0.31058446 0.41640666 0.51214296 0.5804677 0.60790128 0.59557557 0.55487907 0.48697209 0.40127042 0.31318977 0.23036852 0.15375067][-0.0063676988 0.050138522 0.12264737 0.1976359 0.27572718 0.34955221 0.40847671 0.44413128 0.45784009 0.45428079 0.42855084 0.38246065 0.32557037 0.26153904 0.19366096][-0.040870216 -0.0092099076 0.03385891 0.078115165 0.12502891 0.17336331 0.22002986 0.26158768 0.29959941 0.33278659 0.35096419 0.34824246 0.3261933 0.28489217 0.22894718][-0.067600846 -0.056991443 -0.038806297 -0.019757165 0.0018088532 0.028733365 0.063338459 0.10629511 0.15948457 0.21807185 0.26984042 0.30323398 0.31257966 0.29481569 0.25466874][-0.08069665 -0.082808316 -0.080022663 -0.076311618 -0.070675477 -0.058718536 -0.035591107 0.0026300089 0.058829155 0.12727241 0.19666152 0.25291958 0.28581977 0.29013038 0.26952395][-0.080050692 -0.086148947 -0.089020893 -0.091576561 -0.093692571 -0.091003455 -0.077777773 -0.047581002 0.0038510954 0.071797363 0.14663646 0.21365386 0.26038656 0.28156906 0.2793844]]...]
INFO - root - 2017-12-11 00:40:48.970502: step 69510, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 58h:52m:12s remains)
INFO - root - 2017-12-11 00:40:56.769475: step 69520, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 57h:49m:26s remains)
INFO - root - 2017-12-11 00:41:04.585028: step 69530, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.825 sec/batch; 60h:16m:22s remains)
INFO - root - 2017-12-11 00:41:12.399086: step 69540, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 58h:27m:08s remains)
INFO - root - 2017-12-11 00:41:20.218255: step 69550, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.791 sec/batch; 57h:46m:02s remains)
INFO - root - 2017-12-11 00:41:28.020332: step 69560, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 57h:03m:13s remains)
INFO - root - 2017-12-11 00:41:35.884689: step 69570, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 58h:46m:25s remains)
INFO - root - 2017-12-11 00:41:43.588349: step 69580, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 57h:08m:34s remains)
INFO - root - 2017-12-11 00:41:51.420569: step 69590, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 57h:01m:58s remains)
INFO - root - 2017-12-11 00:41:59.223357: step 69600, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 55h:43m:40s remains)
2017-12-11 00:42:00.066672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.032476138 -0.038917925 -0.033695426 -0.021143021 -0.010382834 -0.0063514463 -0.011487174 -0.025506001 -0.042260829 -0.05565967 -0.063077725 -0.065203033 -0.066069774 -0.069401927 -0.066480123][-0.0336923 -0.037247714 -0.026455635 -0.0049633966 0.017773636 0.036096271 0.043481056 0.036169831 0.019180631 -0.00017434526 -0.017153455 -0.030727623 -0.043078881 -0.05539754 -0.054973193][-0.038423229 -0.039231569 -0.022869734 0.0091462005 0.048926242 0.09023501 0.12036504 0.12902915 0.11836257 0.097307682 0.073437013 0.049388882 0.024877306 0.0017615872 -0.0031640437][-0.0437222 -0.041276652 -0.01855373 0.025920769 0.086661153 0.15707892 0.21689752 0.24816035 0.24958923 0.23181644 0.20571081 0.17515199 0.14168805 0.10934263 0.097503468][-0.038950954 -0.028755449 0.0051232455 0.06539122 0.14818074 0.24718666 0.33531705 0.38775215 0.40094942 0.3881177 0.36301595 0.33012816 0.29184216 0.25216147 0.23146226][-0.020789422 0.0030371018 0.054200824 0.13325669 0.23516861 0.35430795 0.46026748 0.52436453 0.54283351 0.5318197 0.50715065 0.47358412 0.43364573 0.38992727 0.36242145][0.0077091451 0.052208062 0.12586291 0.22353947 0.33629915 0.45931643 0.56490785 0.62570924 0.63971704 0.6250391 0.59821182 0.56499112 0.527527 0.48569134 0.45785153][0.037588961 0.10176387 0.1938383 0.30173168 0.41163978 0.52019423 0.60838264 0.6556465 0.66252178 0.64616042 0.6211893 0.59363115 0.56530964 0.53303343 0.51313347][0.057361972 0.1320926 0.23004815 0.33422408 0.42713541 0.50790256 0.56965226 0.60222548 0.60882759 0.60311747 0.59403604 0.58550471 0.57736856 0.563523 0.55887467][0.060229845 0.1310654 0.21888208 0.3056049 0.37283325 0.42205387 0.45783779 0.4810088 0.49722371 0.51620412 0.53891945 0.56368214 0.58625507 0.5977599 0.61279553][0.056847658 0.10994921 0.17359047 0.23312028 0.27232978 0.29351592 0.30835453 0.32697278 0.35777724 0.40760231 0.46951386 0.53327012 0.589982 0.62839967 0.663302][0.0621915 0.091038942 0.12374624 0.15203784 0.16472346 0.1642009 0.16404077 0.18026751 0.22418602 0.3005802 0.39665949 0.492986 0.57695585 0.63698363 0.68803203][0.062470675 0.067859821 0.072048441 0.072996907 0.065926611 0.053471163 0.046767533 0.06266918 0.11388279 0.2040291 0.31804726 0.4306187 0.52747345 0.59954947 0.66171914][0.044747423 0.034724269 0.020780427 0.0049210056 -0.012048737 -0.027056977 -0.033691406 -0.018244516 0.030829508 0.11679122 0.22626206 0.33386937 0.42686337 0.49956417 0.56554365][0.010387535 -0.005578415 -0.025875581 -0.047782078 -0.067112416 -0.080296822 -0.085340828 -0.073459685 -0.036244597 0.029154355 0.11377721 0.19733392 0.27187014 0.33554024 0.39867848]]...]
INFO - root - 2017-12-11 00:42:07.909937: step 69610, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.786 sec/batch; 57h:23m:23s remains)
INFO - root - 2017-12-11 00:42:15.797857: step 69620, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 57h:03m:36s remains)
INFO - root - 2017-12-11 00:42:23.441574: step 69630, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.752 sec/batch; 54h:55m:51s remains)
INFO - root - 2017-12-11 00:42:31.282521: step 69640, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 56h:34m:06s remains)
INFO - root - 2017-12-11 00:42:39.269259: step 69650, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.818 sec/batch; 59h:43m:11s remains)
INFO - root - 2017-12-11 00:42:47.016597: step 69660, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 56h:00m:07s remains)
INFO - root - 2017-12-11 00:42:54.808471: step 69670, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 55h:59m:01s remains)
INFO - root - 2017-12-11 00:43:02.665428: step 69680, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 57h:20m:47s remains)
INFO - root - 2017-12-11 00:43:10.555752: step 69690, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.814 sec/batch; 59h:25m:56s remains)
INFO - root - 2017-12-11 00:43:18.442275: step 69700, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 58h:06m:33s remains)
2017-12-11 00:43:19.400408: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.198254 0.20733571 0.19876182 0.17640473 0.14803186 0.12217805 0.10581585 0.10215419 0.10683611 0.10921078 0.10226223 0.086947545 0.074301533 0.072386779 0.081697151][0.20780262 0.22116286 0.21472695 0.19312321 0.16470642 0.13814424 0.12040123 0.11506999 0.11867885 0.11954942 0.11029572 0.091300815 0.074871257 0.070566945 0.079070844][0.21700837 0.23307432 0.22591557 0.20270625 0.17402263 0.14944562 0.13407426 0.12986365 0.13373652 0.13395226 0.12289217 0.10017083 0.078723758 0.069532707 0.074351437][0.22506535 0.24194621 0.23342024 0.20936388 0.18250784 0.16328086 0.15333638 0.15214702 0.15632951 0.15499961 0.14128187 0.11408801 0.086194247 0.070659332 0.070657469][0.22882554 0.24593741 0.23790748 0.21563029 0.19302815 0.18144025 0.17903101 0.18223253 0.1868497 0.18355075 0.16682352 0.13493434 0.099944934 0.076533869 0.0692143][0.22803544 0.24561906 0.2404509 0.22245647 0.20567557 0.20216265 0.20740558 0.2148978 0.21938628 0.21406025 0.19508509 0.16014151 0.11971566 0.088610306 0.072453909][0.22154741 0.23996176 0.23969844 0.22839919 0.21850704 0.2218222 0.23297362 0.24357325 0.2477314 0.24146412 0.22232145 0.18807086 0.1462456 0.11021433 0.086112261][0.20154303 0.21977396 0.22490513 0.22203992 0.22101319 0.23168519 0.24848662 0.26236898 0.26745135 0.26205355 0.2448092 0.21433027 0.17455249 0.13650097 0.10636635][0.17049281 0.18760937 0.19748765 0.20352237 0.21226038 0.23012988 0.25142142 0.26769695 0.27397773 0.27009302 0.25544107 0.22981654 0.19431493 0.15746012 0.12456537][0.13423543 0.14852369 0.16040196 0.17238878 0.18828362 0.21083607 0.23412992 0.25064933 0.25740516 0.25534284 0.24402595 0.22439551 0.1959234 0.16482651 0.13434805][0.10746972 0.11796961 0.1290473 0.14296275 0.16150266 0.18399797 0.20439811 0.21653138 0.22042844 0.21832439 0.20985994 0.19697642 0.17811331 0.15729976 0.13510452][0.10186326 0.10910238 0.11883262 0.13312359 0.1515709 0.17061786 0.18389814 0.18689021 0.18345059 0.17763725 0.16988252 0.16317329 0.1551196 0.14671922 0.13571081][0.11577242 0.11947963 0.12889689 0.14394835 0.16202553 0.1772317 0.18258108 0.1749794 0.16243257 0.15119325 0.14302018 0.14137992 0.14342088 0.14666678 0.14631484][0.14322193 0.14645258 0.1571414 0.1734353 0.19038814 0.2005607 0.19695534 0.17831105 0.15674138 0.14065751 0.13259824 0.13529819 0.1455085 0.15783781 0.16503985][0.16938157 0.17467533 0.1870279 0.20376383 0.2184383 0.22355998 0.21264924 0.18615234 0.15889055 0.14049365 0.13357382 0.13938761 0.15432493 0.17145517 0.18151049]]...]
INFO - root - 2017-12-11 00:43:27.286457: step 69710, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.788 sec/batch; 57h:33m:11s remains)
INFO - root - 2017-12-11 00:43:34.909558: step 69720, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 57h:21m:05s remains)
INFO - root - 2017-12-11 00:43:42.804977: step 69730, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 57h:12m:22s remains)
INFO - root - 2017-12-11 00:43:50.282113: step 69740, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.758 sec/batch; 55h:18m:12s remains)
INFO - root - 2017-12-11 00:43:58.039394: step 69750, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.753 sec/batch; 54h:56m:18s remains)
INFO - root - 2017-12-11 00:44:05.797054: step 69760, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 57h:26m:41s remains)
INFO - root - 2017-12-11 00:44:13.630614: step 69770, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 57h:04m:28s remains)
INFO - root - 2017-12-11 00:44:21.424784: step 69780, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 58h:13m:48s remains)
INFO - root - 2017-12-11 00:44:29.306051: step 69790, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 55h:35m:44s remains)
INFO - root - 2017-12-11 00:44:37.099057: step 69800, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 55h:48m:45s remains)
2017-12-11 00:44:37.966436: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.096884876 0.11140647 0.11493181 0.11893199 0.12884085 0.14338209 0.15446073 0.16750105 0.190189 0.21730009 0.24513981 0.27008432 0.28088683 0.24937861 0.1668347][0.11709932 0.13262409 0.14058393 0.15728091 0.18752462 0.22686481 0.26157746 0.2957924 0.33803535 0.37773457 0.41081759 0.43836868 0.44793823 0.40051734 0.28112897][0.12234147 0.13889384 0.15402563 0.18730707 0.24118827 0.30707768 0.3638373 0.41395077 0.46721679 0.51006919 0.53987271 0.56307995 0.56837672 0.50923383 0.36595583][0.12112266 0.1414728 0.16928059 0.22483937 0.30489185 0.39456326 0.46395674 0.5142085 0.55905914 0.58840013 0.60300469 0.61153448 0.60456884 0.53578675 0.3855381][0.11761823 0.14636672 0.19194061 0.27232161 0.37641352 0.48157609 0.55111641 0.58645004 0.60941643 0.61927044 0.6182493 0.61138421 0.58892405 0.51298755 0.36721486][0.11689907 0.16111088 0.23106825 0.33827773 0.46354592 0.57695144 0.63820404 0.6505093 0.6473462 0.64267546 0.63724887 0.62610894 0.5959273 0.51982915 0.38381463][0.12214121 0.18522178 0.28090796 0.41116169 0.55115014 0.66742182 0.71829081 0.70849431 0.68131036 0.66672641 0.66347396 0.65596384 0.62801969 0.56149328 0.44183266][0.1359418 0.21549082 0.3303808 0.47311941 0.61573744 0.7273618 0.77016276 0.74773288 0.7042653 0.68037671 0.6755845 0.66787082 0.64122462 0.58382994 0.48061845][0.15673885 0.25033349 0.37638932 0.519351 0.65148246 0.74970084 0.786683 0.7643609 0.71977592 0.6952129 0.68935955 0.678224 0.64654911 0.58771831 0.48967418][0.16752848 0.26819912 0.3935408 0.52211833 0.63157839 0.70924586 0.740606 0.72593409 0.69392139 0.68146694 0.68351609 0.6746797 0.63772047 0.56985271 0.46629709][0.15026858 0.2446738 0.35659507 0.46205869 0.54641527 0.605079 0.63210136 0.62581307 0.60783023 0.60771817 0.61933815 0.616434 0.57914752 0.50492704 0.3974632][0.1061354 0.18138911 0.2692419 0.34801844 0.40945977 0.4522638 0.47399616 0.47178525 0.46038729 0.46064875 0.46929654 0.46452406 0.42869997 0.35815388 0.26113355][0.051520769 0.10138454 0.15939516 0.21014732 0.24898061 0.27451015 0.28634644 0.28255779 0.27154374 0.26447091 0.26096743 0.24698234 0.21234578 0.15367042 0.080425121][0.00046691133 0.027339296 0.05702332 0.080969073 0.097783305 0.10524035 0.10455274 0.096733525 0.086344413 0.076970048 0.067435227 0.049811106 0.021181252 -0.019774228 -0.065010205][-0.045915965 -0.037534941 -0.029563073 -0.026034044 -0.02610747 -0.031851538 -0.041578639 -0.051999442 -0.05971314 -0.064961851 -0.071244039 -0.083586417 -0.10148262 -0.12401202 -0.1456555]]...]
INFO - root - 2017-12-11 00:44:45.649547: step 69810, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 57h:55m:09s remains)
INFO - root - 2017-12-11 00:44:53.239236: step 69820, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 56h:40m:22s remains)
INFO - root - 2017-12-11 00:45:01.019916: step 69830, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.757 sec/batch; 55h:13m:50s remains)
INFO - root - 2017-12-11 00:45:08.897682: step 69840, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 57h:34m:36s remains)
INFO - root - 2017-12-11 00:45:16.818432: step 69850, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 58h:49m:31s remains)
INFO - root - 2017-12-11 00:45:24.644993: step 69860, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 57h:05m:52s remains)
INFO - root - 2017-12-11 00:45:32.549993: step 69870, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 58h:01m:50s remains)
INFO - root - 2017-12-11 00:45:40.400126: step 69880, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 57h:24m:07s remains)
INFO - root - 2017-12-11 00:45:48.440062: step 69890, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 55h:24m:12s remains)
INFO - root - 2017-12-11 00:45:55.969412: step 69900, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 57h:30m:54s remains)
2017-12-11 00:45:56.837729: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22063723 0.27944043 0.32263172 0.33744374 0.31537813 0.26291957 0.20016612 0.14704354 0.11048539 0.091372266 0.10110158 0.15586463 0.2552391 0.36998639 0.46946383][0.091285087 0.12444186 0.15319286 0.16416837 0.1469354 0.10579392 0.058048122 0.021512773 0.0026147005 0.00062436395 0.025885461 0.092791453 0.20017573 0.32161358 0.43083408][0.031379793 0.038979936 0.053800162 0.063914321 0.056873117 0.033522602 0.004302681 -0.017760415 -0.026691843 -0.023802618 -0.0011079636 0.053636394 0.14181186 0.24448213 0.34424391][0.065145016 0.058761586 0.068831578 0.085626505 0.095255286 0.094850652 0.085710347 0.072780654 0.060867082 0.050046928 0.046860393 0.062089134 0.10294086 0.16222617 0.23354726][0.16537842 0.16203342 0.17893842 0.21050577 0.24299353 0.27076715 0.28499341 0.27971083 0.25536513 0.21506652 0.1665154 0.12207263 0.097218551 0.098012015 0.12950562][0.27291316 0.28548938 0.31942955 0.37181112 0.4311201 0.49089664 0.53215337 0.5368306 0.49788129 0.42262292 0.32232174 0.21453288 0.1240857 0.068282 0.060694925][0.32408804 0.355421 0.40937018 0.48398483 0.57000518 0.66126543 0.73058885 0.74861819 0.70043033 0.597666 0.45665112 0.301636 0.16542706 0.071310438 0.036020052][0.29512575 0.33724764 0.40590468 0.49724296 0.60235429 0.71516776 0.80467474 0.83506316 0.78611696 0.67235804 0.51472372 0.34241834 0.19171397 0.085877843 0.039924167][0.19611835 0.2360331 0.30711523 0.40230832 0.51150954 0.62795484 0.72158241 0.75750428 0.71465147 0.60819429 0.46064806 0.30234766 0.1681561 0.076757155 0.038653933][0.064838111 0.093219273 0.15436289 0.23852691 0.334548 0.43529239 0.51556963 0.54733133 0.51309872 0.42696711 0.30943164 0.18734175 0.090153627 0.029796716 0.011412591][-0.053192258 -0.040195975 0.0020481569 0.063270696 0.13283072 0.20444472 0.25992531 0.28024009 0.25369412 0.19234663 0.1128066 0.035379674 -0.018751169 -0.04453744 -0.041559011][-0.12997736 -0.13005482 -0.10896812 -0.07541611 -0.037281368 0.0014149378 0.029901383 0.037354767 0.01717725 -0.021108164 -0.065299325 -0.1022869 -0.1197321 -0.11799703 -0.099864095][-0.16261192 -0.16958778 -0.16424304 -0.15368657 -0.14178972 -0.12948394 -0.12144931 -0.12264889 -0.13649091 -0.15668699 -0.17545192 -0.18547763 -0.18095402 -0.16493531 -0.1413053][-0.16235365 -0.16997693 -0.17212018 -0.17457494 -0.17808878 -0.18121281 -0.1844877 -0.18935889 -0.19760494 -0.20605995 -0.21068306 -0.20803779 -0.19602598 -0.17788517 -0.15685542][-0.14251155 -0.14811195 -0.15169823 -0.15805446 -0.16648372 -0.1748763 -0.18190156 -0.18715669 -0.19137332 -0.19346491 -0.19198774 -0.18585555 -0.17485902 -0.16131645 -0.14719789]]...]
INFO - root - 2017-12-11 00:46:04.695433: step 69910, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 57h:38m:24s remains)
INFO - root - 2017-12-11 00:46:12.563555: step 69920, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 57h:30m:46s remains)
INFO - root - 2017-12-11 00:46:20.403604: step 69930, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 56h:57m:52s remains)
INFO - root - 2017-12-11 00:46:28.313556: step 69940, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 59h:24m:09s remains)
INFO - root - 2017-12-11 00:46:36.257575: step 69950, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 57h:21m:45s remains)
INFO - root - 2017-12-11 00:46:44.061797: step 69960, loss = 0.71, batch loss = 0.66 (10.7 examples/sec; 0.748 sec/batch; 54h:34m:49s remains)
INFO - root - 2017-12-11 00:46:51.849402: step 69970, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 55h:42m:18s remains)
INFO - root - 2017-12-11 00:46:59.430542: step 69980, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.752 sec/batch; 54h:50m:55s remains)
INFO - root - 2017-12-11 00:47:07.298873: step 69990, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 57h:57m:12s remains)
INFO - root - 2017-12-11 00:47:15.125345: step 70000, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 57h:09m:07s remains)
2017-12-11 00:47:15.989806: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.41685677 0.40980533 0.35914996 0.295899 0.24153604 0.20215815 0.18651117 0.21046636 0.26047313 0.30016214 0.3151758 0.290884 0.24629934 0.19620608 0.14247996][0.45622018 0.46149245 0.41298154 0.34348854 0.28080374 0.23274559 0.20814407 0.22192979 0.25778449 0.28223792 0.28610185 0.2614778 0.22620344 0.19023034 0.15059929][0.44801944 0.46460325 0.42559749 0.3623966 0.3073332 0.26808369 0.24919859 0.25899655 0.27844417 0.28174978 0.268216 0.24027175 0.21206111 0.18890311 0.1633267][0.40409657 0.43089214 0.40752706 0.36187568 0.32825643 0.31395003 0.31643328 0.33418411 0.34365004 0.32829994 0.29620692 0.26106489 0.23276368 0.21378423 0.19366249][0.34669417 0.38268 0.37772146 0.35516438 0.34894845 0.36684859 0.39956942 0.4355914 0.44549486 0.42023224 0.37363017 0.3287634 0.29190436 0.2645306 0.2369433][0.31052694 0.35391816 0.36568204 0.36446682 0.38184816 0.42710754 0.48772165 0.54509848 0.56298482 0.53584683 0.47939125 0.42267179 0.36988381 0.32321534 0.27743435][0.31292167 0.36097977 0.38318038 0.39430991 0.42263427 0.480353 0.5567283 0.6307236 0.65946209 0.63631976 0.57510829 0.50775915 0.43787685 0.36886892 0.30168891][0.33719349 0.38534868 0.41075367 0.42526674 0.45184702 0.506017 0.58305311 0.66399819 0.70134354 0.68410152 0.62289613 0.5504325 0.47059053 0.38640922 0.30391255][0.35131642 0.39269361 0.41560328 0.42961234 0.45001161 0.49273777 0.55911767 0.63543856 0.67466688 0.66070634 0.602023 0.53108329 0.45325094 0.36873439 0.28434369][0.33321306 0.35964265 0.37532777 0.38912502 0.407419 0.44176447 0.49520057 0.55889589 0.59188038 0.57591712 0.51944691 0.45469296 0.38909653 0.31884518 0.24692914][0.28002957 0.28749827 0.29428932 0.30900276 0.32992151 0.36151639 0.40385503 0.45038632 0.47031021 0.44872308 0.39610758 0.34309646 0.2975148 0.25137708 0.20111018][0.20640726 0.19644339 0.19423017 0.2076641 0.22959663 0.2582843 0.28953946 0.31769934 0.32369781 0.29957935 0.25790077 0.22496642 0.20565869 0.18750906 0.16070227][0.13425191 0.11015818 0.098654971 0.10646962 0.12462045 0.14760692 0.16822004 0.18178074 0.17896564 0.1582894 0.13336176 0.12472859 0.13288325 0.14104795 0.13485193][0.082090124 0.050288446 0.031401306 0.031020576 0.041836359 0.057314176 0.068690874 0.072803468 0.066874929 0.053164482 0.044777989 0.056911927 0.0869256 0.11498062 0.12390114][0.06155711 0.030114507 0.0084147388 0.000646349 0.0033151109 0.011297379 0.015356291 0.013327544 0.00600553 -0.0023881884 0.0007419539 0.026161861 0.069219038 0.10827057 0.12506667]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 00:47:24.816054: step 70010, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 58h:02m:34s remains)
INFO - root - 2017-12-11 00:47:32.699127: step 70020, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.763 sec/batch; 55h:37m:56s remains)
INFO - root - 2017-12-11 00:47:40.570807: step 70030, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 57h:11m:00s remains)
INFO - root - 2017-12-11 00:47:48.422914: step 70040, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 58h:34m:31s remains)
INFO - root - 2017-12-11 00:47:56.324065: step 70050, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 58h:00m:45s remains)
INFO - root - 2017-12-11 00:48:04.047926: step 70060, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 56h:55m:03s remains)
INFO - root - 2017-12-11 00:48:11.699363: step 70070, loss = 0.71, batch loss = 0.65 (9.4 examples/sec; 0.848 sec/batch; 61h:47m:32s remains)
INFO - root - 2017-12-11 00:48:19.665570: step 70080, loss = 0.69, batch loss = 0.63 (9.0 examples/sec; 0.889 sec/batch; 64h:48m:42s remains)
INFO - root - 2017-12-11 00:48:27.548209: step 70090, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 56h:35m:03s remains)
INFO - root - 2017-12-11 00:48:35.489517: step 70100, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 57h:29m:38s remains)
2017-12-11 00:48:36.394053: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.080166861 0.1251356 0.16911662 0.19815764 0.20925219 0.21466251 0.22597229 0.25023764 0.28220081 0.319053 0.35365602 0.37008819 0.36849385 0.34403542 0.30738509][0.0892094 0.13962223 0.18845403 0.22205554 0.23723221 0.24346404 0.25127834 0.27129316 0.30079028 0.33529416 0.36858615 0.38857758 0.39381996 0.37550405 0.34228262][0.092557892 0.14788316 0.20205723 0.2416887 0.26194721 0.26828095 0.26868349 0.27679098 0.29444557 0.31792361 0.3428086 0.36228544 0.37530795 0.37110347 0.352396][0.093191952 0.152619 0.21224324 0.258544 0.28600544 0.29625952 0.29260531 0.28813058 0.28890547 0.29447517 0.30311239 0.31292617 0.32582489 0.33283734 0.33073586][0.097150728 0.16134168 0.22673759 0.27955398 0.31622732 0.33649433 0.33791938 0.32718512 0.31221345 0.29847702 0.28631487 0.2763184 0.27416945 0.27828711 0.28441954][0.10819346 0.17755026 0.24780878 0.30610719 0.35458541 0.39265034 0.41124824 0.40732571 0.38703424 0.36192378 0.33161643 0.29607162 0.26446688 0.24771351 0.24835934][0.1194637 0.19153532 0.26308385 0.32317996 0.38243684 0.43968672 0.47907752 0.48873451 0.47352 0.45010474 0.412174 0.35605678 0.29415247 0.25133058 0.2404623][0.12002917 0.18939845 0.25690886 0.31405172 0.3786324 0.44748133 0.49991122 0.52030009 0.51450706 0.50146276 0.46676698 0.40227595 0.32246754 0.26192346 0.24220002][0.1031617 0.16282398 0.22000535 0.26880738 0.3297056 0.39792517 0.45147315 0.47557762 0.47805545 0.47630605 0.45052898 0.38956228 0.30799255 0.24334863 0.22082362][0.068737127 0.11236111 0.15364406 0.18910733 0.23726997 0.29293007 0.33627975 0.35627809 0.36153981 0.36553195 0.3486194 0.29991728 0.23145512 0.17616324 0.15730411][0.021404481 0.045375809 0.06831447 0.088559896 0.12026212 0.1587404 0.18830751 0.20205405 0.20687884 0.2113072 0.20062287 0.16720469 0.11899054 0.079389133 0.066318937][-0.025841322 -0.020411287 -0.014585118 -0.0092407819 0.0053903125 0.025898829 0.041526869 0.04863837 0.051724263 0.055060379 0.049778156 0.031694468 0.0048991768 -0.017269021 -0.023447094][-0.056805898 -0.063295461 -0.068627693 -0.073845409 -0.072885469 -0.067593075 -0.063991249 -0.063250989 -0.062549524 -0.060230281 -0.060696416 -0.066141352 -0.075394705 -0.082630739 -0.0819037][-0.067443915 -0.077839077 -0.086398214 -0.0947728 -0.099878892 -0.10255072 -0.10541052 -0.10816307 -0.10891897 -0.10744736 -0.10565497 -0.10457607 -0.10442839 -0.10344927 -0.099081673][-0.066323079 -0.07593324 -0.082944967 -0.089721926 -0.09509223 -0.09955769 -0.10393464 -0.10729545 -0.10833596 -0.10742036 -0.10544652 -0.10273035 -0.099758275 -0.096394539 -0.091855526]]...]
INFO - root - 2017-12-11 00:48:44.352932: step 70110, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 56h:32m:20s remains)
INFO - root - 2017-12-11 00:48:52.200759: step 70120, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.808 sec/batch; 58h:52m:16s remains)
INFO - root - 2017-12-11 00:49:00.156605: step 70130, loss = 0.70, batch loss = 0.64 (8.7 examples/sec; 0.915 sec/batch; 66h:42m:07s remains)
INFO - root - 2017-12-11 00:49:07.689148: step 70140, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 58h:07m:19s remains)
INFO - root - 2017-12-11 00:49:15.620532: step 70150, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 55h:40m:41s remains)
INFO - root - 2017-12-11 00:49:23.303784: step 70160, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 56h:07m:29s remains)
INFO - root - 2017-12-11 00:49:31.142603: step 70170, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 56h:31m:07s remains)
INFO - root - 2017-12-11 00:49:38.950293: step 70180, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 55h:52m:21s remains)
INFO - root - 2017-12-11 00:49:46.853190: step 70190, loss = 0.71, batch loss = 0.65 (9.0 examples/sec; 0.890 sec/batch; 64h:49m:30s remains)
INFO - root - 2017-12-11 00:49:54.699927: step 70200, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 56h:41m:45s remains)
2017-12-11 00:49:55.581936: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049035173 0.0700364 0.096461721 0.12475687 0.15480587 0.18539475 0.21311244 0.23141727 0.24625444 0.26770943 0.28921241 0.3030743 0.31597641 0.34310767 0.38601533][-0.0029479524 0.010924714 0.028060678 0.046413973 0.066976033 0.08822345 0.108543 0.12326807 0.13722898 0.15935376 0.18650684 0.21321937 0.24626976 0.29737854 0.36418483][-0.011208894 0.0033352356 0.019084435 0.032837447 0.046127569 0.059198681 0.0726189 0.082133137 0.091214292 0.10631949 0.12706342 0.15332158 0.19473793 0.25979507 0.34156856][0.013531968 0.0348763 0.058479968 0.078237154 0.095795676 0.11302156 0.13151161 0.14469536 0.15443768 0.16312478 0.17063652 0.18163019 0.20771419 0.25578845 0.3202557][0.049309023 0.080090486 0.11779096 0.15304914 0.18701154 0.22194609 0.25942481 0.28840712 0.30858693 0.31796339 0.31422698 0.30189523 0.29098371 0.29087931 0.30339491][0.081257604 0.12142771 0.17349273 0.22604032 0.28019136 0.33765009 0.39847022 0.44630629 0.47881141 0.49222344 0.48212025 0.44828677 0.39472494 0.33526751 0.2845161][0.1014704 0.14790884 0.20849022 0.27135372 0.33883685 0.41275722 0.48979488 0.54795581 0.58443183 0.59841603 0.58574086 0.53943735 0.45541134 0.35132977 0.25343749][0.10198774 0.14605464 0.20314668 0.26380044 0.33204836 0.41002011 0.49036196 0.54656082 0.5751884 0.58096886 0.56471413 0.51558977 0.42332855 0.30556905 0.19390273][0.074876122 0.10347773 0.14268211 0.18818334 0.24556467 0.31680948 0.3911722 0.43982726 0.45770943 0.45338809 0.43367925 0.38910952 0.30883357 0.20798509 0.11470305][0.029206781 0.035035625 0.049170859 0.072642542 0.1117236 0.16784376 0.22888194 0.26748067 0.2772437 0.26742336 0.24776772 0.21256578 0.15552732 0.089977317 0.034826342][-0.0098682716 -0.019993933 -0.024121227 -0.019106627 -0.00044646312 0.032816995 0.070806429 0.09339606 0.095607653 0.085005634 0.069784775 0.046960559 0.015424139 -0.012286135 -0.024888758][-0.020315258 -0.028942786 -0.034542322 -0.036839984 -0.03610114 -0.030812522 -0.024301512 -0.025138266 -0.03407969 -0.045207422 -0.05381934 -0.062828586 -0.070870012 -0.065612718 -0.041087549][0.00020336152 0.010306141 0.02035374 0.023709504 0.013076905 -0.008667063 -0.036039881 -0.064960308 -0.090201877 -0.10525891 -0.10767996 -0.10252837 -0.090293944 -0.060175005 -0.0083143124][0.03390085 0.06765794 0.099354267 0.11494321 0.10050258 0.060161907 0.0068785027 -0.045588214 -0.086312227 -0.10674199 -0.10471448 -0.087689571 -0.060163926 -0.014064227 0.054045558][0.056213379 0.10414083 0.14978863 0.17502189 0.1623266 0.11576124 0.051207606 -0.013211049 -0.0625382 -0.086247668 -0.08126498 -0.05617274 -0.018634425 0.036324471 0.11143187]]...]
INFO - root - 2017-12-11 00:50:03.488560: step 70210, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 56h:39m:57s remains)
INFO - root - 2017-12-11 00:50:11.040810: step 70220, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 57h:15m:51s remains)
INFO - root - 2017-12-11 00:50:18.854125: step 70230, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 55h:10m:07s remains)
INFO - root - 2017-12-11 00:50:26.665641: step 70240, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 55h:45m:32s remains)
INFO - root - 2017-12-11 00:50:34.339501: step 70250, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.775 sec/batch; 56h:26m:05s remains)
INFO - root - 2017-12-11 00:50:42.213571: step 70260, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 56h:52m:11s remains)
INFO - root - 2017-12-11 00:50:50.188796: step 70270, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 58h:25m:37s remains)
INFO - root - 2017-12-11 00:50:58.008783: step 70280, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 55h:28m:49s remains)
INFO - root - 2017-12-11 00:51:05.833235: step 70290, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 57h:46m:14s remains)
INFO - root - 2017-12-11 00:51:13.473410: step 70300, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 58h:55m:54s remains)
2017-12-11 00:51:14.350846: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28440088 0.26696509 0.24112941 0.21757048 0.19682862 0.19431129 0.20096651 0.20246482 0.18857288 0.16518909 0.14443073 0.11715297 0.080167025 0.040792841 0.014765695][0.30561629 0.28047457 0.25165033 0.23130715 0.21728428 0.22292115 0.23428422 0.23350295 0.210815 0.17594077 0.1445034 0.10947081 0.067754656 0.026358346 -0.00038372041][0.29077357 0.26361522 0.23882507 0.22762632 0.22569279 0.24346621 0.26310956 0.26386505 0.23686063 0.19380935 0.15177183 0.10644596 0.057732757 0.013700943 -0.013806732][0.25933033 0.2394567 0.22756135 0.23079991 0.24428214 0.27635363 0.3064805 0.310368 0.27952698 0.22818583 0.17392638 0.11542599 0.056110866 0.0070458986 -0.022322671][0.20961416 0.20483465 0.21225238 0.23274913 0.26192719 0.30725521 0.347504 0.35520187 0.32127362 0.26177615 0.1959628 0.12580769 0.057321094 0.0037417451 -0.027302446][0.15273164 0.16604435 0.19271067 0.22812976 0.26940614 0.3236061 0.37154648 0.38390872 0.34931186 0.28410879 0.21001936 0.13275714 0.059348997 0.0031625368 -0.029402964][0.121004 0.14950515 0.18916707 0.23267882 0.27836949 0.33254737 0.3809126 0.39535397 0.36206755 0.29519767 0.21734038 0.13795625 0.063733853 0.0064741443 -0.027961576][0.12898 0.16647923 0.21061131 0.2538023 0.29441175 0.33923945 0.37966689 0.39174557 0.36040634 0.29616451 0.21932021 0.14159139 0.069203921 0.012250611 -0.02367658][0.15651652 0.19451094 0.23556587 0.2727578 0.30289423 0.33385235 0.36267042 0.37091675 0.34287506 0.28461212 0.21289802 0.13988718 0.071213938 0.015910385 -0.019810049][0.19063614 0.22213057 0.2533344 0.28013 0.29769656 0.31551343 0.3347151 0.34097135 0.31811929 0.26742762 0.20260134 0.13485545 0.069978818 0.016898541 -0.017453607][0.22084056 0.24203743 0.26003283 0.27462807 0.28091657 0.29009718 0.30429024 0.31042117 0.2918095 0.24704215 0.18818869 0.12512963 0.0639116 0.013865445 -0.018044202][0.22629872 0.23631547 0.24241576 0.24673972 0.24549055 0.25096235 0.26364174 0.26993844 0.25330225 0.21300821 0.16072156 0.1042702 0.049220584 0.0045936322 -0.022896828][0.19804679 0.19816475 0.19589032 0.19362569 0.18881847 0.1934125 0.20570265 0.21132652 0.1952028 0.15942127 0.11567234 0.069154054 0.024165628 -0.011455597 -0.031785969][0.1381925 0.1317967 0.12545519 0.12047228 0.11515868 0.11954305 0.13079883 0.13504481 0.12009507 0.090541832 0.057404492 0.023476662 -0.0083389189 -0.032038134 -0.043182153][0.066989 0.058615688 0.0517296 0.046353288 0.041591104 0.044964686 0.053692505 0.056309078 0.044312194 0.022822663 0.00079831411 -0.020429837 -0.03879017 -0.050352763 -0.05282126]]...]
INFO - root - 2017-12-11 00:51:22.328178: step 70310, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 57h:26m:34s remains)
INFO - root - 2017-12-11 00:51:30.143384: step 70320, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 56h:20m:24s remains)
INFO - root - 2017-12-11 00:51:37.981948: step 70330, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 55h:45m:29s remains)
INFO - root - 2017-12-11 00:51:45.640131: step 70340, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 56h:54m:47s remains)
INFO - root - 2017-12-11 00:51:53.555712: step 70350, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 55h:53m:42s remains)
INFO - root - 2017-12-11 00:52:01.441316: step 70360, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 56h:36m:31s remains)
INFO - root - 2017-12-11 00:52:09.453061: step 70370, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 58h:57m:21s remains)
INFO - root - 2017-12-11 00:52:17.111081: step 70380, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 57h:00m:36s remains)
INFO - root - 2017-12-11 00:52:24.956510: step 70390, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 55h:55m:21s remains)
INFO - root - 2017-12-11 00:52:32.814862: step 70400, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 56h:47m:44s remains)
2017-12-11 00:52:33.771623: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.034470502 -0.032835916 -0.037896164 -0.04894584 -0.061471432 -0.072595388 -0.081547126 -0.089465827 -0.097126335 -0.10376732 -0.10861411 -0.11143812 -0.11180802 -0.11012652 -0.10802941][-0.023672547 -0.01407523 -0.011368942 -0.015118459 -0.02032375 -0.02489261 -0.029496839 -0.036424629 -0.046363961 -0.056953084 -0.06678316 -0.075906008 -0.082957804 -0.087298661 -0.090866975][-0.0092662182 0.010952537 0.026020847 0.035604626 0.044282664 0.052206188 0.056975663 0.054798022 0.044601224 0.03121349 0.018169647 0.0052819615 -0.0067908396 -0.017132113 -0.027420225][0.0075947153 0.036856677 0.064717852 0.08937674 0.11381755 0.13639006 0.1539325 0.16093852 0.15534532 0.14566574 0.13724613 0.12738636 0.11422651 0.099608526 0.0809737][0.024911752 0.062220953 0.10297492 0.14371178 0.1843935 0.22183512 0.25271973 0.27007926 0.2708714 0.26811555 0.2696712 0.26852873 0.25843775 0.24082127 0.21254742][0.042722903 0.087170087 0.14112745 0.199848 0.25882807 0.31201288 0.35568726 0.38155189 0.3857604 0.38669175 0.39722183 0.40634486 0.40209815 0.38352743 0.34793293][0.055649471 0.10570813 0.17160945 0.24817333 0.32563651 0.39459544 0.44991806 0.48221609 0.48815435 0.49049819 0.50904888 0.530099 0.5349772 0.51871294 0.47958642][0.062521562 0.11352038 0.18593363 0.2756182 0.36841971 0.45213526 0.51928091 0.55885416 0.56763113 0.57089823 0.59658086 0.62903643 0.64255261 0.62742412 0.58410043][0.069297656 0.11632456 0.18769836 0.28257576 0.38429439 0.47863907 0.55475825 0.60003334 0.61167139 0.6149447 0.64443851 0.68407857 0.70441967 0.69145584 0.64614469][0.0858171 0.1231368 0.18371238 0.27122444 0.36935607 0.46306008 0.53892064 0.58426344 0.59663182 0.59761161 0.62555128 0.66676944 0.69242644 0.68485314 0.64334548][0.12084226 0.14291853 0.18140081 0.2457552 0.32299897 0.3994441 0.4609037 0.49715313 0.50706744 0.50594884 0.53053492 0.56999403 0.59923065 0.59859794 0.56437904][0.1719355 0.17751713 0.1900558 0.22211142 0.265792 0.31080976 0.34474277 0.36189917 0.36446559 0.36189187 0.38321277 0.41838649 0.4465414 0.44897652 0.42052472][0.22792637 0.22526591 0.22020353 0.22463511 0.23399362 0.24195349 0.23953095 0.22674803 0.21166426 0.20184903 0.21651313 0.24444792 0.26869237 0.27303654 0.25193581][0.2721574 0.273597 0.26705825 0.25862432 0.24353193 0.21827064 0.17827293 0.13024226 0.089812793 0.065785162 0.068257846 0.08540915 0.10433386 0.11204877 0.10227697][0.29067788 0.30671766 0.31307882 0.30590242 0.2774488 0.22727108 0.156291 0.077552542 0.012747624 -0.027254006 -0.039090656 -0.0335895 -0.019774159 -0.008120331 -0.0060499958]]...]
INFO - root - 2017-12-11 00:52:41.767363: step 70410, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 56h:29m:22s remains)
INFO - root - 2017-12-11 00:52:49.587393: step 70420, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 56h:43m:54s remains)
INFO - root - 2017-12-11 00:52:57.341711: step 70430, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 55h:51m:51s remains)
INFO - root - 2017-12-11 00:53:05.348895: step 70440, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 57h:11m:58s remains)
INFO - root - 2017-12-11 00:53:13.295373: step 70450, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 56h:12m:11s remains)
INFO - root - 2017-12-11 00:53:21.061511: step 70460, loss = 0.70, batch loss = 0.65 (9.6 examples/sec; 0.831 sec/batch; 60h:28m:13s remains)
INFO - root - 2017-12-11 00:53:28.981068: step 70470, loss = 0.70, batch loss = 0.65 (9.8 examples/sec; 0.813 sec/batch; 59h:10m:23s remains)
INFO - root - 2017-12-11 00:53:36.919667: step 70480, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 57h:11m:48s remains)
INFO - root - 2017-12-11 00:53:44.724145: step 70490, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 55h:56m:27s remains)
INFO - root - 2017-12-11 00:53:52.601431: step 70500, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.773 sec/batch; 56h:14m:20s remains)
2017-12-11 00:53:53.431673: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19062638 0.16563682 0.13291752 0.1120494 0.10806166 0.11652625 0.13533401 0.15671742 0.17328297 0.18928719 0.21145484 0.23499288 0.24651536 0.23724467 0.20500761][0.20667562 0.18835832 0.16605714 0.15867381 0.16572492 0.17588729 0.18747684 0.19701512 0.20196131 0.21335219 0.24237953 0.28490636 0.32093635 0.33173689 0.30787337][0.19501491 0.19361012 0.19571432 0.21616307 0.24563612 0.26328528 0.26489541 0.25210056 0.23087822 0.22543052 0.25239244 0.30864966 0.36690176 0.39895344 0.38763571][0.1860832 0.2078137 0.24177262 0.298277 0.35862949 0.39130646 0.387566 0.35178486 0.29873896 0.26541358 0.27672544 0.32956845 0.39260408 0.43284437 0.42838478][0.18500932 0.22790454 0.29131582 0.38222763 0.47422668 0.52686232 0.52748418 0.47945544 0.40298927 0.34121424 0.32610837 0.35705623 0.4025622 0.4312399 0.42180315][0.18561131 0.24242151 0.32566074 0.44052383 0.55734479 0.63116419 0.64664304 0.6035164 0.52173769 0.44136494 0.39683139 0.3933889 0.40442234 0.40449747 0.37728158][0.18250605 0.24031654 0.32821235 0.45065951 0.57999426 0.67358023 0.71465921 0.6967364 0.63268518 0.55070066 0.4822543 0.44049007 0.40839234 0.36912507 0.31423208][0.17247725 0.21608858 0.29010195 0.39986321 0.52233708 0.6236816 0.68851006 0.70560372 0.67674983 0.61427265 0.54154062 0.47575155 0.40961087 0.335043 0.25253963][0.15135381 0.17177802 0.21870457 0.30044556 0.39959785 0.49255478 0.567163 0.6105963 0.61602032 0.58299732 0.52583355 0.45923117 0.38121858 0.28994569 0.19341905][0.11089495 0.10764113 0.12400286 0.17045625 0.23616844 0.3068718 0.37440512 0.42612612 0.45166925 0.44472995 0.41281658 0.3637054 0.29568958 0.21022418 0.11986881][0.05182441 0.033944648 0.026188696 0.040260576 0.072217196 0.11504839 0.16433777 0.20944701 0.24037157 0.24998458 0.24097988 0.21422163 0.1667898 0.10130582 0.031852845][-0.010491516 -0.032469347 -0.052069236 -0.058761653 -0.051813126 -0.033710856 -0.0063692369 0.022673957 0.045527086 0.057804476 0.060565274 0.05059566 0.024466723 -0.015325335 -0.056514826][-0.056551456 -0.075562477 -0.096194252 -0.11086041 -0.1171129 -0.11594068 -0.10772131 -0.09642484 -0.087315388 -0.0817251 -0.07804738 -0.0797664 -0.089621522 -0.1058821 -0.12041586][-0.082538575 -0.09589991 -0.11105991 -0.12450702 -0.13397983 -0.14037794 -0.14334126 -0.14413521 -0.14546928 -0.14683886 -0.146333 -0.14611161 -0.14779459 -0.15000491 -0.14836097][-0.094966 -0.1035893 -0.11166135 -0.11993013 -0.12715799 -0.13426639 -0.14077161 -0.14644313 -0.15214378 -0.15694189 -0.15931253 -0.16008884 -0.15998434 -0.1577563 -0.15114802]]...]
INFO - root - 2017-12-11 00:54:01.229308: step 70510, loss = 0.70, batch loss = 0.64 (12.3 examples/sec; 0.653 sec/batch; 47h:31m:18s remains)
INFO - root - 2017-12-11 00:54:09.126374: step 70520, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 57h:21m:20s remains)
INFO - root - 2017-12-11 00:54:17.092698: step 70530, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 56h:38m:27s remains)
INFO - root - 2017-12-11 00:54:24.879856: step 70540, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 57h:17m:53s remains)
INFO - root - 2017-12-11 00:54:32.774138: step 70550, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 56h:52m:58s remains)
INFO - root - 2017-12-11 00:54:40.683681: step 70560, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 56h:33m:58s remains)
INFO - root - 2017-12-11 00:54:48.543442: step 70570, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 57h:42m:14s remains)
INFO - root - 2017-12-11 00:54:56.525734: step 70580, loss = 0.72, batch loss = 0.66 (10.3 examples/sec; 0.778 sec/batch; 56h:34m:53s remains)
INFO - root - 2017-12-11 00:55:04.416106: step 70590, loss = 0.67, batch loss = 0.62 (10.1 examples/sec; 0.791 sec/batch; 57h:31m:01s remains)
INFO - root - 2017-12-11 00:55:12.133420: step 70600, loss = 0.70, batch loss = 0.64 (12.0 examples/sec; 0.666 sec/batch; 48h:26m:48s remains)
2017-12-11 00:55:12.998035: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.045178179 0.0538706 0.063289925 0.078372166 0.10085738 0.12895848 0.16036962 0.19530816 0.22734872 0.24692647 0.24750663 0.22709318 0.18420377 0.12152503 0.054424074][0.077990025 0.091111332 0.10367373 0.12452147 0.15793189 0.19975619 0.24463068 0.29316705 0.33817098 0.36645836 0.36727864 0.33799067 0.27870265 0.19434962 0.10370923][0.11870481 0.13848756 0.15499397 0.18079326 0.22313716 0.27445406 0.32611832 0.37895918 0.42697948 0.45469338 0.4491041 0.40817219 0.33483931 0.23589411 0.13053656][0.14970206 0.17647307 0.19866036 0.23017357 0.27912188 0.33472297 0.38666946 0.43621352 0.47858721 0.497258 0.48101282 0.43133366 0.35183179 0.24792482 0.13730982][0.16053557 0.19130586 0.21926157 0.25701073 0.309894 0.36563894 0.4143478 0.45773011 0.49101487 0.49829543 0.47364753 0.42235157 0.34637812 0.24617794 0.13687634][0.17136228 0.20203695 0.23296531 0.27427679 0.32646668 0.3772687 0.41943687 0.45562035 0.47950247 0.47656551 0.44654471 0.397842 0.3293235 0.23591709 0.13079081][0.20656379 0.23320167 0.26246309 0.30341175 0.35297951 0.39719284 0.43053889 0.45707116 0.47106168 0.46048573 0.42769161 0.38220617 0.3197917 0.23178357 0.13035296][0.24454883 0.26319918 0.28678215 0.32527718 0.37435934 0.4156715 0.44233623 0.46064058 0.46886107 0.45909145 0.43238333 0.39548317 0.34040973 0.256433 0.15536663][0.27242354 0.2810137 0.2966229 0.33148086 0.3814486 0.4231815 0.44692904 0.46147016 0.47071663 0.47112405 0.45942312 0.43629053 0.38962385 0.30736825 0.20187798][0.2895804 0.28958809 0.29683822 0.3265231 0.37578538 0.41769809 0.43974948 0.4522025 0.4637391 0.47398323 0.47661892 0.46621564 0.426801 0.3455475 0.23560295][0.28535074 0.2797637 0.2809132 0.30514693 0.35055366 0.38959396 0.40841898 0.41727713 0.42735183 0.44062796 0.45083693 0.44911155 0.4170886 0.34128559 0.2344787][0.23934311 0.23104453 0.22957389 0.24956009 0.28935823 0.32367155 0.33894068 0.34426996 0.35072091 0.36185595 0.3735171 0.37674317 0.35323524 0.28936759 0.19633615][0.14652427 0.13888025 0.13865742 0.15652864 0.19058576 0.22002836 0.23305126 0.23678561 0.24012019 0.24717669 0.25592041 0.25956938 0.24295285 0.19426835 0.12311137][0.047960658 0.043960504 0.04687681 0.062583417 0.089557558 0.11282206 0.1231814 0.12523216 0.12498017 0.12590912 0.12780134 0.12725322 0.11425773 0.080523409 0.033832349][-0.028370472 -0.028961126 -0.023724332 -0.011005238 0.0084664943 0.0253155 0.0327595 0.032835342 0.028979063 0.023727639 0.018352594 0.012703953 0.0021522918 -0.017992673 -0.042330414]]...]
INFO - root - 2017-12-11 00:55:20.867574: step 70610, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 55h:36m:00s remains)
INFO - root - 2017-12-11 00:55:28.618336: step 70620, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 56h:51m:53s remains)
INFO - root - 2017-12-11 00:55:36.566733: step 70630, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 56h:49m:01s remains)
INFO - root - 2017-12-11 00:55:44.430248: step 70640, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 57h:19m:35s remains)
INFO - root - 2017-12-11 00:55:52.327540: step 70650, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.756 sec/batch; 55h:00m:39s remains)
INFO - root - 2017-12-11 00:56:00.161560: step 70660, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.767 sec/batch; 55h:48m:37s remains)
INFO - root - 2017-12-11 00:56:08.106649: step 70670, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 58h:37m:33s remains)
INFO - root - 2017-12-11 00:56:15.992037: step 70680, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.812 sec/batch; 59h:02m:10s remains)
INFO - root - 2017-12-11 00:56:23.610865: step 70690, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 57h:00m:44s remains)
INFO - root - 2017-12-11 00:56:31.321647: step 70700, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 56h:30m:43s remains)
2017-12-11 00:56:32.095710: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.048817661 0.030700862 0.025935788 0.031279869 0.036820848 0.039026354 0.038603235 0.035214167 0.029986745 0.027316811 0.030236451 0.03866718 0.049956996 0.062729821 0.076707251][0.096403219 0.072501771 0.067710467 0.0782571 0.089240924 0.094485886 0.094523542 0.088930279 0.0795701 0.073067687 0.075580329 0.0876157 0.10545618 0.12793168 0.15330362][0.15116951 0.12550949 0.12268798 0.13962534 0.15818162 0.16925092 0.17213936 0.1655388 0.1513866 0.13894291 0.136951 0.14876664 0.16841786 0.19507955 0.2287277][0.20246859 0.17989247 0.18091574 0.20434721 0.23116817 0.24948654 0.2565248 0.24919814 0.22916394 0.2087945 0.19970073 0.20592161 0.22145569 0.24779356 0.28725529][0.24775232 0.23061243 0.23440927 0.26216894 0.29606256 0.32177955 0.33334658 0.32551384 0.30031592 0.27315366 0.25719178 0.25644064 0.26533729 0.28849366 0.33040324][0.29042086 0.28062963 0.28579792 0.31540596 0.35542309 0.3912636 0.41137135 0.40586239 0.37830785 0.34652483 0.32368013 0.31391177 0.31437582 0.33414418 0.37730625][0.3307057 0.33048409 0.33568355 0.36371255 0.40675917 0.45309114 0.48429373 0.48423061 0.45759565 0.4234764 0.39229026 0.36881533 0.35671064 0.37229156 0.4171606][0.35582092 0.36550018 0.37188318 0.39738867 0.43938136 0.49125677 0.53091943 0.53755242 0.51580739 0.48297146 0.44660419 0.41076925 0.38642886 0.39673415 0.44012886][0.36607277 0.38326246 0.39099219 0.41227797 0.44665781 0.49296388 0.53096533 0.539693 0.5232532 0.49591017 0.462628 0.42571077 0.39927548 0.40863669 0.44818532][0.37030429 0.3892737 0.39539996 0.40845051 0.42754906 0.45674691 0.48170549 0.48521677 0.47106668 0.45071852 0.4263505 0.39836848 0.37908638 0.39089975 0.42628881][0.36404064 0.38391104 0.38898122 0.393193 0.39505237 0.40276027 0.40920588 0.40156162 0.38392583 0.36618793 0.34910676 0.3314417 0.32190585 0.33820626 0.37279159][0.34176773 0.36226818 0.36887783 0.36777934 0.35643703 0.34433606 0.33139089 0.30886188 0.281695 0.25987434 0.24529262 0.23557581 0.23455693 0.25464085 0.29035792][0.30668858 0.32623217 0.33570221 0.33408621 0.31620863 0.28986758 0.26005435 0.2219132 0.18239099 0.15327349 0.13851093 0.13511913 0.14110975 0.16389287 0.20026496][0.268687 0.2852833 0.29579338 0.29540327 0.27639669 0.24274245 0.20117863 0.15019639 0.10018171 0.065180793 0.049674124 0.05043117 0.060705792 0.08406882 0.11815666][0.2352386 0.24885547 0.2587558 0.2592656 0.24143614 0.20578755 0.1578676 0.098121755 0.041140687 0.0024548569 -0.014564808 -0.013726598 -0.0033761712 0.018636586 0.049078014]]...]
INFO - root - 2017-12-11 00:56:40.063524: step 70710, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 57h:44m:10s remains)
INFO - root - 2017-12-11 00:56:47.884150: step 70720, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 56h:31m:10s remains)
INFO - root - 2017-12-11 00:56:55.862040: step 70730, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 58h:48m:43s remains)
INFO - root - 2017-12-11 00:57:03.890273: step 70740, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 57h:25m:01s remains)
INFO - root - 2017-12-11 00:57:11.742882: step 70750, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.767 sec/batch; 55h:45m:03s remains)
INFO - root - 2017-12-11 00:57:19.589917: step 70760, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 55h:51m:47s remains)
INFO - root - 2017-12-11 00:57:27.473225: step 70770, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 58h:00m:32s remains)
INFO - root - 2017-12-11 00:57:35.005292: step 70780, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 57h:58m:59s remains)
INFO - root - 2017-12-11 00:57:42.778324: step 70790, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 56h:57m:21s remains)
INFO - root - 2017-12-11 00:57:50.715620: step 70800, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 56h:53m:20s remains)
2017-12-11 00:57:51.665214: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.030147331 0.027914898 0.025404252 0.023128014 0.022290396 0.023991976 0.026181335 0.026405556 0.023792699 0.018615592 0.010643643 0.0021924144 -0.0020486461 0.00081565289 0.0093747647][0.029739566 0.027256303 0.025098385 0.024538681 0.028653933 0.039105035 0.051354591 0.060257159 0.063105874 0.059755389 0.048271839 0.032133423 0.019742692 0.017111491 0.023481196][0.023286296 0.021646772 0.021510895 0.024676826 0.036842182 0.061055131 0.0895281 0.11250006 0.12463741 0.12449126 0.10796666 0.0797098 0.0529862 0.038732972 0.0382587][0.017623143 0.020274697 0.026014134 0.036406603 0.059851613 0.10146161 0.15031031 0.19095132 0.2137454 0.215912 0.19184779 0.14744116 0.10067715 0.06844905 0.056607172][0.025984978 0.037773822 0.053288724 0.072700933 0.10667463 0.16289684 0.22889866 0.28378922 0.31273329 0.31221581 0.27742165 0.21627702 0.14974311 0.09925241 0.075698771][0.054075014 0.079976328 0.10876235 0.13756353 0.17817593 0.24101089 0.314177 0.37271729 0.39728972 0.38518378 0.33558896 0.25987107 0.18011013 0.11872463 0.08901754][0.09446504 0.1368292 0.18025389 0.21749641 0.25985619 0.320429 0.38975465 0.44090474 0.45174378 0.42173427 0.35604066 0.27000269 0.18544194 0.12253813 0.093344562][0.13055515 0.18731828 0.24320152 0.28627661 0.32542756 0.37512594 0.42985383 0.46448442 0.45662126 0.40888771 0.33202687 0.24423374 0.16513649 0.11011437 0.087362148][0.14711387 0.2120446 0.27445349 0.31813404 0.34828767 0.37876111 0.4091095 0.4204607 0.3941144 0.33588359 0.25906643 0.181319 0.11780272 0.078023255 0.065463744][0.13718358 0.19992542 0.25942603 0.29717818 0.31497309 0.32393843 0.32756606 0.31504431 0.27518135 0.21634713 0.15117723 0.092968158 0.050983477 0.029107293 0.026752703][0.10263301 0.15266578 0.20075126 0.22912972 0.23727696 0.23212458 0.21806562 0.1909022 0.14690632 0.096203484 0.048895258 0.012515293 -0.0091697276 -0.016175514 -0.011386746][0.056110855 0.087044381 0.11841541 0.13630319 0.13921069 0.1299825 0.11167061 0.083694749 0.046628084 0.010296073 -0.018400909 -0.036347903 -0.043457247 -0.041598197 -0.033882692][0.013007035 0.025807304 0.040623948 0.048708733 0.048788309 0.041392528 0.027765317 0.008039929 -0.016058089 -0.036844127 -0.050185081 -0.055440105 -0.05423338 -0.048825432 -0.041280366][-0.016331727 -0.015415595 -0.012216597 -0.011369176 -0.013086916 -0.017257227 -0.023992177 -0.034006681 -0.04596661 -0.054908659 -0.058569215 -0.057316963 -0.053114824 -0.047849491 -0.042282023][-0.029382661 -0.033392485 -0.035186175 -0.037249718 -0.038763836 -0.039593652 -0.041075747 -0.044897243 -0.050124064 -0.05363521 -0.054084025 -0.051795259 -0.04829495 -0.044973794 -0.041918371]]...]
INFO - root - 2017-12-11 00:57:59.654766: step 70810, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.757 sec/batch; 55h:02m:03s remains)
INFO - root - 2017-12-11 00:58:07.558001: step 70820, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 57h:39m:00s remains)
INFO - root - 2017-12-11 00:58:15.657441: step 70830, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 57h:25m:02s remains)
INFO - root - 2017-12-11 00:58:23.543028: step 70840, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 57h:27m:49s remains)
INFO - root - 2017-12-11 00:58:31.475002: step 70850, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 58h:00m:49s remains)
INFO - root - 2017-12-11 00:58:39.240098: step 70860, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 57h:50m:08s remains)
INFO - root - 2017-12-11 00:58:47.071124: step 70870, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 56h:53m:37s remains)
INFO - root - 2017-12-11 00:58:54.879994: step 70880, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 55h:16m:02s remains)
INFO - root - 2017-12-11 00:59:02.738601: step 70890, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 58h:08m:46s remains)
INFO - root - 2017-12-11 00:59:10.647992: step 70900, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.811 sec/batch; 58h:57m:09s remains)
2017-12-11 00:59:11.504012: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25714338 0.27383253 0.251513 0.18979184 0.10557578 0.026823333 -0.026473386 -0.052888419 -0.061165441 -0.060705736 -0.056141447 -0.049488749 -0.043718543 -0.03910416 -0.036444187][0.30177921 0.32980445 0.30915108 0.23912932 0.14228666 0.054349314 -0.0022882882 -0.028720643 -0.037638258 -0.041361231 -0.044577375 -0.047645614 -0.049718436 -0.048869219 -0.046097212][0.31327868 0.35010749 0.33513635 0.267051 0.17197257 0.090621188 0.044920202 0.028943537 0.025161905 0.018792748 0.0067292177 -0.00892062 -0.021400247 -0.024349628 -0.018370507][0.29614487 0.33633581 0.3298797 0.27526727 0.19927834 0.14283712 0.1230168 0.12693661 0.13263077 0.12431756 0.10244309 0.072153613 0.047660969 0.041284204 0.053255625][0.26136765 0.298568 0.30191335 0.27083626 0.22961344 0.21341537 0.23003852 0.25878647 0.27490145 0.26402736 0.23170914 0.18663621 0.15040705 0.14108546 0.15962625][0.22396003 0.25290826 0.26523948 0.26235396 0.2634972 0.29288036 0.34634089 0.39710915 0.42058352 0.40636769 0.36503103 0.30815262 0.26242033 0.2494067 0.27074379][0.19270197 0.21267752 0.23240696 0.25534523 0.29425314 0.36046112 0.43879074 0.50037962 0.52440333 0.50643665 0.46033809 0.39811391 0.34691787 0.32826722 0.34488595][0.16529942 0.17908342 0.20460795 0.24539119 0.30740345 0.39090341 0.47402883 0.53091919 0.54805589 0.52752304 0.483617 0.42519426 0.37445444 0.34916651 0.35330695][0.14085339 0.15088025 0.17765473 0.22329155 0.28846133 0.36682472 0.4355551 0.47545195 0.48181835 0.46234304 0.42889261 0.38428158 0.34150478 0.31174377 0.3000246][0.1296858 0.1339483 0.15202805 0.18612172 0.23524652 0.29146039 0.33535624 0.35485482 0.35272205 0.33976433 0.32343525 0.29936439 0.27031565 0.24050286 0.2152169][0.14304033 0.13791052 0.1370344 0.14618181 0.16722617 0.19455217 0.21378605 0.21784803 0.2132645 0.21108975 0.21384531 0.21123958 0.19789179 0.17253219 0.14016493][0.19174393 0.17775007 0.15430349 0.13389108 0.12386312 0.12260793 0.1207184 0.11407001 0.1099555 0.11793064 0.13613358 0.15056342 0.15094796 0.13289893 0.10079266][0.27519777 0.25813037 0.21757704 0.17287593 0.13556033 0.10735097 0.084057212 0.065619253 0.059353169 0.072399721 0.098967567 0.122988 0.13231161 0.12165719 0.095162757][0.37046918 0.36026412 0.31501168 0.25838953 0.20228864 0.14990821 0.10285495 0.067658111 0.052939873 0.063349754 0.089819908 0.11522402 0.12762938 0.12257621 0.10509921][0.43969941 0.44504392 0.40861908 0.35484272 0.29166907 0.22189341 0.15338148 0.10067099 0.074356206 0.076250575 0.0953069 0.11490479 0.12466169 0.12236384 0.11551232]]...]
INFO - root - 2017-12-11 00:59:19.404962: step 70910, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 59h:13m:29s remains)
INFO - root - 2017-12-11 00:59:27.300514: step 70920, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.804 sec/batch; 58h:23m:58s remains)
INFO - root - 2017-12-11 00:59:35.169162: step 70930, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 57h:43m:49s remains)
INFO - root - 2017-12-11 00:59:42.811057: step 70940, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 57h:32m:59s remains)
INFO - root - 2017-12-11 00:59:50.756741: step 70950, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 56h:32m:08s remains)
INFO - root - 2017-12-11 00:59:58.459795: step 70960, loss = 0.72, batch loss = 0.66 (9.8 examples/sec; 0.813 sec/batch; 59h:04m:21s remains)
INFO - root - 2017-12-11 01:00:06.366110: step 70970, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 56h:06m:21s remains)
INFO - root - 2017-12-11 01:00:14.293586: step 70980, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.812 sec/batch; 59h:00m:05s remains)
INFO - root - 2017-12-11 01:00:22.113218: step 70990, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 55h:57m:39s remains)
INFO - root - 2017-12-11 01:00:29.960935: step 71000, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 57h:47m:27s remains)
2017-12-11 01:00:30.805553: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15215851 0.25942031 0.35248151 0.4144761 0.4407295 0.43609807 0.40508473 0.36170179 0.31847897 0.28831521 0.27778637 0.29058346 0.32060269 0.3582125 0.39117151][0.098062664 0.18788566 0.26646703 0.3177954 0.33855367 0.33399439 0.3083359 0.27446342 0.24358879 0.22395752 0.21950895 0.23359813 0.26284951 0.30062136 0.3354086][0.052802134 0.12623511 0.18888873 0.22458759 0.23194179 0.21796064 0.19025788 0.16196845 0.14320976 0.13590528 0.13807935 0.14931881 0.168393 0.19407471 0.22079533][0.040435106 0.11177182 0.17121044 0.1992261 0.19450858 0.16688578 0.13061395 0.10109729 0.087231524 0.084148675 0.083117433 0.079780005 0.075262591 0.074759923 0.081897534][0.064530045 0.14819379 0.21905665 0.25217882 0.24515569 0.20966414 0.1661533 0.13232961 0.11559495 0.1061836 0.090550743 0.062146921 0.0240634 -0.011966229 -0.032509014][0.1091453 0.21272661 0.30317193 0.35110256 0.35383371 0.32247245 0.27956483 0.24384266 0.22179359 0.20175196 0.16736533 0.11170815 0.039053477 -0.033780292 -0.084482916][0.15111007 0.273754 0.38401529 0.45038408 0.47106647 0.45414931 0.42026249 0.38923898 0.36783195 0.34367532 0.29696178 0.21989632 0.11762652 0.010815926 -0.070602082][0.1797657 0.31471673 0.43787584 0.51828015 0.55577165 0.555279 0.53439051 0.51633233 0.50821489 0.49440819 0.4476161 0.35755685 0.23128638 0.093397409 -0.019004654][0.19642243 0.33320698 0.45550752 0.5363217 0.57905793 0.58782446 0.5782302 0.57849222 0.59475487 0.60466814 0.56997997 0.47713858 0.33632156 0.17553137 0.036797732][0.20296046 0.33224952 0.4415262 0.51037288 0.5461266 0.55378157 0.549924 0.56811351 0.61196244 0.65105605 0.63637167 0.5521121 0.41032594 0.23971894 0.084186524][0.21022289 0.33155385 0.42558339 0.47843197 0.50033176 0.49851045 0.49305269 0.52186573 0.58604389 0.64934009 0.655915 0.5886237 0.45797923 0.28971651 0.1273236][0.23750931 0.35961294 0.44516304 0.48438886 0.48990664 0.47271436 0.45777926 0.48550197 0.55539608 0.62883645 0.64880919 0.59737927 0.48075643 0.32011658 0.15800929][0.28014904 0.41205823 0.498478 0.53042322 0.52229941 0.4890224 0.45947444 0.47440439 0.53294915 0.59723735 0.61497062 0.56911516 0.46186167 0.31071937 0.15687712][0.31341323 0.45516443 0.54583323 0.57522774 0.55963993 0.51629484 0.47472495 0.47251657 0.50840765 0.54804724 0.54875588 0.49772337 0.39607078 0.25884598 0.1233841][0.31571129 0.45869088 0.55038238 0.57947403 0.56377953 0.52006876 0.4736948 0.45650527 0.46698546 0.47671941 0.45469478 0.39510107 0.30171961 0.18704908 0.08039692]]...]
INFO - root - 2017-12-11 01:00:38.757891: step 71010, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.812 sec/batch; 59h:00m:14s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 01:00:46.463433: step 71020, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 57h:38m:51s remains)
INFO - root - 2017-12-11 01:00:54.299649: step 71030, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 56h:36m:16s remains)
INFO - root - 2017-12-11 01:01:02.296896: step 71040, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 57h:08m:56s remains)
INFO - root - 2017-12-11 01:01:09.961771: step 71050, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 57h:44m:39s remains)
INFO - root - 2017-12-11 01:01:17.939557: step 71060, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 57h:15m:04s remains)
INFO - root - 2017-12-11 01:01:25.812596: step 71070, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 56h:44m:20s remains)
INFO - root - 2017-12-11 01:01:33.627082: step 71080, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 58h:04m:44s remains)
INFO - root - 2017-12-11 01:01:41.606325: step 71090, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 58h:17m:29s remains)
INFO - root - 2017-12-11 01:01:49.193829: step 71100, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 57h:57m:18s remains)
2017-12-11 01:01:50.028033: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.37504759 0.32357967 0.25427338 0.21388817 0.2133112 0.24345388 0.27821705 0.29090077 0.2658672 0.2126797 0.16185376 0.1309488 0.1234664 0.13467954 0.14980754][0.49453089 0.42462662 0.3295024 0.27247795 0.271839 0.31732851 0.37170845 0.39557111 0.36710274 0.29767624 0.22559829 0.17403163 0.15183097 0.1576169 0.17364094][0.55770367 0.47696915 0.3680824 0.306156 0.31545526 0.38174349 0.45636398 0.4897224 0.45659798 0.36993545 0.2736724 0.19864948 0.16031635 0.15999874 0.17673817][0.56495732 0.48507309 0.37712923 0.32213849 0.3482928 0.43605831 0.52805656 0.56904924 0.53250331 0.43110979 0.31105715 0.21141204 0.15521991 0.14685532 0.16161902][0.52732915 0.45761409 0.3638095 0.32502466 0.37097946 0.47816798 0.5851177 0.6337449 0.59746879 0.48807618 0.35085407 0.2306169 0.15623362 0.13556783 0.1420449][0.45583668 0.40556672 0.33897716 0.32711554 0.39730424 0.52470618 0.64782393 0.706548 0.67392945 0.5611043 0.412157 0.27502394 0.18124007 0.14134189 0.13111874][0.38942522 0.36159629 0.32606778 0.34335572 0.43727937 0.58489066 0.72854424 0.80594903 0.78644335 0.67885661 0.526206 0.37810376 0.26596886 0.20230421 0.16894425][0.36336976 0.35048717 0.33440271 0.36821109 0.47469082 0.63521963 0.79832166 0.89928156 0.90140313 0.80917203 0.66253203 0.51097345 0.38493848 0.30027851 0.24498928][0.3913711 0.38782576 0.37620237 0.40691879 0.5050106 0.65931934 0.8267203 0.94277006 0.96628129 0.89703441 0.76724297 0.62142581 0.48938495 0.39156675 0.32084742][0.44762564 0.4458043 0.4250488 0.43432829 0.50333577 0.63130641 0.78504884 0.90401578 0.94535136 0.90412432 0.80128103 0.67250454 0.5465138 0.44741422 0.37153533][0.47539204 0.47169712 0.43795156 0.42044485 0.45226908 0.54271108 0.66932619 0.77939934 0.83105808 0.81538075 0.74281335 0.63962358 0.53176618 0.44396871 0.3742097][0.44828802 0.44110408 0.39646253 0.35526606 0.35115469 0.4002327 0.49041954 0.57967716 0.63078606 0.63291335 0.58921045 0.5175032 0.43868119 0.37358883 0.31932545][0.35726148 0.34638539 0.29749146 0.24297939 0.21291272 0.22556084 0.27665564 0.33609089 0.374651 0.38288087 0.36025777 0.31796321 0.27049312 0.23159324 0.19724491][0.23733139 0.22536014 0.1799832 0.12370198 0.080243886 0.066648111 0.083713211 0.11250314 0.13334405 0.14019585 0.13074386 0.11059126 0.087939724 0.070052318 0.05327386][0.11840156 0.10507493 0.067697406 0.019629171 -0.022921234 -0.047078889 -0.050254308 -0.042678531 -0.035610743 -0.032084 -0.034782823 -0.041737605 -0.049047593 -0.053561267 -0.057843294]]...]
INFO - root - 2017-12-11 01:01:57.841582: step 71110, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 56h:22m:34s remains)
INFO - root - 2017-12-11 01:02:05.720117: step 71120, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 55h:49m:23s remains)
INFO - root - 2017-12-11 01:02:13.626915: step 71130, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 57h:10m:49s remains)
INFO - root - 2017-12-11 01:02:21.272568: step 71140, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 57h:15m:35s remains)
INFO - root - 2017-12-11 01:02:29.127420: step 71150, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 57h:22m:23s remains)
INFO - root - 2017-12-11 01:02:37.040643: step 71160, loss = 0.71, batch loss = 0.65 (9.5 examples/sec; 0.840 sec/batch; 60h:59m:31s remains)
INFO - root - 2017-12-11 01:02:44.822836: step 71170, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 55h:30m:02s remains)
INFO - root - 2017-12-11 01:02:52.565779: step 71180, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 56h:58m:34s remains)
INFO - root - 2017-12-11 01:03:00.401748: step 71190, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.788 sec/batch; 57h:13m:38s remains)
INFO - root - 2017-12-11 01:03:08.232381: step 71200, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 55h:47m:17s remains)
2017-12-11 01:03:09.106587: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27681708 0.35254481 0.40412706 0.43261683 0.42433187 0.38894731 0.34774396 0.3192412 0.31213704 0.32435176 0.34022647 0.34403709 0.32551467 0.2890138 0.22689544][0.33288491 0.4164947 0.46902069 0.49356759 0.47971839 0.44153926 0.4027077 0.38073072 0.38094428 0.398051 0.41413334 0.41240445 0.3831518 0.33496851 0.26100981][0.33450687 0.41514492 0.46309659 0.48463127 0.47466597 0.44871724 0.42591098 0.41740566 0.42590225 0.44438231 0.45510247 0.44260159 0.40038931 0.3419944 0.26053274][0.3083784 0.38520363 0.43224689 0.45834267 0.46285444 0.45910922 0.45669541 0.46012083 0.47282261 0.48806646 0.48858285 0.4614673 0.40573704 0.33842093 0.25104254][0.2870639 0.36560005 0.41798586 0.45424706 0.476777 0.4937396 0.50508577 0.51314491 0.526453 0.53867227 0.53017932 0.49022973 0.42395356 0.34942797 0.25534597][0.28738475 0.37398943 0.43625107 0.48321405 0.51897317 0.54591972 0.55963063 0.56566066 0.57881778 0.59227639 0.580864 0.53386509 0.4606398 0.37865707 0.27539131][0.31440586 0.41260573 0.48482141 0.53687394 0.57366723 0.5947122 0.59843796 0.59596443 0.60665029 0.62313348 0.61523414 0.5697515 0.49598593 0.40909126 0.29809844][0.3502928 0.45473349 0.5284428 0.573891 0.59807754 0.60233694 0.59079564 0.57835817 0.58492583 0.60287642 0.60030252 0.56124306 0.49224117 0.40564418 0.29439154][0.35459724 0.45117635 0.51274413 0.54016674 0.54378968 0.52955031 0.50645709 0.48918405 0.49371472 0.5123117 0.51534331 0.48585212 0.42674631 0.34754658 0.24634227][0.2955938 0.36987588 0.40966949 0.415677 0.40044898 0.37371072 0.34639266 0.3301172 0.33502814 0.35405856 0.36298281 0.34575713 0.301812 0.2380455 0.1570323][0.1871597 0.23186134 0.24877183 0.23884083 0.21405937 0.18463668 0.16023356 0.14819714 0.1534518 0.17072974 0.18287218 0.17710994 0.15054828 0.10759393 0.052952863][0.076277137 0.093843162 0.093179844 0.075518757 0.050356697 0.025976278 0.008720112 0.0015198599 0.0055740424 0.017894892 0.028754393 0.030150853 0.01866583 -0.0037106697 -0.032669634][-0.010045693 -0.01059985 -0.020326016 -0.038349696 -0.058261339 -0.074489437 -0.083892554 -0.086479723 -0.083292939 -0.075745016 -0.068108328 -0.0641692 -0.066404834 -0.074258372 -0.084749833][-0.065198936 -0.073445387 -0.083973974 -0.097339347 -0.1102057 -0.11947317 -0.1236928 -0.12357315 -0.12065311 -0.11558832 -0.10993085 -0.10526513 -0.10306004 -0.10321719 -0.10378123][-0.086420052 -0.095302038 -0.1023534 -0.10981249 -0.11654907 -0.12121429 -0.1231321 -0.1225524 -0.12036407 -0.11680698 -0.11249171 -0.10816281 -0.10459049 -0.10174766 -0.09855786]]...]
INFO - root - 2017-12-11 01:03:16.927436: step 71210, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 57h:10m:20s remains)
INFO - root - 2017-12-11 01:03:24.737863: step 71220, loss = 0.70, batch loss = 0.64 (13.6 examples/sec; 0.588 sec/batch; 42h:38m:35s remains)
INFO - root - 2017-12-11 01:03:32.475737: step 71230, loss = 0.68, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 59h:06m:20s remains)
INFO - root - 2017-12-11 01:03:40.306779: step 71240, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.812 sec/batch; 58h:57m:00s remains)
INFO - root - 2017-12-11 01:03:48.085552: step 71250, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 55h:23m:58s remains)
INFO - root - 2017-12-11 01:03:55.739175: step 71260, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 58h:24m:14s remains)
INFO - root - 2017-12-11 01:04:03.609748: step 71270, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 56h:32m:49s remains)
INFO - root - 2017-12-11 01:04:11.519459: step 71280, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 56h:50m:41s remains)
INFO - root - 2017-12-11 01:04:19.434822: step 71290, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 58h:19m:09s remains)
INFO - root - 2017-12-11 01:04:27.382605: step 71300, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 56h:13m:53s remains)
2017-12-11 01:04:28.240998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0011455594 0.018766299 0.032995179 0.039119609 0.042131547 0.043547008 0.040824614 0.033748031 0.022378935 0.0049412553 -0.020818261 -0.051019061 -0.075350255 -0.086757392 -0.087940663][0.0608611 0.10170993 0.13164628 0.14654863 0.15605052 0.16355522 0.16257024 0.15126097 0.13048066 0.098679118 0.051620815 -0.0044365581 -0.052148413 -0.078907564 -0.087538727][0.14349653 0.21060279 0.26139224 0.28983247 0.31138441 0.3316828 0.33764827 0.32339993 0.28972036 0.23670699 0.15869562 0.067065932 -0.011876557 -0.0592557 -0.078093663][0.23756348 0.32762143 0.39515343 0.43465066 0.46898276 0.50624877 0.52590472 0.51478523 0.46999973 0.393399 0.28012073 0.14905261 0.035723872 -0.035246361 -0.066748783][0.32837903 0.42966741 0.50221735 0.54552048 0.58939856 0.64457351 0.68396366 0.68367982 0.63336515 0.53594536 0.39105013 0.22577049 0.082554191 -0.0097501529 -0.053839173][0.41081184 0.51328546 0.58325553 0.62848026 0.681614 0.75394624 0.8147155 0.82991481 0.7792713 0.66562676 0.49589279 0.30349424 0.13489567 0.021525437 -0.037178453][0.47798714 0.57342893 0.63531369 0.68177021 0.74243557 0.8261897 0.9020915 0.93078876 0.88314396 0.76054311 0.57689124 0.36898786 0.18387006 0.053772785 -0.018833207][0.5104838 0.5906952 0.63865018 0.6814273 0.74257934 0.82730693 0.90723181 0.94318575 0.90071929 0.77978438 0.59836513 0.39290437 0.20717399 0.072286673 -0.0068571172][0.4942039 0.55413586 0.58404458 0.6163677 0.66868454 0.74271923 0.81527394 0.85238665 0.81915277 0.71232557 0.5504716 0.36670631 0.19800028 0.0724365 -0.0040820469][0.42374361 0.4649502 0.48002535 0.50128669 0.54016834 0.59611648 0.6524508 0.684261 0.66044182 0.57491779 0.44507283 0.29799756 0.16136418 0.057500582 -0.0076976628][0.31207106 0.3405644 0.34944227 0.3647469 0.39226002 0.43042171 0.4679504 0.48989508 0.47197849 0.40845302 0.3138544 0.20791902 0.10925188 0.032746743 -0.016741563][0.1825161 0.20034911 0.20530924 0.21408808 0.22928868 0.25058177 0.27106002 0.28357735 0.27093935 0.2294614 0.16932938 0.10356337 0.043204755 -0.0036802408 -0.034115165][0.060518619 0.068741947 0.069564968 0.070598386 0.073428847 0.080034517 0.087549537 0.093579836 0.0869644 0.065814957 0.03574476 0.0046779062 -0.02213167 -0.042030726 -0.05400458][-0.028964678 -0.027814351 -0.029876353 -0.033660203 -0.037944674 -0.039869204 -0.039566208 -0.037107974 -0.038785998 -0.045238491 -0.054050047 -0.0612141 -0.065287016 -0.0669431 -0.06640435][-0.082678825 -0.086812675 -0.090156309 -0.094315104 -0.098923229 -0.10210904 -0.10369599 -0.10316819 -0.10308869 -0.10261448 -0.10058516 -0.095644549 -0.088142037 -0.080345914 -0.073145442]]...]
INFO - root - 2017-12-11 01:04:36.019328: step 71310, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 56h:43m:35s remains)
INFO - root - 2017-12-11 01:04:43.895981: step 71320, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 57h:23m:52s remains)
INFO - root - 2017-12-11 01:04:51.661192: step 71330, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 55h:30m:23s remains)
INFO - root - 2017-12-11 01:04:59.378497: step 71340, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 56h:18m:06s remains)
INFO - root - 2017-12-11 01:05:07.225038: step 71350, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 56h:47m:10s remains)
INFO - root - 2017-12-11 01:05:15.119908: step 71360, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 56h:47m:42s remains)
INFO - root - 2017-12-11 01:05:22.990034: step 71370, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 57h:38m:25s remains)
INFO - root - 2017-12-11 01:05:30.982261: step 71380, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 56h:38m:40s remains)
INFO - root - 2017-12-11 01:05:38.828484: step 71390, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 58h:33m:17s remains)
INFO - root - 2017-12-11 01:05:46.495769: step 71400, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.746 sec/batch; 54h:08m:12s remains)
2017-12-11 01:05:47.407886: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.046335205 0.046599038 0.050546627 0.058916744 0.0702311 0.0819827 0.0920617 0.098609135 0.097195707 0.089445926 0.078635745 0.071462147 0.069293112 0.07089895 0.074033305][0.066197842 0.071981825 0.0809682 0.0917 0.10044275 0.10588622 0.10927077 0.11120946 0.10745966 0.09990076 0.091949455 0.08945474 0.091999672 0.096411444 0.10068522][0.085192688 0.095854051 0.10970177 0.12375827 0.13162188 0.13164362 0.12735318 0.12208821 0.11314377 0.10367607 0.0983136 0.10217527 0.11213582 0.12063228 0.12524132][0.1009395 0.11406289 0.12949286 0.14506948 0.15389697 0.15333524 0.14646485 0.1366926 0.12205939 0.10766964 0.10087351 0.10812652 0.12451535 0.13862926 0.14504689][0.10779583 0.12163834 0.13587336 0.15052557 0.16120823 0.16555068 0.16476505 0.15829603 0.14196588 0.12202222 0.10884123 0.11284433 0.1308649 0.1499126 0.15998656][0.10406002 0.11843128 0.13081279 0.14294724 0.15420899 0.16470934 0.17513068 0.1796757 0.16943729 0.1485907 0.1282378 0.12327766 0.13657223 0.15718287 0.17144558][0.093179628 0.10976898 0.12226717 0.13208435 0.1411854 0.15383084 0.17296058 0.19010623 0.19234593 0.17860676 0.15665549 0.14250246 0.14660533 0.16359846 0.17938471][0.083589815 0.10427022 0.11967607 0.12801442 0.13164926 0.13911699 0.15794715 0.18164477 0.1972834 0.19833602 0.18524553 0.16895847 0.16422445 0.1725129 0.18342873][0.079772674 0.10390981 0.12276449 0.13030353 0.12745595 0.12507366 0.13485856 0.15561612 0.17917578 0.19687958 0.20110206 0.19325197 0.18559077 0.18369237 0.18368196][0.082245417 0.10599849 0.12619878 0.13385914 0.1277505 0.11785106 0.11711577 0.1293136 0.15300858 0.18140195 0.20186411 0.20711216 0.20219372 0.19252057 0.18017536][0.09045139 0.10934836 0.12722255 0.13550003 0.13071057 0.11916023 0.11168421 0.11458489 0.13227829 0.16190229 0.19029978 0.20581825 0.20611702 0.19333826 0.17220996][0.095695861 0.10642592 0.11833501 0.12679881 0.12658823 0.11916499 0.11061084 0.1068308 0.11563044 0.13822064 0.16430338 0.18281434 0.18728636 0.17631097 0.15343565][0.087323554 0.088484392 0.093030006 0.10111181 0.10624705 0.10546632 0.099594109 0.092656136 0.093670279 0.1066158 0.1250339 0.1406682 0.14651936 0.13956666 0.12087354][0.060871776 0.054798007 0.053240538 0.059896953 0.068080075 0.072338596 0.070063584 0.063157029 0.059777394 0.064814322 0.075153083 0.08558885 0.090619795 0.087497078 0.075525083][0.027029814 0.018075395 0.01291038 0.016758772 0.024319062 0.0301323 0.030512609 0.025659809 0.021584371 0.022450132 0.027191237 0.032806505 0.035685152 0.034246802 0.027720574]]...]
INFO - root - 2017-12-11 01:05:55.280093: step 71410, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 56h:24m:05s remains)
INFO - root - 2017-12-11 01:06:02.903239: step 71420, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 57h:01m:48s remains)
INFO - root - 2017-12-11 01:06:10.766373: step 71430, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 57h:38m:02s remains)
INFO - root - 2017-12-11 01:06:18.628082: step 71440, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.773 sec/batch; 56h:03m:50s remains)
INFO - root - 2017-12-11 01:06:26.466624: step 71450, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 56h:25m:51s remains)
INFO - root - 2017-12-11 01:06:34.275651: step 71460, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 56h:59m:47s remains)
INFO - root - 2017-12-11 01:06:42.138817: step 71470, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 58h:41m:04s remains)
INFO - root - 2017-12-11 01:06:50.091231: step 71480, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.819 sec/batch; 59h:21m:39s remains)
INFO - root - 2017-12-11 01:06:57.865741: step 71490, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.801 sec/batch; 58h:06m:38s remains)
INFO - root - 2017-12-11 01:07:05.580797: step 71500, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 57h:27m:05s remains)
2017-12-11 01:07:06.485893: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23741522 0.28560987 0.29970866 0.2762447 0.23892359 0.21380131 0.20695817 0.20290327 0.19365782 0.18199794 0.16931111 0.15402815 0.13625588 0.12727302 0.14728408][0.24413411 0.2889668 0.31471932 0.31568405 0.30631226 0.30398244 0.31008017 0.30756307 0.29144102 0.27037928 0.24967909 0.22994418 0.21243936 0.20989436 0.24232666][0.23019557 0.26779795 0.30484927 0.3350113 0.3609122 0.38782558 0.41084948 0.41182393 0.38984266 0.3582626 0.32696155 0.30164042 0.28682944 0.29554683 0.34380376][0.2128036 0.24567196 0.29184988 0.34668034 0.40320808 0.45604056 0.49370223 0.49663934 0.46640891 0.41881192 0.36927855 0.33202282 0.31806916 0.34079915 0.40979975][0.20421441 0.23603977 0.2893208 0.36069813 0.4374609 0.50714713 0.55326724 0.55494773 0.51423466 0.4474633 0.37499806 0.32047594 0.30308694 0.33769917 0.42655352][0.21129924 0.24311917 0.29887289 0.37526378 0.45714831 0.52939284 0.57351106 0.56919116 0.51816863 0.43589067 0.34475222 0.27502903 0.25263089 0.29492995 0.39789706][0.25017041 0.28162453 0.33370987 0.40331966 0.47506583 0.53366858 0.56200087 0.54367447 0.48161033 0.3896409 0.28879046 0.21156992 0.18694773 0.234142 0.34504354][0.30811688 0.33654773 0.38255909 0.44074869 0.49510941 0.53053516 0.53354579 0.49450234 0.41950387 0.32182679 0.21972004 0.14382327 0.12179915 0.17261462 0.28611967][0.35576636 0.38210934 0.42452964 0.47444192 0.51314944 0.52504909 0.50084835 0.43822116 0.34808829 0.24578042 0.14852439 0.082047388 0.068192795 0.121921 0.23205467][0.36152914 0.38697603 0.42832053 0.47343588 0.50145167 0.49669716 0.45349202 0.37476587 0.27529773 0.17394093 0.088385254 0.037929911 0.035838198 0.09076906 0.19109513][0.30847913 0.33231139 0.3711116 0.41102147 0.43191171 0.41923246 0.3698788 0.28931254 0.19342108 0.10259369 0.034570094 0.0025818711 0.011602105 0.063865609 0.14817743][0.20311552 0.22203521 0.25573313 0.2890802 0.30522987 0.29193026 0.24751854 0.17777021 0.09743607 0.025748808 -0.021205049 -0.035590447 -0.018760255 0.026642714 0.09087988][0.062456261 0.073898844 0.10184705 0.12959443 0.14405885 0.13574466 0.10346912 0.052212603 -0.005933715 -0.054780442 -0.081378087 -0.082103185 -0.061627965 -0.025047688 0.019950062][-0.062564544 -0.057582036 -0.034424938 -0.010795731 0.0034154092 0.0014178012 -0.017394219 -0.049134281 -0.084836826 -0.11234376 -0.12253468 -0.11467192 -0.094263092 -0.066777781 -0.037486821][-0.13167411 -0.13045149 -0.11268479 -0.094401814 -0.082108781 -0.079918757 -0.087891556 -0.10325433 -0.1203515 -0.13150063 -0.13141987 -0.12073005 -0.10403723 -0.085709855 -0.0687029]]...]
INFO - root - 2017-12-11 01:07:14.388970: step 71510, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 58h:29m:41s remains)
INFO - root - 2017-12-11 01:07:22.185662: step 71520, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 58h:01m:21s remains)
INFO - root - 2017-12-11 01:07:29.974842: step 71530, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 56h:22m:27s remains)
INFO - root - 2017-12-11 01:07:37.784109: step 71540, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 57h:28m:21s remains)
INFO - root - 2017-12-11 01:07:45.690628: step 71550, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.789 sec/batch; 57h:12m:13s remains)
INFO - root - 2017-12-11 01:07:53.578291: step 71560, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.769 sec/batch; 55h:45m:26s remains)
INFO - root - 2017-12-11 01:08:01.468491: step 71570, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 57h:36m:55s remains)
INFO - root - 2017-12-11 01:08:08.899754: step 71580, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.755 sec/batch; 54h:42m:31s remains)
INFO - root - 2017-12-11 01:08:16.741819: step 71590, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 59h:00m:16s remains)
INFO - root - 2017-12-11 01:08:24.679484: step 71600, loss = 0.68, batch loss = 0.62 (9.8 examples/sec; 0.817 sec/batch; 59h:11m:16s remains)
2017-12-11 01:08:25.481161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.044800688 -0.032440852 -0.014386926 0.0041510994 0.016203545 0.020966148 0.019664658 0.012793845 0.0017529472 -0.010488646 -0.019163931 -0.021296239 -0.011122193 0.009823407 0.029077718][-0.027058056 -0.0032824518 0.030778483 0.067306854 0.095992818 0.11297758 0.11806329 0.10982787 0.089842305 0.064525567 0.042963911 0.031776633 0.044056002 0.075455233 0.10475905][-0.0015141144 0.039265294 0.0952673 0.15558507 0.20662549 0.24180724 0.25887579 0.25269148 0.22454968 0.18315886 0.14355162 0.11715859 0.12444181 0.15940122 0.1933827][0.02622091 0.085993126 0.16559465 0.25060844 0.32500082 0.38163152 0.41728792 0.42070666 0.39036295 0.33464828 0.27518365 0.22910063 0.22182432 0.24891035 0.27860323][0.05097279 0.12906116 0.23025829 0.33626568 0.42938611 0.5042612 0.55906934 0.57569736 0.54658604 0.47812966 0.39873159 0.33065331 0.3023155 0.31226236 0.33023635][0.069062367 0.16269228 0.28292048 0.40765402 0.51612467 0.6052044 0.67630464 0.70662844 0.68039811 0.60112417 0.50382143 0.4145259 0.36181527 0.34738693 0.34610775][0.074577928 0.17701975 0.31006986 0.44877872 0.56950843 0.67084885 0.7580418 0.80439603 0.78564471 0.70210403 0.59319979 0.48666245 0.41020876 0.3709437 0.34981337][0.071730062 0.17484359 0.31031659 0.45269215 0.57809293 0.68591869 0.78508377 0.84492385 0.8347283 0.753175 0.64098024 0.52472305 0.43128365 0.37578973 0.34429902][0.067595512 0.16525269 0.29338729 0.42821604 0.54780316 0.651863 0.75145864 0.81407011 0.80811316 0.732841 0.62629807 0.51179725 0.41586265 0.35882264 0.32963213][0.064214878 0.15314806 0.267336 0.38562566 0.48955002 0.5793035 0.66632909 0.7195254 0.71206665 0.64601719 0.55376309 0.45382503 0.37153184 0.32804042 0.31306991][0.053907529 0.13174012 0.22841233 0.32535738 0.40779921 0.47621489 0.54212165 0.57905585 0.56820363 0.51437896 0.44265854 0.3660304 0.30728 0.28566164 0.29031417][0.030252732 0.094651781 0.17229716 0.24653181 0.30616772 0.35171765 0.39434969 0.41381916 0.40006638 0.359529 0.30932996 0.25701767 0.2219597 0.22075102 0.24242638][0.00032239535 0.050223336 0.10860917 0.16084731 0.19903721 0.22378579 0.24486533 0.24885163 0.23317255 0.20421489 0.17188054 0.13963471 0.12307378 0.13528049 0.16612507][-0.03050039 0.0046513025 0.045465924 0.079533726 0.10123961 0.11081222 0.11569661 0.10872284 0.092464969 0.0725533 0.053799719 0.037279 0.034276109 0.053396206 0.085565291][-0.058083769 -0.036095314 -0.0088401586 0.013312933 0.025629178 0.027233791 0.023139231 0.010943679 -0.0042371959 -0.017534465 -0.026753709 -0.032134075 -0.026973348 -0.0068170093 0.020653948]]...]
INFO - root - 2017-12-11 01:08:33.319517: step 71610, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 58h:16m:04s remains)
INFO - root - 2017-12-11 01:08:41.238829: step 71620, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 56h:29m:31s remains)
INFO - root - 2017-12-11 01:08:49.128592: step 71630, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 56h:37m:49s remains)
INFO - root - 2017-12-11 01:08:56.946701: step 71640, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 55h:11m:47s remains)
INFO - root - 2017-12-11 01:09:04.716284: step 71650, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 55h:52m:36s remains)
INFO - root - 2017-12-11 01:09:12.513216: step 71660, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 55h:26m:21s remains)
INFO - root - 2017-12-11 01:09:20.360913: step 71670, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.758 sec/batch; 54h:54m:59s remains)
INFO - root - 2017-12-11 01:09:28.251436: step 71680, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 57h:44m:16s remains)
INFO - root - 2017-12-11 01:09:36.108008: step 71690, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 56h:06m:09s remains)
INFO - root - 2017-12-11 01:09:44.001282: step 71700, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 56h:58m:37s remains)
2017-12-11 01:09:44.904704: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13462679 0.12836228 0.11851048 0.10774213 0.098622523 0.10098945 0.11950147 0.13838838 0.14000654 0.12487067 0.10058592 0.065928593 0.020928569 -0.022954445 -0.053566791][0.26171574 0.25962421 0.24781129 0.23232533 0.21834785 0.22092105 0.24689493 0.27283069 0.27358705 0.24819423 0.2090705 0.15475623 0.085864276 0.017613487 -0.031689897][0.37859279 0.38292649 0.3714501 0.35337883 0.33614722 0.3389264 0.36867228 0.39555752 0.391238 0.35271218 0.29740828 0.22468708 0.13608442 0.04950171 -0.01369542][0.46558154 0.47701102 0.46653214 0.44641116 0.42731526 0.43059707 0.46190846 0.48731411 0.47792709 0.42953545 0.36163428 0.27436158 0.17030902 0.070392214 -0.0019514161][0.54234296 0.56277233 0.55293679 0.52901763 0.50620824 0.50888956 0.54314435 0.57254469 0.56573844 0.51502043 0.44004458 0.33970839 0.21761885 0.099704452 0.014069291][0.59862137 0.63034022 0.62243873 0.59493065 0.56618619 0.56491578 0.59917432 0.63268489 0.63033986 0.58050382 0.50277936 0.3928985 0.25552574 0.12159722 0.024967592][0.61395222 0.6583128 0.65633488 0.63053423 0.5989601 0.5927555 0.62228668 0.65540749 0.65480191 0.60545379 0.52662063 0.41223183 0.2673479 0.1263034 0.025849916][0.59023875 0.64272732 0.64586419 0.62334 0.59136879 0.58138841 0.60427666 0.63268316 0.63123721 0.58270425 0.50502968 0.39149949 0.24834427 0.11142361 0.016020188][0.52437454 0.57657778 0.58097804 0.56135613 0.53136778 0.51970434 0.53669423 0.56016731 0.55872917 0.51455933 0.44274464 0.33666778 0.2040512 0.08064115 -0.0024841768][0.41351491 0.45837787 0.46203148 0.44599453 0.4204745 0.4093132 0.42176732 0.44085374 0.44023436 0.40292373 0.34107485 0.24909578 0.13564213 0.033492588 -0.031851433][0.26235726 0.29587254 0.29911974 0.28826135 0.2693454 0.2594648 0.26599729 0.27714658 0.27350923 0.24178359 0.19239241 0.12195202 0.038488097 -0.032089956 -0.072167061][0.10907825 0.12878388 0.13092649 0.1255991 0.1151912 0.10941578 0.11251458 0.11672197 0.10990493 0.083383426 0.046566796 -0.00098151783 -0.052764338 -0.090945274 -0.10633378][-0.0055570183 0.00062462618 -6.7564011e-05 -0.0022197287 -0.0055667385 -0.0064425091 -0.0038096602 -0.0031887132 -0.010794017 -0.031030975 -0.055965219 -0.084086396 -0.11043751 -0.12438091 -0.12278025][-0.078559853 -0.081981428 -0.084977269 -0.085852861 -0.085217 -0.083303846 -0.080638744 -0.080802619 -0.086228393 -0.098489583 -0.11211505 -0.12512594 -0.13423641 -0.13381614 -0.12390412][-0.11370265 -0.12135315 -0.12467183 -0.12505199 -0.1232458 -0.12088091 -0.11866052 -0.11836054 -0.12075415 -0.12613364 -0.13149287 -0.13506602 -0.13492881 -0.1284944 -0.11672492]]...]
INFO - root - 2017-12-11 01:09:52.829999: step 71710, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 58h:37m:16s remains)
INFO - root - 2017-12-11 01:10:00.710987: step 71720, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 56h:28m:26s remains)
INFO - root - 2017-12-11 01:10:08.575002: step 71730, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.796 sec/batch; 57h:37m:24s remains)
INFO - root - 2017-12-11 01:10:16.373451: step 71740, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 56h:52m:48s remains)
INFO - root - 2017-12-11 01:10:24.310452: step 71750, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 58h:05m:46s remains)
INFO - root - 2017-12-11 01:10:32.068271: step 71760, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 56h:18m:50s remains)
INFO - root - 2017-12-11 01:10:39.933513: step 71770, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 57h:21m:19s remains)
INFO - root - 2017-12-11 01:10:47.792357: step 71780, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 56h:10m:37s remains)
INFO - root - 2017-12-11 01:10:55.625284: step 71790, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.773 sec/batch; 55h:59m:08s remains)
INFO - root - 2017-12-11 01:11:03.539965: step 71800, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 56h:56m:19s remains)
2017-12-11 01:11:04.399293: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.034233883 0.071595706 0.10320408 0.1193466 0.12130845 0.10559382 0.071186632 0.02860575 -0.014828566 -0.052946772 -0.082500555 -0.097655632 -0.096731924 -0.086085491 -0.068208657][0.11802223 0.1963802 0.26827064 0.31758595 0.34338135 0.33539942 0.28683582 0.21564396 0.13708167 0.063700296 0.0020926134 -0.037958924 -0.049018573 -0.036102992 -0.0016622925][0.23199335 0.36343196 0.48420194 0.5695259 0.615516 0.61056811 0.54468739 0.44611606 0.33821967 0.23832527 0.15477951 0.0955094 0.0721136 0.086889476 0.14220937][0.35490686 0.54026639 0.70864815 0.82328457 0.87736297 0.86745459 0.78751016 0.67567581 0.56127024 0.46204641 0.38103518 0.31508994 0.27731836 0.28658238 0.35717463][0.46223623 0.70024413 0.917305 1.0589892 1.1126148 1.0897344 0.9969334 0.884179 0.78680652 0.71520841 0.65846384 0.59509039 0.53881681 0.53139067 0.60164654][0.52507979 0.807449 1.0693606 1.2345625 1.2821803 1.2406366 1.1354055 1.0319968 0.97096372 0.94576442 0.92490619 0.86901861 0.792458 0.7592814 0.81091774][0.52678186 0.83386564 1.1249543 1.3040543 1.3433261 1.2852187 1.1713524 1.0814961 1.0574657 1.0742 1.0824244 1.0327362 0.93956566 0.88050956 0.90347475][0.47035575 0.77330309 1.0650532 1.2397048 1.2682523 1.2005924 1.0842294 1.0041069 1.000589 1.0389122 1.0622127 1.0189091 0.91923404 0.8406474 0.83059829][0.36342788 0.63134974 0.892054 1.0441456 1.0643268 0.99928766 0.88731468 0.80542916 0.794458 0.826247 0.84925455 0.81651556 0.72938895 0.650196 0.61803883][0.22970499 0.43863979 0.64323592 0.76091969 0.77723408 0.72571927 0.62571865 0.53880292 0.50561577 0.51383352 0.52742136 0.50994796 0.45233139 0.39207068 0.356511][0.10496332 0.24433415 0.37970698 0.45488563 0.46544749 0.4297953 0.34949529 0.2681891 0.22086161 0.20935051 0.21347485 0.209899 0.18454064 0.15206195 0.13048044][0.012132708 0.089530841 0.16152497 0.19584019 0.19614887 0.17105445 0.11319729 0.049466539 0.0050458531 -0.011935921 -0.012192607 -0.0081638955 -0.011022893 -0.018310936 -0.018783379][-0.043995351 -0.0080093658 0.021379298 0.027586054 0.017546117 -0.0039755669 -0.042917367 -0.084047407 -0.11183091 -0.11995233 -0.11670864 -0.10829607 -0.10003845 -0.092183433 -0.078456543][-0.072817609 -0.058950435 -0.05064185 -0.056590151 -0.070978612 -0.0905384 -0.11560192 -0.13696556 -0.14650999 -0.14269783 -0.13339393 -0.12140316 -0.10819888 -0.093669124 -0.075324595][-0.083722882 -0.0798797 -0.079049505 -0.086663291 -0.09933532 -0.11442599 -0.12847209 -0.13541929 -0.13231429 -0.1210649 -0.10780161 -0.093774468 -0.079802968 -0.066011518 -0.051460374]]...]
INFO - root - 2017-12-11 01:11:12.274421: step 71810, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.820 sec/batch; 59h:22m:09s remains)
INFO - root - 2017-12-11 01:11:19.935628: step 71820, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 55h:25m:33s remains)
INFO - root - 2017-12-11 01:11:27.750513: step 71830, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.771 sec/batch; 55h:49m:10s remains)
INFO - root - 2017-12-11 01:11:35.547793: step 71840, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 58h:10m:06s remains)
INFO - root - 2017-12-11 01:11:43.212264: step 71850, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 57h:59m:04s remains)
INFO - root - 2017-12-11 01:11:51.072294: step 71860, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 57h:16m:27s remains)
INFO - root - 2017-12-11 01:11:58.895437: step 71870, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 56h:12m:31s remains)
INFO - root - 2017-12-11 01:12:06.658837: step 71880, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 55h:23m:53s remains)
INFO - root - 2017-12-11 01:12:14.444956: step 71890, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 55h:24m:18s remains)
INFO - root - 2017-12-11 01:12:22.064523: step 71900, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 56h:08m:59s remains)
2017-12-11 01:12:22.944144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040980857 -0.039649013 -0.038094163 -0.038471557 -0.041311298 -0.044339634 -0.043686569 -0.04244443 -0.044048425 -0.049839802 -0.058460049 -0.0692381 -0.081443883 -0.090027444 -0.091680132][0.013292592 0.026371544 0.037004214 0.041589443 0.039430179 0.0352801 0.036380883 0.037895747 0.033106126 0.020264441 0.0031864212 -0.018047716 -0.044690561 -0.0689286 -0.081705026][0.095341906 0.12703662 0.15030664 0.1607677 0.15859614 0.15036266 0.14793654 0.1459776 0.13613303 0.11531443 0.088370569 0.055866923 0.013724858 -0.029024025 -0.057389632][0.19832893 0.25353536 0.28922549 0.3027156 0.29765224 0.28210256 0.27349 0.26743364 0.25289419 0.22321726 0.18345222 0.13650338 0.077509828 0.015916048 -0.029113069][0.30103847 0.38307577 0.43277198 0.45071992 0.44510883 0.42616639 0.41787034 0.41505909 0.40068948 0.36104646 0.30196613 0.23095906 0.14700831 0.062180959 -0.001701088][0.3685292 0.47527215 0.54189187 0.572805 0.58037704 0.57483679 0.58504266 0.59918928 0.59061909 0.53905547 0.45303276 0.346093 0.2251915 0.10959205 0.023526056][0.38347584 0.50475526 0.58670378 0.63644046 0.66952217 0.69454867 0.7394684 0.7809661 0.78150272 0.71842188 0.6038633 0.45780951 0.29667318 0.14936911 0.042260181][0.35777593 0.47990918 0.56805217 0.63185728 0.69034004 0.75031865 0.83437937 0.90850943 0.92442459 0.857369 0.7218107 0.54464674 0.3501105 0.17654444 0.052639086][0.29515389 0.40383163 0.48611587 0.55275261 0.624636 0.7078163 0.81920731 0.91962588 0.95411789 0.895343 0.75788844 0.57154214 0.36414564 0.17961508 0.049252369][0.20093383 0.28663945 0.35457557 0.41389373 0.48430964 0.57145083 0.68783373 0.79599023 0.84254116 0.80069965 0.68256974 0.51390088 0.32112232 0.14870209 0.028106187][0.088243626 0.14597502 0.19514439 0.24093233 0.29957289 0.37536374 0.47647938 0.57253557 0.61949515 0.59530616 0.50795537 0.37606978 0.22135177 0.082872592 -0.011514656][-0.015778726 0.013079594 0.040672757 0.0683822 0.10751238 0.16069022 0.23249786 0.30182418 0.33736333 0.32438451 0.2685312 0.1820365 0.08013396 -0.0091594094 -0.065532304][-0.08413177 -0.078100055 -0.069321372 -0.058773138 -0.039777055 -0.011122737 0.030021928 0.071165338 0.09249042 0.085445151 0.054932967 0.0089280009 -0.043507736 -0.086668372 -0.10871934][-0.11378589 -0.12059754 -0.12308235 -0.12342891 -0.11827336 -0.10734522 -0.088811956 -0.069041982 -0.059286412 -0.063633226 -0.0777238 -0.096588135 -0.11536072 -0.12739691 -0.12772216][-0.11571403 -0.12672536 -0.13335416 -0.13820179 -0.13978358 -0.13824342 -0.13264465 -0.12579957 -0.12331084 -0.12662877 -0.1320333 -0.13722582 -0.13866743 -0.13547389 -0.12649275]]...]
INFO - root - 2017-12-11 01:12:30.746395: step 71910, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 55h:37m:27s remains)
INFO - root - 2017-12-11 01:12:38.603336: step 71920, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 56h:21m:08s remains)
INFO - root - 2017-12-11 01:12:46.532722: step 71930, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 58h:31m:44s remains)
INFO - root - 2017-12-11 01:12:54.220107: step 71940, loss = 0.67, batch loss = 0.61 (10.1 examples/sec; 0.794 sec/batch; 57h:27m:22s remains)
INFO - root - 2017-12-11 01:13:02.078214: step 71950, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 57h:56m:33s remains)
INFO - root - 2017-12-11 01:13:09.959653: step 71960, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 56h:56m:26s remains)
INFO - root - 2017-12-11 01:13:17.756680: step 71970, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 56h:02m:19s remains)
INFO - root - 2017-12-11 01:13:25.388733: step 71980, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 57h:31m:31s remains)
INFO - root - 2017-12-11 01:13:33.220472: step 71990, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 56h:29m:33s remains)
INFO - root - 2017-12-11 01:13:41.053880: step 72000, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 56h:41m:17s remains)
2017-12-11 01:13:41.886890: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24813434 0.24375153 0.265894 0.29848897 0.30660653 0.27590078 0.22376993 0.15929744 0.093177065 0.049017373 0.047522724 0.091370128 0.16716996 0.25613126 0.33453539][0.26128834 0.25662366 0.27928317 0.31513408 0.33077106 0.30820051 0.26204646 0.2002755 0.1323798 0.083632991 0.0769818 0.11646196 0.18997209 0.27834603 0.3569414][0.26069832 0.25454035 0.27287158 0.30705798 0.33046913 0.3219054 0.28955454 0.23784424 0.17402229 0.1238372 0.11176861 0.1428367 0.20622803 0.28377023 0.35214412][0.26635864 0.25925535 0.27285078 0.30475616 0.33706018 0.34571603 0.33185077 0.29520932 0.23975456 0.18976085 0.16919383 0.18423548 0.22623585 0.28114966 0.32924867][0.27797472 0.27216965 0.28386134 0.31576949 0.35682344 0.38186842 0.38667062 0.36632976 0.32123402 0.27218226 0.24117991 0.23621882 0.2525782 0.28257689 0.31027243][0.28370368 0.28476033 0.30268225 0.34083855 0.39059713 0.42833519 0.44724816 0.43843129 0.40043831 0.35073984 0.30934754 0.28583875 0.28012466 0.29105514 0.30538777][0.29228529 0.303053 0.32868007 0.37229183 0.42528683 0.46893486 0.49562606 0.49331081 0.46059409 0.41063374 0.36138308 0.32314986 0.30063325 0.29866773 0.30573192][0.31330085 0.32825479 0.35169017 0.38943562 0.43368959 0.4721272 0.49760154 0.49748531 0.47189641 0.42787197 0.37950954 0.33622372 0.30574837 0.29692745 0.3001034][0.35926294 0.36908588 0.37492564 0.39039078 0.41144326 0.43278915 0.4492586 0.44924694 0.43387002 0.40267017 0.364992 0.32814258 0.29992396 0.29078647 0.29302827][0.41828057 0.41554245 0.39332992 0.37578991 0.36506242 0.36234009 0.36516726 0.36392304 0.35993135 0.34589264 0.32599398 0.30456218 0.28718075 0.28356692 0.28796408][0.45934132 0.44128025 0.39100656 0.34270751 0.30415 0.28030923 0.27107549 0.26946309 0.27711469 0.281452 0.28147766 0.27809894 0.27387485 0.27662882 0.28275105][0.4621619 0.4318212 0.36397102 0.29839486 0.24532099 0.20957671 0.1923819 0.1910342 0.20861869 0.22833145 0.24395803 0.25388157 0.2585173 0.26419544 0.270358][0.42354506 0.38839391 0.31845558 0.25404376 0.20435818 0.17014952 0.15121664 0.14893229 0.16948009 0.19516894 0.21648011 0.23152225 0.23924178 0.24535529 0.25183675][0.36441836 0.33139461 0.27261043 0.22363493 0.19073692 0.16907302 0.15471648 0.15133823 0.16828029 0.18960786 0.20595165 0.21750094 0.22271113 0.22723724 0.23334861][0.28956521 0.26313052 0.22285521 0.19732431 0.18955055 0.18890037 0.18576117 0.18347886 0.19296749 0.20239416 0.20638068 0.20905964 0.20977661 0.21357857 0.2207756]]...]
INFO - root - 2017-12-11 01:13:49.683181: step 72010, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 55h:47m:19s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 01:13:57.337968: step 72020, loss = 0.69, batch loss = 0.63 (12.8 examples/sec; 0.624 sec/batch; 45h:08m:10s remains)
INFO - root - 2017-12-11 01:14:05.169848: step 72030, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 57h:17m:25s remains)
INFO - root - 2017-12-11 01:14:13.161767: step 72040, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 57h:04m:09s remains)
INFO - root - 2017-12-11 01:14:20.966402: step 72050, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 55h:08m:52s remains)
INFO - root - 2017-12-11 01:14:28.719053: step 72060, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 56h:17m:02s remains)
INFO - root - 2017-12-11 01:14:36.592219: step 72070, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 56h:33m:23s remains)
INFO - root - 2017-12-11 01:14:44.488147: step 72080, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.818 sec/batch; 59h:11m:51s remains)
INFO - root - 2017-12-11 01:14:52.284805: step 72090, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.756 sec/batch; 54h:40m:04s remains)
INFO - root - 2017-12-11 01:15:00.095664: step 72100, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.808 sec/batch; 58h:28m:14s remains)
2017-12-11 01:15:00.979998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.01119648 0.017545862 0.050126247 0.075054511 0.088556141 0.092162132 0.085321866 0.073416151 0.071691915 0.079980262 0.089775026 0.093099922 0.091550827 0.079413094 0.045441356][0.013293107 0.055106066 0.10162223 0.14061701 0.16693231 0.17998259 0.17722175 0.16642451 0.16830149 0.17895859 0.18718641 0.18659739 0.1811868 0.16123948 0.10924905][0.04053659 0.096279785 0.15861543 0.21565221 0.26051974 0.28906372 0.29614717 0.29110023 0.29865688 0.31107572 0.31503457 0.30696335 0.29354757 0.26228788 0.1897261][0.058848377 0.12623996 0.20347624 0.28012985 0.34781936 0.39786062 0.42186531 0.42808786 0.44462436 0.4612377 0.46181661 0.4447448 0.41899678 0.37271518 0.27766067][0.070473485 0.14979762 0.24347319 0.34230536 0.43678269 0.51346278 0.55926841 0.57789057 0.6026668 0.62450463 0.62321079 0.59475911 0.55042821 0.48425776 0.36481014][0.086420767 0.18322171 0.29881707 0.42260888 0.54384947 0.64613 0.70976388 0.73220074 0.75644374 0.78056133 0.77841312 0.73794925 0.67252129 0.58561051 0.44375288][0.10771148 0.22675449 0.36825782 0.51689327 0.66133207 0.78414261 0.85758561 0.87205839 0.88360929 0.90301496 0.89769846 0.84496504 0.76014495 0.65699321 0.49962151][0.13015869 0.2723245 0.43815371 0.60558039 0.7633006 0.8950752 0.96530133 0.9594788 0.94844192 0.95712274 0.94738406 0.88544655 0.78791571 0.67719936 0.51477528][0.14761353 0.30514616 0.48365995 0.65411687 0.80596864 0.92720538 0.97916543 0.94642246 0.91081214 0.90753126 0.89537734 0.83198822 0.73374784 0.62733108 0.47344428][0.14866254 0.30596575 0.47924086 0.63485324 0.76317585 0.85813266 0.88378811 0.82732022 0.77249843 0.75850075 0.74546939 0.68845135 0.60173196 0.51076269 0.37831369][0.12453841 0.26397103 0.41463479 0.54241258 0.63803577 0.7002973 0.7004047 0.63019478 0.56523097 0.54270869 0.52935308 0.48424733 0.41776124 0.34975851 0.24826185][0.07296361 0.17777556 0.2901423 0.38025674 0.43982449 0.47064057 0.45314965 0.38379988 0.32213539 0.2978949 0.28715083 0.25718659 0.21408406 0.17092161 0.10340609][0.0094156973 0.072992682 0.14210865 0.19443263 0.22285287 0.23045561 0.20635493 0.15031619 0.10313814 0.084383965 0.078960039 0.063442014 0.040350966 0.017132016 -0.021590356][-0.045011729 -0.0178351 0.014336176 0.036708985 0.043905258 0.039156292 0.018376088 -0.017391082 -0.045101993 -0.054834452 -0.055169344 -0.060452223 -0.0703341 -0.081094213 -0.10006238][-0.080277823 -0.077603407 -0.069644876 -0.06532944 -0.0679666 -0.075227216 -0.08781027 -0.10487087 -0.11648311 -0.11896025 -0.1168175 -0.11735294 -0.12081936 -0.12538177 -0.13303614]]...]
INFO - root - 2017-12-11 01:15:08.294300: step 72110, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 55h:03m:16s remains)
INFO - root - 2017-12-11 01:15:16.044476: step 72120, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 56h:16m:19s remains)
INFO - root - 2017-12-11 01:15:23.864241: step 72130, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 57h:20m:32s remains)
INFO - root - 2017-12-11 01:15:31.562368: step 72140, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 56h:13m:10s remains)
INFO - root - 2017-12-11 01:15:39.432155: step 72150, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 57h:02m:43s remains)
INFO - root - 2017-12-11 01:15:47.173720: step 72160, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 55h:56m:09s remains)
INFO - root - 2017-12-11 01:15:55.007557: step 72170, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 56h:29m:26s remains)
INFO - root - 2017-12-11 01:16:02.872353: step 72180, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 57h:59m:04s remains)
INFO - root - 2017-12-11 01:16:10.739897: step 72190, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 57h:03m:17s remains)
INFO - root - 2017-12-11 01:16:18.381465: step 72200, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 58h:01m:47s remains)
2017-12-11 01:16:19.224555: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.027938066 -0.025812373 -0.022409471 -0.019195681 -0.017297158 -0.017155861 -0.018259689 -0.019747613 -0.021836888 -0.025053205 -0.028666485 -0.032390978 -0.035709 -0.037981112 -0.038797025][-0.017599296 -0.01127381 -0.0034311339 0.0042262385 0.0098721646 0.012990776 0.013978845 0.013335261 0.010095342 0.0035547176 -0.0050588497 -0.015067684 -0.024904257 -0.032726008 -0.037510186][-0.0015393486 0.011449088 0.026695905 0.041808881 0.054060388 0.062820017 0.067527957 0.067401834 0.061029591 0.048273683 0.031230928 0.01141772 -0.0079352912 -0.023511507 -0.033783071][0.018052455 0.03942997 0.064842224 0.08998055 0.11058556 0.12576675 0.13425031 0.13348906 0.12155078 0.099899575 0.072026454 0.040642612 0.010361857 -0.013960146 -0.030130252][0.037810944 0.068252079 0.10484465 0.14059159 0.16930482 0.19007343 0.20138952 0.19898432 0.18067843 0.15011142 0.11193231 0.069338858 0.028089559 -0.0049959566 -0.026919954][0.054835286 0.093725592 0.14050911 0.18549943 0.22085074 0.24594334 0.25943518 0.25581521 0.23223543 0.19420801 0.1472207 0.094709352 0.043473784 0.0025363236 -0.024125535][0.066613577 0.11164305 0.16599898 0.21758066 0.25813532 0.28703195 0.30263785 0.2985732 0.27146894 0.22816403 0.17423518 0.1137794 0.055058565 0.0087930374 -0.020675736][0.069957733 0.11745778 0.1748845 0.22874059 0.27124679 0.3021045 0.31888893 0.314651 0.28580105 0.2404985 0.18409111 0.12030831 0.058472764 0.010259087 -0.019668313][0.06527584 0.11080948 0.16587949 0.21658416 0.25635511 0.28587893 0.30222031 0.29825515 0.27011707 0.22705184 0.17372136 0.11275908 0.05349097 0.0073949285 -0.020639641][0.051735669 0.091083474 0.13855226 0.18129654 0.21458493 0.24037769 0.25511935 0.2518464 0.22687498 0.18963741 0.14397389 0.091012031 0.039414614 -0.00042237094 -0.024019193][0.031401072 0.061324663 0.0977721 0.13024689 0.15579922 0.17679898 0.18909657 0.1866276 0.16617277 0.1363281 0.10031813 0.058388051 0.018034678 -0.012346813 -0.029403413][0.0087281326 0.02819737 0.052965745 0.075201906 0.093181722 0.1090579 0.11831801 0.11644528 0.10103267 0.079164416 0.053432453 0.02348097 -0.0045351507 -0.024633413 -0.034794137][-0.011957155 -0.0018737575 0.012645901 0.025870251 0.036660697 0.047089465 0.053035732 0.051577967 0.041265272 0.027436575 0.011861134 -0.00660444 -0.02336332 -0.034441162 -0.038940139][-0.026579112 -0.023179417 -0.016118672 -0.0096612861 -0.0046463464 0.00076506048 0.0035925934 0.0022717624 -0.0037591723 -0.011000474 -0.018474951 -0.027685191 -0.035643719 -0.04005114 -0.04083607][-0.03596903 -0.036780704 -0.034517288 -0.032384239 -0.03104672 -0.029222975 -0.028633093 -0.029913219 -0.033040237 -0.036101688 -0.038524181 -0.041560054 -0.043557759 -0.043459635 -0.04181594]]...]
INFO - root - 2017-12-11 01:16:27.081597: step 72210, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 57h:45m:59s remains)
INFO - root - 2017-12-11 01:16:34.646621: step 72220, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 55h:37m:39s remains)
INFO - root - 2017-12-11 01:16:42.493671: step 72230, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 55h:56m:31s remains)
INFO - root - 2017-12-11 01:16:50.305263: step 72240, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 56h:29m:30s remains)
INFO - root - 2017-12-11 01:16:58.131577: step 72250, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 56h:24m:38s remains)
INFO - root - 2017-12-11 01:17:05.954150: step 72260, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 55h:09m:58s remains)
INFO - root - 2017-12-11 01:17:13.797644: step 72270, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 57h:22m:06s remains)
INFO - root - 2017-12-11 01:17:21.791679: step 72280, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 57h:02m:02s remains)
INFO - root - 2017-12-11 01:17:29.421754: step 72290, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 56h:21m:56s remains)
INFO - root - 2017-12-11 01:17:37.070948: step 72300, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 56h:48m:14s remains)
2017-12-11 01:17:37.849357: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14164175 0.14838006 0.14135037 0.12172032 0.092434116 0.059874676 0.033270106 0.015540722 0.0099138692 0.016481403 0.038411647 0.075071581 0.12168586 0.16922422 0.20363352][0.25289178 0.26999173 0.26765683 0.24626279 0.21001697 0.1682194 0.13146894 0.1011501 0.082513779 0.079643421 0.098433256 0.13556764 0.17961097 0.22095495 0.2483193][0.36063346 0.39431685 0.40285766 0.38366249 0.34259474 0.29331428 0.24785668 0.20544736 0.17377497 0.16058958 0.17360941 0.20773816 0.24558187 0.27994609 0.3021909][0.446868 0.49833158 0.52012676 0.50601262 0.46395138 0.41101256 0.35938126 0.30620608 0.26273853 0.23968992 0.24407475 0.2701731 0.29983196 0.33116767 0.35611755][0.50123656 0.5645808 0.59763646 0.59059912 0.55284041 0.50156742 0.44669181 0.3845489 0.33139816 0.30064082 0.29665521 0.31370282 0.33761284 0.37331623 0.40974602][0.50655293 0.57262284 0.61412841 0.61836123 0.592491 0.55012625 0.49780774 0.43402645 0.3803429 0.35122925 0.3472763 0.36141065 0.38470063 0.42638159 0.47184169][0.45732051 0.52071935 0.56711483 0.583745 0.57336867 0.54495621 0.50239849 0.44991624 0.41241297 0.40059847 0.40938309 0.42832258 0.45219111 0.4927454 0.53579909][0.36412376 0.41540706 0.45978403 0.48558515 0.49050936 0.47976956 0.45597094 0.42935151 0.42336458 0.44019821 0.46851492 0.49409744 0.51373941 0.54121429 0.56853908][0.24209945 0.27659413 0.31368843 0.34396428 0.36176381 0.37075996 0.37304521 0.38086775 0.41079655 0.45566329 0.50029141 0.52860963 0.53919441 0.54692215 0.55263513][0.12094863 0.13997664 0.16823079 0.1994243 0.2271008 0.25466076 0.28322256 0.32147229 0.37634614 0.43538281 0.48544192 0.51133919 0.51248211 0.50186539 0.48782724][0.030210719 0.039063539 0.059129849 0.088124946 0.12124635 0.16219263 0.20994303 0.26539919 0.32660645 0.38123432 0.42283177 0.43962282 0.43103579 0.40637785 0.37777516][-0.0070639728 -0.005601658 0.0056726076 0.027858926 0.058838639 0.10238893 0.15459287 0.20889477 0.2584739 0.294871 0.31857771 0.32211372 0.30504826 0.27376834 0.24038197][0.0050778561 0.00070008473 0.0036438638 0.016294133 0.038682774 0.074411325 0.11784981 0.15803684 0.18713835 0.20073368 0.20360588 0.19255343 0.16908161 0.138427 0.10942077][0.041338716 0.036582343 0.034794174 0.038994133 0.050866183 0.074259751 0.10363148 0.12711579 0.13760878 0.13390088 0.12174054 0.1008399 0.075197026 0.049553964 0.028409123][0.075712763 0.074681014 0.071417391 0.069078833 0.070795275 0.082306631 0.099882409 0.11279173 0.11513077 0.10738258 0.093284816 0.0727464 0.050878018 0.032049868 0.017670231]]...]
INFO - root - 2017-12-11 01:17:45.593536: step 72310, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 57h:05m:56s remains)
INFO - root - 2017-12-11 01:17:53.508833: step 72320, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 58h:35m:01s remains)
INFO - root - 2017-12-11 01:18:01.504164: step 72330, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 54h:54m:39s remains)
INFO - root - 2017-12-11 01:18:09.365464: step 72340, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 56h:35m:35s remains)
INFO - root - 2017-12-11 01:18:17.206258: step 72350, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 56h:32m:14s remains)
INFO - root - 2017-12-11 01:18:25.067463: step 72360, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 57h:06m:52s remains)
INFO - root - 2017-12-11 01:18:33.061138: step 72370, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 57h:34m:57s remains)
INFO - root - 2017-12-11 01:18:40.695662: step 72380, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 57h:48m:50s remains)
INFO - root - 2017-12-11 01:18:48.705399: step 72390, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 57h:07m:41s remains)
INFO - root - 2017-12-11 01:18:56.577607: step 72400, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 56h:22m:07s remains)
2017-12-11 01:18:57.418816: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.091961674 0.0941234 0.09822239 0.10607915 0.11837108 0.13350073 0.14406599 0.14283442 0.13256252 0.11805106 0.099671543 0.081274338 0.070776373 0.064858317 0.05199302][0.11244709 0.11651097 0.12188824 0.13080521 0.14590988 0.16586988 0.18196118 0.18468228 0.17596613 0.15971173 0.1349148 0.10590203 0.083640404 0.067156143 0.044740256][0.13139783 0.1416467 0.15101407 0.16066135 0.17536931 0.19503044 0.2121222 0.21677162 0.21097815 0.19682929 0.17108352 0.1371167 0.10685296 0.081333391 0.049383793][0.13935803 0.16303779 0.18329959 0.19759089 0.21091379 0.22460355 0.23578489 0.2383398 0.23660405 0.23050052 0.21196482 0.17993985 0.1452651 0.11128283 0.069203511][0.13738017 0.17738572 0.21291697 0.23578686 0.24899662 0.25472724 0.25616592 0.2539677 0.25613466 0.2608957 0.2544421 0.228477 0.19025412 0.14701225 0.095521115][0.137415 0.19147393 0.24131861 0.27459121 0.29005793 0.2889173 0.27929458 0.26882339 0.27000216 0.28141251 0.28561512 0.26682186 0.2268043 0.17592795 0.11774368][0.13922413 0.20149726 0.26046041 0.30247551 0.32171345 0.31732059 0.29960811 0.28021303 0.27441549 0.28352284 0.29290754 0.2827062 0.24685305 0.19497055 0.1354654][0.14936018 0.21729888 0.28175035 0.32905343 0.35025984 0.34363973 0.31963402 0.29039448 0.272089 0.26990443 0.27681324 0.27414706 0.24917488 0.20543972 0.15075442][0.17156038 0.24617513 0.31562251 0.36625832 0.38760653 0.37835878 0.34867561 0.30969337 0.27766663 0.26095316 0.26122668 0.26437739 0.25280836 0.22144297 0.17297208][0.19679466 0.27673915 0.34933046 0.4006055 0.41899928 0.40460378 0.36892971 0.32324478 0.28381115 0.25941178 0.25683779 0.26690143 0.26922685 0.25029525 0.20578694][0.22634135 0.30836773 0.38053066 0.4281747 0.44004515 0.41928011 0.38018256 0.33361122 0.29473934 0.27086732 0.26893646 0.28294593 0.29234144 0.27903625 0.23358081][0.26547053 0.34512246 0.41239461 0.45265245 0.45644629 0.43076456 0.39214271 0.34969375 0.3172898 0.30009779 0.3005932 0.31218171 0.3169972 0.29802874 0.24496801][0.30478337 0.37948623 0.44000587 0.47329286 0.47192019 0.44583705 0.41189033 0.3768113 0.35273668 0.34215647 0.34147742 0.34166348 0.33010215 0.29536331 0.23061572][0.32344067 0.38945112 0.44221953 0.4710483 0.46901253 0.44662619 0.41875753 0.39054742 0.37239796 0.36473089 0.35976827 0.34645867 0.31743696 0.266453 0.19171783][0.30342981 0.35693318 0.39931503 0.42323738 0.42067051 0.40142572 0.37691352 0.35193354 0.33606634 0.32926995 0.32279414 0.30438551 0.2699402 0.21428409 0.13845102]]...]
INFO - root - 2017-12-11 01:19:05.285170: step 72410, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 55h:07m:04s remains)
INFO - root - 2017-12-11 01:19:13.126028: step 72420, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.829 sec/batch; 59h:51m:36s remains)
INFO - root - 2017-12-11 01:19:20.954245: step 72430, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 56h:46m:26s remains)
INFO - root - 2017-12-11 01:19:28.884648: step 72440, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 58h:11m:07s remains)
INFO - root - 2017-12-11 01:19:36.786966: step 72450, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 56h:27m:43s remains)
INFO - root - 2017-12-11 01:19:44.406018: step 72460, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.768 sec/batch; 55h:27m:06s remains)
INFO - root - 2017-12-11 01:19:52.171551: step 72470, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 57h:14m:45s remains)
INFO - root - 2017-12-11 01:20:00.099271: step 72480, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 55h:32m:53s remains)
INFO - root - 2017-12-11 01:20:08.171087: step 72490, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 58h:08m:50s remains)
INFO - root - 2017-12-11 01:20:16.105473: step 72500, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 56h:13m:10s remains)
2017-12-11 01:20:16.907718: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11676881 0.15847047 0.17844661 0.16237988 0.116699 0.060421228 0.013423646 0.0076459488 0.056639425 0.14807521 0.2552073 0.33704451 0.36157307 0.31787294 0.22790679][0.13865629 0.17636737 0.19429033 0.18095303 0.14463954 0.10186432 0.0680318 0.069181912 0.1146039 0.19198307 0.27643794 0.33569905 0.3460494 0.29877546 0.21330777][0.17412241 0.20796718 0.22910185 0.2300763 0.21646293 0.19613819 0.17671205 0.17755826 0.2055565 0.25035128 0.29489478 0.32164645 0.31776756 0.27474549 0.20494334][0.22780386 0.25911495 0.28604591 0.30642751 0.31964558 0.32162875 0.31264773 0.30805945 0.31606454 0.32934523 0.33778915 0.33718184 0.32419074 0.29010585 0.23833732][0.29332656 0.32158124 0.35012543 0.38400942 0.41671231 0.43315297 0.42878026 0.41763163 0.41105166 0.40408969 0.39156082 0.37817305 0.36541042 0.34324369 0.30814][0.35856235 0.38146457 0.40425137 0.43957308 0.47792 0.49821472 0.49351582 0.47719014 0.46279293 0.44684672 0.42749745 0.41448769 0.40980098 0.40096065 0.38032126][0.41064495 0.423787 0.43311965 0.45842841 0.49023542 0.50692922 0.50113106 0.48490793 0.4704062 0.45375887 0.43586946 0.43037102 0.43649253 0.43878227 0.42811662][0.4323023 0.42906004 0.41945142 0.42798054 0.44686475 0.45800093 0.45469519 0.44675979 0.44046617 0.42914402 0.41571653 0.41865736 0.43503493 0.44555914 0.44055989][0.42092913 0.39706188 0.36808991 0.36063746 0.36852041 0.37687543 0.37957978 0.38314417 0.38641617 0.37969241 0.36791003 0.37527269 0.39970836 0.4180066 0.41889086][0.39199623 0.35026747 0.30692825 0.29057848 0.295821 0.3070353 0.3170127 0.32913247 0.33684778 0.32936895 0.31339058 0.31889954 0.34641281 0.36964002 0.37571096][0.3573758 0.3061398 0.25919792 0.24582878 0.25973615 0.27971569 0.29517686 0.30818543 0.31184584 0.29803407 0.27510133 0.27584115 0.30138284 0.32431221 0.33190176][0.32405964 0.27155757 0.23034886 0.22882043 0.2579008 0.28797236 0.3049182 0.31251428 0.30822983 0.28858468 0.26313394 0.26339322 0.2864348 0.30516979 0.30937657][0.29274791 0.24475126 0.21269761 0.22347006 0.26528195 0.30205506 0.31810763 0.32064757 0.31259876 0.29412788 0.27456644 0.28056887 0.30278817 0.31547233 0.31274271][0.26901898 0.22760791 0.20375654 0.220646 0.26551619 0.30133376 0.31454498 0.31595814 0.31140268 0.30201131 0.29489246 0.30986866 0.33187291 0.33791491 0.3276597][0.25195768 0.21766551 0.19807249 0.2121577 0.24943079 0.27766877 0.28790998 0.29316443 0.29810852 0.30173212 0.30695367 0.32751819 0.34739742 0.3481178 0.33388609]]...]
INFO - root - 2017-12-11 01:20:24.792443: step 72510, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 57h:25m:04s remains)
INFO - root - 2017-12-11 01:20:32.795916: step 72520, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 57h:56m:15s remains)
INFO - root - 2017-12-11 01:20:40.625837: step 72530, loss = 0.71, batch loss = 0.65 (10.9 examples/sec; 0.736 sec/batch; 53h:09m:34s remains)
INFO - root - 2017-12-11 01:20:48.256299: step 72540, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 56h:28m:44s remains)
INFO - root - 2017-12-11 01:20:56.138493: step 72550, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.785 sec/batch; 56h:41m:59s remains)
INFO - root - 2017-12-11 01:21:03.879833: step 72560, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 57h:32m:29s remains)
INFO - root - 2017-12-11 01:21:11.772674: step 72570, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 58h:13m:39s remains)
INFO - root - 2017-12-11 01:21:19.568487: step 72580, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 55h:01m:41s remains)
INFO - root - 2017-12-11 01:21:27.605941: step 72590, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 56h:22m:04s remains)
INFO - root - 2017-12-11 01:21:35.532211: step 72600, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 57h:11m:00s remains)
2017-12-11 01:21:36.515080: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.395123 0.38677633 0.36234263 0.34897774 0.35278559 0.36700252 0.37167054 0.3445152 0.28740904 0.22225118 0.17694862 0.16401139 0.18385771 0.23682754 0.31846669][0.45199871 0.44759774 0.4276112 0.41939706 0.4272579 0.44210258 0.44565934 0.41664955 0.355318 0.2829546 0.23149225 0.22019462 0.25076991 0.31802073 0.41101068][0.47899756 0.4871929 0.47706181 0.47319302 0.48082864 0.49257261 0.49476147 0.46861038 0.41272628 0.34554163 0.29839975 0.29450756 0.33488071 0.40722388 0.49292648][0.49597454 0.52306032 0.52208966 0.51552528 0.51577628 0.52231324 0.52650821 0.51150566 0.47353238 0.42618331 0.3942543 0.4000355 0.44158581 0.50087452 0.55463022][0.51671177 0.56341141 0.5680846 0.55204362 0.53926325 0.54004288 0.55017358 0.5532375 0.5421176 0.52360547 0.51163793 0.5227524 0.55076444 0.57680047 0.58057386][0.54408449 0.60520566 0.610354 0.58154893 0.55550331 0.55333251 0.57288945 0.59425193 0.60765982 0.61431605 0.61696905 0.62403208 0.6259889 0.60936552 0.56449258][0.56856889 0.63683277 0.64217579 0.60650861 0.57506329 0.57564133 0.60385919 0.63444412 0.65838289 0.67595726 0.68161 0.67582315 0.64826673 0.59566873 0.52191734][0.590301 0.66428858 0.67541045 0.64410722 0.6167497 0.6210013 0.64776826 0.66883832 0.68146896 0.690476 0.68703818 0.66632611 0.61976933 0.55487847 0.48390007][0.60411936 0.68216562 0.7020756 0.68186653 0.66241163 0.66663504 0.68120641 0.67904139 0.66571385 0.65297085 0.63446093 0.60326517 0.55418533 0.50169939 0.45890817][0.60573912 0.68385583 0.71083206 0.70167935 0.68852454 0.68788981 0.6852566 0.65712357 0.61574423 0.58006239 0.55030936 0.51995581 0.48618281 0.46405172 0.46137023][0.5922963 0.66257071 0.68855888 0.683632 0.67217439 0.6655429 0.64921731 0.60351133 0.54453528 0.49727261 0.46793565 0.45016512 0.44164661 0.4524287 0.48254663][0.53617018 0.58866078 0.60550553 0.59931177 0.587925 0.57897151 0.55886024 0.511214 0.45312312 0.41091281 0.39210236 0.38990021 0.4016102 0.43176904 0.47471771][0.43086082 0.45837474 0.46074069 0.45025125 0.43906918 0.43174791 0.41677535 0.38021559 0.33678424 0.30908197 0.30272257 0.311248 0.33227187 0.36700413 0.40726379][0.27523965 0.27770355 0.26619619 0.2518557 0.24211819 0.2396806 0.23625971 0.21926016 0.1974508 0.18600383 0.18816039 0.1994604 0.21856351 0.2450553 0.2713035][0.1006842 0.087242804 0.068820335 0.053110648 0.044614688 0.045941338 0.052130915 0.052071888 0.048541803 0.049682345 0.056384202 0.065888248 0.077541091 0.090870261 0.10158348]]...]
INFO - root - 2017-12-11 01:21:44.241075: step 72610, loss = 0.71, batch loss = 0.65 (13.3 examples/sec; 0.601 sec/batch; 43h:21m:51s remains)
INFO - root - 2017-12-11 01:21:52.110980: step 72620, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.796 sec/batch; 57h:27m:32s remains)
INFO - root - 2017-12-11 01:22:00.085124: step 72630, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.758 sec/batch; 54h:42m:56s remains)
INFO - root - 2017-12-11 01:22:07.959616: step 72640, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 57h:40m:30s remains)
INFO - root - 2017-12-11 01:22:15.644207: step 72650, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 57h:50m:47s remains)
INFO - root - 2017-12-11 01:22:23.473591: step 72660, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 58h:09m:18s remains)
INFO - root - 2017-12-11 01:22:31.374495: step 72670, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 56h:45m:30s remains)
INFO - root - 2017-12-11 01:22:39.277357: step 72680, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 57h:12m:46s remains)
INFO - root - 2017-12-11 01:22:47.189580: step 72690, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 55h:22m:24s remains)
INFO - root - 2017-12-11 01:22:54.938014: step 72700, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.774 sec/batch; 55h:49m:42s remains)
2017-12-11 01:22:55.753163: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26769918 0.25104132 0.23894095 0.23440963 0.238698 0.2583172 0.29105294 0.32249311 0.33286837 0.31731704 0.279945 0.22540148 0.16262482 0.10969638 0.084978476][0.24707031 0.22891578 0.21426363 0.20824793 0.21293603 0.23179002 0.26094538 0.2887471 0.29854247 0.28552949 0.25334454 0.20648237 0.15360816 0.11082204 0.093212262][0.20486178 0.18703279 0.17186859 0.16532758 0.16996016 0.18614361 0.20969611 0.23188658 0.24027582 0.23148857 0.20861891 0.17557499 0.13911514 0.11214767 0.10539893][0.17378666 0.15784815 0.14378627 0.13781486 0.1425384 0.15629874 0.17536598 0.19237024 0.19800572 0.19099326 0.17506242 0.15349361 0.13066943 0.11686275 0.11902054][0.17392109 0.16061307 0.14867079 0.14427166 0.1493458 0.16134416 0.17707808 0.18872538 0.18883321 0.17868033 0.16343598 0.14640231 0.13032071 0.123861 0.13127628][0.20918442 0.19746186 0.186929 0.18360367 0.18800731 0.197676 0.20982628 0.21531782 0.20771553 0.19042261 0.16994701 0.15068667 0.13502745 0.1304884 0.13946716][0.26347655 0.25183788 0.23877048 0.23214534 0.23163642 0.23583063 0.24252607 0.24148689 0.22686645 0.20277531 0.1761158 0.15257937 0.1350465 0.12980929 0.13823445][0.31880623 0.30641836 0.2868863 0.27172273 0.26095432 0.25528753 0.25357011 0.24544896 0.22553337 0.197408 0.16752124 0.142404 0.12506451 0.1200423 0.12826784][0.35612315 0.3435123 0.31599298 0.28972512 0.26595473 0.24821526 0.23670827 0.22204518 0.19952302 0.17133674 0.14311402 0.12134395 0.10818302 0.10584801 0.11535515][0.36268449 0.3491416 0.31481087 0.27821645 0.24250044 0.21414363 0.19461951 0.17631254 0.15544109 0.13275528 0.11217363 0.098833136 0.093510635 0.095927343 0.10737179][0.3342118 0.32006851 0.28234315 0.24002051 0.1981402 0.16499886 0.14258577 0.12523715 0.10998383 0.096346453 0.086454816 0.083470151 0.086448096 0.093044318 0.10477354][0.28089407 0.26612025 0.22892241 0.18723977 0.14741181 0.11775889 0.099605687 0.08831159 0.081021689 0.076635182 0.076112188 0.0808901 0.088503756 0.0956123 0.10432349][0.21844742 0.20306395 0.16972382 0.13471705 0.10453752 0.085894629 0.078224413 0.076777518 0.078166157 0.081091493 0.085987754 0.093572259 0.10066243 0.10403852 0.10685004][0.15971029 0.14463064 0.11744041 0.092765 0.076317877 0.072310567 0.077601686 0.0866625 0.095569357 0.10304517 0.10949031 0.1156682 0.11862417 0.11638564 0.11342955][0.11507566 0.10174215 0.081412435 0.067027591 0.063195072 0.071966037 0.08817789 0.1055854 0.11998121 0.13011105 0.13643436 0.139883 0.13841245 0.13238522 0.12727334]]...]
INFO - root - 2017-12-11 01:23:03.662091: step 72710, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 55h:50m:34s remains)
INFO - root - 2017-12-11 01:23:11.424604: step 72720, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.749 sec/batch; 54h:03m:16s remains)
INFO - root - 2017-12-11 01:23:19.216077: step 72730, loss = 0.70, batch loss = 0.64 (12.1 examples/sec; 0.663 sec/batch; 47h:50m:08s remains)
INFO - root - 2017-12-11 01:23:27.108837: step 72740, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 56h:49m:50s remains)
INFO - root - 2017-12-11 01:23:35.025498: step 72750, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 57h:11m:18s remains)
INFO - root - 2017-12-11 01:23:42.828999: step 72760, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 54h:59m:42s remains)
INFO - root - 2017-12-11 01:23:50.473671: step 72770, loss = 0.68, batch loss = 0.62 (11.8 examples/sec; 0.676 sec/batch; 48h:48m:13s remains)
INFO - root - 2017-12-11 01:23:58.406051: step 72780, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.812 sec/batch; 58h:35m:42s remains)
INFO - root - 2017-12-11 01:24:06.232231: step 72790, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.788 sec/batch; 56h:52m:57s remains)
INFO - root - 2017-12-11 01:24:14.149532: step 72800, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 55h:25m:56s remains)
2017-12-11 01:24:14.962725: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25973007 0.27529949 0.26786682 0.23728441 0.19495359 0.15312135 0.11463909 0.0805305 0.053884875 0.047451805 0.060671642 0.082115389 0.097685851 0.10116161 0.10197853][0.27533621 0.29146123 0.28283778 0.24683367 0.19602409 0.14472418 0.10038613 0.065786473 0.04299853 0.040976249 0.0560045 0.077014826 0.090825185 0.090677641 0.085884653][0.27653858 0.29074845 0.2804381 0.24113837 0.18771853 0.13454252 0.092438959 0.065989733 0.054121066 0.060013462 0.0759141 0.093698993 0.10255761 0.096313268 0.083363116][0.27190968 0.28003463 0.2663641 0.22611466 0.17634128 0.12929785 0.096738055 0.082886927 0.0844372 0.099493936 0.11561318 0.12767193 0.12900336 0.11578862 0.094494827][0.26764277 0.26666334 0.24761786 0.20887199 0.1697223 0.13776802 0.12192731 0.12236702 0.1350206 0.15531264 0.16807908 0.17230792 0.165581 0.14594287 0.11743429][0.2614992 0.25165585 0.22648628 0.190473 0.16529776 0.15301561 0.15823108 0.17449597 0.19571467 0.21583915 0.22048546 0.2146627 0.19982392 0.17466477 0.14029269][0.26014671 0.24364136 0.21358827 0.18022333 0.16595417 0.16900931 0.19134657 0.22007115 0.24485472 0.25885671 0.25175861 0.23538688 0.21444121 0.18690732 0.15089613][0.26134661 0.24236232 0.21198852 0.18212937 0.17218716 0.17902637 0.20563945 0.2360291 0.25640729 0.260383 0.24307136 0.22100441 0.20080842 0.1788352 0.14879063][0.25549316 0.23882975 0.21422181 0.1903652 0.17947376 0.17856914 0.19539121 0.2164443 0.22760992 0.22395647 0.20401371 0.18521193 0.17429234 0.16509196 0.14622138][0.2431833 0.23199706 0.21574469 0.1980083 0.18308893 0.1692218 0.16974802 0.17718989 0.18022537 0.17502721 0.16096808 0.15354161 0.15729392 0.16283093 0.15494406][0.23064271 0.22452426 0.21275395 0.19668666 0.17587397 0.15044633 0.137059 0.13468465 0.13551478 0.13534589 0.13230473 0.1385504 0.15588205 0.17220443 0.17081189][0.22369055 0.21849062 0.20497194 0.18547277 0.15839061 0.12625645 0.10670678 0.10254763 0.10802179 0.11689733 0.1245162 0.14008929 0.1642224 0.18394187 0.18335864][0.21906458 0.20908767 0.18862343 0.16309302 0.13193442 0.099537514 0.081988469 0.082293555 0.09466242 0.11037858 0.12243786 0.13917561 0.16211441 0.17952822 0.17737523][0.21253696 0.19496012 0.16711155 0.1371648 0.10574709 0.077440038 0.064765304 0.069301121 0.085101977 0.10152888 0.1107527 0.12180048 0.13794172 0.14994344 0.14638653][0.20401922 0.17959179 0.14773673 0.11781957 0.090152271 0.067389041 0.057416454 0.061073255 0.074501581 0.087008916 0.090487562 0.093587831 0.10071456 0.10609996 0.10191823]]...]
INFO - root - 2017-12-11 01:24:22.917660: step 72810, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 56h:55m:54s remains)
INFO - root - 2017-12-11 01:24:30.527517: step 72820, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 57h:22m:46s remains)
INFO - root - 2017-12-11 01:24:38.350376: step 72830, loss = 0.70, batch loss = 0.65 (10.6 examples/sec; 0.755 sec/batch; 54h:26m:48s remains)
INFO - root - 2017-12-11 01:24:46.177845: step 72840, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 55h:46m:54s remains)
INFO - root - 2017-12-11 01:24:53.833451: step 72850, loss = 0.70, batch loss = 0.64 (12.3 examples/sec; 0.648 sec/batch; 46h:45m:43s remains)
INFO - root - 2017-12-11 01:25:01.685531: step 72860, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 57h:49m:25s remains)
INFO - root - 2017-12-11 01:25:09.605653: step 72870, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 55h:00m:20s remains)
INFO - root - 2017-12-11 01:25:17.471860: step 72880, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 56h:26m:37s remains)
INFO - root - 2017-12-11 01:25:25.330716: step 72890, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 56h:15m:29s remains)
INFO - root - 2017-12-11 01:25:33.177433: step 72900, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 57h:34m:29s remains)
2017-12-11 01:25:33.961256: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18205388 0.16309828 0.15187828 0.1625374 0.19486833 0.23510249 0.27365863 0.30422458 0.3198224 0.31450495 0.29276291 0.26900935 0.24577029 0.2198651 0.19078389][0.19572195 0.18314219 0.17554863 0.18671902 0.21775055 0.25470474 0.28891146 0.31521806 0.32614857 0.31594068 0.28854674 0.25729057 0.22570235 0.19183493 0.15725917][0.20271119 0.19624729 0.19080673 0.19940761 0.22378622 0.25290358 0.28078035 0.30172244 0.30901837 0.2966789 0.2677424 0.23219462 0.19442596 0.15534729 0.11649503][0.21737525 0.21361317 0.20858444 0.21294588 0.22849417 0.24883461 0.27181238 0.29057127 0.29776442 0.28650314 0.25812897 0.21932176 0.17514142 0.12909809 0.083652973][0.22653133 0.22623219 0.22499388 0.22944972 0.23951709 0.25333619 0.27288765 0.29146719 0.30093452 0.29279044 0.2664015 0.22511396 0.1742323 0.11932974 0.065553658][0.22464137 0.22992234 0.23559776 0.2436703 0.25119179 0.26016858 0.27630389 0.2939682 0.30509964 0.30043048 0.27804151 0.23718821 0.18226616 0.12015063 0.059689384][0.22503199 0.237455 0.24959059 0.26031539 0.26445162 0.26702011 0.27642104 0.28865564 0.29688662 0.29245722 0.27320245 0.23440705 0.17855777 0.11373779 0.051913854][0.24448071 0.26286811 0.27697307 0.28509644 0.28214455 0.27612194 0.27691865 0.28164643 0.28456718 0.27749383 0.25792494 0.2188527 0.16222605 0.097892828 0.038808588][0.2763429 0.29721621 0.30887526 0.31125781 0.30093828 0.28685334 0.27900451 0.27641293 0.27406627 0.26384965 0.24199097 0.20114751 0.14370689 0.080945529 0.025695549][0.29692462 0.31398708 0.3189958 0.31544435 0.30131435 0.28287524 0.26834452 0.25927418 0.25287658 0.24101815 0.21804872 0.176844 0.12104972 0.062554561 0.012765236][0.29730123 0.30510414 0.29954597 0.2895962 0.27484939 0.25738594 0.24231611 0.23243606 0.22648527 0.21528791 0.19170786 0.15036222 0.097109571 0.043993518 0.00031858066][0.28614387 0.2817032 0.26397139 0.24895717 0.23825987 0.22891496 0.22153346 0.21720253 0.21368587 0.20069019 0.17250785 0.1279483 0.075703263 0.027384164 -0.01024199][0.26625633 0.24930836 0.22201219 0.20665126 0.20507629 0.20848811 0.21288487 0.21563467 0.21248955 0.19380917 0.15785398 0.10870365 0.057395365 0.014121842 -0.017470811][0.23960266 0.21687962 0.18829419 0.17831522 0.18647757 0.19917296 0.20954672 0.21280296 0.20451078 0.17792472 0.13547964 0.085293218 0.038674936 0.0027412034 -0.022241686][0.21737567 0.19639733 0.17181824 0.16724008 0.17917912 0.19222622 0.19897814 0.1952026 0.17831403 0.14461482 0.099543706 0.05323261 0.015427938 -0.01074315 -0.027981859]]...]
INFO - root - 2017-12-11 01:25:41.575456: step 72910, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 54h:59m:26s remains)
INFO - root - 2017-12-11 01:25:49.371527: step 72920, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.761 sec/batch; 54h:51m:15s remains)
INFO - root - 2017-12-11 01:25:57.011136: step 72930, loss = 0.71, batch loss = 0.65 (11.0 examples/sec; 0.730 sec/batch; 52h:38m:08s remains)
INFO - root - 2017-12-11 01:26:04.843332: step 72940, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 56h:48m:58s remains)
INFO - root - 2017-12-11 01:26:12.762683: step 72950, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 56h:41m:47s remains)
INFO - root - 2017-12-11 01:26:20.598056: step 72960, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 56h:41m:58s remains)
INFO - root - 2017-12-11 01:26:28.405427: step 72970, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 55h:34m:16s remains)
INFO - root - 2017-12-11 01:26:36.255358: step 72980, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.766 sec/batch; 55h:15m:10s remains)
INFO - root - 2017-12-11 01:26:43.994861: step 72990, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 56h:10m:10s remains)
INFO - root - 2017-12-11 01:26:51.622382: step 73000, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 58h:02m:37s remains)
2017-12-11 01:26:52.522677: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20790124 0.19815779 0.20112413 0.21644898 0.23618254 0.25202483 0.25649178 0.25367984 0.24344264 0.22016934 0.19223703 0.19681586 0.24887553 0.3122116 0.34257686][0.21042423 0.21266979 0.2256214 0.24668683 0.26942238 0.28861263 0.29776028 0.29846814 0.2850827 0.25521755 0.22006606 0.21748616 0.26383242 0.32344377 0.35444215][0.19065122 0.2080102 0.23301014 0.26119235 0.28781229 0.31032467 0.32348588 0.32577461 0.30790877 0.27117071 0.22897971 0.21617588 0.24847862 0.29390284 0.31822863][0.16250321 0.19483423 0.2314048 0.26718453 0.29904038 0.32566279 0.34279698 0.34662125 0.32656306 0.28522113 0.23677891 0.21079801 0.221634 0.24391939 0.25349745][0.13722132 0.18196711 0.22810686 0.27038595 0.30729097 0.338698 0.36198294 0.37067795 0.35319483 0.31097835 0.25817031 0.21820973 0.202503 0.19465603 0.18277276][0.12351298 0.1766101 0.22855388 0.27433842 0.31379059 0.34934169 0.38064271 0.397131 0.38582054 0.34715226 0.29448885 0.24385343 0.20220256 0.163395 0.12818082][0.12465456 0.1818271 0.23511961 0.28032365 0.31853047 0.35572541 0.39314228 0.41620612 0.4120079 0.38143331 0.33546108 0.28158426 0.22225526 0.15945518 0.10376691][0.13779856 0.19637775 0.24690516 0.28696552 0.31977847 0.35392424 0.39053088 0.41393307 0.41550744 0.3974025 0.36483303 0.31652206 0.25189325 0.17780499 0.10951111][0.15562595 0.21443909 0.26015481 0.29385614 0.321129 0.34940413 0.37812787 0.395202 0.40048742 0.3974202 0.38165003 0.34321252 0.28087583 0.20665638 0.13584477][0.17614524 0.23423983 0.27539417 0.30610415 0.33322036 0.357487 0.37466407 0.37943947 0.38346866 0.39245954 0.39106944 0.361839 0.30434188 0.23731236 0.17329775][0.19597708 0.25133437 0.28874922 0.32289609 0.35865414 0.38425571 0.39097267 0.38100171 0.37795645 0.39111885 0.39733979 0.374762 0.32254818 0.26506227 0.21145947][0.20976427 0.25919935 0.29296514 0.33403462 0.3831234 0.41490924 0.41717049 0.39685982 0.38542008 0.39564189 0.40275887 0.38517326 0.33935085 0.29075807 0.24536055][0.20706743 0.24761978 0.27589869 0.32024506 0.37797043 0.41600102 0.42155495 0.40153128 0.38708016 0.39156917 0.39443061 0.37971064 0.3410123 0.30056185 0.26170248][0.18044245 0.21005738 0.23042168 0.27006704 0.32533234 0.36466181 0.37795028 0.3684606 0.35912076 0.35931212 0.35559452 0.34106508 0.3093968 0.27753928 0.24623007][0.13091075 0.14924653 0.16081938 0.18884119 0.23080282 0.26384279 0.28239095 0.28530586 0.28446591 0.28314045 0.27484736 0.26070851 0.23676948 0.21447706 0.192383]]...]
INFO - root - 2017-12-11 01:27:00.214526: step 73010, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 56h:03m:54s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 01:27:08.036385: step 73020, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 56h:21m:19s remains)
INFO - root - 2017-12-11 01:27:15.846489: step 73030, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 57h:18m:55s remains)
INFO - root - 2017-12-11 01:27:23.640820: step 73040, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 55h:21m:56s remains)
INFO - root - 2017-12-11 01:27:31.440933: step 73050, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 57h:24m:59s remains)
INFO - root - 2017-12-11 01:27:39.169827: step 73060, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 55h:55m:55s remains)
INFO - root - 2017-12-11 01:27:47.023275: step 73070, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 56h:13m:43s remains)
INFO - root - 2017-12-11 01:27:54.841883: step 73080, loss = 0.68, batch loss = 0.62 (10.8 examples/sec; 0.744 sec/batch; 53h:36m:43s remains)
INFO - root - 2017-12-11 01:28:02.425656: step 73090, loss = 0.70, batch loss = 0.64 (12.0 examples/sec; 0.666 sec/batch; 48h:00m:41s remains)
INFO - root - 2017-12-11 01:28:10.424355: step 73100, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 57h:16m:56s remains)
2017-12-11 01:28:11.392733: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12656666 0.14505395 0.15807879 0.17556143 0.19059509 0.19224729 0.18251413 0.18096092 0.19747344 0.22826743 0.2602604 0.27404505 0.26918691 0.25063136 0.25181559][0.17291065 0.20208894 0.2242869 0.25142714 0.27399519 0.27476978 0.25410354 0.2381168 0.24422757 0.27042782 0.30149454 0.31641805 0.31662971 0.3024978 0.300852][0.23071072 0.27066419 0.30040967 0.3370176 0.3680712 0.36955926 0.34016865 0.31039327 0.30316091 0.31848702 0.34104496 0.35281408 0.3578423 0.35186389 0.35248554][0.29864684 0.35028553 0.38615465 0.42853126 0.46315047 0.46420845 0.43077168 0.39473343 0.37950313 0.38415426 0.39621577 0.40393332 0.41293526 0.41497794 0.42026994][0.36003992 0.42191118 0.46194977 0.50407267 0.53346294 0.52995664 0.49564004 0.4611811 0.44584659 0.4449 0.44941068 0.45531583 0.46821007 0.4765971 0.48713565][0.39386573 0.46192831 0.50428158 0.54129273 0.55841935 0.54641247 0.513041 0.48585969 0.47772959 0.47937948 0.48410365 0.49368593 0.51091868 0.52254242 0.53561461][0.37765664 0.44389781 0.48487055 0.51393229 0.51736641 0.49767819 0.4678016 0.45058981 0.45308971 0.46367225 0.475964 0.49322623 0.51476574 0.52741939 0.5404911][0.31533855 0.37116322 0.40517139 0.42322677 0.41375589 0.38827944 0.36275491 0.35471994 0.36656952 0.38727754 0.4106777 0.43742555 0.46325472 0.47639486 0.48844773][0.22786839 0.2671594 0.28790569 0.29145274 0.27024174 0.24051084 0.21875946 0.21780705 0.2364752 0.26537853 0.29886177 0.33344039 0.36209345 0.37457627 0.38232443][0.12966837 0.15050913 0.15608056 0.14638707 0.11827465 0.08769805 0.069274053 0.071475133 0.091096513 0.12105771 0.1568538 0.1935637 0.22311431 0.23632093 0.24018656][0.039481919 0.045717917 0.041242246 0.025656873 -0.0012773887 -0.02752039 -0.042317983 -0.04087561 -0.026604231 -0.004279987 0.023513068 0.053264637 0.077726521 0.089212269 0.089164995][-0.027406247 -0.029209075 -0.036293507 -0.050213996 -0.070282675 -0.089228816 -0.1005763 -0.1019192 -0.095941395 -0.085451007 -0.071051672 -0.053994525 -0.038986292 -0.031280346 -0.03301033][-0.067447171 -0.072715156 -0.078151949 -0.08690384 -0.0986592 -0.10994004 -0.11757032 -0.12071253 -0.12118674 -0.12018615 -0.11687913 -0.11115333 -0.10508969 -0.10131191 -0.10252558][-0.081117079 -0.087524012 -0.090958253 -0.095292486 -0.10064145 -0.10580701 -0.10959115 -0.11184815 -0.11364464 -0.11546452 -0.11666064 -0.11709075 -0.11728484 -0.11798956 -0.12060981][-0.078498177 -0.084939562 -0.087277539 -0.089498006 -0.091645673 -0.093386419 -0.094504595 -0.095229134 -0.096133932 -0.097527549 -0.099279329 -0.10130128 -0.10339986 -0.10544577 -0.10772873]]...]
INFO - root - 2017-12-11 01:28:19.136366: step 73110, loss = 0.69, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 55h:15m:05s remains)
INFO - root - 2017-12-11 01:28:26.940801: step 73120, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.796 sec/batch; 57h:21m:34s remains)
INFO - root - 2017-12-11 01:28:34.786219: step 73130, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.810 sec/batch; 58h:22m:49s remains)
INFO - root - 2017-12-11 01:28:42.667391: step 73140, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 56h:33m:39s remains)
INFO - root - 2017-12-11 01:28:50.577220: step 73150, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 55h:36m:23s remains)
INFO - root - 2017-12-11 01:28:58.380315: step 73160, loss = 0.68, batch loss = 0.63 (10.6 examples/sec; 0.753 sec/batch; 54h:16m:47s remains)
INFO - root - 2017-12-11 01:29:06.104541: step 73170, loss = 0.70, batch loss = 0.64 (12.2 examples/sec; 0.655 sec/batch; 47h:10m:30s remains)
INFO - root - 2017-12-11 01:29:13.787333: step 73180, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.822 sec/batch; 59h:13m:41s remains)
INFO - root - 2017-12-11 01:29:21.600502: step 73190, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 56h:41m:20s remains)
INFO - root - 2017-12-11 01:29:29.464166: step 73200, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 55h:19m:41s remains)
2017-12-11 01:29:30.353480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054055236 -0.055334006 -0.054556582 -0.053080481 -0.052135721 -0.052754175 -0.054799642 -0.05735277 -0.059959613 -0.061788149 -0.061739314 -0.059199262 -0.054970223 -0.050257713 -0.045585237][-0.041013125 -0.041101884 -0.037342638 -0.031612806 -0.026710859 -0.025108863 -0.027246304 -0.032315619 -0.039229963 -0.046008717 -0.050212625 -0.050539166 -0.047895245 -0.043856893 -0.039219674][-0.010690762 -0.0072828806 0.0020184065 0.014787534 0.025760032 0.030670982 0.02836857 0.019317953 0.0052129207 -0.010105454 -0.021896068 -0.028109964 -0.029841661 -0.029190406 -0.026882682][0.041698571 0.05097181 0.068359621 0.091318548 0.1114413 0.12164112 0.11928861 0.10409168 0.078811526 0.050014198 0.025388112 0.0086050974 -0.0013867093 -0.0068037016 -0.00882425][0.11237516 0.13007852 0.15824109 0.19445205 0.22649038 0.24345638 0.24074107 0.21716593 0.17738712 0.13112707 0.089461781 0.05791242 0.036341183 0.023162095 0.016033186][0.1861935 0.21371625 0.25399154 0.30398446 0.34782076 0.37087306 0.36725277 0.33500338 0.28080678 0.2170475 0.15804602 0.11113618 0.077393673 0.056668382 0.04549659][0.24239896 0.27904177 0.32885298 0.38845268 0.43979898 0.4662796 0.46222469 0.42526475 0.36279881 0.28754669 0.21600291 0.15706672 0.1140377 0.088649571 0.07626801][0.26219657 0.30335096 0.35553578 0.41629896 0.4680419 0.49468759 0.49159524 0.45597476 0.39470753 0.31815442 0.24257563 0.17835386 0.13202517 0.10743292 0.098341025][0.23365436 0.27116263 0.31763166 0.37276843 0.42127448 0.4489117 0.45160383 0.425621 0.37631819 0.30946603 0.23913756 0.1771813 0.13399476 0.11510497 0.1125764][0.17073521 0.1970875 0.23199514 0.277339 0.32069936 0.35044077 0.36276412 0.35342395 0.32565078 0.27891073 0.22356834 0.17219222 0.13811894 0.1269066 0.12992026][0.10370723 0.11344138 0.13365015 0.16683213 0.2029742 0.23297681 0.25353664 0.26168308 0.2589711 0.23811936 0.2048943 0.17138042 0.15104018 0.14750744 0.15261601][0.055337105 0.050748192 0.058664214 0.080818743 0.10816367 0.13366976 0.15567796 0.17457511 0.19196104 0.19507603 0.18477872 0.17061605 0.16345586 0.16482981 0.16805021][0.037550922 0.027686162 0.030675413 0.045608155 0.063390605 0.079260066 0.094452724 0.11352036 0.13980247 0.1582098 0.16520494 0.16593432 0.16665629 0.1680185 0.16618516][0.048730012 0.044849135 0.050729919 0.061992697 0.070104524 0.073972821 0.078236967 0.0905353 0.11567524 0.13901097 0.15395942 0.16074246 0.16096494 0.1563089 0.14739883][0.075523488 0.08531528 0.0982903 0.10708819 0.10588434 0.0988104 0.092947826 0.096753635 0.11535984 0.13646154 0.15183796 0.15787241 0.15245706 0.13926294 0.12382051]]...]
INFO - root - 2017-12-11 01:29:38.203533: step 73210, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.762 sec/batch; 54h:54m:04s remains)
INFO - root - 2017-12-11 01:29:46.133768: step 73220, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.775 sec/batch; 55h:48m:17s remains)
INFO - root - 2017-12-11 01:29:54.107832: step 73230, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 58h:16m:00s remains)
INFO - root - 2017-12-11 01:30:01.988367: step 73240, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 58h:02m:24s remains)
INFO - root - 2017-12-11 01:30:09.692206: step 73250, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 54h:25m:41s remains)
INFO - root - 2017-12-11 01:30:17.632372: step 73260, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 57h:48m:02s remains)
INFO - root - 2017-12-11 01:30:25.296577: step 73270, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.763 sec/batch; 54h:58m:06s remains)
INFO - root - 2017-12-11 01:30:33.202311: step 73280, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.772 sec/batch; 55h:33m:34s remains)
INFO - root - 2017-12-11 01:30:41.107107: step 73290, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.758 sec/batch; 54h:35m:00s remains)
INFO - root - 2017-12-11 01:30:48.908390: step 73300, loss = 0.68, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 57h:21m:13s remains)
2017-12-11 01:30:49.813261: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12205075 -0.12520637 -0.12786955 -0.12995832 -0.12946826 -0.12803461 -0.1268491 -0.12794232 -0.1308119 -0.13438727 -0.13699239 -0.13747136 -0.13606541 -0.13353576 -0.13063927][-0.1231972 -0.12831411 -0.1319439 -0.13181199 -0.12526628 -0.11623203 -0.10891617 -0.10814615 -0.11440873 -0.12477692 -0.1345204 -0.14022043 -0.14162609 -0.14002991 -0.13694085][-0.11895529 -0.12583862 -0.12793143 -0.12062915 -0.10143264 -0.078426778 -0.060970929 -0.058130119 -0.071271718 -0.093933374 -0.11651844 -0.13164628 -0.13818383 -0.13811888 -0.1345326][-0.10076128 -0.1069691 -0.10345376 -0.083278567 -0.043594941 0.0038391573 0.042051438 0.053463396 0.033599477 -0.0073126662 -0.053243119 -0.08991342 -0.11226281 -0.12106446 -0.12113897][-0.063473426 -0.064755574 -0.050942745 -0.013127054 0.053148914 0.13467522 0.20458491 0.23146293 0.20465195 0.13881858 0.059077431 -0.010531132 -0.058421757 -0.082785882 -0.090379871][-0.012866868 -0.0063190656 0.021116842 0.079946078 0.17732859 0.30008185 0.40915143 0.45453873 0.41748834 0.31903079 0.19782838 0.089647874 0.013117026 -0.027963532 -0.043215808][0.04032341 0.054723162 0.096943356 0.17817836 0.30640727 0.46762949 0.61063355 0.66801077 0.61485952 0.4809086 0.32127783 0.18189053 0.084639229 0.032686517 0.013263253][0.089292377 0.11065324 0.16702706 0.26823005 0.41852894 0.60084218 0.7557677 0.80904275 0.73531681 0.57294583 0.39066675 0.23893638 0.1366861 0.083181784 0.064042546][0.12855349 0.15458591 0.22074221 0.33330417 0.48830062 0.6642881 0.80184871 0.8350243 0.74394143 0.57168192 0.39202824 0.25216278 0.16232857 0.11631197 0.10091683][0.14694022 0.172977 0.2406535 0.35120553 0.49018526 0.63217604 0.72683489 0.7290104 0.62902945 0.4695898 0.31971461 0.21593769 0.15563932 0.12611358 0.11743337][0.14678784 0.16741706 0.22623815 0.3200314 0.42596373 0.51781213 0.56091666 0.53472245 0.43936715 0.31176478 0.20863484 0.15229055 0.12748875 0.11626665 0.11314133][0.13293649 0.14512995 0.18732628 0.25401929 0.31924728 0.36032575 0.36038283 0.3194474 0.24327394 0.15889639 0.10643734 0.094263159 0.098408833 0.099023893 0.096254691][0.10358037 0.10532439 0.12718801 0.16453181 0.19410759 0.19945078 0.17797743 0.14019467 0.092171438 0.049983095 0.038613189 0.056290567 0.075880572 0.079233229 0.07218904][0.059195586 0.048841402 0.052071922 0.068014912 0.078606568 0.072427124 0.053481452 0.032197274 0.011184051 -0.0017757645 0.0090920562 0.03786923 0.0591543 0.059137724 0.047720108][0.01116759 -0.010192618 -0.018193608 -0.0091133546 0.0024860417 0.00624188 0.0046797972 0.0018726796 -0.0037364971 -0.0075605046 0.0038727906 0.02620928 0.038784012 0.033103574 0.020966342]]...]
INFO - root - 2017-12-11 01:30:57.641297: step 73310, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 57h:10m:58s remains)
INFO - root - 2017-12-11 01:31:05.479138: step 73320, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 56h:29m:50s remains)
INFO - root - 2017-12-11 01:31:13.150654: step 73330, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 56h:35m:35s remains)
INFO - root - 2017-12-11 01:31:21.004697: step 73340, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 55h:53m:22s remains)
INFO - root - 2017-12-11 01:31:28.769225: step 73350, loss = 0.71, batch loss = 0.65 (11.5 examples/sec; 0.698 sec/batch; 50h:15m:34s remains)
INFO - root - 2017-12-11 01:31:36.446045: step 73360, loss = 0.70, batch loss = 0.64 (10.6 examples/sec; 0.755 sec/batch; 54h:21m:20s remains)
INFO - root - 2017-12-11 01:31:44.235788: step 73370, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.764 sec/batch; 54h:58m:50s remains)
INFO - root - 2017-12-11 01:31:52.029362: step 73380, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 57h:59m:32s remains)
INFO - root - 2017-12-11 01:31:59.953191: step 73390, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 56h:50m:55s remains)
INFO - root - 2017-12-11 01:32:07.913572: step 73400, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 58h:07m:00s remains)
2017-12-11 01:32:08.828596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043297678 -0.033236258 -0.024394957 -0.01763479 -0.014572996 -0.017318638 -0.022878969 -0.024210695 -0.021344084 -0.018237824 -0.016049439 -0.015704904 -0.020514993 -0.030852836 -0.042861827][-0.03032792 -0.011250835 0.0082188025 0.025370572 0.035757262 0.03573411 0.030639686 0.030974675 0.03704207 0.043407418 0.049403373 0.05315581 0.0466727 0.028394289 0.0046968292][-0.014430705 0.016527975 0.051186908 0.0836167 0.10511115 0.10999975 0.10692538 0.10986883 0.12059215 0.13203709 0.1446424 0.15567861 0.15027437 0.12427363 0.085777089][-0.00057604222 0.042913552 0.096069328 0.14948116 0.18819794 0.20252256 0.20318669 0.20805198 0.22336067 0.24224295 0.26620805 0.2900629 0.29053646 0.25842747 0.202575][0.0063156052 0.059588131 0.13059153 0.20757304 0.26930115 0.29975155 0.30744559 0.3115792 0.3253625 0.34728789 0.38227493 0.42237568 0.43517274 0.40349793 0.3344273][0.0060019838 0.064495713 0.14891794 0.24803321 0.3362765 0.39062688 0.4119125 0.41494846 0.41864076 0.43185368 0.46748549 0.51765352 0.54389256 0.52101648 0.45045859][0.00081062323 0.058999836 0.14903568 0.26343417 0.37592569 0.45781562 0.50032175 0.50883329 0.50147516 0.49588147 0.515489 0.56008154 0.59409165 0.587564 0.53162313][-0.0072074281 0.045235384 0.1309025 0.24769773 0.37160513 0.47237733 0.53510255 0.55607033 0.54581475 0.52206153 0.51597536 0.54068595 0.57445437 0.58810526 0.56038368][-0.015414963 0.028666466 0.10334608 0.21075433 0.32980478 0.43229365 0.50276262 0.53431916 0.52889687 0.49711403 0.46971875 0.47039914 0.49655661 0.5264833 0.53106385][-0.021352632 0.01488125 0.077502228 0.17082238 0.27533308 0.36502802 0.42699254 0.45832953 0.45669588 0.42428169 0.38561356 0.36803395 0.38341722 0.42051411 0.45062453][-0.021264527 0.0098174522 0.062196635 0.14148805 0.22906685 0.29979482 0.34331968 0.36412191 0.36091971 0.33072317 0.29110205 0.26639178 0.27371803 0.30967766 0.35191163][-0.016786752 0.010777611 0.054982841 0.12239159 0.1953361 0.24914178 0.274079 0.2814585 0.27342421 0.24520411 0.210882 0.18987578 0.19665702 0.22988439 0.27356678][-0.012808717 0.012161851 0.050852504 0.10989304 0.1730784 0.2174381 0.23180833 0.23093207 0.21926686 0.19164665 0.162411 0.14772034 0.1572587 0.18876578 0.23023222][-0.011634964 0.011256379 0.046358783 0.099373609 0.15601616 0.19642374 0.2084996 0.20722722 0.19670917 0.17051713 0.14366601 0.13100508 0.13948482 0.16767026 0.20590006][-0.015506101 0.0042716865 0.034961738 0.080960013 0.13003632 0.16620067 0.17825483 0.18011701 0.17377707 0.15063544 0.12427793 0.10873646 0.11136159 0.13329573 0.16717795]]...]
INFO - root - 2017-12-11 01:32:16.417121: step 73410, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.820 sec/batch; 58h:58m:56s remains)
INFO - root - 2017-12-11 01:32:24.254088: step 73420, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.802 sec/batch; 57h:44m:00s remains)
INFO - root - 2017-12-11 01:32:32.127881: step 73430, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 56h:55m:12s remains)
INFO - root - 2017-12-11 01:32:39.982108: step 73440, loss = 0.70, batch loss = 0.64 (11.7 examples/sec; 0.683 sec/batch; 49h:10m:37s remains)
INFO - root - 2017-12-11 01:32:47.938143: step 73450, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 58h:00m:40s remains)
INFO - root - 2017-12-11 01:32:55.807229: step 73460, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 57h:44m:09s remains)
INFO - root - 2017-12-11 01:33:03.710848: step 73470, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 55h:33m:09s remains)
INFO - root - 2017-12-11 01:33:11.565974: step 73480, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 56h:03m:57s remains)
INFO - root - 2017-12-11 01:33:19.348973: step 73490, loss = 0.72, batch loss = 0.66 (13.0 examples/sec; 0.615 sec/batch; 44h:14m:01s remains)
INFO - root - 2017-12-11 01:33:27.404146: step 73500, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.817 sec/batch; 58h:47m:11s remains)
2017-12-11 01:33:28.251808: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00969398 -0.0010928951 -0.015036656 -0.028920278 -0.038758293 -0.042870261 -0.042310275 -0.040019248 -0.037821665 -0.0364125 -0.035340358 -0.036863297 -0.040083483 -0.040044971 -0.035123497][0.060215473 0.045861535 0.028986635 0.015659137 0.010095139 0.015056963 0.028034044 0.041224577 0.04864613 0.048749123 0.042825297 0.02809724 0.0068053994 -0.0097262608 -0.014904961][0.11644603 0.10744628 0.099196926 0.098401584 0.10826802 0.13370295 0.16989768 0.20056443 0.21276054 0.20499888 0.18342789 0.14422925 0.091843441 0.047126845 0.024113804][0.16360545 0.17299393 0.18954568 0.21639861 0.25436682 0.31131876 0.37980318 0.43168208 0.44386971 0.41772133 0.36683071 0.2890605 0.19158238 0.10730213 0.0599137][0.18867417 0.23152143 0.290989 0.36089981 0.4374963 0.53165418 0.63367671 0.70235932 0.70516312 0.65073377 0.56174928 0.43775734 0.28948474 0.16312186 0.090374395][0.18824345 0.27129912 0.38099906 0.49796015 0.61330485 0.73886573 0.86318666 0.93514949 0.9184798 0.83357382 0.711091 0.55213714 0.36839449 0.21415247 0.12411731][0.18472549 0.2987926 0.44597757 0.59582263 0.73733044 0.88100916 1.0125759 1.074545 1.0345621 0.92789775 0.78890949 0.61988652 0.42909956 0.27025551 0.17467566][0.19423257 0.31830636 0.47395274 0.62630785 0.76747626 0.90696675 1.0283403 1.0735117 1.0195135 0.91267133 0.78416729 0.636133 0.46990588 0.33004707 0.23972498][0.22589269 0.33766949 0.47156984 0.5955534 0.70796895 0.81864357 0.91201127 0.93615937 0.87996024 0.79330742 0.69724554 0.59280968 0.47394636 0.37058112 0.29590636][0.26311788 0.34564558 0.43714386 0.51330733 0.57916689 0.64581543 0.70232707 0.70781726 0.66104096 0.60637212 0.5516386 0.49606144 0.42881915 0.36528262 0.30987716][0.28304103 0.328189 0.3698357 0.39466879 0.41203696 0.43349189 0.45374924 0.44429055 0.40945393 0.38291112 0.36169273 0.3440654 0.31783172 0.28691053 0.249661][0.27243957 0.28426087 0.28369194 0.26813406 0.24881969 0.2360293 0.2285625 0.20847885 0.18155076 0.17032474 0.16623598 0.16786711 0.16514029 0.1557833 0.13499554][0.22287303 0.21444589 0.18926707 0.15168479 0.11271472 0.080709904 0.057139535 0.032832272 0.01254511 0.0091020623 0.012704381 0.021768464 0.030010071 0.033797693 0.028032273][0.15904336 0.1454329 0.11482956 0.074303046 0.033349887 -0.0022723179 -0.030278875 -0.053280056 -0.06775777 -0.066771589 -0.059574794 -0.048844323 -0.03773082 -0.027392138 -0.020356148][0.11744781 0.10680928 0.08130306 0.048604563 0.016740277 -0.011161611 -0.033694718 -0.048979446 -0.054312956 -0.046048928 -0.033402588 -0.020269534 -0.0084382733 0.0045126192 0.02049502]]...]
INFO - root - 2017-12-11 01:33:36.064958: step 73510, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 57h:19m:40s remains)
INFO - root - 2017-12-11 01:33:44.106613: step 73520, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 56h:33m:41s remains)
INFO - root - 2017-12-11 01:33:51.812310: step 73530, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.782 sec/batch; 56h:14m:53s remains)
INFO - root - 2017-12-11 01:33:59.570324: step 73540, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 55h:42m:02s remains)
INFO - root - 2017-12-11 01:34:07.468772: step 73550, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 58h:29m:16s remains)
INFO - root - 2017-12-11 01:34:15.341531: step 73560, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.801 sec/batch; 57h:35m:08s remains)
INFO - root - 2017-12-11 01:34:22.936981: step 73570, loss = 0.68, batch loss = 0.62 (13.9 examples/sec; 0.576 sec/batch; 41h:24m:38s remains)
INFO - root - 2017-12-11 01:34:30.765200: step 73580, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.817 sec/batch; 58h:45m:16s remains)
INFO - root - 2017-12-11 01:34:38.580298: step 73590, loss = 0.72, batch loss = 0.66 (10.5 examples/sec; 0.763 sec/batch; 54h:51m:01s remains)
INFO - root - 2017-12-11 01:34:46.471437: step 73600, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.821 sec/batch; 59h:03m:16s remains)
2017-12-11 01:34:47.301268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.02488612 -0.024121927 -0.023958256 -0.02537846 -0.027913539 -0.031063395 -0.034069438 -0.035680063 -0.034719165 -0.03080238 -0.022129688 -0.010065302 0.0015370995 0.010855399 0.015219767][-0.022803744 -0.020448415 -0.019293172 -0.020429602 -0.023270609 -0.027078245 -0.030718358 -0.03299357 -0.032515008 -0.029009962 -0.02080258 -0.0091553479 0.0024793365 0.012408572 0.017860662][-0.010657812 -0.0049891011 -0.0017593682 -0.002355973 -0.0059154858 -0.011360823 -0.017069397 -0.021501476 -0.022812044 -0.020819535 -0.014375322 -0.0048586074 0.0046779197 0.012973928 0.017623294][0.018227765 0.030654961 0.038543269 0.039878953 0.03562494 0.027190443 0.017232899 0.0086328648 0.0041294671 0.0036436208 0.0070711626 0.012370997 0.016641861 0.019436318 0.019467298][0.065201826 0.088418052 0.10461611 0.11035629 0.10618448 0.093872242 0.078135975 0.064432561 0.05649225 0.053287394 0.052526843 0.050905161 0.045834284 0.038259134 0.0289756][0.11992918 0.15635069 0.18318945 0.19522819 0.19229017 0.17650324 0.155547 0.13814326 0.12854505 0.12381157 0.11869056 0.10824508 0.090505011 0.06842076 0.045882493][0.16470203 0.21217616 0.24809358 0.26590171 0.26481032 0.24736439 0.22397707 0.20614372 0.19783826 0.19377558 0.18582968 0.16742624 0.13738371 0.10063212 0.064355031][0.17860901 0.22978806 0.26863441 0.28877524 0.289299 0.27294871 0.25162461 0.23773259 0.23384242 0.23285083 0.2252145 0.20337366 0.16642994 0.12040956 0.074993223][0.15345293 0.19893715 0.23336148 0.25151634 0.25265238 0.2393849 0.22332221 0.2156236 0.21703294 0.21969026 0.21495481 0.19561726 0.160421 0.11488799 0.069048576][0.097910151 0.13050847 0.15508226 0.16791758 0.16849156 0.15888362 0.14886005 0.14686826 0.15208216 0.15757288 0.15746976 0.14619501 0.12149411 0.086771704 0.0501649][0.035320811 0.054101296 0.068404175 0.075604096 0.075000964 0.067915879 0.061556034 0.061593227 0.066969775 0.072939239 0.077303424 0.076458164 0.066741377 0.048736271 0.027115483][-0.011272606 -0.0026867234 0.0047212522 0.00862344 0.0075765653 0.0019590475 -0.0034772102 -0.0048987325 -0.0024661925 0.0017147119 0.0087510077 0.016696608 0.020557521 0.018368103 0.011301776][-0.028938014 -0.025481919 -0.021042418 -0.018032139 -0.018664582 -0.023546079 -0.029741773 -0.034344051 -0.035899743 -0.034463488 -0.026701238 -0.013666302 -0.0013673292 0.0064827478 0.0085175307][-0.017557405 -0.015061922 -0.011281782 -0.0084806634 -0.009062266 -0.014343536 -0.022310777 -0.030191842 -0.035186082 -0.036224686 -0.029177947 -0.014837005 0.000329731 0.01172293 0.01671716][0.0084015047 0.011904583 0.015307084 0.017193498 0.015605009 0.0090117687 -0.00084786327 -0.011095263 -0.018186087 -0.020812875 -0.015019921 -0.0017442773 0.012676823 0.023471585 0.02763525]]...]
INFO - root - 2017-12-11 01:34:55.096792: step 73610, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 54h:47m:08s remains)
INFO - root - 2017-12-11 01:35:02.844471: step 73620, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 56h:48m:17s remains)
INFO - root - 2017-12-11 01:35:10.685447: step 73630, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 56h:20m:10s remains)
INFO - root - 2017-12-11 01:35:18.663989: step 73640, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.811 sec/batch; 58h:18m:46s remains)
INFO - root - 2017-12-11 01:35:26.383319: step 73650, loss = 0.71, batch loss = 0.65 (12.6 examples/sec; 0.635 sec/batch; 45h:40m:34s remains)
INFO - root - 2017-12-11 01:35:34.170999: step 73660, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 55h:00m:42s remains)
INFO - root - 2017-12-11 01:35:41.992017: step 73670, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 55h:48m:22s remains)
INFO - root - 2017-12-11 01:35:49.961775: step 73680, loss = 0.67, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 55h:21m:12s remains)
INFO - root - 2017-12-11 01:35:57.768869: step 73690, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 58h:17m:04s remains)
INFO - root - 2017-12-11 01:36:05.872467: step 73700, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 55h:20m:07s remains)
2017-12-11 01:36:06.685144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.028797654 -0.020836338 -0.0081032263 0.00522332 0.016115585 0.023156375 0.027576249 0.028471364 0.022391614 0.0097086132 -0.0063121617 -0.019517729 -0.026937215 -0.026872788 -0.020139234][-0.020889 -0.0065052342 0.016225029 0.042576071 0.067546144 0.087310851 0.10087667 0.10541546 0.096830383 0.077577166 0.053314392 0.0317968 0.015252223 0.0060774293 0.0055964][-0.0055064815 0.019695397 0.058067854 0.10369388 0.14823639 0.18443388 0.2079622 0.21434584 0.20142551 0.175321 0.14441587 0.11611655 0.089823812 0.06736394 0.052794039][0.0099276584 0.048654925 0.10746733 0.17829125 0.24804951 0.30424404 0.33769843 0.34277105 0.32245448 0.28947404 0.2552388 0.22442944 0.19123283 0.15415803 0.11949599][0.019300194 0.0706225 0.15011503 0.24740796 0.3441104 0.42137685 0.46434572 0.46675283 0.43872988 0.40169623 0.36990708 0.34386361 0.31044886 0.26252675 0.206797][0.026061822 0.088769607 0.18672203 0.30717298 0.42670187 0.52117485 0.57152408 0.57181275 0.53940022 0.50359309 0.48073131 0.46602327 0.43812785 0.3837375 0.30865321][0.030710962 0.10183895 0.21233821 0.34729102 0.47950873 0.5822044 0.634495 0.63184172 0.59786195 0.56699306 0.5565635 0.55692238 0.54071182 0.4887931 0.40393102][0.030290827 0.10484146 0.21983176 0.35843709 0.49165371 0.59205633 0.639114 0.63076359 0.59527516 0.56931704 0.56962025 0.58403873 0.58320522 0.54414988 0.46467158][0.020823387 0.090787448 0.1993441 0.32933894 0.45257863 0.54287595 0.58187819 0.569542 0.53493887 0.51331484 0.51961792 0.54221833 0.5546726 0.5340153 0.47284195][0.004295357 0.062110696 0.15344089 0.26272729 0.36509126 0.43776277 0.46615419 0.4514823 0.41980639 0.40142077 0.40870625 0.43246755 0.45260322 0.45016164 0.41458613][-0.012423111 0.030362429 0.099369191 0.18142329 0.25628614 0.30606735 0.32090458 0.30329177 0.27440298 0.25750533 0.26187149 0.28099331 0.30221748 0.31272376 0.3015154][-0.028892377 -0.0019740183 0.043639325 0.097330712 0.14409819 0.17090037 0.17192341 0.15075679 0.12338474 0.10599878 0.10505088 0.11697834 0.13584766 0.1545206 0.16276547][-0.044022795 -0.031605009 -0.006907024 0.022221541 0.045895081 0.054923143 0.046181343 0.023755753 -0.00085470587 -0.017524704 -0.022332059 -0.01668191 -0.0021362691 0.018166814 0.036125463][-0.054674204 -0.053927243 -0.045736741 -0.035100725 -0.027530884 -0.0289682 -0.040905509 -0.059643276 -0.077789158 -0.089958735 -0.0943568 -0.091870658 -0.082297176 -0.066517554 -0.049369786][-0.058607157 -0.065161929 -0.067709774 -0.069064528 -0.071361609 -0.077593021 -0.088322818 -0.10107072 -0.11181007 -0.11822431 -0.12004764 -0.11834826 -0.11278533 -0.10343751 -0.092297308]]...]
INFO - root - 2017-12-11 01:36:14.398864: step 73710, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.759 sec/batch; 54h:35m:15s remains)
INFO - root - 2017-12-11 01:36:22.350581: step 73720, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.786 sec/batch; 56h:31m:07s remains)
INFO - root - 2017-12-11 01:36:30.028069: step 73730, loss = 0.70, batch loss = 0.64 (12.0 examples/sec; 0.667 sec/batch; 47h:54m:41s remains)
INFO - root - 2017-12-11 01:36:37.921375: step 73740, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.837 sec/batch; 60h:10m:04s remains)
INFO - root - 2017-12-11 01:36:46.119709: step 73750, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 56h:53m:29s remains)
INFO - root - 2017-12-11 01:36:54.007922: step 73760, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 55h:29m:00s remains)
INFO - root - 2017-12-11 01:37:01.894945: step 73770, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.785 sec/batch; 56h:26m:37s remains)
INFO - root - 2017-12-11 01:37:09.680134: step 73780, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 55h:26m:23s remains)
INFO - root - 2017-12-11 01:37:17.494835: step 73790, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 54h:58m:16s remains)
INFO - root - 2017-12-11 01:37:25.149156: step 73800, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.748 sec/batch; 53h:44m:01s remains)
2017-12-11 01:37:26.017358: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22747302 0.2182838 0.19144991 0.15499251 0.13449475 0.14398181 0.17941545 0.22338673 0.26025262 0.27521205 0.25820747 0.2132144 0.15697648 0.10251995 0.054988552][0.2531485 0.24478427 0.22186148 0.19033238 0.17676553 0.19798228 0.24880467 0.30902591 0.36040246 0.38585415 0.37228927 0.31994978 0.24660121 0.16982332 0.098954327][0.23831695 0.22844256 0.21399204 0.19778846 0.20218916 0.24245051 0.30993441 0.38200498 0.44002628 0.46967909 0.45788625 0.40175247 0.3176291 0.22525327 0.13722572][0.2015276 0.18850853 0.18395819 0.1895242 0.22071405 0.28625935 0.37142891 0.45067161 0.506786 0.53101277 0.51338881 0.45073369 0.35701749 0.25292936 0.15315475][0.18811725 0.17076679 0.1755057 0.20525089 0.26763916 0.36112276 0.46304825 0.54443485 0.59019256 0.59820181 0.56383878 0.48643294 0.37883413 0.2620666 0.15175414][0.21172261 0.19133261 0.20452894 0.2564055 0.34610155 0.46113026 0.57285696 0.65053779 0.6818462 0.66962969 0.61430812 0.51923978 0.39770457 0.26995745 0.15107046][0.25465047 0.23778614 0.2612516 0.33040348 0.43659124 0.55941314 0.66908562 0.73645276 0.75299454 0.723493 0.650791 0.54156846 0.41098297 0.27808547 0.15549655][0.29709175 0.28852966 0.32066885 0.39743087 0.50439233 0.6180535 0.71236074 0.763834 0.76882315 0.731335 0.65346658 0.54190969 0.41285127 0.2835657 0.16319768][0.30393577 0.30365574 0.33851838 0.41103667 0.5040012 0.59538645 0.66581476 0.69939506 0.69749331 0.66272712 0.59533936 0.49866024 0.38698727 0.27374524 0.16433392][0.25450003 0.25748286 0.28893545 0.34928676 0.42118114 0.48691434 0.53448838 0.55459255 0.55201316 0.52836758 0.48235133 0.41256934 0.328934 0.23990549 0.14814088][0.15012763 0.15327217 0.17897972 0.22500318 0.27613851 0.32095554 0.35486948 0.37322414 0.38122356 0.37710732 0.3570804 0.31526935 0.25719237 0.18859199 0.11247982][0.028224787 0.031046353 0.051370237 0.084781833 0.1205351 0.15432774 0.18816912 0.21945904 0.25091612 0.27409354 0.27990326 0.25806019 0.21135716 0.14769354 0.074988067][-0.065928772 -0.063129671 -0.046620313 -0.02188701 0.0055841561 0.037990395 0.081905 0.13446504 0.19464067 0.24648246 0.2738227 0.26210091 0.21277638 0.13953178 0.057631183][-0.11404362 -0.11103652 -0.096745826 -0.076715775 -0.052075986 -0.015979787 0.039920654 0.11133543 0.19440116 0.26807916 0.31044587 0.30330196 0.24709086 0.16076395 0.065446004][-0.11946171 -0.11603527 -0.10307187 -0.085443445 -0.061801143 -0.023335325 0.037890255 0.11670798 0.20876315 0.29167151 0.34105653 0.33649921 0.27733973 0.18459663 0.081513256]]...]
INFO - root - 2017-12-11 01:37:33.821259: step 73810, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 55h:25m:45s remains)
INFO - root - 2017-12-11 01:37:41.658049: step 73820, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 56h:50m:48s remains)
INFO - root - 2017-12-11 01:37:49.437304: step 73830, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 54h:46m:02s remains)
INFO - root - 2017-12-11 01:37:57.409383: step 73840, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.759 sec/batch; 54h:30m:19s remains)
INFO - root - 2017-12-11 01:38:05.206107: step 73850, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 56h:16m:05s remains)
INFO - root - 2017-12-11 01:38:13.046458: step 73860, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 58h:10m:09s remains)
INFO - root - 2017-12-11 01:38:20.919180: step 73870, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 56h:42m:10s remains)
INFO - root - 2017-12-11 01:38:28.773228: step 73880, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 55h:12m:57s remains)
INFO - root - 2017-12-11 01:38:36.322098: step 73890, loss = 0.69, batch loss = 0.63 (12.3 examples/sec; 0.652 sec/batch; 46h:49m:21s remains)
INFO - root - 2017-12-11 01:38:44.130697: step 73900, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 57h:04m:33s remains)
2017-12-11 01:38:45.009004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.070529059 -0.073899247 -0.075540125 -0.07670366 -0.077733919 -0.080086321 -0.084957272 -0.089717537 -0.092510276 -0.092791766 -0.09269084 -0.093779176 -0.095631532 -0.097273856 -0.09670236][-0.039753113 -0.038487215 -0.038289841 -0.038514629 -0.038326837 -0.040336654 -0.045787171 -0.0506615 -0.053323936 -0.055673998 -0.062545076 -0.074186131 -0.087504432 -0.099052235 -0.10591631][0.035843328 0.048368029 0.053588163 0.055225711 0.056655947 0.054776553 0.049134891 0.044938982 0.04285096 0.037052445 0.018669726 -0.010513036 -0.043626864 -0.073723607 -0.096229][0.16641808 0.19956921 0.21546058 0.221764 0.22527836 0.2235444 0.21854529 0.21568123 0.2132539 0.2005733 0.16429369 0.10920966 0.046040926 -0.013640305 -0.062645055][0.33998269 0.40286025 0.43737918 0.4554249 0.46726704 0.471745 0.47383475 0.47537452 0.4705641 0.44315407 0.37775847 0.28443992 0.178956 0.078488775 -0.0071029742][0.51633447 0.610189 0.66771466 0.70476943 0.73369914 0.75426465 0.77278423 0.78305107 0.77249783 0.72009444 0.61368906 0.47224349 0.31836483 0.17378718 0.050593141][0.65313995 0.77021754 0.84754205 0.9053638 0.95562589 0.99788141 1.0371803 1.0567051 1.0363846 0.95269287 0.80153924 0.61331993 0.41811451 0.23972249 0.090191014][0.71915394 0.84632921 0.93536848 1.0106261 1.0809045 1.1439167 1.2004795 1.2245618 1.1905141 1.075842 0.887299 0.66635287 0.44883573 0.25667927 0.099587344][0.69252944 0.81254274 0.90110606 0.98558378 1.0705675 1.150013 1.2178648 1.2419988 1.1949518 1.0579691 0.84903717 0.61769241 0.40229788 0.2200176 0.076082021][0.57769889 0.67399675 0.74775654 0.8269372 0.91341215 0.99732232 1.0658814 1.0862864 1.0333707 0.89338762 0.69107741 0.47796163 0.29065591 0.14028189 0.027093889][0.40786329 0.47121906 0.51991266 0.57928979 0.651064 0.72370929 0.7812022 0.79567325 0.74700272 0.6263594 0.458298 0.2890141 0.14929295 0.044546906 -0.029334063][0.2280782 0.26044253 0.28384998 0.31820035 0.36621052 0.41715616 0.45642993 0.46435195 0.42696053 0.33946911 0.2211255 0.10739359 0.02047471 -0.038466409 -0.0756486][0.071352616 0.080885872 0.086265169 0.10047897 0.1268331 0.1565616 0.17890534 0.18221268 0.15837015 0.10487901 0.034401514 -0.029122554 -0.071827255 -0.094921649 -0.10474953][-0.041504316 -0.046240017 -0.051124748 -0.049468249 -0.038102694 -0.023226827 -0.011585156 -0.009348819 -0.02027438 -0.046179365 -0.079792731 -0.10689729 -0.11979619 -0.12061501 -0.11469077][-0.099529244 -0.11099084 -0.12040357 -0.12565307 -0.12408143 -0.11896235 -0.11367919 -0.1112167 -0.11285259 -0.12004706 -0.12966496 -0.13465856 -0.13124232 -0.1215305 -0.10946023]]...]
INFO - root - 2017-12-11 01:38:52.813206: step 73910, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.771 sec/batch; 55h:23m:14s remains)
INFO - root - 2017-12-11 01:39:00.573570: step 73920, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.746 sec/batch; 53h:35m:06s remains)
INFO - root - 2017-12-11 01:39:08.371023: step 73930, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 55h:15m:04s remains)
INFO - root - 2017-12-11 01:39:16.265260: step 73940, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 57h:40m:13s remains)
INFO - root - 2017-12-11 01:39:24.222275: step 73950, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.824 sec/batch; 59h:08m:51s remains)
INFO - root - 2017-12-11 01:39:32.177271: step 73960, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 57h:50m:06s remains)
INFO - root - 2017-12-11 01:39:39.868247: step 73970, loss = 0.70, batch loss = 0.64 (11.1 examples/sec; 0.719 sec/batch; 51h:38m:58s remains)
INFO - root - 2017-12-11 01:39:47.614235: step 73980, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 56h:49m:34s remains)
INFO - root - 2017-12-11 01:39:55.475742: step 73990, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 58h:04m:05s remains)
INFO - root - 2017-12-11 01:40:03.386698: step 74000, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 58h:08m:41s remains)
2017-12-11 01:40:04.206695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.030072546 -0.036180388 -0.029921304 -0.012078695 0.011898509 0.044651508 0.079144426 0.096469536 0.089753278 0.069406494 0.054490741 0.055563394 0.068950295 0.096231706 0.13193452][-0.042178448 -0.054926589 -0.052863445 -0.035172503 -0.0067375251 0.033811193 0.075196631 0.096314885 0.091582105 0.073636673 0.062129579 0.067518361 0.086853243 0.1234813 0.1681978][-0.023855364 -0.043497521 -0.047766909 -0.03432332 -0.0063108341 0.036503971 0.079094194 0.099515773 0.094633736 0.078251056 0.06873662 0.074525669 0.093643844 0.13331605 0.18241034][0.029528489 0.0060717966 -0.0029783593 0.0052401125 0.029303171 0.069610052 0.10908901 0.12563284 0.11906954 0.10234416 0.091106527 0.09045174 0.099688224 0.13136274 0.17554764][0.10125376 0.080863871 0.072851226 0.078944996 0.099146232 0.13598876 0.17204671 0.18481582 0.17655775 0.15735364 0.1395129 0.12426823 0.11279441 0.12344912 0.15143675][0.16406052 0.15454544 0.15525597 0.16486472 0.18431886 0.21989414 0.25485778 0.26545668 0.25476864 0.22946559 0.19969477 0.16414464 0.12609583 0.10836594 0.11319818][0.19353163 0.20145524 0.21730436 0.23502247 0.25597376 0.2908681 0.32489884 0.33315682 0.31783131 0.28345722 0.23998146 0.18571165 0.12557219 0.083328068 0.066892065][0.17348486 0.19977994 0.23169467 0.25787914 0.27954391 0.31094521 0.34051028 0.34434953 0.32305616 0.28069723 0.22783498 0.1641987 0.095770784 0.043146715 0.015621117][0.10632684 0.14405626 0.18552552 0.21533787 0.23422346 0.25754943 0.27801409 0.27578142 0.25002864 0.20546162 0.15262975 0.0929478 0.032899633 -0.012790959 -0.03767404][0.014559619 0.05211018 0.09259361 0.118993 0.1319246 0.14513442 0.15536849 0.14882429 0.12362482 0.084643118 0.041066002 -0.0044959798 -0.045468584 -0.073564649 -0.086391285][-0.069828212 -0.043380868 -0.013381863 0.0047161924 0.011952664 0.018462814 0.022943459 0.016826129 -0.0024681296 -0.031087151 -0.061805118 -0.090838626 -0.11166941 -0.12100762 -0.12018662][-0.12066557 -0.10956439 -0.093613014 -0.084150076 -0.079428621 -0.073926866 -0.069590181 -0.072033465 -0.08382915 -0.10210001 -0.1213668 -0.13718297 -0.14341874 -0.13983934 -0.13010079][-0.12808917 -0.13026832 -0.12646444 -0.12345085 -0.11900317 -0.1116817 -0.10522944 -0.10405886 -0.11005633 -0.12074623 -0.13203764 -0.13976395 -0.13893782 -0.13073343 -0.11934462][-0.10170531 -0.11187361 -0.11634663 -0.11807726 -0.11463236 -0.10697182 -0.099848472 -0.0969217 -0.099268124 -0.10490207 -0.11111931 -0.11490082 -0.11270829 -0.10595372 -0.098149449][-0.063704096 -0.07723365 -0.0866552 -0.092359349 -0.091684371 -0.0863331 -0.080665119 -0.077280037 -0.07676141 -0.078319386 -0.080977373 -0.0831193 -0.082337588 -0.079614855 -0.077277556]]...]
INFO - root - 2017-12-11 01:40:12.228519: step 74010, loss = 0.71, batch loss = 0.66 (9.8 examples/sec; 0.817 sec/batch; 58h:39m:20s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 01:40:20.060415: step 74020, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.752 sec/batch; 54h:01m:38s remains)
INFO - root - 2017-12-11 01:40:27.954202: step 74030, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.793 sec/batch; 56h:56m:16s remains)
INFO - root - 2017-12-11 01:40:35.790322: step 74040, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 57h:54m:04s remains)
INFO - root - 2017-12-11 01:40:43.467997: step 74050, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.804 sec/batch; 57h:41m:39s remains)
INFO - root - 2017-12-11 01:40:51.200902: step 74060, loss = 0.71, batch loss = 0.65 (10.8 examples/sec; 0.740 sec/batch; 53h:05m:42s remains)
INFO - root - 2017-12-11 01:40:59.028939: step 74070, loss = 0.70, batch loss = 0.64 (8.9 examples/sec; 0.898 sec/batch; 64h:28m:19s remains)
INFO - root - 2017-12-11 01:41:06.999265: step 74080, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 55h:51m:32s remains)
INFO - root - 2017-12-11 01:41:14.822391: step 74090, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 55h:49m:41s remains)
INFO - root - 2017-12-11 01:41:22.705933: step 74100, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 55h:50m:16s remains)
2017-12-11 01:41:23.591229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.056889761 -0.059894707 -0.063974686 -0.069444217 -0.07348232 -0.074369006 -0.072783783 -0.069244258 -0.063784063 -0.057992678 -0.054963503 -0.056700267 -0.062634215 -0.069852285 -0.073545486][-0.013274518 -0.01603142 -0.023797963 -0.035256181 -0.0435619 -0.045718972 -0.043597355 -0.03806641 -0.028901456 -0.018237021 -0.011786046 -0.01415401 -0.024884369 -0.038812969 -0.047148161][0.07015802 0.070505731 0.057068218 0.034789994 0.017901646 0.012781617 0.015531145 0.023744237 0.037513956 0.054165415 0.06528198 0.062861554 0.046421822 0.024763728 0.012120263][0.2018593 0.20530695 0.18138918 0.14334139 0.11608984 0.11074753 0.11911277 0.13394333 0.1537666 0.17620032 0.19180562 0.18925494 0.16653892 0.13696279 0.12164524][0.37444907 0.37835217 0.34003565 0.28627449 0.25375566 0.25711763 0.27983466 0.30495748 0.32869795 0.35194668 0.36810243 0.36371875 0.33507147 0.29974538 0.28590196][0.55502212 0.55694658 0.50687832 0.44635791 0.4204416 0.44369215 0.48740456 0.52110893 0.5391947 0.55101788 0.55885464 0.55020696 0.51966321 0.48610738 0.48215804][0.69401264 0.69423383 0.6449526 0.59622645 0.592382 0.64252269 0.70557654 0.73852259 0.73676032 0.72318143 0.71417975 0.70160282 0.67808437 0.65936232 0.67624766][0.74753165 0.75108355 0.72015136 0.70203704 0.72889787 0.80093658 0.87043697 0.88951838 0.85824615 0.81334418 0.78547877 0.77243984 0.76498431 0.77137172 0.81379676][0.69300944 0.7056607 0.70506716 0.7239396 0.77582937 0.85289323 0.91166097 0.91043973 0.8551693 0.78995246 0.75300896 0.745787 0.75751173 0.78673738 0.84397739][0.54670715 0.57135135 0.59991533 0.64570522 0.70607787 0.77090263 0.81134009 0.79963154 0.74411625 0.68473089 0.65635026 0.65964943 0.68235856 0.71543324 0.76170695][0.35304555 0.3859252 0.43018007 0.48429921 0.53917396 0.58927554 0.62389088 0.62873769 0.60663038 0.58126062 0.57339448 0.58000237 0.59080297 0.5967375 0.60194951][0.16172758 0.19294135 0.23475122 0.28073725 0.32610041 0.37402532 0.42731759 0.47783321 0.51463205 0.53747517 0.54802126 0.54046047 0.51291895 0.46602592 0.41357943][0.012419533 0.032538567 0.05979114 0.090885133 0.13080427 0.1904766 0.27802247 0.38343173 0.47909585 0.5415737 0.5557819 0.51716012 0.43813357 0.33440682 0.23174153][-0.074640594 -0.069401741 -0.058706544 -0.039800126 0.00015625764 0.074775025 0.19046235 0.33160731 0.45872366 0.53463042 0.53616029 0.46452948 0.34640974 0.20980881 0.0864205][-0.09934742 -0.10798868 -0.1107195 -0.099368468 -0.059103709 0.020381793 0.14004651 0.28010017 0.3993037 0.46023995 0.44190013 0.35311311 0.22832496 0.09881442 -0.0076562618]]...]
INFO - root - 2017-12-11 01:41:31.479520: step 74110, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 57h:38m:58s remains)
INFO - root - 2017-12-11 01:41:39.294679: step 74120, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 56h:30m:00s remains)
INFO - root - 2017-12-11 01:41:46.955012: step 74130, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.827 sec/batch; 59h:21m:36s remains)
INFO - root - 2017-12-11 01:41:54.843795: step 74140, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.806 sec/batch; 57h:52m:00s remains)
INFO - root - 2017-12-11 01:42:02.535846: step 74150, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 55h:43m:54s remains)
INFO - root - 2017-12-11 01:42:10.449820: step 74160, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 58h:02m:08s remains)
INFO - root - 2017-12-11 01:42:18.296144: step 74170, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 55h:54m:45s remains)
INFO - root - 2017-12-11 01:42:26.072421: step 74180, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 54h:40m:35s remains)
INFO - root - 2017-12-11 01:42:33.975362: step 74190, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 57h:17m:36s remains)
INFO - root - 2017-12-11 01:42:41.920279: step 74200, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.776 sec/batch; 55h:38m:49s remains)
2017-12-11 01:42:42.827392: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18501039 0.16121778 0.13321623 0.11249672 0.09827283 0.079351492 0.054954425 0.03484327 0.029521836 0.049349263 0.1073423 0.21683249 0.36174235 0.49939302 0.5964697][0.18050282 0.16151978 0.13563639 0.11268948 0.091487572 0.062002003 0.027347254 4.5692446e-05 -0.0081940237 0.01417176 0.079590037 0.20243639 0.36812398 0.53283828 0.659095][0.1972864 0.18727215 0.17105174 0.15681738 0.13979414 0.10963946 0.071152367 0.036471229 0.016220551 0.020203203 0.061323516 0.15584649 0.29610866 0.44557068 0.56849635][0.21894127 0.2221871 0.22378291 0.23121983 0.23362847 0.2187964 0.18974371 0.15451795 0.11951063 0.090135127 0.081186123 0.11243172 0.18625809 0.27840564 0.36125717][0.22209711 0.24203104 0.26941022 0.31213406 0.35193977 0.37236562 0.37113988 0.34882569 0.30512172 0.23996095 0.17203657 0.12520406 0.11143203 0.1202389 0.13718161][0.20266096 0.23723699 0.29130974 0.37399086 0.46082994 0.53034025 0.57161975 0.57562089 0.53427857 0.44227669 0.32374239 0.20685562 0.11036948 0.035539422 -0.017349007][0.17514707 0.21158485 0.27839831 0.38722917 0.51173753 0.62660491 0.711825 0.7490176 0.72204959 0.621058 0.47426444 0.31354243 0.16113228 0.026690781 -0.077239208][0.17298278 0.18755147 0.2364022 0.33715177 0.46761993 0.6018737 0.71347928 0.7775141 0.77114475 0.6805588 0.5336768 0.36343068 0.19347675 0.038493358 -0.08267235][0.23216134 0.20098777 0.19587618 0.24559644 0.33761567 0.44989392 0.555492 0.62744093 0.63832378 0.57100117 0.44826365 0.30078909 0.15079707 0.014235238 -0.089953311][0.36013088 0.27396229 0.19415283 0.16323677 0.18078987 0.23337083 0.30055806 0.35743788 0.37483591 0.33395028 0.24961305 0.14609633 0.040753756 -0.052967273 -0.11994416][0.52044129 0.390168 0.24194264 0.12905918 0.062115651 0.03947001 0.051758762 0.078327447 0.09116888 0.072002321 0.02788442 -0.026144983 -0.0803015 -0.12608391 -0.15347871][0.64232582 0.49575016 0.30944416 0.14066164 0.0086601907 -0.076142035 -0.11213633 -0.11605348 -0.11265352 -0.11783712 -0.13027118 -0.14420491 -0.15721503 -0.16599655 -0.16524473][0.648497 0.51597857 0.33197731 0.14964993 -0.0066593937 -0.11826201 -0.17717579 -0.1976994 -0.2001621 -0.19704473 -0.19032629 -0.18102525 -0.17120439 -0.16067961 -0.14727953][0.50723308 0.4075729 0.25949189 0.10519576 -0.031224048 -0.12966397 -0.18151212 -0.19942679 -0.1988775 -0.18895157 -0.17328715 -0.15587389 -0.13945046 -0.12445063 -0.11029652][0.26862359 0.20509654 0.10934944 0.0077443318 -0.081281923 -0.14193079 -0.16843469 -0.17138588 -0.16167276 -0.14561415 -0.12730443 -0.11059079 -0.096612386 -0.084976628 -0.075552464]]...]
INFO - root - 2017-12-11 01:42:50.625896: step 74210, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 56h:46m:48s remains)
INFO - root - 2017-12-11 01:42:58.490550: step 74220, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 57h:21m:27s remains)
INFO - root - 2017-12-11 01:43:06.371211: step 74230, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.830 sec/batch; 59h:34m:37s remains)
INFO - root - 2017-12-11 01:43:13.960848: step 74240, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.752 sec/batch; 53h:56m:26s remains)
INFO - root - 2017-12-11 01:43:21.799643: step 74250, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.789 sec/batch; 56h:36m:46s remains)
INFO - root - 2017-12-11 01:43:29.655681: step 74260, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 56h:12m:08s remains)
INFO - root - 2017-12-11 01:43:37.603469: step 74270, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 58h:03m:41s remains)
INFO - root - 2017-12-11 01:43:45.503273: step 74280, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 57h:11m:43s remains)
INFO - root - 2017-12-11 01:43:53.199178: step 74290, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 55h:53m:53s remains)
INFO - root - 2017-12-11 01:44:01.069547: step 74300, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.757 sec/batch; 54h:18m:20s remains)
2017-12-11 01:44:01.942692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.058700584 -0.064275444 -0.067441322 -0.068827122 -0.067868181 -0.062380463 -0.054873966 -0.050305858 -0.052406702 -0.062480241 -0.077343076 -0.09464705 -0.11038316 -0.11990027 -0.12125163][-0.029467218 -0.037290607 -0.041693237 -0.041560795 -0.034973551 -0.017466662 0.0029340058 0.0149516 0.012241936 -0.00501058 -0.032298114 -0.066929735 -0.10052662 -0.12333272 -0.13044727][0.018478204 0.012811303 0.01253707 0.019882653 0.039217103 0.078896537 0.12168232 0.145513 0.13998552 0.1092497 0.061036639 -0.001257463 -0.064184114 -0.10986826 -0.12826391][0.078833073 0.083638452 0.097009473 0.12004647 0.16072604 0.23254393 0.30682862 0.34609565 0.33390796 0.28167486 0.20261769 0.10159072 -0.0025043183 -0.0810152 -0.11631611][0.15334421 0.17611021 0.21057069 0.25500768 0.32191727 0.43045047 0.54037881 0.596085 0.57409245 0.49473172 0.37827387 0.23123866 0.077963658 -0.04031685 -0.09669894][0.23841813 0.28399995 0.34367129 0.41171286 0.50432885 0.64500904 0.78475928 0.8520633 0.81682092 0.70808339 0.55367559 0.36169991 0.16088861 0.0038868256 -0.073795825][0.30409127 0.37139565 0.45493039 0.543875 0.65559095 0.8143425 0.96874547 1.038336 0.9885447 0.85453331 0.6709578 0.44810182 0.21638784 0.03463988 -0.057491641][0.32973233 0.4093174 0.50631332 0.60451448 0.71934658 0.87452006 1.0235727 1.08619 1.0260063 0.8797245 0.68585622 0.4567363 0.2214359 0.0372204 -0.057502415][0.31699672 0.39435321 0.48841542 0.57972932 0.67960638 0.80970436 0.93466961 0.98302007 0.91920304 0.77692646 0.59436953 0.38501626 0.17399527 0.010465867 -0.073131517][0.27086243 0.33162135 0.40620229 0.47515288 0.54492092 0.63478643 0.7227729 0.75211376 0.69113541 0.56760848 0.41532612 0.24713637 0.082520947 -0.041526232 -0.1018144][0.19277498 0.22891596 0.27542508 0.31476662 0.34899294 0.39460468 0.44224727 0.452125 0.39909765 0.30376661 0.19277449 0.076774 -0.030923814 -0.10640033 -0.13667947][0.096890338 0.10857531 0.12755436 0.13910961 0.14242515 0.15118934 0.16527578 0.1605619 0.1193161 0.05513633 -0.013403933 -0.0785768 -0.13257743 -0.16288809 -0.16508192][0.014754224 0.0089201471 0.0076389681 -0.00048285295 -0.016822262 -0.030476648 -0.036923151 -0.048895575 -0.077674568 -0.115073 -0.14955387 -0.17638102 -0.1919044 -0.19134296 -0.17545906][-0.042215895 -0.056250118 -0.067766041 -0.084681176 -0.10762172 -0.12879533 -0.14298439 -0.15620853 -0.17465082 -0.1936155 -0.20647551 -0.21086507 -0.20565648 -0.190351 -0.16795361][-0.079620428 -0.094512559 -0.10770838 -0.12487601 -0.14558992 -0.16465901 -0.1781844 -0.1887258 -0.19899482 -0.20661119 -0.20811741 -0.20254399 -0.1899704 -0.17173539 -0.15107335]]...]
INFO - root - 2017-12-11 01:44:09.890703: step 74310, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 55h:47m:28s remains)
INFO - root - 2017-12-11 01:44:17.674884: step 74320, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 57h:50m:01s remains)
INFO - root - 2017-12-11 01:44:25.368588: step 74330, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.826 sec/batch; 59h:16m:16s remains)
INFO - root - 2017-12-11 01:44:33.271138: step 74340, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.799 sec/batch; 57h:18m:38s remains)
INFO - root - 2017-12-11 01:44:41.138881: step 74350, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 55h:59m:08s remains)
INFO - root - 2017-12-11 01:44:49.044524: step 74360, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 55h:53m:26s remains)
INFO - root - 2017-12-11 01:44:56.815531: step 74370, loss = 0.72, batch loss = 0.66 (9.9 examples/sec; 0.812 sec/batch; 58h:13m:25s remains)
INFO - root - 2017-12-11 01:45:04.767621: step 74380, loss = 0.69, batch loss = 0.64 (9.6 examples/sec; 0.830 sec/batch; 59h:31m:41s remains)
INFO - root - 2017-12-11 01:45:12.712488: step 74390, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 55h:09m:47s remains)
INFO - root - 2017-12-11 01:45:20.599879: step 74400, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 54h:56m:03s remains)
2017-12-11 01:45:21.485350: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10420173 0.12838767 0.18199512 0.24980141 0.32301122 0.38508174 0.42394289 0.43516102 0.42281282 0.38830116 0.33520439 0.28183421 0.24275424 0.22384886 0.21685611][0.10497235 0.1319477 0.18492994 0.24882771 0.3160851 0.37619543 0.41875178 0.43570274 0.42698157 0.398826 0.35798159 0.31742898 0.28470868 0.26539961 0.25840247][0.10778255 0.13832846 0.18989387 0.24796866 0.30604932 0.35842249 0.39808357 0.41591597 0.41196883 0.39702404 0.37675714 0.35452908 0.33016646 0.3116881 0.30700928][0.13276704 0.17212841 0.22729312 0.28260925 0.33152112 0.37075758 0.3974686 0.40706885 0.40403539 0.40098616 0.39803189 0.38890883 0.37047958 0.35529822 0.35686588][0.17333724 0.22357008 0.28270879 0.33292171 0.36739135 0.38641256 0.39453351 0.39563668 0.39802569 0.41037917 0.42339367 0.42311022 0.40734622 0.39472523 0.40205005][0.21748206 0.27474022 0.33187175 0.37032709 0.38422921 0.38032922 0.37259188 0.37340072 0.38978675 0.42095602 0.4480826 0.45294809 0.43686989 0.42354891 0.4324972][0.25980282 0.31653914 0.36345285 0.38441786 0.37723577 0.35506427 0.33875668 0.34545541 0.37701926 0.42363 0.460176 0.4680571 0.45122358 0.43575895 0.44294897][0.3000586 0.34725791 0.37592733 0.37669754 0.35374466 0.32376093 0.30882955 0.32421097 0.36605835 0.41979226 0.45884115 0.46637544 0.44779158 0.42920193 0.43161881][0.33345076 0.36263636 0.36632192 0.34683773 0.31587946 0.29069617 0.28820547 0.31644508 0.36579359 0.42034996 0.45634964 0.45961362 0.43604806 0.41037512 0.40333733][0.3438037 0.35090923 0.32994631 0.29588076 0.26628071 0.25632492 0.27442127 0.31911194 0.37529966 0.42710328 0.45628661 0.45351234 0.42522889 0.39338532 0.37782419][0.33486512 0.32310554 0.2859264 0.2470144 0.22596841 0.23412299 0.27188864 0.32958749 0.38777423 0.43134269 0.44842744 0.43720204 0.40646791 0.37520868 0.36043391][0.31449613 0.29335925 0.25348651 0.22108293 0.21422124 0.23864962 0.28861898 0.34836736 0.39715755 0.42416468 0.42495736 0.40568262 0.37729657 0.35590273 0.35284179][0.28405854 0.26345053 0.23251279 0.21496068 0.22581983 0.26529014 0.32203177 0.37497938 0.4047887 0.4097136 0.39452869 0.37072819 0.34870461 0.34129745 0.3538782][0.24563561 0.23176397 0.21551192 0.2160254 0.24482842 0.29635811 0.35362455 0.39287826 0.39984164 0.38416272 0.35872102 0.33649155 0.32410097 0.33077571 0.35763526][0.2059875 0.20224744 0.20313172 0.22184508 0.26432773 0.31928718 0.3671622 0.38757887 0.37409502 0.34517297 0.31744912 0.30199495 0.30124241 0.32163802 0.362828]]...]
INFO - root - 2017-12-11 01:45:29.263264: step 74410, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 55h:38m:25s remains)
INFO - root - 2017-12-11 01:45:36.962594: step 74420, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 57h:57m:43s remains)
INFO - root - 2017-12-11 01:45:45.032060: step 74430, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.824 sec/batch; 59h:04m:35s remains)
INFO - root - 2017-12-11 01:45:52.887619: step 74440, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.786 sec/batch; 56h:19m:05s remains)
INFO - root - 2017-12-11 01:46:00.549173: step 74450, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 55h:50m:04s remains)
INFO - root - 2017-12-11 01:46:08.393277: step 74460, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 56h:26m:31s remains)
INFO - root - 2017-12-11 01:46:16.238389: step 74470, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.781 sec/batch; 55h:58m:47s remains)
INFO - root - 2017-12-11 01:46:24.204317: step 74480, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 54h:25m:44s remains)
INFO - root - 2017-12-11 01:46:32.021224: step 74490, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 55h:51m:56s remains)
INFO - root - 2017-12-11 01:46:39.930142: step 74500, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.792 sec/batch; 56h:45m:50s remains)
2017-12-11 01:46:40.803385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.027668642 -0.022658754 -0.022989009 -0.029629087 -0.033986557 -0.037360866 -0.042686526 -0.053031407 -0.065683693 -0.071668357 -0.072178781 -0.06861981 -0.061572082 -0.053502209 -0.053025171][0.027353991 0.041559167 0.046966113 0.041567653 0.038919482 0.038010005 0.03447932 0.023015495 0.0043346877 -0.008613158 -0.014675334 -0.012446882 -0.0036058114 0.0080467612 0.0093264394][0.10519498 0.13073324 0.14221419 0.13890131 0.14164847 0.14990129 0.15674943 0.15177634 0.12899414 0.10404617 0.085965432 0.083223224 0.093111239 0.1090368 0.11172217][0.18810129 0.22642078 0.24442331 0.24542026 0.25977319 0.28845307 0.31947839 0.332072 0.30862325 0.26593903 0.22499754 0.20559521 0.20802507 0.22324543 0.22499162][0.25542566 0.30750582 0.33291298 0.34168804 0.37462598 0.43344447 0.49841323 0.53436023 0.50985396 0.44256824 0.36711293 0.31928095 0.30659682 0.31712919 0.31577405][0.32681534 0.39538547 0.42960835 0.44805679 0.501826 0.59221822 0.68976 0.74458194 0.71315736 0.61688077 0.50432295 0.42831251 0.40334424 0.41230878 0.408166][0.390618 0.47632387 0.52235615 0.55376416 0.62779176 0.74228632 0.8599323 0.92048585 0.87500536 0.75237709 0.61298394 0.52114379 0.49238843 0.50296885 0.49461645][0.41584733 0.5157721 0.57673573 0.62407321 0.71385324 0.84000736 0.96099812 1.0124978 0.94958997 0.81236416 0.666435 0.575209 0.54945439 0.56002307 0.54359692][0.39267102 0.49734849 0.56950992 0.62981176 0.72592509 0.84821415 0.9557935 0.98865396 0.91363662 0.78067577 0.65071684 0.57373416 0.55221206 0.55606264 0.52620739][0.33272418 0.42790908 0.49848458 0.5593273 0.64618695 0.74653214 0.82487267 0.8341018 0.75732952 0.64819717 0.55391246 0.50320232 0.48826066 0.48325089 0.44137821][0.24632658 0.32134584 0.37974238 0.43103397 0.49846944 0.56758523 0.61144835 0.60079151 0.5339157 0.46043918 0.40903822 0.38681996 0.37762204 0.36346981 0.31395793][0.11945901 0.16750184 0.20939054 0.24905534 0.29758611 0.33976036 0.3579855 0.33867621 0.29003987 0.25149086 0.23471323 0.23075484 0.22092713 0.19747508 0.14701578][-0.017891584 0.0041480716 0.029948581 0.059223123 0.093493745 0.11883252 0.12483013 0.10753313 0.078066081 0.0639182 0.065569565 0.066930026 0.052610256 0.023887726 -0.019648219][-0.10837702 -0.10508048 -0.093374766 -0.075322196 -0.053496417 -0.038381413 -0.035695959 -0.045631696 -0.059484426 -0.060997777 -0.054576203 -0.055182055 -0.072212651 -0.099280812 -0.13092218][-0.14850165 -0.15504117 -0.15341474 -0.14582399 -0.13467057 -0.1263925 -0.12419239 -0.12775761 -0.13228181 -0.1297711 -0.12485821 -0.1274333 -0.14177768 -0.16123997 -0.18000181]]...]
INFO - root - 2017-12-11 01:46:48.563747: step 74510, loss = 0.68, batch loss = 0.63 (9.9 examples/sec; 0.804 sec/batch; 57h:38m:21s remains)
INFO - root - 2017-12-11 01:46:56.537428: step 74520, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 56h:45m:56s remains)
INFO - root - 2017-12-11 01:47:04.119068: step 74530, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 56h:28m:19s remains)
INFO - root - 2017-12-11 01:47:12.000485: step 74540, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 56h:50m:22s remains)
INFO - root - 2017-12-11 01:47:19.864377: step 74550, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.785 sec/batch; 56h:13m:28s remains)
INFO - root - 2017-12-11 01:47:27.706728: step 74560, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 55h:57m:42s remains)
INFO - root - 2017-12-11 01:47:35.641222: step 74570, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 58h:13m:06s remains)
INFO - root - 2017-12-11 01:47:43.563343: step 74580, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 58h:14m:07s remains)
INFO - root - 2017-12-11 01:47:51.571752: step 74590, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 57h:14m:37s remains)
INFO - root - 2017-12-11 01:47:59.242955: step 74600, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 54h:29m:15s remains)
2017-12-11 01:48:00.165946: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14606294 0.12618795 0.11231904 0.10094745 0.085903078 0.068032779 0.054043103 0.050231494 0.058099445 0.071262665 0.080538683 0.087789945 0.092588134 0.083237432 0.056346189][0.11605798 0.096877687 0.083492987 0.073132128 0.06040056 0.046131175 0.036358163 0.034984116 0.042318635 0.054172702 0.063119471 0.070448011 0.075053826 0.066743806 0.043075107][0.077776782 0.066089004 0.060337249 0.057293776 0.053136054 0.048909139 0.047961567 0.050453275 0.05506862 0.061488993 0.065498389 0.067314148 0.065544561 0.052822705 0.029110361][0.05362488 0.05614052 0.064541832 0.074580938 0.083748691 0.092634827 0.10116244 0.10575445 0.10418264 0.099830925 0.092801757 0.08374811 0.070807375 0.049200013 0.021533167][0.063169412 0.084194526 0.11016575 0.13590465 0.15865831 0.17746618 0.18979988 0.19072306 0.17821273 0.15865 0.13651808 0.11392002 0.088832885 0.057448234 0.023460435][0.10758483 0.1471871 0.18867373 0.22693205 0.25760305 0.27795848 0.28544965 0.27696797 0.25092793 0.21564922 0.17928953 0.14614469 0.11302722 0.074233584 0.033545215][0.16666639 0.22149573 0.27315876 0.31751737 0.34880525 0.36325577 0.36022329 0.34034762 0.30287066 0.2564339 0.210787 0.17215413 0.13596314 0.093384936 0.047299694][0.21725477 0.28029555 0.33452818 0.37699866 0.40195006 0.40625656 0.39250916 0.36588141 0.32617089 0.27917239 0.23335128 0.19575663 0.16064119 0.11609396 0.064428322][0.24500608 0.30759281 0.35615543 0.38980177 0.40448368 0.39915088 0.37971616 0.35575831 0.32668093 0.29205072 0.25605175 0.2252306 0.1936025 0.14690869 0.087878227][0.24514119 0.29795545 0.33394709 0.35530117 0.36113366 0.35232148 0.33605874 0.3245931 0.31605846 0.30221364 0.28144294 0.25903231 0.23018448 0.17956081 0.11149673][0.21983089 0.25689381 0.27763093 0.28752649 0.28910244 0.28374761 0.27763876 0.28382033 0.29866987 0.30665386 0.30094758 0.28562564 0.25796798 0.20292665 0.12689631][0.17425586 0.19459835 0.20182343 0.20402247 0.20627563 0.20893346 0.2152753 0.23770583 0.27086881 0.29443863 0.29839551 0.28629982 0.25838512 0.20131996 0.12299219][0.11336336 0.12025943 0.11895694 0.11776163 0.12259821 0.13255347 0.14801721 0.17978227 0.22115813 0.25044665 0.25718838 0.24540417 0.21834761 0.16493353 0.093379095][0.046107158 0.04445425 0.03972375 0.037946533 0.044620235 0.05805175 0.0766224 0.1087707 0.14760859 0.17379165 0.17838831 0.16597548 0.14214142 0.098681785 0.042651102][-0.015507093 -0.021367282 -0.026111605 -0.0273184 -0.020662954 -0.0079761827 0.00815386 0.033779733 0.063124858 0.081477344 0.082443655 0.070434555 0.052062426 0.022237977 -0.014130492]]...]
INFO - root - 2017-12-11 01:48:07.766611: step 74610, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 55h:12m:38s remains)
INFO - root - 2017-12-11 01:48:15.526317: step 74620, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.767 sec/batch; 54h:57m:54s remains)
INFO - root - 2017-12-11 01:48:23.413941: step 74630, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.780 sec/batch; 55h:50m:59s remains)
INFO - root - 2017-12-11 01:48:31.214694: step 74640, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.791 sec/batch; 56h:41m:06s remains)
INFO - root - 2017-12-11 01:48:39.109846: step 74650, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.794 sec/batch; 56h:51m:21s remains)
INFO - root - 2017-12-11 01:48:46.951059: step 74660, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 56h:54m:29s remains)
INFO - root - 2017-12-11 01:48:54.907020: step 74670, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 57h:07m:17s remains)
INFO - root - 2017-12-11 01:49:02.800130: step 74680, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 56h:10m:16s remains)
INFO - root - 2017-12-11 01:49:10.402967: step 74690, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.789 sec/batch; 56h:28m:16s remains)
INFO - root - 2017-12-11 01:49:18.091789: step 74700, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.755 sec/batch; 54h:04m:03s remains)
2017-12-11 01:49:18.989010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.084114611 -0.09159863 -0.095389321 -0.0975062 -0.097075291 -0.093527213 -0.087554313 -0.082778729 -0.080674462 -0.081223741 -0.083023533 -0.086979091 -0.093821213 -0.10037312 -0.10418702][-0.059216153 -0.066976465 -0.072539136 -0.076136939 -0.075631291 -0.068598688 -0.056084789 -0.045376871 -0.039838929 -0.039885536 -0.043545175 -0.053597324 -0.071123384 -0.089095443 -0.10169699][-0.0020745697 -0.007856505 -0.015806027 -0.022049611 -0.022483222 -0.011267375 0.010847815 0.030290734 0.040436275 0.040007763 0.032142807 0.011379497 -0.02407803 -0.061228335 -0.088773489][0.094450332 0.093103319 0.08202254 0.072050981 0.070661955 0.087277457 0.12128518 0.1506909 0.16453981 0.16040628 0.14373942 0.10647249 0.045550857 -0.018784512 -0.067317732][0.22508532 0.23197524 0.21906777 0.20610468 0.20461622 0.22672248 0.27088496 0.30646169 0.31889611 0.30497313 0.27300721 0.21383651 0.12333198 0.028621592 -0.042792052][0.35802859 0.37739009 0.36783773 0.35585061 0.3569811 0.38437143 0.43382734 0.46802264 0.4705945 0.43930966 0.3859213 0.30223393 0.1842943 0.063608833 -0.026118211][0.46618754 0.49974808 0.49662155 0.48732433 0.49177691 0.52245903 0.57063574 0.59467447 0.57888228 0.52501297 0.44829336 0.34335586 0.2069453 0.071611114 -0.026074328][0.54273134 0.58841562 0.59036392 0.5797506 0.58093524 0.60665452 0.64438409 0.64985532 0.61117685 0.53675449 0.44431162 0.32995087 0.19031809 0.055615366 -0.038636673][0.58040261 0.63566428 0.64100891 0.62407047 0.61154789 0.61870867 0.63416821 0.61603081 0.55767596 0.47311643 0.37959009 0.27231941 0.14564297 0.025041651 -0.05740587][0.58490735 0.64937812 0.65783781 0.63092327 0.59494263 0.5704928 0.5541718 0.51180947 0.44200057 0.36115566 0.28167498 0.19483265 0.091755286 -0.007618424 -0.07500118][0.56444317 0.63383442 0.64316154 0.60556477 0.54467988 0.48742378 0.44186866 0.38320771 0.31329551 0.24727876 0.19125317 0.13045043 0.052301425 -0.027395098 -0.082631916][0.51779521 0.58251232 0.58678269 0.53935736 0.45971027 0.37896204 0.31606531 0.25415388 0.19540697 0.15027918 0.11949211 0.08342988 0.026778344 -0.037186272 -0.083691753][0.43148708 0.48105994 0.47573903 0.42282471 0.33760896 0.25086766 0.18743077 0.13638963 0.097229011 0.073571391 0.062991418 0.044829126 0.0039840089 -0.047458377 -0.086304933][0.29868183 0.32907432 0.31599662 0.26627827 0.19189392 0.11807539 0.068669438 0.036808647 0.018291829 0.011022072 0.011881256 0.0035727578 -0.025871728 -0.0656012 -0.09524481][0.15044218 0.16289847 0.14650252 0.10673735 0.053109922 0.0031570464 -0.025666788 -0.038369212 -0.041280143 -0.039260365 -0.034245495 -0.038346115 -0.05899135 -0.087130353 -0.10669341]]...]
INFO - root - 2017-12-11 01:49:26.860180: step 74710, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.759 sec/batch; 54h:20m:23s remains)
INFO - root - 2017-12-11 01:49:34.639439: step 74720, loss = 0.70, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 54h:44m:30s remains)
INFO - root - 2017-12-11 01:49:42.522259: step 74730, loss = 0.67, batch loss = 0.62 (10.1 examples/sec; 0.791 sec/batch; 56h:36m:54s remains)
INFO - root - 2017-12-11 01:49:50.340604: step 74740, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 56h:36m:13s remains)
INFO - root - 2017-12-11 01:49:58.179450: step 74750, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.768 sec/batch; 54h:57m:52s remains)
INFO - root - 2017-12-11 01:50:06.133401: step 74760, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.802 sec/batch; 57h:27m:13s remains)
INFO - root - 2017-12-11 01:50:13.664417: step 74770, loss = 0.69, batch loss = 0.63 (12.2 examples/sec; 0.656 sec/batch; 46h:58m:33s remains)
INFO - root - 2017-12-11 01:50:21.582239: step 74780, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.821 sec/batch; 58h:47m:24s remains)
INFO - root - 2017-12-11 01:50:29.587747: step 74790, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 55h:40m:23s remains)
INFO - root - 2017-12-11 01:50:37.515671: step 74800, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 55h:44m:31s remains)
2017-12-11 01:50:38.356609: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18650384 0.21173787 0.22009476 0.21849819 0.20832075 0.1888383 0.16534826 0.13861544 0.11818063 0.1113506 0.11850058 0.13379303 0.13927026 0.12325844 0.07969062][0.30890262 0.34293002 0.34977376 0.34309265 0.32852238 0.30498749 0.27811733 0.24901503 0.22969599 0.22853947 0.24422829 0.26821494 0.27776957 0.25673702 0.19499071][0.41017458 0.44743332 0.44967693 0.43823239 0.42354625 0.40269008 0.3777526 0.34929982 0.33090949 0.33164448 0.35057938 0.37829506 0.39082468 0.36635122 0.29086432][0.45421231 0.49175411 0.48947981 0.47620925 0.46566167 0.45329288 0.43537691 0.41088352 0.39305589 0.39049107 0.4042668 0.42841133 0.44080922 0.4141916 0.33125013][0.45181942 0.49468026 0.49857709 0.49419174 0.49525607 0.49693796 0.49075893 0.47294694 0.45374486 0.44127634 0.44090134 0.45316365 0.45998088 0.42898226 0.34037018][0.45846865 0.51484609 0.53352189 0.54326957 0.55670363 0.57072425 0.57502943 0.56198591 0.53798741 0.51312709 0.49655765 0.49485147 0.49353963 0.45649013 0.36205849][0.47679049 0.54634 0.580496 0.60424781 0.62908465 0.65347219 0.66643566 0.65614468 0.62528044 0.59013027 0.5620966 0.55128676 0.54485619 0.50522733 0.40891612][0.48866624 0.56083685 0.59958243 0.62717581 0.65544963 0.68517661 0.70385635 0.69576955 0.66088229 0.62286639 0.59302676 0.58074832 0.57490295 0.53918844 0.44761854][0.46674475 0.52814645 0.55702168 0.57507324 0.5962531 0.62394232 0.64423418 0.63996816 0.60946846 0.58109844 0.56301087 0.56014496 0.56199837 0.53558326 0.45340714][0.40383744 0.44681612 0.45890766 0.46090668 0.46911237 0.48894545 0.50687504 0.5062775 0.48542607 0.47303206 0.47337192 0.4861941 0.49985906 0.48478439 0.41393417][0.29996514 0.32240015 0.31700218 0.30299067 0.29739794 0.30754256 0.32180685 0.32453912 0.31350863 0.31424227 0.32963571 0.35672066 0.382785 0.38098609 0.326041][0.15747225 0.16072504 0.14266278 0.11882387 0.10490948 0.10919481 0.12273616 0.13056211 0.12840986 0.13580033 0.15679373 0.18946733 0.22276777 0.2331595 0.19805136][0.021991692 0.01270301 -0.010187905 -0.036140453 -0.052722163 -0.052110735 -0.04021683 -0.029627707 -0.026211729 -0.01729567 0.0019786893 0.0313775 0.063083418 0.078757674 0.060728814][-0.068270639 -0.083393253 -0.10444186 -0.12608403 -0.14039649 -0.14214608 -0.1346197 -0.12602369 -0.12086594 -0.11191765 -0.095863335 -0.072892845 -0.048476063 -0.034689784 -0.042608246][-0.11557078 -0.13162242 -0.14731133 -0.16164368 -0.17083232 -0.17272419 -0.16916303 -0.16437015 -0.1602623 -0.15284769 -0.14069566 -0.12477706 -0.10897695 -0.10001133 -0.1032307]]...]
INFO - root - 2017-12-11 01:50:46.225133: step 74810, loss = 0.73, batch loss = 0.67 (10.1 examples/sec; 0.793 sec/batch; 56h:43m:41s remains)
INFO - root - 2017-12-11 01:50:54.115024: step 74820, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.786 sec/batch; 56h:14m:59s remains)
INFO - root - 2017-12-11 01:51:01.964624: step 74830, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.782 sec/batch; 55h:58m:49s remains)
INFO - root - 2017-12-11 01:51:09.904686: step 74840, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.790 sec/batch; 56h:31m:33s remains)
INFO - root - 2017-12-11 01:51:17.610717: step 74850, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 56h:10m:58s remains)
INFO - root - 2017-12-11 01:51:25.379811: step 74860, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.762 sec/batch; 54h:31m:52s remains)
INFO - root - 2017-12-11 01:51:33.257935: step 74870, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 56h:03m:03s remains)
INFO - root - 2017-12-11 01:51:41.189413: step 74880, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 56h:47m:41s remains)
INFO - root - 2017-12-11 01:51:49.079263: step 74890, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 55h:58m:37s remains)
INFO - root - 2017-12-11 01:51:56.859580: step 74900, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.758 sec/batch; 54h:16m:19s remains)
2017-12-11 01:51:57.733872: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1063868 0.109486 0.10724843 0.10261419 0.098573856 0.10131049 0.11251681 0.12849343 0.14443652 0.15367848 0.15408579 0.15086706 0.14815883 0.14664628 0.14365476][0.11018374 0.10495052 0.096308708 0.091575824 0.093388081 0.10593896 0.12980197 0.15737084 0.17805506 0.18249217 0.17026798 0.15044828 0.13020824 0.11491364 0.10608747][0.14780781 0.13238266 0.11383649 0.10624808 0.11131874 0.1309292 0.1642628 0.19999799 0.22166124 0.21810539 0.19170643 0.15508831 0.11700642 0.086551748 0.069516681][0.21302679 0.19042097 0.16261443 0.15156712 0.15804429 0.1818535 0.22101463 0.2611028 0.28105575 0.26918361 0.23121968 0.18250349 0.13003896 0.084972247 0.057783548][0.26428357 0.24137382 0.21112694 0.20100437 0.21091174 0.23885264 0.28233081 0.3247427 0.34186116 0.32228175 0.27592835 0.2207562 0.16012019 0.10552274 0.07098189][0.29045457 0.2727384 0.24600641 0.23966992 0.25274289 0.2818827 0.32536805 0.36675122 0.38036165 0.35518891 0.30558893 0.25224808 0.19416884 0.14050886 0.10579949][0.2927815 0.28427988 0.26530895 0.26406416 0.27882832 0.30601305 0.34562394 0.3827883 0.39204153 0.36319789 0.31463131 0.27024317 0.22527058 0.18363257 0.15625465][0.26925644 0.27040634 0.26194307 0.26722512 0.2843706 0.31018 0.3456651 0.37790284 0.38410708 0.35503444 0.31153005 0.27993235 0.25282398 0.22721964 0.20789155][0.22630502 0.23557916 0.23737067 0.24917671 0.26929361 0.29504776 0.3278147 0.35660586 0.36258289 0.33773965 0.30273095 0.28482568 0.27490398 0.26468641 0.25235862][0.18463185 0.19714428 0.20426765 0.21896739 0.24104555 0.26812312 0.30069068 0.3286083 0.33670497 0.31832272 0.29268709 0.28737557 0.29275745 0.29736292 0.29268926][0.15882413 0.16722268 0.17215389 0.18512528 0.20716503 0.23520729 0.26839662 0.29682857 0.30843657 0.29808968 0.28347072 0.29215783 0.31348509 0.33318087 0.33635139][0.15287443 0.15250942 0.15208039 0.16303603 0.1861691 0.21676676 0.25215766 0.28248048 0.29833722 0.29658145 0.293794 0.31657034 0.35242236 0.38384297 0.39115998][0.16026206 0.15311605 0.14840791 0.15791696 0.18080211 0.2109677 0.24525034 0.2751537 0.29439315 0.30322641 0.31674507 0.35720372 0.40743679 0.4471488 0.45508683][0.18344083 0.17437313 0.16704509 0.17310499 0.19074939 0.21451949 0.24222636 0.26706773 0.28637031 0.30479273 0.33553708 0.39288098 0.45429477 0.49896505 0.50659704][0.21705741 0.20854771 0.19797853 0.19693595 0.20468692 0.21869813 0.23780821 0.25570315 0.27259716 0.29670331 0.33853921 0.4043318 0.46865305 0.5123837 0.51770204]]...]
INFO - root - 2017-12-11 01:52:05.666335: step 74910, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 58h:08m:46s remains)
INFO - root - 2017-12-11 01:52:13.526931: step 74920, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 55h:49m:31s remains)
INFO - root - 2017-12-11 01:52:21.300417: step 74930, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 55h:47m:12s remains)
INFO - root - 2017-12-11 01:52:29.259481: step 74940, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 55h:59m:55s remains)
INFO - root - 2017-12-11 01:52:37.015812: step 74950, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 57h:34m:30s remains)
INFO - root - 2017-12-11 01:52:44.975808: step 74960, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.783 sec/batch; 56h:01m:33s remains)
INFO - root - 2017-12-11 01:52:52.798350: step 74970, loss = 0.73, batch loss = 0.67 (10.4 examples/sec; 0.767 sec/batch; 54h:50m:50s remains)
INFO - root - 2017-12-11 01:53:00.716369: step 74980, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 57h:00m:01s remains)
INFO - root - 2017-12-11 01:53:08.576696: step 74990, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.757 sec/batch; 54h:07m:30s remains)
INFO - root - 2017-12-11 01:53:16.457652: step 75000, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 55h:58m:05s remains)
2017-12-11 01:53:17.264515: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.014285482 0.042631108 0.070138872 0.086371891 0.08806178 0.080958508 0.066741385 0.055590138 0.052821916 0.057619918 0.063244082 0.062112659 0.058179203 0.057210367 0.061123114][0.056191668 0.09927164 0.13978142 0.16372922 0.1686358 0.16319385 0.1478986 0.13435756 0.13033511 0.13776186 0.14637063 0.14386268 0.13505632 0.13016807 0.13292848][0.1045578 0.16520575 0.2206682 0.25323084 0.2622093 0.25977412 0.24429186 0.22772205 0.22106038 0.22959948 0.24017225 0.23519716 0.21957344 0.20862342 0.20846748][0.14724015 0.2251863 0.29529646 0.33601144 0.3490082 0.34998196 0.33441919 0.3139638 0.30353945 0.31223723 0.32393429 0.31592068 0.29268992 0.27410305 0.26880938][0.17307262 0.26539844 0.34755531 0.39492866 0.41229749 0.41769618 0.40345904 0.37968919 0.36560535 0.37369367 0.38572887 0.375193 0.34588405 0.32071829 0.30955604][0.1800421 0.28124577 0.36998481 0.42021695 0.44062871 0.45015168 0.43890235 0.41458705 0.39999372 0.41002062 0.42488509 0.41615632 0.38567734 0.358059 0.34343338][0.17820626 0.28327265 0.37447187 0.42553779 0.44800135 0.45994809 0.45092812 0.42698643 0.41332307 0.42629281 0.44522408 0.44169363 0.41534436 0.39107308 0.37771168][0.16602549 0.27002904 0.36089829 0.41385654 0.44045803 0.45518753 0.44815317 0.42518142 0.41236037 0.42556354 0.44508272 0.44572547 0.42585164 0.409054 0.40139198][0.14701228 0.24694605 0.33575806 0.39151829 0.42363548 0.44080183 0.4342691 0.41140875 0.39827129 0.40923569 0.42582712 0.42770943 0.412871 0.40315047 0.40184653][0.12662929 0.21926194 0.30305189 0.35925657 0.3939926 0.40993819 0.4008497 0.37635046 0.36139053 0.36839071 0.38105211 0.38365993 0.37310421 0.36905056 0.3736712][0.1040447 0.18515322 0.25905037 0.310392 0.34162948 0.35110447 0.33608133 0.30830526 0.29011402 0.29232168 0.30161849 0.30561697 0.29947487 0.2999925 0.30960888][0.071315959 0.13720137 0.19721515 0.23945004 0.2635352 0.26558369 0.24516432 0.21448915 0.19233021 0.18814045 0.19211872 0.19478653 0.19088963 0.19334403 0.20481126][0.029133329 0.077100947 0.12030675 0.1504198 0.16598786 0.16264698 0.140499 0.10996701 0.085936129 0.075839311 0.0735101 0.072543427 0.068381771 0.069859721 0.079953745][-0.010057564 0.02003892 0.045850493 0.061897289 0.067497358 0.059414286 0.0377553 0.010183468 -0.012115208 -0.02416241 -0.029267335 -0.031833094 -0.035660602 -0.03553151 -0.028128816][-0.045972008 -0.032895908 -0.022694131 -0.019216714 -0.022162136 -0.033208359 -0.051928475 -0.073290691 -0.090092361 -0.099810034 -0.10367737 -0.10470716 -0.10632174 -0.10627842 -0.10130772]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 01:53:24.964144: step 75010, loss = 0.68, batch loss = 0.62 (9.6 examples/sec; 0.832 sec/batch; 59h:30m:13s remains)
INFO - root - 2017-12-11 01:53:32.874397: step 75020, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 57h:34m:44s remains)
INFO - root - 2017-12-11 01:53:40.687184: step 75030, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 54h:54m:10s remains)
INFO - root - 2017-12-11 01:53:48.393327: step 75040, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 55h:47m:11s remains)
INFO - root - 2017-12-11 01:53:56.250278: step 75050, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 54h:25m:42s remains)
INFO - root - 2017-12-11 01:54:04.127766: step 75060, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.822 sec/batch; 58h:47m:49s remains)
INFO - root - 2017-12-11 01:54:12.062210: step 75070, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.787 sec/batch; 56h:16m:06s remains)
INFO - root - 2017-12-11 01:54:19.937853: step 75080, loss = 0.68, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 55h:07m:11s remains)
INFO - root - 2017-12-11 01:54:27.552784: step 75090, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.761 sec/batch; 54h:26m:32s remains)
INFO - root - 2017-12-11 01:54:35.492128: step 75100, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 56h:33m:04s remains)
2017-12-11 01:54:36.408797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12482545 -0.13780157 -0.1383532 -0.13172382 -0.11570084 -0.087893561 -0.048830181 0.0001234417 0.047277007 0.08493039 0.11033575 0.12124453 0.11547844 0.094811082 0.0649679][-0.16730583 -0.1612632 -0.14363325 -0.12138218 -0.091638528 -0.051484417 -0.0037907811 0.049347766 0.096197695 0.12919654 0.14560133 0.14614612 0.13087772 0.10228563 0.067267887][-0.15621535 -0.13986684 -0.11410621 -0.084396794 -0.046597175 0.0014130091 0.0544448 0.10938619 0.15467505 0.18225248 0.18842551 0.17654991 0.14907652 0.11037274 0.069046073][-0.10172024 -0.080821492 -0.05262598 -0.0194619 0.023943204 0.076643266 0.13176829 0.18509778 0.22649187 0.24717061 0.24139057 0.21435198 0.17155662 0.12033764 0.071174622][-0.02343264 0.0022752457 0.034665789 0.074914418 0.12824833 0.18724935 0.24310523 0.29070023 0.32289973 0.33095384 0.3073431 0.25984284 0.198247 0.13299635 0.075527787][0.055470873 0.08899869 0.13084646 0.18525416 0.25577548 0.32560036 0.38236603 0.42051467 0.43670496 0.4236899 0.37480453 0.3023316 0.22120366 0.14338174 0.079287238][0.11598515 0.15858957 0.21213154 0.28346992 0.37375954 0.45675948 0.51499605 0.54263163 0.53995311 0.50248975 0.42715603 0.3323077 0.23671944 0.15211216 0.086337611][0.14808847 0.19608717 0.256742 0.3390649 0.44220036 0.53380424 0.59205228 0.61135137 0.59413016 0.53871804 0.44673729 0.34127882 0.24257088 0.16036041 0.099332906][0.14325672 0.18863563 0.24630064 0.32641914 0.42733982 0.51659828 0.57153529 0.58710182 0.5656693 0.50707871 0.41671219 0.31847352 0.23072159 0.16033952 0.10955426][0.1007335 0.13433968 0.17798056 0.24158052 0.32431281 0.39977 0.44848889 0.46556827 0.45117033 0.40488973 0.33346283 0.25759572 0.19144349 0.13924 0.10195183][0.033910364 0.050512359 0.074073032 0.11268043 0.16716948 0.22063605 0.25964972 0.27927727 0.27673262 0.25045481 0.20633423 0.15935428 0.11868026 0.086725451 0.064269014][-0.0358824 -0.035132274 -0.03004786 -0.015383197 0.010845637 0.040534992 0.066690892 0.084981926 0.091121964 0.082785971 0.063757211 0.042674556 0.024588194 0.0107386 0.001790126][-0.088755183 -0.097777 -0.10391805 -0.10486892 -0.098811485 -0.088346153 -0.075819157 -0.064066105 -0.056687914 -0.055565342 -0.058969304 -0.06319467 -0.066152357 -0.067564949 -0.067311652][-0.11505507 -0.12671565 -0.13596141 -0.14295085 -0.14689343 -0.14806662 -0.14649561 -0.14320114 -0.13987999 -0.13720685 -0.13466391 -0.13125789 -0.12693648 -0.12216024 -0.11739717][-0.11835129 -0.12729564 -0.13401118 -0.14036708 -0.1466103 -0.15267968 -0.15788405 -0.16165395 -0.16371503 -0.1635218 -0.16087171 -0.15593201 -0.14971603 -0.14329037 -0.13751386]]...]
INFO - root - 2017-12-11 01:54:44.322938: step 75110, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.766 sec/batch; 54h:46m:08s remains)
INFO - root - 2017-12-11 01:54:52.330800: step 75120, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.806 sec/batch; 57h:37m:21s remains)
INFO - root - 2017-12-11 01:54:59.996077: step 75130, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 55h:52m:44s remains)
INFO - root - 2017-12-11 01:55:07.949874: step 75140, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 56h:54m:46s remains)
INFO - root - 2017-12-11 01:55:15.851949: step 75150, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.749 sec/batch; 53h:31m:07s remains)
INFO - root - 2017-12-11 01:55:23.816351: step 75160, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.810 sec/batch; 57h:53m:40s remains)
INFO - root - 2017-12-11 01:55:31.531199: step 75170, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.771 sec/batch; 55h:07m:38s remains)
INFO - root - 2017-12-11 01:55:39.463842: step 75180, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.822 sec/batch; 58h:44m:53s remains)
INFO - root - 2017-12-11 01:55:47.246963: step 75190, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 55h:37m:44s remains)
INFO - root - 2017-12-11 01:55:55.004762: step 75200, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.759 sec/batch; 54h:14m:19s remains)
2017-12-11 01:55:55.814775: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13192706 0.1869904 0.23383796 0.25940749 0.25847125 0.23484923 0.19785337 0.16010565 0.12947236 0.12300793 0.13933508 0.16425067 0.18188567 0.18680111 0.1747684][0.10464375 0.15791403 0.20856202 0.24321358 0.25447828 0.24293536 0.21753341 0.18835038 0.16180867 0.16264713 0.19043347 0.22627942 0.24864717 0.25249371 0.23537824][0.069252476 0.12054898 0.1752194 0.21968608 0.24477044 0.2487686 0.23889244 0.22167924 0.2020912 0.2129596 0.25696933 0.30988544 0.34332642 0.34870115 0.32548055][0.042234469 0.096314244 0.15863153 0.21503842 0.25457892 0.27398241 0.2789591 0.27411738 0.26218036 0.28050485 0.33776948 0.40715158 0.45287007 0.45842049 0.42479995][0.028355408 0.090212569 0.164529 0.23567419 0.28942123 0.32116574 0.33626106 0.34043446 0.33333907 0.35135671 0.41155836 0.48961964 0.54350954 0.54618078 0.49999666][0.023591828 0.095673218 0.18375692 0.26986915 0.33492151 0.37279317 0.39053792 0.39745963 0.39004368 0.39968 0.45242134 0.52995449 0.58590186 0.58391458 0.52776086][0.025120286 0.10568783 0.2038644 0.2991415 0.36794412 0.4029741 0.41371581 0.41465273 0.40189633 0.40011981 0.44035059 0.51038319 0.563157 0.55799019 0.49895778][0.029246576 0.11088967 0.20877427 0.3012189 0.36257112 0.38494936 0.38046509 0.36918521 0.34938985 0.33827975 0.36630353 0.42560595 0.47215092 0.46667507 0.41315576][0.030483805 0.1024449 0.18651441 0.26241022 0.3062253 0.31088936 0.28991428 0.26676923 0.24298188 0.22962596 0.25117618 0.30098134 0.3410818 0.33896539 0.29850379][0.02330195 0.075452819 0.13464566 0.18440439 0.20627889 0.19538532 0.16475457 0.13703834 0.11662135 0.11010623 0.13275641 0.17704292 0.21316151 0.21793857 0.19517927][0.010771145 0.038004428 0.067751266 0.089515567 0.09257099 0.07461641 0.046054073 0.024456605 0.015013154 0.021151323 0.048510484 0.087966457 0.11931129 0.12967271 0.12281899][-0.0014235326 0.0027011118 0.0066781114 0.0067747184 -0.00028580666 -0.014675269 -0.03062846 -0.038107492 -0.033198271 -0.014561847 0.016129781 0.0489217 0.072422057 0.082724243 0.084224291][-0.012217321 -0.024734538 -0.037280671 -0.047933139 -0.054692492 -0.057076927 -0.054731838 -0.045168422 -0.02694946 8.5531239e-05 0.030862901 0.055560015 0.068936408 0.073097423 0.074373491][-0.022453239 -0.043230373 -0.061053436 -0.0709551 -0.06840004 -0.054190844 -0.033503205 -0.0095106419 0.017484289 0.048243068 0.076973341 0.093920223 0.096900135 0.091470078 0.086713381][-0.032786835 -0.054108832 -0.0673934 -0.067767993 -0.050862093 -0.020632781 0.013310421 0.045380365 0.075065315 0.10501659 0.13021064 0.14105497 0.1358992 0.12140975 0.1090666]]...]
INFO - root - 2017-12-11 01:56:03.734137: step 75210, loss = 0.72, batch loss = 0.66 (10.4 examples/sec; 0.772 sec/batch; 55h:09m:00s remains)
INFO - root - 2017-12-11 01:56:11.423337: step 75220, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 55h:24m:49s remains)
INFO - root - 2017-12-11 01:56:19.146633: step 75230, loss = 0.71, batch loss = 0.65 (10.8 examples/sec; 0.737 sec/batch; 52h:41m:58s remains)
INFO - root - 2017-12-11 01:56:27.076613: step 75240, loss = 0.70, batch loss = 0.65 (10.0 examples/sec; 0.798 sec/batch; 57h:02m:44s remains)
INFO - root - 2017-12-11 01:56:34.753995: step 75250, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 55h:27m:22s remains)
INFO - root - 2017-12-11 01:56:42.548017: step 75260, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 55h:37m:03s remains)
INFO - root - 2017-12-11 01:56:50.405156: step 75270, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 57h:47m:07s remains)
INFO - root - 2017-12-11 01:56:58.341453: step 75280, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 55h:54m:47s remains)
INFO - root - 2017-12-11 01:57:06.280249: step 75290, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 55h:16m:18s remains)
INFO - root - 2017-12-11 01:57:14.025013: step 75300, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 54h:26m:02s remains)
2017-12-11 01:57:14.882064: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.078125328 0.083503552 0.087859944 0.091112979 0.093742877 0.094516121 0.09349069 0.090558589 0.087213889 0.0852597 0.084687777 0.087106131 0.10828596 0.15012917 0.1921836][0.07647109 0.086552434 0.095023349 0.099968128 0.10257088 0.10212979 0.099598788 0.09564475 0.092211813 0.091893055 0.094501615 0.10180467 0.1322291 0.18490256 0.23430924][0.072230525 0.088988818 0.10339318 0.1110023 0.11287215 0.10921542 0.1023936 0.094253913 0.087095179 0.084204257 0.085982062 0.094554283 0.12853004 0.18460475 0.23570612][0.075178273 0.099018626 0.12035241 0.13206856 0.1343428 0.12906086 0.11952196 0.1074032 0.093973905 0.082854569 0.075786822 0.076389238 0.10174573 0.14835225 0.19346023][0.095123969 0.12481645 0.15302727 0.1712307 0.17907612 0.18049516 0.17712465 0.16686551 0.14744921 0.12274901 0.0966068 0.0754427 0.076016389 0.099100657 0.12947877][0.14884779 0.18044554 0.21209282 0.23647843 0.2545301 0.27236286 0.28630203 0.28601655 0.26323727 0.22242384 0.17036462 0.11766909 0.082931407 0.07365118 0.083278269][0.24343638 0.27192834 0.29915673 0.32299602 0.34820688 0.38317183 0.41729128 0.42991865 0.40575641 0.34993136 0.27232862 0.18795243 0.11845171 0.077937327 0.066730343][0.35608894 0.37844288 0.39443427 0.40903986 0.43136007 0.47215506 0.51592714 0.53446084 0.50742662 0.44052768 0.34608507 0.24172318 0.15112866 0.092117146 0.068172842][0.44737542 0.46047926 0.45905617 0.45471832 0.46026346 0.48861659 0.52414227 0.53754085 0.50776517 0.44036412 0.34709883 0.24441417 0.15539588 0.09810704 0.075310044][0.46925038 0.47049919 0.45099482 0.42533243 0.40809527 0.41345 0.42941529 0.43158749 0.4021914 0.34547246 0.27004391 0.18838374 0.12088319 0.082674041 0.073225409][0.4042674 0.39414898 0.36271754 0.32323188 0.28898707 0.27350715 0.27016735 0.26222539 0.23708227 0.19759132 0.14869137 0.098137625 0.062448308 0.051634029 0.061021905][0.27309456 0.25456521 0.22086988 0.18009132 0.14151898 0.11628418 0.10256276 0.090853542 0.07360021 0.052672621 0.030297155 0.010717495 0.0064704213 0.021583863 0.047570314][0.11999013 0.10009298 0.074871063 0.045539618 0.015966482 -0.0064392341 -0.020763831 -0.030777117 -0.03966276 -0.045821533 -0.048408076 -0.044855762 -0.02585583 0.0070021478 0.042139065][-0.0065985797 -0.02226248 -0.033981446 -0.046803229 -0.061259266 -0.073849536 -0.082935549 -0.088555425 -0.090594389 -0.087537475 -0.079182453 -0.06327153 -0.032800533 0.0070507759 0.043003168][-0.078938141 -0.087425619 -0.087399207 -0.087343425 -0.089540221 -0.092871949 -0.096079551 -0.097459391 -0.095046893 -0.0875631 -0.075310588 -0.055674724 -0.023246072 0.015692107 0.047811966]]...]
INFO - root - 2017-12-11 01:57:22.550627: step 75310, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.769 sec/batch; 54h:57m:05s remains)
INFO - root - 2017-12-11 01:57:30.450207: step 75320, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.774 sec/batch; 55h:16m:33s remains)
INFO - root - 2017-12-11 01:57:38.173309: step 75330, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 56h:38m:11s remains)
INFO - root - 2017-12-11 01:57:46.081516: step 75340, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 55h:54m:02s remains)
INFO - root - 2017-12-11 01:57:54.018429: step 75350, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.807 sec/batch; 57h:39m:14s remains)
INFO - root - 2017-12-11 01:58:02.002153: step 75360, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 57h:42m:10s remains)
INFO - root - 2017-12-11 01:58:09.851330: step 75370, loss = 0.68, batch loss = 0.63 (10.5 examples/sec; 0.765 sec/batch; 54h:37m:07s remains)
INFO - root - 2017-12-11 01:58:17.700751: step 75380, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.772 sec/batch; 55h:08m:33s remains)
INFO - root - 2017-12-11 01:58:25.546375: step 75390, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 55h:01m:01s remains)
INFO - root - 2017-12-11 01:58:33.245095: step 75400, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.764 sec/batch; 54h:34m:12s remains)
2017-12-11 01:58:34.076915: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20705801 0.18336259 0.15529491 0.13218272 0.11341438 0.09437412 0.076748937 0.0627417 0.054941606 0.052588623 0.0559948 0.066612542 0.085654825 0.1067143 0.1240762][0.31104824 0.28733376 0.25419569 0.22555961 0.20063548 0.1696403 0.13578261 0.10754238 0.089558125 0.080241 0.080198839 0.091262147 0.11577047 0.14399327 0.16612782][0.42093951 0.40850255 0.3801496 0.35347787 0.32649818 0.28497285 0.23470972 0.19107078 0.1601284 0.13975349 0.1316769 0.13880457 0.16530639 0.19757517 0.22229078][0.51229542 0.52391863 0.51522744 0.50289357 0.48196992 0.43629545 0.37471825 0.31801993 0.27289107 0.23881596 0.21911655 0.21623223 0.23784165 0.26696104 0.28705925][0.55385524 0.60008645 0.62688565 0.64481568 0.64330751 0.60628092 0.54454482 0.48143637 0.42366973 0.37472287 0.33996475 0.320567 0.33027202 0.35060397 0.36123785][0.53052008 0.61206508 0.68104279 0.73932511 0.76879823 0.754312 0.70678115 0.64848131 0.58556521 0.52619535 0.47742897 0.43976486 0.43571213 0.44821119 0.4519428][0.44646046 0.55244488 0.65733868 0.75508791 0.820741 0.840817 0.82342863 0.7847234 0.72912574 0.66907942 0.61342007 0.56218129 0.54596096 0.55138552 0.54860324][0.33241722 0.44525453 0.56940895 0.69347149 0.78917533 0.84595907 0.86768472 0.86079627 0.82484245 0.77429217 0.72088951 0.66405189 0.63751811 0.63321269 0.62112969][0.21501513 0.31405273 0.43328795 0.56132334 0.670948 0.75469589 0.81256896 0.84091139 0.83273256 0.80096775 0.75928909 0.70704496 0.67633909 0.664375 0.64446414][0.12506476 0.1958615 0.28894705 0.39761275 0.49978921 0.59049761 0.66821659 0.72266334 0.74149507 0.73246682 0.7093029 0.6732797 0.65036529 0.64030933 0.62151164][0.078181796 0.1187345 0.17742191 0.25323498 0.33107325 0.40812156 0.48345825 0.54523075 0.57938379 0.58827889 0.58399147 0.5709129 0.56605446 0.56727833 0.55648738][0.078183673 0.094089366 0.12105032 0.16219516 0.20939253 0.26173055 0.31923532 0.37201473 0.40730512 0.42375004 0.43091169 0.43837205 0.45451087 0.47098377 0.4700017][0.11918508 0.12090298 0.1256271 0.13911329 0.15841676 0.18434587 0.21774603 0.25214112 0.27793348 0.29213697 0.30173376 0.32311463 0.35844603 0.39103326 0.40055841][0.16625057 0.16351838 0.15768528 0.15537058 0.15652739 0.16247636 0.17432179 0.18907784 0.2014311 0.20871092 0.21560331 0.24447703 0.29418933 0.34050715 0.35988861][0.19014457 0.1886476 0.18098855 0.17345192 0.16684856 0.16239843 0.1611587 0.16245545 0.16419294 0.1647941 0.16739374 0.19693136 0.25078398 0.30183879 0.32617456]]...]
INFO - root - 2017-12-11 01:58:41.829841: step 75410, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 56h:02m:21s remains)
INFO - root - 2017-12-11 01:58:49.711804: step 75420, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 56h:56m:42s remains)
INFO - root - 2017-12-11 01:58:57.523531: step 75430, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 56h:40m:28s remains)
INFO - root - 2017-12-11 01:59:05.274924: step 75440, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.765 sec/batch; 54h:37m:42s remains)
INFO - root - 2017-12-11 01:59:13.116824: step 75450, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.801 sec/batch; 57h:12m:57s remains)
INFO - root - 2017-12-11 01:59:20.955064: step 75460, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.787 sec/batch; 56h:10m:43s remains)
INFO - root - 2017-12-11 01:59:28.744034: step 75470, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.760 sec/batch; 54h:14m:20s remains)
INFO - root - 2017-12-11 01:59:36.470588: step 75480, loss = 0.71, batch loss = 0.66 (11.9 examples/sec; 0.671 sec/batch; 47h:56m:24s remains)
INFO - root - 2017-12-11 01:59:44.046470: step 75490, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.809 sec/batch; 57h:44m:09s remains)
INFO - root - 2017-12-11 01:59:51.947884: step 75500, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.804 sec/batch; 57h:22m:46s remains)
2017-12-11 01:59:52.828911: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022835942 0.038082674 0.065025128 0.096016392 0.11318698 0.11017378 0.09835986 0.083078243 0.064164959 0.046818294 0.038649485 0.043487903 0.0578263 0.080078579 0.11657865][0.027874101 0.0680825 0.12994762 0.19904469 0.24589248 0.25511825 0.23811358 0.20125075 0.14918333 0.093358286 0.04990714 0.026191816 0.019634657 0.031171113 0.066184685][0.061904266 0.12366214 0.22264482 0.33791587 0.42634469 0.46054673 0.44826153 0.39401069 0.3062509 0.20362549 0.11334231 0.04840545 0.007210419 -0.0068750386 0.011654167][0.14794406 0.21552601 0.33838281 0.49328297 0.62566507 0.69517154 0.70221549 0.64458555 0.53127164 0.38526282 0.2465089 0.13476996 0.048119195 -0.0071422122 -0.021930268][0.301173 0.35486951 0.47707063 0.65019852 0.81431234 0.91702509 0.95024723 0.90352488 0.78250051 0.61109465 0.43732145 0.28542107 0.15313008 0.049787372 -0.0079651037][0.49746126 0.52492535 0.62067324 0.78338224 0.95423645 1.0731766 1.1267092 1.0996385 0.9918626 0.82303566 0.64084238 0.46895292 0.30289668 0.15662865 0.058047548][0.69356805 0.69469911 0.75010526 0.87963736 1.0322379 1.1456755 1.2047055 1.1964433 1.1162854 0.97544581 0.812288 0.64356446 0.46118051 0.28438994 0.1548018][0.80256146 0.78755873 0.80624259 0.89547569 1.0150396 1.1063907 1.1568372 1.162096 1.1154093 1.0181675 0.8941586 0.74918514 0.57084495 0.38293797 0.24120086][0.76612246 0.75104791 0.75168544 0.80905378 0.89308125 0.9540152 0.98588723 0.99717891 0.98130363 0.92974925 0.85105109 0.74035072 0.58250856 0.40437192 0.27085048][0.59737283 0.59464055 0.59490347 0.63158357 0.68340236 0.7139169 0.72529656 0.73770678 0.74492544 0.73037714 0.69133246 0.61717522 0.49276876 0.3437756 0.23615687][0.34715217 0.35448754 0.35885224 0.38097543 0.40845108 0.41963276 0.42192423 0.43945742 0.46479219 0.47573605 0.46421486 0.420682 0.333809 0.22459243 0.1524094][0.10202087 0.10866428 0.11261486 0.12335708 0.13634956 0.14230998 0.14969175 0.17775026 0.21560745 0.23888694 0.23929065 0.21351418 0.15680611 0.085342534 0.047977824][-0.057572138 -0.058296781 -0.061194215 -0.062494595 -0.061230563 -0.057981554 -0.0436488 -0.00750248 0.035709713 0.062784858 0.0672702 0.051446117 0.015717173 -0.026902856 -0.037437223][-0.087193288 -0.094801456 -0.10784696 -0.1238322 -0.1392059 -0.14818949 -0.1372346 -0.1014462 -0.059217021 -0.032085482 -0.025006192 -0.033700589 -0.056595296 -0.08213529 -0.078075871][-0.0058289645 -0.012256286 -0.034013987 -0.067940004 -0.1089627 -0.14364576 -0.14951175 -0.1233447 -0.087132633 -0.061604489 -0.050815515 -0.052290034 -0.067271829 -0.0843698 -0.074550368]]...]
INFO - root - 2017-12-11 02:00:00.759632: step 75510, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.784 sec/batch; 56h:00m:04s remains)
INFO - root - 2017-12-11 02:00:08.533312: step 75520, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.770 sec/batch; 54h:58m:43s remains)
INFO - root - 2017-12-11 02:00:16.320171: step 75530, loss = 0.67, batch loss = 0.61 (10.6 examples/sec; 0.752 sec/batch; 53h:38m:35s remains)
INFO - root - 2017-12-11 02:00:24.135119: step 75540, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 55h:31m:55s remains)
INFO - root - 2017-12-11 02:00:31.943610: step 75550, loss = 0.69, batch loss = 0.63 (9.7 examples/sec; 0.821 sec/batch; 58h:36m:46s remains)
INFO - root - 2017-12-11 02:00:39.890725: step 75560, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 57h:04m:23s remains)
INFO - root - 2017-12-11 02:00:47.200072: step 75570, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 55h:52m:50s remains)
INFO - root - 2017-12-11 02:00:55.008330: step 75580, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 56h:18m:50s remains)
INFO - root - 2017-12-11 02:01:02.792134: step 75590, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.762 sec/batch; 54h:21m:13s remains)
INFO - root - 2017-12-11 02:01:10.649009: step 75600, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 56h:22m:34s remains)
2017-12-11 02:01:11.521023: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.082542926 0.082254551 0.075924836 0.07223656 0.075104468 0.081654258 0.086506978 0.089572154 0.091107875 0.091170371 0.088329993 0.084253877 0.082735568 0.082176916 0.089292884][0.24074577 0.23805226 0.22205284 0.211733 0.21223304 0.21795085 0.22193925 0.22582206 0.23075032 0.23450959 0.23443362 0.23191518 0.23358397 0.23674205 0.2553404][0.40189308 0.38757983 0.35592288 0.33700624 0.33447981 0.33901197 0.34160224 0.34615406 0.35402107 0.36110818 0.36217031 0.36012834 0.36506918 0.37490359 0.41279426][0.52456093 0.49536148 0.451034 0.42911971 0.42802313 0.43477383 0.43832222 0.44322121 0.45070779 0.45435598 0.44847164 0.43956429 0.4417159 0.45682919 0.51523745][0.59636092 0.56229752 0.52170008 0.51035857 0.51887882 0.53424972 0.54564959 0.55425447 0.56009966 0.55240744 0.52808428 0.49758404 0.47914025 0.48214346 0.54125857][0.60670352 0.59236073 0.58041435 0.59814066 0.62850487 0.66328526 0.69249541 0.70672721 0.70449793 0.67210782 0.61466032 0.54585767 0.48667639 0.45609695 0.48979312][0.56299776 0.58563858 0.61913162 0.6769433 0.73744035 0.80215925 0.85789186 0.87733209 0.85989964 0.79365963 0.69675297 0.58318627 0.47516006 0.40086734 0.39419422][0.47460645 0.53597724 0.61160809 0.7025314 0.7879163 0.87977582 0.95913959 0.98157549 0.95010078 0.85827368 0.73497611 0.59009451 0.44474319 0.33322689 0.2875033][0.34759161 0.43029493 0.52668715 0.63042229 0.7253893 0.82980937 0.91998154 0.94324124 0.90694362 0.81022096 0.68572813 0.53571069 0.37748891 0.24813107 0.17883311][0.19625755 0.27461049 0.36483759 0.45741621 0.54209644 0.63815659 0.72178471 0.743782 0.71474963 0.63693327 0.537803 0.41071367 0.26761281 0.14564155 0.075709306][0.042753763 0.097211719 0.16262431 0.22956774 0.29152894 0.3639892 0.42800361 0.44699544 0.43183747 0.38408035 0.32167622 0.23291978 0.12537535 0.032972977 -0.016320381][-0.0744164 -0.049906559 -0.013695958 0.025691511 0.062924318 0.10763271 0.14787848 0.16199395 0.15853418 0.13709052 0.10707926 0.057215806 -0.0080716629 -0.061608028 -0.081535108][-0.12721881 -0.12622787 -0.1139605 -0.0971942 -0.08128123 -0.061342876 -0.042156972 -0.033550039 -0.030654354 -0.035199683 -0.043271795 -0.063080512 -0.092847414 -0.11401752 -0.11097502][-0.12083225 -0.13245225 -0.13465694 -0.13182557 -0.12924558 -0.12506595 -0.11967 -0.11588053 -0.11176069 -0.10927717 -0.10713165 -0.11034963 -0.1191328 -0.12252903 -0.10906956][-0.086062267 -0.10137798 -0.10968938 -0.11293128 -0.11601742 -0.11893723 -0.12066606 -0.12099864 -0.11924414 -0.11686645 -0.11323362 -0.11098573 -0.11131858 -0.10840102 -0.092878461]]...]
INFO - root - 2017-12-11 02:01:19.387676: step 75610, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 55h:25m:29s remains)
INFO - root - 2017-12-11 02:01:27.170034: step 75620, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.780 sec/batch; 55h:38m:48s remains)
INFO - root - 2017-12-11 02:01:34.990337: step 75630, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 55h:11m:20s remains)
INFO - root - 2017-12-11 02:01:42.891504: step 75640, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 56h:45m:47s remains)
INFO - root - 2017-12-11 02:01:50.569292: step 75650, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 55h:16m:58s remains)
INFO - root - 2017-12-11 02:01:58.237231: step 75660, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.817 sec/batch; 58h:15m:48s remains)
INFO - root - 2017-12-11 02:02:06.046062: step 75670, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 55h:44m:06s remains)
INFO - root - 2017-12-11 02:02:13.795070: step 75680, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.786 sec/batch; 56h:04m:34s remains)
INFO - root - 2017-12-11 02:02:21.703923: step 75690, loss = 0.70, batch loss = 0.65 (9.9 examples/sec; 0.811 sec/batch; 57h:52m:04s remains)
INFO - root - 2017-12-11 02:02:29.512444: step 75700, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.769 sec/batch; 54h:50m:22s remains)
2017-12-11 02:02:30.411743: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17728767 0.18187346 0.18073758 0.19125706 0.22078674 0.26683086 0.31623605 0.34956861 0.35027584 0.31830651 0.27315518 0.23470259 0.21213184 0.20817353 0.22293603][0.21946894 0.22503603 0.22281837 0.2283918 0.25016069 0.28843212 0.33171031 0.35878378 0.35414791 0.32024708 0.27739683 0.243331 0.22601929 0.22820055 0.24881119][0.25495026 0.26195186 0.26243863 0.26864856 0.28671438 0.31776077 0.35132995 0.36570719 0.34883729 0.30811647 0.26494923 0.2342453 0.22171229 0.23013346 0.2566177][0.27417439 0.28535372 0.29466853 0.3098799 0.33264503 0.3626568 0.38912588 0.39008376 0.35743243 0.30505282 0.25652242 0.22512177 0.21500075 0.22918105 0.26237851][0.28352576 0.29959911 0.31860605 0.34605953 0.37882552 0.4133749 0.43700734 0.42762998 0.38072249 0.31605539 0.26008573 0.22502007 0.21480043 0.23297194 0.27216008][0.29655683 0.31590962 0.34044698 0.37792695 0.42172635 0.46425873 0.48921254 0.47422346 0.41740236 0.34273827 0.27935663 0.23880939 0.2262563 0.24601467 0.28948215][0.3124955 0.33590204 0.36370084 0.40793946 0.46063522 0.51103133 0.53850406 0.52072769 0.45749936 0.37554094 0.30644855 0.26142573 0.24726672 0.26778027 0.31313297][0.3135522 0.34410796 0.37640622 0.4257164 0.48444882 0.54076833 0.57095718 0.55351496 0.48922303 0.40506026 0.33396384 0.28755391 0.27454984 0.29619113 0.34037563][0.30247897 0.33735904 0.3709535 0.42083412 0.48147985 0.54088384 0.57477087 0.56368107 0.50830215 0.43111765 0.36332527 0.31770748 0.30598086 0.3261022 0.36326581][0.29882029 0.32937267 0.35419819 0.39451912 0.44877002 0.50597227 0.54245293 0.54210436 0.50542217 0.4459393 0.38827786 0.34577662 0.33400863 0.34806472 0.37095419][0.30780429 0.32257858 0.32635361 0.34710675 0.38834858 0.44027957 0.4800162 0.49460626 0.48382193 0.45059049 0.40924242 0.37207308 0.35775691 0.36023927 0.36208475][0.32183069 0.31296694 0.28751847 0.28318062 0.3091765 0.35619876 0.40222964 0.43561941 0.45335045 0.44814041 0.42377976 0.39157522 0.37141883 0.35751569 0.33468324][0.32428837 0.29321036 0.24114047 0.21523491 0.22956565 0.27476379 0.32972011 0.38260126 0.42569518 0.44269314 0.42969435 0.39886561 0.36955154 0.33658332 0.28953326][0.31377497 0.26860854 0.1986894 0.15749696 0.16344319 0.20796336 0.27136642 0.34063637 0.40290913 0.43526471 0.4283956 0.39571193 0.35514522 0.30306727 0.23549388][0.29399118 0.24649522 0.170266 0.11995847 0.11806625 0.15963703 0.22783095 0.30816153 0.38304034 0.42566416 0.42263845 0.38642779 0.33298853 0.26250654 0.17897636]]...]
INFO - root - 2017-12-11 02:02:38.242101: step 75710, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 56h:55m:59s remains)
INFO - root - 2017-12-11 02:02:46.046378: step 75720, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 55h:20m:18s remains)
INFO - root - 2017-12-11 02:02:53.676102: step 75730, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 55h:52m:07s remains)
INFO - root - 2017-12-11 02:03:01.542162: step 75740, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.790 sec/batch; 56h:22m:40s remains)
INFO - root - 2017-12-11 02:03:09.218800: step 75750, loss = 0.70, batch loss = 0.65 (10.2 examples/sec; 0.783 sec/batch; 55h:52m:43s remains)
INFO - root - 2017-12-11 02:03:16.969397: step 75760, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.798 sec/batch; 56h:55m:47s remains)
INFO - root - 2017-12-11 02:03:24.855809: step 75770, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 56h:24m:00s remains)
INFO - root - 2017-12-11 02:03:32.642036: step 75780, loss = 0.70, batch loss = 0.64 (10.7 examples/sec; 0.750 sec/batch; 53h:27m:00s remains)
INFO - root - 2017-12-11 02:03:40.455814: step 75790, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.768 sec/batch; 54h:46m:43s remains)
INFO - root - 2017-12-11 02:03:48.312754: step 75800, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.773 sec/batch; 55h:08m:49s remains)
2017-12-11 02:03:49.157679: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10726718 0.11228304 0.10826496 0.099167161 0.089677982 0.083317578 0.081173837 0.081009656 0.081216641 0.082842 0.091650225 0.1072377 0.12168569 0.12619027 0.11869924][0.16199231 0.17427288 0.17201304 0.16076908 0.14763007 0.13829541 0.13410148 0.13148893 0.12788007 0.12501734 0.13163525 0.14808264 0.16488126 0.17103226 0.16410947][0.21252109 0.23404951 0.23605131 0.2248852 0.20932978 0.1973051 0.19029136 0.18321547 0.17250898 0.16037531 0.15780714 0.16767143 0.18182205 0.18886961 0.18514229][0.25557765 0.28700355 0.29575521 0.28782055 0.27224606 0.25861481 0.24842356 0.23511687 0.21432659 0.1883713 0.16966386 0.16442327 0.16863066 0.17285123 0.171667][0.28581828 0.32592341 0.34179038 0.33881739 0.32569778 0.31320551 0.30246595 0.28509164 0.25563362 0.21587475 0.17892982 0.15402457 0.14218836 0.13750254 0.13383137][0.29371175 0.33845192 0.3589474 0.36085826 0.35277256 0.34566298 0.3394751 0.32347059 0.29065439 0.24210261 0.19105576 0.1485575 0.119799 0.10258446 0.091504179][0.28563043 0.3287397 0.34868374 0.35272729 0.35000637 0.35066548 0.35286152 0.34351331 0.31375381 0.26417017 0.2079173 0.15628397 0.11650837 0.089007057 0.069724858][0.27295655 0.3088924 0.323126 0.32506379 0.32475349 0.33075869 0.3400608 0.33885232 0.31698793 0.27470928 0.22411621 0.17501087 0.13427433 0.1032056 0.078923233][0.27238607 0.300805 0.30716932 0.30244869 0.2980302 0.3017529 0.31169394 0.31570572 0.30351624 0.27475744 0.2394083 0.2038205 0.17174536 0.14350638 0.11738883][0.28451508 0.30871931 0.30799899 0.29377216 0.27927217 0.273527 0.27764913 0.28258151 0.278797 0.2653788 0.24994636 0.2344802 0.21734153 0.1962173 0.17060219][0.28729844 0.31065342 0.30581722 0.28329572 0.25778839 0.24069683 0.23642543 0.23872331 0.23908271 0.23685874 0.23837997 0.24239036 0.24191485 0.23138651 0.21060704][0.26403037 0.28804818 0.28277785 0.25674075 0.22470364 0.19968846 0.18824054 0.18603279 0.18590169 0.18838228 0.19934385 0.21614778 0.22883238 0.22968253 0.21746801][0.2071373 0.22979681 0.22616638 0.20229854 0.17113902 0.1450517 0.1308018 0.12509869 0.12245211 0.12470017 0.13723324 0.15760528 0.17621326 0.18515486 0.18162678][0.13334417 0.15169941 0.14988169 0.13164188 0.10689344 0.08540944 0.072657086 0.066073082 0.061934412 0.062531084 0.072194234 0.089180484 0.1062727 0.11706193 0.1181553][0.065157175 0.076917917 0.07577756 0.063594982 0.047166549 0.032900814 0.024221132 0.019203141 0.015641613 0.015403846 0.021380246 0.032399736 0.04388072 0.051601563 0.052839465]]...]
INFO - root - 2017-12-11 02:03:56.670644: step 75810, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 56h:24m:21s remains)
INFO - root - 2017-12-11 02:04:04.451796: step 75820, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 56h:14m:26s remains)
INFO - root - 2017-12-11 02:04:12.296842: step 75830, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 56h:50m:11s remains)
INFO - root - 2017-12-11 02:04:19.966595: step 75840, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 56h:22m:15s remains)
INFO - root - 2017-12-11 02:04:27.778437: step 75850, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 56h:52m:46s remains)
INFO - root - 2017-12-11 02:04:35.475950: step 75860, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 55h:17m:07s remains)
INFO - root - 2017-12-11 02:04:43.272413: step 75870, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 54h:22m:41s remains)
INFO - root - 2017-12-11 02:04:51.298951: step 75880, loss = 0.69, batch loss = 0.64 (9.7 examples/sec; 0.824 sec/batch; 58h:44m:31s remains)
INFO - root - 2017-12-11 02:04:59.001913: step 75890, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 56h:46m:42s remains)
INFO - root - 2017-12-11 02:05:06.985014: step 75900, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 54h:57m:38s remains)
2017-12-11 02:05:07.820082: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.063046061 0.023170754 -0.020470083 -0.058828965 -0.084616348 -0.087526232 -0.048705895 0.036308091 0.15052277 0.25408977 0.31299588 0.31260622 0.25657251 0.17454962 0.097973309][0.12838587 0.094984263 0.049118284 0.0031610872 -0.033180591 -0.047842029 -0.019524919 0.057562824 0.16663015 0.26748621 0.32564074 0.32666871 0.27526978 0.20244676 0.1392784][0.1981547 0.17815763 0.13893497 0.093619861 0.0522398 0.027050504 0.039516803 0.095498644 0.1811026 0.26121724 0.30532789 0.30114216 0.25479069 0.1963841 0.15356418][0.26532614 0.26135203 0.23546807 0.19934729 0.16118392 0.13241136 0.13284372 0.16665477 0.22281376 0.27322412 0.29330444 0.27358818 0.22288916 0.17126089 0.14360204][0.31893292 0.33175269 0.3221797 0.29937392 0.2693727 0.24409629 0.24206358 0.26411042 0.29966798 0.32549664 0.32144886 0.27989033 0.21292514 0.15330616 0.12659115][0.35399497 0.38300154 0.39021066 0.38136697 0.36102188 0.3437072 0.34820876 0.37132081 0.40006435 0.41257462 0.3902607 0.32566518 0.23336107 0.15092213 0.10907902][0.36494362 0.40674788 0.42772818 0.43008506 0.41796792 0.40935665 0.4240863 0.45568219 0.48833394 0.49898317 0.46807882 0.38646632 0.26961714 0.15995735 0.0950574][0.34630468 0.39233974 0.41861075 0.4250074 0.41682535 0.41471371 0.43887925 0.48118666 0.52369475 0.54071987 0.51059294 0.42203858 0.29114741 0.16316123 0.0805][0.29084364 0.33145344 0.35349911 0.35663703 0.34823933 0.34957197 0.3793174 0.42895979 0.47957137 0.50359166 0.47825342 0.39231086 0.26294473 0.13481602 0.049917657][0.19314574 0.22189215 0.23526925 0.23279727 0.22311105 0.22566661 0.2554917 0.30466 0.35497946 0.38022023 0.35971791 0.28504989 0.17381175 0.065358922 -0.0049676667][0.073826537 0.089302279 0.095053025 0.089573979 0.080800951 0.083834127 0.10848006 0.14805155 0.18743598 0.20616637 0.18941532 0.1329706 0.0523759 -0.023051372 -0.068913974][-0.037703577 -0.033324242 -0.032412451 -0.037608363 -0.043366387 -0.040386084 -0.02415855 0.00038929749 0.023321878 0.032501969 0.020221142 -0.014428089 -0.060536727 -0.10067548 -0.12182576][-0.11522214 -0.11813364 -0.11942846 -0.12267154 -0.12515545 -0.12285173 -0.1147724 -0.10383681 -0.0949968 -0.093282476 -0.10112778 -0.11788071 -0.13736616 -0.15139593 -0.15498741][-0.1430583 -0.14856465 -0.14980733 -0.15080583 -0.1510316 -0.14970371 -0.14713834 -0.14458254 -0.14365445 -0.14539377 -0.1497923 -0.1559438 -0.16083726 -0.16135871 -0.15652123][-0.12933625 -0.13462684 -0.1351707 -0.13496877 -0.13439749 -0.13374604 -0.13333534 -0.13347043 -0.13443515 -0.13609037 -0.13795155 -0.13932809 -0.13888095 -0.135606 -0.12972732]]...]
INFO - root - 2017-12-11 02:05:15.738724: step 75910, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 57h:33m:14s remains)
INFO - root - 2017-12-11 02:05:23.627189: step 75920, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.766 sec/batch; 54h:35m:03s remains)
INFO - root - 2017-12-11 02:05:31.233832: step 75930, loss = 0.71, batch loss = 0.65 (10.6 examples/sec; 0.757 sec/batch; 53h:56m:07s remains)
INFO - root - 2017-12-11 02:05:39.113981: step 75940, loss = 0.67, batch loss = 0.61 (10.0 examples/sec; 0.804 sec/batch; 57h:17m:10s remains)
INFO - root - 2017-12-11 02:05:47.097092: step 75950, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.781 sec/batch; 55h:38m:55s remains)
INFO - root - 2017-12-11 02:05:55.012465: step 75960, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.781 sec/batch; 55h:39m:16s remains)
INFO - root - 2017-12-11 02:06:02.661978: step 75970, loss = 0.71, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 56h:28m:34s remains)
INFO - root - 2017-12-11 02:06:10.479749: step 75980, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.788 sec/batch; 56h:07m:24s remains)
INFO - root - 2017-12-11 02:06:18.302341: step 75990, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 56h:43m:11s remains)
INFO - root - 2017-12-11 02:06:26.181056: step 76000, loss = 0.68, batch loss = 0.62 (10.5 examples/sec; 0.765 sec/batch; 54h:31m:25s remains)
2017-12-11 02:06:27.083503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.056471355 -0.05678935 -0.055257145 -0.055937249 -0.05836004 -0.061348639 -0.063680619 -0.064331591 -0.063933864 -0.06301152 -0.063331835 -0.064505257 -0.066857539 -0.070925407 -0.074483037][-0.039252367 -0.032660432 -0.026289595 -0.026206827 -0.030571055 -0.036480621 -0.0412309 -0.042988651 -0.042299878 -0.040422283 -0.040821593 -0.043667167 -0.050742976 -0.062754661 -0.074608237][-0.002176058 0.017770417 0.0341005 0.038700555 0.035060089 0.028136531 0.022592748 0.021920728 0.024587488 0.028036613 0.027543791 0.021550527 0.0052171513 -0.022872467 -0.051867638][0.05362187 0.096388556 0.13165909 0.14933774 0.15473841 0.15380281 0.15252562 0.1567141 0.16315457 0.16658729 0.1616039 0.14585306 0.11091976 0.054597218 -0.0034438479][0.12036414 0.19428115 0.25583151 0.29361707 0.31613997 0.32887477 0.33685216 0.34814143 0.35857719 0.35938588 0.34308535 0.30879879 0.24671225 0.15490249 0.062081572][0.18365169 0.29075807 0.38009 0.43945554 0.48228171 0.51319712 0.53339082 0.55220634 0.56473261 0.55868888 0.52401936 0.46525717 0.37456352 0.25008571 0.12625673][0.22847484 0.36170343 0.47238374 0.54795462 0.60754716 0.65540868 0.68670464 0.71062326 0.72108459 0.70429116 0.64879048 0.56729478 0.45648983 0.31306496 0.17155546][0.24657838 0.39178616 0.51163262 0.59382707 0.66254544 0.72054291 0.75667524 0.77935022 0.78220034 0.75270575 0.6792531 0.58347481 0.46680582 0.3229872 0.18195225][0.23792644 0.37805706 0.49322554 0.57136714 0.63916636 0.69568181 0.72551328 0.73873806 0.73079967 0.69265652 0.61138868 0.51516032 0.40881294 0.28231815 0.15838167][0.19901018 0.31653455 0.41370919 0.47875142 0.53606719 0.5798822 0.59469146 0.59574419 0.58066493 0.54325318 0.46839532 0.38653132 0.30428267 0.20790157 0.11221962][0.13789234 0.22293985 0.2954326 0.34363434 0.38585719 0.41249037 0.41185322 0.40507469 0.39034671 0.36202225 0.30401266 0.24472409 0.19121131 0.12739174 0.061571077][0.069911279 0.12261678 0.17062429 0.20271103 0.23041114 0.24255002 0.23265794 0.22459547 0.21533 0.19823721 0.1587061 0.12071127 0.0912213 0.054181002 0.013360696][0.0094051594 0.036817994 0.065029964 0.084430344 0.1014564 0.10550344 0.094155647 0.089873984 0.087368339 0.078883916 0.053219315 0.029763192 0.01506576 -0.0050165351 -0.029027158][-0.034214254 -0.022634609 -0.0072899666 0.0045057903 0.015791044 0.018076459 0.010837213 0.012039206 0.014820897 0.011379141 -0.0053007738 -0.02030839 -0.028064275 -0.039855637 -0.054976586][-0.060600191 -0.057826068 -0.050103422 -0.042513702 -0.034002762 -0.030221367 -0.031785987 -0.027100964 -0.021966539 -0.022912463 -0.033241257 -0.0431393 -0.048202343 -0.056079749 -0.066227041]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 02:06:34.976464: step 76010, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.777 sec/batch; 55h:23m:33s remains)
INFO - root - 2017-12-11 02:06:42.652234: step 76020, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.771 sec/batch; 54h:55m:42s remains)
INFO - root - 2017-12-11 02:06:50.608974: step 76030, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.824 sec/batch; 58h:43m:13s remains)
INFO - root - 2017-12-11 02:06:58.521694: step 76040, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.785 sec/batch; 55h:54m:06s remains)
INFO - root - 2017-12-11 02:07:06.235307: step 76050, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 54h:22m:22s remains)
INFO - root - 2017-12-11 02:07:14.170049: step 76060, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 55h:40m:44s remains)
INFO - root - 2017-12-11 02:07:22.145699: step 76070, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 55h:27m:29s remains)
INFO - root - 2017-12-11 02:07:29.980789: step 76080, loss = 0.69, batch loss = 0.63 (9.8 examples/sec; 0.817 sec/batch; 58h:10m:41s remains)
INFO - root - 2017-12-11 02:07:37.889951: step 76090, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.805 sec/batch; 57h:18m:54s remains)
INFO - root - 2017-12-11 02:07:45.954268: step 76100, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 56h:28m:40s remains)
2017-12-11 02:07:46.803420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.030971803 -0.03503947 -0.027428094 -0.0087262979 0.015657816 0.038690865 0.053931236 0.058707703 0.055980213 0.053385239 0.053320527 0.05869307 0.077366956 0.11018626 0.14293015][-0.023626031 -0.026246302 -0.018321019 0.0010813504 0.026799219 0.052101776 0.069942415 0.077159934 0.074846484 0.068753831 0.062313572 0.060481004 0.073940665 0.10517542 0.13890943][0.0021873207 0.00043109513 0.0059191803 0.022627018 0.046141148 0.07031069 0.087741978 0.094948873 0.090602815 0.077411108 0.060442824 0.047557436 0.052328225 0.07918372 0.11425258][0.044015359 0.043570634 0.04538247 0.056791198 0.074756227 0.094040714 0.10746802 0.11160345 0.10328716 0.081836842 0.053655323 0.0293857 0.025229547 0.046953849 0.082982868][0.10555883 0.10918623 0.10817385 0.1123541 0.11995025 0.12813853 0.13155137 0.12777543 0.11286531 0.083258428 0.046124019 0.013637147 0.0031293565 0.020632813 0.056726556][0.18524948 0.19553074 0.19354446 0.19066733 0.18534076 0.17808051 0.16695531 0.15139219 0.12738851 0.089732632 0.045686334 0.0075850566 -0.0073823705 0.0064759641 0.041223306][0.26619363 0.28301162 0.28100583 0.27279645 0.25620908 0.23421293 0.20855919 0.18089116 0.14766946 0.10317262 0.054382119 0.012820882 -0.0052707978 0.0054372675 0.037753966][0.32472575 0.34387371 0.34065762 0.32906502 0.30669519 0.27710792 0.24400108 0.20995051 0.17205702 0.12482172 0.07478331 0.032061756 0.011932995 0.019832544 0.0485992][0.34980363 0.36893055 0.36405998 0.35113624 0.32847005 0.29896572 0.26652807 0.23328352 0.19715884 0.15279935 0.1061008 0.065196574 0.04440945 0.04987013 0.073821649][0.33295372 0.353131 0.34897494 0.33700514 0.31749028 0.29289991 0.26716933 0.2417317 0.21479641 0.17999949 0.14154951 0.10548095 0.08544939 0.0889734 0.1078826][0.26739365 0.28818464 0.28741211 0.27912435 0.26532406 0.24839909 0.23242006 0.2187086 0.20600028 0.18681379 0.16242246 0.13656011 0.12229742 0.12889773 0.1482534][0.17158131 0.18982689 0.19141705 0.18647912 0.17811112 0.16842031 0.16111289 0.15775734 0.15851216 0.15582848 0.1482195 0.13700028 0.13382739 0.14850307 0.17337684][0.083306678 0.095338359 0.096150778 0.092126533 0.086782977 0.0818792 0.080181234 0.082996584 0.091973275 0.10092922 0.10703471 0.10965545 0.11803129 0.14096999 0.17189933][0.01873227 0.023028055 0.020164471 0.014263513 0.0088896118 0.0056303656 0.0060478444 0.010662936 0.021635016 0.034878954 0.04815352 0.060461897 0.078360476 0.10817456 0.14406393][-0.02256505 -0.024635287 -0.031014131 -0.039399497 -0.046445996 -0.050467059 -0.050749443 -0.047476068 -0.039024651 -0.027533166 -0.013877059 0.0016485702 0.023253389 0.054410473 0.089971364]]...]
INFO - root - 2017-12-11 02:07:54.719118: step 76110, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.780 sec/batch; 55h:32m:25s remains)
INFO - root - 2017-12-11 02:08:02.598161: step 76120, loss = 0.69, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 56h:51m:03s remains)
INFO - root - 2017-12-11 02:08:10.265981: step 76130, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 57h:04m:54s remains)
INFO - root - 2017-12-11 02:08:18.149477: step 76140, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 55h:43m:20s remains)
INFO - root - 2017-12-11 02:08:26.135677: step 76150, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.801 sec/batch; 57h:01m:02s remains)
INFO - root - 2017-12-11 02:08:34.021192: step 76160, loss = 0.71, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 56h:51m:57s remains)
INFO - root - 2017-12-11 02:08:41.892773: step 76170, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 57h:32m:25s remains)
INFO - root - 2017-12-11 02:08:49.811706: step 76180, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.792 sec/batch; 56h:22m:03s remains)
INFO - root - 2017-12-11 02:08:57.523722: step 76190, loss = 0.69, batch loss = 0.63 (12.5 examples/sec; 0.638 sec/batch; 45h:24m:06s remains)
INFO - root - 2017-12-11 02:09:05.432049: step 76200, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 55h:21m:20s remains)
2017-12-11 02:09:06.299658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.077799983 -0.084614918 -0.086569 -0.0858096 -0.084124975 -0.08367122 -0.082668096 -0.082353577 -0.083915316 -0.088001028 -0.093683936 -0.099880733 -0.10908543 -0.11876352 -0.12441462][-0.061805286 -0.066783555 -0.062963687 -0.05465183 -0.045772716 -0.038730815 -0.028525887 -0.018961279 -0.01567659 -0.021441532 -0.03345963 -0.050164673 -0.076442748 -0.10579164 -0.12600693][-0.0185487 -0.016538983 0.00033839609 0.024313955 0.04713618 0.066062182 0.092307426 0.11680896 0.1264732 0.11489261 0.089015335 0.052149173 -0.0039212913 -0.066603288 -0.11242][0.054036707 0.07077755 0.10960523 0.15836573 0.20121357 0.23469312 0.27974787 0.31999654 0.33424544 0.31018347 0.26219875 0.19534932 0.098429032 -0.0074357074 -0.086543731][0.14582059 0.18639976 0.25600436 0.33598498 0.40230331 0.45232984 0.51956332 0.57978827 0.60167372 0.56412697 0.489103 0.38380256 0.23508006 0.075194567 -0.045673318][0.24211663 0.31297719 0.41542536 0.52465779 0.61088663 0.67558312 0.76497173 0.84755707 0.87994659 0.83237088 0.73169696 0.58607274 0.38246241 0.16653211 0.0037354433][0.3339504 0.43122703 0.55677325 0.6824041 0.77792346 0.851569 0.95779604 1.0591413 1.1022011 1.049957 0.92976528 0.74955666 0.50026709 0.24069145 0.047105607][0.40594026 0.51316714 0.63913625 0.75833577 0.84471548 0.91343468 1.0209962 1.1293614 1.1805649 1.1309159 1.0040586 0.807963 0.54013443 0.26593703 0.063873291][0.44858602 0.54624379 0.64979959 0.74056345 0.79846722 0.84424436 0.93132 1.0277647 1.0778393 1.0352625 0.9169184 0.73001504 0.47774369 0.22428876 0.041517563][0.45033428 0.52359074 0.59088659 0.64154327 0.66250223 0.6775049 0.73278338 0.805986 0.8485164 0.81651324 0.7186923 0.5597623 0.34693429 0.13725376 -0.0089225238][0.40540791 0.44768924 0.47612652 0.48766074 0.4750782 0.46152249 0.48354086 0.52917516 0.56065369 0.54010987 0.46895409 0.34819102 0.18670128 0.030761644 -0.072889782][0.31215134 0.32470533 0.32183185 0.30539313 0.27225047 0.24198842 0.24095304 0.26584181 0.29036948 0.28337935 0.23986992 0.15700379 0.043826792 -0.063725695 -0.13124458][0.18116222 0.17416121 0.15547428 0.1279825 0.091442339 0.060814492 0.053800251 0.072285131 0.099074073 0.10852372 0.089862555 0.037092317 -0.041037291 -0.11615936 -0.16248487][0.046815794 0.033459589 0.014438683 -0.009474121 -0.036283586 -0.054643206 -0.052942492 -0.028037658 0.007517193 0.0315368 0.029202588 -0.0064198202 -0.065894455 -0.12522611 -0.16379368][-0.058127712 -0.069284081 -0.080870248 -0.0953211 -0.10884646 -0.11220273 -0.097553127 -0.062655322 -0.01849575 0.014437288 0.019715242 -0.0086766016 -0.059991993 -0.11287639 -0.14968289]]...]
INFO - root - 2017-12-11 02:09:13.619292: step 76210, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 55h:14m:52s remains)
INFO - root - 2017-12-11 02:09:21.482277: step 76220, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.777 sec/batch; 55h:18m:16s remains)
INFO - root - 2017-12-11 02:09:29.372019: step 76230, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.764 sec/batch; 54h:24m:41s remains)
INFO - root - 2017-12-11 02:09:37.234809: step 76240, loss = 0.68, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 55h:49m:53s remains)
INFO - root - 2017-12-11 02:09:45.104432: step 76250, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 54h:38m:06s remains)
INFO - root - 2017-12-11 02:09:53.038399: step 76260, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 56h:33m:42s remains)
INFO - root - 2017-12-11 02:10:01.001725: step 76270, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.805 sec/batch; 57h:19m:23s remains)
INFO - root - 2017-12-11 02:10:08.714987: step 76280, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 56h:49m:20s remains)
INFO - root - 2017-12-11 02:10:16.379192: step 76290, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 55h:27m:14s remains)
INFO - root - 2017-12-11 02:10:24.170220: step 76300, loss = 0.71, batch loss = 0.66 (10.4 examples/sec; 0.769 sec/batch; 54h:42m:58s remains)
2017-12-11 02:10:24.926361: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0362736 0.043390527 0.0458423 0.0454287 0.041781284 0.036880456 0.032083109 0.030749811 0.035246827 0.043217219 0.053192046 0.059459645 0.058735091 0.047622193 0.026265224][0.071130075 0.082944021 0.089135148 0.0935153 0.093796939 0.092287831 0.090869665 0.093874954 0.10327535 0.11522917 0.12873572 0.1365464 0.13364954 0.11467886 0.079933912][0.1071505 0.12322359 0.13366774 0.14435697 0.15118308 0.15692319 0.16282938 0.17264615 0.18757291 0.20290972 0.21859261 0.22595219 0.21867131 0.19004072 0.1405369][0.13841757 0.15780494 0.17167225 0.18829933 0.20223267 0.21637645 0.22995226 0.24470682 0.26156092 0.27583924 0.28929383 0.29311603 0.28052524 0.24398798 0.18333019][0.16909337 0.19025193 0.20472364 0.22385105 0.24220796 0.2624242 0.28167024 0.29911613 0.3152262 0.32593173 0.33445838 0.33288905 0.31464982 0.27146444 0.20282425][0.20003778 0.22331093 0.23758003 0.2565096 0.27628285 0.29983175 0.32304731 0.34241033 0.35779554 0.36624768 0.37038559 0.36274776 0.33761734 0.28651083 0.21008188][0.23008662 0.25665656 0.27184913 0.28925544 0.3076323 0.33069816 0.35426924 0.37241483 0.3852236 0.39149165 0.3917776 0.37909794 0.34836468 0.29174617 0.21077633][0.25989839 0.29041874 0.30686849 0.32068014 0.3337678 0.3511332 0.36949858 0.38172626 0.38794562 0.38817823 0.38125527 0.36195126 0.32633153 0.26869428 0.19090971][0.29220024 0.32689717 0.34344265 0.35088196 0.35462886 0.36158529 0.37078735 0.37508166 0.37354341 0.36615762 0.35049325 0.3233971 0.28305361 0.22751316 0.15912357][0.32200715 0.35931954 0.37324607 0.37096435 0.36243126 0.35763472 0.35816589 0.35795179 0.35361612 0.34323364 0.32228422 0.28917658 0.2451164 0.19225636 0.13393658][0.32514247 0.36376771 0.37534627 0.36509478 0.3470417 0.33383167 0.32879788 0.32743371 0.32446998 0.31508943 0.2924172 0.25649422 0.21150631 0.16285776 0.11407442][0.2927205 0.3312403 0.34323528 0.33100528 0.30973825 0.29255024 0.28367504 0.2802422 0.27641541 0.26621181 0.24218737 0.206251 0.16430366 0.12349628 0.086226441][0.22669971 0.26106447 0.2738145 0.26481509 0.24660571 0.23100178 0.22234665 0.21836422 0.2125563 0.19979346 0.17457063 0.14026214 0.10342978 0.071797341 0.046624351][0.14562434 0.17085227 0.18070377 0.17446537 0.16106126 0.14965913 0.14406332 0.14210553 0.13694632 0.12497622 0.10291201 0.074270822 0.04505752 0.022570902 0.0076613505][0.065035589 0.078025959 0.0811424 0.074482143 0.064668417 0.057915006 0.056437984 0.058274131 0.05697564 0.04982033 0.034987573 0.015168131 -0.0050310511 -0.019685457 -0.027618179]]...]
INFO - root - 2017-12-11 02:10:32.735803: step 76310, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 56h:29m:16s remains)
INFO - root - 2017-12-11 02:10:40.625190: step 76320, loss = 0.68, batch loss = 0.62 (10.6 examples/sec; 0.755 sec/batch; 53h:42m:36s remains)
INFO - root - 2017-12-11 02:10:48.496292: step 76330, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.783 sec/batch; 55h:44m:41s remains)
INFO - root - 2017-12-11 02:10:56.378245: step 76340, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.818 sec/batch; 58h:12m:39s remains)
INFO - root - 2017-12-11 02:11:04.305627: step 76350, loss = 0.68, batch loss = 0.63 (10.1 examples/sec; 0.790 sec/batch; 56h:12m:37s remains)
INFO - root - 2017-12-11 02:11:12.128198: step 76360, loss = 0.68, batch loss = 0.62 (10.2 examples/sec; 0.788 sec/batch; 56h:04m:00s remains)
INFO - root - 2017-12-11 02:11:19.644457: step 76370, loss = 0.71, batch loss = 0.65 (10.2 examples/sec; 0.785 sec/batch; 55h:49m:07s remains)
INFO - root - 2017-12-11 02:11:27.459578: step 76380, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.775 sec/batch; 55h:09m:37s remains)
INFO - root - 2017-12-11 02:11:35.319744: step 76390, loss = 0.71, batch loss = 0.65 (10.7 examples/sec; 0.746 sec/batch; 53h:03m:31s remains)
INFO - root - 2017-12-11 02:11:43.175622: step 76400, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 55h:01m:34s remains)
2017-12-11 02:11:43.987363: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0095572276 0.010023175 0.0090264194 0.010330641 0.016223351 0.022961516 0.022047795 0.013142806 0.0030491091 -0.0064610997 -0.013982621 -0.016861154 -0.014275932 -0.0070677814 -0.00010415364][0.079497606 0.075011365 0.067201488 0.066698253 0.075180791 0.087506436 0.09357816 0.090823762 0.082823873 0.067606069 0.048044689 0.03086674 0.020073324 0.016171377 0.01610796][0.17790724 0.16635929 0.14988697 0.14858811 0.16301329 0.18552046 0.20444179 0.21386336 0.21208912 0.19191676 0.15791556 0.12190466 0.092125349 0.072021551 0.062937416][0.28240475 0.26287237 0.23708259 0.23512623 0.25644946 0.29042414 0.32488197 0.35085365 0.36027443 0.33920547 0.2940132 0.24114177 0.19237064 0.15553841 0.13869628][0.36830661 0.34480935 0.31135464 0.3065823 0.33002838 0.37004015 0.41605592 0.45672202 0.47824866 0.46071815 0.41163087 0.3496 0.28795063 0.2397217 0.2205305][0.42824355 0.40501994 0.36605981 0.35387075 0.370248 0.40545872 0.45265311 0.49895212 0.52787077 0.51616728 0.47145811 0.41128203 0.34766513 0.29748759 0.28177932][0.45983988 0.44012919 0.39773855 0.37388095 0.37395734 0.3932015 0.42981356 0.47015023 0.49873635 0.49260873 0.45823777 0.40877637 0.35247317 0.30828494 0.29886732][0.46277195 0.44661665 0.40298164 0.3669787 0.34716031 0.34452137 0.36170742 0.38552517 0.4051699 0.40168253 0.37900519 0.34411839 0.300354 0.26634121 0.26221183][0.4432272 0.43116874 0.3933982 0.35554916 0.32513076 0.30546394 0.300319 0.29802179 0.29563239 0.28325471 0.26400384 0.2392492 0.20766029 0.18458508 0.18426062][0.40555862 0.400452 0.37884751 0.35495552 0.33015591 0.30493322 0.28032166 0.2468915 0.21151939 0.1770677 0.14892448 0.12539274 0.10164525 0.087389864 0.089223631][0.35097381 0.35858142 0.36121634 0.36271 0.35595614 0.33552164 0.29771379 0.23637919 0.16724403 0.10536358 0.060730089 0.031661227 0.011339547 0.0028815842 0.0060030064][0.28092611 0.30710769 0.33796236 0.36762807 0.38158363 0.36938381 0.32423791 0.24458103 0.1519068 0.068848468 0.0093088457 -0.026280915 -0.045185793 -0.049873974 -0.045020387][0.20462656 0.25060347 0.30700672 0.35928503 0.38866556 0.38251454 0.33415705 0.24800968 0.14792252 0.057823554 -0.0072707217 -0.044204377 -0.059799563 -0.0600731 -0.051465191][0.13276945 0.19219923 0.26313353 0.32563 0.36121449 0.3583788 0.31266657 0.23401536 0.14505851 0.0651039 0.0067279208 -0.024588929 -0.034540668 -0.030533688 -0.018380189][0.071754746 0.13162147 0.19945726 0.25586483 0.2873984 0.28610665 0.25043139 0.19396439 0.1345868 0.081893407 0.042513292 0.023212435 0.019855652 0.025192242 0.036085449]]...]
INFO - root - 2017-12-11 02:11:51.825320: step 76410, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.773 sec/batch; 55h:00m:33s remains)
INFO - root - 2017-12-11 02:11:59.624567: step 76420, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 56h:41m:44s remains)
INFO - root - 2017-12-11 02:12:07.424612: step 76430, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.786 sec/batch; 55h:54m:33s remains)
INFO - root - 2017-12-11 02:12:15.252369: step 76440, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 57h:36m:06s remains)
INFO - root - 2017-12-11 02:12:22.926256: step 76450, loss = 0.70, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 54h:15m:30s remains)
INFO - root - 2017-12-11 02:12:30.594012: step 76460, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.784 sec/batch; 55h:44m:30s remains)
INFO - root - 2017-12-11 02:12:38.488977: step 76470, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 56h:28m:28s remains)
INFO - root - 2017-12-11 02:12:46.357054: step 76480, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 56h:25m:20s remains)
INFO - root - 2017-12-11 02:12:54.214066: step 76490, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.794 sec/batch; 56h:26m:08s remains)
INFO - root - 2017-12-11 02:13:02.135962: step 76500, loss = 0.71, batch loss = 0.65 (9.7 examples/sec; 0.825 sec/batch; 58h:39m:43s remains)
2017-12-11 02:13:02.910384: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33506411 0.33804616 0.32739943 0.32432434 0.31639349 0.29197896 0.25657389 0.22625718 0.20661907 0.18977346 0.16818076 0.14685309 0.12470534 0.099246331 0.0630006][0.37182781 0.37139741 0.35635656 0.35529783 0.35383421 0.33830789 0.312811 0.29144204 0.2767933 0.25818598 0.22629119 0.19006614 0.1540105 0.11688013 0.071505204][0.39342427 0.38610411 0.3643156 0.36047155 0.36229172 0.35650545 0.3446297 0.33590323 0.32917646 0.31202802 0.2745882 0.22901466 0.18385451 0.13746466 0.08452788][0.39379823 0.3828533 0.35473618 0.34379563 0.34288898 0.34086078 0.33754158 0.33871949 0.34070295 0.32951602 0.295097 0.24859484 0.20135352 0.15198563 0.0958041][0.36727178 0.35970923 0.33085781 0.31367597 0.30565718 0.30044019 0.29898444 0.30503187 0.31379268 0.31081739 0.28592104 0.24705365 0.20498984 0.15900554 0.10373914][0.3248989 0.32271835 0.29533798 0.27263618 0.25733411 0.24889903 0.24914564 0.25913456 0.27439696 0.2826111 0.27267578 0.24727461 0.2142112 0.17412572 0.12120852][0.2901426 0.29336843 0.26955819 0.24688618 0.23337352 0.23196454 0.2429018 0.26334083 0.28786257 0.30720216 0.30916888 0.29210988 0.26141182 0.21986903 0.16266562][0.27911121 0.28312284 0.26518103 0.24896123 0.24652159 0.26043501 0.2876865 0.32230341 0.35720137 0.38368753 0.38850924 0.36866552 0.33018082 0.27844131 0.21008627][0.27101344 0.27842358 0.27003032 0.26496589 0.2768254 0.3056106 0.34608334 0.39223334 0.43627095 0.46694416 0.46917287 0.44077665 0.389959 0.32450125 0.24178703][0.22829124 0.24072944 0.24360426 0.24999894 0.27169591 0.30790481 0.35338029 0.40294105 0.44890639 0.47829434 0.47679085 0.44388852 0.38846064 0.31852236 0.23144014][0.14157762 0.15579104 0.16439863 0.17501552 0.19676249 0.23009367 0.27079219 0.31371388 0.35245454 0.37682217 0.37642935 0.35135591 0.30805147 0.25117043 0.17699768][0.042636324 0.053392246 0.060772516 0.06937363 0.086651385 0.11384199 0.14703907 0.18017903 0.20807236 0.22590607 0.2283736 0.21546784 0.1898932 0.15171167 0.0967994][-0.02984084 -0.025831269 -0.022738112 -0.018532755 -0.0068483739 0.013206735 0.0380077 0.061517123 0.079718143 0.0915799 0.094933331 0.089687385 0.076151572 0.051738996 0.013168463][-0.058307085 -0.059604216 -0.059727188 -0.058603484 -0.0512307 -0.03766837 -0.02073211 -0.0047382233 0.0072519192 0.015178806 0.017639771 0.014292798 0.0050151283 -0.012532616 -0.040001236][-0.064493828 -0.068135329 -0.068399206 -0.067753866 -0.063175119 -0.054683991 -0.04411782 -0.033802181 -0.025846597 -0.020895841 -0.020029275 -0.02382282 -0.031793166 -0.044504352 -0.062526435]]...]
INFO - root - 2017-12-11 02:13:10.795466: step 76510, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.772 sec/batch; 54h:51m:36s remains)
INFO - root - 2017-12-11 02:13:18.596193: step 76520, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.768 sec/batch; 54h:35m:53s remains)
INFO - root - 2017-12-11 02:13:26.232959: step 76530, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.770 sec/batch; 54h:45m:46s remains)
INFO - root - 2017-12-11 02:13:34.112238: step 76540, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 56h:15m:34s remains)
INFO - root - 2017-12-11 02:13:41.806245: step 76550, loss = 0.70, batch loss = 0.64 (9.6 examples/sec; 0.834 sec/batch; 59h:17m:13s remains)
INFO - root - 2017-12-11 02:13:49.635391: step 76560, loss = 0.72, batch loss = 0.66 (10.0 examples/sec; 0.798 sec/batch; 56h:45m:35s remains)
INFO - root - 2017-12-11 02:13:57.594765: step 76570, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.774 sec/batch; 55h:02m:58s remains)
INFO - root - 2017-12-11 02:14:05.450855: step 76580, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.772 sec/batch; 54h:51m:48s remains)
INFO - root - 2017-12-11 02:14:13.328547: step 76590, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 57h:05m:01s remains)
INFO - root - 2017-12-11 02:14:21.246997: step 76600, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.779 sec/batch; 55h:23m:55s remains)
2017-12-11 02:14:22.156168: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0017996636 0.043042466 0.1026064 0.16558112 0.21596727 0.24216409 0.25336391 0.25563654 0.25008386 0.24085668 0.23216006 0.22601297 0.21818635 0.20708334 0.19144174][-0.0056622243 0.029075192 0.079660058 0.1358289 0.18638812 0.21871105 0.23778376 0.24758235 0.24960172 0.24545124 0.23701316 0.22833551 0.21901023 0.20935075 0.19624311][-0.011237599 0.017939568 0.060318284 0.11100651 0.16386268 0.20335729 0.2290834 0.24177457 0.24476661 0.23955581 0.22725648 0.21489333 0.20502609 0.19921498 0.19203308][-0.011394425 0.01797162 0.059251193 0.11088881 0.16982658 0.21659885 0.24602129 0.25552732 0.25183886 0.23904617 0.21866855 0.19989552 0.18747957 0.18474388 0.18380845][-0.007437428 0.026478933 0.073076464 0.13066114 0.19660921 0.24826261 0.277618 0.27912739 0.26377302 0.24084827 0.21299726 0.18922137 0.1743829 0.17354608 0.17773013][-0.0019147721 0.037771814 0.091548465 0.15540873 0.22484437 0.27638453 0.30232105 0.29528224 0.26961797 0.23980983 0.21006703 0.18688524 0.17270286 0.17315386 0.17975257][0.0025394249 0.046037778 0.10525835 0.17249103 0.24049966 0.28796998 0.30961996 0.29778206 0.26800019 0.23812149 0.21397747 0.19833378 0.18979856 0.19279167 0.19990589][0.0042316746 0.049055714 0.11081644 0.17838047 0.24199226 0.28374958 0.30152017 0.28968206 0.26215729 0.2372206 0.22157554 0.21512462 0.21341854 0.21881041 0.22558704][0.0037730639 0.048052434 0.11012435 0.17635502 0.23489495 0.270646 0.2840645 0.273824 0.25091308 0.23198436 0.2232846 0.22273636 0.22476266 0.23096478 0.23884413][0.0017447892 0.043815881 0.1029811 0.16511978 0.21760732 0.24651879 0.25367138 0.24280643 0.22288688 0.20825328 0.20356449 0.20549314 0.2090691 0.2169645 0.23089734][-0.00044367983 0.0376109 0.090427205 0.14488612 0.18942264 0.21060881 0.21035382 0.19622728 0.17651914 0.16392303 0.16130936 0.1642026 0.16858782 0.17901057 0.201317][-0.000838398 0.033348206 0.078999341 0.1244093 0.16014342 0.17333448 0.16574739 0.14647287 0.12401114 0.11035616 0.10740639 0.11018315 0.11485077 0.12654343 0.15398951][-0.00023547174 0.030166673 0.068486542 0.1043075 0.13051692 0.1355879 0.12202198 0.098705038 0.07383658 0.058073118 0.052966304 0.054292314 0.058014851 0.068145074 0.094355009][-0.001081482 0.024724593 0.055068482 0.080984041 0.097632177 0.095488146 0.078428254 0.054503247 0.030647431 0.014567471 0.0073373369 0.0065188506 0.0083998935 0.01531001 0.035698313][-0.0035034257 0.017257825 0.039752651 0.056935608 0.066071369 0.05974783 0.042535782 0.021523573 0.0018476156 -0.012412096 -0.020970529 -0.024341524 -0.025190838 -0.0221277 -0.0091894632]]...]
INFO - root - 2017-12-11 02:14:29.913350: step 76610, loss = 0.70, batch loss = 0.65 (10.4 examples/sec; 0.771 sec/batch; 54h:49m:18s remains)
INFO - root - 2017-12-11 02:14:37.769210: step 76620, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.796 sec/batch; 56h:34m:59s remains)
INFO - root - 2017-12-11 02:14:45.581636: step 76630, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 56h:12m:17s remains)
INFO - root - 2017-12-11 02:14:53.242944: step 76640, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 55h:58m:21s remains)
INFO - root - 2017-12-11 02:15:01.167081: step 76650, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 54h:09m:26s remains)
INFO - root - 2017-12-11 02:15:09.119496: step 76660, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.784 sec/batch; 55h:43m:00s remains)
INFO - root - 2017-12-11 02:15:16.997659: step 76670, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.789 sec/batch; 56h:04m:56s remains)
INFO - root - 2017-12-11 02:15:24.877653: step 76680, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 56h:02m:18s remains)
INFO - root - 2017-12-11 02:15:32.517812: step 76690, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 54h:32m:53s remains)
INFO - root - 2017-12-11 02:15:40.330177: step 76700, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 56h:04m:38s remains)
2017-12-11 02:15:41.246909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.021231089 -0.018865539 -0.022407116 -0.032901425 -0.041958626 -0.045131106 -0.04133806 -0.029963301 -0.016292104 -0.0081970748 -0.012776469 -0.027624378 -0.045682065 -0.0645353 -0.080638811][0.037517477 0.053890709 0.05880937 0.049982414 0.039918859 0.034033708 0.033753473 0.043706775 0.058463071 0.066497333 0.055438757 0.028386822 -0.0045176451 -0.039433841 -0.070558384][0.12031763 0.15949187 0.18035474 0.1784285 0.16932243 0.15921338 0.1517196 0.15740643 0.17076392 0.17583369 0.15484354 0.11174449 0.05911465 0.002995247 -0.047994003][0.21677546 0.28755334 0.33380988 0.34689394 0.34285775 0.32827413 0.31172872 0.31075835 0.32015064 0.31846577 0.28407544 0.22173509 0.14520594 0.062735096 -0.013565259][0.29239368 0.39447042 0.46988574 0.50463283 0.51272613 0.49983424 0.47846425 0.47226459 0.47715077 0.46668351 0.41749769 0.335706 0.23613074 0.12792507 0.026185824][0.31849158 0.44162941 0.54192704 0.60124671 0.63043022 0.63082254 0.61497605 0.60959494 0.61253738 0.59419906 0.531236 0.43295807 0.31567481 0.18761422 0.064939216][0.30497256 0.4366335 0.55283 0.63294393 0.68512708 0.70459771 0.70057267 0.69920439 0.7010479 0.67605138 0.60189968 0.49226919 0.3651711 0.22695541 0.092550084][0.26151249 0.388334 0.50710273 0.59648097 0.66214383 0.69426847 0.69783556 0.69613826 0.69334638 0.66205829 0.58256066 0.47220543 0.35002717 0.21943285 0.090910725][0.19176321 0.30069092 0.40759987 0.49193984 0.55733973 0.591902 0.59667885 0.590453 0.581105 0.54668057 0.47083467 0.37209702 0.26936597 0.16292605 0.057356454][0.10107329 0.1807813 0.26324213 0.33022439 0.38425905 0.41403905 0.4172408 0.40692508 0.39308763 0.36113471 0.29792354 0.2206187 0.14644988 0.073987685 0.0024420472][0.013499402 0.061284564 0.11468677 0.15897702 0.19583155 0.21589936 0.21490656 0.20089069 0.18414102 0.15764403 0.11188328 0.060662333 0.017542187 -0.019693188 -0.055437457][-0.045254618 -0.022731928 0.0056150057 0.028491836 0.046496529 0.053538077 0.046025194 0.028651942 0.010303222 -0.01018204 -0.038338158 -0.065195285 -0.082260579 -0.091879763 -0.099536479][-0.079296105 -0.074060358 -0.064083278 -0.057440285 -0.054506306 -0.057949077 -0.069245011 -0.0858662 -0.10189819 -0.11572956 -0.12932487 -0.13807857 -0.13812096 -0.13150118 -0.12277403][-0.10037573 -0.10617328 -0.10755665 -0.11003051 -0.11417736 -0.120676 -0.12976463 -0.14056864 -0.15035446 -0.15709865 -0.16061665 -0.15890644 -0.15104529 -0.13863783 -0.12461532][-0.10611774 -0.11609387 -0.1214953 -0.12625884 -0.13071711 -0.13514045 -0.13966192 -0.14421736 -0.14795084 -0.14947993 -0.14812335 -0.14302348 -0.1344061 -0.12339421 -0.11165116]]...]
INFO - root - 2017-12-11 02:15:49.188673: step 76710, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.808 sec/batch; 57h:23m:54s remains)
INFO - root - 2017-12-11 02:15:56.897782: step 76720, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.577 sec/batch; 40h:57m:40s remains)
INFO - root - 2017-12-11 02:16:04.745548: step 76730, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 56h:30m:25s remains)
INFO - root - 2017-12-11 02:16:12.771113: step 76740, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.760 sec/batch; 53h:58m:55s remains)
INFO - root - 2017-12-11 02:16:20.679990: step 76750, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.817 sec/batch; 58h:00m:34s remains)
INFO - root - 2017-12-11 02:16:28.541890: step 76760, loss = 0.71, batch loss = 0.65 (9.9 examples/sec; 0.812 sec/batch; 57h:40m:17s remains)
INFO - root - 2017-12-11 02:16:36.153314: step 76770, loss = 0.69, batch loss = 0.63 (10.4 examples/sec; 0.767 sec/batch; 54h:27m:08s remains)
INFO - root - 2017-12-11 02:16:44.092195: step 76780, loss = 0.70, batch loss = 0.65 (10.3 examples/sec; 0.779 sec/batch; 55h:21m:23s remains)
INFO - root - 2017-12-11 02:16:51.955553: step 76790, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.816 sec/batch; 57h:59m:45s remains)
INFO - root - 2017-12-11 02:16:59.768928: step 76800, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.769 sec/batch; 54h:37m:31s remains)
2017-12-11 02:17:00.572690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12218306 -0.12767805 -0.13054417 -0.13501962 -0.14026624 -0.14553434 -0.14868417 -0.150748 -0.15242386 -0.15341325 -0.15361507 -0.15284318 -0.15001835 -0.14333418 -0.13350622][-0.10198142 -0.096322328 -0.0885092 -0.087517567 -0.09370026 -0.10356504 -0.11148825 -0.11780193 -0.12560937 -0.13421048 -0.14283966 -0.1510874 -0.15725128 -0.15681119 -0.1479599][-0.053278584 -0.022440791 0.010083423 0.028016763 0.028282017 0.019281661 0.0090401713 -0.0042055245 -0.025643606 -0.05194933 -0.078560352 -0.10550494 -0.13206199 -0.14967802 -0.15026456][0.019089814 0.089702316 0.16275698 0.21261497 0.23220631 0.239326 0.23893467 0.22196852 0.17974055 0.1219847 0.062120661 -0.00055710605 -0.064884514 -0.1166038 -0.1380073][0.10311782 0.22344494 0.35136598 0.44896969 0.50299722 0.54577738 0.57556862 0.56678003 0.5027914 0.40350738 0.29564354 0.17818545 0.055270869 -0.049428903 -0.10508347][0.17535886 0.34210944 0.52610093 0.677027 0.77378792 0.8659839 0.94377422 0.95722568 0.87827176 0.73862404 0.57965577 0.39898142 0.2063534 0.038811922 -0.058362506][0.21026671 0.40385196 0.62467045 0.81539369 0.94938093 1.0888286 1.2183145 1.263613 1.1851594 1.0225596 0.82686251 0.59398985 0.34104863 0.11938732 -0.014224244][0.18309739 0.37141097 0.59369838 0.79327375 0.94279706 1.1078789 1.2724054 1.3481611 1.2878392 1.1304811 0.927539 0.67385995 0.39381388 0.14856964 -0.0010121765][0.096870318 0.24703524 0.43308243 0.60631776 0.74269253 0.90172434 1.0715605 1.1648434 1.1325505 1.0058495 0.82819289 0.59469259 0.333288 0.10593795 -0.031187272][-0.01112172 0.083724454 0.21076618 0.33369234 0.4344559 0.55965137 0.70412087 0.79399282 0.78580314 0.70045668 0.56928456 0.38916078 0.18556891 0.01112642 -0.089731157][-0.10120691 -0.060077455 0.0047954945 0.070031367 0.12478524 0.20056239 0.29790971 0.36427796 0.36784065 0.32019922 0.2405865 0.1280556 0.0015255624 -0.10241015 -0.15496974][-0.15297037 -0.14995655 -0.13252814 -0.11408758 -0.099336885 -0.070054986 -0.02240362 0.012900902 0.0172273 -0.0043473835 -0.042092528 -0.094943836 -0.15136991 -0.19162184 -0.20163429][-0.17027889 -0.18637402 -0.19460785 -0.20256971 -0.21114342 -0.2111332 -0.19867647 -0.18849097 -0.18870024 -0.1969393 -0.20925362 -0.2237547 -0.23479536 -0.23494795 -0.22013734][-0.16567713 -0.18473965 -0.1994371 -0.21543756 -0.23160569 -0.24339156 -0.24841066 -0.2525295 -0.25749847 -0.26106122 -0.26170242 -0.25850612 -0.25007477 -0.23543495 -0.21493351][-0.1542373 -0.16640215 -0.17601104 -0.18944772 -0.20431572 -0.21778259 -0.22876126 -0.2387199 -0.24587321 -0.24729015 -0.2424106 -0.23293257 -0.22063337 -0.20728278 -0.19397105]]...]
INFO - root - 2017-12-11 02:17:08.189843: step 76810, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.776 sec/batch; 55h:05m:29s remains)
INFO - root - 2017-12-11 02:17:16.011402: step 76820, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 55h:21m:55s remains)
INFO - root - 2017-12-11 02:17:23.880478: step 76830, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.778 sec/batch; 55h:15m:49s remains)
INFO - root - 2017-12-11 02:17:31.681233: step 76840, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 56h:36m:40s remains)
INFO - root - 2017-12-11 02:17:39.233128: step 76850, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.770 sec/batch; 54h:38m:43s remains)
INFO - root - 2017-12-11 02:17:47.011045: step 76860, loss = 0.69, batch loss = 0.63 (10.6 examples/sec; 0.756 sec/batch; 53h:39m:01s remains)
INFO - root - 2017-12-11 02:17:54.820649: step 76870, loss = 0.69, batch loss = 0.63 (10.2 examples/sec; 0.782 sec/batch; 55h:32m:59s remains)
INFO - root - 2017-12-11 02:18:02.725631: step 76880, loss = 0.68, batch loss = 0.62 (9.9 examples/sec; 0.806 sec/batch; 57h:12m:35s remains)
INFO - root - 2017-12-11 02:18:10.637448: step 76890, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.799 sec/batch; 56h:45m:24s remains)
INFO - root - 2017-12-11 02:18:18.303969: step 76900, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 54h:30m:50s remains)
2017-12-11 02:18:19.110385: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18157333 0.18742102 0.17375323 0.15474989 0.140478 0.1350037 0.13395151 0.13545549 0.14167316 0.15109144 0.15762205 0.16241008 0.1853267 0.23009852 0.2656098][0.32315949 0.335123 0.31684133 0.29064882 0.2708481 0.26349452 0.2622585 0.26229382 0.26510182 0.26898807 0.26766646 0.26402393 0.28543177 0.33709121 0.37810969][0.45816377 0.47717741 0.45708469 0.42802781 0.40624696 0.39965832 0.40158644 0.40185603 0.3978858 0.38755253 0.36827782 0.34608483 0.35207886 0.39347932 0.42752138][0.55730116 0.58462334 0.56862479 0.54296494 0.52270645 0.51914895 0.52846628 0.53293765 0.52151757 0.49098 0.44680685 0.39885309 0.37607247 0.38914338 0.40448034][0.60132569 0.64146852 0.63925624 0.6244849 0.60923636 0.61091679 0.6317926 0.64545035 0.6287685 0.577402 0.50757891 0.43306845 0.37556297 0.35136622 0.34468335][0.59456915 0.65280348 0.67156291 0.67150843 0.66284233 0.669964 0.70212346 0.72556084 0.70581287 0.63896352 0.55106413 0.45829672 0.37342691 0.31993112 0.29977095][0.56725258 0.642397 0.67795229 0.68538773 0.67711329 0.68446535 0.72132915 0.74987906 0.72890204 0.65705079 0.56574315 0.47104695 0.37950793 0.32117048 0.30563661][0.55877286 0.63914287 0.67342478 0.66869128 0.64538372 0.64102995 0.67002559 0.69504505 0.67419863 0.60971481 0.53216159 0.45456782 0.38092726 0.34525809 0.35231158][0.59235597 0.65735692 0.664229 0.62419361 0.5697276 0.54146183 0.5496859 0.56160027 0.54052812 0.4916783 0.43872818 0.38955665 0.34839511 0.34990293 0.38676378][0.65403318 0.68166637 0.6385873 0.54839408 0.45474654 0.39641842 0.37717837 0.370707 0.34895045 0.31790754 0.2907773 0.26950112 0.26007786 0.29476285 0.35538954][0.71474272 0.693319 0.59401786 0.45581138 0.32744643 0.24123947 0.19605309 0.17254288 0.15103 0.13511047 0.12779593 0.12622684 0.13787724 0.19004634 0.26027879][0.737819 0.67387086 0.53368723 0.36709484 0.22051917 0.11822572 0.058046427 0.026549684 0.0086782537 0.0031184084 0.0059101414 0.012257195 0.030187044 0.081485845 0.14662972][0.687994 0.60425991 0.45407125 0.28902921 0.14808369 0.047688752 -0.012740502 -0.041905168 -0.053384371 -0.053112771 -0.048265055 -0.043207277 -0.030131197 0.0085826572 0.059926394][0.54866028 0.46995649 0.34071687 0.20552485 0.093140945 0.012687821 -0.034907959 -0.054504216 -0.058422856 -0.0546151 -0.051031306 -0.050736595 -0.045853883 -0.021295777 0.01484131][0.33872387 0.27964514 0.18868773 0.098264135 0.026772225 -0.022690466 -0.048971772 -0.054565776 -0.049816698 -0.042281557 -0.039001934 -0.04157256 -0.042079289 -0.027559785 -0.00463057]]...]
INFO - root - 2017-12-11 02:18:26.958311: step 76910, loss = 0.68, batch loss = 0.62 (10.1 examples/sec; 0.789 sec/batch; 56h:01m:51s remains)
INFO - root - 2017-12-11 02:18:34.826190: step 76920, loss = 0.72, batch loss = 0.66 (10.1 examples/sec; 0.792 sec/batch; 56h:14m:30s remains)
INFO - root - 2017-12-11 02:18:42.349318: step 76930, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 55h:21m:29s remains)
INFO - root - 2017-12-11 02:18:50.225768: step 76940, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.776 sec/batch; 55h:03m:53s remains)
INFO - root - 2017-12-11 02:18:58.040188: step 76950, loss = 0.67, batch loss = 0.61 (10.3 examples/sec; 0.779 sec/batch; 55h:18m:56s remains)
INFO - root - 2017-12-11 02:19:05.801545: step 76960, loss = 0.69, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 57h:29m:06s remains)
INFO - root - 2017-12-11 02:19:13.638063: step 76970, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.804 sec/batch; 57h:05m:27s remains)
INFO - root - 2017-12-11 02:19:21.375455: step 76980, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 56h:38m:50s remains)
INFO - root - 2017-12-11 02:19:29.006691: step 76990, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 55h:22m:37s remains)
INFO - root - 2017-12-11 02:19:36.849393: step 77000, loss = 0.69, batch loss = 0.63 (9.6 examples/sec; 0.835 sec/batch; 59h:14m:25s remains)
2017-12-11 02:19:37.671391: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36852616 0.2685473 0.2496627 0.31838521 0.4118354 0.46056452 0.44725052 0.39062217 0.31222671 0.26284823 0.29260084 0.39155832 0.50461674 0.57653087 0.58594239][0.38813448 0.25566486 0.19598451 0.2251983 0.28926584 0.33154953 0.33546737 0.30500969 0.25150412 0.22273649 0.27203947 0.38975695 0.5196082 0.60703129 0.63183415][0.41286018 0.26267228 0.16528548 0.15014966 0.18299061 0.22575375 0.26072818 0.26902762 0.24462043 0.22721544 0.27008167 0.370176 0.48354459 0.56722087 0.60465789][0.46818572 0.31818664 0.19780572 0.1495689 0.16415943 0.22207215 0.299221 0.34981012 0.34740347 0.32215044 0.325734 0.36858577 0.43087062 0.48967275 0.5332675][0.56059366 0.4280805 0.30260539 0.24007878 0.2563794 0.34661177 0.47535855 0.56883883 0.58039749 0.53042549 0.4688673 0.42323726 0.40472019 0.41662744 0.4526287][0.67500472 0.56867081 0.45119894 0.38915744 0.42043328 0.54971623 0.72925627 0.86198759 0.88437688 0.80604643 0.6740492 0.53340662 0.42579389 0.37979609 0.39362666][0.78390205 0.71003687 0.60983396 0.55537713 0.59843791 0.74925953 0.95483786 1.1089478 1.1400489 1.0458211 0.865948 0.65655226 0.48360488 0.3918657 0.38179719][0.87931138 0.83632404 0.75559705 0.70654744 0.74444079 0.8811869 1.0679293 1.2109038 1.2457296 1.159053 0.97488874 0.74854243 0.56176311 0.46203157 0.4474282][0.961225 0.94169956 0.87661457 0.8280713 0.8434689 0.93232852 1.0573856 1.1564134 1.1861714 1.1264894 0.97788972 0.78413463 0.63444704 0.56761634 0.57436889][1.0116314 1.0037851 0.95000237 0.90043533 0.88720304 0.91417927 0.95914418 0.99872905 1.0167316 0.99107635 0.8989501 0.767217 0.68444729 0.67471635 0.7187525][0.98685533 0.98202091 0.9367578 0.88798946 0.85111648 0.82412916 0.79775274 0.78232914 0.78645831 0.79089779 0.75635219 0.69282162 0.681351 0.73025197 0.81063449][0.85766357 0.85033411 0.81314176 0.76978308 0.72228563 0.66465604 0.59600616 0.54629469 0.54012388 0.56396192 0.57231939 0.56336296 0.60445327 0.69216466 0.79179066][0.64227664 0.63258654 0.6039263 0.56885141 0.52338231 0.46136224 0.38523319 0.32832888 0.31928158 0.35027903 0.37922537 0.39903548 0.46081278 0.556336 0.65173495][0.37827018 0.37276259 0.35669088 0.33386585 0.30017292 0.25233865 0.19318506 0.14850374 0.14132145 0.1671464 0.19387566 0.21537951 0.26819447 0.3453846 0.41889733][0.12650968 0.12802233 0.12526381 0.11561894 0.097085722 0.069855914 0.0358248 0.0096923551 0.0053611728 0.0199287 0.032760873 0.041569069 0.072072409 0.12148046 0.16908544]]...]
INFO - root - 2017-12-11 02:19:45.312301: step 77010, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.789 sec/batch; 56h:00m:27s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 02:19:53.155313: step 77020, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 56h:11m:37s remains)
INFO - root - 2017-12-11 02:20:01.104582: step 77030, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.774 sec/batch; 54h:54m:22s remains)
INFO - root - 2017-12-11 02:20:08.955238: step 77040, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.760 sec/batch; 53h:56m:24s remains)
INFO - root - 2017-12-11 02:20:16.814574: step 77050, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.800 sec/batch; 56h:44m:17s remains)
INFO - root - 2017-12-11 02:20:24.719523: step 77060, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.819 sec/batch; 58h:08m:06s remains)
INFO - root - 2017-12-11 02:20:32.512115: step 77070, loss = 0.72, batch loss = 0.66 (10.6 examples/sec; 0.756 sec/batch; 53h:36m:40s remains)
INFO - root - 2017-12-11 02:20:40.240093: step 77080, loss = 0.72, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 55h:32m:14s remains)
INFO - root - 2017-12-11 02:20:47.912938: step 77090, loss = 0.71, batch loss = 0.66 (10.0 examples/sec; 0.797 sec/batch; 56h:30m:46s remains)
INFO - root - 2017-12-11 02:20:55.690782: step 77100, loss = 0.67, batch loss = 0.61 (10.5 examples/sec; 0.759 sec/batch; 53h:49m:25s remains)
2017-12-11 02:20:56.538336: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17628726 0.19322468 0.19571915 0.18420182 0.16038969 0.13525206 0.12025887 0.11814053 0.12470127 0.13049158 0.12561224 0.10024907 0.054402862 0.0027813837 -0.039460488][0.26387051 0.280943 0.27543387 0.2500698 0.20815553 0.16583318 0.1395973 0.13242435 0.13753267 0.14071822 0.13102639 0.098588564 0.045403108 -0.010670006 -0.053403974][0.34338981 0.36153755 0.35122791 0.31598318 0.26082429 0.20695172 0.17469461 0.1655329 0.16884239 0.16584148 0.14586659 0.10065452 0.035657588 -0.027013047 -0.070022523][0.41026616 0.43046433 0.41954985 0.38158613 0.32155645 0.26451066 0.23341915 0.22741637 0.23042923 0.2188566 0.18369037 0.11990017 0.036938723 -0.037777454 -0.084800422][0.47359487 0.49533284 0.48605242 0.4518604 0.39582998 0.34451073 0.32228005 0.32513845 0.32969096 0.30747133 0.25263786 0.16462584 0.056900378 -0.037270386 -0.094550706][0.5195033 0.54154778 0.53734207 0.51509678 0.47506875 0.44233969 0.44039077 0.46006259 0.46973282 0.43639374 0.35781109 0.23939048 0.099563539 -0.02172612 -0.096332505][0.54307926 0.56634331 0.56933779 0.56380725 0.54722327 0.54043752 0.56270534 0.59856474 0.61099309 0.56524038 0.46260732 0.31429824 0.14413857 -0.00218071 -0.092598133][0.53589213 0.56093448 0.57329154 0.58751684 0.59824485 0.61901063 0.66273338 0.7086941 0.71931785 0.66151673 0.540215 0.37058622 0.17974047 0.016858552 -0.084102891][0.46719143 0.49155155 0.51251453 0.54575795 0.58308774 0.62933266 0.69026518 0.74182272 0.74994391 0.68673295 0.56043947 0.38737142 0.19409242 0.028957345 -0.074596547][0.33637983 0.35561079 0.37998331 0.42338839 0.4765521 0.53742313 0.60629559 0.65899229 0.66748768 0.61025161 0.49700388 0.34253395 0.1696967 0.021099145 -0.073382139][0.17546839 0.18772075 0.21022424 0.2534568 0.30914903 0.37202281 0.44013131 0.49236938 0.50709277 0.46712556 0.38013905 0.25778413 0.11871898 -0.0020951692 -0.079794407][0.037600849 0.043048434 0.059406385 0.093377814 0.13870594 0.19003835 0.24654005 0.29288718 0.31386265 0.29468644 0.23887248 0.15351865 0.052938662 -0.035419956 -0.092161961][-0.056899723 -0.057701219 -0.048927356 -0.028139398 0.00035674192 0.033233874 0.072410345 0.10894155 0.13312836 0.13170286 0.10396203 0.052251231 -0.013324053 -0.071717836 -0.10811619][-0.11557361 -0.12202987 -0.12101407 -0.11347495 -0.10175147 -0.086469151 -0.063753672 -0.037881792 -0.014880654 -0.0060216561 -0.014286966 -0.040379759 -0.077568866 -0.11019121 -0.12745135][-0.14010899 -0.15056089 -0.15548329 -0.1580345 -0.15879861 -0.15638843 -0.14670178 -0.13166857 -0.11501399 -0.10519467 -0.1051138 -0.1158602 -0.13280527 -0.14501853 -0.14586696]]...]
INFO - root - 2017-12-11 02:21:04.371873: step 77110, loss = 0.70, batch loss = 0.64 (10.4 examples/sec; 0.768 sec/batch; 54h:29m:36s remains)
INFO - root - 2017-12-11 02:21:12.228168: step 77120, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.788 sec/batch; 55h:52m:15s remains)
INFO - root - 2017-12-11 02:21:20.092888: step 77130, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.761 sec/batch; 54h:00m:01s remains)
INFO - root - 2017-12-11 02:21:27.905823: step 77140, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 56h:09m:15s remains)
INFO - root - 2017-12-11 02:21:35.740134: step 77150, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 55h:06m:51s remains)
INFO - root - 2017-12-11 02:21:43.650759: step 77160, loss = 0.70, batch loss = 0.64 (10.2 examples/sec; 0.787 sec/batch; 55h:49m:04s remains)
INFO - root - 2017-12-11 02:21:50.978485: step 77170, loss = 0.69, batch loss = 0.63 (10.5 examples/sec; 0.762 sec/batch; 54h:04m:09s remains)
INFO - root - 2017-12-11 02:21:58.729458: step 77180, loss = 0.71, batch loss = 0.65 (10.5 examples/sec; 0.765 sec/batch; 54h:16m:55s remains)
INFO - root - 2017-12-11 02:22:06.592301: step 77190, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.788 sec/batch; 55h:55m:05s remains)
INFO - root - 2017-12-11 02:22:14.492955: step 77200, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.816 sec/batch; 57h:52m:30s remains)
2017-12-11 02:22:15.346876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0060869525 -0.010352178 -0.012188034 -0.00950915 -0.0046638623 0.0028980309 0.0127568 0.021863472 0.024473909 0.019870294 0.011040248 -0.001607823 -0.016408442 -0.029437639 -0.036827669][0.031114217 0.026769891 0.023682 0.026865905 0.031400371 0.03948791 0.051911291 0.0647382 0.067673966 0.058915664 0.044520251 0.024843182 0.0017931443 -0.019466309 -0.032505058][0.081360437 0.077442892 0.0723349 0.074962191 0.077524766 0.083643712 0.096719861 0.11306383 0.11733478 0.10507112 0.084593035 0.056824509 0.024685379 -0.0055566523 -0.025002722][0.14560924 0.1418696 0.13294365 0.13316528 0.13182864 0.13285638 0.14329781 0.16156754 0.16723847 0.15166582 0.12409188 0.087332487 0.046049241 0.0073054163 -0.018103996][0.22900209 0.22577634 0.20982783 0.20400889 0.19595313 0.18847489 0.19277726 0.21052253 0.21675166 0.19731081 0.16180515 0.11526699 0.064805239 0.018076573 -0.01276603][0.32539144 0.32337931 0.29713956 0.28077596 0.26373872 0.24652733 0.24337736 0.25891745 0.26473457 0.24124686 0.19784558 0.14174223 0.0823946 0.027975842 -0.0081757735][0.41109723 0.41102725 0.37292 0.34407517 0.32048419 0.29899535 0.29271317 0.30647993 0.31034714 0.28182021 0.23034473 0.16511554 0.097476877 0.036269151 -0.00439991][0.45649606 0.4584111 0.41215691 0.37456936 0.35024372 0.33299392 0.33028078 0.34352222 0.34264261 0.30722564 0.24781664 0.1753958 0.10249186 0.038371731 -0.0037746888][0.45068049 0.4545576 0.40846643 0.37038612 0.35102451 0.34349349 0.34884316 0.3626163 0.35541964 0.31208259 0.2459265 0.16961791 0.095722668 0.033220619 -0.0070173647][0.3981733 0.40540284 0.36905798 0.33985928 0.3296988 0.333744 0.34931052 0.36568475 0.35426721 0.30557412 0.23546256 0.1573772 0.083625138 0.02366028 -0.013761964][0.30598956 0.31744209 0.298388 0.28596047 0.28809008 0.30286196 0.32741216 0.34741348 0.33567244 0.2864852 0.21628979 0.13877256 0.066346243 0.0097072534 -0.023906281][0.19236274 0.20669644 0.206868 0.21366984 0.22819139 0.2498301 0.27772322 0.29806441 0.28698036 0.24084096 0.17516117 0.10353352 0.038245663 -0.010318528 -0.037001904][0.08350347 0.096716084 0.10952996 0.12930359 0.15169205 0.17578642 0.20217679 0.21944673 0.20888655 0.16818784 0.11157561 0.051904593 0.00041471294 -0.034625396 -0.05093338][0.0041801664 0.012509582 0.027427517 0.049316835 0.07220868 0.09439829 0.11634838 0.12878999 0.11828604 0.084178567 0.039724611 -0.0038081533 -0.037252728 -0.055909812 -0.060539544][-0.038112983 -0.035238 -0.02522511 -0.0090794507 0.0092767384 0.027841689 0.045662507 0.054683905 0.045738641 0.019609708 -0.012298199 -0.040571272 -0.058476914 -0.06446857 -0.061304275]]...]
INFO - root - 2017-12-11 02:22:23.239731: step 77210, loss = 0.70, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 56h:01m:17s remains)
INFO - root - 2017-12-11 02:22:31.121503: step 77220, loss = 0.70, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 57h:15m:36s remains)
INFO - root - 2017-12-11 02:22:39.021666: step 77230, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 56h:03m:30s remains)
INFO - root - 2017-12-11 02:22:46.827484: step 77240, loss = 0.67, batch loss = 0.61 (10.0 examples/sec; 0.803 sec/batch; 56h:57m:50s remains)
INFO - root - 2017-12-11 02:22:54.402660: step 77250, loss = 0.68, batch loss = 0.62 (10.0 examples/sec; 0.803 sec/batch; 56h:57m:29s remains)
INFO - root - 2017-12-11 02:23:02.061143: step 77260, loss = 0.70, batch loss = 0.64 (10.3 examples/sec; 0.776 sec/batch; 55h:00m:28s remains)
INFO - root - 2017-12-11 02:23:10.017661: step 77270, loss = 0.70, batch loss = 0.64 (9.7 examples/sec; 0.827 sec/batch; 58h:37m:37s remains)
INFO - root - 2017-12-11 02:23:17.827274: step 77280, loss = 0.68, batch loss = 0.63 (10.3 examples/sec; 0.778 sec/batch; 55h:10m:13s remains)
INFO - root - 2017-12-11 02:23:25.620901: step 77290, loss = 0.69, batch loss = 0.64 (10.2 examples/sec; 0.780 sec/batch; 55h:19m:48s remains)
INFO - root - 2017-12-11 02:23:33.473670: step 77300, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.802 sec/batch; 56h:49m:40s remains)
2017-12-11 02:23:34.291524: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18141581 0.19643822 0.21381626 0.2350094 0.26049104 0.28943008 0.3146508 0.32308322 0.30299723 0.25435793 0.19379827 0.14790983 0.13639233 0.1615762 0.20514411][0.13462672 0.15266778 0.17879854 0.21239683 0.25092557 0.29071835 0.32286432 0.3330951 0.31050515 0.25721964 0.19343235 0.14627239 0.13377543 0.15662019 0.19628792][0.0908525 0.10708248 0.13733363 0.18041663 0.23059882 0.28037998 0.31891993 0.33077666 0.30662739 0.25059479 0.18565162 0.13995422 0.130453 0.15589389 0.19795759][0.062973619 0.074816778 0.10662969 0.15711376 0.21758074 0.27644593 0.32060525 0.33343476 0.30735949 0.24778776 0.17958501 0.13327898 0.1262241 0.15666044 0.20591205][0.048637468 0.05766781 0.091480024 0.14981762 0.2215962 0.29033816 0.33921316 0.35035163 0.31894577 0.25181845 0.17579764 0.12470072 0.11789358 0.1533694 0.21079321][0.048937898 0.058280215 0.095987894 0.16294096 0.24726731 0.32723245 0.38068131 0.38787135 0.34701777 0.26727435 0.17779221 0.11585517 0.10305906 0.13752489 0.19847555][0.080786742 0.094026148 0.13230656 0.20104326 0.29111591 0.37803164 0.43427193 0.43719095 0.38662067 0.29315373 0.18787515 0.11054311 0.084507726 0.10869347 0.1643794][0.14849547 0.16643833 0.19981615 0.25965646 0.34208164 0.4233903 0.47314942 0.46688715 0.40534514 0.30014437 0.18317087 0.093535773 0.053655956 0.062950768 0.10692654][0.24083577 0.26337391 0.28494853 0.32209298 0.37793711 0.43410721 0.46258226 0.44057223 0.3699103 0.2620599 0.14613596 0.05602717 0.0098986821 0.0077037662 0.03919436][0.32385287 0.34443009 0.34613425 0.35018849 0.36758015 0.38836104 0.3904072 0.35450849 0.28279275 0.18444258 0.082749605 0.0038626788 -0.03843366 -0.044033661 -0.020563692][0.35493246 0.36496535 0.34553215 0.31959212 0.30282587 0.29333362 0.27557495 0.23437904 0.17068405 0.090347968 0.010296295 -0.050179865 -0.081255235 -0.083597586 -0.063684419][0.31887344 0.31417841 0.27849898 0.23388971 0.19592421 0.16800262 0.1405371 0.10239768 0.052993327 -0.00450885 -0.058197115 -0.094952613 -0.10976063 -0.10538141 -0.08756385][0.22309579 0.20612337 0.16528369 0.1171973 0.073766649 0.040577143 0.013062628 -0.016192738 -0.048489213 -0.081843764 -0.10826737 -0.1209219 -0.12002543 -0.110213 -0.095524848][0.10836059 0.087931126 0.052240022 0.011211148 -0.026779938 -0.055305023 -0.075727038 -0.093300454 -0.10941568 -0.12254224 -0.12801594 -0.12410705 -0.11420933 -0.10276532 -0.091814481][0.010406678 -0.0070333849 -0.031697139 -0.059956156 -0.086242683 -0.10467508 -0.11554716 -0.12273504 -0.12743497 -0.12841342 -0.12341479 -0.11328653 -0.1015309 -0.091092363 -0.082975864]]...]
INFO - root - 2017-12-11 02:23:42.143520: step 77310, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.777 sec/batch; 55h:05m:54s remains)
INFO - root - 2017-12-11 02:23:49.988288: step 77320, loss = 0.69, batch loss = 0.63 (10.0 examples/sec; 0.797 sec/batch; 56h:29m:43s remains)
INFO - root - 2017-12-11 02:23:57.649786: step 77330, loss = 0.69, batch loss = 0.64 (10.3 examples/sec; 0.779 sec/batch; 55h:12m:12s remains)
INFO - root - 2017-12-11 02:24:05.418993: step 77340, loss = 0.70, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 56h:29m:39s remains)
INFO - root - 2017-12-11 02:24:13.146113: step 77350, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 54h:34m:08s remains)
INFO - root - 2017-12-11 02:24:20.936825: step 77360, loss = 0.71, batch loss = 0.66 (10.3 examples/sec; 0.775 sec/batch; 54h:54m:15s remains)
INFO - root - 2017-12-11 02:24:28.901377: step 77370, loss = 0.69, batch loss = 0.64 (10.1 examples/sec; 0.791 sec/batch; 56h:03m:58s remains)
INFO - root - 2017-12-11 02:24:36.936763: step 77380, loss = 0.70, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 56h:10m:48s remains)
INFO - root - 2017-12-11 02:24:44.843367: step 77390, loss = 0.69, batch loss = 0.63 (10.3 examples/sec; 0.775 sec/batch; 54h:54m:38s remains)
INFO - root - 2017-12-11 02:24:52.753755: step 77400, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.792 sec/batch; 56h:08m:04s remains)
2017-12-11 02:24:53.659501: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.068892367 0.056515023 0.044033777 0.036899559 0.033799965 0.034382563 0.038657717 0.044390079 0.049018472 0.052917387 0.061330564 0.073415406 0.0837585 0.090605959 0.093700029][0.13354301 0.11520199 0.09406396 0.079420052 0.070634641 0.067898884 0.071231969 0.077327929 0.082988851 0.08974196 0.10382305 0.12351078 0.14122753 0.15498711 0.16471547][0.20024379 0.18020178 0.15463297 0.13371904 0.11839048 0.11096798 0.11215105 0.11691753 0.12156571 0.1292638 0.14661393 0.17078792 0.19269583 0.21110579 0.22651784][0.24230197 0.23085201 0.21243742 0.19428925 0.17921971 0.17273973 0.17594227 0.18076792 0.18246855 0.18583861 0.19910797 0.21862684 0.23516236 0.24901417 0.26302987][0.24483813 0.25469536 0.25965846 0.25948462 0.25766119 0.26265949 0.27565187 0.28458318 0.2829186 0.27694786 0.27727711 0.28004891 0.27691194 0.27091002 0.26936454][0.2152151 0.25745758 0.30021474 0.33253938 0.35566854 0.37999633 0.40766346 0.42252132 0.41578168 0.39523411 0.37538052 0.35233369 0.31793067 0.27864584 0.24812788][0.16985041 0.24754047 0.33371037 0.40545565 0.45854121 0.50363719 0.54456729 0.56220859 0.54678947 0.50766933 0.46399045 0.41208375 0.34333402 0.26737538 0.20366108][0.11732851 0.2219338 0.34347159 0.44965407 0.52864778 0.58940274 0.63757169 0.65296656 0.62618107 0.56816268 0.50290418 0.42759457 0.33207375 0.22866975 0.14025724][0.059934422 0.17266658 0.30852008 0.43118685 0.5227747 0.58914971 0.63674259 0.64662176 0.61119252 0.54241127 0.46655965 0.38184562 0.27688721 0.1652422 0.070033304][0.0039782715 0.1031252 0.22724091 0.3419272 0.42710823 0.48596993 0.525005 0.52861577 0.49154311 0.42491668 0.35418051 0.27815825 0.18478826 0.086783975 0.0043639606][-0.0455736 0.024463547 0.11682694 0.20340382 0.26632541 0.30752355 0.33313674 0.33206528 0.30069602 0.24734743 0.19403256 0.14002699 0.073704459 0.0050294497 -0.050866146][-0.083048128 -0.046084743 0.008414208 0.060053326 0.095563829 0.11669473 0.12883659 0.12540236 0.10356841 0.06800168 0.035875529 0.0070149275 -0.028043158 -0.063321739 -0.089623287][-0.10278434 -0.092360646 -0.068615206 -0.045459475 -0.031572003 -0.02572825 -0.023411637 -0.027224656 -0.03984965 -0.060013063 -0.075241491 -0.084733129 -0.095047824 -0.1041123 -0.10781147][-0.1052658 -0.11081929 -0.10533759 -0.098905735 -0.097319767 -0.10047042 -0.10446393 -0.10849186 -0.11527333 -0.12585385 -0.13170359 -0.13092977 -0.12748508 -0.12224008 -0.11427146][-0.096056953 -0.10754617 -0.10902007 -0.10894176 -0.11216988 -0.11975686 -0.1279158 -0.13332556 -0.13870792 -0.14611386 -0.14965665 -0.14673306 -0.13958515 -0.13055699 -0.12002438]]...]
INFO - root - 2017-12-11 02:25:01.294343: step 77410, loss = 0.69, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 57h:09m:16s remains)
INFO - root - 2017-12-11 02:25:09.256819: step 77420, loss = 0.69, batch loss = 0.63 (10.1 examples/sec; 0.791 sec/batch; 56h:02m:41s remains)
INFO - root - 2017-12-11 02:25:17.067064: step 77430, loss = 0.69, batch loss = 0.64 (10.5 examples/sec; 0.763 sec/batch; 54h:03m:38s remains)
INFO - root - 2017-12-11 02:25:24.763365: step 77440, loss = 0.71, batch loss = 0.65 (10.4 examples/sec; 0.768 sec/batch; 54h:26m:25s remains)
INFO - root - 2017-12-11 02:25:32.621597: step 77450, loss = 0.68, batch loss = 0.62 (10.4 examples/sec; 0.768 sec/batch; 54h:24m:31s remains)
INFO - root - 2017-12-11 02:25:40.477576: step 77460, loss = 0.70, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 57h:34m:35s remains)
INFO - root - 2017-12-11 02:25:48.440285: step 77470, loss = 0.71, batch loss = 0.65 (9.8 examples/sec; 0.820 sec/batch; 58h:06m:23s remains)
INFO - root - 2017-12-11 02:25:56.444538: step 77480, loss = 0.71, batch loss = 0.65 (10.3 examples/sec; 0.778 sec/batch; 55h:08m:13s remains)
INFO - root - 2017-12-11 02:26:04.040971: step 77490, loss = 0.68, batch loss = 0.62 (10.3 examples/sec; 0.779 sec/batch; 55h:11m:51s remains)
INFO - root - 2017-12-11 02:26:13.393923: step 77500, loss = 0.71, batch loss = 0.65 (7.4 examples/sec; 1.080 sec/batch; 76h:30m:22s remains)
2017-12-11 02:26:14.420609: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.044743836 0.10428344 0.16501944 0.20903416 0.22502385 0.21356623 0.18196884 0.13521698 0.080633871 0.0298816 -0.0055148145 -0.025106946 -0.03773088 -0.0461742 -0.049532115][0.035917316 0.092530295 0.15062276 0.19253871 0.20699349 0.19535509 0.16522789 0.1213275 0.069622323 0.021440193 -0.011837229 -0.029910468 -0.0418349 -0.049849432 -0.052142017][0.025609918 0.07655035 0.12932779 0.16717206 0.1793448 0.16840565 0.1434271 0.10826694 0.06623143 0.027213965 0.00116815 -0.013352863 -0.025925882 -0.037268404 -0.043359693][0.020571178 0.068379559 0.11903034 0.15578794 0.16891228 0.16254477 0.14719239 0.12464159 0.096276477 0.070832811 0.055474032 0.043894116 0.025405454 0.0026237604 -0.016125008][0.025225664 0.07594423 0.13015355 0.17058484 0.18909712 0.19135389 0.18744929 0.17664696 0.16135882 0.1509137 0.14762427 0.13758968 0.10933555 0.069833733 0.031313017][0.0425106 0.10179764 0.16320392 0.20870537 0.23293661 0.24325313 0.24742566 0.2437295 0.23924537 0.2444462 0.25461403 0.24727936 0.21070918 0.15584363 0.096588708][0.070190728 0.13985065 0.20875627 0.25734827 0.28389198 0.29847535 0.30616543 0.3054941 0.30967066 0.33053681 0.35540494 0.35409421 0.31512704 0.25249743 0.17981628][0.092192724 0.17036253 0.24529962 0.29634744 0.32448751 0.3415091 0.35011727 0.34974387 0.35877511 0.39007357 0.42495108 0.42926547 0.39257064 0.330774 0.25545722][0.098175436 0.1794638 0.25677797 0.30912453 0.3384468 0.35712129 0.36541256 0.36338338 0.372787 0.40701175 0.4445892 0.45188069 0.42139205 0.36961171 0.3039262][0.087585762 0.16586527 0.24063551 0.2914516 0.31959882 0.33699271 0.34194374 0.33549598 0.34055829 0.36997959 0.40303668 0.41083291 0.38950637 0.35386667 0.30649084][0.064152218 0.13328257 0.1997321 0.24459492 0.267531 0.27936587 0.27834505 0.26697195 0.2664645 0.28768152 0.31315804 0.32080904 0.3094258 0.29058832 0.26368222][0.034070652 0.090112947 0.14462024 0.18119361 0.19776353 0.20336391 0.19727816 0.18306231 0.17770377 0.18960145 0.20597267 0.21169087 0.20713665 0.20069756 0.19073667][0.0043946458 0.046269868 0.088081039 0.11610062 0.12718287 0.12824799 0.11959375 0.10469024 0.095544726 0.098439537 0.10541263 0.10696273 0.10435627 0.10353132 0.10302064][-0.016789457 0.012772869 0.043186814 0.062963918 0.068970248 0.066844463 0.057947122 0.044872414 0.035137475 0.033226281 0.034764584 0.033761505 0.031439371 0.03173241 0.033969924][-0.027939815 -0.007277227 0.014141562 0.026740389 0.027904823 0.022646394 0.013733402 0.0032224108 -0.004839804 -0.0078175189 -0.0080091711 -0.00940939 -0.011489371 -0.012077291 -0.011018357]]...]
INFO - root - 2017-12-11 02:26:24.542001: step 77510, loss = 0.69, batch loss = 0.63 (8.0 examples/sec; 1.003 sec/batch; 71h:03m:29s remains)
INFO - root - 2017-12-11 02:26:34.588189: step 77520, loss = 0.70, batch loss = 0.64 (9.0 examples/sec; 0.890 sec/batch; 63h:01m:44s remains)
INFO - root - 2017-12-11 02:26:44.713326: step 77530, loss = 0.69, batch loss = 0.63 (7.8 examples/sec; 1.020 sec/batch; 72h:14m:52s remains)
INFO - root - 2017-12-11 02:26:54.910472: step 77540, loss = 0.69, batch loss = 0.63 (7.7 examples/sec; 1.035 sec/batch; 73h:19m:08s remains)
INFO - root - 2017-12-11 02:27:05.055969: step 77550, loss = 0.69, batch loss = 0.63 (8.4 examples/sec; 0.953 sec/batch; 67h:31m:20s remains)
INFO - root - 2017-12-11 02:27:15.024874: step 77560, loss = 0.70, batch loss = 0.64 (7.9 examples/sec; 1.018 sec/batch; 72h:05m:15s remains)
INFO - root - 2017-12-11 02:27:24.911003: step 77570, loss = 0.69, batch loss = 0.63 (7.9 examples/sec; 1.016 sec/batch; 71h:58m:49s remains)
INFO - root - 2017-12-11 02:27:34.758093: step 77580, loss = 0.68, batch loss = 0.62 (7.9 examples/sec; 1.016 sec/batch; 71h:56m:23s remains)
INFO - root - 2017-12-11 02:27:44.774328: step 77590, loss = 0.71, batch loss = 0.65 (7.6 examples/sec; 1.059 sec/batch; 75h:00m:03s remains)
INFO - root - 2017-12-11 02:27:54.772417: step 77600, loss = 0.70, batch loss = 0.64 (7.6 examples/sec; 1.048 sec/batch; 74h:12m:24s remains)
2017-12-11 02:27:55.734715: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.090866476 0.16114292 0.22319625 0.25907853 0.25910589 0.23241799 0.19585364 0.16763389 0.15846203 0.16750625 0.18996666 0.21590914 0.24310942 0.26934305 0.28646359][0.097266711 0.17486547 0.24597998 0.2900562 0.29542127 0.2711418 0.2355022 0.2076119 0.19831052 0.20646308 0.22829491 0.25532967 0.28487122 0.31328085 0.32987061][0.088370644 0.16765922 0.24433675 0.29657143 0.31095108 0.2955915 0.26865116 0.24793079 0.24221916 0.24891542 0.26598555 0.28819212 0.3142181 0.34109 0.3563256][0.07278306 0.15154749 0.23258893 0.29287955 0.31737465 0.31370774 0.2996839 0.29091066 0.29259408 0.29967195 0.310843 0.3239648 0.34074163 0.36089224 0.37266156][0.05363103 0.12983677 0.21302705 0.27906165 0.31172538 0.31908029 0.31885195 0.32455257 0.33750105 0.34932283 0.35735291 0.36075142 0.36475372 0.37405393 0.38040107][0.035270128 0.10759571 0.19044133 0.25935405 0.29745317 0.31265816 0.32286158 0.34060243 0.36441708 0.3831583 0.39107656 0.38674733 0.37745506 0.37357613 0.3725788][0.029025583 0.10032764 0.18453163 0.25722006 0.30037042 0.320278 0.33416641 0.35434133 0.37939498 0.39879724 0.40467116 0.39343989 0.37272337 0.35714689 0.34952602][0.034617096 0.10709716 0.19377925 0.270586 0.31790662 0.33867934 0.34844935 0.35994756 0.37439436 0.38505378 0.38472015 0.36727965 0.33886823 0.31568977 0.304741][0.041088533 0.11218069 0.19773543 0.27472958 0.32248604 0.33989394 0.33983183 0.33561751 0.33250144 0.32944164 0.32117176 0.29930422 0.26769486 0.24178451 0.23071361][0.043325972 0.10972174 0.18928991 0.26083347 0.30415821 0.31478813 0.30277634 0.28163525 0.2601262 0.24260023 0.2264583 0.20304719 0.17434444 0.1530066 0.14728752][0.037621111 0.095612258 0.16451082 0.22566722 0.261058 0.26411268 0.24204586 0.20750755 0.17127089 0.14163549 0.11912544 0.097092383 0.076263204 0.065094665 0.068422005][0.021662673 0.0677394 0.12243409 0.17040692 0.19675241 0.19436267 0.16760103 0.12690023 0.083199114 0.046819437 0.02133435 0.0025367471 -0.0093409661 -0.0099870609 0.0016160622][-0.0054847053 0.024879061 0.061818093 0.093751386 0.11002571 0.10463648 0.079555072 0.042325784 0.0022566868 -0.0310573 -0.053095851 -0.066542521 -0.071161769 -0.065351807 -0.0503505][-0.035154421 -0.0199511 0.00018502427 0.016911391 0.023936039 0.017425666 -0.0019289819 -0.029163402 -0.057490449 -0.079995073 -0.093442984 -0.099839881 -0.098729037 -0.089892693 -0.075270742][-0.0558991 -0.051167913 -0.042782098 -0.036770862 -0.036099352 -0.042558152 -0.055695336 -0.072407089 -0.088382415 -0.099604666 -0.10461187 -0.10490879 -0.099910691 -0.089989632 -0.077195056]]...]
INFO - root - 2017-12-11 02:28:04.236304: step 77610, loss = 0.69, batch loss = 0.64 (16.9 examples/sec; 0.472 sec/batch; 33h:26m:18s remains)
INFO - root - 2017-12-11 02:28:08.589279: step 77620, loss = 0.71, batch loss = 0.65 (18.0 examples/sec; 0.445 sec/batch; 31h:29m:22s remains)
INFO - root - 2017-12-11 02:28:13.098275: step 77630, loss = 0.69, batch loss = 0.63 (18.0 examples/sec; 0.443 sec/batch; 31h:23m:39s remains)
INFO - root - 2017-12-11 02:28:17.561569: step 77640, loss = 0.69, batch loss = 0.63 (17.4 examples/sec; 0.460 sec/batch; 32h:35m:27s remains)
INFO - root - 2017-12-11 02:28:21.996915: step 77650, loss = 0.71, batch loss = 0.65 (18.6 examples/sec; 0.430 sec/batch; 30h:24m:57s remains)
INFO - root - 2017-12-11 02:28:26.533105: step 77660, loss = 0.68, batch loss = 0.62 (17.6 examples/sec; 0.455 sec/batch; 32h:13m:48s remains)
INFO - root - 2017-12-11 02:28:31.044808: step 77670, loss = 0.68, batch loss = 0.62 (17.8 examples/sec; 0.451 sec/batch; 31h:54m:08s remains)
INFO - root - 2017-12-11 02:28:35.658516: step 77680, loss = 0.71, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 32h:10m:05s remains)
INFO - root - 2017-12-11 02:28:40.163806: step 77690, loss = 0.68, batch loss = 0.62 (18.0 examples/sec; 0.443 sec/batch; 31h:22m:49s remains)
INFO - root - 2017-12-11 02:28:44.699182: step 77700, loss = 0.69, batch loss = 0.64 (18.1 examples/sec; 0.441 sec/batch; 31h:13m:05s remains)
2017-12-11 02:28:45.228585: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012439227 0.025164023 0.032804355 0.03315993 0.029306944 0.024797915 0.020781895 0.01671923 0.011955182 0.0056833895 -0.00066386705 -0.0071493788 -0.014603196 -0.022825081 -0.030752504][0.023140641 0.044878498 0.063635811 0.075399674 0.083469763 0.090697519 0.096260622 0.098309323 0.095843256 0.089161918 0.081825852 0.074867867 0.066364363 0.055619441 0.043988895][0.035903491 0.064056963 0.092753544 0.11640157 0.13816565 0.15931955 0.17689329 0.18786731 0.19035171 0.18578184 0.18232703 0.18179685 0.17872819 0.16929841 0.15539801][0.050809536 0.083329991 0.12086196 0.15675999 0.19299074 0.22819358 0.25763944 0.27731043 0.28421375 0.28143933 0.28336695 0.29216471 0.29767469 0.29134732 0.27547762][0.069444947 0.1053765 0.15071292 0.19949502 0.25113308 0.30028138 0.34095454 0.36835891 0.37748075 0.37268883 0.37590834 0.39171627 0.40533423 0.40221888 0.38440412][0.085584022 0.12398458 0.17628621 0.23840116 0.30593625 0.36832023 0.41853991 0.4519985 0.46189526 0.45370352 0.45548484 0.47712272 0.50081795 0.50376219 0.48656392][0.09571363 0.1343509 0.19161831 0.26554275 0.34772712 0.42233041 0.4815931 0.52114725 0.53279936 0.52243704 0.52228236 0.54922974 0.5833922 0.59297341 0.57546061][0.09922535 0.13755912 0.19754492 0.27972889 0.37280726 0.4562518 0.52171546 0.5654788 0.57868975 0.56681985 0.56364089 0.59357446 0.636095 0.65272939 0.63623947][0.097131945 0.13400516 0.19377594 0.27857941 0.3758294 0.46216941 0.5284667 0.57165116 0.5839414 0.57046497 0.56239039 0.59010512 0.63511181 0.65766656 0.64529777][0.089592882 0.11929791 0.17138512 0.24862792 0.3389357 0.4187142 0.47842407 0.51570582 0.52542657 0.5124566 0.50178647 0.52576858 0.56938624 0.59596294 0.58947676][0.066816762 0.084162213 0.12247527 0.18390556 0.25888368 0.32652584 0.37708503 0.4083451 0.41750029 0.40920588 0.40067294 0.42110643 0.45869198 0.48269343 0.47749582][0.031859592 0.036208443 0.059214707 0.10218299 0.15902677 0.21285446 0.25411931 0.27977279 0.28830579 0.28458363 0.28040755 0.2985909 0.32871675 0.34697866 0.34078151][-0.0020630572 -0.010174558 -0.001503666 0.023825731 0.062867634 0.10277563 0.13444017 0.15381873 0.16069083 0.16030864 0.16056494 0.17687748 0.20036167 0.21487999 0.21207622][-0.021384438 -0.0437569 -0.049948636 -0.041355778 -0.019313216 0.0070046275 0.029268689 0.042975366 0.048660126 0.051235277 0.055138852 0.06893789 0.0867661 0.10002827 0.10449947][-0.021692446 -0.059792466 -0.081457555 -0.087704256 -0.079949789 -0.065111749 -0.050782625 -0.04133315 -0.036207646 -0.031781368 -0.02612742 -0.015202706 -0.0012992106 0.012387643 0.02426799]]...]
INFO - root - 2017-12-11 02:28:49.764549: step 77710, loss = 0.69, batch loss = 0.64 (17.5 examples/sec; 0.457 sec/batch; 32h:19m:30s remains)
INFO - root - 2017-12-11 02:28:54.253783: step 77720, loss = 0.71, batch loss = 0.65 (17.8 examples/sec; 0.450 sec/batch; 31h:51m:25s remains)
INFO - root - 2017-12-11 02:28:58.720183: step 77730, loss = 0.70, batch loss = 0.64 (18.3 examples/sec; 0.436 sec/batch; 30h:52m:59s remains)
INFO - root - 2017-12-11 02:29:03.310931: step 77740, loss = 0.69, batch loss = 0.63 (17.7 examples/sec; 0.452 sec/batch; 31h:57m:58s remains)
INFO - root - 2017-12-11 02:29:07.964664: step 77750, loss = 0.70, batch loss = 0.64 (16.4 examples/sec; 0.488 sec/batch; 34h:32m:31s remains)
INFO - root - 2017-12-11 02:29:12.471166: step 77760, loss = 0.70, batch loss = 0.64 (18.1 examples/sec; 0.443 sec/batch; 31h:18m:44s remains)
INFO - root - 2017-12-11 02:29:16.968358: step 77770, loss = 0.71, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 32h:09m:16s remains)
INFO - root - 2017-12-11 02:29:21.518128: step 77780, loss = 0.70, batch loss = 0.65 (17.0 examples/sec; 0.469 sec/batch; 33h:12m:50s remains)
INFO - root - 2017-12-11 02:29:26.005042: step 77790, loss = 0.70, batch loss = 0.64 (17.6 examples/sec; 0.455 sec/batch; 32h:11m:13s remains)
INFO - root - 2017-12-11 02:29:30.470471: step 77800, loss = 0.69, batch loss = 0.63 (17.9 examples/sec; 0.446 sec/batch; 31h:32m:15s remains)
2017-12-11 02:29:31.023395: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16893548 0.13679561 0.090176232 0.059409369 0.060736004 0.091149554 0.12991077 0.15055645 0.14643559 0.11460528 0.0626625 0.0039578276 -0.045458317 -0.075260267 -0.091626242][0.28275648 0.24908598 0.19092116 0.14883266 0.14324877 0.17113782 0.21003513 0.22979297 0.22096674 0.17909931 0.11399799 0.040463731 -0.023352604 -0.064914 -0.089722142][0.38861185 0.3607721 0.30245662 0.25837761 0.24934159 0.27276635 0.30536878 0.31606159 0.2945509 0.23774244 0.15947868 0.074095063 -0.00062071992 -0.051945809 -0.083229676][0.4707031 0.45505467 0.40793163 0.37429 0.37156397 0.39669967 0.4243699 0.42275047 0.38271737 0.30655232 0.21237537 0.11247793 0.024244042 -0.038393006 -0.076411396][0.5257718 0.52675408 0.49840567 0.48290786 0.49408764 0.5298962 0.56034172 0.55114824 0.4934471 0.39774519 0.2858772 0.16598615 0.057520892 -0.021057772 -0.067130253][0.56398439 0.58689809 0.58234 0.58674824 0.61363494 0.66387528 0.70162338 0.68868387 0.61596632 0.5016275 0.36904398 0.22316228 0.089266546 -0.0062439884 -0.058833681][0.58504277 0.63216895 0.65091026 0.67091411 0.70828331 0.76902026 0.81381756 0.80210841 0.72263253 0.59601176 0.44444388 0.27320066 0.11563245 0.0054567875 -0.052079942][0.57991505 0.64772141 0.683416 0.710609 0.74930578 0.81138265 0.85767376 0.84942883 0.77294338 0.64505571 0.4832164 0.29619008 0.12517747 0.0079905707 -0.051232059][0.53871268 0.6165204 0.65787089 0.68075955 0.7088173 0.7593112 0.79862672 0.79299247 0.72670293 0.61038393 0.45501876 0.27170724 0.1050429 -0.0068188021 -0.061489534][0.45468846 0.52913839 0.56515813 0.57539773 0.58513951 0.61457747 0.63887161 0.63122642 0.57654566 0.48033297 0.34835675 0.19159281 0.050805483 -0.0406185 -0.082076974][0.32992381 0.39030915 0.4154039 0.41265169 0.40537131 0.41254243 0.41810507 0.4033547 0.35762903 0.28430632 0.18629915 0.07247977 -0.026162522 -0.085597105 -0.10721337][0.17723033 0.21453026 0.2250693 0.2127825 0.19578399 0.18859729 0.18164195 0.16377339 0.12936409 0.081151977 0.020849621 -0.045562923 -0.098726004 -0.12476694 -0.12658717][0.03627371 0.049124535 0.045741789 0.027529921 0.0070761857 -0.00588184 -0.016698264 -0.031469546 -0.052014146 -0.076106176 -0.10314362 -0.13005494 -0.14689131 -0.14740239 -0.13518345][-0.061970294 -0.06762898 -0.080138117 -0.10040622 -0.12024716 -0.1331276 -0.14145657 -0.14911713 -0.15653513 -0.16232884 -0.16668291 -0.16860138 -0.16401736 -0.15090826 -0.13262863][-0.11584985 -0.13088784 -0.14597209 -0.16367942 -0.17918453 -0.18822646 -0.19180025 -0.19260597 -0.19105585 -0.1865782 -0.17955483 -0.16988124 -0.15626559 -0.13923417 -0.12143062]]...]
INFO - root - 2017-12-11 02:29:35.576982: step 77810, loss = 0.69, batch loss = 0.63 (18.0 examples/sec; 0.445 sec/batch; 31h:27m:59s remains)
INFO - root - 2017-12-11 02:29:40.062696: step 77820, loss = 0.68, batch loss = 0.63 (18.1 examples/sec; 0.443 sec/batch; 31h:19m:15s remains)
INFO - root - 2017-12-11 02:29:44.507693: step 77830, loss = 0.69, batch loss = 0.64 (16.9 examples/sec; 0.473 sec/batch; 33h:27m:22s remains)
INFO - root - 2017-12-11 02:29:48.985670: step 77840, loss = 0.69, batch loss = 0.63 (17.7 examples/sec; 0.451 sec/batch; 31h:55m:26s remains)
INFO - root - 2017-12-11 02:29:53.546666: step 77850, loss = 0.70, batch loss = 0.64 (17.0 examples/sec; 0.469 sec/batch; 33h:11m:38s remains)
INFO - root - 2017-12-11 02:29:58.043106: step 77860, loss = 0.69, batch loss = 0.63 (17.7 examples/sec; 0.453 sec/batch; 32h:02m:39s remains)
INFO - root - 2017-12-11 02:30:02.609619: step 77870, loss = 0.68, batch loss = 0.62 (17.1 examples/sec; 0.468 sec/batch; 33h:05m:53s remains)
INFO - root - 2017-12-11 02:30:07.089471: step 77880, loss = 0.70, batch loss = 0.64 (18.2 examples/sec; 0.438 sec/batch; 31h:00m:50s remains)
INFO - root - 2017-12-11 02:30:11.722274: step 77890, loss = 0.69, batch loss = 0.63 (17.8 examples/sec; 0.451 sec/batch; 31h:51m:47s remains)
INFO - root - 2017-12-11 02:30:16.252424: step 77900, loss = 0.71, batch loss = 0.65 (17.4 examples/sec; 0.460 sec/batch; 32h:30m:37s remains)
2017-12-11 02:30:16.855051: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20749435 0.22587284 0.25696352 0.29300573 0.32258248 0.32986966 0.31528345 0.28304279 0.24088164 0.20053428 0.16937582 0.16472736 0.1818497 0.20406577 0.22041596][0.26683113 0.27915585 0.29641664 0.31397846 0.3257 0.32144246 0.30240992 0.27337733 0.24062158 0.21147443 0.19018836 0.19034174 0.20851123 0.2314529 0.25003514][0.31744418 0.324153 0.32540005 0.31999776 0.30877075 0.29094347 0.26919204 0.247174 0.22840962 0.21537983 0.20874162 0.21406727 0.22863491 0.24523151 0.25908434][0.35708648 0.36126021 0.351214 0.32805496 0.29902792 0.27328044 0.25423053 0.24217942 0.23695914 0.23780434 0.2419589 0.24616499 0.24871014 0.25000948 0.2515713][0.37704998 0.38363338 0.37134567 0.3430154 0.30985317 0.28781417 0.27873915 0.27776444 0.28089571 0.28624904 0.28966931 0.28196445 0.26399308 0.24354894 0.22811384][0.37540352 0.38685018 0.37912509 0.35754731 0.33429652 0.32633185 0.33182111 0.34040585 0.34486279 0.34451807 0.33643362 0.31000546 0.2695269 0.22800203 0.19586286][0.35421559 0.36909321 0.3691431 0.3607811 0.35456032 0.36339679 0.38125417 0.39415285 0.39400575 0.38349679 0.36174354 0.31987917 0.2648479 0.21080171 0.16701198][0.32312241 0.34071639 0.34859011 0.35253304 0.36005962 0.37824374 0.39890984 0.40814248 0.40053689 0.38287845 0.35527223 0.31140733 0.25746948 0.20478284 0.15734597][0.29622254 0.31553593 0.32686213 0.335802 0.34706855 0.3627381 0.37537807 0.37512222 0.36151347 0.34410131 0.3230916 0.29463267 0.25968772 0.22274116 0.18085444][0.28726462 0.30521676 0.31234249 0.3155269 0.317995 0.31955516 0.31638008 0.30553779 0.29154408 0.28392458 0.28298065 0.28671509 0.28691044 0.27689782 0.24730641][0.29702225 0.30864897 0.30330318 0.29112306 0.27531055 0.25657496 0.23663117 0.2197476 0.2132697 0.22380295 0.25183305 0.29611629 0.33734792 0.35547858 0.33745417][0.31250879 0.31235886 0.28995565 0.25942659 0.22443372 0.18819873 0.15787183 0.14321534 0.15155311 0.18525511 0.24386013 0.32456073 0.39813152 0.43381429 0.4194805][0.32377404 0.30909437 0.26968578 0.22388941 0.17560644 0.13030325 0.0987076 0.093037032 0.11891332 0.17507097 0.25807637 0.360222 0.44690365 0.48400149 0.46530312][0.32856989 0.30043405 0.24917078 0.19547996 0.1427322 0.09780255 0.071747229 0.076306187 0.11556455 0.18626426 0.27996796 0.38260531 0.46118063 0.48656273 0.46042532][0.32143751 0.2867415 0.23269874 0.17927253 0.12941204 0.091163918 0.072838657 0.083478674 0.1271881 0.20089936 0.29002112 0.37488019 0.43036753 0.43808588 0.40542164]]...]
INFO - root - 2017-12-11 02:30:21.463573: step 77910, loss = 0.69, batch loss = 0.64 (16.8 examples/sec; 0.475 sec/batch; 33h:35m:55s remains)
INFO - root - 2017-12-11 02:30:26.001891: step 77920, loss = 0.69, batch loss = 0.63 (17.0 examples/sec; 0.470 sec/batch; 33h:15m:43s remains)
INFO - root - 2017-12-11 02:30:30.568218: step 77930, loss = 0.68, batch loss = 0.62 (17.8 examples/sec; 0.451 sec/batch; 31h:52m:14s remains)
INFO - root - 2017-12-11 02:30:35.082457: step 77940, loss = 0.70, batch loss = 0.64 (17.6 examples/sec; 0.454 sec/batch; 32h:05m:05s remains)
INFO - root - 2017-12-11 02:30:39.668724: step 77950, loss = 0.68, batch loss = 0.62 (17.8 examples/sec; 0.449 sec/batch; 31h:44m:35s remains)
INFO - root - 2017-12-11 02:30:44.068489: step 77960, loss = 0.69, batch loss = 0.63 (18.1 examples/sec; 0.442 sec/batch; 31h:16m:54s remains)
INFO - root - 2017-12-11 02:30:48.566093: step 77970, loss = 0.70, batch loss = 0.65 (17.8 examples/sec; 0.450 sec/batch; 31h:50m:20s remains)
INFO - root - 2017-12-11 02:30:53.123367: step 77980, loss = 0.70, batch loss = 0.64 (17.0 examples/sec; 0.471 sec/batch; 33h:18m:50s remains)
INFO - root - 2017-12-11 02:30:57.578287: step 77990, loss = 0.69, batch loss = 0.63 (22.1 examples/sec; 0.362 sec/batch; 25h:36m:03s remains)
INFO - root - 2017-12-11 02:31:02.178133: step 78000, loss = 0.69, batch loss = 0.63 (17.6 examples/sec; 0.455 sec/batch; 32h:11m:17s remains)
2017-12-11 02:31:02.720087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.075187169 -0.038145036 0.033991631 0.14660186 0.29465863 0.4539156 0.58160913 0.64160872 0.618245 0.51237339 0.34696713 0.16750856 0.022904158 -0.066467829 -0.10875771][-0.067959964 -0.021627236 0.061667621 0.18947032 0.35691351 0.535795 0.67781961 0.74220783 0.71491027 0.59526938 0.40861973 0.20497248 0.040530466 -0.059423428 -0.10691794][-0.058997195 -0.0035211947 0.089341432 0.22912617 0.40966946 0.598709 0.74454397 0.80424088 0.766005 0.63000542 0.42652437 0.20913595 0.036810428 -0.063606128 -0.1085515][-0.047022585 0.020274315 0.12821248 0.28453571 0.47744313 0.67011327 0.80899686 0.85066694 0.78767312 0.62693208 0.40771103 0.18424547 0.012704117 -0.08189287 -0.11945146][-0.029554384 0.05686282 0.190001 0.3711634 0.57930142 0.77252007 0.89567983 0.90703005 0.8078267 0.61709851 0.38353798 0.15826918 -0.0094994362 -0.098057106 -0.128425][-0.01159758 0.094836563 0.25317684 0.45743269 0.6772216 0.866583 0.97043937 0.95232224 0.82177532 0.60864174 0.36766738 0.14438437 -0.0196586 -0.10565285 -0.1327581][-0.0016946412 0.11415721 0.28394729 0.49612662 0.71572065 0.89631754 0.98695272 0.95692521 0.81726605 0.60113525 0.36386555 0.14573897 -0.016849168 -0.10475113 -0.13258938][-0.0057656863 0.10489598 0.26698625 0.46752089 0.67297137 0.84129566 0.92842996 0.90706635 0.78287649 0.58388758 0.36068437 0.1508808 -0.010043442 -0.10101123 -0.13172121][-0.022726761 0.069185 0.20572767 0.37527052 0.55133855 0.70038283 0.78666222 0.78519803 0.69378114 0.52987969 0.33458558 0.14357403 -0.0077960133 -0.097354867 -0.12976846][-0.050778795 0.013294442 0.11158902 0.23724104 0.37388051 0.49743137 0.57917708 0.59715557 0.54255319 0.42182142 0.26556274 0.10706118 -0.021050649 -0.099116623 -0.12758325][-0.082933158 -0.051629193 0.002223898 0.078080975 0.17041299 0.26368886 0.33503827 0.36605594 0.34374759 0.26616254 0.15461819 0.038398493 -0.055336084 -0.11167883 -0.12921169][-0.10691579 -0.10386879 -0.0884719 -0.056999195 -0.00631566 0.055213526 0.11007778 0.14353907 0.14214469 0.099820711 0.030094035 -0.043209989 -0.099544823 -0.12956494 -0.13250108][-0.11648946 -0.12882881 -0.13568757 -0.13313407 -0.1143052 -0.082879387 -0.050980493 -0.028182777 -0.023995368 -0.044008572 -0.08063408 -0.11682098 -0.13948794 -0.14484583 -0.13442881][-0.1169733 -0.13457105 -0.14999992 -0.16173591 -0.16322555 -0.15568309 -0.14616776 -0.13869455 -0.13735807 -0.14515772 -0.15835059 -0.16682157 -0.16501927 -0.15378623 -0.13610765][-0.11744248 -0.13547888 -0.152096 -0.16821085 -0.17881733 -0.18392085 -0.1870254 -0.18854149 -0.18901366 -0.18952866 -0.18849003 -0.18106294 -0.16659361 -0.14807503 -0.1287916]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 02:31:07.291802: step 78010, loss = 0.69, batch loss = 0.63 (19.1 examples/sec; 0.418 sec/batch; 29h:33m:14s remains)
INFO - root - 2017-12-11 02:31:11.923428: step 78020, loss = 0.70, batch loss = 0.65 (17.2 examples/sec; 0.465 sec/batch; 32h:54m:08s remains)
INFO - root - 2017-12-11 02:31:16.545139: step 78030, loss = 0.70, batch loss = 0.64 (17.1 examples/sec; 0.468 sec/batch; 33h:03m:36s remains)
INFO - root - 2017-12-11 02:31:21.100134: step 78040, loss = 0.70, batch loss = 0.65 (16.9 examples/sec; 0.474 sec/batch; 33h:28m:50s remains)
INFO - root - 2017-12-11 02:31:25.694550: step 78050, loss = 0.70, batch loss = 0.64 (17.0 examples/sec; 0.471 sec/batch; 33h:17m:22s remains)
INFO - root - 2017-12-11 02:31:30.303829: step 78060, loss = 0.68, batch loss = 0.63 (17.4 examples/sec; 0.460 sec/batch; 32h:29m:48s remains)
INFO - root - 2017-12-11 02:31:34.889370: step 78070, loss = 0.67, batch loss = 0.62 (17.0 examples/sec; 0.471 sec/batch; 33h:18m:57s remains)
INFO - root - 2017-12-11 02:31:39.531507: step 78080, loss = 0.69, batch loss = 0.63 (17.8 examples/sec; 0.451 sec/batch; 31h:50m:27s remains)
INFO - root - 2017-12-11 02:31:44.151929: step 78090, loss = 0.70, batch loss = 0.64 (17.7 examples/sec; 0.451 sec/batch; 31h:51m:55s remains)
INFO - root - 2017-12-11 02:31:48.706682: step 78100, loss = 0.68, batch loss = 0.63 (17.8 examples/sec; 0.449 sec/batch; 31h:44m:07s remains)
2017-12-11 02:31:49.289562: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049107552 0.058443636 0.075492039 0.097472616 0.12293844 0.14652364 0.16151661 0.16344336 0.15876245 0.15984879 0.17441368 0.20216416 0.23054759 0.25208214 0.25814697][0.064458467 0.0774368 0.098330908 0.12490167 0.15472135 0.18278077 0.20113432 0.20342781 0.19598897 0.19138508 0.19866036 0.21913521 0.24273585 0.2613714 0.26497585][0.1057499 0.119256 0.13867067 0.1635107 0.19167323 0.21973076 0.23944873 0.24205561 0.23255557 0.22316962 0.22438662 0.23868038 0.25724581 0.27192366 0.27221906][0.17308429 0.18130194 0.19375508 0.21326412 0.23724879 0.263379 0.28374648 0.28687468 0.27538607 0.2610473 0.25753564 0.26908746 0.2864427 0.3014493 0.30146724][0.25122905 0.25018418 0.25335792 0.26787117 0.28979287 0.31804463 0.34637538 0.35905793 0.35215187 0.335259 0.3263008 0.33353597 0.3478362 0.36033604 0.35583511][0.33040139 0.32272413 0.31801891 0.32605505 0.34304255 0.37288737 0.4133769 0.4426291 0.44518897 0.42655757 0.40854785 0.40515962 0.40981939 0.41481149 0.4036915][0.3859289 0.38040254 0.37311524 0.37468252 0.38444906 0.4145104 0.46654636 0.51170456 0.52187365 0.4986738 0.4675712 0.44805336 0.43859062 0.43412083 0.41829643][0.41205126 0.41063625 0.40115756 0.39521605 0.39683679 0.42564619 0.48511937 0.54033911 0.55346757 0.52319205 0.4773944 0.44031844 0.41599649 0.40274337 0.38612413][0.40722182 0.40571234 0.3906492 0.37496212 0.36644781 0.38745406 0.44244039 0.49567708 0.50695646 0.47278914 0.42007622 0.37479049 0.34462878 0.32993937 0.31844878][0.35391891 0.349862 0.32971495 0.30652139 0.28915307 0.29804823 0.33892813 0.38085267 0.38771236 0.35446534 0.30339479 0.25850525 0.22919722 0.2176329 0.21426977][0.25617582 0.24938327 0.22695844 0.20018288 0.1766914 0.17236772 0.19354387 0.21759294 0.21685912 0.18599866 0.14149965 0.10322672 0.080732547 0.077577367 0.086071312][0.13044293 0.12354568 0.10494256 0.083219029 0.062782541 0.053275272 0.059313241 0.066170946 0.056719363 0.02776821 -0.0079485234 -0.034862742 -0.045638509 -0.037430942 -0.017526735][0.018551458 0.014484781 0.0040823938 -0.0067014508 -0.016644582 -0.022475354 -0.022732079 -0.026287092 -0.04048587 -0.065388277 -0.090613484 -0.10426678 -0.1027393 -0.085443608 -0.060170606][-0.046864167 -0.045486793 -0.045997649 -0.0447701 -0.0423821 -0.040303171 -0.040418476 -0.047998179 -0.063145824 -0.082966261 -0.098018229 -0.098677926 -0.084999174 -0.060208119 -0.033631004][-0.05244559 -0.043865222 -0.033732504 -0.020420751 -0.0061621564 0.003807612 0.0049566738 -0.0053085261 -0.021885328 -0.039199349 -0.047670469 -0.038656566 -0.015348859 0.014578502 0.04037109]]...]
INFO - root - 2017-12-11 02:31:53.850161: step 78110, loss = 0.69, batch loss = 0.63 (17.2 examples/sec; 0.466 sec/batch; 32h:55m:55s remains)
INFO - root - 2017-12-11 02:31:58.371851: step 78120, loss = 0.68, batch loss = 0.62 (17.7 examples/sec; 0.452 sec/batch; 31h:55m:26s remains)
INFO - root - 2017-12-11 02:32:02.662901: step 78130, loss = 0.70, batch loss = 0.64 (17.5 examples/sec; 0.457 sec/batch; 32h:16m:46s remains)
INFO - root - 2017-12-11 02:32:07.187286: step 78140, loss = 0.69, batch loss = 0.63 (17.5 examples/sec; 0.458 sec/batch; 32h:20m:51s remains)
INFO - root - 2017-12-11 02:32:11.750685: step 78150, loss = 0.71, batch loss = 0.65 (17.4 examples/sec; 0.459 sec/batch; 32h:25m:57s remains)
INFO - root - 2017-12-11 02:32:16.332029: step 78160, loss = 0.71, batch loss = 0.65 (17.3 examples/sec; 0.463 sec/batch; 32h:41m:19s remains)
INFO - root - 2017-12-11 02:32:20.922771: step 78170, loss = 0.68, batch loss = 0.62 (17.3 examples/sec; 0.462 sec/batch; 32h:37m:34s remains)
INFO - root - 2017-12-11 02:32:25.520064: step 78180, loss = 0.68, batch loss = 0.62 (18.0 examples/sec; 0.445 sec/batch; 31h:25m:53s remains)
INFO - root - 2017-12-11 02:32:30.004291: step 78190, loss = 0.70, batch loss = 0.65 (17.7 examples/sec; 0.452 sec/batch; 31h:56m:47s remains)
INFO - root - 2017-12-11 02:32:34.584601: step 78200, loss = 0.69, batch loss = 0.64 (16.5 examples/sec; 0.484 sec/batch; 34h:09m:48s remains)
2017-12-11 02:32:35.159894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053865489 -0.048602156 -0.04310042 -0.041126553 -0.041531205 -0.042099267 -0.03941391 -0.034900133 -0.032865148 -0.034957044 -0.041673418 -0.052008241 -0.063930884 -0.074130453 -0.080450475][-0.026458239 -0.010019131 0.0069226595 0.017578261 0.023281274 0.028032403 0.037023425 0.045060482 0.045932658 0.038372181 0.022332653 -0.00065187126 -0.028022597 -0.053163733 -0.070798449][0.014729836 0.048114069 0.0816926 0.10590864 0.12174377 0.13485138 0.15256061 0.16371064 0.16095671 0.14339654 0.11272264 0.071390666 0.022131154 -0.023139669 -0.055200692][0.062078945 0.1169992 0.17068626 0.21061426 0.23811327 0.2615729 0.2905308 0.30614084 0.29859534 0.26815215 0.21903731 0.15457992 0.078099139 0.0082451021 -0.040549275][0.11076827 0.18936284 0.2651383 0.32171252 0.3610456 0.39525929 0.43499905 0.45462021 0.44023967 0.39451039 0.32491982 0.23529623 0.13029142 0.035207149 -0.029930346][0.14075886 0.23830952 0.33395308 0.40727457 0.45970863 0.50578439 0.55501258 0.57715446 0.55458277 0.49266255 0.40278181 0.29002371 0.16096997 0.045975711 -0.030888192][0.14686817 0.25420266 0.3625212 0.44800594 0.511043 0.56655777 0.62111723 0.64300185 0.61333489 0.54014993 0.43732283 0.31041139 0.16795085 0.043073457 -0.03863892][0.14200602 0.25026032 0.36079445 0.44803062 0.51147979 0.56678015 0.61748636 0.63434875 0.59929276 0.52247632 0.41834745 0.29110038 0.15019423 0.027817674 -0.050944872][0.12715049 0.22758687 0.33014667 0.40840653 0.46173885 0.5067699 0.54493535 0.55277056 0.51437724 0.44105786 0.34596875 0.23171322 0.10779231 0.0011102372 -0.066336438][0.093478858 0.17717892 0.26262411 0.32445076 0.36205703 0.39172396 0.41314268 0.40954772 0.36913434 0.30360457 0.22453828 0.13359031 0.039241329 -0.039678369 -0.087141968][0.038148914 0.0970357 0.1579226 0.19925244 0.22041535 0.23490486 0.24125424 0.22853926 0.19027939 0.13770351 0.080522418 0.019544134 -0.038860783 -0.084221974 -0.10771281][-0.014188998 0.019780898 0.055435937 0.076667562 0.08293888 0.0840425 0.078441493 0.060350787 0.028199116 -0.0082237953 -0.042227533 -0.0742456 -0.1004485 -0.11673104 -0.12014814][-0.051891766 -0.037723694 -0.022671703 -0.017376399 -0.022217512 -0.03075443 -0.04352488 -0.0626783 -0.087025113 -0.10900484 -0.1249064 -0.13581602 -0.13986969 -0.13665421 -0.1268286][-0.083832204 -0.084503219 -0.083742276 -0.088096514 -0.0976226 -0.10873819 -0.12163728 -0.13695796 -0.15263736 -0.16335849 -0.16729237 -0.16537562 -0.15704398 -0.14352627 -0.12709165][-0.10409753 -0.11341935 -0.12008734 -0.12783943 -0.13635984 -0.14422092 -0.15197533 -0.1601425 -0.16711222 -0.16975497 -0.16714558 -0.15987474 -0.14802846 -0.13301736 -0.11708926]]...]
INFO - root - 2017-12-11 02:32:39.685079: step 78210, loss = 0.71, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 32h:04m:18s remains)
INFO - root - 2017-12-11 02:32:44.323740: step 78220, loss = 0.68, batch loss = 0.62 (17.3 examples/sec; 0.462 sec/batch; 32h:37m:22s remains)
INFO - root - 2017-12-11 02:32:48.870499: step 78230, loss = 0.69, batch loss = 0.63 (18.2 examples/sec; 0.439 sec/batch; 31h:00m:46s remains)
INFO - root - 2017-12-11 02:32:53.440658: step 78240, loss = 0.69, batch loss = 0.63 (17.7 examples/sec; 0.453 sec/batch; 31h:58m:31s remains)
INFO - root - 2017-12-11 02:32:57.986380: step 78250, loss = 0.70, batch loss = 0.64 (17.8 examples/sec; 0.450 sec/batch; 31h:47m:10s remains)
INFO - root - 2017-12-11 02:33:02.568179: step 78260, loss = 0.70, batch loss = 0.64 (17.8 examples/sec; 0.449 sec/batch; 31h:44m:04s remains)
INFO - root - 2017-12-11 02:33:07.133463: step 78270, loss = 0.68, batch loss = 0.62 (17.5 examples/sec; 0.458 sec/batch; 32h:20m:44s remains)
INFO - root - 2017-12-11 02:33:11.717897: step 78280, loss = 0.72, batch loss = 0.67 (17.2 examples/sec; 0.465 sec/batch; 32h:51m:36s remains)
INFO - root - 2017-12-11 02:33:16.336474: step 78290, loss = 0.71, batch loss = 0.65 (16.8 examples/sec; 0.476 sec/batch; 33h:35m:25s remains)
INFO - root - 2017-12-11 02:33:20.907964: step 78300, loss = 0.70, batch loss = 0.64 (17.6 examples/sec; 0.456 sec/batch; 32h:11m:07s remains)
2017-12-11 02:33:21.499555: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.11865477 -0.11428006 -0.10413685 -0.095260881 -0.085891008 -0.07167574 -0.054028589 -0.039981544 -0.028645551 -0.0034625551 0.047402695 0.1263629 0.21874498 0.29632071 0.32865843][-0.12593861 -0.10976797 -0.087367073 -0.067488357 -0.049556021 -0.031410709 -0.015628967 -0.0089487974 -0.010134959 -0.0017952973 0.029069135 0.086342379 0.15830079 0.22092256 0.24779958][-0.11671042 -0.08957281 -0.054043096 -0.019583495 0.014586533 0.04897964 0.076852575 0.087902524 0.0795351 0.065619111 0.058633983 0.06833984 0.094784737 0.12600148 0.14244495][-0.096395932 -0.059345257 -0.0091710472 0.042942833 0.098860927 0.15800028 0.20842667 0.23361178 0.22440363 0.18857224 0.13676804 0.087356061 0.0584537 0.054864068 0.06358888][-0.068463318 -0.01763818 0.052621365 0.12721264 0.20771345 0.29326186 0.36724558 0.40710035 0.39762795 0.34159124 0.24878497 0.14670746 0.070547841 0.041163806 0.049318738][-0.035175525 0.031451121 0.12416535 0.22345094 0.3294023 0.44072232 0.53623706 0.58754939 0.57467532 0.49942562 0.3740131 0.23580185 0.13449675 0.099955194 0.12178008][-0.00021932984 0.075951084 0.18352918 0.30109876 0.42745233 0.56006336 0.6726498 0.73057193 0.7103312 0.61706334 0.46953347 0.31609491 0.21665059 0.20353344 0.25779635][0.037994996 0.11454111 0.22495738 0.34977734 0.48692879 0.63184178 0.75407147 0.81429142 0.78717119 0.6820153 0.52439934 0.37271839 0.29229766 0.313214 0.40447298][0.086915076 0.15588666 0.25658041 0.37369558 0.50438964 0.64289987 0.75828838 0.81189239 0.77982825 0.67421389 0.52344477 0.39063862 0.33978695 0.39450154 0.51371354][0.15012111 0.20348893 0.28086963 0.37318295 0.47794178 0.58932906 0.67976093 0.71667111 0.68066269 0.58450115 0.45548007 0.3544471 0.3379038 0.41985708 0.55279034][0.2243773 0.25370622 0.294748 0.347609 0.41271275 0.48499802 0.54107004 0.55573255 0.51468825 0.43029243 0.32717329 0.25892642 0.27087829 0.36743519 0.4992452][0.3050696 0.30725074 0.30574563 0.31044966 0.3274366 0.35436288 0.37319064 0.36567873 0.32125732 0.2498289 0.17169897 0.12927146 0.15499899 0.24690072 0.36061338][0.36894673 0.35382047 0.31862703 0.28054261 0.25085834 0.23234278 0.21500033 0.18841965 0.14515333 0.090251729 0.036278654 0.011392949 0.035577323 0.10443825 0.18512087][0.38179755 0.361183 0.31077644 0.24887376 0.18813667 0.13523707 0.089474432 0.049507044 0.01094394 -0.026470758 -0.059146188 -0.072973363 -0.057971161 -0.018235147 0.027073618][0.33801228 0.32194299 0.2748028 0.21153288 0.14290385 0.076230906 0.017438741 -0.027496492 -0.060236871 -0.0851986 -0.10488 -0.11468402 -0.11062191 -0.095298193 -0.0768417]]...]
INFO - root - 2017-12-11 02:33:26.081004: step 78310, loss = 0.69, batch loss = 0.63 (18.3 examples/sec; 0.437 sec/batch; 30h:50m:49s remains)
INFO - root - 2017-12-11 02:33:30.607992: step 78320, loss = 0.69, batch loss = 0.64 (17.5 examples/sec; 0.458 sec/batch; 32h:20m:59s remains)
INFO - root - 2017-12-11 02:33:35.236004: step 78330, loss = 0.70, batch loss = 0.64 (17.8 examples/sec; 0.449 sec/batch; 31h:42m:23s remains)
INFO - root - 2017-12-11 02:33:39.762194: step 78340, loss = 0.70, batch loss = 0.64 (17.6 examples/sec; 0.454 sec/batch; 32h:04m:47s remains)
INFO - root - 2017-12-11 02:33:44.344223: step 78350, loss = 0.70, batch loss = 0.64 (17.8 examples/sec; 0.451 sec/batch; 31h:48m:14s remains)
INFO - root - 2017-12-11 02:33:48.827494: step 78360, loss = 0.69, batch loss = 0.63 (17.9 examples/sec; 0.446 sec/batch; 31h:30m:52s remains)
INFO - root - 2017-12-11 02:33:53.421511: step 78370, loss = 0.71, batch loss = 0.66 (17.3 examples/sec; 0.461 sec/batch; 32h:33m:28s remains)
INFO - root - 2017-12-11 02:33:57.959829: step 78380, loss = 0.69, batch loss = 0.63 (18.1 examples/sec; 0.442 sec/batch; 31h:12m:40s remains)
INFO - root - 2017-12-11 02:34:02.504809: step 78390, loss = 0.69, batch loss = 0.64 (18.0 examples/sec; 0.445 sec/batch; 31h:25m:24s remains)
INFO - root - 2017-12-11 02:34:06.998804: step 78400, loss = 0.69, batch loss = 0.63 (17.1 examples/sec; 0.468 sec/batch; 33h:00m:56s remains)
2017-12-11 02:34:07.565975: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20202583 0.33193782 0.46770635 0.5996055 0.69257361 0.72380787 0.68583465 0.59450364 0.46902925 0.3283309 0.20476685 0.12751603 0.10003858 0.11191398 0.14982863][0.26872289 0.40805602 0.54857719 0.68426728 0.78032023 0.81145561 0.76813686 0.66770303 0.53120375 0.37573561 0.2346034 0.14031632 0.099559516 0.10186243 0.13342516][0.31456244 0.44615889 0.57553744 0.70160294 0.79196984 0.82095075 0.77763289 0.679293 0.54671693 0.39420411 0.25213957 0.15350093 0.10581052 0.099833116 0.12285932][0.34377137 0.45732656 0.56441605 0.66968304 0.74633855 0.77061272 0.73211831 0.64509255 0.52679813 0.387595 0.25343072 0.15513282 0.10194124 0.087831825 0.10282668][0.36077327 0.44403583 0.519277 0.59712166 0.657944 0.68082029 0.65565968 0.58989418 0.49409065 0.37514204 0.25422722 0.15899192 0.10056042 0.077665381 0.084012255][0.35662013 0.40380576 0.4440107 0.4937363 0.54037374 0.56661135 0.56142777 0.523461 0.45556405 0.36160505 0.2581687 0.16912028 0.10760617 0.077241234 0.075574957][0.32599989 0.34180516 0.35491329 0.3852222 0.42493108 0.45947871 0.47540542 0.46378279 0.42144075 0.35115027 0.26588666 0.18619047 0.12638804 0.094143987 0.089124121][0.27346474 0.26994997 0.26878902 0.28890789 0.32531753 0.36568996 0.39540386 0.40158591 0.37801138 0.32725233 0.26020485 0.19464713 0.1438835 0.11737768 0.11555649][0.2091663 0.19593479 0.18863051 0.20241173 0.23423821 0.27397859 0.30807176 0.3226451 0.31057397 0.27475727 0.22475676 0.17504576 0.13751256 0.12117744 0.12587552][0.13667725 0.12044197 0.11120914 0.11945611 0.14406101 0.17780942 0.20936747 0.22585694 0.22049206 0.19629097 0.16096547 0.12580831 0.10047304 0.092594072 0.1015883][0.051793054 0.037081178 0.028245363 0.030993402 0.046367716 0.070544288 0.094976738 0.10902718 0.10721005 0.0921186 0.069261141 0.046361234 0.030905144 0.028975995 0.040130887][-0.031975791 -0.042787243 -0.049293779 -0.050036702 -0.042660106 -0.028580228 -0.013302555 -0.0042760372 -0.0050305189 -0.013318102 -0.026007578 -0.038636561 -0.046146818 -0.04379493 -0.031871352][-0.087036408 -0.09443292 -0.098643743 -0.10135976 -0.10006865 -0.094916858 -0.08861737 -0.08519385 -0.086293079 -0.0902576 -0.095671363 -0.10049913 -0.10183133 -0.096630022 -0.085355215][-0.11228713 -0.1177385 -0.12060235 -0.12397563 -0.12605685 -0.12660673 -0.12661339 -0.12763248 -0.12990895 -0.13216493 -0.13383982 -0.13435256 -0.13246398 -0.12675597 -0.11793049][-0.12025144 -0.12473261 -0.12660471 -0.12945546 -0.13236053 -0.13504198 -0.13778444 -0.14092223 -0.14407253 -0.14620633 -0.147008 -0.1463993 -0.14396699 -0.13932605 -0.13324931]]...]
INFO - root - 2017-12-11 02:34:12.154661: step 78410, loss = 0.71, batch loss = 0.65 (17.5 examples/sec; 0.457 sec/batch; 32h:16m:48s remains)
INFO - root - 2017-12-11 02:34:16.649145: step 78420, loss = 0.71, batch loss = 0.65 (17.7 examples/sec; 0.451 sec/batch; 31h:48m:38s remains)
INFO - root - 2017-12-11 02:34:21.172804: step 78430, loss = 0.71, batch loss = 0.65 (17.3 examples/sec; 0.461 sec/batch; 32h:33m:29s remains)
INFO - root - 2017-12-11 02:34:25.714033: step 78440, loss = 0.70, batch loss = 0.65 (17.1 examples/sec; 0.467 sec/batch; 32h:59m:19s remains)
INFO - root - 2017-12-11 02:34:30.255108: step 78450, loss = 0.69, batch loss = 0.63 (17.7 examples/sec; 0.452 sec/batch; 31h:54m:42s remains)
INFO - root - 2017-12-11 02:34:34.789287: step 78460, loss = 0.70, batch loss = 0.65 (18.4 examples/sec; 0.434 sec/batch; 30h:37m:12s remains)
INFO - root - 2017-12-11 02:34:39.319059: step 78470, loss = 0.69, batch loss = 0.63 (17.8 examples/sec; 0.450 sec/batch; 31h:44m:05s remains)
INFO - root - 2017-12-11 02:34:43.853883: step 78480, loss = 0.69, batch loss = 0.63 (17.6 examples/sec; 0.454 sec/batch; 32h:02m:46s remains)
INFO - root - 2017-12-11 02:34:48.370677: step 78490, loss = 0.70, batch loss = 0.65 (17.7 examples/sec; 0.451 sec/batch; 31h:49m:26s remains)
INFO - root - 2017-12-11 02:34:52.911538: step 78500, loss = 0.70, batch loss = 0.64 (17.6 examples/sec; 0.454 sec/batch; 32h:01m:12s remains)
2017-12-11 02:34:53.442531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.027032753 -0.012386711 0.0036090442 0.017258676 0.027244082 0.036361925 0.040412113 0.038698129 0.033567376 0.030274153 0.029679647 0.034166161 0.0385254 0.040704753 0.038384274][0.015493718 0.050030895 0.08578597 0.1156396 0.13763893 0.15735996 0.16838276 0.1683259 0.1622474 0.16091961 0.16695057 0.18181691 0.19073404 0.18859239 0.17560463][0.072626114 0.13637362 0.20097765 0.25299519 0.28904727 0.31894454 0.33423945 0.33250251 0.32423252 0.32708621 0.34419397 0.37329194 0.38796949 0.3782993 0.34663287][0.13810714 0.236182 0.33228493 0.40577102 0.45291528 0.4871124 0.49800709 0.4872649 0.4738265 0.4804233 0.50764483 0.54735512 0.56499976 0.54565322 0.49318874][0.20036882 0.32648608 0.44464949 0.52942485 0.5803138 0.61191583 0.612948 0.59104246 0.57335377 0.58325237 0.61400759 0.65417212 0.66785806 0.63811862 0.56976807][0.23893225 0.37994185 0.50777149 0.59518397 0.646578 0.67589116 0.6717453 0.6449672 0.62618971 0.63528335 0.65913355 0.68659633 0.68679106 0.64445925 0.56588644][0.24980745 0.39508051 0.52596045 0.6145097 0.66974562 0.7042073 0.704919 0.68029839 0.66004419 0.66105372 0.6683895 0.67246526 0.65114957 0.5936324 0.50840133][0.24070145 0.38471168 0.51575595 0.6054967 0.66693461 0.7103554 0.7199859 0.69932508 0.67595285 0.66609818 0.65549415 0.63521343 0.59162778 0.51966089 0.43054441][0.21108596 0.34700009 0.4728263 0.56083268 0.62612981 0.67620683 0.69343543 0.67675543 0.65138817 0.63411212 0.61208123 0.57627738 0.51891929 0.4398275 0.35295889][0.16083992 0.27804494 0.38829321 0.46644789 0.52787811 0.57778466 0.59858644 0.5860585 0.56187111 0.54307604 0.51883388 0.47987065 0.42076388 0.34452534 0.26674515][0.090326987 0.17942761 0.26489606 0.32593274 0.37599465 0.41907972 0.43870017 0.42900464 0.40749848 0.39128256 0.37204474 0.33963227 0.28884059 0.22452261 0.16221943][0.016897924 0.075332761 0.1335109 0.17569532 0.21220568 0.2452428 0.26006192 0.25100106 0.23234355 0.22054054 0.20900548 0.18728305 0.15059192 0.1047739 0.062825941][-0.039968241 -0.00790609 0.026357913 0.051336627 0.074048378 0.09491349 0.10209545 0.091621742 0.075369827 0.067724064 0.063480593 0.053102434 0.032660928 0.0071810763 -0.014389829][-0.076949529 -0.064959161 -0.04932173 -0.038384639 -0.028208829 -0.019222504 -0.019240022 -0.02998987 -0.042709496 -0.047076955 -0.046839703 -0.049060117 -0.056336913 -0.065567113 -0.071640216][-0.095941931 -0.097396396 -0.094694585 -0.094044864 -0.093537509 -0.093667381 -0.098475516 -0.10806882 -0.11704355 -0.11951156 -0.11796416 -0.11648876 -0.11630525 -0.11598465 -0.11339401]]...]
INFO - root - 2017-12-11 02:34:57.990534: step 78510, loss = 0.70, batch loss = 0.64 (17.6 examples/sec; 0.453 sec/batch; 31h:58m:49s remains)
INFO - root - 2017-12-11 02:35:02.384138: step 78520, loss = 0.71, batch loss = 0.65 (17.7 examples/sec; 0.452 sec/batch; 31h:51m:36s remains)
INFO - root - 2017-12-11 02:35:06.868484: step 78530, loss = 0.70, batch loss = 0.64 (17.6 examples/sec; 0.455 sec/batch; 32h:08m:02s remains)
INFO - root - 2017-12-11 02:35:11.415453: step 78540, loss = 0.69, batch loss = 0.63 (17.7 examples/sec; 0.452 sec/batch; 31h:54m:19s remains)
INFO - root - 2017-12-11 02:35:15.969774: step 78550, loss = 0.70, batch loss = 0.64 (18.0 examples/sec; 0.443 sec/batch; 31h:16m:50s remains)
INFO - root - 2017-12-11 02:35:20.566426: step 78560, loss = 0.70, batch loss = 0.64 (17.1 examples/sec; 0.467 sec/batch; 32h:56m:58s remains)
INFO - root - 2017-12-11 02:35:25.149016: step 78570, loss = 0.70, batch loss = 0.64 (16.8 examples/sec; 0.477 sec/batch; 33h:38m:44s remains)
INFO - root - 2017-12-11 02:35:29.648464: step 78580, loss = 0.69, batch loss = 0.63 (17.7 examples/sec; 0.451 sec/batch; 31h:50m:22s remains)
INFO - root - 2017-12-11 02:35:34.143103: step 78590, loss = 0.68, batch loss = 0.62 (18.3 examples/sec; 0.436 sec/batch; 30h:46m:36s remains)
INFO - root - 2017-12-11 02:35:38.607085: step 78600, loss = 0.72, batch loss = 0.66 (17.4 examples/sec; 0.459 sec/batch; 32h:21m:26s remains)
2017-12-11 02:35:39.202741: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16080138 0.17785119 0.18636525 0.18537556 0.17862625 0.16963904 0.15021034 0.11538228 0.069629684 0.027242843 -0.00019401933 -0.0068251421 0.011817315 0.044808324 0.077780031][0.19296561 0.21944802 0.23938395 0.25159052 0.25965562 0.26384443 0.24951622 0.21105976 0.15608782 0.10117932 0.060092825 0.043211989 0.058579538 0.093533069 0.13115147][0.19878392 0.23245743 0.26159418 0.2855126 0.30652741 0.32060295 0.30920455 0.26981419 0.21470802 0.15997085 0.11736341 0.098981217 0.1139442 0.14804764 0.18420562][0.20935239 0.24993488 0.28665107 0.31934902 0.34667674 0.35993391 0.34224626 0.30005214 0.25246164 0.21212681 0.18406524 0.17659919 0.19402407 0.22089466 0.24423842][0.26004726 0.31338581 0.36265859 0.40596583 0.43476745 0.43541372 0.39942336 0.34814507 0.30937555 0.29059535 0.28664675 0.29632095 0.31360167 0.32247785 0.31822863][0.35006571 0.42399344 0.49436802 0.55495948 0.58734828 0.57201797 0.51233351 0.4467226 0.41192302 0.41033751 0.42657056 0.44814664 0.45676842 0.4364222 0.39228523][0.43031743 0.52731383 0.62236166 0.70443285 0.74608284 0.72168046 0.6441288 0.56530315 0.52929562 0.53413981 0.55630642 0.57575309 0.566401 0.5130136 0.42810023][0.43244052 0.5435487 0.65625626 0.75654042 0.81279624 0.79405767 0.71521676 0.63267523 0.59388423 0.59382397 0.60515332 0.60745531 0.57598948 0.49718663 0.38516432][0.32045153 0.42634857 0.53970927 0.64590067 0.71571964 0.71575135 0.65513593 0.58401662 0.54635954 0.53557324 0.52612233 0.50469118 0.45607546 0.36921778 0.25458497][0.13309108 0.21590038 0.31349054 0.41122514 0.48668829 0.50849277 0.47658297 0.426848 0.39406458 0.37263212 0.34337777 0.30302912 0.24848774 0.17307849 0.081016965][-0.03251313 0.020936189 0.094808862 0.17432727 0.24594736 0.28443894 0.28046462 0.252288 0.2237922 0.19364972 0.15201354 0.10403387 0.055849012 0.0058564534 -0.04875166][-0.08976046 -0.060795031 -0.010467265 0.047908377 0.10947811 0.15602154 0.17033418 0.15494007 0.12645957 0.089963883 0.044042107 -0.00096098287 -0.034462441 -0.054037381 -0.069705494][-0.023437746 -0.010416185 0.019356806 0.058934949 0.11090409 0.16030976 0.18289568 0.17064962 0.13890897 0.098583475 0.053076968 0.015696626 -0.0018191081 0.0049690213 0.018655013][0.11073681 0.11371852 0.12454507 0.14936009 0.19627596 0.24811189 0.27365488 0.26005197 0.2253111 0.18374597 0.14011386 0.10953151 0.10330083 0.1257627 0.15358244][0.24012156 0.23293869 0.22467178 0.23755731 0.28235871 0.33663616 0.36362994 0.35005856 0.31549823 0.27541089 0.23473552 0.20918693 0.20793021 0.23343189 0.2610108]]...]
INFO - root - 2017-12-11 02:35:43.732756: step 78610, loss = 0.70, batch loss = 0.64 (18.0 examples/sec; 0.445 sec/batch; 31h:23m:55s remains)
INFO - root - 2017-12-11 02:35:48.263989: step 78620, loss = 0.69, batch loss = 0.63 (18.1 examples/sec; 0.443 sec/batch; 31h:12m:56s remains)
INFO - root - 2017-12-11 02:35:52.740571: step 78630, loss = 0.71, batch loss = 0.66 (16.8 examples/sec; 0.475 sec/batch; 33h:30m:04s remains)
INFO - root - 2017-12-11 02:35:57.202500: step 78640, loss = 0.70, batch loss = 0.64 (17.6 examples/sec; 0.454 sec/batch; 31h:59m:40s remains)
INFO - root - 2017-12-11 02:36:01.765395: step 78650, loss = 0.69, batch loss = 0.63 (17.3 examples/sec; 0.463 sec/batch; 32h:36m:58s remains)
INFO - root - 2017-12-11 02:36:06.284746: step 78660, loss = 0.68, batch loss = 0.62 (18.1 examples/sec; 0.442 sec/batch; 31h:08m:22s remains)
INFO - root - 2017-12-11 02:36:10.799486: step 78670, loss = 0.68, batch loss = 0.62 (17.3 examples/sec; 0.463 sec/batch; 32h:39m:55s remains)
INFO - root - 2017-12-11 02:36:15.344878: step 78680, loss = 0.70, batch loss = 0.65 (17.5 examples/sec; 0.457 sec/batch; 32h:12m:26s remains)
INFO - root - 2017-12-11 02:36:19.880278: step 78690, loss = 0.69, batch loss = 0.64 (17.6 examples/sec; 0.453 sec/batch; 31h:57m:56s remains)
INFO - root - 2017-12-11 02:36:24.384536: step 78700, loss = 0.70, batch loss = 0.64 (17.4 examples/sec; 0.460 sec/batch; 32h:25m:50s remains)
2017-12-11 02:36:24.901321: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22853073 0.25663307 0.29171667 0.33832106 0.38081798 0.40610585 0.39575958 0.34789777 0.28179592 0.21350011 0.15215273 0.10486843 0.075938843 0.057346109 0.036285028][0.29396358 0.33666617 0.38944826 0.4566288 0.51926905 0.55980235 0.55408204 0.49839565 0.41466093 0.32257411 0.2347288 0.16358781 0.11689647 0.087597363 0.060617182][0.32922396 0.38537878 0.45413509 0.53868234 0.617776 0.67045712 0.66996562 0.61160463 0.51805019 0.41127896 0.30545446 0.216334 0.15408017 0.11361672 0.080423966][0.34749973 0.41086304 0.48938763 0.58552212 0.67711687 0.7411474 0.74962908 0.69667488 0.60350215 0.49116853 0.37476462 0.270814 0.19126216 0.13522774 0.0919532][0.39500597 0.45107645 0.52745533 0.62806445 0.73027933 0.80966741 0.83514261 0.79709911 0.71373469 0.60232133 0.47748387 0.35391 0.24668495 0.16328721 0.10064884][0.48933291 0.52621418 0.58977592 0.68850082 0.79920685 0.89476681 0.93938458 0.91763294 0.84546161 0.73594987 0.60173696 0.45439461 0.31245294 0.19474055 0.10765107][0.61656696 0.63066411 0.67427742 0.76236582 0.87281609 0.97640127 1.0324292 1.0199354 0.95411646 0.84560615 0.70345813 0.53584683 0.36387619 0.21633273 0.10803059][0.74855882 0.74079376 0.76058143 0.827506 0.92326015 1.0194224 1.0733463 1.0600452 0.99564242 0.88865191 0.74300122 0.5637809 0.37433916 0.21118709 0.092937306][0.81465232 0.79453862 0.79369479 0.83518523 0.90522039 0.97906828 1.0183352 0.99789935 0.93430191 0.83358675 0.69376236 0.51750278 0.32950473 0.16972107 0.056291383][0.76867288 0.74536806 0.73115933 0.74814051 0.78691065 0.82881325 0.84519535 0.81442469 0.75267035 0.66455555 0.54344076 0.39037332 0.22828898 0.094245918 0.0021335145][0.59561259 0.57900739 0.56210816 0.56112272 0.57089061 0.58038139 0.57299179 0.53286761 0.47485903 0.4039003 0.31217015 0.20020075 0.0855011 -0.0038334811 -0.060682848][0.34207731 0.33467707 0.32067987 0.30826834 0.29657844 0.28223711 0.25952595 0.2189239 0.17255707 0.12531011 0.070085607 0.0067802127 -0.053789955 -0.094608039 -0.11461088][0.097557917 0.095589921 0.087075263 0.071829647 0.050969925 0.026390051 -0.00021217538 -0.033282738 -0.063891977 -0.087904751 -0.11054432 -0.1326414 -0.14932646 -0.15293315 -0.14605075][-0.080016926 -0.080827944 -0.084054768 -0.09495122 -0.11279447 -0.13426636 -0.15487325 -0.17613381 -0.19240525 -0.20021318 -0.20223632 -0.19930482 -0.19026098 -0.17368436 -0.15346558][-0.17011552 -0.17370212 -0.17414258 -0.179584 -0.1899589 -0.2025912 -0.21377042 -0.22378239 -0.22954381 -0.22836472 -0.22089227 -0.2076638 -0.18920176 -0.16685797 -0.14424188]]...]
INFO - root - 2017-12-11 02:36:29.436208: step 78710, loss = 0.68, batch loss = 0.62 (17.8 examples/sec; 0.448 sec/batch; 31h:36m:52s remains)
INFO - root - 2017-12-11 02:36:33.987392: step 78720, loss = 0.71, batch loss = 0.65 (17.5 examples/sec; 0.456 sec/batch; 32h:09m:07s remains)
INFO - root - 2017-12-11 02:36:38.436365: step 78730, loss = 0.69, batch loss = 0.63 (17.7 examples/sec; 0.452 sec/batch; 31h:51m:15s remains)
INFO - root - 2017-12-11 02:36:43.024899: step 78740, loss = 0.70, batch loss = 0.64 (17.5 examples/sec; 0.457 sec/batch; 32h:11m:43s remains)
INFO - root - 2017-12-11 02:36:47.511661: step 78750, loss = 0.69, batch loss = 0.63 (17.9 examples/sec; 0.447 sec/batch; 31h:28m:29s remains)
INFO - root - 2017-12-11 02:36:51.915321: step 78760, loss = 0.71, batch loss = 0.65 (17.3 examples/sec; 0.464 sec/batch; 32h:40m:22s remains)
INFO - root - 2017-12-11 02:36:56.464177: step 78770, loss = 0.70, batch loss = 0.64 (17.8 examples/sec; 0.450 sec/batch; 31h:41m:22s remains)
INFO - root - 2017-12-11 02:37:00.969791: step 78780, loss = 0.69, batch loss = 0.64 (17.1 examples/sec; 0.468 sec/batch; 32h:58m:18s remains)
INFO - root - 2017-12-11 02:37:05.532646: step 78790, loss = 0.69, batch loss = 0.63 (17.7 examples/sec; 0.453 sec/batch; 31h:55m:06s remains)
INFO - root - 2017-12-11 02:37:09.976962: step 78800, loss = 0.68, batch loss = 0.63 (17.2 examples/sec; 0.465 sec/batch; 32h:44m:18s remains)
2017-12-11 02:37:10.517480: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29529357 0.28764197 0.26903072 0.24795336 0.23340091 0.23386009 0.24194562 0.24535754 0.24284004 0.24015202 0.24616954 0.26172933 0.28499982 0.3072533 0.31868431][0.35973087 0.348274 0.32066989 0.29002589 0.26835805 0.26471469 0.27003834 0.27004957 0.26234144 0.25402892 0.25662073 0.27032062 0.29055479 0.30829316 0.31530696][0.39434952 0.37977913 0.34428546 0.30560458 0.2784951 0.27218947 0.27576527 0.27311024 0.26098561 0.24684225 0.2433109 0.25072351 0.26317462 0.27280122 0.2743136][0.39151648 0.37404707 0.33353239 0.2907683 0.26226294 0.25662592 0.26166376 0.26045683 0.24869028 0.23231511 0.22323444 0.22230189 0.22425506 0.22358546 0.21919797][0.35272127 0.33540937 0.29654467 0.2574155 0.23435052 0.2347527 0.24626967 0.251895 0.24507584 0.22870085 0.21277116 0.19959319 0.18708493 0.17401771 0.16462922][0.29545003 0.28244984 0.25211486 0.22419843 0.21219303 0.2218738 0.24229072 0.25765339 0.25896546 0.24521558 0.22331448 0.1972761 0.16966812 0.14439379 0.13075738][0.24951832 0.24264176 0.22402123 0.21050166 0.21003361 0.22662339 0.25312188 0.27645552 0.28570622 0.275714 0.25093111 0.21721976 0.18080781 0.14803912 0.13112073][0.22628601 0.22515643 0.21712469 0.21691047 0.22510044 0.24348696 0.26996547 0.29620832 0.31012264 0.30269983 0.27738941 0.24288958 0.20753291 0.17592274 0.1601208][0.23817964 0.24237584 0.24236909 0.25231132 0.26662636 0.28392482 0.30589515 0.32992995 0.34398615 0.33637369 0.31055021 0.27880988 0.25045669 0.22538714 0.21302524][0.28017446 0.28947434 0.29470217 0.31113148 0.33045569 0.347133 0.36389002 0.38288581 0.39396188 0.38452923 0.35780227 0.32900602 0.30755886 0.28910798 0.27922651][0.32496205 0.3373791 0.34348434 0.36113977 0.38242117 0.39830679 0.41065294 0.42490056 0.43398663 0.42521861 0.40066022 0.37720463 0.36317191 0.35073629 0.34098691][0.34644756 0.35780111 0.36167124 0.377006 0.3979474 0.41365963 0.42386389 0.43559554 0.44459513 0.43843007 0.41812894 0.40137708 0.39463869 0.3872081 0.37571317][0.31607258 0.32456169 0.32584354 0.33855826 0.35973382 0.37791249 0.38971433 0.40178272 0.41223964 0.40929347 0.39323428 0.38117298 0.37833858 0.37290332 0.35851303][0.2317441 0.2364535 0.23727427 0.24963194 0.27312696 0.29671556 0.31426138 0.33017528 0.34366098 0.34413108 0.33142117 0.32083029 0.31699374 0.30907726 0.28983173][0.13117139 0.13056143 0.13053639 0.14258619 0.16796626 0.19707039 0.22191766 0.24350272 0.2601828 0.26290664 0.25143149 0.2391053 0.23049903 0.21743076 0.19338527]]...]
INFO - root - 2017-12-11 02:37:14.907553: step 78810, loss = 0.69, batch loss = 0.63 (18.0 examples/sec; 0.444 sec/batch; 31h:16m:08s remains)
INFO - root - 2017-12-11 02:37:19.439584: step 78820, loss = 0.68, batch loss = 0.62 (17.5 examples/sec; 0.457 sec/batch; 32h:10m:17s remains)
INFO - root - 2017-12-11 02:37:23.939199: step 78830, loss = 0.70, batch loss = 0.64 (18.4 examples/sec; 0.434 sec/batch; 30h:33m:53s remains)
INFO - root - 2017-12-11 02:37:28.433488: step 78840, loss = 0.70, batch loss = 0.64 (17.7 examples/sec; 0.451 sec/batch; 31h:47m:55s remains)
INFO - root - 2017-12-11 02:37:32.951410: step 78850, loss = 0.69, batch loss = 0.63 (18.4 examples/sec; 0.436 sec/batch; 30h:42m:07s remains)
INFO - root - 2017-12-11 02:37:37.363302: step 78860, loss = 0.69, batch loss = 0.63 (17.5 examples/sec; 0.458 sec/batch; 32h:16m:26s remains)
INFO - root - 2017-12-11 02:37:41.859037: step 78870, loss = 0.69, batch loss = 0.63 (17.9 examples/sec; 0.446 sec/batch; 31h:26m:03s remains)
INFO - root - 2017-12-11 02:37:46.341849: step 78880, loss = 0.68, batch loss = 0.62 (17.8 examples/sec; 0.449 sec/batch; 31h:35m:59s remains)
INFO - root - 2017-12-11 02:37:50.853753: step 78890, loss = 0.69, batch loss = 0.63 (17.8 examples/sec; 0.449 sec/batch; 31h:36m:57s remains)
INFO - root - 2017-12-11 02:37:55.394525: step 78900, loss = 0.70, batch loss = 0.64 (17.9 examples/sec; 0.447 sec/batch; 31h:30m:18s remains)
2017-12-11 02:37:56.007966: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20880502 0.21203132 0.21089053 0.21017691 0.20536482 0.20512584 0.20688455 0.20710903 0.20749845 0.20327055 0.20220728 0.2029247 0.20143257 0.20336105 0.20286058][0.20430312 0.2065869 0.20402676 0.20145293 0.1948189 0.19212236 0.19201188 0.19206703 0.19473806 0.19415945 0.19514365 0.19589695 0.19350156 0.19418521 0.19370358][0.2008293 0.20132826 0.19607118 0.19046888 0.18156342 0.17599458 0.17393735 0.17452015 0.1799705 0.18331721 0.1852766 0.18436067 0.17907475 0.17581649 0.1730286][0.20563047 0.20471868 0.19725309 0.1895775 0.17912318 0.17124216 0.16791992 0.17039923 0.17984456 0.18739253 0.18964878 0.18497996 0.17378499 0.16337419 0.15573679][0.20800288 0.20827314 0.20243602 0.19667897 0.18777861 0.17965879 0.17596962 0.17995688 0.19196843 0.2013229 0.20143068 0.19092835 0.17270389 0.15531102 0.14331834][0.19485228 0.19915751 0.19859684 0.19859162 0.19471964 0.18899564 0.18533437 0.18870439 0.19979247 0.20754285 0.20382927 0.18782477 0.16451524 0.14296518 0.12888841][0.17939353 0.18970549 0.19544446 0.2019911 0.20351878 0.19994478 0.19494584 0.1948653 0.20183027 0.20570196 0.198184 0.17853574 0.15271348 0.12960124 0.11531995][0.17666826 0.19149967 0.20174091 0.21323754 0.21938886 0.21802436 0.21183904 0.20804957 0.21006891 0.20947203 0.19906901 0.17827672 0.15301682 0.13113768 0.11836101][0.17627871 0.19343235 0.20679371 0.22245774 0.23315729 0.23498967 0.22962523 0.22376564 0.22153775 0.21722825 0.20577542 0.18679856 0.16521005 0.14705691 0.13669458][0.16914475 0.18689829 0.20286661 0.22240004 0.23759183 0.24309184 0.23948582 0.23195717 0.22472747 0.21554711 0.20194736 0.18490522 0.16828166 0.1556429 0.1491624][0.15715168 0.17219205 0.18846126 0.20986015 0.22793546 0.2359962 0.23366269 0.22426854 0.21219821 0.1984992 0.18308476 0.16885349 0.15850838 0.15283726 0.15145862][0.14897411 0.15972807 0.17333643 0.19353373 0.21203831 0.22112998 0.21957524 0.20909441 0.19437943 0.17865574 0.16321197 0.15241383 0.14772464 0.14820647 0.15126926][0.13989367 0.14651771 0.15637629 0.17376067 0.19151832 0.20177279 0.20226881 0.19330736 0.17908259 0.16390947 0.14968449 0.14140533 0.13991876 0.14403951 0.14999224][0.12029517 0.12462622 0.13142149 0.14532796 0.16095081 0.17159308 0.17467055 0.16911414 0.15783857 0.14536692 0.13384143 0.12793274 0.12818705 0.134094 0.14199269][0.10244531 0.10473395 0.10780565 0.11627439 0.12700586 0.13556814 0.13959195 0.13672382 0.12832762 0.11847392 0.10929137 0.10495194 0.10633647 0.11402901 0.12539758]]...]
INFO - root - 2017-12-11 02:38:00.467053: step 78910, loss = 0.68, batch loss = 0.62 (17.8 examples/sec; 0.451 sec/batch; 31h:44m:28s remains)
INFO - root - 2017-12-11 02:38:04.917460: step 78920, loss = 0.69, batch loss = 0.63 (18.4 examples/sec; 0.436 sec/batch; 30h:41m:51s remains)
INFO - root - 2017-12-11 02:38:09.323153: step 78930, loss = 0.73, batch loss = 0.67 (18.3 examples/sec; 0.438 sec/batch; 30h:52m:15s remains)
INFO - root - 2017-12-11 02:38:13.787429: step 78940, loss = 0.71, batch loss = 0.65 (17.7 examples/sec; 0.451 sec/batch; 31h:47m:53s remains)
INFO - root - 2017-12-11 02:38:18.322517: step 78950, loss = 0.70, batch loss = 0.64 (17.5 examples/sec; 0.457 sec/batch; 32h:12m:40s remains)
INFO - root - 2017-12-11 02:38:22.874149: step 78960, loss = 0.69, batch loss = 0.63 (17.4 examples/sec; 0.460 sec/batch; 32h:25m:25s remains)
INFO - root - 2017-12-11 02:38:27.408591: step 78970, loss = 0.70, batch loss = 0.64 (17.7 examples/sec; 0.451 sec/batch; 31h:44m:55s remains)
INFO - root - 2017-12-11 02:38:31.855583: step 78980, loss = 0.71, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 31h:58m:23s remains)
INFO - root - 2017-12-11 02:38:36.431271: step 78990, loss = 0.68, batch loss = 0.62 (17.8 examples/sec; 0.450 sec/batch; 31h:39m:31s remains)
INFO - root - 2017-12-11 02:38:40.993219: step 79000, loss = 0.68, batch loss = 0.62 (17.6 examples/sec; 0.455 sec/batch; 32h:01m:45s remains)
2017-12-11 02:38:41.580005: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31246307 0.31707722 0.3054454 0.29115236 0.27575833 0.25646102 0.22882958 0.18782629 0.13415836 0.07005208 0.010004746 -0.033384684 -0.059218295 -0.073629811 -0.078624427][0.40678537 0.41968325 0.41182896 0.39829731 0.3800427 0.3566255 0.32354158 0.27334186 0.20489389 0.12207048 0.043555513 -0.015248635 -0.05186332 -0.072074838 -0.079368956][0.46128497 0.47905928 0.47496969 0.4625245 0.43989345 0.41105148 0.37298849 0.317642 0.24189481 0.15070723 0.064110905 -0.0023279572 -0.044795953 -0.068260573 -0.077121906][0.48625281 0.50307512 0.49939108 0.48653135 0.45977926 0.42510518 0.38084865 0.32056698 0.24136043 0.14931004 0.063387163 -0.0022542803 -0.044473346 -0.067207739 -0.075072512][0.48518753 0.49951708 0.49593121 0.48629922 0.4630945 0.429934 0.3837828 0.32038465 0.23812023 0.14415784 0.057946574 -0.0072876667 -0.048726685 -0.069825307 -0.075473115][0.45409244 0.46761173 0.46627218 0.46475312 0.45319459 0.43032539 0.38996437 0.32914197 0.24575958 0.14782709 0.057410259 -0.010835687 -0.053633761 -0.074240766 -0.078230195][0.40048337 0.41568637 0.41932508 0.42879683 0.43201339 0.42214 0.39113033 0.33708593 0.25597796 0.15492316 0.059314571 -0.012582093 -0.056841344 -0.077355959 -0.080366306][0.36252925 0.38147926 0.39242789 0.41427803 0.43271029 0.43490463 0.41118819 0.36058876 0.27933407 0.17276496 0.068826966 -0.0091897966 -0.056387324 -0.077825136 -0.081098191][0.35431454 0.37789121 0.39710689 0.4326387 0.46666604 0.47901595 0.45830634 0.40487942 0.31823549 0.20247774 0.087415747 0.00091786962 -0.051365823 -0.075472735 -0.080742754][0.35631669 0.38320246 0.40831935 0.45490921 0.501319 0.52037829 0.49881083 0.43967685 0.34725463 0.22543794 0.10318016 0.010466744 -0.045931611 -0.072570175 -0.080135733][0.34062141 0.36784875 0.39378265 0.44486368 0.49725404 0.51925015 0.49570081 0.4327879 0.33988646 0.22100858 0.10099468 0.00971029 -0.045358829 -0.070987783 -0.078940518][0.29049206 0.3146441 0.33789316 0.38645163 0.43732649 0.45839617 0.4337993 0.37212008 0.28697187 0.18209398 0.0764562 -0.0034509127 -0.050292041 -0.071063831 -0.077421986][0.21546358 0.23550671 0.25520477 0.29649147 0.34037569 0.35813153 0.33490974 0.27977833 0.20831467 0.1238855 0.039537415 -0.023389565 -0.05879005 -0.073265776 -0.077137873][0.1152214 0.13100664 0.1467448 0.17832948 0.21263312 0.227062 0.20843357 0.16465543 0.11080608 0.0500816 -0.0093290182 -0.052001752 -0.073547669 -0.079711646 -0.079148307][0.012101848 0.021898789 0.03295235 0.054576349 0.079033941 0.090539329 0.077879213 0.047311287 0.011599408 -0.025907086 -0.060626414 -0.082990333 -0.090411887 -0.087759376 -0.082016669]]...]
INFO - root - 2017-12-11 02:38:46.129189: step 79010, loss = 0.70, batch loss = 0.64 (18.7 examples/sec; 0.428 sec/batch; 30h:07m:24s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 02:38:50.651443: step 79020, loss = 0.69, batch loss = 0.63 (17.8 examples/sec; 0.450 sec/batch; 31h:41m:41s remains)
INFO - root - 2017-12-11 02:38:55.191258: step 79030, loss = 0.71, batch loss = 0.65 (18.0 examples/sec; 0.445 sec/batch; 31h:17m:58s remains)
INFO - root - 2017-12-11 02:38:59.701477: step 79040, loss = 0.69, batch loss = 0.63 (17.6 examples/sec; 0.455 sec/batch; 32h:00m:51s remains)
INFO - root - 2017-12-11 02:39:04.244709: step 79050, loss = 0.70, batch loss = 0.64 (18.9 examples/sec; 0.424 sec/batch; 29h:50m:59s remains)
INFO - root - 2017-12-11 02:39:08.793210: step 79060, loss = 0.70, batch loss = 0.64 (16.7 examples/sec; 0.478 sec/batch; 33h:37m:41s remains)
INFO - root - 2017-12-11 02:39:13.377338: step 79070, loss = 0.71, batch loss = 0.65 (17.0 examples/sec; 0.471 sec/batch; 33h:07m:38s remains)
INFO - root - 2017-12-11 02:39:17.948587: step 79080, loss = 0.71, batch loss = 0.65 (17.5 examples/sec; 0.457 sec/batch; 32h:08m:16s remains)
INFO - root - 2017-12-11 02:39:22.483663: step 79090, loss = 0.70, batch loss = 0.64 (17.7 examples/sec; 0.453 sec/batch; 31h:51m:50s remains)
INFO - root - 2017-12-11 02:39:27.042846: step 79100, loss = 0.69, batch loss = 0.63 (17.2 examples/sec; 0.465 sec/batch; 32h:44m:04s remains)
2017-12-11 02:39:27.591137: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14526968 0.14906332 0.14007062 0.13931282 0.15300386 0.17254663 0.19813685 0.21462253 0.2096985 0.18751809 0.15789276 0.12632433 0.087030046 0.045698043 0.013705799][0.17904462 0.18987043 0.18053658 0.17366342 0.18295674 0.19970062 0.22119634 0.23340859 0.22805579 0.20804039 0.17717732 0.13968708 0.094266959 0.050225284 0.017974121][0.19677734 0.21358724 0.20514677 0.19318327 0.19742323 0.20985241 0.22442952 0.22981355 0.22402644 0.20923585 0.18126224 0.14276753 0.09673091 0.054343726 0.023718286][0.20380706 0.22113627 0.21246965 0.19787064 0.19948928 0.21045157 0.22192079 0.22497709 0.22126095 0.21206245 0.18875124 0.15332505 0.11052945 0.069597632 0.037024081][0.21570912 0.228716 0.21603778 0.19844083 0.19869933 0.21097477 0.22470467 0.23137641 0.23248041 0.2280191 0.20844539 0.17676285 0.13707925 0.094076037 0.054922823][0.23797816 0.24450399 0.22467864 0.20228113 0.19978978 0.21327496 0.23168764 0.24519658 0.25247702 0.2516852 0.23429875 0.20431893 0.1641147 0.11484563 0.066991888][0.257115 0.26092073 0.23517066 0.20779894 0.20040001 0.21156974 0.23229659 0.25252289 0.26653188 0.26954958 0.25453225 0.22600332 0.18409291 0.12762958 0.071730271][0.262295 0.26639929 0.23859242 0.20736738 0.19280984 0.19707157 0.21578526 0.23999113 0.25976765 0.26696324 0.255928 0.23100686 0.18945701 0.129274 0.068893485][0.24669014 0.25193337 0.22615726 0.19351685 0.17162721 0.16659574 0.17987584 0.20407721 0.22630909 0.23593502 0.22928153 0.20982856 0.17234813 0.1144252 0.055570804][0.21304749 0.21731368 0.19496962 0.16358556 0.13780336 0.1254212 0.13142487 0.15005775 0.16861911 0.1767118 0.17248401 0.15855505 0.12838078 0.079837754 0.030382868][0.15711223 0.15755089 0.13879474 0.11177552 0.087946169 0.074212439 0.075440787 0.086889163 0.098218746 0.10174444 0.097516529 0.087737195 0.066126563 0.031379122 -0.0031258853][0.089339383 0.0857422 0.070735753 0.050103854 0.032199189 0.021784239 0.021486161 0.027335875 0.031998437 0.031147132 0.026353773 0.019702755 0.00631996 -0.014310982 -0.033386074][0.028085582 0.022061676 0.011021512 -0.0031924401 -0.01499095 -0.021240015 -0.021435343 -0.01891432 -0.018447358 -0.021641385 -0.026304858 -0.030615203 -0.037573155 -0.046931285 -0.053741548][-0.021807775 -0.028026458 -0.034739338 -0.042735163 -0.048824679 -0.051109485 -0.050236322 -0.048880816 -0.049769107 -0.053017084 -0.056843143 -0.059760425 -0.062755927 -0.064910777 -0.064050987][-0.053835582 -0.059465457 -0.062535971 -0.0654388 -0.066808261 -0.065907925 -0.063824549 -0.062319528 -0.062713265 -0.064788423 -0.067498192 -0.06961111 -0.070705146 -0.069685049 -0.0658628]]...]
INFO - root - 2017-12-11 02:39:32.152974: step 79110, loss = 0.70, batch loss = 0.64 (17.9 examples/sec; 0.446 sec/batch; 31h:24m:15s remains)
INFO - root - 2017-12-11 02:39:36.664226: step 79120, loss = 0.69, batch loss = 0.63 (17.3 examples/sec; 0.462 sec/batch; 32h:29m:06s remains)
INFO - root - 2017-12-11 02:39:41.213386: step 79130, loss = 0.69, batch loss = 0.63 (17.3 examples/sec; 0.462 sec/batch; 32h:32m:36s remains)
INFO - root - 2017-12-11 02:39:45.728385: step 79140, loss = 0.70, batch loss = 0.65 (17.3 examples/sec; 0.462 sec/batch; 32h:31m:14s remains)
INFO - root - 2017-12-11 02:39:50.246543: step 79150, loss = 0.69, batch loss = 0.64 (17.7 examples/sec; 0.451 sec/batch; 31h:44m:41s remains)
INFO - root - 2017-12-11 02:39:54.698913: step 79160, loss = 0.69, batch loss = 0.64 (17.8 examples/sec; 0.450 sec/batch; 31h:39m:48s remains)
INFO - root - 2017-12-11 02:39:59.177546: step 79170, loss = 0.70, batch loss = 0.64 (18.1 examples/sec; 0.443 sec/batch; 31h:09m:59s remains)
INFO - root - 2017-12-11 02:40:03.681705: step 79180, loss = 0.70, batch loss = 0.64 (17.6 examples/sec; 0.454 sec/batch; 31h:58m:15s remains)
INFO - root - 2017-12-11 02:40:08.165616: step 79190, loss = 0.69, batch loss = 0.63 (17.6 examples/sec; 0.454 sec/batch; 31h:54m:40s remains)
INFO - root - 2017-12-11 02:40:12.779335: step 79200, loss = 0.69, batch loss = 0.63 (17.1 examples/sec; 0.467 sec/batch; 32h:51m:11s remains)
2017-12-11 02:40:13.320225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.050019361 -0.049278576 -0.048029371 -0.047929127 -0.048779391 -0.049679834 -0.05009884 -0.050569881 -0.050784744 -0.051017657 -0.050914954 -0.050309084 -0.049335025 -0.047747448 -0.045922838][-0.047186054 -0.041765857 -0.037153404 -0.036238935 -0.039093729 -0.043482807 -0.047249943 -0.050152 -0.051991832 -0.053276025 -0.05388502 -0.053995512 -0.053675685 -0.052318234 -0.050006911][-0.030686567 -0.016652182 -0.0041827993 0.000529995 -0.0039889412 -0.013215335 -0.022143209 -0.029753916 -0.035756644 -0.040199921 -0.042708561 -0.044500209 -0.046643749 -0.047931 -0.047322668][0.0050962362 0.031701382 0.057003882 0.069333531 0.064888842 0.051410753 0.037340492 0.023806289 0.010790932 -4.2138578e-05 -0.0068746819 -0.012470406 -0.019758588 -0.02693698 -0.031108402][0.059503376 0.10084992 0.14212146 0.16558979 0.16454478 0.14957349 0.13197893 0.11243929 0.090360872 0.070414856 0.057036478 0.045474246 0.030252984 0.013882491 0.0017312013][0.11989986 0.17386276 0.22977458 0.26580802 0.27214375 0.26019496 0.24227791 0.21864155 0.18823269 0.15964715 0.13995233 0.12198115 0.097455435 0.06979569 0.0475394][0.16408864 0.22349396 0.28748134 0.33382839 0.35007155 0.34578019 0.33194664 0.30846637 0.27454555 0.24191035 0.21924253 0.19701172 0.16471116 0.12658951 0.094739743][0.17342053 0.2278517 0.28995052 0.34087837 0.36616972 0.37145644 0.36492059 0.34660676 0.31594378 0.2857838 0.26468313 0.24212523 0.20679899 0.16323555 0.1260246][0.14650053 0.18726221 0.238404 0.28615457 0.31520295 0.32712397 0.32772291 0.31782028 0.29599631 0.27330831 0.25673202 0.23736125 0.20530058 0.1640362 0.12822558][0.098951854 0.12295659 0.15862717 0.19643672 0.22200571 0.23439427 0.23857512 0.23643395 0.22482349 0.21073765 0.19902204 0.18449187 0.16036774 0.12835035 0.10039783][0.056062169 0.067368194 0.089406073 0.11488227 0.13180351 0.13848762 0.14029279 0.14044148 0.13480397 0.12625761 0.11795378 0.10843531 0.093815483 0.073849112 0.056685936][0.037723534 0.046469774 0.062440403 0.078581944 0.085755646 0.082490623 0.075237535 0.069033951 0.061063576 0.052150723 0.044671755 0.0391169 0.032433361 0.022763997 0.015236992][0.0531863 0.070689335 0.0894498 0.10128099 0.099469408 0.083248891 0.061126713 0.040742517 0.023063112 0.00917333 0.00090141682 -0.0017016336 -0.0030750562 -0.0063229487 -0.0076552052][0.094301321 0.12640606 0.15334119 0.16519479 0.15606508 0.12593158 0.086298719 0.048680484 0.019029886 -0.00021392251 -0.0085600438 -0.0080998652 -0.005879086 -0.0058418619 -0.0044007655][0.14346601 0.18768272 0.22212352 0.23530076 0.22014448 0.17770223 0.12285206 0.070800185 0.031747926 0.0094757583 0.0026345483 0.0067089731 0.012274545 0.014858775 0.018081838]]...]
INFO - root - 2017-12-11 02:40:17.920465: step 79210, loss = 0.70, batch loss = 0.64 (16.5 examples/sec; 0.485 sec/batch; 34h:06m:34s remains)
INFO - root - 2017-12-11 02:40:22.514285: step 79220, loss = 0.69, batch loss = 0.64 (17.2 examples/sec; 0.464 sec/batch; 32h:38m:59s remains)
INFO - root - 2017-12-11 02:40:27.040452: step 79230, loss = 0.69, batch loss = 0.63 (17.9 examples/sec; 0.446 sec/batch; 31h:23m:42s remains)
INFO - root - 2017-12-11 02:40:31.592718: step 79240, loss = 0.70, batch loss = 0.64 (17.7 examples/sec; 0.452 sec/batch; 31h:46m:39s remains)
INFO - root - 2017-12-11 02:40:36.017826: step 79250, loss = 0.69, batch loss = 0.64 (17.8 examples/sec; 0.449 sec/batch; 31h:34m:31s remains)
INFO - root - 2017-12-11 02:40:40.497102: step 79260, loss = 0.69, batch loss = 0.63 (17.8 examples/sec; 0.450 sec/batch; 31h:38m:54s remains)
INFO - root - 2017-12-11 02:40:44.967566: step 79270, loss = 0.68, batch loss = 0.62 (17.8 examples/sec; 0.449 sec/batch; 31h:33m:28s remains)
INFO - root - 2017-12-11 02:40:49.400459: step 79280, loss = 0.69, batch loss = 0.63 (18.3 examples/sec; 0.436 sec/batch; 30h:40m:55s remains)
INFO - root - 2017-12-11 02:40:53.849185: step 79290, loss = 0.69, batch loss = 0.63 (17.8 examples/sec; 0.449 sec/batch; 31h:35m:15s remains)
INFO - root - 2017-12-11 02:40:58.349518: step 79300, loss = 0.69, batch loss = 0.63 (17.5 examples/sec; 0.458 sec/batch; 32h:10m:51s remains)
2017-12-11 02:40:58.938869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.079838105 -0.081649236 -0.082802735 -0.0840006 -0.084427096 -0.08371897 -0.081344 -0.078070104 -0.074364133 -0.071735531 -0.070798948 -0.071256638 -0.072270229 -0.072481155 -0.072071783][-0.07326822 -0.073043533 -0.0735983 -0.076118745 -0.079615675 -0.0824099 -0.082518712 -0.08037924 -0.07706888 -0.074701451 -0.074138731 -0.0756849 -0.07810095 -0.079778358 -0.079943366][-0.048853021 -0.043406151 -0.041670747 -0.045635533 -0.053461537 -0.061621215 -0.066256262 -0.066848084 -0.064962648 -0.063898154 -0.064719379 -0.068110123 -0.073185258 -0.07804738 -0.080845669][0.00049451832 0.016422678 0.023320707 0.01825886 0.0044670152 -0.012053089 -0.024161268 -0.028617423 -0.02708775 -0.025342016 -0.026243035 -0.031679604 -0.041938048 -0.05472859 -0.0658124][0.077163905 0.11175309 0.12877992 0.12357941 0.10082079 0.069745176 0.043845411 0.032840889 0.036379851 0.042991497 0.0450428 0.03792616 0.019491952 -0.0073016924 -0.034389965][0.16750571 0.22567897 0.25511408 0.24978027 0.21666959 0.16929202 0.12941074 0.11496236 0.12625755 0.14224647 0.14757557 0.1353949 0.10354682 0.05622277 0.0065490231][0.2523219 0.33001497 0.36816859 0.3615917 0.32142147 0.2652742 0.22082856 0.21025969 0.23312698 0.25780073 0.26108712 0.23738478 0.18755905 0.11787705 0.045104314][0.31030732 0.39536351 0.43550411 0.42915809 0.39172506 0.34222537 0.30766281 0.30733839 0.33888131 0.36353105 0.3547166 0.31209531 0.24246785 0.15417896 0.064629033][0.33302429 0.4158639 0.45596677 0.45762986 0.43702409 0.4092958 0.39389259 0.40244964 0.43085238 0.44002229 0.4061144 0.33729663 0.24770561 0.14742753 0.051677227][0.33304173 0.41041541 0.45540634 0.47677216 0.4859184 0.48861179 0.492422 0.49978346 0.50743926 0.48321611 0.4115693 0.31084803 0.20264937 0.098571941 0.0091109322][0.32329732 0.39641285 0.45089957 0.49762216 0.53937274 0.56997746 0.58504218 0.57933789 0.55276078 0.48569539 0.37387514 0.24498893 0.12595513 0.028698411 -0.042999551][0.31612945 0.38812891 0.45352259 0.522017 0.58686304 0.63159621 0.64404726 0.61509538 0.54884124 0.44037285 0.29791293 0.15478861 0.038452141 -0.041387927 -0.08839751][0.31554592 0.39154992 0.46650863 0.5469141 0.61871105 0.65971744 0.65583462 0.59874469 0.4965162 0.35724339 0.20042661 0.060241673 -0.040360026 -0.09669099 -0.11878829][0.30952832 0.39023036 0.46751228 0.54467416 0.60557425 0.62824655 0.60184449 0.52214336 0.40027371 0.25208497 0.10274782 -0.017375657 -0.093028449 -0.12539129 -0.12828574][0.27427295 0.35168827 0.42008191 0.48025972 0.518671 0.51796573 0.47297585 0.38487327 0.26632783 0.13502827 0.014915925 -0.071343966 -0.11708587 -0.1283762 -0.11958588]]...]
INFO - root - 2017-12-11 02:41:03.374200: step 79310, loss = 0.69, batch loss = 0.63 (18.2 examples/sec; 0.439 sec/batch; 30h:54m:04s remains)
INFO - root - 2017-12-11 02:41:07.841471: step 79320, loss = 0.68, batch loss = 0.63 (18.0 examples/sec; 0.445 sec/batch; 31h:17m:48s remains)
INFO - root - 2017-12-11 02:41:12.343706: step 79330, loss = 0.72, batch loss = 0.66 (17.9 examples/sec; 0.448 sec/batch; 31h:28m:19s remains)
INFO - root - 2017-12-11 02:41:16.915651: step 79340, loss = 0.69, batch loss = 0.63 (17.5 examples/sec; 0.456 sec/batch; 32h:05m:52s remains)
INFO - root - 2017-12-11 02:41:21.404198: step 79350, loss = 0.68, batch loss = 0.62 (17.8 examples/sec; 0.449 sec/batch; 31h:35m:18s remains)
INFO - root - 2017-12-11 02:41:25.898398: step 79360, loss = 0.70, batch loss = 0.64 (18.2 examples/sec; 0.439 sec/batch; 30h:50m:53s remains)
INFO - root - 2017-12-11 02:41:30.355979: step 79370, loss = 0.70, batch loss = 0.65 (17.8 examples/sec; 0.450 sec/batch; 31h:37m:27s remains)
INFO - root - 2017-12-11 02:41:34.929720: step 79380, loss = 0.70, batch loss = 0.64 (17.9 examples/sec; 0.448 sec/batch; 31h:29m:48s remains)
INFO - root - 2017-12-11 02:41:39.306100: step 79390, loss = 0.71, batch loss = 0.65 (23.1 examples/sec; 0.347 sec/batch; 24h:22m:22s remains)
INFO - root - 2017-12-11 02:41:43.823109: step 79400, loss = 0.69, batch loss = 0.63 (17.6 examples/sec; 0.454 sec/batch; 31h:56m:40s remains)
2017-12-11 02:41:44.380153: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.4867 0.51108181 0.49152896 0.45493504 0.43754947 0.44294113 0.45050851 0.43903798 0.39982364 0.33293313 0.23761134 0.13012898 0.033670235 -0.033756603 -0.067866266][0.60574293 0.61942005 0.58346438 0.52959454 0.50198972 0.51584405 0.54403371 0.55181664 0.51711977 0.44147995 0.32980031 0.20001607 0.079247288 -0.0093962941 -0.056676988][0.64313859 0.64232069 0.59583366 0.53504258 0.50488037 0.52919841 0.57514918 0.59921116 0.56999439 0.49011362 0.3705098 0.23024286 0.099518053 0.0023999177 -0.0503501][0.58645785 0.57774776 0.53570175 0.4861708 0.46744186 0.50547373 0.56414849 0.59881681 0.57442385 0.49408656 0.37342009 0.23314323 0.10400961 0.0074438481 -0.046271622][0.46899763 0.46503133 0.4445276 0.42453095 0.43028426 0.48693514 0.55952996 0.60564739 0.58807111 0.5095 0.39034155 0.25140303 0.12229872 0.023042314 -0.035643023][0.35048783 0.36310592 0.37714598 0.39633694 0.43065602 0.50585866 0.59046626 0.64412314 0.62765461 0.54745144 0.42817903 0.28782004 0.15405057 0.046912104 -0.02022236][0.25018117 0.28884748 0.34500471 0.40438595 0.46244803 0.54610866 0.63041383 0.677623 0.64997071 0.56267279 0.4439936 0.30566689 0.17098747 0.059790812 -0.011681359][0.17128482 0.23935859 0.33492127 0.42526877 0.49455574 0.57018095 0.63674229 0.66211247 0.61568916 0.5214712 0.40806204 0.27984077 0.15411408 0.049266282 -0.017484941][0.1107798 0.20186357 0.32103944 0.42346498 0.48800808 0.54196757 0.58065629 0.58111709 0.52124113 0.42868063 0.32751417 0.21688575 0.10918169 0.020590028 -0.034077425][0.069564372 0.16636389 0.28507048 0.37876666 0.42733532 0.45616737 0.46898481 0.45254394 0.39183336 0.31075364 0.2263542 0.13601078 0.050171319 -0.01758622 -0.056737743][0.036457967 0.1189907 0.21603008 0.28691286 0.31624416 0.32437256 0.31841987 0.29290608 0.238842 0.17412548 0.10945617 0.042752754 -0.016900629 -0.060139209 -0.081348471][-0.0036231347 0.053400893 0.11913967 0.16353449 0.17666456 0.17186636 0.15593009 0.12809709 0.084465913 0.037860796 -0.0052553066 -0.045972325 -0.077870652 -0.096368805 -0.1002631][-0.040498834 -0.00841835 0.027735066 0.048823237 0.049908 0.03855823 0.019958764 -0.0044482206 -0.035109207 -0.063559473 -0.086392984 -0.10432243 -0.11399004 -0.11453755 -0.10733861][-0.068301179 -0.055056527 -0.040743086 -0.036039025 -0.042264547 -0.055148751 -0.071023718 -0.088164546 -0.10580645 -0.11944947 -0.1276235 -0.1306216 -0.12713984 -0.11818445 -0.10612182][-0.087200627 -0.087198488 -0.086546347 -0.090802252 -0.099286139 -0.10952706 -0.11939827 -0.12800632 -0.13499644 -0.13877752 -0.13867272 -0.13437451 -0.12548555 -0.11353391 -0.10089111]]...]
INFO - root - 2017-12-11 02:41:48.894025: step 79410, loss = 0.71, batch loss = 0.65 (18.2 examples/sec; 0.440 sec/batch; 30h:57m:10s remains)
INFO - root - 2017-12-11 02:41:53.365564: step 79420, loss = 0.71, batch loss = 0.65 (17.7 examples/sec; 0.452 sec/batch; 31h:47m:56s remains)
INFO - root - 2017-12-11 02:41:57.884131: step 79430, loss = 0.71, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 31h:56m:21s remains)
INFO - root - 2017-12-11 02:42:02.420106: step 79440, loss = 0.69, batch loss = 0.63 (17.6 examples/sec; 0.454 sec/batch; 31h:55m:47s remains)
INFO - root - 2017-12-11 02:42:06.915624: step 79450, loss = 0.68, batch loss = 0.62 (17.3 examples/sec; 0.463 sec/batch; 32h:33m:03s remains)
INFO - root - 2017-12-11 02:42:11.465459: step 79460, loss = 0.69, batch loss = 0.64 (17.7 examples/sec; 0.452 sec/batch; 31h:44m:38s remains)
INFO - root - 2017-12-11 02:42:15.977830: step 79470, loss = 0.68, batch loss = 0.62 (17.5 examples/sec; 0.457 sec/batch; 32h:05m:41s remains)
INFO - root - 2017-12-11 02:42:20.458246: step 79480, loss = 0.68, batch loss = 0.63 (17.8 examples/sec; 0.448 sec/batch; 31h:30m:09s remains)
INFO - root - 2017-12-11 02:42:24.943178: step 79490, loss = 0.70, batch loss = 0.64 (18.3 examples/sec; 0.438 sec/batch; 30h:46m:40s remains)
INFO - root - 2017-12-11 02:42:29.466712: step 79500, loss = 0.70, batch loss = 0.64 (17.5 examples/sec; 0.457 sec/batch; 32h:07m:00s remains)
2017-12-11 02:42:30.008891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029361725 -0.011685443 0.0060373605 0.016408561 0.017132008 0.013582675 0.011176865 0.010329059 0.0111912 0.010200318 0.0081852954 0.0028253032 -0.0082729049 -0.021285154 -0.03035407][-0.0037986862 0.025136841 0.053732768 0.072486505 0.077617668 0.075296134 0.071898431 0.06795159 0.064839423 0.058800332 0.052010782 0.0425233 0.027457539 0.011447417 0.00067945389][0.024978584 0.067260809 0.10985179 0.14130667 0.15585594 0.15923582 0.15725052 0.14976826 0.13977262 0.12424493 0.10880326 0.094028577 0.076414764 0.059846297 0.04913801][0.0506147 0.10770176 0.1680792 0.21827407 0.24982139 0.26619917 0.26979309 0.25930238 0.23940656 0.20998527 0.18232939 0.1612916 0.14247254 0.12711719 0.11713055][0.072475307 0.14573768 0.22771361 0.30278492 0.35879359 0.39496154 0.40721753 0.39329815 0.3605971 0.31462356 0.27403229 0.24787343 0.23063241 0.21843562 0.20916948][0.092539653 0.18261595 0.28746134 0.38926315 0.47153136 0.52756321 0.5459069 0.52537662 0.47851449 0.41866481 0.37022096 0.34435025 0.33359644 0.3279193 0.32113162][0.11657298 0.22312199 0.34891984 0.47325626 0.57445627 0.64045507 0.65521437 0.62281609 0.56326121 0.49674496 0.44877303 0.42921782 0.428912 0.4331108 0.43214178][0.14107034 0.26023293 0.39944857 0.53415459 0.63755614 0.69574994 0.69482106 0.64769238 0.58093435 0.51753235 0.47862855 0.4699825 0.48148987 0.49687746 0.505136][0.15582557 0.27751344 0.41480932 0.54005474 0.62387353 0.65597731 0.63153106 0.572485 0.50803161 0.45807195 0.43536392 0.44036189 0.46378762 0.49147686 0.5128898][0.15350839 0.2643781 0.38237697 0.47923613 0.52809185 0.52642077 0.48098904 0.41851845 0.36549714 0.33424443 0.32836789 0.34508735 0.37784997 0.41731772 0.45404461][0.12745282 0.21502987 0.30112368 0.3604759 0.37314665 0.34483987 0.29115567 0.23710838 0.20165157 0.18831423 0.19412726 0.21599486 0.25116974 0.29628962 0.34379333][0.077743933 0.1358486 0.18852055 0.21559113 0.20475191 0.16668759 0.11974341 0.082015321 0.06322927 0.060764797 0.069683217 0.088226616 0.11722489 0.15824993 0.20642149][0.013680207 0.044374172 0.071519122 0.080510288 0.064500242 0.033905394 0.0036243068 -0.016751498 -0.02440387 -0.023587083 -0.017704885 -0.0071795085 0.010430366 0.039040521 0.076562516][-0.044194628 -0.033478312 -0.021456113 -0.018083612 -0.027286699 -0.042423464 -0.054983255 -0.061966296 -0.063727342 -0.063179553 -0.061320979 -0.057921503 -0.050784819 -0.036230627 -0.014806861][-0.08089418 -0.081584357 -0.076790564 -0.073615134 -0.074494511 -0.076754995 -0.077472784 -0.07714162 -0.076713413 -0.077114142 -0.077903837 -0.078634538 -0.078015722 -0.073795289 -0.066440471]]...]
INFO - root - 2017-12-11 02:42:34.511045: step 79510, loss = 0.69, batch loss = 0.63 (18.5 examples/sec; 0.433 sec/batch; 30h:23m:50s remains)
INFO - root - 2017-12-11 02:42:38.902016: step 79520, loss = 0.68, batch loss = 0.63 (17.2 examples/sec; 0.465 sec/batch; 32h:42m:37s remains)
INFO - root - 2017-12-11 02:42:43.414751: step 79530, loss = 0.69, batch loss = 0.63 (17.4 examples/sec; 0.459 sec/batch; 32h:14m:11s remains)
INFO - root - 2017-12-11 02:42:47.871000: step 79540, loss = 0.68, batch loss = 0.62 (17.6 examples/sec; 0.455 sec/batch; 31h:56m:24s remains)
INFO - root - 2017-12-11 02:42:52.415329: step 79550, loss = 0.70, batch loss = 0.64 (17.2 examples/sec; 0.466 sec/batch; 32h:42m:44s remains)
INFO - root - 2017-12-11 02:42:56.930822: step 79560, loss = 0.71, batch loss = 0.65 (17.3 examples/sec; 0.463 sec/batch; 32h:32m:55s remains)
INFO - root - 2017-12-11 02:43:01.415277: step 79570, loss = 0.69, batch loss = 0.63 (18.0 examples/sec; 0.443 sec/batch; 31h:09m:29s remains)
INFO - root - 2017-12-11 02:43:05.837230: step 79580, loss = 0.69, batch loss = 0.64 (17.3 examples/sec; 0.461 sec/batch; 32h:24m:57s remains)
INFO - root - 2017-12-11 02:43:10.265953: step 79590, loss = 0.69, batch loss = 0.64 (17.5 examples/sec; 0.456 sec/batch; 32h:03m:09s remains)
INFO - root - 2017-12-11 02:43:14.821918: step 79600, loss = 0.69, batch loss = 0.63 (17.4 examples/sec; 0.460 sec/batch; 32h:17m:04s remains)
2017-12-11 02:43:15.342358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.05086847 -0.048839778 -0.043661848 -0.040069144 -0.040204074 -0.043358635 -0.044780161 -0.043648485 -0.041578997 -0.041071448 -0.041424762 -0.042563967 -0.04724865 -0.055763632 -0.065800793][-0.008317736 0.0042603933 0.018086251 0.026142247 0.025200438 0.01742615 0.012258507 0.012755022 0.014823744 0.014723643 0.013040501 0.0096160313 -0.00094191078 -0.020270037 -0.042927932][0.059411641 0.08892756 0.11716577 0.13366246 0.13368289 0.12163804 0.11281084 0.11224914 0.11259716 0.10864442 0.10155246 0.091406405 0.069542475 0.033359919 -0.0077504409][0.13941398 0.19065796 0.23820159 0.266603 0.26936257 0.25344297 0.24036613 0.23718089 0.23264219 0.22097306 0.20406762 0.18306826 0.14585976 0.088940255 0.02683215][0.218311 0.29251817 0.36031452 0.40125185 0.40740371 0.38861886 0.3722795 0.36643964 0.35570604 0.33348447 0.3036249 0.26895943 0.21457647 0.13653314 0.054077264][0.275833 0.36775848 0.45142 0.50345808 0.51562053 0.49950105 0.48541105 0.47987977 0.46283308 0.42774048 0.38195163 0.33153129 0.26079032 0.16556175 0.068506539][0.29409108 0.39371273 0.48600346 0.54725724 0.56941217 0.56388283 0.55944127 0.55906463 0.53881985 0.49304849 0.43313274 0.36817718 0.28377321 0.17658716 0.071097225][0.2707265 0.36842513 0.46244451 0.53013873 0.56379163 0.57285076 0.58154517 0.58828336 0.56817997 0.51696843 0.44830415 0.37282163 0.278962 0.16592963 0.0586781][0.21457873 0.30117422 0.38789156 0.45459157 0.49403757 0.51314896 0.53084195 0.54307342 0.52682096 0.4786782 0.41161332 0.33576033 0.24302803 0.13510163 0.035391785][0.13956791 0.20674071 0.27646109 0.33256662 0.36909872 0.39096323 0.41224959 0.42787328 0.4184081 0.38029706 0.32380068 0.25731757 0.17661177 0.085361339 0.0030695421][0.058888685 0.10243197 0.15038007 0.19096614 0.21989374 0.24007319 0.26084676 0.27722815 0.27405575 0.24788322 0.20571873 0.15424567 0.0924832 0.024984712 -0.03397378][-0.0089552747 0.013242627 0.040835604 0.065491587 0.084473237 0.09914536 0.11508292 0.12891927 0.13035254 0.11613219 0.089898318 0.056402326 0.016223034 -0.026858829 -0.063616626][-0.052864973 -0.045724947 -0.033372376 -0.022041827 -0.01315085 -0.0062919008 0.0018901378 0.010312312 0.013502868 0.008647183 -0.0035334497 -0.020109439 -0.040425759 -0.062541515 -0.081527054][-0.07567472 -0.078010336 -0.0759488 -0.074421816 -0.074040122 -0.0746986 -0.074142888 -0.07162568 -0.069061905 -0.068635821 -0.071130075 -0.075040057 -0.080279633 -0.086648449 -0.092298508][-0.084376924 -0.091796517 -0.095350318 -0.099292479 -0.10369168 -0.10843572 -0.11208875 -0.11339886 -0.11255886 -0.1109386 -0.10945702 -0.10722481 -0.1044383 -0.10173664 -0.0990126]]...]
INFO - root - 2017-12-11 02:43:19.935181: step 79610, loss = 0.71, batch loss = 0.65 (17.5 examples/sec; 0.456 sec/batch; 32h:03m:55s remains)
INFO - root - 2017-12-11 02:43:24.446306: step 79620, loss = 0.69, batch loss = 0.63 (17.5 examples/sec; 0.457 sec/batch; 32h:04m:41s remains)
INFO - root - 2017-12-11 02:43:29.053243: step 79630, loss = 0.69, batch loss = 0.64 (17.5 examples/sec; 0.458 sec/batch; 32h:10m:41s remains)
INFO - root - 2017-12-11 02:43:33.683068: step 79640, loss = 0.70, batch loss = 0.64 (17.6 examples/sec; 0.454 sec/batch; 31h:51m:17s remains)
INFO - root - 2017-12-11 02:43:38.168308: step 79650, loss = 0.69, batch loss = 0.63 (17.5 examples/sec; 0.458 sec/batch; 32h:08m:03s remains)
INFO - root - 2017-12-11 02:43:42.717423: step 79660, loss = 0.70, batch loss = 0.64 (18.0 examples/sec; 0.444 sec/batch; 31h:12m:10s remains)
INFO - root - 2017-12-11 02:43:47.296540: step 79670, loss = 0.69, batch loss = 0.63 (17.2 examples/sec; 0.465 sec/batch; 32h:39m:29s remains)
INFO - root - 2017-12-11 02:43:51.874096: step 79680, loss = 0.70, batch loss = 0.64 (18.2 examples/sec; 0.440 sec/batch; 30h:53m:07s remains)
INFO - root - 2017-12-11 02:43:56.462123: step 79690, loss = 0.68, batch loss = 0.62 (17.8 examples/sec; 0.450 sec/batch; 31h:37m:36s remains)
INFO - root - 2017-12-11 02:44:01.019164: step 79700, loss = 0.70, batch loss = 0.64 (18.1 examples/sec; 0.442 sec/batch; 31h:02m:12s remains)
2017-12-11 02:44:01.569101: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13956745 0.12082029 0.093057789 0.06663692 0.050097197 0.048088446 0.06235493 0.088006034 0.11408226 0.13237414 0.13830224 0.13105389 0.11464034 0.096308716 0.089296088][0.19905134 0.175903 0.14301026 0.11267282 0.094214663 0.091829851 0.10698819 0.13438596 0.16222282 0.18269187 0.1908218 0.18478131 0.1688163 0.14938191 0.13877015][0.25646114 0.23138429 0.19478801 0.16076133 0.13897462 0.13335064 0.14535141 0.1699717 0.19563551 0.21582654 0.22549506 0.22211047 0.21013758 0.19476564 0.18640058][0.3059389 0.28123549 0.24264333 0.2051855 0.17851916 0.16707389 0.17337935 0.19367985 0.21664894 0.236112 0.24620916 0.24445222 0.23658232 0.22814687 0.22762989][0.34803936 0.3252331 0.28623372 0.24631068 0.21478105 0.19706862 0.19781059 0.21498579 0.23651442 0.25448683 0.26187733 0.25813761 0.25183517 0.25071779 0.26155084][0.38368246 0.36254373 0.32363385 0.28347191 0.25080535 0.23143165 0.2311154 0.24825256 0.2689971 0.28198397 0.28036383 0.26856422 0.25928026 0.26285744 0.28400898][0.40175784 0.3793816 0.34134647 0.3059026 0.27980652 0.26691508 0.27160218 0.29110441 0.30964488 0.31301636 0.29689398 0.27318662 0.25814375 0.26273865 0.28874815][0.3991622 0.3735837 0.33820394 0.31284609 0.30038777 0.29982156 0.31152996 0.33085626 0.34197035 0.3304697 0.29711056 0.26162538 0.24127761 0.24409814 0.26745802][0.37798896 0.35104388 0.32174724 0.30941242 0.31174758 0.3214477 0.33460703 0.34607235 0.34365186 0.31578779 0.26913142 0.22780107 0.20651241 0.20773771 0.22461884][0.3457745 0.32303518 0.30308715 0.30310807 0.31590211 0.32885423 0.3349151 0.33061722 0.31044084 0.26798427 0.21458918 0.17420419 0.15677173 0.15904105 0.17197248][0.31206352 0.29878974 0.28795448 0.29473132 0.30937934 0.31695905 0.30930018 0.28631887 0.24966066 0.19809023 0.14468189 0.10959303 0.098461546 0.1045958 0.11852152][0.28823256 0.28437391 0.27851355 0.28379506 0.29046687 0.28449109 0.2592341 0.21897648 0.17113486 0.11812415 0.071394235 0.045244627 0.042255811 0.054973278 0.074722245][0.2795876 0.28258896 0.27745283 0.27518147 0.26740345 0.24340411 0.20037723 0.14712925 0.094625 0.046949707 0.0118835 -0.0029266092 0.00281168 0.022747952 0.050274204][0.273536 0.28127602 0.27659863 0.26685426 0.2444575 0.20369157 0.14719716 0.087999612 0.0381071 0.0017685243 -0.017998353 -0.020476423 -0.0078161778 0.015761888 0.04777218][0.26228368 0.27309251 0.26968655 0.25499764 0.22240035 0.17131655 0.10958453 0.052852776 0.012163278 -0.009471707 -0.013995053 -0.0063151019 0.0092297178 0.030390535 0.059689917]]...]
INFO - root - 2017-12-11 02:44:06.170015: step 79710, loss = 0.69, batch loss = 0.63 (18.3 examples/sec; 0.438 sec/batch; 30h:44m:34s remains)
INFO - root - 2017-12-11 02:44:10.718187: step 79720, loss = 0.68, batch loss = 0.62 (17.2 examples/sec; 0.465 sec/batch; 32h:38m:07s remains)
INFO - root - 2017-12-11 02:44:15.288896: step 79730, loss = 0.71, batch loss = 0.65 (19.0 examples/sec; 0.421 sec/batch; 29h:33m:40s remains)
INFO - root - 2017-12-11 02:44:19.775867: step 79740, loss = 0.70, batch loss = 0.64 (18.1 examples/sec; 0.443 sec/batch; 31h:05m:53s remains)
INFO - root - 2017-12-11 02:44:24.119339: step 79750, loss = 0.70, batch loss = 0.64 (18.1 examples/sec; 0.441 sec/batch; 30h:59m:08s remains)
INFO - root - 2017-12-11 02:44:28.688465: step 79760, loss = 0.69, batch loss = 0.63 (17.8 examples/sec; 0.450 sec/batch; 31h:34m:19s remains)
INFO - root - 2017-12-11 02:44:33.211342: step 79770, loss = 0.67, batch loss = 0.61 (17.9 examples/sec; 0.447 sec/batch; 31h:24m:54s remains)
INFO - root - 2017-12-11 02:44:37.710727: step 79780, loss = 0.69, batch loss = 0.63 (17.2 examples/sec; 0.464 sec/batch; 32h:34m:03s remains)
INFO - root - 2017-12-11 02:44:42.213902: step 79790, loss = 0.69, batch loss = 0.63 (17.8 examples/sec; 0.448 sec/batch; 31h:28m:04s remains)
INFO - root - 2017-12-11 02:44:46.743264: step 79800, loss = 0.69, batch loss = 0.63 (17.6 examples/sec; 0.454 sec/batch; 31h:53m:59s remains)
2017-12-11 02:44:47.288734: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.077736318 0.063108549 0.071915865 0.1098357 0.17199972 0.24394706 0.29692483 0.30818459 0.2695182 0.19368884 0.10007709 0.01205882 -0.055048466 -0.092297904 -0.10140391][0.12810591 0.11733109 0.12974937 0.17490032 0.24965496 0.33924782 0.41011447 0.43126142 0.39036715 0.29965717 0.1827659 0.069597051 -0.021210309 -0.077854447 -0.099865288][0.16140217 0.15403418 0.16921276 0.22045648 0.30494994 0.40767783 0.49283695 0.5243727 0.48650795 0.38877556 0.25685829 0.12573767 0.016802223 -0.056519672 -0.091199376][0.17372562 0.16672242 0.18437836 0.24151126 0.33396712 0.44699332 0.54368621 0.58475012 0.55348039 0.45599654 0.31765193 0.1757513 0.054381881 -0.031249169 -0.076156206][0.16931997 0.15876655 0.17752077 0.23837985 0.334496 0.45250925 0.55602533 0.60490835 0.58292204 0.49292386 0.35704535 0.2126175 0.086326696 -0.0047694324 -0.054900676][0.14978023 0.13235275 0.14876187 0.20897645 0.30328614 0.420406 0.52701 0.58428508 0.5757786 0.49967316 0.37392476 0.23394856 0.10959849 0.019583657 -0.030047685][0.12544222 0.096218288 0.10255006 0.154433 0.24213661 0.35736722 0.47033164 0.54312491 0.5564732 0.50121951 0.39038736 0.25741482 0.13612998 0.047933023 0.00035594942][0.11705703 0.072708711 0.060385831 0.094022878 0.1692719 0.2826 0.40565783 0.49885935 0.53847486 0.50768924 0.41339105 0.28672197 0.16616572 0.077772893 0.032914072][0.13339344 0.076172367 0.043228723 0.053721014 0.11199152 0.22017328 0.35016426 0.45949167 0.52034253 0.51099771 0.43193623 0.31061307 0.18888018 0.098894812 0.057304487][0.16293366 0.10223272 0.055427112 0.045936476 0.08616326 0.18330933 0.3113353 0.42627463 0.49755862 0.50187182 0.43432269 0.31791773 0.19541672 0.10417484 0.065704651][0.18804629 0.13476826 0.084663853 0.063751735 0.089642555 0.17245555 0.29092306 0.402582 0.47497952 0.48382413 0.42180496 0.30915037 0.18759575 0.096304782 0.059510875][0.20264222 0.16445518 0.1209314 0.097871296 0.11498485 0.18354914 0.2871013 0.38735157 0.45158508 0.45608151 0.39396769 0.28541163 0.1695524 0.082749777 0.048279572][0.20352326 0.18668351 0.15862697 0.14235713 0.15643319 0.21099842 0.29251638 0.36834043 0.40952405 0.39806277 0.33106968 0.22984883 0.12880231 0.056975242 0.031936932][0.1865883 0.193322 0.18743767 0.18740904 0.20782444 0.25400543 0.31284004 0.35778379 0.3668865 0.33185452 0.25626796 0.16322012 0.080939472 0.029267751 0.017439252][0.15718208 0.18427311 0.20340914 0.22805549 0.26614693 0.31521517 0.36053285 0.3809689 0.36151594 0.30300462 0.21609718 0.12634383 0.057318889 0.020702278 0.018483972]]...]
INFO - root - 2017-12-11 02:44:51.681182: step 79810, loss = 0.69, batch loss = 0.63 (18.5 examples/sec; 0.433 sec/batch; 30h:25m:38s remains)
INFO - root - 2017-12-11 02:44:56.139694: step 79820, loss = 0.68, batch loss = 0.62 (17.9 examples/sec; 0.446 sec/batch; 31h:19m:05s remains)
INFO - root - 2017-12-11 02:45:00.655353: step 79830, loss = 0.69, batch loss = 0.63 (17.9 examples/sec; 0.447 sec/batch; 31h:23m:03s remains)
INFO - root - 2017-12-11 02:45:05.113150: step 79840, loss = 0.69, batch loss = 0.63 (18.2 examples/sec; 0.441 sec/batch; 30h:55m:23s remains)
INFO - root - 2017-12-11 02:45:09.547042: step 79850, loss = 0.69, batch loss = 0.63 (18.0 examples/sec; 0.444 sec/batch; 31h:08m:46s remains)
INFO - root - 2017-12-11 02:45:14.090673: step 79860, loss = 0.70, batch loss = 0.64 (17.8 examples/sec; 0.450 sec/batch; 31h:36m:17s remains)
INFO - root - 2017-12-11 02:45:18.579399: step 79870, loss = 0.71, batch loss = 0.66 (17.8 examples/sec; 0.450 sec/batch; 31h:34m:11s remains)
INFO - root - 2017-12-11 02:45:23.033801: step 79880, loss = 0.70, batch loss = 0.64 (17.2 examples/sec; 0.466 sec/batch; 32h:40m:51s remains)
INFO - root - 2017-12-11 02:45:27.509282: step 79890, loss = 0.69, batch loss = 0.63 (17.5 examples/sec; 0.457 sec/batch; 32h:02m:10s remains)
INFO - root - 2017-12-11 02:45:31.969124: step 79900, loss = 0.69, batch loss = 0.63 (17.8 examples/sec; 0.448 sec/batch; 31h:27m:15s remains)
2017-12-11 02:45:32.545538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0077320486 -0.0073551564 0.019987771 0.06372489 0.11044251 0.15030719 0.17191738 0.16550815 0.13553913 0.11057638 0.11831047 0.1579162 0.20837282 0.24618183 0.25399071][-0.0018215791 0.0011020737 0.026298875 0.062940516 0.099813543 0.13052611 0.14679173 0.1400193 0.11316425 0.092223108 0.10519927 0.15296109 0.21360497 0.26027566 0.27352542][0.023396607 0.040688317 0.0712227 0.10344502 0.1294028 0.14805695 0.15563938 0.14622147 0.12124532 0.099553883 0.10495326 0.14105168 0.1905039 0.22916245 0.23885486][0.073313773 0.11622829 0.16533349 0.20714396 0.23572749 0.25483423 0.263266 0.25530562 0.22944632 0.19669914 0.17546611 0.17382821 0.18414658 0.19147068 0.18251386][0.13805746 0.21112712 0.289977 0.35855007 0.41049576 0.45138073 0.478037 0.47974673 0.45015988 0.39515433 0.33048293 0.27004951 0.219768 0.17779453 0.13679421][0.19865286 0.29661793 0.404926 0.50511545 0.58896154 0.66119438 0.7146253 0.73100638 0.69845617 0.62043351 0.51292086 0.39561391 0.28654256 0.19548962 0.12035741][0.23833735 0.34595552 0.46767211 0.58514386 0.68915445 0.78312784 0.85738236 0.88753444 0.85724872 0.76868594 0.63867593 0.4904111 0.34872988 0.22962227 0.13336611][0.26102081 0.35979202 0.47329384 0.58583069 0.68824708 0.78284538 0.8603949 0.89381504 0.86557055 0.77881175 0.65166706 0.507345 0.3694762 0.25291669 0.15680763][0.27396119 0.35076606 0.44086245 0.531831 0.61451346 0.69017524 0.7518068 0.77345586 0.74049515 0.66136879 0.55674148 0.44609979 0.34373996 0.25629854 0.17925186][0.28684327 0.33935726 0.40194416 0.46496439 0.519276 0.56542456 0.59847283 0.59612906 0.55017835 0.4782671 0.40385324 0.340218 0.28786275 0.2415905 0.19194598][0.29842496 0.33344889 0.37372956 0.41183364 0.43978152 0.45750251 0.46119863 0.43291977 0.37158704 0.30393964 0.25499094 0.23152487 0.22165067 0.21016517 0.18372098][0.2786544 0.30452809 0.3323468 0.35485145 0.36537889 0.36404917 0.34788942 0.302788 0.23212139 0.16747335 0.13365199 0.13264793 0.14574353 0.15320213 0.14062482][0.20049177 0.22088827 0.24292399 0.25824779 0.2617394 0.25384155 0.23209152 0.18506795 0.11714131 0.058471471 0.031214135 0.035407949 0.052344132 0.063867912 0.057742007][0.071083412 0.084069327 0.10049304 0.11135102 0.11336418 0.10683917 0.089735933 0.053022061 5.3338052e-05 -0.045602959 -0.0670243 -0.064534962 -0.052145761 -0.042689107 -0.044228271][-0.061601322 -0.058373321 -0.049299076 -0.042878158 -0.040743411 -0.043222737 -0.051932838 -0.073362522 -0.10555498 -0.13333687 -0.14614968 -0.1441796 -0.13589811 -0.12843803 -0.12577319]]...]
INFO - root - 2017-12-11 02:45:37.026440: step 79910, loss = 0.70, batch loss = 0.64 (18.4 examples/sec; 0.434 sec/batch; 30h:26m:12s remains)
INFO - root - 2017-12-11 02:45:41.565454: step 79920, loss = 0.70, batch loss = 0.65 (17.8 examples/sec; 0.448 sec/batch; 31h:27m:25s remains)
INFO - root - 2017-12-11 02:45:46.015986: step 79930, loss = 0.72, batch loss = 0.66 (17.8 examples/sec; 0.450 sec/batch; 31h:32m:39s remains)
INFO - root - 2017-12-11 02:45:50.497437: step 79940, loss = 0.70, batch loss = 0.64 (16.9 examples/sec; 0.474 sec/batch; 33h:13m:56s remains)
INFO - root - 2017-12-11 02:45:54.954637: step 79950, loss = 0.70, batch loss = 0.64 (17.4 examples/sec; 0.459 sec/batch; 32h:13m:14s remains)
INFO - root - 2017-12-11 02:45:59.383434: step 79960, loss = 0.70, batch loss = 0.64 (17.9 examples/sec; 0.446 sec/batch; 31h:16m:49s remains)
INFO - root - 2017-12-11 02:46:03.822284: step 79970, loss = 0.68, batch loss = 0.62 (18.2 examples/sec; 0.439 sec/batch; 30h:48m:23s remains)
INFO - root - 2017-12-11 02:46:08.233767: step 79980, loss = 0.69, batch loss = 0.63 (17.9 examples/sec; 0.446 sec/batch; 31h:17m:31s remains)
INFO - root - 2017-12-11 02:46:12.714819: step 79990, loss = 0.69, batch loss = 0.63 (17.6 examples/sec; 0.454 sec/batch; 31h:50m:13s remains)
INFO - root - 2017-12-11 02:46:17.166840: step 80000, loss = 0.70, batch loss = 0.65 (17.9 examples/sec; 0.448 sec/batch; 31h:23m:30s remains)
2017-12-11 02:46:17.691539: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.045826137 0.032307904 0.009451814 -0.015029439 -0.029700484 -0.026893564 -0.012009816 0.010426193 0.03867919 0.061328169 0.066866294 0.050778449 0.025558148 -0.0034148628 -0.034879617][0.12956779 0.12029146 0.0963111 0.068415731 0.053128328 0.063378081 0.094216935 0.13560165 0.17470145 0.19465464 0.18532324 0.14501566 0.08954192 0.0302818 -0.022057751][0.23999119 0.23943096 0.21891464 0.19083226 0.1784326 0.20319217 0.26175913 0.33261019 0.38480192 0.39578426 0.36093128 0.28392312 0.18314618 0.078116395 -0.0060117114][0.35481972 0.3671425 0.357683 0.33827728 0.33801624 0.38614988 0.48175263 0.58856052 0.65336061 0.650843 0.58539909 0.46656618 0.31179398 0.14935462 0.022955315][0.4415293 0.47632548 0.49562338 0.505765 0.53548414 0.61534339 0.747295 0.88234532 0.94736445 0.91972643 0.81572992 0.65253884 0.44425008 0.22568351 0.057422861][0.47565958 0.54619229 0.61648464 0.68075031 0.7579245 0.87126255 1.0255855 1.165989 1.2097208 1.1414334 0.9897812 0.78124136 0.5294131 0.27288175 0.079186223][0.46546572 0.58039153 0.712303 0.83877307 0.96096396 1.0907148 1.2362795 1.3473682 1.3465816 1.2285433 1.031721 0.78962356 0.51886863 0.25830331 0.068707295][0.42812207 0.57632363 0.75053227 0.91417193 1.0511862 1.1644149 1.2675765 1.3239568 1.2726612 1.1165547 0.89654249 0.6490562 0.3947663 0.1689834 0.01557042][0.36125028 0.5140574 0.68972939 0.84742081 0.96107668 1.0300775 1.0738013 1.0732789 0.98894757 0.82736713 0.62340337 0.41016793 0.20841262 0.045982927 -0.052204393][0.26050618 0.3856208 0.52375078 0.64066863 0.70962954 0.72968876 0.72142345 0.679845 0.585992 0.44813362 0.29197627 0.14254782 0.014827775 -0.073654257 -0.11356391][0.14456373 0.22020325 0.29827845 0.35787043 0.37925616 0.36303627 0.32598817 0.27042189 0.19172618 0.097519964 0.0033654482 -0.075863667 -0.13262683 -0.15855293 -0.15379761][0.037473314 0.062375423 0.0832751 0.09224271 0.079370037 0.046871666 0.005840939 -0.039157838 -0.086606242 -0.13250901 -0.17076284 -0.19483219 -0.20233396 -0.19067626 -0.16361026][-0.045497347 -0.054908227 -0.069142871 -0.088385135 -0.11447364 -0.14480601 -0.17351392 -0.19732679 -0.21433595 -0.22473656 -0.22832839 -0.22282018 -0.20752689 -0.18250416 -0.15183665][-0.09394832 -0.11614338 -0.14054194 -0.16560936 -0.1893357 -0.20939568 -0.22311763 -0.22910775 -0.22709407 -0.21987605 -0.20952645 -0.19534789 -0.1766274 -0.1540767 -0.13065375][-0.10737773 -0.12746771 -0.14676376 -0.16528651 -0.1804131 -0.19060411 -0.19489715 -0.192933 -0.18519379 -0.17470795 -0.16375966 -0.15236637 -0.13961409 -0.1257655 -0.11230695]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-80000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-80000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 02:46:23.112312: step 80010, loss = 0.68, batch loss = 0.63 (17.2 examples/sec; 0.464 sec/batch; 32h:31m:54s remains)
INFO - root - 2017-12-11 02:46:27.637002: step 80020, loss = 0.71, batch loss = 0.65 (17.9 examples/sec; 0.448 sec/batch; 31h:23m:42s remains)
INFO - root - 2017-12-11 02:46:32.237103: step 80030, loss = 0.68, batch loss = 0.62 (17.4 examples/sec; 0.461 sec/batch; 32h:17m:58s remains)
INFO - root - 2017-12-11 02:46:36.772092: step 80040, loss = 0.70, batch loss = 0.65 (17.9 examples/sec; 0.447 sec/batch; 31h:21m:41s remains)
INFO - root - 2017-12-11 02:46:41.305902: step 80050, loss = 0.70, batch loss = 0.64 (17.6 examples/sec; 0.454 sec/batch; 31h:50m:48s remains)
INFO - root - 2017-12-11 02:46:45.834364: step 80060, loss = 0.69, batch loss = 0.63 (17.9 examples/sec; 0.447 sec/batch; 31h:22m:38s remains)
INFO - root - 2017-12-11 02:46:50.350745: step 80070, loss = 0.71, batch loss = 0.65 (17.2 examples/sec; 0.466 sec/batch; 32h:42m:12s remains)
INFO - root - 2017-12-11 02:46:54.913399: step 80080, loss = 0.69, batch loss = 0.63 (17.9 examples/sec; 0.448 sec/batch; 31h:23m:04s remains)
INFO - root - 2017-12-11 02:46:59.344551: step 80090, loss = 0.70, batch loss = 0.64 (17.9 examples/sec; 0.448 sec/batch; 31h:23m:39s remains)
INFO - root - 2017-12-11 02:47:03.881482: step 80100, loss = 0.69, batch loss = 0.63 (17.6 examples/sec; 0.454 sec/batch; 31h:49m:54s remains)
2017-12-11 02:47:04.488531: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21555135 0.23290299 0.24840674 0.26587331 0.27354965 0.2671119 0.2432967 0.20924976 0.17011198 0.13237856 0.11292779 0.11510553 0.13439302 0.15505539 0.15686767][0.25772414 0.28042224 0.30411792 0.33428729 0.35532206 0.3586984 0.33575362 0.29401791 0.24253821 0.19183514 0.16337 0.16045015 0.17664546 0.19340721 0.18864152][0.28364661 0.31010449 0.33908778 0.37813073 0.41146344 0.42790109 0.41274431 0.36964059 0.30913666 0.24742801 0.21016517 0.20159863 0.21257268 0.22316173 0.21208236][0.30506235 0.33690381 0.37158722 0.41743314 0.46018505 0.48936269 0.48579305 0.44690141 0.38072693 0.308403 0.2609444 0.24551952 0.25058338 0.254788 0.23846027][0.32372993 0.36185491 0.40400583 0.45663258 0.50609243 0.54419887 0.550782 0.51811117 0.4491998 0.36746106 0.30964798 0.28774703 0.28919789 0.2897898 0.27063784][0.34150586 0.38297898 0.43003309 0.48617244 0.53702885 0.57630348 0.58695215 0.55899113 0.49088648 0.40396395 0.33818668 0.31145602 0.31187448 0.31341082 0.29611641][0.35370019 0.39192727 0.43825924 0.49309263 0.53969079 0.57209474 0.58003956 0.55601019 0.49564657 0.41353908 0.34681082 0.31762135 0.31690431 0.32072774 0.30802706][0.3398428 0.36984771 0.4117344 0.46358967 0.50465316 0.52672738 0.5262447 0.50268251 0.4531585 0.38423207 0.3249177 0.29731563 0.29628593 0.30262667 0.29516727][0.29273623 0.31364563 0.35075802 0.40071487 0.43949783 0.45414561 0.44391188 0.41549355 0.37179297 0.31624421 0.2690931 0.24820311 0.25064567 0.26110846 0.25900784][0.21495111 0.22514176 0.25421914 0.29895398 0.33555615 0.34683591 0.33117321 0.29875109 0.25798103 0.21305935 0.17810175 0.16633561 0.17487618 0.1904321 0.19344819][0.12042429 0.1194354 0.13609523 0.16880549 0.19819136 0.20715235 0.19265635 0.16385397 0.13060936 0.097632974 0.074643433 0.0709368 0.0835337 0.10153132 0.10798842][0.025121495 0.017443232 0.02286467 0.040785439 0.0587222 0.06407439 0.054083437 0.035101146 0.014925885 -0.0029797098 -0.013289336 -0.01095581 0.0013650609 0.016494244 0.0229184][-0.056899935 -0.0647954 -0.064448096 -0.0571225 -0.049140751 -0.047597956 -0.053998314 -0.0642659 -0.073261917 -0.07886821 -0.079250783 -0.073153935 -0.063116468 -0.053103145 -0.048727736][-0.1062508 -0.11153446 -0.11123724 -0.10791449 -0.10451636 -0.10469212 -0.10907023 -0.11509933 -0.11937351 -0.12004683 -0.11672407 -0.11005831 -0.1026032 -0.096617647 -0.09399461][-0.11473891 -0.11893482 -0.11852743 -0.11649251 -0.11423197 -0.11385354 -0.1160714 -0.11978158 -0.12297168 -0.12392163 -0.12208925 -0.11799726 -0.11346221 -0.10990842 -0.10800891]]...]
INFO - root - 2017-12-11 02:47:08.984431: step 80110, loss = 0.70, batch loss = 0.64 (17.8 examples/sec; 0.450 sec/batch; 31h:32m:48s remains)
INFO - root - 2017-12-11 02:47:13.498256: step 80120, loss = 0.71, batch loss = 0.65 (17.5 examples/sec; 0.457 sec/batch; 32h:03m:00s remains)
INFO - root - 2017-12-11 02:47:17.993147: step 80130, loss = 0.69, batch loss = 0.63 (17.0 examples/sec; 0.472 sec/batch; 33h:04m:34s remains)
INFO - root - 2017-12-11 02:47:22.506829: step 80140, loss = 0.71, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 31h:51m:00s remains)
INFO - root - 2017-12-11 02:47:27.151249: step 80150, loss = 0.71, batch loss = 0.65 (16.8 examples/sec; 0.477 sec/batch; 33h:26m:45s remains)
INFO - root - 2017-12-11 02:47:31.716598: step 80160, loss = 0.71, batch loss = 0.65 (18.1 examples/sec; 0.442 sec/batch; 31h:00m:41s remains)
INFO - root - 2017-12-11 02:47:36.219086: step 80170, loss = 0.70, batch loss = 0.64 (17.5 examples/sec; 0.458 sec/batch; 32h:07m:26s remains)
INFO - root - 2017-12-11 02:47:40.802723: step 80180, loss = 0.68, batch loss = 0.63 (17.4 examples/sec; 0.460 sec/batch; 32h:14m:50s remains)
INFO - root - 2017-12-11 02:47:45.397182: step 80190, loss = 0.68, batch loss = 0.63 (17.2 examples/sec; 0.464 sec/batch; 32h:32m:41s remains)
INFO - root - 2017-12-11 02:47:49.977179: step 80200, loss = 0.69, batch loss = 0.63 (17.5 examples/sec; 0.457 sec/batch; 32h:01m:25s remains)
2017-12-11 02:47:50.503413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.048353571 -0.046281908 -0.042805385 -0.04001436 -0.039352134 -0.042328171 -0.045528635 -0.046030454 -0.043678585 -0.0400005 -0.03716198 -0.035980541 -0.037800118 -0.040798992 -0.041601587][-0.043592736 -0.039366879 -0.033960022 -0.02941829 -0.026670631 -0.025917696 -0.022727847 -0.016123718 -0.00881292 -0.0042879307 -0.0049524852 -0.011568034 -0.023646474 -0.035920043 -0.042728204][-0.018789783 -0.010351249 -0.00095966342 0.0063889651 0.011432333 0.016902529 0.028957533 0.045023128 0.058381222 0.063684344 0.058430444 0.041635025 0.015654474 -0.010369846 -0.027825404][0.034725342 0.04985531 0.06438674 0.073698334 0.0786063 0.086771205 0.10696869 0.13244209 0.15175831 0.15735681 0.14681543 0.11831391 0.075943351 0.032801837 0.00091358][0.10979203 0.13174847 0.15029737 0.15931559 0.16189192 0.17183316 0.19930319 0.23248115 0.25568211 0.25982636 0.24266656 0.20126149 0.14163105 0.08076366 0.033855051][0.18570332 0.21342461 0.2343919 0.24219786 0.24297285 0.25528935 0.28754714 0.32293788 0.34432828 0.34329 0.31802696 0.26467726 0.19100134 0.11680562 0.058701202][0.24902686 0.28109658 0.301868 0.30709362 0.30604121 0.31918019 0.35068026 0.38041034 0.39343637 0.38372415 0.3501178 0.28853029 0.20801681 0.12928733 0.0677625][0.29988819 0.33410779 0.35115913 0.35131869 0.34675586 0.35677296 0.38090298 0.39865011 0.39946932 0.37936392 0.33764157 0.27127224 0.19080207 0.11587878 0.058772244][0.337167 0.37060672 0.38228875 0.37753081 0.36971256 0.37402937 0.38602173 0.38810486 0.37468824 0.34299281 0.292846 0.22390705 0.14837705 0.083561808 0.036902949][0.3588044 0.38685584 0.39294872 0.38575715 0.37670171 0.37471816 0.37284607 0.35793886 0.32880405 0.28492704 0.22745192 0.1593603 0.093454376 0.043116212 0.010461335][0.3693271 0.38741979 0.38775581 0.38009396 0.37021345 0.36103961 0.34527558 0.31434554 0.2699106 0.21492945 0.1534919 0.09110669 0.039117571 0.0053467411 -0.012835][0.37196165 0.37671724 0.37008986 0.36070132 0.34759438 0.33040944 0.30301636 0.26025751 0.2049295 0.14379188 0.084252112 0.032462724 -0.0036258509 -0.022215525 -0.028729249][0.36160147 0.35353002 0.33963612 0.3263689 0.30797035 0.28322372 0.24761657 0.19864848 0.13964765 0.079987623 0.028512964 -0.0098101581 -0.031203 -0.038425714 -0.037594333][0.33669508 0.31994578 0.29961056 0.28122118 0.2565597 0.22454138 0.18311042 0.13237797 0.076522492 0.025289498 -0.013640793 -0.03772407 -0.047031712 -0.046488266 -0.041562196][0.31067419 0.29034925 0.26391396 0.2377402 0.2041136 0.1632605 0.11680244 0.067630962 0.020016335 -0.018395353 -0.042848207 -0.053892441 -0.054219905 -0.049081981 -0.04240191]]...]
INFO - root - 2017-12-11 02:47:55.021055: step 80210, loss = 0.71, batch loss = 0.65 (17.9 examples/sec; 0.447 sec/batch; 31h:19m:27s remains)
INFO - root - 2017-12-11 02:47:59.598846: step 80220, loss = 0.69, batch loss = 0.63 (17.6 examples/sec; 0.454 sec/batch; 31h:49m:15s remains)
INFO - root - 2017-12-11 02:48:04.058508: step 80230, loss = 0.71, batch loss = 0.65 (18.6 examples/sec; 0.430 sec/batch; 30h:06m:08s remains)
INFO - root - 2017-12-11 02:48:08.481656: step 80240, loss = 0.70, batch loss = 0.64 (18.9 examples/sec; 0.422 sec/batch; 29h:35m:50s remains)
INFO - root - 2017-12-11 02:48:12.942851: step 80250, loss = 0.70, batch loss = 0.64 (17.7 examples/sec; 0.453 sec/batch; 31h:44m:04s remains)
INFO - root - 2017-12-11 02:48:17.375883: step 80260, loss = 0.68, batch loss = 0.62 (18.9 examples/sec; 0.422 sec/batch; 29h:34m:59s remains)
INFO - root - 2017-12-11 02:48:21.876404: step 80270, loss = 0.70, batch loss = 0.64 (17.3 examples/sec; 0.463 sec/batch; 32h:24m:59s remains)
INFO - root - 2017-12-11 02:48:26.337127: step 80280, loss = 0.70, batch loss = 0.64 (17.9 examples/sec; 0.447 sec/batch; 31h:20m:03s remains)
INFO - root - 2017-12-11 02:48:30.879386: step 80290, loss = 0.69, batch loss = 0.63 (17.7 examples/sec; 0.451 sec/batch; 31h:34m:59s remains)
INFO - root - 2017-12-11 02:48:35.288742: step 80300, loss = 0.68, batch loss = 0.62 (17.9 examples/sec; 0.447 sec/batch; 31h:19m:02s remains)
2017-12-11 02:48:35.863758: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28520253 0.27332979 0.27434337 0.28504497 0.30963635 0.34363827 0.37045991 0.376201 0.35423714 0.31201375 0.24620898 0.15877259 0.068684116 -0.002368866 -0.042309236][0.18565914 0.18229359 0.19504271 0.2165008 0.24759886 0.28346235 0.30942044 0.31461892 0.29429305 0.2555953 0.19518836 0.11649668 0.037361208 -0.023554662 -0.056182172][0.087368123 0.095341027 0.12389682 0.1614805 0.20412706 0.24523237 0.27111927 0.27386174 0.25077116 0.21008082 0.1503325 0.077072941 0.0067787324 -0.045232709 -0.070767611][0.030454459 0.052133586 0.099287756 0.15640418 0.21449931 0.26446897 0.29200754 0.29076973 0.25967023 0.20962419 0.14152156 0.063624308 -0.0079337237 -0.059503172 -0.083143987][0.018302597 0.058001302 0.12636268 0.20374773 0.27737856 0.33695257 0.36732212 0.36210066 0.3209554 0.25771546 0.17531911 0.083556227 -0.00020907975 -0.061019298 -0.089403205][0.041630022 0.099764563 0.18683055 0.2806578 0.36612061 0.4331975 0.46648085 0.45898205 0.409956 0.33443165 0.2367367 0.12742364 0.026302569 -0.048721611 -0.086157158][0.080890894 0.15044504 0.24677359 0.3484709 0.4392117 0.50915873 0.54413968 0.53765267 0.48661733 0.40449062 0.2959227 0.17267235 0.057165302 -0.030494507 -0.077128172][0.1234932 0.19219093 0.2833623 0.37994564 0.46564558 0.530881 0.56370491 0.55929697 0.51172233 0.43116951 0.32150632 0.19476768 0.074684612 -0.018249191 -0.070055977][0.16686416 0.22497384 0.29955044 0.37956035 0.45028594 0.50293082 0.52851915 0.52471024 0.48343098 0.41124523 0.31055373 0.19171697 0.076986127 -0.014446847 -0.067785531][0.21274835 0.2573567 0.31142664 0.36972773 0.42073438 0.45730704 0.47351328 0.46899202 0.43501362 0.37545803 0.28984988 0.18449986 0.078558013 -0.0098810429 -0.064417124][0.259467 0.29295561 0.32899311 0.36720324 0.40013906 0.42296174 0.43150702 0.42565846 0.39698806 0.34826627 0.27537507 0.1805809 0.081319451 -0.0040746005 -0.0583969][0.2934095 0.32158136 0.34709427 0.37265378 0.39492288 0.41158986 0.41825202 0.41277951 0.38663688 0.34312233 0.27538532 0.18373528 0.08631286 0.0029045411 -0.050028261][0.29903036 0.32595193 0.3486374 0.36972818 0.38908631 0.40674594 0.41688603 0.41407806 0.38936082 0.3472859 0.28008467 0.18822022 0.091409609 0.010658707 -0.039901193][0.26534641 0.29186574 0.31570184 0.33728865 0.35847402 0.38052186 0.39591822 0.39625666 0.37243849 0.33045796 0.2640152 0.17520569 0.084155262 0.010949997 -0.033992693][0.19374672 0.21644723 0.23941036 0.26103076 0.28401965 0.30960932 0.32872036 0.33097169 0.30799019 0.26722628 0.20575963 0.1276568 0.051269732 -0.0067637675 -0.040490344]]...]
INFO - root - 2017-12-11 02:48:40.409641: step 80310, loss = 0.70, batch loss = 0.65 (17.4 examples/sec; 0.459 sec/batch; 32h:08m:18s remains)
INFO - root - 2017-12-11 02:48:45.043321: step 80320, loss = 0.69, batch loss = 0.63 (17.6 examples/sec; 0.454 sec/batch; 31h:48m:20s remains)
INFO - root - 2017-12-11 02:48:49.573006: step 80330, loss = 0.69, batch loss = 0.63 (17.4 examples/sec; 0.459 sec/batch; 32h:09m:28s remains)
INFO - root - 2017-12-11 02:48:54.072853: step 80340, loss = 0.70, batch loss = 0.64 (17.6 examples/sec; 0.454 sec/batch; 31h:48m:43s remains)
INFO - root - 2017-12-11 02:48:58.628078: step 80350, loss = 0.71, batch loss = 0.65 (18.4 examples/sec; 0.435 sec/batch; 30h:27m:20s remains)
INFO - root - 2017-12-11 02:49:03.008798: step 80360, loss = 0.69, batch loss = 0.63 (18.0 examples/sec; 0.446 sec/batch; 31h:12m:12s remains)
INFO - root - 2017-12-11 02:49:07.440540: step 80370, loss = 0.71, batch loss = 0.65 (18.7 examples/sec; 0.428 sec/batch; 29h:58m:07s remains)
INFO - root - 2017-12-11 02:49:11.942031: step 80380, loss = 0.70, batch loss = 0.64 (18.3 examples/sec; 0.438 sec/batch; 30h:39m:55s remains)
INFO - root - 2017-12-11 02:49:16.414582: step 80390, loss = 0.69, batch loss = 0.63 (18.4 examples/sec; 0.434 sec/batch; 30h:22m:18s remains)
INFO - root - 2017-12-11 02:49:20.907711: step 80400, loss = 0.69, batch loss = 0.63 (18.4 examples/sec; 0.434 sec/batch; 30h:23m:00s remains)
2017-12-11 02:49:21.479481: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20864148 0.21881709 0.22135903 0.21780616 0.21272017 0.21045013 0.2096881 0.2110088 0.21330608 0.21278566 0.20452777 0.19235893 0.18215443 0.17234434 0.15737823][0.34045509 0.35941663 0.36299172 0.35540116 0.34763163 0.34634662 0.34761134 0.350207 0.35107017 0.34478933 0.32751504 0.30821872 0.29673174 0.29031116 0.27628538][0.46491978 0.49432638 0.49732479 0.48255715 0.46891695 0.46590176 0.4669089 0.46739498 0.46176186 0.44483143 0.41694322 0.39381856 0.38780406 0.39331722 0.38954708][0.54542512 0.58052057 0.58019304 0.55805784 0.53966779 0.53646791 0.53873 0.53702432 0.52313405 0.49451253 0.45844045 0.43753552 0.44490772 0.47134829 0.48686656][0.57328135 0.60991573 0.60381889 0.57550466 0.55603123 0.55743784 0.56551725 0.5647471 0.54491717 0.50782651 0.46788687 0.45291117 0.4750638 0.52146584 0.55375][0.55822247 0.59459829 0.5840708 0.55422592 0.53949493 0.55012053 0.56607682 0.56564552 0.53863335 0.49302793 0.44954005 0.43869346 0.47010574 0.52758253 0.56789589][0.51242524 0.5468806 0.5348174 0.50828445 0.50173748 0.52215689 0.54467314 0.543273 0.50965285 0.4572233 0.410486 0.40010867 0.433033 0.4916622 0.53304738][0.45555308 0.48805851 0.47665951 0.45428687 0.45315742 0.47928315 0.50645852 0.50720543 0.47393808 0.42113185 0.37245777 0.35637325 0.38010803 0.42876834 0.46510151][0.40596107 0.44029263 0.43420577 0.41814429 0.42106894 0.45064107 0.48243043 0.48965853 0.46300817 0.41395962 0.36285537 0.33573422 0.34300768 0.37497196 0.40147853][0.36983755 0.41165793 0.41636574 0.40966478 0.41806734 0.45164907 0.48888579 0.50422668 0.48455173 0.43704244 0.37949997 0.33737457 0.32556811 0.33923236 0.35456878][0.33617443 0.38861176 0.40791908 0.41290864 0.42756066 0.46372402 0.50395161 0.52486145 0.50929403 0.46026132 0.39443144 0.33850735 0.31241786 0.31546932 0.32862309][0.29763383 0.35782528 0.39023739 0.40616012 0.42623532 0.46365494 0.50626588 0.53360075 0.52478844 0.47846 0.41021213 0.34850159 0.31802744 0.3207261 0.34029928][0.25363332 0.31286979 0.35133654 0.37445015 0.39971468 0.44127762 0.49166396 0.53215003 0.53792471 0.50329947 0.44262129 0.38508779 0.35826728 0.3646141 0.38908136][0.2041768 0.25431952 0.29154941 0.31798521 0.34808466 0.39578074 0.45662192 0.51274759 0.53706843 0.52117187 0.47746846 0.43301132 0.41383767 0.42167324 0.44335768][0.16884828 0.20544803 0.23538294 0.26064858 0.29253057 0.34294528 0.40902731 0.47435147 0.5130282 0.51605415 0.49275276 0.46484005 0.45271894 0.4580096 0.47151682]]...]
INFO - root - 2017-12-11 02:49:25.966254: step 80410, loss = 0.71, batch loss = 0.66 (17.9 examples/sec; 0.448 sec/batch; 31h:22m:49s remains)
INFO - root - 2017-12-11 02:49:30.472788: step 80420, loss = 0.70, batch loss = 0.64 (17.7 examples/sec; 0.451 sec/batch; 31h:35m:26s remains)
INFO - root - 2017-12-11 02:49:35.023986: step 80430, loss = 0.70, batch loss = 0.64 (17.9 examples/sec; 0.448 sec/batch; 31h:20m:08s remains)
INFO - root - 2017-12-11 02:49:39.558414: step 80440, loss = 0.71, batch loss = 0.65 (17.5 examples/sec; 0.457 sec/batch; 32h:01m:19s remains)
INFO - root - 2017-12-11 02:49:44.026691: step 80450, loss = 0.71, batch loss = 0.66 (17.6 examples/sec; 0.455 sec/batch; 31h:49m:33s remains)
INFO - root - 2017-12-11 02:49:48.564679: step 80460, loss = 0.68, batch loss = 0.62 (17.8 examples/sec; 0.451 sec/batch; 31h:32m:35s remains)
INFO - root - 2017-12-11 02:49:53.153183: step 80470, loss = 0.69, batch loss = 0.63 (17.3 examples/sec; 0.461 sec/batch; 32h:17m:00s remains)
INFO - root - 2017-12-11 02:49:57.661348: step 80480, loss = 0.71, batch loss = 0.66 (17.4 examples/sec; 0.461 sec/batch; 32h:16m:23s remains)
INFO - root - 2017-12-11 02:50:02.259632: step 80490, loss = 0.71, batch loss = 0.66 (17.4 examples/sec; 0.460 sec/batch; 32h:12m:56s remains)
INFO - root - 2017-12-11 02:50:06.778229: step 80500, loss = 0.69, batch loss = 0.63 (17.5 examples/sec; 0.456 sec/batch; 31h:54m:48s remains)
2017-12-11 02:50:07.400325: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40670472 0.39632815 0.35421628 0.28153211 0.19344284 0.11755251 0.068999641 0.044615444 0.037059739 0.042687297 0.050941493 0.054881029 0.052773226 0.046144884 0.03682702][0.46078563 0.44287297 0.38417318 0.29370546 0.19129264 0.10388692 0.046625335 0.017297456 0.0088945543 0.016406853 0.029971261 0.043245398 0.053261783 0.057808261 0.055498049][0.45135522 0.42988226 0.36623415 0.27471346 0.17712513 0.096949346 0.046849284 0.024032975 0.020879595 0.030976832 0.045811191 0.060345151 0.072934143 0.080203235 0.0794072][0.38483143 0.36673892 0.31385419 0.2410666 0.16865864 0.11472994 0.086914614 0.080268957 0.086470857 0.098063588 0.10866192 0.11518079 0.11888611 0.11875982 0.11206795][0.28367457 0.27803168 0.25341004 0.21973264 0.19113992 0.17852512 0.1823038 0.19416393 0.2061884 0.2116652 0.20785309 0.19504881 0.17889591 0.16266392 0.14427403][0.18863572 0.20309393 0.21868205 0.23560126 0.25783578 0.28884304 0.32190359 0.3457517 0.35443309 0.3441419 0.31664744 0.2761921 0.2331475 0.19553547 0.16261178][0.13988255 0.17341337 0.22610377 0.28740078 0.350625 0.41414633 0.46647295 0.49365258 0.49142739 0.45985293 0.40508968 0.33439052 0.26225904 0.20189212 0.15498045][0.14174229 0.18476225 0.25941646 0.3463639 0.4314791 0.51180464 0.57321644 0.59942579 0.58565265 0.53477043 0.45591348 0.35796383 0.25912136 0.17839102 0.12162738][0.1665951 0.20860468 0.28524935 0.37541664 0.46303907 0.54638857 0.61033082 0.63709855 0.61919534 0.55992156 0.46855208 0.35426378 0.23782526 0.14317027 0.08124198][0.17521095 0.2058205 0.26868 0.34575528 0.42290628 0.50027329 0.56310052 0.59316421 0.58006597 0.52611113 0.43867636 0.32597059 0.20924392 0.11441103 0.055642154][0.14298075 0.15707327 0.19862683 0.25483704 0.31466547 0.37954414 0.43689269 0.47008851 0.46695024 0.42835706 0.35943624 0.2668263 0.1698658 0.092853568 0.049772449][0.071367957 0.068626322 0.088458829 0.12252381 0.16291255 0.21129802 0.25847965 0.29106161 0.29678884 0.27564234 0.23145033 0.16930991 0.10489964 0.058193482 0.040101085][-0.0048537562 -0.016695416 -0.012724305 0.0016444722 0.021838387 0.050011627 0.081438594 0.10711303 0.1164671 0.10858954 0.086172909 0.052977365 0.020840602 0.0042769369 0.010015151][-0.025621267 -0.032085694 -0.034575861 -0.038208473 -0.04380494 -0.046075884 -0.041669916 -0.033059046 -0.027584417 -0.028308924 -0.035445567 -0.047384579 -0.056363277 -0.05299167 -0.034239255][0.034625094 0.045018382 0.042203669 0.020626538 -0.016565342 -0.058072839 -0.090064168 -0.10703899 -0.11239044 -0.11194836 -0.11038085 -0.10921445 -0.10501122 -0.09268225 -0.071773581]]...]
INFO - root - 2017-12-11 02:50:11.808810: step 80510, loss = 0.70, batch loss = 0.64 (23.5 examples/sec; 0.341 sec/batch; 23h:51m:46s remains)
INFO - root - 2017-12-11 02:50:14.555765: step 80520, loss = 0.70, batch loss = 0.64 (31.6 examples/sec; 0.253 sec/batch; 17h:43m:48s remains)
INFO - root - 2017-12-11 02:50:17.165331: step 80530, loss = 0.71, batch loss = 0.65 (31.3 examples/sec; 0.256 sec/batch; 17h:53m:17s remains)
INFO - root - 2017-12-11 02:50:19.837196: step 80540, loss = 0.72, batch loss = 0.66 (31.1 examples/sec; 0.257 sec/batch; 17h:59m:37s remains)
INFO - root - 2017-12-11 02:50:22.512694: step 80550, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 18h:32m:47s remains)
INFO - root - 2017-12-11 02:50:25.142244: step 80560, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 18h:09m:54s remains)
INFO - root - 2017-12-11 02:50:27.761539: step 80570, loss = 0.68, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 18h:15m:06s remains)
INFO - root - 2017-12-11 02:50:30.390630: step 80580, loss = 0.69, batch loss = 0.64 (30.0 examples/sec; 0.266 sec/batch; 18h:37m:59s remains)
INFO - root - 2017-12-11 02:50:33.029821: step 80590, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 18h:33m:26s remains)
INFO - root - 2017-12-11 02:50:35.633828: step 80600, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.259 sec/batch; 18h:09m:11s remains)
2017-12-11 02:50:36.003032: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2279731 0.25449181 0.27499178 0.28235194 0.27438027 0.25085202 0.22555634 0.20846415 0.20015992 0.20007911 0.20405133 0.21789131 0.2412989 0.25032157 0.2322076][0.24513154 0.2702907 0.29093024 0.30172879 0.29748264 0.27518296 0.24979872 0.23176487 0.2207889 0.21613012 0.2156897 0.22776274 0.25415751 0.26797417 0.25458515][0.25224972 0.27459347 0.29500502 0.30987728 0.31035292 0.29037806 0.26593786 0.24842821 0.23642109 0.22987235 0.22810809 0.23985802 0.2685166 0.28664461 0.27728829][0.25382173 0.27184674 0.28923333 0.3038666 0.30610603 0.2891477 0.268586 0.25569654 0.24711075 0.24354786 0.24382836 0.25714472 0.28966215 0.31412703 0.30979747][0.24429743 0.25649202 0.26885921 0.28083783 0.28425008 0.27331308 0.26122949 0.25809303 0.25783986 0.26040712 0.26262292 0.27543569 0.30870375 0.33647832 0.33400813][0.23015146 0.23532127 0.24009469 0.24671781 0.24975629 0.24513255 0.24278578 0.25150141 0.26258877 0.27317366 0.27824295 0.28992021 0.32056767 0.34583318 0.33952007][0.21680234 0.2139661 0.20959044 0.20964594 0.21230702 0.21400172 0.22003663 0.23729314 0.25636548 0.27238429 0.27950478 0.28964028 0.31619343 0.33656234 0.32600671][0.20372291 0.19076575 0.17696214 0.17191538 0.17620608 0.1853925 0.19867395 0.22051559 0.24338755 0.26299545 0.27349955 0.28436792 0.30797389 0.32301417 0.30833098][0.193273 0.17229612 0.15179145 0.14373775 0.1492082 0.16305564 0.18020679 0.2033231 0.22726902 0.24907361 0.26373965 0.27815762 0.3019869 0.31436151 0.29853252][0.19402809 0.17550765 0.15734571 0.15114696 0.15793607 0.17262498 0.18898484 0.20958811 0.23090225 0.25061333 0.26550063 0.2810415 0.30366406 0.31294975 0.29537877][0.21971069 0.21755567 0.20976658 0.20839268 0.21648166 0.22941181 0.24121816 0.25650644 0.27235743 0.28573236 0.29598713 0.30954659 0.33089644 0.33828446 0.31790379][0.2506085 0.26908419 0.27453768 0.27939624 0.29103467 0.30364197 0.3101885 0.31864893 0.3265121 0.33007064 0.331732 0.34170988 0.36370516 0.37239254 0.35010546][0.26529938 0.29781342 0.31221768 0.32200125 0.33863387 0.35307619 0.35642624 0.35926905 0.35930103 0.35306934 0.34649903 0.35426936 0.37804702 0.38826898 0.36334202][0.25234255 0.28730518 0.30234537 0.31142104 0.32814491 0.34115997 0.34263614 0.34448794 0.34285805 0.33322939 0.322831 0.32851377 0.35012877 0.35770863 0.32970822][0.200935 0.22911683 0.23922284 0.24427477 0.25748426 0.26687521 0.26754102 0.27161211 0.27324378 0.26590791 0.25530109 0.25714758 0.27154723 0.27329046 0.2445811]]...]
INFO - root - 2017-12-11 02:50:38.661790: step 80610, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 18h:09m:59s remains)
INFO - root - 2017-12-11 02:50:41.269928: step 80620, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 18h:15m:07s remains)
INFO - root - 2017-12-11 02:50:43.877358: step 80630, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 18h:20m:19s remains)
INFO - root - 2017-12-11 02:50:46.483232: step 80640, loss = 0.69, batch loss = 0.63 (31.1 examples/sec; 0.257 sec/batch; 18h:00m:39s remains)
INFO - root - 2017-12-11 02:50:49.120188: step 80650, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 17h:59m:18s remains)
INFO - root - 2017-12-11 02:50:51.750623: step 80660, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 18h:30m:27s remains)
INFO - root - 2017-12-11 02:50:54.390901: step 80670, loss = 0.71, batch loss = 0.66 (30.0 examples/sec; 0.267 sec/batch; 18h:39m:30s remains)
INFO - root - 2017-12-11 02:50:57.030924: step 80680, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 18h:09m:21s remains)
INFO - root - 2017-12-11 02:50:59.657051: step 80690, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 18h:27m:29s remains)
INFO - root - 2017-12-11 02:51:02.296413: step 80700, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.264 sec/batch; 18h:25m:50s remains)
2017-12-11 02:51:02.705309: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2279771 0.21976832 0.18679477 0.14304961 0.10498221 0.08574491 0.087083 0.099585973 0.11232848 0.1260307 0.13616443 0.14399378 0.15314454 0.16943085 0.20259723][0.225797 0.23100588 0.21206804 0.17871302 0.14592966 0.12922543 0.13486384 0.15543476 0.1770121 0.19706742 0.20604473 0.20540519 0.20083572 0.1994514 0.21346137][0.23423143 0.25467834 0.25052854 0.22718802 0.19786416 0.1830119 0.19471264 0.22802471 0.26452863 0.295336 0.30496022 0.29412112 0.27011904 0.24311328 0.23013707][0.25983974 0.2933028 0.3001917 0.28286189 0.25355089 0.23931172 0.25798616 0.30595151 0.35966018 0.40247023 0.41486427 0.39613408 0.35498956 0.30548444 0.27045491][0.28726232 0.32905927 0.34266084 0.32865316 0.29872486 0.28554016 0.30999231 0.3677521 0.43208712 0.48122707 0.49598911 0.47443876 0.4271805 0.37043393 0.33086261][0.303459 0.34775761 0.36355141 0.35108078 0.32142138 0.30938506 0.33501962 0.39188942 0.4541114 0.49961507 0.51438844 0.49590951 0.45690581 0.41243178 0.38874808][0.29883865 0.33944023 0.35310096 0.34066185 0.31228286 0.30041486 0.32095635 0.36551082 0.41343018 0.44715673 0.46002167 0.44973487 0.43009895 0.41198081 0.41608524][0.27196935 0.30423456 0.3135508 0.30089724 0.27496949 0.26237953 0.27359211 0.29926541 0.32588741 0.34318507 0.351506 0.34938616 0.34940261 0.35720912 0.3854197][0.22279337 0.24398829 0.24779724 0.23525913 0.21306993 0.20047718 0.20322947 0.21250446 0.2207491 0.22460333 0.22845784 0.23152785 0.24306346 0.26451179 0.30106771][0.15489511 0.16571382 0.16513269 0.15382545 0.13651866 0.12545927 0.12344974 0.12352152 0.12165622 0.11848542 0.11909936 0.12325916 0.13624366 0.15714428 0.1864412][0.085585371 0.0906378 0.08867722 0.079768665 0.067393333 0.058701813 0.055279184 0.05254072 0.047974925 0.043004192 0.041616984 0.044072267 0.052152827 0.064076573 0.078663468][0.025559964 0.029204324 0.029038731 0.023831952 0.016346958 0.010555875 0.0074998462 0.0046736663 0.00046842196 -0.0041729324 -0.0066302256 -0.0064113457 -0.0039035594 -0.00078254286 0.001759173][-0.016285175 -0.012330456 -0.009674618 -0.011070578 -0.014431735 -0.017496888 -0.019614328 -0.021878751 -0.025264481 -0.029265638 -0.032217156 -0.033793289 -0.034995027 -0.03672995 -0.039610445][-0.039264455 -0.036713734 -0.033359013 -0.03276005 -0.033636395 -0.034778468 -0.035801522 -0.037111353 -0.0391644 -0.041703314 -0.043868996 -0.045591753 -0.047557041 -0.050022181 -0.053143892][-0.047146291 -0.047233529 -0.045044038 -0.044204894 -0.044183023 -0.044459928 -0.044845615 -0.045428276 -0.046340041 -0.047472704 -0.04850968 -0.049496911 -0.050733652 -0.052196138 -0.053850759]]...]
INFO - root - 2017-12-11 02:51:05.341045: step 80710, loss = 0.72, batch loss = 0.66 (31.2 examples/sec; 0.256 sec/batch; 17h:54m:31s remains)
INFO - root - 2017-12-11 02:51:08.000004: step 80720, loss = 0.70, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 18h:35m:21s remains)
INFO - root - 2017-12-11 02:51:10.620416: step 80730, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 18h:22m:38s remains)
INFO - root - 2017-12-11 02:51:13.204710: step 80740, loss = 0.69, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 18h:08m:00s remains)
INFO - root - 2017-12-11 02:51:15.830801: step 80750, loss = 0.69, batch loss = 0.64 (31.6 examples/sec; 0.253 sec/batch; 17h:42m:26s remains)
INFO - root - 2017-12-11 02:51:18.437464: step 80760, loss = 0.73, batch loss = 0.67 (28.9 examples/sec; 0.277 sec/batch; 19h:22m:36s remains)
INFO - root - 2017-12-11 02:51:21.076893: step 80770, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 19h:07m:57s remains)
INFO - root - 2017-12-11 02:51:23.709220: step 80780, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.261 sec/batch; 18h:15m:23s remains)
INFO - root - 2017-12-11 02:51:26.314839: step 80790, loss = 0.68, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 18h:25m:13s remains)
INFO - root - 2017-12-11 02:51:28.951193: step 80800, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 18h:08m:50s remains)
2017-12-11 02:51:29.335462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.057325657 -0.059995722 -0.060630925 -0.062329944 -0.063898459 -0.0638235 -0.061560679 -0.058309656 -0.05543749 -0.053527422 -0.052713621 -0.052785464 -0.053201165 -0.052869212 -0.05109952][-0.059565671 -0.061335508 -0.06165237 -0.064756058 -0.068307385 -0.068956226 -0.064952672 -0.05837765 -0.052128509 -0.047953509 -0.046942331 -0.048984949 -0.052604936 -0.055084769 -0.054527819][-0.044037361 -0.044269603 -0.044203363 -0.04847905 -0.053726055 -0.05464159 -0.048217952 -0.037563145 -0.027614294 -0.021769216 -0.022310743 -0.029335596 -0.04002532 -0.048967469 -0.052175518][0.00066133408 0.0021925555 0.0026209317 -0.0011214918 -0.005887216 -0.0055011963 0.0035107725 0.016988054 0.027712991 0.031011868 0.023949979 0.0066289962 -0.016241467 -0.035993967 -0.046074662][0.081911676 0.084313333 0.085234977 0.08588668 0.086450793 0.091249906 0.10209721 0.11375455 0.1176791 0.10967647 0.088110782 0.054212462 0.014940658 -0.018172503 -0.036797624][0.19086143 0.19131525 0.19326541 0.20351171 0.21595384 0.22886461 0.23949446 0.24180508 0.2280423 0.19757679 0.15303841 0.098263167 0.042115722 -0.0028491023 -0.0283976][0.29701963 0.29433629 0.29911825 0.3232474 0.35228279 0.37492421 0.38179222 0.36696547 0.32619596 0.26554438 0.19413608 0.11958574 0.051682521 0.0013304215 -0.025875894][0.35900381 0.3545723 0.364401 0.40315163 0.44850355 0.47907567 0.4787575 0.44328853 0.37437209 0.28556043 0.19254774 0.10646424 0.036690325 -0.0096366731 -0.031674791][0.34796602 0.34431896 0.35995772 0.40871939 0.46405059 0.49774477 0.48937345 0.43788028 0.35117537 0.2482197 0.14859426 0.06505432 0.0051966403 -0.028932199 -0.041108917][0.26589924 0.26470315 0.28458515 0.33460113 0.38963339 0.4204936 0.40662655 0.35028216 0.26382014 0.16724665 0.079627581 0.012857919 -0.028255487 -0.046179064 -0.047801606][0.14434406 0.14571182 0.16574042 0.20738536 0.252064 0.27538484 0.26074776 0.21264426 0.1443098 0.072380312 0.011236886 -0.0302797 -0.050256755 -0.053631626 -0.04771084][0.029578857 0.031675603 0.047214527 0.074845612 0.10434656 0.11913293 0.10822794 0.076724254 0.035470605 -0.0047097625 -0.035629634 -0.052378476 -0.0553283 -0.049644578 -0.040863179][-0.045247927 -0.045609415 -0.037309825 -0.02348282 -0.0077877603 0.0005209322 -0.00461555 -0.018807843 -0.035223585 -0.048721381 -0.056092348 -0.055429272 -0.048454057 -0.03937263 -0.031223474][-0.072529413 -0.075750947 -0.073967025 -0.07016591 -0.064359576 -0.060610969 -0.061443668 -0.063800126 -0.0646597 -0.062836915 -0.057674333 -0.048783258 -0.038387738 -0.029522009 -0.023367612][-0.065773845 -0.070146032 -0.072155476 -0.074041337 -0.074341848 -0.07399524 -0.0733349 -0.070583731 -0.06543225 -0.058717694 -0.050454263 -0.040611755 -0.031285159 -0.024433538 -0.020248415]]...]
INFO - root - 2017-12-11 02:51:31.941433: step 80810, loss = 0.69, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 18h:04m:07s remains)
INFO - root - 2017-12-11 02:51:34.606469: step 80820, loss = 0.72, batch loss = 0.66 (30.1 examples/sec; 0.266 sec/batch; 18h:33m:56s remains)
INFO - root - 2017-12-11 02:51:37.240920: step 80830, loss = 0.70, batch loss = 0.64 (28.8 examples/sec; 0.278 sec/batch; 19h:25m:35s remains)
INFO - root - 2017-12-11 02:51:39.850642: step 80840, loss = 0.71, batch loss = 0.65 (31.3 examples/sec; 0.256 sec/batch; 17h:52m:13s remains)
INFO - root - 2017-12-11 02:51:42.493791: step 80850, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 18h:24m:22s remains)
INFO - root - 2017-12-11 02:51:45.108502: step 80860, loss = 0.71, batch loss = 0.65 (29.7 examples/sec; 0.269 sec/batch; 18h:48m:26s remains)
INFO - root - 2017-12-11 02:51:47.772185: step 80870, loss = 0.69, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 18h:14m:18s remains)
INFO - root - 2017-12-11 02:51:50.401858: step 80880, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 18h:30m:57s remains)
INFO - root - 2017-12-11 02:51:53.040624: step 80890, loss = 0.68, batch loss = 0.62 (30.7 examples/sec; 0.261 sec/batch; 18h:12m:58s remains)
INFO - root - 2017-12-11 02:51:55.670920: step 80900, loss = 0.68, batch loss = 0.62 (31.1 examples/sec; 0.257 sec/batch; 17h:58m:33s remains)
2017-12-11 02:51:56.089415: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27467918 0.259995 0.24622273 0.24737923 0.26104897 0.27967185 0.29383594 0.2986213 0.29034388 0.27134797 0.25400043 0.25108242 0.27146068 0.31355062 0.36782682][0.30434969 0.30734196 0.3063423 0.3143169 0.32876018 0.34362978 0.35163131 0.348819 0.32933962 0.29442421 0.25917158 0.24051484 0.25069994 0.28943115 0.34802383][0.30986032 0.33104023 0.34650445 0.36612737 0.38548636 0.3991197 0.40128255 0.38854855 0.35338902 0.29779729 0.24102874 0.2041913 0.20163253 0.232542 0.28986821][0.30503392 0.33738303 0.36426932 0.39259481 0.41628185 0.42999622 0.42798775 0.40566823 0.35423535 0.27808914 0.20126243 0.14858301 0.13460533 0.15696552 0.20914227][0.296555 0.33137628 0.36121482 0.39170712 0.41719997 0.43354535 0.434001 0.40983993 0.3492285 0.2579869 0.16414417 0.096611112 0.071047947 0.083905719 0.12830606][0.29163697 0.32368371 0.34963021 0.37697598 0.40244949 0.42454913 0.43531418 0.4191753 0.35915187 0.26057631 0.15389565 0.0722075 0.033992335 0.036910418 0.073745169][0.29636496 0.31829035 0.33700493 0.36319837 0.39589834 0.43397048 0.46492434 0.46316293 0.4056257 0.29817316 0.175328 0.076393016 0.024216341 0.018522102 0.050910808][0.2973741 0.30636787 0.31800374 0.34771925 0.39526948 0.45690089 0.51380163 0.52991921 0.47666276 0.36038056 0.21996467 0.10210542 0.034781154 0.020381005 0.049437381][0.28683722 0.28428802 0.29198897 0.32780108 0.38991356 0.47086763 0.54806358 0.57938057 0.53370577 0.41593167 0.26573074 0.13443875 0.054364108 0.031757288 0.057548143][0.27064976 0.26012391 0.26696596 0.30885431 0.38040036 0.47068131 0.55677933 0.59638292 0.55885917 0.44608071 0.295049 0.1582291 0.070945509 0.043041185 0.066953242][0.25275356 0.23902439 0.24460976 0.28741965 0.3586008 0.44475642 0.52557 0.56360662 0.53151405 0.42848381 0.28675592 0.15602171 0.071599536 0.045330025 0.071295813][0.23256698 0.21572003 0.2162573 0.25184864 0.31237727 0.3837314 0.44962332 0.47961202 0.45045468 0.35978013 0.23500417 0.12084011 0.050088886 0.034491342 0.068593219][0.20577107 0.18505263 0.17743 0.19957517 0.2418232 0.29131332 0.33687484 0.35623714 0.33079323 0.25761941 0.1588202 0.071298115 0.02255615 0.022514069 0.065923423][0.17393313 0.15132138 0.136648 0.14421366 0.1661834 0.19284031 0.21821748 0.22779286 0.20753044 0.15469307 0.085851789 0.028042402 0.0019522324 0.015479657 0.064233385][0.13973859 0.12007271 0.10296657 0.099970482 0.10583257 0.11468825 0.12434635 0.1264725 0.11092011 0.075857237 0.0329321 -0.00036224749 -0.0098244268 0.010708836 0.058057141]]...]
INFO - root - 2017-12-11 02:51:58.739810: step 80910, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 18h:38m:27s remains)
INFO - root - 2017-12-11 02:52:01.355829: step 80920, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.256 sec/batch; 17h:54m:46s remains)
INFO - root - 2017-12-11 02:52:03.987147: step 80930, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 18h:03m:02s remains)
INFO - root - 2017-12-11 02:52:06.622623: step 80940, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.270 sec/batch; 18h:50m:51s remains)
INFO - root - 2017-12-11 02:52:09.234368: step 80950, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 18h:39m:23s remains)
INFO - root - 2017-12-11 02:52:11.853330: step 80960, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 17h:57m:05s remains)
INFO - root - 2017-12-11 02:52:14.492194: step 80970, loss = 0.70, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 18h:31m:59s remains)
INFO - root - 2017-12-11 02:52:17.114578: step 80980, loss = 0.71, batch loss = 0.66 (31.2 examples/sec; 0.256 sec/batch; 17h:53m:29s remains)
INFO - root - 2017-12-11 02:52:19.730209: step 80990, loss = 0.70, batch loss = 0.65 (31.3 examples/sec; 0.256 sec/batch; 17h:52m:23s remains)
INFO - root - 2017-12-11 02:52:22.377101: step 81000, loss = 0.69, batch loss = 0.63 (28.4 examples/sec; 0.282 sec/batch; 19h:41m:55s remains)
2017-12-11 02:52:22.805430: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0063330005 0.0016455288 0.0010052948 0.0055488911 0.015894836 0.031121405 0.047030162 0.0596965 0.065102391 0.064365745 0.0611283 0.060863353 0.068947434 0.081336394 0.091032505][0.0062208255 0.0053379177 0.010364014 0.021452168 0.038531475 0.06047865 0.081573352 0.09607558 0.099307068 0.093078852 0.082309119 0.074259765 0.076919496 0.087363333 0.098904222][0.018524552 0.022963857 0.03489219 0.052244961 0.073811114 0.099651858 0.12317775 0.13749415 0.13760328 0.12668785 0.11091875 0.098127633 0.097511359 0.1065609 0.11937693][0.047599208 0.058182012 0.07770209 0.10111842 0.1251425 0.15165834 0.17421162 0.18566594 0.18075426 0.16448091 0.14535093 0.13073754 0.12885967 0.13695933 0.15035066][0.093969882 0.10985751 0.13742968 0.16809972 0.19523333 0.22217931 0.24259315 0.24894777 0.23527144 0.20983003 0.18550327 0.16865385 0.16458961 0.16977613 0.18180494][0.151114 0.16976017 0.2037642 0.24157752 0.27266088 0.30127558 0.32028005 0.32061708 0.29504183 0.2567926 0.22479329 0.20423093 0.19644688 0.19689955 0.20604822][0.20802967 0.22674868 0.26308942 0.3042883 0.33696339 0.3664628 0.38479531 0.38046512 0.34362033 0.29235306 0.2518 0.22641705 0.21398109 0.2094464 0.21595387][0.2529611 0.26857617 0.30065376 0.3373633 0.36541927 0.39227393 0.40986377 0.4042156 0.36160856 0.30260167 0.25632715 0.22704326 0.21049738 0.20239733 0.20782499][0.27827421 0.28661439 0.30674428 0.32944778 0.34516251 0.36450285 0.38039377 0.37749848 0.33845261 0.2822181 0.23701514 0.20642592 0.18706758 0.17767225 0.18405901][0.28176513 0.27979654 0.28262794 0.28561184 0.28522852 0.29481447 0.30873495 0.31171152 0.28419727 0.24073304 0.20351616 0.17483465 0.15472688 0.14653739 0.15581217][0.26960835 0.25813386 0.24384272 0.22833507 0.21448344 0.21589176 0.22776611 0.23647831 0.22291093 0.19605336 0.16973133 0.14509043 0.1262465 0.12075312 0.13291703][0.25511456 0.23700017 0.20994656 0.18151642 0.16032234 0.15747972 0.16787954 0.18042079 0.17850538 0.16584913 0.1482884 0.1269232 0.1092329 0.10503701 0.11757386][0.24517784 0.22547466 0.19240674 0.15832166 0.13669813 0.13484998 0.14575684 0.15996653 0.16421308 0.15895762 0.14452794 0.1233393 0.10520024 0.099891894 0.1098981][0.23840871 0.22316039 0.19158104 0.1592356 0.1432379 0.14709066 0.16111915 0.17590921 0.18078807 0.17524986 0.15738045 0.13180777 0.10980821 0.10009365 0.10526734][0.2330098 0.2251516 0.19947919 0.17257573 0.16390799 0.17393281 0.19109385 0.20500587 0.2065565 0.19548753 0.17019361 0.13816819 0.1113421 0.096643865 0.096929222]]...]
INFO - root - 2017-12-11 02:52:25.431741: step 81010, loss = 0.69, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:47m:32s remains)
INFO - root - 2017-12-11 02:52:28.045761: step 81020, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 18h:23m:40s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 02:52:30.698027: step 81030, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 18h:12m:55s remains)
INFO - root - 2017-12-11 02:52:33.325933: step 81040, loss = 0.70, batch loss = 0.64 (28.3 examples/sec; 0.283 sec/batch; 19h:44m:16s remains)
INFO - root - 2017-12-11 02:52:35.966725: step 81050, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 19h:16m:21s remains)
INFO - root - 2017-12-11 02:52:38.672299: step 81060, loss = 0.71, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 19h:07m:41s remains)
INFO - root - 2017-12-11 02:52:41.292648: step 81070, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 18h:21m:39s remains)
INFO - root - 2017-12-11 02:52:43.921707: step 81080, loss = 0.69, batch loss = 0.63 (31.0 examples/sec; 0.258 sec/batch; 18h:02m:04s remains)
INFO - root - 2017-12-11 02:52:46.582797: step 81090, loss = 0.68, batch loss = 0.62 (30.6 examples/sec; 0.261 sec/batch; 18h:15m:26s remains)
INFO - root - 2017-12-11 02:52:49.224327: step 81100, loss = 0.69, batch loss = 0.63 (28.3 examples/sec; 0.283 sec/batch; 19h:44m:57s remains)
2017-12-11 02:52:49.631819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00520378 -0.003578733 0.0019935935 0.0096205277 0.015257693 0.016133461 0.012847665 0.0068695853 0.00032529319 -0.0046379478 -0.0071727308 -0.0076886611 -0.0083550345 -0.010061777 -0.01226523][-0.029510358 -0.021404464 -0.0064712833 0.011617812 0.026622629 0.03388305 0.033471331 0.026587056 0.015980756 0.00444574 -0.0067543406 -0.01698654 -0.027534256 -0.037512057 -0.045183938][-0.049831297 -0.030532835 2.5264741e-05 0.036063746 0.0684716 0.090614356 0.10143235 0.099804208 0.087091722 0.0659273 0.039017368 0.009118991 -0.022182783 -0.050112139 -0.070177741][-0.056141708 -0.020705035 0.032056294 0.094261684 0.1536222 0.20117928 0.23344746 0.24432698 0.23174164 0.19696073 0.1454543 0.084209263 0.020118177 -0.036254946 -0.076447226][-0.044152278 0.011458043 0.091568574 0.18638666 0.27991018 0.35990542 0.4192819 0.44558305 0.43204194 0.37825468 0.29445431 0.19378984 0.089852788 -0.0010150605 -0.06609109][-0.018687334 0.058400873 0.16641732 0.29417464 0.42153421 0.53167188 0.61334705 0.64925736 0.62962264 0.55280495 0.43475455 0.295721 0.15576176 0.034849275 -0.051875103][0.0058488771 0.10076729 0.23037992 0.3819654 0.53159636 0.65737832 0.74443054 0.77508873 0.74094337 0.64120305 0.49679959 0.333661 0.1756608 0.04279149 -0.050807405][0.016713334 0.11804962 0.25350502 0.40894231 0.55884159 0.67748672 0.74846816 0.75925094 0.70710367 0.59428996 0.44447914 0.28546575 0.14030473 0.024238221 -0.0531879][0.0055892793 0.096882924 0.21749198 0.35311165 0.48002693 0.57163477 0.61282557 0.59933317 0.536094 0.42968997 0.30353287 0.18278293 0.084102556 0.014107499 -0.023916215][-0.022642834 0.044195402 0.13315134 0.23161735 0.32044825 0.37557966 0.38573688 0.35501668 0.29514194 0.21688657 0.14057954 0.085486315 0.058310989 0.054994836 0.07214202][-0.0545109 -0.018520486 0.03175132 0.087061942 0.13465537 0.15612209 0.14551005 0.11295311 0.073836379 0.041284021 0.030319873 0.052501608 0.10404607 0.17087118 0.24389921][-0.077112891 -0.067709237 -0.050672319 -0.031380262 -0.016418289 -0.017158708 -0.034124695 -0.054304391 -0.062043652 -0.043862376 0.0096489834 0.10216004 0.22050409 0.34234971 0.45504218][-0.083602652 -0.089392677 -0.091824681 -0.093603045 -0.097173914 -0.10754248 -0.12092067 -0.12348574 -0.10074241 -0.039844394 0.063625962 0.20565829 0.36521584 0.513827 0.63706285][-0.076225407 -0.084410295 -0.091544569 -0.099653281 -0.1095049 -0.12191308 -0.13035539 -0.12087954 -0.079706967 0.0048021092 0.13276997 0.29364046 0.46129194 0.6041792 0.70751828][-0.064989887 -0.068293341 -0.071402162 -0.077158265 -0.086435646 -0.09853597 -0.10637234 -0.0964151 -0.054965723 0.028099215 0.15031092 0.29757589 0.44324124 0.55653483 0.62323648]]...]
INFO - root - 2017-12-11 02:52:52.261449: step 81110, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 18h:12m:20s remains)
INFO - root - 2017-12-11 02:52:54.943203: step 81120, loss = 0.69, batch loss = 0.64 (29.9 examples/sec; 0.267 sec/batch; 18h:39m:28s remains)
INFO - root - 2017-12-11 02:52:57.543789: step 81130, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.259 sec/batch; 18h:06m:33s remains)
INFO - root - 2017-12-11 02:53:00.159339: step 81140, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 17h:48m:42s remains)
INFO - root - 2017-12-11 02:53:02.741279: step 81150, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.262 sec/batch; 18h:16m:38s remains)
INFO - root - 2017-12-11 02:53:05.358426: step 81160, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.260 sec/batch; 18h:10m:16s remains)
INFO - root - 2017-12-11 02:53:07.964824: step 81170, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 17h:57m:21s remains)
INFO - root - 2017-12-11 02:53:10.665162: step 81180, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 18h:44m:37s remains)
INFO - root - 2017-12-11 02:53:13.328400: step 81190, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.263 sec/batch; 18h:20m:06s remains)
INFO - root - 2017-12-11 02:53:15.930928: step 81200, loss = 0.68, batch loss = 0.62 (30.7 examples/sec; 0.261 sec/batch; 18h:12m:12s remains)
2017-12-11 02:53:16.310452: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.015382092 0.04346998 0.053144161 0.044609185 0.024180148 0.0042458652 -0.0081850765 -0.013402829 -0.018468935 -0.02909434 -0.048110981 -0.079854257 -0.11694259 -0.14779736 -0.16518426][0.11888845 0.17090945 0.19452663 0.19150656 0.16964418 0.14679188 0.13136402 0.12442656 0.11541679 0.095635287 0.061632089 0.0080587082 -0.053840809 -0.10883106 -0.14569515][0.23430412 0.3153314 0.3589679 0.36841205 0.35412782 0.33683088 0.32393062 0.31797522 0.30604434 0.27642694 0.22414677 0.14490482 0.054020915 -0.030824518 -0.095082656][0.32344404 0.43271294 0.5002799 0.53027934 0.53725946 0.53911233 0.53871596 0.54088897 0.53200454 0.49542135 0.42291921 0.31512356 0.19238761 0.073211767 -0.025130074][0.3710351 0.50929517 0.60661638 0.66638386 0.7077741 0.74129057 0.76345152 0.7826454 0.78525615 0.74665082 0.65394253 0.5153535 0.35722986 0.19795778 0.059018251][0.39468026 0.56929886 0.70459718 0.79966021 0.879451 0.94688553 0.9928267 1.0319239 1.0510643 1.0159203 0.90550828 0.73585063 0.53898907 0.33456123 0.15029727][0.41774386 0.63290167 0.80765653 0.9329204 1.0396492 1.1276176 1.186108 1.2383659 1.2722224 1.2440575 1.1219966 0.92761379 0.69605809 0.4500632 0.22462285][0.43850675 0.684028 0.88539058 1.0237011 1.1333359 1.2170044 1.2687079 1.321439 1.363327 1.343804 1.2201805 1.0155165 0.76506388 0.49620745 0.24925852][0.42986596 0.68031865 0.88291287 1.0113828 1.0987955 1.1540699 1.1809924 1.2210191 1.2634627 1.2548991 1.1450611 0.95354307 0.71226013 0.4513188 0.21316193][0.36121854 0.58338553 0.75928986 0.85966021 0.91102153 0.92729431 0.92273456 0.94189656 0.9776293 0.97978312 0.8968699 0.74117362 0.53860635 0.31779233 0.11892196][0.22729267 0.39153236 0.51766932 0.57936311 0.59438568 0.57806057 0.54987085 0.54972905 0.57425863 0.58281922 0.53073621 0.4230881 0.27854475 0.12035958 -0.017782761][0.05997828 0.15329768 0.22109461 0.2437472 0.2315459 0.19845408 0.16252893 0.15289396 0.16762635 0.17833467 0.1529596 0.092742726 0.0097734109 -0.079753712 -0.15196827][-0.089930788 -0.058651973 -0.039487217 -0.04477454 -0.069074668 -0.10212164 -0.13138197 -0.14048688 -0.13174938 -0.12232333 -0.13034548 -0.15509334 -0.18885858 -0.22204736 -0.24122456][-0.17988667 -0.18692994 -0.19505563 -0.21163686 -0.23400596 -0.25661322 -0.27317318 -0.27730232 -0.27152541 -0.26473916 -0.26468068 -0.27005333 -0.27610478 -0.27783608 -0.26923352][-0.20082706 -0.22164942 -0.2374254 -0.25326812 -0.26787511 -0.27933943 -0.28581071 -0.28620338 -0.28248662 -0.27856562 -0.2768968 -0.2761175 -0.27325457 -0.26557839 -0.25069782]]...]
INFO - root - 2017-12-11 02:53:18.939874: step 81210, loss = 0.70, batch loss = 0.65 (28.1 examples/sec; 0.285 sec/batch; 19h:52m:20s remains)
INFO - root - 2017-12-11 02:53:21.634742: step 81220, loss = 0.70, batch loss = 0.64 (27.7 examples/sec; 0.288 sec/batch; 20h:08m:11s remains)
INFO - root - 2017-12-11 02:53:24.280297: step 81230, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 18h:22m:56s remains)
INFO - root - 2017-12-11 02:53:26.937522: step 81240, loss = 0.72, batch loss = 0.66 (30.2 examples/sec; 0.265 sec/batch; 18h:27m:46s remains)
INFO - root - 2017-12-11 02:53:29.579764: step 81250, loss = 0.68, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 18h:34m:44s remains)
INFO - root - 2017-12-11 02:53:32.242219: step 81260, loss = 0.68, batch loss = 0.62 (30.4 examples/sec; 0.263 sec/batch; 18h:23m:01s remains)
INFO - root - 2017-12-11 02:53:34.891418: step 81270, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 18h:41m:34s remains)
INFO - root - 2017-12-11 02:53:37.517384: step 81280, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 18h:34m:33s remains)
INFO - root - 2017-12-11 02:53:40.147023: step 81290, loss = 0.66, batch loss = 0.61 (27.9 examples/sec; 0.287 sec/batch; 19h:59m:47s remains)
INFO - root - 2017-12-11 02:53:42.791804: step 81300, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 18h:26m:22s remains)
2017-12-11 02:53:43.197229: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.037439182 0.061862145 0.093863726 0.12081475 0.13347518 0.12653664 0.1024701 0.067633323 0.029176699 -0.0048721828 -0.030471239 -0.045806009 -0.051681329 -0.05037364 -0.046975449][0.034760334 0.058917996 0.092861235 0.12306568 0.13826807 0.13252012 0.10754627 0.069594108 0.027825503 -0.0079323817 -0.031071356 -0.038555376 -0.032230914 -0.017709421 -0.0032344742][0.041759264 0.06767524 0.10670618 0.14646608 0.17217915 0.17420997 0.15090007 0.10818772 0.058564879 0.015507214 -0.0097965524 -0.010779328 0.010211846 0.04275823 0.073656395][0.066813648 0.10106722 0.15348408 0.21379341 0.26076746 0.27802303 0.25876394 0.20880088 0.14496142 0.085648216 0.049020786 0.048016146 0.080981672 0.13232249 0.18193485][0.10432999 0.15276042 0.22506739 0.31350765 0.38960317 0.42961079 0.420681 0.36683914 0.28725865 0.20552363 0.14825751 0.1366457 0.17204972 0.23618104 0.30214974][0.14240216 0.20567325 0.2971361 0.41130775 0.51593435 0.58297783 0.59355116 0.54604334 0.45744532 0.35492238 0.27291411 0.24016945 0.26345259 0.32679811 0.3999114][0.16019413 0.23212861 0.33456779 0.46272916 0.58588761 0.67721587 0.71367085 0.68532473 0.6021381 0.49122047 0.39102703 0.33351991 0.33167434 0.377447 0.44343942][0.14673671 0.21689607 0.31794003 0.44345796 0.5682 0.67127514 0.72973245 0.72705615 0.66463357 0.56486553 0.46324807 0.38859662 0.35858652 0.37607631 0.42212465][0.11255617 0.17055096 0.25775045 0.36464766 0.47270837 0.56963772 0.6375 0.658335 0.6261878 0.55481946 0.4703497 0.3935068 0.34394616 0.33438009 0.35569856][0.073597722 0.11239481 0.1777211 0.25733069 0.33916408 0.41864526 0.48417446 0.51981717 0.51519138 0.4755381 0.41593203 0.34811467 0.2908816 0.26295337 0.26269513][0.044885363 0.065321743 0.10900479 0.16248636 0.21960881 0.28098023 0.33865875 0.37880993 0.38939321 0.37113729 0.33169442 0.27618709 0.22085948 0.1851186 0.17029436][0.03043868 0.041205212 0.071444161 0.10773809 0.14909755 0.19923703 0.25054991 0.28923792 0.30311984 0.29269314 0.26317939 0.21655144 0.16662559 0.13049772 0.10826783][0.027261296 0.0385134 0.066009156 0.096429728 0.13188863 0.17814884 0.22715901 0.26476964 0.27882972 0.2699703 0.24286476 0.19858021 0.15033004 0.11353061 0.086397946][0.031703927 0.048303176 0.080316417 0.11361768 0.15021415 0.19672304 0.24680558 0.28849095 0.30925661 0.30716261 0.28428009 0.24033366 0.18852204 0.14492537 0.10892012][0.046112116 0.06906525 0.11095429 0.15523976 0.19953705 0.2486396 0.29952139 0.34567353 0.3756817 0.38501427 0.372157 0.33334315 0.279256 0.227092 0.17987849]]...]
INFO - root - 2017-12-11 02:53:45.836085: step 81310, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 18h:24m:49s remains)
INFO - root - 2017-12-11 02:53:48.459405: step 81320, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.276 sec/batch; 19h:17m:30s remains)
INFO - root - 2017-12-11 02:53:51.076409: step 81330, loss = 0.72, batch loss = 0.66 (29.9 examples/sec; 0.268 sec/batch; 18h:41m:04s remains)
INFO - root - 2017-12-11 02:53:53.770947: step 81340, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.258 sec/batch; 17h:58m:12s remains)
INFO - root - 2017-12-11 02:53:56.410038: step 81350, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 17h:45m:27s remains)
INFO - root - 2017-12-11 02:53:59.147378: step 81360, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 18h:26m:43s remains)
INFO - root - 2017-12-11 02:54:01.774973: step 81370, loss = 0.69, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 18h:06m:50s remains)
INFO - root - 2017-12-11 02:54:04.360256: step 81380, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 18h:06m:18s remains)
INFO - root - 2017-12-11 02:54:06.983419: step 81390, loss = 0.68, batch loss = 0.62 (30.4 examples/sec; 0.263 sec/batch; 18h:21m:19s remains)
INFO - root - 2017-12-11 02:54:09.614272: step 81400, loss = 0.71, batch loss = 0.65 (30.8 examples/sec; 0.259 sec/batch; 18h:05m:24s remains)
2017-12-11 02:54:10.017230: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10154611 0.13122456 0.14373711 0.14031976 0.13354453 0.13805091 0.15145555 0.16041836 0.15498759 0.140091 0.12573259 0.11631221 0.11758063 0.13010606 0.15664844][0.16559365 0.20999725 0.23129439 0.23238644 0.23077424 0.24457097 0.26856124 0.28435 0.27844241 0.2566703 0.23372819 0.21667925 0.21145333 0.21861528 0.24184823][0.21595368 0.27211249 0.30077955 0.30668184 0.31085229 0.33278644 0.36505389 0.38556626 0.37789363 0.34924355 0.31808966 0.293174 0.2785227 0.27378532 0.28389248][0.24008431 0.30384585 0.33951938 0.35317639 0.36681107 0.39842951 0.43831629 0.46111763 0.44860116 0.41082662 0.37031347 0.33682632 0.31047869 0.28946906 0.28013477][0.24139109 0.3105644 0.35567853 0.38271016 0.41148227 0.4559629 0.50324833 0.52494341 0.50228876 0.44994125 0.39570877 0.35054508 0.31001437 0.27063474 0.2412212][0.22934 0.30396619 0.36225361 0.40882236 0.4583143 0.5184359 0.57190788 0.58753151 0.54772013 0.47358879 0.39964718 0.33951962 0.28563148 0.2328642 0.19276004][0.21027625 0.29078782 0.36483747 0.43438491 0.50675142 0.58185405 0.637338 0.64069593 0.57830507 0.47834969 0.38175589 0.30641508 0.24537246 0.19235256 0.15978764][0.18427452 0.27024543 0.35998711 0.45156264 0.54426265 0.6297819 0.68120414 0.667104 0.5804013 0.45534554 0.33785003 0.25030473 0.19009165 0.15073411 0.14354286][0.15170284 0.23932414 0.33835408 0.44292334 0.5463956 0.63433754 0.67697167 0.64576149 0.5404197 0.39875162 0.268563 0.175403 0.12222629 0.10355499 0.12769748][0.11430016 0.1955311 0.2901608 0.39007533 0.48655361 0.56377161 0.5932194 0.55099148 0.44161698 0.30167931 0.17541951 0.08830712 0.04753248 0.048754036 0.096270375][0.074615017 0.14055328 0.21675046 0.29554114 0.36958462 0.42557082 0.44032267 0.39561093 0.29822025 0.17886657 0.073327191 0.0038551255 -0.021135587 -0.005795937 0.050259791][0.038366206 0.084689714 0.13623421 0.18687001 0.23209348 0.26294306 0.26402053 0.22344755 0.14811231 0.060197573 -0.01562198 -0.06227643 -0.072940066 -0.052227724 -0.0020881121][0.012981107 0.041463595 0.070038311 0.093978748 0.11192594 0.11994849 0.11087129 0.078601569 0.027869936 -0.028361717 -0.075689822 -0.10211644 -0.1034636 -0.084549926 -0.048299335][0.003720917 0.02044848 0.032417316 0.036277164 0.034093164 0.026702229 0.01353306 -0.0079925042 -0.037166435 -0.069126062 -0.096409783 -0.10993076 -0.10745455 -0.093723454 -0.072172441][0.006749806 0.017542554 0.020062741 0.012493317 -2.2018434e-05 -0.013340397 -0.024319764 -0.034698479 -0.048355889 -0.066072345 -0.0833367 -0.091627829 -0.089323655 -0.080987811 -0.070236377]]...]
INFO - root - 2017-12-11 02:54:12.651865: step 81410, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 18h:32m:39s remains)
INFO - root - 2017-12-11 02:54:15.238493: step 81420, loss = 0.68, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 17h:43m:02s remains)
INFO - root - 2017-12-11 02:54:17.852187: step 81430, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 17h:55m:52s remains)
INFO - root - 2017-12-11 02:54:20.468208: step 81440, loss = 0.72, batch loss = 0.66 (30.4 examples/sec; 0.263 sec/batch; 18h:19m:58s remains)
INFO - root - 2017-12-11 02:54:23.195803: step 81450, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 18h:49m:34s remains)
INFO - root - 2017-12-11 02:54:25.822931: step 81460, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 18h:44m:13s remains)
INFO - root - 2017-12-11 02:54:28.459759: step 81470, loss = 0.69, batch loss = 0.63 (31.1 examples/sec; 0.257 sec/batch; 17h:56m:26s remains)
INFO - root - 2017-12-11 02:54:31.044796: step 81480, loss = 0.68, batch loss = 0.62 (31.4 examples/sec; 0.255 sec/batch; 17h:45m:31s remains)
INFO - root - 2017-12-11 02:54:33.616546: step 81490, loss = 0.70, batch loss = 0.64 (31.5 examples/sec; 0.254 sec/batch; 17h:42m:35s remains)
INFO - root - 2017-12-11 02:54:36.234161: step 81500, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 17h:57m:56s remains)
2017-12-11 02:54:36.651554: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040736787 -0.044956595 -0.048388258 -0.051748812 -0.053995911 -0.0545514 -0.053268414 -0.049734164 -0.04537902 -0.043051604 -0.042876568 -0.044835512 -0.04932975 -0.055393759 -0.059981875][-0.032880694 -0.036156449 -0.039630104 -0.044166494 -0.0474059 -0.04659903 -0.042083614 -0.034868363 -0.0280806 -0.024830302 -0.0244457 -0.027496472 -0.035291 -0.04690893 -0.057703279][-0.010317449 -0.010573485 -0.013954979 -0.020112287 -0.023956373 -0.019228147 -0.0085187973 0.0042132684 0.014145669 0.018764993 0.019254643 0.013003889 -0.0011352325 -0.021380551 -0.041436616][0.027795728 0.032729752 0.03045211 0.023463547 0.020610834 0.0318493 0.051168635 0.071161553 0.084988929 0.091191649 0.090995565 0.078898855 0.055169053 0.023103066 -0.00924265][0.084277377 0.099658348 0.10333823 0.099553525 0.10003145 0.11664426 0.14213648 0.16794337 0.18556279 0.19385883 0.19186564 0.17102057 0.13401835 0.086896613 0.03952663][0.14557046 0.17796357 0.1946103 0.19893642 0.20324311 0.21991676 0.24513733 0.27322522 0.29448915 0.30583167 0.30122626 0.27019873 0.21871458 0.15620707 0.09367235][0.19607735 0.25013357 0.28527874 0.30076391 0.30746198 0.31827119 0.33668587 0.36299819 0.38665166 0.40011498 0.39124098 0.35065272 0.28831655 0.21506444 0.14107057][0.23073848 0.30635244 0.3616491 0.38962507 0.39776865 0.39912128 0.40539736 0.42380458 0.44437629 0.45575532 0.44201213 0.39664683 0.33089548 0.25384238 0.17369343][0.23752721 0.32748708 0.40015751 0.44231507 0.45547104 0.44941062 0.44083539 0.44269398 0.44916487 0.45064279 0.43286052 0.39228296 0.3354637 0.26492774 0.18585308][0.20867181 0.30020687 0.38210279 0.43718022 0.45921624 0.45024869 0.42696768 0.40619045 0.38957807 0.37493861 0.35460532 0.32708779 0.2899982 0.23703657 0.16942544][0.14437369 0.22096187 0.2971569 0.35578191 0.38469747 0.37718052 0.34556946 0.30785346 0.27263227 0.24506371 0.22418348 0.21010664 0.19400449 0.16377525 0.11733471][0.069040366 0.12064418 0.17748466 0.22572647 0.25262478 0.24735121 0.21706104 0.17763437 0.1399761 0.11133844 0.093793824 0.088465475 0.085737772 0.073433824 0.048613485][0.0085764444 0.034976434 0.066330329 0.093474232 0.1082556 0.10228544 0.079616018 0.051645454 0.026603734 0.0087916246 -0.00057972147 -0.00061412621 0.0016346207 -0.0021331196 -0.01306369][-0.035997633 -0.030187024 -0.021775344 -0.015664104 -0.014254657 -0.021314217 -0.034453731 -0.047454685 -0.056721989 -0.061907031 -0.063345581 -0.060490429 -0.057314806 -0.05776285 -0.060737163][-0.0676406 -0.073973075 -0.078194112 -0.082834631 -0.087259784 -0.092497565 -0.097534657 -0.10037857 -0.10097117 -0.1007739 -0.099823967 -0.097293414 -0.094638631 -0.093094684 -0.09074863]]...]
INFO - root - 2017-12-11 02:54:39.297223: step 81510, loss = 0.72, batch loss = 0.66 (31.1 examples/sec; 0.257 sec/batch; 17h:54m:24s remains)
INFO - root - 2017-12-11 02:54:41.968606: step 81520, loss = 0.71, batch loss = 0.65 (30.8 examples/sec; 0.260 sec/batch; 18h:06m:06s remains)
INFO - root - 2017-12-11 02:54:44.588448: step 81530, loss = 0.68, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 18h:06m:26s remains)
INFO - root - 2017-12-11 02:54:47.224555: step 81540, loss = 0.69, batch loss = 0.63 (31.1 examples/sec; 0.257 sec/batch; 17h:56m:24s remains)
INFO - root - 2017-12-11 02:54:49.862983: step 81550, loss = 0.71, batch loss = 0.65 (30.8 examples/sec; 0.259 sec/batch; 18h:04m:36s remains)
INFO - root - 2017-12-11 02:54:52.530365: step 81560, loss = 0.70, batch loss = 0.64 (28.7 examples/sec; 0.279 sec/batch; 19h:25m:12s remains)
INFO - root - 2017-12-11 02:54:55.211608: step 81570, loss = 0.69, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 18h:17m:11s remains)
INFO - root - 2017-12-11 02:54:58.054956: step 81580, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 18h:43m:07s remains)
INFO - root - 2017-12-11 02:55:02.767666: step 81590, loss = 0.70, batch loss = 0.64 (16.0 examples/sec; 0.501 sec/batch; 34h:54m:25s remains)
INFO - root - 2017-12-11 02:55:07.771782: step 81600, loss = 0.69, batch loss = 0.63 (16.2 examples/sec; 0.494 sec/batch; 34h:26m:06s remains)
2017-12-11 02:55:08.471243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.069027513 -0.068607591 -0.063083716 -0.050006706 -0.027862027 0.0022387926 0.038578156 0.088498466 0.15074089 0.21417046 0.26730505 0.29877442 0.3072249 0.279716 0.21195564][-0.13047314 -0.13080542 -0.12322562 -0.10550886 -0.07365521 -0.028600207 0.028996514 0.10664086 0.19792739 0.28309062 0.34335622 0.3652378 0.34976107 0.28803903 0.18385296][-0.14520156 -0.14405677 -0.13361005 -0.11108848 -0.068592221 -0.0057252659 0.075991645 0.18130484 0.29860976 0.40127566 0.46405667 0.47271779 0.42989346 0.33138096 0.19163448][-0.12729804 -0.12182014 -0.10523099 -0.072908476 -0.012930649 0.0741 0.18177941 0.30845973 0.43968922 0.54890281 0.60974646 0.60867006 0.54729092 0.42513394 0.26433474][-0.088835619 -0.079222389 -0.054423083 -0.00521077 0.082601309 0.20140888 0.33445767 0.47102645 0.59739143 0.6964497 0.74753976 0.73972237 0.67298549 0.549064 0.39091438][-0.047414202 -0.032867234 0.0040185093 0.07865575 0.20472465 0.36105964 0.51680082 0.65231484 0.75792581 0.8327499 0.86578321 0.85022235 0.78569525 0.67403448 0.53270292][-0.015224976 0.0064971009 0.059527468 0.16412178 0.33043897 0.52268887 0.69576383 0.82389057 0.90278387 0.94810188 0.95947772 0.93448383 0.87444937 0.77913296 0.65563351][-0.0040203249 0.025413362 0.093954153 0.22217365 0.41469496 0.62549675 0.80138505 0.91519856 0.96706277 0.984904 0.97844964 0.9501037 0.90222347 0.82828569 0.72263551][-0.017500717 0.013402116 0.08581046 0.21733709 0.40765446 0.6082803 0.76647651 0.85775381 0.88539094 0.88294363 0.86749208 0.84517217 0.81665248 0.76672357 0.67945164][-0.039771885 -0.016302774 0.044465289 0.15603222 0.31519312 0.47858515 0.6009109 0.66250575 0.66919619 0.65480518 0.638744 0.62740666 0.6172784 0.58640164 0.51597553][-0.044142932 -0.031072564 0.011253471 0.089478046 0.19892037 0.30672231 0.37988433 0.4050135 0.39048645 0.36777821 0.35393468 0.35156834 0.35233355 0.33304897 0.2788395][-0.007674946 0.0006298924 0.028901855 0.073773615 0.12848075 0.17256284 0.18869856 0.17074622 0.1305095 0.09679535 0.082462773 0.084412955 0.090504579 0.080602333 0.046270907][0.089553185 0.10094348 0.12467466 0.14609958 0.1546447 0.14067161 0.10148409 0.037506778 -0.034182437 -0.085320838 -0.10605131 -0.10363022 -0.092824817 -0.090332806 -0.10074059][0.23447667 0.25346076 0.27897447 0.28928369 0.26908123 0.21614884 0.13715342 0.035708446 -0.065619782 -0.13543528 -0.16489729 -0.16402502 -0.14944233 -0.13490339 -0.12401916][0.38811496 0.41445029 0.4447763 0.45515943 0.42706889 0.36134779 0.26697224 0.14689748 0.027923334 -0.053131565 -0.087375119 -0.086785235 -0.070284478 -0.048748095 -0.024797868]]...]
INFO - root - 2017-12-11 02:55:13.656706: step 81610, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.515 sec/batch; 35h:52m:33s remains)
INFO - root - 2017-12-11 02:55:18.739377: step 81620, loss = 0.70, batch loss = 0.64 (15.8 examples/sec; 0.507 sec/batch; 35h:20m:12s remains)
INFO - root - 2017-12-11 02:55:23.865112: step 81630, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.512 sec/batch; 35h:39m:16s remains)
INFO - root - 2017-12-11 02:55:28.527237: step 81640, loss = 0.71, batch loss = 0.65 (22.5 examples/sec; 0.355 sec/batch; 24h:45m:06s remains)
INFO - root - 2017-12-11 02:55:31.164378: step 81650, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 18h:21m:17s remains)
INFO - root - 2017-12-11 02:55:33.801753: step 81660, loss = 0.71, batch loss = 0.65 (29.7 examples/sec; 0.269 sec/batch; 18h:45m:00s remains)
INFO - root - 2017-12-11 02:55:36.423581: step 81670, loss = 0.68, batch loss = 0.63 (31.0 examples/sec; 0.258 sec/batch; 17h:58m:12s remains)
INFO - root - 2017-12-11 02:55:39.062705: step 81680, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 18h:03m:31s remains)
INFO - root - 2017-12-11 02:55:41.667311: step 81690, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:48m:04s remains)
INFO - root - 2017-12-11 02:55:44.291558: step 81700, loss = 0.69, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 18h:58m:58s remains)
2017-12-11 02:55:44.701176: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29724395 0.30850503 0.29229492 0.24995349 0.20166671 0.17105035 0.17696686 0.22113392 0.28637367 0.3480483 0.38208193 0.37980983 0.34487841 0.29599983 0.25750953][0.31910372 0.33207881 0.31182295 0.25820357 0.19033687 0.13264306 0.10329121 0.10799774 0.13678838 0.17222132 0.19193943 0.18408574 0.1517216 0.1086586 0.07759314][0.32752776 0.33748618 0.31228116 0.25266111 0.1752736 0.10300177 0.051477309 0.028254945 0.028989594 0.041731324 0.046362054 0.030041268 -0.0034036029 -0.042693678 -0.067151785][0.334447 0.34124795 0.31363177 0.25616047 0.18374713 0.11669991 0.066825546 0.040867738 0.035410486 0.038555447 0.031819284 0.0036354868 -0.039931528 -0.087176 -0.11788686][0.34722176 0.35687247 0.33413762 0.288583 0.23379952 0.18710326 0.15700582 0.14945465 0.15852641 0.16626163 0.1532007 0.11006144 0.046160508 -0.024286913 -0.078675725][0.36897117 0.38714197 0.37401429 0.34479886 0.31278712 0.29228467 0.28975898 0.31075263 0.34519556 0.36477351 0.34728011 0.28696141 0.1970955 0.095006287 0.0075876163][0.40219283 0.43135643 0.42858285 0.41443369 0.40296888 0.40532324 0.42580807 0.47031879 0.52591455 0.55367428 0.52805072 0.448027 0.3324019 0.20161001 0.08581569][0.44458663 0.48128885 0.4844296 0.47969174 0.48153806 0.49653426 0.52608317 0.57705861 0.636966 0.66094881 0.62176126 0.52314246 0.39004984 0.24540465 0.11896436][0.48372945 0.51821512 0.51995593 0.51719058 0.52294195 0.53796631 0.55995864 0.59733617 0.64213467 0.65065104 0.5967927 0.48829842 0.35415581 0.21727532 0.10262278][0.49677408 0.52053988 0.51476157 0.50736344 0.50777614 0.51150513 0.51345974 0.52351332 0.54037488 0.52858037 0.46628323 0.36310017 0.2477482 0.13982761 0.05620737][0.45917869 0.46818653 0.4526265 0.43715018 0.42668334 0.41373825 0.39245173 0.37313956 0.36065438 0.33207947 0.27222559 0.18996353 0.10892229 0.042972963 0.00019110108][0.36362183 0.35957795 0.33809569 0.31671724 0.29614943 0.26829416 0.22910501 0.18891233 0.15639926 0.12116043 0.07441289 0.022046765 -0.020022683 -0.043795258 -0.048063144][0.22165865 0.21036363 0.18949251 0.16807617 0.14432804 0.11100382 0.06639985 0.02017737 -0.01757771 -0.048547316 -0.077045426 -0.10042436 -0.10962132 -0.10127194 -0.078340448][0.069453925 0.056646273 0.041751895 0.026766255 0.0083534839 -0.019190097 -0.056012493 -0.094167076 -0.12428333 -0.14442147 -0.15669273 -0.16011582 -0.14919636 -0.12346373 -0.0876419][-0.046268106 -0.056271005 -0.063870884 -0.070638917 -0.080362536 -0.096554436 -0.11882834 -0.14171 -0.15865064 -0.16799058 -0.17063382 -0.16546583 -0.14769727 -0.1178216 -0.0803153]]...]
INFO - root - 2017-12-11 02:55:47.315987: step 81710, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 18h:12m:36s remains)
INFO - root - 2017-12-11 02:55:49.932045: step 81720, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 18h:07m:05s remains)
INFO - root - 2017-12-11 02:55:52.567057: step 81730, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 18h:55m:25s remains)
INFO - root - 2017-12-11 02:55:55.168863: step 81740, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 18h:20m:27s remains)
INFO - root - 2017-12-11 02:55:57.824253: step 81750, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 18h:37m:28s remains)
INFO - root - 2017-12-11 02:56:00.476337: step 81760, loss = 0.72, batch loss = 0.66 (31.0 examples/sec; 0.258 sec/batch; 17h:57m:51s remains)
INFO - root - 2017-12-11 02:56:03.076940: step 81770, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 18h:34m:11s remains)
INFO - root - 2017-12-11 02:56:05.710121: step 81780, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 18h:31m:51s remains)
INFO - root - 2017-12-11 02:56:08.355317: step 81790, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 18h:31m:54s remains)
INFO - root - 2017-12-11 02:56:10.999363: step 81800, loss = 0.72, batch loss = 0.66 (30.5 examples/sec; 0.262 sec/batch; 18h:15m:11s remains)
2017-12-11 02:56:11.409700: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.098005719 0.082580678 0.049762048 0.011803935 -0.016994392 -0.034929588 -0.048910625 -0.059973154 -0.064832449 -0.059505578 -0.032354008 0.025027769 0.10625727 0.19150159 0.26008222][0.080319569 0.069042623 0.042174134 0.011695055 -0.0096781729 -0.023631502 -0.038597517 -0.053625859 -0.063066795 -0.060466874 -0.03130414 0.038182735 0.1428854 0.25718108 0.35201561][0.055728778 0.052924912 0.040717684 0.030379781 0.029446192 0.029489268 0.020261323 0.0038321002 -0.01300631 -0.021628976 -0.0013970032 0.068952806 0.18490656 0.31649971 0.42860836][0.045586962 0.057175279 0.069164328 0.091631763 0.12288456 0.14527726 0.14756419 0.13217311 0.10452533 0.0745635 0.071715124 0.12617826 0.23475426 0.36574408 0.48185903][0.06138454 0.094320029 0.13925682 0.20373045 0.27403659 0.3234314 0.34186 0.33160353 0.29261628 0.2342975 0.19566232 0.21710543 0.29708984 0.40624177 0.5102191][0.10004703 0.15879682 0.23992832 0.34654519 0.45344746 0.52837843 0.56509143 0.56421536 0.51683462 0.42985776 0.35084867 0.33046716 0.36863387 0.44103643 0.52098829][0.14477015 0.2255269 0.3358334 0.47344896 0.60537016 0.69976175 0.75615221 0.77049947 0.72259122 0.61621547 0.50431764 0.44420224 0.4360944 0.46403649 0.51314193][0.17396304 0.26428184 0.38622633 0.53443265 0.67367852 0.77720976 0.85081476 0.88445675 0.8466807 0.73789996 0.61066777 0.5216729 0.47101939 0.45389315 0.470595][0.18162602 0.26724014 0.37990525 0.51499736 0.64056373 0.73701137 0.81651247 0.86594123 0.8466258 0.75431758 0.63394094 0.53299588 0.45066676 0.394873 0.38274336][0.17362119 0.24270047 0.32838178 0.43048608 0.52406824 0.59684622 0.66592014 0.72027308 0.72081786 0.65809816 0.56376755 0.47012314 0.37449965 0.29582819 0.26495156][0.16227311 0.20864895 0.25811547 0.31687617 0.36814407 0.40576279 0.44965747 0.49537334 0.50981259 0.47873557 0.41906729 0.34689212 0.25813258 0.17762299 0.14164463][0.15851203 0.18555854 0.20250225 0.22130698 0.23181579 0.23261461 0.24524333 0.27431002 0.29552975 0.29010496 0.26259834 0.21671131 0.14775085 0.080301851 0.047503036][0.16355771 0.1803603 0.17766309 0.16987252 0.15035141 0.12083792 0.10404558 0.11376433 0.13491765 0.14529768 0.14144382 0.12021241 0.076700836 0.029626181 0.0034647447][0.16598694 0.18033011 0.17137156 0.15233408 0.11746697 0.070294328 0.03327588 0.026677042 0.043273348 0.060217373 0.070620745 0.068935312 0.049816344 0.023466226 0.0043644756][0.14756989 0.16169254 0.15383458 0.1338532 0.096167333 0.045357767 0.0017757978 -0.012478775 0.00010896683 0.018827904 0.036396738 0.04731058 0.045946941 0.034881342 0.020750701]]...]
INFO - root - 2017-12-11 02:56:14.048952: step 81810, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 18h:18m:43s remains)
INFO - root - 2017-12-11 02:56:16.717188: step 81820, loss = 0.69, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:59m:26s remains)
INFO - root - 2017-12-11 02:56:19.388130: step 81830, loss = 0.68, batch loss = 0.63 (28.4 examples/sec; 0.282 sec/batch; 19h:38m:05s remains)
INFO - root - 2017-12-11 02:56:22.048287: step 81840, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 18h:26m:27s remains)
INFO - root - 2017-12-11 02:56:24.732468: step 81850, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 18h:36m:32s remains)
INFO - root - 2017-12-11 02:56:27.415386: step 81860, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 18h:29m:17s remains)
INFO - root - 2017-12-11 02:56:30.111509: step 81870, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.262 sec/batch; 18h:15m:27s remains)
INFO - root - 2017-12-11 02:56:32.811089: step 81880, loss = 0.69, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 18h:58m:08s remains)
INFO - root - 2017-12-11 02:56:35.514540: step 81890, loss = 0.68, batch loss = 0.62 (29.6 examples/sec; 0.270 sec/batch; 18h:48m:26s remains)
INFO - root - 2017-12-11 02:56:38.216500: step 81900, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 18h:31m:36s remains)
2017-12-11 02:56:38.710115: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15415436 0.14826906 0.15873615 0.19683677 0.24253203 0.27044854 0.27935076 0.28722915 0.30696017 0.32983115 0.35285398 0.382517 0.41437912 0.43268532 0.43273425][0.16307065 0.16244408 0.17147771 0.20485441 0.24715173 0.27325332 0.27917397 0.28386793 0.30352718 0.33151343 0.36092106 0.3932983 0.42344394 0.43916655 0.4380872][0.17231424 0.17516474 0.18109649 0.20790048 0.24630748 0.27150258 0.27625728 0.27845606 0.29538736 0.32355919 0.35356575 0.38332716 0.40824002 0.42088953 0.42028713][0.19974709 0.20591702 0.21014991 0.23185943 0.266199 0.28907704 0.2923874 0.29223952 0.30483624 0.32771507 0.34955782 0.36746505 0.38030142 0.38732192 0.38717794][0.24942529 0.25683963 0.25824609 0.27359533 0.3011888 0.31844908 0.31905258 0.31702316 0.32602525 0.34135738 0.34983864 0.35052511 0.3476828 0.34692547 0.34562045][0.30449483 0.31259078 0.31146827 0.32046995 0.340165 0.34982416 0.34718537 0.34432265 0.35138607 0.35994807 0.35603169 0.34196895 0.32559118 0.31631485 0.31038728][0.34556133 0.35314655 0.34930524 0.35304248 0.36579418 0.3690865 0.36538056 0.36476356 0.37219459 0.37662435 0.36592922 0.34463671 0.32010353 0.30136332 0.28574404][0.35280976 0.35944903 0.3564277 0.3615149 0.37407994 0.3768149 0.37512898 0.37713087 0.38339302 0.38338003 0.36993983 0.347251 0.31970966 0.29292926 0.26638263][0.32687232 0.335778 0.34143579 0.35628664 0.37475133 0.37931171 0.37649977 0.3755185 0.37549564 0.37010625 0.35693449 0.33788112 0.31425357 0.28658694 0.25450122][0.26374984 0.28184173 0.3085885 0.34460044 0.37524965 0.38143706 0.37140217 0.36022988 0.34818396 0.33493531 0.3230727 0.31307286 0.30343911 0.2866706 0.2596342][0.19041905 0.22227721 0.27768704 0.34335506 0.39170155 0.39973325 0.37839633 0.35228974 0.32331023 0.29704726 0.28218892 0.28068191 0.28879568 0.2898 0.27623743][0.14674355 0.18968691 0.26810297 0.35935843 0.42257637 0.42970133 0.39405626 0.35020289 0.3030349 0.26259723 0.2436367 0.25037697 0.27658036 0.29653317 0.29847807][0.14052662 0.18861522 0.27671903 0.37972084 0.44784537 0.44951564 0.39877889 0.33814174 0.2776269 0.22898465 0.21147436 0.23059893 0.27743447 0.31711978 0.33268902][0.1478375 0.1965033 0.2837126 0.3848013 0.44806004 0.44154337 0.37880236 0.30703345 0.2423629 0.19592364 0.18846123 0.22501197 0.29375964 0.35164776 0.37623471][0.1467572 0.19967869 0.28811207 0.38623592 0.44431952 0.43306214 0.36479321 0.28810152 0.22403276 0.18376832 0.18747917 0.23891789 0.32412133 0.39418879 0.42224154]]...]
INFO - root - 2017-12-11 02:56:41.406741: step 81910, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.263 sec/batch; 18h:16m:33s remains)
INFO - root - 2017-12-11 02:56:44.088193: step 81920, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 19h:12m:56s remains)
INFO - root - 2017-12-11 02:56:46.802972: step 81930, loss = 0.68, batch loss = 0.62 (29.8 examples/sec; 0.268 sec/batch; 18h:41m:07s remains)
INFO - root - 2017-12-11 02:56:49.503371: step 81940, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 19h:02m:40s remains)
INFO - root - 2017-12-11 02:56:52.219620: step 81950, loss = 0.71, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 18h:59m:04s remains)
INFO - root - 2017-12-11 02:56:54.950193: step 81960, loss = 0.70, batch loss = 0.64 (28.4 examples/sec; 0.281 sec/batch; 19h:34m:18s remains)
INFO - root - 2017-12-11 02:56:57.700999: step 81970, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:44m:35s remains)
INFO - root - 2017-12-11 02:57:00.341314: step 81980, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 18h:21m:59s remains)
INFO - root - 2017-12-11 02:57:03.049681: step 81990, loss = 0.69, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 18h:48m:06s remains)
INFO - root - 2017-12-11 02:57:05.791017: step 82000, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 19h:09m:03s remains)
2017-12-11 02:57:06.360352: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.02589893 0.0384408 0.047584627 0.052761257 0.055628128 0.058023 0.059454858 0.059117988 0.057364717 0.052296717 0.044927951 0.032878961 0.013932928 -0.0083067548 -0.02879446][0.085433781 0.11086153 0.12838049 0.13726407 0.13969724 0.13956362 0.13788703 0.13579358 0.13191316 0.1225674 0.10882951 0.086665317 0.054901309 0.018868126 -0.01357136][0.16257685 0.20179349 0.22415005 0.23126636 0.22722553 0.21907367 0.21095064 0.20666419 0.2017723 0.19001502 0.17043954 0.13868745 0.094717689 0.046382517 0.0035665247][0.24149147 0.29218981 0.31317896 0.31241402 0.29808027 0.27956536 0.26476029 0.26127017 0.2608301 0.25178471 0.22836973 0.18755852 0.13208425 0.07234247 0.019375017][0.31613538 0.37396395 0.38743648 0.37428442 0.34822425 0.31990144 0.30119196 0.30319026 0.3136301 0.31231582 0.28708443 0.23717976 0.17034605 0.099587828 0.036274649][0.38042784 0.44089329 0.44365025 0.41637856 0.37792617 0.34121719 0.32257345 0.33465433 0.35945791 0.3668671 0.3395513 0.27962595 0.20131589 0.12007914 0.048005119][0.4189111 0.4777953 0.4705067 0.43066233 0.38139033 0.33972475 0.32585347 0.35024634 0.38823617 0.40108445 0.36982265 0.30001953 0.2123986 0.12423326 0.0480001][0.4158558 0.46887311 0.45503774 0.40795466 0.35321656 0.31223354 0.30661553 0.34088227 0.3853879 0.39751229 0.36102152 0.28537714 0.19504331 0.10749688 0.0342847][0.36622444 0.4097383 0.39368346 0.34646228 0.29286173 0.25781521 0.2608481 0.2995137 0.34286374 0.35004842 0.31123579 0.23799406 0.15438941 0.075674295 0.011589387][0.28473103 0.31769031 0.30419222 0.26367542 0.21659116 0.18936987 0.19754268 0.23292641 0.26719236 0.26702905 0.23016828 0.16743046 0.0981681 0.034415253 -0.015888534][0.18875347 0.2118452 0.20272899 0.17141271 0.13268094 0.11150517 0.11924446 0.14553376 0.16760726 0.16215284 0.13202845 0.085068062 0.034176346 -0.011464883 -0.045300189][0.094375081 0.10888429 0.10379548 0.081963718 0.0530309 0.037586339 0.043037571 0.059736673 0.071929552 0.065613531 0.045238588 0.014909891 -0.017942198 -0.046052638 -0.064439431][0.019078588 0.025992204 0.023155654 0.0096700061 -0.009080762 -0.018781316 -0.015464516 -0.0062294179 -0.00046377184 -0.0052817552 -0.016735407 -0.033233147 -0.050930675 -0.0643387 -0.070664607][-0.030310506 -0.029381134 -0.031842228 -0.039787643 -0.0507821 -0.056574538 -0.055413168 -0.051332187 -0.049155846 -0.052111011 -0.057386577 -0.064333647 -0.070876323 -0.0735128 -0.071695559][-0.055446662 -0.058196072 -0.0610091 -0.066197455 -0.072598331 -0.076230459 -0.076537356 -0.075334437 -0.07474573 -0.0761326 -0.077876508 -0.079420291 -0.079488955 -0.076350547 -0.070774771]]...]
INFO - root - 2017-12-11 02:57:09.111856: step 82010, loss = 0.70, batch loss = 0.65 (29.6 examples/sec; 0.271 sec/batch; 18h:49m:20s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 02:57:11.814059: step 82020, loss = 0.71, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 19h:05m:25s remains)
INFO - root - 2017-12-11 02:57:14.527841: step 82030, loss = 0.67, batch loss = 0.61 (29.6 examples/sec; 0.270 sec/batch; 18h:47m:07s remains)
INFO - root - 2017-12-11 02:57:17.226468: step 82040, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.273 sec/batch; 18h:57m:43s remains)
INFO - root - 2017-12-11 02:57:19.925834: step 82050, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 18h:54m:23s remains)
INFO - root - 2017-12-11 02:57:22.676083: step 82060, loss = 0.69, batch loss = 0.63 (29.3 examples/sec; 0.273 sec/batch; 18h:58m:24s remains)
INFO - root - 2017-12-11 02:57:25.367374: step 82070, loss = 0.69, batch loss = 0.64 (28.8 examples/sec; 0.278 sec/batch; 19h:19m:24s remains)
INFO - root - 2017-12-11 02:57:28.054089: step 82080, loss = 0.68, batch loss = 0.63 (28.0 examples/sec; 0.286 sec/batch; 19h:53m:03s remains)
INFO - root - 2017-12-11 02:57:30.760603: step 82090, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.268 sec/batch; 18h:39m:16s remains)
INFO - root - 2017-12-11 02:57:33.468965: step 82100, loss = 0.69, batch loss = 0.63 (28.8 examples/sec; 0.278 sec/batch; 19h:19m:41s remains)
2017-12-11 02:57:33.939785: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.48461103 0.45397395 0.41699263 0.40817726 0.42725214 0.44786116 0.4492377 0.43130139 0.38863972 0.32270202 0.24747048 0.18248855 0.1406419 0.12518562 0.14228849][0.41345659 0.38331437 0.35002193 0.34364423 0.36289725 0.383753 0.3866829 0.37025198 0.32774338 0.26143733 0.18736714 0.12411402 0.082672648 0.063722782 0.071049631][0.330623 0.30359879 0.27823049 0.27661511 0.29601809 0.31835189 0.32806557 0.3194401 0.28379443 0.22405592 0.15662567 0.096631959 0.05222369 0.024855744 0.020553505][0.28963166 0.26983884 0.25853163 0.26834998 0.29303658 0.32137105 0.34410733 0.34943935 0.32486719 0.27257848 0.20886587 0.1447884 0.084260069 0.033904031 0.0085056387][0.28490487 0.28271371 0.29698142 0.32990503 0.36832845 0.40737814 0.44607875 0.46755961 0.45406744 0.4058224 0.33974305 0.26265305 0.17339467 0.086449757 0.030786088][0.28727818 0.3087543 0.35427204 0.41552377 0.47135329 0.52252656 0.57619947 0.61161453 0.60512078 0.55550981 0.48250237 0.3901346 0.27248287 0.15063918 0.066442296][0.27046376 0.31220838 0.38193506 0.46431482 0.53324747 0.59416264 0.65931988 0.70469326 0.70106196 0.64692724 0.566506 0.46388337 0.33066976 0.19088449 0.091780342][0.24169303 0.2932258 0.36843282 0.45100528 0.51635164 0.57428187 0.63872576 0.68430644 0.68004119 0.62463874 0.54479116 0.44523075 0.3176803 0.18497218 0.090538807][0.22702946 0.27594993 0.33522671 0.39409462 0.43526658 0.47207928 0.51801348 0.550413 0.54244441 0.49275753 0.42531341 0.3436476 0.24145268 0.13790341 0.066496693][0.22421369 0.26124689 0.29164281 0.31367049 0.3192333 0.32355967 0.33937094 0.35040972 0.33771762 0.2991448 0.25172091 0.19667071 0.13015677 0.067226492 0.029223954][0.2259499 0.24735381 0.24738514 0.23372728 0.20609321 0.17973903 0.16707243 0.15934898 0.1449746 0.12000495 0.093125686 0.063758425 0.030667806 0.005732737 0.00077457051][0.23191023 0.23789777 0.21604215 0.1806609 0.13470715 0.09121462 0.06191989 0.044076748 0.031245762 0.018187249 0.0069788438 -0.0037387067 -0.01336984 -0.011937256 0.008386761][0.24692942 0.24032991 0.20862508 0.16982655 0.12477721 0.081566975 0.050092418 0.031204928 0.021527864 0.01629521 0.014200421 0.013859078 0.016439172 0.031496003 0.068180308][0.2755267 0.25955713 0.22661778 0.19639647 0.1648304 0.1338786 0.11043726 0.0964699 0.090402819 0.089214176 0.090848483 0.094253823 0.10039475 0.11973849 0.16548432][0.29291898 0.27112892 0.24055521 0.22150142 0.20555875 0.18963604 0.17736354 0.17022264 0.16758905 0.16821775 0.17051089 0.17379849 0.1788248 0.19715704 0.2455056]]...]
INFO - root - 2017-12-11 02:57:36.657070: step 82110, loss = 0.69, batch loss = 0.63 (28.4 examples/sec; 0.282 sec/batch; 19h:37m:24s remains)
INFO - root - 2017-12-11 02:57:39.403954: step 82120, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.261 sec/batch; 18h:09m:49s remains)
INFO - root - 2017-12-11 02:57:42.083172: step 82130, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 18h:42m:52s remains)
INFO - root - 2017-12-11 02:57:44.794668: step 82140, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:47m:36s remains)
INFO - root - 2017-12-11 02:57:47.553819: step 82150, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 19h:12m:35s remains)
INFO - root - 2017-12-11 02:57:50.323253: step 82160, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.273 sec/batch; 18h:57m:09s remains)
INFO - root - 2017-12-11 02:57:52.992394: step 82170, loss = 0.69, batch loss = 0.63 (31.0 examples/sec; 0.258 sec/batch; 17h:56m:43s remains)
INFO - root - 2017-12-11 02:57:55.689766: step 82180, loss = 0.68, batch loss = 0.62 (29.7 examples/sec; 0.270 sec/batch; 18h:44m:34s remains)
INFO - root - 2017-12-11 02:57:58.425697: step 82190, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 18h:54m:27s remains)
INFO - root - 2017-12-11 02:58:01.165273: step 82200, loss = 0.73, batch loss = 0.67 (29.8 examples/sec; 0.269 sec/batch; 18h:41m:13s remains)
2017-12-11 02:58:01.625727: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15465344 0.16600698 0.1793752 0.19459252 0.20746006 0.21406622 0.21273382 0.2000069 0.17355894 0.13873784 0.10915339 0.096714027 0.10583932 0.13529012 0.17380923][0.17671442 0.18986617 0.20410709 0.2211967 0.23598076 0.24086927 0.23410384 0.21456794 0.18262203 0.14356056 0.10970903 0.09258794 0.095642336 0.11747488 0.14964159][0.20299338 0.21516508 0.22821486 0.24595082 0.26173475 0.26439154 0.25191703 0.22592649 0.19058494 0.15011878 0.1148263 0.09499196 0.092231721 0.10416128 0.12424407][0.23266408 0.24214725 0.25284496 0.27008516 0.2858403 0.28701553 0.27190351 0.24296826 0.20817149 0.17101768 0.13896635 0.11932952 0.11236649 0.11554042 0.12275995][0.25337139 0.26142487 0.27089614 0.28790534 0.30389848 0.30560756 0.29197878 0.26444563 0.23340437 0.20183018 0.17508024 0.1573875 0.14732905 0.14284737 0.13798811][0.25163057 0.26916346 0.28794155 0.31254449 0.33352584 0.33855811 0.32807514 0.30196735 0.27144161 0.23927379 0.21098518 0.18993078 0.1743266 0.16311328 0.14951493][0.24038151 0.28146529 0.32209995 0.3629958 0.39197126 0.39815065 0.385307 0.35351643 0.31478 0.27235544 0.23388472 0.20434654 0.18280174 0.16841283 0.15164217][0.24487434 0.30936849 0.37198251 0.42914018 0.46440533 0.46691775 0.4433668 0.39581105 0.33948848 0.2808581 0.23065102 0.19610071 0.17652558 0.16935706 0.1598013][0.25160813 0.33047262 0.40811789 0.47695163 0.51558161 0.51242214 0.4753077 0.40968984 0.33542347 0.26372004 0.20788929 0.17518695 0.16319975 0.16687027 0.1673269][0.23116633 0.310865 0.39321488 0.46628848 0.5055303 0.49862948 0.45261094 0.3769792 0.29522 0.22155246 0.1702234 0.146723 0.14531489 0.15872397 0.16730595][0.17119841 0.23750612 0.31191033 0.37987551 0.41710183 0.41174442 0.36855447 0.29829195 0.22385837 0.15966935 0.11944698 0.10720661 0.11520737 0.13541858 0.14984657][0.097756691 0.14133082 0.19526695 0.24817698 0.27911812 0.27778596 0.24593227 0.19206069 0.13433196 0.085284248 0.057341639 0.05407133 0.067853138 0.091127984 0.1089013][0.036098745 0.055982668 0.08550249 0.11862681 0.13982329 0.14082313 0.12077049 0.084397756 0.043980189 0.01015047 -0.0066379444 -0.0036909697 0.012518143 0.035387937 0.053680744][-0.012342742 -0.0090804789 0.0020231504 0.019196695 0.032425139 0.035159275 0.024659112 0.0025254076 -0.02345519 -0.044354837 -0.052190188 -0.045671459 -0.030236691 -0.011484686 0.0033792306][-0.038866028 -0.044680692 -0.045423046 -0.040522829 -0.034535222 -0.032003976 -0.0363568 -0.047773547 -0.061689436 -0.071596123 -0.072535448 -0.064173169 -0.050662439 -0.036446877 -0.025704121]]...]
INFO - root - 2017-12-11 02:58:04.311466: step 82210, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 17h:46m:55s remains)
INFO - root - 2017-12-11 02:58:07.079714: step 82220, loss = 0.70, batch loss = 0.64 (28.4 examples/sec; 0.282 sec/batch; 19h:34m:30s remains)
INFO - root - 2017-12-11 02:58:09.778521: step 82230, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:47m:34s remains)
INFO - root - 2017-12-11 02:58:12.467511: step 82240, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 19h:09m:18s remains)
INFO - root - 2017-12-11 02:58:15.213947: step 82250, loss = 0.69, batch loss = 0.63 (28.5 examples/sec; 0.280 sec/batch; 19h:29m:20s remains)
INFO - root - 2017-12-11 02:58:17.939955: step 82260, loss = 0.68, batch loss = 0.62 (29.5 examples/sec; 0.272 sec/batch; 18h:52m:48s remains)
INFO - root - 2017-12-11 02:58:20.669877: step 82270, loss = 0.69, batch loss = 0.63 (28.1 examples/sec; 0.284 sec/batch; 19h:45m:21s remains)
INFO - root - 2017-12-11 02:58:23.419946: step 82280, loss = 0.68, batch loss = 0.62 (29.8 examples/sec; 0.268 sec/batch; 18h:39m:37s remains)
INFO - root - 2017-12-11 02:58:26.176842: step 82290, loss = 0.68, batch loss = 0.63 (30.0 examples/sec; 0.266 sec/batch; 18h:30m:18s remains)
INFO - root - 2017-12-11 02:58:28.908528: step 82300, loss = 0.68, batch loss = 0.63 (28.9 examples/sec; 0.277 sec/batch; 19h:13m:13s remains)
2017-12-11 02:58:29.367989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.066058785 -0.064944319 -0.061430473 -0.058565345 -0.056393169 -0.055293079 -0.053723682 -0.051672917 -0.051148124 -0.053786952 -0.060244866 -0.068843059 -0.077607609 -0.084626034 -0.088297732][-0.037071541 -0.035741437 -0.031863619 -0.028663786 -0.0265841 -0.026269685 -0.024772173 -0.021504227 -0.020506788 -0.024653835 -0.035323981 -0.050156772 -0.066110849 -0.079868384 -0.088177294][0.017974537 0.020553639 0.025262168 0.029438296 0.031550355 0.030439828 0.03025144 0.032109972 0.030096617 0.020770028 0.0023639319 -0.021992752 -0.047893763 -0.070326813 -0.084437862][0.10437412 0.11081816 0.11885136 0.12616023 0.12994827 0.12833706 0.12497389 0.12053635 0.10819497 0.0852757 0.051041596 0.010525296 -0.029575897 -0.062354937 -0.082207233][0.2212052 0.23568852 0.25055352 0.2636607 0.27116206 0.27133662 0.26506874 0.25097606 0.22201625 0.17740732 0.11860909 0.053548288 -0.0075310748 -0.054598179 -0.081585057][0.35815123 0.38313076 0.40585974 0.42507598 0.43679702 0.44147342 0.4368647 0.41672251 0.37257442 0.30463278 0.21733919 0.12113345 0.031052042 -0.037364513 -0.076450318][0.49389529 0.52778077 0.55572635 0.57883537 0.59346694 0.60449135 0.60861081 0.59214079 0.54213494 0.45689723 0.34299105 0.21355289 0.088941708 -0.0073401644 -0.064026184][0.60031474 0.63800186 0.66486037 0.68625909 0.69942081 0.71511775 0.73206007 0.72856832 0.68462867 0.59405249 0.46340165 0.30711502 0.1506505 0.026272142 -0.049306337][0.66827118 0.70056057 0.71647733 0.72694671 0.73057616 0.74364597 0.77005643 0.78264517 0.75381345 0.67176819 0.54096693 0.37451425 0.19984321 0.055897221 -0.034631655][0.67985731 0.69559032 0.69062328 0.68230593 0.67019022 0.67384869 0.70380926 0.73133045 0.72268182 0.66103768 0.54796374 0.3926512 0.22004661 0.071767412 -0.025004609][0.62291789 0.61457205 0.5845589 0.55606508 0.52827227 0.52019238 0.547913 0.58542055 0.59654874 0.5604704 0.47611612 0.34878194 0.1977932 0.0625078 -0.028377717][0.51145351 0.47846457 0.42783377 0.38529721 0.34835571 0.33162355 0.35345092 0.39362976 0.41813236 0.40414822 0.3485035 0.25458127 0.13632041 0.02684189 -0.047616761][0.36990175 0.31984359 0.25883323 0.21136679 0.17350289 0.154202 0.17011912 0.20736036 0.23739266 0.23800448 0.20473616 0.14072149 0.056778498 -0.021799089 -0.0745111][0.22114353 0.16424626 0.103306 0.05877981 0.026668504 0.011582461 0.026035447 0.06026509 0.091851793 0.10123795 0.083689295 0.04248824 -0.012788632 -0.063783616 -0.096511193][0.076965354 0.024205934 -0.027380465 -0.062613793 -0.084146269 -0.089787483 -0.071773313 -0.037737168 -0.0053091929 0.0096215522 0.0027981799 -0.022637704 -0.057816189 -0.089459196 -0.10824439]]...]
INFO - root - 2017-12-11 02:58:32.079258: step 82310, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:43m:36s remains)
INFO - root - 2017-12-11 02:58:34.830723: step 82320, loss = 0.69, batch loss = 0.64 (28.5 examples/sec; 0.281 sec/batch; 19h:32m:18s remains)
INFO - root - 2017-12-11 02:58:37.552041: step 82330, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.270 sec/batch; 18h:45m:34s remains)
INFO - root - 2017-12-11 02:58:40.277111: step 82340, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 18h:39m:56s remains)
INFO - root - 2017-12-11 02:58:42.979281: step 82350, loss = 0.70, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 19h:00m:18s remains)
INFO - root - 2017-12-11 02:58:45.762154: step 82360, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.270 sec/batch; 18h:44m:46s remains)
INFO - root - 2017-12-11 02:58:48.520083: step 82370, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 18h:19m:23s remains)
INFO - root - 2017-12-11 02:58:51.230590: step 82380, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 19h:02m:32s remains)
INFO - root - 2017-12-11 02:58:53.963927: step 82390, loss = 0.71, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 18h:54m:52s remains)
INFO - root - 2017-12-11 02:58:56.693182: step 82400, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.269 sec/batch; 18h:40m:02s remains)
2017-12-11 02:58:57.177968: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19534838 0.19562542 0.18840605 0.16564927 0.13038628 0.098777674 0.083141573 0.083956257 0.092695482 0.10356233 0.11794747 0.13202995 0.13819289 0.12876047 0.10529874][0.15045618 0.15362807 0.1500569 0.13134722 0.10002343 0.069783285 0.052002214 0.047597416 0.04941871 0.053337004 0.062218692 0.073283568 0.079335704 0.072371796 0.053546209][0.10004488 0.1052652 0.10383504 0.0895044 0.065819919 0.042348407 0.027312052 0.02084551 0.01820779 0.016216872 0.017506264 0.022308432 0.025269216 0.019246668 0.0047466168][0.076356813 0.084265128 0.083260208 0.07090465 0.054131955 0.038763028 0.029946756 0.026197487 0.023720182 0.018522542 0.011722491 0.00782584 0.0049525853 -0.0029506094 -0.01622426][0.088003114 0.10046434 0.099354677 0.086142324 0.072749846 0.063023388 0.060454607 0.062388565 0.063901491 0.05880532 0.045699503 0.033530015 0.024148272 0.012106192 -0.0042010555][0.12133633 0.13926886 0.13848718 0.12325749 0.10988995 0.10229077 0.10431002 0.11322956 0.12129141 0.11962107 0.10410299 0.086986072 0.072874695 0.0556403 0.033691827][0.1545182 0.17660245 0.17658161 0.16006114 0.1454058 0.13830484 0.14432129 0.16117193 0.17739096 0.18164805 0.1675707 0.14891908 0.13216548 0.11031583 0.083137684][0.17168321 0.19463773 0.19512069 0.17916223 0.16408552 0.15800311 0.16831626 0.19297808 0.21730396 0.22797671 0.21722703 0.19902045 0.18150356 0.15739061 0.12784357][0.16997859 0.1889468 0.188801 0.17557488 0.16239069 0.15875636 0.17271662 0.20240685 0.23217039 0.24805415 0.24145891 0.22500397 0.20876813 0.18594633 0.15789126][0.1546738 0.16614527 0.16531214 0.15771879 0.14958236 0.14980119 0.16557649 0.19573511 0.22700864 0.24627733 0.24413337 0.23025618 0.21624793 0.1967729 0.17239144][0.13050009 0.13565803 0.136721 0.13773145 0.13704325 0.14145988 0.15675102 0.18304443 0.21172878 0.23165382 0.23275514 0.22104865 0.20914879 0.19390199 0.17429839][0.1025157 0.10560091 0.11174826 0.12227433 0.12926611 0.1369921 0.14994977 0.1701085 0.19418468 0.21362348 0.21748164 0.20797355 0.19861609 0.18858333 0.1742809][0.075405337 0.079993166 0.091602162 0.10881311 0.12069337 0.12959211 0.13980274 0.15551302 0.17769532 0.19981591 0.20943333 0.20485972 0.19966695 0.19444282 0.18376485][0.051703725 0.058590822 0.073254064 0.092398651 0.10550774 0.11411299 0.12296479 0.1381676 0.16341269 0.19368933 0.21455866 0.21963505 0.22068037 0.21886079 0.2094354][0.03228759 0.040310126 0.054233987 0.070478737 0.081622995 0.089631811 0.10002384 0.11953579 0.15261075 0.19474041 0.22976401 0.2468072 0.25389525 0.25278834 0.24236082]]...]
INFO - root - 2017-12-11 02:58:59.971606: step 82410, loss = 0.68, batch loss = 0.62 (29.4 examples/sec; 0.272 sec/batch; 18h:52m:57s remains)
INFO - root - 2017-12-11 02:59:02.732054: step 82420, loss = 0.70, batch loss = 0.64 (28.2 examples/sec; 0.284 sec/batch; 19h:41m:58s remains)
INFO - root - 2017-12-11 02:59:05.445961: step 82430, loss = 0.68, batch loss = 0.62 (28.4 examples/sec; 0.281 sec/batch; 19h:33m:00s remains)
INFO - root - 2017-12-11 02:59:08.171753: step 82440, loss = 0.69, batch loss = 0.63 (28.2 examples/sec; 0.284 sec/batch; 19h:41m:46s remains)
INFO - root - 2017-12-11 02:59:10.896785: step 82450, loss = 0.70, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 19h:01m:21s remains)
INFO - root - 2017-12-11 02:59:13.715303: step 82460, loss = 0.68, batch loss = 0.62 (27.7 examples/sec; 0.288 sec/batch; 20h:01m:29s remains)
INFO - root - 2017-12-11 02:59:16.462043: step 82470, loss = 0.70, batch loss = 0.64 (28.7 examples/sec; 0.278 sec/batch; 19h:20m:05s remains)
INFO - root - 2017-12-11 02:59:19.168594: step 82480, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 18h:19m:38s remains)
INFO - root - 2017-12-11 02:59:21.956097: step 82490, loss = 0.68, batch loss = 0.62 (28.3 examples/sec; 0.283 sec/batch; 19h:37m:29s remains)
INFO - root - 2017-12-11 02:59:24.793554: step 82500, loss = 0.71, batch loss = 0.65 (27.6 examples/sec; 0.290 sec/batch; 20h:07m:01s remains)
2017-12-11 02:59:25.309610: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.041157782 0.046099376 0.0628823 0.086706087 0.1115849 0.13088462 0.14013943 0.13929924 0.13085938 0.11755081 0.10328729 0.091509 0.084971331 0.081446022 0.078815274][0.05352629 0.055402379 0.074326575 0.10552556 0.14180076 0.1733216 0.19119507 0.19271165 0.18139631 0.1641311 0.14628813 0.13088109 0.12069611 0.11415579 0.10799873][0.059017271 0.058315236 0.079540074 0.11833326 0.16601378 0.20941 0.23541915 0.23877119 0.22377546 0.20080549 0.17824815 0.16010791 0.14855964 0.14098246 0.13233729][0.057255864 0.055278849 0.079018384 0.12436284 0.18109742 0.23383458 0.26692688 0.27322316 0.25689444 0.23009318 0.20348534 0.18293968 0.17174211 0.16677612 0.15931471][0.050771553 0.048879948 0.07444565 0.12357689 0.18526179 0.24371965 0.28227198 0.29260445 0.27819183 0.25089797 0.22214648 0.20035253 0.19105181 0.19067967 0.18697941][0.044148743 0.0428062 0.067470841 0.11558078 0.17721836 0.23804738 0.28107241 0.29665285 0.28672072 0.2619521 0.23324877 0.2112333 0.20365722 0.20662287 0.20634077][0.040680289 0.039503489 0.059436787 0.1003492 0.15530007 0.21254972 0.25632927 0.27590328 0.27152744 0.25249091 0.22917064 0.21230841 0.20840324 0.21297783 0.21351248][0.04785496 0.046428464 0.058674108 0.08713194 0.12841898 0.17423479 0.21222615 0.23211607 0.23243608 0.2205853 0.20633765 0.19842887 0.19963431 0.203677 0.20219493][0.067272343 0.065599613 0.069465637 0.083704785 0.10802826 0.13856998 0.16839477 0.18852681 0.19502366 0.19245787 0.18882221 0.19000953 0.19455494 0.19496992 0.18808024][0.094303444 0.093095385 0.091312096 0.094263442 0.10329016 0.11951634 0.14269306 0.16493982 0.17834638 0.18425258 0.18862641 0.1955446 0.19982688 0.19385235 0.17967752][0.11948528 0.12017714 0.11761905 0.11583715 0.11585853 0.1228746 0.143158 0.16911055 0.18883063 0.20140046 0.21104234 0.22014259 0.22089954 0.20724376 0.18604156][0.14072518 0.14258347 0.14176519 0.14047495 0.13786858 0.14112534 0.1608628 0.18937545 0.21281579 0.23049453 0.2456001 0.25796491 0.25711063 0.2389448 0.21322627][0.16175836 0.16165714 0.16037437 0.16011664 0.15849027 0.16238807 0.18308961 0.21264647 0.2377893 0.25940675 0.27976787 0.29571876 0.2947911 0.27510118 0.24750407][0.17817789 0.17181349 0.16519313 0.16353141 0.16454524 0.17313628 0.1972317 0.22784972 0.25400442 0.27806285 0.30182096 0.32012296 0.31993464 0.30197889 0.27636251][0.18363047 0.16932988 0.1547571 0.1494451 0.15259662 0.16689943 0.19431239 0.22407912 0.24811585 0.27010548 0.29220793 0.30911097 0.30907148 0.29523391 0.27584517]]...]
INFO - root - 2017-12-11 02:59:28.073428: step 82510, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 18h:18m:25s remains)
INFO - root - 2017-12-11 02:59:30.787802: step 82520, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:42m:42s remains)
INFO - root - 2017-12-11 02:59:33.482431: step 82530, loss = 0.68, batch loss = 0.62 (29.3 examples/sec; 0.273 sec/batch; 18h:56m:02s remains)
INFO - root - 2017-12-11 02:59:36.255671: step 82540, loss = 0.71, batch loss = 0.65 (27.5 examples/sec; 0.290 sec/batch; 20h:09m:57s remains)
INFO - root - 2017-12-11 02:59:39.040963: step 82550, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.280 sec/batch; 19h:24m:51s remains)
INFO - root - 2017-12-11 02:59:41.805769: step 82560, loss = 0.71, batch loss = 0.65 (28.2 examples/sec; 0.284 sec/batch; 19h:43m:45s remains)
INFO - root - 2017-12-11 02:59:44.518869: step 82570, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 18h:27m:41s remains)
INFO - root - 2017-12-11 02:59:47.211966: step 82580, loss = 0.68, batch loss = 0.62 (28.9 examples/sec; 0.277 sec/batch; 19h:14m:34s remains)
INFO - root - 2017-12-11 02:59:49.991715: step 82590, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 19h:08m:28s remains)
INFO - root - 2017-12-11 02:59:52.748649: step 82600, loss = 0.71, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 19h:01m:08s remains)
2017-12-11 02:59:53.201193: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026816927 0.096512355 0.17813984 0.25200576 0.31407335 0.37259224 0.4108904 0.41866928 0.40452847 0.38811213 0.36789572 0.33027312 0.2833322 0.23981687 0.20634465][0.040102731 0.12503847 0.22766878 0.32413107 0.40708792 0.48404589 0.53735673 0.55240422 0.53978688 0.524954 0.50756258 0.46973157 0.41554993 0.35778993 0.30585024][0.046822466 0.14016603 0.25422245 0.36452577 0.46030134 0.54678237 0.60654718 0.62405789 0.61253 0.59988588 0.58699417 0.55370396 0.49753204 0.42894265 0.35966256][0.052848939 0.15058959 0.269331 0.38538805 0.48570639 0.57202935 0.62796354 0.63902396 0.62315822 0.60927397 0.59769756 0.56622314 0.5080812 0.43191224 0.34957436][0.061658625 0.16288383 0.28458688 0.40379104 0.50471985 0.58350509 0.62704861 0.62437749 0.59920603 0.57969952 0.56516093 0.53325975 0.47394478 0.39321029 0.30309168][0.069013327 0.17405467 0.29976088 0.42383426 0.52566534 0.59493995 0.62308109 0.60473239 0.56807393 0.539645 0.51945305 0.48798409 0.43160346 0.35258037 0.26274556][0.072728746 0.18140221 0.31113642 0.43956569 0.54144675 0.60147762 0.61484343 0.58202648 0.53390253 0.49548474 0.46987879 0.44341666 0.39778036 0.32950142 0.24919692][0.070624776 0.1799607 0.3100912 0.43825296 0.5370127 0.58901554 0.59090692 0.5465064 0.48803172 0.44147867 0.4144806 0.39872679 0.37148964 0.32362163 0.2640017][0.057619404 0.16169459 0.28659156 0.40900379 0.50177056 0.54759008 0.54280084 0.49170548 0.42654866 0.37758645 0.3563841 0.3575111 0.35631624 0.33870855 0.31136966][0.034094393 0.12781492 0.24251778 0.35490993 0.43920332 0.48021761 0.47480166 0.42586696 0.3623625 0.3189798 0.31029409 0.33135125 0.35693768 0.37119508 0.3788957][0.0072993627 0.088637993 0.19139647 0.29117391 0.3636874 0.3992238 0.39779812 0.36022678 0.30922052 0.2810733 0.29041478 0.33003744 0.37459305 0.4101209 0.44166371][-0.01259787 0.058299351 0.15009318 0.23604785 0.29465303 0.32441488 0.32839224 0.30600569 0.27443606 0.26866096 0.29888704 0.35230276 0.40365919 0.44325247 0.47989026][-0.01679056 0.048815738 0.13223481 0.2052452 0.25051406 0.27454183 0.28075045 0.26919979 0.25644836 0.27434805 0.32548881 0.38759902 0.43696973 0.46894583 0.49697945][-0.0086116036 0.056306828 0.13432924 0.19851798 0.23538336 0.25523603 0.25913253 0.25076354 0.25158581 0.28902882 0.35621405 0.41989169 0.4600884 0.47812155 0.49061069][0.0049159853 0.0730571 0.15084942 0.21272418 0.24620822 0.26120925 0.25748426 0.2448062 0.25087354 0.29893 0.373204 0.43273333 0.46058843 0.4626849 0.4566938]]...]
INFO - root - 2017-12-11 02:59:55.904744: step 82610, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 18h:20m:17s remains)
INFO - root - 2017-12-11 02:59:58.606911: step 82620, loss = 0.68, batch loss = 0.62 (29.3 examples/sec; 0.273 sec/batch; 18h:58m:38s remains)
INFO - root - 2017-12-11 03:00:01.321750: step 82630, loss = 0.72, batch loss = 0.66 (29.3 examples/sec; 0.273 sec/batch; 18h:58m:05s remains)
INFO - root - 2017-12-11 03:00:04.056631: step 82640, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 18h:33m:43s remains)
INFO - root - 2017-12-11 03:00:06.808446: step 82650, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 19h:12m:24s remains)
INFO - root - 2017-12-11 03:00:09.563221: step 82660, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.274 sec/batch; 19h:02m:56s remains)
INFO - root - 2017-12-11 03:00:12.258573: step 82670, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 18h:48m:01s remains)
INFO - root - 2017-12-11 03:00:15.057655: step 82680, loss = 0.70, batch loss = 0.64 (28.7 examples/sec; 0.279 sec/batch; 19h:21m:35s remains)
INFO - root - 2017-12-11 03:00:17.828874: step 82690, loss = 0.68, batch loss = 0.62 (29.6 examples/sec; 0.271 sec/batch; 18h:47m:03s remains)
INFO - root - 2017-12-11 03:00:20.617258: step 82700, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 19h:13m:07s remains)
2017-12-11 03:00:21.168304: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0071551134 -0.018207839 -0.026100198 -0.027993156 -0.02530076 -0.022114409 -0.021853749 -0.026267003 -0.034048334 -0.040495858 -0.042786069 -0.041260455 -0.038893182 -0.037490442 -0.03664466][-0.014862807 -0.026158914 -0.031591553 -0.0288381 -0.020721558 -0.012564915 -0.0092632463 -0.013707652 -0.024565004 -0.035989031 -0.043487918 -0.045992564 -0.045846246 -0.045207575 -0.044589687][-0.02031574 -0.029528134 -0.028939068 -0.016226724 0.0044399397 0.025283054 0.03775207 0.035765748 0.01908421 -0.00489862 -0.027357614 -0.042484574 -0.049944337 -0.052031785 -0.051116593][-0.021301726 -0.024965808 -0.013467351 0.016559955 0.059730224 0.10374849 0.13347663 0.13697641 0.11083053 0.064508729 0.014556297 -0.024581309 -0.047535997 -0.056494374 -0.0564583][-0.019128755 -0.013286998 0.014489135 0.069584578 0.14533249 0.22278813 0.27748671 0.289505 0.251089 0.17429563 0.085931808 0.012217949 -0.034438059 -0.055687957 -0.059168559][-0.014643341 0.0030161974 0.049356382 0.13241893 0.24415138 0.35802415 0.43950531 0.46084121 0.41026443 0.30221039 0.17359784 0.062745914 -0.010696301 -0.047538981 -0.057132196][-0.010437928 0.017496442 0.078708805 0.18341091 0.32249969 0.46348441 0.56423867 0.59220755 0.53368706 0.40434879 0.24745575 0.10947864 0.015446091 -0.034509685 -0.050107181][-0.01191153 0.020329569 0.086383395 0.19736552 0.34414247 0.49258918 0.59837884 0.62861121 0.5701437 0.43786269 0.27532819 0.13041064 0.030194776 -0.024227822 -0.041856028][-0.021465348 0.0070005688 0.065292679 0.16351937 0.29369214 0.42565188 0.51984 0.54788816 0.49909011 0.38530228 0.24364144 0.11594746 0.027228786 -0.02065851 -0.034930285][-0.035668086 -0.017526036 0.02299173 0.093511075 0.18846412 0.28608167 0.35703152 0.3804684 0.34860736 0.26880002 0.16714548 0.074305162 0.0099448441 -0.023679798 -0.031166002][-0.049666435 -0.044021118 -0.024836287 0.01263262 0.065800391 0.12289559 0.16695473 0.1852973 0.17258435 0.1314301 0.075675949 0.023259962 -0.013070873 -0.030689577 -0.031407304][-0.059704319 -0.064210713 -0.06268964 -0.052783694 -0.034545206 -0.011245557 0.010769224 0.025288029 0.028512506 0.019450225 0.0020534191 -0.016618749 -0.030164216 -0.03589268 -0.033185221][-0.063829713 -0.073536016 -0.081596151 -0.087243415 -0.089246251 -0.086145893 -0.07779181 -0.066119306 -0.053634066 -0.043015972 -0.0363158 -0.033888321 -0.034138147 -0.034717366 -0.033335816][-0.063170455 -0.073157668 -0.082499072 -0.09168943 -0.0997815 -0.10422324 -0.10237464 -0.093586661 -0.0789073 -0.060990009 -0.044195931 -0.032774821 -0.028161179 -0.028887479 -0.031801131][-0.061221864 -0.069250159 -0.075561643 -0.081508286 -0.087204546 -0.091266945 -0.091329709 -0.085778981 -0.0741391 -0.057728138 -0.040568896 -0.027994931 -0.022945046 -0.025073012 -0.031469796]]...]
INFO - root - 2017-12-11 03:00:23.964389: step 82710, loss = 0.68, batch loss = 0.62 (29.3 examples/sec; 0.273 sec/batch; 18h:57m:18s remains)
INFO - root - 2017-12-11 03:00:26.682213: step 82720, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 18h:26m:32s remains)
INFO - root - 2017-12-11 03:00:29.436119: step 82730, loss = 0.68, batch loss = 0.62 (28.9 examples/sec; 0.277 sec/batch; 19h:12m:49s remains)
INFO - root - 2017-12-11 03:00:32.180867: step 82740, loss = 0.69, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 19h:05m:48s remains)
INFO - root - 2017-12-11 03:00:34.942172: step 82750, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:55m:34s remains)
INFO - root - 2017-12-11 03:00:37.776636: step 82760, loss = 0.71, batch loss = 0.65 (28.5 examples/sec; 0.281 sec/batch; 19h:27m:33s remains)
INFO - root - 2017-12-11 03:00:40.491618: step 82770, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:55m:12s remains)
INFO - root - 2017-12-11 03:00:43.206158: step 82780, loss = 0.71, batch loss = 0.65 (29.8 examples/sec; 0.269 sec/batch; 18h:37m:52s remains)
INFO - root - 2017-12-11 03:00:45.967098: step 82790, loss = 0.71, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 18h:59m:18s remains)
INFO - root - 2017-12-11 03:00:48.697180: step 82800, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:39m:39s remains)
2017-12-11 03:00:49.236889: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24825898 0.26095107 0.25247946 0.22642647 0.19423296 0.17937903 0.17706437 0.17485909 0.167297 0.15756594 0.14044389 0.10402931 0.056441128 0.013680443 -0.015393609][0.33449268 0.35029653 0.33532384 0.29448012 0.24509358 0.21766111 0.20888767 0.20412658 0.19208395 0.17286673 0.14346369 0.09523952 0.03961312 -0.0067434926 -0.034544375][0.4169094 0.43870509 0.42247576 0.37373507 0.314729 0.28221652 0.27314571 0.26769972 0.24942772 0.2167263 0.16890913 0.10175392 0.031577241 -0.022784883 -0.051324159][0.50697976 0.53137606 0.51350921 0.46144998 0.39999914 0.36984944 0.3677969 0.36635828 0.34133217 0.29111695 0.2194919 0.12725377 0.035623934 -0.032527637 -0.066021875][0.58374316 0.6075204 0.59155774 0.54727066 0.49798459 0.48499832 0.50379372 0.51521647 0.48490211 0.411987 0.30843168 0.18156776 0.058490735 -0.031452607 -0.075114563][0.63592649 0.65476912 0.64309412 0.6141836 0.58693045 0.59990823 0.64503664 0.67149049 0.63579059 0.53885829 0.40207985 0.23947337 0.084392555 -0.027376221 -0.081193194][0.63951719 0.65824658 0.65785873 0.654493 0.65909672 0.70237517 0.77032059 0.80477911 0.75871366 0.63708961 0.47080672 0.27972797 0.10182706 -0.023450609 -0.0824865][0.57679105 0.5985567 0.61455369 0.64036 0.67694336 0.74482423 0.82496291 0.8591516 0.80313271 0.66680324 0.48803902 0.288988 0.10728572 -0.018533371 -0.077650778][0.44512707 0.46774703 0.49568206 0.5416972 0.59825611 0.67669982 0.756546 0.78641546 0.72859621 0.59704453 0.43175811 0.25298658 0.091253445 -0.020590188 -0.073524319][0.27421576 0.29642358 0.3302277 0.38418636 0.44645888 0.52338189 0.59587377 0.62268776 0.57423711 0.46563259 0.33343944 0.19248295 0.063746236 -0.026950318 -0.070622861][0.1210651 0.14022651 0.17038676 0.21652348 0.26746288 0.32918084 0.38672298 0.41079533 0.3794888 0.30493453 0.21535729 0.11874674 0.028020792 -0.037895214 -0.069887765][0.017725976 0.033429664 0.055517133 0.086082295 0.11744405 0.15764186 0.1973387 0.21819142 0.2038862 0.16125517 0.10921685 0.05067046 -0.0065691816 -0.04962042 -0.070124857][-0.036688093 -0.026739148 -0.014915688 -0.0015563002 0.0099190008 0.029894602 0.054018803 0.071513474 0.069557361 0.050089654 0.024363186 -0.0071579251 -0.039302628 -0.063791439 -0.073990174][-0.065055028 -0.062162943 -0.060234256 -0.060671333 -0.063568451 -0.05790817 -0.045259442 -0.032265335 -0.028763613 -0.035358466 -0.0455214 -0.059374664 -0.07304024 -0.08193174 -0.082194507][-0.071789049 -0.074177004 -0.078019522 -0.085105821 -0.093926206 -0.095202863 -0.0898445 -0.082263753 -0.079335228 -0.082046695 -0.08619146 -0.091402471 -0.094265155 -0.093601122 -0.08776357]]...]
INFO - root - 2017-12-11 03:00:51.933871: step 82810, loss = 0.71, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 18h:52m:56s remains)
INFO - root - 2017-12-11 03:00:54.703811: step 82820, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 18h:30m:32s remains)
INFO - root - 2017-12-11 03:00:57.385239: step 82830, loss = 0.72, batch loss = 0.66 (30.0 examples/sec; 0.267 sec/batch; 18h:30m:40s remains)
INFO - root - 2017-12-11 03:01:00.140328: step 82840, loss = 0.68, batch loss = 0.63 (28.4 examples/sec; 0.282 sec/batch; 19h:32m:57s remains)
INFO - root - 2017-12-11 03:01:02.859638: step 82850, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 18h:20m:18s remains)
INFO - root - 2017-12-11 03:01:05.635724: step 82860, loss = 0.69, batch loss = 0.63 (28.8 examples/sec; 0.278 sec/batch; 19h:17m:44s remains)
INFO - root - 2017-12-11 03:01:08.336804: step 82870, loss = 0.68, batch loss = 0.62 (29.0 examples/sec; 0.276 sec/batch; 19h:09m:26s remains)
INFO - root - 2017-12-11 03:01:11.082604: step 82880, loss = 0.69, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 19h:12m:35s remains)
INFO - root - 2017-12-11 03:01:13.834338: step 82890, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 18h:46m:53s remains)
INFO - root - 2017-12-11 03:01:16.568737: step 82900, loss = 0.72, batch loss = 0.66 (29.1 examples/sec; 0.275 sec/batch; 19h:02m:01s remains)
2017-12-11 03:01:17.016554: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11292184 0.1524554 0.17774992 0.18104774 0.16848759 0.14370738 0.10968677 0.069886893 0.036646686 0.019111598 0.016769992 0.028764836 0.052058551 0.084125847 0.11975257][0.079480745 0.10881253 0.12425902 0.11910605 0.10146059 0.07679946 0.047561165 0.015702436 -0.0082800779 -0.018973786 -0.01785611 -0.0022525121 0.028939944 0.074838862 0.12922138][0.033479419 0.05139843 0.057502061 0.047333006 0.029907659 0.012003249 -0.0053679468 -0.022898223 -0.033510976 -0.036529373 -0.033848029 -0.019451329 0.013181361 0.06783402 0.13972241][0.010283875 0.025685456 0.031495303 0.025208687 0.015557244 0.010884235 0.010042702 0.0086856121 0.0099445572 0.010102408 0.0080813263 0.011919854 0.033740934 0.084162928 0.16412874][0.024386652 0.04963316 0.067252629 0.075369358 0.08088769 0.094282 0.11214398 0.12580955 0.13484907 0.13223784 0.11793778 0.099661089 0.094653361 0.12221868 0.19276808][0.065133594 0.1092517 0.14758545 0.17721601 0.20138736 0.23361039 0.26892012 0.29416996 0.30610853 0.2957015 0.26361734 0.21485518 0.17096564 0.16010118 0.20305754][0.10675541 0.1703507 0.23024952 0.28093597 0.32068455 0.36668178 0.41367739 0.44458914 0.45435145 0.43385249 0.38424715 0.30608106 0.22366187 0.17222895 0.17948659][0.12356155 0.19740781 0.26956585 0.33135921 0.37653157 0.42471364 0.47217816 0.50072241 0.50431448 0.47537437 0.41561946 0.32165748 0.21754405 0.14077787 0.12010328][0.10473919 0.17384103 0.24262188 0.30003041 0.3379668 0.37574708 0.41222969 0.43144846 0.42788276 0.39629629 0.33860517 0.24949251 0.14949389 0.072168864 0.04207512][0.054531634 0.10477214 0.15506534 0.1944546 0.21615031 0.23576921 0.25459418 0.2616154 0.2534526 0.22684185 0.18311906 0.117181 0.04371722 -0.012572774 -0.03474661][-0.0098819872 0.014555636 0.039424296 0.055854749 0.060126942 0.06227 0.064859055 0.062531382 0.054160073 0.037867691 0.013892724 -0.021119114 -0.058794331 -0.085330009 -0.091712579][-0.065067016 -0.062768109 -0.059123691 -0.060178552 -0.066873372 -0.074432962 -0.08049956 -0.086385123 -0.091765672 -0.097529165 -0.1044402 -0.11398955 -0.12263849 -0.12547307 -0.11954079][-0.095030494 -0.10422157 -0.11099762 -0.11914781 -0.12825602 -0.13701339 -0.14408408 -0.14903881 -0.15116811 -0.15075645 -0.14879328 -0.14565033 -0.1405334 -0.13331358 -0.12427031][-0.099518485 -0.1103083 -0.11763195 -0.12452235 -0.13047583 -0.1356357 -0.13963597 -0.1420214 -0.14234047 -0.14048769 -0.13706385 -0.13207693 -0.12578872 -0.11928357 -0.1136568][-0.091134705 -0.098885641 -0.10270856 -0.10581172 -0.10803317 -0.10975798 -0.11101431 -0.11166998 -0.111572 -0.11056373 -0.10881996 -0.1062938 -0.10334639 -0.10074533 -0.099247217]]...]
INFO - root - 2017-12-11 03:01:19.764603: step 82910, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:42m:32s remains)
INFO - root - 2017-12-11 03:01:22.529896: step 82920, loss = 0.68, batch loss = 0.62 (29.5 examples/sec; 0.271 sec/batch; 18h:48m:50s remains)
INFO - root - 2017-12-11 03:01:25.228392: step 82930, loss = 0.68, batch loss = 0.63 (28.3 examples/sec; 0.283 sec/batch; 19h:35m:16s remains)
INFO - root - 2017-12-11 03:01:27.918127: step 82940, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.267 sec/batch; 18h:31m:54s remains)
INFO - root - 2017-12-11 03:01:30.636226: step 82950, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 18h:34m:14s remains)
INFO - root - 2017-12-11 03:01:33.364788: step 82960, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 18h:35m:02s remains)
INFO - root - 2017-12-11 03:01:36.030973: step 82970, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 18h:17m:07s remains)
INFO - root - 2017-12-11 03:01:38.763897: step 82980, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 19h:00m:30s remains)
INFO - root - 2017-12-11 03:01:41.449508: step 82990, loss = 0.71, batch loss = 0.65 (28.8 examples/sec; 0.277 sec/batch; 19h:13m:35s remains)
INFO - root - 2017-12-11 03:01:44.134808: step 83000, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 18h:15m:40s remains)
2017-12-11 03:01:44.581116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.028972078 0.0018471146 0.05044141 0.11517743 0.18137799 0.22868414 0.25021726 0.26226246 0.27297521 0.26432595 0.2344851 0.2061258 0.1998975 0.21862565 0.23817262][-0.028190043 0.0051113209 0.055597976 0.12084988 0.18539378 0.23021828 0.2496258 0.25867268 0.26543409 0.25527677 0.23004633 0.21158497 0.2182347 0.25307438 0.287532][-0.025132714 0.011766945 0.064365819 0.12787803 0.18763398 0.22858815 0.2461509 0.25148132 0.25315443 0.24191354 0.22398485 0.21936281 0.23984861 0.28709584 0.33211806][-0.022023523 0.018387452 0.072493412 0.13272695 0.18691722 0.2246182 0.24166767 0.24501863 0.24364893 0.23403467 0.22526087 0.23408848 0.26473451 0.31638125 0.36277473][-0.020915521 0.020065714 0.07284075 0.12744632 0.17479417 0.20868008 0.22534408 0.22895887 0.22801271 0.22350636 0.22521894 0.24575959 0.28237307 0.33085403 0.37063783][-0.020990837 0.018250214 0.067838848 0.11625162 0.15665682 0.18624252 0.20190892 0.20755102 0.209878 0.21188132 0.22347999 0.25282037 0.29199374 0.33321053 0.36183563][-0.020013947 0.018590035 0.067702614 0.11451183 0.15180165 0.17840993 0.19195719 0.1986371 0.20328514 0.20914976 0.22669837 0.26102871 0.30188993 0.33700955 0.35415536][-0.016342256 0.025332032 0.080165468 0.13387738 0.17501017 0.20126377 0.21094358 0.21505308 0.21823035 0.22318794 0.24144328 0.27741006 0.32160524 0.35535634 0.36354285][-0.01112162 0.036150631 0.10033961 0.16552152 0.21393211 0.24081589 0.2455056 0.24457872 0.2441301 0.24543796 0.2608844 0.29613686 0.34523714 0.3829596 0.38697314][-0.0070541082 0.045122676 0.11686876 0.19128089 0.24551643 0.27285954 0.2736468 0.26820895 0.26467896 0.2625801 0.27473241 0.30863234 0.36301816 0.40738121 0.41145545][-0.0069609606 0.046246707 0.11916495 0.19538276 0.25067663 0.27759504 0.27660328 0.26845443 0.26314005 0.25894064 0.26821107 0.29890171 0.3541902 0.40212464 0.40768251][-0.013544312 0.033463646 0.097805083 0.1654647 0.2152568 0.24023403 0.23970608 0.23138098 0.2257141 0.22077921 0.22757222 0.25255367 0.30199122 0.34753108 0.35454792][-0.02721631 0.005981789 0.052136164 0.10108817 0.13822737 0.15831813 0.1588743 0.15240747 0.14791457 0.14393869 0.14946197 0.16799577 0.20693241 0.24525805 0.25333136][-0.043255657 -0.027428683 -0.0036962035 0.021722093 0.041769344 0.053651165 0.05380382 0.049285296 0.0464508 0.044288117 0.049356215 0.06183536 0.088256538 0.1159276 0.12341625][-0.055479135 -0.054131698 -0.048822559 -0.043012124 -0.038139537 -0.034892168 -0.036601488 -0.040354129 -0.042374268 -0.043231905 -0.038921405 -0.031640723 -0.016871989 -0.0003552456 0.0052657779]]...]
INFO - root - 2017-12-11 03:01:47.328302: step 83010, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.265 sec/batch; 18h:23m:29s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 03:01:50.107074: step 83020, loss = 0.70, batch loss = 0.64 (28.1 examples/sec; 0.285 sec/batch; 19h:45m:43s remains)
INFO - root - 2017-12-11 03:01:52.840934: step 83030, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 18h:52m:35s remains)
INFO - root - 2017-12-11 03:01:55.576261: step 83040, loss = 0.70, batch loss = 0.64 (28.4 examples/sec; 0.282 sec/batch; 19h:32m:29s remains)
INFO - root - 2017-12-11 03:01:58.310672: step 83050, loss = 0.70, batch loss = 0.64 (28.5 examples/sec; 0.281 sec/batch; 19h:28m:36s remains)
INFO - root - 2017-12-11 03:02:01.015411: step 83060, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 18h:51m:50s remains)
INFO - root - 2017-12-11 03:02:03.736623: step 83070, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.266 sec/batch; 18h:27m:42s remains)
INFO - root - 2017-12-11 03:02:06.490646: step 83080, loss = 0.68, batch loss = 0.62 (29.5 examples/sec; 0.272 sec/batch; 18h:48m:43s remains)
INFO - root - 2017-12-11 03:02:09.241812: step 83090, loss = 0.68, batch loss = 0.62 (29.6 examples/sec; 0.271 sec/batch; 18h:44m:50s remains)
INFO - root - 2017-12-11 03:02:11.935574: step 83100, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 18h:46m:14s remains)
2017-12-11 03:02:12.388394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0847415 -0.083375521 -0.077050388 -0.06912829 -0.061625864 -0.057507724 -0.058969896 -0.064967252 -0.072246924 -0.077676408 -0.078877538 -0.075559773 -0.069192551 -0.062372059 -0.057268236][-0.083311021 -0.082114331 -0.069780014 -0.051263586 -0.031310033 -0.017317215 -0.015121497 -0.023854962 -0.037792791 -0.051626649 -0.060921587 -0.063175738 -0.057967216 -0.04818638 -0.037057795][-0.053841509 -0.056873627 -0.040805083 -0.01085443 0.025375891 0.054831758 0.065937668 0.058272891 0.039410356 0.016155992 -0.004380099 -0.015224583 -0.012656502 -3.0073168e-05 0.017790152][0.0076187863 -0.0037815934 0.01383395 0.056102082 0.11268017 0.16402796 0.19087225 0.18930405 0.16735129 0.13373385 0.098553807 0.07493525 0.071528628 0.084871255 0.10856809][0.091885418 0.073436625 0.096210092 0.15711169 0.24269423 0.32598743 0.37823275 0.38880327 0.3649123 0.3176617 0.26074696 0.21522284 0.19710223 0.20442939 0.22820012][0.17898564 0.1615313 0.19868946 0.28857639 0.41402829 0.54005677 0.62660229 0.65290797 0.62482595 0.556439 0.46630034 0.38577992 0.34106472 0.33291656 0.34778994][0.24111702 0.23624799 0.29685566 0.42177168 0.59212118 0.76561344 0.89025158 0.93265253 0.89670885 0.7990464 0.66334611 0.53403872 0.45056245 0.41781557 0.4164623][0.25700673 0.27625698 0.36570856 0.522969 0.73015052 0.94141227 1.0962067 1.1498296 1.1040955 0.97627103 0.79302329 0.610805 0.48280662 0.42022315 0.39818579][0.21703367 0.265873 0.38135627 0.55711162 0.778256 1.0010251 1.1641055 1.2192718 1.1676295 1.0237533 0.81131691 0.59214395 0.42915261 0.34019294 0.29892391][0.13593653 0.20727496 0.33478063 0.50637466 0.71034926 0.9100948 1.0524395 1.0966109 1.045347 0.90769851 0.69880116 0.47601405 0.30363479 0.20359047 0.15264203][0.041576266 0.11842984 0.23820066 0.38456354 0.54764491 0.69986218 0.801508 0.82588935 0.77847844 0.66340357 0.48650059 0.29330146 0.14057168 0.049926747 0.0039827731][-0.040842369 0.021544889 0.11488029 0.2220576 0.3343285 0.43289277 0.49155977 0.49726796 0.45708397 0.37246963 0.24291255 0.099404894 -0.014462827 -0.081021853 -0.11094014][-0.094185427 -0.058130521 -0.0001537018 0.0654801 0.13136081 0.18557738 0.21210726 0.20684193 0.17645682 0.12188061 0.040788736 -0.049030207 -0.11877403 -0.15665177 -0.16788113][-0.11102151 -0.10084334 -0.07530012 -0.043685604 -0.011777516 0.013113001 0.021470314 0.013447929 -0.0057256869 -0.034529969 -0.074616641 -0.11743772 -0.14741556 -0.159314 -0.15558194][-0.099663243 -0.10618168 -0.1022127 -0.0925428 -0.08083269 -0.07148505 -0.070116185 -0.075291313 -0.082800724 -0.090813451 -0.10000255 -0.10701761 -0.10578678 -0.096882418 -0.083277732]]...]
INFO - root - 2017-12-11 03:02:15.171088: step 83110, loss = 0.70, batch loss = 0.64 (28.5 examples/sec; 0.281 sec/batch; 19h:26m:14s remains)
INFO - root - 2017-12-11 03:02:17.902760: step 83120, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 18h:35m:24s remains)
INFO - root - 2017-12-11 03:02:20.613606: step 83130, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:56m:26s remains)
INFO - root - 2017-12-11 03:02:23.396737: step 83140, loss = 0.71, batch loss = 0.65 (28.9 examples/sec; 0.277 sec/batch; 19h:11m:00s remains)
INFO - root - 2017-12-11 03:02:26.091964: step 83150, loss = 0.68, batch loss = 0.62 (30.4 examples/sec; 0.263 sec/batch; 18h:14m:11s remains)
INFO - root - 2017-12-11 03:02:28.870871: step 83160, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 18h:45m:09s remains)
INFO - root - 2017-12-11 03:02:31.633353: step 83170, loss = 0.72, batch loss = 0.66 (29.6 examples/sec; 0.270 sec/batch; 18h:41m:34s remains)
INFO - root - 2017-12-11 03:02:34.324987: step 83180, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 17h:56m:49s remains)
INFO - root - 2017-12-11 03:02:37.089626: step 83190, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 18h:49m:50s remains)
INFO - root - 2017-12-11 03:02:39.848174: step 83200, loss = 0.71, batch loss = 0.65 (28.2 examples/sec; 0.284 sec/batch; 19h:39m:11s remains)
2017-12-11 03:02:40.345620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.033399794 -0.014716834 0.010703053 0.033010043 0.045860238 0.049229167 0.045677554 0.033264615 0.014605762 -0.0016385842 -0.0064920234 0.00069768145 0.00984557 0.014590457 0.011579506][-0.0095965024 0.0250944 0.071012393 0.11296356 0.14019644 0.1507137 0.1471106 0.12605549 0.092148393 0.061003011 0.048337191 0.055483207 0.066592157 0.071707793 0.0663077][0.021991808 0.076332 0.14698306 0.21237077 0.25645679 0.27486214 0.27109939 0.24116462 0.19170395 0.14530346 0.12408565 0.12893333 0.13854435 0.14029562 0.13022132][0.049180117 0.12137096 0.21509416 0.3028971 0.3638241 0.391495 0.39047647 0.35619307 0.29574603 0.23691252 0.2069893 0.20609391 0.20957008 0.20316014 0.18514496][0.064173982 0.14866839 0.25893396 0.36379817 0.43886268 0.47592354 0.47993913 0.44597325 0.38054466 0.31377968 0.27626166 0.26839557 0.26460293 0.24954776 0.2241084][0.067080885 0.15738589 0.27656239 0.39186385 0.47716367 0.52272552 0.53298283 0.50202012 0.43529427 0.36288697 0.31749243 0.3013387 0.29034889 0.26983267 0.24257405][0.062007792 0.15247457 0.27333242 0.39174888 0.48150325 0.53208131 0.546752 0.51813477 0.45080113 0.37425849 0.322261 0.29856735 0.28217366 0.26050207 0.23787786][0.052298557 0.13812213 0.25400731 0.36793682 0.45444202 0.50271451 0.515615 0.48572296 0.41817662 0.34222102 0.29110408 0.26809904 0.25360885 0.23658387 0.2223064][0.039149065 0.11616411 0.22056615 0.32264003 0.39866987 0.43761557 0.44259468 0.40810066 0.34125295 0.27067506 0.22760686 0.2136583 0.20928103 0.20336562 0.19989699][0.024003275 0.089743137 0.17840487 0.26364729 0.3243846 0.34980032 0.34355235 0.30271971 0.23725888 0.17511442 0.14447004 0.14474349 0.15520701 0.16383974 0.17166294][0.0046860012 0.057063561 0.12747829 0.19390407 0.23882133 0.25257513 0.23854905 0.19564129 0.13528864 0.083725564 0.065115973 0.076943986 0.098334841 0.11694254 0.13113555][-0.019843424 0.0167882 0.067184448 0.11394532 0.14399101 0.14975245 0.13374324 0.095914833 0.046963528 0.0094493739 0.0020549661 0.020804472 0.046321683 0.067080669 0.080699563][-0.045169778 -0.024863554 0.0055177375 0.033131972 0.049352974 0.049293958 0.0349699 0.0074269497 -0.02476109 -0.045157235 -0.041932192 -0.020165712 0.0041428483 0.021866489 0.031047456][-0.066449478 -0.060220826 -0.046979152 -0.035252087 -0.029715901 -0.032761972 -0.042868897 -0.058418889 -0.073769979 -0.079403825 -0.0696674 -0.049699176 -0.030654564 -0.018597415 -0.014560337][-0.079187982 -0.082406923 -0.080457777 -0.078688659 -0.078926593 -0.0819946 -0.08711195 -0.093030274 -0.096735157 -0.0940224 -0.083381049 -0.06902691 -0.05743419 -0.051791307 -0.052087963]]...]
INFO - root - 2017-12-11 03:02:43.052225: step 83210, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 19h:11m:48s remains)
INFO - root - 2017-12-11 03:02:45.802086: step 83220, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.261 sec/batch; 18h:05m:28s remains)
INFO - root - 2017-12-11 03:02:48.526676: step 83230, loss = 0.70, batch loss = 0.64 (28.2 examples/sec; 0.283 sec/batch; 19h:36m:58s remains)
INFO - root - 2017-12-11 03:02:51.247478: step 83240, loss = 0.70, batch loss = 0.64 (28.7 examples/sec; 0.279 sec/batch; 19h:17m:33s remains)
INFO - root - 2017-12-11 03:02:53.996127: step 83250, loss = 0.72, batch loss = 0.66 (29.1 examples/sec; 0.275 sec/batch; 19h:03m:00s remains)
INFO - root - 2017-12-11 03:02:56.777835: step 83260, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 19h:03m:02s remains)
INFO - root - 2017-12-11 03:02:59.447330: step 83270, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 18h:33m:39s remains)
INFO - root - 2017-12-11 03:03:02.142897: step 83280, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:38m:30s remains)
INFO - root - 2017-12-11 03:03:04.886735: step 83290, loss = 0.72, batch loss = 0.66 (28.1 examples/sec; 0.285 sec/batch; 19h:41m:57s remains)
INFO - root - 2017-12-11 03:03:07.631814: step 83300, loss = 0.68, batch loss = 0.63 (29.3 examples/sec; 0.273 sec/batch; 18h:54m:05s remains)
2017-12-11 03:03:08.112646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.020964984 -0.056234736 -0.0737294 -0.071905918 -0.054613397 -0.027303481 0.0036105444 0.028213156 0.03608381 0.020479511 -0.014164002 -0.053752292 -0.084455542 -0.099074654 -0.098956123][0.066961937 0.0073433765 -0.035059802 -0.050647553 -0.039914127 -0.007135428 0.036677603 0.073880374 0.087251082 0.066568226 0.018128367 -0.037967626 -0.08172407 -0.10356645 -0.10540514][0.17114453 0.0922527 0.025990264 -0.0095054554 -0.008252955 0.029252877 0.0881433 0.14091459 0.16151716 0.13541092 0.070812017 -0.0053553241 -0.066420935 -0.099265262 -0.10594407][0.26211935 0.17745098 0.097346857 0.046232637 0.039287355 0.081678271 0.15611939 0.22472169 0.25198236 0.21919219 0.13638178 0.037768032 -0.043419976 -0.089842066 -0.10320836][0.33529171 0.25848028 0.17606777 0.11596853 0.10391181 0.15101348 0.23877244 0.31988746 0.35063082 0.30868709 0.20620476 0.084372669 -0.017938722 -0.079079211 -0.10020875][0.39008725 0.33354339 0.26028121 0.19903167 0.18466076 0.23477973 0.33167988 0.42078567 0.45140961 0.39915982 0.27799982 0.13336875 0.009254883 -0.067812316 -0.097890645][0.41509861 0.38604647 0.33254287 0.28086641 0.27039415 0.32309052 0.42283347 0.51221806 0.53738928 0.47449094 0.33813226 0.17483205 0.032323044 -0.058352489 -0.095990635][0.41636127 0.41251615 0.38306475 0.34750831 0.34461239 0.397212 0.49119797 0.57153362 0.58597207 0.51346463 0.36778858 0.19399327 0.041560456 -0.05571954 -0.0962831][0.40635628 0.42167315 0.41214365 0.39073333 0.39230511 0.43851623 0.51801956 0.58238989 0.58475274 0.50733012 0.36125869 0.18750794 0.035367653 -0.06064133 -0.099374622][0.38978311 0.41546372 0.41921353 0.407323 0.4082756 0.44196224 0.5010649 0.54626441 0.53797239 0.4612585 0.323716 0.16014984 0.017596779 -0.070541129 -0.10417503][0.36257315 0.39218068 0.40394819 0.3989791 0.39795944 0.41785207 0.45539376 0.4814176 0.46403638 0.39078408 0.26673818 0.12062047 -0.0053702015 -0.081354737 -0.1084038][0.33033571 0.36010525 0.37635031 0.37723488 0.3752864 0.38298428 0.40047297 0.40807748 0.38163552 0.31157827 0.20262827 0.078236625 -0.027222384 -0.0893804 -0.10985549][0.30926985 0.3347159 0.35007295 0.35356188 0.35010737 0.34750921 0.34806162 0.33959219 0.3049877 0.23831019 0.14527906 0.04391028 -0.041114472 -0.091228679 -0.10744382][0.30775687 0.32667226 0.33759877 0.3410185 0.33527797 0.32455042 0.31246316 0.29214454 0.25183448 0.18892907 0.11034014 0.028155023 -0.041948948 -0.085368276 -0.10128618][0.31837246 0.33004925 0.33661765 0.33983985 0.33350381 0.31912854 0.30054063 0.27363318 0.23002365 0.16960113 0.10018153 0.028891915 -0.034369394 -0.076725863 -0.095211238]]...]
INFO - root - 2017-12-11 03:03:10.827483: step 83310, loss = 0.69, batch loss = 0.63 (28.8 examples/sec; 0.278 sec/batch; 19h:13m:26s remains)
INFO - root - 2017-12-11 03:03:13.572983: step 83320, loss = 0.68, batch loss = 0.63 (28.4 examples/sec; 0.282 sec/batch; 19h:31m:53s remains)
INFO - root - 2017-12-11 03:03:16.314268: step 83330, loss = 0.68, batch loss = 0.62 (28.1 examples/sec; 0.285 sec/batch; 19h:44m:24s remains)
INFO - root - 2017-12-11 03:03:19.007164: step 83340, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:38m:47s remains)
INFO - root - 2017-12-11 03:03:21.792313: step 83350, loss = 0.71, batch loss = 0.65 (29.0 examples/sec; 0.276 sec/batch; 19h:04m:09s remains)
INFO - root - 2017-12-11 03:03:24.560936: step 83360, loss = 0.69, batch loss = 0.63 (28.1 examples/sec; 0.285 sec/batch; 19h:42m:41s remains)
INFO - root - 2017-12-11 03:03:27.266143: step 83370, loss = 0.69, batch loss = 0.63 (28.9 examples/sec; 0.276 sec/batch; 19h:07m:52s remains)
INFO - root - 2017-12-11 03:03:29.960402: step 83380, loss = 0.70, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 18h:56m:51s remains)
INFO - root - 2017-12-11 03:03:32.647412: step 83390, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.273 sec/batch; 18h:51m:39s remains)
INFO - root - 2017-12-11 03:03:35.328613: step 83400, loss = 0.68, batch loss = 0.63 (30.0 examples/sec; 0.266 sec/batch; 18h:25m:41s remains)
2017-12-11 03:03:35.796398: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053135775 -0.055937666 -0.055869292 -0.055096157 -0.054903094 -0.056623988 -0.059583459 -0.061775807 -0.061605502 -0.058892254 -0.05512812 -0.052644726 -0.053844217 -0.058290858 -0.063045964][-0.040046263 -0.040231574 -0.035247006 -0.027643774 -0.02057153 -0.017354021 -0.017682113 -0.018691614 -0.018476645 -0.01698287 -0.016041704 -0.01831848 -0.026858641 -0.04034356 -0.053508509][-0.017839853 -0.011156489 0.0057654604 0.029197473 0.052142978 0.067697026 0.07454893 0.074810639 0.071332179 0.065429032 0.0565686 0.042714037 0.021214481 -0.0050793448 -0.029315729][0.0053133355 0.022630895 0.058227342 0.10562493 0.15244398 0.18644254 0.20323044 0.20325327 0.1913902 0.17164911 0.14647928 0.1159891 0.078748137 0.038914245 0.0033652575][0.022198029 0.052431885 0.10950665 0.18345562 0.25588945 0.30905581 0.33483124 0.33180529 0.307836 0.27022225 0.22577243 0.17782554 0.1266057 0.076415882 0.032608218][0.032169167 0.074745521 0.15101142 0.24727151 0.34056878 0.40967441 0.44278538 0.43596223 0.39963809 0.34417862 0.28068739 0.2159335 0.15222393 0.09437862 0.045977402][0.042049821 0.094466776 0.18339244 0.29307479 0.39988405 0.482287 0.52524114 0.52044511 0.47806177 0.40939918 0.32816425 0.24452181 0.16476078 0.096658878 0.0435133][0.068290323 0.12978488 0.2248228 0.33733949 0.44712153 0.53615129 0.588138 0.59016162 0.54901612 0.4738397 0.3783924 0.27556312 0.17675273 0.0947926 0.034399539][0.11567528 0.18833949 0.28536603 0.3916536 0.49240687 0.5768699 0.63143247 0.64121282 0.607792 0.53414434 0.43105263 0.31238991 0.19496329 0.097548142 0.027729325][0.17167802 0.25642389 0.35341206 0.44840005 0.53182983 0.60208571 0.65187633 0.66803092 0.646658 0.58211964 0.47936383 0.35069823 0.21768208 0.10537063 0.025472596][0.2183459 0.30976728 0.40179184 0.48109451 0.54254335 0.59232396 0.63077164 0.64918828 0.64002931 0.59014571 0.49717629 0.36965898 0.23115183 0.11125882 0.025506044][0.23465526 0.32216689 0.40103644 0.4599092 0.49740297 0.5241428 0.54661089 0.56161708 0.56096864 0.52748811 0.45278853 0.34077662 0.21346354 0.10074228 0.019497536][0.20646811 0.27976987 0.33928511 0.37632912 0.39200124 0.39809072 0.40418568 0.41193861 0.41493049 0.39586183 0.34340382 0.25735047 0.15550436 0.063999183 -0.0016526718][0.13528141 0.18787463 0.22714774 0.24662 0.24813996 0.24168129 0.23645157 0.236826 0.23936766 0.22999424 0.19790728 0.1404133 0.070007615 0.0066936421 -0.037009858][0.048601605 0.079512835 0.10149698 0.10932673 0.10481681 0.094623342 0.085337758 0.081400715 0.081800237 0.077296704 0.059923176 0.026736667 -0.014751263 -0.051065553 -0.07328409]]...]
INFO - root - 2017-12-11 03:03:38.526682: step 83410, loss = 0.70, batch loss = 0.64 (28.3 examples/sec; 0.283 sec/batch; 19h:33m:43s remains)
INFO - root - 2017-12-11 03:03:41.249756: step 83420, loss = 0.70, batch loss = 0.64 (28.3 examples/sec; 0.283 sec/batch; 19h:34m:30s remains)
INFO - root - 2017-12-11 03:03:44.016504: step 83430, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 18h:50m:51s remains)
INFO - root - 2017-12-11 03:03:46.758495: step 83440, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 19h:05m:41s remains)
INFO - root - 2017-12-11 03:03:49.505170: step 83450, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.268 sec/batch; 18h:31m:59s remains)
INFO - root - 2017-12-11 03:03:52.220572: step 83460, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 19h:04m:37s remains)
INFO - root - 2017-12-11 03:03:54.940208: step 83470, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 18h:50m:33s remains)
INFO - root - 2017-12-11 03:03:57.679905: step 83480, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.271 sec/batch; 18h:43m:06s remains)
INFO - root - 2017-12-11 03:04:00.400853: step 83490, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.270 sec/batch; 18h:42m:01s remains)
INFO - root - 2017-12-11 03:04:03.085314: step 83500, loss = 0.71, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 18h:52m:08s remains)
2017-12-11 03:04:03.533154: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28571692 0.32034573 0.34712216 0.37048343 0.37468991 0.35874641 0.33465126 0.30875564 0.27839369 0.23997456 0.19917674 0.16462305 0.1429356 0.13468027 0.14513949][0.20897268 0.23814976 0.26658738 0.29887733 0.31916463 0.32130244 0.30912581 0.29072255 0.26809669 0.2371707 0.2011707 0.17125729 0.15595278 0.15192778 0.15947443][0.1544832 0.18568803 0.21896037 0.257558 0.28770107 0.30125654 0.29528737 0.27796105 0.25903574 0.23770693 0.21558289 0.19967628 0.19592488 0.20029113 0.20819905][0.1493794 0.19312164 0.2368241 0.28039578 0.31424585 0.33286312 0.32835761 0.3060613 0.28389367 0.2682206 0.25965711 0.25442553 0.25490531 0.26131687 0.26773861][0.19648124 0.26390466 0.32721078 0.38072741 0.4180783 0.43961704 0.43456465 0.40303895 0.36953717 0.349165 0.34335074 0.33557144 0.32511172 0.31979281 0.31838208][0.27446371 0.37075174 0.45894495 0.52792645 0.57234311 0.59729022 0.59187859 0.55057204 0.50115 0.46482825 0.4455674 0.41951984 0.38436946 0.35646242 0.34404767][0.35211098 0.47079238 0.5789848 0.66270024 0.71594959 0.74532038 0.743545 0.70078605 0.64154607 0.58559364 0.54151446 0.48825032 0.423225 0.36880687 0.34493673][0.39652896 0.51942778 0.630109 0.71598595 0.770418 0.79981887 0.80390018 0.77153826 0.71766877 0.65255034 0.58831805 0.51365221 0.42721614 0.35344338 0.32209888][0.391931 0.50003988 0.59377539 0.66584224 0.71003473 0.73186183 0.73874891 0.72152454 0.6839661 0.62369657 0.55357414 0.47342575 0.38381222 0.30721304 0.27689639][0.336632 0.4181 0.48409373 0.53311217 0.56032175 0.57005131 0.57464832 0.56821942 0.54668617 0.49906552 0.43634295 0.36572817 0.28960961 0.22618562 0.20524853][0.23485664 0.28530851 0.32182789 0.34666938 0.35663721 0.35487172 0.35452884 0.35282844 0.34248641 0.3101902 0.26353595 0.21200404 0.15881738 0.11673543 0.10726166][0.10765309 0.13020356 0.1436139 0.15084724 0.15008995 0.14333679 0.14098951 0.14191742 0.13933973 0.12220476 0.094184779 0.062800407 0.03134007 0.0076984693 0.0047822078][-0.00871706 -0.0059164143 -0.0066876789 -0.0090687014 -0.013735028 -0.019636717 -0.020901104 -0.018501665 -0.016746998 -0.022540914 -0.035436168 -0.051257968 -0.066990539 -0.078344733 -0.079238668][-0.084485479 -0.092152849 -0.09815973 -0.10291687 -0.10677315 -0.10954236 -0.10921809 -0.10702094 -0.10469045 -0.10524689 -0.10963082 -0.11630847 -0.12300432 -0.12778778 -0.128617][-0.11557577 -0.12595734 -0.13149695 -0.13470948 -0.13601096 -0.13573839 -0.13400531 -0.13197903 -0.12993087 -0.12869443 -0.12945026 -0.13190076 -0.13465682 -0.13704215 -0.13865076]]...]
INFO - root - 2017-12-11 03:04:06.254593: step 83510, loss = 0.71, batch loss = 0.66 (29.2 examples/sec; 0.274 sec/batch; 18h:57m:15s remains)
INFO - root - 2017-12-11 03:04:08.974453: step 83520, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:36m:17s remains)
INFO - root - 2017-12-11 03:04:11.672726: step 83530, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 18h:19m:23s remains)
INFO - root - 2017-12-11 03:04:14.369100: step 83540, loss = 0.69, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 18h:42m:04s remains)
INFO - root - 2017-12-11 03:04:17.033840: step 83550, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 18h:22m:55s remains)
INFO - root - 2017-12-11 03:04:19.767823: step 83560, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 18h:48m:09s remains)
INFO - root - 2017-12-11 03:04:22.475052: step 83570, loss = 0.69, batch loss = 0.63 (28.2 examples/sec; 0.283 sec/batch; 19h:35m:04s remains)
INFO - root - 2017-12-11 03:04:25.254691: step 83580, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 18h:36m:22s remains)
INFO - root - 2017-12-11 03:04:28.002247: step 83590, loss = 0.68, batch loss = 0.62 (29.2 examples/sec; 0.274 sec/batch; 18h:54m:40s remains)
INFO - root - 2017-12-11 03:04:30.722169: step 83600, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.270 sec/batch; 18h:41m:27s remains)
2017-12-11 03:04:31.159706: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.017971581 0.015142958 0.012333542 0.013671787 0.020484388 0.028296128 0.029877868 0.02618245 0.021191632 0.017004438 0.012240298 0.006845206 0.0043107453 0.0085208137 0.021411559][0.026254894 0.026952175 0.027182199 0.030571943 0.038596798 0.04836943 0.053415984 0.053473335 0.049354907 0.042801086 0.033507913 0.022082102 0.015230102 0.019472158 0.0368344][0.050196808 0.057368387 0.0626592 0.0689881 0.077903554 0.090211593 0.10136601 0.10817154 0.10568973 0.095264733 0.0797243 0.060719516 0.047689393 0.049444955 0.068246663][0.089377463 0.10765918 0.12251534 0.13559458 0.14746954 0.16342436 0.18176235 0.19509351 0.19219266 0.17539228 0.15265685 0.12660189 0.10828995 0.10830722 0.12860847][0.14066632 0.17416617 0.2027715 0.22601841 0.24253029 0.26188362 0.28584835 0.30245492 0.29530478 0.26997718 0.2409973 0.21204449 0.19393766 0.19725931 0.22179319][0.20095083 0.25192055 0.29540753 0.32864791 0.34833798 0.36753169 0.39212167 0.4073039 0.39372012 0.36083603 0.32909456 0.30371809 0.29350314 0.30598563 0.33605686][0.26462129 0.33308804 0.38917089 0.42809042 0.44528714 0.45644879 0.47263667 0.48063117 0.46126497 0.42598775 0.39798769 0.38329816 0.38728365 0.41142377 0.44457412][0.32269654 0.40472621 0.46699849 0.5036965 0.510299 0.50443673 0.50316113 0.49951109 0.47732466 0.44706747 0.42958507 0.42984584 0.44879422 0.481376 0.51246226][0.36204836 0.44896153 0.50766444 0.53314054 0.52337688 0.49649975 0.47387251 0.45650521 0.43382993 0.41414645 0.41075671 0.42492417 0.45263213 0.48535177 0.50890213][0.37584433 0.45798612 0.50286692 0.50872076 0.47930294 0.43238163 0.39084634 0.36263907 0.34280047 0.33592457 0.34505153 0.36681649 0.39264309 0.41398689 0.42395124][0.35724333 0.42723787 0.45299998 0.4371489 0.39049241 0.330412 0.27913496 0.24837618 0.23532699 0.23894691 0.25325677 0.27228171 0.28599435 0.28905067 0.28477648][0.31358591 0.36881477 0.37771532 0.34639728 0.291311 0.22912808 0.18010467 0.15546563 0.15040477 0.15854259 0.16968952 0.17953081 0.17991669 0.16925402 0.15842658][0.26337159 0.30658183 0.30694553 0.27264154 0.22266774 0.17046428 0.13261236 0.11734156 0.11660597 0.1220895 0.12459123 0.12470117 0.11755262 0.10353641 0.096027091][0.22372545 0.2602886 0.26073182 0.23439284 0.20069081 0.16709459 0.14394684 0.13565715 0.13340706 0.13219789 0.12676451 0.12210324 0.11553534 0.106433 0.10674812][0.20916331 0.24464075 0.25014666 0.23568009 0.22033204 0.20563634 0.1945318 0.18809129 0.17978498 0.17188774 0.16370222 0.16196366 0.16359247 0.16350409 0.16984594]]...]
INFO - root - 2017-12-11 03:04:33.901065: step 83610, loss = 0.68, batch loss = 0.62 (29.3 examples/sec; 0.273 sec/batch; 18h:52m:44s remains)
INFO - root - 2017-12-11 03:04:36.658936: step 83620, loss = 0.71, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 18h:55m:39s remains)
INFO - root - 2017-12-11 03:04:39.377162: step 83630, loss = 0.70, batch loss = 0.65 (28.4 examples/sec; 0.281 sec/batch; 19h:27m:02s remains)
INFO - root - 2017-12-11 03:04:42.110247: step 83640, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.270 sec/batch; 18h:37m:56s remains)
INFO - root - 2017-12-11 03:04:44.811143: step 83650, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 18h:17m:23s remains)
INFO - root - 2017-12-11 03:04:47.573837: step 83660, loss = 0.70, batch loss = 0.64 (28.8 examples/sec; 0.278 sec/batch; 19h:10m:56s remains)
INFO - root - 2017-12-11 03:04:50.312168: step 83670, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.268 sec/batch; 18h:31m:05s remains)
INFO - root - 2017-12-11 03:04:53.042772: step 83680, loss = 0.69, batch loss = 0.64 (29.7 examples/sec; 0.270 sec/batch; 18h:37m:43s remains)
INFO - root - 2017-12-11 03:04:55.746326: step 83690, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 18h:39m:11s remains)
INFO - root - 2017-12-11 03:04:58.488617: step 83700, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 18h:30m:36s remains)
2017-12-11 03:04:58.944508: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.030876262 0.061258003 0.07612586 0.072301134 0.05263802 0.023100143 -0.0070435777 -0.0298384 -0.041752584 -0.045999303 -0.045819577 -0.045658864 -0.048528351 -0.055440303 -0.066089146][0.14580154 0.20104407 0.22738144 0.22252697 0.1924289 0.14693922 0.10043129 0.065260425 0.045590639 0.036866974 0.034680549 0.031048642 0.020687116 0.002399086 -0.022139614][0.27871808 0.36222747 0.40309754 0.40044791 0.36441973 0.3084507 0.25224739 0.21049272 0.18635438 0.17351426 0.1662835 0.1534324 0.12714264 0.088378422 0.041386824][0.39346847 0.50247014 0.55916226 0.56416893 0.53220534 0.47861153 0.4264338 0.38948163 0.36672649 0.34974429 0.3317298 0.30003527 0.2467967 0.1780514 0.10278857][0.45760992 0.58347273 0.65557867 0.67626494 0.66320974 0.63094282 0.60229158 0.58582962 0.57197738 0.55019826 0.51370186 0.45165783 0.35994223 0.25294939 0.14657326][0.46832624 0.60045415 0.68613625 0.72976327 0.74906039 0.75521487 0.76759261 0.78491229 0.78520828 0.75623012 0.69359046 0.59174216 0.45350784 0.30456471 0.1684202][0.44273591 0.57068956 0.66472757 0.73093587 0.78547686 0.83534539 0.8943423 0.949404 0.96486616 0.9277333 0.8364687 0.69351643 0.5109247 0.32533059 0.16700788][0.39478037 0.50734085 0.59859943 0.6767118 0.75516814 0.8372525 0.93110037 1.0132133 1.03802 0.99309504 0.88126737 0.71218616 0.50487971 0.30278024 0.13908303][0.33024356 0.41759324 0.49343282 0.56792313 0.65129322 0.7441265 0.84942609 0.93835 0.96304154 0.914313 0.79945445 0.6318481 0.43217683 0.24234746 0.093543231][0.24979609 0.30703929 0.35844314 0.41540024 0.48514044 0.56719691 0.66155291 0.739444 0.75797135 0.7122491 0.61273295 0.47245491 0.30833462 0.15459639 0.036488712][0.156775 0.18580751 0.21139446 0.24416837 0.28887931 0.34540212 0.41337624 0.46891323 0.47887674 0.44221988 0.37017456 0.27220079 0.15830454 0.0525648 -0.027091425][0.057630964 0.064261958 0.068359748 0.078136273 0.09651795 0.12425304 0.16180809 0.19278944 0.19544843 0.17122087 0.12963068 0.075583719 0.012575747 -0.045155697 -0.086932115][-0.029905835 -0.037610687 -0.046750251 -0.052258477 -0.052676365 -0.046720654 -0.033353537 -0.021832479 -0.02377056 -0.037025742 -0.054468326 -0.074577518 -0.097735219 -0.11755482 -0.12959015][-0.089128323 -0.1022988 -0.11532168 -0.1263862 -0.13494405 -0.13978386 -0.13968116 -0.13889335 -0.1422269 -0.14791285 -0.15142596 -0.15291791 -0.15389438 -0.15237121 -0.14782339][-0.11772646 -0.13047023 -0.14142035 -0.15139696 -0.16039558 -0.16754541 -0.17163667 -0.17409909 -0.17639434 -0.1775791 -0.17543498 -0.17065315 -0.1645163 -0.15639645 -0.1470097]]...]
INFO - root - 2017-12-11 03:05:01.706378: step 83710, loss = 0.70, batch loss = 0.65 (29.7 examples/sec; 0.269 sec/batch; 18h:35m:01s remains)
INFO - root - 2017-12-11 03:05:04.433482: step 83720, loss = 0.68, batch loss = 0.62 (30.1 examples/sec; 0.266 sec/batch; 18h:22m:13s remains)
INFO - root - 2017-12-11 03:05:07.157259: step 83730, loss = 0.71, batch loss = 0.65 (29.7 examples/sec; 0.270 sec/batch; 18h:38m:40s remains)
INFO - root - 2017-12-11 03:05:09.925525: step 83740, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 18h:59m:27s remains)
INFO - root - 2017-12-11 03:05:12.648736: step 83750, loss = 0.70, batch loss = 0.65 (28.9 examples/sec; 0.277 sec/batch; 19h:06m:51s remains)
INFO - root - 2017-12-11 03:05:15.427029: step 83760, loss = 0.69, batch loss = 0.63 (27.4 examples/sec; 0.292 sec/batch; 20h:11m:52s remains)
INFO - root - 2017-12-11 03:05:18.161905: step 83770, loss = 0.69, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 18h:26m:55s remains)
INFO - root - 2017-12-11 03:05:20.907394: step 83780, loss = 0.71, batch loss = 0.65 (28.9 examples/sec; 0.277 sec/batch; 19h:07m:20s remains)
INFO - root - 2017-12-11 03:05:23.675814: step 83790, loss = 0.68, batch loss = 0.63 (28.9 examples/sec; 0.276 sec/batch; 19h:06m:05s remains)
INFO - root - 2017-12-11 03:05:26.378770: step 83800, loss = 0.69, batch loss = 0.63 (28.1 examples/sec; 0.285 sec/batch; 19h:39m:51s remains)
2017-12-11 03:05:26.830232: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20618826 0.21574284 0.20806549 0.17942604 0.13713776 0.096343666 0.072728641 0.071621872 0.088931888 0.11115552 0.12913135 0.14592935 0.16220951 0.17166846 0.17680952][0.1487409 0.15593599 0.14870429 0.12339756 0.085252747 0.046647187 0.020947302 0.013252489 0.020087391 0.030250762 0.037364032 0.045089755 0.056372546 0.06859342 0.083624095][0.10864638 0.10967224 0.10043947 0.078011982 0.045473788 0.012107018 -0.01148702 -0.020859685 -0.019256894 -0.016239868 -0.016204586 -0.015116393 -0.008361239 0.0044916719 0.024874937][0.0836071 0.077514447 0.066824891 0.049498275 0.026457967 0.00359507 -0.011317099 -0.014952461 -0.011046084 -0.0090015726 -0.012406077 -0.016641263 -0.016224027 -0.00812878 0.0097644357][0.076663859 0.061275974 0.048617359 0.038681369 0.029837938 0.02407958 0.026432551 0.038470734 0.054246467 0.06245872 0.058521751 0.04683971 0.034003109 0.026950397 0.031112958][0.0830154 0.058425661 0.044919092 0.044823628 0.054172963 0.071410112 0.09851037 0.13367841 0.16668861 0.18309745 0.17678978 0.15230703 0.11850471 0.088284269 0.072257578][0.0897927 0.058411282 0.044799164 0.05451471 0.082554027 0.12496425 0.17974666 0.23965669 0.28870055 0.30935997 0.295617 0.25332138 0.19515885 0.14052056 0.10511168][0.089963026 0.054847881 0.041929737 0.059764568 0.10381156 0.16862948 0.24710913 0.32500979 0.3813121 0.39814913 0.37218904 0.31171256 0.23312438 0.16002667 0.11080235][0.073648237 0.039997071 0.029937586 0.053395491 0.10723598 0.18514952 0.27547243 0.35797375 0.40970412 0.41595218 0.37812775 0.306543 0.21869856 0.13832389 0.083826244][0.045791529 0.019693803 0.016719667 0.044790313 0.10125221 0.17884363 0.26421267 0.33559063 0.3730298 0.36828256 0.32673281 0.2577731 0.17544685 0.09953472 0.046536677][0.020326642 0.0073064351 0.015281972 0.047901362 0.10107484 0.16673797 0.23303171 0.28157 0.29947549 0.28662539 0.25051922 0.19686918 0.13294332 0.071831152 0.026516236][0.012844624 0.01257047 0.030647004 0.066198751 0.11210175 0.1593096 0.19949943 0.22085747 0.21928951 0.20226133 0.17753667 0.14594266 0.10713675 0.06675107 0.033208255][0.030120993 0.03753319 0.061520666 0.097396642 0.13462813 0.163413 0.17919666 0.17679766 0.15982525 0.14120533 0.12806252 0.11668221 0.10052535 0.079215683 0.056497481][0.054614864 0.0640133 0.0877567 0.11891766 0.1454369 0.15814482 0.15573923 0.13786381 0.11275303 0.095412284 0.091889635 0.095387153 0.095713951 0.088783421 0.074720748][0.067282073 0.07550592 0.09429694 0.11616114 0.13012573 0.12951565 0.11574143 0.091102116 0.065422267 0.0528233 0.057890024 0.072027259 0.0832339 0.085635819 0.077397585]]...]
INFO - root - 2017-12-11 03:05:29.602235: step 83810, loss = 0.71, batch loss = 0.65 (28.8 examples/sec; 0.278 sec/batch; 19h:12m:03s remains)
INFO - root - 2017-12-11 03:05:32.359574: step 83820, loss = 0.68, batch loss = 0.62 (30.1 examples/sec; 0.265 sec/batch; 18h:19m:47s remains)
INFO - root - 2017-12-11 03:05:35.042562: step 83830, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:38m:59s remains)
INFO - root - 2017-12-11 03:05:37.778077: step 83840, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 19h:02m:26s remains)
INFO - root - 2017-12-11 03:05:40.487365: step 83850, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 18h:22m:47s remains)
INFO - root - 2017-12-11 03:05:43.233369: step 83860, loss = 0.70, batch loss = 0.65 (28.9 examples/sec; 0.277 sec/batch; 19h:08m:45s remains)
INFO - root - 2017-12-11 03:05:45.982294: step 83870, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:35m:55s remains)
INFO - root - 2017-12-11 03:05:48.680443: step 83880, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 18h:47m:18s remains)
INFO - root - 2017-12-11 03:05:51.418201: step 83890, loss = 0.69, batch loss = 0.63 (28.5 examples/sec; 0.280 sec/batch; 19h:21m:18s remains)
INFO - root - 2017-12-11 03:05:54.165579: step 83900, loss = 0.71, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 18h:52m:03s remains)
2017-12-11 03:05:54.621303: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.039364938 0.098796383 0.15536778 0.19426258 0.20808434 0.19915156 0.17233662 0.13867326 0.11016879 0.092411809 0.086805895 0.090226479 0.09899351 0.10644332 0.10802294][0.047164679 0.11354676 0.17859462 0.22640784 0.2487179 0.24639741 0.2225842 0.18789433 0.15561081 0.13372917 0.12451421 0.12550028 0.13224103 0.13649838 0.13239434][0.048853204 0.11932816 0.19091676 0.24793079 0.28171474 0.29127631 0.27734691 0.24774234 0.21455112 0.18796861 0.17324021 0.1696361 0.17273289 0.17303291 0.16251108][0.050476406 0.12535647 0.20382786 0.27046704 0.31659591 0.33978713 0.33855993 0.317484 0.28617498 0.25577176 0.23502222 0.22595522 0.22456108 0.21987136 0.20190729][0.055341389 0.13506179 0.22014226 0.29515466 0.35163942 0.38639015 0.39631608 0.38322353 0.35391751 0.31909683 0.29055265 0.27327394 0.26490161 0.25420147 0.22985938][0.056194223 0.13838975 0.22733642 0.30770761 0.37153646 0.41521421 0.43470278 0.42964187 0.40343702 0.36505607 0.32768521 0.29931763 0.28067204 0.26249215 0.23421808][0.054363694 0.13698135 0.22787842 0.31172976 0.38082021 0.43098736 0.45776808 0.45939878 0.43650943 0.39595008 0.35044947 0.30984381 0.278151 0.25051725 0.21928772][0.056260753 0.13895112 0.230989 0.31683546 0.38884512 0.44227636 0.47257382 0.47827077 0.45873085 0.41859213 0.36805877 0.31644157 0.27027094 0.23036318 0.19410656][0.058333963 0.14005841 0.23158872 0.31727102 0.38988668 0.44445834 0.47622937 0.48465246 0.46922287 0.43160754 0.37786409 0.31533787 0.25303489 0.1985448 0.15541922][0.057829324 0.13632363 0.22380044 0.30447915 0.37230483 0.42314285 0.45318636 0.46327558 0.45338783 0.42127419 0.36801013 0.29888615 0.22519737 0.16066426 0.11391855][0.058799282 0.13279611 0.21296513 0.28309125 0.33868983 0.37864417 0.40205079 0.41175675 0.40829018 0.38563594 0.34013215 0.27441663 0.20059709 0.135486 0.090413533][0.0657694 0.13697807 0.2101188 0.26776162 0.30693215 0.33084723 0.3431305 0.34917906 0.34982672 0.33638656 0.30161792 0.24598756 0.18129177 0.12435688 0.086211][0.075133435 0.14563917 0.21346089 0.25996897 0.28337407 0.29111189 0.29166839 0.29280764 0.2950224 0.28796768 0.26239029 0.21788135 0.16632573 0.1230906 0.096870884][0.083867371 0.1556994 0.22112061 0.26028207 0.27183834 0.26614735 0.2558192 0.2509445 0.25229555 0.24898714 0.2309193 0.19727015 0.1602685 0.13354543 0.12254567][0.092887849 0.16860941 0.23564944 0.27241293 0.27676141 0.26037341 0.23899159 0.22582555 0.2231316 0.22095875 0.20938893 0.18682975 0.16475114 0.1543877 0.15794896]]...]
INFO - root - 2017-12-11 03:05:57.375245: step 83910, loss = 0.70, batch loss = 0.64 (28.1 examples/sec; 0.284 sec/batch; 19h:38m:09s remains)
INFO - root - 2017-12-11 03:06:00.116566: step 83920, loss = 0.69, batch loss = 0.63 (28.7 examples/sec; 0.279 sec/batch; 19h:15m:48s remains)
INFO - root - 2017-12-11 03:06:02.870168: step 83930, loss = 0.70, batch loss = 0.64 (28.2 examples/sec; 0.283 sec/batch; 19h:34m:00s remains)
INFO - root - 2017-12-11 03:06:05.603029: step 83940, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 18h:17m:22s remains)
INFO - root - 2017-12-11 03:06:08.383710: step 83950, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.272 sec/batch; 18h:44m:48s remains)
INFO - root - 2017-12-11 03:06:11.108883: step 83960, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 18h:21m:17s remains)
INFO - root - 2017-12-11 03:06:13.784568: step 83970, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 19h:01m:37s remains)
INFO - root - 2017-12-11 03:06:16.503113: step 83980, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:38m:05s remains)
INFO - root - 2017-12-11 03:06:19.237069: step 83990, loss = 0.67, batch loss = 0.61 (29.3 examples/sec; 0.273 sec/batch; 18h:52m:04s remains)
INFO - root - 2017-12-11 03:06:21.964185: step 84000, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.266 sec/batch; 18h:23m:21s remains)
2017-12-11 03:06:22.526716: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0763077 0.036230303 0.0078976294 0.0053659482 0.020255826 0.034621291 0.038253333 0.028259747 0.005939649 -0.022360444 -0.046930484 -0.0617531 -0.066343024 -0.0606993 -0.043860927][0.11988682 0.0896763 0.068413652 0.070925146 0.086614355 0.096509181 0.091191292 0.069850475 0.035911024 -0.001835373 -0.031848513 -0.049061753 -0.055037919 -0.051391277 -0.035228293][0.16532575 0.15413448 0.15000243 0.16571906 0.18667643 0.19337825 0.17774665 0.14193845 0.092329711 0.041083667 0.0018067666 -0.021711404 -0.033123977 -0.035789508 -0.024081338][0.21742283 0.231972 0.25189313 0.28637525 0.31648782 0.32269326 0.29862174 0.24977218 0.18515418 0.11934917 0.067000911 0.030751795 0.00701429 -0.0075935824 -0.0046867067][0.28492078 0.326892 0.37174398 0.42471534 0.46495637 0.47381267 0.44540286 0.38720095 0.31018198 0.230409 0.16167924 0.10684916 0.065955564 0.03823562 0.0333399][0.35877144 0.4261761 0.49175644 0.55945057 0.60986263 0.62573272 0.59986079 0.53934628 0.45436844 0.36034462 0.27013367 0.19178541 0.13362062 0.099430166 0.096628129][0.41686156 0.50263184 0.58035791 0.65620106 0.71573806 0.74318057 0.72797853 0.67387277 0.586918 0.48071358 0.3684926 0.26811975 0.19951329 0.17150229 0.1846626][0.45237222 0.54363441 0.62108016 0.69484878 0.75815862 0.79725373 0.797038 0.75555134 0.67327642 0.56195861 0.43689162 0.32695505 0.26268622 0.25594577 0.29639298][0.45724392 0.54008377 0.60475665 0.66599363 0.72314751 0.76464909 0.77290165 0.74238759 0.670526 0.5668568 0.44714418 0.34688008 0.30175757 0.32344729 0.39110342][0.42549977 0.48979035 0.53562003 0.57773876 0.6184687 0.64885247 0.65448964 0.63100135 0.575861 0.49387136 0.39730659 0.32071474 0.29864386 0.34169832 0.42372251][0.35355759 0.39980423 0.42749873 0.44904053 0.46769556 0.47860253 0.47551706 0.45753059 0.42264736 0.36990047 0.30545276 0.25661224 0.25206473 0.30158249 0.38021043][0.26453993 0.29336688 0.30178389 0.29991928 0.29346704 0.28323206 0.27175263 0.26029053 0.24609566 0.22342314 0.19228566 0.16955671 0.17525175 0.21815585 0.27938062][0.17663456 0.18899141 0.17831784 0.15419669 0.12581924 0.099451751 0.083041921 0.077820331 0.080000363 0.081250355 0.077190429 0.074382193 0.083913885 0.11309708 0.15086763][0.084126212 0.084876344 0.063765742 0.027834 -0.011776313 -0.046016812 -0.064465411 -0.066116735 -0.054922275 -0.039803069 -0.027718324 -0.01978893 -0.012262133 0.00069072249 0.015375291][-0.0038299772 -0.0063175871 -0.025462072 -0.059150681 -0.097565919 -0.13124686 -0.14922032 -0.14983854 -0.13630371 -0.11674055 -0.099127837 -0.088485911 -0.085327432 -0.084975339 -0.086396806]]...]
INFO - root - 2017-12-11 03:06:25.328620: step 84010, loss = 0.68, batch loss = 0.62 (28.2 examples/sec; 0.283 sec/batch; 19h:33m:47s remains)
INFO - root - 2017-12-11 03:06:28.064824: step 84020, loss = 0.68, batch loss = 0.62 (27.6 examples/sec; 0.289 sec/batch; 19h:58m:46s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 03:06:30.772238: step 84030, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 18h:08m:59s remains)
INFO - root - 2017-12-11 03:06:33.473589: step 84040, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.270 sec/batch; 18h:37m:49s remains)
INFO - root - 2017-12-11 03:06:36.234841: step 84050, loss = 0.71, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 18h:48m:09s remains)
INFO - root - 2017-12-11 03:06:38.948987: step 84060, loss = 0.69, batch loss = 0.63 (28.4 examples/sec; 0.282 sec/batch; 19h:26m:05s remains)
INFO - root - 2017-12-11 03:06:41.673853: step 84070, loss = 0.70, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 18h:43m:02s remains)
INFO - root - 2017-12-11 03:06:44.369843: step 84080, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 19h:03m:07s remains)
INFO - root - 2017-12-11 03:06:47.113599: step 84090, loss = 0.71, batch loss = 0.65 (28.2 examples/sec; 0.283 sec/batch; 19h:33m:07s remains)
INFO - root - 2017-12-11 03:06:49.836706: step 84100, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.268 sec/batch; 18h:27m:41s remains)
2017-12-11 03:06:50.299954: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2220145 0.25706053 0.27725488 0.28006616 0.25902924 0.22057895 0.18157443 0.15426463 0.14249085 0.14641853 0.15409493 0.15427242 0.14716227 0.13206629 0.11829488][0.21835259 0.24960271 0.27181798 0.282347 0.27066797 0.23864114 0.19861399 0.16232131 0.13788106 0.12769382 0.12259685 0.11469611 0.10601855 0.095790744 0.088758409][0.19609217 0.22254002 0.24707113 0.26738203 0.270114 0.25302973 0.22270791 0.18721573 0.15657635 0.13374445 0.11344467 0.091726854 0.073873729 0.061731171 0.056425169][0.16651623 0.18953511 0.2160833 0.24494968 0.26220685 0.26344839 0.25008211 0.22495958 0.19738819 0.1681333 0.13531211 0.099175766 0.068296604 0.047658533 0.036751766][0.13370132 0.15485306 0.18263862 0.21693595 0.24574617 0.2648046 0.27170739 0.26403803 0.24790104 0.22007783 0.18060394 0.13298835 0.088581607 0.055431835 0.033597868][0.098745063 0.11895845 0.1470128 0.18387891 0.22109042 0.25574452 0.28252053 0.29431477 0.2932367 0.27253082 0.23223604 0.1778682 0.12240901 0.076356538 0.042126924][0.062213417 0.081652053 0.10925941 0.14669889 0.1898791 0.23707496 0.28011909 0.30810702 0.31969374 0.30720815 0.2699095 0.21404451 0.15302213 0.098630726 0.055387087][0.026501 0.045629431 0.0734108 0.11114553 0.158643 0.21437223 0.26772162 0.30497018 0.32287356 0.31575948 0.28251156 0.22967255 0.16991849 0.11470145 0.069079183][-0.0062791635 0.013922967 0.043531053 0.082663558 0.13329408 0.19329347 0.25061041 0.29001766 0.30778936 0.30154416 0.27116561 0.22391391 0.17099811 0.12200422 0.080632143][-0.030341031 -0.0083480682 0.024430413 0.066146582 0.11896 0.17988566 0.23628406 0.27261451 0.28588313 0.27670747 0.24690184 0.20517549 0.16134451 0.1224914 0.089686684][-0.041573618 -0.018193856 0.017272774 0.061157953 0.11476959 0.17409092 0.22638585 0.2570931 0.26485848 0.2520884 0.22220786 0.18562631 0.15107483 0.12304186 0.099568471][-0.041474938 -0.017745789 0.018622991 0.063250236 0.11643817 0.17297928 0.21994336 0.2452509 0.24956322 0.23565143 0.20730954 0.17605063 0.14999577 0.1309479 0.11436991][-0.034076091 -0.0095701525 0.027302686 0.072434068 0.12506147 0.17870487 0.22087644 0.2434891 0.24892931 0.23851158 0.21472763 0.18986516 0.17170416 0.15871549 0.14390118][-0.022630407 0.0044535906 0.043646943 0.091442645 0.14518443 0.19649999 0.23427857 0.25604358 0.26610795 0.26279098 0.24594606 0.22915962 0.21988028 0.21158637 0.19409588][-0.0088207861 0.022780836 0.0674343 0.12156232 0.17882951 0.22858787 0.26204306 0.28272846 0.2972936 0.30045849 0.28994587 0.2821601 0.28373241 0.28098267 0.25938118]]...]
INFO - root - 2017-12-11 03:06:53.019323: step 84110, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 18h:57m:43s remains)
INFO - root - 2017-12-11 03:06:55.792669: step 84120, loss = 0.68, batch loss = 0.62 (29.8 examples/sec; 0.268 sec/batch; 18h:29m:43s remains)
INFO - root - 2017-12-11 03:06:58.489645: step 84130, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 18h:41m:28s remains)
INFO - root - 2017-12-11 03:07:01.236605: step 84140, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 18h:25m:29s remains)
INFO - root - 2017-12-11 03:07:03.941605: step 84150, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 18h:20m:07s remains)
INFO - root - 2017-12-11 03:07:06.686710: step 84160, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 18h:33m:16s remains)
INFO - root - 2017-12-11 03:07:09.466807: step 84170, loss = 0.69, batch loss = 0.63 (27.8 examples/sec; 0.288 sec/batch; 19h:50m:38s remains)
INFO - root - 2017-12-11 03:07:12.242670: step 84180, loss = 0.69, batch loss = 0.63 (27.5 examples/sec; 0.291 sec/batch; 20h:05m:23s remains)
INFO - root - 2017-12-11 03:07:14.970249: step 84190, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 19h:02m:21s remains)
INFO - root - 2017-12-11 03:07:17.679047: step 84200, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 18h:15m:02s remains)
2017-12-11 03:07:18.121575: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27769428 0.31978974 0.3630923 0.39615741 0.40916628 0.40197152 0.3690066 0.30872524 0.23309584 0.16349232 0.11489208 0.099238567 0.13681054 0.22874689 0.33879304][0.34570619 0.38705161 0.42824382 0.46009657 0.47456813 0.47134957 0.44106168 0.37688789 0.28909951 0.20454268 0.14020385 0.10797889 0.12899604 0.20764543 0.31128895][0.37310356 0.41585252 0.45971459 0.49745211 0.52192467 0.53208566 0.51455432 0.45433751 0.35925013 0.26147693 0.18003944 0.12623884 0.11977772 0.16969229 0.25149849][0.36869973 0.42149037 0.48151389 0.54060566 0.58852714 0.62095523 0.62115151 0.56688416 0.46419021 0.34897444 0.2437945 0.16064212 0.11763336 0.12943321 0.18118069][0.33763906 0.40696424 0.49228537 0.5810926 0.65825123 0.71592718 0.7355122 0.689806 0.58308047 0.45299906 0.32497242 0.21139926 0.12953241 0.099540576 0.11620815][0.28340614 0.36897048 0.47937608 0.59629411 0.70052028 0.7809307 0.81699163 0.77912921 0.67175955 0.53376329 0.39114138 0.25614393 0.1450056 0.081295185 0.06668216][0.22973646 0.32001737 0.43966928 0.56586552 0.67898673 0.76658487 0.8070417 0.77185678 0.66797537 0.53456539 0.39512265 0.26060221 0.14483817 0.070176393 0.040338047][0.18939665 0.26899183 0.37577388 0.48632562 0.58533162 0.66201216 0.69576317 0.66218942 0.57010734 0.45568773 0.33755723 0.2244571 0.12710938 0.062497668 0.032860719][0.16009547 0.21899472 0.2968193 0.37456313 0.4439092 0.49827453 0.51998127 0.48964378 0.41505322 0.32716182 0.23935623 0.15739466 0.088256784 0.042324938 0.020762596][0.13516276 0.16862936 0.21147722 0.25143772 0.28718436 0.3169665 0.32713938 0.30246028 0.24777621 0.18739451 0.12969767 0.077444546 0.034664609 0.0064799348 -0.0056926808][0.11772788 0.12950547 0.14333314 0.15335253 0.16333137 0.17474462 0.17719664 0.15802756 0.11909754 0.078586429 0.041756097 0.0098050423 -0.014742848 -0.030408552 -0.035911366][0.10354599 0.1005263 0.095156863 0.086252905 0.080605887 0.080897234 0.079278253 0.06528151 0.03877392 0.012827049 -0.0097519252 -0.029006118 -0.042800907 -0.0508416 -0.052449267][0.077748291 0.065281644 0.04938069 0.031999674 0.020857828 0.018322468 0.018174829 0.011850235 -0.002020465 -0.014420649 -0.024967752 -0.034914095 -0.042069275 -0.045857161 -0.046315886][0.043647513 0.026274759 0.0063793692 -0.012389836 -0.022696083 -0.023058588 -0.018471375 -0.016249303 -0.018448841 -0.019203618 -0.020090943 -0.023092534 -0.025929285 -0.027434843 -0.028353512][0.013629818 -0.00346694 -0.021488396 -0.03691671 -0.043745082 -0.041424152 -0.0337557 -0.02683677 -0.022647744 -0.017486943 -0.013718238 -0.013224212 -0.013735291 -0.013804486 -0.015066226]]...]
INFO - root - 2017-12-11 03:07:20.840016: step 84210, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 18h:11m:28s remains)
INFO - root - 2017-12-11 03:07:23.559239: step 84220, loss = 0.71, batch loss = 0.65 (29.8 examples/sec; 0.268 sec/batch; 18h:29m:14s remains)
INFO - root - 2017-12-11 03:07:26.296252: step 84230, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:53m:28s remains)
INFO - root - 2017-12-11 03:07:28.999651: step 84240, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 19h:00m:24s remains)
INFO - root - 2017-12-11 03:07:31.708990: step 84250, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 19h:00m:38s remains)
INFO - root - 2017-12-11 03:07:34.443120: step 84260, loss = 0.72, batch loss = 0.66 (30.2 examples/sec; 0.265 sec/batch; 18h:14m:57s remains)
INFO - root - 2017-12-11 03:07:37.210900: step 84270, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 18h:54m:17s remains)
INFO - root - 2017-12-11 03:07:39.962688: step 84280, loss = 0.70, batch loss = 0.64 (28.7 examples/sec; 0.278 sec/batch; 19h:11m:31s remains)
INFO - root - 2017-12-11 03:07:42.688001: step 84290, loss = 0.71, batch loss = 0.65 (28.5 examples/sec; 0.281 sec/batch; 19h:21m:37s remains)
INFO - root - 2017-12-11 03:07:45.418644: step 84300, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.263 sec/batch; 18h:05m:57s remains)
2017-12-11 03:07:45.950421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.04406878 -0.043905366 -0.045450281 -0.052134037 -0.061445143 -0.069895878 -0.074435383 -0.075621828 -0.074507646 -0.073116325 -0.073341116 -0.075545274 -0.079497181 -0.08445327 -0.088926926][-0.0011773205 0.0065701162 0.0086241215 0.00048337175 -0.0144704 -0.031009598 -0.042954564 -0.04825107 -0.045269132 -0.038287736 -0.034164995 -0.035944778 -0.042776465 -0.053580351 -0.066616856][0.0731507 0.099161796 0.11223929 0.10628607 0.0852331 0.055286825 0.026676198 0.00933891 0.012477278 0.029220734 0.043103341 0.044667549 0.035982408 0.018125024 -0.0085198963][0.15964641 0.21256222 0.24436353 0.24691875 0.22394481 0.17943302 0.12755722 0.090603076 0.091123268 0.11888557 0.1452273 0.15299574 0.14501335 0.11996102 0.075432315][0.22378865 0.30533132 0.36147678 0.38204312 0.36868697 0.31859687 0.24758895 0.18996711 0.18422754 0.2194113 0.25672141 0.27019656 0.26407826 0.23311882 0.17039892][0.25013247 0.35686135 0.44028908 0.48803511 0.49762732 0.45681283 0.37956107 0.30883265 0.29641449 0.33307281 0.37328553 0.38644892 0.37823567 0.33847812 0.25532433][0.24734578 0.37196204 0.47881749 0.553577 0.590468 0.56963629 0.5001477 0.42814127 0.41129056 0.44380647 0.4791398 0.48592666 0.4710235 0.42030928 0.31781721][0.227301 0.36118475 0.48480758 0.58079022 0.64243811 0.64681685 0.596863 0.53543347 0.5173986 0.54080641 0.56345481 0.55717969 0.52951527 0.46403089 0.3441692][0.18600743 0.31806651 0.44829342 0.55571795 0.63355571 0.66088516 0.63557851 0.5922212 0.57676893 0.590515 0.59908074 0.57995218 0.54026186 0.46270955 0.33280575][0.13220556 0.24852931 0.36910114 0.4707914 0.54785168 0.58513272 0.57924879 0.55498844 0.54526329 0.55292588 0.55243337 0.52781767 0.48427325 0.40481868 0.27926853][0.079615444 0.16984968 0.26696607 0.34839776 0.40973374 0.44291928 0.44582096 0.43436769 0.42852134 0.43143743 0.42612094 0.40275139 0.36466712 0.29588521 0.19000022][0.023742002 0.082020804 0.14776197 0.20100346 0.23912752 0.25990376 0.26301295 0.25739855 0.25331724 0.25385493 0.24852894 0.23149052 0.2046693 0.15465868 0.078375027][-0.027961684 0.00035034562 0.035331193 0.061402429 0.077287048 0.084389985 0.083582841 0.079005577 0.0749955 0.074264288 0.070683964 0.060735837 0.045637231 0.015938891 -0.02926892][-0.063921615 -0.058397729 -0.047302678 -0.041413795 -0.041282609 -0.044418681 -0.04919431 -0.054193053 -0.058332376 -0.059890117 -0.061894771 -0.066245027 -0.072190374 -0.085090853 -0.10479855][-0.081776649 -0.089089826 -0.091934323 -0.097102031 -0.10471444 -0.11295713 -0.12004386 -0.12538347 -0.12920941 -0.13086757 -0.13170822 -0.13248117 -0.13284712 -0.13515507 -0.13890079]]...]
INFO - root - 2017-12-11 03:07:48.642440: step 84310, loss = 0.72, batch loss = 0.66 (29.4 examples/sec; 0.272 sec/batch; 18h:43m:43s remains)
INFO - root - 2017-12-11 03:07:51.321167: step 84320, loss = 0.71, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 17h:49m:56s remains)
INFO - root - 2017-12-11 03:07:54.008471: step 84330, loss = 0.68, batch loss = 0.62 (29.8 examples/sec; 0.269 sec/batch; 18h:31m:13s remains)
INFO - root - 2017-12-11 03:07:56.728325: step 84340, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:54m:12s remains)
INFO - root - 2017-12-11 03:07:59.421551: step 84350, loss = 0.68, batch loss = 0.62 (29.0 examples/sec; 0.276 sec/batch; 19h:00m:38s remains)
INFO - root - 2017-12-11 03:08:02.227685: step 84360, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.268 sec/batch; 18h:26m:18s remains)
INFO - root - 2017-12-11 03:08:04.907939: step 84370, loss = 0.70, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 18h:44m:10s remains)
INFO - root - 2017-12-11 03:08:07.631030: step 84380, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 17h:53m:32s remains)
INFO - root - 2017-12-11 03:08:10.369008: step 84390, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 18h:32m:58s remains)
INFO - root - 2017-12-11 03:08:13.152147: step 84400, loss = 0.71, batch loss = 0.65 (28.8 examples/sec; 0.277 sec/batch; 19h:06m:53s remains)
2017-12-11 03:08:13.620092: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049842957 0.055733722 0.054020006 0.050629411 0.056027673 0.073752873 0.099331148 0.12305898 0.133032 0.12137359 0.089400224 0.051176798 0.026699567 0.029939815 0.065105245][0.064450726 0.073824115 0.074186817 0.069101825 0.068144411 0.076657049 0.0917 0.10532826 0.10807038 0.09473189 0.067763217 0.03926925 0.026366314 0.038663935 0.077411592][0.072029285 0.087481514 0.096386418 0.098816276 0.10043114 0.10656346 0.11526076 0.12029105 0.11441834 0.095703281 0.069133446 0.046381615 0.042173352 0.062282763 0.10435137][0.074413463 0.097324073 0.12134392 0.14286017 0.16099302 0.17813645 0.19078165 0.1931555 0.17908548 0.14956868 0.11428025 0.086954139 0.081772506 0.10272267 0.14417735][0.07953088 0.10944103 0.15139765 0.19945438 0.24518788 0.28531155 0.3115328 0.31652057 0.29411331 0.24821147 0.19517216 0.15335895 0.13826048 0.15343249 0.1902494][0.099028811 0.13263597 0.18908349 0.26236886 0.33744225 0.40323153 0.44437966 0.4513061 0.41831505 0.35313293 0.27919167 0.21979746 0.19163021 0.1980027 0.22795175][0.14464112 0.17448115 0.23273613 0.31771562 0.41136879 0.49484694 0.54492724 0.54976761 0.50572151 0.42505178 0.33647528 0.26578853 0.22879623 0.22770096 0.25024861][0.22318834 0.24032725 0.28278083 0.357268 0.44816941 0.53236794 0.58095 0.58036017 0.52940422 0.44356373 0.35313964 0.28258631 0.24478215 0.24013187 0.25629795][0.33402914 0.33224559 0.34401515 0.3867428 0.45214483 0.51804513 0.55418295 0.54644877 0.49515903 0.41585946 0.33606258 0.27626094 0.24577707 0.24286766 0.25570506][0.44861338 0.42696896 0.40293616 0.40389347 0.43163458 0.46924537 0.48949465 0.47846982 0.43494308 0.37052557 0.3069146 0.26080632 0.23928116 0.23969235 0.2511853][0.52581543 0.48922336 0.43565798 0.399336 0.3919681 0.40430054 0.4144156 0.40803486 0.37955415 0.33401641 0.28632075 0.25076959 0.23443086 0.23568644 0.24527036][0.535928 0.49558342 0.42907381 0.37204188 0.34333193 0.34164083 0.35074237 0.35504577 0.34417054 0.31695488 0.28260168 0.25414065 0.23996817 0.24058162 0.24825056][0.47665071 0.4464922 0.388913 0.33413649 0.30324897 0.30116844 0.31657892 0.33293614 0.336519 0.32237789 0.29644191 0.27142373 0.25760195 0.25780463 0.26487353][0.38184324 0.36967748 0.33396044 0.29562172 0.2739183 0.27834377 0.30162287 0.32736954 0.34081575 0.33579749 0.31642249 0.29445803 0.28137106 0.28213039 0.28970987][0.28865275 0.29655993 0.28582218 0.26726821 0.25618717 0.26591066 0.29300722 0.32230189 0.33996609 0.34058246 0.32725605 0.30941415 0.29853207 0.30093947 0.30949789]]...]
INFO - root - 2017-12-11 03:08:16.369132: step 84410, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 18h:23m:24s remains)
INFO - root - 2017-12-11 03:08:19.128848: step 84420, loss = 0.71, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 18h:40m:49s remains)
INFO - root - 2017-12-11 03:08:21.823931: step 84430, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:37m:41s remains)
INFO - root - 2017-12-11 03:08:24.586858: step 84440, loss = 0.70, batch loss = 0.64 (28.0 examples/sec; 0.286 sec/batch; 19h:41m:47s remains)
INFO - root - 2017-12-11 03:08:27.312757: step 84450, loss = 0.69, batch loss = 0.64 (28.7 examples/sec; 0.279 sec/batch; 19h:14m:10s remains)
INFO - root - 2017-12-11 03:08:30.065158: step 84460, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 18h:18m:56s remains)
INFO - root - 2017-12-11 03:08:32.792563: step 84470, loss = 0.69, batch loss = 0.64 (27.1 examples/sec; 0.296 sec/batch; 20h:21m:40s remains)
INFO - root - 2017-12-11 03:08:35.514471: step 84480, loss = 0.67, batch loss = 0.61 (28.7 examples/sec; 0.279 sec/batch; 19h:13m:25s remains)
INFO - root - 2017-12-11 03:08:38.204229: step 84490, loss = 0.71, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 18h:39m:53s remains)
INFO - root - 2017-12-11 03:08:40.911879: step 84500, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 19h:02m:06s remains)
2017-12-11 03:08:41.409191: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25874966 0.24651387 0.22159638 0.1944712 0.17362276 0.16990578 0.18147178 0.19533363 0.20694658 0.21251698 0.2120356 0.20791364 0.20471811 0.20920987 0.22279948][0.24981506 0.25247905 0.24210314 0.22624636 0.21375172 0.21681117 0.22983909 0.2407003 0.25149816 0.26286483 0.26946193 0.26778078 0.26325545 0.26613867 0.27934158][0.25677496 0.27329457 0.27574307 0.26876631 0.26023123 0.26260638 0.26917568 0.27157918 0.27829841 0.294121 0.31139907 0.32118398 0.32552248 0.33538851 0.3556973][0.26648432 0.28960952 0.29984078 0.29962704 0.29429206 0.29392508 0.29200649 0.2847392 0.28490111 0.29989788 0.32316372 0.34359235 0.35890034 0.37927866 0.41146404][0.27416211 0.29946384 0.31543246 0.32337052 0.32549489 0.3265743 0.31944039 0.30380312 0.2949926 0.3015655 0.3192206 0.3380909 0.35484877 0.38004035 0.42074472][0.28750184 0.317762 0.34309343 0.36363992 0.3792789 0.38864833 0.38181186 0.35945591 0.33824849 0.32788074 0.3253243 0.32422706 0.32624722 0.34493119 0.3856999][0.30820459 0.35016415 0.38990313 0.42560527 0.45706314 0.47868863 0.47566861 0.44709784 0.41044757 0.37756377 0.3453005 0.31168717 0.28838384 0.29349566 0.32777083][0.3156625 0.37046942 0.42420894 0.47114849 0.51306486 0.54321444 0.54224223 0.50730133 0.45670438 0.40533841 0.34907666 0.28920934 0.24725218 0.24421962 0.27384371][0.27450231 0.33324105 0.39113492 0.43852931 0.47971481 0.50906712 0.505803 0.46719289 0.41254756 0.35867983 0.30028749 0.23967417 0.20207956 0.20639154 0.23931973][0.17069642 0.21918446 0.26730067 0.30304092 0.33251655 0.35268897 0.34589371 0.31152508 0.2686803 0.23331176 0.19914165 0.16700938 0.15814631 0.18401533 0.22696286][0.032735698 0.060675938 0.089525469 0.10743956 0.12151316 0.13184974 0.12681189 0.10787161 0.091258556 0.088952959 0.094319388 0.10574416 0.13566419 0.18564273 0.23718211][-0.077562287 -0.070808969 -0.061528843 -0.058488674 -0.054406345 -0.046643853 -0.041298505 -0.036932584 -0.023148121 0.0067572081 0.046940159 0.093313783 0.14928107 0.20913874 0.25781351][-0.103227 -0.1099853 -0.1131814 -0.1161568 -0.11211581 -0.09760157 -0.076561265 -0.050579041 -0.016511729 0.028267393 0.08051645 0.13495362 0.18941404 0.23699267 0.26945925][-0.043327328 -0.053238064 -0.059493337 -0.0584958 -0.044340692 -0.015294766 0.02234808 0.061892472 0.099220641 0.13435587 0.16874187 0.20008846 0.2265306 0.24506444 0.25428742][0.052441638 0.04576147 0.042762835 0.053887062 0.08404047 0.1311104 0.18330528 0.22882578 0.25751185 0.26717454 0.2653327 0.25788987 0.24834709 0.23866688 0.23053721]]...]
INFO - root - 2017-12-11 03:08:44.180755: step 84510, loss = 0.70, batch loss = 0.64 (28.7 examples/sec; 0.279 sec/batch; 19h:11m:35s remains)
INFO - root - 2017-12-11 03:08:46.890962: step 84520, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:32m:45s remains)
INFO - root - 2017-12-11 03:08:49.637072: step 84530, loss = 0.69, batch loss = 0.63 (26.7 examples/sec; 0.299 sec/batch; 20h:36m:30s remains)
INFO - root - 2017-12-11 03:08:52.338544: step 84540, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:37m:32s remains)
INFO - root - 2017-12-11 03:08:55.077324: step 84550, loss = 0.70, batch loss = 0.64 (28.1 examples/sec; 0.284 sec/batch; 19h:34m:39s remains)
INFO - root - 2017-12-11 03:08:57.862650: step 84560, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:52m:49s remains)
INFO - root - 2017-12-11 03:09:00.606340: step 84570, loss = 0.70, batch loss = 0.64 (28.5 examples/sec; 0.280 sec/batch; 19h:18m:47s remains)
INFO - root - 2017-12-11 03:09:03.292318: step 84580, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.271 sec/batch; 18h:38m:21s remains)
INFO - root - 2017-12-11 03:09:05.995680: step 84590, loss = 0.69, batch loss = 0.63 (28.4 examples/sec; 0.282 sec/batch; 19h:23m:37s remains)
INFO - root - 2017-12-11 03:09:08.704797: step 84600, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.268 sec/batch; 18h:25m:42s remains)
2017-12-11 03:09:09.280649: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.39257053 0.34925523 0.29227081 0.24646059 0.21865363 0.21299507 0.21657878 0.21023461 0.18993965 0.1732164 0.18294752 0.22495629 0.30052203 0.39273974 0.47050747][0.42704391 0.39137176 0.34228775 0.30642834 0.29260591 0.30384928 0.31835061 0.30774939 0.26926604 0.23053151 0.22295061 0.2526339 0.3197144 0.41135865 0.4950265][0.41911995 0.39691356 0.36913845 0.360718 0.37784389 0.41457283 0.44051909 0.42161983 0.35849497 0.28922361 0.25351149 0.2620514 0.31508476 0.40115863 0.48500484][0.39434004 0.38896129 0.39094943 0.42007557 0.476152 0.54038519 0.57625777 0.54577315 0.45285 0.34465772 0.2700676 0.24853306 0.28056809 0.35648003 0.43793947][0.38999787 0.4008213 0.4285199 0.48646671 0.56918144 0.6520642 0.69977683 0.66911268 0.55972791 0.4198935 0.30564576 0.24714783 0.24735554 0.29937476 0.36828882][0.41718355 0.44151783 0.4813768 0.5475533 0.636568 0.73132008 0.80100662 0.792372 0.6915164 0.54061085 0.39696896 0.29969063 0.25555503 0.26431969 0.30367255][0.46499592 0.49427968 0.530035 0.586653 0.66908312 0.77390409 0.87247759 0.89898247 0.82416135 0.68137842 0.523759 0.39498034 0.30560151 0.26385513 0.26435763][0.51310742 0.53981471 0.56288636 0.602594 0.67060733 0.77383655 0.88753521 0.94142061 0.89910686 0.78368449 0.63755047 0.4975476 0.37591454 0.29176486 0.25700584][0.54496 0.5708167 0.58423209 0.60822064 0.65515006 0.73566908 0.83547747 0.89509964 0.88277268 0.81025904 0.70141864 0.57279688 0.43692702 0.32600826 0.26822075][0.55609065 0.588249 0.59964108 0.61243576 0.63439405 0.67642581 0.73752517 0.77983493 0.78328693 0.75108993 0.686413 0.5815413 0.44938105 0.33352664 0.27265322][0.52332348 0.56428111 0.57978386 0.58788848 0.58974493 0.59428453 0.61036134 0.62343705 0.62544471 0.61564445 0.58204776 0.50153774 0.38727015 0.28785723 0.24317798][0.42918319 0.47419164 0.49573323 0.50553095 0.4985809 0.4798978 0.4619641 0.44720864 0.4363268 0.42891389 0.40825176 0.34877425 0.26188627 0.19183522 0.17138983][0.27961656 0.32276052 0.3497704 0.36418319 0.35691252 0.32918158 0.29355484 0.26119936 0.23766623 0.22451575 0.20816112 0.16868959 0.11370878 0.075411081 0.075902365][0.10725465 0.14323477 0.17165226 0.18873899 0.18466069 0.15854548 0.12124161 0.086129963 0.06008089 0.045117673 0.032748718 0.010881966 -0.01617521 -0.029676668 -0.017029908][-0.03296 -0.0093143657 0.012583752 0.025624532 0.022884851 0.0035080682 -0.024745394 -0.050803691 -0.069433488 -0.079653636 -0.085998788 -0.094240531 -0.10212493 -0.1009544 -0.084498748]]...]
INFO - root - 2017-12-11 03:09:11.973679: step 84610, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 18h:36m:04s remains)
INFO - root - 2017-12-11 03:09:14.647323: step 84620, loss = 0.72, batch loss = 0.66 (28.7 examples/sec; 0.278 sec/batch; 19h:10m:24s remains)
INFO - root - 2017-12-11 03:09:17.384467: step 84630, loss = 0.71, batch loss = 0.66 (29.3 examples/sec; 0.273 sec/batch; 18h:47m:38s remains)
INFO - root - 2017-12-11 03:09:20.156604: step 84640, loss = 0.69, batch loss = 0.63 (27.8 examples/sec; 0.288 sec/batch; 19h:49m:02s remains)
INFO - root - 2017-12-11 03:09:22.848832: step 84650, loss = 0.70, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 18h:22m:56s remains)
INFO - root - 2017-12-11 03:09:25.569889: step 84660, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 18h:19m:13s remains)
INFO - root - 2017-12-11 03:09:28.302006: step 84670, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.266 sec/batch; 18h:19m:58s remains)
INFO - root - 2017-12-11 03:09:31.023857: step 84680, loss = 0.70, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 18h:15m:30s remains)
INFO - root - 2017-12-11 03:09:33.753165: step 84690, loss = 0.68, batch loss = 0.62 (29.0 examples/sec; 0.276 sec/batch; 19h:00m:54s remains)
INFO - root - 2017-12-11 03:09:36.482253: step 84700, loss = 0.71, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 18h:48m:22s remains)
2017-12-11 03:09:36.931919: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12175371 0.16984461 0.19513969 0.19121304 0.16472352 0.12986338 0.10041601 0.090850182 0.10236478 0.12538943 0.14174995 0.13715357 0.11081 0.070873536 0.027705751][0.10932093 0.15748356 0.18453653 0.18435106 0.16308439 0.13339509 0.10765646 0.09971711 0.11089969 0.13191013 0.14617337 0.1408599 0.11469384 0.07516259 0.03232953][0.090368696 0.13683185 0.16495897 0.16967127 0.15678303 0.13726063 0.12086935 0.11877809 0.13136631 0.14978406 0.15928315 0.14923927 0.11949866 0.078076363 0.035474714][0.070321508 0.11501619 0.14491117 0.15562715 0.15256149 0.1455403 0.14133227 0.14714906 0.16187175 0.17653163 0.1787132 0.16041474 0.12375634 0.078034244 0.034578662][0.047414385 0.089606479 0.12215967 0.14056556 0.14907905 0.1562161 0.16529033 0.1786038 0.19336011 0.20196998 0.19499731 0.16677977 0.1223113 0.072490409 0.028993445][0.02205269 0.060238693 0.095200017 0.12208726 0.1430417 0.16439942 0.18593365 0.20507535 0.21755689 0.21834755 0.20185815 0.16456814 0.11385607 0.06189245 0.019836603][-0.00078111654 0.032058373 0.068133622 0.1019382 0.13315067 0.16537909 0.19575386 0.21805379 0.22718365 0.22124079 0.19804102 0.15543616 0.10219588 0.050903916 0.011378346][-0.016079498 0.011171426 0.046563447 0.084005214 0.12064929 0.15752527 0.19095148 0.21357094 0.22015901 0.211248 0.18672055 0.1446799 0.093484953 0.045116298 0.0080314549][-0.02291834 -0.00045607379 0.032893255 0.070720188 0.10796833 0.14359196 0.17457469 0.19512008 0.20089377 0.19367918 0.17375275 0.13841313 0.093739547 0.049714647 0.013613375][-0.022454118 -0.0026591951 0.028783822 0.065293349 0.10002893 0.13040462 0.15481402 0.17085481 0.176451 0.17437324 0.16409598 0.14086097 0.10663503 0.068168469 0.031655416][-0.016199579 0.0035881044 0.034381509 0.0691723 0.099733531 0.12239242 0.13725077 0.14618132 0.15065081 0.15458521 0.15635063 0.14862821 0.12815267 0.0973073 0.060930505][-0.0050803456 0.017403724 0.048996296 0.081832476 0.10688065 0.12014205 0.12356791 0.12317082 0.12428121 0.1323752 0.14519784 0.15325995 0.14827561 0.12786339 0.09501949][0.01025079 0.037254907 0.070184223 0.10015522 0.11822818 0.12110816 0.1125325 0.10176606 0.097297587 0.10622028 0.12641795 0.1474752 0.15760837 0.15023124 0.125675][0.027277375 0.059042294 0.092720442 0.11854295 0.12857029 0.1209501 0.10101088 0.080184028 0.06888523 0.075567722 0.098884344 0.12892064 0.15237722 0.15959477 0.14766325][0.042433411 0.077891663 0.11123562 0.1322078 0.13439614 0.11747728 0.08810395 0.059032489 0.041378513 0.044255685 0.067188554 0.10196804 0.13560829 0.15674411 0.15927787]]...]
INFO - root - 2017-12-11 03:09:39.652085: step 84710, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 18h:58m:52s remains)
INFO - root - 2017-12-11 03:09:42.332637: step 84720, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 18h:28m:02s remains)
INFO - root - 2017-12-11 03:09:45.057321: step 84730, loss = 0.70, batch loss = 0.64 (28.1 examples/sec; 0.284 sec/batch; 19h:33m:52s remains)
INFO - root - 2017-12-11 03:09:47.777383: step 84740, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:49m:43s remains)
INFO - root - 2017-12-11 03:09:50.522933: step 84750, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 18h:29m:25s remains)
INFO - root - 2017-12-11 03:09:53.331415: step 84760, loss = 0.72, batch loss = 0.66 (29.2 examples/sec; 0.274 sec/batch; 18h:50m:58s remains)
INFO - root - 2017-12-11 03:09:56.032789: step 84770, loss = 0.71, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 18h:41m:54s remains)
INFO - root - 2017-12-11 03:09:58.763260: step 84780, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 18h:50m:52s remains)
INFO - root - 2017-12-11 03:10:01.506434: step 84790, loss = 0.71, batch loss = 0.65 (28.5 examples/sec; 0.281 sec/batch; 19h:18m:59s remains)
INFO - root - 2017-12-11 03:10:04.282186: step 84800, loss = 0.67, batch loss = 0.61 (29.2 examples/sec; 0.274 sec/batch; 18h:51m:49s remains)
2017-12-11 03:10:04.781309: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16817008 0.22914958 0.27246606 0.29152319 0.28412476 0.24466835 0.17581223 0.10314998 0.060005207 0.058484279 0.083694644 0.11654527 0.14137119 0.1513738 0.13805321][0.21380059 0.27693391 0.31278744 0.31638545 0.29108679 0.23670311 0.16016209 0.088459931 0.054830983 0.06969402 0.11311663 0.15998213 0.19097655 0.20040563 0.18109268][0.2591556 0.30878812 0.3218374 0.30012763 0.25445962 0.19250363 0.12276674 0.067589454 0.05477431 0.090739585 0.15115336 0.20729099 0.23855531 0.24306636 0.21751761][0.29786447 0.32032549 0.30020213 0.25127417 0.19125478 0.13387945 0.085007615 0.056940638 0.068277396 0.12084135 0.18888997 0.2439016 0.26841 0.26569319 0.23690407][0.32553741 0.31457669 0.26036775 0.19062455 0.12770067 0.086802848 0.06704282 0.067815036 0.097524375 0.15471287 0.2163596 0.25925007 0.27316254 0.26646176 0.24268019][0.33857393 0.29827374 0.21998776 0.1430921 0.090326324 0.073288485 0.08201801 0.10369372 0.13882224 0.18573292 0.22759937 0.25113937 0.25628832 0.253849 0.24484515][0.32842812 0.27192912 0.18795373 0.12078769 0.088726066 0.09648978 0.12545295 0.15431742 0.1801798 0.20481065 0.22070539 0.2270932 0.23266949 0.24500084 0.25754476][0.29360712 0.23777205 0.16766688 0.12382625 0.11682082 0.14419104 0.18084262 0.20213783 0.20679742 0.203143 0.19454606 0.1913742 0.20659085 0.2389904 0.27213758][0.23702909 0.19657819 0.15335743 0.13786338 0.15253302 0.18969452 0.22183207 0.22634651 0.20704806 0.17938158 0.1558588 0.15264937 0.18249738 0.2329438 0.27859062][0.17460632 0.15506497 0.13967471 0.14717864 0.17359757 0.20906316 0.22870074 0.21632887 0.18158598 0.14410208 0.11927657 0.12280421 0.16457888 0.22373222 0.27028009][0.12405571 0.12248801 0.12670948 0.14554936 0.17115067 0.19492759 0.19879609 0.17536183 0.13799584 0.10559659 0.090103477 0.10188664 0.14802165 0.20484954 0.24328409][0.096493267 0.10548238 0.11628792 0.13253766 0.14609274 0.15271857 0.14239603 0.11567739 0.086299293 0.068384051 0.067710452 0.087414637 0.13247181 0.18105039 0.20893364][0.095448993 0.10602622 0.11151925 0.11534052 0.11239504 0.10367364 0.084818147 0.06120507 0.04415258 0.041516002 0.053483073 0.077300757 0.11728188 0.1565319 0.17645343][0.1167661 0.12122554 0.11298355 0.10041547 0.082896061 0.064893112 0.044021908 0.026846239 0.020937216 0.028726464 0.047059733 0.070499383 0.10396594 0.13640633 0.15318252][0.14971092 0.14264859 0.11675904 0.088440694 0.061517209 0.041112013 0.023432497 0.013180017 0.014468358 0.026761133 0.046022683 0.066761784 0.093986347 0.12190432 0.13860482]]...]
INFO - root - 2017-12-11 03:10:07.586542: step 84810, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:52m:11s remains)
INFO - root - 2017-12-11 03:10:10.387357: step 84820, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 18h:30m:33s remains)
INFO - root - 2017-12-11 03:10:13.178402: step 84830, loss = 0.68, batch loss = 0.62 (28.8 examples/sec; 0.278 sec/batch; 19h:06m:35s remains)
INFO - root - 2017-12-11 03:10:15.949746: step 84840, loss = 0.70, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 18h:04m:33s remains)
INFO - root - 2017-12-11 03:10:18.715709: step 84850, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:51m:21s remains)
INFO - root - 2017-12-11 03:10:21.431998: step 84860, loss = 0.68, batch loss = 0.62 (29.7 examples/sec; 0.270 sec/batch; 18h:33m:31s remains)
INFO - root - 2017-12-11 03:10:24.171685: step 84870, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 18h:55m:35s remains)
INFO - root - 2017-12-11 03:10:26.959566: step 84880, loss = 0.70, batch loss = 0.64 (28.6 examples/sec; 0.280 sec/batch; 19h:15m:57s remains)
INFO - root - 2017-12-11 03:10:29.716434: step 84890, loss = 0.69, batch loss = 0.63 (28.3 examples/sec; 0.283 sec/batch; 19h:26m:22s remains)
INFO - root - 2017-12-11 03:10:32.521506: step 84900, loss = 0.71, batch loss = 0.65 (28.4 examples/sec; 0.282 sec/batch; 19h:22m:24s remains)
2017-12-11 03:10:32.989470: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.019383583 0.0378792 0.059741154 0.074969657 0.077349171 0.063163146 0.034417987 0.00067999226 -0.025391286 -0.037978303 -0.041705165 -0.043354139 -0.045895528 -0.047333356 -0.043796737][0.055440914 0.096937411 0.14030144 0.1704036 0.17910472 0.16202702 0.12215506 0.072781943 0.032631103 0.010189205 -0.0015163956 -0.012762837 -0.027791342 -0.042958811 -0.050799143][0.0983153 0.16992989 0.2429679 0.29588267 0.3176364 0.30263588 0.2546663 0.19057703 0.13531123 0.10026762 0.07517577 0.0460892 0.0098391958 -0.025855493 -0.049493495][0.14393398 0.24409193 0.34607577 0.422696 0.45995113 0.45123014 0.40130255 0.33002353 0.26580209 0.21990581 0.1774933 0.12326948 0.058888521 -0.00045909121 -0.04035183][0.18397227 0.30175754 0.42233267 0.51472276 0.56322539 0.56171954 0.51709378 0.45095479 0.39045545 0.34162989 0.28434652 0.20520951 0.11460258 0.03552714 -0.017286981][0.20040426 0.32059297 0.44690669 0.54646713 0.60319036 0.61274672 0.58360767 0.53568155 0.4903906 0.44564518 0.37810385 0.280638 0.17440781 0.087082669 0.029083597][0.18635538 0.2915493 0.40842745 0.50659394 0.57120425 0.5988993 0.59596843 0.57620448 0.55142158 0.51211208 0.43737569 0.33196706 0.22724885 0.14912193 0.098957777][0.15965165 0.23390828 0.32609028 0.41331095 0.48348728 0.53272378 0.56065333 0.5708037 0.56466174 0.529057 0.45198217 0.35300428 0.26947153 0.21955611 0.19226359][0.12923098 0.1641406 0.22247761 0.29046261 0.35979179 0.42478022 0.47661453 0.50877267 0.51616538 0.4869222 0.41920647 0.34367597 0.29773641 0.29020166 0.29873282][0.0870443 0.089306585 0.11917394 0.16946998 0.23355334 0.30272713 0.36208785 0.39978445 0.4109773 0.38950089 0.34152809 0.30116311 0.30046925 0.33945391 0.38782853][0.032474596 0.019096604 0.035663523 0.076864615 0.13448605 0.19656755 0.24643953 0.27325216 0.27762324 0.26110616 0.23423155 0.2281055 0.26632154 0.34209847 0.42129457][-0.020102922 -0.033824328 -0.018756866 0.017619355 0.064630009 0.10935453 0.13823634 0.14570826 0.13852525 0.12395809 0.1133849 0.1313819 0.19263929 0.28647134 0.37930226][-0.061566181 -0.06643375 -0.047666483 -0.016814023 0.014753615 0.036951635 0.042790622 0.032561336 0.016282929 0.0031906359 0.0025564444 0.031237401 0.096476488 0.18726355 0.27518514][-0.092974812 -0.086800143 -0.064015858 -0.040094506 -0.024721857 -0.022858433 -0.034456596 -0.055026859 -0.074578054 -0.085320033 -0.081955478 -0.0553049 -0.0030960094 0.066682033 0.13475341][-0.1119379 -0.097302467 -0.073000535 -0.055694334 -0.052385569 -0.063383311 -0.083847769 -0.10749569 -0.12641433 -0.13567291 -0.13347791 -0.11651745 -0.085003816 -0.043130219 -0.0010258598]]...]
INFO - root - 2017-12-11 03:10:35.763295: step 84910, loss = 0.68, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 18h:44m:19s remains)
INFO - root - 2017-12-11 03:10:38.505821: step 84920, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:46m:14s remains)
INFO - root - 2017-12-11 03:10:41.277912: step 84930, loss = 0.70, batch loss = 0.64 (28.2 examples/sec; 0.284 sec/batch; 19h:30m:34s remains)
INFO - root - 2017-12-11 03:10:43.952309: step 84940, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.268 sec/batch; 18h:25m:31s remains)
INFO - root - 2017-12-11 03:10:46.679377: step 84950, loss = 0.68, batch loss = 0.62 (28.3 examples/sec; 0.283 sec/batch; 19h:27m:44s remains)
INFO - root - 2017-12-11 03:10:49.463625: step 84960, loss = 0.69, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 18h:12m:29s remains)
INFO - root - 2017-12-11 03:10:52.169701: step 84970, loss = 0.69, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 18h:17m:42s remains)
INFO - root - 2017-12-11 03:10:54.843471: step 84980, loss = 0.68, batch loss = 0.62 (30.0 examples/sec; 0.267 sec/batch; 18h:21m:14s remains)
INFO - root - 2017-12-11 03:10:57.495237: step 84990, loss = 0.71, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 18h:45m:36s remains)
INFO - root - 2017-12-11 03:11:00.245961: step 85000, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:48m:55s remains)
2017-12-11 03:11:00.743175: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1522409 0.14366382 0.12840194 0.11531467 0.11093959 0.11668625 0.12972891 0.14533919 0.15944964 0.16882814 0.17368098 0.17529833 0.18022315 0.20425065 0.24467802][0.23917432 0.21451515 0.16815244 0.11681354 0.0775896 0.05921597 0.058942288 0.067858413 0.0792062 0.088506237 0.095096461 0.099169105 0.10575259 0.12876758 0.16628104][0.35908535 0.32135463 0.241688 0.14514351 0.060996674 0.0077165263 -0.017018365 -0.026079118 -0.027408792 -0.025478894 -0.022060083 -0.019496694 -0.015657937 0.00020874788 0.028620157][0.47313476 0.43689552 0.34258792 0.21875282 0.10242731 0.019361457 -0.032473158 -0.06717924 -0.090308115 -0.103926 -0.11062304 -0.11642032 -0.12299602 -0.12242016 -0.11017545][0.52836365 0.51716274 0.44372162 0.33227041 0.21798711 0.12699457 0.056942109 -0.0029971926 -0.052144457 -0.087815814 -0.11230143 -0.13503575 -0.16131882 -0.18534836 -0.19744344][0.48853868 0.51870304 0.50137603 0.44700411 0.37775779 0.3113589 0.24319749 0.16887851 0.09838742 0.041471735 -0.0030731584 -0.047371179 -0.099733956 -0.15536696 -0.19988614][0.3786912 0.44833937 0.49823284 0.52238882 0.52443713 0.50750637 0.46248317 0.391188 0.31366789 0.24620594 0.18828049 0.12565289 0.048017032 -0.041283846 -0.12429828][0.27034464 0.35331288 0.44382468 0.526813 0.59164369 0.62729681 0.61845022 0.56929827 0.50618541 0.44775695 0.39174861 0.32230726 0.22844598 0.11119863 -0.0088938372][0.22736931 0.28778294 0.36923471 0.46154568 0.55141687 0.62069607 0.64801711 0.63418984 0.60478127 0.57463664 0.53741252 0.47652498 0.38125902 0.24979354 0.10387529][0.28061 0.29021353 0.31678909 0.36652488 0.43658566 0.51158053 0.56640053 0.59234416 0.6059764 0.61346573 0.60388678 0.56195563 0.47857711 0.34975004 0.19658051][0.38656428 0.3408801 0.29399413 0.27504462 0.29942232 0.36213827 0.43738848 0.50308222 0.56030887 0.60373354 0.61989373 0.59799975 0.53265113 0.41965407 0.27761263][0.45145637 0.36828262 0.26565036 0.19063273 0.17510092 0.22603345 0.32066649 0.42427 0.5197131 0.58971077 0.62199092 0.61346585 0.5641889 0.47177687 0.35147816][0.4093014 0.31709877 0.19843708 0.10547475 0.077686958 0.12977356 0.24385242 0.37752125 0.49837571 0.58042252 0.61595392 0.6113261 0.57239676 0.500011 0.40609011][0.27008936 0.19369192 0.096870027 0.02417534 0.012340302 0.076970354 0.20489259 0.35242683 0.47934413 0.55693853 0.58370245 0.57414126 0.53919452 0.48423272 0.41861132][0.1032603 0.053054005 -0.00569883 -0.041522503 -0.026938241 0.049230497 0.17764246 0.3192763 0.4345091 0.49681136 0.50971544 0.49179772 0.45727816 0.41672876 0.37852627]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 03:11:03.477482: step 85010, loss = 0.71, batch loss = 0.65 (28.8 examples/sec; 0.278 sec/batch; 19h:06m:53s remains)
INFO - root - 2017-12-11 03:11:06.221325: step 85020, loss = 0.71, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 18h:43m:23s remains)
INFO - root - 2017-12-11 03:11:08.953634: step 85030, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.270 sec/batch; 18h:31m:49s remains)
INFO - root - 2017-12-11 03:11:11.633327: step 85040, loss = 0.72, batch loss = 0.66 (30.0 examples/sec; 0.267 sec/batch; 18h:20m:48s remains)
INFO - root - 2017-12-11 03:11:14.332616: step 85050, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.279 sec/batch; 19h:12m:08s remains)
INFO - root - 2017-12-11 03:11:17.019904: step 85060, loss = 0.70, batch loss = 0.64 (28.5 examples/sec; 0.281 sec/batch; 19h:18m:00s remains)
INFO - root - 2017-12-11 03:11:19.800585: step 85070, loss = 0.67, batch loss = 0.61 (29.5 examples/sec; 0.271 sec/batch; 18h:38m:16s remains)
INFO - root - 2017-12-11 03:11:22.553267: step 85080, loss = 0.70, batch loss = 0.64 (28.5 examples/sec; 0.280 sec/batch; 19h:16m:32s remains)
INFO - root - 2017-12-11 03:11:25.296105: step 85090, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.273 sec/batch; 18h:43m:48s remains)
INFO - root - 2017-12-11 03:11:28.020675: step 85100, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.270 sec/batch; 18h:31m:21s remains)
2017-12-11 03:11:28.530320: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.004099438 0.014194331 0.032807171 0.061903872 0.099991657 0.1375607 0.1634184 0.17266834 0.16935301 0.15712455 0.13874601 0.11703761 0.097340554 0.084417045 0.075837173][0.0030651132 0.011764176 0.030558124 0.060567919 0.099854834 0.13914378 0.16686422 0.17599046 0.1682398 0.14691028 0.11683701 0.084323309 0.056576252 0.038477555 0.027678957][0.0029302675 0.011307186 0.03152068 0.064132825 0.10684534 0.15035029 0.182952 0.19444405 0.18325759 0.15223603 0.10896999 0.064234525 0.027817765 0.0054013217 -0.0055349926][0.0031916352 0.012247163 0.035058945 0.07199946 0.12074246 0.17185391 0.21251948 0.22874907 0.21566184 0.17598322 0.12000664 0.06298551 0.017909296 -0.0081437379 -0.01820071][0.0034239122 0.013873576 0.039623316 0.081019655 0.13582307 0.19425325 0.24206427 0.26226142 0.24807124 0.20294182 0.13871813 0.073656388 0.023339311 -0.0037925493 -0.011635964][0.0041318666 0.016924141 0.045894086 0.091270469 0.15057591 0.21356133 0.26488465 0.28619245 0.27078056 0.22314796 0.15547764 0.087197036 0.035272829 0.0092030074 0.0041895411][0.0054915776 0.021223787 0.0537664 0.10324603 0.16600896 0.2309128 0.28191763 0.30103341 0.28331575 0.23440449 0.16642432 0.0985758 0.047708545 0.023444084 0.020157093][0.0064035039 0.025185097 0.061675344 0.11567399 0.181233 0.24586679 0.29336509 0.30744275 0.28582245 0.23540911 0.16871004 0.10384697 0.055945102 0.033447854 0.030280359][0.0070934682 0.029031556 0.069707178 0.12821946 0.19585493 0.2587699 0.30127671 0.3096731 0.28394696 0.23183353 0.16655381 0.10493424 0.059675872 0.037488863 0.032878932][0.0069784471 0.031391196 0.075370468 0.13730545 0.20617802 0.26726958 0.30574104 0.31038618 0.2819916 0.22835459 0.16343631 0.10326902 0.058762416 0.035525728 0.029637463][0.0054250187 0.030279238 0.074989 0.13803059 0.20735925 0.26778039 0.30521476 0.30995759 0.28186584 0.22789319 0.16267207 0.10246513 0.057590287 0.033182923 0.027422896][0.0026034242 0.025169237 0.066706382 0.12680379 0.19417027 0.25390631 0.29244065 0.30047727 0.27597323 0.22460666 0.16132981 0.10294736 0.059993595 0.037541986 0.034926303][0.00025499726 0.018989434 0.054397848 0.10769939 0.16974767 0.22719876 0.26695642 0.27966255 0.26170167 0.21718852 0.16014874 0.10695533 0.068532661 0.0503479 0.051861785][0.00024271394 0.015685776 0.045027629 0.090778768 0.14613338 0.20001887 0.23972087 0.25617805 0.24556966 0.21073888 0.163415 0.11811022 0.0854885 0.071363084 0.075175889][0.0025850183 0.016147578 0.041264754 0.081173405 0.13047802 0.17982098 0.21702543 0.23421746 0.22909595 0.20407042 0.16838086 0.13334826 0.10807018 0.098005407 0.10225893]]...]
INFO - root - 2017-12-11 03:11:31.269587: step 85110, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 18h:02m:06s remains)
INFO - root - 2017-12-11 03:11:33.984994: step 85120, loss = 0.71, batch loss = 0.65 (29.0 examples/sec; 0.276 sec/batch; 18h:59m:02s remains)
INFO - root - 2017-12-11 03:11:36.720986: step 85130, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:29m:55s remains)
INFO - root - 2017-12-11 03:11:39.397012: step 85140, loss = 0.71, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 18h:36m:47s remains)
INFO - root - 2017-12-11 03:11:42.146245: step 85150, loss = 0.69, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 18h:27m:19s remains)
INFO - root - 2017-12-11 03:11:44.839662: step 85160, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:44m:14s remains)
INFO - root - 2017-12-11 03:11:47.500299: step 85170, loss = 0.72, batch loss = 0.66 (30.4 examples/sec; 0.263 sec/batch; 18h:05m:00s remains)
INFO - root - 2017-12-11 03:11:50.262038: step 85180, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 18h:11m:34s remains)
INFO - root - 2017-12-11 03:11:53.033203: step 85190, loss = 0.71, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 18h:42m:34s remains)
INFO - root - 2017-12-11 03:11:55.850985: step 85200, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 18h:15m:39s remains)
2017-12-11 03:11:56.335170: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11490881 0.09460482 0.067458779 0.046160288 0.038473289 0.041316248 0.047744181 0.05608099 0.062650017 0.063405395 0.059357714 0.051515497 0.036400728 0.012406968 -0.015551412][0.18568633 0.16446359 0.13436316 0.11217186 0.10624405 0.111186 0.11856629 0.12806787 0.13755658 0.14045498 0.13810104 0.12961887 0.11000299 0.07630565 0.034168229][0.22790577 0.21043773 0.18543974 0.17086317 0.17250466 0.18246654 0.1918112 0.20274603 0.21405397 0.21910752 0.21943378 0.21197051 0.18964121 0.14768888 0.092809729][0.22306554 0.21114294 0.19524895 0.19244488 0.20469202 0.22168429 0.23458891 0.24755055 0.26081872 0.26996189 0.27594137 0.27283478 0.251569 0.20596048 0.14279011][0.18015072 0.17524588 0.1711884 0.18259132 0.20793463 0.23472549 0.25361153 0.26903078 0.28380105 0.29739651 0.30980769 0.31111103 0.29225361 0.24652416 0.17897771][0.12878968 0.13377647 0.14593008 0.17500381 0.21642415 0.25523588 0.28024128 0.29517791 0.30665743 0.31897396 0.33242506 0.3347663 0.31701005 0.27192509 0.20143519][0.085839033 0.10182178 0.13041003 0.17635053 0.23304911 0.28378245 0.31480011 0.32839593 0.33438563 0.34026203 0.34870702 0.34724826 0.32725447 0.28067228 0.20690159][0.0619799 0.0858548 0.12522475 0.18030311 0.2434409 0.2995123 0.33342129 0.3457078 0.34766221 0.34821051 0.35099539 0.34449822 0.32067797 0.27180472 0.19604018][0.058151331 0.084732041 0.12615997 0.18053658 0.24027352 0.29332715 0.32489082 0.33464277 0.33556959 0.33534479 0.33660206 0.32814258 0.30357435 0.25540549 0.18013553][0.064359777 0.088059694 0.12462228 0.17131032 0.22099726 0.26450714 0.28918985 0.29526183 0.29679909 0.29908445 0.30225953 0.2951653 0.27324143 0.22913985 0.15798615][0.066769287 0.08398772 0.11138489 0.14658865 0.18312995 0.21406339 0.22975944 0.23156486 0.23300722 0.2372946 0.24279447 0.23913911 0.22253154 0.18559568 0.1230787][0.05284581 0.062289134 0.079387009 0.10214101 0.1251722 0.14347588 0.15081197 0.14940476 0.15092872 0.15709518 0.16535787 0.16619149 0.15586077 0.12749292 0.076926321][0.025971441 0.029581368 0.038378488 0.050489802 0.062042505 0.070247255 0.071492217 0.0680226 0.069007404 0.07549426 0.08485058 0.088716179 0.083567157 0.063942194 0.0270829][-0.0033239061 -0.0033577196 -8.3968167e-05 0.0048193121 0.00884499 0.010638905 0.0081740785 0.0032946698 0.0025922877 0.0069427756 0.014651982 0.019350864 0.01814777 0.0070170634 -0.016164649][-0.029418921 -0.031746231 -0.031878095 -0.031514484 -0.03204377 -0.033835605 -0.037790552 -0.042675182 -0.04443825 -0.042336978 -0.037051652 -0.032730985 -0.031600166 -0.036321871 -0.048535]]...]
INFO - root - 2017-12-11 03:11:59.151480: step 85210, loss = 0.70, batch loss = 0.64 (27.7 examples/sec; 0.289 sec/batch; 19h:49m:55s remains)
INFO - root - 2017-12-11 03:12:01.924483: step 85220, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:48m:48s remains)
INFO - root - 2017-12-11 03:12:04.717995: step 85230, loss = 0.70, batch loss = 0.65 (28.0 examples/sec; 0.286 sec/batch; 19h:37m:54s remains)
INFO - root - 2017-12-11 03:12:07.508779: step 85240, loss = 0.70, batch loss = 0.64 (28.3 examples/sec; 0.283 sec/batch; 19h:26m:41s remains)
INFO - root - 2017-12-11 03:12:10.302235: step 85250, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.280 sec/batch; 19h:13m:45s remains)
INFO - root - 2017-12-11 03:12:13.028311: step 85260, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 18h:52m:27s remains)
INFO - root - 2017-12-11 03:12:15.767839: step 85270, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 18h:41m:51s remains)
INFO - root - 2017-12-11 03:12:18.491646: step 85280, loss = 0.68, batch loss = 0.62 (29.5 examples/sec; 0.271 sec/batch; 18h:38m:34s remains)
INFO - root - 2017-12-11 03:12:21.296009: step 85290, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 18h:53m:02s remains)
INFO - root - 2017-12-11 03:12:24.001303: step 85300, loss = 0.68, batch loss = 0.62 (29.7 examples/sec; 0.270 sec/batch; 18h:31m:25s remains)
2017-12-11 03:12:24.492627: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.086274892 0.090937778 0.091354609 0.086498253 0.0732218 0.056200709 0.043113124 0.035418473 0.032564156 0.032901574 0.035235655 0.036907822 0.035344392 0.031226983 0.027573952][0.093095 0.10938375 0.1214236 0.12382049 0.10954005 0.08416117 0.058158603 0.035716027 0.018812289 0.0078363437 0.0030542642 0.0010152283 -0.0018528119 -0.0051306849 -0.0069061588][0.084327929 0.11841381 0.15127039 0.17052884 0.16442204 0.13781892 0.1023137 0.063810155 0.027690561 -0.0014252434 -0.0207529 -0.033484466 -0.0426848 -0.046986476 -0.045742251][0.064598866 0.11943161 0.18020698 0.22653775 0.24176189 0.22776771 0.19394098 0.14510135 0.089397818 0.037068993 -0.0053931507 -0.038928289 -0.063459992 -0.074585669 -0.072920986][0.0331007 0.1031779 0.190438 0.2687453 0.31749865 0.33341354 0.31930581 0.275077 0.20856629 0.13466357 0.064678922 0.0017551347 -0.047645308 -0.073475696 -0.07670027][-0.002385132 0.070426896 0.1723761 0.27681515 0.36219522 0.41923946 0.44121048 0.41952732 0.35732809 0.27124482 0.17654799 0.0822432 0.0026825943 -0.044912495 -0.059633464][-0.031698808 0.02987314 0.12840557 0.24234293 0.35292366 0.447485 0.50998706 0.52177525 0.4781481 0.39376387 0.28538984 0.16716032 0.061021306 -0.0085412525 -0.03736335][-0.056442536 -0.015799783 0.062630542 0.16567817 0.28047243 0.39509985 0.48855743 0.53419852 0.51872671 0.45118827 0.34664211 0.22185701 0.10336161 0.020484529 -0.019096917][-0.077666685 -0.06095846 -0.012127801 0.0636806 0.16060996 0.27076992 0.37412745 0.4417257 0.45427528 0.41361123 0.33026496 0.22039609 0.11113918 0.031434473 -0.0091081392][-0.089133739 -0.092059344 -0.072417669 -0.030447302 0.033617653 0.11698788 0.20516472 0.27349898 0.30218974 0.28743896 0.23452042 0.15639332 0.076855868 0.018823616 -0.0093803694][-0.084936738 -0.097260311 -0.097050682 -0.083725892 -0.055154748 -0.010432824 0.043310091 0.090552419 0.11703192 0.11724556 0.092519671 0.051657747 0.011792848 -0.013757691 -0.021168668][-0.070080467 -0.080837034 -0.085701987 -0.086974144 -0.083904177 -0.074677005 -0.060040917 -0.045072436 -0.034871247 -0.032625787 -0.039147645 -0.049355794 -0.054062586 -0.049668774 -0.037504923][-0.051088277 -0.0516665 -0.047484469 -0.046638314 -0.052605417 -0.065337785 -0.081948243 -0.098258018 -0.10989495 -0.1152783 -0.11589475 -0.10913516 -0.09225405 -0.069087788 -0.044337329][-0.024472417 -0.010168164 0.012033759 0.026589431 0.024586707 0.0028522848 -0.034293585 -0.075250156 -0.1075774 -0.12435018 -0.12719509 -0.11542913 -0.090105638 -0.059958067 -0.030653214][0.013095837 0.04184318 0.083219916 0.11490849 0.12274086 0.09978655 0.050507586 -0.0078246407 -0.056339506 -0.083073005 -0.089701854 -0.077870883 -0.050720803 -0.020297581 0.0083082393]]...]
INFO - root - 2017-12-11 03:12:27.230294: step 85310, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.266 sec/batch; 18h:16m:58s remains)
INFO - root - 2017-12-11 03:12:29.971708: step 85320, loss = 0.69, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:44m:31s remains)
INFO - root - 2017-12-11 03:12:32.684971: step 85330, loss = 0.71, batch loss = 0.65 (28.8 examples/sec; 0.277 sec/batch; 19h:02m:27s remains)
INFO - root - 2017-12-11 03:12:35.353711: step 85340, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.272 sec/batch; 18h:38m:25s remains)
INFO - root - 2017-12-11 03:12:38.066583: step 85350, loss = 0.69, batch loss = 0.64 (29.6 examples/sec; 0.271 sec/batch; 18h:34m:19s remains)
INFO - root - 2017-12-11 03:12:40.752936: step 85360, loss = 0.71, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 18h:37m:11s remains)
INFO - root - 2017-12-11 03:12:43.505718: step 85370, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.268 sec/batch; 18h:22m:51s remains)
INFO - root - 2017-12-11 03:12:46.221449: step 85380, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 18h:24m:22s remains)
INFO - root - 2017-12-11 03:12:48.974878: step 85390, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:29m:22s remains)
INFO - root - 2017-12-11 03:12:51.706052: step 85400, loss = 0.70, batch loss = 0.64 (28.8 examples/sec; 0.278 sec/batch; 19h:03m:28s remains)
2017-12-11 03:12:52.173980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0083133336 -0.015684294 -0.024364879 -0.031161398 -0.034149811 -0.032734897 -0.025569292 -0.015219755 -0.00050679973 0.01698258 0.028034668 0.025398541 0.010371787 -0.0099447025 -0.032702513][0.039950363 0.031999934 0.022859605 0.019578291 0.023213144 0.0328409 0.049357593 0.068685107 0.091786079 0.11622342 0.12986718 0.12242454 0.095833711 0.059913673 0.018218379][0.099611647 0.095571063 0.092703626 0.10131621 0.1204265 0.14585042 0.17696159 0.20743324 0.23635246 0.26244587 0.27328232 0.25786102 0.21753076 0.16251664 0.0969051][0.15547642 0.16039892 0.17041138 0.19761573 0.23785098 0.283381 0.33231348 0.37534547 0.40675044 0.4273144 0.42893463 0.40233633 0.34685305 0.27185991 0.18158513][0.20101491 0.2170091 0.24203962 0.2886644 0.34942883 0.4145337 0.48183697 0.53804708 0.57040507 0.58184624 0.57057607 0.5301823 0.45760927 0.36189035 0.24859487][0.2519716 0.27810648 0.31497541 0.374663 0.44765934 0.52567512 0.607268 0.67507076 0.70815039 0.71029204 0.68500125 0.62912112 0.53879327 0.42323783 0.29077744][0.31871805 0.35318738 0.3956019 0.45691544 0.52696562 0.60338628 0.68664151 0.758386 0.78983694 0.78358477 0.74698317 0.6798948 0.578262 0.4503032 0.30750889][0.39589545 0.43825117 0.47849178 0.52392834 0.56670082 0.61819392 0.68607986 0.75362736 0.78407842 0.77631229 0.73928475 0.67326784 0.57255745 0.44382089 0.30115494][0.47433591 0.52779508 0.5603683 0.5736022 0.56680375 0.57203335 0.60893363 0.66506112 0.6971395 0.69791424 0.67475063 0.62363505 0.5360418 0.4165318 0.28147051][0.53595936 0.60085088 0.62314457 0.59860086 0.53646225 0.48934305 0.48982075 0.53080255 0.56667739 0.58410388 0.5843783 0.555907 0.48718667 0.38126045 0.25666234][0.55408227 0.62312967 0.6341818 0.5799253 0.47720194 0.39095607 0.36281154 0.39149526 0.43295461 0.46828127 0.49019089 0.47995117 0.426204 0.33284992 0.22077766][0.49193716 0.55213529 0.55138797 0.48284134 0.36729518 0.26937988 0.23077184 0.25356987 0.29871127 0.34455836 0.37728781 0.37505603 0.33024395 0.25045678 0.15790391][0.34845471 0.38828421 0.37707466 0.3107281 0.20987391 0.12704085 0.095192105 0.11692179 0.16106556 0.2078899 0.2412878 0.24083406 0.20396809 0.14212231 0.076148078][0.16554463 0.18115583 0.16429269 0.11350633 0.045192294 -0.006613438 -0.021917272 0.00020131684 0.03802368 0.07728789 0.1042487 0.10384183 0.0771306 0.0361791 -0.0024681245][0.004305481 -0.0010626259 -0.019484643 -0.051269047 -0.086101308 -0.10720727 -0.1064525 -0.085265353 -0.056777805 -0.029069992 -0.011272014 -0.011788113 -0.027979759 -0.050072804 -0.067025021]]...]
INFO - root - 2017-12-11 03:12:54.912462: step 85410, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 18h:17m:36s remains)
INFO - root - 2017-12-11 03:12:57.646918: step 85420, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.272 sec/batch; 18h:38m:33s remains)
INFO - root - 2017-12-11 03:13:00.356792: step 85430, loss = 0.69, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 18h:26m:20s remains)
INFO - root - 2017-12-11 03:13:03.114108: step 85440, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:43m:49s remains)
INFO - root - 2017-12-11 03:13:05.837261: step 85450, loss = 0.72, batch loss = 0.66 (30.4 examples/sec; 0.263 sec/batch; 18h:04m:25s remains)
INFO - root - 2017-12-11 03:13:08.562834: step 85460, loss = 0.70, batch loss = 0.64 (28.4 examples/sec; 0.282 sec/batch; 19h:20m:19s remains)
INFO - root - 2017-12-11 03:13:11.263219: step 85470, loss = 0.71, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 18h:45m:40s remains)
INFO - root - 2017-12-11 03:13:13.993342: step 85480, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 18h:53m:35s remains)
INFO - root - 2017-12-11 03:13:16.677226: step 85490, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 18h:17m:11s remains)
INFO - root - 2017-12-11 03:13:19.384070: step 85500, loss = 0.70, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:30m:45s remains)
2017-12-11 03:13:19.855170: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13775358 0.13932091 0.13841812 0.13660602 0.13398536 0.13118757 0.12869169 0.12667631 0.12499669 0.12380692 0.13032232 0.14802036 0.16913041 0.18356085 0.18525191][0.16195962 0.16353294 0.16022384 0.15557504 0.15041409 0.14578703 0.14216684 0.13935129 0.13676403 0.13458478 0.14046152 0.15750058 0.17744483 0.19053005 0.19131987][0.16131344 0.16331066 0.15929994 0.15410461 0.14915259 0.14559706 0.14339511 0.1417176 0.139589 0.13674113 0.14012043 0.15289213 0.16800635 0.17774262 0.17815378][0.14262861 0.1472656 0.14703645 0.14669846 0.14752246 0.15017967 0.1536644 0.15612467 0.15634964 0.15333781 0.15283063 0.15799782 0.16466513 0.16818404 0.16602866][0.11548241 0.12481757 0.13116813 0.13882165 0.14887294 0.16116624 0.17336209 0.1822888 0.18643896 0.18414566 0.17981718 0.17728226 0.17562202 0.17288911 0.16659179][0.088083647 0.10278641 0.11665754 0.13338542 0.15416417 0.17826471 0.20135611 0.21807939 0.22612232 0.22321928 0.21314527 0.20190205 0.19228153 0.18429478 0.17344302][0.072191671 0.092761859 0.11506426 0.14245225 0.17592356 0.2135082 0.24797718 0.27091852 0.27897084 0.27031624 0.24976043 0.22665173 0.20732377 0.19332618 0.17789653][0.063257046 0.0905508 0.12408783 0.16605251 0.21506645 0.26593018 0.30829653 0.33152956 0.33270365 0.31267431 0.27941367 0.2448311 0.21639247 0.19607519 0.17581446][0.057241146 0.092374623 0.13885601 0.19624695 0.25833958 0.31564176 0.35658571 0.37177694 0.36117846 0.32957625 0.28835532 0.24953498 0.21818733 0.19472633 0.17044835][0.057130352 0.098333932 0.15426473 0.2212794 0.28829569 0.34268475 0.37379849 0.37602019 0.35319272 0.31361324 0.27096191 0.23570867 0.2088023 0.18769847 0.16320722][0.068779394 0.1119443 0.170607 0.23891671 0.30270979 0.34837607 0.36731949 0.35777235 0.32649767 0.28374353 0.24430995 0.21706367 0.19951743 0.18589097 0.16630122][0.087466531 0.12677348 0.17993325 0.24075536 0.29492027 0.3299199 0.33967379 0.32494754 0.29297492 0.253369 0.22133335 0.20482253 0.19910602 0.1954831 0.18331908][0.11845025 0.15019749 0.19087709 0.23699217 0.27676907 0.30066338 0.30517438 0.29206875 0.26692617 0.23686969 0.21684158 0.21354753 0.22025816 0.2257982 0.21912256][0.15668251 0.17882653 0.20350233 0.23112345 0.2542364 0.2675007 0.26977533 0.26268 0.24898046 0.23233689 0.22638856 0.2360689 0.25284979 0.26390806 0.25851107][0.16806746 0.18181339 0.19301252 0.20526142 0.21517274 0.2209909 0.22302067 0.22212312 0.21855521 0.21307366 0.21751124 0.23517078 0.25659209 0.26891646 0.2630226]]...]
INFO - root - 2017-12-11 03:13:22.559709: step 85510, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 18h:35m:42s remains)
INFO - root - 2017-12-11 03:13:25.271393: step 85520, loss = 0.70, batch loss = 0.64 (28.5 examples/sec; 0.281 sec/batch; 19h:16m:34s remains)
INFO - root - 2017-12-11 03:13:28.025860: step 85530, loss = 0.69, batch loss = 0.63 (28.7 examples/sec; 0.279 sec/batch; 19h:07m:57s remains)
INFO - root - 2017-12-11 03:13:30.793275: step 85540, loss = 0.69, batch loss = 0.64 (28.1 examples/sec; 0.284 sec/batch; 19h:30m:21s remains)
INFO - root - 2017-12-11 03:13:33.493158: step 85550, loss = 0.71, batch loss = 0.65 (29.7 examples/sec; 0.269 sec/batch; 18h:26m:58s remains)
INFO - root - 2017-12-11 03:13:36.267982: step 85560, loss = 0.69, batch loss = 0.63 (28.0 examples/sec; 0.285 sec/batch; 19h:34m:25s remains)
INFO - root - 2017-12-11 03:13:39.022860: step 85570, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 18h:51m:10s remains)
INFO - root - 2017-12-11 03:13:41.705051: step 85580, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.267 sec/batch; 18h:19m:54s remains)
INFO - root - 2017-12-11 03:13:44.446175: step 85590, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.280 sec/batch; 19h:12m:46s remains)
INFO - root - 2017-12-11 03:13:47.223703: step 85600, loss = 0.71, batch loss = 0.65 (28.3 examples/sec; 0.283 sec/batch; 19h:23m:13s remains)
2017-12-11 03:13:47.770992: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30771706 0.30250365 0.29205734 0.28655866 0.2872583 0.28852642 0.28990459 0.29448536 0.30266821 0.30266958 0.29674318 0.29394233 0.29240009 0.27680367 0.23569459][0.2957944 0.29284298 0.28716823 0.287473 0.28966188 0.28717506 0.28114825 0.27630785 0.27674049 0.2719869 0.26132417 0.25327715 0.24960993 0.23790967 0.20555687][0.3006191 0.29103577 0.28108057 0.28145605 0.28460017 0.27946466 0.26437777 0.24501097 0.23038073 0.21615699 0.20036303 0.18961218 0.18618566 0.17975782 0.15770954][0.33490545 0.31053823 0.28565279 0.27865028 0.28130606 0.27699891 0.25733006 0.22558983 0.19397803 0.16678834 0.1448552 0.13434024 0.13561793 0.13802373 0.12770972][0.36395228 0.32607529 0.28507191 0.26825196 0.27031207 0.27220002 0.25820893 0.22420676 0.18091257 0.13930295 0.10726534 0.095845655 0.10475279 0.1208977 0.12548259][0.38966888 0.34310204 0.2874217 0.25804728 0.25737005 0.268882 0.270935 0.24804634 0.2032021 0.14866847 0.10190164 0.085574821 0.10369983 0.13931595 0.16451247][0.41440278 0.36336079 0.29452568 0.2506088 0.24498098 0.2671926 0.29198647 0.29119605 0.25585306 0.19452658 0.13256504 0.1070259 0.1314245 0.18768997 0.23748225][0.41947782 0.36775813 0.2923066 0.2382832 0.2279651 0.25799918 0.30319113 0.32697988 0.30906495 0.2511791 0.17984754 0.14158225 0.16142067 0.22692822 0.29525006][0.38946772 0.34292138 0.27136093 0.21632212 0.20381558 0.23591112 0.29180673 0.33436549 0.33677071 0.29279146 0.22314802 0.17466277 0.18031257 0.23859796 0.31151626][0.32363617 0.28493544 0.22481647 0.17706177 0.165724 0.19489509 0.25067657 0.30246884 0.32271156 0.29867318 0.24169926 0.19089866 0.18042842 0.21921416 0.28141817][0.23043273 0.19939083 0.15326871 0.11698329 0.10942612 0.13367203 0.18225481 0.23396449 0.26481441 0.25940359 0.22056225 0.17642359 0.15598556 0.17340906 0.21586098][0.12136563 0.095626853 0.061969046 0.037432443 0.035130121 0.055699337 0.095215142 0.14004607 0.17181315 0.17706251 0.15426426 0.12186649 0.1003103 0.1028204 0.12475285][0.011304651 -0.010356262 -0.032750983 -0.046506085 -0.043442249 -0.024908962 0.0061186654 0.040483508 0.065866232 0.073439531 0.061214328 0.040754281 0.024251465 0.019982355 0.026226154][-0.071453713 -0.088119119 -0.10082584 -0.10648602 -0.1003306 -0.084307969 -0.06094275 -0.036777023 -0.019201528 -0.012926991 -0.019038016 -0.030064842 -0.039665289 -0.044432562 -0.045927994][-0.11233245 -0.12506436 -0.13200958 -0.13419349 -0.12872392 -0.1168594 -0.10058893 -0.084536754 -0.072929874 -0.068145238 -0.070639163 -0.075438805 -0.079537496 -0.082397379 -0.085630774]]...]
INFO - root - 2017-12-11 03:13:50.530176: step 85610, loss = 0.69, batch loss = 0.64 (28.1 examples/sec; 0.284 sec/batch; 19h:29m:26s remains)
INFO - root - 2017-12-11 03:13:53.258550: step 85620, loss = 0.70, batch loss = 0.64 (28.3 examples/sec; 0.283 sec/batch; 19h:25m:09s remains)
INFO - root - 2017-12-11 03:13:56.019196: step 85630, loss = 0.70, batch loss = 0.64 (28.7 examples/sec; 0.278 sec/batch; 19h:05m:27s remains)
INFO - root - 2017-12-11 03:13:58.786218: step 85640, loss = 0.71, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 18h:46m:55s remains)
INFO - root - 2017-12-11 03:14:01.505772: step 85650, loss = 0.68, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 18h:53m:58s remains)
INFO - root - 2017-12-11 03:14:04.267433: step 85660, loss = 0.69, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:44m:56s remains)
INFO - root - 2017-12-11 03:14:07.052018: step 85670, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.272 sec/batch; 18h:37m:00s remains)
INFO - root - 2017-12-11 03:14:09.796718: step 85680, loss = 0.69, batch loss = 0.63 (28.3 examples/sec; 0.283 sec/batch; 19h:24m:54s remains)
INFO - root - 2017-12-11 03:14:12.565207: step 85690, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 18h:56m:29s remains)
INFO - root - 2017-12-11 03:14:15.304458: step 85700, loss = 0.70, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 18h:49m:38s remains)
2017-12-11 03:14:15.848742: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17318335 0.13961741 0.11417142 0.092017524 0.0667147 0.04216902 0.01719765 -0.0081692711 -0.03159241 -0.047786146 -0.049246378 -0.012976682 0.064088173 0.16588017 0.26381809][0.22838964 0.19324915 0.1657512 0.1428162 0.11772893 0.0949108 0.072595738 0.048237652 0.022820679 0.0016496268 -0.0074440422 0.0202192 0.089112386 0.18293959 0.27235797][0.26189566 0.23347923 0.21262309 0.19819996 0.18344471 0.17288382 0.16329262 0.1472342 0.12284673 0.095248565 0.073001362 0.080503382 0.12396 0.18958303 0.25115815][0.24968621 0.235784 0.23160315 0.23534535 0.2403363 0.25171262 0.26461831 0.26487502 0.24698035 0.21520345 0.17863816 0.15920502 0.16535969 0.18925242 0.2122727][0.20187138 0.20927745 0.22893706 0.25681093 0.28775442 0.32795423 0.36974317 0.39119902 0.38314509 0.34979466 0.30036336 0.25300604 0.2191086 0.19849031 0.18255366][0.14443207 0.17682533 0.22395776 0.27841732 0.33766049 0.40801215 0.47744778 0.51743042 0.51626259 0.48031747 0.41895181 0.34729323 0.27991572 0.22338846 0.17743464][0.094927192 0.14743137 0.21800718 0.29523596 0.37870663 0.47277516 0.56036431 0.609949 0.60912424 0.56705344 0.49464145 0.40666375 0.32050151 0.24585557 0.18586279][0.064437412 0.12450825 0.20427388 0.29131979 0.38671473 0.4903962 0.5813868 0.630221 0.6256485 0.57754362 0.49880856 0.40689209 0.32048506 0.24841557 0.19217646][0.050107349 0.10455246 0.17662016 0.25635529 0.34625232 0.44126445 0.51952994 0.55766749 0.54727393 0.49728364 0.42159927 0.34045768 0.2709493 0.21805058 0.17887202][0.038428988 0.078617834 0.13147739 0.19158696 0.26276532 0.33634382 0.39205062 0.41401735 0.39766431 0.35046875 0.2860752 0.2252668 0.18111148 0.1540343 0.13676219][0.012883648 0.035653822 0.065677628 0.10179157 0.14880213 0.19699754 0.22947335 0.2366997 0.21725716 0.17698197 0.12835288 0.090050019 0.070181593 0.065539569 0.066649631][-0.023146978 -0.015487012 -0.0053518163 0.008364277 0.031318504 0.055653002 0.069096424 0.066356994 0.048040554 0.018366262 -0.012414736 -0.030221786 -0.03172309 -0.021751568 -0.0093140807][-0.0558161 -0.057173222 -0.059021249 -0.060275633 -0.055321719 -0.048721921 -0.047791872 -0.0551808 -0.069278784 -0.087252706 -0.10209846 -0.10516172 -0.0966941 -0.081238508 -0.065409593][-0.078128763 -0.082981206 -0.088891514 -0.09605547 -0.10001667 -0.10257649 -0.10728361 -0.11523467 -0.12442596 -0.13302377 -0.13732214 -0.13329777 -0.12276705 -0.10893346 -0.095680431][-0.089249708 -0.094600543 -0.099829488 -0.10672214 -0.11245046 -0.11727852 -0.12265946 -0.12890205 -0.1341804 -0.13731773 -0.13694818 -0.1318782 -0.12382606 -0.11451758 -0.10586359]]...]
INFO - root - 2017-12-11 03:14:18.626376: step 85710, loss = 0.71, batch loss = 0.65 (29.0 examples/sec; 0.276 sec/batch; 18h:53m:21s remains)
INFO - root - 2017-12-11 03:14:21.449899: step 85720, loss = 0.68, batch loss = 0.63 (27.5 examples/sec; 0.291 sec/batch; 19h:55m:10s remains)
INFO - root - 2017-12-11 03:14:24.225411: step 85730, loss = 0.69, batch loss = 0.63 (28.5 examples/sec; 0.281 sec/batch; 19h:16m:25s remains)
INFO - root - 2017-12-11 03:14:26.944283: step 85740, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.271 sec/batch; 18h:32m:39s remains)
INFO - root - 2017-12-11 03:14:29.702189: step 85750, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 18h:49m:50s remains)
INFO - root - 2017-12-11 03:14:32.462352: step 85760, loss = 0.67, batch loss = 0.61 (28.8 examples/sec; 0.278 sec/batch; 19h:01m:30s remains)
INFO - root - 2017-12-11 03:14:35.226823: step 85770, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 18h:54m:10s remains)
INFO - root - 2017-12-11 03:14:38.028833: step 85780, loss = 0.70, batch loss = 0.64 (27.4 examples/sec; 0.292 sec/batch; 20h:00m:48s remains)
INFO - root - 2017-12-11 03:14:40.834221: step 85790, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.280 sec/batch; 19h:11m:45s remains)
INFO - root - 2017-12-11 03:14:43.612369: step 85800, loss = 0.70, batch loss = 0.65 (28.3 examples/sec; 0.283 sec/batch; 19h:22m:32s remains)
2017-12-11 03:14:44.131423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.012099702 -0.012064828 -0.009967776 -0.0025460282 0.0049272575 0.012120754 0.020260094 0.036997903 0.068704553 0.11104468 0.1523646 0.18110278 0.19838548 0.20587929 0.20887756][0.031693477 0.032091714 0.037826 0.052895531 0.067412943 0.077763937 0.083784044 0.096251108 0.12557031 0.16758424 0.20823397 0.23337321 0.24433936 0.24440877 0.23970911][0.10466597 0.10379472 0.11326661 0.13712861 0.16031349 0.17601164 0.18260145 0.19012982 0.20927531 0.23661394 0.25977278 0.26599327 0.2589505 0.24505119 0.23123148][0.19022073 0.18603957 0.19889444 0.23148121 0.26481453 0.29192016 0.31050873 0.32248536 0.33121622 0.33388636 0.32486984 0.29721481 0.25915682 0.22324656 0.19913115][0.26350754 0.258155 0.27867702 0.3236208 0.37215555 0.42125958 0.46855819 0.49953997 0.50169683 0.47296578 0.41997713 0.34621784 0.2665793 0.20289698 0.16966978][0.29787239 0.30013898 0.33795217 0.40392774 0.47638786 0.55758804 0.64505279 0.70429915 0.70236528 0.63865668 0.53609318 0.41254982 0.29015341 0.20014468 0.16139451][0.28888717 0.30773681 0.36933473 0.46128702 0.56160575 0.67547685 0.79933512 0.88296551 0.87664706 0.78176886 0.63661009 0.47346556 0.31999558 0.21357593 0.17375459][0.24884208 0.28665876 0.36930135 0.48225823 0.60407931 0.73886174 0.88056427 0.973456 0.96144807 0.84771466 0.67984623 0.49931598 0.33566365 0.22704433 0.18959327][0.19649658 0.24867705 0.34143198 0.46132085 0.58852756 0.72294885 0.85520858 0.93607342 0.91522652 0.79751647 0.63112825 0.45910126 0.30910712 0.21385527 0.18341698][0.1583204 0.21411274 0.29995504 0.40699163 0.5188036 0.63047707 0.72936028 0.78100222 0.74941671 0.64045668 0.49500662 0.35197791 0.23554744 0.16801393 0.15113813][0.15727776 0.20346773 0.2653152 0.34152558 0.42096144 0.49628049 0.553522 0.57379705 0.536467 0.44629964 0.33251658 0.22759084 0.15288416 0.11923712 0.11938361][0.20384909 0.23222858 0.26138398 0.29785439 0.33790213 0.37635002 0.40104178 0.40349656 0.37201366 0.30670223 0.22457941 0.15192059 0.1099138 0.10204765 0.11500239][0.28492051 0.29806119 0.29904822 0.30046293 0.3063297 0.31703046 0.32526374 0.32728633 0.31212074 0.27220634 0.21388291 0.15906765 0.13142936 0.13375135 0.15158665][0.3824816 0.39061373 0.37607625 0.35484073 0.33773518 0.33140868 0.3341181 0.3434661 0.34576815 0.32585126 0.28151661 0.23174384 0.20369057 0.20441622 0.22129118][0.47281942 0.48553675 0.46764475 0.4363876 0.40785652 0.39372647 0.39585593 0.41170368 0.42588782 0.41823062 0.38047054 0.32875675 0.2928572 0.28556886 0.29716671]]...]
INFO - root - 2017-12-11 03:14:46.877154: step 85810, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.264 sec/batch; 18h:07m:24s remains)
INFO - root - 2017-12-11 03:14:49.603979: step 85820, loss = 0.71, batch loss = 0.65 (27.8 examples/sec; 0.288 sec/batch; 19h:42m:03s remains)
INFO - root - 2017-12-11 03:14:52.453870: step 85830, loss = 0.69, batch loss = 0.63 (26.9 examples/sec; 0.297 sec/batch; 20h:21m:51s remains)
INFO - root - 2017-12-11 03:14:55.222938: step 85840, loss = 0.68, batch loss = 0.62 (28.7 examples/sec; 0.279 sec/batch; 19h:05m:55s remains)
INFO - root - 2017-12-11 03:14:58.009156: step 85850, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.266 sec/batch; 18h:14m:39s remains)
INFO - root - 2017-12-11 03:15:00.759150: step 85860, loss = 0.71, batch loss = 0.65 (28.1 examples/sec; 0.285 sec/batch; 19h:29m:41s remains)
INFO - root - 2017-12-11 03:15:03.489181: step 85870, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 18h:59m:33s remains)
INFO - root - 2017-12-11 03:15:06.247393: step 85880, loss = 0.71, batch loss = 0.65 (27.0 examples/sec; 0.296 sec/batch; 20h:16m:29s remains)
INFO - root - 2017-12-11 03:15:09.014172: step 85890, loss = 0.71, batch loss = 0.65 (28.8 examples/sec; 0.277 sec/batch; 18h:59m:47s remains)
INFO - root - 2017-12-11 03:15:11.772461: step 85900, loss = 0.68, batch loss = 0.62 (29.4 examples/sec; 0.272 sec/batch; 18h:37m:30s remains)
2017-12-11 03:15:12.229544: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0048029446 -0.0033072627 -0.014494424 -0.025158837 -0.036466192 -0.050893195 -0.069211386 -0.093016557 -0.12427308 -0.16081257 -0.19573542 -0.22062284 -0.22920448 -0.21885724 -0.19183254][-0.012378586 -0.019333024 -0.023677906 -0.01928808 -0.00791026 0.0044817775 0.010872116 0.0039457683 -0.023455082 -0.068760209 -0.12137845 -0.16700229 -0.19308768 -0.19404045 -0.17235044][-0.02486611 -0.024458257 -0.01546426 0.010073414 0.051727265 0.10130447 0.14502022 0.16643821 0.15007181 0.09653116 0.019588757 -0.058523182 -0.11606862 -0.14062719 -0.13423777][-0.025682284 -0.012781182 0.014872258 0.06674403 0.14604367 0.24390818 0.33817568 0.39849538 0.39705807 0.33027419 0.21683484 0.090507194 -0.013602624 -0.072479159 -0.086910039][-0.013076874 0.015824983 0.065889783 0.14768858 0.26973346 0.42434594 0.57954508 0.68583465 0.69899261 0.61083573 0.44863415 0.26210237 0.10357791 0.0085992059 -0.023761559][0.007574799 0.056565188 0.1328169 0.24659671 0.41106173 0.62011206 0.83154905 0.97611767 0.99450642 0.87645131 0.66066438 0.41482496 0.20851311 0.087113529 0.046089068][0.020820497 0.087070458 0.18735996 0.32980463 0.52920818 0.77908355 1.0286707 1.1943101 1.2072314 1.0584087 0.79728717 0.50711656 0.27026847 0.13623527 0.095651582][0.0090603642 0.079478279 0.19079997 0.34952608 0.56802654 0.83648366 1.0986021 1.2647692 1.2642987 1.0927529 0.80767167 0.500612 0.258456 0.12859723 0.097526692][-0.024588777 0.034919772 0.13904975 0.2930367 0.50385088 0.75776494 0.99891776 1.1428756 1.1262391 0.95124149 0.67689472 0.39080587 0.1732171 0.064627305 0.049769152][-0.062117312 -0.0223778 0.05902081 0.18589722 0.35995325 0.5654968 0.754398 0.85778683 0.82697064 0.66984403 0.43906569 0.20743681 0.039650824 -0.03384978 -0.028414186][-0.093013816 -0.076427609 -0.027807798 0.056014594 0.17374821 0.31068343 0.43153271 0.48892733 0.45185265 0.33063471 0.16573237 0.0090722283 -0.094932318 -0.12746951 -0.10324986][-0.111893 -0.11732692 -0.10328721 -0.066551588 -0.0087324036 0.059451751 0.11686234 0.13705482 0.10435118 0.028340021 -0.064832307 -0.14467359 -0.18714134 -0.18423431 -0.14897718][-0.11519309 -0.13327593 -0.142865 -0.14124401 -0.12911183 -0.11269834 -0.101909 -0.10727161 -0.133109 -0.17134696 -0.20744629 -0.2276087 -0.22401011 -0.19762079 -0.15963165][-0.10614756 -0.12182071 -0.13523048 -0.14686784 -0.15786956 -0.17176886 -0.19137497 -0.21630204 -0.24205336 -0.26039857 -0.2638267 -0.24909782 -0.21919166 -0.18154004 -0.14612141][-0.090175517 -0.0911734 -0.090464905 -0.093785912 -0.10548905 -0.12968452 -0.16602042 -0.20759109 -0.24328712 -0.26202437 -0.25815827 -0.23327824 -0.19617893 -0.15782733 -0.12775557]]...]
INFO - root - 2017-12-11 03:15:15.019694: step 85910, loss = 0.69, batch loss = 0.63 (28.7 examples/sec; 0.279 sec/batch; 19h:06m:58s remains)
INFO - root - 2017-12-11 03:15:17.771831: step 85920, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 18h:52m:51s remains)
INFO - root - 2017-12-11 03:15:20.542869: step 85930, loss = 0.71, batch loss = 0.65 (28.2 examples/sec; 0.284 sec/batch; 19h:26m:01s remains)
INFO - root - 2017-12-11 03:15:23.302056: step 85940, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.266 sec/batch; 18h:14m:52s remains)
INFO - root - 2017-12-11 03:15:25.999088: step 85950, loss = 0.71, batch loss = 0.65 (29.8 examples/sec; 0.269 sec/batch; 18h:23m:52s remains)
INFO - root - 2017-12-11 03:15:28.796272: step 85960, loss = 0.70, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 18h:46m:42s remains)
INFO - root - 2017-12-11 03:15:31.551169: step 85970, loss = 0.70, batch loss = 0.65 (28.3 examples/sec; 0.282 sec/batch; 19h:20m:08s remains)
INFO - root - 2017-12-11 03:15:34.372232: step 85980, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.271 sec/batch; 18h:31m:28s remains)
INFO - root - 2017-12-11 03:15:37.104908: step 85990, loss = 0.70, batch loss = 0.65 (28.0 examples/sec; 0.286 sec/batch; 19h:33m:02s remains)
INFO - root - 2017-12-11 03:15:39.950908: step 86000, loss = 0.69, batch loss = 0.63 (28.2 examples/sec; 0.283 sec/batch; 19h:23m:46s remains)
2017-12-11 03:15:40.426966: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0097146761 -0.0095380917 -0.010234455 -0.011464283 -0.01312605 -0.015083573 -0.017315822 -0.019744562 -0.02207651 -0.024346277 -0.025737034 -0.026757741 -0.027878484 -0.028757848 -0.028508188][-0.0053205234 -0.0032359674 -0.0020712714 -0.0014300972 -0.001706358 -0.0030542018 -0.0056120693 -0.0091756331 -0.013432272 -0.017538881 -0.019939838 -0.021571588 -0.023062021 -0.024290513 -0.023781192][-0.0022711053 0.0039161546 0.010903865 0.018347623 0.024149733 0.026981268 0.02642162 0.022432227 0.014951911 0.0061988975 -0.00057720148 -0.00542786 -0.0092340456 -0.012273152 -0.012520157][-0.00097897917 0.011030546 0.027333185 0.045941256 0.062207721 0.0732597 0.07789667 0.074995652 0.063103676 0.046328355 0.031281576 0.020254251 0.012234474 0.0062964042 0.0044796108][-0.0011474724 0.017242325 0.044370577 0.076049484 0.10523143 0.12767361 0.14056031 0.1403982 0.12336577 0.096087888 0.070029 0.050657894 0.037400935 0.027960537 0.024182793][-0.0011300736 0.022926254 0.059413653 0.1024963 0.14385325 0.17824058 0.20070161 0.20421903 0.18259658 0.14510314 0.10804182 0.080217458 0.062086575 0.049442723 0.044200785][-0.00073128514 0.026224554 0.067679815 0.11710777 0.16644105 0.21002927 0.24042509 0.24738453 0.22373159 0.18089779 0.13741991 0.10382419 0.082245439 0.068018876 0.062934384][-0.0020192033 0.023992479 0.064786717 0.11385489 0.16470625 0.21162446 0.24547789 0.25450704 0.23253983 0.191872 0.14962614 0.11571378 0.093964539 0.08078856 0.077530906][-0.0031233064 0.018219227 0.052965254 0.095430441 0.14116368 0.18442097 0.2158595 0.22484176 0.20791394 0.17648466 0.14287075 0.11444993 0.096449479 0.087190293 0.086973421][0.0022123146 0.016467741 0.041122522 0.072624192 0.10840645 0.14230424 0.16618326 0.17313109 0.16337492 0.14553206 0.12477568 0.10526656 0.0932026 0.089233473 0.092195593][0.02176482 0.029475376 0.042854406 0.061773706 0.085349731 0.10631372 0.11859968 0.12104184 0.11773121 0.11359491 0.10617796 0.096469082 0.090575553 0.091246217 0.095942266][0.056868318 0.060685124 0.0645184 0.071891621 0.083466932 0.090794772 0.089993045 0.085986391 0.08594086 0.091856718 0.0952833 0.094633587 0.094534546 0.09865433 0.10340635][0.10227478 0.10558054 0.10353328 0.10267816 0.10457499 0.099866807 0.086839944 0.075495012 0.074786775 0.085348532 0.095517367 0.10197405 0.10714569 0.11424775 0.11884218][0.15239514 0.158379 0.15445706 0.14937888 0.14475933 0.13099268 0.10810199 0.089374 0.084486604 0.094570272 0.10758644 0.11876992 0.12810513 0.13823888 0.14461866][0.20000581 0.2098366 0.20658717 0.20126136 0.19456227 0.17636986 0.14780265 0.12375057 0.11342425 0.11955994 0.13190107 0.14478283 0.15581745 0.16811717 0.17805567]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 03:15:43.192356: step 86010, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 18h:35m:08s remains)
INFO - root - 2017-12-11 03:15:45.963029: step 86020, loss = 0.71, batch loss = 0.65 (28.5 examples/sec; 0.281 sec/batch; 19h:12m:33s remains)
INFO - root - 2017-12-11 03:15:48.756881: step 86030, loss = 0.69, batch loss = 0.63 (27.3 examples/sec; 0.294 sec/batch; 20h:05m:57s remains)
INFO - root - 2017-12-11 03:15:51.526977: step 86040, loss = 0.70, batch loss = 0.65 (28.7 examples/sec; 0.278 sec/batch; 19h:03m:03s remains)
INFO - root - 2017-12-11 03:15:54.261978: step 86050, loss = 0.71, batch loss = 0.66 (28.0 examples/sec; 0.286 sec/batch; 19h:33m:36s remains)
INFO - root - 2017-12-11 03:15:57.024298: step 86060, loss = 0.71, batch loss = 0.65 (27.9 examples/sec; 0.287 sec/batch; 19h:38m:15s remains)
INFO - root - 2017-12-11 03:15:59.762193: step 86070, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.265 sec/batch; 18h:10m:27s remains)
INFO - root - 2017-12-11 03:16:02.505575: step 86080, loss = 0.70, batch loss = 0.64 (28.3 examples/sec; 0.283 sec/batch; 19h:22m:17s remains)
INFO - root - 2017-12-11 03:16:05.223611: step 86090, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 18h:33m:19s remains)
INFO - root - 2017-12-11 03:16:07.997729: step 86100, loss = 0.68, batch loss = 0.62 (29.6 examples/sec; 0.270 sec/batch; 18h:28m:13s remains)
2017-12-11 03:16:08.497092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043613154 -0.037485108 -0.031207906 -0.02767127 -0.0215994 -0.011924157 -0.00098458864 0.0081228307 0.012682813 0.0081421882 -0.011033505 -0.038401596 -0.063869029 -0.083214268 -0.096276306][0.00045794679 0.020135103 0.036762949 0.046494205 0.061237697 0.080616795 0.098152131 0.11203678 0.11919771 0.11370519 0.081920095 0.033393934 -0.013853889 -0.052402917 -0.0807838][0.0676023 0.11000924 0.1452432 0.16660495 0.19334772 0.22259785 0.24294703 0.25693518 0.26257545 0.25436106 0.20818266 0.13557494 0.061823681 -0.0014651795 -0.049927097][0.14385766 0.216057 0.27670613 0.31600246 0.35846585 0.39701059 0.41599056 0.42647555 0.42824912 0.41698566 0.35836479 0.26353157 0.16191974 0.068936042 -0.0053150258][0.21829616 0.32866073 0.42724562 0.49999106 0.57051909 0.62434673 0.64221406 0.64516366 0.63561636 0.61080933 0.533252 0.41411293 0.28201672 0.15391234 0.047203127][0.27160254 0.42041513 0.56321913 0.68129003 0.79184794 0.87093085 0.89706028 0.896564 0.87174809 0.82117641 0.71444392 0.5664922 0.40128219 0.23537052 0.094490282][0.28621158 0.46465823 0.64833444 0.81309766 0.96710575 1.0807525 1.1303424 1.1372232 1.1001529 1.0192996 0.87855327 0.69856209 0.498768 0.29691964 0.1260041][0.25961939 0.44725528 0.64987266 0.84030205 1.0182424 1.1551166 1.2274163 1.2463619 1.2072803 1.1107349 0.95292836 0.75652748 0.53723186 0.3156966 0.13015479][0.19996926 0.37130895 0.5627898 0.74842435 0.92279631 1.0618633 1.1449652 1.1743704 1.1435262 1.0520252 0.90194756 0.71313995 0.49810973 0.28099933 0.10252508][0.12136861 0.25990084 0.42106205 0.58183753 0.73459023 0.85946757 0.93899465 0.96924484 0.94510746 0.86629105 0.73613811 0.57013249 0.37942904 0.19055261 0.041230913][0.032412324 0.12743282 0.24369021 0.36283147 0.47809687 0.574511 0.638036 0.66173267 0.64289939 0.58168727 0.47995302 0.34939027 0.20133021 0.061317615 -0.041342851][-0.047070507 0.0023120805 0.06770125 0.1366826 0.2059983 0.26709548 0.30956239 0.3259261 0.31461436 0.27536932 0.20783564 0.12034684 0.023805838 -0.060833223 -0.11462021][-0.094950907 -0.0813838 -0.058639579 -0.034036133 -0.00680969 0.019795552 0.039677788 0.047584478 0.042254977 0.022957157 -0.012167005 -0.057944007 -0.1061117 -0.14290163 -0.15841833][-0.11278794 -0.11899123 -0.12067722 -0.12254383 -0.12182128 -0.11913081 -0.11713845 -0.11792773 -0.12196578 -0.12973353 -0.14313984 -0.15935969 -0.17360932 -0.17899489 -0.17185162][-0.11496165 -0.12860446 -0.13900171 -0.15009746 -0.15951933 -0.16723274 -0.17420839 -0.18050523 -0.18544136 -0.1887818 -0.19156821 -0.19241925 -0.18906724 -0.17927963 -0.16297874]]...]
INFO - root - 2017-12-11 03:16:11.290848: step 86110, loss = 0.70, batch loss = 0.64 (27.5 examples/sec; 0.290 sec/batch; 19h:52m:52s remains)
INFO - root - 2017-12-11 03:16:13.978352: step 86120, loss = 0.71, batch loss = 0.65 (28.6 examples/sec; 0.280 sec/batch; 19h:10m:32s remains)
INFO - root - 2017-12-11 03:16:16.697126: step 86130, loss = 0.69, batch loss = 0.63 (28.7 examples/sec; 0.279 sec/batch; 19h:04m:27s remains)
INFO - root - 2017-12-11 03:16:19.449415: step 86140, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:25m:57s remains)
INFO - root - 2017-12-11 03:16:22.233642: step 86150, loss = 0.69, batch loss = 0.64 (28.1 examples/sec; 0.285 sec/batch; 19h:28m:42s remains)
INFO - root - 2017-12-11 03:16:25.016928: step 86160, loss = 0.71, batch loss = 0.65 (28.0 examples/sec; 0.285 sec/batch; 19h:31m:32s remains)
INFO - root - 2017-12-11 03:16:27.819271: step 86170, loss = 0.68, batch loss = 0.62 (27.2 examples/sec; 0.294 sec/batch; 20h:05m:19s remains)
INFO - root - 2017-12-11 03:16:30.562536: step 86180, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 18h:04m:27s remains)
INFO - root - 2017-12-11 03:16:33.338377: step 86190, loss = 0.71, batch loss = 0.65 (28.9 examples/sec; 0.277 sec/batch; 18h:57m:43s remains)
INFO - root - 2017-12-11 03:16:36.099666: step 86200, loss = 0.69, batch loss = 0.64 (30.0 examples/sec; 0.266 sec/batch; 18h:13m:51s remains)
2017-12-11 03:16:36.667600: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21136518 0.12786852 0.051898468 -0.0019788237 -0.03687099 -0.059070978 -0.0715238 -0.079399757 -0.085444368 -0.087911472 -0.084065177 -0.073059574 -0.057252042 -0.040078454 -0.026968263][0.12968735 0.055817965 -0.0066892533 -0.048309091 -0.074169822 -0.09048073 -0.10004608 -0.1063367 -0.11041363 -0.11005307 -0.10311187 -0.089742258 -0.071585424 -0.051324617 -0.033504][0.041291103 -0.013011646 -0.05366258 -0.076572835 -0.0882048 -0.094014086 -0.096600391 -0.098260887 -0.099160694 -0.097537115 -0.092150927 -0.082746848 -0.069057748 -0.050996333 -0.030734658][-0.030544262 -0.063483074 -0.081658684 -0.086010836 -0.083355159 -0.0777565 -0.071446955 -0.066426992 -0.062926844 -0.0602795 -0.058060125 -0.055383481 -0.049134634 -0.035345223 -0.012821279][-0.073578827 -0.089306563 -0.090533391 -0.081707612 -0.068778262 -0.054207042 -0.039857946 -0.027183669 -0.016876562 -0.00938649 -0.0052223513 -0.0036685201 -0.00017096711 0.012145496 0.038002312][-0.085282214 -0.089609079 -0.080666877 -0.064518794 -0.045773167 -0.025518179 -0.0049591372 0.015793495 0.035635274 0.053406984 0.067602828 0.077302754 0.085907459 0.10116095 0.13038686][-0.065251119 -0.062109489 -0.047594421 -0.027738377 -0.0061754803 0.016426973 0.039629187 0.065614842 0.093844712 0.12423232 0.15453108 0.18087083 0.20187499 0.22309345 0.25291386][-0.024262803 -0.015417886 0.0030799562 0.025720662 0.048787408 0.070681922 0.091693163 0.11674645 0.14791703 0.18847632 0.23603238 0.28271362 0.31988788 0.34825477 0.37529817][0.028051469 0.042015694 0.064487368 0.090005726 0.11364647 0.13175468 0.1450406 0.16131626 0.18731476 0.23079434 0.28988332 0.35279629 0.40391672 0.43797162 0.46006072][0.088277943 0.10697994 0.13361804 0.16256949 0.18716429 0.20033644 0.2026324 0.20374951 0.2156245 0.25012574 0.30636725 0.37157723 0.42631686 0.46041641 0.47572866][0.15124343 0.17341016 0.20313631 0.23519917 0.26168472 0.27185476 0.26498732 0.25035894 0.24359056 0.25903577 0.29716846 0.34772775 0.39208338 0.41772354 0.42317227][0.20562987 0.23025811 0.26068002 0.29347989 0.32149971 0.33191252 0.32214463 0.29838422 0.27686507 0.2718274 0.28453854 0.30922398 0.33142781 0.3396219 0.33037975][0.23557132 0.26213384 0.29094458 0.3216694 0.3503122 0.36504635 0.36195186 0.34262753 0.31929544 0.30159098 0.28993434 0.28347835 0.27435783 0.25636184 0.22752874][0.23661628 0.26527584 0.29125369 0.31753111 0.34494728 0.36581856 0.37649012 0.37398297 0.36291519 0.34460798 0.3171266 0.28357461 0.24270941 0.19617447 0.14659196][0.20804304 0.23767553 0.26011756 0.28044569 0.30433014 0.33020848 0.35614461 0.37560683 0.38478047 0.37625915 0.34535885 0.29754075 0.23605417 0.16919309 0.10430034]]...]
INFO - root - 2017-12-11 03:16:39.508288: step 86210, loss = 0.71, batch loss = 0.65 (28.4 examples/sec; 0.282 sec/batch; 19h:17m:45s remains)
INFO - root - 2017-12-11 03:16:42.248397: step 86220, loss = 0.69, batch loss = 0.63 (27.8 examples/sec; 0.288 sec/batch; 19h:41m:15s remains)
INFO - root - 2017-12-11 03:16:45.048328: step 86230, loss = 0.71, batch loss = 0.65 (27.6 examples/sec; 0.290 sec/batch; 19h:49m:00s remains)
INFO - root - 2017-12-11 03:16:47.797073: step 86240, loss = 0.70, batch loss = 0.64 (28.4 examples/sec; 0.282 sec/batch; 19h:18m:03s remains)
INFO - root - 2017-12-11 03:16:50.589014: step 86250, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 18h:32m:03s remains)
INFO - root - 2017-12-11 03:16:53.334923: step 86260, loss = 0.71, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 18h:34m:11s remains)
INFO - root - 2017-12-11 03:16:56.102457: step 86270, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.280 sec/batch; 19h:08m:37s remains)
INFO - root - 2017-12-11 03:16:58.909186: step 86280, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.279 sec/batch; 19h:05m:59s remains)
INFO - root - 2017-12-11 03:17:01.629858: step 86290, loss = 0.69, batch loss = 0.63 (29.3 examples/sec; 0.273 sec/batch; 18h:38m:41s remains)
INFO - root - 2017-12-11 03:17:04.422283: step 86300, loss = 0.70, batch loss = 0.64 (28.6 examples/sec; 0.280 sec/batch; 19h:08m:15s remains)
2017-12-11 03:17:04.951518: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.024001841 0.12994617 0.24857022 0.35090023 0.41959915 0.44006032 0.41432968 0.36046439 0.31513327 0.30183929 0.30974182 0.3217437 0.32830817 0.32631293 0.30283496][0.01451479 0.1193793 0.23708235 0.33883512 0.40716493 0.42820475 0.40324223 0.34930259 0.30128145 0.28442827 0.28865084 0.29600948 0.29700851 0.28922188 0.2617723][0.0029730417 0.099858068 0.20911416 0.30513534 0.37365124 0.40359616 0.39373654 0.35331237 0.30987275 0.28869373 0.28293145 0.27534536 0.2598305 0.23872839 0.206944][-0.007487969 0.078715518 0.17790197 0.26921058 0.34443495 0.39724481 0.42245051 0.41478208 0.38900995 0.3682518 0.34950849 0.31740344 0.271056 0.22256239 0.1762787][-0.013251717 0.062927164 0.1540069 0.24425001 0.33283016 0.4193089 0.49426931 0.53470737 0.53817582 0.52288687 0.48986855 0.42639607 0.33779258 0.24971154 0.17831977][-0.014251923 0.0525692 0.13663526 0.22705236 0.32933024 0.4487156 0.5735473 0.66447675 0.70095563 0.69459325 0.65017605 0.557503 0.42681175 0.297865 0.1985276][-0.012391663 0.045924243 0.12279542 0.21098545 0.32008284 0.45940572 0.61652863 0.74186128 0.80259603 0.8054136 0.75621623 0.64518243 0.48525763 0.32727981 0.20750359][-0.011195832 0.038960055 0.10650049 0.18688835 0.29162657 0.43152925 0.59448117 0.72898507 0.7994442 0.81048256 0.76590192 0.65286392 0.48456427 0.3173697 0.19141544][-0.014868623 0.026978867 0.0827887 0.14950287 0.23842211 0.35875344 0.49998349 0.61786467 0.68239892 0.69746917 0.66265595 0.56266904 0.40925619 0.25620675 0.14203331][-0.024043275 0.011010048 0.055609927 0.10701643 0.17404483 0.26237226 0.36467957 0.44914669 0.49557868 0.5078274 0.48192784 0.40243903 0.27942276 0.15853214 0.071935587][-0.03344363 -0.0029177018 0.032660697 0.069683991 0.11375785 0.16679917 0.22589129 0.2733587 0.29898551 0.30608958 0.28765383 0.23069033 0.1443722 0.063459963 0.011644425][-0.034862276 -0.0044568428 0.026456738 0.052469883 0.076408267 0.097396709 0.11803182 0.13389927 0.14305475 0.14715883 0.13668092 0.10156003 0.050137974 0.0062512513 -0.013761709][-0.03007726 0.0032712384 0.033720519 0.053759821 0.06431073 0.06292963 0.057182141 0.053035278 0.05419739 0.059908532 0.058793969 0.042616345 0.018273458 0.00037043574 0.00087574008][-0.02766945 0.0080661392 0.039612222 0.058127854 0.063392021 0.051874589 0.033741117 0.021090379 0.020881105 0.030539157 0.038195606 0.036535021 0.02960901 0.025271513 0.033988588][-0.032739833 0.0020604497 0.033598538 0.052460816 0.057742778 0.045070849 0.024637353 0.01040002 0.010660646 0.02242949 0.034940798 0.041446768 0.042818889 0.04262786 0.051217243]]...]
INFO - root - 2017-12-11 03:17:07.712417: step 86310, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 18h:00m:47s remains)
INFO - root - 2017-12-11 03:17:10.462314: step 86320, loss = 0.68, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 18h:48m:18s remains)
INFO - root - 2017-12-11 03:17:13.195484: step 86330, loss = 0.70, batch loss = 0.64 (28.2 examples/sec; 0.283 sec/batch; 19h:22m:21s remains)
INFO - root - 2017-12-11 03:17:15.955720: step 86340, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 18h:17m:09s remains)
INFO - root - 2017-12-11 03:17:18.696443: step 86350, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:38m:35s remains)
INFO - root - 2017-12-11 03:17:21.453994: step 86360, loss = 0.70, batch loss = 0.64 (28.8 examples/sec; 0.277 sec/batch; 18h:57m:37s remains)
INFO - root - 2017-12-11 03:17:24.214585: step 86370, loss = 0.70, batch loss = 0.64 (28.4 examples/sec; 0.282 sec/batch; 19h:15m:47s remains)
INFO - root - 2017-12-11 03:17:27.063617: step 86380, loss = 0.70, batch loss = 0.64 (28.1 examples/sec; 0.285 sec/batch; 19h:27m:05s remains)
INFO - root - 2017-12-11 03:17:29.786429: step 86390, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:23m:02s remains)
INFO - root - 2017-12-11 03:17:32.554355: step 86400, loss = 0.69, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 18h:55m:40s remains)
2017-12-11 03:17:33.057914: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28348973 0.43076637 0.51584291 0.52085 0.45768496 0.35528702 0.25282547 0.19475645 0.19068071 0.21225715 0.23779239 0.24869768 0.23403315 0.19303291 0.13861698][0.23701639 0.36703274 0.43609238 0.42757744 0.35863826 0.26383382 0.17825729 0.13605048 0.13977391 0.16044496 0.17483445 0.1668677 0.13423096 0.085825138 0.036355052][0.16802792 0.27353108 0.32720625 0.3150135 0.25567433 0.18565464 0.132395 0.1160555 0.13160603 0.15207636 0.15590206 0.13047604 0.081600361 0.026043547 -0.020191323][0.12426168 0.21591002 0.265328 0.25937581 0.21610063 0.1722309 0.14922926 0.15582173 0.18191856 0.20214202 0.19647841 0.15535951 0.090841144 0.024669522 -0.025749989][0.13096973 0.22658305 0.28577283 0.29211447 0.26215279 0.2348358 0.23164366 0.25562516 0.29354498 0.31894654 0.30967742 0.25629458 0.17476664 0.089847215 0.022070851][0.18853667 0.30478859 0.38375881 0.40193284 0.37423295 0.34606189 0.34665403 0.37960926 0.4286902 0.46278414 0.4543677 0.39153743 0.29250306 0.18362635 0.090866677][0.2673541 0.40922797 0.50810826 0.53313792 0.4975937 0.45521721 0.44506252 0.47523776 0.52707642 0.56468159 0.55579704 0.4865585 0.37737268 0.25416771 0.14570609][0.3228862 0.48075953 0.59068286 0.61685038 0.57012779 0.50974905 0.48144734 0.49930009 0.54413468 0.5784232 0.56913877 0.50177288 0.39720997 0.27837086 0.17217749][0.31745821 0.46932238 0.57401735 0.59548825 0.54132813 0.46979162 0.42647544 0.42895511 0.46082637 0.48849148 0.48190305 0.42656013 0.34101462 0.24368644 0.15592769][0.24093944 0.36366218 0.44756275 0.46104255 0.40782493 0.33792982 0.29033774 0.28241768 0.3022804 0.32435867 0.32415023 0.28739768 0.22766727 0.15891154 0.096660778][0.1210982 0.20130393 0.25553623 0.25974312 0.21430202 0.15618704 0.11356448 0.10045855 0.11014801 0.12635736 0.13158262 0.11290652 0.078369424 0.037728515 0.0014025898][0.0049469951 0.044117786 0.07010106 0.066884831 0.032926675 -0.0087435637 -0.041511245 -0.055877872 -0.05404669 -0.044111766 -0.036657114 -0.041698951 -0.055453986 -0.072137326 -0.086084761][-0.073006675 -0.062393382 -0.055149082 -0.06134095 -0.083139747 -0.10866399 -0.13008647 -0.14211379 -0.14420953 -0.13898762 -0.1316732 -0.12898375 -0.12981981 -0.13137104 -0.13148206][-0.11030257 -0.11512871 -0.11768444 -0.12408202 -0.13614284 -0.14912269 -0.1604498 -0.16796564 -0.17048118 -0.16801132 -0.16253702 -0.15784745 -0.15388526 -0.14952269 -0.1444253][-0.11734321 -0.12766518 -0.13356878 -0.13904633 -0.14519343 -0.1508307 -0.15558961 -0.15892558 -0.16015473 -0.15878816 -0.15535264 -0.15153244 -0.14746597 -0.14274774 -0.1375559]]...]
INFO - root - 2017-12-11 03:17:35.835741: step 86410, loss = 0.68, batch loss = 0.62 (29.0 examples/sec; 0.276 sec/batch; 18h:52m:07s remains)
INFO - root - 2017-12-11 03:17:38.632893: step 86420, loss = 0.68, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:43m:28s remains)
INFO - root - 2017-12-11 03:17:41.390035: step 86430, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 18h:02m:06s remains)
INFO - root - 2017-12-11 03:17:44.166611: step 86440, loss = 0.69, batch loss = 0.63 (27.2 examples/sec; 0.294 sec/batch; 20h:06m:16s remains)
INFO - root - 2017-12-11 03:17:46.934787: step 86450, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 18h:33m:03s remains)
INFO - root - 2017-12-11 03:17:49.726083: step 86460, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.264 sec/batch; 18h:04m:32s remains)
INFO - root - 2017-12-11 03:17:52.461512: step 86470, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 18h:09m:39s remains)
INFO - root - 2017-12-11 03:17:55.208724: step 86480, loss = 0.70, batch loss = 0.64 (28.0 examples/sec; 0.285 sec/batch; 19h:30m:11s remains)
INFO - root - 2017-12-11 03:17:57.938695: step 86490, loss = 0.72, batch loss = 0.66 (29.9 examples/sec; 0.268 sec/batch; 18h:17m:17s remains)
INFO - root - 2017-12-11 03:18:00.677793: step 86500, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.280 sec/batch; 19h:06m:37s remains)
2017-12-11 03:18:01.153679: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.58596158 0.62112218 0.60045683 0.52538079 0.408525 0.28823844 0.19843236 0.14514259 0.1351855 0.16943328 0.23874922 0.33134016 0.42172262 0.47652382 0.46825838][0.60556149 0.65303749 0.64349473 0.57400012 0.45511538 0.32579243 0.22296211 0.15362646 0.12121265 0.12667941 0.16673534 0.23540401 0.30979386 0.36004794 0.35966098][0.55014354 0.60361916 0.61117423 0.56398815 0.46612921 0.34960917 0.24856941 0.17045523 0.11751522 0.092519507 0.10032208 0.14011967 0.19370481 0.23515424 0.24030513][0.44513294 0.49829826 0.52363855 0.50981939 0.45266786 0.37088618 0.28879705 0.21178591 0.14316139 0.089844227 0.0647313 0.073935859 0.10440838 0.13531947 0.14348668][0.31822485 0.36612087 0.407078 0.43163955 0.4290494 0.40088734 0.35528675 0.29224327 0.21602543 0.13882186 0.081890866 0.060041621 0.06622079 0.084189445 0.0913469][0.20051807 0.24268699 0.29666483 0.35873917 0.415156 0.451304 0.45645574 0.42103025 0.34649634 0.24992177 0.16142201 0.10689694 0.086063072 0.08726804 0.088018142][0.11961102 0.15247832 0.21050182 0.29724124 0.40085128 0.4962889 0.55624759 0.55783725 0.49480832 0.38670808 0.27210736 0.1880676 0.14193727 0.12634221 0.11827547][0.10996433 0.12980339 0.17953688 0.26945698 0.3936927 0.5253765 0.626507 0.66140574 0.61404181 0.50308853 0.37267995 0.2675657 0.20274907 0.17440198 0.15911973][0.1612483 0.16824931 0.20138994 0.27628785 0.39493307 0.53495562 0.65488446 0.71067691 0.67872065 0.57320487 0.43841371 0.32168397 0.2444573 0.20645961 0.18540514][0.23142476 0.226518 0.23941036 0.28998187 0.38672984 0.51333457 0.62977117 0.68990481 0.66832352 0.57411736 0.44644651 0.32957086 0.24836951 0.20573796 0.18219611][0.27577266 0.26001313 0.2525852 0.27609766 0.34330922 0.44366488 0.54139388 0.59316957 0.57670784 0.49784571 0.38723817 0.28184587 0.20637456 0.16618001 0.14553769][0.27889121 0.255278 0.22949995 0.22524828 0.25867391 0.32527605 0.3957006 0.43251479 0.41837361 0.35714033 0.27009794 0.18531288 0.12444786 0.093759753 0.0811695][0.24474671 0.21813112 0.17964579 0.15199475 0.15416761 0.1878023 0.230422 0.251427 0.23793876 0.19301172 0.13039124 0.069901079 0.028230207 0.0105864 0.0077639925][0.17522323 0.14910707 0.10631926 0.066260263 0.048787452 0.059593379 0.082198747 0.0922943 0.080322415 0.049044024 0.0074365581 -0.031015163 -0.054938119 -0.060976923 -0.056119133][0.092633322 0.069529496 0.029849658 -0.01150507 -0.036503691 -0.038016282 -0.027335336 -0.022689594 -0.031195527 -0.050754722 -0.075630486 -0.09700793 -0.10777234 -0.10620628 -0.097385637]]...]
INFO - root - 2017-12-11 03:18:03.966780: step 86510, loss = 0.71, batch loss = 0.65 (28.5 examples/sec; 0.281 sec/batch; 19h:11m:50s remains)
INFO - root - 2017-12-11 03:18:06.770961: step 86520, loss = 0.68, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 18h:34m:28s remains)
INFO - root - 2017-12-11 03:18:09.502874: step 86530, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:27m:29s remains)
INFO - root - 2017-12-11 03:18:12.275062: step 86540, loss = 0.70, batch loss = 0.64 (28.8 examples/sec; 0.277 sec/batch; 18h:57m:32s remains)
INFO - root - 2017-12-11 03:18:15.015093: step 86550, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 18h:51m:10s remains)
INFO - root - 2017-12-11 03:18:17.790820: step 86560, loss = 0.69, batch loss = 0.63 (28.8 examples/sec; 0.278 sec/batch; 18h:59m:47s remains)
INFO - root - 2017-12-11 03:18:20.560517: step 86570, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:41m:29s remains)
INFO - root - 2017-12-11 03:18:23.296552: step 86580, loss = 0.71, batch loss = 0.65 (29.8 examples/sec; 0.268 sec/batch; 18h:20m:07s remains)
INFO - root - 2017-12-11 03:18:26.010778: step 86590, loss = 0.69, batch loss = 0.63 (28.8 examples/sec; 0.278 sec/batch; 18h:59m:10s remains)
INFO - root - 2017-12-11 03:18:28.790552: step 86600, loss = 0.68, batch loss = 0.62 (28.6 examples/sec; 0.280 sec/batch; 19h:07m:12s remains)
2017-12-11 03:18:29.297540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0060438081 0.032134768 0.0794388 0.13408044 0.19324452 0.24415974 0.26634607 0.26733461 0.28338143 0.34216833 0.45547873 0.59754747 0.71956658 0.764642 0.71893865][0.014322861 0.066126838 0.12590152 0.18159321 0.22970979 0.26499033 0.27426371 0.2647154 0.27040735 0.320655 0.42021289 0.53898 0.63289219 0.66513371 0.63037521][0.056815661 0.13143161 0.21046235 0.27058876 0.30860347 0.3264803 0.31883806 0.29300073 0.27818653 0.30516109 0.37864456 0.46978143 0.5365988 0.55759436 0.53671479][0.10177404 0.2030468 0.30726859 0.38044554 0.4201721 0.43297362 0.41870597 0.38366476 0.34818393 0.34596804 0.38841805 0.45304874 0.49809897 0.5086295 0.49710035][0.13320459 0.25717926 0.38822329 0.48421419 0.54222167 0.5710454 0.57308716 0.54960328 0.50499696 0.47747433 0.48992622 0.52877116 0.55058879 0.54371011 0.52971309][0.15562709 0.29679704 0.45204857 0.57547444 0.66138685 0.72107929 0.75998068 0.7701661 0.73470378 0.69374126 0.68259561 0.69520342 0.68641466 0.64843792 0.61210406][0.17693533 0.3278293 0.49798873 0.64200372 0.7501657 0.83749139 0.91388535 0.96228993 0.94793791 0.90908611 0.88599372 0.87434739 0.828528 0.748513 0.67256057][0.19340748 0.34654146 0.51839012 0.66923541 0.7858448 0.88513446 0.98152494 1.0538937 1.0612603 1.0351827 1.0101005 0.97834182 0.89686888 0.77577251 0.65721023][0.19319777 0.33831882 0.49652654 0.63674891 0.74466735 0.83645988 0.92876947 1.0029026 1.0241617 1.0140196 0.99423712 0.95221269 0.85145158 0.70968032 0.56786829][0.16048987 0.28496659 0.415007 0.52872258 0.61355424 0.68289816 0.75249696 0.81045693 0.83581305 0.83913541 0.82830888 0.78773588 0.69014055 0.55571204 0.41989025][0.091616541 0.18403138 0.27629262 0.35434932 0.4090001 0.44969311 0.4888286 0.522511 0.54398686 0.55478889 0.55296379 0.52339715 0.44802552 0.34405383 0.23829289][0.0022334673 0.054955482 0.10599009 0.14706516 0.17267217 0.18822432 0.20160621 0.21458015 0.22929084 0.24138205 0.2446287 0.22821911 0.18258066 0.11899798 0.054173771][-0.0776222 -0.060923755 -0.044805013 -0.033559766 -0.029588997 -0.030479556 -0.032196283 -0.030713405 -0.021236081 -0.010803988 -0.0059007183 -0.011975361 -0.03207944 -0.060150012 -0.087457784][-0.12421782 -0.13097614 -0.13642879 -0.1428851 -0.1507072 -0.15905246 -0.16617569 -0.1681888 -0.16153218 -0.15299644 -0.14762548 -0.14717716 -0.15115714 -0.15651833 -0.15926117][-0.13791995 -0.15464698 -0.16877697 -0.1820282 -0.19366628 -0.20332746 -0.21043624 -0.21234724 -0.20745805 -0.20060347 -0.1951672 -0.19161084 -0.18871549 -0.18486685 -0.17818505]]...]
INFO - root - 2017-12-11 03:18:32.051379: step 86610, loss = 0.71, batch loss = 0.65 (28.8 examples/sec; 0.278 sec/batch; 18h:59m:07s remains)
INFO - root - 2017-12-11 03:18:34.826417: step 86620, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.280 sec/batch; 19h:08m:13s remains)
INFO - root - 2017-12-11 03:18:37.608669: step 86630, loss = 0.71, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 18h:30m:45s remains)
INFO - root - 2017-12-11 03:18:40.370483: step 86640, loss = 0.72, batch loss = 0.66 (28.9 examples/sec; 0.277 sec/batch; 18h:55m:20s remains)
INFO - root - 2017-12-11 03:18:43.145465: step 86650, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 18h:48m:16s remains)
INFO - root - 2017-12-11 03:18:45.897067: step 86660, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:37m:45s remains)
INFO - root - 2017-12-11 03:18:48.697257: step 86670, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.276 sec/batch; 18h:52m:18s remains)
INFO - root - 2017-12-11 03:18:51.433911: step 86680, loss = 0.68, batch loss = 0.62 (27.6 examples/sec; 0.290 sec/batch; 19h:49m:01s remains)
INFO - root - 2017-12-11 03:18:54.239816: step 86690, loss = 0.70, batch loss = 0.64 (27.9 examples/sec; 0.286 sec/batch; 19h:32m:44s remains)
INFO - root - 2017-12-11 03:18:57.015834: step 86700, loss = 0.70, batch loss = 0.64 (28.6 examples/sec; 0.280 sec/batch; 19h:07m:01s remains)
2017-12-11 03:18:57.495343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.044238556 -0.017553346 0.039677888 0.13481943 0.26380324 0.42094979 0.58672112 0.72485673 0.79448074 0.77010286 0.65269887 0.46718577 0.26063848 0.086604282 -0.022072313][-0.053077027 -0.034846887 0.0076988605 0.079801247 0.18001397 0.3061862 0.444407 0.56345141 0.62388927 0.60111719 0.49704182 0.33713681 0.16481312 0.025409479 -0.05583423][-0.064401284 -0.058200885 -0.037393309 0.0026971705 0.063986279 0.14688569 0.2424548 0.32716331 0.36838472 0.34643853 0.26548922 0.14904071 0.0314636 -0.055216417 -0.096195243][-0.073679678 -0.075888991 -0.070399113 -0.052467659 -0.018851299 0.030471643 0.088409983 0.13808861 0.15555905 0.12867792 0.0660076 -0.011006795 -0.07778351 -0.1165946 -0.1223885][-0.075251378 -0.076278813 -0.070278168 -0.052632213 -0.021744786 0.01966833 0.062833235 0.09213917 0.088429578 0.0467575 -0.016586324 -0.078355335 -0.12026971 -0.13511685 -0.12435158][-0.064080358 -0.052330393 -0.027308103 0.015343052 0.072500259 0.13620774 0.1920903 0.21847603 0.19483781 0.12333701 0.03265569 -0.045847163 -0.094990514 -0.11345316 -0.10635327][-0.038670871 -0.0049489215 0.054000366 0.14270926 0.25055358 0.36012733 0.44604006 0.47501481 0.42292616 0.30270553 0.16091135 0.042204827 -0.035103232 -0.073995925 -0.082491264][-0.0030603334 0.056244012 0.15521407 0.29775903 0.46327597 0.62065154 0.73024791 0.74883008 0.65437281 0.47633407 0.27974144 0.1202903 0.015108949 -0.044006184 -0.067635059][0.033211686 0.11521389 0.25003588 0.43797866 0.64662033 0.82956231 0.93631619 0.92350292 0.78036171 0.55428338 0.32203785 0.14206757 0.026234087 -0.039405689 -0.067751914][0.056833986 0.15189174 0.30709296 0.51585561 0.73524928 0.90787655 0.98223615 0.92626655 0.74588943 0.50051314 0.26718882 0.097547621 -0.0046670991 -0.058482319 -0.078267395][0.056797963 0.14910328 0.29888484 0.49210182 0.68173152 0.81032288 0.836351 0.74537981 0.55757838 0.33320937 0.13798055 0.0092665488 -0.057847172 -0.0853309 -0.087389573][0.031966049 0.10491608 0.22306816 0.36852744 0.49928507 0.56939006 0.55350411 0.45259285 0.29241657 0.12335183 -0.0075346376 -0.079506144 -0.10414491 -0.10279945 -0.086761616][-0.0084611019 0.033977952 0.1051043 0.18879153 0.25508681 0.2753692 0.23941872 0.15611839 0.047714237 -0.051349018 -0.11399242 -0.13361743 -0.12449647 -0.10353905 -0.076297276][-0.047690827 -0.036158282 -0.0098439753 0.020903692 0.039549578 0.032564089 -0.0013379422 -0.052721914 -0.10711953 -0.14585663 -0.15749778 -0.14395146 -0.11818349 -0.090750784 -0.061115745][-0.070241489 -0.079317018 -0.082027175 -0.082818247 -0.088069491 -0.10203609 -0.12289482 -0.1442305 -0.1591263 -0.16062552 -0.14666405 -0.12197281 -0.0958159 -0.07297051 -0.048444416]]...]
INFO - root - 2017-12-11 03:19:00.316123: step 86710, loss = 0.68, batch loss = 0.62 (29.6 examples/sec; 0.271 sec/batch; 18h:28m:36s remains)
INFO - root - 2017-12-11 03:19:03.075415: step 86720, loss = 0.68, batch loss = 0.62 (28.4 examples/sec; 0.282 sec/batch; 19h:15m:13s remains)
INFO - root - 2017-12-11 03:19:05.801263: step 86730, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 18h:45m:46s remains)
INFO - root - 2017-12-11 03:19:08.516425: step 86740, loss = 0.69, batch loss = 0.64 (28.3 examples/sec; 0.283 sec/batch; 19h:17m:59s remains)
INFO - root - 2017-12-11 03:19:11.325037: step 86750, loss = 0.70, batch loss = 0.65 (28.7 examples/sec; 0.279 sec/batch; 19h:02m:56s remains)
INFO - root - 2017-12-11 03:19:14.059270: step 86760, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:41m:10s remains)
INFO - root - 2017-12-11 03:19:16.801526: step 86770, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 18h:46m:42s remains)
INFO - root - 2017-12-11 03:19:19.577511: step 86780, loss = 0.70, batch loss = 0.64 (28.5 examples/sec; 0.281 sec/batch; 19h:10m:09s remains)
INFO - root - 2017-12-11 03:19:22.380796: step 86790, loss = 0.69, batch loss = 0.63 (31.1 examples/sec; 0.258 sec/batch; 17h:35m:02s remains)
INFO - root - 2017-12-11 03:19:25.200338: step 86800, loss = 0.68, batch loss = 0.63 (29.3 examples/sec; 0.273 sec/batch; 18h:37m:58s remains)
2017-12-11 03:19:25.674310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.065075316 -0.061306119 -0.056060661 -0.051616497 -0.048437525 -0.045549754 -0.043077555 -0.041449767 -0.041019894 -0.041906584 -0.043682504 -0.046383843 -0.0497445 -0.05379308 -0.05825438][-0.068345554 -0.065459512 -0.06031229 -0.055803623 -0.05289245 -0.050710857 -0.049341645 -0.048742663 -0.048884906 -0.049539134 -0.0503939 -0.05201257 -0.054463558 -0.058156416 -0.062566869][-0.069182284 -0.066269226 -0.05975312 -0.053705294 -0.050379664 -0.049609438 -0.051437523 -0.055075213 -0.058999088 -0.061427765 -0.061819948 -0.061307497 -0.060833454 -0.061484586 -0.063354127][-0.058427837 -0.050818879 -0.037844561 -0.02559563 -0.018253846 -0.017118467 -0.022641368 -0.033313476 -0.045344722 -0.054753534 -0.059800234 -0.061129898 -0.060134929 -0.058244374 -0.056672126][-0.030896684 -0.011845734 0.013970702 0.038447872 0.055277683 0.060681663 0.05302659 0.034153566 0.010322462 -0.012183172 -0.029318811 -0.039855488 -0.04405959 -0.042608783 -0.038317233][0.0098401 0.044951424 0.087556444 0.12852798 0.15963115 0.17398022 0.1675957 0.14182083 0.10480951 0.064737305 0.028857265 0.0019698869 -0.013327954 -0.016286813 -0.011329983][0.049894724 0.10016527 0.15825288 0.21507601 0.26132074 0.28745896 0.28694025 0.2593323 0.21313243 0.15684074 0.10087986 0.054341339 0.023810145 0.01229995 0.014470353][0.072504081 0.13132191 0.19754578 0.26266217 0.31755805 0.35206777 0.35893095 0.33591804 0.29005772 0.22765663 0.16015764 0.09969832 0.056308482 0.035334695 0.031902049][0.067600355 0.12548894 0.18968304 0.25207609 0.30508575 0.34028903 0.35238421 0.33831167 0.30249187 0.24743932 0.18284242 0.12119299 0.073703729 0.047017053 0.037984628][0.037563428 0.08593709 0.13975665 0.19100031 0.23387307 0.2627773 0.27576786 0.27014461 0.24811408 0.20922063 0.16027528 0.11152932 0.07216996 0.047864851 0.037077095][-0.0003890915 0.035708982 0.076944172 0.11497165 0.14486967 0.16345575 0.17224172 0.17055951 0.15999147 0.13904786 0.11153366 0.083746448 0.06054578 0.044630963 0.035248302][-0.027772581 -0.00030602742 0.031509489 0.058453552 0.07582444 0.08236929 0.082974754 0.080351539 0.076728858 0.070571482 0.062925056 0.056003362 0.049899835 0.043717995 0.036868874][-0.039962497 -0.01648733 0.010150964 0.029754121 0.037556849 0.033529066 0.025033619 0.018288244 0.016951097 0.020085359 0.026961489 0.036634 0.045229174 0.048271842 0.044727497][-0.041879535 -0.019896653 0.0045477627 0.020832773 0.023936098 0.014125287 -0.00041175319 -0.011339088 -0.013656559 -0.007125468 0.0076342616 0.028059129 0.047515415 0.058234118 0.058165111][-0.037860747 -0.017833292 0.0044730655 0.018877046 0.020562459 0.0098145884 -0.0058452385 -0.018119207 -0.021553103 -0.014736569 0.0031643955 0.029135674 0.055084467 0.070736013 0.073050119]]...]
INFO - root - 2017-12-11 03:19:28.449178: step 86810, loss = 0.70, batch loss = 0.64 (28.5 examples/sec; 0.281 sec/batch; 19h:11m:15s remains)
INFO - root - 2017-12-11 03:19:31.221181: step 86820, loss = 0.68, batch loss = 0.62 (29.6 examples/sec; 0.270 sec/batch; 18h:26m:10s remains)
INFO - root - 2017-12-11 03:19:34.014044: step 86830, loss = 0.70, batch loss = 0.64 (27.9 examples/sec; 0.287 sec/batch; 19h:34m:17s remains)
INFO - root - 2017-12-11 03:19:36.826865: step 86840, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 18h:52m:47s remains)
INFO - root - 2017-12-11 03:19:39.586292: step 86850, loss = 0.70, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 18h:39m:04s remains)
INFO - root - 2017-12-11 03:19:42.275868: step 86860, loss = 0.72, batch loss = 0.66 (29.4 examples/sec; 0.272 sec/batch; 18h:32m:31s remains)
INFO - root - 2017-12-11 03:19:44.977863: step 86870, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:20m:53s remains)
INFO - root - 2017-12-11 03:19:47.747940: step 86880, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 18h:45m:09s remains)
INFO - root - 2017-12-11 03:19:50.520504: step 86890, loss = 0.71, batch loss = 0.65 (27.7 examples/sec; 0.288 sec/batch; 19h:40m:35s remains)
INFO - root - 2017-12-11 03:19:53.368123: step 86900, loss = 0.70, batch loss = 0.64 (28.1 examples/sec; 0.285 sec/batch; 19h:24m:38s remains)
2017-12-11 03:19:53.862054: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.09131673 0.12579958 0.14806208 0.15610389 0.1532713 0.14144897 0.12504108 0.11293595 0.10833874 0.10761942 0.10354248 0.09447065 0.086251512 0.080219768 0.084563754][0.13783501 0.18767817 0.22162405 0.23647319 0.23499821 0.21934782 0.19667843 0.1782136 0.16822211 0.16408247 0.15753502 0.14626867 0.13669507 0.13112153 0.140545][0.1771936 0.24215591 0.28901333 0.31290984 0.3160207 0.30106989 0.27735919 0.25620657 0.24161845 0.23188432 0.22049941 0.20665894 0.1953882 0.18920967 0.20204486][0.19877373 0.27529448 0.33266756 0.36518329 0.3752147 0.36565343 0.34607282 0.32651287 0.31049961 0.29702011 0.28171298 0.2647875 0.25030053 0.24163322 0.25415036][0.19725683 0.27894926 0.34216323 0.38123795 0.39878657 0.3973113 0.38535836 0.37045932 0.35584846 0.3417477 0.32665846 0.31026819 0.29375094 0.28097937 0.28851679][0.18431756 0.26728696 0.33479896 0.38104168 0.40727732 0.41390157 0.40907249 0.3985033 0.38566896 0.37197712 0.35827437 0.34465471 0.326861 0.30769947 0.30480543][0.16511197 0.24887925 0.32263151 0.37892976 0.41637045 0.43217966 0.43427345 0.42773911 0.41699749 0.40376395 0.3894518 0.3757804 0.35389107 0.32502162 0.30751485][0.13950211 0.22392142 0.30500644 0.37251389 0.42244875 0.44911391 0.45944771 0.45962092 0.45550063 0.44622061 0.43090045 0.413162 0.38316604 0.34110188 0.30549291][0.11745749 0.20263116 0.29055366 0.36782217 0.42800727 0.46371442 0.48054543 0.48688716 0.49102283 0.48912263 0.4758741 0.45388991 0.41535586 0.36066455 0.30900034][0.10544706 0.19041066 0.28194252 0.36362392 0.42700586 0.46489406 0.48174557 0.48867226 0.49692032 0.50122541 0.49195114 0.46928042 0.427772 0.36783287 0.30894044][0.096594065 0.17712349 0.26531771 0.34333083 0.40225792 0.43567583 0.44692153 0.44964254 0.45791018 0.46596405 0.45993951 0.43802038 0.3981725 0.34127998 0.28634188][0.074505948 0.14375873 0.22060139 0.28742969 0.33625257 0.36216694 0.36705482 0.36512247 0.372059 0.38245302 0.37921241 0.35798219 0.32070857 0.27046475 0.22616243][0.038236607 0.088982351 0.14654067 0.19577692 0.23019254 0.24710451 0.24701592 0.24241768 0.24846144 0.26056403 0.26117238 0.24301314 0.21055515 0.16961768 0.13875288][0.0024164983 0.032212328 0.0672953 0.09631706 0.1142911 0.12109964 0.11644672 0.10977621 0.11389368 0.1256894 0.13039869 0.11919988 0.096974306 0.0702961 0.054878816][-0.024746122 -0.013181468 0.0015509754 0.012256347 0.015679434 0.013638421 0.0056258389 -0.0024075138 -0.0012601691 0.0076505318 0.014312951 0.010838415 0.0012304164 -0.0093975868 -0.010636752]]...]
INFO - root - 2017-12-11 03:19:56.617368: step 86910, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.274 sec/batch; 18h:43m:32s remains)
INFO - root - 2017-12-11 03:19:59.371607: step 86920, loss = 0.72, batch loss = 0.66 (29.5 examples/sec; 0.271 sec/batch; 18h:28m:06s remains)
INFO - root - 2017-12-11 03:20:02.141362: step 86930, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 18h:21m:46s remains)
INFO - root - 2017-12-11 03:20:04.869132: step 86940, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 18h:40m:23s remains)
INFO - root - 2017-12-11 03:20:07.703167: step 86950, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 18h:26m:41s remains)
INFO - root - 2017-12-11 03:20:10.453754: step 86960, loss = 0.70, batch loss = 0.64 (28.8 examples/sec; 0.278 sec/batch; 18h:55m:43s remains)
INFO - root - 2017-12-11 03:20:13.225024: step 86970, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 18h:29m:46s remains)
INFO - root - 2017-12-11 03:20:15.937301: step 86980, loss = 0.71, batch loss = 0.66 (30.2 examples/sec; 0.265 sec/batch; 18h:04m:55s remains)
INFO - root - 2017-12-11 03:20:18.718196: step 86990, loss = 0.69, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 18h:49m:50s remains)
INFO - root - 2017-12-11 03:20:21.493806: step 87000, loss = 0.71, batch loss = 0.66 (29.9 examples/sec; 0.268 sec/batch; 18h:14m:32s remains)
2017-12-11 03:20:22.011272: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.08403004 0.06452629 0.037558492 0.0092645874 -0.01558526 -0.025758628 -0.010758656 0.028930627 0.087552316 0.15034726 0.19898094 0.21802546 0.20844124 0.17971542 0.14614861][0.037241884 0.022255169 0.00569397 -0.0074847452 -0.012695195 -0.0006394196 0.032849491 0.080861457 0.13476449 0.18490642 0.21776627 0.22288358 0.20404464 0.17339732 0.14445172][-0.017972741 -0.030264204 -0.034206577 -0.026143003 -0.00027848055 0.049831178 0.11808223 0.1843491 0.23246095 0.25863552 0.25861439 0.23133485 0.18679734 0.14359836 0.11489391][-0.0637568 -0.076207124 -0.068216637 -0.037999164 0.023891518 0.12158961 0.23703197 0.33154523 0.37722203 0.3743 0.33052021 0.25596866 0.17028373 0.10161506 0.06485787][-0.092533551 -0.1046439 -0.085238993 -0.034143247 0.063975692 0.21164849 0.37825733 0.50468296 0.55084467 0.5192045 0.428593 0.29987627 0.16455214 0.061624572 0.0090247][-0.10253388 -0.10951645 -0.076352559 -0.0050347866 0.12537928 0.314477 0.51974231 0.66648042 0.70797652 0.64872438 0.5151794 0.33718514 0.157315 0.022820657 -0.046760593][-0.097998604 -0.091813922 -0.040055741 0.0527197 0.20978308 0.42437962 0.64386046 0.78796536 0.81303972 0.72678161 0.55946153 0.34714252 0.13949481 -0.013762131 -0.0941313][-0.086458713 -0.059449602 0.016006334 0.13206828 0.30872151 0.52961552 0.73565274 0.85250592 0.8487066 0.73636514 0.54677474 0.319623 0.10633109 -0.047401279 -0.12775795][-0.072188161 -0.022402307 0.076210521 0.21128532 0.39384586 0.5981825 0.76550317 0.83757037 0.79926372 0.66699821 0.47145241 0.25325716 0.059334651 -0.075393975 -0.14373316][-0.061553355 0.0029579012 0.11334834 0.25287983 0.42180732 0.58928245 0.70430452 0.72935694 0.66425759 0.52725369 0.34627712 0.15970051 0.0050984882 -0.096317887 -0.14431071][-0.06100554 0.00294759 0.10653551 0.23044649 0.36704278 0.4864786 0.55094767 0.54171586 0.46743008 0.34465823 0.19729738 0.058419172 -0.046233494 -0.10859583 -0.13375787][-0.070910223 -0.021924332 0.058001366 0.14993177 0.24379557 0.3162415 0.34357828 0.31952828 0.25488114 0.16204575 0.059819307 -0.026460132 -0.08243268 -0.10954863 -0.11548971][-0.0848165 -0.056988247 -0.0079620248 0.046253465 0.097968236 0.1326744 0.13814448 0.11418868 0.069977269 0.013315009 -0.042904407 -0.08222352 -0.099476151 -0.10100123 -0.094567686][-0.093332961 -0.084730946 -0.06402678 -0.042937256 -0.024300659 -0.014665439 -0.019123321 -0.035992827 -0.058510803 -0.082816422 -0.10216062 -0.10806732 -0.10089665 -0.088119708 -0.0762231][-0.089842074 -0.09360373 -0.0917681 -0.092289604 -0.093514748 -0.096819587 -0.10404549 -0.11352947 -0.12018123 -0.12306075 -0.1206209 -0.10970925 -0.092414521 -0.075064518 -0.062910646]]...]
INFO - root - 2017-12-11 03:20:24.753759: step 87010, loss = 0.68, batch loss = 0.62 (29.4 examples/sec; 0.272 sec/batch; 18h:33m:50s remains)
INFO - root - 2017-12-11 03:20:27.512801: step 87020, loss = 0.70, batch loss = 0.64 (28.5 examples/sec; 0.280 sec/batch; 19h:06m:27s remains)
INFO - root - 2017-12-11 03:20:30.273136: step 87030, loss = 0.69, batch loss = 0.63 (28.9 examples/sec; 0.277 sec/batch; 18h:54m:20s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 03:20:33.072572: step 87040, loss = 0.70, batch loss = 0.64 (28.6 examples/sec; 0.279 sec/batch; 19h:03m:07s remains)
INFO - root - 2017-12-11 03:20:35.786996: step 87050, loss = 0.70, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:26m:17s remains)
INFO - root - 2017-12-11 03:20:38.489882: step 87060, loss = 0.68, batch loss = 0.62 (29.5 examples/sec; 0.271 sec/batch; 18h:29m:17s remains)
INFO - root - 2017-12-11 03:20:41.280386: step 87070, loss = 0.69, batch loss = 0.63 (28.8 examples/sec; 0.277 sec/batch; 18h:54m:53s remains)
INFO - root - 2017-12-11 03:20:43.984761: step 87080, loss = 0.68, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 18h:21m:52s remains)
INFO - root - 2017-12-11 03:20:46.692603: step 87090, loss = 0.68, batch loss = 0.62 (28.4 examples/sec; 0.282 sec/batch; 19h:13m:06s remains)
INFO - root - 2017-12-11 03:20:49.417555: step 87100, loss = 0.68, batch loss = 0.63 (28.4 examples/sec; 0.281 sec/batch; 19h:10m:14s remains)
2017-12-11 03:20:49.976537: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.050429575 0.05840946 0.061581567 0.065657683 0.070328847 0.073281132 0.07355535 0.073186055 0.072702259 0.069175757 0.059858728 0.041553609 0.013632412 -0.017210454 -0.041737482][0.10787871 0.12780738 0.14089353 0.15631543 0.17289226 0.185604 0.18938491 0.18471125 0.17346898 0.15454367 0.12920915 0.095977165 0.05342846 0.00853048 -0.027705224][0.16149527 0.1940809 0.2181394 0.24609187 0.27644569 0.3018595 0.31250039 0.30569002 0.28289461 0.24528164 0.19954291 0.14838521 0.090565108 0.032407515 -0.014445756][0.19525863 0.23906907 0.27317122 0.31158435 0.3528581 0.3886399 0.40617794 0.40001383 0.37002578 0.31774348 0.25419712 0.18670617 0.11531521 0.046493188 -0.0083331456][0.2076204 0.26054254 0.30354387 0.35007486 0.39848298 0.44023305 0.46166345 0.45597652 0.42183638 0.36035594 0.2853139 0.20728406 0.12754123 0.052591752 -0.0070658117][0.21388799 0.27561358 0.32642922 0.37781546 0.42843112 0.47052017 0.49101233 0.48269546 0.44401497 0.37674555 0.2962147 0.21413612 0.13191684 0.055376634 -0.006502144][0.23023443 0.30205691 0.35906205 0.40995285 0.45565131 0.49197 0.50858593 0.49814352 0.4579215 0.39015469 0.309729 0.22797871 0.14576331 0.0680667 0.0027774354][0.2471839 0.32716939 0.38663992 0.43090072 0.4641346 0.48861873 0.49996004 0.49113098 0.45696113 0.39753389 0.32432747 0.24724869 0.16686368 0.088124692 0.01893197][0.25468215 0.33880773 0.39757973 0.43314809 0.45171162 0.46204841 0.46688482 0.46120352 0.43780124 0.39290595 0.33233774 0.263513 0.18714951 0.10889083 0.037196878][0.24958234 0.33119571 0.38488862 0.41167256 0.41827595 0.41733545 0.41669983 0.41389552 0.40136042 0.37184215 0.32538161 0.26640368 0.19600101 0.12064692 0.049776308][0.22212486 0.29379436 0.33795649 0.35631874 0.35557911 0.34910572 0.34653032 0.3468996 0.34345913 0.3268086 0.29301876 0.24412429 0.18162748 0.11264511 0.047356173][0.16478528 0.22081284 0.25295293 0.2638053 0.25967091 0.25253627 0.25137374 0.2553491 0.25880042 0.25183457 0.22895512 0.19087078 0.1393428 0.081582047 0.027558107][0.0838684 0.12094838 0.14018413 0.14416344 0.13815036 0.131727 0.13193654 0.13789053 0.14546323 0.1456655 0.13304196 0.10770211 0.071232423 0.030164642 -0.0069258218][0.0075426125 0.026643887 0.034897808 0.034088571 0.027863476 0.022897996 0.023757737 0.029602094 0.038271535 0.042824168 0.038604051 0.025393775 0.0042221779 -0.019657863 -0.039844617][-0.043688007 -0.037943121 -0.036994655 -0.040163372 -0.045239016 -0.048288926 -0.047048092 -0.042512905 -0.035209451 -0.029470094 -0.028732117 -0.033286273 -0.042848255 -0.053643845 -0.06157013]]...]
INFO - root - 2017-12-11 03:20:52.729356: step 87110, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.268 sec/batch; 18h:15m:39s remains)
INFO - root - 2017-12-11 03:20:55.493135: step 87120, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 18h:43m:32s remains)
INFO - root - 2017-12-11 03:20:58.209739: step 87130, loss = 0.71, batch loss = 0.65 (28.2 examples/sec; 0.284 sec/batch; 19h:19m:56s remains)
INFO - root - 2017-12-11 03:21:00.919772: step 87140, loss = 0.70, batch loss = 0.64 (28.4 examples/sec; 0.282 sec/batch; 19h:13m:46s remains)
INFO - root - 2017-12-11 03:21:03.707085: step 87150, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 18h:44m:53s remains)
INFO - root - 2017-12-11 03:21:06.443965: step 87160, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 18h:52m:40s remains)
INFO - root - 2017-12-11 03:21:09.149602: step 87170, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:36m:03s remains)
INFO - root - 2017-12-11 03:21:11.820260: step 87180, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.261 sec/batch; 17h:47m:19s remains)
INFO - root - 2017-12-11 03:21:14.532972: step 87190, loss = 0.69, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 18h:03m:44s remains)
INFO - root - 2017-12-11 03:21:17.236767: step 87200, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.270 sec/batch; 18h:21m:57s remains)
2017-12-11 03:21:17.733227: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14840786 0.17058524 0.17951137 0.17000067 0.14831848 0.1343305 0.1472432 0.19151203 0.25018528 0.2972334 0.31439456 0.3004725 0.27350104 0.255526 0.257982][0.1715416 0.21954536 0.24971719 0.2484822 0.21784095 0.18078853 0.16667008 0.18944833 0.23665592 0.28134277 0.30232111 0.29581898 0.27604589 0.25944951 0.25390908][0.2046386 0.27815062 0.328821 0.33422315 0.29103461 0.22459929 0.17406364 0.164284 0.19097741 0.2294118 0.25709286 0.26707375 0.26557118 0.25907546 0.24913964][0.2328977 0.32177332 0.38393039 0.39163625 0.33856863 0.25089884 0.17285487 0.13578986 0.14266007 0.17289488 0.20586778 0.23387812 0.25508928 0.26478717 0.25678283][0.23967005 0.32751667 0.38791671 0.39456877 0.34233096 0.25588483 0.17648183 0.13289237 0.1290132 0.14840488 0.17655009 0.21044257 0.24561799 0.26990777 0.26984164][0.21585692 0.28810671 0.33630848 0.34291783 0.30780819 0.25057516 0.2005191 0.17444886 0.17003217 0.17417431 0.18185964 0.20067793 0.23139311 0.26121071 0.27192196][0.16279049 0.21305323 0.24697052 0.25908944 0.25528362 0.24744056 0.24671693 0.2516852 0.25044104 0.23332682 0.20712468 0.19311883 0.20244652 0.22728327 0.24781215][0.095872119 0.12754348 0.15230739 0.1748251 0.20498589 0.24727125 0.29526603 0.32972816 0.32963875 0.28862795 0.225779 0.17505948 0.15809934 0.17292379 0.19991194][0.041003451 0.063495785 0.086061671 0.11788532 0.17021604 0.24316783 0.3195217 0.36885154 0.36502537 0.30506596 0.21658494 0.14162025 0.10837843 0.11730876 0.14888132][0.015832635 0.038666911 0.062732853 0.09630765 0.15029787 0.22412637 0.29889229 0.34316039 0.33187982 0.26478189 0.17172673 0.095628954 0.064751223 0.078439325 0.11627837][0.023442723 0.053435478 0.079894826 0.10670333 0.14331497 0.19147356 0.23849982 0.26122475 0.24109706 0.17951714 0.10249737 0.046297073 0.034126274 0.062289517 0.10963582][0.052989651 0.093925647 0.12403012 0.14087981 0.15101236 0.1603086 0.16677943 0.16017015 0.13078628 0.080837384 0.029544249 0.0026188891 0.015904687 0.062547654 0.12133727][0.086607248 0.13871734 0.17450316 0.18435124 0.17083377 0.14406811 0.11305555 0.079648472 0.041605212 0.0013173677 -0.027893942 -0.029615914 0.0052006226 0.067054935 0.13522382][0.1110018 0.16994496 0.21111731 0.21916209 0.19284947 0.14451025 0.090574257 0.040557504 -0.0037460844 -0.040012538 -0.058321189 -0.047368202 -0.00217894 0.066507712 0.13896176][0.1214578 0.17831634 0.21941386 0.22786681 0.20000291 0.14768834 0.089439012 0.037136231 -0.0073216786 -0.042355761 -0.059607822 -0.048854962 -0.0054411851 0.061077766 0.13285413]]...]
INFO - root - 2017-12-11 03:21:20.477928: step 87210, loss = 0.70, batch loss = 0.65 (28.3 examples/sec; 0.282 sec/batch; 19h:13m:38s remains)
INFO - root - 2017-12-11 03:21:23.198086: step 87220, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 18h:01m:55s remains)
INFO - root - 2017-12-11 03:21:25.909836: step 87230, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:40m:29s remains)
INFO - root - 2017-12-11 03:21:28.728296: step 87240, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 18h:30m:26s remains)
INFO - root - 2017-12-11 03:21:31.525813: step 87250, loss = 0.68, batch loss = 0.62 (29.0 examples/sec; 0.276 sec/batch; 18h:46m:33s remains)
INFO - root - 2017-12-11 03:21:34.364206: step 87260, loss = 0.67, batch loss = 0.61 (25.7 examples/sec; 0.311 sec/batch; 21h:11m:08s remains)
INFO - root - 2017-12-11 03:21:37.234117: step 87270, loss = 0.70, batch loss = 0.64 (25.6 examples/sec; 0.312 sec/batch; 21h:15m:09s remains)
INFO - root - 2017-12-11 03:21:40.021624: step 87280, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.279 sec/batch; 19h:01m:48s remains)
INFO - root - 2017-12-11 03:21:42.771203: step 87290, loss = 0.69, batch loss = 0.63 (28.1 examples/sec; 0.285 sec/batch; 19h:22m:54s remains)
INFO - root - 2017-12-11 03:21:45.672302: step 87300, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 18h:39m:30s remains)
2017-12-11 03:21:46.172755: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17232546 0.16328405 0.14268172 0.11998601 0.1040877 0.10221352 0.10891557 0.11604961 0.1222049 0.12834394 0.13771613 0.14731498 0.15271363 0.15098456 0.14311214][0.16290633 0.15561816 0.13765189 0.11906939 0.10790836 0.11194468 0.12599397 0.14132322 0.15524183 0.16890535 0.18771309 0.20618425 0.21612716 0.2132255 0.19903666][0.14565171 0.14201184 0.13249716 0.12606521 0.12663662 0.14012006 0.16140696 0.18264504 0.20058987 0.21832286 0.24417433 0.2703124 0.28432253 0.2802968 0.2595304][0.14528912 0.14833118 0.15126465 0.16032527 0.17397185 0.19588119 0.22138497 0.24393098 0.25994024 0.27564535 0.30303639 0.3332366 0.34997511 0.34475464 0.31756368][0.17620523 0.19107899 0.20772384 0.22851941 0.248497 0.27173629 0.29567188 0.3144159 0.32328373 0.33093849 0.352486 0.37979329 0.3948296 0.38675362 0.3533673][0.22768064 0.25740352 0.28535426 0.31045523 0.32850209 0.346512 0.36478779 0.37694517 0.37671518 0.37314779 0.38250744 0.39933231 0.40630868 0.39131665 0.35113513][0.28061518 0.32282594 0.3558664 0.37887788 0.3917338 0.40356538 0.41611105 0.4216809 0.41346419 0.39896411 0.39327535 0.39458963 0.38814282 0.36237112 0.31449771][0.33099157 0.380567 0.41330758 0.4314141 0.4391852 0.4455235 0.45191872 0.45040268 0.43557149 0.41207522 0.39114532 0.37467667 0.35230508 0.31463465 0.26016292][0.36323416 0.41634029 0.44678739 0.4601492 0.4639557 0.46558842 0.46582246 0.45839229 0.44071043 0.41284224 0.37934816 0.34520349 0.30672097 0.25847504 0.20035477][0.37071577 0.42669716 0.45660165 0.46707 0.4679451 0.46566889 0.46127409 0.45097017 0.43415368 0.40554 0.36274123 0.31332794 0.26112136 0.20599523 0.14858828][0.36022317 0.41711146 0.44715935 0.45579103 0.45436525 0.45067272 0.44585061 0.43558004 0.4192546 0.3891615 0.33983755 0.28051955 0.22079936 0.16531549 0.11468581][0.33805555 0.39259622 0.42046958 0.42453405 0.41811004 0.41316843 0.4100765 0.40026668 0.38197762 0.34929588 0.29786465 0.23700325 0.17858563 0.12987104 0.090989232][0.29796103 0.34774527 0.37205824 0.370996 0.35888159 0.3520422 0.34935826 0.33770081 0.31509846 0.28062642 0.23254637 0.17850246 0.12919769 0.092041954 0.066911034][0.22651567 0.26831681 0.28799197 0.28319743 0.26753935 0.25877619 0.25484318 0.24072509 0.215873 0.18410596 0.14555457 0.10494895 0.069750443 0.045942184 0.033259][0.1387873 0.17035002 0.18441035 0.17697632 0.15930529 0.14752506 0.14007927 0.12395078 0.10068952 0.076413758 0.051501859 0.027330063 0.0077271387 -0.0033707868 -0.0065891058]]...]
INFO - root - 2017-12-11 03:21:48.953198: step 87310, loss = 0.70, batch loss = 0.64 (27.7 examples/sec; 0.289 sec/batch; 19h:40m:16s remains)
INFO - root - 2017-12-11 03:21:51.685907: step 87320, loss = 0.70, batch loss = 0.65 (27.2 examples/sec; 0.294 sec/batch; 20h:00m:26s remains)
INFO - root - 2017-12-11 03:21:54.396831: step 87330, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 18h:03m:22s remains)
INFO - root - 2017-12-11 03:21:57.132239: step 87340, loss = 0.71, batch loss = 0.65 (29.0 examples/sec; 0.275 sec/batch; 18h:45m:25s remains)
INFO - root - 2017-12-11 03:21:59.851285: step 87350, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.275 sec/batch; 18h:45m:13s remains)
INFO - root - 2017-12-11 03:22:02.584978: step 87360, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 18h:52m:32s remains)
INFO - root - 2017-12-11 03:22:05.357430: step 87370, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 18h:10m:40s remains)
INFO - root - 2017-12-11 03:22:08.135198: step 87380, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 18h:20m:37s remains)
INFO - root - 2017-12-11 03:22:10.871651: step 87390, loss = 0.69, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 18h:10m:29s remains)
INFO - root - 2017-12-11 03:22:13.558487: step 87400, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 18h:44m:33s remains)
2017-12-11 03:22:14.186374: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2601276 0.26339391 0.27056354 0.28381598 0.29384705 0.29918763 0.29948169 0.29859355 0.29884481 0.29593065 0.28285244 0.25149766 0.21142477 0.17803904 0.16256659][0.26750794 0.27577943 0.28462392 0.29680708 0.30449921 0.30614838 0.30209494 0.29744625 0.29636216 0.29490432 0.28666675 0.26333261 0.23182893 0.20625414 0.19736314][0.23880522 0.24999005 0.26029986 0.27305946 0.2822848 0.28568614 0.28353474 0.2805942 0.28105909 0.28165284 0.27727017 0.2614767 0.23895852 0.2212484 0.21788688][0.18602034 0.19832791 0.21031973 0.22577232 0.24040654 0.2510089 0.25673515 0.260571 0.26550615 0.2690199 0.26726985 0.25614554 0.2390503 0.22600146 0.22518404][0.12945175 0.14423166 0.16123743 0.18296295 0.20656753 0.22820604 0.24552226 0.25924507 0.270339 0.27654952 0.27479336 0.26313767 0.24566451 0.23184378 0.22906962][0.08712171 0.10797467 0.13475651 0.16744395 0.20357816 0.23900521 0.26977438 0.29331031 0.30788061 0.31171864 0.30290744 0.28208223 0.25581583 0.23479167 0.22580907][0.062631533 0.092865296 0.133029 0.18076667 0.23270454 0.28400525 0.328986 0.36044744 0.37325269 0.36694735 0.34199843 0.30250305 0.25850439 0.22386174 0.20535825][0.056376848 0.097098641 0.15138751 0.21424583 0.28113839 0.346575 0.40343744 0.439517 0.44623324 0.42447492 0.37755904 0.31305054 0.24468099 0.19066529 0.15917185][0.06149216 0.11100651 0.17596339 0.24891432 0.32475424 0.3980104 0.46080062 0.49729866 0.49666011 0.46045595 0.39327577 0.30505842 0.21296245 0.1396067 0.095005944][0.0663086 0.11887354 0.18632984 0.25974423 0.33413339 0.40492097 0.4649061 0.49736789 0.49110743 0.4477061 0.37084615 0.27077243 0.16581884 0.081184186 0.028513489][0.058963519 0.1070031 0.16754451 0.23143764 0.29436398 0.35284474 0.40138596 0.4251014 0.41475394 0.37220693 0.29953095 0.2050532 0.10476959 0.0230785 -0.028114][0.037648216 0.0738908 0.119729 0.16701835 0.21242063 0.25359595 0.28681821 0.30044279 0.2884807 0.25365943 0.19686656 0.12344357 0.044231627 -0.020899972 -0.061713129][0.012424084 0.033603542 0.061204396 0.088705555 0.11400098 0.13628435 0.15372868 0.15852833 0.147794 0.1246111 0.089175351 0.044195618 -0.0049233269 -0.045104623 -0.069260851][-0.0054686703 0.0019696704 0.012896736 0.02259445 0.030127073 0.036100961 0.040581219 0.039614849 0.032227051 0.02085017 0.0052730963 -0.013483689 -0.03370855 -0.0487405 -0.05492476][-0.0048376638 -0.0060853469 -0.0063165547 -0.008053137 -0.011531669 -0.015541864 -0.018928254 -0.022791689 -0.026930233 -0.029789781 -0.03185261 -0.033070251 -0.033528954 -0.03084033 -0.023934713]]...]
INFO - root - 2017-12-11 03:22:16.970077: step 87410, loss = 0.67, batch loss = 0.62 (29.0 examples/sec; 0.276 sec/batch; 18h:47m:54s remains)
INFO - root - 2017-12-11 03:22:19.719918: step 87420, loss = 0.69, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 18h:02m:37s remains)
INFO - root - 2017-12-11 03:22:22.479152: step 87430, loss = 0.70, batch loss = 0.64 (28.5 examples/sec; 0.281 sec/batch; 19h:06m:01s remains)
INFO - root - 2017-12-11 03:22:25.265486: step 87440, loss = 0.70, batch loss = 0.64 (28.7 examples/sec; 0.279 sec/batch; 18h:57m:30s remains)
INFO - root - 2017-12-11 03:22:28.016963: step 87450, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 18h:48m:11s remains)
INFO - root - 2017-12-11 03:22:30.724029: step 87460, loss = 0.67, batch loss = 0.61 (30.4 examples/sec; 0.263 sec/batch; 17h:55m:25s remains)
INFO - root - 2017-12-11 03:22:33.459803: step 87470, loss = 0.68, batch loss = 0.62 (29.2 examples/sec; 0.274 sec/batch; 18h:38m:20s remains)
INFO - root - 2017-12-11 03:22:36.196655: step 87480, loss = 0.70, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 18h:36m:39s remains)
INFO - root - 2017-12-11 03:22:38.964922: step 87490, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.270 sec/batch; 18h:23m:04s remains)
INFO - root - 2017-12-11 03:22:41.726217: step 87500, loss = 0.70, batch loss = 0.65 (28.8 examples/sec; 0.278 sec/batch; 18h:53m:25s remains)
2017-12-11 03:22:42.232246: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29723728 0.29591736 0.27037415 0.23743574 0.20618516 0.19695829 0.2220619 0.27319938 0.31946808 0.33539426 0.31989992 0.27699414 0.20576979 0.11561418 0.029589998][0.48800877 0.49813581 0.47285697 0.43265483 0.38934243 0.36860457 0.388987 0.4410142 0.48747733 0.49800497 0.46964321 0.40722743 0.30916488 0.18973248 0.077290393][0.64523017 0.67913479 0.66858 0.63360953 0.585383 0.55149359 0.55662763 0.59411287 0.62589705 0.62162042 0.57692516 0.49422756 0.37126014 0.22732951 0.095773228][0.73845118 0.80621254 0.82291812 0.80547357 0.76241678 0.72088921 0.70954496 0.72777772 0.74005842 0.71948874 0.65941596 0.55770653 0.41238356 0.24636024 0.098055638][0.78963917 0.89336479 0.94157839 0.94743788 0.91793692 0.87896019 0.86137921 0.87098855 0.87380582 0.84395462 0.77185881 0.65202796 0.48472098 0.29417372 0.12347562][0.82935172 0.96278816 1.0378226 1.0664442 1.0555974 1.0282449 1.017437 1.0321226 1.0371622 1.0028187 0.91640407 0.77468008 0.58296132 0.36515993 0.16769943][0.83743531 0.9843477 1.0741287 1.1213919 1.1336948 1.1271638 1.134475 1.1645348 1.1782445 1.1400591 1.0363616 0.87309343 0.66179872 0.42375982 0.2064402][0.77295816 0.90850139 0.99364907 1.0485222 1.0795968 1.0937611 1.1184981 1.1595013 1.1764624 1.1318201 1.0152239 0.84506834 0.63770884 0.40910652 0.20001969][0.62472069 0.72888541 0.7934795 0.84261167 0.88091558 0.9068687 0.93875259 0.97949922 0.99358219 0.94679946 0.83318996 0.68060613 0.50641626 0.31862372 0.14611128][0.41785276 0.48449963 0.52350533 0.55837452 0.59256375 0.61994308 0.64938444 0.68190336 0.6909107 0.64821917 0.55160004 0.43196422 0.3046861 0.17161563 0.050250698][0.19241555 0.22414547 0.23927936 0.25674891 0.27933621 0.29952979 0.31923366 0.33926961 0.34304151 0.30935824 0.23790419 0.15679868 0.077408731 -0.0011477013 -0.069066972][0.0041219639 0.0077767908 0.0034550745 0.0041235085 0.012538525 0.022425042 0.031255048 0.040584736 0.042006135 0.020461148 -0.02400215 -0.069932438 -0.10905754 -0.1422714 -0.16472146][-0.1086683 -0.12207767 -0.13780105 -0.1485188 -0.15168169 -0.1509072 -0.15066604 -0.14927568 -0.14930287 -0.1603839 -0.18275379 -0.20276912 -0.21391813 -0.21640301 -0.20917802][-0.15344228 -0.17204195 -0.18910563 -0.20253947 -0.21016367 -0.21357186 -0.21682736 -0.21858317 -0.21845976 -0.22186479 -0.2296392 -0.23480462 -0.23269124 -0.22297594 -0.20606849][-0.15439524 -0.17110445 -0.18405473 -0.19517405 -0.2029427 -0.20748459 -0.21108374 -0.21289869 -0.21213037 -0.21162957 -0.21230067 -0.21134238 -0.20604157 -0.19583303 -0.180995]]...]
INFO - root - 2017-12-11 03:22:44.946741: step 87510, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 18h:02m:36s remains)
INFO - root - 2017-12-11 03:22:47.740447: step 87520, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 18h:50m:55s remains)
INFO - root - 2017-12-11 03:22:50.495925: step 87530, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:37m:16s remains)
INFO - root - 2017-12-11 03:22:53.248854: step 87540, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.268 sec/batch; 18h:12m:18s remains)
INFO - root - 2017-12-11 03:22:55.988681: step 87550, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.271 sec/batch; 18h:25m:14s remains)
INFO - root - 2017-12-11 03:22:58.733178: step 87560, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.268 sec/batch; 18h:14m:25s remains)
INFO - root - 2017-12-11 03:23:01.453469: step 87570, loss = 0.72, batch loss = 0.66 (29.8 examples/sec; 0.268 sec/batch; 18h:14m:52s remains)
INFO - root - 2017-12-11 03:23:04.168486: step 87580, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 18h:08m:36s remains)
INFO - root - 2017-12-11 03:23:06.918773: step 87590, loss = 0.70, batch loss = 0.64 (28.6 examples/sec; 0.280 sec/batch; 19h:01m:58s remains)
INFO - root - 2017-12-11 03:23:09.614392: step 87600, loss = 0.70, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:22m:35s remains)
2017-12-11 03:23:10.139310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.035258312 -0.0363352 -0.040016618 -0.044819675 -0.049094677 -0.05250657 -0.053610653 -0.048987169 -0.043258753 -0.042441931 -0.048058096 -0.058233071 -0.071135834 -0.085191682 -0.095788874][0.014020897 0.019289151 0.019490888 0.015905663 0.011058363 0.0060348753 0.0042178347 0.011177301 0.018187013 0.015932091 0.0037487757 -0.01502609 -0.038943388 -0.066543937 -0.089892887][0.094394445 0.11424527 0.12484491 0.12471944 0.11661114 0.10563798 0.10121321 0.1101158 0.11673415 0.11021779 0.090856738 0.063279264 0.025316976 -0.021675456 -0.064274892][0.19609791 0.24103127 0.27261087 0.28264102 0.27271074 0.25468305 0.24717607 0.25779048 0.26132271 0.24654159 0.21679057 0.1781683 0.12143494 0.047428943 -0.021602128][0.30179766 0.38212806 0.44638827 0.4748112 0.46694925 0.44396436 0.43539193 0.44609797 0.44016671 0.40979019 0.36466751 0.31285915 0.23398352 0.12796141 0.02859945][0.40368387 0.52380729 0.623829 0.67073846 0.66456205 0.63828164 0.63191044 0.64121586 0.62214023 0.57503945 0.51661575 0.45441553 0.3538591 0.21456307 0.083929144][0.47897196 0.62998635 0.75718582 0.82087481 0.82226861 0.80227369 0.805005 0.81467521 0.78350496 0.72320229 0.65595 0.58642787 0.467078 0.2981942 0.13976857][0.50972438 0.67270631 0.81048948 0.88681227 0.90365142 0.90173048 0.9225722 0.93943185 0.90376425 0.83698112 0.76197988 0.68176931 0.54313338 0.35109067 0.17389727][0.48038486 0.63387245 0.76512688 0.847288 0.880621 0.8977676 0.93395025 0.95774251 0.92507082 0.86021829 0.78162467 0.69224471 0.54336417 0.34574863 0.16845173][0.39257565 0.51715809 0.62581044 0.70227522 0.7431007 0.7696135 0.81057888 0.83739054 0.81455135 0.76226747 0.69046509 0.60242885 0.46105608 0.28184474 0.12588668][0.26938674 0.35426661 0.42985824 0.48980659 0.52715838 0.55266309 0.58996379 0.61849558 0.61057204 0.57743222 0.52020067 0.44377005 0.32529926 0.18195389 0.061172098][0.12434209 0.1687002 0.20989934 0.24913292 0.27756864 0.29815903 0.32956991 0.35926223 0.36473221 0.34742734 0.30397847 0.24322 0.15527 0.055665676 -0.023387071][-0.0083121955 0.0049798014 0.020173293 0.041208006 0.059858859 0.074937381 0.099557042 0.1261244 0.13700907 0.12825134 0.096524358 0.052979786 -0.00409063 -0.0627219 -0.10344309][-0.094946764 -0.10015598 -0.10044879 -0.092824429 -0.08326564 -0.074039817 -0.057721935 -0.039129708 -0.030439779 -0.036120214 -0.056721464 -0.082088523 -0.11122293 -0.13723496 -0.14980482][-0.13437316 -0.14796194 -0.15507652 -0.15530911 -0.15233196 -0.14783536 -0.13865216 -0.12796457 -0.12330128 -0.12726815 -0.13814344 -0.14873822 -0.15842073 -0.16415869 -0.16110542]]...]
INFO - root - 2017-12-11 03:23:12.911883: step 87610, loss = 0.70, batch loss = 0.65 (26.9 examples/sec; 0.297 sec/batch; 20h:13m:56s remains)
INFO - root - 2017-12-11 03:23:15.649658: step 87620, loss = 0.69, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 18h:47m:49s remains)
INFO - root - 2017-12-11 03:23:18.370418: step 87630, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.262 sec/batch; 17h:47m:36s remains)
INFO - root - 2017-12-11 03:23:21.049445: step 87640, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.267 sec/batch; 18h:11m:22s remains)
INFO - root - 2017-12-11 03:23:23.806543: step 87650, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 18h:06m:09s remains)
INFO - root - 2017-12-11 03:23:26.549657: step 87660, loss = 0.69, batch loss = 0.63 (28.7 examples/sec; 0.279 sec/batch; 18h:57m:35s remains)
INFO - root - 2017-12-11 03:23:29.324076: step 87670, loss = 0.70, batch loss = 0.65 (28.7 examples/sec; 0.279 sec/batch; 18h:58m:27s remains)
INFO - root - 2017-12-11 03:23:32.088042: step 87680, loss = 0.69, batch loss = 0.63 (28.5 examples/sec; 0.280 sec/batch; 19h:04m:23s remains)
INFO - root - 2017-12-11 03:23:34.830128: step 87690, loss = 0.72, batch loss = 0.66 (29.3 examples/sec; 0.273 sec/batch; 18h:35m:42s remains)
INFO - root - 2017-12-11 03:23:37.593293: step 87700, loss = 0.70, batch loss = 0.64 (28.6 examples/sec; 0.280 sec/batch; 19h:01m:50s remains)
2017-12-11 03:23:38.054689: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14793684 0.15820649 0.16271916 0.16622199 0.16764578 0.16519466 0.15368094 0.13662308 0.12061312 0.11052488 0.10981502 0.11520258 0.12200162 0.12456072 0.11833736][0.16830438 0.1764378 0.17782502 0.17996459 0.18207622 0.18113813 0.17097698 0.15503217 0.13891831 0.12905613 0.13144331 0.14336592 0.15784663 0.16312189 0.15153559][0.17401768 0.17570324 0.17150339 0.17165974 0.17469126 0.17568584 0.16768672 0.15417802 0.14046678 0.13491218 0.14477286 0.1654184 0.18559113 0.18867768 0.16715354][0.16727963 0.16047125 0.15037687 0.14896826 0.1540783 0.15863553 0.15496238 0.14575022 0.13606693 0.13626488 0.15401505 0.18175898 0.2036158 0.20085938 0.16953756][0.14853659 0.13720597 0.125137 0.12424968 0.13262382 0.1423375 0.14529957 0.14268921 0.13810468 0.1429112 0.16502349 0.19558188 0.21591134 0.20749424 0.17130317][0.13768828 0.12674026 0.11823539 0.12142392 0.13364063 0.14681476 0.153486 0.15395552 0.15000652 0.15354885 0.17336252 0.20183197 0.21962437 0.20876971 0.17358103][0.14334013 0.13711226 0.13485107 0.1432078 0.15749012 0.16931252 0.17302652 0.16982093 0.16098502 0.1588008 0.17248677 0.19651493 0.212412 0.20265247 0.17222114][0.15480845 0.15418708 0.15859842 0.17237546 0.18852943 0.19745275 0.19522569 0.1849142 0.16877457 0.15945736 0.16578825 0.18381235 0.19699086 0.18887085 0.16382675][0.16089612 0.16330995 0.17143027 0.1897867 0.20953785 0.21854615 0.21294291 0.19688688 0.17433171 0.15822703 0.15697287 0.1676407 0.17655048 0.16926923 0.14921486][0.15727971 0.15776682 0.16570318 0.1866118 0.21017846 0.22160082 0.2157371 0.19729817 0.17225608 0.15310064 0.14743364 0.15212516 0.15646428 0.14925005 0.13340032][0.14541204 0.14044176 0.14611076 0.16827539 0.19424145 0.2071577 0.20101304 0.18136063 0.15645058 0.13712479 0.12945938 0.12908429 0.1280725 0.11967034 0.10683921][0.12516084 0.11446691 0.11743215 0.13921607 0.16452809 0.17606775 0.1682139 0.14780094 0.12468804 0.10705104 0.098797746 0.094085686 0.087492026 0.077224873 0.066781238][0.099306129 0.084115431 0.08439669 0.1038734 0.12619296 0.13505787 0.12598245 0.10664818 0.087092042 0.07245139 0.064434305 0.056767076 0.04632378 0.035304304 0.027453536][0.069448225 0.053725656 0.052615549 0.068234876 0.0857019 0.091304667 0.082221843 0.065955892 0.051142074 0.04036 0.033781055 0.026199739 0.016234318 0.007720137 0.0036063767][0.038628396 0.025869584 0.025174821 0.0361987 0.047579631 0.049856003 0.042153414 0.030846445 0.022030691 0.016251264 0.012595627 0.0075372383 0.00090650562 -0.0035826084 -0.0042130644]]...]
INFO - root - 2017-12-11 03:23:40.776960: step 87710, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 18h:04m:23s remains)
INFO - root - 2017-12-11 03:23:43.499339: step 87720, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 18h:44m:58s remains)
INFO - root - 2017-12-11 03:23:46.264716: step 87730, loss = 0.72, batch loss = 0.66 (29.5 examples/sec; 0.272 sec/batch; 18h:28m:08s remains)
INFO - root - 2017-12-11 03:23:49.045445: step 87740, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 18h:36m:27s remains)
INFO - root - 2017-12-11 03:23:51.775640: step 87750, loss = 0.71, batch loss = 0.66 (29.8 examples/sec; 0.268 sec/batch; 18h:15m:13s remains)
INFO - root - 2017-12-11 03:23:54.487205: step 87760, loss = 0.72, batch loss = 0.66 (29.9 examples/sec; 0.268 sec/batch; 18h:11m:42s remains)
INFO - root - 2017-12-11 03:23:57.249908: step 87770, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 18h:41m:41s remains)
INFO - root - 2017-12-11 03:23:59.995652: step 87780, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 18h:44m:57s remains)
INFO - root - 2017-12-11 03:24:02.732518: step 87790, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:18m:40s remains)
INFO - root - 2017-12-11 03:24:05.468039: step 87800, loss = 0.69, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:17m:42s remains)
2017-12-11 03:24:06.003745: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.046083618 0.051949628 0.044768058 0.024254071 -0.0018588086 -0.023354296 -0.034811836 -0.0413008 -0.047653232 -0.053746358 -0.057310432 -0.057692219 -0.049630113 -0.025319196 0.014861802][0.038795624 0.050522096 0.048648875 0.033412844 0.014580989 4.5173885e-05 -0.0064265341 -0.012131825 -0.021131545 -0.034152616 -0.048947107 -0.060718503 -0.061160274 -0.039698549 0.0032617943][0.048970249 0.079343773 0.096966647 0.09961354 0.096312657 0.091874912 0.088530764 0.077823386 0.057873689 0.028555561 -0.0070871841 -0.039922517 -0.05864682 -0.049150255 -0.01061015][0.093242 0.15863581 0.21244721 0.24618034 0.26567534 0.27337614 0.27044809 0.24750459 0.20681348 0.1501482 0.0829337 0.018854531 -0.02882676 -0.04216034 -0.018407555][0.16356678 0.27528557 0.37785867 0.45395395 0.50363266 0.52813452 0.52793705 0.49152839 0.42461145 0.33190808 0.22449934 0.12043009 0.034913622 -0.0099926759 -0.0094710542][0.22802889 0.38239461 0.53349805 0.65563405 0.74108332 0.78926861 0.80019355 0.75936145 0.67248136 0.54657638 0.40081161 0.25724518 0.13298781 0.052963793 0.025479173][0.25140336 0.4290168 0.61305386 0.77330709 0.89393741 0.97211796 1.007014 0.97863138 0.88716072 0.74143142 0.57086235 0.40032864 0.2471797 0.13859648 0.086570241][0.22197008 0.39696622 0.58839363 0.76636612 0.90987659 1.0150784 1.0808634 1.0805136 1.0043308 0.85999584 0.68562186 0.50744945 0.34294727 0.21999891 0.15349162][0.15603313 0.30660084 0.48033434 0.65064973 0.79545814 0.91165668 1.0011448 1.0335047 0.9881317 0.86737049 0.71174634 0.54751432 0.3917343 0.27133486 0.2022652][0.080863573 0.19405657 0.3329376 0.47612521 0.60278505 0.71045303 0.80520427 0.85857594 0.84349 0.757274 0.63662881 0.50512379 0.37726691 0.276091 0.21651596][0.023030976 0.097147539 0.19494759 0.30040896 0.3957389 0.47855958 0.55799848 0.61217451 0.6154837 0.56409866 0.48663363 0.40074006 0.31478885 0.24433254 0.20146857][-0.0020355682 0.042669572 0.10587904 0.17463671 0.23464461 0.28386888 0.33337662 0.37049764 0.37769258 0.3533833 0.31615537 0.27554408 0.23236521 0.19345646 0.16762458][0.0044616358 0.032549284 0.0736886 0.11524887 0.14495219 0.16137071 0.17712535 0.18921599 0.19082421 0.18356037 0.17602158 0.16946809 0.15844297 0.14283016 0.12947848][0.026843507 0.048136592 0.078128189 0.10369378 0.11356531 0.10728573 0.097997271 0.089971274 0.08537589 0.0881874 0.0984738 0.11070624 0.11605664 0.11102053 0.10222569][0.044900626 0.0637478 0.087801106 0.10439055 0.10408353 0.08832863 0.069313049 0.054022152 0.049379617 0.059785951 0.079128139 0.097115353 0.10459504 0.098155215 0.08591266]]...]
INFO - root - 2017-12-11 03:24:08.748454: step 87810, loss = 0.69, batch loss = 0.63 (29.3 examples/sec; 0.273 sec/batch; 18h:32m:58s remains)
INFO - root - 2017-12-11 03:24:11.479883: step 87820, loss = 0.70, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 18h:25m:43s remains)
INFO - root - 2017-12-11 03:24:14.169815: step 87830, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 18h:28m:26s remains)
INFO - root - 2017-12-11 03:24:16.870599: step 87840, loss = 0.68, batch loss = 0.62 (29.5 examples/sec; 0.271 sec/batch; 18h:26m:23s remains)
INFO - root - 2017-12-11 03:24:19.643521: step 87850, loss = 0.70, batch loss = 0.64 (27.5 examples/sec; 0.291 sec/batch; 19h:47m:01s remains)
INFO - root - 2017-12-11 03:24:22.406908: step 87860, loss = 0.69, batch loss = 0.63 (26.3 examples/sec; 0.304 sec/batch; 20h:38m:42s remains)
INFO - root - 2017-12-11 03:24:25.164550: step 87870, loss = 0.67, batch loss = 0.62 (29.1 examples/sec; 0.275 sec/batch; 18h:40m:38s remains)
INFO - root - 2017-12-11 03:24:27.883854: step 87880, loss = 0.70, batch loss = 0.64 (28.4 examples/sec; 0.282 sec/batch; 19h:07m:52s remains)
INFO - root - 2017-12-11 03:24:30.709328: step 87890, loss = 0.69, batch loss = 0.63 (27.9 examples/sec; 0.287 sec/batch; 19h:30m:47s remains)
INFO - root - 2017-12-11 03:24:33.419075: step 87900, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 18h:13m:09s remains)
2017-12-11 03:24:33.912972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029321639 -0.031589713 -0.029926324 -0.026546853 -0.022149434 -0.018272065 -0.01637746 -0.01643512 -0.017568365 -0.019244466 -0.02086178 -0.021706587 -0.021457471 -0.020180661 -0.018526882][-0.02339563 -0.028994592 -0.028988095 -0.025340639 -0.019481035 -0.013745704 -0.010541063 -0.010133828 -0.011638517 -0.014321976 -0.017579801 -0.020485964 -0.022064699 -0.022073472 -0.021044653][-0.008160471 -0.017967867 -0.020365519 -0.016354317 -0.00834665 0.00046027711 0.0065991045 0.0089699067 0.0079120994 0.004213219 -0.0017878705 -0.0087231491 -0.014367125 -0.017505074 -0.01869349][0.01671852 0.0028305994 -0.0018453761 0.0035925095 0.015528927 0.029715877 0.041440036 0.048226297 0.049151212 0.044403277 0.034082513 0.020509502 0.0083315559 0.00030309297 -0.0045425682][0.04698481 0.030955981 0.025461633 0.033875979 0.051585078 0.0731623 0.092637911 0.10612141 0.11085042 0.10549571 0.090001732 0.068121955 0.047860976 0.033896718 0.024362789][0.075326517 0.060254563 0.056037914 0.068232462 0.091957107 0.120687 0.14766468 0.16829498 0.1777814 0.17277528 0.15295966 0.12357216 0.09585391 0.076071173 0.06171966][0.092222981 0.080984354 0.079255037 0.094106831 0.12142698 0.1541844 0.18555732 0.21124674 0.22500648 0.22149388 0.19994049 0.16662563 0.13456844 0.11086814 0.093018666][0.089280955 0.082506083 0.082953848 0.097545125 0.12388579 0.15540089 0.18610524 0.2127973 0.22886598 0.22768217 0.20781188 0.17574315 0.14422296 0.12031192 0.10200486][0.06338302 0.060770303 0.062855728 0.074847154 0.096203916 0.12176444 0.14715135 0.1706813 0.18644421 0.18762946 0.1722497 0.14595884 0.11957731 0.0990615 0.08331316][0.023840342 0.024055364 0.027186345 0.035773363 0.05045997 0.067861207 0.085498445 0.1029271 0.11578733 0.11831621 0.10836695 0.090152472 0.071548693 0.05691579 0.045927174][-0.013364975 -0.01156148 -0.0077686924 -0.0019381467 0.0066467435 0.01618327 0.025930978 0.036114451 0.044188242 0.046278816 0.0407167 0.0303025 0.019873159 0.012156877 0.0070307711][-0.03612145 -0.033691239 -0.029272521 -0.024797736 -0.020288536 -0.016563388 -0.013012736 -0.0092132445 -0.0061876597 -0.0057872287 -0.0087922746 -0.013509208 -0.017570561 -0.01942138 -0.019540774][-0.041846562 -0.039051041 -0.033919722 -0.029645601 -0.02709071 -0.026631353 -0.026821679 -0.027327348 -0.028300364 -0.029877469 -0.03192322 -0.033502035 -0.033837542 -0.032005347 -0.029002642][-0.034825932 -0.031361312 -0.025480004 -0.020963129 -0.01915017 -0.020165639 -0.022192165 -0.025040051 -0.028541302 -0.031686198 -0.03360986 -0.033808071 -0.032409336 -0.028881928 -0.024633348][-0.021852421 -0.018070977 -0.012189356 -0.0081823664 -0.0074048312 -0.009598027 -0.012739783 -0.016693514 -0.021334352 -0.025063477 -0.02665814 -0.0259175 -0.023572108 -0.019454647 -0.015081708]]...]
INFO - root - 2017-12-11 03:24:36.705473: step 87910, loss = 0.68, batch loss = 0.62 (29.5 examples/sec; 0.271 sec/batch; 18h:23m:51s remains)
INFO - root - 2017-12-11 03:24:39.427088: step 87920, loss = 0.68, batch loss = 0.62 (28.5 examples/sec; 0.280 sec/batch; 19h:02m:28s remains)
INFO - root - 2017-12-11 03:24:42.178958: step 87930, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 18h:41m:52s remains)
INFO - root - 2017-12-11 03:24:44.890257: step 87940, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.268 sec/batch; 18h:11m:17s remains)
INFO - root - 2017-12-11 03:24:47.663960: step 87950, loss = 0.67, batch loss = 0.61 (29.9 examples/sec; 0.268 sec/batch; 18h:11m:15s remains)
INFO - root - 2017-12-11 03:24:50.393930: step 87960, loss = 0.70, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 18h:30m:35s remains)
INFO - root - 2017-12-11 03:24:53.164303: step 87970, loss = 0.69, batch loss = 0.64 (27.1 examples/sec; 0.295 sec/batch; 20h:01m:11s remains)
INFO - root - 2017-12-11 03:24:55.878731: step 87980, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 17h:55m:26s remains)
INFO - root - 2017-12-11 03:24:58.599005: step 87990, loss = 0.68, batch loss = 0.62 (29.9 examples/sec; 0.268 sec/batch; 18h:11m:30s remains)
INFO - root - 2017-12-11 03:25:01.320989: step 88000, loss = 0.68, batch loss = 0.62 (28.5 examples/sec; 0.280 sec/batch; 19h:02m:45s remains)
2017-12-11 03:25:01.823359: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32206538 0.30232278 0.26048476 0.23070718 0.21879108 0.21836436 0.20832317 0.1815587 0.15970476 0.15436737 0.16002217 0.17357105 0.19736238 0.2477252 0.29710087][0.26909223 0.24221739 0.20125502 0.17503643 0.16987225 0.17756061 0.17782317 0.16342874 0.1520994 0.14993531 0.15006019 0.15275078 0.16706324 0.21562383 0.27113998][0.22893751 0.2030614 0.16884938 0.14906757 0.14783764 0.15767424 0.1641276 0.16301343 0.16495834 0.16748072 0.16201515 0.15216148 0.15189163 0.1890257 0.23993814][0.23559423 0.21775851 0.19386725 0.18121296 0.18063992 0.18704337 0.19490182 0.20593397 0.22334471 0.23381077 0.22475965 0.2029268 0.18417555 0.20000999 0.23244239][0.28206131 0.2760359 0.26328322 0.25736606 0.25652534 0.25878504 0.26736829 0.29127407 0.32654604 0.34682992 0.33341128 0.29731467 0.25515854 0.2416824 0.24511108][0.34233513 0.34700418 0.34353402 0.34254867 0.34123635 0.34098804 0.35103291 0.38557315 0.43427354 0.45907128 0.43610269 0.38226524 0.31557241 0.27437451 0.25127757][0.37492317 0.38690278 0.3911247 0.393874 0.39264259 0.39286733 0.40623102 0.44692695 0.49961641 0.51988733 0.48324338 0.41279164 0.3280234 0.27027774 0.23371476][0.35119683 0.3667458 0.37674463 0.3818883 0.38143554 0.38511324 0.40353304 0.4447737 0.49135813 0.50053191 0.45228952 0.37416995 0.28591406 0.22844948 0.19411838][0.27505746 0.28829858 0.30009165 0.30544937 0.30605125 0.31491238 0.3406277 0.38175043 0.42096451 0.42267537 0.37421584 0.30376998 0.22821565 0.18311714 0.15881354][0.18624139 0.19344148 0.20285831 0.20695955 0.20864686 0.22239934 0.25700489 0.30207524 0.34030506 0.34380317 0.30728272 0.25625655 0.20196098 0.17227225 0.15823525][0.13344567 0.13407166 0.13769934 0.13868991 0.13993995 0.15635709 0.20031418 0.25478613 0.30023053 0.3122285 0.29175767 0.26157588 0.22715895 0.20800635 0.1984005][0.13585269 0.13476944 0.13383606 0.13185528 0.13109355 0.14781144 0.20125264 0.26874617 0.32519948 0.344712 0.33452809 0.31722441 0.294638 0.27832016 0.2673234][0.17818967 0.18228309 0.18209016 0.17973715 0.17680125 0.19209947 0.25222835 0.32859161 0.38941562 0.40687728 0.39462757 0.37870467 0.36055183 0.34438923 0.33257061][0.21731552 0.23192729 0.24027686 0.24374869 0.24185397 0.25474095 0.31366861 0.38597891 0.43686646 0.44046542 0.41529897 0.39319602 0.37806103 0.36804092 0.36459249][0.21012029 0.23474824 0.2549015 0.26743913 0.26923358 0.27899882 0.32710806 0.38134664 0.4106 0.39539528 0.35500297 0.32611114 0.31733122 0.32222828 0.33595556]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 03:25:04.623044: step 88010, loss = 0.69, batch loss = 0.63 (28.8 examples/sec; 0.277 sec/batch; 18h:50m:03s remains)
INFO - root - 2017-12-11 03:25:07.435095: step 88020, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.268 sec/batch; 18h:10m:51s remains)
INFO - root - 2017-12-11 03:25:10.194025: step 88030, loss = 0.69, batch loss = 0.63 (28.7 examples/sec; 0.278 sec/batch; 18h:53m:53s remains)
INFO - root - 2017-12-11 03:25:12.889399: step 88040, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 18h:02m:24s remains)
INFO - root - 2017-12-11 03:25:15.653978: step 88050, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 18h:48m:30s remains)
INFO - root - 2017-12-11 03:25:18.414061: step 88060, loss = 0.68, batch loss = 0.62 (28.7 examples/sec; 0.279 sec/batch; 18h:57m:27s remains)
INFO - root - 2017-12-11 03:25:21.136107: step 88070, loss = 0.69, batch loss = 0.63 (28.1 examples/sec; 0.285 sec/batch; 19h:20m:47s remains)
INFO - root - 2017-12-11 03:25:23.882917: step 88080, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:17m:17s remains)
INFO - root - 2017-12-11 03:25:26.612749: step 88090, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.267 sec/batch; 18h:08m:23s remains)
INFO - root - 2017-12-11 03:25:29.314935: step 88100, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.260 sec/batch; 17h:40m:38s remains)
2017-12-11 03:25:29.876603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.034004238 -0.04068793 -0.046255235 -0.049877591 -0.051214002 -0.051068567 -0.050509021 -0.050302889 -0.050288171 -0.050359268 -0.050459649 -0.050107051 -0.049277429 -0.047799077 -0.045104869][-0.046170115 -0.050283175 -0.052911408 -0.054075085 -0.053794738 -0.05259284 -0.051460795 -0.051098537 -0.051234633 -0.051514622 -0.051758118 -0.05136076 -0.049985193 -0.047220781 -0.041935764][-0.052854028 -0.054677103 -0.054988429 -0.05435453 -0.052905865 -0.050665818 -0.0485743 -0.047385141 -0.046946127 -0.046996012 -0.047308933 -0.04701867 -0.045312628 -0.041362636 -0.033560965][-0.053620461 -0.054018106 -0.052944388 -0.05104449 -0.048162021 -0.043681879 -0.038653024 -0.03428873 -0.031196797 -0.029831225 -0.030205088 -0.031060167 -0.030734953 -0.027711863 -0.019630708][-0.05147076 -0.050800674 -0.048587933 -0.04505514 -0.039243516 -0.029943328 -0.018255562 -0.0065434715 0.0027335044 0.0074670892 0.006848475 0.0028985587 -0.0015306044 -0.003642027 4.450226e-05][-0.048446633 -0.047038797 -0.043649759 -0.037829842 -0.02771097 -0.011275568 0.010417626 0.033238843 0.051750287 0.061302133 0.059905246 0.050431728 0.037345529 0.025391249 0.019665418][-0.045584075 -0.043693505 -0.039343644 -0.031188967 -0.016498944 0.0076506408 0.040397253 0.075555794 0.10424634 0.11873115 0.11569868 0.098915108 0.074655034 0.050071642 0.032152645][-0.043833904 -0.041654259 -0.036737408 -0.0271172 -0.009322579 0.020314988 0.06116252 0.10536338 0.14126004 0.15876228 0.15344019 0.12983872 0.09568958 0.060324922 0.032151662][-0.043624323 -0.0414166 -0.036506351 -0.026889451 -0.0088377232 0.021576421 0.0640359 0.11014087 0.14721045 0.16445029 0.15711884 0.1300557 0.091396689 0.051486507 0.019035589][-0.044913311 -0.0429886 -0.038784768 -0.030871168 -0.015879301 0.009872078 0.046502948 0.086502641 0.118411 0.13252485 0.12459185 0.098930687 0.062764533 0.025826933 -0.0040424406][-0.047081929 -0.045778848 -0.042847119 -0.037766136 -0.027996128 -0.010487826 0.015267706 0.043771945 0.06645786 0.075940564 0.068969242 0.048659083 0.020444054 -0.007746588 -0.029856555][-0.049741194 -0.049331829 -0.047822412 -0.045600154 -0.041078992 -0.03211664 -0.017949415 -0.001795774 0.01111849 0.01606554 0.010916051 -0.0024623671 -0.020654587 -0.03812144 -0.050926078][-0.052675124 -0.053263895 -0.052953858 -0.052809808 -0.05208075 -0.049531765 -0.044369005 -0.037954133 -0.032761186 -0.03117526 -0.034346513 -0.041402951 -0.050618511 -0.058755513 -0.063777737][-0.055348866 -0.0569123 -0.057472043 -0.05842416 -0.059609272 -0.060458213 -0.060295336 -0.059388053 -0.058527689 -0.058622234 -0.060138665 -0.062843315 -0.066009738 -0.068267867 -0.0687589][-0.057138987 -0.059405465 -0.060411584 -0.061622649 -0.063140474 -0.064769559 -0.066055864 -0.066879354 -0.067357264 -0.067757927 -0.06822855 -0.068830654 -0.069298238 -0.069142662 -0.068095952]]...]
INFO - root - 2017-12-11 03:25:32.592923: step 88110, loss = 0.70, batch loss = 0.65 (28.0 examples/sec; 0.286 sec/batch; 19h:23m:55s remains)
INFO - root - 2017-12-11 03:25:35.334551: step 88120, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.271 sec/batch; 18h:22m:35s remains)
INFO - root - 2017-12-11 03:25:38.049650: step 88130, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.270 sec/batch; 18h:18m:00s remains)
INFO - root - 2017-12-11 03:25:40.738280: step 88140, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 17h:57m:24s remains)
INFO - root - 2017-12-11 03:25:43.506902: step 88150, loss = 0.68, batch loss = 0.62 (28.7 examples/sec; 0.278 sec/batch; 18h:53m:46s remains)
INFO - root - 2017-12-11 03:25:46.279017: step 88160, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 18h:37m:01s remains)
INFO - root - 2017-12-11 03:25:49.046514: step 88170, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 18h:45m:15s remains)
INFO - root - 2017-12-11 03:25:51.799010: step 88180, loss = 0.71, batch loss = 0.65 (28.6 examples/sec; 0.280 sec/batch; 18h:58m:17s remains)
INFO - root - 2017-12-11 03:25:54.558917: step 88190, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 18h:14m:29s remains)
INFO - root - 2017-12-11 03:25:57.332675: step 88200, loss = 0.70, batch loss = 0.64 (28.7 examples/sec; 0.278 sec/batch; 18h:53m:00s remains)
2017-12-11 03:25:57.897798: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12148947 0.09480916 0.070758775 0.060282923 0.065180682 0.083176225 0.11087812 0.14038138 0.16136836 0.1680146 0.15965948 0.13724789 0.10461053 0.069296874 0.039975654][0.12493584 0.09871769 0.076899953 0.069019258 0.07711032 0.09855321 0.12804215 0.15619178 0.17409176 0.17681351 0.16345523 0.13521929 0.097074769 0.058191508 0.027389241][0.12476572 0.10290502 0.086963117 0.084374525 0.097257085 0.12246571 0.1528433 0.17901404 0.19420822 0.1943509 0.17734438 0.14453185 0.10228948 0.060953274 0.028699761][0.11427997 0.10139132 0.096011505 0.10314715 0.12447405 0.15564646 0.18796425 0.213702 0.22875679 0.22923076 0.21111768 0.17566788 0.13023917 0.0855795 0.049304225][0.087373622 0.088307939 0.098225929 0.11966046 0.15275465 0.19156891 0.22669742 0.25353613 0.2709595 0.27486804 0.25928321 0.2247837 0.17856804 0.13048793 0.088131189][0.0530457 0.067732453 0.093432881 0.13017434 0.17527276 0.22132072 0.25901151 0.28747571 0.30843237 0.31770051 0.30771297 0.27786806 0.23398933 0.1841803 0.13589241][0.024447763 0.046922423 0.0827321 0.13005938 0.18268211 0.23188752 0.26902667 0.29699427 0.32021052 0.33520979 0.333375 0.31246236 0.27589774 0.22892591 0.17875193][0.011325517 0.032443572 0.068304561 0.11675134 0.16890511 0.21499842 0.24736919 0.27184546 0.29473773 0.31396562 0.32071826 0.31166387 0.28727809 0.24952918 0.20444588][0.018472482 0.029267535 0.054395229 0.093303055 0.13617054 0.17239146 0.19577681 0.21383666 0.23348054 0.25381416 0.26704472 0.27043477 0.2617223 0.23918138 0.20693284][0.050628938 0.044417895 0.049527682 0.069169819 0.093807451 0.11271245 0.1223721 0.13154691 0.14653172 0.16633524 0.18486258 0.20074649 0.20951033 0.20590422 0.19134597][0.097315133 0.074266538 0.057116572 0.053801268 0.055695377 0.053067554 0.0463209 0.045622058 0.056100089 0.075601928 0.099845737 0.12809616 0.15388446 0.16917814 0.17258516][0.14483318 0.11225934 0.079443194 0.057534054 0.038820162 0.015509146 -0.007331084 -0.017652936 -0.011384757 0.0081884563 0.038103331 0.077032134 0.11649735 0.14639039 0.1631767][0.18607 0.15527743 0.11808475 0.086502023 0.053566188 0.014316161 -0.021587677 -0.040500391 -0.038349323 -0.018434567 0.016686501 0.063450053 0.11083217 0.14756754 0.16892336][0.21228841 0.19643205 0.1686046 0.13896811 0.1009558 0.053267382 0.0092312107 -0.016898274 -0.019893901 -0.00090081029 0.03761448 0.08841376 0.13743483 0.1729109 0.18961789][0.2238697 0.23244874 0.22528189 0.20796633 0.17334138 0.1240878 0.076331131 0.044162329 0.034398872 0.050233442 0.089856438 0.14130437 0.18702619 0.21484226 0.21959524]]...]
INFO - root - 2017-12-11 03:26:00.639783: step 88210, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 17h:55m:30s remains)
INFO - root - 2017-12-11 03:26:03.394942: step 88220, loss = 0.70, batch loss = 0.64 (28.8 examples/sec; 0.277 sec/batch; 18h:49m:35s remains)
INFO - root - 2017-12-11 03:26:06.149363: step 88230, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 18h:09m:48s remains)
INFO - root - 2017-12-11 03:26:08.840140: step 88240, loss = 0.67, batch loss = 0.61 (30.3 examples/sec; 0.264 sec/batch; 17h:54m:27s remains)
INFO - root - 2017-12-11 03:26:11.561430: step 88250, loss = 0.71, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 18h:27m:05s remains)
INFO - root - 2017-12-11 03:26:14.340690: step 88260, loss = 0.70, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 18h:22m:57s remains)
INFO - root - 2017-12-11 03:26:17.118960: step 88270, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.280 sec/batch; 19h:00m:07s remains)
INFO - root - 2017-12-11 03:26:19.868353: step 88280, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 18h:24m:19s remains)
INFO - root - 2017-12-11 03:26:22.562188: step 88290, loss = 0.70, batch loss = 0.65 (28.8 examples/sec; 0.278 sec/batch; 18h:49m:42s remains)
INFO - root - 2017-12-11 03:26:25.312272: step 88300, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:31m:45s remains)
2017-12-11 03:26:25.867699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.083985768 -0.083489954 -0.078332312 -0.073363878 -0.071941987 -0.074755527 -0.080463961 -0.087602757 -0.093893133 -0.099945165 -0.10490052 -0.10742524 -0.10713387 -0.1047466 -0.10355067][-0.075098574 -0.067276992 -0.051449582 -0.034144875 -0.0209769 -0.014237898 -0.014659137 -0.020573242 -0.030435761 -0.046174861 -0.065305404 -0.080988027 -0.088419005 -0.090872712 -0.094695076][-0.047205359 -0.025124047 0.011229192 0.053727951 0.09450841 0.12599403 0.14131021 0.14034335 0.12266252 0.085891023 0.03711639 -0.005771474 -0.031528588 -0.046218924 -0.060533695][-0.0034727098 0.040131945 0.10827787 0.19101608 0.27758533 0.35096785 0.39265817 0.39664721 0.36073223 0.28754181 0.19465356 0.11354557 0.060500525 0.02557113 -0.0061087804][0.056952883 0.13116333 0.24148254 0.3760961 0.52098769 0.64705294 0.71838051 0.7192986 0.64986938 0.52685368 0.38392931 0.26359302 0.1818549 0.12269698 0.0685278][0.1245288 0.23393382 0.38873172 0.57481015 0.77530533 0.94973236 1.0446337 1.0344098 0.92460686 0.750222 0.56173182 0.40914798 0.3042531 0.22380577 0.1481574][0.18629713 0.33033308 0.52353752 0.74708062 0.98108643 1.1795685 1.2801687 1.2517298 1.1057806 0.89226812 0.67362195 0.50282615 0.3859657 0.2931813 0.2043505][0.22954839 0.40096417 0.61810714 0.85280704 1.080711 1.259955 1.3360064 1.2835163 1.1206063 0.90203428 0.68764615 0.5235399 0.40986925 0.31560704 0.22361049][0.24333839 0.42724183 0.64766788 0.86486053 1.0480399 1.1667708 1.189703 1.1105902 0.95585722 0.77294183 0.60311961 0.47419429 0.38019446 0.29475978 0.20848569][0.22151309 0.39609507 0.59617668 0.77469933 0.8961817 0.94221407 0.90705097 0.80774546 0.67830253 0.55267817 0.44757318 0.368647 0.30494654 0.23743933 0.16552021][0.15894987 0.29960608 0.45728281 0.58779746 0.65718621 0.65469521 0.58917779 0.4906567 0.39448085 0.32191256 0.27250484 0.23660417 0.2009083 0.15348053 0.10008533][0.071684316 0.16507636 0.27009889 0.35331172 0.388792 0.36980551 0.3069877 0.22940592 0.16673246 0.13188389 0.11779023 0.10921297 0.09318281 0.06253843 0.026111886][-0.017344672 0.028596273 0.081601262 0.12062174 0.13128719 0.11099695 0.06791535 0.022163669 -0.0069834031 -0.013346736 -0.0051652025 0.0030726853 0.000599144 -0.016619783 -0.039218266][-0.092691272 -0.085032575 -0.0733936 -0.069214 -0.077099755 -0.095771268 -0.11916165 -0.13662797 -0.13894141 -0.12564391 -0.10419077 -0.085592613 -0.0773836 -0.081613116 -0.090888143][-0.13854542 -0.15414853 -0.16601361 -0.1799524 -0.19528754 -0.20935223 -0.21846108 -0.21874425 -0.20748121 -0.18718734 -0.16351247 -0.14288501 -0.12923364 -0.12352825 -0.12193967]]...]
INFO - root - 2017-12-11 03:26:28.654794: step 88310, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 18h:40m:38s remains)
INFO - root - 2017-12-11 03:26:31.404201: step 88320, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 17h:51m:55s remains)
INFO - root - 2017-12-11 03:26:34.144031: step 88330, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.268 sec/batch; 18h:10m:59s remains)
INFO - root - 2017-12-11 03:26:36.907739: step 88340, loss = 0.73, batch loss = 0.67 (29.2 examples/sec; 0.274 sec/batch; 18h:36m:13s remains)
INFO - root - 2017-12-11 03:26:39.652395: step 88350, loss = 0.70, batch loss = 0.65 (30.5 examples/sec; 0.263 sec/batch; 17h:48m:51s remains)
INFO - root - 2017-12-11 03:26:42.409442: step 88360, loss = 0.69, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:15m:49s remains)
INFO - root - 2017-12-11 03:26:45.202218: step 88370, loss = 0.70, batch loss = 0.64 (25.9 examples/sec; 0.309 sec/batch; 20h:56m:03s remains)
INFO - root - 2017-12-11 03:26:47.984397: step 88380, loss = 0.70, batch loss = 0.64 (28.6 examples/sec; 0.279 sec/batch; 18h:56m:43s remains)
INFO - root - 2017-12-11 03:26:50.784886: step 88390, loss = 0.71, batch loss = 0.65 (28.7 examples/sec; 0.279 sec/batch; 18h:55m:52s remains)
INFO - root - 2017-12-11 03:26:53.537158: step 88400, loss = 0.69, batch loss = 0.63 (28.1 examples/sec; 0.284 sec/batch; 19h:16m:51s remains)
2017-12-11 03:26:54.006143: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.090684451 0.081503712 0.08265844 0.096289657 0.12253493 0.15821552 0.19654691 0.22872609 0.24810231 0.25005966 0.23703346 0.2181537 0.2022237 0.1896873 0.17318149][0.1160136 0.097839952 0.091679879 0.10148405 0.1257282 0.1609624 0.1999203 0.23238692 0.25107819 0.25177494 0.23865944 0.2223265 0.21107632 0.20483097 0.19464755][0.14896218 0.12633541 0.11833376 0.12828735 0.15182076 0.183853 0.21763891 0.24394776 0.25679848 0.25343511 0.24100505 0.23081294 0.22833396 0.23119865 0.22779806][0.1814245 0.16131355 0.16030216 0.1778339 0.20493215 0.23391803 0.25932461 0.27451241 0.2764664 0.26527274 0.25108463 0.24539033 0.25083667 0.26238507 0.264817][0.19785725 0.18840609 0.20145552 0.23394999 0.27192071 0.30372974 0.32299396 0.32574818 0.31340924 0.29110846 0.27189374 0.26696917 0.27676517 0.29366365 0.29893038][0.18977465 0.19271465 0.220299 0.26886567 0.32153091 0.3627502 0.38267922 0.37812647 0.35441378 0.3223938 0.29772365 0.29187965 0.30377051 0.32360488 0.32959616][0.15705262 0.16995737 0.20820326 0.26851651 0.33448318 0.38811702 0.41555756 0.41223672 0.3850739 0.34911519 0.32162771 0.31480235 0.32733002 0.34810257 0.35303625][0.10990809 0.12846677 0.17161556 0.23527844 0.3057529 0.36587954 0.400069 0.40257698 0.38016853 0.34948462 0.32662895 0.3230291 0.33719867 0.35768509 0.36030728][0.065139256 0.087021865 0.12793967 0.18320794 0.24370195 0.29692882 0.32983053 0.33736214 0.32524279 0.30876943 0.29963362 0.30604553 0.32606053 0.34853262 0.35113907][0.034863558 0.058414232 0.092513435 0.13177574 0.17181608 0.20709586 0.23023994 0.23905899 0.23781408 0.23762682 0.24415791 0.26156276 0.28790396 0.31353688 0.31969753][0.023115473 0.048071522 0.076262429 0.099958427 0.11747113 0.12933683 0.13560894 0.13913698 0.14366144 0.15477134 0.17218527 0.19643122 0.22570302 0.25295308 0.26389012][0.025146073 0.055039722 0.082880244 0.097611278 0.097281024 0.086403646 0.072140835 0.063241147 0.064677082 0.077822715 0.098560475 0.1248349 0.15450096 0.18250428 0.19812936][0.029325612 0.066546023 0.099903338 0.1143119 0.1049805 0.077918068 0.045299027 0.020923508 0.01211624 0.01849479 0.034990583 0.058590189 0.086788192 0.11537487 0.1356699][0.024949187 0.068201639 0.11079262 0.13373877 0.1280022 0.0975769 0.055637948 0.01882508 -0.0026006529 -0.0079531064 -0.001063315 0.015123942 0.037930369 0.063544311 0.0850822][0.013887597 0.061081808 0.11471584 0.15322068 0.16238049 0.14097081 0.099104658 0.054181457 0.019894898 0.00056689075 -0.0051732427 0.00041556932 0.014901822 0.034692623 0.054464854]]...]
INFO - root - 2017-12-11 03:26:56.762063: step 88410, loss = 0.71, batch loss = 0.65 (28.3 examples/sec; 0.283 sec/batch; 19h:10m:19s remains)
INFO - root - 2017-12-11 03:26:59.509335: step 88420, loss = 0.71, batch loss = 0.65 (29.7 examples/sec; 0.269 sec/batch; 18h:15m:00s remains)
INFO - root - 2017-12-11 03:27:02.240029: step 88430, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 18h:22m:22s remains)
INFO - root - 2017-12-11 03:27:04.980610: step 88440, loss = 0.71, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 18h:32m:26s remains)
INFO - root - 2017-12-11 03:27:07.819844: step 88450, loss = 0.71, batch loss = 0.65 (28.6 examples/sec; 0.280 sec/batch; 18h:57m:14s remains)
INFO - root - 2017-12-11 03:27:10.648986: step 88460, loss = 0.70, batch loss = 0.64 (28.2 examples/sec; 0.283 sec/batch; 19h:12m:23s remains)
INFO - root - 2017-12-11 03:27:13.421060: step 88470, loss = 0.71, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 18h:30m:44s remains)
INFO - root - 2017-12-11 03:27:16.180805: step 88480, loss = 0.70, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:18m:04s remains)
INFO - root - 2017-12-11 03:27:18.973478: step 88490, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:31m:10s remains)
INFO - root - 2017-12-11 03:27:21.680185: step 88500, loss = 0.71, batch loss = 0.66 (29.8 examples/sec; 0.269 sec/batch; 18h:12m:55s remains)
2017-12-11 03:27:22.256655: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15049586 0.16978678 0.18214867 0.18030299 0.15716663 0.12932032 0.11755199 0.12707226 0.15142085 0.18048957 0.20136318 0.21962355 0.24155259 0.26113167 0.28194419][0.16570908 0.18629612 0.19812873 0.19344807 0.16553667 0.13112479 0.11432347 0.12195665 0.14880824 0.183699 0.21233201 0.23793621 0.26527247 0.28835645 0.31379509][0.16617949 0.18303847 0.1888679 0.17806421 0.14692602 0.11188053 0.09697713 0.10829178 0.14126058 0.18393302 0.22024278 0.25024459 0.2782861 0.30126882 0.3285124][0.16920161 0.18518625 0.18655729 0.17090592 0.1398408 0.10993057 0.10179078 0.11863679 0.15563007 0.20033197 0.23597354 0.26079345 0.28087994 0.29834646 0.32422626][0.18081613 0.20177932 0.20531395 0.19089054 0.16514875 0.14521672 0.14676517 0.16798398 0.20223252 0.2379057 0.260338 0.26822564 0.27185148 0.27890962 0.30079684][0.19781838 0.22760299 0.24039222 0.23469861 0.21830595 0.20945367 0.2200287 0.2430135 0.26867977 0.2874634 0.28930357 0.27492964 0.25845721 0.25203362 0.26670673][0.21741343 0.25570634 0.27959761 0.28511715 0.27750295 0.27548689 0.28921592 0.30850455 0.32114831 0.32058319 0.30195516 0.26832026 0.23539075 0.21689926 0.22356097][0.22947562 0.27121896 0.30182585 0.3151018 0.31224176 0.31056264 0.31981397 0.32962751 0.32755902 0.31013563 0.27744207 0.23443089 0.19500419 0.17099836 0.17232285][0.22034791 0.2583476 0.28815767 0.30306923 0.30064037 0.29528183 0.29673487 0.29635927 0.28312916 0.2560614 0.21900252 0.17759292 0.14168641 0.11891259 0.11793343][0.18364191 0.21003398 0.23023239 0.23955761 0.23373833 0.22370569 0.21848945 0.21203829 0.19541897 0.16852143 0.13700114 0.10601001 0.080801979 0.064601578 0.064353257][0.130173 0.14170222 0.14782318 0.14749558 0.13706446 0.12425579 0.11598113 0.10845376 0.094879277 0.074936479 0.054243997 0.036734186 0.023814019 0.015693756 0.017626692][0.080320239 0.078993328 0.07204137 0.062334988 0.048301954 0.034871824 0.026173711 0.019952005 0.011410052 -0.00035513879 -0.010762247 -0.01722642 -0.02046039 -0.021672692 -0.017017161][0.045415673 0.035527758 0.01960494 0.003484814 -0.012299226 -0.024814183 -0.0324243 -0.036631953 -0.040821005 -0.04649654 -0.050269336 -0.050509628 -0.048577804 -0.045742985 -0.038985152][0.025506254 0.010960078 -0.0097865434 -0.029004021 -0.044618774 -0.055024471 -0.0604683 -0.062405866 -0.063306741 -0.065042906 -0.065382525 -0.063446835 -0.060368489 -0.056661349 -0.0494179][0.017075691 0.00075218111 -0.021805305 -0.041816484 -0.056308378 -0.064539239 -0.068071611 -0.068422645 -0.067417279 -0.067068487 -0.066107772 -0.064125508 -0.061761048 -0.058711622 -0.052069183]]...]
INFO - root - 2017-12-11 03:27:25.010345: step 88510, loss = 0.70, batch loss = 0.65 (29.7 examples/sec; 0.269 sec/batch; 18h:15m:15s remains)
INFO - root - 2017-12-11 03:27:27.774063: step 88520, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 18h:40m:29s remains)
INFO - root - 2017-12-11 03:27:30.526410: step 88530, loss = 0.71, batch loss = 0.65 (29.7 examples/sec; 0.269 sec/batch; 18h:14m:13s remains)
INFO - root - 2017-12-11 03:27:33.266155: step 88540, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 17h:59m:54s remains)
INFO - root - 2017-12-11 03:27:35.988213: step 88550, loss = 0.69, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 18h:25m:24s remains)
INFO - root - 2017-12-11 03:27:38.719918: step 88560, loss = 0.70, batch loss = 0.64 (28.8 examples/sec; 0.278 sec/batch; 18h:50m:52s remains)
INFO - root - 2017-12-11 03:27:41.494554: step 88570, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 18h:32m:08s remains)
INFO - root - 2017-12-11 03:27:44.317701: step 88580, loss = 0.70, batch loss = 0.64 (28.2 examples/sec; 0.284 sec/batch; 19h:12m:43s remains)
INFO - root - 2017-12-11 03:27:47.057804: step 88590, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 18h:17m:21s remains)
INFO - root - 2017-12-11 03:27:49.809669: step 88600, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:31m:07s remains)
2017-12-11 03:27:50.279879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.035459548 -0.034127571 -0.029688347 -0.023774693 -0.018041259 -0.01449586 -0.011146208 -0.0049517886 0.0025077926 0.00746943 0.0068606944 0.0018583756 -0.006389312 -0.016157731 -0.026794517][-0.01122192 -0.0055649378 0.0027665608 0.011913798 0.019712297 0.023571888 0.027793352 0.038542215 0.050324656 0.0572358 0.056060839 0.048811391 0.035849359 0.017437577 -0.0029652254][0.029808782 0.045070671 0.061258376 0.075840391 0.0857002 0.087394856 0.088751636 0.099343129 0.11043237 0.1138799 0.1082331 0.097890496 0.0808434 0.05396596 0.023155373][0.085132264 0.1170973 0.1473505 0.17185789 0.186231 0.1845503 0.17886756 0.18309216 0.18603508 0.17959067 0.16455533 0.148356 0.12535004 0.088538855 0.046452176][0.157387 0.20928673 0.25521335 0.28975281 0.30843389 0.30256492 0.28817666 0.28212947 0.27263168 0.25281686 0.22512171 0.20010526 0.16777985 0.11905942 0.0647008][0.22586465 0.29531911 0.35553291 0.39948261 0.42280492 0.41519913 0.39480057 0.37979043 0.35754767 0.32265463 0.27944076 0.24229445 0.19827119 0.13683552 0.07147868][0.26271528 0.34194857 0.41228482 0.46472004 0.49393293 0.48984537 0.47089121 0.45387229 0.42393062 0.37641153 0.31848288 0.26871979 0.21314606 0.14100777 0.06812562][0.26422432 0.34376204 0.41784912 0.475329 0.5081619 0.50849867 0.49483737 0.48179257 0.45157504 0.39879897 0.3325868 0.2745041 0.21115878 0.13333809 0.057536662][0.23152822 0.29919589 0.36509702 0.4170928 0.44535065 0.44574085 0.43609434 0.42879036 0.40399587 0.35542208 0.29204059 0.23487471 0.17309576 0.10065521 0.03243342][0.17060162 0.21793763 0.26581562 0.30297133 0.31964767 0.3155885 0.30663657 0.30309305 0.28552991 0.24714705 0.19535264 0.14844286 0.099386126 0.044140533 -0.006069859][0.095996313 0.12449546 0.15516526 0.178248 0.18534268 0.17843965 0.16981891 0.16757847 0.1550841 0.12639059 0.087680317 0.053899553 0.021624608 -0.012476888 -0.04156037][0.03066726 0.045681324 0.063922696 0.077626757 0.080013536 0.073498227 0.0665341 0.065068819 0.056735322 0.03681 0.010185511 -0.011744462 -0.029970357 -0.047239359 -0.060340978][-0.0078343377 -0.0016588927 0.0077144424 0.014665731 0.014253823 0.0084151914 0.0025216285 0.00091115 -0.0044172574 -0.01694295 -0.033400681 -0.046242155 -0.055132885 -0.061826181 -0.065410651][-0.030510731 -0.030596754 -0.027774721 -0.025944935 -0.028235154 -0.03351694 -0.038751658 -0.040909722 -0.044350244 -0.051251717 -0.059672344 -0.0653302 -0.067436092 -0.06718877 -0.064947881][-0.043193664 -0.045823503 -0.04566947 -0.045889772 -0.048257105 -0.052334651 -0.056552488 -0.058894284 -0.061198328 -0.064540423 -0.067908637 -0.069129735 -0.067549482 -0.064166091 -0.059798039]]...]
INFO - root - 2017-12-11 03:27:53.074237: step 88610, loss = 0.69, batch loss = 0.63 (28.5 examples/sec; 0.281 sec/batch; 19h:01m:30s remains)
INFO - root - 2017-12-11 03:27:55.791099: step 88620, loss = 0.71, batch loss = 0.66 (29.2 examples/sec; 0.274 sec/batch; 18h:34m:30s remains)
INFO - root - 2017-12-11 03:27:58.523647: step 88630, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:30m:51s remains)
INFO - root - 2017-12-11 03:28:01.272784: step 88640, loss = 0.71, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 18h:29m:57s remains)
INFO - root - 2017-12-11 03:28:04.046515: step 88650, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 18h:36m:31s remains)
INFO - root - 2017-12-11 03:28:06.789302: step 88660, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 18h:00m:47s remains)
INFO - root - 2017-12-11 03:28:09.577237: step 88670, loss = 0.71, batch loss = 0.65 (28.8 examples/sec; 0.278 sec/batch; 18h:48m:46s remains)
INFO - root - 2017-12-11 03:28:12.378179: step 88680, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.270 sec/batch; 18h:15m:10s remains)
INFO - root - 2017-12-11 03:28:15.123499: step 88690, loss = 0.69, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 18h:08m:14s remains)
INFO - root - 2017-12-11 03:28:17.844751: step 88700, loss = 0.72, batch loss = 0.66 (30.9 examples/sec; 0.259 sec/batch; 17h:31m:43s remains)
2017-12-11 03:28:18.305880: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15007456 0.0991352 0.051546708 0.019275075 0.010212356 0.03303507 0.079999082 0.12885796 0.15989003 0.16260028 0.12951444 0.064010024 -0.0092859007 -0.0709156 -0.10953007][0.13966367 0.086338446 0.035379559 0.001002945 -0.0078568673 0.017028956 0.066864736 0.11805802 0.14966899 0.1511976 0.11721783 0.052648228 -0.017349778 -0.074641824 -0.11048239][0.1530048 0.10568253 0.0599324 0.0328234 0.033630595 0.066941015 0.12043607 0.16887751 0.19102463 0.17938599 0.13148093 0.056316018 -0.018203082 -0.0745508 -0.10792794][0.19367771 0.16168025 0.13122502 0.12137168 0.14122535 0.19025782 0.24965227 0.29120481 0.29404804 0.25556168 0.17748035 0.07611873 -0.014722611 -0.076054938 -0.10787946][0.23459794 0.22715166 0.22375214 0.24235927 0.2897824 0.36047825 0.42750943 0.45880195 0.43537074 0.36138052 0.24329171 0.10542549 -0.010396744 -0.0816901 -0.11309505][0.27110994 0.29313576 0.32419729 0.37807626 0.45741904 0.55102378 0.62364113 0.63990933 0.58510441 0.47173274 0.31230462 0.13693389 -0.0056830295 -0.089522459 -0.12270186][0.29395527 0.34135407 0.40400341 0.49003634 0.59703994 0.7073366 0.77841264 0.77425587 0.68749332 0.54003555 0.350141 0.150746 -0.0078800665 -0.099248953 -0.13366526][0.28807643 0.34918603 0.43050843 0.53528386 0.65643656 0.77193344 0.83521503 0.81189567 0.70242572 0.53651816 0.336961 0.13577521 -0.020918336 -0.10960781 -0.14211908][0.26602015 0.32661989 0.40816391 0.51089704 0.62624967 0.73269629 0.78538 0.75258434 0.63889211 0.47592568 0.28776467 0.10352199 -0.037457611 -0.11606284 -0.14452535][0.24836186 0.29682875 0.36174536 0.44389206 0.53722119 0.62479687 0.66788906 0.63755947 0.53801966 0.39633459 0.23276614 0.072655469 -0.049494144 -0.11716151 -0.14180107][0.23855485 0.26891664 0.30862355 0.36136058 0.4262132 0.49172831 0.52675265 0.50553042 0.42968678 0.31832355 0.18363039 0.0472119 -0.058627769 -0.11705126 -0.13810557][0.23468676 0.24823748 0.26389477 0.28959438 0.33010766 0.37833598 0.40883565 0.39848331 0.34575924 0.26191226 0.15055433 0.030799733 -0.064837866 -0.11762469 -0.13608256][0.219784 0.22145309 0.2213814 0.22930232 0.25493705 0.29459277 0.32595718 0.32704854 0.2928929 0.22909313 0.13362876 0.024079962 -0.066440433 -0.11732468 -0.13537976][0.18323384 0.1798375 0.17386122 0.17414691 0.19376244 0.23151423 0.26701453 0.27836707 0.25769281 0.20755237 0.12391882 0.022828564 -0.063849092 -0.11485069 -0.13481086][0.12344955 0.11942755 0.11457972 0.11475344 0.13357195 0.17111386 0.20923191 0.22759312 0.21739547 0.17965968 0.10959711 0.020420585 -0.059555862 -0.11005398 -0.13298596]]...]
INFO - root - 2017-12-11 03:28:21.022367: step 88710, loss = 0.71, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 18h:33m:33s remains)
INFO - root - 2017-12-11 03:28:23.759368: step 88720, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 18h:38m:39s remains)
INFO - root - 2017-12-11 03:28:26.592877: step 88730, loss = 0.69, batch loss = 0.63 (28.5 examples/sec; 0.281 sec/batch; 18h:59m:44s remains)
INFO - root - 2017-12-11 03:28:29.330712: step 88740, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 18h:26m:33s remains)
INFO - root - 2017-12-11 03:28:32.053624: step 88750, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 18h:25m:22s remains)
INFO - root - 2017-12-11 03:28:34.846528: step 88760, loss = 0.68, batch loss = 0.62 (29.4 examples/sec; 0.272 sec/batch; 18h:24m:08s remains)
INFO - root - 2017-12-11 03:28:37.593469: step 88770, loss = 0.71, batch loss = 0.65 (28.8 examples/sec; 0.278 sec/batch; 18h:48m:06s remains)
INFO - root - 2017-12-11 03:28:40.358047: step 88780, loss = 0.68, batch loss = 0.62 (29.1 examples/sec; 0.275 sec/batch; 18h:38m:35s remains)
INFO - root - 2017-12-11 03:28:43.059512: step 88790, loss = 0.71, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 18h:32m:09s remains)
INFO - root - 2017-12-11 03:28:45.858522: step 88800, loss = 0.69, batch loss = 0.64 (28.6 examples/sec; 0.280 sec/batch; 18h:57m:24s remains)
2017-12-11 03:28:46.397017: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.194809 0.17862086 0.14916724 0.12250337 0.10142154 0.087076008 0.075742774 0.06527216 0.050601278 0.022871176 -0.014572423 -0.0486885 -0.070804812 -0.082294911 -0.085813493][0.2360789 0.21790531 0.18395938 0.1562025 0.14066273 0.13479684 0.12818885 0.11860485 0.10317863 0.074859604 0.033739306 -0.0094423 -0.042395044 -0.063541166 -0.074748278][0.25160646 0.23690769 0.2077771 0.18733427 0.18461607 0.19368501 0.19747429 0.19101892 0.17377745 0.1421445 0.093543448 0.03706719 -0.010711426 -0.04392447 -0.063632466][0.24374606 0.23814811 0.2214355 0.21312232 0.2243987 0.2481153 0.26260939 0.257932 0.23552565 0.19625539 0.13673997 0.066476561 0.005663292 -0.036144644 -0.060177378][0.22735734 0.23215723 0.22991031 0.23395389 0.25612435 0.28999653 0.31259125 0.30839184 0.27996007 0.23090684 0.1593051 0.077251546 0.007591763 -0.038066629 -0.062234934][0.21488863 0.22621974 0.23583166 0.25131458 0.28339151 0.32491094 0.3528536 0.34770587 0.31286764 0.25306782 0.16902649 0.077295616 0.002466118 -0.043750461 -0.066473924][0.20706533 0.21923271 0.23645085 0.26256582 0.30584738 0.35472506 0.38588643 0.37826988 0.33663553 0.26597878 0.16913141 0.068405308 -0.0093188025 -0.052864488 -0.072080269][0.19696705 0.20587209 0.22623332 0.25999779 0.3133162 0.3687906 0.402136 0.3923052 0.34557608 0.26703498 0.16020508 0.052700084 -0.025884343 -0.06459596 -0.078678094][0.17712626 0.18295713 0.20537597 0.24474223 0.30643305 0.36764055 0.40337953 0.39373988 0.3467516 0.26666564 0.15508267 0.042906009 -0.037040569 -0.072668076 -0.083141811][0.14354451 0.14976026 0.17641273 0.22135292 0.2903139 0.35707766 0.39642844 0.38987818 0.34699926 0.27065784 0.159415 0.045329988 -0.036688492 -0.072921887 -0.083731495][0.10211735 0.11106604 0.14185992 0.19005162 0.26279813 0.33326003 0.37699366 0.37585497 0.33880529 0.26805678 0.16063829 0.048490651 -0.034053244 -0.072189331 -0.08485803][0.06413652 0.073126592 0.10244901 0.1477797 0.21873339 0.29048225 0.34036008 0.34857705 0.31995285 0.25566661 0.1535221 0.045803491 -0.035366952 -0.075389363 -0.089646026][0.034876309 0.040109765 0.062338058 0.10028112 0.16574189 0.23706135 0.29375178 0.31466341 0.29822683 0.24248289 0.14656113 0.043518163 -0.036493745 -0.079474218 -0.095967151][0.016490463 0.018658694 0.034373727 0.065398 0.12453023 0.19287416 0.25248739 0.283559 0.279311 0.23323138 0.14449959 0.045948967 -0.033555649 -0.0803291 -0.099786647][0.001820778 0.0043244171 0.01783199 0.045658804 0.10016244 0.164047 0.22177476 0.25732762 0.26203835 0.2258013 0.14589286 0.052361164 -0.026962889 -0.077615134 -0.10004932]]...]
INFO - root - 2017-12-11 03:28:49.173929: step 88810, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 18h:36m:22s remains)
INFO - root - 2017-12-11 03:28:51.888832: step 88820, loss = 0.71, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 18h:34m:06s remains)
INFO - root - 2017-12-11 03:28:54.628513: step 88830, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 18h:39m:49s remains)
INFO - root - 2017-12-11 03:28:57.386575: step 88840, loss = 0.68, batch loss = 0.62 (29.6 examples/sec; 0.270 sec/batch; 18h:17m:21s remains)
INFO - root - 2017-12-11 03:29:00.157228: step 88850, loss = 0.72, batch loss = 0.66 (29.0 examples/sec; 0.276 sec/batch; 18h:40m:31s remains)
INFO - root - 2017-12-11 03:29:02.917361: step 88860, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 18h:10m:55s remains)
INFO - root - 2017-12-11 03:29:05.702801: step 88870, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 18h:35m:34s remains)
INFO - root - 2017-12-11 03:29:08.496587: step 88880, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 18h:35m:09s remains)
INFO - root - 2017-12-11 03:29:11.236740: step 88890, loss = 0.70, batch loss = 0.64 (28.6 examples/sec; 0.280 sec/batch; 18h:56m:45s remains)
INFO - root - 2017-12-11 03:29:13.956575: step 88900, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 18h:32m:10s remains)
2017-12-11 03:29:14.468910: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.082311362 0.086661443 0.091268115 0.10119739 0.11394048 0.12587887 0.1384268 0.15238038 0.1638134 0.16983235 0.16710034 0.15630943 0.13992806 0.1299703 0.13552174][0.085280687 0.090799809 0.096157372 0.10659596 0.11994745 0.13190681 0.14339766 0.15670648 0.16902232 0.17724049 0.17621574 0.16611408 0.14872599 0.13598388 0.13874327][0.084136121 0.091489427 0.097759269 0.10838579 0.12148196 0.13204077 0.14052749 0.15063106 0.16088344 0.168831 0.168689 0.16099222 0.14631005 0.13463017 0.13748865][0.087025 0.095701464 0.10266364 0.11333933 0.1255798 0.13380162 0.13838477 0.14397362 0.15023212 0.15614712 0.15632308 0.15208463 0.14271681 0.13549075 0.14151078][0.094998494 0.10445621 0.11114766 0.12069678 0.13050544 0.13482028 0.13420358 0.13420728 0.13549545 0.13906482 0.14058155 0.14162664 0.13998951 0.13990821 0.15069906][0.10619106 0.11649746 0.12259391 0.13035356 0.13702296 0.13724969 0.1322317 0.12789391 0.12535384 0.12766261 0.13144279 0.13779233 0.14293577 0.14857952 0.16249442][0.11668653 0.12810391 0.13354096 0.13889481 0.14161964 0.13758536 0.1293017 0.12268826 0.11861336 0.12137702 0.12814406 0.13898493 0.14868201 0.15747888 0.17201661][0.12684423 0.13876317 0.14316976 0.14615963 0.14528464 0.13775709 0.12775178 0.12103916 0.11778349 0.12222419 0.13170405 0.14508232 0.15617082 0.16458021 0.1764171][0.13603224 0.14690937 0.14997242 0.15149094 0.1484621 0.13882777 0.128233 0.12247621 0.12098019 0.12682393 0.13754068 0.15084518 0.16037971 0.16550177 0.17193793][0.13931726 0.14770924 0.14896469 0.14937867 0.14516555 0.13458182 0.12407209 0.11973929 0.12052418 0.1276065 0.13857044 0.15026765 0.15666336 0.15721402 0.15736765][0.13228616 0.13800795 0.13828425 0.13933881 0.13668893 0.12834316 0.12040446 0.11889203 0.12257154 0.13054541 0.14046359 0.14889991 0.15029933 0.14454719 0.13733661][0.11827282 0.12152173 0.1214676 0.12402833 0.12440452 0.12020862 0.11614496 0.11761416 0.12350978 0.13132392 0.1392238 0.14391229 0.14037894 0.12901868 0.1158152][0.10240732 0.10312325 0.10276038 0.10655844 0.10990676 0.1103339 0.11058731 0.11482945 0.1220486 0.12898621 0.13464986 0.13615918 0.12922223 0.11451809 0.098130308][0.084933534 0.084354952 0.084569566 0.089740887 0.095780812 0.10030541 0.10402015 0.10960428 0.11628444 0.12113976 0.12424911 0.12350903 0.11556554 0.10101599 0.085225023][0.07071396 0.070758119 0.071967728 0.077434145 0.084201969 0.090445772 0.095398918 0.10046525 0.10531826 0.10801112 0.10932454 0.1079724 0.10148088 0.090278111 0.078184925]]...]
INFO - root - 2017-12-11 03:29:17.177554: step 88910, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 18h:40m:05s remains)
INFO - root - 2017-12-11 03:29:19.923617: step 88920, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 18h:19m:24s remains)
INFO - root - 2017-12-11 03:29:22.692558: step 88930, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.271 sec/batch; 18h:18m:50s remains)
INFO - root - 2017-12-11 03:29:25.431022: step 88940, loss = 0.70, batch loss = 0.64 (28.7 examples/sec; 0.279 sec/batch; 18h:50m:35s remains)
INFO - root - 2017-12-11 03:29:28.262128: step 88950, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 18h:19m:14s remains)
INFO - root - 2017-12-11 03:29:30.976172: step 88960, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 18h:22m:43s remains)
INFO - root - 2017-12-11 03:29:33.747086: step 88970, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 18h:12m:14s remains)
INFO - root - 2017-12-11 03:29:36.542977: step 88980, loss = 0.71, batch loss = 0.65 (28.6 examples/sec; 0.280 sec/batch; 18h:54m:23s remains)
INFO - root - 2017-12-11 03:29:39.280511: step 88990, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 18h:32m:37s remains)
INFO - root - 2017-12-11 03:29:42.059863: step 89000, loss = 0.69, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 18h:43m:01s remains)
2017-12-11 03:29:42.563397: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.53097695 0.43188223 0.30055022 0.19134107 0.10174857 0.032087918 -0.013078301 -0.037566956 -0.053371884 -0.066682167 -0.073774979 -0.075197011 -0.072612092 -0.066743746 -0.060270708][0.51200038 0.40916792 0.28171706 0.18483752 0.11149787 0.058848418 0.026930956 0.0086821448 -0.0084767537 -0.029300211 -0.04711628 -0.059878737 -0.066298783 -0.065571018 -0.061700456][0.40117574 0.31358296 0.21675968 0.15652719 0.12203695 0.10646278 0.10257172 0.098741457 0.082393683 0.051274892 0.016048146 -0.01627464 -0.039473027 -0.050850123 -0.054212071][0.24943987 0.19576976 0.15075809 0.14553559 0.1657384 0.19924128 0.23025954 0.24210185 0.2228744 0.17528021 0.11561564 0.056736868 0.0093500521 -0.021533975 -0.038088404][0.11757953 0.10996587 0.12735616 0.18350033 0.25982234 0.33856425 0.39769909 0.41710153 0.38669398 0.31615153 0.22911426 0.14323713 0.070779912 0.017523529 -0.01604221][0.044646915 0.084600635 0.16063318 0.27040991 0.39014924 0.49641007 0.56561434 0.57781118 0.52663821 0.4295688 0.31754595 0.2114536 0.1206795 0.049795128 0.0016159974][0.026866563 0.10581064 0.2251586 0.37052378 0.51237559 0.62293667 0.68151212 0.67279482 0.59636796 0.47630233 0.34834743 0.23321426 0.13525113 0.057199433 0.0031045687][0.036097612 0.13752024 0.27781624 0.43449238 0.57441229 0.66837293 0.70136845 0.66552407 0.56766564 0.43613783 0.30645809 0.19643143 0.10508195 0.033073518 -0.015845139][0.045850571 0.14813633 0.28269154 0.42434898 0.54051441 0.60457861 0.60793668 0.550189 0.44467086 0.31989026 0.20580396 0.11489777 0.042801842 -0.011633072 -0.04632175][0.037121538 0.12031112 0.22709794 0.33413562 0.41422397 0.44672111 0.42860016 0.36448243 0.27004352 0.16918831 0.083345294 0.019925546 -0.026327135 -0.057669871 -0.074378952][0.00554892 0.05938004 0.12950365 0.19685613 0.24225886 0.25230744 0.22752316 0.173735 0.10458733 0.036776796 -0.01672703 -0.052613862 -0.075300239 -0.086826086 -0.088927396][-0.037383955 -0.012252519 0.024717048 0.059312209 0.080071159 0.080055378 0.061296269 0.02832045 -0.010273567 -0.045269594 -0.070392653 -0.084791347 -0.091378666 -0.091375656 -0.086402766][-0.073069215 -0.070382096 -0.057378862 -0.044327211 -0.036886159 -0.037954651 -0.045528311 -0.057263982 -0.069640271 -0.0792066 -0.083993182 -0.084314846 -0.082149908 -0.078138188 -0.072920144][-0.088492163 -0.099513054 -0.10047493 -0.099054679 -0.097687446 -0.097195655 -0.09605059 -0.093579367 -0.089575611 -0.084136523 -0.077675752 -0.071083531 -0.0658282 -0.062127188 -0.059482258][-0.08245822 -0.098359413 -0.10372431 -0.10591449 -0.10648505 -0.10533763 -0.10137702 -0.094578207 -0.085849732 -0.076562382 -0.067945093 -0.060894825 -0.056398787 -0.054351851 -0.053744249]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 03:29:45.318824: step 89010, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 18h:19m:54s remains)
INFO - root - 2017-12-11 03:29:48.087496: step 89020, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.271 sec/batch; 18h:17m:50s remains)
INFO - root - 2017-12-11 03:29:50.817978: step 89030, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 18h:05m:00s remains)
INFO - root - 2017-12-11 03:29:53.622232: step 89040, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 18h:20m:05s remains)
INFO - root - 2017-12-11 03:29:56.389374: step 89050, loss = 0.70, batch loss = 0.64 (28.4 examples/sec; 0.282 sec/batch; 19h:03m:40s remains)
INFO - root - 2017-12-11 03:29:59.197348: step 89060, loss = 0.69, batch loss = 0.63 (28.4 examples/sec; 0.282 sec/batch; 19h:03m:24s remains)
INFO - root - 2017-12-11 03:30:01.939053: step 89070, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.272 sec/batch; 18h:21m:51s remains)
INFO - root - 2017-12-11 03:30:04.728177: step 89080, loss = 0.69, batch loss = 0.63 (28.2 examples/sec; 0.283 sec/batch; 19h:09m:07s remains)
INFO - root - 2017-12-11 03:30:07.490287: step 89090, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 18h:35m:10s remains)
INFO - root - 2017-12-11 03:30:10.264926: step 89100, loss = 0.70, batch loss = 0.64 (28.6 examples/sec; 0.279 sec/batch; 18h:53m:09s remains)
2017-12-11 03:30:10.755346: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.036152698 -0.03802634 -0.038709976 -0.038581278 -0.038191326 -0.038351741 -0.039829515 -0.043151356 -0.04842734 -0.05502905 -0.061731398 -0.066622332 -0.069368869 -0.069677383 -0.06764219][-0.013788811 -0.014528594 -0.015777459 -0.016600849 -0.017040929 -0.017681148 -0.019826857 -0.024377123 -0.03126058 -0.039761424 -0.048800819 -0.056495134 -0.062053423 -0.065244265 -0.066309758][0.030561529 0.034058589 0.033868257 0.032895889 0.031708617 0.030632595 0.02820175 0.02312097 0.01525625 0.0049742241 -0.0067758313 -0.017338369 -0.02471162 -0.028599037 -0.029732984][0.095311791 0.10787519 0.11239965 0.11385338 0.11309835 0.11208375 0.10984081 0.10456538 0.095469519 0.081837311 0.065000273 0.049211986 0.038275119 0.032973066 0.03206579][0.16544323 0.1903425 0.2033992 0.21057816 0.21229392 0.21255931 0.21163066 0.20728315 0.19715248 0.17887025 0.15420058 0.12939017 0.11070057 0.09994901 0.09622582][0.21863367 0.25485465 0.27705556 0.29097202 0.29630965 0.29890963 0.30063692 0.2986376 0.28836572 0.26613438 0.23367745 0.19873692 0.16948141 0.15001352 0.14059433][0.22967717 0.27203012 0.30007258 0.31838152 0.32623833 0.3311874 0.33614919 0.33774385 0.32975757 0.30720228 0.27148947 0.23060092 0.19388276 0.16760795 0.15326473][0.19877496 0.23969479 0.26723647 0.28433606 0.29070136 0.294894 0.30093861 0.30566078 0.30201849 0.28386325 0.25201738 0.21376179 0.1780708 0.15213156 0.13790461][0.14665203 0.17943384 0.20043369 0.21124192 0.21233189 0.21244927 0.21694636 0.22380018 0.22626473 0.21727376 0.19650236 0.16957049 0.14384101 0.12570398 0.11639652][0.0880679 0.10844769 0.11925675 0.12114511 0.11537528 0.11012465 0.11222208 0.12168324 0.13279115 0.13771826 0.1338315 0.12428098 0.11366907 0.10649579 0.10309452][0.035472941 0.04448428 0.046218112 0.040730961 0.029424341 0.019446459 0.01922442 0.031257991 0.051479448 0.071060576 0.084726863 0.092729367 0.09668842 0.099038973 0.099734463][-0.0014163018 0.00080247695 -0.0018131525 -0.0097608073 -0.022559369 -0.034685191 -0.036654703 -0.023760084 0.0019745608 0.031370219 0.057192896 0.076924168 0.0898993 0.096629128 0.097213462][-0.019412283 -0.019438021 -0.021891598 -0.027717371 -0.037740465 -0.048449166 -0.050606959 -0.038697947 -0.012946052 0.018592434 0.047852434 0.07060276 0.084598482 0.089597233 0.085991569][-0.02902266 -0.028488899 -0.027991418 -0.028956031 -0.033359241 -0.039721567 -0.040771592 -0.031594254 -0.010976066 0.015447534 0.040331736 0.058724176 0.067826159 0.067492232 0.058511913][-0.042635866 -0.041106097 -0.03711053 -0.033189431 -0.031985987 -0.033775657 -0.033980705 -0.028930478 -0.016460542 0.00079549314 0.017219426 0.028066911 0.030646041 0.025430743 0.013717692]]...]
INFO - root - 2017-12-11 03:30:13.442898: step 89110, loss = 0.70, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 18h:29m:26s remains)
INFO - root - 2017-12-11 03:30:16.227446: step 89120, loss = 0.68, batch loss = 0.63 (28.9 examples/sec; 0.277 sec/batch; 18h:44m:29s remains)
INFO - root - 2017-12-11 03:30:19.025616: step 89130, loss = 0.72, batch loss = 0.66 (28.3 examples/sec; 0.282 sec/batch; 19h:05m:03s remains)
INFO - root - 2017-12-11 03:30:21.832298: step 89140, loss = 0.71, batch loss = 0.65 (28.2 examples/sec; 0.283 sec/batch; 19h:08m:50s remains)
INFO - root - 2017-12-11 03:30:24.656592: step 89150, loss = 0.71, batch loss = 0.65 (29.8 examples/sec; 0.268 sec/batch; 18h:08m:19s remains)
INFO - root - 2017-12-11 03:30:27.381206: step 89160, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.271 sec/batch; 18h:17m:49s remains)
INFO - root - 2017-12-11 03:30:30.121792: step 89170, loss = 0.69, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 18h:32m:54s remains)
INFO - root - 2017-12-11 03:30:32.825213: step 89180, loss = 0.72, batch loss = 0.66 (28.8 examples/sec; 0.278 sec/batch; 18h:47m:03s remains)
INFO - root - 2017-12-11 03:30:35.585207: step 89190, loss = 0.69, batch loss = 0.63 (28.9 examples/sec; 0.277 sec/batch; 18h:42m:03s remains)
INFO - root - 2017-12-11 03:30:38.326613: step 89200, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 17h:58m:23s remains)
2017-12-11 03:30:38.757640: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17035204 0.2495915 0.30348724 0.31508675 0.29247481 0.25444424 0.21956348 0.20524153 0.22332542 0.2633636 0.30828348 0.3450323 0.36691329 0.37914017 0.38367966][0.14897455 0.220427 0.26888657 0.27935934 0.25899231 0.22464435 0.19530079 0.18689997 0.21191585 0.26480392 0.33415216 0.40613198 0.46418962 0.50860953 0.53905147][0.11619262 0.17772801 0.21997954 0.23004885 0.21429011 0.18744349 0.16743098 0.16737583 0.19991888 0.26488215 0.35536349 0.45718855 0.54509073 0.61394906 0.66188228][0.10110006 0.16198723 0.20827118 0.22752059 0.22465181 0.21123935 0.20146699 0.20591338 0.23869835 0.30533719 0.40194452 0.51454312 0.61329436 0.68776739 0.73492032][0.11165052 0.1875993 0.25588617 0.30108425 0.32498935 0.333914 0.33465824 0.33495167 0.35338107 0.40359527 0.48481813 0.5835951 0.67077613 0.73226005 0.76330876][0.14460716 0.24941285 0.35548306 0.44150314 0.50442982 0.543375 0.55484205 0.54211348 0.53060108 0.5449965 0.58977193 0.65293843 0.70942622 0.74475044 0.751799][0.18315379 0.31828266 0.46371084 0.59242344 0.69655085 0.76675624 0.78715211 0.75643229 0.70698583 0.67486835 0.671056 0.6871742 0.70529425 0.71222305 0.697571][0.21068212 0.36523625 0.535804 0.69229376 0.82399285 0.91416973 0.93743569 0.88999254 0.80870008 0.73644608 0.68915671 0.66288185 0.6480481 0.63462919 0.60907972][0.2124304 0.36646876 0.53650063 0.69279778 0.82523078 0.91412365 0.93124831 0.873036 0.77662504 0.68460166 0.61277592 0.56086391 0.52731282 0.50614917 0.48220974][0.18059534 0.31275588 0.45578125 0.58379519 0.69034475 0.757741 0.7622025 0.70238912 0.61106092 0.52351874 0.4513402 0.39675286 0.36254209 0.34547508 0.331303][0.11604856 0.21044765 0.30904487 0.39234409 0.45854837 0.4948515 0.48578882 0.43330556 0.36252135 0.29713917 0.24231042 0.20174326 0.179351 0.17261033 0.17011984][0.02955525 0.080960676 0.13237372 0.1708542 0.19812565 0.20642814 0.18828996 0.14832962 0.10338301 0.06541539 0.03384785 0.012453576 0.0043557854 0.0068859216 0.012554318][-0.050269991 -0.034583528 -0.019592086 -0.013254977 -0.012262053 -0.020464674 -0.039909039 -0.0656232 -0.088004991 -0.10340162 -0.11532221 -0.12091041 -0.11863465 -0.11104221 -0.10205276][-0.10257473 -0.1086614 -0.1135302 -0.12236599 -0.13213395 -0.14462055 -0.15939625 -0.17285644 -0.18059047 -0.18295379 -0.18345384 -0.18040638 -0.17373848 -0.16554338 -0.15723915][-0.12585227 -0.14060976 -0.15149868 -0.16269791 -0.17252892 -0.1820426 -0.19030322 -0.19544762 -0.196132 -0.19371186 -0.19026335 -0.18516679 -0.1790648 -0.17330442 -0.16786276]]...]
INFO - root - 2017-12-11 03:30:41.514444: step 89210, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 18h:39m:20s remains)
INFO - root - 2017-12-11 03:30:44.243844: step 89220, loss = 0.70, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 18h:22m:22s remains)
INFO - root - 2017-12-11 03:30:46.990894: step 89230, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 18h:15m:25s remains)
INFO - root - 2017-12-11 03:30:49.743811: step 89240, loss = 0.69, batch loss = 0.63 (28.5 examples/sec; 0.281 sec/batch; 18h:59m:06s remains)
INFO - root - 2017-12-11 03:30:52.548208: step 89250, loss = 0.69, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:26m:51s remains)
INFO - root - 2017-12-11 03:30:55.304348: step 89260, loss = 0.71, batch loss = 0.65 (28.1 examples/sec; 0.284 sec/batch; 19h:12m:39s remains)
INFO - root - 2017-12-11 03:30:58.058627: step 89270, loss = 0.69, batch loss = 0.63 (28.5 examples/sec; 0.281 sec/batch; 18h:58m:00s remains)
INFO - root - 2017-12-11 03:31:00.827396: step 89280, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 17h:52m:57s remains)
INFO - root - 2017-12-11 03:31:03.616871: step 89290, loss = 0.68, batch loss = 0.63 (25.9 examples/sec; 0.309 sec/batch; 20h:52m:09s remains)
INFO - root - 2017-12-11 03:31:06.461541: step 89300, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 18h:29m:39s remains)
2017-12-11 03:31:06.979960: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.009889313 -0.01272535 -0.026505128 -0.023286683 -0.00649614 0.017682329 0.054283861 0.11725038 0.20601784 0.32076618 0.45295757 0.57431507 0.64055115 0.62577128 0.54538178][0.0044284519 -0.016347684 -0.024952924 -0.014737245 0.0091218576 0.038098712 0.073283613 0.12172779 0.18450227 0.26290163 0.35334972 0.43391708 0.46866277 0.43663949 0.3537868][-0.0026030047 -0.015112603 -0.011022366 0.013578644 0.0521692 0.094019286 0.13359088 0.16842826 0.19888572 0.22776498 0.25751555 0.27964514 0.27433687 0.22848117 0.15407734][-0.0050021745 -0.004526047 0.01863862 0.065494426 0.12818357 0.19374186 0.24880454 0.28050894 0.28657237 0.27089944 0.24079496 0.20297924 0.15693487 0.10029615 0.039708041][0.0010426483 0.016280672 0.061021093 0.13429044 0.22831587 0.32855466 0.41256467 0.45578545 0.44995156 0.40109468 0.32092106 0.22946967 0.14470974 0.078921393 0.03510689][0.016512696 0.046144105 0.11132029 0.21025071 0.3368918 0.47606272 0.59541774 0.65759361 0.64567482 0.5711813 0.45251825 0.3193984 0.20338522 0.13065252 0.10266963][0.033829547 0.074882418 0.1563773 0.27567354 0.42751145 0.59616542 0.74206948 0.81729168 0.79694194 0.70098907 0.55665344 0.39924586 0.26637822 0.1923553 0.1772617][0.037267536 0.081836991 0.17082071 0.30076647 0.46446911 0.64455819 0.79841352 0.87381119 0.8422128 0.7306906 0.57481062 0.4117153 0.27914014 0.21134141 0.20529169][0.020560334 0.058918294 0.14317188 0.26975462 0.4279443 0.59804678 0.73865992 0.80049455 0.75803965 0.64271009 0.49403188 0.34574807 0.23080832 0.17628983 0.1747909][-0.0078398064 0.01768467 0.085922457 0.19422594 0.32909968 0.46991152 0.58093071 0.62269813 0.57738364 0.47647694 0.35729882 0.24530944 0.16257356 0.12468785 0.12158646][-0.036399323 -0.026880506 0.018285066 0.09847156 0.19904618 0.30104616 0.37860614 0.40690866 0.37680426 0.31490105 0.24967323 0.19321552 0.15171011 0.12884781 0.11434788][-0.054560222 -0.062115911 -0.042218547 0.0075236782 0.072796084 0.13839759 0.19132243 0.22272955 0.23044533 0.23211448 0.24149191 0.25290257 0.25440341 0.24064417 0.20456643][-0.049576249 -0.06990651 -0.0708468 -0.046993028 -0.010792824 0.027872102 0.068808183 0.11774559 0.17742096 0.25466985 0.34432918 0.42105183 0.45621589 0.44095582 0.37616926][-0.018463731 -0.042130072 -0.053918585 -0.04647797 -0.028903581 -0.004416428 0.036461487 0.1087746 0.21766208 0.36038247 0.513298 0.63478309 0.68454 0.655687 0.56083971][0.022278657 0.0027655794 -0.010019784 -0.008827622 0.00057122804 0.021553406 0.069052249 0.16087402 0.30094728 0.47913867 0.65985721 0.79355228 0.83638954 0.78639251 0.66778386]]...]
INFO - root - 2017-12-11 03:31:09.721159: step 89310, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 18h:43m:32s remains)
INFO - root - 2017-12-11 03:31:12.448836: step 89320, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 17h:52m:18s remains)
INFO - root - 2017-12-11 03:31:15.238622: step 89330, loss = 0.70, batch loss = 0.64 (28.0 examples/sec; 0.286 sec/batch; 19h:18m:58s remains)
INFO - root - 2017-12-11 03:31:18.010725: step 89340, loss = 0.67, batch loss = 0.62 (30.0 examples/sec; 0.267 sec/batch; 18h:01m:32s remains)
INFO - root - 2017-12-11 03:31:20.786578: step 89350, loss = 0.70, batch loss = 0.65 (28.2 examples/sec; 0.284 sec/batch; 19h:11m:21s remains)
INFO - root - 2017-12-11 03:31:23.533059: step 89360, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 17h:50m:44s remains)
INFO - root - 2017-12-11 03:31:26.269130: step 89370, loss = 0.71, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 18h:17m:31s remains)
INFO - root - 2017-12-11 03:31:29.007235: step 89380, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 18h:20m:50s remains)
INFO - root - 2017-12-11 03:31:31.786162: step 89390, loss = 0.69, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 18h:35m:25s remains)
INFO - root - 2017-12-11 03:31:34.606401: step 89400, loss = 0.70, batch loss = 0.64 (28.3 examples/sec; 0.283 sec/batch; 19h:05m:59s remains)
2017-12-11 03:31:35.128536: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.062479787 0.024849636 -0.0049411957 -0.01997463 -0.025115196 -0.02576258 -0.022566956 -0.018469658 -0.015384684 -0.013831393 -0.013269166 -0.013061539 -0.013033777 -0.013206948 -0.013364445][0.083474264 0.045125827 0.011563756 -0.0075113489 -0.015809365 -0.01833068 -0.016145572 -0.011788657 -0.0072471737 -0.0036522131 -0.0016979192 -0.0013769628 -0.0027154887 -0.00496386 -0.0075249714][0.099429779 0.062887035 0.027965002 0.0060045207 -0.0047860756 -0.0083230343 -0.0063235057 -0.00055929949 0.0069771446 0.014190884 0.018383633 0.018525723 0.014641465 0.00807933 0.00083422853][0.11437026 0.078494981 0.042598266 0.018681379 0.0065740016 0.0037060911 0.00783507 0.017860627 0.031057168 0.044057596 0.051269136 0.050822154 0.042775109 0.029095003 0.014415406][0.12859719 0.092759617 0.05717795 0.033499919 0.022088643 0.021631306 0.030367246 0.047978953 0.069454379 0.089394346 0.099404149 0.09770678 0.084262818 0.061903697 0.038516376][0.13975498 0.1058861 0.074407667 0.055203341 0.04758928 0.051124554 0.066096619 0.093404345 0.12441938 0.15039116 0.16129877 0.15696838 0.13750982 0.10669532 0.075829543][0.14723863 0.11985963 0.097634077 0.087798536 0.087191455 0.095665291 0.116541 0.15236586 0.19110732 0.22009975 0.22937198 0.22132447 0.19628398 0.15890253 0.12368049][0.1509691 0.1336765 0.1238717 0.12603542 0.13392755 0.14712073 0.17138773 0.21069507 0.25165582 0.2791754 0.28543887 0.27449 0.24596815 0.20528741 0.16946326][0.14854063 0.1421663 0.14421986 0.15723354 0.17219181 0.18802285 0.21122536 0.2462292 0.28159255 0.30317122 0.30694067 0.29605916 0.26822123 0.22900124 0.19634457][0.1304459 0.13257203 0.14268573 0.16184874 0.17966112 0.19453135 0.2120588 0.23601779 0.25949684 0.27253461 0.27506226 0.2674419 0.24462143 0.21180554 0.18549445][0.085951827 0.09177085 0.10466892 0.12410563 0.14019647 0.15133707 0.16134141 0.17280945 0.18350632 0.18861453 0.19081859 0.18760182 0.17224747 0.14891502 0.13033399][0.020781338 0.025598239 0.036129538 0.050794039 0.0619111 0.068336152 0.072117224 0.074777089 0.077073522 0.077708155 0.079886936 0.079916745 0.071612433 0.057727173 0.04625446][-0.045140304 -0.044605125 -0.039390925 -0.031712171 -0.026363656 -0.023894211 -0.023595657 -0.024561394 -0.025205972 -0.025432283 -0.023252733 -0.021807076 -0.025243914 -0.031908464 -0.03777964][-0.0869198 -0.090420023 -0.089892291 -0.087916933 -0.086730026 -0.08651045 -0.087287016 -0.088426352 -0.088749945 -0.0881274 -0.085958309 -0.084291831 -0.085246921 -0.087863117 -0.090414055][-0.094747558 -0.09942621 -0.10078285 -0.10134995 -0.10179465 -0.10213997 -0.1026156 -0.10285495 -0.10236701 -0.10128669 -0.099711686 -0.0986449 -0.098949209 -0.10009081 -0.10123876]]...]
INFO - root - 2017-12-11 03:31:37.893882: step 89410, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 18h:22m:54s remains)
INFO - root - 2017-12-11 03:31:40.636604: step 89420, loss = 0.70, batch loss = 0.64 (28.5 examples/sec; 0.280 sec/batch; 18h:55m:27s remains)
INFO - root - 2017-12-11 03:31:43.346590: step 89430, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 18h:33m:50s remains)
INFO - root - 2017-12-11 03:31:46.096158: step 89440, loss = 0.68, batch loss = 0.62 (28.1 examples/sec; 0.285 sec/batch; 19h:13m:07s remains)
INFO - root - 2017-12-11 03:31:48.815496: step 89450, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 17h:55m:35s remains)
INFO - root - 2017-12-11 03:31:51.578415: step 89460, loss = 0.70, batch loss = 0.65 (29.8 examples/sec; 0.268 sec/batch; 18h:05m:47s remains)
INFO - root - 2017-12-11 03:31:54.283926: step 89470, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 17h:45m:54s remains)
INFO - root - 2017-12-11 03:31:56.973518: step 89480, loss = 0.71, batch loss = 0.65 (28.8 examples/sec; 0.278 sec/batch; 18h:46m:45s remains)
INFO - root - 2017-12-11 03:31:59.809802: step 89490, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 18h:33m:46s remains)
INFO - root - 2017-12-11 03:32:02.536200: step 89500, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 18h:17m:03s remains)
2017-12-11 03:32:03.029671: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029831974 -0.029993329 -0.027428754 -0.024296626 -0.022242498 -0.022827122 -0.023430055 -0.02072002 -0.016183475 -0.012290112 -0.012282849 -0.016734492 -0.024158729 -0.032144375 -0.037460547][0.0067546465 0.011052835 0.020453081 0.032853868 0.044223174 0.050311565 0.052985456 0.055667777 0.05726688 0.055994503 0.049600381 0.038757913 0.025787544 0.012689976 0.0020539255][0.058882147 0.069787391 0.088662229 0.11483369 0.14144135 0.16004425 0.16898063 0.16996714 0.16282645 0.14859517 0.12953635 0.10927337 0.090690389 0.073614292 0.058355361][0.10548481 0.12353768 0.15398628 0.19876029 0.24736284 0.28491992 0.30293718 0.29979271 0.27735031 0.24215578 0.20404322 0.17149903 0.1476505 0.12906669 0.11189651][0.13313287 0.15852974 0.20217739 0.26892325 0.34387282 0.40393576 0.43163851 0.42191321 0.38000292 0.31977403 0.25980964 0.21340854 0.18405622 0.16485339 0.14778602][0.13982974 0.17143384 0.22721371 0.31448343 0.4138346 0.49421495 0.529698 0.51268572 0.45208794 0.36885542 0.28912911 0.22981736 0.1948206 0.17496525 0.15919267][0.13123594 0.16424063 0.22583438 0.3252466 0.43979281 0.53312606 0.57369113 0.55192947 0.47871536 0.38013268 0.28751668 0.21979947 0.18121935 0.16181549 0.14923579][0.12148661 0.14822648 0.20579103 0.30581972 0.42467314 0.52367896 0.56829619 0.54668546 0.46942905 0.36507192 0.26791829 0.19778369 0.1588513 0.14143072 0.133021][0.12401752 0.13750026 0.18264906 0.27481863 0.39110702 0.49189335 0.54093546 0.52327579 0.44805527 0.34483868 0.24941185 0.18203779 0.14602864 0.13183162 0.12674722][0.15147865 0.14375167 0.16670254 0.24063273 0.34506392 0.44112504 0.49236891 0.48069465 0.41352874 0.31968606 0.23411033 0.17612661 0.14728263 0.13761835 0.13396096][0.19839323 0.16258958 0.15321817 0.19753425 0.27930588 0.36255404 0.41268417 0.40908217 0.35657981 0.28080055 0.21287921 0.17004059 0.15156497 0.14719515 0.14324266][0.24162771 0.17840569 0.13565958 0.14714183 0.20137897 0.26663691 0.31189871 0.31594545 0.28071523 0.22680067 0.17955346 0.1530084 0.14470786 0.14456517 0.13910703][0.25453565 0.17581318 0.1126539 0.10032552 0.13098687 0.17768939 0.21434957 0.22210382 0.20140465 0.16715 0.13783833 0.12346505 0.12080822 0.1212622 0.11310633][0.215611 0.13989438 0.076262295 0.054501772 0.068573065 0.097553238 0.1221558 0.12856773 0.11749296 0.098576635 0.082922414 0.075969882 0.074659027 0.07346455 0.064032868][0.13259394 0.074707977 0.025347894 0.0042570163 0.006208689 0.017776465 0.028102927 0.029750818 0.024391733 0.017127009 0.01232642 0.0111549 0.011079886 0.0097497562 0.0025613939]]...]
INFO - root - 2017-12-11 03:32:05.867761: step 89510, loss = 0.70, batch loss = 0.64 (27.5 examples/sec; 0.291 sec/batch; 19h:38m:12s remains)
INFO - root - 2017-12-11 03:32:08.645913: step 89520, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 18h:33m:59s remains)
INFO - root - 2017-12-11 03:32:11.418749: step 89530, loss = 0.70, batch loss = 0.64 (27.9 examples/sec; 0.287 sec/batch; 19h:22m:33s remains)
INFO - root - 2017-12-11 03:32:14.161121: step 89540, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 18h:00m:40s remains)
INFO - root - 2017-12-11 03:32:16.905534: step 89550, loss = 0.69, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 18h:37m:06s remains)
INFO - root - 2017-12-11 03:32:19.663838: step 89560, loss = 0.70, batch loss = 0.64 (28.8 examples/sec; 0.277 sec/batch; 18h:43m:00s remains)
INFO - root - 2017-12-11 03:32:22.434476: step 89570, loss = 0.71, batch loss = 0.65 (29.7 examples/sec; 0.269 sec/batch; 18h:10m:30s remains)
INFO - root - 2017-12-11 03:32:25.172740: step 89580, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 18h:26m:40s remains)
INFO - root - 2017-12-11 03:32:27.934289: step 89590, loss = 0.72, batch loss = 0.66 (29.2 examples/sec; 0.274 sec/batch; 18h:28m:58s remains)
INFO - root - 2017-12-11 03:32:30.772708: step 89600, loss = 0.68, batch loss = 0.62 (28.5 examples/sec; 0.281 sec/batch; 18h:55m:43s remains)
2017-12-11 03:32:31.353350: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.068781689 0.061473794 0.050735004 0.034233965 0.016244905 0.005827798 0.0075877048 0.01946206 0.037107836 0.05450974 0.06961748 0.082238346 0.093198061 0.10505156 0.11607666][0.06696149 0.06337139 0.055459049 0.041128859 0.027187392 0.024202771 0.035491049 0.057356317 0.083149537 0.10428599 0.11599863 0.11889869 0.11683062 0.1158074 0.11593317][0.049970463 0.053035244 0.053257696 0.047730457 0.044886369 0.054854229 0.079109758 0.11225011 0.14654174 0.1706807 0.17686909 0.16709675 0.14815557 0.12961315 0.11368778][0.035444561 0.04819046 0.060655929 0.068979181 0.082631715 0.11060029 0.15216008 0.20022786 0.24446781 0.27085319 0.26898456 0.2431941 0.20393883 0.16399018 0.12904902][0.048131861 0.073966913 0.10130797 0.12649582 0.15983884 0.20871758 0.26826423 0.327928 0.37372386 0.39179176 0.37340596 0.32748029 0.2685689 0.21074635 0.16151106][0.10252635 0.14228007 0.18357432 0.22505976 0.27618185 0.34218889 0.41196278 0.47081929 0.50286591 0.49817839 0.45504451 0.38812622 0.31506357 0.24763528 0.19204402][0.19833761 0.24719612 0.29614979 0.34725767 0.40698361 0.47714311 0.54096156 0.58195794 0.58451188 0.54589635 0.4751775 0.39272222 0.31555966 0.24930774 0.19679414][0.29268873 0.34230572 0.38968733 0.43990928 0.49474782 0.55275476 0.59477711 0.60682219 0.57602239 0.50706989 0.41828352 0.33236063 0.26212823 0.20575233 0.1622681][0.32655397 0.36869103 0.40677834 0.44731376 0.48665479 0.52150089 0.53526115 0.5203383 0.4680903 0.38716578 0.29896447 0.22345997 0.16829281 0.12669763 0.096009336][0.28044149 0.30966285 0.33525389 0.3626481 0.38351491 0.39423969 0.3842029 0.353126 0.29678336 0.22373253 0.1527167 0.098203585 0.063665994 0.040672313 0.025933748][0.18272243 0.19647361 0.20851228 0.22276774 0.22948799 0.22559315 0.20595998 0.17483348 0.13024692 0.077966154 0.031594697 0.000721962 -0.013268814 -0.018491002 -0.018692598][0.083568893 0.08484751 0.08625903 0.0907544 0.090133153 0.082088992 0.064844944 0.043575194 0.016777419 -0.012724885 -0.036410734 -0.048074573 -0.04722207 -0.040713176 -0.03196067][0.015548094 0.010831929 0.0068480144 0.0057510454 0.0023654434 -0.0042148964 -0.01416849 -0.023942417 -0.035111543 -0.046845794 -0.054548569 -0.054478839 -0.046374887 -0.035337564 -0.023618283][-0.020926619 -0.026631765 -0.031265035 -0.033715859 -0.036417145 -0.039256342 -0.041997503 -0.043623094 -0.04565575 -0.047703337 -0.0471467 -0.041943055 -0.032274626 -0.021573391 -0.01009267][-0.027845589 -0.0321543 -0.034845278 -0.03573167 -0.035799198 -0.034878831 -0.0331168 -0.031396832 -0.030642508 -0.029595317 -0.026099561 -0.019417351 -0.01057635 -0.0016405116 0.0093171373]]...]
