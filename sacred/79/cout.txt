INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "79"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-06 02:16:21.705475: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 02:16:21.705625: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 02:16:21.705631: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 02:16:21.705636: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 02:16:21.705651: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 02:16:22.281648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-06 02:16:22.281687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-06 02:16:22.281693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-06 02:16:22.281701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-06 02:16:25.288442: step 0, loss = 2.03, batch loss = 1.97 (3.5 examples/sec; 2.260 sec/batch; 208h:45m:16s remains)
2017-12-06 02:16:25.655229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3165417 -4.3153415 -4.314796 -4.3141642 -4.3113132 -4.3039975 -4.2900429 -4.2713113 -4.25656 -4.2538633 -4.2625327 -4.2808051 -4.3011446 -4.3194928 -4.3313637][-4.3114953 -4.3075914 -4.30235 -4.2969604 -4.2887878 -4.275 -4.253046 -4.2263532 -4.2134809 -4.2203565 -4.2392898 -4.2667561 -4.2947268 -4.319169 -4.3330226][-4.3027563 -4.295187 -4.2839913 -4.2722812 -4.256196 -4.2316422 -4.1952424 -4.1562815 -4.1452017 -4.1643372 -4.1995726 -4.2424855 -4.2826548 -4.3160763 -4.3343997][-4.2902803 -4.2810526 -4.2663693 -4.2488909 -4.2230053 -4.1813865 -4.1229324 -4.066967 -4.054872 -4.089293 -4.1436024 -4.2069159 -4.2638988 -4.3077106 -4.3327222][-4.2749882 -4.2666783 -4.2516747 -4.2259183 -4.1854382 -4.1215754 -4.0338831 -3.9567943 -3.9477639 -4.0051942 -4.0840859 -4.1708927 -4.2481642 -4.3032031 -4.3327951][-4.2575779 -4.2493668 -4.2325196 -4.1958432 -4.1368484 -4.0430408 -3.9157603 -3.810535 -3.8159335 -3.9090548 -4.0204978 -4.1335645 -4.23108 -4.2977266 -4.3314333][-4.2419596 -4.2314839 -4.2082763 -4.1593609 -4.0813932 -3.9566073 -3.7857375 -3.6503499 -3.68941 -3.8318477 -3.9777079 -4.110795 -4.2185869 -4.2921424 -4.3284965][-4.2287407 -4.2104125 -4.1752114 -4.1142836 -4.0222759 -3.8806558 -3.686089 -3.5524762 -3.6516094 -3.8345797 -3.9951766 -4.124598 -4.2261109 -4.2947187 -4.3284497][-4.2121239 -4.1843705 -4.1406169 -4.0791092 -3.9936924 -3.8716276 -3.7141905 -3.6407704 -3.7646925 -3.9334106 -4.0692248 -4.1688309 -4.2509346 -4.3061357 -4.3327055][-4.1932774 -4.1643009 -4.1240606 -4.0728273 -4.0113678 -3.9304459 -3.83325 -3.8116646 -3.9157712 -4.04239 -4.1404676 -4.2086048 -4.2716641 -4.3157 -4.3367929][-4.176322 -4.1495409 -4.1183157 -4.0818367 -4.0450387 -3.9998059 -3.9464703 -3.9489398 -4.0254221 -4.1137991 -4.1816707 -4.2296829 -4.2798176 -4.319355 -4.3380513][-4.1664724 -4.1460238 -4.1271873 -4.105525 -4.0858526 -4.0646629 -4.0382123 -4.0479445 -4.0997 -4.1566281 -4.2037921 -4.2408977 -4.2837663 -4.3215489 -4.3388362][-4.1713476 -4.1613016 -4.1548467 -4.1446643 -4.1361146 -4.1302257 -4.1175904 -4.1230006 -4.1509776 -4.1840143 -4.2157989 -4.2464333 -4.2850971 -4.3230934 -4.3397961][-4.199142 -4.1979728 -4.1991134 -4.1954417 -4.1934137 -4.1917348 -4.1780696 -4.1716762 -4.1789756 -4.1938186 -4.215827 -4.24488 -4.2843933 -4.3234658 -4.3398089][-4.2287269 -4.2359695 -4.2407837 -4.2395658 -4.2379036 -4.2305284 -4.2058654 -4.1851468 -4.178196 -4.1838441 -4.2058825 -4.2394767 -4.282465 -4.32269 -4.3400464]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 02:16:28.399919: step 10, loss = 2.04, batch loss = 1.98 (35.3 examples/sec; 0.227 sec/batch; 20h:57m:03s remains)
INFO - root - 2017-12-06 02:16:30.620917: step 20, loss = 2.03, batch loss = 1.98 (35.5 examples/sec; 0.225 sec/batch; 20h:47m:17s remains)
INFO - root - 2017-12-06 02:16:32.820631: step 30, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 20h:12m:10s remains)
INFO - root - 2017-12-06 02:16:35.019735: step 40, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 20h:42m:37s remains)
INFO - root - 2017-12-06 02:16:37.241234: step 50, loss = 2.08, batch loss = 2.02 (37.5 examples/sec; 0.213 sec/batch; 19h:41m:14s remains)
INFO - root - 2017-12-06 02:16:39.471535: step 60, loss = 2.08, batch loss = 2.02 (36.6 examples/sec; 0.219 sec/batch; 20h:12m:05s remains)
INFO - root - 2017-12-06 02:16:41.689960: step 70, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:44m:41s remains)
INFO - root - 2017-12-06 02:16:43.923733: step 80, loss = 2.05, batch loss = 1.99 (36.2 examples/sec; 0.221 sec/batch; 20h:24m:13s remains)
INFO - root - 2017-12-06 02:16:46.143937: step 90, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:17m:48s remains)
INFO - root - 2017-12-06 02:16:48.381291: step 100, loss = 2.07, batch loss = 2.01 (36.1 examples/sec; 0.221 sec/batch; 20h:26m:48s remains)
2017-12-06 02:16:48.667646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2196279 -4.2206645 -4.20802 -4.1863184 -4.1605144 -4.1438904 -4.1436343 -4.1406703 -4.1455154 -4.1787257 -4.2301908 -4.258718 -4.262073 -4.2529693 -4.2469][-4.2342587 -4.232573 -4.216114 -4.1869669 -4.1531086 -4.1332541 -4.1351514 -4.1337767 -4.1358104 -4.1707134 -4.229064 -4.2643437 -4.2716503 -4.2672272 -4.2632222][-4.240109 -4.2292762 -4.2070756 -4.1766019 -4.1453524 -4.1322784 -4.13694 -4.1349087 -4.1318235 -4.1580305 -4.2124205 -4.2509165 -4.2631917 -4.2656426 -4.2683372][-4.2310328 -4.2118621 -4.1777821 -4.142539 -4.11806 -4.1224227 -4.1395936 -4.1418242 -4.1412907 -4.1605587 -4.2067571 -4.2437487 -4.2594624 -4.264894 -4.270771][-4.2030792 -4.1722527 -4.1233506 -4.081181 -4.0632277 -4.0853763 -4.1214118 -4.1402035 -4.1528955 -4.1756563 -4.2122974 -4.2405467 -4.2516565 -4.2581797 -4.2665644][-4.1694741 -4.1291132 -4.0703197 -4.0198865 -4.00305 -4.0355659 -4.0928059 -4.1359372 -4.163878 -4.1909819 -4.2169175 -4.232307 -4.2361264 -4.2456684 -4.2570682][-4.1595216 -4.1178427 -4.0559573 -4.0023894 -3.9823887 -4.0169678 -4.0873518 -4.1476293 -4.1852984 -4.2114515 -4.2235107 -4.2249408 -4.2215505 -4.230576 -4.2425132][-4.1673174 -4.1389956 -4.0894766 -4.0442424 -4.0274639 -4.0546155 -4.1155481 -4.1708937 -4.2101092 -4.2344122 -4.2386127 -4.2317882 -4.2238712 -4.2265348 -4.2338023][-4.1961646 -4.1839776 -4.1518421 -4.1201558 -4.1094575 -4.1300941 -4.1734776 -4.2147484 -4.2460389 -4.2650385 -4.2649069 -4.2528548 -4.2381425 -4.229383 -4.2282014][-4.2424722 -4.23761 -4.2172742 -4.1984987 -4.1918654 -4.2039752 -4.2307873 -4.2571044 -4.2790875 -4.2918787 -4.2882485 -4.2708559 -4.2484822 -4.2293873 -4.2181025][-4.2751446 -4.273653 -4.2614636 -4.2536373 -4.2487273 -4.2531657 -4.2704148 -4.2862449 -4.2975712 -4.3036966 -4.29783 -4.2771325 -4.248632 -4.220243 -4.1987758][-4.2850027 -4.2825785 -4.2745595 -4.2718749 -4.2674394 -4.2692204 -4.2838912 -4.2959056 -4.3017206 -4.3044376 -4.29667 -4.2727275 -4.2401433 -4.2052932 -4.174561][-4.2901444 -4.2868586 -4.2808514 -4.2800055 -4.2780132 -4.2786741 -4.2898211 -4.2990174 -4.3028259 -4.3025074 -4.2906957 -4.2639961 -4.2335887 -4.200119 -4.1656423][-4.3001304 -4.2980509 -4.2957211 -4.2977595 -4.2981935 -4.29974 -4.3060613 -4.3099933 -4.3100014 -4.303885 -4.2861767 -4.2603493 -4.2332911 -4.2015557 -4.1673574][-4.3039508 -4.303894 -4.3045273 -4.3069396 -4.3087964 -4.3112049 -4.3127809 -4.3117833 -4.3080707 -4.299912 -4.283052 -4.2627726 -4.2398362 -4.2126966 -4.1862121]]...]
INFO - root - 2017-12-06 02:16:50.860572: step 110, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.221 sec/batch; 20h:26m:02s remains)
INFO - root - 2017-12-06 02:16:53.086575: step 120, loss = 2.04, batch loss = 1.98 (35.3 examples/sec; 0.227 sec/batch; 20h:56m:54s remains)
INFO - root - 2017-12-06 02:16:55.318707: step 130, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 20h:31m:12s remains)
INFO - root - 2017-12-06 02:16:57.566745: step 140, loss = 2.08, batch loss = 2.02 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:18s remains)
INFO - root - 2017-12-06 02:16:59.813036: step 150, loss = 2.09, batch loss = 2.03 (36.6 examples/sec; 0.219 sec/batch; 20h:12m:22s remains)
INFO - root - 2017-12-06 02:17:02.009584: step 160, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 20h:17m:59s remains)
INFO - root - 2017-12-06 02:17:04.237793: step 170, loss = 2.08, batch loss = 2.02 (36.8 examples/sec; 0.217 sec/batch; 20h:03m:40s remains)
INFO - root - 2017-12-06 02:17:06.474249: step 180, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:16s remains)
INFO - root - 2017-12-06 02:17:08.727401: step 190, loss = 2.08, batch loss = 2.02 (33.5 examples/sec; 0.239 sec/batch; 22h:02m:09s remains)
INFO - root - 2017-12-06 02:17:10.986638: step 200, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:58m:11s remains)
2017-12-06 02:17:11.274519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3224483 -4.3194985 -4.3173423 -4.3174982 -4.3203545 -4.3237686 -4.3246222 -4.3233032 -4.3232527 -4.32758 -4.3319659 -4.3350167 -4.3375955 -4.33855 -4.3388262][-4.2998304 -4.2936172 -4.2921381 -4.2978907 -4.3066182 -4.3113289 -4.3073196 -4.2986975 -4.2975006 -4.3050108 -4.3138857 -4.3218665 -4.3288751 -4.33209 -4.3339505][-4.264535 -4.2533779 -4.2545347 -4.2697024 -4.2851372 -4.2889462 -4.2758718 -4.257863 -4.2558331 -4.2696123 -4.2857609 -4.3022728 -4.3172736 -4.3253446 -4.3293061][-4.2324314 -4.2196956 -4.2247748 -4.2468262 -4.2655854 -4.266068 -4.2425957 -4.2116656 -4.2071652 -4.227808 -4.252315 -4.2779913 -4.302659 -4.3181233 -4.3250461][-4.2147803 -4.2011743 -4.2069674 -4.2293363 -4.2433028 -4.2345281 -4.1968632 -4.153791 -4.1526027 -4.1845222 -4.2205248 -4.2543216 -4.2882843 -4.3109913 -4.3219824][-4.1974025 -4.179141 -4.177639 -4.1904774 -4.1926551 -4.1658716 -4.1049862 -4.0551414 -4.0757895 -4.1338773 -4.188148 -4.2329736 -4.2763805 -4.3058276 -4.3206925][-4.1720362 -4.1464419 -4.1336784 -4.1318264 -4.1150551 -4.0581803 -3.9666219 -3.9226255 -3.98729 -4.081368 -4.1554995 -4.2104588 -4.2627616 -4.2991214 -4.3178196][-4.1409388 -4.1063757 -4.076961 -4.0580215 -4.0216732 -3.9436486 -3.8412607 -3.8227673 -3.9317279 -4.0522308 -4.1375465 -4.1986947 -4.2567253 -4.2979579 -4.3188171][-4.106739 -4.06988 -4.0336905 -4.0097275 -3.9767048 -3.9169612 -3.8508172 -3.8629813 -3.9676783 -4.0753174 -4.1539931 -4.2136106 -4.2691584 -4.3082 -4.3268652][-4.0913277 -4.0635557 -4.0369492 -4.0283613 -4.0205245 -3.9971368 -3.9693072 -3.9876742 -4.0611191 -4.135746 -4.1958947 -4.2448096 -4.2887087 -4.3202596 -4.3348494][-4.1087232 -4.0950942 -4.0878596 -4.0993714 -4.1124153 -4.1102648 -4.098578 -4.1113691 -4.155056 -4.199255 -4.2386179 -4.2736988 -4.3066273 -4.33024 -4.340271][-4.1546011 -4.152781 -4.1620226 -4.1821842 -4.1990333 -4.2018228 -4.1965456 -4.2042289 -4.227324 -4.2520938 -4.276073 -4.2999306 -4.3227663 -4.338469 -4.3439536][-4.2124891 -4.2165222 -4.2300634 -4.24686 -4.2584496 -4.2601504 -4.2575722 -4.2613196 -4.27233 -4.2866321 -4.3024249 -4.3185673 -4.333158 -4.3423257 -4.3451924][-4.267909 -4.2733736 -4.2832818 -4.292079 -4.2970557 -4.2976594 -4.2959719 -4.2963047 -4.2998967 -4.3058929 -4.3146472 -4.3261781 -4.3367596 -4.3422189 -4.3424597][-4.3061428 -4.309464 -4.3136792 -4.3165474 -4.3171382 -4.3165379 -4.3161149 -4.3156018 -4.3143339 -4.3154116 -4.3211179 -4.3295789 -4.3366561 -4.3389134 -4.3378234]]...]
INFO - root - 2017-12-06 02:17:13.499453: step 210, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:50s remains)
INFO - root - 2017-12-06 02:17:15.748547: step 220, loss = 2.07, batch loss = 2.01 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:54s remains)
INFO - root - 2017-12-06 02:17:17.964064: step 230, loss = 2.05, batch loss = 1.99 (36.2 examples/sec; 0.221 sec/batch; 20h:22m:32s remains)
INFO - root - 2017-12-06 02:17:20.209111: step 240, loss = 2.04, batch loss = 1.98 (35.7 examples/sec; 0.224 sec/batch; 20h:42m:05s remains)
INFO - root - 2017-12-06 02:17:22.472576: step 250, loss = 2.06, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 21h:23m:18s remains)
INFO - root - 2017-12-06 02:17:24.797934: step 260, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:31s remains)
INFO - root - 2017-12-06 02:17:27.085686: step 270, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 20h:50m:48s remains)
INFO - root - 2017-12-06 02:17:29.333626: step 280, loss = 2.05, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:41m:12s remains)
INFO - root - 2017-12-06 02:17:31.563978: step 290, loss = 2.08, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 20h:00m:54s remains)
INFO - root - 2017-12-06 02:17:33.798605: step 300, loss = 2.07, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 20h:41m:57s remains)
2017-12-06 02:17:34.116341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1559868 -4.1382174 -4.1193275 -4.12356 -4.1376667 -4.1439228 -4.1527019 -4.159133 -4.1553822 -4.1553097 -4.162951 -4.1675525 -4.1558185 -4.1251006 -4.1027479][-4.1547775 -4.1481194 -4.1379304 -4.1388407 -4.1413488 -4.1383185 -4.1398187 -4.1448126 -4.1470294 -4.1509185 -4.1600275 -4.1676264 -4.159812 -4.1339707 -4.1204085][-4.1447544 -4.1522522 -4.1585164 -4.166667 -4.1639991 -4.1506057 -4.1426706 -4.145442 -4.1495342 -4.1530957 -4.1623559 -4.1708212 -4.1647472 -4.1483674 -4.1457925][-4.1266007 -4.1440592 -4.1629233 -4.17903 -4.1809659 -4.1659946 -4.1522131 -4.1471572 -4.1435919 -4.1429071 -4.1510305 -4.164052 -4.1672115 -4.1676207 -4.17366][-4.1186433 -4.1349959 -4.1572952 -4.1755605 -4.1790481 -4.1640143 -4.1476531 -4.1327586 -4.1151943 -4.1028709 -4.1092496 -4.1312642 -4.1536756 -4.1720347 -4.1866336][-4.1357784 -4.1391511 -4.1551328 -4.1699014 -4.1673565 -4.1427627 -4.1171732 -4.0922828 -4.0660057 -4.0454097 -4.0505219 -4.0796928 -4.1182837 -4.1546535 -4.1819725][-4.171185 -4.1603241 -4.1639566 -4.1707788 -4.1623659 -4.1279263 -4.0884252 -4.0517907 -4.020957 -3.9965239 -3.9946055 -4.0229549 -4.0691605 -4.1215453 -4.1671977][-4.2065868 -4.1928854 -4.1888351 -4.1912365 -4.1803474 -4.1418591 -4.094099 -4.052084 -4.0156951 -3.9842813 -3.9683058 -3.9789882 -4.0181088 -4.08391 -4.14429][-4.2335415 -4.22195 -4.2170343 -4.219985 -4.2120233 -4.1806579 -4.1371737 -4.0958033 -4.054028 -4.0146151 -3.9863005 -3.9755318 -3.996846 -4.0600543 -4.1245484][-4.2542071 -4.2461677 -4.24166 -4.2426772 -4.2361994 -4.2159634 -4.1854081 -4.1526628 -4.1128492 -4.0731425 -4.0375729 -4.0064316 -4.0057292 -4.0537071 -4.1094055][-4.2668915 -4.266428 -4.2645369 -4.2626314 -4.2570829 -4.24446 -4.2229652 -4.1991944 -4.1660357 -4.1318216 -4.0950947 -4.0538898 -4.0359058 -4.061337 -4.1000066][-4.2623487 -4.2697735 -4.2727981 -4.2726851 -4.2738647 -4.2697515 -4.2569633 -4.2364283 -4.2077947 -4.1779947 -4.14095 -4.0977559 -4.0709553 -4.0768104 -4.0981665][-4.2467961 -4.2597246 -4.2662349 -4.2689624 -4.2774353 -4.2842889 -4.2840395 -4.2711425 -4.2488685 -4.2212663 -4.1828694 -4.1432843 -4.1150794 -4.1070633 -4.1108818][-4.240499 -4.2546535 -4.2605324 -4.2630267 -4.2740364 -4.2864814 -4.2971478 -4.2938452 -4.2798915 -4.2576284 -4.2219558 -4.1888137 -4.1611753 -4.141489 -4.1265149][-4.2475729 -4.2592573 -4.2616591 -4.2603388 -4.2683153 -4.2809811 -4.2963395 -4.3001542 -4.2934465 -4.280663 -4.254292 -4.2277484 -4.2027903 -4.1729293 -4.1389852]]...]
INFO - root - 2017-12-06 02:17:36.344371: step 310, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 20h:25m:06s remains)
INFO - root - 2017-12-06 02:17:38.579206: step 320, loss = 2.04, batch loss = 1.99 (34.7 examples/sec; 0.230 sec/batch; 21h:14m:54s remains)
INFO - root - 2017-12-06 02:17:40.840346: step 330, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 21h:00m:38s remains)
INFO - root - 2017-12-06 02:17:43.104988: step 340, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:05s remains)
INFO - root - 2017-12-06 02:17:45.337272: step 350, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.224 sec/batch; 20h:37m:18s remains)
INFO - root - 2017-12-06 02:17:47.569850: step 360, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.222 sec/batch; 20h:26m:26s remains)
INFO - root - 2017-12-06 02:17:49.839689: step 370, loss = 2.07, batch loss = 2.01 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:23s remains)
INFO - root - 2017-12-06 02:17:52.080424: step 380, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 20h:26m:35s remains)
INFO - root - 2017-12-06 02:17:54.343613: step 390, loss = 2.05, batch loss = 1.99 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:46s remains)
INFO - root - 2017-12-06 02:17:56.582923: step 400, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.232 sec/batch; 21h:26m:11s remains)
2017-12-06 02:17:56.880292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.245378 -4.2398171 -4.24596 -4.2495627 -4.2696056 -4.2794108 -4.2758465 -4.2726469 -4.2661929 -4.2598896 -4.2578144 -4.25758 -4.2673664 -4.2820654 -4.2849231][-4.2338085 -4.2277846 -4.2356248 -4.2426591 -4.2658014 -4.27444 -4.26757 -4.2600832 -4.2483439 -4.241518 -4.2369161 -4.2351756 -4.2438459 -4.2584805 -4.2637534][-4.2284527 -4.2176633 -4.22397 -4.23395 -4.2600908 -4.2699537 -4.2622929 -4.2510991 -4.2359891 -4.2285914 -4.2249479 -4.2258835 -4.2323513 -4.2414169 -4.2465734][-4.2126093 -4.1874685 -4.1817083 -4.1917386 -4.2212148 -4.2366271 -4.2325115 -4.2229505 -4.206461 -4.1996646 -4.20073 -4.2075605 -4.2147789 -4.2213321 -4.2273993][-4.1974792 -4.1535735 -4.1264458 -4.1260414 -4.151257 -4.17013 -4.1748981 -4.1742558 -4.1665998 -4.1670532 -4.174469 -4.1876192 -4.1994557 -4.2066612 -4.2154069][-4.2022147 -4.1523447 -4.110003 -4.094336 -4.1049604 -4.1126795 -4.1147423 -4.1191969 -4.1257691 -4.1394939 -4.1554503 -4.175 -4.191534 -4.202301 -4.2153249][-4.219089 -4.17837 -4.1381054 -4.1179996 -4.1227789 -4.1160712 -4.0969033 -4.088182 -4.0962157 -4.1168513 -4.1427922 -4.1707907 -4.1920753 -4.2085519 -4.2275314][-4.2228165 -4.1965361 -4.1700325 -4.1630011 -4.1792517 -4.1737423 -4.1410847 -4.1163549 -4.1116886 -4.1210365 -4.1407108 -4.1672621 -4.1918125 -4.2158308 -4.2419238][-4.200016 -4.1799316 -4.1648259 -4.17476 -4.2138367 -4.2283654 -4.2058692 -4.1812859 -4.1634164 -4.153976 -4.156672 -4.1703038 -4.1915722 -4.2187591 -4.2514739][-4.1602798 -4.1389604 -4.1305752 -4.1526704 -4.2081046 -4.2433934 -4.2429709 -4.2323465 -4.2154369 -4.1972179 -4.1857815 -4.1848707 -4.1990385 -4.2240477 -4.2583041][-4.1370926 -4.1051722 -4.0908718 -4.113945 -4.1758823 -4.2236423 -4.24017 -4.2440305 -4.2374687 -4.2217264 -4.21021 -4.20477 -4.2127442 -4.2324519 -4.2636628][-4.1529808 -4.1169281 -4.0959816 -4.1129017 -4.1696653 -4.2137566 -4.2338934 -4.2419147 -4.2398052 -4.2294369 -4.2226653 -4.2201662 -4.227416 -4.2421207 -4.2667017][-4.1883259 -4.158834 -4.1381035 -4.1497049 -4.1929693 -4.2228889 -4.2366123 -4.2433267 -4.2423186 -4.2351332 -4.2312908 -4.2320228 -4.2390308 -4.2509956 -4.2696576][-4.2221742 -4.2012129 -4.1857152 -4.1931252 -4.2228622 -4.2414107 -4.2483163 -4.2524028 -4.25141 -4.2455149 -4.241097 -4.2409306 -4.2462687 -4.256135 -4.2717938][-4.2465968 -4.2329531 -4.2225 -4.2268424 -4.2456422 -4.2560287 -4.2601795 -4.2633357 -4.2632494 -4.2602563 -4.2565269 -4.2543755 -4.2570281 -4.2644753 -4.278194]]...]
INFO - root - 2017-12-06 02:17:59.092681: step 410, loss = 2.08, batch loss = 2.02 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:40s remains)
INFO - root - 2017-12-06 02:18:01.339428: step 420, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 20h:50m:08s remains)
INFO - root - 2017-12-06 02:18:03.632336: step 430, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 21h:01m:13s remains)
INFO - root - 2017-12-06 02:18:05.868728: step 440, loss = 2.06, batch loss = 2.00 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:56s remains)
INFO - root - 2017-12-06 02:18:08.111115: step 450, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:20s remains)
INFO - root - 2017-12-06 02:18:10.398547: step 460, loss = 2.06, batch loss = 2.01 (37.8 examples/sec; 0.211 sec/batch; 19h:30m:23s remains)
INFO - root - 2017-12-06 02:18:12.656744: step 470, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 20h:35m:01s remains)
INFO - root - 2017-12-06 02:18:14.926353: step 480, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.226 sec/batch; 20h:48m:11s remains)
INFO - root - 2017-12-06 02:18:17.216326: step 490, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:53s remains)
INFO - root - 2017-12-06 02:18:19.482439: step 500, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 21h:32m:44s remains)
2017-12-06 02:18:19.778414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1711326 -4.207922 -4.2497296 -4.2839017 -4.3014455 -4.2874756 -4.2529087 -4.22637 -4.2085624 -4.1995845 -4.2016935 -4.2199297 -4.242527 -4.2643137 -4.264369][-4.1442575 -4.1943755 -4.2466049 -4.2871008 -4.3084087 -4.2984591 -4.2622738 -4.2277 -4.2043629 -4.1925921 -4.1940928 -4.2092352 -4.2272511 -4.2487016 -4.252738][-4.1304789 -4.193861 -4.2563529 -4.2959485 -4.3158674 -4.3070097 -4.2692828 -4.228354 -4.2004204 -4.1851892 -4.185092 -4.1985068 -4.2174339 -4.2397051 -4.2489853][-4.14085 -4.2103572 -4.2718191 -4.3010421 -4.3135386 -4.3015094 -4.2597179 -4.212698 -4.1768155 -4.1599126 -4.1671414 -4.1918497 -4.2194486 -4.2428141 -4.2567973][-4.1708379 -4.233448 -4.28099 -4.2983437 -4.3019376 -4.2833409 -4.23391 -4.17589 -4.1289158 -4.1204948 -4.1477928 -4.1923189 -4.2289739 -4.2533007 -4.27121][-4.203845 -4.2474375 -4.2797713 -4.2901163 -4.2848973 -4.2594109 -4.1974945 -4.1171141 -4.0643711 -4.0772772 -4.1299129 -4.1915455 -4.2324724 -4.2524824 -4.2710481][-4.2173753 -4.2397032 -4.260262 -4.2651777 -4.2548022 -4.2190332 -4.1322026 -4.0161591 -3.9718246 -4.0234942 -4.1058154 -4.1833358 -4.2280979 -4.2457056 -4.2631445][-4.2087426 -4.215107 -4.2258797 -4.2272863 -4.2095194 -4.1581168 -4.0444098 -3.9039772 -3.8921444 -3.989989 -4.0945497 -4.1776433 -4.2229986 -4.2418132 -4.2584333][-4.1830587 -4.1808972 -4.184514 -4.1846023 -4.1640482 -4.1079674 -3.9973173 -3.8833776 -3.9111381 -4.0196981 -4.1171808 -4.1871462 -4.2257724 -4.2450871 -4.2598686][-4.1479974 -4.1448069 -4.1473188 -4.1520109 -4.1336946 -4.0840263 -4.00177 -3.9418523 -3.9890506 -4.0776134 -4.1542406 -4.2086864 -4.23781 -4.2532668 -4.2636685][-4.1236014 -4.1189404 -4.1179953 -4.1262813 -4.1151457 -4.0823164 -4.0347958 -4.0148044 -4.0638132 -4.1281037 -4.1854858 -4.2294464 -4.2538624 -4.2676067 -4.2735052][-4.1091142 -4.1010633 -4.0973215 -4.1075087 -4.1075435 -4.0946536 -4.0750322 -4.0739155 -4.1118426 -4.1562343 -4.2015634 -4.2400584 -4.2630954 -4.2772908 -4.282743][-4.10254 -4.1000051 -4.1009502 -4.1159596 -4.1278973 -4.1298108 -4.12502 -4.128541 -4.151206 -4.181344 -4.2196302 -4.2530437 -4.2738423 -4.2865324 -4.2908554][-4.138082 -4.1401992 -4.1460876 -4.162415 -4.1789341 -4.1870747 -4.1874757 -4.1897311 -4.20152 -4.2220063 -4.2514553 -4.276998 -4.2920895 -4.3002586 -4.3018179][-4.2111144 -4.2131557 -4.2182231 -4.2314363 -4.242661 -4.2479906 -4.2476463 -4.2464709 -4.2521043 -4.2649565 -4.2852821 -4.3032136 -4.3123064 -4.3155346 -4.31382]]...]
INFO - root - 2017-12-06 02:18:22.055479: step 510, loss = 2.07, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 20h:52m:05s remains)
INFO - root - 2017-12-06 02:18:24.326357: step 520, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.226 sec/batch; 20h:48m:05s remains)
INFO - root - 2017-12-06 02:18:26.560957: step 530, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 21h:02m:31s remains)
INFO - root - 2017-12-06 02:18:28.815044: step 540, loss = 2.04, batch loss = 1.98 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:50s remains)
INFO - root - 2017-12-06 02:18:31.063044: step 550, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:43s remains)
INFO - root - 2017-12-06 02:18:33.326352: step 560, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 20h:37m:12s remains)
INFO - root - 2017-12-06 02:18:35.650205: step 570, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.226 sec/batch; 20h:52m:52s remains)
INFO - root - 2017-12-06 02:18:37.947179: step 580, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.226 sec/batch; 20h:52m:33s remains)
INFO - root - 2017-12-06 02:18:40.253178: step 590, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 21h:12m:53s remains)
INFO - root - 2017-12-06 02:18:42.531445: step 600, loss = 2.04, batch loss = 1.98 (35.5 examples/sec; 0.225 sec/batch; 20h:46m:16s remains)
2017-12-06 02:18:42.823915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0477676 -4.0320311 -4.0397878 -4.0716414 -4.1042981 -4.1131253 -4.1023273 -4.0928583 -4.1089406 -4.1475296 -4.1912608 -4.2307458 -4.266541 -4.2944417 -4.3186722][-4.0173349 -3.9946837 -4.0018911 -4.0379281 -4.0824881 -4.1049447 -4.0990262 -4.0865178 -4.0961361 -4.1351066 -4.1849847 -4.2251935 -4.2643127 -4.2975235 -4.3227258][-4.0113945 -3.9880898 -4.0009193 -4.0454874 -4.0957994 -4.1228619 -4.116703 -4.1039591 -4.11084 -4.1459041 -4.1949325 -4.2278509 -4.2607689 -4.2922511 -4.3139658][-4.02915 -4.010128 -4.0269017 -4.0715594 -4.1171913 -4.142724 -4.139204 -4.1324034 -4.1427927 -4.1729751 -4.2103462 -4.2312331 -4.2553539 -4.2807226 -4.2951703][-4.05751 -4.0420184 -4.0543385 -4.0828514 -4.1145754 -4.1404104 -4.1480794 -4.1529784 -4.1668968 -4.1889219 -4.2136784 -4.2287893 -4.2476573 -4.2675514 -4.2772856][-4.06778 -4.0436692 -4.0387187 -4.043746 -4.0643229 -4.0986657 -4.1238441 -4.1410708 -4.1582227 -4.1805954 -4.2037716 -4.2218661 -4.2422957 -4.2579112 -4.26326][-4.0565143 -4.0065441 -3.969861 -3.9519467 -3.9671793 -4.0127263 -4.0548244 -4.087873 -4.1171231 -4.1524129 -4.1866775 -4.2149963 -4.2392278 -4.2527452 -4.2580953][-4.0513687 -3.9816079 -3.917336 -3.8780153 -3.8873591 -3.9344902 -3.9835448 -4.0283003 -4.0716562 -4.1178737 -4.1593871 -4.1962028 -4.2254658 -4.2428327 -4.2540627][-4.0763617 -4.0162482 -3.9563773 -3.9151158 -3.9183133 -3.9565501 -3.9972119 -4.0344687 -4.0743475 -4.1164703 -4.1548786 -4.19217 -4.2212558 -4.2425766 -4.2602024][-4.1234446 -4.0887485 -4.05363 -4.0261555 -4.0307622 -4.0589709 -4.0851073 -4.1044297 -4.1280546 -4.1559205 -4.1830773 -4.2143884 -4.2393441 -4.2588291 -4.2754345][-4.1662784 -4.1450238 -4.1275911 -4.1133518 -4.1240492 -4.1527863 -4.1740274 -4.1811886 -4.1917648 -4.2073565 -4.2247605 -4.2457986 -4.2636642 -4.2759781 -4.2864122][-4.1922641 -4.1751151 -4.1645527 -4.1587963 -4.174861 -4.2040734 -4.2244153 -4.2284064 -4.2339582 -4.242805 -4.2535319 -4.2677517 -4.2802205 -4.2865577 -4.2896714][-4.2132468 -4.1985903 -4.188354 -4.1848021 -4.1983814 -4.2203074 -4.2346163 -4.2360916 -4.2390394 -4.2451453 -4.2548227 -4.2697616 -4.2828774 -4.2883286 -4.2895308][-4.2363148 -4.224061 -4.2110405 -4.2046652 -4.2099576 -4.22069 -4.2269716 -4.2257328 -4.2274213 -4.2332568 -4.2428241 -4.2614417 -4.2786012 -4.2857757 -4.2869992][-4.258935 -4.2495537 -4.2362347 -4.2300234 -4.2308307 -4.2359347 -4.2387009 -4.2361326 -4.23638 -4.2410564 -4.2493792 -4.266715 -4.2822018 -4.287828 -4.2875876]]...]
INFO - root - 2017-12-06 02:18:45.427855: step 610, loss = 2.08, batch loss = 2.02 (23.5 examples/sec; 0.341 sec/batch; 31h:25m:28s remains)
INFO - root - 2017-12-06 02:18:49.863523: step 620, loss = 2.05, batch loss = 1.99 (16.7 examples/sec; 0.478 sec/batch; 44h:06m:25s remains)
INFO - root - 2017-12-06 02:18:54.445712: step 630, loss = 2.05, batch loss = 2.00 (17.1 examples/sec; 0.468 sec/batch; 43h:08m:20s remains)
INFO - root - 2017-12-06 02:18:59.111862: step 640, loss = 2.08, batch loss = 2.03 (17.5 examples/sec; 0.458 sec/batch; 42h:11m:01s remains)
INFO - root - 2017-12-06 02:19:03.700017: step 650, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.435 sec/batch; 40h:07m:05s remains)
INFO - root - 2017-12-06 02:19:08.365650: step 660, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 0.482 sec/batch; 44h:26m:05s remains)
INFO - root - 2017-12-06 02:19:13.061117: step 670, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.451 sec/batch; 41h:33m:40s remains)
INFO - root - 2017-12-06 02:19:17.753973: step 680, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 0.489 sec/batch; 45h:05m:51s remains)
INFO - root - 2017-12-06 02:19:22.403680: step 690, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.466 sec/batch; 42h:55m:51s remains)
INFO - root - 2017-12-06 02:19:27.095095: step 700, loss = 2.05, batch loss = 1.99 (16.8 examples/sec; 0.475 sec/batch; 43h:48m:48s remains)
2017-12-06 02:19:27.605545: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1373725 -4.1465497 -4.147965 -4.1520066 -4.1569424 -4.1743755 -4.192872 -4.19801 -4.1966748 -4.1873088 -4.1813536 -4.1682477 -4.1477313 -4.1341419 -4.1271811][-4.1636829 -4.1696873 -4.1663675 -4.1627865 -4.1604528 -4.1759582 -4.2012281 -4.2172217 -4.2245927 -4.2173657 -4.2027388 -4.1703873 -4.1330543 -4.1102562 -4.1084366][-4.2160115 -4.2160492 -4.2065849 -4.1937814 -4.1818514 -4.1865997 -4.20655 -4.2289982 -4.2463918 -4.2453704 -4.2244077 -4.1770754 -4.1247463 -4.0939522 -4.0955467][-4.2564855 -4.252306 -4.2393718 -4.2203779 -4.1992531 -4.1827478 -4.1801019 -4.1998439 -4.2322388 -4.2455859 -4.2310991 -4.1822538 -4.1177478 -4.0768294 -4.0808568][-4.2619996 -4.2545528 -4.2355385 -4.2141552 -4.1833577 -4.1438584 -4.1155038 -4.1268029 -4.1710644 -4.2081003 -4.2151146 -4.1789823 -4.1137629 -4.0665483 -4.0660152][-4.2269673 -4.2115035 -4.1860528 -4.1596456 -4.1114326 -4.0465097 -3.9955435 -3.9959831 -4.0518579 -4.1226521 -4.1611171 -4.1456852 -4.0935893 -4.046988 -4.0371909][-4.18161 -4.1613083 -4.1287284 -4.0893 -4.0232196 -3.9311044 -3.8454602 -3.812917 -3.8746505 -3.9880412 -4.0719929 -4.0897822 -4.060946 -4.0244842 -4.0083227][-4.1562757 -4.1413474 -4.1102257 -4.0628562 -3.9899755 -3.8841133 -3.7647064 -3.6824255 -3.7334852 -3.8817987 -4.0102954 -4.0672493 -4.0651417 -4.0409169 -4.0153446][-4.1423616 -4.1353645 -4.11696 -4.0781455 -4.0157757 -3.9268584 -3.8202524 -3.7423713 -3.7711346 -3.8984532 -4.026855 -4.0940876 -4.1034951 -4.0876746 -4.0623207][-4.1238132 -4.1216841 -4.1152115 -4.0940018 -4.0542207 -4.002512 -3.9407954 -3.9005718 -3.9173794 -3.9911585 -4.0763025 -4.1223545 -4.1231222 -4.1073284 -4.0887971][-4.119319 -4.1163235 -4.1131363 -4.1022553 -4.0827641 -4.0682588 -4.0495186 -4.0364604 -4.0455408 -4.0763249 -4.1133742 -4.1274881 -4.1127505 -4.093977 -4.0819426][-4.1536651 -4.1469235 -4.137682 -4.1247025 -4.1166 -4.1252742 -4.1371436 -4.144712 -4.1501122 -4.1530824 -4.1550341 -4.1455688 -4.1192832 -4.0944166 -4.0837932][-4.2037859 -4.2003331 -4.1893506 -4.1715446 -4.1627412 -4.1775894 -4.2018609 -4.2209072 -4.2251873 -4.212141 -4.1932907 -4.1720719 -4.1447077 -4.1190147 -4.1076117][-4.2357426 -4.2379117 -4.2306662 -4.2120528 -4.2009406 -4.2150841 -4.2387457 -4.2577472 -4.2571464 -4.2342372 -4.2059364 -4.1823893 -4.1604595 -4.1410823 -4.1390762][-4.2492766 -4.2543936 -4.25155 -4.235043 -4.2240558 -4.2337265 -4.2492518 -4.260962 -4.2526283 -4.2265425 -4.2028947 -4.1867085 -4.1727338 -4.162106 -4.1714435]]...]
INFO - root - 2017-12-06 02:19:32.325800: step 710, loss = 2.07, batch loss = 2.01 (16.3 examples/sec; 0.490 sec/batch; 45h:08m:25s remains)
INFO - root - 2017-12-06 02:19:36.874076: step 720, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.472 sec/batch; 43h:28m:06s remains)
INFO - root - 2017-12-06 02:19:41.489239: step 730, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.455 sec/batch; 41h:55m:36s remains)
INFO - root - 2017-12-06 02:19:46.173836: step 740, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.476 sec/batch; 43h:52m:37s remains)
INFO - root - 2017-12-06 02:19:50.837388: step 750, loss = 2.06, batch loss = 2.00 (17.6 examples/sec; 0.455 sec/batch; 41h:57m:27s remains)
INFO - root - 2017-12-06 02:19:55.501448: step 760, loss = 2.09, batch loss = 2.03 (17.0 examples/sec; 0.470 sec/batch; 43h:19m:21s remains)
INFO - root - 2017-12-06 02:20:00.234270: step 770, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.458 sec/batch; 42h:09m:57s remains)
INFO - root - 2017-12-06 02:20:04.934421: step 780, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.464 sec/batch; 42h:43m:41s remains)
INFO - root - 2017-12-06 02:20:09.689005: step 790, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.480 sec/batch; 44h:14m:22s remains)
INFO - root - 2017-12-06 02:20:14.397321: step 800, loss = 2.09, batch loss = 2.03 (17.3 examples/sec; 0.463 sec/batch; 42h:40m:37s remains)
2017-12-06 02:20:14.886052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.16927 -4.1792374 -4.1851096 -4.1898785 -4.2087994 -4.2163582 -4.1998439 -4.1709166 -4.1400962 -4.12081 -4.1062236 -4.1034665 -4.1045709 -4.1064329 -4.1110444][-4.2015753 -4.2139688 -4.2258077 -4.235302 -4.2406416 -4.2296724 -4.1909165 -4.1424847 -4.10054 -4.0802708 -4.0718017 -4.0739689 -4.080987 -4.0838437 -4.0862217][-4.2029142 -4.2109909 -4.2236676 -4.2347908 -4.2298574 -4.206244 -4.1591163 -4.1144161 -4.08471 -4.0749846 -4.078753 -4.0826588 -4.0891495 -4.0894008 -4.085906][-4.1985488 -4.2020679 -4.2098913 -4.2167563 -4.2009997 -4.1710296 -4.1300235 -4.1001697 -4.0838757 -4.0803704 -4.0860538 -4.0875087 -4.08996 -4.0868444 -4.0792966][-4.1968622 -4.1949534 -4.1966419 -4.1990485 -4.1785264 -4.147059 -4.1143417 -4.0911088 -4.0781393 -4.0753732 -4.0808196 -4.0864744 -4.0958576 -4.0995693 -4.0939641][-4.1908307 -4.1889081 -4.1881022 -4.1901121 -4.1703863 -4.139854 -4.1062012 -4.0802217 -4.0771265 -4.0899739 -4.1044641 -4.1166763 -4.1338434 -4.1389875 -4.129108][-4.1959815 -4.1947408 -4.194128 -4.1941037 -4.1705508 -4.1310983 -4.0773058 -4.03403 -4.0446835 -4.0893006 -4.1304483 -4.1551032 -4.1714439 -4.1664467 -4.1445017][-4.1996703 -4.2008815 -4.1974449 -4.1875463 -4.151535 -4.0831237 -3.9824972 -3.9070914 -3.9426277 -4.0271883 -4.1062212 -4.1528974 -4.1722474 -4.1630354 -4.1386971][-4.1995597 -4.2029009 -4.1939692 -4.1747141 -4.12588 -4.0346761 -3.9026761 -3.8137658 -3.8806458 -3.9958441 -4.0940208 -4.1443286 -4.1594925 -4.1524768 -4.1324081][-4.1993642 -4.2050457 -4.1958208 -4.17766 -4.1372581 -4.0680861 -3.9732385 -3.9208224 -3.979336 -4.0672646 -4.1382418 -4.1674552 -4.1716795 -4.1627512 -4.143507][-4.2124524 -4.2175293 -4.209538 -4.1959524 -4.1760345 -4.1406555 -4.0915952 -4.0680442 -4.1057315 -4.1564965 -4.1946135 -4.206089 -4.2052689 -4.1964731 -4.1772995][-4.2240767 -4.22576 -4.218976 -4.2137146 -4.211319 -4.1999111 -4.1785345 -4.1654639 -4.1839218 -4.211967 -4.2292347 -4.233798 -4.2335825 -4.2291961 -4.2131462][-4.2170105 -4.2227292 -4.2256045 -4.2295485 -4.2374148 -4.2381721 -4.2314792 -4.2258716 -4.2341371 -4.2482233 -4.2528787 -4.2563982 -4.2579961 -4.2562346 -4.2437458][-4.1952581 -4.2082 -4.2244582 -4.2376375 -4.251976 -4.2605081 -4.2601128 -4.2594481 -4.2652335 -4.273037 -4.2735643 -4.2772927 -4.278914 -4.274992 -4.2615991][-4.1626539 -4.1763949 -4.1986971 -4.2181063 -4.2423897 -4.2613621 -4.2680435 -4.2681813 -4.2707238 -4.2755089 -4.2743268 -4.2777138 -4.2793226 -4.274056 -4.2624154]]...]
INFO - root - 2017-12-06 02:20:19.403572: step 810, loss = 2.05, batch loss = 1.99 (16.6 examples/sec; 0.483 sec/batch; 44h:30m:54s remains)
INFO - root - 2017-12-06 02:20:24.210727: step 820, loss = 2.02, batch loss = 1.97 (17.7 examples/sec; 0.452 sec/batch; 41h:39m:24s remains)
INFO - root - 2017-12-06 02:20:28.847948: step 830, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.460 sec/batch; 42h:22m:22s remains)
INFO - root - 2017-12-06 02:20:33.564454: step 840, loss = 2.04, batch loss = 1.98 (17.1 examples/sec; 0.469 sec/batch; 43h:12m:36s remains)
INFO - root - 2017-12-06 02:20:38.240802: step 850, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.459 sec/batch; 42h:18m:57s remains)
INFO - root - 2017-12-06 02:20:42.952633: step 860, loss = 2.09, batch loss = 2.03 (17.1 examples/sec; 0.468 sec/batch; 43h:08m:24s remains)
INFO - root - 2017-12-06 02:20:47.630865: step 870, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.440 sec/batch; 40h:32m:58s remains)
INFO - root - 2017-12-06 02:20:52.356692: step 880, loss = 2.08, batch loss = 2.02 (16.9 examples/sec; 0.474 sec/batch; 43h:39m:19s remains)
INFO - root - 2017-12-06 02:20:57.051773: step 890, loss = 2.09, batch loss = 2.03 (16.5 examples/sec; 0.485 sec/batch; 44h:38m:17s remains)
INFO - root - 2017-12-06 02:21:01.697624: step 900, loss = 2.04, batch loss = 1.99 (16.8 examples/sec; 0.476 sec/batch; 43h:52m:08s remains)
2017-12-06 02:21:02.163414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1185894 -4.0947123 -4.0817523 -4.0786572 -4.0905023 -4.1071377 -4.1302009 -4.1483765 -4.160368 -4.1624575 -4.16498 -4.1603193 -4.1474843 -4.1193209 -4.1102638][-4.0989504 -4.0898938 -4.0834384 -4.0774245 -4.0851808 -4.099719 -4.1190171 -4.1425881 -4.16153 -4.1671033 -4.1627889 -4.14939 -4.133112 -4.1064606 -4.0981603][-4.0715289 -4.0931716 -4.1011243 -4.0924282 -4.0933428 -4.1058249 -4.1195846 -4.1367416 -4.1545157 -4.1648588 -4.1580687 -4.1408563 -4.119628 -4.0989008 -4.0962815][-4.0480952 -4.1062355 -4.1302919 -4.1257248 -4.1127286 -4.1145344 -4.1186218 -4.1249752 -4.137548 -4.1524191 -4.1540818 -4.1438603 -4.125895 -4.1128387 -4.1157107][-4.0725374 -4.1481495 -4.1753497 -4.1652131 -4.1333709 -4.1110015 -4.0951095 -4.0951562 -4.110147 -4.130024 -4.1409049 -4.1451869 -4.1358213 -4.124157 -4.1307325][-4.1252332 -4.1807189 -4.1926346 -4.1662636 -4.12159 -4.0763874 -4.0372772 -4.037426 -4.0658355 -4.0980897 -4.1210289 -4.137969 -4.1281853 -4.1081591 -4.1143012][-4.1606231 -4.181262 -4.1707182 -4.1243572 -4.0671487 -4.0020452 -3.9477365 -3.9607008 -4.015173 -4.064703 -4.0958586 -4.1168804 -4.1019793 -4.075129 -4.080667][-4.1812429 -4.1654968 -4.12219 -4.0539641 -3.9752669 -3.8869634 -3.8251305 -3.8773804 -3.9789486 -4.0513091 -4.0858655 -4.0985336 -4.0760922 -4.0460367 -4.0527072][-4.183404 -4.1435757 -4.0809317 -4.0049424 -3.9237566 -3.8398418 -3.7923963 -3.877615 -4.0049148 -4.0816803 -4.1069779 -4.1065392 -4.0705585 -4.0345769 -4.0382447][-4.1840963 -4.1421471 -4.0879498 -4.0304956 -3.9778492 -3.9355268 -3.9203703 -3.9917438 -4.0859194 -4.1364522 -4.1432247 -4.1328216 -4.0939026 -4.059165 -4.0651312][-4.1957932 -4.170557 -4.1328712 -4.0964527 -4.0684142 -4.0561037 -4.0608587 -4.1032205 -4.1553364 -4.1749134 -4.1652393 -4.151998 -4.1203237 -4.0893474 -4.0972404][-4.209434 -4.2015657 -4.1817079 -4.1615 -4.1471615 -4.146399 -4.1555715 -4.1752186 -4.1996098 -4.1988792 -4.1758914 -4.1614342 -4.1352544 -4.1080766 -4.1092076][-4.215239 -4.2145834 -4.2073703 -4.1992874 -4.1953192 -4.1994157 -4.2110229 -4.222403 -4.2318969 -4.2208819 -4.1942759 -4.1828189 -4.1584477 -4.1289616 -4.1141586][-4.2215271 -4.2223582 -4.2211661 -4.2193832 -4.2211604 -4.2278509 -4.2385173 -4.24728 -4.2509418 -4.2404084 -4.2181759 -4.2112913 -4.1909103 -4.1568975 -4.1315994][-4.2277069 -4.22735 -4.2267437 -4.2280221 -4.2313356 -4.2367935 -4.2465329 -4.2557821 -4.25714 -4.2481661 -4.2346082 -4.2334776 -4.2156467 -4.1848097 -4.1569014]]...]
INFO - root - 2017-12-06 02:21:06.684774: step 910, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 0.489 sec/batch; 45h:02m:06s remains)
INFO - root - 2017-12-06 02:21:11.314581: step 920, loss = 2.09, batch loss = 2.03 (17.4 examples/sec; 0.460 sec/batch; 42h:21m:39s remains)
INFO - root - 2017-12-06 02:21:15.923900: step 930, loss = 2.09, batch loss = 2.03 (17.1 examples/sec; 0.467 sec/batch; 42h:59m:21s remains)
INFO - root - 2017-12-06 02:21:20.649221: step 940, loss = 2.05, batch loss = 1.99 (16.7 examples/sec; 0.479 sec/batch; 44h:04m:47s remains)
INFO - root - 2017-12-06 02:21:25.364776: step 950, loss = 2.09, batch loss = 2.03 (17.0 examples/sec; 0.470 sec/batch; 43h:19m:31s remains)
INFO - root - 2017-12-06 02:21:30.079475: step 960, loss = 2.05, batch loss = 1.99 (17.9 examples/sec; 0.446 sec/batch; 41h:05m:41s remains)
INFO - root - 2017-12-06 02:21:34.847772: step 970, loss = 2.04, batch loss = 1.99 (16.6 examples/sec; 0.482 sec/batch; 44h:21m:27s remains)
INFO - root - 2017-12-06 02:21:39.488546: step 980, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.459 sec/batch; 42h:16m:10s remains)
INFO - root - 2017-12-06 02:21:44.137091: step 990, loss = 2.04, batch loss = 1.98 (17.1 examples/sec; 0.469 sec/batch; 43h:11m:14s remains)
INFO - root - 2017-12-06 02:21:48.937836: step 1000, loss = 2.04, batch loss = 1.98 (16.8 examples/sec; 0.477 sec/batch; 43h:53m:17s remains)
2017-12-06 02:21:49.366373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1290207 -4.1313949 -4.136477 -4.1507382 -4.1767421 -4.1882224 -4.17059 -4.1384592 -4.1144986 -4.1233258 -4.1456246 -4.1734233 -4.210475 -4.2508593 -4.2808585][-4.1081867 -4.1136441 -4.1246543 -4.1438088 -4.1722584 -4.1844029 -4.1636524 -4.1230254 -4.0941854 -4.1040006 -4.1312337 -4.1669421 -4.2098346 -4.2545395 -4.2861962][-4.1012216 -4.1128626 -4.1320319 -4.1546087 -4.1790462 -4.1879816 -4.1633911 -4.1152906 -4.0830145 -4.0941191 -4.1251965 -4.1635489 -4.2052732 -4.2499266 -4.284317][-4.1117835 -4.1288824 -4.1509252 -4.1704378 -4.1860619 -4.1873374 -4.1564164 -4.1012139 -4.0670614 -4.0816059 -4.1181846 -4.1577425 -4.1959395 -4.2388954 -4.2763906][-4.1461434 -4.1613793 -4.1780696 -4.1853242 -4.1829081 -4.1665931 -4.1255527 -4.066288 -4.0350823 -4.0591183 -4.1073666 -4.154222 -4.1923065 -4.2348275 -4.2734289][-4.1860342 -4.1954432 -4.1991687 -4.1901236 -4.1688905 -4.1356254 -4.0842738 -4.0176163 -3.9893663 -4.0307088 -4.0980349 -4.155952 -4.1972423 -4.2388535 -4.2761025][-4.2106204 -4.2127481 -4.2032852 -4.1811037 -4.1464243 -4.099577 -4.0358882 -3.9578545 -3.9322381 -3.9968266 -4.0882549 -4.1603189 -4.206831 -4.2468944 -4.2816281][-4.2190566 -4.2128334 -4.1949806 -4.1684895 -4.1321616 -4.0821581 -4.0137806 -3.9298294 -3.9098458 -3.992511 -4.0981126 -4.1777692 -4.2246065 -4.2602248 -4.2901831][-4.2285242 -4.2173166 -4.19636 -4.1740355 -4.1473641 -4.1090045 -4.0544391 -3.9845212 -3.9692333 -4.0403123 -4.1339417 -4.2045517 -4.2435479 -4.2723026 -4.2978334][-4.2307396 -4.2166862 -4.1940241 -4.1779222 -4.1670051 -4.1497374 -4.1181612 -4.0679121 -4.0502381 -4.0939903 -4.1625972 -4.2173977 -4.24782 -4.2728653 -4.2981572][-4.2222781 -4.2065821 -4.1846781 -4.1751871 -4.1772041 -4.1770911 -4.1650553 -4.129684 -4.1069427 -4.1274948 -4.1754045 -4.2168269 -4.2422333 -4.2678738 -4.2954564][-4.2137389 -4.1971145 -4.1772647 -4.1738515 -4.18377 -4.1932845 -4.1937828 -4.1692491 -4.1447983 -4.153779 -4.1899309 -4.22184 -4.24531 -4.2715182 -4.2995615][-4.2113824 -4.1963458 -4.180593 -4.182024 -4.1942921 -4.205842 -4.2101235 -4.1906667 -4.1672421 -4.1734166 -4.2047396 -4.232327 -4.2565536 -4.2838783 -4.3107324][-4.2205286 -4.2087855 -4.1970158 -4.20028 -4.2111306 -4.2201576 -4.2222333 -4.2024565 -4.1795864 -4.1873231 -4.2183638 -4.2464385 -4.2734613 -4.3012438 -4.3250289][-4.2373066 -4.2298527 -4.222105 -4.22442 -4.231596 -4.2371631 -4.236567 -4.2176213 -4.1997805 -4.2100768 -4.23996 -4.2683086 -4.2952051 -4.3191724 -4.3380342]]...]
INFO - root - 2017-12-06 02:21:53.877901: step 1010, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.475 sec/batch; 43h:46m:01s remains)
INFO - root - 2017-12-06 02:21:58.472280: step 1020, loss = 2.04, batch loss = 1.98 (17.2 examples/sec; 0.466 sec/batch; 42h:55m:09s remains)
INFO - root - 2017-12-06 02:22:03.258009: step 1030, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.469 sec/batch; 43h:10m:38s remains)
INFO - root - 2017-12-06 02:22:07.989357: step 1040, loss = 2.08, batch loss = 2.02 (16.9 examples/sec; 0.474 sec/batch; 43h:40m:28s remains)
INFO - root - 2017-12-06 02:22:12.654161: step 1050, loss = 2.10, batch loss = 2.04 (17.4 examples/sec; 0.460 sec/batch; 42h:18m:58s remains)
INFO - root - 2017-12-06 02:22:17.371729: step 1060, loss = 2.05, batch loss = 1.99 (16.5 examples/sec; 0.486 sec/batch; 44h:44m:50s remains)
INFO - root - 2017-12-06 02:22:22.169028: step 1070, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.448 sec/batch; 41h:13m:00s remains)
INFO - root - 2017-12-06 02:22:26.849333: step 1080, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.463 sec/batch; 42h:37m:04s remains)
INFO - root - 2017-12-06 02:22:31.549979: step 1090, loss = 2.03, batch loss = 1.97 (17.1 examples/sec; 0.468 sec/batch; 43h:05m:59s remains)
INFO - root - 2017-12-06 02:22:36.080500: step 1100, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.473 sec/batch; 43h:32m:08s remains)
2017-12-06 02:22:36.585870: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1202469 -4.0796113 -4.059382 -4.0815353 -4.1205025 -4.1506181 -4.1958952 -4.2417488 -4.270896 -4.278399 -4.2784071 -4.2778044 -4.2942963 -4.2990179 -4.2890553][-4.1414337 -4.0995827 -4.0648665 -4.0572834 -4.0590677 -4.0680437 -4.1235666 -4.1976814 -4.2464533 -4.2636991 -4.2670059 -4.2690907 -4.2906957 -4.3039646 -4.2991476][-4.1549587 -4.1069045 -4.0545149 -4.0116229 -3.9752767 -3.9746845 -4.0475483 -4.1437078 -4.2110209 -4.2433109 -4.255384 -4.2626748 -4.2862196 -4.3053246 -4.306026][-4.1731315 -4.1266155 -4.0717525 -4.017868 -3.9641516 -3.9556699 -4.0265908 -4.1224437 -4.1932583 -4.233428 -4.2521214 -4.2638216 -4.2846179 -4.3035827 -4.308085][-4.1864309 -4.1486826 -4.1042223 -4.0588098 -4.0098462 -3.9960771 -4.0449467 -4.12117 -4.1834412 -4.2237282 -4.2493634 -4.2682533 -4.2884564 -4.3037858 -4.3093686][-4.1714983 -4.143415 -4.1106925 -4.0778089 -4.0371661 -4.0200629 -4.0468025 -4.0970888 -4.1476212 -4.193315 -4.2272806 -4.2565846 -4.2819953 -4.3004284 -4.3087068][-4.1390567 -4.1156306 -4.0873241 -4.0620694 -4.0272765 -4.0032511 -4.0061255 -4.0308952 -4.0733371 -4.1267958 -4.1743426 -4.22073 -4.2584648 -4.2886295 -4.3044729][-4.1296797 -4.1067333 -4.079668 -4.0580153 -4.0256991 -3.9951286 -3.9772866 -3.9794314 -4.0127749 -4.0708184 -4.1270733 -4.1855021 -4.2358942 -4.2780423 -4.3011732][-4.1636839 -4.1374497 -4.1095386 -4.0892725 -4.0609035 -4.0330448 -4.0112019 -4.0013661 -4.0192327 -4.0670528 -4.11887 -4.1759033 -4.2291236 -4.2767353 -4.3055787][-4.2061729 -4.1795926 -4.1500053 -4.1246929 -4.095418 -4.0702438 -4.0516996 -4.0403905 -4.0484333 -4.0826397 -4.1292386 -4.1831541 -4.2353811 -4.2827115 -4.3141875][-4.2170839 -4.1955881 -4.1690722 -4.1394467 -4.1068897 -4.0805254 -4.0649281 -4.057651 -4.0610194 -4.0856171 -4.1320624 -4.1881785 -4.2418523 -4.28767 -4.3195395][-4.2067761 -4.1947927 -4.1790552 -4.1547685 -4.12347 -4.0948906 -4.0773864 -4.0709524 -4.0673943 -4.0805435 -4.12295 -4.1799073 -4.23554 -4.2836246 -4.3165627][-4.2136049 -4.2104764 -4.205615 -4.1916776 -4.1684361 -4.1419573 -4.1221075 -4.1099415 -4.0966587 -4.0963984 -4.1285667 -4.17882 -4.232451 -4.2792683 -4.3115373][-4.2386689 -4.2374687 -4.2357526 -4.2290945 -4.2160554 -4.1980076 -4.1811562 -4.1663847 -4.14537 -4.1323757 -4.14986 -4.1900868 -4.2382889 -4.2763753 -4.3039174][-4.253274 -4.2495203 -4.2472978 -4.2451825 -4.2415013 -4.231545 -4.2178793 -4.2005019 -4.1747422 -4.1534586 -4.1597376 -4.1913638 -4.2341189 -4.2658143 -4.2901492]]...]
INFO - root - 2017-12-06 02:22:41.277641: step 1110, loss = 2.04, batch loss = 1.99 (16.4 examples/sec; 0.487 sec/batch; 44h:50m:58s remains)
INFO - root - 2017-12-06 02:22:45.940874: step 1120, loss = 2.08, batch loss = 2.02 (17.2 examples/sec; 0.465 sec/batch; 42h:45m:46s remains)
INFO - root - 2017-12-06 02:22:50.656781: step 1130, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.479 sec/batch; 44h:07m:08s remains)
INFO - root - 2017-12-06 02:22:55.392666: step 1140, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.485 sec/batch; 44h:36m:15s remains)
INFO - root - 2017-12-06 02:23:00.079439: step 1150, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 0.484 sec/batch; 44h:31m:55s remains)
INFO - root - 2017-12-06 02:23:04.805830: step 1160, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.473 sec/batch; 43h:33m:31s remains)
INFO - root - 2017-12-06 02:23:09.435294: step 1170, loss = 2.07, batch loss = 2.02 (17.4 examples/sec; 0.460 sec/batch; 42h:19m:28s remains)
INFO - root - 2017-12-06 02:23:14.134003: step 1180, loss = 2.04, batch loss = 1.98 (16.9 examples/sec; 0.474 sec/batch; 43h:36m:05s remains)
INFO - root - 2017-12-06 02:23:18.785370: step 1190, loss = 2.05, batch loss = 1.99 (17.8 examples/sec; 0.451 sec/batch; 41h:28m:31s remains)
INFO - root - 2017-12-06 02:23:23.209969: step 1200, loss = 2.05, batch loss = 1.99 (16.7 examples/sec; 0.480 sec/batch; 44h:12m:51s remains)
2017-12-06 02:23:23.667067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2407479 -4.2455993 -4.2473378 -4.2479329 -4.24527 -4.2405734 -4.2396789 -4.2357635 -4.2267604 -4.2145004 -4.201695 -4.1870995 -4.1823583 -4.1961026 -4.2186208][-4.231894 -4.2391372 -4.2452126 -4.25028 -4.2461195 -4.2389712 -4.2381821 -4.2339749 -4.2232561 -4.2131448 -4.20677 -4.1998358 -4.1983562 -4.2065487 -4.2199469][-4.2071576 -4.2126732 -4.2184405 -4.2262592 -4.2216229 -4.215713 -4.2179861 -4.2162728 -4.2101126 -4.2037835 -4.2052937 -4.2093458 -4.2126975 -4.2162147 -4.2202163][-4.1860509 -4.1830449 -4.1835337 -4.1883225 -4.1801734 -4.1752963 -4.180171 -4.1853938 -4.1892929 -4.1886225 -4.194694 -4.2081842 -4.2179861 -4.2208829 -4.2197437][-4.1613789 -4.1479945 -4.1402559 -4.1412158 -4.1318 -4.1280293 -4.1337695 -4.1458182 -4.1620278 -4.16899 -4.1759148 -4.19293 -4.2069521 -4.2099118 -4.2043509][-4.1345358 -4.1163225 -4.1029005 -4.097672 -4.0884933 -4.0822973 -4.0813055 -4.0969996 -4.126555 -4.1411362 -4.1493516 -4.1663728 -4.1813912 -4.1872807 -4.1795774][-4.1056619 -4.0913868 -4.0750647 -4.0574422 -4.0408068 -4.0258589 -4.0048862 -4.0098 -4.052309 -4.08252 -4.1010113 -4.1239848 -4.1444926 -4.1645489 -4.167376][-4.0787253 -4.0774703 -4.0605249 -4.024766 -3.9910996 -3.9542527 -3.9029531 -3.8874443 -3.945673 -4.006217 -4.0468774 -4.0836582 -4.1151075 -4.1510811 -4.1681471][-4.0698056 -4.0807285 -4.0620756 -4.0145059 -3.9688435 -3.91973 -3.8555002 -3.8253942 -3.8824925 -3.9616332 -4.0217161 -4.0721874 -4.113894 -4.1550589 -4.1792126][-4.0987878 -4.1122923 -4.0940852 -4.0503922 -4.0123038 -3.9727266 -3.9270422 -3.9069438 -3.9402277 -3.999476 -4.052886 -4.104012 -4.1466341 -4.1839833 -4.2085676][-4.1514039 -4.1644387 -4.1507931 -4.1168137 -4.0880013 -4.0584335 -4.0282469 -4.0166388 -4.0324078 -4.0657754 -4.1026874 -4.1464171 -4.185358 -4.2175231 -4.2411132][-4.2116981 -4.2213154 -4.2129354 -4.1930218 -4.1700239 -4.1447887 -4.1264415 -4.1222157 -4.1331148 -4.1502485 -4.17057 -4.198669 -4.2253246 -4.2469721 -4.26396][-4.2522717 -4.2581692 -4.2544804 -4.2450433 -4.22949 -4.2115149 -4.2026153 -4.2037697 -4.2114906 -4.2181177 -4.2270594 -4.24024 -4.2533813 -4.2627206 -4.2715936][-4.2564692 -4.2599735 -4.2577634 -4.2532411 -4.24272 -4.2322383 -4.228302 -4.2295036 -4.2331195 -4.2346907 -4.2371483 -4.2422371 -4.2466359 -4.2497749 -4.2524858][-4.2350793 -4.2375779 -4.2372179 -4.2364082 -4.2327385 -4.2289128 -4.2263393 -4.226161 -4.2272749 -4.2252975 -4.2235289 -4.2241335 -4.2247772 -4.2248049 -4.2241726]]...]
INFO - root - 2017-12-06 02:23:28.397040: step 1210, loss = 2.05, batch loss = 2.00 (16.8 examples/sec; 0.477 sec/batch; 43h:54m:07s remains)
INFO - root - 2017-12-06 02:23:33.042804: step 1220, loss = 2.05, batch loss = 2.00 (17.2 examples/sec; 0.465 sec/batch; 42h:48m:31s remains)
INFO - root - 2017-12-06 02:23:37.739099: step 1230, loss = 2.05, batch loss = 1.99 (18.5 examples/sec; 0.434 sec/batch; 39h:53m:44s remains)
INFO - root - 2017-12-06 02:23:42.372548: step 1240, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.468 sec/batch; 43h:04m:33s remains)
INFO - root - 2017-12-06 02:23:47.091377: step 1250, loss = 2.09, batch loss = 2.03 (17.0 examples/sec; 0.471 sec/batch; 43h:22m:43s remains)
INFO - root - 2017-12-06 02:23:51.822156: step 1260, loss = 2.05, batch loss = 1.99 (16.7 examples/sec; 0.480 sec/batch; 44h:10m:52s remains)
INFO - root - 2017-12-06 02:23:56.558215: step 1270, loss = 2.08, batch loss = 2.02 (16.5 examples/sec; 0.484 sec/batch; 44h:31m:55s remains)
INFO - root - 2017-12-06 02:24:01.373120: step 1280, loss = 2.05, batch loss = 2.00 (17.5 examples/sec; 0.456 sec/batch; 41h:57m:04s remains)
INFO - root - 2017-12-06 02:24:05.778724: step 1290, loss = 2.06, batch loss = 2.00 (25.0 examples/sec; 0.320 sec/batch; 29h:26m:53s remains)
INFO - root - 2017-12-06 02:24:10.452735: step 1300, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.431 sec/batch; 39h:40m:42s remains)
2017-12-06 02:24:10.927468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2144237 -4.2311125 -4.2406292 -4.2444768 -4.2425218 -4.2326336 -4.1994653 -4.1334457 -4.0657592 -4.074358 -4.1118674 -4.1487527 -4.1987991 -4.2499337 -4.28374][-4.1825581 -4.1972885 -4.2054043 -4.2087517 -4.2116141 -4.210598 -4.1850195 -4.1201162 -4.0514069 -4.0612149 -4.0985661 -4.1346116 -4.1871805 -4.2433391 -4.2801981][-4.1495719 -4.1602468 -4.16602 -4.1690912 -4.1795444 -4.19097 -4.1727381 -4.1093264 -4.0426807 -4.0557613 -4.0933528 -4.1262589 -4.1777434 -4.2356415 -4.2750793][-4.1214123 -4.1255217 -4.1287451 -4.13218 -4.152586 -4.178164 -4.166131 -4.103631 -4.0404005 -4.0593634 -4.099124 -4.1305375 -4.1802363 -4.2365637 -4.2751589][-4.0986724 -4.0954709 -4.0964556 -4.0991507 -4.1270618 -4.1654153 -4.1572304 -4.0934429 -4.032742 -4.0611053 -4.1075592 -4.141181 -4.1911006 -4.2447753 -4.2800226][-4.0763307 -4.065743 -4.0642986 -4.0639434 -4.0929227 -4.1384063 -4.1310329 -4.0631218 -4.0054088 -4.0473514 -4.1054945 -4.1466818 -4.2012539 -4.2549553 -4.2870045][-4.0768728 -4.0572324 -4.04686 -4.0371194 -4.0559769 -4.0970736 -4.0834842 -4.0081387 -3.9537015 -4.012939 -4.0873194 -4.1402488 -4.2040877 -4.2613082 -4.292274][-4.1129479 -4.086566 -4.0647645 -4.0437875 -4.0465493 -4.0706635 -4.0408444 -3.9534564 -3.9004068 -3.974052 -4.0620208 -4.1262112 -4.2001104 -4.2626858 -4.2942114][-4.1673732 -4.1433969 -4.1174097 -4.0917883 -4.0837178 -4.0883579 -4.0433288 -3.9493093 -3.8978875 -3.971415 -4.0575833 -4.1207881 -4.1965823 -4.2617478 -4.2942557][-4.2173805 -4.203845 -4.1818051 -4.1594381 -4.1505079 -4.1426449 -4.0915189 -4.0042624 -3.9570723 -4.0168657 -4.0869312 -4.1377687 -4.2033486 -4.2636814 -4.2942944][-4.25565 -4.252121 -4.2356505 -4.2175965 -4.2111888 -4.198267 -4.1493998 -4.0753331 -4.0330873 -4.0793452 -4.1335244 -4.1715779 -4.2230096 -4.2730513 -4.2979822][-4.2832513 -4.2863231 -4.2752352 -4.2606268 -4.256207 -4.2419057 -4.2012968 -4.1416869 -4.1044073 -4.139616 -4.1819353 -4.2116446 -4.251152 -4.2888389 -4.3054547][-4.3062348 -4.3122067 -4.30395 -4.2917886 -4.2886696 -4.2751722 -4.2430792 -4.19666 -4.1670666 -4.1959038 -4.2293911 -4.2521005 -4.2812047 -4.3069887 -4.3154607][-4.3256316 -4.3314729 -4.3243184 -4.3153682 -4.3128495 -4.3008542 -4.2766247 -4.2426825 -4.2225571 -4.2487535 -4.2762928 -4.2936368 -4.3142114 -4.328568 -4.3284163][-4.3365111 -4.3395805 -4.3328171 -4.3266492 -4.3244829 -4.3147926 -4.2984176 -4.2766948 -4.264421 -4.2876773 -4.3107743 -4.3250818 -4.33996 -4.3465714 -4.3404045]]...]
INFO - root - 2017-12-06 02:24:15.561026: step 1310, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.455 sec/batch; 41h:51m:03s remains)
INFO - root - 2017-12-06 02:24:20.207285: step 1320, loss = 2.05, batch loss = 2.00 (17.5 examples/sec; 0.457 sec/batch; 42h:01m:48s remains)
INFO - root - 2017-12-06 02:24:24.931313: step 1330, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.471 sec/batch; 43h:18m:16s remains)
INFO - root - 2017-12-06 02:24:29.537798: step 1340, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.467 sec/batch; 42h:59m:07s remains)
INFO - root - 2017-12-06 02:24:34.252138: step 1350, loss = 2.06, batch loss = 2.01 (17.5 examples/sec; 0.458 sec/batch; 42h:09m:02s remains)
INFO - root - 2017-12-06 02:24:38.912881: step 1360, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.462 sec/batch; 42h:31m:55s remains)
INFO - root - 2017-12-06 02:24:43.683742: step 1370, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.468 sec/batch; 43h:05m:26s remains)
INFO - root - 2017-12-06 02:24:48.302800: step 1380, loss = 2.04, batch loss = 1.98 (17.3 examples/sec; 0.462 sec/batch; 42h:29m:54s remains)
INFO - root - 2017-12-06 02:24:52.801028: step 1390, loss = 2.04, batch loss = 1.98 (16.6 examples/sec; 0.481 sec/batch; 44h:13m:24s remains)
INFO - root - 2017-12-06 02:24:57.464044: step 1400, loss = 2.04, batch loss = 1.98 (16.9 examples/sec; 0.473 sec/batch; 43h:27m:54s remains)
2017-12-06 02:24:57.910531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2036462 -4.2108488 -4.212316 -4.2075233 -4.1979442 -4.1914535 -4.1861496 -4.1757493 -4.1626325 -4.1449213 -4.13941 -4.1413832 -4.1430616 -4.1542573 -4.1746655][-4.1975503 -4.2086034 -4.2126637 -4.2087903 -4.1996861 -4.1943879 -4.1916347 -4.1822405 -4.1729226 -4.1591082 -4.1551156 -4.1550269 -4.1514554 -4.1577559 -4.1747789][-4.1894169 -4.1982217 -4.2015729 -4.1978025 -4.1881833 -4.1833739 -4.1822653 -4.1761694 -4.1755528 -4.170114 -4.1699281 -4.1703343 -4.1648293 -4.1676092 -4.1821628][-4.167387 -4.1672587 -4.1668944 -4.1613445 -4.1493311 -4.1425 -4.1469507 -4.151639 -4.163866 -4.1720643 -4.1785169 -4.181119 -4.17541 -4.1749358 -4.1832433][-4.1359315 -4.1248589 -4.1194015 -4.10977 -4.092041 -4.0803041 -4.0865817 -4.1001596 -4.1261377 -4.149354 -4.1664491 -4.1755528 -4.176981 -4.1777992 -4.1817265][-4.1197124 -4.0968075 -4.0771189 -4.0529547 -4.0193377 -3.995888 -4.0004177 -4.0194883 -4.065166 -4.1132021 -4.1440258 -4.1620903 -4.1721773 -4.1780181 -4.17953][-4.1418233 -4.106905 -4.070951 -4.0262675 -3.9670281 -3.9216304 -3.9178555 -3.9373679 -3.9994295 -4.0766129 -4.1301808 -4.1648474 -4.1849947 -4.1978531 -4.1993651][-4.1756649 -4.1339626 -4.090126 -4.0355587 -3.9670937 -3.9114389 -3.9026167 -3.9158149 -3.9803023 -4.0735078 -4.1407576 -4.1871629 -4.2156382 -4.2366171 -4.2423263][-4.2119389 -4.1699572 -4.1269846 -4.073822 -4.0114994 -3.9643128 -3.958889 -3.9663761 -4.0196252 -4.1015725 -4.1607494 -4.203094 -4.2324638 -4.2580976 -4.2690506][-4.2492814 -4.2136831 -4.1733952 -4.1248107 -4.0719137 -4.0346136 -4.0344205 -4.0417962 -4.0822611 -4.14281 -4.18568 -4.2158508 -4.2345262 -4.2527113 -4.26253][-4.2752924 -4.2504716 -4.2195311 -4.1813688 -4.1390257 -4.1056018 -4.1008635 -4.1046562 -4.1301489 -4.1669483 -4.192584 -4.2136259 -4.224731 -4.2360468 -4.2441463][-4.2883272 -4.2741561 -4.2527084 -4.2270193 -4.196764 -4.167706 -4.153964 -4.1455374 -4.1519694 -4.1663609 -4.1787009 -4.1960478 -4.2092028 -4.2223749 -4.2330494][-4.2829967 -4.28168 -4.2716551 -4.257906 -4.2382636 -4.2130136 -4.1900859 -4.1689839 -4.1624517 -4.1617 -4.1664982 -4.1848469 -4.2027345 -4.2218976 -4.2372565][-4.2541933 -4.2659292 -4.2672567 -4.2648153 -4.2537642 -4.2346659 -4.2109537 -4.1854973 -4.1720214 -4.1636977 -4.166132 -4.1861482 -4.2053857 -4.2291803 -4.2480931][-4.2113 -4.2304244 -4.2414613 -4.2492075 -4.2478085 -4.2376943 -4.2189865 -4.1973882 -4.1829638 -4.173295 -4.1758142 -4.1964307 -4.2160883 -4.2373009 -4.253746]]...]
INFO - root - 2017-12-06 02:25:02.556369: step 1410, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.459 sec/batch; 42h:10m:34s remains)
INFO - root - 2017-12-06 02:25:07.135640: step 1420, loss = 2.06, batch loss = 2.01 (17.1 examples/sec; 0.468 sec/batch; 43h:00m:34s remains)
INFO - root - 2017-12-06 02:25:11.874820: step 1430, loss = 2.09, batch loss = 2.03 (17.0 examples/sec; 0.471 sec/batch; 43h:19m:12s remains)
INFO - root - 2017-12-06 02:25:16.617255: step 1440, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 40h:12m:12s remains)
INFO - root - 2017-12-06 02:25:21.226515: step 1450, loss = 2.07, batch loss = 2.01 (16.3 examples/sec; 0.490 sec/batch; 45h:02m:38s remains)
INFO - root - 2017-12-06 02:25:25.989250: step 1460, loss = 2.09, batch loss = 2.03 (16.2 examples/sec; 0.494 sec/batch; 45h:23m:01s remains)
INFO - root - 2017-12-06 02:25:30.639084: step 1470, loss = 2.09, batch loss = 2.03 (17.2 examples/sec; 0.464 sec/batch; 42h:41m:58s remains)
INFO - root - 2017-12-06 02:25:35.326610: step 1480, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 0.471 sec/batch; 43h:19m:32s remains)
INFO - root - 2017-12-06 02:25:39.755464: step 1490, loss = 2.07, batch loss = 2.02 (16.9 examples/sec; 0.472 sec/batch; 43h:26m:27s remains)
INFO - root - 2017-12-06 02:25:44.458301: step 1500, loss = 2.10, batch loss = 2.04 (16.4 examples/sec; 0.487 sec/batch; 44h:49m:19s remains)
2017-12-06 02:25:44.930025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2403851 -4.2354283 -4.230752 -4.2241473 -4.2196078 -4.2334027 -4.2489638 -4.2685032 -4.2778883 -4.2615924 -4.2422743 -4.2476268 -4.2684188 -4.2710247 -4.2621684][-4.194334 -4.1860986 -4.1913776 -4.2022152 -4.2020192 -4.2112579 -4.2238569 -4.2461977 -4.255856 -4.2355518 -4.2182989 -4.2322459 -4.263483 -4.2742662 -4.263433][-4.1331692 -4.122736 -4.1437831 -4.1731415 -4.1754065 -4.1757069 -4.1852202 -4.2112455 -4.2241945 -4.2072058 -4.19786 -4.2204146 -4.2594709 -4.2749977 -4.2646546][-4.0937572 -4.0850959 -4.1158476 -4.1533818 -4.1492977 -4.1337514 -4.1366911 -4.1639991 -4.1814995 -4.1765242 -4.1810293 -4.2119079 -4.2547331 -4.2734947 -4.2674351][-4.0985456 -4.0858259 -4.1130056 -4.1424093 -4.1238666 -4.0943456 -4.09333 -4.1200333 -4.1438065 -4.1530194 -4.167913 -4.2077837 -4.25121 -4.2746887 -4.276525][-4.1123281 -4.095922 -4.1159086 -4.1333385 -4.1031528 -4.0658808 -4.0649118 -4.0937243 -4.1243377 -4.1416106 -4.1574955 -4.2036457 -4.2520618 -4.28255 -4.288372][-4.1208658 -4.1087704 -4.1254845 -4.1293817 -4.0911689 -4.0426664 -4.03589 -4.065886 -4.1065779 -4.1302896 -4.1432323 -4.1873832 -4.2402425 -4.2778606 -4.2871885][-4.1099315 -4.1052785 -4.1190577 -4.1126757 -4.0671573 -4.0051842 -3.9865718 -4.0136542 -4.0633864 -4.0944753 -4.1059046 -4.1436396 -4.1978874 -4.2463489 -4.2667971][-4.0839849 -4.0850396 -4.0980992 -4.0898733 -4.0457697 -3.9740841 -3.9376788 -3.9531667 -4.00566 -4.0409603 -4.0474892 -4.0791416 -4.136991 -4.1983733 -4.2303991][-4.0748191 -4.0786781 -4.09003 -4.0836554 -4.0464044 -3.9764755 -3.9259295 -3.9275365 -3.9742696 -4.0075078 -4.0079241 -4.0333657 -4.0917487 -4.1580706 -4.199841][-4.0790277 -4.0800829 -4.0883584 -4.08512 -4.0604782 -4.0088058 -3.9626446 -3.9547758 -3.9853513 -4.0095496 -4.0067134 -4.0244331 -4.0737038 -4.1338997 -4.1758718][-4.0823965 -4.0822086 -4.0943737 -4.0999341 -4.0954552 -4.0687928 -4.0342321 -4.0206318 -4.0333152 -4.0448136 -4.0411882 -4.0550795 -4.0953164 -4.1450348 -4.1768694][-4.1076803 -4.1080952 -4.1233463 -4.135056 -4.1437778 -4.1363244 -4.1164188 -4.1039281 -4.1047544 -4.10717 -4.1041269 -4.1159348 -4.1470509 -4.1843171 -4.2067447][-4.1555696 -4.1552348 -4.1705475 -4.1848574 -4.199235 -4.2037187 -4.1962204 -4.1850648 -4.1787248 -4.1751637 -4.1709843 -4.1778493 -4.1977592 -4.2253761 -4.245626][-4.2192397 -4.2196479 -4.2323184 -4.2460823 -4.259902 -4.2669272 -4.2639823 -4.2554989 -4.2494516 -4.247252 -4.2448812 -4.2478056 -4.257267 -4.2750931 -4.2902684]]...]
INFO - root - 2017-12-06 02:25:49.584222: step 1510, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.462 sec/batch; 42h:29m:53s remains)
INFO - root - 2017-12-06 02:25:54.357087: step 1520, loss = 2.05, batch loss = 1.99 (16.3 examples/sec; 0.491 sec/batch; 45h:06m:19s remains)
INFO - root - 2017-12-06 02:25:59.054326: step 1530, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.473 sec/batch; 43h:28m:32s remains)
INFO - root - 2017-12-06 02:26:03.734043: step 1540, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.471 sec/batch; 43h:16m:12s remains)
INFO - root - 2017-12-06 02:26:08.410904: step 1550, loss = 2.05, batch loss = 1.99 (16.9 examples/sec; 0.473 sec/batch; 43h:28m:05s remains)
INFO - root - 2017-12-06 02:26:13.022906: step 1560, loss = 2.05, batch loss = 1.99 (16.6 examples/sec; 0.482 sec/batch; 44h:16m:48s remains)
INFO - root - 2017-12-06 02:26:17.735364: step 1570, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.473 sec/batch; 43h:29m:42s remains)
INFO - root - 2017-12-06 02:26:22.218530: step 1580, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.456 sec/batch; 41h:56m:48s remains)
INFO - root - 2017-12-06 02:26:26.868944: step 1590, loss = 2.08, batch loss = 2.03 (15.7 examples/sec; 0.509 sec/batch; 46h:45m:25s remains)
INFO - root - 2017-12-06 02:26:31.649327: step 1600, loss = 2.05, batch loss = 1.99 (16.7 examples/sec; 0.480 sec/batch; 44h:05m:50s remains)
2017-12-06 02:26:32.121486: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.19011 -4.18727 -4.1787195 -4.1679544 -4.1600862 -4.1584311 -4.1519461 -4.1452327 -4.1527987 -4.16419 -4.165081 -4.1591582 -4.157537 -4.1532736 -4.1438761][-4.1792784 -4.1761274 -4.1670308 -4.1560125 -4.14382 -4.1388912 -4.1294627 -4.1200066 -4.1283531 -4.141181 -4.1441936 -4.1400013 -4.1368704 -4.1273131 -4.1094885][-4.1608171 -4.1630235 -4.158164 -4.1503959 -4.1372204 -4.1327157 -4.125411 -4.1170545 -4.1207814 -4.1287847 -4.1329947 -4.1348209 -4.1359425 -4.1251659 -4.1021547][-4.1194043 -4.1307478 -4.1342249 -4.1356392 -4.1302371 -4.1286931 -4.1227679 -4.1131396 -4.1091571 -4.1103539 -4.119853 -4.1360521 -4.1448159 -4.1360741 -4.1124153][-4.0754757 -4.0920663 -4.1006665 -4.1094208 -4.1136184 -4.1178446 -4.1154146 -4.1055241 -4.0952048 -4.0927563 -4.109817 -4.1352882 -4.146482 -4.1378207 -4.1174464][-4.0432549 -4.0585394 -4.0630331 -4.0689774 -4.0764971 -4.0873036 -4.0932961 -4.0885773 -4.0780706 -4.0769043 -4.1004944 -4.1297131 -4.138411 -4.1244979 -4.1031761][-4.0425115 -4.0547218 -4.0549273 -4.0513892 -4.0494509 -4.0502996 -4.0433869 -4.0219865 -4.001718 -4.01234 -4.0526309 -4.0929742 -4.10919 -4.1028147 -4.0841756][-4.0842867 -4.0924335 -4.0834928 -4.069047 -4.0559063 -4.0446997 -4.0169783 -3.9690847 -3.9284062 -3.9470809 -4.006145 -4.0614009 -4.0902052 -4.0997858 -4.0945339][-4.12243 -4.1319203 -4.1178937 -4.1003418 -4.0895352 -4.0831718 -4.0593758 -4.008534 -3.96274 -3.9716511 -4.024549 -4.0758057 -4.103477 -4.1203918 -4.1238918][-4.1552448 -4.1682291 -4.1583376 -4.1440439 -4.1408234 -4.1465364 -4.1399851 -4.1069355 -4.0744166 -4.0763073 -4.1049 -4.1324644 -4.1460752 -4.1601152 -4.167779][-4.1839652 -4.2029638 -4.20197 -4.191586 -4.1888852 -4.1956763 -4.1933942 -4.1728387 -4.1551619 -4.1613045 -4.1788049 -4.1888881 -4.192543 -4.2035842 -4.2142606][-4.2101889 -4.2307062 -4.2350578 -4.2279038 -4.2244954 -4.226871 -4.2238488 -4.2105894 -4.2032537 -4.2134743 -4.2298679 -4.2376819 -4.2383947 -4.2446861 -4.2538285][-4.23586 -4.2488079 -4.2534037 -4.2521443 -4.2492003 -4.2490983 -4.2471056 -4.238265 -4.2342887 -4.2433567 -4.2575536 -4.2678709 -4.2715034 -4.2752419 -4.2818122][-4.2573271 -4.2612953 -4.2636728 -4.2648373 -4.265018 -4.267499 -4.2707534 -4.2688642 -4.2649736 -4.2679877 -4.2750053 -4.2813129 -4.2849731 -4.288404 -4.2920651][-4.2584825 -4.254199 -4.2528205 -4.2545552 -4.25885 -4.26365 -4.2702823 -4.272481 -4.268754 -4.2651024 -4.2614655 -4.259582 -4.2608423 -4.2674055 -4.2754283]]...]
INFO - root - 2017-12-06 02:26:36.842376: step 1610, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.474 sec/batch; 43h:34m:34s remains)
INFO - root - 2017-12-06 02:26:41.535272: step 1620, loss = 2.03, batch loss = 1.97 (17.0 examples/sec; 0.471 sec/batch; 43h:19m:53s remains)
INFO - root - 2017-12-06 02:26:46.222493: step 1630, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.440 sec/batch; 40h:28m:04s remains)
INFO - root - 2017-12-06 02:26:50.972260: step 1640, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.469 sec/batch; 43h:04m:34s remains)
INFO - root - 2017-12-06 02:26:55.626745: step 1650, loss = 2.05, batch loss = 2.00 (16.9 examples/sec; 0.474 sec/batch; 43h:31m:05s remains)
INFO - root - 2017-12-06 02:27:00.356128: step 1660, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.467 sec/batch; 42h:57m:32s remains)
INFO - root - 2017-12-06 02:27:05.000275: step 1670, loss = 2.06, batch loss = 2.00 (16.6 examples/sec; 0.482 sec/batch; 44h:20m:25s remains)
INFO - root - 2017-12-06 02:27:09.422875: step 1680, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.474 sec/batch; 43h:31m:57s remains)
INFO - root - 2017-12-06 02:27:14.122180: step 1690, loss = 2.09, batch loss = 2.03 (16.9 examples/sec; 0.474 sec/batch; 43h:34m:42s remains)
INFO - root - 2017-12-06 02:27:18.841553: step 1700, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.484 sec/batch; 44h:25m:58s remains)
2017-12-06 02:27:19.296318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2129092 -4.2031746 -4.1845965 -4.16265 -4.1534538 -4.16209 -4.1719794 -4.1822605 -4.1902232 -4.1985173 -4.2084193 -4.2049894 -4.1927743 -4.1791582 -4.175004][-4.2446861 -4.2408428 -4.2190819 -4.1903677 -4.176909 -4.1834593 -4.1915088 -4.2009907 -4.2083211 -4.2181849 -4.2293119 -4.22647 -4.219203 -4.208055 -4.2017384][-4.2713728 -4.2695355 -4.2443252 -4.2137375 -4.1933818 -4.1910057 -4.1941233 -4.2012005 -4.2132349 -4.227809 -4.2430997 -4.2418895 -4.2368536 -4.2287245 -4.2207332][-4.2799053 -4.2781734 -4.2528944 -4.2218204 -4.1944795 -4.1815109 -4.1728811 -4.1739016 -4.194447 -4.2200155 -4.2419505 -4.2456946 -4.241024 -4.2326741 -4.2229548][-4.2762675 -4.2755194 -4.2482357 -4.2122192 -4.1730857 -4.1368437 -4.1015229 -4.0933261 -4.1334224 -4.1799059 -4.2146115 -4.2286921 -4.2292671 -4.2234211 -4.2159162][-4.2647958 -4.2620091 -4.2286077 -4.1826115 -4.1244469 -4.04727 -3.9639747 -3.9428344 -4.023807 -4.1144028 -4.1757317 -4.2056909 -4.2140145 -4.211432 -4.210351][-4.2495737 -4.2414789 -4.2051625 -4.1508732 -4.0750475 -3.9554033 -3.814503 -3.7773266 -3.9072232 -4.0445914 -4.1374846 -4.1873527 -4.2052383 -4.208209 -4.2118845][-4.2165713 -4.204215 -4.1743617 -4.1287117 -4.0628142 -3.9558811 -3.8282366 -3.7916162 -3.9086926 -4.0410757 -4.1347709 -4.1869144 -4.2019053 -4.2026958 -4.2045927][-4.1782684 -4.1681614 -4.153873 -4.1320319 -4.1000247 -4.0473175 -3.9846323 -3.9661047 -4.0222387 -4.0986571 -4.163383 -4.1976233 -4.20298 -4.1965089 -4.1910658][-4.1674604 -4.1592751 -4.1558008 -4.1515641 -4.1449327 -4.1316566 -4.1173816 -4.1184521 -4.1377678 -4.1657395 -4.1970611 -4.2120061 -4.2094007 -4.1996946 -4.1925421][-4.1886058 -4.1809225 -4.17526 -4.1711259 -4.1718335 -4.1742597 -4.1840057 -4.1979828 -4.2045569 -4.2053251 -4.2162118 -4.2198529 -4.2118931 -4.2002149 -4.1925311][-4.2101188 -4.2085171 -4.1994181 -4.1953845 -4.1973085 -4.199749 -4.2099905 -4.2240467 -4.22606 -4.2195511 -4.2235227 -4.2206941 -4.2095041 -4.1938038 -4.1796842][-4.2261992 -4.2296424 -4.2203989 -4.2180238 -4.2207084 -4.22235 -4.229373 -4.2362266 -4.2323694 -4.2247496 -4.2245936 -4.2166824 -4.2037239 -4.1848879 -4.1650062][-4.23409 -4.2410827 -4.2347846 -4.2356195 -4.240386 -4.2417965 -4.2471895 -4.2471375 -4.2377415 -4.2265062 -4.2223134 -4.2149425 -4.2076278 -4.1954746 -4.1763859][-4.2384787 -4.2454276 -4.2400408 -4.2402573 -4.2450528 -4.24743 -4.2525291 -4.2499161 -4.2417626 -4.2292538 -4.2207294 -4.2157936 -4.2190733 -4.2199411 -4.2081819]]...]
INFO - root - 2017-12-06 02:27:23.967994: step 1710, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 0.472 sec/batch; 43h:21m:43s remains)
INFO - root - 2017-12-06 02:27:28.695879: step 1720, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.460 sec/batch; 42h:14m:23s remains)
INFO - root - 2017-12-06 02:27:33.355192: step 1730, loss = 2.05, batch loss = 2.00 (17.3 examples/sec; 0.462 sec/batch; 42h:29m:36s remains)
INFO - root - 2017-12-06 02:27:38.081986: step 1740, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.469 sec/batch; 43h:03m:54s remains)
INFO - root - 2017-12-06 02:27:42.746411: step 1750, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.473 sec/batch; 43h:27m:25s remains)
INFO - root - 2017-12-06 02:27:47.468785: step 1760, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.467 sec/batch; 42h:56m:27s remains)
INFO - root - 2017-12-06 02:27:52.136434: step 1770, loss = 2.06, batch loss = 2.00 (15.9 examples/sec; 0.502 sec/batch; 46h:09m:45s remains)
INFO - root - 2017-12-06 02:27:56.633329: step 1780, loss = 2.09, batch loss = 2.03 (17.1 examples/sec; 0.468 sec/batch; 42h:59m:17s remains)
INFO - root - 2017-12-06 02:28:01.259755: step 1790, loss = 2.05, batch loss = 2.00 (17.9 examples/sec; 0.447 sec/batch; 41h:03m:13s remains)
INFO - root - 2017-12-06 02:28:05.903004: step 1800, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 0.482 sec/batch; 44h:19m:09s remains)
2017-12-06 02:28:06.337376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1644025 -4.1737533 -4.1660404 -4.1440735 -4.1336331 -4.1168389 -4.0762429 -4.0373783 -4.0699635 -4.1405306 -4.1762257 -4.1847072 -4.1850562 -4.1768122 -4.1671624][-4.1923141 -4.1996961 -4.194953 -4.1793103 -4.1749625 -4.1524677 -4.0950937 -4.051384 -4.0961113 -4.1685538 -4.2016983 -4.2104936 -4.2107286 -4.2026558 -4.1898394][-4.2121081 -4.2156129 -4.2126803 -4.2064805 -4.206389 -4.1798949 -4.1109095 -4.0642748 -4.1164351 -4.1881704 -4.2176566 -4.2281408 -4.2271814 -4.222651 -4.2135057][-4.2201152 -4.2209134 -4.2221837 -4.2205009 -4.2206974 -4.1938348 -4.1190138 -4.0700283 -4.1249819 -4.1954651 -4.222558 -4.2369766 -4.2379761 -4.23583 -4.2303267][-4.2104535 -4.2096047 -4.2122655 -4.2105513 -4.2124476 -4.1852331 -4.1069918 -4.0569496 -4.1143818 -4.1828351 -4.20823 -4.2261972 -4.2354546 -4.2383761 -4.2338119][-4.1920915 -4.188364 -4.1898541 -4.1880751 -4.1872478 -4.1553483 -4.0737548 -4.022891 -4.0875869 -4.1573753 -4.1860795 -4.2087693 -4.2224536 -4.2297468 -4.2219625][-4.1671824 -4.1596184 -4.1619158 -4.1612382 -4.1545143 -4.1140962 -4.0198965 -3.9584651 -4.0336657 -4.1184506 -4.1602058 -4.1870723 -4.2015386 -4.2091103 -4.1983747][-4.1369953 -4.1304135 -4.1368918 -4.1359892 -4.12122 -4.0682454 -3.9508584 -3.8693488 -3.9596722 -4.071496 -4.130589 -4.15904 -4.17403 -4.1795578 -4.1676617][-4.1171484 -4.1187353 -4.1255708 -4.1172876 -4.0938206 -4.0288672 -3.8956449 -3.802619 -3.9076385 -4.0437465 -4.1185575 -4.1529021 -4.1685152 -4.168457 -4.1534729][-4.1108675 -4.1239548 -4.130734 -4.1184468 -4.0895805 -4.0234327 -3.902952 -3.8320217 -3.9355917 -4.0675 -4.1429019 -4.1789403 -4.1901865 -4.1796885 -4.1620274][-4.129034 -4.1530862 -4.1657724 -4.1565733 -4.1291809 -4.0761471 -3.9920893 -3.95123 -4.0256491 -4.1249723 -4.1823821 -4.2082167 -4.2083025 -4.1879721 -4.1718512][-4.1682849 -4.19566 -4.2125578 -4.2061563 -4.1867361 -4.1546035 -4.1001258 -4.0699482 -4.11208 -4.17914 -4.2164288 -4.2275844 -4.2146764 -4.1901064 -4.1812906][-4.2061324 -4.2258973 -4.2419829 -4.2390289 -4.2275829 -4.2116003 -4.178616 -4.15081 -4.1670241 -4.2108054 -4.2299895 -4.2290382 -4.2067046 -4.1828523 -4.1811962][-4.2108364 -4.2159534 -4.2324071 -4.2403593 -4.2366166 -4.2306929 -4.2123117 -4.183413 -4.1821604 -4.2090011 -4.2186351 -4.2097592 -4.1825857 -4.154953 -4.1623964][-4.1874008 -4.1807151 -4.1978703 -4.2114038 -4.2115536 -4.2110777 -4.2006936 -4.1735096 -4.1647005 -4.1858807 -4.1971374 -4.1891251 -4.157589 -4.1282277 -4.1435223]]...]
INFO - root - 2017-12-06 02:28:10.998888: step 1810, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.470 sec/batch; 43h:08m:44s remains)
INFO - root - 2017-12-06 02:28:15.727663: step 1820, loss = 2.07, batch loss = 2.02 (17.1 examples/sec; 0.469 sec/batch; 43h:04m:47s remains)
INFO - root - 2017-12-06 02:28:20.389745: step 1830, loss = 2.04, batch loss = 1.98 (17.2 examples/sec; 0.465 sec/batch; 42h:45m:00s remains)
INFO - root - 2017-12-06 02:28:25.130637: step 1840, loss = 2.06, batch loss = 2.01 (16.5 examples/sec; 0.485 sec/batch; 44h:34m:10s remains)
INFO - root - 2017-12-06 02:28:29.834904: step 1850, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.473 sec/batch; 43h:27m:57s remains)
INFO - root - 2017-12-06 02:28:34.602432: step 1860, loss = 2.01, batch loss = 1.95 (15.6 examples/sec; 0.514 sec/batch; 47h:09m:49s remains)
INFO - root - 2017-12-06 02:28:38.997579: step 1870, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 0.482 sec/batch; 44h:15m:54s remains)
INFO - root - 2017-12-06 02:28:43.696886: step 1880, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 0.488 sec/batch; 44h:47m:18s remains)
INFO - root - 2017-12-06 02:28:48.426548: step 1890, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.463 sec/batch; 42h:29m:09s remains)
INFO - root - 2017-12-06 02:28:53.135608: step 1900, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.468 sec/batch; 42h:57m:35s remains)
2017-12-06 02:28:53.622340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1961379 -4.1843433 -4.1731257 -4.1796813 -4.1975126 -4.2110777 -4.2176352 -4.2205486 -4.2305412 -4.2426138 -4.2394209 -4.223258 -4.2065611 -4.19516 -4.2087641][-4.1720829 -4.14281 -4.1176481 -4.1250038 -4.1580024 -4.1818008 -4.1900744 -4.189796 -4.2017612 -4.2201838 -4.221736 -4.2108707 -4.2070632 -4.2006197 -4.2078457][-4.16013 -4.1162896 -4.0742083 -4.069777 -4.1006126 -4.1316233 -4.1453204 -4.14694 -4.1613197 -4.1855354 -4.1883988 -4.1832123 -4.190558 -4.1903849 -4.1946592][-4.1576476 -4.1144543 -4.0611525 -4.0336046 -4.0391088 -4.0712972 -4.0939627 -4.1054635 -4.1257648 -4.1538782 -4.1614318 -4.1639686 -4.1770172 -4.1823416 -4.1885018][-4.135747 -4.1049604 -4.0595145 -4.0241914 -4.0082111 -4.0293269 -4.0560942 -4.077208 -4.1085162 -4.1395569 -4.1517744 -4.157927 -4.1738558 -4.185205 -4.1940532][-4.1109662 -4.1004825 -4.0731454 -4.0385556 -4.0066772 -4.0066767 -4.0245314 -4.0506935 -4.0886879 -4.1215768 -4.129981 -4.1320825 -4.1499877 -4.1679211 -4.1850491][-4.1049075 -4.1060009 -4.089632 -4.0632625 -4.0288587 -4.0083432 -4.0080409 -4.0249815 -4.0582685 -4.094018 -4.1038375 -4.1002007 -4.1167212 -4.1406584 -4.1700473][-4.1034818 -4.1087255 -4.0986114 -4.0801883 -4.0538163 -4.0251646 -4.0045576 -4.0028644 -4.0232635 -4.0591836 -4.0761685 -4.0714078 -4.0923166 -4.1229062 -4.1651444][-4.1020989 -4.1037855 -4.0936031 -4.079051 -4.0606222 -4.0334554 -4.0043659 -3.9870706 -3.9901402 -4.0228829 -4.0493083 -4.0532064 -4.0824652 -4.1173921 -4.1635118][-4.0972195 -4.0935273 -4.0797315 -4.0657334 -4.0540218 -4.0356145 -4.0080986 -3.9855318 -3.980679 -4.0076218 -4.038444 -4.0528345 -4.0910892 -4.1302638 -4.171998][-4.1089859 -4.1025825 -4.0863047 -4.06974 -4.058877 -4.0475688 -4.0304585 -4.0153584 -4.0084896 -4.022459 -4.0425434 -4.0541553 -4.0944915 -4.1389489 -4.1751781][-4.1427579 -4.1372905 -4.1233449 -4.1077204 -4.0984097 -4.0924311 -4.085104 -4.0776138 -4.0720377 -4.0742726 -4.0767832 -4.0768991 -4.1062822 -4.1440115 -4.1719952][-4.1863265 -4.1839557 -4.1753454 -4.1647005 -4.1589303 -4.1564431 -4.1540184 -4.1503668 -4.1481566 -4.1462865 -4.1396961 -4.1277294 -4.1406517 -4.1647406 -4.1822128][-4.2379837 -4.2365861 -4.2310257 -4.2244544 -4.2204328 -4.2195926 -4.2187905 -4.2167277 -4.2142406 -4.2113194 -4.2034678 -4.19089 -4.1936412 -4.2039719 -4.2117043][-4.276289 -4.2751822 -4.2724376 -4.2696943 -4.2680068 -4.267478 -4.2661619 -4.2641025 -4.2622118 -4.2596025 -4.253593 -4.2449121 -4.2436318 -4.246665 -4.2483492]]...]
INFO - root - 2017-12-06 02:28:58.421117: step 1910, loss = 2.04, batch loss = 1.98 (17.7 examples/sec; 0.452 sec/batch; 41h:30m:35s remains)
INFO - root - 2017-12-06 02:29:03.140917: step 1920, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.470 sec/batch; 43h:11m:04s remains)
INFO - root - 2017-12-06 02:29:07.878054: step 1930, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.476 sec/batch; 43h:44m:20s remains)
INFO - root - 2017-12-06 02:29:12.581758: step 1940, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.466 sec/batch; 42h:49m:42s remains)
INFO - root - 2017-12-06 02:29:17.190541: step 1950, loss = 2.08, batch loss = 2.02 (16.5 examples/sec; 0.484 sec/batch; 44h:26m:44s remains)
INFO - root - 2017-12-06 02:29:21.942463: step 1960, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.479 sec/batch; 44h:00m:48s remains)
INFO - root - 2017-12-06 02:29:26.478563: step 1970, loss = 2.09, batch loss = 2.03 (16.9 examples/sec; 0.474 sec/batch; 43h:29m:32s remains)
INFO - root - 2017-12-06 02:29:31.144399: step 1980, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.460 sec/batch; 42h:15m:32s remains)
INFO - root - 2017-12-06 02:29:35.784835: step 1990, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.469 sec/batch; 43h:02m:20s remains)
INFO - root - 2017-12-06 02:29:40.574610: step 2000, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.458 sec/batch; 42h:05m:00s remains)
2017-12-06 02:29:41.062569: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2597871 -4.2500191 -4.2513952 -4.2599344 -4.2633853 -4.26038 -4.2607751 -4.2608905 -4.2553377 -4.2546577 -4.2578182 -4.2583008 -4.2588987 -4.2620039 -4.270824][-4.2486405 -4.2335105 -4.2321949 -4.2410064 -4.2473607 -4.2448149 -4.2468143 -4.25003 -4.2469487 -4.2401872 -4.2332158 -4.2255354 -4.2215443 -4.2246995 -4.2408862][-4.2385621 -4.2242355 -4.2209005 -4.2274408 -4.2324553 -4.2257557 -4.2253861 -4.2311878 -4.2331858 -4.2232108 -4.2077622 -4.1933937 -4.1819677 -4.1791868 -4.1951156][-4.229156 -4.2176142 -4.2132282 -4.2164969 -4.2165108 -4.2026711 -4.1949115 -4.1991377 -4.21091 -4.20876 -4.193326 -4.1773057 -4.1586103 -4.1444988 -4.153306][-4.2148018 -4.2004452 -4.1933031 -4.191535 -4.1834245 -4.1598 -4.1399302 -4.1414037 -4.168088 -4.18234 -4.177114 -4.1658654 -4.1466193 -4.1272507 -4.131876][-4.1975908 -4.1760149 -4.1645665 -4.1609168 -4.1485572 -4.1105208 -4.0674267 -4.0567985 -4.1013961 -4.1435833 -4.1593819 -4.1595407 -4.1454639 -4.1283092 -4.13593][-4.20105 -4.17318 -4.1499672 -4.1385155 -4.1207504 -4.0688853 -3.9930091 -3.9559112 -4.0187597 -4.1000161 -4.1459522 -4.1591563 -4.1509066 -4.139801 -4.1528125][-4.2083259 -4.1753058 -4.1429677 -4.12405 -4.1024995 -4.0491924 -3.9607983 -3.903194 -3.9684451 -4.0691123 -4.1331949 -4.157496 -4.1579375 -4.15209 -4.1678696][-4.2066894 -4.1732054 -4.14408 -4.1304855 -4.1190486 -4.089448 -4.0291471 -3.9817128 -4.0200486 -4.093946 -4.1462636 -4.16616 -4.167675 -4.1643682 -4.1787291][-4.199264 -4.1714087 -4.1539316 -4.1515093 -4.1552377 -4.1530867 -4.1275816 -4.0968647 -4.1088533 -4.1476364 -4.1803656 -4.1919904 -4.1894569 -4.1836524 -4.193913][-4.1928124 -4.1721721 -4.164814 -4.171422 -4.182476 -4.191206 -4.1837711 -4.1620526 -4.1618409 -4.1841516 -4.20692 -4.2152467 -4.2135668 -4.2077847 -4.2126465][-4.202189 -4.1872787 -4.185472 -4.1940556 -4.2058034 -4.2169733 -4.2164135 -4.2015247 -4.1985631 -4.2149134 -4.2313066 -4.2383757 -4.2391338 -4.2356248 -4.2375426][-4.2261691 -4.2118306 -4.20885 -4.2138739 -4.2218122 -4.2317824 -4.2371244 -4.2304049 -4.2280955 -4.2394881 -4.250195 -4.2554846 -4.2588081 -4.2598367 -4.2629089][-4.2448583 -4.2284255 -4.2223687 -4.2219777 -4.2240777 -4.2322388 -4.2416348 -4.2422996 -4.2424836 -4.249289 -4.2560439 -4.2606921 -4.2652822 -4.2694283 -4.2738686][-4.263092 -4.248188 -4.2399225 -4.2343264 -4.2318516 -4.2366381 -4.2462354 -4.2513356 -4.2541432 -4.2600965 -4.2655077 -4.2694645 -4.2744637 -4.2777877 -4.2801809]]...]
INFO - root - 2017-12-06 02:29:45.793323: step 2010, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.476 sec/batch; 43h:40m:38s remains)
INFO - root - 2017-12-06 02:29:50.501809: step 2020, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 0.487 sec/batch; 44h:42m:26s remains)
INFO - root - 2017-12-06 02:29:55.153301: step 2030, loss = 2.04, batch loss = 1.98 (17.0 examples/sec; 0.471 sec/batch; 43h:14m:08s remains)
INFO - root - 2017-12-06 02:29:59.765046: step 2040, loss = 2.05, batch loss = 1.99 (16.5 examples/sec; 0.485 sec/batch; 44h:30m:51s remains)
INFO - root - 2017-12-06 02:30:04.375331: step 2050, loss = 2.08, batch loss = 2.02 (17.5 examples/sec; 0.456 sec/batch; 41h:51m:31s remains)
INFO - root - 2017-12-06 02:30:08.808581: step 2060, loss = 2.05, batch loss = 2.00 (19.3 examples/sec; 0.414 sec/batch; 37h:58m:26s remains)
INFO - root - 2017-12-06 02:30:13.581523: step 2070, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.457 sec/batch; 41h:55m:42s remains)
INFO - root - 2017-12-06 02:30:18.310767: step 2080, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.467 sec/batch; 42h:50m:24s remains)
INFO - root - 2017-12-06 02:30:23.070821: step 2090, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.470 sec/batch; 43h:07m:15s remains)
INFO - root - 2017-12-06 02:30:27.854401: step 2100, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.469 sec/batch; 43h:00m:04s remains)
2017-12-06 02:30:28.329934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3288226 -4.3124022 -4.2905445 -4.2604423 -4.2274756 -4.1990738 -4.18098 -4.1854496 -4.2051258 -4.2338066 -4.2667675 -4.29343 -4.3128667 -4.3311405 -4.34778][-4.3243279 -4.3054142 -4.2794886 -4.2414503 -4.2006907 -4.1708007 -4.1517925 -4.1546655 -4.1773925 -4.2091608 -4.2480669 -4.2820473 -4.3065495 -4.326159 -4.3439665][-4.3196626 -4.2996349 -4.2713013 -4.2243176 -4.1713061 -4.1376266 -4.1202221 -4.1236 -4.1470838 -4.1830812 -4.2287264 -4.2705989 -4.300055 -4.3200359 -4.3376331][-4.3156004 -4.2957478 -4.2676482 -4.2142005 -4.1472573 -4.1041489 -4.0890012 -4.0951939 -4.1179738 -4.15439 -4.205204 -4.255794 -4.2912874 -4.3125491 -4.3293228][-4.3125529 -4.2930694 -4.2672868 -4.2107649 -4.1306071 -4.0697484 -4.0551519 -4.0699539 -4.0947547 -4.1270027 -4.1778336 -4.2365212 -4.2806296 -4.3042111 -4.3207421][-4.3116031 -4.2926841 -4.2696705 -4.2142162 -4.122798 -4.0388536 -4.0182915 -4.0477414 -4.0817547 -4.1081171 -4.1547527 -4.2184377 -4.2697563 -4.296999 -4.3135843][-4.3126597 -4.2936659 -4.2727942 -4.2254887 -4.1347055 -4.0332241 -3.9966722 -4.0353045 -4.0803175 -4.1029034 -4.1427908 -4.2069755 -4.2609558 -4.2909317 -4.3081317][-4.3148923 -4.2977295 -4.2801166 -4.2464347 -4.1726851 -4.0700274 -4.0097642 -4.0380883 -4.0839663 -4.1069784 -4.1408257 -4.2008514 -4.2542462 -4.28581 -4.3044271][-4.3186178 -4.3048663 -4.2903166 -4.2690711 -4.2203059 -4.1380672 -4.0676742 -4.06517 -4.0931034 -4.1111321 -4.1393242 -4.1930881 -4.2466841 -4.2818584 -4.3030396][-4.3219943 -4.3131113 -4.2993731 -4.2847633 -4.2552214 -4.1980677 -4.1339989 -4.1050463 -4.1057529 -4.1126008 -4.1354856 -4.185811 -4.2412119 -4.2789721 -4.3019633][-4.322381 -4.3194036 -4.3089123 -4.2971034 -4.2767968 -4.2373824 -4.1869025 -4.1462994 -4.1250796 -4.1183996 -4.1339865 -4.1808395 -4.2365603 -4.2778597 -4.3028231][-4.3226738 -4.323936 -4.3192263 -4.3104639 -4.2941852 -4.2666545 -4.2313013 -4.1922588 -4.159368 -4.1384025 -4.1436729 -4.1839089 -4.2379241 -4.2814522 -4.3090038][-4.3267713 -4.3295712 -4.3284993 -4.3228769 -4.3075747 -4.2873316 -4.2641358 -4.2360177 -4.2041144 -4.1758194 -4.1709933 -4.20201 -4.2502394 -4.2917814 -4.3188248][-4.3307338 -4.3333015 -4.3330054 -4.3300371 -4.3171959 -4.3008013 -4.2864938 -4.2689781 -4.2477221 -4.2230148 -4.2131557 -4.2345877 -4.2722349 -4.3056321 -4.3272963][-4.3318381 -4.3333559 -4.3316174 -4.3279576 -4.3176017 -4.3044105 -4.2957234 -4.28685 -4.2772675 -4.2616129 -4.2537851 -4.2669239 -4.2917466 -4.3150787 -4.3299947]]...]
INFO - root - 2017-12-06 02:30:32.977831: step 2110, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.436 sec/batch; 40h:00m:04s remains)
INFO - root - 2017-12-06 02:30:37.587810: step 2120, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.467 sec/batch; 42h:51m:42s remains)
INFO - root - 2017-12-06 02:30:42.354770: step 2130, loss = 2.05, batch loss = 1.99 (16.7 examples/sec; 0.480 sec/batch; 44h:00m:17s remains)
INFO - root - 2017-12-06 02:30:46.986817: step 2140, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.450 sec/batch; 41h:18m:52s remains)
INFO - root - 2017-12-06 02:30:51.640146: step 2150, loss = 2.08, batch loss = 2.02 (17.2 examples/sec; 0.464 sec/batch; 42h:36m:37s remains)
INFO - root - 2017-12-06 02:30:56.151243: step 2160, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.470 sec/batch; 43h:05m:15s remains)
INFO - root - 2017-12-06 02:31:00.905367: step 2170, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.466 sec/batch; 42h:46m:37s remains)
INFO - root - 2017-12-06 02:31:05.571505: step 2180, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 0.481 sec/batch; 44h:05m:27s remains)
INFO - root - 2017-12-06 02:31:10.266466: step 2190, loss = 2.04, batch loss = 1.98 (17.1 examples/sec; 0.468 sec/batch; 42h:56m:50s remains)
INFO - root - 2017-12-06 02:31:15.041894: step 2200, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.462 sec/batch; 42h:22m:33s remains)
2017-12-06 02:31:15.479574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2048917 -4.214889 -4.2191887 -4.2220459 -4.225183 -4.2296209 -4.2335844 -4.2359972 -4.2378769 -4.2380757 -4.2364607 -4.2354159 -4.2342377 -4.2296329 -4.2254515][-4.1931562 -4.2057323 -4.2136621 -4.2186913 -4.2218575 -4.2253847 -4.2285218 -4.2304354 -4.231688 -4.2332335 -4.2339149 -4.2321444 -4.2281165 -4.2204566 -4.2118406][-4.2171488 -4.232429 -4.2417583 -4.2446795 -4.2436786 -4.2415915 -4.2411103 -4.2412395 -4.2422123 -4.2461796 -4.2512436 -4.2519922 -4.2493343 -4.2430739 -4.2338157][-4.2412338 -4.2535353 -4.2597728 -4.2586832 -4.2536278 -4.2452464 -4.2384448 -4.2338681 -4.2326016 -4.2408285 -4.2540178 -4.2601476 -4.2626252 -4.2642431 -4.26259][-4.2444787 -4.2503376 -4.2485347 -4.2395229 -4.2284908 -4.2118025 -4.1962328 -4.1840062 -4.1788206 -4.1950817 -4.2214231 -4.23699 -4.248436 -4.2612529 -4.2720375][-4.2149649 -4.2109885 -4.1978765 -4.1743321 -4.1491785 -4.1191216 -4.0923285 -4.0702181 -4.0635972 -4.0929193 -4.1365719 -4.1676879 -4.1945324 -4.2223911 -4.2490358][-4.1656709 -4.1483183 -4.1231885 -4.0839014 -4.0403171 -3.9941678 -3.9561694 -3.9245026 -3.9176486 -3.9619563 -4.0227461 -4.0688567 -4.1107497 -4.1524639 -4.19489][-4.14755 -4.1222749 -4.0945129 -4.0538239 -4.0070395 -3.9631748 -3.9322596 -3.9052362 -3.8960152 -3.9342885 -3.9888656 -4.0276303 -4.0635881 -4.1041374 -4.1525369][-4.1876488 -4.1685095 -4.14806 -4.1193514 -4.0881166 -4.0632215 -4.0478621 -4.0330791 -4.0235777 -4.0428672 -4.0728054 -4.0908365 -4.1077762 -4.1346307 -4.1755743][-4.2529316 -4.2436166 -4.2315378 -4.2147985 -4.1980653 -4.185266 -4.1777406 -4.1718764 -4.1679406 -4.1787548 -4.1939521 -4.2008224 -4.2074852 -4.2225986 -4.2486181][-4.3020554 -4.2998486 -4.2951713 -4.286603 -4.2771039 -4.2702217 -4.2667155 -4.2644548 -4.2640252 -4.2720609 -4.2810335 -4.2855034 -4.2898211 -4.2999215 -4.3135056][-4.322279 -4.3208718 -4.3182364 -4.3126893 -4.3063426 -4.3022432 -4.3000255 -4.2995 -4.3022528 -4.3090405 -4.3160238 -4.3211145 -4.3247914 -4.3301845 -4.3362393][-4.32986 -4.3256383 -4.32128 -4.3158646 -4.3094044 -4.3044291 -4.3022442 -4.30348 -4.3085413 -4.3170118 -4.3258853 -4.3320112 -4.3344808 -4.3367534 -4.3399315][-4.3370991 -4.3337731 -4.3298559 -4.3261037 -4.3211646 -4.31715 -4.315001 -4.3162508 -4.3220105 -4.3300414 -4.3375793 -4.3416104 -4.3406925 -4.3388109 -4.3391314][-4.3454957 -4.3454962 -4.3431287 -4.3395705 -4.3330417 -4.3272057 -4.3234367 -4.3233232 -4.3278866 -4.3348866 -4.3412967 -4.3441935 -4.3413858 -4.3370113 -4.3346405]]...]
INFO - root - 2017-12-06 02:31:20.144969: step 2210, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 0.472 sec/batch; 43h:16m:35s remains)
INFO - root - 2017-12-06 02:31:24.881041: step 2220, loss = 2.04, batch loss = 1.99 (16.4 examples/sec; 0.486 sec/batch; 44h:37m:11s remains)
INFO - root - 2017-12-06 02:31:29.623161: step 2230, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.479 sec/batch; 43h:57m:12s remains)
INFO - root - 2017-12-06 02:31:34.253796: step 2240, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.461 sec/batch; 42h:19m:25s remains)
INFO - root - 2017-12-06 02:31:38.997656: step 2250, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.460 sec/batch; 42h:10m:26s remains)
INFO - root - 2017-12-06 02:31:43.491401: step 2260, loss = 2.06, batch loss = 2.00 (16.6 examples/sec; 0.482 sec/batch; 44h:11m:18s remains)
INFO - root - 2017-12-06 02:31:48.204860: step 2270, loss = 2.08, batch loss = 2.02 (17.3 examples/sec; 0.462 sec/batch; 42h:22m:52s remains)
INFO - root - 2017-12-06 02:31:52.946145: step 2280, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.476 sec/batch; 43h:39m:22s remains)
INFO - root - 2017-12-06 02:31:57.645612: step 2290, loss = 2.06, batch loss = 2.00 (16.2 examples/sec; 0.495 sec/batch; 45h:22m:22s remains)
INFO - root - 2017-12-06 02:32:02.227312: step 2300, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.474 sec/batch; 43h:25m:55s remains)
2017-12-06 02:32:02.706449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0736175 -3.9749849 -3.9440958 -3.9848523 -4.0445738 -4.0895543 -4.1052108 -4.105124 -4.1034794 -4.110043 -4.119504 -4.1270971 -4.1444154 -4.1638565 -4.1772184][-4.0837717 -3.9953022 -3.9626558 -3.9946404 -4.0462341 -4.0853276 -4.1029134 -4.1126075 -4.113719 -4.1143265 -4.1141715 -4.1099644 -4.1218672 -4.1440215 -4.1629024][-4.0951648 -4.0216093 -3.9946153 -4.0176568 -4.0615554 -4.0948882 -4.1150856 -4.1279192 -4.1265397 -4.1170449 -4.1068954 -4.0954404 -4.1023183 -4.1181912 -4.1370058][-4.1152954 -4.0561371 -4.0344834 -4.0461669 -4.0773516 -4.1091332 -4.1351418 -4.1536407 -4.1514735 -4.1336751 -4.1176906 -4.1058683 -4.1081505 -4.1126094 -4.1230273][-4.1377115 -4.0882082 -4.06752 -4.0695844 -4.0887179 -4.1180873 -4.1487265 -4.1726484 -4.177546 -4.161612 -4.1480742 -4.1427727 -4.1413431 -4.1347995 -4.136507][-4.1567492 -4.1149421 -4.0985522 -4.0939331 -4.1008143 -4.123035 -4.1500487 -4.1737947 -4.1841569 -4.1775513 -4.1719956 -4.1748815 -4.1744547 -4.1647334 -4.1616578][-4.1644378 -4.1262054 -4.1115522 -4.1026883 -4.1009216 -4.1153741 -4.1372008 -4.155128 -4.1636968 -4.1646028 -4.1675406 -4.1746159 -4.1800895 -4.1770754 -4.1746655][-4.1642489 -4.1273084 -4.1119356 -4.1021385 -4.0962214 -4.099144 -4.1073184 -4.112072 -4.1209393 -4.1321912 -4.1451631 -4.1549969 -4.1634946 -4.1648235 -4.1635404][-4.1623297 -4.12813 -4.1182528 -4.1137929 -4.1027632 -4.0905633 -4.0840354 -4.0778737 -4.0889621 -4.1106195 -4.1299515 -4.1395092 -4.14769 -4.1517258 -4.1505127][-4.1599565 -4.1286254 -4.1268616 -4.1323347 -4.1259913 -4.1058087 -4.0843658 -4.0723619 -4.0871015 -4.1156397 -4.1376972 -4.1468706 -4.1532183 -4.1527038 -4.1422892][-4.160459 -4.1248198 -4.1237154 -4.1349721 -4.140955 -4.1267033 -4.0997863 -4.0809345 -4.09063 -4.1190534 -4.1425385 -4.1515465 -4.1535778 -4.144948 -4.1214561][-4.1630182 -4.120708 -4.115222 -4.1288013 -4.1460252 -4.1429954 -4.1180534 -4.093411 -4.0919118 -4.1076169 -4.1241879 -4.132194 -4.13225 -4.1215825 -4.09321][-4.1700225 -4.1238909 -4.1127086 -4.1249938 -4.1477194 -4.1533213 -4.1339765 -4.1055946 -4.0870905 -4.0795908 -4.0828667 -4.09447 -4.104805 -4.1034613 -4.0819817][-4.1765275 -4.1330132 -4.1165962 -4.1260405 -4.1515856 -4.1626716 -4.1534357 -4.1289425 -4.0997419 -4.0756226 -4.0713792 -4.0859213 -4.1021171 -4.1019087 -4.0872149][-4.1867838 -4.1532559 -4.1392512 -4.1455755 -4.1657534 -4.1771173 -4.1777987 -4.1656666 -4.1403675 -4.11088 -4.0982919 -4.1037583 -4.1092687 -4.0983486 -4.0835562]]...]
INFO - root - 2017-12-06 02:32:07.453335: step 2310, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.476 sec/batch; 43h:42m:09s remains)
INFO - root - 2017-12-06 02:32:12.166291: step 2320, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.469 sec/batch; 43h:03m:21s remains)
INFO - root - 2017-12-06 02:32:16.840236: step 2330, loss = 2.08, batch loss = 2.02 (16.5 examples/sec; 0.484 sec/batch; 44h:24m:52s remains)
INFO - root - 2017-12-06 02:32:21.506479: step 2340, loss = 2.06, batch loss = 2.01 (16.7 examples/sec; 0.479 sec/batch; 43h:54m:16s remains)
INFO - root - 2017-12-06 02:32:26.022113: step 2350, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.479 sec/batch; 43h:55m:29s remains)
INFO - root - 2017-12-06 02:32:30.808625: step 2360, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.474 sec/batch; 43h:26m:59s remains)
INFO - root - 2017-12-06 02:32:35.485497: step 2370, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.473 sec/batch; 43h:24m:03s remains)
INFO - root - 2017-12-06 02:32:40.317614: step 2380, loss = 2.05, batch loss = 1.99 (16.6 examples/sec; 0.481 sec/batch; 44h:07m:56s remains)
INFO - root - 2017-12-06 02:32:45.011064: step 2390, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.463 sec/batch; 42h:25m:23s remains)
INFO - root - 2017-12-06 02:32:49.639534: step 2400, loss = 2.03, batch loss = 1.97 (17.3 examples/sec; 0.462 sec/batch; 42h:21m:05s remains)
2017-12-06 02:32:50.082100: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0800695 -4.0984359 -4.1212521 -4.1498356 -4.1748838 -4.201046 -4.2203021 -4.2272205 -4.2273831 -4.2312527 -4.2367034 -4.233943 -4.2267051 -4.2194276 -4.2163258][-4.1057329 -4.1151018 -4.1310349 -4.15876 -4.1841016 -4.2133651 -4.232235 -4.2367411 -4.230926 -4.2286158 -4.228806 -4.219593 -4.2108297 -4.2031217 -4.2022057][-4.1292481 -4.137383 -4.1506367 -4.175138 -4.1946249 -4.2150993 -4.2243533 -4.221694 -4.2111092 -4.2078996 -4.2076416 -4.1974111 -4.1885309 -4.1774826 -4.175004][-4.1414232 -4.1509819 -4.1626229 -4.1782875 -4.1873012 -4.1915526 -4.1875443 -4.1734223 -4.1583338 -4.161418 -4.1704869 -4.1711411 -4.1697979 -4.1628337 -4.1578565][-4.1386175 -4.1450582 -4.1519055 -4.1573339 -4.1537838 -4.1369648 -4.110219 -4.0790668 -4.0655522 -4.0860915 -4.1186886 -4.1422548 -4.158514 -4.1653786 -4.1602478][-4.1195951 -4.1209893 -4.1216097 -4.1173639 -4.10075 -4.06108 -4.0046968 -3.9547477 -3.9510467 -3.9999352 -4.0631652 -4.1161275 -4.1550384 -4.1758871 -4.1730943][-4.1069679 -4.1037207 -4.1009111 -4.0927186 -4.06528 -4.01075 -3.9379952 -3.8817217 -3.8913527 -3.9571991 -4.0335588 -4.1017408 -4.1541567 -4.1823354 -4.1824365][-4.122375 -4.1181312 -4.1166387 -4.1130533 -4.0874233 -4.0388246 -3.9806373 -3.9420536 -3.95146 -3.9959109 -4.0536747 -4.1111903 -4.1596093 -4.1890292 -4.1920919][-4.1560521 -4.1467676 -4.1468062 -4.1485157 -4.1366076 -4.1066289 -4.0751972 -4.0568094 -4.0575638 -4.0730062 -4.103364 -4.140152 -4.1778655 -4.2042708 -4.2071095][-4.19451 -4.1781254 -4.1717248 -4.1740603 -4.1755795 -4.1651816 -4.1564045 -4.1498137 -4.1424813 -4.1367111 -4.1453333 -4.165267 -4.1923127 -4.2120433 -4.2134037][-4.2194328 -4.2016621 -4.1882892 -4.1894875 -4.1991172 -4.2041564 -4.2089472 -4.2024322 -4.1862888 -4.1680226 -4.1628366 -4.1716528 -4.189909 -4.202508 -4.2042446][-4.2252989 -4.2161546 -4.2047505 -4.2077208 -4.2206831 -4.2301474 -4.2354236 -4.22193 -4.1994061 -4.1743379 -4.15611 -4.15247 -4.1609793 -4.1662612 -4.1682005][-4.2143736 -4.2166595 -4.2146058 -4.2202096 -4.2324862 -4.2404132 -4.2420135 -4.224287 -4.1972785 -4.1659813 -4.1334696 -4.1121 -4.1075091 -4.1073952 -4.1159196][-4.1823421 -4.1938291 -4.2024636 -4.211803 -4.224081 -4.2324853 -4.2325377 -4.2148876 -4.1882486 -4.15656 -4.1202903 -4.0903945 -4.0783167 -4.0781822 -4.0968451][-4.14746 -4.1616726 -4.1757545 -4.1865053 -4.1947312 -4.1998658 -4.1976686 -4.1828251 -4.1639209 -4.14487 -4.122869 -4.1024995 -4.0945854 -4.0965881 -4.1213841]]...]
INFO - root - 2017-12-06 02:32:54.728499: step 2410, loss = 2.08, batch loss = 2.02 (17.3 examples/sec; 0.462 sec/batch; 42h:23m:07s remains)
INFO - root - 2017-12-06 02:32:59.426962: step 2420, loss = 2.08, batch loss = 2.02 (16.5 examples/sec; 0.486 sec/batch; 44h:31m:09s remains)
INFO - root - 2017-12-06 02:33:04.167685: step 2430, loss = 2.04, batch loss = 1.98 (16.8 examples/sec; 0.477 sec/batch; 43h:41m:47s remains)
INFO - root - 2017-12-06 02:33:08.760629: step 2440, loss = 2.04, batch loss = 1.98 (18.6 examples/sec; 0.430 sec/batch; 39h:27m:41s remains)
INFO - root - 2017-12-06 02:33:13.243635: step 2450, loss = 2.03, batch loss = 1.97 (16.9 examples/sec; 0.474 sec/batch; 43h:27m:57s remains)
INFO - root - 2017-12-06 02:33:17.952544: step 2460, loss = 2.05, batch loss = 1.99 (17.2 examples/sec; 0.464 sec/batch; 42h:34m:03s remains)
INFO - root - 2017-12-06 02:33:22.615268: step 2470, loss = 2.05, batch loss = 1.99 (16.4 examples/sec; 0.487 sec/batch; 44h:40m:12s remains)
INFO - root - 2017-12-06 02:33:27.297678: step 2480, loss = 2.05, batch loss = 2.00 (17.4 examples/sec; 0.461 sec/batch; 42h:15m:41s remains)
INFO - root - 2017-12-06 02:33:32.096200: step 2490, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.480 sec/batch; 43h:57m:23s remains)
INFO - root - 2017-12-06 02:33:36.802838: step 2500, loss = 2.05, batch loss = 1.99 (17.2 examples/sec; 0.465 sec/batch; 42h:39m:42s remains)
2017-12-06 02:33:37.264574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2790031 -4.2741137 -4.2746983 -4.2780795 -4.2830019 -4.2851982 -4.2886047 -4.2943058 -4.3024921 -4.3120103 -4.31352 -4.3045382 -4.2938776 -4.2898974 -4.2917223][-4.2533693 -4.2426605 -4.2414484 -4.2463465 -4.2491016 -4.2479615 -4.2487297 -4.2571611 -4.27406 -4.2928576 -4.3001509 -4.2914362 -4.2772012 -4.2690659 -4.2680578][-4.2161317 -4.194726 -4.1876807 -4.1910405 -4.1930676 -4.194169 -4.1923666 -4.1972532 -4.2218256 -4.2542162 -4.2701178 -4.2654963 -4.2514715 -4.2421803 -4.2390471][-4.1695366 -4.1359472 -4.1224766 -4.1243258 -4.1281614 -4.1313558 -4.1230035 -4.120748 -4.1488914 -4.1969366 -4.2261553 -4.2273512 -4.2162342 -4.2077947 -4.2051878][-4.1420603 -4.1070347 -4.0898614 -4.0865636 -4.0880547 -4.0876031 -4.0672965 -4.050849 -4.0741014 -4.1327248 -4.1792483 -4.188386 -4.1817565 -4.1792173 -4.1789455][-4.1212058 -4.0874882 -4.068933 -4.0538068 -4.042737 -4.0343089 -4.0038791 -3.9678032 -3.9751656 -4.0451126 -4.1199756 -4.1469502 -4.1495028 -4.1566577 -4.1667328][-4.1110549 -4.0757117 -4.0568571 -4.0372553 -4.0134587 -3.9880772 -3.9422987 -3.8772871 -3.8558369 -3.927484 -4.0416918 -4.1019158 -4.1221495 -4.1390963 -4.1559162][-4.1280222 -4.0957828 -4.0795455 -4.0580974 -4.0256987 -3.9884763 -3.9374666 -3.8625264 -3.8250751 -3.8868473 -4.0085511 -4.0815272 -4.1102371 -4.1332507 -4.1570144][-4.1611919 -4.1300249 -4.1140795 -4.0944662 -4.0622358 -4.0281496 -3.9915977 -3.9384937 -3.9127016 -3.9551628 -4.0417418 -4.0937605 -4.1160593 -4.1360159 -4.1625605][-4.188313 -4.1594272 -4.1465235 -4.1311469 -4.1067309 -4.0828075 -4.0572696 -4.0197678 -3.9965541 -4.0118451 -4.0572143 -4.0852356 -4.1097617 -4.1338768 -4.162693][-4.1865363 -4.1539841 -4.1405396 -4.1348081 -4.1270571 -4.1171074 -4.1064625 -4.0845895 -4.0617332 -4.0544577 -4.0661459 -4.0735869 -4.0972614 -4.1253409 -4.1536202][-4.1865644 -4.1543379 -4.1387095 -4.1353769 -4.1381755 -4.1398287 -4.1412144 -4.1348195 -4.1230626 -4.1119156 -4.1055851 -4.0957565 -4.1119123 -4.1411939 -4.1692815][-4.2107124 -4.1866488 -4.1715174 -4.1680384 -4.1725554 -4.177834 -4.1863375 -4.1904931 -4.1886697 -4.1835656 -4.1691055 -4.1492944 -4.1580405 -4.1838017 -4.2071729][-4.2399111 -4.225482 -4.2154469 -4.2135687 -4.2170124 -4.2232847 -4.2336845 -4.2411261 -4.2450957 -4.2493014 -4.239264 -4.2209244 -4.2242546 -4.2438459 -4.2604218][-4.2749233 -4.2687545 -4.2643557 -4.2643313 -4.2675037 -4.2746978 -4.2864728 -4.2922997 -4.2979369 -4.3065195 -4.301754 -4.290987 -4.2925782 -4.304265 -4.3130722]]...]
INFO - root - 2017-12-06 02:33:41.970222: step 2510, loss = 2.05, batch loss = 1.99 (16.7 examples/sec; 0.478 sec/batch; 43h:50m:42s remains)
INFO - root - 2017-12-06 02:33:46.624144: step 2520, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.451 sec/batch; 41h:21m:00s remains)
INFO - root - 2017-12-06 02:33:51.343962: step 2530, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.464 sec/batch; 42h:33m:35s remains)
INFO - root - 2017-12-06 02:33:55.870088: step 2540, loss = 2.05, batch loss = 1.99 (26.7 examples/sec; 0.300 sec/batch; 27h:30m:36s remains)
INFO - root - 2017-12-06 02:34:00.643738: step 2550, loss = 2.05, batch loss = 1.99 (16.4 examples/sec; 0.488 sec/batch; 44h:41m:53s remains)
INFO - root - 2017-12-06 02:34:05.216994: step 2560, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.443 sec/batch; 40h:33m:24s remains)
INFO - root - 2017-12-06 02:34:09.891851: step 2570, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.451 sec/batch; 41h:18m:59s remains)
INFO - root - 2017-12-06 02:34:14.649776: step 2580, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.469 sec/batch; 43h:00m:02s remains)
INFO - root - 2017-12-06 02:34:19.318610: step 2590, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.463 sec/batch; 42h:26m:35s remains)
INFO - root - 2017-12-06 02:34:24.101572: step 2600, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.469 sec/batch; 42h:59m:08s remains)
2017-12-06 02:34:24.562037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.18008 -4.1759434 -4.166327 -4.163084 -4.1691613 -4.1808491 -4.1917615 -4.1958575 -4.1918268 -4.1761456 -4.1593418 -4.1337461 -4.110846 -4.1216383 -4.1470876][-4.1639 -4.15592 -4.1490154 -4.1556034 -4.1691904 -4.1858535 -4.2005854 -4.2042727 -4.197329 -4.1787105 -4.1553597 -4.1249795 -4.1053166 -4.1152282 -4.1339741][-4.1591825 -4.1546226 -4.1561246 -4.1748548 -4.1931558 -4.2049642 -4.2115383 -4.2038889 -4.1909704 -4.1734252 -4.1499777 -4.1207933 -4.0984635 -4.0961709 -4.0988884][-4.1606708 -4.1638288 -4.1748509 -4.2029767 -4.2205267 -4.2224412 -4.2162662 -4.1960292 -4.1793542 -4.170011 -4.1560345 -4.1325774 -4.1048031 -4.0860844 -4.0713387][-4.1690731 -4.1793451 -4.195868 -4.2233629 -4.23493 -4.230813 -4.2160215 -4.1870928 -4.164804 -4.1625605 -4.1605978 -4.1453586 -4.1164904 -4.08588 -4.0619383][-4.1765223 -4.1893106 -4.2051659 -4.2249227 -4.2298384 -4.2207479 -4.2003393 -4.1653275 -4.1355872 -4.1328478 -4.1391783 -4.1336021 -4.1127129 -4.0882812 -4.0732679][-4.1628881 -4.1825356 -4.1996665 -4.2114415 -4.209578 -4.194438 -4.16933 -4.13117 -4.0992494 -4.0965471 -4.1081042 -4.114686 -4.1135955 -4.1072273 -4.1069155][-4.1465559 -4.1795459 -4.2013369 -4.2071347 -4.1987386 -4.1763897 -4.1483893 -4.1156249 -4.0906305 -4.0915861 -4.1010542 -4.1125293 -4.1262107 -4.1316662 -4.1372905][-4.1347222 -4.1756635 -4.2019639 -4.2065535 -4.1971903 -4.1736441 -4.1466589 -4.1203556 -4.1051049 -4.11271 -4.1219063 -4.1339021 -4.1520491 -4.1592093 -4.1661735][-4.1374331 -4.1769304 -4.2029514 -4.2093334 -4.2026772 -4.1813378 -4.1570277 -4.1351013 -4.12809 -4.1421556 -4.1567154 -4.1661768 -4.1753988 -4.1756353 -4.1791177][-4.1503949 -4.1790123 -4.1974463 -4.2054572 -4.206336 -4.1924233 -4.1712608 -4.1523223 -4.1497865 -4.1675963 -4.18373 -4.1901345 -4.1888628 -4.1800213 -4.1815858][-4.1622343 -4.1821861 -4.1946793 -4.2048855 -4.2124038 -4.2049847 -4.1885505 -4.1735992 -4.173975 -4.191884 -4.2059417 -4.2089276 -4.2012744 -4.1889882 -4.1908851][-4.1831713 -4.1958952 -4.204329 -4.2140436 -4.2231841 -4.222177 -4.2116137 -4.2011638 -4.2022624 -4.2163796 -4.226624 -4.2271547 -4.2168832 -4.2043686 -4.2050581][-4.21848 -4.2226448 -4.2267728 -4.233748 -4.241981 -4.2438273 -4.2383347 -4.2322779 -4.2339921 -4.2431254 -4.2500057 -4.2492666 -4.2408862 -4.2329011 -4.234024][-4.2533236 -4.2528305 -4.2536755 -4.2570477 -4.263092 -4.2661748 -4.2640209 -4.2613459 -4.2636681 -4.2691917 -4.2736516 -4.2737455 -4.2685442 -4.2647181 -4.2655134]]...]
INFO - root - 2017-12-06 02:34:29.230484: step 2610, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.445 sec/batch; 40h:46m:42s remains)
INFO - root - 2017-12-06 02:34:33.895642: step 2620, loss = 2.05, batch loss = 1.99 (17.4 examples/sec; 0.458 sec/batch; 42h:00m:43s remains)
INFO - root - 2017-12-06 02:34:38.525254: step 2630, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.476 sec/batch; 43h:37m:04s remains)
INFO - root - 2017-12-06 02:34:43.076316: step 2640, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 42h:38m:58s remains)
INFO - root - 2017-12-06 02:34:47.858902: step 2650, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.469 sec/batch; 43h:00m:47s remains)
INFO - root - 2017-12-06 02:34:52.544233: step 2660, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.476 sec/batch; 43h:36m:36s remains)
INFO - root - 2017-12-06 02:34:57.274173: step 2670, loss = 2.10, batch loss = 2.04 (15.7 examples/sec; 0.508 sec/batch; 46h:32m:58s remains)
INFO - root - 2017-12-06 02:35:02.012270: step 2680, loss = 2.08, batch loss = 2.02 (17.2 examples/sec; 0.466 sec/batch; 42h:41m:12s remains)
INFO - root - 2017-12-06 02:35:06.676986: step 2690, loss = 2.07, batch loss = 2.02 (17.1 examples/sec; 0.468 sec/batch; 42h:51m:25s remains)
INFO - root - 2017-12-06 02:35:11.439374: step 2700, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.472 sec/batch; 43h:12m:53s remains)
2017-12-06 02:35:11.898365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1432495 -4.1405039 -4.1712193 -4.2080431 -4.2357678 -4.2492309 -4.2556076 -4.2624664 -4.2668028 -4.2603393 -4.23598 -4.1939273 -4.1550121 -4.1419492 -4.1513515][-4.1126022 -4.0997968 -4.132463 -4.1750402 -4.2122722 -4.2350206 -4.2456694 -4.2570319 -4.2685628 -4.269166 -4.2433915 -4.1853981 -4.1247473 -4.10008 -4.1081409][-4.0787024 -4.0626478 -4.1009455 -4.1496792 -4.1915822 -4.2130284 -4.2159944 -4.2245398 -4.2454429 -4.2586188 -4.238421 -4.1758871 -4.1048293 -4.0716052 -4.0785942][-4.0766292 -4.0657668 -4.1092491 -4.1547141 -4.1862116 -4.1900826 -4.1701179 -4.1693592 -4.2043066 -4.2416992 -4.2422872 -4.1946044 -4.1334138 -4.1017351 -4.1067443][-4.1171637 -4.1104317 -4.1462088 -4.1759453 -4.188324 -4.1658463 -4.112792 -4.092227 -4.1383185 -4.2119465 -4.2504354 -4.2346873 -4.1923895 -4.1647167 -4.1669292][-4.1547246 -4.1544561 -4.18169 -4.1943693 -4.186204 -4.1291909 -4.0339222 -3.9853344 -4.0476613 -4.1581259 -4.2374511 -4.254149 -4.2332807 -4.2145367 -4.2148314][-4.1874447 -4.1923571 -4.2052836 -4.194623 -4.1567211 -4.0528326 -3.8971581 -3.8103659 -3.8953629 -4.0500231 -4.1722159 -4.2250814 -4.2346449 -4.2359796 -4.2442083][-4.2055674 -4.2090755 -4.2035117 -4.1685457 -4.1005712 -3.9545126 -3.7404327 -3.6128056 -3.7264853 -3.9325736 -4.0942273 -4.1784554 -4.2153549 -4.2361603 -4.2540193][-4.1982851 -4.20071 -4.1872449 -4.149879 -4.0793271 -3.9287791 -3.7090726 -3.5845671 -3.7102308 -3.924119 -4.0882187 -4.1742864 -4.2159081 -4.240767 -4.2626276][-4.1794853 -4.1900392 -4.1895995 -4.1724234 -4.1241479 -4.0041046 -3.8289733 -3.7349119 -3.8309078 -4.0002427 -4.1312528 -4.2006011 -4.2359591 -4.2579007 -4.2757063][-4.1597519 -4.1800146 -4.2002149 -4.2095938 -4.1852655 -4.0981059 -3.9710517 -3.9039159 -3.965836 -4.0833616 -4.1781764 -4.2304673 -4.2594824 -4.2775879 -4.2892709][-4.1386762 -4.1650877 -4.2061782 -4.2369866 -4.23062 -4.1729364 -4.0881042 -4.0438719 -4.0789471 -4.1544757 -4.2194858 -4.2587261 -4.2860808 -4.302351 -4.3079529][-4.125906 -4.1557679 -4.2109756 -4.250793 -4.25849 -4.2315712 -4.1869764 -4.1630487 -4.1798224 -4.22215 -4.2608361 -4.2840042 -4.3038859 -4.3179865 -4.3195934][-4.1282396 -4.1571841 -4.2157116 -4.2559466 -4.2743006 -4.2721043 -4.2587652 -4.2512589 -4.2577224 -4.2758274 -4.2914696 -4.3024321 -4.3143196 -4.325665 -4.3261967][-4.1455288 -4.1702809 -4.224575 -4.2623544 -4.285327 -4.2945647 -4.3009953 -4.3021755 -4.302774 -4.3061366 -4.3076506 -4.3090572 -4.3151393 -4.3225279 -4.3231363]]...]
INFO - root - 2017-12-06 02:35:16.699793: step 2710, loss = 2.11, batch loss = 2.05 (17.5 examples/sec; 0.456 sec/batch; 41h:47m:43s remains)
INFO - root - 2017-12-06 02:35:21.341201: step 2720, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.464 sec/batch; 42h:32m:46s remains)
INFO - root - 2017-12-06 02:35:26.009243: step 2730, loss = 2.05, batch loss = 1.99 (17.2 examples/sec; 0.464 sec/batch; 42h:29m:48s remains)
INFO - root - 2017-12-06 02:35:30.463104: step 2740, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.461 sec/batch; 42h:11m:31s remains)
INFO - root - 2017-12-06 02:35:35.188487: step 2750, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.469 sec/batch; 42h:56m:39s remains)
INFO - root - 2017-12-06 02:35:39.836686: step 2760, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.461 sec/batch; 42h:12m:41s remains)
INFO - root - 2017-12-06 02:35:44.612856: step 2770, loss = 2.05, batch loss = 2.00 (16.4 examples/sec; 0.487 sec/batch; 44h:35m:03s remains)
INFO - root - 2017-12-06 02:35:49.293648: step 2780, loss = 2.04, batch loss = 1.98 (17.4 examples/sec; 0.460 sec/batch; 42h:07m:24s remains)
INFO - root - 2017-12-06 02:35:53.906312: step 2790, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.451 sec/batch; 41h:19m:22s remains)
INFO - root - 2017-12-06 02:35:58.542335: step 2800, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.463 sec/batch; 42h:24m:02s remains)
2017-12-06 02:35:59.018764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32381 -4.3277988 -4.3337526 -4.3375444 -4.3405724 -4.3423777 -4.3411179 -4.3371792 -4.3343029 -4.3341284 -4.336 -4.3393693 -4.3425679 -4.3448915 -4.3465981][-4.3036866 -4.3081408 -4.3179436 -4.3249736 -4.3310204 -4.3337259 -4.3333011 -4.3281131 -4.3258505 -4.3262382 -4.3298488 -4.3352046 -4.3402181 -4.342854 -4.3448567][-4.2752452 -4.2813072 -4.295929 -4.3067102 -4.3167458 -4.3199806 -4.3188963 -4.3117003 -4.3112845 -4.3141894 -4.3204966 -4.3286662 -4.3360562 -4.3396235 -4.3411722][-4.2390342 -4.2445974 -4.2604632 -4.2716551 -4.285316 -4.2907639 -4.2886252 -4.2801361 -4.2817144 -4.2903075 -4.3036976 -4.3183732 -4.3303428 -4.3362861 -4.3378043][-4.2012086 -4.2006664 -4.2112918 -4.2195292 -4.2340388 -4.2401447 -4.23306 -4.2239223 -4.23019 -4.2456217 -4.2686644 -4.2914624 -4.30911 -4.3198886 -4.3258839][-4.1630549 -4.1557469 -4.157196 -4.1567197 -4.165556 -4.1629577 -4.1424541 -4.1312852 -4.1460967 -4.1723833 -4.2075839 -4.240737 -4.2656693 -4.2830372 -4.2987685][-4.1228952 -4.1070294 -4.0961 -4.0820327 -4.075758 -4.0517864 -4.0050945 -3.9876189 -4.0175858 -4.0630088 -4.1176882 -4.1673765 -4.2041802 -4.232811 -4.2635908][-4.10077 -4.0783825 -4.0533242 -4.0267816 -4.0053067 -3.9567547 -3.8837047 -3.8550987 -3.8931236 -3.9530556 -4.0250435 -4.0960937 -4.1486969 -4.1921353 -4.23847][-4.1356349 -4.1230145 -4.1037917 -4.0852332 -4.0748248 -4.0396667 -3.9856241 -3.9581873 -3.9775887 -4.0175338 -4.0688806 -4.1249733 -4.1682549 -4.2065916 -4.2483144][-4.1826329 -4.1785641 -4.17226 -4.17046 -4.1756439 -4.1581912 -4.1260953 -4.1060925 -4.1148286 -4.139421 -4.1715641 -4.2071242 -4.234231 -4.2567182 -4.2819605][-4.2322283 -4.2323837 -4.234859 -4.2399349 -4.2481756 -4.2407336 -4.2240057 -4.2128253 -4.2206755 -4.2397528 -4.2619638 -4.2843738 -4.2988563 -4.3064351 -4.3155208][-4.2735929 -4.2767429 -4.281507 -4.2866249 -4.2929888 -4.2907748 -4.2823458 -4.2763252 -4.2831693 -4.2966256 -4.3107615 -4.3234839 -4.3295522 -4.3306975 -4.3319731][-4.2966633 -4.2990355 -4.3035369 -4.3074527 -4.3116179 -4.31119 -4.3060293 -4.3020735 -4.3068876 -4.3162837 -4.3264737 -4.3354058 -4.3386283 -4.3373523 -4.3373985][-4.3211288 -4.3220549 -4.3245845 -4.3272853 -4.3300619 -4.33036 -4.3273211 -4.32517 -4.3282466 -4.3339033 -4.34046 -4.3453 -4.3465438 -4.344173 -4.3429632][-4.3406353 -4.340116 -4.3406568 -4.3416519 -4.3427286 -4.3425856 -4.3413424 -4.3404555 -4.3419404 -4.3450017 -4.3487535 -4.3513374 -4.3521309 -4.3512263 -4.3503132]]...]
INFO - root - 2017-12-06 02:36:03.815354: step 2810, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.465 sec/batch; 42h:33m:25s remains)
INFO - root - 2017-12-06 02:36:08.442547: step 2820, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.467 sec/batch; 42h:44m:47s remains)
INFO - root - 2017-12-06 02:36:12.889899: step 2830, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.439 sec/batch; 40h:10m:40s remains)
INFO - root - 2017-12-06 02:36:17.562678: step 2840, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.467 sec/batch; 42h:44m:56s remains)
INFO - root - 2017-12-06 02:36:22.369892: step 2850, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.479 sec/batch; 43h:51m:41s remains)
INFO - root - 2017-12-06 02:36:27.038224: step 2860, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.456 sec/batch; 41h:45m:47s remains)
INFO - root - 2017-12-06 02:36:31.735156: step 2870, loss = 2.08, batch loss = 2.02 (16.3 examples/sec; 0.492 sec/batch; 45h:03m:11s remains)
INFO - root - 2017-12-06 02:36:36.465153: step 2880, loss = 2.06, batch loss = 2.01 (16.8 examples/sec; 0.477 sec/batch; 43h:38m:45s remains)
INFO - root - 2017-12-06 02:36:41.152302: step 2890, loss = 2.05, batch loss = 1.99 (16.7 examples/sec; 0.479 sec/batch; 43h:53m:10s remains)
INFO - root - 2017-12-06 02:36:45.952339: step 2900, loss = 2.08, batch loss = 2.02 (16.0 examples/sec; 0.500 sec/batch; 45h:45m:54s remains)
2017-12-06 02:36:46.463824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2574568 -4.2713943 -4.2801094 -4.2869616 -4.2940497 -4.2883925 -4.2659931 -4.2359357 -4.2089391 -4.2074041 -4.2341828 -4.2616458 -4.2732396 -4.2633743 -4.25177][-4.2632909 -4.2742052 -4.2788582 -4.2802792 -4.2825842 -4.2746634 -4.2519503 -4.2246537 -4.2023406 -4.206604 -4.2412996 -4.2750363 -4.2903495 -4.2817197 -4.268487][-4.2658677 -4.2721615 -4.26861 -4.2630434 -4.2629509 -4.260788 -4.2466364 -4.227139 -4.2097883 -4.2157712 -4.2512765 -4.2853012 -4.2971 -4.2872853 -4.2747245][-4.2705636 -4.272305 -4.2606177 -4.2491755 -4.2511582 -4.2582974 -4.2530541 -4.2395329 -4.2254391 -4.2289248 -4.2562814 -4.2819958 -4.2888446 -4.2792821 -4.2696757][-4.273097 -4.2738209 -4.2602987 -4.2496161 -4.2554793 -4.2668552 -4.2657595 -4.254993 -4.2424326 -4.2415333 -4.2570109 -4.2698641 -4.2686129 -4.2587309 -4.2521858][-4.2717781 -4.2787051 -4.2730722 -4.2690678 -4.2759061 -4.282649 -4.2793016 -4.2691264 -4.2575378 -4.2513027 -4.2532191 -4.252306 -4.2445364 -4.2349291 -4.231442][-4.2686253 -4.284802 -4.2890449 -4.2920976 -4.2977653 -4.2968206 -4.2879405 -4.2780657 -4.2685976 -4.2621322 -4.2597384 -4.253252 -4.2423244 -4.2328091 -4.2293181][-4.2657876 -4.2897358 -4.3016472 -4.3055511 -4.3052855 -4.297596 -4.2855468 -4.2772136 -4.2711267 -4.269896 -4.2717218 -4.2685847 -4.2610927 -4.2549653 -4.2516575][-4.260447 -4.2938566 -4.3117423 -4.3137236 -4.3054066 -4.2920351 -4.27758 -4.2684665 -4.2646742 -4.2679257 -4.275311 -4.2783041 -4.2769189 -4.2778363 -4.2779779][-4.2468667 -4.2911615 -4.3151231 -4.3168983 -4.3034248 -4.2834864 -4.2656713 -4.2556953 -4.2537718 -4.2598386 -4.2685823 -4.2731862 -4.27639 -4.2849364 -4.2911682][-4.2319717 -4.2843118 -4.312511 -4.3163428 -4.3006868 -4.2748785 -4.2545919 -4.2454767 -4.24282 -4.2449832 -4.2493043 -4.2493715 -4.2526383 -4.2669134 -4.2809][-4.2338505 -4.2885647 -4.3162928 -4.3190784 -4.3017812 -4.272758 -4.2510352 -4.241807 -4.2357326 -4.2321668 -4.2280388 -4.2187324 -4.2153497 -4.2289867 -4.2475233][-4.2557216 -4.2978559 -4.31601 -4.31511 -4.2986884 -4.2702427 -4.2490582 -4.2399731 -4.2346787 -4.2300749 -4.2200303 -4.20301 -4.1914797 -4.1991873 -4.2123971][-4.2752118 -4.2994547 -4.3069739 -4.30569 -4.2957721 -4.2740817 -4.2537966 -4.2437272 -4.241581 -4.2400794 -4.2282848 -4.2078834 -4.1923647 -4.192739 -4.1908603][-4.2809258 -4.2913327 -4.2920852 -4.29195 -4.2898946 -4.277298 -4.2612081 -4.25385 -4.2553387 -4.2570682 -4.2471652 -4.2295327 -4.2169895 -4.2115579 -4.19503]]...]
INFO - root - 2017-12-06 02:36:51.206087: step 2910, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.457 sec/batch; 41h:49m:25s remains)
INFO - root - 2017-12-06 02:36:55.896747: step 2920, loss = 2.06, batch loss = 2.00 (16.4 examples/sec; 0.488 sec/batch; 44h:39m:50s remains)
INFO - root - 2017-12-06 02:37:00.382883: step 2930, loss = 2.10, batch loss = 2.04 (16.6 examples/sec; 0.483 sec/batch; 44h:11m:54s remains)
INFO - root - 2017-12-06 02:37:05.186812: step 2940, loss = 2.05, batch loss = 1.99 (16.6 examples/sec; 0.481 sec/batch; 44h:00m:46s remains)
INFO - root - 2017-12-06 02:37:09.875487: step 2950, loss = 2.10, batch loss = 2.04 (17.0 examples/sec; 0.471 sec/batch; 43h:09m:33s remains)
INFO - root - 2017-12-06 02:37:14.664011: step 2960, loss = 2.10, batch loss = 2.04 (16.8 examples/sec; 0.476 sec/batch; 43h:35m:17s remains)
INFO - root - 2017-12-06 02:37:19.309051: step 2970, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.453 sec/batch; 41h:27m:04s remains)
INFO - root - 2017-12-06 02:37:24.145088: step 2980, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.473 sec/batch; 43h:20m:07s remains)
INFO - root - 2017-12-06 02:37:28.797713: step 2990, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.475 sec/batch; 43h:29m:51s remains)
INFO - root - 2017-12-06 02:37:33.540590: step 3000, loss = 2.06, batch loss = 2.01 (16.7 examples/sec; 0.479 sec/batch; 43h:50m:59s remains)
2017-12-06 02:37:33.999005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1888309 -4.2140875 -4.2274637 -4.2236328 -4.1945491 -4.1528039 -4.1225295 -4.1143184 -4.1327915 -4.1593857 -4.1793375 -4.202404 -4.2128973 -4.2207465 -4.2195225][-4.1757751 -4.2059045 -4.2267518 -4.2306404 -4.2048345 -4.1579309 -4.1194363 -4.1053653 -4.1208882 -4.1451054 -4.162097 -4.1841807 -4.1955271 -4.2048979 -4.2060184][-4.1556668 -4.187757 -4.2180915 -4.2345729 -4.2170515 -4.1776 -4.1456609 -4.1334944 -4.141664 -4.15848 -4.1702409 -4.18746 -4.1952896 -4.2033463 -4.2068262][-4.1290274 -4.15348 -4.1865988 -4.21392 -4.2100983 -4.1899242 -4.1782618 -4.18115 -4.1907692 -4.2059007 -4.2181988 -4.2331843 -4.2352076 -4.236485 -4.2363][-4.1042309 -4.1155591 -4.1383543 -4.164259 -4.1646857 -4.1544819 -4.1524658 -4.1667771 -4.1871686 -4.2114582 -4.2315583 -4.2480688 -4.2509909 -4.2572203 -4.2607465][-4.0685716 -4.0667219 -4.0729856 -4.0913181 -4.0872326 -4.0722976 -4.0634933 -4.0878134 -4.130188 -4.1729727 -4.202105 -4.2210903 -4.2307043 -4.2459974 -4.2566433][-4.0201149 -3.9969995 -3.9832025 -3.9910209 -3.979023 -3.9469178 -3.9142463 -3.9438407 -4.0144172 -4.0799785 -4.1217976 -4.1478214 -4.1696405 -4.1951189 -4.209034][-3.9990013 -3.9400959 -3.9006758 -3.8959935 -3.8675065 -3.7959659 -3.7112837 -3.7408395 -3.8553138 -3.9561756 -4.0179405 -4.0499868 -4.0811582 -4.1159649 -4.12928][-4.0466452 -3.9772723 -3.9271638 -3.9092395 -3.8702707 -3.790185 -3.6985295 -3.7192938 -3.8323557 -3.9322751 -3.9903879 -4.011342 -4.0345049 -4.0628233 -4.069562][-4.1315093 -4.0734043 -4.0322943 -4.0122457 -3.9822609 -3.9302022 -3.8811407 -3.8973596 -3.9611144 -4.0240831 -4.0617361 -4.0650988 -4.0679059 -4.0761151 -4.06993][-4.2082171 -4.1674786 -4.1384616 -4.12473 -4.1057367 -4.0766387 -4.052979 -4.0605235 -4.0851264 -4.1128745 -4.1250148 -4.1156096 -4.104351 -4.0940208 -4.0795784][-4.2611566 -4.2389345 -4.2233706 -4.2166262 -4.2073474 -4.1891255 -4.1718121 -4.1674714 -4.1695395 -4.168117 -4.1506939 -4.1186991 -4.0908971 -4.0620427 -4.0346003][-4.2881265 -4.278398 -4.2721939 -4.2676835 -4.2606716 -4.2470946 -4.230722 -4.2175851 -4.2078114 -4.1877232 -4.147687 -4.1003413 -4.0579538 -4.0186348 -3.9811606][-4.2954788 -4.2896943 -4.2879624 -4.2866592 -4.2814951 -4.2706161 -4.255445 -4.2384119 -4.2200913 -4.1887307 -4.1333876 -4.0738845 -4.0214252 -3.9826956 -3.945447][-4.3031492 -4.2965574 -4.2935085 -4.2923493 -4.2886724 -4.2810712 -4.2676253 -4.250339 -4.2287207 -4.1953945 -4.1396179 -4.0799508 -4.0305109 -3.9949987 -3.9569232]]...]
INFO - root - 2017-12-06 02:37:38.670711: step 3010, loss = 2.06, batch loss = 2.00 (16.6 examples/sec; 0.483 sec/batch; 44h:11m:25s remains)
INFO - root - 2017-12-06 02:37:43.238564: step 3020, loss = 2.07, batch loss = 2.01 (28.4 examples/sec; 0.282 sec/batch; 25h:48m:13s remains)
INFO - root - 2017-12-06 02:37:47.986602: step 3030, loss = 2.06, batch loss = 2.00 (16.6 examples/sec; 0.481 sec/batch; 44h:03m:54s remains)
INFO - root - 2017-12-06 02:37:52.584530: step 3040, loss = 2.05, batch loss = 1.99 (17.4 examples/sec; 0.459 sec/batch; 41h:57m:55s remains)
INFO - root - 2017-12-06 02:37:57.298882: step 3050, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.476 sec/batch; 43h:35m:29s remains)
INFO - root - 2017-12-06 02:38:01.969885: step 3060, loss = 2.09, batch loss = 2.03 (16.9 examples/sec; 0.473 sec/batch; 43h:14m:49s remains)
INFO - root - 2017-12-06 02:38:06.614282: step 3070, loss = 2.06, batch loss = 2.01 (17.8 examples/sec; 0.450 sec/batch; 41h:10m:27s remains)
INFO - root - 2017-12-06 02:38:11.274371: step 3080, loss = 2.06, batch loss = 2.00 (15.9 examples/sec; 0.502 sec/batch; 45h:58m:36s remains)
INFO - root - 2017-12-06 02:38:15.977127: step 3090, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.460 sec/batch; 42h:02m:57s remains)
INFO - root - 2017-12-06 02:38:20.584614: step 3100, loss = 2.07, batch loss = 2.01 (19.2 examples/sec; 0.417 sec/batch; 38h:09m:02s remains)
2017-12-06 02:38:21.008310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3232703 -4.3218627 -4.3197551 -4.316854 -4.3146591 -4.3125911 -4.30932 -4.3106227 -4.3155713 -4.3183966 -4.3180442 -4.312418 -4.3107476 -4.3111668 -4.3072634][-4.3138232 -4.3131537 -4.3095326 -4.3057804 -4.3027725 -4.298594 -4.2906995 -4.2911844 -4.2990675 -4.3030519 -4.3011312 -4.294138 -4.2911782 -4.2912774 -4.2833519][-4.2961521 -4.296967 -4.2922583 -4.2876472 -4.2824955 -4.2756948 -4.2657351 -4.2645292 -4.273272 -4.2795949 -4.2806025 -4.2727685 -4.2669673 -4.2626133 -4.2479234][-4.2723985 -4.2763276 -4.2714748 -4.2654548 -4.25609 -4.2452822 -4.2329855 -4.22468 -4.23037 -4.2400832 -4.24399 -4.2340307 -4.2205944 -4.2066417 -4.183218][-4.2385073 -4.2445364 -4.2386642 -4.2294335 -4.2152028 -4.1983023 -4.1801491 -4.161231 -4.1644588 -4.1834755 -4.1903853 -4.1757894 -4.1518497 -4.1314945 -4.1040068][-4.2050581 -4.20938 -4.1970763 -4.1783667 -4.1561294 -4.1269784 -4.095993 -4.0637813 -4.06712 -4.1044316 -4.1182432 -4.1006756 -4.073699 -4.0603409 -4.046258][-4.1962948 -4.1917868 -4.1656156 -4.1261115 -4.08705 -4.0412316 -3.9910464 -3.9378436 -3.9412792 -4.0058708 -4.0419946 -4.0362248 -4.0263968 -4.0403571 -4.0561738][-4.2141867 -4.1993651 -4.15865 -4.0949545 -4.0317192 -3.9696131 -3.8999956 -3.8213172 -3.8250048 -3.9219072 -3.9880571 -4.0072823 -4.0281925 -4.0741615 -4.1200743][-4.2431955 -4.2211466 -4.1746836 -4.1009288 -4.0237846 -3.9579167 -3.8902142 -3.8153853 -3.8190217 -3.9157019 -3.9903553 -4.0267549 -4.0678949 -4.1302347 -4.1909556][-4.2748656 -4.2530389 -4.2143364 -4.1484661 -4.0747843 -4.0144129 -3.964134 -3.9174502 -3.9169354 -3.9764807 -4.033041 -4.072648 -4.1189051 -4.180119 -4.2423124][-4.302258 -4.28747 -4.2638059 -4.2166486 -4.1590657 -4.1112614 -4.0758424 -4.0500402 -4.044445 -4.0717487 -4.1064482 -4.1403146 -4.1791682 -4.2252426 -4.2735791][-4.3197227 -4.3148527 -4.3051038 -4.2789674 -4.2441525 -4.2136 -4.1904511 -4.1751356 -4.1701035 -4.1815925 -4.1994047 -4.2203641 -4.2423382 -4.2665663 -4.2916131][-4.328002 -4.3297024 -4.3283391 -4.3169951 -4.3002152 -4.2858739 -4.2751875 -4.268671 -4.2671437 -4.2724457 -4.2797642 -4.2867017 -4.29204 -4.2971816 -4.3023553][-4.3282194 -4.3313327 -4.3327641 -4.3295836 -4.3235121 -4.319128 -4.3182812 -4.3187551 -4.3205376 -4.3230886 -4.3248782 -4.3239994 -4.3204932 -4.3165369 -4.3120232][-4.3232131 -4.3256912 -4.3271589 -4.3268242 -4.32563 -4.3271441 -4.331984 -4.3362103 -4.3389297 -4.3404307 -4.3403673 -4.3370481 -4.3323135 -4.3282142 -4.3228035]]...]
INFO - root - 2017-12-06 02:38:25.634206: step 3110, loss = 2.03, batch loss = 1.98 (16.7 examples/sec; 0.479 sec/batch; 43h:48m:27s remains)
INFO - root - 2017-12-06 02:38:30.180837: step 3120, loss = 2.05, batch loss = 2.00 (16.8 examples/sec; 0.477 sec/batch; 43h:40m:02s remains)
INFO - root - 2017-12-06 02:38:34.975143: step 3130, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.458 sec/batch; 41h:55m:03s remains)
INFO - root - 2017-12-06 02:38:39.687775: step 3140, loss = 2.09, batch loss = 2.03 (16.9 examples/sec; 0.472 sec/batch; 43h:12m:26s remains)
INFO - root - 2017-12-06 02:38:44.385760: step 3150, loss = 2.04, batch loss = 1.98 (17.9 examples/sec; 0.447 sec/batch; 40h:52m:34s remains)
INFO - root - 2017-12-06 02:38:49.118396: step 3160, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.462 sec/batch; 42h:17m:40s remains)
INFO - root - 2017-12-06 02:38:53.773781: step 3170, loss = 2.07, batch loss = 2.02 (16.8 examples/sec; 0.475 sec/batch; 43h:29m:22s remains)
INFO - root - 2017-12-06 02:38:58.525478: step 3180, loss = 2.08, batch loss = 2.03 (16.9 examples/sec; 0.473 sec/batch; 43h:14m:33s remains)
INFO - root - 2017-12-06 02:39:03.140103: step 3190, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.461 sec/batch; 42h:08m:57s remains)
INFO - root - 2017-12-06 02:39:07.841315: step 3200, loss = 2.05, batch loss = 2.00 (16.3 examples/sec; 0.492 sec/batch; 45h:01m:51s remains)
2017-12-06 02:39:08.328540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2728405 -4.2705908 -4.2700133 -4.2731991 -4.2824535 -4.2929416 -4.2917714 -4.2828674 -4.2755985 -4.272891 -4.2761507 -4.2818379 -4.2865763 -4.2961769 -4.3047085][-4.2339535 -4.2306228 -4.2307987 -4.2350836 -4.2438769 -4.2566128 -4.2542715 -4.2403932 -4.2319012 -4.2330651 -4.2430425 -4.2570343 -4.2663255 -4.2806716 -4.2926822][-4.2046485 -4.1993194 -4.1971288 -4.1956744 -4.1975594 -4.206481 -4.1963658 -4.1749287 -4.1686559 -4.1754203 -4.1928082 -4.2196145 -4.2381496 -4.2630324 -4.2814813][-4.182054 -4.1671715 -4.1562948 -4.1430917 -4.1360421 -4.1407909 -4.1262722 -4.103117 -4.1034837 -4.1151471 -4.1392722 -4.1786966 -4.2098236 -4.2459946 -4.2728539][-4.1566944 -4.1246634 -4.1003079 -4.0736418 -4.059607 -4.0632052 -4.04953 -4.0260377 -4.0283904 -4.0441236 -4.0733852 -4.1230369 -4.1701484 -4.2239294 -4.2621174][-4.1266136 -4.0772324 -4.0382652 -4.0032911 -3.9837546 -3.9866338 -3.9677703 -3.9327862 -3.9243274 -3.9377389 -3.9708266 -4.031569 -4.1055083 -4.1908636 -4.2464323][-4.0845733 -4.0222626 -3.9711301 -3.9365447 -3.9205403 -3.9254956 -3.8978498 -3.8460982 -3.8274777 -3.8388319 -3.8796587 -3.9565604 -4.0603714 -4.1696548 -4.2371926][-4.0551581 -3.9991307 -3.9520776 -3.9207439 -3.9049034 -3.9056537 -3.8695049 -3.8145432 -3.7995892 -3.8203464 -3.8754432 -3.9593792 -4.0673404 -4.1763062 -4.2440662][-4.0478106 -4.0063596 -3.9728041 -3.9505923 -3.9370143 -3.9377563 -3.9098344 -3.8688176 -3.8645716 -3.8927963 -3.9535387 -4.02793 -4.1140575 -4.203568 -4.2600584][-4.0800567 -4.0467687 -4.0269604 -4.0169954 -4.01116 -4.0179543 -4.0078964 -3.9851563 -3.9875283 -4.0140438 -4.0652623 -4.1211863 -4.1804438 -4.2449374 -4.2863421][-4.1400461 -4.1123896 -4.103406 -4.1062913 -4.1100907 -4.12136 -4.1227212 -4.1145868 -4.120008 -4.1407056 -4.1753516 -4.2134666 -4.2510266 -4.2937179 -4.3199654][-4.2113333 -4.1952028 -4.1948738 -4.205451 -4.2134709 -4.2232742 -4.22671 -4.2265148 -4.231123 -4.2427506 -4.2634721 -4.2886972 -4.3107638 -4.335886 -4.3479543][-4.2721434 -4.2650642 -4.2717562 -4.2836728 -4.29065 -4.2980351 -4.3012896 -4.3033557 -4.3047061 -4.3083377 -4.3192835 -4.3338928 -4.3455219 -4.3576765 -4.3602033][-4.3177695 -4.3143806 -4.3217034 -4.3306832 -4.3336787 -4.33679 -4.3388062 -4.3401003 -4.339993 -4.3409863 -4.346458 -4.3529944 -4.3579135 -4.3621216 -4.3603849][-4.3381844 -4.3361244 -4.3394532 -4.3424239 -4.3421741 -4.3420258 -4.3428268 -4.3435 -4.3435454 -4.3440757 -4.34669 -4.3493266 -4.3516331 -4.3533845 -4.3523083]]...]
INFO - root - 2017-12-06 02:39:13.028107: step 3210, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 0.486 sec/batch; 44h:25m:10s remains)
INFO - root - 2017-12-06 02:39:17.617757: step 3220, loss = 2.06, batch loss = 2.01 (17.6 examples/sec; 0.455 sec/batch; 41h:37m:26s remains)
INFO - root - 2017-12-06 02:39:22.359494: step 3230, loss = 2.06, batch loss = 2.01 (16.5 examples/sec; 0.484 sec/batch; 44h:18m:20s remains)
INFO - root - 2017-12-06 02:39:27.085939: step 3240, loss = 2.07, batch loss = 2.01 (17.9 examples/sec; 0.448 sec/batch; 40h:56m:28s remains)
INFO - root - 2017-12-06 02:39:31.801243: step 3250, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.473 sec/batch; 43h:16m:33s remains)
INFO - root - 2017-12-06 02:39:36.550935: step 3260, loss = 2.07, batch loss = 2.01 (16.3 examples/sec; 0.490 sec/batch; 44h:48m:06s remains)
INFO - root - 2017-12-06 02:39:41.221544: step 3270, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.476 sec/batch; 43h:34m:24s remains)
INFO - root - 2017-12-06 02:39:45.903848: step 3280, loss = 2.04, batch loss = 1.98 (16.4 examples/sec; 0.487 sec/batch; 44h:31m:26s remains)
INFO - root - 2017-12-06 02:39:50.569948: step 3290, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.460 sec/batch; 42h:04m:12s remains)
INFO - root - 2017-12-06 02:39:55.256400: step 3300, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.461 sec/batch; 42h:11m:33s remains)
2017-12-06 02:39:55.723741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1988611 -4.2396579 -4.2709427 -4.2832808 -4.2857985 -4.2857542 -4.2795644 -4.2625189 -4.2437062 -4.2228866 -4.1998324 -4.1890359 -4.2065883 -4.2282028 -4.229702][-4.1967726 -4.23817 -4.2691731 -4.28144 -4.284246 -4.287631 -4.289083 -4.2869711 -4.2834 -4.2761869 -4.2654157 -4.2570825 -4.2601542 -4.2590833 -4.2383533][-4.1946168 -4.2322392 -4.2577686 -4.2665415 -4.2665968 -4.2690949 -4.2712355 -4.2777724 -4.2882686 -4.2960114 -4.2979078 -4.2928133 -4.2865968 -4.2710338 -4.236114][-4.2058768 -4.2293477 -4.2406592 -4.2380986 -4.2281365 -4.2182155 -4.2088847 -4.2149673 -4.2369447 -4.2621746 -4.2821903 -4.29136 -4.2911735 -4.278585 -4.2472248][-4.2216625 -4.2261963 -4.2186255 -4.1997528 -4.1685662 -4.1303506 -4.0959206 -4.0967641 -4.135849 -4.1847749 -4.2292137 -4.2616587 -4.277638 -4.2763739 -4.2548628][-4.233089 -4.2226796 -4.1977749 -4.1593051 -4.1004858 -4.024632 -3.958395 -3.9554443 -4.020021 -4.0997505 -4.1695728 -4.2226057 -4.2514486 -4.2572455 -4.243012][-4.2527032 -4.2335668 -4.1928544 -4.134562 -4.0490265 -3.9354453 -3.8356981 -3.8303776 -3.9214292 -4.0326948 -4.1253514 -4.1941795 -4.2308097 -4.2410679 -4.2326722][-4.2783818 -4.2592115 -4.2134843 -4.1480646 -4.0504847 -3.917093 -3.7980156 -3.790484 -3.8889289 -4.0106416 -4.1110048 -4.1853809 -4.2233386 -4.2344 -4.2306104][-4.3040257 -4.28664 -4.2442069 -4.1853762 -4.0981116 -3.9811153 -3.88157 -3.8769627 -3.9562974 -4.0561018 -4.1399231 -4.2029867 -4.2341852 -4.2435169 -4.2438478][-4.3205738 -4.3026567 -4.2656422 -4.2196388 -4.1558018 -4.0747223 -4.0140839 -4.018188 -4.0749536 -4.1424422 -4.1982 -4.2423797 -4.2646995 -4.2714472 -4.27362][-4.3251247 -4.3063245 -4.2762794 -4.2460394 -4.2077079 -4.16251 -4.1373167 -4.1487713 -4.1849394 -4.2223096 -4.2494287 -4.2726521 -4.28685 -4.2928829 -4.295948][-4.3114238 -4.2888927 -4.2637553 -4.2486267 -4.2340069 -4.21784 -4.2185068 -4.2345724 -4.2545524 -4.2676468 -4.2705531 -4.2740946 -4.2817588 -4.29041 -4.2976418][-4.2814431 -4.256 -4.2351875 -4.2328057 -4.2372289 -4.2432685 -4.2598329 -4.277606 -4.2861309 -4.2788262 -4.2600369 -4.245975 -4.2491379 -4.2645731 -4.2793245][-4.2548456 -4.2354231 -4.2226043 -4.2295837 -4.2435522 -4.2588682 -4.2796826 -4.2963605 -4.2992506 -4.2803478 -4.2467914 -4.2200689 -4.2174015 -4.2337689 -4.2537508][-4.2433147 -4.235177 -4.2324266 -4.2452593 -4.2624216 -4.278873 -4.295373 -4.3051019 -4.3019671 -4.2796354 -4.2440352 -4.21532 -4.2098827 -4.2243676 -4.2464013]]...]
INFO - root - 2017-12-06 02:40:00.178062: step 3310, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.478 sec/batch; 43h:41m:07s remains)
INFO - root - 2017-12-06 02:40:04.808909: step 3320, loss = 2.09, batch loss = 2.03 (17.4 examples/sec; 0.460 sec/batch; 42h:05m:01s remains)
INFO - root - 2017-12-06 02:40:09.579549: step 3330, loss = 2.06, batch loss = 2.01 (16.5 examples/sec; 0.486 sec/batch; 44h:24m:59s remains)
INFO - root - 2017-12-06 02:40:14.351933: step 3340, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.472 sec/batch; 43h:09m:40s remains)
INFO - root - 2017-12-06 02:40:19.028808: step 3350, loss = 2.08, batch loss = 2.02 (16.6 examples/sec; 0.481 sec/batch; 44h:00m:41s remains)
INFO - root - 2017-12-06 02:40:23.657928: step 3360, loss = 2.06, batch loss = 2.01 (17.8 examples/sec; 0.449 sec/batch; 41h:02m:53s remains)
INFO - root - 2017-12-06 02:40:28.297758: step 3370, loss = 2.04, batch loss = 1.98 (17.3 examples/sec; 0.464 sec/batch; 42h:23m:35s remains)
INFO - root - 2017-12-06 02:40:33.019779: step 3380, loss = 2.04, batch loss = 1.98 (16.9 examples/sec; 0.473 sec/batch; 43h:16m:13s remains)
INFO - root - 2017-12-06 02:40:37.728203: step 3390, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 0.470 sec/batch; 42h:56m:13s remains)
INFO - root - 2017-12-06 02:40:42.380896: step 3400, loss = 2.04, batch loss = 1.98 (17.1 examples/sec; 0.468 sec/batch; 42h:47m:59s remains)
2017-12-06 02:40:42.856490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.073945 -4.0963154 -4.1195383 -4.1385546 -4.1487942 -4.15045 -4.1500626 -4.1609793 -4.1782036 -4.189784 -4.2003951 -4.2003407 -4.1932964 -4.1894259 -4.1859946][-4.0822282 -4.1177607 -4.1466022 -4.1660581 -4.1801634 -4.1863589 -4.184186 -4.1854434 -4.1896996 -4.2003555 -4.2167559 -4.2244382 -4.2175531 -4.2029462 -4.1852965][-4.10031 -4.1457562 -4.1786971 -4.1969562 -4.2123184 -4.2180686 -4.20965 -4.1963286 -4.191236 -4.20211 -4.2220259 -4.2388158 -4.2352858 -4.2102327 -4.1816764][-4.1360493 -4.1782131 -4.2015781 -4.2108779 -4.2223873 -4.2235937 -4.2059836 -4.1815004 -4.1739855 -4.1890769 -4.2172637 -4.2429905 -4.2428808 -4.2150688 -4.185267][-4.16294 -4.1935654 -4.204524 -4.2050481 -4.2092905 -4.1999888 -4.1685853 -4.131424 -4.1257448 -4.1481934 -4.1846476 -4.2193565 -4.2248545 -4.2045174 -4.1847968][-4.169343 -4.1891351 -4.1917577 -4.184401 -4.1786404 -4.1545143 -4.1021624 -4.0444565 -4.0385237 -4.0813909 -4.134479 -4.1782837 -4.1932282 -4.1892056 -4.1866078][-4.1535788 -4.1580415 -4.1511154 -4.13802 -4.12156 -4.0793552 -4.0012746 -3.9219024 -3.9277635 -4.0077372 -4.0911407 -4.152215 -4.182281 -4.192369 -4.1969943][-4.1442022 -4.1306615 -4.1147952 -4.0995488 -4.076829 -4.0248303 -3.9352486 -3.8590672 -3.8923697 -4.0032759 -4.1027284 -4.1684928 -4.2050371 -4.2187276 -4.2180572][-4.1527419 -4.1371784 -4.1207795 -4.1052666 -4.0826321 -4.0448575 -3.9896402 -3.9573524 -3.9993536 -4.0880837 -4.1610489 -4.2049818 -4.2290888 -4.23516 -4.2280765][-4.1632457 -4.1529737 -4.1423149 -4.1330566 -4.1199431 -4.1014485 -4.07824 -4.0708041 -4.1016822 -4.1584148 -4.2023821 -4.2198486 -4.2237577 -4.2179585 -4.2042985][-4.17112 -4.1592197 -4.1572466 -4.1565676 -4.1481743 -4.1364851 -4.1238379 -4.124022 -4.1474776 -4.1852107 -4.2126126 -4.2157269 -4.2020917 -4.180347 -4.1576376][-4.1814766 -4.1647291 -4.16462 -4.1688967 -4.1633081 -4.1536269 -4.1446409 -4.1488161 -4.1693106 -4.1978297 -4.215414 -4.206748 -4.178802 -4.1436625 -4.1151185][-4.1794705 -4.1583424 -4.155622 -4.1573586 -4.1516395 -4.1478367 -4.1440544 -4.15265 -4.1755853 -4.2015295 -4.2123671 -4.1934924 -4.156569 -4.120697 -4.0982537][-4.166925 -4.149231 -4.14168 -4.1378975 -4.1292896 -4.1249571 -4.1194911 -4.1262274 -4.1520267 -4.17798 -4.190763 -4.1727757 -4.1356473 -4.110209 -4.1008554][-4.1653118 -4.1549034 -4.1454053 -4.1319294 -4.1154537 -4.102252 -4.09256 -4.0993171 -4.1236362 -4.14899 -4.1656885 -4.1555371 -4.1265373 -4.1139793 -4.1166716]]...]
INFO - root - 2017-12-06 02:40:47.386870: step 3410, loss = 2.04, batch loss = 1.99 (16.7 examples/sec; 0.478 sec/batch; 43h:44m:16s remains)
INFO - root - 2017-12-06 02:40:52.012813: step 3420, loss = 2.05, batch loss = 2.00 (17.0 examples/sec; 0.470 sec/batch; 42h:57m:06s remains)
INFO - root - 2017-12-06 02:40:56.677924: step 3430, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.480 sec/batch; 43h:54m:03s remains)
INFO - root - 2017-12-06 02:41:01.480233: step 3440, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.439 sec/batch; 40h:08m:40s remains)
INFO - root - 2017-12-06 02:41:06.124216: step 3450, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.477 sec/batch; 43h:33m:39s remains)
INFO - root - 2017-12-06 02:41:10.816045: step 3460, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.476 sec/batch; 43h:32m:58s remains)
INFO - root - 2017-12-06 02:41:15.555108: step 3470, loss = 2.05, batch loss = 2.00 (16.4 examples/sec; 0.488 sec/batch; 44h:35m:52s remains)
INFO - root - 2017-12-06 02:41:20.267743: step 3480, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.468 sec/batch; 42h:46m:10s remains)
INFO - root - 2017-12-06 02:41:24.891330: step 3490, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.462 sec/batch; 42h:14m:42s remains)
INFO - root - 2017-12-06 02:41:29.526318: step 3500, loss = 2.03, batch loss = 1.98 (17.1 examples/sec; 0.466 sec/batch; 42h:37m:50s remains)
2017-12-06 02:41:29.966270: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3491912 -4.3486757 -4.3490682 -4.3494735 -4.3492851 -4.3499255 -4.3511434 -4.35218 -4.3536472 -4.3543382 -4.3547282 -4.3546114 -4.3539429 -4.3541679 -4.3556414][-4.3402753 -4.3386655 -4.3372459 -4.3377709 -4.3389492 -4.3410258 -4.3445568 -4.3474178 -4.3501406 -4.3509092 -4.34994 -4.3484507 -4.3467669 -4.3468738 -4.3492513][-4.3267217 -4.3220019 -4.3173418 -4.316772 -4.3195052 -4.32235 -4.3266873 -4.3291893 -4.3319139 -4.3335652 -4.3322086 -4.3304753 -4.3299513 -4.3320575 -4.3371763][-4.3127775 -4.3070369 -4.3002625 -4.2966537 -4.2966814 -4.2964973 -4.2960277 -4.2938824 -4.2946086 -4.2965479 -4.2980165 -4.298727 -4.303091 -4.3077106 -4.3168931][-4.2999973 -4.2941003 -4.2847223 -4.2752385 -4.2672524 -4.2606792 -4.2538633 -4.2431426 -4.2400026 -4.2455854 -4.2534165 -4.262156 -4.271595 -4.2800231 -4.2911][-4.2855468 -4.2801476 -4.267817 -4.2513871 -4.2346067 -4.218133 -4.2005062 -4.1827531 -4.1820626 -4.19713 -4.2104416 -4.2255745 -4.2432837 -4.2565565 -4.2680492][-4.2724147 -4.2671309 -4.2530704 -4.2333393 -4.2106771 -4.1852474 -4.1550918 -4.1310511 -4.1341248 -4.1568933 -4.1726003 -4.1909943 -4.2145219 -4.2321911 -4.2443027][-4.2628508 -4.2575464 -4.2420774 -4.219933 -4.1919417 -4.1575718 -4.1188078 -4.0912495 -4.0942068 -4.1217618 -4.1489425 -4.177012 -4.2021446 -4.2187452 -4.2319827][-4.2603035 -4.253633 -4.2367334 -4.2088647 -4.17304 -4.13364 -4.0959721 -4.0730143 -4.0778646 -4.1125941 -4.1506867 -4.1858153 -4.2107968 -4.2274528 -4.2416263][-4.2577519 -4.2517476 -4.2356162 -4.2045021 -4.1651344 -4.1272697 -4.098176 -4.0881248 -4.1017056 -4.1364427 -4.1726475 -4.2055426 -4.2301159 -4.2481174 -4.2601857][-4.2612677 -4.2601395 -4.2520943 -4.2270622 -4.1964288 -4.1655293 -4.1427989 -4.1400061 -4.1578274 -4.1845465 -4.2091289 -4.2315111 -4.2530489 -4.272006 -4.281764][-4.2739673 -4.2782946 -4.2770844 -4.264339 -4.2485223 -4.2291741 -4.21486 -4.2152557 -4.2289147 -4.2420826 -4.2520409 -4.2622094 -4.2777781 -4.2919855 -4.29798][-4.2755685 -4.284039 -4.2883377 -4.2860227 -4.2820196 -4.2749133 -4.2694387 -4.2721167 -4.2822618 -4.2873459 -4.2887754 -4.2926168 -4.2981863 -4.3005562 -4.3007307][-4.2603331 -4.2724018 -4.2814493 -4.2856545 -4.2886276 -4.2879887 -4.2863531 -4.2879453 -4.2965446 -4.301517 -4.3013043 -4.3039937 -4.3044381 -4.2991948 -4.2959275][-4.2394505 -4.2516623 -4.2603216 -4.2640724 -4.2688174 -4.2723742 -4.2726946 -4.2751093 -4.2834864 -4.2886128 -4.2876368 -4.2889605 -4.2891588 -4.2854195 -4.2852974]]...]
INFO - root - 2017-12-06 02:41:34.715252: step 3510, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.459 sec/batch; 41h:56m:47s remains)
INFO - root - 2017-12-06 02:41:39.396326: step 3520, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.474 sec/batch; 43h:17m:39s remains)
INFO - root - 2017-12-06 02:41:44.208887: step 3530, loss = 2.06, batch loss = 2.00 (16.0 examples/sec; 0.500 sec/batch; 45h:41m:36s remains)
INFO - root - 2017-12-06 02:41:48.934591: step 3540, loss = 2.08, batch loss = 2.02 (16.6 examples/sec; 0.483 sec/batch; 44h:08m:51s remains)
INFO - root - 2017-12-06 02:41:53.665777: step 3550, loss = 2.05, batch loss = 1.99 (17.5 examples/sec; 0.458 sec/batch; 41h:51m:51s remains)
INFO - root - 2017-12-06 02:41:58.348413: step 3560, loss = 2.07, batch loss = 2.02 (16.8 examples/sec; 0.476 sec/batch; 43h:29m:33s remains)
INFO - root - 2017-12-06 02:42:03.018495: step 3570, loss = 2.07, batch loss = 2.02 (17.4 examples/sec; 0.460 sec/batch; 42h:00m:52s remains)
INFO - root - 2017-12-06 02:42:07.702920: step 3580, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.456 sec/batch; 41h:41m:32s remains)
INFO - root - 2017-12-06 02:42:12.330498: step 3590, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.461 sec/batch; 42h:08m:47s remains)
INFO - root - 2017-12-06 02:42:16.809112: step 3600, loss = 2.05, batch loss = 2.00 (17.3 examples/sec; 0.463 sec/batch; 42h:19m:45s remains)
2017-12-06 02:42:17.267874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.26623 -4.2526813 -4.2305622 -4.1895895 -4.1347442 -4.0988755 -4.0955119 -4.1142054 -4.1299076 -4.1482658 -4.1788054 -4.19315 -4.1814923 -4.1533232 -4.1286168][-4.2614675 -4.2474866 -4.2336717 -4.1999846 -4.139782 -4.0843596 -4.064868 -4.0826941 -4.1111121 -4.14189 -4.1787353 -4.1967993 -4.1863708 -4.1547055 -4.1221347][-4.2423725 -4.2377577 -4.2362909 -4.2142272 -4.1604109 -4.0979571 -4.0658708 -4.0806532 -4.1196427 -4.158144 -4.1931682 -4.2111883 -4.2075391 -4.1836658 -4.1524611][-4.2235556 -4.2306085 -4.2403679 -4.2314534 -4.1881952 -4.1263142 -4.0869579 -4.0962043 -4.1413193 -4.18624 -4.2178822 -4.2348089 -4.2372913 -4.2226005 -4.1970372][-4.1995316 -4.2179518 -4.2337275 -4.2322555 -4.1990552 -4.1431422 -4.1046872 -4.1104631 -4.1569691 -4.2060847 -4.2370319 -4.254921 -4.2618232 -4.253129 -4.2326074][-4.1705852 -4.1939621 -4.2111173 -4.2116542 -4.1871605 -4.1395464 -4.1040692 -4.108789 -4.1559048 -4.2065969 -4.240653 -4.2614617 -4.27358 -4.2703986 -4.2537079][-4.1445661 -4.1607838 -4.1744881 -4.1763792 -4.1592288 -4.1205778 -4.0893188 -4.0948877 -4.1405573 -4.189672 -4.2261095 -4.2492018 -4.2665462 -4.2704611 -4.2584038][-4.1357384 -4.133604 -4.142128 -4.1494274 -4.1407676 -4.1118236 -4.0868855 -4.0906229 -4.1290622 -4.1703687 -4.2028184 -4.22488 -4.2464457 -4.2584481 -4.2532897][-4.1532173 -4.1337624 -4.1346068 -4.1471467 -4.1483788 -4.1324649 -4.1176953 -4.1184049 -4.1433797 -4.1682625 -4.1880984 -4.2034669 -4.2258511 -4.2418389 -4.2444777][-4.1930618 -4.1646643 -4.155108 -4.1624894 -4.1677179 -4.1619225 -4.156836 -4.1603923 -4.1765771 -4.1860161 -4.1927638 -4.1994686 -4.2162085 -4.2294722 -4.2373209][-4.2270784 -4.2012854 -4.1856337 -4.1843424 -4.1865559 -4.1835504 -4.1818695 -4.1873283 -4.1979074 -4.199811 -4.2013211 -4.2073808 -4.220715 -4.2280126 -4.2345157][-4.2409554 -4.2227902 -4.2062845 -4.1993027 -4.1959443 -4.1907916 -4.1867313 -4.1902308 -4.194396 -4.1947689 -4.1980028 -4.2088752 -4.221148 -4.2250948 -4.2296453][-4.2343221 -4.2237816 -4.2109389 -4.2031527 -4.1983228 -4.1907811 -4.1830487 -4.1802812 -4.1780334 -4.1771536 -4.1842103 -4.1987886 -4.2117305 -4.2164788 -4.2203722][-4.2310634 -4.2250905 -4.2165542 -4.2121897 -4.2102985 -4.2051034 -4.1972976 -4.1908712 -4.1850772 -4.1819749 -4.1880555 -4.2028265 -4.2162442 -4.2215877 -4.2251983][-4.2384791 -4.236032 -4.2309036 -4.229722 -4.2325015 -4.2322726 -4.2279229 -4.220953 -4.213275 -4.207469 -4.20959 -4.2212214 -4.2326922 -4.2368069 -4.23942]]...]
INFO - root - 2017-12-06 02:42:22.053120: step 3610, loss = 2.08, batch loss = 2.02 (16.5 examples/sec; 0.486 sec/batch; 44h:22m:58s remains)
INFO - root - 2017-12-06 02:42:26.811658: step 3620, loss = 2.03, batch loss = 1.97 (16.9 examples/sec; 0.472 sec/batch; 43h:07m:25s remains)
INFO - root - 2017-12-06 02:42:31.573619: step 3630, loss = 2.04, batch loss = 1.98 (17.3 examples/sec; 0.464 sec/batch; 42h:20m:44s remains)
INFO - root - 2017-12-06 02:42:36.288055: step 3640, loss = 2.04, batch loss = 1.98 (17.1 examples/sec; 0.468 sec/batch; 42h:46m:58s remains)
INFO - root - 2017-12-06 02:42:40.973641: step 3650, loss = 2.06, batch loss = 2.00 (18.8 examples/sec; 0.425 sec/batch; 38h:48m:09s remains)
INFO - root - 2017-12-06 02:42:45.668770: step 3660, loss = 2.10, batch loss = 2.04 (17.1 examples/sec; 0.468 sec/batch; 42h:44m:49s remains)
INFO - root - 2017-12-06 02:42:50.337771: step 3670, loss = 2.05, batch loss = 2.00 (16.4 examples/sec; 0.488 sec/batch; 44h:35m:39s remains)
INFO - root - 2017-12-06 02:42:55.151453: step 3680, loss = 2.05, batch loss = 1.99 (16.2 examples/sec; 0.495 sec/batch; 45h:12m:23s remains)
INFO - root - 2017-12-06 02:42:59.783825: step 3690, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.456 sec/batch; 41h:41m:09s remains)
INFO - root - 2017-12-06 02:43:04.264024: step 3700, loss = 2.05, batch loss = 1.99 (16.6 examples/sec; 0.483 sec/batch; 44h:07m:37s remains)
2017-12-06 02:43:04.750271: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2228041 -4.2333879 -4.22792 -4.1990724 -4.1741471 -4.1630716 -4.1596303 -4.1545725 -4.1443586 -4.1261292 -4.1273646 -4.142684 -4.1508288 -4.1547031 -4.1638217][-4.187408 -4.2034264 -4.2035933 -4.1775084 -4.1572628 -4.1532507 -4.1561937 -4.1487517 -4.124444 -4.0960159 -4.0949669 -4.117856 -4.1350393 -4.1436152 -4.1530304][-4.1474872 -4.1616373 -4.1632528 -4.140111 -4.1290989 -4.1354017 -4.1465931 -4.1441493 -4.1250563 -4.1034918 -4.1117435 -4.1415124 -4.1595793 -4.1654038 -4.1677175][-4.1085606 -4.116281 -4.1147842 -4.0950761 -4.0944409 -4.1077991 -4.1234188 -4.1338587 -4.1329961 -4.1308327 -4.1508489 -4.18216 -4.1963382 -4.1927781 -4.1804423][-4.0840287 -4.086062 -4.0808845 -4.0643 -4.0661664 -4.0754037 -4.0849452 -4.097496 -4.1078691 -4.1182089 -4.1427064 -4.17359 -4.1865835 -4.1798525 -4.1621451][-4.0608711 -4.0521727 -4.0338688 -4.0095615 -4.0002122 -3.9938829 -3.9841995 -3.9884055 -4.0085387 -4.03582 -4.0735149 -4.109014 -4.1262007 -4.1265159 -4.1190362][-3.9896863 -3.9521749 -3.9037583 -3.8545132 -3.8327646 -3.8166122 -3.7925162 -3.7962139 -3.8364356 -3.8890355 -3.9472272 -3.9970243 -4.02679 -4.0443859 -4.0635166][-3.953038 -3.8914995 -3.81804 -3.7491119 -3.7179554 -3.6995349 -3.6750407 -3.6877258 -3.7422495 -3.8049254 -3.8685625 -3.9202685 -3.952441 -3.9801173 -4.0211463][-4.0351152 -3.9840522 -3.9261658 -3.8778794 -3.8574824 -3.8473604 -3.8359485 -3.8439493 -3.8745604 -3.9090204 -3.94728 -3.9785225 -3.9958935 -4.0122862 -4.0445681][-4.1473379 -4.1161361 -4.080905 -4.0528884 -4.0419617 -4.038486 -4.03567 -4.0377502 -4.047719 -4.0598993 -4.0771642 -4.0939918 -4.1002855 -4.10428 -4.1191754][-4.2281423 -4.2076721 -4.1834536 -4.1638818 -4.1552515 -4.1540661 -4.155509 -4.1581054 -4.1618028 -4.165637 -4.1735907 -4.1841846 -4.1865778 -4.1849618 -4.189013][-4.2760859 -4.2615147 -4.2445583 -4.2303309 -4.2237916 -4.2240529 -4.2259526 -4.2278919 -4.2295451 -4.229744 -4.2322111 -4.2371469 -4.2380915 -4.2373862 -4.2389884][-4.2975268 -4.2858744 -4.2734609 -4.2624674 -4.2559614 -4.2549086 -4.2550831 -4.2560239 -4.2581725 -4.259491 -4.2614026 -4.2640634 -4.2656713 -4.2660708 -4.2665691][-4.3066435 -4.297339 -4.2868915 -4.2777781 -4.2721529 -4.2718925 -4.2725563 -4.2729177 -4.2732415 -4.2732735 -4.2741361 -4.2757106 -4.2774754 -4.2787075 -4.2794108][-4.3113379 -4.305006 -4.2973914 -4.2899132 -4.2853208 -4.287293 -4.2907515 -4.2923446 -4.2928395 -4.2920985 -4.290904 -4.2900705 -4.289638 -4.2882676 -4.2872262]]...]
INFO - root - 2017-12-06 02:43:09.382339: step 3710, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.466 sec/batch; 42h:32m:10s remains)
INFO - root - 2017-12-06 02:43:14.019422: step 3720, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.453 sec/batch; 41h:21m:25s remains)
INFO - root - 2017-12-06 02:43:18.719799: step 3730, loss = 2.05, batch loss = 1.99 (17.5 examples/sec; 0.458 sec/batch; 41h:46m:52s remains)
INFO - root - 2017-12-06 02:43:23.543154: step 3740, loss = 2.09, batch loss = 2.03 (16.9 examples/sec; 0.474 sec/batch; 43h:19m:09s remains)
INFO - root - 2017-12-06 02:43:28.170626: step 3750, loss = 2.08, batch loss = 2.02 (17.3 examples/sec; 0.463 sec/batch; 42h:18m:41s remains)
INFO - root - 2017-12-06 02:43:32.900516: step 3760, loss = 2.07, batch loss = 2.01 (16.3 examples/sec; 0.492 sec/batch; 44h:55m:32s remains)
INFO - root - 2017-12-06 02:43:37.613599: step 3770, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.452 sec/batch; 41h:17m:06s remains)
INFO - root - 2017-12-06 02:43:42.266034: step 3780, loss = 2.03, batch loss = 1.98 (17.1 examples/sec; 0.468 sec/batch; 42h:46m:24s remains)
INFO - root - 2017-12-06 02:43:46.843097: step 3790, loss = 2.06, batch loss = 2.01 (29.4 examples/sec; 0.272 sec/batch; 24h:51m:32s remains)
INFO - root - 2017-12-06 02:43:51.687589: step 3800, loss = 2.05, batch loss = 1.99 (16.1 examples/sec; 0.498 sec/batch; 45h:30m:15s remains)
2017-12-06 02:43:52.165243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3120093 -4.299675 -4.2959805 -4.297514 -4.29992 -4.3015866 -4.3038063 -4.3065081 -4.3065767 -4.3052354 -4.304647 -4.306273 -4.308218 -4.3098726 -4.3133097][-4.2989964 -4.281086 -4.2753105 -4.2771735 -4.2787261 -4.2791314 -4.2808318 -4.2854853 -4.287219 -4.2858467 -4.2856145 -4.2874546 -4.2916336 -4.2948914 -4.300282][-4.2761769 -4.2529497 -4.2439151 -4.2435637 -4.2475452 -4.2519603 -4.25486 -4.2612877 -4.2668471 -4.2670288 -4.2675028 -4.2687182 -4.272563 -4.2757387 -4.2804675][-4.2474828 -4.2125983 -4.1957731 -4.192986 -4.2014856 -4.2131314 -4.221324 -4.2336059 -4.2461195 -4.2500877 -4.2518821 -4.2531939 -4.2601032 -4.2645593 -4.2674909][-4.2247405 -4.1754222 -4.1473393 -4.1378694 -4.1471896 -4.1629953 -4.17578 -4.2022185 -4.2348247 -4.24814 -4.2529664 -4.2582483 -4.2692118 -4.2716727 -4.2722182][-4.2141829 -4.1517372 -4.1059694 -4.0769897 -4.0704565 -4.0753527 -4.0868707 -4.131834 -4.1906524 -4.2237654 -4.2390704 -4.2507625 -4.2685833 -4.273838 -4.2762847][-4.2110181 -4.1397405 -4.077148 -4.0218186 -3.9804618 -3.9490871 -3.9328642 -3.9908824 -4.09263 -4.1586828 -4.1908259 -4.2108593 -4.2377758 -4.2525554 -4.2657537][-4.2172065 -4.1446714 -4.0743685 -4.0025554 -3.9254544 -3.8343484 -3.7525396 -3.7961328 -3.9443526 -4.0553026 -4.1157584 -4.1524634 -4.1929603 -4.2199664 -4.2488675][-4.2345181 -4.172267 -4.1115608 -4.049365 -3.972719 -3.8601644 -3.7295055 -3.7217739 -3.8519464 -3.9731064 -4.0560083 -4.1124649 -4.1646962 -4.2021737 -4.2418432][-4.2503681 -4.2002349 -4.1581779 -4.12399 -4.0814333 -4.0093875 -3.9172106 -3.8957624 -3.9545491 -4.0325475 -4.1006112 -4.1508713 -4.192627 -4.2219343 -4.2528739][-4.2608018 -4.2188115 -4.1874232 -4.171474 -4.1557131 -4.1276765 -4.0900702 -4.0849943 -4.1070986 -4.1466832 -4.1890149 -4.2216682 -4.2444735 -4.2583928 -4.272891][-4.2707086 -4.2366757 -4.2102966 -4.2004895 -4.1973577 -4.1929593 -4.1887875 -4.2015953 -4.2156587 -4.2385316 -4.2638812 -4.2831459 -4.2940922 -4.2980123 -4.3004484][-4.2797704 -4.2552238 -4.2337456 -4.2245827 -4.22439 -4.2272973 -4.2348762 -4.2535529 -4.2680073 -4.289001 -4.309762 -4.3231668 -4.3278251 -4.3272357 -4.3242507][-4.2988849 -4.28795 -4.276485 -4.2681689 -4.2663741 -4.2692075 -4.2787809 -4.2926769 -4.3032193 -4.3182621 -4.3343863 -4.3447833 -4.3445673 -4.3393536 -4.3343968][-4.3201265 -4.3170967 -4.3166361 -4.3133059 -4.3109779 -4.3114467 -4.3169646 -4.3228035 -4.3271489 -4.3344288 -4.3439412 -4.3499537 -4.3478127 -4.3426523 -4.3386588]]...]
INFO - root - 2017-12-06 02:43:56.872083: step 3810, loss = 2.05, batch loss = 1.99 (16.9 examples/sec; 0.474 sec/batch; 43h:19m:10s remains)
INFO - root - 2017-12-06 02:44:01.561151: step 3820, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.469 sec/batch; 42h:51m:27s remains)
INFO - root - 2017-12-06 02:44:06.188489: step 3830, loss = 2.04, batch loss = 1.98 (17.3 examples/sec; 0.462 sec/batch; 42h:13m:18s remains)
INFO - root - 2017-12-06 02:44:10.828635: step 3840, loss = 2.05, batch loss = 1.99 (16.8 examples/sec; 0.476 sec/batch; 43h:27m:57s remains)
INFO - root - 2017-12-06 02:44:15.535106: step 3850, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 0.469 sec/batch; 42h:51m:06s remains)
INFO - root - 2017-12-06 02:44:20.267719: step 3860, loss = 2.05, batch loss = 1.99 (16.8 examples/sec; 0.476 sec/batch; 43h:29m:23s remains)
INFO - root - 2017-12-06 02:44:25.139396: step 3870, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.470 sec/batch; 42h:52m:04s remains)
INFO - root - 2017-12-06 02:44:29.826285: step 3880, loss = 2.08, batch loss = 2.03 (16.7 examples/sec; 0.478 sec/batch; 43h:39m:47s remains)
INFO - root - 2017-12-06 02:44:34.502626: step 3890, loss = 2.08, batch loss = 2.02 (15.7 examples/sec; 0.509 sec/batch; 46h:29m:58s remains)
INFO - root - 2017-12-06 02:44:39.211538: step 3900, loss = 2.05, batch loss = 1.99 (16.7 examples/sec; 0.479 sec/batch; 43h:42m:14s remains)
2017-12-06 02:44:39.681353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2435265 -4.1917996 -4.1311646 -4.0930662 -4.0788794 -4.0938082 -4.1321778 -4.1741209 -4.2015829 -4.1987605 -4.1880546 -4.1764779 -4.1636152 -4.1525726 -4.1475835][-4.2401218 -4.1835642 -4.112412 -4.0644736 -4.0486298 -4.0788431 -4.133379 -4.1747522 -4.196754 -4.1899743 -4.1761479 -4.1659374 -4.1564341 -4.1554637 -4.1572828][-4.23563 -4.1735425 -4.0926542 -4.0347075 -4.020472 -4.0676455 -4.1389685 -4.1792932 -4.1978173 -4.1908126 -4.1765847 -4.1676626 -4.1593938 -4.1603136 -4.1605582][-4.2320914 -4.1692095 -4.0855427 -4.0185866 -4.0031419 -4.0628314 -4.1469288 -4.1907673 -4.2091041 -4.201086 -4.1850042 -4.1761808 -4.1656613 -4.1612792 -4.159152][-4.2327733 -4.1733365 -4.0939384 -4.0207663 -3.9944954 -4.0565929 -4.1487417 -4.2013235 -4.2235584 -4.2166834 -4.1979733 -4.1848612 -4.1716104 -4.1656718 -4.1647973][-4.239922 -4.1885524 -4.1154513 -4.0400963 -4.0013075 -4.0564055 -4.147593 -4.2037725 -4.2285137 -4.2245173 -4.2069883 -4.1905947 -4.1771169 -4.1749668 -4.1774182][-4.2469234 -4.2037797 -4.1394782 -4.0677376 -4.0217872 -4.0631528 -4.1443281 -4.1970077 -4.2237711 -4.2265267 -4.2097754 -4.1877723 -4.172821 -4.1724844 -4.1780968][-4.2507973 -4.213295 -4.1571407 -4.0946593 -4.0507121 -4.0844588 -4.1527524 -4.197474 -4.220263 -4.2243967 -4.2069621 -4.1804914 -4.1613159 -4.1576653 -4.1642685][-4.2518797 -4.2173839 -4.1696448 -4.1202235 -4.0885797 -4.1197252 -4.1760964 -4.2103305 -4.22544 -4.2286763 -4.2120032 -4.1881943 -4.1724958 -4.1644168 -4.1667991][-4.2493834 -4.2176337 -4.1753182 -4.1359119 -4.1171632 -4.1506238 -4.20096 -4.2287712 -4.2378459 -4.2377281 -4.222425 -4.2050447 -4.1947803 -4.18505 -4.1827707][-4.2399807 -4.2097292 -4.1714692 -4.1383524 -4.1279087 -4.1613603 -4.2099528 -4.2401404 -4.2492 -4.2468867 -4.2338605 -4.2235665 -4.2179294 -4.2070465 -4.201333][-4.2331409 -4.2062788 -4.1726432 -4.14362 -4.1369934 -4.1660447 -4.2095842 -4.2412791 -4.2545505 -4.2540212 -4.2443233 -4.2394357 -4.2371616 -4.2292647 -4.2254114][-4.2317095 -4.2091541 -4.1815495 -4.1569471 -4.1515346 -4.1733956 -4.2087388 -4.2382874 -4.2516541 -4.2536812 -4.2469759 -4.24584 -4.2460818 -4.2415266 -4.243474][-4.2325478 -4.215909 -4.1956086 -4.1764488 -4.1723213 -4.1897759 -4.2186208 -4.2436886 -4.2531767 -4.2519126 -4.2452188 -4.2473168 -4.2520676 -4.2536521 -4.2590981][-4.2400665 -4.228209 -4.2141852 -4.200171 -4.1975846 -4.2123203 -4.235189 -4.2525229 -4.2565231 -4.2498269 -4.2398605 -4.2396908 -4.2471471 -4.2559414 -4.2653809]]...]
INFO - root - 2017-12-06 02:44:44.393662: step 3910, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.453 sec/batch; 41h:18m:37s remains)
INFO - root - 2017-12-06 02:44:49.119772: step 3920, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 42h:27m:57s remains)
INFO - root - 2017-12-06 02:44:53.903815: step 3930, loss = 2.10, batch loss = 2.04 (17.0 examples/sec; 0.471 sec/batch; 43h:01m:08s remains)
INFO - root - 2017-12-06 02:44:58.481255: step 3940, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.458 sec/batch; 41h:46m:55s remains)
INFO - root - 2017-12-06 02:45:03.138811: step 3950, loss = 2.10, batch loss = 2.04 (16.7 examples/sec; 0.480 sec/batch; 43h:48m:40s remains)
INFO - root - 2017-12-06 02:45:07.839063: step 3960, loss = 2.05, batch loss = 1.99 (18.1 examples/sec; 0.442 sec/batch; 40h:22m:13s remains)
INFO - root - 2017-12-06 02:45:12.552823: step 3970, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.480 sec/batch; 43h:48m:26s remains)
INFO - root - 2017-12-06 02:45:17.199307: step 3980, loss = 2.09, batch loss = 2.03 (17.0 examples/sec; 0.470 sec/batch; 42h:51m:01s remains)
INFO - root - 2017-12-06 02:45:21.684469: step 3990, loss = 2.05, batch loss = 1.99 (16.8 examples/sec; 0.475 sec/batch; 43h:20m:30s remains)
INFO - root - 2017-12-06 02:45:26.484556: step 4000, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.468 sec/batch; 42h:40m:33s remains)
2017-12-06 02:45:26.964402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2003026 -4.2093706 -4.2235003 -4.23447 -4.2427521 -4.2575078 -4.2770715 -4.2913418 -4.3005533 -4.3040476 -4.2977867 -4.2870116 -4.2815237 -4.2770619 -4.2706838][-4.1934571 -4.2026572 -4.2162457 -4.2265897 -4.2280369 -4.231781 -4.242744 -4.2608252 -4.2791219 -4.2916007 -4.2892618 -4.2809739 -4.2763119 -4.2687449 -4.2566285][-4.182415 -4.185173 -4.1967478 -4.2063274 -4.205936 -4.2031531 -4.207077 -4.2271962 -4.2532754 -4.2739143 -4.2773004 -4.2712097 -4.2672095 -4.2582097 -4.2412219][-4.1714272 -4.1675472 -4.1791272 -4.1917939 -4.1928921 -4.1864724 -4.1842856 -4.2038288 -4.2324605 -4.2554708 -4.2619963 -4.2578049 -4.2535958 -4.2455783 -4.2272792][-4.1717534 -4.1627388 -4.1733856 -4.1869764 -4.1880937 -4.1765094 -4.1650286 -4.177258 -4.2053471 -4.2297668 -4.240869 -4.2443352 -4.2453113 -4.2420053 -4.2248368][-4.18239 -4.173244 -4.1799345 -4.1884851 -4.1831207 -4.1603661 -4.1306219 -4.1283493 -4.1591182 -4.1916642 -4.2130671 -4.2316661 -4.2449603 -4.249104 -4.2375412][-4.1968088 -4.19253 -4.1942821 -4.1958947 -4.1818552 -4.1412759 -4.0854225 -4.0633473 -4.1048055 -4.1572075 -4.1961164 -4.2290106 -4.2508426 -4.2598629 -4.2518554][-4.2073283 -4.210053 -4.2074986 -4.2050037 -4.1880827 -4.1384974 -4.0647507 -4.023839 -4.0717206 -4.1397839 -4.1921592 -4.231884 -4.2559037 -4.2646909 -4.2579126][-4.2177653 -4.2230487 -4.2173696 -4.2118759 -4.1959715 -4.1524649 -4.0841446 -4.0405736 -4.0763512 -4.140254 -4.1931024 -4.2328014 -4.2554393 -4.2628355 -4.2584238][-4.2248759 -4.2308021 -4.225265 -4.2190971 -4.2039652 -4.1673369 -4.1119494 -4.0734096 -4.0940456 -4.14558 -4.1955585 -4.2348614 -4.2564993 -4.26461 -4.2643013][-4.2328086 -4.2391772 -4.2356777 -4.2314529 -4.2192063 -4.1891823 -4.1430321 -4.1071687 -4.1149912 -4.1564617 -4.20292 -4.2404761 -4.2623396 -4.2725697 -4.2723508][-4.2408056 -4.2459788 -4.24175 -4.2374616 -4.2290268 -4.2080083 -4.1733961 -4.1442528 -4.1462846 -4.1811514 -4.2215791 -4.2528796 -4.2715769 -4.2805257 -4.2759905][-4.2521496 -4.2556076 -4.24828 -4.2382097 -4.2291965 -4.214716 -4.1924109 -4.1742921 -4.17694 -4.2074318 -4.2413244 -4.2643533 -4.2761116 -4.2805233 -4.2725334][-4.2686315 -4.2688627 -4.2569966 -4.2375712 -4.220396 -4.2064075 -4.1938553 -4.1856771 -4.1904526 -4.2174058 -4.25055 -4.269453 -4.2775106 -4.2796516 -4.2706256][-4.2795014 -4.2781024 -4.261363 -4.2329249 -4.2089972 -4.1931214 -4.1870408 -4.1857648 -4.18948 -4.2135019 -4.2495222 -4.270319 -4.2809343 -4.2852859 -4.2771173]]...]
INFO - root - 2017-12-06 02:45:31.746536: step 4010, loss = 2.05, batch loss = 1.99 (16.9 examples/sec; 0.473 sec/batch; 43h:07m:31s remains)
INFO - root - 2017-12-06 02:45:36.448296: step 4020, loss = 2.04, batch loss = 1.98 (17.0 examples/sec; 0.469 sec/batch; 42h:49m:32s remains)
INFO - root - 2017-12-06 02:45:41.180402: step 4030, loss = 2.04, batch loss = 1.98 (16.8 examples/sec; 0.477 sec/batch; 43h:30m:54s remains)
INFO - root - 2017-12-06 02:45:45.870853: step 4040, loss = 2.03, batch loss = 1.97 (17.2 examples/sec; 0.465 sec/batch; 42h:25m:47s remains)
INFO - root - 2017-12-06 02:45:50.631084: step 4050, loss = 2.05, batch loss = 1.99 (16.5 examples/sec; 0.484 sec/batch; 44h:09m:12s remains)
INFO - root - 2017-12-06 02:45:55.332465: step 4060, loss = 2.04, batch loss = 1.98 (17.4 examples/sec; 0.460 sec/batch; 42h:00m:05s remains)
INFO - root - 2017-12-06 02:46:00.035595: step 4070, loss = 2.04, batch loss = 1.98 (16.0 examples/sec; 0.499 sec/batch; 45h:32m:46s remains)
INFO - root - 2017-12-06 02:46:04.616522: step 4080, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.473 sec/batch; 43h:06m:41s remains)
INFO - root - 2017-12-06 02:46:09.254296: step 4090, loss = 2.04, batch loss = 1.99 (18.1 examples/sec; 0.442 sec/batch; 40h:20m:28s remains)
INFO - root - 2017-12-06 02:46:13.918910: step 4100, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.473 sec/batch; 43h:10m:08s remains)
2017-12-06 02:46:14.404416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3314323 -4.3326778 -4.3325739 -4.3327994 -4.3350768 -4.3404636 -4.3407769 -4.3379183 -4.3399587 -4.341783 -4.3374653 -4.3323383 -4.329772 -4.3290782 -4.3279018][-4.3088522 -4.3092413 -4.3078713 -4.3079014 -4.31289 -4.3207006 -4.321465 -4.3195357 -4.32287 -4.3226004 -4.3148346 -4.307857 -4.3043628 -4.3076172 -4.3114567][-4.2681341 -4.2680092 -4.2680125 -4.2712774 -4.2799549 -4.2909808 -4.2924595 -4.2924929 -4.297833 -4.2928524 -4.2800112 -4.2747259 -4.2752557 -4.2830081 -4.2908297][-4.2271013 -4.22864 -4.2310929 -4.2396545 -4.2522407 -4.26354 -4.2619495 -4.2592516 -4.26177 -4.2506566 -4.2366982 -4.2386227 -4.2459922 -4.2585173 -4.2700877][-4.1938548 -4.1959329 -4.2006145 -4.2138877 -4.2266388 -4.2315769 -4.2174826 -4.2050261 -4.2023869 -4.1919451 -4.190093 -4.2089849 -4.22701 -4.2414832 -4.2514458][-4.1809454 -4.1819468 -4.186193 -4.1981025 -4.2042065 -4.1889272 -4.1469622 -4.1166582 -4.1228042 -4.1370072 -4.1635628 -4.2003603 -4.2235565 -4.2326055 -4.2354593][-4.1828027 -4.1797218 -4.1757131 -4.1748214 -4.1660924 -4.1228452 -4.0437617 -4.00006 -4.0427732 -4.1048913 -4.1615443 -4.2035995 -4.222692 -4.2242818 -4.2213693][-4.1911187 -4.1760244 -4.1579962 -4.1491308 -4.1313572 -4.0674305 -3.9644954 -3.9242671 -4.0102196 -4.1083984 -4.1759343 -4.2098384 -4.218998 -4.2114773 -4.2003026][-4.1995392 -4.1750436 -4.1556683 -4.153501 -4.1440425 -4.0982141 -4.03017 -4.0155916 -4.0884781 -4.1643729 -4.2086568 -4.2220635 -4.214026 -4.19591 -4.187211][-4.1955748 -4.17555 -4.1659665 -4.1724496 -4.1752129 -4.1605053 -4.1335096 -4.1316504 -4.173193 -4.2106614 -4.2283 -4.2179675 -4.1935434 -4.1746616 -4.1826105][-4.1853347 -4.1784916 -4.182013 -4.193819 -4.2044253 -4.2095575 -4.2032847 -4.1984844 -4.2116885 -4.2187924 -4.214674 -4.18963 -4.1647096 -4.1616635 -4.1859574][-4.1750422 -4.1800342 -4.1916976 -4.2046208 -4.2157521 -4.2295041 -4.236042 -4.2304587 -4.2230129 -4.210289 -4.1899691 -4.1638846 -4.1534734 -4.1658111 -4.1985087][-4.1443563 -4.1550951 -4.1654449 -4.17346 -4.1818318 -4.199019 -4.2142482 -4.2122231 -4.1993971 -4.1781287 -4.1566176 -4.1417065 -4.1472106 -4.1682053 -4.2045536][-4.0929661 -4.1000252 -4.1045156 -4.1090035 -4.1173716 -4.1345854 -4.1527839 -4.1560187 -4.148345 -4.1373219 -4.1269813 -4.1238194 -4.1341553 -4.1558146 -4.1911011][-4.0649905 -4.0657344 -4.0648742 -4.0671859 -4.075664 -4.0900249 -4.106842 -4.112844 -4.1145091 -4.1178946 -4.1200943 -4.1203985 -4.1274953 -4.1447086 -4.1740165]]...]
INFO - root - 2017-12-06 02:46:19.177110: step 4110, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.463 sec/batch; 42h:16m:17s remains)
INFO - root - 2017-12-06 02:46:23.865710: step 4120, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.470 sec/batch; 42h:53m:34s remains)
INFO - root - 2017-12-06 02:46:28.574090: step 4130, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.473 sec/batch; 43h:11m:12s remains)
INFO - root - 2017-12-06 02:46:33.254695: step 4140, loss = 2.03, batch loss = 1.97 (16.2 examples/sec; 0.493 sec/batch; 45h:00m:37s remains)
INFO - root - 2017-12-06 02:46:37.979320: step 4150, loss = 2.03, batch loss = 1.97 (17.3 examples/sec; 0.462 sec/batch; 42h:08m:35s remains)
INFO - root - 2017-12-06 02:46:42.704853: step 4160, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.463 sec/batch; 42h:14m:15s remains)
INFO - root - 2017-12-06 02:46:47.311134: step 4170, loss = 2.10, batch loss = 2.04 (17.1 examples/sec; 0.468 sec/batch; 42h:39m:15s remains)
INFO - root - 2017-12-06 02:46:51.716319: step 4180, loss = 2.05, batch loss = 2.00 (16.7 examples/sec; 0.478 sec/batch; 43h:34m:33s remains)
INFO - root - 2017-12-06 02:46:56.447396: step 4190, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.478 sec/batch; 43h:36m:06s remains)
INFO - root - 2017-12-06 02:47:01.144189: step 4200, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.462 sec/batch; 42h:09m:14s remains)
2017-12-06 02:47:01.675802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1819811 -4.2035403 -4.2098355 -4.206418 -4.1829562 -4.1445312 -4.1067905 -4.0871964 -4.0974455 -4.1117139 -4.1132283 -4.1143112 -4.1261458 -4.1592445 -4.1966343][-4.1994882 -4.2272868 -4.2371683 -4.23136 -4.2029328 -4.1604629 -4.1246991 -4.1068182 -4.1143417 -4.1210847 -4.1140456 -4.1083555 -4.1130328 -4.1346292 -4.1627688][-4.2202735 -4.2462258 -4.2550755 -4.2436295 -4.2098203 -4.1594286 -4.1167235 -4.1004148 -4.1143951 -4.1290231 -4.1232066 -4.1104512 -4.1109891 -4.1265163 -4.1426187][-4.2371616 -4.2546096 -4.2564511 -4.2357612 -4.195044 -4.1346149 -4.0790315 -4.0584831 -4.0814276 -4.1128488 -4.1242838 -4.1161933 -4.1204677 -4.1365418 -4.1511092][-4.2545371 -4.26072 -4.25385 -4.2252822 -4.1725512 -4.0973387 -4.0272336 -4.0014734 -4.029458 -4.0802574 -4.1176219 -4.1298161 -4.1415911 -4.1596885 -4.1760526][-4.2671628 -4.26524 -4.2549462 -4.2233129 -4.1627564 -4.0760012 -3.9979656 -3.9636817 -3.9855444 -4.0440125 -4.1076221 -4.1474485 -4.1677046 -4.1844811 -4.2034521][-4.276413 -4.2729783 -4.2614961 -4.2301006 -4.1685982 -4.0872369 -4.0137777 -3.9703197 -3.9721203 -4.0173225 -4.08852 -4.1474457 -4.1787205 -4.1975613 -4.2205458][-4.2797885 -4.27611 -4.264977 -4.2355556 -4.1833224 -4.1194177 -4.0584049 -4.014461 -3.9990325 -4.0196776 -4.0766625 -4.1380372 -4.1801553 -4.2084885 -4.2366872][-4.279305 -4.2759018 -4.266531 -4.2414775 -4.2013474 -4.156579 -4.1086984 -4.0667672 -4.04364 -4.0468211 -4.0861821 -4.1404977 -4.1866832 -4.2208214 -4.2502236][-4.2801914 -4.2757058 -4.2692323 -4.2546983 -4.2301664 -4.20061 -4.1596255 -4.117547 -4.0906858 -4.0891457 -4.1185164 -4.163487 -4.2053866 -4.2394204 -4.2659082][-4.290956 -4.2854595 -4.2811022 -4.2749214 -4.2632046 -4.2443976 -4.2105551 -4.1699691 -4.1421027 -4.136373 -4.158916 -4.1958919 -4.2325568 -4.2650661 -4.2875071][-4.3082247 -4.3030529 -4.3003812 -4.2967029 -4.2904034 -4.2773767 -4.2544351 -4.2244663 -4.2011828 -4.1904054 -4.2020254 -4.2298336 -4.2593551 -4.2856431 -4.3041992][-4.3143635 -4.3092117 -4.3081942 -4.3073974 -4.305994 -4.3011346 -4.2890043 -4.2692838 -4.2524252 -4.2417641 -4.2461352 -4.2623196 -4.2806888 -4.29731 -4.3109288][-4.316721 -4.3117242 -4.3101459 -4.310894 -4.3124413 -4.3130879 -4.3083372 -4.2977281 -4.285871 -4.2772112 -4.2789745 -4.2887158 -4.29919 -4.3061776 -4.313168][-4.322927 -4.3190732 -4.3164911 -4.3158965 -4.3161654 -4.3164206 -4.3150139 -4.3109794 -4.3040524 -4.299716 -4.3011904 -4.3082619 -4.3151894 -4.3172836 -4.3197746]]...]
INFO - root - 2017-12-06 02:47:06.577859: step 4210, loss = 2.07, batch loss = 2.02 (16.3 examples/sec; 0.491 sec/batch; 44h:44m:02s remains)
INFO - root - 2017-12-06 02:47:11.342516: step 4220, loss = 2.10, batch loss = 2.04 (16.7 examples/sec; 0.480 sec/batch; 43h:44m:31s remains)
INFO - root - 2017-12-06 02:47:16.115980: step 4230, loss = 2.03, batch loss = 1.98 (16.3 examples/sec; 0.491 sec/batch; 44h:46m:36s remains)
INFO - root - 2017-12-06 02:47:20.812748: step 4240, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 0.484 sec/batch; 44h:08m:17s remains)
INFO - root - 2017-12-06 02:47:25.441409: step 4250, loss = 2.04, batch loss = 1.98 (16.9 examples/sec; 0.475 sec/batch; 43h:17m:02s remains)
INFO - root - 2017-12-06 02:47:30.200284: step 4260, loss = 2.09, batch loss = 2.03 (16.7 examples/sec; 0.479 sec/batch; 43h:40m:05s remains)
INFO - root - 2017-12-06 02:47:34.788757: step 4270, loss = 2.06, batch loss = 2.00 (21.7 examples/sec; 0.369 sec/batch; 33h:36m:22s remains)
INFO - root - 2017-12-06 02:47:39.348389: step 4280, loss = 2.04, batch loss = 1.98 (16.9 examples/sec; 0.474 sec/batch; 43h:14m:38s remains)
INFO - root - 2017-12-06 02:47:44.105993: step 4290, loss = 2.08, batch loss = 2.02 (16.1 examples/sec; 0.498 sec/batch; 45h:26m:14s remains)
INFO - root - 2017-12-06 02:47:48.810947: step 4300, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.452 sec/batch; 41h:13m:08s remains)
2017-12-06 02:47:49.302572: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2636423 -4.2298918 -4.166369 -4.0966444 -4.0450048 -4.0029387 -3.9791777 -4.0191345 -4.0751262 -4.0994382 -4.1003737 -4.0942764 -4.0902286 -4.097014 -4.1052618][-4.2636809 -4.2258267 -4.1554618 -4.0798693 -4.0256186 -3.9847012 -3.965065 -4.0150509 -4.0768337 -4.0986047 -4.0968666 -4.0882468 -4.0859933 -4.0926147 -4.102293][-4.2597308 -4.2178392 -4.143537 -4.0666294 -4.0182958 -3.9869602 -3.9704936 -4.0182567 -4.0709853 -4.0846438 -4.08245 -4.0778036 -4.0812831 -4.095592 -4.109211][-4.2557888 -4.2142 -4.139256 -4.0653982 -4.024456 -3.9988348 -3.9793122 -4.016789 -4.058342 -4.0667834 -4.0670652 -4.0734153 -4.0848432 -4.1052251 -4.1205835][-4.2571259 -4.217236 -4.1437397 -4.0701261 -4.0277448 -3.9924605 -3.9550297 -3.9782472 -4.0218921 -4.0440831 -4.0571127 -4.0740452 -4.0883431 -4.107913 -4.1208282][-4.2619572 -4.2225513 -4.1506538 -4.0748153 -4.0199232 -3.9532948 -3.8723764 -3.8752086 -3.947372 -4.0131683 -4.0531716 -4.0767736 -4.085319 -4.0951176 -4.0990443][-4.2676029 -4.2260113 -4.1537533 -4.0754809 -4.0083103 -3.9016228 -3.7600069 -3.7464168 -3.8786221 -4.003016 -4.0675907 -4.090734 -4.091969 -4.0911994 -4.0853386][-4.2688308 -4.2241092 -4.1527662 -4.0767355 -4.0093403 -3.891778 -3.7436936 -3.7451882 -3.9081938 -4.0357356 -4.0936842 -4.1092868 -4.1069026 -4.0991626 -4.090127][-4.2649751 -4.2185507 -4.1499362 -4.0820341 -4.0321288 -3.9509058 -3.8643603 -3.8909593 -4.0085225 -4.0837035 -4.1124024 -4.11819 -4.1192565 -4.1161938 -4.1137075][-4.2621827 -4.218678 -4.1538634 -4.0938597 -4.0590687 -4.0162668 -3.9794154 -4.013823 -4.0841155 -4.117384 -4.1291504 -4.1329527 -4.1429687 -4.1461511 -4.1454849][-4.2648869 -4.2264113 -4.1653214 -4.1086173 -4.0786963 -4.0492735 -4.029326 -4.0627017 -4.1145573 -4.1327505 -4.1360335 -4.1399813 -4.1568775 -4.1676059 -4.1678629][-4.2702827 -4.2358623 -4.1764445 -4.1191931 -4.0869431 -4.0576468 -4.0421124 -4.076345 -4.1276422 -4.1432996 -4.1422195 -4.1412878 -4.1553493 -4.1692567 -4.1700883][-4.2729492 -4.2382321 -4.1791186 -4.1216521 -4.0889521 -4.0600414 -4.0476432 -4.0853233 -4.1361661 -4.1523833 -4.1520438 -4.1498079 -4.1550512 -4.162487 -4.1610851][-4.2727537 -4.2341566 -4.1755948 -4.118628 -4.0862317 -4.0628853 -4.0615659 -4.1047649 -4.150866 -4.1673942 -4.1681671 -4.1635027 -4.1571803 -4.1552773 -4.1525788][-4.2739568 -4.2333541 -4.1751094 -4.1154823 -4.0781412 -4.0603957 -4.071373 -4.118556 -4.1586828 -4.1748691 -4.17742 -4.1721344 -4.1630316 -4.1589656 -4.1541524]]...]
INFO - root - 2017-12-06 02:47:54.181820: step 4310, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.472 sec/batch; 43h:01m:13s remains)
INFO - root - 2017-12-06 02:47:58.816923: step 4320, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.460 sec/batch; 41h:54m:44s remains)
INFO - root - 2017-12-06 02:48:03.644272: step 4330, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.424 sec/batch; 38h:36m:43s remains)
INFO - root - 2017-12-06 02:48:08.349123: step 4340, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.470 sec/batch; 42h:50m:47s remains)
INFO - root - 2017-12-06 02:48:13.069210: step 4350, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.468 sec/batch; 42h:40m:40s remains)
INFO - root - 2017-12-06 02:48:17.743215: step 4360, loss = 2.05, batch loss = 1.99 (16.7 examples/sec; 0.480 sec/batch; 43h:45m:19s remains)
INFO - root - 2017-12-06 02:48:22.235577: step 4370, loss = 2.09, batch loss = 2.03 (16.8 examples/sec; 0.477 sec/batch; 43h:31m:00s remains)
INFO - root - 2017-12-06 02:48:26.885031: step 4380, loss = 2.05, batch loss = 1.99 (17.8 examples/sec; 0.450 sec/batch; 41h:01m:41s remains)
INFO - root - 2017-12-06 02:48:31.664644: step 4390, loss = 2.04, batch loss = 1.98 (16.0 examples/sec; 0.502 sec/batch; 45h:42m:27s remains)
INFO - root - 2017-12-06 02:48:36.232806: step 4400, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.459 sec/batch; 41h:49m:37s remains)
2017-12-06 02:48:36.674972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2546825 -4.2533083 -4.255127 -4.2587104 -4.2608962 -4.2643342 -4.2644 -4.2643671 -4.2628603 -4.2582884 -4.2522912 -4.2372766 -4.2188249 -4.2108397 -4.2151351][-4.2389526 -4.2409272 -4.2441435 -4.2459893 -4.2435393 -4.2398443 -4.233808 -4.232666 -4.2356443 -4.2348819 -4.2387576 -4.2321706 -4.2134743 -4.1973982 -4.197494][-4.222477 -4.2265024 -4.23007 -4.2288327 -4.2207327 -4.2082515 -4.1936035 -4.1875296 -4.1928639 -4.1984596 -4.2136631 -4.2214875 -4.2095523 -4.1933537 -4.188385][-4.2017689 -4.2110467 -4.2153993 -4.2124262 -4.1994581 -4.1771851 -4.1513958 -4.1382804 -4.1432428 -4.1545849 -4.1794748 -4.20358 -4.2031941 -4.1938338 -4.1893225][-4.1778336 -4.193378 -4.1974373 -4.1911511 -4.1731262 -4.1420207 -4.10465 -4.082736 -4.0882092 -4.106236 -4.1383681 -4.1760597 -4.1921668 -4.1926451 -4.1941671][-4.1565676 -4.1780167 -4.1803761 -4.1691332 -4.1429749 -4.1031046 -4.054657 -4.0279117 -4.0389924 -4.0688872 -4.1074586 -4.1535339 -4.1853428 -4.196475 -4.2016826][-4.1387277 -4.1610913 -4.1603923 -4.1450415 -4.1138458 -4.0692573 -4.0140367 -3.9834528 -4.0034213 -4.0497785 -4.0949216 -4.1436219 -4.1850057 -4.2028728 -4.2095408][-4.1273661 -4.1462722 -4.1433911 -4.1324663 -4.1036744 -4.0554223 -3.9938853 -3.9553373 -3.9830937 -4.04753 -4.0982137 -4.1425571 -4.1808176 -4.2014914 -4.2090831][-4.1194811 -4.13971 -4.1425476 -4.142055 -4.118896 -4.0670013 -3.99718 -3.9449162 -3.9719968 -4.0485988 -4.1054425 -4.144877 -4.1732616 -4.1910095 -4.1991258][-4.1091838 -4.1324034 -4.1484246 -4.1647692 -4.151546 -4.1009169 -4.0265808 -3.9591327 -3.9723377 -4.049603 -4.1069641 -4.1445961 -4.1662197 -4.1809945 -4.1884265][-4.1085286 -4.1354427 -4.1590152 -4.1833611 -4.1785855 -4.1320863 -4.0568657 -3.9823198 -3.9800503 -4.0516577 -4.1091928 -4.1461911 -4.1627088 -4.173429 -4.1788111][-4.1032329 -4.1301041 -4.1530523 -4.1768951 -4.17831 -4.13957 -4.0657043 -3.9888811 -3.9762061 -4.0423541 -4.1023455 -4.1402845 -4.1558089 -4.163928 -4.169014][-4.0883665 -4.106976 -4.1264038 -4.1469297 -4.1516223 -4.1246467 -4.0592833 -3.9848313 -3.9625862 -4.0192976 -4.0803337 -4.121665 -4.1390595 -4.14862 -4.1602225][-4.0789771 -4.0800285 -4.0921 -4.1107903 -4.1227846 -4.1121745 -4.0655994 -4.0043883 -3.9793215 -4.0188651 -4.0702744 -4.1129336 -4.136035 -4.1497397 -4.1665177][-4.084466 -4.0683341 -4.0685077 -4.0854325 -4.1058841 -4.1141343 -4.087307 -4.038888 -4.0116954 -4.0365448 -4.0780649 -4.1204729 -4.1508427 -4.1705608 -4.1879673]]...]
INFO - root - 2017-12-06 02:48:41.351281: step 4410, loss = 2.08, batch loss = 2.02 (16.9 examples/sec; 0.475 sec/batch; 43h:14m:53s remains)
INFO - root - 2017-12-06 02:48:46.052653: step 4420, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.468 sec/batch; 42h:39m:16s remains)
INFO - root - 2017-12-06 02:48:50.819959: step 4430, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.467 sec/batch; 42h:31m:36s remains)
INFO - root - 2017-12-06 02:48:55.496698: step 4440, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.467 sec/batch; 42h:35m:44s remains)
INFO - root - 2017-12-06 02:49:00.358890: step 4450, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.472 sec/batch; 42h:58m:32s remains)
INFO - root - 2017-12-06 02:49:04.955137: step 4460, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.469 sec/batch; 42h:46m:37s remains)
INFO - root - 2017-12-06 02:49:09.405944: step 4470, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.463 sec/batch; 42h:08m:53s remains)
INFO - root - 2017-12-06 02:49:13.973465: step 4480, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.459 sec/batch; 41h:47m:36s remains)
INFO - root - 2017-12-06 02:49:18.652761: step 4490, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.459 sec/batch; 41h:50m:41s remains)
INFO - root - 2017-12-06 02:49:23.415675: step 4500, loss = 2.04, batch loss = 1.98 (17.6 examples/sec; 0.454 sec/batch; 41h:20m:05s remains)
2017-12-06 02:49:23.902156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28211 -4.2906184 -4.2982836 -4.3016238 -4.2952261 -4.2752395 -4.24019 -4.216249 -4.2205782 -4.2382736 -4.2546897 -4.2633748 -4.2695961 -4.2796869 -4.2853489][-4.2640929 -4.2657366 -4.274631 -4.2857637 -4.2907043 -4.2771692 -4.2432542 -4.2161565 -4.2220821 -4.243958 -4.2582612 -4.2642808 -4.2654657 -4.266542 -4.2609243][-4.2443414 -4.2353668 -4.2356148 -4.2424655 -4.2491417 -4.2376347 -4.2058496 -4.1853752 -4.1981058 -4.2251868 -4.2408819 -4.2436256 -4.2364068 -4.221087 -4.2030654][-4.2187486 -4.1991396 -4.1895895 -4.1854348 -4.1827507 -4.1682458 -4.1402693 -4.128633 -4.1512437 -4.1863174 -4.2047577 -4.2058649 -4.1897578 -4.1634216 -4.1433487][-4.1896691 -4.1599903 -4.1390066 -4.1226921 -4.1122603 -4.1004715 -4.0798573 -4.0727458 -4.104455 -4.1519303 -4.1801353 -4.1841464 -4.1643171 -4.1407428 -4.1285577][-4.1535211 -4.1157575 -4.0854878 -4.0584 -4.0402732 -4.0308418 -4.0123138 -4.000411 -4.0391216 -4.110137 -4.16068 -4.1767468 -4.16683 -4.1582942 -4.1518521][-4.1227818 -4.0828934 -4.0487342 -4.0107317 -3.9843516 -3.9681561 -3.9340746 -3.9011254 -3.9421787 -4.0433121 -4.1264672 -4.1653552 -4.1731982 -4.18316 -4.1785059][-4.1208477 -4.0913606 -4.0582471 -4.0179238 -3.9885774 -3.9636016 -3.9158053 -3.8643124 -3.8987806 -4.0066328 -4.0998735 -4.1493816 -4.1705847 -4.1968703 -4.1959591][-4.1452203 -4.130414 -4.1083889 -4.0820012 -4.0625682 -4.03626 -3.9906135 -3.9468617 -3.9716525 -4.049129 -4.1158953 -4.15261 -4.1724939 -4.2066855 -4.214777][-4.1818829 -4.1798644 -4.1699014 -4.1561227 -4.1455226 -4.1185555 -4.0784607 -4.0446424 -4.0596642 -4.1044569 -4.1454587 -4.166368 -4.1796279 -4.2109661 -4.2248368][-4.219913 -4.2263441 -4.221354 -4.2115989 -4.2024803 -4.1773925 -4.1457195 -4.1227021 -4.1294088 -4.1514792 -4.1751585 -4.1871676 -4.1954656 -4.2188478 -4.2276416][-4.2469172 -4.2571054 -4.2518091 -4.24346 -4.2349186 -4.2145348 -4.1935334 -4.1801915 -4.1822324 -4.1886063 -4.2018156 -4.2103457 -4.2151213 -4.228972 -4.2301846][-4.2596211 -4.2701769 -4.2676716 -4.26292 -4.25773 -4.2469311 -4.2388811 -4.2339811 -4.2330136 -4.232686 -4.237329 -4.240469 -4.2371874 -4.2393408 -4.2320046][-4.2635341 -4.273849 -4.2738538 -4.2720623 -4.2718225 -4.2706003 -4.2730503 -4.2744455 -4.2731967 -4.270196 -4.2705216 -4.2688818 -4.2600536 -4.2553239 -4.2442193][-4.2694745 -4.2763462 -4.2717962 -4.2664924 -4.2670717 -4.2715297 -4.2796335 -4.2847805 -4.2862935 -4.2858219 -4.2864685 -4.2835183 -4.2742481 -4.2655015 -4.2529674]]...]
INFO - root - 2017-12-06 02:49:28.722098: step 4510, loss = 2.04, batch loss = 1.98 (16.3 examples/sec; 0.492 sec/batch; 44h:47m:44s remains)
INFO - root - 2017-12-06 02:49:33.362779: step 4520, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.466 sec/batch; 42h:28m:01s remains)
INFO - root - 2017-12-06 02:49:38.044973: step 4530, loss = 2.06, batch loss = 2.00 (18.5 examples/sec; 0.431 sec/batch; 39h:18m:28s remains)
INFO - root - 2017-12-06 02:49:42.731684: step 4540, loss = 2.04, batch loss = 1.98 (16.9 examples/sec; 0.474 sec/batch; 43h:11m:45s remains)
INFO - root - 2017-12-06 02:49:47.387451: step 4550, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.485 sec/batch; 44h:11m:29s remains)
INFO - root - 2017-12-06 02:49:51.963593: step 4560, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.467 sec/batch; 42h:34m:19s remains)
INFO - root - 2017-12-06 02:49:56.610432: step 4570, loss = 2.06, batch loss = 2.01 (17.5 examples/sec; 0.456 sec/batch; 41h:32m:45s remains)
INFO - root - 2017-12-06 02:50:01.275664: step 4580, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.468 sec/batch; 42h:40m:02s remains)
INFO - root - 2017-12-06 02:50:05.941106: step 4590, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.439 sec/batch; 39h:58m:14s remains)
INFO - root - 2017-12-06 02:50:10.625625: step 4600, loss = 2.08, batch loss = 2.03 (17.4 examples/sec; 0.460 sec/batch; 41h:51m:15s remains)
2017-12-06 02:50:11.110715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2194557 -4.2189827 -4.2211165 -4.2235055 -4.225358 -4.226274 -4.2265086 -4.2267728 -4.2252297 -4.2236967 -4.2264981 -4.2310615 -4.2317538 -4.230166 -4.2319555][-4.2097044 -4.208868 -4.2106829 -4.213377 -4.2155209 -4.2164454 -4.216198 -4.2164435 -4.2140388 -4.2102127 -4.2095127 -4.2118306 -4.2123351 -4.2116718 -4.2155418][-4.21122 -4.21023 -4.2110367 -4.212636 -4.2146926 -4.2155757 -4.2150474 -4.2155914 -4.2146606 -4.2106733 -4.2083158 -4.2110305 -4.212501 -4.2150135 -4.2213349][-4.2075291 -4.2035441 -4.2010541 -4.1994238 -4.2007976 -4.2025967 -4.2042146 -4.20898 -4.2143703 -4.2147145 -4.2134342 -4.217761 -4.2224851 -4.2285762 -4.2361374][-4.1995854 -4.1910977 -4.1827712 -4.17607 -4.1730828 -4.171174 -4.1711607 -4.1791234 -4.1936779 -4.2040205 -4.2121086 -4.2236319 -4.2341094 -4.2434173 -4.2527175][-4.1649261 -4.1514769 -4.1362376 -4.1240835 -4.1132092 -4.1010008 -4.09383 -4.101943 -4.1263633 -4.1509037 -4.1761742 -4.2041841 -4.2283497 -4.246666 -4.2621303][-4.1148515 -4.0948462 -4.0725036 -4.052177 -4.0271387 -3.997709 -3.9762471 -3.9794555 -4.0104871 -4.049088 -4.0937042 -4.1427069 -4.1850886 -4.2164621 -4.2414136][-4.1150355 -4.0938058 -4.0729704 -4.0523505 -4.0199323 -3.9785848 -3.9418938 -3.9295137 -3.9472282 -3.9828377 -4.03328 -4.091455 -4.1436362 -4.1822419 -4.2142854][-4.1587358 -4.1427417 -4.1323752 -4.1221685 -4.0982671 -4.0658212 -4.0353932 -4.0187526 -4.0200086 -4.0364132 -4.071074 -4.1188865 -4.1635633 -4.1961069 -4.2238917][-4.2044272 -4.1937604 -4.1878619 -4.1825404 -4.1692495 -4.15125 -4.1337185 -4.123888 -4.1245651 -4.1354742 -4.1594243 -4.194767 -4.2262225 -4.24628 -4.2621264][-4.2414122 -4.2356343 -4.2326846 -4.230866 -4.2264671 -4.2193308 -4.2120357 -4.2088175 -4.2110157 -4.2187667 -4.2348776 -4.2599115 -4.2812862 -4.2918377 -4.295969][-4.272891 -4.2688942 -4.2668643 -4.26723 -4.268486 -4.2693505 -4.2698345 -4.2721863 -4.2738209 -4.2758951 -4.2823119 -4.29466 -4.3069711 -4.3121157 -4.3112655][-4.283205 -4.2827063 -4.2823029 -4.2844558 -4.2888365 -4.2913089 -4.294004 -4.3003335 -4.3005586 -4.2958279 -4.29257 -4.2941 -4.29906 -4.3009048 -4.2987037][-4.2670116 -4.2686391 -4.2699485 -4.2733617 -4.278316 -4.2797632 -4.2806749 -4.286922 -4.2863717 -4.2753983 -4.265049 -4.2619605 -4.265182 -4.2709541 -4.2733235][-4.2531576 -4.253993 -4.2541404 -4.2571321 -4.260746 -4.2605414 -4.2592397 -4.263454 -4.2617607 -4.2460785 -4.2285528 -4.222692 -4.2262421 -4.238874 -4.2487583]]...]
INFO - root - 2017-12-06 02:50:15.842778: step 4610, loss = 2.03, batch loss = 1.97 (16.5 examples/sec; 0.485 sec/batch; 44h:07m:49s remains)
INFO - root - 2017-12-06 02:50:20.570135: step 4620, loss = 2.07, batch loss = 2.01 (16.4 examples/sec; 0.488 sec/batch; 44h:28m:28s remains)
INFO - root - 2017-12-06 02:50:25.280558: step 4630, loss = 2.05, batch loss = 2.00 (17.3 examples/sec; 0.462 sec/batch; 42h:05m:42s remains)
INFO - root - 2017-12-06 02:50:29.933696: step 4640, loss = 2.05, batch loss = 1.99 (17.8 examples/sec; 0.449 sec/batch; 40h:54m:48s remains)
INFO - root - 2017-12-06 02:50:34.641029: step 4650, loss = 2.03, batch loss = 1.98 (16.9 examples/sec; 0.473 sec/batch; 43h:03m:08s remains)
INFO - root - 2017-12-06 02:50:39.004848: step 4660, loss = 2.09, batch loss = 2.03 (17.2 examples/sec; 0.466 sec/batch; 42h:26m:27s remains)
INFO - root - 2017-12-06 02:50:43.753106: step 4670, loss = 2.04, batch loss = 1.98 (17.0 examples/sec; 0.470 sec/batch; 42h:49m:51s remains)
INFO - root - 2017-12-06 02:50:48.471535: step 4680, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.461 sec/batch; 41h:59m:29s remains)
INFO - root - 2017-12-06 02:50:53.207347: step 4690, loss = 2.03, batch loss = 1.97 (16.6 examples/sec; 0.481 sec/batch; 43h:48m:56s remains)
INFO - root - 2017-12-06 02:50:57.867874: step 4700, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.465 sec/batch; 42h:19m:55s remains)
2017-12-06 02:50:58.359211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2528234 -4.2608604 -4.2629938 -4.2668624 -4.2750163 -4.28049 -4.2794042 -4.2734337 -4.2649198 -4.260335 -4.2620258 -4.2647657 -4.2607989 -4.2489924 -4.233933][-4.2916636 -4.3029413 -4.3072071 -4.3092752 -4.31063 -4.3069363 -4.2987866 -4.2893229 -4.2784286 -4.2689223 -4.2638569 -4.2587762 -4.2478395 -4.2314744 -4.2180481][-4.3087211 -4.3224597 -4.3293424 -4.3306193 -4.3245254 -4.3099012 -4.2946997 -4.2829571 -4.2724538 -4.2607379 -4.2501788 -4.2344761 -4.2122474 -4.1917462 -4.1843629][-4.2981229 -4.3137937 -4.322485 -4.3224316 -4.3103471 -4.2882586 -4.2682295 -4.2572112 -4.2504721 -4.2415318 -4.2272353 -4.2022319 -4.1726437 -4.1488957 -4.1445293][-4.2689962 -4.2832365 -4.2912936 -4.2884789 -4.2718973 -4.2438364 -4.2223296 -4.2187238 -4.2210989 -4.2173276 -4.1998878 -4.1693883 -4.1383562 -4.1170607 -4.112505][-4.2282948 -4.2348394 -4.2382612 -4.2307978 -4.206357 -4.16879 -4.148684 -4.1585193 -4.1774778 -4.1831055 -4.1684833 -4.1405973 -4.1182 -4.1057158 -4.1012797][-4.1962433 -4.1903477 -4.1813803 -4.160851 -4.1207981 -4.06983 -4.0534024 -4.0821676 -4.1229239 -4.1420155 -4.1381927 -4.1205077 -4.1111884 -4.1154141 -4.1218796][-4.1886783 -4.1701484 -4.1462121 -4.110249 -4.0516858 -3.9858739 -3.96877 -4.0128903 -4.0725574 -4.1042957 -4.1099811 -4.0976624 -4.0977974 -4.1211267 -4.1468992][-4.186039 -4.1644559 -4.1382413 -4.10561 -4.0500932 -3.9885175 -3.9705927 -4.0093389 -4.0658159 -4.0978856 -4.1076035 -4.0959458 -4.0958552 -4.1238356 -4.160635][-4.1774287 -4.1628408 -4.1482506 -4.1339383 -4.1019363 -4.0650377 -4.05542 -4.0804849 -4.1174965 -4.1394668 -4.1453252 -4.1331797 -4.12568 -4.1426711 -4.1716352][-4.1713843 -4.1679821 -4.16989 -4.1723385 -4.1609635 -4.1455317 -4.1437955 -4.1570082 -4.1750822 -4.1885657 -4.1913323 -4.1795464 -4.1660438 -4.1655822 -4.1796374][-4.1685214 -4.1751733 -4.19265 -4.2087541 -4.2113452 -4.2065744 -4.2083383 -4.2130747 -4.2182117 -4.2269845 -4.2286434 -4.21882 -4.2023892 -4.1880879 -4.1850905][-4.1889453 -4.1976075 -4.22116 -4.2420015 -4.2464232 -4.2380023 -4.2317362 -4.2298517 -4.2327828 -4.2437768 -4.2491808 -4.2435145 -4.2288842 -4.209455 -4.1947384][-4.2299685 -4.2357044 -4.2543068 -4.2659688 -4.259975 -4.23922 -4.221086 -4.215301 -4.2203574 -4.2379746 -4.2502289 -4.2497907 -4.2382345 -4.2209592 -4.2038045][-4.2756147 -4.2759237 -4.2836337 -4.2814283 -4.2640185 -4.2336125 -4.2069106 -4.1952281 -4.1991625 -4.2213063 -4.2407718 -4.2450938 -4.2366304 -4.2243195 -4.2122784]]...]
INFO - root - 2017-12-06 02:51:03.065127: step 4710, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.461 sec/batch; 41h:59m:02s remains)
INFO - root - 2017-12-06 02:51:07.746624: step 4720, loss = 2.03, batch loss = 1.98 (16.4 examples/sec; 0.488 sec/batch; 44h:25m:45s remains)
INFO - root - 2017-12-06 02:51:12.406383: step 4730, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.471 sec/batch; 42h:50m:57s remains)
INFO - root - 2017-12-06 02:51:17.121011: step 4740, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.460 sec/batch; 41h:53m:34s remains)
INFO - root - 2017-12-06 02:51:21.784410: step 4750, loss = 2.04, batch loss = 1.98 (16.5 examples/sec; 0.484 sec/batch; 44h:02m:00s remains)
INFO - root - 2017-12-06 02:51:26.387071: step 4760, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.472 sec/batch; 43h:00m:29s remains)
INFO - root - 2017-12-06 02:51:31.051265: step 4770, loss = 2.05, batch loss = 1.99 (17.4 examples/sec; 0.459 sec/batch; 41h:44m:52s remains)
INFO - root - 2017-12-06 02:51:35.784223: step 4780, loss = 2.09, batch loss = 2.03 (16.7 examples/sec; 0.478 sec/batch; 43h:29m:22s remains)
INFO - root - 2017-12-06 02:51:40.434448: step 4790, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.461 sec/batch; 41h:58m:57s remains)
INFO - root - 2017-12-06 02:51:45.102638: step 4800, loss = 2.04, batch loss = 1.98 (17.8 examples/sec; 0.448 sec/batch; 40h:49m:05s remains)
2017-12-06 02:51:45.565851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2556572 -4.2642674 -4.2652316 -4.2617197 -4.2609143 -4.2696114 -4.2825828 -4.29742 -4.3127136 -4.3261538 -4.3298631 -4.3215218 -4.3076811 -4.3009143 -4.3046727][-4.1998873 -4.2168875 -4.2255898 -4.2277102 -4.2283916 -4.2381191 -4.25254 -4.2712688 -4.2940416 -4.3155646 -4.3267674 -4.3252177 -4.3140535 -4.3052607 -4.306519][-4.1540432 -4.1745653 -4.1859636 -4.18965 -4.1891356 -4.1961136 -4.2076836 -4.2260327 -4.2554011 -4.2872095 -4.3076839 -4.3146343 -4.3102489 -4.304091 -4.3050694][-4.1190157 -4.1406736 -4.1489849 -4.1501827 -4.148016 -4.1501055 -4.1546364 -4.1685848 -4.2014785 -4.2419553 -4.2732072 -4.2920275 -4.2970948 -4.297081 -4.300169][-4.1046262 -4.1193008 -4.1205158 -4.1152925 -4.109262 -4.1048741 -4.0991125 -4.1068196 -4.1424451 -4.19187 -4.2321949 -4.2588148 -4.2720394 -4.2790484 -4.286377][-4.1198168 -4.1213169 -4.1088257 -4.0896721 -4.0715733 -4.0536666 -4.0297461 -4.0244164 -4.0641613 -4.1297374 -4.1840091 -4.2155051 -4.2323952 -4.2439528 -4.2550583][-4.1588035 -4.1489296 -4.1206527 -4.080863 -4.0425072 -4.0029979 -3.9525228 -3.9254911 -3.9706883 -4.0608807 -4.1336875 -4.1722293 -4.1896753 -4.2016039 -4.2134309][-4.2014637 -4.1920295 -4.1560197 -4.1017561 -4.0447264 -3.9830987 -3.90856 -3.855969 -3.8930192 -3.9985862 -4.0892644 -4.1367359 -4.15699 -4.1689682 -4.1807117][-4.2351422 -4.2334471 -4.1987567 -4.1414843 -4.0804591 -4.0100584 -3.9292448 -3.8665934 -3.8844051 -3.9778266 -4.0665293 -4.1144772 -4.135107 -4.148983 -4.1625133][-4.2573714 -4.2631826 -4.2360682 -4.188982 -4.137856 -4.0757108 -4.0050554 -3.9485724 -3.9472823 -4.0103111 -4.0803137 -4.1201386 -4.1369915 -4.1480465 -4.1590819][-4.2651424 -4.2786074 -4.2660913 -4.2373734 -4.2011757 -4.1522779 -4.0975103 -4.0546327 -4.0455332 -4.0824585 -4.1298909 -4.1615105 -4.1750779 -4.1778021 -4.1758623][-4.2639327 -4.2844958 -4.2871513 -4.2760882 -4.2518449 -4.2166657 -4.178925 -4.1531572 -4.147768 -4.1677537 -4.1941133 -4.2169275 -4.228528 -4.2264409 -4.2119608][-4.2567935 -4.2789359 -4.2889748 -4.2857475 -4.2702084 -4.2508354 -4.2317109 -4.2253108 -4.23064 -4.2422552 -4.25246 -4.2639208 -4.2716246 -4.2698383 -4.25419][-4.2403245 -4.2574463 -4.2683015 -4.2709179 -4.2645116 -4.2586675 -4.2552581 -4.26406 -4.27729 -4.2834878 -4.2834563 -4.287272 -4.2911115 -4.2925205 -4.2837229][-4.2205215 -4.2310462 -4.2396307 -4.2472773 -4.2499146 -4.2516308 -4.2568321 -4.2716026 -4.2877531 -4.2923117 -4.2870836 -4.2865539 -4.29125 -4.2986894 -4.2989697]]...]
INFO - root - 2017-12-06 02:51:50.297344: step 4810, loss = 2.04, batch loss = 1.98 (16.2 examples/sec; 0.494 sec/batch; 44h:57m:21s remains)
INFO - root - 2017-12-06 02:51:54.985299: step 4820, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.463 sec/batch; 42h:07m:05s remains)
INFO - root - 2017-12-06 02:51:59.630723: step 4830, loss = 2.05, batch loss = 2.00 (17.2 examples/sec; 0.466 sec/batch; 42h:24m:17s remains)
INFO - root - 2017-12-06 02:52:04.393923: step 4840, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.463 sec/batch; 42h:09m:34s remains)
INFO - root - 2017-12-06 02:52:08.894741: step 4850, loss = 2.06, batch loss = 2.01 (17.0 examples/sec; 0.470 sec/batch; 42h:46m:56s remains)
INFO - root - 2017-12-06 02:52:13.476658: step 4860, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.436 sec/batch; 39h:40m:32s remains)
INFO - root - 2017-12-06 02:52:18.151949: step 4870, loss = 2.06, batch loss = 2.01 (16.8 examples/sec; 0.475 sec/batch; 43h:12m:45s remains)
INFO - root - 2017-12-06 02:52:22.877212: step 4880, loss = 2.05, batch loss = 1.99 (17.7 examples/sec; 0.453 sec/batch; 41h:13m:38s remains)
INFO - root - 2017-12-06 02:52:27.594687: step 4890, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.454 sec/batch; 41h:20m:43s remains)
INFO - root - 2017-12-06 02:52:32.392102: step 4900, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.473 sec/batch; 43h:03m:07s remains)
2017-12-06 02:52:32.847969: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2983074 -4.2932682 -4.2855659 -4.276504 -4.2643723 -4.2524652 -4.2445116 -4.2414608 -4.2493372 -4.2654133 -4.2789364 -4.2861185 -4.2868252 -4.283052 -4.2787776][-4.284606 -4.2831659 -4.2760921 -4.2658405 -4.2539663 -4.2426257 -4.2374196 -4.2399988 -4.2541962 -4.2756081 -4.2908936 -4.2959342 -4.293601 -4.2885675 -4.2835579][-4.2543983 -4.2642837 -4.26177 -4.2528996 -4.2444668 -4.2359529 -4.23279 -4.2372174 -4.2514229 -4.2711086 -4.2833738 -4.2850585 -4.2824297 -4.2815337 -4.2801576][-4.2148352 -4.2400889 -4.2463813 -4.242835 -4.2386947 -4.2307987 -4.2252979 -4.2235651 -4.2301612 -4.2422695 -4.2495484 -4.2488666 -4.2496319 -4.258173 -4.2645516][-4.1776838 -4.2118049 -4.2226286 -4.2228942 -4.2204537 -4.2110057 -4.1991463 -4.1881866 -4.1867008 -4.1927543 -4.1975636 -4.19693 -4.2026367 -4.2198853 -4.2342172][-4.1591687 -4.1868525 -4.1928515 -4.1873341 -4.1770306 -4.1603117 -4.1421371 -4.1291547 -4.1323962 -4.1448212 -4.1538606 -4.156055 -4.1644964 -4.1827011 -4.198895][-4.1610422 -4.1669388 -4.1522403 -4.1306758 -4.1053691 -4.0772142 -4.0560617 -4.0554762 -4.0827985 -4.1150422 -4.13821 -4.1475587 -4.1563859 -4.1675806 -4.1762385][-4.1562672 -4.1327925 -4.0954404 -4.0611515 -4.0303245 -4.0028048 -3.9869151 -4.0057936 -4.058095 -4.1083817 -4.14193 -4.1579642 -4.1666036 -4.1695776 -4.1687717][-4.1522918 -4.114943 -4.0724993 -4.0379643 -4.0114546 -3.9973392 -3.9993989 -4.0320625 -4.0857921 -4.1303883 -4.1586413 -4.1743979 -4.1820693 -4.180892 -4.1733818][-4.1461191 -4.121232 -4.0988493 -4.0823078 -4.0679851 -4.065423 -4.0753937 -4.1044769 -4.1427679 -4.1684079 -4.1824145 -4.1923194 -4.2013588 -4.2056394 -4.2021394][-4.1305723 -4.1272669 -4.1332498 -4.1413593 -4.1434836 -4.1484327 -4.1576576 -4.177175 -4.1976533 -4.2053785 -4.2048321 -4.2049537 -4.2123876 -4.2225671 -4.2245374][-4.1109538 -4.1261568 -4.1524568 -4.1784821 -4.1959553 -4.2102108 -4.2216568 -4.2344146 -4.24081 -4.2342205 -4.219883 -4.2075362 -4.2048163 -4.208169 -4.205678][-4.1113181 -4.130095 -4.1597486 -4.1901608 -4.2165909 -4.2377582 -4.2528834 -4.2633848 -4.2613945 -4.2498198 -4.2324009 -4.2123904 -4.1956487 -4.1832714 -4.1674][-4.1457825 -4.1574287 -4.1785073 -4.20267 -4.22482 -4.2426343 -4.2563186 -4.26466 -4.2624774 -4.256928 -4.2470469 -4.2275009 -4.2012572 -4.1734047 -4.1434336][-4.1859751 -4.1935306 -4.2059369 -4.2207747 -4.2326612 -4.2399459 -4.245605 -4.2498827 -4.2512255 -4.254518 -4.254 -4.2390666 -4.2110529 -4.1774087 -4.1433043]]...]
INFO - root - 2017-12-06 02:52:37.671041: step 4910, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.465 sec/batch; 42h:16m:49s remains)
INFO - root - 2017-12-06 02:52:42.357948: step 4920, loss = 2.05, batch loss = 2.00 (17.1 examples/sec; 0.468 sec/batch; 42h:35m:38s remains)
INFO - root - 2017-12-06 02:52:47.080523: step 4930, loss = 2.07, batch loss = 2.02 (17.4 examples/sec; 0.460 sec/batch; 41h:53m:42s remains)
INFO - root - 2017-12-06 02:52:51.689478: step 4940, loss = 2.09, batch loss = 2.03 (17.4 examples/sec; 0.459 sec/batch; 41h:45m:22s remains)
INFO - root - 2017-12-06 02:52:56.144210: step 4950, loss = 2.05, batch loss = 1.99 (16.7 examples/sec; 0.480 sec/batch; 43h:40m:12s remains)
INFO - root - 2017-12-06 02:53:00.805440: step 4960, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.450 sec/batch; 40h:53m:54s remains)
INFO - root - 2017-12-06 02:53:05.526274: step 4970, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.477 sec/batch; 43h:21m:42s remains)
INFO - root - 2017-12-06 02:53:10.280893: step 4980, loss = 2.09, batch loss = 2.03 (16.8 examples/sec; 0.477 sec/batch; 43h:26m:07s remains)
INFO - root - 2017-12-06 02:53:14.953922: step 4990, loss = 2.05, batch loss = 1.99 (16.4 examples/sec; 0.487 sec/batch; 44h:16m:12s remains)
INFO - root - 2017-12-06 02:53:19.668119: step 5000, loss = 2.06, batch loss = 2.00 (18.0 examples/sec; 0.444 sec/batch; 40h:23m:48s remains)
2017-12-06 02:53:20.103858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1650243 -4.1905909 -4.2014766 -4.1665468 -4.1119604 -4.0860839 -4.0948687 -4.12293 -4.1563511 -4.1960826 -4.2292447 -4.249362 -4.2605844 -4.2687469 -4.266304][-4.1683655 -4.20191 -4.2147603 -4.1766005 -4.117877 -4.0891814 -4.0977345 -4.1205554 -4.1425362 -4.1720066 -4.2021284 -4.2288632 -4.2490082 -4.25875 -4.2580657][-4.1671748 -4.2071581 -4.2197046 -4.1811094 -4.1230783 -4.0920978 -4.0936055 -4.1106339 -4.1263738 -4.1537867 -4.1850538 -4.2145772 -4.2375317 -4.246407 -4.249701][-4.1427608 -4.1927829 -4.212224 -4.1835361 -4.1348071 -4.0986056 -4.0822659 -4.0874453 -4.1053047 -4.139369 -4.1713204 -4.1994424 -4.2236605 -4.2359762 -4.2464337][-4.1090317 -4.1757989 -4.20613 -4.1890469 -4.1520119 -4.1095014 -4.0675735 -4.0539169 -4.0805082 -4.123672 -4.1519084 -4.1749048 -4.2006326 -4.2218313 -4.2400608][-4.1204815 -4.192049 -4.2200727 -4.2004566 -4.160872 -4.0996 -4.0151253 -3.9724391 -4.0221863 -4.0904732 -4.12619 -4.1485624 -4.1798196 -4.2097278 -4.2349553][-4.1460519 -4.2028742 -4.2150879 -4.183939 -4.1318111 -4.0372615 -3.9000728 -3.8287325 -3.9232287 -4.036458 -4.09036 -4.1154413 -4.1501951 -4.1889591 -4.2219086][-4.1567764 -4.1962996 -4.1940193 -4.1507287 -4.0847278 -3.9691875 -3.7964935 -3.7084279 -3.8510215 -4.0025263 -4.0636811 -4.0758419 -4.1000986 -4.1463919 -4.1919332][-4.1744089 -4.199945 -4.1908364 -4.145309 -4.0813065 -3.9896121 -3.8557789 -3.7994175 -3.9372513 -4.0608058 -4.0940785 -4.0789671 -4.0726519 -4.1065226 -4.1517363][-4.2061224 -4.2191591 -4.2065535 -4.168395 -4.1241837 -4.0718894 -3.9972229 -3.9750996 -4.0645237 -4.1381021 -4.1460557 -4.1162515 -4.0983787 -4.1210012 -4.1509209][-4.2322474 -4.2374554 -4.2247224 -4.1987038 -4.1717973 -4.1428237 -4.1031876 -4.0936971 -4.1407156 -4.1791596 -4.1695213 -4.133625 -4.1153011 -4.1372933 -4.1567006][-4.2445583 -4.2453251 -4.238574 -4.2241545 -4.2086554 -4.1921196 -4.1699047 -4.1624146 -4.1838651 -4.2036095 -4.1884241 -4.1516881 -4.1310067 -4.1442523 -4.1544666][-4.2566891 -4.2568274 -4.2553549 -4.2499027 -4.2422523 -4.231863 -4.2155786 -4.2071228 -4.2148829 -4.2234468 -4.2082849 -4.1769834 -4.1558924 -4.1577158 -4.161778][-4.2779574 -4.2780094 -4.2767963 -4.270308 -4.2633538 -4.2534337 -4.2402782 -4.2310162 -4.2316561 -4.2340546 -4.2277656 -4.2122655 -4.1949787 -4.1925583 -4.1945906][-4.2935934 -4.2929244 -4.2912197 -4.2843137 -4.2742696 -4.2633839 -4.2524128 -4.2441154 -4.2417164 -4.2421379 -4.2445726 -4.2421222 -4.2321568 -4.2272778 -4.2265067]]...]
INFO - root - 2017-12-06 02:53:24.735970: step 5010, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.470 sec/batch; 42h:44m:01s remains)
INFO - root - 2017-12-06 02:53:29.367987: step 5020, loss = 2.08, batch loss = 2.02 (17.5 examples/sec; 0.457 sec/batch; 41h:34m:48s remains)
INFO - root - 2017-12-06 02:53:34.056429: step 5030, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.476 sec/batch; 43h:19m:25s remains)
INFO - root - 2017-12-06 02:53:38.564135: step 5040, loss = 2.07, batch loss = 2.02 (30.6 examples/sec; 0.262 sec/batch; 23h:49m:04s remains)
INFO - root - 2017-12-06 02:53:43.326657: step 5050, loss = 2.06, batch loss = 2.00 (16.4 examples/sec; 0.487 sec/batch; 44h:17m:55s remains)
INFO - root - 2017-12-06 02:53:48.014624: step 5060, loss = 2.04, batch loss = 1.98 (16.4 examples/sec; 0.488 sec/batch; 44h:21m:05s remains)
INFO - root - 2017-12-06 02:53:52.745742: step 5070, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.455 sec/batch; 41h:25m:44s remains)
INFO - root - 2017-12-06 02:53:57.383043: step 5080, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.463 sec/batch; 42h:06m:53s remains)
INFO - root - 2017-12-06 02:54:02.094326: step 5090, loss = 2.08, batch loss = 2.02 (16.6 examples/sec; 0.481 sec/batch; 43h:43m:15s remains)
INFO - root - 2017-12-06 02:54:06.891332: step 5100, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.454 sec/batch; 41h:19m:04s remains)
2017-12-06 02:54:07.348251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1628261 -4.1789942 -4.1769032 -4.1700058 -4.1809716 -4.1940813 -4.19378 -4.1967921 -4.1968188 -4.2016339 -4.2229419 -4.2371621 -4.2331791 -4.2119784 -4.1901493][-4.100286 -4.1261373 -4.1329947 -4.1317563 -4.1455922 -4.1651626 -4.1704803 -4.1753125 -4.1729627 -4.17702 -4.1992288 -4.21483 -4.2065172 -4.1761117 -4.1453681][-4.0779333 -4.1153054 -4.1333127 -4.1379552 -4.1435909 -4.1555228 -4.156919 -4.1549997 -4.1473455 -4.1524663 -4.1783953 -4.2007356 -4.2015376 -4.1768546 -4.1482134][-4.1349268 -4.1705 -4.187366 -4.1888528 -4.1783986 -4.1683455 -4.1560779 -4.1481366 -4.1390839 -4.1489105 -4.1783204 -4.2046885 -4.2144303 -4.1991024 -4.1799884][-4.2058678 -4.2259288 -4.2302179 -4.223177 -4.2020969 -4.1756825 -4.1535306 -4.1460538 -4.1425109 -4.157239 -4.1864748 -4.2099814 -4.2219639 -4.21461 -4.2085967][-4.2475672 -4.2495136 -4.2373853 -4.2201543 -4.1922603 -4.1591392 -4.1300545 -4.1234517 -4.127666 -4.1446052 -4.1744223 -4.202878 -4.2211885 -4.2280145 -4.2373805][-4.25409 -4.2483034 -4.2182369 -4.1788135 -4.1328278 -4.0817165 -4.0350528 -4.0295539 -4.0492449 -4.0782633 -4.1155005 -4.158154 -4.1935058 -4.2212596 -4.2442484][-4.2222505 -4.21207 -4.1623821 -4.0960574 -4.0195565 -3.9421382 -3.8784404 -3.8815789 -3.9394212 -3.9987085 -4.0556364 -4.1136413 -4.1624203 -4.2040896 -4.2347064][-4.1923194 -4.1765938 -4.1172605 -4.0407915 -3.94904 -3.8599787 -3.7900255 -3.8053372 -3.8983338 -3.9876831 -4.0591273 -4.119072 -4.164238 -4.1977167 -4.2218323][-4.1801233 -4.1681519 -4.1175437 -4.0569949 -3.9869525 -3.9244246 -3.8827555 -3.9053137 -3.9845972 -4.0584192 -4.112967 -4.1546779 -4.1786447 -4.1922159 -4.2033782][-4.1901236 -4.1870685 -4.1551819 -4.1175566 -4.0779347 -4.044209 -4.0253167 -4.0393324 -4.0854564 -4.1292162 -4.1607919 -4.1805177 -4.1797848 -4.1728849 -4.1761885][-4.2111821 -4.22045 -4.2078938 -4.1839142 -4.1618319 -4.14465 -4.139101 -4.1473494 -4.1666017 -4.1860776 -4.1991606 -4.2020373 -4.1890616 -4.1733847 -4.1726789][-4.2396183 -4.2549725 -4.2557759 -4.2418208 -4.2289257 -4.2213788 -4.220758 -4.2237048 -4.2280006 -4.2313251 -4.229229 -4.2198577 -4.2025232 -4.1862049 -4.1809721][-4.2705655 -4.2839565 -4.2895069 -4.2838836 -4.279192 -4.2775908 -4.2764292 -4.2736387 -4.2704034 -4.2672949 -4.2589664 -4.2444787 -4.2273622 -4.2113223 -4.200685][-4.2934933 -4.2988586 -4.3008919 -4.2979155 -4.2970862 -4.2976451 -4.2967415 -4.2936373 -4.289988 -4.2866473 -4.2800212 -4.270575 -4.260519 -4.24651 -4.230998]]...]
INFO - root - 2017-12-06 02:54:12.069459: step 5110, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.480 sec/batch; 43h:36m:33s remains)
INFO - root - 2017-12-06 02:54:16.648112: step 5120, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.468 sec/batch; 42h:31m:02s remains)
INFO - root - 2017-12-06 02:54:21.426437: step 5130, loss = 2.05, batch loss = 2.00 (16.9 examples/sec; 0.474 sec/batch; 43h:04m:33s remains)
INFO - root - 2017-12-06 02:54:25.936699: step 5140, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.480 sec/batch; 43h:40m:35s remains)
INFO - root - 2017-12-06 02:54:30.643019: step 5150, loss = 2.04, batch loss = 1.99 (17.5 examples/sec; 0.458 sec/batch; 41h:37m:33s remains)
INFO - root - 2017-12-06 02:54:35.269609: step 5160, loss = 2.09, batch loss = 2.03 (17.8 examples/sec; 0.451 sec/batch; 40h:58m:00s remains)
INFO - root - 2017-12-06 02:54:40.069499: step 5170, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.478 sec/batch; 43h:29m:06s remains)
INFO - root - 2017-12-06 02:54:44.731513: step 5180, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.469 sec/batch; 42h:37m:40s remains)
INFO - root - 2017-12-06 02:54:49.493212: step 5190, loss = 2.05, batch loss = 1.99 (16.6 examples/sec; 0.483 sec/batch; 43h:52m:07s remains)
INFO - root - 2017-12-06 02:54:54.190383: step 5200, loss = 2.06, batch loss = 2.01 (17.5 examples/sec; 0.458 sec/batch; 41h:36m:02s remains)
2017-12-06 02:54:54.670281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3037944 -4.2973976 -4.2911053 -4.2868304 -4.2861977 -4.2880836 -4.2910237 -4.2930565 -4.2944007 -4.29532 -4.29705 -4.2990608 -4.2992558 -4.2966166 -4.2934532][-4.3047652 -4.2995634 -4.2890658 -4.2775893 -4.2712278 -4.2729187 -4.2809219 -4.2888503 -4.2942204 -4.2966337 -4.2977943 -4.298255 -4.2976613 -4.2951159 -4.2927027][-4.2954931 -4.2951326 -4.2816734 -4.2627664 -4.24766 -4.2472544 -4.2618017 -4.2800546 -4.2952762 -4.3031526 -4.3051882 -4.3038073 -4.3006859 -4.2971725 -4.294579][-4.2610593 -4.2725091 -4.2567291 -4.2275171 -4.2016182 -4.1974382 -4.2205725 -4.2556944 -4.2888665 -4.3089871 -4.3158855 -4.3143897 -4.3091316 -4.302351 -4.2977133][-4.2018514 -4.2303252 -4.2128544 -4.1684585 -4.1256261 -4.1122851 -4.1407771 -4.1954317 -4.2546778 -4.2989559 -4.3198686 -4.3232069 -4.3184795 -4.3097157 -4.3018894][-4.143507 -4.1893978 -4.173934 -4.1135921 -4.0435886 -4.0056229 -4.0277114 -4.0990963 -4.1852975 -4.2597647 -4.3051386 -4.3215976 -4.32101 -4.3133984 -4.3054032][-4.10623 -4.1667051 -4.1593156 -4.0934081 -3.9955766 -3.9139228 -3.9079261 -3.9855733 -4.0950255 -4.1976085 -4.2701902 -4.30532 -4.3133945 -4.3095703 -4.3046393][-4.119616 -4.1759496 -4.17465 -4.1152897 -4.0156236 -3.9065106 -3.8528326 -3.906601 -4.0169015 -4.1343946 -4.2260318 -4.2793417 -4.2996097 -4.3015513 -4.2998495][-4.1836624 -4.2154269 -4.21149 -4.1627908 -4.0848641 -3.9994206 -3.941165 -3.94957 -4.0186949 -4.1163397 -4.2040472 -4.26333 -4.2919226 -4.2982907 -4.2970386][-4.255415 -4.2605252 -4.2445736 -4.2061963 -4.1588683 -4.1130624 -4.0813451 -4.0784636 -4.10802 -4.1654987 -4.2274494 -4.2752733 -4.3015065 -4.3067894 -4.3019819][-4.2952781 -4.280498 -4.2527575 -4.2243385 -4.20549 -4.1952753 -4.189775 -4.1913624 -4.2027726 -4.2296667 -4.2671237 -4.3003616 -4.3210859 -4.3230939 -4.313499][-4.2971563 -4.2707796 -4.2373328 -4.2146406 -4.2123246 -4.2224574 -4.2303743 -4.2357473 -4.2363591 -4.2445068 -4.271945 -4.3056111 -4.3308344 -4.3362293 -4.3255544][-4.2707248 -4.2405934 -4.2091079 -4.1916304 -4.1940942 -4.2031293 -4.2038803 -4.19725 -4.1860142 -4.1854534 -4.2141366 -4.2589931 -4.30264 -4.3261886 -4.3262739][-4.2409711 -4.2139111 -4.1933765 -4.1868873 -4.1915188 -4.1877623 -4.1634703 -4.1202621 -4.0764265 -4.0600247 -4.0981178 -4.1649017 -4.2349043 -4.2878342 -4.3100672][-4.2233362 -4.2037492 -4.1981096 -4.2054453 -4.2153072 -4.2041187 -4.1600285 -4.0845127 -3.9958892 -3.9374604 -3.9660389 -4.0524211 -4.1490846 -4.2306952 -4.2789292]]...]
INFO - root - 2017-12-06 02:54:59.378065: step 5210, loss = 2.05, batch loss = 2.00 (16.9 examples/sec; 0.473 sec/batch; 43h:00m:08s remains)
INFO - root - 2017-12-06 02:55:04.037600: step 5220, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.476 sec/batch; 43h:15m:19s remains)
INFO - root - 2017-12-06 02:55:08.735049: step 5230, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 0.482 sec/batch; 43h:49m:48s remains)
INFO - root - 2017-12-06 02:55:13.278808: step 5240, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.452 sec/batch; 41h:02m:45s remains)
INFO - root - 2017-12-06 02:55:18.011241: step 5250, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.461 sec/batch; 41h:55m:02s remains)
INFO - root - 2017-12-06 02:55:22.734962: step 5260, loss = 2.09, batch loss = 2.04 (17.3 examples/sec; 0.463 sec/batch; 42h:05m:46s remains)
INFO - root - 2017-12-06 02:55:27.349540: step 5270, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.457 sec/batch; 41h:31m:21s remains)
INFO - root - 2017-12-06 02:55:32.096980: step 5280, loss = 2.07, batch loss = 2.01 (16.1 examples/sec; 0.498 sec/batch; 45h:13m:41s remains)
INFO - root - 2017-12-06 02:55:36.773430: step 5290, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.457 sec/batch; 41h:30m:28s remains)
INFO - root - 2017-12-06 02:55:41.435377: step 5300, loss = 2.05, batch loss = 2.00 (16.8 examples/sec; 0.476 sec/batch; 43h:16m:35s remains)
2017-12-06 02:55:41.925249: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2469373 -4.2558608 -4.2596416 -4.2569823 -4.247366 -4.2409496 -4.2413011 -4.2456222 -4.2420692 -4.2400851 -4.2401147 -4.2462893 -4.25558 -4.2683845 -4.2798209][-4.2288003 -4.2443228 -4.2519207 -4.2481608 -4.2331696 -4.2207732 -4.2190366 -4.2200804 -4.2146478 -4.2122078 -4.2130837 -4.221683 -4.2356262 -4.2553682 -4.2695112][-4.198873 -4.2217765 -4.235805 -4.2316055 -4.2115116 -4.1931252 -4.1860619 -4.1837239 -4.1800137 -4.1809273 -4.1824155 -4.191215 -4.2069631 -4.2310305 -4.2453961][-4.1678362 -4.1989055 -4.217845 -4.21334 -4.1847982 -4.1548629 -4.13793 -4.1348066 -4.1406074 -4.1530347 -4.1596527 -4.1685367 -4.1821275 -4.2048526 -4.2180343][-4.1433687 -4.1807046 -4.2034187 -4.1989408 -4.1613541 -4.1198921 -4.0924783 -4.0905113 -4.1083126 -4.13402 -4.1482558 -4.1577592 -4.1701469 -4.1916714 -4.2043724][-4.1202631 -4.1592588 -4.1829586 -4.1758885 -4.1277857 -4.076611 -4.0429621 -4.0459795 -4.0768566 -4.1151032 -4.1397495 -4.1530542 -4.1670847 -4.1863031 -4.1971674][-4.1111927 -4.1511564 -4.1718836 -4.1564465 -4.094017 -4.0275097 -3.9855747 -3.9931662 -4.0380487 -4.0915904 -4.1272092 -4.1451 -4.1617408 -4.1795087 -4.189765][-4.1157627 -4.1575584 -4.1719766 -4.1455522 -4.0722189 -3.9927006 -3.9442272 -3.9539406 -4.0074325 -4.0712485 -4.1143479 -4.1362791 -4.1548452 -4.1739178 -4.1883779][-4.1199527 -4.1619244 -4.1719 -4.1417732 -4.069767 -3.9903193 -3.9446793 -3.9566984 -4.01067 -4.0710788 -4.1096377 -4.1287193 -4.1465235 -4.1674814 -4.1878748][-4.1171036 -4.1568985 -4.1680727 -4.14269 -4.0826 -4.015626 -3.9792712 -3.9897742 -4.0324125 -4.0771465 -4.1020637 -4.1137848 -4.1304665 -4.1539965 -4.1802893][-4.1161757 -4.1489058 -4.1596112 -4.1410937 -4.0957284 -4.0445623 -4.0170789 -4.0254827 -4.0574675 -4.0877547 -4.1005116 -4.1068239 -4.1245317 -4.152113 -4.1827512][-4.1160617 -4.1414919 -4.1515617 -4.1414256 -4.1113186 -4.0764856 -4.0581255 -4.0657825 -4.0904784 -4.1099854 -4.1154065 -4.1194124 -4.1385212 -4.168231 -4.1973853][-4.118711 -4.13879 -4.14719 -4.1408553 -4.1199489 -4.0967131 -4.0870886 -4.0987024 -4.1205025 -4.135736 -4.1385679 -4.1405988 -4.1572738 -4.1853642 -4.2097669][-4.1270418 -4.1390886 -4.1427484 -4.1380544 -4.1239128 -4.1098843 -4.1092362 -4.1257648 -4.1471853 -4.15955 -4.1594596 -4.1594868 -4.1720071 -4.1964478 -4.2164612][-4.1329942 -4.1366253 -4.1346931 -4.1312432 -4.1242971 -4.11977 -4.1253409 -4.1436257 -4.1633072 -4.17301 -4.171989 -4.1707768 -4.1785979 -4.1977191 -4.211195]]...]
INFO - root - 2017-12-06 02:55:46.697689: step 5310, loss = 2.09, batch loss = 2.03 (16.1 examples/sec; 0.498 sec/batch; 45h:16m:07s remains)
INFO - root - 2017-12-06 02:55:51.313313: step 5320, loss = 2.05, batch loss = 2.00 (17.6 examples/sec; 0.456 sec/batch; 41h:25m:02s remains)
INFO - root - 2017-12-06 02:55:55.750442: step 5330, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.476 sec/batch; 43h:14m:17s remains)
INFO - root - 2017-12-06 02:56:00.454379: step 5340, loss = 2.04, batch loss = 1.98 (16.7 examples/sec; 0.478 sec/batch; 43h:24m:44s remains)
INFO - root - 2017-12-06 02:56:05.107617: step 5350, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.468 sec/batch; 42h:33m:01s remains)
INFO - root - 2017-12-06 02:56:09.783572: step 5360, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.479 sec/batch; 43h:29m:18s remains)
INFO - root - 2017-12-06 02:56:14.541127: step 5370, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.486 sec/batch; 44h:07m:03s remains)
INFO - root - 2017-12-06 02:56:19.231138: step 5380, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.466 sec/batch; 42h:18m:00s remains)
INFO - root - 2017-12-06 02:56:24.059775: step 5390, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.477 sec/batch; 43h:22m:58s remains)
INFO - root - 2017-12-06 02:56:28.778654: step 5400, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.462 sec/batch; 41h:58m:54s remains)
2017-12-06 02:56:29.268268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3089738 -4.2995625 -4.2931004 -4.289156 -4.2852259 -4.2847214 -4.283987 -4.2852874 -4.2911868 -4.2897305 -4.2883492 -4.290328 -4.2932086 -4.3001089 -4.3103704][-4.2952456 -4.2905397 -4.2899361 -4.2900944 -4.2884088 -4.2878323 -4.2889466 -4.2921166 -4.2973576 -4.2926688 -4.285574 -4.2837043 -4.284307 -4.2891021 -4.2986231][-4.2877569 -4.2867827 -4.2889009 -4.2896132 -4.2881274 -4.2870636 -4.2912545 -4.3003526 -4.3093553 -4.3031893 -4.2912574 -4.2861686 -4.2827945 -4.2844076 -4.2897873][-4.2786007 -4.2761669 -4.2749844 -4.2736769 -4.2700391 -4.2644482 -4.2677379 -4.279582 -4.29252 -4.2896423 -4.2789788 -4.2733188 -4.270906 -4.2742114 -4.2796779][-4.2660017 -4.2591944 -4.2528095 -4.246479 -4.2387385 -4.2276554 -4.22703 -4.2405639 -4.2628512 -4.2729506 -4.271791 -4.2707253 -4.2709575 -4.2762475 -4.2801785][-4.2534509 -4.23567 -4.2148585 -4.1973543 -4.1807346 -4.1601696 -4.14679 -4.1587343 -4.19849 -4.2332187 -4.2526155 -4.2666807 -4.277451 -4.2882609 -4.2916951][-4.2437506 -4.2156057 -4.18122 -4.148931 -4.1178994 -4.0810752 -4.0425749 -4.0407972 -4.0998697 -4.1669803 -4.2132812 -4.2493768 -4.2734737 -4.2914109 -4.2988706][-4.2279487 -4.1931586 -4.1475735 -4.0976467 -4.0433989 -3.9782095 -3.9054384 -3.8759675 -3.9396851 -4.0348182 -4.1148186 -4.1797752 -4.224885 -4.2601771 -4.2847252][-4.2148705 -4.182013 -4.1362262 -4.0794153 -4.0101476 -3.9256258 -3.8299749 -3.7738013 -3.8236108 -3.9207795 -4.0151873 -4.0997367 -4.1618476 -4.2118649 -4.2509308][-4.223928 -4.2037597 -4.1773267 -4.1394663 -4.088798 -4.026834 -3.9543877 -3.899442 -3.9140255 -3.9664116 -4.0226054 -4.0829711 -4.1336975 -4.1801324 -4.2192764][-4.2353244 -4.2278218 -4.2189884 -4.2018633 -4.1742058 -4.1408949 -4.1006579 -4.0625372 -4.0542641 -4.0591054 -4.0691776 -4.0956082 -4.1231875 -4.1585212 -4.19337][-4.2344494 -4.22896 -4.2255569 -4.2181306 -4.2050152 -4.19498 -4.1817145 -4.1638966 -4.153235 -4.1348448 -4.1190014 -4.1214886 -4.1299119 -4.1536512 -4.1818185][-4.2345448 -4.2244625 -4.2201576 -4.2168822 -4.2131023 -4.2197747 -4.2255678 -4.2245889 -4.2205348 -4.1977668 -4.1718869 -4.16158 -4.1596265 -4.1744742 -4.1961455][-4.2477093 -4.2334089 -4.224896 -4.2201824 -4.21924 -4.232265 -4.2475419 -4.2575946 -4.2621365 -4.245245 -4.2224259 -4.2114482 -4.2094378 -4.2182827 -4.2312322][-4.2683053 -4.2528048 -4.2434893 -4.240562 -4.241199 -4.25311 -4.2676349 -4.2783794 -4.2857857 -4.2783709 -4.2659888 -4.2587028 -4.2560487 -4.2594414 -4.2649856]]...]
INFO - root - 2017-12-06 02:56:33.972411: step 5410, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.469 sec/batch; 42h:34m:45s remains)
INFO - root - 2017-12-06 02:56:38.638555: step 5420, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.470 sec/batch; 42h:43m:28s remains)
INFO - root - 2017-12-06 02:56:43.096331: step 5430, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.473 sec/batch; 42h:57m:51s remains)
INFO - root - 2017-12-06 02:56:47.839845: step 5440, loss = 2.05, batch loss = 1.99 (17.2 examples/sec; 0.466 sec/batch; 42h:22m:09s remains)
INFO - root - 2017-12-06 02:56:52.478065: step 5450, loss = 2.06, batch loss = 2.01 (17.1 examples/sec; 0.468 sec/batch; 42h:31m:03s remains)
INFO - root - 2017-12-06 02:56:57.155705: step 5460, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.441 sec/batch; 40h:06m:26s remains)
INFO - root - 2017-12-06 02:57:01.912471: step 5470, loss = 2.08, batch loss = 2.02 (16.5 examples/sec; 0.485 sec/batch; 44h:02m:20s remains)
INFO - root - 2017-12-06 02:57:06.503353: step 5480, loss = 2.09, batch loss = 2.03 (17.2 examples/sec; 0.466 sec/batch; 42h:21m:15s remains)
INFO - root - 2017-12-06 02:57:11.095278: step 5490, loss = 2.08, batch loss = 2.02 (17.3 examples/sec; 0.463 sec/batch; 42h:04m:05s remains)
INFO - root - 2017-12-06 02:57:15.857869: step 5500, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.467 sec/batch; 42h:25m:02s remains)
2017-12-06 02:57:16.326496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1607871 -4.1663728 -4.16901 -4.1657743 -4.1627431 -4.1644831 -4.1775794 -4.1950293 -4.2132783 -4.2267904 -4.2125816 -4.20024 -4.194488 -4.1731968 -4.1683564][-4.1476665 -4.155529 -4.1570597 -4.1540923 -4.1525378 -4.1544027 -4.165123 -4.1810007 -4.1958251 -4.2099161 -4.1958942 -4.180306 -4.1732225 -4.155633 -4.1587605][-4.1330242 -4.1428742 -4.1457176 -4.14706 -4.1477375 -4.1516728 -4.1694918 -4.1896267 -4.2024131 -4.212152 -4.1936178 -4.170392 -4.16124 -4.1480064 -4.1589365][-4.1207719 -4.1328797 -4.1373167 -4.144424 -4.1457739 -4.1483383 -4.1694489 -4.19039 -4.1998811 -4.2073317 -4.1912842 -4.1638985 -4.1496768 -4.1409521 -4.1585326][-4.1102858 -4.1274805 -4.1324048 -4.1419144 -4.1433129 -4.1387758 -4.147429 -4.1608768 -4.1736679 -4.1838236 -4.1767235 -4.1532512 -4.1346626 -4.1250091 -4.1436987][-4.0986805 -4.1137443 -4.1105852 -4.1169963 -4.1180758 -4.1002588 -4.0857363 -4.0968504 -4.1255846 -4.1477051 -4.1519361 -4.1328516 -4.111866 -4.096416 -4.1123867][-4.068284 -4.0674391 -4.0465078 -4.046968 -4.0460563 -4.0013318 -3.9342563 -3.9428253 -4.0230441 -4.0820045 -4.1019149 -4.0906491 -4.0852103 -4.0802841 -4.0919271][-4.0225344 -4.0020308 -3.9577525 -3.9470763 -3.9426553 -3.8705246 -3.7318051 -3.7170463 -3.8606172 -3.9731476 -4.019031 -4.0189757 -4.0384946 -4.0665264 -4.0902863][-4.0158029 -3.9954998 -3.9555895 -3.9477818 -3.9548831 -3.8999207 -3.7583334 -3.7039833 -3.8342552 -3.9534082 -4.0060854 -4.0092497 -4.0384512 -4.084157 -4.1155868][-4.0444784 -4.0418491 -4.0239749 -4.035316 -4.0646739 -4.0505157 -3.9646742 -3.9145434 -3.9825182 -4.057199 -4.0864091 -4.0798349 -4.0986495 -4.1305413 -4.1530395][-4.0834255 -4.0971804 -4.0957389 -4.1109734 -4.1431465 -4.1502585 -4.1083493 -4.0742373 -4.09904 -4.1336269 -4.1392965 -4.11938 -4.1314535 -4.1599836 -4.1795917][-4.120204 -4.1367497 -4.1471829 -4.1613913 -4.1816263 -4.1943274 -4.1790829 -4.1559935 -4.1541152 -4.1618195 -4.146596 -4.1072693 -4.1070786 -4.1373148 -4.1628351][-4.1279473 -4.1505513 -4.1754513 -4.1909213 -4.2039218 -4.2134809 -4.2088361 -4.1940989 -4.1789813 -4.1669645 -4.1341319 -4.0772452 -4.0638547 -4.0964217 -4.1316576][-4.1143856 -4.1468906 -4.1759353 -4.1900659 -4.203824 -4.2144866 -4.2170157 -4.2096782 -4.1937041 -4.1744785 -4.1382771 -4.0867023 -4.0770946 -4.1140246 -4.1497984][-4.1304951 -4.1560273 -4.1788173 -4.1868324 -4.1954417 -4.2056494 -4.2094469 -4.204587 -4.1943016 -4.1842208 -4.1614661 -4.1319556 -4.1336 -4.1675043 -4.1987162]]...]
INFO - root - 2017-12-06 02:57:21.091403: step 5510, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.467 sec/batch; 42h:23m:15s remains)
INFO - root - 2017-12-06 02:57:25.684609: step 5520, loss = 2.08, batch loss = 2.02 (20.0 examples/sec; 0.399 sec/batch; 36h:15m:08s remains)
INFO - root - 2017-12-06 02:57:30.161337: step 5530, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.461 sec/batch; 41h:53m:14s remains)
INFO - root - 2017-12-06 02:57:34.853568: step 5540, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 0.470 sec/batch; 42h:43m:13s remains)
INFO - root - 2017-12-06 02:57:39.578642: step 5550, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.469 sec/batch; 42h:34m:55s remains)
INFO - root - 2017-12-06 02:57:44.310132: step 5560, loss = 2.06, batch loss = 2.01 (16.9 examples/sec; 0.472 sec/batch; 42h:53m:16s remains)
INFO - root - 2017-12-06 02:57:48.974416: step 5570, loss = 2.04, batch loss = 1.98 (17.2 examples/sec; 0.466 sec/batch; 42h:19m:25s remains)
INFO - root - 2017-12-06 02:57:53.701528: step 5580, loss = 2.08, batch loss = 2.03 (16.5 examples/sec; 0.483 sec/batch; 43h:53m:53s remains)
INFO - root - 2017-12-06 02:57:58.388645: step 5590, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.464 sec/batch; 42h:07m:42s remains)
INFO - root - 2017-12-06 02:58:03.048997: step 5600, loss = 2.07, batch loss = 2.02 (16.8 examples/sec; 0.476 sec/batch; 43h:14m:56s remains)
2017-12-06 02:58:03.509180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2554188 -4.2444458 -4.2358994 -4.2267866 -4.2218776 -4.2261572 -4.236783 -4.2492518 -4.2626653 -4.2731848 -4.2810297 -4.28384 -4.2837543 -4.28636 -4.291369][-4.2297029 -4.2124224 -4.1976919 -4.1846762 -4.1788197 -4.1815548 -4.1897774 -4.2027636 -4.221436 -4.2392745 -4.2547164 -4.2631326 -4.2653446 -4.2679687 -4.2741437][-4.2070112 -4.1844397 -4.1672773 -4.1533809 -4.1444707 -4.1393881 -4.13564 -4.1437068 -4.1685381 -4.1970925 -4.2242022 -4.2419615 -4.2490759 -4.2501211 -4.2551165][-4.1891427 -4.1617651 -4.1442809 -4.1300459 -4.1149468 -4.0956974 -4.07455 -4.0752921 -4.1038084 -4.1466684 -4.1893997 -4.2185516 -4.2310672 -4.230648 -4.2345386][-4.1771965 -4.1494093 -4.13615 -4.1232748 -4.0981073 -4.0589957 -4.0175605 -4.0030217 -4.0277066 -4.0849953 -4.1505733 -4.1954331 -4.2136436 -4.2139959 -4.2185121][-4.1693511 -4.14489 -4.1358643 -4.1234589 -4.0904417 -4.0320597 -3.97118 -3.9372394 -3.9488213 -4.0187368 -4.1140952 -4.1780052 -4.2028456 -4.2060361 -4.2119465][-4.160975 -4.1408119 -4.1363935 -4.1306424 -4.1016393 -4.0326638 -3.9547448 -3.8903131 -3.8692088 -3.9420443 -4.066853 -4.1513014 -4.18382 -4.1921773 -4.2044735][-4.1446323 -4.128026 -4.1299458 -4.1350021 -4.1193357 -4.0530787 -3.9562321 -3.8444052 -3.77894 -3.8621228 -4.0158925 -4.1180134 -4.1547532 -4.16885 -4.1905165][-4.1425338 -4.126523 -4.1335707 -4.1478729 -4.1399922 -4.0846982 -3.988543 -3.8656242 -3.7911139 -3.8749964 -4.0219908 -4.1150875 -4.1435304 -4.1559682 -4.1820464][-4.166564 -4.1473923 -4.1558943 -4.1742177 -4.173296 -4.1335955 -4.0551162 -3.9578867 -3.9016361 -3.9609623 -4.0681152 -4.1315532 -4.1463866 -4.1528044 -4.1787033][-4.2114329 -4.1862593 -4.1874542 -4.2020631 -4.2033482 -4.1791878 -4.1221852 -4.0549345 -4.0161424 -4.0457921 -4.1129813 -4.154706 -4.1636171 -4.1666269 -4.1897106][-4.2597704 -4.2345753 -4.2300315 -4.2376876 -4.2398858 -4.2266827 -4.1905503 -4.146997 -4.1188149 -4.1268053 -4.1631455 -4.1939168 -4.204628 -4.2099643 -4.2277][-4.3003683 -4.2814488 -4.2744756 -4.2741771 -4.2745495 -4.2639875 -4.2370834 -4.2063527 -4.1858516 -4.1890745 -4.2116179 -4.2372518 -4.2528338 -4.2612486 -4.2732086][-4.3222179 -4.3110437 -4.3037815 -4.2981682 -4.2974696 -4.2899714 -4.2707996 -4.2508149 -4.2383842 -4.239573 -4.2509694 -4.2701707 -4.2884212 -4.298593 -4.3061447][-4.3288989 -4.3231158 -4.318253 -4.3139729 -4.3127565 -4.3092594 -4.2975049 -4.2861242 -4.27949 -4.27665 -4.2790308 -4.2904849 -4.3075528 -4.3197021 -4.3253174]]...]
INFO - root - 2017-12-06 02:58:08.217097: step 5610, loss = 2.08, batch loss = 2.02 (16.9 examples/sec; 0.474 sec/batch; 43h:02m:35s remains)
INFO - root - 2017-12-06 02:58:12.650415: step 5620, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.479 sec/batch; 43h:28m:13s remains)
INFO - root - 2017-12-06 02:58:17.394273: step 5630, loss = 2.09, batch loss = 2.03 (16.4 examples/sec; 0.487 sec/batch; 44h:12m:35s remains)
INFO - root - 2017-12-06 02:58:22.072190: step 5640, loss = 2.05, batch loss = 1.99 (18.5 examples/sec; 0.432 sec/batch; 39h:15m:33s remains)
INFO - root - 2017-12-06 02:58:26.753780: step 5650, loss = 2.05, batch loss = 1.99 (17.7 examples/sec; 0.452 sec/batch; 41h:01m:16s remains)
INFO - root - 2017-12-06 02:58:31.524370: step 5660, loss = 2.08, batch loss = 2.02 (16.1 examples/sec; 0.497 sec/batch; 45h:06m:20s remains)
INFO - root - 2017-12-06 02:58:36.255897: step 5670, loss = 2.07, batch loss = 2.02 (16.9 examples/sec; 0.473 sec/batch; 42h:55m:44s remains)
INFO - root - 2017-12-06 02:58:41.029420: step 5680, loss = 2.06, batch loss = 2.00 (16.4 examples/sec; 0.488 sec/batch; 44h:17m:51s remains)
INFO - root - 2017-12-06 02:58:45.675251: step 5690, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.469 sec/batch; 42h:35m:36s remains)
INFO - root - 2017-12-06 02:58:50.430970: step 5700, loss = 2.05, batch loss = 1.99 (17.2 examples/sec; 0.465 sec/batch; 42h:14m:28s remains)
2017-12-06 02:58:50.896420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2568436 -4.2417984 -4.20739 -4.1773996 -4.150353 -4.1403961 -4.1253004 -4.1086268 -4.105041 -4.0982852 -4.0872145 -4.08463 -4.0831985 -4.0769219 -4.0666971][-4.2535634 -4.2324719 -4.1886444 -4.1431937 -4.1020722 -4.0842595 -4.0729861 -4.0579157 -4.052074 -4.04324 -4.0338416 -4.0346522 -4.0355291 -4.0292821 -4.0200582][-4.2528114 -4.2316232 -4.1864328 -4.1339712 -4.0894966 -4.0694985 -4.0626807 -4.0503993 -4.0427766 -4.0338535 -4.024754 -4.0238857 -4.0215082 -4.0134358 -4.0066991][-4.2538114 -4.2403297 -4.205349 -4.1611695 -4.1265116 -4.1122408 -4.112186 -4.1058855 -4.0997958 -4.0944266 -4.0867081 -4.0785031 -4.0660319 -4.052134 -4.0452509][-4.2549853 -4.2522111 -4.2332187 -4.2052741 -4.1861734 -4.1806059 -4.1871943 -4.188108 -4.188385 -4.1874719 -4.1801639 -4.165103 -4.1460395 -4.1282706 -4.1194825][-4.253437 -4.2578387 -4.2506 -4.2346568 -4.2248077 -4.2231908 -4.2355652 -4.24673 -4.2558918 -4.2595129 -4.2540703 -4.2382703 -4.2193971 -4.2023125 -4.1919165][-4.2498984 -4.2561193 -4.2529449 -4.2401481 -4.2300649 -4.2277555 -4.2444019 -4.2627559 -4.2776132 -4.283505 -4.2830133 -4.2756543 -4.2652516 -4.2533751 -4.2445412][-4.2468615 -4.2513995 -4.245472 -4.2275133 -4.2115464 -4.2074046 -4.2253342 -4.2446451 -4.2588921 -4.2650423 -4.2711611 -4.274888 -4.2751431 -4.2718282 -4.2684174][-4.2436666 -4.2453828 -4.2343497 -4.2095561 -4.1868753 -4.1802006 -4.1964035 -4.2123184 -4.2221041 -4.2282281 -4.240068 -4.2525139 -4.2616563 -4.2658405 -4.2689357][-4.24224 -4.2412047 -4.2264204 -4.1985364 -4.1727562 -4.162899 -4.1744027 -4.1849775 -4.1917691 -4.198822 -4.2139935 -4.231339 -4.2455654 -4.2541885 -4.2612271][-4.2430053 -4.24163 -4.2276592 -4.2012992 -4.1753817 -4.1620474 -4.16858 -4.1765141 -4.1827154 -4.1906161 -4.2059412 -4.2247276 -4.2419033 -4.2524896 -4.2600064][-4.2488456 -4.2489529 -4.2381511 -4.2143331 -4.1891112 -4.1738482 -4.17809 -4.1859684 -4.1932182 -4.2009163 -4.21478 -4.2320251 -4.2494283 -4.2591572 -4.26448][-4.2591929 -4.2606978 -4.2515917 -4.2301655 -4.2066283 -4.191421 -4.1961923 -4.20571 -4.2143617 -4.2216954 -4.2324128 -4.2466683 -4.2615442 -4.2681618 -4.2703624][-4.2718573 -4.2741046 -4.26606 -4.246325 -4.2245374 -4.2110362 -4.2171955 -4.2288327 -4.2386947 -4.2455878 -4.2539129 -4.2641125 -4.2752056 -4.27872 -4.2781129][-4.28167 -4.2849765 -4.2786736 -4.2608738 -4.24167 -4.2299762 -4.2370343 -4.2492318 -4.2592411 -4.2656198 -4.2727294 -4.2799377 -4.2877984 -4.2891278 -4.2863331]]...]
INFO - root - 2017-12-06 02:58:55.674249: step 5710, loss = 2.05, batch loss = 1.99 (15.9 examples/sec; 0.504 sec/batch; 45h:43m:16s remains)
INFO - root - 2017-12-06 02:59:00.261662: step 5720, loss = 2.05, batch loss = 1.99 (15.7 examples/sec; 0.510 sec/batch; 46h:16m:24s remains)
INFO - root - 2017-12-06 02:59:04.935232: step 5730, loss = 2.04, batch loss = 1.99 (17.3 examples/sec; 0.464 sec/batch; 42h:04m:32s remains)
INFO - root - 2017-12-06 02:59:09.632473: step 5740, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.479 sec/batch; 43h:27m:51s remains)
INFO - root - 2017-12-06 02:59:14.388165: step 5750, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.450 sec/batch; 40h:50m:07s remains)
INFO - root - 2017-12-06 02:59:19.049792: step 5760, loss = 2.10, batch loss = 2.04 (17.2 examples/sec; 0.466 sec/batch; 42h:17m:24s remains)
INFO - root - 2017-12-06 02:59:23.782591: step 5770, loss = 2.06, batch loss = 2.01 (17.4 examples/sec; 0.461 sec/batch; 41h:49m:21s remains)
INFO - root - 2017-12-06 02:59:28.410729: step 5780, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.475 sec/batch; 43h:09m:13s remains)
INFO - root - 2017-12-06 02:59:33.226244: step 5790, loss = 2.09, batch loss = 2.03 (16.1 examples/sec; 0.496 sec/batch; 45h:01m:36s remains)
INFO - root - 2017-12-06 02:59:37.842809: step 5800, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.468 sec/batch; 42h:27m:17s remains)
2017-12-06 02:59:38.355362: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.161417 -4.1639009 -4.1885834 -4.2162261 -4.2300434 -4.22995 -4.2288651 -4.2357359 -4.2504339 -4.2576537 -4.2631187 -4.271873 -4.2789335 -4.2710834 -4.2474][-4.158422 -4.1649489 -4.1921792 -4.2197647 -4.2351336 -4.2354302 -4.2333617 -4.2429204 -4.2608423 -4.2663665 -4.2679358 -4.2742391 -4.2808251 -4.276269 -4.2575111][-4.1707807 -4.1822395 -4.20788 -4.230566 -4.2377458 -4.231349 -4.2271495 -4.241045 -4.2655134 -4.2715573 -4.2676225 -4.2688642 -4.2785687 -4.28327 -4.2765565][-4.199409 -4.2149425 -4.2326236 -4.2409096 -4.2313504 -4.2090373 -4.1981282 -4.2189479 -4.2569962 -4.2708645 -4.2654848 -4.2626448 -4.27177 -4.2789316 -4.2761035][-4.229311 -4.242321 -4.2455139 -4.235909 -4.206121 -4.1619406 -4.1315718 -4.1524992 -4.2113423 -4.2459865 -4.2518945 -4.2523446 -4.2570825 -4.2584677 -4.2529573][-4.2381883 -4.2475686 -4.2391262 -4.2136817 -4.1600285 -4.087316 -4.0234351 -4.0357618 -4.1202755 -4.1892567 -4.219243 -4.2280016 -4.2283478 -4.2259393 -4.2216721][-4.2215657 -4.2319841 -4.2241035 -4.1926765 -4.1265388 -4.0313635 -3.9321404 -3.9269481 -4.02528 -4.1196895 -4.1713271 -4.1881418 -4.1901789 -4.190351 -4.1901879][-4.2065291 -4.222847 -4.2227955 -4.2010036 -4.1473656 -4.0658722 -3.9745688 -3.9508665 -4.0155149 -4.0917459 -4.1415653 -4.1577978 -4.1594429 -4.1612768 -4.1632972][-4.2042756 -4.2272363 -4.2366128 -4.2280521 -4.1971459 -4.1500096 -4.0943007 -4.0686922 -4.0945868 -4.1350603 -4.1639791 -4.1678658 -4.1583176 -4.1530514 -4.1539497][-4.2093487 -4.2380633 -4.2517934 -4.251431 -4.2381768 -4.2171211 -4.1896772 -4.172143 -4.1815352 -4.1998124 -4.2164969 -4.2120566 -4.1910868 -4.1735287 -4.1708083][-4.2148175 -4.24296 -4.2548442 -4.2569909 -4.2515783 -4.2437654 -4.2325931 -4.2248158 -4.2318873 -4.2436113 -4.2560892 -4.2514515 -4.2311773 -4.2130628 -4.2085958][-4.2255788 -4.2474241 -4.2534823 -4.2527986 -4.2486858 -4.24749 -4.2436852 -4.2420006 -4.249393 -4.2610073 -4.2736726 -4.2711782 -4.2581635 -4.2483158 -4.2470636][-4.2377577 -4.2538228 -4.2540722 -4.24741 -4.2387419 -4.2383256 -4.2394614 -4.2438455 -4.2540426 -4.265852 -4.2779036 -4.2775702 -4.2722597 -4.2713709 -4.2741103][-4.2403617 -4.2532449 -4.2529449 -4.2443562 -4.2325897 -4.2317829 -4.2337265 -4.2398357 -4.248889 -4.2582045 -4.2684636 -4.27171 -4.2735887 -4.27828 -4.2824736][-4.2343554 -4.2479219 -4.2507014 -4.2467208 -4.2399578 -4.243084 -4.2469888 -4.2467165 -4.2453513 -4.2470255 -4.2517176 -4.2537484 -4.2588682 -4.2664766 -4.27244]]...]
INFO - root - 2017-12-06 02:59:42.859576: step 5810, loss = 2.05, batch loss = 1.99 (17.2 examples/sec; 0.465 sec/batch; 42h:12m:50s remains)
INFO - root - 2017-12-06 02:59:47.547611: step 5820, loss = 2.08, batch loss = 2.02 (16.6 examples/sec; 0.482 sec/batch; 43h:43m:31s remains)
INFO - root - 2017-12-06 02:59:52.243648: step 5830, loss = 2.06, batch loss = 2.01 (17.0 examples/sec; 0.469 sec/batch; 42h:34m:37s remains)
INFO - root - 2017-12-06 02:59:56.985752: step 5840, loss = 2.08, batch loss = 2.02 (16.6 examples/sec; 0.481 sec/batch; 43h:39m:57s remains)
INFO - root - 2017-12-06 03:00:01.735137: step 5850, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 42h:11m:39s remains)
INFO - root - 2017-12-06 03:00:06.365568: step 5860, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.474 sec/batch; 42h:59m:33s remains)
INFO - root - 2017-12-06 03:00:11.088726: step 5870, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.475 sec/batch; 43h:06m:20s remains)
INFO - root - 2017-12-06 03:00:15.714054: step 5880, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.468 sec/batch; 42h:28m:14s remains)
INFO - root - 2017-12-06 03:00:20.437885: step 5890, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 0.482 sec/batch; 43h:41m:08s remains)
INFO - root - 2017-12-06 03:00:25.074200: step 5900, loss = 2.09, batch loss = 2.03 (17.2 examples/sec; 0.466 sec/batch; 42h:17m:12s remains)
2017-12-06 03:00:25.554544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2331004 -4.2382894 -4.2462006 -4.2581511 -4.2685084 -4.2703519 -4.2593255 -4.25172 -4.2552309 -4.259634 -4.2586651 -4.2510786 -4.2374458 -4.2247534 -4.2167807][-4.1839657 -4.1921754 -4.2031341 -4.219759 -4.2323675 -4.23655 -4.2206974 -4.2093039 -4.2177582 -4.226377 -4.2220345 -4.2069426 -4.1870413 -4.1693416 -4.1597743][-4.132093 -4.1401453 -4.1556416 -4.1833944 -4.20238 -4.202425 -4.1776228 -4.1626291 -4.1755967 -4.1892686 -4.1876254 -4.1718264 -4.1443777 -4.1193995 -4.104425][-4.091866 -4.0965767 -4.1159229 -4.153626 -4.1761885 -4.1671333 -4.1264477 -4.1036043 -4.1246629 -4.1442604 -4.150969 -4.1429477 -4.1179962 -4.0866475 -4.0604515][-4.0582113 -4.0635276 -4.0872421 -4.1293273 -4.1423893 -4.1188445 -4.0592527 -4.024816 -4.0609775 -4.0953164 -4.1134243 -4.1158948 -4.0977054 -4.0638518 -4.0260191][-4.0440559 -4.0508738 -4.0757904 -4.1135631 -4.1098971 -4.0598159 -3.9693456 -3.9256182 -3.9800901 -4.039762 -4.0765805 -4.0952382 -4.0903144 -4.0627475 -4.015574][-4.0580945 -4.0591183 -4.0743237 -4.102098 -4.0808091 -4.0011196 -3.8806906 -3.8335092 -3.9050558 -3.9886072 -4.0503206 -4.0900483 -4.101655 -4.0795183 -4.0298944][-4.1082 -4.0969453 -4.0904708 -4.0998421 -4.0662718 -3.9695795 -3.8456185 -3.8001635 -3.8712597 -3.9617534 -4.0372314 -4.0956216 -4.1214428 -4.1071296 -4.0688639][-4.1609631 -4.140027 -4.118721 -4.120893 -4.0927896 -4.0120969 -3.9150636 -3.8825815 -3.938184 -4.0051389 -4.0662813 -4.1229715 -4.1537108 -4.1443381 -4.1135464][-4.1805964 -4.1570616 -4.1353774 -4.1426473 -4.1326413 -4.0837631 -4.0237603 -4.0028796 -4.0419054 -4.0792656 -4.1150737 -4.1588573 -4.1866889 -4.1734166 -4.1390033][-4.188664 -4.1675191 -4.1542416 -4.17032 -4.1757274 -4.15545 -4.1245437 -4.1080227 -4.1252055 -4.1430569 -4.1604562 -4.1922975 -4.2143273 -4.19619 -4.1585517][-4.203064 -4.1846743 -4.1789088 -4.2012911 -4.216063 -4.2143755 -4.2035141 -4.1942024 -4.2002106 -4.2055826 -4.2090158 -4.2295718 -4.2435088 -4.2236295 -4.1862259][-4.2205772 -4.2170177 -4.219974 -4.2423015 -4.2585669 -4.2639093 -4.2582736 -4.2555933 -4.2582636 -4.2569718 -4.2546787 -4.2648158 -4.2694464 -4.2463903 -4.2134347][-4.2462373 -4.2561479 -4.2653627 -4.2830019 -4.29248 -4.2967038 -4.2930675 -4.2945275 -4.2968135 -4.2953558 -4.2970843 -4.2994132 -4.2932515 -4.2675786 -4.2397127][-4.2815084 -4.2938495 -4.3046532 -4.3199825 -4.3220448 -4.3213787 -4.319119 -4.3208852 -4.3217716 -4.3210983 -4.3254519 -4.3252125 -4.3139429 -4.29226 -4.271667]]...]
INFO - root - 2017-12-06 03:00:30.167364: step 5910, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.480 sec/batch; 43h:30m:36s remains)
INFO - root - 2017-12-06 03:00:34.805942: step 5920, loss = 2.06, batch loss = 2.00 (17.6 examples/sec; 0.456 sec/batch; 41h:19m:21s remains)
INFO - root - 2017-12-06 03:00:39.427672: step 5930, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.459 sec/batch; 41h:36m:07s remains)
INFO - root - 2017-12-06 03:00:44.133606: step 5940, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.440 sec/batch; 39h:55m:37s remains)
INFO - root - 2017-12-06 03:00:48.738531: step 5950, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.456 sec/batch; 41h:21m:49s remains)
INFO - root - 2017-12-06 03:00:53.420061: step 5960, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.472 sec/batch; 42h:47m:44s remains)
INFO - root - 2017-12-06 03:00:58.173802: step 5970, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.464 sec/batch; 42h:04m:38s remains)
INFO - root - 2017-12-06 03:01:02.830940: step 5980, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.466 sec/batch; 42h:14m:46s remains)
INFO - root - 2017-12-06 03:01:07.538846: step 5990, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.479 sec/batch; 43h:27m:53s remains)
INFO - root - 2017-12-06 03:01:12.241893: step 6000, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.474 sec/batch; 42h:56m:47s remains)
2017-12-06 03:01:12.654257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1032095 -4.1162906 -4.1483364 -4.1904278 -4.220048 -4.2331934 -4.2359324 -4.2470589 -4.2570739 -4.25517 -4.2509894 -4.2371216 -4.2066031 -4.1728339 -4.1429291][-4.0738955 -4.0839186 -4.1146245 -4.1591835 -4.1935925 -4.2147813 -4.2257061 -4.2415361 -4.2520351 -4.248518 -4.2449322 -4.2325592 -4.1998429 -4.1607313 -4.1319284][-4.0700569 -4.0792804 -4.1050091 -4.1416564 -4.1683049 -4.187953 -4.2044449 -4.2267346 -4.2404084 -4.2379618 -4.2356896 -4.2282987 -4.1969662 -4.1578007 -4.1339269][-4.0888548 -4.0972857 -4.1150408 -4.1347365 -4.1492596 -4.1627712 -4.1818128 -4.2038131 -4.2180386 -4.2209687 -4.2245412 -4.2238393 -4.1976161 -4.1620398 -4.1407204][-4.1171379 -4.12569 -4.1331367 -4.1349716 -4.1361947 -4.1396375 -4.157536 -4.1780157 -4.1911 -4.1981225 -4.2097487 -4.2185626 -4.205174 -4.1787205 -4.1576724][-4.143249 -4.14903 -4.1491475 -4.141253 -4.1336336 -4.1236091 -4.132081 -4.14795 -4.1546221 -4.1560717 -4.1675553 -4.1937633 -4.2055526 -4.1960378 -4.1812954][-4.1557384 -4.1617661 -4.1624413 -4.1556644 -4.14396 -4.1231518 -4.1193318 -4.124732 -4.1177578 -4.0980024 -4.0934887 -4.1349845 -4.1784515 -4.1992006 -4.2097631][-4.1389451 -4.1546564 -4.1646824 -4.1689181 -4.1625977 -4.1385942 -4.1253471 -4.1129551 -4.0827065 -4.0315218 -4.0079927 -4.0604472 -4.135129 -4.1917338 -4.2322893][-4.1133423 -4.139492 -4.1652222 -4.1873851 -4.1950712 -4.1766477 -4.1545491 -4.1227226 -4.0639696 -3.978637 -3.9354546 -3.9932365 -4.090332 -4.1738243 -4.2383342][-4.0876579 -4.1287918 -4.1752634 -4.2170029 -4.24059 -4.2332559 -4.2077508 -4.1572676 -4.0751915 -3.9662466 -3.9075232 -3.9634089 -4.0696049 -4.1666036 -4.2405839][-4.051939 -4.1147289 -4.1816192 -4.23615 -4.2711353 -4.2762218 -4.2519336 -4.193675 -4.1079955 -3.9988928 -3.9385977 -3.9865763 -4.0826278 -4.174221 -4.2438145][-4.0210981 -4.1016536 -4.1741171 -4.2297468 -4.2687335 -4.2817292 -4.2601213 -4.205822 -4.1326833 -4.0455103 -3.9974458 -4.0371118 -4.1155524 -4.192564 -4.2520509][-3.9995878 -4.0836563 -4.1481833 -4.1954637 -4.2373204 -4.2562661 -4.2396927 -4.1971231 -4.1417041 -4.083106 -4.0555162 -4.09158 -4.1499367 -4.2074237 -4.2517262][-3.9936357 -4.0667109 -4.113234 -4.1488485 -4.1925635 -4.2163234 -4.2056265 -4.1763215 -4.1397266 -4.1061406 -4.097095 -4.1316223 -4.1736789 -4.2155237 -4.2475753][-4.0090637 -4.0661707 -4.0952487 -4.1251068 -4.1695867 -4.1909981 -4.1809025 -4.1603379 -4.1425247 -4.1309476 -4.1354518 -4.1683965 -4.2012281 -4.232254 -4.2543392]]...]
INFO - root - 2017-12-06 03:01:17.189032: step 6010, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.478 sec/batch; 43h:22m:15s remains)
INFO - root - 2017-12-06 03:01:21.914233: step 6020, loss = 2.04, batch loss = 1.98 (16.7 examples/sec; 0.480 sec/batch; 43h:32m:57s remains)
INFO - root - 2017-12-06 03:01:26.625686: step 6030, loss = 2.05, batch loss = 1.99 (17.6 examples/sec; 0.454 sec/batch; 41h:07m:55s remains)
INFO - root - 2017-12-06 03:01:31.290023: step 6040, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.479 sec/batch; 43h:28m:45s remains)
INFO - root - 2017-12-06 03:01:36.093705: step 6050, loss = 2.07, batch loss = 2.01 (16.3 examples/sec; 0.492 sec/batch; 44h:36m:26s remains)
INFO - root - 2017-12-06 03:01:40.851015: step 6060, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.471 sec/batch; 42h:42m:39s remains)
INFO - root - 2017-12-06 03:01:45.577701: step 6070, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.473 sec/batch; 42h:53m:28s remains)
INFO - root - 2017-12-06 03:01:50.244914: step 6080, loss = 2.05, batch loss = 1.99 (17.5 examples/sec; 0.458 sec/batch; 41h:30m:51s remains)
INFO - root - 2017-12-06 03:01:54.994586: step 6090, loss = 2.04, batch loss = 1.99 (17.0 examples/sec; 0.471 sec/batch; 42h:42m:32s remains)
INFO - root - 2017-12-06 03:01:59.477018: step 6100, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 0.485 sec/batch; 43h:55m:59s remains)
2017-12-06 03:01:59.948146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2988539 -4.2945571 -4.2965465 -4.2972641 -4.2963386 -4.2936859 -4.2851772 -4.2801504 -4.2825866 -4.2891369 -4.2924967 -4.2935987 -4.2911296 -4.2848911 -4.2785511][-4.291131 -4.2818151 -4.2790976 -4.2754607 -4.2734566 -4.2695451 -4.2576694 -4.2512188 -4.2547336 -4.2640462 -4.2720518 -4.2781839 -4.2781777 -4.2695346 -4.2590094][-4.2865973 -4.2737865 -4.267674 -4.2605119 -4.2564335 -4.2491207 -4.2310443 -4.2186947 -4.2229877 -4.2386456 -4.2504244 -4.262722 -4.2709217 -4.2625136 -4.2477727][-4.2804995 -4.2654061 -4.2591367 -4.2512417 -4.2456861 -4.2319427 -4.2072043 -4.1908841 -4.1977215 -4.2181721 -4.2311492 -4.2470059 -4.2654095 -4.2584181 -4.2389631][-4.271594 -4.2545781 -4.2487817 -4.2414041 -4.2349048 -4.2160869 -4.1866632 -4.1700525 -4.1783504 -4.2034793 -4.2166572 -4.2303004 -4.2484736 -4.2411137 -4.2192612][-4.26949 -4.2511454 -4.2444859 -4.2380071 -4.2312059 -4.2102809 -4.1779127 -4.1616974 -4.1682177 -4.1964583 -4.2142968 -4.2275672 -4.2394595 -4.225944 -4.1987338][-4.2746406 -4.253993 -4.242969 -4.2332687 -4.226058 -4.2066746 -4.1778712 -4.1634922 -4.1671662 -4.1979065 -4.2257962 -4.2399664 -4.245472 -4.2250633 -4.1918612][-4.2810421 -4.2614245 -4.2512369 -4.2402639 -4.2327728 -4.212173 -4.1832685 -4.1688318 -4.172173 -4.199347 -4.2332325 -4.2513185 -4.2556319 -4.2374458 -4.2098703][-4.2871108 -4.2727647 -4.2671719 -4.258471 -4.2476559 -4.22012 -4.1871991 -4.1692066 -4.1696315 -4.1923089 -4.2277927 -4.2499661 -4.2545066 -4.2442241 -4.2287412][-4.2926607 -4.2822471 -4.2809391 -4.2756362 -4.2631149 -4.231679 -4.1903582 -4.1691885 -4.1694088 -4.1908021 -4.2236633 -4.2449713 -4.2481766 -4.2433114 -4.23821][-4.2973619 -4.2894711 -4.2881913 -4.2837849 -4.2701049 -4.2366142 -4.1922722 -4.1693797 -4.1701508 -4.1921015 -4.2212205 -4.2399197 -4.2422428 -4.2410655 -4.2425275][-4.300457 -4.2930813 -4.2909331 -4.2845464 -4.2696285 -4.2373786 -4.2006636 -4.1838927 -4.1846633 -4.2021456 -4.2261324 -4.2412596 -4.2437739 -4.24278 -4.2444005][-4.30296 -4.29633 -4.2933397 -4.2844238 -4.2698 -4.2444305 -4.2210011 -4.2131453 -4.21588 -4.2308412 -4.2482677 -4.2570882 -4.2569957 -4.25533 -4.2569919][-4.3037992 -4.2985854 -4.2963591 -4.2903028 -4.2792964 -4.2635241 -4.2521305 -4.2510529 -4.2548575 -4.26414 -4.2727451 -4.2752275 -4.2727437 -4.2701497 -4.2708898][-4.3057704 -4.3027263 -4.3029962 -4.3002763 -4.29298 -4.2834578 -4.2784476 -4.280715 -4.283709 -4.286869 -4.288516 -4.2876968 -4.2856688 -4.2845078 -4.2851019]]...]
INFO - root - 2017-12-06 03:02:04.540256: step 6110, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.451 sec/batch; 40h:55m:37s remains)
INFO - root - 2017-12-06 03:02:09.241456: step 6120, loss = 2.10, batch loss = 2.04 (16.1 examples/sec; 0.498 sec/batch; 45h:08m:52s remains)
INFO - root - 2017-12-06 03:02:13.869050: step 6130, loss = 2.05, batch loss = 2.00 (17.2 examples/sec; 0.466 sec/batch; 42h:16m:30s remains)
INFO - root - 2017-12-06 03:02:18.555683: step 6140, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 42h:06m:52s remains)
INFO - root - 2017-12-06 03:02:23.309807: step 6150, loss = 2.06, batch loss = 2.00 (16.2 examples/sec; 0.493 sec/batch; 44h:43m:17s remains)
INFO - root - 2017-12-06 03:02:27.968520: step 6160, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.458 sec/batch; 41h:31m:41s remains)
INFO - root - 2017-12-06 03:02:32.581562: step 6170, loss = 2.08, batch loss = 2.02 (16.9 examples/sec; 0.473 sec/batch; 42h:54m:10s remains)
INFO - root - 2017-12-06 03:02:37.357163: step 6180, loss = 2.07, batch loss = 2.02 (16.6 examples/sec; 0.481 sec/batch; 43h:38m:32s remains)
INFO - root - 2017-12-06 03:02:42.048645: step 6190, loss = 2.09, batch loss = 2.03 (16.4 examples/sec; 0.487 sec/batch; 44h:08m:49s remains)
INFO - root - 2017-12-06 03:02:46.557108: step 6200, loss = 2.04, batch loss = 1.98 (17.4 examples/sec; 0.460 sec/batch; 41h:39m:46s remains)
2017-12-06 03:02:47.006130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2104731 -4.1953578 -4.1926427 -4.2008786 -4.198061 -4.1861978 -4.1491594 -4.0929804 -4.0658884 -4.0903568 -4.1396379 -4.1800532 -4.2058277 -4.2366247 -4.2698488][-4.2209716 -4.212275 -4.2121248 -4.2192144 -4.2125497 -4.1952586 -4.1550822 -4.1049685 -4.0811391 -4.1029468 -4.1574206 -4.2048488 -4.2404389 -4.2744722 -4.2995296][-4.2371583 -4.2367029 -4.2375169 -4.2400827 -4.2275338 -4.1985755 -4.1491375 -4.1022496 -4.0833268 -4.1084142 -4.1676626 -4.2214065 -4.2648711 -4.2990408 -4.3125496][-4.2567749 -4.2611895 -4.2628813 -4.2565308 -4.22885 -4.1793971 -4.1131988 -4.0661535 -4.0589852 -4.0975103 -4.1667581 -4.2292409 -4.2783122 -4.3084207 -4.3106637][-4.2690411 -4.2747469 -4.27355 -4.2527151 -4.2030907 -4.1292973 -4.0456038 -4.0049829 -4.0240531 -4.0830979 -4.1638818 -4.2334104 -4.2861104 -4.3086224 -4.2959809][-4.2726765 -4.2744803 -4.2688665 -4.2338777 -4.1602359 -4.0555363 -3.9594445 -3.9457824 -4.0096645 -4.0942421 -4.1806107 -4.2486334 -4.2933192 -4.3009195 -4.2710595][-4.255734 -4.2547 -4.2438941 -4.1967964 -4.1032043 -3.9801 -3.8922677 -3.9220757 -4.02209 -4.11883 -4.2020454 -4.2595196 -4.2885923 -4.2803764 -4.2375536][-4.2226806 -4.22016 -4.2063408 -4.1519284 -4.0535207 -3.9458356 -3.9005792 -3.9650402 -4.0655732 -4.1493773 -4.2155833 -4.2544565 -4.2668757 -4.2443042 -4.1949883][-4.1885734 -4.1889029 -4.1740947 -4.1189828 -4.028542 -3.9551191 -3.9572928 -4.0328836 -4.1186643 -4.1835074 -4.2298741 -4.2497654 -4.2459641 -4.2118568 -4.1631513][-4.1670823 -4.1725154 -4.1532669 -4.097064 -4.0173645 -3.9763749 -4.0095534 -4.0869846 -4.156733 -4.2078891 -4.2401514 -4.2465124 -4.2297573 -4.1879597 -4.1402407][-4.1635375 -4.1627665 -4.13579 -4.0831947 -4.0200295 -4.008801 -4.0613666 -4.134757 -4.1907773 -4.2323031 -4.2544718 -4.2513051 -4.2248611 -4.1766262 -4.1296883][-4.1695747 -4.1598239 -4.1298723 -4.0877323 -4.044836 -4.0504913 -4.1051426 -4.1717343 -4.22098 -4.2576857 -4.2720037 -4.260756 -4.2233796 -4.1689878 -4.122437][-4.1822724 -4.1665225 -4.1339469 -4.1011109 -4.0759072 -4.0907564 -4.1415329 -4.199203 -4.23993 -4.2683167 -4.2758021 -4.2586303 -4.2138667 -4.1561604 -4.1106958][-4.1976442 -4.1807079 -4.1484919 -4.1249523 -4.113205 -4.1308894 -4.173903 -4.2183938 -4.2439685 -4.2597666 -4.261035 -4.2405415 -4.1960039 -4.1403637 -4.1008887][-4.2178121 -4.2087827 -4.1837015 -4.164712 -4.15283 -4.160171 -4.1881952 -4.2169271 -4.2281713 -4.234767 -4.2309952 -4.2102342 -4.1700964 -4.1222663 -4.0939655]]...]
INFO - root - 2017-12-06 03:02:51.768424: step 6210, loss = 2.06, batch loss = 2.01 (17.4 examples/sec; 0.460 sec/batch; 41h:44m:00s remains)
INFO - root - 2017-12-06 03:02:56.555450: step 6220, loss = 2.06, batch loss = 2.00 (16.2 examples/sec; 0.493 sec/batch; 44h:42m:47s remains)
INFO - root - 2017-12-06 03:03:01.250670: step 6230, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.466 sec/batch; 42h:13m:36s remains)
INFO - root - 2017-12-06 03:03:06.018305: step 6240, loss = 2.06, batch loss = 2.00 (16.4 examples/sec; 0.486 sec/batch; 44h:04m:58s remains)
INFO - root - 2017-12-06 03:03:10.714169: step 6250, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.467 sec/batch; 42h:20m:44s remains)
INFO - root - 2017-12-06 03:03:15.485307: step 6260, loss = 2.09, batch loss = 2.03 (16.2 examples/sec; 0.495 sec/batch; 44h:52m:04s remains)
INFO - root - 2017-12-06 03:03:20.262154: step 6270, loss = 2.05, batch loss = 1.99 (17.4 examples/sec; 0.461 sec/batch; 41h:46m:38s remains)
INFO - root - 2017-12-06 03:03:24.990782: step 6280, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.471 sec/batch; 42h:43m:31s remains)
INFO - root - 2017-12-06 03:03:29.575907: step 6290, loss = 2.07, batch loss = 2.01 (26.8 examples/sec; 0.298 sec/batch; 27h:01m:15s remains)
INFO - root - 2017-12-06 03:03:34.279013: step 6300, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.468 sec/batch; 42h:21m:50s remains)
2017-12-06 03:03:34.769491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.281745 -4.2388978 -4.175477 -4.0966725 -4.03446 -4.0178075 -4.0546045 -4.1020489 -4.1280231 -4.142169 -4.1485057 -4.150579 -4.1636009 -4.1816335 -4.1960783][-4.2763162 -4.2332425 -4.1700029 -4.0888271 -4.0151534 -3.9786561 -4.0136733 -4.0834022 -4.1298347 -4.1495414 -4.15513 -4.1585355 -4.1767211 -4.2008634 -4.2206349][-4.2759666 -4.2338219 -4.1719127 -4.091567 -4.0080776 -3.9470766 -3.9691467 -4.0574613 -4.124804 -4.1512566 -4.1543951 -4.15639 -4.1772914 -4.2066703 -4.23126][-4.2778215 -4.2372603 -4.1788559 -4.1029253 -4.0170269 -3.9379172 -3.9409878 -4.036715 -4.1183562 -4.1502028 -4.1520567 -4.1514544 -4.1712651 -4.1998253 -4.2243323][-4.2792325 -4.2406826 -4.1874757 -4.1190081 -4.0353775 -3.9447253 -3.9239483 -4.0161514 -4.1098595 -4.1503482 -4.1560769 -4.1529241 -4.1674395 -4.1877327 -4.2034488][-4.279604 -4.2427115 -4.1960068 -4.1388164 -4.0612235 -3.963732 -3.9176726 -3.992667 -4.093071 -4.1463375 -4.1631589 -4.1598654 -4.1656752 -4.1711349 -4.1722946][-4.2808213 -4.2468514 -4.2086306 -4.1655807 -4.1002917 -4.0088758 -3.9459889 -3.9939549 -4.0876102 -4.150485 -4.1779366 -4.1768155 -4.1733689 -4.1614308 -4.1448803][-4.283782 -4.2518234 -4.2213373 -4.19045 -4.1394057 -4.0649066 -4.0020576 -4.0265908 -4.1007781 -4.1664748 -4.2048259 -4.2108459 -4.2046871 -4.1830606 -4.1516261][-4.2893186 -4.2596631 -4.2353339 -4.2137752 -4.1777248 -4.1233511 -4.071075 -4.0818839 -4.1342263 -4.1917429 -4.2363663 -4.2487617 -4.2422252 -4.2187295 -4.1833906][-4.2959108 -4.2705784 -4.2512774 -4.2351685 -4.2118282 -4.1767697 -4.1351385 -4.1382704 -4.1729169 -4.2194133 -4.2672958 -4.2854376 -4.2823834 -4.2609487 -4.2258348][-4.3043971 -4.2830858 -4.2676358 -4.2552738 -4.240355 -4.2199864 -4.1851916 -4.1816678 -4.2040625 -4.243608 -4.2937212 -4.3163023 -4.3178873 -4.3024783 -4.2728767][-4.3176522 -4.2985907 -4.2837214 -4.2725453 -4.261538 -4.2497764 -4.2217846 -4.2139115 -4.23189 -4.2685986 -4.3160791 -4.3378749 -4.3430023 -4.334898 -4.3135557][-4.3286448 -4.3100553 -4.2929993 -4.2802091 -4.2686539 -4.2585263 -4.2378902 -4.2298875 -4.24588 -4.2800889 -4.3226833 -4.3423567 -4.34846 -4.3448358 -4.3323493][-4.3318553 -4.3152084 -4.2970209 -4.2817278 -4.2691979 -4.2609019 -4.2473526 -4.2425094 -4.2555823 -4.2848816 -4.3186579 -4.3346534 -4.3400364 -4.3378096 -4.3309345][-4.33119 -4.317431 -4.30134 -4.288897 -4.2787037 -4.2717319 -4.2618313 -4.25724 -4.2647395 -4.2851028 -4.3085632 -4.3208308 -4.3248234 -4.3240995 -4.3204627]]...]
INFO - root - 2017-12-06 03:03:39.465542: step 6310, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.463 sec/batch; 41h:56m:10s remains)
INFO - root - 2017-12-06 03:03:44.128746: step 6320, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.469 sec/batch; 42h:29m:39s remains)
INFO - root - 2017-12-06 03:03:48.864840: step 6330, loss = 2.04, batch loss = 1.99 (18.4 examples/sec; 0.434 sec/batch; 39h:21m:23s remains)
INFO - root - 2017-12-06 03:03:53.526389: step 6340, loss = 2.09, batch loss = 2.03 (16.6 examples/sec; 0.482 sec/batch; 43h:41m:03s remains)
INFO - root - 2017-12-06 03:03:58.116890: step 6350, loss = 2.05, batch loss = 1.99 (17.4 examples/sec; 0.460 sec/batch; 41h:37m:48s remains)
INFO - root - 2017-12-06 03:04:02.748745: step 6360, loss = 2.06, batch loss = 2.00 (16.6 examples/sec; 0.482 sec/batch; 43h:40m:21s remains)
INFO - root - 2017-12-06 03:04:07.441698: step 6370, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.476 sec/batch; 43h:06m:54s remains)
INFO - root - 2017-12-06 03:04:12.131201: step 6380, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.475 sec/batch; 43h:02m:10s remains)
INFO - root - 2017-12-06 03:04:16.739663: step 6390, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.471 sec/batch; 42h:40m:06s remains)
INFO - root - 2017-12-06 03:04:21.465556: step 6400, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.467 sec/batch; 42h:16m:24s remains)
2017-12-06 03:04:21.943766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1261344 -4.1371894 -4.1342049 -4.1468277 -4.1612988 -4.1613755 -4.1604881 -4.171876 -4.181437 -4.1727071 -4.1471806 -4.1214309 -4.0990181 -4.07628 -4.0685225][-4.1396089 -4.1462874 -4.1457529 -4.1618996 -4.1778631 -4.1794634 -4.1768589 -4.1813631 -4.1903429 -4.1831584 -4.1599951 -4.1347346 -4.1127667 -4.0912971 -4.0856419][-4.1466951 -4.1471786 -4.1473379 -4.1667008 -4.1847014 -4.1880817 -4.1868768 -4.1898317 -4.2002735 -4.1994829 -4.1816931 -4.1597018 -4.1376405 -4.117435 -4.1115012][-4.1361303 -4.1334081 -4.134263 -4.1527815 -4.1687546 -4.1720529 -4.1772089 -4.182375 -4.1993494 -4.20948 -4.1962528 -4.1753621 -4.1506705 -4.129025 -4.1156087][-4.0999627 -4.0957937 -4.0932322 -4.1064596 -4.1177244 -4.1171784 -4.1264019 -4.1418843 -4.1715889 -4.1934581 -4.1881342 -4.1684489 -4.1417923 -4.1148548 -4.0936246][-4.0498424 -4.0449862 -4.033875 -4.040597 -4.0438032 -4.0380287 -4.0545821 -4.0823169 -4.1216397 -4.148253 -4.1499443 -4.1332493 -4.1054115 -4.0757055 -4.0575433][-4.028379 -4.0246434 -4.0038404 -3.9947476 -3.9736209 -3.9446912 -3.9686327 -4.0087485 -4.0419531 -4.0663295 -4.0858512 -4.0883684 -4.0696163 -4.0490694 -4.0472965][-4.0332007 -4.0323014 -4.0047789 -3.9728634 -3.9141748 -3.8523965 -3.8760829 -3.9231954 -3.9458997 -3.9773388 -4.0191221 -4.0401797 -4.0353847 -4.0316753 -4.0487514][-4.0778613 -4.0831895 -4.0542545 -4.0062785 -3.9274013 -3.8494382 -3.8587766 -3.8958414 -3.9151976 -3.9590414 -4.0149441 -4.04381 -4.0473423 -4.0579062 -4.0845518][-4.1272497 -4.1406579 -4.1162152 -4.0728793 -4.0052338 -3.945044 -3.9382272 -3.94672 -3.9622536 -4.0088048 -4.05449 -4.0801153 -4.0911984 -4.1093578 -4.1333771][-4.1731915 -4.196075 -4.1816921 -4.1491871 -4.1021938 -4.0628252 -4.0436149 -4.0367517 -4.049859 -4.0876489 -4.1143308 -4.132688 -4.1486239 -4.1680365 -4.184206][-4.2207856 -4.2460723 -4.2378774 -4.2193961 -4.1894817 -4.1604261 -4.1387262 -4.1308432 -4.1417961 -4.164371 -4.180057 -4.1943727 -4.2084117 -4.224123 -4.2336388][-4.2596574 -4.2801332 -4.2777205 -4.2705913 -4.2561502 -4.2356582 -4.2196493 -4.216639 -4.2207193 -4.2278023 -4.2339654 -4.2445407 -4.256 -4.2668061 -4.2729387][-4.2920413 -4.3042669 -4.3037295 -4.3033032 -4.2979474 -4.2858772 -4.2774973 -4.2776418 -4.2780657 -4.2779093 -4.2786283 -4.2849584 -4.2938938 -4.3027258 -4.3069735][-4.3101425 -4.313972 -4.3126493 -4.31253 -4.3106794 -4.3066587 -4.3043237 -4.3055511 -4.3064008 -4.3069916 -4.3073292 -4.3097596 -4.3136191 -4.3174405 -4.3203506]]...]
INFO - root - 2017-12-06 03:04:26.576371: step 6410, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.461 sec/batch; 41h:43m:13s remains)
INFO - root - 2017-12-06 03:04:31.228309: step 6420, loss = 2.08, batch loss = 2.03 (17.3 examples/sec; 0.462 sec/batch; 41h:52m:13s remains)
INFO - root - 2017-12-06 03:04:36.009537: step 6430, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.470 sec/batch; 42h:34m:52s remains)
INFO - root - 2017-12-06 03:04:40.674407: step 6440, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.466 sec/batch; 42h:12m:00s remains)
INFO - root - 2017-12-06 03:04:45.397580: step 6450, loss = 2.05, batch loss = 1.99 (16.3 examples/sec; 0.491 sec/batch; 44h:29m:43s remains)
INFO - root - 2017-12-06 03:04:50.102177: step 6460, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.455 sec/batch; 41h:11m:35s remains)
INFO - root - 2017-12-06 03:04:54.750362: step 6470, loss = 2.07, batch loss = 2.02 (17.0 examples/sec; 0.471 sec/batch; 42h:39m:12s remains)
INFO - root - 2017-12-06 03:04:59.504453: step 6480, loss = 2.03, batch loss = 1.97 (17.3 examples/sec; 0.462 sec/batch; 41h:48m:02s remains)
INFO - root - 2017-12-06 03:05:03.984475: step 6490, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.462 sec/batch; 41h:51m:16s remains)
INFO - root - 2017-12-06 03:05:08.693208: step 6500, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.450 sec/batch; 40h:44m:38s remains)
2017-12-06 03:05:09.247574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2924509 -4.2761045 -4.250217 -4.221446 -4.1970716 -4.1830173 -4.1795115 -4.1936212 -4.2187314 -4.2389612 -4.2448864 -4.2417121 -4.234468 -4.22785 -4.2186666][-4.277432 -4.2563562 -4.2227678 -4.1891217 -4.1611133 -4.1441121 -4.1395411 -4.1589069 -4.1922908 -4.2206874 -4.2276578 -4.2181649 -4.2012472 -4.1894984 -4.1810822][-4.2629981 -4.23588 -4.192667 -4.1513443 -4.1180172 -4.0962448 -4.0892963 -4.11381 -4.154388 -4.1932821 -4.2034988 -4.19029 -4.1683736 -4.1513638 -4.1400042][-4.2517691 -4.2191291 -4.1682167 -4.1210918 -4.0828719 -4.0565376 -4.0508022 -4.0816531 -4.12777 -4.1709347 -4.1817083 -4.1663938 -4.1449819 -4.1258388 -4.1085129][-4.2372589 -4.1991096 -4.1446886 -4.0975146 -4.05817 -4.0307589 -4.0294919 -4.0660372 -4.1139669 -4.1579647 -4.1666675 -4.1527667 -4.1341619 -4.1127543 -4.0888162][-4.2203197 -4.1749082 -4.1172318 -4.0699267 -4.0338063 -4.0089827 -4.011003 -4.0449452 -4.0914588 -4.13043 -4.1384315 -4.1326113 -4.1199274 -4.0970984 -4.0709872][-4.2104583 -4.1565666 -4.0902171 -4.0339684 -3.9921095 -3.9653084 -3.9649308 -3.9921107 -4.0363774 -4.0752225 -4.0904913 -4.0980387 -4.0951295 -4.0748038 -4.0471277][-4.2051029 -4.1405392 -4.0624671 -3.9929118 -3.9403973 -3.9088516 -3.9025121 -3.9224265 -3.9648259 -4.0072064 -4.0326037 -4.0530763 -4.0613446 -4.0476437 -4.0194616][-4.1999059 -4.1271319 -4.0399103 -3.9604096 -3.9028857 -3.8683262 -3.8582494 -3.8763695 -3.9186289 -3.9617615 -3.9888375 -4.0144162 -4.0274558 -4.0175819 -3.9898107][-4.1931653 -4.1169839 -4.0271149 -3.9472566 -3.8930678 -3.8586833 -3.8459663 -3.8641353 -3.9073129 -3.9489543 -3.9731145 -3.9994192 -4.0148993 -4.0089908 -3.9794607][-4.1919351 -4.1179576 -4.0333481 -3.9624846 -3.9179118 -3.8870232 -3.8726907 -3.8910058 -3.9353237 -3.9755511 -3.9999826 -4.0251436 -4.04014 -4.0340538 -4.0050716][-4.1974797 -4.1318507 -4.0599494 -4.0029736 -3.9681971 -3.9425485 -3.9310908 -3.9504685 -3.9900851 -4.0245557 -4.0477118 -4.0697002 -4.0815029 -4.0756769 -4.053668][-4.2060633 -4.1508131 -4.0953484 -4.0539985 -4.0274038 -4.0096149 -4.0059891 -4.0273042 -4.0584083 -4.081192 -4.0990191 -4.1162333 -4.1244969 -4.1214762 -4.1077805][-4.2237725 -4.1821265 -4.144001 -4.1184216 -4.1025681 -4.0943723 -4.0964971 -4.1176763 -4.1398306 -4.1505604 -4.1588488 -4.1674237 -4.17108 -4.1690979 -4.1616616][-4.2515211 -4.2246656 -4.2001572 -4.1846833 -4.1756797 -4.1724129 -4.1748829 -4.1914892 -4.2063327 -4.2127523 -4.2154913 -4.2190108 -4.22118 -4.2197981 -4.2156153]]...]
INFO - root - 2017-12-06 03:05:13.867202: step 6510, loss = 2.04, batch loss = 1.98 (17.4 examples/sec; 0.461 sec/batch; 41h:42m:25s remains)
INFO - root - 2017-12-06 03:05:18.586432: step 6520, loss = 2.07, batch loss = 2.02 (17.6 examples/sec; 0.454 sec/batch; 41h:05m:25s remains)
INFO - root - 2017-12-06 03:05:23.257691: step 6530, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.462 sec/batch; 41h:48m:05s remains)
INFO - root - 2017-12-06 03:05:27.912286: step 6540, loss = 2.04, batch loss = 1.98 (18.7 examples/sec; 0.429 sec/batch; 38h:49m:57s remains)
INFO - root - 2017-12-06 03:05:32.748909: step 6550, loss = 2.09, batch loss = 2.03 (17.0 examples/sec; 0.469 sec/batch; 42h:29m:15s remains)
INFO - root - 2017-12-06 03:05:37.454087: step 6560, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.478 sec/batch; 43h:19m:03s remains)
INFO - root - 2017-12-06 03:05:42.145560: step 6570, loss = 2.02, batch loss = 1.97 (16.8 examples/sec; 0.477 sec/batch; 43h:10m:06s remains)
INFO - root - 2017-12-06 03:05:46.600181: step 6580, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.469 sec/batch; 42h:26m:20s remains)
INFO - root - 2017-12-06 03:05:51.325466: step 6590, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.485 sec/batch; 43h:52m:31s remains)
INFO - root - 2017-12-06 03:05:56.082571: step 6600, loss = 2.07, batch loss = 2.01 (16.1 examples/sec; 0.497 sec/batch; 44h:57m:06s remains)
2017-12-06 03:05:56.513754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2768884 -4.2768917 -4.2761931 -4.275682 -4.2747607 -4.2735767 -4.2720909 -4.2700772 -4.2685566 -4.2697821 -4.2750497 -4.2817492 -4.2872953 -4.2868419 -4.2814765][-4.2795725 -4.2777839 -4.2735128 -4.2694726 -4.265614 -4.26173 -4.258841 -4.2541776 -4.2481279 -4.2481952 -4.2600589 -4.2718744 -4.2800388 -4.2802305 -4.2750325][-4.2852049 -4.2800903 -4.2702627 -4.2612157 -4.2529812 -4.24643 -4.2418609 -4.2338729 -4.2233491 -4.2262182 -4.2467432 -4.2637224 -4.2748775 -4.2796597 -4.2767849][-4.2849069 -4.2745209 -4.2587504 -4.244998 -4.2337618 -4.2274079 -4.2211246 -4.2101045 -4.2032981 -4.2165184 -4.2445993 -4.26386 -4.2769542 -4.2865639 -4.2847767][-4.2708173 -4.2526197 -4.2304626 -4.2122622 -4.200707 -4.1951265 -4.1883163 -4.181479 -4.1867094 -4.2121511 -4.2422547 -4.2624636 -4.280849 -4.2979341 -4.2968545][-4.2460866 -4.2215123 -4.1943474 -4.1734767 -4.1617904 -4.1573477 -4.1547184 -4.1571469 -4.1733961 -4.2014232 -4.2303743 -4.2588754 -4.2907152 -4.3137865 -4.3105679][-4.2191997 -4.1899152 -4.1611838 -4.1408315 -4.1330261 -4.1344519 -4.1376228 -4.1449442 -4.1625729 -4.1914406 -4.2270985 -4.266118 -4.3028021 -4.3252845 -4.3196445][-4.1929407 -4.1612535 -4.1345444 -4.1198654 -4.1208372 -4.1323056 -4.1421027 -4.1488066 -4.1666179 -4.1981039 -4.2372761 -4.2795019 -4.3142629 -4.3338423 -4.3238564][-4.1722589 -4.1405416 -4.1155391 -4.1053185 -4.116684 -4.1432743 -4.1598568 -4.1677365 -4.1859446 -4.2148914 -4.2503929 -4.2899013 -4.3192644 -4.3320303 -4.3158011][-4.1601996 -4.1334205 -4.1122923 -4.1086578 -4.1292925 -4.1669803 -4.1898026 -4.1996727 -4.2130246 -4.2339358 -4.262701 -4.2938094 -4.3156071 -4.3207335 -4.3009386][-4.1672516 -4.1487093 -4.1334043 -4.134697 -4.1612096 -4.2000656 -4.2232571 -4.2317724 -4.2357578 -4.2464795 -4.2650928 -4.289351 -4.3062153 -4.3069453 -4.2869296][-4.1856241 -4.175282 -4.1678634 -4.1747646 -4.2025375 -4.2375517 -4.2586775 -4.2611389 -4.2531958 -4.2529964 -4.2653465 -4.2856331 -4.2990685 -4.2954469 -4.274159][-4.2144279 -4.2124114 -4.2113461 -4.2201223 -4.2443814 -4.2726793 -4.2885351 -4.2832375 -4.2659063 -4.258956 -4.268178 -4.2853379 -4.2955503 -4.2886353 -4.2664108][-4.2405005 -4.2452707 -4.2481656 -4.2563353 -4.2755017 -4.2959013 -4.3006754 -4.2855177 -4.2617922 -4.2534566 -4.2666535 -4.285964 -4.2960277 -4.2899895 -4.2717633][-4.251163 -4.2584457 -4.2657723 -4.27524 -4.289712 -4.3026772 -4.2988586 -4.2791171 -4.253828 -4.2484908 -4.2668486 -4.2884421 -4.2998414 -4.3000927 -4.2916102]]...]
INFO - root - 2017-12-06 03:06:01.313524: step 6610, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 42h:03m:24s remains)
INFO - root - 2017-12-06 03:06:05.963929: step 6620, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.462 sec/batch; 41h:48m:44s remains)
INFO - root - 2017-12-06 03:06:10.662636: step 6630, loss = 2.06, batch loss = 2.01 (18.3 examples/sec; 0.438 sec/batch; 39h:39m:13s remains)
INFO - root - 2017-12-06 03:06:15.414753: step 6640, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.474 sec/batch; 42h:55m:22s remains)
INFO - root - 2017-12-06 03:06:20.068212: step 6650, loss = 2.06, batch loss = 2.01 (17.4 examples/sec; 0.461 sec/batch; 41h:44m:05s remains)
INFO - root - 2017-12-06 03:06:24.833729: step 6660, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.469 sec/batch; 42h:25m:28s remains)
INFO - root - 2017-12-06 03:06:29.535448: step 6670, loss = 2.08, batch loss = 2.02 (17.5 examples/sec; 0.457 sec/batch; 41h:21m:14s remains)
INFO - root - 2017-12-06 03:06:34.001518: step 6680, loss = 2.08, batch loss = 2.02 (17.3 examples/sec; 0.462 sec/batch; 41h:46m:26s remains)
INFO - root - 2017-12-06 03:06:38.832508: step 6690, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.478 sec/batch; 43h:13m:31s remains)
INFO - root - 2017-12-06 03:06:43.454239: step 6700, loss = 2.08, batch loss = 2.02 (17.2 examples/sec; 0.464 sec/batch; 41h:59m:03s remains)
2017-12-06 03:06:43.906402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2317529 -4.2352462 -4.2231731 -4.2045336 -4.1713247 -4.1167021 -4.0609593 -4.0287862 -4.044621 -4.0922732 -4.1394873 -4.1629558 -4.1683888 -4.174252 -4.1792488][-4.2199311 -4.2189531 -4.21054 -4.1993847 -4.1755204 -4.1278439 -4.0705914 -4.0315952 -4.0338798 -4.0597677 -4.0908937 -4.111784 -4.1271329 -4.1459551 -4.1628666][-4.2080274 -4.211513 -4.207037 -4.1997547 -4.1832876 -4.14205 -4.0909848 -4.0550866 -4.0499496 -4.0540638 -4.0633054 -4.0722384 -4.0922036 -4.1218505 -4.1483345][-4.1914186 -4.2092409 -4.2165103 -4.2127657 -4.1985431 -4.1613774 -4.1166153 -4.0876031 -4.0816951 -4.0747457 -4.0673666 -4.0687885 -4.0882239 -4.1160512 -4.14465][-4.1626391 -4.1926007 -4.2095475 -4.2088718 -4.1943316 -4.1565051 -4.1121774 -4.0874419 -4.0921474 -4.0982056 -4.0952206 -4.1017704 -4.1165581 -4.1351042 -4.1559129][-4.1312666 -4.1659789 -4.1904583 -4.1907997 -4.1683965 -4.1208348 -4.0674715 -4.0357671 -4.0510697 -4.0852532 -4.11053 -4.1370196 -4.1531997 -4.1631203 -4.1781688][-4.1101217 -4.144413 -4.1730537 -4.1737089 -4.1402617 -4.0799351 -4.018436 -3.9803407 -4.0020642 -4.0559344 -4.1057854 -4.1515751 -4.1768966 -4.1871314 -4.2013979][-4.1062355 -4.1321435 -4.1539211 -4.1479273 -4.1122074 -4.0627208 -4.0153775 -3.9835954 -3.9985027 -4.0420122 -4.0916061 -4.1415048 -4.1716509 -4.1859879 -4.2012262][-4.1028304 -4.1157875 -4.123971 -4.1118555 -4.0862112 -4.0618806 -4.0404315 -4.0219164 -4.0335574 -4.0597143 -4.0944672 -4.1301408 -4.153327 -4.1674218 -4.1819158][-4.0981455 -4.1087041 -4.1102662 -4.0965338 -4.0785818 -4.0651016 -4.0606055 -4.0583806 -4.0758862 -4.0965347 -4.1215887 -4.14648 -4.1591616 -4.1645122 -4.1739483][-4.0939136 -4.1148558 -4.1198549 -4.1070127 -4.0849543 -4.0708351 -4.0732436 -4.0833387 -4.1122031 -4.1385918 -4.1637812 -4.1845217 -4.1915851 -4.1887326 -4.1913686][-4.1025062 -4.1236858 -4.1301966 -4.1228294 -4.1081734 -4.09779 -4.1008296 -4.1139488 -4.1463304 -4.17841 -4.2068162 -4.2266774 -4.2305307 -4.2230191 -4.2158418][-4.1370316 -4.1507893 -4.1545825 -4.1509647 -4.1452918 -4.1414804 -4.1436663 -4.1556063 -4.1827474 -4.2119622 -4.2388783 -4.2577267 -4.2638512 -4.2584305 -4.2467179][-4.1971612 -4.2027473 -4.2067022 -4.2085042 -4.2100496 -4.2088714 -4.2096753 -4.2163005 -4.2338662 -4.2546558 -4.2746615 -4.2887793 -4.2959423 -4.2936192 -4.2833767][-4.2490187 -4.2522635 -4.2575736 -4.2614231 -4.263144 -4.2616086 -4.2604365 -4.2628732 -4.2717934 -4.286767 -4.3021388 -4.3125124 -4.3181062 -4.3166924 -4.3099174]]...]
INFO - root - 2017-12-06 03:06:48.603786: step 6710, loss = 2.04, batch loss = 1.98 (17.5 examples/sec; 0.457 sec/batch; 41h:21m:36s remains)
INFO - root - 2017-12-06 03:06:53.265952: step 6720, loss = 2.04, batch loss = 1.98 (17.4 examples/sec; 0.459 sec/batch; 41h:34m:42s remains)
INFO - root - 2017-12-06 03:06:58.001415: step 6730, loss = 2.05, batch loss = 2.00 (17.1 examples/sec; 0.467 sec/batch; 42h:13m:31s remains)
INFO - root - 2017-12-06 03:07:02.685319: step 6740, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.472 sec/batch; 42h:44m:58s remains)
INFO - root - 2017-12-06 03:07:07.272667: step 6750, loss = 2.09, batch loss = 2.03 (17.1 examples/sec; 0.467 sec/batch; 42h:14m:01s remains)
INFO - root - 2017-12-06 03:07:11.990361: step 6760, loss = 2.07, batch loss = 2.02 (17.3 examples/sec; 0.462 sec/batch; 41h:45m:42s remains)
INFO - root - 2017-12-06 03:07:16.464723: step 6770, loss = 2.08, batch loss = 2.02 (33.8 examples/sec; 0.237 sec/batch; 21h:25m:17s remains)
INFO - root - 2017-12-06 03:07:21.267679: step 6780, loss = 2.05, batch loss = 1.99 (17.4 examples/sec; 0.459 sec/batch; 41h:32m:13s remains)
INFO - root - 2017-12-06 03:07:25.937322: step 6790, loss = 2.08, batch loss = 2.03 (17.4 examples/sec; 0.459 sec/batch; 41h:31m:35s remains)
INFO - root - 2017-12-06 03:07:30.701380: step 6800, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.473 sec/batch; 42h:50m:14s remains)
2017-12-06 03:07:31.143769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.272212 -4.2476072 -4.2288408 -4.2176027 -4.2009068 -4.1904078 -4.1972656 -4.2177482 -4.2312489 -4.2341294 -4.2419386 -4.2479415 -4.2366796 -4.226202 -4.220181][-4.2678838 -4.2347431 -4.206665 -4.1885524 -4.17163 -4.1628847 -4.176229 -4.2014022 -4.2164412 -4.214119 -4.2169681 -4.2235885 -4.2111406 -4.1947155 -4.1842909][-4.2560606 -4.2170596 -4.1830125 -4.1627254 -4.1470795 -4.1410069 -4.1582656 -4.1799459 -4.1874146 -4.1775784 -4.1738787 -4.1796856 -4.1708837 -4.1565495 -4.1504245][-4.2514958 -4.2163539 -4.1852627 -4.1657829 -4.1475773 -4.1355228 -4.1411581 -4.1521316 -4.1497192 -4.132987 -4.1258669 -4.136508 -4.1388793 -4.1318264 -4.1347804][-4.2571645 -4.2314005 -4.2048473 -4.1843424 -4.1585188 -4.1350026 -4.1270418 -4.1240926 -4.11364 -4.0926323 -4.0868783 -4.1034679 -4.1205239 -4.1266341 -4.1377254][-4.2631555 -4.2405329 -4.2152729 -4.1924391 -4.1618414 -4.13092 -4.1168647 -4.1075187 -4.0907326 -4.0702834 -4.0711756 -4.0950112 -4.1217413 -4.1376715 -4.1505661][-4.2575474 -4.2308774 -4.2018838 -4.1765122 -4.1444688 -4.1148219 -4.1018991 -4.0948882 -4.0750093 -4.0539331 -4.0630565 -4.0939908 -4.1235242 -4.1420617 -4.1540918][-4.24576 -4.2066188 -4.1638079 -4.1300106 -4.0994954 -4.0762982 -4.0696554 -4.0688276 -4.0557027 -4.0432415 -4.0602894 -4.0882173 -4.1128554 -4.1323438 -4.1470156][-4.2254529 -4.1738358 -4.11819 -4.0756011 -4.043622 -4.0270953 -4.0264654 -4.0341091 -4.0378833 -4.0386386 -4.0590272 -4.0815597 -4.101871 -4.121717 -4.1387062][-4.2152781 -4.1606827 -4.1073742 -4.0619054 -4.0340185 -4.022192 -4.0262489 -4.0419474 -4.0594759 -4.0704651 -4.0883937 -4.103066 -4.1189489 -4.1323447 -4.1499538][-4.2272744 -4.1798654 -4.1371684 -4.1000876 -4.0774012 -4.0673823 -4.0713253 -4.0896378 -4.1110439 -4.1214852 -4.1333847 -4.1424918 -4.154923 -4.1629725 -4.1776857][-4.2531204 -4.2192559 -4.1891637 -4.1600437 -4.1362963 -4.119628 -4.1177449 -4.1313334 -4.1530476 -4.1607513 -4.1684928 -4.1754751 -4.1851664 -4.1905174 -4.2006183][-4.2759161 -4.253654 -4.2316866 -4.2078395 -4.183042 -4.1643157 -4.1585574 -4.1663542 -4.1880646 -4.1949239 -4.2019267 -4.2068043 -4.2140546 -4.2163219 -4.2205777][-4.284502 -4.26539 -4.2458425 -4.2258654 -4.2083149 -4.1962123 -4.1935868 -4.2043295 -4.2237668 -4.2290192 -4.2359848 -4.2394795 -4.2441545 -4.2436805 -4.2429562][-4.2837486 -4.2633309 -4.2441154 -4.23116 -4.226727 -4.2277069 -4.2323275 -4.24413 -4.2585573 -4.2616458 -4.2634525 -4.26378 -4.2639632 -4.2608824 -4.2588167]]...]
INFO - root - 2017-12-06 03:07:35.749314: step 6810, loss = 2.09, batch loss = 2.03 (17.1 examples/sec; 0.468 sec/batch; 42h:20m:15s remains)
INFO - root - 2017-12-06 03:07:40.466549: step 6820, loss = 2.05, batch loss = 1.99 (17.5 examples/sec; 0.457 sec/batch; 41h:22m:29s remains)
INFO - root - 2017-12-06 03:07:45.193831: step 6830, loss = 2.06, batch loss = 2.00 (16.4 examples/sec; 0.489 sec/batch; 44h:12m:01s remains)
INFO - root - 2017-12-06 03:07:50.022538: step 6840, loss = 2.06, batch loss = 2.01 (16.8 examples/sec; 0.477 sec/batch; 43h:10m:48s remains)
INFO - root - 2017-12-06 03:07:54.683915: step 6850, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 0.485 sec/batch; 43h:52m:27s remains)
INFO - root - 2017-12-06 03:07:59.447722: step 6860, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.458 sec/batch; 41h:27m:26s remains)
INFO - root - 2017-12-06 03:08:04.067061: step 6870, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.480 sec/batch; 43h:25m:26s remains)
INFO - root - 2017-12-06 03:08:08.701488: step 6880, loss = 2.05, batch loss = 1.99 (16.8 examples/sec; 0.477 sec/batch; 43h:08m:11s remains)
INFO - root - 2017-12-06 03:08:13.508085: step 6890, loss = 2.05, batch loss = 1.99 (16.8 examples/sec; 0.476 sec/batch; 43h:01m:14s remains)
INFO - root - 2017-12-06 03:08:18.112279: step 6900, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.471 sec/batch; 42h:38m:00s remains)
2017-12-06 03:08:18.601896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2216258 -4.1896482 -4.1754112 -4.1736159 -4.1610408 -4.1404653 -4.123064 -4.1248269 -4.1554928 -4.1910782 -4.2121778 -4.2338161 -4.2489891 -4.2395062 -4.2039852][-4.213378 -4.1771469 -4.1606894 -4.1622095 -4.1504588 -4.1242156 -4.0972934 -4.0968652 -4.1397314 -4.1875582 -4.2121339 -4.2344942 -4.2444334 -4.2273674 -4.1841292][-4.207098 -4.1687274 -4.1523161 -4.1544313 -4.1454325 -4.1232672 -4.0937681 -4.0935678 -4.1480455 -4.2013984 -4.2233968 -4.2379694 -4.2410393 -4.2197356 -4.1691189][-4.2003336 -4.1579976 -4.1395931 -4.1420608 -4.1400628 -4.1284389 -4.0991387 -4.093678 -4.1413665 -4.1882668 -4.2108231 -4.2301683 -4.2335005 -4.2135253 -4.1667743][-4.2048507 -4.1649122 -4.1443734 -4.1422057 -4.142148 -4.1319928 -4.0922513 -4.0664525 -4.1010728 -4.1539559 -4.1890745 -4.2191749 -4.2260365 -4.2147379 -4.177494][-4.2087317 -4.17005 -4.1476283 -4.1414919 -4.1364059 -4.1132503 -4.04559 -3.9839492 -4.0083671 -4.0867352 -4.1499343 -4.1923389 -4.2079005 -4.2132263 -4.1918869][-4.2126179 -4.1770244 -4.1495614 -4.1268106 -4.1020837 -4.0506248 -3.9345345 -3.8261385 -3.8653104 -4.0007629 -4.1091385 -4.1692719 -4.2009912 -4.2234979 -4.2171011][-4.2185016 -4.1857595 -4.1544828 -4.1202407 -4.0755687 -3.9823337 -3.8009052 -3.6463003 -3.7368624 -3.9360356 -4.0795894 -4.1549749 -4.2039437 -4.2386365 -4.2389126][-4.2189379 -4.1845579 -4.152657 -4.12473 -4.0816908 -3.9816973 -3.8040533 -3.6735895 -3.7723546 -3.956213 -4.0818706 -4.1537967 -4.21052 -4.2465286 -4.2461925][-4.2170143 -4.1788645 -4.1505456 -4.13186 -4.0984092 -4.0278211 -3.9218006 -3.8638499 -3.9318495 -4.0453777 -4.1252904 -4.1703334 -4.2141347 -4.2381387 -4.2333016][-4.2232585 -4.186409 -4.164844 -4.1515245 -4.122766 -4.0736289 -4.0123687 -3.9886973 -4.0326381 -4.1007323 -4.1514854 -4.1817207 -4.2087131 -4.2169309 -4.2036819][-4.2386389 -4.20739 -4.1891227 -4.1814241 -4.1563745 -4.1182122 -4.0771 -4.0586853 -4.0801883 -4.12259 -4.1561422 -4.1794915 -4.1955218 -4.20049 -4.1891985][-4.2512922 -4.2250681 -4.2070765 -4.2006526 -4.1802969 -4.1545978 -4.1275673 -4.1120057 -4.1243587 -4.1492271 -4.1706338 -4.1841655 -4.1941895 -4.200109 -4.1972208][-4.2601061 -4.2375007 -4.220552 -4.2108788 -4.1961308 -4.1801825 -4.1599913 -4.1472821 -4.1579766 -4.1722088 -4.1878538 -4.20103 -4.2138157 -4.2233939 -4.2218318][-4.264122 -4.24444 -4.2312036 -4.2215772 -4.2099543 -4.1987195 -4.1824641 -4.1740961 -4.1845365 -4.1975985 -4.2131705 -4.2290835 -4.2465215 -4.2580695 -4.2544384]]...]
INFO - root - 2017-12-06 03:08:23.280849: step 6910, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.473 sec/batch; 42h:47m:17s remains)
INFO - root - 2017-12-06 03:08:28.085616: step 6920, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.468 sec/batch; 42h:22m:08s remains)
INFO - root - 2017-12-06 03:08:32.813366: step 6930, loss = 2.06, batch loss = 2.00 (16.4 examples/sec; 0.487 sec/batch; 44h:00m:56s remains)
INFO - root - 2017-12-06 03:08:37.533887: step 6940, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.475 sec/batch; 42h:56m:29s remains)
INFO - root - 2017-12-06 03:08:42.185323: step 6950, loss = 2.07, batch loss = 2.02 (18.1 examples/sec; 0.441 sec/batch; 39h:54m:54s remains)
INFO - root - 2017-12-06 03:08:46.928218: step 6960, loss = 2.08, batch loss = 2.02 (16.3 examples/sec; 0.491 sec/batch; 44h:23m:41s remains)
INFO - root - 2017-12-06 03:08:51.448649: step 6970, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.467 sec/batch; 42h:14m:18s remains)
INFO - root - 2017-12-06 03:08:56.097986: step 6980, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.468 sec/batch; 42h:16m:59s remains)
INFO - root - 2017-12-06 03:09:00.776722: step 6990, loss = 2.05, batch loss = 1.99 (16.8 examples/sec; 0.477 sec/batch; 43h:09m:32s remains)
INFO - root - 2017-12-06 03:09:05.487054: step 7000, loss = 2.05, batch loss = 1.99 (16.9 examples/sec; 0.473 sec/batch; 42h:43m:59s remains)
2017-12-06 03:09:05.948761: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0937328 -4.13001 -4.1298304 -4.1131229 -4.0893211 -4.0686731 -4.0579214 -4.055109 -4.04687 -4.0374537 -4.039475 -4.0505524 -4.06203 -4.07639 -4.0929818][-4.081398 -4.1201639 -4.1256108 -4.1154346 -4.0958905 -4.0791378 -4.0730338 -4.0681453 -4.0547285 -4.0469894 -4.0466061 -4.0561118 -4.0670705 -4.0782061 -4.0888824][-4.0684114 -4.0947857 -4.1022711 -4.0903106 -4.0705462 -4.0606647 -4.0609789 -4.0589366 -4.0559173 -4.0607114 -4.0599928 -4.0640111 -4.0599332 -4.0599 -4.0644331][-4.0642948 -4.0777493 -4.0864596 -4.0752683 -4.0553408 -4.0427327 -4.0383229 -4.0353966 -4.0381336 -4.058619 -4.0702448 -4.0676208 -4.042387 -4.034411 -4.0444584][-4.0734243 -4.083744 -4.0922 -4.0833154 -4.0636559 -4.0397735 -4.0202618 -4.0050187 -4.0071025 -4.0453358 -4.0730472 -4.0688438 -4.0326395 -4.0247164 -4.0413852][-4.0703235 -4.0905666 -4.1015558 -4.0919795 -4.07708 -4.0465717 -4.0070014 -3.9705739 -3.9753962 -4.0391917 -4.0846057 -4.0917172 -4.0646496 -4.0608392 -4.0753613][-4.0442438 -4.0671854 -4.0872717 -4.0855846 -4.0782666 -4.0459895 -3.9792335 -3.9117994 -3.9350762 -4.0357065 -4.1024709 -4.1230903 -4.1122169 -4.1083469 -4.1159062][-3.9946229 -4.0178628 -4.0489845 -4.0598574 -4.0567651 -4.0144668 -3.9199429 -3.8339419 -3.8900852 -4.0232897 -4.098259 -4.1297555 -4.1346569 -4.12697 -4.1282821][-3.9559908 -3.9759617 -4.016397 -4.0382452 -4.0358448 -3.9852114 -3.8884273 -3.8145556 -3.8878338 -4.0155497 -4.0815926 -4.1130981 -4.1279421 -4.1232824 -4.1243324][-3.9551253 -3.9813652 -4.028379 -4.053422 -4.0496759 -4.005703 -3.9371042 -3.8985569 -3.9534786 -4.0341239 -4.0706744 -4.0925274 -4.1073594 -4.1051517 -4.1058164][-3.9764376 -4.0153294 -4.0552192 -4.0738478 -4.0688319 -4.0427647 -3.9995914 -3.9791892 -4.0165081 -4.06174 -4.0742569 -4.087317 -4.0973878 -4.0858674 -4.0788088][-4.0069704 -4.0497208 -4.0746694 -4.0803409 -4.0766954 -4.0585651 -4.0238671 -4.0091963 -4.0399075 -4.0711622 -4.0792503 -4.0917015 -4.0952759 -4.0730186 -4.0566926][-4.0548053 -4.0877237 -4.0958195 -4.0940604 -4.0848022 -4.0584307 -4.0221372 -4.0127988 -4.0451188 -4.0778632 -4.0904326 -4.1048284 -4.101975 -4.0743904 -4.055294][-4.0769191 -4.1023097 -4.10222 -4.0981574 -4.08629 -4.0586014 -4.0311565 -4.0332751 -4.0703273 -4.1000624 -4.1140971 -4.1263528 -4.1174564 -4.08702 -4.0712013][-4.0687404 -4.0857525 -4.0808783 -4.0815597 -4.0790219 -4.0658946 -4.0523548 -4.0548291 -4.0836067 -4.1087232 -4.1222997 -4.1288052 -4.1167893 -4.0901246 -4.0793786]]...]
INFO - root - 2017-12-06 03:09:10.610038: step 7010, loss = 2.06, batch loss = 2.01 (18.5 examples/sec; 0.433 sec/batch; 39h:07m:43s remains)
INFO - root - 2017-12-06 03:09:15.364713: step 7020, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.478 sec/batch; 43h:15m:19s remains)
INFO - root - 2017-12-06 03:09:20.129439: step 7030, loss = 2.05, batch loss = 1.99 (17.5 examples/sec; 0.457 sec/batch; 41h:20m:46s remains)
INFO - root - 2017-12-06 03:09:24.817906: step 7040, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.475 sec/batch; 42h:58m:40s remains)
INFO - root - 2017-12-06 03:09:29.598538: step 7050, loss = 2.04, batch loss = 1.98 (17.1 examples/sec; 0.468 sec/batch; 42h:18m:13s remains)
INFO - root - 2017-12-06 03:09:34.134858: step 7060, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.462 sec/batch; 41h:47m:09s remains)
INFO - root - 2017-12-06 03:09:38.785066: step 7070, loss = 2.06, batch loss = 2.00 (18.3 examples/sec; 0.438 sec/batch; 39h:33m:20s remains)
INFO - root - 2017-12-06 03:09:43.514876: step 7080, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.479 sec/batch; 43h:17m:52s remains)
INFO - root - 2017-12-06 03:09:48.227133: step 7090, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.453 sec/batch; 40h:58m:54s remains)
INFO - root - 2017-12-06 03:09:52.864504: step 7100, loss = 2.10, batch loss = 2.04 (16.7 examples/sec; 0.479 sec/batch; 43h:16m:18s remains)
2017-12-06 03:09:53.325424: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.233871 -4.2255406 -4.2155747 -4.2100315 -4.1985674 -4.1797438 -4.1623297 -4.1545925 -4.1579146 -4.1727705 -4.1976476 -4.2226052 -4.2334085 -4.221581 -4.193706][-4.2354908 -4.22321 -4.2115645 -4.2013364 -4.181139 -4.1508484 -4.1213813 -4.1057186 -4.1068888 -4.1313186 -4.1728516 -4.2134714 -4.2339854 -4.2264848 -4.20126][-4.2396841 -4.2235479 -4.2105985 -4.1956329 -4.1670089 -4.1288819 -4.0923014 -4.0692277 -4.070632 -4.1045632 -4.1629863 -4.2212105 -4.2522812 -4.2500019 -4.2241163][-4.2417936 -4.2205238 -4.2025919 -4.1795154 -4.1443167 -4.1013317 -4.058404 -4.0299778 -4.0371532 -4.0778742 -4.1467266 -4.2176304 -4.2573876 -4.2616215 -4.2392974][-4.24978 -4.2268782 -4.2023191 -4.1714597 -4.1291389 -4.077929 -4.0287919 -3.9983954 -4.0121493 -4.0565991 -4.1289067 -4.2044086 -4.24955 -4.2605486 -4.2441459][-4.2547965 -4.2284074 -4.195292 -4.1555462 -4.1060944 -4.0490961 -3.9986985 -3.9768002 -3.9982502 -4.0456891 -4.1148481 -4.1853371 -4.2358346 -4.2567468 -4.2487164][-4.2433877 -4.2148671 -4.1749253 -4.1271639 -4.0712743 -4.0085955 -3.9631281 -3.9557707 -3.9841383 -4.0313 -4.09128 -4.1512136 -4.203712 -4.2377787 -4.24577][-4.227 -4.1996336 -4.1534681 -4.0966988 -4.0318794 -3.9659421 -3.9226413 -3.9239979 -3.9585805 -4.0107512 -4.06636 -4.1163931 -4.1665273 -4.2112489 -4.2368336][-4.2251163 -4.2008395 -4.1518822 -4.0908766 -4.0237203 -3.9619851 -3.9230485 -3.9246335 -3.9569497 -4.0151944 -4.073894 -4.1222806 -4.1722031 -4.2201695 -4.2481918][-4.2083621 -4.1911187 -4.14669 -4.0922575 -4.0347824 -3.98722 -3.9596503 -3.9604015 -3.9864359 -4.0408168 -4.0952168 -4.1402617 -4.1850085 -4.2214961 -4.2349782][-4.1723471 -4.1651578 -4.1312857 -4.0919642 -4.0542107 -4.0250435 -4.0107346 -4.0168996 -4.0385475 -4.0781388 -4.1198449 -4.1536856 -4.1829939 -4.1996336 -4.1912661][-4.1466308 -4.1476359 -4.1249709 -4.1019588 -4.0836883 -4.0723462 -4.0723133 -4.0855145 -4.1039319 -4.127367 -4.1528077 -4.173789 -4.1853647 -4.1798162 -4.14919][-4.1527333 -4.1616449 -4.1512685 -4.1407185 -4.1313624 -4.1278396 -4.1337509 -4.1473513 -4.1587267 -4.1706443 -4.1827331 -4.1905289 -4.1885538 -4.1681676 -4.1265535][-4.1890688 -4.2057605 -4.2046666 -4.2025466 -4.1986485 -4.1980987 -4.2029791 -4.2102189 -4.2131419 -4.214644 -4.2160292 -4.2155094 -4.2083149 -4.1826353 -4.1428537][-4.228971 -4.2424097 -4.2438979 -4.2450004 -4.2446837 -4.245316 -4.2479973 -4.2500992 -4.2490034 -4.2465277 -4.2444739 -4.2421412 -4.235785 -4.214469 -4.1860156]]...]
INFO - root - 2017-12-06 03:09:57.952537: step 7110, loss = 2.10, batch loss = 2.04 (17.5 examples/sec; 0.457 sec/batch; 41h:18m:22s remains)
INFO - root - 2017-12-06 03:10:02.737423: step 7120, loss = 2.04, batch loss = 1.98 (16.5 examples/sec; 0.486 sec/batch; 43h:55m:15s remains)
INFO - root - 2017-12-06 03:10:07.423445: step 7130, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.477 sec/batch; 43h:08m:30s remains)
INFO - root - 2017-12-06 03:10:12.176864: step 7140, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 0.487 sec/batch; 44h:01m:17s remains)
INFO - root - 2017-12-06 03:10:16.850214: step 7150, loss = 2.06, batch loss = 2.00 (17.8 examples/sec; 0.449 sec/batch; 40h:33m:31s remains)
INFO - root - 2017-12-06 03:10:21.340665: step 7160, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 0.471 sec/batch; 42h:35m:33s remains)
INFO - root - 2017-12-06 03:10:25.996333: step 7170, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.463 sec/batch; 41h:50m:55s remains)
INFO - root - 2017-12-06 03:10:30.743347: step 7180, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.452 sec/batch; 40h:52m:30s remains)
INFO - root - 2017-12-06 03:10:35.498861: step 7190, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.476 sec/batch; 42h:58m:59s remains)
INFO - root - 2017-12-06 03:10:40.215051: step 7200, loss = 2.05, batch loss = 1.99 (17.2 examples/sec; 0.466 sec/batch; 42h:07m:03s remains)
2017-12-06 03:10:40.680826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2976294 -4.2883716 -4.2854538 -4.2868233 -4.2866926 -4.2887936 -4.2933059 -4.2961097 -4.2993093 -4.2993965 -4.30144 -4.306159 -4.3111258 -4.3195472 -4.3269806][-4.2645226 -4.2461281 -4.2399025 -4.243288 -4.24243 -4.2418947 -4.2456684 -4.24824 -4.2528167 -4.2560182 -4.2626185 -4.2736373 -4.2858658 -4.3024096 -4.3166738][-4.2175078 -4.18679 -4.1745038 -4.1771116 -4.1745095 -4.1694355 -4.1746383 -4.17927 -4.1880774 -4.199296 -4.2136812 -4.2310553 -4.2492137 -4.2728615 -4.2960987][-4.1715083 -4.1294575 -4.1066847 -4.1010656 -4.0920672 -4.0799847 -4.0890632 -4.1038141 -4.1195073 -4.1388974 -4.1643829 -4.1891651 -4.2097058 -4.2379718 -4.2679386][-4.142343 -4.0926533 -4.0536437 -4.026989 -4.0049582 -3.9838929 -3.9985986 -4.0281076 -4.0553617 -4.0805578 -4.114778 -4.1498847 -4.1784577 -4.2132611 -4.2504878][-4.11997 -4.0623107 -4.0077138 -3.96071 -3.9231443 -3.8906138 -3.9126654 -3.9591212 -3.995836 -4.0245695 -4.063221 -4.1095524 -4.1543679 -4.2008924 -4.2427087][-4.1036253 -4.034812 -3.967689 -3.9114351 -3.8700106 -3.8328018 -3.8501008 -3.8975859 -3.9359076 -3.9712737 -4.0148487 -4.0657 -4.1255336 -4.1819367 -4.2280469][-4.1053929 -4.0377836 -3.9738371 -3.9237504 -3.89369 -3.864197 -3.8727925 -3.9022679 -3.9331975 -3.9682252 -4.0095668 -4.0556417 -4.11478 -4.1697445 -4.2167315][-4.1170416 -4.0588503 -4.0093055 -3.974396 -3.9557238 -3.9361525 -3.9441082 -3.9598467 -3.9817095 -4.0089946 -4.0403042 -4.0746632 -4.1261044 -4.174192 -4.2219543][-4.1349397 -4.0882664 -4.057137 -4.0406389 -4.0322762 -4.0264931 -4.0422635 -4.0495152 -4.061892 -4.0799308 -4.1020193 -4.1255679 -4.1635203 -4.1989045 -4.24169][-4.1696124 -4.1364856 -4.1197844 -4.1162891 -4.1169357 -4.125555 -4.1486645 -4.1516209 -4.15326 -4.1655269 -4.1840334 -4.202549 -4.22586 -4.2451968 -4.2731934][-4.2222662 -4.20243 -4.197226 -4.1998458 -4.2057514 -4.2201838 -4.2452092 -4.2448568 -4.2389431 -4.2437038 -4.2574711 -4.2724833 -4.285955 -4.2927723 -4.3043904][-4.2730265 -4.2641749 -4.2642074 -4.2677841 -4.2739038 -4.2866168 -4.3077483 -4.3046927 -4.2947869 -4.2956967 -4.307725 -4.3195572 -4.3251829 -4.3227229 -4.322372][-4.30664 -4.3018303 -4.303741 -4.3060451 -4.31086 -4.3206487 -4.334188 -4.3309646 -4.32186 -4.3223333 -4.3316517 -4.3384542 -4.3384829 -4.3305426 -4.3249812][-4.3212276 -4.3168855 -4.31798 -4.3191748 -4.3220391 -4.3278651 -4.3340836 -4.3317356 -4.3266954 -4.3273993 -4.3335352 -4.3374166 -4.3358426 -4.3285995 -4.323976]]...]
INFO - root - 2017-12-06 03:10:45.446551: step 7210, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 0.484 sec/batch; 43h:41m:35s remains)
INFO - root - 2017-12-06 03:10:50.136727: step 7220, loss = 2.08, batch loss = 2.02 (16.2 examples/sec; 0.492 sec/batch; 44h:30m:00s remains)
INFO - root - 2017-12-06 03:10:54.892229: step 7230, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.477 sec/batch; 43h:06m:51s remains)
INFO - root - 2017-12-06 03:10:59.631564: step 7240, loss = 2.04, batch loss = 1.98 (16.8 examples/sec; 0.477 sec/batch; 43h:07m:15s remains)
INFO - root - 2017-12-06 03:11:04.102484: step 7250, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.460 sec/batch; 41h:33m:35s remains)
INFO - root - 2017-12-06 03:11:08.839486: step 7260, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.480 sec/batch; 43h:22m:15s remains)
INFO - root - 2017-12-06 03:11:13.470100: step 7270, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 41h:59m:21s remains)
INFO - root - 2017-12-06 03:11:18.235294: step 7280, loss = 2.08, batch loss = 2.02 (16.1 examples/sec; 0.496 sec/batch; 44h:46m:52s remains)
INFO - root - 2017-12-06 03:11:22.994775: step 7290, loss = 2.05, batch loss = 1.99 (15.8 examples/sec; 0.506 sec/batch; 45h:41m:27s remains)
INFO - root - 2017-12-06 03:11:27.810471: step 7300, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.486 sec/batch; 43h:55m:10s remains)
2017-12-06 03:11:28.252073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2580976 -4.262403 -4.2727208 -4.2826633 -4.2900991 -4.2908654 -4.2892218 -4.2864976 -4.2853789 -4.2821474 -4.277072 -4.2764063 -4.2816238 -4.2932105 -4.3084159][-4.2089076 -4.2097692 -4.2238069 -4.2405796 -4.252892 -4.254715 -4.2517543 -4.2500386 -4.24815 -4.2411942 -4.2339973 -4.2361832 -4.2450981 -4.2574053 -4.2772765][-4.1593928 -4.1548991 -4.1703434 -4.1940379 -4.212029 -4.2156353 -4.213181 -4.2131591 -4.20971 -4.1980758 -4.1895614 -4.1994362 -4.2154746 -4.2278714 -4.2480936][-4.1386251 -4.130527 -4.1442666 -4.1706181 -4.1893597 -4.1953306 -4.1942525 -4.1953382 -4.1929359 -4.1842284 -4.1808476 -4.2006154 -4.2193379 -4.2270236 -4.2419596][-4.1428213 -4.1356449 -4.1446433 -4.1636662 -4.1709232 -4.1682029 -4.1631393 -4.1695652 -4.1836739 -4.1881232 -4.1922288 -4.214684 -4.2307549 -4.2326479 -4.2437911][-4.1615329 -4.1577272 -4.1625366 -4.1664338 -4.1530123 -4.1259613 -4.1019797 -4.1143112 -4.1571546 -4.1879315 -4.2060647 -4.2258615 -4.2325115 -4.2267227 -4.2340083][-4.1836882 -4.1838889 -4.1871786 -4.1761622 -4.1392016 -4.0747533 -4.0048609 -4.0151696 -4.1004395 -4.1686244 -4.2083187 -4.2294779 -4.231843 -4.2231088 -4.22811][-4.1966825 -4.199688 -4.2040706 -4.1887293 -4.1398048 -4.0451479 -3.9321728 -3.9355497 -4.0513916 -4.1440315 -4.1939588 -4.2222638 -4.2316861 -4.2294731 -4.2344275][-4.199502 -4.2054305 -4.2127695 -4.2029586 -4.1605387 -4.0772433 -3.9888239 -3.9906902 -4.0777483 -4.1501703 -4.185029 -4.2117224 -4.2305155 -4.2367229 -4.2411666][-4.1908917 -4.2027464 -4.2182217 -4.2198119 -4.1969995 -4.147788 -4.1031208 -4.1070709 -4.1529188 -4.1878104 -4.19624 -4.2081218 -4.2212257 -4.2262168 -4.229538][-4.1764035 -4.1861124 -4.2057009 -4.2157583 -4.2117195 -4.191308 -4.1744823 -4.1832161 -4.21152 -4.2260923 -4.2235293 -4.221653 -4.2177939 -4.216002 -4.2180252][-4.16913 -4.174952 -4.1922545 -4.2040958 -4.2039981 -4.1977458 -4.1984205 -4.2129078 -4.23618 -4.2461619 -4.2477026 -4.2485609 -4.2395906 -4.2320671 -4.2281613][-4.1859312 -4.1897192 -4.1959844 -4.1996789 -4.1993575 -4.20034 -4.2091646 -4.2264838 -4.2496033 -4.2628937 -4.272543 -4.2833729 -4.2778425 -4.2659926 -4.253284][-4.2139697 -4.2137437 -4.2090559 -4.2059355 -4.2057018 -4.2109504 -4.2216816 -4.2359271 -4.2553129 -4.2689476 -4.2802382 -4.2935991 -4.2929454 -4.2810106 -4.2641187][-4.2383394 -4.2342367 -4.227406 -4.2258525 -4.2290139 -4.2335987 -4.2408028 -4.2495947 -4.2627187 -4.2720337 -4.2801137 -4.2910357 -4.2936573 -4.2840533 -4.2691426]]...]
INFO - root - 2017-12-06 03:11:33.003391: step 7310, loss = 2.09, batch loss = 2.03 (15.8 examples/sec; 0.508 sec/batch; 45h:50m:35s remains)
INFO - root - 2017-12-06 03:11:37.695610: step 7320, loss = 2.06, batch loss = 2.00 (17.6 examples/sec; 0.454 sec/batch; 41h:00m:38s remains)
INFO - root - 2017-12-06 03:11:42.392171: step 7330, loss = 2.04, batch loss = 1.98 (16.4 examples/sec; 0.487 sec/batch; 43h:58m:29s remains)
INFO - root - 2017-12-06 03:11:47.110337: step 7340, loss = 2.09, batch loss = 2.03 (17.5 examples/sec; 0.458 sec/batch; 41h:24m:25s remains)
INFO - root - 2017-12-06 03:11:51.630728: step 7350, loss = 2.05, batch loss = 2.00 (17.4 examples/sec; 0.461 sec/batch; 41h:37m:45s remains)
INFO - root - 2017-12-06 03:11:56.276632: step 7360, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.448 sec/batch; 40h:28m:01s remains)
INFO - root - 2017-12-06 03:12:01.020075: step 7370, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 0.486 sec/batch; 43h:52m:58s remains)
INFO - root - 2017-12-06 03:12:05.688928: step 7380, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.469 sec/batch; 42h:22m:47s remains)
INFO - root - 2017-12-06 03:12:10.280501: step 7390, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.459 sec/batch; 41h:27m:55s remains)
INFO - root - 2017-12-06 03:12:15.061393: step 7400, loss = 2.07, batch loss = 2.01 (16.1 examples/sec; 0.497 sec/batch; 44h:51m:49s remains)
2017-12-06 03:12:15.531049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24886 -4.2380748 -4.2148843 -4.1983352 -4.1766176 -4.1269579 -4.0670891 -4.0319138 -4.0476646 -4.0877037 -4.1295552 -4.1562924 -4.1733255 -4.1960158 -4.2206659][-4.2470913 -4.2412653 -4.2307119 -4.2211552 -4.1957703 -4.1380377 -4.0773892 -4.0463557 -4.06588 -4.1101475 -4.1558642 -4.185987 -4.2039647 -4.2209158 -4.2408886][-4.2381544 -4.2343588 -4.2290735 -4.2242684 -4.2013817 -4.1486707 -4.093142 -4.0614548 -4.076726 -4.1320634 -4.1846504 -4.213593 -4.234849 -4.2539 -4.2719846][-4.243288 -4.2341757 -4.2277594 -4.2217851 -4.1988959 -4.148262 -4.0911818 -4.0533214 -4.0659866 -4.1266766 -4.1853914 -4.2186537 -4.2475786 -4.2750306 -4.3003263][-4.2607827 -4.2427797 -4.2261834 -4.2043095 -4.1655469 -4.101285 -4.03136 -3.9933033 -4.0193725 -4.0913954 -4.1589165 -4.2042322 -4.2415929 -4.2771387 -4.308383][-4.280159 -4.256175 -4.2266636 -4.1786723 -4.1074076 -4.0078707 -3.9038906 -3.8754034 -3.9446099 -4.0467548 -4.130506 -4.1889672 -4.2337251 -4.2746811 -4.3083682][-4.2888751 -4.2655859 -4.2263603 -4.1563015 -4.0476537 -3.897181 -3.7441015 -3.729831 -3.8584783 -4.0064969 -4.1127267 -4.1819968 -4.2309217 -4.2749896 -4.3109426][-4.289588 -4.2660694 -4.2226071 -4.1461763 -4.0209794 -3.8464839 -3.6658485 -3.6488233 -3.8020215 -3.9738209 -4.0965519 -4.1763639 -4.2297406 -4.2766294 -4.3148503][-4.2904406 -4.2684 -4.2306442 -4.1697178 -4.0706248 -3.9333673 -3.7838445 -3.7406635 -3.8407102 -3.98133 -4.096487 -4.1807003 -4.2374258 -4.2826548 -4.3204622][-4.2925529 -4.274508 -4.2473364 -4.2065315 -4.1461425 -4.0614223 -3.9552791 -3.8979118 -3.9343975 -4.022788 -4.1146784 -4.1950488 -4.2517304 -4.2931066 -4.3273258][-4.2966776 -4.2815928 -4.2619228 -4.2368765 -4.2019787 -4.1541066 -4.0850353 -4.0336857 -4.0408869 -4.0942459 -4.1630874 -4.2284045 -4.275475 -4.3085065 -4.3347526][-4.3021579 -4.2880473 -4.2716618 -4.255064 -4.2341352 -4.20637 -4.1636219 -4.1281581 -4.1293116 -4.1655912 -4.2171149 -4.2655516 -4.3000503 -4.3232059 -4.3405986][-4.3059182 -4.2925286 -4.2763414 -4.2612495 -4.2449965 -4.2264724 -4.1983776 -4.1775413 -4.1804276 -4.2104468 -4.2521143 -4.2892265 -4.3150883 -4.3329134 -4.3444476][-4.3073664 -4.2981052 -4.2829318 -4.2664547 -4.2502365 -4.2363582 -4.217485 -4.2040792 -4.2078896 -4.234684 -4.2695069 -4.2996941 -4.3210454 -4.3354793 -4.3428326][-4.3025894 -4.2979617 -4.2876563 -4.2717533 -4.2561646 -4.2444839 -4.2313647 -4.2224803 -4.2271852 -4.2501664 -4.2790823 -4.3036795 -4.3206124 -4.3304381 -4.3333206]]...]
INFO - root - 2017-12-06 03:12:20.252151: step 7410, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.463 sec/batch; 41h:47m:37s remains)
INFO - root - 2017-12-06 03:12:24.925442: step 7420, loss = 2.05, batch loss = 2.00 (17.2 examples/sec; 0.464 sec/batch; 41h:53m:29s remains)
INFO - root - 2017-12-06 03:12:29.578409: step 7430, loss = 2.06, batch loss = 2.01 (16.6 examples/sec; 0.481 sec/batch; 43h:26m:16s remains)
INFO - root - 2017-12-06 03:12:34.141629: step 7440, loss = 2.06, batch loss = 2.00 (26.5 examples/sec; 0.302 sec/batch; 27h:15m:18s remains)
INFO - root - 2017-12-06 03:12:38.813625: step 7450, loss = 2.09, batch loss = 2.03 (18.4 examples/sec; 0.434 sec/batch; 39h:09m:40s remains)
INFO - root - 2017-12-06 03:12:43.519218: step 7460, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 0.481 sec/batch; 43h:27m:47s remains)
INFO - root - 2017-12-06 03:12:48.334386: step 7470, loss = 2.05, batch loss = 1.99 (18.1 examples/sec; 0.442 sec/batch; 39h:53m:15s remains)
INFO - root - 2017-12-06 03:12:53.031077: step 7480, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.457 sec/batch; 41h:13m:48s remains)
INFO - root - 2017-12-06 03:12:57.767937: step 7490, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.480 sec/batch; 43h:20m:08s remains)
INFO - root - 2017-12-06 03:13:02.436340: step 7500, loss = 2.03, batch loss = 1.98 (16.8 examples/sec; 0.476 sec/batch; 42h:59m:09s remains)
2017-12-06 03:13:02.899925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2362785 -4.2457242 -4.250247 -4.2455907 -4.2343941 -4.2201571 -4.2066307 -4.2023926 -4.2131529 -4.2337675 -4.25048 -4.2605519 -4.2658854 -4.2694612 -4.267065][-4.2259688 -4.2359972 -4.2414742 -4.2338367 -4.21547 -4.1919527 -4.1666861 -4.1504431 -4.1596594 -4.1881261 -4.2202797 -4.2408237 -4.2497287 -4.2548833 -4.2543874][-4.21757 -4.224988 -4.2244077 -4.2072144 -4.1796179 -4.144958 -4.1052504 -4.0753579 -4.0855818 -4.128252 -4.1807609 -4.214788 -4.228261 -4.2328534 -4.2335148][-4.2090688 -4.2113934 -4.2014337 -4.1708455 -4.1313772 -4.0839772 -4.0245147 -3.9790487 -3.9892118 -4.0513244 -4.1268249 -4.1775961 -4.199439 -4.2089696 -4.2169781][-4.2044992 -4.1992378 -4.1862416 -4.1490078 -4.0977592 -4.0371304 -3.9596512 -3.8994167 -3.9165225 -4.00559 -4.1003151 -4.1621227 -4.1884661 -4.2040434 -4.2182407][-4.1953707 -4.1879897 -4.17736 -4.1414056 -4.0822859 -4.0113435 -3.9162807 -3.8362861 -3.865129 -3.9883773 -4.1057053 -4.1737752 -4.199986 -4.2130413 -4.22456][-4.2015162 -4.1936021 -4.1862922 -4.1529517 -4.0888562 -4.0069251 -3.8965654 -3.7945273 -3.8295326 -3.9822555 -4.1142964 -4.1891961 -4.2190514 -4.2278504 -4.2306027][-4.2150211 -4.2063227 -4.20278 -4.1790247 -4.1154289 -4.0250106 -3.9060421 -3.7989831 -3.834955 -3.99459 -4.1248369 -4.19809 -4.2245789 -4.2230611 -4.2162476][-4.2144904 -4.2125435 -4.2166371 -4.2044253 -4.1526985 -4.0724096 -3.9756277 -3.8968358 -3.93058 -4.0515919 -4.1457257 -4.1995659 -4.2111392 -4.19504 -4.1730728][-4.2003665 -4.2072258 -4.21585 -4.2125926 -4.1791344 -4.122365 -4.0622926 -4.015914 -4.0370889 -4.1025305 -4.1512585 -4.1809826 -4.1781182 -4.1534405 -4.1189408][-4.1781468 -4.1874485 -4.1984549 -4.2033157 -4.1885457 -4.1545086 -4.1240468 -4.1003184 -4.1079149 -4.1285138 -4.14548 -4.1582308 -4.1499228 -4.1241574 -4.0853882][-4.1585579 -4.1684422 -4.1798425 -4.1871481 -4.1797919 -4.1602993 -4.1422629 -4.1287117 -4.1296716 -4.1385179 -4.1487565 -4.1564236 -4.1507397 -4.1287336 -4.0974617][-4.1587625 -4.1643181 -4.17194 -4.17141 -4.1585922 -4.1399474 -4.1187072 -4.10962 -4.1193523 -4.1415076 -4.1615639 -4.1726909 -4.1709456 -4.1619277 -4.1477237][-4.1645646 -4.1663461 -4.1638684 -4.1517658 -4.134696 -4.1184549 -4.09894 -4.0953922 -4.11565 -4.1491313 -4.1786633 -4.1910686 -4.19196 -4.1980343 -4.197567][-4.1747112 -4.1711955 -4.1535974 -4.1295333 -4.1121984 -4.1086688 -4.1037912 -4.110157 -4.1346483 -4.1703577 -4.1985316 -4.2065759 -4.2047229 -4.2142825 -4.2203326]]...]
INFO - root - 2017-12-06 03:13:07.577237: step 7510, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.459 sec/batch; 41h:23m:36s remains)
INFO - root - 2017-12-06 03:13:12.314827: step 7520, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 0.484 sec/batch; 43h:40m:01s remains)
INFO - root - 2017-12-06 03:13:17.017785: step 7530, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.466 sec/batch; 42h:02m:34s remains)
INFO - root - 2017-12-06 03:13:21.584848: step 7540, loss = 2.07, batch loss = 2.02 (17.3 examples/sec; 0.462 sec/batch; 41h:44m:35s remains)
INFO - root - 2017-12-06 03:13:26.262303: step 7550, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.460 sec/batch; 41h:29m:22s remains)
INFO - root - 2017-12-06 03:13:30.946265: step 7560, loss = 2.06, batch loss = 2.01 (16.8 examples/sec; 0.476 sec/batch; 42h:56m:02s remains)
INFO - root - 2017-12-06 03:13:35.729128: step 7570, loss = 2.08, batch loss = 2.03 (17.1 examples/sec; 0.467 sec/batch; 42h:08m:27s remains)
INFO - root - 2017-12-06 03:13:40.457681: step 7580, loss = 2.07, batch loss = 2.02 (16.0 examples/sec; 0.501 sec/batch; 45h:11m:57s remains)
INFO - root - 2017-12-06 03:13:45.259198: step 7590, loss = 2.05, batch loss = 1.99 (16.8 examples/sec; 0.475 sec/batch; 42h:53m:14s remains)
INFO - root - 2017-12-06 03:13:49.951825: step 7600, loss = 2.04, batch loss = 1.98 (16.8 examples/sec; 0.477 sec/batch; 43h:02m:52s remains)
2017-12-06 03:13:50.431055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3231244 -4.294539 -4.2461615 -4.1932387 -4.1527328 -4.1173263 -4.11733 -4.1447082 -4.1751022 -4.2018938 -4.2270532 -4.255157 -4.2696319 -4.2608414 -4.2253][-4.3229094 -4.2958112 -4.2491708 -4.1971226 -4.1498756 -4.0992336 -4.0843654 -4.1043663 -4.129591 -4.1551294 -4.192555 -4.2357836 -4.2618356 -4.2587495 -4.2269011][-4.3205814 -4.3022747 -4.2704382 -4.229784 -4.1805878 -4.11435 -4.0803719 -4.0873618 -4.1031718 -4.1222548 -4.163291 -4.2153716 -4.2497544 -4.2554054 -4.2291927][-4.3191915 -4.310648 -4.2963719 -4.2695289 -4.2218542 -4.1491423 -4.1071887 -4.1077795 -4.1163616 -4.1222477 -4.1547003 -4.2030897 -4.2370248 -4.2501097 -4.2363005][-4.3157191 -4.3124127 -4.3027763 -4.27881 -4.229435 -4.1557403 -4.116293 -4.1241879 -4.1365209 -4.1318288 -4.1464095 -4.18559 -4.2213397 -4.2461014 -4.2518125][-4.3112206 -4.306303 -4.2894306 -4.2558289 -4.1918616 -4.111382 -4.07892 -4.0953808 -4.1068015 -4.0931344 -4.0941534 -4.138998 -4.1935511 -4.2358446 -4.2605944][-4.3091888 -4.3016715 -4.2790627 -4.2358203 -4.1523113 -4.0544734 -4.0191569 -4.0443177 -4.0526266 -4.0259786 -4.0239592 -4.0876451 -4.1634245 -4.2188435 -4.2560873][-4.3114395 -4.3065977 -4.2839518 -4.238297 -4.1499691 -4.041543 -3.9952526 -4.0236521 -4.0377293 -4.008029 -4.0012507 -4.0659885 -4.1459718 -4.2060075 -4.2507191][-4.3096857 -4.3071022 -4.2878814 -4.2483377 -4.1752367 -4.079895 -4.0350604 -4.0644169 -4.0880437 -4.0687432 -4.0550747 -4.0942268 -4.1561718 -4.2097898 -4.2519932][-4.30894 -4.3060646 -4.2913208 -4.26123 -4.2125306 -4.1445627 -4.1124382 -4.138155 -4.1652355 -4.1603007 -4.146595 -4.1623764 -4.1972127 -4.2336245 -4.2624941][-4.3134847 -4.3098292 -4.2995739 -4.2759838 -4.241703 -4.1954932 -4.1796741 -4.2044086 -4.2311468 -4.2362576 -4.2286029 -4.2367244 -4.2534103 -4.2704005 -4.2836356][-4.3176241 -4.3137536 -4.3038883 -4.2814837 -4.2500863 -4.2150559 -4.2104635 -4.2402534 -4.2685776 -4.2771583 -4.2735868 -4.2818789 -4.2938519 -4.3017755 -4.3080297][-4.3187127 -4.31368 -4.3005767 -4.27432 -4.239727 -4.2069545 -4.2070642 -4.2397017 -4.2689915 -4.2789831 -4.2787676 -4.2861137 -4.2989597 -4.3087912 -4.3165402][-4.3204827 -4.31399 -4.2967329 -4.2668533 -4.2324796 -4.2025208 -4.2056079 -4.2379136 -4.265316 -4.2739882 -4.2736921 -4.2794042 -4.291821 -4.3034306 -4.3115144][-4.3273578 -4.3218193 -4.3067484 -4.2812147 -4.2528372 -4.2296906 -4.2339697 -4.2614017 -4.283237 -4.2900348 -4.2896638 -4.2926369 -4.3004866 -4.3084497 -4.3139515]]...]
INFO - root - 2017-12-06 03:13:55.097367: step 7610, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.476 sec/batch; 42h:59m:43s remains)
INFO - root - 2017-12-06 03:13:59.812634: step 7620, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.466 sec/batch; 42h:05m:46s remains)
INFO - root - 2017-12-06 03:14:04.509608: step 7630, loss = 2.06, batch loss = 2.01 (16.8 examples/sec; 0.477 sec/batch; 43h:00m:44s remains)
INFO - root - 2017-12-06 03:14:09.025114: step 7640, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 0.483 sec/batch; 43h:32m:56s remains)
INFO - root - 2017-12-06 03:14:13.634648: step 7650, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.477 sec/batch; 43h:05m:10s remains)
INFO - root - 2017-12-06 03:14:18.293507: step 7660, loss = 2.04, batch loss = 1.99 (16.5 examples/sec; 0.484 sec/batch; 43h:40m:13s remains)
INFO - root - 2017-12-06 03:14:23.049790: step 7670, loss = 2.04, batch loss = 1.98 (16.8 examples/sec; 0.476 sec/batch; 42h:57m:49s remains)
INFO - root - 2017-12-06 03:14:27.737772: step 7680, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 0.481 sec/batch; 43h:22m:53s remains)
INFO - root - 2017-12-06 03:14:32.529570: step 7690, loss = 2.05, batch loss = 2.00 (16.6 examples/sec; 0.483 sec/batch; 43h:33m:47s remains)
INFO - root - 2017-12-06 03:14:37.276728: step 7700, loss = 2.09, batch loss = 2.03 (17.1 examples/sec; 0.467 sec/batch; 42h:05m:51s remains)
2017-12-06 03:14:37.724625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.26328 -4.2639728 -4.2659035 -4.2647147 -4.2614517 -4.2586923 -4.2582884 -4.2573309 -4.2446041 -4.2179012 -4.1905437 -4.1810818 -4.1961684 -4.2224755 -4.2504373][-4.2324424 -4.2368393 -4.2394419 -4.2380643 -4.2320781 -4.2250285 -4.222868 -4.2269707 -4.2202058 -4.1971278 -4.1692715 -4.1540141 -4.1669073 -4.194376 -4.2269216][-4.1912441 -4.2000713 -4.2047958 -4.2017908 -4.191165 -4.1782846 -4.1705232 -4.1761675 -4.1795964 -4.1691556 -4.1479349 -4.1310358 -4.1402802 -4.1680074 -4.2026215][-4.1610289 -4.1760478 -4.18046 -4.1711473 -4.1510372 -4.1282964 -4.1081543 -4.1070113 -4.1156573 -4.121985 -4.119544 -4.1142588 -4.1242623 -4.1552677 -4.1920147][-4.1466112 -4.1709952 -4.17624 -4.1571121 -4.1195383 -4.0767007 -4.036097 -4.0210242 -4.0358863 -4.0673437 -4.0985465 -4.121068 -4.1421676 -4.1748557 -4.2089214][-4.1397276 -4.1690092 -4.1722193 -4.1418676 -4.0845561 -4.0106692 -3.9387681 -3.904799 -3.9314106 -3.999598 -4.0721464 -4.1297784 -4.1717429 -4.2085381 -4.2375808][-4.1365242 -4.1649566 -4.166841 -4.1342015 -4.0643234 -3.9620976 -3.864188 -3.8294039 -3.8778324 -3.9756184 -4.0734615 -4.1514645 -4.2065458 -4.2438073 -4.2657065][-4.129962 -4.1605353 -4.1752806 -4.1550994 -4.09228 -3.9974868 -3.9174364 -3.9053042 -3.9560974 -4.0343127 -4.1073651 -4.174284 -4.2286587 -4.263566 -4.2805953][-4.1231303 -4.1534638 -4.1866989 -4.1898265 -4.1491623 -4.0839252 -4.0382972 -4.0405064 -4.06813 -4.1017842 -4.1358676 -4.1829324 -4.2332172 -4.267982 -4.2839475][-4.1189289 -4.1444516 -4.1888356 -4.2108064 -4.1891913 -4.150157 -4.1297336 -4.1372371 -4.1483078 -4.1504164 -4.1579652 -4.1923227 -4.2392855 -4.2723379 -4.2857428][-4.1302776 -4.1446748 -4.1868062 -4.2140017 -4.2022386 -4.1780033 -4.168489 -4.1786251 -4.1874504 -4.1811867 -4.1814218 -4.2097712 -4.2509885 -4.2801185 -4.2902708][-4.1418266 -4.15152 -4.1882887 -4.2102575 -4.1951866 -4.1733947 -4.1725583 -4.1920557 -4.2106323 -4.2127218 -4.2159405 -4.2377625 -4.2698913 -4.2915363 -4.2960877][-4.1436563 -4.1566324 -4.1918192 -4.2032542 -4.183291 -4.1635871 -4.171104 -4.2021418 -4.2335835 -4.247251 -4.25357 -4.2680163 -4.289391 -4.2996078 -4.2951207][-4.1451178 -4.1646061 -4.2024527 -4.2089977 -4.1920047 -4.1791859 -4.1916924 -4.2230425 -4.2564735 -4.2716074 -4.2785091 -4.2876363 -4.298306 -4.2990904 -4.2896881][-4.1346078 -4.1606545 -4.2046332 -4.2184172 -4.2123222 -4.2063713 -4.2214265 -4.2486343 -4.274652 -4.283648 -4.2865257 -4.2902446 -4.2932868 -4.290401 -4.2844481]]...]
INFO - root - 2017-12-06 03:14:42.415603: step 7710, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.475 sec/batch; 42h:53m:46s remains)
INFO - root - 2017-12-06 03:14:47.099642: step 7720, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.480 sec/batch; 43h:19m:27s remains)
INFO - root - 2017-12-06 03:14:51.618044: step 7730, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.462 sec/batch; 41h:40m:52s remains)
INFO - root - 2017-12-06 03:14:56.309123: step 7740, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.478 sec/batch; 43h:06m:21s remains)
INFO - root - 2017-12-06 03:15:01.113092: step 7750, loss = 2.05, batch loss = 1.99 (16.2 examples/sec; 0.495 sec/batch; 44h:40m:28s remains)
INFO - root - 2017-12-06 03:15:05.758952: step 7760, loss = 2.08, batch loss = 2.03 (17.4 examples/sec; 0.460 sec/batch; 41h:28m:08s remains)
INFO - root - 2017-12-06 03:15:10.472566: step 7770, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.479 sec/batch; 43h:10m:19s remains)
INFO - root - 2017-12-06 03:15:15.157162: step 7780, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.468 sec/batch; 42h:14m:19s remains)
INFO - root - 2017-12-06 03:15:19.812597: step 7790, loss = 2.08, batch loss = 2.02 (16.6 examples/sec; 0.481 sec/batch; 43h:24m:47s remains)
INFO - root - 2017-12-06 03:15:24.551558: step 7800, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.442 sec/batch; 39h:52m:15s remains)
2017-12-06 03:15:25.028204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2358088 -4.2327285 -4.2297707 -4.2261233 -4.2237463 -4.228292 -4.2349987 -4.2384171 -4.2395153 -4.2438087 -4.2516904 -4.2634592 -4.2707033 -4.26843 -4.2600446][-4.2104554 -4.2033648 -4.2028131 -4.2006054 -4.1969156 -4.2001429 -4.2064109 -4.208951 -4.2096848 -4.2150674 -4.2235279 -4.2358236 -4.2441392 -4.2440119 -4.2380686][-4.1737413 -4.1661696 -4.1763787 -4.1838775 -4.18263 -4.1790304 -4.1789594 -4.181457 -4.1840153 -4.1907668 -4.2003822 -4.2098403 -4.2163038 -4.2205467 -4.2174816][-4.1446815 -4.1376019 -4.1507373 -4.1564164 -4.1496744 -4.1351981 -4.1218858 -4.1230493 -4.1385622 -4.1568646 -4.174058 -4.1873136 -4.1962624 -4.2059836 -4.206687][-4.1527257 -4.1417513 -4.1457672 -4.1467953 -4.1394749 -4.1139555 -4.0822964 -4.080987 -4.1147957 -4.150188 -4.1764183 -4.1899276 -4.1956463 -4.2045693 -4.2074471][-4.1622767 -4.1501856 -4.1481819 -4.1481776 -4.138401 -4.1000352 -4.04597 -4.0358849 -4.0870142 -4.1395836 -4.1739111 -4.1899376 -4.1974969 -4.2102389 -4.2193246][-4.153367 -4.1419406 -4.1329622 -4.1224594 -4.0987487 -4.0388069 -3.9508855 -3.9218159 -4.0013695 -4.0873237 -4.1411405 -4.1691451 -4.1823368 -4.2026525 -4.2191243][-4.1522455 -4.1363773 -4.1086783 -4.0788178 -4.0317879 -3.9382226 -3.8001776 -3.7401359 -3.8605466 -3.9952221 -4.0767446 -4.1234903 -4.1478291 -4.1730242 -4.1937876][-4.1777182 -4.1658096 -4.1324935 -4.0888896 -4.0276093 -3.9276011 -3.7816739 -3.7024295 -3.8239408 -3.9718137 -4.0584278 -4.1027365 -4.1287379 -4.1514492 -4.1692123][-4.2057424 -4.2030778 -4.1772509 -4.1421528 -4.09581 -4.0284958 -3.9379454 -3.8852887 -3.9554286 -4.0557814 -4.1148133 -4.1434751 -4.1624975 -4.1791024 -4.188673][-4.2274313 -4.2343979 -4.219955 -4.1984472 -4.1671615 -4.1193275 -4.0615582 -4.028933 -4.0695291 -4.134 -4.1716866 -4.1909504 -4.206358 -4.218811 -4.224041][-4.2494678 -4.2664547 -4.263381 -4.25089 -4.2287498 -4.1925325 -4.1501856 -4.1260767 -4.1457243 -4.1863642 -4.2108221 -4.2242322 -4.2349896 -4.2440395 -4.2482972][-4.2705655 -4.2966881 -4.304522 -4.2959986 -4.2758141 -4.2485948 -4.2209048 -4.206327 -4.2148566 -4.2372918 -4.2514858 -4.26026 -4.267127 -4.2712088 -4.272182][-4.2743039 -4.3064113 -4.3228183 -4.319653 -4.3034239 -4.2855473 -4.273355 -4.2702875 -4.2737603 -4.2814207 -4.2856712 -4.2884436 -4.2916813 -4.293395 -4.2924232][-4.2705426 -4.3023624 -4.3186207 -4.3197203 -4.3094392 -4.2974482 -4.2926741 -4.2940712 -4.2958708 -4.2968454 -4.2974377 -4.2976375 -4.2988405 -4.2999821 -4.3002172]]...]
INFO - root - 2017-12-06 03:15:29.862988: step 7810, loss = 2.05, batch loss = 1.99 (17.7 examples/sec; 0.451 sec/batch; 40h:41m:30s remains)
INFO - root - 2017-12-06 03:15:34.663099: step 7820, loss = 2.04, batch loss = 1.98 (17.4 examples/sec; 0.461 sec/batch; 41h:34m:10s remains)
INFO - root - 2017-12-06 03:15:39.142777: step 7830, loss = 2.06, batch loss = 2.01 (16.8 examples/sec; 0.476 sec/batch; 42h:54m:45s remains)
INFO - root - 2017-12-06 03:15:43.893969: step 7840, loss = 2.03, batch loss = 1.97 (16.8 examples/sec; 0.477 sec/batch; 42h:59m:23s remains)
INFO - root - 2017-12-06 03:15:48.557866: step 7850, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.465 sec/batch; 41h:56m:44s remains)
INFO - root - 2017-12-06 03:15:53.270758: step 7860, loss = 2.04, batch loss = 1.98 (18.2 examples/sec; 0.439 sec/batch; 39h:33m:11s remains)
INFO - root - 2017-12-06 03:15:58.010673: step 7870, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.462 sec/batch; 41h:40m:38s remains)
INFO - root - 2017-12-06 03:16:02.728544: step 7880, loss = 2.06, batch loss = 2.00 (16.6 examples/sec; 0.482 sec/batch; 43h:28m:11s remains)
INFO - root - 2017-12-06 03:16:07.362053: step 7890, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.426 sec/batch; 38h:22m:49s remains)
INFO - root - 2017-12-06 03:16:12.011135: step 7900, loss = 2.07, batch loss = 2.01 (17.9 examples/sec; 0.447 sec/batch; 40h:17m:29s remains)
2017-12-06 03:16:12.474414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2937436 -4.2878857 -4.2904711 -4.2944565 -4.2979131 -4.3005867 -4.3013554 -4.3034511 -4.3025131 -4.3001251 -4.3027673 -4.3094816 -4.3171988 -4.3186932 -4.3164697][-4.2546821 -4.2479362 -4.2512345 -4.2562056 -4.2599134 -4.2635837 -4.2668848 -4.2682261 -4.26182 -4.2553034 -4.2572713 -4.26771 -4.2817917 -4.28728 -4.2873745][-4.2126827 -4.2014704 -4.2008424 -4.204689 -4.2076426 -4.2123742 -4.2189422 -4.2189879 -4.208221 -4.2005439 -4.2039857 -4.2203526 -4.2435608 -4.2541566 -4.2570643][-4.1699123 -4.1454849 -4.13318 -4.133604 -4.1323714 -4.1341925 -4.14355 -4.14258 -4.129097 -4.12577 -4.1387286 -4.1701441 -4.2073212 -4.2245684 -4.2314672][-4.1167789 -4.0723829 -4.0417581 -4.0347261 -4.0273666 -4.0219641 -4.0304847 -4.0211277 -4.0140243 -4.033597 -4.0645533 -4.1140733 -4.1610317 -4.1874251 -4.2083664][-4.0701342 -4.0120363 -3.9643867 -3.9427023 -3.9180279 -3.8973684 -3.8877091 -3.8476887 -3.8566725 -3.9192152 -3.9825044 -4.0577908 -4.1133866 -4.1459665 -4.1805224][-4.0566478 -3.9949021 -3.9311881 -3.8918149 -3.8517833 -3.8163836 -3.771265 -3.6804395 -3.6996129 -3.8065784 -3.9096415 -4.0060072 -4.0595126 -4.0930219 -4.1392236][-4.08072 -4.0285048 -3.96959 -3.9266653 -3.8898747 -3.8614063 -3.821228 -3.7502539 -3.7637396 -3.8468113 -3.9389653 -4.0194149 -4.0519495 -4.0733657 -4.1116171][-4.11399 -4.0741682 -4.0286603 -3.9931002 -3.9658666 -3.9503193 -3.9307857 -3.8994355 -3.908958 -3.953984 -4.0197411 -4.073235 -4.0920105 -4.1014824 -4.1221666][-4.1513462 -4.1212025 -4.0899186 -4.065042 -4.049191 -4.0448818 -4.0342655 -4.0184441 -4.023746 -4.0448642 -4.0836034 -4.116785 -4.1324263 -4.1374183 -4.1459846][-4.1895676 -4.1719112 -4.155107 -4.1418376 -4.1362419 -4.1373477 -4.1321077 -4.1234055 -4.12281 -4.1268582 -4.1436315 -4.1623144 -4.17661 -4.1802793 -4.1820602][-4.2144842 -4.2032847 -4.1949468 -4.19064 -4.1905251 -4.1922722 -4.1915946 -4.18959 -4.189446 -4.187571 -4.1940017 -4.2046027 -4.2166128 -4.2235122 -4.2277174][-4.2361178 -4.2284703 -4.2260218 -4.2276058 -4.2318583 -4.2361832 -4.2380009 -4.2392845 -4.2418294 -4.241847 -4.2451291 -4.2525434 -4.2628617 -4.2705622 -4.2748942][-4.2625685 -4.2562361 -4.2546597 -4.2570558 -4.2619648 -4.2673512 -4.270184 -4.27238 -4.276155 -4.28016 -4.2849588 -4.2917924 -4.3012786 -4.3086233 -4.3119264][-4.2907934 -4.2850194 -4.2832336 -4.2836308 -4.2863073 -4.28953 -4.2919631 -4.2936854 -4.2971287 -4.3018641 -4.3066015 -4.3118353 -4.3192229 -4.3254118 -4.3285341]]...]
INFO - root - 2017-12-06 03:16:17.216972: step 7910, loss = 2.05, batch loss = 1.99 (17.2 examples/sec; 0.466 sec/batch; 42h:00m:55s remains)
INFO - root - 2017-12-06 03:16:21.800762: step 7920, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 41h:56m:41s remains)
INFO - root - 2017-12-06 03:16:26.646215: step 7930, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.458 sec/batch; 41h:17m:13s remains)
INFO - root - 2017-12-06 03:16:31.324102: step 7940, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.462 sec/batch; 41h:40m:53s remains)
INFO - root - 2017-12-06 03:16:36.036299: step 7950, loss = 2.09, batch loss = 2.03 (16.9 examples/sec; 0.472 sec/batch; 42h:34m:17s remains)
INFO - root - 2017-12-06 03:16:40.654500: step 7960, loss = 2.09, batch loss = 2.03 (17.5 examples/sec; 0.458 sec/batch; 41h:16m:17s remains)
INFO - root - 2017-12-06 03:16:45.363084: step 7970, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 41h:56m:09s remains)
INFO - root - 2017-12-06 03:16:50.110000: step 7980, loss = 2.05, batch loss = 2.00 (16.3 examples/sec; 0.490 sec/batch; 44h:12m:36s remains)
INFO - root - 2017-12-06 03:16:54.871289: step 7990, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 0.489 sec/batch; 44h:05m:53s remains)
INFO - root - 2017-12-06 03:16:59.691262: step 8000, loss = 2.05, batch loss = 1.99 (16.5 examples/sec; 0.485 sec/batch; 43h:41m:49s remains)
2017-12-06 03:17:00.170785: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2871408 -4.26341 -4.2407351 -4.2117805 -4.1866384 -4.1746345 -4.1466212 -4.1010475 -4.0747252 -4.1167021 -4.181777 -4.21252 -4.22202 -4.2297668 -4.2402205][-4.2813067 -4.2579107 -4.2386761 -4.2128091 -4.1960115 -4.1899443 -4.1661863 -4.1254244 -4.0995913 -4.1395683 -4.1991043 -4.2243428 -4.2260909 -4.2298346 -4.2383862][-4.2783437 -4.2530565 -4.2329984 -4.2120619 -4.2028952 -4.2033167 -4.183651 -4.1460757 -4.1158233 -4.1462398 -4.1958594 -4.2151303 -4.2137361 -4.212163 -4.2099648][-4.2794805 -4.2525644 -4.2306395 -4.2137504 -4.2084937 -4.2085671 -4.1873555 -4.1474733 -4.1108584 -4.1333652 -4.1730638 -4.1886973 -4.1873589 -4.1831093 -4.1650319][-4.2812314 -4.2539697 -4.2321744 -4.2144728 -4.2040091 -4.1965051 -4.167295 -4.1119595 -4.0618386 -4.0847116 -4.1285696 -4.1502028 -4.1516485 -4.14007 -4.1028638][-4.2821951 -4.255322 -4.2350216 -4.2137985 -4.1924872 -4.169487 -4.115068 -4.0187392 -3.9469991 -3.9954815 -4.0660372 -4.0998726 -4.1058521 -4.0929422 -4.0492134][-4.2794724 -4.2509947 -4.2294931 -4.1991129 -4.1628728 -4.1175709 -4.0256453 -3.8793507 -3.7989113 -3.9103487 -4.0225239 -4.0702925 -4.0842962 -4.0847759 -4.0602684][-4.2763095 -4.2462006 -4.2175741 -4.1763129 -4.1278996 -4.0708604 -3.9685693 -3.8270154 -3.786037 -3.9240124 -4.037704 -4.0858269 -4.1063037 -4.1198449 -4.1150484][-4.27907 -4.2496648 -4.2172117 -4.1746154 -4.1341529 -4.0917215 -4.0266309 -3.9440942 -3.9302557 -4.0260959 -4.101902 -4.1355658 -4.1530366 -4.1628656 -4.1608253][-4.2912564 -4.2664886 -4.2374597 -4.2021112 -4.1757669 -4.152235 -4.1143379 -4.0622616 -4.0540028 -4.10869 -4.1521535 -4.1729846 -4.1858397 -4.1910496 -4.1871581][-4.3085785 -4.290822 -4.2675343 -4.2375531 -4.21817 -4.2033296 -4.1719661 -4.1316943 -4.1252265 -4.1579828 -4.1851916 -4.1991773 -4.2081714 -4.21039 -4.2058496][-4.3207707 -4.3070745 -4.2861037 -4.25792 -4.2373238 -4.2235436 -4.1974516 -4.16714 -4.1594939 -4.1780725 -4.1954718 -4.2076488 -4.214797 -4.2163677 -4.2103438][-4.3238344 -4.3105674 -4.2910986 -4.265502 -4.2447 -4.2326865 -4.211926 -4.189157 -4.1790471 -4.1872668 -4.1993389 -4.2117195 -4.2179475 -4.2178254 -4.2122793][-4.3232331 -4.310462 -4.2941146 -4.2739625 -4.2573895 -4.2465305 -4.2295465 -4.2125034 -4.2042294 -4.210649 -4.221664 -4.2302222 -4.2321925 -4.2301359 -4.2256141][-4.3251481 -4.3159347 -4.3048563 -4.2908478 -4.2797217 -4.2713876 -4.2600241 -4.2505279 -4.2480755 -4.2537518 -4.2605815 -4.263432 -4.2631741 -4.2619023 -4.25952]]...]
INFO - root - 2017-12-06 03:17:04.813167: step 8010, loss = 2.07, batch loss = 2.02 (17.2 examples/sec; 0.464 sec/batch; 41h:49m:04s remains)
INFO - root - 2017-12-06 03:17:09.321230: step 8020, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.461 sec/batch; 41h:34m:49s remains)
INFO - root - 2017-12-06 03:17:14.049703: step 8030, loss = 2.06, batch loss = 2.00 (16.6 examples/sec; 0.482 sec/batch; 43h:29m:16s remains)
INFO - root - 2017-12-06 03:17:18.680665: step 8040, loss = 2.05, batch loss = 2.00 (17.0 examples/sec; 0.471 sec/batch; 42h:27m:04s remains)
INFO - root - 2017-12-06 03:17:23.438083: step 8050, loss = 2.04, batch loss = 1.98 (16.9 examples/sec; 0.475 sec/batch; 42h:46m:28s remains)
INFO - root - 2017-12-06 03:17:28.162535: step 8060, loss = 2.05, batch loss = 1.99 (16.2 examples/sec; 0.495 sec/batch; 44h:36m:43s remains)
INFO - root - 2017-12-06 03:17:32.808931: step 8070, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.461 sec/batch; 41h:30m:37s remains)
INFO - root - 2017-12-06 03:17:37.458085: step 8080, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.476 sec/batch; 42h:55m:53s remains)
INFO - root - 2017-12-06 03:17:42.210801: step 8090, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.480 sec/batch; 43h:12m:48s remains)
INFO - root - 2017-12-06 03:17:46.870080: step 8100, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.462 sec/batch; 41h:39m:54s remains)
2017-12-06 03:17:47.344971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2215757 -4.2098827 -4.2010431 -4.2095113 -4.2301531 -4.2463627 -4.2423677 -4.2294054 -4.2288847 -4.219389 -4.2024722 -4.2051549 -4.2254696 -4.2483225 -4.2716794][-4.2183228 -4.2026367 -4.1923981 -4.1997857 -4.2172012 -4.2314343 -4.2219543 -4.203958 -4.2065053 -4.2070251 -4.1986318 -4.2029 -4.2199507 -4.241312 -4.2652483][-4.2067509 -4.1902347 -4.1777129 -4.1796761 -4.19057 -4.2022152 -4.1909318 -4.1734996 -4.1826282 -4.1946063 -4.1963806 -4.2036643 -4.2170191 -4.235363 -4.259522][-4.1995974 -4.1878047 -4.1747541 -4.1701078 -4.1709771 -4.1762824 -4.1656284 -4.154273 -4.1704884 -4.1896358 -4.198009 -4.2089839 -4.2197247 -4.2337227 -4.2562637][-4.1885943 -4.1857181 -4.1784925 -4.1715097 -4.1621661 -4.1569619 -4.1434894 -4.1374331 -4.1594687 -4.1847239 -4.1993728 -4.2142172 -4.2231345 -4.2326517 -4.2544146][-4.1726494 -4.1774373 -4.1772947 -4.1702313 -4.1530328 -4.1359882 -4.112535 -4.1067128 -4.1369381 -4.170743 -4.1921234 -4.214633 -4.2255583 -4.2319303 -4.2539835][-4.1587415 -4.16688 -4.1681395 -4.1603541 -4.1391158 -4.1105285 -4.0690627 -4.05499 -4.0955238 -4.1438055 -4.1775174 -4.2095165 -4.2256918 -4.2338653 -4.257885][-4.1635127 -4.1681404 -4.1622834 -4.1475997 -4.123445 -4.084959 -4.0194039 -3.987499 -4.0411963 -4.1105833 -4.1577759 -4.1963286 -4.2184815 -4.2345552 -4.2638888][-4.1926365 -4.1873794 -4.1688833 -4.1454034 -4.121676 -4.0828829 -4.0093284 -3.964505 -4.0202947 -4.097518 -4.1480107 -4.1846724 -4.2100506 -4.2355828 -4.2727108][-4.2318525 -4.219 -4.1897821 -4.1593213 -4.1427279 -4.1204982 -4.0699482 -4.0353546 -4.0770149 -4.1374736 -4.1744947 -4.2021594 -4.2241392 -4.2508969 -4.2879062][-4.2654529 -4.2504063 -4.2177749 -4.1864119 -4.1786537 -4.1764016 -4.1558685 -4.1385303 -4.1649785 -4.2020121 -4.2207522 -4.236351 -4.2507544 -4.2722211 -4.3019714][-4.2754765 -4.2627726 -4.2346025 -4.2085648 -4.2079387 -4.2229061 -4.2254915 -4.2206087 -4.2339816 -4.2517424 -4.2583046 -4.2643557 -4.2714391 -4.2871075 -4.3088579][-4.2699385 -4.260488 -4.2392673 -4.2204866 -4.2230306 -4.24554 -4.2601752 -4.2624407 -4.267519 -4.2721295 -4.2728033 -4.2742109 -4.2774525 -4.2899485 -4.3069324][-4.2615075 -4.2531776 -4.2370582 -4.224731 -4.2287669 -4.2485309 -4.2646766 -4.2696271 -4.2721715 -4.2724848 -4.2712483 -4.2716856 -4.275249 -4.287518 -4.3019056][-4.2633944 -4.2584252 -4.24717 -4.2378535 -4.2401967 -4.253418 -4.2671456 -4.2719207 -4.2725277 -4.2711196 -4.2689862 -4.2690716 -4.2718649 -4.2819271 -4.2936034]]...]
INFO - root - 2017-12-06 03:17:52.020593: step 8110, loss = 2.04, batch loss = 1.98 (18.5 examples/sec; 0.431 sec/batch; 38h:52m:43s remains)
INFO - root - 2017-12-06 03:17:56.576782: step 8120, loss = 2.06, batch loss = 2.01 (17.0 examples/sec; 0.471 sec/batch; 42h:26m:21s remains)
INFO - root - 2017-12-06 03:18:01.207499: step 8130, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.431 sec/batch; 38h:50m:54s remains)
INFO - root - 2017-12-06 03:18:05.821346: step 8140, loss = 2.02, batch loss = 1.96 (16.5 examples/sec; 0.485 sec/batch; 43h:42m:14s remains)
INFO - root - 2017-12-06 03:18:10.663019: step 8150, loss = 2.06, batch loss = 2.00 (15.4 examples/sec; 0.521 sec/batch; 46h:56m:21s remains)
INFO - root - 2017-12-06 03:18:15.264879: step 8160, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.475 sec/batch; 42h:47m:08s remains)
INFO - root - 2017-12-06 03:18:20.075108: step 8170, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.476 sec/batch; 42h:51m:13s remains)
INFO - root - 2017-12-06 03:18:24.776435: step 8180, loss = 2.08, batch loss = 2.02 (16.2 examples/sec; 0.494 sec/batch; 44h:28m:43s remains)
INFO - root - 2017-12-06 03:18:29.575426: step 8190, loss = 2.04, batch loss = 1.99 (17.2 examples/sec; 0.465 sec/batch; 41h:54m:31s remains)
INFO - root - 2017-12-06 03:18:34.290359: step 8200, loss = 2.07, batch loss = 2.02 (17.3 examples/sec; 0.461 sec/batch; 41h:33m:59s remains)
2017-12-06 03:18:34.768480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3308287 -4.3136773 -4.2821679 -4.2378182 -4.1957116 -4.1643453 -4.1490784 -4.1736088 -4.1981378 -4.1980033 -4.1934237 -4.196105 -4.2145381 -4.2411666 -4.2657948][-4.3307142 -4.3093095 -4.2708325 -4.2187705 -4.1689878 -4.1309891 -4.1195107 -4.1553621 -4.1921811 -4.2044573 -4.2078123 -4.2126904 -4.228622 -4.2504587 -4.2699018][-4.3269787 -4.3022771 -4.2573676 -4.1975403 -4.1398449 -4.0970411 -4.0938253 -4.1459937 -4.1985197 -4.2270093 -4.2412429 -4.2503505 -4.2643356 -4.2781076 -4.2874327][-4.3206534 -4.2942653 -4.2485113 -4.18863 -4.1277332 -4.0794744 -4.0759325 -4.1338592 -4.1959219 -4.2366214 -4.26154 -4.2777667 -4.2926211 -4.3023248 -4.3060236][-4.3190823 -4.2954478 -4.2526584 -4.194694 -4.1306305 -4.0698533 -4.0499005 -4.1014318 -4.1679144 -4.2180109 -4.2518878 -4.2758508 -4.2951012 -4.3076611 -4.3124709][-4.3185081 -4.2990489 -4.2604957 -4.2040024 -4.13609 -4.0555611 -4.0037308 -4.0361929 -4.10288 -4.1645436 -4.2099471 -4.2439461 -4.2712593 -4.29083 -4.3008986][-4.3204908 -4.3035212 -4.2669773 -4.2086072 -4.1293955 -4.0190835 -3.9217272 -3.9286392 -3.9995241 -4.077281 -4.1413465 -4.1892128 -4.2274585 -4.256969 -4.2744122][-4.3235326 -4.3059363 -4.2691746 -4.2094502 -4.1209497 -3.9825976 -3.8454864 -3.8365085 -3.9166322 -4.0064907 -4.0847111 -4.1414828 -4.1850209 -4.2194929 -4.2407451][-4.3211966 -4.3021812 -4.2683592 -4.2154164 -4.1338506 -3.9999163 -3.8659649 -3.8623381 -3.9429214 -4.0274835 -4.0980844 -4.1419673 -4.1753926 -4.2027149 -4.2200632][-4.3170214 -4.2992539 -4.2723408 -4.233254 -4.172852 -4.0684509 -3.9651644 -3.9710798 -4.0393009 -4.1049652 -4.1555071 -4.1801438 -4.1968503 -4.2102385 -4.2183452][-4.3165259 -4.3017364 -4.2823296 -4.2548246 -4.2134924 -4.1410336 -4.0714865 -4.0836515 -4.1339531 -4.1786933 -4.2120156 -4.2248797 -4.2317653 -4.2336693 -4.2328539][-4.3193111 -4.3071389 -4.2936521 -4.27388 -4.2475019 -4.2021651 -4.1602783 -4.173882 -4.2089372 -4.2355442 -4.2539659 -4.2606006 -4.2620144 -4.2562189 -4.2496967][-4.32011 -4.3061829 -4.293088 -4.2759757 -4.2579384 -4.2320104 -4.2111545 -4.2292256 -4.25883 -4.2777743 -4.2868857 -4.2865763 -4.2811842 -4.2712097 -4.2617812][-4.3205709 -4.3056016 -4.28831 -4.2667389 -4.2486367 -4.2317052 -4.2225094 -4.2471542 -4.2805262 -4.30114 -4.3085055 -4.3041515 -4.2929158 -4.2802281 -4.2691631][-4.3226328 -4.3081093 -4.2859273 -4.2569814 -4.2319283 -4.2108345 -4.2004824 -4.2285476 -4.2688251 -4.2971191 -4.3098359 -4.306468 -4.2959051 -4.2846551 -4.2735858]]...]
INFO - root - 2017-12-06 03:18:39.347362: step 8210, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.479 sec/batch; 43h:07m:54s remains)
INFO - root - 2017-12-06 03:18:44.082636: step 8220, loss = 2.05, batch loss = 1.99 (16.9 examples/sec; 0.475 sec/batch; 42h:45m:36s remains)
INFO - root - 2017-12-06 03:18:48.751404: step 8230, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.453 sec/batch; 40h:48m:12s remains)
INFO - root - 2017-12-06 03:18:53.504811: step 8240, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 0.472 sec/batch; 42h:28m:10s remains)
INFO - root - 2017-12-06 03:18:58.134806: step 8250, loss = 2.05, batch loss = 1.99 (17.5 examples/sec; 0.458 sec/batch; 41h:13m:11s remains)
INFO - root - 2017-12-06 03:19:02.904276: step 8260, loss = 2.05, batch loss = 1.99 (16.1 examples/sec; 0.497 sec/batch; 44h:44m:04s remains)
INFO - root - 2017-12-06 03:19:07.583933: step 8270, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 41h:53m:17s remains)
INFO - root - 2017-12-06 03:19:12.265536: step 8280, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.484 sec/batch; 43h:35m:04s remains)
INFO - root - 2017-12-06 03:19:17.048818: step 8290, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.462 sec/batch; 41h:38m:35s remains)
INFO - root - 2017-12-06 03:19:21.690027: step 8300, loss = 2.08, batch loss = 2.02 (16.1 examples/sec; 0.496 sec/batch; 44h:40m:26s remains)
2017-12-06 03:19:22.150767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.341321 -4.3354192 -4.3321919 -4.3307147 -4.3307648 -4.33141 -4.33074 -4.3295708 -4.3297882 -4.3318911 -4.3353057 -4.3397913 -4.3449121 -4.350842 -4.3568239][-4.3287749 -4.3208394 -4.3157635 -4.3133178 -4.3136888 -4.3150392 -4.314467 -4.3135 -4.3144135 -4.3174911 -4.32174 -4.3270941 -4.3329458 -4.3397317 -4.3480997][-4.3125911 -4.3030539 -4.29691 -4.2932086 -4.2912235 -4.2897277 -4.2877846 -4.2852006 -4.2857943 -4.2896872 -4.2948122 -4.3022103 -4.3091569 -4.3172359 -4.329011][-4.2917671 -4.2767315 -4.266973 -4.2608738 -4.2546458 -4.2478924 -4.2398081 -4.2324696 -4.23284 -4.2393618 -4.2467589 -4.2578292 -4.2688165 -4.2802677 -4.2979875][-4.2607803 -4.2344928 -4.2170205 -4.2065349 -4.1955719 -4.1808958 -4.1617217 -4.1462889 -4.148859 -4.1630864 -4.1746073 -4.191256 -4.209147 -4.2274036 -4.2552738][-4.2135859 -4.1743374 -4.1495132 -4.1349235 -4.1192589 -4.0945892 -4.0583248 -4.0329537 -4.048203 -4.0818667 -4.1034322 -4.1264062 -4.1517158 -4.1776485 -4.2160368][-4.1662431 -4.1192231 -4.09298 -4.0756464 -4.0491443 -4.0068512 -3.9471838 -3.9118347 -3.9541261 -4.0178056 -4.0548697 -4.0824323 -4.1105318 -4.1414576 -4.1890616][-4.1443429 -4.0971775 -4.0681248 -4.04028 -3.9959137 -3.9355726 -3.8588009 -3.8262825 -3.9016781 -3.9932375 -4.0418482 -4.0655684 -4.0885067 -4.122519 -4.1768894][-4.1584549 -4.1157184 -4.0840993 -4.046576 -3.9954808 -3.9384298 -3.8783755 -3.8682446 -3.9465482 -4.036746 -4.0812426 -4.0971856 -4.1133275 -4.1427946 -4.1921368][-4.1875424 -4.1515522 -4.1201029 -4.0824952 -4.0377636 -3.9956093 -3.9635115 -3.9685509 -4.03 -4.1022205 -4.1397 -4.15565 -4.167624 -4.1858196 -4.2193594][-4.2044168 -4.1754947 -4.1497469 -4.1230359 -4.0940213 -4.0685363 -4.0531287 -4.0596247 -4.0977364 -4.1483622 -4.1827664 -4.2022414 -4.2124205 -4.222177 -4.2403674][-4.2293825 -4.2124443 -4.1986885 -4.1851997 -4.1711612 -4.1581583 -4.1491284 -4.1501613 -4.16728 -4.2000904 -4.2277007 -4.2465906 -4.2550631 -4.2589231 -4.26713][-4.264421 -4.2574496 -4.253437 -4.2494764 -4.2442718 -4.2385054 -4.2332144 -4.2310653 -4.2347555 -4.2504921 -4.2665386 -4.2810392 -4.2885904 -4.2913513 -4.2961483][-4.2956929 -4.2928896 -4.2954068 -4.2983708 -4.3014865 -4.3033385 -4.3039026 -4.3026142 -4.29961 -4.3002286 -4.3026686 -4.3100591 -4.3161039 -4.3185878 -4.3223715][-4.3252583 -4.3235307 -4.3270741 -4.3306012 -4.3350472 -4.3397226 -4.344316 -4.34519 -4.342402 -4.3375697 -4.3330421 -4.3342867 -4.338594 -4.3413773 -4.3437238]]...]
INFO - root - 2017-12-06 03:19:26.645360: step 8310, loss = 2.05, batch loss = 1.99 (15.6 examples/sec; 0.511 sec/batch; 46h:02m:17s remains)
INFO - root - 2017-12-06 03:19:31.401393: step 8320, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.461 sec/batch; 41h:31m:49s remains)
INFO - root - 2017-12-06 03:19:36.167254: step 8330, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.464 sec/batch; 41h:45m:01s remains)
INFO - root - 2017-12-06 03:19:40.848070: step 8340, loss = 2.06, batch loss = 2.00 (16.6 examples/sec; 0.482 sec/batch; 43h:22m:36s remains)
INFO - root - 2017-12-06 03:19:45.583019: step 8350, loss = 2.10, batch loss = 2.05 (17.0 examples/sec; 0.469 sec/batch; 42h:15m:00s remains)
INFO - root - 2017-12-06 03:19:50.282250: step 8360, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.468 sec/batch; 42h:10m:26s remains)
INFO - root - 2017-12-06 03:19:55.111387: step 8370, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.479 sec/batch; 43h:06m:34s remains)
INFO - root - 2017-12-06 03:19:59.776001: step 8380, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.473 sec/batch; 42h:36m:05s remains)
INFO - root - 2017-12-06 03:20:04.582906: step 8390, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.480 sec/batch; 43h:13m:27s remains)
INFO - root - 2017-12-06 03:20:09.096177: step 8400, loss = 2.08, batch loss = 2.02 (26.3 examples/sec; 0.304 sec/batch; 27h:22m:54s remains)
2017-12-06 03:20:09.576007: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3250847 -4.3153367 -4.2990837 -4.2638712 -4.2093263 -4.1516485 -4.1211777 -4.1250648 -4.1568007 -4.2090044 -4.2628741 -4.2945719 -4.2995749 -4.2899594 -4.2782912][-4.332232 -4.3244748 -4.3179679 -4.2961383 -4.252512 -4.2007241 -4.1661129 -4.1547771 -4.16457 -4.1997261 -4.2486038 -4.2852283 -4.3007193 -4.2970128 -4.2882752][-4.3370433 -4.3318586 -4.3331094 -4.3225956 -4.2906408 -4.248982 -4.2143073 -4.1855321 -4.1687264 -4.1832252 -4.2260928 -4.2710686 -4.3015094 -4.3110328 -4.3103676][-4.3380132 -4.3344827 -4.3384833 -4.3335624 -4.3082156 -4.2724342 -4.2353511 -4.1913295 -4.1521339 -4.1495981 -4.1905036 -4.2473574 -4.2949505 -4.3221335 -4.3325219][-4.340652 -4.3379612 -4.340694 -4.3364129 -4.3126388 -4.2762351 -4.2310023 -4.174108 -4.1186485 -4.10068 -4.1418762 -4.2115655 -4.274745 -4.3196707 -4.3437333][-4.3429241 -4.3407145 -4.3396544 -4.3304958 -4.302834 -4.2621522 -4.2082419 -4.1379991 -4.0654421 -4.0339341 -4.0812454 -4.1657996 -4.2438154 -4.3040648 -4.3419061][-4.34354 -4.3394418 -4.331316 -4.312007 -4.2775049 -4.2302709 -4.1677589 -4.0803118 -3.9859297 -3.9410794 -4.0039506 -4.1087885 -4.2019992 -4.2756314 -4.3274565][-4.3419404 -4.3344665 -4.3177738 -4.2850051 -4.2397752 -4.182838 -4.1103311 -4.0071983 -3.8927655 -3.8428309 -3.9284375 -4.0556579 -4.162148 -4.2459235 -4.3078361][-4.3400812 -4.3271217 -4.3008709 -4.2563028 -4.2025666 -4.142067 -4.0692377 -3.967392 -3.8547504 -3.81448 -3.9071047 -4.03565 -4.1421366 -4.2263861 -4.290195][-4.3395386 -4.3204165 -4.28683 -4.2351966 -4.1772747 -4.1184945 -4.0560141 -3.9762115 -3.895051 -3.8802309 -3.9597974 -4.066052 -4.1567273 -4.2283359 -4.2836771][-4.3403692 -4.3192434 -4.2850246 -4.2364764 -4.18255 -4.1304884 -4.0805635 -4.0261097 -3.9814241 -3.9884093 -4.0496759 -4.1281829 -4.1991911 -4.2539415 -4.2936759][-4.3406992 -4.3220305 -4.2955804 -4.2601347 -4.218986 -4.1778293 -4.1408343 -4.1073575 -4.0880404 -4.1039715 -4.1460395 -4.2009778 -4.2524166 -4.2896476 -4.3106132][-4.3421063 -4.3289089 -4.3133297 -4.2934108 -4.2678351 -4.2405481 -4.21614 -4.197783 -4.1933208 -4.2104492 -4.2389154 -4.2736239 -4.3045893 -4.32242 -4.3240576][-4.3444991 -4.3365893 -4.3291793 -4.3202009 -4.3080883 -4.2935333 -4.2794428 -4.271019 -4.2739863 -4.2888002 -4.306005 -4.3227854 -4.3344636 -4.3351707 -4.3254662][-4.3438172 -4.338748 -4.335721 -4.3329744 -4.3292966 -4.3237581 -4.3184719 -4.3167038 -4.3215065 -4.33 -4.3363829 -4.3381739 -4.3341861 -4.3273954 -4.31669]]...]
INFO - root - 2017-12-06 03:20:14.266251: step 8410, loss = 2.05, batch loss = 1.99 (16.5 examples/sec; 0.484 sec/batch; 43h:36m:31s remains)
INFO - root - 2017-12-06 03:20:19.080013: step 8420, loss = 2.03, batch loss = 1.97 (16.5 examples/sec; 0.486 sec/batch; 43h:44m:06s remains)
INFO - root - 2017-12-06 03:20:23.813879: step 8430, loss = 2.07, batch loss = 2.01 (16.4 examples/sec; 0.488 sec/batch; 43h:58m:07s remains)
INFO - root - 2017-12-06 03:20:28.527525: step 8440, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.474 sec/batch; 42h:38m:39s remains)
INFO - root - 2017-12-06 03:20:33.309216: step 8450, loss = 2.02, batch loss = 1.96 (16.4 examples/sec; 0.487 sec/batch; 43h:52m:03s remains)
INFO - root - 2017-12-06 03:20:38.022396: step 8460, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.477 sec/batch; 42h:53m:34s remains)
INFO - root - 2017-12-06 03:20:42.761078: step 8470, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.466 sec/batch; 41h:56m:27s remains)
INFO - root - 2017-12-06 03:20:47.329929: step 8480, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.457 sec/batch; 41h:10m:11s remains)
INFO - root - 2017-12-06 03:20:52.165957: step 8490, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.478 sec/batch; 43h:00m:52s remains)
INFO - root - 2017-12-06 03:20:56.713754: step 8500, loss = 2.08, batch loss = 2.02 (16.0 examples/sec; 0.499 sec/batch; 44h:56m:10s remains)
2017-12-06 03:20:57.123985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3007212 -4.303196 -4.3054752 -4.3063712 -4.3066926 -4.3061824 -4.3046761 -4.3016057 -4.297627 -4.2946815 -4.2922792 -4.2900982 -4.2886195 -4.2901616 -4.2934518][-4.3028674 -4.3056116 -4.3075314 -4.3080873 -4.3079758 -4.3062811 -4.3031478 -4.2990446 -4.2944579 -4.2909951 -4.2886095 -4.287631 -4.2874842 -4.2898026 -4.2927771][-4.3081503 -4.3098221 -4.3102226 -4.3093224 -4.3087225 -4.3064103 -4.3021364 -4.2979465 -4.2940087 -4.2918816 -4.2914886 -4.2932124 -4.2954445 -4.2987218 -4.3009882][-4.3041067 -4.3025446 -4.2980447 -4.2926416 -4.2894697 -4.2859645 -4.2807174 -4.2764988 -4.2744184 -4.27644 -4.281683 -4.2887144 -4.2937918 -4.2972708 -4.2976794][-4.2912164 -4.28441 -4.2717414 -4.258667 -4.2497215 -4.24312 -4.2345986 -4.2290711 -4.2294159 -4.2376719 -4.2514057 -4.266048 -4.2745094 -4.2768 -4.273447][-4.2627511 -4.2481432 -4.2249026 -4.2008648 -4.1832418 -4.1699238 -4.1546092 -4.1459217 -4.1502304 -4.1675444 -4.1934147 -4.2186685 -4.2323155 -4.2335615 -4.2276664][-4.2546167 -4.2364149 -4.2007875 -4.1618061 -4.1315289 -4.1059361 -4.0791097 -4.0635729 -4.0673971 -4.0888824 -4.1215138 -4.1529527 -4.1677637 -4.1664906 -4.1601806][-4.2465978 -4.2333488 -4.1968946 -4.1573195 -4.1296716 -4.1065288 -4.0813966 -4.0674796 -4.0694141 -4.0854759 -4.1110015 -4.1360579 -4.1441908 -4.1373787 -4.129621][-4.2183557 -4.2103195 -4.1809535 -4.1514907 -4.137207 -4.1307492 -4.1259117 -4.1281991 -4.13811 -4.153017 -4.1707039 -4.1868744 -4.1897225 -4.182344 -4.1770225][-4.228384 -4.2216296 -4.199801 -4.1814079 -4.1771178 -4.1813831 -4.1904583 -4.2031 -4.2174931 -4.230979 -4.2414484 -4.248322 -4.2469568 -4.2408648 -4.2375317][-4.2538424 -4.2506251 -4.2410159 -4.2355895 -4.2388072 -4.2465978 -4.2590036 -4.2719879 -4.2837987 -4.2935553 -4.2989922 -4.3011451 -4.299655 -4.29629 -4.294591][-4.2883663 -4.2867708 -4.2832332 -4.282536 -4.285645 -4.290926 -4.2990971 -4.3074274 -4.3146234 -4.3197551 -4.3216867 -4.3222265 -4.3225112 -4.3225517 -4.3231163][-4.3176589 -4.316092 -4.3133869 -4.3108668 -4.3085232 -4.3075161 -4.3089209 -4.3116269 -4.3141885 -4.3155961 -4.3151455 -4.3143511 -4.3144917 -4.3152823 -4.3167777][-4.3205352 -4.3184357 -4.316216 -4.3134356 -4.3091955 -4.3055863 -4.3040137 -4.3038278 -4.3039212 -4.3030291 -4.3000445 -4.2964754 -4.2936845 -4.2926683 -4.2944078][-4.3004766 -4.2975316 -4.296773 -4.2972331 -4.2967906 -4.2964754 -4.2974958 -4.2989893 -4.3001895 -4.2996745 -4.2958302 -4.2906032 -4.2855277 -4.283114 -4.2845526]]...]
INFO - root - 2017-12-06 03:21:01.796540: step 8510, loss = 2.08, batch loss = 2.03 (17.1 examples/sec; 0.468 sec/batch; 42h:04m:28s remains)
INFO - root - 2017-12-06 03:21:06.541178: step 8520, loss = 2.06, batch loss = 2.01 (16.7 examples/sec; 0.478 sec/batch; 43h:03m:13s remains)
INFO - root - 2017-12-06 03:21:11.247040: step 8530, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.472 sec/batch; 42h:28m:27s remains)
INFO - root - 2017-12-06 03:21:16.014737: step 8540, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.467 sec/batch; 41h:59m:56s remains)
INFO - root - 2017-12-06 03:21:20.749335: step 8550, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 0.470 sec/batch; 42h:20m:15s remains)
INFO - root - 2017-12-06 03:21:25.422109: step 8560, loss = 2.05, batch loss = 1.99 (17.4 examples/sec; 0.460 sec/batch; 41h:23m:14s remains)
INFO - root - 2017-12-06 03:21:30.044066: step 8570, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.461 sec/batch; 41h:30m:25s remains)
INFO - root - 2017-12-06 03:21:34.784832: step 8580, loss = 2.06, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 41h:49m:04s remains)
INFO - root - 2017-12-06 03:21:39.304382: step 8590, loss = 2.04, batch loss = 1.98 (22.3 examples/sec; 0.359 sec/batch; 32h:16m:36s remains)
INFO - root - 2017-12-06 03:21:43.876008: step 8600, loss = 2.04, batch loss = 1.98 (16.8 examples/sec; 0.475 sec/batch; 42h:44m:52s remains)
2017-12-06 03:21:44.336549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2493725 -4.248908 -4.2486339 -4.2491875 -4.24833 -4.24623 -4.2426605 -4.2390771 -4.2375708 -4.2385507 -4.2410231 -4.2431145 -4.24203 -4.2384973 -4.229147][-4.2600203 -4.2616935 -4.2648497 -4.2689614 -4.2699409 -4.2662854 -4.2590981 -4.2518888 -4.2477803 -4.249548 -4.2542953 -4.2589188 -4.2585711 -4.25367 -4.2422185][-4.2687688 -4.2726049 -4.2803259 -4.2901134 -4.2929525 -4.2880225 -4.278523 -4.2703633 -4.2668958 -4.2683682 -4.2740064 -4.2787724 -4.2776279 -4.2718449 -4.260283][-4.2702436 -4.2725396 -4.2814207 -4.2924395 -4.2932081 -4.283215 -4.2729125 -4.2702332 -4.2749252 -4.2808542 -4.286869 -4.2889338 -4.2854047 -4.2789598 -4.2692728][-4.2529464 -4.2510819 -4.2566767 -4.2582135 -4.2472048 -4.2235441 -4.2053518 -4.2118182 -4.2358732 -4.2565703 -4.2664638 -4.2669449 -4.2645717 -4.2625365 -4.2593355][-4.2061343 -4.2102933 -4.2216396 -4.2210832 -4.1944518 -4.1429396 -4.0982652 -4.1062446 -4.1573148 -4.2008848 -4.2183943 -4.2194133 -4.2191834 -4.2230668 -4.2286835][-4.1535082 -4.1657891 -4.1876273 -4.1923437 -4.1548967 -4.0702233 -3.9805079 -3.9722583 -4.0481873 -4.1188393 -4.1498203 -4.1580024 -4.1633639 -4.174 -4.1879063][-4.1255174 -4.1350989 -4.1591883 -4.167995 -4.12781 -4.0260053 -3.9036977 -3.8724937 -3.9535322 -4.042026 -4.0928197 -4.1146464 -4.1295066 -4.1457548 -4.164474][-4.1179409 -4.1232948 -4.1416783 -4.1521711 -4.1297731 -4.0611539 -3.9705694 -3.9352989 -3.9865985 -4.054563 -4.1031404 -4.1271992 -4.1415367 -4.1548738 -4.1728311][-4.1190991 -4.1237812 -4.1332178 -4.1380615 -4.1333342 -4.1079865 -4.0598626 -4.0314255 -4.0486751 -4.0860057 -4.1227279 -4.141746 -4.15079 -4.158412 -4.1748581][-4.1151228 -4.1204023 -4.1255832 -4.1258793 -4.124362 -4.1219749 -4.1042314 -4.0819221 -4.0815325 -4.1026988 -4.1339145 -4.1526904 -4.158916 -4.1625915 -4.1744862][-4.1032658 -4.1094618 -4.1148219 -4.1177349 -4.1227756 -4.1317911 -4.1306148 -4.1152897 -4.1119766 -4.1318088 -4.1622953 -4.1816559 -4.1870089 -4.1874781 -4.1925297][-4.1177216 -4.1245031 -4.1285057 -4.1290927 -4.1323829 -4.1435342 -4.1513753 -4.1441445 -4.1441655 -4.1649013 -4.1927786 -4.2097621 -4.2147546 -4.217473 -4.2218871][-4.1709733 -4.172297 -4.1715431 -4.1688943 -4.1687756 -4.1748686 -4.180378 -4.1766586 -4.1773567 -4.1920877 -4.2093267 -4.2203188 -4.2269936 -4.2335658 -4.2411265][-4.2264218 -4.2221165 -4.2176609 -4.2143416 -4.2104344 -4.2081642 -4.2084837 -4.2042341 -4.2036943 -4.2116761 -4.2201982 -4.2259293 -4.2336254 -4.2438807 -4.2538848]]...]
INFO - root - 2017-12-06 03:21:49.045702: step 8610, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.461 sec/batch; 41h:26m:56s remains)
INFO - root - 2017-12-06 03:21:53.654248: step 8620, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.452 sec/batch; 40h:38m:43s remains)
INFO - root - 2017-12-06 03:21:58.432473: step 8630, loss = 2.05, batch loss = 1.99 (17.5 examples/sec; 0.457 sec/batch; 41h:09m:15s remains)
INFO - root - 2017-12-06 03:22:03.063982: step 8640, loss = 2.04, batch loss = 1.98 (17.3 examples/sec; 0.463 sec/batch; 41h:39m:44s remains)
INFO - root - 2017-12-06 03:22:07.737632: step 8650, loss = 2.04, batch loss = 1.98 (16.6 examples/sec; 0.483 sec/batch; 43h:24m:52s remains)
INFO - root - 2017-12-06 03:22:12.523922: step 8660, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.470 sec/batch; 42h:17m:08s remains)
INFO - root - 2017-12-06 03:22:17.401142: step 8670, loss = 2.06, batch loss = 2.00 (16.1 examples/sec; 0.496 sec/batch; 44h:35m:14s remains)
INFO - root - 2017-12-06 03:22:22.044871: step 8680, loss = 2.05, batch loss = 1.99 (18.5 examples/sec; 0.433 sec/batch; 38h:58m:50s remains)
INFO - root - 2017-12-06 03:22:26.542566: step 8690, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.467 sec/batch; 41h:57m:59s remains)
INFO - root - 2017-12-06 03:22:31.328805: step 8700, loss = 2.02, batch loss = 1.97 (15.8 examples/sec; 0.507 sec/batch; 45h:35m:28s remains)
2017-12-06 03:22:31.789716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2365723 -4.2341752 -4.2317867 -4.2316632 -4.2315702 -4.2348738 -4.2404203 -4.2427335 -4.2432733 -4.2453995 -4.2484679 -4.2513356 -4.252708 -4.2526813 -4.2524118][-4.214406 -4.2104387 -4.2071586 -4.2080235 -4.2094612 -4.2144132 -4.2224374 -4.2271533 -4.230804 -4.2361522 -4.2417274 -4.2465167 -4.249105 -4.2490592 -4.2481136][-4.1870403 -4.1803 -4.1733227 -4.1695867 -4.1687984 -4.1741438 -4.1856055 -4.1945548 -4.2023625 -4.2124166 -4.2223415 -4.2320156 -4.2382107 -4.2402358 -4.2395716][-4.1622849 -4.1523838 -4.1379523 -4.1240578 -4.1165633 -4.1158209 -4.1263289 -4.1420341 -4.1586866 -4.1765666 -4.1929336 -4.213491 -4.2305522 -4.2369418 -4.2353587][-4.1431041 -4.1287951 -4.1042032 -4.0767088 -4.0581865 -4.0449533 -4.04849 -4.0671816 -4.0963936 -4.1297369 -4.1573939 -4.1915259 -4.2220459 -4.2350163 -4.2346926][-4.1285944 -4.1033864 -4.0656633 -4.0227709 -3.9875641 -3.9553945 -3.9511497 -3.9745212 -4.0147772 -4.0620975 -4.10666 -4.1577897 -4.2014875 -4.2222233 -4.2272291][-4.1124916 -4.0710135 -4.0179615 -3.9609983 -3.9060659 -3.8491952 -3.8350878 -3.8751631 -3.9300992 -3.9860737 -4.0474524 -4.1090736 -4.1608577 -4.1888928 -4.2029][-4.0882659 -4.0348821 -3.9735668 -3.9104929 -3.8407278 -3.7597466 -3.7397816 -3.8104844 -3.8833897 -3.9371798 -3.9983218 -4.0571938 -4.1045823 -4.1344967 -4.1561375][-4.0911779 -4.0361757 -3.9758251 -3.9211032 -3.8619795 -3.7893248 -3.7738898 -3.8415587 -3.8995197 -3.9304709 -3.9722424 -4.0159254 -4.0546908 -4.0829411 -4.10923][-4.1191211 -4.0771952 -4.0323234 -3.9969203 -3.9556925 -3.9064145 -3.89787 -3.938024 -3.9572408 -3.9600408 -3.981318 -4.0014133 -4.0207329 -4.0419946 -4.072969][-4.1566114 -4.1287618 -4.0994978 -4.0771813 -4.0475492 -4.0156379 -4.0167365 -4.0375042 -4.0305138 -4.0094805 -4.0072703 -4.0024161 -4.0048828 -4.0200529 -4.0553975][-4.1942096 -4.1704793 -4.1448655 -4.1268029 -4.1025472 -4.0773425 -4.0809622 -4.0920115 -4.0793428 -4.0471077 -4.02368 -4.00661 -4.0060449 -4.0187969 -4.0481315][-4.2186356 -4.1926727 -4.1618972 -4.1362953 -4.1116862 -4.0921855 -4.0957971 -4.1033878 -4.0972362 -4.0739779 -4.0467272 -4.0274725 -4.0251245 -4.0358267 -4.0530586][-4.2202497 -4.1897545 -4.1510968 -4.1179733 -4.0909405 -4.0748892 -4.0764756 -4.085043 -4.0890937 -4.08427 -4.0706806 -4.0543923 -4.0479269 -4.0599179 -4.0662][-4.1976843 -4.1634946 -4.1197624 -4.08365 -4.0572133 -4.0418043 -4.0374269 -4.0420647 -4.0531545 -4.0630307 -4.0629196 -4.0526943 -4.0510826 -4.0683293 -4.0680294]]...]
INFO - root - 2017-12-06 03:22:36.544189: step 8710, loss = 2.06, batch loss = 2.00 (16.4 examples/sec; 0.488 sec/batch; 43h:52m:07s remains)
INFO - root - 2017-12-06 03:22:41.242060: step 8720, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.463 sec/batch; 41h:41m:06s remains)
INFO - root - 2017-12-06 03:22:45.902163: step 8730, loss = 2.06, batch loss = 2.00 (16.6 examples/sec; 0.481 sec/batch; 43h:13m:26s remains)
INFO - root - 2017-12-06 03:22:50.682179: step 8740, loss = 2.05, batch loss = 1.99 (16.2 examples/sec; 0.493 sec/batch; 44h:17m:37s remains)
INFO - root - 2017-12-06 03:22:55.361721: step 8750, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.462 sec/batch; 41h:33m:20s remains)
INFO - root - 2017-12-06 03:23:00.021612: step 8760, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 0.486 sec/batch; 43h:42m:26s remains)
INFO - root - 2017-12-06 03:23:04.738717: step 8770, loss = 2.05, batch loss = 2.00 (16.7 examples/sec; 0.480 sec/batch; 43h:08m:29s remains)
INFO - root - 2017-12-06 03:23:09.440335: step 8780, loss = 2.08, batch loss = 2.03 (17.4 examples/sec; 0.460 sec/batch; 41h:19m:30s remains)
INFO - root - 2017-12-06 03:23:13.941608: step 8790, loss = 2.02, batch loss = 1.96 (17.0 examples/sec; 0.471 sec/batch; 42h:20m:04s remains)
INFO - root - 2017-12-06 03:23:18.647598: step 8800, loss = 2.06, batch loss = 2.01 (18.1 examples/sec; 0.441 sec/batch; 39h:38m:55s remains)
2017-12-06 03:23:19.126622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0623426 -4.0720844 -4.0710449 -4.0632119 -4.0603113 -4.0595708 -4.0590897 -4.0581837 -4.0577736 -4.0541625 -4.0759873 -4.0927625 -4.0822978 -4.0557165 -4.0321937][-4.0763969 -4.079071 -4.0785604 -4.0651808 -4.0601292 -4.0657296 -4.0717025 -4.0764475 -4.0833559 -4.0867023 -4.1047711 -4.1096749 -4.0910349 -4.0608139 -4.03829][-4.0855246 -4.0870476 -4.0873871 -4.067565 -4.0585566 -4.0600214 -4.0629172 -4.0686507 -4.08669 -4.1032672 -4.1221485 -4.12744 -4.1097727 -4.0812211 -4.0583982][-4.0840383 -4.0847116 -4.0826526 -4.0602713 -4.0493135 -4.0467649 -4.0456271 -4.0450878 -4.0636339 -4.0888472 -4.1076846 -4.1151576 -4.1069193 -4.0899105 -4.0786614][-4.0829368 -4.0744929 -4.0615983 -4.0394998 -4.0340672 -4.03192 -4.0296159 -4.0237088 -4.0366936 -4.0621819 -4.074224 -4.0790281 -4.0783129 -4.0737333 -4.0705652][-4.072113 -4.0639138 -4.0438576 -4.0205269 -4.0120263 -4.0094447 -4.0067663 -3.9949689 -3.9987817 -4.0131426 -4.0182576 -4.0216355 -4.0304332 -4.0410843 -4.0481234][-4.0601792 -4.0640278 -4.046545 -4.021347 -4.0003285 -3.9818213 -3.9704731 -3.9588873 -3.9606321 -3.9659681 -3.9672735 -3.9682336 -3.981384 -4.0059347 -4.0298953][-4.0839114 -4.0911522 -4.0740709 -4.0413346 -4.0056844 -3.9808939 -3.9726861 -3.9779592 -3.9843969 -3.9804454 -3.9704428 -3.9653244 -3.9745116 -3.9967334 -4.0240049][-4.1352916 -4.144208 -4.1298442 -4.09047 -4.0378656 -4.0078297 -4.0125179 -4.034214 -4.0458374 -4.0332775 -4.0188913 -4.0109549 -4.0146933 -4.0247707 -4.0408216][-4.1744418 -4.1822586 -4.1683908 -4.1230083 -4.06252 -4.0362234 -4.0528526 -4.078486 -4.0874038 -4.0678091 -4.0558643 -4.0566411 -4.0578671 -4.0563641 -4.05728][-4.173152 -4.1755772 -4.1586647 -4.1129546 -4.0656881 -4.0572248 -4.0865531 -4.1138473 -4.1113243 -4.0862937 -4.0749702 -4.079905 -4.0799212 -4.0706573 -4.06561][-4.157012 -4.1554427 -4.1369009 -4.0951109 -4.0642004 -4.0703239 -4.1065283 -4.1366286 -4.1350632 -4.1164193 -4.1044936 -4.1050353 -4.1013179 -4.0883427 -4.0811706][-4.1540966 -4.1495295 -4.1296568 -4.0959048 -4.0773654 -4.0894775 -4.1287866 -4.1615133 -4.1649184 -4.153553 -4.141839 -4.1390042 -4.1268458 -4.1083117 -4.0947542][-4.1532927 -4.1443186 -4.1221504 -4.0978308 -4.0873828 -4.1005545 -4.1420269 -4.1718178 -4.1775193 -4.17344 -4.16555 -4.1622524 -4.1475406 -4.1232772 -4.1039872][-4.1510139 -4.1420293 -4.1255426 -4.1068311 -4.0974369 -4.1065578 -4.14331 -4.1661758 -4.169107 -4.1674471 -4.1631494 -4.16085 -4.1460781 -4.1225061 -4.1093435]]...]
INFO - root - 2017-12-06 03:23:23.829673: step 8810, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.436 sec/batch; 39h:13m:24s remains)
INFO - root - 2017-12-06 03:23:28.487165: step 8820, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.459 sec/batch; 41h:15m:00s remains)
INFO - root - 2017-12-06 03:23:33.158684: step 8830, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.474 sec/batch; 42h:35m:26s remains)
INFO - root - 2017-12-06 03:23:37.882196: step 8840, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.469 sec/batch; 42h:09m:00s remains)
INFO - root - 2017-12-06 03:23:42.570062: step 8850, loss = 2.08, batch loss = 2.02 (17.7 examples/sec; 0.451 sec/batch; 40h:33m:10s remains)
INFO - root - 2017-12-06 03:23:47.309889: step 8860, loss = 2.06, batch loss = 2.01 (17.1 examples/sec; 0.469 sec/batch; 42h:08m:32s remains)
INFO - root - 2017-12-06 03:23:51.930646: step 8870, loss = 2.05, batch loss = 2.00 (17.3 examples/sec; 0.461 sec/batch; 41h:28m:59s remains)
INFO - root - 2017-12-06 03:23:56.539263: step 8880, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.486 sec/batch; 43h:39m:51s remains)
INFO - root - 2017-12-06 03:24:01.332499: step 8890, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.479 sec/batch; 43h:03m:35s remains)
INFO - root - 2017-12-06 03:24:05.941540: step 8900, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.470 sec/batch; 42h:13m:19s remains)
2017-12-06 03:24:06.373867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1525221 -4.1458769 -4.1518288 -4.1598973 -4.1556783 -4.1593385 -4.1560903 -4.1409674 -4.1340261 -4.1389122 -4.142065 -4.14857 -4.1471686 -4.1285229 -4.1260638][-4.1038184 -4.1066012 -4.1223307 -4.1413488 -4.1518941 -4.1743474 -4.1757483 -4.1630764 -4.1609879 -4.1700058 -4.1701159 -4.1627383 -4.1395621 -4.1062365 -4.1016717][-4.0524273 -4.0734482 -4.1079106 -4.1441116 -4.1668625 -4.1994467 -4.1990218 -4.1832681 -4.17715 -4.1837368 -4.1774063 -4.1554279 -4.1101356 -4.0660386 -4.0610843][-3.9810042 -4.0299525 -4.091887 -4.153646 -4.1857638 -4.2120838 -4.2022557 -4.1777897 -4.168509 -4.1730876 -4.1637893 -4.1347547 -4.0819421 -4.0376763 -4.0361595][-3.9305186 -4.008235 -4.0933776 -4.1636639 -4.1915374 -4.1997976 -4.165225 -4.121511 -4.1099482 -4.1226354 -4.1211271 -4.1027918 -4.0672874 -4.0448685 -4.054944][-3.982986 -4.0666432 -4.1410565 -4.1844034 -4.1851559 -4.16172 -4.095068 -4.03282 -4.0306997 -4.0663991 -4.0842819 -4.0891776 -4.0860982 -4.0919991 -4.1111603][-4.0742512 -4.1416645 -4.1832809 -4.1863441 -4.1487241 -4.08797 -3.9897113 -3.9175158 -3.9500494 -4.0253277 -4.0735407 -4.1032262 -4.1255312 -4.1432838 -4.1577][-4.1155772 -4.157083 -4.1660862 -4.1284409 -4.0477014 -3.9509294 -3.8315659 -3.7616918 -3.8513956 -3.9747956 -4.0554113 -4.1061616 -4.1414194 -4.1540394 -4.150836][-4.1432424 -4.1567636 -4.1305952 -4.0546527 -3.9426558 -3.8307259 -3.7149951 -3.6632371 -3.801764 -3.9529619 -4.0453258 -4.1032848 -4.1421723 -4.1433792 -4.1245551][-4.1653581 -4.1580997 -4.1098943 -4.0223689 -3.9189451 -3.8336742 -3.7701476 -3.759028 -3.8815567 -4.0110483 -4.0848107 -4.1316047 -4.1666079 -4.160594 -4.1393795][-4.1656852 -4.1486006 -4.0975513 -4.0201139 -3.9409657 -3.8902917 -3.8736708 -3.8920085 -3.9830728 -4.0773048 -4.1307173 -4.1637239 -4.1904597 -4.1826668 -4.1655641][-4.1724634 -4.1505909 -4.1030841 -4.04299 -3.9884789 -3.9669592 -3.9776671 -4.009901 -4.07525 -4.13853 -4.17252 -4.1883488 -4.2029095 -4.1930766 -4.1802654][-4.209331 -4.1865692 -4.1494775 -4.1139026 -4.0858603 -4.0809593 -4.1007414 -4.1309342 -4.169745 -4.2026219 -4.2187123 -4.2232075 -4.2271504 -4.219285 -4.2137976][-4.2624993 -4.2431555 -4.2177176 -4.2000184 -4.18896 -4.1932788 -4.2120867 -4.2328906 -4.251791 -4.2652488 -4.2725573 -4.2732406 -4.2728353 -4.268116 -4.26773][-4.304122 -4.2906528 -4.2756844 -4.2681952 -4.2661257 -4.2741952 -4.2877035 -4.3002529 -4.3079414 -4.3120923 -4.3137259 -4.3131914 -4.3117466 -4.3082552 -4.3091917]]...]
INFO - root - 2017-12-06 03:24:11.136283: step 8910, loss = 2.05, batch loss = 1.99 (17.5 examples/sec; 0.458 sec/batch; 41h:09m:10s remains)
INFO - root - 2017-12-06 03:24:15.840792: step 8920, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.467 sec/batch; 41h:57m:15s remains)
INFO - root - 2017-12-06 03:24:20.608202: step 8930, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.464 sec/batch; 41h:44m:20s remains)
INFO - root - 2017-12-06 03:24:25.343131: step 8940, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 0.478 sec/batch; 42h:57m:52s remains)
INFO - root - 2017-12-06 03:24:30.033946: step 8950, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.464 sec/batch; 41h:43m:12s remains)
INFO - root - 2017-12-06 03:24:34.631999: step 8960, loss = 2.07, batch loss = 2.02 (17.3 examples/sec; 0.462 sec/batch; 41h:31m:59s remains)
INFO - root - 2017-12-06 03:24:39.247154: step 8970, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.461 sec/batch; 41h:26m:36s remains)
INFO - root - 2017-12-06 03:24:43.725532: step 8980, loss = 2.05, batch loss = 2.00 (17.0 examples/sec; 0.470 sec/batch; 42h:14m:18s remains)
INFO - root - 2017-12-06 03:24:48.407122: step 8990, loss = 2.06, batch loss = 2.00 (16.9 examples/sec; 0.472 sec/batch; 42h:26m:07s remains)
INFO - root - 2017-12-06 03:24:53.061167: step 9000, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.471 sec/batch; 42h:20m:06s remains)
2017-12-06 03:24:53.544675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0435057 -4.0578308 -4.0520482 -4.0296679 -4.0104055 -4.0317407 -4.07773 -4.1162844 -4.132297 -4.1204152 -4.0959697 -4.074894 -4.0776086 -4.0919924 -4.0977139][-4.081563 -4.0948777 -4.084197 -4.061008 -4.0444345 -4.0628171 -4.1078033 -4.14551 -4.1553059 -4.1346035 -4.0992184 -4.0695381 -4.0667558 -4.0767012 -4.0835052][-4.0895667 -4.1071424 -4.0997806 -4.0791974 -4.0645103 -4.0791855 -4.1198788 -4.1562014 -4.1634293 -4.1391878 -4.1021152 -4.0702219 -4.06399 -4.0719738 -4.0833731][-4.0865517 -4.1044188 -4.0993505 -4.0827141 -4.0721407 -4.0821943 -4.1130314 -4.1437569 -4.1505852 -4.1302629 -4.0953035 -4.0641356 -4.0590549 -4.0714192 -4.0913386][-4.0895066 -4.0975971 -4.0852385 -4.0678096 -4.0575666 -4.0619855 -4.0830927 -4.1086478 -4.120553 -4.1088185 -4.0756154 -4.0444078 -4.0372028 -4.0567966 -4.0881763][-4.0831466 -4.0822005 -4.058167 -4.0320649 -4.0159912 -4.0114965 -4.0203414 -4.0433469 -4.0659223 -4.0696354 -4.0477223 -4.0214276 -4.0122213 -4.0359755 -4.0774908][-4.0713243 -4.0612335 -4.0281549 -3.9989314 -3.9783347 -3.9584074 -3.946003 -3.9631183 -4.003726 -4.0280685 -4.0265813 -4.01651 -4.0067697 -4.0268035 -4.0708776][-4.0578618 -4.0416055 -4.0034537 -3.9743795 -3.9498951 -3.9127173 -3.8637111 -3.8555758 -3.9168007 -3.9783769 -4.010396 -4.0235672 -4.0210209 -4.0366788 -4.078177][-4.045588 -4.0326695 -3.9998367 -3.9684634 -3.9409215 -3.8986595 -3.8301487 -3.7895775 -3.851861 -3.9423559 -4.0049796 -4.040556 -4.051446 -4.0672936 -4.1021132][-4.0669413 -4.0514574 -4.0206327 -3.9891839 -3.96726 -3.945487 -3.9024405 -3.8633413 -3.889627 -3.9630508 -4.0303712 -4.0718017 -4.0851917 -4.0984168 -4.1256366][-4.1086135 -4.0914278 -4.0621872 -4.0342722 -4.0223465 -4.0186768 -3.9990492 -3.9646792 -3.9633143 -4.01033 -4.0671062 -4.1024094 -4.1127763 -4.1217895 -4.1401854][-4.1472535 -4.1399856 -4.1190434 -4.0959477 -4.0889897 -4.091167 -4.0805821 -4.052886 -4.0435739 -4.0720048 -4.1137919 -4.139698 -4.144464 -4.1442046 -4.1500549][-4.1907787 -4.1938014 -4.181005 -4.1607337 -4.152102 -4.1547675 -4.1483331 -4.1284356 -4.1163607 -4.1292934 -4.1540051 -4.167417 -4.1586742 -4.1423573 -4.1383958][-4.2270522 -4.2360845 -4.2319813 -4.2171211 -4.2081757 -4.2060204 -4.1971059 -4.1766615 -4.157752 -4.1572704 -4.1692581 -4.17181 -4.1542377 -4.1325283 -4.1240458][-4.2447405 -4.256319 -4.2596197 -4.2527738 -4.2457995 -4.238894 -4.2278109 -4.2076349 -4.1852412 -4.1764922 -4.1815252 -4.181232 -4.1678557 -4.149837 -4.137362]]...]
INFO - root - 2017-12-06 03:24:58.232990: step 9010, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.464 sec/batch; 41h:40m:15s remains)
INFO - root - 2017-12-06 03:25:02.904995: step 9020, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.477 sec/batch; 42h:52m:40s remains)
INFO - root - 2017-12-06 03:25:07.673984: step 9030, loss = 2.06, batch loss = 2.00 (17.0 examples/sec; 0.470 sec/batch; 42h:16m:31s remains)
INFO - root - 2017-12-06 03:25:12.390695: step 9040, loss = 2.09, batch loss = 2.03 (16.9 examples/sec; 0.474 sec/batch; 42h:33m:55s remains)
INFO - root - 2017-12-06 03:25:17.170242: step 9050, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.463 sec/batch; 41h:33m:21s remains)
INFO - root - 2017-12-06 03:25:21.845097: step 9060, loss = 2.11, batch loss = 2.05 (17.1 examples/sec; 0.468 sec/batch; 42h:02m:50s remains)
INFO - root - 2017-12-06 03:25:26.419960: step 9070, loss = 2.06, batch loss = 2.00 (26.7 examples/sec; 0.300 sec/batch; 26h:55m:28s remains)
INFO - root - 2017-12-06 03:25:31.200050: step 9080, loss = 2.05, batch loss = 1.99 (16.9 examples/sec; 0.472 sec/batch; 42h:25m:28s remains)
INFO - root - 2017-12-06 03:25:35.891764: step 9090, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.474 sec/batch; 42h:33m:06s remains)
INFO - root - 2017-12-06 03:25:40.556994: step 9100, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.485 sec/batch; 43h:32m:08s remains)
2017-12-06 03:25:40.992813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2181091 -4.2303 -4.238236 -4.2468414 -4.258707 -4.2577863 -4.2494555 -4.2534571 -4.2667489 -4.2711391 -4.2673273 -4.2618861 -4.2513046 -4.23938 -4.2350373][-4.187274 -4.2052693 -4.2187023 -4.2317209 -4.2467518 -4.2448797 -4.2312479 -4.2319455 -4.2467866 -4.2486029 -4.2396297 -4.2306972 -4.217648 -4.2051253 -4.2033453][-4.16804 -4.1918111 -4.208643 -4.22126 -4.2359576 -4.2327013 -4.214601 -4.211463 -4.2251463 -4.2192388 -4.1982584 -4.187644 -4.1772308 -4.1682582 -4.1730094][-4.1497512 -4.1756053 -4.1921082 -4.1996746 -4.2121286 -4.2054372 -4.1783471 -4.1700611 -4.1885953 -4.1844788 -4.1568289 -4.1421585 -4.1329355 -4.1323891 -4.1489272][-4.1383681 -4.1612439 -4.1747 -4.1783261 -4.1846824 -4.1647177 -4.1172285 -4.0994406 -4.1369495 -4.1527038 -4.1325822 -4.1190529 -4.1161423 -4.1268744 -4.1499348][-4.1327081 -4.1498656 -4.1595888 -4.1587629 -4.1557941 -4.1163659 -4.0372863 -4.0038815 -4.070632 -4.12653 -4.1327553 -4.1316071 -4.1391397 -4.15866 -4.1848087][-4.1282907 -4.1377106 -4.1440468 -4.1386027 -4.1206608 -4.0589457 -3.9418972 -3.8871405 -3.9902439 -4.0948706 -4.1410122 -4.1683354 -4.1881495 -4.2121282 -4.2361469][-4.1066766 -4.1130986 -4.1218524 -4.1146307 -4.0844154 -4.0001993 -3.8392539 -3.7619817 -3.9079242 -4.0528822 -4.1330829 -4.1875515 -4.2232313 -4.2517734 -4.2777028][-4.0747232 -4.0769453 -4.0948405 -4.1032219 -4.081439 -3.9960132 -3.8299761 -3.7461858 -3.893728 -4.0406957 -4.1276255 -4.1907468 -4.2336812 -4.2608042 -4.2841287][-4.0701489 -4.0706282 -4.0970116 -4.1191607 -4.1112819 -4.0517673 -3.9353256 -3.8781185 -3.9789236 -4.0886345 -4.1550579 -4.2052684 -4.24021 -4.2615075 -4.2786827][-4.0961208 -4.0950251 -4.118175 -4.139133 -4.1379123 -4.103518 -4.0377035 -4.0094905 -4.0755286 -4.14885 -4.1890388 -4.2208333 -4.2442064 -4.2567239 -4.263783][-4.1334143 -4.1295848 -4.1460619 -4.1616321 -4.1640615 -4.1453247 -4.1107059 -4.0985694 -4.146811 -4.1975856 -4.2199507 -4.2374249 -4.2518239 -4.253653 -4.2468257][-4.1654429 -4.1663651 -4.1799707 -4.1941843 -4.2014823 -4.1921964 -4.1728454 -4.1660829 -4.2002697 -4.2367253 -4.2503433 -4.255178 -4.2565584 -4.2494802 -4.2325482][-4.1828685 -4.1910229 -4.2079997 -4.2244816 -4.2350354 -4.2338881 -4.2221003 -4.2159157 -4.2367649 -4.2606287 -4.2668967 -4.2620225 -4.2547994 -4.2438264 -4.2265873][-4.1985011 -4.2067981 -4.22854 -4.2465 -4.2600179 -4.2647262 -4.2578392 -4.2521853 -4.2636638 -4.2778773 -4.2790956 -4.2698245 -4.259768 -4.2495651 -4.2400827]]...]
INFO - root - 2017-12-06 03:25:45.621802: step 9110, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.467 sec/batch; 41h:55m:36s remains)
INFO - root - 2017-12-06 03:25:50.403032: step 9120, loss = 2.03, batch loss = 1.98 (17.0 examples/sec; 0.471 sec/batch; 42h:18m:05s remains)
INFO - root - 2017-12-06 03:25:55.097256: step 9130, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.467 sec/batch; 41h:54m:21s remains)
INFO - root - 2017-12-06 03:25:59.849523: step 9140, loss = 2.08, batch loss = 2.02 (16.5 examples/sec; 0.484 sec/batch; 43h:30m:03s remains)
INFO - root - 2017-12-06 03:26:04.528861: step 9150, loss = 2.05, batch loss = 1.99 (16.9 examples/sec; 0.472 sec/batch; 42h:24m:12s remains)
INFO - root - 2017-12-06 03:26:09.209254: step 9160, loss = 2.03, batch loss = 1.97 (17.0 examples/sec; 0.471 sec/batch; 42h:18m:07s remains)
INFO - root - 2017-12-06 03:26:13.768596: step 9170, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 0.483 sec/batch; 43h:24m:15s remains)
INFO - root - 2017-12-06 03:26:18.451152: step 9180, loss = 2.09, batch loss = 2.03 (17.2 examples/sec; 0.466 sec/batch; 41h:52m:22s remains)
INFO - root - 2017-12-06 03:26:23.218086: step 9190, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.467 sec/batch; 41h:56m:18s remains)
INFO - root - 2017-12-06 03:26:27.880656: step 9200, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 0.485 sec/batch; 43h:31m:54s remains)
2017-12-06 03:26:28.439207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3111477 -4.290051 -4.2710414 -4.2559667 -4.2416482 -4.2352338 -4.2416382 -4.2578197 -4.2787762 -4.2908273 -4.29407 -4.2942247 -4.2923727 -4.2942796 -4.3004017][-4.3054028 -4.2772946 -4.2505713 -4.2247128 -4.1977763 -4.1808529 -4.1861529 -4.20818 -4.2406125 -4.2630868 -4.2700977 -4.2717786 -4.2683086 -4.2669077 -4.2703118][-4.3014107 -4.2654958 -4.225853 -4.1805468 -4.1332436 -4.1006274 -4.1008577 -4.1299605 -4.1795549 -4.2201028 -4.2340403 -4.2370653 -4.2360392 -4.2369814 -4.2444172][-4.29706 -4.2538815 -4.2019949 -4.1425805 -4.0822954 -4.0403042 -4.0263915 -4.0548944 -4.1208549 -4.177742 -4.1972685 -4.2020512 -4.2069039 -4.21261 -4.2259426][-4.2923493 -4.2461209 -4.1878676 -4.1261806 -4.0640163 -4.0031085 -3.9575691 -3.9756191 -4.0588822 -4.1328053 -4.1635818 -4.1717005 -4.178329 -4.186975 -4.2077308][-4.2908278 -4.2462058 -4.1838932 -4.11843 -4.0513239 -3.9655647 -3.8807859 -3.8827267 -3.9841988 -4.0793281 -4.1270413 -4.1437449 -4.1569271 -4.166698 -4.1919789][-4.2848916 -4.2393374 -4.17218 -4.1035194 -4.0290337 -3.9167697 -3.7852168 -3.7496507 -3.8685112 -4.0034366 -4.0768442 -4.1074429 -4.129735 -4.1456423 -4.176312][-4.2728815 -4.2235532 -4.1555772 -4.0845428 -3.9906478 -3.8512211 -3.6729057 -3.5969887 -3.7351985 -3.914223 -4.0138388 -4.0598536 -4.095778 -4.1232986 -4.1637306][-4.2617264 -4.2038279 -4.1303592 -4.0528364 -3.9440386 -3.8003588 -3.6457293 -3.5947046 -3.7314901 -3.897799 -3.9932759 -4.0386209 -4.0723462 -4.1034112 -4.1459327][-4.2518773 -4.191824 -4.1158862 -4.039547 -3.9447765 -3.8362381 -3.7563915 -3.7605412 -3.8603373 -3.9653859 -4.0285239 -4.0550785 -4.0730338 -4.0972013 -4.1318197][-4.2514038 -4.2003193 -4.1342268 -4.0703993 -4.0010815 -3.9294519 -3.8940768 -3.9248822 -3.9935532 -4.0468812 -4.0756564 -4.0888066 -4.0981894 -4.1122613 -4.1337423][-4.2619452 -4.2210169 -4.1671228 -4.1151104 -4.063385 -4.0163817 -4.004137 -4.0383825 -4.0774736 -4.0969677 -4.1071081 -4.1145115 -4.1217194 -4.1291461 -4.143115][-4.28054 -4.2486033 -4.2052155 -4.1615257 -4.1206956 -4.0894113 -4.0878553 -4.1124191 -4.1279006 -4.127903 -4.1286378 -4.136559 -4.1479173 -4.1538887 -4.1680446][-4.3054619 -4.2808414 -4.2483082 -4.2144437 -4.1852856 -4.1668105 -4.1694803 -4.1846743 -4.1898561 -4.1844225 -4.1784444 -4.1820288 -4.192616 -4.1957769 -4.2066078][-4.3269548 -4.308238 -4.2852807 -4.263135 -4.2434616 -4.2341866 -4.2409291 -4.2545972 -4.2612233 -4.2572236 -4.2488751 -4.2468944 -4.2511778 -4.2520504 -4.2588205]]...]
INFO - root - 2017-12-06 03:26:33.145701: step 9210, loss = 2.03, batch loss = 1.98 (16.7 examples/sec; 0.479 sec/batch; 43h:00m:58s remains)
INFO - root - 2017-12-06 03:26:37.880612: step 9220, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.468 sec/batch; 42h:01m:48s remains)
INFO - root - 2017-12-06 03:26:42.505782: step 9230, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.450 sec/batch; 40h:25m:07s remains)
INFO - root - 2017-12-06 03:26:47.133129: step 9240, loss = 2.09, batch loss = 2.03 (17.9 examples/sec; 0.447 sec/batch; 40h:10m:22s remains)
INFO - root - 2017-12-06 03:26:51.914515: step 9250, loss = 2.05, batch loss = 1.99 (16.9 examples/sec; 0.474 sec/batch; 42h:34m:07s remains)
INFO - root - 2017-12-06 03:26:56.601777: step 9260, loss = 2.07, batch loss = 2.02 (17.1 examples/sec; 0.469 sec/batch; 42h:07m:40s remains)
INFO - root - 2017-12-06 03:27:01.118688: step 9270, loss = 2.05, batch loss = 1.99 (17.3 examples/sec; 0.462 sec/batch; 41h:27m:38s remains)
INFO - root - 2017-12-06 03:27:05.804821: step 9280, loss = 2.09, batch loss = 2.03 (16.7 examples/sec; 0.480 sec/batch; 43h:03m:55s remains)
INFO - root - 2017-12-06 03:27:10.521348: step 9290, loss = 2.04, batch loss = 1.98 (17.2 examples/sec; 0.464 sec/batch; 41h:41m:52s remains)
INFO - root - 2017-12-06 03:27:15.141466: step 9300, loss = 2.05, batch loss = 2.00 (17.6 examples/sec; 0.455 sec/batch; 40h:49m:27s remains)
2017-12-06 03:27:15.623564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2269974 -4.2266612 -4.2197347 -4.2078643 -4.1967945 -4.1907024 -4.184124 -4.1892204 -4.2108974 -4.2333755 -4.2448435 -4.2434068 -4.2302508 -4.2187538 -4.2200809][-4.2100472 -4.2060852 -4.1951756 -4.1811113 -4.1693711 -4.1636071 -4.159143 -4.1639562 -4.1871538 -4.2134008 -4.2294555 -4.234066 -4.2264347 -4.2194328 -4.2220864][-4.1819305 -4.1745977 -4.1612968 -4.1449437 -4.1332207 -4.1307197 -4.1315651 -4.1380033 -4.1638775 -4.1933365 -4.2113819 -4.2189121 -4.2175651 -4.215435 -4.2203975][-4.1487732 -4.1385474 -4.123415 -4.1043015 -4.0920715 -4.0935912 -4.1001153 -4.1084127 -4.1364851 -4.1698346 -4.1917548 -4.2018542 -4.2040548 -4.2065759 -4.2152333][-4.1341772 -4.1220403 -4.1068368 -4.0882382 -4.0790553 -4.0826459 -4.0900722 -4.0972176 -4.1237655 -4.1589031 -4.1855021 -4.1991315 -4.2044563 -4.2079668 -4.21623][-4.1376076 -4.1284137 -4.1139312 -4.0972528 -4.0894303 -4.0900531 -4.0914464 -4.0927844 -4.1151943 -4.1495662 -4.1782494 -4.196207 -4.2063417 -4.210566 -4.2176704][-4.1347575 -4.1272769 -4.1104021 -4.0930481 -4.0831356 -4.0798969 -4.077054 -4.0778747 -4.1004505 -4.1361542 -4.1678944 -4.1890883 -4.2023721 -4.2081447 -4.216167][-4.1122551 -4.1081095 -4.0885353 -4.0670166 -4.05213 -4.0472059 -4.0472879 -4.0562148 -4.0858541 -4.1235442 -4.1578703 -4.180613 -4.1957026 -4.2049432 -4.2158866][-4.0830288 -4.0829248 -4.0648117 -4.0418139 -4.0234938 -4.0168648 -4.0223947 -4.0420375 -4.0796885 -4.1197863 -4.1553974 -4.1787128 -4.1932292 -4.2040749 -4.2175889][-4.0826173 -4.0899796 -4.0767765 -4.0547 -4.0350914 -4.0286932 -4.0362091 -4.0613604 -4.1007338 -4.137301 -4.1695333 -4.1903362 -4.2008848 -4.209332 -4.2223063][-4.088006 -4.0980282 -4.0890269 -4.0692124 -4.0505724 -4.0444851 -4.0532074 -4.0794325 -4.1180329 -4.1527376 -4.1854377 -4.2074966 -4.214747 -4.2185078 -4.2289543][-4.0812125 -4.090713 -4.0842953 -4.06974 -4.05758 -4.0547609 -4.0629454 -4.0868006 -4.1222534 -4.1571941 -4.1927762 -4.2163754 -4.2213736 -4.2222867 -4.2314696][-4.0654197 -4.075069 -4.07344 -4.0675969 -4.064507 -4.0656147 -4.0699415 -4.0870681 -4.1176291 -4.1518178 -4.1880884 -4.21255 -4.2167969 -4.2169156 -4.2259574][-4.057179 -4.0634732 -4.0631824 -4.0607953 -4.061842 -4.0623407 -4.0594306 -4.0686116 -4.0953875 -4.1300292 -4.1678076 -4.1944337 -4.2019062 -4.2049613 -4.2156453][-4.0670042 -4.0682974 -4.0641675 -4.0582633 -4.0567417 -4.0555978 -4.0495296 -4.054368 -4.0772867 -4.109519 -4.1465921 -4.1748028 -4.1873794 -4.1949959 -4.20749]]...]
INFO - root - 2017-12-06 03:27:20.236672: step 9310, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.459 sec/batch; 41h:12m:44s remains)
INFO - root - 2017-12-06 03:27:24.964548: step 9320, loss = 2.05, batch loss = 1.99 (15.9 examples/sec; 0.502 sec/batch; 45h:01m:41s remains)
INFO - root - 2017-12-06 03:27:29.668298: step 9330, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.460 sec/batch; 41h:19m:22s remains)
INFO - root - 2017-12-06 03:27:34.320551: step 9340, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.477 sec/batch; 42h:51m:32s remains)
INFO - root - 2017-12-06 03:27:39.071915: step 9350, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.471 sec/batch; 42h:15m:47s remains)
INFO - root - 2017-12-06 03:27:43.569197: step 9360, loss = 2.05, batch loss = 1.99 (16.9 examples/sec; 0.474 sec/batch; 42h:33m:06s remains)
INFO - root - 2017-12-06 03:27:48.229679: step 9370, loss = 2.10, batch loss = 2.04 (17.3 examples/sec; 0.464 sec/batch; 41h:36m:38s remains)
INFO - root - 2017-12-06 03:27:52.910019: step 9380, loss = 2.05, batch loss = 1.99 (16.8 examples/sec; 0.475 sec/batch; 42h:39m:24s remains)
INFO - root - 2017-12-06 03:27:57.651497: step 9390, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.477 sec/batch; 42h:50m:52s remains)
INFO - root - 2017-12-06 03:28:02.392872: step 9400, loss = 2.06, batch loss = 2.00 (16.4 examples/sec; 0.487 sec/batch; 43h:42m:43s remains)
2017-12-06 03:28:02.849805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1471491 -4.1478729 -4.1693449 -4.1853452 -4.1879168 -4.1954741 -4.2165742 -4.227726 -4.2225065 -4.1749682 -4.1176147 -4.112102 -4.1539865 -4.1906877 -4.222898][-4.1488118 -4.1463523 -4.1797047 -4.2110586 -4.2201691 -4.2244554 -4.235558 -4.2428508 -4.2381864 -4.1877317 -4.1251354 -4.1141267 -4.156806 -4.1983743 -4.2359896][-4.1760292 -4.1692576 -4.2017984 -4.2290645 -4.2319922 -4.2299023 -4.2358871 -4.2411761 -4.2343903 -4.1887813 -4.125957 -4.1105871 -4.1566181 -4.2051687 -4.2468905][-4.2067232 -4.197063 -4.2233248 -4.2371674 -4.2310805 -4.2287283 -4.2338309 -4.2333364 -4.2225418 -4.1808448 -4.1198478 -4.10659 -4.1600113 -4.2124863 -4.250989][-4.2152958 -4.208499 -4.2239318 -4.2148242 -4.2007837 -4.2037191 -4.2110276 -4.2124963 -4.199194 -4.1601167 -4.1027961 -4.0983772 -4.1584358 -4.2083926 -4.2368751][-4.1957059 -4.188447 -4.1853995 -4.146709 -4.1149921 -4.1115265 -4.119319 -4.1221461 -4.1067166 -4.0741954 -4.0336089 -4.04664 -4.1173325 -4.1742167 -4.2049031][-4.1434097 -4.1334262 -4.1143317 -4.0469847 -3.9899592 -3.9815774 -3.9912777 -3.9845798 -3.9580681 -3.9298849 -3.9104109 -3.9501131 -4.0377026 -4.1094403 -4.1602635][-4.0944004 -4.0779648 -4.044662 -3.9633732 -3.8978097 -3.8949831 -3.9164176 -3.9112723 -3.8814874 -3.868571 -3.8742201 -3.9228132 -4.005784 -4.0768213 -4.1350489][-4.073597 -4.0559053 -4.0217576 -3.9432063 -3.8788502 -3.8788872 -3.905869 -3.8988574 -3.8747079 -3.8861749 -3.915942 -3.9678092 -4.0360518 -4.0956511 -4.1513028][-4.0843334 -4.0721846 -4.0494256 -3.988019 -3.929888 -3.92237 -3.9398427 -3.9244115 -3.9048724 -3.936976 -3.9846196 -4.037775 -4.0973392 -4.1534309 -4.2082028][-4.1254778 -4.1258883 -4.1164742 -4.0825014 -4.0437407 -4.0272808 -4.0291939 -4.0115118 -3.9992719 -4.029048 -4.0739737 -4.1221366 -4.1731138 -4.2230511 -4.2693596][-4.171536 -4.1849842 -4.1886148 -4.1773591 -4.1585183 -4.1434865 -4.1372366 -4.122797 -4.1130037 -4.1293297 -4.1573534 -4.1945562 -4.2316027 -4.2689 -4.3026533][-4.2182 -4.2295284 -4.2357621 -4.2366276 -4.231421 -4.2248793 -4.221067 -4.213057 -4.2056694 -4.2042236 -4.2124572 -4.2382307 -4.2647638 -4.2866678 -4.3063378][-4.2539916 -4.2582088 -4.2601686 -4.2617636 -4.2624054 -4.26209 -4.2599821 -4.2549963 -4.2459569 -4.2299752 -4.2220135 -4.2411027 -4.2623363 -4.2757134 -4.2910533][-4.2729006 -4.2709537 -4.266005 -4.2632494 -4.2629275 -4.2642727 -4.2651534 -4.2642097 -4.257937 -4.233881 -4.21119 -4.2222295 -4.2402778 -4.2532973 -4.2711625]]...]
INFO - root - 2017-12-06 03:28:07.558973: step 9410, loss = 2.07, batch loss = 2.01 (17.5 examples/sec; 0.457 sec/batch; 41h:03m:31s remains)
INFO - root - 2017-12-06 03:28:12.350877: step 9420, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.469 sec/batch; 42h:05m:31s remains)
INFO - root - 2017-12-06 03:28:17.133143: step 9430, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 0.470 sec/batch; 42h:09m:31s remains)
INFO - root - 2017-12-06 03:28:21.905072: step 9440, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.480 sec/batch; 43h:04m:57s remains)
INFO - root - 2017-12-06 03:28:26.661444: step 9450, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 0.469 sec/batch; 42h:02m:49s remains)
INFO - root - 2017-12-06 03:28:31.169751: step 9460, loss = 2.08, batch loss = 2.02 (16.5 examples/sec; 0.486 sec/batch; 43h:34m:57s remains)
INFO - root - 2017-12-06 03:28:35.872021: step 9470, loss = 2.03, batch loss = 1.98 (18.0 examples/sec; 0.445 sec/batch; 39h:54m:45s remains)
INFO - root - 2017-12-06 03:28:40.561022: step 9480, loss = 2.07, batch loss = 2.02 (17.1 examples/sec; 0.467 sec/batch; 41h:55m:03s remains)
INFO - root - 2017-12-06 03:28:45.275917: step 9490, loss = 2.04, batch loss = 1.98 (16.1 examples/sec; 0.497 sec/batch; 44h:37m:10s remains)
INFO - root - 2017-12-06 03:28:50.007349: step 9500, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.469 sec/batch; 42h:04m:53s remains)
2017-12-06 03:28:50.478604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1760273 -4.1863375 -4.2059307 -4.2144136 -4.2112823 -4.1889906 -4.1552305 -4.1286178 -4.116745 -4.1151781 -4.1199837 -4.1295905 -4.155901 -4.195652 -4.2200637][-4.2079024 -4.2153125 -4.23325 -4.2414427 -4.2418723 -4.2229657 -4.1901426 -4.1595492 -4.1437521 -4.1397772 -4.1417794 -4.1462574 -4.1674957 -4.2045355 -4.2296391][-4.2221279 -4.2299933 -4.242732 -4.2496276 -4.255177 -4.2442327 -4.2165027 -4.1885409 -4.177072 -4.1764441 -4.1788807 -4.1816649 -4.1941109 -4.22059 -4.2407041][-4.2286282 -4.2345252 -4.2405624 -4.242094 -4.2442265 -4.2377958 -4.2166471 -4.2001376 -4.2018123 -4.2096496 -4.2125168 -4.2110963 -4.2119513 -4.2261481 -4.2410517][-4.2276993 -4.2316837 -4.2328358 -4.2275043 -4.2188139 -4.2023172 -4.1792884 -4.176055 -4.1972361 -4.21716 -4.2225442 -4.2171831 -4.2096577 -4.2157116 -4.2284756][-4.2177238 -4.2206597 -4.2191429 -4.2078729 -4.1868238 -4.1530981 -4.1193457 -4.1230779 -4.1638365 -4.2004976 -4.214128 -4.2108064 -4.1998119 -4.1995077 -4.2080808][-4.2046919 -4.2043662 -4.2008371 -4.185246 -4.1543345 -4.10462 -4.059701 -4.0658326 -4.1232696 -4.177783 -4.2042456 -4.2045059 -4.1899419 -4.1838226 -4.1867647][-4.1983881 -4.1956086 -4.1923308 -4.1779857 -4.1460443 -4.0924554 -4.0405078 -4.0424571 -4.10085 -4.1602511 -4.1932249 -4.1957893 -4.1811833 -4.1730657 -4.17295][-4.1934805 -4.190032 -4.1920481 -4.1853518 -4.1580005 -4.1061997 -4.052279 -4.0493517 -4.095952 -4.1449466 -4.1776524 -4.1846104 -4.1772652 -4.1733294 -4.1748414][-4.1823063 -4.1827154 -4.1902585 -4.1921153 -4.1707282 -4.1243057 -4.0771222 -4.0757046 -4.1082096 -4.1408095 -4.167 -4.1773844 -4.1806307 -4.1831689 -4.1872458][-4.1701722 -4.1749606 -4.1850681 -4.192028 -4.1764674 -4.1376038 -4.1035285 -4.1060381 -4.1284742 -4.1463666 -4.1639209 -4.1777692 -4.1912556 -4.19879 -4.2022133][-4.1666584 -4.1770749 -4.1904693 -4.1977873 -4.183435 -4.1477547 -4.1232963 -4.12887 -4.1471572 -4.1584263 -4.1704664 -4.1860991 -4.205564 -4.2137604 -4.2141633][-4.1688128 -4.1864848 -4.2034616 -4.20875 -4.1936316 -4.1593757 -4.1425366 -4.1525531 -4.1722794 -4.1808705 -4.1879635 -4.2004523 -4.2178307 -4.2234073 -4.2197614][-4.1701503 -4.1899805 -4.2089581 -4.2153759 -4.2035589 -4.1787906 -4.1726828 -4.1862025 -4.2036395 -4.2065935 -4.2064452 -4.2112594 -4.219274 -4.2181144 -4.2113714][-4.1702189 -4.1891761 -4.2094259 -4.2190242 -4.212707 -4.2006693 -4.2048984 -4.219233 -4.2281747 -4.2229371 -4.2155681 -4.2097654 -4.2035561 -4.1952615 -4.1883726]]...]
INFO - root - 2017-12-06 03:28:55.230334: step 9510, loss = 2.04, batch loss = 1.99 (17.5 examples/sec; 0.457 sec/batch; 41h:02m:10s remains)
INFO - root - 2017-12-06 03:28:59.905703: step 9520, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.470 sec/batch; 42h:10m:50s remains)
INFO - root - 2017-12-06 03:29:04.467120: step 9530, loss = 2.06, batch loss = 2.00 (17.5 examples/sec; 0.457 sec/batch; 41h:00m:44s remains)
INFO - root - 2017-12-06 03:29:09.235077: step 9540, loss = 2.06, batch loss = 2.01 (16.8 examples/sec; 0.477 sec/batch; 42h:49m:09s remains)
INFO - root - 2017-12-06 03:29:13.719243: step 9550, loss = 2.06, batch loss = 2.00 (21.7 examples/sec; 0.369 sec/batch; 33h:05m:51s remains)
INFO - root - 2017-12-06 03:29:18.378460: step 9560, loss = 2.09, batch loss = 2.03 (17.7 examples/sec; 0.453 sec/batch; 40h:39m:06s remains)
INFO - root - 2017-12-06 03:29:23.152156: step 9570, loss = 2.05, batch loss = 1.99 (17.2 examples/sec; 0.465 sec/batch; 41h:43m:46s remains)
INFO - root - 2017-12-06 03:29:27.770311: step 9580, loss = 2.03, batch loss = 1.98 (17.0 examples/sec; 0.469 sec/batch; 42h:06m:39s remains)
INFO - root - 2017-12-06 03:29:32.489377: step 9590, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 0.481 sec/batch; 43h:09m:03s remains)
INFO - root - 2017-12-06 03:29:37.296077: step 9600, loss = 2.07, batch loss = 2.01 (16.3 examples/sec; 0.492 sec/batch; 44h:08m:29s remains)
2017-12-06 03:29:37.767602: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3178778 -4.3228388 -4.3224244 -4.3069892 -4.2723427 -4.229259 -4.1877747 -4.1669612 -4.1555943 -4.144258 -4.1349893 -4.1297822 -4.118453 -4.0941143 -4.0674071][-4.2970767 -4.3049064 -4.3104672 -4.296782 -4.258729 -4.2145896 -4.1748776 -4.1590252 -4.1556554 -4.1485453 -4.1418962 -4.14532 -4.1430707 -4.12034 -4.0897436][-4.2620335 -4.2740269 -4.2865658 -4.2786722 -4.24459 -4.2051611 -4.1764259 -4.1719 -4.1774 -4.1718507 -4.1673894 -4.1763396 -4.179255 -4.1629486 -4.1383276][-4.2071042 -4.223834 -4.2464652 -4.2503052 -4.2281947 -4.2009449 -4.185986 -4.1905508 -4.1990309 -4.19494 -4.1930766 -4.2054234 -4.2135057 -4.2051435 -4.1901145][-4.1350613 -4.1580725 -4.1942668 -4.2172022 -4.2109838 -4.1975389 -4.1904063 -4.1962867 -4.2024541 -4.2022204 -4.2079515 -4.2260971 -4.2381721 -4.2366681 -4.2280393][-4.0830035 -4.1106138 -4.1531315 -4.1864924 -4.1938224 -4.1884704 -4.1798763 -4.1788082 -4.1798511 -4.1846852 -4.2054067 -4.2368026 -4.2568974 -4.2632504 -4.2607841][-4.07632 -4.096127 -4.1271691 -4.15501 -4.1669927 -4.1620469 -4.1473413 -4.1365004 -4.1330709 -4.1469889 -4.1897426 -4.237906 -4.2695565 -4.28216 -4.283391][-4.0825772 -4.0907149 -4.1076889 -4.1191807 -4.1208634 -4.1066852 -4.085022 -4.0685825 -4.0685029 -4.0995007 -4.1619921 -4.2253847 -4.2686424 -4.2881465 -4.2952652][-4.0875854 -4.0860176 -4.0895162 -4.0839972 -4.0671582 -4.0420666 -4.025506 -4.0165009 -4.0249734 -4.0710187 -4.1447768 -4.2101679 -4.2571206 -4.28114 -4.2912865][-4.1039686 -4.0998354 -4.0958428 -4.0832152 -4.0583096 -4.0340471 -4.0320144 -4.0307031 -4.0411797 -4.0895448 -4.1597223 -4.2114363 -4.2497287 -4.2716346 -4.2808576][-4.1232347 -4.1215234 -4.1175003 -4.1056919 -4.0895905 -4.0748191 -4.0806565 -4.0841732 -4.0929861 -4.1346679 -4.1921911 -4.2288313 -4.2548809 -4.2696366 -4.2752538][-4.1512094 -4.1485109 -4.1455908 -4.1392612 -4.1351504 -4.1324358 -4.1399884 -4.1475759 -4.1560631 -4.1873126 -4.2267051 -4.2487416 -4.2652669 -4.2728257 -4.2728949][-4.1821938 -4.1797 -4.179677 -4.1817684 -4.188199 -4.1952085 -4.2031407 -4.2129884 -4.2216158 -4.2402449 -4.2590661 -4.2693462 -4.2780046 -4.2794318 -4.2734146][-4.2191586 -4.2170649 -4.2198186 -4.2283807 -4.24111 -4.25074 -4.2563033 -4.2670622 -4.2743778 -4.2813091 -4.2870393 -4.2879124 -4.2885761 -4.2838707 -4.2704244][-4.249423 -4.2461238 -4.2492471 -4.2587142 -4.271594 -4.2803049 -4.2832117 -4.2905622 -4.2945666 -4.2937603 -4.2936072 -4.2909279 -4.2870684 -4.2785072 -4.2594881]]...]
INFO - root - 2017-12-06 03:29:42.504199: step 9610, loss = 2.06, batch loss = 2.00 (17.2 examples/sec; 0.464 sec/batch; 41h:36m:45s remains)
INFO - root - 2017-12-06 03:29:47.174448: step 9620, loss = 2.09, batch loss = 2.03 (16.3 examples/sec; 0.491 sec/batch; 44h:03m:41s remains)
INFO - root - 2017-12-06 03:29:51.993847: step 9630, loss = 2.06, batch loss = 2.00 (17.7 examples/sec; 0.452 sec/batch; 40h:31m:38s remains)
INFO - root - 2017-12-06 03:29:56.691464: step 9640, loss = 2.07, batch loss = 2.02 (17.0 examples/sec; 0.470 sec/batch; 42h:10m:05s remains)
INFO - root - 2017-12-06 03:30:01.201434: step 9650, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.474 sec/batch; 42h:27m:55s remains)
INFO - root - 2017-12-06 03:30:05.870170: step 9660, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.474 sec/batch; 42h:30m:48s remains)
INFO - root - 2017-12-06 03:30:10.558758: step 9670, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.450 sec/batch; 40h:19m:09s remains)
INFO - root - 2017-12-06 03:30:15.215505: step 9680, loss = 2.07, batch loss = 2.02 (16.6 examples/sec; 0.482 sec/batch; 43h:15m:23s remains)
INFO - root - 2017-12-06 03:30:19.944929: step 9690, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.459 sec/batch; 41h:10m:30s remains)
INFO - root - 2017-12-06 03:30:24.660568: step 9700, loss = 2.07, batch loss = 2.01 (17.4 examples/sec; 0.460 sec/batch; 41h:15m:37s remains)
2017-12-06 03:30:25.123239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2487049 -4.2528005 -4.2467208 -4.246151 -4.2508349 -4.2558513 -4.2464733 -4.2353425 -4.2405672 -4.2553072 -4.2729 -4.2934523 -4.3124318 -4.3219132 -4.3244472][-4.1937637 -4.1976666 -4.188179 -4.1831889 -4.1865616 -4.1952515 -4.1825171 -4.1612682 -4.1698966 -4.1999445 -4.2320852 -4.2662563 -4.2956271 -4.3123779 -4.3183761][-4.1642842 -4.1666017 -4.1507926 -4.1372724 -4.1363964 -4.144351 -4.1201587 -4.0820475 -4.0937767 -4.1416259 -4.18747 -4.2343931 -4.2724576 -4.2973843 -4.30816][-4.1451368 -4.1438937 -4.1202731 -4.1003146 -4.0944214 -4.0972466 -4.0548978 -3.9924529 -4.0056853 -4.0732417 -4.13714 -4.1980519 -4.245389 -4.2780285 -4.29467][-4.1366024 -4.1332526 -4.1052976 -4.0795379 -4.0667381 -4.0581374 -3.9897463 -3.8878946 -3.9037752 -4.0041909 -4.0926089 -4.1664491 -4.2188926 -4.2589092 -4.284236][-4.1585808 -4.1475182 -4.1184258 -4.0897574 -4.0659232 -4.0307164 -3.9147575 -3.7589452 -3.7990198 -3.9540296 -4.0688553 -4.1525974 -4.2085285 -4.2532582 -4.2837081][-4.1863232 -4.1696372 -4.148869 -4.1208792 -4.0777316 -4.0043974 -3.8319969 -3.6219094 -3.702621 -3.9144456 -4.0520682 -4.1415329 -4.201673 -4.2535825 -4.286551][-4.203258 -4.1918521 -4.1822848 -4.1580415 -4.1040206 -4.0164666 -3.8433986 -3.6513655 -3.7372832 -3.9422832 -4.070961 -4.1515818 -4.2099519 -4.264183 -4.2941766][-4.20632 -4.2049327 -4.2109694 -4.1962261 -4.1481895 -4.0671329 -3.9280796 -3.7818959 -3.84708 -4.0088549 -4.10628 -4.1690474 -4.2249708 -4.2769394 -4.3022342][-4.1938815 -4.1989222 -4.21681 -4.2126102 -4.176053 -4.1097326 -3.9981434 -3.8850658 -3.9323778 -4.0626535 -4.1377974 -4.1865473 -4.2373185 -4.2853341 -4.3062539][-4.1798029 -4.190474 -4.2152076 -4.2225909 -4.1998329 -4.15182 -4.0682082 -3.9815152 -4.0139174 -4.1150589 -4.1728587 -4.209919 -4.2544985 -4.2955427 -4.3107643][-4.1822152 -4.1960359 -4.2266822 -4.2430315 -4.2350421 -4.2051682 -4.1482191 -4.0869946 -4.1038065 -4.1719837 -4.212008 -4.23768 -4.2720032 -4.3032169 -4.315145][-4.1948757 -4.2118134 -4.2433848 -4.2617888 -4.25999 -4.2437773 -4.2085319 -4.1652231 -4.1696906 -4.2144446 -4.2457767 -4.2631469 -4.2861466 -4.3083463 -4.3182626][-4.2034945 -4.2197208 -4.252234 -4.27506 -4.2794538 -4.27435 -4.252686 -4.2213268 -4.219162 -4.2466707 -4.269341 -4.2769489 -4.2896576 -4.3066874 -4.3175321][-4.2420764 -4.2567072 -4.2831297 -4.300777 -4.3080311 -4.3079643 -4.29167 -4.2670388 -4.2591968 -4.2720647 -4.2836013 -4.2833009 -4.287714 -4.2999411 -4.3118219]]...]
INFO - root - 2017-12-06 03:30:29.894106: step 9710, loss = 2.05, batch loss = 1.99 (16.9 examples/sec; 0.475 sec/batch; 42h:33m:54s remains)
INFO - root - 2017-12-06 03:30:34.651680: step 9720, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 0.487 sec/batch; 43h:41m:54s remains)
INFO - root - 2017-12-06 03:30:39.333199: step 9730, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 0.464 sec/batch; 41h:33m:48s remains)
INFO - root - 2017-12-06 03:30:44.096391: step 9740, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 0.512 sec/batch; 45h:54m:28s remains)
INFO - root - 2017-12-06 03:30:48.670144: step 9750, loss = 2.07, batch loss = 2.02 (17.3 examples/sec; 0.462 sec/batch; 41h:22m:39s remains)
INFO - root - 2017-12-06 03:30:53.394374: step 9760, loss = 2.03, batch loss = 1.97 (16.9 examples/sec; 0.473 sec/batch; 42h:22m:56s remains)
INFO - root - 2017-12-06 03:30:58.168602: step 9770, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 0.474 sec/batch; 42h:27m:10s remains)
INFO - root - 2017-12-06 03:31:02.824346: step 9780, loss = 2.03, batch loss = 1.97 (17.0 examples/sec; 0.471 sec/batch; 42h:11m:10s remains)
INFO - root - 2017-12-06 03:31:07.549097: step 9790, loss = 2.05, batch loss = 1.99 (16.6 examples/sec; 0.481 sec/batch; 43h:05m:59s remains)
INFO - root - 2017-12-06 03:31:12.181667: step 9800, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.456 sec/batch; 40h:50m:35s remains)
2017-12-06 03:31:12.639842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3141894 -4.316246 -4.3163419 -4.3156657 -4.3144689 -4.3137622 -4.3149333 -4.3169413 -4.3188214 -4.3186088 -4.3160691 -4.3100524 -4.304543 -4.3013654 -4.2951293][-4.3015084 -4.3052912 -4.304491 -4.3020134 -4.2993574 -4.2985497 -4.30102 -4.3056159 -4.3100572 -4.3106565 -4.3071694 -4.3008595 -4.295836 -4.2905989 -4.2842455][-4.2949414 -4.2979665 -4.2944589 -4.2886629 -4.2832308 -4.2809467 -4.2854939 -4.2945995 -4.3031507 -4.3062634 -4.3046503 -4.2999678 -4.2938128 -4.2851248 -4.2772303][-4.2698126 -4.2705975 -4.2644978 -4.2549062 -4.245492 -4.2395921 -4.2444057 -4.2565842 -4.2680268 -4.2739687 -4.2777643 -4.2803903 -4.2785316 -4.2726517 -4.2649512][-4.2400589 -4.240818 -4.2307992 -4.2140594 -4.1965489 -4.1841879 -4.1871643 -4.200809 -4.2148623 -4.2248797 -4.2364726 -4.2496552 -4.2544312 -4.2528014 -4.2468796][-4.1920776 -4.1936278 -4.179213 -4.1532726 -4.124475 -4.102746 -4.0996494 -4.1100807 -4.1243205 -4.1401734 -4.164113 -4.1917505 -4.207902 -4.2177143 -4.2198997][-4.1530876 -4.1572356 -4.1430373 -4.1139278 -4.079289 -4.0500927 -4.0361667 -4.0342007 -4.0408454 -4.05867 -4.0950208 -4.137044 -4.1643772 -4.1822352 -4.1900477][-4.19479 -4.1996679 -4.1886411 -4.1646652 -4.1377606 -4.1135654 -4.0945811 -4.0805798 -4.0764666 -4.0873189 -4.1205931 -4.16133 -4.1861186 -4.2004614 -4.2062454][-4.2512083 -4.249897 -4.2410312 -4.2263103 -4.2132845 -4.20333 -4.1924548 -4.1793847 -4.1694522 -4.1687937 -4.1853647 -4.2110147 -4.2265129 -4.2341089 -4.2349868][-4.2678704 -4.2628169 -4.2553949 -4.2470737 -4.2428617 -4.2433891 -4.2415023 -4.233912 -4.2244716 -4.2169971 -4.2192 -4.230473 -4.2376461 -4.24107 -4.2409682][-4.2713881 -4.2670307 -4.2616358 -4.2548442 -4.2517915 -4.2541728 -4.2559175 -4.251256 -4.2451277 -4.2387161 -4.2364025 -4.2394333 -4.2405758 -4.2419171 -4.2413607][-4.2849264 -4.2817187 -4.2767596 -4.2677679 -4.258028 -4.25198 -4.249464 -4.244524 -4.239851 -4.2359719 -4.236393 -4.2381616 -4.2368736 -4.2376966 -4.2381387][-4.298594 -4.2954917 -4.2899475 -4.2787509 -4.2633629 -4.2477379 -4.2371864 -4.2300634 -4.225297 -4.2224388 -4.2243738 -4.2261806 -4.2239218 -4.2251444 -4.2281046][-4.311327 -4.3093491 -4.303124 -4.29134 -4.2741928 -4.2530241 -4.2350764 -4.2227387 -4.2142367 -4.2089863 -4.2104154 -4.2095594 -4.2035213 -4.2040896 -4.210361][-4.3173881 -4.3181229 -4.3139386 -4.3044524 -4.2904077 -4.2711291 -4.2522492 -4.2362247 -4.223362 -4.2142482 -4.2117758 -4.2078447 -4.1989808 -4.1974144 -4.204031]]...]
INFO - root - 2017-12-06 03:31:17.344852: step 9810, loss = 2.07, batch loss = 2.02 (16.9 examples/sec; 0.475 sec/batch; 42h:32m:10s remains)
INFO - root - 2017-12-06 03:31:22.058564: step 9820, loss = 2.04, batch loss = 1.98 (16.9 examples/sec; 0.473 sec/batch; 42h:22m:36s remains)
INFO - root - 2017-12-06 03:31:26.838715: step 9830, loss = 2.04, batch loss = 1.99 (17.0 examples/sec; 0.471 sec/batch; 42h:12m:24s remains)
INFO - root - 2017-12-06 03:31:31.261499: step 9840, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.485 sec/batch; 43h:27m:59s remains)
INFO - root - 2017-12-06 03:31:36.001063: step 9850, loss = 2.04, batch loss = 1.98 (17.1 examples/sec; 0.468 sec/batch; 41h:57m:21s remains)
INFO - root - 2017-12-06 03:31:40.742124: step 9860, loss = 2.05, batch loss = 1.99 (17.0 examples/sec; 0.471 sec/batch; 42h:12m:04s remains)
INFO - root - 2017-12-06 03:31:45.488572: step 9870, loss = 2.10, batch loss = 2.04 (16.4 examples/sec; 0.489 sec/batch; 43h:47m:45s remains)
INFO - root - 2017-12-06 03:31:50.234840: step 9880, loss = 2.05, batch loss = 1.99 (17.1 examples/sec; 0.467 sec/batch; 41h:51m:43s remains)
INFO - root - 2017-12-06 03:31:54.946992: step 9890, loss = 2.04, batch loss = 1.98 (16.5 examples/sec; 0.484 sec/batch; 43h:21m:33s remains)
INFO - root - 2017-12-06 03:31:59.694872: step 9900, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.466 sec/batch; 41h:47m:22s remains)
2017-12-06 03:32:00.175819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2778058 -4.2683592 -4.2651024 -4.2691755 -4.2780747 -4.2888803 -4.2939682 -4.2941289 -4.296073 -4.2976928 -4.299933 -4.3037357 -4.3072033 -4.3060055 -4.3013706][-4.2594709 -4.2462044 -4.2396383 -4.24149 -4.2494583 -4.2632027 -4.271421 -4.273747 -4.2776227 -4.28179 -4.2861633 -4.2908773 -4.2942581 -4.2927332 -4.2878776][-4.2461658 -4.2300215 -4.2178097 -4.2113519 -4.2121949 -4.2254634 -4.2343979 -4.238266 -4.244411 -4.2531061 -4.2600627 -4.26619 -4.2711883 -4.2721405 -4.2706909][-4.2370887 -4.2197146 -4.2029967 -4.1870027 -4.176157 -4.1808138 -4.1828418 -4.1858306 -4.1951876 -4.2073903 -4.2186084 -4.2322726 -4.2471371 -4.2544832 -4.2573667][-4.2307076 -4.2110314 -4.1889682 -4.1666059 -4.1492538 -4.146349 -4.1350255 -4.1290188 -4.1392679 -4.1550198 -4.1752625 -4.2023311 -4.2307482 -4.2440429 -4.2477317][-4.223516 -4.1977744 -4.1688285 -4.1398993 -4.1176405 -4.1028352 -4.0669122 -4.0459433 -4.060348 -4.0915356 -4.1315274 -4.1793232 -4.2177129 -4.2337003 -4.2367988][-4.2270355 -4.1959586 -4.1584883 -4.1182919 -4.0881519 -4.0570908 -3.9966955 -3.9622743 -3.9896779 -4.0445104 -4.1074924 -4.1729422 -4.2171793 -4.2334018 -4.2351961][-4.2356834 -4.2018147 -4.1591005 -4.1142654 -4.0783873 -4.0340719 -3.958595 -3.9162016 -3.9605219 -4.0333982 -4.1046581 -4.1753392 -4.22099 -4.2395477 -4.2417164][-4.2506423 -4.2199335 -4.1802316 -4.1386161 -4.1064577 -4.0620508 -3.9872026 -3.9474061 -3.9957225 -4.0636792 -4.1246977 -4.1836171 -4.2251458 -4.2479334 -4.2536054][-4.2599769 -4.2345223 -4.2012672 -4.1706734 -4.1557908 -4.1318793 -4.07751 -4.0465164 -4.0803037 -4.1221156 -4.158258 -4.1954851 -4.2280264 -4.2536154 -4.263226][-4.2587509 -4.2340379 -4.204607 -4.1844969 -4.1858172 -4.18107 -4.150526 -4.1276984 -4.1474228 -4.1713171 -4.1894116 -4.2122498 -4.2370167 -4.2610073 -4.2706933][-4.2652779 -4.2420006 -4.2179956 -4.2039967 -4.2107105 -4.2126307 -4.1980338 -4.1841078 -4.1951218 -4.208251 -4.2170215 -4.2295327 -4.24817 -4.2667723 -4.2763195][-4.2795796 -4.260736 -4.2430434 -4.2335663 -4.2370043 -4.2374177 -4.2312927 -4.2241383 -4.2287121 -4.2319093 -4.2336621 -4.243154 -4.2608409 -4.2752261 -4.2839003][-4.2938638 -4.2791734 -4.267313 -4.2612495 -4.2605619 -4.2601523 -4.2558627 -4.2518682 -4.250896 -4.2454996 -4.2451944 -4.2563267 -4.2732778 -4.285027 -4.29201][-4.2989721 -4.2876358 -4.2792087 -4.27575 -4.2760816 -4.276989 -4.2760634 -4.2755527 -4.2718105 -4.2616515 -4.2609854 -4.2724037 -4.2863288 -4.2957516 -4.3000822]]...]
INFO - root - 2017-12-06 03:32:04.903725: step 9910, loss = 2.06, batch loss = 2.01 (17.4 examples/sec; 0.460 sec/batch; 41h:13m:56s remains)
INFO - root - 2017-12-06 03:32:09.610646: step 9920, loss = 2.07, batch loss = 2.01 (17.6 examples/sec; 0.456 sec/batch; 40h:49m:10s remains)
INFO - root - 2017-12-06 03:32:14.298241: step 9930, loss = 2.08, batch loss = 2.03 (16.1 examples/sec; 0.495 sec/batch; 44h:23m:28s remains)
INFO - root - 2017-12-06 03:32:18.908960: step 9940, loss = 2.05, batch loss = 2.00 (17.4 examples/sec; 0.459 sec/batch; 41h:10m:14s remains)
INFO - root - 2017-12-06 03:32:23.616326: step 9950, loss = 2.07, batch loss = 2.02 (16.6 examples/sec; 0.482 sec/batch; 43h:11m:48s remains)
INFO - root - 2017-12-06 03:32:28.450711: step 9960, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 0.475 sec/batch; 42h:35m:18s remains)
INFO - root - 2017-12-06 03:32:33.060002: step 9970, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 0.471 sec/batch; 42h:12m:23s remains)
INFO - root - 2017-12-06 03:32:37.798361: step 9980, loss = 2.05, batch loss = 1.99 (16.4 examples/sec; 0.489 sec/batch; 43h:49m:58s remains)
INFO - root - 2017-12-06 03:32:42.591496: step 9990, loss = 2.07, batch loss = 2.02 (17.1 examples/sec; 0.467 sec/batch; 41h:50m:55s remains)
INFO - root - 2017-12-06 03:32:47.262981: step 10000, loss = 2.09, batch loss = 2.04 (17.2 examples/sec; 0.465 sec/batch; 41h:41m:18s remains)
2017-12-06 03:32:47.737883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2664442 -4.2489967 -4.2342982 -4.2276011 -4.2263618 -4.2267 -4.2290292 -4.2362046 -4.2459717 -4.253046 -4.2610474 -4.2728257 -4.2845511 -4.2937803 -4.2986031][-4.2437372 -4.2157874 -4.1885157 -4.17327 -4.1662207 -4.1626616 -4.1666241 -4.1801739 -4.1955929 -4.2055469 -4.2177954 -4.237083 -4.2592144 -4.2780743 -4.2880788][-4.224031 -4.1837687 -4.1440492 -4.1226878 -4.1112828 -4.1031513 -4.1031861 -4.1209512 -4.143909 -4.1563272 -4.1698761 -4.1930046 -4.2226706 -4.2513318 -4.2692342][-4.2104006 -4.1602325 -4.1134968 -4.0885983 -4.0713406 -4.0501447 -4.0347381 -4.0532851 -4.0910482 -4.1193891 -4.140729 -4.1681733 -4.2047491 -4.2392473 -4.2619019][-4.2048607 -4.148303 -4.0938272 -4.0590677 -4.0282187 -3.9790027 -3.9289303 -3.939816 -4.0062351 -4.0666122 -4.1098156 -4.1496849 -4.1981297 -4.2406774 -4.2667179][-4.2066722 -4.1493797 -4.0877757 -4.0382829 -3.9835191 -3.8952479 -3.7892373 -3.7713625 -3.8770823 -3.9843385 -4.0600958 -4.1161656 -4.174293 -4.2258711 -4.2591205][-4.2190056 -4.1698332 -4.1090255 -4.0465674 -3.9682598 -3.8467031 -3.687634 -3.6262832 -3.7634661 -3.9139595 -4.023438 -4.0962753 -4.1600056 -4.2160487 -4.2507272][-4.2292967 -4.1910019 -4.1417146 -4.0869184 -4.0174932 -3.9159894 -3.7859638 -3.7281022 -3.8296194 -3.9576535 -4.0577111 -4.1285739 -4.1876955 -4.2354984 -4.2596221][-4.2310667 -4.1975112 -4.156528 -4.1142154 -4.0723171 -4.0151181 -3.9418478 -3.9095633 -3.9733052 -4.0612464 -4.1364417 -4.1926775 -4.2343006 -4.2643938 -4.2760682][-4.2253757 -4.1920061 -4.1509233 -4.1151247 -4.0911 -4.067975 -4.0416937 -4.0372019 -4.0869007 -4.1496906 -4.2058473 -4.2482119 -4.2759805 -4.2902246 -4.2904553][-4.2304192 -4.2029009 -4.1660552 -4.1375313 -4.1238818 -4.1216121 -4.1219516 -4.13425 -4.1776876 -4.2223387 -4.263082 -4.2962241 -4.3138661 -4.315516 -4.3045788][-4.2543674 -4.239604 -4.2145319 -4.1938648 -4.1878853 -4.1920753 -4.1982932 -4.2084551 -4.2419181 -4.2732263 -4.29816 -4.3188658 -4.3278308 -4.3229923 -4.3077478][-4.2805195 -4.2768188 -4.2614417 -4.2457442 -4.2417846 -4.2450047 -4.2525511 -4.2596383 -4.2833233 -4.3052106 -4.3196225 -4.3297763 -4.329772 -4.3209906 -4.3069639][-4.3008971 -4.3006024 -4.2924137 -4.2818751 -4.2770929 -4.2785015 -4.2852335 -4.291903 -4.310576 -4.3255744 -4.3313379 -4.3338466 -4.329318 -4.3199444 -4.310081][-4.3117323 -4.3109 -4.307415 -4.3021097 -4.2991552 -4.2981 -4.3012819 -4.3073521 -4.3203869 -4.3307538 -4.3322926 -4.3306961 -4.3247108 -4.3167424 -4.3105979]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 03:32:52.851443: step 10010, loss = 2.05, batch loss = 1.99 (17.2 examples/sec; 0.466 sec/batch; 41h:45m:24s remains)
INFO - root - 2017-12-06 03:32:57.551717: step 10020, loss = 2.07, batch loss = 2.01 (16.0 examples/sec; 0.501 sec/batch; 44h:53m:09s remains)
INFO - root - 2017-12-06 03:33:02.134595: step 10030, loss = 2.07, batch loss = 2.01 (15.7 examples/sec; 0.509 sec/batch; 45h:35m:34s remains)
INFO - root - 2017-12-06 03:33:06.844171: step 10040, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.467 sec/batch; 41h:50m:35s remains)
INFO - root - 2017-12-06 03:33:11.508501: step 10050, loss = 2.04, batch loss = 1.98 (16.8 examples/sec; 0.476 sec/batch; 42h:39m:19s remains)
INFO - root - 2017-12-06 03:33:16.252936: step 10060, loss = 2.08, batch loss = 2.02 (17.2 examples/sec; 0.464 sec/batch; 41h:33m:28s remains)
INFO - root - 2017-12-06 03:33:19.441673: step 10070, loss = 2.09, batch loss = 2.03 (35.5 examples/sec; 0.225 sec/batch; 20h:09m:58s remains)
INFO - root - 2017-12-06 03:33:21.746928: step 10080, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.236 sec/batch; 21h:10m:04s remains)
INFO - root - 2017-12-06 03:33:24.103594: step 10090, loss = 2.05, batch loss = 1.99 (33.2 examples/sec; 0.241 sec/batch; 21h:33m:19s remains)
INFO - root - 2017-12-06 03:33:26.442516: step 10100, loss = 2.07, batch loss = 2.01 (33.7 examples/sec; 0.238 sec/batch; 21h:16m:22s remains)
2017-12-06 03:33:26.735881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2147608 -4.2119203 -4.2250071 -4.2525678 -4.2789316 -4.3013153 -4.3157496 -4.3151565 -4.3032012 -4.284678 -4.2713757 -4.271369 -4.27694 -4.2818055 -4.2853875][-4.2060461 -4.19201 -4.1925354 -4.2123237 -4.237885 -4.2640114 -4.2872887 -4.2957706 -4.2887144 -4.2709765 -4.2560477 -4.2514329 -4.2518086 -4.2549667 -4.2624588][-4.1987591 -4.1737452 -4.1640081 -4.173491 -4.19229 -4.2129893 -4.2359028 -4.2507877 -4.2520442 -4.2423654 -4.2304592 -4.2227993 -4.2196145 -4.2240257 -4.2380018][-4.1880693 -4.151484 -4.1312122 -4.1267815 -4.1298118 -4.1350427 -4.1503811 -4.1723161 -4.1923575 -4.2029042 -4.2028427 -4.1988344 -4.1990023 -4.209631 -4.2262163][-4.1811943 -4.1363916 -4.1041083 -4.0817204 -4.0578866 -4.03174 -4.02239 -4.04682 -4.0954585 -4.1408839 -4.1669121 -4.1791468 -4.1932564 -4.2113528 -4.2240372][-4.1832681 -4.1362872 -4.0933928 -4.0494084 -3.9910412 -3.9207187 -3.8726091 -3.896862 -3.983557 -4.0749617 -4.1362553 -4.1719427 -4.2011313 -4.2246776 -4.2311625][-4.1894841 -4.1483846 -4.1053934 -4.049017 -3.9617889 -3.8484457 -3.7544382 -3.766551 -3.8775959 -4.0017715 -4.0949965 -4.1562634 -4.2004495 -4.2304378 -4.23835][-4.1907649 -4.161293 -4.1299539 -4.0833874 -4.0031848 -3.8930614 -3.7948322 -3.7864585 -3.8707561 -3.9796898 -4.0738106 -4.1434131 -4.1919069 -4.2249908 -4.2380762][-4.1732111 -4.1572957 -4.1449323 -4.1242542 -4.0752463 -3.999629 -3.9274278 -3.9087369 -3.9533203 -4.0268073 -4.0956655 -4.1489034 -4.1838369 -4.2052512 -4.215735][-4.1429882 -4.1421728 -4.15304 -4.1650534 -4.1516147 -4.11429 -4.0722108 -4.0554166 -4.0743833 -4.1123681 -4.1440444 -4.1665511 -4.1776795 -4.1782384 -4.1746416][-4.1266308 -4.1346064 -4.1611729 -4.1967573 -4.2103167 -4.2024493 -4.1857209 -4.1752424 -4.1799569 -4.1916728 -4.1920137 -4.1851797 -4.1725717 -4.15071 -4.1287546][-4.1418939 -4.1564236 -4.1911635 -4.2352519 -4.2581429 -4.2617331 -4.2534733 -4.2419581 -4.2349005 -4.2264738 -4.2069683 -4.1842117 -4.1615076 -4.1330504 -4.1057234][-4.1905718 -4.206717 -4.2400193 -4.2794333 -4.296174 -4.2943387 -4.2802181 -4.2591853 -4.2377009 -4.2117867 -4.1818638 -4.15586 -4.1368217 -4.1213689 -4.1073956][-4.2488275 -4.2633252 -4.2909274 -4.3176379 -4.320508 -4.3084192 -4.2861471 -4.2560997 -4.2203126 -4.1790605 -4.1432543 -4.1214595 -4.1140089 -4.1172318 -4.124558][-4.2901964 -4.3033834 -4.3240433 -4.3376169 -4.3325133 -4.32051 -4.30126 -4.2713208 -4.230021 -4.182869 -4.1466842 -4.1299038 -4.1308174 -4.145896 -4.1671171]]...]
INFO - root - 2017-12-06 03:33:29.066877: step 10110, loss = 2.06, batch loss = 2.01 (34.1 examples/sec; 0.235 sec/batch; 21h:00m:33s remains)
INFO - root - 2017-12-06 03:33:31.369081: step 10120, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 20h:40m:27s remains)
INFO - root - 2017-12-06 03:33:33.636254: step 10130, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.230 sec/batch; 20h:37m:41s remains)
INFO - root - 2017-12-06 03:33:35.921636: step 10140, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 20h:18m:10s remains)
INFO - root - 2017-12-06 03:33:38.230362: step 10150, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.230 sec/batch; 20h:37m:20s remains)
INFO - root - 2017-12-06 03:33:40.547122: step 10160, loss = 2.03, batch loss = 1.98 (33.9 examples/sec; 0.236 sec/batch; 21h:08m:05s remains)
INFO - root - 2017-12-06 03:33:42.899670: step 10170, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:56m:15s remains)
INFO - root - 2017-12-06 03:33:45.204941: step 10180, loss = 2.07, batch loss = 2.02 (35.6 examples/sec; 0.224 sec/batch; 20h:05m:45s remains)
INFO - root - 2017-12-06 03:33:47.529874: step 10190, loss = 2.06, batch loss = 2.00 (33.6 examples/sec; 0.238 sec/batch; 21h:19m:32s remains)
INFO - root - 2017-12-06 03:33:49.839214: step 10200, loss = 2.07, batch loss = 2.02 (35.3 examples/sec; 0.227 sec/batch; 20h:17m:28s remains)
2017-12-06 03:33:50.137680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2861266 -4.2867942 -4.283783 -4.2826519 -4.2872372 -4.2980084 -4.30908 -4.318882 -4.3251042 -4.3227558 -4.31662 -4.3143849 -4.3147559 -4.3168111 -4.3189025][-4.3102984 -4.3143311 -4.3106475 -4.3059921 -4.3080974 -4.3146515 -4.3189678 -4.3203969 -4.319293 -4.3139057 -4.3092241 -4.3084664 -4.3100748 -4.31418 -4.3180881][-4.3179955 -4.3241735 -4.319766 -4.312592 -4.3107123 -4.3114204 -4.3086157 -4.3008981 -4.2922873 -4.2877092 -4.2891097 -4.293148 -4.2990766 -4.3077092 -4.3138957][-4.309979 -4.3190804 -4.3151646 -4.307189 -4.3025851 -4.2986298 -4.28985 -4.2718468 -4.2544665 -4.252099 -4.2630725 -4.2739224 -4.2845135 -4.2945943 -4.2983532][-4.3011928 -4.3131118 -4.3098111 -4.300828 -4.292943 -4.2848663 -4.2689633 -4.2392807 -4.2127452 -4.213644 -4.2357421 -4.2524538 -4.263916 -4.2710948 -4.270504][-4.2957206 -4.3098884 -4.3067613 -4.2941566 -4.2790666 -4.2595382 -4.2285037 -4.1825666 -4.1500573 -4.1628838 -4.2031684 -4.2323122 -4.2490225 -4.2537479 -4.2481651][-4.2928567 -4.3076687 -4.3025784 -4.2799344 -4.2485528 -4.2085109 -4.1540532 -4.08717 -4.0544128 -4.0911946 -4.1593857 -4.2112722 -4.2416768 -4.2488818 -4.2400346][-4.2950459 -4.3071761 -4.2957125 -4.2600837 -4.212182 -4.1533718 -4.078084 -3.9944582 -3.9703512 -4.0354185 -4.1287823 -4.2001557 -4.2410111 -4.2500858 -4.2383132][-4.2964249 -4.3062143 -4.2891827 -4.2457924 -4.1896987 -4.1275239 -4.0535717 -3.9793873 -3.9746222 -4.049644 -4.1446233 -4.215817 -4.2528067 -4.2540975 -4.2363567][-4.3056965 -4.3114223 -4.2914333 -4.2465925 -4.1935492 -4.143446 -4.092989 -4.0524511 -4.0632796 -4.122457 -4.1950097 -4.2477393 -4.2674813 -4.2543774 -4.2309322][-4.3249016 -4.3265805 -4.3056459 -4.2651381 -4.2209368 -4.1858153 -4.1601286 -4.1467872 -4.1618 -4.1995258 -4.2448311 -4.2761345 -4.2796931 -4.2579193 -4.2343388][-4.3427014 -4.3433042 -4.3264155 -4.29248 -4.2571378 -4.2339153 -4.2244468 -4.2249336 -4.2368646 -4.2584267 -4.2836771 -4.3007722 -4.2974834 -4.2770681 -4.2606778][-4.3484788 -4.3502293 -4.3388638 -4.3119903 -4.2855821 -4.2715778 -4.2702885 -4.27494 -4.282876 -4.2958016 -4.3106542 -4.3204346 -4.3157587 -4.3013535 -4.2914972][-4.340888 -4.3418965 -4.3322506 -4.3109856 -4.291707 -4.2856617 -4.2890182 -4.29462 -4.3018494 -4.3114343 -4.3216491 -4.3291054 -4.3258095 -4.3156028 -4.3108921][-4.3236685 -4.3203449 -4.3084526 -4.2898674 -4.2747307 -4.2727423 -4.2792559 -4.2853355 -4.291822 -4.2981238 -4.3054762 -4.3154955 -4.3186502 -4.3159752 -4.3164215]]...]
INFO - root - 2017-12-06 03:33:52.435535: step 10210, loss = 2.05, batch loss = 1.99 (33.9 examples/sec; 0.236 sec/batch; 21h:07m:57s remains)
INFO - root - 2017-12-06 03:33:54.740423: step 10220, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 19h:56m:50s remains)
INFO - root - 2017-12-06 03:33:57.050102: step 10230, loss = 2.11, batch loss = 2.05 (33.8 examples/sec; 0.237 sec/batch; 21h:12m:08s remains)
INFO - root - 2017-12-06 03:33:59.356773: step 10240, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 20h:28m:31s remains)
INFO - root - 2017-12-06 03:34:01.699880: step 10250, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 20h:07m:48s remains)
INFO - root - 2017-12-06 03:34:03.987462: step 10260, loss = 2.07, batch loss = 2.01 (34.3 examples/sec; 0.234 sec/batch; 20h:54m:22s remains)
INFO - root - 2017-12-06 03:34:06.258903: step 10270, loss = 2.06, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 20h:53m:40s remains)
INFO - root - 2017-12-06 03:34:08.572444: step 10280, loss = 2.03, batch loss = 1.97 (35.1 examples/sec; 0.228 sec/batch; 20h:22m:49s remains)
INFO - root - 2017-12-06 03:34:10.920920: step 10290, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.233 sec/batch; 20h:50m:02s remains)
INFO - root - 2017-12-06 03:34:13.236658: step 10300, loss = 2.06, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:55m:39s remains)
2017-12-06 03:34:13.565416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2227483 -4.2307115 -4.2361007 -4.2401981 -4.242178 -4.2421017 -4.2411389 -4.2384076 -4.2341 -4.2244825 -4.2084045 -4.197525 -4.191267 -4.1983385 -4.2246165][-4.2111859 -4.2204332 -4.2248774 -4.2282233 -4.2304306 -4.2309117 -4.2303376 -4.2285614 -4.226368 -4.2194405 -4.2071023 -4.1990914 -4.1954088 -4.2037106 -4.2293639][-4.2113166 -4.2200851 -4.2247858 -4.2293267 -4.2320118 -4.2301631 -4.2271633 -4.2258844 -4.228158 -4.2268867 -4.22134 -4.217371 -4.213419 -4.2188969 -4.2399049][-4.2058597 -4.2163219 -4.2226334 -4.2302709 -4.2330484 -4.2261038 -4.2176318 -4.2160125 -4.227541 -4.23794 -4.2405453 -4.2405858 -4.2350259 -4.2366157 -4.2517085][-4.1728854 -4.1856618 -4.1959352 -4.2069559 -4.2086029 -4.1928954 -4.1730471 -4.1687479 -4.1943235 -4.2256784 -4.2437129 -4.2498603 -4.2419405 -4.237185 -4.2468767][-4.1138268 -4.1287355 -4.1489224 -4.1677346 -4.1702995 -4.1402888 -4.0960917 -4.0794849 -4.1190557 -4.178483 -4.2183323 -4.2331204 -4.2240906 -4.214643 -4.2253036][-4.0461411 -4.0684762 -4.1029654 -4.1269312 -4.125185 -4.0736227 -3.9965353 -3.9575362 -4.00716 -4.0950713 -4.1608996 -4.1907167 -4.1888371 -4.1863294 -4.2064085][-3.9870358 -4.0186286 -4.074152 -4.1131086 -4.1134925 -4.0479083 -3.9425585 -3.8746653 -3.9181454 -4.0157752 -4.0963397 -4.1446037 -4.1598034 -4.173512 -4.2078023][-3.9550502 -3.9918492 -4.0629573 -4.1214094 -4.1377993 -4.0869093 -3.9971547 -3.9267263 -3.9418507 -4.0100446 -4.0753279 -4.1259723 -4.1563897 -4.1911697 -4.2376418][-3.9775803 -4.0133066 -4.0830112 -4.1428347 -4.16533 -4.1361046 -4.0828705 -4.0318012 -4.0297403 -4.0636759 -4.1005735 -4.1389275 -4.1732216 -4.2198529 -4.2716131][-4.0355353 -4.07002 -4.1291304 -4.1804171 -4.1998944 -4.1854954 -4.1584206 -4.125051 -4.1164193 -4.12461 -4.1347713 -4.1575255 -4.189055 -4.23749 -4.288063][-4.0781088 -4.1116605 -4.1669946 -4.2185969 -4.2402592 -4.2356629 -4.2218189 -4.1945271 -4.1786871 -4.1681514 -4.1607614 -4.1709623 -4.1948028 -4.2352638 -4.2817764][-4.0896254 -4.1211209 -4.1774569 -4.2364826 -4.2677007 -4.2754889 -4.2711458 -4.2509074 -4.2314935 -4.2115841 -4.1945553 -4.1937089 -4.2076612 -4.2351375 -4.2722573][-4.0851021 -4.1110196 -4.1662436 -4.230566 -4.276185 -4.3007612 -4.3083458 -4.2974191 -4.276154 -4.249299 -4.2268715 -4.217804 -4.2234955 -4.2404246 -4.2677116][-4.0775604 -4.0931115 -4.1425791 -4.2049232 -4.2593184 -4.2970624 -4.3148818 -4.308331 -4.2852807 -4.257412 -4.2362518 -4.2243824 -4.2246222 -4.236433 -4.2589278]]...]
INFO - root - 2017-12-06 03:34:15.855192: step 10310, loss = 2.05, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 20h:27m:11s remains)
INFO - root - 2017-12-06 03:34:18.171294: step 10320, loss = 2.05, batch loss = 1.99 (34.3 examples/sec; 0.233 sec/batch; 20h:50m:56s remains)
INFO - root - 2017-12-06 03:34:20.465786: step 10330, loss = 2.08, batch loss = 2.02 (33.9 examples/sec; 0.236 sec/batch; 21h:07m:21s remains)
INFO - root - 2017-12-06 03:34:22.777701: step 10340, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 20h:06m:30s remains)
INFO - root - 2017-12-06 03:34:25.088323: step 10350, loss = 2.09, batch loss = 2.03 (34.9 examples/sec; 0.229 sec/batch; 20h:31m:36s remains)
INFO - root - 2017-12-06 03:34:27.401101: step 10360, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:55m:45s remains)
INFO - root - 2017-12-06 03:34:29.697061: step 10370, loss = 2.06, batch loss = 2.00 (33.3 examples/sec; 0.240 sec/batch; 21h:29m:55s remains)
INFO - root - 2017-12-06 03:34:31.983664: step 10380, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:06m:14s remains)
INFO - root - 2017-12-06 03:34:34.285328: step 10390, loss = 2.06, batch loss = 2.00 (33.3 examples/sec; 0.240 sec/batch; 21h:29m:10s remains)
INFO - root - 2017-12-06 03:34:36.566233: step 10400, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:53m:11s remains)
2017-12-06 03:34:36.861761: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3188853 -4.3154569 -4.3070822 -4.2921696 -4.2724085 -4.2552514 -4.2493486 -4.2598948 -4.2822647 -4.3042212 -4.3186517 -4.3273716 -4.3304405 -4.3285971 -4.3255386][-4.3165255 -4.3111067 -4.2993069 -4.2809129 -4.2599688 -4.2436128 -4.2404056 -4.2546382 -4.2790604 -4.3024583 -4.3194342 -4.3299065 -4.3326154 -4.330039 -4.3264718][-4.3125725 -4.3035221 -4.2859759 -4.2630448 -4.2394338 -4.22302 -4.2220712 -4.2403841 -4.2683206 -4.2944088 -4.3155594 -4.3295584 -4.3332276 -4.3310637 -4.3283625][-4.3001962 -4.2878675 -4.2642407 -4.2348237 -4.2056518 -4.1881642 -4.1901126 -4.214479 -4.2503052 -4.2827644 -4.3108621 -4.3301377 -4.3348246 -4.3327479 -4.3305144][-4.2762961 -4.2595558 -4.2277579 -4.189991 -4.1555872 -4.1379452 -4.1428967 -4.1726146 -4.2155514 -4.2577152 -4.2977676 -4.3248682 -4.3331003 -4.3328843 -4.3322067][-4.2333555 -4.2098522 -4.1685619 -4.1214252 -4.0854721 -4.0712576 -4.0795484 -4.110991 -4.1545877 -4.2064934 -4.2638497 -4.3045635 -4.3218355 -4.3271575 -4.3298836][-4.1773081 -4.1439662 -4.0928369 -4.0410652 -4.0113125 -4.0042639 -4.015614 -4.0451989 -4.0842352 -4.1425371 -4.2159047 -4.2716155 -4.3025808 -4.317338 -4.3249192][-4.1410327 -4.1009121 -4.0482664 -4.0017776 -3.9804051 -3.9778571 -3.9880991 -4.0109754 -4.0405612 -4.0988717 -4.1790876 -4.2437134 -4.2859917 -4.3096294 -4.3226504][-4.1369586 -4.0992346 -4.0549755 -4.0186405 -4.0010571 -3.9955893 -3.9971976 -4.0100226 -4.0308037 -4.084837 -4.1625552 -4.2275739 -4.2759371 -4.3058658 -4.3237705][-4.1498065 -4.1205449 -4.0886931 -4.0630975 -4.0481243 -4.0376797 -4.0317712 -4.0355358 -4.0491667 -4.0960016 -4.1639247 -4.2240252 -4.273303 -4.3062129 -4.3263984][-4.175312 -4.1546412 -4.1332626 -4.1179628 -4.108736 -4.1002469 -4.093585 -4.0921435 -4.099925 -4.1341124 -4.1857853 -4.234468 -4.2771764 -4.3080964 -4.3270459][-4.2126074 -4.2006769 -4.1893678 -4.1841059 -4.1824455 -4.1802506 -4.1763248 -4.1732583 -4.1768126 -4.1974134 -4.2315426 -4.2653818 -4.2951369 -4.3169284 -4.3294835][-4.2533026 -4.2484941 -4.2445579 -4.2464414 -4.2503762 -4.2531042 -4.2528453 -4.25048 -4.2504368 -4.25891 -4.2785459 -4.2999945 -4.3165145 -4.3275495 -4.3329182][-4.2850556 -4.2826304 -4.2819304 -4.286159 -4.2912745 -4.2951803 -4.2980552 -4.2975969 -4.2952952 -4.2964773 -4.306428 -4.3194742 -4.3275795 -4.3311696 -4.3322067][-4.3038483 -4.3021655 -4.3012977 -4.3034515 -4.306107 -4.3084626 -4.3117566 -4.3131752 -4.3116441 -4.3113508 -4.3162174 -4.3234673 -4.3267817 -4.3271794 -4.3273396]]...]
INFO - root - 2017-12-06 03:34:39.132939: step 10410, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.230 sec/batch; 20h:36m:02s remains)
INFO - root - 2017-12-06 03:34:41.418169: step 10420, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 20h:57m:04s remains)
INFO - root - 2017-12-06 03:34:43.731943: step 10430, loss = 2.05, batch loss = 1.99 (34.0 examples/sec; 0.235 sec/batch; 21h:01m:46s remains)
INFO - root - 2017-12-06 03:34:46.030054: step 10440, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 20h:56m:15s remains)
INFO - root - 2017-12-06 03:34:48.322094: step 10450, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.224 sec/batch; 20h:00m:47s remains)
INFO - root - 2017-12-06 03:34:50.594351: step 10460, loss = 2.04, batch loss = 1.98 (33.3 examples/sec; 0.240 sec/batch; 21h:27m:43s remains)
INFO - root - 2017-12-06 03:34:52.856249: step 10470, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:29m:36s remains)
INFO - root - 2017-12-06 03:34:55.142424: step 10480, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:11m:20s remains)
INFO - root - 2017-12-06 03:34:57.405510: step 10490, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:22m:30s remains)
INFO - root - 2017-12-06 03:34:59.706535: step 10500, loss = 2.08, batch loss = 2.02 (36.0 examples/sec; 0.222 sec/batch; 19h:51m:29s remains)
2017-12-06 03:35:00.040057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2904491 -4.2886724 -4.2816176 -4.2727022 -4.2693939 -4.2759461 -4.2871771 -4.2978177 -4.3047214 -4.3100519 -4.3141556 -4.3158565 -4.3132868 -4.3045077 -4.278245][-4.2731276 -4.273252 -4.2678466 -4.26327 -4.26618 -4.27798 -4.2928185 -4.3056927 -4.3118396 -4.3141961 -4.3185234 -4.3217196 -4.3160467 -4.2987771 -4.2627134][-4.2555647 -4.2584591 -4.2574673 -4.2600164 -4.2704349 -4.2849646 -4.298768 -4.3100367 -4.314064 -4.314455 -4.3202181 -4.3237071 -4.3135867 -4.2867379 -4.2431841][-4.2324734 -4.2404628 -4.2463417 -4.256568 -4.2730761 -4.2883444 -4.2998791 -4.3083391 -4.3115253 -4.313458 -4.3210568 -4.320322 -4.3018837 -4.2658629 -4.2212043][-4.2048664 -4.2213106 -4.2365217 -4.2545991 -4.2762632 -4.2912083 -4.2988257 -4.3021345 -4.301898 -4.3039184 -4.3071823 -4.2965922 -4.2690458 -4.2295637 -4.1947169][-4.1800842 -4.2073789 -4.2341642 -4.258193 -4.2799983 -4.2903438 -4.28917 -4.2803378 -4.270225 -4.2673869 -4.2644453 -4.2486973 -4.2218256 -4.191566 -4.1771159][-4.1720381 -4.2056823 -4.2359152 -4.2571278 -4.2724023 -4.2724538 -4.2570281 -4.2306309 -4.21098 -4.2097869 -4.2082677 -4.1984158 -4.1850557 -4.1749358 -4.1829991][-4.1789885 -4.2075214 -4.2320728 -4.2472835 -4.2544026 -4.2437525 -4.2118082 -4.1667438 -4.1428113 -4.1520329 -4.1628013 -4.1679435 -4.1736555 -4.1839738 -4.2066469][-4.20345 -4.2191877 -4.2343135 -4.2427983 -4.2417645 -4.2210712 -4.1759319 -4.1180048 -4.0986729 -4.1256371 -4.1531253 -4.1749921 -4.195363 -4.2152939 -4.2392755][-4.2250786 -4.2298641 -4.2385654 -4.2426047 -4.2352471 -4.2096367 -4.1610317 -4.1045766 -4.1007185 -4.14474 -4.1820464 -4.2098279 -4.2315331 -4.2481432 -4.2661419][-4.2262578 -4.2284188 -4.2365847 -4.2407131 -4.2326584 -4.21002 -4.1715188 -4.1323957 -4.1433964 -4.1873455 -4.2214966 -4.2434182 -4.2588596 -4.2708154 -4.2836561][-4.2177377 -4.22206 -4.23199 -4.2374759 -4.2329845 -4.2195539 -4.19743 -4.1758566 -4.1871605 -4.2155228 -4.2391047 -4.25444 -4.2647343 -4.2740693 -4.2827806][-4.21699 -4.2211318 -4.22945 -4.23781 -4.2363858 -4.2272544 -4.214283 -4.2011123 -4.2045903 -4.2179751 -4.2353106 -4.2466021 -4.2529669 -4.2604504 -4.2664223][-4.2262931 -4.2245126 -4.2274933 -4.23615 -4.2347531 -4.2259579 -4.2173476 -4.208487 -4.2068319 -4.2133532 -4.2268062 -4.2326488 -4.2342958 -4.2399592 -4.2450914][-4.23298 -4.2256927 -4.2225957 -4.2271266 -4.2253284 -4.216176 -4.210743 -4.2053809 -4.2055712 -4.210968 -4.2206593 -4.2206631 -4.2183642 -4.2205353 -4.2230749]]...]
INFO - root - 2017-12-06 03:35:02.312656: step 10510, loss = 2.10, batch loss = 2.04 (35.2 examples/sec; 0.227 sec/batch; 20h:19m:52s remains)
INFO - root - 2017-12-06 03:35:04.603493: step 10520, loss = 2.04, batch loss = 1.98 (34.7 examples/sec; 0.230 sec/batch; 20h:36m:33s remains)
INFO - root - 2017-12-06 03:35:06.953511: step 10530, loss = 2.04, batch loss = 1.98 (32.5 examples/sec; 0.246 sec/batch; 21h:58m:54s remains)
INFO - root - 2017-12-06 03:35:09.295112: step 10540, loss = 2.06, batch loss = 2.00 (33.8 examples/sec; 0.237 sec/batch; 21h:11m:36s remains)
INFO - root - 2017-12-06 03:35:11.581218: step 10550, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 20h:17m:33s remains)
INFO - root - 2017-12-06 03:35:13.873786: step 10560, loss = 2.06, batch loss = 2.00 (34.0 examples/sec; 0.235 sec/batch; 21h:02m:54s remains)
INFO - root - 2017-12-06 03:35:16.169557: step 10570, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 20h:20m:14s remains)
INFO - root - 2017-12-06 03:35:18.495086: step 10580, loss = 2.04, batch loss = 1.98 (34.9 examples/sec; 0.229 sec/batch; 20h:28m:12s remains)
INFO - root - 2017-12-06 03:35:20.828065: step 10590, loss = 2.05, batch loss = 1.99 (33.9 examples/sec; 0.236 sec/batch; 21h:04m:57s remains)
INFO - root - 2017-12-06 03:35:23.116939: step 10600, loss = 2.09, batch loss = 2.03 (35.2 examples/sec; 0.228 sec/batch; 20h:21m:00s remains)
2017-12-06 03:35:23.413097: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1137123 -4.0543256 -4.0053053 -3.9593379 -3.9742196 -4.0217261 -4.0484166 -4.0658751 -4.0719404 -4.0770812 -4.098742 -4.1151233 -4.1215754 -4.13649 -4.1488237][-4.1041913 -4.0485716 -4.0082464 -3.9824815 -4.0035286 -4.0407777 -4.0486369 -4.0508842 -4.0535059 -4.0674896 -4.1070714 -4.1379828 -4.1510792 -4.1642923 -4.1724548][-4.121613 -4.0735497 -4.0445313 -4.0405035 -4.0700259 -4.1000185 -4.0957146 -4.0876164 -4.08698 -4.1032653 -4.14249 -4.1728897 -4.18399 -4.1904941 -4.1916084][-4.1576104 -4.1129117 -4.0886583 -4.0957594 -4.1285467 -4.1564503 -4.1562691 -4.158258 -4.1653123 -4.1769919 -4.1925321 -4.20118 -4.199512 -4.1998024 -4.1960473][-4.18855 -4.1426435 -4.1120629 -4.1114759 -4.1328087 -4.151216 -4.1571703 -4.1746688 -4.2100692 -4.2288446 -4.2251744 -4.212976 -4.1970487 -4.1906633 -4.1911354][-4.1992722 -4.14498 -4.10228 -4.0834684 -4.0854006 -4.0880766 -4.0881619 -4.1121154 -4.1695871 -4.2045326 -4.2031355 -4.1895905 -4.1762834 -4.1767192 -4.19088][-4.1856494 -4.1200438 -4.0657325 -4.0324535 -4.0212374 -4.0122285 -3.9990838 -4.01215 -4.0717168 -4.11572 -4.1221237 -4.1221294 -4.1310186 -4.1566 -4.1948714][-4.1556287 -4.0811434 -4.0232568 -3.9849992 -3.9674957 -3.9510283 -3.924191 -3.9206424 -3.9745932 -4.0186095 -4.0274878 -4.0500317 -4.092267 -4.1458907 -4.1977291][-4.130259 -4.0490794 -3.9869802 -3.949275 -3.9345932 -3.9172058 -3.88387 -3.8684335 -3.9089708 -3.9429386 -3.9527173 -4.00049 -4.0773211 -4.1517072 -4.2004018][-4.1252732 -4.0450191 -3.9813182 -3.9461749 -3.9390333 -3.928915 -3.909049 -3.9013681 -3.9250939 -3.9401557 -3.9461219 -4.0012183 -4.0912671 -4.1656284 -4.1971269][-4.1389503 -4.0665326 -4.0130773 -3.9872129 -3.9850564 -3.9828436 -3.9804761 -3.9873369 -4.0066166 -4.0178604 -4.0236783 -4.0616145 -4.1274161 -4.1740036 -4.1818395][-4.1596022 -4.0956759 -4.0590849 -4.0488386 -4.0583239 -4.0658607 -4.0673885 -4.0689974 -4.0834761 -4.1008072 -4.1150746 -4.1356926 -4.1677008 -4.1861639 -4.1744714][-4.1813025 -4.1275005 -4.103065 -4.1045737 -4.1274734 -4.144382 -4.148993 -4.1496835 -4.1599321 -4.1773691 -4.1907 -4.1998286 -4.2072639 -4.2050748 -4.1818547][-4.1955352 -4.1503825 -4.1359072 -4.1457682 -4.1757126 -4.197999 -4.20595 -4.2092938 -4.2188387 -4.2346129 -4.2437716 -4.2476611 -4.2471709 -4.2356687 -4.20656][-4.1981244 -4.15572 -4.1468949 -4.1607194 -4.1946254 -4.22086 -4.230125 -4.2385306 -4.2506471 -4.2653027 -4.27236 -4.2750468 -4.2730522 -4.259656 -4.2304974]]...]
INFO - root - 2017-12-06 03:35:25.718147: step 10610, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.226 sec/batch; 20h:10m:05s remains)
INFO - root - 2017-12-06 03:35:27.990391: step 10620, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.230 sec/batch; 20h:35m:10s remains)
INFO - root - 2017-12-06 03:35:30.261936: step 10630, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 19h:35m:39s remains)
INFO - root - 2017-12-06 03:35:32.585614: step 10640, loss = 2.04, batch loss = 1.98 (35.5 examples/sec; 0.225 sec/batch; 20h:08m:39s remains)
INFO - root - 2017-12-06 03:35:34.887931: step 10650, loss = 2.08, batch loss = 2.02 (34.6 examples/sec; 0.231 sec/batch; 20h:41m:35s remains)
INFO - root - 2017-12-06 03:35:37.227923: step 10660, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 21h:05m:17s remains)
INFO - root - 2017-12-06 03:35:39.579724: step 10670, loss = 2.05, batch loss = 1.99 (33.5 examples/sec; 0.239 sec/batch; 21h:20m:02s remains)
INFO - root - 2017-12-06 03:35:41.871638: step 10680, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:55m:23s remains)
INFO - root - 2017-12-06 03:35:44.162763: step 10690, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 20h:38m:21s remains)
INFO - root - 2017-12-06 03:35:46.495796: step 10700, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:34m:07s remains)
2017-12-06 03:35:46.765386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.196147 -4.2167563 -4.2400246 -4.2594805 -4.2721386 -4.2779093 -4.2772927 -4.2702928 -4.2615881 -4.2545214 -4.2499518 -4.2455258 -4.2418556 -4.24016 -4.2385716][-4.2171078 -4.237627 -4.2566614 -4.27095 -4.2783194 -4.2794223 -4.2747793 -4.26613 -4.2573767 -4.2496624 -4.2435069 -4.2382913 -4.2349997 -4.2340517 -4.2336512][-4.240478 -4.2589154 -4.2708192 -4.2760696 -4.274436 -4.2689314 -4.2609005 -4.2521553 -4.2446737 -4.2376437 -4.2307844 -4.2255125 -4.223484 -4.2239585 -4.2252154][-4.2512779 -4.2627964 -4.2644229 -4.2587061 -4.2486992 -4.23917 -4.2310395 -4.2253 -4.2219834 -4.2190852 -4.2151923 -4.2125263 -4.2133374 -4.21612 -4.2188611][-4.2519813 -4.2529192 -4.2424951 -4.2266417 -4.21039 -4.199367 -4.1940961 -4.1944904 -4.1988592 -4.2035661 -4.2062669 -4.2086792 -4.2125258 -4.2170177 -4.2201815][-4.2484212 -4.2397857 -4.22165 -4.2005959 -4.1826372 -4.1730251 -4.1723733 -4.1796741 -4.19138 -4.202991 -4.2115054 -4.2176061 -4.2227173 -4.2268457 -4.2290888][-4.2430758 -4.2317147 -4.2141113 -4.1957445 -4.1817904 -4.1760345 -4.1787472 -4.188807 -4.2020717 -4.2142487 -4.2230091 -4.2293921 -4.2342215 -4.2377729 -4.2398963][-4.2408895 -4.2321525 -4.2194629 -4.2063513 -4.1968026 -4.1937227 -4.1973486 -4.2066369 -4.2170339 -4.2254438 -4.2311454 -4.2359848 -4.2402754 -4.2436323 -4.2465134][-4.2427583 -4.2376966 -4.2297373 -4.2210536 -4.2147589 -4.2132092 -4.2163572 -4.2228427 -4.2287459 -4.2321281 -4.2337613 -4.236289 -4.2397442 -4.2429128 -4.2462273][-4.2472696 -4.2444181 -4.2396612 -4.2345066 -4.2309237 -4.2303319 -4.2325854 -4.2361531 -4.2380958 -4.2372479 -4.235806 -4.2362823 -4.2385182 -4.2411408 -4.2443671][-4.2503543 -4.2489944 -4.246552 -4.2438946 -4.2420697 -4.2415524 -4.24239 -4.2435369 -4.2430282 -4.240274 -4.2377043 -4.2373018 -4.2387795 -4.2408218 -4.2438841][-4.2488737 -4.2484756 -4.2471585 -4.245791 -4.2448258 -4.2442379 -4.2442904 -4.2443781 -4.243329 -4.2407484 -4.2387066 -4.2384553 -4.2394462 -4.2409105 -4.2432404][-4.24335 -4.2433128 -4.2422953 -4.241488 -4.2409711 -4.240387 -4.2402091 -4.2402992 -4.2395887 -4.2378292 -4.2366128 -4.2364988 -4.2369905 -4.2377243 -4.2391138][-4.2357125 -4.2355876 -4.23445 -4.2336941 -4.2331476 -4.2323174 -4.2318764 -4.2318263 -4.2311935 -4.2299428 -4.2291765 -4.2291284 -4.2291675 -4.2293057 -4.2300868][-4.2286172 -4.2287545 -4.2281713 -4.2281051 -4.2281561 -4.2277741 -4.2273726 -4.226861 -4.2254019 -4.2234521 -4.2218275 -4.2207365 -4.2196183 -4.2187252 -4.2188072]]...]
INFO - root - 2017-12-06 03:35:49.068799: step 10710, loss = 2.07, batch loss = 2.01 (32.2 examples/sec; 0.249 sec/batch; 22h:13m:43s remains)
INFO - root - 2017-12-06 03:35:51.401880: step 10720, loss = 2.06, batch loss = 2.00 (33.6 examples/sec; 0.238 sec/batch; 21h:18m:42s remains)
INFO - root - 2017-12-06 03:35:53.699402: step 10730, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:50m:35s remains)
INFO - root - 2017-12-06 03:35:55.991348: step 10740, loss = 2.09, batch loss = 2.03 (35.8 examples/sec; 0.224 sec/batch; 19h:59m:44s remains)
INFO - root - 2017-12-06 03:35:58.304871: step 10750, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:54m:09s remains)
INFO - root - 2017-12-06 03:36:00.615896: step 10760, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:56m:26s remains)
INFO - root - 2017-12-06 03:36:02.958593: step 10770, loss = 2.04, batch loss = 1.98 (32.7 examples/sec; 0.245 sec/batch; 21h:51m:33s remains)
INFO - root - 2017-12-06 03:36:05.260910: step 10780, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 20h:02m:24s remains)
INFO - root - 2017-12-06 03:36:07.568176: step 10790, loss = 2.05, batch loss = 1.99 (33.6 examples/sec; 0.238 sec/batch; 21h:15m:25s remains)
INFO - root - 2017-12-06 03:36:09.872690: step 10800, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 20h:26m:48s remains)
2017-12-06 03:36:10.163086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3263793 -4.3276968 -4.3226376 -4.3166981 -4.3096323 -4.3005486 -4.2905803 -4.2814221 -4.2749252 -4.2695565 -4.2687287 -4.2589049 -4.239079 -4.2183266 -4.1966095][-4.3129821 -4.31037 -4.3002915 -4.2892261 -4.2758083 -4.2593913 -4.2438927 -4.2325625 -4.2275529 -4.2263589 -4.2286878 -4.2183084 -4.1969862 -4.175396 -4.1444516][-4.2890573 -4.28219 -4.2660666 -4.2464266 -4.2266889 -4.209466 -4.1966233 -4.1907673 -4.1892266 -4.1930723 -4.1996 -4.1895766 -4.1678319 -4.1400323 -4.0984807][-4.2560377 -4.2454944 -4.2265768 -4.2044015 -4.1873012 -4.1752725 -4.1704526 -4.1712332 -4.17126 -4.1775246 -4.19 -4.1854997 -4.1661582 -4.1394911 -4.1036406][-4.2253656 -4.2159438 -4.197916 -4.1766987 -4.1629267 -4.1502833 -4.1438746 -4.147635 -4.1520252 -4.1629887 -4.1830544 -4.1903758 -4.1853633 -4.1756959 -4.1559935][-4.2085104 -4.1999607 -4.1828713 -4.161994 -4.1429257 -4.1174026 -4.0943809 -4.0929527 -4.106925 -4.1277175 -4.156508 -4.1771398 -4.1856031 -4.1929774 -4.1887407][-4.2118459 -4.2030134 -4.1804419 -4.150629 -4.1154652 -4.0577626 -3.9962792 -3.9888418 -4.0257673 -4.0643144 -4.1031494 -4.1323457 -4.1547856 -4.183754 -4.1988225][-4.2242584 -4.2122169 -4.1766362 -4.1281424 -4.0677004 -3.9802227 -3.8862507 -3.8785689 -3.952898 -4.0157213 -4.0594568 -4.0964556 -4.1353531 -4.1814389 -4.2121778][-4.2293825 -4.2147832 -4.1650739 -4.1000109 -4.0291471 -3.9463835 -3.8629661 -3.8697233 -3.9567015 -4.0183716 -4.0526662 -4.0927887 -4.141717 -4.1969242 -4.2373915][-4.2093482 -4.1959691 -4.1534281 -4.098403 -4.0446272 -3.9987712 -3.9616163 -3.9710343 -4.0212517 -4.0579109 -4.0798955 -4.1169152 -4.1687489 -4.2230477 -4.2610254][-4.1638365 -4.1621556 -4.1478744 -4.1237173 -4.0998554 -4.0878997 -4.0806923 -4.0802407 -4.0956388 -4.1095037 -4.1255026 -4.1602073 -4.2051105 -4.2462673 -4.2712717][-4.119688 -4.1411266 -4.1578274 -4.160666 -4.1559782 -4.1564713 -4.1581116 -4.1548142 -4.1556664 -4.1570535 -4.164063 -4.1855855 -4.2155433 -4.2437916 -4.2598267][-4.1085978 -4.1524367 -4.1798925 -4.1890783 -4.1877894 -4.1876416 -4.1887364 -4.1898346 -4.1890635 -4.1823692 -4.1830225 -4.1937175 -4.2133212 -4.2338405 -4.2486138][-4.1328316 -4.1757731 -4.1933846 -4.1933193 -4.1847105 -4.1801567 -4.1814775 -4.1865726 -4.1893468 -4.1897221 -4.1967983 -4.2077188 -4.2274141 -4.2510824 -4.2703848][-4.1568079 -4.1822152 -4.18723 -4.1788282 -4.1655993 -4.161725 -4.1712756 -4.1830359 -4.1969881 -4.2119036 -4.2275772 -4.24091 -4.2611771 -4.2848277 -4.3025002]]...]
INFO - root - 2017-12-06 03:36:12.442450: step 10810, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.224 sec/batch; 20h:03m:24s remains)
INFO - root - 2017-12-06 03:36:14.770148: step 10820, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 20h:06m:54s remains)
INFO - root - 2017-12-06 03:36:17.085589: step 10830, loss = 2.03, batch loss = 1.97 (33.7 examples/sec; 0.237 sec/batch; 21h:11m:34s remains)
INFO - root - 2017-12-06 03:36:19.390656: step 10840, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 20h:03m:36s remains)
INFO - root - 2017-12-06 03:36:21.688681: step 10850, loss = 2.11, batch loss = 2.05 (33.2 examples/sec; 0.241 sec/batch; 21h:29m:51s remains)
INFO - root - 2017-12-06 03:36:24.001055: step 10860, loss = 2.04, batch loss = 1.98 (35.2 examples/sec; 0.227 sec/batch; 20h:17m:09s remains)
INFO - root - 2017-12-06 03:36:26.301865: step 10870, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:10m:30s remains)
INFO - root - 2017-12-06 03:36:28.602612: step 10880, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 20h:27m:29s remains)
INFO - root - 2017-12-06 03:36:30.906376: step 10890, loss = 2.09, batch loss = 2.03 (30.7 examples/sec; 0.260 sec/batch; 23h:14m:35s remains)
INFO - root - 2017-12-06 03:36:33.157034: step 10900, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 19h:38m:21s remains)
2017-12-06 03:36:33.459195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3218312 -4.3062749 -4.2919331 -4.2827077 -4.2815847 -4.2864742 -4.2933717 -4.3002348 -4.3113747 -4.321609 -4.3297253 -4.3349938 -4.3372006 -4.3401179 -4.3416243][-4.3044276 -4.2832465 -4.265676 -4.2554216 -4.2532907 -4.2584195 -4.2630186 -4.2655883 -4.2804766 -4.29734 -4.31267 -4.3205962 -4.3225913 -4.3283482 -4.3322344][-4.285635 -4.2621884 -4.2452583 -4.2353988 -4.2311 -4.2345066 -4.2326708 -4.2246127 -4.2360563 -4.2576423 -4.2817707 -4.2950611 -4.2991862 -4.3074346 -4.3145733][-4.2684288 -4.2443404 -4.2254295 -4.2131662 -4.2060146 -4.2028556 -4.18855 -4.160655 -4.1665869 -4.2035589 -4.2453928 -4.2709374 -4.279871 -4.2898107 -4.2956185][-4.2501483 -4.2245126 -4.2003593 -4.1847482 -4.1761541 -4.159802 -4.1189442 -4.0612078 -4.067369 -4.136878 -4.2055712 -4.2471933 -4.2639828 -4.27527 -4.2794161][-4.2364755 -4.2060275 -4.1707397 -4.1430717 -4.1232238 -4.0839033 -4.005672 -3.9124599 -3.9251742 -4.0346642 -4.1351318 -4.1975126 -4.2313681 -4.25325 -4.2627363][-4.2293863 -4.1880784 -4.1370187 -4.0865145 -4.0425825 -3.9766212 -3.8670197 -3.7444351 -3.7538931 -3.8958819 -4.0363851 -4.1285906 -4.1884274 -4.2329917 -4.2545767][-4.2245932 -4.17132 -4.103013 -4.0278678 -3.9613721 -3.8813307 -3.769464 -3.6528924 -3.6606841 -3.8142538 -3.9794495 -4.0915685 -4.1672115 -4.2277861 -4.2585239][-4.2279425 -4.1664982 -4.084136 -3.9919086 -3.9097133 -3.8282087 -3.7440636 -3.6733212 -3.6971283 -3.8292398 -3.9828014 -4.0939312 -4.1676474 -4.2294564 -4.2617278][-4.2382421 -4.1736221 -4.0860224 -3.9864626 -3.8903604 -3.7993591 -3.735528 -3.7070751 -3.7526493 -3.8656151 -3.9994953 -4.1064744 -4.1765361 -4.2315259 -4.2632656][-4.25544 -4.197824 -4.1157866 -4.021657 -3.9253259 -3.8318861 -3.7735448 -3.7663097 -3.8231206 -3.9203172 -4.031302 -4.1285858 -4.1931348 -4.24149 -4.2720737][-4.2743878 -4.2302265 -4.1647143 -4.0919094 -4.0166273 -3.9466052 -3.9026198 -3.9033668 -3.9507031 -4.0200458 -4.1027875 -4.179131 -4.232357 -4.2707114 -4.2942767][-4.29539 -4.2687192 -4.2266016 -4.1822805 -4.1379914 -4.1003127 -4.0766125 -4.080761 -4.1116409 -4.1530356 -4.2036071 -4.2516866 -4.2851257 -4.3076878 -4.3194489][-4.3117967 -4.3002977 -4.2812176 -4.2604823 -4.2422905 -4.2281275 -4.2210469 -4.2263789 -4.2426834 -4.2630978 -4.2883239 -4.3108768 -4.3254056 -4.3328223 -4.3343139][-4.3198619 -4.3144808 -4.3064094 -4.2981839 -4.2928743 -4.2897019 -4.2900481 -4.2966485 -4.3060336 -4.3151641 -4.3258724 -4.3345733 -4.3384976 -4.338769 -4.3369851]]...]
INFO - root - 2017-12-06 03:36:35.739935: step 10910, loss = 2.05, batch loss = 1.99 (37.7 examples/sec; 0.212 sec/batch; 18h:57m:30s remains)
INFO - root - 2017-12-06 03:36:38.070162: step 10920, loss = 2.04, batch loss = 1.98 (34.4 examples/sec; 0.232 sec/batch; 20h:45m:25s remains)
INFO - root - 2017-12-06 03:36:40.434327: step 10930, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 20h:07m:20s remains)
INFO - root - 2017-12-06 03:36:42.726700: step 10940, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:43m:09s remains)
INFO - root - 2017-12-06 03:36:45.040639: step 10950, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:03m:54s remains)
INFO - root - 2017-12-06 03:36:47.311953: step 10960, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.233 sec/batch; 20h:46m:09s remains)
INFO - root - 2017-12-06 03:36:49.671054: step 10970, loss = 2.05, batch loss = 1.99 (33.8 examples/sec; 0.236 sec/batch; 21h:07m:18s remains)
INFO - root - 2017-12-06 03:36:51.988331: step 10980, loss = 2.05, batch loss = 1.99 (36.0 examples/sec; 0.222 sec/batch; 19h:51m:47s remains)
INFO - root - 2017-12-06 03:36:54.299739: step 10990, loss = 2.09, batch loss = 2.03 (34.3 examples/sec; 0.233 sec/batch; 20h:50m:12s remains)
INFO - root - 2017-12-06 03:36:56.556156: step 11000, loss = 2.06, batch loss = 2.01 (36.6 examples/sec; 0.218 sec/batch; 19h:30m:30s remains)
2017-12-06 03:36:56.826105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3149757 -4.3071055 -4.3033442 -4.3064146 -4.3158245 -4.3208847 -4.3215675 -4.3227725 -4.3273382 -4.3318262 -4.3320794 -4.3294992 -4.3319459 -4.3388209 -4.3462386][-4.2873626 -4.2758584 -4.2690468 -4.2715182 -4.2837887 -4.29127 -4.2920246 -4.2936325 -4.2994804 -4.3068786 -4.3078275 -4.3030529 -4.307755 -4.3205667 -4.3349304][-4.2554903 -4.2393713 -4.2277613 -4.2272396 -4.2387295 -4.2457457 -4.2454724 -4.2471137 -4.2563782 -4.268858 -4.2682533 -4.2565894 -4.2624865 -4.284996 -4.3109555][-4.2233276 -4.2003932 -4.183444 -4.179625 -4.1866512 -4.1868429 -4.1777143 -4.1741023 -4.195261 -4.2199621 -4.22042 -4.2050457 -4.2148333 -4.2460036 -4.2815394][-4.1945505 -4.1615472 -4.1360497 -4.1277752 -4.1272831 -4.1135421 -4.0845108 -4.0777235 -4.1225958 -4.1696796 -4.1782293 -4.1639104 -4.1804309 -4.2150874 -4.2541828][-4.1745114 -4.1309004 -4.0938864 -4.0744958 -4.0554914 -4.0122948 -3.949204 -3.94082 -4.0234442 -4.1016498 -4.1267495 -4.1210418 -4.1459455 -4.1853514 -4.2263904][-4.1706071 -4.12378 -4.0783591 -4.0418129 -4.0000358 -3.9218516 -3.8192158 -3.8106039 -3.9293568 -4.0353255 -4.0834475 -4.0975857 -4.1302404 -4.1741438 -4.212687][-4.18063 -4.1384044 -4.0956397 -4.05596 -4.0072632 -3.9247415 -3.8215532 -3.8170013 -3.9245594 -4.0210452 -4.07334 -4.0989828 -4.1348667 -4.1816511 -4.2164][-4.1975193 -4.1623125 -4.1281829 -4.0943151 -4.0559163 -3.9973469 -3.9284706 -3.9280818 -3.9974451 -4.0565391 -4.0922556 -4.1137371 -4.147923 -4.19246 -4.2257123][-4.2161403 -4.1909928 -4.1666536 -4.1430779 -4.1209469 -4.0901966 -4.0550814 -4.056644 -4.0942984 -4.1246648 -4.1407738 -4.1484036 -4.17171 -4.20747 -4.2339964][-4.2357211 -4.2196312 -4.2039218 -4.1911449 -4.1848669 -4.1767373 -4.1650481 -4.1703677 -4.1879354 -4.2007422 -4.204052 -4.1996346 -4.21195 -4.2359343 -4.2515249][-4.2557912 -4.2433057 -4.2323208 -4.2269783 -4.2320395 -4.2387185 -4.2426763 -4.2538772 -4.2659974 -4.2721715 -4.2705245 -4.2607894 -4.2621946 -4.2707744 -4.2762575][-4.2733216 -4.2622643 -4.2555857 -4.2557187 -4.2657781 -4.2774596 -4.285924 -4.297215 -4.3078151 -4.312593 -4.3113337 -4.3028693 -4.3004484 -4.3017311 -4.3008537][-4.2883463 -4.2777128 -4.274435 -4.2790313 -4.2908731 -4.3031273 -4.3124866 -4.322998 -4.3314738 -4.3346987 -4.3328433 -4.3264074 -4.3231936 -4.3225117 -4.3204703][-4.3094988 -4.2997293 -4.2963495 -4.3009176 -4.3107686 -4.3209028 -4.3295975 -4.3380961 -4.3449593 -4.3470297 -4.3454428 -4.3411713 -4.3387074 -4.3369555 -4.3348169]]...]
INFO - root - 2017-12-06 03:36:59.115892: step 11010, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 20h:30m:32s remains)
INFO - root - 2017-12-06 03:37:01.404110: step 11020, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.226 sec/batch; 20h:08m:48s remains)
INFO - root - 2017-12-06 03:37:03.695041: step 11030, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 20h:32m:52s remains)
INFO - root - 2017-12-06 03:37:05.993328: step 11040, loss = 2.07, batch loss = 2.02 (34.0 examples/sec; 0.235 sec/batch; 21h:00m:33s remains)
INFO - root - 2017-12-06 03:37:08.306923: step 11050, loss = 2.07, batch loss = 2.02 (35.0 examples/sec; 0.229 sec/batch; 20h:25m:44s remains)
INFO - root - 2017-12-06 03:37:10.584675: step 11060, loss = 2.05, batch loss = 2.00 (35.6 examples/sec; 0.224 sec/batch; 20h:02m:42s remains)
INFO - root - 2017-12-06 03:37:12.890013: step 11070, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:20m:50s remains)
INFO - root - 2017-12-06 03:37:15.174949: step 11080, loss = 2.09, batch loss = 2.03 (35.2 examples/sec; 0.227 sec/batch; 20h:17m:19s remains)
INFO - root - 2017-12-06 03:37:17.507793: step 11090, loss = 2.09, batch loss = 2.03 (35.8 examples/sec; 0.223 sec/batch; 19h:57m:02s remains)
INFO - root - 2017-12-06 03:37:19.780670: step 11100, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.230 sec/batch; 20h:29m:31s remains)
2017-12-06 03:37:20.066485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1721964 -4.1486106 -4.1580267 -4.166285 -4.16522 -4.1820135 -4.205822 -4.223999 -4.2305503 -4.2280192 -4.2243676 -4.2188482 -4.2249079 -4.2349339 -4.2485981][-4.1362267 -4.1158051 -4.1273375 -4.1258497 -4.118886 -4.1379657 -4.1661634 -4.1902566 -4.2003903 -4.1986675 -4.1986871 -4.1997232 -4.2113295 -4.2223978 -4.2358403][-4.1207714 -4.1079035 -4.1146097 -4.1017137 -4.0948033 -4.1154404 -4.1408629 -4.1644187 -4.178772 -4.1791768 -4.1824484 -4.1893716 -4.1941504 -4.2016907 -4.2128367][-4.1184511 -4.1088586 -4.1079135 -4.08814 -4.0843987 -4.1014147 -4.1163816 -4.135603 -4.1475377 -4.150795 -4.1652379 -4.1794143 -4.1804132 -4.181016 -4.18985][-4.128758 -4.1189156 -4.1127806 -4.0874748 -4.07525 -4.0778165 -4.0820012 -4.0991688 -4.1091123 -4.1209869 -4.1571803 -4.1831293 -4.1831098 -4.1753392 -4.1817389][-4.1339445 -4.1281066 -4.122293 -4.0922847 -4.072928 -4.0657644 -4.0565424 -4.0600495 -4.0557766 -4.0783658 -4.1421709 -4.1799407 -4.1812329 -4.1681991 -4.1685042][-4.1176119 -4.1186771 -4.1173882 -4.0924535 -4.0726953 -4.0567117 -4.0228 -3.9901834 -3.9543517 -3.9891372 -4.082593 -4.1428223 -4.1580443 -4.1425352 -4.1403403][-4.1033063 -4.1036534 -4.1060576 -4.08819 -4.0625825 -4.0257792 -3.9619951 -3.89985 -3.8480897 -3.9081571 -4.0294518 -4.1029305 -4.1268625 -4.1206 -4.1265583][-4.102107 -4.0993977 -4.1023126 -4.0904789 -4.0628405 -4.0167704 -3.9461868 -3.888171 -3.8560185 -3.9305222 -4.0366282 -4.0968628 -4.1157551 -4.1155705 -4.1236725][-4.0946879 -4.0925369 -4.1016397 -4.1021481 -4.0816374 -4.0435848 -3.9885268 -3.9481871 -3.9354293 -3.9912975 -4.0564389 -4.0935988 -4.1022415 -4.1034441 -4.1081276][-4.1064563 -4.1065764 -4.1202703 -4.1285529 -4.1173992 -4.0916843 -4.0608635 -4.0302863 -4.0212455 -4.0518746 -4.0824661 -4.1002927 -4.1052141 -4.1053019 -4.113378][-4.1326647 -4.1306195 -4.1418886 -4.1546874 -4.15386 -4.1405797 -4.1251597 -4.1017923 -4.0960131 -4.1132655 -4.1286287 -4.1356263 -4.1370749 -4.1370592 -4.1478386][-4.1628947 -4.1579332 -4.1648035 -4.1785679 -4.1856823 -4.1831794 -4.1766524 -4.1614056 -4.1607633 -4.1729207 -4.1808238 -4.1846323 -4.180913 -4.179832 -4.1876173][-4.2091165 -4.2067909 -4.213881 -4.226912 -4.2356014 -4.2362061 -4.2308025 -4.2196522 -4.2201462 -4.2284102 -4.2330661 -4.2374086 -4.23389 -4.2293444 -4.2320256][-4.2517481 -4.2522936 -4.2609482 -4.2719879 -4.2778525 -4.2769928 -4.2712483 -4.2624269 -4.2626023 -4.2706051 -4.2752838 -4.2796683 -4.27833 -4.2734432 -4.2715459]]...]
INFO - root - 2017-12-06 03:37:22.369447: step 11110, loss = 2.10, batch loss = 2.04 (32.8 examples/sec; 0.244 sec/batch; 21h:45m:42s remains)
INFO - root - 2017-12-06 03:37:24.693396: step 11120, loss = 2.04, batch loss = 1.98 (36.2 examples/sec; 0.221 sec/batch; 19h:43m:24s remains)
INFO - root - 2017-12-06 03:37:27.043380: step 11130, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:22m:26s remains)
INFO - root - 2017-12-06 03:37:29.363223: step 11140, loss = 2.06, batch loss = 2.00 (33.8 examples/sec; 0.237 sec/batch; 21h:07m:23s remains)
INFO - root - 2017-12-06 03:37:31.646845: step 11150, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.232 sec/batch; 20h:44m:19s remains)
INFO - root - 2017-12-06 03:37:33.929573: step 11160, loss = 2.04, batch loss = 1.99 (36.6 examples/sec; 0.219 sec/batch; 19h:30m:28s remains)
INFO - root - 2017-12-06 03:37:36.242839: step 11170, loss = 2.07, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:43m:15s remains)
INFO - root - 2017-12-06 03:37:38.559233: step 11180, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:20m:42s remains)
INFO - root - 2017-12-06 03:37:40.879296: step 11190, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 20h:05m:30s remains)
INFO - root - 2017-12-06 03:37:43.153108: step 11200, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.224 sec/batch; 19h:57m:12s remains)
2017-12-06 03:37:43.433845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0207057 -4.031446 -4.0446548 -4.0706611 -4.1023588 -4.125298 -4.1372709 -4.146266 -4.1529474 -4.1576624 -4.15229 -4.15266 -4.15981 -4.1638579 -4.1561961][-4.0383053 -4.0602946 -4.0804205 -4.1081004 -4.1387892 -4.1552787 -4.1573048 -4.1599426 -4.1615586 -4.1585207 -4.1388621 -4.1289392 -4.1349621 -4.13544 -4.1310854][-4.0798845 -4.0983834 -4.1143532 -4.139009 -4.1614032 -4.1664991 -4.1601233 -4.1572008 -4.1562657 -4.1460404 -4.1167111 -4.0977049 -4.1023488 -4.1045213 -4.104939][-4.1084557 -4.1229782 -4.1369462 -4.1567197 -4.1727877 -4.17385 -4.1603312 -4.148314 -4.1404386 -4.123795 -4.095747 -4.0782518 -4.0850654 -4.0971227 -4.1051865][-4.1255293 -4.1358423 -4.1470242 -4.156157 -4.1640015 -4.164 -4.1467638 -4.12541 -4.11192 -4.1022248 -4.0897179 -4.083056 -4.0949731 -4.11709 -4.1334181][-4.1241112 -4.1279278 -4.13138 -4.1299996 -4.1337194 -4.1320629 -4.1167994 -4.0920849 -4.0838985 -4.0868921 -4.0855918 -4.0927114 -4.1139951 -4.14006 -4.1561513][-4.1098919 -4.1080341 -4.104146 -4.0968075 -4.0978541 -4.0929966 -4.0790315 -4.0479455 -4.040493 -4.0584908 -4.0702996 -4.0878024 -4.1166291 -4.1392407 -4.1536274][-4.0987024 -4.0906444 -4.0825772 -4.0638518 -4.0452771 -4.0283937 -4.0142536 -3.9791782 -3.9767237 -4.0101528 -4.0397062 -4.0701656 -4.1102452 -4.1323023 -4.1463919][-4.0970926 -4.0796227 -4.05828 -4.007699 -3.9547026 -3.9457703 -3.9557164 -3.9361119 -3.945478 -3.998343 -4.0439243 -4.0754051 -4.1109324 -4.1296968 -4.1404843][-4.0765309 -4.0491481 -4.0285811 -3.9767106 -3.9259114 -3.9429648 -3.9862244 -3.9884725 -3.9964471 -4.0386486 -4.07581 -4.0940247 -4.1171556 -4.1242909 -4.1271806][-4.0487003 -4.0275593 -4.033524 -4.0183773 -3.9984112 -4.0201879 -4.0571609 -4.0633979 -4.0637527 -4.0865459 -4.1113639 -4.1230607 -4.1362634 -4.1387014 -4.1327977][-4.0553694 -4.04263 -4.0596571 -4.0675688 -4.0621877 -4.0737696 -4.0944433 -4.1036592 -4.1001143 -4.1121287 -4.1355133 -4.1530395 -4.1703138 -4.17571 -4.163548][-4.066709 -4.0551472 -4.0713711 -4.0906734 -4.0924606 -4.0952687 -4.104372 -4.108984 -4.1074843 -4.1215067 -4.1530042 -4.1828332 -4.2080374 -4.215169 -4.19904][-4.0730119 -4.0627475 -4.0779829 -4.1019816 -4.1098638 -4.1087074 -4.1108351 -4.1115222 -4.1163263 -4.1345515 -4.1677566 -4.199903 -4.2273669 -4.233438 -4.2183466][-4.0571222 -4.0571938 -4.0732808 -4.0974693 -4.1097169 -4.1033573 -4.1083126 -4.1201773 -4.1321788 -4.1505513 -4.1797323 -4.2103562 -4.2342305 -4.2383766 -4.22481]]...]
INFO - root - 2017-12-06 03:37:45.746582: step 11210, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 20h:38m:13s remains)
INFO - root - 2017-12-06 03:37:48.023179: step 11220, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 19h:34m:40s remains)
INFO - root - 2017-12-06 03:37:50.323129: step 11230, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 20h:03m:12s remains)
INFO - root - 2017-12-06 03:37:52.627254: step 11240, loss = 2.05, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 19h:42m:02s remains)
INFO - root - 2017-12-06 03:37:54.936683: step 11250, loss = 2.05, batch loss = 1.99 (34.2 examples/sec; 0.234 sec/batch; 20h:50m:54s remains)
INFO - root - 2017-12-06 03:37:57.265624: step 11260, loss = 2.07, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 20h:29m:28s remains)
INFO - root - 2017-12-06 03:37:59.580746: step 11270, loss = 2.07, batch loss = 2.01 (33.5 examples/sec; 0.239 sec/batch; 21h:17m:39s remains)
INFO - root - 2017-12-06 03:38:01.860272: step 11280, loss = 2.07, batch loss = 2.01 (33.9 examples/sec; 0.236 sec/batch; 21h:03m:43s remains)
INFO - root - 2017-12-06 03:38:04.200388: step 11290, loss = 2.06, batch loss = 2.00 (30.6 examples/sec; 0.261 sec/batch; 23h:17m:41s remains)
INFO - root - 2017-12-06 03:38:06.514640: step 11300, loss = 2.04, batch loss = 1.98 (33.8 examples/sec; 0.237 sec/batch; 21h:07m:59s remains)
2017-12-06 03:38:06.803111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1695738 -4.2029929 -4.2183943 -4.2236118 -4.2181416 -4.2079239 -4.1957841 -4.1888471 -4.1831813 -4.1620278 -4.1258335 -4.0887933 -4.08526 -4.1014905 -4.1345663][-4.1593652 -4.1933541 -4.2063608 -4.2068577 -4.2035403 -4.1973085 -4.189177 -4.1849403 -4.1788254 -4.156919 -4.1182308 -4.0787044 -4.0791135 -4.0974784 -4.1266017][-4.1426263 -4.1738129 -4.1849127 -4.1829495 -4.1833544 -4.1824374 -4.1772127 -4.1753845 -4.1722169 -4.1551142 -4.1209831 -4.086113 -4.090385 -4.1082745 -4.13441][-4.1304541 -4.154726 -4.165257 -4.1673269 -4.169693 -4.1667814 -4.158514 -4.1545267 -4.1500716 -4.1413603 -4.1216035 -4.1008124 -4.1135907 -4.1333728 -4.1596146][-4.1183934 -4.1372619 -4.1489167 -4.1528621 -4.153769 -4.147357 -4.1328707 -4.123425 -4.1147904 -4.1110249 -4.1046262 -4.1013474 -4.1258764 -4.154067 -4.1855717][-4.0973735 -4.1169143 -4.1328244 -4.1407862 -4.139482 -4.1264791 -4.1058192 -4.0904536 -4.077601 -4.0688682 -4.060379 -4.062674 -4.0969672 -4.1363244 -4.1758442][-4.089499 -4.1120639 -4.1316576 -4.1393027 -4.1325073 -4.1081543 -4.0776806 -4.0568085 -4.0415215 -4.0265684 -4.0072203 -4.0016241 -4.0466104 -4.0986443 -4.140636][-4.0950713 -4.11579 -4.1317453 -4.13378 -4.1224375 -4.0944247 -4.0644336 -4.0474052 -4.0357904 -4.0235829 -4.0038157 -3.9948652 -4.0393028 -4.0895452 -4.1269493][-4.1090965 -4.1239119 -4.1325612 -4.1283784 -4.1175289 -4.0965652 -4.0747385 -4.0648928 -4.0620546 -4.0631819 -4.0589042 -4.0548396 -4.0848036 -4.12401 -4.156754][-4.1182504 -4.1281948 -4.132288 -4.1267157 -4.1176171 -4.1074562 -4.0942287 -4.0868406 -4.0892992 -4.0982475 -4.100769 -4.097733 -4.1188784 -4.1541977 -4.1911783][-4.1161442 -4.1272621 -4.1297817 -4.1235795 -4.11761 -4.1157002 -4.110352 -4.1045938 -4.1074023 -4.1129045 -4.110743 -4.1048369 -4.1234145 -4.1628337 -4.2071781][-4.1043162 -4.1172318 -4.11965 -4.11708 -4.1188693 -4.1257324 -4.1298981 -4.1272669 -4.1239238 -4.1184855 -4.109796 -4.1011271 -4.120079 -4.16573 -4.211637][-4.0902905 -4.1037955 -4.1100707 -4.1120238 -4.12266 -4.1390986 -4.1543031 -4.1562524 -4.1479235 -4.1384625 -4.1273184 -4.1147823 -4.1289716 -4.1699514 -4.2078862][-4.085196 -4.102838 -4.1165748 -4.1237473 -4.1408763 -4.1641212 -4.1838408 -4.1860256 -4.1774335 -4.1700997 -4.1626997 -4.1507406 -4.1572328 -4.1832323 -4.2075453][-4.0911613 -4.1118193 -4.1338177 -4.1486864 -4.1675591 -4.1884918 -4.2079263 -4.2115545 -4.2059588 -4.2034268 -4.201292 -4.1938677 -4.1971273 -4.2096882 -4.217823]]...]
INFO - root - 2017-12-06 03:38:09.092561: step 11310, loss = 2.07, batch loss = 2.01 (33.7 examples/sec; 0.237 sec/batch; 21h:10m:59s remains)
INFO - root - 2017-12-06 03:38:11.404304: step 11320, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:53m:25s remains)
INFO - root - 2017-12-06 03:38:13.691771: step 11330, loss = 2.05, batch loss = 2.00 (33.6 examples/sec; 0.238 sec/batch; 21h:12m:55s remains)
INFO - root - 2017-12-06 03:38:15.974032: step 11340, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 19h:38m:00s remains)
INFO - root - 2017-12-06 03:38:18.266173: step 11350, loss = 2.09, batch loss = 2.03 (33.6 examples/sec; 0.238 sec/batch; 21h:15m:25s remains)
INFO - root - 2017-12-06 03:38:20.569122: step 11360, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 19h:47m:17s remains)
INFO - root - 2017-12-06 03:38:22.894667: step 11370, loss = 2.06, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:10m:13s remains)
INFO - root - 2017-12-06 03:38:25.245641: step 11380, loss = 2.09, batch loss = 2.03 (33.7 examples/sec; 0.237 sec/batch; 21h:10m:53s remains)
INFO - root - 2017-12-06 03:38:27.608052: step 11390, loss = 2.06, batch loss = 2.00 (33.4 examples/sec; 0.240 sec/batch; 21h:22m:26s remains)
INFO - root - 2017-12-06 03:38:29.883780: step 11400, loss = 2.06, batch loss = 2.01 (35.2 examples/sec; 0.228 sec/batch; 20h:17m:37s remains)
2017-12-06 03:38:30.177602: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2938175 -4.3001466 -4.3072577 -4.3126278 -4.3134413 -4.3147249 -4.3154979 -4.3149776 -4.314477 -4.3178329 -4.3232737 -4.3280983 -4.334353 -4.3429747 -4.3522234][-4.2611284 -4.2752681 -4.2889833 -4.2973647 -4.2963009 -4.2941518 -4.2937322 -4.2943707 -4.2954378 -4.2993546 -4.3048525 -4.3104029 -4.3201571 -4.33351 -4.3467197][-4.2239156 -4.2414632 -4.2569728 -4.2651315 -4.2625031 -4.2585812 -4.2563744 -4.2588687 -4.263309 -4.2704325 -4.2802095 -4.2904816 -4.3051248 -4.3235297 -4.3407545][-4.1872129 -4.2019591 -4.2178783 -4.2243819 -4.2175536 -4.2069573 -4.2014523 -4.2051992 -4.2149744 -4.2283673 -4.2458158 -4.2640233 -4.2864857 -4.3126388 -4.3347349][-4.1680746 -4.175632 -4.18561 -4.1840615 -4.1675472 -4.1480303 -4.1410379 -4.1509571 -4.16628 -4.1816187 -4.2015762 -4.2260818 -4.2566538 -4.2918787 -4.3218508][-4.1738033 -4.1712604 -4.168932 -4.15644 -4.1328297 -4.1058216 -4.0982218 -4.1135955 -4.1285634 -4.1395617 -4.1572785 -4.184289 -4.2202325 -4.2625647 -4.3006473][-4.1698651 -4.1593709 -4.1461849 -4.1256938 -4.1027079 -4.085063 -4.0924244 -4.1171789 -4.1305604 -4.1369791 -4.1494951 -4.1715574 -4.2030158 -4.2433805 -4.2818303][-4.1597872 -4.1414652 -4.1226435 -4.1015396 -4.0870161 -4.0869241 -4.1092234 -4.1389642 -4.1528373 -4.1604595 -4.1743584 -4.19217 -4.2145748 -4.2471271 -4.2788219][-4.14844 -4.12732 -4.1100135 -4.0980058 -4.0960774 -4.1069145 -4.1304483 -4.15478 -4.1676159 -4.1814475 -4.200387 -4.2189183 -4.2358952 -4.2621722 -4.2869487][-4.1463566 -4.1287756 -4.117135 -4.1156888 -4.1248193 -4.1388211 -4.1564689 -4.1715255 -4.1799912 -4.1965952 -4.2173009 -4.2343922 -4.2478962 -4.2723575 -4.2952566][-4.1487346 -4.1450539 -4.145174 -4.1506867 -4.1636262 -4.174252 -4.1795206 -4.1801085 -4.180346 -4.1978722 -4.2224092 -4.2411251 -4.254159 -4.276577 -4.2999635][-4.1653838 -4.1719503 -4.1775346 -4.1844025 -4.1963115 -4.20388 -4.1971483 -4.1855927 -4.1817551 -4.2006068 -4.2288823 -4.249784 -4.2623825 -4.2837481 -4.3083844][-4.1987205 -4.2116742 -4.2192831 -4.222455 -4.228013 -4.2299404 -4.2187071 -4.205462 -4.2047558 -4.2240577 -4.2503471 -4.2706485 -4.282249 -4.3012648 -4.3227715][-4.2418332 -4.2566357 -4.2643061 -4.2636 -4.2599807 -4.2582374 -4.2488995 -4.2406974 -4.2443132 -4.2610588 -4.2817483 -4.2982273 -4.30883 -4.3240147 -4.3381591][-4.2776794 -4.2904243 -4.296103 -4.2923627 -4.2863503 -4.2837057 -4.278748 -4.2756915 -4.279458 -4.2901044 -4.3037162 -4.3169394 -4.327775 -4.339715 -4.3493056]]...]
INFO - root - 2017-12-06 03:38:32.438488: step 11410, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 19h:45m:34s remains)
INFO - root - 2017-12-06 03:38:34.732044: step 11420, loss = 2.07, batch loss = 2.01 (33.9 examples/sec; 0.236 sec/batch; 21h:02m:26s remains)
INFO - root - 2017-12-06 03:38:37.040488: step 11430, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:53m:24s remains)
INFO - root - 2017-12-06 03:38:39.360220: step 11440, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 20h:22m:50s remains)
INFO - root - 2017-12-06 03:38:41.656349: step 11450, loss = 2.03, batch loss = 1.97 (34.9 examples/sec; 0.229 sec/batch; 20h:24m:58s remains)
INFO - root - 2017-12-06 03:38:44.011432: step 11460, loss = 2.05, batch loss = 1.99 (33.4 examples/sec; 0.240 sec/batch; 21h:22m:28s remains)
INFO - root - 2017-12-06 03:38:46.293876: step 11470, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.230 sec/batch; 20h:33m:05s remains)
INFO - root - 2017-12-06 03:38:48.592484: step 11480, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 20h:08m:11s remains)
INFO - root - 2017-12-06 03:38:50.912395: step 11490, loss = 2.09, batch loss = 2.03 (33.8 examples/sec; 0.237 sec/batch; 21h:07m:46s remains)
INFO - root - 2017-12-06 03:38:53.211291: step 11500, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 20h:02m:47s remains)
2017-12-06 03:38:53.541729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2710924 -4.2647338 -4.2560759 -4.2474723 -4.238296 -4.2243981 -4.2082586 -4.2013474 -4.21017 -4.2287822 -4.2485633 -4.2626958 -4.274704 -4.2872977 -4.3011394][-4.2669559 -4.2577453 -4.2440081 -4.2299833 -4.2159176 -4.1957965 -4.1735907 -4.1637597 -4.1770954 -4.2021785 -4.2256446 -4.2429614 -4.2587194 -4.2765517 -4.2948937][-4.2628012 -4.2514477 -4.2331057 -4.2133317 -4.1934295 -4.1664739 -4.1379652 -4.1261005 -4.1466103 -4.1794581 -4.206933 -4.227397 -4.2461491 -4.2688551 -4.2915392][-4.2600303 -4.2479491 -4.2266445 -4.2017655 -4.1736412 -4.1367664 -4.0986018 -4.0835738 -4.11242 -4.1532845 -4.1850648 -4.2103233 -4.2352147 -4.263607 -4.29128][-4.2557335 -4.2429848 -4.2178159 -4.185132 -4.1458645 -4.0945215 -4.0410371 -4.0231323 -4.0639877 -4.1163673 -4.1575465 -4.1920967 -4.2264676 -4.2618065 -4.292901][-4.2396507 -4.226439 -4.1979513 -4.1575832 -4.1050978 -4.0342312 -3.9607253 -3.9427 -4.002872 -4.0735083 -4.1293073 -4.1758556 -4.2189455 -4.2599897 -4.292995][-4.2170563 -4.2033668 -4.1737747 -4.130271 -4.0684214 -3.9805155 -3.8859375 -3.8707418 -3.9534464 -4.0422864 -4.1106939 -4.1651077 -4.2130876 -4.2563629 -4.2901907][-4.2038269 -4.1958194 -4.1735673 -4.1363955 -4.0786171 -3.9909296 -3.8945355 -3.8875322 -3.9727347 -4.0607414 -4.1274414 -4.1768413 -4.2193804 -4.2576871 -4.2876348][-4.216063 -4.2192669 -4.2099495 -4.184813 -4.1388183 -4.0652919 -3.985239 -3.9809093 -4.0440903 -4.1115866 -4.1638713 -4.2009425 -4.2355042 -4.2664719 -4.2901592][-4.2324991 -4.244823 -4.2460465 -4.2310467 -4.19663 -4.1397276 -4.0755587 -4.0662937 -4.1015096 -4.1459022 -4.1847711 -4.2145152 -4.2473025 -4.2745404 -4.29373][-4.2406178 -4.2568026 -4.2619543 -4.2535639 -4.2295136 -4.187325 -4.1338363 -4.1169815 -4.1311188 -4.1579385 -4.1880374 -4.2160034 -4.2506948 -4.2792253 -4.2982607][-4.2350311 -4.2505355 -4.2544913 -4.2498164 -4.234303 -4.2031927 -4.1590309 -4.1402235 -4.14503 -4.1620531 -4.1851425 -4.2144604 -4.2532668 -4.2856903 -4.3051248][-4.2223182 -4.2361183 -4.2388091 -4.2362652 -4.2289848 -4.206903 -4.1723819 -4.1576266 -4.1591506 -4.1703467 -4.1884818 -4.2195807 -4.2615643 -4.2969317 -4.3143406][-4.2218356 -4.2312965 -4.2291951 -4.227149 -4.2231183 -4.2086091 -4.1857224 -4.176681 -4.1756167 -4.1811872 -4.1946144 -4.2275062 -4.2718015 -4.30745 -4.3216438][-4.2437515 -4.2454009 -4.2366829 -4.2321606 -4.2268195 -4.2124286 -4.1921024 -4.1833792 -4.1798081 -4.183485 -4.1969056 -4.2328115 -4.2777014 -4.3123374 -4.3249283]]...]
INFO - root - 2017-12-06 03:38:55.849319: step 11510, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.235 sec/batch; 20h:56m:14s remains)
INFO - root - 2017-12-06 03:38:58.136041: step 11520, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 20h:23m:54s remains)
INFO - root - 2017-12-06 03:39:00.419586: step 11530, loss = 2.04, batch loss = 1.98 (35.6 examples/sec; 0.225 sec/batch; 20h:03m:09s remains)
INFO - root - 2017-12-06 03:39:02.722588: step 11540, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 20h:09m:13s remains)
INFO - root - 2017-12-06 03:39:05.041598: step 11550, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:08m:25s remains)
INFO - root - 2017-12-06 03:39:07.318228: step 11560, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.226 sec/batch; 20h:11m:31s remains)
INFO - root - 2017-12-06 03:39:09.621936: step 11570, loss = 2.02, batch loss = 1.97 (35.4 examples/sec; 0.226 sec/batch; 20h:08m:45s remains)
INFO - root - 2017-12-06 03:39:11.908550: step 11580, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:18m:49s remains)
INFO - root - 2017-12-06 03:39:14.261223: step 11590, loss = 2.05, batch loss = 2.00 (31.8 examples/sec; 0.252 sec/batch; 22h:26m:43s remains)
INFO - root - 2017-12-06 03:39:16.543352: step 11600, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.228 sec/batch; 20h:16m:57s remains)
2017-12-06 03:39:16.812570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2949724 -4.2999573 -4.30773 -4.3151073 -4.3192744 -4.3230205 -4.3274231 -4.3300705 -4.3319426 -4.3313174 -4.3289304 -4.3293843 -4.3313684 -4.3343821 -4.3383517][-4.2595587 -4.2655354 -4.2765961 -4.2852659 -4.2873425 -4.2891369 -4.2958879 -4.3007536 -4.3029685 -4.3028784 -4.3013439 -4.303761 -4.3087621 -4.31609 -4.3245339][-4.2154808 -4.2191234 -4.2336135 -4.2462459 -4.2493382 -4.2496524 -4.2550216 -4.2590756 -4.2611222 -4.2644467 -4.2687774 -4.276021 -4.2847552 -4.2956543 -4.3075728][-4.1565638 -4.1530294 -4.1685143 -4.1868377 -4.1952829 -4.1960754 -4.1998086 -4.1997671 -4.19976 -4.2087603 -4.2230692 -4.2395525 -4.254528 -4.2670841 -4.2822237][-4.1027875 -4.0914607 -4.1017523 -4.1203918 -4.1292038 -4.1254892 -4.1215262 -4.117908 -4.1228275 -4.1437936 -4.1733994 -4.2026548 -4.2260075 -4.2439728 -4.2628584][-4.0575557 -4.0371189 -4.0364175 -4.0461407 -4.0477157 -4.0334072 -4.0127091 -3.9984422 -4.0099339 -4.050498 -4.101562 -4.147706 -4.1870422 -4.2165484 -4.2396641][-4.0423474 -4.0126028 -4.0016041 -3.9967408 -3.9829915 -3.9577272 -3.9122424 -3.8742959 -3.8865392 -3.9464998 -4.0198 -4.0872788 -4.1471238 -4.1914134 -4.2229953][-4.0650625 -4.0331626 -4.0172186 -4.0053329 -3.9820967 -3.9488246 -3.8828173 -3.8178103 -3.8258963 -3.893723 -3.9779069 -4.0564451 -4.1258135 -4.1801653 -4.2181997][-4.0930595 -4.0684628 -4.0585685 -4.0521107 -4.0334888 -4.0028224 -3.9412122 -3.8763375 -3.8765528 -3.9288173 -3.9986656 -4.0700808 -4.135231 -4.189971 -4.2255917][-4.1112633 -4.0962439 -4.0939407 -4.0959058 -4.0832796 -4.05668 -4.0063529 -3.9584014 -3.9538255 -3.9828842 -4.0308208 -4.09159 -4.1525288 -4.2065511 -4.2386689][-4.1268721 -4.11692 -4.1185894 -4.1227555 -4.1114326 -4.0906262 -4.0502038 -4.0136714 -4.0073452 -4.0222335 -4.0557489 -4.1075196 -4.163187 -4.2126551 -4.2433968][-4.153132 -4.1462526 -4.149137 -4.1521888 -4.1404915 -4.1211696 -4.0871172 -4.0572944 -4.0516424 -4.0609984 -4.0874414 -4.1290274 -4.1745796 -4.2159853 -4.2465739][-4.2040744 -4.2001572 -4.2038136 -4.2053595 -4.19189 -4.1726046 -4.1421428 -4.1162367 -4.10718 -4.1122732 -4.1350441 -4.166966 -4.2005997 -4.2340288 -4.2615433][-4.2644882 -4.2615161 -4.2650733 -4.266346 -4.2556267 -4.2405639 -4.21881 -4.1995921 -4.1895728 -4.1913733 -4.2059107 -4.2267828 -4.2485476 -4.27237 -4.2918897][-4.3075628 -4.3043561 -4.3067303 -4.3086743 -4.3035607 -4.2959681 -4.2845249 -4.2744246 -4.2685132 -4.2688093 -4.2764792 -4.2865448 -4.2970119 -4.310679 -4.3225918]]...]
INFO - root - 2017-12-06 03:39:19.089964: step 11610, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.225 sec/batch; 20h:04m:09s remains)
INFO - root - 2017-12-06 03:39:21.382628: step 11620, loss = 2.09, batch loss = 2.03 (30.7 examples/sec; 0.261 sec/batch; 23h:14m:40s remains)
INFO - root - 2017-12-06 03:39:23.696891: step 11630, loss = 2.06, batch loss = 2.00 (32.9 examples/sec; 0.243 sec/batch; 21h:40m:33s remains)
INFO - root - 2017-12-06 03:39:25.987491: step 11640, loss = 2.07, batch loss = 2.01 (36.1 examples/sec; 0.221 sec/batch; 19h:43m:43s remains)
INFO - root - 2017-12-06 03:39:28.288163: step 11650, loss = 2.06, batch loss = 2.00 (34.0 examples/sec; 0.235 sec/batch; 20h:57m:52s remains)
INFO - root - 2017-12-06 03:39:30.600611: step 11660, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:19m:46s remains)
INFO - root - 2017-12-06 03:39:32.864977: step 11670, loss = 2.09, batch loss = 2.03 (35.3 examples/sec; 0.226 sec/batch; 20h:10m:48s remains)
INFO - root - 2017-12-06 03:39:35.187069: step 11680, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.226 sec/batch; 20h:10m:43s remains)
INFO - root - 2017-12-06 03:39:37.484049: step 11690, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 20h:02m:50s remains)
INFO - root - 2017-12-06 03:39:39.767171: step 11700, loss = 2.05, batch loss = 1.99 (34.0 examples/sec; 0.235 sec/batch; 20h:56m:30s remains)
2017-12-06 03:39:40.068760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.136097 -4.1186237 -4.1249862 -4.1463656 -4.1650996 -4.1712646 -4.16379 -4.144084 -4.1368732 -4.141994 -4.1474285 -4.134263 -4.1045685 -4.0952191 -4.1115208][-4.1424255 -4.1181345 -4.1206079 -4.1444607 -4.1646891 -4.1683378 -4.1584125 -4.1416583 -4.1357713 -4.1461792 -4.1591759 -4.15805 -4.1365581 -4.1247716 -4.1297088][-4.1422825 -4.1189442 -4.124198 -4.1502943 -4.1671844 -4.1678977 -4.1590352 -4.1459079 -4.1403079 -4.1535645 -4.1715512 -4.1770716 -4.163466 -4.1539087 -4.151969][-4.14256 -4.1229892 -4.131124 -4.1574292 -4.1701207 -4.1684971 -4.1624575 -4.1547937 -4.1531382 -4.1683397 -4.1861582 -4.1929741 -4.1857619 -4.18009 -4.17278][-4.1291041 -4.1132455 -4.1219058 -4.1481709 -4.1615062 -4.1595173 -4.1548429 -4.1509809 -4.1591535 -4.1799135 -4.1936741 -4.1964645 -4.1930814 -4.1959829 -4.1914635][-4.1019526 -4.0890059 -4.0994463 -4.1272745 -4.1389418 -4.1274552 -4.1121783 -4.1108723 -4.1352339 -4.167141 -4.1843615 -4.1843762 -4.1857605 -4.1994638 -4.2012668][-4.0570602 -4.0438466 -4.057611 -4.0873494 -4.0921097 -4.0574765 -4.0166669 -4.0184369 -4.0676169 -4.1205096 -4.1511083 -4.1578827 -4.1643071 -4.1837974 -4.191134][-4.0281734 -4.0153928 -4.0288363 -4.05048 -4.0385461 -3.9739544 -3.8994224 -3.9060051 -3.9852858 -4.062633 -4.1086407 -4.1265478 -4.1376209 -4.15614 -4.1631079][-4.0441232 -4.0361328 -4.0503869 -4.0628448 -4.0391665 -3.9588013 -3.8710537 -3.8755972 -3.959585 -4.0372567 -4.0805616 -4.10297 -4.1163912 -4.130981 -4.1313314][-4.0875216 -4.0867715 -4.1026044 -4.116354 -4.099493 -4.0374937 -3.9729791 -3.9714124 -4.0221953 -4.0704455 -4.0951371 -4.1099191 -4.1224346 -4.13409 -4.1308417][-4.1380763 -4.1427507 -4.1569881 -4.1716132 -4.1649494 -4.1281934 -4.087646 -4.0791025 -4.0986977 -4.1201434 -4.1318464 -4.140923 -4.1506314 -4.1621065 -4.1635714][-4.1805325 -4.189353 -4.2038021 -4.2172742 -4.2151313 -4.1915317 -4.16072 -4.1431308 -4.1388755 -4.144155 -4.1498547 -4.1562986 -4.167068 -4.1809258 -4.1907845][-4.2110944 -4.2194018 -4.2315111 -4.242527 -4.239625 -4.2166986 -4.1876235 -4.1674128 -4.1522541 -4.1499705 -4.1569519 -4.1659055 -4.1785755 -4.1920691 -4.2052317][-4.2404737 -4.2415915 -4.2443013 -4.2478828 -4.2381225 -4.2120867 -4.1853733 -4.1697822 -4.153697 -4.151176 -4.1625128 -4.1754665 -4.1915555 -4.2064319 -4.220696][-4.2627435 -4.2518387 -4.2443447 -4.242507 -4.2271814 -4.195015 -4.1674132 -4.1537542 -4.141932 -4.1418338 -4.1584768 -4.1795192 -4.1994081 -4.2147088 -4.2251978]]...]
INFO - root - 2017-12-06 03:39:42.384829: step 11710, loss = 2.06, batch loss = 2.00 (32.2 examples/sec; 0.249 sec/batch; 22h:09m:58s remains)
INFO - root - 2017-12-06 03:39:44.712590: step 11720, loss = 2.09, batch loss = 2.03 (33.9 examples/sec; 0.236 sec/batch; 21h:02m:06s remains)
INFO - root - 2017-12-06 03:39:47.042303: step 11730, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 20h:25m:19s remains)
INFO - root - 2017-12-06 03:39:49.344030: step 11740, loss = 2.05, batch loss = 2.00 (33.7 examples/sec; 0.237 sec/batch; 21h:07m:49s remains)
INFO - root - 2017-12-06 03:39:51.646258: step 11750, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:47m:51s remains)
INFO - root - 2017-12-06 03:39:53.945635: step 11760, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 19h:33m:51s remains)
INFO - root - 2017-12-06 03:39:56.192423: step 11770, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 20h:07m:12s remains)
INFO - root - 2017-12-06 03:39:58.526636: step 11780, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 19h:34m:50s remains)
INFO - root - 2017-12-06 03:40:00.810287: step 11790, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:28m:21s remains)
INFO - root - 2017-12-06 03:40:03.128723: step 11800, loss = 2.07, batch loss = 2.01 (34.4 examples/sec; 0.232 sec/batch; 20h:41m:33s remains)
2017-12-06 03:40:03.403904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1906404 -4.1921315 -4.2124839 -4.21857 -4.2168646 -4.2166305 -4.2149715 -4.2192707 -4.2302 -4.2404385 -4.2583179 -4.2739873 -4.2770786 -4.2793951 -4.2902412][-4.1447496 -4.1535339 -4.1843266 -4.1973152 -4.1990032 -4.2019181 -4.2050385 -4.216362 -4.2307062 -4.2375751 -4.2531362 -4.2715631 -4.2773628 -4.2775288 -4.2837629][-4.113668 -4.1208615 -4.151279 -4.1658974 -4.1724348 -4.1817436 -4.1933074 -4.2160306 -4.2396903 -4.2489953 -4.2640219 -4.2828979 -4.2892709 -4.2842355 -4.28279][-4.0963092 -4.0901146 -4.1065621 -4.1149597 -4.1239533 -4.1379752 -4.1560416 -4.1901083 -4.2285337 -4.2503929 -4.2712474 -4.2936044 -4.3020935 -4.2943 -4.2890248][-4.0874214 -4.074183 -4.07833 -4.0773869 -4.0806618 -4.088778 -4.1045656 -4.1427431 -4.1906633 -4.2253327 -4.25489 -4.2856908 -4.3011174 -4.29884 -4.2966466][-4.0924664 -4.0839553 -4.0814776 -4.0713058 -4.0674553 -4.0678043 -4.0734348 -4.1043711 -4.1527629 -4.1934032 -4.2247896 -4.2589941 -4.2790942 -4.2847147 -4.2900853][-4.0829997 -4.082818 -4.0827827 -4.0702505 -4.0665121 -4.0619292 -4.0611467 -4.0813165 -4.1215606 -4.1592474 -4.1858482 -4.2151465 -4.2376623 -4.2522149 -4.2693543][-4.0486 -4.05073 -4.053093 -4.0414915 -4.0377169 -4.0316672 -4.0314188 -4.0524864 -4.0925064 -4.12764 -4.1507583 -4.1734204 -4.192492 -4.2124624 -4.2394533][-4.0241117 -4.02413 -4.0270987 -4.0160956 -4.00518 -3.9971488 -4.0006418 -4.0280509 -4.0696549 -4.1046386 -4.1295185 -4.1503658 -4.1660914 -4.1852412 -4.2168097][-4.0369663 -4.0319405 -4.0375857 -4.0298314 -4.0152359 -4.005301 -4.0084534 -4.03161 -4.0655694 -4.0954752 -4.1194773 -4.1379027 -4.1507583 -4.1681137 -4.2040305][-4.0748534 -4.0674467 -4.0705547 -4.0639596 -4.0503979 -4.0435672 -4.0469441 -4.0638871 -4.0849738 -4.1020427 -4.1181455 -4.1321125 -4.1428537 -4.1597686 -4.2004][-4.0999751 -4.094749 -4.09807 -4.0922766 -4.0825596 -4.0802555 -4.0851464 -4.098937 -4.1113648 -4.1181245 -4.1275635 -4.1384344 -4.1469235 -4.1636896 -4.2046537][-4.1405973 -4.1375747 -4.1405396 -4.1375751 -4.1325545 -4.1325803 -4.1375885 -4.1472054 -4.1527224 -4.1531105 -4.1578279 -4.163702 -4.1681738 -4.1813641 -4.2158136][-4.2059312 -4.2046571 -4.206852 -4.2067223 -4.2059216 -4.2065992 -4.2087121 -4.2124877 -4.2129564 -4.2106013 -4.2122526 -4.2128134 -4.2121239 -4.217227 -4.2388153][-4.2577453 -4.2583532 -4.2598987 -4.2604036 -4.2604876 -4.2601638 -4.2601829 -4.2617707 -4.2623987 -4.2607093 -4.2608514 -4.2604933 -4.2590351 -4.2603717 -4.2718658]]...]
INFO - root - 2017-12-06 03:40:05.700207: step 11810, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:14m:20s remains)
INFO - root - 2017-12-06 03:40:07.978133: step 11820, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 20h:17m:22s remains)
INFO - root - 2017-12-06 03:40:10.286923: step 11830, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.230 sec/batch; 20h:26m:36s remains)
INFO - root - 2017-12-06 03:40:12.618126: step 11840, loss = 2.08, batch loss = 2.03 (34.5 examples/sec; 0.232 sec/batch; 20h:38m:09s remains)
INFO - root - 2017-12-06 03:40:14.924595: step 11850, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 19h:34m:16s remains)
INFO - root - 2017-12-06 03:40:17.202746: step 11860, loss = 2.03, batch loss = 1.97 (33.8 examples/sec; 0.237 sec/batch; 21h:06m:28s remains)
INFO - root - 2017-12-06 03:40:19.471398: step 11870, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 20h:01m:00s remains)
INFO - root - 2017-12-06 03:40:21.784463: step 11880, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:01m:01s remains)
INFO - root - 2017-12-06 03:40:24.091882: step 11890, loss = 2.06, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 20h:23m:01s remains)
INFO - root - 2017-12-06 03:40:26.355994: step 11900, loss = 2.05, batch loss = 1.99 (34.1 examples/sec; 0.235 sec/batch; 20h:54m:24s remains)
2017-12-06 03:40:26.651655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1824937 -4.16412 -4.1522813 -4.1482592 -4.1436296 -4.1379542 -4.132782 -4.1392174 -4.1668124 -4.2017922 -4.2184749 -4.1998711 -4.1715021 -4.1458545 -4.1404285][-4.1856604 -4.1750512 -4.1715584 -4.1692548 -4.1615725 -4.1469517 -4.1375484 -4.1414528 -4.1600857 -4.1929016 -4.2185173 -4.2094941 -4.1793213 -4.1460009 -4.1300826][-4.1857553 -4.1872892 -4.1924658 -4.190443 -4.176424 -4.1563573 -4.1429434 -4.1422668 -4.1535363 -4.18583 -4.2185049 -4.2180204 -4.1854281 -4.1436291 -4.1142349][-4.1850758 -4.1930995 -4.2022457 -4.2048721 -4.191123 -4.1654506 -4.1365151 -4.1232648 -4.1357388 -4.1733446 -4.211184 -4.2196813 -4.19156 -4.1503377 -4.11489][-4.1788583 -4.1870766 -4.19801 -4.2071939 -4.1975126 -4.1592665 -4.1058321 -4.0830722 -4.1061511 -4.1509571 -4.1895056 -4.2080007 -4.1929755 -4.1604056 -4.1291828][-4.1740932 -4.1759939 -4.1880083 -4.2003083 -4.1862364 -4.1250043 -4.0437183 -4.020947 -4.0700817 -4.1296105 -4.1676879 -4.1937466 -4.1948075 -4.1765852 -4.1549692][-4.169837 -4.1649737 -4.1756697 -4.1874714 -4.1612868 -4.0640807 -3.9431715 -3.9342 -4.0250988 -4.1084313 -4.15353 -4.1819773 -4.1938562 -4.19035 -4.1787968][-4.1683717 -4.1599274 -4.1698146 -4.178638 -4.1397161 -4.0181494 -3.8589377 -3.8561172 -3.9881644 -4.0970974 -4.1527777 -4.1791615 -4.19256 -4.1966643 -4.19323][-4.1633158 -4.1572418 -4.1655612 -4.1749897 -4.1421289 -4.0377078 -3.8958483 -3.8750873 -3.9960949 -4.1053696 -4.160481 -4.180757 -4.18822 -4.1929731 -4.1954126][-4.1577406 -4.1548924 -4.1554732 -4.1664314 -4.1483593 -4.0846133 -3.9942553 -3.9681904 -4.0414047 -4.1242437 -4.1697564 -4.1836414 -4.1820827 -4.1835842 -4.1846271][-4.1512923 -4.1510668 -4.1472054 -4.1525784 -4.146935 -4.1139441 -4.0665188 -4.0515933 -4.0852113 -4.1382108 -4.1755953 -4.186285 -4.1743011 -4.1689425 -4.1659021][-4.1536241 -4.1523128 -4.1415248 -4.1397824 -4.1391516 -4.1262355 -4.10355 -4.0980644 -4.110384 -4.1414056 -4.1711011 -4.1836534 -4.1682105 -4.1530895 -4.1429911][-4.1617017 -4.154839 -4.137105 -4.1303916 -4.132338 -4.131444 -4.1216679 -4.1183453 -4.1205745 -4.1366453 -4.1604505 -4.1760674 -4.1659746 -4.1479158 -4.1299515][-4.1781321 -4.1691985 -4.1497426 -4.1368284 -4.134778 -4.1387091 -4.1266522 -4.1197138 -4.1230783 -4.1352005 -4.1528692 -4.1689324 -4.1670122 -4.1538978 -4.1333623][-4.1999321 -4.1903062 -4.1721969 -4.1573906 -4.1471457 -4.1416178 -4.1233954 -4.1120768 -4.1252489 -4.1430049 -4.1578345 -4.172605 -4.1747522 -4.1655693 -4.1494555]]...]
INFO - root - 2017-12-06 03:40:28.981594: step 11910, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:14m:10s remains)
INFO - root - 2017-12-06 03:40:31.320226: step 11920, loss = 2.06, batch loss = 2.00 (33.4 examples/sec; 0.240 sec/batch; 21h:21m:40s remains)
INFO - root - 2017-12-06 03:40:33.614439: step 11930, loss = 2.07, batch loss = 2.02 (36.8 examples/sec; 0.217 sec/batch; 19h:21m:39s remains)
INFO - root - 2017-12-06 03:40:35.912479: step 11940, loss = 2.05, batch loss = 2.00 (32.9 examples/sec; 0.243 sec/batch; 21h:40m:13s remains)
INFO - root - 2017-12-06 03:40:38.243150: step 11950, loss = 2.04, batch loss = 1.98 (34.4 examples/sec; 0.233 sec/batch; 20h:42m:27s remains)
INFO - root - 2017-12-06 03:40:40.559420: step 11960, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.226 sec/batch; 20h:09m:03s remains)
INFO - root - 2017-12-06 03:40:42.856652: step 11970, loss = 2.05, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 19h:53m:32s remains)
INFO - root - 2017-12-06 03:40:45.170466: step 11980, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 19h:34m:08s remains)
INFO - root - 2017-12-06 03:40:47.475069: step 11990, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:50m:52s remains)
INFO - root - 2017-12-06 03:40:49.759447: step 12000, loss = 2.06, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:07m:46s remains)
2017-12-06 03:40:50.050798: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2453065 -4.2361221 -4.2310605 -4.22883 -4.2343993 -4.2375026 -4.2366028 -4.2326994 -4.2342081 -4.2426291 -4.2471938 -4.2480464 -4.2462635 -4.2433734 -4.2435164][-4.2185426 -4.211113 -4.2057605 -4.2031097 -4.2073574 -4.2067647 -4.198657 -4.1865697 -4.18608 -4.2009816 -4.2123032 -4.2160678 -4.2135506 -4.2067709 -4.2082849][-4.1925817 -4.1869464 -4.181551 -4.1778455 -4.1772442 -4.1696725 -4.1538367 -4.1321578 -4.1287885 -4.1460958 -4.163064 -4.1709042 -4.1701612 -4.1637292 -4.169764][-4.1709623 -4.1662831 -4.1614614 -4.158267 -4.1534386 -4.139915 -4.1190915 -4.0911317 -4.0841737 -4.0974278 -4.11142 -4.1224823 -4.1292095 -4.1315155 -4.1428566][-4.1663728 -4.1606836 -4.1499119 -4.1446066 -4.1370769 -4.1218882 -4.1006603 -4.0729017 -4.0579128 -4.0557466 -4.0587854 -4.079237 -4.1027551 -4.1162333 -4.1306915][-4.172905 -4.1582785 -4.1353617 -4.1241636 -4.1191936 -4.1064687 -4.0868011 -4.0573363 -4.0297966 -4.0063329 -3.9993291 -4.0310135 -4.0753303 -4.1039534 -4.1218014][-4.1778164 -4.158083 -4.1310759 -4.1163359 -4.1132793 -4.1006351 -4.0760007 -4.0441914 -4.0041809 -3.9603457 -3.9425745 -3.9873347 -4.050488 -4.0954804 -4.121871][-4.1817536 -4.16653 -4.1459351 -4.130631 -4.1242766 -4.1058087 -4.0785947 -4.0508909 -4.0084019 -3.9558809 -3.9343452 -3.9810951 -4.0460539 -4.0923729 -4.1266623][-4.1846442 -4.1754565 -4.1615667 -4.144433 -4.13459 -4.1157894 -4.0956631 -4.0747685 -4.0403996 -3.9994459 -3.9927104 -4.0323262 -4.0743127 -4.1052933 -4.1437821][-4.1851306 -4.1697507 -4.1510444 -4.1297908 -4.1244841 -4.1210203 -4.1177735 -4.1105628 -4.0908442 -4.0711355 -4.0749989 -4.094831 -4.1048689 -4.1154776 -4.1489115][-4.1896081 -4.1607556 -4.12607 -4.0986772 -4.1013412 -4.120276 -4.1362658 -4.1417875 -4.1308556 -4.1255193 -4.1295547 -4.1283484 -4.1144924 -4.1116347 -4.13645][-4.1960363 -4.1595907 -4.1127176 -4.0793262 -4.0874009 -4.1232834 -4.1471958 -4.1568918 -4.1516533 -4.15177 -4.1502771 -4.1295977 -4.1002932 -4.09557 -4.1141863][-4.2165861 -4.1852093 -4.1383305 -4.1005778 -4.1051445 -4.1399965 -4.1603923 -4.1700749 -4.1684847 -4.1643071 -4.1550021 -4.1258607 -4.0962677 -4.0971508 -4.1121607][-4.2399797 -4.2162323 -4.1764526 -4.1375895 -4.13895 -4.1697736 -4.1817608 -4.186933 -4.1863184 -4.181036 -4.1731381 -4.1536989 -4.1347766 -4.1415648 -4.1512637][-4.2601404 -4.2407093 -4.2077 -4.1732841 -4.1731896 -4.1913724 -4.1946917 -4.2004819 -4.2041979 -4.205647 -4.207211 -4.2012305 -4.1906824 -4.1951618 -4.1949248]]...]
INFO - root - 2017-12-06 03:40:52.370296: step 12010, loss = 2.03, batch loss = 1.97 (35.6 examples/sec; 0.224 sec/batch; 19h:58m:52s remains)
INFO - root - 2017-12-06 03:40:54.652400: step 12020, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 20h:50m:59s remains)
INFO - root - 2017-12-06 03:40:56.996789: step 12030, loss = 2.04, batch loss = 1.98 (35.2 examples/sec; 0.227 sec/batch; 20h:12m:39s remains)
INFO - root - 2017-12-06 03:40:59.273914: step 12040, loss = 2.03, batch loss = 1.97 (34.7 examples/sec; 0.230 sec/batch; 20h:30m:11s remains)
INFO - root - 2017-12-06 03:41:01.586274: step 12050, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:33m:26s remains)
INFO - root - 2017-12-06 03:41:03.873441: step 12060, loss = 2.07, batch loss = 2.01 (36.5 examples/sec; 0.219 sec/batch; 19h:31m:28s remains)
INFO - root - 2017-12-06 03:41:06.164391: step 12070, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:56m:48s remains)
INFO - root - 2017-12-06 03:41:08.443156: step 12080, loss = 2.07, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 20h:03m:40s remains)
INFO - root - 2017-12-06 03:41:10.780788: step 12090, loss = 2.09, batch loss = 2.03 (34.2 examples/sec; 0.234 sec/batch; 20h:49m:16s remains)
INFO - root - 2017-12-06 03:41:13.102906: step 12100, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 20h:10m:55s remains)
2017-12-06 03:41:13.410145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2581568 -4.2626224 -4.2531013 -4.2393317 -4.2358 -4.2470894 -4.2565594 -4.2513018 -4.2277036 -4.1983724 -4.1835947 -4.20252 -4.235116 -4.2610211 -4.2711596][-4.2476115 -4.2482243 -4.2336655 -4.2168202 -4.2125525 -4.2207346 -4.22282 -4.2077184 -4.1764817 -4.1454062 -4.1363816 -4.1632791 -4.2022209 -4.2307296 -4.2434769][-4.2294173 -4.2238841 -4.2019529 -4.1789556 -4.1714883 -4.1770935 -4.1759109 -4.1552978 -4.125586 -4.1035576 -4.1045365 -4.1369152 -4.1763477 -4.2035704 -4.2198033][-4.2055154 -4.1902604 -4.1574435 -4.1276321 -4.1208515 -4.1279306 -4.12855 -4.1129613 -4.0958567 -4.0912218 -4.1048951 -4.1385527 -4.1731553 -4.1984377 -4.2170815][-4.1839094 -4.1600571 -4.1211224 -4.090168 -4.0835562 -4.0905437 -4.0938368 -4.0898566 -4.0912061 -4.1052785 -4.1311893 -4.1659732 -4.1964164 -4.2196641 -4.2368646][-4.1826439 -4.1578221 -4.1221676 -4.0930448 -4.0801558 -4.0803432 -4.0831728 -4.0913668 -4.109561 -4.1368217 -4.1687045 -4.2030659 -4.2286386 -4.2455959 -4.2582741][-4.1980948 -4.1739864 -4.1416817 -4.1114435 -4.0915852 -4.0897746 -4.0955787 -4.1141663 -4.1410136 -4.1697917 -4.1991849 -4.2267151 -4.2447176 -4.2551174 -4.2638078][-4.207818 -4.18595 -4.157032 -4.126308 -4.1032023 -4.101604 -4.1118469 -4.1391344 -4.1718812 -4.1954646 -4.2142572 -4.2304211 -4.24108 -4.2442236 -4.2452021][-4.1955953 -4.1786256 -4.156673 -4.1298022 -4.1099887 -4.1105342 -4.1249032 -4.1601906 -4.1946149 -4.2083011 -4.2134557 -4.2194781 -4.2236938 -4.215951 -4.2070441][-4.1633844 -4.1530738 -4.1442695 -4.1310358 -4.1218281 -4.1245561 -4.1422076 -4.1775169 -4.2050519 -4.2086291 -4.2061672 -4.2081418 -4.205 -4.1860595 -4.1692333][-4.1424179 -4.1353674 -4.137248 -4.1378675 -4.1385508 -4.1459684 -4.1663127 -4.1948934 -4.210885 -4.2064939 -4.202951 -4.2022018 -4.1921253 -4.1637163 -4.1413546][-4.1442056 -4.1365905 -4.1416807 -4.1505013 -4.1572084 -4.1689253 -4.1915364 -4.2128267 -4.217164 -4.20778 -4.2036562 -4.1964679 -4.179738 -4.1448069 -4.1219659][-4.1671619 -4.1605358 -4.16321 -4.1710563 -4.1753731 -4.1868534 -4.2076545 -4.2231116 -4.2196326 -4.2094841 -4.2062783 -4.1937318 -4.1693273 -4.1316123 -4.1155748][-4.1960897 -4.1885185 -4.1866431 -4.1877756 -4.1865644 -4.1944423 -4.2094879 -4.2170024 -4.2078228 -4.1981459 -4.1978254 -4.1825247 -4.1541348 -4.1232548 -4.1223068][-4.2089782 -4.1996217 -4.1983213 -4.1982 -4.1966343 -4.2007256 -4.2067752 -4.2030644 -4.1869388 -4.1767235 -4.1784835 -4.1654248 -4.14281 -4.1272345 -4.1419086]]...]
INFO - root - 2017-12-06 03:41:15.687726: step 12110, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.221 sec/batch; 19h:37m:44s remains)
INFO - root - 2017-12-06 03:41:18.007448: step 12120, loss = 2.06, batch loss = 2.00 (33.4 examples/sec; 0.240 sec/batch; 21h:20m:31s remains)
INFO - root - 2017-12-06 03:41:20.323210: step 12130, loss = 2.04, batch loss = 1.98 (34.8 examples/sec; 0.230 sec/batch; 20h:25m:52s remains)
INFO - root - 2017-12-06 03:41:22.691487: step 12140, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.233 sec/batch; 20h:41m:46s remains)
INFO - root - 2017-12-06 03:41:24.968609: step 12150, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.232 sec/batch; 20h:36m:09s remains)
INFO - root - 2017-12-06 03:41:27.284446: step 12160, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 20h:00m:16s remains)
INFO - root - 2017-12-06 03:41:29.573389: step 12170, loss = 2.05, batch loss = 1.99 (34.1 examples/sec; 0.234 sec/batch; 20h:50m:57s remains)
INFO - root - 2017-12-06 03:41:31.833870: step 12180, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 20h:27m:07s remains)
INFO - root - 2017-12-06 03:41:34.155243: step 12190, loss = 2.04, batch loss = 1.99 (33.8 examples/sec; 0.237 sec/batch; 21h:04m:31s remains)
INFO - root - 2017-12-06 03:41:36.469937: step 12200, loss = 2.05, batch loss = 1.99 (33.5 examples/sec; 0.239 sec/batch; 21h:16m:05s remains)
2017-12-06 03:41:36.764411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2781291 -4.2868824 -4.2948995 -4.2998586 -4.3019309 -4.3048205 -4.3088837 -4.3140526 -4.3181963 -4.3194671 -4.3167024 -4.3120403 -4.3091841 -4.3084817 -4.3077526][-4.2642846 -4.2755318 -4.288218 -4.2960029 -4.2979107 -4.2980266 -4.2978363 -4.3025317 -4.3114386 -4.3179121 -4.3187261 -4.3152356 -4.3121028 -4.3116364 -4.3105588][-4.236156 -4.247716 -4.2640429 -4.2734985 -4.2723861 -4.2660503 -4.2582841 -4.2617722 -4.2784643 -4.2947392 -4.3045831 -4.3081374 -4.30981 -4.3132758 -4.3135304][-4.1950688 -4.2015319 -4.2170868 -4.224565 -4.2170076 -4.1999311 -4.1817179 -4.1838884 -4.2106266 -4.2416182 -4.265871 -4.2815623 -4.2924709 -4.3008065 -4.3030028][-4.1456537 -4.1411576 -4.1534009 -4.1575575 -4.1427755 -4.1130648 -4.08207 -4.0777612 -4.1131325 -4.161159 -4.2031536 -4.2330723 -4.2542014 -4.2667041 -4.2676792][-4.0966148 -4.0772195 -4.08124 -4.0812826 -4.0588365 -4.0144453 -3.961077 -3.9359288 -3.977808 -4.0507841 -4.1185956 -4.167233 -4.1988292 -4.217144 -4.2192893][-4.0620146 -4.0277734 -4.0183406 -4.0101986 -3.9783597 -3.9200225 -3.838685 -3.7820692 -3.8302531 -3.9350057 -4.0336256 -4.1040668 -4.14554 -4.170855 -4.1793842][-4.0460458 -3.999248 -3.9726615 -3.9533923 -3.9208918 -3.8696523 -3.78793 -3.7195942 -3.7663326 -3.8785732 -3.9828222 -4.0569057 -4.0981593 -4.1252909 -4.143086][-4.0482335 -3.9955914 -3.9600463 -3.9357388 -3.9167118 -3.8964088 -3.8537374 -3.8123956 -3.8380961 -3.9109933 -3.9853806 -4.0408273 -4.0711808 -4.0930061 -4.1152129][-4.0657954 -4.0145388 -3.9804192 -3.9628553 -3.9609954 -3.9662647 -3.9550111 -3.9352202 -3.9431651 -3.9755614 -4.0141706 -4.0455656 -4.0661864 -4.086009 -4.1095061][-4.0912952 -4.0480661 -4.0220132 -4.0140305 -4.0214972 -4.0364151 -4.03985 -4.0304523 -4.0296545 -4.04088 -4.0553861 -4.068903 -4.0822945 -4.1012697 -4.12374][-4.116466 -4.0806513 -4.062973 -4.0615573 -4.0717249 -4.0878963 -4.0966425 -4.0935059 -4.0926018 -4.0979362 -4.1041179 -4.1104746 -4.1209421 -4.1369057 -4.1562514][-4.1436815 -4.1150107 -4.1039958 -4.10671 -4.1166544 -4.1294374 -4.1365023 -4.1358709 -4.1370239 -4.1416802 -4.1480551 -4.1573424 -4.1693058 -4.1831288 -4.1978507][-4.1793342 -4.1598864 -4.1549044 -4.1599555 -4.1682696 -4.1768622 -4.1802783 -4.1799579 -4.1809468 -4.1843548 -4.1899228 -4.1996136 -4.2121692 -4.2252121 -4.2365351][-4.2231092 -4.2134213 -4.2123742 -4.2168112 -4.2223086 -4.2270613 -4.2280602 -4.2269454 -4.226635 -4.2279644 -4.2312908 -4.2376661 -4.24624 -4.2562637 -4.2649508]]...]
INFO - root - 2017-12-06 03:41:39.057063: step 12210, loss = 2.04, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 20h:34m:47s remains)
INFO - root - 2017-12-06 03:41:41.370785: step 12220, loss = 2.04, batch loss = 1.98 (34.4 examples/sec; 0.233 sec/batch; 20h:42m:47s remains)
INFO - root - 2017-12-06 03:41:43.683055: step 12230, loss = 2.07, batch loss = 2.01 (32.9 examples/sec; 0.243 sec/batch; 21h:39m:32s remains)
INFO - root - 2017-12-06 03:41:46.012111: step 12240, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 20h:16m:49s remains)
INFO - root - 2017-12-06 03:41:48.335754: step 12250, loss = 2.09, batch loss = 2.03 (35.1 examples/sec; 0.228 sec/batch; 20h:16m:05s remains)
INFO - root - 2017-12-06 03:41:50.598264: step 12260, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 20h:00m:42s remains)
INFO - root - 2017-12-06 03:41:52.856548: step 12270, loss = 2.07, batch loss = 2.01 (33.4 examples/sec; 0.239 sec/batch; 21h:16m:39s remains)
INFO - root - 2017-12-06 03:41:55.189685: step 12280, loss = 2.05, batch loss = 1.99 (34.9 examples/sec; 0.229 sec/batch; 20h:24m:23s remains)
INFO - root - 2017-12-06 03:41:57.516943: step 12290, loss = 2.03, batch loss = 1.97 (35.7 examples/sec; 0.224 sec/batch; 19h:55m:41s remains)
INFO - root - 2017-12-06 03:41:59.808883: step 12300, loss = 2.05, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 21h:00m:50s remains)
2017-12-06 03:42:00.113429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2107553 -4.20355 -4.2045846 -4.2025967 -4.1947064 -4.1854925 -4.1706753 -4.1554084 -4.1682258 -4.2011971 -4.2219639 -4.2393107 -4.2600822 -4.278048 -4.2869105][-4.185997 -4.1865182 -4.1938019 -4.1962156 -4.1944385 -4.18753 -4.1731114 -4.1532397 -4.157577 -4.18463 -4.2069097 -4.225287 -4.2446041 -4.2609782 -4.2699785][-4.1657882 -4.176434 -4.1909776 -4.1972532 -4.1996994 -4.1922035 -4.1751213 -4.1535177 -4.1505661 -4.1715541 -4.1934452 -4.2107434 -4.2257152 -4.2394781 -4.2481995][-4.1477084 -4.1657424 -4.1835432 -4.1941609 -4.197145 -4.1872382 -4.1642308 -4.1418619 -4.13786 -4.1597676 -4.1836605 -4.2001462 -4.2105608 -4.2190976 -4.2263756][-4.1205907 -4.1466355 -4.1698928 -4.184957 -4.1860337 -4.1667366 -4.1367912 -4.1147003 -4.1154165 -4.1439285 -4.1724215 -4.18789 -4.1944518 -4.1973581 -4.1990032][-4.0857124 -4.1197252 -4.1513076 -4.1718321 -4.1686263 -4.1350865 -4.0966754 -4.0697436 -4.0709929 -4.1098852 -4.1474705 -4.1650615 -4.1712813 -4.1702886 -4.1652555][-4.0701456 -4.1058788 -4.1425967 -4.1630421 -4.1508884 -4.1031623 -4.0499845 -4.0040169 -3.9979916 -4.0543718 -4.1075435 -4.1279993 -4.1333704 -4.1321058 -4.1255479][-4.068769 -4.0988874 -4.1329751 -4.151052 -4.1322155 -4.0761442 -4.0049276 -3.9327848 -3.9150178 -3.9936814 -4.0641975 -4.0840006 -4.0874748 -4.0964308 -4.099453][-4.0776439 -4.1019812 -4.1284709 -4.1423535 -4.1262646 -4.0807166 -4.0100656 -3.9304557 -3.906672 -3.9929042 -4.0670586 -4.0919819 -4.0969443 -4.1076031 -4.1116605][-4.1105623 -4.130116 -4.1486354 -4.1579762 -4.1531153 -4.1326857 -4.0884848 -4.02801 -4.0070081 -4.0652208 -4.1130242 -4.1319561 -4.1339674 -4.1364613 -4.1357884][-4.1727729 -4.1804857 -4.1862473 -4.1883888 -4.1911278 -4.1915097 -4.1728287 -4.1352315 -4.1165147 -4.1384382 -4.1549459 -4.1550865 -4.1489935 -4.1468072 -4.1482873][-4.2238765 -4.2242122 -4.2188563 -4.2129097 -4.2124138 -4.2146506 -4.2072115 -4.1898975 -4.1820984 -4.1863322 -4.180706 -4.1602712 -4.1462011 -4.1513491 -4.1609578][-4.2502604 -4.2500224 -4.2421832 -4.229598 -4.2200584 -4.2174191 -4.2143135 -4.2106862 -4.2132535 -4.2141109 -4.1965876 -4.1669931 -4.1525388 -4.1666079 -4.1830235][-4.2535253 -4.2538085 -4.2473187 -4.2367344 -4.2284946 -4.2226582 -4.2196727 -4.2199187 -4.2260323 -4.2245808 -4.2042847 -4.1784015 -4.1735072 -4.1931243 -4.2058864][-4.2407055 -4.2395306 -4.2345629 -4.2333617 -4.2327676 -4.2262111 -4.2221179 -4.2244034 -4.2296615 -4.2278624 -4.209568 -4.1889296 -4.1891685 -4.2074437 -4.2207866]]...]
INFO - root - 2017-12-06 03:42:02.407371: step 12310, loss = 2.08, batch loss = 2.02 (34.1 examples/sec; 0.235 sec/batch; 20h:53m:18s remains)
INFO - root - 2017-12-06 03:42:04.729865: step 12320, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 20h:34m:04s remains)
INFO - root - 2017-12-06 03:42:06.996964: step 12330, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:26m:34s remains)
INFO - root - 2017-12-06 03:42:09.335915: step 12340, loss = 2.07, batch loss = 2.01 (33.0 examples/sec; 0.243 sec/batch; 21h:34m:56s remains)
INFO - root - 2017-12-06 03:42:11.631960: step 12350, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:49m:14s remains)
INFO - root - 2017-12-06 03:42:13.943090: step 12360, loss = 2.06, batch loss = 2.00 (34.0 examples/sec; 0.235 sec/batch; 20h:54m:49s remains)
INFO - root - 2017-12-06 03:42:16.223163: step 12370, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 19h:59m:22s remains)
INFO - root - 2017-12-06 03:42:18.523366: step 12380, loss = 2.05, batch loss = 1.99 (34.2 examples/sec; 0.234 sec/batch; 20h:49m:06s remains)
INFO - root - 2017-12-06 03:42:20.791462: step 12390, loss = 2.09, batch loss = 2.03 (35.8 examples/sec; 0.223 sec/batch; 19h:51m:25s remains)
INFO - root - 2017-12-06 03:42:23.126743: step 12400, loss = 2.07, batch loss = 2.01 (33.1 examples/sec; 0.242 sec/batch; 21h:30m:06s remains)
2017-12-06 03:42:23.416529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1811666 -4.1756229 -4.1816821 -4.1957803 -4.2102332 -4.2256365 -4.2415457 -4.2427082 -4.2246118 -4.2058735 -4.2041817 -4.2140436 -4.2231593 -4.2295876 -4.2413483][-4.1636224 -4.1588936 -4.1634617 -4.1742058 -4.1851139 -4.1990924 -4.2178154 -4.2186704 -4.1968069 -4.1731019 -4.1702976 -4.1808558 -4.1914454 -4.2019124 -4.2194524][-4.2016206 -4.1968513 -4.1947651 -4.1940055 -4.1932392 -4.1993065 -4.2144542 -4.2148395 -4.1973639 -4.1795263 -4.1806231 -4.1928682 -4.2046361 -4.2156167 -4.2315536][-4.236135 -4.2280059 -4.2180653 -4.2107372 -4.2061381 -4.20389 -4.2062759 -4.198277 -4.1805573 -4.1692262 -4.1776762 -4.1989379 -4.2201 -4.2350821 -4.24651][-4.2284541 -4.2109828 -4.1913805 -4.1841741 -4.1851721 -4.1813359 -4.1682787 -4.1427112 -4.1162066 -4.1093354 -4.1285491 -4.1653428 -4.2004848 -4.2237992 -4.2325726][-4.1821513 -4.1556339 -4.1286411 -4.1223125 -4.1289 -4.1238575 -4.0960803 -4.0483074 -4.0069914 -4.0009689 -4.0310912 -4.0844049 -4.1337872 -4.1671972 -4.1775212][-4.13032 -4.1026578 -4.0775471 -4.0709047 -4.0758028 -4.0643921 -4.02069 -3.9501002 -3.8937025 -3.8902569 -3.927264 -3.980171 -4.024631 -4.0557208 -4.066009][-4.1204247 -4.1027193 -4.0874324 -4.0789452 -4.071207 -4.048543 -4.0003757 -3.9275296 -3.8715141 -3.872498 -3.9088924 -3.9477766 -3.968049 -3.9744804 -3.9692252][-4.1430941 -4.1365185 -4.1271973 -4.1131806 -4.0935631 -4.0729666 -4.0445747 -4.0027709 -3.9731345 -3.9807529 -4.0040679 -4.0226936 -4.0270624 -4.015368 -3.9891827][-4.1468668 -4.1489496 -4.1425653 -4.1256204 -4.1040406 -4.0930581 -4.0887241 -4.0803547 -4.0761027 -4.0914989 -4.1079712 -4.1149716 -4.117301 -4.1104932 -4.0860043][-4.1335006 -4.1364722 -4.1321106 -4.1182795 -4.0998497 -4.0921307 -4.0973039 -4.1072121 -4.1183386 -4.142745 -4.1610484 -4.1646886 -4.1667228 -4.1671567 -4.1553769][-4.1354294 -4.1368713 -4.1316004 -4.1204634 -4.1035857 -4.0926743 -4.091032 -4.097168 -4.1094146 -4.1358824 -4.1570921 -4.1639776 -4.1675191 -4.1730909 -4.1742287][-4.139514 -4.14495 -4.1429892 -4.1370649 -4.119483 -4.10462 -4.0948796 -4.0902653 -4.0922666 -4.1085234 -4.1261921 -4.1345849 -4.1428652 -4.1530924 -4.1619143][-4.1318474 -4.143621 -4.1502585 -4.1507883 -4.1359897 -4.12296 -4.112452 -4.0993791 -4.08888 -4.0886712 -4.0965757 -4.10421 -4.1158381 -4.1289215 -4.1397319][-4.1095 -4.1289859 -4.1447945 -4.1503415 -4.139555 -4.1320958 -4.12729 -4.1160297 -4.10072 -4.089489 -4.0876203 -4.0927124 -4.1028709 -4.1148472 -4.1236916]]...]
INFO - root - 2017-12-06 03:42:25.722500: step 12410, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 20h:12m:59s remains)
INFO - root - 2017-12-06 03:42:28.043893: step 12420, loss = 2.04, batch loss = 1.98 (34.2 examples/sec; 0.234 sec/batch; 20h:46m:53s remains)
INFO - root - 2017-12-06 03:42:30.331647: step 12430, loss = 2.06, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:49m:30s remains)
INFO - root - 2017-12-06 03:42:32.636721: step 12440, loss = 2.05, batch loss = 2.00 (34.4 examples/sec; 0.232 sec/batch; 20h:39m:25s remains)
INFO - root - 2017-12-06 03:42:34.962334: step 12450, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:15m:00s remains)
INFO - root - 2017-12-06 03:42:37.266873: step 12460, loss = 2.06, batch loss = 2.01 (34.4 examples/sec; 0.233 sec/batch; 20h:41m:50s remains)
INFO - root - 2017-12-06 03:42:39.584617: step 12470, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.233 sec/batch; 20h:40m:52s remains)
INFO - root - 2017-12-06 03:42:41.890921: step 12480, loss = 2.05, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:48m:50s remains)
INFO - root - 2017-12-06 03:42:44.208399: step 12490, loss = 2.06, batch loss = 2.01 (33.6 examples/sec; 0.238 sec/batch; 21h:09m:47s remains)
INFO - root - 2017-12-06 03:42:46.556921: step 12500, loss = 2.05, batch loss = 1.99 (33.1 examples/sec; 0.242 sec/batch; 21h:29m:54s remains)
2017-12-06 03:42:46.850453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0980229 -4.1124969 -4.1340685 -4.1452107 -4.1566744 -4.1516838 -4.12977 -4.1172328 -4.1402116 -4.1750493 -4.1969581 -4.2147961 -4.2249851 -4.2220616 -4.2116718][-4.0946393 -4.1048183 -4.1209493 -4.1288948 -4.1457825 -4.1515536 -4.1375818 -4.1266613 -4.1472321 -4.1792369 -4.1934013 -4.2033472 -4.2099814 -4.2087235 -4.2053375][-4.1126218 -4.1139212 -4.1255379 -4.1318173 -4.152276 -4.1688142 -4.1631007 -4.1510258 -4.1632857 -4.1930361 -4.2022834 -4.2041717 -4.2051864 -4.2019219 -4.1993394][-4.1501579 -4.1397972 -4.1410584 -4.1404877 -4.1594491 -4.1777453 -4.1724911 -4.1578121 -4.1653152 -4.1920638 -4.2024684 -4.2028375 -4.2033043 -4.2027297 -4.1961589][-4.1921487 -4.1767945 -4.1702209 -4.1599112 -4.1646194 -4.1635046 -4.1442585 -4.1190567 -4.1211338 -4.15563 -4.1824565 -4.1946158 -4.2046976 -4.2090664 -4.2034011][-4.2112279 -4.1953673 -4.1849523 -4.1651406 -4.1482472 -4.118578 -4.0742331 -4.0241389 -4.0159187 -4.0763826 -4.1402836 -4.1753364 -4.200367 -4.2137561 -4.2154107][-4.1907144 -4.1716595 -4.1573925 -4.1288824 -4.0951014 -4.0442214 -3.9724441 -3.8862522 -3.867815 -3.9711616 -4.0826206 -4.1489897 -4.192749 -4.217977 -4.2232432][-4.1576967 -4.1379614 -4.12251 -4.0917435 -4.0495262 -3.9950821 -3.9165976 -3.8092227 -3.7751946 -3.9035196 -4.04179 -4.1263113 -4.18183 -4.2132335 -4.2177911][-4.1486564 -4.132339 -4.120512 -4.0941691 -4.0587821 -4.0197058 -3.9643362 -3.8796396 -3.8437316 -3.9454174 -4.0614243 -4.1350613 -4.1859951 -4.2106776 -4.2086415][-4.1690497 -4.1609292 -4.1579251 -4.1436963 -4.1207476 -4.0972352 -4.0675535 -4.02189 -3.9979424 -4.0553784 -4.124773 -4.1713285 -4.2063751 -4.21291 -4.1968465][-4.2062674 -4.2063212 -4.209095 -4.2032371 -4.1876173 -4.1710725 -4.1529408 -4.1282291 -4.1113529 -4.1442094 -4.1819844 -4.2054095 -4.2242966 -4.2154164 -4.1868353][-4.2448459 -4.247231 -4.2491851 -4.2447929 -4.2322674 -4.2166567 -4.1998243 -4.1808414 -4.1698093 -4.1954865 -4.2211833 -4.2317033 -4.2404914 -4.2207689 -4.1847839][-4.2679024 -4.270792 -4.2695594 -4.2645183 -4.2559829 -4.2430515 -4.2280622 -4.2096834 -4.205565 -4.2322559 -4.2543526 -4.2598362 -4.2624393 -4.2388482 -4.2020373][-4.2834344 -4.28419 -4.2793922 -4.270968 -4.2642016 -4.257894 -4.2473574 -4.230391 -4.2305355 -4.2546391 -4.2744927 -4.2790318 -4.2789712 -4.2571411 -4.2219791][-4.2897792 -4.2854533 -4.2780404 -4.2686882 -4.2654281 -4.2677379 -4.2631326 -4.2484674 -4.2472396 -4.2640958 -4.2796545 -4.2852521 -4.2876906 -4.2725563 -4.24251]]...]
INFO - root - 2017-12-06 03:42:49.127139: step 12510, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 20h:08m:54s remains)
INFO - root - 2017-12-06 03:42:51.423895: step 12520, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:58m:17s remains)
INFO - root - 2017-12-06 03:42:53.727864: step 12530, loss = 2.06, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:16m:04s remains)
INFO - root - 2017-12-06 03:42:56.025531: step 12540, loss = 2.07, batch loss = 2.02 (35.6 examples/sec; 0.224 sec/batch; 19h:56m:42s remains)
INFO - root - 2017-12-06 03:42:58.281957: step 12550, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.235 sec/batch; 20h:52m:34s remains)
INFO - root - 2017-12-06 03:43:00.564625: step 12560, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 20h:00m:04s remains)
INFO - root - 2017-12-06 03:43:02.835299: step 12570, loss = 2.03, batch loss = 1.97 (34.5 examples/sec; 0.232 sec/batch; 20h:36m:43s remains)
INFO - root - 2017-12-06 03:43:05.117183: step 12580, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.226 sec/batch; 20h:07m:38s remains)
INFO - root - 2017-12-06 03:43:07.423905: step 12590, loss = 2.05, batch loss = 1.99 (34.5 examples/sec; 0.232 sec/batch; 20h:36m:58s remains)
INFO - root - 2017-12-06 03:43:09.724623: step 12600, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:21m:35s remains)
2017-12-06 03:43:10.020241: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0491033 -4.0541072 -4.0501413 -4.0302286 -4.0209808 -4.0336032 -4.0508552 -4.0660563 -4.076334 -4.0746031 -4.0603743 -4.0738978 -4.1138959 -4.15137 -4.1838236][-4.0353689 -4.036871 -4.0257454 -4.0002389 -3.9915316 -4.0011635 -4.0132623 -4.0284719 -4.0411835 -4.0427871 -4.0335264 -4.0473247 -4.0885134 -4.1305518 -4.1705718][-4.0431843 -4.041481 -4.0233574 -3.9945023 -3.9854612 -3.9855337 -3.986078 -4.0047207 -4.026022 -4.0351686 -4.0286794 -4.0422382 -4.0781183 -4.1167145 -4.1562634][-4.0594282 -4.0569272 -4.0341711 -4.0030556 -3.9859807 -3.9718025 -3.9590118 -3.97828 -4.0073476 -4.027113 -4.0337377 -4.0568323 -4.0879278 -4.1167297 -4.1501741][-4.0786576 -4.0769095 -4.0504975 -4.0212445 -3.99228 -3.9616437 -3.9374204 -3.9511375 -3.9839282 -4.020215 -4.0461812 -4.0808883 -4.1110282 -4.1336837 -4.1601281][-4.0961757 -4.1048512 -4.0823412 -4.0549946 -4.0158749 -3.9644914 -3.9219031 -3.9256604 -3.9563363 -4.0030279 -4.0477691 -4.0996337 -4.1405759 -4.1679659 -4.1884222][-4.1047144 -4.1228995 -4.1106238 -4.0859532 -4.0413179 -3.9777308 -3.9236231 -3.9205632 -3.9482746 -3.9913998 -4.0412679 -4.1044207 -4.1613851 -4.1974549 -4.2163172][-4.1129069 -4.1344538 -4.1317005 -4.1104116 -4.0683122 -4.0104909 -3.9625874 -3.954973 -3.9683747 -3.9946661 -4.037003 -4.1017661 -4.1710968 -4.2142882 -4.2325439][-4.1286831 -4.1492748 -4.151021 -4.1341949 -4.0971088 -4.0478535 -4.009213 -4.0004611 -4.0040979 -4.0174522 -4.0505934 -4.1110497 -4.1788278 -4.2195044 -4.2357359][-4.1451297 -4.1666722 -4.1736655 -4.1636982 -4.1330667 -4.0921078 -4.0621786 -4.0544991 -4.0545554 -4.0618191 -4.0832686 -4.1314344 -4.1848893 -4.2170553 -4.2311435][-4.162621 -4.1847458 -4.196537 -4.1946244 -4.1751704 -4.1479855 -4.1274381 -4.1203108 -4.1174316 -4.1171527 -4.1234732 -4.1524305 -4.1889682 -4.2151527 -4.2316666][-4.1738415 -4.1918864 -4.2069435 -4.2121344 -4.2048054 -4.1907411 -4.1776023 -4.169426 -4.1653843 -4.1604762 -4.1582961 -4.1748977 -4.2024221 -4.223423 -4.2386751][-4.1941438 -4.2047424 -4.2166533 -4.2225485 -4.2217326 -4.2161217 -4.21011 -4.2058153 -4.2041121 -4.1993628 -4.1955843 -4.2066369 -4.2278271 -4.2438364 -4.2559314][-4.2296104 -4.2333379 -4.2367544 -4.2389278 -4.2392454 -4.2376361 -4.2352204 -4.2332754 -4.2335443 -4.2317348 -4.2296672 -4.2373552 -4.25244 -4.264358 -4.2731543][-4.2535563 -4.2535877 -4.2526951 -4.2524538 -4.2523956 -4.2520804 -4.2512522 -4.2511697 -4.2525988 -4.252882 -4.2522254 -4.2577558 -4.2684708 -4.2772098 -4.284656]]...]
INFO - root - 2017-12-06 03:43:12.350045: step 12610, loss = 2.06, batch loss = 2.00 (34.1 examples/sec; 0.234 sec/batch; 20h:49m:33s remains)
INFO - root - 2017-12-06 03:43:14.682346: step 12620, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 19h:51m:29s remains)
INFO - root - 2017-12-06 03:43:17.028980: step 12630, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 20h:19m:26s remains)
INFO - root - 2017-12-06 03:43:19.339596: step 12640, loss = 2.02, batch loss = 1.96 (34.0 examples/sec; 0.235 sec/batch; 20h:52m:41s remains)
INFO - root - 2017-12-06 03:43:21.597495: step 12650, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 20h:09m:29s remains)
INFO - root - 2017-12-06 03:43:23.930567: step 12660, loss = 2.09, batch loss = 2.03 (34.2 examples/sec; 0.234 sec/batch; 20h:46m:55s remains)
INFO - root - 2017-12-06 03:43:26.245953: step 12670, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 20h:56m:33s remains)
INFO - root - 2017-12-06 03:43:28.521862: step 12680, loss = 2.09, batch loss = 2.03 (34.0 examples/sec; 0.235 sec/batch; 20h:53m:28s remains)
INFO - root - 2017-12-06 03:43:30.809709: step 12690, loss = 2.06, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:45m:16s remains)
INFO - root - 2017-12-06 03:43:33.087928: step 12700, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:37m:23s remains)
2017-12-06 03:43:33.404594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2131333 -4.229454 -4.2379465 -4.2377477 -4.235559 -4.2369671 -4.2413054 -4.2508783 -4.2675056 -4.2798734 -4.2842879 -4.2741733 -4.2507939 -4.2219806 -4.2041712][-4.2522063 -4.2633142 -4.2686458 -4.2693748 -4.2680912 -4.2720404 -4.280262 -4.2919507 -4.3064585 -4.3139739 -4.312757 -4.2933931 -4.2583742 -4.2255216 -4.2085409][-4.2655077 -4.2688856 -4.2716074 -4.2736235 -4.2720966 -4.278553 -4.2898655 -4.299027 -4.3077984 -4.3116212 -4.3094583 -4.2917757 -4.2578421 -4.2317767 -4.2216988][-4.2630816 -4.2609849 -4.2618685 -4.2628531 -4.2587709 -4.2650576 -4.2755594 -4.2786555 -4.2831306 -4.2865891 -4.2883124 -4.2767649 -4.2524719 -4.2442551 -4.2459226][-4.2629552 -4.2575512 -4.2546697 -4.2508006 -4.2391663 -4.2366595 -4.2396874 -4.2328429 -4.2334404 -4.2371216 -4.2395024 -4.2356467 -4.2254786 -4.2317252 -4.2470694][-4.2564058 -4.2486997 -4.2406244 -4.2281785 -4.2049432 -4.1874924 -4.1746378 -4.1555042 -4.1591673 -4.1704473 -4.1753035 -4.1808562 -4.1879759 -4.1993332 -4.2149606][-4.2302532 -4.2194657 -4.2026553 -4.1795869 -4.148766 -4.1200809 -4.080862 -4.0350423 -4.0450807 -4.073122 -4.0881639 -4.1041508 -4.1232643 -4.1376262 -4.1487803][-4.1948791 -4.1753736 -4.1430082 -4.1068478 -4.0718184 -4.0297818 -3.9501498 -3.8581455 -3.8805807 -3.9468524 -3.9925878 -4.0299063 -4.0650992 -4.0834608 -4.0897121][-4.1453886 -4.1079698 -4.0567327 -4.0084839 -3.964963 -3.9104846 -3.8101456 -3.6943293 -3.7619359 -3.8898659 -3.972976 -4.0297771 -4.073153 -4.0882893 -4.0863552][-4.0985126 -4.04913 -3.997035 -3.9527426 -3.9132729 -3.8725159 -3.8259087 -3.7882104 -3.8680413 -3.9779518 -4.0481787 -4.0966654 -4.1339331 -4.137197 -4.128417][-4.0837169 -4.0426545 -4.0110359 -3.9854033 -3.9608417 -3.949002 -3.9595187 -3.9780474 -4.0377545 -4.1014743 -4.1439648 -4.1766119 -4.203938 -4.20267 -4.198482][-4.1173468 -4.1022615 -4.0985632 -4.091321 -4.0782142 -4.0787745 -4.1001163 -4.1212506 -4.1555181 -4.1903219 -4.2168355 -4.2349153 -4.2533841 -4.2546735 -4.2541995][-4.1500292 -4.1560292 -4.1703787 -4.1728559 -4.169476 -4.1748242 -4.1901274 -4.2000527 -4.2147808 -4.2299461 -4.2425694 -4.2517028 -4.26099 -4.2646508 -4.2648015][-4.1492958 -4.1706967 -4.1970243 -4.2083597 -4.2116833 -4.2153778 -4.2231417 -4.2241578 -4.2255383 -4.2261567 -4.2286344 -4.23406 -4.238512 -4.2437358 -4.2479753][-4.1105752 -4.1421332 -4.1765127 -4.191021 -4.1950412 -4.1957188 -4.1973915 -4.1928377 -4.1867332 -4.1806765 -4.1770658 -4.1757703 -4.1746645 -4.1808443 -4.18988]]...]
INFO - root - 2017-12-06 03:43:35.714609: step 12710, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 20h:03m:19s remains)
INFO - root - 2017-12-06 03:43:38.037064: step 12720, loss = 2.06, batch loss = 2.00 (33.7 examples/sec; 0.237 sec/batch; 21h:05m:43s remains)
INFO - root - 2017-12-06 03:43:40.304680: step 12730, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 20h:00m:43s remains)
INFO - root - 2017-12-06 03:43:42.599267: step 12740, loss = 2.08, batch loss = 2.03 (34.7 examples/sec; 0.231 sec/batch; 20h:30m:23s remains)
INFO - root - 2017-12-06 03:43:44.913391: step 12750, loss = 2.06, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 19h:57m:31s remains)
INFO - root - 2017-12-06 03:43:47.238877: step 12760, loss = 2.09, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:16m:30s remains)
INFO - root - 2017-12-06 03:43:49.531276: step 12770, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:13m:36s remains)
INFO - root - 2017-12-06 03:43:51.848272: step 12780, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.233 sec/batch; 20h:40m:27s remains)
INFO - root - 2017-12-06 03:43:54.168134: step 12790, loss = 2.06, batch loss = 2.00 (32.5 examples/sec; 0.246 sec/batch; 21h:52m:19s remains)
INFO - root - 2017-12-06 03:43:56.474362: step 12800, loss = 2.08, batch loss = 2.02 (34.6 examples/sec; 0.232 sec/batch; 20h:33m:33s remains)
2017-12-06 03:43:56.791534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1802864 -4.1775017 -4.1825943 -4.1997356 -4.2282448 -4.2568073 -4.2732387 -4.2703366 -4.2492485 -4.2240396 -4.2088108 -4.1975689 -4.1869893 -4.1808996 -4.1914077][-4.1559091 -4.1570573 -4.1621094 -4.1744428 -4.2027025 -4.2320852 -4.2482796 -4.247303 -4.2309594 -4.2134447 -4.203867 -4.1920276 -4.1730261 -4.1559348 -4.1620507][-4.1459804 -4.14806 -4.1486883 -4.1543641 -4.1785431 -4.2046409 -4.2176833 -4.2201495 -4.2135077 -4.21008 -4.2117233 -4.2043386 -4.1810961 -4.1551371 -4.155488][-4.1470842 -4.1459861 -4.1402688 -4.13784 -4.1546044 -4.1747241 -4.182404 -4.1814308 -4.186636 -4.2021861 -4.2190695 -4.2205977 -4.1965213 -4.1636252 -4.1627879][-4.1623006 -4.1554775 -4.140274 -4.1259346 -4.1313114 -4.1397223 -4.134232 -4.1249051 -4.1483393 -4.1914363 -4.2258763 -4.2382455 -4.2148252 -4.1740313 -4.1691022][-4.1856046 -4.1734915 -4.1502728 -4.123559 -4.1102414 -4.0990391 -4.0740619 -4.0529475 -4.095438 -4.1717958 -4.2274108 -4.2521348 -4.2331228 -4.1859069 -4.1769552][-4.201838 -4.186419 -4.157217 -4.1196585 -4.0840478 -4.0509667 -4.0021534 -3.9626155 -4.0186772 -4.129868 -4.208034 -4.2464228 -4.2320933 -4.1820064 -4.1747303][-4.2129931 -4.2002716 -4.171957 -4.1279 -4.0769825 -4.0229206 -3.9509852 -3.8906717 -3.9513111 -4.08689 -4.1806369 -4.2279291 -4.2174296 -4.16965 -4.1694322][-4.2250633 -4.215117 -4.191359 -4.1526513 -4.1005745 -4.039257 -3.9611492 -3.894753 -3.9477229 -4.0763268 -4.1674519 -4.21402 -4.2061558 -4.1657114 -4.17086][-4.2405386 -4.2304955 -4.2129316 -4.1866994 -4.1452866 -4.0900173 -4.01847 -3.961333 -4.0014806 -4.0988646 -4.1723175 -4.2101126 -4.2042346 -4.1739717 -4.1815486][-4.2532721 -4.2404847 -4.2261295 -4.2092338 -4.1805363 -4.1346045 -4.075448 -4.02892 -4.0511751 -4.1165285 -4.1722822 -4.2068839 -4.2069697 -4.1867967 -4.1948142][-4.25234 -4.2349839 -4.21837 -4.2054324 -4.1861305 -4.1516237 -4.10778 -4.0708728 -4.0787759 -4.1188822 -4.1602902 -4.1953616 -4.2033525 -4.1929936 -4.1994476][-4.2479892 -4.2291088 -4.2115793 -4.20199 -4.1899681 -4.1638937 -4.132802 -4.106566 -4.1089091 -4.1322927 -4.16087 -4.19247 -4.2034655 -4.1968522 -4.1995821][-4.2462983 -4.2295418 -4.21462 -4.2094555 -4.2031188 -4.184689 -4.1642032 -4.1476746 -4.1487675 -4.1610241 -4.1771183 -4.2002616 -4.2103987 -4.2045255 -4.2031679][-4.2510142 -4.2379093 -4.226481 -4.2244954 -4.2222242 -4.2104568 -4.1994777 -4.1920004 -4.192369 -4.1971817 -4.2030339 -4.2168555 -4.2239189 -4.21892 -4.2162762]]...]
INFO - root - 2017-12-06 03:43:59.094446: step 12810, loss = 2.05, batch loss = 1.99 (33.4 examples/sec; 0.240 sec/batch; 21h:17m:27s remains)
INFO - root - 2017-12-06 03:44:01.401888: step 12820, loss = 2.08, batch loss = 2.02 (34.1 examples/sec; 0.235 sec/batch; 20h:49m:32s remains)
INFO - root - 2017-12-06 03:44:03.713019: step 12830, loss = 2.06, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:15m:12s remains)
INFO - root - 2017-12-06 03:44:05.987277: step 12840, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 20h:21m:55s remains)
INFO - root - 2017-12-06 03:44:08.305535: step 12850, loss = 2.07, batch loss = 2.01 (31.0 examples/sec; 0.258 sec/batch; 22h:54m:45s remains)
INFO - root - 2017-12-06 03:44:10.610153: step 12860, loss = 2.05, batch loss = 1.99 (34.7 examples/sec; 0.231 sec/batch; 20h:29m:00s remains)
INFO - root - 2017-12-06 03:44:12.956549: step 12870, loss = 2.09, batch loss = 2.03 (35.4 examples/sec; 0.226 sec/batch; 20h:02m:18s remains)
INFO - root - 2017-12-06 03:44:15.287245: step 12880, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 20h:10m:23s remains)
INFO - root - 2017-12-06 03:44:17.549667: step 12890, loss = 2.04, batch loss = 1.98 (35.8 examples/sec; 0.223 sec/batch; 19h:50m:21s remains)
INFO - root - 2017-12-06 03:44:19.865311: step 12900, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 20h:00m:19s remains)
2017-12-06 03:44:20.155500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2946734 -4.3049669 -4.3082056 -4.309474 -4.30866 -4.3095531 -4.3118305 -4.3145337 -4.315877 -4.3142467 -4.3109031 -4.3026934 -4.2966595 -4.300211 -4.3054018][-4.2712 -4.2876258 -4.29315 -4.2935476 -4.2905488 -4.2894659 -4.2900314 -4.2931576 -4.2948146 -4.2890816 -4.27889 -4.2645526 -4.2539363 -4.2582726 -4.2674069][-4.2486157 -4.2718363 -4.2810988 -4.2787304 -4.2716441 -4.267343 -4.261178 -4.2623634 -4.2677732 -4.264513 -4.2514682 -4.2299085 -4.2133741 -4.2181616 -4.2317514][-4.228096 -4.260159 -4.2729464 -4.2654924 -4.2514634 -4.2404523 -4.2280593 -4.2294908 -4.243897 -4.2472739 -4.234807 -4.2097774 -4.1932845 -4.2003155 -4.2158928][-4.2045817 -4.2423739 -4.259253 -4.24869 -4.2229795 -4.19578 -4.1720572 -4.1741343 -4.2019448 -4.2174559 -4.2129145 -4.1924219 -4.1787519 -4.1854644 -4.1987691][-4.19195 -4.2304749 -4.2442503 -4.2252035 -4.1851654 -4.1359024 -4.0925221 -4.0969729 -4.145772 -4.1760883 -4.1803288 -4.1664577 -4.1574731 -4.1634851 -4.1692414][-4.187633 -4.2255168 -4.2327347 -4.2031264 -4.1461563 -4.0668488 -3.9941046 -4.005137 -4.0860658 -4.1363821 -4.1478615 -4.1382442 -4.1304293 -4.1325569 -4.1279583][-4.1925888 -4.2280617 -4.2330618 -4.197906 -4.1286478 -4.0211034 -3.91464 -3.9319766 -4.0424051 -4.1141257 -4.1359334 -4.1331835 -4.1303921 -4.1292453 -4.1138415][-4.201417 -4.2355266 -4.243319 -4.2117662 -4.1436749 -4.033617 -3.9232268 -3.9384367 -4.0465417 -4.1221833 -4.148087 -4.1502085 -4.1495848 -4.1452761 -4.1260772][-4.2020712 -4.2342496 -4.2416997 -4.2207956 -4.1712613 -4.0884581 -4.0116529 -4.0232677 -4.099864 -4.1597419 -4.1824927 -4.1833205 -4.1769314 -4.1650805 -4.1461639][-4.2011085 -4.2278237 -4.2310724 -4.2177086 -4.1908669 -4.1436276 -4.1036835 -4.1148868 -4.1606131 -4.2002187 -4.2162948 -4.2160912 -4.2039289 -4.1855373 -4.1695614][-4.1953359 -4.2134085 -4.21201 -4.2012639 -4.18886 -4.1690145 -4.1551552 -4.1680179 -4.1943955 -4.2183518 -4.2299228 -4.22897 -4.2169628 -4.1979103 -4.1840467][-4.1843419 -4.1920395 -4.1933522 -4.1906047 -4.189641 -4.189692 -4.192173 -4.2066178 -4.2181807 -4.2257481 -4.2279425 -4.2255955 -4.2183585 -4.2049532 -4.1938214][-4.1700869 -4.1708221 -4.1864796 -4.2042966 -4.2173691 -4.2281733 -4.2360716 -4.2465391 -4.2459984 -4.2366295 -4.2266889 -4.2237272 -4.2236128 -4.216867 -4.2076864][-4.1554275 -4.1525345 -4.1832204 -4.2263441 -4.2511587 -4.2604313 -4.2651997 -4.2728634 -4.2687936 -4.2522922 -4.2365718 -4.2325664 -4.2344527 -4.2316484 -4.2254686]]...]
INFO - root - 2017-12-06 03:44:22.424117: step 12910, loss = 2.08, batch loss = 2.03 (35.4 examples/sec; 0.226 sec/batch; 20h:04m:06s remains)
INFO - root - 2017-12-06 03:44:24.735972: step 12920, loss = 2.05, batch loss = 2.00 (33.7 examples/sec; 0.237 sec/batch; 21h:03m:08s remains)
INFO - root - 2017-12-06 03:44:27.010606: step 12930, loss = 2.07, batch loss = 2.02 (36.3 examples/sec; 0.220 sec/batch; 19h:33m:20s remains)
INFO - root - 2017-12-06 03:44:29.272320: step 12940, loss = 2.03, batch loss = 1.97 (35.4 examples/sec; 0.226 sec/batch; 20h:03m:34s remains)
INFO - root - 2017-12-06 03:44:31.549216: step 12950, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 20h:31m:00s remains)
INFO - root - 2017-12-06 03:44:33.874520: step 12960, loss = 2.09, batch loss = 2.03 (34.3 examples/sec; 0.233 sec/batch; 20h:43m:15s remains)
INFO - root - 2017-12-06 03:44:36.170554: step 12970, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 19h:26m:23s remains)
INFO - root - 2017-12-06 03:44:38.479812: step 12980, loss = 2.05, batch loss = 1.99 (34.2 examples/sec; 0.234 sec/batch; 20h:45m:53s remains)
INFO - root - 2017-12-06 03:44:40.817370: step 12990, loss = 2.05, batch loss = 1.99 (34.4 examples/sec; 0.233 sec/batch; 20h:39m:02s remains)
INFO - root - 2017-12-06 03:44:43.134195: step 13000, loss = 2.08, batch loss = 2.02 (33.7 examples/sec; 0.238 sec/batch; 21h:05m:03s remains)
2017-12-06 03:44:43.410871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3056808 -4.3104553 -4.3099122 -4.3045077 -4.2987604 -4.294672 -4.2913 -4.2875695 -4.28469 -4.284152 -4.2854362 -4.2873249 -4.2898912 -4.2929835 -4.2963648][-4.3058863 -4.3084531 -4.3056345 -4.2986212 -4.2913132 -4.2852449 -4.2794008 -4.2732358 -4.268466 -4.2673826 -4.2702312 -4.2751703 -4.2812018 -4.2878127 -4.2944908][-4.3052239 -4.3046002 -4.2986484 -4.2897058 -4.2804947 -4.2708559 -4.2602711 -4.24972 -4.2411122 -4.2386489 -4.2440691 -4.2541966 -4.2652388 -4.2762418 -4.2871828][-4.3042321 -4.3013806 -4.2922916 -4.2801018 -4.2660975 -4.2483768 -4.2271919 -4.2059321 -4.1888232 -4.1830997 -4.1921043 -4.2105 -4.2304997 -4.2497945 -4.268959][-4.2952766 -4.2927547 -4.2829571 -4.2678695 -4.2460842 -4.2132578 -4.1709251 -4.1289406 -4.0984693 -4.0894384 -4.1065693 -4.1417012 -4.1787491 -4.21226 -4.2432761][-4.2675343 -4.2664704 -4.2576971 -4.2415838 -4.2130432 -4.1649518 -4.0995979 -4.0342731 -3.9915452 -3.9843788 -4.0151868 -4.0737629 -4.1324573 -4.181663 -4.223495][-4.2238059 -4.2271328 -4.2253356 -4.21647 -4.1902423 -4.1379895 -4.06368 -3.9885447 -3.9436216 -3.9429879 -3.9857984 -4.0616431 -4.1323104 -4.1860132 -4.2269735][-4.16876 -4.1749945 -4.1850247 -4.1947227 -4.1885338 -4.1554461 -4.1013427 -4.0443707 -4.01082 -4.0119987 -4.0473843 -4.1124868 -4.1744351 -4.2187481 -4.2486548][-4.11299 -4.1163678 -4.1364474 -4.1675305 -4.1911125 -4.1911349 -4.1713138 -4.1424708 -4.1230192 -4.1235914 -4.1445179 -4.1877189 -4.2307711 -4.2598047 -4.2757158][-4.07152 -4.0688577 -4.0950246 -4.1407886 -4.1880708 -4.2183013 -4.2301307 -4.2288446 -4.2237616 -4.2244635 -4.2338338 -4.2564983 -4.2801061 -4.29325 -4.29667][-4.0624075 -4.0535469 -4.0791359 -4.1278977 -4.184638 -4.2331843 -4.2663383 -4.2855797 -4.2929745 -4.294137 -4.295084 -4.3015304 -4.3087826 -4.3089452 -4.3030586][-4.0565805 -4.0436234 -4.0670881 -4.1159153 -4.1747618 -4.23207 -4.2777705 -4.3101244 -4.3246632 -4.3225327 -4.3134809 -4.3072166 -4.3031659 -4.2956719 -4.2861505][-4.0523772 -4.037106 -4.0576482 -4.1031017 -4.1616197 -4.2221022 -4.2713571 -4.30323 -4.3120947 -4.2977414 -4.2763453 -4.2632308 -4.256772 -4.2504387 -4.2455516][-4.0636096 -4.0481195 -4.0649867 -4.1038356 -4.1555662 -4.2145114 -4.261797 -4.283895 -4.2746296 -4.238575 -4.2016072 -4.1846852 -4.1816454 -4.1830816 -4.187767][-4.0937653 -4.0834513 -4.0978608 -4.1292176 -4.1718564 -4.2222905 -4.2597871 -4.2649069 -4.2315593 -4.1719103 -4.1202478 -4.1011605 -4.1043181 -4.11687 -4.1334553]]...]
INFO - root - 2017-12-06 03:44:45.704566: step 13010, loss = 2.04, batch loss = 1.98 (35.9 examples/sec; 0.223 sec/batch; 19h:48m:04s remains)
INFO - root - 2017-12-06 03:44:47.967525: step 13020, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:55m:50s remains)
INFO - root - 2017-12-06 03:44:50.258532: step 13030, loss = 2.04, batch loss = 1.98 (35.2 examples/sec; 0.227 sec/batch; 20h:10m:10s remains)
INFO - root - 2017-12-06 03:44:52.529178: step 13040, loss = 2.08, batch loss = 2.03 (36.8 examples/sec; 0.218 sec/batch; 19h:19m:02s remains)
INFO - root - 2017-12-06 03:44:54.804312: step 13050, loss = 2.08, batch loss = 2.02 (34.7 examples/sec; 0.231 sec/batch; 20h:28m:09s remains)
INFO - root - 2017-12-06 03:44:57.113627: step 13060, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:42m:07s remains)
INFO - root - 2017-12-06 03:44:59.385632: step 13070, loss = 2.05, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 20h:35m:51s remains)
INFO - root - 2017-12-06 03:45:01.673248: step 13080, loss = 2.05, batch loss = 1.99 (36.5 examples/sec; 0.219 sec/batch; 19h:27m:19s remains)
INFO - root - 2017-12-06 03:45:03.943490: step 13090, loss = 2.05, batch loss = 1.99 (35.2 examples/sec; 0.227 sec/batch; 20h:09m:47s remains)
INFO - root - 2017-12-06 03:45:06.243735: step 13100, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 20h:57m:29s remains)
2017-12-06 03:45:06.514150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3191118 -4.3035088 -4.294416 -4.2886071 -4.2889 -4.2881908 -4.2852507 -4.2857423 -4.2885904 -4.2904658 -4.2933283 -4.2997251 -4.3080153 -4.3182387 -4.3269944][-4.294436 -4.2717571 -4.2557721 -4.2442646 -4.2414479 -4.2361083 -4.2284427 -4.2293782 -4.2391944 -4.2454624 -4.2485967 -4.25972 -4.2687798 -4.2770333 -4.2815065][-4.2624526 -4.2294412 -4.2023096 -4.1820269 -4.1723428 -4.1580453 -4.1448169 -4.1469083 -4.1638408 -4.1760993 -4.1796231 -4.1928115 -4.2020679 -4.2071147 -4.21013][-4.2271962 -4.1801071 -4.137536 -4.1046534 -4.0797272 -4.0541277 -4.0386224 -4.0443215 -4.067225 -4.08494 -4.0886855 -4.1025634 -4.1126385 -4.1155281 -4.1244087][-4.2009149 -4.1436014 -4.084981 -4.0370374 -3.9973106 -3.9616866 -3.9403894 -3.9480543 -3.9726324 -3.9840231 -3.9825771 -4.0003896 -4.0226169 -4.0329919 -4.0496855][-4.1831851 -4.1165824 -4.0445952 -3.9839396 -3.9301071 -3.8771083 -3.8387923 -3.8393302 -3.8650618 -3.87223 -3.8741133 -3.9119141 -3.9606366 -3.9865327 -4.006331][-4.1743422 -4.0945759 -4.0085115 -3.9357984 -3.8639507 -3.7835822 -3.7214196 -3.7248526 -3.7831531 -3.8092718 -3.8211799 -3.8755648 -3.9461319 -3.9839635 -4.0046482][-4.1651049 -4.0711665 -3.9764423 -3.9028611 -3.8279126 -3.7459884 -3.6869149 -3.7167306 -3.8157918 -3.8666053 -3.8820753 -3.9318638 -3.9970355 -4.0282407 -4.0407624][-4.1587744 -4.0656281 -3.9804878 -3.9278102 -3.8836312 -3.8358145 -3.8030105 -3.8402019 -3.9313581 -3.9795375 -3.9909737 -4.0261803 -4.0768285 -4.1006689 -4.1106014][-4.1645842 -4.0855513 -4.0184836 -3.9826057 -3.9615939 -3.9416735 -3.9246235 -3.9490511 -4.0149736 -4.050458 -4.06386 -4.0955644 -4.1414585 -4.1674471 -4.1776505][-4.1905804 -4.1314898 -4.0802679 -4.0542431 -4.0439939 -4.0393648 -4.0302057 -4.0412664 -4.0856929 -4.1124988 -4.1263657 -4.1515617 -4.1855235 -4.2072134 -4.216218][-4.2278042 -4.1872511 -4.1507835 -4.1315451 -4.1210122 -4.1202917 -4.1162472 -4.1251249 -4.1596303 -4.1838765 -4.1967678 -4.2148886 -4.2339015 -4.24568 -4.2493153][-4.2562704 -4.2280765 -4.2036586 -4.1879072 -4.1769948 -4.178133 -4.18141 -4.1964469 -4.2288771 -4.2534308 -4.2639804 -4.2720008 -4.2770395 -4.2783618 -4.275794][-4.2785754 -4.2579169 -4.2429538 -4.2319179 -4.2245169 -4.2307892 -4.2440681 -4.2634888 -4.2889366 -4.3056707 -4.3099809 -4.3093729 -4.3076038 -4.3051939 -4.3018227][-4.2999806 -4.2827249 -4.2713223 -4.2636113 -4.2604046 -4.2677765 -4.2816048 -4.299367 -4.3179221 -4.3285332 -4.3309169 -4.3290696 -4.3268561 -4.3259363 -4.3236346]]...]
INFO - root - 2017-12-06 03:45:08.796679: step 13110, loss = 2.07, batch loss = 2.01 (37.0 examples/sec; 0.216 sec/batch; 19h:12m:07s remains)
INFO - root - 2017-12-06 03:45:11.100694: step 13120, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.227 sec/batch; 20h:07m:11s remains)
INFO - root - 2017-12-06 03:45:13.429631: step 13130, loss = 2.06, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:45m:26s remains)
INFO - root - 2017-12-06 03:45:15.729544: step 13140, loss = 2.09, batch loss = 2.03 (33.3 examples/sec; 0.240 sec/batch; 21h:19m:23s remains)
INFO - root - 2017-12-06 03:45:18.032251: step 13150, loss = 2.10, batch loss = 2.04 (34.5 examples/sec; 0.232 sec/batch; 20h:33m:43s remains)
INFO - root - 2017-12-06 03:45:20.331004: step 13160, loss = 2.07, batch loss = 2.01 (33.4 examples/sec; 0.239 sec/batch; 21h:14m:33s remains)
INFO - root - 2017-12-06 03:45:22.696583: step 13170, loss = 2.09, batch loss = 2.03 (34.1 examples/sec; 0.234 sec/batch; 20h:47m:36s remains)
INFO - root - 2017-12-06 03:45:25.048037: step 13180, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.230 sec/batch; 20h:26m:26s remains)
INFO - root - 2017-12-06 03:45:27.323111: step 13190, loss = 2.04, batch loss = 1.98 (34.9 examples/sec; 0.230 sec/batch; 20h:21m:28s remains)
INFO - root - 2017-12-06 03:45:29.632629: step 13200, loss = 2.09, batch loss = 2.03 (34.7 examples/sec; 0.231 sec/batch; 20h:26m:51s remains)
2017-12-06 03:45:29.953506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20922 -4.2181816 -4.2223887 -4.2138233 -4.1918311 -4.1694541 -4.1523137 -4.1436687 -4.1473169 -4.1585126 -4.1823072 -4.2051749 -4.2276464 -4.2463346 -4.248754][-4.2033668 -4.2087264 -4.2049103 -4.1878257 -4.1620917 -4.1413803 -4.1253552 -4.1189418 -4.1270361 -4.1427279 -4.17031 -4.1953 -4.2201571 -4.2389145 -4.2435164][-4.1957831 -4.196928 -4.1850243 -4.1606617 -4.1333318 -4.1135097 -4.0973129 -4.0942907 -4.1049061 -4.1189084 -4.1437006 -4.1685152 -4.1948562 -4.2129726 -4.2202659][-4.1854372 -4.1861038 -4.1702437 -4.1428418 -4.1149979 -4.0921836 -4.0724359 -4.0676513 -4.0753183 -4.0820212 -4.0981126 -4.1227374 -4.1551085 -4.1804023 -4.1926479][-4.1734705 -4.1753693 -4.1613545 -4.1361828 -4.1073613 -4.075737 -4.0437484 -4.0275984 -4.0263643 -4.0243654 -4.0354681 -4.0684628 -4.1137114 -4.15067 -4.1670756][-4.166029 -4.165204 -4.1514583 -4.1277881 -4.0962753 -4.0504456 -3.9975982 -3.9653859 -3.9565706 -3.9514225 -3.9675028 -4.0192304 -4.0804167 -4.1271939 -4.1457062][-4.1612439 -4.1559763 -4.1369066 -4.1090217 -4.0708656 -4.0136638 -3.9474483 -3.9113543 -3.9069824 -3.9128656 -3.9424772 -4.0062633 -4.0724244 -4.1171489 -4.1353693][-4.152359 -4.1459336 -4.1215992 -4.08645 -4.0452223 -3.9923623 -3.9405825 -3.9281497 -3.9402714 -3.9588082 -3.9925787 -4.0466886 -4.0995569 -4.1305737 -4.1408486][-4.1334033 -4.1283484 -4.104857 -4.0717664 -4.0382485 -4.004653 -3.9818742 -3.9927948 -4.0153961 -4.0378346 -4.06608 -4.1040373 -4.1411495 -4.1591549 -4.1578512][-4.109375 -4.1086264 -4.092339 -4.0737166 -4.0568962 -4.0454421 -4.0443678 -4.0641437 -4.0863657 -4.1078348 -4.1313238 -4.1552529 -4.1776657 -4.1827378 -4.1713705][-4.1022468 -4.1090422 -4.102 -4.0965548 -4.0967374 -4.1009851 -4.1117878 -4.132071 -4.1517863 -4.1708364 -4.1890655 -4.2000847 -4.2092924 -4.2049127 -4.1890965][-4.116755 -4.1320009 -4.1321068 -4.1319833 -4.1399446 -4.1522737 -4.1685452 -4.1877007 -4.2030478 -4.219749 -4.2346816 -4.2368708 -4.2390194 -4.2327266 -4.2178926][-4.1484985 -4.1640978 -4.164897 -4.1610394 -4.1674995 -4.1853666 -4.209754 -4.2288642 -4.2391858 -4.2521048 -4.26191 -4.2574968 -4.2569237 -4.2539353 -4.2415237][-4.1829305 -4.1960931 -4.1960168 -4.18889 -4.1909704 -4.2086911 -4.2333031 -4.2491627 -4.2563753 -4.265861 -4.2721395 -4.2647862 -4.2631607 -4.2636323 -4.2537756][-4.2051454 -4.2101231 -4.2064433 -4.2011361 -4.2059059 -4.22111 -4.2409272 -4.2545538 -4.2621126 -4.2690468 -4.2726359 -4.2657952 -4.2657056 -4.2679791 -4.26264]]...]
INFO - root - 2017-12-06 03:45:32.185831: step 13210, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 20h:30m:40s remains)
INFO - root - 2017-12-06 03:45:34.493689: step 13220, loss = 2.09, batch loss = 2.03 (35.5 examples/sec; 0.225 sec/batch; 19h:59m:22s remains)
INFO - root - 2017-12-06 03:45:36.806887: step 13230, loss = 2.03, batch loss = 1.97 (34.1 examples/sec; 0.234 sec/batch; 20h:46m:50s remains)
INFO - root - 2017-12-06 03:45:39.153647: step 13240, loss = 2.06, batch loss = 2.00 (33.7 examples/sec; 0.237 sec/batch; 21h:03m:13s remains)
INFO - root - 2017-12-06 03:45:41.467725: step 13250, loss = 2.07, batch loss = 2.01 (34.0 examples/sec; 0.235 sec/batch; 20h:51m:58s remains)
INFO - root - 2017-12-06 03:45:43.799299: step 13260, loss = 2.06, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:42m:52s remains)
INFO - root - 2017-12-06 03:45:46.099022: step 13270, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:45m:33s remains)
INFO - root - 2017-12-06 03:45:48.396349: step 13280, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:28m:55s remains)
INFO - root - 2017-12-06 03:45:50.644505: step 13290, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.226 sec/batch; 20h:04m:02s remains)
INFO - root - 2017-12-06 03:45:52.922968: step 13300, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 20h:27m:34s remains)
2017-12-06 03:45:53.244784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1454616 -4.1526856 -4.1508346 -4.1346288 -4.1093783 -4.089797 -4.0674148 -4.0339637 -3.9937949 -3.9628108 -3.9685128 -4.0007572 -4.0289192 -4.034462 -4.0149493][-4.1742697 -4.1864247 -4.1864696 -4.17059 -4.1457024 -4.1272507 -4.1065149 -4.0769634 -4.0419235 -4.0134559 -4.0129457 -4.0359211 -4.0599632 -4.0636606 -4.0417638][-4.2039652 -4.2177205 -4.2148552 -4.19568 -4.1691637 -4.1539617 -4.1413054 -4.118392 -4.0891037 -4.0639014 -4.0543308 -4.0573621 -4.065239 -4.0644584 -4.0568852][-4.2185645 -4.2284651 -4.2172656 -4.1877227 -4.1589971 -4.1467228 -4.1385894 -4.1203966 -4.0965443 -4.0768657 -4.0596652 -4.0503888 -4.0489144 -4.0477839 -4.0532007][-4.2249851 -4.2296653 -4.2098417 -4.1712613 -4.1371846 -4.1200452 -4.1082377 -4.0878677 -4.0653543 -4.0496778 -4.0336294 -4.0343995 -4.0416489 -4.0452805 -4.0582209][-4.2248316 -4.2211795 -4.1939816 -4.1483965 -4.1052094 -4.0805435 -4.0650816 -4.0395203 -4.0160336 -4.001318 -3.9895077 -4.007906 -4.0341806 -4.0440969 -4.0584946][-4.1838579 -4.173039 -4.1456957 -4.0993338 -4.0542831 -4.0357237 -4.0314512 -4.0095921 -3.9844687 -3.9627485 -3.9468424 -3.9750407 -4.0168338 -4.0291076 -4.04488][-4.1275682 -4.1141891 -4.0893059 -4.0421486 -4.0009155 -4.0013547 -4.0150609 -4.00155 -3.9799306 -3.9531159 -3.9323156 -3.958447 -3.9999983 -4.0121369 -4.0296659][-4.0825763 -4.073225 -4.0481234 -4.0013466 -3.9732401 -3.9899111 -4.0116014 -4.0115652 -4.00368 -3.981719 -3.9623041 -3.9741535 -3.9946694 -3.9988711 -4.0204639][-4.0619478 -4.0560493 -4.0335922 -3.9946969 -3.9820237 -4.009388 -4.0356913 -4.0482907 -4.058887 -4.0499225 -4.0341463 -4.0290251 -4.02332 -4.0118728 -4.0247064][-4.0599265 -4.0578356 -4.0392566 -4.0090909 -4.007081 -4.0384932 -4.064054 -4.0803509 -4.1030931 -4.1131544 -4.1031737 -4.0886416 -4.065176 -4.037529 -4.036953][-4.0731688 -4.0717335 -4.0518246 -4.0271969 -4.0329056 -4.0554714 -4.0688176 -4.0807528 -4.1091065 -4.1340733 -4.1352639 -4.12083 -4.092247 -4.0602555 -4.049901][-4.1032972 -4.0963798 -4.0719781 -4.0499115 -4.0534048 -4.0622759 -4.0583568 -4.0612483 -4.0896831 -4.1199083 -4.1250319 -4.1129823 -4.0914316 -4.07125 -4.0601335][-4.1374774 -4.1284261 -4.1059246 -4.0858483 -4.0793915 -4.0738821 -4.0599217 -4.0566297 -4.0817904 -4.1080918 -4.1093149 -4.0979519 -4.0847421 -4.0762038 -4.0685706][-4.1573882 -4.1465483 -4.128046 -4.1098089 -4.095717 -4.0824523 -4.0683665 -4.0671663 -4.0884109 -4.1074052 -4.1055822 -4.0963445 -4.0879464 -4.08363 -4.0819569]]...]
INFO - root - 2017-12-06 03:45:55.544589: step 13310, loss = 2.07, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 20h:45m:33s remains)
INFO - root - 2017-12-06 03:45:57.841558: step 13320, loss = 2.06, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:28m:39s remains)
INFO - root - 2017-12-06 03:46:00.174305: step 13330, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 20h:20m:18s remains)
INFO - root - 2017-12-06 03:46:02.477020: step 13340, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 20h:44m:36s remains)
INFO - root - 2017-12-06 03:46:04.764181: step 13350, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 20h:19m:31s remains)
INFO - root - 2017-12-06 03:46:07.084321: step 13360, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 19h:51m:32s remains)
INFO - root - 2017-12-06 03:46:09.400308: step 13370, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:28m:07s remains)
INFO - root - 2017-12-06 03:46:11.709833: step 13380, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:44m:46s remains)
INFO - root - 2017-12-06 03:46:14.017945: step 13390, loss = 2.09, batch loss = 2.03 (32.4 examples/sec; 0.247 sec/batch; 21h:54m:37s remains)
INFO - root - 2017-12-06 03:46:16.335134: step 13400, loss = 2.04, batch loss = 1.98 (35.8 examples/sec; 0.223 sec/batch; 19h:48m:11s remains)
2017-12-06 03:46:16.618880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3165975 -4.30056 -4.2753811 -4.2369938 -4.1991544 -4.1628881 -4.138803 -4.1186028 -4.0987377 -4.1039495 -4.1182318 -4.1355505 -4.1701307 -4.2097631 -4.2457933][-4.3120642 -4.2941251 -4.2644958 -4.2231808 -4.1812587 -4.1441731 -4.120079 -4.0996304 -4.0862756 -4.1021633 -4.1325626 -4.1619878 -4.2040792 -4.2460494 -4.2808213][-4.3063617 -4.2857175 -4.2543979 -4.212975 -4.1687093 -4.1309896 -4.1053858 -4.0843682 -4.0801778 -4.106257 -4.1441646 -4.1785874 -4.2239647 -4.2692814 -4.3018293][-4.3013358 -4.2780786 -4.2441125 -4.1992178 -4.1502781 -4.1071534 -4.0789285 -4.0596642 -4.0654144 -4.100812 -4.14283 -4.1791244 -4.2246208 -4.2733645 -4.3068862][-4.2993093 -4.2747846 -4.235364 -4.1794882 -4.1181903 -4.0623641 -4.0247412 -4.0072913 -4.0254641 -4.07201 -4.1205678 -4.1625695 -4.2129803 -4.2657347 -4.3050504][-4.2975564 -4.2705617 -4.225318 -4.15708 -4.0750017 -3.9978404 -3.9441831 -3.9339511 -3.9776969 -4.0452228 -4.10182 -4.1504931 -4.2060428 -4.259666 -4.3047109][-4.2972293 -4.2660236 -4.2133703 -4.1299367 -4.022778 -3.92077 -3.8498979 -3.8505905 -3.9310603 -4.0248489 -4.09222 -4.1496758 -4.2091293 -4.2617407 -4.3071985][-4.3020415 -4.2688565 -4.2123513 -4.1181364 -3.994127 -3.8721149 -3.7894845 -3.804224 -3.9103773 -4.0208769 -4.0940433 -4.1566577 -4.2187724 -4.2704172 -4.3130608][-4.3092742 -4.2792444 -4.2284961 -4.1380157 -4.011683 -3.8830285 -3.800652 -3.8230011 -3.9348464 -4.0429564 -4.1167212 -4.1815963 -4.2421703 -4.2864161 -4.3219647][-4.3149514 -4.2906241 -4.2512569 -4.1730261 -4.0580263 -3.9412751 -3.8733792 -3.8969514 -3.9940305 -4.0903325 -4.1646709 -4.2276807 -4.2782578 -4.3092728 -4.3346896][-4.32222 -4.3027139 -4.270442 -4.2063112 -4.1117344 -4.0192323 -3.9745624 -3.9970467 -4.0696459 -4.148047 -4.2148814 -4.268065 -4.3076859 -4.3297386 -4.3461337][-4.3326125 -4.3156414 -4.2876744 -4.2359214 -4.1584296 -4.0873156 -4.0640135 -4.0894132 -4.1455917 -4.2041283 -4.2565122 -4.2966828 -4.3259263 -4.3430872 -4.3551631][-4.340126 -4.3232064 -4.2982888 -4.2565231 -4.1932907 -4.1404772 -4.1321163 -4.1618438 -4.2092204 -4.2531743 -4.2924476 -4.3205056 -4.3391013 -4.3529763 -4.3617988][-4.3424072 -4.3273 -4.3064427 -4.2734857 -4.2241869 -4.1852641 -4.1843257 -4.2153358 -4.2556405 -4.2893176 -4.3174348 -4.3368945 -4.3490582 -4.3580332 -4.3617797][-4.342608 -4.3310561 -4.3157525 -4.2913694 -4.2556696 -4.227026 -4.226757 -4.2521353 -4.2843761 -4.3107052 -4.3312869 -4.3448658 -4.3527727 -4.3568945 -4.3573713]]...]
INFO - root - 2017-12-06 03:46:18.943855: step 13410, loss = 2.07, batch loss = 2.01 (33.3 examples/sec; 0.241 sec/batch; 21h:19m:12s remains)
INFO - root - 2017-12-06 03:46:21.226164: step 13420, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 19h:45m:08s remains)
INFO - root - 2017-12-06 03:46:23.498911: step 13430, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:21m:35s remains)
INFO - root - 2017-12-06 03:46:25.825549: step 13440, loss = 2.05, batch loss = 1.99 (34.7 examples/sec; 0.231 sec/batch; 20h:25m:55s remains)
INFO - root - 2017-12-06 03:46:28.124750: step 13450, loss = 2.06, batch loss = 2.00 (36.7 examples/sec; 0.218 sec/batch; 19h:20m:21s remains)
INFO - root - 2017-12-06 03:46:30.408267: step 13460, loss = 2.04, batch loss = 1.98 (35.9 examples/sec; 0.223 sec/batch; 19h:44m:33s remains)
INFO - root - 2017-12-06 03:46:32.711762: step 13470, loss = 2.05, batch loss = 2.00 (32.9 examples/sec; 0.243 sec/batch; 21h:32m:25s remains)
INFO - root - 2017-12-06 03:46:35.032167: step 13480, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:52m:57s remains)
INFO - root - 2017-12-06 03:46:37.283152: step 13490, loss = 2.06, batch loss = 2.00 (33.3 examples/sec; 0.240 sec/batch; 21h:16m:49s remains)
INFO - root - 2017-12-06 03:46:39.596005: step 13500, loss = 2.11, batch loss = 2.05 (34.8 examples/sec; 0.230 sec/batch; 20h:20m:56s remains)
2017-12-06 03:46:39.897598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2414813 -4.2323656 -4.22679 -4.2259388 -4.23225 -4.240334 -4.2523074 -4.268816 -4.2852554 -4.2949944 -4.3034482 -4.310492 -4.3063807 -4.2959929 -4.2859979][-4.248651 -4.23479 -4.2258821 -4.2177014 -4.2204041 -4.2279553 -4.2356353 -4.25346 -4.2759247 -4.2905512 -4.3006716 -4.3055668 -4.3010654 -4.2914839 -4.278194][-4.2601371 -4.2455535 -4.2353749 -4.2213464 -4.2161784 -4.215498 -4.2137341 -4.2290792 -4.2555127 -4.2752066 -4.2878637 -4.2946472 -4.2908511 -4.2805119 -4.2639103][-4.2691207 -4.2587175 -4.2453332 -4.2247591 -4.2117534 -4.2028346 -4.1906877 -4.2025585 -4.2315822 -4.2607026 -4.2780385 -4.2838225 -4.277864 -4.2637253 -4.2458878][-4.2733831 -4.2655239 -4.2474031 -4.2206578 -4.2013516 -4.18273 -4.1557379 -4.1590209 -4.1881876 -4.2289548 -4.2560382 -4.2679386 -4.2651381 -4.2494993 -4.230083][-4.2686887 -4.2634392 -4.241497 -4.207387 -4.1760683 -4.1444492 -4.1050558 -4.0995088 -4.1266518 -4.1764646 -4.2188759 -4.2472978 -4.254529 -4.24121 -4.2180247][-4.2593045 -4.2577066 -4.2367768 -4.1936369 -4.1453352 -4.0964646 -4.04554 -4.0315318 -4.054985 -4.1096125 -4.1697412 -4.2219567 -4.2497506 -4.2447276 -4.2214942][-4.2559867 -4.255197 -4.2324171 -4.182271 -4.1158609 -4.0460334 -3.9771893 -3.9491887 -3.9701824 -4.03097 -4.1112051 -4.1873274 -4.236269 -4.2428989 -4.2249818][-4.2576594 -4.2515635 -4.2237277 -4.1671433 -4.089972 -4.0065336 -3.9283929 -3.8951726 -3.9150319 -3.979687 -4.0736213 -4.1611629 -4.2223616 -4.2400928 -4.2317553][-4.2593737 -4.2479248 -4.215517 -4.1585054 -4.0815744 -3.9974937 -3.9274664 -3.9056883 -3.9342253 -3.9994824 -4.0887656 -4.1651921 -4.2201619 -4.2399411 -4.2408261][-4.2580042 -4.245965 -4.2145271 -4.164484 -4.0984306 -4.0263705 -3.9737511 -3.9688447 -4.0036807 -4.0615516 -4.1306977 -4.1796827 -4.2103124 -4.22227 -4.2270041][-4.2561431 -4.2455554 -4.2211471 -4.1842737 -4.1384082 -4.0898805 -4.05882 -4.0637479 -4.0991926 -4.14244 -4.1819649 -4.1964889 -4.1921797 -4.1829963 -4.1802764][-4.2447782 -4.2379565 -4.2226696 -4.2011275 -4.1755862 -4.1531472 -4.1457505 -4.1592255 -4.1878614 -4.2115459 -4.2194996 -4.1999145 -4.1623855 -4.12686 -4.1084032][-4.2126584 -4.211659 -4.2076197 -4.2009587 -4.19042 -4.1822639 -4.1906261 -4.2138219 -4.2388368 -4.2496424 -4.2365241 -4.1940923 -4.1352582 -4.077383 -4.0406737][-4.1730871 -4.1739073 -4.1702986 -4.166028 -4.1627078 -4.1607037 -4.1787467 -4.2148252 -4.2469344 -4.251883 -4.2281971 -4.1730108 -4.1009579 -4.0302405 -3.9815834]]...]
INFO - root - 2017-12-06 03:46:42.185203: step 13510, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:41m:35s remains)
INFO - root - 2017-12-06 03:46:44.492286: step 13520, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 20h:13m:00s remains)
INFO - root - 2017-12-06 03:46:46.790967: step 13530, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.233 sec/batch; 20h:37m:58s remains)
INFO - root - 2017-12-06 03:46:49.076583: step 13540, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 20h:08m:26s remains)
INFO - root - 2017-12-06 03:46:51.363196: step 13550, loss = 2.08, batch loss = 2.03 (34.9 examples/sec; 0.229 sec/batch; 20h:19m:09s remains)
INFO - root - 2017-12-06 03:46:53.656301: step 13560, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.226 sec/batch; 19h:58m:53s remains)
INFO - root - 2017-12-06 03:46:55.998790: step 13570, loss = 2.03, batch loss = 1.97 (35.2 examples/sec; 0.227 sec/batch; 20h:07m:52s remains)
INFO - root - 2017-12-06 03:46:58.322995: step 13580, loss = 2.04, batch loss = 1.98 (33.8 examples/sec; 0.237 sec/batch; 20h:59m:33s remains)
INFO - root - 2017-12-06 03:47:00.637587: step 13590, loss = 2.08, batch loss = 2.03 (35.7 examples/sec; 0.224 sec/batch; 19h:49m:30s remains)
INFO - root - 2017-12-06 03:47:02.898901: step 13600, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.226 sec/batch; 20h:03m:49s remains)
2017-12-06 03:47:03.189771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1494122 -4.1628504 -4.1396217 -4.0900903 -4.0696254 -4.052135 -4.0380411 -4.0219979 -3.9916337 -3.9722843 -3.9703467 -3.9883301 -4.0021067 -4.0185328 -4.0344591][-4.1469154 -4.1511388 -4.1213975 -4.0655985 -4.0344396 -4.0166817 -4.0058613 -4.0029655 -3.9994402 -3.9978731 -3.991904 -3.9877596 -3.9814098 -3.9891751 -4.0085506][-4.1402245 -4.1362605 -4.099371 -4.0411739 -3.9976647 -3.9713824 -3.9607289 -3.9798257 -4.0109639 -4.030098 -4.0251584 -4.0029845 -3.9813044 -3.9862096 -4.0066671][-4.138011 -4.1318064 -4.0889087 -4.0294533 -3.9756994 -3.9342971 -3.9149406 -3.9512773 -4.0094371 -4.0478516 -4.0487838 -4.0221829 -3.9907107 -3.9840524 -3.9984212][-4.13922 -4.1331644 -4.0878754 -4.0260024 -3.9625726 -3.8964696 -3.8573687 -3.9055481 -3.9913282 -4.0490031 -4.0623374 -4.0431504 -4.0106268 -3.9903631 -3.9931622][-4.1314826 -4.1258416 -4.0846686 -4.0200887 -3.9425282 -3.8432808 -3.7764108 -3.8392029 -3.9564302 -4.0353794 -4.0684462 -4.0664835 -4.042726 -4.0173855 -4.0105448][-4.1257949 -4.1253133 -4.0892687 -4.0190921 -3.9250402 -3.7909245 -3.689966 -3.7685301 -3.9147854 -4.0104051 -4.0630417 -4.08331 -4.075264 -4.0559378 -4.0444727][-4.1294093 -4.1332884 -4.1016526 -4.0269952 -3.9261963 -3.786469 -3.6802261 -3.7554014 -3.8969171 -3.9944923 -4.056129 -4.0941033 -4.1070967 -4.0963268 -4.079423][-4.132731 -4.1363592 -4.1075592 -4.0412207 -3.9592581 -3.8588362 -3.792505 -3.8333836 -3.9233909 -3.9978311 -4.0549603 -4.0990152 -4.1254816 -4.1225729 -4.1029329][-4.1318178 -4.1305404 -4.1024308 -4.0515084 -3.9998276 -3.9489884 -3.9177465 -3.929527 -3.9671342 -4.0135446 -4.0579524 -4.0983272 -4.1284676 -4.1322865 -4.113975][-4.1277819 -4.1223631 -4.0937729 -4.0559435 -4.0302081 -4.0106745 -3.9968717 -3.9925075 -4.0021524 -4.03002 -4.0630393 -4.095253 -4.1239896 -4.1292529 -4.1073][-4.1170173 -4.1123142 -4.083746 -4.055274 -4.0485473 -4.0454497 -4.0377789 -4.0261083 -4.0245647 -4.0403175 -4.0652819 -4.0881338 -4.1101775 -4.1130195 -4.0890279][-4.0912318 -4.0896845 -4.0633492 -4.0467854 -4.0569391 -4.0660405 -4.0585165 -4.0460753 -4.0440874 -4.0564146 -4.0716949 -4.0821743 -4.0958424 -4.0946722 -4.0672026][-4.0569763 -4.060298 -4.0395579 -4.0380721 -4.060617 -4.0749068 -4.0680637 -4.0530896 -4.0544229 -4.0697608 -4.0809522 -4.08644 -4.090199 -4.0811896 -4.0469623][-4.0459642 -4.0515079 -4.0359049 -4.0375819 -4.0550609 -4.0618448 -4.05188 -4.0358758 -4.0432138 -4.0638056 -4.0807881 -4.0942111 -4.1005297 -4.0895796 -4.0527987]]...]
INFO - root - 2017-12-06 03:47:05.572907: step 13610, loss = 2.08, batch loss = 2.02 (34.6 examples/sec; 0.231 sec/batch; 20h:30m:00s remains)
INFO - root - 2017-12-06 03:47:07.832565: step 13620, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 20h:16m:54s remains)
INFO - root - 2017-12-06 03:47:10.163887: step 13630, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 20h:30m:04s remains)
INFO - root - 2017-12-06 03:47:12.462621: step 13640, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 20h:54m:47s remains)
INFO - root - 2017-12-06 03:47:14.825176: step 13650, loss = 2.06, batch loss = 2.00 (32.8 examples/sec; 0.244 sec/batch; 21h:37m:36s remains)
INFO - root - 2017-12-06 03:47:17.122843: step 13660, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:41m:46s remains)
INFO - root - 2017-12-06 03:47:19.447749: step 13670, loss = 2.04, batch loss = 1.98 (34.9 examples/sec; 0.229 sec/batch; 20h:17m:53s remains)
INFO - root - 2017-12-06 03:47:21.788369: step 13680, loss = 2.09, batch loss = 2.03 (35.2 examples/sec; 0.227 sec/batch; 20h:06m:40s remains)
INFO - root - 2017-12-06 03:47:24.086580: step 13690, loss = 2.06, batch loss = 2.00 (34.1 examples/sec; 0.235 sec/batch; 20h:46m:34s remains)
INFO - root - 2017-12-06 03:47:26.353104: step 13700, loss = 2.09, batch loss = 2.03 (35.2 examples/sec; 0.227 sec/batch; 20h:05m:52s remains)
2017-12-06 03:47:26.667795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2446418 -4.242712 -4.2419972 -4.2392416 -4.2385669 -4.24481 -4.2494106 -4.2568965 -4.2618947 -4.2571015 -4.23696 -4.2095833 -4.1784124 -4.1435652 -4.1022549][-4.2464662 -4.2352195 -4.2262588 -4.2188749 -4.2163382 -4.2244163 -4.2296977 -4.2401218 -4.2489376 -4.2467737 -4.2284307 -4.2023525 -4.1717997 -4.1369123 -4.0926709][-4.2477117 -4.2275786 -4.2079506 -4.1908298 -4.1840158 -4.1924791 -4.2012959 -4.2174106 -4.2330985 -4.2373781 -4.2237682 -4.2007904 -4.1749663 -4.1415782 -4.0951371][-4.23842 -4.207747 -4.17448 -4.1457267 -4.131886 -4.1417723 -4.1588917 -4.188128 -4.2137341 -4.2261796 -4.2193613 -4.2019472 -4.1856041 -4.1604528 -4.1197176][-4.2220044 -4.1790547 -4.13445 -4.1020064 -4.088284 -4.1043825 -4.1307907 -4.1665177 -4.19649 -4.2133989 -4.2140727 -4.2035928 -4.1977096 -4.1868272 -4.1593184][-4.2115541 -4.1623445 -4.11419 -4.0855665 -4.0773697 -4.0993915 -4.1260586 -4.15304 -4.174 -4.19009 -4.1953688 -4.1886263 -4.1915455 -4.1993661 -4.1902046][-4.2023082 -4.153513 -4.1056027 -4.0821891 -4.0817404 -4.1065626 -4.1235456 -4.13056 -4.1356049 -4.1474123 -4.1566353 -4.1574554 -4.1744766 -4.2036166 -4.2123647][-4.1980953 -4.1548953 -4.1134992 -4.1042724 -4.115242 -4.14041 -4.1448503 -4.130034 -4.1132565 -4.1172781 -4.129025 -4.1407824 -4.1701412 -4.2099786 -4.2235694][-4.1982841 -4.1606579 -4.1324482 -4.1395736 -4.1611609 -4.1851125 -4.1823339 -4.1545105 -4.1214166 -4.1205864 -4.1361666 -4.1556258 -4.1854043 -4.2219567 -4.2300591][-4.1901903 -4.1614685 -4.1510339 -4.1733747 -4.2023859 -4.2228065 -4.2121058 -4.1766243 -4.1388516 -4.1387429 -4.1591287 -4.1812816 -4.2054968 -4.231214 -4.2287426][-4.1769419 -4.1594543 -4.1680231 -4.2034774 -4.2390223 -4.2601638 -4.2464805 -4.2082005 -4.1734514 -4.1741014 -4.191277 -4.2088628 -4.2258973 -4.2388296 -4.2271113][-4.1734633 -4.1680026 -4.1876831 -4.2292433 -4.2696567 -4.2952924 -4.2850695 -4.2497025 -4.2202129 -4.220747 -4.2302828 -4.2351971 -4.2403784 -4.2439203 -4.2273536][-4.1809621 -4.1901875 -4.2172513 -4.2580571 -4.2978415 -4.32692 -4.3231368 -4.2963314 -4.2717762 -4.27058 -4.269176 -4.2574377 -4.2500315 -4.24664 -4.2296257][-4.1995578 -4.2211719 -4.2521811 -4.2885413 -4.3236094 -4.3500409 -4.34921 -4.331111 -4.3116732 -4.3063917 -4.2958779 -4.2750521 -4.259758 -4.2520547 -4.2347059][-4.2318239 -4.2575488 -4.2870903 -4.3169632 -4.3448491 -4.3638716 -4.3606577 -4.3445148 -4.3255363 -4.3167663 -4.3003941 -4.2733879 -4.2563748 -4.2497663 -4.2345777]]...]
INFO - root - 2017-12-06 03:47:28.963729: step 13710, loss = 2.04, batch loss = 1.98 (34.4 examples/sec; 0.233 sec/batch; 20h:37m:15s remains)
INFO - root - 2017-12-06 03:47:31.234822: step 13720, loss = 2.06, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:47m:13s remains)
INFO - root - 2017-12-06 03:47:33.522410: step 13730, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:01m:54s remains)
INFO - root - 2017-12-06 03:47:35.834239: step 13740, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 20h:22m:00s remains)
INFO - root - 2017-12-06 03:47:38.135682: step 13750, loss = 2.05, batch loss = 1.99 (35.3 examples/sec; 0.227 sec/batch; 20h:05m:07s remains)
INFO - root - 2017-12-06 03:47:40.403461: step 13760, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:54m:17s remains)
INFO - root - 2017-12-06 03:47:42.684763: step 13770, loss = 2.06, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 20h:30m:23s remains)
INFO - root - 2017-12-06 03:47:45.023486: step 13780, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.231 sec/batch; 20h:24m:33s remains)
INFO - root - 2017-12-06 03:47:47.361983: step 13790, loss = 2.05, batch loss = 1.99 (33.8 examples/sec; 0.237 sec/batch; 20h:57m:27s remains)
INFO - root - 2017-12-06 03:47:49.646329: step 13800, loss = 2.08, batch loss = 2.03 (35.1 examples/sec; 0.228 sec/batch; 20h:11m:03s remains)
2017-12-06 03:47:49.929071: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1894183 -4.1655445 -4.1697702 -4.1867676 -4.1981959 -4.1946778 -4.1795487 -4.1653914 -4.1708956 -4.1971073 -4.2329412 -4.2575607 -4.2640638 -4.2624044 -4.2532382][-4.1692629 -4.1380129 -4.1366706 -4.1576443 -4.1744018 -4.1684918 -4.1429191 -4.1169572 -4.1144819 -4.1484313 -4.2031932 -4.2365694 -4.2445617 -4.2430511 -4.2377315][-4.15772 -4.1249723 -4.1186652 -4.1381221 -4.1539917 -4.144846 -4.1064024 -4.0627089 -4.0463691 -4.0918107 -4.1719923 -4.2207794 -4.2306886 -4.2290797 -4.2257047][-4.1515703 -4.1218739 -4.11224 -4.1220994 -4.1268911 -4.1164455 -4.0658069 -3.9908285 -3.9519913 -4.0172968 -4.1251082 -4.1908479 -4.2071834 -4.2080121 -4.2047176][-4.1504016 -4.1211529 -4.1060023 -4.1060424 -4.1034465 -4.0891991 -4.0228386 -3.9085588 -3.8375652 -3.9325137 -4.08061 -4.1649694 -4.1905494 -4.1922746 -4.1856618][-4.1644835 -4.1376982 -4.1150465 -4.105401 -4.0932593 -4.0739913 -3.9988604 -3.8565931 -3.7509463 -3.8620217 -4.0453663 -4.1497927 -4.1812787 -4.1823 -4.1714268][-4.1870632 -4.1643167 -4.1330352 -4.1113548 -4.0899343 -4.0717688 -4.0079055 -3.8771055 -3.7721667 -3.8681178 -4.0400181 -4.1427407 -4.1735888 -4.1758118 -4.1603193][-4.2039695 -4.1865797 -4.1556869 -4.1235704 -4.0945415 -4.0828013 -4.0462551 -3.9601469 -3.8901646 -3.9571805 -4.07493 -4.1462555 -4.1643529 -4.1644564 -4.1483569][-4.2091861 -4.1966109 -4.1667304 -4.1224327 -4.0912151 -4.0919428 -4.0872836 -4.0448861 -4.0147734 -4.0575695 -4.1189995 -4.150629 -4.1530147 -4.1506314 -4.1410341][-4.2181458 -4.2109995 -4.1857648 -4.1425757 -4.1146235 -4.1201696 -4.1329427 -4.1214328 -4.11432 -4.1409664 -4.1608915 -4.1637983 -4.1528234 -4.1432977 -4.1453233][-4.2338748 -4.2301664 -4.2115254 -4.1802092 -4.16457 -4.1701827 -4.1821508 -4.1800494 -4.17634 -4.187861 -4.1839681 -4.1732697 -4.1505942 -4.1376433 -4.151526][-4.2509966 -4.2442422 -4.2268414 -4.2066512 -4.2035942 -4.2143736 -4.2269492 -4.2243662 -4.2119904 -4.2100415 -4.1922879 -4.1706586 -4.1452088 -4.1411934 -4.1729307][-4.2641335 -4.2514677 -4.2366648 -4.2276816 -4.2343016 -4.2497 -4.2626028 -4.2636948 -4.2546282 -4.2464275 -4.2264237 -4.2021451 -4.1803074 -4.1849704 -4.2226219][-4.2751288 -4.25811 -4.24526 -4.2435613 -4.2557392 -4.2758961 -4.2904038 -4.2941327 -4.2887583 -4.2817926 -4.2654424 -4.2453341 -4.2307253 -4.2384381 -4.268784][-4.2901287 -4.2735815 -4.2619843 -4.262444 -4.2737851 -4.2933493 -4.3071604 -4.3112192 -4.309504 -4.3075228 -4.2982779 -4.2839856 -4.2740326 -4.2787204 -4.2973351]]...]
INFO - root - 2017-12-06 03:47:52.195773: step 13810, loss = 2.06, batch loss = 2.01 (35.0 examples/sec; 0.228 sec/batch; 20h:12m:55s remains)
INFO - root - 2017-12-06 03:47:54.488971: step 13820, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 20h:14m:56s remains)
INFO - root - 2017-12-06 03:47:56.787383: step 13830, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:11m:13s remains)
INFO - root - 2017-12-06 03:47:59.096400: step 13840, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:39m:16s remains)
INFO - root - 2017-12-06 03:48:01.380472: step 13850, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:00m:40s remains)
INFO - root - 2017-12-06 03:48:03.639080: step 13860, loss = 2.04, batch loss = 1.98 (35.8 examples/sec; 0.223 sec/batch; 19h:46m:17s remains)
INFO - root - 2017-12-06 03:48:05.909959: step 13870, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:51m:19s remains)
INFO - root - 2017-12-06 03:48:08.212022: step 13880, loss = 2.05, batch loss = 1.99 (33.6 examples/sec; 0.238 sec/batch; 21h:04m:09s remains)
INFO - root - 2017-12-06 03:48:10.502168: step 13890, loss = 2.07, batch loss = 2.02 (34.7 examples/sec; 0.231 sec/batch; 20h:25m:39s remains)
INFO - root - 2017-12-06 03:48:12.780214: step 13900, loss = 2.05, batch loss = 1.99 (34.9 examples/sec; 0.229 sec/batch; 20h:15m:55s remains)
2017-12-06 03:48:13.072445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2768841 -4.259582 -4.2419038 -4.2285285 -4.2230835 -4.2285595 -4.2397251 -4.2445164 -4.2393441 -4.231482 -4.2269258 -4.2261076 -4.2257285 -4.2261362 -4.230185][-4.2492576 -4.2216034 -4.196486 -4.1809149 -4.1813059 -4.1978617 -4.2200456 -4.2315764 -4.2302856 -4.2248259 -4.2211895 -4.2208452 -4.2201829 -4.2197285 -4.2224216][-4.2213349 -4.1854038 -4.1554675 -4.1408691 -4.1471 -4.1708956 -4.1993361 -4.2161608 -4.2207661 -4.2214618 -4.2197175 -4.2188449 -4.2171664 -4.2148333 -4.2140069][-4.1994033 -4.1606908 -4.1305876 -4.1177115 -4.1254168 -4.1489077 -4.1759372 -4.1941919 -4.2056131 -4.2149973 -4.2173066 -4.2159142 -4.211338 -4.2052841 -4.2003098][-4.1817255 -4.1443324 -4.1178284 -4.10818 -4.1158738 -4.1364479 -4.1596079 -4.1768007 -4.1930509 -4.2089038 -4.2142124 -4.2106948 -4.201436 -4.190731 -4.1835084][-4.1678257 -4.1359539 -4.1161232 -4.1100473 -4.1167769 -4.1329403 -4.1509924 -4.167841 -4.1891413 -4.2106328 -4.218411 -4.2136531 -4.200171 -4.1851878 -4.1780863][-4.1602516 -4.1330857 -4.1166258 -4.1100879 -4.1146617 -4.1277275 -4.1428375 -4.1611433 -4.1863651 -4.2135725 -4.2259326 -4.2232981 -4.2096696 -4.1973815 -4.197504][-4.1593475 -4.1314421 -4.1122465 -4.1038594 -4.1059194 -4.1176548 -4.1311474 -4.148953 -4.1732168 -4.1998935 -4.2155137 -4.2181597 -4.21102 -4.2099719 -4.2226138][-4.1603742 -4.128572 -4.1072145 -4.0990419 -4.1015263 -4.115818 -4.131155 -4.1480446 -4.1690774 -4.1915431 -4.2051549 -4.2099285 -4.2099919 -4.2190003 -4.2383027][-4.1637053 -4.13008 -4.108407 -4.1017189 -4.1052504 -4.1226821 -4.1417542 -4.159904 -4.1795211 -4.1954374 -4.2001882 -4.2011409 -4.2044086 -4.2158589 -4.23545][-4.1732726 -4.1395793 -4.1185927 -4.1122656 -4.1144347 -4.1283493 -4.1459212 -4.1611724 -4.1759267 -4.1855206 -4.1863694 -4.1883731 -4.1961536 -4.2079115 -4.2223182][-4.191083 -4.1584563 -4.1358924 -4.1236234 -4.1157293 -4.1174569 -4.1285229 -4.1407261 -4.152339 -4.1601229 -4.163362 -4.1730433 -4.1869421 -4.1978693 -4.2051063][-4.20395 -4.1674662 -4.137291 -4.1140409 -4.0939717 -4.0847058 -4.0924878 -4.1039538 -4.1128826 -4.1191735 -4.1254663 -4.1405067 -4.1570525 -4.1676035 -4.1706057][-4.2003088 -4.1533608 -4.1103134 -4.076684 -4.0533066 -4.0442724 -4.0562534 -4.0708447 -4.080502 -4.08808 -4.097383 -4.1162834 -4.133893 -4.1428127 -4.142539][-4.1749077 -4.1168666 -4.065886 -4.0308895 -4.0112453 -4.0096726 -4.0294232 -4.0476112 -4.0592303 -4.0696721 -4.0821991 -4.1026173 -4.1201506 -4.1281552 -4.1268945]]...]
INFO - root - 2017-12-06 03:48:15.376916: step 13910, loss = 2.06, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 20h:04m:03s remains)
INFO - root - 2017-12-06 03:48:17.670859: step 13920, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:11m:04s remains)
INFO - root - 2017-12-06 03:48:19.967497: step 13930, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 19h:53m:13s remains)
INFO - root - 2017-12-06 03:48:22.251606: step 13940, loss = 2.04, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 19h:50m:55s remains)
INFO - root - 2017-12-06 03:48:24.544739: step 13950, loss = 2.05, batch loss = 1.99 (33.4 examples/sec; 0.239 sec/batch; 21h:10m:49s remains)
INFO - root - 2017-12-06 03:48:26.855888: step 13960, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.228 sec/batch; 20h:12m:47s remains)
INFO - root - 2017-12-06 03:48:29.145453: step 13970, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 19h:55m:16s remains)
INFO - root - 2017-12-06 03:48:31.405206: step 13980, loss = 2.06, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:42m:50s remains)
INFO - root - 2017-12-06 03:48:33.701222: step 13990, loss = 2.07, batch loss = 2.01 (33.5 examples/sec; 0.239 sec/batch; 21h:07m:17s remains)
INFO - root - 2017-12-06 03:48:35.983192: step 14000, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:19m:48s remains)
2017-12-06 03:48:36.277983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1623545 -4.1597056 -4.1731205 -4.1762085 -4.1716952 -4.1716003 -4.1756163 -4.1934915 -4.2177582 -4.2224636 -4.2230635 -4.224462 -4.2263708 -4.2277889 -4.2282867][-4.1727228 -4.1738415 -4.1914959 -4.2003145 -4.1958237 -4.1924467 -4.1921196 -4.2072973 -4.2320023 -4.2359772 -4.2358322 -4.2366223 -4.240099 -4.2447309 -4.247447][-4.1917686 -4.1937914 -4.2119813 -4.2216606 -4.2153168 -4.207427 -4.2011757 -4.2129016 -4.2362866 -4.2374458 -4.233644 -4.2310443 -4.2358952 -4.245163 -4.2505207][-4.2062478 -4.209374 -4.22939 -4.2379694 -4.2278829 -4.2142134 -4.2022638 -4.2092834 -4.2275233 -4.2259312 -4.2183442 -4.2123375 -4.218 -4.2301 -4.2354455][-4.2085176 -4.2154627 -4.2365828 -4.2432728 -4.2225509 -4.1922059 -4.162672 -4.1564655 -4.1702504 -4.1707969 -4.1661973 -4.1600952 -4.1670957 -4.1809292 -4.183362][-4.2077832 -4.2136726 -4.2280917 -4.2210336 -4.1778955 -4.1161914 -4.0507431 -4.0276837 -4.0509133 -4.0691414 -4.0828829 -4.089766 -4.1039081 -4.1223974 -4.12712][-4.2025285 -4.1986165 -4.1968789 -4.1638246 -4.0845208 -3.980943 -3.8655567 -3.8243775 -3.8802166 -3.9434803 -3.9994383 -4.037323 -4.0676889 -4.0949774 -4.1043863][-4.1971269 -4.1834383 -4.1709862 -4.12629 -4.0333877 -3.9169145 -3.7845821 -3.746887 -3.8299522 -3.9257002 -4.0055585 -4.0572495 -4.0898709 -4.1162133 -4.1216092][-4.1969686 -4.1844769 -4.1780539 -4.1512046 -4.0884385 -4.0141959 -3.9283171 -3.9095361 -3.9699986 -4.0412636 -4.1004734 -4.1358719 -4.1555471 -4.1710653 -4.169652][-4.2015686 -4.1950874 -4.1997018 -4.1920485 -4.1597481 -4.1212788 -4.0748758 -4.0661836 -4.0990462 -4.1369081 -4.1710958 -4.1921554 -4.2037435 -4.212203 -4.20871][-4.1995225 -4.1942163 -4.2024651 -4.2027535 -4.1869693 -4.1695609 -4.1481419 -4.1507359 -4.174614 -4.1930614 -4.2093511 -4.2205105 -4.2259116 -4.2275434 -4.2178903][-4.1917872 -4.182261 -4.190454 -4.195725 -4.1903625 -4.1879592 -4.1827641 -4.1945381 -4.2195415 -4.2292743 -4.2344818 -4.2379932 -4.2370834 -4.2324991 -4.21718][-4.185782 -4.17417 -4.1797667 -4.1861911 -4.1851954 -4.1883106 -4.1901507 -4.2050157 -4.2288876 -4.2327118 -4.2332625 -4.2343941 -4.2337632 -4.23201 -4.2214637][-4.1864758 -4.17471 -4.1797595 -4.186111 -4.1850457 -4.1860728 -4.1872158 -4.2003121 -4.2176452 -4.2157984 -4.2131276 -4.2149987 -4.2192068 -4.2223954 -4.22211][-4.18468 -4.1703596 -4.17335 -4.1804724 -4.1789222 -4.1766949 -4.1762495 -4.1913018 -4.2099066 -4.2098002 -4.209188 -4.2123494 -4.2198062 -4.2274685 -4.2341213]]...]
INFO - root - 2017-12-06 03:48:38.619948: step 14010, loss = 2.06, batch loss = 2.00 (33.5 examples/sec; 0.239 sec/batch; 21h:06m:26s remains)
INFO - root - 2017-12-06 03:48:40.906311: step 14020, loss = 2.09, batch loss = 2.03 (34.1 examples/sec; 0.234 sec/batch; 20h:44m:21s remains)
INFO - root - 2017-12-06 03:48:43.198262: step 14030, loss = 2.06, batch loss = 2.00 (33.6 examples/sec; 0.238 sec/batch; 21h:03m:55s remains)
INFO - root - 2017-12-06 03:48:45.514612: step 14040, loss = 2.04, batch loss = 1.98 (33.7 examples/sec; 0.237 sec/batch; 21h:00m:31s remains)
INFO - root - 2017-12-06 03:48:47.808820: step 14050, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.221 sec/batch; 19h:34m:52s remains)
INFO - root - 2017-12-06 03:48:50.091664: step 14060, loss = 2.04, batch loss = 1.98 (36.0 examples/sec; 0.222 sec/batch; 19h:38m:24s remains)
INFO - root - 2017-12-06 03:48:52.393837: step 14070, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 19h:53m:06s remains)
INFO - root - 2017-12-06 03:48:54.722896: step 14080, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 20h:13m:59s remains)
INFO - root - 2017-12-06 03:48:57.003573: step 14090, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.226 sec/batch; 20h:01m:48s remains)
INFO - root - 2017-12-06 03:48:59.274950: step 14100, loss = 2.03, batch loss = 1.97 (34.8 examples/sec; 0.230 sec/batch; 20h:18m:23s remains)
2017-12-06 03:48:59.562697: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2639818 -4.2561088 -4.2502642 -4.2456679 -4.2441196 -4.2489262 -4.258997 -4.2699461 -4.2785826 -4.2837129 -4.2860808 -4.2840071 -4.2803516 -4.2745562 -4.26827][-4.2341866 -4.2200093 -4.2072291 -4.1959877 -4.1894217 -4.1934838 -4.208096 -4.2284923 -4.248313 -4.2603221 -4.26285 -4.2588544 -4.2528524 -4.2465744 -4.2406368][-4.2058854 -4.1814985 -4.1541123 -4.1305008 -4.1168108 -4.1193027 -4.1408672 -4.1750193 -4.211339 -4.23415 -4.2399917 -4.2349863 -4.2282734 -4.2242446 -4.2225728][-4.17591 -4.1428881 -4.1000657 -4.0638437 -4.0434866 -4.0418434 -4.0637774 -4.1068563 -4.1590943 -4.196712 -4.2089577 -4.2087774 -4.2074151 -4.2094746 -4.2152863][-4.1367078 -4.0970926 -4.0506234 -4.0098767 -3.9795685 -3.9607623 -3.9655838 -4.005765 -4.0759153 -4.1382408 -4.1669645 -4.1789174 -4.1866994 -4.1979661 -4.2108588][-4.1066885 -4.0643115 -4.0202541 -3.9734945 -3.9192736 -3.8589466 -3.8183732 -3.8424144 -3.9385417 -4.0400119 -4.0968962 -4.1235385 -4.1395664 -4.1604476 -4.1809449][-4.0911236 -4.0479426 -4.0033245 -3.9471581 -3.8628926 -3.7476366 -3.6401048 -3.6372004 -3.7686031 -3.9154034 -4.0079727 -4.0569782 -4.0874887 -4.1186886 -4.1439171][-4.0852919 -4.0403314 -3.9918079 -3.9412136 -3.8630848 -3.7504342 -3.6319036 -3.6084607 -3.7298114 -3.8748307 -3.9709926 -4.0319729 -4.0757594 -4.1166921 -4.1420808][-4.0881348 -4.0365887 -3.9890997 -3.9662044 -3.9333217 -3.8764529 -3.8091245 -3.7893224 -3.8554015 -3.9394884 -3.9945605 -4.0397186 -4.0909653 -4.1413746 -4.1637187][-4.0836582 -4.0202513 -3.9740567 -3.9795465 -3.9924669 -3.9882081 -3.9605486 -3.9402442 -3.9611411 -3.9910309 -4.0068178 -4.0354218 -4.096601 -4.1568909 -4.1793628][-4.1002822 -4.0325494 -3.9906726 -4.0126066 -4.0495529 -4.0723586 -4.06372 -4.0440063 -4.0425596 -4.0447717 -4.0398679 -4.0554914 -4.1135874 -4.1740341 -4.1976347][-4.1396837 -4.0811443 -4.0501976 -4.080996 -4.1234226 -4.1518912 -4.1523643 -4.1404428 -4.1358705 -4.1325293 -4.12239 -4.1290479 -4.1694169 -4.2109337 -4.2266445][-4.1737041 -4.1314859 -4.1148152 -4.1478567 -4.185297 -4.2096181 -4.2129655 -4.2079091 -4.2090855 -4.2138977 -4.2118979 -4.2144475 -4.2348485 -4.2549853 -4.2616348][-4.2015715 -4.1756568 -4.1723309 -4.200295 -4.2253103 -4.2394633 -4.2399 -4.2368588 -4.2414193 -4.2529206 -4.2592063 -4.2624846 -4.274044 -4.2831383 -4.2860456][-4.2341061 -4.2212567 -4.2247691 -4.2418971 -4.2520723 -4.2559328 -4.2535481 -4.25089 -4.2539129 -4.2635365 -4.2724886 -4.2789059 -4.28797 -4.2941933 -4.2974]]...]
INFO - root - 2017-12-06 03:49:01.826694: step 14110, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.224 sec/batch; 19h:50m:55s remains)
INFO - root - 2017-12-06 03:49:04.188342: step 14120, loss = 2.07, batch loss = 2.01 (33.4 examples/sec; 0.239 sec/batch; 21h:09m:19s remains)
INFO - root - 2017-12-06 03:49:06.495198: step 14130, loss = 2.05, batch loss = 1.99 (34.7 examples/sec; 0.230 sec/batch; 20h:21m:45s remains)
INFO - root - 2017-12-06 03:49:08.767099: step 14140, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:10m:59s remains)
INFO - root - 2017-12-06 03:49:11.086018: step 14150, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 19h:36m:38s remains)
INFO - root - 2017-12-06 03:49:13.391886: step 14160, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:25m:02s remains)
INFO - root - 2017-12-06 03:49:15.703805: step 14170, loss = 2.08, batch loss = 2.02 (34.3 examples/sec; 0.233 sec/batch; 20h:38m:37s remains)
INFO - root - 2017-12-06 03:49:17.993544: step 14180, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 19h:55m:14s remains)
INFO - root - 2017-12-06 03:49:20.271881: step 14190, loss = 2.08, batch loss = 2.02 (33.6 examples/sec; 0.238 sec/batch; 21h:01m:23s remains)
INFO - root - 2017-12-06 03:49:22.565895: step 14200, loss = 2.07, batch loss = 2.02 (34.4 examples/sec; 0.233 sec/batch; 20h:33m:52s remains)
2017-12-06 03:49:22.879183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3359146 -4.32952 -4.3271632 -4.3280239 -4.3286166 -4.3269744 -4.3247919 -4.3246303 -4.3252196 -4.3259754 -4.3268361 -4.3282757 -4.3297539 -4.3307204 -4.333406][-4.3270397 -4.3172507 -4.3127232 -4.312645 -4.3115325 -4.3094664 -4.3086619 -4.3093042 -4.3103814 -4.3119931 -4.3160043 -4.3197646 -4.3211064 -4.32186 -4.3254476][-4.3111515 -4.2973008 -4.2902255 -4.2874813 -4.2830124 -4.2809868 -4.2808709 -4.2809782 -4.2807608 -4.2796588 -4.2839017 -4.2910342 -4.2963896 -4.3011575 -4.3066578][-4.2856536 -4.2650375 -4.255147 -4.2500939 -4.244597 -4.2422762 -4.2392755 -4.2371464 -4.2351794 -4.23028 -4.2326446 -4.24101 -4.2533755 -4.2657533 -4.2743855][-4.2535172 -4.223052 -4.2116523 -4.2068205 -4.2025318 -4.2001719 -4.192925 -4.1853709 -4.1766605 -4.1653452 -4.1672463 -4.1802096 -4.2018037 -4.2224269 -4.2351828][-4.2251134 -4.1885166 -4.1777177 -4.17366 -4.1666741 -4.1577196 -4.1398335 -4.1182294 -4.0921469 -4.0706296 -4.0799603 -4.1090093 -4.1450496 -4.1741629 -4.1880674][-4.2139883 -4.178257 -4.1695762 -4.1642318 -4.1483979 -4.12253 -4.0852809 -4.0402136 -3.990144 -3.9514318 -3.9660187 -4.0186906 -4.068553 -4.1068959 -4.1271362][-4.2187304 -4.1876121 -4.1794877 -4.1747484 -4.1571608 -4.1204238 -4.0662627 -3.9982746 -3.9233925 -3.8660407 -3.8784871 -3.942924 -4.0000777 -4.0447235 -4.0756373][-4.2296691 -4.2063618 -4.2000222 -4.1998477 -4.1899862 -4.1580968 -4.1054506 -4.0375118 -3.9569826 -3.8922968 -3.8927102 -3.946394 -3.9967368 -4.0375729 -4.0715237][-4.2367449 -4.2199192 -4.2174668 -4.2188087 -4.2175426 -4.2017064 -4.1683092 -4.1175604 -4.0573473 -4.0072556 -3.9977338 -4.0250974 -4.0553384 -4.0828571 -4.1083684][-4.2340579 -4.2203488 -4.2201157 -4.2225776 -4.2266645 -4.2269483 -4.2179213 -4.1915808 -4.1544924 -4.1221838 -4.1108627 -4.1195865 -4.1357388 -4.1491947 -4.1564031][-4.2317638 -4.2164416 -4.2124424 -4.2138677 -4.2224197 -4.2348547 -4.243125 -4.2346091 -4.2160664 -4.1997652 -4.1938524 -4.1984339 -4.2093205 -4.2097297 -4.1985211][-4.23942 -4.2196493 -4.2082381 -4.2072549 -4.2165246 -4.2314949 -4.2469091 -4.2445583 -4.2328315 -4.2263 -4.2290397 -4.2389774 -4.2506614 -4.2438092 -4.2229209][-4.2510271 -4.2250853 -4.20693 -4.2023354 -4.2064691 -4.2171621 -4.2330494 -4.2296934 -4.2174072 -4.2133031 -4.2197089 -4.2323413 -4.2473249 -4.243475 -4.2251172][-4.2638617 -4.2339792 -4.2121134 -4.2047329 -4.2034039 -4.2070336 -4.2171931 -4.2085695 -4.1946898 -4.1903682 -4.196969 -4.21091 -4.2283211 -4.2314577 -4.2175217]]...]
INFO - root - 2017-12-06 03:49:25.177982: step 14210, loss = 2.08, batch loss = 2.02 (34.4 examples/sec; 0.233 sec/batch; 20h:34m:00s remains)
INFO - root - 2017-12-06 03:49:27.487712: step 14220, loss = 2.04, batch loss = 1.98 (34.8 examples/sec; 0.230 sec/batch; 20h:17m:52s remains)
INFO - root - 2017-12-06 03:49:29.804157: step 14230, loss = 2.09, batch loss = 2.03 (34.5 examples/sec; 0.232 sec/batch; 20h:28m:45s remains)
INFO - root - 2017-12-06 03:49:32.118388: step 14240, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.226 sec/batch; 19h:56m:22s remains)
INFO - root - 2017-12-06 03:49:34.386408: step 14250, loss = 2.05, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 19h:48m:08s remains)
INFO - root - 2017-12-06 03:49:36.700877: step 14260, loss = 2.05, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 20h:20m:46s remains)
INFO - root - 2017-12-06 03:49:39.013945: step 14270, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:37m:03s remains)
INFO - root - 2017-12-06 03:49:41.312741: step 14280, loss = 2.04, batch loss = 1.98 (33.3 examples/sec; 0.240 sec/batch; 21h:13m:56s remains)
INFO - root - 2017-12-06 03:49:43.619338: step 14290, loss = 2.09, batch loss = 2.03 (34.0 examples/sec; 0.235 sec/batch; 20h:48m:51s remains)
INFO - root - 2017-12-06 03:49:45.924182: step 14300, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 20h:16m:29s remains)
2017-12-06 03:49:46.237245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2873006 -4.2733469 -4.2778625 -4.2851028 -4.2820287 -4.2746134 -4.275363 -4.28351 -4.2834873 -4.2754908 -4.2636495 -4.2414961 -4.2203941 -4.2076836 -4.2080317][-4.295598 -4.2847261 -4.2867327 -4.2892089 -4.2820969 -4.2693229 -4.2653122 -4.2713332 -4.272522 -4.2630224 -4.249773 -4.2252383 -4.1993427 -4.1840286 -4.1847315][-4.3028121 -4.2946658 -4.2921753 -4.2883492 -4.277411 -4.2553868 -4.2426038 -4.243732 -4.2435727 -4.2347617 -4.223794 -4.2023516 -4.1786904 -4.1705565 -4.1812019][-4.3069439 -4.2995486 -4.2911682 -4.2799439 -4.264214 -4.2341909 -4.2063122 -4.1956873 -4.1935487 -4.1925077 -4.1937771 -4.1901035 -4.1829886 -4.188324 -4.2066531][-4.3093081 -4.3005795 -4.2856331 -4.2654872 -4.2407789 -4.1994529 -4.1454468 -4.1073961 -4.1037459 -4.1262755 -4.1601539 -4.1859722 -4.2035465 -4.2226839 -4.2416115][-4.3112178 -4.3027959 -4.28367 -4.2554078 -4.2176013 -4.1546574 -4.0591989 -3.9784479 -3.9799676 -4.0444021 -4.1211228 -4.1765952 -4.2152419 -4.2475109 -4.2717104][-4.3120179 -4.3052 -4.2862706 -4.2541685 -4.2050643 -4.119606 -3.9774804 -3.8438654 -3.8596063 -3.9802423 -4.0947304 -4.1682453 -4.2191315 -4.2628164 -4.2936077][-4.3112235 -4.3058004 -4.2894821 -4.2609749 -4.2136025 -4.1265397 -3.980267 -3.8342829 -3.8512046 -3.9841886 -4.1043425 -4.1761732 -4.2289762 -4.2741609 -4.3048816][-4.3072209 -4.3015766 -4.2881088 -4.2657375 -4.2310925 -4.1694775 -4.0683856 -3.9646642 -3.9677994 -4.0602469 -4.1480546 -4.20322 -4.2461309 -4.2837853 -4.30816][-4.3033433 -4.2941542 -4.2798796 -4.26056 -4.2341933 -4.1952243 -4.1383786 -4.078505 -4.0763521 -4.1307597 -4.1871867 -4.2279787 -4.2629623 -4.2950711 -4.3138542][-4.3023376 -4.289011 -4.2711177 -4.2503605 -4.2256141 -4.1964321 -4.1632686 -4.1298614 -4.1318421 -4.1727686 -4.2153845 -4.2468452 -4.2772117 -4.3063464 -4.3190231][-4.3050137 -4.2898335 -4.2689066 -4.2447014 -4.2157121 -4.1849203 -4.1552491 -4.1347222 -4.1463094 -4.1884 -4.2314944 -4.2606664 -4.2877746 -4.3107028 -4.3142323][-4.309844 -4.2972708 -4.277307 -4.2502241 -4.21562 -4.1783133 -4.1417794 -4.1205678 -4.1368604 -4.1831422 -4.2281713 -4.2570057 -4.2829576 -4.3001165 -4.2991319][-4.3144436 -4.3071775 -4.2901754 -4.2615986 -4.2243557 -4.1846189 -4.1442275 -4.1208062 -4.1355557 -4.1814713 -4.2243137 -4.2478404 -4.2691007 -4.2820692 -4.2842383][-4.3143687 -4.3080416 -4.29243 -4.2637758 -4.2285457 -4.1896548 -4.1510091 -4.1313472 -4.1471 -4.190424 -4.2274451 -4.2464876 -4.2630682 -4.2717986 -4.2756958]]...]
INFO - root - 2017-12-06 03:49:48.533205: step 14310, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:09m:40s remains)
INFO - root - 2017-12-06 03:49:50.840898: step 14320, loss = 2.05, batch loss = 2.00 (36.6 examples/sec; 0.218 sec/batch; 19h:18m:12s remains)
INFO - root - 2017-12-06 03:49:53.152857: step 14330, loss = 2.06, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 20h:30m:49s remains)
INFO - root - 2017-12-06 03:49:55.444513: step 14340, loss = 2.09, batch loss = 2.03 (33.9 examples/sec; 0.236 sec/batch; 20h:50m:32s remains)
INFO - root - 2017-12-06 03:49:57.725100: step 14350, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 19h:36m:27s remains)
INFO - root - 2017-12-06 03:50:00.036183: step 14360, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:38m:34s remains)
INFO - root - 2017-12-06 03:50:02.341394: step 14370, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 19h:52m:25s remains)
INFO - root - 2017-12-06 03:50:04.647662: step 14380, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.236 sec/batch; 20h:53m:34s remains)
INFO - root - 2017-12-06 03:50:06.896895: step 14390, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 19h:24m:54s remains)
INFO - root - 2017-12-06 03:50:09.200576: step 14400, loss = 2.08, batch loss = 2.02 (33.8 examples/sec; 0.237 sec/batch; 20h:55m:13s remains)
2017-12-06 03:50:09.493507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1705084 -4.1655746 -4.1736646 -4.1782608 -4.1893029 -4.2096777 -4.2172027 -4.2018466 -4.1922669 -4.2080555 -4.2400804 -4.2710042 -4.2918391 -4.3030667 -4.3031778][-4.1547565 -4.14863 -4.1539264 -4.1595268 -4.16877 -4.188705 -4.1949825 -4.1767073 -4.1640153 -4.1789227 -4.2101927 -4.2425451 -4.2673411 -4.2855787 -4.2929864][-4.1332021 -4.12077 -4.1219831 -4.127111 -4.1361885 -4.1543226 -4.1563172 -4.1350379 -4.1247907 -4.1451473 -4.1774483 -4.2106919 -4.2374158 -4.2600617 -4.2744827][-4.1290627 -4.1099453 -4.1069574 -4.1049061 -4.1069713 -4.1161108 -4.1061387 -4.0735683 -4.067441 -4.1021461 -4.1457028 -4.1848598 -4.2118354 -4.23431 -4.2508645][-4.136734 -4.1173644 -4.1103387 -4.1008687 -4.0911665 -4.0804291 -4.0470238 -3.9924014 -3.9877656 -4.0485229 -4.1166639 -4.1642241 -4.1886225 -4.2077851 -4.22434][-4.1461945 -4.1296563 -4.1207404 -4.1049275 -4.083952 -4.0550065 -3.9873278 -3.8907487 -3.8805842 -3.9795256 -4.08038 -4.1421976 -4.1669283 -4.1819558 -4.1970844][-4.1623554 -4.1496291 -4.1403971 -4.1208863 -4.09494 -4.0545006 -3.9534197 -3.799798 -3.7735023 -3.9103191 -4.0361457 -4.1089892 -4.1366978 -4.14871 -4.1599283][-4.1801205 -4.1750464 -4.1715255 -4.1574359 -4.1359396 -4.0943327 -3.9839878 -3.8128572 -3.7764359 -3.9064345 -4.0245895 -4.0891056 -4.1098228 -4.1130905 -4.1161842][-4.1958456 -4.1982985 -4.2024364 -4.1970406 -4.1800365 -4.1411295 -4.0477061 -3.9150028 -3.89189 -3.9817872 -4.0647 -4.1066241 -4.1124792 -4.1021628 -4.0946112][-4.2067857 -4.2102695 -4.2158685 -4.2173748 -4.2059555 -4.1734409 -4.1028819 -4.0161214 -4.0061927 -4.0642519 -4.1199207 -4.1440558 -4.1373358 -4.112978 -4.0967336][-4.2046571 -4.2066092 -4.2145581 -4.2228332 -4.2161245 -4.1877384 -4.1373138 -4.0818481 -4.0776229 -4.1146622 -4.1542454 -4.1718588 -4.1577287 -4.1231837 -4.0994587][-4.1939435 -4.1969004 -4.2077594 -4.2173023 -4.2117715 -4.1863379 -4.1503711 -4.1168013 -4.1156073 -4.146184 -4.1815629 -4.2004447 -4.1768308 -4.1301551 -4.0983644][-4.1877189 -4.1935768 -4.2045741 -4.2157588 -4.2106318 -4.1852155 -4.155046 -4.1382303 -4.1455469 -4.1761036 -4.2112379 -4.2271867 -4.1947794 -4.1389613 -4.1006994][-4.1851635 -4.1913934 -4.1985307 -4.2076077 -4.2019095 -4.1774817 -4.1558151 -4.1535115 -4.1697807 -4.1993222 -4.2312527 -4.2423682 -4.2106905 -4.1546063 -4.1135054][-4.1882763 -4.1895909 -4.1918983 -4.1959777 -4.1843381 -4.1646581 -4.1559758 -4.1635404 -4.1815619 -4.2057586 -4.2313 -4.2400622 -4.2173605 -4.1732593 -4.1383185]]...]
INFO - root - 2017-12-06 03:50:11.792493: step 14410, loss = 2.05, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:39m:56s remains)
INFO - root - 2017-12-06 03:50:14.076914: step 14420, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:07m:28s remains)
INFO - root - 2017-12-06 03:50:16.358516: step 14430, loss = 2.05, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:37m:03s remains)
INFO - root - 2017-12-06 03:50:18.629620: step 14440, loss = 2.05, batch loss = 2.00 (36.6 examples/sec; 0.218 sec/batch; 19h:18m:07s remains)
INFO - root - 2017-12-06 03:50:20.933427: step 14450, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:42m:42s remains)
INFO - root - 2017-12-06 03:50:23.235750: step 14460, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 20h:03m:42s remains)
INFO - root - 2017-12-06 03:50:25.550509: step 14470, loss = 2.08, batch loss = 2.02 (33.9 examples/sec; 0.236 sec/batch; 20h:49m:50s remains)
INFO - root - 2017-12-06 03:50:27.838295: step 14480, loss = 2.04, batch loss = 1.98 (36.3 examples/sec; 0.220 sec/batch; 19h:28m:09s remains)
INFO - root - 2017-12-06 03:50:30.199585: step 14490, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:06m:35s remains)
INFO - root - 2017-12-06 03:50:32.505471: step 14500, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:59m:22s remains)
2017-12-06 03:50:32.781604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2071548 -4.2086535 -4.189014 -4.148241 -4.1029897 -4.0661426 -4.0692825 -4.1009774 -4.1339722 -4.1534691 -4.1485529 -4.1582332 -4.176815 -4.1659842 -4.1277971][-4.2366629 -4.228756 -4.2061672 -4.1656241 -4.1161852 -4.0666065 -4.0524411 -4.07532 -4.1054549 -4.1214542 -4.1145425 -4.1267939 -4.1555452 -4.1533055 -4.1222782][-4.249073 -4.2323079 -4.2106228 -4.176187 -4.1281242 -4.0705433 -4.0351148 -4.0424805 -4.0699229 -4.0859818 -4.080626 -4.0983672 -4.1393108 -4.1487408 -4.1264915][-4.2485704 -4.2275438 -4.2067075 -4.1791749 -4.1379738 -4.0756068 -4.0187531 -4.0091453 -4.0364208 -4.0584254 -4.0584245 -4.0796151 -4.1287107 -4.14872 -4.135047][-4.2458515 -4.223711 -4.2030363 -4.1808147 -4.1486039 -4.0787425 -4.0018883 -3.9771614 -4.0113111 -4.0426168 -4.0452585 -4.063746 -4.1157846 -4.1471481 -4.1465545][-4.2360644 -4.21784 -4.1969938 -4.1752324 -4.1499496 -4.0739865 -3.977747 -3.9424748 -3.994868 -4.0413523 -4.0420065 -4.0473661 -4.0941396 -4.1338615 -4.144969][-4.2181158 -4.2028632 -4.1800523 -4.1573081 -4.1357012 -4.0580578 -3.9470246 -3.9040551 -3.9833241 -4.0543933 -4.0493693 -4.0322313 -4.0631833 -4.1058216 -4.1211214][-4.1985254 -4.1871433 -4.1646137 -4.1406269 -4.1233683 -4.0510759 -3.9283819 -3.8725114 -3.9668021 -4.0584559 -4.0557361 -4.0210953 -4.0336843 -4.0750732 -4.0917234][-4.1916542 -4.1878223 -4.171299 -4.148263 -4.13218 -4.0681968 -3.9414983 -3.8688955 -3.9561975 -4.0507789 -4.0549607 -4.0158687 -4.0170116 -4.0528836 -4.0694523][-4.1943631 -4.1961145 -4.1869445 -4.1681156 -4.1525283 -4.1017909 -3.9864128 -3.9006586 -3.9616871 -4.0433187 -4.05096 -4.0180845 -4.0194335 -4.0504956 -4.0685964][-4.2023396 -4.2024255 -4.1956472 -4.181273 -4.1667552 -4.1289139 -4.0344987 -3.9500413 -3.98184 -4.0435309 -4.0489187 -4.0270929 -4.0386529 -4.0678625 -4.0849919][-4.2133479 -4.2060375 -4.1949162 -4.1830115 -4.1705847 -4.1402864 -4.0672646 -3.9958489 -4.0092816 -4.0538521 -4.0557051 -4.0439105 -4.0626307 -4.0865364 -4.1034203][-4.2304683 -4.2126288 -4.1920919 -4.1790385 -4.1706648 -4.1469269 -4.0923128 -4.0384078 -4.041851 -4.0707951 -4.0687041 -4.0656404 -4.0880475 -4.1055126 -4.1204462][-4.2507553 -4.2245126 -4.1931515 -4.1746531 -4.1693573 -4.1577663 -4.122468 -4.0847869 -4.08304 -4.0972776 -4.08893 -4.086359 -4.1081753 -4.1231441 -4.1377239][-4.2695961 -4.242044 -4.2051978 -4.178833 -4.1700835 -4.1672745 -4.149435 -4.1253128 -4.1200018 -4.1208291 -4.1060143 -4.0949163 -4.1114984 -4.1300917 -4.1488452]]...]
INFO - root - 2017-12-06 03:50:35.091127: step 14510, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 19h:53m:42s remains)
INFO - root - 2017-12-06 03:50:37.407073: step 14520, loss = 2.06, batch loss = 2.00 (33.0 examples/sec; 0.242 sec/batch; 21h:24m:22s remains)
INFO - root - 2017-12-06 03:50:39.693196: step 14530, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 19h:58m:08s remains)
INFO - root - 2017-12-06 03:50:42.003641: step 14540, loss = 2.03, batch loss = 1.97 (33.2 examples/sec; 0.241 sec/batch; 21h:18m:17s remains)
INFO - root - 2017-12-06 03:50:44.317500: step 14550, loss = 2.05, batch loss = 1.99 (35.3 examples/sec; 0.226 sec/batch; 20h:00m:08s remains)
INFO - root - 2017-12-06 03:50:46.590125: step 14560, loss = 2.07, batch loss = 2.02 (33.6 examples/sec; 0.238 sec/batch; 21h:00m:07s remains)
INFO - root - 2017-12-06 03:50:48.876102: step 14570, loss = 2.06, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 19h:48m:13s remains)
INFO - root - 2017-12-06 03:50:51.169345: step 14580, loss = 2.09, batch loss = 2.03 (36.2 examples/sec; 0.221 sec/batch; 19h:30m:08s remains)
INFO - root - 2017-12-06 03:50:53.464699: step 14590, loss = 2.06, batch loss = 2.00 (33.7 examples/sec; 0.238 sec/batch; 20h:59m:24s remains)
INFO - root - 2017-12-06 03:50:55.824565: step 14600, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.228 sec/batch; 20h:05m:26s remains)
2017-12-06 03:50:56.131499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1861768 -4.2154188 -4.2214422 -4.1892595 -4.1373396 -4.0841579 -4.0438919 -4.051271 -4.101017 -4.1471043 -4.1689639 -4.1745839 -4.1734452 -4.1781888 -4.2000833][-4.2221131 -4.2452393 -4.2447815 -4.2133045 -4.1646566 -4.114778 -4.0689139 -4.0534792 -4.0802426 -4.1165476 -4.1318893 -4.1359258 -4.1454892 -4.1644559 -4.197145][-4.2539639 -4.2668519 -4.2557182 -4.220212 -4.1697354 -4.124362 -4.0822592 -4.0596023 -4.0732079 -4.1003256 -4.1091204 -4.1124945 -4.1314545 -4.1596012 -4.1933084][-4.2728949 -4.2788868 -4.2543287 -4.2064 -4.1461954 -4.0975056 -4.0659204 -4.0575871 -4.0808253 -4.1082807 -4.114603 -4.1178951 -4.1396451 -4.167613 -4.1915545][-4.2873039 -4.2837033 -4.2449956 -4.1774592 -4.101862 -4.0466413 -4.0226779 -4.0432858 -4.0964427 -4.1361446 -4.1468921 -4.1558614 -4.17604 -4.1983986 -4.2126656][-4.2835088 -4.2730694 -4.223577 -4.1397395 -4.0458164 -3.9751196 -3.9446137 -3.9906559 -4.0884132 -4.1543126 -4.1799741 -4.2020683 -4.2244697 -4.2389688 -4.2494574][-4.2772603 -4.2619562 -4.1991224 -4.0938368 -3.9757948 -3.8730707 -3.8128309 -3.8743834 -4.0192428 -4.1196051 -4.1734891 -4.2209511 -4.2560658 -4.272191 -4.2845092][-4.2661462 -4.2491803 -4.1864958 -4.0788169 -3.9535506 -3.827764 -3.7337878 -3.7900748 -3.9569912 -4.0775819 -4.15412 -4.2220068 -4.2706146 -4.2951274 -4.3114963][-4.2408848 -4.2384124 -4.2008548 -4.1242228 -4.0267353 -3.9204304 -3.8398073 -3.8782353 -4.0045137 -4.1003995 -4.1704612 -4.2365103 -4.2839928 -4.3096328 -4.3247242][-4.1977005 -4.2226453 -4.2194304 -4.1836815 -4.1288443 -4.0629926 -4.0123138 -4.0324397 -4.1042447 -4.1651239 -4.2127256 -4.2609186 -4.2965121 -4.3149619 -4.3236904][-4.150341 -4.2058792 -4.2354059 -4.2349405 -4.2167869 -4.1855474 -4.1594577 -4.1666617 -4.2024736 -4.2381239 -4.2648125 -4.2904453 -4.3101149 -4.3206663 -4.3235006][-4.1252327 -4.2037535 -4.2519917 -4.2741361 -4.2795215 -4.2683268 -4.2542381 -4.2547784 -4.2737489 -4.2949557 -4.3057423 -4.3162565 -4.3231673 -4.3265409 -4.3252983][-4.1357937 -4.2186933 -4.2700353 -4.3000808 -4.3178539 -4.3187046 -4.3106732 -4.3076682 -4.3164754 -4.3283906 -4.3315759 -4.3340917 -4.3343563 -4.3330641 -4.3289871][-4.1826911 -4.2500386 -4.2919536 -4.3169541 -4.3334417 -4.3364625 -4.3314643 -4.32826 -4.3318748 -4.3391037 -4.3412147 -4.3409348 -4.3386145 -4.3361006 -4.33086][-4.2388158 -4.2824397 -4.3063993 -4.3188944 -4.3260937 -4.3263178 -4.3230996 -4.32147 -4.3239603 -4.32867 -4.3306675 -4.3305545 -4.328846 -4.326726 -4.3222489]]...]
INFO - root - 2017-12-06 03:50:58.429469: step 14610, loss = 2.05, batch loss = 1.99 (34.1 examples/sec; 0.235 sec/batch; 20h:42m:37s remains)
INFO - root - 2017-12-06 03:51:00.780504: step 14620, loss = 2.05, batch loss = 1.99 (34.5 examples/sec; 0.232 sec/batch; 20h:30m:14s remains)
INFO - root - 2017-12-06 03:51:03.058129: step 14630, loss = 2.06, batch loss = 2.00 (33.6 examples/sec; 0.238 sec/batch; 21h:02m:01s remains)
INFO - root - 2017-12-06 03:51:05.355751: step 14640, loss = 2.08, batch loss = 2.02 (34.0 examples/sec; 0.235 sec/batch; 20h:46m:35s remains)
INFO - root - 2017-12-06 03:51:07.656812: step 14650, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.231 sec/batch; 20h:22m:34s remains)
INFO - root - 2017-12-06 03:51:09.992658: step 14660, loss = 2.09, batch loss = 2.03 (34.2 examples/sec; 0.234 sec/batch; 20h:40m:26s remains)
INFO - root - 2017-12-06 03:51:12.302357: step 14670, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:48m:22s remains)
INFO - root - 2017-12-06 03:51:14.631276: step 14680, loss = 2.06, batch loss = 2.00 (34.1 examples/sec; 0.234 sec/batch; 20h:41m:31s remains)
INFO - root - 2017-12-06 03:51:16.908205: step 14690, loss = 2.05, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 18h:44m:07s remains)
INFO - root - 2017-12-06 03:51:19.206038: step 14700, loss = 2.04, batch loss = 1.98 (32.4 examples/sec; 0.247 sec/batch; 21h:48m:12s remains)
2017-12-06 03:51:19.494869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3101168 -4.3121057 -4.30485 -4.30149 -4.2958245 -4.2961121 -4.2971 -4.29566 -4.2907138 -4.28387 -4.2837825 -4.2860589 -4.2866516 -4.2887039 -4.2934561][-4.3054681 -4.3028345 -4.2983508 -4.3008881 -4.2979445 -4.296948 -4.2989626 -4.2976131 -4.2910128 -4.2811317 -4.2793121 -4.2813482 -4.2838397 -4.2881 -4.2916584][-4.2844019 -4.2781229 -4.2752619 -4.2815127 -4.2838969 -4.2833891 -4.282568 -4.2773395 -4.2674637 -4.2573981 -4.2558737 -4.257926 -4.2604403 -4.2637248 -4.2651944][-4.2449007 -4.2369595 -4.2331853 -4.2404857 -4.2480183 -4.2470722 -4.2419152 -4.2324791 -4.2207904 -4.2101169 -4.2075605 -4.2065568 -4.2066646 -4.2074242 -4.2066035][-4.1956472 -4.181088 -4.1707931 -4.1720872 -4.1804905 -4.182529 -4.1774631 -4.1674519 -4.1545067 -4.1420331 -4.1364136 -4.130662 -4.1253295 -4.1234403 -4.1228337][-4.1309671 -4.1027846 -4.0809464 -4.0751514 -4.0820289 -4.0851765 -4.0811548 -4.0730228 -4.0617733 -4.05064 -4.0419269 -4.0295467 -4.0185256 -4.0147314 -4.0140963][-4.0778856 -4.0405722 -4.0094571 -3.996068 -3.9979815 -3.9980228 -3.9930363 -3.9879179 -3.9824276 -3.9793272 -3.973918 -3.9615471 -3.9507079 -3.9468431 -3.9459536][-4.0954418 -4.06099 -4.0298057 -4.0111952 -4.0049706 -3.9942317 -3.9842262 -3.9838967 -3.9924688 -4.0052881 -4.012135 -4.0078492 -4.0015287 -3.9978089 -3.9960203][-4.1560988 -4.1330528 -4.1073346 -4.08756 -4.0730023 -4.0508413 -4.0342093 -4.0348148 -4.0529361 -4.0791225 -4.1009893 -4.1106229 -4.1124554 -4.1103239 -4.1069827][-4.1938505 -4.1783552 -4.1584797 -4.1426988 -4.1277723 -4.1040759 -4.0857043 -4.083766 -4.100482 -4.1280532 -4.1556396 -4.1721263 -4.17833 -4.1778493 -4.1740909][-4.204617 -4.1911144 -4.1732368 -4.161623 -4.15412 -4.1389875 -4.1266294 -4.124465 -4.1344337 -4.1549244 -4.1777382 -4.1918654 -4.1963058 -4.1958108 -4.1931334][-4.21083 -4.1968632 -4.1761613 -4.1657176 -4.1646776 -4.1584806 -4.1542807 -4.1543632 -4.1576581 -4.1676221 -4.1805015 -4.1879969 -4.1882753 -4.1846046 -4.1811843][-4.2229466 -4.2100916 -4.1848068 -4.1710286 -4.1722431 -4.169816 -4.1688075 -4.1675954 -4.1646976 -4.1640511 -4.1655216 -4.1678977 -4.1677694 -4.1658888 -4.1646051][-4.23651 -4.2253966 -4.1994071 -4.1844492 -4.1864271 -4.1817813 -4.1771393 -4.1722145 -4.1653514 -4.1592603 -4.1540942 -4.1537905 -4.1556277 -4.156599 -4.1569963][-4.2503567 -4.2377868 -4.2128477 -4.1994362 -4.2019615 -4.194119 -4.1863046 -4.1811075 -4.1758804 -4.17021 -4.1636505 -4.161603 -4.1648269 -4.1686249 -4.1717663]]...]
INFO - root - 2017-12-06 03:51:21.793750: step 14710, loss = 2.09, batch loss = 2.03 (35.6 examples/sec; 0.225 sec/batch; 19h:51m:12s remains)
INFO - root - 2017-12-06 03:51:24.109016: step 14720, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.230 sec/batch; 20h:20m:18s remains)
INFO - root - 2017-12-06 03:51:26.442555: step 14730, loss = 2.09, batch loss = 2.03 (34.0 examples/sec; 0.236 sec/batch; 20h:47m:56s remains)
INFO - root - 2017-12-06 03:51:28.776670: step 14740, loss = 2.04, batch loss = 1.98 (35.7 examples/sec; 0.224 sec/batch; 19h:45m:39s remains)
INFO - root - 2017-12-06 03:51:31.041372: step 14750, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:57m:24s remains)
INFO - root - 2017-12-06 03:51:33.314236: step 14760, loss = 2.05, batch loss = 1.99 (34.0 examples/sec; 0.235 sec/batch; 20h:45m:05s remains)
INFO - root - 2017-12-06 03:51:35.630345: step 14770, loss = 2.04, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 20h:06m:07s remains)
INFO - root - 2017-12-06 03:51:37.911784: step 14780, loss = 2.05, batch loss = 1.99 (32.6 examples/sec; 0.245 sec/batch; 21h:37m:31s remains)
INFO - root - 2017-12-06 03:51:40.183993: step 14790, loss = 2.08, batch loss = 2.02 (34.7 examples/sec; 0.231 sec/batch; 20h:21m:03s remains)
INFO - root - 2017-12-06 03:51:42.515068: step 14800, loss = 2.07, batch loss = 2.01 (32.4 examples/sec; 0.247 sec/batch; 21h:48m:19s remains)
2017-12-06 03:51:42.794076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1977987 -4.200603 -4.1963868 -4.1825838 -4.1621661 -4.1498055 -4.1454859 -4.159718 -4.1953206 -4.2371688 -4.2756844 -4.2953644 -4.311203 -4.3262954 -4.3333845][-4.1438622 -4.1508207 -4.1423106 -4.1176224 -4.0892267 -4.0719714 -4.0695248 -4.0944657 -4.1430917 -4.1976233 -4.2489629 -4.276999 -4.2993574 -4.3174286 -4.3259931][-4.1001863 -4.1091852 -4.0966311 -4.0617905 -4.0246463 -4.00071 -3.9984012 -4.0312715 -4.0904355 -4.1559224 -4.2185912 -4.2561622 -4.2831917 -4.301 -4.3045387][-4.0844889 -4.0929666 -4.0719867 -4.0206208 -3.9745505 -3.9460526 -3.9465637 -3.980679 -4.0390282 -4.1098375 -4.1822376 -4.2274137 -4.2570624 -4.2736034 -4.2688713][-4.1075807 -4.106626 -4.0733495 -4.004415 -3.9398339 -3.9016857 -3.9026499 -3.938561 -3.9939778 -4.0649219 -4.14109 -4.1909151 -4.221539 -4.2356272 -4.2266994][-4.1436505 -4.125114 -4.0794263 -4.0001364 -3.9189093 -3.8666396 -3.8616216 -3.890084 -3.9365113 -4.0039616 -4.0788503 -4.1328907 -4.1700497 -4.1898432 -4.1865091][-4.1529284 -4.1155295 -4.06635 -3.9948511 -3.9218755 -3.8706765 -3.8624587 -3.8836482 -3.913765 -3.9642797 -4.0249166 -4.0777755 -4.1239166 -4.152247 -4.1547627][-4.1417422 -4.0954642 -4.0546417 -4.0083585 -3.9620774 -3.9326596 -3.9337227 -3.9466929 -3.9548402 -3.9790423 -4.0210085 -4.0691619 -4.1142206 -4.1420159 -4.1492596][-4.1235819 -4.0758991 -4.0453544 -4.0278306 -4.0150938 -4.0130234 -4.02444 -4.0315733 -4.0263667 -4.0329328 -4.0603647 -4.1012926 -4.1451631 -4.17335 -4.1868553][-4.1212039 -4.07693 -4.0569048 -4.0633349 -4.0781841 -4.0958543 -4.114326 -4.1176519 -4.1087608 -4.1084528 -4.1204834 -4.1471009 -4.1841745 -4.2114325 -4.2300606][-4.1438246 -4.1119351 -4.1026907 -4.1221004 -4.147666 -4.173255 -4.1935472 -4.1969419 -4.1910729 -4.18799 -4.1889305 -4.2000237 -4.2202797 -4.2333279 -4.2442131][-4.1785612 -4.1649504 -4.1692791 -4.195713 -4.225472 -4.2528133 -4.2709665 -4.2735639 -4.2687082 -4.2636905 -4.2592816 -4.2589345 -4.25916 -4.2534871 -4.2517004][-4.2165265 -4.2170153 -4.2316647 -4.26222 -4.2912993 -4.314147 -4.3266907 -4.3262467 -4.318305 -4.3110209 -4.3053288 -4.2998815 -4.2872171 -4.2666883 -4.249578][-4.2589335 -4.2655816 -4.2819653 -4.3087521 -4.3330951 -4.3480706 -4.3526254 -4.3463006 -4.3342795 -4.3224297 -4.3126712 -4.3039575 -4.2896891 -4.2657852 -4.2441463][-4.2975907 -4.3058944 -4.3180871 -4.33538 -4.348103 -4.3511519 -4.3459377 -4.3352108 -4.32041 -4.3050046 -4.2933159 -4.284698 -4.2732716 -4.25317 -4.2325149]]...]
INFO - root - 2017-12-06 03:51:45.108276: step 14810, loss = 2.06, batch loss = 2.01 (34.7 examples/sec; 0.231 sec/batch; 20h:20m:28s remains)
INFO - root - 2017-12-06 03:51:47.411263: step 14820, loss = 2.04, batch loss = 1.98 (34.3 examples/sec; 0.233 sec/batch; 20h:35m:04s remains)
INFO - root - 2017-12-06 03:51:49.686944: step 14830, loss = 2.07, batch loss = 2.02 (34.4 examples/sec; 0.232 sec/batch; 20h:30m:03s remains)
INFO - root - 2017-12-06 03:51:52.026107: step 14840, loss = 2.04, batch loss = 1.98 (32.9 examples/sec; 0.243 sec/batch; 21h:26m:15s remains)
INFO - root - 2017-12-06 03:51:54.379846: step 14850, loss = 2.05, batch loss = 2.00 (33.4 examples/sec; 0.240 sec/batch; 21h:09m:23s remains)
INFO - root - 2017-12-06 03:51:56.681712: step 14860, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 20h:14m:44s remains)
INFO - root - 2017-12-06 03:51:59.006967: step 14870, loss = 2.06, batch loss = 2.00 (34.1 examples/sec; 0.235 sec/batch; 20h:41m:41s remains)
INFO - root - 2017-12-06 03:52:01.304057: step 14880, loss = 2.03, batch loss = 1.97 (34.8 examples/sec; 0.230 sec/batch; 20h:16m:32s remains)
INFO - root - 2017-12-06 03:52:03.575095: step 14890, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 19h:23m:18s remains)
INFO - root - 2017-12-06 03:52:05.868282: step 14900, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.232 sec/batch; 20h:29m:52s remains)
2017-12-06 03:52:06.168716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3430867 -4.3447642 -4.3446145 -4.3452659 -4.3459177 -4.3464103 -4.3469033 -4.3472419 -4.3474483 -4.34705 -4.3467107 -4.3466568 -4.346715 -4.3468843 -4.3472562][-4.3378696 -4.3390775 -4.3378949 -4.3394547 -4.3416939 -4.3429379 -4.3439093 -4.3443632 -4.3447046 -4.3444481 -4.3447819 -4.3458772 -4.3463688 -4.3458757 -4.3450069][-4.3297429 -4.3304296 -4.326962 -4.3291321 -4.3331742 -4.3354855 -4.3374381 -4.337945 -4.3383293 -4.3398032 -4.342617 -4.3456597 -4.3458371 -4.3425651 -4.3387432][-4.3121891 -4.311646 -4.3032842 -4.3037548 -4.3089318 -4.3128777 -4.3174653 -4.3178329 -4.3178358 -4.3226185 -4.3296909 -4.3367391 -4.335825 -4.3263125 -4.3180394][-4.2816753 -4.279592 -4.2673316 -4.2668839 -4.2734523 -4.2800193 -4.2881169 -4.2876673 -4.2890358 -4.29814 -4.3089042 -4.3169751 -4.3114176 -4.2907038 -4.2755833][-4.2447014 -4.240952 -4.2274914 -4.22892 -4.23748 -4.2472892 -4.2565274 -4.2520504 -4.2546277 -4.2695456 -4.284481 -4.2925291 -4.2790279 -4.2444382 -4.2216506][-4.2045674 -4.1966314 -4.1815333 -4.1879649 -4.2046409 -4.2200875 -4.2291517 -4.2212372 -4.2219658 -4.2420158 -4.2613297 -4.2684646 -4.2490163 -4.2045007 -4.175848][-4.1727157 -4.1592641 -4.1435871 -4.1560616 -4.1814947 -4.1994243 -4.2058978 -4.1956692 -4.19629 -4.2196894 -4.2404957 -4.2453518 -4.2229123 -4.1765113 -4.1488943][-4.1631074 -4.1470585 -4.1296496 -4.1444697 -4.175117 -4.1934257 -4.1966515 -4.1855154 -4.1866846 -4.2080603 -4.2254076 -4.2264237 -4.204041 -4.1642985 -4.1432076][-4.1666851 -4.1502705 -4.1319919 -4.1444268 -4.1759911 -4.1941667 -4.1950841 -4.1835237 -4.1825442 -4.1988196 -4.2096691 -4.2075243 -4.1914682 -4.1670446 -4.15546][-4.189539 -4.1695838 -4.1465082 -4.156405 -4.1897707 -4.2087183 -4.2087321 -4.1986542 -4.1947231 -4.2026725 -4.20593 -4.1995273 -4.1883163 -4.1751151 -4.1700163][-4.2172856 -4.1910038 -4.1584144 -4.162457 -4.19908 -4.2223415 -4.2277122 -4.22232 -4.2167797 -4.21676 -4.2118316 -4.2019033 -4.1931787 -4.1868329 -4.1871405][-4.2231359 -4.1967087 -4.1571708 -4.1542258 -4.1917734 -4.2214966 -4.2347274 -4.23614 -4.2323146 -4.226522 -4.2164621 -4.2061944 -4.2019925 -4.2047095 -4.2112226][-4.2236285 -4.2010117 -4.1613908 -4.1539631 -4.1923137 -4.2266693 -4.2469797 -4.2550292 -4.2538915 -4.2447672 -4.2322278 -4.2226 -4.2235689 -4.2351 -4.2470722][-4.241745 -4.2221522 -4.187005 -4.17916 -4.2157164 -4.2512937 -4.2748723 -4.2854462 -4.2831388 -4.271172 -4.2559476 -4.2464123 -4.2509151 -4.2674761 -4.2823944]]...]
INFO - root - 2017-12-06 03:52:08.467560: step 14910, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 20h:49m:27s remains)
INFO - root - 2017-12-06 03:52:10.774953: step 14920, loss = 2.04, batch loss = 1.98 (35.7 examples/sec; 0.224 sec/batch; 19h:44m:35s remains)
INFO - root - 2017-12-06 03:52:13.100196: step 14930, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:05m:17s remains)
INFO - root - 2017-12-06 03:52:15.430704: step 14940, loss = 2.05, batch loss = 1.99 (34.0 examples/sec; 0.236 sec/batch; 20h:46m:53s remains)
INFO - root - 2017-12-06 03:52:17.742905: step 14950, loss = 2.05, batch loss = 1.99 (33.7 examples/sec; 0.237 sec/batch; 20h:55m:54s remains)
INFO - root - 2017-12-06 03:52:20.070342: step 14960, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:26m:54s remains)
INFO - root - 2017-12-06 03:52:22.419688: step 14970, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:07m:34s remains)
INFO - root - 2017-12-06 03:52:24.727477: step 14980, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 20h:16m:48s remains)
INFO - root - 2017-12-06 03:52:27.025730: step 14990, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:40m:18s remains)
INFO - root - 2017-12-06 03:52:29.313060: step 15000, loss = 2.09, batch loss = 2.03 (35.0 examples/sec; 0.228 sec/batch; 20h:08m:30s remains)
2017-12-06 03:52:29.626499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3210764 -4.31113 -4.2986393 -4.2865543 -4.2720761 -4.2543306 -4.2462044 -4.2541456 -4.2675142 -4.2721395 -4.2765517 -4.2885203 -4.3032913 -4.3174162 -4.3292279][-4.3168769 -4.3042669 -4.2910151 -4.2797465 -4.2616377 -4.2337484 -4.2205329 -4.2289271 -4.2443252 -4.2485514 -4.2543716 -4.2705779 -4.2877131 -4.3052464 -4.3229785][-4.3089118 -4.2957792 -4.286046 -4.2772088 -4.2565508 -4.22003 -4.1987748 -4.2031136 -4.2207489 -4.2282557 -4.2383094 -4.2580767 -4.2780232 -4.2967153 -4.3153696][-4.2960567 -4.285284 -4.2798061 -4.2723413 -4.2521591 -4.2123179 -4.1832266 -4.1831508 -4.2064357 -4.2215691 -4.2334766 -4.2544522 -4.2764077 -4.2947841 -4.3098717][-4.2753468 -4.2639055 -4.2559929 -4.2453685 -4.2258949 -4.1843543 -4.146224 -4.1406112 -4.1763616 -4.2059917 -4.2263055 -4.2502952 -4.2724934 -4.2926254 -4.3047905][-4.2447104 -4.2257605 -4.2069216 -4.1894708 -4.1638727 -4.1157422 -4.0569062 -4.0397558 -4.0974903 -4.1533556 -4.1915765 -4.22769 -4.2578397 -4.2837281 -4.2966905][-4.1818275 -4.1471715 -4.1166954 -4.090106 -4.0523739 -3.9905539 -3.9143786 -3.8795292 -3.9595554 -4.0510149 -4.12037 -4.1768212 -4.2202439 -4.256711 -4.2766538][-4.0959973 -4.0408907 -3.99504 -3.9508173 -3.8956714 -3.8263433 -3.7487147 -3.7111068 -3.8101249 -3.9298387 -4.0294919 -4.1092467 -4.1665277 -4.2120795 -4.2422328][-4.0323029 -3.9630651 -3.9054661 -3.8507669 -3.7955744 -3.7477 -3.7023687 -3.6858196 -3.767693 -3.870194 -3.9660411 -4.0481129 -4.114572 -4.1690016 -4.2120476][-4.0354037 -3.9646502 -3.9064529 -3.8582883 -3.8232207 -3.8134043 -3.8098288 -3.8104315 -3.847456 -3.9037423 -3.9724526 -4.0427871 -4.1075253 -4.165473 -4.2150288][-4.1027341 -4.0424109 -3.9967344 -3.9686913 -3.9585242 -3.9716549 -3.9877388 -3.9886839 -3.9922385 -4.0102253 -4.0516105 -4.1071854 -4.1601963 -4.2102757 -4.2535129][-4.192534 -4.1500483 -4.1224384 -4.1137419 -4.1189666 -4.1368418 -4.1556926 -4.156064 -4.1482778 -4.1445317 -4.1649928 -4.2020226 -4.2360048 -4.2700772 -4.2995596][-4.2648053 -4.2415962 -4.2301555 -4.2319388 -4.2414684 -4.2556152 -4.2683554 -4.2703438 -4.2648668 -4.2559428 -4.2611518 -4.2795353 -4.2992105 -4.3181677 -4.3338375][-4.3100929 -4.3015208 -4.2994971 -4.3023806 -4.3095069 -4.3167214 -4.3218989 -4.3221703 -4.3192344 -4.3131123 -4.3121734 -4.3196216 -4.3301659 -4.3400803 -4.3474345][-4.3265467 -4.3228221 -4.3229342 -4.3248072 -4.3284483 -4.3311172 -4.3327994 -4.3329358 -4.3320932 -4.3296371 -4.3280516 -4.3308434 -4.3362913 -4.3416781 -4.3460832]]...]
INFO - root - 2017-12-06 03:52:31.932006: step 15010, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.237 sec/batch; 20h:52m:13s remains)
INFO - root - 2017-12-06 03:52:34.197941: step 15020, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.224 sec/batch; 19h:44m:03s remains)
INFO - root - 2017-12-06 03:52:36.516219: step 15030, loss = 2.05, batch loss = 1.99 (34.4 examples/sec; 0.233 sec/batch; 20h:31m:13s remains)
INFO - root - 2017-12-06 03:52:38.859904: step 15040, loss = 2.07, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 20h:13m:02s remains)
INFO - root - 2017-12-06 03:52:41.192735: step 15050, loss = 2.05, batch loss = 1.99 (36.0 examples/sec; 0.222 sec/batch; 19h:34m:29s remains)
INFO - root - 2017-12-06 03:52:43.494064: step 15060, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 18h:51m:21s remains)
INFO - root - 2017-12-06 03:52:45.826933: step 15070, loss = 2.11, batch loss = 2.05 (35.2 examples/sec; 0.227 sec/batch; 20h:01m:25s remains)
INFO - root - 2017-12-06 03:52:48.099290: step 15080, loss = 2.06, batch loss = 2.00 (36.9 examples/sec; 0.217 sec/batch; 19h:08m:18s remains)
INFO - root - 2017-12-06 03:52:50.427972: step 15090, loss = 2.09, batch loss = 2.03 (33.4 examples/sec; 0.239 sec/batch; 21h:06m:02s remains)
INFO - root - 2017-12-06 03:52:52.736029: step 15100, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 19h:31m:49s remains)
2017-12-06 03:52:53.045833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3135872 -4.3241687 -4.3293986 -4.3322864 -4.3335829 -4.3333054 -4.3302383 -4.3239794 -4.3174849 -4.3140154 -4.3175206 -4.3276873 -4.3357472 -4.3428683 -4.3494463][-4.2943888 -4.2987781 -4.3009439 -4.3049459 -4.3088937 -4.3097062 -4.307157 -4.2984467 -4.2872615 -4.2797275 -4.2818961 -4.2936969 -4.3044195 -4.3155 -4.3282127][-4.2560344 -4.2514176 -4.2496691 -4.2530012 -4.2566524 -4.2570977 -4.2556496 -4.24926 -4.240191 -4.23211 -4.2318892 -4.2435846 -4.2574134 -4.275032 -4.29651][-4.2081423 -4.1991119 -4.1933222 -4.19375 -4.190587 -4.1853685 -4.1827226 -4.1815195 -4.18022 -4.1786966 -4.1820579 -4.1964374 -4.2136011 -4.2371669 -4.264966][-4.1609092 -4.153089 -4.1454029 -4.1378875 -4.1226325 -4.10788 -4.103848 -4.1092548 -4.1169043 -4.1247606 -4.1358824 -4.1551881 -4.1760035 -4.2026067 -4.2342453][-4.1225929 -4.1201243 -4.1132674 -4.1003318 -4.0774765 -4.0556288 -4.0471363 -4.0554914 -4.0696535 -4.0843525 -4.1042676 -4.1291432 -4.1534657 -4.1819754 -4.2148695][-4.1093383 -4.10601 -4.0991473 -4.0891047 -4.0699267 -4.0479226 -4.0359192 -4.0436678 -4.0630283 -4.0859213 -4.109457 -4.1312795 -4.1528888 -4.1780958 -4.2107182][-4.1134219 -4.1058407 -4.0996785 -4.0967131 -4.0881023 -4.0724254 -4.0578952 -4.0606656 -4.0840063 -4.1123538 -4.1342096 -4.1515274 -4.166482 -4.1839595 -4.2127724][-4.1257176 -4.1155953 -4.1093335 -4.1147108 -4.114687 -4.1055241 -4.0865431 -4.0771241 -4.0953908 -4.1242361 -4.1454334 -4.1631832 -4.1782694 -4.1930566 -4.218051][-4.140645 -4.1276426 -4.1217713 -4.1369057 -4.1468663 -4.1403589 -4.11458 -4.089591 -4.091239 -4.108707 -4.1308537 -4.1573124 -4.1819329 -4.2026296 -4.2274256][-4.1542835 -4.1393833 -4.1337342 -4.1541276 -4.1734338 -4.1751308 -4.1517744 -4.1228266 -4.1146684 -4.1222105 -4.1453819 -4.1754389 -4.2029629 -4.2207565 -4.2400208][-4.1924019 -4.1798782 -4.17053 -4.1834016 -4.2021785 -4.206027 -4.1882844 -4.1667933 -4.1611676 -4.1687112 -4.1917791 -4.2174015 -4.2370143 -4.2436643 -4.2521715][-4.2397985 -4.2315483 -4.2201595 -4.2209229 -4.2295756 -4.230042 -4.220171 -4.2090206 -4.2089124 -4.2188873 -4.2421651 -4.2617793 -4.2714114 -4.2689037 -4.2694774][-4.2869554 -4.2848344 -4.2753234 -4.269784 -4.269825 -4.2687736 -4.26482 -4.2605181 -4.2618566 -4.27237 -4.2939034 -4.3071117 -4.3071685 -4.298955 -4.2961693][-4.3309984 -4.3347325 -4.330862 -4.3243032 -4.3200135 -4.3174219 -4.3154974 -4.3148069 -4.3162007 -4.3248639 -4.3405986 -4.3483453 -4.344245 -4.3340569 -4.3287454]]...]
INFO - root - 2017-12-06 03:52:55.346083: step 15110, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:14m:40s remains)
INFO - root - 2017-12-06 03:52:57.606771: step 15120, loss = 2.09, batch loss = 2.03 (35.1 examples/sec; 0.228 sec/batch; 20h:06m:41s remains)
INFO - root - 2017-12-06 03:52:59.889392: step 15130, loss = 2.07, batch loss = 2.01 (33.9 examples/sec; 0.236 sec/batch; 20h:48m:23s remains)
INFO - root - 2017-12-06 03:53:02.169030: step 15140, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:03m:57s remains)
INFO - root - 2017-12-06 03:53:04.485163: step 15150, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:14m:46s remains)
INFO - root - 2017-12-06 03:53:06.764707: step 15160, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:11m:43s remains)
INFO - root - 2017-12-06 03:53:09.066827: step 15170, loss = 2.07, batch loss = 2.01 (34.4 examples/sec; 0.233 sec/batch; 20h:31m:35s remains)
INFO - root - 2017-12-06 03:53:11.376404: step 15180, loss = 2.04, batch loss = 1.98 (35.1 examples/sec; 0.228 sec/batch; 20h:04m:05s remains)
INFO - root - 2017-12-06 03:53:13.668914: step 15190, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 19h:42m:07s remains)
INFO - root - 2017-12-06 03:53:15.966095: step 15200, loss = 2.03, batch loss = 1.98 (36.0 examples/sec; 0.222 sec/batch; 19h:35m:09s remains)
2017-12-06 03:53:16.238097: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2765751 -4.2637391 -4.2336764 -4.2113843 -4.2020893 -4.2004223 -4.1967516 -4.1883459 -4.1865416 -4.1995215 -4.217042 -4.2415562 -4.2714915 -4.2930675 -4.3005228][-4.293088 -4.2780323 -4.249763 -4.2262039 -4.2154975 -4.2118936 -4.2089825 -4.2028837 -4.2031231 -4.217092 -4.2329454 -4.2518921 -4.2734261 -4.2921882 -4.2984467][-4.2856317 -4.2716632 -4.2470551 -4.2242217 -4.2122769 -4.2043028 -4.2008295 -4.1960316 -4.1991787 -4.21376 -4.2300959 -4.2467694 -4.2642817 -4.2836666 -4.29371][-4.2637062 -4.2541704 -4.23521 -4.216435 -4.2057185 -4.1947 -4.1875873 -4.1792688 -4.1791234 -4.1909237 -4.2067041 -4.2240953 -4.2471585 -4.272212 -4.288837][-4.2351274 -4.2298741 -4.2159657 -4.2013392 -4.1924295 -4.1794167 -4.16539 -4.1485028 -4.1397486 -4.1457314 -4.1608424 -4.1880608 -4.227294 -4.2632761 -4.2858806][-4.2041044 -4.2025371 -4.1926904 -4.1818452 -4.1716576 -4.1502209 -4.1300335 -4.1061687 -4.0854325 -4.0845561 -4.105258 -4.1542377 -4.21382 -4.2613344 -4.2865891][-4.1739206 -4.177906 -4.1739411 -4.168726 -4.1587343 -4.1314716 -4.1038747 -4.0694776 -4.0369029 -4.0395164 -4.0744214 -4.1417174 -4.2148514 -4.2666144 -4.2904873][-4.1501374 -4.159399 -4.1615567 -4.1623974 -4.1551442 -4.1264381 -4.0913553 -4.0456944 -4.0103288 -4.023057 -4.0751119 -4.1507192 -4.2259107 -4.2744141 -4.2954106][-4.1340194 -4.1494646 -4.156321 -4.1586385 -4.1489706 -4.1174855 -4.0752778 -4.0283794 -4.0057325 -4.0381985 -4.1005821 -4.174654 -4.2434855 -4.2842493 -4.300571][-4.1512618 -4.1670341 -4.1716838 -4.1661181 -4.1476493 -4.1128106 -4.07573 -4.0474572 -4.0530262 -4.0987973 -4.1550579 -4.2120132 -4.2644143 -4.2951303 -4.3053727][-4.1875587 -4.2008529 -4.1997452 -4.1808519 -4.1517878 -4.1201458 -4.0960407 -4.092752 -4.117384 -4.1651978 -4.2113023 -4.2504292 -4.286737 -4.30565 -4.3093243][-4.2240858 -4.231812 -4.2242451 -4.2009563 -4.1734557 -4.1526833 -4.1402159 -4.1462822 -4.1744494 -4.2203841 -4.2587104 -4.285552 -4.3095422 -4.3179269 -4.3152413][-4.2671185 -4.2658339 -4.2531343 -4.2332435 -4.2156568 -4.2045088 -4.1981983 -4.2051582 -4.2309403 -4.2711 -4.2985134 -4.3162837 -4.3291945 -4.328321 -4.3200994][-4.3015676 -4.2917762 -4.2749934 -4.2581983 -4.246171 -4.2418041 -4.2429509 -4.2558613 -4.2820077 -4.3145418 -4.3325663 -4.3410835 -4.3418097 -4.333611 -4.3213935][-4.3193164 -4.3071518 -4.2927294 -4.2779908 -4.2665453 -4.2657743 -4.273633 -4.2912321 -4.3164191 -4.3384252 -4.3478289 -4.3479185 -4.341239 -4.3301358 -4.3184004]]...]
INFO - root - 2017-12-06 03:53:18.552560: step 15210, loss = 2.06, batch loss = 2.00 (32.3 examples/sec; 0.248 sec/batch; 21h:49m:59s remains)
INFO - root - 2017-12-06 03:53:20.827830: step 15220, loss = 2.05, batch loss = 1.99 (35.0 examples/sec; 0.228 sec/batch; 20h:07m:57s remains)
INFO - root - 2017-12-06 03:53:23.147080: step 15230, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 20h:09m:54s remains)
INFO - root - 2017-12-06 03:53:25.463371: step 15240, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:44m:25s remains)
INFO - root - 2017-12-06 03:53:27.835047: step 15250, loss = 2.06, batch loss = 2.00 (32.8 examples/sec; 0.244 sec/batch; 21h:31m:08s remains)
INFO - root - 2017-12-06 03:53:30.119919: step 15260, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.224 sec/batch; 19h:42m:03s remains)
INFO - root - 2017-12-06 03:53:32.414943: step 15270, loss = 2.07, batch loss = 2.02 (34.1 examples/sec; 0.235 sec/batch; 20h:41m:01s remains)
INFO - root - 2017-12-06 03:53:34.709979: step 15280, loss = 2.05, batch loss = 1.99 (33.0 examples/sec; 0.243 sec/batch; 21h:22m:06s remains)
INFO - root - 2017-12-06 03:53:37.027795: step 15290, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 19h:37m:11s remains)
INFO - root - 2017-12-06 03:53:39.362182: step 15300, loss = 2.06, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:21m:54s remains)
2017-12-06 03:53:39.659601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3214388 -4.312418 -4.3083019 -4.3069415 -4.309525 -4.3147607 -4.3206306 -4.3257146 -4.3281937 -4.3271503 -4.3233528 -4.3212752 -4.3209391 -4.3210187 -4.3233886][-4.2947044 -4.2828493 -4.2775741 -4.2775173 -4.2816849 -4.2888994 -4.2948675 -4.3004894 -4.3060327 -4.3068905 -4.3038383 -4.2997046 -4.2975645 -4.2967277 -4.3007388][-4.2613864 -4.2452168 -4.2360492 -4.2356887 -4.2425413 -4.2511692 -4.2551188 -4.2587862 -4.2707381 -4.2820449 -4.2840977 -4.2771077 -4.2707434 -4.2690191 -4.2755933][-4.2323041 -4.2054834 -4.1851296 -4.1797867 -4.1880264 -4.19756 -4.198844 -4.1982751 -4.2203197 -4.249558 -4.2631054 -4.2580204 -4.2499294 -4.2471013 -4.2553821][-4.2053337 -4.1649632 -4.131793 -4.1209364 -4.1283646 -4.1352286 -4.1279745 -4.1187077 -4.1514459 -4.2017531 -4.2299867 -4.2331138 -4.2301488 -4.2296157 -4.2402868][-4.177124 -4.1272554 -4.0820661 -4.0655718 -4.0678911 -4.0587449 -4.0269542 -3.9940879 -4.0400658 -4.1204543 -4.1690006 -4.1859374 -4.1980915 -4.2087793 -4.22593][-4.145504 -4.087254 -4.0300517 -4.0064197 -3.9991775 -3.9678371 -3.8974004 -3.8219798 -3.8792145 -3.9993496 -4.07642 -4.115139 -4.151711 -4.1799135 -4.2050233][-4.1228743 -4.0555372 -3.9872591 -3.9557796 -3.9442165 -3.899709 -3.7977679 -3.680475 -3.7396796 -3.8864756 -3.9846723 -4.0426874 -4.1008382 -4.1439385 -4.1775422][-4.1199026 -4.0479531 -3.9766014 -3.9494505 -3.9492803 -3.916723 -3.8257587 -3.7132363 -3.7479353 -3.8664067 -3.9465618 -3.9992986 -4.0622096 -4.1127105 -4.1510606][-4.145216 -4.0756483 -4.0126872 -4.0010509 -4.018744 -4.0050883 -3.9463687 -3.8677192 -3.8807125 -3.9503655 -3.9915779 -4.0202546 -4.0682111 -4.11258 -4.1450524][-4.2034235 -4.1471062 -4.100636 -4.1004314 -4.1241474 -4.1203303 -4.086524 -4.0411034 -4.0463977 -4.0828671 -4.0986204 -4.1073108 -4.1327162 -4.15857 -4.1753507][-4.2730942 -4.23416 -4.2038155 -4.2076068 -4.2291689 -4.2309546 -4.2154045 -4.1942234 -4.2010436 -4.222013 -4.2247787 -4.22361 -4.2322636 -4.2395387 -4.2404222][-4.32104 -4.2989774 -4.2825565 -4.2858682 -4.3005624 -4.3076596 -4.3052983 -4.2993951 -4.305624 -4.3172488 -4.3161049 -4.3141851 -4.3160443 -4.3137059 -4.3052249][-4.3411441 -4.3292017 -4.3190169 -4.3189864 -4.3256054 -4.3315287 -4.3328342 -4.3318396 -4.3355055 -4.3430724 -4.34503 -4.3467655 -4.3505039 -4.3483057 -4.338922][-4.349092 -4.3417759 -4.3355069 -4.333693 -4.3355279 -4.3382621 -4.339788 -4.3399391 -4.3418541 -4.3459249 -4.3482409 -4.3514886 -4.3552589 -4.3543205 -4.3485131]]...]
INFO - root - 2017-12-06 03:53:41.945092: step 15310, loss = 2.09, batch loss = 2.03 (34.6 examples/sec; 0.231 sec/batch; 20h:20m:41s remains)
INFO - root - 2017-12-06 03:53:44.290887: step 15320, loss = 2.09, batch loss = 2.03 (36.2 examples/sec; 0.221 sec/batch; 19h:27m:24s remains)
INFO - root - 2017-12-06 03:53:46.570723: step 15330, loss = 2.03, batch loss = 1.97 (34.9 examples/sec; 0.229 sec/batch; 20h:11m:07s remains)
INFO - root - 2017-12-06 03:53:48.871710: step 15340, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 19h:57m:49s remains)
INFO - root - 2017-12-06 03:53:51.136361: step 15350, loss = 2.09, batch loss = 2.03 (36.1 examples/sec; 0.222 sec/batch; 19h:32m:47s remains)
INFO - root - 2017-12-06 03:53:53.428722: step 15360, loss = 2.04, batch loss = 1.98 (34.8 examples/sec; 0.230 sec/batch; 20h:14m:59s remains)
INFO - root - 2017-12-06 03:53:55.716679: step 15370, loss = 2.06, batch loss = 2.00 (34.1 examples/sec; 0.235 sec/batch; 20h:41m:43s remains)
INFO - root - 2017-12-06 03:53:58.023131: step 15380, loss = 2.04, batch loss = 1.98 (35.2 examples/sec; 0.227 sec/batch; 20h:00m:08s remains)
INFO - root - 2017-12-06 03:54:00.321555: step 15390, loss = 2.09, batch loss = 2.03 (35.2 examples/sec; 0.227 sec/batch; 20h:00m:36s remains)
INFO - root - 2017-12-06 03:54:02.603216: step 15400, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.228 sec/batch; 20h:02m:33s remains)
2017-12-06 03:54:02.905446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.299943 -4.30054 -4.301342 -4.3010883 -4.298964 -4.295053 -4.2909188 -4.2886171 -4.2886019 -4.2896185 -4.2928486 -4.2965088 -4.2989745 -4.3007326 -4.3012094][-4.2807245 -4.2821479 -4.2838993 -4.2848043 -4.2829275 -4.278141 -4.2727041 -4.2685027 -4.2670565 -4.267395 -4.2703643 -4.2740655 -4.2771554 -4.2814221 -4.2840505][-4.2498908 -4.2521787 -4.2553253 -4.2569671 -4.2546587 -4.2465868 -4.2361975 -4.2271342 -4.2240062 -4.2257175 -4.2307549 -4.2359376 -4.2414427 -4.2495475 -4.2550821][-4.2121897 -4.21442 -4.2175169 -4.2179689 -4.2137375 -4.2002463 -4.1815987 -4.1647983 -4.161325 -4.1707644 -4.1835575 -4.1929502 -4.2028522 -4.214839 -4.2225327][-4.1832724 -4.1832757 -4.1823869 -4.1779909 -4.16883 -4.1465621 -4.1139894 -4.0866385 -4.0878716 -4.1141129 -4.1405983 -4.1565895 -4.1711721 -4.1878777 -4.1969175][-4.1674657 -4.1610909 -4.1508536 -4.13741 -4.1204357 -4.0866742 -4.0351753 -3.9986434 -4.0174608 -4.0681844 -4.107729 -4.1300011 -4.14867 -4.1673169 -4.1760435][-4.1624436 -4.1449952 -4.1199312 -4.0955143 -4.0727921 -4.03131 -3.9650958 -3.92615 -3.9664795 -4.0357924 -4.0850077 -4.1144667 -4.1360941 -4.15355 -4.1615949][-4.1759295 -4.1519217 -4.1172791 -4.086833 -4.0659328 -4.0324426 -3.9779921 -3.9485774 -3.9886773 -4.0485415 -4.0881424 -4.1135731 -4.1321974 -4.1469297 -4.1558676][-4.1899514 -4.1685987 -4.1374793 -4.1130676 -4.1046844 -4.0912762 -4.063128 -4.0488653 -4.0770979 -4.1132727 -4.132628 -4.1444588 -4.1522064 -4.1554551 -4.1574221][-4.2023792 -4.1879563 -4.1692948 -4.1579504 -4.1615276 -4.1611929 -4.1494966 -4.1426115 -4.1588964 -4.1777225 -4.1857629 -4.1903472 -4.1898441 -4.1792526 -4.1706915][-4.2170238 -4.2153149 -4.21128 -4.2118688 -4.2200484 -4.2239065 -4.219789 -4.2145977 -4.22229 -4.2311163 -4.2350974 -4.2362251 -4.2327356 -4.2169023 -4.2004323][-4.2273784 -4.2376652 -4.2416153 -4.2464304 -4.2568946 -4.2642808 -4.2657318 -4.2622685 -4.2665753 -4.2705431 -4.2705889 -4.267869 -4.2610493 -4.2430797 -4.2241325][-4.2372918 -4.2509556 -4.2515616 -4.2520733 -4.26298 -4.2752585 -4.2814617 -4.281527 -4.2850986 -4.2841125 -4.2799311 -4.2745676 -4.2677374 -4.2551155 -4.2417507][-4.2367215 -4.246428 -4.2394552 -4.2331963 -4.2445269 -4.2616286 -4.2712584 -4.2748013 -4.2790565 -4.275784 -4.2702127 -4.2661519 -4.2641106 -4.2606773 -4.254221][-4.2209554 -4.2263856 -4.2166681 -4.2086353 -4.2198339 -4.2419081 -4.2563438 -4.2636318 -4.2699118 -4.2679276 -4.2630267 -4.2602515 -4.2629151 -4.2650928 -4.2612042]]...]
INFO - root - 2017-12-06 03:54:05.179466: step 15410, loss = 2.05, batch loss = 1.99 (34.3 examples/sec; 0.233 sec/batch; 20h:33m:59s remains)
INFO - root - 2017-12-06 03:54:07.449295: step 15420, loss = 2.04, batch loss = 1.98 (36.2 examples/sec; 0.221 sec/batch; 19h:28m:07s remains)
INFO - root - 2017-12-06 03:54:09.753283: step 15430, loss = 2.09, batch loss = 2.03 (33.9 examples/sec; 0.236 sec/batch; 20h:47m:17s remains)
INFO - root - 2017-12-06 03:54:12.095412: step 15440, loss = 2.07, batch loss = 2.01 (33.7 examples/sec; 0.237 sec/batch; 20h:55m:01s remains)
INFO - root - 2017-12-06 03:54:14.380663: step 15450, loss = 2.05, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 20h:14m:19s remains)
INFO - root - 2017-12-06 03:54:16.676872: step 15460, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 20h:08m:44s remains)
INFO - root - 2017-12-06 03:54:18.992724: step 15470, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:34m:50s remains)
INFO - root - 2017-12-06 03:54:21.304484: step 15480, loss = 2.06, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 20h:24m:44s remains)
INFO - root - 2017-12-06 03:54:23.597932: step 15490, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.224 sec/batch; 19h:41m:45s remains)
INFO - root - 2017-12-06 03:54:25.886322: step 15500, loss = 2.05, batch loss = 2.00 (34.4 examples/sec; 0.233 sec/batch; 20h:29m:45s remains)
2017-12-06 03:54:26.171662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1120124 -4.1194859 -4.1215219 -4.1256909 -4.1355181 -4.1377373 -4.1602306 -4.1872606 -4.1952066 -4.2125912 -4.2357864 -4.245647 -4.2455349 -4.2390556 -4.2236156][-4.0637817 -4.0706859 -4.0805321 -4.1021891 -4.130692 -4.1425505 -4.1675115 -4.1948471 -4.197185 -4.2034984 -4.2213993 -4.2293134 -4.2264495 -4.2204871 -4.2055769][-4.0529652 -4.0682983 -4.0912395 -4.12842 -4.1674619 -4.1819115 -4.1971841 -4.2087369 -4.2015252 -4.1970692 -4.210619 -4.2195206 -4.2200217 -4.2211022 -4.2078881][-4.097167 -4.1193361 -4.1509523 -4.1862454 -4.2104707 -4.2122674 -4.2041316 -4.1940193 -4.1791935 -4.17107 -4.1851635 -4.1976557 -4.2035227 -4.2136488 -4.2088943][-4.1597524 -4.1818442 -4.2062583 -4.2244353 -4.2237291 -4.2000289 -4.1536331 -4.1197209 -4.1140361 -4.118948 -4.1447954 -4.166059 -4.1778135 -4.1983414 -4.2096872][-4.2198186 -4.2303667 -4.2313752 -4.2211528 -4.1905661 -4.13031 -4.0337067 -3.9755774 -3.9990215 -4.038847 -4.0837803 -4.1298962 -4.1596575 -4.1964445 -4.2280574][-4.2520466 -4.2440567 -4.222867 -4.1867161 -4.1251583 -4.0247273 -3.8702581 -3.7774627 -3.8491631 -3.9438 -4.0166478 -4.0939827 -4.1529226 -4.2085142 -4.2567668][-4.2572241 -4.2383494 -4.2017574 -4.1457691 -4.0616236 -3.9438946 -3.7791264 -3.6904902 -3.7988055 -3.9297314 -4.0234818 -4.1147213 -4.1862369 -4.2441649 -4.2919526][-4.2607613 -4.2319613 -4.1901221 -4.1250033 -4.0395331 -3.9458802 -3.8469706 -3.8106441 -3.907351 -4.0200047 -4.1063724 -4.1941457 -4.2595878 -4.2996922 -4.3261061][-4.2647977 -4.2295427 -4.1860495 -4.1271238 -4.0621858 -4.0120096 -3.9808748 -3.99126 -4.06762 -4.1457458 -4.2094016 -4.2763481 -4.32148 -4.3374639 -4.33776][-4.2576962 -4.2120767 -4.1600966 -4.1110206 -4.0731878 -4.0642519 -4.0788236 -4.1188908 -4.1857362 -4.2378683 -4.2747211 -4.310904 -4.3319421 -4.3295431 -4.318265][-4.2289305 -4.1659474 -4.1015391 -4.0549479 -4.0483484 -4.0798988 -4.1257648 -4.1841702 -4.2454972 -4.2778606 -4.2873645 -4.2939067 -4.2945437 -4.28338 -4.2698765][-4.1880369 -4.1141462 -4.042448 -4.0015616 -4.0259409 -4.0916963 -4.1576052 -4.2188306 -4.265676 -4.2783175 -4.2633996 -4.2425308 -4.2299461 -4.2206845 -4.21096][-4.1733484 -4.1031919 -4.0365367 -4.005177 -4.0417743 -4.1188221 -4.1876864 -4.2356272 -4.2571859 -4.2495847 -4.21846 -4.1796007 -4.1585093 -4.1611586 -4.16305][-4.2074919 -4.1578503 -4.1069031 -4.0793457 -4.1054807 -4.16689 -4.2200804 -4.2458978 -4.2461033 -4.2258344 -4.1886778 -4.1457829 -4.1249065 -4.1431956 -4.1619635]]...]
INFO - root - 2017-12-06 03:54:28.499745: step 15510, loss = 2.05, batch loss = 1.99 (30.7 examples/sec; 0.260 sec/batch; 22h:55m:30s remains)
INFO - root - 2017-12-06 03:54:30.816379: step 15520, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 20h:13m:59s remains)
INFO - root - 2017-12-06 03:54:33.130248: step 15530, loss = 2.06, batch loss = 2.00 (36.2 examples/sec; 0.221 sec/batch; 19h:26m:46s remains)
INFO - root - 2017-12-06 03:54:35.433156: step 15540, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.235 sec/batch; 20h:39m:18s remains)
INFO - root - 2017-12-06 03:54:37.727723: step 15550, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 20h:08m:53s remains)
INFO - root - 2017-12-06 03:54:40.055026: step 15560, loss = 2.08, batch loss = 2.02 (33.8 examples/sec; 0.237 sec/batch; 20h:49m:46s remains)
INFO - root - 2017-12-06 03:54:42.320341: step 15570, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:45m:54s remains)
INFO - root - 2017-12-06 03:54:44.604834: step 15580, loss = 2.03, batch loss = 1.97 (35.1 examples/sec; 0.228 sec/batch; 20h:02m:22s remains)
INFO - root - 2017-12-06 03:54:46.853027: step 15590, loss = 2.09, batch loss = 2.04 (35.5 examples/sec; 0.225 sec/batch; 19h:49m:56s remains)
INFO - root - 2017-12-06 03:54:49.117519: step 15600, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.221 sec/batch; 19h:25m:13s remains)
2017-12-06 03:54:49.405259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2261124 -4.2372975 -4.2449188 -4.226193 -4.1981568 -4.2005887 -4.2303352 -4.25624 -4.2530923 -4.2328424 -4.2126236 -4.2002568 -4.2033429 -4.2269249 -4.265913][-4.2155151 -4.226779 -4.2333412 -4.2131352 -4.1765876 -4.1723866 -4.2045097 -4.2347417 -4.2378035 -4.2245536 -4.2113013 -4.2052584 -4.2113113 -4.2350321 -4.2692842][-4.2228961 -4.2336717 -4.2349224 -4.2078419 -4.1637278 -4.1493425 -4.1797466 -4.2114787 -4.2195148 -4.2138772 -4.2100043 -4.2129636 -4.221952 -4.2462006 -4.2760906][-4.2414885 -4.2534904 -4.2474737 -4.2100363 -4.1556277 -4.1283 -4.1488256 -4.174685 -4.1852069 -4.1888676 -4.1979175 -4.2140861 -4.2300735 -4.2562466 -4.2841249][-4.2610793 -4.277482 -4.2689714 -4.2242908 -4.1608262 -4.1158705 -4.1119666 -4.1177764 -4.1223574 -4.1338444 -4.158576 -4.1913147 -4.2229972 -4.2586927 -4.2892971][-4.2821631 -4.3006077 -4.2944269 -4.2524219 -4.188139 -4.1253362 -4.0831594 -4.0526218 -4.0386763 -4.05507 -4.0981183 -4.1514606 -4.2006249 -4.2483625 -4.2873778][-4.3018565 -4.3197703 -4.3153896 -4.2820683 -4.22569 -4.1555295 -4.0800304 -4.0071855 -3.9680893 -3.9856412 -4.0456371 -4.11704 -4.1790257 -4.2343469 -4.2799773][-4.3123097 -4.3290777 -4.3275104 -4.3053904 -4.2638125 -4.2047215 -4.1255684 -4.039752 -3.9877725 -3.9972551 -4.0510631 -4.1198664 -4.1806636 -4.2372689 -4.2836][-4.3102636 -4.326159 -4.3268542 -4.3136835 -4.2839918 -4.2406273 -4.1800952 -4.1139827 -4.0727224 -4.0743771 -4.1032996 -4.1476688 -4.19536 -4.2481141 -4.2923851][-4.2980585 -4.311553 -4.3144555 -4.3073573 -4.2870388 -4.2548275 -4.2144685 -4.174191 -4.1540093 -4.1557555 -4.1665616 -4.1844687 -4.2134323 -4.2557068 -4.2954097][-4.2851434 -4.294209 -4.2952137 -4.2894158 -4.2723818 -4.2455459 -4.2184825 -4.1977534 -4.1968708 -4.20501 -4.2139611 -4.2227221 -4.2400122 -4.2701406 -4.3016415][-4.2808161 -4.2833595 -4.2769165 -4.2654653 -4.2447381 -4.2169023 -4.1952538 -4.1874971 -4.2041535 -4.2215967 -4.2343941 -4.2429595 -4.2579694 -4.2819958 -4.3067651][-4.2879453 -4.2827058 -4.2645473 -4.2408495 -4.2114315 -4.1782403 -4.1549888 -4.1514778 -4.1790934 -4.2098165 -4.23139 -4.2443118 -4.2596178 -4.2811856 -4.3029137][-4.2936411 -4.2800512 -4.248991 -4.2083817 -4.1634831 -4.1234303 -4.0986681 -4.0966067 -4.1324773 -4.1774063 -4.2101564 -4.2288752 -4.2454486 -4.267127 -4.2901049][-4.2799411 -4.2612586 -4.2235017 -4.1711574 -4.1157479 -4.0742254 -4.0522189 -4.05142 -4.0901837 -4.1465263 -4.1878338 -4.2102757 -4.2270174 -4.2489753 -4.2773185]]...]
INFO - root - 2017-12-06 03:54:51.712798: step 15610, loss = 2.08, batch loss = 2.02 (34.6 examples/sec; 0.231 sec/batch; 20h:21m:20s remains)
INFO - root - 2017-12-06 03:54:53.972545: step 15620, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.228 sec/batch; 20h:05m:45s remains)
INFO - root - 2017-12-06 03:54:56.233331: step 15630, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 19h:49m:49s remains)
INFO - root - 2017-12-06 03:54:58.571646: step 15640, loss = 2.08, batch loss = 2.02 (34.4 examples/sec; 0.233 sec/batch; 20h:28m:00s remains)
INFO - root - 2017-12-06 03:55:00.854465: step 15650, loss = 2.09, batch loss = 2.03 (34.4 examples/sec; 0.233 sec/batch; 20h:29m:25s remains)
INFO - root - 2017-12-06 03:55:03.143788: step 15660, loss = 2.08, batch loss = 2.02 (37.9 examples/sec; 0.211 sec/batch; 18h:35m:37s remains)
INFO - root - 2017-12-06 03:55:05.460036: step 15670, loss = 2.07, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 20h:25m:25s remains)
INFO - root - 2017-12-06 03:55:07.774210: step 15680, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 19h:56m:10s remains)
INFO - root - 2017-12-06 03:55:10.128886: step 15690, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.230 sec/batch; 20h:12m:05s remains)
INFO - root - 2017-12-06 03:55:12.397489: step 15700, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.221 sec/batch; 19h:24m:51s remains)
2017-12-06 03:55:12.699149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3197761 -4.3159752 -4.3134441 -4.3115568 -4.3107042 -4.3105459 -4.3107691 -4.310955 -4.3115478 -4.31164 -4.3131371 -4.3156343 -4.3197961 -4.3248277 -4.3301053][-4.3133616 -4.3064218 -4.3020072 -4.2992244 -4.297976 -4.2976112 -4.2982526 -4.2974253 -4.2969337 -4.2969608 -4.2957544 -4.2942929 -4.2984591 -4.3077641 -4.3167171][-4.3068061 -4.2950053 -4.2871566 -4.2812667 -4.2761054 -4.2728949 -4.271976 -4.270349 -4.269928 -4.271914 -4.268446 -4.2627015 -4.2668047 -4.2808003 -4.29327][-4.2933373 -4.2754188 -4.2621365 -4.250669 -4.2373691 -4.2283845 -4.2223511 -4.2186375 -4.2240014 -4.2330437 -4.2295518 -4.2208791 -4.2247548 -4.2428646 -4.2575459][-4.2797327 -4.2549119 -4.2341332 -4.2133875 -4.1886373 -4.1662 -4.1474385 -4.140461 -4.1607418 -4.1901789 -4.1983495 -4.1887927 -4.1904926 -4.2078853 -4.2217674][-4.2721033 -4.2427535 -4.2156911 -4.1847405 -4.1442471 -4.0967407 -4.04521 -4.0174847 -4.0556254 -4.1213989 -4.1557722 -4.16229 -4.1721191 -4.1897984 -4.2029004][-4.2618685 -4.2294264 -4.1932459 -4.1491213 -4.0871887 -4.0023303 -3.8980131 -3.8298323 -3.8886566 -3.9989696 -4.0730972 -4.1099129 -4.1417093 -4.1727252 -4.1897354][-4.2472315 -4.20988 -4.1632619 -4.1072941 -4.0279536 -3.9114685 -3.7568543 -3.6416538 -3.7188368 -3.8711691 -3.9824977 -4.0461922 -4.0991049 -4.1452713 -4.167882][-4.2308016 -4.1901989 -4.136961 -4.0795693 -4.0070591 -3.9016256 -3.757241 -3.6413841 -3.6998978 -3.8324044 -3.9387033 -4.0070553 -4.0662742 -4.1213856 -4.1522202][-4.2068486 -4.1635857 -4.10962 -4.061799 -4.0164375 -3.956146 -3.8719096 -3.8036978 -3.8321035 -3.9083138 -3.9771857 -4.02465 -4.0714297 -4.1205807 -4.1550465][-4.169744 -4.1235952 -4.0787625 -4.0523396 -4.0408535 -4.0270514 -3.9949574 -3.9658978 -3.977704 -4.01134 -4.047461 -4.0713725 -4.1000147 -4.1407905 -4.1765347][-4.1496172 -4.1046333 -4.0695806 -4.0598063 -4.0748286 -4.0921674 -4.0938816 -4.0892487 -4.0924511 -4.1031318 -4.1178675 -4.1287427 -4.1466875 -4.1786189 -4.2109566][-4.157948 -4.1211691 -4.0967236 -4.0979481 -4.1252975 -4.1538973 -4.1708179 -4.1757283 -4.1764793 -4.1813812 -4.1927152 -4.2049465 -4.2196164 -4.2396865 -4.2589817][-4.1995516 -4.1751966 -4.1619663 -4.1695023 -4.195118 -4.2216172 -4.2419672 -4.2524247 -4.2553916 -4.258595 -4.2676311 -4.2787919 -4.2886214 -4.2967205 -4.3017416][-4.2490807 -4.2315488 -4.2247972 -4.2340856 -4.25352 -4.2727771 -4.2886763 -4.3008742 -4.3071394 -4.3105564 -4.3160806 -4.3224983 -4.3270392 -4.3285418 -4.327086]]...]
INFO - root - 2017-12-06 03:55:15.002912: step 15710, loss = 2.05, batch loss = 1.99 (33.5 examples/sec; 0.239 sec/batch; 21h:02m:33s remains)
INFO - root - 2017-12-06 03:55:17.299848: step 15720, loss = 2.08, batch loss = 2.02 (35.0 examples/sec; 0.229 sec/batch; 20h:07m:38s remains)
INFO - root - 2017-12-06 03:55:19.613500: step 15730, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 19h:49m:08s remains)
INFO - root - 2017-12-06 03:55:21.928918: step 15740, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:32m:16s remains)
INFO - root - 2017-12-06 03:55:24.272403: step 15750, loss = 2.06, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 20h:22m:44s remains)
INFO - root - 2017-12-06 03:55:26.539590: step 15760, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 19h:46m:45s remains)
INFO - root - 2017-12-06 03:55:28.857781: step 15770, loss = 2.04, batch loss = 1.98 (31.8 examples/sec; 0.252 sec/batch; 22h:08m:11s remains)
INFO - root - 2017-12-06 03:55:31.179934: step 15780, loss = 2.08, batch loss = 2.02 (33.8 examples/sec; 0.237 sec/batch; 20h:50m:35s remains)
INFO - root - 2017-12-06 03:55:33.453149: step 15790, loss = 2.07, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 20h:25m:24s remains)
INFO - root - 2017-12-06 03:55:35.764787: step 15800, loss = 2.09, batch loss = 2.03 (35.8 examples/sec; 0.224 sec/batch; 19h:40m:21s remains)
2017-12-06 03:55:36.056553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2846832 -4.2548728 -4.2355719 -4.2280741 -4.222662 -4.2252083 -4.2368078 -4.2566376 -4.2721515 -4.2744522 -4.2703261 -4.2613673 -4.2502542 -4.2466359 -4.2548809][-4.2482085 -4.2026229 -4.1734166 -4.1615191 -4.1467447 -4.1451654 -4.158865 -4.1867375 -4.2083273 -4.2140336 -4.2060747 -4.1901808 -4.1729894 -4.1686659 -4.182631][-4.2235436 -4.1677418 -4.1301603 -4.1077485 -4.0780568 -4.066123 -4.076139 -4.1079979 -4.13372 -4.1452284 -4.1401515 -4.1255479 -4.1104245 -4.1108465 -4.1283736][-4.2132974 -4.1489739 -4.09857 -4.0552611 -4.0069871 -3.9877932 -4.0038147 -4.0424981 -4.0709434 -4.0838223 -4.08607 -4.08139 -4.079011 -4.0913377 -4.1139197][-4.220571 -4.1496267 -4.0794749 -4.0058441 -3.9314127 -3.8918638 -3.9007089 -3.9501879 -3.9914389 -4.0126524 -4.0296421 -4.0468869 -4.0665374 -4.0995646 -4.1342368][-4.2337995 -4.157464 -4.0664554 -3.9616969 -3.8551769 -3.7692595 -3.7405143 -3.8033211 -3.8803554 -3.922138 -3.9632635 -4.0128536 -4.0583396 -4.1147985 -4.1693306][-4.2391558 -4.1591177 -4.052381 -3.9267068 -3.790863 -3.6440914 -3.550936 -3.6360478 -3.7675393 -3.8441586 -3.9136624 -3.9943893 -4.0599928 -4.1334167 -4.2061253][-4.2299709 -4.15288 -4.0487452 -3.9230835 -3.7711325 -3.5852027 -3.4403195 -3.5427029 -3.7207706 -3.8289151 -3.9188073 -4.0121326 -4.0877042 -4.1631646 -4.2374592][-4.2363706 -4.1713467 -4.0910926 -3.9885645 -3.8577785 -3.7075152 -3.5945296 -3.6803892 -3.8250628 -3.9179282 -3.9964368 -4.0797868 -4.1462917 -4.2043295 -4.2603364][-4.2626705 -4.216579 -4.167613 -4.0899348 -3.9908674 -3.9003127 -3.8484766 -3.9070442 -3.9957154 -4.0562305 -4.1080365 -4.1682978 -4.2133627 -4.2500391 -4.2834373][-4.2925944 -4.265131 -4.2381492 -4.18826 -4.1233397 -4.0753064 -4.0555091 -4.0922012 -4.1463327 -4.1822195 -4.2118773 -4.2470136 -4.2726617 -4.2934461 -4.3094926][-4.3151574 -4.3029919 -4.2873483 -4.259891 -4.2260451 -4.2040887 -4.1957107 -4.2147517 -4.249372 -4.2749209 -4.2908721 -4.3075619 -4.3210168 -4.3315587 -4.3349066][-4.3278294 -4.3231144 -4.3165712 -4.3025732 -4.2841244 -4.2746024 -4.273067 -4.2870193 -4.3102255 -4.3314238 -4.3426986 -4.3496523 -4.3551688 -4.3568439 -4.3515859][-4.3314047 -4.3283877 -4.3258557 -4.3212652 -4.3118868 -4.3092237 -4.3135023 -4.3260241 -4.3398309 -4.354001 -4.3618455 -4.3652449 -4.3656154 -4.3625097 -4.3547969][-4.332725 -4.3286481 -4.3263307 -4.3239703 -4.3192949 -4.3188734 -4.3231831 -4.3319373 -4.3406124 -4.3483434 -4.3536711 -4.3547778 -4.3534312 -4.3495049 -4.3439322]]...]
INFO - root - 2017-12-06 03:55:38.366709: step 15810, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 20h:08m:56s remains)
INFO - root - 2017-12-06 03:55:40.626341: step 15820, loss = 2.06, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 19h:42m:00s remains)
INFO - root - 2017-12-06 03:55:42.914444: step 15830, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 19h:42m:41s remains)
INFO - root - 2017-12-06 03:55:45.197614: step 15840, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.234 sec/batch; 20h:37m:35s remains)
INFO - root - 2017-12-06 03:55:47.539021: step 15850, loss = 2.04, batch loss = 1.98 (34.7 examples/sec; 0.230 sec/batch; 20h:15m:25s remains)
INFO - root - 2017-12-06 03:55:49.829940: step 15860, loss = 2.05, batch loss = 1.99 (35.0 examples/sec; 0.229 sec/batch; 20h:06m:18s remains)
INFO - root - 2017-12-06 03:55:52.126847: step 15870, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 19h:43m:03s remains)
INFO - root - 2017-12-06 03:55:54.459799: step 15880, loss = 2.06, batch loss = 2.01 (33.3 examples/sec; 0.240 sec/batch; 21h:06m:21s remains)
INFO - root - 2017-12-06 03:55:56.749934: step 15890, loss = 2.07, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 20h:00m:00s remains)
INFO - root - 2017-12-06 03:55:59.016148: step 15900, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 20h:34m:18s remains)
2017-12-06 03:55:59.334849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3083358 -4.3131003 -4.308701 -4.30681 -4.3039808 -4.3058863 -4.3093462 -4.310636 -4.3096871 -4.3046813 -4.2988958 -4.2947969 -4.2958441 -4.300838 -4.3075604][-4.2807274 -4.2891359 -4.2837176 -4.2808652 -4.2790914 -4.284513 -4.2930374 -4.2981248 -4.29628 -4.2870784 -4.2748971 -4.2659597 -4.2665343 -4.2759743 -4.2885766][-4.2428942 -4.2511148 -4.241601 -4.2354226 -4.2338719 -4.2433181 -4.257297 -4.2689672 -4.2682195 -4.2549944 -4.2361565 -4.2239013 -4.227705 -4.2451005 -4.2656379][-4.19699 -4.1983962 -4.1808438 -4.1704469 -4.168705 -4.1798592 -4.198947 -4.2180443 -4.2269797 -4.2211571 -4.2048945 -4.195025 -4.2046275 -4.2298136 -4.255672][-4.1602192 -4.1510296 -4.1236687 -4.1069713 -4.1015754 -4.1057997 -4.1187487 -4.1400542 -4.1647248 -4.1835523 -4.1856232 -4.1877203 -4.2028503 -4.232945 -4.2582936][-4.1469026 -4.1304903 -4.0965157 -4.0701389 -4.0534644 -4.03366 -4.0209956 -4.0294065 -4.0718789 -4.1264281 -4.1603942 -4.1840739 -4.2089767 -4.2411585 -4.2647157][-4.1570473 -4.1405559 -4.1076889 -4.0745153 -4.0386329 -3.9807725 -3.9199183 -3.8937361 -3.9548812 -4.0502543 -4.1161809 -4.16088 -4.1963573 -4.2326946 -4.2599692][-4.1728306 -4.1651382 -4.144464 -4.1140542 -4.0642838 -3.976557 -3.8749642 -3.8142288 -3.8794403 -3.9892938 -4.06083 -4.1082015 -4.1493826 -4.1942444 -4.2355056][-4.1764264 -4.1860194 -4.1845574 -4.1658883 -4.1203918 -4.03659 -3.9445016 -3.8939757 -3.9344397 -4.001421 -4.0365524 -4.0605845 -4.0947866 -4.1463952 -4.2020254][-4.1734061 -4.2036796 -4.2235317 -4.2199497 -4.1908622 -4.1307173 -4.068778 -4.0348692 -4.0445094 -4.0606241 -4.0547009 -4.0533323 -4.0750461 -4.124548 -4.1809268][-4.1827965 -4.2201643 -4.2541132 -4.2632904 -4.2511449 -4.2153158 -4.1783438 -4.1560326 -4.1496215 -4.1387587 -4.115634 -4.1021986 -4.1110373 -4.1468349 -4.1888766][-4.209712 -4.2395668 -4.274004 -4.290215 -4.2907758 -4.2729635 -4.2523828 -4.2397141 -4.2309289 -4.2151089 -4.1941433 -4.1814628 -4.18422 -4.2040315 -4.2260208][-4.2475743 -4.2621531 -4.2884426 -4.3065867 -4.315186 -4.3085761 -4.29847 -4.2914896 -4.2848806 -4.2748713 -4.2639294 -4.2579169 -4.2595263 -4.2696481 -4.2770729][-4.28998 -4.2935104 -4.3092327 -4.3232565 -4.3322344 -4.3311663 -4.3275418 -4.3229036 -4.3197536 -4.3166976 -4.3143396 -4.3138914 -4.31701 -4.32182 -4.3207846][-4.3297262 -4.329411 -4.3382964 -4.3452907 -4.3491325 -4.3484097 -4.3464546 -4.3441296 -4.3437982 -4.3442245 -4.3455939 -4.3465495 -4.3504272 -4.3536358 -4.3502803]]...]
INFO - root - 2017-12-06 03:56:01.593676: step 15910, loss = 2.09, batch loss = 2.03 (32.7 examples/sec; 0.245 sec/batch; 21h:31m:52s remains)
INFO - root - 2017-12-06 03:56:03.896880: step 15920, loss = 2.05, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 19h:19m:37s remains)
INFO - root - 2017-12-06 03:56:06.200478: step 15930, loss = 2.05, batch loss = 1.99 (36.9 examples/sec; 0.217 sec/batch; 19h:03m:16s remains)
INFO - root - 2017-12-06 03:56:08.478903: step 15940, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:33m:19s remains)
INFO - root - 2017-12-06 03:56:10.787963: step 15950, loss = 2.04, batch loss = 1.99 (34.7 examples/sec; 0.231 sec/batch; 20h:16m:06s remains)
INFO - root - 2017-12-06 03:56:13.121820: step 15960, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:32m:41s remains)
INFO - root - 2017-12-06 03:56:15.454661: step 15970, loss = 2.05, batch loss = 2.00 (34.4 examples/sec; 0.233 sec/batch; 20h:28m:13s remains)
INFO - root - 2017-12-06 03:56:17.761276: step 15980, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 20h:06m:13s remains)
INFO - root - 2017-12-06 03:56:20.060389: step 15990, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:43m:36s remains)
INFO - root - 2017-12-06 03:56:22.376298: step 16000, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 20h:17m:17s remains)
2017-12-06 03:56:22.651054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3250847 -4.3286643 -4.3329992 -4.3362775 -4.3364058 -4.3308954 -4.3198247 -4.3094707 -4.3058467 -4.3104296 -4.3214045 -4.3339086 -4.3476973 -4.3556743 -4.359405][-4.2817154 -4.2869725 -4.29611 -4.3053689 -4.3096485 -4.3025155 -4.28625 -4.2715492 -4.2658291 -4.2739625 -4.2930126 -4.31263 -4.333817 -4.3462133 -4.3535051][-4.2219229 -4.2263055 -4.2363915 -4.251564 -4.262361 -4.253335 -4.2299695 -4.21174 -4.2069025 -4.2224879 -4.2559142 -4.2832756 -4.3107471 -4.3297939 -4.3427758][-4.158957 -4.160799 -4.169117 -4.1860833 -4.198205 -4.180336 -4.1448326 -4.1182384 -4.1141815 -4.1440835 -4.198885 -4.2406163 -4.2779355 -4.3066211 -4.3276486][-4.1157036 -4.113019 -4.1148396 -4.1280737 -4.1358805 -4.1049037 -4.0528755 -4.0128202 -4.0060697 -4.0493841 -4.1305385 -4.1919732 -4.2424183 -4.2822161 -4.3121581][-4.10288 -4.091341 -4.0822883 -4.0893583 -4.0898385 -4.0458865 -3.9786763 -3.9248171 -3.9115412 -3.9643433 -4.0692949 -4.1528749 -4.216917 -4.2661762 -4.3035221][-4.0984225 -4.0811672 -4.0693483 -4.0736322 -4.0725031 -4.0293417 -3.9649236 -3.9135947 -3.90059 -3.952008 -4.050889 -4.1365705 -4.2052584 -4.2593675 -4.3016834][-4.0894089 -4.0676413 -4.05604 -4.0585704 -4.0589066 -4.0245004 -3.9738743 -3.940433 -3.9446373 -3.9926143 -4.0676141 -4.141654 -4.2069626 -4.2620029 -4.305738][-4.0904832 -4.0654349 -4.0552015 -4.0553551 -4.0580978 -4.0369854 -4.0022779 -3.9872072 -4.0070658 -4.0482664 -4.0992861 -4.1600966 -4.2179065 -4.2702231 -4.3112321][-4.1264138 -4.1119657 -4.1085014 -4.1086683 -4.1129794 -4.1019135 -4.0762367 -4.0722771 -4.0973325 -4.1263318 -4.1562953 -4.2008328 -4.2462287 -4.2879729 -4.32011][-4.1760488 -4.17206 -4.170857 -4.1646557 -4.1686487 -4.1628489 -4.1428747 -4.1459727 -4.171576 -4.1885147 -4.20442 -4.2368 -4.2711067 -4.303503 -4.3277435][-4.2227468 -4.2223907 -4.218864 -4.2054329 -4.2073164 -4.2044945 -4.1900463 -4.1988935 -4.22138 -4.2317705 -4.2421975 -4.2663865 -4.2926683 -4.3170319 -4.3350592][-4.2689505 -4.2672853 -4.2600126 -4.2440658 -4.2430515 -4.24428 -4.2370148 -4.2445617 -4.2594032 -4.2680392 -4.2769742 -4.2945285 -4.312633 -4.3297362 -4.3430991][-4.3138957 -4.3103375 -4.3023362 -4.2901978 -4.2880855 -4.2923479 -4.2910585 -4.2960672 -4.3042307 -4.3106565 -4.3168044 -4.3263636 -4.3358989 -4.3444467 -4.35162][-4.3479257 -4.3434381 -4.3365951 -4.3297038 -4.3292036 -4.3342576 -4.3355012 -4.3379292 -4.3412313 -4.3441334 -4.346879 -4.3494954 -4.3520703 -4.3547559 -4.3575392]]...]
INFO - root - 2017-12-06 03:56:24.966440: step 16010, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 20h:02m:04s remains)
INFO - root - 2017-12-06 03:56:27.302543: step 16020, loss = 2.08, batch loss = 2.02 (32.9 examples/sec; 0.243 sec/batch; 21h:22m:03s remains)
INFO - root - 2017-12-06 03:56:29.661138: step 16030, loss = 2.05, batch loss = 1.99 (32.8 examples/sec; 0.244 sec/batch; 21h:27m:15s remains)
INFO - root - 2017-12-06 03:56:31.954606: step 16040, loss = 2.09, batch loss = 2.03 (35.2 examples/sec; 0.227 sec/batch; 19h:59m:08s remains)
INFO - root - 2017-12-06 03:56:34.240957: step 16050, loss = 2.06, batch loss = 2.00 (33.3 examples/sec; 0.240 sec/batch; 21h:06m:10s remains)
INFO - root - 2017-12-06 03:56:36.554703: step 16060, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 20h:01m:36s remains)
INFO - root - 2017-12-06 03:56:38.840817: step 16070, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:18m:19s remains)
INFO - root - 2017-12-06 03:56:41.202628: step 16080, loss = 2.05, batch loss = 1.99 (32.3 examples/sec; 0.247 sec/batch; 21h:45m:01s remains)
INFO - root - 2017-12-06 03:56:43.514688: step 16090, loss = 2.07, batch loss = 2.02 (34.1 examples/sec; 0.234 sec/batch; 20h:36m:10s remains)
INFO - root - 2017-12-06 03:56:45.804784: step 16100, loss = 2.04, batch loss = 1.98 (36.7 examples/sec; 0.218 sec/batch; 19h:09m:31s remains)
2017-12-06 03:56:46.079015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2401953 -4.2259216 -4.2020559 -4.1791382 -4.1692324 -4.1697454 -4.1796575 -4.2042327 -4.2373452 -4.2645254 -4.2786112 -4.2788892 -4.2614493 -4.2288408 -4.1973748][-4.2255487 -4.2065196 -4.1747689 -4.1401868 -4.1154189 -4.09875 -4.0981274 -4.12949 -4.1804314 -4.228478 -4.2570658 -4.2618937 -4.2420673 -4.2008643 -4.1530156][-4.2050004 -4.1844139 -4.1502619 -4.1033373 -4.0587268 -4.018487 -3.9992554 -4.0328722 -4.1028528 -4.1744437 -4.22158 -4.2359147 -4.2204056 -4.1772108 -4.1155186][-4.1899681 -4.1683922 -4.1371789 -4.0829344 -4.0182972 -3.9504814 -3.9042315 -3.9309604 -4.0168939 -4.1111274 -4.1798158 -4.208982 -4.2039905 -4.1674695 -4.0982509][-4.1912341 -4.1715355 -4.1478577 -4.0951095 -4.0159874 -3.9228706 -3.8453436 -3.8539805 -3.9442267 -4.0547128 -4.1415625 -4.1891637 -4.1996069 -4.1745791 -4.1040177][-4.2055793 -4.1908231 -4.1755314 -4.1318665 -4.052278 -3.9436653 -3.8373377 -3.8155363 -3.8955588 -4.0099292 -4.1076255 -4.1766596 -4.2082558 -4.1967368 -4.1322412][-4.2255459 -4.2177954 -4.2124496 -4.1820889 -4.112319 -4.0023651 -3.8807549 -3.8290873 -3.8885753 -3.9958255 -4.0978756 -4.184299 -4.2320156 -4.23187 -4.1790166][-4.241878 -4.2431712 -4.250195 -4.2378078 -4.1847081 -4.0860586 -3.9647853 -3.8911672 -3.9216263 -4.0105314 -4.1085577 -4.2056837 -4.264957 -4.2744632 -4.236136][-4.2418814 -4.2510743 -4.2687535 -4.273118 -4.2382936 -4.1568432 -4.0441017 -3.9558218 -3.9563603 -4.021771 -4.1092372 -4.2114625 -4.2817678 -4.303555 -4.2847748][-4.2197571 -4.2336206 -4.2605824 -4.2797222 -4.2644444 -4.2040062 -4.1081371 -4.0208912 -4.0015945 -4.044488 -4.1150422 -4.2092257 -4.2825384 -4.3132806 -4.3096113][-4.2025628 -4.2146816 -4.2462335 -4.2761178 -4.2790813 -4.2414451 -4.1699672 -4.0980382 -4.0740876 -4.1014538 -4.1536927 -4.2283063 -4.2925649 -4.3235726 -4.3263874][-4.2108045 -4.2169237 -4.2441788 -4.2743864 -4.2859244 -4.2655249 -4.215744 -4.1625943 -4.1419096 -4.1609445 -4.1989174 -4.2546263 -4.3044395 -4.329793 -4.332994][-4.2391663 -4.2401795 -4.2609134 -4.2876577 -4.3014464 -4.2927561 -4.2603197 -4.224844 -4.2115455 -4.2258377 -4.2524986 -4.2893481 -4.321641 -4.3352814 -4.3352365][-4.2838335 -4.2827463 -4.2967305 -4.3163977 -4.328011 -4.324986 -4.3062425 -4.2854972 -4.2782269 -4.28787 -4.303781 -4.3244214 -4.3408537 -4.3446712 -4.3411355][-4.32632 -4.3257675 -4.3333783 -4.3442378 -4.3500371 -4.3482647 -4.3385167 -4.3292074 -4.3272567 -4.3327684 -4.3396959 -4.3485889 -4.3553929 -4.354444 -4.3500891]]...]
INFO - root - 2017-12-06 03:56:48.354367: step 16110, loss = 2.06, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:31m:07s remains)
INFO - root - 2017-12-06 03:56:50.658844: step 16120, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 19h:52m:05s remains)
INFO - root - 2017-12-06 03:56:52.922541: step 16130, loss = 2.09, batch loss = 2.04 (35.6 examples/sec; 0.225 sec/batch; 19h:44m:00s remains)
INFO - root - 2017-12-06 03:56:55.273944: step 16140, loss = 2.09, batch loss = 2.04 (34.8 examples/sec; 0.230 sec/batch; 20h:12m:30s remains)
INFO - root - 2017-12-06 03:56:57.604773: step 16150, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 20h:13m:28s remains)
INFO - root - 2017-12-06 03:56:59.906050: step 16160, loss = 2.04, batch loss = 1.99 (33.6 examples/sec; 0.238 sec/batch; 20h:54m:26s remains)
INFO - root - 2017-12-06 03:57:02.232442: step 16170, loss = 2.06, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 20h:07m:53s remains)
INFO - root - 2017-12-06 03:57:04.527851: step 16180, loss = 2.05, batch loss = 1.99 (33.0 examples/sec; 0.243 sec/batch; 21h:18m:55s remains)
INFO - root - 2017-12-06 03:57:06.815750: step 16190, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.227 sec/batch; 19h:56m:15s remains)
INFO - root - 2017-12-06 03:57:09.170590: step 16200, loss = 2.05, batch loss = 1.99 (33.1 examples/sec; 0.241 sec/batch; 21h:12m:20s remains)
2017-12-06 03:57:09.468781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1209049 -4.12072 -4.1311579 -4.1470542 -4.1656532 -4.167201 -4.1520448 -4.130434 -4.111402 -4.1149497 -4.1179137 -4.117496 -4.1221294 -4.1343307 -4.14544][-4.1219997 -4.1116252 -4.1123505 -4.124681 -4.1497746 -4.1586456 -4.1536918 -4.1378584 -4.1247325 -4.1321731 -4.1363378 -4.1364355 -4.1414037 -4.1487327 -4.1545062][-4.122601 -4.1092553 -4.103807 -4.1097703 -4.1338315 -4.1466451 -4.1536307 -4.142477 -4.1318021 -4.1427736 -4.15551 -4.16228 -4.1701746 -4.1697645 -4.1655579][-4.1208997 -4.1120081 -4.1075954 -4.1121812 -4.1309295 -4.1404991 -4.14428 -4.1291595 -4.1176844 -4.1333404 -4.1573434 -4.172441 -4.1814103 -4.1765409 -4.1651416][-4.1182 -4.1150036 -4.1182175 -4.1256256 -4.1371918 -4.1365404 -4.1215019 -4.0935073 -4.0865717 -4.1118522 -4.1459103 -4.1674643 -4.1772122 -4.169055 -4.1547952][-4.1084256 -4.1096869 -4.1248064 -4.1414456 -4.1436353 -4.1197815 -4.07257 -4.0276752 -4.039886 -4.0886049 -4.1313362 -4.1581531 -4.1665311 -4.1563339 -4.1389289][-4.0918937 -4.0998368 -4.1277604 -4.1488037 -4.1366653 -4.0825014 -3.9963043 -3.9382179 -3.9847686 -4.0700641 -4.1267362 -4.1544981 -4.1625938 -4.1501894 -4.1343145][-4.0895915 -4.10515 -4.1327395 -4.148222 -4.1227441 -4.047965 -3.9441133 -3.8914006 -3.9665668 -4.0702143 -4.1282725 -4.1515031 -4.1556635 -4.1406689 -4.1266518][-4.0988922 -4.1193614 -4.1356983 -4.1354394 -4.1025681 -4.0340867 -3.95558 -3.9333611 -4.0058737 -4.0882649 -4.1273909 -4.1397009 -4.13878 -4.1218381 -4.1069741][-4.1194062 -4.1346087 -4.1353097 -4.122272 -4.0927238 -4.0439229 -4.0044703 -4.0127416 -4.0659313 -4.1145129 -4.1327195 -4.1362891 -4.13633 -4.1215811 -4.1072407][-4.145968 -4.154336 -4.1417084 -4.1220016 -4.0996957 -4.0736156 -4.0626373 -4.0845795 -4.1189775 -4.144702 -4.1530876 -4.15348 -4.1546822 -4.1437593 -4.1366405][-4.1586471 -4.1603556 -4.1419845 -4.1213412 -4.1123295 -4.1093655 -4.1162562 -4.1395488 -4.1594276 -4.1696091 -4.1699667 -4.1655068 -4.1632919 -4.155961 -4.1576409][-4.1707587 -4.1666422 -4.1471925 -4.13133 -4.1304283 -4.1392641 -4.1529813 -4.1725993 -4.1868792 -4.1920819 -4.1883988 -4.1806469 -4.1768575 -4.176363 -4.1821322][-4.1933994 -4.1832533 -4.1668344 -4.1570807 -4.1576128 -4.1666441 -4.1789541 -4.192399 -4.2036457 -4.2095213 -4.2072778 -4.2036862 -4.2029 -4.2046676 -4.2089896][-4.2040787 -4.194612 -4.1830573 -4.1786079 -4.1812387 -4.1878176 -4.1938753 -4.2005386 -4.2060652 -4.2102609 -4.2104845 -4.2118974 -4.2146039 -4.2159753 -4.2173252]]...]
INFO - root - 2017-12-06 03:57:11.788306: step 16210, loss = 2.06, batch loss = 2.01 (34.0 examples/sec; 0.235 sec/batch; 20h:41m:07s remains)
INFO - root - 2017-12-06 03:57:14.099540: step 16220, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 20h:17m:40s remains)
INFO - root - 2017-12-06 03:57:16.394826: step 16230, loss = 2.05, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 20h:16m:58s remains)
INFO - root - 2017-12-06 03:57:18.678413: step 16240, loss = 2.04, batch loss = 1.98 (36.2 examples/sec; 0.221 sec/batch; 19h:25m:58s remains)
INFO - root - 2017-12-06 03:57:21.002189: step 16250, loss = 2.06, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 20h:04m:25s remains)
INFO - root - 2017-12-06 03:57:23.285001: step 16260, loss = 2.03, batch loss = 1.98 (34.9 examples/sec; 0.229 sec/batch; 20h:09m:30s remains)
INFO - root - 2017-12-06 03:57:25.653181: step 16270, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:51m:11s remains)
INFO - root - 2017-12-06 03:57:27.966141: step 16280, loss = 2.04, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 19h:49m:22s remains)
INFO - root - 2017-12-06 03:57:30.291541: step 16290, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 19h:57m:08s remains)
INFO - root - 2017-12-06 03:57:32.565020: step 16300, loss = 2.07, batch loss = 2.01 (34.4 examples/sec; 0.232 sec/batch; 20h:25m:01s remains)
2017-12-06 03:57:32.855288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3028412 -4.2997103 -4.293335 -4.285522 -4.2787552 -4.2733035 -4.2720127 -4.2778888 -4.2883534 -4.30009 -4.3064947 -4.3014655 -4.2823405 -4.2631559 -4.2570939][-4.2746196 -4.2698145 -4.2631373 -4.2552471 -4.2461848 -4.2367282 -4.2304592 -4.2328415 -4.2456565 -4.2650518 -4.2763128 -4.2696843 -4.2431207 -4.2163482 -4.2090688][-4.2490525 -4.2468023 -4.2461219 -4.2405887 -4.2287 -4.2103658 -4.1931052 -4.1876507 -4.2029362 -4.2344551 -4.2549605 -4.2481437 -4.2125845 -4.1725163 -4.1586185][-4.2338052 -4.2350984 -4.2416983 -4.2406063 -4.225534 -4.1973586 -4.1648269 -4.1429648 -4.1590967 -4.2080183 -4.2467408 -4.2478209 -4.2110829 -4.160224 -4.1343513][-4.2233086 -4.2288566 -4.2420173 -4.2454233 -4.2286959 -4.19213 -4.1395807 -4.0927348 -4.1049509 -4.1738396 -4.2369781 -4.2557077 -4.228991 -4.1754107 -4.1357317][-4.2142529 -4.2265353 -4.2459588 -4.2528939 -4.23274 -4.1870565 -4.11507 -4.0413456 -4.0450487 -4.1330471 -4.2229242 -4.2657371 -4.2576232 -4.2105751 -4.1613941][-4.1972065 -4.217834 -4.2405162 -4.2447824 -4.2202682 -4.1661172 -4.0790462 -3.9821827 -3.9709961 -4.0715685 -4.186604 -4.2571969 -4.2723436 -4.2391639 -4.1875658][-4.1659274 -4.1940427 -4.2258472 -4.236691 -4.2157559 -4.158145 -4.0665846 -3.9565547 -3.9217412 -4.0123234 -4.1396236 -4.2324924 -4.268033 -4.2502112 -4.2029161][-4.1446118 -4.1712985 -4.2099819 -4.2322493 -4.2256861 -4.1833982 -4.1097469 -4.0140996 -3.9653966 -4.01918 -4.1278186 -4.22072 -4.2597942 -4.2491379 -4.2067385][-4.1427164 -4.1577859 -4.1947393 -4.2266645 -4.2389183 -4.2200961 -4.172709 -4.1034808 -4.0557394 -4.0734129 -4.1476107 -4.2223296 -4.2516551 -4.2413754 -4.204546][-4.1646056 -4.165256 -4.1914673 -4.225225 -4.2511563 -4.2551708 -4.2338166 -4.1888127 -4.1484227 -4.1430378 -4.1812229 -4.231895 -4.25413 -4.2489705 -4.2236691][-4.2022386 -4.1911964 -4.2054811 -4.234642 -4.2637239 -4.2797427 -4.2776456 -4.2512794 -4.2204523 -4.2054172 -4.2184896 -4.2473946 -4.2633066 -4.2627406 -4.2502003][-4.2445526 -4.2288733 -4.2317286 -4.2503371 -4.2737012 -4.2916732 -4.3005247 -4.2919312 -4.2748022 -4.2614169 -4.2596087 -4.2724919 -4.2814817 -4.2832193 -4.278173][-4.2806606 -4.2665205 -4.2627926 -4.270844 -4.283535 -4.2967577 -4.3082671 -4.3126388 -4.3102303 -4.3036346 -4.2975922 -4.3011074 -4.3051295 -4.306272 -4.3035073][-4.3031459 -4.2958369 -4.2920403 -4.292809 -4.2958579 -4.302424 -4.3118663 -4.3193941 -4.3233924 -4.3215375 -4.3169971 -4.3173795 -4.3196297 -4.3207417 -4.3193932]]...]
INFO - root - 2017-12-06 03:57:35.151058: step 16310, loss = 2.08, batch loss = 2.03 (35.8 examples/sec; 0.223 sec/batch; 19h:35m:59s remains)
INFO - root - 2017-12-06 03:57:37.468021: step 16320, loss = 2.05, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:01m:31s remains)
INFO - root - 2017-12-06 03:57:39.792605: step 16330, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:33m:08s remains)
INFO - root - 2017-12-06 03:57:42.104997: step 16340, loss = 2.07, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 20h:27m:41s remains)
INFO - root - 2017-12-06 03:57:44.407602: step 16350, loss = 2.08, batch loss = 2.02 (34.2 examples/sec; 0.234 sec/batch; 20h:31m:32s remains)
INFO - root - 2017-12-06 03:57:46.727417: step 16360, loss = 2.10, batch loss = 2.04 (34.8 examples/sec; 0.230 sec/batch; 20h:10m:00s remains)
INFO - root - 2017-12-06 03:57:49.021368: step 16370, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 20h:15m:41s remains)
INFO - root - 2017-12-06 03:57:51.353013: step 16380, loss = 2.06, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 20h:23m:04s remains)
INFO - root - 2017-12-06 03:57:53.674866: step 16390, loss = 2.06, batch loss = 2.00 (33.7 examples/sec; 0.237 sec/batch; 20h:50m:31s remains)
INFO - root - 2017-12-06 03:57:55.977095: step 16400, loss = 2.05, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 20h:00m:57s remains)
2017-12-06 03:57:56.253984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1679668 -4.1622725 -4.1500278 -4.12772 -4.0872288 -4.0392694 -4.0171371 -4.0424604 -4.0856361 -4.1364245 -4.1882887 -4.2211885 -4.2419324 -4.2645493 -4.29997][-4.1672254 -4.1605735 -4.1483679 -4.1270838 -4.0860524 -4.0393081 -4.0154362 -4.0389214 -4.0786729 -4.1301055 -4.1830068 -4.2159233 -4.2351289 -4.257576 -4.2958374][-4.153698 -4.144187 -4.142231 -4.1357937 -4.107502 -4.0600109 -4.0265808 -4.03789 -4.0651784 -4.1152143 -4.1697721 -4.2056355 -4.230432 -4.2570796 -4.2957377][-4.1200695 -4.1091013 -4.120738 -4.1373553 -4.1225619 -4.0676436 -4.0149732 -4.009407 -4.0341377 -4.0935187 -4.1544447 -4.1971221 -4.2303686 -4.260869 -4.2973776][-4.0810332 -4.0700502 -4.0866251 -4.1122732 -4.1030269 -4.0303354 -3.9423118 -3.9282215 -3.9781308 -4.0657554 -4.139946 -4.1883521 -4.2287035 -4.2604814 -4.2963562][-4.046999 -4.0395088 -4.05471 -4.0737357 -4.0475349 -3.9343791 -3.7945697 -3.7817204 -3.8875213 -4.0247765 -4.1217 -4.1785564 -4.2304058 -4.2651429 -4.299386][-4.0294061 -4.0285211 -4.0368195 -4.0369334 -3.9811754 -3.8169539 -3.6274397 -3.6396601 -3.8199356 -4.0013065 -4.1183524 -4.17983 -4.2354631 -4.2711473 -4.3032646][-4.04623 -4.0509391 -4.0520558 -4.0364337 -3.9638948 -3.7916512 -3.6200969 -3.6687379 -3.8666139 -4.0455585 -4.1485095 -4.20088 -4.246798 -4.2751441 -4.3043242][-4.0946813 -4.1027179 -4.0992031 -4.0797038 -4.0177603 -3.9041038 -3.812995 -3.8585968 -3.988205 -4.1091795 -4.1808686 -4.2212725 -4.2587509 -4.2802877 -4.3056293][-4.1514411 -4.1590252 -4.1517415 -4.1286383 -4.0815258 -4.0168471 -3.9748383 -4.0061126 -4.0754719 -4.1490664 -4.2023673 -4.2393479 -4.2729044 -4.2864337 -4.3068433][-4.1788592 -4.1863732 -4.1768413 -4.1556177 -4.1247153 -4.0905285 -4.0703273 -4.0871792 -4.1214342 -4.1727538 -4.2193594 -4.2551479 -4.2816215 -4.2890005 -4.3062682][-4.1718597 -4.1815081 -4.1741486 -4.15808 -4.1360378 -4.1125412 -4.0940757 -4.0974746 -4.1152177 -4.1571317 -4.2056518 -4.2491364 -4.27708 -4.2853746 -4.3049207][-4.1549969 -4.1623096 -4.1584706 -4.1495843 -4.1333609 -4.1113334 -4.091845 -4.0917125 -4.1059551 -4.1449366 -4.198874 -4.2504754 -4.2805772 -4.2901139 -4.3089533][-4.1321106 -4.1324434 -4.1322422 -4.1375728 -4.1337934 -4.1177287 -4.0991411 -4.0992255 -4.1174498 -4.1598105 -4.2172151 -4.2664218 -4.2916589 -4.2997475 -4.3165216][-4.1343532 -4.1311994 -4.1353092 -4.1502352 -4.1571727 -4.1490107 -4.1353359 -4.1375542 -4.1594038 -4.2009215 -4.2522411 -4.2889719 -4.3050122 -4.3104529 -4.32418]]...]
INFO - root - 2017-12-06 03:57:58.542799: step 16410, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 19h:41m:30s remains)
INFO - root - 2017-12-06 03:58:00.800586: step 16420, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.226 sec/batch; 19h:47m:59s remains)
INFO - root - 2017-12-06 03:58:03.090180: step 16430, loss = 2.04, batch loss = 1.98 (35.4 examples/sec; 0.226 sec/batch; 19h:48m:58s remains)
INFO - root - 2017-12-06 03:58:05.419464: step 16440, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:11m:06s remains)
INFO - root - 2017-12-06 03:58:07.716878: step 16450, loss = 2.07, batch loss = 2.01 (34.4 examples/sec; 0.232 sec/batch; 20h:23m:22s remains)
INFO - root - 2017-12-06 03:58:10.038493: step 16460, loss = 2.08, batch loss = 2.02 (33.9 examples/sec; 0.236 sec/batch; 20h:41m:23s remains)
INFO - root - 2017-12-06 03:58:12.371415: step 16470, loss = 2.05, batch loss = 1.99 (34.1 examples/sec; 0.235 sec/batch; 20h:35m:51s remains)
INFO - root - 2017-12-06 03:58:14.714686: step 16480, loss = 2.07, batch loss = 2.01 (33.4 examples/sec; 0.239 sec/batch; 21h:00m:58s remains)
INFO - root - 2017-12-06 03:58:17.028291: step 16490, loss = 2.07, batch loss = 2.01 (33.6 examples/sec; 0.238 sec/batch; 20h:55m:42s remains)
INFO - root - 2017-12-06 03:58:19.334150: step 16500, loss = 2.10, batch loss = 2.04 (34.1 examples/sec; 0.234 sec/batch; 20h:34m:36s remains)
2017-12-06 03:58:19.648947: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2249465 -4.2235551 -4.2175164 -4.2111254 -4.2101965 -4.2154746 -4.2271814 -4.2428341 -4.2559075 -4.2628651 -4.2646456 -4.2652235 -4.2632184 -4.2569733 -4.2485719][-4.2113757 -4.2158737 -4.208333 -4.1952262 -4.1859374 -4.1820016 -4.1865587 -4.1995478 -4.2144237 -4.2270017 -4.2372956 -4.2484975 -4.2548146 -4.2544661 -4.247808][-4.1824541 -4.1930394 -4.1837444 -4.1626539 -4.1428595 -4.1264343 -4.1191425 -4.1234651 -4.1358328 -4.1537523 -4.1749783 -4.1990056 -4.2175188 -4.2279291 -4.2239757][-4.1342416 -4.150991 -4.1441207 -4.122014 -4.1005592 -4.0797482 -4.0646586 -4.0605321 -4.06317 -4.0743761 -4.0945578 -4.1211743 -4.1498184 -4.174139 -4.17659][-4.0838757 -4.1038728 -4.1024609 -4.0872688 -4.0732565 -4.0590248 -4.0446033 -4.0361147 -4.021656 -4.0145144 -4.0206118 -4.0367594 -4.064815 -4.0986214 -4.1155858][-4.0567856 -4.0738153 -4.0744128 -4.0657024 -4.058393 -4.0479894 -4.0343022 -4.02361 -3.9938824 -3.9680815 -3.9590094 -3.9597287 -3.9787498 -4.0131345 -4.0422421][-4.046742 -4.0588045 -4.0626388 -4.0605278 -4.058537 -4.0514555 -4.0407643 -4.0290751 -3.9893632 -3.9492598 -3.9262259 -3.9088874 -3.9118991 -3.9403329 -3.9774549][-4.0457592 -4.0503945 -4.0517883 -4.0531368 -4.0545635 -4.052516 -4.0530591 -4.0489979 -4.0161362 -3.9802186 -3.9564867 -3.9347391 -3.9291925 -3.9423022 -3.9704504][-4.0556183 -4.0521965 -4.046948 -4.0465474 -4.0449047 -4.0434442 -4.0531054 -4.0614161 -4.0476313 -4.025743 -4.0113163 -3.9992726 -3.9996395 -3.9993353 -4.0009565][-4.0723987 -4.0634866 -4.05467 -4.0519185 -4.0416889 -4.0335941 -4.0432906 -4.0580864 -4.0607095 -4.0522842 -4.0492244 -4.0484357 -4.0536742 -4.0471196 -4.0331631][-4.0860505 -4.0762191 -4.0675359 -4.0634241 -4.0485973 -4.0345759 -4.0369992 -4.0498009 -4.0567145 -4.0546279 -4.0587144 -4.0694418 -4.081881 -4.0762024 -4.0623107][-4.0989714 -4.0972371 -4.0934486 -4.0906482 -4.0724673 -4.0531459 -4.0476875 -4.0544004 -4.0604482 -4.0600257 -4.0667677 -4.0847459 -4.1039844 -4.1058726 -4.10124][-4.10886 -4.1180415 -4.1174221 -4.1133766 -4.0963845 -4.0763574 -4.0682287 -4.0707836 -4.0722857 -4.0718503 -4.0791473 -4.0979548 -4.1186256 -4.1299672 -4.1426072][-4.1125236 -4.1230464 -4.1188903 -4.1136336 -4.1021004 -4.087575 -4.0804353 -4.0826015 -4.0833673 -4.0837941 -4.0910077 -4.1089125 -4.127502 -4.1452527 -4.1733027][-4.104455 -4.1107216 -4.10047 -4.0944586 -4.095942 -4.0946708 -4.0901189 -4.0935674 -4.1009655 -4.1040354 -4.1102037 -4.1227536 -4.1334662 -4.15186 -4.18586]]...]
INFO - root - 2017-12-06 03:58:21.941184: step 16510, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:50m:44s remains)
INFO - root - 2017-12-06 03:58:24.287286: step 16520, loss = 2.08, batch loss = 2.02 (32.8 examples/sec; 0.244 sec/batch; 21h:23m:57s remains)
INFO - root - 2017-12-06 03:58:26.602128: step 16530, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 19h:57m:52s remains)
INFO - root - 2017-12-06 03:58:28.893085: step 16540, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 19h:58m:38s remains)
INFO - root - 2017-12-06 03:58:31.186926: step 16550, loss = 2.05, batch loss = 1.99 (35.8 examples/sec; 0.223 sec/batch; 19h:35m:42s remains)
INFO - root - 2017-12-06 03:58:33.445401: step 16560, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:40m:48s remains)
INFO - root - 2017-12-06 03:58:35.732235: step 16570, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 20h:15m:46s remains)
INFO - root - 2017-12-06 03:58:38.019351: step 16580, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.223 sec/batch; 19h:36m:30s remains)
INFO - root - 2017-12-06 03:58:40.346524: step 16590, loss = 2.08, batch loss = 2.02 (34.6 examples/sec; 0.231 sec/batch; 20h:17m:20s remains)
INFO - root - 2017-12-06 03:58:42.644864: step 16600, loss = 2.04, batch loss = 1.98 (35.3 examples/sec; 0.227 sec/batch; 19h:53m:25s remains)
2017-12-06 03:58:42.921651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0504317 -4.0521092 -4.0454903 -4.0136871 -4.0217414 -4.0889888 -4.1161952 -4.1165538 -4.127583 -4.1299868 -4.117991 -4.1216578 -4.1431432 -4.1575155 -4.1508021][-4.0680037 -4.0720468 -4.0630713 -4.026897 -4.026372 -4.0911736 -4.1150231 -4.1209636 -4.1359911 -4.1405559 -4.1309471 -4.1363091 -4.1578074 -4.1601424 -4.1407771][-4.0747738 -4.0771985 -4.0700603 -4.0400677 -4.0361233 -4.0913997 -4.1060658 -4.1088614 -4.1245794 -4.1363134 -4.137691 -4.1502471 -4.1680794 -4.1630383 -4.1405468][-4.0635653 -4.0593853 -4.0559964 -4.030664 -4.0227823 -4.0643148 -4.0753675 -4.0840769 -4.1036868 -4.1200161 -4.131649 -4.1478052 -4.1579003 -4.1411753 -4.1193128][-4.04592 -4.0296984 -4.0230908 -3.9978571 -3.9920125 -4.0238342 -4.0214448 -4.0354843 -4.0720692 -4.0988655 -4.1149073 -4.1426468 -4.1567006 -4.1312265 -4.1073403][-4.0288897 -3.9964626 -3.9730723 -3.9364121 -3.9231791 -3.92388 -3.8843658 -3.9032497 -3.9888542 -4.0542417 -4.0891027 -4.1308494 -4.1587148 -4.1484604 -4.1347194][-4.0251489 -3.9727697 -3.9312022 -3.8769598 -3.8374422 -3.7801051 -3.670326 -3.6966662 -3.8646383 -3.9907179 -4.0507197 -4.1097422 -4.1600685 -4.1728816 -4.1741562][-4.0339785 -3.9789758 -3.9371545 -3.8855386 -3.8379982 -3.7579024 -3.6127815 -3.6236582 -3.8220041 -3.9724519 -4.0384097 -4.1012335 -4.1601162 -4.1790056 -4.1803622][-4.0443511 -4.0003672 -3.9772854 -3.9428399 -3.912951 -3.8709121 -3.7866979 -3.7845387 -3.9179034 -4.0335064 -4.075511 -4.1226397 -4.1781669 -4.1999068 -4.1953893][-4.0457268 -4.019434 -4.014451 -3.9882534 -3.9792843 -3.9717879 -3.9344373 -3.9305654 -4.019453 -4.1006174 -4.1232877 -4.1543522 -4.2014341 -4.221067 -4.2146921][-4.0220814 -4.0065703 -4.0129075 -3.9982979 -4.0108929 -4.0303707 -4.0176663 -4.0106521 -4.0736251 -4.1305332 -4.1431913 -4.16561 -4.2103996 -4.2259011 -4.214756][-3.9971137 -3.9842119 -3.9977944 -3.9945314 -4.0284834 -4.0710187 -4.0754223 -4.0709028 -4.1154742 -4.1556525 -4.1599417 -4.1756048 -4.2079964 -4.2085524 -4.1916342][-4.0020595 -3.9913568 -4.0066786 -4.0091815 -4.0490251 -4.102354 -4.120635 -4.1156292 -4.1377416 -4.1587071 -4.1539755 -4.1611147 -4.1794043 -4.1744652 -4.1614494][-4.0370727 -4.0286474 -4.03783 -4.0403113 -4.0734859 -4.1251879 -4.1469369 -4.1405659 -4.1494451 -4.1548448 -4.1459322 -4.1494141 -4.1626129 -4.1593289 -4.157012][-4.1010265 -4.0970821 -4.0993505 -4.0976586 -4.1195464 -4.1584182 -4.1746483 -4.1641669 -4.1609197 -4.1566072 -4.1446242 -4.1433921 -4.1529312 -4.1551933 -4.1655068]]...]
INFO - root - 2017-12-06 03:58:45.172647: step 16610, loss = 2.05, batch loss = 1.99 (35.3 examples/sec; 0.226 sec/batch; 19h:51m:48s remains)
INFO - root - 2017-12-06 03:58:47.465531: step 16620, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 19h:09m:02s remains)
INFO - root - 2017-12-06 03:58:49.754661: step 16630, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 20h:08m:44s remains)
INFO - root - 2017-12-06 03:58:52.047274: step 16640, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:50m:02s remains)
INFO - root - 2017-12-06 03:58:54.345283: step 16650, loss = 2.07, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 20h:26m:32s remains)
INFO - root - 2017-12-06 03:58:56.632073: step 16660, loss = 2.05, batch loss = 2.00 (38.0 examples/sec; 0.210 sec/batch; 18h:27m:36s remains)
INFO - root - 2017-12-06 03:58:58.911699: step 16670, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:09m:48s remains)
INFO - root - 2017-12-06 03:59:01.227999: step 16680, loss = 2.05, batch loss = 1.99 (33.7 examples/sec; 0.238 sec/batch; 20h:50m:57s remains)
INFO - root - 2017-12-06 03:59:03.516805: step 16690, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 19h:57m:16s remains)
INFO - root - 2017-12-06 03:59:05.856276: step 16700, loss = 2.04, batch loss = 1.98 (34.2 examples/sec; 0.234 sec/batch; 20h:31m:34s remains)
2017-12-06 03:59:06.183665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1547546 -4.165174 -4.1743422 -4.1710634 -4.1527376 -4.132627 -4.1184974 -4.1290631 -4.1483188 -4.1525941 -4.14012 -4.09808 -4.0582504 -4.0502996 -4.0590177][-4.129343 -4.1455059 -4.1615477 -4.15664 -4.1266718 -4.0872855 -4.0573907 -4.0760145 -4.1127443 -4.130239 -4.1236868 -4.0755458 -4.029469 -4.018785 -4.0207114][-4.1182728 -4.1415687 -4.1622758 -4.1554422 -4.1109424 -4.0454121 -3.9907317 -4.0178552 -4.0731115 -4.1045666 -4.1018453 -4.0485172 -3.9972272 -3.9866531 -3.98683][-4.1315041 -4.1542468 -4.1711955 -4.1607275 -4.1019974 -4.0076509 -3.9247091 -3.965682 -4.0445976 -4.0904922 -4.0909424 -4.0374403 -3.9837492 -3.9717546 -3.9740541][-4.1433377 -4.1601243 -4.1698318 -4.1537738 -4.07709 -3.9509969 -3.8351772 -3.8956738 -4.0020823 -4.0638638 -4.0714631 -4.0257368 -3.9746647 -3.9632454 -3.969718][-4.130321 -4.1415672 -4.1467266 -4.1285939 -4.0387778 -3.8855729 -3.7427802 -3.8214254 -3.9567454 -4.0356703 -4.0527029 -4.0178614 -3.9708128 -3.9613693 -3.9739513][-4.1101327 -4.11732 -4.1197419 -4.1024232 -4.010623 -3.8497045 -3.6989112 -3.7871506 -3.9405596 -4.0284824 -4.0483961 -4.0178738 -3.9716184 -3.9631622 -3.9823945][-4.1196194 -4.1245146 -4.1309323 -4.1219049 -4.0442505 -3.9036074 -3.7748318 -3.8509758 -3.9889717 -4.068419 -4.0806632 -4.0481396 -4.0023561 -3.9897988 -4.0077748][-4.1433434 -4.1515226 -4.1640115 -4.1642041 -4.1050582 -3.9938476 -3.8941274 -3.9473565 -4.0507541 -4.1115894 -4.1146603 -4.0848742 -4.046021 -4.0331087 -4.0447865][-4.15979 -4.1684718 -4.178102 -4.1749268 -4.1319952 -4.0525851 -3.9815898 -4.0133572 -4.0856476 -4.1309733 -4.1348333 -4.1184821 -4.0949059 -4.0840793 -4.0864239][-4.173142 -4.1778407 -4.1795416 -4.1727476 -4.1456232 -4.0942349 -4.0478396 -4.0633969 -4.1154094 -4.1517539 -4.15955 -4.1557751 -4.1464343 -4.1356745 -4.1304889][-4.1864886 -4.1864266 -4.1817546 -4.1732869 -4.1598597 -4.1293678 -4.096889 -4.1015439 -4.1352015 -4.1594038 -4.1661572 -4.1680532 -4.1674566 -4.1619167 -4.1604457][-4.2008109 -4.1987567 -4.1926837 -4.1839838 -4.1766615 -4.1568084 -4.132822 -4.133532 -4.1550817 -4.1705351 -4.1748285 -4.1754351 -4.1753926 -4.1744165 -4.1796889][-4.2148261 -4.2138219 -4.2093239 -4.203876 -4.1989584 -4.1840067 -4.1665516 -4.1653371 -4.1812677 -4.1939578 -4.1958528 -4.1909375 -4.1865597 -4.1842341 -4.1916151][-4.2417412 -4.2397552 -4.2366214 -4.2337985 -4.2292047 -4.2181234 -4.205636 -4.2054939 -4.2193885 -4.2288051 -4.2255692 -4.2154016 -4.2091246 -4.2067366 -4.2161541]]...]
INFO - root - 2017-12-06 03:59:08.518997: step 16710, loss = 2.08, batch loss = 2.02 (31.4 examples/sec; 0.255 sec/batch; 22h:21m:47s remains)
INFO - root - 2017-12-06 03:59:10.821947: step 16720, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.224 sec/batch; 19h:41m:21s remains)
INFO - root - 2017-12-06 03:59:13.148781: step 16730, loss = 2.06, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 20h:04m:50s remains)
INFO - root - 2017-12-06 03:59:15.473913: step 16740, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 20h:10m:52s remains)
INFO - root - 2017-12-06 03:59:17.794177: step 16750, loss = 2.05, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 19h:56m:57s remains)
INFO - root - 2017-12-06 03:59:20.108511: step 16760, loss = 2.08, batch loss = 2.02 (34.6 examples/sec; 0.232 sec/batch; 20h:18m:26s remains)
INFO - root - 2017-12-06 03:59:22.391244: step 16770, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 19h:48m:59s remains)
INFO - root - 2017-12-06 03:59:24.719141: step 16780, loss = 2.07, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 19h:58m:53s remains)
INFO - root - 2017-12-06 03:59:27.057381: step 16790, loss = 2.10, batch loss = 2.04 (35.8 examples/sec; 0.223 sec/batch; 19h:35m:18s remains)
INFO - root - 2017-12-06 03:59:29.369841: step 16800, loss = 2.04, batch loss = 1.98 (34.8 examples/sec; 0.230 sec/batch; 20h:10m:53s remains)
2017-12-06 03:59:29.682445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2535229 -4.2680135 -4.2806516 -4.2899122 -4.2924638 -4.2852006 -4.2710757 -4.2563629 -4.2399135 -4.2351532 -4.246532 -4.2700162 -4.2996206 -4.3215709 -4.3395019][-4.2433982 -4.2657123 -4.2825651 -4.2947989 -4.2988696 -4.2909045 -4.2750916 -4.2581167 -4.2377267 -4.2305436 -4.2393808 -4.2624 -4.2923689 -4.317039 -4.3363819][-4.2417669 -4.2629871 -4.2781458 -4.2902403 -4.2966237 -4.2947912 -4.2870059 -4.2731724 -4.2514725 -4.2448249 -4.2520504 -4.2691669 -4.2923536 -4.3151312 -4.3338666][-4.23326 -4.2472258 -4.2589869 -4.2689958 -4.2753429 -4.2799153 -4.2806635 -4.2694349 -4.2477546 -4.2446046 -4.2536268 -4.2673607 -4.2863574 -4.3085828 -4.3278351][-4.2160969 -4.2210569 -4.228209 -4.233933 -4.23768 -4.2426333 -4.2460122 -4.2387056 -4.2234845 -4.22764 -4.2416868 -4.2561555 -4.2766657 -4.3006148 -4.3205705][-4.1907177 -4.1857495 -4.1845379 -4.1861253 -4.1849122 -4.1795616 -4.1739535 -4.1658468 -4.1619849 -4.1815119 -4.2122984 -4.240212 -4.2705474 -4.2977233 -4.3163724][-4.1521087 -4.1364031 -4.1250997 -4.1188583 -4.1072531 -4.0864143 -4.0693 -4.0610404 -4.0694542 -4.1105165 -4.167233 -4.2168827 -4.2628665 -4.2972188 -4.3166089][-4.1022792 -4.082788 -4.0688734 -4.0577593 -4.0347061 -4.0017567 -3.976845 -3.9735436 -3.9956303 -4.0545154 -4.1277676 -4.1917677 -4.2497416 -4.2911196 -4.3137927][-4.0536962 -4.040627 -4.0342817 -4.0257077 -4.0002241 -3.9655972 -3.9466457 -3.9558737 -3.9890192 -4.0503187 -4.1157665 -4.1748462 -4.2317114 -4.2756572 -4.3041396][-4.0350103 -4.0303607 -4.0313249 -4.0279541 -4.0101666 -3.9826081 -3.9722986 -3.9850397 -4.0104232 -4.0572863 -4.1057739 -4.1531482 -4.2051911 -4.2506204 -4.2865882][-4.0354943 -4.0299115 -4.0310392 -4.0323615 -4.0231442 -4.006732 -4.0063996 -4.018023 -4.0281277 -4.0526562 -4.0821095 -4.11907 -4.1694217 -4.2198629 -4.2654462][-4.0440273 -4.0352845 -4.0373077 -4.0436845 -4.0427094 -4.0353336 -4.0397 -4.0466218 -4.0395341 -4.0434561 -4.0604186 -4.09472 -4.1464643 -4.2032623 -4.2560725][-4.0665479 -4.0560341 -4.0582023 -4.06657 -4.0733066 -4.0744896 -4.0816011 -4.0875826 -4.0778265 -4.0777392 -4.0944448 -4.1307654 -4.1792312 -4.2294636 -4.2749][-4.113853 -4.1070008 -4.1128678 -4.1259689 -4.1409626 -4.1512961 -4.1630678 -4.1722288 -4.1681085 -4.1689243 -4.1828575 -4.2111263 -4.2442217 -4.2752914 -4.303688][-4.1951632 -4.1966147 -4.2050385 -4.2176 -4.2306676 -4.2394962 -4.24802 -4.2547235 -4.2524495 -4.2528048 -4.2604132 -4.2767739 -4.2940965 -4.3089271 -4.3236194]]...]
INFO - root - 2017-12-06 03:59:32.031420: step 16810, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.234 sec/batch; 20h:28m:47s remains)
INFO - root - 2017-12-06 03:59:34.364936: step 16820, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 19h:53m:20s remains)
INFO - root - 2017-12-06 03:59:36.660474: step 16830, loss = 2.04, batch loss = 1.98 (33.7 examples/sec; 0.237 sec/batch; 20h:49m:23s remains)
INFO - root - 2017-12-06 03:59:39.005712: step 16840, loss = 2.08, batch loss = 2.02 (32.6 examples/sec; 0.245 sec/batch; 21h:29m:44s remains)
INFO - root - 2017-12-06 03:59:41.284019: step 16850, loss = 2.05, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:31m:34s remains)
INFO - root - 2017-12-06 03:59:43.585004: step 16860, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 19h:44m:33s remains)
INFO - root - 2017-12-06 03:59:45.887135: step 16870, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 19h:17m:21s remains)
INFO - root - 2017-12-06 03:59:48.192712: step 16880, loss = 2.10, batch loss = 2.04 (33.9 examples/sec; 0.236 sec/batch; 20h:42m:16s remains)
INFO - root - 2017-12-06 03:59:50.473600: step 16890, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 19h:31m:24s remains)
INFO - root - 2017-12-06 03:59:52.821080: step 16900, loss = 2.07, batch loss = 2.01 (34.4 examples/sec; 0.233 sec/batch; 20h:23m:45s remains)
2017-12-06 03:59:53.175305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1296992 -4.129364 -4.1392426 -4.1613894 -4.1734939 -4.1635528 -4.1526995 -4.1499004 -4.1211786 -4.0800843 -4.0611224 -4.0704818 -4.1031389 -4.1419497 -4.1687894][-4.1164594 -4.1267977 -4.1389141 -4.1589193 -4.164278 -4.15162 -4.1400275 -4.1406631 -4.1159549 -4.0762482 -4.0609045 -4.0775027 -4.1188326 -4.165719 -4.1950221][-4.1089888 -4.1307983 -4.1388903 -4.1504197 -4.1527872 -4.1463184 -4.1400132 -4.1483073 -4.1363525 -4.1047511 -4.0879254 -4.0993037 -4.138711 -4.1827221 -4.2067761][-4.10383 -4.1301165 -4.1308179 -4.1397104 -4.1458759 -4.1475091 -4.1389961 -4.1450086 -4.1426854 -4.1243844 -4.1125965 -4.1191688 -4.1463246 -4.1768866 -4.1918306][-4.0841069 -4.1087518 -4.1040859 -4.1112924 -4.1186676 -4.1221943 -4.0966063 -4.0811996 -4.0964146 -4.1057968 -4.1096272 -4.1203313 -4.1392345 -4.1505413 -4.1517444][-4.0639963 -4.0859041 -4.0802469 -4.0818563 -4.0822139 -4.0651584 -3.9958537 -3.9259119 -3.9618535 -4.032063 -4.077023 -4.0974183 -4.1115146 -4.1138706 -4.1026864][-4.0678358 -4.0871482 -4.0810924 -4.0709276 -4.0551605 -4.0095487 -3.88684 -3.742867 -3.7743835 -3.9089837 -4.01516 -4.06251 -4.0826144 -4.0875726 -4.0789647][-4.0909986 -4.1046119 -4.1030288 -4.0934434 -4.0804152 -4.0389452 -3.9311793 -3.7898579 -3.7737722 -3.8862936 -4.0035253 -4.062551 -4.0814457 -4.0900388 -4.0882163][-4.1291203 -4.1362352 -4.1435466 -4.1423097 -4.1381159 -4.1203742 -4.06298 -3.9816103 -3.9546621 -4.00947 -4.0843496 -4.1212964 -4.1237888 -4.1209927 -4.1139903][-4.163373 -4.1662035 -4.1728439 -4.1767349 -4.1813941 -4.1782207 -4.1524506 -4.114821 -4.105906 -4.1344404 -4.175118 -4.1893239 -4.1721697 -4.1571279 -4.1446414][-4.1747351 -4.1776948 -4.1830215 -4.1911793 -4.2005739 -4.2017694 -4.1859832 -4.1683335 -4.1737604 -4.1975145 -4.222477 -4.2275019 -4.2084689 -4.1871333 -4.1773419][-4.1704736 -4.1721859 -4.1734228 -4.1781163 -4.1823158 -4.1810517 -4.1706405 -4.1620955 -4.1746192 -4.2023706 -4.2228861 -4.2298131 -4.2146349 -4.1997561 -4.1938581][-4.1814389 -4.1744041 -4.1690073 -4.1651073 -4.1596923 -4.1519208 -4.1422215 -4.1303816 -4.1409621 -4.1658387 -4.1837044 -4.1931586 -4.1839423 -4.17633 -4.1796784][-4.2206397 -4.2070265 -4.1939712 -4.1797147 -4.16239 -4.1471047 -4.1344714 -4.1173081 -4.1164012 -4.1223183 -4.1266341 -4.1350737 -4.1336875 -4.1325207 -4.139112][-4.2672553 -4.2540607 -4.2358828 -4.2123618 -4.1865969 -4.1662397 -4.1536422 -4.1364326 -4.1235342 -4.1113534 -4.0996108 -4.0953441 -4.091958 -4.0922709 -4.0978756]]...]
INFO - root - 2017-12-06 03:59:55.503182: step 16910, loss = 2.04, batch loss = 1.98 (34.4 examples/sec; 0.232 sec/batch; 20h:22m:30s remains)
INFO - root - 2017-12-06 03:59:57.793598: step 16920, loss = 2.08, batch loss = 2.02 (32.0 examples/sec; 0.250 sec/batch; 21h:53m:55s remains)
INFO - root - 2017-12-06 04:00:00.105475: step 16930, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:04m:28s remains)
INFO - root - 2017-12-06 04:00:02.425111: step 16940, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 19h:31m:02s remains)
INFO - root - 2017-12-06 04:00:04.729193: step 16950, loss = 2.07, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 19h:59m:18s remains)
INFO - root - 2017-12-06 04:00:07.012560: step 16960, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 19h:48m:04s remains)
INFO - root - 2017-12-06 04:00:09.328404: step 16970, loss = 2.05, batch loss = 1.99 (32.6 examples/sec; 0.245 sec/batch; 21h:28m:32s remains)
INFO - root - 2017-12-06 04:00:11.625235: step 16980, loss = 2.05, batch loss = 1.99 (36.2 examples/sec; 0.221 sec/batch; 19h:21m:01s remains)
INFO - root - 2017-12-06 04:00:13.925629: step 16990, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:09m:57s remains)
INFO - root - 2017-12-06 04:00:16.232756: step 17000, loss = 2.06, batch loss = 2.01 (35.3 examples/sec; 0.226 sec/batch; 19h:50m:57s remains)
2017-12-06 04:00:16.533450: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.203258 -4.2265439 -4.2271237 -4.2099128 -4.1897707 -4.1801548 -4.1720395 -4.1685257 -4.1733913 -4.1833768 -4.2074041 -4.2345243 -4.2441082 -4.2372723 -4.2382407][-4.2343917 -4.2519765 -4.24568 -4.2198615 -4.1950145 -4.1853027 -4.1828694 -4.1822348 -4.1883383 -4.200211 -4.2259579 -4.2538657 -4.2646122 -4.2555337 -4.2534595][-4.2259355 -4.2400823 -4.2296004 -4.1994581 -4.1730165 -4.1711464 -4.1828074 -4.1921916 -4.1983242 -4.2088566 -4.2320948 -4.2575293 -4.2682433 -4.2598958 -4.2611213][-4.1788249 -4.198432 -4.1907558 -4.1606231 -4.1361117 -4.1468935 -4.1751552 -4.1928434 -4.1976261 -4.2061539 -4.2237678 -4.2416492 -4.2524562 -4.2502031 -4.2581844][-4.10938 -4.1323328 -4.12988 -4.1051331 -4.0848341 -4.1014786 -4.1393194 -4.165287 -4.1718717 -4.1822 -4.1965733 -4.210721 -4.2239728 -4.2292781 -4.2438583][-4.0496531 -4.069685 -4.0678062 -4.0486994 -4.0338368 -4.0447831 -4.0779595 -4.1058769 -4.113368 -4.1284571 -4.1469245 -4.1649928 -4.1843982 -4.1987238 -4.2199578][-4.0140009 -4.023345 -4.0152726 -4.0006113 -3.9925802 -3.9947574 -4.011703 -4.0321174 -4.0370965 -4.0601864 -4.0918865 -4.1179194 -4.144002 -4.1688 -4.1985254][-4.0155249 -4.0111146 -3.9884176 -3.9655666 -3.9557726 -3.951375 -3.9557123 -3.9658296 -3.96895 -4.0005283 -4.0467253 -4.0789618 -4.1102114 -4.1487136 -4.1894383][-4.0473547 -4.032156 -4.0035114 -3.977124 -3.96423 -3.9572122 -3.9573283 -3.960264 -3.9600759 -3.9917712 -4.0414643 -4.0742064 -4.1047831 -4.1516562 -4.1998158][-4.0878272 -4.0671105 -4.0410652 -4.0177536 -4.0043154 -3.9985859 -4.0029411 -4.0067787 -4.0057778 -4.031034 -4.0751615 -4.1071596 -4.1351633 -4.1799445 -4.227098][-4.1430378 -4.1226907 -4.1022139 -4.0831451 -4.0692987 -4.0656638 -4.0742664 -4.0816426 -4.0827813 -4.1012597 -4.1358647 -4.16423 -4.187387 -4.2221522 -4.2584777][-4.1964822 -4.1824884 -4.1723175 -4.1628108 -4.1527495 -4.1512747 -4.162219 -4.1712141 -4.173492 -4.1847539 -4.2055984 -4.2235723 -4.2386289 -4.2610335 -4.2837405][-4.2316217 -4.2250891 -4.2240615 -4.2243977 -4.2205615 -4.221086 -4.2315397 -4.2392659 -4.2420788 -4.2472606 -4.2545867 -4.2611852 -4.2686138 -4.2818341 -4.29364][-4.2466364 -4.2440906 -4.249197 -4.2565517 -4.2582903 -4.2601829 -4.267303 -4.2719274 -4.2723346 -4.2723289 -4.2710447 -4.2710137 -4.2755904 -4.2844896 -4.2909293][-4.2507157 -4.250833 -4.2579627 -4.26635 -4.270926 -4.2739673 -4.2774725 -4.2780914 -4.2757463 -4.2719622 -4.265728 -4.26338 -4.2687426 -4.2775445 -4.2846885]]...]
INFO - root - 2017-12-06 04:00:18.836959: step 17010, loss = 2.07, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 20h:19m:32s remains)
INFO - root - 2017-12-06 04:00:21.124255: step 17020, loss = 2.08, batch loss = 2.02 (34.7 examples/sec; 0.231 sec/batch; 20h:13m:44s remains)
INFO - root - 2017-12-06 04:00:23.453653: step 17030, loss = 2.06, batch loss = 2.01 (33.1 examples/sec; 0.241 sec/batch; 21h:09m:08s remains)
INFO - root - 2017-12-06 04:00:25.756883: step 17040, loss = 2.06, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 19h:41m:16s remains)
INFO - root - 2017-12-06 04:00:28.079678: step 17050, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:46m:35s remains)
INFO - root - 2017-12-06 04:00:30.376485: step 17060, loss = 2.04, batch loss = 1.99 (34.3 examples/sec; 0.233 sec/batch; 20h:27m:21s remains)
INFO - root - 2017-12-06 04:00:32.640331: step 17070, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.224 sec/batch; 19h:40m:02s remains)
INFO - root - 2017-12-06 04:00:35.002388: step 17080, loss = 2.08, batch loss = 2.02 (32.4 examples/sec; 0.247 sec/batch; 21h:38m:41s remains)
INFO - root - 2017-12-06 04:00:37.340251: step 17090, loss = 2.09, batch loss = 2.03 (34.3 examples/sec; 0.234 sec/batch; 20h:27m:28s remains)
INFO - root - 2017-12-06 04:00:39.685062: step 17100, loss = 2.03, batch loss = 1.97 (35.4 examples/sec; 0.226 sec/batch; 19h:48m:29s remains)
2017-12-06 04:00:39.989683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2220197 -4.2243676 -4.2262278 -4.2275214 -4.2297144 -4.2319875 -4.2344518 -4.2364321 -4.2377005 -4.238236 -4.2384915 -4.2407575 -4.2445197 -4.2470922 -4.2460966][-4.2263913 -4.2316079 -4.2347093 -4.23722 -4.2413669 -4.2458305 -4.2502484 -4.2530909 -4.2535963 -4.2523937 -4.2513771 -4.2541184 -4.2598286 -4.2646103 -4.2660174][-4.2380924 -4.2428088 -4.2438335 -4.2458234 -4.2513623 -4.2582664 -4.2655163 -4.2709641 -4.273068 -4.2725048 -4.2719073 -4.2752309 -4.2808285 -4.2851806 -4.2871861][-4.2455964 -4.244441 -4.2384305 -4.2355576 -4.2395515 -4.2457719 -4.2550159 -4.2647653 -4.2736783 -4.2790437 -4.28117 -4.2855573 -4.2891397 -4.2915583 -4.293067][-4.2384868 -4.2293639 -4.2139425 -4.2041178 -4.2040524 -4.2083426 -4.2179203 -4.2291636 -4.2434978 -4.2547636 -4.2609558 -4.2669277 -4.2699952 -4.2715244 -4.2740493][-4.2046442 -4.1902022 -4.169508 -4.153564 -4.1461029 -4.1416459 -4.1473446 -4.1584482 -4.1761413 -4.1904416 -4.1989346 -4.208416 -4.2144108 -4.2185531 -4.2250752][-4.1544614 -4.1386681 -4.11528 -4.0929317 -4.0715542 -4.052608 -4.0507841 -4.0616536 -4.0867219 -4.1063886 -4.1170683 -4.1272764 -4.1364884 -4.1460667 -4.1601257][-4.1132817 -4.0981379 -4.0727563 -4.0446839 -4.0143094 -3.9870613 -3.9776256 -3.9901595 -4.0244532 -4.0499973 -4.0589147 -4.0640607 -4.0736656 -4.0907073 -4.1174831][-4.1178241 -4.1032562 -4.0801215 -4.0556421 -4.0315247 -4.01068 -4.0001841 -4.0090876 -4.039309 -4.0579472 -4.0549736 -4.0468445 -4.0507412 -4.0723886 -4.1102314][-4.1577497 -4.1459332 -4.1301985 -4.1188903 -4.1109576 -4.1052632 -4.1019125 -4.1060028 -4.1184559 -4.1180024 -4.0994225 -4.0770597 -4.0692425 -4.0858617 -4.1243672][-4.2162709 -4.2071624 -4.19623 -4.1942139 -4.1983314 -4.2029357 -4.2051067 -4.2078505 -4.208354 -4.1963391 -4.1710234 -4.1420026 -4.1241455 -4.1280985 -4.1535625][-4.2603326 -4.2525334 -4.2415123 -4.2396965 -4.2443204 -4.2508988 -4.2564912 -4.261591 -4.2608275 -4.2477131 -4.2241712 -4.1959267 -4.1728454 -4.1636267 -4.17081][-4.2538652 -4.2475033 -4.2362108 -4.2304282 -4.2292562 -4.2325988 -4.2383375 -4.2477388 -4.2526674 -4.2468386 -4.230741 -4.20737 -4.1839237 -4.1651492 -4.1601725][-4.2010069 -4.1990242 -4.1913733 -4.1833754 -4.17511 -4.1726441 -4.1768446 -4.1911769 -4.2050962 -4.2115984 -4.2073703 -4.191011 -4.1713085 -4.1509395 -4.1426249][-4.1518416 -4.1560183 -4.1551714 -4.1488066 -4.1375 -4.1305213 -4.1316485 -4.146914 -4.1661711 -4.1820354 -4.1890249 -4.1834669 -4.1736288 -4.1612363 -4.1561632]]...]
INFO - root - 2017-12-06 04:00:42.300066: step 17110, loss = 2.04, batch loss = 1.98 (36.2 examples/sec; 0.221 sec/batch; 19h:20m:22s remains)
INFO - root - 2017-12-06 04:00:44.547906: step 17120, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.226 sec/batch; 19h:46m:08s remains)
INFO - root - 2017-12-06 04:00:46.838409: step 17130, loss = 2.04, batch loss = 1.98 (35.1 examples/sec; 0.228 sec/batch; 19h:59m:10s remains)
INFO - root - 2017-12-06 04:00:49.146329: step 17140, loss = 2.09, batch loss = 2.03 (34.6 examples/sec; 0.231 sec/batch; 20h:14m:11s remains)
INFO - root - 2017-12-06 04:00:51.436271: step 17150, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:25m:04s remains)
INFO - root - 2017-12-06 04:00:53.759480: step 17160, loss = 2.07, batch loss = 2.01 (33.9 examples/sec; 0.236 sec/batch; 20h:41m:17s remains)
INFO - root - 2017-12-06 04:00:56.072790: step 17170, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.221 sec/batch; 19h:23m:14s remains)
INFO - root - 2017-12-06 04:00:58.358285: step 17180, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 19h:59m:14s remains)
INFO - root - 2017-12-06 04:01:00.683793: step 17190, loss = 2.05, batch loss = 1.99 (34.7 examples/sec; 0.230 sec/batch; 20h:11m:16s remains)
INFO - root - 2017-12-06 04:01:02.979942: step 17200, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:48m:17s remains)
2017-12-06 04:01:03.311038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2225623 -4.2228537 -4.2173324 -4.2078962 -4.1948485 -4.1818423 -4.1792393 -4.1898451 -4.2081575 -4.2254066 -4.2366786 -4.2445416 -4.25355 -4.2602067 -4.2634759][-4.2228718 -4.2211881 -4.2128043 -4.1984072 -4.1778755 -4.1571 -4.1515117 -4.1670613 -4.1956372 -4.2236462 -4.2431688 -4.2541561 -4.2615275 -4.2650228 -4.2658992][-4.2220583 -4.2188888 -4.20865 -4.1903548 -4.1628551 -4.1328177 -4.1199365 -4.1347361 -4.1705875 -4.2104111 -4.2409077 -4.2578735 -4.2660966 -4.2680058 -4.2675571][-4.2214828 -4.2176614 -4.2072811 -4.187777 -4.1560197 -4.1179037 -4.0948253 -4.1017523 -4.1387005 -4.1894011 -4.23138 -4.2560511 -4.2674818 -4.2693462 -4.2686505][-4.2194161 -4.2167997 -4.2080545 -4.1898751 -4.1569657 -4.1136293 -4.0798864 -4.0737209 -4.1063433 -4.165401 -4.2180705 -4.2513142 -4.2673063 -4.2701859 -4.26948][-4.2149296 -4.2150745 -4.2095022 -4.1948056 -4.163765 -4.1195769 -4.0795603 -4.0597768 -4.0832691 -4.1454773 -4.2065306 -4.2473693 -4.2671924 -4.2708254 -4.270143][-4.2155623 -4.2170568 -4.2124376 -4.2006912 -4.1731706 -4.1321278 -4.0912867 -4.0628691 -4.0751023 -4.1334105 -4.198226 -4.2451897 -4.2685413 -4.2726846 -4.2714438][-4.2229733 -4.2242017 -4.217629 -4.2069092 -4.1839242 -4.1473851 -4.1079273 -4.0768738 -4.0813937 -4.131536 -4.194 -4.2434154 -4.2696066 -4.2749662 -4.2733169][-4.2312384 -4.2332249 -4.2257352 -4.215405 -4.1965656 -4.16433 -4.126348 -4.0960765 -4.0966029 -4.136313 -4.1913424 -4.2401538 -4.2691965 -4.2768846 -4.2753344][-4.2380581 -4.2423077 -4.2363515 -4.2275958 -4.2125 -4.1839452 -4.1468754 -4.116344 -4.1112609 -4.1390681 -4.1849761 -4.231709 -4.2637343 -4.2754045 -4.2756643][-4.241878 -4.2481875 -4.2457786 -4.241221 -4.2316 -4.2092786 -4.1750412 -4.142396 -4.1280193 -4.1426082 -4.1788378 -4.2216716 -4.2554502 -4.270772 -4.273344][-4.2398267 -4.2482595 -4.2497368 -4.2502832 -4.2471542 -4.2335067 -4.2055693 -4.1734886 -4.1512041 -4.1535292 -4.1798558 -4.21751 -4.2493629 -4.2654643 -4.2697248][-4.2316718 -4.2426848 -4.2473645 -4.2513175 -4.2535048 -4.2484932 -4.2294388 -4.2029753 -4.1791215 -4.1731334 -4.1897039 -4.220479 -4.2481441 -4.2626605 -4.2673254][-4.2259297 -4.23803 -4.2430091 -4.2472768 -4.2517071 -4.2532377 -4.2439752 -4.2262907 -4.2070365 -4.1988711 -4.2079716 -4.2302403 -4.2519455 -4.2633486 -4.2669206][-4.2296724 -4.2385712 -4.2407093 -4.2427673 -4.2465568 -4.2505145 -4.2486148 -4.2397027 -4.2282348 -4.2234597 -4.2298179 -4.2451029 -4.2599053 -4.2668748 -4.2681189]]...]
INFO - root - 2017-12-06 04:01:05.592336: step 17210, loss = 2.04, batch loss = 1.98 (32.8 examples/sec; 0.244 sec/batch; 21h:21m:40s remains)
INFO - root - 2017-12-06 04:01:07.908978: step 17220, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 20h:13m:45s remains)
INFO - root - 2017-12-06 04:01:10.228148: step 17230, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 19h:44m:28s remains)
INFO - root - 2017-12-06 04:01:12.539929: step 17240, loss = 2.04, batch loss = 1.98 (34.1 examples/sec; 0.235 sec/batch; 20h:32m:46s remains)
INFO - root - 2017-12-06 04:01:14.887113: step 17250, loss = 2.10, batch loss = 2.04 (34.6 examples/sec; 0.231 sec/batch; 20h:13m:08s remains)
INFO - root - 2017-12-06 04:01:17.172342: step 17260, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 19h:46m:14s remains)
INFO - root - 2017-12-06 04:01:19.510341: step 17270, loss = 2.08, batch loss = 2.03 (34.3 examples/sec; 0.233 sec/batch; 20h:24m:10s remains)
INFO - root - 2017-12-06 04:01:21.762287: step 17280, loss = 2.05, batch loss = 1.99 (35.0 examples/sec; 0.228 sec/batch; 19h:59m:56s remains)
INFO - root - 2017-12-06 04:01:24.063307: step 17290, loss = 2.08, batch loss = 2.02 (32.2 examples/sec; 0.249 sec/batch; 21h:46m:57s remains)
INFO - root - 2017-12-06 04:01:26.388184: step 17300, loss = 2.05, batch loss = 1.99 (36.2 examples/sec; 0.221 sec/batch; 19h:20m:09s remains)
2017-12-06 04:01:26.709840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23112 -4.2451992 -4.2634635 -4.2711706 -4.2647705 -4.2631578 -4.2760339 -4.2877297 -4.28945 -4.2918792 -4.29398 -4.2883883 -4.2889819 -4.2919216 -4.2900414][-4.2498536 -4.2574058 -4.2710867 -4.2799406 -4.27753 -4.2772532 -4.2906566 -4.3010659 -4.2954869 -4.2868466 -4.2793379 -4.2666092 -4.2629294 -4.2615852 -4.2563438][-4.258811 -4.2604842 -4.2697911 -4.2778077 -4.277185 -4.2746162 -4.2863646 -4.2984481 -4.2933764 -4.2824364 -4.271348 -4.2533417 -4.2432728 -4.2349553 -4.2259412][-4.2535219 -4.2488036 -4.2531338 -4.2591782 -4.2578893 -4.25162 -4.2590494 -4.2736306 -4.2755594 -4.271266 -4.2651863 -4.2483559 -4.2355795 -4.2240272 -4.2138205][-4.2378016 -4.2259088 -4.2254944 -4.232049 -4.2332644 -4.2263379 -4.2280784 -4.2417049 -4.2512674 -4.25517 -4.2570233 -4.2466211 -4.23677 -4.2246428 -4.2125888][-4.2095857 -4.1901045 -4.1815076 -4.1835418 -4.1856909 -4.1813097 -4.1811066 -4.1934733 -4.2122736 -4.2267189 -4.2404518 -4.2427197 -4.2408271 -4.2318053 -4.2180767][-4.1745872 -4.1436515 -4.1207514 -4.109714 -4.1056404 -4.1020718 -4.1028666 -4.1145477 -4.1390462 -4.1624045 -4.1877704 -4.2050886 -4.2169337 -4.2194943 -4.2124639][-4.1481929 -4.1050491 -4.0664291 -4.0437522 -4.0312581 -4.0233235 -4.0247269 -4.0308814 -4.0542045 -4.0850716 -4.1278629 -4.1652837 -4.1943378 -4.2139573 -4.21971][-4.1374793 -4.0875297 -4.037077 -4.0041704 -3.9851241 -3.9743376 -3.978318 -3.9809318 -3.997869 -4.0335197 -4.0938458 -4.1500568 -4.19426 -4.2293587 -4.2479939][-4.1552086 -4.11071 -4.06164 -4.0297103 -4.0138226 -4.0023003 -4.006052 -4.0095429 -4.0181069 -4.0446463 -4.1015964 -4.1583366 -4.2047949 -4.2410831 -4.2637973][-4.1910968 -4.158782 -4.1231933 -4.1033912 -4.098455 -4.0922346 -4.0972438 -4.1053982 -4.1094985 -4.1209612 -4.1566734 -4.1987505 -4.2342362 -4.2582436 -4.2740526][-4.2184162 -4.1981597 -4.1744485 -4.1613479 -4.1619477 -4.1611552 -4.1730442 -4.1900783 -4.194448 -4.1961985 -4.211606 -4.232204 -4.249289 -4.2568536 -4.2634673][-4.2097192 -4.1981435 -4.1846185 -4.1788626 -4.1850948 -4.193419 -4.2134686 -4.2408991 -4.2488079 -4.2469554 -4.2490134 -4.2510934 -4.2498627 -4.242938 -4.2405128][-4.1741652 -4.170764 -4.1666975 -4.1650705 -4.1754022 -4.1915445 -4.2146158 -4.2462997 -4.2571721 -4.2579942 -4.2559323 -4.2500515 -4.2395935 -4.2248054 -4.2163863][-4.1307468 -4.1357684 -4.1370211 -4.1372366 -4.1465974 -4.1617594 -4.1838927 -4.215086 -4.2278004 -4.2301989 -4.2278256 -4.2195373 -4.2062259 -4.1905012 -4.1813664]]...]
INFO - root - 2017-12-06 04:01:29.047538: step 17310, loss = 2.08, batch loss = 2.02 (33.0 examples/sec; 0.242 sec/batch; 21h:11m:41s remains)
INFO - root - 2017-12-06 04:01:31.349612: step 17320, loss = 2.05, batch loss = 1.99 (35.3 examples/sec; 0.227 sec/batch; 19h:50m:16s remains)
INFO - root - 2017-12-06 04:01:33.606852: step 17330, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 19h:16m:01s remains)
INFO - root - 2017-12-06 04:01:35.911178: step 17340, loss = 2.03, batch loss = 1.98 (34.8 examples/sec; 0.230 sec/batch; 20h:08m:39s remains)
INFO - root - 2017-12-06 04:01:38.179573: step 17350, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.234 sec/batch; 20h:26m:49s remains)
INFO - root - 2017-12-06 04:01:40.499392: step 17360, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:06m:56s remains)
INFO - root - 2017-12-06 04:01:42.769955: step 17370, loss = 2.04, batch loss = 1.98 (36.5 examples/sec; 0.219 sec/batch; 19h:10m:30s remains)
INFO - root - 2017-12-06 04:01:45.066802: step 17380, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:33m:32s remains)
INFO - root - 2017-12-06 04:01:47.379113: step 17390, loss = 2.07, batch loss = 2.01 (32.2 examples/sec; 0.249 sec/batch; 21h:45m:51s remains)
INFO - root - 2017-12-06 04:01:49.657421: step 17400, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:13m:59s remains)
2017-12-06 04:01:49.966466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1596684 -4.1402755 -4.1162171 -4.0987477 -4.1142211 -4.1590395 -4.2069263 -4.2424865 -4.2703729 -4.28183 -4.2757573 -4.2669635 -4.2641826 -4.2648311 -4.2669225][-4.1748819 -4.1566734 -4.1324258 -4.115344 -4.1274233 -4.1673975 -4.2126145 -4.2456059 -4.2661223 -4.2743134 -4.268765 -4.2590446 -4.2554865 -4.2574081 -4.2595472][-4.2092419 -4.1898274 -4.1653647 -4.1512747 -4.1532907 -4.172431 -4.2022176 -4.2307858 -4.2468767 -4.2555132 -4.2512259 -4.2381167 -4.2347732 -4.2382741 -4.24268][-4.2244592 -4.202014 -4.1832519 -4.1709948 -4.1624579 -4.1588674 -4.1673374 -4.1864214 -4.2063451 -4.223578 -4.2251253 -4.2100773 -4.2017317 -4.2056551 -4.21558][-4.2189069 -4.1948457 -4.18126 -4.1688924 -4.1461124 -4.1197639 -4.1077929 -4.1213579 -4.1521964 -4.18085 -4.1899004 -4.1745839 -4.16142 -4.1658072 -4.1783128][-4.1971354 -4.1659775 -4.1484532 -4.1267939 -4.0841513 -4.027359 -3.9846346 -3.9999263 -4.0679245 -4.122129 -4.1438437 -4.127924 -4.1062841 -4.1060181 -4.1224685][-4.1721792 -4.1327262 -4.1043758 -4.0679703 -4.0023322 -3.9031341 -3.8090866 -3.8350914 -3.9702392 -4.0649886 -4.0978746 -4.0836749 -4.05674 -4.0489931 -4.0743537][-4.1608291 -4.1164818 -4.0811787 -4.04237 -3.9697409 -3.8376751 -3.6891866 -3.7240057 -3.9115739 -4.0247817 -4.062427 -4.0548339 -4.0384769 -4.0382314 -4.0718546][-4.1702204 -4.1332793 -4.1014686 -4.07708 -4.024457 -3.9109607 -3.78296 -3.818135 -3.9681089 -4.0533547 -4.0742221 -4.0591292 -4.0519238 -4.0645227 -4.0996871][-4.1837945 -4.1607037 -4.142673 -4.1355257 -4.1099443 -4.0407891 -3.9613476 -3.9825516 -4.0631633 -4.1079078 -4.109776 -4.0846639 -4.080864 -4.1057024 -4.1345048][-4.1836066 -4.1689348 -4.1658368 -4.1757231 -4.1699367 -4.13495 -4.0916739 -4.0992932 -4.1315517 -4.1487331 -4.140595 -4.1169043 -4.1194606 -4.1489439 -4.1672039][-4.1814566 -4.1714478 -4.1749773 -4.1934767 -4.2009969 -4.1897306 -4.1715517 -4.1701436 -4.1786237 -4.1817107 -4.1710529 -4.1575341 -4.1634865 -4.1867628 -4.1945891][-4.1847262 -4.1794152 -4.1822371 -4.1970973 -4.2095814 -4.2121677 -4.2064614 -4.2035947 -4.2065434 -4.2099991 -4.2070317 -4.2040648 -4.2093463 -4.2233615 -4.2247195][-4.2071452 -4.2046428 -4.2054396 -4.2141638 -4.2234812 -4.2308326 -4.2333965 -4.2321134 -4.2322912 -4.237504 -4.2421255 -4.2441473 -4.2484694 -4.253005 -4.2519093][-4.2411919 -4.2373395 -4.2351995 -4.2389936 -4.2430854 -4.2497864 -4.25793 -4.2576542 -4.2576933 -4.2639928 -4.2720866 -4.2764878 -4.279356 -4.2804103 -4.2774415]]...]
INFO - root - 2017-12-06 04:01:52.291497: step 17410, loss = 2.03, batch loss = 1.97 (32.9 examples/sec; 0.243 sec/batch; 21h:16m:32s remains)
INFO - root - 2017-12-06 04:01:54.610766: step 17420, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.223 sec/batch; 19h:32m:43s remains)
INFO - root - 2017-12-06 04:01:56.907595: step 17430, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:26m:35s remains)
INFO - root - 2017-12-06 04:01:59.186259: step 17440, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:28m:17s remains)
INFO - root - 2017-12-06 04:02:01.474921: step 17450, loss = 2.05, batch loss = 2.00 (33.0 examples/sec; 0.242 sec/batch; 21h:11m:13s remains)
INFO - root - 2017-12-06 04:02:03.818155: step 17460, loss = 2.06, batch loss = 2.00 (34.0 examples/sec; 0.235 sec/batch; 20h:34m:13s remains)
INFO - root - 2017-12-06 04:02:06.106028: step 17470, loss = 2.03, batch loss = 1.97 (33.7 examples/sec; 0.237 sec/batch; 20h:46m:55s remains)
INFO - root - 2017-12-06 04:02:08.389730: step 17480, loss = 2.08, batch loss = 2.02 (33.3 examples/sec; 0.241 sec/batch; 21h:02m:56s remains)
INFO - root - 2017-12-06 04:02:10.698994: step 17490, loss = 2.07, batch loss = 2.01 (34.4 examples/sec; 0.233 sec/batch; 20h:21m:42s remains)
INFO - root - 2017-12-06 04:02:13.008781: step 17500, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:06m:38s remains)
2017-12-06 04:02:13.287214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1880507 -4.1940994 -4.1935825 -4.1947389 -4.2014761 -4.19375 -4.1755919 -4.1673908 -4.1700392 -4.1774406 -4.1874938 -4.1942458 -4.1929216 -4.1860209 -4.1822343][-4.154511 -4.1646814 -4.1699529 -4.1780653 -4.1867142 -4.1725903 -4.1427369 -4.1198888 -4.1174788 -4.1320143 -4.1514344 -4.1623611 -4.1586661 -4.1461244 -4.1418047][-4.1161427 -4.1313858 -4.147758 -4.1667609 -4.18011 -4.1577873 -4.110404 -4.0686331 -4.0659804 -4.0939145 -4.1245046 -4.1398826 -4.13108 -4.1098695 -4.1016264][-4.0817432 -4.0999875 -4.1294184 -4.1586761 -4.1687088 -4.1335745 -4.0608692 -3.9959726 -3.9952953 -4.0443282 -4.0955114 -4.1227369 -4.1121359 -4.0840311 -4.0685844][-4.0760045 -4.0979042 -4.1337829 -4.1659751 -4.1662135 -4.1109428 -4.002686 -3.8998895 -3.9024937 -3.9912403 -4.07598 -4.1183777 -4.1082444 -4.0754595 -4.0574694][-4.0988197 -4.1227822 -4.1581616 -4.1838703 -4.1677284 -4.084579 -3.9297845 -3.7779717 -3.8044391 -3.9495897 -4.0651083 -4.1174688 -4.1089973 -4.0706296 -4.0496373][-4.1352706 -4.1590195 -4.1825228 -4.1895862 -4.1514053 -4.0353045 -3.8284039 -3.6345224 -3.7221694 -3.9278259 -4.0615315 -4.11804 -4.1122046 -4.0681019 -4.0434589][-4.1696019 -4.1953859 -4.20424 -4.1904755 -4.1333137 -3.9962122 -3.7745001 -3.5976934 -3.7426002 -3.9558778 -4.07623 -4.1230187 -4.1126127 -4.0623903 -4.0404148][-4.187007 -4.2080116 -4.2079592 -4.1828861 -4.1187654 -3.9989147 -3.8385196 -3.7561738 -3.8838983 -4.0319986 -4.1087742 -4.1362362 -4.1137319 -4.0585194 -4.040628][-4.1749611 -4.1938639 -4.1924715 -4.1631136 -4.1031733 -4.0221858 -3.9420507 -3.9335852 -4.0227375 -4.1053991 -4.1419253 -4.1478505 -4.1126137 -4.0582423 -4.04638][-4.1470318 -4.1634459 -4.1669555 -4.1431341 -4.0979271 -4.0533404 -4.0262752 -4.045228 -4.1026626 -4.1471233 -4.1615047 -4.1478043 -4.1046362 -4.0543685 -4.0499797][-4.1216021 -4.1371875 -4.1472287 -4.1374702 -4.1114335 -4.0907164 -4.0859232 -4.1076765 -4.1423187 -4.1622257 -4.1617351 -4.1404634 -4.1000452 -4.0604458 -4.0626578][-4.1091776 -4.1296186 -4.14534 -4.1453438 -4.1322708 -4.1253767 -4.1290193 -4.1442127 -4.1624985 -4.1650758 -4.1522713 -4.1306696 -4.0999904 -4.0763993 -4.0837569][-4.1239 -4.1429276 -4.1659265 -4.1759028 -4.1700354 -4.168097 -4.1773844 -4.1893458 -4.1972251 -4.1893334 -4.1695275 -4.1498966 -4.1325688 -4.1222363 -4.1297808][-4.1476288 -4.1649227 -4.1924181 -4.2091565 -4.2103209 -4.2127218 -4.2235184 -4.232635 -4.2322521 -4.2174582 -4.1983924 -4.1864629 -4.1812925 -4.1792927 -4.1846862]]...]
INFO - root - 2017-12-06 04:02:15.588943: step 17510, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:45m:31s remains)
INFO - root - 2017-12-06 04:02:17.902424: step 17520, loss = 2.03, batch loss = 1.98 (33.9 examples/sec; 0.236 sec/batch; 20h:38m:30s remains)
INFO - root - 2017-12-06 04:02:20.187344: step 17530, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:02m:40s remains)
INFO - root - 2017-12-06 04:02:22.485638: step 17540, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:26m:20s remains)
INFO - root - 2017-12-06 04:02:24.826747: step 17550, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 20h:38m:14s remains)
INFO - root - 2017-12-06 04:02:27.123044: step 17560, loss = 2.06, batch loss = 2.00 (34.0 examples/sec; 0.235 sec/batch; 20h:34m:40s remains)
INFO - root - 2017-12-06 04:02:29.450380: step 17570, loss = 2.08, batch loss = 2.02 (33.0 examples/sec; 0.242 sec/batch; 21h:11m:32s remains)
INFO - root - 2017-12-06 04:02:31.746659: step 17580, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.228 sec/batch; 19h:54m:09s remains)
INFO - root - 2017-12-06 04:02:34.009469: step 17590, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:08m:04s remains)
INFO - root - 2017-12-06 04:02:36.361466: step 17600, loss = 2.09, batch loss = 2.03 (35.3 examples/sec; 0.227 sec/batch; 19h:50m:43s remains)
2017-12-06 04:02:36.657450: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2023482 -4.2128959 -4.2249575 -4.234869 -4.2432942 -4.2488422 -4.2533946 -4.2549372 -4.2558637 -4.257864 -4.2595253 -4.2602735 -4.260241 -4.2596083 -4.258152][-4.1739497 -4.1901011 -4.2085028 -4.2239094 -4.2369123 -4.2447667 -4.2509851 -4.2558742 -4.2578831 -4.2569408 -4.2534151 -4.2478309 -4.2428393 -4.2426047 -4.2452407][-4.1688323 -4.1864471 -4.2049766 -4.221818 -4.2381358 -4.2481461 -4.25505 -4.2624383 -4.2681584 -4.2694693 -4.2659569 -4.2559705 -4.2451453 -4.2408581 -4.2418408][-4.1844988 -4.1987906 -4.2105327 -4.2234454 -4.2356753 -4.2429214 -4.2463508 -4.2516832 -4.2593956 -4.2644925 -4.2666135 -4.2556186 -4.2412605 -4.2369223 -4.2384038][-4.2116303 -4.2156343 -4.2132373 -4.212729 -4.2121248 -4.2092052 -4.2024884 -4.1983776 -4.2086377 -4.2241769 -4.2386775 -4.2380958 -4.2286067 -4.226346 -4.2247968][-4.2012296 -4.191504 -4.1736364 -4.1544414 -4.1371837 -4.1198716 -4.0948248 -4.0778852 -4.0955458 -4.1298347 -4.1663995 -4.1853848 -4.1845641 -4.1822648 -4.1732612][-4.139523 -4.1108046 -4.0786681 -4.0427127 -4.0032768 -3.9615467 -3.9075105 -3.8703976 -3.9009926 -3.9656694 -4.0289893 -4.0716219 -4.0809412 -4.0804253 -4.0677891][-4.070684 -4.027524 -3.9879138 -3.9446058 -3.8901036 -3.8297427 -3.7532051 -3.6999176 -3.7455244 -3.8317361 -3.9072611 -3.9590294 -3.9780231 -3.9862177 -3.9868267][-4.0536804 -4.0207415 -3.9935079 -3.9627657 -3.9209244 -3.8782377 -3.8267136 -3.7918394 -3.8273444 -3.8857055 -3.9328651 -3.9663725 -3.9802272 -3.9908876 -4.0017996][-4.0977173 -4.0854769 -4.074152 -4.0577216 -4.0355496 -4.0181541 -4.0001211 -3.9856155 -4.0054517 -4.0340543 -4.0563931 -4.0729561 -4.0786071 -4.0856614 -4.0947614][-4.1639214 -4.1644173 -4.1624031 -4.158257 -4.1518826 -4.1510596 -4.1528931 -4.1503792 -4.1584854 -4.1703453 -4.1814775 -4.1904459 -4.1911349 -4.1944175 -4.2009592][-4.228168 -4.2344394 -4.2374287 -4.2389579 -4.2394471 -4.2440581 -4.2503119 -4.2511134 -4.2548704 -4.2597003 -4.2647119 -4.2702212 -4.270618 -4.2718797 -4.2744656][-4.2534075 -4.260839 -4.2640038 -4.2649212 -4.2638788 -4.2653403 -4.2684827 -4.2700624 -4.2730007 -4.2755723 -4.2782412 -4.2824287 -4.2848468 -4.2860975 -4.2869453][-4.2421885 -4.2462435 -4.2465606 -4.2450657 -4.2418957 -4.2405906 -4.2418885 -4.24352 -4.24556 -4.2476292 -4.2498684 -4.2533097 -4.2558827 -4.2568398 -4.256866][-4.2082567 -4.2113833 -4.2105594 -4.2085977 -4.2062154 -4.2048693 -4.2052 -4.206007 -4.2072692 -4.2090693 -4.2109346 -4.2129064 -4.2143941 -4.2146912 -4.21434]]...]
INFO - root - 2017-12-06 04:02:38.998266: step 17610, loss = 2.08, batch loss = 2.02 (33.7 examples/sec; 0.237 sec/batch; 20h:46m:20s remains)
INFO - root - 2017-12-06 04:02:41.287774: step 17620, loss = 2.09, batch loss = 2.03 (35.5 examples/sec; 0.226 sec/batch; 19h:43m:44s remains)
INFO - root - 2017-12-06 04:02:43.568673: step 17630, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.234 sec/batch; 20h:29m:47s remains)
INFO - root - 2017-12-06 04:02:45.882797: step 17640, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:22m:12s remains)
INFO - root - 2017-12-06 04:02:48.163535: step 17650, loss = 2.06, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 19h:36m:43s remains)
INFO - root - 2017-12-06 04:02:50.436059: step 17660, loss = 2.06, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 20h:17m:43s remains)
INFO - root - 2017-12-06 04:02:52.726686: step 17670, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:55m:00s remains)
INFO - root - 2017-12-06 04:02:55.039262: step 17680, loss = 2.06, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:26m:31s remains)
INFO - root - 2017-12-06 04:02:57.353647: step 17690, loss = 2.05, batch loss = 1.99 (35.2 examples/sec; 0.227 sec/batch; 19h:51m:18s remains)
INFO - root - 2017-12-06 04:02:59.606079: step 17700, loss = 2.08, batch loss = 2.03 (35.8 examples/sec; 0.224 sec/batch; 19h:33m:27s remains)
2017-12-06 04:02:59.888561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1865582 -4.1828775 -4.1714635 -4.1624103 -4.1554651 -4.1472344 -4.1362886 -4.119029 -4.127305 -4.1532507 -4.1789927 -4.2070456 -4.2387347 -4.2656446 -4.2778773][-4.1895742 -4.1912651 -4.1840162 -4.1770382 -4.1745234 -4.1671968 -4.1534033 -4.1325941 -4.1338449 -4.1491904 -4.1703339 -4.2012472 -4.2345877 -4.2639 -4.2792072][-4.183918 -4.1878228 -4.1825995 -4.1750412 -4.1694717 -4.1616869 -4.1468105 -4.1284075 -4.1294689 -4.1438413 -4.1652369 -4.1975021 -4.2336268 -4.2669306 -4.2832355][-4.1789832 -4.1831126 -4.177238 -4.1681919 -4.1610274 -4.1521621 -4.1341472 -4.115622 -4.1203361 -4.1408696 -4.1664252 -4.1962862 -4.2292371 -4.2661691 -4.282865][-4.1696792 -4.1722388 -4.1633315 -4.1536717 -4.1450925 -4.1335297 -4.1129365 -4.0914817 -4.0968933 -4.1305103 -4.1633272 -4.18961 -4.2204733 -4.2618022 -4.2825336][-4.1603441 -4.1545639 -4.1405387 -4.1281295 -4.1109281 -4.0888133 -4.0621529 -4.0335822 -4.0450268 -4.1019192 -4.1531553 -4.1844993 -4.2193151 -4.2643881 -4.2863865][-4.1624069 -4.1491518 -4.1317873 -4.1096339 -4.0702705 -4.019331 -3.9715881 -3.9271474 -3.9418929 -4.0302014 -4.1102953 -4.1651659 -4.2163916 -4.2694798 -4.2919612][-4.1873832 -4.172852 -4.1569614 -4.1306262 -4.0758715 -4.0042253 -3.9270754 -3.8460248 -3.8374038 -3.9315844 -4.0325947 -4.1142759 -4.1894064 -4.2573967 -4.2875996][-4.2028875 -4.2011766 -4.2007947 -4.1856918 -4.1396427 -4.0773082 -3.9961431 -3.8943605 -3.8468616 -3.9026191 -3.9873486 -4.0666552 -4.1487746 -4.2292891 -4.2730389][-4.1795564 -4.1968908 -4.2178197 -4.2222424 -4.2000484 -4.1606069 -4.1016512 -4.0166044 -3.9652967 -3.9882188 -4.0407171 -4.0914078 -4.1489019 -4.2182479 -4.2638626][-4.1326466 -4.1642151 -4.1987939 -4.2180133 -4.21655 -4.2006383 -4.1670408 -4.1079607 -4.0708284 -4.089325 -4.1286697 -4.1647115 -4.1995978 -4.2466726 -4.2798424][-4.1232662 -4.15534 -4.1894407 -4.2092066 -4.2137985 -4.2081017 -4.1871486 -4.1454225 -4.1245179 -4.1454072 -4.1839957 -4.2197814 -4.2476177 -4.2795935 -4.2999029][-4.1691213 -4.1940627 -4.2167897 -4.2272668 -4.2286973 -4.2242827 -4.2062197 -4.1738024 -4.1597757 -4.1776142 -4.2104583 -4.2448287 -4.2699919 -4.29369 -4.3083816][-4.231483 -4.2503948 -4.2643452 -4.2691736 -4.2677021 -4.2627969 -4.2472563 -4.2213116 -4.2090974 -4.2191596 -4.2414346 -4.2677183 -4.2881932 -4.3061671 -4.317647][-4.2760582 -4.2908773 -4.3008618 -4.3049836 -4.3046675 -4.3013358 -4.29116 -4.2743535 -4.2648864 -4.267755 -4.2794423 -4.2962937 -4.3106527 -4.321178 -4.3270264]]...]
INFO - root - 2017-12-06 04:03:02.194894: step 17710, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 20h:05m:50s remains)
INFO - root - 2017-12-06 04:03:04.518566: step 17720, loss = 2.08, batch loss = 2.03 (35.0 examples/sec; 0.229 sec/batch; 19h:59m:58s remains)
INFO - root - 2017-12-06 04:03:06.805448: step 17730, loss = 2.06, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:34m:34s remains)
INFO - root - 2017-12-06 04:03:09.138759: step 17740, loss = 2.07, batch loss = 2.01 (34.0 examples/sec; 0.235 sec/batch; 20h:32m:53s remains)
INFO - root - 2017-12-06 04:03:11.481249: step 17750, loss = 2.09, batch loss = 2.03 (33.9 examples/sec; 0.236 sec/batch; 20h:37m:12s remains)
INFO - root - 2017-12-06 04:03:13.789221: step 17760, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:38m:22s remains)
INFO - root - 2017-12-06 04:03:16.070399: step 17770, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 19h:14m:40s remains)
INFO - root - 2017-12-06 04:03:18.351563: step 17780, loss = 2.05, batch loss = 1.99 (34.7 examples/sec; 0.231 sec/batch; 20h:10m:03s remains)
INFO - root - 2017-12-06 04:03:20.650416: step 17790, loss = 2.08, batch loss = 2.02 (34.3 examples/sec; 0.234 sec/batch; 20h:24m:48s remains)
INFO - root - 2017-12-06 04:03:22.958046: step 17800, loss = 2.04, batch loss = 1.98 (34.8 examples/sec; 0.230 sec/batch; 20h:06m:07s remains)
2017-12-06 04:03:23.271066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.313931 -4.2909079 -4.2624211 -4.2387691 -4.2328305 -4.244863 -4.2606163 -4.2723017 -4.2802243 -4.2861309 -4.2927036 -4.2991486 -4.30468 -4.3049932 -4.2990031][-4.3073845 -4.2775707 -4.2458882 -4.2270432 -4.2295213 -4.245851 -4.2594461 -4.2677708 -4.2747579 -4.2816329 -4.2884016 -4.2920508 -4.2922935 -4.2869577 -4.2764616][-4.3012567 -4.2712078 -4.2448206 -4.2356868 -4.2441826 -4.2567687 -4.260026 -4.2588005 -4.2611909 -4.2664204 -4.2725482 -4.2738733 -4.2696075 -4.261282 -4.2519221][-4.3000693 -4.2769375 -4.2599697 -4.2569075 -4.2622995 -4.2629781 -4.2508578 -4.236834 -4.2313881 -4.2359958 -4.2458744 -4.2508869 -4.2486353 -4.2443132 -4.2432728][-4.3042927 -4.2909141 -4.2830467 -4.2803264 -4.276958 -4.2621279 -4.2334414 -4.2000675 -4.1836429 -4.1947613 -4.2174268 -4.2326007 -4.2387404 -4.2445526 -4.2539887][-4.3092022 -4.3013725 -4.2969012 -4.2899327 -4.2750354 -4.2439771 -4.1967769 -4.1444025 -4.1275744 -4.1596584 -4.2008171 -4.225554 -4.2388349 -4.2538767 -4.2709966][-4.3030753 -4.2931361 -4.2860465 -4.272346 -4.2447529 -4.1999273 -4.1410031 -4.0859332 -4.0886493 -4.1457596 -4.1968746 -4.2235541 -4.2374959 -4.2538357 -4.2731886][-4.2914395 -4.2774339 -4.2673092 -4.249352 -4.2133241 -4.1622634 -4.1075177 -4.076086 -4.1034403 -4.1621189 -4.2050834 -4.2247849 -4.2325063 -4.2429476 -4.2625046][-4.2876387 -4.2711034 -4.2564421 -4.2339458 -4.1960096 -4.1507463 -4.119144 -4.1207104 -4.1522484 -4.1905832 -4.2160959 -4.2245088 -4.2211237 -4.223784 -4.2444744][-4.2874222 -4.2701449 -4.2502961 -4.2227573 -4.1888695 -4.1584835 -4.1542673 -4.1739621 -4.1969771 -4.213768 -4.2236948 -4.223762 -4.213181 -4.2090564 -4.2294044][-4.2876725 -4.2663221 -4.2399025 -4.2099185 -4.1841249 -4.1723928 -4.1871715 -4.2123137 -4.2267337 -4.2299862 -4.228992 -4.2243466 -4.2118454 -4.2041674 -4.22169][-4.2723784 -4.2469444 -4.2166386 -4.1885881 -4.1716957 -4.174448 -4.1990089 -4.2241068 -4.2353587 -4.2347612 -4.2294316 -4.2227268 -4.2122607 -4.2063136 -4.2196712][-4.2441168 -4.2202806 -4.1912127 -4.1686969 -4.1587176 -4.1680427 -4.1935329 -4.2162995 -4.227427 -4.2287774 -4.2238841 -4.2172418 -4.2093225 -4.2060294 -4.2152643][-4.2138071 -4.19624 -4.171967 -4.155303 -4.152 -4.1636229 -4.185185 -4.2041154 -4.21581 -4.2198048 -4.2162805 -4.2117944 -4.2068577 -4.2056971 -4.2126508][-4.19947 -4.1875768 -4.1712403 -4.160677 -4.163023 -4.1756649 -4.193429 -4.2077346 -4.2176652 -4.2211175 -4.2172709 -4.2143774 -4.2112341 -4.2105064 -4.2155209]]...]
INFO - root - 2017-12-06 04:03:25.594777: step 17810, loss = 2.07, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 20h:23m:07s remains)
INFO - root - 2017-12-06 04:03:27.879826: step 17820, loss = 2.06, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 19h:34m:25s remains)
INFO - root - 2017-12-06 04:03:30.169514: step 17830, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:22m:08s remains)
INFO - root - 2017-12-06 04:03:32.519594: step 17840, loss = 2.07, batch loss = 2.01 (33.1 examples/sec; 0.242 sec/batch; 21h:09m:13s remains)
INFO - root - 2017-12-06 04:03:34.815727: step 17850, loss = 2.06, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:36m:09s remains)
INFO - root - 2017-12-06 04:03:37.104271: step 17860, loss = 2.08, batch loss = 2.03 (33.6 examples/sec; 0.238 sec/batch; 20h:49m:32s remains)
INFO - root - 2017-12-06 04:03:39.384423: step 17870, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 19h:47m:45s remains)
INFO - root - 2017-12-06 04:03:41.661292: step 17880, loss = 2.07, batch loss = 2.01 (33.7 examples/sec; 0.237 sec/batch; 20h:43m:01s remains)
INFO - root - 2017-12-06 04:03:43.997612: step 17890, loss = 2.05, batch loss = 1.99 (34.4 examples/sec; 0.232 sec/batch; 20h:18m:53s remains)
INFO - root - 2017-12-06 04:03:46.279121: step 17900, loss = 2.06, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:26m:48s remains)
2017-12-06 04:03:46.568340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.197988 -4.221312 -4.2242575 -4.219245 -4.2143192 -4.2103996 -4.1965766 -4.1889253 -4.1852088 -4.1681142 -4.1631703 -4.173049 -4.1829629 -4.1821795 -4.1821876][-4.1755228 -4.2077503 -4.2205386 -4.2238822 -4.2266712 -4.2281904 -4.2192316 -4.2151175 -4.215354 -4.20126 -4.199841 -4.2171693 -4.2296963 -4.2306228 -4.2381859][-4.1659412 -4.20003 -4.2111216 -4.2158766 -4.2177992 -4.2235365 -4.2192788 -4.2151446 -4.2222276 -4.2195315 -4.223443 -4.2470384 -4.2624865 -4.2677617 -4.278862][-4.188756 -4.2083468 -4.2057714 -4.1998663 -4.1991425 -4.2092428 -4.2101264 -4.2092743 -4.2214885 -4.2262993 -4.2326722 -4.2622843 -4.2849689 -4.2964444 -4.3088355][-4.2239738 -4.2191525 -4.2021794 -4.1838431 -4.1816397 -4.1918845 -4.1930685 -4.1907043 -4.2008257 -4.2106552 -4.2226362 -4.260335 -4.2950358 -4.3137784 -4.32785][-4.2432165 -4.2153926 -4.1854439 -4.1583009 -4.1534581 -4.1558843 -4.1415992 -4.1245055 -4.1294394 -4.1487045 -4.1767488 -4.2321134 -4.28687 -4.3184004 -4.336338][-4.239532 -4.1970897 -4.1586347 -4.127131 -4.1146693 -4.099534 -4.0542812 -4.0083942 -4.0064149 -4.0407543 -4.095212 -4.1743212 -4.2522111 -4.3030615 -4.3282366][-4.2200747 -4.1766634 -4.1340718 -4.0950675 -4.0680666 -4.0297728 -3.9485655 -3.8677649 -3.858345 -3.91647 -4.0048838 -4.1081214 -4.2063985 -4.2744355 -4.3074574][-4.1790438 -4.14592 -4.1110282 -4.0704064 -4.0312858 -3.9815252 -3.8900156 -3.7946179 -3.7796698 -3.8502421 -3.9553049 -4.0656028 -4.1688905 -4.2435808 -4.2796845][-4.13313 -4.10947 -4.0898023 -4.0604763 -4.0259695 -3.9912257 -3.9321337 -3.8671217 -3.8541861 -3.9082828 -3.9912884 -4.0799685 -4.1674175 -4.2315049 -4.2596245][-4.1268816 -4.109025 -4.0998087 -4.0845838 -4.0665979 -4.0583444 -4.0433621 -4.019897 -4.0158958 -4.0492859 -4.0993271 -4.1550145 -4.2143021 -4.2559757 -4.2677684][-4.1746817 -4.1640148 -4.1616378 -4.1546984 -4.1478734 -4.1551361 -4.1642542 -4.16397 -4.165957 -4.184629 -4.2115493 -4.2411489 -4.2749147 -4.297328 -4.2962112][-4.2418146 -4.2368088 -4.2370634 -4.236567 -4.2355337 -4.246058 -4.2604022 -4.2680359 -4.2724586 -4.2792883 -4.2908845 -4.3036714 -4.31938 -4.3274803 -4.3195505][-4.2923818 -4.2916527 -4.2937241 -4.2968421 -4.2988358 -4.3069878 -4.3184605 -4.3271384 -4.3310671 -4.3326893 -4.33616 -4.3392968 -4.3441973 -4.3446946 -4.3361588][-4.320137 -4.3218126 -4.3243151 -4.3278947 -4.3303385 -4.3360243 -4.3442135 -4.3517275 -4.355207 -4.3551092 -4.3557591 -4.35607 -4.3566489 -4.355319 -4.3501191]]...]
INFO - root - 2017-12-06 04:03:48.909943: step 17910, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.235 sec/batch; 20h:30m:15s remains)
INFO - root - 2017-12-06 04:03:51.181186: step 17920, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.224 sec/batch; 19h:36m:40s remains)
INFO - root - 2017-12-06 04:03:53.524729: step 17930, loss = 2.05, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 20h:00m:28s remains)
INFO - root - 2017-12-06 04:03:55.826827: step 17940, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 19h:55m:03s remains)
INFO - root - 2017-12-06 04:03:58.121144: step 17950, loss = 2.05, batch loss = 1.99 (34.0 examples/sec; 0.235 sec/batch; 20h:33m:54s remains)
INFO - root - 2017-12-06 04:04:00.438475: step 17960, loss = 2.06, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:26m:46s remains)
INFO - root - 2017-12-06 04:04:02.774698: step 17970, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.235 sec/batch; 20h:30m:36s remains)
INFO - root - 2017-12-06 04:04:05.051848: step 17980, loss = 2.04, batch loss = 1.98 (34.7 examples/sec; 0.231 sec/batch; 20h:08m:44s remains)
INFO - root - 2017-12-06 04:04:07.384251: step 17990, loss = 2.08, batch loss = 2.02 (32.2 examples/sec; 0.249 sec/batch; 21h:42m:46s remains)
INFO - root - 2017-12-06 04:04:09.663681: step 18000, loss = 2.04, batch loss = 1.98 (35.3 examples/sec; 0.227 sec/batch; 19h:48m:28s remains)
2017-12-06 04:04:09.959054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3211923 -4.3017459 -4.2809186 -4.259913 -4.2382259 -4.2240334 -4.2236376 -4.2367654 -4.2571797 -4.2769661 -4.2906475 -4.2960434 -4.2960072 -4.2844644 -4.2643471][-4.315433 -4.2913175 -4.2631416 -4.2358351 -4.2112246 -4.1985097 -4.1985683 -4.2118125 -4.2382541 -4.262291 -4.2784896 -4.2817521 -4.2782431 -4.2624197 -4.2398157][-4.3107352 -4.2837195 -4.24949 -4.2155657 -4.1887112 -4.1757846 -4.1741505 -4.1874108 -4.215632 -4.2407594 -4.2606821 -4.2680225 -4.26492 -4.2482858 -4.2274389][-4.3060384 -4.2781096 -4.2407494 -4.2024961 -4.1758718 -4.164711 -4.163259 -4.1734557 -4.1962476 -4.2214484 -4.2465477 -4.2614837 -4.2609096 -4.2480869 -4.2312555][-4.3007493 -4.27365 -4.2392645 -4.204421 -4.1792774 -4.169456 -4.1665854 -4.1681213 -4.1793785 -4.2021117 -4.2328362 -4.2575703 -4.2618785 -4.2566619 -4.2466555][-4.2964354 -4.2704062 -4.2413821 -4.2133904 -4.1892171 -4.1778922 -4.1687617 -4.1569858 -4.1528172 -4.1712508 -4.208528 -4.2480612 -4.2618289 -4.2657628 -4.2611341][-4.2931166 -4.266665 -4.2394695 -4.2125506 -4.1882305 -4.1734085 -4.1547179 -4.1279387 -4.1134658 -4.1351 -4.1829743 -4.238832 -4.2642937 -4.2737737 -4.270288][-4.2916927 -4.2646065 -4.2361364 -4.2059937 -4.1766853 -4.1523223 -4.1242847 -4.0868049 -4.067986 -4.0991621 -4.1572766 -4.2242146 -4.2596483 -4.2726636 -4.2671857][-4.2935925 -4.2680306 -4.2399631 -4.2063837 -4.1692696 -4.1337237 -4.0955877 -4.0539684 -4.0362582 -4.0763 -4.1450562 -4.2146435 -4.2520957 -4.2637892 -4.2548809][-4.2974448 -4.2748146 -4.2486982 -4.2146111 -4.1738453 -4.1337671 -4.0909204 -4.0483127 -4.0346627 -4.0807776 -4.1561947 -4.2231612 -4.2560458 -4.2634768 -4.2507114][-4.2999148 -4.2766767 -4.2512116 -4.2181177 -4.181726 -4.1477027 -4.1098919 -4.0713372 -4.0639281 -4.1097307 -4.1808944 -4.2389822 -4.2681112 -4.2729239 -4.2590008][-4.2998629 -4.2736907 -4.2466125 -4.2140703 -4.1842113 -4.16194 -4.1383395 -4.1119714 -4.1122985 -4.1494074 -4.2038388 -4.2487922 -4.2734933 -4.2772746 -4.2638516][-4.2988095 -4.2699285 -4.237978 -4.2032442 -4.1770086 -4.1616168 -4.1516895 -4.1447706 -4.1589842 -4.1901236 -4.228353 -4.2602835 -4.2781181 -4.2795234 -4.2676268][-4.297718 -4.2657418 -4.2281685 -4.1913166 -4.167871 -4.1605611 -4.16259 -4.1717386 -4.1973853 -4.2270846 -4.2558351 -4.2784219 -4.2869868 -4.280848 -4.2668037][-4.3015461 -4.2679176 -4.2275496 -4.1919131 -4.1742558 -4.1764603 -4.1886992 -4.2056208 -4.2327986 -4.2595744 -4.2808 -4.2948885 -4.2944031 -4.2821937 -4.2684364]]...]
INFO - root - 2017-12-06 04:04:12.219873: step 18010, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:25m:00s remains)
INFO - root - 2017-12-06 04:04:14.508193: step 18020, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 19h:33m:48s remains)
INFO - root - 2017-12-06 04:04:16.813718: step 18030, loss = 2.05, batch loss = 1.99 (34.3 examples/sec; 0.233 sec/batch; 20h:23m:33s remains)
INFO - root - 2017-12-06 04:04:19.111986: step 18040, loss = 2.10, batch loss = 2.04 (35.7 examples/sec; 0.224 sec/batch; 19h:35m:53s remains)
INFO - root - 2017-12-06 04:04:21.460010: step 18050, loss = 2.07, batch loss = 2.01 (33.9 examples/sec; 0.236 sec/batch; 20h:35m:22s remains)
INFO - root - 2017-12-06 04:04:23.768871: step 18060, loss = 2.09, batch loss = 2.03 (34.6 examples/sec; 0.231 sec/batch; 20h:12m:34s remains)
INFO - root - 2017-12-06 04:04:26.094512: step 18070, loss = 2.07, batch loss = 2.01 (36.2 examples/sec; 0.221 sec/batch; 19h:17m:51s remains)
INFO - root - 2017-12-06 04:04:28.380265: step 18080, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 19h:36m:32s remains)
INFO - root - 2017-12-06 04:04:30.743083: step 18090, loss = 2.07, batch loss = 2.01 (34.0 examples/sec; 0.235 sec/batch; 20h:32m:14s remains)
INFO - root - 2017-12-06 04:04:33.019785: step 18100, loss = 2.07, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:16m:13s remains)
2017-12-06 04:04:33.333788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2898293 -4.2873921 -4.2879829 -4.2901845 -4.2909 -4.2909188 -4.2908216 -4.291996 -4.2935214 -4.2936034 -4.2917857 -4.2880282 -4.2878809 -4.2896461 -4.2903256][-4.2497091 -4.2469182 -4.2445526 -4.2433138 -4.2437868 -4.2456427 -4.2461739 -4.2493339 -4.254076 -4.2560272 -4.2541065 -4.2478156 -4.2448521 -4.2450852 -4.2444549][-4.2140179 -4.21215 -4.2072878 -4.2027078 -4.2034922 -4.2074952 -4.2082396 -4.2092752 -4.212265 -4.2149925 -4.2165704 -4.2118769 -4.2066646 -4.2061367 -4.2057672][-4.2077742 -4.2073178 -4.2017441 -4.1981783 -4.1980853 -4.1989274 -4.1915231 -4.1815972 -4.1754069 -4.1777725 -4.1853662 -4.1889462 -4.1873088 -4.1865778 -4.1889653][-4.2154803 -4.2156916 -4.2101769 -4.2091122 -4.2083621 -4.2023759 -4.1866083 -4.1667142 -4.1546922 -4.153863 -4.1652756 -4.1789031 -4.1892657 -4.1921067 -4.192976][-4.2141871 -4.21167 -4.2049413 -4.2028041 -4.2001262 -4.1926055 -4.1775408 -4.1599622 -4.1443615 -4.1359706 -4.1435571 -4.1615334 -4.1815286 -4.187439 -4.1846132][-4.19821 -4.190701 -4.1819205 -4.174933 -4.1678 -4.1563387 -4.1396065 -4.1222625 -4.1022048 -4.0881324 -4.0926623 -4.1133246 -4.1343908 -4.1394038 -4.1344967][-4.1439681 -4.134346 -4.1252122 -4.1162519 -4.1083083 -4.0942411 -4.073349 -4.0536242 -4.0334997 -4.0197883 -4.0258579 -4.0488787 -4.0670133 -4.0671086 -4.0577216][-4.0956445 -4.0885997 -4.0810204 -4.0689917 -4.0607357 -4.0467057 -4.0227566 -4.0008392 -3.9829969 -3.9798558 -3.9957893 -4.0200596 -4.0366278 -4.0351267 -4.0245976][-4.0949516 -4.0908394 -4.084703 -4.0700541 -4.06064 -4.0490131 -4.0282807 -4.0130806 -4.0038695 -4.0064692 -4.0217032 -4.0406775 -4.0532608 -4.0522914 -4.045361][-4.130291 -4.129282 -4.1224504 -4.1112657 -4.1018076 -4.0905161 -4.0688744 -4.0557823 -4.0520639 -4.0558796 -4.0636654 -4.0744872 -4.0827017 -4.0824404 -4.0756092][-4.168653 -4.1716928 -4.1690664 -4.1672134 -4.1633272 -4.1563644 -4.1391716 -4.1253982 -4.1216092 -4.1224236 -4.1209097 -4.1217618 -4.1254392 -4.1261334 -4.1194692][-4.1719775 -4.181664 -4.1889782 -4.1971226 -4.2003121 -4.203732 -4.1992288 -4.1904273 -4.1857939 -4.1825991 -4.1724648 -4.1619835 -4.1543565 -4.145071 -4.1364036][-4.1511168 -4.1656752 -4.1780972 -4.1897879 -4.1964054 -4.2038927 -4.2091961 -4.20978 -4.2085438 -4.2019711 -4.1889915 -4.1758943 -4.1624746 -4.1465235 -4.1362696][-4.1385221 -4.149786 -4.1585279 -4.1650367 -4.1676278 -4.1710763 -4.178287 -4.1864166 -4.1928616 -4.1911049 -4.1851115 -4.1783009 -4.1641636 -4.1435175 -4.1265917]]...]
INFO - root - 2017-12-06 04:04:35.624036: step 18110, loss = 2.08, batch loss = 2.02 (33.6 examples/sec; 0.238 sec/batch; 20h:46m:05s remains)
INFO - root - 2017-12-06 04:04:37.929873: step 18120, loss = 2.07, batch loss = 2.02 (34.1 examples/sec; 0.235 sec/batch; 20h:29m:21s remains)
INFO - root - 2017-12-06 04:04:40.226164: step 18130, loss = 2.02, batch loss = 1.96 (35.1 examples/sec; 0.228 sec/batch; 19h:52m:43s remains)
INFO - root - 2017-12-06 04:04:42.492735: step 18140, loss = 2.06, batch loss = 2.01 (35.8 examples/sec; 0.224 sec/batch; 19h:32m:14s remains)
INFO - root - 2017-12-06 04:04:44.788307: step 18150, loss = 2.06, batch loss = 2.00 (33.7 examples/sec; 0.237 sec/batch; 20h:43m:58s remains)
INFO - root - 2017-12-06 04:04:47.081816: step 18160, loss = 2.04, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 20h:04m:17s remains)
INFO - root - 2017-12-06 04:04:49.372207: step 18170, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.232 sec/batch; 20h:12m:52s remains)
INFO - root - 2017-12-06 04:04:51.675427: step 18180, loss = 2.03, batch loss = 1.97 (35.5 examples/sec; 0.225 sec/batch; 19h:39m:39s remains)
INFO - root - 2017-12-06 04:04:54.003163: step 18190, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 20h:10m:30s remains)
INFO - root - 2017-12-06 04:04:56.315115: step 18200, loss = 2.09, batch loss = 2.03 (34.6 examples/sec; 0.231 sec/batch; 20h:09m:41s remains)
2017-12-06 04:04:56.599771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2704449 -4.2581248 -4.2569971 -4.2603254 -4.2636986 -4.2706175 -4.2805552 -4.2925639 -4.2989426 -4.3007293 -4.3076816 -4.3156896 -4.3164382 -4.313025 -4.3090696][-4.2392149 -4.2265711 -4.2231426 -4.2230816 -4.2216043 -4.2270532 -4.2393718 -4.2576733 -4.2693563 -4.2739391 -4.2863078 -4.2986 -4.3031769 -4.3044667 -4.3019371][-4.2045627 -4.1889472 -4.1770949 -4.1670246 -4.1598806 -4.1615376 -4.1740813 -4.1973562 -4.2177253 -4.2316666 -4.2518015 -4.2689629 -4.2776337 -4.2861433 -4.2869163][-4.1683183 -4.14559 -4.1221085 -4.1049495 -4.0962105 -4.0955381 -4.1017694 -4.1225138 -4.1506548 -4.1748161 -4.2022867 -4.2257032 -4.242312 -4.2604871 -4.2656951][-4.1356993 -4.1015339 -4.0643106 -4.039144 -4.0306611 -4.0244284 -4.0181804 -4.0300846 -4.0655007 -4.100225 -4.1362095 -4.1680093 -4.196908 -4.2312136 -4.2451015][-4.1114798 -4.0645518 -4.0108695 -3.97148 -3.9629946 -3.9586921 -3.9493046 -3.9467428 -3.9843814 -4.0284629 -4.0710363 -4.105772 -4.1422157 -4.1895242 -4.2155852][-4.1104822 -4.053339 -3.9892185 -3.9422891 -3.9354491 -3.9344971 -3.9182792 -3.8976874 -3.9306154 -3.9785366 -4.0174913 -4.0462174 -4.0882578 -4.1461825 -4.1866989][-4.1415048 -4.0825219 -4.0182734 -3.9730835 -3.9602916 -3.9506888 -3.924135 -3.8849034 -3.9033244 -3.9480712 -3.984494 -4.0076604 -4.053812 -4.116703 -4.1668348][-4.1844254 -4.1303954 -4.0759177 -4.0351467 -4.0136309 -3.9928503 -3.9650838 -3.9219861 -3.9191678 -3.9472623 -3.9748306 -3.9924598 -4.0392346 -4.0997076 -4.1495609][-4.2113 -4.1669335 -4.128943 -4.1021743 -4.08546 -4.0622144 -4.03725 -4.0001221 -3.9809055 -3.9858298 -3.9965675 -4.0009546 -4.0356259 -4.0865893 -4.1340718][-4.2125039 -4.1724706 -4.1450672 -4.133287 -4.1300783 -4.1118011 -4.0934882 -4.0707884 -4.0518632 -4.0470624 -4.0382442 -4.02395 -4.0358753 -4.0732903 -4.115129][-4.1987133 -4.1610703 -4.139904 -4.1350913 -4.1398625 -4.1306653 -4.1246171 -4.1210246 -4.1132207 -4.1056285 -4.0878139 -4.058084 -4.0401592 -4.06137 -4.098907][-4.1849623 -4.1518784 -4.1344619 -4.1321464 -4.1370678 -4.1360588 -4.1415505 -4.1521544 -4.1527033 -4.1510072 -4.132462 -4.097959 -4.0648518 -4.0720539 -4.0984936][-4.18788 -4.1626892 -4.1507945 -4.1483555 -4.1490741 -4.1522951 -4.1622081 -4.1776 -4.1855121 -4.19069 -4.1768918 -4.1472807 -4.1167636 -4.1147466 -4.1277637][-4.211071 -4.1941404 -4.1878161 -4.1883311 -4.1903224 -4.1977515 -4.2112169 -4.2248888 -4.23131 -4.2363992 -4.2283435 -4.2107267 -4.1873159 -4.1832509 -4.1883039]]...]
INFO - root - 2017-12-06 04:04:58.913567: step 18210, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.226 sec/batch; 19h:41m:22s remains)
INFO - root - 2017-12-06 04:05:01.227518: step 18220, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:54m:45s remains)
INFO - root - 2017-12-06 04:05:03.519259: step 18230, loss = 2.03, batch loss = 1.97 (35.6 examples/sec; 0.225 sec/batch; 19h:36m:00s remains)
INFO - root - 2017-12-06 04:05:05.865983: step 18240, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 19h:43m:00s remains)
INFO - root - 2017-12-06 04:05:08.119414: step 18250, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 19h:40m:07s remains)
INFO - root - 2017-12-06 04:05:10.415245: step 18260, loss = 2.05, batch loss = 1.99 (34.0 examples/sec; 0.235 sec/batch; 20h:32m:58s remains)
INFO - root - 2017-12-06 04:05:12.724612: step 18270, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:28m:54s remains)
INFO - root - 2017-12-06 04:05:15.022077: step 18280, loss = 2.05, batch loss = 2.00 (33.6 examples/sec; 0.238 sec/batch; 20h:46m:38s remains)
INFO - root - 2017-12-06 04:05:17.290336: step 18290, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.226 sec/batch; 19h:41m:21s remains)
INFO - root - 2017-12-06 04:05:19.637603: step 18300, loss = 2.06, batch loss = 2.00 (32.1 examples/sec; 0.249 sec/batch; 21h:46m:11s remains)
2017-12-06 04:05:19.941441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2077861 -4.2408385 -4.2530332 -4.2547083 -4.2598953 -4.2595725 -4.25589 -4.2536507 -4.2552443 -4.264348 -4.2717938 -4.2742448 -4.275207 -4.2699947 -4.2560678][-4.2467413 -4.2744727 -4.2736759 -4.2575717 -4.2445254 -4.2279725 -4.2142844 -4.2127733 -4.2260032 -4.249373 -4.2680097 -4.2760611 -4.2779565 -4.2725191 -4.2570524][-4.2693934 -4.2909203 -4.2766142 -4.2418404 -4.2066369 -4.16802 -4.139564 -4.1377845 -4.1662774 -4.2099609 -4.2476573 -4.271112 -4.2807436 -4.2788668 -4.2646275][-4.2701812 -4.285347 -4.2597775 -4.2088971 -4.1531644 -4.0913839 -4.044405 -4.0414863 -4.0868011 -4.153 -4.212769 -4.2558856 -4.2777567 -4.2830238 -4.2719431][-4.2647963 -4.274508 -4.2408662 -4.178277 -4.1057563 -4.0213337 -3.9516377 -3.9455137 -4.0069766 -4.0948467 -4.1741366 -4.2346716 -4.2678728 -4.280777 -4.2746239][-4.2541618 -4.2598739 -4.221447 -4.1505203 -4.0628777 -3.9571829 -3.8632364 -3.8519583 -3.9313519 -4.0431728 -4.1407995 -4.2154584 -4.257411 -4.27577 -4.2733235][-4.23774 -4.240099 -4.1987538 -4.1210737 -4.0200095 -3.894958 -3.7739131 -3.752284 -3.8539224 -3.9953341 -4.1122513 -4.1989107 -4.2492204 -4.2730775 -4.2737203][-4.2193203 -4.2189112 -4.1768446 -4.09832 -3.992934 -3.8596516 -3.7163854 -3.6774457 -3.7935171 -3.9601867 -4.092104 -4.1860461 -4.2429519 -4.27095 -4.2740459][-4.2092557 -4.2116413 -4.1764445 -4.109385 -4.0187526 -3.9012797 -3.766835 -3.7204344 -3.8169358 -3.972553 -4.0963087 -4.1836276 -4.2376046 -4.2643213 -4.2672858][-4.2067995 -4.2156844 -4.1965303 -4.1528363 -4.0919685 -4.013237 -3.919878 -3.8835979 -3.939846 -4.04837 -4.1341529 -4.1943369 -4.2353544 -4.2590432 -4.2623639][-4.1944695 -4.2098331 -4.2072682 -4.1887026 -4.1596961 -4.1232939 -4.0754914 -4.0580292 -4.0848436 -4.1428571 -4.1839695 -4.2115173 -4.2336359 -4.2529931 -4.25678][-4.1618543 -4.1821847 -4.19431 -4.2004061 -4.2021704 -4.1993484 -4.1824679 -4.1758308 -4.1841879 -4.2002239 -4.2013025 -4.2026587 -4.2117186 -4.228529 -4.2344728][-4.1171069 -4.1414247 -4.1653666 -4.1928697 -4.21921 -4.2395434 -4.243597 -4.2419209 -4.238306 -4.223496 -4.1916919 -4.1725636 -4.1736908 -4.1909127 -4.2010465][-4.0870075 -4.1146965 -4.1455612 -4.1845765 -4.2260814 -4.258153 -4.2733479 -4.2725024 -4.2598495 -4.2239923 -4.1723518 -4.141777 -4.1405668 -4.1585293 -4.1720047][-4.0952835 -4.1237097 -4.1538563 -4.1908765 -4.2345209 -4.2691464 -4.2889791 -4.2900863 -4.2737007 -4.2300062 -4.1729345 -4.140244 -4.1394739 -4.1545439 -4.1636529]]...]
INFO - root - 2017-12-06 04:05:22.266203: step 18310, loss = 2.07, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 19h:49m:35s remains)
INFO - root - 2017-12-06 04:05:24.573489: step 18320, loss = 2.09, batch loss = 2.03 (34.3 examples/sec; 0.233 sec/batch; 20h:20m:27s remains)
INFO - root - 2017-12-06 04:05:26.873501: step 18330, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.230 sec/batch; 20h:05m:44s remains)
INFO - root - 2017-12-06 04:05:29.159017: step 18340, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 20h:03m:15s remains)
INFO - root - 2017-12-06 04:05:31.470974: step 18350, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.230 sec/batch; 20h:05m:32s remains)
INFO - root - 2017-12-06 04:05:33.729483: step 18360, loss = 2.05, batch loss = 1.99 (35.0 examples/sec; 0.229 sec/batch; 19h:58m:02s remains)
INFO - root - 2017-12-06 04:05:36.072306: step 18370, loss = 2.08, batch loss = 2.02 (33.3 examples/sec; 0.240 sec/batch; 20h:57m:39s remains)
INFO - root - 2017-12-06 04:05:38.362671: step 18380, loss = 2.07, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 19h:52m:46s remains)
INFO - root - 2017-12-06 04:05:40.661721: step 18390, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:42m:03s remains)
INFO - root - 2017-12-06 04:05:43.014288: step 18400, loss = 2.08, batch loss = 2.03 (34.0 examples/sec; 0.235 sec/batch; 20h:32m:17s remains)
2017-12-06 04:05:43.309504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2084284 -4.1963706 -4.1847467 -4.1818962 -4.1917777 -4.2028031 -4.2192197 -4.2239761 -4.231596 -4.2476587 -4.2669187 -4.2726383 -4.269372 -4.268086 -4.27316][-4.2171884 -4.2107768 -4.2023797 -4.2050648 -4.2153959 -4.2218552 -4.2353544 -4.2410455 -4.2457409 -4.2521729 -4.2600565 -4.2568865 -4.2518368 -4.2529945 -4.2644691][-4.2176328 -4.2160821 -4.2161179 -4.2241411 -4.2330852 -4.2349381 -4.2434072 -4.2456212 -4.2403893 -4.2325134 -4.2280607 -4.2208538 -4.2196012 -4.2306108 -4.2549648][-4.2162166 -4.2199574 -4.2259932 -4.2323432 -4.235249 -4.2364955 -4.2434907 -4.2377696 -4.2207017 -4.2001905 -4.1872568 -4.1808062 -4.1888876 -4.2122178 -4.245636][-4.1988044 -4.2051306 -4.2085915 -4.2116036 -4.2139878 -4.2202778 -4.2302566 -4.2232 -4.2022471 -4.1813626 -4.1710405 -4.1718874 -4.1898017 -4.2163954 -4.2452393][-4.1855073 -4.1940703 -4.1849995 -4.1741018 -4.1696148 -4.1742926 -4.182929 -4.1779509 -4.1624031 -4.1572766 -4.1679559 -4.1899114 -4.2180915 -4.2372289 -4.2546229][-4.1841874 -4.1821475 -4.1554809 -4.1261687 -4.1075249 -4.1000061 -4.1017876 -4.0964513 -4.0841575 -4.0997181 -4.1424594 -4.1960964 -4.2397394 -4.2588181 -4.2673774][-4.1791682 -4.1647787 -4.1268845 -4.0857892 -4.04969 -4.0323443 -4.0331173 -4.0294466 -4.0190172 -4.0478182 -4.1119785 -4.1888151 -4.2442331 -4.2683167 -4.2776155][-4.1762938 -4.1602325 -4.1200085 -4.0699978 -4.0202379 -3.9997826 -4.0102568 -4.0181122 -4.0156317 -4.0496888 -4.1129646 -4.1851583 -4.2400975 -4.2682695 -4.282011][-4.1885533 -4.1754031 -4.1348925 -4.0788155 -4.0278997 -4.0155935 -4.0409231 -4.0615315 -4.0656571 -4.0971184 -4.1467776 -4.2005172 -4.24196 -4.2698612 -4.2878451][-4.204186 -4.1954222 -4.1603131 -4.1101289 -4.0700254 -4.07235 -4.1086044 -4.1356492 -4.1425014 -4.1649804 -4.1958575 -4.225914 -4.2524757 -4.2783442 -4.2973289][-4.2149191 -4.2141886 -4.1955132 -4.1655769 -4.1432018 -4.154058 -4.1882277 -4.2128329 -4.219 -4.2285271 -4.2388973 -4.2457633 -4.2590823 -4.2820654 -4.3021979][-4.2415361 -4.2455444 -4.2393146 -4.2275958 -4.2186308 -4.2304554 -4.2555084 -4.2716575 -4.2716131 -4.270184 -4.2683706 -4.2626181 -4.2670636 -4.2842069 -4.3035965][-4.2808414 -4.2829924 -4.280724 -4.2765479 -4.274106 -4.28308 -4.2998877 -4.3073568 -4.2995944 -4.2918768 -4.2861848 -4.277142 -4.2777286 -4.2907176 -4.3071303][-4.3084993 -4.3062134 -4.303751 -4.303308 -4.3050051 -4.3124194 -4.322896 -4.3232746 -4.3111377 -4.3018484 -4.2973008 -4.2886796 -4.2868404 -4.2962632 -4.3114419]]...]
INFO - root - 2017-12-06 04:05:45.586053: step 18410, loss = 2.05, batch loss = 1.99 (34.3 examples/sec; 0.233 sec/batch; 20h:20m:14s remains)
INFO - root - 2017-12-06 04:05:47.892984: step 18420, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 19h:39m:28s remains)
INFO - root - 2017-12-06 04:05:50.188240: step 18430, loss = 2.05, batch loss = 2.00 (32.4 examples/sec; 0.247 sec/batch; 21h:33m:41s remains)
INFO - root - 2017-12-06 04:05:52.509132: step 18440, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 19h:34m:17s remains)
INFO - root - 2017-12-06 04:05:54.800912: step 18450, loss = 2.05, batch loss = 1.99 (34.0 examples/sec; 0.235 sec/batch; 20h:30m:41s remains)
INFO - root - 2017-12-06 04:05:57.100580: step 18460, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:23m:17s remains)
INFO - root - 2017-12-06 04:05:59.373092: step 18470, loss = 2.08, batch loss = 2.02 (33.8 examples/sec; 0.237 sec/batch; 20h:38m:25s remains)
INFO - root - 2017-12-06 04:06:01.665358: step 18480, loss = 2.07, batch loss = 2.02 (33.7 examples/sec; 0.237 sec/batch; 20h:41m:09s remains)
INFO - root - 2017-12-06 04:06:03.937706: step 18490, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:26m:50s remains)
INFO - root - 2017-12-06 04:06:06.200815: step 18500, loss = 2.05, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 20h:03m:40s remains)
2017-12-06 04:06:06.475192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2787638 -4.2879171 -4.2970309 -4.3052015 -4.3106318 -4.3152847 -4.3198738 -4.3225751 -4.3217144 -4.3185177 -4.31314 -4.3025947 -4.2861376 -4.2676387 -4.2551527][-4.2722836 -4.2822194 -4.2923336 -4.3025093 -4.3109541 -4.3187628 -4.32742 -4.334034 -4.3357811 -4.3329759 -4.3259764 -4.3131018 -4.2944927 -4.2739916 -4.2591877][-4.2650909 -4.269309 -4.2747803 -4.2838659 -4.2945414 -4.3052964 -4.3185282 -4.3323774 -4.3410234 -4.34187 -4.3354082 -4.3214679 -4.3016891 -4.2795353 -4.262476][-4.2540936 -4.2459588 -4.2403674 -4.2438221 -4.2541661 -4.26513 -4.2807922 -4.3030958 -4.3236594 -4.3340607 -4.3328509 -4.3214841 -4.3036246 -4.2816248 -4.2632065][-4.2382483 -4.2149205 -4.194334 -4.18734 -4.1921434 -4.1975155 -4.2092881 -4.2377939 -4.2725878 -4.2976041 -4.3070393 -4.3034339 -4.2914367 -4.2740984 -4.2583685][-4.2179742 -4.181087 -4.1461797 -4.1269693 -4.1212049 -4.115201 -4.1151943 -4.1441164 -4.1923509 -4.23417 -4.2580552 -4.2665014 -4.2650876 -4.2568073 -4.2482595][-4.1969228 -4.1529784 -4.1091452 -4.079123 -4.0598378 -4.037334 -4.01954 -4.0422392 -4.1011982 -4.1597152 -4.199399 -4.2212043 -4.2321749 -4.2345567 -4.2353821][-4.1799221 -4.1370125 -4.0923529 -4.0562677 -4.0257797 -3.9871521 -3.9518113 -3.9636416 -4.0279636 -4.0997176 -4.1531229 -4.185657 -4.2056093 -4.216114 -4.2237353][-4.1689968 -4.13047 -4.0900445 -4.0548229 -4.0226765 -3.9798543 -3.9392951 -3.9413 -4.0002036 -4.0747094 -4.1338964 -4.1706085 -4.1936889 -4.2071686 -4.2171578][-4.1721368 -4.1376925 -4.1026554 -4.0722027 -4.0467348 -4.0126414 -3.980087 -3.9784317 -4.0241542 -4.0900116 -4.1441889 -4.1773248 -4.197515 -4.2086182 -4.2171869][-4.1954741 -4.1660938 -4.136981 -4.1129956 -4.0962582 -4.0749426 -4.0536137 -4.0521283 -4.0835724 -4.1326995 -4.1734281 -4.1968713 -4.2103968 -4.2152367 -4.2191262][-4.2384686 -4.2184153 -4.1972995 -4.1799841 -4.1692071 -4.1560245 -4.1416578 -4.1387978 -4.1563535 -4.185751 -4.2103233 -4.2220893 -4.2262654 -4.2219129 -4.2179317][-4.2814417 -4.2755289 -4.2646112 -4.252769 -4.2443061 -4.2341046 -4.2221327 -4.2160954 -4.2219548 -4.2343664 -4.244144 -4.2454662 -4.2406139 -4.2251859 -4.2112517][-4.2916508 -4.3040056 -4.3065968 -4.3021274 -4.2960334 -4.2871265 -4.2761016 -4.2677183 -4.2649617 -4.2653208 -4.2635279 -4.2564325 -4.2431512 -4.21732 -4.1933155][-4.255322 -4.2860436 -4.3068166 -4.3160839 -4.31749 -4.3130922 -4.3047447 -4.2951655 -4.2857409 -4.2770109 -4.2657986 -4.2514186 -4.2302656 -4.19614 -4.1649928]]...]
INFO - root - 2017-12-06 04:06:08.803563: step 18510, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 19h:48m:40s remains)
INFO - root - 2017-12-06 04:06:11.127068: step 18520, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.226 sec/batch; 19h:44m:26s remains)
INFO - root - 2017-12-06 04:06:13.443362: step 18530, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 20h:34m:55s remains)
INFO - root - 2017-12-06 04:06:15.761260: step 18540, loss = 2.05, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 19h:53m:17s remains)
INFO - root - 2017-12-06 04:06:18.020546: step 18550, loss = 2.05, batch loss = 1.99 (35.0 examples/sec; 0.229 sec/batch; 19h:56m:48s remains)
INFO - root - 2017-12-06 04:06:20.326447: step 18560, loss = 2.04, batch loss = 1.99 (33.9 examples/sec; 0.236 sec/batch; 20h:32m:59s remains)
INFO - root - 2017-12-06 04:06:22.663624: step 18570, loss = 2.06, batch loss = 2.01 (34.4 examples/sec; 0.233 sec/batch; 20h:17m:58s remains)
INFO - root - 2017-12-06 04:06:24.973963: step 18580, loss = 2.04, batch loss = 1.98 (33.8 examples/sec; 0.237 sec/batch; 20h:37m:26s remains)
INFO - root - 2017-12-06 04:06:27.280370: step 18590, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 19h:45m:09s remains)
INFO - root - 2017-12-06 04:06:29.607978: step 18600, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 20h:06m:30s remains)
2017-12-06 04:06:29.899685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2507367 -4.2398391 -4.2204032 -4.2076378 -4.21454 -4.2237892 -4.2224689 -4.2174735 -4.2194896 -4.2331405 -4.242651 -4.251307 -4.2623687 -4.2686582 -4.2592907][-4.2536893 -4.2371635 -4.2129297 -4.1940761 -4.1986332 -4.2062268 -4.19915 -4.1898851 -4.1944337 -4.2143192 -4.2288203 -4.2399721 -4.2582135 -4.2697115 -4.2607675][-4.2611313 -4.2400064 -4.2139888 -4.1931009 -4.1938262 -4.1954479 -4.1811991 -4.1650882 -4.1704359 -4.1966386 -4.216239 -4.2302856 -4.256402 -4.2761669 -4.2709055][-4.2653556 -4.2431226 -4.2133818 -4.1894226 -4.183876 -4.177918 -4.1556315 -4.13225 -4.1361036 -4.1683316 -4.1963596 -4.219749 -4.2538686 -4.2797041 -4.2797971][-4.2720103 -4.2551918 -4.2265048 -4.2014055 -4.1894946 -4.1683788 -4.13033 -4.0972219 -4.1029921 -4.1418409 -4.1824532 -4.2178359 -4.2576447 -4.2861576 -4.2903852][-4.2634115 -4.257134 -4.2365894 -4.2150893 -4.1982822 -4.162591 -4.1064787 -4.0661459 -4.0801253 -4.1278687 -4.178586 -4.22202 -4.2660089 -4.2950697 -4.2988749][-4.2296352 -4.2315412 -4.2205663 -4.2085595 -4.1878924 -4.1392293 -4.0615034 -4.0155377 -4.047442 -4.1095309 -4.1689105 -4.217 -4.26538 -4.2940164 -4.29477][-4.1662169 -4.1712551 -4.1650858 -4.1639557 -4.1486621 -4.090651 -3.9917111 -3.9436615 -3.9965785 -4.0799761 -4.1502819 -4.2040038 -4.2517662 -4.2766471 -4.2721019][-4.103334 -4.1010942 -4.0996675 -4.1142373 -4.1102505 -4.0478268 -3.94018 -3.8988869 -3.9664943 -4.06417 -4.138309 -4.1915159 -4.2329721 -4.2494717 -4.2422247][-4.0698028 -4.0677838 -4.0755243 -4.1047158 -4.110827 -4.0567317 -3.9646511 -3.9405131 -4.0098248 -4.0994096 -4.1575971 -4.1939631 -4.2181945 -4.224371 -4.2125049][-4.0814152 -4.0893164 -4.1035333 -4.1343293 -4.1454325 -4.1085048 -4.0497465 -4.0435662 -4.0964341 -4.158628 -4.1942081 -4.2086816 -4.2140427 -4.2093606 -4.1940708][-4.124989 -4.1384649 -4.1544824 -4.1771078 -4.186676 -4.162282 -4.1317315 -4.1372256 -4.1703172 -4.2041183 -4.2226496 -4.2263474 -4.2217512 -4.2123857 -4.1970854][-4.17904 -4.196044 -4.211165 -4.2244473 -4.2270246 -4.2099261 -4.1965714 -4.2054839 -4.2243891 -4.24502 -4.2574563 -4.2608986 -4.2570949 -4.2481165 -4.2352648][-4.223207 -4.24143 -4.2563758 -4.2671695 -4.2677402 -4.2545042 -4.24823 -4.2568398 -4.2687888 -4.2833233 -4.2946258 -4.2995958 -4.2978921 -4.2936358 -4.2865148][-4.2471657 -4.2649684 -4.2796865 -4.293788 -4.2978377 -4.28942 -4.2859759 -4.2909741 -4.2993469 -4.3131242 -4.3264871 -4.3335919 -4.3338013 -4.332356 -4.327909]]...]
INFO - root - 2017-12-06 04:06:32.200621: step 18610, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:41m:47s remains)
INFO - root - 2017-12-06 04:06:34.489668: step 18620, loss = 2.06, batch loss = 2.00 (35.2 examples/sec; 0.227 sec/batch; 19h:48m:53s remains)
INFO - root - 2017-12-06 04:06:36.788393: step 18630, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:40m:40s remains)
INFO - root - 2017-12-06 04:06:39.135988: step 18640, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:21m:43s remains)
INFO - root - 2017-12-06 04:06:41.441882: step 18650, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:12m:14s remains)
INFO - root - 2017-12-06 04:06:43.762180: step 18660, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.228 sec/batch; 19h:55m:03s remains)
INFO - root - 2017-12-06 04:06:46.047808: step 18670, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:51m:42s remains)
INFO - root - 2017-12-06 04:06:48.401204: step 18680, loss = 2.04, batch loss = 1.98 (35.1 examples/sec; 0.228 sec/batch; 19h:50m:29s remains)
INFO - root - 2017-12-06 04:06:50.665919: step 18690, loss = 2.06, batch loss = 2.01 (32.9 examples/sec; 0.243 sec/batch; 21h:11m:52s remains)
INFO - root - 2017-12-06 04:06:52.970535: step 18700, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.237 sec/batch; 20h:39m:22s remains)
2017-12-06 04:06:53.275330: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2181025 -4.2043209 -4.1810312 -4.1476274 -4.1149769 -4.1359653 -4.183322 -4.2165322 -4.2271328 -4.2126579 -4.1905422 -4.1670146 -4.1354003 -4.1324339 -4.1413536][-4.1954541 -4.1807933 -4.1527104 -4.1090336 -4.0629125 -4.0760489 -4.1299872 -4.1748161 -4.1959634 -4.1939058 -4.1861706 -4.17225 -4.1479039 -4.1456208 -4.1533957][-4.1743579 -4.1610355 -4.1326504 -4.0813384 -4.0230284 -4.019587 -4.066874 -4.1125035 -4.1416717 -4.1549883 -4.1711335 -4.1737633 -4.1593451 -4.1579113 -4.1640968][-4.1746421 -4.1625266 -4.127449 -4.07013 -4.0024071 -3.9778309 -4.0101156 -4.05545 -4.0970526 -4.1310205 -4.1681509 -4.1824613 -4.1768374 -4.172533 -4.1765165][-4.1942286 -4.1819768 -4.1422524 -4.0853682 -4.0141568 -3.9665308 -3.9737275 -4.0135274 -4.0693712 -4.1231308 -4.1727948 -4.1943588 -4.1913671 -4.186564 -4.1911383][-4.2232842 -4.2065558 -4.1592083 -4.1016631 -4.032588 -3.9751554 -3.961947 -3.9950633 -4.0520368 -4.1095071 -4.1587253 -4.1862688 -4.191246 -4.193399 -4.2030063][-4.2401648 -4.2188349 -4.163455 -4.1033387 -4.039186 -3.9891605 -3.977078 -4.0031533 -4.0440831 -4.0784593 -4.1103921 -4.1419296 -4.1679358 -4.1883073 -4.2045803][-4.2387195 -4.2185187 -4.1678872 -4.1138616 -4.0605822 -4.0220208 -4.0124412 -4.0189142 -4.0224981 -4.0151544 -4.0159993 -4.0499449 -4.1071124 -4.15645 -4.1878591][-4.2252989 -4.2117434 -4.17885 -4.1456122 -4.1160297 -4.0922055 -4.0749364 -4.0483241 -4.0068178 -3.9565134 -3.9278016 -3.9606967 -4.0386257 -4.1149783 -4.16211][-4.2170677 -4.2116532 -4.2005558 -4.195395 -4.192317 -4.184516 -4.161128 -4.1088963 -4.0375834 -3.9648569 -3.9234624 -3.9469197 -4.0210934 -4.0996923 -4.1479259][-4.2131014 -4.2204037 -4.2279854 -4.2423115 -4.2546062 -4.2532053 -4.22995 -4.1745782 -4.1025753 -4.0331063 -3.9965734 -4.0158534 -4.0711374 -4.1279092 -4.161911][-4.2221518 -4.2366142 -4.2469988 -4.2616043 -4.2740364 -4.2729621 -4.2529645 -4.206213 -4.1481185 -4.0962729 -4.0751343 -4.0984373 -4.1431231 -4.1812296 -4.1986108][-4.2419128 -4.2563548 -4.2607689 -4.2662044 -4.2740812 -4.2747006 -4.25842 -4.221818 -4.1801844 -4.1487293 -4.1452785 -4.1709414 -4.203145 -4.2277722 -4.2346005][-4.2617431 -4.2768435 -4.2793379 -4.2782736 -4.2826757 -4.283864 -4.273983 -4.2522097 -4.2270322 -4.2084141 -4.2086711 -4.2256222 -4.2435064 -4.2565417 -4.259552][-4.2857404 -4.2986712 -4.3024936 -4.301578 -4.3046236 -4.3068185 -4.3041596 -4.2952428 -4.2821965 -4.2713661 -4.2699509 -4.2780037 -4.2860222 -4.2901206 -4.2897024]]...]
INFO - root - 2017-12-06 04:06:55.613979: step 18710, loss = 2.10, batch loss = 2.04 (34.8 examples/sec; 0.230 sec/batch; 20h:01m:15s remains)
INFO - root - 2017-12-06 04:06:57.939736: step 18720, loss = 2.07, batch loss = 2.01 (34.4 examples/sec; 0.232 sec/batch; 20h:15m:34s remains)
INFO - root - 2017-12-06 04:07:00.231988: step 18730, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 19h:41m:27s remains)
INFO - root - 2017-12-06 04:07:02.519407: step 18740, loss = 2.04, batch loss = 1.98 (35.7 examples/sec; 0.224 sec/batch; 19h:32m:57s remains)
INFO - root - 2017-12-06 04:07:04.794345: step 18750, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.236 sec/batch; 20h:36m:13s remains)
INFO - root - 2017-12-06 04:07:07.095569: step 18760, loss = 2.06, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 19h:08m:07s remains)
INFO - root - 2017-12-06 04:07:09.454955: step 18770, loss = 2.06, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:31m:52s remains)
INFO - root - 2017-12-06 04:07:11.770547: step 18780, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.225 sec/batch; 19h:37m:10s remains)
INFO - root - 2017-12-06 04:07:14.139106: step 18790, loss = 2.07, batch loss = 2.01 (31.8 examples/sec; 0.251 sec/batch; 21h:53m:23s remains)
INFO - root - 2017-12-06 04:07:16.431310: step 18800, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:34m:32s remains)
2017-12-06 04:07:16.737103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.236001 -4.215169 -4.1742759 -4.1375456 -4.1108909 -4.0969334 -4.1081157 -4.1319766 -4.1500726 -4.1447554 -4.1193833 -4.1043124 -4.0972505 -4.1000371 -4.1097918][-4.2237329 -4.2010837 -4.1568284 -4.1170864 -4.084929 -4.0671659 -4.0812 -4.1159277 -4.1492796 -4.1551948 -4.1351619 -4.1194611 -4.1093307 -4.1091189 -4.1145992][-4.2061539 -4.1842275 -4.1380954 -4.0936842 -4.0533957 -4.0267525 -4.04196 -4.0868068 -4.1260605 -4.1403184 -4.132833 -4.1261749 -4.116488 -4.1124697 -4.1111927][-4.1754169 -4.1595545 -4.1172576 -4.07044 -4.0196471 -3.9782724 -3.9868991 -4.0364075 -4.0817795 -4.1099939 -4.1258664 -4.1300082 -4.1201715 -4.115139 -4.1145711][-4.1293778 -4.1205511 -4.0886021 -4.0407009 -3.9762917 -3.9132309 -3.9039631 -3.9536488 -4.0231948 -4.0812354 -4.1226926 -4.1384697 -4.1360521 -4.13758 -4.139185][-4.0738912 -4.0762687 -4.0617051 -4.0176635 -3.9384317 -3.8581924 -3.8319771 -3.8896673 -3.9882615 -4.0736761 -4.1306591 -4.152328 -4.158658 -4.1690955 -4.1737342][-4.0170078 -4.0331969 -4.041954 -4.0122728 -3.9327078 -3.8468509 -3.8184011 -3.887208 -4.0019588 -4.0981541 -4.15568 -4.1740642 -4.1845531 -4.1973138 -4.2009211][-3.9998164 -4.0233707 -4.0453205 -4.0383739 -3.9746342 -3.9033055 -3.8902349 -3.9639556 -4.0696692 -4.1479912 -4.19036 -4.2024655 -4.215054 -4.2266679 -4.2272129][-4.0543547 -4.0792766 -4.1018567 -4.1044822 -4.0602565 -4.0143628 -4.0159044 -4.08127 -4.1596537 -4.2060795 -4.2248278 -4.2263317 -4.2364435 -4.2481327 -4.243731][-4.1448765 -4.1620007 -4.1762385 -4.1739926 -4.1404119 -4.1120529 -4.1217523 -4.1717091 -4.2245474 -4.2468467 -4.2459655 -4.23826 -4.2466607 -4.2598929 -4.2533484][-4.2220039 -4.2278876 -4.2274151 -4.2135468 -4.1828556 -4.1661038 -4.1803794 -4.2211552 -4.2578073 -4.2690177 -4.2607088 -4.2490849 -4.2508669 -4.2599769 -4.2535429][-4.2617059 -4.257803 -4.2460666 -4.2237897 -4.1958551 -4.18924 -4.2109189 -4.248291 -4.277698 -4.2851224 -4.2723436 -4.2515 -4.2404985 -4.2426033 -4.2383628][-4.2812595 -4.2734585 -4.2600708 -4.2419443 -4.2238126 -4.2217011 -4.2386627 -4.265655 -4.2852325 -4.2861853 -4.2687421 -4.2415419 -4.223526 -4.21854 -4.2180157][-4.3003268 -4.2931647 -4.2843428 -4.2762265 -4.2675281 -4.2653756 -4.2712359 -4.2823358 -4.2907505 -4.2871866 -4.270709 -4.2480788 -4.2316289 -4.2280164 -4.2353306][-4.316566 -4.3117056 -4.3072691 -4.3050027 -4.3024893 -4.3017244 -4.3024569 -4.3049979 -4.307138 -4.3029294 -4.2916908 -4.277997 -4.2684159 -4.270401 -4.2807331]]...]
INFO - root - 2017-12-06 04:07:19.065620: step 18810, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:18m:35s remains)
INFO - root - 2017-12-06 04:07:21.404546: step 18820, loss = 2.06, batch loss = 2.01 (33.8 examples/sec; 0.237 sec/batch; 20h:36m:44s remains)
INFO - root - 2017-12-06 04:07:23.750883: step 18830, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 20h:00m:29s remains)
INFO - root - 2017-12-06 04:07:26.103007: step 18840, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 19h:41m:07s remains)
INFO - root - 2017-12-06 04:07:28.415718: step 18850, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 20h:08m:08s remains)
INFO - root - 2017-12-06 04:07:30.733146: step 18860, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 19h:54m:51s remains)
INFO - root - 2017-12-06 04:07:33.005825: step 18870, loss = 2.08, batch loss = 2.02 (34.7 examples/sec; 0.231 sec/batch; 20h:05m:33s remains)
INFO - root - 2017-12-06 04:07:35.339756: step 18880, loss = 2.06, batch loss = 2.00 (34.1 examples/sec; 0.235 sec/batch; 20h:27m:39s remains)
INFO - root - 2017-12-06 04:07:37.641089: step 18890, loss = 2.04, batch loss = 1.98 (35.2 examples/sec; 0.227 sec/batch; 19h:47m:50s remains)
INFO - root - 2017-12-06 04:07:39.961022: step 18900, loss = 2.06, batch loss = 2.00 (33.6 examples/sec; 0.238 sec/batch; 20h:43m:11s remains)
2017-12-06 04:07:40.248674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2528639 -4.2588043 -4.2597408 -4.2510223 -4.2348394 -4.2251334 -4.2166271 -4.2076192 -4.2106237 -4.22014 -4.2313075 -4.2404733 -4.2436314 -4.2451053 -4.2482328][-4.2169485 -4.2260213 -4.2313471 -4.2234778 -4.207469 -4.1997619 -4.1864839 -4.1732469 -4.1768327 -4.1926346 -4.2075706 -4.215241 -4.2160797 -4.2214422 -4.232305][-4.1681013 -4.1797357 -4.1940136 -4.1907582 -4.174592 -4.1657071 -4.1445365 -4.124248 -4.1323242 -4.1531329 -4.1736283 -4.1854463 -4.190547 -4.2046676 -4.2271509][-4.1265583 -4.1353073 -4.152986 -4.1528354 -4.1344619 -4.1192222 -4.0867534 -4.0645418 -4.0838408 -4.1200638 -4.1532578 -4.1744928 -4.1857677 -4.2068038 -4.2347589][-4.0884838 -4.0942955 -4.1178241 -4.1213126 -4.0953169 -4.0610895 -4.016293 -4.0130067 -4.0527773 -4.1022515 -4.1464691 -4.1771617 -4.1924877 -4.2169714 -4.2423511][-4.07752 -4.0631757 -4.0768542 -4.07603 -4.0406027 -3.9795077 -3.9287376 -3.9630449 -4.0353103 -4.0985446 -4.1489935 -4.1816611 -4.1981969 -4.2221189 -4.2415991][-4.1002398 -4.0761909 -4.072185 -4.0486207 -3.9809773 -3.8689532 -3.8020868 -3.89318 -4.0152221 -4.0972409 -4.1471238 -4.1786084 -4.1983104 -4.2176518 -4.231945][-4.1198978 -4.1063204 -4.1051497 -4.0745735 -3.9928441 -3.8575313 -3.7772522 -3.8824072 -4.0177193 -4.1013041 -4.1457429 -4.1690788 -4.190906 -4.2095709 -4.2199025][-4.120028 -4.11801 -4.1231465 -4.1064119 -4.05444 -3.9681363 -3.914341 -3.9622858 -4.0482392 -4.1094375 -4.1453056 -4.1675577 -4.18749 -4.2044978 -4.2140517][-4.1024923 -4.0975695 -4.1064415 -4.1082344 -4.0920124 -4.0547104 -4.0238042 -4.0357494 -4.0771585 -4.1085081 -4.1362019 -4.1631474 -4.1842484 -4.2025995 -4.2166338][-4.0810609 -4.07539 -4.0843539 -4.0940661 -4.0949607 -4.0854015 -4.07719 -4.08762 -4.108397 -4.1179056 -4.1379848 -4.1663895 -4.18964 -4.2090554 -4.2275734][-4.0733061 -4.0730724 -4.0841966 -4.0953546 -4.1034923 -4.1049113 -4.1066341 -4.1187758 -4.1287889 -4.1263876 -4.1422596 -4.1705437 -4.1909547 -4.2108922 -4.2339969][-4.086442 -4.0885625 -4.1033297 -4.1213808 -4.1362228 -4.1363611 -4.135108 -4.1417236 -4.1448045 -4.1428342 -4.1571875 -4.1785417 -4.19286 -4.2132587 -4.2382736][-4.1141248 -4.11842 -4.1355839 -4.155436 -4.1706758 -4.1703291 -4.1672325 -4.1691341 -4.1690426 -4.1662631 -4.1734877 -4.1853037 -4.1962614 -4.2156234 -4.2397461][-4.1424737 -4.1459532 -4.1584077 -4.1766133 -4.1899238 -4.1903138 -4.1885123 -4.192606 -4.193572 -4.1894803 -4.1888952 -4.1921458 -4.2001829 -4.2139292 -4.23365]]...]
INFO - root - 2017-12-06 04:07:42.523629: step 18910, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:12m:56s remains)
INFO - root - 2017-12-06 04:07:44.814794: step 18920, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:33m:51s remains)
INFO - root - 2017-12-06 04:07:47.060909: step 18930, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.237 sec/batch; 20h:37m:32s remains)
INFO - root - 2017-12-06 04:07:49.335290: step 18940, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 19h:19m:07s remains)
INFO - root - 2017-12-06 04:07:51.656556: step 18950, loss = 2.04, batch loss = 1.98 (35.2 examples/sec; 0.227 sec/batch; 19h:48m:49s remains)
INFO - root - 2017-12-06 04:07:53.966585: step 18960, loss = 2.07, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:13m:02s remains)
INFO - root - 2017-12-06 04:07:56.292126: step 18970, loss = 2.08, batch loss = 2.02 (32.9 examples/sec; 0.244 sec/batch; 21h:12m:32s remains)
INFO - root - 2017-12-06 04:07:58.589633: step 18980, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 19h:58m:23s remains)
INFO - root - 2017-12-06 04:08:00.868771: step 18990, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.220 sec/batch; 19h:11m:41s remains)
INFO - root - 2017-12-06 04:08:03.167248: step 19000, loss = 2.06, batch loss = 2.00 (33.8 examples/sec; 0.236 sec/batch; 20h:35m:14s remains)
2017-12-06 04:08:03.462002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2495561 -4.2472391 -4.2516022 -4.2588859 -4.2741156 -4.2924438 -4.3029323 -4.3063035 -4.3062558 -4.3063359 -4.3164258 -4.3374228 -4.35685 -4.3699632 -4.3783493][-4.2549372 -4.2608595 -4.27036 -4.2801747 -4.2943897 -4.3109508 -4.3218412 -4.3275495 -4.3300834 -4.3312459 -4.3412466 -4.3599033 -4.3752875 -4.3839145 -4.3881426][-4.2657919 -4.2756257 -4.2851586 -4.291378 -4.299613 -4.31209 -4.3237324 -4.3341389 -4.34276 -4.3480606 -4.3581133 -4.3715734 -4.3806915 -4.3831444 -4.3821125][-4.2696052 -4.2802343 -4.2879539 -4.2881613 -4.2870693 -4.2939844 -4.3060942 -4.3212194 -4.3382816 -4.3534131 -4.3673282 -4.3771639 -4.3800015 -4.376791 -4.3719854][-4.2632108 -4.2689204 -4.2725859 -4.2667394 -4.2555032 -4.2536778 -4.2656531 -4.287611 -4.3143024 -4.3402905 -4.36223 -4.375433 -4.3780594 -4.3730345 -4.3670826][-4.2463903 -4.2381177 -4.2296133 -4.2123137 -4.1860423 -4.1698213 -4.1798811 -4.2148666 -4.2606068 -4.3038011 -4.3402143 -4.36371 -4.3727303 -4.3714485 -4.3672223][-4.220006 -4.1908031 -4.1607428 -4.1209412 -4.0700846 -4.0316424 -4.0391316 -4.0974641 -4.1738844 -4.2435646 -4.30341 -4.343761 -4.3630304 -4.3689723 -4.3688288][-4.2045035 -4.1557379 -4.1016817 -4.0378428 -3.9595711 -3.8948476 -3.8978598 -3.9797702 -4.0854378 -4.1800594 -4.2620921 -4.3203068 -4.3518553 -4.3665967 -4.3710957][-4.2093964 -4.1486688 -4.078186 -4.000423 -3.9113557 -3.8307528 -3.8210819 -3.9036787 -4.0189705 -4.1283121 -4.2263188 -4.2983747 -4.341073 -4.3638391 -4.372086][-4.2297735 -4.1675444 -4.0942388 -4.0154972 -3.9372175 -3.872227 -3.864181 -3.9300544 -4.0292983 -4.1317911 -4.2279353 -4.2990675 -4.3423648 -4.3659291 -4.373848][-4.2677808 -4.2151103 -4.1522508 -4.0870228 -4.0316715 -3.9938254 -3.9957638 -4.04544 -4.118866 -4.197989 -4.2752638 -4.3308411 -4.3617086 -4.3763771 -4.3779278][-4.3065982 -4.2699947 -4.2272553 -4.1839752 -4.151556 -4.1364517 -4.1469254 -4.1840138 -4.2349339 -4.2891173 -4.3401947 -4.3731585 -4.3862352 -4.3872075 -4.3806248][-4.3340063 -4.3128719 -4.2902217 -4.26937 -4.2565675 -4.2564363 -4.2692027 -4.2936459 -4.3259397 -4.358449 -4.3866591 -4.400847 -4.4013548 -4.3929434 -4.3812838][-4.3522053 -4.3428779 -4.3348927 -4.3290343 -4.3277106 -4.3326697 -4.3432064 -4.3581219 -4.3755679 -4.3919969 -4.4040456 -4.40704 -4.4014277 -4.3901572 -4.3781958][-4.3626914 -4.359333 -4.3578334 -4.3589358 -4.3628373 -4.3695607 -4.3775291 -4.3852668 -4.3918695 -4.39661 -4.3979878 -4.3955145 -4.3890705 -4.3802428 -4.3717628]]...]
INFO - root - 2017-12-06 04:08:05.793123: step 19010, loss = 2.06, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 19h:33m:45s remains)
INFO - root - 2017-12-06 04:08:08.090793: step 19020, loss = 2.05, batch loss = 1.99 (34.5 examples/sec; 0.232 sec/batch; 20h:13m:06s remains)
INFO - root - 2017-12-06 04:08:10.399763: step 19030, loss = 2.05, batch loss = 2.00 (32.3 examples/sec; 0.247 sec/batch; 21h:32m:08s remains)
INFO - root - 2017-12-06 04:08:12.710672: step 19040, loss = 2.07, batch loss = 2.01 (34.0 examples/sec; 0.235 sec/batch; 20h:28m:19s remains)
INFO - root - 2017-12-06 04:08:14.985556: step 19050, loss = 2.05, batch loss = 1.99 (34.0 examples/sec; 0.235 sec/batch; 20h:28m:36s remains)
INFO - root - 2017-12-06 04:08:17.300348: step 19060, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.226 sec/batch; 19h:42m:15s remains)
INFO - root - 2017-12-06 04:08:19.585050: step 19070, loss = 2.05, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 19h:43m:50s remains)
INFO - root - 2017-12-06 04:08:21.920632: step 19080, loss = 2.07, batch loss = 2.01 (33.1 examples/sec; 0.242 sec/batch; 21h:03m:44s remains)
INFO - root - 2017-12-06 04:08:24.236814: step 19090, loss = 2.05, batch loss = 1.99 (34.5 examples/sec; 0.232 sec/batch; 20h:11m:11s remains)
INFO - root - 2017-12-06 04:08:26.534638: step 19100, loss = 2.05, batch loss = 1.99 (32.7 examples/sec; 0.244 sec/batch; 21h:15m:59s remains)
2017-12-06 04:08:26.850188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2818918 -4.2765727 -4.271069 -4.2620482 -4.247582 -4.2337651 -4.2344379 -4.2370167 -4.2393093 -4.2480321 -4.2593007 -4.2682886 -4.27026 -4.2773 -4.2830129][-4.2760105 -4.2694688 -4.256577 -4.2369647 -4.2112823 -4.1875243 -4.185905 -4.2008882 -4.2237754 -4.2507868 -4.2698593 -4.277 -4.2731504 -4.2735944 -4.278028][-4.2727408 -4.2662983 -4.2475214 -4.2154937 -4.1728239 -4.1335645 -4.1241083 -4.1467452 -4.1924071 -4.2407537 -4.2706084 -4.2758722 -4.2679515 -4.2639828 -4.2678223][-4.251039 -4.2430139 -4.2187505 -4.1765957 -4.1160693 -4.0573483 -4.0304608 -4.0518227 -4.121943 -4.1981492 -4.2417779 -4.2499232 -4.2426119 -4.2371383 -4.2389727][-4.2224026 -4.2128615 -4.1853824 -4.1358204 -4.0595927 -3.9786513 -3.9223776 -3.928405 -4.0181732 -4.125073 -4.18581 -4.2017279 -4.2031164 -4.2027216 -4.2031097][-4.2106962 -4.2040048 -4.1774035 -4.1227217 -4.0356746 -3.935535 -3.8432519 -3.8189526 -3.9245694 -4.0622587 -4.140038 -4.16792 -4.1816425 -4.1875939 -4.1893992][-4.2184849 -4.2175093 -4.192317 -4.1347756 -4.0445352 -3.9388118 -3.8182645 -3.7562342 -3.8636725 -4.0212173 -4.114851 -4.1561484 -4.1806264 -4.191977 -4.1935673][-4.2389064 -4.2397561 -4.2164979 -4.1593823 -4.0722952 -3.9785376 -3.8722615 -3.8105259 -3.8958962 -4.0376306 -4.1304073 -4.1772056 -4.2069817 -4.220871 -4.2205582][-4.2425842 -4.2443652 -4.2261486 -4.1723008 -4.0934887 -4.0184608 -3.9420347 -3.9061348 -3.9661543 -4.0726814 -4.1550612 -4.2058411 -4.2381706 -4.2527938 -4.2523141][-4.2346983 -4.239121 -4.2267628 -4.1783924 -4.109498 -4.0528607 -4.0018444 -3.9858556 -4.02966 -4.1059318 -4.1718526 -4.2196894 -4.2504187 -4.2632828 -4.261982][-4.2369552 -4.2441745 -4.2376046 -4.1980577 -4.1420274 -4.1004605 -4.0700979 -4.0680389 -4.102459 -4.1538119 -4.1990876 -4.2334175 -4.2539296 -4.2621803 -4.2598972][-4.255971 -4.2652 -4.262486 -4.2324014 -4.1891723 -4.1603136 -4.143157 -4.1465769 -4.17668 -4.2115498 -4.2383256 -4.2573504 -4.2646918 -4.2650728 -4.2607884][-4.2822804 -4.2890105 -4.2879519 -4.2671051 -4.2356129 -4.2161665 -4.2076058 -4.2144346 -4.2406955 -4.2636418 -4.2767978 -4.2826343 -4.2809777 -4.2751932 -4.2701187][-4.3058586 -4.3108826 -4.3109627 -4.298028 -4.2763419 -4.2631083 -4.2571073 -4.2624011 -4.2814026 -4.2957792 -4.3032036 -4.3045325 -4.3006592 -4.2955894 -4.293529][-4.3153172 -4.3187575 -4.3199925 -4.3141184 -4.3023877 -4.2939644 -4.2901154 -4.2928786 -4.3037252 -4.3117652 -4.3167591 -4.3193331 -4.3175311 -4.316103 -4.3166747]]...]
INFO - root - 2017-12-06 04:08:29.179938: step 19110, loss = 2.05, batch loss = 1.99 (34.2 examples/sec; 0.234 sec/batch; 20h:22m:57s remains)
INFO - root - 2017-12-06 04:08:31.525063: step 19120, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.224 sec/batch; 19h:28m:32s remains)
INFO - root - 2017-12-06 04:08:33.807837: step 19130, loss = 2.03, batch loss = 1.97 (35.7 examples/sec; 0.224 sec/batch; 19h:31m:06s remains)
INFO - root - 2017-12-06 04:08:36.123315: step 19140, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 19h:17m:19s remains)
INFO - root - 2017-12-06 04:08:38.404629: step 19150, loss = 2.06, batch loss = 2.01 (34.1 examples/sec; 0.235 sec/batch; 20h:26m:30s remains)
INFO - root - 2017-12-06 04:08:40.717807: step 19160, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 20h:31m:35s remains)
INFO - root - 2017-12-06 04:08:43.024497: step 19170, loss = 2.05, batch loss = 2.00 (33.2 examples/sec; 0.241 sec/batch; 20h:59m:32s remains)
INFO - root - 2017-12-06 04:08:45.317664: step 19180, loss = 2.10, batch loss = 2.04 (35.7 examples/sec; 0.224 sec/batch; 19h:31m:44s remains)
INFO - root - 2017-12-06 04:08:47.600303: step 19190, loss = 2.07, batch loss = 2.02 (33.1 examples/sec; 0.242 sec/batch; 21h:03m:09s remains)
INFO - root - 2017-12-06 04:08:49.874336: step 19200, loss = 2.04, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 19h:06m:27s remains)
2017-12-06 04:08:50.168504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1744056 -4.1696315 -4.15845 -4.1452894 -4.1288466 -4.1169248 -4.1031656 -4.1015573 -4.1191692 -4.1179676 -4.1187835 -4.1430817 -4.1651926 -4.1592784 -4.13772][-4.1408997 -4.1245589 -4.1026826 -4.0737977 -4.0575027 -4.0663285 -4.0649896 -4.0729156 -4.1079793 -4.1192751 -4.1249418 -4.1572003 -4.17746 -4.1680622 -4.1493382][-4.0628314 -4.0343695 -4.008101 -3.9719076 -3.9709733 -4.0178123 -4.0453548 -4.071157 -4.1234918 -4.14755 -4.154881 -4.1820459 -4.188941 -4.1711936 -4.1571779][-3.964992 -3.9443104 -3.938385 -3.9149232 -3.9291463 -3.9907093 -4.02848 -4.0629897 -4.1209812 -4.1507325 -4.1621394 -4.1807785 -4.1774745 -4.1509714 -4.14056][-3.9201293 -3.9277725 -3.9463222 -3.9277787 -3.931953 -3.9600508 -3.9753277 -4.008028 -4.0705423 -4.112215 -4.1322069 -4.150413 -4.1431031 -4.1151562 -4.1040683][-3.9720469 -3.9859324 -3.9902945 -3.9636633 -3.9466717 -3.9276977 -3.8938856 -3.9043374 -3.9566553 -4.0095191 -4.0502348 -4.0807004 -4.0872707 -4.0750465 -4.0782084][-4.026845 -4.0213389 -3.9998777 -3.9614956 -3.9280343 -3.860908 -3.7654867 -3.7427635 -3.7894874 -3.8733296 -3.9517651 -4.0026274 -4.0358071 -4.0539641 -4.0781903][-4.0570593 -4.0371108 -3.9882812 -3.9290175 -3.8820252 -3.7945368 -3.6808834 -3.6585338 -3.7188611 -3.8276496 -3.9246666 -3.9744358 -4.0205712 -4.060884 -4.1014071][-4.0654883 -4.0325952 -3.9735332 -3.9187331 -3.8970556 -3.8461847 -3.7748449 -3.773736 -3.8201842 -3.8921683 -3.9630377 -3.9994435 -4.0449643 -4.0896158 -4.1348038][-4.0823641 -4.05535 -4.0164862 -3.9881504 -3.9838681 -3.959805 -3.9182129 -3.9206228 -3.9441617 -3.9812527 -4.0202713 -4.0438585 -4.0744104 -4.1098394 -4.1482821][-4.1349049 -4.1133914 -4.087554 -4.0688257 -4.0645051 -4.0483775 -4.0246124 -4.0275173 -4.0360765 -4.0540571 -4.0694427 -4.0786939 -4.0923071 -4.11969 -4.1511579][-4.1772327 -4.1527696 -4.1247792 -4.1051598 -4.0994034 -4.0927572 -4.0812435 -4.0830479 -4.0837078 -4.092061 -4.1020975 -4.1028905 -4.1081381 -4.129714 -4.1528525][-4.1874614 -4.1630354 -4.1278067 -4.1066208 -4.1029472 -4.1092463 -4.1119318 -4.1128364 -4.1048851 -4.1071296 -4.1188335 -4.1186275 -4.115304 -4.1303248 -4.1461778][-4.1799679 -4.1619272 -4.1324615 -4.1160192 -4.1164489 -4.1302838 -4.1392941 -4.1375928 -4.1204925 -4.1194634 -4.1336117 -4.1352372 -4.1269503 -4.1317658 -4.1405778][-4.1686072 -4.1632729 -4.1507568 -4.1441045 -4.1474333 -4.1564274 -4.161448 -4.1594162 -4.1403213 -4.1372981 -4.156446 -4.1630716 -4.1504889 -4.1430626 -4.1384134]]...]
INFO - root - 2017-12-06 04:08:52.504121: step 19210, loss = 2.08, batch loss = 2.02 (33.2 examples/sec; 0.241 sec/batch; 20h:56m:47s remains)
INFO - root - 2017-12-06 04:08:54.798257: step 19220, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:18m:43s remains)
INFO - root - 2017-12-06 04:08:57.092670: step 19230, loss = 2.03, batch loss = 1.97 (35.8 examples/sec; 0.224 sec/batch; 19h:27m:25s remains)
INFO - root - 2017-12-06 04:08:59.394040: step 19240, loss = 2.09, batch loss = 2.03 (34.0 examples/sec; 0.235 sec/batch; 20h:28m:50s remains)
INFO - root - 2017-12-06 04:09:01.703923: step 19250, loss = 2.10, batch loss = 2.04 (34.4 examples/sec; 0.232 sec/batch; 20h:13m:03s remains)
INFO - root - 2017-12-06 04:09:04.087442: step 19260, loss = 2.06, batch loss = 2.01 (32.9 examples/sec; 0.243 sec/batch; 21h:10m:33s remains)
INFO - root - 2017-12-06 04:09:06.413375: step 19270, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 20h:05m:34s remains)
INFO - root - 2017-12-06 04:09:08.684746: step 19280, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:06m:54s remains)
INFO - root - 2017-12-06 04:09:10.990725: step 19290, loss = 2.05, batch loss = 1.99 (34.9 examples/sec; 0.229 sec/batch; 19h:55m:43s remains)
INFO - root - 2017-12-06 04:09:13.259347: step 19300, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 19h:17m:07s remains)
2017-12-06 04:09:13.550129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2244911 -4.2330103 -4.2279224 -4.2188797 -4.2095137 -4.2019606 -4.2010393 -4.2060122 -4.2136583 -4.2205181 -4.2231522 -4.2203393 -4.2139111 -4.2086816 -4.2064734][-4.2067342 -4.2229815 -4.2208548 -4.2089071 -4.1939697 -4.1789913 -4.1741209 -4.180594 -4.1947813 -4.2100468 -4.2191 -4.2185493 -4.2115 -4.2031355 -4.1956325][-4.1830249 -4.2078357 -4.210361 -4.1941924 -4.1705146 -4.1453323 -4.1330771 -4.1392803 -4.1615939 -4.1886125 -4.2077379 -4.2135916 -4.2081709 -4.1984668 -4.1858668][-4.1733727 -4.2021961 -4.2072496 -4.1850181 -4.1478996 -4.1063185 -4.0797949 -4.0812311 -4.1131954 -4.15463 -4.1855712 -4.2002425 -4.2011023 -4.1953173 -4.1829557][-4.1861334 -4.2091689 -4.2084537 -4.1765137 -4.1224432 -4.0592623 -4.0105968 -4.0035267 -4.04762 -4.1073461 -4.1530027 -4.1808839 -4.1925192 -4.1951437 -4.1888762][-4.2085533 -4.2241783 -4.2135258 -4.1692338 -4.0975723 -4.0131764 -3.9428473 -3.9291258 -3.9865103 -4.0600986 -4.117568 -4.1576471 -4.1805477 -4.1927404 -4.1938906][-4.216774 -4.2327518 -4.2162142 -4.1633635 -4.0813437 -3.9897778 -3.9150074 -3.9029858 -3.9631832 -4.0331936 -4.0867558 -4.12899 -4.1571541 -4.1751485 -4.182972][-4.2017126 -4.2264266 -4.214642 -4.1658196 -4.0903363 -4.0115132 -3.9517829 -3.9467256 -3.9968514 -4.0455923 -4.0770659 -4.1039338 -4.1270971 -4.1454082 -4.1578174][-4.193821 -4.2268877 -4.2267604 -4.1917248 -4.1328621 -4.07516 -4.0341153 -4.0322051 -4.0645971 -4.0879006 -4.0938129 -4.1019583 -4.11543 -4.1267042 -4.1361189][-4.2069292 -4.2404714 -4.2487216 -4.22729 -4.1858106 -4.1469064 -4.1192513 -4.1149292 -4.1304278 -4.1342239 -4.1222515 -4.1203575 -4.1276989 -4.1320477 -4.1342077][-4.2238936 -4.2559175 -4.269372 -4.2579422 -4.2292638 -4.2026668 -4.1811204 -4.173099 -4.1771421 -4.1692324 -4.1501846 -4.1462874 -4.1530709 -4.15529 -4.1529312][-4.2456532 -4.2740412 -4.2888927 -4.282228 -4.2615247 -4.2413092 -4.2233195 -4.21368 -4.2126837 -4.2015929 -4.18336 -4.1836243 -4.1940079 -4.200695 -4.199266][-4.2630935 -4.28919 -4.3048544 -4.3026881 -4.286902 -4.2686052 -4.252306 -4.2419305 -4.2386923 -4.2282739 -4.2146416 -4.21929 -4.2353992 -4.2487249 -4.2512155][-4.2747469 -4.2993455 -4.3167143 -4.3194737 -4.307611 -4.2893877 -4.2721462 -4.2594934 -4.2538939 -4.2453809 -4.2353249 -4.2404871 -4.2581792 -4.2752614 -4.2816911][-4.2873139 -4.3093448 -4.3271427 -4.33281 -4.3231611 -4.3053813 -4.28818 -4.2743793 -4.2670116 -4.25926 -4.2494044 -4.2493315 -4.2610111 -4.2752242 -4.2836156]]...]
INFO - root - 2017-12-06 04:09:15.870232: step 19310, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.237 sec/batch; 20h:35m:23s remains)
INFO - root - 2017-12-06 04:09:18.176723: step 19320, loss = 2.07, batch loss = 2.02 (33.2 examples/sec; 0.241 sec/batch; 20h:56m:27s remains)
INFO - root - 2017-12-06 04:09:20.453635: step 19330, loss = 2.05, batch loss = 2.00 (32.8 examples/sec; 0.244 sec/batch; 21h:13m:53s remains)
INFO - root - 2017-12-06 04:09:22.803256: step 19340, loss = 2.08, batch loss = 2.02 (32.5 examples/sec; 0.246 sec/batch; 21h:23m:09s remains)
INFO - root - 2017-12-06 04:09:25.132485: step 19350, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:17m:38s remains)
INFO - root - 2017-12-06 04:09:27.424720: step 19360, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 19h:27m:05s remains)
INFO - root - 2017-12-06 04:09:29.754152: step 19370, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:11m:36s remains)
INFO - root - 2017-12-06 04:09:32.082639: step 19380, loss = 2.09, batch loss = 2.03 (35.0 examples/sec; 0.228 sec/batch; 19h:52m:11s remains)
INFO - root - 2017-12-06 04:09:34.366474: step 19390, loss = 2.07, batch loss = 2.02 (36.0 examples/sec; 0.222 sec/batch; 19h:20m:39s remains)
INFO - root - 2017-12-06 04:09:36.685321: step 19400, loss = 2.05, batch loss = 1.99 (34.9 examples/sec; 0.229 sec/batch; 19h:56m:15s remains)
2017-12-06 04:09:36.970509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3058553 -4.3077807 -4.3104897 -4.3118739 -4.3123984 -4.3119245 -4.3112688 -4.3110986 -4.3097167 -4.3069911 -4.3050685 -4.3035059 -4.2985592 -4.2922297 -4.2890091][-4.29988 -4.3021741 -4.3044167 -4.3057427 -4.3061423 -4.3048172 -4.3033218 -4.3023376 -4.2999587 -4.2971754 -4.2953582 -4.2922945 -4.2856555 -4.2785692 -4.2764807][-4.2975044 -4.2989521 -4.2986035 -4.296999 -4.2939363 -4.2893991 -4.2855835 -4.2828774 -4.28035 -4.2797079 -4.2822161 -4.2821426 -4.2776623 -4.2714262 -4.266643][-4.3038669 -4.303318 -4.2973609 -4.287343 -4.2760668 -4.2638431 -4.2530317 -4.2471433 -4.2464786 -4.2518015 -4.2639313 -4.2743583 -4.2783728 -4.2747083 -4.2638683][-4.3040557 -4.3017688 -4.2886128 -4.2674575 -4.2437463 -4.2168732 -4.1925058 -4.1825314 -4.1892977 -4.2070932 -4.2354903 -4.2638903 -4.28216 -4.2831697 -4.2659416][-4.2874589 -4.2849936 -4.2627945 -4.2248044 -4.1788054 -4.1254873 -4.07965 -4.0689368 -4.0989375 -4.1438036 -4.1967516 -4.2487388 -4.28489 -4.2945542 -4.2754154][-4.2585945 -4.2551212 -4.2205133 -4.1578197 -4.0805387 -3.99428 -3.9313877 -3.9339864 -4.0061555 -4.0903807 -4.1704574 -4.2412109 -4.2898793 -4.3079457 -4.2934031][-4.2358027 -4.2270889 -4.1793318 -4.0926785 -3.9848483 -3.868741 -3.7962427 -3.8203828 -3.9359169 -4.0548244 -4.1571984 -4.2388687 -4.2926569 -4.3185959 -4.3135719][-4.2415285 -4.2226648 -4.1649637 -4.0625005 -3.9343846 -3.8001397 -3.7295442 -3.7800555 -3.9230318 -4.0567126 -4.1676631 -4.250586 -4.301846 -4.3297 -4.3312588][-4.2726669 -4.2462945 -4.1883731 -4.0916271 -3.9720721 -3.8560309 -3.8064542 -3.8646312 -3.9945185 -4.1129618 -4.2120757 -4.2835464 -4.3228059 -4.3432317 -4.3437696][-4.3065057 -4.2805223 -4.2332091 -4.1601853 -4.0747733 -3.9994593 -3.9729276 -4.0178032 -4.1115203 -4.197648 -4.2712932 -4.323 -4.343647 -4.3494797 -4.3424635][-4.32989 -4.312439 -4.2829814 -4.2400055 -4.1939406 -4.1570439 -4.1458187 -4.1735678 -4.2297649 -4.2820764 -4.326354 -4.3524685 -4.3543224 -4.3462873 -4.3321013][-4.3359952 -4.3291974 -4.3162088 -4.297792 -4.2801743 -4.2675676 -4.2649984 -4.2794714 -4.3074141 -4.334415 -4.3536196 -4.3586211 -4.3497481 -4.3356738 -4.3194556][-4.3327818 -4.3336844 -4.3325348 -4.3297911 -4.32793 -4.3258548 -4.3261595 -4.3317351 -4.3416276 -4.3515062 -4.3543186 -4.3489017 -4.3377972 -4.3240318 -4.3092356][-4.3275189 -4.3308082 -4.3353882 -4.3403997 -4.3434134 -4.3442078 -4.3447628 -4.3459945 -4.3478656 -4.3481345 -4.3439641 -4.336679 -4.3275647 -4.3173308 -4.3059382]]...]
INFO - root - 2017-12-06 04:09:39.265893: step 19410, loss = 2.04, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 19h:30m:05s remains)
INFO - root - 2017-12-06 04:09:41.528078: step 19420, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:25m:26s remains)
INFO - root - 2017-12-06 04:09:43.821951: step 19430, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 20h:04m:23s remains)
INFO - root - 2017-12-06 04:09:46.148184: step 19440, loss = 2.06, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:37m:36s remains)
INFO - root - 2017-12-06 04:09:48.439529: step 19450, loss = 2.06, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 20h:16m:34s remains)
INFO - root - 2017-12-06 04:09:50.706636: step 19460, loss = 2.05, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 20h:09m:22s remains)
INFO - root - 2017-12-06 04:09:52.998197: step 19470, loss = 2.07, batch loss = 2.01 (32.5 examples/sec; 0.246 sec/batch; 21h:23m:44s remains)
INFO - root - 2017-12-06 04:09:55.283987: step 19480, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:32m:28s remains)
INFO - root - 2017-12-06 04:09:57.603074: step 19490, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 20h:31m:10s remains)
INFO - root - 2017-12-06 04:09:59.903221: step 19500, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:06m:45s remains)
2017-12-06 04:10:00.198139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1876969 -4.1965785 -4.1978612 -4.1948948 -4.192132 -4.1902466 -4.1897826 -4.1875706 -4.1851325 -4.1850982 -4.1839008 -4.1723733 -4.1655493 -4.1638589 -4.1635404][-4.20714 -4.2148972 -4.215004 -4.207562 -4.1986847 -4.1931214 -4.1915407 -4.1884236 -4.1844435 -4.1869636 -4.1897545 -4.1857419 -4.1811857 -4.1803246 -4.1835594][-4.2373953 -4.246387 -4.2442122 -4.2316332 -4.2164803 -4.2063322 -4.2021294 -4.1934576 -4.1833296 -4.1845622 -4.1902289 -4.1924076 -4.1882977 -4.18478 -4.1905632][-4.2711315 -4.2842574 -4.2812285 -4.2657151 -4.2468729 -4.2325311 -4.2220163 -4.2062826 -4.1876316 -4.1811848 -4.1871557 -4.1917911 -4.1831012 -4.1698122 -4.16961][-4.2977715 -4.3080406 -4.3002224 -4.2827678 -4.2617579 -4.2426362 -4.2229648 -4.1978445 -4.1771951 -4.1723337 -4.1838932 -4.1869416 -4.1703086 -4.1466794 -4.1426654][-4.2943125 -4.2983642 -4.2867851 -4.26975 -4.2469912 -4.2180471 -4.1859083 -4.1564541 -4.1442757 -4.1567783 -4.1794744 -4.1845517 -4.1665034 -4.1416378 -4.1385241][-4.2689228 -4.2647762 -4.246038 -4.2259693 -4.1986518 -4.1607037 -4.1251235 -4.1061711 -4.1158171 -4.1505585 -4.1850538 -4.1955037 -4.1871734 -4.1724877 -4.1678233][-4.2340455 -4.2197065 -4.19413 -4.1712341 -4.1445222 -4.1100717 -4.08572 -4.0883188 -4.1190338 -4.1643414 -4.2003016 -4.2135906 -4.2154536 -4.2096992 -4.2003126][-4.1940541 -4.1716018 -4.1442389 -4.1291027 -4.1175604 -4.1014514 -4.097436 -4.1153007 -4.146183 -4.1831012 -4.2091026 -4.2179213 -4.2223039 -4.2229042 -4.2174764][-4.1538377 -4.1309614 -4.1140113 -4.1163507 -4.1238055 -4.1252127 -4.1354742 -4.1567211 -4.1782622 -4.201118 -4.2142959 -4.2159414 -4.2179523 -4.2215223 -4.2236409][-4.1348705 -4.1207752 -4.1169939 -4.1322227 -4.1494384 -4.157681 -4.1694665 -4.18728 -4.201879 -4.216466 -4.2226052 -4.2198877 -4.2172222 -4.2183495 -4.2218227][-4.1528778 -4.1470795 -4.1472764 -4.1577239 -4.1687603 -4.1758986 -4.1852756 -4.1981196 -4.2113819 -4.2251992 -4.2331991 -4.23094 -4.2263374 -4.2223725 -4.2186637][-4.1873312 -4.1811419 -4.1764293 -4.1745186 -4.1754079 -4.1815753 -4.1900754 -4.2010531 -4.216496 -4.2314239 -4.2404542 -4.2397356 -4.2344451 -4.2271166 -4.2156405][-4.2125115 -4.2015347 -4.18907 -4.1788292 -4.1753736 -4.1826658 -4.1923184 -4.2019258 -4.2154908 -4.2287073 -4.2364297 -4.236454 -4.2323289 -4.224587 -4.2088318][-4.2138844 -4.1959705 -4.1804328 -4.1739168 -4.17611 -4.1831851 -4.1900425 -4.1962404 -4.2031732 -4.2114019 -4.2173028 -4.2190685 -4.217731 -4.2116475 -4.1946516]]...]
INFO - root - 2017-12-06 04:10:02.478329: step 19510, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 19h:43m:53s remains)
INFO - root - 2017-12-06 04:10:04.753565: step 19520, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 20h:06m:07s remains)
INFO - root - 2017-12-06 04:10:07.103001: step 19530, loss = 2.05, batch loss = 1.99 (34.4 examples/sec; 0.233 sec/batch; 20h:13m:01s remains)
INFO - root - 2017-12-06 04:10:09.421278: step 19540, loss = 2.07, batch loss = 2.01 (35.6 examples/sec; 0.225 sec/batch; 19h:31m:46s remains)
INFO - root - 2017-12-06 04:10:11.719956: step 19550, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:49m:30s remains)
INFO - root - 2017-12-06 04:10:13.991450: step 19560, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.224 sec/batch; 19h:30m:40s remains)
INFO - root - 2017-12-06 04:10:16.309701: step 19570, loss = 2.05, batch loss = 1.99 (33.3 examples/sec; 0.240 sec/batch; 20h:53m:19s remains)
INFO - root - 2017-12-06 04:10:18.596504: step 19580, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.223 sec/batch; 19h:24m:36s remains)
INFO - root - 2017-12-06 04:10:20.920620: step 19590, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.224 sec/batch; 19h:26m:59s remains)
INFO - root - 2017-12-06 04:10:23.238853: step 19600, loss = 2.08, batch loss = 2.02 (33.6 examples/sec; 0.238 sec/batch; 20h:42m:50s remains)
2017-12-06 04:10:23.522770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2863507 -4.2807817 -4.2781496 -4.2773662 -4.2758665 -4.2770429 -4.2665119 -4.2455416 -4.2361751 -4.2438016 -4.2528253 -4.2571354 -4.2625241 -4.2689128 -4.27877][-4.2729673 -4.2627463 -4.2586761 -4.2591352 -4.26063 -4.2657566 -4.2538829 -4.2257748 -4.2144327 -4.2264748 -4.2376633 -4.2371359 -4.2364454 -4.2451849 -4.2628112][-4.2558441 -4.241477 -4.2359281 -4.2374353 -4.2400942 -4.2472963 -4.2341075 -4.2013907 -4.1877894 -4.2026467 -4.2165189 -4.2122712 -4.2070942 -4.2191658 -4.2446113][-4.2279005 -4.2065172 -4.1979642 -4.1995668 -4.2026815 -4.213809 -4.203609 -4.1696091 -4.1557522 -4.177134 -4.195219 -4.1903915 -4.1846476 -4.20031 -4.2298155][-4.1816978 -4.1472387 -4.1320634 -4.1352739 -4.1425209 -4.1601453 -4.1525364 -4.1167507 -4.107224 -4.1460876 -4.1770039 -4.1747723 -4.1688566 -4.1859269 -4.213851][-4.1297369 -4.0780215 -4.0486007 -4.04651 -4.0562878 -4.0757437 -4.0642176 -4.0195169 -4.023119 -4.0969219 -4.1533232 -4.1659784 -4.1658831 -4.1842704 -4.2042089][-4.0931468 -4.0213289 -3.9669559 -3.9445636 -3.9417191 -3.9511859 -3.9301548 -3.8784771 -3.9072659 -4.0325131 -4.1242385 -4.1574488 -4.1693621 -4.1883802 -4.1997485][-4.0781026 -3.9981585 -3.9228108 -3.8753161 -3.8484318 -3.8272278 -3.7753458 -3.7032225 -3.7533274 -3.9281664 -4.0611386 -4.1221504 -4.1532226 -4.1791158 -4.19237][-4.0980577 -4.0266867 -3.9532189 -3.8977628 -3.8500471 -3.7983708 -3.7152481 -3.6198826 -3.6628845 -3.8399816 -3.9855604 -4.0680718 -4.1176605 -4.1533904 -4.1779585][-4.1497464 -4.09447 -4.0348916 -3.986999 -3.9348741 -3.868804 -3.7799284 -3.68348 -3.6921043 -3.81485 -3.9368036 -4.020431 -4.0784173 -4.1263833 -4.1690354][-4.2121978 -4.1739984 -4.1318026 -4.0956793 -4.0530705 -3.9909043 -3.9084666 -3.8148687 -3.7888532 -3.8535867 -3.9407659 -4.0107861 -4.067306 -4.1241369 -4.1828895][-4.2633905 -4.2371879 -4.2103548 -4.1879444 -4.1605945 -4.1150684 -4.0517321 -3.9758604 -3.9398351 -3.9656675 -4.0198293 -4.0674524 -4.114059 -4.1684895 -4.22417][-4.296608 -4.2812376 -4.2663021 -4.254446 -4.2402506 -4.213635 -4.1726947 -4.124207 -4.0990934 -4.1104531 -4.1424804 -4.1711588 -4.2034922 -4.2397952 -4.2736592][-4.3139162 -4.3057194 -4.2970562 -4.2910213 -4.2840891 -4.2708755 -4.2476134 -4.2207594 -4.2073317 -4.2159424 -4.2372651 -4.2564578 -4.275445 -4.2935514 -4.3074489][-4.3173947 -4.3132753 -4.3085036 -4.3053312 -4.3016448 -4.2947927 -4.2825928 -4.2704358 -4.2644882 -4.2701297 -4.2834535 -4.2968259 -4.3070989 -4.3144903 -4.3183608]]...]
INFO - root - 2017-12-06 04:10:25.831670: step 19610, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 19h:23m:12s remains)
INFO - root - 2017-12-06 04:10:28.147751: step 19620, loss = 2.07, batch loss = 2.01 (34.4 examples/sec; 0.233 sec/batch; 20h:12m:31s remains)
INFO - root - 2017-12-06 04:10:30.438843: step 19630, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:47m:23s remains)
INFO - root - 2017-12-06 04:10:32.775107: step 19640, loss = 2.03, batch loss = 1.97 (33.8 examples/sec; 0.237 sec/batch; 20h:34m:53s remains)
INFO - root - 2017-12-06 04:10:35.055263: step 19650, loss = 2.07, batch loss = 2.02 (34.7 examples/sec; 0.231 sec/batch; 20h:02m:49s remains)
INFO - root - 2017-12-06 04:10:37.371198: step 19660, loss = 2.03, batch loss = 1.98 (34.4 examples/sec; 0.233 sec/batch; 20h:12m:32s remains)
INFO - root - 2017-12-06 04:10:39.704483: step 19670, loss = 2.07, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 20h:09m:27s remains)
INFO - root - 2017-12-06 04:10:41.996332: step 19680, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 19h:47m:18s remains)
INFO - root - 2017-12-06 04:10:44.297115: step 19690, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.230 sec/batch; 20h:00m:31s remains)
INFO - root - 2017-12-06 04:10:46.577631: step 19700, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:39m:06s remains)
2017-12-06 04:10:46.862364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2319717 -4.2523537 -4.2643552 -4.2696776 -4.2738585 -4.2679377 -4.2614355 -4.2662635 -4.27645 -4.2826438 -4.2899084 -4.2931681 -4.2961044 -4.2937512 -4.283174][-4.2424464 -4.2567663 -4.2600904 -4.26064 -4.2589669 -4.24531 -4.2383065 -4.2411957 -4.2544541 -4.2639503 -4.27293 -4.2771597 -4.2844048 -4.2880707 -4.2804694][-4.2350507 -4.2427835 -4.2423134 -4.2389908 -4.2310758 -4.215004 -4.2124352 -4.2168894 -4.231596 -4.2464533 -4.2583461 -4.26387 -4.2711177 -4.2755961 -4.2707243][-4.2118082 -4.2173066 -4.2156034 -4.2096825 -4.1954608 -4.1800766 -4.1860538 -4.1985631 -4.2161808 -4.2350459 -4.2477736 -4.253561 -4.2579708 -4.2611933 -4.25482][-4.1986895 -4.2057157 -4.2030172 -4.193327 -4.1736722 -4.1552324 -4.1618719 -4.1832161 -4.2035956 -4.2217946 -4.2367363 -4.2434163 -4.2421246 -4.2435079 -4.237546][-4.1822739 -4.1979108 -4.1965814 -4.184104 -4.159759 -4.1315637 -4.1245341 -4.1485233 -4.1732278 -4.1938057 -4.215694 -4.227695 -4.2218471 -4.2243376 -4.2265139][-4.1556654 -4.180913 -4.179915 -4.1631031 -4.1284089 -4.0777879 -4.0418849 -4.0694857 -4.1136947 -4.14329 -4.1730022 -4.1917763 -4.1787496 -4.1782937 -4.1877542][-4.1075945 -4.1351414 -4.1302505 -4.1053128 -4.058177 -3.9844241 -3.9213316 -3.9542787 -4.0238481 -4.0673375 -4.1021051 -4.1240487 -4.1066527 -4.1044827 -4.1231079][-4.0706167 -4.094481 -4.0914478 -4.0687213 -4.0265422 -3.9637773 -3.9050624 -3.9300716 -3.9976771 -4.044292 -4.0744891 -4.0910797 -4.0781322 -4.0793638 -4.1006079][-4.0802526 -4.0992174 -4.10168 -4.0903578 -4.0696936 -4.0436068 -4.015264 -4.02485 -4.0677223 -4.1046395 -4.1221576 -4.1273026 -4.117908 -4.1183324 -4.1343093][-4.1271935 -4.1393132 -4.1436396 -4.1395736 -4.1340771 -4.1313987 -4.1242871 -4.1264906 -4.1481333 -4.1695061 -4.1758189 -4.1757936 -4.1698532 -4.1677537 -4.1768146][-4.1843209 -4.1875262 -4.1875691 -4.1854162 -4.1858978 -4.1919613 -4.1927204 -4.1910839 -4.1995597 -4.210557 -4.2145238 -4.2169342 -4.2161345 -4.2126021 -4.2152491][-4.2221622 -4.2202954 -4.2182331 -4.2175078 -4.2195387 -4.2253103 -4.227603 -4.2252059 -4.227169 -4.2327485 -4.2361455 -4.2414775 -4.2457318 -4.2444592 -4.2432528][-4.2460675 -4.2413363 -4.2390332 -4.2383995 -4.2393303 -4.24174 -4.2409344 -4.2337675 -4.2290983 -4.2309194 -4.2350545 -4.2431445 -4.252449 -4.2567267 -4.2570381][-4.2666936 -4.2620564 -4.2585487 -4.2545133 -4.2501416 -4.2462764 -4.2388825 -4.2238874 -4.2123384 -4.2108307 -4.2151818 -4.2262192 -4.2408285 -4.2519569 -4.258038]]...]
INFO - root - 2017-12-06 04:10:49.152840: step 19710, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.226 sec/batch; 19h:40m:12s remains)
INFO - root - 2017-12-06 04:10:51.517053: step 19720, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 19h:54m:25s remains)
INFO - root - 2017-12-06 04:10:53.849962: step 19730, loss = 2.05, batch loss = 1.99 (36.0 examples/sec; 0.222 sec/batch; 19h:19m:41s remains)
INFO - root - 2017-12-06 04:10:56.133224: step 19740, loss = 2.08, batch loss = 2.03 (34.5 examples/sec; 0.232 sec/batch; 20h:10m:09s remains)
INFO - root - 2017-12-06 04:10:58.428466: step 19750, loss = 2.06, batch loss = 2.00 (33.7 examples/sec; 0.237 sec/batch; 20h:37m:13s remains)
INFO - root - 2017-12-06 04:11:00.694867: step 19760, loss = 2.07, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:22m:03s remains)
INFO - root - 2017-12-06 04:11:03.054837: step 19770, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 19h:43m:41s remains)
INFO - root - 2017-12-06 04:11:05.354265: step 19780, loss = 2.12, batch loss = 2.06 (35.6 examples/sec; 0.225 sec/batch; 19h:32m:15s remains)
INFO - root - 2017-12-06 04:11:07.663247: step 19790, loss = 2.05, batch loss = 1.99 (34.5 examples/sec; 0.232 sec/batch; 20h:07m:45s remains)
INFO - root - 2017-12-06 04:11:09.946450: step 19800, loss = 2.10, batch loss = 2.04 (34.3 examples/sec; 0.233 sec/batch; 20h:13m:50s remains)
2017-12-06 04:11:10.223609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1548853 -4.1668696 -4.1707597 -4.173902 -4.1897988 -4.20107 -4.2085214 -4.2244525 -4.2409253 -4.2419214 -4.2219629 -4.1815195 -4.1305876 -4.1052876 -4.1203933][-4.1636572 -4.1677694 -4.1669779 -4.1666489 -4.1782846 -4.1853576 -4.1866293 -4.2002511 -4.2224641 -4.2305713 -4.213006 -4.1658134 -4.0949984 -4.0492783 -4.0629115][-4.2008061 -4.1960616 -4.1867933 -4.1782975 -4.1772342 -4.1737146 -4.1645703 -4.1688347 -4.1914053 -4.2096481 -4.2022119 -4.1528473 -4.0674973 -4.0050678 -4.01929][-4.2302585 -4.2131653 -4.1950531 -4.1804767 -4.1687803 -4.158411 -4.1411691 -4.1334438 -4.1551619 -4.188695 -4.198988 -4.1580429 -4.067647 -3.9925776 -3.997807][-4.2390037 -4.2096081 -4.1818109 -4.1629167 -4.148705 -4.1382184 -4.1149449 -4.0926495 -4.1077347 -4.1523161 -4.185267 -4.1643662 -4.0869093 -4.0068226 -3.993845][-4.237607 -4.1949964 -4.1525097 -4.1275988 -4.113493 -4.1024008 -4.0752449 -4.0422778 -4.044796 -4.0949283 -4.1525517 -4.1583691 -4.1067019 -4.0350232 -4.0014277][-4.23782 -4.1886334 -4.135849 -4.1047382 -4.0846081 -4.0667024 -4.037499 -4.0091014 -4.0119944 -4.0687218 -4.1428509 -4.1661882 -4.1354485 -4.0716715 -4.0186715][-4.2367773 -4.1917014 -4.1401682 -4.1040268 -4.07437 -4.0495973 -4.0185089 -4.0023947 -4.0116949 -4.0663176 -4.1448331 -4.1785011 -4.1607008 -4.1013327 -4.0383372][-4.2387304 -4.2021608 -4.1546793 -4.1143136 -4.0717716 -4.0360775 -4.0015616 -3.9946194 -4.0122604 -4.0639968 -4.144743 -4.1925282 -4.1879511 -4.1293044 -4.0604639][-4.2440372 -4.2140894 -4.1706634 -4.1290579 -4.0767422 -4.0320797 -3.9991302 -4.0013056 -4.027802 -4.0795503 -4.1590919 -4.2114868 -4.2110081 -4.1504555 -4.0806131][-4.2472734 -4.2251248 -4.1928039 -4.157434 -4.1007118 -4.0521064 -4.0228004 -4.0292315 -4.0599346 -4.1082716 -4.180634 -4.2250576 -4.2159243 -4.1540093 -4.0848579][-4.2382646 -4.2262697 -4.205687 -4.183578 -4.134244 -4.0876913 -4.0644722 -4.0702291 -4.0998363 -4.1423039 -4.2004185 -4.2352271 -4.2201872 -4.1565113 -4.090621][-4.2226338 -4.2203803 -4.2106347 -4.2006736 -4.1645761 -4.1276765 -4.1127281 -4.1201043 -4.1434708 -4.1750569 -4.215158 -4.2387543 -4.2170787 -4.1527686 -4.0927916][-4.2096109 -4.2145534 -4.2146535 -4.2122655 -4.1885138 -4.1644897 -4.1556635 -4.157362 -4.1704865 -4.1855831 -4.2043223 -4.220058 -4.1979427 -4.1371317 -4.0824451][-4.19281 -4.2000127 -4.2071676 -4.21314 -4.2026963 -4.19191 -4.1898642 -4.186687 -4.1904368 -4.1895113 -4.1879177 -4.1957479 -4.1768193 -4.1218882 -4.0694122]]...]
INFO - root - 2017-12-06 04:11:12.491389: step 19810, loss = 2.07, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 20h:14m:45s remains)
INFO - root - 2017-12-06 04:11:14.786700: step 19820, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 19h:20m:07s remains)
INFO - root - 2017-12-06 04:11:17.071539: step 19830, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 20h:04m:14s remains)
INFO - root - 2017-12-06 04:11:19.366540: step 19840, loss = 2.06, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 19h:58m:27s remains)
INFO - root - 2017-12-06 04:11:21.687390: step 19850, loss = 2.05, batch loss = 1.99 (33.1 examples/sec; 0.242 sec/batch; 20h:58m:47s remains)
INFO - root - 2017-12-06 04:11:23.995914: step 19860, loss = 2.05, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 20h:01m:23s remains)
INFO - root - 2017-12-06 04:11:26.295525: step 19870, loss = 2.07, batch loss = 2.01 (36.3 examples/sec; 0.220 sec/batch; 19h:06m:53s remains)
INFO - root - 2017-12-06 04:11:28.616084: step 19880, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 19h:53m:38s remains)
INFO - root - 2017-12-06 04:11:30.942117: step 19890, loss = 2.10, batch loss = 2.04 (35.3 examples/sec; 0.227 sec/batch; 19h:40m:42s remains)
INFO - root - 2017-12-06 04:11:33.295961: step 19900, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 19h:37m:06s remains)
2017-12-06 04:11:33.574669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0308294 -3.995321 -3.995846 -4.0290112 -4.0729008 -4.0959253 -4.1084113 -4.1207352 -4.1274385 -4.1345148 -4.1451597 -4.153789 -4.1505704 -4.1392689 -4.1368785][-4.0452676 -4.030334 -4.0432234 -4.0700479 -4.1015406 -4.1145167 -4.1174393 -4.1208692 -4.1233225 -4.1273417 -4.1330147 -4.1394043 -4.1390343 -4.1330857 -4.1390266][-4.06786 -4.067081 -4.0839939 -4.10209 -4.1242476 -4.1347151 -4.132875 -4.1235318 -4.1168542 -4.1128526 -4.1031876 -4.0966926 -4.0956759 -4.0958419 -4.1088319][-4.0984745 -4.1033931 -4.1224222 -4.1390362 -4.1533003 -4.156991 -4.1490827 -4.1287932 -4.1054649 -4.0857544 -4.0598869 -4.0354156 -4.032124 -4.0418582 -4.059885][-4.1160693 -4.12079 -4.1422749 -4.1600237 -4.1657577 -4.1572728 -4.1408734 -4.1140218 -4.0811563 -4.0502653 -4.0186768 -3.9845612 -3.9755259 -3.9894462 -4.0111117][-4.1132593 -4.1134653 -4.1347079 -4.1527543 -4.1508846 -4.1291938 -4.1029267 -4.0739894 -4.0383024 -4.006649 -3.9829535 -3.9556053 -3.9492307 -3.9609244 -3.9764204][-4.1031675 -4.0978413 -4.1122355 -4.1247716 -4.1122336 -4.0776324 -4.038764 -4.01074 -3.9898596 -3.9734991 -3.9668062 -3.9564657 -3.9591494 -3.9669685 -3.9758356][-4.0872407 -4.0722027 -4.0756083 -4.078774 -4.0596776 -4.0207381 -3.978615 -3.9589567 -3.9593251 -3.9655023 -3.9788394 -3.9839637 -3.9895198 -3.9936085 -4.0009875][-4.0613623 -4.0381556 -4.036346 -4.0386691 -4.0229363 -3.9942038 -3.9683516 -3.9625587 -3.9734895 -3.984699 -4.0022163 -4.0100613 -4.0155849 -4.0185118 -4.0226479][-4.0346627 -4.0154247 -4.0192661 -4.0266585 -4.0192933 -4.00697 -3.9992032 -4.0010576 -4.0068188 -4.0062447 -4.0137587 -4.0193839 -4.0262766 -4.028409 -4.0301552][-4.0213652 -4.01289 -4.0248313 -4.0373225 -4.0360479 -4.0311589 -4.0300674 -4.0318751 -4.0312443 -4.0219665 -4.0197258 -4.0218468 -4.0315237 -4.0369706 -4.0407667][-4.0443096 -4.0471435 -4.06717 -4.0846405 -4.087121 -4.0832915 -4.08205 -4.0813122 -4.0784345 -4.0671196 -4.0594144 -4.0577211 -4.0663033 -4.0737967 -4.0788565][-4.1136045 -4.1249595 -4.1493859 -4.1679749 -4.1720252 -4.16656 -4.1609554 -4.156466 -4.152247 -4.1421685 -4.1342931 -4.1310968 -4.1370912 -4.1422687 -4.1478477][-4.1998963 -4.2155805 -4.2371435 -4.2529006 -4.2550969 -4.2479081 -4.2407956 -4.2345014 -4.2302451 -4.2215509 -4.2154489 -4.2132611 -4.21634 -4.2188115 -4.2244091][-4.273675 -4.2875566 -4.3034353 -4.3138037 -4.3121061 -4.3040571 -4.2968488 -4.2897382 -4.2864895 -4.2815971 -4.2790527 -4.2798452 -4.2830129 -4.2853923 -4.2898974]]...]
INFO - root - 2017-12-06 04:11:35.882524: step 19910, loss = 2.07, batch loss = 2.02 (36.0 examples/sec; 0.222 sec/batch; 19h:17m:03s remains)
INFO - root - 2017-12-06 04:11:38.215338: step 19920, loss = 2.03, batch loss = 1.97 (32.1 examples/sec; 0.249 sec/batch; 21h:38m:14s remains)
INFO - root - 2017-12-06 04:11:40.464508: step 19930, loss = 2.04, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 19h:28m:51s remains)
INFO - root - 2017-12-06 04:11:42.775971: step 19940, loss = 2.09, batch loss = 2.03 (35.5 examples/sec; 0.226 sec/batch; 19h:34m:48s remains)
INFO - root - 2017-12-06 04:11:45.053134: step 19950, loss = 2.05, batch loss = 1.99 (34.5 examples/sec; 0.232 sec/batch; 20h:09m:11s remains)
INFO - root - 2017-12-06 04:11:47.365629: step 19960, loss = 2.06, batch loss = 2.00 (34.5 examples/sec; 0.232 sec/batch; 20h:07m:56s remains)
INFO - root - 2017-12-06 04:11:49.680312: step 19970, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:28m:07s remains)
INFO - root - 2017-12-06 04:11:51.999643: step 19980, loss = 2.04, batch loss = 1.98 (34.3 examples/sec; 0.233 sec/batch; 20h:14m:31s remains)
INFO - root - 2017-12-06 04:11:54.328925: step 19990, loss = 2.04, batch loss = 1.98 (35.6 examples/sec; 0.225 sec/batch; 19h:31m:42s remains)
INFO - root - 2017-12-06 04:11:56.646308: step 20000, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.226 sec/batch; 19h:34m:44s remains)
2017-12-06 04:11:56.947960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3099794 -4.2946382 -4.2846179 -4.2860579 -4.2962012 -4.3118896 -4.3280053 -4.3411956 -4.3501744 -4.3512836 -4.3420897 -4.3282986 -4.3174205 -4.313673 -4.3190589][-4.29749 -4.2722878 -4.2498531 -4.2406225 -4.2466283 -4.2689905 -4.3004055 -4.3252635 -4.3434858 -4.3515692 -4.3453631 -4.3296971 -4.3123646 -4.3041906 -4.3103528][-4.282146 -4.2453637 -4.2055449 -4.1767497 -4.1692433 -4.1947012 -4.24752 -4.2935791 -4.3279428 -4.34683 -4.3450041 -4.3294477 -4.3085675 -4.296236 -4.3022971][-4.2712355 -4.2282095 -4.1725984 -4.1235886 -4.0968456 -4.1127133 -4.1741753 -4.2398148 -4.2967 -4.3331456 -4.3414168 -4.327651 -4.3044271 -4.2888417 -4.2930841][-4.2661109 -4.2235074 -4.1607618 -4.0951161 -4.0482917 -4.0463715 -4.0939503 -4.1609282 -4.2364874 -4.2984252 -4.3267336 -4.3247261 -4.30425 -4.2861924 -4.2861071][-4.2649083 -4.2238336 -4.1611614 -4.0866838 -4.0215368 -3.992557 -4.0074553 -4.0547028 -4.1378632 -4.2301822 -4.2904367 -4.313591 -4.3082972 -4.2933283 -4.2881017][-4.2735949 -4.2346897 -4.1746669 -4.0950184 -4.0081959 -3.9395018 -3.9067395 -3.9230876 -4.0100074 -4.132544 -4.2272811 -4.2836442 -4.3070912 -4.3068438 -4.3006754][-4.2844343 -4.2524557 -4.2020073 -4.1255989 -4.0239291 -3.9163818 -3.8315613 -3.8131261 -3.9012556 -4.0415883 -4.1558447 -4.2354712 -4.2870069 -4.3093328 -4.3115973][-4.2933097 -4.2730551 -4.2403784 -4.1831856 -4.094686 -3.9836311 -3.8729753 -3.8216062 -3.8831978 -4.0037613 -4.1052613 -4.1839328 -4.2483826 -4.2906761 -4.3085132][-4.297214 -4.2869492 -4.2733922 -4.2421021 -4.1830287 -4.0988641 -4.0076685 -3.9489617 -3.9626546 -4.0259833 -4.0900154 -4.1503215 -4.211236 -4.2620726 -4.2940822][-4.2926974 -4.2884517 -4.2848673 -4.2735906 -4.2423491 -4.1885238 -4.1262894 -4.0777712 -4.0627494 -4.0788693 -4.1072149 -4.1464691 -4.1947904 -4.2413092 -4.27942][-4.279634 -4.2821174 -4.2842779 -4.2813916 -4.2646327 -4.2282805 -4.1842952 -4.1494684 -4.1299706 -4.1280541 -4.139204 -4.1631474 -4.1985464 -4.2343154 -4.2693114][-4.2690148 -4.2777638 -4.2850151 -4.2872653 -4.2772379 -4.2490959 -4.2130618 -4.184576 -4.1674371 -4.1639051 -4.1712093 -4.1861253 -4.2096677 -4.2348323 -4.26305][-4.2663989 -4.2793512 -4.2911482 -4.2969584 -4.2925081 -4.2736688 -4.2448134 -4.2197676 -4.20391 -4.2000923 -4.2044067 -4.2132454 -4.2272358 -4.2440524 -4.2658076][-4.270977 -4.2856851 -4.2976003 -4.3038244 -4.3043723 -4.2972031 -4.27987 -4.2610145 -4.2465105 -4.241518 -4.2438722 -4.2480164 -4.2536759 -4.2626705 -4.2782984]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 04:11:59.678441: step 20010, loss = 2.05, batch loss = 2.00 (34.4 examples/sec; 0.232 sec/batch; 20h:10m:44s remains)
INFO - root - 2017-12-06 04:12:01.993075: step 20020, loss = 2.03, batch loss = 1.97 (34.9 examples/sec; 0.229 sec/batch; 19h:52m:21s remains)
INFO - root - 2017-12-06 04:12:04.300718: step 20030, loss = 2.09, batch loss = 2.03 (35.3 examples/sec; 0.227 sec/batch; 19h:39m:45s remains)
INFO - root - 2017-12-06 04:12:06.569842: step 20040, loss = 2.07, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 19h:54m:10s remains)
INFO - root - 2017-12-06 04:12:08.868907: step 20050, loss = 2.05, batch loss = 1.99 (35.3 examples/sec; 0.227 sec/batch; 19h:41m:43s remains)
INFO - root - 2017-12-06 04:12:11.223715: step 20060, loss = 2.05, batch loss = 1.99 (35.2 examples/sec; 0.227 sec/batch; 19h:44m:25s remains)
INFO - root - 2017-12-06 04:12:13.550953: step 20070, loss = 2.08, batch loss = 2.02 (34.4 examples/sec; 0.232 sec/batch; 20h:09m:59s remains)
INFO - root - 2017-12-06 04:12:15.894038: step 20080, loss = 2.08, batch loss = 2.02 (36.0 examples/sec; 0.222 sec/batch; 19h:16m:26s remains)
INFO - root - 2017-12-06 04:12:18.157091: step 20090, loss = 2.06, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 19h:01m:43s remains)
INFO - root - 2017-12-06 04:12:20.472541: step 20100, loss = 2.08, batch loss = 2.03 (35.5 examples/sec; 0.226 sec/batch; 19h:34m:31s remains)
2017-12-06 04:12:20.750057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1953311 -4.2117147 -4.22734 -4.2300844 -4.2285132 -4.223103 -4.2156014 -4.212431 -4.2139673 -4.2257886 -4.2428145 -4.256248 -4.2725086 -4.2951212 -4.3118668][-4.1418114 -4.1588941 -4.1797476 -4.1884513 -4.1890125 -4.1808968 -4.170351 -4.1649704 -4.1671247 -4.1867161 -4.214035 -4.2337122 -4.2547579 -4.2796082 -4.2983031][-4.104104 -4.1147938 -4.1367416 -4.1533504 -4.157795 -4.1525092 -4.1483479 -4.1434712 -4.1479406 -4.1715093 -4.2035861 -4.2264004 -4.2504878 -4.2782021 -4.2971196][-4.0776725 -4.0817094 -4.101326 -4.1160889 -4.1180439 -4.1163821 -4.12432 -4.1268005 -4.1419163 -4.1717477 -4.2039986 -4.2266994 -4.250268 -4.2793207 -4.3008704][-4.063591 -4.0735574 -4.0924444 -4.0987835 -4.0912275 -4.0869713 -4.1001005 -4.1141825 -4.1367488 -4.173017 -4.2073674 -4.2297592 -4.2517438 -4.2789187 -4.3031516][-4.0729771 -4.0880642 -4.1050944 -4.1000853 -4.0806823 -4.0672288 -4.0757279 -4.0983047 -4.1283393 -4.1639848 -4.2038975 -4.2322459 -4.2537775 -4.2766438 -4.2980189][-4.0875349 -4.099195 -4.1160235 -4.10898 -4.0818491 -4.0547924 -4.05226 -4.0825558 -4.1188655 -4.1528883 -4.1957946 -4.2281303 -4.2499728 -4.269176 -4.2871013][-4.083333 -4.0973244 -4.1198368 -4.1192603 -4.0870209 -4.04882 -4.0372438 -4.0654078 -4.1049271 -4.1419606 -4.18842 -4.223505 -4.2461758 -4.26348 -4.2801766][-4.06565 -4.0815806 -4.1085954 -4.1195416 -4.0968332 -4.065804 -4.05784 -4.0816822 -4.1190271 -4.1568184 -4.2013154 -4.2339754 -4.2545576 -4.2691383 -4.2821555][-4.0557933 -4.0679774 -4.0874677 -4.1064243 -4.1039019 -4.0905156 -4.091763 -4.1132593 -4.1487532 -4.182445 -4.22104 -4.2526736 -4.2728577 -4.2864242 -4.2926517][-4.0678539 -4.0727873 -4.0814276 -4.0940051 -4.1045952 -4.1079679 -4.1180048 -4.1392555 -4.1768155 -4.2091637 -4.2432718 -4.27418 -4.2960577 -4.3103819 -4.3084135][-4.1108861 -4.1147709 -4.1179829 -4.1207318 -4.1287804 -4.1361303 -4.1446748 -4.1625557 -4.1967669 -4.2259421 -4.2574024 -4.289494 -4.3188415 -4.3368483 -4.3302326][-4.1478424 -4.1577764 -4.1624012 -4.1598697 -4.1595526 -4.160038 -4.1639237 -4.1776533 -4.2071562 -4.2366776 -4.2667432 -4.2976894 -4.3292785 -4.348125 -4.3436689][-4.1760187 -4.1886525 -4.1934915 -4.1931753 -4.1890769 -4.182797 -4.1867647 -4.20063 -4.2233577 -4.2472897 -4.27313 -4.3001809 -4.3288794 -4.3451242 -4.3434482][-4.1928997 -4.2059894 -4.2099066 -4.2149386 -4.2138305 -4.2055459 -4.20934 -4.2251663 -4.24335 -4.2601361 -4.2798953 -4.3029413 -4.3249145 -4.3366618 -4.3377495]]...]
INFO - root - 2017-12-06 04:12:23.068304: step 20110, loss = 2.07, batch loss = 2.01 (33.6 examples/sec; 0.238 sec/batch; 20h:40m:40s remains)
INFO - root - 2017-12-06 04:12:25.383655: step 20120, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 19h:31m:44s remains)
INFO - root - 2017-12-06 04:12:27.683722: step 20130, loss = 2.06, batch loss = 2.00 (33.8 examples/sec; 0.237 sec/batch; 20h:32m:13s remains)
INFO - root - 2017-12-06 04:12:29.983065: step 20140, loss = 2.08, batch loss = 2.02 (33.6 examples/sec; 0.238 sec/batch; 20h:38m:40s remains)
INFO - root - 2017-12-06 04:12:32.258755: step 20150, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 19h:57m:45s remains)
INFO - root - 2017-12-06 04:12:34.554500: step 20160, loss = 2.08, batch loss = 2.02 (34.4 examples/sec; 0.232 sec/batch; 20h:09m:33s remains)
INFO - root - 2017-12-06 04:12:36.817765: step 20170, loss = 2.05, batch loss = 1.99 (36.8 examples/sec; 0.217 sec/batch; 18h:50m:20s remains)
INFO - root - 2017-12-06 04:12:39.118488: step 20180, loss = 2.06, batch loss = 2.00 (33.7 examples/sec; 0.237 sec/batch; 20h:35m:30s remains)
INFO - root - 2017-12-06 04:12:41.428112: step 20190, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 19h:32m:59s remains)
INFO - root - 2017-12-06 04:12:43.757357: step 20200, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.231 sec/batch; 20h:00m:31s remains)
2017-12-06 04:12:44.037072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1768465 -4.1918316 -4.1914053 -4.1725388 -4.1518106 -4.1272745 -4.0909967 -4.0601954 -4.0542226 -4.0716739 -4.0929871 -4.1117854 -4.127583 -4.1286688 -4.1317873][-4.1885104 -4.1994052 -4.1896176 -4.1581984 -4.1284027 -4.1052585 -4.0843153 -4.0741248 -4.0838594 -4.109983 -4.1257415 -4.1270885 -4.1228809 -4.1125746 -4.1087666][-4.1965427 -4.1981134 -4.1755471 -4.1331043 -4.0897937 -4.062037 -4.0542407 -4.0673246 -4.0947113 -4.1277843 -4.145463 -4.1342163 -4.1084619 -4.0856991 -4.0810828][-4.2070117 -4.1940551 -4.1563492 -4.1061912 -4.0498147 -4.0054765 -4.0007148 -4.0306869 -4.076086 -4.1189518 -4.1435509 -4.1411457 -4.1238661 -4.1020751 -4.0978951][-4.2059188 -4.18258 -4.1341434 -4.0748062 -3.998728 -3.9276798 -3.90867 -3.9526978 -4.0198159 -4.0831718 -4.12488 -4.1428046 -4.1445665 -4.1318293 -4.1301479][-4.1955824 -4.1684308 -4.1153564 -4.0427923 -3.9441583 -3.8352382 -3.7854929 -3.8459299 -3.9440789 -4.0336432 -4.0989594 -4.135282 -4.1467977 -4.144217 -4.1438704][-4.1736212 -4.1514387 -4.1028609 -4.0247345 -3.917233 -3.7761307 -3.682852 -3.7440176 -3.8697879 -3.9761586 -4.0568767 -4.1003327 -4.1124067 -4.1225815 -4.1367464][-4.1520419 -4.1423697 -4.1123605 -4.0413504 -3.9348719 -3.7867675 -3.6667473 -3.6995006 -3.8273723 -3.937247 -4.0167565 -4.0559826 -4.0653586 -4.0824642 -4.1116915][-4.1365509 -4.1456275 -4.1412015 -4.0931091 -4.0065637 -3.8891273 -3.79135 -3.7840223 -3.8740735 -3.9666975 -4.0180631 -4.0358968 -4.03727 -4.0420856 -4.0699935][-4.1080155 -4.1305852 -4.1503119 -4.1341138 -4.0825419 -4.0180922 -3.96994 -3.9426315 -3.9812484 -4.0436993 -4.0651803 -4.0548019 -4.0394378 -4.0213985 -4.0304794][-4.0639477 -4.0939136 -4.1296344 -4.1405811 -4.1166778 -4.0907354 -4.0834508 -4.0628829 -4.0751247 -4.1132922 -4.1176214 -4.0942068 -4.0690465 -4.0439248 -4.0346847][-4.0401974 -4.070097 -4.1075373 -4.1290679 -4.1211362 -4.1110811 -4.1209431 -4.1188216 -4.1312017 -4.1562686 -4.1508956 -4.1234803 -4.097805 -4.0815558 -4.0744519][-4.0615859 -4.0853124 -4.1086187 -4.1179438 -4.1113958 -4.1059413 -4.125401 -4.1436534 -4.1602855 -4.1765342 -4.16922 -4.1425714 -4.1177554 -4.1061153 -4.1169477][-4.109179 -4.1217442 -4.1224966 -4.1073895 -4.0962424 -4.0942135 -4.11625 -4.1426787 -4.162631 -4.1755481 -4.170023 -4.1461868 -4.1261797 -4.1228218 -4.1521025][-4.1509466 -4.1500311 -4.132102 -4.1027308 -4.0901742 -4.09152 -4.10985 -4.1343269 -4.1519537 -4.1609583 -4.1611767 -4.1462183 -4.1342125 -4.1409197 -4.1755786]]...]
INFO - root - 2017-12-06 04:12:46.334764: step 20210, loss = 2.08, batch loss = 2.02 (34.6 examples/sec; 0.231 sec/batch; 20h:04m:25s remains)
INFO - root - 2017-12-06 04:12:48.618676: step 20220, loss = 2.05, batch loss = 1.99 (35.4 examples/sec; 0.226 sec/batch; 19h:36m:16s remains)
INFO - root - 2017-12-06 04:12:50.964409: step 20230, loss = 2.06, batch loss = 2.00 (35.7 examples/sec; 0.224 sec/batch; 19h:26m:41s remains)
INFO - root - 2017-12-06 04:12:53.256467: step 20240, loss = 2.09, batch loss = 2.03 (33.8 examples/sec; 0.237 sec/batch; 20h:33m:27s remains)
INFO - root - 2017-12-06 04:12:55.542855: step 20250, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 19h:25m:01s remains)
INFO - root - 2017-12-06 04:12:57.876613: step 20260, loss = 2.08, batch loss = 2.02 (34.1 examples/sec; 0.235 sec/batch; 20h:20m:50s remains)
INFO - root - 2017-12-06 04:13:00.168115: step 20270, loss = 2.08, batch loss = 2.02 (36.0 examples/sec; 0.222 sec/batch; 19h:17m:24s remains)
INFO - root - 2017-12-06 04:13:02.485532: step 20280, loss = 2.08, batch loss = 2.02 (32.1 examples/sec; 0.249 sec/batch; 21h:37m:58s remains)
INFO - root - 2017-12-06 04:13:04.790588: step 20290, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 20h:02m:38s remains)
INFO - root - 2017-12-06 04:13:07.091828: step 20300, loss = 2.09, batch loss = 2.03 (34.1 examples/sec; 0.234 sec/batch; 20h:19m:33s remains)
2017-12-06 04:13:07.380658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3096166 -4.3140206 -4.3096738 -4.3008761 -4.2870784 -4.2766228 -4.2748208 -4.2756066 -4.2696929 -4.263443 -4.26598 -4.2785811 -4.2919245 -4.2981205 -4.3003626][-4.3036761 -4.3071003 -4.2985086 -4.2850194 -4.2658858 -4.2501955 -4.2413263 -4.2399249 -4.2361345 -4.2303872 -4.2351041 -4.2545314 -4.2784543 -4.2951179 -4.3008723][-4.2742138 -4.2732306 -4.2632437 -4.2469611 -4.2245779 -4.2037592 -4.1888309 -4.1848197 -4.1852179 -4.1842847 -4.1911011 -4.2104292 -4.2415023 -4.2710266 -4.2849889][-4.2267761 -4.2217336 -4.2103772 -4.189723 -4.1634297 -4.1401477 -4.12017 -4.1106963 -4.115788 -4.1210008 -4.1287694 -4.1478105 -4.1841478 -4.2297535 -4.255765][-4.1812072 -4.1724157 -4.1619134 -4.1381812 -4.1081543 -4.0754204 -4.0414052 -4.0183039 -4.0208058 -4.0347524 -4.0472555 -4.073729 -4.1231689 -4.1842432 -4.2220588][-4.1465797 -4.1312776 -4.1211724 -4.0991073 -4.06618 -4.0196123 -3.9595084 -3.9026957 -3.8986607 -3.9280295 -3.9529696 -3.9935803 -4.0626664 -4.1408706 -4.193645][-4.115417 -4.0972772 -4.08292 -4.06215 -4.028224 -3.9626436 -3.8707342 -3.7913623 -3.7934558 -3.8440537 -3.8873537 -3.9388354 -4.019444 -4.1054316 -4.1701517][-4.087265 -4.0710835 -4.0536079 -4.0365529 -4.0114045 -3.9512689 -3.8663571 -3.8077774 -3.8245397 -3.8692307 -3.9022145 -3.9436023 -4.0151448 -4.0939445 -4.1596112][-4.0760241 -4.0673385 -4.0499854 -4.0443912 -4.0409431 -4.0122991 -3.970922 -3.9483111 -3.9609597 -3.9710207 -3.9722636 -3.9898827 -4.0444083 -4.10891 -4.1698604][-4.0998669 -4.0884337 -4.0699625 -4.0742111 -4.0906458 -4.0914621 -4.0870609 -4.0845861 -4.0868597 -4.0760026 -4.0579 -4.0604239 -4.0992532 -4.1503863 -4.2027087][-4.1500568 -4.1281233 -4.1021385 -4.1018925 -4.1228123 -4.1433411 -4.16146 -4.16873 -4.1683879 -4.15814 -4.1433454 -4.1446095 -4.1696162 -4.2056041 -4.2417388][-4.1909709 -4.164854 -4.1358757 -4.1309166 -4.1483059 -4.1765642 -4.201798 -4.2179751 -4.2249417 -4.226295 -4.222569 -4.2246776 -4.238492 -4.2560086 -4.2753434][-4.2200274 -4.20079 -4.1781907 -4.1734614 -4.1892686 -4.2134948 -4.2358727 -4.2538018 -4.26488 -4.2676492 -4.2660236 -4.268189 -4.2799978 -4.2886171 -4.2960958][-4.2454777 -4.2336216 -4.2186751 -4.2133622 -4.2247863 -4.2429066 -4.2601566 -4.2737269 -4.2791448 -4.2757263 -4.2706676 -4.2718263 -4.2859492 -4.2933216 -4.2958536][-4.2627554 -4.2543025 -4.2453914 -4.2409225 -4.2484593 -4.2592 -4.2691631 -4.2740512 -4.2705951 -4.2623429 -4.2539916 -4.2530966 -4.2667732 -4.2770729 -4.2802625]]...]
INFO - root - 2017-12-06 04:13:09.681619: step 20310, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 19h:44m:41s remains)
INFO - root - 2017-12-06 04:13:11.999382: step 20320, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:45m:17s remains)
INFO - root - 2017-12-06 04:13:14.320385: step 20330, loss = 2.07, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 20h:12m:54s remains)
INFO - root - 2017-12-06 04:13:16.650020: step 20340, loss = 2.05, batch loss = 2.00 (34.8 examples/sec; 0.230 sec/batch; 19h:57m:01s remains)
INFO - root - 2017-12-06 04:13:18.934503: step 20350, loss = 2.05, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 19h:54m:46s remains)
INFO - root - 2017-12-06 04:13:21.300480: step 20360, loss = 2.03, batch loss = 1.97 (33.2 examples/sec; 0.241 sec/batch; 20h:53m:54s remains)
INFO - root - 2017-12-06 04:13:23.643025: step 20370, loss = 2.07, batch loss = 2.01 (33.1 examples/sec; 0.242 sec/batch; 20h:56m:25s remains)
INFO - root - 2017-12-06 04:13:25.952500: step 20380, loss = 2.06, batch loss = 2.00 (32.0 examples/sec; 0.250 sec/batch; 21h:39m:50s remains)
INFO - root - 2017-12-06 04:13:28.253080: step 20390, loss = 2.06, batch loss = 2.00 (32.8 examples/sec; 0.244 sec/batch; 21h:10m:20s remains)
INFO - root - 2017-12-06 04:13:30.560711: step 20400, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 20h:01m:35s remains)
2017-12-06 04:13:30.869772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.260334 -4.259016 -4.2564135 -4.2547789 -4.2549691 -4.255415 -4.254724 -4.2530079 -4.2500086 -4.2482185 -4.2485538 -4.2508149 -4.2550473 -4.2598491 -4.2625871][-4.2557721 -4.2545433 -4.2508254 -4.2492623 -4.2496457 -4.2479615 -4.2441111 -4.2409053 -4.2377062 -4.2352629 -4.2350473 -4.2389965 -4.2475371 -4.2568922 -4.2620878][-4.2500439 -4.2487259 -4.2428088 -4.2400045 -4.2387624 -4.2326031 -4.2242165 -4.2213764 -4.2204642 -4.2168746 -4.2147064 -4.2206473 -4.2342434 -4.2486367 -4.256855][-4.2339745 -4.2300019 -4.2195039 -4.213171 -4.2094684 -4.19858 -4.184957 -4.18174 -4.1849227 -4.1823125 -4.1795311 -4.1901574 -4.2104425 -4.2307973 -4.2441463][-4.206368 -4.1986513 -4.1839747 -4.1723008 -4.1645226 -4.1489849 -4.1304893 -4.1246843 -4.1318054 -4.1316795 -4.1329951 -4.1532516 -4.1814728 -4.2097373 -4.2304254][-4.1714387 -4.156733 -4.1331191 -4.1098232 -4.0952592 -4.075871 -4.0514517 -4.0412927 -4.0523019 -4.0625348 -4.0754595 -4.1089892 -4.1484904 -4.1852589 -4.2141204][-4.1278067 -4.1046844 -4.0712938 -4.0314894 -4.0064645 -3.9878368 -3.965188 -3.9506054 -3.9682927 -3.994348 -4.0274229 -4.0758004 -4.1209111 -4.1614151 -4.197494][-4.0979905 -4.0726576 -4.0377045 -3.9894197 -3.9518602 -3.9361334 -3.9208734 -3.9085512 -3.9326422 -3.9667149 -4.0046778 -4.0593367 -4.1057038 -4.1441278 -4.1838017][-4.1060767 -4.0776792 -4.0469103 -4.0035372 -3.9639461 -3.9488115 -3.9375906 -3.9263103 -3.9472854 -3.9772408 -4.0077782 -4.0601292 -4.1099567 -4.1470933 -4.1838017][-4.1081495 -4.0782757 -4.0525427 -4.018537 -3.9885755 -3.9802091 -3.98068 -3.9773164 -3.9944041 -4.0203323 -4.0412121 -4.0866351 -4.1319561 -4.1649828 -4.1968675][-4.1042833 -4.0833287 -4.0628862 -4.0381975 -4.0220246 -4.0254917 -4.0368457 -4.0424519 -4.0554605 -4.0787311 -4.0989203 -4.1386557 -4.1768556 -4.2025547 -4.2218943][-4.0982165 -4.091001 -4.0793986 -4.0641 -4.0644984 -4.0811377 -4.1020041 -4.1149592 -4.1235318 -4.1400132 -4.1597781 -4.1922007 -4.2226892 -4.2391405 -4.2454462][-4.1028705 -4.1053534 -4.1065979 -4.1070986 -4.1224446 -4.1477628 -4.1698995 -4.1776323 -4.1759162 -4.1808782 -4.1918392 -4.2154264 -4.24203 -4.2557707 -4.257165][-4.1279197 -4.130754 -4.1374493 -4.1513472 -4.1756482 -4.2041936 -4.2222853 -4.2177849 -4.200017 -4.1905727 -4.1933169 -4.2125363 -4.2422934 -4.2588563 -4.2624717][-4.1555371 -4.1566958 -4.1659746 -4.1897955 -4.2201567 -4.2459893 -4.2553425 -4.2355232 -4.1976833 -4.1715236 -4.1677985 -4.1879439 -4.2210836 -4.2423778 -4.2511883]]...]
INFO - root - 2017-12-06 04:13:33.151485: step 20410, loss = 2.05, batch loss = 1.99 (33.7 examples/sec; 0.237 sec/batch; 20h:33m:58s remains)
INFO - root - 2017-12-06 04:13:35.468081: step 20420, loss = 2.09, batch loss = 2.04 (36.1 examples/sec; 0.222 sec/batch; 19h:12m:41s remains)
INFO - root - 2017-12-06 04:13:37.795697: step 20430, loss = 2.06, batch loss = 2.00 (32.3 examples/sec; 0.247 sec/batch; 21h:26m:33s remains)
INFO - root - 2017-12-06 04:13:40.086496: step 20440, loss = 2.04, batch loss = 1.98 (35.5 examples/sec; 0.226 sec/batch; 19h:33m:13s remains)
INFO - root - 2017-12-06 04:13:42.399270: step 20450, loss = 2.05, batch loss = 1.99 (35.8 examples/sec; 0.223 sec/batch; 19h:20m:45s remains)
INFO - root - 2017-12-06 04:13:44.698329: step 20460, loss = 2.04, batch loss = 1.98 (35.3 examples/sec; 0.226 sec/batch; 19h:37m:09s remains)
INFO - root - 2017-12-06 04:13:46.986742: step 20470, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 19h:32m:27s remains)
INFO - root - 2017-12-06 04:13:49.323382: step 20480, loss = 2.07, batch loss = 2.02 (34.3 examples/sec; 0.233 sec/batch; 20h:11m:54s remains)
INFO - root - 2017-12-06 04:13:51.588342: step 20490, loss = 2.05, batch loss = 1.99 (36.7 examples/sec; 0.218 sec/batch; 18h:53m:45s remains)
INFO - root - 2017-12-06 04:13:53.913752: step 20500, loss = 2.04, batch loss = 1.98 (33.6 examples/sec; 0.238 sec/batch; 20h:38m:32s remains)
2017-12-06 04:13:54.214734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1251554 -4.1265311 -4.1247883 -4.134305 -4.1440563 -4.1496143 -4.1539626 -4.149405 -4.1547523 -4.1520915 -4.1498 -4.155282 -4.1496811 -4.1375027 -4.1308689][-4.0997496 -4.1104612 -4.1303039 -4.1546412 -4.1710024 -4.181725 -4.1903896 -4.1889172 -4.1928191 -4.1908946 -4.1798849 -4.1831489 -4.1749287 -4.1626821 -4.1545224][-4.0880466 -4.0961666 -4.127656 -4.1616516 -4.1790161 -4.1877241 -4.1970387 -4.2007685 -4.2068419 -4.2032485 -4.1858559 -4.1844983 -4.1784768 -4.171207 -4.1671352][-4.0999432 -4.1031914 -4.1336241 -4.1621928 -4.1716342 -4.1690421 -4.1687317 -4.1727238 -4.1791692 -4.1722603 -4.1516838 -4.1503692 -4.1524291 -4.1567149 -4.1623354][-4.1297426 -4.1262231 -4.1418762 -4.1508856 -4.1436334 -4.1239324 -4.1105475 -4.1189241 -4.1369119 -4.1317086 -4.1117105 -4.1080756 -4.11416 -4.128839 -4.1392136][-4.1466517 -4.1410236 -4.1381297 -4.1180792 -4.0862088 -4.0460958 -4.0144033 -4.0300536 -4.0694752 -4.0785894 -4.0644541 -4.0601912 -4.0659661 -4.0836134 -4.0979881][-4.1488624 -4.1490221 -4.1318374 -4.0832577 -4.0219831 -3.9439869 -3.8753 -3.9064977 -3.9836726 -4.026145 -4.0337629 -4.0315704 -4.03234 -4.043252 -4.0548553][-4.1617994 -4.1610775 -4.1358652 -4.0763087 -3.991164 -3.8649378 -3.736798 -3.774343 -3.8934126 -3.9769673 -4.0092912 -4.0109768 -4.0071568 -4.00346 -4.0067539][-4.1776452 -4.18209 -4.1619096 -4.1164012 -4.0459871 -3.9320838 -3.8074732 -3.8097444 -3.8932173 -3.9665484 -3.9951835 -3.9937387 -3.9871149 -3.9798539 -3.9821572][-4.1746612 -4.19251 -4.187675 -4.171176 -4.1397185 -4.0727282 -3.9884953 -3.9623568 -3.9916148 -4.0309529 -4.0423245 -4.0332384 -4.0277596 -4.0207162 -4.0145388][-4.1603761 -4.1889033 -4.1977592 -4.206367 -4.2028966 -4.1716194 -4.1223683 -4.0928459 -4.0947776 -4.1090088 -4.1120386 -4.1036024 -4.0994167 -4.0899906 -4.0786662][-4.1424084 -4.1688552 -4.1918473 -4.2187357 -4.2314749 -4.2234464 -4.2021012 -4.1818686 -4.1724124 -4.1724544 -4.1711326 -4.1669259 -4.1637306 -4.151926 -4.1406364][-4.1230826 -4.1369734 -4.1609163 -4.1980624 -4.2233896 -4.2327752 -4.2310362 -4.2242746 -4.2186294 -4.218924 -4.2198973 -4.2239962 -4.2231493 -4.2138376 -4.2028909][-4.12212 -4.1209216 -4.1325917 -4.1636596 -4.189208 -4.2023363 -4.2103791 -4.2205939 -4.2287917 -4.2366533 -4.2440805 -4.2570891 -4.2615714 -4.2549734 -4.2413387][-4.1525288 -4.1422548 -4.1368709 -4.1466246 -4.1552973 -4.1561704 -4.1654067 -4.185864 -4.2081804 -4.2277956 -4.2434897 -4.2615147 -4.2709932 -4.2694721 -4.2600951]]...]
INFO - root - 2017-12-06 04:13:56.519191: step 20510, loss = 2.08, batch loss = 2.02 (36.3 examples/sec; 0.221 sec/batch; 19h:07m:27s remains)
INFO - root - 2017-12-06 04:13:58.829702: step 20520, loss = 2.05, batch loss = 2.00 (33.5 examples/sec; 0.239 sec/batch; 20h:42m:09s remains)
INFO - root - 2017-12-06 04:14:01.130138: step 20530, loss = 2.04, batch loss = 1.99 (34.1 examples/sec; 0.235 sec/batch; 20h:20m:58s remains)
INFO - root - 2017-12-06 04:14:03.446170: step 20540, loss = 2.07, batch loss = 2.01 (34.0 examples/sec; 0.235 sec/batch; 20h:22m:49s remains)
INFO - root - 2017-12-06 04:14:05.746495: step 20550, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 19h:19m:33s remains)
INFO - root - 2017-12-06 04:14:08.054747: step 20560, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.226 sec/batch; 19h:37m:17s remains)
INFO - root - 2017-12-06 04:14:10.353801: step 20570, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 19h:18m:46s remains)
INFO - root - 2017-12-06 04:14:12.645784: step 20580, loss = 2.07, batch loss = 2.01 (31.3 examples/sec; 0.256 sec/batch; 22h:08m:46s remains)
INFO - root - 2017-12-06 04:14:14.967991: step 20590, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 19h:39m:16s remains)
INFO - root - 2017-12-06 04:14:17.259833: step 20600, loss = 2.09, batch loss = 2.03 (35.4 examples/sec; 0.226 sec/batch; 19h:36m:03s remains)
2017-12-06 04:14:17.568175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2396479 -4.245913 -4.2563987 -4.2725639 -4.2800364 -4.2845073 -4.2861586 -4.2918391 -4.3079166 -4.3290563 -4.3366537 -4.3219728 -4.2910991 -4.2602534 -4.2507887][-4.2413769 -4.2436814 -4.2536168 -4.2684569 -4.2736912 -4.2732224 -4.2662764 -4.265029 -4.281002 -4.3077574 -4.3199506 -4.3078651 -4.2794089 -4.2531576 -4.2477236][-4.2195115 -4.2208104 -4.2339907 -4.255806 -4.2689171 -4.2649112 -4.248116 -4.2385812 -4.2560124 -4.2886095 -4.3053737 -4.2973576 -4.2723885 -4.2508545 -4.2469182][-4.1789875 -4.1754494 -4.1927814 -4.2284493 -4.2554884 -4.2533226 -4.22976 -4.2149048 -4.2336106 -4.2710843 -4.2950583 -4.2946162 -4.2757444 -4.2559981 -4.2473316][-4.123982 -4.1127148 -4.13622 -4.1874266 -4.2264652 -4.2278662 -4.1993 -4.1770654 -4.1950107 -4.2391968 -4.2751245 -4.285512 -4.2728853 -4.2540784 -4.2384896][-4.0579944 -4.0401716 -4.0736513 -4.1420217 -4.18955 -4.1902871 -4.153635 -4.1210585 -4.1370277 -4.1893473 -4.238976 -4.2619648 -4.2568526 -4.2418051 -4.2227478][-4.007678 -3.9899833 -4.0330644 -4.1082778 -4.1521273 -4.1437826 -4.0937424 -4.0482969 -4.0633621 -4.1305876 -4.1988354 -4.2370863 -4.2437687 -4.2338052 -4.2130694][-4.0262809 -4.0230575 -4.0614858 -4.1159344 -4.1352415 -4.106823 -4.0408974 -3.987402 -4.0075531 -4.0872674 -4.1679406 -4.2170172 -4.2338972 -4.229321 -4.210393][-4.0894914 -4.0950928 -4.1166029 -4.1389561 -4.1331291 -4.0997763 -4.0446539 -4.0081987 -4.0322461 -4.1019382 -4.1706696 -4.2136908 -4.2324281 -4.230679 -4.2120705][-4.1314259 -4.13393 -4.1412325 -4.1419697 -4.1271462 -4.1079879 -4.0851412 -4.0754375 -4.0956559 -4.1423454 -4.1873956 -4.2152562 -4.2287636 -4.2270231 -4.2069526][-4.1492248 -4.1441913 -4.1397491 -4.1275911 -4.1098557 -4.1014452 -4.1022739 -4.1095409 -4.1288624 -4.1646652 -4.1975789 -4.2167807 -4.2247133 -4.2208648 -4.1991873][-4.1666489 -4.1550684 -4.1416287 -4.1211748 -4.0997071 -4.0942688 -4.1019726 -4.113925 -4.1328826 -4.1651864 -4.1960578 -4.214767 -4.22034 -4.2141943 -4.1915727][-4.184896 -4.1700335 -4.1548839 -4.135067 -4.1137738 -4.10632 -4.1099811 -4.1193757 -4.1360588 -4.1651273 -4.1957078 -4.2137733 -4.2180185 -4.2118883 -4.1907434][-4.1992 -4.1866379 -4.1766963 -4.1629581 -4.1449909 -4.1355071 -4.1341033 -4.1418557 -4.1568694 -4.1813865 -4.2092705 -4.2253475 -4.2291064 -4.2243519 -4.2042418][-4.2075467 -4.1983581 -4.1942477 -4.189424 -4.1799707 -4.1728525 -4.1696234 -4.1765432 -4.1902885 -4.2102132 -4.2321477 -4.2437224 -4.2466583 -4.2421002 -4.221777]]...]
INFO - root - 2017-12-06 04:14:19.835909: step 20610, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 19h:28m:26s remains)
INFO - root - 2017-12-06 04:14:22.144060: step 20620, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.236 sec/batch; 20h:29m:03s remains)
INFO - root - 2017-12-06 04:14:24.459238: step 20630, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 19h:27m:18s remains)
INFO - root - 2017-12-06 04:14:26.755399: step 20640, loss = 2.05, batch loss = 1.99 (35.8 examples/sec; 0.223 sec/batch; 19h:20m:16s remains)
INFO - root - 2017-12-06 04:14:29.036258: step 20650, loss = 2.08, batch loss = 2.02 (35.7 examples/sec; 0.224 sec/batch; 19h:23m:30s remains)
INFO - root - 2017-12-06 04:14:31.368049: step 20660, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:29m:16s remains)
INFO - root - 2017-12-06 04:14:33.675029: step 20670, loss = 2.05, batch loss = 1.99 (33.7 examples/sec; 0.237 sec/batch; 20h:34m:17s remains)
INFO - root - 2017-12-06 04:14:35.963236: step 20680, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:34m:42s remains)
INFO - root - 2017-12-06 04:14:38.255798: step 20690, loss = 2.06, batch loss = 2.01 (34.7 examples/sec; 0.230 sec/batch; 19h:57m:30s remains)
INFO - root - 2017-12-06 04:14:40.587342: step 20700, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 19h:28m:21s remains)
2017-12-06 04:14:40.881198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2624712 -4.2668586 -4.271904 -4.2703114 -4.2543054 -4.2331042 -4.2157197 -4.2004085 -4.1837263 -4.1731529 -4.1619043 -4.1472783 -4.140521 -4.1530008 -4.17264][-4.2657437 -4.2751751 -4.2799568 -4.2718625 -4.2476597 -4.2189012 -4.1960945 -4.1765447 -4.1570296 -4.1487656 -4.1416821 -4.1316829 -4.1327562 -4.155479 -4.1817203][-4.2520714 -4.2647882 -4.2702069 -4.2592487 -4.2338614 -4.20417 -4.1769543 -4.1532559 -4.1344523 -4.1334314 -4.1377153 -4.1386304 -4.14866 -4.1781378 -4.2060471][-4.2260647 -4.2369175 -4.2410731 -4.2306004 -4.2133284 -4.1933722 -4.1695528 -4.1462903 -4.1302295 -4.1336541 -4.150744 -4.1629338 -4.1800613 -4.2119837 -4.2366343][-4.2028437 -4.2080393 -4.2082281 -4.1944494 -4.1831164 -4.1747789 -4.1583824 -4.1414442 -4.1325955 -4.14231 -4.1703758 -4.1915956 -4.2100506 -4.23715 -4.2562714][-4.1922984 -4.1888771 -4.1819892 -4.1627851 -4.1515522 -4.1471181 -4.1319885 -4.1166325 -4.1151581 -4.134305 -4.1732216 -4.2030869 -4.2202816 -4.2402277 -4.2552156][-4.192482 -4.178431 -4.1633925 -4.1400909 -4.12392 -4.1116743 -4.0861807 -4.062325 -4.06386 -4.0920796 -4.1413088 -4.1793003 -4.2007165 -4.2222714 -4.2409658][-4.1997352 -4.1774869 -4.1572466 -4.1318779 -4.1095223 -4.0819416 -4.0382042 -3.9978848 -3.9953761 -4.0286465 -4.0864139 -4.1348238 -4.1679034 -4.1998839 -4.2270021][-4.2066197 -4.1784806 -4.1574645 -4.1330767 -4.1109715 -4.0804276 -4.0340061 -3.9862666 -3.9763167 -4.0052042 -4.0594277 -4.1101117 -4.1456017 -4.1794963 -4.2107706][-4.2112985 -4.1830111 -4.165978 -4.1461463 -4.1289654 -4.1051059 -4.0706367 -4.0314364 -4.0213976 -4.04543 -4.0887613 -4.131825 -4.1616716 -4.1893234 -4.213624][-4.2115664 -4.1896968 -4.178515 -4.1665363 -4.1561751 -4.1400671 -4.1186581 -4.0930023 -4.0854163 -4.1046042 -4.1361094 -4.171072 -4.197031 -4.2190909 -4.2351332][-4.2093182 -4.2009373 -4.1949329 -4.1866541 -4.1806264 -4.1704693 -4.158761 -4.1458116 -4.1419029 -4.1549234 -4.17495 -4.2021079 -4.2253709 -4.24236 -4.2536368][-4.215611 -4.2195597 -4.2179885 -4.210392 -4.2044787 -4.1997428 -4.1935821 -4.1883497 -4.1884103 -4.1970086 -4.2091055 -4.2268815 -4.2446237 -4.2570205 -4.2654948][-4.229075 -4.2416692 -4.2446856 -4.2380061 -4.2287626 -4.2220058 -4.2146187 -4.2105317 -4.2125554 -4.2204924 -4.2290912 -4.2404761 -4.2540421 -4.2644997 -4.2727275][-4.2501049 -4.2616434 -4.2645369 -4.2574439 -4.2460985 -4.2360129 -4.2263007 -4.2217703 -4.2237811 -4.2298532 -4.2369294 -4.24592 -4.2572136 -4.266789 -4.2738185]]...]
INFO - root - 2017-12-06 04:14:43.193020: step 20710, loss = 2.07, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 20h:04m:53s remains)
INFO - root - 2017-12-06 04:14:45.497503: step 20720, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:20m:07s remains)
INFO - root - 2017-12-06 04:14:47.803893: step 20730, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 19h:48m:09s remains)
INFO - root - 2017-12-06 04:14:50.103280: step 20740, loss = 2.08, batch loss = 2.02 (35.1 examples/sec; 0.228 sec/batch; 19h:43m:09s remains)
INFO - root - 2017-12-06 04:14:52.384262: step 20750, loss = 2.05, batch loss = 1.99 (35.0 examples/sec; 0.229 sec/batch; 19h:47m:50s remains)
INFO - root - 2017-12-06 04:14:54.685989: step 20760, loss = 2.04, batch loss = 1.98 (34.7 examples/sec; 0.230 sec/batch; 19h:56m:43s remains)
INFO - root - 2017-12-06 04:14:56.994183: step 20770, loss = 2.05, batch loss = 1.99 (35.3 examples/sec; 0.227 sec/batch; 19h:38m:08s remains)
INFO - root - 2017-12-06 04:14:59.269675: step 20780, loss = 2.06, batch loss = 2.00 (34.1 examples/sec; 0.235 sec/batch; 20h:19m:27s remains)
INFO - root - 2017-12-06 04:15:01.589964: step 20790, loss = 2.10, batch loss = 2.04 (35.3 examples/sec; 0.227 sec/batch; 19h:37m:17s remains)
INFO - root - 2017-12-06 04:15:03.938079: step 20800, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.228 sec/batch; 19h:46m:20s remains)
2017-12-06 04:15:04.216194: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1959138 -4.1860342 -4.1909194 -4.2032766 -4.2103381 -4.2040586 -4.1938043 -4.1967821 -4.2223358 -4.2625079 -4.2981009 -4.3060503 -4.2906404 -4.2547054 -4.22259][-4.18791 -4.175787 -4.1796484 -4.19738 -4.2096429 -4.2001224 -4.1847343 -4.1861143 -4.2151852 -4.258719 -4.2948208 -4.29916 -4.2799273 -4.2409644 -4.2128758][-4.1869893 -4.17613 -4.180666 -4.2021856 -4.213038 -4.192987 -4.16623 -4.161788 -4.1948628 -4.2448983 -4.2809415 -4.2825036 -4.2591066 -4.2226214 -4.2034893][-4.1783094 -4.1689906 -4.1770706 -4.2015576 -4.2114 -4.1835995 -4.1453137 -4.1308165 -4.1631336 -4.2192979 -4.2579355 -4.2595525 -4.2350836 -4.2014689 -4.1912417][-4.1609864 -4.1561627 -4.1700206 -4.1953182 -4.1996083 -4.16056 -4.1140532 -4.0923619 -4.123342 -4.1853714 -4.2295933 -4.2348056 -4.2121868 -4.18368 -4.1773925][-4.1441779 -4.1470108 -4.1672168 -4.1960788 -4.1952157 -4.1407256 -4.0794272 -4.0440845 -4.0727777 -4.1416483 -4.1931782 -4.2109175 -4.200007 -4.1838994 -4.1827846][-4.1266708 -4.1340475 -4.1564689 -4.1873846 -4.1812706 -4.1108375 -4.030406 -3.983201 -4.0150776 -4.0908647 -4.1514969 -4.1850019 -4.1931462 -4.195735 -4.2041497][-4.0977225 -4.1100936 -4.1384068 -4.1706257 -4.1565576 -4.0705385 -3.9672484 -3.9082036 -3.9503114 -4.0380664 -4.1101756 -4.1575575 -4.1843309 -4.2057343 -4.2279906][-4.0521536 -4.0741577 -4.1138945 -4.1556921 -4.1386123 -4.045845 -3.9330881 -3.8791914 -3.9298608 -4.0184727 -4.0925655 -4.1438885 -4.1776953 -4.2089281 -4.2415504][-4.0126057 -4.0500674 -4.101635 -4.1494956 -4.1426167 -4.0656071 -3.971983 -3.9382234 -3.9829221 -4.0488572 -4.1060061 -4.1477237 -4.1783504 -4.2114706 -4.250453][-4.0281796 -4.0747108 -4.1253729 -4.1678853 -4.1685324 -4.1138763 -4.0499544 -4.0338221 -4.0660462 -4.1027389 -4.13389 -4.1586509 -4.1822619 -4.2152843 -4.2570457][-4.0876656 -4.1375179 -4.1811337 -4.2133875 -4.2151675 -4.17621 -4.1312208 -4.1275787 -4.1508203 -4.1625495 -4.1705637 -4.1772766 -4.1923723 -4.2210298 -4.2591605][-4.1583743 -4.2056637 -4.2387023 -4.2605414 -4.2616034 -4.2324009 -4.1985507 -4.1988072 -4.2110772 -4.2038813 -4.1923256 -4.1854067 -4.1937833 -4.2188416 -4.2547913][-4.2330618 -4.2707109 -4.2888889 -4.2959647 -4.2858567 -4.2584805 -4.2342353 -4.2331491 -4.2318377 -4.2110662 -4.1888719 -4.1756949 -4.1834545 -4.208919 -4.2452369][-4.2871127 -4.3153982 -4.3197303 -4.3109465 -4.2885966 -4.2566843 -4.2360692 -4.2321825 -4.2239723 -4.2022181 -4.1817517 -4.1709032 -4.1802969 -4.2040038 -4.2381287]]...]
INFO - root - 2017-12-06 04:15:06.500810: step 20810, loss = 2.05, batch loss = 1.99 (36.0 examples/sec; 0.222 sec/batch; 19h:14m:43s remains)
INFO - root - 2017-12-06 04:15:08.803219: step 20820, loss = 2.04, batch loss = 1.98 (34.9 examples/sec; 0.229 sec/batch; 19h:51m:16s remains)
INFO - root - 2017-12-06 04:15:11.129074: step 20830, loss = 2.04, batch loss = 1.99 (34.8 examples/sec; 0.230 sec/batch; 19h:53m:52s remains)
INFO - root - 2017-12-06 04:15:13.415381: step 20840, loss = 2.05, batch loss = 1.99 (34.4 examples/sec; 0.232 sec/batch; 20h:06m:41s remains)
INFO - root - 2017-12-06 04:15:15.682807: step 20850, loss = 2.03, batch loss = 1.97 (35.7 examples/sec; 0.224 sec/batch; 19h:24m:52s remains)
INFO - root - 2017-12-06 04:15:18.028499: step 20860, loss = 2.07, batch loss = 2.01 (34.4 examples/sec; 0.232 sec/batch; 20h:07m:19s remains)
INFO - root - 2017-12-06 04:15:20.326095: step 20870, loss = 2.04, batch loss = 1.98 (34.3 examples/sec; 0.233 sec/batch; 20h:12m:27s remains)
INFO - root - 2017-12-06 04:15:22.675114: step 20880, loss = 2.05, batch loss = 1.99 (34.9 examples/sec; 0.229 sec/batch; 19h:49m:30s remains)
INFO - root - 2017-12-06 04:15:25.007269: step 20890, loss = 2.04, batch loss = 1.99 (32.5 examples/sec; 0.246 sec/batch; 21h:16m:55s remains)
INFO - root - 2017-12-06 04:15:27.288933: step 20900, loss = 2.06, batch loss = 2.00 (33.8 examples/sec; 0.236 sec/batch; 20h:28m:08s remains)
2017-12-06 04:15:27.583362: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9728622 -4.1064768 -4.2024231 -4.2654433 -4.2893233 -4.2931695 -4.2877984 -4.2790227 -4.269608 -4.2668843 -4.2650595 -4.2617168 -4.2583814 -4.2558913 -4.2567992][-3.9961627 -4.1189556 -4.2104821 -4.2685776 -4.2899179 -4.2941365 -4.2878385 -4.2787828 -4.2713037 -4.2704854 -4.2668538 -4.2590418 -4.2515569 -4.2446413 -4.242311][-4.100297 -4.1898708 -4.2539792 -4.2886338 -4.2945433 -4.2922397 -4.2802234 -4.2666116 -4.2630734 -4.270648 -4.2711644 -4.2620149 -4.2499914 -4.2329574 -4.2198019][-4.1986756 -4.2518234 -4.2842522 -4.2921057 -4.2805266 -4.2667847 -4.2445936 -4.2251258 -4.2274604 -4.2484317 -4.2644057 -4.2660317 -4.2611113 -4.24109 -4.2197227][-4.264617 -4.285676 -4.2872119 -4.2690897 -4.2391763 -4.208714 -4.174922 -4.1481395 -4.1577249 -4.2011647 -4.244453 -4.2678361 -4.27932 -4.2645817 -4.2393546][-4.2896004 -4.2836909 -4.258924 -4.21155 -4.1562166 -4.103931 -4.0516648 -4.0095282 -4.0230923 -4.1011662 -4.1863041 -4.2421646 -4.279387 -4.2814837 -4.2636886][-4.2907314 -4.2651391 -4.2185073 -4.1453743 -4.0629277 -3.9820743 -3.8969882 -3.8246369 -3.8357949 -3.9529209 -4.0855393 -4.1772447 -4.2435803 -4.2714024 -4.2710118][-4.2933278 -4.2608671 -4.2089806 -4.1302891 -4.0373054 -3.9369001 -3.8258708 -3.7248845 -3.7158611 -3.845221 -4.0029683 -4.1185727 -4.205379 -4.2523966 -4.2643962][-4.3096819 -4.2816291 -4.2398968 -4.1761818 -4.0983839 -4.0105586 -3.9107332 -3.8178315 -3.7881331 -3.8772316 -4.010252 -4.1221452 -4.2078485 -4.2562647 -4.2685742][-4.3294969 -4.3132625 -4.2846727 -4.2397532 -4.186214 -4.1270185 -4.0619574 -3.9964466 -3.9622159 -4.0063934 -4.0946198 -4.1801677 -4.250412 -4.2855616 -4.2887273][-4.3406372 -4.3346334 -4.3190103 -4.2919369 -4.2583609 -4.2243309 -4.1868281 -4.1446342 -4.1154814 -4.1344914 -4.1886187 -4.24946 -4.2997227 -4.3207126 -4.3162103][-4.3402176 -4.3389091 -4.3337955 -4.3233809 -4.308877 -4.2925525 -4.2733574 -4.2479234 -4.227119 -4.2353082 -4.2683158 -4.3064785 -4.3360009 -4.3448868 -4.3360538][-4.3364282 -4.3355021 -4.3337317 -4.3321967 -4.32934 -4.3241329 -4.3171787 -4.3055182 -4.2946472 -4.2991867 -4.3178787 -4.3373966 -4.3498993 -4.3505549 -4.3404155][-4.3335047 -4.3323669 -4.3311405 -4.3322086 -4.3338084 -4.3337946 -4.3329506 -4.3289118 -4.3259091 -4.3305445 -4.3407855 -4.3482351 -4.3500667 -4.3463359 -4.3376131][-4.332706 -4.3311658 -4.3302569 -4.3321042 -4.3348446 -4.3364034 -4.337081 -4.3364458 -4.3361096 -4.3400908 -4.3465028 -4.3495293 -4.3476148 -4.3425908 -4.336617]]...]
INFO - root - 2017-12-06 04:15:29.846657: step 20910, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:10m:43s remains)
INFO - root - 2017-12-06 04:15:32.166265: step 20920, loss = 2.07, batch loss = 2.01 (35.2 examples/sec; 0.228 sec/batch; 19h:41m:26s remains)
INFO - root - 2017-12-06 04:15:34.464704: step 20930, loss = 2.07, batch loss = 2.01 (33.9 examples/sec; 0.236 sec/batch; 20h:23m:58s remains)
INFO - root - 2017-12-06 04:15:36.744656: step 20940, loss = 2.04, batch loss = 1.98 (34.2 examples/sec; 0.234 sec/batch; 20h:15m:18s remains)
INFO - root - 2017-12-06 04:15:39.076390: step 20950, loss = 2.09, batch loss = 2.03 (34.1 examples/sec; 0.235 sec/batch; 20h:19m:45s remains)
INFO - root - 2017-12-06 04:15:41.402842: step 20960, loss = 2.09, batch loss = 2.03 (35.4 examples/sec; 0.226 sec/batch; 19h:32m:46s remains)
INFO - root - 2017-12-06 04:15:43.707363: step 20970, loss = 2.05, batch loss = 1.99 (33.5 examples/sec; 0.239 sec/batch; 20h:38m:52s remains)
INFO - root - 2017-12-06 04:15:46.040007: step 20980, loss = 2.07, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 19h:47m:04s remains)
INFO - root - 2017-12-06 04:15:48.368842: step 20990, loss = 2.08, batch loss = 2.02 (34.6 examples/sec; 0.231 sec/batch; 20h:00m:20s remains)
INFO - root - 2017-12-06 04:15:50.677089: step 21000, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.224 sec/batch; 19h:25m:06s remains)
2017-12-06 04:15:50.959943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.319602 -4.322722 -4.3278351 -4.3301835 -4.3257065 -4.30588 -4.2697334 -4.2295513 -4.21045 -4.2286339 -4.2643929 -4.2925563 -4.3071127 -4.3117442 -4.3137307][-4.3064194 -4.309196 -4.3133979 -4.3166704 -4.314362 -4.2909508 -4.2396355 -4.178328 -4.1530619 -4.1846447 -4.2368259 -4.27493 -4.292583 -4.2998381 -4.3043022][-4.278821 -4.2853785 -4.2935 -4.300004 -4.2993445 -4.26975 -4.200398 -4.1136813 -4.0858946 -4.1395535 -4.2120214 -4.2597575 -4.2774467 -4.2859759 -4.2910609][-4.2452636 -4.2563362 -4.2685175 -4.2766809 -4.2748375 -4.2344127 -4.1412845 -4.0261283 -3.9975681 -4.082469 -4.178225 -4.2387123 -4.2627659 -4.2746849 -4.2815032][-4.2130136 -4.2242384 -4.2348938 -4.2381697 -4.2327089 -4.1811585 -4.0648971 -3.9186671 -3.8860178 -4.0100074 -4.1368661 -4.2165303 -4.2505541 -4.2665396 -4.2755342][-4.1889491 -4.1946545 -4.199512 -4.1969433 -4.1897039 -4.1295218 -3.9917068 -3.8101211 -3.7703595 -3.933197 -4.0923052 -4.1913381 -4.23449 -4.2527189 -4.261879][-4.1861668 -4.1837778 -4.1815267 -4.1779838 -4.1734204 -4.1135988 -3.9708788 -3.7780762 -3.7348294 -3.9047072 -4.0671144 -4.1687188 -4.2177596 -4.2372704 -4.2473335][-4.2004714 -4.1928473 -4.186461 -4.1878018 -4.1900535 -4.1473112 -4.0333858 -3.8849306 -3.8482904 -3.9699988 -4.0905485 -4.1682534 -4.2110596 -4.2274251 -4.23649][-4.2155762 -4.2082214 -4.2040391 -4.2124343 -4.2208314 -4.1934013 -4.1172442 -4.0303473 -4.003468 -4.0712967 -4.1430373 -4.1894412 -4.21656 -4.2241478 -4.2280331][-4.2341218 -4.2282128 -4.2266808 -4.2369075 -4.2461481 -4.2285819 -4.1812634 -4.1353559 -4.1201353 -4.1569471 -4.1994033 -4.2244978 -4.2369094 -4.2334318 -4.2254777][-4.2490878 -4.2440109 -4.2412395 -4.2491579 -4.254786 -4.23838 -4.2049928 -4.1769891 -4.174768 -4.2002454 -4.22619 -4.24207 -4.2512336 -4.2434297 -4.2252665][-4.2614169 -4.2525067 -4.2449756 -4.2477803 -4.2442369 -4.2188077 -4.1858587 -4.1632471 -4.1675515 -4.1925468 -4.2127628 -4.2278552 -4.2431684 -4.2414942 -4.2244859][-4.2590632 -4.2390342 -4.2249556 -4.2263021 -4.2174683 -4.1832476 -4.1491446 -4.1280804 -4.1343122 -4.1622787 -4.1813908 -4.1982241 -4.2224932 -4.2307973 -4.220273][-4.24262 -4.2167845 -4.2034292 -4.2109418 -4.2046652 -4.1695247 -4.1363983 -4.1166186 -4.1202364 -4.1432519 -4.1560807 -4.1689534 -4.1993184 -4.2176714 -4.2155743][-4.2322559 -4.2060709 -4.1988306 -4.2140689 -4.2136235 -4.185822 -4.1582861 -4.1395488 -4.1353812 -4.1445336 -4.1471806 -4.1573563 -4.1886449 -4.2117004 -4.2157369]]...]
INFO - root - 2017-12-06 04:15:53.228472: step 21010, loss = 2.07, batch loss = 2.01 (33.3 examples/sec; 0.240 sec/batch; 20h:45m:32s remains)
INFO - root - 2017-12-06 04:15:55.486409: step 21020, loss = 2.06, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:41m:40s remains)
INFO - root - 2017-12-06 04:15:57.806830: step 21030, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 20h:26m:14s remains)
INFO - root - 2017-12-06 04:16:00.111077: step 21040, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:22m:47s remains)
INFO - root - 2017-12-06 04:16:02.410368: step 21050, loss = 2.05, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 20h:00m:18s remains)
INFO - root - 2017-12-06 04:16:04.724337: step 21060, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 19h:26m:02s remains)
INFO - root - 2017-12-06 04:16:06.999973: step 21070, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 19h:36m:45s remains)
INFO - root - 2017-12-06 04:16:09.354280: step 21080, loss = 2.09, batch loss = 2.03 (33.3 examples/sec; 0.240 sec/batch; 20h:46m:24s remains)
INFO - root - 2017-12-06 04:16:11.660728: step 21090, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 19h:53m:11s remains)
INFO - root - 2017-12-06 04:16:14.009345: step 21100, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 19h:58m:10s remains)
2017-12-06 04:16:14.327057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3076253 -4.3121672 -4.3123894 -4.3101444 -4.3024769 -4.289432 -4.2777233 -4.26815 -4.2649317 -4.268836 -4.2717471 -4.2724638 -4.2728424 -4.2722135 -4.2722473][-4.2879996 -4.2945843 -4.2912164 -4.2816992 -4.2663016 -4.2472534 -4.2344413 -4.225214 -4.2276173 -4.2398634 -4.2457309 -4.2446485 -4.243391 -4.2435803 -4.245153][-4.2663112 -4.2746453 -4.2663312 -4.2476845 -4.2227077 -4.1967773 -4.1832213 -4.178144 -4.1925111 -4.2169065 -4.2265439 -4.2238827 -4.2192974 -4.217978 -4.2197366][-4.2471395 -4.2574825 -4.2438354 -4.2160025 -4.1810923 -4.1472249 -4.1326108 -4.1321678 -4.1594663 -4.1948481 -4.2061176 -4.2024488 -4.1953888 -4.1932373 -4.1961417][-4.234776 -4.2491922 -4.2315931 -4.19375 -4.1465168 -4.1017036 -4.0814466 -4.0805097 -4.1212077 -4.1678576 -4.1816826 -4.1809011 -4.1744995 -4.1737638 -4.1795979][-4.2290912 -4.2473354 -4.2259564 -4.1755557 -4.1125169 -4.053031 -4.0174413 -4.0087047 -4.0652847 -4.1286187 -4.14954 -4.1524968 -4.1461253 -4.1494441 -4.1639709][-4.2236938 -4.24106 -4.2133551 -4.1490893 -4.0666018 -3.9850979 -3.9206409 -3.894794 -3.970011 -4.0553646 -4.0876355 -4.0953383 -4.0906968 -4.1019516 -4.12888][-4.2225113 -4.2317462 -4.193419 -4.1169443 -4.0157285 -3.9076576 -3.809413 -3.7670145 -3.8614669 -3.9656544 -4.0101075 -4.0239959 -4.0245323 -4.0435715 -4.0804539][-4.2355347 -4.2373328 -4.1929197 -4.1150732 -4.0121307 -3.8990898 -3.7907293 -3.7514 -3.8459129 -3.9404154 -3.9837074 -3.9968653 -3.9997334 -4.0225506 -4.0595627][-4.2540364 -4.2549791 -4.2180223 -4.1553183 -4.0737782 -3.9834788 -3.8950143 -3.8692584 -3.941555 -4.0052519 -4.0346165 -4.0420818 -4.0443835 -4.0679612 -4.0979357][-4.2640429 -4.2679992 -4.2443419 -4.2032256 -4.1486416 -4.0866623 -4.024508 -4.0085135 -4.0547695 -4.0884466 -4.1049047 -4.1079412 -4.1107917 -4.1330152 -4.1536913][-4.2643542 -4.2702146 -4.2596173 -4.2386255 -4.2061996 -4.1693039 -4.1319304 -4.1245756 -4.15324 -4.1690073 -4.17701 -4.174921 -4.1738448 -4.1911488 -4.2025619][-4.25621 -4.2611046 -4.2574148 -4.2505445 -4.2336493 -4.2150598 -4.197865 -4.201776 -4.2238617 -4.2321911 -4.2336831 -4.2259951 -4.2193069 -4.2281585 -4.2326][-4.2419052 -4.2460618 -4.2446136 -4.2445369 -4.2386379 -4.2326736 -4.2304268 -4.2412653 -4.25833 -4.2621703 -4.258007 -4.2456751 -4.2342763 -4.234375 -4.2350893][-4.22794 -4.2302437 -4.2294216 -4.2326961 -4.2344694 -4.2361221 -4.241365 -4.2527542 -4.2638016 -4.2644434 -4.2563939 -4.2415829 -4.2282681 -4.2248368 -4.2250066]]...]
INFO - root - 2017-12-06 04:16:16.605725: step 21110, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:31m:47s remains)
INFO - root - 2017-12-06 04:16:18.888753: step 21120, loss = 2.07, batch loss = 2.01 (33.6 examples/sec; 0.238 sec/batch; 20h:36m:02s remains)
INFO - root - 2017-12-06 04:16:21.205537: step 21130, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 19h:46m:34s remains)
INFO - root - 2017-12-06 04:16:23.507647: step 21140, loss = 2.06, batch loss = 2.00 (33.6 examples/sec; 0.238 sec/batch; 20h:34m:42s remains)
INFO - root - 2017-12-06 04:16:25.793531: step 21150, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 19h:27m:11s remains)
INFO - root - 2017-12-06 04:16:28.052741: step 21160, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 19h:02m:05s remains)
INFO - root - 2017-12-06 04:16:30.342342: step 21170, loss = 2.06, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 19h:58m:12s remains)
INFO - root - 2017-12-06 04:16:32.641427: step 21180, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.226 sec/batch; 19h:30m:06s remains)
INFO - root - 2017-12-06 04:16:34.915108: step 21190, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 19h:24m:51s remains)
INFO - root - 2017-12-06 04:16:37.229454: step 21200, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.226 sec/batch; 19h:30m:50s remains)
2017-12-06 04:16:37.528976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1980863 -4.1759562 -4.1538019 -4.1300006 -4.1248503 -4.1405654 -4.1630263 -4.1738453 -4.1764131 -4.1727791 -4.163166 -4.1711693 -4.1988864 -4.2214789 -4.2359638][-4.1814804 -4.1596537 -4.14195 -4.124115 -4.1200485 -4.1404023 -4.1710253 -4.1872511 -4.1938086 -4.19275 -4.1824121 -4.1878 -4.212255 -4.2294173 -4.2365742][-4.1666765 -4.1478982 -4.1385093 -4.1313124 -4.1263385 -4.1442957 -4.1732559 -4.1878524 -4.1960168 -4.1985841 -4.192399 -4.1982889 -4.2169514 -4.2263603 -4.2278705][-4.1508493 -4.142252 -4.1461039 -4.1485257 -4.1388655 -4.1463275 -4.1633334 -4.1671791 -4.1706109 -4.17513 -4.1786451 -4.1922879 -4.2063603 -4.2065477 -4.2018447][-4.1363645 -4.1451654 -4.1626596 -4.1724539 -4.1567612 -4.1510968 -4.1461277 -4.1265717 -4.114872 -4.1233826 -4.1488371 -4.1797023 -4.1963992 -4.1933737 -4.1808972][-4.147583 -4.1704831 -4.1931829 -4.2001348 -4.171258 -4.1427245 -4.1097393 -4.058466 -4.0241785 -4.0366368 -4.0915837 -4.1482911 -4.1742697 -4.1748319 -4.1591778][-4.1805558 -4.2096062 -4.2296648 -4.2308598 -4.1878424 -4.1302705 -4.0654893 -3.9845591 -3.9309788 -3.9519727 -4.03499 -4.1177721 -4.1480508 -4.1500125 -4.1358986][-4.2143736 -4.2425628 -4.2591486 -4.2568903 -4.2098026 -4.1317897 -4.0371618 -3.9343195 -3.8699856 -3.9006033 -3.9968438 -4.0946412 -4.1291504 -4.1283879 -4.1123428][-4.2420559 -4.2621727 -4.2752304 -4.2730565 -4.236217 -4.1627822 -4.0673451 -3.9729576 -3.9222736 -3.9480488 -4.028162 -4.1122322 -4.142065 -4.1315575 -4.1096511][-4.2574158 -4.2697167 -4.2780356 -4.2777734 -4.2546449 -4.2022457 -4.1310463 -4.0734844 -4.0555553 -4.0733113 -4.1164117 -4.1655717 -4.1811337 -4.1633568 -4.1345921][-4.2626286 -4.2707124 -4.2745256 -4.2728343 -4.2565889 -4.2211208 -4.1697044 -4.1368294 -4.1423225 -4.1598616 -4.1789346 -4.2016644 -4.2061486 -4.1866198 -4.1568866][-4.2608433 -4.2670441 -4.265913 -4.260366 -4.2436819 -4.2162862 -4.1762533 -4.1528 -4.1685195 -4.1886764 -4.1999936 -4.2098422 -4.2101731 -4.1920004 -4.1597786][-4.2633219 -4.2736521 -4.2713461 -4.2633295 -4.2455311 -4.2230206 -4.1920886 -4.1768227 -4.1959786 -4.2121143 -4.2158122 -4.2171106 -4.2105603 -4.1906171 -4.1552405][-4.2762365 -4.290916 -4.2897425 -4.2790642 -4.2605252 -4.2425103 -4.2212653 -4.2153387 -4.2359862 -4.2463541 -4.2431941 -4.234952 -4.2204132 -4.1975508 -4.1619611][-4.2847085 -4.3014641 -4.3007059 -4.2900305 -4.2724404 -4.2563062 -4.2402577 -4.2405043 -4.2618403 -4.2719278 -4.2689519 -4.2568374 -4.2359076 -4.207005 -4.1655812]]...]
INFO - root - 2017-12-06 04:16:39.871867: step 21210, loss = 2.05, batch loss = 1.99 (34.1 examples/sec; 0.235 sec/batch; 20h:17m:35s remains)
INFO - root - 2017-12-06 04:16:42.169031: step 21220, loss = 2.04, batch loss = 1.98 (33.9 examples/sec; 0.236 sec/batch; 20h:25m:57s remains)
INFO - root - 2017-12-06 04:16:44.456992: step 21230, loss = 2.05, batch loss = 1.99 (35.8 examples/sec; 0.223 sec/batch; 19h:18m:36s remains)
INFO - root - 2017-12-06 04:16:46.745513: step 21240, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 20h:01m:25s remains)
INFO - root - 2017-12-06 04:16:49.057956: step 21250, loss = 2.06, batch loss = 2.00 (35.5 examples/sec; 0.226 sec/batch; 19h:29m:52s remains)
INFO - root - 2017-12-06 04:16:51.375063: step 21260, loss = 2.08, batch loss = 2.02 (35.0 examples/sec; 0.228 sec/batch; 19h:44m:02s remains)
INFO - root - 2017-12-06 04:16:53.680081: step 21270, loss = 2.08, batch loss = 2.02 (33.7 examples/sec; 0.237 sec/batch; 20h:31m:04s remains)
INFO - root - 2017-12-06 04:16:55.961148: step 21280, loss = 2.09, batch loss = 2.04 (34.5 examples/sec; 0.232 sec/batch; 20h:01m:48s remains)
INFO - root - 2017-12-06 04:16:58.217012: step 21290, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 19h:28m:07s remains)
INFO - root - 2017-12-06 04:17:00.500810: step 21300, loss = 2.05, batch loss = 2.00 (35.5 examples/sec; 0.225 sec/batch; 19h:27m:19s remains)
2017-12-06 04:17:00.786951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2626724 -4.2638559 -4.2577014 -4.2418585 -4.2216506 -4.2089009 -4.2071657 -4.2145433 -4.227581 -4.2409859 -4.2449746 -4.2389436 -4.2344575 -4.2349072 -4.2452545][-4.2495003 -4.2430587 -4.2306137 -4.2137489 -4.1985726 -4.1924243 -4.1966896 -4.2105656 -4.2303653 -4.2531791 -4.2643118 -4.2582951 -4.25176 -4.2494845 -4.2536321][-4.2336617 -4.2183628 -4.1984363 -4.1788573 -4.168469 -4.1674914 -4.17432 -4.1925654 -4.2197647 -4.2526631 -4.2699604 -4.2641082 -4.25564 -4.2523804 -4.2525153][-4.2274437 -4.2067118 -4.1814384 -4.1573792 -4.149693 -4.1512613 -4.1570039 -4.1768537 -4.2100449 -4.2501335 -4.2697678 -4.264401 -4.2569494 -4.25322 -4.2504911][-4.2338176 -4.2135482 -4.1864438 -4.155818 -4.1411905 -4.135324 -4.1336603 -4.1543841 -4.1975441 -4.2462454 -4.2714992 -4.27036 -4.2660651 -4.2617397 -4.2543917][-4.2448282 -4.22793 -4.2001653 -4.1609654 -4.1334295 -4.1107116 -4.0936556 -4.1155615 -4.173573 -4.2348881 -4.2687645 -4.2718778 -4.2708831 -4.2670746 -4.2560129][-4.2542129 -4.2404189 -4.2077956 -4.1586976 -4.1172705 -4.0742164 -4.0406151 -4.0659227 -4.1399474 -4.2131543 -4.2556539 -4.263865 -4.2646379 -4.2648387 -4.2546077][-4.2518749 -4.2429328 -4.2119622 -4.1636882 -4.1180778 -4.0659289 -4.0265884 -4.0525336 -4.1288013 -4.2002726 -4.2444406 -4.2570591 -4.25928 -4.2639174 -4.2574668][-4.2324457 -4.2294111 -4.2101474 -4.1741295 -4.13626 -4.0919642 -4.0605617 -4.0807681 -4.1421571 -4.1977782 -4.2366529 -4.2547822 -4.2575092 -4.2647614 -4.2634482][-4.2252274 -4.2259288 -4.2197342 -4.1982894 -4.1675477 -4.1314135 -4.1036968 -4.1097927 -4.1506138 -4.18917 -4.2244382 -4.2460017 -4.2461939 -4.2549486 -4.2576504][-4.2290769 -4.231667 -4.232276 -4.2209983 -4.1935453 -4.1599431 -4.1348186 -4.1302156 -4.1539435 -4.1827741 -4.2189188 -4.242733 -4.2415957 -4.2474852 -4.2509403][-4.2324915 -4.236486 -4.239296 -4.2336655 -4.2104826 -4.1840076 -4.1665692 -4.1598015 -4.1686535 -4.189281 -4.2226992 -4.2425666 -4.2397084 -4.2402997 -4.2459664][-4.2272692 -4.2300138 -4.2322941 -4.2311444 -4.2191987 -4.2062783 -4.1979041 -4.1894555 -4.1848044 -4.1970863 -4.2248116 -4.23835 -4.234262 -4.2346163 -4.2450314][-4.2293067 -4.229 -4.2272944 -4.2249675 -4.2185316 -4.2148981 -4.210742 -4.1958075 -4.1779962 -4.1816688 -4.2071767 -4.2197766 -4.220891 -4.2300677 -4.2479534][-4.2338634 -4.232965 -4.227334 -4.2239938 -4.2209525 -4.2229581 -4.2176595 -4.1979122 -4.1737208 -4.1718597 -4.1952548 -4.2064052 -4.2083211 -4.2262487 -4.2488194]]...]
INFO - root - 2017-12-06 04:17:03.054652: step 21310, loss = 2.04, batch loss = 1.99 (34.7 examples/sec; 0.231 sec/batch; 19h:56m:25s remains)
INFO - root - 2017-12-06 04:17:05.388664: step 21320, loss = 2.10, batch loss = 2.04 (34.3 examples/sec; 0.233 sec/batch; 20h:10m:34s remains)
INFO - root - 2017-12-06 04:17:07.694502: step 21330, loss = 2.05, batch loss = 1.99 (33.8 examples/sec; 0.237 sec/batch; 20h:28m:03s remains)
INFO - root - 2017-12-06 04:17:10.032594: step 21340, loss = 2.08, batch loss = 2.02 (33.4 examples/sec; 0.240 sec/batch; 20h:42m:19s remains)
INFO - root - 2017-12-06 04:17:12.354078: step 21350, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 19h:48m:28s remains)
INFO - root - 2017-12-06 04:17:14.715151: step 21360, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:31m:18s remains)
INFO - root - 2017-12-06 04:17:17.013048: step 21370, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.221 sec/batch; 19h:03m:51s remains)
INFO - root - 2017-12-06 04:17:19.326275: step 21380, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 19h:48m:19s remains)
INFO - root - 2017-12-06 04:17:21.622117: step 21390, loss = 2.08, batch loss = 2.02 (36.0 examples/sec; 0.223 sec/batch; 19h:13m:44s remains)
INFO - root - 2017-12-06 04:17:23.944998: step 21400, loss = 2.06, batch loss = 2.01 (32.5 examples/sec; 0.247 sec/batch; 21h:18m:09s remains)
2017-12-06 04:17:24.253340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3094673 -4.3051414 -4.3076291 -4.3116241 -4.3154197 -4.3178558 -4.3176956 -4.3174424 -4.3151579 -4.3183303 -4.3177533 -4.309535 -4.306077 -4.3083386 -4.3116188][-4.2896233 -4.280921 -4.28413 -4.2915339 -4.2967434 -4.2975326 -4.29424 -4.2936468 -4.2942882 -4.3038273 -4.3069687 -4.2966728 -4.2847981 -4.2810049 -4.2857862][-4.2697606 -4.2571154 -4.2605009 -4.2708435 -4.2772317 -4.2754278 -4.2667756 -4.2617469 -4.2633591 -4.2785525 -4.2872696 -4.2806025 -4.262867 -4.2535772 -4.2587738][-4.2507334 -4.2367878 -4.2410793 -4.2543583 -4.2631254 -4.2572618 -4.2412 -4.2280622 -4.2256942 -4.2448735 -4.264441 -4.268137 -4.2514057 -4.2388039 -4.2407904][-4.2229304 -4.2088933 -4.2148342 -4.2302265 -4.2438927 -4.2344475 -4.2100158 -4.184936 -4.172492 -4.1954279 -4.2309494 -4.24612 -4.2340388 -4.2202554 -4.2167153][-4.1925492 -4.1772976 -4.185667 -4.2044363 -4.2218471 -4.2062173 -4.1636243 -4.1145597 -4.0852594 -4.116291 -4.1752996 -4.2081509 -4.2056975 -4.1946626 -4.1878562][-4.173964 -4.1602397 -4.17149 -4.1913447 -4.2071886 -4.1822338 -4.1161323 -4.0311146 -3.9739482 -4.013504 -4.102253 -4.1586418 -4.1753778 -4.1790085 -4.1773052][-4.1731472 -4.1621256 -4.1725655 -4.1906581 -4.204576 -4.1776242 -4.1014881 -3.9924998 -3.9115438 -3.9500897 -4.053875 -4.1254978 -4.1593547 -4.1821 -4.19139][-4.1847439 -4.1763945 -4.1823349 -4.194036 -4.2054715 -4.1849785 -4.12589 -4.0350447 -3.9688332 -3.9907856 -4.066493 -4.1238127 -4.1582994 -4.1889429 -4.2067456][-4.19091 -4.179956 -4.1788988 -4.1871095 -4.1989059 -4.1908536 -4.1622367 -4.1115885 -4.0739546 -4.0778604 -4.1076117 -4.1346054 -4.1554947 -4.1809711 -4.2035093][-4.1857486 -4.1688046 -4.160347 -4.1652017 -4.1789255 -4.1824765 -4.180418 -4.1661482 -4.1464047 -4.1338491 -4.1359434 -4.1436577 -4.1497974 -4.164988 -4.1865864][-4.1827831 -4.162684 -4.1504078 -4.1505876 -4.1601267 -4.1676955 -4.1817093 -4.188983 -4.1786542 -4.1590838 -4.1497269 -4.1474433 -4.14486 -4.1535125 -4.1722302][-4.1783352 -4.161603 -4.1535892 -4.1583424 -4.167819 -4.1781549 -4.1969805 -4.211164 -4.2060108 -4.1903224 -4.1799107 -4.1699157 -4.1571465 -4.157094 -4.16917][-4.1946478 -4.1875081 -4.1904821 -4.2041659 -4.2170124 -4.2272172 -4.24101 -4.252284 -4.2491078 -4.2401166 -4.2323995 -4.2180333 -4.1980915 -4.1888723 -4.1925631][-4.248981 -4.2493377 -4.258265 -4.2728114 -4.2829752 -4.2872996 -4.2930427 -4.2980447 -4.2945948 -4.2893896 -4.2840776 -4.2725739 -4.2567649 -4.2472296 -4.2467632]]...]
INFO - root - 2017-12-06 04:17:26.533912: step 21410, loss = 2.06, batch loss = 2.01 (36.1 examples/sec; 0.221 sec/batch; 19h:08m:16s remains)
INFO - root - 2017-12-06 04:17:28.800380: step 21420, loss = 2.05, batch loss = 1.99 (36.4 examples/sec; 0.220 sec/batch; 19h:00m:51s remains)
INFO - root - 2017-12-06 04:17:31.081551: step 21430, loss = 2.06, batch loss = 2.00 (33.5 examples/sec; 0.239 sec/batch; 20h:39m:05s remains)
INFO - root - 2017-12-06 04:17:33.372284: step 21440, loss = 2.06, batch loss = 2.01 (35.9 examples/sec; 0.223 sec/batch; 19h:16m:36s remains)
INFO - root - 2017-12-06 04:17:35.685353: step 21450, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.235 sec/batch; 20h:17m:30s remains)
INFO - root - 2017-12-06 04:17:37.935670: step 21460, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:30m:40s remains)
INFO - root - 2017-12-06 04:17:40.256341: step 21470, loss = 2.09, batch loss = 2.03 (33.4 examples/sec; 0.240 sec/batch; 20h:42m:09s remains)
INFO - root - 2017-12-06 04:17:42.576346: step 21480, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 19h:10m:58s remains)
INFO - root - 2017-12-06 04:17:44.896250: step 21490, loss = 2.05, batch loss = 1.99 (35.5 examples/sec; 0.226 sec/batch; 19h:29m:26s remains)
INFO - root - 2017-12-06 04:17:47.210622: step 21500, loss = 2.08, batch loss = 2.02 (34.4 examples/sec; 0.232 sec/batch; 20h:04m:46s remains)
2017-12-06 04:17:47.516102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.206017 -4.2157235 -4.2211003 -4.2237968 -4.2243505 -4.2177882 -4.2132921 -4.2084222 -4.206655 -4.2132392 -4.2223306 -4.2294269 -4.2353086 -4.2374587 -4.23627][-4.208735 -4.2188864 -4.2253723 -4.2346487 -4.2407808 -4.2346764 -4.2287903 -4.2221441 -4.2197528 -4.2246447 -4.23197 -4.2388077 -4.2415028 -4.2363949 -4.2294211][-4.2154307 -4.2255926 -4.2330904 -4.2439761 -4.2533503 -4.2494969 -4.2446642 -4.2384691 -4.2351818 -4.2326775 -4.2320719 -4.233912 -4.2320852 -4.2266703 -4.2250829][-4.2192988 -4.2337966 -4.2454062 -4.256279 -4.2636871 -4.2581544 -4.2495546 -4.2408381 -4.2343807 -4.2287369 -4.2248106 -4.2231255 -4.2191439 -4.2149615 -4.2138777][-4.2210956 -4.2361879 -4.2480383 -4.2548752 -4.2575288 -4.2491846 -4.23466 -4.2182069 -4.2073369 -4.199542 -4.1969957 -4.2009377 -4.2010508 -4.1975226 -4.1949191][-4.22989 -4.2400017 -4.2429748 -4.2386675 -4.2331014 -4.2167349 -4.1920242 -4.1667876 -4.1531987 -4.1480689 -4.1534324 -4.1680083 -4.1777511 -4.1810961 -4.1795521][-4.2332025 -4.2316775 -4.220665 -4.2026811 -4.1851921 -4.1593757 -4.1248674 -4.0927939 -4.0843883 -4.0936337 -4.1132073 -4.1408753 -4.1614943 -4.1734776 -4.1730957][-4.2106843 -4.1962729 -4.1745348 -4.1466269 -4.1218195 -4.0849209 -4.0407109 -4.0085769 -4.0171733 -4.0514379 -4.0868969 -4.1245914 -4.153183 -4.1692271 -4.1682577][-4.1711316 -4.1486855 -4.1255655 -4.0951462 -4.0618572 -4.0086222 -3.9552376 -3.9352474 -3.9752042 -4.0361342 -4.082325 -4.1214142 -4.1537848 -4.172792 -4.1706686][-4.1443963 -4.1239824 -4.1003428 -4.0624557 -4.0138521 -3.9456604 -3.8998506 -3.9118779 -3.9806054 -4.0498075 -4.0944672 -4.1294365 -4.1607847 -4.180562 -4.1802235][-4.1408081 -4.1277781 -4.1052241 -4.0580993 -3.9999473 -3.93893 -3.9211719 -3.955605 -4.0249376 -4.0838985 -4.1195989 -4.1434245 -4.1664681 -4.1839285 -4.1850977][-4.1472082 -4.1428385 -4.1257877 -4.0818276 -4.0371475 -4.0034518 -4.0013204 -4.0335865 -4.0886936 -4.1319609 -4.151947 -4.1578565 -4.1676712 -4.1755552 -4.1738086][-4.168931 -4.1767325 -4.1710906 -4.1419268 -4.1158462 -4.1015792 -4.1000152 -4.1217504 -4.1641283 -4.1960416 -4.1971855 -4.1815343 -4.1712933 -4.1688485 -4.1673965][-4.2011147 -4.2179646 -4.22214 -4.2038269 -4.1881847 -4.1831193 -4.1805854 -4.1948533 -4.2249322 -4.2449479 -4.2313824 -4.2011213 -4.1808262 -4.1758332 -4.175211][-4.2262053 -4.2459793 -4.2536583 -4.2419214 -4.2319179 -4.2299943 -4.2286263 -4.2380495 -4.2552681 -4.2614589 -4.2407107 -4.2068081 -4.1874871 -4.1863194 -4.1861277]]...]
INFO - root - 2017-12-06 04:17:49.816005: step 21510, loss = 2.06, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 19h:44m:30s remains)
INFO - root - 2017-12-06 04:17:52.143853: step 21520, loss = 2.06, batch loss = 2.01 (33.5 examples/sec; 0.239 sec/batch; 20h:39m:04s remains)
INFO - root - 2017-12-06 04:17:54.462254: step 21530, loss = 2.05, batch loss = 1.99 (36.2 examples/sec; 0.221 sec/batch; 19h:06m:29s remains)
INFO - root - 2017-12-06 04:17:56.745573: step 21540, loss = 2.04, batch loss = 1.98 (34.2 examples/sec; 0.234 sec/batch; 20h:13m:27s remains)
INFO - root - 2017-12-06 04:17:59.046591: step 21550, loss = 2.05, batch loss = 1.99 (34.7 examples/sec; 0.230 sec/batch; 19h:53m:23s remains)
INFO - root - 2017-12-06 04:18:01.352596: step 21560, loss = 2.08, batch loss = 2.03 (36.1 examples/sec; 0.221 sec/batch; 19h:07m:35s remains)
INFO - root - 2017-12-06 04:18:03.654066: step 21570, loss = 2.09, batch loss = 2.03 (35.2 examples/sec; 0.227 sec/batch; 19h:37m:05s remains)
INFO - root - 2017-12-06 04:18:05.950052: step 21580, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 19h:24m:18s remains)
INFO - root - 2017-12-06 04:18:08.269054: step 21590, loss = 2.07, batch loss = 2.01 (33.2 examples/sec; 0.241 sec/batch; 20h:48m:05s remains)
INFO - root - 2017-12-06 04:18:10.619038: step 21600, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 19h:41m:46s remains)
2017-12-06 04:18:10.927468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2404881 -4.2500319 -4.2569957 -4.2562628 -4.2415314 -4.2200556 -4.2057939 -4.207016 -4.2191148 -4.2331529 -4.2494688 -4.2647686 -4.2786717 -4.2913618 -4.3033142][-4.2464828 -4.251996 -4.2563705 -4.2543335 -4.238461 -4.2143922 -4.1976943 -4.1979527 -4.2109575 -4.2277822 -4.247818 -4.2660666 -4.2797627 -4.2899637 -4.301527][-4.2258363 -4.2235413 -4.2223325 -4.2164288 -4.1978078 -4.1718888 -4.1576109 -4.163867 -4.1830716 -4.2072983 -4.2343831 -4.2583151 -4.2743831 -4.2847834 -4.2968531][-4.1809149 -4.1727118 -4.1673975 -4.1576381 -4.136692 -4.1109266 -4.1036496 -4.1238608 -4.1559162 -4.19085 -4.2265997 -4.2558756 -4.2741432 -4.2842317 -4.2953839][-4.1204724 -4.112865 -4.1103592 -4.1024423 -4.0829043 -4.0583625 -4.0581427 -4.0913215 -4.1346602 -4.17805 -4.2200341 -4.2528782 -4.2732005 -4.2840152 -4.2952137][-4.065845 -4.0618763 -4.0634689 -4.0587616 -4.0431337 -4.0186186 -4.0189328 -4.0581064 -4.1085505 -4.1585665 -4.2072892 -4.2440743 -4.2674508 -4.2809577 -4.2946029][-4.0359144 -4.03453 -4.0374503 -4.0335212 -4.0187135 -3.9907904 -3.9877651 -4.0276546 -4.0809431 -4.1349497 -4.1887069 -4.2308445 -4.2599192 -4.2773018 -4.2936687][-4.0471296 -4.0455074 -4.0432425 -4.0346642 -4.0152149 -3.9833665 -3.9751949 -4.0106516 -4.06133 -4.1151881 -4.1697087 -4.2159948 -4.2511687 -4.2728806 -4.291913][-4.0684562 -4.0659342 -4.0581985 -4.045156 -4.0207739 -3.98733 -3.9760149 -4.0034809 -4.0487514 -4.1016231 -4.1560812 -4.2043958 -4.2429881 -4.2678065 -4.2893114][-4.0869679 -4.0877242 -4.0810728 -4.070333 -4.046886 -4.0139413 -3.9987121 -4.014492 -4.0507097 -4.0973177 -4.1483774 -4.1960049 -4.2360716 -4.263978 -4.2879319][-4.1143551 -4.1196146 -4.1192379 -4.1138058 -4.0944896 -4.0626922 -4.041533 -4.0459075 -4.0711069 -4.1068363 -4.151073 -4.194942 -4.2340136 -4.2640634 -4.2893229][-4.151895 -4.1573839 -4.158432 -4.1556263 -4.14271 -4.1185293 -4.0999236 -4.1008873 -4.119307 -4.1446376 -4.1777859 -4.2128768 -4.2461071 -4.2727985 -4.2947464][-4.1997371 -4.2013769 -4.2005773 -4.1983719 -4.190547 -4.1744881 -4.1610694 -4.162714 -4.1771841 -4.1950946 -4.2174363 -4.241889 -4.2654672 -4.2854719 -4.3016405][-4.2255898 -4.2270103 -4.2268806 -4.2251344 -4.2176309 -4.2031007 -4.19208 -4.1960073 -4.2105823 -4.226851 -4.2455368 -4.2643881 -4.2818112 -4.296679 -4.3082557][-4.2074261 -4.210319 -4.2108068 -4.2095547 -4.2014208 -4.186904 -4.1796603 -4.190393 -4.2111363 -4.231873 -4.2535181 -4.2737441 -4.2912807 -4.3046861 -4.3136172]]...]
INFO - root - 2017-12-06 04:18:13.209798: step 21610, loss = 2.06, batch loss = 2.00 (36.1 examples/sec; 0.222 sec/batch; 19h:08m:12s remains)
INFO - root - 2017-12-06 04:18:15.465933: step 21620, loss = 2.06, batch loss = 2.01 (33.5 examples/sec; 0.239 sec/batch; 20h:37m:48s remains)
INFO - root - 2017-12-06 04:18:17.778106: step 21630, loss = 2.04, batch loss = 1.98 (34.2 examples/sec; 0.234 sec/batch; 20h:11m:07s remains)
INFO - root - 2017-12-06 04:18:20.134214: step 21640, loss = 2.09, batch loss = 2.03 (34.1 examples/sec; 0.235 sec/batch; 20h:14m:58s remains)
INFO - root - 2017-12-06 04:18:22.481971: step 21650, loss = 2.03, batch loss = 1.97 (36.4 examples/sec; 0.220 sec/batch; 18h:59m:23s remains)
INFO - root - 2017-12-06 04:18:24.781810: step 21660, loss = 2.06, batch loss = 2.00 (34.0 examples/sec; 0.235 sec/batch; 20h:19m:37s remains)
INFO - root - 2017-12-06 04:18:27.052080: step 21670, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:31m:12s remains)
INFO - root - 2017-12-06 04:18:29.345411: step 21680, loss = 2.05, batch loss = 2.00 (34.0 examples/sec; 0.235 sec/batch; 20h:17m:57s remains)
INFO - root - 2017-12-06 04:18:31.634027: step 21690, loss = 2.08, batch loss = 2.02 (36.1 examples/sec; 0.222 sec/batch; 19h:07m:26s remains)
INFO - root - 2017-12-06 04:18:33.968032: step 21700, loss = 2.09, batch loss = 2.03 (36.2 examples/sec; 0.221 sec/batch; 19h:03m:15s remains)
2017-12-06 04:18:34.249582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.142004 -4.1542263 -4.17977 -4.2099252 -4.2366695 -4.2582283 -4.2662425 -4.2644238 -4.2547946 -4.2314577 -4.2119069 -4.2094502 -4.2054772 -4.202446 -4.2121668][-4.1347394 -4.1436429 -4.1691108 -4.196444 -4.2175941 -4.2326283 -4.2401562 -4.2376213 -4.2284975 -4.2119761 -4.2011905 -4.2041903 -4.202527 -4.2049856 -4.2150655][-4.1442175 -4.1466393 -4.1630588 -4.1852551 -4.2020712 -4.2094259 -4.2096028 -4.2018394 -4.1910353 -4.1843648 -4.1895165 -4.2046084 -4.2086964 -4.2146955 -4.2188578][-4.1504135 -4.153131 -4.1646557 -4.1844897 -4.1966972 -4.193367 -4.1819048 -4.1609783 -4.1380095 -4.1320529 -4.1614404 -4.1972413 -4.2126632 -4.218358 -4.2169709][-4.14604 -4.1546726 -4.1688375 -4.1885834 -4.1975403 -4.1861339 -4.16152 -4.1189795 -4.06681 -4.0471816 -4.1025796 -4.17097 -4.2006068 -4.2091908 -4.20409][-4.1345787 -4.1542306 -4.1780682 -4.2028465 -4.2080479 -4.1871414 -4.1508169 -4.086834 -3.994071 -3.9469957 -4.0280209 -4.1247792 -4.1674314 -4.1827793 -4.175662][-4.1214142 -4.1523895 -4.1851335 -4.2137733 -4.2194924 -4.1946716 -4.1485977 -4.0692787 -3.9540765 -3.8921187 -3.9878895 -4.0953069 -4.1441641 -4.1625061 -4.1517339][-4.0980339 -4.13565 -4.1748633 -4.2044563 -4.2112541 -4.1940989 -4.1511197 -4.0793223 -3.9811304 -3.930176 -4.0138397 -4.1067567 -4.1536989 -4.1714606 -4.1653781][-4.0836835 -4.1227813 -4.1648207 -4.1946406 -4.205512 -4.2008352 -4.1678386 -4.1130695 -4.0402484 -4.005991 -4.0673957 -4.134728 -4.1740055 -4.1922851 -4.1953888][-4.0885339 -4.1305504 -4.1703382 -4.1965609 -4.2037172 -4.2068763 -4.1888208 -4.1521492 -4.1012769 -4.0782156 -4.1194663 -4.1640487 -4.1906409 -4.208324 -4.2212443][-4.0904074 -4.1326351 -4.166368 -4.1871448 -4.1920052 -4.2036428 -4.2020097 -4.18409 -4.1553741 -4.1434288 -4.1666718 -4.1890745 -4.2040725 -4.2212362 -4.2401829][-4.084372 -4.1267161 -4.1576862 -4.1764946 -4.1867518 -4.2047029 -4.21227 -4.2053847 -4.1960464 -4.1919494 -4.2022071 -4.2125382 -4.2229748 -4.2387075 -4.2550898][-4.0870485 -4.1320863 -4.1634154 -4.1798129 -4.1957045 -4.2158961 -4.2244763 -4.2215343 -4.2211809 -4.2195029 -4.2223382 -4.230176 -4.2422709 -4.2551732 -4.2639737][-4.1048865 -4.1556482 -4.1864362 -4.201345 -4.2197719 -4.2347093 -4.2390528 -4.238008 -4.2387509 -4.238512 -4.2411995 -4.2467475 -4.2593775 -4.2709923 -4.2747822][-4.1327887 -4.1904182 -4.218679 -4.2322564 -4.25071 -4.258131 -4.2578359 -4.256732 -4.2567196 -4.2581568 -4.2611303 -4.2640452 -4.2737045 -4.2820182 -4.2838092]]...]
INFO - root - 2017-12-06 04:18:36.565042: step 21710, loss = 2.06, batch loss = 2.00 (34.1 examples/sec; 0.234 sec/batch; 20h:13m:32s remains)
INFO - root - 2017-12-06 04:18:38.905639: step 21720, loss = 2.07, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 20h:08m:06s remains)
INFO - root - 2017-12-06 04:18:41.204679: step 21730, loss = 2.08, batch loss = 2.02 (34.0 examples/sec; 0.235 sec/batch; 20h:18m:01s remains)
INFO - root - 2017-12-06 04:18:43.536755: step 21740, loss = 2.10, batch loss = 2.04 (34.3 examples/sec; 0.233 sec/batch; 20h:06m:35s remains)
INFO - root - 2017-12-06 04:18:45.838298: step 21750, loss = 2.06, batch loss = 2.01 (33.8 examples/sec; 0.237 sec/batch; 20h:26m:13s remains)
INFO - root - 2017-12-06 04:18:48.099362: step 21760, loss = 2.09, batch loss = 2.03 (35.2 examples/sec; 0.228 sec/batch; 19h:38m:30s remains)
INFO - root - 2017-12-06 04:18:50.386546: step 21770, loss = 2.04, batch loss = 1.98 (35.6 examples/sec; 0.225 sec/batch; 19h:24m:02s remains)
INFO - root - 2017-12-06 04:18:52.692312: step 21780, loss = 2.06, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:38m:51s remains)
INFO - root - 2017-12-06 04:18:55.008031: step 21790, loss = 2.10, batch loss = 2.04 (34.2 examples/sec; 0.234 sec/batch; 20h:10m:21s remains)
INFO - root - 2017-12-06 04:18:57.283744: step 21800, loss = 2.04, batch loss = 1.98 (35.7 examples/sec; 0.224 sec/batch; 19h:18m:51s remains)
2017-12-06 04:18:57.555858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1493735 -4.141685 -4.1432247 -4.138258 -4.130507 -4.1221771 -4.1230364 -4.1423888 -4.1651926 -4.1896405 -4.2164216 -4.2302141 -4.2280583 -4.2258496 -4.2285061][-4.1206131 -4.1137571 -4.1178102 -4.1135521 -4.100616 -4.0869989 -4.0855255 -4.1095071 -4.140296 -4.1741509 -4.2098217 -4.23644 -4.2458324 -4.2545843 -4.2643838][-4.082931 -4.0818729 -4.0897217 -4.0894604 -4.0770926 -4.0693545 -4.0704904 -4.0979557 -4.1368685 -4.1768827 -4.2167377 -4.250679 -4.2685194 -4.2828832 -4.2944822][-4.0397758 -4.044837 -4.0574627 -4.0635805 -4.0577989 -4.0609012 -4.0721917 -4.1078234 -4.1570597 -4.198678 -4.2356124 -4.2690725 -4.2899842 -4.3013496 -4.3073497][-4.0215216 -4.0292797 -4.0457973 -4.0540476 -4.0506682 -4.0619354 -4.0888414 -4.137569 -4.1962552 -4.2388453 -4.2714839 -4.2978134 -4.3105488 -4.3084493 -4.3022027][-4.0505633 -4.0571585 -4.0743208 -4.0781274 -4.0699282 -4.0787745 -4.1087184 -4.1617165 -4.2259636 -4.27182 -4.3044505 -4.3230219 -4.3230548 -4.3073716 -4.28596][-4.1010885 -4.1049352 -4.121192 -4.1204753 -4.1042223 -4.0993452 -4.115706 -4.160439 -4.2270117 -4.2802534 -4.3142657 -4.3276358 -4.3221049 -4.2999363 -4.2685423][-4.1505165 -4.147325 -4.1547356 -4.14969 -4.131155 -4.1169209 -4.117887 -4.1534772 -4.216373 -4.2674494 -4.2995176 -4.3116302 -4.3052983 -4.2803788 -4.24747][-4.1824889 -4.1667314 -4.15687 -4.1428523 -4.1222019 -4.1062818 -4.1052241 -4.1379004 -4.1916552 -4.231926 -4.2590723 -4.2710261 -4.2672281 -4.2450776 -4.2195649][-4.1812315 -4.153326 -4.1250281 -4.1004543 -4.0788 -4.0694518 -4.0832844 -4.1234407 -4.1673532 -4.1971784 -4.2211108 -4.2313542 -4.2278509 -4.2109179 -4.1971364][-4.1644487 -4.1290207 -4.0917063 -4.0613418 -4.04692 -4.0512938 -4.0825033 -4.1304946 -4.1683707 -4.1935592 -4.2141223 -4.2175794 -4.2107649 -4.1970229 -4.1905947][-4.150207 -4.1210556 -4.0922642 -4.0706854 -4.0679326 -4.0840831 -4.1198959 -4.1652727 -4.1988564 -4.2237244 -4.2411919 -4.2389712 -4.2279568 -4.2147245 -4.2072845][-4.1244617 -4.1160183 -4.1116014 -4.1145978 -4.1314754 -4.1555476 -4.186563 -4.2197962 -4.2459764 -4.2696061 -4.2837787 -4.2815318 -4.2705903 -4.2554216 -4.2407413][-4.1042633 -4.11188 -4.1293831 -4.1559572 -4.1890674 -4.2209496 -4.2490454 -4.2722206 -4.2884717 -4.30443 -4.3139987 -4.314198 -4.3072109 -4.2918591 -4.2723589][-4.1095176 -4.1256537 -4.1540627 -4.1905904 -4.23061 -4.26486 -4.2901907 -4.3034172 -4.3088756 -4.3155217 -4.3209386 -4.3225756 -4.3180966 -4.3064332 -4.2893524]]...]
INFO - root - 2017-12-06 04:18:59.815216: step 21810, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.216 sec/batch; 18h:36m:25s remains)
INFO - root - 2017-12-06 04:19:02.117799: step 21820, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 19h:40m:04s remains)
INFO - root - 2017-12-06 04:19:04.408260: step 21830, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.230 sec/batch; 19h:52m:54s remains)
INFO - root - 2017-12-06 04:19:06.748048: step 21840, loss = 2.06, batch loss = 2.00 (34.0 examples/sec; 0.235 sec/batch; 20h:18m:13s remains)
INFO - root - 2017-12-06 04:19:09.066682: step 21850, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:09m:25s remains)
INFO - root - 2017-12-06 04:19:11.356142: step 21860, loss = 2.06, batch loss = 2.00 (32.2 examples/sec; 0.249 sec/batch; 21h:27m:46s remains)
INFO - root - 2017-12-06 04:19:13.660249: step 21870, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.226 sec/batch; 19h:31m:57s remains)
INFO - root - 2017-12-06 04:19:15.969840: step 21880, loss = 2.10, batch loss = 2.04 (35.8 examples/sec; 0.224 sec/batch; 19h:17m:20s remains)
INFO - root - 2017-12-06 04:19:18.254659: step 21890, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.225 sec/batch; 19h:25m:55s remains)
INFO - root - 2017-12-06 04:19:20.557433: step 21900, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 20h:20m:52s remains)
2017-12-06 04:19:20.830850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1140718 -4.0650678 -4.0434637 -4.0582237 -4.0889373 -4.1199784 -4.1467094 -4.1538067 -4.15698 -4.1641703 -4.1770306 -4.1890669 -4.1951232 -4.196425 -4.1963329][-4.12726 -4.086164 -4.0625396 -4.0690808 -4.0911975 -4.119102 -4.1400023 -4.145359 -4.151947 -4.1655955 -4.1824069 -4.1971974 -4.2080374 -4.2182174 -4.2319679][-4.1074724 -4.0898438 -4.0835319 -4.085669 -4.0977516 -4.1220808 -4.1350389 -4.1337914 -4.132525 -4.1425972 -4.1613364 -4.1791315 -4.1968784 -4.2155008 -4.23698][-4.0579 -4.067399 -4.0792255 -4.088984 -4.1038041 -4.1257896 -4.1330833 -4.1216197 -4.1115055 -4.1204891 -4.1427908 -4.16323 -4.184454 -4.2058854 -4.2280092][-4.0156007 -4.0351939 -4.0516829 -4.0644526 -4.0886731 -4.1170926 -4.1212425 -4.095295 -4.075954 -4.0813494 -4.1098347 -4.144743 -4.1756778 -4.1970758 -4.2121797][-4.0021896 -4.01536 -4.0151258 -4.0119295 -4.0315175 -4.0675831 -4.0771542 -4.04957 -4.0335984 -4.0435119 -4.0775237 -4.1296625 -4.1750402 -4.1956515 -4.1954961][-3.9936204 -3.9948761 -3.9696741 -3.9469914 -3.9621086 -4.0064449 -4.0318952 -4.0240116 -4.0235815 -4.0415511 -4.07832 -4.1406159 -4.1894851 -4.1993227 -4.1825037][-3.9935236 -3.9846356 -3.9527123 -3.9352126 -3.954917 -4.0079212 -4.0541925 -4.0685658 -4.0794029 -4.101191 -4.1319485 -4.1833506 -4.2213974 -4.2157593 -4.1870089][-4.0424886 -4.03734 -4.00963 -3.9968231 -4.0116715 -4.0584664 -4.1104383 -4.1360755 -4.1482167 -4.1654911 -4.1879745 -4.2245069 -4.2491684 -4.2356272 -4.2040639][-4.1038823 -4.1094 -4.0868845 -4.0690703 -4.0721006 -4.1052647 -4.1524034 -4.1793556 -4.1856265 -4.195086 -4.2136087 -4.2445841 -4.2620864 -4.2472644 -4.2165847][-4.14857 -4.1614223 -4.14122 -4.11547 -4.1075888 -4.1332679 -4.169795 -4.1856852 -4.1811452 -4.186944 -4.20753 -4.2392974 -4.2539449 -4.2422581 -4.2137284][-4.1716866 -4.1814041 -4.1587782 -4.1309066 -4.116437 -4.1343122 -4.1632719 -4.1710243 -4.1579437 -4.15853 -4.1802278 -4.2124252 -4.2251205 -4.2153721 -4.1936393][-4.1795273 -4.1831937 -4.1671886 -4.147665 -4.1386929 -4.1521363 -4.1750116 -4.17824 -4.1582518 -4.1469846 -4.1615262 -4.1892633 -4.1977429 -4.1850028 -4.166214][-4.1785564 -4.1775665 -4.1716728 -4.1672745 -4.1691628 -4.180738 -4.1972055 -4.1993117 -4.181632 -4.1639872 -4.1706352 -4.1924281 -4.195838 -4.1739159 -4.151176][-4.20403 -4.2041416 -4.2034979 -4.2040749 -4.2093506 -4.2162056 -4.2235823 -4.2224536 -4.2115703 -4.1965189 -4.1986952 -4.2160192 -4.2163663 -4.1893063 -4.1642737]]...]
INFO - root - 2017-12-06 04:19:23.174107: step 21910, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 19h:45m:39s remains)
INFO - root - 2017-12-06 04:19:25.477591: step 21920, loss = 2.06, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:12m:06s remains)
INFO - root - 2017-12-06 04:19:27.783235: step 21930, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 19h:56m:33s remains)
INFO - root - 2017-12-06 04:19:30.134538: step 21940, loss = 2.04, batch loss = 1.98 (34.0 examples/sec; 0.236 sec/batch; 20h:19m:01s remains)
INFO - root - 2017-12-06 04:19:32.427328: step 21950, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 20h:20m:50s remains)
INFO - root - 2017-12-06 04:19:34.709438: step 21960, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 19h:26m:06s remains)
INFO - root - 2017-12-06 04:19:37.013230: step 21970, loss = 2.07, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 18h:56m:13s remains)
INFO - root - 2017-12-06 04:19:39.308525: step 21980, loss = 2.06, batch loss = 2.00 (36.3 examples/sec; 0.221 sec/batch; 19h:01m:21s remains)
INFO - root - 2017-12-06 04:19:41.607370: step 21990, loss = 2.06, batch loss = 2.00 (33.2 examples/sec; 0.241 sec/batch; 20h:48m:09s remains)
INFO - root - 2017-12-06 04:19:43.892571: step 22000, loss = 2.04, batch loss = 1.98 (34.7 examples/sec; 0.230 sec/batch; 19h:51m:35s remains)
2017-12-06 04:19:44.186165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3041062 -4.2969227 -4.2887354 -4.2821026 -4.2737536 -4.2675972 -4.2629676 -4.2568865 -4.2643013 -4.2776709 -4.2846603 -4.29168 -4.2930493 -4.2801714 -4.2632446][-4.2913313 -4.2787414 -4.2665467 -4.2605405 -4.2503042 -4.2364874 -4.2237668 -4.2138863 -4.22834 -4.2503581 -4.2634397 -4.276258 -4.2800026 -4.2675323 -4.2472019][-4.2804933 -4.2607889 -4.2445421 -4.242733 -4.23305 -4.2086325 -4.1821189 -4.162137 -4.1804676 -4.2144036 -4.2365327 -4.2612481 -4.270946 -4.2636414 -4.244226][-4.2750139 -4.2497654 -4.2303658 -4.2310328 -4.2229042 -4.18545 -4.1397204 -4.1066513 -4.1266842 -4.1697011 -4.2011781 -4.2418423 -4.2603106 -4.2574048 -4.2411261][-4.2754674 -4.2473679 -4.2253776 -4.2244616 -4.2140751 -4.1627769 -4.0963221 -4.0515833 -4.0747008 -4.1261234 -4.1678247 -4.222652 -4.2500753 -4.2497368 -4.2324538][-4.2753487 -4.2469392 -4.2215395 -4.2115498 -4.1894846 -4.1196332 -4.0326958 -3.987092 -4.0276732 -4.1019535 -4.1556163 -4.2210889 -4.2543011 -4.2535391 -4.2329736][-4.2737164 -4.2465358 -4.2214584 -4.1979365 -4.1511374 -4.0504937 -3.9430816 -3.9037142 -3.9733207 -4.07495 -4.1450272 -4.2107134 -4.2443242 -4.2457781 -4.2315259][-4.2730522 -4.2473893 -4.2225337 -4.1835003 -4.116785 -3.9964631 -3.8726485 -3.8320982 -3.9206963 -4.0402837 -4.1173511 -4.1768527 -4.20617 -4.2137194 -4.2136073][-4.2700186 -4.2472339 -4.2232437 -4.1815968 -4.1162248 -4.0059614 -3.8948855 -3.8503861 -3.9299023 -4.0429797 -4.1075044 -4.1511841 -4.1758513 -4.1879978 -4.1968031][-4.2659578 -4.2475047 -4.2251897 -4.1840858 -4.1253295 -4.0387664 -3.9635956 -3.9285944 -3.9871776 -4.0751219 -4.1216087 -4.1504183 -4.1720948 -4.1842866 -4.1960025][-4.2606339 -4.2442431 -4.2252607 -4.1876845 -4.1373458 -4.0748806 -4.0322242 -4.0132809 -4.0524426 -4.1156292 -4.1486993 -4.1666775 -4.1826649 -4.1917109 -4.2022247][-4.2584767 -4.2399135 -4.224422 -4.2003593 -4.165606 -4.1287632 -4.110425 -4.1046033 -4.1331306 -4.17517 -4.1986814 -4.2103648 -4.2190595 -4.2273078 -4.2371378][-4.2648335 -4.246367 -4.2386208 -4.2316685 -4.2151585 -4.1960564 -4.1907878 -4.1953244 -4.2194924 -4.2480168 -4.2657285 -4.2747374 -4.2807035 -4.2842011 -4.2879376][-4.2781105 -4.2635217 -4.2618995 -4.266551 -4.2645288 -4.2578483 -4.2596021 -4.2685595 -4.2865252 -4.3038087 -4.3156805 -4.3220239 -4.3266606 -4.3252912 -4.3214464][-4.29598 -4.2857122 -4.2861342 -4.2945085 -4.3000383 -4.3002687 -4.304143 -4.3133616 -4.323678 -4.3309197 -4.3353491 -4.339839 -4.3436022 -4.3399525 -4.3327527]]...]
INFO - root - 2017-12-06 04:19:46.482302: step 22010, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 19h:27m:53s remains)
INFO - root - 2017-12-06 04:19:48.785733: step 22020, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:40m:13s remains)
INFO - root - 2017-12-06 04:19:51.122360: step 22030, loss = 2.05, batch loss = 2.00 (33.3 examples/sec; 0.240 sec/batch; 20h:43m:24s remains)
INFO - root - 2017-12-06 04:19:53.408261: step 22040, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 19h:39m:23s remains)
INFO - root - 2017-12-06 04:19:55.716048: step 22050, loss = 2.06, batch loss = 2.01 (33.3 examples/sec; 0.240 sec/batch; 20h:44m:10s remains)
INFO - root - 2017-12-06 04:19:58.032450: step 22060, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.227 sec/batch; 19h:32m:31s remains)
INFO - root - 2017-12-06 04:20:00.348163: step 22070, loss = 2.07, batch loss = 2.01 (34.4 examples/sec; 0.233 sec/batch; 20h:03m:14s remains)
INFO - root - 2017-12-06 04:20:02.666421: step 22080, loss = 2.06, batch loss = 2.00 (34.1 examples/sec; 0.235 sec/batch; 20h:15m:31s remains)
INFO - root - 2017-12-06 04:20:04.937155: step 22090, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 19h:24m:02s remains)
INFO - root - 2017-12-06 04:20:07.274510: step 22100, loss = 2.09, batch loss = 2.04 (34.3 examples/sec; 0.233 sec/batch; 20h:07m:36s remains)
2017-12-06 04:20:07.556199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2196612 -4.2251821 -4.2185125 -4.2047086 -4.1954465 -4.2042923 -4.2099934 -4.1990714 -4.1880126 -4.1744003 -4.1679387 -4.1776772 -4.2109103 -4.2601423 -4.302772][-4.1847115 -4.1921034 -4.1869068 -4.1752429 -4.1705976 -4.1783295 -4.1840467 -4.1786571 -4.170958 -4.159286 -4.1559715 -4.1720905 -4.2118254 -4.2640696 -4.305819][-4.1456037 -4.1540823 -4.1482172 -4.143683 -4.1422544 -4.1432853 -4.1440258 -4.1454239 -4.1482558 -4.1469321 -4.1538191 -4.1810203 -4.2224832 -4.27131 -4.3066206][-4.1170006 -4.12973 -4.1260405 -4.1301212 -4.1281247 -4.1178336 -4.1078429 -4.1076708 -4.116312 -4.1318951 -4.1565833 -4.193419 -4.2352448 -4.279799 -4.3095222][-4.08968 -4.1114907 -4.1187782 -4.131556 -4.1283503 -4.1084867 -4.0820932 -4.071321 -4.0822687 -4.1114364 -4.1543322 -4.2012448 -4.2430668 -4.2848082 -4.3112569][-4.076623 -4.0996051 -4.1156158 -4.1308222 -4.123529 -4.0954466 -4.0540252 -4.0321765 -4.0419269 -4.084538 -4.1475739 -4.2034431 -4.2486219 -4.2883854 -4.313128][-4.0866456 -4.107542 -4.1288104 -4.144259 -4.1335735 -4.1009407 -4.0505586 -4.0192013 -4.0249443 -4.0727892 -4.1436892 -4.2028103 -4.2508583 -4.2899446 -4.3144612][-4.1219921 -4.133203 -4.1530371 -4.1653857 -4.1534867 -4.12034 -4.0663834 -4.0310564 -4.0344005 -4.079607 -4.1471462 -4.2017355 -4.2481489 -4.2883244 -4.3148894][-4.1625557 -4.1663532 -4.1775055 -4.1831088 -4.1751041 -4.1524725 -4.1115389 -4.0806789 -4.08497 -4.1223979 -4.1748371 -4.2156539 -4.2555208 -4.2950134 -4.3193612][-4.1869249 -4.1914382 -4.1992617 -4.203053 -4.2006216 -4.1904368 -4.1670518 -4.1483817 -4.1543374 -4.1840792 -4.2190814 -4.2449913 -4.275362 -4.3091636 -4.3278747][-4.2085595 -4.2154746 -4.222156 -4.2251964 -4.222816 -4.2223644 -4.2121086 -4.2011647 -4.2103705 -4.2369542 -4.2631035 -4.2815776 -4.3031912 -4.3261914 -4.3385358][-4.2439089 -4.2534051 -4.258935 -4.2616506 -4.2588415 -4.2583103 -4.2506208 -4.2397313 -4.2458076 -4.2699513 -4.293263 -4.3097577 -4.3241091 -4.3384113 -4.3452325][-4.2743807 -4.2860174 -4.2938442 -4.2967067 -4.2945046 -4.2912221 -4.2831125 -4.2709012 -4.2718296 -4.2890687 -4.3086762 -4.3253093 -4.3350568 -4.3433018 -4.3462429][-4.300509 -4.3117633 -4.3198757 -4.322722 -4.3217936 -4.318027 -4.3110452 -4.3015447 -4.2990789 -4.3079777 -4.3207321 -4.3341861 -4.3409123 -4.3455153 -4.3471308][-4.3206987 -4.3309851 -4.3378468 -4.3409266 -4.3407569 -4.3379517 -4.3326173 -4.32702 -4.3249693 -4.3270068 -4.3332777 -4.3413482 -4.3457775 -4.3478189 -4.3487096]]...]
INFO - root - 2017-12-06 04:20:09.876264: step 22110, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 19h:37m:43s remains)
INFO - root - 2017-12-06 04:20:12.181010: step 22120, loss = 2.06, batch loss = 2.01 (35.2 examples/sec; 0.227 sec/batch; 19h:36m:19s remains)
INFO - root - 2017-12-06 04:20:14.442199: step 22130, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 19h:25m:23s remains)
INFO - root - 2017-12-06 04:20:16.744223: step 22140, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:08m:59s remains)
INFO - root - 2017-12-06 04:20:19.025762: step 22150, loss = 2.09, batch loss = 2.03 (34.1 examples/sec; 0.235 sec/batch; 20h:14m:59s remains)
INFO - root - 2017-12-06 04:20:21.374851: step 22160, loss = 2.07, batch loss = 2.01 (31.3 examples/sec; 0.255 sec/batch; 22h:00m:57s remains)
INFO - root - 2017-12-06 04:20:23.690520: step 22170, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:05m:27s remains)
INFO - root - 2017-12-06 04:20:25.967550: step 22180, loss = 2.05, batch loss = 1.99 (35.0 examples/sec; 0.229 sec/batch; 19h:43m:29s remains)
INFO - root - 2017-12-06 04:20:28.301952: step 22190, loss = 2.08, batch loss = 2.02 (35.9 examples/sec; 0.223 sec/batch; 19h:11m:31s remains)
INFO - root - 2017-12-06 04:20:30.561163: step 22200, loss = 2.02, batch loss = 1.96 (35.4 examples/sec; 0.226 sec/batch; 19h:29m:04s remains)
2017-12-06 04:20:30.842277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2123442 -4.2205877 -4.2247715 -4.2363739 -4.2508068 -4.2583179 -4.25331 -4.2445707 -4.2351179 -4.2228942 -4.2109437 -4.1983428 -4.1839294 -4.1753731 -4.168005][-4.2094884 -4.219039 -4.2215142 -4.2257819 -4.2379723 -4.2453961 -4.2442527 -4.2411361 -4.2386904 -4.2324963 -4.2220039 -4.2055311 -4.1870489 -4.1764355 -4.1731253][-4.2006845 -4.2111087 -4.2171507 -4.2189746 -4.2271857 -4.2337971 -4.2338195 -4.2340145 -4.2354746 -4.2343764 -4.2259455 -4.210237 -4.1904669 -4.1753554 -4.1756811][-4.1770811 -4.190259 -4.2007728 -4.2092628 -4.2194171 -4.2251496 -4.2298689 -4.2334771 -4.2328568 -4.2317824 -4.2261686 -4.2109361 -4.1914406 -4.1742783 -4.1754026][-4.14894 -4.1685042 -4.1857157 -4.1954117 -4.2025828 -4.2038469 -4.2096567 -4.2119708 -4.2087274 -4.2120738 -4.2110004 -4.1978021 -4.1811614 -4.1659741 -4.1679521][-4.1222816 -4.1418324 -4.1592941 -4.1640215 -4.1599851 -4.148181 -4.1433291 -4.139946 -4.1418986 -4.1616325 -4.1723452 -4.1686711 -4.1564651 -4.1424308 -4.1426244][-4.0845075 -4.0928063 -4.1013918 -4.097054 -4.0794048 -4.0498981 -4.0329323 -4.0315037 -4.0512791 -4.0947046 -4.1228142 -4.1306696 -4.121182 -4.1067982 -4.1050959][-4.0498734 -4.0453181 -4.0404134 -4.025774 -3.997076 -3.9667723 -3.961205 -3.9763041 -4.0131783 -4.0666347 -4.1037593 -4.1171227 -4.110714 -4.0998011 -4.0954618][-4.0714159 -4.0657463 -4.0588832 -4.0450392 -4.0248408 -4.0070581 -4.0086746 -4.0225306 -4.0549607 -4.1019936 -4.132771 -4.1467185 -4.1451068 -4.138494 -4.1322107][-4.1160331 -4.1201024 -4.125248 -4.1246042 -4.1193776 -4.1130276 -4.11084 -4.1138163 -4.133152 -4.1588044 -4.1686721 -4.1731563 -4.175456 -4.1729403 -4.1680818][-4.1540489 -4.16119 -4.17659 -4.1846604 -4.1868072 -4.1879225 -4.1861629 -4.1818275 -4.1842351 -4.1897755 -4.1876745 -4.1875415 -4.1915932 -4.1917043 -4.1907668][-4.1804543 -4.1814494 -4.191257 -4.1980152 -4.2019715 -4.2078514 -4.2077479 -4.1994967 -4.1907935 -4.18661 -4.1813092 -4.1817923 -4.189127 -4.1941681 -4.1966405][-4.2115192 -4.209394 -4.21326 -4.2158589 -4.2160044 -4.2184911 -4.2170162 -4.2111583 -4.2051606 -4.1983414 -4.1894503 -4.1867046 -4.1924958 -4.1983337 -4.1998229][-4.2475352 -4.2435312 -4.2448297 -4.2441187 -4.2389197 -4.2352977 -4.2327251 -4.2310162 -4.2276139 -4.2218475 -4.2139583 -4.2101307 -4.2120628 -4.2147708 -4.2155795][-4.2648387 -4.2602091 -4.2599425 -4.258039 -4.2527776 -4.2478137 -4.2446189 -4.2449784 -4.2445321 -4.2415915 -4.2360821 -4.2334371 -4.2348037 -4.2365522 -4.2398882]]...]
INFO - root - 2017-12-06 04:20:33.160574: step 22210, loss = 2.08, batch loss = 2.02 (34.7 examples/sec; 0.231 sec/batch; 19h:53m:21s remains)
INFO - root - 2017-12-06 04:20:35.444687: step 22220, loss = 2.05, batch loss = 1.99 (32.9 examples/sec; 0.243 sec/batch; 20h:56m:31s remains)
INFO - root - 2017-12-06 04:20:37.732664: step 22230, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 19h:34m:25s remains)
INFO - root - 2017-12-06 04:20:40.057312: step 22240, loss = 2.05, batch loss = 1.99 (34.4 examples/sec; 0.233 sec/batch; 20h:03m:55s remains)
INFO - root - 2017-12-06 04:20:42.362753: step 22250, loss = 2.05, batch loss = 1.99 (34.3 examples/sec; 0.233 sec/batch; 20h:06m:59s remains)
INFO - root - 2017-12-06 04:20:44.692254: step 22260, loss = 2.06, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:10m:51s remains)
INFO - root - 2017-12-06 04:20:46.982368: step 22270, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 19h:37m:20s remains)
INFO - root - 2017-12-06 04:20:49.256774: step 22280, loss = 2.04, batch loss = 1.98 (35.5 examples/sec; 0.226 sec/batch; 19h:26m:46s remains)
INFO - root - 2017-12-06 04:20:51.573512: step 22290, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.233 sec/batch; 20h:03m:08s remains)
INFO - root - 2017-12-06 04:20:53.875159: step 22300, loss = 2.05, batch loss = 1.99 (35.2 examples/sec; 0.227 sec/batch; 19h:35m:20s remains)
2017-12-06 04:20:54.231396: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1798172 -4.2059679 -4.2397985 -4.2521367 -4.2423987 -4.2210331 -4.1875672 -4.154253 -4.1433034 -4.1588063 -4.191133 -4.2297807 -4.2640333 -4.2790933 -4.2796378][-4.192493 -4.2196045 -4.2560067 -4.2689657 -4.2563362 -4.2275791 -4.1830993 -4.14207 -4.13307 -4.1598225 -4.207128 -4.2574825 -4.2961087 -4.307282 -4.2976193][-4.1960998 -4.2200928 -4.2571607 -4.2705841 -4.2549314 -4.2167621 -4.1611719 -4.1142693 -4.1103058 -4.1486487 -4.2097411 -4.2715473 -4.3129568 -4.3231878 -4.3086638][-4.2030954 -4.2218685 -4.2558441 -4.26429 -4.2403064 -4.1886563 -4.1188922 -4.0705061 -4.0792866 -4.1332493 -4.2087655 -4.2793946 -4.3231568 -4.334177 -4.3215547][-4.2207232 -4.2326512 -4.2579546 -4.2559628 -4.2178454 -4.1476545 -4.0623322 -4.0169811 -4.0444217 -4.1166883 -4.2055421 -4.2820749 -4.3273525 -4.3418522 -4.3334789][-4.2453823 -4.2540703 -4.2702012 -4.25508 -4.1968818 -4.1002736 -3.9954088 -3.9586565 -4.0114479 -4.1030521 -4.2020807 -4.2803268 -4.3255835 -4.3435955 -4.3394103][-4.2703085 -4.2759681 -4.2847753 -4.2586126 -4.1809483 -4.0591216 -3.941812 -3.9209185 -3.999516 -4.1067181 -4.2069387 -4.2791905 -4.3186283 -4.3360834 -4.3369875][-4.2861228 -4.2864127 -4.2910604 -4.2591324 -4.1701703 -4.0398884 -3.9315894 -3.9322143 -4.0239692 -4.1319509 -4.22021 -4.2800455 -4.3110104 -4.326827 -4.3314638][-4.2814121 -4.2787471 -4.2859249 -4.2595472 -4.1746955 -4.0574536 -3.9742165 -3.9877896 -4.071559 -4.1644731 -4.2366738 -4.2816525 -4.304213 -4.3177819 -4.3252506][-4.2559795 -4.2570357 -4.2732072 -4.2574124 -4.1873994 -4.09186 -4.0311069 -4.050065 -4.1225038 -4.1979375 -4.2528872 -4.279099 -4.2887406 -4.2967706 -4.3061819][-4.21926 -4.2253261 -4.2500596 -4.2462196 -4.1963105 -4.1237769 -4.0813894 -4.1036453 -4.16459 -4.2220392 -4.2575893 -4.264317 -4.2584577 -4.2610917 -4.2737994][-4.1874166 -4.1986666 -4.2291675 -4.2333026 -4.2016354 -4.1506429 -4.1207538 -4.1441021 -4.1945057 -4.236455 -4.2541571 -4.2427759 -4.2232924 -4.2231174 -4.240674][-4.1727176 -4.186954 -4.2199254 -4.2291932 -4.2078533 -4.1687484 -4.1447763 -4.1666327 -4.2090058 -4.2376471 -4.2402081 -4.2169042 -4.1940923 -4.1992016 -4.22288][-4.1736875 -4.1911726 -4.2260866 -4.2379594 -4.2194862 -4.1822009 -4.1578488 -4.1725311 -4.2032886 -4.2194219 -4.2130995 -4.189043 -4.1766868 -4.1916819 -4.2186656][-4.1896229 -4.2114496 -4.244195 -4.2541242 -4.2308168 -4.1876225 -4.1555018 -4.1573615 -4.1737294 -4.1816053 -4.1749892 -4.1598811 -4.1660194 -4.1918879 -4.2191229]]...]
INFO - root - 2017-12-06 04:20:56.547016: step 22310, loss = 2.06, batch loss = 2.00 (34.1 examples/sec; 0.235 sec/batch; 20h:14m:05s remains)
INFO - root - 2017-12-06 04:20:58.820933: step 22320, loss = 2.05, batch loss = 2.00 (36.5 examples/sec; 0.219 sec/batch; 18h:51m:48s remains)
INFO - root - 2017-12-06 04:21:01.141545: step 22330, loss = 2.08, batch loss = 2.02 (34.4 examples/sec; 0.233 sec/batch; 20h:02m:22s remains)
INFO - root - 2017-12-06 04:21:03.416774: step 22340, loss = 2.10, batch loss = 2.04 (35.9 examples/sec; 0.223 sec/batch; 19h:11m:56s remains)
INFO - root - 2017-12-06 04:21:05.734348: step 22350, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 19h:11m:32s remains)
INFO - root - 2017-12-06 04:21:08.021817: step 22360, loss = 2.07, batch loss = 2.01 (35.7 examples/sec; 0.224 sec/batch; 19h:19m:22s remains)
INFO - root - 2017-12-06 04:21:10.363178: step 22370, loss = 2.06, batch loss = 2.00 (34.9 examples/sec; 0.229 sec/batch; 19h:44m:51s remains)
INFO - root - 2017-12-06 04:21:12.697925: step 22380, loss = 2.03, batch loss = 1.98 (34.1 examples/sec; 0.235 sec/batch; 20h:12m:10s remains)
INFO - root - 2017-12-06 04:21:14.983959: step 22390, loss = 2.06, batch loss = 2.00 (35.4 examples/sec; 0.226 sec/batch; 19h:28m:05s remains)
INFO - root - 2017-12-06 04:21:17.286359: step 22400, loss = 2.04, batch loss = 1.98 (35.0 examples/sec; 0.229 sec/batch; 19h:41m:11s remains)
2017-12-06 04:21:17.571675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3183112 -4.3198318 -4.3098288 -4.2965317 -4.2847528 -4.275691 -4.2662759 -4.2675171 -4.2792277 -4.2862358 -4.2834978 -4.2783928 -4.2763443 -4.2778392 -4.2856064][-4.3144517 -4.3141713 -4.3008442 -4.2868447 -4.2788019 -4.2734709 -4.2664042 -4.26762 -4.2786269 -4.2842221 -4.2770681 -4.2711277 -4.2716222 -4.2732081 -4.2784433][-4.3095026 -4.3066573 -4.2867088 -4.2692232 -4.2609015 -4.2558265 -4.2497506 -4.253767 -4.2671208 -4.2729073 -4.2631392 -4.2585959 -4.2650642 -4.2647362 -4.2615204][-4.3053823 -4.2980256 -4.2695937 -4.247128 -4.2362046 -4.2289214 -4.2252617 -4.2336607 -4.2475424 -4.2493644 -4.2351246 -4.231843 -4.2447419 -4.2405891 -4.2261081][-4.3023987 -4.2909284 -4.2583447 -4.2335849 -4.2211523 -4.2071991 -4.1962566 -4.2006059 -4.2108712 -4.2072468 -4.1866918 -4.181932 -4.2025228 -4.2003226 -4.1831193][-4.2999759 -4.2868481 -4.254539 -4.2260661 -4.2054329 -4.1776137 -4.1480546 -4.1406622 -4.1502175 -4.1504889 -4.1271377 -4.1224613 -4.1492324 -4.1559091 -4.1494226][-4.2968922 -4.2818193 -4.2461562 -4.2067833 -4.1706266 -4.1263366 -4.0813036 -4.0690413 -4.0901985 -4.10663 -4.0944419 -4.0942941 -4.1211224 -4.1296124 -4.1340017][-4.2901163 -4.2715359 -4.22932 -4.1761432 -4.1229286 -4.0671535 -4.0183277 -4.0116792 -4.0542445 -4.0937271 -4.0981603 -4.0988903 -4.1121321 -4.1151948 -4.1247149][-4.2788215 -4.2590151 -4.2160335 -4.1632795 -4.1074119 -4.0462055 -3.99554 -3.9860945 -4.0328245 -4.0818591 -4.0941691 -4.0931034 -4.0975819 -4.099081 -4.1104918][-4.2644687 -4.2470989 -4.2118382 -4.1709647 -4.1264906 -4.0738873 -4.0205836 -3.9973655 -4.0291181 -4.0709052 -4.0841403 -4.0817127 -4.0861845 -4.0932679 -4.1073208][-4.2480412 -4.2339082 -4.209024 -4.1817217 -4.1517825 -4.11731 -4.0757313 -4.0497823 -4.0644383 -4.0910778 -4.0966687 -4.0920248 -4.0970984 -4.1061831 -4.1178284][-4.2391095 -4.2241163 -4.2045484 -4.18505 -4.1663265 -4.1479797 -4.1220875 -4.1047907 -4.1131306 -4.1267223 -4.1244416 -4.1172342 -4.12158 -4.1288323 -4.1343384][-4.2414012 -4.2243285 -4.2056489 -4.1889839 -4.1777172 -4.1691813 -4.1553311 -4.1487823 -4.1605334 -4.1708617 -4.1625161 -4.1513481 -4.152812 -4.1549368 -4.1514583][-4.2516494 -4.2347808 -4.21922 -4.2059565 -4.1980228 -4.19327 -4.1862116 -4.1853681 -4.1977005 -4.2067161 -4.1976752 -4.1859097 -4.1855025 -4.1852012 -4.1801715][-4.2661748 -4.2518535 -4.2426677 -4.2353415 -4.2311707 -4.2282228 -4.2237377 -4.2241259 -4.2329082 -4.2385759 -4.2296796 -4.2184477 -4.21745 -4.2194781 -4.2214222]]...]
INFO - root - 2017-12-06 04:21:19.876371: step 22410, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.225 sec/batch; 19h:22m:14s remains)
INFO - root - 2017-12-06 04:21:22.177588: step 22420, loss = 2.09, batch loss = 2.03 (33.2 examples/sec; 0.241 sec/batch; 20h:44m:16s remains)
INFO - root - 2017-12-06 04:21:24.472062: step 22430, loss = 2.01, batch loss = 1.95 (34.0 examples/sec; 0.235 sec/batch; 20h:15m:02s remains)
INFO - root - 2017-12-06 04:21:26.787987: step 22440, loss = 2.05, batch loss = 1.99 (33.1 examples/sec; 0.241 sec/batch; 20h:47m:52s remains)
INFO - root - 2017-12-06 04:21:29.082064: step 22450, loss = 2.05, batch loss = 1.99 (34.2 examples/sec; 0.234 sec/batch; 20h:09m:10s remains)
INFO - root - 2017-12-06 04:21:31.351730: step 22460, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:07m:28s remains)
INFO - root - 2017-12-06 04:21:33.674722: step 22470, loss = 2.06, batch loss = 2.00 (36.0 examples/sec; 0.222 sec/batch; 19h:07m:54s remains)
INFO - root - 2017-12-06 04:21:35.970492: step 22480, loss = 2.12, batch loss = 2.06 (33.7 examples/sec; 0.237 sec/batch; 20h:25m:55s remains)
INFO - root - 2017-12-06 04:21:38.307253: step 22490, loss = 2.05, batch loss = 1.99 (32.1 examples/sec; 0.249 sec/batch; 21h:27m:24s remains)
INFO - root - 2017-12-06 04:21:40.612813: step 22500, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 19h:18m:53s remains)
2017-12-06 04:21:40.914396: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2900548 -4.2899928 -4.2885332 -4.2863073 -4.28474 -4.2832088 -4.2813053 -4.2799091 -4.2792983 -4.2783918 -4.2764263 -4.2736311 -4.2702909 -4.2680964 -4.264082][-4.2998915 -4.298851 -4.2952852 -4.289948 -4.284121 -4.2785735 -4.27492 -4.2750063 -4.2765713 -4.2750559 -4.2700424 -4.2647371 -4.2592111 -4.2577357 -4.260303][-4.3035812 -4.3021297 -4.2971282 -4.2889657 -4.27809 -4.2671471 -4.2601662 -4.2602158 -4.262506 -4.2601295 -4.2542291 -4.2502789 -4.2477717 -4.2491603 -4.2565212][-4.3056297 -4.30375 -4.2976475 -4.2867413 -4.2704911 -4.2532635 -4.2404809 -4.2368207 -4.236608 -4.2320418 -4.2280564 -4.2317495 -4.2385135 -4.245985 -4.2554555][-4.3083525 -4.30589 -4.2988071 -4.285708 -4.2645249 -4.2407084 -4.2215419 -4.2126713 -4.2079687 -4.1992078 -4.1962795 -4.2074747 -4.2221971 -4.23531 -4.2477884][-4.3100142 -4.3069725 -4.2993379 -4.2859607 -4.2624912 -4.2345533 -4.2104211 -4.1959786 -4.1861157 -4.1724582 -4.1674819 -4.1800056 -4.1977086 -4.2157516 -4.2327757][-4.3088179 -4.3057113 -4.2988758 -4.2878 -4.2658024 -4.2386694 -4.2150192 -4.1980371 -4.1845 -4.165381 -4.1525064 -4.1582284 -4.172524 -4.1930642 -4.2141695][-4.3053217 -4.3022146 -4.2963891 -4.2880855 -4.2692647 -4.2452707 -4.22525 -4.2085347 -4.1918492 -4.1665993 -4.145031 -4.1406927 -4.1495996 -4.1727457 -4.1975107][-4.2996731 -4.2968268 -4.2917767 -4.28493 -4.2670059 -4.2440934 -4.2259583 -4.2079272 -4.1878495 -4.1579595 -4.1331115 -4.1214309 -4.1297603 -4.1579242 -4.1851707][-4.289876 -4.2882562 -4.2844691 -4.2771349 -4.2582655 -4.234736 -4.2186079 -4.1997504 -4.1752124 -4.1439171 -4.1184254 -4.1034989 -4.1155739 -4.1484675 -4.1771946][-4.2794466 -4.2806382 -4.2782521 -4.2685709 -4.2465448 -4.2230659 -4.2100735 -4.1936321 -4.1693258 -4.1403155 -4.1156144 -4.0997849 -4.1126752 -4.1447144 -4.1720986][-4.2669644 -4.2736111 -4.2737222 -4.2615175 -4.2366457 -4.2127743 -4.2007227 -4.190527 -4.171937 -4.1449556 -4.1192131 -4.102396 -4.112309 -4.1392975 -4.1626987][-4.2578135 -4.2688622 -4.2703171 -4.2562852 -4.2316232 -4.2076483 -4.1936488 -4.1898141 -4.1794939 -4.1581388 -4.1371446 -4.1206541 -4.1252046 -4.1439652 -4.1583219][-4.2487483 -4.260973 -4.262445 -4.2497511 -4.2297406 -4.2096434 -4.1972828 -4.2015233 -4.20287 -4.1910267 -4.1743331 -4.155117 -4.148283 -4.1555595 -4.1583681][-4.2364569 -4.2473707 -4.2495341 -4.2421274 -4.2309914 -4.2192106 -4.2135911 -4.2234898 -4.2320466 -4.2266388 -4.2098374 -4.1847939 -4.1653848 -4.1609468 -4.1573153]]...]
INFO - root - 2017-12-06 04:21:43.214003: step 22510, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.231 sec/batch; 19h:51m:36s remains)
INFO - root - 2017-12-06 04:21:45.569438: step 22520, loss = 2.06, batch loss = 2.01 (35.0 examples/sec; 0.229 sec/batch; 19h:41m:49s remains)
INFO - root - 2017-12-06 04:21:47.874640: step 22530, loss = 2.06, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:28m:26s remains)
INFO - root - 2017-12-06 04:21:50.170690: step 22540, loss = 2.09, batch loss = 2.03 (34.2 examples/sec; 0.234 sec/batch; 20h:08m:14s remains)
INFO - root - 2017-12-06 04:21:52.473402: step 22550, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.224 sec/batch; 19h:19m:20s remains)
INFO - root - 2017-12-06 04:21:54.782144: step 22560, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.235 sec/batch; 20h:11m:36s remains)
INFO - root - 2017-12-06 04:21:57.082852: step 22570, loss = 2.03, batch loss = 1.97 (35.5 examples/sec; 0.225 sec/batch; 19h:24m:00s remains)
INFO - root - 2017-12-06 04:21:59.395531: step 22580, loss = 2.05, batch loss = 1.99 (32.9 examples/sec; 0.243 sec/batch; 20h:56m:08s remains)
INFO - root - 2017-12-06 04:22:01.677450: step 22590, loss = 2.07, batch loss = 2.01 (34.4 examples/sec; 0.233 sec/batch; 20h:01m:28s remains)
INFO - root - 2017-12-06 04:22:03.973818: step 22600, loss = 2.08, batch loss = 2.02 (33.5 examples/sec; 0.239 sec/batch; 20h:32m:47s remains)
2017-12-06 04:22:04.272787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2793264 -4.2713671 -4.2643762 -4.2637663 -4.2696214 -4.2780895 -4.2870421 -4.3020678 -4.3129482 -4.3165779 -4.3177748 -4.3202715 -4.3209891 -4.3183827 -4.3147449][-4.2530951 -4.232512 -4.2143016 -4.2052288 -4.206696 -4.2128968 -4.2223363 -4.2431321 -4.2620339 -4.2679176 -4.2711067 -4.2771888 -4.2820263 -4.2817807 -4.2794127][-4.2209506 -4.185576 -4.1585627 -4.1424122 -4.13731 -4.1391444 -4.1466641 -4.17763 -4.20949 -4.2163005 -4.2196589 -4.2290626 -4.2393913 -4.2445383 -4.2448497][-4.1886649 -4.1384487 -4.1022944 -4.0773354 -4.0599804 -4.0512109 -4.053874 -4.094768 -4.1510143 -4.1634326 -4.1678009 -4.1792021 -4.1934981 -4.2090898 -4.214324][-4.1628175 -4.0990911 -4.0521736 -4.0137653 -3.9805248 -3.9543433 -3.9314783 -3.974694 -4.0650487 -4.0888915 -4.0977945 -4.1134272 -4.1385665 -4.1701331 -4.1893334][-4.1297336 -4.0441742 -3.9778838 -3.9202709 -3.867053 -3.797709 -3.7200778 -3.771873 -3.9130273 -3.9584756 -3.9728637 -4.0047708 -4.056653 -4.110014 -4.14852][-4.0983715 -3.9900911 -3.9018745 -3.8260021 -3.7470844 -3.6215262 -3.4626663 -3.5346911 -3.7484846 -3.8276412 -3.856328 -3.9130497 -3.9906626 -4.0556827 -4.1061082][-4.0988894 -3.9896688 -3.9000959 -3.8332949 -3.7600949 -3.6361535 -3.4930665 -3.5751107 -3.7703071 -3.8523529 -3.8911932 -3.9585338 -4.0238881 -4.0691791 -4.1100564][-4.1301165 -4.0444255 -3.9816186 -3.9437144 -3.8978386 -3.8250189 -3.7623949 -3.8184886 -3.9276171 -3.9731922 -3.9992247 -4.048944 -4.0894642 -4.1157637 -4.1450214][-4.1570616 -4.0921068 -4.0449438 -4.0177369 -3.9822745 -3.9412775 -3.9253531 -3.9629405 -4.0153575 -4.0388775 -4.0534453 -4.0840464 -4.1132779 -4.1376019 -4.1661072][-4.187614 -4.1332784 -4.0889297 -4.0568876 -4.0168233 -3.9883702 -3.9952981 -4.02442 -4.0557308 -4.0704231 -4.0829568 -4.1055293 -4.1269636 -4.152504 -4.1818032][-4.2195339 -4.1773467 -4.1416106 -4.1121893 -4.0782571 -4.0597458 -4.0734096 -4.1021328 -4.1260133 -4.1327329 -4.1455431 -4.1662211 -4.1840076 -4.2083082 -4.2309408][-4.2622 -4.2359538 -4.2144818 -4.1964407 -4.1765051 -4.1676822 -4.1808991 -4.2003541 -4.2135482 -4.2134328 -4.2202892 -4.2384171 -4.2542768 -4.2718763 -4.2858567][-4.2959118 -4.28156 -4.2705269 -4.25925 -4.246726 -4.2447219 -4.2538762 -4.2647057 -4.271203 -4.2700591 -4.2717381 -4.2827482 -4.2939873 -4.305932 -4.3133111][-4.3123908 -4.3050876 -4.2998877 -4.2926145 -4.2853503 -4.2858639 -4.2912574 -4.2972507 -4.3010917 -4.3002758 -4.3005381 -4.3066878 -4.3130503 -4.3192997 -4.3226752]]...]
INFO - root - 2017-12-06 04:22:06.540087: step 22610, loss = 2.07, batch loss = 2.01 (34.9 examples/sec; 0.229 sec/batch; 19h:43m:48s remains)
INFO - root - 2017-12-06 04:22:08.810714: step 22620, loss = 2.08, batch loss = 2.03 (35.2 examples/sec; 0.227 sec/batch; 19h:32m:08s remains)
INFO - root - 2017-12-06 04:22:11.191574: step 22630, loss = 2.05, batch loss = 1.99 (32.9 examples/sec; 0.243 sec/batch; 20h:57m:23s remains)
INFO - root - 2017-12-06 04:22:13.442816: step 22640, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.226 sec/batch; 19h:28m:58s remains)
INFO - root - 2017-12-06 04:22:15.741916: step 22650, loss = 2.06, batch loss = 2.00 (36.8 examples/sec; 0.217 sec/batch; 18h:42m:03s remains)
INFO - root - 2017-12-06 04:22:18.009392: step 22660, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 19h:47m:44s remains)
INFO - root - 2017-12-06 04:22:20.357742: step 22670, loss = 2.07, batch loss = 2.01 (34.2 examples/sec; 0.234 sec/batch; 20h:09m:28s remains)
INFO - root - 2017-12-06 04:22:22.652897: step 22680, loss = 2.08, batch loss = 2.02 (35.0 examples/sec; 0.228 sec/batch; 19h:39m:45s remains)
INFO - root - 2017-12-06 04:22:24.948709: step 22690, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 19h:37m:03s remains)
INFO - root - 2017-12-06 04:22:27.280575: step 22700, loss = 2.05, batch loss = 1.99 (35.0 examples/sec; 0.228 sec/batch; 19h:38m:33s remains)
2017-12-06 04:22:27.549884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2167192 -4.1908636 -4.16109 -4.1236653 -4.1012139 -4.0997124 -4.0880437 -4.081655 -4.1025019 -4.1007314 -4.0775971 -4.0581536 -4.0673971 -4.1257515 -4.1922832][-4.2285676 -4.202002 -4.1686835 -4.1258259 -4.10639 -4.1043224 -4.0911632 -4.0851836 -4.1008854 -4.0875893 -4.0544729 -4.0209765 -4.0210505 -4.0821481 -4.1563044][-4.1968842 -4.1758 -4.1444254 -4.1045461 -4.0882959 -4.0855513 -4.0762615 -4.0806317 -4.0985265 -4.0866518 -4.0507441 -4.01045 -3.9977775 -4.0498567 -4.1190286][-4.1317205 -4.1162438 -4.0949636 -4.0676374 -4.0523214 -4.0428796 -4.03594 -4.0530567 -4.0816684 -4.0808706 -4.0548415 -4.0190668 -4.0015016 -4.0332212 -4.0887842][-4.0783153 -4.0687194 -4.0568471 -4.0366716 -4.021709 -4.0089135 -4.0051947 -4.0327215 -4.0689421 -4.0752239 -4.0537753 -4.0235023 -4.0079618 -4.0236945 -4.0616665][-4.0198884 -4.0216155 -4.0149221 -3.9927578 -3.9755113 -3.9585733 -3.947819 -3.9903123 -4.0452 -4.0637112 -4.0520091 -4.0289817 -4.0107145 -4.0017257 -4.0142746][-3.9988589 -4.0079236 -3.996758 -3.9623241 -3.9253376 -3.8708167 -3.8266096 -3.8845024 -3.9704244 -4.0140414 -4.021646 -4.0131073 -3.9929132 -3.9646008 -3.946557][-4.0390625 -4.0510674 -4.0392017 -4.001039 -3.9461088 -3.8521438 -3.7656467 -3.8145351 -3.9140639 -3.9752984 -3.9991975 -4.0069256 -3.9951653 -3.9632468 -3.933877][-4.0870681 -4.1004543 -4.0986738 -4.0787334 -4.0395412 -3.9632263 -3.8920743 -3.9095259 -3.9725142 -4.0161996 -4.0367765 -4.0481124 -4.0446477 -4.0209131 -3.9920726][-4.1254044 -4.1338797 -4.1345634 -4.1253991 -4.0996852 -4.0480976 -4.0043631 -4.01445 -4.0523033 -4.0769529 -4.0900741 -4.0976458 -4.0933423 -4.0723619 -4.0451393][-4.1641846 -4.1711216 -4.1709819 -4.1579561 -4.1306586 -4.0863976 -4.0515847 -4.0570087 -4.0878091 -4.1101284 -4.1254573 -4.13215 -4.12842 -4.1112866 -4.0856037][-4.2078848 -4.21177 -4.2093267 -4.1959205 -4.1713858 -4.1324568 -4.0998406 -4.0989833 -4.120779 -4.1424842 -4.1636815 -4.1731849 -4.1741915 -4.1654382 -4.1466475][-4.2326217 -4.2314277 -4.2245178 -4.2144675 -4.2023721 -4.183001 -4.1623211 -4.1606679 -4.1752915 -4.1887364 -4.202477 -4.2107553 -4.21654 -4.2142992 -4.2026467][-4.2483387 -4.2424812 -4.2285323 -4.2152677 -4.2087359 -4.2036066 -4.1955009 -4.1952758 -4.2062225 -4.2155318 -4.2216573 -4.2273664 -4.2334571 -4.2321367 -4.2240849][-4.2639213 -4.2560287 -4.2387652 -4.2207737 -4.2118335 -4.2076569 -4.2021704 -4.2037296 -4.2134948 -4.2194176 -4.2217746 -4.2271948 -4.23468 -4.2369413 -4.2347074]]...]
INFO - root - 2017-12-06 04:22:29.832508: step 22710, loss = 2.05, batch loss = 1.99 (34.5 examples/sec; 0.232 sec/batch; 19h:56m:00s remains)
INFO - root - 2017-12-06 04:22:32.154772: step 22720, loss = 2.05, batch loss = 1.99 (34.5 examples/sec; 0.232 sec/batch; 19h:56m:14s remains)
INFO - root - 2017-12-06 04:22:34.444106: step 22730, loss = 2.06, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:12m:53s remains)
INFO - root - 2017-12-06 04:22:36.767852: step 22740, loss = 2.09, batch loss = 2.03 (34.8 examples/sec; 0.230 sec/batch; 19h:47m:27s remains)
INFO - root - 2017-12-06 04:22:39.043226: step 22750, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 19h:55m:06s remains)
INFO - root - 2017-12-06 04:22:41.323970: step 22760, loss = 2.04, batch loss = 1.98 (33.9 examples/sec; 0.236 sec/batch; 20h:17m:24s remains)
INFO - root - 2017-12-06 04:22:43.629671: step 22770, loss = 2.09, batch loss = 2.03 (35.3 examples/sec; 0.227 sec/batch; 19h:31m:16s remains)
INFO - root - 2017-12-06 04:22:45.929883: step 22780, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.231 sec/batch; 19h:50m:56s remains)
INFO - root - 2017-12-06 04:22:48.207411: step 22790, loss = 2.05, batch loss = 1.99 (35.0 examples/sec; 0.229 sec/batch; 19h:41m:22s remains)
INFO - root - 2017-12-06 04:22:50.495432: step 22800, loss = 2.04, batch loss = 1.98 (35.6 examples/sec; 0.225 sec/batch; 19h:19m:17s remains)
2017-12-06 04:22:50.798963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2813797 -4.2919412 -4.2959404 -4.2905693 -4.2767572 -4.2671466 -4.265213 -4.265 -4.2685962 -4.2737722 -4.28066 -4.2868996 -4.292707 -4.3001184 -4.2968693][-4.2645612 -4.2717733 -4.2755418 -4.270195 -4.2574782 -4.2504897 -4.2528658 -4.2569094 -4.2667909 -4.2792087 -4.2875409 -4.2913256 -4.2929163 -4.2949014 -4.2863355][-4.25739 -4.2558556 -4.2511716 -4.2368312 -4.2227659 -4.2204623 -4.2267003 -4.2349205 -4.2529559 -4.27305 -4.2830706 -4.2872419 -4.2894273 -4.2887921 -4.2777462][-4.2536869 -4.240119 -4.2204838 -4.1929812 -4.1728206 -4.1690626 -4.1743164 -4.1884584 -4.2193227 -4.2496929 -4.2658434 -4.2760639 -4.2835083 -4.2841616 -4.27466][-4.2543321 -4.2292509 -4.1898732 -4.1427684 -4.1087518 -4.0924683 -4.0930905 -4.1149597 -4.1605473 -4.204041 -4.2327132 -4.2515392 -4.2666168 -4.2716618 -4.2693071][-4.2434225 -4.2077127 -4.1503406 -4.0847421 -4.0333586 -4.00014 -3.9942079 -4.0270967 -4.0873709 -4.1454058 -4.1874213 -4.2134395 -4.2342968 -4.2477603 -4.2600074][-4.2223673 -4.1771922 -4.1088233 -4.035038 -3.9737539 -3.9237566 -3.906534 -3.9471745 -4.0156016 -4.0850639 -4.1378555 -4.1692414 -4.1946335 -4.2177906 -4.2458849][-4.1965041 -4.1490068 -4.0831518 -4.01591 -3.9589913 -3.9008183 -3.873383 -3.9056854 -3.9674728 -4.039814 -4.1004572 -4.13843 -4.1737747 -4.2058558 -4.2409868][-4.1848474 -4.1451874 -4.0918489 -4.0387597 -3.991466 -3.9370568 -3.9100945 -3.9276214 -3.9698839 -4.0301638 -4.0867004 -4.1275792 -4.172533 -4.2113676 -4.2458396][-4.1942363 -4.1649966 -4.1267757 -4.0898943 -4.0599442 -4.0239635 -4.0052381 -4.0127373 -4.0361571 -4.0764842 -4.1155009 -4.1490397 -4.1902146 -4.2246385 -4.2508607][-4.2144017 -4.1937833 -4.1687288 -4.1501212 -4.1395388 -4.1208792 -4.1090064 -4.1109543 -4.1218019 -4.1463561 -4.1672735 -4.189456 -4.2156615 -4.2346807 -4.2470183][-4.229692 -4.2170582 -4.2036872 -4.1999702 -4.2031336 -4.1979523 -4.1934848 -4.1920371 -4.1962767 -4.2082429 -4.2150149 -4.2258043 -4.2372527 -4.2404351 -4.2428641][-4.2380137 -4.2331071 -4.2295151 -4.2347846 -4.2450924 -4.2491412 -4.2512636 -4.250535 -4.2511964 -4.2508478 -4.246069 -4.2430081 -4.2430787 -4.2362547 -4.2329969][-4.2415228 -4.237885 -4.2382231 -4.2453637 -4.2580848 -4.2727065 -4.2843194 -4.2877913 -4.2880187 -4.2796116 -4.2633462 -4.248404 -4.2401471 -4.22953 -4.225019][-4.2498274 -4.238987 -4.2344828 -4.23718 -4.2497325 -4.2714972 -4.2895784 -4.2996759 -4.3024487 -4.2898483 -4.2692142 -4.2512269 -4.2395954 -4.2271609 -4.2228355]]...]
INFO - root - 2017-12-06 04:22:53.070629: step 22810, loss = 2.06, batch loss = 2.00 (34.3 examples/sec; 0.233 sec/batch; 20h:02m:36s remains)
INFO - root - 2017-12-06 04:22:55.394056: step 22820, loss = 2.05, batch loss = 1.99 (34.9 examples/sec; 0.229 sec/batch; 19h:44m:01s remains)
INFO - root - 2017-12-06 04:22:57.702002: step 22830, loss = 2.10, batch loss = 2.04 (32.6 examples/sec; 0.245 sec/batch; 21h:06m:07s remains)
INFO - root - 2017-12-06 04:23:00.005719: step 22840, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 19h:45m:32s remains)
INFO - root - 2017-12-06 04:23:02.308479: step 22850, loss = 2.04, batch loss = 1.99 (35.5 examples/sec; 0.225 sec/batch; 19h:21m:42s remains)
INFO - root - 2017-12-06 04:23:04.627874: step 22860, loss = 2.10, batch loss = 2.04 (34.5 examples/sec; 0.232 sec/batch; 19h:55m:13s remains)
INFO - root - 2017-12-06 04:23:06.907106: step 22870, loss = 2.03, batch loss = 1.97 (35.3 examples/sec; 0.226 sec/batch; 19h:28m:21s remains)
INFO - root - 2017-12-06 04:23:09.246547: step 22880, loss = 2.06, batch loss = 2.01 (31.9 examples/sec; 0.251 sec/batch; 21h:35m:26s remains)
INFO - root - 2017-12-06 04:23:11.608677: step 22890, loss = 2.10, batch loss = 2.05 (33.5 examples/sec; 0.239 sec/batch; 20h:32m:10s remains)
INFO - root - 2017-12-06 04:23:13.920868: step 22900, loss = 2.02, batch loss = 1.96 (34.4 examples/sec; 0.233 sec/batch; 20h:00m:22s remains)
2017-12-06 04:23:14.228204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3153229 -4.3114843 -4.3086953 -4.3042884 -4.2999034 -4.2983837 -4.3001752 -4.3051486 -4.3114781 -4.3182034 -4.32234 -4.3230996 -4.3221436 -4.3206682 -4.3193197][-4.3019342 -4.2970223 -4.2925644 -4.2853785 -4.2775326 -4.2742052 -4.2753706 -4.2806964 -4.2896543 -4.3004961 -4.3114686 -4.3185449 -4.3212466 -4.3205867 -4.3180094][-4.28183 -4.2745094 -4.2668533 -4.2532477 -4.239048 -4.2314544 -4.2306223 -4.2367139 -4.2498765 -4.2668233 -4.2845311 -4.2999969 -4.3107958 -4.3146987 -4.3131618][-4.2438779 -4.2338481 -4.222827 -4.2039132 -4.1817608 -4.1655097 -4.1602159 -4.1663337 -4.1861649 -4.2123013 -4.2378941 -4.2629366 -4.2823548 -4.2923245 -4.2953591][-4.1847463 -4.1738691 -4.1628623 -4.1401219 -4.1080389 -4.0788064 -4.0613837 -4.0612769 -4.0925736 -4.1343803 -4.1734195 -4.2123914 -4.2409925 -4.2563238 -4.2631011][-4.111382 -4.0984287 -4.0872312 -4.0593586 -4.0147157 -3.9646676 -3.9166651 -3.902643 -3.9545188 -4.0230284 -4.0832534 -4.1424022 -4.1847515 -4.2057195 -4.2171764][-4.0448465 -4.02546 -4.0101881 -3.9758978 -3.9218123 -3.8508251 -3.7661395 -3.7342186 -3.8084753 -3.9014206 -3.9816916 -4.059999 -4.1164103 -4.1477408 -4.1662788][-4.0190406 -3.997937 -3.9812863 -3.9514768 -3.9124022 -3.8596859 -3.788918 -3.7596009 -3.8143387 -3.8856733 -3.9551535 -4.0294757 -4.0865669 -4.122149 -4.1426229][-4.0575385 -4.0420189 -4.0304651 -4.0137134 -3.9996893 -3.981395 -3.9479489 -3.9296975 -3.9504232 -3.9867208 -4.0336123 -4.0891142 -4.1333127 -4.1614656 -4.1776905][-4.1339164 -4.1261525 -4.120676 -4.1098385 -4.102654 -4.0982671 -4.0859122 -4.07376 -4.0818949 -4.1040297 -4.1381769 -4.1788096 -4.2096124 -4.2278004 -4.2380381][-4.2015929 -4.1988153 -4.19602 -4.1895819 -4.1857185 -4.1860948 -4.1836143 -4.177002 -4.1811013 -4.1930323 -4.2122984 -4.2385926 -4.2593913 -4.26956 -4.27443][-4.2294512 -4.2296019 -4.2291865 -4.2258654 -4.2248015 -4.226696 -4.2278662 -4.2254806 -4.2285252 -4.2333159 -4.2422862 -4.2570963 -4.269084 -4.2727785 -4.2727523][-4.2125554 -4.2141676 -4.214776 -4.2136908 -4.2140942 -4.2165227 -4.2177458 -4.2167292 -4.2185869 -4.2203341 -4.223814 -4.231132 -4.2370863 -4.2373066 -4.23635][-4.1954722 -4.1973615 -4.1970525 -4.1957812 -4.1956706 -4.1960549 -4.1958461 -4.1946449 -4.1948519 -4.1935959 -4.1932592 -4.1955662 -4.1983829 -4.198854 -4.2013311][-4.2090144 -4.2092338 -4.207593 -4.2070875 -4.2078266 -4.2081084 -4.2077641 -4.2077742 -4.2071738 -4.2040062 -4.2017255 -4.2016239 -4.2022486 -4.2031932 -4.2071643]]...]
INFO - root - 2017-12-06 04:23:16.538491: step 22910, loss = 2.08, batch loss = 2.02 (33.0 examples/sec; 0.242 sec/batch; 20h:49m:32s remains)
INFO - root - 2017-12-06 04:23:18.847046: step 22920, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 19h:25m:21s remains)
INFO - root - 2017-12-06 04:23:21.201935: step 22930, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 19h:51m:44s remains)
INFO - root - 2017-12-06 04:23:23.496659: step 22940, loss = 2.08, batch loss = 2.02 (33.6 examples/sec; 0.238 sec/batch; 20h:27m:01s remains)
INFO - root - 2017-12-06 04:23:25.831755: step 22950, loss = 2.06, batch loss = 2.00 (35.3 examples/sec; 0.227 sec/batch; 19h:28m:44s remains)
INFO - root - 2017-12-06 04:23:28.142634: step 22960, loss = 2.05, batch loss = 1.99 (35.6 examples/sec; 0.224 sec/batch; 19h:17m:56s remains)
INFO - root - 2017-12-06 04:23:30.434829: step 22970, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 19h:24m:45s remains)
INFO - root - 2017-12-06 04:23:32.727292: step 22980, loss = 2.05, batch loss = 1.99 (35.7 examples/sec; 0.224 sec/batch; 19h:16m:45s remains)
INFO - root - 2017-12-06 04:23:35.000049: step 22990, loss = 2.10, batch loss = 2.04 (35.6 examples/sec; 0.225 sec/batch; 19h:20m:02s remains)
INFO - root - 2017-12-06 04:23:37.313332: step 23000, loss = 2.06, batch loss = 2.00 (31.7 examples/sec; 0.252 sec/batch; 21h:39m:58s remains)
2017-12-06 04:23:37.592339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1553597 -4.1820192 -4.1831994 -4.1544557 -4.1140285 -4.0737829 -4.0616813 -4.0718312 -4.0866694 -4.0982785 -4.1158257 -4.1300611 -4.1162825 -4.1147218 -4.1360168][-4.1699495 -4.1915445 -4.1967354 -4.1776667 -4.1438041 -4.0990829 -4.0809655 -4.0920048 -4.1184034 -4.13692 -4.1567264 -4.1800733 -4.1692271 -4.1489763 -4.1481671][-4.1806192 -4.2018051 -4.2111096 -4.2006192 -4.1727595 -4.1306758 -4.1110654 -4.1161966 -4.1382866 -4.1512742 -4.1702003 -4.2037411 -4.2063203 -4.1873841 -4.1714849][-4.177237 -4.2003045 -4.21302 -4.2073941 -4.1841598 -4.1461205 -4.1197271 -4.1102967 -4.1270761 -4.14146 -4.1644759 -4.2031717 -4.21527 -4.210176 -4.193162][-4.1590343 -4.1774979 -4.1917348 -4.1936765 -4.1786289 -4.1470413 -4.1202035 -4.10164 -4.1098514 -4.1246419 -4.1545725 -4.1953592 -4.2100654 -4.2148204 -4.20309][-4.1471853 -4.15581 -4.1691756 -4.1795778 -4.1713567 -4.1468582 -4.126524 -4.1084208 -4.1053681 -4.1167965 -4.1463141 -4.1822691 -4.1951566 -4.2124953 -4.2115846][-4.1340775 -4.133328 -4.1456604 -4.1601286 -4.1531568 -4.1341982 -4.121789 -4.114006 -4.105691 -4.1078386 -4.1269579 -4.1492896 -4.1598377 -4.1885157 -4.1981645][-4.1222987 -4.1133852 -4.1223669 -4.132874 -4.1217995 -4.1050491 -4.1006722 -4.1086454 -4.1085725 -4.1043005 -4.1086097 -4.1134396 -4.1217241 -4.1545444 -4.169847][-4.1262479 -4.1133833 -4.1192193 -4.121912 -4.1079988 -4.0881381 -4.08671 -4.1129093 -4.133986 -4.1390152 -4.1387744 -4.1333437 -4.1348743 -4.1607556 -4.1746154][-4.122191 -4.1131005 -4.12009 -4.1232491 -4.1137061 -4.0910316 -4.0844722 -4.1209674 -4.1647215 -4.18779 -4.1949868 -4.1947856 -4.1988788 -4.2127171 -4.2182565][-4.1107039 -4.1046491 -4.1185989 -4.1377182 -4.13965 -4.1190567 -4.1069884 -4.1376672 -4.185719 -4.217176 -4.2311492 -4.2378721 -4.2481451 -4.260354 -4.2642231][-4.1031327 -4.0948963 -4.1174932 -4.1555595 -4.1734638 -4.163157 -4.147203 -4.15927 -4.1923461 -4.2159052 -4.2297029 -4.2439313 -4.2600179 -4.2755532 -4.2842817][-4.0894041 -4.0698972 -4.0872574 -4.1313629 -4.16799 -4.1757345 -4.1641459 -4.1662483 -4.1879635 -4.2048321 -4.2142091 -4.2335091 -4.253365 -4.271296 -4.2792811][-4.0613313 -4.03685 -4.0476995 -4.0941272 -4.1413231 -4.1599288 -4.1581759 -4.1593847 -4.174932 -4.19083 -4.20101 -4.21854 -4.2377114 -4.2523265 -4.2580123][-4.0191813 -3.9970737 -4.0082564 -4.063427 -4.12621 -4.1530304 -4.1500449 -4.1444416 -4.1508117 -4.1627607 -4.1751208 -4.1917224 -4.2083907 -4.2211013 -4.227324]]...]
INFO - root - 2017-12-06 04:23:39.901825: step 23010, loss = 2.07, batch loss = 2.01 (35.1 examples/sec; 0.228 sec/batch; 19h:35m:49s remains)
INFO - root - 2017-12-06 04:23:42.195041: step 23020, loss = 2.06, batch loss = 2.01 (33.6 examples/sec; 0.238 sec/batch; 20h:26m:33s remains)
INFO - root - 2017-12-06 04:23:44.466862: step 23030, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.226 sec/batch; 19h:23m:43s remains)
INFO - root - 2017-12-06 04:23:46.783728: step 23040, loss = 2.05, batch loss = 1.99 (33.3 examples/sec; 0.240 sec/batch; 20h:38m:13s remains)
INFO - root - 2017-12-06 04:23:49.050343: step 23050, loss = 2.09, batch loss = 2.04 (35.0 examples/sec; 0.229 sec/batch; 19h:39m:03s remains)
INFO - root - 2017-12-06 04:23:51.384232: step 23060, loss = 2.06, batch loss = 2.00 (33.7 examples/sec; 0.237 sec/batch; 20h:22m:50s remains)
INFO - root - 2017-12-06 04:23:53.666134: step 23070, loss = 2.08, batch loss = 2.02 (34.5 examples/sec; 0.232 sec/batch; 19h:57m:35s remains)
INFO - root - 2017-12-06 04:23:55.941925: step 23080, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 19h:36m:44s remains)
INFO - root - 2017-12-06 04:23:58.250565: step 23090, loss = 2.05, batch loss = 1.99 (34.3 examples/sec; 0.233 sec/batch; 20h:01m:19s remains)
INFO - root - 2017-12-06 04:24:00.560411: step 23100, loss = 2.06, batch loss = 2.00 (32.1 examples/sec; 0.249 sec/batch; 21h:23m:19s remains)
2017-12-06 04:24:00.863794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2212844 -4.2415109 -4.259975 -4.2736592 -4.2806253 -4.2796 -4.2650142 -4.2450056 -4.2426176 -4.2602234 -4.2897081 -4.31893 -4.3384361 -4.34911 -4.3587904][-4.20511 -4.2251034 -4.2460375 -4.2611661 -4.268559 -4.2684474 -4.2527575 -4.2308087 -4.2271805 -4.2457185 -4.2771454 -4.3079052 -4.3263106 -4.3354139 -4.3478069][-4.2086253 -4.2235093 -4.2421737 -4.2564898 -4.2652774 -4.2681975 -4.251956 -4.2264533 -4.2175522 -4.2305713 -4.2607956 -4.2943807 -4.3137827 -4.3250232 -4.3420286][-4.2369637 -4.2445006 -4.2578812 -4.2691817 -4.2777872 -4.2776365 -4.2547779 -4.2223258 -4.2049217 -4.2102475 -4.2406197 -4.2764387 -4.2985306 -4.3155107 -4.3389053][-4.2693582 -4.2736917 -4.2828279 -4.2891493 -4.2915859 -4.2800169 -4.2442713 -4.2007604 -4.1763177 -4.1799374 -4.2119341 -4.2492328 -4.2771468 -4.30511 -4.3367376][-4.2607927 -4.2674341 -4.2764511 -4.2809076 -4.2771912 -4.2518935 -4.197948 -4.1429529 -4.1204987 -4.1321669 -4.1710615 -4.214818 -4.2566237 -4.2990332 -4.3379793][-4.1977105 -4.2179079 -4.2314796 -4.2352028 -4.221848 -4.1787391 -4.1021719 -4.0355453 -4.0253444 -4.0611925 -4.1225128 -4.1830091 -4.2426124 -4.2965136 -4.3408332][-4.1002388 -4.1452341 -4.17001 -4.168304 -4.1362252 -4.0686321 -3.9639897 -3.8896804 -3.9124613 -3.9903715 -4.0857768 -4.1683455 -4.2398844 -4.2981372 -4.343555][-4.0040665 -4.0727873 -4.1058826 -4.0942121 -4.0429463 -3.9568107 -3.8365188 -3.776931 -3.8484185 -3.967999 -4.0889587 -4.1838965 -4.257184 -4.3116941 -4.3507452][-3.9554818 -4.0340419 -4.0692544 -4.049469 -3.9939618 -3.9191248 -3.8224096 -3.7996171 -3.8913665 -4.012363 -4.1296911 -4.219543 -4.2864947 -4.3311348 -4.3591161][-4.0104256 -4.0711479 -4.0988793 -4.0805221 -4.0386553 -3.9966705 -3.9442263 -3.9453373 -4.0145764 -4.1014919 -4.1935563 -4.2677279 -4.3213129 -4.3530293 -4.3684664][-4.1248817 -4.1581883 -4.1738038 -4.16317 -4.1406393 -4.1231918 -4.0987253 -4.1044555 -4.1446638 -4.1961985 -4.2598705 -4.3144603 -4.3541503 -4.3726635 -4.3774967][-4.2398553 -4.2529187 -4.2552929 -4.24731 -4.2387581 -4.2373433 -4.2275467 -4.2310863 -4.2502074 -4.2792592 -4.3208256 -4.3562684 -4.3813534 -4.3889008 -4.3857636][-4.3143778 -4.3181195 -4.3163347 -4.3110819 -4.3097725 -4.3132396 -4.3111935 -4.3123317 -4.32019 -4.3376989 -4.364037 -4.383008 -4.3948846 -4.395072 -4.3888173][-4.3491 -4.3486905 -4.3461285 -4.3428736 -4.3434377 -4.346818 -4.3466606 -4.3461785 -4.3490977 -4.3600841 -4.3755026 -4.385067 -4.3897934 -4.3886509 -4.3845773]]...]
INFO - root - 2017-12-06 04:24:03.184771: step 23110, loss = 2.08, batch loss = 2.02 (35.4 examples/sec; 0.226 sec/batch; 19h:23m:40s remains)
INFO - root - 2017-12-06 04:24:05.479570: step 23120, loss = 2.06, batch loss = 2.00 (33.6 examples/sec; 0.238 sec/batch; 20h:26m:20s remains)
INFO - root - 2017-12-06 04:24:07.742373: step 23130, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.221 sec/batch; 19h:01m:48s remains)
INFO - root - 2017-12-06 04:24:10.031848: step 23140, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 19h:43m:45s remains)
INFO - root - 2017-12-06 04:24:12.341292: step 23150, loss = 2.08, batch loss = 2.02 (34.0 examples/sec; 0.235 sec/batch; 20h:12m:33s remains)
INFO - root - 2017-12-06 04:24:14.649451: step 23160, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 19h:53m:20s remains)
INFO - root - 2017-12-06 04:24:16.925164: step 23170, loss = 2.03, batch loss = 1.97 (35.5 examples/sec; 0.226 sec/batch; 19h:22m:58s remains)
INFO - root - 2017-12-06 04:24:19.207503: step 23180, loss = 2.06, batch loss = 2.01 (33.4 examples/sec; 0.240 sec/batch; 20h:35m:18s remains)
INFO - root - 2017-12-06 04:24:21.546617: step 23190, loss = 2.05, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 19h:52m:21s remains)
INFO - root - 2017-12-06 04:24:23.831683: step 23200, loss = 2.06, batch loss = 2.01 (32.6 examples/sec; 0.245 sec/batch; 21h:04m:28s remains)
2017-12-06 04:24:24.176173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3247862 -4.3154826 -4.3061414 -4.3031311 -4.3057456 -4.3112135 -4.3144088 -4.3167915 -4.3199344 -4.3217015 -4.3254504 -4.3320446 -4.3397212 -4.3463974 -4.3503804][-4.3151827 -4.3040748 -4.2929854 -4.29026 -4.293551 -4.2976561 -4.2970338 -4.29654 -4.2992034 -4.3012977 -4.30831 -4.3198543 -4.3325863 -4.3435187 -4.3504162][-4.3088965 -4.296412 -4.2831879 -4.2779646 -4.2785788 -4.2769403 -4.2679939 -4.2616782 -4.26313 -4.2674766 -4.2822962 -4.30275 -4.3227105 -4.3396277 -4.351397][-4.3030295 -4.285131 -4.2641215 -4.250299 -4.2433252 -4.2344356 -4.2193952 -4.2079883 -4.2084174 -4.2187371 -4.246057 -4.2785273 -4.3080916 -4.3330188 -4.3502016][-4.2931867 -4.26505 -4.2321954 -4.2074113 -4.1908216 -4.17716 -4.1593904 -4.1456432 -4.1441541 -4.1634789 -4.2063389 -4.2501497 -4.28919 -4.3222914 -4.3454623][-4.2787161 -4.239305 -4.1936493 -4.1544614 -4.12215 -4.1002655 -4.0803313 -4.0648537 -4.0642142 -4.0970192 -4.1600471 -4.2172642 -4.2648783 -4.3070421 -4.3388734][-4.2699447 -4.2242751 -4.1695204 -4.1168447 -4.0710087 -4.043385 -4.0218558 -4.0032406 -4.0032682 -4.0482321 -4.1276445 -4.1943069 -4.2478652 -4.2966304 -4.3347478][-4.27161 -4.2251282 -4.16645 -4.1060991 -4.0535121 -4.0260191 -4.004458 -3.9850128 -3.9868839 -4.0349813 -4.116888 -4.1839771 -4.2386761 -4.2911468 -4.3315659][-4.2836928 -4.2420216 -4.1868992 -4.130446 -4.0827575 -4.059967 -4.0433388 -4.0278053 -4.0298243 -4.070045 -4.1403341 -4.1990466 -4.2480454 -4.295907 -4.3312263][-4.3001356 -4.2689266 -4.2263713 -4.1832156 -4.1471 -4.1297989 -4.1186318 -4.107697 -4.1073351 -4.133173 -4.1836195 -4.229075 -4.2667542 -4.3040318 -4.331687][-4.3173428 -4.2956309 -4.2647676 -4.2359776 -4.2132826 -4.2023253 -4.1960554 -4.1879644 -4.1836357 -4.1958203 -4.22833 -4.261909 -4.288733 -4.3137922 -4.33295][-4.3321443 -4.3179221 -4.2971554 -4.2800317 -4.2690167 -4.2648525 -4.2648306 -4.2596564 -4.2525349 -4.2575579 -4.2780309 -4.301548 -4.3159766 -4.3280668 -4.338407][-4.3401465 -4.3309493 -4.3175688 -4.3074517 -4.3023267 -4.3024249 -4.3059592 -4.3027215 -4.2961655 -4.3006382 -4.3152828 -4.3304172 -4.3363152 -4.3401966 -4.3446612][-4.3427854 -4.3380814 -4.3314548 -4.3254027 -4.3212347 -4.3217645 -4.3262067 -4.3244419 -4.320919 -4.3262277 -4.3369837 -4.3456545 -4.3468981 -4.3467546 -4.34767][-4.3427248 -4.3400788 -4.3371487 -4.3333626 -4.3286533 -4.3272262 -4.3299346 -4.3301692 -4.3310375 -4.3360114 -4.3429179 -4.3475928 -4.3476443 -4.3473306 -4.34843]]...]
INFO - root - 2017-12-06 04:24:26.498319: step 23210, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.233 sec/batch; 20h:00m:01s remains)
INFO - root - 2017-12-06 04:24:28.802545: step 23220, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.236 sec/batch; 20h:18m:56s remains)
INFO - root - 2017-12-06 04:24:31.111560: step 23230, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 19h:46m:01s remains)
INFO - root - 2017-12-06 04:24:33.422136: step 23240, loss = 2.05, batch loss = 1.99 (34.7 examples/sec; 0.231 sec/batch; 19h:49m:18s remains)
INFO - root - 2017-12-06 04:24:35.698514: step 23250, loss = 2.03, batch loss = 1.97 (33.8 examples/sec; 0.237 sec/batch; 20h:19m:51s remains)
INFO - root - 2017-12-06 04:24:37.991662: step 23260, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 19h:10m:49s remains)
INFO - root - 2017-12-06 04:24:40.320796: step 23270, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 19h:35m:47s remains)
INFO - root - 2017-12-06 04:24:42.666139: step 23280, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.227 sec/batch; 19h:28m:26s remains)
INFO - root - 2017-12-06 04:24:44.981174: step 23290, loss = 2.04, batch loss = 1.98 (33.1 examples/sec; 0.242 sec/batch; 20h:44m:53s remains)
INFO - root - 2017-12-06 04:24:47.340795: step 23300, loss = 2.09, batch loss = 2.03 (35.4 examples/sec; 0.226 sec/batch; 19h:23m:56s remains)
2017-12-06 04:24:47.628336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2485085 -4.2449045 -4.2492151 -4.2618966 -4.2627954 -4.2318211 -4.2135963 -4.237257 -4.2551012 -4.2596655 -4.2584896 -4.2357359 -4.1960788 -4.184608 -4.2127352][-4.2329931 -4.2228646 -4.2264671 -4.2410107 -4.2409325 -4.2140489 -4.1983333 -4.2232671 -4.2476 -4.254406 -4.2495356 -4.2233229 -4.1838641 -4.1742835 -4.2050834][-4.242425 -4.2279086 -4.227582 -4.236136 -4.2288041 -4.2016683 -4.18577 -4.208869 -4.2377062 -4.2513361 -4.2473731 -4.2211518 -4.1821337 -4.1727943 -4.2031021][-4.25611 -4.2406683 -4.2319145 -4.2253861 -4.2054448 -4.1707306 -4.1488714 -4.1708579 -4.2102642 -4.2403951 -4.2453785 -4.223443 -4.1871543 -4.1758733 -4.2056456][-4.2686982 -4.2590361 -4.2397275 -4.2142758 -4.1719942 -4.1161823 -4.0750618 -4.093833 -4.1579113 -4.216773 -4.2387094 -4.2230206 -4.189187 -4.1751919 -4.1995888][-4.2702518 -4.2626534 -4.2316227 -4.1854844 -4.1170845 -4.0273323 -3.9522581 -3.9656756 -4.0641408 -4.1636577 -4.2110772 -4.2047248 -4.1720848 -4.1608181 -4.1877632][-4.2521839 -4.2417331 -4.1993761 -4.1331491 -4.0407443 -3.9225194 -3.8112206 -3.8164039 -3.9403605 -4.0770645 -4.1580057 -4.1678581 -4.1420279 -4.1448178 -4.18056][-4.2404108 -4.2309489 -4.1902251 -4.1264911 -4.0447631 -3.9467323 -3.8525758 -3.861032 -3.9631362 -4.0821881 -4.1591773 -4.1703148 -4.1502743 -4.1641974 -4.1994753][-4.2466736 -4.2388272 -4.2045197 -4.1577363 -4.1063409 -4.04705 -3.9917231 -4.0015531 -4.0662675 -4.1445627 -4.1938305 -4.1932011 -4.1707549 -4.1864047 -4.2197566][-4.265027 -4.2594442 -4.2327561 -4.2030392 -4.1736317 -4.1395078 -4.1127949 -4.126193 -4.1667662 -4.2149096 -4.2411189 -4.2301178 -4.2040629 -4.2173328 -4.2470093][-4.2788982 -4.27646 -4.2597528 -4.2422447 -4.226387 -4.2085228 -4.1994872 -4.2117758 -4.2346282 -4.2608833 -4.2694178 -4.2477708 -4.2196722 -4.2318368 -4.2591019][-4.2791281 -4.2786946 -4.2710218 -4.2635512 -4.25543 -4.2474461 -4.2461677 -4.2527275 -4.2620163 -4.2774267 -4.2766767 -4.250154 -4.220952 -4.23223 -4.2559071][-4.2850447 -4.2828007 -4.2786584 -4.2742152 -4.2666631 -4.2586036 -4.2553816 -4.2571821 -4.2618165 -4.2721558 -4.2667212 -4.2370205 -4.2081933 -4.221642 -4.2454147][-4.2961855 -4.2899175 -4.2815967 -4.272398 -4.2610846 -4.2497663 -4.24145 -4.2378225 -4.2421937 -4.250607 -4.2466273 -4.21885 -4.1918464 -4.2091084 -4.2354975][-4.3066869 -4.2980313 -4.2853065 -4.2712264 -4.2566948 -4.2428908 -4.2326565 -4.2302876 -4.2368879 -4.244453 -4.2412972 -4.2134318 -4.1832075 -4.1989565 -4.2267375]]...]
INFO - root - 2017-12-06 04:24:49.914176: step 23310, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.221 sec/batch; 19h:00m:41s remains)
INFO - root - 2017-12-06 04:24:52.195297: step 23320, loss = 2.03, batch loss = 1.97 (32.4 examples/sec; 0.247 sec/batch; 21h:14m:14s remains)
INFO - root - 2017-12-06 04:24:54.524287: step 23330, loss = 2.07, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 20h:02m:24s remains)
INFO - root - 2017-12-06 04:24:56.816622: step 23340, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 19h:12m:10s remains)
INFO - root - 2017-12-06 04:24:59.103942: step 23350, loss = 2.07, batch loss = 2.01 (34.0 examples/sec; 0.235 sec/batch; 20h:11m:08s remains)
INFO - root - 2017-12-06 04:25:01.395252: step 23360, loss = 2.08, batch loss = 2.02 (32.3 examples/sec; 0.248 sec/batch; 21h:18m:01s remains)
INFO - root - 2017-12-06 04:25:03.729709: step 23370, loss = 2.09, batch loss = 2.03 (35.2 examples/sec; 0.227 sec/batch; 19h:29m:21s remains)
INFO - root - 2017-12-06 04:25:06.044255: step 23380, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.231 sec/batch; 19h:50m:08s remains)
INFO - root - 2017-12-06 04:25:08.360291: step 23390, loss = 2.06, batch loss = 2.01 (34.1 examples/sec; 0.234 sec/batch; 20h:07m:07s remains)
INFO - root - 2017-12-06 04:25:10.666401: step 23400, loss = 2.08, batch loss = 2.02 (35.6 examples/sec; 0.225 sec/batch; 19h:17m:00s remains)
2017-12-06 04:25:10.959715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2877946 -4.2673078 -4.2513123 -4.2322702 -4.2109547 -4.1980186 -4.1883731 -4.1808667 -4.1760545 -4.1851463 -4.2141309 -4.232614 -4.233757 -4.2257962 -4.2176933][-4.2917218 -4.2687516 -4.2506652 -4.2292328 -4.2078576 -4.1969056 -4.1891656 -4.1822343 -4.1811285 -4.1971984 -4.2297888 -4.2524724 -4.2563777 -4.2491035 -4.24202][-4.2937765 -4.2690368 -4.2475677 -4.224309 -4.2043042 -4.19692 -4.191927 -4.181951 -4.1769605 -4.1931987 -4.2244143 -4.2465191 -4.2543926 -4.2561121 -4.2571135][-4.294528 -4.2691069 -4.2430425 -4.21653 -4.195765 -4.1883073 -4.18172 -4.1670642 -4.1590252 -4.17804 -4.2098212 -4.23175 -4.2380576 -4.2477889 -4.2586961][-4.2970166 -4.2723761 -4.24443 -4.2162309 -4.1923547 -4.1783857 -4.1636095 -4.1462617 -4.1459637 -4.1760974 -4.2143993 -4.2337332 -4.2288022 -4.2340727 -4.2464843][-4.2992749 -4.2756386 -4.2488751 -4.2213349 -4.196682 -4.1761966 -4.1544557 -4.1385469 -4.1466813 -4.1821675 -4.2226543 -4.236753 -4.2182989 -4.2139297 -4.2205729][-4.29904 -4.2769318 -4.2524529 -4.226285 -4.2032242 -4.1787872 -4.1560984 -4.1463375 -4.157 -4.1881948 -4.2252097 -4.2358189 -4.2127795 -4.2059822 -4.2061124][-4.2964053 -4.27472 -4.2510242 -4.2249851 -4.202435 -4.1778708 -4.163259 -4.1610165 -4.1681719 -4.1894574 -4.2172637 -4.22403 -4.20057 -4.1950679 -4.1890726][-4.2929063 -4.2699971 -4.2442126 -4.214036 -4.1856103 -4.15847 -4.1476169 -4.1505957 -4.1559625 -4.1728516 -4.1952152 -4.199646 -4.1789317 -4.1738234 -4.1628661][-4.2866445 -4.2596989 -4.2269177 -4.1834574 -4.1364732 -4.0960169 -4.0794239 -4.0867486 -4.0971956 -4.1166029 -4.13486 -4.1293521 -4.1047082 -4.0993829 -4.0932803][-4.2778668 -4.2439051 -4.2007375 -4.1410489 -4.0754209 -4.021018 -3.9993711 -4.0079517 -4.0205741 -4.0425892 -4.0592871 -4.0449347 -4.0190916 -4.0197864 -4.0257154][-4.2732568 -4.2377348 -4.1935124 -4.13471 -4.0722795 -4.0210843 -3.9998307 -4.0011826 -4.0064416 -4.0253544 -4.0468922 -4.0408168 -4.020288 -4.0224509 -4.0288262][-4.2770605 -4.2478957 -4.2123213 -4.1666379 -4.118011 -4.0779395 -4.059093 -4.0525627 -4.0502262 -4.0615435 -4.081677 -4.0824604 -4.0670252 -4.0695643 -4.0752583][-4.2873068 -4.2676854 -4.2437034 -4.2123027 -4.1799498 -4.1515579 -4.1359487 -4.1261311 -4.1191926 -4.1219854 -4.13616 -4.1387939 -4.129416 -4.1353078 -4.1426821][-4.2969871 -4.28525 -4.2717562 -4.2546163 -4.2381921 -4.2234983 -4.214148 -4.2074838 -4.1997108 -4.1973243 -4.206357 -4.2100315 -4.2064996 -4.2139978 -4.2219081]]...]
INFO - root - 2017-12-06 04:25:13.286050: step 23410, loss = 2.03, batch loss = 1.97 (32.5 examples/sec; 0.246 sec/batch; 21h:06m:46s remains)
INFO - root - 2017-12-06 04:25:15.582672: step 23420, loss = 2.05, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 19h:33m:04s remains)
INFO - root - 2017-12-06 04:25:17.891858: step 23430, loss = 2.08, batch loss = 2.02 (33.9 examples/sec; 0.236 sec/batch; 20h:17m:17s remains)
INFO - root - 2017-12-06 04:25:20.200280: step 23440, loss = 2.06, batch loss = 2.01 (35.3 examples/sec; 0.226 sec/batch; 19h:25m:57s remains)
INFO - root - 2017-12-06 04:25:22.506292: step 23450, loss = 2.08, batch loss = 2.02 (35.8 examples/sec; 0.223 sec/batch; 19h:10m:39s remains)
INFO - root - 2017-12-06 04:25:24.849568: step 23460, loss = 2.05, batch loss = 1.99 (34.9 examples/sec; 0.229 sec/batch; 19h:41m:17s remains)
INFO - root - 2017-12-06 04:25:27.125641: step 23470, loss = 2.05, batch loss = 1.99 (35.9 examples/sec; 0.223 sec/batch; 19h:06m:47s remains)
INFO - root - 2017-12-06 04:25:29.438241: step 23480, loss = 2.08, batch loss = 2.03 (35.5 examples/sec; 0.225 sec/batch; 19h:20m:48s remains)
INFO - root - 2017-12-06 04:25:31.756195: step 23490, loss = 2.06, batch loss = 2.00 (34.6 examples/sec; 0.231 sec/batch; 19h:50m:23s remains)
INFO - root - 2017-12-06 04:25:34.059452: step 23500, loss = 2.05, batch loss = 2.00 (33.6 examples/sec; 0.238 sec/batch; 20h:24m:52s remains)
2017-12-06 04:25:34.351868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.219171 -4.2198162 -4.2227035 -4.22827 -4.2195148 -4.2083712 -4.21473 -4.216814 -4.2104788 -4.2015948 -4.1896811 -4.1950793 -4.2028561 -4.2031808 -4.2200527][-4.1912513 -4.1982222 -4.2088757 -4.2172203 -4.2093515 -4.2017336 -4.21022 -4.2124434 -4.2057214 -4.1904731 -4.1670341 -4.1717033 -4.185061 -4.1896405 -4.2095919][-4.1723208 -4.1805229 -4.1914353 -4.1997266 -4.1937842 -4.19352 -4.20862 -4.2123737 -4.2117414 -4.1979761 -4.1673756 -4.1674266 -4.1793184 -4.1868281 -4.2070856][-4.1686969 -4.1716757 -4.1761847 -4.1816254 -4.177846 -4.179841 -4.1931529 -4.192636 -4.2037239 -4.2039475 -4.1761189 -4.1710782 -4.1782131 -4.1863008 -4.2049422][-4.1865606 -4.1842637 -4.1807065 -4.1810431 -4.1780481 -4.1737723 -4.1656923 -4.1427684 -4.1658759 -4.1928797 -4.1810822 -4.1779909 -4.1831913 -4.1871982 -4.2026539][-4.2068119 -4.2015657 -4.1936779 -4.1892743 -4.1885037 -4.1715126 -4.1258183 -4.0579448 -4.0803852 -4.1454916 -4.1685662 -4.18113 -4.18948 -4.1891456 -4.2030225][-4.2046123 -4.2000222 -4.19879 -4.1956716 -4.19162 -4.1652389 -4.0809979 -3.9552011 -3.963104 -4.0741372 -4.1433072 -4.1806612 -4.1972208 -4.1959786 -4.206501][-4.1888976 -4.1909351 -4.201488 -4.2066441 -4.2067351 -4.1856685 -4.0982733 -3.950738 -3.9402359 -4.0624022 -4.148262 -4.1957736 -4.2177472 -4.2169094 -4.2223139][-4.1747336 -4.188817 -4.2117682 -4.2245836 -4.2316265 -4.2283459 -4.1687832 -4.0567503 -4.0374031 -4.1154494 -4.17657 -4.2168422 -4.2393136 -4.2406898 -4.24481][-4.1835771 -4.2016535 -4.2252192 -4.23941 -4.2492657 -4.2565603 -4.2248092 -4.1554894 -4.1382565 -4.17584 -4.2065926 -4.23363 -4.2568235 -4.2636433 -4.2708011][-4.204771 -4.2205443 -4.2349725 -4.2454181 -4.2525215 -4.2606964 -4.2483683 -4.2120156 -4.2051625 -4.2225471 -4.2323818 -4.2468424 -4.2701569 -4.2828369 -4.2939234][-4.2282257 -4.2376842 -4.2469959 -4.256134 -4.2577949 -4.2622685 -4.2629476 -4.248085 -4.2506604 -4.2568631 -4.2546725 -4.262691 -4.2850966 -4.299264 -4.3113852][-4.2606773 -4.2667341 -4.271976 -4.2785287 -4.2786531 -4.2781925 -4.2822175 -4.2772555 -4.2810316 -4.282937 -4.277739 -4.284059 -4.3004541 -4.3100853 -4.3210173][-4.2781706 -4.2830348 -4.2879615 -4.2940478 -4.2934308 -4.2903256 -4.2941804 -4.2950954 -4.2978883 -4.2977233 -4.2940946 -4.30058 -4.3097553 -4.3154759 -4.3249888][-4.2870092 -4.2909427 -4.2959719 -4.301713 -4.3002615 -4.294817 -4.2973261 -4.30034 -4.3027234 -4.30252 -4.3024826 -4.3098421 -4.3150687 -4.3185649 -4.3264012]]...]
INFO - root - 2017-12-06 04:25:36.616812: step 23510, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.226 sec/batch; 19h:21m:31s remains)
INFO - root - 2017-12-06 04:25:38.929347: step 23520, loss = 2.05, batch loss = 1.99 (34.2 examples/sec; 0.234 sec/batch; 20h:03m:57s remains)
INFO - root - 2017-12-06 04:25:41.215117: step 23530, loss = 2.06, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 19h:34m:46s remains)
INFO - root - 2017-12-06 04:25:43.534592: step 23540, loss = 2.10, batch loss = 2.04 (34.5 examples/sec; 0.232 sec/batch; 19h:54m:10s remains)
INFO - root - 2017-12-06 04:25:45.817871: step 23550, loss = 2.07, batch loss = 2.01 (36.1 examples/sec; 0.222 sec/batch; 19h:02m:24s remains)
INFO - root - 2017-12-06 04:25:48.142222: step 23560, loss = 2.05, batch loss = 1.99 (33.5 examples/sec; 0.239 sec/batch; 20h:29m:17s remains)
INFO - root - 2017-12-06 04:25:50.446540: step 23570, loss = 2.05, batch loss = 1.99 (34.5 examples/sec; 0.232 sec/batch; 19h:55m:11s remains)
INFO - root - 2017-12-06 04:25:52.743033: step 23580, loss = 2.08, batch loss = 2.02 (34.3 examples/sec; 0.233 sec/batch; 20h:01m:21s remains)
INFO - root - 2017-12-06 04:25:55.063759: step 23590, loss = 2.06, batch loss = 2.00 (32.8 examples/sec; 0.244 sec/batch; 20h:55m:14s remains)
INFO - root - 2017-12-06 04:25:57.363333: step 23600, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 19h:33m:13s remains)
2017-12-06 04:25:57.676010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3268518 -4.3089881 -4.2904215 -4.2700982 -4.2479024 -4.2247882 -4.2140374 -4.2212267 -4.2387419 -4.2501073 -4.2490015 -4.2456946 -4.2475524 -4.2526512 -4.2480259][-4.3158092 -4.2910509 -4.2687855 -4.2458725 -4.2204461 -4.1928654 -4.1805863 -4.1924758 -4.2185106 -4.2318058 -4.2262535 -4.2149715 -4.2146463 -4.2206059 -4.2190309][-4.297915 -4.26722 -4.2442856 -4.2238216 -4.2007504 -4.1746578 -4.1624908 -4.1783085 -4.2095294 -4.2226973 -4.2153945 -4.1963067 -4.1915226 -4.1955066 -4.1975808][-4.2777138 -4.2408552 -4.2152 -4.1968541 -4.1785865 -4.1550889 -4.1406126 -4.1596742 -4.1963549 -4.2123446 -4.2054482 -4.1837 -4.176096 -4.1755052 -4.1762366][-4.2656732 -4.2241387 -4.1933284 -4.1708021 -4.1490374 -4.1207304 -4.0967813 -4.1163359 -4.1619744 -4.1855021 -4.1870193 -4.1735396 -4.1677232 -4.162271 -4.1574593][-4.2653718 -4.2217917 -4.1842971 -4.1537094 -4.1194267 -4.0749269 -4.0320754 -4.0474572 -4.1114321 -4.1522045 -4.168366 -4.1663179 -4.1624122 -4.1537032 -4.1430755][-4.2727518 -4.2315879 -4.1880364 -4.1470113 -4.09595 -4.0262074 -3.9516144 -3.9533646 -4.0385566 -4.1032124 -4.1383286 -4.1433878 -4.1411572 -4.131382 -4.1204228][-4.2808809 -4.2459111 -4.2013812 -4.1547232 -4.0930896 -4.0035968 -3.901299 -3.8785286 -3.9733744 -4.055758 -4.10528 -4.1157761 -4.1105981 -4.1021228 -4.0999784][-4.2869315 -4.2582383 -4.2187414 -4.175375 -4.1179433 -4.0347295 -3.9395552 -3.9052634 -3.9805558 -4.0537987 -4.0998163 -4.1054115 -4.0951281 -4.0922089 -4.0993166][-4.2898693 -4.2650666 -4.2289338 -4.1887331 -4.1402044 -4.0787568 -4.0135098 -3.9815507 -4.0248156 -4.0736775 -4.1036468 -4.0998249 -4.0903125 -4.0967112 -4.114912][-4.2886534 -4.2640448 -4.227468 -4.1883559 -4.1479449 -4.1043453 -4.0630941 -4.0376506 -4.0628257 -4.0927091 -4.1073 -4.0968971 -4.0904779 -4.10376 -4.1265182][-4.2872472 -4.2632051 -4.229053 -4.1888289 -4.1543279 -4.1228504 -4.0979447 -4.0831661 -4.1023612 -4.1196976 -4.1231632 -4.1089354 -4.102478 -4.115315 -4.1391826][-4.2874131 -4.2632442 -4.2352834 -4.1994495 -4.1706753 -4.1459208 -4.1282811 -4.1202788 -4.1397443 -4.1524048 -4.1526995 -4.1401734 -4.1352329 -4.1416068 -4.1579466][-4.2901025 -4.2680035 -4.2448354 -4.2139874 -4.1886148 -4.1675286 -4.150444 -4.141468 -4.1601243 -4.1762109 -4.1805553 -4.1746731 -4.1714287 -4.1727881 -4.179297][-4.2959247 -4.2777333 -4.2580261 -4.2304511 -4.204854 -4.183073 -4.1659422 -4.1562743 -4.1706071 -4.1859665 -4.1923981 -4.1931534 -4.1929131 -4.1916509 -4.1948614]]...]
INFO - root - 2017-12-06 04:25:59.960706: step 23610, loss = 2.05, batch loss = 1.99 (35.1 examples/sec; 0.228 sec/batch; 19h:33m:55s remains)
INFO - root - 2017-12-06 04:26:02.276441: step 23620, loss = 2.07, batch loss = 2.01 (34.0 examples/sec; 0.235 sec/batch; 20h:11m:27s remains)
INFO - root - 2017-12-06 04:26:04.619639: step 23630, loss = 2.06, batch loss = 2.00 (30.9 examples/sec; 0.259 sec/batch; 22h:12m:07s remains)
INFO - root - 2017-12-06 04:26:06.922366: step 23640, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:16m:34s remains)
INFO - root - 2017-12-06 04:26:09.254084: step 23650, loss = 2.09, batch loss = 2.03 (35.2 examples/sec; 0.227 sec/batch; 19h:29m:19s remains)
INFO - root - 2017-12-06 04:26:11.543401: step 23660, loss = 2.05, batch loss = 1.99 (33.8 examples/sec; 0.236 sec/batch; 20h:17m:08s remains)
INFO - root - 2017-12-06 04:26:13.815289: step 23670, loss = 2.08, batch loss = 2.02 (34.9 examples/sec; 0.229 sec/batch; 19h:38m:48s remains)
INFO - root - 2017-12-06 04:26:16.138609: step 23680, loss = 2.05, batch loss = 1.99 (34.3 examples/sec; 0.234 sec/batch; 20h:02m:02s remains)
INFO - root - 2017-12-06 04:26:18.449134: step 23690, loss = 2.07, batch loss = 2.01 (34.5 examples/sec; 0.232 sec/batch; 19h:54m:15s remains)
INFO - root - 2017-12-06 04:26:20.744551: step 23700, loss = 2.08, batch loss = 2.02 (35.5 examples/sec; 0.226 sec/batch; 19h:20m:49s remains)
2017-12-06 04:26:21.019060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0439935 -4.0396433 -4.0171952 -4.0025187 -4.0239282 -4.0723028 -4.12167 -4.1623988 -4.1840768 -4.1878991 -4.1760569 -4.1442842 -4.0986671 -4.0616012 -4.0537992][-4.0449595 -4.03592 -4.0138469 -4.003799 -4.0268936 -4.0738878 -4.1179218 -4.1542206 -4.1753564 -4.1809692 -4.172821 -4.1485829 -4.1166892 -4.091846 -4.0877485][-4.0673718 -4.0604978 -4.0410376 -4.0324469 -4.0520396 -4.0874987 -4.1192412 -4.1479211 -4.1657729 -4.1698666 -4.1653256 -4.1538963 -4.1390581 -4.1279459 -4.1303067][-4.0987191 -4.0958118 -4.0767574 -4.062326 -4.0727034 -4.0947614 -4.1143627 -4.1345568 -4.1469464 -4.1513062 -4.1549182 -4.15573 -4.1517758 -4.149159 -4.1576934][-4.1295853 -4.1255279 -4.103137 -4.0821319 -4.0820365 -4.0923095 -4.0989141 -4.1064668 -4.1140275 -4.121182 -4.1311193 -4.1362667 -4.1347938 -4.13741 -4.154644][-4.1534395 -4.1367531 -4.1019521 -4.0675631 -4.0549583 -4.0534825 -4.0506911 -4.054842 -4.0672865 -4.0814714 -4.0964065 -4.1078 -4.1084495 -4.1137438 -4.1371083][-4.1564074 -4.1266127 -4.0755935 -4.0246725 -4.0008097 -3.9914436 -3.9835787 -3.9920456 -4.01151 -4.0350442 -4.0614014 -4.0876546 -4.0942936 -4.1022525 -4.12763][-4.1397872 -4.1032915 -4.0409179 -3.9759986 -3.9353857 -3.9136336 -3.9037409 -3.9152317 -3.9332502 -3.9588108 -4.0009503 -4.0450048 -4.0618458 -4.0745072 -4.1029391][-4.1215634 -4.0841703 -4.0191388 -3.94603 -3.8920219 -3.857393 -3.8395159 -3.8405447 -3.8399811 -3.8635297 -3.9231372 -3.9818377 -4.015697 -4.0408287 -4.0778251][-4.1167855 -4.0826654 -4.0225854 -3.9534621 -3.9001429 -3.8674397 -3.848881 -3.8427739 -3.837548 -3.857868 -3.9108751 -3.9638004 -4.00211 -4.0350952 -4.07535][-4.1215196 -4.0934834 -4.0435367 -3.9848645 -3.9423122 -3.9193864 -3.9072192 -3.9059677 -3.9060764 -3.9182582 -3.9459 -3.9772675 -4.0082626 -4.0446477 -4.0885458][-4.1299052 -4.1096473 -4.0730729 -4.0269361 -3.9945977 -3.9761989 -3.9682682 -3.9691725 -3.9688931 -3.9720469 -3.9835622 -3.9982638 -4.0185871 -4.0552473 -4.0985627][-4.1394048 -4.1323538 -4.1113448 -4.0773191 -4.0510664 -4.0316949 -4.02205 -4.0170918 -4.0105948 -4.0103383 -4.0185981 -4.0244656 -4.035141 -4.0686455 -4.105876][-4.1501136 -4.157012 -4.1504178 -4.1288233 -4.1061239 -4.0829358 -4.065557 -4.0464392 -4.0284867 -4.0256095 -4.0344167 -4.0402489 -4.052464 -4.087153 -4.1184373][-4.1662197 -4.1830344 -4.189734 -4.1787863 -4.1556678 -4.1233935 -4.09361 -4.061954 -4.0364122 -4.0299163 -4.0386834 -4.0489006 -4.0672941 -4.1047039 -4.1312304]]...]
INFO - root - 2017-12-06 04:26:23.341980: step 23710, loss = 2.04, batch loss = 1.98 (33.3 examples/sec; 0.240 sec/batch; 20h:35m:46s remains)
INFO - root - 2017-12-06 04:26:25.653717: step 23720, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 18h:50m:55s remains)
INFO - root - 2017-12-06 04:26:27.928427: step 23730, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.222 sec/batch; 19h:01m:27s remains)
INFO - root - 2017-12-06 04:26:30.240133: step 23740, loss = 2.06, batch loss = 2.00 (34.0 examples/sec; 0.236 sec/batch; 20h:12m:23s remains)
INFO - root - 2017-12-06 04:26:32.497624: step 23750, loss = 2.06, batch loss = 2.00 (35.9 examples/sec; 0.223 sec/batch; 19h:06m:46s remains)
INFO - root - 2017-12-06 04:26:34.900597: step 23760, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 18h:53m:13s remains)
INFO - root - 2017-12-06 04:26:37.258503: step 23770, loss = 2.04, batch loss = 1.99 (33.5 examples/sec; 0.238 sec/batch; 20h:27m:02s remains)
INFO - root - 2017-12-06 04:26:39.590311: step 23780, loss = 2.09, batch loss = 2.03 (34.7 examples/sec; 0.230 sec/batch; 19h:45m:28s remains)
INFO - root - 2017-12-06 04:26:41.894391: step 23790, loss = 2.07, batch loss = 2.01 (34.1 examples/sec; 0.234 sec/batch; 20h:06m:19s remains)
INFO - root - 2017-12-06 04:26:44.195918: step 23800, loss = 2.06, batch loss = 2.00 (35.6 examples/sec; 0.225 sec/batch; 19h:15m:13s remains)
2017-12-06 04:26:44.470352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.216186 -4.2073212 -4.1977572 -4.1904192 -4.1842537 -4.1773386 -4.1759443 -4.1861949 -4.2015891 -4.2144213 -4.2208967 -4.2186823 -4.205184 -4.1927791 -4.1942062][-4.2138667 -4.2044768 -4.1864972 -4.1661477 -4.1474352 -4.1318827 -4.1276393 -4.1439619 -4.1737189 -4.2041006 -4.2251019 -4.23282 -4.2281971 -4.2196903 -4.2203612][-4.22527 -4.2180843 -4.1948385 -4.162818 -4.1319766 -4.1071463 -4.0968685 -4.1143985 -4.1557302 -4.203476 -4.239089 -4.2586474 -4.2650294 -4.2637625 -4.2653003][-4.2190194 -4.2227573 -4.2027059 -4.164155 -4.1247735 -4.093842 -4.0767684 -4.0890613 -4.1347876 -4.1925359 -4.2392368 -4.2704773 -4.2862358 -4.2906523 -4.2926626][-4.1954203 -4.2119384 -4.1999736 -4.1594267 -4.1133657 -4.0748248 -4.054368 -4.0610642 -4.1051354 -4.1663785 -4.2177958 -4.2555084 -4.2770858 -4.2840834 -4.2843633][-4.14609 -4.1769867 -4.1794291 -4.1461225 -4.0993438 -4.0521064 -4.0256681 -4.0272355 -4.0623469 -4.1191831 -4.170115 -4.2090869 -4.2317505 -4.2374148 -4.2373476][-4.061996 -4.1082277 -4.137032 -4.1257644 -4.0891938 -4.0418243 -4.0130734 -4.0093575 -4.030652 -4.0754504 -4.1223125 -4.1610379 -4.1853395 -4.1937976 -4.1993451][-3.9902911 -4.0440183 -4.09672 -4.1101375 -4.0906153 -4.054522 -4.0291238 -4.023489 -4.0339541 -4.0666418 -4.1083264 -4.1453266 -4.168498 -4.1796026 -4.1921287][-4.0056348 -4.0507727 -4.1046181 -4.1287904 -4.1212111 -4.0991683 -4.07878 -4.0738173 -4.0777259 -4.097846 -4.1304245 -4.160675 -4.1768217 -4.1847939 -4.1995735][-4.0855021 -4.117126 -4.1606226 -4.1848221 -4.1835113 -4.169076 -4.1523108 -4.1458931 -4.1447291 -4.1521206 -4.1714272 -4.1913214 -4.2006845 -4.203887 -4.215466][-4.1737137 -4.1939664 -4.2242904 -4.2429967 -4.2460117 -4.2359462 -4.2208323 -4.2086587 -4.2011786 -4.1997185 -4.2064338 -4.21524 -4.2183056 -4.2170558 -4.221034][-4.2437034 -4.2563519 -4.2734833 -4.2837267 -4.2873645 -4.2836404 -4.2746987 -4.2636805 -4.2552915 -4.251235 -4.2499619 -4.2485 -4.2442985 -4.2366982 -4.2302227][-4.2827353 -4.2954693 -4.3075562 -4.3136873 -4.3176913 -4.3176665 -4.314539 -4.3090777 -4.3031087 -4.29856 -4.2923446 -4.281404 -4.2672257 -4.2494516 -4.2343078][-4.2598515 -4.2803049 -4.3011675 -4.3147807 -4.323164 -4.3271809 -4.3277073 -4.3265905 -4.3226647 -4.3170605 -4.3071976 -4.2934456 -4.2782912 -4.2588739 -4.2434983][-4.2001472 -4.2310886 -4.2663617 -4.2937708 -4.3104186 -4.3196259 -4.3217969 -4.3216996 -4.3182063 -4.3119464 -4.301528 -4.2902451 -4.2787881 -4.2623539 -4.2499466]]...]
INFO - root - 2017-12-06 04:26:46.739266: step 23810, loss = 2.07, batch loss = 2.01 (35.8 examples/sec; 0.223 sec/batch; 19h:09m:28s remains)
INFO - root - 2017-12-06 04:26:49.044437: step 23820, loss = 2.08, batch loss = 2.02 (34.1 examples/sec; 0.235 sec/batch; 20h:07m:51s remains)
INFO - root - 2017-12-06 04:26:51.368556: step 23830, loss = 2.05, batch loss = 2.00 (35.1 examples/sec; 0.228 sec/batch; 19h:32m:42s remains)
INFO - root - 2017-12-06 04:26:53.648539: step 23840, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.237 sec/batch; 20h:17m:34s remains)
INFO - root - 2017-12-06 04:26:55.956450: step 23850, loss = 2.04, batch loss = 1.98 (35.1 examples/sec; 0.228 sec/batch; 19h:32m:26s remains)
INFO - root - 2017-12-06 04:26:58.311556: step 23860, loss = 2.07, batch loss = 2.02 (31.3 examples/sec; 0.255 sec/batch; 21h:52m:57s remains)
INFO - root - 2017-12-06 04:27:00.620811: step 23870, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 19h:43m:24s remains)
INFO - root - 2017-12-06 04:27:02.888508: step 23880, loss = 2.05, batch loss = 1.99 (36.1 examples/sec; 0.222 sec/batch; 19h:00m:24s remains)
INFO - root - 2017-12-06 04:27:05.178801: step 23890, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 19h:35m:43s remains)
INFO - root - 2017-12-06 04:27:07.470840: step 23900, loss = 2.07, batch loss = 2.01 (33.3 examples/sec; 0.240 sec/batch; 20h:35m:30s remains)
2017-12-06 04:27:07.743918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2488256 -4.2535334 -4.2543697 -4.2555614 -4.257566 -4.2546382 -4.2470932 -4.2400675 -4.2422662 -4.2552257 -4.2668672 -4.274024 -4.280149 -4.2862015 -4.2963405][-4.20523 -4.2061887 -4.2055426 -4.2110896 -4.2211628 -4.2229295 -4.2101688 -4.191504 -4.1940918 -4.2198939 -4.2457328 -4.26616 -4.2841196 -4.2953582 -4.3044629][-4.1661344 -4.1558213 -4.1472416 -4.1517029 -4.1665978 -4.1676073 -4.1372452 -4.0907731 -4.0895486 -4.1367073 -4.1879716 -4.2276897 -4.2639523 -4.2852864 -4.2984204][-4.1544147 -4.1344962 -4.1186419 -4.1175361 -4.1292849 -4.1216431 -4.0641789 -3.9780996 -3.9697797 -4.04136 -4.11721 -4.1730852 -4.2268829 -4.2604628 -4.281868][-4.1471181 -4.1186547 -4.09552 -4.0912876 -4.10669 -4.1000443 -4.0273561 -3.9139276 -3.8958182 -3.9819329 -4.0689545 -4.1275926 -4.1928897 -4.2388148 -4.2671013][-4.1236997 -4.0851812 -4.0549121 -4.0494504 -4.0669971 -4.0562315 -3.9672122 -3.8248575 -3.8044665 -3.9217496 -4.0324779 -4.0973973 -4.1691551 -4.224133 -4.2579589][-4.1130028 -4.0706429 -4.0368967 -4.0248632 -4.0276704 -3.9878004 -3.8567247 -3.6652288 -3.6500449 -3.8219504 -3.9707973 -4.0569334 -4.1432481 -4.2061605 -4.2480564][-4.1318951 -4.092998 -4.0602927 -4.0444603 -4.0317678 -3.9643242 -3.803324 -3.5939472 -3.5912175 -3.780951 -3.9375312 -4.0300617 -4.1249013 -4.19311 -4.2388654][-4.15537 -4.1224709 -4.1010976 -4.0950012 -4.0853896 -4.0250177 -3.8900671 -3.7293403 -3.7268686 -3.8724558 -3.9942539 -4.0633936 -4.1412239 -4.2004232 -4.2418046][-4.1615133 -4.1331573 -4.1192427 -4.1215248 -4.1181731 -4.0755286 -3.976954 -3.8605192 -3.8522964 -3.9618671 -4.0590253 -4.1121335 -4.1716118 -4.2193818 -4.2520847][-4.1634407 -4.1398225 -4.1240768 -4.1264219 -4.1263633 -4.095355 -4.0210643 -3.9226656 -3.9000645 -3.9877679 -4.0777764 -4.1313882 -4.1864891 -4.2303705 -4.2602654][-4.1783929 -4.1611557 -4.1463776 -4.1474452 -4.1452475 -4.1210065 -4.0676064 -3.9859316 -3.9542837 -4.0184669 -4.0948672 -4.145772 -4.196672 -4.239152 -4.2695246][-4.2145195 -4.2062545 -4.1987467 -4.1985574 -4.1930685 -4.1762371 -4.1387944 -4.0765824 -4.0443382 -4.08345 -4.1365948 -4.1759253 -4.2179036 -4.2551613 -4.2843995][-4.2638907 -4.2612076 -4.2562861 -4.2510781 -4.243608 -4.2301583 -4.2007141 -4.1516252 -4.1233835 -4.14444 -4.1767855 -4.2051115 -4.2398024 -4.2712617 -4.2962437][-4.3005381 -4.3001633 -4.2954693 -4.2880416 -4.2790909 -4.2657752 -4.2374554 -4.1967297 -4.1731272 -4.1858349 -4.206131 -4.22705 -4.2556305 -4.2824969 -4.3028631]]...]
INFO - root - 2017-12-06 04:27:10.058664: step 23910, loss = 2.08, batch loss = 2.02 (34.1 examples/sec; 0.235 sec/batch; 20h:07m:03s remains)
INFO - root - 2017-12-06 04:27:12.381339: step 23920, loss = 2.05, batch loss = 1.99 (35.2 examples/sec; 0.227 sec/batch; 19h:29m:27s remains)
INFO - root - 2017-12-06 04:27:14.675822: step 23930, loss = 2.03, batch loss = 1.97 (33.6 examples/sec; 0.238 sec/batch; 20h:24m:11s remains)
INFO - root - 2017-12-06 04:27:16.961836: step 23940, loss = 2.03, batch loss = 1.98 (36.0 examples/sec; 0.222 sec/batch; 19h:02m:35s remains)
INFO - root - 2017-12-06 04:27:19.286548: step 23950, loss = 2.07, batch loss = 2.01 (33.6 examples/sec; 0.238 sec/batch; 20h:25m:15s remains)
INFO - root - 2017-12-06 04:27:21.626492: step 23960, loss = 2.05, batch loss = 1.99 (34.0 examples/sec; 0.235 sec/batch; 20h:08m:27s remains)
INFO - root - 2017-12-06 04:27:23.975456: step 23970, loss = 2.06, batch loss = 2.00 (34.7 examples/sec; 0.230 sec/batch; 19h:45m:03s remains)
INFO - root - 2017-12-06 04:27:26.321362: step 23980, loss = 2.04, batch loss = 1.99 (34.6 examples/sec; 0.231 sec/batch; 19h:49m:33s remains)
INFO - root - 2017-12-06 04:27:28.571690: step 23990, loss = 2.08, batch loss = 2.02 (35.3 examples/sec; 0.226 sec/batch; 19h:23m:58s remains)
INFO - root - 2017-12-06 04:27:30.881205: step 24000, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.237 sec/batch; 20h:16m:45s remains)
2017-12-06 04:27:31.173837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2015567 -4.1917205 -4.1836634 -4.1803117 -4.1799474 -4.1748281 -4.1764879 -4.1806636 -4.1906147 -4.2025909 -4.2301788 -4.250895 -4.2675805 -4.2985549 -4.320498][-4.187202 -4.1738954 -4.1643548 -4.1627584 -4.1670413 -4.1613512 -4.1625347 -4.1778979 -4.1976781 -4.2117844 -4.2438674 -4.2685795 -4.2872829 -4.319746 -4.338922][-4.1730905 -4.1611238 -4.1512485 -4.1526794 -4.1628604 -4.1604018 -4.16332 -4.1854181 -4.2106075 -4.2242422 -4.252769 -4.2740307 -4.293643 -4.3283591 -4.345046][-4.1718445 -4.1631603 -4.15503 -4.1570539 -4.1651621 -4.1615343 -4.1623993 -4.1826115 -4.2102852 -4.2306256 -4.2599173 -4.2806745 -4.3000851 -4.3307924 -4.3432965][-4.1797042 -4.1733189 -4.1621103 -4.1554208 -4.1488376 -4.1351867 -4.1256995 -4.1413317 -4.1769409 -4.2121267 -4.2513037 -4.2791314 -4.2995296 -4.325417 -4.3325515][-4.1832919 -4.1749406 -4.1591263 -4.1430907 -4.1193466 -4.0897317 -4.0646324 -4.0671568 -4.1060038 -4.1574821 -4.2122664 -4.2541008 -4.2834616 -4.3103004 -4.3195953][-4.1827955 -4.1744857 -4.1600084 -4.1410589 -4.1076274 -4.0672736 -4.0245323 -4.0020833 -4.0245695 -4.0796223 -4.1480751 -4.2059345 -4.2489171 -4.2856245 -4.30529][-4.1966887 -4.1903439 -4.1802168 -4.1643696 -4.1347589 -4.0985913 -4.0589652 -4.0285282 -4.0271392 -4.0632496 -4.1218438 -4.1775327 -4.2244749 -4.2671742 -4.2956438][-4.2115579 -4.2098351 -4.2061439 -4.1959105 -4.1785564 -4.1562424 -4.1323977 -4.1107659 -4.1051955 -4.1280565 -4.1645465 -4.2024612 -4.2378988 -4.2747507 -4.2990174][-4.2232418 -4.2275438 -4.2319307 -4.2307973 -4.22507 -4.2136321 -4.1991091 -4.1865773 -4.1849761 -4.1995482 -4.2234435 -4.2502923 -4.2738581 -4.29935 -4.3144474][-4.2377162 -4.2441263 -4.2510681 -4.2535529 -4.2535486 -4.2498622 -4.2424817 -4.2363148 -4.2371345 -4.2471442 -4.2658205 -4.288022 -4.3048 -4.3219805 -4.3298426][-4.2476606 -4.2522063 -4.2581124 -4.2623987 -4.2644138 -4.2642412 -4.2604551 -4.2529607 -4.2503066 -4.2601223 -4.2795258 -4.2995992 -4.3152909 -4.3303552 -4.3346453][-4.2443213 -4.2469611 -4.2500362 -4.2537374 -4.2558589 -4.2550354 -4.2480474 -4.2328491 -4.2267323 -4.2366834 -4.2545981 -4.274725 -4.2947626 -4.31358 -4.3208981][-4.2401543 -4.2400045 -4.2397 -4.2399497 -4.2388768 -4.2338161 -4.2207918 -4.1963391 -4.1852236 -4.1913018 -4.2056308 -4.2303839 -4.256197 -4.2796903 -4.2953868][-4.23095 -4.2312984 -4.2297397 -4.2279253 -4.2249765 -4.2182522 -4.2020059 -4.1733408 -4.1620426 -4.1692691 -4.1835346 -4.208281 -4.233037 -4.2581186 -4.2804847]]...]
INFO - root - 2017-12-06 04:27:33.454591: step 24010, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.223 sec/batch; 19h:08m:42s remains)
INFO - root - 2017-12-06 04:27:35.786865: step 24020, loss = 2.03, batch loss = 1.97 (35.2 examples/sec; 0.227 sec/batch; 19h:28m:10s remains)
INFO - root - 2017-12-06 04:27:38.103455: step 24030, loss = 2.04, batch loss = 1.99 (34.0 examples/sec; 0.235 sec/batch; 20h:08m:52s remains)
INFO - root - 2017-12-06 04:27:40.385738: step 24040, loss = 2.08, batch loss = 2.02 (35.2 examples/sec; 0.227 sec/batch; 19h:29m:00s remains)
INFO - root - 2017-12-06 04:27:42.714877: step 24050, loss = 2.05, batch loss = 1.99 (33.8 examples/sec; 0.237 sec/batch; 20h:18m:21s remains)
INFO - root - 2017-12-06 04:27:45.010924: step 24060, loss = 2.09, batch loss = 2.03 (33.4 examples/sec; 0.240 sec/batch; 20h:33m:02s remains)
INFO - root - 2017-12-06 04:27:47.349357: step 24070, loss = 2.05, batch loss = 1.99 (33.9 examples/sec; 0.236 sec/batch; 20h:13m:55s remains)
INFO - root - 2017-12-06 04:27:49.629110: step 24080, loss = 2.07, batch loss = 2.01 (34.6 examples/sec; 0.232 sec/batch; 19h:50m:07s remains)
INFO - root - 2017-12-06 04:27:51.921294: step 24090, loss = 2.07, batch loss = 2.01 (35.3 examples/sec; 0.227 sec/batch; 19h:25m:24s remains)
INFO - root - 2017-12-06 04:27:54.248273: step 24100, loss = 2.07, batch loss = 2.01 (33.8 examples/sec; 0.237 sec/batch; 20h:16m:58s remains)
2017-12-06 04:27:54.544930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2903957 -4.2896023 -4.2898273 -4.2907085 -4.2986274 -4.3022256 -4.2901926 -4.2686377 -4.2566714 -4.2598653 -4.2706509 -4.2891126 -4.3090196 -4.3207645 -4.3177357][-4.2997875 -4.3031178 -4.3042631 -4.3054786 -4.3092556 -4.3009334 -4.2725062 -4.2333975 -4.208508 -4.2108836 -4.2272458 -4.2573571 -4.2913675 -4.3131065 -4.3161654][-4.3043904 -4.3136678 -4.3164458 -4.3161874 -4.3137078 -4.2928143 -4.247992 -4.1914716 -4.1558728 -4.1579347 -4.1809583 -4.22351 -4.2734985 -4.3059111 -4.314435][-4.3014622 -4.3170853 -4.3205905 -4.3187695 -4.3102994 -4.27858 -4.2195692 -4.1497808 -4.1073604 -4.1069651 -4.1331277 -4.1886921 -4.2569451 -4.3018923 -4.3151832][-4.2861304 -4.3099151 -4.3163457 -4.3142662 -4.3020096 -4.2644053 -4.1996689 -4.1255393 -4.0809379 -4.0750861 -4.0995336 -4.16236 -4.2398796 -4.2918377 -4.3112011][-4.2669411 -4.2976227 -4.3067231 -4.3040996 -4.2895122 -4.2515197 -4.1920424 -4.1275277 -4.087306 -4.0764456 -4.0934939 -4.1496024 -4.2174749 -4.265882 -4.2900763][-4.2539706 -4.285182 -4.2932534 -4.2898483 -4.2790704 -4.2507224 -4.2055397 -4.1573162 -4.1251121 -4.1109166 -4.1180096 -4.1542597 -4.1960959 -4.2285008 -4.2507663][-4.2629147 -4.2898335 -4.294652 -4.2886314 -4.2804384 -4.2619939 -4.2300544 -4.1951432 -4.1697097 -4.1549206 -4.1564584 -4.1730204 -4.1879253 -4.20231 -4.2190385][-4.2852769 -4.3051314 -4.3048067 -4.2944183 -4.2869534 -4.2745166 -4.2511168 -4.2250013 -4.2076039 -4.1983652 -4.2002978 -4.2048473 -4.2022605 -4.2046642 -4.2153816][-4.303328 -4.3178535 -4.3147492 -4.3030963 -4.2959709 -4.2866292 -4.2702851 -4.2523994 -4.242754 -4.2410321 -4.2459149 -4.2473783 -4.2399888 -4.2377443 -4.2430921][-4.3152261 -4.3261805 -4.3217273 -4.3106456 -4.3044572 -4.297215 -4.2859864 -4.2743335 -4.2700543 -4.2717781 -4.2771153 -4.2788067 -4.2743421 -4.27453 -4.278511][-4.3230624 -4.3308468 -4.3270931 -4.3195195 -4.3151178 -4.3105764 -4.3035188 -4.296051 -4.2931561 -4.2922387 -4.2934852 -4.2952061 -4.29632 -4.3006625 -4.3057628][-4.3271208 -4.3320966 -4.3294678 -4.3262973 -4.3248711 -4.3231683 -4.3193431 -4.3148823 -4.3113151 -4.3067956 -4.3038893 -4.305171 -4.3089638 -4.3156576 -4.3231235][-4.3274632 -4.3310995 -4.3305869 -4.3304391 -4.3309984 -4.3312178 -4.3286147 -4.32346 -4.3167405 -4.3093367 -4.3046784 -4.3062744 -4.3107152 -4.3178616 -4.3266931][-4.3230152 -4.3260775 -4.3271079 -4.328671 -4.3303437 -4.3310132 -4.3279777 -4.3209729 -4.3126278 -4.3055186 -4.301383 -4.3026147 -4.3065529 -4.313653 -4.3236141]]...]
INFO - root - 2017-12-06 04:27:56.810868: step 24110, loss = 2.07, batch loss = 2.01 (34.8 examples/sec; 0.230 sec/batch; 19h:42m:26s remains)
INFO - root - 2017-12-06 04:27:59.108908: step 24120, loss = 2.05, batch loss = 1.99 (36.0 examples/sec; 0.222 sec/batch; 19h:01m:05s remains)
INFO - root - 2017-12-06 04:28:01.431493: step 24130, loss = 2.06, batch loss = 2.01 (35.0 examples/sec; 0.228 sec/batch; 19h:33m:15s remains)
INFO - root - 2017-12-06 04:28:03.692737: step 24140, loss = 2.05, batch loss = 1.99 (35.3 examples/sec; 0.226 sec/batch; 19h:23m:33s remains)
INFO - root - 2017-12-06 04:28:06.009080: step 24150, loss = 2.06, batch loss = 2.00 (33.9 examples/sec; 0.236 sec/batch; 20h:11m:02s remains)
INFO - root - 2017-12-06 04:28:08.310891: step 24160, loss = 2.06, batch loss = 2.00 (35.8 examples/sec; 0.224 sec/batch; 19h:08m:59s remains)
INFO - root - 2017-12-06 04:28:10.658013: step 24170, loss = 2.05, batch loss = 1.99 (32.9 examples/sec; 0.243 sec/batch; 20h:50m:53s remains)
INFO - root - 2017-12-06 04:28:12.979647: step 24180, loss = 2.06, batch loss = 2.00 (34.4 examples/sec; 0.233 sec/batch; 19h:55m:18s remains)
INFO - root - 2017-12-06 04:28:15.277599: step 24190, loss = 2.05, batch loss = 2.00 (36.4 examples/sec; 0.220 sec/batch; 18h:48m:23s remains)
INFO - root - 2017-12-06 04:28:17.596245: step 24200, loss = 2.07, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 19h:59m:23s remains)
2017-12-06 04:28:17.900743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.149262 -4.1744728 -4.2022705 -4.2356415 -4.2563353 -4.2634411 -4.2667341 -4.2723818 -4.279047 -4.2793045 -4.2697439 -4.2595587 -4.2567453 -4.2519026 -4.2315264][-4.1121182 -4.1472888 -4.1779633 -4.217557 -4.2469459 -4.2592053 -4.2629089 -4.2661128 -4.2680006 -4.2598805 -4.245698 -4.2365785 -4.2357955 -4.2307391 -4.2073631][-4.1178517 -4.1529326 -4.1762352 -4.2093167 -4.23661 -4.2485962 -4.2514277 -4.2546153 -4.2549391 -4.241219 -4.2261844 -4.2229323 -4.2260551 -4.2203074 -4.1970072][-4.157866 -4.1833181 -4.1892056 -4.2029619 -4.2174253 -4.2245545 -4.22527 -4.2274437 -4.2253118 -4.2075057 -4.1947894 -4.20321 -4.2163033 -4.2164159 -4.1997][-4.1817918 -4.1896815 -4.1700373 -4.1545987 -4.154551 -4.1622405 -4.1686616 -4.1749272 -4.1740174 -4.1606717 -4.1586843 -4.1840491 -4.2101026 -4.2203879 -4.2115989][-4.1789346 -4.1626587 -4.112587 -4.0644507 -4.0509462 -4.0644522 -4.0850163 -4.1019077 -4.1087322 -4.1082006 -4.1220722 -4.16528 -4.2049284 -4.225563 -4.2231584][-4.1632977 -4.1270604 -4.0539103 -3.9850113 -3.9682047 -3.9899597 -4.0224452 -4.047513 -4.0637741 -4.0833158 -4.1168036 -4.1705251 -4.21804 -4.2452 -4.245893][-4.1319966 -4.0895371 -4.017571 -3.9603977 -3.9571252 -3.9825244 -4.0143447 -4.0369759 -4.0572214 -4.09278 -4.1397715 -4.1957297 -4.2426968 -4.2701817 -4.2697535][-4.1113868 -4.0857954 -4.0416985 -4.0171347 -4.0321078 -4.0522223 -4.0721469 -4.0833297 -4.1002665 -4.1386285 -4.1859407 -4.2346883 -4.2746487 -4.2960997 -4.290206][-4.1232305 -4.1259642 -4.1133065 -4.1144376 -4.1361661 -4.1474338 -4.1545734 -4.1534953 -4.1601195 -4.1889734 -4.2264237 -4.2616935 -4.2924886 -4.3076186 -4.2973056][-4.1551647 -4.1744785 -4.1792803 -4.1922255 -4.213902 -4.2206721 -4.2216787 -4.216095 -4.2148137 -4.2306767 -4.2555203 -4.2785916 -4.2992826 -4.3071704 -4.2939525][-4.1768332 -4.2023435 -4.2149315 -4.2287626 -4.2474766 -4.2552905 -4.25839 -4.2560916 -4.2514534 -4.2561135 -4.2689147 -4.2852144 -4.298842 -4.298645 -4.2832756][-4.1682758 -4.1961713 -4.2093177 -4.2187281 -4.2336149 -4.2441769 -4.2528625 -4.2584133 -4.2563586 -4.2541494 -4.2583184 -4.2712469 -4.2825227 -4.2782221 -4.2607179][-4.1430459 -4.1701436 -4.1834583 -4.1949649 -4.2117028 -4.225605 -4.2373147 -4.2458143 -4.2463365 -4.2437387 -4.2432 -4.2514906 -4.2604256 -4.2557631 -4.2415481][-4.12336 -4.1470933 -4.1632538 -4.1851592 -4.2073607 -4.2204056 -4.2283778 -4.2341013 -4.2339439 -4.2310476 -4.2272625 -4.229157 -4.2373185 -4.2394223 -4.2365513]]...]
INFO - root - 2017-12-06 04:28:20.202477: step 24210, loss = 2.06, batch loss = 2.01 (35.2 examples/sec; 0.228 sec/batch; 19h:29m:23s remains)
INFO - root - 2017-12-06 04:28:22.498583: step 24220, loss = 2.08, batch loss = 2.02 (34.8 examples/sec; 0.230 sec/batch; 19h:39m:50s remains)
INFO - root - 2017-12-06 04:28:24.875014: step 24230, loss = 2.06, batch loss = 2.00 (34.2 examples/sec; 0.234 sec/batch; 20h:01m:26s remains)
INFO - root - 2017-12-06 04:28:27.423199: step 24240, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.425 sec/batch; 36h:22m:57s remains)
INFO - root - 2017-12-06 04:28:31.299153: step 24250, loss = 2.07, batch loss = 2.02 (35.8 examples/sec; 0.224 sec/batch; 19h:08m:45s remains)
INFO - root - 2017-12-06 04:28:35.777978: step 24260, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.468 sec/batch; 40h:01m:50s remains)
INFO - root - 2017-12-06 04:28:40.587977: step 24270, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 0.465 sec/batch; 39h:51m:18s remains)
