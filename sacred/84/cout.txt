INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "84"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-06 05:21:09.821647: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 05:21:09.821781: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 05:21:09.821787: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 05:21:09.821792: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 05:21:09.821795: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-06 05:21:14.594943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 6.39GiB
2017-12-06 05:21:14.594989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-06 05:21:14.594996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-06 05:21:14.595004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-06 05:21:37.708014: step 0, loss = 2.02, batch loss = 1.97 (0.5 examples/sec; 14.801 sec/batch; 1367h:00m:10s remains)
2017-12-06 05:21:39.525895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1637034 -4.1397376 -4.1267252 -4.1353092 -4.1486526 -4.1605473 -4.169508 -4.1718063 -4.1696715 -4.1712584 -4.1851377 -4.2084012 -4.2215905 -4.2249312 -4.2301335][-4.1651273 -4.1523113 -4.141715 -4.1433578 -4.1519127 -4.1677136 -4.1839809 -4.1882558 -4.1806459 -4.1818814 -4.200953 -4.2236629 -4.2310863 -4.2316046 -4.2346492][-4.1659842 -4.1604085 -4.148407 -4.14255 -4.14593 -4.1645608 -4.185873 -4.1886721 -4.1799922 -4.18449 -4.2094884 -4.2285595 -4.2276573 -4.2230883 -4.2257366][-4.167048 -4.1649566 -4.1519885 -4.1419778 -4.1427245 -4.1626124 -4.1866536 -4.1894407 -4.1826773 -4.1919327 -4.2173853 -4.2308769 -4.2220206 -4.2119575 -4.2125955][-4.1747484 -4.1757083 -4.1655445 -4.1541295 -4.1519241 -4.1683612 -4.1911845 -4.1972461 -4.1977992 -4.2139707 -4.2371249 -4.2441859 -4.2318678 -4.2176909 -4.2128944][-4.1867108 -4.1896882 -4.1809778 -4.1707149 -4.1660647 -4.1787586 -4.1951137 -4.2000337 -4.2093086 -4.2358 -4.25749 -4.2613325 -4.2503033 -4.2336493 -4.2258372][-4.196054 -4.1966114 -4.1846786 -4.1723428 -4.1658554 -4.1722536 -4.1778026 -4.1802745 -4.2026091 -4.2413688 -4.2672467 -4.2722287 -4.2622747 -4.2478442 -4.2409663][-4.2059832 -4.1986313 -4.1798739 -4.1659007 -4.161377 -4.1598516 -4.1531677 -4.1507487 -4.1842885 -4.235075 -4.2663579 -4.2724485 -4.2633476 -4.2516809 -4.2471642][-4.2256641 -4.2097454 -4.1893535 -4.1808815 -4.18074 -4.1707354 -4.1544313 -4.1475534 -4.1828671 -4.2306495 -4.2583113 -4.2600355 -4.2489605 -4.2415433 -4.2402849][-4.2408867 -4.2212105 -4.2056332 -4.2091107 -4.216444 -4.2067933 -4.1936464 -4.1882176 -4.2116065 -4.238441 -4.2512608 -4.2433619 -4.2282543 -4.2245336 -4.2277431][-4.2453365 -4.2235913 -4.2121425 -4.2236395 -4.241251 -4.2452025 -4.2480974 -4.2484345 -4.2556391 -4.2588549 -4.2531228 -4.2343783 -4.2171135 -4.2123666 -4.2145681][-4.2484608 -4.2220664 -4.2072387 -4.219275 -4.243475 -4.2639356 -4.279809 -4.2834554 -4.2800937 -4.2696819 -4.2520981 -4.2276015 -4.2091117 -4.202374 -4.2017994][-4.2659206 -4.2355781 -4.2102289 -4.213428 -4.2392869 -4.2700343 -4.2916012 -4.2950363 -4.284338 -4.2680287 -4.2479067 -4.2246795 -4.2079291 -4.2015905 -4.2015743][-4.2875552 -4.2579 -4.2257414 -4.2177882 -4.237464 -4.2709961 -4.2948318 -4.2976761 -4.2861595 -4.2703285 -4.2531142 -4.2325416 -4.2189078 -4.2162032 -4.2209697][-4.3042688 -4.2819433 -4.2512426 -4.2350621 -4.2440653 -4.2697845 -4.2905288 -4.2985468 -4.2960868 -4.2895284 -4.2793455 -4.2634983 -4.2519298 -4.251379 -4.2593293]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 05:21:47.657930: step 10, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 62h:03m:47s remains)
INFO - root - 2017-12-06 05:21:54.505610: step 20, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 58h:28m:32s remains)
INFO - root - 2017-12-06 05:22:00.053770: step 30, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.441 sec/batch; 40h:42m:09s remains)
INFO - root - 2017-12-06 05:22:05.542968: step 40, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.653 sec/batch; 60h:19m:02s remains)
INFO - root - 2017-12-06 05:22:12.121822: step 50, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 59h:55m:44s remains)
INFO - root - 2017-12-06 05:22:18.579105: step 60, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.647 sec/batch; 59h:46m:28s remains)
INFO - root - 2017-12-06 05:22:24.985807: step 70, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 60h:46m:51s remains)
INFO - root - 2017-12-06 05:22:31.489641: step 80, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.617 sec/batch; 56h:56m:04s remains)
INFO - root - 2017-12-06 05:22:38.138681: step 90, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 59h:52m:47s remains)
INFO - root - 2017-12-06 05:22:44.720764: step 100, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 63h:07m:25s remains)
2017-12-06 05:22:45.388340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2561178 -4.2278705 -4.1942172 -4.1656756 -4.1596627 -4.1726542 -4.1883607 -4.2018309 -4.2181945 -4.22895 -4.2273726 -4.2226543 -4.2155447 -4.1957688 -4.178679][-4.2667089 -4.23419 -4.18969 -4.1455297 -4.1251888 -4.1349788 -4.1562176 -4.1723208 -4.1887488 -4.2013578 -4.2037754 -4.2055826 -4.204422 -4.1882558 -4.1760292][-4.2749586 -4.2383385 -4.184576 -4.1281233 -4.0956807 -4.0985384 -4.1185851 -4.1343141 -4.1505537 -4.1675277 -4.1770473 -4.1840415 -4.1866322 -4.1743727 -4.1652894][-4.2784028 -4.2390289 -4.1818218 -4.1229825 -4.0878057 -4.0864043 -4.1026058 -4.1146145 -4.1312776 -4.1541529 -4.1700416 -4.17814 -4.1784105 -4.1651664 -4.154582][-4.278595 -4.2380686 -4.1815143 -4.1240692 -4.0884013 -4.082046 -4.0938568 -4.101347 -4.1173573 -4.1441617 -4.1656408 -4.1745319 -4.1733627 -4.1622543 -4.1533666][-4.2769175 -4.2369165 -4.1833038 -4.1295538 -4.0907197 -4.0754128 -4.0789156 -4.0770059 -4.087657 -4.1168861 -4.1454983 -4.1587615 -4.1606092 -4.1557031 -4.1519952][-4.2747107 -4.2350516 -4.1840725 -4.1317282 -4.0877042 -4.063839 -4.0574856 -4.0421982 -4.0416322 -4.071054 -4.1081419 -4.1288071 -4.1375422 -4.1409965 -4.1430664][-4.2723336 -4.2322307 -4.1803989 -4.1251974 -4.0769472 -4.0509667 -4.0411663 -4.0200777 -4.0121975 -4.0388565 -4.0786238 -4.1018782 -4.115562 -4.12565 -4.1318579][-4.2721462 -4.2325616 -4.1806946 -4.1236033 -4.0756083 -4.0525751 -4.0437307 -4.0273585 -4.0225058 -4.0469255 -4.0807281 -4.0989647 -4.1104894 -4.1202254 -4.1234837][-4.2727757 -4.2321725 -4.1797309 -4.1213059 -4.0754504 -4.0544071 -4.0473967 -4.0398579 -4.0449057 -4.0701866 -4.0955286 -4.1047907 -4.1089792 -4.1123538 -4.110744][-4.277421 -4.2357287 -4.1834235 -4.126698 -4.08468 -4.0653505 -4.0596266 -4.05767 -4.0681834 -4.0927982 -4.1130481 -4.1158848 -4.1120152 -4.1087918 -4.1059346][-4.2860427 -4.2459526 -4.1967998 -4.1459608 -4.1105037 -4.0962043 -4.0954709 -4.0995784 -4.1104469 -4.1286755 -4.1414671 -4.1409574 -4.1330514 -4.1269622 -4.12472][-4.2958183 -4.2595139 -4.215807 -4.1729169 -4.1446505 -4.1361642 -4.1429982 -4.1558428 -4.16761 -4.1789451 -4.184701 -4.1818166 -4.1707168 -4.1625438 -4.1603842][-4.3087897 -4.280344 -4.24595 -4.2124066 -4.1895871 -4.1849289 -4.1959743 -4.2136965 -4.2270794 -4.2346773 -4.2354336 -4.2297359 -4.21737 -4.2095265 -4.209619][-4.3238072 -4.3058028 -4.2841911 -4.2626734 -4.245779 -4.24129 -4.2495942 -4.263886 -4.2752347 -4.2808404 -4.2802072 -4.2748857 -4.2652664 -4.2605143 -4.2628093]]...]
INFO - root - 2017-12-06 05:22:51.705348: step 110, loss = 2.08, batch loss = 2.02 (14.0 examples/sec; 0.572 sec/batch; 52h:51m:05s remains)
INFO - root - 2017-12-06 05:22:58.121759: step 120, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 60h:00m:54s remains)
INFO - root - 2017-12-06 05:23:04.574349: step 130, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 59h:47m:00s remains)
INFO - root - 2017-12-06 05:23:10.289048: step 140, loss = 2.06, batch loss = 2.00 (18.3 examples/sec; 0.436 sec/batch; 40h:16m:45s remains)
INFO - root - 2017-12-06 05:23:15.509925: step 150, loss = 2.10, batch loss = 2.05 (12.5 examples/sec; 0.639 sec/batch; 58h:59m:16s remains)
INFO - root - 2017-12-06 05:23:21.922550: step 160, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 60h:44m:06s remains)
INFO - root - 2017-12-06 05:23:28.275793: step 170, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.621 sec/batch; 57h:17m:21s remains)
INFO - root - 2017-12-06 05:23:34.716552: step 180, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 61h:04m:16s remains)
INFO - root - 2017-12-06 05:23:41.178054: step 190, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 59h:03m:14s remains)
INFO - root - 2017-12-06 05:23:47.626888: step 200, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 59h:15m:12s remains)
2017-12-06 05:23:49.795757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3176031 -4.3116169 -4.3070493 -4.3086014 -4.3146629 -4.3208551 -4.3239274 -4.3240013 -4.3242435 -4.3270869 -4.3336496 -4.3420134 -4.3489389 -4.3527608 -4.3512883][-4.3004503 -4.2897167 -4.2860274 -4.2921185 -4.3006372 -4.3038497 -4.30131 -4.2944098 -4.2896228 -4.2934127 -4.3071566 -4.3243423 -4.3385611 -4.3453093 -4.3446479][-4.2815008 -4.2687044 -4.2696209 -4.2790403 -4.2847486 -4.2801156 -4.2677565 -4.2485943 -4.2378793 -4.2481008 -4.2746429 -4.3020334 -4.3244219 -4.3347836 -4.33584][-4.2714081 -4.2592578 -4.2639828 -4.2707119 -4.265295 -4.2488408 -4.2233429 -4.1885681 -4.1746011 -4.1968441 -4.2379789 -4.2735887 -4.3022962 -4.3180389 -4.3234019][-4.265605 -4.2548232 -4.2577267 -4.2569652 -4.2386246 -4.2090907 -4.1670003 -4.119647 -4.1131349 -4.153172 -4.2068233 -4.2465811 -4.2789354 -4.3004913 -4.3109651][-4.2367506 -4.2198987 -4.2153192 -4.2048893 -4.1730075 -4.1257424 -4.064796 -4.0151992 -4.0364819 -4.1031919 -4.1688809 -4.2135525 -4.2531605 -4.2840276 -4.3013272][-4.18844 -4.1561627 -4.1365666 -4.110373 -4.0603452 -3.9913552 -3.9174027 -3.8867905 -3.9541049 -4.051734 -4.1264877 -4.1778736 -4.2278357 -4.2695551 -4.2932076][-4.1446166 -4.0977535 -4.0629354 -4.0225654 -3.9604518 -3.88059 -3.8075182 -3.8067226 -3.9102564 -4.0252848 -4.1048427 -4.1638031 -4.2220292 -4.2678108 -4.291492][-4.0992346 -4.05124 -4.01936 -3.9894631 -3.9436455 -3.8876727 -3.8462329 -3.8626423 -3.9528136 -4.0492516 -4.1202545 -4.1785212 -4.2360215 -4.2782149 -4.2980819][-4.0768147 -4.0407276 -4.0282755 -4.0263567 -4.0137458 -3.98924 -3.9721754 -3.9870265 -4.044229 -4.1081104 -4.1617427 -4.2119918 -4.2601151 -4.2934222 -4.3084006][-4.09602 -4.0790405 -4.0869493 -4.1054063 -4.115355 -4.1061797 -4.0969772 -4.1057944 -4.136694 -4.1731544 -4.2101974 -4.2489433 -4.285212 -4.3108 -4.3225946][-4.1521335 -4.1511388 -4.167913 -4.1897774 -4.20444 -4.2008481 -4.1953354 -4.2017078 -4.2177591 -4.2369547 -4.2588687 -4.2858076 -4.3125033 -4.3312368 -4.3394275][-4.2233195 -4.2272711 -4.2418633 -4.2554717 -4.2624016 -4.2559576 -4.2504711 -4.2538171 -4.2634134 -4.2764635 -4.2920423 -4.3123522 -4.3329625 -4.3472586 -4.352644][-4.2773247 -4.2823687 -4.2903576 -4.294373 -4.2932172 -4.2849936 -4.279305 -4.2799726 -4.2855763 -4.2945142 -4.3065 -4.3244319 -4.3426566 -4.3538313 -4.3573346][-4.3117909 -4.3140817 -4.3162441 -4.31478 -4.3110766 -4.3067088 -4.3050642 -4.3062596 -4.3082538 -4.3115454 -4.3193836 -4.33355 -4.3472648 -4.3541822 -4.3552818]]...]
INFO - root - 2017-12-06 05:23:56.161527: step 210, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 60h:27m:54s remains)
INFO - root - 2017-12-06 05:24:02.582316: step 220, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 60h:44m:47s remains)
INFO - root - 2017-12-06 05:24:08.985657: step 230, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.620 sec/batch; 57h:12m:25s remains)
INFO - root - 2017-12-06 05:24:15.474843: step 240, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.637 sec/batch; 58h:45m:49s remains)
INFO - root - 2017-12-06 05:24:21.833005: step 250, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 61h:07m:50s remains)
INFO - root - 2017-12-06 05:24:28.351363: step 260, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 60h:27m:16s remains)
INFO - root - 2017-12-06 05:24:34.873462: step 270, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 59h:28m:21s remains)
INFO - root - 2017-12-06 05:24:41.370858: step 280, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 61h:53m:16s remains)
INFO - root - 2017-12-06 05:24:48.034168: step 290, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 58h:09m:20s remains)
INFO - root - 2017-12-06 05:24:54.343327: step 300, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 58h:39m:17s remains)
2017-12-06 05:24:56.282705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0746303 -4.066812 -4.0800033 -4.1120634 -4.1588507 -4.1987276 -4.2122827 -4.202002 -4.1742115 -4.1442575 -4.1321135 -4.1338353 -4.1323156 -4.1322842 -4.1394296][-4.1508093 -4.1289706 -4.1129627 -4.1159487 -4.1511784 -4.1963792 -4.2193151 -4.2180552 -4.2028909 -4.1868405 -4.17989 -4.1778908 -4.1676946 -4.1550779 -4.1495752][-4.213376 -4.1841793 -4.1460352 -4.1201348 -4.13711 -4.1809168 -4.211575 -4.2221918 -4.2237525 -4.2212939 -4.2159824 -4.2077055 -4.1863284 -4.1579614 -4.1404214][-4.23933 -4.2079482 -4.1575704 -4.1119413 -4.1111169 -4.1474538 -4.1787972 -4.1984339 -4.214643 -4.223052 -4.2170877 -4.201365 -4.1729536 -4.1366625 -4.1163316][-4.22563 -4.197607 -4.1510811 -4.0999632 -4.0850563 -4.1076684 -4.1344194 -4.1596308 -4.185338 -4.2027354 -4.1944528 -4.1671176 -4.1321907 -4.0938187 -4.0792904][-4.1914167 -4.1669827 -4.1320815 -4.0912886 -4.0737648 -4.0804892 -4.0977993 -4.1214366 -4.1463661 -4.1618605 -4.1489005 -4.1148515 -4.0764933 -4.040113 -4.033484][-4.1647162 -4.1435828 -4.1191249 -4.0956845 -4.0854893 -4.08322 -4.0924611 -4.1131964 -4.128479 -4.1314378 -4.1115975 -4.0711012 -4.0239182 -3.9865763 -3.9895463][-4.1554317 -4.1360521 -4.117867 -4.1073065 -4.1054688 -4.0994091 -4.1019354 -4.1193662 -4.1287918 -4.1200614 -4.0968943 -4.0561428 -4.0021739 -3.9624352 -3.9755838][-4.165494 -4.1558371 -4.1489172 -4.1460629 -4.1442127 -4.13459 -4.1293836 -4.1398449 -4.1422076 -4.1278133 -4.1076727 -4.0735455 -4.0227375 -3.988513 -4.0084567][-4.1816826 -4.1878061 -4.1948409 -4.1986895 -4.1954155 -4.18133 -4.170115 -4.1712027 -4.1667233 -4.1529307 -4.1381855 -4.1149197 -4.0740438 -4.046443 -4.0640988][-4.1811209 -4.2008362 -4.21659 -4.2263713 -4.2266331 -4.2132516 -4.1975069 -4.1884613 -4.1778221 -4.1664639 -4.162889 -4.15512 -4.1346478 -4.1197577 -4.1347141][-4.1680617 -4.1996489 -4.2209649 -4.2379751 -4.2461238 -4.2352529 -4.2192268 -4.2042203 -4.1870074 -4.1716304 -4.1766953 -4.1833239 -4.1776915 -4.1724067 -4.1837263][-4.1515608 -4.1861849 -4.2089162 -4.2311449 -4.243773 -4.2374187 -4.2273283 -4.2114534 -4.1876841 -4.1684718 -4.1780515 -4.1960196 -4.1991682 -4.1985822 -4.2064862][-4.1458173 -4.1717429 -4.1921082 -4.2144241 -4.2293744 -4.2301712 -4.2275538 -4.2132096 -4.1866331 -4.1641755 -4.17424 -4.1962805 -4.2033725 -4.2054591 -4.2117262][-4.1489716 -4.1580095 -4.1691875 -4.1878796 -4.2047176 -4.2129326 -4.2183871 -4.207634 -4.1818819 -4.1593313 -4.1674581 -4.185585 -4.1903281 -4.1900129 -4.1925616]]...]
INFO - root - 2017-12-06 05:25:02.701789: step 310, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 62h:22m:00s remains)
INFO - root - 2017-12-06 05:25:09.337761: step 320, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.656 sec/batch; 60h:30m:47s remains)
INFO - root - 2017-12-06 05:25:15.878169: step 330, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 61h:10m:37s remains)
INFO - root - 2017-12-06 05:25:22.303880: step 340, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 58h:46m:38s remains)
INFO - root - 2017-12-06 05:25:27.763190: step 350, loss = 2.08, batch loss = 2.02 (19.6 examples/sec; 0.407 sec/batch; 37h:34m:24s remains)
INFO - root - 2017-12-06 05:25:33.626703: step 360, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 59h:33m:59s remains)
INFO - root - 2017-12-06 05:25:40.187281: step 370, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 60h:46m:45s remains)
INFO - root - 2017-12-06 05:25:46.562238: step 380, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 60h:44m:14s remains)
INFO - root - 2017-12-06 05:25:53.063957: step 390, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 60h:35m:31s remains)
INFO - root - 2017-12-06 05:25:59.575984: step 400, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 58h:45m:15s remains)
2017-12-06 05:26:00.206276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1886539 -4.1840758 -4.1848512 -4.1917706 -4.1856203 -4.1881294 -4.20557 -4.2111373 -4.2186594 -4.2436028 -4.267221 -4.2786818 -4.2819905 -4.2827463 -4.2822051][-4.1751289 -4.1820812 -4.1832271 -4.1891322 -4.1834888 -4.1826835 -4.1921492 -4.1960545 -4.2069597 -4.2335987 -4.2597308 -4.2737341 -4.2784104 -4.2807031 -4.2803612][-4.1481462 -4.1699247 -4.1778316 -4.184351 -4.1792397 -4.1735482 -4.1689911 -4.1697993 -4.190557 -4.2222805 -4.2489266 -4.2654333 -4.2710671 -4.273602 -4.2709002][-4.1402588 -4.1642132 -4.1726766 -4.1724467 -4.163455 -4.1479087 -4.1210146 -4.1200819 -4.1622348 -4.2060528 -4.2365665 -4.2566576 -4.2628231 -4.2629676 -4.2591124][-4.1764426 -4.185575 -4.1741362 -4.1520634 -4.1283264 -4.0857477 -4.0303411 -4.0426755 -4.1250439 -4.1823487 -4.2142406 -4.2360606 -4.237505 -4.2294841 -4.2237258][-4.207304 -4.1981106 -4.1657634 -4.1210327 -4.0733013 -3.9860566 -3.8924692 -3.9325943 -4.0659904 -4.1348672 -4.16245 -4.186192 -4.1847968 -4.1685762 -4.1605449][-4.2103815 -4.1912727 -4.1454258 -4.0868745 -4.0188065 -3.8904512 -3.7573574 -3.8169281 -3.9833691 -4.0615959 -4.085803 -4.1125431 -4.1126442 -4.0946522 -4.0863104][-4.2045441 -4.1885715 -4.1443768 -4.091013 -4.0311427 -3.9146733 -3.7823858 -3.8222663 -3.9611621 -4.029829 -4.05241 -4.0791578 -4.079752 -4.0590363 -4.0440307][-4.2204857 -4.2112813 -4.176928 -4.1342087 -4.09512 -4.0210633 -3.9341624 -3.9498467 -4.027339 -4.0692148 -4.0877748 -4.11512 -4.1178107 -4.0976763 -4.0821848][-4.2562547 -4.2534609 -4.2286263 -4.1917214 -4.1615334 -4.1226768 -4.0782924 -4.0802097 -4.1129313 -4.1338382 -4.1531115 -4.1816692 -4.1894712 -4.17835 -4.1703434][-4.2894664 -4.2889881 -4.2730827 -4.2449989 -4.2157288 -4.1940484 -4.1741762 -4.1689067 -4.1766915 -4.1895208 -4.2107711 -4.2372432 -4.2459145 -4.2420063 -4.2404895][-4.301784 -4.3001833 -4.2924013 -4.271986 -4.2443395 -4.2294059 -4.2195663 -4.2096038 -4.20592 -4.2123785 -4.2311134 -4.2534876 -4.2614074 -4.2606163 -4.26357][-4.2806692 -4.2812204 -4.2805223 -4.2685542 -4.2437549 -4.2310476 -4.2225471 -4.2069931 -4.1949906 -4.194088 -4.2084169 -4.2271876 -4.2324886 -4.2315712 -4.2389774][-4.2388167 -4.2458234 -4.2530332 -4.249629 -4.2313185 -4.2195058 -4.2122793 -4.19556 -4.175127 -4.1630635 -4.1700335 -4.1835833 -4.1824503 -4.1766906 -4.1859288][-4.1973271 -4.2107639 -4.2245464 -4.2313466 -4.2215657 -4.2131367 -4.2097688 -4.1982265 -4.1753163 -4.1551094 -4.1520753 -4.1549072 -4.1477618 -4.1359696 -4.1420531]]...]
INFO - root - 2017-12-06 05:26:06.541121: step 410, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 59h:40m:08s remains)
INFO - root - 2017-12-06 05:26:12.843262: step 420, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.621 sec/batch; 57h:16m:17s remains)
INFO - root - 2017-12-06 05:26:19.247985: step 430, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 58h:04m:34s remains)
INFO - root - 2017-12-06 05:26:25.588389: step 440, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 58h:36m:01s remains)
INFO - root - 2017-12-06 05:26:32.128682: step 450, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 62h:09m:03s remains)
INFO - root - 2017-12-06 05:26:38.421944: step 460, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.615 sec/batch; 56h:45m:08s remains)
INFO - root - 2017-12-06 05:26:44.921839: step 470, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 59h:13m:35s remains)
INFO - root - 2017-12-06 05:26:51.390496: step 480, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 61h:01m:29s remains)
INFO - root - 2017-12-06 05:26:57.874753: step 490, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 59h:41m:36s remains)
INFO - root - 2017-12-06 05:27:04.433806: step 500, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 60h:25m:59s remains)
2017-12-06 05:27:06.708666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.226747 -4.2229061 -4.2140117 -4.1923146 -4.181488 -4.1878552 -4.196558 -4.2033391 -4.2047396 -4.1983209 -4.1864395 -4.1620703 -4.1345038 -4.1075015 -4.0824375][-4.1895103 -4.1850419 -4.1720395 -4.1465507 -4.1334872 -4.1409855 -4.1579447 -4.1805525 -4.1960583 -4.1945772 -4.1772285 -4.1427355 -4.105783 -4.0792356 -4.053339][-4.1630034 -4.1515284 -4.1283727 -4.0982523 -4.0868049 -4.1024213 -4.133595 -4.1719851 -4.1950245 -4.1882792 -4.1586041 -4.1183362 -4.0783234 -4.0536942 -4.036571][-4.1451826 -4.1197062 -4.081357 -4.045907 -4.0409937 -4.0688682 -4.1145949 -4.1658225 -4.1898632 -4.1740985 -4.1365643 -4.09579 -4.0595908 -4.0376759 -4.0321465][-4.1227512 -4.0839562 -4.0362515 -4.001616 -4.0006967 -4.0309553 -4.0818563 -4.1401367 -4.1667037 -4.1467628 -4.1060376 -4.072413 -4.0446644 -4.0355363 -4.0466919][-4.1067867 -4.0627985 -4.0126028 -3.9750352 -3.9621809 -3.9761877 -4.0161943 -4.0721264 -4.1041842 -4.0866013 -4.0523357 -4.0356231 -4.0251846 -4.0322905 -4.0551577][-4.1135106 -4.0750289 -4.0283957 -3.9764655 -3.9289818 -3.8984509 -3.9026449 -3.9502208 -3.997066 -3.9953716 -3.9824185 -3.9940345 -4.0024428 -4.0207777 -4.051672][-4.1157413 -4.0943265 -4.0584431 -3.9972486 -3.9144683 -3.8262818 -3.7777743 -3.8164229 -3.892175 -3.924731 -3.9466953 -3.9855008 -4.0080252 -4.0257087 -4.0529108][-4.0965161 -4.0833049 -4.0568738 -4.0039139 -3.9231265 -3.8207657 -3.7480981 -3.7751966 -3.8590341 -3.907769 -3.9550278 -4.0114188 -4.038806 -4.0489588 -4.0680542][-4.0712137 -4.0585728 -4.0419769 -4.0120935 -3.96231 -3.8926537 -3.8403916 -3.8580124 -3.9141982 -3.9498553 -3.9977174 -4.0580597 -4.0863066 -4.08832 -4.0925121][-4.0606947 -4.0573344 -4.0551147 -4.0469918 -4.0278335 -3.9948409 -3.9668889 -3.9713192 -3.9913859 -3.999954 -4.0320163 -4.0842409 -4.1110988 -4.1091933 -4.1025033][-4.0827947 -4.0923219 -4.0945525 -4.0926075 -4.0871034 -4.0733528 -4.0573282 -4.0521865 -4.0518918 -4.0379333 -4.0463419 -4.0828295 -4.1074781 -4.1056385 -4.1001625][-4.125432 -4.1364331 -4.1337013 -4.1248555 -4.1196032 -4.1103034 -4.0930471 -4.0832367 -4.0770659 -4.0590477 -4.0551696 -4.0785351 -4.1003232 -4.106761 -4.1138182][-4.1572566 -4.1621871 -4.154151 -4.1390424 -4.1310883 -4.1230927 -4.1068864 -4.0995617 -4.0984731 -4.0900908 -4.0867395 -4.0972004 -4.109664 -4.1239686 -4.1396656][-4.1741629 -4.1771464 -4.1687069 -4.1533637 -4.1460414 -4.1374454 -4.1272907 -4.1279855 -4.1335959 -4.1345382 -4.130815 -4.1251264 -4.1281457 -4.1431961 -4.1589785]]...]
INFO - root - 2017-12-06 05:27:13.202999: step 510, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 58h:53m:02s remains)
INFO - root - 2017-12-06 05:27:19.646878: step 520, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 62h:27m:50s remains)
INFO - root - 2017-12-06 05:27:26.117856: step 530, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 60h:46m:42s remains)
INFO - root - 2017-12-06 05:27:32.531086: step 540, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 58h:02m:56s remains)
INFO - root - 2017-12-06 05:27:39.012349: step 550, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 61h:31m:41s remains)
INFO - root - 2017-12-06 05:27:45.295597: step 560, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 60h:55m:09s remains)
INFO - root - 2017-12-06 05:27:51.786325: step 570, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 59h:33m:16s remains)
INFO - root - 2017-12-06 05:27:58.242694: step 580, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 57h:39m:19s remains)
INFO - root - 2017-12-06 05:28:04.767344: step 590, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 59h:58m:03s remains)
INFO - root - 2017-12-06 05:28:11.117488: step 600, loss = 2.04, batch loss = 1.98 (16.8 examples/sec; 0.475 sec/batch; 43h:48m:07s remains)
2017-12-06 05:28:12.221373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1961422 -4.1769166 -4.1704593 -4.1692924 -4.1684303 -4.1660933 -4.1760774 -4.2076082 -4.2432437 -4.2611465 -4.2674751 -4.2542057 -4.2255549 -4.2099338 -4.2201924][-4.1939139 -4.1805444 -4.1789074 -4.1804295 -4.179821 -4.1766062 -4.1876779 -4.2188768 -4.2500534 -4.262352 -4.2651157 -4.254755 -4.2355857 -4.2292972 -4.2437396][-4.1786227 -4.1642113 -4.1642146 -4.1743045 -4.1818786 -4.1828876 -4.1965408 -4.2268767 -4.2496529 -4.2531462 -4.2481737 -4.2339439 -4.2179551 -4.2189846 -4.2409248][-4.1615419 -4.14619 -4.1439939 -4.1541686 -4.1624894 -4.1575584 -4.1616516 -4.1838093 -4.1999245 -4.2018352 -4.199656 -4.1930685 -4.1816392 -4.1864715 -4.2116265][-4.1395874 -4.1313653 -4.1301408 -4.1391659 -4.1422591 -4.1234093 -4.1138654 -4.1304321 -4.1481271 -4.158566 -4.1663089 -4.1664062 -4.154377 -4.1508904 -4.1678729][-4.1083174 -4.1095362 -4.1064849 -4.1045394 -4.0923805 -4.057549 -4.0381255 -4.0592518 -4.0926838 -4.121819 -4.1400309 -4.137991 -4.1147404 -4.0986996 -4.1061335][-4.0938344 -4.1070061 -4.0973787 -4.0732512 -4.0319862 -3.9725013 -3.9422891 -3.9691384 -4.0189495 -4.0591927 -4.0796156 -4.0724163 -4.0393105 -4.018785 -4.0321493][-4.100637 -4.1162014 -4.101892 -4.0656519 -4.0069 -3.9372666 -3.9030576 -3.92633 -3.9700313 -3.9983327 -4.0050373 -3.9905066 -3.9574792 -3.9472206 -3.9827807][-4.1269641 -4.132761 -4.1160641 -4.0818238 -4.0286436 -3.9703465 -3.9413908 -3.9528649 -3.9715276 -3.9761169 -3.9642148 -3.9412172 -3.9146376 -3.9221654 -3.979419][-4.164268 -4.1634536 -4.1515512 -4.1278038 -4.0903072 -4.0519795 -4.0315561 -4.0335808 -4.0356441 -4.0267577 -4.0054336 -3.9779932 -3.9559286 -3.970799 -4.0264153][-4.1978602 -4.1915984 -4.1830177 -4.17008 -4.1502633 -4.1324067 -4.126667 -4.130343 -4.1278915 -4.1141167 -4.090754 -4.0643506 -4.0483265 -4.0610385 -4.099278][-4.2113194 -4.201395 -4.1897249 -4.1825466 -4.1778078 -4.1799641 -4.1914968 -4.2019081 -4.2025084 -4.193038 -4.1764288 -4.1555643 -4.1431241 -4.1493607 -4.1699734][-4.2057519 -4.1980214 -4.1834354 -4.1775026 -4.182754 -4.199964 -4.2235761 -4.2392712 -4.2430568 -4.238524 -4.2283225 -4.2148781 -4.2051907 -4.2079816 -4.2168388][-4.20066 -4.1998854 -4.1847658 -4.1758795 -4.1839814 -4.2068696 -4.2321539 -4.2468033 -4.2508917 -4.2491655 -4.24263 -4.2335038 -4.2255416 -4.2276287 -4.2328][-4.22055 -4.2216549 -4.20111 -4.1886997 -4.1969743 -4.216022 -4.2342739 -4.2428451 -4.2435 -4.2394004 -4.2324238 -4.2247281 -4.2183533 -4.2216992 -4.2273946]]...]
INFO - root - 2017-12-06 05:28:18.735470: step 610, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.638 sec/batch; 58h:51m:31s remains)
INFO - root - 2017-12-06 05:28:25.138759: step 620, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 59h:56m:40s remains)
INFO - root - 2017-12-06 05:28:31.513279: step 630, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 58h:36m:22s remains)
INFO - root - 2017-12-06 05:28:37.974299: step 640, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 58h:51m:57s remains)
INFO - root - 2017-12-06 05:28:44.317180: step 650, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 59h:10m:54s remains)
INFO - root - 2017-12-06 05:28:50.684235: step 660, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 60h:16m:40s remains)
INFO - root - 2017-12-06 05:28:57.066159: step 670, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 58h:50m:24s remains)
INFO - root - 2017-12-06 05:29:03.482121: step 680, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 60h:54m:29s remains)
INFO - root - 2017-12-06 05:29:09.939306: step 690, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 59h:07m:04s remains)
INFO - root - 2017-12-06 05:29:16.348553: step 700, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 58h:39m:56s remains)
2017-12-06 05:29:16.914236: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20925 -4.2057304 -4.2049317 -4.205543 -4.2067928 -4.2078638 -4.2077665 -4.2066545 -4.2058635 -4.2058811 -4.2058306 -4.2043028 -4.1970286 -4.1863337 -4.1754761][-4.2110658 -4.2045841 -4.200428 -4.1973271 -4.1949692 -4.1924758 -4.1883292 -4.1830282 -4.1790104 -4.1777344 -4.177763 -4.1762247 -4.1677184 -4.1559787 -4.1445985][-4.2313404 -4.2235227 -4.2179708 -4.212286 -4.2068624 -4.2013226 -4.1933975 -4.1838207 -4.1759 -4.1713018 -4.1689777 -4.1659608 -4.1575785 -4.1484747 -4.1395035][-4.2494597 -4.2423334 -4.2374353 -4.2320304 -4.2265859 -4.2211485 -4.2120447 -4.2007718 -4.1904607 -4.1838245 -4.18003 -4.1763964 -4.1698518 -4.1645555 -4.1586862][-4.2402496 -4.233748 -4.229435 -4.2271872 -4.226171 -4.2251086 -4.2196922 -4.2106357 -4.2024107 -4.1993742 -4.1980233 -4.1955252 -4.1899524 -4.1847262 -4.17819][-4.2113662 -4.2012477 -4.1933675 -4.1932945 -4.1988859 -4.203609 -4.2019429 -4.1964107 -4.1937723 -4.2003465 -4.2084503 -4.2125158 -4.2103791 -4.2048845 -4.1957297][-4.180686 -4.1622972 -4.1486826 -4.1490774 -4.1590419 -4.1654053 -4.1619425 -4.1564817 -4.16016 -4.1788893 -4.200994 -4.2167969 -4.2227082 -4.2200122 -4.2098217][-4.1598554 -4.1367154 -4.1210032 -4.1210833 -4.128891 -4.1291933 -4.1149216 -4.10214 -4.1086578 -4.1364889 -4.1702733 -4.1985612 -4.2159371 -4.2209039 -4.2144036][-4.1534543 -4.1351318 -4.1233759 -4.124723 -4.1289706 -4.1207361 -4.0970235 -4.074923 -4.0777249 -4.1052666 -4.140378 -4.1727958 -4.1963267 -4.2080293 -4.2076397][-4.1597548 -4.1523533 -4.1490541 -4.1535511 -4.1568332 -4.1486158 -4.1279988 -4.1066394 -4.1047077 -4.1228604 -4.1491184 -4.1748629 -4.1943297 -4.2044969 -4.2037749][-4.1817875 -4.1805191 -4.1803527 -4.18312 -4.1831803 -4.1750422 -4.160543 -4.1467214 -4.1475358 -4.1615772 -4.180985 -4.1990795 -4.210372 -4.2128563 -4.2066083][-4.2136874 -4.2117033 -4.2088461 -4.205482 -4.1989527 -4.1877217 -4.1754332 -4.1678162 -4.1736493 -4.1882548 -4.2051654 -4.220325 -4.2271643 -4.223969 -4.2121153][-4.2423835 -4.2388754 -4.2328911 -4.2264547 -4.2181134 -4.2062221 -4.19357 -4.1862664 -4.1909046 -4.2027736 -4.2165461 -4.2282314 -4.2330155 -4.2293077 -4.2174978][-4.2544565 -4.249651 -4.2431164 -4.2386689 -4.2347636 -4.2274504 -4.2173615 -4.2101021 -4.2105074 -4.2165666 -4.225316 -4.2325025 -4.2345343 -4.2302923 -4.2203259][-4.2526379 -4.2474551 -4.2415762 -4.2391863 -4.2379937 -4.2348256 -4.2298036 -4.2252407 -4.2243209 -4.2265477 -4.2306767 -4.2343621 -4.234592 -4.2297626 -4.2214804]]...]
INFO - root - 2017-12-06 05:29:23.370003: step 710, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 59h:00m:25s remains)
INFO - root - 2017-12-06 05:29:29.765055: step 720, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 58h:19m:20s remains)
INFO - root - 2017-12-06 05:29:36.227098: step 730, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 58h:45m:31s remains)
INFO - root - 2017-12-06 05:29:42.770188: step 740, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.648 sec/batch; 59h:40m:55s remains)
INFO - root - 2017-12-06 05:29:49.213849: step 750, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 59h:34m:51s remains)
INFO - root - 2017-12-06 05:29:55.504621: step 760, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.665 sec/batch; 61h:14m:13s remains)
INFO - root - 2017-12-06 05:30:01.973554: step 770, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 58h:17m:53s remains)
INFO - root - 2017-12-06 05:30:08.566061: step 780, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 60h:08m:26s remains)
INFO - root - 2017-12-06 05:30:15.024443: step 790, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 59h:12m:36s remains)
INFO - root - 2017-12-06 05:30:21.326632: step 800, loss = 2.10, batch loss = 2.04 (18.4 examples/sec; 0.435 sec/batch; 40h:04m:59s remains)
2017-12-06 05:30:21.978119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1578417 -4.1567955 -4.14649 -4.1249347 -4.1339536 -4.1760979 -4.2018065 -4.2040854 -4.1839714 -4.158021 -4.1468096 -4.1519413 -4.1579442 -4.1608224 -4.159831][-4.2063503 -4.2085352 -4.2049761 -4.1881657 -4.1913471 -4.2090149 -4.2087741 -4.188086 -4.1513691 -4.1187048 -4.1085577 -4.1100407 -4.1121917 -4.1156273 -4.1244464][-4.2204952 -4.2304077 -4.2313333 -4.2193308 -4.2135348 -4.2146044 -4.1969342 -4.1657553 -4.1292219 -4.1043 -4.1062837 -4.1090889 -4.1074181 -4.1056089 -4.1146293][-4.2226114 -4.237926 -4.2374935 -4.2262187 -4.2152333 -4.2030668 -4.1715508 -4.1365309 -4.1086926 -4.0980721 -4.1090078 -4.1117687 -4.1027503 -4.0962763 -4.1031685][-4.2247558 -4.2378454 -4.2258868 -4.2119207 -4.1971393 -4.1739354 -4.1361151 -4.103416 -4.0853043 -4.090229 -4.1084194 -4.1115642 -4.103312 -4.0969443 -4.1014438][-4.2112794 -4.2234278 -4.210988 -4.2010193 -4.1866484 -4.1532168 -4.0986934 -4.0515103 -4.0457211 -4.0802426 -4.1169481 -4.1291375 -4.1287766 -4.1224303 -4.1237893][-4.1893964 -4.2047486 -4.2035747 -4.1987562 -4.1794758 -4.1322317 -4.0412664 -3.9546158 -3.9557381 -4.0255718 -4.0915132 -4.1234217 -4.1370888 -4.1319203 -4.1330733][-4.165761 -4.1863704 -4.1904006 -4.1758046 -4.1328788 -4.0500741 -3.9010475 -3.770376 -3.7989614 -3.9227166 -4.0317321 -4.0939817 -4.1204724 -4.1186061 -4.1220722][-4.1634789 -4.1874719 -4.1902823 -4.1635876 -4.111464 -4.0158916 -3.8548563 -3.7334981 -3.7894335 -3.9277773 -4.0408587 -4.1003475 -4.1238613 -4.1251831 -4.1313977][-4.18113 -4.2072988 -4.2132034 -4.1953311 -4.1590109 -4.0950112 -3.98874 -3.9165955 -3.960444 -4.0502338 -4.1229744 -4.156486 -4.1701627 -4.1716886 -4.175952][-4.1968017 -4.2191591 -4.2261167 -4.2215457 -4.2127008 -4.1892509 -4.1374874 -4.1011782 -4.1224346 -4.1685557 -4.2114515 -4.2280445 -4.2348161 -4.2343006 -4.2336164][-4.1873946 -4.2076268 -4.2170043 -4.2223649 -4.2330117 -4.2377124 -4.2194161 -4.2024379 -4.2137918 -4.2400527 -4.2670708 -4.2750721 -4.2773151 -4.2743435 -4.2730422][-4.1479712 -4.1671824 -4.1839452 -4.2025948 -4.228651 -4.2532272 -4.2567654 -4.2512908 -4.2570758 -4.2717161 -4.2861314 -4.2879529 -4.2848167 -4.278553 -4.2767081][-4.0993433 -4.1203361 -4.1414547 -4.1703806 -4.2077384 -4.2463312 -4.2633882 -4.2648897 -4.2692428 -4.2799215 -4.288064 -4.2856 -4.2791028 -4.2699118 -4.2658143][-4.05469 -4.0724425 -4.0964856 -4.1293378 -4.1717229 -4.2176456 -4.2458358 -4.2513371 -4.2550316 -4.2640018 -4.2705541 -4.2707963 -4.2684584 -4.2631235 -4.2602944]]...]
INFO - root - 2017-12-06 05:30:28.500032: step 810, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 58h:08m:09s remains)
INFO - root - 2017-12-06 05:30:34.947700: step 820, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 58h:09m:09s remains)
INFO - root - 2017-12-06 05:30:41.388293: step 830, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 62h:31m:19s remains)
INFO - root - 2017-12-06 05:30:47.928214: step 840, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 61h:42m:04s remains)
INFO - root - 2017-12-06 05:30:54.479148: step 850, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 58h:56m:38s remains)
INFO - root - 2017-12-06 05:31:00.791542: step 860, loss = 2.09, batch loss = 2.03 (17.7 examples/sec; 0.453 sec/batch; 41h:41m:13s remains)
INFO - root - 2017-12-06 05:31:06.461985: step 870, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 60h:54m:12s remains)
INFO - root - 2017-12-06 05:31:12.819950: step 880, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 59h:57m:33s remains)
INFO - root - 2017-12-06 05:31:19.252270: step 890, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 61h:13m:31s remains)
INFO - root - 2017-12-06 05:31:25.299464: step 900, loss = 2.04, batch loss = 1.98 (18.7 examples/sec; 0.429 sec/batch; 39h:29m:04s remains)
2017-12-06 05:31:30.243748: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3026586 -4.2865868 -4.2773447 -4.2774348 -4.2815719 -4.2910047 -4.302289 -4.3110185 -4.3155766 -4.31656 -4.3150058 -4.3151803 -4.3174677 -4.3213415 -4.3255029][-4.2927251 -4.2680907 -4.2507167 -4.2448769 -4.247272 -4.2602568 -4.2787161 -4.29441 -4.3066549 -4.31462 -4.3170209 -4.319941 -4.3225451 -4.3228664 -4.3236012][-4.2781 -4.2465286 -4.2194262 -4.2025933 -4.1989121 -4.2146826 -4.2403088 -4.2619414 -4.2809806 -4.2970324 -4.3087106 -4.3188853 -4.3280649 -4.3289423 -4.3252783][-4.2602415 -4.2215037 -4.180017 -4.1479897 -4.1355548 -4.1506095 -4.1828084 -4.2153907 -4.2453017 -4.2661519 -4.2841954 -4.3048434 -4.3252873 -4.33321 -4.3289475][-4.2408648 -4.1965685 -4.140224 -4.0920272 -4.0662513 -4.070466 -4.103579 -4.1469159 -4.1978736 -4.2302055 -4.2535996 -4.2797289 -4.3080659 -4.32679 -4.3294792][-4.2267156 -4.1783648 -4.1109118 -4.0468478 -4.0001869 -3.9789369 -3.9932766 -4.0403347 -4.1161737 -4.1725826 -4.2124372 -4.2460117 -4.27896 -4.3050838 -4.3181362][-4.2231984 -4.1746292 -4.1076555 -4.0387039 -3.9744587 -3.916748 -3.8860335 -3.9105997 -4.0043693 -4.0889645 -4.1509094 -4.2001796 -4.2449465 -4.2778306 -4.2989192][-4.2340708 -4.1932678 -4.136467 -4.0763583 -4.009747 -3.9296722 -3.8528085 -3.8227265 -3.894515 -3.9929047 -4.0770063 -4.1448221 -4.2079353 -4.2537923 -4.2830768][-4.250596 -4.2206745 -4.1782832 -4.1329079 -4.077795 -4.0038857 -3.9172783 -3.8465729 -3.8604093 -3.9304287 -4.0160074 -4.097445 -4.1787157 -4.2394581 -4.2777333][-4.2626219 -4.24532 -4.2203712 -4.1900458 -4.1467862 -4.0930624 -4.0319386 -3.9670959 -3.9445028 -3.9626098 -4.0138226 -4.0837703 -4.1687474 -4.2370005 -4.2797842][-4.27418 -4.2674751 -4.2571788 -4.240871 -4.2117691 -4.1771255 -4.1427059 -4.1046219 -4.0828023 -4.0792823 -4.0916972 -4.1293077 -4.1933861 -4.2531438 -4.2906709][-4.28674 -4.2835445 -4.28036 -4.2732449 -4.2574716 -4.2394776 -4.2257342 -4.2104297 -4.2024779 -4.201117 -4.1975856 -4.2086282 -4.2430854 -4.2824 -4.307929][-4.3026571 -4.3000021 -4.2987742 -4.2962079 -4.2892303 -4.2815909 -4.277452 -4.2723889 -4.2727585 -4.2777758 -4.2751179 -4.2765117 -4.2916455 -4.3112731 -4.3248768][-4.3173752 -4.315587 -4.3144116 -4.31202 -4.3077326 -4.3050413 -4.3051014 -4.302824 -4.3050413 -4.3117175 -4.313077 -4.3130512 -4.3188758 -4.3285728 -4.3344345][-4.3273454 -4.3266439 -4.3256207 -4.3238339 -4.3211679 -4.320035 -4.3208814 -4.320209 -4.3213239 -4.3253636 -4.3266888 -4.326602 -4.3282671 -4.333324 -4.3362927]]...]
INFO - root - 2017-12-06 05:31:36.768550: step 910, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 60h:26m:21s remains)
INFO - root - 2017-12-06 05:31:43.249910: step 920, loss = 2.09, batch loss = 2.04 (12.5 examples/sec; 0.641 sec/batch; 59h:00m:54s remains)
INFO - root - 2017-12-06 05:31:49.592128: step 930, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 58h:21m:14s remains)
INFO - root - 2017-12-06 05:31:56.001111: step 940, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 57h:43m:52s remains)
INFO - root - 2017-12-06 05:32:02.277122: step 950, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.663 sec/batch; 61h:03m:56s remains)
INFO - root - 2017-12-06 05:32:08.770778: step 960, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 59h:35m:14s remains)
INFO - root - 2017-12-06 05:32:15.106768: step 970, loss = 2.04, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 57h:56m:13s remains)
INFO - root - 2017-12-06 05:32:21.475025: step 980, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 58h:25m:52s remains)
INFO - root - 2017-12-06 05:32:27.862868: step 990, loss = 2.03, batch loss = 1.97 (12.7 examples/sec; 0.629 sec/batch; 57h:54m:51s remains)
INFO - root - 2017-12-06 05:32:34.258034: step 1000, loss = 2.04, batch loss = 1.98 (12.9 examples/sec; 0.621 sec/batch; 57h:13m:36s remains)
2017-12-06 05:32:34.840408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2955642 -4.2856584 -4.2797322 -4.2843485 -4.298286 -4.3115244 -4.3188057 -4.3218017 -4.3187728 -4.3101487 -4.2986608 -4.293016 -4.2927809 -4.29965 -4.30989][-4.2609134 -4.2489867 -4.2384062 -4.2421155 -4.2616868 -4.2775245 -4.283813 -4.2846336 -4.2758312 -4.2606134 -4.2430749 -4.2371593 -4.2408729 -4.2545714 -4.27203][-4.2316713 -4.2170992 -4.2029233 -4.2037463 -4.2252374 -4.2419481 -4.2443514 -4.2430029 -4.2351356 -4.2194514 -4.1976638 -4.1908197 -4.1992922 -4.2181959 -4.2381687][-4.2223253 -4.20534 -4.1858225 -4.1773582 -4.1892471 -4.1960311 -4.1902108 -4.192946 -4.200182 -4.1959257 -4.1749787 -4.1683526 -4.1803689 -4.1971455 -4.212038][-4.2100711 -4.1892452 -4.1696014 -4.1531544 -4.144794 -4.1269722 -4.107388 -4.1226282 -4.1554337 -4.1698046 -4.15684 -4.1555467 -4.1712656 -4.1820731 -4.1880779][-4.1939297 -4.1698918 -4.1500068 -4.1244707 -4.0911956 -4.0395 -3.9973056 -4.030633 -4.09956 -4.1397266 -4.1397748 -4.1469455 -4.1728139 -4.1835613 -4.18489][-4.1898265 -4.1641188 -4.1403046 -4.1056447 -4.0482092 -3.9563348 -3.882916 -3.9356127 -4.0426779 -4.1046157 -4.1176243 -4.1340065 -4.1691113 -4.18885 -4.1977806][-4.1886969 -4.1665988 -4.1445622 -4.1069508 -4.0431767 -3.9387581 -3.8527858 -3.904011 -4.0104084 -4.0649943 -4.07586 -4.0923491 -4.1319432 -4.1652794 -4.1924367][-4.1790304 -4.1592045 -4.1361728 -4.1031361 -4.054925 -3.9797778 -3.91719 -3.9467337 -4.0132332 -4.0404067 -4.0418482 -4.054307 -4.090436 -4.1302452 -4.169961][-4.1604996 -4.1427507 -4.1204023 -4.0969243 -4.0709634 -4.0282259 -3.989347 -4.0033555 -4.0345025 -4.0379815 -4.0307817 -4.038271 -4.0709248 -4.111299 -4.1527462][-4.1573482 -4.1436973 -4.12856 -4.1158485 -4.1067996 -4.0924988 -4.0720215 -4.0782804 -4.0888228 -4.073616 -4.0525603 -4.0493417 -4.0754542 -4.114687 -4.155499][-4.1897821 -4.1754704 -4.165874 -4.1617002 -4.1637926 -4.1662455 -4.1612339 -4.1684637 -4.1710391 -4.1474581 -4.11662 -4.1061425 -4.1275558 -4.1611314 -4.1937532][-4.2452545 -4.228375 -4.2185755 -4.2184978 -4.2275872 -4.2390184 -4.2428231 -4.2497392 -4.2505016 -4.2278919 -4.1966586 -4.1841612 -4.1999598 -4.2258673 -4.2488465][-4.29262 -4.2796516 -4.2739387 -4.2773819 -4.2872262 -4.2983961 -4.303442 -4.308042 -4.3065834 -4.2892804 -4.2649622 -4.2535949 -4.2634325 -4.2796736 -4.292407][-4.323482 -4.3131294 -4.3112054 -4.3148794 -4.320715 -4.3280439 -4.332263 -4.3359871 -4.3367429 -4.3276124 -4.3119273 -4.3035665 -4.3095407 -4.3196583 -4.32554]]...]
INFO - root - 2017-12-06 05:32:41.263603: step 1010, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 59h:13m:13s remains)
INFO - root - 2017-12-06 05:32:47.814445: step 1020, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 61h:51m:54s remains)
INFO - root - 2017-12-06 05:32:54.208102: step 1030, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 58h:18m:41s remains)
INFO - root - 2017-12-06 05:33:00.644720: step 1040, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 59h:22m:05s remains)
INFO - root - 2017-12-06 05:33:07.051550: step 1050, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 0.526 sec/batch; 48h:25m:44s remains)
INFO - root - 2017-12-06 05:33:13.402443: step 1060, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 57h:10m:59s remains)
INFO - root - 2017-12-06 05:33:19.926977: step 1070, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 59h:51m:46s remains)
INFO - root - 2017-12-06 05:33:26.342659: step 1080, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 60h:23m:36s remains)
INFO - root - 2017-12-06 05:33:32.753634: step 1090, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 59h:56m:22s remains)
INFO - root - 2017-12-06 05:33:39.046967: step 1100, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.446 sec/batch; 41h:03m:33s remains)
2017-12-06 05:33:40.686838: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2257366 -4.2589579 -4.2671919 -4.2560968 -4.2420859 -4.2231164 -4.2147775 -4.2270265 -4.2471256 -4.2644486 -4.2738876 -4.2740984 -4.2752118 -4.2812095 -4.2902465][-4.2318263 -4.2596884 -4.267221 -4.2580338 -4.2497635 -4.2398844 -4.2313976 -4.2313924 -4.2382846 -4.2462811 -4.2520967 -4.2599669 -4.2703061 -4.2860241 -4.3011312][-4.2281823 -4.2562652 -4.266633 -4.25748 -4.2486324 -4.234169 -4.2113404 -4.1958623 -4.1987896 -4.2139454 -4.2255435 -4.2404685 -4.2561741 -4.2814312 -4.3037691][-4.21967 -4.2510824 -4.26566 -4.2523222 -4.234 -4.2035255 -4.15604 -4.1246543 -4.1331825 -4.1669774 -4.1897712 -4.2130308 -4.2360392 -4.2672086 -4.2910657][-4.2145419 -4.2442884 -4.2565336 -4.2371025 -4.2052131 -4.1497784 -4.0711546 -4.0303574 -4.0616174 -4.1229157 -4.1620164 -4.1890635 -4.2158933 -4.2467914 -4.2700648][-4.2178836 -4.2413011 -4.2471747 -4.2190661 -4.1695514 -4.0814071 -3.9676042 -3.9265831 -3.9965281 -4.0915079 -4.1451221 -4.1717877 -4.1963282 -4.220717 -4.2405324][-4.2212849 -4.238451 -4.2381306 -4.2004538 -4.1345797 -4.017292 -3.8814068 -3.8534243 -3.9619391 -4.0784373 -4.13311 -4.154079 -4.1727266 -4.188725 -4.2041283][-4.2126722 -4.2223697 -4.2147918 -4.1777487 -4.1119509 -4.0017452 -3.8872855 -3.8870039 -3.9972954 -4.0925226 -4.1302152 -4.1438303 -4.1561294 -4.1633124 -4.1682243][-4.1824689 -4.1826887 -4.172884 -4.1503472 -4.1109967 -4.0405197 -3.970273 -3.9864609 -4.0618768 -4.1153064 -4.1342163 -4.1435981 -4.151278 -4.1533184 -4.1483994][-4.1528139 -4.142889 -4.1347656 -4.1325612 -4.1272378 -4.0943837 -4.05295 -4.0664573 -4.1081119 -4.1338067 -4.1431527 -4.1501055 -4.1542606 -4.155653 -4.1485343][-4.1412873 -4.1273904 -4.122458 -4.1329074 -4.1495905 -4.1436739 -4.1197243 -4.1232057 -4.1417575 -4.1497564 -4.1536765 -4.1621666 -4.1680822 -4.171174 -4.1639018][-4.1666217 -4.1538591 -4.1497512 -4.1624908 -4.18998 -4.2002139 -4.18591 -4.1811194 -4.186131 -4.1894965 -4.1930938 -4.2036896 -4.2093782 -4.2122259 -4.2043962][-4.2210364 -4.211741 -4.2042303 -4.2118931 -4.2373915 -4.252008 -4.2410936 -4.2301755 -4.2318053 -4.2403579 -4.2510123 -4.2635841 -4.2635183 -4.2601104 -4.2501469][-4.2739773 -4.2755394 -4.269259 -4.2691116 -4.2824984 -4.2898331 -4.2816911 -4.271112 -4.2715 -4.2797594 -4.2935524 -4.3047318 -4.30243 -4.2945094 -4.2829566][-4.3064175 -4.3136053 -4.3085346 -4.3040752 -4.3066258 -4.3067503 -4.3026071 -4.2977633 -4.2985826 -4.3046036 -4.3145814 -4.3215513 -4.3167233 -4.3062367 -4.295567]]...]
INFO - root - 2017-12-06 05:33:47.180097: step 1110, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 60h:15m:22s remains)
INFO - root - 2017-12-06 05:33:53.860682: step 1120, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 59h:13m:23s remains)
INFO - root - 2017-12-06 05:34:00.244014: step 1130, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 58h:33m:21s remains)
INFO - root - 2017-12-06 05:34:06.601870: step 1140, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 58h:04m:56s remains)
INFO - root - 2017-12-06 05:34:12.902366: step 1150, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 59h:00m:51s remains)
INFO - root - 2017-12-06 05:34:19.525959: step 1160, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 61h:43m:00s remains)
INFO - root - 2017-12-06 05:34:25.946691: step 1170, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 58h:41m:17s remains)
INFO - root - 2017-12-06 05:34:32.388087: step 1180, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 60h:24m:53s remains)
INFO - root - 2017-12-06 05:34:38.858295: step 1190, loss = 2.05, batch loss = 1.99 (13.3 examples/sec; 0.601 sec/batch; 55h:17m:23s remains)
INFO - root - 2017-12-06 05:34:45.236635: step 1200, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 58h:06m:31s remains)
2017-12-06 05:34:45.876068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2540746 -4.2454085 -4.2476931 -4.2506466 -4.2611809 -4.2776604 -4.29092 -4.2989316 -4.2877913 -4.2628393 -4.2431846 -4.2458768 -4.2654247 -4.2857418 -4.30004][-4.2500815 -4.237021 -4.2300067 -4.2243495 -4.2312565 -4.2465305 -4.2622418 -4.2762885 -4.2681079 -4.2453327 -4.2291212 -4.2335472 -4.2534862 -4.270627 -4.283215][-4.2289991 -4.2140903 -4.1951318 -4.1751552 -4.1759825 -4.1936359 -4.21748 -4.2433443 -4.2481236 -4.2397833 -4.2332363 -4.2402287 -4.2540269 -4.260426 -4.2622857][-4.1928139 -4.1764855 -4.1441216 -4.1058226 -4.098453 -4.1202645 -4.1533351 -4.1885195 -4.2133813 -4.2327108 -4.243742 -4.2566586 -4.264564 -4.2598991 -4.2485003][-4.1568074 -4.14256 -4.0988612 -4.0389423 -4.0173664 -4.036716 -4.0637765 -4.0886292 -4.1323919 -4.1910772 -4.2302632 -4.2579889 -4.2712936 -4.26487 -4.2477026][-4.133688 -4.125689 -4.0732918 -3.99707 -3.9597797 -3.9600105 -3.950458 -3.9411628 -4.0000639 -4.1064286 -4.1843114 -4.2348733 -4.2624969 -4.2643476 -4.2518854][-4.1309075 -4.1362214 -4.09146 -4.0121584 -3.9518435 -3.9064343 -3.8288388 -3.7575793 -3.8277171 -3.988771 -4.1112342 -4.1857271 -4.2301636 -4.2478671 -4.24766][-4.1416006 -4.1675587 -4.1450477 -4.078301 -4.002409 -3.9098685 -3.7574468 -3.6085682 -3.6886888 -3.894969 -4.0435014 -4.1300654 -4.1874976 -4.218092 -4.2298484][-4.1543303 -4.2003341 -4.2086663 -4.1730814 -4.1113696 -4.0137033 -3.8433931 -3.6754508 -3.7364268 -3.90953 -4.0313158 -4.1027188 -4.1570311 -4.1914153 -4.2125597][-4.164444 -4.2191143 -4.2493005 -4.2469654 -4.2164392 -4.1547451 -4.0298285 -3.9064679 -3.92833 -4.0217428 -4.0844049 -4.1204529 -4.1587195 -4.1935487 -4.2167072][-4.1837268 -4.233367 -4.2666073 -4.2811947 -4.2749591 -4.2440648 -4.1702332 -4.0948324 -4.1026292 -4.147758 -4.1721005 -4.1836004 -4.2037086 -4.2272916 -4.2410331][-4.2134371 -4.2488985 -4.2699461 -4.2874746 -4.2931705 -4.2825284 -4.2419109 -4.1959238 -4.19866 -4.2253857 -4.2416759 -4.2468033 -4.2508559 -4.2549095 -4.2555237][-4.2265611 -4.2446136 -4.255208 -4.273572 -4.2860565 -4.2853866 -4.259963 -4.2251315 -4.2248025 -4.2429447 -4.2608242 -4.2659087 -4.2610979 -4.2525568 -4.2482147][-4.2213149 -4.2250395 -4.2290769 -4.2471538 -4.262289 -4.2656765 -4.2457967 -4.2155914 -4.2110229 -4.2170429 -4.2301135 -4.23359 -4.2190423 -4.2029033 -4.20475][-4.2174983 -4.2098756 -4.2051773 -4.2154222 -4.2312927 -4.2403684 -4.2267165 -4.202446 -4.1893082 -4.1747565 -4.1687794 -4.1636329 -4.1451716 -4.1330266 -4.1528678]]...]
INFO - root - 2017-12-06 05:34:52.270549: step 1210, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 59h:49m:01s remains)
INFO - root - 2017-12-06 05:34:58.785766: step 1220, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 62h:02m:48s remains)
INFO - root - 2017-12-06 05:35:05.254113: step 1230, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 60h:05m:50s remains)
INFO - root - 2017-12-06 05:35:11.828192: step 1240, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 62h:22m:58s remains)
INFO - root - 2017-12-06 05:35:18.323252: step 1250, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.651 sec/batch; 59h:55m:49s remains)
INFO - root - 2017-12-06 05:35:22.975986: step 1260, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.436 sec/batch; 40h:05m:03s remains)
INFO - root - 2017-12-06 05:35:29.102623: step 1270, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 59h:37m:46s remains)
INFO - root - 2017-12-06 05:35:35.613066: step 1280, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 60h:37m:56s remains)
INFO - root - 2017-12-06 05:35:42.279597: step 1290, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 59h:12m:32s remains)
INFO - root - 2017-12-06 05:35:48.497097: step 1300, loss = 2.08, batch loss = 2.03 (17.9 examples/sec; 0.447 sec/batch; 41h:05m:56s remains)
2017-12-06 05:35:51.779362: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2372785 -4.2525864 -4.262197 -4.2623677 -4.25169 -4.2261982 -4.1683016 -4.08674 -4.0544243 -4.0811381 -4.1172562 -4.1632681 -4.2189474 -4.2649865 -4.2905688][-4.2035656 -4.2196479 -4.2312369 -4.2330027 -4.2256794 -4.2072511 -4.15395 -4.0725546 -4.0375233 -4.0625405 -4.0974336 -4.142736 -4.2020521 -4.2551026 -4.2856274][-4.1634283 -4.1796474 -4.1936407 -4.1992869 -4.199398 -4.1912971 -4.1431894 -4.0609341 -4.0209394 -4.0421777 -4.0741606 -4.11958 -4.1832418 -4.24331 -4.2785869][-4.1195574 -4.1347375 -4.1525116 -4.1646323 -4.1759677 -4.1796455 -4.135963 -4.0532432 -4.0101638 -4.030262 -4.0611143 -4.1074252 -4.1736135 -4.2371306 -4.2741227][-4.0769606 -4.0899243 -4.1121359 -4.1316438 -4.1539197 -4.167037 -4.1244278 -4.0395246 -3.9979892 -4.0243535 -4.06023 -4.1097064 -4.1762686 -4.2388411 -4.2743688][-4.0343275 -4.0418196 -4.066196 -4.0889339 -4.1172376 -4.1364865 -4.0932369 -4.0058212 -3.9714804 -4.0119596 -4.0607457 -4.1187248 -4.1866994 -4.2462287 -4.2784181][-4.0131855 -4.0106921 -4.0286736 -4.0411377 -4.0611386 -4.0784135 -4.0312676 -3.9419818 -3.9234583 -3.985147 -4.0516486 -4.1233306 -4.195806 -4.253612 -4.2830377][-4.0322528 -4.0186949 -4.0228171 -4.0154128 -4.0153346 -4.0188527 -3.9610462 -3.8675823 -3.8676367 -3.9513841 -4.0332727 -4.1175675 -4.1968408 -4.2562051 -4.2853208][-4.0770721 -4.0590453 -4.0518022 -4.0304971 -4.0152068 -4.0020819 -3.9326651 -3.8385582 -3.8521261 -3.9447548 -4.0275083 -4.1129055 -4.1950793 -4.2555275 -4.2850952][-4.1329288 -4.1141963 -4.102036 -4.0774288 -4.0576863 -4.034164 -3.9651084 -3.8825464 -3.9034395 -3.9862852 -4.0527306 -4.1245761 -4.1985054 -4.2545838 -4.2834425][-4.179616 -4.1610918 -4.1484647 -4.1293955 -4.1145773 -4.0883431 -4.0287275 -3.9617362 -3.9836915 -4.0514407 -4.098176 -4.1507106 -4.2095504 -4.2570524 -4.282661][-4.214417 -4.1962337 -4.1840172 -4.1725097 -4.1630507 -4.1395612 -4.09369 -4.0399895 -4.0572681 -4.1102104 -4.1440339 -4.1819453 -4.227212 -4.2648211 -4.285357][-4.2454548 -4.23002 -4.2190447 -4.2127528 -4.2070222 -4.1863871 -4.1527438 -4.1088171 -4.1188788 -4.1586409 -4.1841083 -4.2117705 -4.2471185 -4.2764573 -4.29164][-4.2734861 -4.2630396 -4.2547808 -4.2519431 -4.2485995 -4.231956 -4.2078176 -4.1736727 -4.17848 -4.2055159 -4.2229462 -4.2430367 -4.270216 -4.2921305 -4.3017821][-4.2953525 -4.2887149 -4.2840185 -4.2833138 -4.2820158 -4.2722411 -4.2574916 -4.2325587 -4.2343016 -4.2507868 -4.2616844 -4.2762966 -4.2963152 -4.3105831 -4.3145914]]...]
INFO - root - 2017-12-06 05:35:58.317723: step 1310, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 59h:32m:32s remains)
INFO - root - 2017-12-06 05:36:04.763789: step 1320, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 60h:00m:37s remains)
INFO - root - 2017-12-06 05:36:11.243743: step 1330, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 58h:22m:22s remains)
INFO - root - 2017-12-06 05:36:17.741774: step 1340, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 60h:21m:27s remains)
INFO - root - 2017-12-06 05:36:24.160504: step 1350, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 58h:41m:46s remains)
INFO - root - 2017-12-06 05:36:30.458296: step 1360, loss = 2.05, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 57h:52m:27s remains)
INFO - root - 2017-12-06 05:36:36.970632: step 1370, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 58h:04m:21s remains)
INFO - root - 2017-12-06 05:36:43.402002: step 1380, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.653 sec/batch; 60h:00m:57s remains)
INFO - root - 2017-12-06 05:36:49.968638: step 1390, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 60h:23m:51s remains)
INFO - root - 2017-12-06 05:36:56.434021: step 1400, loss = 2.05, batch loss = 1.99 (13.6 examples/sec; 0.589 sec/batch; 54h:07m:46s remains)
2017-12-06 05:36:57.430184: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1975408 -4.1948123 -4.1875291 -4.1803212 -4.1789622 -4.181344 -4.1819277 -4.1735668 -4.1601005 -4.1535883 -4.1533208 -4.1585541 -4.1667223 -4.1796012 -4.1980381][-4.1939793 -4.1979122 -4.1959767 -4.1881719 -4.1830177 -4.1818051 -4.1803455 -4.1718659 -4.1605544 -4.1558981 -4.1555562 -4.1572762 -4.1606507 -4.1722159 -4.1925116][-4.194315 -4.203475 -4.2072411 -4.2014527 -4.195446 -4.1924911 -4.1875033 -4.1775508 -4.1681576 -4.1658611 -4.1686358 -4.16922 -4.1692691 -4.1762381 -4.1921811][-4.1939287 -4.2043509 -4.2100158 -4.2060208 -4.1994271 -4.1957631 -4.1898685 -4.1800804 -4.1718636 -4.173038 -4.1799183 -4.1822472 -4.1824923 -4.185791 -4.1975069][-4.1852078 -4.19128 -4.1950855 -4.1915565 -4.1838217 -4.1790361 -4.1743727 -4.1657639 -4.1596041 -4.165206 -4.1786051 -4.1862803 -4.1878152 -4.1897569 -4.1980519][-4.1624279 -4.1597338 -4.1588321 -4.15541 -4.1459618 -4.1351867 -4.1284156 -4.12204 -4.1210151 -4.1315856 -4.1528635 -4.1705618 -4.1782103 -4.180512 -4.18642][-4.1371055 -4.1223817 -4.1131639 -4.1042576 -4.0878191 -4.0672712 -4.0582986 -4.0588565 -4.072392 -4.0950708 -4.123847 -4.1488876 -4.1616983 -4.1642671 -4.1658368][-4.1390781 -4.1073866 -4.0825324 -4.0580988 -4.024116 -3.9887316 -3.979064 -3.9911339 -4.026886 -4.0699906 -4.1077547 -4.138639 -4.1562095 -4.1589665 -4.1549239][-4.1729603 -4.1281152 -4.088378 -4.0492759 -3.999 -3.9477901 -3.9349973 -3.9537251 -4.0066872 -4.0686216 -4.1191063 -4.1595583 -4.1840448 -4.189086 -4.1815281][-4.2124443 -4.1691818 -4.13113 -4.0928335 -4.0428581 -3.9912925 -3.9764631 -3.9928293 -4.0423574 -4.1007915 -4.1486773 -4.1867161 -4.2124186 -4.221498 -4.2164407][-4.2429771 -4.2070885 -4.1770959 -4.1463575 -4.1097803 -4.0733905 -4.06561 -4.0802965 -4.1145062 -4.1500359 -4.1785655 -4.1972141 -4.2094259 -4.2151432 -4.2140436][-4.2634974 -4.2357092 -4.2121253 -4.1868992 -4.1613607 -4.1379113 -4.1322289 -4.1392708 -4.1576972 -4.174068 -4.1854482 -4.1873951 -4.1854825 -4.1872525 -4.1904554][-4.2705836 -4.2535973 -4.2359605 -4.21621 -4.1949615 -4.1720662 -4.1560888 -4.1492786 -4.1533132 -4.1582966 -4.1633983 -4.1653013 -4.1652308 -4.1714945 -4.1830568][-4.2659826 -4.264658 -4.2554016 -4.2406268 -4.2221613 -4.1966467 -4.1686144 -4.1465945 -4.1393414 -4.1403642 -4.1488886 -4.1618257 -4.1742816 -4.1927624 -4.2115235][-4.2447019 -4.256135 -4.2576141 -4.2504697 -4.2386627 -4.2161732 -4.1874094 -4.1615138 -4.148356 -4.1463985 -4.1560893 -4.176383 -4.1998792 -4.2262721 -4.2480531]]...]
INFO - root - 2017-12-06 05:37:03.988976: step 1410, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 62h:21m:03s remains)
INFO - root - 2017-12-06 05:37:10.488646: step 1420, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 59h:08m:19s remains)
INFO - root - 2017-12-06 05:37:16.902574: step 1430, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 60h:46m:09s remains)
INFO - root - 2017-12-06 05:37:23.390436: step 1440, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 58h:55m:20s remains)
INFO - root - 2017-12-06 05:37:29.832142: step 1450, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 59h:37m:01s remains)
INFO - root - 2017-12-06 05:37:36.088671: step 1460, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 59h:13m:20s remains)
INFO - root - 2017-12-06 05:37:42.548639: step 1470, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 59h:32m:37s remains)
INFO - root - 2017-12-06 05:37:48.958929: step 1480, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 59h:03m:45s remains)
INFO - root - 2017-12-06 05:37:55.410221: step 1490, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 59h:30m:41s remains)
INFO - root - 2017-12-06 05:38:01.759261: step 1500, loss = 2.09, batch loss = 2.03 (13.8 examples/sec; 0.581 sec/batch; 53h:27m:01s remains)
2017-12-06 05:38:02.376271: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3014607 -4.3264093 -4.3425126 -4.3447146 -4.3397589 -4.3303471 -4.3175082 -4.3032722 -4.2895012 -4.2765365 -4.2649302 -4.2600889 -4.2579803 -4.2625575 -4.2753778][-4.2952366 -4.3215494 -4.3372688 -4.3373957 -4.3282661 -4.3124313 -4.2913094 -4.2706833 -4.2538309 -4.2398052 -4.2283788 -4.225606 -4.2275448 -4.2333927 -4.2444429][-4.2952251 -4.32053 -4.3339281 -4.3292055 -4.3119779 -4.2845755 -4.2516317 -4.2230573 -4.2035823 -4.1933141 -4.1903458 -4.1958861 -4.2046151 -4.2099056 -4.2158513][-4.2960563 -4.3159866 -4.3238511 -4.3111935 -4.2815566 -4.2401276 -4.1973157 -4.1616106 -4.1405549 -4.13732 -4.1425204 -4.1618004 -4.1771951 -4.1814747 -4.1860051][-4.2896948 -4.300118 -4.3006907 -4.2773018 -4.2333474 -4.1790729 -4.1312623 -4.094779 -4.0762787 -4.0796165 -4.091053 -4.1236444 -4.1431227 -4.1472459 -4.1529703][-4.2787094 -4.2810264 -4.2740335 -4.2377524 -4.1787868 -4.1113243 -4.0604191 -4.0243549 -4.0041862 -4.006072 -4.0211258 -4.0647855 -4.0879974 -4.0980477 -4.1126132][-4.253962 -4.247242 -4.2346072 -4.1898232 -4.1175137 -4.0383315 -3.9846835 -3.9469166 -3.9244194 -3.9281881 -3.9492457 -4.0006742 -4.0351796 -4.0649529 -4.0991507][-4.2180257 -4.2036166 -4.1867976 -4.1401753 -4.0572371 -3.9635072 -3.9034007 -3.8678002 -3.8584733 -3.8701251 -3.9063766 -3.9766955 -4.0341191 -4.0848241 -4.1360288][-4.170752 -4.1537995 -4.13816 -4.1027808 -4.0285244 -3.9319592 -3.8704393 -3.8416326 -3.844254 -3.8625469 -3.9136062 -4.0010319 -4.0826387 -4.1483479 -4.2026005][-4.1362824 -4.1222506 -4.1124487 -4.0961871 -4.0458894 -3.9717298 -3.9230466 -3.9035959 -3.9091024 -3.9256468 -3.9764464 -4.06482 -4.1519046 -4.2118568 -4.2529888][-4.1170211 -4.1080451 -4.1052256 -4.10908 -4.0889082 -4.0516019 -4.0227079 -4.0147414 -4.0201035 -4.0313888 -4.0675788 -4.1393156 -4.214231 -4.2588105 -4.2795925][-4.0955648 -4.0916195 -4.0955377 -4.1139574 -4.1198626 -4.1138439 -4.1039362 -4.1047235 -4.1088128 -4.1191235 -4.1470313 -4.2035375 -4.2611365 -4.2865357 -4.2865171][-4.081316 -4.0932484 -4.1054745 -4.123261 -4.1351361 -4.1429696 -4.146121 -4.1547093 -4.1647453 -4.182466 -4.2068353 -4.2473192 -4.2813916 -4.2875009 -4.2704263][-4.078887 -4.1043839 -4.1252761 -4.1416297 -4.1560445 -4.17169 -4.185554 -4.196877 -4.2078733 -4.2278805 -4.245266 -4.2675724 -4.2799263 -4.2685227 -4.2388711][-4.0710745 -4.1035814 -4.1330047 -4.1566024 -4.1826849 -4.2060795 -4.2297893 -4.2425575 -4.2488484 -4.2582574 -4.2622638 -4.2680116 -4.2654552 -4.2409711 -4.1962295]]...]
INFO - root - 2017-12-06 05:38:08.885497: step 1510, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 58h:57m:53s remains)
INFO - root - 2017-12-06 05:38:15.502491: step 1520, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 59h:14m:02s remains)
INFO - root - 2017-12-06 05:38:21.885203: step 1530, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 58h:14m:08s remains)
INFO - root - 2017-12-06 05:38:28.361664: step 1540, loss = 2.07, batch loss = 2.01 (13.4 examples/sec; 0.598 sec/batch; 54h:57m:04s remains)
INFO - root - 2017-12-06 05:38:34.742825: step 1550, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 60h:25m:35s remains)
INFO - root - 2017-12-06 05:38:40.986711: step 1560, loss = 2.06, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 57h:25m:51s remains)
INFO - root - 2017-12-06 05:38:47.352427: step 1570, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 60h:55m:36s remains)
INFO - root - 2017-12-06 05:38:53.816368: step 1580, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 59h:29m:06s remains)
INFO - root - 2017-12-06 05:39:00.278112: step 1590, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 59h:02m:49s remains)
INFO - root - 2017-12-06 05:39:06.589777: step 1600, loss = 2.07, batch loss = 2.01 (14.2 examples/sec; 0.563 sec/batch; 51h:42m:28s remains)
2017-12-06 05:39:07.271580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3383374 -4.339489 -4.3381371 -4.338728 -4.3346767 -4.3274822 -4.3182607 -4.312191 -4.3154607 -4.3220391 -4.3292265 -4.3382006 -4.3480983 -4.3538737 -4.3562284][-4.324687 -4.3246083 -4.3249259 -4.329123 -4.3252511 -4.3169589 -4.3069844 -4.300663 -4.3065262 -4.3164577 -4.3260493 -4.3367391 -4.3441315 -4.34408 -4.340003][-4.306529 -4.3022079 -4.302599 -4.3085895 -4.3038111 -4.2936449 -4.2831979 -4.2769065 -4.2852249 -4.2999282 -4.3129754 -4.3246169 -4.3302517 -4.3237848 -4.3125482][-4.287641 -4.2791796 -4.2779217 -4.2805171 -4.2713871 -4.257266 -4.2445445 -4.2394447 -4.2557058 -4.2795658 -4.3009014 -4.3168826 -4.3212776 -4.31131 -4.294744][-4.27135 -4.2577515 -4.2505636 -4.2455029 -4.22829 -4.2041245 -4.1814637 -4.1719208 -4.1943445 -4.2334509 -4.2699127 -4.2958407 -4.3048549 -4.2993026 -4.2859931][-4.2601695 -4.2397966 -4.2202506 -4.2022295 -4.1740947 -4.1372628 -4.0984769 -4.0749736 -4.1036773 -4.1649227 -4.2228141 -4.2634082 -4.2817345 -4.2856355 -4.2809782][-4.2612162 -4.2359438 -4.2048697 -4.1711669 -4.1225309 -4.0587349 -3.9849277 -3.9306445 -3.9686489 -4.0622044 -4.1490517 -4.2128692 -4.251298 -4.2692142 -4.27463][-4.2689061 -4.2431288 -4.2067242 -4.1648283 -4.1023436 -4.0166154 -3.904016 -3.8103843 -3.852746 -3.9683731 -4.0750484 -4.1614137 -4.2186165 -4.2498503 -4.2653728][-4.2784538 -4.263339 -4.2409234 -4.2189021 -4.1777997 -4.1121774 -4.0160365 -3.9327257 -3.9567046 -4.0364881 -4.1157866 -4.1860118 -4.2340455 -4.2604513 -4.2724786][-4.2826824 -4.2774129 -4.2697058 -4.2666659 -4.2441745 -4.1936727 -4.11887 -4.0572248 -4.0669713 -4.1106739 -4.1569676 -4.204977 -4.2408147 -4.2608738 -4.2705388][-4.2667265 -4.2610626 -4.2583337 -4.263546 -4.2503104 -4.208468 -4.1530647 -4.1112852 -4.1203976 -4.1504869 -4.1786437 -4.2108049 -4.2370996 -4.2541571 -4.2632642][-4.2488523 -4.237772 -4.2350059 -4.2435493 -4.2355165 -4.2024236 -4.1664114 -4.1431503 -4.1551332 -4.1831217 -4.2042236 -4.2257295 -4.2450886 -4.2595606 -4.2681079][-4.2365994 -4.2179937 -4.210464 -4.213623 -4.2052422 -4.17907 -4.1568923 -4.1433411 -4.1594014 -4.1876173 -4.2054119 -4.2249904 -4.2455249 -4.2617364 -4.2703633][-4.2287931 -4.2006154 -4.1866684 -4.1823273 -4.1728349 -4.1549454 -4.1431432 -4.1362352 -4.155695 -4.1856537 -4.2060843 -4.2284789 -4.2511458 -4.2681222 -4.2749987][-4.2401052 -4.2086143 -4.1911206 -4.1833806 -4.1746221 -4.1629272 -4.1557178 -4.1527033 -4.17061 -4.1961079 -4.2155066 -4.2379808 -4.261538 -4.2789769 -4.2851076]]...]
INFO - root - 2017-12-06 05:39:13.731965: step 1610, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 58h:28m:36s remains)
INFO - root - 2017-12-06 05:39:20.205018: step 1620, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.653 sec/batch; 60h:01m:35s remains)
INFO - root - 2017-12-06 05:39:26.699954: step 1630, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 59h:39m:23s remains)
INFO - root - 2017-12-06 05:39:33.169295: step 1640, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 57h:55m:51s remains)
INFO - root - 2017-12-06 05:39:39.620949: step 1650, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.653 sec/batch; 60h:03m:24s remains)
INFO - root - 2017-12-06 05:39:45.841690: step 1660, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 0.534 sec/batch; 49h:04m:25s remains)
INFO - root - 2017-12-06 05:39:52.253760: step 1670, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 59h:11m:05s remains)
INFO - root - 2017-12-06 05:39:58.690165: step 1680, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 57h:16m:45s remains)
INFO - root - 2017-12-06 05:40:05.140908: step 1690, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 59h:37m:38s remains)
INFO - root - 2017-12-06 05:40:11.532683: step 1700, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 61h:13m:41s remains)
2017-12-06 05:40:12.194797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1846318 -4.1733408 -4.1581273 -4.1378112 -4.1239667 -4.124989 -4.1308041 -4.1396394 -4.1583781 -4.1778502 -4.1892343 -4.1850166 -4.1756845 -4.1646767 -4.1581764][-4.2144866 -4.2102013 -4.1951375 -4.1690526 -4.1525722 -4.1569877 -4.1672826 -4.1783638 -4.2002664 -4.22312 -4.2319565 -4.2199645 -4.2029538 -4.1891022 -4.1863575][-4.2569108 -4.2562008 -4.2363677 -4.2057452 -4.1856093 -4.1839886 -4.1870394 -4.1966858 -4.2184796 -4.2421584 -4.25409 -4.2447667 -4.2277317 -4.2141232 -4.2149749][-4.2774434 -4.276763 -4.2554092 -4.2251396 -4.2043366 -4.1920233 -4.1837792 -4.1896658 -4.2125845 -4.2380743 -4.2544169 -4.2514181 -4.2359385 -4.21986 -4.2170172][-4.2751064 -4.272356 -4.2485619 -4.2218308 -4.1998024 -4.1752052 -4.1506782 -4.1493378 -4.1763635 -4.2098465 -4.2348409 -4.2397976 -4.2300639 -4.2172742 -4.2141805][-4.2710638 -4.2644958 -4.2380896 -4.2072411 -4.17262 -4.1223674 -4.06909 -4.0578866 -4.1024995 -4.1574836 -4.2023664 -4.2224245 -4.227788 -4.22738 -4.227169][-4.2725883 -4.26382 -4.2312241 -4.1879144 -4.1267834 -4.0327873 -3.927618 -3.9005051 -3.9797573 -4.0755558 -4.15021 -4.192718 -4.2164946 -4.2324133 -4.2432728][-4.2598157 -4.2486382 -4.2131562 -4.1652389 -4.0944924 -3.9732096 -3.8229847 -3.7718263 -3.8748279 -4.0021935 -4.0966063 -4.1564732 -4.1976738 -4.2271461 -4.244278][-4.22593 -4.2216306 -4.2010484 -4.1684079 -4.1132298 -4.014421 -3.8893812 -3.8397489 -3.9070253 -4.0115576 -4.0968556 -4.1565351 -4.1972947 -4.2261863 -4.2413588][-4.191412 -4.1962905 -4.1951866 -4.1832957 -4.151722 -4.0937772 -4.024435 -3.9967954 -4.0256791 -4.0878944 -4.1480045 -4.1883826 -4.2142262 -4.2292562 -4.2348828][-4.1839843 -4.1836853 -4.185813 -4.1858206 -4.1750441 -4.1461372 -4.115478 -4.1066113 -4.12322 -4.1565003 -4.193501 -4.2162876 -4.2287774 -4.2328038 -4.230835][-4.2023396 -4.1924925 -4.1837835 -4.1841369 -4.1828489 -4.1682091 -4.1562338 -4.1575069 -4.1662354 -4.1818538 -4.2057519 -4.2245007 -4.2304773 -4.2255793 -4.2165837][-4.2244368 -4.2153449 -4.199564 -4.1957269 -4.1955166 -4.182138 -4.1751552 -4.1793122 -4.1821141 -4.1875758 -4.2039127 -4.2229495 -4.2251225 -4.2122893 -4.1919489][-4.2315116 -4.2314515 -4.223074 -4.2190347 -4.2151051 -4.201561 -4.1959252 -4.1972561 -4.1962891 -4.1954465 -4.2028823 -4.2133451 -4.21386 -4.2020559 -4.1805573][-4.2292519 -4.2370071 -4.2387414 -4.2398281 -4.2365208 -4.2252235 -4.2194161 -4.2191205 -4.2158132 -4.2105913 -4.2073393 -4.2088504 -4.2117267 -4.2117076 -4.1991062]]...]
INFO - root - 2017-12-06 05:40:18.866709: step 1710, loss = 2.11, batch loss = 2.05 (11.9 examples/sec; 0.670 sec/batch; 61h:31m:26s remains)
INFO - root - 2017-12-06 05:40:27.108680: step 1720, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 76h:17m:31s remains)
INFO - root - 2017-12-06 05:40:35.786850: step 1730, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 77h:58m:30s remains)
INFO - root - 2017-12-06 05:40:44.403685: step 1740, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 80h:32m:40s remains)
INFO - root - 2017-12-06 05:40:52.916234: step 1750, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.814 sec/batch; 74h:48m:51s remains)
INFO - root - 2017-12-06 05:41:01.447174: step 1760, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 79h:40m:06s remains)
INFO - root - 2017-12-06 05:41:10.131386: step 1770, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 80h:04m:38s remains)
INFO - root - 2017-12-06 05:41:18.933060: step 1780, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.867 sec/batch; 79h:40m:43s remains)
INFO - root - 2017-12-06 05:41:27.515678: step 1790, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 80h:49m:23s remains)
INFO - root - 2017-12-06 05:41:36.112277: step 1800, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 80h:00m:04s remains)
2017-12-06 05:41:36.977001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.157793 -4.1450753 -4.1446781 -4.1481657 -4.1557374 -4.171989 -4.1881237 -4.2077646 -4.2253976 -4.2292266 -4.2242031 -4.2176576 -4.2159405 -4.213253 -4.2041707][-4.1103697 -4.0928035 -4.0944738 -4.1004381 -4.1173291 -4.1402588 -4.162044 -4.1893358 -4.2104778 -4.2124095 -4.2023511 -4.1920223 -4.1897664 -4.1895247 -4.1840358][-4.0876541 -4.0602474 -4.0547886 -4.0573869 -4.0787625 -4.1080041 -4.1414943 -4.1787772 -4.2012992 -4.1984053 -4.1838155 -4.1717949 -4.1723351 -4.1782117 -4.1796832][-4.1201348 -4.0940418 -4.0725393 -4.0589933 -4.0682945 -4.0948358 -4.1408777 -4.1888318 -4.2123966 -4.2008886 -4.1810369 -4.164896 -4.1635857 -4.1699514 -4.1779346][-4.1672392 -4.1476641 -4.1172123 -4.0818467 -4.0621614 -4.0721288 -4.1208773 -4.1825323 -4.2145057 -4.20414 -4.1806145 -4.1604605 -4.149384 -4.1446481 -4.1500254][-4.1928735 -4.1884408 -4.16335 -4.111773 -4.0569549 -4.0354061 -4.075901 -4.1459184 -4.1977887 -4.2033467 -4.1804595 -4.1545124 -4.1261683 -4.0933728 -4.0849991][-4.1971564 -4.202086 -4.190465 -4.1397905 -4.0617704 -4.0006814 -4.0141211 -4.0911956 -4.1694355 -4.1941471 -4.171319 -4.1401896 -4.0933638 -4.0238819 -3.9931333][-4.1786613 -4.1909852 -4.1942668 -4.1518927 -4.0537667 -3.944068 -3.922729 -4.0102725 -4.1140909 -4.1578283 -4.146378 -4.11752 -4.0627451 -3.9666905 -3.9108958][-4.141995 -4.1580715 -4.1678605 -4.136497 -4.0310192 -3.8807435 -3.82347 -3.9217658 -4.0519466 -4.1137433 -4.1213098 -4.1046381 -4.0604048 -3.9666924 -3.9021587][-4.1033211 -4.1242118 -4.1335554 -4.1117134 -4.0362711 -3.9047961 -3.8278635 -3.9010067 -4.0254321 -4.09313 -4.1124187 -4.1105137 -4.0896034 -4.0282521 -3.9772332][-4.0771446 -4.1048708 -4.1195889 -4.1108131 -4.080688 -4.015296 -3.9579587 -3.9829409 -4.0612559 -4.1083612 -4.1256595 -4.1346793 -4.1403403 -4.1118326 -4.0760341][-4.0940752 -4.1222486 -4.1420555 -4.1411033 -4.1418262 -4.1313486 -4.1095462 -4.1106548 -4.1405153 -4.1614647 -4.1721768 -4.1784081 -4.1903977 -4.179358 -4.15175][-4.156599 -4.1700811 -4.1834116 -4.1822309 -4.1922522 -4.2048888 -4.2055011 -4.2041054 -4.2114859 -4.2207336 -4.227952 -4.2272363 -4.2333364 -4.2295132 -4.2069139][-4.2216926 -4.2205305 -4.2275033 -4.227767 -4.238225 -4.2529864 -4.2591534 -4.258471 -4.25972 -4.2661557 -4.2719722 -4.2697563 -4.2709956 -4.267817 -4.2498536][-4.2826161 -4.2761846 -4.2759581 -4.2743511 -4.2826285 -4.2942305 -4.3012619 -4.3021827 -4.3023963 -4.3082719 -4.31565 -4.3146935 -4.314466 -4.3112283 -4.2972636]]...]
INFO - root - 2017-12-06 05:41:45.684772: step 1810, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 81h:18m:56s remains)
INFO - root - 2017-12-06 05:41:54.245186: step 1820, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 80h:09m:45s remains)
INFO - root - 2017-12-06 05:42:02.878336: step 1830, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.862 sec/batch; 79h:11m:50s remains)
INFO - root - 2017-12-06 05:42:11.515215: step 1840, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 81h:38m:43s remains)
INFO - root - 2017-12-06 05:42:20.219622: step 1850, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 79h:27m:02s remains)
INFO - root - 2017-12-06 05:42:29.137701: step 1860, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 80h:35m:12s remains)
INFO - root - 2017-12-06 05:42:37.590300: step 1870, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.828 sec/batch; 76h:02m:21s remains)
INFO - root - 2017-12-06 05:42:46.209715: step 1880, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 81h:25m:29s remains)
INFO - root - 2017-12-06 05:42:54.893433: step 1890, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 80h:40m:29s remains)
INFO - root - 2017-12-06 05:43:02.528108: step 1900, loss = 2.06, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 56h:58m:38s remains)
2017-12-06 05:43:03.870659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2950969 -4.2912326 -4.2913356 -4.2932339 -4.2938313 -4.2943878 -4.302299 -4.3164191 -4.3268309 -4.3297014 -4.3270874 -4.3199115 -4.3142376 -4.3117843 -4.3119726][-4.2769475 -4.2679038 -4.2668362 -4.2705636 -4.2714272 -4.2703872 -4.2782207 -4.29418 -4.306684 -4.3105125 -4.3111777 -4.31012 -4.30707 -4.3049374 -4.3052373][-4.2710052 -4.2559705 -4.2505832 -4.2526 -4.2498822 -4.2436342 -4.2456236 -4.256094 -4.2680449 -4.2726493 -4.278739 -4.2838507 -4.2843652 -4.2796059 -4.2766132][-4.2487597 -4.2296314 -4.2199054 -4.2171183 -4.2094169 -4.1995683 -4.1937342 -4.1982069 -4.2114162 -4.2186046 -4.2277765 -4.2401814 -4.2481737 -4.2436776 -4.23708][-4.2091889 -4.1854758 -4.1734238 -4.1673474 -4.1596465 -4.1541433 -4.14717 -4.1480269 -4.159514 -4.1678185 -4.1772323 -4.1921592 -4.2072453 -4.2031641 -4.1908436][-4.1797948 -4.1566663 -4.1451406 -4.1372013 -4.1312885 -4.1365628 -4.1348972 -4.1352692 -4.1420617 -4.1513953 -4.1584263 -4.1648984 -4.17818 -4.1736307 -4.1561642][-4.1626654 -4.1442394 -4.1347771 -4.1241226 -4.1178231 -4.1317015 -4.1412945 -4.1466379 -4.1549516 -4.1667886 -4.173955 -4.1680765 -4.1683636 -4.1587734 -4.1418314][-4.1503434 -4.1371779 -4.1273346 -4.1100039 -4.0948362 -4.1113362 -4.1359329 -4.1527972 -4.1720252 -4.1954865 -4.2064085 -4.1932569 -4.1817694 -4.1684723 -4.154933][-4.1421843 -4.1360941 -4.1178803 -4.0868878 -4.062758 -4.074965 -4.1083231 -4.13608 -4.1709857 -4.2111893 -4.2342787 -4.2293687 -4.2157788 -4.2020164 -4.1887293][-4.1527796 -4.1494703 -4.1149874 -4.0651989 -4.0342875 -4.0468168 -4.0841179 -4.119566 -4.1595364 -4.2069821 -4.2442818 -4.2567849 -4.2532039 -4.2449636 -4.2328944][-4.1873403 -4.1710858 -4.12066 -4.0581436 -4.0208898 -4.0296674 -4.0687957 -4.1097016 -4.1502366 -4.198391 -4.2429914 -4.2658877 -4.2767081 -4.2799649 -4.2743807][-4.2479897 -4.220109 -4.16039 -4.0890036 -4.0360513 -4.0269804 -4.0624065 -4.1058011 -4.1450076 -4.1879878 -4.22932 -4.2571707 -4.2786489 -4.2912602 -4.2929854][-4.3057995 -4.2733889 -4.21538 -4.1457486 -4.0809131 -4.0584173 -4.0866394 -4.1262717 -4.1575809 -4.187047 -4.2173519 -4.2428665 -4.2695861 -4.2893648 -4.2967567][-4.34106 -4.3159351 -4.2731056 -4.2198768 -4.1579814 -4.1259708 -4.1352539 -4.1562757 -4.1703796 -4.18824 -4.2119594 -4.2317271 -4.2579684 -4.2834454 -4.2937455][-4.3518257 -4.3410468 -4.3184595 -4.2873011 -4.2451363 -4.2170382 -4.2082124 -4.2039008 -4.1995764 -4.2098746 -4.2276754 -4.2396488 -4.2588854 -4.2787166 -4.2861428]]...]
INFO - root - 2017-12-06 05:43:12.656851: step 1910, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 81h:06m:39s remains)
INFO - root - 2017-12-06 05:43:20.630734: step 1920, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 79h:10m:50s remains)
INFO - root - 2017-12-06 05:43:29.374264: step 1930, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 80h:47m:43s remains)
INFO - root - 2017-12-06 05:43:38.085159: step 1940, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 78h:50m:58s remains)
INFO - root - 2017-12-06 05:43:46.701486: step 1950, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 80h:11m:33s remains)
INFO - root - 2017-12-06 05:43:55.499382: step 1960, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 81h:26m:42s remains)
INFO - root - 2017-12-06 05:44:04.072426: step 1970, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 0.784 sec/batch; 71h:56m:52s remains)
INFO - root - 2017-12-06 05:44:12.731576: step 1980, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 81h:50m:09s remains)
INFO - root - 2017-12-06 05:44:21.405817: step 1990, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 79h:12m:20s remains)
INFO - root - 2017-12-06 05:44:29.941668: step 2000, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 81h:14m:39s remains)
2017-12-06 05:44:30.775490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3578496 -4.3574548 -4.3550563 -4.3548732 -4.3563304 -4.3566756 -4.3589683 -4.3626051 -4.3632894 -4.3612452 -4.3572974 -4.35646 -4.3574262 -4.3578911 -4.3596067][-4.3507137 -4.346652 -4.3407888 -4.3379869 -4.3366046 -4.3353786 -4.3385735 -4.3411307 -4.3361597 -4.3267255 -4.3172588 -4.3132739 -4.3162932 -4.3230672 -4.33453][-4.3426824 -4.3323731 -4.3210721 -4.3132439 -4.3057127 -4.3002567 -4.3021917 -4.3019533 -4.2887397 -4.27019 -4.2553868 -4.2481518 -4.2539039 -4.2707896 -4.2966909][-4.3363767 -4.320066 -4.3019772 -4.2869558 -4.2722836 -4.2607756 -4.2597756 -4.2540817 -4.2304835 -4.2017775 -4.1761084 -4.1626124 -4.17247 -4.2024431 -4.2450013][-4.3283553 -4.3053308 -4.2805467 -4.2564635 -4.2345457 -4.220952 -4.2189083 -4.2071958 -4.1700268 -4.1277776 -4.0905676 -4.072382 -4.0891318 -4.1334972 -4.1927047][-4.3052726 -4.2720184 -4.2418947 -4.2126951 -4.1879406 -4.1739488 -4.1715503 -4.1522841 -4.1015196 -4.0484567 -4.0076761 -3.994226 -4.0249505 -4.0842423 -4.1578164][-4.2693391 -4.2290025 -4.1945262 -4.1580687 -4.13111 -4.1186824 -4.11605 -4.09237 -4.0312505 -3.9807825 -3.9596639 -3.9667702 -4.0112491 -4.0812273 -4.1591363][-4.2249818 -4.17339 -4.1238155 -4.0668693 -4.0317392 -4.0227304 -4.0321951 -4.0145483 -3.9589071 -3.936116 -3.9513063 -3.9823785 -4.0344558 -4.1068654 -4.1821709][-4.1769466 -4.1082716 -4.0352693 -3.9455831 -3.9043188 -3.9151838 -3.9560022 -3.9642942 -3.9397972 -3.9506474 -3.9907753 -4.0335417 -4.0862875 -4.1511741 -4.214941][-4.1429453 -4.0609241 -3.9692454 -3.8582971 -3.8268485 -3.8717206 -3.9501407 -3.9937024 -4.0030689 -4.0312557 -4.072813 -4.1133285 -4.1575551 -4.2078648 -4.2540069][-4.1600418 -4.0860853 -4.0108886 -3.9296029 -3.9212546 -3.9749808 -4.0541363 -4.1041088 -4.1231074 -4.1471858 -4.1745796 -4.1991906 -4.2273369 -4.2607169 -4.2891703][-4.2248106 -4.1760025 -4.136838 -4.1020136 -4.1114326 -4.1507359 -4.2036138 -4.2334032 -4.2406464 -4.2490182 -4.2583051 -4.2686057 -4.2838902 -4.3040209 -4.3194265][-4.2901177 -4.2634845 -4.2483907 -4.2406025 -4.2545581 -4.2790236 -4.3068361 -4.318285 -4.3138919 -4.3107023 -4.3104453 -4.3130083 -4.3200545 -4.3309612 -4.3385139][-4.3299789 -4.3188343 -4.3143768 -4.3155212 -4.3248277 -4.3369532 -4.3483667 -4.3497105 -4.3424091 -4.3369222 -4.3346739 -4.3354073 -4.3384347 -4.3434806 -4.3474946][-4.346662 -4.3439403 -4.3423796 -4.3440332 -4.3479013 -4.3526139 -4.3561044 -4.3557172 -4.3519421 -4.348783 -4.3478732 -4.3482447 -4.3494353 -4.3510628 -4.3523874]]...]
INFO - root - 2017-12-06 05:44:39.413007: step 2010, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 75h:14m:38s remains)
INFO - root - 2017-12-06 05:44:47.843677: step 2020, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.888 sec/batch; 81h:28m:34s remains)
INFO - root - 2017-12-06 05:44:56.450170: step 2030, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 78h:08m:59s remains)
INFO - root - 2017-12-06 05:45:05.037280: step 2040, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 78h:35m:35s remains)
INFO - root - 2017-12-06 05:45:13.731053: step 2050, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 80h:44m:29s remains)
INFO - root - 2017-12-06 05:45:22.295734: step 2060, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 79h:00m:30s remains)
INFO - root - 2017-12-06 05:45:30.969311: step 2070, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 81h:16m:45s remains)
INFO - root - 2017-12-06 05:45:39.600880: step 2080, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 80h:56m:31s remains)
INFO - root - 2017-12-06 05:45:48.300917: step 2090, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 80h:52m:45s remains)
INFO - root - 2017-12-06 05:45:56.959114: step 2100, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 82h:17m:40s remains)
2017-12-06 05:46:00.059404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1453514 -4.1507535 -4.1638808 -4.1885481 -4.210268 -4.2141032 -4.2203083 -4.2353334 -4.254118 -4.2740369 -4.3022337 -4.3184133 -4.328052 -4.3291926 -4.3214121][-4.1633677 -4.1703153 -4.1850309 -4.2038641 -4.2196908 -4.2245593 -4.2321887 -4.2471752 -4.2692137 -4.2950225 -4.32423 -4.3367386 -4.3448305 -4.34481 -4.3322968][-4.1822529 -4.1784258 -4.1928177 -4.2119188 -4.2222137 -4.2211871 -4.2231736 -4.2321205 -4.2519794 -4.283566 -4.3178835 -4.3325214 -4.3444495 -4.3452973 -4.3313894][-4.1947994 -4.1833868 -4.1967435 -4.2173796 -4.2215333 -4.2032037 -4.1871266 -4.1868262 -4.2052116 -4.2481146 -4.292079 -4.3135443 -4.3312669 -4.3348064 -4.320673][-4.2121215 -4.2033191 -4.2125998 -4.2258024 -4.2178078 -4.1720309 -4.1245966 -4.1111064 -4.1339931 -4.1934972 -4.2522435 -4.2880411 -4.3149409 -4.3270645 -4.3189149][-4.2259421 -4.2284622 -4.2365828 -4.2386937 -4.2160044 -4.1465135 -4.0634365 -4.0243015 -4.0510859 -4.12976 -4.2050729 -4.2588811 -4.2973905 -4.3197985 -4.3202419][-4.2411733 -4.2544441 -4.2643352 -4.2597971 -4.2280884 -4.151083 -4.04834 -3.9789059 -3.9961379 -4.0843658 -4.1711946 -4.2385926 -4.2871528 -4.3138371 -4.3181806][-4.2467318 -4.2690744 -4.2782121 -4.2696347 -4.2368984 -4.1659265 -4.0649323 -3.9828916 -3.98796 -4.070715 -4.1566396 -4.2264662 -4.281105 -4.3061657 -4.3067555][-4.2374945 -4.2632747 -4.2684026 -4.2564988 -4.225636 -4.1676888 -4.0811415 -4.0046372 -4.0027337 -4.0688782 -4.148365 -4.2182088 -4.2764697 -4.3028793 -4.2989521][-4.2246428 -4.24964 -4.2533951 -4.2417097 -4.21476 -4.1684022 -4.0949841 -4.0311556 -4.0341396 -4.0885158 -4.1532116 -4.215302 -4.2688074 -4.2951183 -4.2905049][-4.2329021 -4.2510872 -4.2492018 -4.2370219 -4.2148194 -4.1788292 -4.1195631 -4.0690913 -4.0773969 -4.127306 -4.1795254 -4.2272654 -4.2640891 -4.28156 -4.2799525][-4.2581458 -4.2674761 -4.2556024 -4.2406163 -4.2200131 -4.1939507 -4.1526074 -4.1200018 -4.1328349 -4.1781054 -4.2176423 -4.2464123 -4.2607865 -4.2607989 -4.2587347][-4.2889786 -4.2951932 -4.2802906 -4.2609882 -4.2398219 -4.2189431 -4.1915593 -4.1748443 -4.1899447 -4.228251 -4.2539077 -4.2648878 -4.2530336 -4.2276535 -4.2177987][-4.2932196 -4.3028407 -4.2966666 -4.2830181 -4.26791 -4.2551789 -4.2395039 -4.2334075 -4.2486634 -4.27671 -4.2893071 -4.2845592 -4.2517982 -4.2037826 -4.1817932][-4.2796941 -4.28965 -4.2904029 -4.2880812 -4.28589 -4.28304 -4.2781172 -4.2796841 -4.2935052 -4.3110437 -4.3123288 -4.2952161 -4.2490015 -4.19201 -4.1639237]]...]
INFO - root - 2017-12-06 05:46:08.895419: step 2110, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 83h:19m:20s remains)
INFO - root - 2017-12-06 05:46:17.565445: step 2120, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 80h:56m:30s remains)
INFO - root - 2017-12-06 05:46:26.077755: step 2130, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 78h:57m:02s remains)
INFO - root - 2017-12-06 05:46:34.853055: step 2140, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 83h:02m:29s remains)
INFO - root - 2017-12-06 05:46:43.663668: step 2150, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 80h:37m:55s remains)
INFO - root - 2017-12-06 05:46:52.393344: step 2160, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 78h:22m:36s remains)
INFO - root - 2017-12-06 05:47:01.073588: step 2170, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 78h:22m:53s remains)
INFO - root - 2017-12-06 05:47:09.741254: step 2180, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 79h:56m:34s remains)
INFO - root - 2017-12-06 05:47:18.491031: step 2190, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 79h:31m:35s remains)
INFO - root - 2017-12-06 05:47:26.916010: step 2200, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 76h:52m:14s remains)
2017-12-06 05:47:27.653112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2511992 -4.266427 -4.28674 -4.2859807 -4.2674651 -4.237052 -4.1901355 -4.1355805 -4.1170359 -4.1280379 -4.1450729 -4.1611962 -4.1924944 -4.2185755 -4.2385][-4.23683 -4.2537746 -4.2781906 -4.2835732 -4.2668633 -4.2350221 -4.1869068 -4.1330557 -4.1136 -4.1270804 -4.1498523 -4.1740155 -4.2117348 -4.245615 -4.2698922][-4.2220259 -4.2455769 -4.2741218 -4.2818074 -4.2587895 -4.2186074 -4.16637 -4.1141572 -4.1022272 -4.12403 -4.1546936 -4.1876793 -4.23141 -4.2716227 -4.3000178][-4.2149324 -4.24782 -4.2813892 -4.286222 -4.251771 -4.1962266 -4.1287313 -4.0705638 -4.0685139 -4.1072359 -4.1563568 -4.2036138 -4.2557321 -4.2998915 -4.3281207][-4.21543 -4.2544045 -4.2921505 -4.2915044 -4.2442155 -4.1706557 -4.0835018 -4.016151 -4.0205603 -4.0798163 -4.1534166 -4.2215948 -4.2826738 -4.3263507 -4.3479891][-4.2227564 -4.2657351 -4.3027864 -4.2933321 -4.2318912 -4.14012 -4.0297856 -3.9525819 -3.9617658 -4.0445194 -4.1469464 -4.2353315 -4.3044662 -4.3428087 -4.3529673][-4.2352476 -4.2812991 -4.3135071 -4.2934203 -4.2189293 -4.1102524 -3.9747214 -3.8806875 -3.8904383 -3.9945643 -4.1227655 -4.228621 -4.3052464 -4.3414826 -4.3448071][-4.2566514 -4.2972293 -4.3204865 -4.2927523 -4.2135067 -4.0937343 -3.9350867 -3.8153338 -3.8103142 -3.9256015 -4.0730209 -4.1982665 -4.2879281 -4.3278427 -4.3251553][-4.2835894 -4.3132324 -4.3247962 -4.2914696 -4.2150545 -4.0993094 -3.9341073 -3.7942286 -3.7631996 -3.8700652 -4.0224295 -4.1601281 -4.2643633 -4.3120489 -4.3091063][-4.3122549 -4.3316979 -4.3338509 -4.2977343 -4.227932 -4.1254244 -3.9751778 -3.8367238 -3.7891502 -3.8719821 -4.0093036 -4.141449 -4.2465343 -4.2963719 -4.296483][-4.334096 -4.3480191 -4.3461661 -4.3129954 -4.2558403 -4.1743412 -4.0531316 -3.9310179 -3.8753977 -3.9258456 -4.0329981 -4.1476917 -4.2384429 -4.285594 -4.2895832][-4.3432393 -4.3529477 -4.3490634 -4.3211646 -4.2781534 -4.2200875 -4.1323562 -4.037838 -3.9853959 -4.0070391 -4.0798492 -4.1699142 -4.2405949 -4.2796388 -4.2840123][-4.33893 -4.3430772 -4.3371797 -4.3164811 -4.2870207 -4.2480969 -4.1882825 -4.1243238 -4.0858135 -4.0927191 -4.1394577 -4.20402 -4.2542057 -4.2839909 -4.2872477][-4.3287039 -4.3251009 -4.3159747 -4.3015347 -4.2829227 -4.25593 -4.2165046 -4.1781831 -4.159462 -4.1678667 -4.202702 -4.2457318 -4.2765207 -4.2954774 -4.29422][-4.3185582 -4.3079786 -4.2959218 -4.2856169 -4.2741752 -4.255753 -4.2284918 -4.2048655 -4.1964722 -4.2058358 -4.234273 -4.264133 -4.2835431 -4.2962914 -4.2956548]]...]
INFO - root - 2017-12-06 05:47:36.475795: step 2210, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.821 sec/batch; 75h:18m:18s remains)
INFO - root - 2017-12-06 05:47:45.184160: step 2220, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 79h:31m:45s remains)
INFO - root - 2017-12-06 05:47:53.886760: step 2230, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 79h:43m:52s remains)
INFO - root - 2017-12-06 05:48:02.610747: step 2240, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 81h:29m:50s remains)
INFO - root - 2017-12-06 05:48:11.366862: step 2250, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 81h:03m:01s remains)
INFO - root - 2017-12-06 05:48:19.915109: step 2260, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 79h:08m:27s remains)
INFO - root - 2017-12-06 05:48:28.604813: step 2270, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 80h:24m:38s remains)
INFO - root - 2017-12-06 05:48:37.284520: step 2280, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 76h:06m:11s remains)
INFO - root - 2017-12-06 05:48:46.002838: step 2290, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 81h:03m:25s remains)
INFO - root - 2017-12-06 05:48:53.914710: step 2300, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 82h:34m:34s remains)
2017-12-06 05:48:54.713451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2539659 -4.2547755 -4.2484751 -4.232924 -4.2031465 -4.1826229 -4.171381 -4.15392 -4.1471291 -4.1392651 -4.1260242 -4.1021552 -4.0826678 -4.0788546 -4.1122046][-4.2487411 -4.2454782 -4.2318397 -4.2110415 -4.1808362 -4.1576324 -4.139915 -4.11901 -4.1046081 -4.0819917 -4.0666962 -4.0656343 -4.0778322 -4.0984197 -4.1416149][-4.2525263 -4.2368712 -4.2077103 -4.1735735 -4.1417341 -4.1195827 -4.1013765 -4.081933 -4.0583143 -4.0214567 -4.0126028 -4.0436964 -4.0878482 -4.1292157 -4.1800156][-4.247015 -4.2230835 -4.1877508 -4.1501207 -4.123096 -4.1048789 -4.0884476 -4.0683994 -4.0357275 -3.9996016 -4.0050755 -4.0484476 -4.102139 -4.151238 -4.2051115][-4.246326 -4.2221413 -4.1909294 -4.1538754 -4.1231627 -4.09836 -4.0783434 -4.051589 -4.0216274 -4.0172744 -4.0449705 -4.0829124 -4.1317267 -4.1784105 -4.2269826][-4.2419548 -4.222188 -4.2028103 -4.1664662 -4.1239452 -4.0803123 -4.0346456 -3.9858594 -3.9639008 -4.0140662 -4.0799556 -4.1254163 -4.1742611 -4.2169528 -4.2563262][-4.2239795 -4.2045503 -4.1970267 -4.1616511 -4.1018753 -4.0263147 -3.945056 -3.8682575 -3.8670721 -3.9780321 -4.0834017 -4.1480613 -4.2045541 -4.2490921 -4.2787724][-4.1941686 -4.1732974 -4.1659389 -4.1324639 -4.0691867 -3.9904704 -3.9077935 -3.8389163 -3.8591971 -3.986326 -4.0977058 -4.1670332 -4.2222519 -4.2647157 -4.288403][-4.1394072 -4.1155009 -4.1122713 -4.1017666 -4.0758991 -4.0409884 -4.0020704 -3.9769683 -4.0076427 -4.0913639 -4.1637263 -4.2083049 -4.2453947 -4.2718029 -4.2859917][-4.0801973 -4.0723062 -4.0948391 -4.112803 -4.1147494 -4.1096845 -4.1038523 -4.1083026 -4.1388874 -4.1886559 -4.2303963 -4.2557907 -4.2737179 -4.2815676 -4.28637][-4.0715833 -4.0872426 -4.1194749 -4.1398268 -4.1464057 -4.15218 -4.1597948 -4.1757741 -4.2015333 -4.2347631 -4.2612205 -4.2821054 -4.2945137 -4.2955842 -4.2961268][-4.1126189 -4.1326685 -4.154181 -4.1649466 -4.171248 -4.180851 -4.1884437 -4.203342 -4.2254996 -4.2503977 -4.2704654 -4.29185 -4.3054996 -4.3084164 -4.3099113][-4.1548023 -4.1722474 -4.1861491 -4.1924443 -4.1984534 -4.2031388 -4.2038741 -4.2110109 -4.2309394 -4.2535419 -4.2723508 -4.2942319 -4.310462 -4.3165021 -4.3203731][-4.1745896 -4.1893859 -4.2003922 -4.2030797 -4.2059317 -4.210032 -4.2135062 -4.2217174 -4.239996 -4.2641621 -4.2837682 -4.3044438 -4.3196168 -4.3246751 -4.3265123][-4.1871829 -4.2031946 -4.2163148 -4.215765 -4.2136803 -4.2218275 -4.2372651 -4.2509737 -4.2703271 -4.2909679 -4.3057809 -4.3206782 -4.3296466 -4.3309441 -4.3307009]]...]
INFO - root - 2017-12-06 05:49:03.348848: step 2310, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 79h:45m:55s remains)
INFO - root - 2017-12-06 05:49:12.182685: step 2320, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 81h:04m:10s remains)
INFO - root - 2017-12-06 05:49:20.853840: step 2330, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.884 sec/batch; 81h:02m:16s remains)
INFO - root - 2017-12-06 05:49:29.599073: step 2340, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 76h:21m:38s remains)
INFO - root - 2017-12-06 05:49:38.242407: step 2350, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 79h:38m:37s remains)
INFO - root - 2017-12-06 05:49:47.021716: step 2360, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 81h:17m:18s remains)
INFO - root - 2017-12-06 05:49:55.799441: step 2370, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 80h:37m:59s remains)
INFO - root - 2017-12-06 05:50:04.477465: step 2380, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 78h:38m:28s remains)
INFO - root - 2017-12-06 05:50:12.987511: step 2390, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 81h:05m:48s remains)
INFO - root - 2017-12-06 05:50:21.610382: step 2400, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 81h:05m:54s remains)
2017-12-06 05:50:24.130228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1178451 -4.1178889 -4.1510663 -4.1886563 -4.2092042 -4.2193742 -4.21884 -4.2148724 -4.2169762 -4.2278061 -4.2349477 -4.2355771 -4.2216067 -4.2075167 -4.2124162][-4.0879569 -4.0871077 -4.1236029 -4.1658888 -4.19388 -4.2133012 -4.2218375 -4.2220092 -4.2228084 -4.2292671 -4.2339039 -4.2319355 -4.222321 -4.2198277 -4.2348056][-4.0981526 -4.0931706 -4.1231103 -4.1641979 -4.197813 -4.2225204 -4.234839 -4.2364173 -4.2354541 -4.2379866 -4.2404833 -4.2342863 -4.2293777 -4.2332454 -4.2474303][-4.127512 -4.122139 -4.1472507 -4.1852283 -4.2179251 -4.2375565 -4.2409506 -4.2375965 -4.231051 -4.2312636 -4.2350187 -4.2304177 -4.2315283 -4.234498 -4.2390428][-4.1512661 -4.151834 -4.1757708 -4.2063708 -4.2295451 -4.2356305 -4.2216811 -4.2068224 -4.1930933 -4.1942635 -4.2098441 -4.2211304 -4.2347336 -4.2364087 -4.2306843][-4.1552796 -4.161366 -4.1860023 -4.2076936 -4.2148724 -4.1976686 -4.1590219 -4.1236324 -4.1046438 -4.1224189 -4.1646762 -4.2023239 -4.2335811 -4.2413888 -4.2303638][-4.14359 -4.1493468 -4.1706133 -4.1842575 -4.1747565 -4.129611 -4.0589714 -3.9964786 -3.9764159 -4.02689 -4.1098514 -4.1773477 -4.2264729 -4.2473965 -4.2356033][-4.1360793 -4.1403623 -4.15819 -4.1650333 -4.1382642 -4.06625 -3.966141 -3.8813295 -3.865685 -3.947639 -4.06059 -4.1479044 -4.2099452 -4.243474 -4.2377439][-4.1426935 -4.1463156 -4.161243 -4.1666842 -4.1335034 -4.0566897 -3.9603777 -3.8853126 -3.8792915 -3.9553823 -4.0567617 -4.136466 -4.1995606 -4.24096 -4.2427392][-4.1610918 -4.1592555 -4.1705689 -4.1783957 -4.1547923 -4.1006961 -4.0407653 -4.0008612 -3.9989588 -4.0399327 -4.101387 -4.1549797 -4.2056637 -4.2444615 -4.2492418][-4.1910634 -4.179832 -4.1799865 -4.1829157 -4.1721144 -4.1490393 -4.128963 -4.1188283 -4.116045 -4.1274071 -4.1531024 -4.1818066 -4.2159948 -4.2455678 -4.2515259][-4.2143049 -4.1982102 -4.1884427 -4.186676 -4.18827 -4.1893449 -4.1931515 -4.1952929 -4.1881423 -4.180233 -4.1808472 -4.1896677 -4.210207 -4.2298503 -4.2366552][-4.2240705 -4.2112651 -4.2002344 -4.1991205 -4.2091241 -4.2223082 -4.2327304 -4.2305179 -4.214222 -4.1944151 -4.1763816 -4.1667733 -4.1753483 -4.1886582 -4.2005072][-4.2189918 -4.2112851 -4.2053065 -4.2084427 -4.2214823 -4.237298 -4.2481484 -4.2391691 -4.2126012 -4.1832466 -4.1527467 -4.1284165 -4.125258 -4.1328821 -4.1512685][-4.1944852 -4.1911516 -4.19146 -4.1986518 -4.210619 -4.2249107 -4.234735 -4.2230721 -4.192328 -4.1606145 -4.1286297 -4.1016049 -4.0923638 -4.0972834 -4.1214423]]...]
INFO - root - 2017-12-06 05:50:32.668045: step 2410, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 81h:47m:02s remains)
INFO - root - 2017-12-06 05:50:41.415036: step 2420, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 77h:53m:43s remains)
INFO - root - 2017-12-06 05:50:49.949194: step 2430, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 77h:43m:38s remains)
INFO - root - 2017-12-06 05:50:58.602230: step 2440, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 78h:42m:36s remains)
INFO - root - 2017-12-06 05:51:07.137966: step 2450, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 78h:24m:08s remains)
INFO - root - 2017-12-06 05:51:15.787219: step 2460, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 79h:03m:48s remains)
INFO - root - 2017-12-06 05:51:24.347746: step 2470, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 76h:49m:13s remains)
INFO - root - 2017-12-06 05:51:32.944536: step 2480, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 77h:52m:49s remains)
INFO - root - 2017-12-06 05:51:41.705708: step 2490, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.908 sec/batch; 83h:12m:01s remains)
INFO - root - 2017-12-06 05:51:50.394496: step 2500, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 79h:51m:31s remains)
2017-12-06 05:51:51.164344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3068171 -4.3000884 -4.293664 -4.2876167 -4.2812567 -4.2762117 -4.2802181 -4.293263 -4.3044119 -4.309732 -4.3067517 -4.2975736 -4.2905469 -4.2908573 -4.294673][-4.2969079 -4.2838273 -4.2712197 -4.2595835 -4.2492909 -4.2398076 -4.2428336 -4.2621174 -4.2813082 -4.2917385 -4.2935405 -4.2867403 -4.2759042 -4.2691708 -4.2692361][-4.2662768 -4.243897 -4.2258186 -4.2126064 -4.2009654 -4.1881132 -4.1917505 -4.2201138 -4.2465568 -4.2606568 -4.2686129 -4.2681928 -4.2602243 -4.2499409 -4.2478194][-4.2196903 -4.1873889 -4.1655817 -4.1544118 -4.1427112 -4.1262364 -4.1262689 -4.1616492 -4.1964645 -4.2145109 -4.22593 -4.2321348 -4.2296386 -4.2176666 -4.2150655][-4.1970911 -4.1546106 -4.1272922 -4.1128945 -4.0958228 -4.0709043 -4.0589614 -4.0952559 -4.1420465 -4.1679878 -4.1812568 -4.1889329 -4.1917129 -4.1869359 -4.18577][-4.1808753 -4.1353245 -4.0947351 -4.0693264 -4.0425935 -4.004045 -3.9707518 -3.9986866 -4.0656581 -4.1142588 -4.1369963 -4.14958 -4.1607556 -4.16989 -4.1754675][-4.1767216 -4.14032 -4.0882034 -4.0422659 -3.9946644 -3.9298778 -3.8631763 -3.8676028 -3.9597557 -4.0462985 -4.09355 -4.1177039 -4.1354747 -4.1534514 -4.1670737][-4.1942825 -4.1654634 -4.1104927 -4.0521092 -3.9956818 -3.9224136 -3.8419704 -3.8365271 -3.9262686 -4.0205059 -4.074532 -4.1029449 -4.120749 -4.1422057 -4.1615276][-4.2080007 -4.180759 -4.125145 -4.0622978 -4.0136461 -3.9583786 -3.8991141 -3.9111295 -3.9837406 -4.0511765 -4.0829058 -4.0966134 -4.1062384 -4.126977 -4.1485605][-4.2192121 -4.19594 -4.14881 -4.0910177 -4.0453367 -4.0012426 -3.9551487 -3.9701774 -4.0234261 -4.0656519 -4.0838575 -4.0891714 -4.0905032 -4.1041775 -4.125349][-4.1930695 -4.1777749 -4.1509719 -4.1165271 -4.082644 -4.0455904 -4.0092163 -4.0120082 -4.0424123 -4.062798 -4.0685406 -4.0728521 -4.0779839 -4.0891652 -4.1096773][-4.1733947 -4.1665769 -4.1585693 -4.1450934 -4.1241045 -4.0958962 -4.07152 -4.0709729 -4.0850792 -4.0849276 -4.0754404 -4.0787988 -4.0930009 -4.1060839 -4.1255941][-4.1850896 -4.186563 -4.1842194 -4.1756592 -4.1597662 -4.1388707 -4.1201138 -4.1191611 -4.1280913 -4.1181364 -4.1022568 -4.1052814 -4.1271338 -4.1436124 -4.1622243][-4.2047057 -4.21211 -4.2158318 -4.2091002 -4.2015038 -4.1898909 -4.1765094 -4.1769867 -4.1858988 -4.1752553 -4.1583953 -4.1614838 -4.1793127 -4.1918554 -4.2062259][-4.2423005 -4.2523742 -4.260437 -4.256628 -4.2552638 -4.2501645 -4.2430625 -4.2479048 -4.2570019 -4.25113 -4.2393188 -4.2407665 -4.250587 -4.2559319 -4.2627611]]...]
INFO - root - 2017-12-06 05:51:59.784740: step 2510, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 81h:21m:12s remains)
INFO - root - 2017-12-06 05:52:08.676724: step 2520, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 81h:10m:14s remains)
INFO - root - 2017-12-06 05:52:17.274650: step 2530, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 79h:38m:18s remains)
INFO - root - 2017-12-06 05:52:25.943110: step 2540, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 78h:39m:33s remains)
INFO - root - 2017-12-06 05:52:34.539606: step 2550, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 78h:09m:13s remains)
INFO - root - 2017-12-06 05:52:43.024179: step 2560, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 76h:35m:43s remains)
INFO - root - 2017-12-06 05:52:51.603840: step 2570, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 79h:03m:03s remains)
INFO - root - 2017-12-06 05:53:00.255588: step 2580, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 78h:30m:01s remains)
INFO - root - 2017-12-06 05:53:08.816695: step 2590, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.816 sec/batch; 74h:46m:20s remains)
INFO - root - 2017-12-06 05:53:17.567894: step 2600, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 80h:06m:55s remains)
2017-12-06 05:53:18.337513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.183392 -4.1800332 -4.1826296 -4.1904869 -4.202569 -4.2056112 -4.2039862 -4.1960788 -4.174324 -4.1461425 -4.1236491 -4.1010466 -4.087049 -4.1004577 -4.1248875][-4.1697845 -4.1676245 -4.1792216 -4.1955652 -4.2089963 -4.2112188 -4.2106071 -4.205194 -4.1843834 -4.146565 -4.1114888 -4.0845332 -4.0733414 -4.0874739 -4.1085663][-4.1677742 -4.1707358 -4.1882596 -4.2104359 -4.2225661 -4.217751 -4.2080708 -4.196445 -4.176652 -4.1403961 -4.10543 -4.0790477 -4.0651879 -4.0667615 -4.0739365][-4.1784606 -4.1851873 -4.2023053 -4.2219682 -4.2259984 -4.212389 -4.1945968 -4.1751041 -4.1563892 -4.1328483 -4.1091685 -4.0846066 -4.0620255 -4.0483694 -4.0399241][-4.1868558 -4.1936641 -4.2084379 -4.2219982 -4.2174129 -4.1968274 -4.17213 -4.14221 -4.1212978 -4.1123524 -4.1055279 -4.090219 -4.0664492 -4.0442972 -4.0247674][-4.1866713 -4.1892328 -4.2012696 -4.2112737 -4.2016296 -4.1763062 -4.1435494 -4.1033745 -4.0779772 -4.0792584 -4.0868511 -4.0814037 -4.0628786 -4.0430727 -4.0281863][-4.1722326 -4.1758542 -4.1873145 -4.1967444 -4.1855478 -4.155829 -4.1171694 -4.0705457 -4.0442963 -4.0519571 -4.0681295 -4.0743294 -4.0663643 -4.0541215 -4.047925][-4.1615992 -4.1779618 -4.1935229 -4.2016439 -4.1898794 -4.1573696 -4.1157594 -4.0705471 -4.0506024 -4.0588164 -4.0729871 -4.0862489 -4.087328 -4.0785584 -4.0728884][-4.1592951 -4.1905656 -4.2101269 -4.216125 -4.2061663 -4.176568 -4.1362333 -4.0969467 -4.0834332 -4.09105 -4.0998645 -4.1145906 -4.1236358 -4.1180086 -4.1109667][-4.1639957 -4.2045307 -4.2265921 -4.2317028 -4.2232342 -4.1972685 -4.162251 -4.1322351 -4.1258583 -4.139051 -4.1488876 -4.158267 -4.1622744 -4.1538405 -4.1455193][-4.1748204 -4.2146139 -4.238348 -4.2461872 -4.2402906 -4.2181349 -4.1884375 -4.1678915 -4.171751 -4.192277 -4.2027068 -4.2020421 -4.1937842 -4.177402 -4.1705861][-4.1869965 -4.2229867 -4.2476287 -4.2605639 -4.2614098 -4.2452469 -4.2205992 -4.2050052 -4.2123275 -4.2324548 -4.2392688 -4.2335439 -4.2185259 -4.2007627 -4.1973791][-4.2021322 -4.2304235 -4.2544451 -4.2714381 -4.2784181 -4.2702751 -4.253098 -4.2425146 -4.2487907 -4.2638564 -4.2670774 -4.2591734 -4.24252 -4.226584 -4.2242322][-4.2295251 -4.2440591 -4.2621469 -4.279037 -4.2895503 -4.2876058 -4.2782812 -4.2722492 -4.2772379 -4.2867613 -4.2874894 -4.28094 -4.2691979 -4.2585196 -4.2577171][-4.2650352 -4.2689581 -4.278749 -4.2899394 -4.2983418 -4.2994909 -4.2952394 -4.2922082 -4.2959609 -4.3028021 -4.3044548 -4.3013167 -4.29492 -4.2892742 -4.289186]]...]
INFO - root - 2017-12-06 05:53:26.949990: step 2610, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 78h:00m:59s remains)
INFO - root - 2017-12-06 05:53:35.510356: step 2620, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 77h:35m:14s remains)
INFO - root - 2017-12-06 05:53:44.201971: step 2630, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 77h:33m:03s remains)
INFO - root - 2017-12-06 05:53:52.806834: step 2640, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 79h:37m:11s remains)
INFO - root - 2017-12-06 05:54:01.365464: step 2650, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 77h:26m:01s remains)
INFO - root - 2017-12-06 05:54:09.983416: step 2660, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 79h:20m:56s remains)
INFO - root - 2017-12-06 05:54:18.520052: step 2670, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 79h:28m:46s remains)
INFO - root - 2017-12-06 05:54:27.138194: step 2680, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 81h:04m:03s remains)
INFO - root - 2017-12-06 05:54:35.545078: step 2690, loss = 2.07, batch loss = 2.02 (10.5 examples/sec; 0.762 sec/batch; 69h:47m:02s remains)
INFO - root - 2017-12-06 05:54:44.027109: step 2700, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 79h:54m:22s remains)
2017-12-06 05:54:44.730654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2250118 -4.2460494 -4.2744327 -4.289309 -4.2918396 -4.2930574 -4.297574 -4.30148 -4.3010488 -4.2987046 -4.2962375 -4.2946 -4.2938538 -4.2927523 -4.29002][-4.232039 -4.2481661 -4.2699018 -4.2817802 -4.2845788 -4.2864804 -4.2910333 -4.2949429 -4.2945557 -4.2923441 -4.2904873 -4.289794 -4.2896914 -4.2887955 -4.2866821][-4.2595797 -4.2638626 -4.2693009 -4.2709231 -4.2666907 -4.2647128 -4.2691965 -4.2746029 -4.2776718 -4.280282 -4.2833791 -4.2874107 -4.2897344 -4.2890277 -4.2880607][-4.2783618 -4.2706962 -4.2619872 -4.2534175 -4.242959 -4.2369518 -4.2425323 -4.2520814 -4.2594132 -4.2670789 -4.275147 -4.2827082 -4.2860427 -4.2831135 -4.2798963][-4.2540116 -4.2378497 -4.2219138 -4.2078109 -4.1936293 -4.1876845 -4.1977482 -4.2147784 -4.2269292 -4.2386556 -4.2510972 -4.263895 -4.2690721 -4.262619 -4.2551308][-4.1947656 -4.1733356 -4.1557012 -4.1402788 -4.1289692 -4.1276541 -4.1447272 -4.1741934 -4.1966863 -4.2148476 -4.2324672 -4.2491493 -4.2531157 -4.2375021 -4.2194943][-4.1246328 -4.0972705 -4.0756888 -4.0596957 -4.0455728 -4.042388 -4.0615315 -4.1034331 -4.1445479 -4.1762414 -4.200799 -4.2204938 -4.2234812 -4.2024555 -4.1784153][-4.0484343 -4.0129008 -3.9857521 -3.9657087 -3.9459491 -3.9337571 -3.9516175 -4.0063238 -4.0693765 -4.1189089 -4.1521888 -4.1721072 -4.1698074 -4.1458673 -4.1252775][-4.0084352 -3.9712586 -3.9469359 -3.9273276 -3.9005527 -3.8756192 -3.8850574 -3.9437647 -4.0208263 -4.081162 -4.1202474 -4.1389446 -4.1336541 -4.1101241 -4.0890422][-4.0464048 -4.0248208 -4.0153732 -4.0084729 -3.991976 -3.9724832 -3.9790242 -4.0248585 -4.0867476 -4.1329865 -4.161912 -4.1697483 -4.1580772 -4.1336603 -4.1032491][-4.1268964 -4.1143746 -4.110384 -4.1096525 -4.1011562 -4.0884509 -4.0962639 -4.1284423 -4.1685638 -4.1962233 -4.2133861 -4.2154694 -4.2058587 -4.1904521 -4.1627712][-4.1914048 -4.1810679 -4.1723819 -4.1715555 -4.1666818 -4.15789 -4.1645732 -4.1844296 -4.2067852 -4.219429 -4.2255435 -4.2236805 -4.2163544 -4.2072668 -4.1817942][-4.1961126 -4.1823616 -4.16602 -4.1617007 -4.1595831 -4.1532264 -4.1569915 -4.1665359 -4.1769814 -4.1812453 -4.1809621 -4.1804805 -4.180439 -4.1808395 -4.1664944][-4.1753569 -4.153389 -4.1296196 -4.1221213 -4.1227336 -4.1189885 -4.1221671 -4.1273623 -4.1320262 -4.1331897 -4.1358738 -4.1435747 -4.1536627 -4.1650271 -4.1652107][-4.165627 -4.1375933 -4.1134529 -4.1075144 -4.1104569 -4.1105824 -4.1139679 -4.117178 -4.1203628 -4.1230593 -4.1289115 -4.1410441 -4.1565886 -4.1751084 -4.1831112]]...]
INFO - root - 2017-12-06 05:54:53.312675: step 2710, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.872 sec/batch; 79h:54m:40s remains)
INFO - root - 2017-12-06 05:55:01.904212: step 2720, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 81h:31m:51s remains)
INFO - root - 2017-12-06 05:55:10.693411: step 2730, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 80h:35m:11s remains)
INFO - root - 2017-12-06 05:55:19.448499: step 2740, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 80h:17m:22s remains)
INFO - root - 2017-12-06 05:55:28.162455: step 2750, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 77h:22m:50s remains)
INFO - root - 2017-12-06 05:55:36.710942: step 2760, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 79h:48m:30s remains)
INFO - root - 2017-12-06 05:55:45.330133: step 2770, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 81h:36m:47s remains)
INFO - root - 2017-12-06 05:55:53.868854: step 2780, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 78h:10m:31s remains)
INFO - root - 2017-12-06 05:56:02.486122: step 2790, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 82h:17m:41s remains)
INFO - root - 2017-12-06 05:56:11.088047: step 2800, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 80h:27m:38s remains)
2017-12-06 05:56:11.926716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.159039 -4.1612182 -4.1608667 -4.1594958 -4.155736 -4.14742 -4.1089745 -4.0562258 -4.0284691 -4.0605335 -4.129756 -4.2000461 -4.2538419 -4.2946987 -4.319942][-4.1476126 -4.1348305 -4.1223741 -4.1157122 -4.1174321 -4.1165204 -4.0889168 -4.0416007 -4.0141792 -4.0482373 -4.1219091 -4.1955233 -4.2494164 -4.2878666 -4.3117838][-4.1723008 -4.14387 -4.1111908 -4.0913973 -4.0921984 -4.0998926 -4.089869 -4.0545373 -4.0286927 -4.0646734 -4.1365805 -4.2032924 -4.251935 -4.2878628 -4.3095837][-4.1999383 -4.1657844 -4.1259904 -4.0945792 -4.0820251 -4.0810127 -4.0779877 -4.0559096 -4.0333953 -4.0770316 -4.1545558 -4.21509 -4.2564559 -4.2925591 -4.3144073][-4.2212071 -4.1833353 -4.1395617 -4.1003947 -4.0693483 -4.0476122 -4.0388803 -4.0232263 -4.0044241 -4.0558691 -4.146256 -4.2114229 -4.2523594 -4.2905426 -4.3157887][-4.2320609 -4.1937528 -4.1472578 -4.0917554 -4.0351043 -3.9966998 -3.9783971 -3.9574614 -3.9399822 -4.005609 -4.11385 -4.1917977 -4.2378812 -4.2820573 -4.313766][-4.2304335 -4.194469 -4.1449919 -4.0725384 -3.9885035 -3.9352987 -3.9130576 -3.8886766 -3.877269 -3.9578958 -4.07971 -4.1707873 -4.2253346 -4.2752204 -4.3125315][-4.2232647 -4.1877551 -4.1366987 -4.059628 -3.963274 -3.9066398 -3.8838058 -3.8595033 -3.8534846 -3.94182 -4.0689006 -4.165647 -4.2246361 -4.2751808 -4.313261][-4.2214642 -4.1857915 -4.1384125 -4.0793638 -4.0111046 -3.9670863 -3.9400032 -3.9070911 -3.8928628 -3.9668798 -4.0834908 -4.1764607 -4.2340164 -4.2804732 -4.314364][-4.2158813 -4.1792917 -4.1368876 -4.1083236 -4.0896039 -4.0758734 -4.050251 -4.00375 -3.9705372 -4.017849 -4.1145906 -4.196455 -4.2468514 -4.2853537 -4.3143167][-4.1904793 -4.1506009 -4.1144552 -4.1045728 -4.1135745 -4.1223922 -4.1052804 -4.0614419 -4.028707 -4.0633364 -4.1440892 -4.2136207 -4.2553296 -4.2890415 -4.3154655][-4.1631041 -4.1174622 -4.090982 -4.0912495 -4.1045957 -4.1166439 -4.10356 -4.0736876 -4.057797 -4.0899129 -4.1576486 -4.2171206 -4.2557335 -4.2915497 -4.319345][-4.1601534 -4.1185217 -4.1018219 -4.1087227 -4.1166363 -4.1190319 -4.1018653 -4.0806341 -4.0790854 -4.1124997 -4.168633 -4.2178164 -4.2544608 -4.2926035 -4.3230071][-4.1786366 -4.13619 -4.1229687 -4.1365781 -4.1428857 -4.1328211 -4.1076441 -4.0885649 -4.0942645 -4.1291742 -4.1797514 -4.2210951 -4.2542806 -4.2935162 -4.325809][-4.1820364 -4.1372252 -4.1210675 -4.1343951 -4.1461134 -4.1438475 -4.1251903 -4.1117787 -4.1204619 -4.1509652 -4.1921144 -4.2275715 -4.2575831 -4.2956891 -4.3269396]]...]
INFO - root - 2017-12-06 05:56:20.723595: step 2810, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 78h:09m:16s remains)
INFO - root - 2017-12-06 05:56:29.362953: step 2820, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 79h:14m:34s remains)
INFO - root - 2017-12-06 05:56:37.945574: step 2830, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 79h:26m:55s remains)
INFO - root - 2017-12-06 05:56:46.387354: step 2840, loss = 2.03, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 80h:31m:06s remains)
INFO - root - 2017-12-06 05:56:54.906682: step 2850, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 79h:02m:37s remains)
INFO - root - 2017-12-06 05:57:03.532697: step 2860, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 78h:18m:21s remains)
INFO - root - 2017-12-06 05:57:12.049714: step 2870, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 78h:51m:50s remains)
INFO - root - 2017-12-06 05:57:20.675502: step 2880, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 78h:46m:12s remains)
INFO - root - 2017-12-06 05:57:29.162258: step 2890, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 78h:54m:26s remains)
INFO - root - 2017-12-06 05:57:37.509968: step 2900, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.829 sec/batch; 75h:51m:57s remains)
2017-12-06 05:57:38.300199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31175 -4.3177567 -4.3143115 -4.3043556 -4.2894855 -4.2636642 -4.2286825 -4.2054219 -4.2043691 -4.2307844 -4.2715459 -4.3002691 -4.3057771 -4.2917728 -4.2732067][-4.3085971 -4.304769 -4.2888966 -4.2764087 -4.271307 -4.2601404 -4.2359161 -4.2163811 -4.2151232 -4.2428679 -4.2855334 -4.317441 -4.3248649 -4.311224 -4.28985][-4.3049655 -4.2913427 -4.267674 -4.2577167 -4.2664728 -4.2724185 -4.2599325 -4.2401905 -4.2296944 -4.2487235 -4.2860041 -4.3141217 -4.3210645 -4.3092723 -4.2879272][-4.3054552 -4.286912 -4.2630825 -4.2582359 -4.2739358 -4.2876415 -4.2829709 -4.2646832 -4.2473145 -4.2544379 -4.2819324 -4.3020053 -4.3041115 -4.2916651 -4.2710505][-4.3078337 -4.2925324 -4.27279 -4.27034 -4.2830639 -4.2941413 -4.2924852 -4.2783184 -4.2590985 -4.2561059 -4.27138 -4.2829156 -4.27817 -4.2631187 -4.2447891][-4.3103323 -4.3027716 -4.2905645 -4.2900953 -4.2978268 -4.3028684 -4.3009543 -4.2901459 -4.273035 -4.2651639 -4.2715435 -4.2754531 -4.2636251 -4.2431955 -4.2246127][-4.3088341 -4.3091474 -4.3041339 -4.3049955 -4.30717 -4.3057261 -4.3033447 -4.2967148 -4.2866921 -4.283217 -4.2901158 -4.2920628 -4.2754092 -4.2501297 -4.2280407][-4.30158 -4.3109951 -4.3096418 -4.3045917 -4.2957034 -4.2884712 -4.287571 -4.285141 -4.2827082 -4.2866716 -4.297246 -4.3015594 -4.2869458 -4.2641821 -4.2429695][-4.2841578 -4.3058438 -4.3066254 -4.2922621 -4.2719865 -4.2610111 -4.2633557 -4.2637 -4.2641521 -4.2696476 -4.2783875 -4.2831631 -4.2747159 -4.26274 -4.2516141][-4.2613564 -4.2945142 -4.2972379 -4.2748923 -4.2465639 -4.2344 -4.2420826 -4.2446771 -4.2423539 -4.2417917 -4.244092 -4.2480812 -4.2462091 -4.2463264 -4.2490635][-4.2543406 -4.2922297 -4.2953572 -4.2695103 -4.2378879 -4.22461 -4.2342739 -4.236721 -4.2269073 -4.2159524 -4.2081528 -4.2048039 -4.2039375 -4.2151809 -4.2335882][-4.271718 -4.3033657 -4.3015761 -4.2759862 -4.2464919 -4.2337832 -4.242085 -4.2431612 -4.2291908 -4.209825 -4.18836 -4.1720428 -4.1680131 -4.1865172 -4.2121987][-4.2905788 -4.3072047 -4.2973857 -4.273386 -4.2528815 -4.2472029 -4.2560019 -4.2577047 -4.2460561 -4.22509 -4.194262 -4.1667614 -4.1596251 -4.1792393 -4.1994629][-4.2921324 -4.293829 -4.2799954 -4.2633538 -4.2573533 -4.2606049 -4.2695851 -4.2712016 -4.262321 -4.2435226 -4.2116146 -4.1825619 -4.1773748 -4.1933761 -4.1989336][-4.2728992 -4.26558 -4.2553387 -4.2502527 -4.2574735 -4.2677236 -4.2762127 -4.2795267 -4.2755032 -4.2614479 -4.2347 -4.2092471 -4.20664 -4.2167311 -4.2096262]]...]
INFO - root - 2017-12-06 05:57:46.776824: step 2910, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 79h:11m:14s remains)
INFO - root - 2017-12-06 05:57:55.491265: step 2920, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 78h:46m:20s remains)
INFO - root - 2017-12-06 05:58:04.058812: step 2930, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.858 sec/batch; 78h:33m:23s remains)
INFO - root - 2017-12-06 05:58:12.788826: step 2940, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 80h:51m:24s remains)
INFO - root - 2017-12-06 05:58:21.518746: step 2950, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.856 sec/batch; 78h:24m:12s remains)
INFO - root - 2017-12-06 05:58:30.289610: step 2960, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.846 sec/batch; 77h:26m:11s remains)
INFO - root - 2017-12-06 05:58:39.021481: step 2970, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 80h:21m:54s remains)
INFO - root - 2017-12-06 05:58:47.619013: step 2980, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 81h:54m:06s remains)
INFO - root - 2017-12-06 05:58:56.112726: step 2990, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 80h:11m:05s remains)
INFO - root - 2017-12-06 05:59:04.911196: step 3000, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.914 sec/batch; 83h:41m:50s remains)
2017-12-06 05:59:05.651625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1686249 -4.1891627 -4.2296171 -4.2637796 -4.2726903 -4.2459087 -4.1998115 -4.1636496 -4.1626873 -4.18154 -4.2037759 -4.2150259 -4.231883 -4.2307849 -4.2233195][-4.1681051 -4.1845164 -4.2211227 -4.2542877 -4.2640181 -4.2351708 -4.1832738 -4.1435533 -4.1446962 -4.1691046 -4.1939378 -4.2021346 -4.2186518 -4.2204285 -4.2173891][-4.1527686 -4.1649222 -4.1987362 -4.2359762 -4.2518816 -4.2290587 -4.183784 -4.1497469 -4.1543212 -4.1823368 -4.2035627 -4.2055554 -4.2168951 -4.2182422 -4.2157559][-4.1393509 -4.1464572 -4.1700935 -4.2065463 -4.2282562 -4.2152658 -4.1868629 -4.1716042 -4.1906385 -4.223208 -4.24183 -4.24236 -4.2484465 -4.2458968 -4.2421408][-4.1363497 -4.1355224 -4.14098 -4.1617732 -4.1776319 -4.1687989 -4.1541867 -4.153616 -4.1839881 -4.2248492 -4.249136 -4.2545104 -4.2610006 -4.2617507 -4.2637396][-4.1197882 -4.1119852 -4.099864 -4.1046348 -4.1188645 -4.1107159 -4.0900636 -4.0819426 -4.1164765 -4.1712122 -4.2063189 -4.2175322 -4.2245374 -4.2320356 -4.2473841][-4.0980697 -4.0735478 -4.0338149 -4.0163679 -4.0218887 -4.0059695 -3.9586649 -3.92494 -3.9697251 -4.0615845 -4.1236577 -4.1455526 -4.15544 -4.1719742 -4.2018032][-4.1099033 -4.0584793 -3.9854596 -3.9385953 -3.9276152 -3.8925841 -3.8023806 -3.7220755 -3.7856193 -3.9329028 -4.0315108 -4.0629606 -4.068974 -4.0902319 -4.1275315][-4.1660333 -4.1133127 -4.03849 -3.9807775 -3.9585688 -3.9214206 -3.830956 -3.7471745 -3.804863 -3.9488459 -4.0453725 -4.0637217 -4.050168 -4.0552034 -4.081244][-4.23346 -4.19725 -4.1468143 -4.1030612 -4.0845079 -4.0630436 -4.0119529 -3.9652531 -3.9979272 -4.08482 -4.1449203 -4.1438689 -4.1124392 -4.0972986 -4.1032357][-4.2779489 -4.2589674 -4.233592 -4.2080307 -4.1948647 -4.1804476 -4.1544304 -4.1290946 -4.1402082 -4.1782942 -4.2009773 -4.1849084 -4.1501021 -4.1288342 -4.1193132][-4.2977381 -4.2882309 -4.2772803 -4.2655535 -4.2575669 -4.2489357 -4.2330337 -4.2146192 -4.2108212 -4.2147174 -4.2039518 -4.1681232 -4.1291471 -4.1003947 -4.0733423][-4.3087053 -4.3034024 -4.29867 -4.2934422 -4.2894182 -4.2818351 -4.26682 -4.2474828 -4.2350516 -4.2210402 -4.1876111 -4.1356215 -4.0886106 -4.0521493 -4.0148993][-4.3171539 -4.3122635 -4.3088369 -4.3060694 -4.3037944 -4.2991962 -4.2874327 -4.2690706 -4.2498188 -4.2274818 -4.18682 -4.1299276 -4.0803485 -4.0439539 -4.011302][-4.325573 -4.3200793 -4.3153844 -4.3122749 -4.3105497 -4.3071675 -4.2990046 -4.2846942 -4.2660637 -4.2435308 -4.2100015 -4.1659846 -4.1244178 -4.096921 -4.0722241]]...]
INFO - root - 2017-12-06 05:59:14.277165: step 3010, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 77h:38m:34s remains)
INFO - root - 2017-12-06 05:59:22.855032: step 3020, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 77h:23m:41s remains)
INFO - root - 2017-12-06 05:59:31.412547: step 3030, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 78h:45m:19s remains)
INFO - root - 2017-12-06 05:59:40.173138: step 3040, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 79h:52m:23s remains)
INFO - root - 2017-12-06 05:59:48.814204: step 3050, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 79h:37m:04s remains)
INFO - root - 2017-12-06 05:59:57.717415: step 3060, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.912 sec/batch; 83h:28m:28s remains)
INFO - root - 2017-12-06 06:00:06.383731: step 3070, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 77h:50m:22s remains)
INFO - root - 2017-12-06 06:00:15.007551: step 3080, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 79h:04m:43s remains)
INFO - root - 2017-12-06 06:00:23.474887: step 3090, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.872 sec/batch; 79h:44m:43s remains)
INFO - root - 2017-12-06 06:00:32.021276: step 3100, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 81h:56m:50s remains)
2017-12-06 06:00:32.756721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2593875 -4.2496047 -4.2416744 -4.2379 -4.2314563 -4.2327828 -4.2402678 -4.2498236 -4.2573781 -4.2543068 -4.2408042 -4.2188535 -4.2017069 -4.1889458 -4.1861467][-4.2630844 -4.2503209 -4.2419019 -4.2383733 -4.2340407 -4.2357578 -4.2457814 -4.2604084 -4.2698274 -4.2656932 -4.2533565 -4.2335634 -4.2181354 -4.2105131 -4.2170815][-4.2677016 -4.2527518 -4.2433162 -4.2372317 -4.2322154 -4.2317 -4.2376556 -4.251204 -4.2624731 -4.2634811 -4.2601166 -4.2515392 -4.2444973 -4.2454286 -4.2583318][-4.2697968 -4.2551489 -4.2426758 -4.2304573 -4.2167397 -4.202291 -4.1955252 -4.2057786 -4.225563 -4.2411947 -4.2541456 -4.2617722 -4.2660255 -4.2730389 -4.2866611][-4.2600408 -4.2405367 -4.221889 -4.2008762 -4.1733241 -4.1417208 -4.1226783 -4.1336927 -4.1680222 -4.2048135 -4.2362289 -4.256803 -4.2690082 -4.2784667 -4.2908616][-4.222271 -4.1930709 -4.1659694 -4.1350484 -4.0946841 -4.0519876 -4.0314975 -4.0515418 -4.1045256 -4.162694 -4.208941 -4.2378378 -4.25428 -4.2673187 -4.2832131][-4.160593 -4.1238756 -4.0911455 -4.0512872 -3.9995866 -3.951956 -3.9393046 -3.9727621 -4.0436397 -4.1213446 -4.1812682 -4.2192512 -4.2375727 -4.2516179 -4.2718487][-4.1067777 -4.0658464 -4.0292234 -3.9823248 -3.9229488 -3.8752041 -3.8737891 -3.9231734 -4.0067778 -4.0968976 -4.1646171 -4.207242 -4.2226534 -4.2331562 -4.2541051][-4.0960593 -4.0576916 -4.0222955 -3.9782999 -3.9269969 -3.8933337 -3.9063809 -3.9614813 -4.0418696 -4.12192 -4.1796 -4.2117572 -4.2155733 -4.2182851 -4.2366753][-4.1243157 -4.0906034 -4.058629 -4.0213614 -3.988976 -3.9793012 -4.0076518 -4.0597935 -4.1242552 -4.1826644 -4.2186871 -4.2307611 -4.2197704 -4.2139149 -4.2258439][-4.1777735 -4.1525817 -4.1280351 -4.1007433 -4.0842323 -4.090601 -4.1244564 -4.1653581 -4.2074275 -4.2435665 -4.2622662 -4.2593465 -4.2383971 -4.2266569 -4.230567][-4.2326221 -4.220777 -4.2091565 -4.1932039 -4.183589 -4.1919684 -4.2170858 -4.2421069 -4.2639728 -4.2827225 -4.2931361 -4.2850776 -4.2622442 -4.24785 -4.24494][-4.2647433 -4.2633681 -4.2619057 -4.2546983 -4.2478361 -4.2505889 -4.2637281 -4.27461 -4.2839103 -4.2930746 -4.3000026 -4.293644 -4.2741055 -4.2585106 -4.2494984][-4.2661748 -4.27161 -4.2775245 -4.277873 -4.273406 -4.2703772 -4.2717552 -4.2710652 -4.2697849 -4.2729058 -4.278543 -4.276217 -4.2612205 -4.2441754 -4.2322779][-4.2436047 -4.2522912 -4.2625551 -4.2672653 -4.2637377 -4.2558017 -4.2468829 -4.2363091 -4.2292428 -4.2292547 -4.2337589 -4.2330284 -4.2223945 -4.2105637 -4.202457]]...]
INFO - root - 2017-12-06 06:00:41.332174: step 3110, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 80h:20m:45s remains)
INFO - root - 2017-12-06 06:00:49.894945: step 3120, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 77h:58m:29s remains)
INFO - root - 2017-12-06 06:00:58.590565: step 3130, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 78h:10m:53s remains)
INFO - root - 2017-12-06 06:01:07.131994: step 3140, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 78h:44m:16s remains)
INFO - root - 2017-12-06 06:01:15.747630: step 3150, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 76h:51m:49s remains)
INFO - root - 2017-12-06 06:01:24.473216: step 3160, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 78h:31m:13s remains)
INFO - root - 2017-12-06 06:01:33.340494: step 3170, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 76h:50m:18s remains)
INFO - root - 2017-12-06 06:01:42.043107: step 3180, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.857 sec/batch; 78h:25m:02s remains)
INFO - root - 2017-12-06 06:01:50.617955: step 3190, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.876 sec/batch; 80h:07m:42s remains)
INFO - root - 2017-12-06 06:01:59.155204: step 3200, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 80h:29m:01s remains)
2017-12-06 06:01:59.912037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.317802 -4.3083663 -4.2984858 -4.2937055 -4.2977233 -4.3039608 -4.3029895 -4.2982111 -4.2918324 -4.2873354 -4.284759 -4.282342 -4.2841907 -4.2924905 -4.3026466][-4.3012943 -4.2833495 -4.2660551 -4.2569532 -4.2641921 -4.2788124 -4.2806005 -4.2739787 -4.2658024 -4.262279 -4.2630196 -4.263042 -4.2669449 -4.2775908 -4.2876558][-4.2829404 -4.25398 -4.2269454 -4.21124 -4.2191548 -4.2399292 -4.2421489 -4.2317333 -4.2232122 -4.2250056 -4.232976 -4.2398443 -4.2502441 -4.2654314 -4.2760658][-4.2597756 -4.2212491 -4.1855669 -4.1672392 -4.1744356 -4.196651 -4.190238 -4.1692281 -4.1631837 -4.1719756 -4.1881356 -4.2037773 -4.2228646 -4.2454143 -4.26186][-4.2354164 -4.1888204 -4.1454663 -4.1233292 -4.12579 -4.1415033 -4.1237879 -4.0943923 -4.0926533 -4.1087723 -4.1310043 -4.1554132 -4.1863847 -4.2216797 -4.2469931][-4.2117977 -4.1603394 -4.1088233 -4.0768981 -4.0659823 -4.0689054 -4.0432177 -4.0128126 -4.0128202 -4.0303755 -4.0513663 -4.078969 -4.1264315 -4.1841173 -4.2261934][-4.1816282 -4.128479 -4.0716858 -4.0301385 -4.0075922 -3.9981132 -3.9660749 -3.9285662 -3.9181886 -3.9229598 -3.9342659 -3.9673803 -4.0390263 -4.1276436 -4.1960373][-4.1364202 -4.0789084 -4.0173469 -3.9679582 -3.9374907 -3.9251375 -3.8944755 -3.8477969 -3.8220661 -3.8155019 -3.8231306 -3.8631816 -3.9564881 -4.0752273 -4.1663132][-4.0985732 -4.0435653 -3.9859872 -3.937571 -3.9048851 -3.8899617 -3.8594382 -3.8088098 -3.7819364 -3.7799129 -3.794214 -3.839911 -3.9397838 -4.065455 -4.1615911][-4.1060228 -4.0643859 -4.0205231 -3.9825976 -3.9525604 -3.9350855 -3.9072902 -3.8643649 -3.8492017 -3.8592741 -3.8830271 -3.928874 -4.01288 -4.1147437 -4.1936116][-4.1489449 -4.1207013 -4.0885644 -4.0651703 -4.0453763 -4.0316095 -4.0131092 -3.9864626 -3.9801488 -3.9936216 -4.0167837 -4.0554104 -4.1155806 -4.1870546 -4.2434196][-4.2023025 -4.1845374 -4.1651387 -4.1571746 -4.1485658 -4.1409459 -4.1330166 -4.1239591 -4.1243525 -4.133235 -4.1495104 -4.1776552 -4.2170763 -4.2617822 -4.2960291][-4.2538681 -4.247231 -4.2391186 -4.2383304 -4.2367778 -4.2327466 -4.2292485 -4.22825 -4.2318044 -4.2384849 -4.2506986 -4.268991 -4.2928157 -4.3155031 -4.3308091][-4.290575 -4.2892814 -4.2868938 -4.2882981 -4.2889485 -4.2881913 -4.286829 -4.2862859 -4.29016 -4.2951427 -4.3033018 -4.3152914 -4.3290539 -4.3387971 -4.344943][-4.3165803 -4.3163581 -4.3168869 -4.3198152 -4.321188 -4.32156 -4.3204074 -4.3195605 -4.3213549 -4.3239155 -4.3283052 -4.3344564 -4.3415337 -4.3466005 -4.3494682]]...]
INFO - root - 2017-12-06 06:02:08.507914: step 3210, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 78h:48m:18s remains)
INFO - root - 2017-12-06 06:02:17.054936: step 3220, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 80h:27m:28s remains)
INFO - root - 2017-12-06 06:02:25.789461: step 3230, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 80h:28m:29s remains)
INFO - root - 2017-12-06 06:02:34.281203: step 3240, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 76h:14m:38s remains)
INFO - root - 2017-12-06 06:02:42.801196: step 3250, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 77h:11m:19s remains)
INFO - root - 2017-12-06 06:02:51.532081: step 3260, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 80h:19m:59s remains)
INFO - root - 2017-12-06 06:03:00.250740: step 3270, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 80h:20m:43s remains)
INFO - root - 2017-12-06 06:03:08.940576: step 3280, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 80h:05m:49s remains)
INFO - root - 2017-12-06 06:03:17.523418: step 3290, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 78h:28m:08s remains)
INFO - root - 2017-12-06 06:03:26.234022: step 3300, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 78h:54m:04s remains)
2017-12-06 06:03:26.980510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2202191 -4.1901884 -4.1904387 -4.2196555 -4.2549148 -4.2854247 -4.3086219 -4.3110623 -4.2979279 -4.273665 -4.2479515 -4.2310147 -4.2154808 -4.2180386 -4.2408686][-4.2284589 -4.2081542 -4.214869 -4.2450261 -4.2780066 -4.3058577 -4.3297534 -4.3392286 -4.3356309 -4.31988 -4.3000169 -4.2835488 -4.265697 -4.2610464 -4.2694311][-4.2152 -4.2039757 -4.2202744 -4.2542543 -4.2866712 -4.3117142 -4.333529 -4.3471355 -4.3506265 -4.3450904 -4.3377557 -4.3316007 -4.3204465 -4.3111539 -4.3019986][-4.2028475 -4.1992741 -4.221406 -4.2546954 -4.2822356 -4.3004389 -4.3136654 -4.3218961 -4.3245945 -4.3277807 -4.33451 -4.3425269 -4.3446712 -4.3400364 -4.32441][-4.2222214 -4.223032 -4.2397943 -4.2591395 -4.2708888 -4.2729511 -4.2702336 -4.26589 -4.2619615 -4.2701068 -4.2904143 -4.3173404 -4.3379655 -4.3465629 -4.3359723][-4.2541232 -4.256618 -4.2603054 -4.2597017 -4.2501783 -4.2291923 -4.2023 -4.1759357 -4.15734 -4.16698 -4.2031775 -4.2533159 -4.2996097 -4.3302832 -4.3347974][-4.2696953 -4.2775493 -4.2736249 -4.2570658 -4.2263241 -4.1801848 -4.1237712 -4.068202 -4.0297403 -4.0438437 -4.1008835 -4.1741438 -4.244328 -4.2948594 -4.3138018][-4.2688169 -4.287951 -4.2861676 -4.2617073 -4.2133584 -4.1440573 -4.0592604 -3.9709425 -3.9084196 -3.9282546 -4.0087633 -4.1027937 -4.1912332 -4.2544918 -4.2835417][-4.2717781 -4.3020329 -4.3072071 -4.2817755 -4.2255535 -4.1443973 -4.0435724 -3.9336689 -3.8534527 -3.8767524 -3.9697728 -4.0721049 -4.1664019 -4.2311544 -4.2641497][-4.2909479 -4.3216372 -4.3292418 -4.3065438 -4.2537003 -4.1788559 -4.0891719 -3.9943328 -3.9283435 -3.9509566 -4.02894 -4.1120329 -4.1899605 -4.2448506 -4.2762046][-4.3222132 -4.3448253 -4.3479753 -4.3263555 -4.2822552 -4.2263632 -4.1652589 -4.1071782 -4.071322 -4.0892043 -4.1359415 -4.1861925 -4.237226 -4.2771983 -4.3048811][-4.3367972 -4.3482938 -4.3454895 -4.3249307 -4.2924328 -4.2596383 -4.2292161 -4.2064776 -4.1967731 -4.2083449 -4.2257171 -4.2447691 -4.2701855 -4.2964916 -4.3227577][-4.332521 -4.3351364 -4.32754 -4.3095946 -4.2901144 -4.2794938 -4.2744241 -4.2760921 -4.2815685 -4.2866621 -4.281446 -4.2737532 -4.2747145 -4.2865829 -4.3112078][-4.3203282 -4.3188591 -4.307538 -4.2898741 -4.2796845 -4.2847209 -4.2972541 -4.3136015 -4.3266745 -4.3270955 -4.3076472 -4.2798815 -4.2615418 -4.2607613 -4.2830906][-4.3010859 -4.3007274 -4.2900243 -4.2742381 -4.2693591 -4.2827873 -4.3035703 -4.3246408 -4.33966 -4.3394294 -4.3173227 -4.2825966 -4.2547088 -4.2458172 -4.2645421]]...]
INFO - root - 2017-12-06 06:03:35.705563: step 3310, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.829 sec/batch; 75h:47m:37s remains)
INFO - root - 2017-12-06 06:03:44.171579: step 3320, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.855 sec/batch; 78h:11m:45s remains)
INFO - root - 2017-12-06 06:03:52.921733: step 3330, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 80h:54m:32s remains)
INFO - root - 2017-12-06 06:04:01.513745: step 3340, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 81h:24m:22s remains)
INFO - root - 2017-12-06 06:04:10.125484: step 3350, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 79h:05m:15s remains)
INFO - root - 2017-12-06 06:04:18.663413: step 3360, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.856 sec/batch; 78h:13m:03s remains)
INFO - root - 2017-12-06 06:04:27.511679: step 3370, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 82h:47m:56s remains)
INFO - root - 2017-12-06 06:04:36.214591: step 3380, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 78h:31m:13s remains)
INFO - root - 2017-12-06 06:04:44.739048: step 3390, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 79h:58m:09s remains)
INFO - root - 2017-12-06 06:04:53.275521: step 3400, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 78h:47m:46s remains)
2017-12-06 06:04:54.074814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0725875 -4.109066 -4.1499748 -4.1911573 -4.2184043 -4.2238784 -4.2184615 -4.2088928 -4.20607 -4.2125282 -4.2229991 -4.2319889 -4.2306509 -4.2292242 -4.226531][-4.07762 -4.1273551 -4.1768436 -4.2209382 -4.2445335 -4.2480731 -4.2439046 -4.2305465 -4.2183514 -4.2179255 -4.2215185 -4.2260261 -4.2278728 -4.2242093 -4.2090468][-4.1173882 -4.1660919 -4.2128973 -4.248775 -4.2627597 -4.2614803 -4.2533526 -4.2342472 -4.2154527 -4.2125144 -4.2174735 -4.2243862 -4.2323818 -4.2274089 -4.2005167][-4.1655779 -4.2050972 -4.2425632 -4.2646055 -4.2663989 -4.2573695 -4.2394614 -4.2140503 -4.1934791 -4.1952538 -4.2087803 -4.2211971 -4.2332582 -4.2282085 -4.2009063][-4.1993361 -4.2260365 -4.2486768 -4.2566848 -4.2507195 -4.23227 -4.2030735 -4.1647291 -4.1366477 -4.1454782 -4.172699 -4.19792 -4.2183027 -4.2205262 -4.2069011][-4.2015724 -4.2201457 -4.2272725 -4.2240195 -4.2096286 -4.1790123 -4.1307564 -4.0702095 -4.0294027 -4.0513568 -4.1054397 -4.1560173 -4.19335 -4.2114983 -4.2158403][-4.1850038 -4.1917114 -4.1866126 -4.1743865 -4.1515236 -4.1082072 -4.043138 -3.9602821 -3.9093232 -3.9597356 -4.0601969 -4.1428342 -4.1985335 -4.2293882 -4.2462296][-4.1823006 -4.1743517 -4.1596189 -4.1443272 -4.1195354 -4.074697 -4.0063372 -3.9275005 -3.8974314 -3.9790461 -4.0966554 -4.1804047 -4.2324634 -4.2569208 -4.2701697][-4.1936755 -4.1790252 -4.16424 -4.1529694 -4.1372747 -4.1062613 -4.0669165 -4.0343065 -4.0381694 -4.1018324 -4.1821265 -4.2313447 -4.2548847 -4.2582164 -4.2605014][-4.1970859 -4.1839213 -4.1773787 -4.1773462 -4.1770344 -4.1645093 -4.1543412 -4.1509366 -4.1602659 -4.1932878 -4.2347355 -4.2509589 -4.2450132 -4.2296944 -4.2246671][-4.2009397 -4.1827154 -4.1804957 -4.1957693 -4.2083263 -4.2057853 -4.204113 -4.2082 -4.2153611 -4.2310977 -4.2504983 -4.245285 -4.2210054 -4.1920214 -4.1793022][-4.2058244 -4.1808262 -4.17332 -4.1896296 -4.2093039 -4.2116213 -4.2115583 -4.2182188 -4.2257576 -4.237751 -4.249949 -4.2342978 -4.19525 -4.1586051 -4.14199][-4.2010431 -4.1743226 -4.1587849 -4.1700797 -4.1890173 -4.1919255 -4.1925006 -4.2022982 -4.2131925 -4.2252235 -4.2307849 -4.211442 -4.1714687 -4.1335454 -4.1200261][-4.1949468 -4.1721573 -4.1583257 -4.1640854 -4.1742558 -4.171834 -4.167357 -4.1739216 -4.1843791 -4.1991653 -4.2073965 -4.1944647 -4.1630983 -4.1316314 -4.1245112][-4.2081733 -4.1924357 -4.1823926 -4.178586 -4.1724319 -4.1617727 -4.1523566 -4.1509161 -4.1598439 -4.1789246 -4.1916556 -4.186779 -4.1641092 -4.14259 -4.1430964]]...]
INFO - root - 2017-12-06 06:05:02.671040: step 3410, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 77h:48m:04s remains)
INFO - root - 2017-12-06 06:05:11.334029: step 3420, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 81h:16m:55s remains)
INFO - root - 2017-12-06 06:05:19.984209: step 3430, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 79h:46m:15s remains)
INFO - root - 2017-12-06 06:05:28.512315: step 3440, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 77h:55m:30s remains)
INFO - root - 2017-12-06 06:05:37.188524: step 3450, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 76h:40m:13s remains)
INFO - root - 2017-12-06 06:05:45.740415: step 3460, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 76h:49m:52s remains)
INFO - root - 2017-12-06 06:05:54.402782: step 3470, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 77h:57m:51s remains)
INFO - root - 2017-12-06 06:06:02.926689: step 3480, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 77h:05m:20s remains)
INFO - root - 2017-12-06 06:06:11.447495: step 3490, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 79h:31m:03s remains)
INFO - root - 2017-12-06 06:06:20.059319: step 3500, loss = 2.03, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 82h:32m:32s remains)
2017-12-06 06:06:20.919183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2273259 -4.2309942 -4.2345161 -4.2405853 -4.2480164 -4.2508283 -4.2484016 -4.2491612 -4.2454853 -4.2431574 -4.2429214 -4.2314253 -4.2106328 -4.1869559 -4.1632462][-4.2178206 -4.2157383 -4.216702 -4.224576 -4.2338257 -4.2372346 -4.2318168 -4.2313914 -4.2296844 -4.229918 -4.2335978 -4.2211251 -4.1935382 -4.1613655 -4.1324468][-4.182281 -4.1813226 -4.1859636 -4.199389 -4.2119346 -4.2154646 -4.206171 -4.2060704 -4.2077665 -4.2127032 -4.2200127 -4.20926 -4.1766443 -4.1379762 -4.1058497][-4.1450014 -4.1456318 -4.1550288 -4.1707892 -4.18232 -4.1794276 -4.1633868 -4.1638665 -4.1722493 -4.1825552 -4.1915855 -4.1825352 -4.1451416 -4.099401 -4.0628948][-4.1376171 -4.1363325 -4.1486311 -4.1642795 -4.1705527 -4.158061 -4.1341181 -4.1338658 -4.1478882 -4.1648602 -4.1753626 -4.1662307 -4.1238847 -4.0696983 -4.0318689][-4.1558518 -4.1412859 -4.1478925 -4.1577468 -4.1625915 -4.1475215 -4.1234145 -4.1270127 -4.1477351 -4.1694407 -4.1827459 -4.1758 -4.1384845 -4.0840507 -4.04652][-4.1573434 -4.132112 -4.1322975 -4.1344571 -4.1330214 -4.1153431 -4.0953693 -4.1062756 -4.1373773 -4.1670513 -4.1845975 -4.1845737 -4.1582394 -4.1052036 -4.0601416][-4.1460128 -4.1190214 -4.1121526 -4.1061149 -4.0945406 -4.0717359 -4.0541615 -4.0675592 -4.1044683 -4.1358304 -4.1554713 -4.162528 -4.1424088 -4.090826 -4.0446358][-4.1426244 -4.1219168 -4.1135736 -4.1071649 -4.0940228 -4.0743179 -4.0624385 -4.0735888 -4.1014113 -4.1207018 -4.1328855 -4.1383119 -4.1204572 -4.0769358 -4.0372772][-4.14743 -4.1365957 -4.1380291 -4.1450706 -4.1437211 -4.1366005 -4.1327605 -4.1387887 -4.1508403 -4.1549859 -4.1549926 -4.1538315 -4.140408 -4.1099238 -4.0814385][-4.1677728 -4.1657896 -4.1769891 -4.1933761 -4.2015152 -4.2047076 -4.2068605 -4.2106948 -4.2125664 -4.2091103 -4.202733 -4.1974096 -4.18657 -4.1667237 -4.1505003][-4.1807408 -4.1870794 -4.2026439 -4.222878 -4.2381477 -4.2484159 -4.2576609 -4.2646413 -4.2665796 -4.2637634 -4.2580996 -4.2511582 -4.2402449 -4.2247219 -4.2141843][-4.2009134 -4.20941 -4.2251272 -4.2463346 -4.2654824 -4.2791605 -4.2910995 -4.3011541 -4.3074985 -4.3092222 -4.3061924 -4.2999048 -4.2892342 -4.2763777 -4.2675891][-4.22052 -4.2278757 -4.24368 -4.263597 -4.28202 -4.2945313 -4.3034906 -4.3114524 -4.318327 -4.3227816 -4.3232679 -4.3201146 -4.3123364 -4.3025918 -4.2922192][-4.2336698 -4.2388191 -4.2525487 -4.2699647 -4.2870297 -4.2993464 -4.3060517 -4.309341 -4.311245 -4.3141551 -4.3160868 -4.3155928 -4.3122253 -4.3065376 -4.2972145]]...]
INFO - root - 2017-12-06 06:06:29.603124: step 3510, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 80h:51m:08s remains)
INFO - root - 2017-12-06 06:06:38.350482: step 3520, loss = 2.10, batch loss = 2.04 (10.4 examples/sec; 0.771 sec/batch; 70h:26m:38s remains)
INFO - root - 2017-12-06 06:06:47.207189: step 3530, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 79h:48m:15s remains)
INFO - root - 2017-12-06 06:06:55.845432: step 3540, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 79h:13m:48s remains)
INFO - root - 2017-12-06 06:07:04.519494: step 3550, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 79h:16m:24s remains)
INFO - root - 2017-12-06 06:07:13.041087: step 3560, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 80h:20m:45s remains)
INFO - root - 2017-12-06 06:07:21.740734: step 3570, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.818 sec/batch; 74h:42m:36s remains)
INFO - root - 2017-12-06 06:07:30.516509: step 3580, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 80h:37m:57s remains)
INFO - root - 2017-12-06 06:07:39.137732: step 3590, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.767 sec/batch; 70h:06m:26s remains)
INFO - root - 2017-12-06 06:07:47.745531: step 3600, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 79h:58m:57s remains)
2017-12-06 06:07:48.543134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3166676 -4.32336 -4.3208361 -4.3045616 -4.274405 -4.2350674 -4.2042265 -4.180088 -4.1592236 -4.1530604 -4.1795959 -4.2354231 -4.28193 -4.3068614 -4.3153396][-4.3118172 -4.3192139 -4.3185525 -4.306612 -4.280715 -4.2439575 -4.2211227 -4.2010212 -4.185823 -4.1867981 -4.2111554 -4.2507691 -4.281817 -4.2967858 -4.3016491][-4.2879009 -4.2965984 -4.293046 -4.2798324 -4.2550578 -4.2187634 -4.1998277 -4.1885486 -4.1875505 -4.1965027 -4.2190495 -4.2470126 -4.2622156 -4.2653985 -4.2694535][-4.2634611 -4.2659278 -4.2537141 -4.2318034 -4.2028952 -4.1658559 -4.1490378 -4.1516919 -4.1681247 -4.1822648 -4.2004561 -4.2196708 -4.2235789 -4.2190461 -4.2260923][-4.2448645 -4.2317696 -4.2040629 -4.1694088 -4.129425 -4.0834785 -4.0670872 -4.0851808 -4.1138105 -4.1405292 -4.1671462 -4.1828084 -4.1805153 -4.1728606 -4.1831932][-4.2248516 -4.1934605 -4.1521311 -4.1074471 -4.0545368 -3.9944487 -3.9729204 -4.0027294 -4.0468802 -4.0950575 -4.1380506 -4.1526833 -4.1476274 -4.1413403 -4.1573081][-4.2105956 -4.1696315 -4.1265926 -4.089479 -4.0423651 -3.9822292 -3.9560976 -3.9863586 -4.0315595 -4.090095 -4.1372995 -4.1422014 -4.125855 -4.1133289 -4.13324][-4.2097287 -4.1761937 -4.1449142 -4.1259356 -4.0996842 -4.0612841 -4.0461884 -4.0699396 -4.098959 -4.1428 -4.1765213 -4.1652822 -4.1358924 -4.11967 -4.1416879][-4.218657 -4.1990056 -4.1842284 -4.1833444 -4.1786342 -4.1599736 -4.1541438 -4.17318 -4.1865616 -4.2144279 -4.2329812 -4.2101612 -4.177042 -4.16604 -4.1906304][-4.2346611 -4.2261376 -4.2248363 -4.2356505 -4.2432203 -4.236928 -4.23652 -4.25137 -4.2535524 -4.2687397 -4.2756271 -4.2510977 -4.2251835 -4.2254577 -4.2505965][-4.2527094 -4.2495284 -4.25469 -4.2708683 -4.2836809 -4.2853479 -4.2896724 -4.2971911 -4.2904024 -4.29404 -4.2916527 -4.274178 -4.2643709 -4.27692 -4.3002105][-4.2642274 -4.2602277 -4.2650189 -4.2813015 -4.2955823 -4.2998238 -4.3034358 -4.3045211 -4.2950382 -4.2939329 -4.2897158 -4.2852674 -4.2887244 -4.307189 -4.3306413][-4.2626848 -4.2548585 -4.2569079 -4.270442 -4.28465 -4.2899108 -4.2909131 -4.2895346 -4.2829995 -4.2856946 -4.2897706 -4.2954764 -4.3049207 -4.3229284 -4.34412][-4.2656727 -4.25237 -4.2471819 -4.254281 -4.2640533 -4.2660179 -4.2633452 -4.2620144 -4.2637362 -4.2755055 -4.2906947 -4.3048582 -4.3177948 -4.3349433 -4.3534336][-4.2796 -4.2623076 -4.2487559 -4.2438889 -4.2415762 -4.2352543 -4.2275519 -4.2271485 -4.2406678 -4.2614164 -4.2833691 -4.3031874 -4.3187957 -4.3392639 -4.3594308]]...]
INFO - root - 2017-12-06 06:07:57.159538: step 3610, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 78h:03m:32s remains)
INFO - root - 2017-12-06 06:08:05.805948: step 3620, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.851 sec/batch; 77h:45m:03s remains)
INFO - root - 2017-12-06 06:08:14.257528: step 3630, loss = 2.04, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 75h:21m:44s remains)
INFO - root - 2017-12-06 06:08:22.825572: step 3640, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 77h:19m:33s remains)
INFO - root - 2017-12-06 06:08:31.390090: step 3650, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 80h:50m:13s remains)
INFO - root - 2017-12-06 06:08:40.055630: step 3660, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.866 sec/batch; 79h:04m:16s remains)
INFO - root - 2017-12-06 06:08:48.608361: step 3670, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 77h:36m:42s remains)
INFO - root - 2017-12-06 06:08:57.387553: step 3680, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 80h:52m:22s remains)
INFO - root - 2017-12-06 06:09:05.984639: step 3690, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 80h:17m:20s remains)
INFO - root - 2017-12-06 06:09:14.898400: step 3700, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 79h:12m:06s remains)
2017-12-06 06:09:15.668439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2401228 -4.2335863 -4.2366714 -4.2484574 -4.2614188 -4.2755308 -4.2854772 -4.2767949 -4.2569752 -4.2316365 -4.2227197 -4.23696 -4.273315 -4.3028803 -4.3181062][-4.1785183 -4.1775627 -4.1889992 -4.211298 -4.23086 -4.2475653 -4.2577844 -4.2412791 -4.2119889 -4.1768003 -4.1688442 -4.1902676 -4.2387233 -4.280705 -4.3059926][-4.1556263 -4.1613355 -4.1790671 -4.2041011 -4.2235441 -4.2386775 -4.24077 -4.2129612 -4.1777287 -4.1413817 -4.1407371 -4.1678309 -4.2181325 -4.2600317 -4.2894044][-4.1700382 -4.1794977 -4.1995139 -4.2238641 -4.2392154 -4.2414136 -4.2201166 -4.1751804 -4.1406655 -4.11933 -4.1384273 -4.1720133 -4.2214537 -4.257462 -4.2806687][-4.1807232 -4.1903458 -4.2115397 -4.2336912 -4.2425303 -4.2255373 -4.1724849 -4.1011429 -4.0718217 -4.0834126 -4.1356435 -4.1837935 -4.2370439 -4.2682586 -4.2843742][-4.1904597 -4.1954579 -4.2110071 -4.2260194 -4.22162 -4.1754346 -4.0820856 -3.9834855 -3.9805403 -4.0456014 -4.1325173 -4.197082 -4.2554364 -4.282939 -4.2928939][-4.2089109 -4.2046394 -4.2067528 -4.2036285 -4.1763949 -4.0860095 -3.9322033 -3.7942724 -3.8330035 -3.9708598 -4.1024766 -4.1890788 -4.2583008 -4.2884049 -4.297153][-4.233489 -4.226851 -4.216434 -4.1911211 -4.1337481 -3.9973941 -3.7686262 -3.572926 -3.672982 -3.8937702 -4.06771 -4.1754856 -4.2524352 -4.2865124 -4.2962227][-4.2641807 -4.256094 -4.2371426 -4.2014875 -4.1305866 -3.9904168 -3.7647028 -3.596241 -3.7118793 -3.9242682 -4.0841532 -4.1832047 -4.2470584 -4.2753086 -4.2867417][-4.2785296 -4.2658172 -4.247736 -4.214098 -4.1531162 -4.0505466 -3.897814 -3.8127141 -3.910079 -4.0506997 -4.1551108 -4.2193837 -4.2525177 -4.26417 -4.2735515][-4.2842145 -4.2719026 -4.2558336 -4.2257428 -4.1815138 -4.1171656 -4.0347981 -4.0097861 -4.0849528 -4.170774 -4.229785 -4.26443 -4.27374 -4.2703285 -4.2725167][-4.2907238 -4.2823029 -4.2686353 -4.2424393 -4.211194 -4.1733179 -4.1339054 -4.1372705 -4.1925054 -4.2413535 -4.2728558 -4.29301 -4.2942023 -4.2843442 -4.2814813][-4.2940464 -4.2886415 -4.2803764 -4.2632203 -4.245266 -4.2253757 -4.2042089 -4.2093163 -4.2450514 -4.2717547 -4.2879686 -4.2989326 -4.2997289 -4.2920732 -4.2901073][-4.2907515 -4.2897334 -4.2864637 -4.2779689 -4.27168 -4.2589397 -4.2411118 -4.2425838 -4.2676535 -4.2882872 -4.2993326 -4.3075271 -4.3090897 -4.3045111 -4.3019705][-4.2818003 -4.2820239 -4.282671 -4.2812028 -4.2785344 -4.265995 -4.2475328 -4.2489824 -4.2743669 -4.295022 -4.3030434 -4.3096962 -4.3144431 -4.3134933 -4.3116651]]...]
INFO - root - 2017-12-06 06:09:24.280790: step 3710, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 77h:56m:51s remains)
INFO - root - 2017-12-06 06:09:32.790381: step 3720, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 76h:09m:07s remains)
INFO - root - 2017-12-06 06:09:41.272210: step 3730, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 78h:08m:37s remains)
INFO - root - 2017-12-06 06:09:49.902571: step 3740, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 77h:32m:10s remains)
INFO - root - 2017-12-06 06:09:58.383974: step 3750, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 77h:48m:13s remains)
INFO - root - 2017-12-06 06:10:07.118509: step 3760, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.908 sec/batch; 82h:53m:38s remains)
INFO - root - 2017-12-06 06:10:15.791289: step 3770, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 77h:34m:23s remains)
INFO - root - 2017-12-06 06:10:24.498799: step 3780, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 76h:39m:32s remains)
INFO - root - 2017-12-06 06:10:33.109719: step 3790, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 81h:59m:29s remains)
INFO - root - 2017-12-06 06:10:42.020234: step 3800, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 82h:37m:48s remains)
2017-12-06 06:10:42.795928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3329597 -4.3253322 -4.3162475 -4.3121271 -4.310895 -4.312551 -4.3122282 -4.3081059 -4.3004222 -4.2949905 -4.2895708 -4.2857375 -4.28662 -4.2913609 -4.2928715][-4.3339586 -4.3227563 -4.310463 -4.305367 -4.3051662 -4.30782 -4.3057761 -4.2994809 -4.288662 -4.2815151 -4.2767391 -4.271131 -4.2718611 -4.2771077 -4.27871][-4.3293252 -4.3131776 -4.2968836 -4.2916794 -4.2959433 -4.3044119 -4.3042197 -4.2977653 -4.2870073 -4.2794628 -4.2754903 -4.2676435 -4.266788 -4.269856 -4.2696552][-4.3151979 -4.2901006 -4.2663569 -4.2576375 -4.2657518 -4.2808805 -4.284987 -4.2839065 -4.2814636 -4.2762222 -4.274457 -4.2674875 -4.2673597 -4.2667241 -4.2598338][-4.290802 -4.2554259 -4.2200589 -4.19685 -4.1928144 -4.2033677 -4.2068825 -4.2166562 -4.2346225 -4.2452941 -4.2521596 -4.2494192 -4.2526503 -4.2535973 -4.2442222][-4.2645311 -4.2217426 -4.1752629 -4.1365414 -4.1108069 -4.1007609 -4.0853591 -4.0972443 -4.1480994 -4.1890173 -4.2130623 -4.2212138 -4.2364926 -4.2436008 -4.2348895][-4.2498326 -4.2052722 -4.1523151 -4.0978341 -4.0438757 -3.9950147 -3.9318805 -3.9224796 -4.0095491 -4.0967269 -4.1488113 -4.1774821 -4.2118244 -4.2333279 -4.2309957][-4.2493448 -4.2107968 -4.1600456 -4.0993133 -4.0294838 -3.9472156 -3.8259361 -3.7664196 -3.8688335 -3.9894407 -4.0735207 -4.1283331 -4.1815276 -4.2198758 -4.2310333][-4.2707462 -4.2494946 -4.21455 -4.1664591 -4.1074977 -4.0309348 -3.9110172 -3.836729 -3.8974612 -3.9917338 -4.0685148 -4.1275911 -4.1836247 -4.2251282 -4.2421207][-4.2978597 -4.2910938 -4.2762132 -4.2474093 -4.2113166 -4.1589975 -4.0831151 -4.0377941 -4.063942 -4.1077752 -4.1469569 -4.180892 -4.2213249 -4.25204 -4.2643104][-4.3147378 -4.3162045 -4.3150311 -4.3059449 -4.2903309 -4.2598743 -4.2207875 -4.2037654 -4.2148857 -4.226943 -4.2355461 -4.2460608 -4.2665334 -4.2833652 -4.2894788][-4.3119116 -4.3162889 -4.32122 -4.3238912 -4.3243065 -4.3137341 -4.2980361 -4.2964826 -4.3037481 -4.3060284 -4.3029275 -4.3015418 -4.3067703 -4.3113823 -4.3112078][-4.3080549 -4.3132005 -4.3194766 -4.3257709 -4.3329115 -4.3338027 -4.3311734 -4.3342128 -4.3380342 -4.3383265 -4.3342695 -4.3305244 -4.3284659 -4.3281212 -4.3249969][-4.3104134 -4.3135586 -4.3182731 -4.3249249 -4.3319449 -4.3375444 -4.3402724 -4.3427796 -4.3431511 -4.3424129 -4.3389969 -4.3364553 -4.3346076 -4.3342929 -4.3310838][-4.3155084 -4.3156347 -4.3160906 -4.3191638 -4.3231845 -4.3272848 -4.3292851 -4.3297973 -4.3298044 -4.3303065 -4.3298116 -4.3297729 -4.330431 -4.3324623 -4.3305235]]...]
INFO - root - 2017-12-06 06:10:51.520282: step 3810, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 81h:03m:26s remains)
INFO - root - 2017-12-06 06:11:00.139958: step 3820, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 79h:08m:08s remains)
INFO - root - 2017-12-06 06:11:08.879095: step 3830, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 79h:40m:17s remains)
INFO - root - 2017-12-06 06:11:17.437087: step 3840, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 81h:23m:11s remains)
INFO - root - 2017-12-06 06:11:26.220057: step 3850, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 78h:25m:45s remains)
INFO - root - 2017-12-06 06:11:34.841828: step 3860, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 77h:44m:59s remains)
INFO - root - 2017-12-06 06:11:43.548202: step 3870, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 79h:14m:57s remains)
INFO - root - 2017-12-06 06:11:52.265097: step 3880, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 79h:40m:16s remains)
INFO - root - 2017-12-06 06:12:00.957907: step 3890, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 79h:25m:43s remains)
INFO - root - 2017-12-06 06:12:09.776286: step 3900, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.904 sec/batch; 82h:31m:55s remains)
2017-12-06 06:12:10.504981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.246635 -4.2488723 -4.2600965 -4.2725773 -4.2832947 -4.2819476 -4.268702 -4.2565465 -4.2584167 -4.2737322 -4.2930532 -4.3068852 -4.3116035 -4.3083286 -4.301837][-4.2644005 -4.2662454 -4.2746239 -4.2837439 -4.2877479 -4.277369 -4.2581692 -4.2432871 -4.2444229 -4.2604647 -4.2808356 -4.2945132 -4.3000526 -4.2976179 -4.2925043][-4.2769837 -4.2834482 -4.2899809 -4.293108 -4.2889071 -4.2706079 -4.2441411 -4.2252979 -4.2260613 -4.2470284 -4.2727842 -4.2882152 -4.2942228 -4.2901 -4.2833285][-4.2823105 -4.2905951 -4.29583 -4.2950277 -4.2845516 -4.2606959 -4.2288618 -4.2084055 -4.2131176 -4.2414293 -4.2738543 -4.2944555 -4.3017421 -4.2948117 -4.284771][-4.2746725 -4.2825351 -4.2858386 -4.2816234 -4.2665548 -4.2376676 -4.2012105 -4.1836052 -4.1978774 -4.2373605 -4.2783146 -4.3041649 -4.3124523 -4.3027658 -4.2883][-4.2604523 -4.2670288 -4.2674856 -4.2585187 -4.2382274 -4.2007594 -4.1577649 -4.1446681 -4.1725769 -4.22627 -4.2762752 -4.3074622 -4.3188114 -4.3107443 -4.2937284][-4.2530866 -4.2613811 -4.2598557 -4.2443871 -4.2157516 -4.169086 -4.1182775 -4.1031089 -4.1391249 -4.2044635 -4.2629237 -4.2989569 -4.31467 -4.3128424 -4.298327][-4.2544632 -4.2631063 -4.2597976 -4.2426252 -4.212956 -4.1663 -4.1096077 -4.0800667 -4.1075654 -4.1749883 -4.2396226 -4.281702 -4.3012605 -4.3033471 -4.2934313][-4.2640734 -4.2682242 -4.2635722 -4.2497468 -4.2313771 -4.2002611 -4.14773 -4.1025362 -4.1100135 -4.1656246 -4.2294044 -4.275373 -4.297205 -4.2995281 -4.2911062][-4.275382 -4.2759237 -4.2724061 -4.2642212 -4.2581239 -4.242897 -4.2024088 -4.1560163 -4.1499238 -4.1893096 -4.2445059 -4.2875113 -4.3069134 -4.3058214 -4.2940254][-4.2742434 -4.2770185 -4.2786994 -4.2757082 -4.2751279 -4.2687531 -4.2389903 -4.1989064 -4.1884661 -4.2134643 -4.2557187 -4.291523 -4.30564 -4.3002086 -4.2852812][-4.2658362 -4.2789965 -4.2903562 -4.2917047 -4.2914691 -4.2868376 -4.2645626 -4.2347012 -4.2262487 -4.2403922 -4.2668824 -4.2902474 -4.2993445 -4.2928548 -4.2774277][-4.2647228 -4.2833819 -4.2975655 -4.3001347 -4.2985039 -4.29323 -4.2773614 -4.2575274 -4.2515612 -4.2571745 -4.2703681 -4.2844496 -4.2902951 -4.2849221 -4.2728329][-4.2611966 -4.2766137 -4.2878814 -4.2897568 -4.2862659 -4.2803516 -4.2679095 -4.2546611 -4.2489491 -4.2485194 -4.2547131 -4.2638912 -4.2712054 -4.2726164 -4.2692933][-4.2465253 -4.2543607 -4.2604828 -4.2605267 -4.2573366 -4.25331 -4.2447858 -4.2350707 -4.2280335 -4.2218227 -4.22302 -4.2295165 -4.2413764 -4.2542181 -4.2626734]]...]
INFO - root - 2017-12-06 06:12:19.251970: step 3910, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 76h:35m:37s remains)
INFO - root - 2017-12-06 06:12:27.797009: step 3920, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 77h:31m:19s remains)
INFO - root - 2017-12-06 06:12:36.427327: step 3930, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 77h:31m:47s remains)
INFO - root - 2017-12-06 06:12:45.047351: step 3940, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 81h:10m:08s remains)
INFO - root - 2017-12-06 06:12:53.923409: step 3950, loss = 2.08, batch loss = 2.03 (8.9 examples/sec; 0.899 sec/batch; 82h:02m:01s remains)
INFO - root - 2017-12-06 06:13:02.582122: step 3960, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 76h:50m:39s remains)
INFO - root - 2017-12-06 06:13:11.210027: step 3970, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 76h:35m:55s remains)
INFO - root - 2017-12-06 06:13:19.751652: step 3980, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.852 sec/batch; 77h:44m:24s remains)
INFO - root - 2017-12-06 06:13:28.297024: step 3990, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 79h:43m:32s remains)
INFO - root - 2017-12-06 06:13:36.929789: step 4000, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 0.819 sec/batch; 74h:43m:27s remains)
2017-12-06 06:13:37.765915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2034478 -4.2089391 -4.239953 -4.2633848 -4.2708321 -4.2725439 -4.2710328 -4.2693114 -4.2720633 -4.2763014 -4.2875509 -4.292768 -4.287498 -4.2826819 -4.2753005][-4.19325 -4.1942105 -4.2233229 -4.2503939 -4.2589817 -4.2567024 -4.252573 -4.250154 -4.2569337 -4.2673259 -4.278553 -4.282835 -4.2796388 -4.27421 -4.2647729][-4.1878819 -4.1791506 -4.1984334 -4.2207813 -4.227499 -4.2197437 -4.2102332 -4.2093439 -4.2277422 -4.25414 -4.2740369 -4.27912 -4.2750864 -4.2661304 -4.2549777][-4.1789088 -4.1612453 -4.1714892 -4.188818 -4.1935797 -4.1843514 -4.17352 -4.1752124 -4.2001653 -4.2405148 -4.2697988 -4.2753792 -4.2670574 -4.2540941 -4.24267][-4.1748548 -4.1503172 -4.1541052 -4.1708779 -4.1762037 -4.1684837 -4.1570678 -4.157836 -4.1850576 -4.2295785 -4.2615862 -4.2658672 -4.2542048 -4.2395115 -4.2280769][-4.188096 -4.1596003 -4.1563759 -4.1674461 -4.1723886 -4.1632442 -4.1432633 -4.1338682 -4.1558123 -4.2005692 -4.2390866 -4.2505651 -4.2443023 -4.2348 -4.2249331][-4.2097096 -4.1845889 -4.1737733 -4.1759586 -4.1735778 -4.1537318 -4.1173563 -4.0881948 -4.0993052 -4.1508641 -4.2050819 -4.2354336 -4.2448049 -4.2433033 -4.2365007][-4.2193313 -4.2052546 -4.1945076 -4.1902757 -4.1780081 -4.1433253 -4.0897079 -4.0385904 -4.0358543 -4.0994582 -4.17512 -4.22851 -4.2537627 -4.2595425 -4.256516][-4.2216244 -4.2197285 -4.2127233 -4.2049856 -4.1882634 -4.1506214 -4.0917087 -4.0282564 -4.0113692 -4.075068 -4.1594715 -4.2238917 -4.2587361 -4.2698913 -4.27175][-4.2282524 -4.2353144 -4.2295141 -4.2172623 -4.2006874 -4.1727009 -4.12472 -4.0682392 -4.0452747 -4.08859 -4.1582556 -4.2163343 -4.252986 -4.2695913 -4.2778292][-4.2359653 -4.2454162 -4.24218 -4.2323527 -4.2201767 -4.2006364 -4.1648927 -4.1206331 -4.0957432 -4.1187282 -4.16859 -4.2146816 -4.2485566 -4.2679777 -4.2805591][-4.2537541 -4.2602711 -4.2590222 -4.2544847 -4.2505946 -4.2376862 -4.2085218 -4.1726208 -4.1477261 -4.1584077 -4.1932316 -4.22814 -4.2532692 -4.2702112 -4.2824278][-4.2743549 -4.2764292 -4.2744823 -4.2726188 -4.274076 -4.2667336 -4.2448721 -4.2178769 -4.199204 -4.205092 -4.2301407 -4.2517109 -4.2625537 -4.272007 -4.2806821][-4.2851381 -4.2851658 -4.28281 -4.2792182 -4.2774744 -4.2708983 -4.2579918 -4.2457185 -4.23697 -4.2422476 -4.2594261 -4.2710953 -4.2731538 -4.2755857 -4.279182][-4.2798672 -4.2789049 -4.2778831 -4.2694459 -4.258307 -4.2470336 -4.2406254 -4.2419276 -4.2415595 -4.2484031 -4.2640676 -4.2751021 -4.2771091 -4.2775912 -4.2798624]]...]
INFO - root - 2017-12-06 06:13:46.276662: step 4010, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 77h:19m:14s remains)
INFO - root - 2017-12-06 06:13:54.941347: step 4020, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 80h:02m:55s remains)
INFO - root - 2017-12-06 06:14:03.439582: step 4030, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 76h:57m:16s remains)
INFO - root - 2017-12-06 06:14:12.000643: step 4040, loss = 2.04, batch loss = 1.98 (10.2 examples/sec; 0.781 sec/batch; 71h:14m:29s remains)
INFO - root - 2017-12-06 06:14:20.660569: step 4050, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 77h:13m:12s remains)
INFO - root - 2017-12-06 06:14:29.326759: step 4060, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.874 sec/batch; 79h:44m:07s remains)
INFO - root - 2017-12-06 06:14:38.047817: step 4070, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 79h:11m:06s remains)
INFO - root - 2017-12-06 06:14:46.764966: step 4080, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 80h:56m:39s remains)
INFO - root - 2017-12-06 06:14:55.394933: step 4090, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 78h:22m:26s remains)
INFO - root - 2017-12-06 06:15:04.155150: step 4100, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 77h:41m:49s remains)
2017-12-06 06:15:04.934773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3189583 -4.3182993 -4.3198233 -4.3209224 -4.3195291 -4.3187408 -4.3196478 -4.3224912 -4.3252063 -4.3260708 -4.32655 -4.3270144 -4.3270879 -4.3269386 -4.3269763][-4.284399 -4.2827439 -4.28612 -4.2872534 -4.283886 -4.2821655 -4.2851329 -4.2925053 -4.2998228 -4.3031855 -4.3044219 -4.3051682 -4.305275 -4.3050036 -4.3054347][-4.2427959 -4.2393284 -4.2439427 -4.243638 -4.236146 -4.2329574 -4.2392244 -4.2542572 -4.2709627 -4.2812977 -4.2857184 -4.2878032 -4.2885175 -4.287272 -4.28583][-4.2141247 -4.2061849 -4.2053652 -4.196403 -4.179256 -4.1696281 -4.1753087 -4.1980076 -4.2284117 -4.2551684 -4.2724895 -4.2826319 -4.2866344 -4.2849426 -4.27902][-4.2064471 -4.1922593 -4.1811767 -4.159883 -4.1299524 -4.1070852 -4.1005831 -4.1193533 -4.162487 -4.2129622 -4.2510734 -4.2761407 -4.2893786 -4.290719 -4.2826185][-4.198751 -4.1782961 -4.1584654 -4.1264462 -4.0838542 -4.03959 -4.0016909 -3.9930458 -4.0383797 -4.1152849 -4.1803589 -4.2250109 -4.252214 -4.2638206 -4.26326][-4.1903057 -4.1630087 -4.13591 -4.0937934 -4.0405593 -3.9745326 -3.8941443 -3.8321695 -3.8580658 -3.9569077 -4.0487733 -4.1143208 -4.1604004 -4.1935811 -4.214736][-4.1983504 -4.17147 -4.1422119 -4.1002917 -4.0466204 -3.9700508 -3.8653889 -3.764432 -3.7574599 -3.8524618 -3.9493859 -4.0190392 -4.0744076 -4.126091 -4.1672506][-4.2267046 -4.2072392 -4.1843896 -4.15275 -4.1127868 -4.0521603 -3.9690428 -3.892653 -3.8800232 -3.9341075 -3.9932585 -4.036221 -4.0745196 -4.1166143 -4.1521611][-4.2611537 -4.253129 -4.2410645 -4.2224565 -4.1996503 -4.1639733 -4.1169 -4.0751824 -4.0681949 -4.0904837 -4.11179 -4.1226168 -4.1328392 -4.151104 -4.1652169][-4.2901397 -4.2938313 -4.294138 -4.2885938 -4.2799087 -4.2640295 -4.2427826 -4.2223153 -4.2160015 -4.2199483 -4.2210279 -4.2106853 -4.1984773 -4.1927896 -4.1897345][-4.3046827 -4.3161244 -4.3265047 -4.3309832 -4.3315263 -4.3270969 -4.3186 -4.3080916 -4.3010087 -4.297967 -4.2895141 -4.2692704 -4.2450666 -4.227005 -4.214776][-4.3038425 -4.3186579 -4.3352222 -4.3472381 -4.3536429 -4.3536434 -4.3478603 -4.3385134 -4.3307624 -4.3232088 -4.3106384 -4.286562 -4.2608643 -4.2419534 -4.2276897][-4.2948508 -4.3104753 -4.3293734 -4.34397 -4.3513713 -4.3518229 -4.3441539 -4.3317857 -4.3211126 -4.3115277 -4.2951527 -4.2712665 -4.2494025 -4.2333436 -4.2227597][-4.2784605 -4.2944536 -4.3129053 -4.3261113 -4.3302956 -4.3269358 -4.3131189 -4.295866 -4.282403 -4.2704763 -4.2511897 -4.2297854 -4.2147813 -4.2034645 -4.1945333]]...]
INFO - root - 2017-12-06 06:15:13.759643: step 4110, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 79h:48m:34s remains)
INFO - root - 2017-12-06 06:15:22.384307: step 4120, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 77h:17m:10s remains)
INFO - root - 2017-12-06 06:15:30.988148: step 4130, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 78h:20m:11s remains)
INFO - root - 2017-12-06 06:15:39.616858: step 4140, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.860 sec/batch; 78h:26m:58s remains)
INFO - root - 2017-12-06 06:15:48.059672: step 4150, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 80h:18m:00s remains)
INFO - root - 2017-12-06 06:15:56.882919: step 4160, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 79h:25m:23s remains)
INFO - root - 2017-12-06 06:16:05.635202: step 4170, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.857 sec/batch; 78h:11m:52s remains)
INFO - root - 2017-12-06 06:16:14.290296: step 4180, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 78h:20m:30s remains)
INFO - root - 2017-12-06 06:16:22.748978: step 4190, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 79h:45m:59s remains)
INFO - root - 2017-12-06 06:16:31.366036: step 4200, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 78h:52m:40s remains)
2017-12-06 06:16:32.308394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1850638 -4.2082386 -4.225338 -4.2328773 -4.2271562 -4.2067614 -4.17025 -4.139957 -4.1432676 -4.1499643 -4.1354156 -4.1134205 -4.1221414 -4.1536608 -4.1915817][-4.1929069 -4.222435 -4.2449846 -4.2504454 -4.2389 -4.2134972 -4.1744223 -4.1434846 -4.1448092 -4.1467161 -4.1253862 -4.0983849 -4.104085 -4.1307325 -4.1637745][-4.208468 -4.2363758 -4.2574391 -4.2586045 -4.2405348 -4.2040081 -4.1558895 -4.1233239 -4.1281171 -4.1330872 -4.115509 -4.0912151 -4.0960975 -4.1168251 -4.1442237][-4.2332468 -4.2550392 -4.269011 -4.260426 -4.2286587 -4.1689835 -4.0974746 -4.0569892 -4.0703468 -4.0916343 -4.0947537 -4.08648 -4.095849 -4.1143451 -4.1430507][-4.2672381 -4.2826047 -4.2875786 -4.2670946 -4.2167482 -4.1321535 -4.0352168 -3.9861717 -4.00427 -4.0454268 -4.07774 -4.0933919 -4.1115541 -4.1299176 -4.159471][-4.29609 -4.3052015 -4.3023524 -4.2757287 -4.2167845 -4.123239 -4.0193238 -3.95898 -3.9637299 -4.0083647 -4.0617781 -4.1023822 -4.1337447 -4.1554008 -4.1866183][-4.3083196 -4.3133583 -4.305995 -4.2772322 -4.2208881 -4.1384387 -4.0484996 -3.9814415 -3.9609218 -3.9908655 -4.0502615 -4.108923 -4.1549907 -4.1837826 -4.2187619][-4.3148742 -4.3188934 -4.313653 -4.2875514 -4.2393384 -4.1763673 -4.1024895 -4.0327039 -3.9946036 -4.0040331 -4.0558681 -4.1221604 -4.1775503 -4.2169228 -4.2530179][-4.3193288 -4.3232903 -4.3227897 -4.3031516 -4.2660532 -4.2182641 -4.1557846 -4.0875559 -4.0423918 -4.0362463 -4.0739136 -4.1366711 -4.1959715 -4.2403665 -4.2736173][-4.31956 -4.3238935 -4.327486 -4.3178005 -4.2940645 -4.2580209 -4.2076416 -4.1465364 -4.0995336 -4.0823612 -4.1069579 -4.1589594 -4.215899 -4.2623711 -4.2935362][-4.3227615 -4.3277607 -4.3344445 -4.333982 -4.3221779 -4.2990632 -4.2635117 -4.2135248 -4.1689868 -4.143465 -4.1537066 -4.1917214 -4.2402344 -4.2839208 -4.3107548][-4.3283896 -4.332325 -4.3399062 -4.3434577 -4.3390808 -4.3258672 -4.304801 -4.2708087 -4.2358003 -4.2088194 -4.20698 -4.2289019 -4.2636709 -4.2990079 -4.3213062][-4.32601 -4.3282595 -4.3349247 -4.3400335 -4.3391581 -4.3339262 -4.3239207 -4.3045111 -4.2811384 -4.259459 -4.2531238 -4.2624469 -4.2805071 -4.302999 -4.3202553][-4.3193016 -4.3194122 -4.3235021 -4.3292665 -4.3320441 -4.3327212 -4.3293953 -4.3186707 -4.3027959 -4.28817 -4.2826571 -4.2864642 -4.2940779 -4.3054934 -4.3153596][-4.3167381 -4.3151174 -4.3163128 -4.3194637 -4.3221846 -4.3244381 -4.3248191 -4.3206186 -4.3106518 -4.3015113 -4.2983527 -4.3010197 -4.3058667 -4.311029 -4.3149557]]...]
INFO - root - 2017-12-06 06:16:41.043000: step 4210, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 81h:19m:52s remains)
INFO - root - 2017-12-06 06:16:49.853727: step 4220, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.879 sec/batch; 80h:11m:23s remains)
INFO - root - 2017-12-06 06:16:58.459638: step 4230, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.820 sec/batch; 74h:47m:41s remains)
INFO - root - 2017-12-06 06:17:07.028370: step 4240, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 79h:01m:14s remains)
INFO - root - 2017-12-06 06:17:15.475608: step 4250, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 76h:34m:28s remains)
INFO - root - 2017-12-06 06:17:23.976320: step 4260, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 76h:49m:44s remains)
INFO - root - 2017-12-06 06:17:32.709816: step 4270, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 79h:28m:02s remains)
INFO - root - 2017-12-06 06:17:41.217863: step 4280, loss = 2.04, batch loss = 1.99 (10.0 examples/sec; 0.800 sec/batch; 72h:55m:06s remains)
INFO - root - 2017-12-06 06:17:49.784736: step 4290, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.899 sec/batch; 81h:57m:42s remains)
INFO - root - 2017-12-06 06:17:58.408026: step 4300, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 77h:31m:09s remains)
2017-12-06 06:17:59.221364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.16239 -4.1632853 -4.1636515 -4.1569877 -4.1404743 -4.1181335 -4.0995975 -4.0994034 -4.1151853 -4.1357312 -4.1527977 -4.1651974 -4.1690097 -4.164784 -4.1578827][-4.1475163 -4.1520972 -4.1528091 -4.1409144 -4.1141095 -4.0793066 -4.0530319 -4.0578094 -4.0836606 -4.1118417 -4.1357183 -4.1562762 -4.1652083 -4.1610723 -4.1517205][-4.1361051 -4.1424856 -4.1392851 -4.117444 -4.077209 -4.0294852 -4.0001564 -4.0140333 -4.0519342 -4.0886493 -4.1195412 -4.146822 -4.1593013 -4.1543093 -4.1403418][-4.1322179 -4.137435 -4.1268649 -4.0932231 -4.04019 -3.9838226 -3.9552369 -3.9776733 -4.0268531 -4.0729785 -4.1104646 -4.1409965 -4.1533856 -4.1456981 -4.1243958][-4.1365781 -4.1353531 -4.11593 -4.0717797 -4.0091181 -3.9487507 -3.9233403 -3.9553559 -4.0175657 -4.0740495 -4.1172905 -4.1479363 -4.156302 -4.1415124 -4.1077189][-4.1480074 -4.135211 -4.10628 -4.0533743 -3.9841814 -3.9215019 -3.8980303 -3.9412811 -4.0188718 -4.0858383 -4.1345534 -4.1645603 -4.1666451 -4.1397295 -4.0892305][-4.1584487 -4.135304 -4.0997419 -4.0418296 -3.9686074 -3.9013977 -3.8745322 -3.9269512 -4.0184631 -4.0934882 -4.14624 -4.1771245 -4.1761494 -4.1389208 -4.074728][-4.1592231 -4.1316605 -4.0970984 -4.0412092 -3.9672582 -3.8939457 -3.8623323 -3.9201026 -4.0218925 -4.1020732 -4.1576653 -4.1892772 -4.1881571 -4.1481724 -4.0788813][-4.1485748 -4.1225476 -4.0985889 -4.0589819 -3.9978323 -3.9292655 -3.8961151 -3.9495051 -4.047605 -4.1235795 -4.1753492 -4.2056994 -4.2064056 -4.1709356 -4.1068206][-4.1326466 -4.1107407 -4.1017394 -4.0860887 -4.0482817 -3.9972188 -3.9715793 -4.0160394 -4.0974669 -4.1576228 -4.1973419 -4.2210221 -4.221067 -4.1942253 -4.1416793][-4.1133051 -4.09677 -4.1034131 -4.1116686 -4.099503 -4.0668607 -4.0488935 -4.0823469 -4.1428013 -4.1859913 -4.2138214 -4.2298222 -4.2284722 -4.2098088 -4.167][-4.0909858 -4.0839429 -4.108377 -4.1387262 -4.1475329 -4.1298866 -4.1166024 -4.1380167 -4.1781807 -4.2063107 -4.2241588 -4.2337551 -4.2314167 -4.2173033 -4.1787624][-4.0737514 -4.078207 -4.1170511 -4.1612353 -4.1834984 -4.1783013 -4.1706553 -4.1831479 -4.2082109 -4.2272797 -4.2390618 -4.2428832 -4.2356725 -4.2180381 -4.1767044][-4.0657578 -4.0798144 -4.1237254 -4.1715288 -4.1988854 -4.20023 -4.1961045 -4.2046242 -4.2212167 -4.2372508 -4.248219 -4.2502556 -4.2403364 -4.2177558 -4.1752872][-4.06347 -4.0867276 -4.1287155 -4.1705189 -4.1935453 -4.194366 -4.1898332 -4.1935358 -4.2049928 -4.2231388 -4.2381983 -4.2424216 -4.2327747 -4.2093754 -4.1716223]]...]
INFO - root - 2017-12-06 06:18:07.793788: step 4310, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.881 sec/batch; 80h:19m:27s remains)
INFO - root - 2017-12-06 06:18:16.328341: step 4320, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 75h:41m:44s remains)
INFO - root - 2017-12-06 06:18:23.010752: step 4330, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 60h:33m:51s remains)
INFO - root - 2017-12-06 06:18:29.667208: step 4340, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 62h:04m:14s remains)
INFO - root - 2017-12-06 06:18:36.316838: step 4350, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.617 sec/batch; 56h:15m:43s remains)
INFO - root - 2017-12-06 06:18:42.708505: step 4360, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 59h:52m:18s remains)
INFO - root - 2017-12-06 06:18:49.335514: step 4370, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 58h:25m:37s remains)
INFO - root - 2017-12-06 06:18:55.772494: step 4380, loss = 2.06, batch loss = 2.00 (13.8 examples/sec; 0.579 sec/batch; 52h:46m:03s remains)
INFO - root - 2017-12-06 06:19:02.396097: step 4390, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 60h:59m:33s remains)
INFO - root - 2017-12-06 06:19:09.054810: step 4400, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 59h:54m:37s remains)
2017-12-06 06:19:09.765166: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1657686 -4.1745009 -4.186347 -4.1777091 -4.1594214 -4.1622667 -4.1811175 -4.1863046 -4.1644664 -4.1455517 -4.1463065 -4.1678686 -4.2013865 -4.2089715 -4.1927791][-4.17094 -4.1839013 -4.196197 -4.1842523 -4.1706042 -4.1803246 -4.1938887 -4.1942449 -4.1732225 -4.1572013 -4.1598806 -4.1795177 -4.1974225 -4.1911526 -4.1745381][-4.1519189 -4.1609936 -4.16862 -4.1549168 -4.143013 -4.1545343 -4.1690836 -4.17245 -4.1622834 -4.15659 -4.1610465 -4.1731157 -4.1742859 -4.1627622 -4.1526508][-4.0829005 -4.0892167 -4.09923 -4.0924892 -4.0818706 -4.0898876 -4.1065655 -4.120193 -4.1247311 -4.1280732 -4.1312823 -4.1290259 -4.11371 -4.1007204 -4.096633][-3.986624 -3.99304 -4.0132847 -4.0162096 -4.0104079 -4.0179219 -4.0312362 -4.0468321 -4.0578771 -4.0669584 -4.0728822 -4.062922 -4.042232 -4.0309992 -4.0235672][-3.9432416 -3.9513934 -3.970603 -3.9704881 -3.9609346 -3.9607737 -3.955672 -3.957006 -3.9764032 -3.9992828 -4.0077357 -3.9930646 -3.9763684 -3.9734287 -3.968194][-3.9513354 -3.949446 -3.9469261 -3.9286683 -3.9069881 -3.8840213 -3.8403916 -3.8178644 -3.8587306 -3.9086695 -3.9303148 -3.9289687 -3.9272573 -3.9379334 -3.9516039][-3.9786634 -3.9589877 -3.935456 -3.9019363 -3.8632023 -3.8103042 -3.730515 -3.692395 -3.7622969 -3.8394396 -3.8773565 -3.9025307 -3.9270091 -3.9596434 -3.9974282][-4.0458889 -4.0164075 -3.9847159 -3.9494605 -3.9094939 -3.8510745 -3.7773576 -3.7521987 -3.8225758 -3.8903809 -3.9254994 -3.9621091 -3.9998679 -4.0388441 -4.0829768][-4.1267586 -4.0974226 -4.069221 -4.0421658 -4.0131388 -3.9737945 -3.9306598 -3.9233932 -3.9715853 -4.0137386 -4.0360346 -4.0632863 -4.0929074 -4.1239543 -4.1607456][-4.1895413 -4.1709857 -4.154726 -4.1369286 -4.116981 -4.0941386 -4.0735321 -4.0745435 -4.101192 -4.121954 -4.1315637 -4.1447892 -4.1597767 -4.1784878 -4.2017765][-4.2326822 -4.2223244 -4.2161126 -4.2076263 -4.1967034 -4.1873865 -4.1800404 -4.1800637 -4.1895132 -4.198245 -4.2010465 -4.20348 -4.2033005 -4.2043023 -4.2089124][-4.2680988 -4.2592859 -4.2541528 -4.2488809 -4.2437835 -4.2410741 -4.2384186 -4.2357197 -4.2358556 -4.2373738 -4.232955 -4.2244711 -4.2081819 -4.1925287 -4.1825948][-4.3023953 -4.2950745 -4.2890587 -4.2844992 -4.2810388 -4.2785296 -4.275631 -4.2711673 -4.2674212 -4.2644525 -4.2554421 -4.2378759 -4.2079182 -4.1778097 -4.152761][-4.3251929 -4.3224406 -4.3186874 -4.31539 -4.312397 -4.3088737 -4.3048573 -4.3004823 -4.2959709 -4.2914948 -4.2823348 -4.2640524 -4.229496 -4.1887412 -4.1538725]]...]
INFO - root - 2017-12-06 06:19:16.126798: step 4410, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 59h:26m:14s remains)
INFO - root - 2017-12-06 06:19:22.814386: step 4420, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 58h:53m:08s remains)
INFO - root - 2017-12-06 06:19:29.494403: step 4430, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 58h:26m:04s remains)
INFO - root - 2017-12-06 06:19:36.052090: step 4440, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 60h:08m:05s remains)
INFO - root - 2017-12-06 06:19:42.778893: step 4450, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 59h:05m:03s remains)
INFO - root - 2017-12-06 06:19:49.179377: step 4460, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 61h:10m:17s remains)
INFO - root - 2017-12-06 06:19:55.737679: step 4470, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 61h:09m:26s remains)
INFO - root - 2017-12-06 06:20:02.293087: step 4480, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 59h:58m:57s remains)
INFO - root - 2017-12-06 06:20:08.824773: step 4490, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:58m:33s remains)
INFO - root - 2017-12-06 06:20:15.434861: step 4500, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 60h:08m:38s remains)
2017-12-06 06:20:16.147913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3333631 -4.3306522 -4.3255711 -4.31538 -4.2996593 -4.2879272 -4.2726507 -4.2552171 -4.25444 -4.2640052 -4.271553 -4.2811952 -4.2974677 -4.3114972 -4.3170285][-4.3321266 -4.3307719 -4.3272204 -4.3175349 -4.2959127 -4.2757883 -4.2449722 -4.2177978 -4.2181726 -4.2318611 -4.24258 -4.2614179 -4.2847514 -4.3035555 -4.3110256][-4.3338394 -4.3350172 -4.3316245 -4.3182287 -4.2884092 -4.2568941 -4.2096343 -4.1731358 -4.18584 -4.2126656 -4.2276554 -4.252141 -4.2835279 -4.3057313 -4.3130374][-4.3305173 -4.3334994 -4.3292084 -4.3099833 -4.2726483 -4.2292862 -4.1608157 -4.11862 -4.1521292 -4.2015486 -4.2231545 -4.2486277 -4.2819347 -4.3089328 -4.3182249][-4.3217607 -4.3227587 -4.3153791 -4.2870703 -4.2409477 -4.1869092 -4.1013169 -4.0558906 -4.1093416 -4.1821465 -4.2150717 -4.2453532 -4.2795515 -4.3093705 -4.321167][-4.3067279 -4.3041582 -4.2923627 -4.253891 -4.1969867 -4.1271353 -4.0263915 -3.9840763 -4.0621133 -4.1535673 -4.2005811 -4.2399297 -4.2789574 -4.3103771 -4.3216019][-4.2928071 -4.2888541 -4.2746091 -4.2268023 -4.1592426 -4.0635228 -3.9306033 -3.8859951 -3.9981303 -4.117372 -4.1814518 -4.2340961 -4.2808313 -4.3151884 -4.3254185][-4.286427 -4.2883873 -4.275733 -4.2249908 -4.1514149 -4.0359545 -3.8674598 -3.8116448 -3.9582193 -4.1039081 -4.1781831 -4.2368331 -4.2849145 -4.3181543 -4.3281841][-4.2815294 -4.2921548 -4.2867441 -4.2436371 -4.1777892 -4.0724659 -3.9192216 -3.8618059 -3.9895315 -4.1234865 -4.1936851 -4.2494917 -4.2900295 -4.3158417 -4.324398][-4.2713037 -4.2859 -4.2869949 -4.2591276 -4.2129097 -4.1361032 -4.0316782 -3.9958539 -4.0781021 -4.1688824 -4.2179174 -4.2611713 -4.2906928 -4.3065996 -4.3112397][-4.255023 -4.2669063 -4.270216 -4.2563243 -4.2287607 -4.1792026 -4.1150322 -4.0975575 -4.1519337 -4.2110157 -4.2416239 -4.2647386 -4.2797408 -4.2863894 -4.288331][-4.2359629 -4.2442064 -4.2497907 -4.2441053 -4.2271523 -4.1970816 -4.1620283 -4.1538658 -4.1891708 -4.2276545 -4.2484655 -4.2568617 -4.2595 -4.2588143 -4.2599196][-4.21593 -4.2234316 -4.2333446 -4.23231 -4.221159 -4.2062788 -4.1925993 -4.18873 -4.2070041 -4.2272873 -4.2398348 -4.2377996 -4.2339787 -4.23209 -4.2329221][-4.2061849 -4.2165956 -4.2294154 -4.2303362 -4.2222867 -4.2167521 -4.2163682 -4.2176337 -4.22346 -4.2287021 -4.2337961 -4.2244716 -4.2163377 -4.2108412 -4.208714][-4.2181778 -4.2322464 -4.2458229 -4.24526 -4.2378836 -4.2335887 -4.2336245 -4.2334809 -4.2338128 -4.233674 -4.2346249 -4.2255073 -4.2150836 -4.2071242 -4.2031775]]...]
INFO - root - 2017-12-06 06:20:22.960524: step 4510, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 63h:46m:50s remains)
INFO - root - 2017-12-06 06:20:29.577787: step 4520, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 59h:09m:30s remains)
INFO - root - 2017-12-06 06:20:36.228021: step 4530, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 60h:43m:36s remains)
INFO - root - 2017-12-06 06:20:42.772832: step 4540, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 58h:08m:56s remains)
INFO - root - 2017-12-06 06:20:49.411018: step 4550, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 60h:07m:49s remains)
INFO - root - 2017-12-06 06:20:55.821737: step 4560, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 60h:23m:28s remains)
INFO - root - 2017-12-06 06:21:02.332759: step 4570, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 57h:44m:38s remains)
INFO - root - 2017-12-06 06:21:08.707262: step 4580, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 61h:50m:14s remains)
INFO - root - 2017-12-06 06:21:15.321001: step 4590, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:45m:12s remains)
INFO - root - 2017-12-06 06:21:21.888001: step 4600, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 59h:15m:24s remains)
2017-12-06 06:21:22.512208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2860346 -4.2851276 -4.286171 -4.2878332 -4.2894325 -4.2901998 -4.2905068 -4.2906508 -4.289742 -4.282577 -4.2724581 -4.2670341 -4.2664847 -4.2656393 -4.2641649][-4.2939649 -4.291636 -4.2915196 -4.2924566 -4.2931948 -4.2934437 -4.2937512 -4.2951784 -4.2957277 -4.2875981 -4.2751083 -4.2651262 -4.2602835 -4.2576509 -4.2545953][-4.2848492 -4.2794995 -4.276875 -4.275897 -4.2752023 -4.2744327 -4.2747321 -4.2782722 -4.2815666 -4.2777505 -4.2703757 -4.2616868 -4.2575374 -4.2563753 -4.254231][-4.2647424 -4.2557335 -4.2479577 -4.2431288 -4.2395906 -4.2370996 -4.2375 -4.2417345 -4.2464581 -4.2492471 -4.2509193 -4.2505045 -4.2522893 -4.2565966 -4.2586775][-4.2444429 -4.230423 -4.2169204 -4.2043371 -4.195271 -4.1884 -4.1852541 -4.1873875 -4.1933985 -4.2051005 -4.2181735 -4.2294765 -4.2408609 -4.2516418 -4.2578497][-4.2047009 -4.1877656 -4.16808 -4.1454797 -4.1284194 -4.1141305 -4.10096 -4.0950818 -4.100709 -4.1204219 -4.1458464 -4.1728921 -4.1980195 -4.2196722 -4.2342944][-4.1619463 -4.1461945 -4.1231346 -4.0919313 -4.064445 -4.0364013 -4.0061769 -3.9845333 -3.9840612 -4.00676 -4.0434642 -4.0882597 -4.1300058 -4.1648088 -4.1914706][-4.1628437 -4.1538734 -4.1359172 -4.1071243 -4.0785351 -4.045959 -4.0078468 -3.9717212 -3.9556336 -3.9632709 -3.9941654 -4.0426145 -4.0921016 -4.1341972 -4.167058][-4.199348 -4.1949296 -4.1844773 -4.1669474 -4.14713 -4.1256223 -4.1008348 -4.0735431 -4.0543895 -4.0484581 -4.0625639 -4.0936532 -4.12973 -4.1619825 -4.1883974][-4.2461305 -4.2472911 -4.245441 -4.2367511 -4.2249718 -4.2149673 -4.2029233 -4.1871004 -4.1716628 -4.1636553 -4.1691513 -4.1845355 -4.2049022 -4.2234983 -4.2359328][-4.2747321 -4.27883 -4.2826023 -4.2803812 -4.2753434 -4.2726974 -4.2696428 -4.2622552 -4.2533464 -4.2481842 -4.2497506 -4.2530141 -4.2596235 -4.2665863 -4.268167][-4.2819781 -4.283083 -4.2886705 -4.2909541 -4.2926636 -4.297123 -4.3020358 -4.3020906 -4.2990723 -4.292654 -4.2863312 -4.2780924 -4.273757 -4.2724156 -4.2678514][-4.2622371 -4.2624221 -4.2697949 -4.2763743 -4.2838545 -4.292202 -4.2998238 -4.3034964 -4.3042421 -4.293334 -4.2775745 -4.2605186 -4.2503896 -4.2480779 -4.2453651][-4.2335963 -4.2318144 -4.2390552 -4.2458076 -4.2535982 -4.2613316 -4.2685504 -4.2732906 -4.2754784 -4.2607708 -4.2381124 -4.2162552 -4.2055564 -4.2078981 -4.2125487][-4.2207937 -4.216805 -4.2219124 -4.2263474 -4.2312508 -4.2345047 -4.2389464 -4.2422457 -4.2438641 -4.2291808 -4.2053461 -4.1799474 -4.168488 -4.176949 -4.1910591]]...]
INFO - root - 2017-12-06 06:21:29.138034: step 4610, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 61h:17m:31s remains)
INFO - root - 2017-12-06 06:21:35.694960: step 4620, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 59h:54m:22s remains)
INFO - root - 2017-12-06 06:21:42.160618: step 4630, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 59h:10m:10s remains)
INFO - root - 2017-12-06 06:21:48.737779: step 4640, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 58h:44m:25s remains)
INFO - root - 2017-12-06 06:21:55.448412: step 4650, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.665 sec/batch; 60h:32m:37s remains)
INFO - root - 2017-12-06 06:22:02.086143: step 4660, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 59h:34m:29s remains)
INFO - root - 2017-12-06 06:22:08.529888: step 4670, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 60h:42m:27s remains)
INFO - root - 2017-12-06 06:22:14.949339: step 4680, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 59h:30m:59s remains)
INFO - root - 2017-12-06 06:22:21.547402: step 4690, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 62h:18m:24s remains)
INFO - root - 2017-12-06 06:22:28.238694: step 4700, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 61h:51m:55s remains)
2017-12-06 06:22:28.895886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2582679 -4.256886 -4.2726712 -4.2925072 -4.2932172 -4.2763715 -4.2626548 -4.2677107 -4.2751474 -4.2800803 -4.2692242 -4.2399535 -4.2103338 -4.2175183 -4.2419095][-4.2464194 -4.2552867 -4.2801385 -4.304368 -4.3021922 -4.275281 -4.2548814 -4.2595496 -4.2657557 -4.2716603 -4.2668672 -4.2459831 -4.225028 -4.2367225 -4.2643147][-4.2091279 -4.23 -4.2678127 -4.3012514 -4.3035727 -4.271646 -4.2436409 -4.24776 -4.2591987 -4.2668924 -4.2586346 -4.2394624 -4.2255721 -4.2411594 -4.2708597][-4.1763749 -4.2011032 -4.2484322 -4.2902365 -4.2956805 -4.2556396 -4.2128296 -4.2159867 -4.2414455 -4.2573051 -4.2456694 -4.2255416 -4.220295 -4.239943 -4.2697186][-4.1740966 -4.1941576 -4.2424207 -4.280221 -4.272275 -4.2069149 -4.139051 -4.1487627 -4.19911 -4.2337952 -4.2273655 -4.2064986 -4.2081361 -4.2334247 -4.2675281][-4.185308 -4.2023206 -4.2468581 -4.2671046 -4.2292314 -4.1226344 -4.0279336 -4.0561495 -4.141263 -4.193882 -4.1914663 -4.169992 -4.1764617 -4.2096806 -4.2553287][-4.1956415 -4.2114253 -4.24664 -4.2474427 -4.1796718 -4.0403337 -3.9318421 -3.9779711 -4.0947642 -4.1593313 -4.1522694 -4.1230984 -4.1329255 -4.17557 -4.2345772][-4.2164636 -4.23395 -4.260097 -4.2490687 -4.1744213 -4.0397716 -3.9416742 -3.9859934 -4.1018882 -4.1629534 -4.1471324 -4.1100655 -4.1216817 -4.1711287 -4.2348838][-4.2553883 -4.2717037 -4.2872443 -4.2692757 -4.2040238 -4.0972271 -4.0145855 -4.045785 -4.1409993 -4.1917248 -4.1689739 -4.1287155 -4.140954 -4.1898618 -4.2534251][-4.29266 -4.3052812 -4.3128858 -4.2908292 -4.236701 -4.1570773 -4.0880775 -4.1046352 -4.1781397 -4.2207146 -4.1955719 -4.1523514 -4.1613345 -4.2103705 -4.273078][-4.3100467 -4.3181243 -4.3209567 -4.3001332 -4.2573552 -4.2002063 -4.1505022 -4.1561589 -4.2102747 -4.2465067 -4.2251148 -4.1819315 -4.1812563 -4.2270079 -4.2846518][-4.3104868 -4.3180604 -4.3188224 -4.3037214 -4.2724733 -4.2333708 -4.2012844 -4.2039671 -4.2413149 -4.2699265 -4.2558975 -4.215909 -4.2077026 -4.2468286 -4.294476][-4.3073945 -4.315814 -4.3187518 -4.3098583 -4.2895417 -4.2643218 -4.2455888 -4.2492781 -4.2734551 -4.2903552 -4.279911 -4.250875 -4.246243 -4.278152 -4.3112769][-4.3128691 -4.3222451 -4.3264365 -4.3229709 -4.3116035 -4.2943068 -4.2823186 -4.2870488 -4.3014374 -4.3064466 -4.2964063 -4.2784224 -4.2802715 -4.3039083 -4.3237638][-4.3177495 -4.3268204 -4.3332286 -4.33566 -4.3314819 -4.3196731 -4.3099561 -4.3125372 -4.3203578 -4.3192525 -4.309042 -4.2972593 -4.3011746 -4.315762 -4.3268456]]...]
INFO - root - 2017-12-06 06:22:35.400720: step 4710, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 57h:44m:53s remains)
INFO - root - 2017-12-06 06:22:41.884909: step 4720, loss = 2.03, batch loss = 1.98 (12.5 examples/sec; 0.642 sec/batch; 58h:26m:14s remains)
INFO - root - 2017-12-06 06:22:48.483031: step 4730, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 62h:25m:19s remains)
INFO - root - 2017-12-06 06:22:55.212386: step 4740, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 61h:04m:52s remains)
INFO - root - 2017-12-06 06:23:01.737456: step 4750, loss = 2.03, batch loss = 1.98 (12.5 examples/sec; 0.642 sec/batch; 58h:28m:35s remains)
INFO - root - 2017-12-06 06:23:08.324172: step 4760, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 60h:37m:13s remains)
INFO - root - 2017-12-06 06:23:14.732827: step 4770, loss = 2.05, batch loss = 1.99 (15.0 examples/sec; 0.533 sec/batch; 48h:33m:36s remains)
INFO - root - 2017-12-06 06:23:21.326734: step 4780, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 59h:32m:52s remains)
INFO - root - 2017-12-06 06:23:28.014336: step 4790, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 62h:46m:14s remains)
INFO - root - 2017-12-06 06:23:34.547860: step 4800, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 59h:37m:33s remains)
2017-12-06 06:23:35.145256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2381525 -4.2382393 -4.2392378 -4.2372708 -4.2390933 -4.2445426 -4.249465 -4.2593727 -4.2745662 -4.2873344 -4.2925148 -4.2848196 -4.273931 -4.2725039 -4.2817559][-4.18861 -4.192874 -4.1969786 -4.19759 -4.1998448 -4.204658 -4.2095661 -4.2223749 -4.2428813 -4.2647629 -4.2807136 -4.2847991 -4.2814441 -4.2811723 -4.2901082][-4.1450844 -4.1557631 -4.1624894 -4.1642237 -4.1654572 -4.1664433 -4.1676292 -4.1791916 -4.2026005 -4.2313957 -4.2549229 -4.2687106 -4.2748671 -4.2795334 -4.2907419][-4.1133265 -4.1321955 -4.1408796 -4.1438909 -4.1410136 -4.1355128 -4.131433 -4.1395965 -4.1625152 -4.1941209 -4.2222166 -4.242002 -4.2562265 -4.2665138 -4.2801762][-4.1029439 -4.1193094 -4.1237545 -4.1219172 -4.1092415 -4.0938034 -4.081996 -4.0867729 -4.1150155 -4.15304 -4.1844983 -4.2076645 -4.2264223 -4.2403345 -4.2544432][-4.1202788 -4.1258817 -4.1169267 -4.0964603 -4.064137 -4.0305514 -4.0019808 -3.9990532 -4.041038 -4.1006823 -4.1437788 -4.1716194 -4.19178 -4.2058816 -4.2185364][-4.1559415 -4.1559362 -4.1336193 -4.090539 -4.0338855 -3.9746289 -3.9160194 -3.89281 -3.9483659 -4.0371089 -4.1011496 -4.1376219 -4.158258 -4.170507 -4.1826773][-4.1962671 -4.2001038 -4.17155 -4.1152754 -4.0421414 -3.9653833 -3.8849931 -3.8368969 -3.8850157 -3.9850368 -4.0620737 -4.1065235 -4.1304569 -4.1448278 -4.1595087][-4.2247806 -4.2366977 -4.212709 -4.1587143 -4.0861135 -4.0097289 -3.9326441 -3.8795419 -3.9020452 -3.9778585 -4.042088 -4.0839806 -4.1094055 -4.1286764 -4.1473975][-4.2396641 -4.2588968 -4.2486391 -4.21036 -4.1530375 -4.0896716 -4.0287471 -3.9846561 -3.9854803 -4.0257845 -4.0636091 -4.0916395 -4.1108685 -4.1262693 -4.1408958][-4.2416959 -4.2619767 -4.2677875 -4.2496967 -4.2118134 -4.1666718 -4.1267381 -4.0976815 -4.0921369 -4.1082568 -4.1244192 -4.1377416 -4.1458168 -4.14896 -4.1491351][-4.2364564 -4.2521806 -4.2687845 -4.2656722 -4.2440796 -4.2168994 -4.1964545 -4.1834641 -4.1830864 -4.1889014 -4.1923084 -4.1945009 -4.1951165 -4.1888938 -4.1757593][-4.2263017 -4.2329168 -4.24954 -4.2550545 -4.2483249 -4.2367215 -4.2312155 -4.2338281 -4.2409658 -4.2461014 -4.243598 -4.2382164 -4.2359138 -4.2304406 -4.2164688][-4.214335 -4.2111712 -4.2226334 -4.2309685 -4.2348723 -4.233788 -4.2392449 -4.2516026 -4.2633638 -4.2719941 -4.2707996 -4.263339 -4.262383 -4.2623835 -4.2560277][-4.2000904 -4.1908026 -4.1989908 -4.207643 -4.2161622 -4.2218208 -4.2337127 -4.2483907 -4.2609596 -4.2708306 -4.2719097 -4.2662973 -4.2675943 -4.2721944 -4.2733779]]...]
INFO - root - 2017-12-06 06:23:41.800158: step 4810, loss = 2.04, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 61h:38m:59s remains)
INFO - root - 2017-12-06 06:23:48.419685: step 4820, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 59h:33m:25s remains)
INFO - root - 2017-12-06 06:23:54.925781: step 4830, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:47m:02s remains)
INFO - root - 2017-12-06 06:24:01.541594: step 4840, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 59h:48m:38s remains)
INFO - root - 2017-12-06 06:24:08.065138: step 4850, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 60h:44m:12s remains)
INFO - root - 2017-12-06 06:24:14.722113: step 4860, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 61h:17m:34s remains)
INFO - root - 2017-12-06 06:24:20.969226: step 4870, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 60h:57m:09s remains)
INFO - root - 2017-12-06 06:24:27.512183: step 4880, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 57h:54m:49s remains)
INFO - root - 2017-12-06 06:24:33.964063: step 4890, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 59h:54m:33s remains)
INFO - root - 2017-12-06 06:24:40.544534: step 4900, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 58h:36m:20s remains)
2017-12-06 06:24:41.253780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1381745 -4.1838627 -4.2255707 -4.2329092 -4.2043552 -4.173737 -4.1715436 -4.1850996 -4.2038527 -4.2350988 -4.2712545 -4.2964406 -4.301363 -4.2817855 -4.2572789][-4.1417856 -4.1954603 -4.2426314 -4.2520027 -4.22326 -4.1890593 -4.1809583 -4.1916456 -4.2162871 -4.2529216 -4.2877579 -4.3066969 -4.3105721 -4.292078 -4.2608886][-4.1350594 -4.1905661 -4.2414713 -4.254396 -4.2264643 -4.1878281 -4.1714149 -4.1801395 -4.2141781 -4.2580924 -4.2918229 -4.302855 -4.3039846 -4.2883291 -4.2555008][-4.1158957 -4.1693869 -4.2198262 -4.2348104 -4.2089438 -4.1659489 -4.1395087 -4.1454949 -4.1911316 -4.2472882 -4.28532 -4.2943172 -4.2959828 -4.282433 -4.2475023][-4.1008983 -4.1482229 -4.1950569 -4.2114515 -4.185586 -4.1358933 -4.0947323 -4.0908804 -4.1466494 -4.2182851 -4.26625 -4.281702 -4.2836857 -4.26441 -4.222909][-4.1089025 -4.1481633 -4.1881242 -4.2017722 -4.1781931 -4.1241093 -4.0652781 -4.0387053 -4.0976419 -4.1840835 -4.2440248 -4.2680173 -4.2687297 -4.2398167 -4.1909018][-4.1414213 -4.1704597 -4.1977448 -4.2036972 -4.18291 -4.1320615 -4.0712857 -4.0335746 -4.0872626 -4.176918 -4.2411046 -4.2689734 -4.2676711 -4.2331619 -4.1800566][-4.1779218 -4.1943064 -4.2034631 -4.1956859 -4.172039 -4.130796 -4.0892658 -4.06874 -4.1164079 -4.1933079 -4.2518406 -4.2777543 -4.2772264 -4.2446504 -4.1952586][-4.1999078 -4.2011423 -4.1936989 -4.1716971 -4.1431193 -4.1132417 -4.0985618 -4.1066875 -4.1529074 -4.2169147 -4.2660275 -4.2874107 -4.287199 -4.2616649 -4.2236824][-4.1937814 -4.1837292 -4.1684427 -4.1419897 -4.1147809 -4.0995979 -4.1100373 -4.13916 -4.1834378 -4.2347836 -4.2748017 -4.2914958 -4.2912588 -4.2738457 -4.2487717][-4.1844859 -4.1702008 -4.1555457 -4.1321473 -4.1098251 -4.1060524 -4.1307006 -4.1695538 -4.211318 -4.252017 -4.2820272 -4.2938914 -4.2918448 -4.2799592 -4.2669168][-4.1938434 -4.1805344 -4.1679888 -4.1484375 -4.1300535 -4.1318078 -4.1642947 -4.205667 -4.2424026 -4.2729044 -4.2921171 -4.2976141 -4.2928276 -4.284893 -4.2809978][-4.2198539 -4.2085981 -4.198854 -4.1838403 -4.1678934 -4.1709337 -4.2040424 -4.24156 -4.268682 -4.2903366 -4.3011065 -4.3000979 -4.2931 -4.2874994 -4.2883754][-4.2466464 -4.2406254 -4.2353873 -4.2263713 -4.2167325 -4.2195649 -4.2450566 -4.2713847 -4.2871714 -4.2986374 -4.3021789 -4.2975473 -4.2906823 -4.2876287 -4.2919879][-4.2626486 -4.2632656 -4.2645226 -4.2634983 -4.2604642 -4.2606392 -4.2725744 -4.283608 -4.2882667 -4.291513 -4.2899265 -4.2837629 -4.2771883 -4.2757964 -4.2819815]]...]
INFO - root - 2017-12-06 06:24:47.885560: step 4910, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 62h:53m:54s remains)
INFO - root - 2017-12-06 06:24:54.466993: step 4920, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 60h:33m:22s remains)
INFO - root - 2017-12-06 06:25:01.005464: step 4930, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 60h:51m:21s remains)
INFO - root - 2017-12-06 06:25:07.656970: step 4940, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.638 sec/batch; 58h:03m:38s remains)
INFO - root - 2017-12-06 06:25:14.235558: step 4950, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 58h:18m:37s remains)
INFO - root - 2017-12-06 06:25:20.767574: step 4960, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.677 sec/batch; 61h:35m:46s remains)
INFO - root - 2017-12-06 06:25:27.292806: step 4970, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 60h:14m:53s remains)
INFO - root - 2017-12-06 06:25:33.762488: step 4980, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.667 sec/batch; 60h:42m:22s remains)
INFO - root - 2017-12-06 06:25:40.502099: step 4990, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 60h:14m:59s remains)
INFO - root - 2017-12-06 06:25:47.105194: step 5000, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 59h:05m:44s remains)
2017-12-06 06:25:47.852644: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22004 -4.2226696 -4.2128072 -4.1709204 -4.120985 -4.0907359 -4.0899272 -4.1155238 -4.151845 -4.1909194 -4.22016 -4.2397313 -4.2565684 -4.2712865 -4.2646837][-4.2137017 -4.2240353 -4.2195115 -4.1828771 -4.1305161 -4.0985656 -4.0989256 -4.1181426 -4.1452508 -4.1775842 -4.2031784 -4.22521 -4.2451124 -4.2600951 -4.2588816][-4.2034922 -4.2211947 -4.2161942 -4.1803451 -4.1295209 -4.1043086 -4.1080093 -4.124342 -4.1443777 -4.1691265 -4.1935415 -4.215785 -4.2341838 -4.2494106 -4.2525077][-4.1797829 -4.210525 -4.2129278 -4.1809731 -4.137259 -4.1147423 -4.1104403 -4.116993 -4.1306081 -4.1468759 -4.1712618 -4.1972318 -4.2170696 -4.2332711 -4.2446][-4.1567965 -4.2049661 -4.2203822 -4.1965985 -4.1622295 -4.1368179 -4.1051197 -4.0951858 -4.1113563 -4.1290946 -4.1535311 -4.1809974 -4.2003522 -4.216466 -4.2362137][-4.1548305 -4.2107186 -4.2327967 -4.2090883 -4.1759977 -4.1348171 -4.0595007 -4.0286064 -4.06739 -4.1075563 -4.13973 -4.1667509 -4.186717 -4.2076769 -4.2326422][-4.1675911 -4.2167149 -4.2352619 -4.2041731 -4.1595664 -4.0847034 -3.9523902 -3.906209 -3.9921024 -4.07384 -4.1211948 -4.147058 -4.1687527 -4.1986103 -4.2296357][-4.1885438 -4.2266593 -4.2362018 -4.1965971 -4.1361504 -4.0213132 -3.827599 -3.7721679 -3.9204564 -4.0458727 -4.1002197 -4.1139507 -4.1262569 -4.1601443 -4.2025905][-4.2133446 -4.2399945 -4.2392325 -4.1923451 -4.1301064 -4.0145354 -3.8204303 -3.782136 -3.9517035 -4.0761142 -4.1138725 -4.1010466 -4.0932927 -4.1239462 -4.1721525][-4.2470322 -4.2645292 -4.2536235 -4.2058492 -4.1579895 -4.0796185 -3.9483988 -3.9331841 -4.0606375 -4.1438727 -4.152597 -4.1163678 -4.0931168 -4.1204939 -4.1616426][-4.2818294 -4.2890015 -4.2775574 -4.237905 -4.19912 -4.1477537 -4.0698218 -4.0652623 -4.1471486 -4.19225 -4.1776638 -4.1329465 -4.1105051 -4.1390233 -4.1746969][-4.2948971 -4.29673 -4.290853 -4.2666869 -4.2394423 -4.2095232 -4.1651874 -4.16032 -4.2021155 -4.219264 -4.1991997 -4.1563692 -4.1353383 -4.1586418 -4.1861491][-4.3024492 -4.3027134 -4.3034825 -4.2911453 -4.27253 -4.2545986 -4.2293839 -4.2204614 -4.2357168 -4.2390504 -4.2250705 -4.1959114 -4.1712804 -4.18043 -4.1928811][-4.315434 -4.3153744 -4.3147769 -4.3074989 -4.2943759 -4.2836161 -4.2712464 -4.2627578 -4.2672524 -4.2654762 -4.2578259 -4.2383657 -4.2101617 -4.2017736 -4.2025852][-4.3275437 -4.3267097 -4.3248358 -4.3177795 -4.3058887 -4.297327 -4.2909265 -4.2844644 -4.2841043 -4.2847176 -4.2828512 -4.273562 -4.2519808 -4.2368941 -4.2327938]]...]
INFO - root - 2017-12-06 06:25:54.400407: step 5010, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 59h:07m:22s remains)
INFO - root - 2017-12-06 06:26:00.928620: step 5020, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:29m:48s remains)
INFO - root - 2017-12-06 06:26:07.596448: step 5030, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 61h:04m:03s remains)
INFO - root - 2017-12-06 06:26:14.169833: step 5040, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 59h:20m:10s remains)
INFO - root - 2017-12-06 06:26:20.645705: step 5050, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 60h:00m:52s remains)
INFO - root - 2017-12-06 06:26:27.298196: step 5060, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.671 sec/batch; 60h:59m:37s remains)
INFO - root - 2017-12-06 06:26:33.636243: step 5070, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 59h:53m:07s remains)
INFO - root - 2017-12-06 06:26:40.109093: step 5080, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 60h:49m:50s remains)
INFO - root - 2017-12-06 06:26:46.681524: step 5090, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 60h:07m:49s remains)
INFO - root - 2017-12-06 06:26:53.206369: step 5100, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 58h:58m:39s remains)
2017-12-06 06:26:53.809957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2564487 -4.261889 -4.2619538 -4.2583084 -4.2383218 -4.21638 -4.2006373 -4.1928549 -4.1766257 -4.164691 -4.1479483 -4.1388941 -4.1351423 -4.1436439 -4.166852][-4.2550769 -4.26012 -4.2608929 -4.2556643 -4.2412367 -4.2304664 -4.22395 -4.2089725 -4.1776733 -4.1504135 -4.1240592 -4.1086454 -4.1130466 -4.1345806 -4.1679745][-4.24325 -4.2501726 -4.2533879 -4.2487369 -4.2406316 -4.2380753 -4.2341018 -4.2122703 -4.1721683 -4.1367192 -4.1069946 -4.0914779 -4.1061997 -4.1403189 -4.18043][-4.2260747 -4.2358952 -4.2456508 -4.2455735 -4.2430892 -4.2426691 -4.2334418 -4.2042866 -4.1637845 -4.1309462 -4.1060061 -4.0951691 -4.1152124 -4.1556282 -4.196269][-4.2216759 -4.2296152 -4.2393813 -4.24181 -4.2447309 -4.2420492 -4.2200618 -4.1831617 -4.15318 -4.1330357 -4.1157022 -4.11141 -4.1309214 -4.1664286 -4.2008228][-4.2134194 -4.2208452 -4.2290645 -4.2364798 -4.2417 -4.2273674 -4.1856947 -4.1407185 -4.123096 -4.121274 -4.1205916 -4.127521 -4.1471996 -4.1730247 -4.1980143][-4.1965 -4.2143855 -4.22276 -4.2272029 -4.212914 -4.1758919 -4.1121221 -4.058866 -4.05527 -4.082099 -4.1094875 -4.1346312 -4.157939 -4.1756287 -4.1926494][-4.1928725 -4.2213016 -4.2258825 -4.2169824 -4.1744485 -4.1036649 -4.0099134 -3.9435408 -3.960336 -4.024406 -4.0858393 -4.1342235 -4.1629791 -4.1755557 -4.1851435][-4.20383 -4.2363205 -4.2359753 -4.2080917 -4.140379 -4.0373468 -3.9199498 -3.8564181 -3.9040997 -3.9970489 -4.0790133 -4.13891 -4.1681995 -4.17638 -4.1768265][-4.2191114 -4.2410321 -4.2314854 -4.1938047 -4.1179752 -4.0082154 -3.9051518 -3.879776 -3.9528544 -4.0403147 -4.107852 -4.156661 -4.1818 -4.1865597 -4.1818314][-4.2342753 -4.2419634 -4.2215095 -4.1788297 -4.1135921 -4.0295663 -3.9707079 -3.9825525 -4.053894 -4.1176453 -4.1597471 -4.1888843 -4.2049809 -4.2068305 -4.2028532][-4.2591496 -4.25118 -4.2136168 -4.1659532 -4.1139765 -4.0644431 -4.0402517 -4.0741215 -4.1394205 -4.187079 -4.2140551 -4.230422 -4.2398167 -4.2406321 -4.2415729][-4.2832756 -4.2614312 -4.2098236 -4.1521835 -4.0992622 -4.0716705 -4.0724959 -4.1215539 -4.1829262 -4.2221279 -4.2424145 -4.2530293 -4.2615976 -4.2669749 -4.274509][-4.2865629 -4.2560496 -4.1898084 -4.1175561 -4.0658545 -4.0566711 -4.0806022 -4.1393394 -4.1944666 -4.2263942 -4.2433357 -4.2520952 -4.2631173 -4.2739215 -4.2868085][-4.275692 -4.2343664 -4.1517978 -4.0701852 -4.0243769 -4.0324788 -4.0769405 -4.14311 -4.1920242 -4.2176442 -4.2324777 -4.2435026 -4.2585778 -4.2736378 -4.28972]]...]
INFO - root - 2017-12-06 06:27:00.518929: step 5110, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 59h:44m:00s remains)
INFO - root - 2017-12-06 06:27:07.161899: step 5120, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 62h:29m:03s remains)
INFO - root - 2017-12-06 06:27:13.708582: step 5130, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 58h:19m:01s remains)
INFO - root - 2017-12-06 06:27:20.289388: step 5140, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 60h:33m:10s remains)
INFO - root - 2017-12-06 06:27:26.894577: step 5150, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 60h:49m:34s remains)
INFO - root - 2017-12-06 06:27:33.376325: step 5160, loss = 2.09, batch loss = 2.03 (15.9 examples/sec; 0.504 sec/batch; 45h:51m:53s remains)
INFO - root - 2017-12-06 06:27:39.946211: step 5170, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 61h:03m:07s remains)
INFO - root - 2017-12-06 06:27:46.372247: step 5180, loss = 2.06, batch loss = 2.01 (12.8 examples/sec; 0.623 sec/batch; 56h:40m:28s remains)
INFO - root - 2017-12-06 06:27:52.925933: step 5190, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.687 sec/batch; 62h:25m:28s remains)
INFO - root - 2017-12-06 06:27:59.644786: step 5200, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 60h:57m:23s remains)
2017-12-06 06:28:00.247214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3472075 -4.3474364 -4.3462024 -4.3465972 -4.3473172 -4.3476367 -4.3465137 -4.34494 -4.3445 -4.3450069 -4.3457665 -4.3462853 -4.3467083 -4.3479514 -4.3500896][-4.3399329 -4.3379736 -4.3367672 -4.3371429 -4.3375883 -4.3375907 -4.3356557 -4.3336635 -4.33533 -4.3371139 -4.338182 -4.3385649 -4.3380609 -4.3396168 -4.3431754][-4.3230119 -4.3176713 -4.3153806 -4.3143735 -4.3140097 -4.3134313 -4.3086 -4.3058553 -4.312438 -4.3170443 -4.3178511 -4.3171868 -4.317986 -4.3206015 -4.3257704][-4.3010759 -4.2940412 -4.2924867 -4.2917471 -4.2905807 -4.2909207 -4.2825522 -4.2752662 -4.2846813 -4.2905555 -4.2896047 -4.2866564 -4.2892776 -4.293262 -4.3008156][-4.2809591 -4.2724204 -4.2723536 -4.2743239 -4.27261 -4.2706594 -4.2521319 -4.2341013 -4.2454882 -4.2552781 -4.2548051 -4.2499547 -4.2542992 -4.2605748 -4.2693686][-4.2512693 -4.2355719 -4.2344913 -4.2374277 -4.2338209 -4.2282648 -4.1935568 -4.1600614 -4.1781359 -4.1991954 -4.2044153 -4.2004738 -4.2063446 -4.2163076 -4.2298985][-4.2182083 -4.1931224 -4.1873245 -4.185205 -4.1739511 -4.1584139 -4.0972219 -4.0403757 -4.0767379 -4.1231546 -4.1434679 -4.1478462 -4.1578279 -4.1727405 -4.194901][-4.1856065 -4.1506448 -4.1360245 -4.1178308 -4.0898147 -4.0580692 -3.9647017 -3.8808913 -3.9450171 -4.0346837 -4.0838141 -4.1105614 -4.1313162 -4.1518497 -4.183188][-4.168581 -4.1278658 -4.1067224 -4.07924 -4.0484934 -4.0186415 -3.9319286 -3.8575583 -3.9369376 -4.0358553 -4.0943851 -4.1284819 -4.1497731 -4.1710396 -4.20449][-4.1924672 -4.1602192 -4.145474 -4.129797 -4.1183262 -4.1094508 -4.0590706 -4.0131087 -4.0655951 -4.1250043 -4.1607475 -4.1815305 -4.1958504 -4.2119923 -4.2381454][-4.230021 -4.2123227 -4.2117949 -4.210639 -4.2123008 -4.2132649 -4.1856618 -4.1558776 -4.1822662 -4.2074323 -4.2234125 -4.2346177 -4.2449059 -4.2558427 -4.27434][-4.2741261 -4.2688112 -4.2759314 -4.2811441 -4.2840943 -4.2850547 -4.2668629 -4.2459731 -4.2587967 -4.2647414 -4.268105 -4.2761583 -4.28617 -4.293334 -4.3057714][-4.3049293 -4.3031073 -4.3093238 -4.3138666 -4.3136172 -4.3122635 -4.2965136 -4.2779593 -4.2841277 -4.2845392 -4.2829781 -4.2914338 -4.3031621 -4.3103619 -4.3191547][-4.3203974 -4.3185635 -4.3211508 -4.3227072 -4.3212051 -4.3204317 -4.3091068 -4.2956028 -4.2990279 -4.2982059 -4.2973642 -4.3054376 -4.3157835 -4.32082 -4.3262691][-4.334826 -4.3321128 -4.3315763 -4.3311772 -4.3302412 -4.3302236 -4.3247843 -4.3190155 -4.3208423 -4.320118 -4.3202887 -4.3243785 -4.3292527 -4.3307619 -4.333982]]...]
INFO - root - 2017-12-06 06:28:06.679690: step 5210, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 59h:25m:34s remains)
INFO - root - 2017-12-06 06:28:13.144078: step 5220, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 60h:31m:21s remains)
INFO - root - 2017-12-06 06:28:19.759380: step 5230, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 60h:09m:09s remains)
INFO - root - 2017-12-06 06:28:26.339429: step 5240, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 59h:06m:48s remains)
INFO - root - 2017-12-06 06:28:32.855522: step 5250, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 59h:36m:33s remains)
INFO - root - 2017-12-06 06:28:39.343104: step 5260, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 58h:31m:35s remains)
INFO - root - 2017-12-06 06:28:45.834747: step 5270, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:48m:43s remains)
INFO - root - 2017-12-06 06:28:52.576033: step 5280, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 63h:58m:20s remains)
INFO - root - 2017-12-06 06:28:59.010078: step 5290, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 59h:01m:20s remains)
INFO - root - 2017-12-06 06:29:05.551218: step 5300, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 60h:06m:55s remains)
2017-12-06 06:29:06.229919: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2868457 -4.2913179 -4.2969766 -4.2986989 -4.2886448 -4.2709312 -4.25977 -4.2507558 -4.2449632 -4.2437553 -4.2403994 -4.2454052 -4.2565241 -4.2686763 -4.2844076][-4.2628136 -4.2693791 -4.276051 -4.2768936 -4.2640972 -4.2411647 -4.2265062 -4.2161565 -4.2121129 -4.2115006 -4.2044091 -4.2070417 -4.219317 -4.2356238 -4.2560458][-4.2240252 -4.2336431 -4.2419434 -4.2418337 -4.2246885 -4.1971879 -4.1781292 -4.1690936 -4.1728935 -4.1777086 -4.1698694 -4.1700377 -4.1810818 -4.1981969 -4.2201958][-4.1826429 -4.1952243 -4.2081313 -4.2105212 -4.1881237 -4.1534729 -4.1257033 -4.1167679 -4.1293468 -4.1438174 -4.14009 -4.1403165 -4.1513772 -4.1684146 -4.1904988][-4.1472297 -4.1601939 -4.1779175 -4.18432 -4.1598744 -4.1176567 -4.0791245 -4.0693192 -4.089848 -4.1152496 -4.1208091 -4.1239729 -4.1343784 -4.1515279 -4.1729817][-4.1198187 -4.1314387 -4.1516733 -4.1631565 -4.1376052 -4.0848613 -4.0360975 -4.0257645 -4.0541058 -4.0899715 -4.1079788 -4.1173248 -4.127264 -4.1440816 -4.1633549][-4.1130991 -4.1273727 -4.1489015 -4.1606331 -4.1274204 -4.0578384 -3.9938433 -3.9797838 -4.0153971 -4.0635695 -4.0969915 -4.114058 -4.1264887 -4.1435328 -4.1592093][-4.1195765 -4.1391859 -4.162353 -4.1709795 -4.1299677 -4.0495024 -3.9767025 -3.9594755 -3.9972522 -4.0504951 -4.0934815 -4.1152043 -4.1284218 -4.1450634 -4.1611195][-4.1210594 -4.1442294 -4.1669607 -4.1734567 -4.134788 -4.0607533 -3.9959705 -3.985774 -4.0223217 -4.066854 -4.1022053 -4.1180377 -4.1264977 -4.1405654 -4.1590867][-4.1175075 -4.1421413 -4.1646085 -4.1724553 -4.1431131 -4.087028 -4.0386653 -4.0328445 -4.0597782 -4.0893831 -4.110064 -4.1158409 -4.1186481 -4.131567 -4.1529741][-4.1193805 -4.1390147 -4.1583014 -4.1684918 -4.1496291 -4.1102748 -4.075161 -4.0693564 -4.0859303 -4.1038814 -4.1146603 -4.1162071 -4.1192908 -4.1333261 -4.156558][-4.1187029 -4.1312432 -4.146596 -4.1591964 -4.1510673 -4.1269264 -4.104569 -4.1019387 -4.1144 -4.1248751 -4.1299739 -4.1308484 -4.1353087 -4.1524553 -4.1758933][-4.1214786 -4.1284151 -4.1394439 -4.1531429 -4.1524715 -4.1376109 -4.1243596 -4.1268811 -4.1397982 -4.1479692 -4.1498814 -4.150629 -4.1549416 -4.1739197 -4.1965709][-4.1315851 -4.1340747 -4.1401811 -4.1521826 -4.1560307 -4.1472821 -4.1402 -4.146265 -4.1604414 -4.1681895 -4.16805 -4.16786 -4.1716695 -4.18903 -4.209928][-4.138103 -4.1389813 -4.1414847 -4.15277 -4.16109 -4.156826 -4.1542468 -4.1612043 -4.1757336 -4.1843758 -4.1837645 -4.1836662 -4.1879845 -4.2031741 -4.2225938]]...]
INFO - root - 2017-12-06 06:29:12.822639: step 5310, loss = 2.11, batch loss = 2.06 (12.3 examples/sec; 0.649 sec/batch; 58h:58m:06s remains)
INFO - root - 2017-12-06 06:29:19.406376: step 5320, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 59h:35m:52s remains)
INFO - root - 2017-12-06 06:29:25.982154: step 5330, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 60h:00m:05s remains)
INFO - root - 2017-12-06 06:29:32.554325: step 5340, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.662 sec/batch; 60h:08m:59s remains)
INFO - root - 2017-12-06 06:29:39.253576: step 5350, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.687 sec/batch; 62h:23m:44s remains)
INFO - root - 2017-12-06 06:29:45.765097: step 5360, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 60h:09m:27s remains)
INFO - root - 2017-12-06 06:29:52.376412: step 5370, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 60h:20m:27s remains)
INFO - root - 2017-12-06 06:29:58.836211: step 5380, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 58h:18m:31s remains)
INFO - root - 2017-12-06 06:30:05.205835: step 5390, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 59h:42m:46s remains)
INFO - root - 2017-12-06 06:30:11.801753: step 5400, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 61h:06m:05s remains)
2017-12-06 06:30:12.419216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3082156 -4.3054519 -4.3016229 -4.2937856 -4.2885661 -4.2880225 -4.2865419 -4.2861977 -4.2817597 -4.2747221 -4.2702832 -4.2679124 -4.2705464 -4.2791495 -4.28875][-4.3108668 -4.3160405 -4.3187418 -4.3124976 -4.3069468 -4.3075843 -4.3072925 -4.3079796 -4.3031378 -4.2923074 -4.2829981 -4.2726116 -4.267457 -4.2706752 -4.2770643][-4.3094225 -4.3200078 -4.326385 -4.3208323 -4.3128357 -4.3113995 -4.311522 -4.3129435 -4.3107514 -4.2992444 -4.2871017 -4.2702432 -4.2582574 -4.258132 -4.2624941][-4.2938051 -4.30416 -4.30985 -4.3000383 -4.289 -4.286447 -4.2897549 -4.2969851 -4.3026872 -4.2949615 -4.2822809 -4.2627616 -4.2485023 -4.2468047 -4.2493033][-4.2772837 -4.2824621 -4.285923 -4.2732282 -4.2573247 -4.2476482 -4.2506933 -4.266016 -4.284503 -4.2842937 -4.2747388 -4.2576509 -4.2445436 -4.2403865 -4.2423697][-4.2609076 -4.2565732 -4.2490396 -4.2281246 -4.2007213 -4.1733379 -4.1681132 -4.1965947 -4.2370853 -4.2564988 -4.2603049 -4.2533875 -4.2476177 -4.2466469 -4.2484188][-4.2492061 -4.2357759 -4.2173944 -4.1875677 -4.1450834 -4.0922322 -4.0672159 -4.1042147 -4.1663852 -4.2079372 -4.2304077 -4.2381868 -4.243187 -4.2510242 -4.2576685][-4.2370892 -4.2191591 -4.1905432 -4.1414046 -4.0714173 -3.9877248 -3.930181 -3.9625714 -4.0442324 -4.1124172 -4.1626806 -4.1991029 -4.2209511 -4.2396841 -4.253459][-4.2253733 -4.2093449 -4.1781216 -4.1168633 -4.0325069 -3.9347379 -3.85635 -3.8702171 -3.952631 -4.0322556 -4.1024294 -4.1595097 -4.19042 -4.2133155 -4.2295175][-4.2353563 -4.2352209 -4.2224407 -4.1804948 -4.1203361 -4.0469403 -3.97728 -3.9645398 -4.0079737 -4.0583258 -4.1113682 -4.1543007 -4.1755328 -4.1905212 -4.2037511][-4.2480702 -4.2578292 -4.2575808 -4.2363482 -4.2058382 -4.1656713 -4.1170669 -4.0901709 -4.09295 -4.1069651 -4.1300521 -4.1513262 -4.1600609 -4.1709609 -4.1862311][-4.2354245 -4.2436285 -4.249217 -4.2408447 -4.2289815 -4.2138681 -4.1870875 -4.1635032 -4.14827 -4.1415324 -4.1448722 -4.14999 -4.151598 -4.1630907 -4.181982][-4.2258725 -4.23274 -4.2417183 -4.2410707 -4.2396297 -4.2385745 -4.2296782 -4.2170024 -4.2000284 -4.183857 -4.1783619 -4.1769786 -4.1771297 -4.1884928 -4.2079878][-4.2384086 -4.2415943 -4.2474232 -4.2489219 -4.2558227 -4.2646837 -4.268713 -4.2670465 -4.2573872 -4.2443986 -4.2360168 -4.2304235 -4.2274241 -4.2338448 -4.246315][-4.2630811 -4.264184 -4.2691364 -4.2725916 -4.2801409 -4.2898893 -4.296267 -4.2985959 -4.2971592 -4.2924395 -4.2844634 -4.277101 -4.27319 -4.2738619 -4.2791252]]...]
INFO - root - 2017-12-06 06:30:19.005049: step 5410, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 57h:19m:43s remains)
INFO - root - 2017-12-06 06:30:25.589599: step 5420, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 58h:17m:58s remains)
INFO - root - 2017-12-06 06:30:31.995329: step 5430, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 59h:54m:26s remains)
INFO - root - 2017-12-06 06:30:38.440721: step 5440, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 59h:45m:10s remains)
INFO - root - 2017-12-06 06:30:45.045468: step 5450, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 59h:19m:59s remains)
INFO - root - 2017-12-06 06:30:51.526393: step 5460, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 59h:03m:41s remains)
INFO - root - 2017-12-06 06:30:58.182590: step 5470, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 61h:53m:34s remains)
INFO - root - 2017-12-06 06:31:04.810235: step 5480, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.672 sec/batch; 61h:03m:18s remains)
INFO - root - 2017-12-06 06:31:11.363511: step 5490, loss = 2.08, batch loss = 2.02 (14.4 examples/sec; 0.555 sec/batch; 50h:22m:42s remains)
INFO - root - 2017-12-06 06:31:17.761765: step 5500, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 59h:58m:40s remains)
2017-12-06 06:31:18.454897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2147884 -4.2158051 -4.2092738 -4.2045255 -4.2086229 -4.2157269 -4.2225628 -4.2315454 -4.2403822 -4.243536 -4.2360063 -4.2374787 -4.2514749 -4.2556396 -4.2523541][-4.2029119 -4.208725 -4.2001605 -4.1927905 -4.1973758 -4.2021112 -4.2028995 -4.2048941 -4.2117715 -4.2177525 -4.2116408 -4.2158475 -4.2364321 -4.2465162 -4.2453041][-4.1771355 -4.1865926 -4.179594 -4.176044 -4.1862426 -4.1963315 -4.1981969 -4.2017074 -4.20597 -4.2111168 -4.2030091 -4.2019591 -4.2206488 -4.2370348 -4.2397518][-4.1611533 -4.1688018 -4.1638694 -4.1650867 -4.1747308 -4.1835842 -4.1884365 -4.1966696 -4.203567 -4.2091918 -4.2016444 -4.1918321 -4.2052469 -4.2259483 -4.2357879][-4.1525173 -4.1560869 -4.1544957 -4.1600575 -4.1654778 -4.1661506 -4.1708922 -4.1816216 -4.191175 -4.1975741 -4.1906981 -4.1714606 -4.1765027 -4.1994491 -4.2181063][-4.1435575 -4.1384954 -4.1324368 -4.1390681 -4.1429596 -4.1331525 -4.1269307 -4.1373515 -4.1542826 -4.1667762 -4.1648765 -4.14048 -4.13595 -4.1556592 -4.1750851][-4.1408014 -4.1243072 -4.1012335 -4.0929885 -4.0845423 -4.046504 -4.0061483 -4.0212512 -4.0726557 -4.1136837 -4.129096 -4.1115704 -4.1047444 -4.1216474 -4.1363659][-4.1131611 -4.0902057 -4.055409 -4.026731 -3.9965844 -3.921277 -3.8186908 -3.8248227 -3.9415011 -4.0340762 -4.07931 -4.0763154 -4.0821481 -4.1080217 -4.1248379][-4.0898156 -4.0616407 -4.0242376 -3.9881914 -3.9577494 -3.8745308 -3.73215 -3.7031424 -3.8535287 -3.9823732 -4.0422368 -4.0526376 -4.0784545 -4.1199188 -4.1411352][-4.0838513 -4.0706911 -4.0551891 -4.0344291 -4.0272632 -3.9799144 -3.8853431 -3.8537359 -3.9507902 -4.0481763 -4.0919504 -4.1097717 -4.139781 -4.1759462 -4.1906734][-4.089252 -4.0890317 -4.09441 -4.0989161 -4.1162663 -4.1071568 -4.06164 -4.0431118 -4.0834675 -4.1349564 -4.1545095 -4.1709657 -4.1960268 -4.2199512 -4.2275786][-4.10819 -4.1211138 -4.1411419 -4.1571259 -4.1809931 -4.1938548 -4.1798086 -4.1701884 -4.1764684 -4.1911459 -4.1924968 -4.2009616 -4.222415 -4.240777 -4.2461791][-4.135541 -4.1513448 -4.1809855 -4.2048755 -4.2238674 -4.2349143 -4.2332096 -4.2264466 -4.2167935 -4.2128787 -4.1959343 -4.18348 -4.1960211 -4.2152057 -4.2289052][-4.1541166 -4.16962 -4.2032065 -4.223238 -4.2356215 -4.2437015 -4.245461 -4.237833 -4.2231817 -4.2084088 -4.1809483 -4.1559277 -4.1687794 -4.2014618 -4.2248988][-4.1678128 -4.1769528 -4.2059326 -4.2214637 -4.2279081 -4.2355475 -4.2410064 -4.2386322 -4.2308164 -4.2196054 -4.1998153 -4.1831045 -4.1986861 -4.2352223 -4.2595239]]...]
INFO - root - 2017-12-06 06:31:25.075506: step 5510, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 60h:34m:33s remains)
INFO - root - 2017-12-06 06:31:31.612694: step 5520, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 59h:39m:13s remains)
INFO - root - 2017-12-06 06:31:38.166225: step 5530, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 60h:03m:04s remains)
INFO - root - 2017-12-06 06:31:44.657703: step 5540, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 58h:36m:17s remains)
INFO - root - 2017-12-06 06:31:51.029114: step 5550, loss = 2.08, batch loss = 2.02 (15.8 examples/sec; 0.505 sec/batch; 45h:50m:35s remains)
INFO - root - 2017-12-06 06:31:57.663632: step 5560, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 63h:00m:07s remains)
INFO - root - 2017-12-06 06:32:04.173000: step 5570, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.624 sec/batch; 56h:41m:28s remains)
INFO - root - 2017-12-06 06:32:10.607495: step 5580, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 59h:21m:51s remains)
INFO - root - 2017-12-06 06:32:17.136521: step 5590, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:59m:41s remains)
INFO - root - 2017-12-06 06:32:23.558624: step 5600, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 60h:25m:51s remains)
2017-12-06 06:32:24.244786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2872705 -4.2787757 -4.2687936 -4.2596087 -4.2488775 -4.24369 -4.2430649 -4.2464428 -4.254828 -4.2671566 -4.2805419 -4.28827 -4.2879963 -4.2883992 -4.2943][-4.2707996 -4.2556343 -4.2395244 -4.2247658 -4.209024 -4.2019978 -4.1958532 -4.1928415 -4.2003174 -4.2207508 -4.2445884 -4.2600746 -4.2640772 -4.2663288 -4.2742867][-4.2526989 -4.2306809 -4.211441 -4.1948051 -4.1746469 -4.1563277 -4.1348138 -4.1223578 -4.1302619 -4.1632209 -4.2032313 -4.2314205 -4.242465 -4.2449641 -4.2507253][-4.2365828 -4.20866 -4.1901217 -4.1766481 -4.1481705 -4.1067681 -4.0578046 -4.032011 -4.04193 -4.0920658 -4.1547222 -4.1982675 -4.2197461 -4.2245841 -4.2280946][-4.2215405 -4.190907 -4.1776543 -4.1701241 -4.134243 -4.0679312 -3.9898741 -3.9443974 -3.9510846 -4.0185475 -4.1050968 -4.1652493 -4.19751 -4.2076268 -4.2113886][-4.203867 -4.1758122 -4.1707177 -4.1690941 -4.1281266 -4.0427346 -3.9351387 -3.8580132 -3.8493116 -3.9349077 -4.0514731 -4.1313391 -4.1759171 -4.193068 -4.2015038][-4.1779933 -4.1509519 -4.1552629 -4.1656165 -4.13428 -4.0471334 -3.9223816 -3.8058853 -3.7609494 -3.8524251 -3.9981318 -4.0961366 -4.1497726 -4.1732969 -4.1893244][-4.1512232 -4.1222191 -4.1315722 -4.153079 -4.1372805 -4.0635376 -3.9316275 -3.7824142 -3.7042966 -3.8027654 -3.9636312 -4.063735 -4.1171694 -4.1428094 -4.1652646][-4.1481061 -4.1171355 -4.1281672 -4.1585965 -4.1603813 -4.1117873 -3.9969153 -3.8535674 -3.7765169 -3.862196 -3.9973195 -4.070888 -4.1080828 -4.1252561 -4.1458793][-4.172245 -4.1397729 -4.1460195 -4.1779175 -4.1928706 -4.169539 -4.0851154 -3.9648979 -3.8945942 -3.9486017 -4.0461307 -4.0938478 -4.1120052 -4.1197672 -4.1391487][-4.220542 -4.190155 -4.188869 -4.2106156 -4.2286878 -4.2257638 -4.1727953 -4.0797143 -4.0153594 -4.03894 -4.1039252 -4.1375451 -4.1456604 -4.1498547 -4.1657224][-4.2714753 -4.2480626 -4.2421589 -4.2526255 -4.26763 -4.2750354 -4.2443404 -4.1745925 -4.1199965 -4.1239781 -4.1654644 -4.1915812 -4.19674 -4.2005281 -4.2128572][-4.3029432 -4.2872972 -4.2828803 -4.2883558 -4.2970076 -4.3037081 -4.284059 -4.2329264 -4.1904426 -4.1872425 -4.2137103 -4.2341347 -4.2426834 -4.2486153 -4.2583218][-4.3132687 -4.3048663 -4.3039031 -4.3072109 -4.3110008 -4.315917 -4.3031082 -4.2701631 -4.2404957 -4.2353234 -4.2490931 -4.2640119 -4.2771544 -4.2853456 -4.2927418][-4.3158689 -4.3118052 -4.3129892 -4.3149505 -4.3137527 -4.3114624 -4.3022852 -4.2849617 -4.267961 -4.2637386 -4.2698212 -4.2804589 -4.2955389 -4.3075643 -4.314064]]...]
INFO - root - 2017-12-06 06:32:30.857600: step 5610, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.666 sec/batch; 60h:28m:11s remains)
INFO - root - 2017-12-06 06:32:37.329673: step 5620, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 58h:46m:01s remains)
INFO - root - 2017-12-06 06:32:43.924908: step 5630, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 61h:01m:26s remains)
INFO - root - 2017-12-06 06:32:50.550382: step 5640, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 61h:46m:30s remains)
INFO - root - 2017-12-06 06:32:56.971743: step 5650, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 58h:23m:56s remains)
INFO - root - 2017-12-06 06:33:03.533577: step 5660, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 58h:35m:28s remains)
INFO - root - 2017-12-06 06:33:10.002933: step 5670, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 58h:09m:16s remains)
INFO - root - 2017-12-06 06:33:16.577672: step 5680, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 59h:58m:36s remains)
INFO - root - 2017-12-06 06:33:23.220929: step 5690, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 60h:21m:36s remains)
INFO - root - 2017-12-06 06:33:29.602022: step 5700, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 59h:07m:39s remains)
2017-12-06 06:33:30.244283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2383218 -4.2340355 -4.2297316 -4.232295 -4.243434 -4.25619 -4.2624855 -4.2620163 -4.2577648 -4.2543254 -4.2551723 -4.2616549 -4.2708411 -4.278861 -4.2834649][-4.2047272 -4.1955223 -4.1862254 -4.1877575 -4.2013636 -4.2182055 -4.227582 -4.2289896 -4.2260571 -4.2226658 -4.2228789 -4.2310376 -4.2424493 -4.2508955 -4.2546573][-4.1766915 -4.1621952 -4.1476669 -4.1470485 -4.1614914 -4.1796665 -4.1928616 -4.1970215 -4.1969118 -4.1929188 -4.1906028 -4.1995788 -4.21072 -4.2170997 -4.2179575][-4.1525135 -4.1382852 -4.1225491 -4.118124 -4.1266856 -4.13997 -4.152873 -4.1601992 -4.1675615 -4.1682777 -4.1653886 -4.1728959 -4.1800513 -4.1801238 -4.1749086][-4.141923 -4.1336131 -4.1182222 -4.1057081 -4.09795 -4.0958681 -4.1043143 -4.1184554 -4.1385827 -4.1526127 -4.1549106 -4.1602464 -4.1604085 -4.1483121 -4.1309319][-4.1496563 -4.1468396 -4.1281824 -4.1022487 -4.0700607 -4.0427256 -4.0418773 -4.0655694 -4.1067705 -4.1427488 -4.1567426 -4.1617937 -4.1561251 -4.1317959 -4.0998049][-4.1749873 -4.1690855 -4.1413774 -4.0953622 -4.0343313 -3.9759579 -3.9549825 -3.9846251 -4.0527778 -4.119082 -4.1534 -4.1664252 -4.162045 -4.1312575 -4.0830383][-4.2143555 -4.1987739 -4.1586828 -4.0894775 -4.0001564 -3.9082768 -3.8554468 -3.8831754 -3.9808006 -4.0851231 -4.148097 -4.1760893 -4.1798816 -4.1441936 -4.072134][-4.2539062 -4.2318296 -4.1800413 -4.0944719 -3.9846418 -3.8627472 -3.7802947 -3.8122079 -3.9395051 -4.0703492 -4.1533909 -4.1952729 -4.2051878 -4.1613607 -4.06388][-4.2832427 -4.25908 -4.2015834 -4.1113157 -3.9966969 -3.8710206 -3.7915745 -3.8373797 -3.9752481 -4.1000881 -4.1747246 -4.2131228 -4.2218814 -4.1760826 -4.0660133][-4.3029628 -4.279376 -4.2236085 -4.1426549 -4.044827 -3.9465668 -3.9043679 -3.960865 -4.0791569 -4.1738439 -4.2162337 -4.2297206 -4.231576 -4.1937017 -4.0957427][-4.3116164 -4.2921972 -4.2487283 -4.1896019 -4.1233754 -4.0706315 -4.067771 -4.1181593 -4.1965003 -4.2530727 -4.2621617 -4.2457328 -4.2343974 -4.207799 -4.1391964][-4.3080654 -4.3006582 -4.2766409 -4.2462173 -4.2149515 -4.197031 -4.2086124 -4.2405 -4.2808003 -4.3035693 -4.2903738 -4.2552147 -4.2315965 -4.2144103 -4.1787324][-4.3033676 -4.3137231 -4.3096881 -4.297677 -4.2853665 -4.2799611 -4.2884226 -4.3027487 -4.3170972 -4.3164649 -4.2951565 -4.2638717 -4.2436805 -4.2375207 -4.2299786][-4.2936468 -4.3208418 -4.3316574 -4.3286719 -4.3212113 -4.3149657 -4.317018 -4.3195515 -4.319675 -4.3101258 -4.2923765 -4.2747774 -4.2655478 -4.2681608 -4.275229]]...]
INFO - root - 2017-12-06 06:33:36.818143: step 5710, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 60h:07m:36s remains)
INFO - root - 2017-12-06 06:33:43.392553: step 5720, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 58h:41m:17s remains)
INFO - root - 2017-12-06 06:33:49.837554: step 5730, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 57h:44m:30s remains)
INFO - root - 2017-12-06 06:33:56.394959: step 5740, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 58h:16m:25s remains)
INFO - root - 2017-12-06 06:34:02.971819: step 5750, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 59h:00m:22s remains)
INFO - root - 2017-12-06 06:34:09.559048: step 5760, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.664 sec/batch; 60h:16m:47s remains)
INFO - root - 2017-12-06 06:34:16.096532: step 5770, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 59h:41m:44s remains)
INFO - root - 2017-12-06 06:34:22.657860: step 5780, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 58h:48m:12s remains)
INFO - root - 2017-12-06 06:34:29.184634: step 5790, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 58h:20m:36s remains)
INFO - root - 2017-12-06 06:34:35.717996: step 5800, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 59h:46m:01s remains)
2017-12-06 06:34:36.336313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2079697 -4.2017922 -4.2016211 -4.2108126 -4.2329311 -4.2594495 -4.2792625 -4.2825909 -4.2648244 -4.2296743 -4.1862054 -4.1638622 -4.1726856 -4.184967 -4.1822491][-4.1644416 -4.170279 -4.1822081 -4.1948962 -4.215333 -4.2395296 -4.2587438 -4.26239 -4.2469649 -4.216753 -4.1777415 -4.1566429 -4.1654372 -4.1811213 -4.1852407][-4.1318488 -4.1507664 -4.1824837 -4.2074118 -4.2279124 -4.2433367 -4.2508521 -4.249742 -4.2360239 -4.2127972 -4.1859941 -4.171092 -4.1786017 -4.1907449 -4.1929374][-4.1278014 -4.1499972 -4.1957097 -4.2239971 -4.2344141 -4.2302556 -4.2260079 -4.2232881 -4.2162075 -4.2055488 -4.1933374 -4.1854081 -4.1877766 -4.1927896 -4.1902556][-4.1290345 -4.1507416 -4.1937666 -4.2142167 -4.2057753 -4.1812325 -4.1704879 -4.1741724 -4.1768336 -4.1760349 -4.1771259 -4.1786418 -4.1808209 -4.1777916 -4.1670122][-4.1060858 -4.1200409 -4.1450438 -4.1495194 -4.1172709 -4.0706558 -4.0549712 -4.0752034 -4.0943046 -4.1076941 -4.1250749 -4.1442776 -4.1543345 -4.1442742 -4.1219015][-4.0960145 -4.0894408 -4.0812244 -4.051734 -3.9837739 -3.9065361 -3.8811741 -3.9284661 -3.9842265 -4.023942 -4.0649376 -4.1094031 -4.137424 -4.1336017 -4.1054368][-4.1077023 -4.07642 -4.0343165 -3.9675815 -3.8635719 -3.7441339 -3.6798797 -3.7455463 -3.8494997 -3.9264979 -3.9958315 -4.0698051 -4.121943 -4.1313152 -4.1125813][-4.1389632 -4.0966492 -4.0430503 -3.9683855 -3.8727746 -3.7623029 -3.6804852 -3.7232316 -3.829288 -3.9128973 -3.9835675 -4.058032 -4.1116982 -4.132031 -4.1320219][-4.1870041 -4.1503725 -4.1065531 -4.04859 -3.9871213 -3.9277954 -3.8779995 -3.8916898 -3.9558718 -4.0144138 -4.0616255 -4.1102619 -4.1407132 -4.1533251 -4.1577582][-4.2337322 -4.2061052 -4.1758347 -4.1368594 -4.0989509 -4.0739689 -4.0553722 -4.060987 -4.0995541 -4.1403689 -4.1702685 -4.2007923 -4.213428 -4.2106638 -4.2015691][-4.2639313 -4.2450328 -4.2294917 -4.2100153 -4.1908355 -4.1848536 -4.1844583 -4.1882772 -4.21039 -4.2380877 -4.2574816 -4.2756472 -4.282012 -4.2726517 -4.2535968][-4.2873111 -4.2753739 -4.2681165 -4.2621484 -4.2566738 -4.2603655 -4.2665925 -4.269197 -4.2790604 -4.2918143 -4.30294 -4.3132777 -4.3168077 -4.3082137 -4.2904997][-4.3014464 -4.29409 -4.2883344 -4.2856879 -4.2864485 -4.2920012 -4.2990952 -4.301095 -4.3055029 -4.3099723 -4.3145909 -4.3181062 -4.3206215 -4.3176236 -4.3075824][-4.3051672 -4.300611 -4.2975268 -4.2964687 -4.2989 -4.3036771 -4.3093286 -4.3131189 -4.315424 -4.3166718 -4.3170466 -4.3165259 -4.316802 -4.3164549 -4.3120155]]...]
INFO - root - 2017-12-06 06:34:42.873118: step 5810, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 58h:38m:24s remains)
INFO - root - 2017-12-06 06:34:49.513913: step 5820, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 62h:48m:35s remains)
INFO - root - 2017-12-06 06:34:56.012773: step 5830, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 58h:32m:59s remains)
INFO - root - 2017-12-06 06:35:02.543568: step 5840, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 59h:55m:12s remains)
INFO - root - 2017-12-06 06:35:08.928276: step 5850, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:21m:34s remains)
INFO - root - 2017-12-06 06:35:15.412320: step 5860, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 61h:00m:08s remains)
INFO - root - 2017-12-06 06:35:22.109668: step 5870, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 61h:23m:15s remains)
INFO - root - 2017-12-06 06:35:28.675261: step 5880, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:56m:54s remains)
INFO - root - 2017-12-06 06:35:35.260025: step 5890, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 57h:48m:13s remains)
INFO - root - 2017-12-06 06:35:41.805653: step 5900, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 59h:38m:12s remains)
2017-12-06 06:35:42.505901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2094388 -4.2075114 -4.2165947 -4.2274876 -4.2327828 -4.2308831 -4.2289357 -4.2316384 -4.2350755 -4.2386107 -4.2452745 -4.253365 -4.2567186 -4.2605 -4.2652936][-4.1847038 -4.1788988 -4.1870561 -4.1966252 -4.2007561 -4.1978006 -4.1962929 -4.2038674 -4.2124782 -4.2202067 -4.2302589 -4.2407961 -4.2436972 -4.2447581 -4.2468753][-4.174242 -4.1655078 -4.1729527 -4.1810513 -4.1824961 -4.1765981 -4.1738081 -4.1848507 -4.1989665 -4.2111192 -4.2247267 -4.2366509 -4.2375827 -4.2338762 -4.2293148][-4.17059 -4.1632018 -4.1747727 -4.1850157 -4.1838808 -4.1737723 -4.1675258 -4.1796789 -4.1969018 -4.2134004 -4.23168 -4.2461286 -4.2476416 -4.240438 -4.22898][-4.1637206 -4.1587873 -4.1811628 -4.2004795 -4.2000265 -4.1846442 -4.1720495 -4.180439 -4.1984992 -4.2184424 -4.2398305 -4.2570739 -4.2605658 -4.2513189 -4.2355924][-4.1473536 -4.1446371 -4.1796308 -4.2087045 -4.2089577 -4.1878057 -4.1700583 -4.17433 -4.1919007 -4.2136145 -4.2356048 -4.2542696 -4.2612848 -4.2524981 -4.2330842][-4.1246848 -4.1263294 -4.1719284 -4.20723 -4.20792 -4.1839557 -4.1668153 -4.1707954 -4.1875534 -4.2073474 -4.2267475 -4.2453728 -4.2554817 -4.2480531 -4.2270637][-4.114789 -4.122983 -4.1755228 -4.2118397 -4.2097888 -4.1838765 -4.1683345 -4.1717253 -4.1843548 -4.2014956 -4.2192369 -4.2384338 -4.2524037 -4.2488413 -4.2302742][-4.1135287 -4.1292682 -4.1847873 -4.21793 -4.2098355 -4.1797256 -4.1642718 -4.1702762 -4.1836514 -4.2016544 -4.219614 -4.2403316 -4.257266 -4.2563982 -4.2414923][-4.1254973 -4.1468315 -4.2010207 -4.2295766 -4.2161164 -4.1824093 -4.1676373 -4.1776114 -4.1927671 -4.20957 -4.2251859 -4.2442379 -4.260911 -4.2612033 -4.2505][-4.1551723 -4.1766653 -4.2226691 -4.24373 -4.2266049 -4.1929641 -4.1807556 -4.1956434 -4.2126026 -4.2262392 -4.2391148 -4.2553325 -4.2697554 -4.2692308 -4.2600522][-4.1874676 -4.2050109 -4.2391381 -4.2519422 -4.2341528 -4.2052188 -4.1972318 -4.2145147 -4.2311525 -4.2420912 -4.2518859 -4.2648888 -4.2765937 -4.275218 -4.2665944][-4.2127733 -4.2243896 -4.248374 -4.2572942 -4.2435718 -4.223279 -4.2190986 -4.2342019 -4.2469349 -4.254446 -4.26169 -4.2721071 -4.2819624 -4.2807069 -4.2734623][-4.2394986 -4.2448182 -4.2604504 -4.2681451 -4.2616153 -4.2505193 -4.2489758 -4.2591586 -4.2668023 -4.2716146 -4.2767749 -4.2841558 -4.2910805 -4.2904782 -4.2866569][-4.272728 -4.2736573 -4.2832627 -4.2902222 -4.2899961 -4.2862692 -4.2865963 -4.2921224 -4.2955694 -4.2981882 -4.301013 -4.305388 -4.3090668 -4.3083563 -4.3067427]]...]
INFO - root - 2017-12-06 06:35:49.038290: step 5910, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 59h:17m:24s remains)
INFO - root - 2017-12-06 06:35:55.441636: step 5920, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 57h:24m:01s remains)
INFO - root - 2017-12-06 06:36:01.983091: step 5930, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 58h:33m:41s remains)
INFO - root - 2017-12-06 06:36:08.464292: step 5940, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 59h:52m:43s remains)
INFO - root - 2017-12-06 06:36:15.160587: step 5950, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 60h:38m:32s remains)
INFO - root - 2017-12-06 06:36:21.659501: step 5960, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 61h:00m:10s remains)
INFO - root - 2017-12-06 06:36:28.213387: step 5970, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 59h:05m:31s remains)
INFO - root - 2017-12-06 06:36:34.853101: step 5980, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 59h:24m:59s remains)
INFO - root - 2017-12-06 06:36:41.455418: step 5990, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 57h:29m:55s remains)
INFO - root - 2017-12-06 06:36:47.984394: step 6000, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 60h:05m:05s remains)
2017-12-06 06:36:48.639910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3538809 -4.3538575 -4.3549671 -4.3556514 -4.3533611 -4.3426313 -4.3234143 -4.3040967 -4.2928853 -4.294373 -4.3056493 -4.3249626 -4.3413272 -4.3508916 -4.3525658][-4.3624611 -4.3642578 -4.3658624 -4.3632159 -4.352479 -4.3283715 -4.2930875 -4.2607594 -4.2454967 -4.2506185 -4.2707214 -4.3032265 -4.3318496 -4.3486657 -4.3532677][-4.3667078 -4.3702083 -4.3709769 -4.3609519 -4.337719 -4.2962427 -4.2417378 -4.1964793 -4.1802015 -4.1938658 -4.2282019 -4.2763882 -4.3161826 -4.3395386 -4.3482471][-4.367775 -4.3720255 -4.3697495 -4.3506 -4.31302 -4.25279 -4.1793966 -4.1258068 -4.1164813 -4.1457682 -4.1965365 -4.2579708 -4.3055964 -4.3337541 -4.3439283][-4.3694677 -4.3727117 -4.3654242 -4.3341336 -4.2773004 -4.1931219 -4.1009789 -4.0442762 -4.0512681 -4.1050634 -4.1773562 -4.2505069 -4.3033085 -4.3326807 -4.34214][-4.3650613 -4.3664894 -4.3526354 -4.3054228 -4.2253556 -4.1152749 -4.0023251 -3.9470294 -3.9805188 -4.0667744 -4.1651688 -4.2492275 -4.3059435 -4.33537 -4.3440318][-4.3461032 -4.3489389 -4.3297396 -4.2689257 -4.1682305 -4.0326519 -3.8985746 -3.85519 -3.9237344 -4.0394287 -4.1571755 -4.249258 -4.3099976 -4.3402233 -4.3472977][-4.3153958 -4.322329 -4.3002114 -4.2283359 -4.1142406 -3.9630723 -3.8263302 -3.81507 -3.9199405 -4.0520482 -4.1725779 -4.2608657 -4.318459 -4.3457022 -4.3495893][-4.2815971 -4.2958221 -4.2759628 -4.2028084 -4.0882955 -3.9460654 -3.8363905 -3.8576317 -3.9790037 -4.1079969 -4.2130265 -4.2856464 -4.3318758 -4.349875 -4.3483596][-4.2569776 -4.2824421 -4.2724247 -4.208456 -4.1101656 -4.0024457 -3.9383178 -3.9768841 -4.0845809 -4.1876712 -4.2681208 -4.3214865 -4.3496184 -4.3551121 -4.3470325][-4.2422647 -4.2803535 -4.2853093 -4.2439623 -4.1765552 -4.1092172 -4.0812483 -4.1183105 -4.1932435 -4.2628441 -4.3183742 -4.3531828 -4.36566 -4.3612413 -4.3485365][-4.2327576 -4.2861938 -4.3063197 -4.2918677 -4.2538557 -4.2126346 -4.1965032 -4.2198906 -4.2652755 -4.3083148 -4.3465362 -4.3700066 -4.3755875 -4.3671293 -4.353241][-4.2274833 -4.2938576 -4.3280835 -4.3308764 -4.3062158 -4.27384 -4.2567244 -4.264472 -4.2885885 -4.3162251 -4.3473516 -4.3684893 -4.3748426 -4.3680372 -4.3560042][-4.2403221 -4.31105 -4.3481441 -4.3501964 -4.3227496 -4.285759 -4.2617693 -4.2594237 -4.2740593 -4.2963142 -4.327251 -4.3527508 -4.3644857 -4.3624139 -4.3547449][-4.2744756 -4.331141 -4.354773 -4.3415313 -4.3010907 -4.2548633 -4.2242093 -4.218051 -4.2332954 -4.2602491 -4.29893 -4.331048 -4.3489008 -4.3534651 -4.3505487]]...]
INFO - root - 2017-12-06 06:36:55.138238: step 6010, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 59h:25m:34s remains)
INFO - root - 2017-12-06 06:37:01.604351: step 6020, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 58h:48m:58s remains)
INFO - root - 2017-12-06 06:37:08.259339: step 6030, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.658 sec/batch; 59h:42m:46s remains)
INFO - root - 2017-12-06 06:37:14.597978: step 6040, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 58h:36m:46s remains)
INFO - root - 2017-12-06 06:37:21.206874: step 6050, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 61h:19m:11s remains)
INFO - root - 2017-12-06 06:37:27.768403: step 6060, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 58h:22m:08s remains)
INFO - root - 2017-12-06 06:37:34.253524: step 6070, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 61h:19m:40s remains)
INFO - root - 2017-12-06 06:37:40.727084: step 6080, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 56h:50m:52s remains)
INFO - root - 2017-12-06 06:37:47.244123: step 6090, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 59h:40m:57s remains)
INFO - root - 2017-12-06 06:37:53.786029: step 6100, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 59h:06m:24s remains)
2017-12-06 06:37:54.380249: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3241224 -4.3326473 -4.3395104 -4.34063 -4.33304 -4.3199015 -4.3098536 -4.3078213 -4.3112183 -4.3211942 -4.3307924 -4.3377929 -4.3380294 -4.3312221 -4.321218][-4.3167491 -4.3226972 -4.3279424 -4.3282704 -4.3173981 -4.298399 -4.2852125 -4.283165 -4.2850704 -4.2952819 -4.3115673 -4.3244205 -4.3254933 -4.3199263 -4.313642][-4.3134212 -4.3150687 -4.3170457 -4.3155417 -4.3006344 -4.2761059 -4.2603836 -4.2616239 -4.26365 -4.2725863 -4.293848 -4.3123527 -4.3132429 -4.3061361 -4.3015165][-4.3093939 -4.3054333 -4.3008523 -4.2954521 -4.27738 -4.2494226 -4.2293382 -4.230268 -4.23367 -4.2443724 -4.2677236 -4.2901325 -4.2899966 -4.2810874 -4.2760611][-4.299221 -4.2885303 -4.2786632 -4.2741108 -4.259223 -4.2295575 -4.2031679 -4.2021003 -4.2082071 -4.2195969 -4.2363124 -4.2521834 -4.2470961 -4.2349396 -4.2303691][-4.2938166 -4.2783742 -4.2638788 -4.2556343 -4.240715 -4.2101502 -4.181541 -4.1842685 -4.1993523 -4.2141337 -4.2238851 -4.2292366 -4.2152014 -4.194653 -4.1882844][-4.2900538 -4.2699661 -4.2489309 -4.2333255 -4.2150555 -4.1827407 -4.15318 -4.1576991 -4.1800213 -4.2044764 -4.2227783 -4.2273593 -4.2080388 -4.181181 -4.1693344][-4.2883306 -4.2647905 -4.2388439 -4.2162747 -4.1924706 -4.1586452 -4.1276064 -4.1280394 -4.1542554 -4.1923356 -4.2260017 -4.2349787 -4.2174969 -4.1934977 -4.1775155][-4.2909584 -4.2691827 -4.2428226 -4.2172117 -4.1903386 -4.156796 -4.1248441 -4.1176953 -4.137959 -4.1800537 -4.2215023 -4.233778 -4.2254114 -4.2134013 -4.2012367][-4.2940779 -4.2777858 -4.2545972 -4.2308459 -4.2031703 -4.1687016 -4.134479 -4.1234488 -4.1390729 -4.1743574 -4.2103229 -4.2246819 -4.224483 -4.2203336 -4.2103105][-4.2916112 -4.2792168 -4.2594585 -4.237762 -4.2114396 -4.1784077 -4.144557 -4.1360326 -4.1513066 -4.1808224 -4.2111368 -4.2268558 -4.2310042 -4.2295384 -4.2217159][-4.2910614 -4.2802639 -4.2641816 -4.2458787 -4.22523 -4.1991692 -4.1713042 -4.1664672 -4.1831503 -4.2081161 -4.2308636 -4.2430248 -4.2474384 -4.2470207 -4.2423544][-4.292232 -4.2849689 -4.2756104 -4.26493 -4.2554851 -4.2417212 -4.2246065 -4.2227011 -4.2351818 -4.2516828 -4.2638659 -4.2702308 -4.27178 -4.2717466 -4.2708726][-4.2949257 -4.2912197 -4.28894 -4.2863421 -4.2854247 -4.2814765 -4.2749076 -4.2754393 -4.2824268 -4.2899671 -4.2940416 -4.2951975 -4.2949972 -4.294838 -4.2946057][-4.2965527 -4.2934875 -4.2932491 -4.293396 -4.2958031 -4.29719 -4.296989 -4.2990179 -4.3029313 -4.3048811 -4.3045368 -4.30348 -4.3026266 -4.3022923 -4.3018136]]...]
INFO - root - 2017-12-06 06:38:00.851291: step 6110, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.617 sec/batch; 55h:56m:36s remains)
INFO - root - 2017-12-06 06:38:07.323779: step 6120, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.653 sec/batch; 59h:10m:32s remains)
INFO - root - 2017-12-06 06:38:13.950241: step 6130, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 58h:31m:27s remains)
INFO - root - 2017-12-06 06:38:20.392943: step 6140, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 61h:32m:26s remains)
INFO - root - 2017-12-06 06:38:27.036174: step 6150, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:55m:13s remains)
INFO - root - 2017-12-06 06:38:33.649031: step 6160, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 61h:38m:09s remains)
INFO - root - 2017-12-06 06:38:40.317451: step 6170, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 64h:02m:22s remains)
INFO - root - 2017-12-06 06:38:46.852515: step 6180, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 60h:58m:31s remains)
INFO - root - 2017-12-06 06:38:53.222420: step 6190, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.628 sec/batch; 56h:57m:46s remains)
INFO - root - 2017-12-06 06:38:59.808097: step 6200, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 60h:29m:04s remains)
2017-12-06 06:39:00.495496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.215488 -4.2013164 -4.1996503 -4.2089739 -4.2085962 -4.189764 -4.1571517 -4.10929 -4.0690107 -4.0804777 -4.1328673 -4.17859 -4.2116756 -4.2512369 -4.2956281][-4.2152019 -4.2070942 -4.2136307 -4.2247624 -4.2169523 -4.1854491 -4.1434627 -4.0936871 -4.0597906 -4.077847 -4.1362267 -4.1881223 -4.2313175 -4.2771568 -4.3217988][-4.2322726 -4.2297249 -4.2396722 -4.2487726 -4.2339931 -4.1918073 -4.1427593 -4.0943675 -4.0681534 -4.0893111 -4.1495404 -4.2075968 -4.2580142 -4.3050609 -4.3414593][-4.2540669 -4.2576256 -4.2692409 -4.2731977 -4.2482905 -4.1934276 -4.1334586 -4.084455 -4.0676804 -4.0985427 -4.1640015 -4.226583 -4.278861 -4.3188872 -4.3445821][-4.2782011 -4.2825565 -4.2907434 -4.2854137 -4.2472587 -4.1774893 -4.0988984 -4.0473843 -4.0472493 -4.0971775 -4.1751361 -4.2457151 -4.2959065 -4.3228807 -4.3314934][-4.2977009 -4.3003716 -4.3012395 -4.2835145 -4.2306972 -4.1409078 -4.038548 -3.9889305 -4.02098 -4.1005883 -4.1943078 -4.27091 -4.3154664 -4.3258247 -4.3116555][-4.3068147 -4.3051538 -4.2948256 -4.2628946 -4.189013 -4.0727487 -3.9555819 -3.9293761 -4.0094085 -4.1192265 -4.2206583 -4.2921166 -4.3241272 -4.3173766 -4.281652][-4.2921858 -4.2844405 -4.2650862 -4.2168217 -4.1216879 -3.9920268 -3.8933625 -3.9134669 -4.0313015 -4.1506448 -4.2436028 -4.3006415 -4.3167114 -4.2935891 -4.2416444][-4.2549324 -4.2467465 -4.2247677 -4.16651 -4.0650349 -3.9551013 -3.9115062 -3.9717915 -4.0886669 -4.1918178 -4.2653346 -4.3027773 -4.3021665 -4.2647662 -4.2051969][-4.2170572 -4.216578 -4.1950274 -4.1323018 -4.0357542 -3.9586182 -3.9636743 -4.0425372 -4.1443019 -4.2287855 -4.2851748 -4.3014927 -4.2847095 -4.2358775 -4.1745572][-4.1836457 -4.1900625 -4.1670065 -4.1028233 -4.0211511 -3.9804435 -4.0201011 -4.1060538 -4.1917439 -4.259119 -4.2996173 -4.3005958 -4.2690668 -4.2102642 -4.1487846][-4.1653609 -4.173286 -4.1474829 -4.0899792 -4.0357833 -4.0273662 -4.0841079 -4.163743 -4.2281766 -4.2761827 -4.3043394 -4.2943025 -4.2513113 -4.1856384 -4.1246939][-4.1749821 -4.1752777 -4.1455064 -4.1016707 -4.0734577 -4.0831885 -4.1390839 -4.2060523 -4.254735 -4.2905931 -4.3071547 -4.2868595 -4.2356329 -4.1629982 -4.1018381][-4.1876974 -4.1821184 -4.1541691 -4.1256857 -4.1174541 -4.1341338 -4.1825995 -4.2343068 -4.2721081 -4.2969351 -4.3011265 -4.2733154 -4.2171111 -4.1404667 -4.0835156][-4.2036643 -4.2053533 -4.185864 -4.1673579 -4.1660542 -4.1782756 -4.2121511 -4.2470875 -4.273212 -4.2863703 -4.2808542 -4.2481351 -4.191206 -4.1217608 -4.0819421]]...]
INFO - root - 2017-12-06 06:39:07.079204: step 6210, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 58h:05m:43s remains)
INFO - root - 2017-12-06 06:39:13.501036: step 6220, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 59h:35m:38s remains)
INFO - root - 2017-12-06 06:39:20.095872: step 6230, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 61h:03m:07s remains)
INFO - root - 2017-12-06 06:39:26.621911: step 6240, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 60h:15m:32s remains)
INFO - root - 2017-12-06 06:39:33.179478: step 6250, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.642 sec/batch; 58h:12m:10s remains)
INFO - root - 2017-12-06 06:39:39.534218: step 6260, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 58h:07m:43s remains)
INFO - root - 2017-12-06 06:39:46.119022: step 6270, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 58h:44m:06s remains)
INFO - root - 2017-12-06 06:39:52.755020: step 6280, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 62h:36m:25s remains)
INFO - root - 2017-12-06 06:39:59.279196: step 6290, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.658 sec/batch; 59h:39m:53s remains)
INFO - root - 2017-12-06 06:40:05.863872: step 6300, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 59h:32m:17s remains)
2017-12-06 06:40:06.609510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3244128 -4.2870727 -4.2379756 -4.1681528 -4.09426 -4.066721 -4.0994735 -4.1396594 -4.1551433 -4.1615734 -4.1677871 -4.1745429 -4.1812782 -4.196229 -4.2135811][-4.3267994 -4.2875309 -4.2351918 -4.15744 -4.0684037 -4.0215492 -4.0569453 -4.1193423 -4.1479168 -4.1553073 -4.1579418 -4.1637654 -4.1748991 -4.197331 -4.221611][-4.3314705 -4.2941961 -4.2422996 -4.1577225 -4.0532165 -3.9801753 -4.0095224 -4.0932221 -4.1386404 -4.1495571 -4.1496348 -4.154129 -4.1683478 -4.1981449 -4.2291288][-4.335557 -4.3030763 -4.2546778 -4.1687341 -4.0563235 -3.9584143 -3.9689951 -4.0648942 -4.1253319 -4.141355 -4.137702 -4.1387491 -4.1560731 -4.1923962 -4.2285471][-4.336195 -4.3089828 -4.2657881 -4.185822 -4.0753808 -3.960803 -3.9460652 -4.0432367 -4.1145411 -4.1381521 -4.1338725 -4.1310105 -4.1476455 -4.1841025 -4.2193294][-4.3347912 -4.3130608 -4.2779632 -4.2103186 -4.1082091 -3.9870772 -3.9478111 -4.0306435 -4.1060119 -4.1358461 -4.1363983 -4.1323 -4.1462216 -4.1773825 -4.2045975][-4.3334627 -4.3159571 -4.2892251 -4.2329178 -4.1400337 -4.018487 -3.9587948 -4.0164733 -4.0910392 -4.131866 -4.1432757 -4.1413426 -4.1486325 -4.1676826 -4.1797781][-4.3324533 -4.3179383 -4.2985144 -4.2541108 -4.1754889 -4.0643058 -3.9943507 -4.0243454 -4.0880752 -4.1394143 -4.163599 -4.1675873 -4.1683393 -4.1709485 -4.1652327][-4.3320618 -4.3184366 -4.3033524 -4.2697558 -4.2088003 -4.1160474 -4.0477581 -4.0553412 -4.0988469 -4.1516976 -4.186974 -4.2029748 -4.2033067 -4.1939225 -4.1744437][-4.333518 -4.3200045 -4.3074975 -4.2820454 -4.2406149 -4.1693907 -4.1117492 -4.1076641 -4.1303906 -4.1741581 -4.2131157 -4.2391448 -4.2432737 -4.2284508 -4.2020016][-4.3361282 -4.3239384 -4.3128872 -4.2927036 -4.2667565 -4.2175989 -4.172143 -4.1615806 -4.1681128 -4.199502 -4.2409153 -4.2784982 -4.29001 -4.2758369 -4.2457061][-4.3396392 -4.3285146 -4.3167648 -4.2983193 -4.28353 -4.2561803 -4.222249 -4.2084556 -4.2041698 -4.2251458 -4.2688079 -4.3156791 -4.3332639 -4.3203592 -4.2908][-4.34378 -4.3340459 -4.3228045 -4.3069692 -4.2978215 -4.2848358 -4.2616858 -4.24657 -4.2390008 -4.25539 -4.2963233 -4.3450494 -4.3665576 -4.3558517 -4.3298597][-4.3482103 -4.3387 -4.327035 -4.3119183 -4.3053508 -4.3013511 -4.2879725 -4.2748656 -4.2681279 -4.2817464 -4.3154073 -4.3576 -4.3775949 -4.3699527 -4.3507304][-4.3492279 -4.3394928 -4.3284335 -4.3169641 -4.3134985 -4.3146415 -4.3089247 -4.2979007 -4.2887011 -4.2936454 -4.315732 -4.3493223 -4.3691454 -4.3678508 -4.3559818]]...]
INFO - root - 2017-12-06 06:40:13.053892: step 6310, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 59h:01m:50s remains)
INFO - root - 2017-12-06 06:40:19.414006: step 6320, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 59h:15m:46s remains)
INFO - root - 2017-12-06 06:40:25.764700: step 6330, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 59h:42m:49s remains)
INFO - root - 2017-12-06 06:40:32.256063: step 6340, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 57h:48m:17s remains)
INFO - root - 2017-12-06 06:40:38.898506: step 6350, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 60h:29m:59s remains)
INFO - root - 2017-12-06 06:40:45.385480: step 6360, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 56h:31m:41s remains)
INFO - root - 2017-12-06 06:40:51.889613: step 6370, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 58h:19m:38s remains)
INFO - root - 2017-12-06 06:40:58.450444: step 6380, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 59h:59m:02s remains)
INFO - root - 2017-12-06 06:41:05.144541: step 6390, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 60h:01m:33s remains)
INFO - root - 2017-12-06 06:41:11.630277: step 6400, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 59h:20m:00s remains)
2017-12-06 06:41:12.320152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.223753 -4.2278228 -4.2313037 -4.224731 -4.2205472 -4.2117333 -4.2025328 -4.1960912 -4.199636 -4.2105165 -4.2184939 -4.2170272 -4.2062736 -4.1844912 -4.1686683][-4.2289352 -4.2232909 -4.2190919 -4.2059622 -4.1970716 -4.1892204 -4.185286 -4.1801538 -4.1883907 -4.2085843 -4.2211256 -4.2212992 -4.2150722 -4.1940227 -4.1755743][-4.2223492 -4.2146087 -4.2087064 -4.1935582 -4.178431 -4.1644163 -4.1573496 -4.153749 -4.1687565 -4.1983972 -4.2152858 -4.215137 -4.207932 -4.1874413 -4.1677456][-4.2134562 -4.2088432 -4.2055225 -4.1891179 -4.1649985 -4.13742 -4.1209488 -4.1227994 -4.1502919 -4.1889229 -4.2058239 -4.199296 -4.1846042 -4.1646862 -4.14695][-4.2015824 -4.2001495 -4.1996274 -4.1832209 -4.1529689 -4.1156397 -4.0921578 -4.0984492 -4.1352334 -4.1766319 -4.1894712 -4.1744494 -4.1504636 -4.1312242 -4.1156988][-4.1819286 -4.1838059 -4.1869612 -4.1733055 -4.1412563 -4.0977693 -4.0686374 -4.079215 -4.1246395 -4.1651568 -4.1719494 -4.1524029 -4.1243172 -4.1067386 -4.1007109][-4.1546183 -4.1539645 -4.1582165 -4.1510749 -4.123302 -4.079525 -4.0463715 -4.0597939 -4.1097856 -4.1475735 -4.1566067 -4.1464357 -4.1314163 -4.1248789 -4.127655][-4.1282115 -4.120832 -4.1241097 -4.1275716 -4.1093144 -4.0698357 -4.0366 -4.0472569 -4.0920177 -4.1269317 -4.1387138 -4.146574 -4.1564331 -4.1685915 -4.1765623][-4.12004 -4.1043353 -4.1036077 -4.1150484 -4.1081729 -4.0785384 -4.0485229 -4.0521965 -4.0831642 -4.1072769 -4.1192093 -4.138823 -4.1727352 -4.2046618 -4.2181292][-4.1339197 -4.1114554 -4.1043539 -4.1195173 -4.1267147 -4.1114945 -4.0861063 -4.0798845 -4.0942235 -4.1084385 -4.1240287 -4.1527882 -4.1959352 -4.2322946 -4.24193][-4.1531529 -4.1271119 -4.1163836 -4.13078 -4.1480656 -4.1493134 -4.1338687 -4.1227293 -4.1250477 -4.1303754 -4.1452537 -4.172009 -4.2070136 -4.2337489 -4.23264][-4.176897 -4.1526704 -4.1393585 -4.1483197 -4.16568 -4.1770329 -4.1736636 -4.164567 -4.1640286 -4.1658244 -4.1730962 -4.1863322 -4.2036738 -4.2166238 -4.2062616][-4.200407 -4.1821947 -4.1699085 -4.1721478 -4.1835532 -4.1976724 -4.2014222 -4.1970696 -4.1985493 -4.2011585 -4.19948 -4.1970544 -4.1952462 -4.1943641 -4.1808186][-4.2232966 -4.2083936 -4.1974597 -4.1932049 -4.197093 -4.2083373 -4.214797 -4.2155766 -4.2198482 -4.22386 -4.2154737 -4.2026415 -4.1910329 -4.1851192 -4.1735497][-4.2492247 -4.2360744 -4.2246757 -4.2156072 -4.2133117 -4.2200232 -4.223546 -4.2245264 -4.2310224 -4.236156 -4.2263579 -4.2118654 -4.202867 -4.199646 -4.1934938]]...]
INFO - root - 2017-12-06 06:41:18.849809: step 6410, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 60h:30m:31s remains)
INFO - root - 2017-12-06 06:41:25.283672: step 6420, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 62h:15m:45s remains)
INFO - root - 2017-12-06 06:41:31.720312: step 6430, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 63h:14m:52s remains)
INFO - root - 2017-12-06 06:41:38.319352: step 6440, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.669 sec/batch; 60h:34m:41s remains)
INFO - root - 2017-12-06 06:41:44.778554: step 6450, loss = 2.04, batch loss = 1.99 (13.3 examples/sec; 0.601 sec/batch; 54h:28m:17s remains)
INFO - root - 2017-12-06 06:41:51.409607: step 6460, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 62h:16m:46s remains)
INFO - root - 2017-12-06 06:41:58.017591: step 6470, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 61h:12m:38s remains)
INFO - root - 2017-12-06 06:42:04.608117: step 6480, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.670 sec/batch; 60h:42m:30s remains)
INFO - root - 2017-12-06 06:42:11.325790: step 6490, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 60h:57m:47s remains)
INFO - root - 2017-12-06 06:42:17.846394: step 6500, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 60h:15m:02s remains)
2017-12-06 06:42:18.523914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3493872 -4.36131 -4.3627338 -4.3510962 -4.313354 -4.2652459 -4.2164888 -4.1756454 -4.1502409 -4.1382947 -4.1414351 -4.1702819 -4.2127171 -4.2600408 -4.2930255][-4.354043 -4.3642335 -4.3616567 -4.3432031 -4.2957821 -4.235918 -4.1824303 -4.1409893 -4.1204448 -4.1241693 -4.1381164 -4.1663084 -4.2037072 -4.2516923 -4.2850418][-4.3584847 -4.3653955 -4.355876 -4.3279047 -4.2686543 -4.1993456 -4.1422911 -4.1040912 -4.0860038 -4.0984211 -4.119451 -4.1473083 -4.1862803 -4.240468 -4.2775874][-4.3620949 -4.3680682 -4.3530979 -4.3141665 -4.2420878 -4.1656218 -4.1063137 -4.0700154 -4.0527387 -4.06959 -4.0963035 -4.1255069 -4.1744676 -4.2352419 -4.2759795][-4.367084 -4.3734083 -4.3536682 -4.2974963 -4.2086077 -4.1216965 -4.0563264 -4.0161958 -4.012722 -4.0425963 -4.0799832 -4.1217117 -4.18374 -4.2490573 -4.286685][-4.3724146 -4.3768306 -4.353157 -4.2806187 -4.1707091 -4.065896 -3.9851174 -3.9406152 -3.959882 -4.0093303 -4.0667014 -4.1289849 -4.2017159 -4.2692175 -4.3052535][-4.3721552 -4.3735871 -4.3447332 -4.2571063 -4.1251283 -3.9922981 -3.8962455 -3.870362 -3.920222 -3.9919138 -4.0703635 -4.149 -4.2248063 -4.2865772 -4.3196077][-4.3673329 -4.35968 -4.3220649 -4.2197714 -4.0649962 -3.9063177 -3.8096256 -3.8190303 -3.8974864 -3.9879138 -4.0827007 -4.1708078 -4.2447743 -4.2953014 -4.3180232][-4.358295 -4.3384 -4.2859035 -4.1746435 -4.0158696 -3.8505912 -3.7618909 -3.790565 -3.8830025 -3.9822671 -4.0854864 -4.18115 -4.2549453 -4.2947783 -4.3032255][-4.3472028 -4.3151073 -4.2487411 -4.1384473 -4.0024562 -3.870692 -3.8047109 -3.8291872 -3.9087367 -4.0026617 -4.1004004 -4.1931891 -4.2598295 -4.2924714 -4.2935858][-4.3369479 -4.2965579 -4.2236729 -4.1283441 -4.0313225 -3.9515262 -3.9166033 -3.93344 -3.9905686 -4.0655403 -4.1416583 -4.2161255 -4.2659793 -4.2879624 -4.283154][-4.3326163 -4.2913618 -4.2227993 -4.1484089 -4.084681 -4.0395784 -4.0248966 -4.0420604 -4.0870104 -4.140708 -4.1951752 -4.2461853 -4.2772479 -4.2881126 -4.2806907][-4.33413 -4.2995443 -4.2424574 -4.1837106 -4.1396346 -4.1095214 -4.1036005 -4.1240096 -4.1631069 -4.2032065 -4.2434878 -4.2776828 -4.295959 -4.3008409 -4.293057][-4.3429022 -4.3223348 -4.2804632 -4.2360568 -4.2030115 -4.18199 -4.1799212 -4.1959109 -4.226449 -4.2548552 -4.2835693 -4.3040357 -4.3131166 -4.315578 -4.3096991][-4.34959 -4.3410583 -4.3152785 -4.2835588 -4.259202 -4.2464242 -4.247015 -4.257328 -4.2786746 -4.2962241 -4.3115416 -4.3211312 -4.3247013 -4.3252983 -4.3217087]]...]
INFO - root - 2017-12-06 06:42:25.087108: step 6510, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 59h:15m:18s remains)
INFO - root - 2017-12-06 06:42:31.773622: step 6520, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 59h:51m:51s remains)
INFO - root - 2017-12-06 06:42:38.046147: step 6530, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 59h:49m:36s remains)
INFO - root - 2017-12-06 06:42:44.533737: step 6540, loss = 2.03, batch loss = 1.98 (12.5 examples/sec; 0.638 sec/batch; 57h:46m:38s remains)
INFO - root - 2017-12-06 06:42:51.203669: step 6550, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 60h:45m:05s remains)
INFO - root - 2017-12-06 06:42:57.760647: step 6560, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 59h:35m:19s remains)
INFO - root - 2017-12-06 06:43:04.313188: step 6570, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.690 sec/batch; 62h:28m:28s remains)
INFO - root - 2017-12-06 06:43:10.968863: step 6580, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 61h:58m:31s remains)
INFO - root - 2017-12-06 06:43:17.601155: step 6590, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 59h:30m:24s remains)
INFO - root - 2017-12-06 06:43:24.354178: step 6600, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 60h:53m:44s remains)
2017-12-06 06:43:24.995681: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3230085 -4.3031859 -4.2636862 -4.215929 -4.1828713 -4.1637344 -4.1474748 -4.1186609 -4.0921884 -4.0801468 -4.0861707 -4.0981574 -4.124743 -4.1535821 -4.1549425][-4.327673 -4.3079262 -4.2711811 -4.2267036 -4.1889734 -4.1658273 -4.1526189 -4.1290526 -4.1066737 -4.0935783 -4.0959129 -4.1131759 -4.14524 -4.1770577 -4.1793952][-4.3266172 -4.302958 -4.26698 -4.2257805 -4.1885085 -4.161068 -4.1468229 -4.1357942 -4.1270247 -4.1205678 -4.12034 -4.1335578 -4.1618028 -4.185317 -4.1858993][-4.3214073 -4.2925916 -4.251586 -4.2071595 -4.1693983 -4.1449461 -4.1319418 -4.1274271 -4.1286674 -4.1294956 -4.1328783 -4.1373081 -4.1554022 -4.1770082 -4.187633][-4.3152442 -4.2789187 -4.2310128 -4.1857471 -4.150104 -4.1245751 -4.0996981 -4.0896473 -4.0992122 -4.1127043 -4.1225724 -4.1218662 -4.1347528 -4.1596441 -4.1793833][-4.3048491 -4.259223 -4.2022362 -4.1539316 -4.1197085 -4.0824656 -4.0346479 -4.0193052 -4.0427623 -4.07487 -4.0971346 -4.1023636 -4.1145744 -4.1423664 -4.1667023][-4.29191 -4.2382107 -4.1706772 -4.1148396 -4.0716667 -4.0178509 -3.9535451 -3.9376137 -3.9737518 -4.0214143 -4.0558457 -4.0712585 -4.0891533 -4.1185336 -4.1456][-4.2824512 -4.228147 -4.160408 -4.1047959 -4.0571055 -4.0009747 -3.932816 -3.9124022 -3.944746 -3.9912488 -4.03199 -4.0542555 -4.0712428 -4.0929413 -4.1119542][-4.2795582 -4.2333069 -4.1805782 -4.1373515 -4.09702 -4.0499806 -3.985625 -3.9569883 -3.9736331 -4.0110106 -4.04828 -4.0688114 -4.0749598 -4.0737858 -4.0765419][-4.2817245 -4.24479 -4.2047625 -4.1746078 -4.1442184 -4.1104069 -4.0567541 -4.0270562 -4.0341697 -4.0647845 -4.0964532 -4.1059356 -4.0934734 -4.0710053 -4.0618591][-4.2799454 -4.247745 -4.2144456 -4.1943111 -4.1738596 -4.1528244 -4.1202936 -4.1008387 -4.10098 -4.1175985 -4.1353741 -4.1334677 -4.112237 -4.0808234 -4.0692625][-4.2799554 -4.2501688 -4.2214522 -4.2047219 -4.192771 -4.1827688 -4.1683664 -4.1593218 -4.1549783 -4.1541605 -4.1547308 -4.1418643 -4.1160574 -4.0858388 -4.0804572][-4.289783 -4.2627707 -4.2355809 -4.2187262 -4.2115984 -4.2060528 -4.197793 -4.1929979 -4.1878462 -4.1818805 -4.1717334 -4.1592264 -4.1400862 -4.1147056 -4.1113358][-4.306294 -4.2827063 -4.257174 -4.2408652 -4.2360868 -4.2352095 -4.2325296 -4.2289505 -4.2238717 -4.2175779 -4.2080088 -4.1986051 -4.1844592 -4.1634884 -4.1566296][-4.3234296 -4.3062177 -4.2879634 -4.2791319 -4.2795868 -4.2831793 -4.2822728 -4.2763338 -4.2700176 -4.2638373 -4.255054 -4.2472749 -4.2370892 -4.2210512 -4.215095]]...]
INFO - root - 2017-12-06 06:43:31.580967: step 6610, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 62h:19m:16s remains)
INFO - root - 2017-12-06 06:43:38.164709: step 6620, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 58h:04m:27s remains)
INFO - root - 2017-12-06 06:43:44.566763: step 6630, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 59h:25m:08s remains)
INFO - root - 2017-12-06 06:43:51.117663: step 6640, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 61h:34m:27s remains)
INFO - root - 2017-12-06 06:43:57.816562: step 6650, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 61h:29m:08s remains)
INFO - root - 2017-12-06 06:44:04.382500: step 6660, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 57h:55m:30s remains)
INFO - root - 2017-12-06 06:44:11.003784: step 6670, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 61h:04m:11s remains)
INFO - root - 2017-12-06 06:44:17.554461: step 6680, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 58h:03m:06s remains)
INFO - root - 2017-12-06 06:44:24.111905: step 6690, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 60h:29m:53s remains)
INFO - root - 2017-12-06 06:44:30.834695: step 6700, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 61h:10m:13s remains)
2017-12-06 06:44:31.549406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3153605 -4.3071675 -4.2793021 -4.2472796 -4.222991 -4.2174716 -4.2266331 -4.2358942 -4.2368555 -4.2233977 -4.203692 -4.1970258 -4.1987143 -4.2096834 -4.2232027][-4.3184428 -4.3131285 -4.2907643 -4.2642069 -4.2430525 -4.2374763 -4.2432804 -4.2411456 -4.22928 -4.204402 -4.1773787 -4.170218 -4.1756754 -4.19358 -4.2127247][-4.323801 -4.3200607 -4.2992449 -4.2732358 -4.2499332 -4.2422209 -4.2403951 -4.2259293 -4.2058239 -4.1790395 -4.157496 -4.1591678 -4.169332 -4.18801 -4.2121172][-4.3289542 -4.3258548 -4.30474 -4.274262 -4.2430534 -4.2253022 -4.2081175 -4.1779323 -4.1525483 -4.1376076 -4.1370921 -4.1516085 -4.163053 -4.1810184 -4.2092667][-4.3316154 -4.3283696 -4.3030543 -4.2623253 -4.2173128 -4.1845155 -4.143219 -4.0914464 -4.0662613 -4.0706997 -4.0995522 -4.131588 -4.148633 -4.1709642 -4.2070951][-4.3308821 -4.3233385 -4.2884469 -4.2339115 -4.17376 -4.1231117 -4.0492873 -3.971864 -3.9565 -3.9935656 -4.0556359 -4.1067948 -4.1350775 -4.1661582 -4.2059007][-4.3248644 -4.3099494 -4.2656388 -4.198318 -4.1230392 -4.0479 -3.9399571 -3.847214 -3.8600564 -3.9417517 -4.0332975 -4.09608 -4.1329446 -4.1655736 -4.2012715][-4.3162379 -4.2952757 -4.2405496 -4.1612015 -4.0682592 -3.9638512 -3.8269711 -3.74059 -3.7954926 -3.9157653 -4.0260081 -4.0967007 -4.1341572 -4.1634812 -4.1972375][-4.3081989 -4.2829833 -4.2173252 -4.1279874 -4.025001 -3.9076028 -3.7748404 -3.7197387 -3.8046558 -3.9384229 -4.0475636 -4.1122575 -4.1437635 -4.16663 -4.1977539][-4.3012762 -4.274579 -4.20621 -4.1208925 -4.0314665 -3.9351451 -3.8441195 -3.8213968 -3.9009171 -4.0110731 -4.0950456 -4.1446657 -4.1695995 -4.1848674 -4.2095037][-4.2990184 -4.2766948 -4.2174153 -4.1490269 -4.0869012 -4.0286636 -3.9835854 -3.9814918 -4.0368013 -4.1059422 -4.1577253 -4.1897893 -4.2086463 -4.2169776 -4.2320356][-4.3003197 -4.2827864 -4.2373786 -4.1859236 -4.1477232 -4.1182694 -4.1022778 -4.1110067 -4.1484876 -4.1887636 -4.2227273 -4.2445016 -4.2560716 -4.2552605 -4.2624044][-4.3040204 -4.2893162 -4.2533317 -4.2141371 -4.1918612 -4.1829257 -4.1859355 -4.2011752 -4.2302403 -4.2561393 -4.2783928 -4.290946 -4.2961988 -4.2919827 -4.2926264][-4.3098321 -4.2985706 -4.26929 -4.2375784 -4.2223325 -4.225637 -4.2400188 -4.2600069 -4.2851353 -4.3051562 -4.3182473 -4.3233523 -4.3234415 -4.3165421 -4.3124161][-4.3153286 -4.3059587 -4.2780137 -4.2486091 -4.2351847 -4.2423754 -4.262702 -4.2866521 -4.312541 -4.3320332 -4.3409896 -4.3424449 -4.3395162 -4.3303838 -4.3234782]]...]
INFO - root - 2017-12-06 06:44:38.128720: step 6710, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 59h:13m:48s remains)
INFO - root - 2017-12-06 06:44:44.512978: step 6720, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 59h:14m:33s remains)
INFO - root - 2017-12-06 06:44:50.938791: step 6730, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 62h:14m:43s remains)
INFO - root - 2017-12-06 06:44:57.630372: step 6740, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 60h:50m:26s remains)
INFO - root - 2017-12-06 06:45:04.241623: step 6750, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 58h:25m:36s remains)
INFO - root - 2017-12-06 06:45:10.856965: step 6760, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.681 sec/batch; 61h:37m:31s remains)
INFO - root - 2017-12-06 06:45:17.421435: step 6770, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:23m:39s remains)
INFO - root - 2017-12-06 06:45:24.088834: step 6780, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 62h:09m:36s remains)
INFO - root - 2017-12-06 06:45:30.660100: step 6790, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 59h:49m:48s remains)
INFO - root - 2017-12-06 06:45:37.224552: step 6800, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 59h:23m:53s remains)
2017-12-06 06:45:37.441075: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 06:45:37.441107: E tensorflow/core/util/events_writer.cc:131] Failed to flush 2 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 06:45:37.864839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2513971 -4.2222133 -4.2025976 -4.205584 -4.2336397 -4.2571683 -4.2653427 -4.2658005 -4.2661791 -4.2628026 -4.258213 -4.2433457 -4.241251 -4.2290635 -4.2066808][-4.2432027 -4.2051249 -4.1784267 -4.1741238 -4.196722 -4.2195005 -4.2252283 -4.22767 -4.2334013 -4.241756 -4.2417259 -4.2264628 -4.2230587 -4.2115312 -4.1772308][-4.2432504 -4.2022438 -4.1700463 -4.1542721 -4.1664805 -4.1851077 -4.1850567 -4.1818433 -4.1902356 -4.2118454 -4.218421 -4.2032671 -4.1962733 -4.1792479 -4.1342387][-4.2494993 -4.208209 -4.1723881 -4.1433225 -4.1403952 -4.1492438 -4.1451988 -4.138432 -4.1512542 -4.1821942 -4.1915154 -4.1754041 -4.1677718 -4.1451206 -4.0993352][-4.2575407 -4.2172818 -4.1829538 -4.1499 -4.1374893 -4.1392307 -4.1350112 -4.1237035 -4.1337171 -4.1627007 -4.1728468 -4.1617374 -4.1569896 -4.1364651 -4.0981417][-4.2621155 -4.2224922 -4.1914363 -4.1649051 -4.1525869 -4.1483793 -4.1427794 -4.1327658 -4.1379943 -4.1589184 -4.1701841 -4.1711454 -4.1675696 -4.1484213 -4.1207509][-4.2528791 -4.20924 -4.1769171 -4.15716 -4.1472464 -4.1374884 -4.13044 -4.1258407 -4.1335821 -4.1502657 -4.1675286 -4.1770425 -4.1761384 -4.162158 -4.1451983][-4.233717 -4.183156 -4.1479282 -4.1322041 -4.1235104 -4.1041036 -4.09108 -4.0855112 -4.0973892 -4.1175041 -4.1408424 -4.1561823 -4.1602869 -4.1544528 -4.1459312][-4.2086577 -4.1508703 -4.1101069 -4.094089 -4.0866661 -4.062222 -4.0413175 -4.0279088 -4.0400052 -4.0672922 -4.0982766 -4.1236057 -4.1367164 -4.1400919 -4.1393361][-4.1859884 -4.1236911 -4.0765376 -4.0587907 -4.0581241 -4.0426669 -4.0238533 -4.0055337 -4.0174532 -4.0498943 -4.0824103 -4.1109624 -4.1302996 -4.1426277 -4.1498656][-4.1863708 -4.1258426 -4.0774922 -4.0584297 -4.0603848 -4.0567374 -4.0485034 -4.0382209 -4.0481052 -4.0763373 -4.1047583 -4.1309614 -4.1526771 -4.1716714 -4.1834769][-4.2087178 -4.1584535 -4.1205554 -4.1044059 -4.1083 -4.1142316 -4.1155825 -4.1112309 -4.1156421 -4.1359596 -4.1588745 -4.1790171 -4.1980405 -4.2152042 -4.2251015][-4.2484317 -4.21195 -4.189126 -4.1805058 -4.1867051 -4.1951723 -4.199923 -4.1972146 -4.1923275 -4.2021346 -4.2168913 -4.2276921 -4.2390614 -4.2512593 -4.2584038][-4.2881112 -4.2621188 -4.2473917 -4.2438593 -4.2494454 -4.2567978 -4.2611141 -4.25748 -4.2484479 -4.2511978 -4.2584109 -4.2636232 -4.2706938 -4.2795711 -4.2854385][-4.3116384 -4.2929993 -4.2825828 -4.282238 -4.2880435 -4.2952757 -4.2989707 -4.2958035 -4.2874632 -4.2854815 -4.2877965 -4.2906857 -4.2955627 -4.3027625 -4.3090148]]...]
INFO - root - 2017-12-06 06:45:44.564596: step 6810, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 60h:20m:48s remains)
INFO - root - 2017-12-06 06:45:50.994333: step 6820, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 58h:55m:27s remains)
INFO - root - 2017-12-06 06:45:57.739544: step 6830, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 60h:21m:17s remains)
INFO - root - 2017-12-06 06:46:04.169569: step 6840, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 59h:09m:01s remains)
INFO - root - 2017-12-06 06:46:10.638378: step 6850, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 57h:02m:58s remains)
INFO - root - 2017-12-06 06:46:17.207914: step 6860, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 59h:37m:47s remains)
INFO - root - 2017-12-06 06:46:23.810969: step 6870, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 59h:43m:09s remains)
INFO - root - 2017-12-06 06:46:30.345185: step 6880, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 60h:17m:47s remains)
INFO - root - 2017-12-06 06:46:37.039434: step 6890, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 59h:26m:09s remains)
INFO - root - 2017-12-06 06:46:43.673566: step 6900, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 60h:55m:12s remains)
2017-12-06 06:46:44.330482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2608285 -4.2368479 -4.2135625 -4.1965275 -4.1801982 -4.1598511 -4.1442227 -4.1347518 -4.1441412 -4.176918 -4.2079053 -4.2284627 -4.2325168 -4.2298 -4.2063646][-4.2583456 -4.2320914 -4.2062674 -4.1902022 -4.1814818 -4.1583753 -4.1305275 -4.1088266 -4.1239715 -4.1690969 -4.2063365 -4.2268305 -4.2293019 -4.228673 -4.20178][-4.264389 -4.2314067 -4.2018867 -4.1878123 -4.1878896 -4.169632 -4.1355562 -4.1032848 -4.1189365 -4.1756129 -4.2144465 -4.2323084 -4.2311935 -4.2320848 -4.2029228][-4.2663717 -4.2283378 -4.1977577 -4.1872864 -4.1921782 -4.1808167 -4.1471162 -4.1082921 -4.1173444 -4.1734753 -4.2076726 -4.2228847 -4.2244596 -4.2313542 -4.2080421][-4.2687092 -4.2336822 -4.2071071 -4.1968331 -4.1988897 -4.1912618 -4.155756 -4.1030369 -4.097363 -4.1518378 -4.1896572 -4.2061734 -4.219779 -4.2337432 -4.2200952][-4.2723041 -4.2451243 -4.2229052 -4.2050371 -4.1905437 -4.1754379 -4.1294184 -4.0493226 -4.0240746 -4.0887294 -4.1507006 -4.1770058 -4.2057619 -4.23345 -4.2320671][-4.2709017 -4.2490821 -4.2283812 -4.1984835 -4.1616755 -4.1213055 -4.0444188 -3.9156988 -3.874239 -3.9830706 -4.0950618 -4.1512828 -4.2018929 -4.244895 -4.2577953][-4.2689991 -4.2504215 -4.2262878 -4.1888885 -4.1385512 -4.0683732 -3.9410739 -3.7536371 -3.7174511 -3.8941629 -4.0519414 -4.1352043 -4.2046781 -4.2595487 -4.2777567][-4.2654042 -4.2487903 -4.2221336 -4.1904621 -4.149683 -4.073843 -3.9304066 -3.7432654 -3.7336836 -3.9112098 -4.0565467 -4.1335058 -4.1977663 -4.2549534 -4.2715316][-4.2628908 -4.2419291 -4.2115741 -4.1862936 -4.1581664 -4.0973725 -3.9957163 -3.8906927 -3.8991694 -4.0098877 -4.1018786 -4.1488047 -4.190661 -4.2321754 -4.2429838][-4.2690163 -4.2456288 -4.2145696 -4.1934257 -4.1673088 -4.1121941 -4.0447497 -3.9981756 -4.0133457 -4.0817618 -4.139173 -4.1673937 -4.1924911 -4.2134256 -4.2137494][-4.279314 -4.2593179 -4.233901 -4.2189469 -4.1946898 -4.1425877 -4.0937662 -4.0694504 -4.0819278 -4.1253595 -4.1624246 -4.1829786 -4.2017684 -4.2123547 -4.2057405][-4.2877417 -4.2734303 -4.2559214 -4.24771 -4.2329574 -4.1935182 -4.1594682 -4.1437325 -4.1550245 -4.1825576 -4.2035041 -4.2160568 -4.2264447 -4.2294869 -4.2211266][-4.2930932 -4.2867975 -4.2793474 -4.274415 -4.2643671 -4.2383785 -4.2203813 -4.2113709 -4.225667 -4.2423291 -4.2465296 -4.2504692 -4.25353 -4.2540178 -4.2466474][-4.2944412 -4.2943654 -4.297699 -4.297359 -4.2886372 -4.270669 -4.2610064 -4.2561073 -4.2685304 -4.28033 -4.2803917 -4.2846646 -4.2869759 -4.2890935 -4.2860007]]...]
INFO - root - 2017-12-06 06:46:50.842267: step 6910, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 59h:46m:13s remains)
INFO - root - 2017-12-06 06:46:57.327798: step 6920, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 59h:47m:41s remains)
INFO - root - 2017-12-06 06:47:03.866635: step 6930, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 56h:40m:18s remains)
INFO - root - 2017-12-06 06:47:10.441962: step 6940, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 61h:57m:49s remains)
INFO - root - 2017-12-06 06:47:17.078827: step 6950, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 59h:02m:24s remains)
INFO - root - 2017-12-06 06:47:23.653818: step 6960, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 61h:08m:31s remains)
INFO - root - 2017-12-06 06:47:30.235178: step 6970, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 59h:23m:50s remains)
INFO - root - 2017-12-06 06:47:36.910416: step 6980, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 60h:49m:12s remains)
INFO - root - 2017-12-06 06:47:43.505368: step 6990, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 57h:57m:43s remains)
INFO - root - 2017-12-06 06:47:49.993122: step 7000, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 57h:20m:38s remains)
2017-12-06 06:47:50.205676: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 06:47:50.205715: E tensorflow/core/util/events_writer.cc:131] Failed to flush 4 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 06:47:50.681367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1447387 -4.1393561 -4.120934 -4.1052389 -4.0871081 -4.0919204 -4.11075 -4.1246233 -4.12694 -4.11582 -4.0976667 -4.0906811 -4.0945826 -4.0973959 -4.1030245][-4.1330771 -4.1294036 -4.1115103 -4.0947905 -4.0817432 -4.0895896 -4.1080532 -4.1247578 -4.1284204 -4.120101 -4.1017613 -4.0909424 -4.0928373 -4.09189 -4.0902128][-4.1207151 -4.1170335 -4.1008105 -4.0757437 -4.0595584 -4.0681129 -4.0912204 -4.1088996 -4.1156049 -4.115181 -4.1048951 -4.0923119 -4.08207 -4.0711274 -4.0635509][-4.1165919 -4.1231828 -4.1140337 -4.0852284 -4.064846 -4.0605621 -4.0732112 -4.0813661 -4.0883131 -4.1008277 -4.1063509 -4.0945468 -4.0651178 -4.042273 -4.0378623][-4.1131086 -4.1290483 -4.1251197 -4.09929 -4.0776429 -4.0645761 -4.0574183 -4.0477529 -4.0526114 -4.0838718 -4.1093225 -4.1001797 -4.0689244 -4.052712 -4.0566053][-4.1080475 -4.1264338 -4.12856 -4.1099477 -4.0945535 -4.0763211 -4.0412779 -4.0067458 -4.01826 -4.07585 -4.122952 -4.1220665 -4.1035085 -4.0980167 -4.1043148][-4.0915961 -4.1126127 -4.1185985 -4.109344 -4.0974131 -4.0688453 -4.0063758 -3.9523637 -3.9809051 -4.069972 -4.1333423 -4.1420884 -4.1389365 -4.1438379 -4.1479859][-4.0509171 -4.0797076 -4.0955882 -4.0929871 -4.0749269 -4.03043 -3.9468703 -3.8863151 -3.9531605 -4.06785 -4.1365128 -4.15353 -4.163784 -4.1743951 -4.1683588][-4.015728 -4.04892 -4.0739555 -4.0753803 -4.0554628 -4.0051966 -3.9242492 -3.8871784 -3.9732361 -4.0786719 -4.1292257 -4.1423635 -4.1553974 -4.1661625 -4.1612887][-4.0221152 -4.0568247 -4.0755138 -4.0737014 -4.0598598 -4.0230556 -3.9717073 -3.9670293 -4.0354571 -4.1008115 -4.125402 -4.1317244 -4.1422997 -4.1502028 -4.1461906][-4.0475512 -4.0777235 -4.0860157 -4.0791368 -4.0746403 -4.0548005 -4.0299377 -4.0412364 -4.088768 -4.1203551 -4.1285682 -4.1348319 -4.1448073 -4.1435452 -4.129437][-4.0681424 -4.1005621 -4.1054196 -4.095232 -4.0932274 -4.0802283 -4.0652227 -4.0767736 -4.1054392 -4.122231 -4.1315494 -4.14298 -4.1512589 -4.141932 -4.1166134][-4.1010346 -4.13118 -4.1324997 -4.120986 -4.1187282 -4.1018825 -4.0792832 -4.0812793 -4.1020169 -4.1196647 -4.1344118 -4.1511946 -4.1557541 -4.14144 -4.1171913][-4.1180334 -4.1408067 -4.134943 -4.12688 -4.1300516 -4.1156216 -4.0928121 -4.0962591 -4.1203051 -4.1406789 -4.150847 -4.160049 -4.1572366 -4.1397333 -4.1235003][-4.1177783 -4.134922 -4.1311908 -4.1342316 -4.142776 -4.1312227 -4.1132436 -4.1185322 -4.1392055 -4.1510897 -4.1526566 -4.148139 -4.1352115 -4.1189876 -4.1123271]]...]
INFO - root - 2017-12-06 06:47:57.356566: step 7010, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 60h:13m:35s remains)
INFO - root - 2017-12-06 06:48:03.747976: step 7020, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 61h:10m:58s remains)
INFO - root - 2017-12-06 06:48:10.365688: step 7030, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.616 sec/batch; 55h:40m:56s remains)
INFO - root - 2017-12-06 06:48:16.694133: step 7040, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 59h:11m:50s remains)
INFO - root - 2017-12-06 06:48:23.330766: step 7050, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 62h:35m:33s remains)
INFO - root - 2017-12-06 06:48:30.008295: step 7060, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 61h:26m:13s remains)
INFO - root - 2017-12-06 06:48:36.583753: step 7070, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 58h:38m:23s remains)
INFO - root - 2017-12-06 06:48:43.155103: step 7080, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.681 sec/batch; 61h:32m:20s remains)
INFO - root - 2017-12-06 06:48:49.817921: step 7090, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 60h:20m:24s remains)
INFO - root - 2017-12-06 06:48:56.402322: step 7100, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.657 sec/batch; 59h:22m:33s remains)
2017-12-06 06:48:57.000714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3084455 -4.3037515 -4.2940097 -4.2785754 -4.2636003 -4.2605691 -4.2714949 -4.2860394 -4.295383 -4.2942104 -4.2880912 -4.2872396 -4.2918191 -4.2944713 -4.294899][-4.3081493 -4.2936239 -4.2729053 -4.2493353 -4.2281437 -4.2252831 -4.2469449 -4.274065 -4.29213 -4.2956753 -4.2879939 -4.2785449 -4.2715468 -4.2644944 -4.2609353][-4.3029346 -4.2802973 -4.2487917 -4.2157292 -4.184021 -4.1754785 -4.2042904 -4.2454844 -4.2740936 -4.2822208 -4.2766185 -4.26492 -4.25164 -4.2362814 -4.2282906][-4.3000021 -4.2751369 -4.23433 -4.18825 -4.1413412 -4.121789 -4.1533732 -4.210907 -4.2541618 -4.2710352 -4.275012 -4.2707105 -4.25998 -4.2419581 -4.2294865][-4.3019886 -4.2765989 -4.2260642 -4.1630383 -4.0943279 -4.0593128 -4.0921559 -4.1697555 -4.2310338 -4.2602425 -4.2742825 -4.2825131 -4.2794437 -4.2667255 -4.2547193][-4.2953777 -4.2742596 -4.2213039 -4.1452794 -4.0579023 -4.0068974 -4.0364218 -4.1258874 -4.2011261 -4.2388511 -4.2572327 -4.2733 -4.2819362 -4.2826977 -4.2784142][-4.27773 -4.2632456 -4.2172313 -4.143321 -4.0529203 -3.9929934 -4.0134854 -4.1004624 -4.177319 -4.2145166 -4.2311769 -4.2493134 -4.266439 -4.2793336 -4.2823744][-4.2621555 -4.253458 -4.2222967 -4.1650743 -4.0875177 -4.0280485 -4.0366073 -4.1085868 -4.1729193 -4.2048192 -4.2162185 -4.2309351 -4.253654 -4.2730432 -4.2774963][-4.2649794 -4.2577329 -4.2384567 -4.1973004 -4.1368952 -4.08602 -4.0845585 -4.1425428 -4.1942205 -4.2179751 -4.222898 -4.2321835 -4.2533889 -4.2723904 -4.2745309][-4.2814817 -4.2748137 -4.2641659 -4.2359986 -4.1920424 -4.1536479 -4.1470356 -4.1940484 -4.23403 -4.2484217 -4.2464261 -4.2493029 -4.2623525 -4.276268 -4.2766161][-4.2883224 -4.2848926 -4.2815051 -4.2656088 -4.2371154 -4.2122321 -4.2047114 -4.2370605 -4.26489 -4.2745252 -4.273088 -4.2734342 -4.2803917 -4.2891421 -4.2877641][-4.2809644 -4.28385 -4.2886434 -4.2831984 -4.2655196 -4.246758 -4.2358074 -4.2502155 -4.2665663 -4.2763076 -4.2826796 -4.2870827 -4.2924047 -4.297339 -4.2935][-4.2648468 -4.2724481 -4.2836552 -4.2877383 -4.2791934 -4.259841 -4.2413096 -4.2359414 -4.2396121 -4.2520223 -4.2699323 -4.2823772 -4.2892127 -4.2913251 -4.2870083][-4.2517018 -4.2613797 -4.2743149 -4.2841682 -4.2812705 -4.2611547 -4.2333236 -4.2098622 -4.1997828 -4.2132106 -4.2395926 -4.2593942 -4.2722521 -4.2784619 -4.2777085][-4.2483182 -4.2587066 -4.2722182 -4.2845135 -4.2865553 -4.2694287 -4.2364149 -4.1983128 -4.1754909 -4.1844492 -4.2118969 -4.2377281 -4.2583089 -4.2726884 -4.2787571]]...]
INFO - root - 2017-12-06 06:49:03.463293: step 7110, loss = 2.11, batch loss = 2.05 (11.9 examples/sec; 0.673 sec/batch; 60h:49m:45s remains)
INFO - root - 2017-12-06 06:49:10.016256: step 7120, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 61h:15m:00s remains)
INFO - root - 2017-12-06 06:49:16.532240: step 7130, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 61h:19m:42s remains)
INFO - root - 2017-12-06 06:49:23.163391: step 7140, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.703 sec/batch; 63h:30m:15s remains)
INFO - root - 2017-12-06 06:49:29.549112: step 7150, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 58h:09m:36s remains)
INFO - root - 2017-12-06 06:49:36.005749: step 7160, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 59h:18m:47s remains)
INFO - root - 2017-12-06 06:49:42.512897: step 7170, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 57h:50m:15s remains)
INFO - root - 2017-12-06 06:49:49.109984: step 7180, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.675 sec/batch; 61h:02m:31s remains)
INFO - root - 2017-12-06 06:49:55.799166: step 7190, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 56h:52m:35s remains)
INFO - root - 2017-12-06 06:50:02.425196: step 7200, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 58h:10m:23s remains)
2017-12-06 06:50:02.620194: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 06:50:02.620231: E tensorflow/core/util/events_writer.cc:131] Failed to flush 6 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 06:50:03.080681: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2209773 -4.2261887 -4.2333055 -4.2350512 -4.2438893 -4.2602963 -4.2618504 -4.2587676 -4.2600584 -4.2582283 -4.2524118 -4.244956 -4.2426782 -4.25065 -4.2607169][-4.2532196 -4.2628341 -4.2727571 -4.2732649 -4.2801509 -4.293541 -4.2930346 -4.2863326 -4.283175 -4.2726512 -4.2640715 -4.2594461 -4.2614031 -4.2720208 -4.28086][-4.2963824 -4.30387 -4.3079391 -4.3024712 -4.3006692 -4.3045959 -4.2973161 -4.2855992 -4.2795486 -4.27214 -4.2679234 -4.2640963 -4.2631445 -4.26788 -4.2691607][-4.3228631 -4.3278937 -4.3262167 -4.3155332 -4.3039913 -4.2962561 -4.2774711 -4.255548 -4.245976 -4.248044 -4.2573957 -4.2631397 -4.2599587 -4.2576914 -4.2508144][-4.3137732 -4.3269811 -4.3263121 -4.3118892 -4.2933912 -4.2735662 -4.2407475 -4.2082562 -4.203114 -4.2263222 -4.2569227 -4.2747383 -4.2734084 -4.2651191 -4.2528987][-4.2836413 -4.3049006 -4.3084598 -4.2857609 -4.2511654 -4.2108674 -4.1565986 -4.1119251 -4.1181407 -4.1761146 -4.2399035 -4.2791753 -4.2900138 -4.287221 -4.2733588][-4.2565904 -4.2822976 -4.2857847 -4.247457 -4.1860518 -4.114728 -4.0275655 -3.9648886 -3.9875197 -4.0859032 -4.188633 -4.2588472 -4.2976017 -4.3130593 -4.3081818][-4.2559834 -4.28156 -4.2759304 -4.2155 -4.1247954 -4.0139747 -3.88163 -3.7855649 -3.8267908 -3.9766238 -4.1223936 -4.2209749 -4.2862864 -4.3198171 -4.3206954][-4.265645 -4.2922463 -4.2815123 -4.208724 -4.1002345 -3.9698672 -3.8096931 -3.68044 -3.7235894 -3.9024928 -4.0700674 -4.1821256 -4.2608581 -4.3011842 -4.2996826][-4.2601957 -4.2905283 -4.2897248 -4.2312574 -4.1371307 -4.02582 -3.8916168 -3.7766497 -3.7926912 -3.9298725 -4.0697432 -4.1662188 -4.2329149 -4.2649541 -4.258564][-4.255322 -4.2880211 -4.2970943 -4.2615862 -4.1997948 -4.1265616 -4.0413723 -3.9658728 -3.9623096 -4.0348892 -4.12042 -4.1852045 -4.2264562 -4.2391777 -4.2229276][-4.2803235 -4.3066173 -4.3135414 -4.2901955 -4.2517052 -4.2104874 -4.1671119 -4.1291189 -4.1228809 -4.1559806 -4.1987538 -4.23029 -4.2440834 -4.23508 -4.2057095][-4.3096747 -4.3266273 -4.32882 -4.31177 -4.2837696 -4.2583442 -4.2392144 -4.2248597 -4.223825 -4.2415786 -4.2654424 -4.2785797 -4.275866 -4.2547426 -4.2206316][-4.318974 -4.3315 -4.330317 -4.3198996 -4.3029141 -4.2872372 -4.2785144 -4.2753911 -4.2782359 -4.2883711 -4.3030953 -4.3101439 -4.3050075 -4.2864351 -4.2620125][-4.3136435 -4.3228645 -4.3180532 -4.3111291 -4.3031874 -4.2928252 -4.2869186 -4.2857862 -4.2879853 -4.2955127 -4.3073015 -4.3151722 -4.3116837 -4.2959924 -4.2797475]]...]
INFO - root - 2017-12-06 06:50:09.500791: step 7210, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:05m:30s remains)
INFO - root - 2017-12-06 06:50:16.070649: step 7220, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.638 sec/batch; 57h:40m:31s remains)
INFO - root - 2017-12-06 06:50:22.645807: step 7230, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 60h:17m:54s remains)
INFO - root - 2017-12-06 06:50:29.242914: step 7240, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.681 sec/batch; 61h:31m:49s remains)
INFO - root - 2017-12-06 06:50:35.768889: step 7250, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 59h:11m:52s remains)
INFO - root - 2017-12-06 06:50:42.189820: step 7260, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 56h:59m:37s remains)
INFO - root - 2017-12-06 06:50:48.728853: step 7270, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 58h:02m:24s remains)
INFO - root - 2017-12-06 06:50:55.408056: step 7280, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 61h:46m:42s remains)
INFO - root - 2017-12-06 06:51:02.077494: step 7290, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 59h:35m:24s remains)
INFO - root - 2017-12-06 06:51:08.707304: step 7300, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 60h:24m:18s remains)
2017-12-06 06:51:09.348600: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2858772 -4.2746897 -4.2824068 -4.2978759 -4.3073864 -4.3068719 -4.2935419 -4.2904048 -4.2991667 -4.3070946 -4.3054461 -4.2897711 -4.2750707 -4.2727823 -4.2793274][-4.2351 -4.2120047 -4.2180934 -4.2401505 -4.2557468 -4.2590232 -4.24549 -4.2406135 -4.2490387 -4.2563624 -4.2517805 -4.2351375 -4.2261882 -4.2325964 -4.2444239][-4.1955733 -4.1640897 -4.1635346 -4.1909175 -4.2149282 -4.2216578 -4.2115831 -4.2073736 -4.213397 -4.2171836 -4.2096019 -4.196866 -4.1986055 -4.215919 -4.233603][-4.1808281 -4.1501975 -4.14661 -4.1715708 -4.1979504 -4.2054162 -4.1992369 -4.1934938 -4.1988311 -4.2034883 -4.20075 -4.1986604 -4.2082114 -4.2247882 -4.237536][-4.1836967 -4.1585531 -4.1599255 -4.179708 -4.1944695 -4.1921997 -4.1756821 -4.1601195 -4.1608505 -4.1790423 -4.1967015 -4.2076983 -4.2207274 -4.2268724 -4.2285504][-4.1962028 -4.1785655 -4.1840434 -4.198195 -4.1993427 -4.1796904 -4.1384454 -4.0921168 -4.0829239 -4.1308551 -4.1857123 -4.2127085 -4.2248564 -4.2230911 -4.2187181][-4.213161 -4.2005982 -4.2068105 -4.2187157 -4.2115936 -4.1758804 -4.1007242 -4.0035076 -3.9694824 -4.05973 -4.1621275 -4.2067614 -4.2235212 -4.224618 -4.2219057][-4.2227306 -4.2101212 -4.2159686 -4.2268481 -4.2213717 -4.1822648 -4.0931277 -3.965368 -3.90133 -4.0130339 -4.13714 -4.1941137 -4.2169638 -4.2298822 -4.2335353][-4.21728 -4.2093453 -4.2213268 -4.2415929 -4.2475595 -4.2220111 -4.1539307 -4.0576878 -4.0108123 -4.0738821 -4.1595235 -4.2013097 -4.2180991 -4.2306132 -4.2350264][-4.2116709 -4.209568 -4.2272692 -4.2564235 -4.2734013 -4.2635884 -4.2275643 -4.1778879 -4.1545691 -4.1772661 -4.2122416 -4.2237697 -4.22609 -4.2253342 -4.2206597][-4.2116294 -4.207509 -4.2259822 -4.2595606 -4.2820559 -4.2814608 -4.2691755 -4.2511239 -4.2438951 -4.2522583 -4.2605934 -4.25733 -4.2530384 -4.2419415 -4.2289839][-4.2231236 -4.216404 -4.2320256 -4.2613778 -4.2815776 -4.2812133 -4.2789326 -4.27792 -4.2808628 -4.2883306 -4.292449 -4.2932558 -4.2955585 -4.2869081 -4.2704735][-4.248601 -4.243186 -4.2526779 -4.2716675 -4.2835956 -4.2834139 -4.2810597 -4.2833319 -4.2945023 -4.3083448 -4.3160071 -4.322588 -4.3289542 -4.3225484 -4.3063631][-4.2740784 -4.2720757 -4.2791214 -4.2885437 -4.293222 -4.2919965 -4.2888231 -4.2911415 -4.3031321 -4.3192763 -4.3287954 -4.3369184 -4.3427873 -4.3383355 -4.3243246][-4.2896252 -4.2888703 -4.2931089 -4.2980638 -4.2983031 -4.2983494 -4.2958302 -4.2959533 -4.3041277 -4.3193641 -4.330595 -4.3365469 -4.3391566 -4.3370581 -4.3284173]]...]
INFO - root - 2017-12-06 06:51:15.939436: step 7310, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 61h:03m:59s remains)
INFO - root - 2017-12-06 06:51:22.542914: step 7320, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 58h:59m:08s remains)
INFO - root - 2017-12-06 06:51:29.028418: step 7330, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 61h:24m:46s remains)
INFO - root - 2017-12-06 06:51:35.554334: step 7340, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.647 sec/batch; 58h:26m:04s remains)
INFO - root - 2017-12-06 06:51:42.016240: step 7350, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 59h:23m:38s remains)
INFO - root - 2017-12-06 06:51:48.636371: step 7360, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 60h:42m:30s remains)
INFO - root - 2017-12-06 06:51:55.135952: step 7370, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 56h:41m:25s remains)
INFO - root - 2017-12-06 06:52:01.707635: step 7380, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 60h:06m:06s remains)
INFO - root - 2017-12-06 06:52:08.444522: step 7390, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 61h:48m:06s remains)
INFO - root - 2017-12-06 06:52:15.045818: step 7400, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:26m:19s remains)
2017-12-06 06:52:15.256927: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 06:52:15.256967: E tensorflow/core/util/events_writer.cc:131] Failed to flush 8 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 06:52:15.654160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3340015 -4.3280525 -4.3240304 -4.3206515 -4.3126554 -4.3016419 -4.2915387 -4.2818656 -4.2753983 -4.2783327 -4.2862163 -4.2946234 -4.3045535 -4.318716 -4.3341084][-4.3123221 -4.3006139 -4.2955751 -4.2921262 -4.2802119 -4.2614069 -4.2403836 -4.2179356 -4.200829 -4.2043118 -4.2219534 -4.2429261 -4.2642984 -4.2903461 -4.315815][-4.2746029 -4.2553148 -4.2490053 -4.2517686 -4.2433147 -4.2238026 -4.1961613 -4.1604757 -4.13358 -4.1362696 -4.1601357 -4.1904759 -4.22436 -4.2640257 -4.3004918][-4.2116795 -4.1833158 -4.1780529 -4.193584 -4.2009974 -4.1920357 -4.1657457 -4.1200037 -4.0855055 -4.0870481 -4.1151333 -4.15105 -4.1923065 -4.2410197 -4.2856402][-4.1330123 -4.0941858 -4.0890293 -4.1250019 -4.1588912 -4.165247 -4.1403518 -4.0842242 -4.0446625 -4.0509214 -4.0860329 -4.132093 -4.1777411 -4.2267323 -4.2719574][-4.0678916 -4.0200262 -4.0152659 -4.0689025 -4.1235476 -4.1381984 -4.1045136 -4.0266075 -3.9785075 -3.9964659 -4.047142 -4.1111422 -4.1629338 -4.2102771 -4.2550769][-4.0459132 -3.9931009 -3.9834583 -4.0355372 -4.09008 -4.0957232 -4.0415354 -3.9266496 -3.8521287 -3.8863568 -3.9653814 -4.0506363 -4.118309 -4.1747508 -4.2277713][-4.0747476 -4.0189152 -3.9982007 -4.034821 -4.0742135 -4.0634174 -3.9891269 -3.8425925 -3.7426693 -3.7907434 -3.8927088 -3.9932923 -4.0723958 -4.140851 -4.2050133][-4.1424856 -4.0959382 -4.0761828 -4.0998693 -4.122664 -4.102551 -4.0313306 -3.9053853 -3.8235025 -3.8554816 -3.9375381 -4.025239 -4.0983825 -4.1625228 -4.2226753][-4.2239289 -4.1955709 -4.1848 -4.2014279 -4.2134094 -4.1912656 -4.1384082 -4.0537829 -3.9997339 -4.0094466 -4.0557346 -4.1154351 -4.1714711 -4.2209954 -4.2680674][-4.2850981 -4.27091 -4.2668533 -4.2769256 -4.2813344 -4.2632723 -4.22863 -4.1793613 -4.1468091 -4.1480746 -4.1717138 -4.2068968 -4.24283 -4.2766938 -4.3101935][-4.3177261 -4.3085465 -4.3036327 -4.3047419 -4.3036718 -4.2914252 -4.27106 -4.246 -4.2296171 -4.2322135 -4.2468824 -4.2676187 -4.2912865 -4.31454 -4.337708][-4.3331151 -4.3253512 -4.3196969 -4.3162236 -4.3119011 -4.3026853 -4.2915163 -4.282259 -4.2784076 -4.2827563 -4.2920194 -4.3047843 -4.3204222 -4.3379855 -4.3534245][-4.3479652 -4.3426809 -4.33816 -4.3349524 -4.3304248 -4.3235383 -4.3173618 -4.3130474 -4.3121133 -4.3155208 -4.32115 -4.3283672 -4.3391595 -4.3515468 -4.3614917][-4.3590212 -4.3562284 -4.3526106 -4.3497386 -4.3463321 -4.3426023 -4.3396444 -4.3375397 -4.3367953 -4.3383918 -4.3418694 -4.3462706 -4.3520103 -4.3585553 -4.3641]]...]
INFO - root - 2017-12-06 06:52:22.070057: step 7410, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 58h:06m:32s remains)
INFO - root - 2017-12-06 06:52:28.634806: step 7420, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 60h:30m:52s remains)
INFO - root - 2017-12-06 06:52:35.347454: step 7430, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 61h:09m:59s remains)
INFO - root - 2017-12-06 06:52:41.918861: step 7440, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 60h:09m:54s remains)
INFO - root - 2017-12-06 06:52:48.438041: step 7450, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 58h:19m:39s remains)
INFO - root - 2017-12-06 06:52:54.861263: step 7460, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:23m:02s remains)
INFO - root - 2017-12-06 06:53:01.399591: step 7470, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 57h:54m:41s remains)
INFO - root - 2017-12-06 06:53:07.904576: step 7480, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 58h:58m:01s remains)
INFO - root - 2017-12-06 06:53:14.561281: step 7490, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 59h:02m:06s remains)
INFO - root - 2017-12-06 06:53:21.057497: step 7500, loss = 2.06, batch loss = 2.00 (15.6 examples/sec; 0.512 sec/batch; 46h:13m:45s remains)
2017-12-06 06:53:21.745944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3417592 -4.3385525 -4.3367672 -4.3356652 -4.3293118 -4.3125639 -4.2928786 -4.2855935 -4.2908111 -4.2957263 -4.298429 -4.2973046 -4.2915187 -4.2806368 -4.2650223][-4.3400741 -4.3350325 -4.3315668 -4.3253727 -4.3067393 -4.2743669 -4.2409048 -4.2299967 -4.2444754 -4.2617087 -4.2746 -4.2779412 -4.2700586 -4.2532058 -4.2260652][-4.3326325 -4.3289771 -4.3280764 -4.3168244 -4.2835383 -4.2304115 -4.1778536 -4.160995 -4.1901231 -4.2279472 -4.256876 -4.2693748 -4.2626333 -4.2355976 -4.1919584][-4.3263311 -4.3268065 -4.3270764 -4.3097391 -4.2615285 -4.18632 -4.1092162 -4.0855746 -4.136147 -4.1996479 -4.2422619 -4.2646065 -4.2579336 -4.221096 -4.1663952][-4.3183451 -4.3248596 -4.3246179 -4.297895 -4.2357407 -4.1372609 -4.0343661 -3.9990921 -4.0690036 -4.161304 -4.2202015 -4.2497172 -4.2422333 -4.2018442 -4.1439071][-4.3130827 -4.318841 -4.3100257 -4.2732382 -4.1990261 -4.0810027 -3.9539888 -3.898015 -3.9793236 -4.1047263 -4.1882024 -4.2316961 -4.2304792 -4.1937757 -4.1413183][-4.3075018 -4.3040056 -4.284905 -4.2418547 -4.1648388 -4.0431294 -3.8947246 -3.8058326 -3.8933477 -4.0513282 -4.1557517 -4.2102485 -4.212965 -4.1795058 -4.139329][-4.296 -4.2822967 -4.2539635 -4.2106166 -4.1494813 -4.0559516 -3.9260349 -3.8296545 -3.8972101 -4.0442042 -4.139636 -4.1833277 -4.1773453 -4.1409097 -4.1156073][-4.2758794 -4.2561216 -4.2288957 -4.1958818 -4.1614456 -4.1128392 -4.0401974 -3.981523 -4.01488 -4.0998435 -4.1514311 -4.1729331 -4.1560616 -4.115849 -4.0982075][-4.2477984 -4.2304969 -4.2081819 -4.1886616 -4.1783037 -4.16129 -4.121325 -4.08451 -4.1031685 -4.1511607 -4.1753616 -4.1841297 -4.1643305 -4.1259956 -4.1082363][-4.22176 -4.2071495 -4.1943865 -4.1878657 -4.1908555 -4.1935263 -4.1762319 -4.15056 -4.1570764 -4.184648 -4.1965737 -4.198554 -4.1822534 -4.15029 -4.132895][-4.2030554 -4.1915975 -4.1870093 -4.1883655 -4.19728 -4.2133417 -4.2130971 -4.198616 -4.2003465 -4.2172303 -4.2236977 -4.2244635 -4.214139 -4.1931319 -4.1790838][-4.2020588 -4.1941843 -4.1950808 -4.2003112 -4.2126312 -4.2336779 -4.2427654 -4.2355738 -4.2365675 -4.2484717 -4.2534332 -4.2560935 -4.2525859 -4.2399435 -4.2290506][-4.225842 -4.2233062 -4.22965 -4.236186 -4.2469912 -4.2651029 -4.2751575 -4.2731643 -4.2750206 -4.28341 -4.2887797 -4.2922378 -4.2933297 -4.2910314 -4.2846022][-4.2664676 -4.2662568 -4.2728891 -4.2774377 -4.2835426 -4.2963834 -4.3067775 -4.3097677 -4.3121467 -4.3168159 -4.3203745 -4.323483 -4.3251982 -4.3261843 -4.3208022]]...]
INFO - root - 2017-12-06 06:53:28.229777: step 7510, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 59h:08m:17s remains)
INFO - root - 2017-12-06 06:53:34.830535: step 7520, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 58h:52m:01s remains)
INFO - root - 2017-12-06 06:53:41.326392: step 7530, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 58h:49m:36s remains)
INFO - root - 2017-12-06 06:53:47.951980: step 7540, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 58h:51m:24s remains)
INFO - root - 2017-12-06 06:53:54.587230: step 7550, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 60h:10m:11s remains)
INFO - root - 2017-12-06 06:54:01.019118: step 7560, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.650 sec/batch; 58h:42m:30s remains)
INFO - root - 2017-12-06 06:54:07.575032: step 7570, loss = 2.09, batch loss = 2.04 (12.0 examples/sec; 0.668 sec/batch; 60h:16m:16s remains)
INFO - root - 2017-12-06 06:54:14.215372: step 7580, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.653 sec/batch; 58h:55m:05s remains)
INFO - root - 2017-12-06 06:54:20.637147: step 7590, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 58h:26m:09s remains)
INFO - root - 2017-12-06 06:54:27.038495: step 7600, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.638 sec/batch; 57h:32m:24s remains)
2017-12-06 06:54:27.243860: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 06:54:27.243885: E tensorflow/core/util/events_writer.cc:131] Failed to flush 10 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 06:54:27.677611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3284564 -4.3214412 -4.3131895 -4.3098359 -4.3087354 -4.3081174 -4.3077307 -4.3094015 -4.3106689 -4.3102789 -4.30771 -4.3129487 -4.3237863 -4.3332272 -4.3352575][-4.3062749 -4.2938991 -4.2804732 -4.27281 -4.2660112 -4.2611256 -4.2585793 -4.2628336 -4.2692885 -4.2742682 -4.2742319 -4.2840581 -4.3012323 -4.3148408 -4.3150434][-4.2821126 -4.2655287 -4.2492151 -4.2374649 -4.2216077 -4.2055693 -4.1960778 -4.2026157 -4.2141514 -4.2269764 -4.2351604 -4.2494836 -4.2718754 -4.289165 -4.2873363][-4.2607312 -4.2421856 -4.2258439 -4.2124949 -4.1887302 -4.1576982 -4.1393075 -4.1493177 -4.1635208 -4.1773381 -4.1935792 -4.2123413 -4.2392721 -4.2593923 -4.2585588][-4.2440772 -4.2239542 -4.2066007 -4.1906309 -4.1588764 -4.1129613 -4.0887203 -4.1049147 -4.1201186 -4.1283669 -4.1432915 -4.1614 -4.1884632 -4.2137866 -4.2228246][-4.2312884 -4.208323 -4.1853762 -4.1621141 -4.118937 -4.0600858 -4.0321827 -4.0619063 -4.0828066 -4.0857286 -4.0920877 -4.1021652 -4.1255035 -4.1524992 -4.1764503][-4.2240591 -4.1971917 -4.16745 -4.1331315 -4.0708714 -3.9911129 -3.9573309 -4.0100675 -4.0477109 -4.0542879 -4.0563874 -4.0568819 -4.07386 -4.1046367 -4.1450343][-4.2229733 -4.1942029 -4.1618729 -4.1212273 -4.0387554 -3.9272702 -3.881705 -3.9555292 -4.0178947 -4.0358663 -4.0390697 -4.0334692 -4.042563 -4.0741053 -4.1259642][-4.2295327 -4.2041717 -4.1767955 -4.1406264 -4.0574255 -3.9384823 -3.881459 -3.9455447 -4.013813 -4.038322 -4.0433741 -4.0343781 -4.0358734 -4.0614252 -4.1099811][-4.2402158 -4.2232952 -4.2050281 -4.1793094 -4.1177597 -4.0280051 -3.9771414 -4.0066776 -4.0463328 -4.0630221 -4.0719767 -4.0734911 -4.077734 -4.0901494 -4.1175933][-4.2507334 -4.2407885 -4.227169 -4.2077065 -4.1662574 -4.1085596 -4.07286 -4.0804172 -4.0929 -4.0965662 -4.1087761 -4.12679 -4.1385646 -4.142241 -4.1471715][-4.2547741 -4.2474189 -4.236527 -4.2192082 -4.1896482 -4.1549854 -4.136477 -4.1407866 -4.1417794 -4.1352506 -4.1430058 -4.1652484 -4.1809492 -4.1831431 -4.1794033][-4.248467 -4.2388663 -4.2275858 -4.2115903 -4.1907897 -4.1770115 -4.1790943 -4.1915545 -4.1911206 -4.1783705 -4.1808853 -4.1966691 -4.20963 -4.2125769 -4.2102919][-4.2385697 -4.2235613 -4.2078886 -4.1902642 -4.1734548 -4.1730762 -4.1900306 -4.2144637 -4.2207222 -4.2064967 -4.2029614 -4.2113829 -4.221067 -4.2244296 -4.2223577][-4.234539 -4.213388 -4.1899447 -4.1659636 -4.1485143 -4.1520405 -4.17522 -4.2087803 -4.224195 -4.2143068 -4.2059851 -4.2093973 -4.2168646 -4.2205744 -4.2193589]]...]
INFO - root - 2017-12-06 06:54:34.294721: step 7610, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 59h:42m:45s remains)
INFO - root - 2017-12-06 06:54:41.003395: step 7620, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 57h:09m:22s remains)
INFO - root - 2017-12-06 06:54:47.475254: step 7630, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 58h:42m:27s remains)
INFO - root - 2017-12-06 06:54:54.145794: step 7640, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 60h:46m:42s remains)
INFO - root - 2017-12-06 06:55:00.852953: step 7650, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 60h:01m:56s remains)
INFO - root - 2017-12-06 06:55:07.266532: step 7660, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 62h:05m:05s remains)
INFO - root - 2017-12-06 06:55:13.794539: step 7670, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 59h:29m:26s remains)
INFO - root - 2017-12-06 06:55:20.489252: step 7680, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.681 sec/batch; 61h:25m:42s remains)
INFO - root - 2017-12-06 06:55:27.008218: step 7690, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 58h:50m:31s remains)
INFO - root - 2017-12-06 06:55:33.384335: step 7700, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 59h:38m:10s remains)
2017-12-06 06:55:34.024577: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2462406 -4.2588749 -4.2592363 -4.2472711 -4.230732 -4.223114 -4.234961 -4.2631936 -4.2926016 -4.3123717 -4.3281565 -4.332231 -4.3160648 -4.2891769 -4.2695236][-4.1973381 -4.2277923 -4.2320695 -4.2192917 -4.2032905 -4.1959095 -4.209074 -4.2412791 -4.2721434 -4.2893529 -4.2996774 -4.300374 -4.2816792 -4.2506232 -4.22764][-4.1393328 -4.185276 -4.1962295 -4.1848187 -4.1742797 -4.1705503 -4.1854887 -4.2162771 -4.2420774 -4.2537556 -4.2597404 -4.2595468 -4.2434344 -4.2169957 -4.2002592][-4.119524 -4.1668291 -4.1797318 -4.1714292 -4.1671233 -4.1668758 -4.1781354 -4.198667 -4.2144318 -4.2184806 -4.2205515 -4.2235041 -4.2168679 -4.2024236 -4.1993122][-4.1694026 -4.1976562 -4.2017193 -4.191782 -4.1845913 -4.1762285 -4.169682 -4.1754036 -4.184782 -4.1866169 -4.1866446 -4.19341 -4.1986365 -4.1979313 -4.2090936][-4.2283359 -4.228075 -4.2162647 -4.2013087 -4.1817007 -4.1543903 -4.1249576 -4.1216307 -4.1326952 -4.1353493 -4.1351147 -4.1464615 -4.1600027 -4.1664376 -4.1931152][-4.2491837 -4.22967 -4.1981177 -4.1648078 -4.12544 -4.0766215 -4.0318093 -4.0310073 -4.0505261 -4.0581112 -4.06186 -4.0827751 -4.1044078 -4.1194167 -4.1624694][-4.2520552 -4.2262235 -4.1777234 -4.1224194 -4.0586338 -3.9955895 -3.9499564 -3.9589896 -3.9922488 -4.0117307 -4.0234127 -4.0484505 -4.0729437 -4.0975013 -4.1505203][-4.2550693 -4.2319622 -4.179462 -4.1130109 -4.0365162 -3.9746866 -3.9370384 -3.9480057 -3.9867065 -4.0241461 -4.0497561 -4.0732422 -4.0984364 -4.1303158 -4.1852808][-4.2560606 -4.2399673 -4.1970515 -4.1426868 -4.0820336 -4.0349984 -4.0091047 -4.0204506 -4.0585337 -4.1091962 -4.1441264 -4.1647825 -4.1894264 -4.219615 -4.2579708][-4.2502031 -4.2416954 -4.2134962 -4.1831288 -4.1513367 -4.12924 -4.1212482 -4.1377778 -4.172998 -4.2184186 -4.2479429 -4.26199 -4.27842 -4.2948537 -4.3124704][-4.229538 -4.2299156 -4.223783 -4.2237263 -4.2239027 -4.2247081 -4.231472 -4.248282 -4.274579 -4.3020153 -4.3140759 -4.3177567 -4.3231158 -4.3317719 -4.339098][-4.1837883 -4.1966467 -4.2187586 -4.248723 -4.274363 -4.2897668 -4.2987309 -4.3074265 -4.3213558 -4.3338709 -4.3361177 -4.3363624 -4.3390236 -4.3451052 -4.3501415][-4.1237946 -4.1464853 -4.1911879 -4.24427 -4.2866168 -4.3085461 -4.3152409 -4.3171558 -4.3239794 -4.3309178 -4.3321481 -4.3342452 -4.3379297 -4.3430657 -4.34666][-4.0768261 -4.1020775 -4.1527028 -4.2156224 -4.2669091 -4.2933025 -4.3031983 -4.3074837 -4.3129377 -4.3191109 -4.3222241 -4.3256164 -4.3292089 -4.3323197 -4.3348131]]...]
INFO - root - 2017-12-06 06:55:40.676835: step 7710, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 59h:26m:29s remains)
INFO - root - 2017-12-06 06:55:47.248418: step 7720, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 59h:19m:01s remains)
INFO - root - 2017-12-06 06:55:53.776169: step 7730, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 57h:42m:24s remains)
INFO - root - 2017-12-06 06:56:00.401853: step 7740, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 61h:07m:35s remains)
INFO - root - 2017-12-06 06:56:06.869796: step 7750, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 56h:45m:11s remains)
INFO - root - 2017-12-06 06:56:13.498944: step 7760, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 60h:35m:32s remains)
INFO - root - 2017-12-06 06:56:19.943406: step 7770, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 59h:09m:06s remains)
INFO - root - 2017-12-06 06:56:26.451802: step 7780, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 57h:09m:40s remains)
INFO - root - 2017-12-06 06:56:32.965982: step 7790, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 59h:24m:42s remains)
INFO - root - 2017-12-06 06:56:39.445355: step 7800, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.617 sec/batch; 55h:39m:03s remains)
2017-12-06 06:56:39.658613: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 06:56:39.658645: E tensorflow/core/util/events_writer.cc:131] Failed to flush 12 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 06:56:40.114587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2145662 -4.2236214 -4.2421184 -4.2631121 -4.2835178 -4.3016405 -4.3049488 -4.3031325 -4.2915444 -4.2688947 -4.2556782 -4.24836 -4.2378964 -4.2309713 -4.2394152][-4.1923795 -4.2016635 -4.2212858 -4.2452068 -4.2681932 -4.2850618 -4.285171 -4.2825446 -4.2711396 -4.2516079 -4.2417631 -4.23633 -4.2279792 -4.2211466 -4.2291093][-4.1685157 -4.1711273 -4.18455 -4.2082839 -4.2366824 -4.25669 -4.2589684 -4.2569952 -4.2466688 -4.2345672 -4.228332 -4.2255845 -4.2221055 -4.2174726 -4.2248983][-4.1470342 -4.1434045 -4.1491714 -4.1684914 -4.2009749 -4.2257667 -4.2270622 -4.2220907 -4.2119212 -4.2073116 -4.2082086 -4.2118645 -4.2162547 -4.2168031 -4.2247109][-4.1273313 -4.1130271 -4.1067033 -4.1209126 -4.1538162 -4.1773481 -4.1734238 -4.1608753 -4.14176 -4.1448 -4.1619563 -4.1802397 -4.1999106 -4.2124553 -4.2254462][-4.0896406 -4.0622673 -4.0446854 -4.0486741 -4.0704827 -4.0806708 -4.0667768 -4.0445447 -4.0115762 -4.019033 -4.0603294 -4.104548 -4.146039 -4.1783895 -4.2107487][-4.0916882 -4.0585728 -4.0308185 -4.0174327 -4.0183673 -4.0050316 -3.972326 -3.9339609 -3.8795655 -3.8828878 -3.9459028 -4.0192308 -4.083746 -4.1394463 -4.1889009][-4.1264639 -4.0978112 -4.0743017 -4.05408 -4.0450845 -4.0242133 -3.9894049 -3.9551811 -3.9019079 -3.8954968 -3.9488313 -4.0208869 -4.0895119 -4.1487241 -4.1974912][-4.1960306 -4.1795473 -4.1658697 -4.1471152 -4.1374483 -4.1208882 -4.0946217 -4.0713625 -4.0327382 -4.0200791 -4.0501652 -4.0991268 -4.1506948 -4.193696 -4.2273278][-4.2728696 -4.262198 -4.2544227 -4.2416644 -4.2330904 -4.2207084 -4.2011113 -4.183157 -4.1540813 -4.1408 -4.1570888 -4.1842351 -4.2143054 -4.2348375 -4.2531042][-4.3220186 -4.3156338 -4.3115044 -4.3040175 -4.2984743 -4.2910728 -4.2754946 -4.2616587 -4.2404895 -4.2313185 -4.2412553 -4.2548537 -4.2690611 -4.2728372 -4.2776055][-4.3403964 -4.3376617 -4.33699 -4.3333583 -4.33159 -4.3320613 -4.3253407 -4.31619 -4.2979627 -4.2898936 -4.2958341 -4.3019271 -4.3077736 -4.3020368 -4.2974105][-4.32389 -4.3228788 -4.3253732 -4.326499 -4.3290071 -4.3336277 -4.3324904 -4.3256874 -4.3104973 -4.30325 -4.3095655 -4.317246 -4.3224397 -4.3173065 -4.3101988][-4.3092036 -4.3068833 -4.3087454 -4.3110337 -4.3132591 -4.3145018 -4.3102956 -4.3012843 -4.2916989 -4.2921805 -4.3014221 -4.3129048 -4.3226528 -4.3234658 -4.3177738][-4.2954221 -4.2933893 -4.2931528 -4.2941689 -4.295867 -4.2951069 -4.2877903 -4.278707 -4.2735486 -4.2775755 -4.28793 -4.3018641 -4.3156486 -4.3211875 -4.3195567]]...]
INFO - root - 2017-12-06 06:56:46.783722: step 7810, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 59h:33m:05s remains)
INFO - root - 2017-12-06 06:56:53.206003: step 7820, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.622 sec/batch; 56h:07m:41s remains)
INFO - root - 2017-12-06 06:56:59.830611: step 7830, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 59h:54m:00s remains)
INFO - root - 2017-12-06 06:57:06.522363: step 7840, loss = 2.03, batch loss = 1.97 (11.6 examples/sec; 0.692 sec/batch; 62h:23m:32s remains)
INFO - root - 2017-12-06 06:57:13.121883: step 7850, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 58h:07m:50s remains)
INFO - root - 2017-12-06 06:57:19.674433: step 7860, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 58h:28m:23s remains)
INFO - root - 2017-12-06 06:57:26.178778: step 7870, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 61h:19m:38s remains)
INFO - root - 2017-12-06 06:57:32.727272: step 7880, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 59h:20m:01s remains)
INFO - root - 2017-12-06 06:57:39.150538: step 7890, loss = 2.09, batch loss = 2.03 (14.0 examples/sec; 0.571 sec/batch; 51h:28m:06s remains)
INFO - root - 2017-12-06 06:57:45.723802: step 7900, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 58h:49m:26s remains)
2017-12-06 06:57:46.366557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.27829 -4.2661562 -4.2566352 -4.2535205 -4.2564254 -4.2636013 -4.2704873 -4.274621 -4.2758694 -4.2788687 -4.2839222 -4.287426 -4.2875843 -4.2810693 -4.2736115][-4.2473316 -4.2261343 -4.2145171 -4.2153158 -4.2211185 -4.2275577 -4.2345357 -4.2418423 -4.2468204 -4.2537909 -4.264122 -4.2716713 -4.2724223 -4.2618842 -4.2505274][-4.2198567 -4.1909828 -4.1808262 -4.1884136 -4.196445 -4.2005448 -4.2058058 -4.2147613 -4.2227149 -4.2324991 -4.2471523 -4.2578669 -4.2589025 -4.2449794 -4.2292366][-4.2052732 -4.1725507 -4.1692762 -4.1852837 -4.1949129 -4.1937485 -4.1929951 -4.197607 -4.2055564 -4.2177854 -4.2374125 -4.2523952 -4.25381 -4.238821 -4.2201905][-4.2036452 -4.1747637 -4.1774979 -4.19476 -4.2034764 -4.1982155 -4.1904869 -4.1879444 -4.1915607 -4.2017322 -4.2238803 -4.2432146 -4.2468829 -4.2343578 -4.2174969][-4.2037535 -4.1825576 -4.1863494 -4.1989484 -4.203722 -4.1948161 -4.1852055 -4.1791062 -4.1766191 -4.1802564 -4.1991735 -4.2193937 -4.225575 -4.2179193 -4.2091775][-4.2080812 -4.1949821 -4.1972394 -4.2030797 -4.2006245 -4.18739 -4.1791272 -4.1727672 -4.1659336 -4.1635332 -4.1755128 -4.192852 -4.1989064 -4.196888 -4.2008367][-4.2095776 -4.1980457 -4.1978989 -4.1982551 -4.1901937 -4.1779113 -4.1732049 -4.1691055 -4.1616278 -4.1560678 -4.1620812 -4.1753507 -4.1802783 -4.182056 -4.1938324][-4.203352 -4.1869483 -4.1850715 -4.18526 -4.1788168 -4.1731315 -4.1746984 -4.1748986 -4.1679506 -4.15805 -4.1576915 -4.1666431 -4.1702261 -4.1706591 -4.1820602][-4.1874409 -4.1663685 -4.1668973 -4.1731124 -4.1765041 -4.178236 -4.1847644 -4.1873527 -4.1743484 -4.158113 -4.15439 -4.1604023 -4.161345 -4.1565719 -4.1637459][-4.1680975 -4.1475582 -4.1553535 -4.1717668 -4.1857262 -4.1919026 -4.1991563 -4.1989608 -4.1777482 -4.1559472 -4.1467404 -4.1474762 -4.1424336 -4.1306996 -4.1314392][-4.1561975 -4.1373124 -4.150835 -4.1752172 -4.1951852 -4.20054 -4.2034044 -4.1987562 -4.1730747 -4.1509762 -4.1400414 -4.13885 -4.1296635 -4.1115632 -4.1070328][-4.1505175 -4.133141 -4.1489077 -4.1759439 -4.197196 -4.200841 -4.1972485 -4.1880331 -4.16135 -4.1419978 -4.13166 -4.1309271 -4.1236005 -4.1088524 -4.1028662][-4.1497345 -4.135221 -4.1502905 -4.1764021 -4.1982021 -4.2053485 -4.2005234 -4.1895909 -4.1657529 -4.1488409 -4.1401229 -4.1418695 -4.139636 -4.131218 -4.1267228][-4.1662474 -4.1555305 -4.1650162 -4.1840224 -4.2022004 -4.211628 -4.2082143 -4.1964483 -4.176302 -4.1631479 -4.1582155 -4.1623755 -4.1653914 -4.1625466 -4.1610117]]...]
INFO - root - 2017-12-06 06:57:53.023689: step 7910, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 58h:26m:46s remains)
INFO - root - 2017-12-06 06:57:59.585796: step 7920, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 59h:34m:27s remains)
INFO - root - 2017-12-06 06:58:06.151585: step 7930, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 57h:48m:08s remains)
INFO - root - 2017-12-06 06:58:12.626228: step 7940, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 58h:11m:15s remains)
INFO - root - 2017-12-06 06:58:19.305036: step 7950, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 63h:43m:40s remains)
INFO - root - 2017-12-06 06:58:26.002238: step 7960, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 60h:31m:31s remains)
INFO - root - 2017-12-06 06:58:32.309981: step 7970, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:10m:37s remains)
INFO - root - 2017-12-06 06:58:38.822224: step 7980, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 62h:02m:53s remains)
INFO - root - 2017-12-06 06:58:45.320985: step 7990, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:17m:52s remains)
INFO - root - 2017-12-06 06:58:51.973869: step 8000, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 60h:32m:33s remains)
2017-12-06 06:58:52.176583: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 06:58:52.176629: E tensorflow/core/util/events_writer.cc:131] Failed to flush 14 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 06:58:52.624572: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2206793 -4.2188549 -4.2248578 -4.2330127 -4.2309608 -4.2247577 -4.2240667 -4.2311478 -4.2363744 -4.2449079 -4.2638183 -4.2764854 -4.2792492 -4.2752676 -4.26486][-4.2137871 -4.2059155 -4.2105756 -4.2147093 -4.2071328 -4.1992764 -4.2002029 -4.2080474 -4.216022 -4.2278986 -4.2520328 -4.2730231 -4.2805076 -4.2750864 -4.2604547][-4.2026114 -4.191494 -4.1926618 -4.1896152 -4.1730781 -4.1578145 -4.1540022 -4.1605124 -4.17663 -4.2024326 -4.2370872 -4.2653718 -4.2780905 -4.275507 -4.2612967][-4.1924586 -4.178422 -4.1724782 -4.1597171 -4.1333981 -4.1068087 -4.0933046 -4.0967808 -4.12321 -4.1673918 -4.2144079 -4.2487216 -4.2669754 -4.2711897 -4.263298][-4.1870441 -4.1736007 -4.1617203 -4.141408 -4.1074848 -4.0689034 -4.0391088 -4.0360365 -4.0720091 -4.1331024 -4.188478 -4.2260547 -4.2497005 -4.26301 -4.2638273][-4.1864619 -4.1732054 -4.1555429 -4.1299148 -4.094111 -4.0453262 -3.9954436 -3.9822788 -4.0276518 -4.1022859 -4.1631513 -4.2019048 -4.2287207 -4.2485662 -4.2545128][-4.18457 -4.1694417 -4.1433411 -4.1089725 -4.0654259 -4.0021019 -3.9287713 -3.9002414 -3.9593265 -4.050055 -4.1209922 -4.165688 -4.20241 -4.232954 -4.2448268][-4.1813993 -4.1632509 -4.1293645 -4.085526 -4.0330639 -3.9608214 -3.8734493 -3.8330622 -3.9043117 -4.0031338 -4.0779829 -4.1285496 -4.1767955 -4.219799 -4.2429218][-4.1773334 -4.15831 -4.1245441 -4.0808916 -4.0319958 -3.9759939 -3.9125152 -3.8821187 -3.93279 -4.0045266 -4.0598388 -4.1030474 -4.1513228 -4.1977396 -4.2294135][-4.1718788 -4.1528931 -4.1224527 -4.0844707 -4.0471854 -4.0187178 -3.9917846 -3.9787126 -4.0040307 -4.0410595 -4.0715022 -4.0953283 -4.1286368 -4.1678305 -4.2021537][-4.164279 -4.1415606 -4.1107426 -4.0769005 -4.0508347 -4.0453653 -4.0464206 -4.0466971 -4.0567183 -4.0722032 -4.0882726 -4.0987244 -4.1188631 -4.1517777 -4.1879382][-4.16134 -4.1337748 -4.1007581 -4.0688462 -4.0518756 -4.0658813 -4.085587 -4.0933251 -4.0956984 -4.1000838 -4.110301 -4.112483 -4.1260624 -4.1571879 -4.1889763][-4.169601 -4.1403904 -4.1067438 -4.0786724 -4.0733175 -4.1008558 -4.1270781 -4.1315174 -4.1239042 -4.1198997 -4.12608 -4.1258116 -4.1362839 -4.160491 -4.1795654][-4.1932836 -4.1664982 -4.1350327 -4.1119647 -4.1157141 -4.1475892 -4.1662335 -4.1607614 -4.1428432 -4.1302438 -4.1320744 -4.1335974 -4.1466737 -4.166945 -4.1743236][-4.2251773 -4.2034788 -4.1757607 -4.1587691 -4.1654596 -4.1930118 -4.1990108 -4.180789 -4.1537747 -4.1346116 -4.1337657 -4.1422439 -4.166687 -4.1890192 -4.1852703]]...]
INFO - root - 2017-12-06 06:58:59.164228: step 8010, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 58h:58m:25s remains)
INFO - root - 2017-12-06 06:59:05.665029: step 8020, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.662 sec/batch; 59h:39m:13s remains)
INFO - root - 2017-12-06 06:59:12.179094: step 8030, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 57h:41m:30s remains)
INFO - root - 2017-12-06 06:59:18.780433: step 8040, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 62h:31m:00s remains)
INFO - root - 2017-12-06 06:59:25.316255: step 8050, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 57h:42m:54s remains)
INFO - root - 2017-12-06 06:59:31.817017: step 8060, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 61h:12m:47s remains)
INFO - root - 2017-12-06 06:59:38.404404: step 8070, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 60h:58m:53s remains)
INFO - root - 2017-12-06 06:59:44.885692: step 8080, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 60h:31m:04s remains)
INFO - root - 2017-12-06 06:59:51.331916: step 8090, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 60h:46m:55s remains)
INFO - root - 2017-12-06 06:59:57.865250: step 8100, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 56h:31m:57s remains)
2017-12-06 06:59:58.557104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3178329 -4.3127723 -4.3055367 -4.29767 -4.2877355 -4.279048 -4.2754264 -4.2743812 -4.2706347 -4.2723536 -4.2771807 -4.2837987 -4.2934828 -4.3058538 -4.3173532][-4.3179541 -4.3115659 -4.3037705 -4.2925949 -4.2740593 -4.2559457 -4.2480793 -4.2448697 -4.2353921 -4.2356019 -4.2400961 -4.2466874 -4.25916 -4.2788219 -4.3000364][-4.3232489 -4.3177371 -4.3066292 -4.285737 -4.2562194 -4.2283435 -4.212863 -4.206327 -4.1900535 -4.1852231 -4.1912384 -4.2004623 -4.2176247 -4.2457485 -4.2761407][-4.3269796 -4.3209047 -4.3055544 -4.2734861 -4.2330942 -4.195868 -4.1732879 -4.164938 -4.1436148 -4.133615 -4.1397634 -4.1528378 -4.1758852 -4.2090664 -4.245513][-4.3223009 -4.3153033 -4.2918077 -4.2490563 -4.1961169 -4.1450224 -4.1171284 -4.1104312 -4.087676 -4.076272 -4.0820975 -4.0953288 -4.1242542 -4.165113 -4.2089958][-4.3115487 -4.30161 -4.2704878 -4.2193236 -4.1574378 -4.0916257 -4.057725 -4.0508275 -4.0272589 -4.0173616 -4.019814 -4.0288043 -4.0631695 -4.1176968 -4.1731114][-4.2975717 -4.2805924 -4.2403784 -4.1863222 -4.1251698 -4.0588422 -4.0259366 -4.0130124 -3.9822686 -3.9609718 -3.9452114 -3.9491961 -3.9928033 -4.0704432 -4.1437807][-4.2832189 -4.2609859 -4.2165194 -4.1610217 -4.1043706 -4.0488219 -4.0212064 -4.0014024 -3.9620459 -3.9223354 -3.8833485 -3.8844349 -3.9421563 -4.0397663 -4.1295881][-4.2736206 -4.2529011 -4.2140183 -4.1630416 -4.1120143 -4.0626478 -4.0391831 -4.0129375 -3.9706683 -3.9234865 -3.87198 -3.8726909 -3.9385228 -4.0425549 -4.1354485][-4.2797761 -4.2704005 -4.2441783 -4.2031341 -4.1643395 -4.1228929 -4.101059 -4.0667686 -4.0256958 -3.9826291 -3.9319291 -3.93327 -3.9943933 -4.0881615 -4.168364][-4.2951522 -4.2967649 -4.2846918 -4.2597737 -4.2353058 -4.200305 -4.1768041 -4.1395879 -4.1016831 -4.0708847 -4.0299397 -4.0310926 -4.0823298 -4.15985 -4.219789][-4.314189 -4.3231454 -4.3227649 -4.3142781 -4.2990952 -4.2687306 -4.2459135 -4.2132077 -4.1849213 -4.1692858 -4.1404595 -4.1389828 -4.1775131 -4.2356663 -4.272356][-4.3251014 -4.3400192 -4.3499432 -4.3529897 -4.3426104 -4.3204141 -4.3002653 -4.2751045 -4.2541614 -4.2453594 -4.2267261 -4.225419 -4.2536035 -4.2931404 -4.3109207][-4.3234859 -4.3409953 -4.35786 -4.3664937 -4.3603635 -4.3459282 -4.3294611 -4.3130789 -4.2970591 -4.2895732 -4.2790046 -4.2778854 -4.2955856 -4.320281 -4.3292384][-4.3134189 -4.3263111 -4.3416481 -4.3495979 -4.3477149 -4.3418121 -4.3338771 -4.3253117 -4.3150983 -4.3109803 -4.3086691 -4.3084512 -4.3169818 -4.331285 -4.337007]]...]
INFO - root - 2017-12-06 07:00:05.130353: step 8110, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 58h:54m:29s remains)
INFO - root - 2017-12-06 07:00:11.739281: step 8120, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 59h:23m:26s remains)
INFO - root - 2017-12-06 07:00:18.298715: step 8130, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 58h:06m:20s remains)
INFO - root - 2017-12-06 07:00:24.928600: step 8140, loss = 2.02, batch loss = 1.96 (11.8 examples/sec; 0.679 sec/batch; 61h:10m:35s remains)
INFO - root - 2017-12-06 07:00:31.616662: step 8150, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.681 sec/batch; 61h:19m:40s remains)
INFO - root - 2017-12-06 07:00:38.193842: step 8160, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 58h:26m:13s remains)
INFO - root - 2017-12-06 07:00:44.778698: step 8170, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 59h:21m:49s remains)
INFO - root - 2017-12-06 07:00:50.934749: step 8180, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.664 sec/batch; 59h:50m:42s remains)
INFO - root - 2017-12-06 07:00:57.469552: step 8190, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 61h:13m:35s remains)
INFO - root - 2017-12-06 07:01:03.992263: step 8200, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 57h:55m:56s remains)
2017-12-06 07:01:04.197711: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 07:01:04.197736: E tensorflow/core/util/events_writer.cc:131] Failed to flush 16 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 07:01:04.692738: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3335853 -4.3408351 -4.3281164 -4.2948985 -4.2488575 -4.2060952 -4.1787338 -4.1910133 -4.2167444 -4.2261357 -4.2313027 -4.2466111 -4.2715979 -4.2966948 -4.3155384][-4.3303952 -4.332202 -4.3129144 -4.2732553 -4.2202907 -4.1764421 -4.1527967 -4.1762228 -4.2130675 -4.23079 -4.2405682 -4.255877 -4.2757397 -4.2927308 -4.3041525][-4.3263187 -4.3238068 -4.2997065 -4.2565451 -4.1998782 -4.1545658 -4.1352553 -4.1698871 -4.2175121 -4.2455087 -4.2602973 -4.2745037 -4.2880831 -4.2961407 -4.299346][-4.3253174 -4.3223295 -4.2979889 -4.253983 -4.1942039 -4.145442 -4.1228271 -4.15721 -4.2103629 -4.2482572 -4.2688718 -4.2835884 -4.2938342 -4.2980623 -4.2985964][-4.3258705 -4.3248482 -4.302978 -4.2561426 -4.1903396 -4.1293902 -4.0868912 -4.1075888 -4.1655579 -4.2192268 -4.2554374 -4.2793951 -4.2931767 -4.2991886 -4.3005691][-4.3268461 -4.3274317 -4.305285 -4.2531114 -4.1761441 -4.0904236 -4.0091629 -4.0043187 -4.0709682 -4.147079 -4.2077804 -4.2510262 -4.2779818 -4.2918377 -4.2967181][-4.3302402 -4.3328891 -4.3095441 -4.2481747 -4.1530776 -4.0340762 -3.9094493 -3.8803198 -3.9585578 -4.0551043 -4.137753 -4.2007027 -4.24394 -4.267787 -4.2780643][-4.3307443 -4.3364263 -4.3147597 -4.2527666 -4.1545391 -4.0243216 -3.8858409 -3.8541355 -3.9370031 -4.0308876 -4.1094646 -4.1714435 -4.2159696 -4.2386508 -4.2492552][-4.3250008 -4.3315907 -4.3151712 -4.2610464 -4.1793785 -4.0688543 -3.9537115 -3.9387624 -4.0157137 -4.0911417 -4.1470909 -4.1866117 -4.2139215 -4.2245145 -4.230319][-4.3184729 -4.32463 -4.3136477 -4.2742925 -4.2208381 -4.1439667 -4.06313 -4.0606375 -4.1208048 -4.1729827 -4.2039652 -4.2205443 -4.2299266 -4.2279854 -4.228888][-4.3172464 -4.319942 -4.3136215 -4.2902336 -4.2599173 -4.2113585 -4.1584749 -4.1594625 -4.198369 -4.2287869 -4.2420492 -4.2470093 -4.2474322 -4.2389445 -4.2374458][-4.3222232 -4.3210516 -4.3150544 -4.2988591 -4.2820477 -4.252583 -4.2202497 -4.2208643 -4.2454576 -4.2614565 -4.2628617 -4.2627935 -4.2585025 -4.2484922 -4.2475362][-4.328608 -4.3254237 -4.3175869 -4.3004179 -4.2847247 -4.265274 -4.2468481 -4.249835 -4.2696223 -4.2798553 -4.2772322 -4.2726555 -4.2659621 -4.2577672 -4.2576728][-4.3319578 -4.327116 -4.316947 -4.2944069 -4.2714772 -4.25102 -4.2356739 -4.2441859 -4.2714872 -4.2866945 -4.2891207 -4.2861218 -4.2801414 -4.2746234 -4.2736158][-4.3327227 -4.328023 -4.316957 -4.2892108 -4.2551875 -4.2203054 -4.1939735 -4.2055016 -4.2493668 -4.2789946 -4.2934833 -4.2985392 -4.2969074 -4.2929678 -4.2878189]]...]
INFO - root - 2017-12-06 07:01:11.285584: step 8210, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 60h:16m:23s remains)
INFO - root - 2017-12-06 07:01:17.891975: step 8220, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 57h:42m:18s remains)
INFO - root - 2017-12-06 07:01:24.456343: step 8230, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 58h:39m:54s remains)
INFO - root - 2017-12-06 07:01:31.075874: step 8240, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 58h:30m:46s remains)
INFO - root - 2017-12-06 07:01:37.561892: step 8250, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 58h:28m:24s remains)
INFO - root - 2017-12-06 07:01:44.033918: step 8260, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 60h:45m:55s remains)
INFO - root - 2017-12-06 07:01:50.524248: step 8270, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 59h:49m:57s remains)
INFO - root - 2017-12-06 07:01:56.639100: step 8280, loss = 2.07, batch loss = 2.01 (22.5 examples/sec; 0.355 sec/batch; 31h:57m:41s remains)
INFO - root - 2017-12-06 07:02:03.136996: step 8290, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 57h:20m:09s remains)
INFO - root - 2017-12-06 07:02:09.714053: step 8300, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 57h:54m:49s remains)
2017-12-06 07:02:10.387819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.36058 -4.3549156 -4.3487115 -4.3454709 -4.3456335 -4.3466883 -4.345552 -4.3421783 -4.3409858 -4.3418465 -4.3428426 -4.3432035 -4.3446784 -4.3490281 -4.35691][-4.3551264 -4.347959 -4.3399959 -4.3345075 -4.333703 -4.3343453 -4.3321738 -4.3272185 -4.3267832 -4.3286486 -4.3300166 -4.3291221 -4.3279114 -4.33134 -4.3416963][-4.3464737 -4.335834 -4.3243852 -4.3161659 -4.3132844 -4.3113322 -4.3056993 -4.298604 -4.298727 -4.3011713 -4.302073 -4.300539 -4.2987552 -4.3017287 -4.31474][-4.3338194 -4.3166008 -4.2990379 -4.2848473 -4.2745123 -4.264863 -4.25336 -4.2456012 -4.248662 -4.2521663 -4.251246 -4.2485943 -4.2477517 -4.2527308 -4.2716622][-4.3088918 -4.2827091 -4.2582178 -4.236618 -4.2148461 -4.19303 -4.1714683 -4.1623187 -4.1723351 -4.182404 -4.18267 -4.1808362 -4.1847448 -4.195518 -4.2222147][-4.2704859 -4.2355466 -4.203135 -4.1736565 -4.1408195 -4.1035843 -4.0662951 -4.0537872 -4.0787711 -4.1059642 -4.114356 -4.1174316 -4.1260915 -4.1428437 -4.1786776][-4.2336454 -4.193501 -4.1554823 -4.1187291 -4.07478 -4.0186553 -3.9575524 -3.9421706 -3.995522 -4.0528803 -4.0754943 -4.083014 -4.0915689 -4.1093621 -4.1521463][-4.21014 -4.1697569 -4.1286368 -4.0795145 -4.0191288 -3.9431467 -3.8601556 -3.84966 -3.9411352 -4.0287523 -4.0648055 -4.0715222 -4.0735769 -4.0914688 -4.1393385][-4.2128229 -4.1752896 -4.1323156 -4.0713377 -4.0009747 -3.9237168 -3.8482766 -3.8527241 -3.95533 -4.0521007 -4.0908628 -4.0934906 -4.0884533 -4.1001358 -4.1415663][-4.2323146 -4.1964316 -4.1533327 -4.0943651 -4.0343194 -3.9778633 -3.9302669 -3.9443755 -4.0270095 -4.1090145 -4.1423755 -4.1452403 -4.1410866 -4.144877 -4.1720748][-4.2497344 -4.2182112 -4.1809306 -4.1342483 -4.0894709 -4.0535593 -4.027329 -4.0386949 -4.0917912 -4.1508942 -4.1824908 -4.1951728 -4.1956334 -4.1927328 -4.206193][-4.2737389 -4.24882 -4.222168 -4.191318 -4.1637917 -4.14394 -4.1309538 -4.13329 -4.15732 -4.192512 -4.21977 -4.2378221 -4.2423615 -4.2368956 -4.2423549][-4.3074641 -4.2921791 -4.2777104 -4.2618728 -4.2484679 -4.2397785 -4.2349911 -4.2312303 -4.234642 -4.2493477 -4.2666764 -4.2834864 -4.2888479 -4.2844362 -4.2845855][-4.3384814 -4.3321939 -4.3280482 -4.3230634 -4.3202682 -4.3212323 -4.3220019 -4.3150134 -4.306509 -4.3048782 -4.3099189 -4.3223724 -4.3300581 -4.329576 -4.3275137][-4.3596926 -4.3575311 -4.3570085 -4.3567181 -4.3584919 -4.3633008 -4.366519 -4.3609862 -4.3505878 -4.3421111 -4.3402476 -4.3476253 -4.3549361 -4.3570495 -4.35492]]...]
INFO - root - 2017-12-06 07:02:17.079703: step 8310, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 59h:47m:30s remains)
INFO - root - 2017-12-06 07:02:23.486896: step 8320, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 58h:33m:27s remains)
INFO - root - 2017-12-06 07:02:30.063721: step 8330, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 59h:27m:34s remains)
INFO - root - 2017-12-06 07:02:36.817395: step 8340, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 61h:05m:42s remains)
INFO - root - 2017-12-06 07:02:43.360837: step 8350, loss = 2.11, batch loss = 2.05 (12.4 examples/sec; 0.647 sec/batch; 58h:12m:49s remains)
INFO - root - 2017-12-06 07:02:50.004157: step 8360, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 61h:52m:31s remains)
INFO - root - 2017-12-06 07:02:56.536196: step 8370, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 57h:51m:44s remains)
INFO - root - 2017-12-06 07:03:03.065323: step 8380, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:51m:16s remains)
INFO - root - 2017-12-06 07:03:09.546509: step 8390, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 61h:51m:17s remains)
INFO - root - 2017-12-06 07:03:16.102428: step 8400, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:00m:56s remains)
2017-12-06 07:03:16.305472: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 07:03:16.305505: E tensorflow/core/util/events_writer.cc:131] Failed to flush 18 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 07:03:16.829577: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3457031 -4.3470306 -4.3411055 -4.3291011 -4.3114572 -4.2914209 -4.2750664 -4.26302 -4.2626524 -4.27331 -4.2887058 -4.3010235 -4.3056526 -4.30399 -4.2953005][-4.3455191 -4.3463311 -4.338336 -4.3223772 -4.29632 -4.2611752 -4.2283864 -4.2088041 -4.2139978 -4.237186 -4.263905 -4.2861795 -4.2959671 -4.2915812 -4.2774343][-4.3456688 -4.346643 -4.3359542 -4.3138814 -4.2767954 -4.222455 -4.1696105 -4.1429725 -4.1559305 -4.1916313 -4.2272735 -4.257627 -4.2768216 -4.2772131 -4.260788][-4.3481789 -4.3490825 -4.3334765 -4.3010268 -4.2522578 -4.1822147 -4.1202469 -4.0955367 -4.1203465 -4.1649971 -4.2041588 -4.23891 -4.2655516 -4.2741423 -4.26446][-4.352335 -4.3518286 -4.3279138 -4.2793026 -4.216579 -4.1352596 -4.0708733 -4.05404 -4.093914 -4.1422977 -4.1760406 -4.2083774 -4.2383676 -4.2564664 -4.2569785][-4.3545909 -4.35264 -4.3214111 -4.2578726 -4.1818409 -4.0923581 -4.0260181 -4.0165854 -4.0688767 -4.1227407 -4.1500549 -4.1748662 -4.1990108 -4.2192411 -4.2297144][-4.354003 -4.3504481 -4.3144455 -4.2423525 -4.1550913 -4.0628653 -3.9964421 -3.9907744 -4.0512338 -4.1049156 -4.1273575 -4.1438441 -4.156219 -4.1702867 -4.1874509][-4.3529229 -4.3465643 -4.304863 -4.2281618 -4.1401248 -4.0548162 -3.9965088 -3.9926417 -4.0550194 -4.1076703 -4.12438 -4.1287427 -4.1274548 -4.1359582 -4.1500697][-4.3520479 -4.3432131 -4.2952175 -4.2178559 -4.1392517 -4.0685186 -4.0211253 -4.0182161 -4.0718174 -4.11627 -4.1317697 -4.1369119 -4.1330047 -4.1364942 -4.144865][-4.3503604 -4.3411326 -4.2903881 -4.2143388 -4.1491671 -4.092051 -4.0496821 -4.04346 -4.0860462 -4.1193314 -4.1299467 -4.1392326 -4.1457787 -4.1530752 -4.1626163][-4.3482971 -4.3410621 -4.2940841 -4.2233024 -4.1699367 -4.11965 -4.0745273 -4.0563526 -4.0897236 -4.1158843 -4.1202288 -4.1292768 -4.1419563 -4.1587214 -4.1730304][-4.3461866 -4.3421068 -4.3026652 -4.2367735 -4.184236 -4.128758 -4.0752134 -4.0406895 -4.0678368 -4.0968676 -4.1068344 -4.1209979 -4.138423 -4.1601615 -4.1757841][-4.3431334 -4.3421082 -4.3126535 -4.2531776 -4.1980281 -4.1386075 -4.0814648 -4.0369682 -4.0614853 -4.0934196 -4.1074963 -4.1225462 -4.1396408 -4.162447 -4.1783695][-4.3399453 -4.3402715 -4.3199749 -4.2701669 -4.21289 -4.1558032 -4.10745 -4.0707884 -4.0920329 -4.1198182 -4.1296477 -4.1316824 -4.13732 -4.1525164 -4.1705089][-4.337934 -4.3388796 -4.3272667 -4.2901812 -4.236311 -4.1853337 -4.1487823 -4.1229196 -4.1418152 -4.1623516 -4.1678734 -4.1632657 -4.1618633 -4.1726069 -4.1920891]]...]
INFO - root - 2017-12-06 07:03:23.443930: step 8410, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.680 sec/batch; 61h:15m:24s remains)
INFO - root - 2017-12-06 07:03:29.868081: step 8420, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.647 sec/batch; 58h:15m:09s remains)
INFO - root - 2017-12-06 07:03:36.366502: step 8430, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 57h:19m:36s remains)
INFO - root - 2017-12-06 07:03:42.842598: step 8440, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 58h:45m:16s remains)
INFO - root - 2017-12-06 07:03:49.486794: step 8450, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.664 sec/batch; 59h:45m:25s remains)
INFO - root - 2017-12-06 07:03:56.080818: step 8460, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 60h:44m:10s remains)
INFO - root - 2017-12-06 07:04:02.570420: step 8470, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 56h:56m:49s remains)
INFO - root - 2017-12-06 07:04:08.908421: step 8480, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 57h:16m:57s remains)
INFO - root - 2017-12-06 07:04:15.395469: step 8490, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 57h:32m:11s remains)
INFO - root - 2017-12-06 07:04:21.885172: step 8500, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 58h:52m:59s remains)
2017-12-06 07:04:22.560426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3236914 -4.3285041 -4.3272543 -4.3261142 -4.3254728 -4.3268514 -4.328856 -4.3298054 -4.3287458 -4.3277707 -4.3289371 -4.3300066 -4.3297081 -4.3286376 -4.3261847][-4.3350616 -4.3413153 -4.3402929 -4.3381042 -4.33652 -4.3371091 -4.3380589 -4.3377647 -4.3356156 -4.3335667 -4.3337307 -4.334311 -4.3346586 -4.3351717 -4.3344312][-4.3315673 -4.3362832 -4.3344131 -4.3312135 -4.3299389 -4.3313756 -4.332592 -4.3324308 -4.3314109 -4.3303461 -4.3304868 -4.3309345 -4.3320017 -4.3340573 -4.3348126][-4.3206196 -4.3226829 -4.3186321 -4.3131623 -4.31053 -4.3109479 -4.3110728 -4.3107991 -4.310986 -4.3116865 -4.3139191 -4.3169584 -4.3212228 -4.3262315 -4.3286715][-4.3049736 -4.3023171 -4.2917676 -4.2792206 -4.2713308 -4.2682195 -4.2662077 -4.2646842 -4.2643313 -4.2661138 -4.2728014 -4.2816291 -4.2905574 -4.2983589 -4.3017092][-4.2828 -4.2748728 -4.2561393 -4.2337956 -4.216476 -4.2063012 -4.1982884 -4.19168 -4.1871552 -4.1883879 -4.2017694 -4.2208543 -4.2373085 -4.2485871 -4.2519159][-4.261744 -4.2540984 -4.2293825 -4.1949096 -4.1628914 -4.1393337 -4.1192589 -4.1005416 -4.0866537 -4.0851364 -4.1051936 -4.1363811 -4.1619968 -4.1779461 -4.1830621][-4.2354913 -4.2322478 -4.2085314 -4.168499 -4.1256 -4.0920248 -4.0624905 -4.0328355 -4.010262 -4.0040975 -4.026103 -4.0633297 -4.0930271 -4.1099424 -4.1149387][-4.1971436 -4.2018185 -4.1873531 -4.1541128 -4.11663 -4.0894451 -4.0685425 -4.04727 -4.0316687 -4.0265241 -4.0428762 -4.0730095 -4.0961742 -4.1074748 -4.1105537][-4.176827 -4.1871023 -4.1812367 -4.1598358 -4.1364541 -4.1243806 -4.1206737 -4.1190014 -4.1199903 -4.1224861 -4.1343904 -4.1535916 -4.1674356 -4.1739368 -4.1764231][-4.2087688 -4.2208376 -4.2206006 -4.2089977 -4.1960583 -4.1921048 -4.1951728 -4.203002 -4.2129307 -4.2214665 -4.2313218 -4.2429276 -4.2510424 -4.2549844 -4.25703][-4.2644277 -4.2732558 -4.2739844 -4.2680216 -4.2620578 -4.2606559 -4.2633085 -4.2705383 -4.2801833 -4.2890158 -4.2959161 -4.3016548 -4.3056765 -4.3081927 -4.3099027][-4.3062563 -4.3111105 -4.3113618 -4.3088026 -4.30595 -4.3042722 -4.3045058 -4.3081975 -4.3139806 -4.3199782 -4.3238544 -4.32602 -4.3278933 -4.32935 -4.330081][-4.3258524 -4.326725 -4.3251376 -4.3229632 -4.3212767 -4.3199067 -4.31822 -4.318152 -4.3202338 -4.32334 -4.3246078 -4.3249125 -4.3253379 -4.325675 -4.3259587][-4.3256574 -4.3252282 -4.3219957 -4.3199568 -4.3201017 -4.3200159 -4.3186665 -4.3177476 -4.3181987 -4.3189945 -4.318316 -4.3169541 -4.3155408 -4.3143411 -4.314045]]...]
INFO - root - 2017-12-06 07:04:29.116094: step 8510, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 57h:58m:15s remains)
INFO - root - 2017-12-06 07:04:35.811108: step 8520, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 59h:59m:45s remains)
INFO - root - 2017-12-06 07:04:42.332785: step 8530, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 59h:23m:47s remains)
INFO - root - 2017-12-06 07:04:48.859051: step 8540, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:48m:05s remains)
INFO - root - 2017-12-06 07:04:55.396633: step 8550, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 60h:41m:04s remains)
INFO - root - 2017-12-06 07:05:01.939263: step 8560, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 59h:25m:57s remains)
INFO - root - 2017-12-06 07:05:08.459367: step 8570, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 59h:45m:25s remains)
INFO - root - 2017-12-06 07:05:14.871089: step 8580, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 56h:50m:52s remains)
INFO - root - 2017-12-06 07:05:21.443393: step 8590, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 59h:55m:17s remains)
INFO - root - 2017-12-06 07:05:27.801533: step 8600, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.663 sec/batch; 59h:36m:31s remains)
2017-12-06 07:05:28.007727: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 07:05:28.007765: E tensorflow/core/util/events_writer.cc:131] Failed to flush 20 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 07:05:28.471429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2645526 -4.2608557 -4.2590942 -4.2580309 -4.2563004 -4.2532864 -4.2499514 -4.2473211 -4.247015 -4.2485833 -4.2510271 -4.2518649 -4.2495313 -4.245779 -4.2386785][-4.2826834 -4.2811913 -4.2804937 -4.2803521 -4.2783661 -4.2725635 -4.2655187 -4.2595534 -4.2576852 -4.2601109 -4.2647424 -4.2671795 -4.2642903 -4.2573986 -4.2460718][-4.2903128 -4.2901545 -4.293673 -4.2988491 -4.2998557 -4.2926598 -4.2812686 -4.2707844 -4.2670021 -4.2709622 -4.2786827 -4.2834964 -4.28009 -4.2697182 -4.2550311][-4.289505 -4.2892179 -4.2970939 -4.3088984 -4.314867 -4.305903 -4.2899404 -4.2776504 -4.2767234 -4.2842808 -4.294168 -4.3007622 -4.2966323 -4.2847486 -4.2694411][-4.2752585 -4.2692819 -4.2763963 -4.2918773 -4.2988334 -4.2865043 -4.2666979 -4.2555552 -4.2660866 -4.2832351 -4.2949944 -4.3017416 -4.29683 -4.2838922 -4.2697334][-4.2467451 -4.2375679 -4.2434878 -4.2565618 -4.2550263 -4.2293034 -4.1960273 -4.1812348 -4.2076149 -4.2415085 -4.2613835 -4.2716775 -4.2702794 -4.2583861 -4.248045][-4.2021246 -4.1972179 -4.2114782 -4.2262588 -4.2166128 -4.1706705 -4.108017 -4.0670733 -4.1017909 -4.1589994 -4.1941295 -4.2149076 -4.22258 -4.216444 -4.2121711][-4.1612868 -4.1578088 -4.1789975 -4.1978903 -4.1832857 -4.1159825 -4.016252 -3.9337749 -3.9664111 -4.0450525 -4.0957813 -4.1289558 -4.1478963 -4.1513114 -4.1559238][-4.1507397 -4.1428819 -4.1592979 -4.1742024 -4.1547928 -4.0780411 -3.961782 -3.8570104 -3.8823748 -3.9664462 -4.0292568 -4.075182 -4.1004229 -4.109251 -4.1166992][-4.14111 -4.1328015 -4.1496248 -4.1651859 -4.1561661 -4.1007276 -4.0203295 -3.9456801 -3.9553025 -4.0113425 -4.06118 -4.1034474 -4.1215882 -4.1262193 -4.1278043][-4.1306529 -4.1292515 -4.1442575 -4.1581964 -4.1581635 -4.1272736 -4.0891747 -4.0474591 -4.0468426 -4.0747871 -4.1090555 -4.1461706 -4.15949 -4.1605496 -4.15564][-4.1124134 -4.115037 -4.1253877 -4.1357679 -4.1413689 -4.1277142 -4.1177268 -4.1019464 -4.1003895 -4.1176462 -4.1486249 -4.1829643 -4.1945004 -4.1935949 -4.1832123][-4.1011806 -4.1060929 -4.1132426 -4.122695 -4.1334238 -4.13436 -4.1411629 -4.1391339 -4.1363058 -4.1499114 -4.18115 -4.2108307 -4.2197623 -4.2192078 -4.2092342][-4.1308513 -4.1346059 -4.1403894 -4.1464295 -4.1577792 -4.1632853 -4.1730065 -4.1763215 -4.17394 -4.184526 -4.2099457 -4.2307243 -4.2364163 -4.2386479 -4.2351069][-4.18769 -4.1889939 -4.1935263 -4.198658 -4.2069416 -4.2104063 -4.2159686 -4.2175503 -4.2141404 -4.2197251 -4.2342372 -4.2447033 -4.249989 -4.2537174 -4.2532096]]...]
INFO - root - 2017-12-06 07:05:35.004711: step 8610, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.622 sec/batch; 55h:56m:19s remains)
INFO - root - 2017-12-06 07:05:41.515531: step 8620, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 57h:12m:45s remains)
INFO - root - 2017-12-06 07:05:48.085688: step 8630, loss = 2.03, batch loss = 1.98 (12.5 examples/sec; 0.639 sec/batch; 57h:30m:00s remains)
INFO - root - 2017-12-06 07:05:54.705019: step 8640, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 61h:38m:32s remains)
INFO - root - 2017-12-06 07:06:01.293713: step 8650, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.658 sec/batch; 59h:09m:36s remains)
INFO - root - 2017-12-06 07:06:07.709205: step 8660, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 57h:28m:27s remains)
INFO - root - 2017-12-06 07:06:14.225521: step 8670, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 0.552 sec/batch; 49h:38m:55s remains)
INFO - root - 2017-12-06 07:06:20.706902: step 8680, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 59h:19m:38s remains)
INFO - root - 2017-12-06 07:06:27.374609: step 8690, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 59h:36m:03s remains)
INFO - root - 2017-12-06 07:06:33.770480: step 8700, loss = 2.03, batch loss = 1.98 (12.1 examples/sec; 0.660 sec/batch; 59h:21m:05s remains)
2017-12-06 07:06:34.424633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.212317 -4.214839 -4.2162318 -4.21583 -4.213244 -4.2123861 -4.2133865 -4.2147856 -4.2167377 -4.2201462 -4.2218685 -4.2239718 -4.2278557 -4.2330289 -4.2371693][-4.1917987 -4.1934857 -4.1943231 -4.1918616 -4.1856666 -4.1842842 -4.1868587 -4.1901159 -4.1946397 -4.2001643 -4.2026005 -4.2052341 -4.2122049 -4.2208219 -4.2270031][-4.1720228 -4.1703677 -4.1670332 -4.158628 -4.1468067 -4.14223 -4.1421943 -4.1453977 -4.1557627 -4.1688938 -4.1760011 -4.18228 -4.1956248 -4.2087216 -4.2152038][-4.1537638 -4.1455946 -4.1362052 -4.1193838 -4.0965123 -4.0803838 -4.0671439 -4.0658927 -4.0840931 -4.1107006 -4.1307669 -4.1479187 -4.1722517 -4.1942673 -4.2039838][-4.1440787 -4.1256843 -4.1048942 -4.0760293 -4.0394917 -4.0017738 -3.9654005 -3.9567149 -3.9853311 -4.025991 -4.06196 -4.0934 -4.1297808 -4.1650014 -4.1829681][-4.1410284 -4.1091146 -4.0743208 -4.0323734 -3.9836006 -3.9215839 -3.8558998 -3.8418994 -3.8880041 -3.9472151 -3.9969363 -4.0346413 -4.0738568 -4.1157432 -4.1480694][-4.1508493 -4.1081886 -4.0610976 -4.0109515 -3.9579241 -3.8840981 -3.79918 -3.7761931 -3.8302107 -3.8973813 -3.9520068 -3.9881275 -4.0210085 -4.06482 -4.1110549][-4.1685715 -4.1234965 -4.0756259 -4.0328064 -3.9904983 -3.9316349 -3.864831 -3.8453426 -3.8724654 -3.9095058 -3.9461451 -3.9677055 -3.9887288 -4.0281749 -4.08556][-4.1837311 -4.1475396 -4.1139474 -4.0898528 -4.064702 -4.0251145 -3.9835525 -3.9734621 -3.9734776 -3.976368 -3.9856737 -3.9828 -3.9850533 -4.0131259 -4.0676184][-4.20861 -4.1854553 -4.1658659 -4.1524277 -4.1330843 -4.1024127 -4.0781474 -4.0768156 -4.0673103 -4.05122 -4.0321178 -4.0044641 -3.9942811 -4.0143371 -4.0578194][-4.23177 -4.2128472 -4.199975 -4.1904144 -4.1692753 -4.1401677 -4.1252441 -4.12868 -4.1207638 -4.101397 -4.0695367 -4.0360308 -4.0265412 -4.0390968 -4.0660934][-4.2391453 -4.2252078 -4.2152348 -4.203166 -4.1795974 -4.1532726 -4.1427035 -4.1435103 -4.1388311 -4.1296163 -4.1043234 -4.0763621 -4.0664792 -4.0697064 -4.0824561][-4.2256742 -4.212822 -4.2025075 -4.1896839 -4.1678839 -4.1487646 -4.1404762 -4.1366906 -4.1328254 -4.1261754 -4.1095104 -4.0919361 -4.0829978 -4.0781841 -4.0779881][-4.2016811 -4.1893654 -4.1807628 -4.1679535 -4.1522946 -4.14109 -4.1332426 -4.1261759 -4.1153426 -4.1052566 -4.0936589 -4.0817 -4.0797143 -4.0804853 -4.0720096][-4.1757426 -4.1692762 -4.165287 -4.1580806 -4.1520567 -4.1457696 -4.13835 -4.1312466 -4.1187344 -4.1049104 -4.0953803 -4.0803452 -4.077383 -4.0830588 -4.06985]]...]
INFO - root - 2017-12-06 07:06:40.991889: step 8710, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:46m:33s remains)
INFO - root - 2017-12-06 07:06:47.569379: step 8720, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 58h:40m:29s remains)
INFO - root - 2017-12-06 07:06:53.987468: step 8730, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 58h:57m:30s remains)
INFO - root - 2017-12-06 07:07:00.584841: step 8740, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 59h:37m:27s remains)
INFO - root - 2017-12-06 07:07:07.201491: step 8750, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:04m:35s remains)
INFO - root - 2017-12-06 07:07:13.654234: step 8760, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 59h:14m:12s remains)
INFO - root - 2017-12-06 07:07:20.147770: step 8770, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 60h:02m:00s remains)
INFO - root - 2017-12-06 07:07:26.695781: step 8780, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 59h:12m:11s remains)
INFO - root - 2017-12-06 07:07:33.210661: step 8790, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 59h:57m:33s remains)
INFO - root - 2017-12-06 07:07:39.423454: step 8800, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 58h:18m:08s remains)
2017-12-06 07:07:39.633128: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 07:07:39.633170: E tensorflow/core/util/events_writer.cc:131] Failed to flush 22 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 07:07:40.066781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1718388 -4.1858072 -4.1926737 -4.1879559 -4.1842012 -4.182148 -4.18097 -4.1856389 -4.2003312 -4.2263083 -4.2539587 -4.2778411 -4.2949467 -4.3072681 -4.3040743][-4.1532369 -4.1794524 -4.19872 -4.2020249 -4.2047415 -4.2030673 -4.1964426 -4.1943722 -4.2002993 -4.219203 -4.2434807 -4.2652087 -4.2858233 -4.3023596 -4.299355][-4.1502929 -4.1860123 -4.21006 -4.2178783 -4.2191763 -4.211338 -4.1968212 -4.183567 -4.1785965 -4.1912966 -4.2168422 -4.2414665 -4.2688832 -4.28913 -4.286828][-4.1664052 -4.2066059 -4.230278 -4.235105 -4.2261105 -4.2034264 -4.1714668 -4.1372952 -4.1247592 -4.1443248 -4.1816092 -4.2174754 -4.2546248 -4.2767477 -4.2735429][-4.1972895 -4.2411633 -4.261446 -4.2560716 -4.2314034 -4.1853809 -4.1186829 -4.0478573 -4.0298843 -4.0801926 -4.1498613 -4.2032886 -4.2466478 -4.2666645 -4.2599359][-4.2320943 -4.2761669 -4.2913055 -4.2761993 -4.2381048 -4.1644993 -4.0525055 -3.9327345 -3.9155643 -4.0158587 -4.1306157 -4.2039127 -4.2479124 -4.2610426 -4.2492208][-4.2576809 -4.2920928 -4.2996259 -4.2813034 -4.2400784 -4.1486731 -4.0005364 -3.8456364 -3.8409529 -3.9854598 -4.1267462 -4.2115555 -4.251914 -4.25758 -4.2414718][-4.271534 -4.2913375 -4.2972054 -4.2832623 -4.2463913 -4.1576724 -4.0103292 -3.8696597 -3.8765879 -4.0177383 -4.1440506 -4.2170763 -4.2492828 -4.2512007 -4.2377748][-4.2706714 -4.2807555 -4.2919984 -4.2906065 -4.26563 -4.1938047 -4.0756145 -3.9769573 -3.9849932 -4.0845847 -4.1714168 -4.2222772 -4.24387 -4.2447309 -4.2399225][-4.2606592 -4.2634234 -4.2795043 -4.293427 -4.2882729 -4.2418141 -4.1591187 -4.0961666 -4.0955338 -4.1490555 -4.1988554 -4.2293596 -4.23887 -4.2404261 -4.2458582][-4.241889 -4.2413878 -4.2630754 -4.2960243 -4.3107772 -4.2882614 -4.2353506 -4.1907616 -4.1753669 -4.1934929 -4.2220092 -4.2398019 -4.240222 -4.2404442 -4.2504539][-4.2249117 -4.22922 -4.2576075 -4.3029375 -4.328073 -4.31893 -4.2843184 -4.246285 -4.2216506 -4.2203374 -4.2348647 -4.2457232 -4.2439013 -4.2444143 -4.25432][-4.2215075 -4.2350125 -4.2664585 -4.3096914 -4.3345566 -4.3300829 -4.3057733 -4.2731962 -4.2433829 -4.2331238 -4.23851 -4.246676 -4.2458367 -4.2461567 -4.2526736][-4.2395735 -4.2554111 -4.2834997 -4.3148475 -4.3302479 -4.3245568 -4.3045945 -4.2747645 -4.2404366 -4.2235327 -4.22571 -4.2368298 -4.2367558 -4.2321558 -4.233942][-4.2647514 -4.2779374 -4.2966714 -4.3150878 -4.3217082 -4.3145585 -4.2930188 -4.2614441 -4.2240911 -4.2005658 -4.1991568 -4.2091241 -4.2081976 -4.1994143 -4.1924744]]...]
INFO - root - 2017-12-06 07:07:46.649413: step 8810, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:46m:38s remains)
INFO - root - 2017-12-06 07:07:53.316651: step 8820, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 62h:34m:35s remains)
INFO - root - 2017-12-06 07:07:59.895050: step 8830, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 56h:23m:30s remains)
INFO - root - 2017-12-06 07:08:06.379377: step 8840, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 58h:56m:25s remains)
INFO - root - 2017-12-06 07:08:13.030114: step 8850, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 60h:52m:51s remains)
INFO - root - 2017-12-06 07:08:19.694065: step 8860, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 59h:15m:28s remains)
INFO - root - 2017-12-06 07:08:26.155069: step 8870, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 60h:51m:20s remains)
INFO - root - 2017-12-06 07:08:32.669631: step 8880, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 57h:36m:21s remains)
INFO - root - 2017-12-06 07:08:39.189635: step 8890, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 57h:33m:14s remains)
INFO - root - 2017-12-06 07:08:45.905028: step 8900, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 61h:20m:51s remains)
2017-12-06 07:08:46.466416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1382713 -4.1394176 -4.1523743 -4.1772652 -4.2009397 -4.2106414 -4.2188778 -4.2203469 -4.221983 -4.2286572 -4.2357526 -4.2235136 -4.1890936 -4.1475205 -4.1162906][-4.070425 -4.0905976 -4.1286454 -4.172544 -4.2063546 -4.2206092 -4.223567 -4.2181392 -4.2160382 -4.2190013 -4.2207994 -4.1966166 -4.1448636 -4.0871167 -4.0487275][-3.9919643 -4.0366898 -4.1053915 -4.17065 -4.2082644 -4.2166409 -4.2053628 -4.1891623 -4.1817255 -4.179625 -4.17894 -4.1536546 -4.1003675 -4.0453839 -4.0160427][-3.9866004 -4.0554237 -4.1389394 -4.1989565 -4.2196927 -4.2074823 -4.1711831 -4.1337094 -4.1146564 -4.112042 -4.1191373 -4.10951 -4.0790639 -4.0519981 -4.0458531][-4.0831008 -4.149755 -4.2126589 -4.2389421 -4.2287827 -4.1864181 -4.1191249 -4.053112 -4.0311308 -4.0464492 -4.0769329 -4.0945659 -4.097333 -4.0994239 -4.1066341][-4.1696644 -4.21453 -4.2470903 -4.2394795 -4.1949897 -4.1259985 -4.0284576 -3.9319685 -3.9255412 -3.9862974 -4.05239 -4.099668 -4.1285639 -4.1478996 -4.1572719][-4.2173157 -4.2368121 -4.23773 -4.1965027 -4.1173677 -4.0181613 -3.8929198 -3.77523 -3.8086755 -3.928488 -4.0276155 -4.0961466 -4.1444168 -4.1726847 -4.1730375][-4.2353091 -4.2344141 -4.2094808 -4.1469169 -4.0510674 -3.946238 -3.8332555 -3.7449965 -3.8146546 -3.9524632 -4.0484257 -4.1110134 -4.1624928 -4.1876831 -4.172461][-4.2365804 -4.2220731 -4.1839552 -4.1197567 -4.0380054 -3.959748 -3.8951225 -3.8619871 -3.9270406 -4.0334463 -4.1022439 -4.1443477 -4.1865921 -4.2028584 -4.1806607][-4.2214394 -4.200633 -4.1568012 -4.0974832 -4.036624 -3.9907293 -3.9689574 -3.9712474 -4.0197277 -4.0905313 -4.1374211 -4.1642871 -4.1929731 -4.2004118 -4.1805072][-4.2175674 -4.1934581 -4.1478748 -4.0989537 -4.0592918 -4.0388618 -4.0433812 -4.0643382 -4.1010714 -4.1436448 -4.1721578 -4.1863079 -4.2005343 -4.2018309 -4.1890965][-4.2306771 -4.2093091 -4.1751552 -4.1461697 -4.1303077 -4.12677 -4.1387663 -4.158586 -4.1782155 -4.1973853 -4.2115417 -4.2185931 -4.2249508 -4.2253509 -4.2221422][-4.2635627 -4.2493639 -4.2292948 -4.2162576 -4.2122574 -4.2135444 -4.2235708 -4.2374644 -4.244503 -4.248765 -4.25537 -4.2600369 -4.2625289 -4.265429 -4.2685075][-4.2942266 -4.2864413 -4.2754297 -4.2712212 -4.27237 -4.2754827 -4.2840791 -4.2953477 -4.2995667 -4.2984071 -4.3016162 -4.3044448 -4.30534 -4.3073387 -4.3098593][-4.3103166 -4.3082728 -4.3048296 -4.3049569 -4.3064651 -4.3079777 -4.3121676 -4.3177791 -4.3187776 -4.3162856 -4.31657 -4.3179317 -4.318078 -4.3182983 -4.3198633]]...]
INFO - root - 2017-12-06 07:08:53.179949: step 8910, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 58h:45m:57s remains)
INFO - root - 2017-12-06 07:08:59.685258: step 8920, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.639 sec/batch; 57h:25m:40s remains)
INFO - root - 2017-12-06 07:09:06.306543: step 8930, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 63h:05m:00s remains)
INFO - root - 2017-12-06 07:09:12.898981: step 8940, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 60h:02m:34s remains)
INFO - root - 2017-12-06 07:09:19.331436: step 8950, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 57h:25m:46s remains)
INFO - root - 2017-12-06 07:09:25.919013: step 8960, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 57h:58m:32s remains)
INFO - root - 2017-12-06 07:09:32.314290: step 8970, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 55h:46m:10s remains)
INFO - root - 2017-12-06 07:09:38.828526: step 8980, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 60h:23m:08s remains)
INFO - root - 2017-12-06 07:09:45.438567: step 8990, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 59h:59m:58s remains)
INFO - root - 2017-12-06 07:09:51.937177: step 9000, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 60h:12m:27s remains)
2017-12-06 07:09:52.162859: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 07:09:52.162892: E tensorflow/core/util/events_writer.cc:131] Failed to flush 24 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 07:09:52.596581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1734109 -4.1657448 -4.1601086 -4.1559496 -4.1460562 -4.1428604 -4.1436009 -4.1446857 -4.1498532 -4.1589746 -4.1645026 -4.1626291 -4.1493979 -4.126801 -4.1081052][-4.16896 -4.1571679 -4.1508446 -4.1474733 -4.1384673 -4.1314764 -4.1255927 -4.1236472 -4.1289654 -4.142395 -4.1545515 -4.1572976 -4.1458397 -4.1221323 -4.0997863][-4.160954 -4.1415391 -4.1318884 -4.1318107 -4.1280293 -4.1223221 -4.1151681 -4.1132059 -4.1196866 -4.1370158 -4.1541567 -4.1568646 -4.1414757 -4.1126289 -4.0852284][-4.1526961 -4.1283145 -4.1158242 -4.1188283 -4.1224971 -4.1198587 -4.113987 -4.1137137 -4.1231337 -4.1459112 -4.1671567 -4.1694155 -4.1500955 -4.1171589 -4.0866227][-4.12997 -4.107141 -4.0951476 -4.1003761 -4.1076279 -4.1050711 -4.0957708 -4.0950613 -4.1112986 -4.147202 -4.1785073 -4.1853619 -4.1702566 -4.142478 -4.11623][-4.0771761 -4.0650167 -4.056181 -4.0556087 -4.0564542 -4.0466337 -4.0265326 -4.0229754 -4.0526166 -4.1101213 -4.1606932 -4.180253 -4.1778255 -4.1619697 -4.1464868][-4.0206814 -4.0152521 -4.0092955 -4.0018287 -3.9923167 -3.9686127 -3.9296196 -3.9157786 -3.9596348 -4.0419335 -4.1153078 -4.1530819 -4.1653323 -4.159781 -4.1532736][-4.0275426 -4.0162029 -4.0080514 -3.9979355 -3.9831476 -3.9533174 -3.8984816 -3.8636668 -3.9000821 -3.9851544 -4.0663218 -4.1144891 -4.1349306 -4.1320696 -4.1267586][-4.0850821 -4.0727243 -4.0678072 -4.0643668 -4.0589352 -4.0444946 -4.0058017 -3.9708166 -3.9838691 -4.0365906 -4.0914283 -4.1245718 -4.1373873 -4.1281209 -4.1117144][-4.139864 -4.1307569 -4.1299281 -4.132071 -4.1353397 -4.133637 -4.1162081 -4.0974169 -4.1031475 -4.1294813 -4.1573987 -4.1753807 -4.1837463 -4.17609 -4.1555195][-4.1747584 -4.1703467 -4.17502 -4.182476 -4.1923661 -4.1970458 -4.1898432 -4.1814823 -4.1841207 -4.1937504 -4.2057471 -4.2156806 -4.2230029 -4.2200661 -4.2048078][-4.1903305 -4.188839 -4.1946206 -4.2029638 -4.2134323 -4.22156 -4.2236304 -4.2242975 -4.22665 -4.229 -4.2335706 -4.239717 -4.2452888 -4.2438684 -4.2341666][-4.2102284 -4.205687 -4.2062731 -4.2070246 -4.2084551 -4.2135425 -4.2198606 -4.224833 -4.2279181 -4.2297678 -4.2360415 -4.2451119 -4.25068 -4.2503419 -4.2450953][-4.2232933 -4.2167106 -4.2149086 -4.21113 -4.2084346 -4.2119842 -4.2189832 -4.2228823 -4.2235026 -4.2229691 -4.2263088 -4.2326074 -4.2357087 -4.2358379 -4.2342653][-4.2205291 -4.215838 -4.2163458 -4.2140369 -4.212256 -4.2165723 -4.2239017 -4.2272744 -4.2268219 -4.2261863 -4.2280006 -4.2322989 -4.2351074 -4.2363982 -4.2367363]]...]
INFO - root - 2017-12-06 07:09:59.046631: step 9010, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 59h:12m:42s remains)
INFO - root - 2017-12-06 07:10:05.665581: step 9020, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 57h:30m:39s remains)
INFO - root - 2017-12-06 07:10:12.233710: step 9030, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 58h:54m:23s remains)
INFO - root - 2017-12-06 07:10:18.835734: step 9040, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 61h:07m:48s remains)
INFO - root - 2017-12-06 07:10:25.443837: step 9050, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 57h:58m:16s remains)
INFO - root - 2017-12-06 07:10:31.748333: step 9060, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.656 sec/batch; 58h:55m:09s remains)
INFO - root - 2017-12-06 07:10:38.393714: step 9070, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 58h:41m:28s remains)
INFO - root - 2017-12-06 07:10:44.902495: step 9080, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 57h:32m:11s remains)
INFO - root - 2017-12-06 07:10:51.367959: step 9090, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 59h:25m:08s remains)
INFO - root - 2017-12-06 07:10:57.861901: step 9100, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 59h:25m:34s remains)
2017-12-06 07:10:58.571029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3248124 -4.3065243 -4.2681231 -4.2250185 -4.2028289 -4.2039928 -4.2123055 -4.2129321 -4.2106895 -4.2082815 -4.2132616 -4.22471 -4.2483888 -4.2695603 -4.2707753][-4.3249159 -4.3043642 -4.2654653 -4.22295 -4.2029433 -4.204113 -4.2177882 -4.2260485 -4.2256303 -4.223639 -4.2217326 -4.2252073 -4.2436228 -4.2632947 -4.2657166][-4.3287797 -4.3066368 -4.2681952 -4.2227149 -4.1974916 -4.1956596 -4.2097683 -4.2249684 -4.2357864 -4.2415705 -4.2371435 -4.229125 -4.2321067 -4.2432408 -4.2452159][-4.3367944 -4.3143725 -4.2752218 -4.2230864 -4.1848288 -4.172616 -4.1818104 -4.2015171 -4.2265444 -4.24426 -4.2429862 -4.2309875 -4.2232108 -4.2242146 -4.2232242][-4.3453054 -4.3237576 -4.2827549 -4.22322 -4.1693077 -4.1375456 -4.1338768 -4.1525826 -4.1896725 -4.2273693 -4.2421117 -4.2355638 -4.2234478 -4.2161269 -4.2092323][-4.3524 -4.3348651 -4.2945604 -4.2299294 -4.1582289 -4.0971689 -4.0670629 -4.0710588 -4.117662 -4.1841702 -4.2281313 -4.241704 -4.2354503 -4.2211628 -4.2087808][-4.3554325 -4.34269 -4.3069639 -4.2439275 -4.1633029 -4.0779362 -4.0098033 -3.9734564 -4.01806 -4.1138663 -4.1935382 -4.2365551 -4.2480674 -4.2393851 -4.22627][-4.3570166 -4.3485389 -4.318716 -4.263073 -4.1869845 -4.0936861 -3.9947951 -3.909034 -3.931262 -4.0427003 -4.1483746 -4.217762 -4.250031 -4.2575059 -4.2527285][-4.3592272 -4.3528185 -4.3268332 -4.2795267 -4.2171421 -4.1426339 -4.0558872 -3.9724951 -3.9699972 -4.0441771 -4.128902 -4.1958203 -4.237793 -4.2606845 -4.2656918][-4.3602729 -4.3554559 -4.3340311 -4.2944827 -4.2443142 -4.1935358 -4.137836 -4.08568 -4.0757031 -4.0988283 -4.1398983 -4.1861334 -4.2249265 -4.2523594 -4.2629523][-4.3594871 -4.3536429 -4.334012 -4.2971654 -4.2507691 -4.2151036 -4.184381 -4.1613045 -4.1572461 -4.1582742 -4.1705608 -4.1965466 -4.2280645 -4.2498274 -4.2584786][-4.35646 -4.3494983 -4.3279152 -4.2891288 -4.2438121 -4.215611 -4.1988878 -4.1884494 -4.1887875 -4.1909838 -4.2001257 -4.2165585 -4.2369103 -4.2476864 -4.2466578][-4.3531728 -4.3446217 -4.3196783 -4.277072 -4.2316804 -4.203413 -4.1873884 -4.1804905 -4.1860871 -4.2000394 -4.2195382 -4.2358389 -4.24644 -4.2446785 -4.232542][-4.3512859 -4.3412728 -4.3134003 -4.26814 -4.2231784 -4.1925626 -4.1749859 -4.1723104 -4.1876931 -4.2113161 -4.2355013 -4.2488356 -4.2531443 -4.2440042 -4.2240615][-4.3506474 -4.3401117 -4.312048 -4.2699986 -4.2327428 -4.2063122 -4.1888824 -4.189539 -4.21096 -4.2378225 -4.2550807 -4.260366 -4.2583919 -4.2455015 -4.2245808]]...]
INFO - root - 2017-12-06 07:11:05.031143: step 9110, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 57h:21m:12s remains)
INFO - root - 2017-12-06 07:11:11.602453: step 9120, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 59h:02m:05s remains)
INFO - root - 2017-12-06 07:11:18.276807: step 9130, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 61h:42m:44s remains)
INFO - root - 2017-12-06 07:11:24.858715: step 9140, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 57h:51m:54s remains)
INFO - root - 2017-12-06 07:11:31.443275: step 9150, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 64h:50m:10s remains)
INFO - root - 2017-12-06 07:11:37.900110: step 9160, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.668 sec/batch; 59h:59m:56s remains)
INFO - root - 2017-12-06 07:11:44.456334: step 9170, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 58h:40m:57s remains)
INFO - root - 2017-12-06 07:11:51.113573: step 9180, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.654 sec/batch; 58h:44m:02s remains)
INFO - root - 2017-12-06 07:11:57.689636: step 9190, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 57h:39m:57s remains)
INFO - root - 2017-12-06 07:12:04.369793: step 9200, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 59h:45m:43s remains)
2017-12-06 07:12:04.586549: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 07:12:04.586581: E tensorflow/core/util/events_writer.cc:131] Failed to flush 26 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 07:12:04.987751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.317112 -4.3087435 -4.3030515 -4.2954335 -4.2848644 -4.27868 -4.2829432 -4.2899718 -4.2931914 -4.2952104 -4.2981176 -4.3035254 -4.3050537 -4.306695 -4.3076286][-4.3093419 -4.2990818 -4.2862115 -4.2660794 -4.2439966 -4.2332678 -4.2409 -4.2537141 -4.2592559 -4.26442 -4.2711735 -4.2791934 -4.2795978 -4.2780848 -4.2759976][-4.308073 -4.2946982 -4.270257 -4.2325 -4.1968365 -4.1791077 -4.1827178 -4.1987605 -4.2145782 -4.2285514 -4.2412162 -4.2484493 -4.24967 -4.2469707 -4.2413168][-4.3076077 -4.2893486 -4.2530885 -4.2035432 -4.1588955 -4.1359611 -4.1276488 -4.1409135 -4.1681795 -4.1923594 -4.2135291 -4.2209353 -4.2215433 -4.2166362 -4.2100945][-4.3087621 -4.28586 -4.2421937 -4.1862192 -4.1379128 -4.1029682 -4.0674744 -4.0709596 -4.1130705 -4.1532679 -4.1850352 -4.1917343 -4.1857162 -4.1790967 -4.1743426][-4.31027 -4.2872882 -4.2390003 -4.1813197 -4.1286712 -4.0733929 -3.9991434 -3.9813619 -4.0413995 -4.1017947 -4.1454396 -4.157619 -4.1493878 -4.1423364 -4.1419058][-4.308845 -4.2849569 -4.2327137 -4.1700363 -4.1091638 -4.0290728 -3.908725 -3.8508372 -3.9241354 -4.0233321 -4.0886 -4.1131511 -4.1102328 -4.1080675 -4.117043][-4.3025088 -4.2790146 -4.224226 -4.1554623 -4.0798264 -3.970042 -3.8013825 -3.6939692 -3.7883193 -3.9426847 -4.0363407 -4.0761185 -4.0812745 -4.0844989 -4.1010494][-4.2925968 -4.266479 -4.209806 -4.1345477 -4.0482292 -3.9270182 -3.7604446 -3.6637459 -3.7706881 -3.9327872 -4.0264907 -4.067441 -4.0719233 -4.0767446 -4.0948114][-4.2812238 -4.2583551 -4.2088041 -4.1424794 -4.0691938 -3.9722283 -3.8613923 -3.8214226 -3.9011312 -4.0065436 -4.0662942 -4.0903034 -4.0870028 -4.0873184 -4.1030693][-4.2757735 -4.2589221 -4.2209392 -4.1743073 -4.1249847 -4.0537553 -3.982414 -3.9690905 -4.0179291 -4.0690846 -4.0972109 -4.1121783 -4.1120944 -4.1125088 -4.1251321][-4.2753358 -4.2664104 -4.2473307 -4.2193995 -4.179842 -4.12221 -4.07179 -4.0698824 -4.0955944 -4.1137247 -4.1243348 -4.1360726 -4.1467085 -4.1523323 -4.1605697][-4.28243 -4.2783303 -4.2692451 -4.2495093 -4.2173576 -4.1751566 -4.1429043 -4.1481462 -4.159061 -4.15901 -4.1558352 -4.1660995 -4.1842542 -4.1942019 -4.1989827][-4.300458 -4.2987061 -4.2926164 -4.2795877 -4.2581725 -4.2333369 -4.2177143 -4.2217007 -4.2222767 -4.2136478 -4.204968 -4.2122164 -4.2320724 -4.2412934 -4.2397766][-4.3184023 -4.317903 -4.3150134 -4.30906 -4.2949953 -4.2818465 -4.2786541 -4.2833691 -4.282115 -4.2736149 -4.2644315 -4.2650642 -4.2788544 -4.2862759 -4.2799411]]...]
INFO - root - 2017-12-06 07:12:11.454365: step 9210, loss = 2.04, batch loss = 1.98 (15.4 examples/sec; 0.518 sec/batch; 46h:31m:34s remains)
INFO - root - 2017-12-06 07:12:17.991924: step 9220, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 58h:13m:13s remains)
INFO - root - 2017-12-06 07:12:24.430067: step 9230, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 58h:58m:03s remains)
INFO - root - 2017-12-06 07:12:30.976416: step 9240, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.646 sec/batch; 58h:01m:28s remains)
INFO - root - 2017-12-06 07:12:37.543111: step 9250, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 59h:12m:39s remains)
INFO - root - 2017-12-06 07:12:44.075479: step 9260, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 61h:44m:37s remains)
INFO - root - 2017-12-06 07:12:50.661739: step 9270, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 58h:34m:52s remains)
INFO - root - 2017-12-06 07:12:57.234330: step 9280, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.681 sec/batch; 61h:08m:08s remains)
INFO - root - 2017-12-06 07:13:03.820724: step 9290, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 63h:06m:24s remains)
INFO - root - 2017-12-06 07:13:10.476153: step 9300, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 58h:30m:54s remains)
2017-12-06 07:13:11.141022: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1885252 -4.16563 -4.1454854 -4.1270294 -4.1211486 -4.1271935 -4.1432638 -4.1712418 -4.1954365 -4.2046914 -4.2011447 -4.1867857 -4.1661935 -4.1542697 -4.1678896][-4.1554513 -4.1302896 -4.10805 -4.0868111 -4.0839963 -4.0963902 -4.1172667 -4.14677 -4.1720462 -4.1807933 -4.1794853 -4.1718917 -4.1594672 -4.1550903 -4.1731853][-4.1142645 -4.0900979 -4.0702744 -4.0528398 -4.0558186 -4.0730228 -4.0936294 -4.1190228 -4.1433887 -4.1542506 -4.1572275 -4.1536736 -4.1479874 -4.1499662 -4.1713457][-4.0988064 -4.0777531 -4.0628095 -4.0510664 -4.0571446 -4.0732846 -4.0871954 -4.1054788 -4.1309505 -4.1464596 -4.1530714 -4.1539731 -4.1518226 -4.1537056 -4.1724663][-4.09281 -4.0743008 -4.0613623 -4.0515075 -4.0539441 -4.0636506 -4.0675807 -4.0796304 -4.1081858 -4.1286964 -4.1394711 -4.1474237 -4.1498208 -4.1528883 -4.1716471][-4.0714054 -4.0482 -4.0321727 -4.0206838 -4.0202394 -4.0277371 -4.029438 -4.0428743 -4.0775957 -4.1043868 -4.1206532 -4.133553 -4.1394358 -4.1451216 -4.1674862][-4.0412078 -4.0129437 -3.9918976 -3.9776733 -3.9798234 -3.9923925 -4.0023904 -4.0235219 -4.0627289 -4.0939431 -4.1130719 -4.1254759 -4.1317978 -4.140079 -4.1672273][-4.0190892 -3.9931207 -3.9711182 -3.9561899 -3.9603221 -3.9768817 -3.9987986 -4.0300703 -4.07071 -4.1033635 -4.1221948 -4.1306758 -4.1355319 -4.1451244 -4.1729164][-4.0295749 -4.0121083 -3.9958513 -3.9821093 -3.9842641 -3.9981141 -4.0253363 -4.0630465 -4.10311 -4.1364098 -4.1544685 -4.1579475 -4.1572943 -4.1611824 -4.1831751][-4.0536733 -4.0415616 -4.0272121 -4.0122814 -4.0116148 -4.0221872 -4.0483646 -4.0881333 -4.1285334 -4.1641607 -4.1848474 -4.1861987 -4.1805296 -4.1790266 -4.1946855][-4.0758047 -4.0665088 -4.0526309 -4.0379577 -4.0363007 -4.0447702 -4.0674295 -4.1037693 -4.1413126 -4.1772194 -4.2004709 -4.2027745 -4.1940184 -4.1901031 -4.2023497][-4.0843344 -4.0795097 -4.0709143 -4.0598469 -4.0585847 -4.0632696 -4.0765829 -4.103745 -4.1367879 -4.1705608 -4.1950083 -4.1980577 -4.1897411 -4.187747 -4.2007952][-4.0794067 -4.0730453 -4.0659909 -4.0587463 -4.0560455 -4.052597 -4.0523319 -4.0683336 -4.0971084 -4.1301827 -4.1600823 -4.1699891 -4.1674371 -4.1712513 -4.1889062][-4.0783334 -4.0670328 -4.0567522 -4.048265 -4.0436616 -4.0371017 -4.0293846 -4.0360327 -4.0598483 -4.0920758 -4.1263623 -4.1418624 -4.1438341 -4.1517191 -4.1726952][-4.0854731 -4.0759468 -4.06624 -4.0598297 -4.0592 -4.0586209 -4.0532088 -4.0568843 -4.0782528 -4.107738 -4.1389108 -4.1501718 -4.1461043 -4.1466146 -4.1635118]]...]
INFO - root - 2017-12-06 07:13:17.690512: step 9310, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 59h:40m:04s remains)
INFO - root - 2017-12-06 07:13:24.247972: step 9320, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:41m:30s remains)
INFO - root - 2017-12-06 07:13:30.938776: step 9330, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 57h:51m:36s remains)
INFO - root - 2017-12-06 07:13:37.542045: step 9340, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 60h:16m:41s remains)
INFO - root - 2017-12-06 07:13:44.055047: step 9350, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 57h:41m:24s remains)
INFO - root - 2017-12-06 07:13:50.399936: step 9360, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 58h:47m:36s remains)
INFO - root - 2017-12-06 07:13:56.926495: step 9370, loss = 2.09, batch loss = 2.04 (12.2 examples/sec; 0.656 sec/batch; 58h:52m:20s remains)
INFO - root - 2017-12-06 07:14:03.552682: step 9380, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.703 sec/batch; 63h:06m:25s remains)
INFO - root - 2017-12-06 07:14:10.099185: step 9390, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 57h:42m:45s remains)
INFO - root - 2017-12-06 07:14:16.594709: step 9400, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 57h:57m:01s remains)
2017-12-06 07:14:16.810166: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 07:14:16.810206: E tensorflow/core/util/events_writer.cc:131] Failed to flush 28 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 07:14:17.188305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2746124 -4.2691579 -4.2706227 -4.2757788 -4.2784081 -4.2780228 -4.2740707 -4.2701283 -4.2665515 -4.2709379 -4.2788892 -4.2832122 -4.2911148 -4.2978745 -4.3016739][-4.2449374 -4.2360811 -4.2362227 -4.2402768 -4.243567 -4.2414155 -4.2334876 -4.2297664 -4.2283959 -4.2372661 -4.2477083 -4.2512336 -4.2617793 -4.2738633 -4.2834244][-4.2113662 -4.1962748 -4.196116 -4.1987982 -4.2013211 -4.1932316 -4.1776037 -4.179184 -4.182744 -4.1947231 -4.2073908 -4.2069273 -4.2199421 -4.2411427 -4.258152][-4.1802392 -4.1608815 -4.1634884 -4.1664038 -4.167603 -4.1481948 -4.1199837 -4.1309414 -4.1444378 -4.1559048 -4.1672559 -4.160789 -4.1736646 -4.2058096 -4.2334104][-4.1612988 -4.1446223 -4.1503463 -4.1485682 -4.1363859 -4.0944033 -4.0510716 -4.0721769 -4.102675 -4.1182833 -4.1285715 -4.1203928 -4.1356173 -4.1773186 -4.2119665][-4.1508517 -4.1392879 -4.1471257 -4.134037 -4.0969443 -4.0255032 -3.9645526 -3.9993045 -4.05815 -4.0896354 -4.102499 -4.0966272 -4.1208787 -4.1691828 -4.2081394][-4.1607661 -4.1496086 -4.1530066 -4.1218624 -4.0539241 -3.9484856 -3.8645661 -3.9211872 -4.0140414 -4.0654087 -4.0845075 -4.086422 -4.120616 -4.1754127 -4.2160482][-4.1888237 -4.1722479 -4.1613693 -4.1120334 -4.0178757 -3.88126 -3.7760696 -3.8585937 -3.9817855 -4.0473771 -4.072495 -4.08006 -4.11849 -4.1757951 -4.218401][-4.22442 -4.1991463 -4.1711726 -4.1124468 -4.0129161 -3.8762286 -3.78197 -3.877008 -4.003211 -4.0651383 -4.0891337 -4.0984335 -4.1366606 -4.1895666 -4.2235336][-4.2416658 -4.2085052 -4.1733871 -4.1193018 -4.0371194 -3.9353919 -3.8789604 -3.963465 -4.0630918 -4.1087303 -4.1232357 -4.1281085 -4.156755 -4.2003746 -4.2220316][-4.2298 -4.1913271 -4.1578145 -4.11792 -4.0625691 -3.999649 -3.9728558 -4.0342808 -4.10375 -4.1371779 -4.1432848 -4.1439457 -4.1635609 -4.1975403 -4.2054114][-4.2077808 -4.1664004 -4.1409721 -4.1218586 -4.095046 -4.0629015 -4.0472116 -4.0818167 -4.1277246 -4.1575727 -4.16354 -4.158927 -4.1680074 -4.1906958 -4.1884689][-4.193553 -4.1524487 -4.1358066 -4.1375003 -4.1345243 -4.1172209 -4.0996523 -4.1110539 -4.1392641 -4.1672544 -4.1763539 -4.1717324 -4.1721268 -4.1843553 -4.1812029][-4.1748061 -4.1364121 -4.1312742 -4.1522756 -4.1663647 -4.1532984 -4.1307793 -4.1332111 -4.153481 -4.1796741 -4.193347 -4.1931663 -4.1907096 -4.1940989 -4.1977162][-4.1535764 -4.1202536 -4.1290016 -4.16236 -4.1848464 -4.17872 -4.1584382 -4.1627116 -4.1818414 -4.203464 -4.2183766 -4.2223468 -4.220099 -4.2262912 -4.2389903]]...]
INFO - root - 2017-12-06 07:14:23.781066: step 9410, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.663 sec/batch; 59h:28m:11s remains)
INFO - root - 2017-12-06 07:14:30.263189: step 9420, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 60h:28m:02s remains)
INFO - root - 2017-12-06 07:14:36.773112: step 9430, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 56h:53m:49s remains)
INFO - root - 2017-12-06 07:14:43.268169: step 9440, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 60h:10m:50s remains)
INFO - root - 2017-12-06 07:14:49.575375: step 9450, loss = 2.07, batch loss = 2.01 (13.8 examples/sec; 0.578 sec/batch; 51h:50m:09s remains)
INFO - root - 2017-12-06 07:14:56.055890: step 9460, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 58h:59m:23s remains)
INFO - root - 2017-12-06 07:15:02.695203: step 9470, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 58h:42m:32s remains)
INFO - root - 2017-12-06 07:15:09.312083: step 9480, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 58h:49m:57s remains)
INFO - root - 2017-12-06 07:15:15.803796: step 9490, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.664 sec/batch; 59h:32m:11s remains)
INFO - root - 2017-12-06 07:15:22.369805: step 9500, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 60h:29m:47s remains)
2017-12-06 07:15:23.063088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2108727 -4.2009988 -4.2110977 -4.2247243 -4.2271881 -4.219873 -4.2012281 -4.17288 -4.1553144 -4.1479645 -4.1461406 -4.1486454 -4.1556115 -4.1772985 -4.2034693][-4.2210932 -4.2151179 -4.2285857 -4.2468548 -4.2543912 -4.2514491 -4.2330589 -4.2054586 -4.1877627 -4.1778507 -4.1719995 -4.1715817 -4.1751332 -4.1919451 -4.2149019][-4.2285767 -4.2257323 -4.2410903 -4.2607465 -4.2696619 -4.2688184 -4.252512 -4.2262049 -4.2144446 -4.2091432 -4.2038078 -4.20115 -4.1996822 -4.2069383 -4.2236004][-4.2296896 -4.2331042 -4.248168 -4.26537 -4.2686009 -4.2612181 -4.2397132 -4.211544 -4.207211 -4.2152138 -4.2193108 -4.2169719 -4.2089024 -4.2054667 -4.2155328][-4.2206855 -4.2322879 -4.2491684 -4.26033 -4.2552953 -4.2367768 -4.201889 -4.1682439 -4.1721187 -4.1974192 -4.2134442 -4.2145147 -4.2022152 -4.1898 -4.194644][-4.2089672 -4.2252417 -4.244741 -4.2499423 -4.2336125 -4.199791 -4.1474028 -4.1070085 -4.1176257 -4.1595173 -4.1879463 -4.1938515 -4.1818743 -4.1652093 -4.1657872][-4.1988196 -4.2152987 -4.2357674 -4.2395239 -4.2163882 -4.17206 -4.1078458 -4.0610924 -4.0746408 -4.1244092 -4.1603875 -4.1720991 -4.16266 -4.1456237 -4.1420918][-4.1911364 -4.2067909 -4.2269468 -4.2330365 -4.2107573 -4.1660523 -4.1005721 -4.0552506 -4.0672474 -4.111506 -4.1455517 -4.1611738 -4.1553373 -4.1413417 -4.1371312][-4.1770372 -4.1912742 -4.2103276 -4.21788 -4.1980162 -4.1581941 -4.1008124 -4.0668607 -4.0786572 -4.1114206 -4.1380544 -4.1542912 -4.1541719 -4.1476889 -4.1474013][-4.1620293 -4.1763792 -4.193819 -4.2011781 -4.1843462 -4.1512489 -4.1074572 -4.0879312 -4.09923 -4.11897 -4.1356745 -4.1498523 -4.1582007 -4.1624436 -4.166604][-4.1539183 -4.1695576 -4.1894007 -4.197772 -4.1838202 -4.1559715 -4.1211481 -4.10969 -4.1191382 -4.131494 -4.1435485 -4.1588669 -4.1745758 -4.185194 -4.1895394][-4.1512861 -4.1710696 -4.1958823 -4.206687 -4.196363 -4.1707554 -4.1411667 -4.1343737 -4.1446557 -4.15316 -4.16196 -4.1766238 -4.1929207 -4.2038927 -4.2067885][-4.1506963 -4.1722732 -4.1987514 -4.2112761 -4.2051806 -4.1856909 -4.1666541 -4.1673851 -4.1766829 -4.180665 -4.184989 -4.1937671 -4.2047224 -4.2109904 -4.2107739][-4.15334 -4.1706333 -4.1956348 -4.2110343 -4.2118306 -4.1992345 -4.1888566 -4.1936917 -4.1989808 -4.1996193 -4.1995454 -4.198741 -4.1984062 -4.1959758 -4.1913261][-4.1556592 -4.1704841 -4.1943011 -4.21174 -4.2182522 -4.2117176 -4.2073412 -4.2123089 -4.2106161 -4.205574 -4.2033005 -4.1968012 -4.1867609 -4.1759377 -4.1675482]]...]
INFO - root - 2017-12-06 07:15:29.555868: step 9510, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.660 sec/batch; 59h:15m:11s remains)
INFO - root - 2017-12-06 07:15:36.117557: step 9520, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 60h:26m:51s remains)
INFO - root - 2017-12-06 07:15:42.555846: step 9530, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 57h:40m:59s remains)
INFO - root - 2017-12-06 07:15:49.044114: step 9540, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 57h:46m:27s remains)
INFO - root - 2017-12-06 07:15:55.421789: step 9550, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 60h:15m:14s remains)
INFO - root - 2017-12-06 07:16:02.087532: step 9560, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.663 sec/batch; 59h:30m:28s remains)
INFO - root - 2017-12-06 07:16:08.558463: step 9570, loss = 2.03, batch loss = 1.97 (12.3 examples/sec; 0.652 sec/batch; 58h:30m:05s remains)
INFO - root - 2017-12-06 07:16:14.965485: step 9580, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 58h:43m:37s remains)
INFO - root - 2017-12-06 07:16:21.414622: step 9590, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 57h:55m:29s remains)
INFO - root - 2017-12-06 07:16:28.008231: step 9600, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 59h:56m:57s remains)
2017-12-06 07:16:28.222052: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116 has disappeared.
2017-12-06 07:16:28.222090: E tensorflow/core/util/events_writer.cc:131] Failed to flush 30 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian/events.out.tfevents.1512537669.GCRAZGDL116
2017-12-06 07:16:28.643434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3002396 -4.3018131 -4.3062086 -4.3024569 -4.2875962 -4.2614727 -4.2217345 -4.1880803 -4.176569 -4.1742492 -4.1663156 -4.1502094 -4.134243 -4.0934386 -4.0365548][-4.2788992 -4.2821393 -4.2915058 -4.2921777 -4.2784119 -4.249043 -4.208425 -4.1783371 -4.1730752 -4.1743355 -4.168663 -4.1557145 -4.1502523 -4.1155224 -4.0623093][-4.2518997 -4.2587619 -4.2751865 -4.2828259 -4.2715478 -4.2413864 -4.206511 -4.1861939 -4.1902704 -4.197361 -4.19429 -4.1899838 -4.1974869 -4.1812038 -4.1458879][-4.2046208 -4.2149868 -4.2393012 -4.2573843 -4.2562242 -4.2318745 -4.2100043 -4.2007027 -4.2123971 -4.2256064 -4.2259564 -4.2266154 -4.2419691 -4.2423277 -4.2243552][-4.1460605 -4.1575255 -4.1874461 -4.2187195 -4.2312255 -4.2168913 -4.2061691 -4.2030973 -4.2157841 -4.2322 -4.2373357 -4.2433238 -4.263659 -4.276298 -4.2721891][-4.0954118 -4.1076169 -4.1390023 -4.17727 -4.2012582 -4.196044 -4.1903825 -4.1868973 -4.1985412 -4.219337 -4.2297945 -4.24056 -4.2678533 -4.288846 -4.2944765][-4.0901618 -4.09744 -4.1162934 -4.1424313 -4.1634531 -4.1609955 -4.1549125 -4.1488385 -4.159565 -4.18773 -4.2075343 -4.2251549 -4.2595024 -4.2856569 -4.2975345][-4.1195092 -4.1174369 -4.1156363 -4.1178761 -4.1203256 -4.10407 -4.0842447 -4.073051 -4.085866 -4.1271219 -4.1633778 -4.1944404 -4.2405696 -4.2751718 -4.2912617][-4.1428509 -4.128377 -4.1096897 -4.0866332 -4.0600085 -4.0202818 -3.9831653 -3.9685605 -3.9865887 -4.0455155 -4.1076345 -4.1557522 -4.2120333 -4.254261 -4.2767143][-4.1680512 -4.1455069 -4.1156816 -4.0779462 -4.033 -3.9862201 -3.9522552 -3.9456384 -3.9658637 -4.0273995 -4.0996275 -4.1498957 -4.1970367 -4.2353745 -4.2608023][-4.1863723 -4.1621904 -4.1330481 -4.0964384 -4.0587111 -4.0281243 -4.0108738 -4.0171146 -4.0322471 -4.0754395 -4.1369338 -4.1773911 -4.2058496 -4.2325511 -4.2528772][-4.191153 -4.1693258 -4.150012 -4.1264124 -4.1050386 -4.0957723 -4.0899243 -4.1014633 -4.1130729 -4.1402779 -4.1871114 -4.2177677 -4.2319875 -4.2455597 -4.257061][-4.204288 -4.1845613 -4.1771812 -4.1698413 -4.164927 -4.1716976 -4.1776643 -4.1885247 -4.1966362 -4.2138348 -4.2447562 -4.2636976 -4.2689342 -4.2726912 -4.2736363][-4.2421522 -4.22618 -4.2290487 -4.235023 -4.23903 -4.2524471 -4.2627807 -4.2712784 -4.2763205 -4.2866178 -4.3020625 -4.3073988 -4.3047123 -4.3002615 -4.2916188][-4.2906175 -4.2807932 -4.28929 -4.3003669 -4.3076191 -4.3209853 -4.3264909 -4.327487 -4.326736 -4.3281612 -4.3319654 -4.3292484 -4.3225045 -4.3136263 -4.2988129]]...]
INFO - root - 2017-12-06 07:16:35.200414: step 9610, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:19m:50s remains)
INFO - root - 2017-12-06 07:16:41.649516: step 9620, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 57h:54m:28s remains)
INFO - root - 2017-12-06 07:16:47.881151: step 9630, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 56h:39m:41s remains)
INFO - root - 2017-12-06 07:16:54.316929: step 9640, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 58h:36m:56s remains)
INFO - root - 2017-12-06 07:17:00.703174: step 9650, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 57h:45m:45s remains)
INFO - root - 2017-12-06 07:17:07.275366: step 9660, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 59h:17m:58s remains)
INFO - root - 2017-12-06 07:17:13.809688: step 9670, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.613 sec/batch; 54h:57m:26s remains)
INFO - root - 2017-12-06 07:17:20.416720: step 9680, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 58h:53m:40s remains)
INFO - root - 2017-12-06 07:17:26.888926: step 9690, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 57h:46m:43s remains)
INFO - root - 2017-12-06 07:17:33.503227: step 9700, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 58h:55m:02s remains)
2017-12-06 07:17:34.159461: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2872286 -4.2986875 -4.3017316 -4.299232 -4.2989116 -4.3041897 -4.3093977 -4.3051138 -4.3008914 -4.3008809 -4.3058286 -4.3158116 -4.323647 -4.3246727 -4.321219][-4.2739353 -4.2877245 -4.289115 -4.2856894 -4.2822843 -4.2881837 -4.2929864 -4.2799954 -4.2706852 -4.2747469 -4.2871165 -4.3063068 -4.3229165 -4.3259315 -4.3214684][-4.2579832 -4.2743354 -4.2716789 -4.2649689 -4.2542081 -4.2581267 -4.2597752 -4.2371793 -4.2267108 -4.2400351 -4.2631359 -4.2930737 -4.3172855 -4.3215384 -4.315855][-4.2467904 -4.2639284 -4.2569757 -4.2436914 -4.2262015 -4.2221212 -4.2113376 -4.1724877 -4.1607847 -4.1885009 -4.227468 -4.272121 -4.3049445 -4.312808 -4.3072162][-4.2371268 -4.2545052 -4.2433462 -4.2218995 -4.1942706 -4.181066 -4.1486859 -4.0846796 -4.0721841 -4.1221113 -4.1804729 -4.241632 -4.2850857 -4.301775 -4.2994304][-4.2416663 -4.25555 -4.2385035 -4.2112112 -4.1755047 -4.1498766 -4.089242 -3.9895892 -3.9772179 -4.0522013 -4.1296082 -4.2069874 -4.2627144 -4.2899327 -4.2936177][-4.2582111 -4.2676077 -4.2467256 -4.2144256 -4.1703334 -4.1270967 -4.027101 -3.8804054 -3.8773072 -3.9923773 -4.0942569 -4.1846061 -4.2488279 -4.2810812 -4.2901692][-4.2725449 -4.2795367 -4.2653069 -4.2331228 -4.17551 -4.0984554 -3.9405322 -3.7311687 -3.7521105 -3.9355388 -4.0698667 -4.1734118 -4.2435656 -4.2801361 -4.2910981][-4.2603173 -4.2669454 -4.2671561 -4.2456923 -4.1796689 -4.0721812 -3.8689473 -3.6142726 -3.66953 -3.909982 -4.06654 -4.1744537 -4.24545 -4.2833118 -4.293376][-4.2399874 -4.2449794 -4.257339 -4.2489071 -4.1877046 -4.0798206 -3.8979218 -3.695868 -3.7581553 -3.967798 -4.0991683 -4.1904263 -4.2533693 -4.289165 -4.2970757][-4.2256231 -4.2303996 -4.2511721 -4.255384 -4.2061892 -4.1120081 -3.9642684 -3.8205338 -3.8792593 -4.0393581 -4.1402974 -4.2142453 -4.2692513 -4.2978854 -4.3012881][-4.210568 -4.2155628 -4.2448139 -4.2587261 -4.225 -4.1506081 -4.0349684 -3.9309657 -3.9790592 -4.0955291 -4.1755795 -4.2366071 -4.2848787 -4.3062615 -4.306469][-4.1958537 -4.2043877 -4.2423697 -4.2653255 -4.2519932 -4.2066264 -4.12461 -4.0504355 -4.0828147 -4.159832 -4.217607 -4.2659273 -4.3038816 -4.3158216 -4.3125682][-4.1987638 -4.2098584 -4.251967 -4.2799854 -4.2841072 -4.2634697 -4.2108903 -4.1584215 -4.1745381 -4.2194266 -4.2613225 -4.2970285 -4.323235 -4.3249993 -4.317533][-4.2062016 -4.2200766 -4.2612991 -4.2892113 -4.3009939 -4.2948055 -4.2614193 -4.22472 -4.2319555 -4.2601519 -4.2906814 -4.3145075 -4.3310518 -4.3271594 -4.3180561]]...]
INFO - root - 2017-12-06 07:17:40.767989: step 9710, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 57h:58m:33s remains)
