INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "171"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip-zeroinit-from-scratch
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-09 06:17:49.527068: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:17:49.527102: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:17:49.527108: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:17:49.527113: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:17:49.527117: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-09 06:17:56.549083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 6.38GiB
2017-12-09 06:17:56.549122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-09 06:17:56.549128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-09 06:17:56.549136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-09 06:18:19.548924: step 0, loss = 0.90, batch loss = 0.69 (0.5 examples/sec; 16.086 sec/batch; 1485h:44m:23s remains)
2017-12-09 06:18:20.488345: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00012153334 0.00012217792 0.00012449989 0.00012523022 0.00012922173 0.00013485733 0.00013652735 0.00013475379 0.00013620393 0.00013561279 0.0001346358 0.00013567765 0.00013891493 0.00013822576 0.00013462575][0.00012416342 0.000126547 0.00013036176 0.00013348898 0.00013639094 0.00013526964 0.00014000766 0.00015338454 0.00016499725 0.00016649472 0.00016762673 0.00016256738 0.00015745596 0.00014789193 0.00014197067][0.00012579182 0.00012964924 0.00013727871 0.00014056888 0.0001391097 0.00013944227 0.00016082342 0.00019740811 0.00022412965 0.00022576854 0.00022354758 0.00021043602 0.00019053354 0.00016138051 0.00014573714][0.00012908896 0.0001336937 0.00014230708 0.00014565939 0.00015008367 0.00017544976 0.000225103 0.00028926056 0.00031373941 0.00031702203 0.00030460683 0.00026586992 0.00023141455 0.00018286012 0.00015447663][0.00013650778 0.00013811309 0.000143476 0.00015281435 0.0001840421 0.00022950412 0.00030701785 0.00039212315 0.00041621865 0.00039387093 0.00035973798 0.00030603856 0.00025829082 0.00020274437 0.00016597318][0.00015036017 0.00014808161 0.00015222571 0.00017776933 0.000225516 0.00029229009 0.00039409703 0.0005184242 0.00055225287 0.00048665292 0.000409169 0.00033799125 0.00026956809 0.00021082762 0.00017232771][0.00015715802 0.00015183046 0.00015862311 0.00019386043 0.00025724975 0.0003473059 0.0005004757 0.00070703228 0.00070584862 0.00055709342 0.00043498541 0.00033212986 0.00025017769 0.00019625814 0.00016694523][0.00015792671 0.00015639699 0.00016903206 0.00020440105 0.00027332443 0.00036504626 0.00051008788 0.00069319585 0.00064307539 0.00049625279 0.0003855227 0.00028906093 0.0002139509 0.00017614712 0.00015406674][0.00015617364 0.0001587537 0.00017438005 0.0002098378 0.0002613721 0.00033342111 0.0004204929 0.00046871614 0.0004119013 0.00034050408 0.0002815625 0.000221467 0.00017711619 0.00015922387 0.00014622617][0.00015448048 0.00016163659 0.00017920158 0.00021463515 0.00024708064 0.00029451514 0.00033808791 0.00032105279 0.00027402685 0.00024099213 0.00020939097 0.00018375178 0.00016246643 0.00015604075 0.00014665088][0.00014606086 0.00015423224 0.00017349898 0.00020055726 0.0002182864 0.00024373121 0.00025390566 0.00023245795 0.00019948813 0.00017935131 0.00016985537 0.00016437336 0.00015792968 0.00015735271 0.00014996235][0.0001476928 0.00015700978 0.00017407328 0.00018794386 0.00019334958 0.0002042321 0.00020970609 0.00020060269 0.0001799298 0.0001630606 0.0001553539 0.0001536238 0.00015276713 0.0001556825 0.00014871424][0.00015021565 0.00015632711 0.00016375723 0.00016672746 0.00017307037 0.00018356503 0.00018748197 0.00018358287 0.0001715362 0.00015838399 0.00015301195 0.00014969117 0.00014829361 0.00015228087 0.00014442562][0.00015431279 0.00015829758 0.00016015348 0.00016036072 0.00016510455 0.00017279536 0.00017580205 0.00017367245 0.00016551126 0.00015571601 0.00014990914 0.00015237577 0.00015068553 0.00015151432 0.00014089218][0.00015692686 0.00016036218 0.00016214592 0.00016230291 0.00016340267 0.0001662123 0.00016682762 0.0001690561 0.0001673934 0.00015741402 0.0001501501 0.00015247881 0.00015098996 0.00014806009 0.00014067264]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip-zeroinit-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip-zeroinit-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 06:18:29.173184: step 10, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:29m:00s remains)
INFO - root - 2017-12-09 06:18:35.890716: step 20, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.694 sec/batch; 64h:06m:17s remains)
INFO - root - 2017-12-09 06:18:42.545853: step 30, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.674 sec/batch; 62h:17m:27s remains)
INFO - root - 2017-12-09 06:18:49.448477: step 40, loss = 0.90, batch loss = 0.69 (11.4 examples/sec; 0.700 sec/batch; 64h:36m:13s remains)
INFO - root - 2017-12-09 06:18:56.137204: step 50, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.671 sec/batch; 62h:00m:14s remains)
INFO - root - 2017-12-09 06:19:02.735629: step 60, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:55m:48s remains)
INFO - root - 2017-12-09 06:19:09.138424: step 70, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:28m:41s remains)
INFO - root - 2017-12-09 06:19:15.766701: step 80, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.635 sec/batch; 58h:35m:52s remains)
INFO - root - 2017-12-09 06:19:22.525821: step 90, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.673 sec/batch; 62h:07m:09s remains)
INFO - root - 2017-12-09 06:19:28.968567: step 100, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.643 sec/batch; 59h:24m:16s remains)
2017-12-09 06:19:29.806681: I tensorflow/core/kernels/logging_ops.cc:79] [[[6.0232116e-05 6.419405e-05 6.87306e-05 7.240058e-05 7.5183554e-05 7.5521246e-05 7.5834389e-05 7.7348312e-05 7.8650904e-05 8.0328427e-05 7.9377183e-05 7.6836463e-05 7.4278367e-05 7.0793525e-05 6.8868088e-05][6.0948565e-05 6.4438653e-05 7.0055328e-05 7.5266158e-05 7.79127e-05 7.8967591e-05 8.0029138e-05 8.1547987e-05 8.227041e-05 8.2860373e-05 8.126178e-05 7.8721641e-05 7.5644042e-05 7.2744784e-05 7.1754061e-05][6.0368548e-05 6.2987419e-05 6.7527006e-05 7.2023242e-05 7.4694675e-05 7.6972668e-05 7.9200589e-05 8.0381804e-05 8.1329672e-05 8.0565791e-05 7.8379613e-05 7.6434386e-05 7.389755e-05 7.2304269e-05 7.2519026e-05][6.0077233e-05 6.2586019e-05 6.593181e-05 6.9661924e-05 7.3083618e-05 7.5893215e-05 7.7525976e-05 7.7742734e-05 7.7267672e-05 7.6602635e-05 7.5150114e-05 7.4084041e-05 7.3596668e-05 7.2452705e-05 7.2306779e-05][5.9467297e-05 6.2961692e-05 6.5985565e-05 6.9821908e-05 7.30458e-05 7.6007869e-05 7.7205521e-05 7.628672e-05 7.5901611e-05 7.5181655e-05 7.3166659e-05 7.2840674e-05 7.2747614e-05 7.15849e-05 7.0066948e-05][5.9154219e-05 6.2974053e-05 6.7759865e-05 7.1610026e-05 7.3969684e-05 7.52964e-05 7.6489159e-05 7.73101e-05 7.7619392e-05 7.5842545e-05 7.2542738e-05 7.13807e-05 7.1043381e-05 6.94514e-05 6.6815162e-05][5.8586513e-05 6.1489773e-05 6.6136265e-05 7.0420007e-05 7.2765411e-05 7.3260337e-05 7.3990552e-05 7.6834331e-05 7.8895646e-05 7.5634125e-05 7.08643e-05 6.8303249e-05 6.64953e-05 6.5065586e-05 6.1313105e-05][5.8132311e-05 6.0592778e-05 6.4009335e-05 6.78372e-05 7.0815979e-05 7.1214141e-05 7.1365095e-05 7.3235518e-05 7.5407144e-05 7.296502e-05 6.9597925e-05 6.6624096e-05 6.3934807e-05 6.0661347e-05 5.6393343e-05][5.7946389e-05 5.9950355e-05 6.2783882e-05 6.57015e-05 6.8292495e-05 6.8758229e-05 6.8804336e-05 6.8556496e-05 6.8890906e-05 6.9508322e-05 6.7952788e-05 6.6255037e-05 6.3914325e-05 6.0128492e-05 5.4087119e-05][5.7927296e-05 5.9136029e-05 6.1576204e-05 6.440065e-05 6.7406771e-05 6.8532376e-05 6.910869e-05 6.9438589e-05 6.908823e-05 6.9354981e-05 6.8084948e-05 6.6997658e-05 6.5222572e-05 6.3030719e-05 5.7857353e-05][5.7889272e-05 5.9213809e-05 6.1113191e-05 6.3310479e-05 6.6420711e-05 6.9488371e-05 7.194106e-05 7.1770817e-05 7.0223905e-05 6.9219459e-05 6.8703142e-05 6.7943118e-05 6.722418e-05 6.7111039e-05 6.3499327e-05][5.7804857e-05 5.9139449e-05 6.119036e-05 6.2652252e-05 6.4902051e-05 6.7940528e-05 7.0789087e-05 7.0038252e-05 6.8170484e-05 6.6871486e-05 6.6293294e-05 6.6909823e-05 6.7064961e-05 6.78452e-05 6.5768159e-05][5.8029218e-05 5.8885613e-05 6.093306e-05 6.2560161e-05 6.3351836e-05 6.4291991e-05 6.5697073e-05 6.5794862e-05 6.5052729e-05 6.449371e-05 6.4257118e-05 6.6090506e-05 6.7216468e-05 6.7571826e-05 6.6595872e-05][5.7916906e-05 5.8316429e-05 5.9552498e-05 6.1706873e-05 6.2456995e-05 6.2011139e-05 6.2151339e-05 6.3026688e-05 6.332856e-05 6.37935e-05 6.4316315e-05 6.6442408e-05 6.7857531e-05 6.7167588e-05 6.6295142e-05][5.8015568e-05 5.8473503e-05 5.9156926e-05 6.0462065e-05 6.17263e-05 6.16346e-05 6.1492814e-05 6.2263382e-05 6.250415e-05 6.2506355e-05 6.2836167e-05 6.380651e-05 6.4701322e-05 6.4338652e-05 6.3766856e-05]]...]
INFO - root - 2017-12-09 06:19:36.524893: step 110, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:35m:53s remains)
INFO - root - 2017-12-09 06:19:43.181991: step 120, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.669 sec/batch; 61h:47m:20s remains)
INFO - root - 2017-12-09 06:19:49.768686: step 130, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.653 sec/batch; 60h:19m:39s remains)
INFO - root - 2017-12-09 06:19:56.407335: step 140, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.671 sec/batch; 61h:55m:21s remains)
INFO - root - 2017-12-09 06:20:03.090534: step 150, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:56m:28s remains)
INFO - root - 2017-12-09 06:20:09.953880: step 160, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.693 sec/batch; 63h:57m:09s remains)
INFO - root - 2017-12-09 06:20:16.479282: step 170, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.661 sec/batch; 61h:01m:32s remains)
INFO - root - 2017-12-09 06:20:22.984116: step 180, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.622 sec/batch; 57h:24m:13s remains)
INFO - root - 2017-12-09 06:20:29.576853: step 190, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.653 sec/batch; 60h:16m:58s remains)
INFO - root - 2017-12-09 06:20:36.132978: step 200, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:26m:03s remains)
2017-12-09 06:20:36.843517: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.000329331 0.00035019216 0.00036902839 0.00037184471 0.00034655022 0.00032735776 0.00031329121 0.000307661 0.00028554912 0.00025547005 0.00023034457 0.00019950871 0.00015980571 0.00014099275 0.00013405013][0.00047335643 0.00051827671 0.00053678569 0.00053725031 0.0005180611 0.00049816561 0.00047783859 0.00046205407 0.00043529828 0.00039137807 0.00033834588 0.0002823621 0.00022226185 0.00018850077 0.00018261012][0.00063504116 0.00070875476 0.00073514041 0.00072247663 0.00069509412 0.00068128051 0.00066182541 0.00063296035 0.00059522619 0.00053248147 0.00045204148 0.00037362747 0.00029982513 0.00026316664 0.00025665914][0.00084498327 0.00094957347 0.00097675866 0.00095714547 0.00092087407 0.00090598746 0.00087324542 0.00083154533 0.0007801454 0.00070668734 0.0005920605 0.00046945017 0.00037566508 0.00033360257 0.0003312201][0.0010697292 0.001217481 0.0012447597 0.0011987252 0.0011654003 0.0011595684 0.001142742 0.0010843036 0.0010069007 0.00090490456 0.00074962247 0.00056870608 0.00043056396 0.00036770137 0.00036248323][0.0012683746 0.0014484433 0.0014860508 0.001433973 0.0014062067 0.0014231452 0.0014501497 0.0014023019 0.0012698559 0.0011177023 0.00091906649 0.0006793685 0.00048819589 0.00040038922 0.00038831972][0.0014254538 0.0016335902 0.0016807182 0.0016407345 0.0016280104 0.0016694382 0.0017413725 0.0017429248 0.0015725287 0.0013690511 0.0011212303 0.00085468392 0.00063566712 0.00051005569 0.00047417506][0.0015527329 0.0017903326 0.0018473921 0.0018021204 0.0017731523 0.0018109472 0.0019152323 0.0019975631 0.0018168313 0.0015628056 0.0012985241 0.0010440262 0.00082456949 0.00068116712 0.00062434643][0.0016017364 0.0018691987 0.0019355437 0.0018766697 0.0018218348 0.0018154914 0.0018566785 0.0018945893 0.0018018782 0.0016206169 0.0013890015 0.0011643415 0.00095542613 0.00080618041 0.00075252389][0.0015509879 0.0018340996 0.0019290778 0.0018788783 0.0018143822 0.0017753893 0.0017732928 0.0017813869 0.001736065 0.0016155032 0.0014432393 0.0012577325 0.0010704549 0.00092778087 0.00086848647][0.0014496638 0.0017451347 0.0018781984 0.0018557054 0.0017883264 0.0017459532 0.0017217622 0.0017182536 0.0016954223 0.0016326498 0.0015162537 0.0013731419 0.0012010474 0.0010529917 0.00098430261][0.0012734586 0.0015574049 0.0017266165 0.0017575993 0.0017245359 0.0016842446 0.0016488384 0.0016250598 0.0016239609 0.0016184655 0.0015610384 0.0014510092 0.0012928421 0.001135049 0.0010546399][0.0010904549 0.0013564914 0.0015343839 0.0016102229 0.0016140916 0.0015879184 0.0015557618 0.0015312704 0.0015336643 0.0015508858 0.0015323481 0.0014423706 0.0013115746 0.001170204 0.001075497][0.00091714156 0.0011423677 0.0013067665 0.0014015149 0.0014284868 0.0014162491 0.0013876816 0.001370666 0.0013762676 0.0013921406 0.0013827615 0.0013219392 0.0012191769 0.0011077195 0.0010211585][0.00073052023 0.00090779166 0.0010429875 0.0011190948 0.0011413767 0.0011285729 0.0010982662 0.0010777235 0.0010798759 0.0011013534 0.0011038342 0.0010567514 0.0009766313 0.00089220563 0.00083241914]]...]
INFO - root - 2017-12-09 06:20:43.500329: step 210, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.673 sec/batch; 62h:07m:48s remains)
INFO - root - 2017-12-09 06:20:50.148464: step 220, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.668 sec/batch; 61h:40m:54s remains)
INFO - root - 2017-12-09 06:20:56.744396: step 230, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.641 sec/batch; 59h:11m:11s remains)
INFO - root - 2017-12-09 06:21:03.208566: step 240, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.665 sec/batch; 61h:20m:39s remains)
INFO - root - 2017-12-09 06:21:09.850669: step 250, loss = 0.90, batch loss = 0.69 (11.4 examples/sec; 0.704 sec/batch; 64h:58m:47s remains)
INFO - root - 2017-12-09 06:21:16.475936: step 260, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.647 sec/batch; 59h:40m:40s remains)
INFO - root - 2017-12-09 06:21:22.851137: step 270, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.640 sec/batch; 59h:05m:53s remains)
INFO - root - 2017-12-09 06:21:29.391443: step 280, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.680 sec/batch; 62h:47m:24s remains)
INFO - root - 2017-12-09 06:21:36.070894: step 290, loss = 0.89, batch loss = 0.68 (11.8 examples/sec; 0.678 sec/batch; 62h:32m:24s remains)
INFO - root - 2017-12-09 06:21:42.693210: step 300, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.618 sec/batch; 57h:00m:49s remains)
2017-12-09 06:21:43.382317: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00090510363 -0.000906181 -0.000908983 -0.000913145 -0.00091667043 -0.00092114706 -0.00092472974 -0.00092379795 -0.00092012557 -0.00091675459 -0.00091560843 -0.00091661426 -0.00091821968 -0.00091932825 -0.0009201644][-0.00091227458 -0.00091140246 -0.000911586 -0.00091157382 -0.00091280648 -0.00091668166 -0.00092258578 -0.00092409819 -0.00092231721 -0.00092018396 -0.0009183599 -0.00091716443 -0.00091624219 -0.00091578462 -0.00091572391][-0.00091789028 -0.00091576815 -0.000913937 -0.00090999459 -0.00090839772 -0.00091163721 -0.0009171702 -0.00091993227 -0.00092057604 -0.00091906264 -0.0009154749 -0.00091309816 -0.00091135688 -0.00090995181 -0.00091020594][-0.0009203176 -0.00091705751 -0.00091182516 -0.00090265763 -0.00089689466 -0.0008981498 -0.00090534659 -0.00091045065 -0.000914783 -0.00091511116 -0.00091083732 -0.00090643694 -0.00090416323 -0.00090307544 -0.00090244372][-0.00091615808 -0.00090930267 -0.00089750421 -0.00088126113 -0.00087125145 -0.00087043172 -0.000878647 -0.00088920764 -0.00089994934 -0.00090507092 -0.00090431073 -0.00089943554 -0.00089557847 -0.00089380541 -0.00089248572][-0.00091022195 -0.00090158929 -0.00088399078 -0.00086374045 -0.00084952242 -0.00084501185 -0.00084734097 -0.00085656514 -0.00087437395 -0.00088567159 -0.00089213549 -0.000892765 -0.00089009455 -0.00088587851 -0.00088243635][-0.00091031357 -0.00089799456 -0.00087632827 -0.00085067959 -0.0008349251 -0.0008273095 -0.00082548021 -0.00083257415 -0.00085020874 -0.00086459762 -0.00087474711 -0.00088260521 -0.00088408642 -0.00087996083 -0.00087638525][-0.00091430714 -0.0009044942 -0.00088316679 -0.00085863459 -0.00084160786 -0.00083026069 -0.00082710385 -0.00083109282 -0.00084576628 -0.00086142012 -0.000874279 -0.000884059 -0.00088511559 -0.00088278356 -0.00088242057][-0.0009160823 -0.00091075758 -0.00089695107 -0.0008809499 -0.00086827471 -0.00086183968 -0.00085773983 -0.00085597252 -0.00086486212 -0.00087513181 -0.00088616263 -0.00089384534 -0.00089687976 -0.00089622207 -0.00089521485][-0.00090519327 -0.00090596505 -0.0009040049 -0.00089687668 -0.00089040148 -0.00088805205 -0.0008862887 -0.0008862323 -0.00089148781 -0.00089534966 -0.00090020627 -0.00090452132 -0.00090673409 -0.00090683589 -0.000907158][-0.00089257339 -0.00089788972 -0.000906217 -0.000907431 -0.0009060005 -0.00090375834 -0.00090387219 -0.00090583117 -0.00090939389 -0.00091081415 -0.00091149216 -0.00091346778 -0.00091409311 -0.00091288576 -0.00091230776][-0.00088631921 -0.00089260429 -0.000904942 -0.0009111205 -0.00091239176 -0.00091255084 -0.00091328932 -0.00091559184 -0.00091636303 -0.0009150696 -0.00091387104 -0.00091487018 -0.00091569754 -0.00091366482 -0.00091359008][-0.00089188037 -0.00089502661 -0.00090381817 -0.00090982008 -0.00091305363 -0.00091633177 -0.000918432 -0.000919491 -0.00091944053 -0.000918318 -0.000916402 -0.00091612118 -0.00091616967 -0.000915022 -0.00091465132][-0.00090327254 -0.00090367184 -0.00090645219 -0.00090870791 -0.00091127295 -0.00091351714 -0.00091410358 -0.00091441488 -0.00091477012 -0.00091406197 -0.0009121222 -0.00091165851 -0.00091204327 -0.00091210613 -0.000912166][-0.00091003685 -0.00090941868 -0.00091031328 -0.00091044273 -0.00091051112 -0.00091072824 -0.00091035169 -0.00091038004 -0.00091092283 -0.00091040175 -0.00090911076 -0.00090862124 -0.00090827816 -0.00090809591 -0.00090824824]]...]
INFO - root - 2017-12-09 06:21:50.033196: step 310, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 61h:08m:14s remains)
INFO - root - 2017-12-09 06:21:56.764952: step 320, loss = 0.91, batch loss = 0.70 (12.2 examples/sec; 0.653 sec/batch; 60h:16m:07s remains)
INFO - root - 2017-12-09 06:22:03.474502: step 330, loss = 0.90, batch loss = 0.70 (12.0 examples/sec; 0.667 sec/batch; 61h:30m:47s remains)
INFO - root - 2017-12-09 06:22:10.151243: step 340, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.658 sec/batch; 60h:44m:41s remains)
INFO - root - 2017-12-09 06:22:16.653301: step 350, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.630 sec/batch; 58h:06m:22s remains)
INFO - root - 2017-12-09 06:22:23.286913: step 360, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.673 sec/batch; 62h:06m:04s remains)
INFO - root - 2017-12-09 06:22:29.801867: step 370, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.672 sec/batch; 61h:58m:22s remains)
INFO - root - 2017-12-09 06:22:36.499492: step 380, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.696 sec/batch; 64h:09m:52s remains)
INFO - root - 2017-12-09 06:22:43.069760: step 390, loss = 0.91, batch loss = 0.70 (12.1 examples/sec; 0.662 sec/batch; 61h:03m:40s remains)
INFO - root - 2017-12-09 06:22:49.640685: step 400, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 61h:07m:43s remains)
2017-12-09 06:22:50.293625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0037477354 -0.0037432755 -0.0037406078 -0.0037384611 -0.0037359043 -0.0037340557 -0.0037334808 -0.0037334689 -0.0037327425 -0.0037320219 -0.0037318273 -0.0037324703 -0.0037341088 -0.0037356524 -0.0037369253][-0.0037555667 -0.0037510279 -0.0037474469 -0.0037440241 -0.0037408657 -0.0037389793 -0.0037386855 -0.0037386 -0.0037381845 -0.0037382334 -0.0037386636 -0.0037397472 -0.003741974 -0.0037438336 -0.0037450178][-0.0037598936 -0.0037535483 -0.0037471044 -0.0037410476 -0.0037357595 -0.0037329302 -0.0037327907 -0.0037346913 -0.0037374964 -0.0037419335 -0.0037453985 -0.003747531 -0.0037507284 -0.0037531441 -0.0037542889][-0.0037585127 -0.0037498013 -0.0037389216 -0.0037244952 -0.0037064643 -0.003689222 -0.0036797149 -0.0036825424 -0.0037001723 -0.0037237497 -0.0037425924 -0.003750209 -0.0037555869 -0.0037590498 -0.0037610377][-0.0037530665 -0.0037403791 -0.0037149552 -0.0036578283 -0.0035591556 -0.0034333752 -0.0033350529 -0.0033285429 -0.0034336336 -0.0035836415 -0.0036953636 -0.0037419007 -0.0037545445 -0.0037603278 -0.0037637111][-0.0037418774 -0.0037223536 -0.0036676894 -0.0035193674 -0.0032254271 -0.0028205952 -0.0024813167 -0.0024297505 -0.0027341773 -0.0031973119 -0.0035547488 -0.00371246 -0.0037487382 -0.0037597031 -0.0037657917][-0.0037320629 -0.0037077812 -0.0036235396 -0.0033701286 -0.0028371329 -0.0020783672 -0.0014142506 -0.0012792521 -0.0018307983 -0.0026984951 -0.0033736571 -0.0036758468 -0.0037417638 -0.003758928 -0.0037683367][-0.0037314005 -0.003706903 -0.0036150767 -0.0033249776 -0.0026970278 -0.0017801868 -0.00095256278 -0.00075860554 -0.0014045928 -0.0024483707 -0.0032736622 -0.0036525875 -0.003735563 -0.0037561441 -0.0037678573][-0.0037433116 -0.0037257688 -0.0036560849 -0.0034295362 -0.0029291362 -0.0021855731 -0.0014970875 -0.0013089674 -0.001800447 -0.0026460793 -0.0033362121 -0.0036590467 -0.0037327346 -0.0037536924 -0.0037661989][-0.003757061 -0.0037481363 -0.0037095896 -0.0035846536 -0.0033070848 -0.0028907051 -0.002492283 -0.0023641372 -0.0026178788 -0.003090858 -0.0034928774 -0.0036858125 -0.0037352568 -0.0037543839 -0.0037657716][-0.0037672708 -0.0037634105 -0.0037446278 -0.003687832 -0.0035662195 -0.003388461 -0.0032198569 -0.0031641186 -0.0032697716 -0.0034647326 -0.0036339161 -0.0037201974 -0.0037472851 -0.0037607437 -0.0037684927][-0.0037736041 -0.00377255 -0.003763153 -0.0037360187 -0.0036830271 -0.0036130736 -0.0035550061 -0.0035486966 -0.0036007315 -0.0036711325 -0.0037224584 -0.0037505634 -0.0037627965 -0.0037703097 -0.0037744641][-0.0037768541 -0.003776826 -0.0037724047 -0.0037597679 -0.0037369709 -0.0037091761 -0.0036901489 -0.0036956319 -0.0037228176 -0.003750074 -0.0037632477 -0.0037715181 -0.0037767091 -0.0037796514 -0.0037804246][-0.0037793557 -0.0037798954 -0.0037781007 -0.003773364 -0.003764553 -0.0037535669 -0.0037461561 -0.0037488169 -0.0037606582 -0.0037722432 -0.0037771561 -0.0037811338 -0.0037837515 -0.0037844805 -0.0037838926][-0.003781531 -0.0037824591 -0.0037817883 -0.0037806015 -0.0037785345 -0.0037755326 -0.0037729971 -0.0037734804 -0.0037764919 -0.0037798402 -0.0037816016 -0.0037833834 -0.0037847932 -0.0037851788 -0.0037850675]]...]
INFO - root - 2017-12-09 06:22:56.855589: step 410, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.664 sec/batch; 61h:13m:07s remains)
INFO - root - 2017-12-09 06:23:03.495118: step 420, loss = 0.89, batch loss = 0.68 (12.0 examples/sec; 0.669 sec/batch; 61h:43m:30s remains)
INFO - root - 2017-12-09 06:23:10.115192: step 430, loss = 0.91, batch loss = 0.70 (12.2 examples/sec; 0.656 sec/batch; 60h:31m:16s remains)
INFO - root - 2017-12-09 06:23:16.743534: step 440, loss = 0.91, batch loss = 0.70 (12.2 examples/sec; 0.656 sec/batch; 60h:32m:39s remains)
INFO - root - 2017-12-09 06:23:23.410160: step 450, loss = 0.88, batch loss = 0.67 (11.4 examples/sec; 0.700 sec/batch; 64h:31m:23s remains)
INFO - root - 2017-12-09 06:23:30.108740: step 460, loss = 0.91, batch loss = 0.70 (11.9 examples/sec; 0.673 sec/batch; 62h:02m:24s remains)
INFO - root - 2017-12-09 06:23:36.653030: step 470, loss = 0.89, batch loss = 0.68 (12.0 examples/sec; 0.667 sec/batch; 61h:31m:24s remains)
INFO - root - 2017-12-09 06:23:43.312282: step 480, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.684 sec/batch; 63h:03m:52s remains)
INFO - root - 2017-12-09 06:23:49.971127: step 490, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.672 sec/batch; 61h:56m:17s remains)
INFO - root - 2017-12-09 06:23:56.664475: step 500, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.667 sec/batch; 61h:30m:44s remains)
2017-12-09 06:23:57.383275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0071744905 -0.0071720853 -0.0071716015 -0.0071725389 -0.0071739336 -0.0071751894 -0.0071762293 -0.0071762507 -0.0071747671 -0.0071731443 -0.0071730572 -0.0071735089 -0.0071738204 -0.0071727606 -0.0071729342][-0.0071680443 -0.0071640434 -0.0071631172 -0.007164075 -0.0071651288 -0.0071669854 -0.0071685403 -0.0071680844 -0.0071664625 -0.0071649081 -0.0071647023 -0.0071640979 -0.0071639479 -0.0071646175 -0.0071668741][-0.0071639493 -0.0071584727 -0.0071567874 -0.0071567767 -0.0071586268 -0.0071603586 -0.007161438 -0.0071602031 -0.0071576433 -0.0071561071 -0.0071569444 -0.0071578571 -0.0071595954 -0.0071612243 -0.0071653845][-0.0071606524 -0.0071546561 -0.0071520391 -0.0071501536 -0.0071515883 -0.0071535874 -0.0071546049 -0.0071532791 -0.0071494053 -0.0071475934 -0.0071499269 -0.0071516847 -0.007155675 -0.0071608042 -0.0071685803][-0.0071604704 -0.0071562063 -0.0071530943 -0.0071476661 -0.0071450565 -0.0071459622 -0.0071472153 -0.0071464651 -0.00714358 -0.0071421419 -0.0071431817 -0.007146352 -0.0071538761 -0.0071627926 -0.0071735657][-0.0071631535 -0.0071613211 -0.0071587861 -0.0071528447 -0.007145308 -0.00714149 -0.0071398136 -0.007138059 -0.0071377414 -0.00713967 -0.0071421522 -0.007144799 -0.0071519725 -0.0071634739 -0.0071763047][-0.0071655032 -0.0071666441 -0.0071664783 -0.0071621421 -0.0071542235 -0.0071465191 -0.0071388623 -0.0071328231 -0.0071357633 -0.0071440251 -0.0071486225 -0.0071505881 -0.007156238 -0.007165303 -0.00717658][-0.00716839 -0.0071707438 -0.0071715643 -0.0071694287 -0.0071653719 -0.007158889 -0.0071510277 -0.007142751 -0.0071436604 -0.0071524596 -0.0071587157 -0.0071618403 -0.0071667922 -0.0071721836 -0.0071792281][-0.0071723517 -0.0071745669 -0.0071760095 -0.0071760048 -0.0071754241 -0.0071724565 -0.0071673403 -0.0071614478 -0.00716137 -0.0071665817 -0.0071713096 -0.0071747722 -0.0071795266 -0.0071832449 -0.0071867919][-0.007177942 -0.0071798936 -0.0071815751 -0.0071829171 -0.0071846754 -0.0071834177 -0.0071804412 -0.007177324 -0.0071778968 -0.007180661 -0.0071832826 -0.00718539 -0.0071888966 -0.0071919803 -0.007193658][-0.0071846945 -0.0071866643 -0.0071883015 -0.0071898429 -0.0071910238 -0.0071903965 -0.0071888473 -0.0071875355 -0.0071880934 -0.0071895947 -0.0071911295 -0.0071924697 -0.0071947849 -0.0071969251 -0.0071975687][-0.0071900864 -0.007192282 -0.0071939956 -0.0071954629 -0.0071958364 -0.0071947281 -0.0071938 -0.0071932343 -0.0071935044 -0.0071942685 -0.0071949349 -0.007196073 -0.0071971221 -0.0071982015 -0.0071986089][-0.007194024 -0.0071958266 -0.0071969321 -0.0071975724 -0.0071975384 -0.0071965754 -0.0071956213 -0.0071951281 -0.007195265 -0.0071953717 -0.0071951263 -0.007195761 -0.0071966276 -0.007197462 -0.0071981666][-0.0071935607 -0.0071946979 -0.0071954075 -0.0071956529 -0.0071955826 -0.0071948864 -0.0071941838 -0.0071935738 -0.0071932888 -0.007193171 -0.0071925088 -0.007192357 -0.00719289 -0.0071938522 -0.0071948506][-0.0071892557 -0.007189773 -0.0071905679 -0.0071906769 -0.007190268 -0.0071895085 -0.0071886047 -0.0071879453 -0.0071875444 -0.0071871476 -0.0071863481 -0.0071858326 -0.0071859378 -0.0071869972 -0.0071883877]]...]
INFO - root - 2017-12-09 06:24:03.860941: step 510, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.666 sec/batch; 61h:23m:52s remains)
INFO - root - 2017-12-09 06:24:10.549927: step 520, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 60h:30m:19s remains)
INFO - root - 2017-12-09 06:24:17.149169: step 530, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.673 sec/batch; 62h:04m:35s remains)
INFO - root - 2017-12-09 06:24:23.643678: step 540, loss = 0.91, batch loss = 0.70 (12.3 examples/sec; 0.649 sec/batch; 59h:51m:47s remains)
INFO - root - 2017-12-09 06:24:30.290447: step 550, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.666 sec/batch; 61h:25m:46s remains)
INFO - root - 2017-12-09 06:24:37.008797: step 560, loss = 0.90, batch loss = 0.70 (11.9 examples/sec; 0.674 sec/batch; 62h:07m:23s remains)
INFO - root - 2017-12-09 06:24:43.259721: step 570, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.626 sec/batch; 57h:41m:13s remains)
INFO - root - 2017-12-09 06:24:49.936171: step 580, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.681 sec/batch; 62h:47m:08s remains)
INFO - root - 2017-12-09 06:24:56.489381: step 590, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.653 sec/batch; 60h:13m:47s remains)
INFO - root - 2017-12-09 06:25:03.203523: step 600, loss = 0.91, batch loss = 0.70 (13.1 examples/sec; 0.612 sec/batch; 56h:24m:01s remains)
2017-12-09 06:25:03.903840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00036334246 0.0047673713 0.013276952 0.025377575 0.041116085 0.059569217 0.079381041 0.098263256 0.11335393 0.12272227 0.12543748 0.12086171 0.11014404 0.094041333 0.077114195][0.0086977808 0.016756371 0.028768573 0.043995775 0.061889708 0.081070907 0.10015549 0.11707711 0.12943785 0.13588142 0.13538188 0.12699224 0.11178162 0.091224805 0.069966689][0.023085356 0.03479819 0.051028248 0.070150055 0.09085045 0.11127865 0.12969771 0.14434661 0.15354225 0.15661705 0.15241323 0.14015262 0.12022908 0.095484778 0.070264079][0.043671068 0.059080094 0.079305686 0.10206703 0.12523019 0.14682867 0.16464144 0.17744464 0.18393777 0.18405522 0.17689976 0.1612 0.13712049 0.10840666 0.079440266][0.070156187 0.088399589 0.11111855 0.13584003 0.15973926 0.18144991 0.19825211 0.20942028 0.21374249 0.21155059 0.20214726 0.18380958 0.15662508 0.12438489 0.0921324][0.10071 0.12070101 0.14430563 0.16893061 0.19130661 0.21160351 0.22646002 0.23580775 0.23784679 0.23344812 0.2223001 0.20216413 0.17293815 0.13765611 0.10312618][0.13218959 0.15285869 0.1762097 0.19935329 0.21909738 0.23662633 0.24845496 0.25528556 0.25398114 0.24680617 0.23359801 0.21253508 0.18276301 0.14630094 0.11082172][0.15991054 0.17995301 0.20181431 0.22242662 0.23925687 0.25350139 0.26241589 0.26591012 0.2605032 0.25007978 0.23433828 0.21316543 0.18369676 0.14805293 0.1131963][0.17717931 0.19539671 0.21460225 0.23209448 0.24649945 0.25805947 0.2648426 0.26524863 0.25674593 0.2436485 0.22617012 0.20519881 0.17680757 0.14381352 0.11066949][0.18066512 0.19676402 0.2131052 0.22750235 0.23986425 0.24982563 0.2559942 0.25474575 0.24503505 0.2317405 0.21504112 0.19622615 0.17112514 0.14205933 0.11189591][0.17573126 0.18999118 0.20313989 0.21548073 0.22701326 0.2360011 0.24164999 0.24041732 0.23214377 0.21996307 0.20475557 0.18841511 0.1674466 0.14271773 0.11590622][0.16979367 0.18309031 0.19462642 0.20574972 0.2168065 0.22606696 0.23264664 0.23234545 0.22603923 0.21599279 0.2030831 0.18828496 0.17021233 0.14931881 0.12595719][0.16966313 0.181704 0.19150198 0.20236261 0.21415187 0.22480117 0.23281063 0.23518653 0.23228598 0.22442061 0.2129171 0.1988735 0.18296902 0.16425388 0.14317907][0.17324126 0.18248732 0.18973763 0.1995696 0.21123271 0.22444172 0.23607413 0.24233505 0.2429018 0.23807718 0.23006374 0.21773563 0.2038261 0.18775773 0.16992456][0.18156175 0.1876844 0.19176315 0.19984496 0.21135372 0.22576416 0.23945473 0.24989286 0.25580141 0.25550678 0.25147024 0.24208716 0.23112749 0.21825328 0.20270374]]...]
INFO - root - 2017-12-09 06:25:10.433244: step 610, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.675 sec/batch; 62h:11m:51s remains)
INFO - root - 2017-12-09 06:25:17.156395: step 620, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.664 sec/batch; 61h:10m:54s remains)
INFO - root - 2017-12-09 06:25:23.881023: step 630, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.677 sec/batch; 62h:25m:31s remains)
INFO - root - 2017-12-09 06:25:30.523012: step 640, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:30m:32s remains)
INFO - root - 2017-12-09 06:25:37.150360: step 650, loss = 0.89, batch loss = 0.68 (12.4 examples/sec; 0.644 sec/batch; 59h:20m:28s remains)
INFO - root - 2017-12-09 06:25:43.755540: step 660, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.633 sec/batch; 58h:18m:17s remains)
INFO - root - 2017-12-09 06:25:50.118044: step 670, loss = 0.90, batch loss = 0.69 (15.4 examples/sec; 0.520 sec/batch; 47h:57m:03s remains)
INFO - root - 2017-12-09 06:25:56.701847: step 680, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.625 sec/batch; 57h:39m:01s remains)
INFO - root - 2017-12-09 06:26:03.130341: step 690, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.655 sec/batch; 60h:20m:54s remains)
INFO - root - 2017-12-09 06:26:09.819542: step 700, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.654 sec/batch; 60h:18m:20s remains)
2017-12-09 06:26:10.597596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.010384459 -0.010383867 -0.010383475 -0.010383228 -0.010383082 -0.010382951 -0.010382683 -0.01038231 -0.010381852 -0.010381399 -0.01038097 -0.010380679 -0.010380711 -0.010381159 -0.010381967][-0.010383696 -0.010383071 -0.010382682 -0.010382367 -0.010382112 -0.010381857 -0.010381555 -0.010381161 -0.010380653 -0.01038007 -0.010379514 -0.010379107 -0.01037908 -0.010379561 -0.010380496][-0.010382632 -0.010382227 -0.01038209 -0.010381946 -0.010381721 -0.01038142 -0.010381084 -0.010380721 -0.010380254 -0.010379677 -0.01037909 -0.010378581 -0.01037843 -0.010378802 -0.010379642][-0.010381502 -0.010381378 -0.010381666 -0.010381837 -0.010381798 -0.01038165 -0.010381448 -0.010381293 -0.010381071 -0.010380745 -0.010380379 -0.010379866 -0.010379591 -0.010379713 -0.010380301][-0.010380552 -0.010380395 -0.010380999 -0.010381483 -0.010381756 -0.010381951 -0.010382071 -0.010382218 -0.010382359 -0.010382492 -0.010382527 -0.010382166 -0.010381816 -0.0103817 -0.010381987][-0.010380019 -0.010379569 -0.010380114 -0.010380705 -0.010381268 -0.010381793 -0.010382184 -0.010382503 -0.010382935 -0.010383537 -0.010384031 -0.01038401 -0.01038379 -0.010383624 -0.01038374][-0.010380439 -0.010379652 -0.010379853 -0.010380314 -0.010380958 -0.010381566 -0.010381969 -0.010382267 -0.010382769 -0.010383611 -0.010384479 -0.010384873 -0.010385045 -0.010385117 -0.010385285][-0.010382233 -0.01038137 -0.010381258 -0.010381396 -0.010381849 -0.010382287 -0.010382507 -0.010382586 -0.010382875 -0.010383617 -0.010384546 -0.010385207 -0.010385773 -0.010386204 -0.010386632][-0.01038448 -0.010383955 -0.010383942 -0.010384042 -0.010384345 -0.010384581 -0.010384659 -0.010384559 -0.010384516 -0.010384819 -0.010385393 -0.010385947 -0.010386534 -0.010387116 -0.010387731][-0.010387008 -0.010386903 -0.010387029 -0.01038718 -0.010387439 -0.010387635 -0.010387768 -0.010387633 -0.010387446 -0.010387347 -0.010387396 -0.0103875 -0.010387726 -0.010388126 -0.01038871][-0.010389621 -0.010389581 -0.010389652 -0.010389744 -0.010389914 -0.010390076 -0.010390298 -0.010390325 -0.010390165 -0.010389919 -0.010389693 -0.010389436 -0.010389319 -0.010389416 -0.010389798][-0.010392395 -0.010392295 -0.010392187 -0.01039212 -0.010392123 -0.010392115 -0.010392176 -0.010392172 -0.010392002 -0.010391767 -0.010391499 -0.010391202 -0.010391 -0.010390938 -0.010391171][-0.010395109 -0.010394902 -0.010394714 -0.010394586 -0.010394492 -0.010394328 -0.010394135 -0.010393851 -0.010393479 -0.010393144 -0.010392799 -0.010392538 -0.010392418 -0.010392424 -0.010392664][-0.010396769 -0.010396582 -0.010396509 -0.010396506 -0.010396454 -0.010396239 -0.010395879 -0.010395362 -0.010394725 -0.010394113 -0.010393607 -0.010393345 -0.01039331 -0.010393386 -0.010393674][-0.010397109 -0.010396974 -0.010397057 -0.010397193 -0.010397222 -0.010397039 -0.010396647 -0.010396007 -0.010395178 -0.010394351 -0.010393717 -0.010393378 -0.010393367 -0.010393506 -0.010393837]]...]
INFO - root - 2017-12-09 06:26:16.990974: step 710, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.643 sec/batch; 59h:15m:38s remains)
INFO - root - 2017-12-09 06:26:23.696914: step 720, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.631 sec/batch; 58h:07m:36s remains)
INFO - root - 2017-12-09 06:26:30.239307: step 730, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.650 sec/batch; 59h:53m:35s remains)
INFO - root - 2017-12-09 06:26:36.677804: step 740, loss = 0.91, batch loss = 0.70 (12.3 examples/sec; 0.649 sec/batch; 59h:49m:28s remains)
INFO - root - 2017-12-09 06:26:43.397828: step 750, loss = 0.91, batch loss = 0.70 (12.2 examples/sec; 0.656 sec/batch; 60h:27m:19s remains)
INFO - root - 2017-12-09 06:26:50.047610: step 760, loss = 0.89, batch loss = 0.68 (11.9 examples/sec; 0.670 sec/batch; 61h:42m:31s remains)
INFO - root - 2017-12-09 06:26:56.743302: step 770, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.688 sec/batch; 63h:23m:39s remains)
INFO - root - 2017-12-09 06:27:03.280127: step 780, loss = 0.90, batch loss = 0.69 (11.3 examples/sec; 0.705 sec/batch; 64h:57m:14s remains)
INFO - root - 2017-12-09 06:27:09.809537: step 790, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:22m:23s remains)
INFO - root - 2017-12-09 06:27:16.540072: step 800, loss = 0.90, batch loss = 0.69 (10.9 examples/sec; 0.732 sec/batch; 67h:25m:24s remains)
2017-12-09 06:27:17.291641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.011943816 -0.011936923 -0.01193341 -0.011929389 -0.011925751 -0.011923222 -0.011924877 -0.011932869 -0.011945375 -0.011957798 -0.01196519 -0.011968011 -0.0119685 -0.011967718 -0.011967599][-0.011942685 -0.011935304 -0.011932325 -0.01192848 -0.011924248 -0.011920995 -0.011921604 -0.011929019 -0.011941906 -0.011955883 -0.011965566 -0.011970164 -0.011970975 -0.011968886 -0.011967076][-0.011940623 -0.011932721 -0.011929926 -0.011926442 -0.01192236 -0.011918169 -0.011917308 -0.011922831 -0.011935123 -0.011949493 -0.011961 -0.011967722 -0.011968977 -0.011966175 -0.011963333][-0.011937127 -0.011928723 -0.011925667 -0.011922389 -0.011918523 -0.011913573 -0.011910584 -0.011913382 -0.011924899 -0.01194024 -0.011953239 -0.011961496 -0.011963092 -0.01196028 -0.011957977][-0.011932869 -0.011924111 -0.011920345 -0.011916899 -0.011913465 -0.011908778 -0.011905489 -0.011906576 -0.011916436 -0.011931944 -0.011945256 -0.01195391 -0.011955637 -0.011953522 -0.011952781][-0.011928014 -0.011917921 -0.011913109 -0.011909285 -0.011906771 -0.011903601 -0.011901563 -0.011902713 -0.011911819 -0.011926334 -0.011939504 -0.011948183 -0.011950795 -0.011949544 -0.011949846][-0.011923337 -0.011911194 -0.011904847 -0.011900131 -0.011898032 -0.011895843 -0.011895492 -0.011897877 -0.011907678 -0.011922197 -0.011935757 -0.011945114 -0.011948721 -0.011948143 -0.011948812][-0.011916586 -0.011903772 -0.01189643 -0.011891253 -0.011889987 -0.011889438 -0.011891064 -0.011895292 -0.011906467 -0.01192091 -0.011934473 -0.011943993 -0.011947741 -0.011947431 -0.011948092][-0.011910412 -0.011898778 -0.011891646 -0.011886473 -0.011886364 -0.011888725 -0.01189345 -0.011899903 -0.011910425 -0.011922769 -0.011934604 -0.011943392 -0.011947647 -0.011947626 -0.011947826][-0.01190911 -0.011899282 -0.011893064 -0.011889401 -0.01189142 -0.011896506 -0.011902949 -0.01190946 -0.0119173 -0.011925624 -0.011934731 -0.011942142 -0.011946508 -0.011946801 -0.011947027][-0.011916567 -0.011909693 -0.011904989 -0.011902922 -0.011905503 -0.011910317 -0.011915356 -0.011919443 -0.011923565 -0.011927536 -0.011933555 -0.011938278 -0.011941276 -0.011941847 -0.011943315][-0.01192759 -0.011923649 -0.011920529 -0.011919111 -0.011920393 -0.011922701 -0.011925077 -0.011926454 -0.01192749 -0.011928467 -0.01193133 -0.011933303 -0.01193448 -0.011935191 -0.011937088][-0.011934032 -0.011931635 -0.011929781 -0.01192875 -0.01192895 -0.011929289 -0.011929745 -0.011929668 -0.011929155 -0.011928801 -0.011929289 -0.01192907 -0.011929109 -0.011929666 -0.011931734][-0.01193259 -0.011930454 -0.011929941 -0.011929389 -0.011929275 -0.011928681 -0.011928095 -0.01192731 -0.011926183 -0.011925354 -0.011924951 -0.011924762 -0.011924884 -0.011925566 -0.011928129][-0.011928133 -0.011926166 -0.01192612 -0.011925279 -0.011924689 -0.01192388 -0.011923245 -0.011922788 -0.011921888 -0.011921619 -0.011921048 -0.011920832 -0.011921086 -0.011922264 -0.011925117]]...]
INFO - root - 2017-12-09 06:27:23.727591: step 810, loss = 0.89, batch loss = 0.68 (12.1 examples/sec; 0.664 sec/batch; 61h:09m:11s remains)
INFO - root - 2017-12-09 06:27:30.351328: step 820, loss = 0.91, batch loss = 0.70 (11.7 examples/sec; 0.683 sec/batch; 62h:56m:31s remains)
INFO - root - 2017-12-09 06:27:36.950298: step 830, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.684 sec/batch; 63h:01m:13s remains)
INFO - root - 2017-12-09 06:27:43.599473: step 840, loss = 0.89, batch loss = 0.68 (12.0 examples/sec; 0.668 sec/batch; 61h:32m:29s remains)
INFO - root - 2017-12-09 06:27:50.360966: step 850, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.688 sec/batch; 63h:23m:08s remains)
INFO - root - 2017-12-09 06:27:57.049187: step 860, loss = 0.91, batch loss = 0.70 (11.9 examples/sec; 0.674 sec/batch; 62h:02m:39s remains)
INFO - root - 2017-12-09 06:28:03.758528: step 870, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:16m:51s remains)
INFO - root - 2017-12-09 06:28:10.281638: step 880, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.667 sec/batch; 61h:27m:38s remains)
INFO - root - 2017-12-09 06:28:16.874470: step 890, loss = 0.91, batch loss = 0.70 (12.5 examples/sec; 0.642 sec/batch; 59h:06m:27s remains)
INFO - root - 2017-12-09 06:28:23.321320: step 900, loss = 0.89, batch loss = 0.68 (12.3 examples/sec; 0.651 sec/batch; 59h:55m:36s remains)
2017-12-09 06:28:24.063125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.014330409 -0.014342982 -0.014342919 -0.014335341 -0.014333508 -0.014346635 -0.014364521 -0.014374752 -0.014377831 -0.014380004 -0.014377886 -0.014377678 -0.01437428 -0.01436313 -0.014347942][-0.013735362 -0.013756757 -0.013734514 -0.013650836 -0.013609727 -0.013642547 -0.013759778 -0.013893247 -0.014030121 -0.014077299 -0.014075602 -0.014057393 -0.01401873 -0.013903803 -0.013688564][-0.011505943 -0.011469747 -0.011430222 -0.011350929 -0.011394146 -0.011548148 -0.012060069 -0.01269343 -0.013199413 -0.013392484 -0.013450149 -0.013373523 -0.013226271 -0.012876433 -0.012089401][-0.0073160734 -0.0071216146 -0.0068321857 -0.0063918047 -0.0063011758 -0.0068723671 -0.00850438 -0.010389348 -0.012011697 -0.012789278 -0.012854621 -0.012415755 -0.011832055 -0.010862843 -0.009289518][-0.0019059144 -0.001751245 -0.0015333062 -0.00097144209 -0.00087328069 -0.0017288737 -0.0043223994 -0.0076121567 -0.010627556 -0.012242978 -0.012444105 -0.011578988 -0.010144792 -0.0080727655 -0.0051398212][0.0042406265 0.0044650231 0.0044804923 0.004856158 0.0047836751 0.0034945142 -0.00019836798 -0.0048405491 -0.0091695841 -0.011676002 -0.012093576 -0.010766748 -0.0085010361 -0.0054378528 -0.0012449808][0.010124566 0.010810016 0.011049598 0.011417836 0.010880696 0.0087161791 0.003910603 -0.0020075636 -0.0075977403 -0.01101112 -0.011709275 -0.0099759605 -0.0068955766 -0.0030553434 0.0020594746][0.014908575 0.016108578 0.016807782 0.017778032 0.017353557 0.014351316 0.0084098633 0.0012915079 -0.0054140519 -0.0098754419 -0.011262212 -0.0097515816 -0.0063252645 -0.0019676238 0.003584953][0.018716417 0.020771842 0.021617431 0.022686057 0.022391845 0.019141395 0.012961594 0.0050569419 -0.0026653027 -0.0080145579 -0.01005924 -0.0089401137 -0.0055176262 -0.0010504192 0.004400894][0.020859189 0.02422801 0.025351543 0.026316702 0.025763359 0.02207261 0.016063206 0.00858992 0.0010794532 -0.0047184238 -0.0074901255 -0.0069838315 -0.0040672449 -0.00014450029 0.0045103747][0.022175383 0.026601672 0.027679041 0.028682735 0.028380435 0.024815205 0.019346461 0.012543928 0.0056340788 0.00011674967 -0.0030051786 -0.0034063952 -0.0016804049 0.001047302 0.0042095352][0.02294511 0.028581351 0.029428244 0.030192055 0.029981729 0.026985936 0.022542082 0.016861837 0.010942023 0.005887609 0.002458645 0.0010465486 0.001277782 0.0024691392 0.0038024485][0.022870492 0.029542375 0.030286167 0.030874338 0.030769359 0.028491028 0.025125697 0.020700861 0.015880508 0.011411818 0.0078022536 0.0054850876 0.0043423362 0.0041481629 0.0038744863][0.021631137 0.028620239 0.029344652 0.030068979 0.030430719 0.029118385 0.026751917 0.023402285 0.01963472 0.015836539 0.012463642 0.0099646505 0.008148836 0.0067630615 0.0050773453][0.0191494 0.025500193 0.026227023 0.0273218 0.028345324 0.028090615 0.02689451 0.025052004 0.022441674 0.019313291 0.016200013 0.013693705 0.011486059 0.0094437189 0.0069416743]]...]
INFO - root - 2017-12-09 06:28:30.418644: step 910, loss = 0.90, batch loss = 0.68 (16.5 examples/sec; 0.486 sec/batch; 44h:46m:17s remains)
INFO - root - 2017-12-09 06:28:37.025723: step 920, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.667 sec/batch; 61h:27m:58s remains)
INFO - root - 2017-12-09 06:28:43.864694: step 930, loss = 0.91, batch loss = 0.70 (12.3 examples/sec; 0.651 sec/batch; 59h:59m:36s remains)
INFO - root - 2017-12-09 06:28:50.569918: step 940, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.673 sec/batch; 62h:00m:57s remains)
INFO - root - 2017-12-09 06:28:57.209369: step 950, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.662 sec/batch; 60h:59m:15s remains)
INFO - root - 2017-12-09 06:29:03.981235: step 960, loss = 0.91, batch loss = 0.69 (11.6 examples/sec; 0.692 sec/batch; 63h:46m:27s remains)
INFO - root - 2017-12-09 06:29:10.687256: step 970, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.680 sec/batch; 62h:34m:51s remains)
INFO - root - 2017-12-09 06:29:17.208755: step 980, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:31m:14s remains)
INFO - root - 2017-12-09 06:29:23.776029: step 990, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.620 sec/batch; 57h:06m:47s remains)
INFO - root - 2017-12-09 06:29:30.311601: step 1000, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.668 sec/batch; 61h:30m:46s remains)
2017-12-09 06:29:30.995146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.01635476 -0.016356848 -0.016360827 -0.016365482 -0.016370188 -0.016373444 -0.016375318 -0.016376052 -0.016376544 -0.016377272 -0.016378198 -0.016379105 -0.016378667 -0.016377345 -0.0163758][-0.016348626 -0.016350791 -0.016355647 -0.016361384 -0.016367467 -0.016371626 -0.016373746 -0.016374137 -0.016374188 -0.016374713 -0.016375862 -0.016377099 -0.016377056 -0.016376305 -0.016375152][-0.016349103 -0.016351275 -0.016356461 -0.016362231 -0.016367737 -0.016371462 -0.01637318 -0.016372899 -0.016372146 -0.016372234 -0.016373338 -0.016374821 -0.016375095 -0.016374853 -0.01637413][-0.016353123 -0.016354688 -0.016358662 -0.016363217 -0.01636713 -0.016369525 -0.016370473 -0.016369559 -0.016368201 -0.016368257 -0.016369976 -0.01637226 -0.016373033 -0.016373206 -0.016372913][-0.016356472 -0.016356789 -0.016358851 -0.01636114 -0.01636298 -0.016363859 -0.016363909 -0.016362472 -0.016361143 -0.01636208 -0.016365331 -0.01636878 -0.016370388 -0.016371083 -0.016371496][-0.016358051 -0.016357711 -0.016357886 -0.016357884 -0.016357441 -0.016356284 -0.016354833 -0.016352665 -0.01635256 -0.016355911 -0.016361298 -0.016365971 -0.016368357 -0.016369699 -0.016370662][-0.016359746 -0.016359007 -0.016357927 -0.016356042 -0.016353656 -0.016350886 -0.016347732 -0.016344858 -0.016346497 -0.016352596 -0.016359709 -0.016364936 -0.016367482 -0.016368892 -0.016370146][-0.016361775 -0.016361073 -0.016359521 -0.016356977 -0.016353838 -0.01635051 -0.016346723 -0.016344039 -0.016347149 -0.016353989 -0.016361032 -0.016365776 -0.016367773 -0.016368713 -0.016369835][-0.016364146 -0.0163634 -0.016362073 -0.016359892 -0.016356904 -0.016353823 -0.016350733 -0.016349304 -0.016352564 -0.01635843 -0.016364055 -0.016367555 -0.016368771 -0.016369179 -0.016370021][-0.01636735 -0.016366495 -0.016365614 -0.016364176 -0.016362138 -0.016360072 -0.016358087 -0.016357489 -0.016360182 -0.016364502 -0.016368078 -0.016369915 -0.016370177 -0.01637011 -0.016370693][-0.016371405 -0.016370315 -0.016369674 -0.016368954 -0.016368078 -0.016367231 -0.016366435 -0.016366446 -0.016368313 -0.016370747 -0.016372262 -0.016372712 -0.016372131 -0.016371546 -0.016371729][-0.016375626 -0.016374696 -0.016374296 -0.016374078 -0.016374007 -0.016374115 -0.01637426 -0.016374743 -0.016375748 -0.016376564 -0.016376564 -0.0163758 -0.016374504 -0.016373288 -0.016372941][-0.016379384 -0.016378835 -0.016378645 -0.016378665 -0.016378902 -0.016379349 -0.016379779 -0.016380291 -0.016380753 -0.016380725 -0.016379951 -0.016378529 -0.016376739 -0.016375069 -0.01637421][-0.016382424 -0.016382102 -0.016381858 -0.016381843 -0.016382046 -0.016382447 -0.016382847 -0.01638316 -0.016383221 -0.016382763 -0.01638175 -0.016380027 -0.016378021 -0.016376177 -0.01637499][-0.016383976 -0.016383713 -0.016383298 -0.016383152 -0.016383227 -0.016383432 -0.0163836 -0.016383555 -0.016383244 -0.016382625 -0.016381605 -0.016379919 -0.016377997 -0.016376292 -0.016375195]]...]
INFO - root - 2017-12-09 06:29:37.582532: step 1010, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.683 sec/batch; 62h:55m:49s remains)
INFO - root - 2017-12-09 06:29:44.108140: step 1020, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.666 sec/batch; 61h:19m:59s remains)
INFO - root - 2017-12-09 06:29:50.785492: step 1030, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.655 sec/batch; 60h:19m:56s remains)
INFO - root - 2017-12-09 06:29:57.606101: step 1040, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.686 sec/batch; 63h:08m:46s remains)
INFO - root - 2017-12-09 06:30:04.194514: step 1050, loss = 0.90, batch loss = 0.68 (12.2 examples/sec; 0.657 sec/batch; 60h:27m:30s remains)
INFO - root - 2017-12-09 06:30:10.890811: step 1060, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.677 sec/batch; 62h:20m:31s remains)
INFO - root - 2017-12-09 06:30:17.520141: step 1070, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.669 sec/batch; 61h:36m:10s remains)
INFO - root - 2017-12-09 06:30:23.795922: step 1080, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.640 sec/batch; 58h:57m:09s remains)
INFO - root - 2017-12-09 06:30:30.613995: step 1090, loss = 0.90, batch loss = 0.69 (11.2 examples/sec; 0.716 sec/batch; 65h:56m:12s remains)
INFO - root - 2017-12-09 06:30:37.331561: step 1100, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.675 sec/batch; 62h:09m:40s remains)
2017-12-09 06:30:38.196467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0040465146 -0.003961687 -0.0044589173 -0.0054582451 -0.0069169328 -0.0086551979 -0.010069543 -0.011004259 -0.011075336 -0.009613351 -0.0053230794 0.0022312887 0.012001535 0.022220284 0.031121913][-0.00304023 -0.0023444481 -0.0019941069 -0.0021372447 -0.0030112881 -0.004470082 -0.005818964 -0.0068020169 -0.0070123011 -0.0055949762 -0.0013464931 0.0059601404 0.015419137 0.025269661 0.033817329][-0.0022983653 -0.00093857385 0.0002640523 0.0010303762 0.00091731735 -1.92374e-05 -0.0011089649 -0.0020465944 -0.0023388751 -0.0010652784 0.0029057413 0.0099107679 0.018856619 0.027829006 0.035561424][-0.0015562987 0.00036261603 0.002280226 0.00388786 0.0046208948 0.0044013485 0.0037358757 0.0029239543 0.002570508 0.0036701765 0.0073096696 0.013746254 0.021903202 0.030026894 0.036974903][-0.00010706298 0.0023083482 0.0048690587 0.0072778147 0.0089067519 0.0095194168 0.0093766227 0.0087095685 0.0082152542 0.0090016108 0.012148883 0.017862275 0.025091693 0.032336313 0.038524546][0.0023461692 0.0051870942 0.0082987808 0.011342248 0.013693918 0.015047394 0.015385713 0.015015151 0.014598273 0.015366275 0.018022586 0.023013711 0.029303834 0.035472643 0.04053387][0.0049603265 0.0079897512 0.011310237 0.014582731 0.01727853 0.019110423 0.019904327 0.019771975 0.019496094 0.020372331 0.023165032 0.028197031 0.034300525 0.039584272 0.04355292][0.0069904514 0.0099445339 0.013177611 0.016392481 0.019182168 0.021301173 0.022482943 0.022884969 0.023199726 0.024585061 0.027679164 0.032957956 0.039280802 0.044664267 0.048038013][0.008690346 0.011279393 0.014163546 0.017105859 0.019790247 0.022001419 0.023459718 0.024245121 0.025076464 0.027014762 0.030664019 0.036149118 0.042751834 0.04860051 0.052397236][0.010645958 0.012697427 0.015088864 0.017640866 0.020084683 0.022208489 0.023768406 0.024816297 0.025927804 0.028034519 0.031756073 0.037449114 0.044432122 0.050451033 0.054284185][0.01259963 0.014054898 0.015854638 0.017876003 0.019878276 0.021666542 0.023073327 0.02419652 0.025512829 0.02783389 0.0317754 0.037685566 0.044739649 0.050909027 0.054934986][0.014504239 0.015418462 0.016570967 0.017894555 0.019209038 0.020370323 0.021305773 0.022141892 0.023328181 0.025605608 0.029775165 0.036054932 0.043517824 0.050063096 0.054340057][0.01665362 0.01715301 0.017707467 0.018290836 0.018789072 0.019133426 0.019367684 0.019706022 0.020547144 0.022636279 0.02676877 0.033162396 0.04085879 0.047877952 0.052717589][0.019375354 0.019561574 0.019558154 0.01936236 0.018944342 0.018356185 0.017776914 0.01742734 0.017697185 0.019322626 0.02322847 0.029650126 0.0375247 0.044768333 0.049996346][0.022337746 0.022213299 0.021637622 0.020643815 0.019299716 0.017786656 0.016399477 0.015438445 0.015245404 0.01650624 0.020157542 0.026544519 0.0344735 0.041788928 0.046994925]]...]
INFO - root - 2017-12-09 06:30:44.764607: step 1110, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:25m:24s remains)
INFO - root - 2017-12-09 06:30:51.237068: step 1120, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.630 sec/batch; 58h:01m:57s remains)
INFO - root - 2017-12-09 06:30:57.912074: step 1130, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.667 sec/batch; 61h:21m:14s remains)
INFO - root - 2017-12-09 06:31:04.492016: step 1140, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.654 sec/batch; 60h:14m:25s remains)
INFO - root - 2017-12-09 06:31:11.029796: step 1150, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:46m:38s remains)
INFO - root - 2017-12-09 06:31:17.581695: step 1160, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 60h:22m:23s remains)
INFO - root - 2017-12-09 06:31:24.215776: step 1170, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.670 sec/batch; 61h:42m:26s remains)
INFO - root - 2017-12-09 06:31:30.751534: step 1180, loss = 0.90, batch loss = 0.68 (11.9 examples/sec; 0.675 sec/batch; 62h:06m:40s remains)
INFO - root - 2017-12-09 06:31:37.436970: step 1190, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.673 sec/batch; 61h:57m:04s remains)
INFO - root - 2017-12-09 06:31:44.110645: step 1200, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.669 sec/batch; 61h:32m:53s remains)
2017-12-09 06:31:44.804160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.017167134 -0.017147992 -0.017099526 -0.016863905 -0.016649373 -0.016612589 -0.016865067 -0.017214969 -0.017468389 -0.017550625 -0.01749922 -0.017395535 -0.017290957 -0.017185986 -0.016929168][-0.017154526 -0.017131796 -0.016905246 -0.015871402 -0.014348652 -0.013688548 -0.014274049 -0.015447819 -0.016601436 -0.017369997 -0.017680738 -0.017646147 -0.017468156 -0.017276388 -0.016913271][-0.017112726 -0.017065953 -0.016151499 -0.013076454 -0.0086338455 -0.0058690226 -0.0064310338 -0.0094734691 -0.012773512 -0.015411799 -0.017004175 -0.017628184 -0.017632645 -0.01742306 -0.016992709][-0.016922183 -0.016702114 -0.014148533 -0.0072969496 0.0021543372 0.0085254814 0.0083703343 0.0037313327 -0.0031015938 -0.0093327789 -0.013884018 -0.016514979 -0.01747624 -0.017554346 -0.017152777][-0.016383089 -0.015877636 -0.01133982 -0.00010626391 0.015379786 0.026703678 0.028273243 0.022652123 0.01300968 0.002949113 -0.0064078141 -0.012952548 -0.016279915 -0.01741156 -0.017304296][-0.015047984 -0.014635197 -0.0085977837 0.0060967728 0.027150251 0.04338045 0.048269458 0.042508882 0.030145306 0.016359769 0.0032286644 -0.006503907 -0.012977254 -0.016073359 -0.017017275][-0.013237818 -0.013443226 -0.0072160838 0.0091923624 0.035130806 0.056317441 0.066404566 0.062159367 0.04816 0.030954342 0.013057178 -0.00090050139 -0.010258369 -0.014925521 -0.01676707][-0.009810877 -0.010705938 -0.0058224872 0.0093400162 0.036776319 0.061553389 0.079275087 0.079918072 0.067719005 0.048244506 0.02708856 0.0092510283 -0.0038153585 -0.011265367 -0.014866114][-0.0036638696 -0.0059209168 -0.0038715731 0.0074454714 0.0328076 0.05810684 0.080981582 0.087771386 0.082594529 0.067073084 0.046637036 0.025874089 0.0083207879 -0.0033069765 -0.0093231816][0.0047043506 0.00056080706 -0.0011770725 0.0046583693 0.025099646 0.049220435 0.07646022 0.089840755 0.092913814 0.084795743 0.0681669 0.048817948 0.028868046 0.012799883 0.0032039173][0.012935042 0.007470293 0.0024224762 0.0031211842 0.017553777 0.039028294 0.068763591 0.089911483 0.10305794 0.10406736 0.094240494 0.077701628 0.057450391 0.037764292 0.024250008][0.017840654 0.012020851 0.0055141915 0.0032567512 0.01292643 0.031717341 0.061536424 0.088496327 0.11027915 0.12050314 0.11845944 0.10706551 0.089809224 0.070119329 0.053829342][0.021254063 0.015031196 0.0083148759 0.0051859189 0.011659078 0.029302586 0.059274293 0.089209691 0.11658924 0.13435 0.14044213 0.13595533 0.12398402 0.10696763 0.090220012][0.023587838 0.016521137 0.010708861 0.0079392381 0.012682876 0.028144211 0.055682018 0.084877335 0.11607947 0.14048865 0.15465569 0.15804994 0.15286237 0.13864297 0.12276348][0.028477304 0.020000681 0.014312673 0.012115261 0.016123325 0.029365603 0.053257994 0.079026647 0.10823859 0.13365117 0.15163866 0.16194236 0.16590893 0.15829381 0.14664778]]...]
INFO - root - 2017-12-09 06:31:51.676279: step 1210, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.658 sec/batch; 60h:34m:37s remains)
INFO - root - 2017-12-09 06:31:58.231849: step 1220, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.669 sec/batch; 61h:33m:17s remains)
INFO - root - 2017-12-09 06:32:04.865316: step 1230, loss = 0.88, batch loss = 0.67 (11.6 examples/sec; 0.688 sec/batch; 63h:16m:25s remains)
INFO - root - 2017-12-09 06:32:11.371074: step 1240, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:45m:41s remains)
INFO - root - 2017-12-09 06:32:17.985671: step 1250, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.665 sec/batch; 61h:09m:07s remains)
INFO - root - 2017-12-09 06:32:24.719817: step 1260, loss = 0.89, batch loss = 0.68 (12.0 examples/sec; 0.669 sec/batch; 61h:31m:23s remains)
INFO - root - 2017-12-09 06:32:31.372464: step 1270, loss = 0.90, batch loss = 0.68 (12.0 examples/sec; 0.667 sec/batch; 61h:22m:12s remains)
INFO - root - 2017-12-09 06:32:37.950606: step 1280, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:25m:17s remains)
INFO - root - 2017-12-09 06:32:44.691782: step 1290, loss = 0.91, batch loss = 0.70 (12.3 examples/sec; 0.650 sec/batch; 59h:50m:41s remains)
INFO - root - 2017-12-09 06:32:51.464522: step 1300, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.668 sec/batch; 61h:30m:00s remains)
2017-12-09 06:32:52.206994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.018897945 -0.018889226 -0.018876459 -0.018858971 -0.018836405 -0.018810622 -0.018787343 -0.018774636 -0.018785134 -0.018814269 -0.01884733 -0.018874722 -0.018893695 -0.018903518 -0.018909102][-0.018890779 -0.018877702 -0.018859975 -0.018838843 -0.018815193 -0.018790426 -0.018769376 -0.018759057 -0.018772434 -0.01880512 -0.018841313 -0.018871084 -0.018891625 -0.01890252 -0.018908273][-0.018880596 -0.018861134 -0.018837409 -0.018813597 -0.018788742 -0.018757954 -0.01872332 -0.018699037 -0.018710472 -0.018757395 -0.018813871 -0.01885928 -0.018887784 -0.018899824 -0.018904043][-0.01886801 -0.018840617 -0.01881014 -0.018781923 -0.018740686 -0.018660419 -0.018540082 -0.018439237 -0.018436896 -0.018543679 -0.018687313 -0.018801147 -0.018866433 -0.018893681 -0.018902823][-0.018856145 -0.01882085 -0.01878286 -0.018739998 -0.018646874 -0.018434538 -0.018105585 -0.017827282 -0.01780515 -0.018065479 -0.018414801 -0.018670047 -0.018791998 -0.018832734 -0.018851224][-0.018849386 -0.01880805 -0.018759169 -0.018678129 -0.01847489 -0.018043717 -0.017416148 -0.016900718 -0.01686075 -0.017344004 -0.018010927 -0.018511441 -0.018748388 -0.018819578 -0.018830698][-0.018848212 -0.018804224 -0.018739259 -0.018594367 -0.018234238 -0.017537601 -0.01659671 -0.015866486 -0.015846845 -0.01658405 -0.017589 -0.018349543 -0.0187199 -0.018833186 -0.018848283][-0.01885009 -0.018806737 -0.018729733 -0.018521702 -0.018004192 -0.017061051 -0.015884846 -0.015041593 -0.015094642 -0.016045606 -0.017300906 -0.018246576 -0.018709792 -0.018853294 -0.018876601][-0.018854555 -0.01881505 -0.018741891 -0.01853136 -0.017967701 -0.016938262 -0.015707165 -0.014885446 -0.015008433 -0.016016772 -0.017299272 -0.018258275 -0.018727718 -0.018872017 -0.018896759][-0.018862387 -0.018829741 -0.018772271 -0.018617716 -0.018203147 -0.017407477 -0.016400343 -0.01567783 -0.015736803 -0.016558006 -0.017612714 -0.018392697 -0.018767675 -0.018882934 -0.018905805][-0.018871756 -0.018847084 -0.018803708 -0.018713186 -0.018478289 -0.018009841 -0.01739509 -0.016941473 -0.016979177 -0.017490897 -0.018128807 -0.018590836 -0.018809704 -0.018882496 -0.01890506][-0.018881055 -0.018863797 -0.018834511 -0.018789334 -0.018674316 -0.018453391 -0.018154839 -0.017934628 -0.017960457 -0.018220328 -0.018528549 -0.018744653 -0.018844839 -0.018885484 -0.018904991][-0.018889878 -0.018878724 -0.018859977 -0.018836869 -0.01878817 -0.018707486 -0.018591121 -0.018500557 -0.018496338 -0.018587468 -0.01870786 -0.018807705 -0.018865628 -0.018894847 -0.018908385][-0.018897802 -0.018891107 -0.018880131 -0.018867588 -0.018846193 -0.018819187 -0.018781485 -0.018751644 -0.018747516 -0.018769704 -0.0187958 -0.018819632 -0.018846137 -0.01887713 -0.018901868][-0.018903442 -0.018899834 -0.018894473 -0.018888386 -0.018879844 -0.018871563 -0.018855415 -0.018840913 -0.018836623 -0.018842809 -0.018851338 -0.018858254 -0.018872095 -0.018890666 -0.018906277]]...]
INFO - root - 2017-12-09 06:32:58.888995: step 1310, loss = 0.91, batch loss = 0.69 (11.4 examples/sec; 0.702 sec/batch; 64h:34m:01s remains)
INFO - root - 2017-12-09 06:33:05.465443: step 1320, loss = 0.91, batch loss = 0.70 (11.7 examples/sec; 0.685 sec/batch; 63h:00m:31s remains)
INFO - root - 2017-12-09 06:33:12.343250: step 1330, loss = 0.91, batch loss = 0.70 (11.5 examples/sec; 0.698 sec/batch; 64h:12m:52s remains)
INFO - root - 2017-12-09 06:33:19.093937: step 1340, loss = 0.91, batch loss = 0.70 (11.6 examples/sec; 0.690 sec/batch; 63h:26m:41s remains)
INFO - root - 2017-12-09 06:33:25.846817: step 1350, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:28m:11s remains)
INFO - root - 2017-12-09 06:33:32.500391: step 1360, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:24m:55s remains)
INFO - root - 2017-12-09 06:33:39.364335: step 1370, loss = 0.91, batch loss = 0.70 (12.1 examples/sec; 0.660 sec/batch; 60h:43m:44s remains)
INFO - root - 2017-12-09 06:33:45.897070: step 1380, loss = 0.91, batch loss = 0.69 (12.2 examples/sec; 0.657 sec/batch; 60h:26m:49s remains)
INFO - root - 2017-12-09 06:33:52.530129: step 1390, loss = 0.91, batch loss = 0.70 (11.6 examples/sec; 0.691 sec/batch; 63h:33m:11s remains)
INFO - root - 2017-12-09 06:33:59.173032: step 1400, loss = 0.91, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 60h:59m:39s remains)
2017-12-09 06:33:59.919039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.022869758 -0.023910685 -0.024093311 -0.023084182 -0.021401262 -0.019165186 -0.017427193 -0.016460225 -0.016221242 -0.016076734 -0.016111962 -0.016198181 -0.016371278 -0.016202521 -0.015992504][-0.026490236 -0.029263573 -0.031078409 -0.030592019 -0.028073974 -0.024104271 -0.020242441 -0.016986819 -0.01489352 -0.014259554 -0.01442579 -0.014785117 -0.015166763 -0.015284356 -0.015149392][-0.030221801 -0.035270642 -0.039289184 -0.040544227 -0.039049849 -0.034296066 -0.028390398 -0.022254219 -0.016753433 -0.013715496 -0.012547689 -0.012307767 -0.012901753 -0.013770085 -0.014130441][-0.033050381 -0.040260002 -0.046790976 -0.050436486 -0.050517168 -0.0463078 -0.039612137 -0.031024834 -0.022142831 -0.015760606 -0.0120078 -0.0099494569 -0.010364318 -0.011770908 -0.01265499][-0.034451574 -0.043538027 -0.052732859 -0.059262689 -0.061930366 -0.05923897 -0.053029351 -0.042321675 -0.029866464 -0.019734286 -0.012965718 -0.0084035238 -0.0080023743 -0.0099423053 -0.011490367][-0.034447119 -0.044192366 -0.054996956 -0.064447619 -0.070366427 -0.069890559 -0.065291181 -0.054235034 -0.03884273 -0.024743116 -0.014704322 -0.0075913062 -0.0055422094 -0.0079365848 -0.010652488][-0.033269152 -0.042824879 -0.053866845 -0.064144112 -0.07216727 -0.0744731 -0.071927644 -0.061542552 -0.045269459 -0.028123572 -0.014201129 -0.0051385108 -0.0018726364 -0.0044866269 -0.0086958855][-0.031234816 -0.039463058 -0.049551494 -0.059406187 -0.068012133 -0.0712672 -0.070901051 -0.063018888 -0.048940614 -0.032093674 -0.017058725 -0.0066954335 -0.0025286451 -0.0034814291 -0.0066265687][-0.029112451 -0.035952531 -0.04442507 -0.052286375 -0.059560798 -0.062982321 -0.064060912 -0.058443043 -0.047593836 -0.033499353 -0.019500177 -0.00908702 -0.00478875 -0.0049037226 -0.0069782818][-0.026299508 -0.031578176 -0.038301062 -0.04449185 -0.050117765 -0.052609824 -0.053954519 -0.050185017 -0.042925563 -0.031750347 -0.019790258 -0.010837508 -0.006981439 -0.0067806328 -0.0088404343][-0.023746785 -0.027023111 -0.031582341 -0.035568252 -0.039236143 -0.040713482 -0.042110354 -0.039750148 -0.03526476 -0.027753696 -0.019518359 -0.012285396 -0.00831333 -0.007718957 -0.0092168152][-0.021615509 -0.02385062 -0.027369395 -0.029798599 -0.03130091 -0.031012936 -0.031100284 -0.028744524 -0.026628764 -0.022166612 -0.017401431 -0.013014611 -0.010312397 -0.0095175188 -0.00992596][-0.019752052 -0.020912418 -0.023701062 -0.026026173 -0.02703505 -0.026401313 -0.025667068 -0.022639725 -0.020436596 -0.017237611 -0.015060051 -0.012982497 -0.012210279 -0.011449417 -0.011619708][-0.018442856 -0.019047646 -0.021236323 -0.023488993 -0.024167981 -0.023938648 -0.022702599 -0.020158777 -0.017394304 -0.013838526 -0.011831425 -0.010590737 -0.010974845 -0.011324773 -0.012259938][-0.017841605 -0.017694052 -0.019188415 -0.021364128 -0.022188649 -0.022278272 -0.020663394 -0.018356219 -0.015685719 -0.012107445 -0.010322414 -0.0094494577 -0.0097839441 -0.010562718 -0.012376845]]...]
INFO - root - 2017-12-09 06:34:06.551557: step 1410, loss = 0.91, batch loss = 0.69 (12.2 examples/sec; 0.654 sec/batch; 60h:10m:51s remains)
INFO - root - 2017-12-09 06:34:13.030613: step 1420, loss = 0.91, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 60h:57m:57s remains)
INFO - root - 2017-12-09 06:34:19.684099: step 1430, loss = 0.91, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 60h:18m:09s remains)
INFO - root - 2017-12-09 06:34:26.336930: step 1440, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.627 sec/batch; 57h:41m:50s remains)
INFO - root - 2017-12-09 06:34:32.985451: step 1450, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.658 sec/batch; 60h:29m:42s remains)
INFO - root - 2017-12-09 06:34:39.702604: step 1460, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.672 sec/batch; 61h:46m:43s remains)
INFO - root - 2017-12-09 06:34:46.414517: step 1470, loss = 0.91, batch loss = 0.69 (12.2 examples/sec; 0.654 sec/batch; 60h:06m:22s remains)
INFO - root - 2017-12-09 06:34:52.899757: step 1480, loss = 0.89, batch loss = 0.68 (13.6 examples/sec; 0.588 sec/batch; 54h:06m:25s remains)
INFO - root - 2017-12-09 06:34:59.413401: step 1490, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.642 sec/batch; 59h:04m:28s remains)
INFO - root - 2017-12-09 06:35:05.991100: step 1500, loss = 0.91, batch loss = 0.70 (11.7 examples/sec; 0.684 sec/batch; 62h:55m:41s remains)
2017-12-09 06:35:06.692394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.017349374 -0.015582425 -0.012102999 -0.0070829615 -0.001797827 0.0020340588 0.0035400894 0.0021760203 -0.001479676 -0.00625911 -0.010751741 -0.0141264 -0.016163679 -0.017072827 -0.017346647][-0.014442408 -0.010519926 -0.003716832 0.0054968894 0.015376555 0.022883626 0.026269024 0.024316093 0.017657703 0.0082613882 -0.00092932768 -0.0082250247 -0.01318546 -0.015857726 -0.01695274][-0.00835209 -0.0013519991 0.0096864905 0.024103845 0.039754324 0.052292563 0.058750764 0.0568275 0.046915315 0.031494 0.015382366 0.0019230638 -0.00790145 -0.013728088 -0.016352557][0.0022513829 0.013380149 0.029372992 0.049394161 0.071028337 0.089531161 0.10016565 0.099107645 0.086100295 0.064079739 0.03918729 0.017387023 0.00090279244 -0.0093672015 -0.014354404][0.017180262 0.033358 0.054621674 0.07984785 0.10656172 0.13016781 0.14485259 0.14556384 0.13017584 0.10194902 0.068443105 0.037516616 0.013315091 -0.0024439581 -0.010761157][0.034463935 0.055845104 0.081905946 0.11126016 0.14156763 0.16875435 0.18672071 0.18919428 0.17282552 0.14048298 0.10003931 0.060351424 0.027926372 0.0056966078 -0.0071508437][0.050928317 0.0766382 0.10636526 0.13816892 0.17010058 0.19909091 0.21912335 0.22350079 0.20757622 0.17333379 0.1281683 0.081622876 0.042156532 0.014111182 -0.0033128802][0.061022855 0.089229904 0.12049563 0.15291867 0.18479966 0.21386118 0.23463883 0.240873 0.22689323 0.19412369 0.14773738 0.097843416 0.054069728 0.021536054 0.00038599968][0.061099522 0.089076646 0.11943106 0.15041584 0.18047409 0.20765358 0.22777997 0.2353899 0.2244027 0.19546486 0.15171111 0.1027606 0.058594346 0.024821611 0.002203824][0.051457368 0.076139867 0.10297213 0.13035505 0.1569113 0.18099897 0.19914889 0.20708823 0.19976066 0.17643484 0.13883936 0.095272027 0.054729015 0.023045311 0.0012993291][0.036153205 0.055628978 0.076745428 0.098611087 0.12017235 0.14004956 0.15529798 0.16287667 0.15885678 0.14217775 0.11263163 0.077586822 0.04420843 0.01742941 -0.0016509648][0.0184754 0.032148764 0.047315381 0.063424423 0.079497494 0.094475709 0.10637775 0.11271696 0.11094113 0.10024138 0.079222366 0.053780392 0.029094839 0.00908865 -0.005247836][0.0021744706 0.01040131 0.02003842 0.030640388 0.0414204 0.052199818 0.061006039 0.065956667 0.065597549 0.05926457 0.045925789 0.029587733 0.013337174 2.999045e-05 -0.009428422][-0.0095311161 -0.0056325383 -0.00036664121 0.0056979693 0.012043102 0.019182378 0.024782186 0.028159788 0.028893614 0.025732702 0.018049987 0.0091371238 -0.00018874928 -0.0076880055 -0.012826622][-0.015453039 -0.014078239 -0.011839709 -0.0090313619 -0.0060296562 -0.0015211776 0.0014905166 0.0032388512 0.0039030127 0.0028381217 -0.00080638565 -0.0049005 -0.0091312649 -0.012691867 -0.014984138]]...]
INFO - root - 2017-12-09 06:35:13.340034: step 1510, loss = 0.91, batch loss = 0.69 (11.5 examples/sec; 0.693 sec/batch; 63h:44m:45s remains)
INFO - root - 2017-12-09 06:35:19.947469: step 1520, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.647 sec/batch; 59h:29m:07s remains)
INFO - root - 2017-12-09 06:35:26.774530: step 1530, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.696 sec/batch; 63h:56m:42s remains)
INFO - root - 2017-12-09 06:35:33.432686: step 1540, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.680 sec/batch; 62h:28m:07s remains)
INFO - root - 2017-12-09 06:35:40.155346: step 1550, loss = 0.91, batch loss = 0.69 (11.9 examples/sec; 0.674 sec/batch; 61h:57m:43s remains)
INFO - root - 2017-12-09 06:35:46.764767: step 1560, loss = 0.91, batch loss = 0.70 (12.6 examples/sec; 0.637 sec/batch; 58h:32m:23s remains)
INFO - root - 2017-12-09 06:35:53.432501: step 1570, loss = 0.94, batch loss = 0.72 (11.8 examples/sec; 0.676 sec/batch; 62h:07m:57s remains)
INFO - root - 2017-12-09 06:35:59.889919: step 1580, loss = 0.91, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 59h:56m:50s remains)
INFO - root - 2017-12-09 06:36:06.514973: step 1590, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:07m:06s remains)
INFO - root - 2017-12-09 06:36:13.250628: step 1600, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.690 sec/batch; 63h:25m:18s remains)
2017-12-09 06:36:13.993588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.019021086 -0.019020503 -0.019020623 -0.019020917 -0.01902134 -0.019021846 -0.019022251 -0.019022455 -0.019022487 -0.01902239 -0.019022092 -0.019021777 -0.019021668 -0.019021628 -0.019022152][-0.019021126 -0.019020727 -0.019021522 -0.019022716 -0.019024223 -0.0190259 -0.019027473 -0.019028569 -0.019029176 -0.019029276 -0.019028747 -0.019027803 -0.019026674 -0.019025398 -0.01902451][-0.019021552 -0.01902153 -0.019023024 -0.019025192 -0.019027881 -0.019030921 -0.019034008 -0.019036422 -0.019038282 -0.019039435 -0.019039482 -0.019038508 -0.019036563 -0.019033648 -0.019030528][-0.019022113 -0.019022316 -0.019024147 -0.019026848 -0.019030314 -0.019034337 -0.019038755 -0.019042816 -0.019046498 -0.019049412 -0.019050891 -0.019050771 -0.019048812 -0.019044578 -0.01903894][-0.01902207 -0.019022251 -0.019023698 -0.01902598 -0.019029222 -0.019033426 -0.019038726 -0.019044377 -0.019050201 -0.019055411 -0.019059137 -0.019060686 -0.019059496 -0.019054847 -0.019047359][-0.019021574 -0.019021241 -0.019021733 -0.019022729 -0.0190246 -0.019027684 -0.019032748 -0.019039169 -0.019046767 -0.01905432 -0.019060487 -0.019064335 -0.019064911 -0.019060852 -0.019052761][-0.019020887 -0.019020274 -0.019019864 -0.01901946 -0.019019529 -0.019020351 -0.019023625 -0.019029468 -0.019037848 -0.019047245 -0.019055929 -0.01906215 -0.019064439 -0.019061482 -0.019054027][-0.01902047 -0.019019851 -0.019019164 -0.01901824 -0.019017316 -0.019016452 -0.019017439 -0.019021492 -0.019029388 -0.019039392 -0.019049481 -0.019057253 -0.019060783 -0.019058675 -0.019052222][-0.019020632 -0.019020321 -0.019019958 -0.019019373 -0.019018753 -0.019018043 -0.019018443 -0.019020978 -0.019027039 -0.019035552 -0.019044656 -0.019052114 -0.019055646 -0.01905386 -0.019048473][-0.019021325 -0.019021589 -0.019021887 -0.019022031 -0.019022148 -0.019022217 -0.019023037 -0.019025324 -0.019029507 -0.019035066 -0.019041475 -0.019046957 -0.019049494 -0.019047864 -0.01904349][-0.019021971 -0.019022638 -0.019023607 -0.019024536 -0.019025374 -0.019026013 -0.019027082 -0.01902898 -0.019031713 -0.019034807 -0.019038416 -0.0190416 -0.019042984 -0.019041603 -0.019038353][-0.019022405 -0.019023214 -0.019024439 -0.019025784 -0.019026959 -0.01902773 -0.019028705 -0.019030098 -0.019031802 -0.019033637 -0.0190356 -0.019037163 -0.019037459 -0.019036083 -0.019033724][-0.019022623 -0.019023143 -0.019024162 -0.019025313 -0.019026332 -0.019027023 -0.019027822 -0.01902882 -0.019029912 -0.019031156 -0.019032367 -0.019033138 -0.019032851 -0.019031521 -0.019029818][-0.019022673 -0.019022711 -0.019023305 -0.019023966 -0.019024566 -0.019025013 -0.019025585 -0.019026261 -0.019026909 -0.019027535 -0.0190281 -0.019028407 -0.019028047 -0.019027151 -0.019026224][-0.019023886 -0.019023538 -0.019023808 -0.019024085 -0.019024387 -0.019024586 -0.019024892 -0.019025307 -0.019025663 -0.019025754 -0.019025683 -0.01902538 -0.01902472 -0.019024132 -0.019023929]]...]
INFO - root - 2017-12-09 06:36:20.736800: step 1610, loss = 0.89, batch loss = 0.68 (12.0 examples/sec; 0.669 sec/batch; 61h:31m:47s remains)
INFO - root - 2017-12-09 06:36:27.366682: step 1620, loss = 0.91, batch loss = 0.70 (11.9 examples/sec; 0.672 sec/batch; 61h:48m:25s remains)
INFO - root - 2017-12-09 06:36:34.009743: step 1630, loss = 0.88, batch loss = 0.67 (11.9 examples/sec; 0.674 sec/batch; 61h:59m:17s remains)
INFO - root - 2017-12-09 06:36:40.661797: step 1640, loss = 0.91, batch loss = 0.70 (10.9 examples/sec; 0.732 sec/batch; 67h:16m:08s remains)
INFO - root - 2017-12-09 06:36:47.202056: step 1650, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:05m:18s remains)
INFO - root - 2017-12-09 06:36:53.918925: step 1660, loss = 0.91, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 60h:53m:23s remains)
INFO - root - 2017-12-09 06:37:00.672288: step 1670, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.677 sec/batch; 62h:15m:34s remains)
INFO - root - 2017-12-09 06:37:07.416823: step 1680, loss = 0.91, batch loss = 0.69 (12.2 examples/sec; 0.657 sec/batch; 60h:24m:52s remains)
INFO - root - 2017-12-09 06:37:13.863930: step 1690, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:38m:10s remains)
INFO - root - 2017-12-09 06:37:20.514566: step 1700, loss = 0.91, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:17m:36s remains)
2017-12-09 06:37:21.329997: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.082833081 0.079365507 0.0747769 0.076984182 0.079849884 0.0827709 0.089884728 0.0882466 0.08483097 0.075707093 0.070180386 0.063543916 0.059598606 0.059384186 0.059841324][0.05173697 0.051439893 0.050850715 0.056129027 0.062417787 0.065633893 0.070626646 0.067801073 0.063676611 0.056546662 0.052571427 0.048231166 0.046182241 0.04540642 0.046565946][0.032596823 0.034425016 0.036486123 0.042544708 0.050182868 0.058133457 0.061516691 0.05841988 0.053709704 0.049591128 0.048478056 0.046944719 0.049058456 0.048908029 0.05178513][0.015043393 0.016971834 0.020952962 0.030375447 0.039828643 0.050217319 0.054006394 0.055127349 0.054310847 0.053040195 0.054603357 0.055307861 0.059062105 0.060030323 0.06420581][0.000648614 0.0068797134 0.016850818 0.029304169 0.041041367 0.054423522 0.058257539 0.059667189 0.05872212 0.062402297 0.068258658 0.071554512 0.078204155 0.082256228 0.091185912][-0.0051425956 0.0051537994 0.019950356 0.036874373 0.052215863 0.067223325 0.070081264 0.069655 0.066332549 0.069509551 0.074835449 0.079856753 0.091391936 0.10027623 0.11502595][-0.0040842118 0.011790236 0.031687487 0.052102383 0.069241613 0.082966 0.0848825 0.077357844 0.068135589 0.066189751 0.065789819 0.068975657 0.08036004 0.096568212 0.11753364][0.00051767938 0.019195728 0.040637359 0.060951706 0.078347042 0.092451721 0.0939756 0.083588213 0.069931328 0.061517436 0.052491058 0.049012955 0.058066282 0.076230437 0.10215077][0.007052144 0.025505401 0.044371936 0.060007278 0.0720873 0.079070807 0.075935915 0.062082816 0.0462231 0.034414936 0.024507228 0.022202697 0.031480912 0.050457504 0.075876981][0.011301981 0.027479541 0.042622264 0.053595062 0.059918109 0.06116106 0.053449955 0.038486816 0.022407323 0.010086112 0.0017882101 -0.00032876991 0.008167088 0.024465721 0.047268223][0.011132427 0.023233879 0.033121336 0.03874018 0.039207261 0.0353015 0.026468921 0.014579702 0.0030108839 -0.0052937679 -0.0090719862 -0.0088438587 -0.0021103304 0.010549268 0.029665396][0.0039323047 0.012012452 0.017801791 0.019709598 0.01719676 0.011152441 0.003056353 -0.0052332124 -0.012132414 -0.016049644 -0.016351394 -0.013839336 -0.0082482155 0.0018602889 0.017118104][-0.0045935158 0.00015331432 0.0032509416 0.0035107154 0.00071309507 -0.0041718986 -0.00974573 -0.014544317 -0.017991152 -0.01940085 -0.018640997 -0.016350877 -0.01256275 -0.0045709442 0.0074143428][-0.013299035 -0.01119314 -0.00983525 -0.0099499393 -0.011711497 -0.014404319 -0.016938128 -0.018463766 -0.019336097 -0.019343656 -0.018619698 -0.017468031 -0.016198253 -0.011689248 -0.0044113724][-0.018717783 -0.01843603 -0.018287545 -0.018492475 -0.019111989 -0.019840265 -0.02023278 -0.019721223 -0.019416997 -0.018923678 -0.018328233 -0.017706119 -0.017511873 -0.015449172 -0.012392458]]...]
INFO - root - 2017-12-09 06:37:27.914796: step 1710, loss = 0.91, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 60h:17m:13s remains)
INFO - root - 2017-12-09 06:37:34.447850: step 1720, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.665 sec/batch; 61h:06m:39s remains)
INFO - root - 2017-12-09 06:37:41.148238: step 1730, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:08m:42s remains)
INFO - root - 2017-12-09 06:37:47.846789: step 1740, loss = 0.91, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 60h:16m:23s remains)
INFO - root - 2017-12-09 06:37:54.575770: step 1750, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.674 sec/batch; 61h:55m:33s remains)
INFO - root - 2017-12-09 06:38:01.339974: step 1760, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.687 sec/batch; 63h:08m:03s remains)
INFO - root - 2017-12-09 06:38:08.011375: step 1770, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.688 sec/batch; 63h:10m:46s remains)
INFO - root - 2017-12-09 06:38:14.527227: step 1780, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.677 sec/batch; 62h:11m:52s remains)
INFO - root - 2017-12-09 06:38:20.936202: step 1790, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:38m:24s remains)
INFO - root - 2017-12-09 06:38:27.670281: step 1800, loss = 0.91, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 60h:55m:48s remains)
2017-12-09 06:38:28.444342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.019784771 -0.019809529 -0.019819673 -0.019829066 -0.019876352 -0.019454038 -0.017226832 -0.011883563 -0.0044169463 0.0042835306 0.0090282448 0.010154057 0.005748909 -0.0018116906 -0.0094498359][-0.019698832 -0.019717056 -0.019710464 -0.019751109 -0.01972576 -0.018772515 -0.015790483 -0.0099884467 -0.0031863786 0.0046262816 0.0081277285 0.0079211015 0.0031622183 -0.0034483261 -0.010005783][-0.019625233 -0.019636292 -0.019610815 -0.019295709 -0.017902019 -0.015126795 -0.010000432 -0.0019645654 0.005896613 0.013273476 0.015056165 0.013632534 0.0076919105 0.00022707321 -0.0062618582][-0.01951771 -0.01951528 -0.019384859 -0.018106103 -0.014039071 -0.0067382529 0.0040239878 0.017914603 0.028870476 0.036808938 0.036976919 0.032550044 0.02260468 0.011473676 0.0024957117][-0.019393247 -0.019341197 -0.018882686 -0.015443816 -0.0066955667 0.0073076393 0.026026702 0.047597088 0.063990243 0.074766785 0.073508292 0.064813316 0.049143061 0.032042563 0.017393949][-0.0192967 -0.019134589 -0.017843498 -0.011445136 0.0028121881 0.025338063 0.053868435 0.084074378 0.10637777 0.11893854 0.11662205 0.10407015 0.083012611 0.059698388 0.038898475][-0.019244077 -0.018776638 -0.016292367 -0.0063437428 0.013516774 0.044079348 0.081223167 0.11967815 0.14800023 0.16224529 0.15804015 0.14135455 0.11683033 0.08849363 0.063272469][-0.01924428 -0.018559633 -0.014845582 -0.0011258274 0.024182534 0.062472753 0.10753163 0.15377979 0.18781793 0.2046175 0.19952254 0.17974657 0.1504689 0.11702956 0.088313937][-0.019350629 -0.018393749 -0.013261017 0.0036491361 0.033037178 0.077595346 0.1291334 0.18159904 0.22079566 0.24066502 0.23455098 0.21308628 0.18081957 0.14417368 0.11192385][-0.019255219 -0.017458603 -0.010259781 0.010122837 0.043480009 0.092505358 0.14750785 0.20288822 0.24456635 0.26486748 0.25790837 0.23595238 0.20271143 0.1669029 0.1345143][-0.018615082 -0.015185849 -0.0051194 0.018497305 0.054053307 0.10559771 0.16225581 0.21815732 0.2596927 0.27880436 0.27019659 0.2471506 0.21266505 0.17896987 0.14852896][-0.017234124 -0.011713421 0.0011197291 0.027066777 0.063019492 0.11316197 0.1658041 0.21760741 0.2563386 0.2741324 0.26527202 0.24261415 0.20863445 0.17812662 0.15109809][-0.014606504 -0.0066405647 0.0089416243 0.036192402 0.070977353 0.11713447 0.16286616 0.204741 0.23372346 0.24474022 0.23460257 0.21418393 0.18497521 0.16218589 0.14189097][-0.01018647 0.000672115 0.018911364 0.046345413 0.078593634 0.11793455 0.15371613 0.1832058 0.20072722 0.20293637 0.18979697 0.1708127 0.14725035 0.13224798 0.11989657][-0.00617426 0.0078214314 0.028565535 0.056055687 0.084967889 0.1169583 0.14191645 0.15868782 0.16485901 0.15859331 0.14275604 0.12420101 0.10518539 0.095469482 0.0895458]]...]
INFO - root - 2017-12-09 06:38:35.057997: step 1810, loss = 0.91, batch loss = 0.69 (11.5 examples/sec; 0.693 sec/batch; 63h:41m:49s remains)
INFO - root - 2017-12-09 06:38:41.537522: step 1820, loss = 0.90, batch loss = 0.69 (13.0 examples/sec; 0.615 sec/batch; 56h:27m:49s remains)
INFO - root - 2017-12-09 06:38:48.108154: step 1830, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.626 sec/batch; 57h:29m:40s remains)
INFO - root - 2017-12-09 06:38:54.800038: step 1840, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.677 sec/batch; 62h:10m:44s remains)
INFO - root - 2017-12-09 06:39:01.471538: step 1850, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.691 sec/batch; 63h:28m:46s remains)
INFO - root - 2017-12-09 06:39:08.229428: step 1860, loss = 0.91, batch loss = 0.69 (11.4 examples/sec; 0.702 sec/batch; 64h:30m:43s remains)
INFO - root - 2017-12-09 06:39:14.939129: step 1870, loss = 0.90, batch loss = 0.68 (11.6 examples/sec; 0.693 sec/batch; 63h:36m:34s remains)
INFO - root - 2017-12-09 06:39:21.506949: step 1880, loss = 0.91, batch loss = 0.69 (12.3 examples/sec; 0.653 sec/batch; 59h:55m:48s remains)
INFO - root - 2017-12-09 06:39:27.879964: step 1890, loss = 0.91, batch loss = 0.69 (12.0 examples/sec; 0.666 sec/batch; 61h:08m:19s remains)
INFO - root - 2017-12-09 06:39:34.459110: step 1900, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.657 sec/batch; 60h:18m:32s remains)
2017-12-09 06:39:35.184146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.020580254 -0.021009916 -0.021225549 -0.022017555 -0.022785416 -0.023668498 -0.024026031 -0.024457429 -0.024579138 -0.024393259 -0.024751736 -0.024799319 -0.025262868 -0.024828356 -0.024483822][-0.019834481 -0.020434737 -0.020775393 -0.021320373 -0.021720516 -0.022172622 -0.022211161 -0.022098284 -0.02192156 -0.021898258 -0.022032466 -0.021964718 -0.02213346 -0.022125367 -0.022008216][-0.01858105 -0.018946063 -0.019129744 -0.01976105 -0.019895785 -0.020049352 -0.019938624 -0.020220442 -0.020708976 -0.021308813 -0.021760562 -0.022170279 -0.02254598 -0.022629404 -0.022715021][-0.018814597 -0.019093063 -0.019201165 -0.019649668 -0.019405881 -0.0192639 -0.018509075 -0.018329376 -0.01895117 -0.020095391 -0.021243092 -0.02221713 -0.023181152 -0.02366128 -0.023571461][-0.018465314 -0.018267076 -0.018177804 -0.018248409 -0.017973203 -0.01780913 -0.017101552 -0.017102413 -0.017689165 -0.019095762 -0.020349085 -0.021355392 -0.022498839 -0.023203049 -0.023475377][-0.017390009 -0.016710507 -0.016273629 -0.015904529 -0.015490154 -0.015461162 -0.01521607 -0.015529886 -0.016128834 -0.017612558 -0.019169211 -0.020469576 -0.021498971 -0.0221125 -0.022301052][-0.017978732 -0.017142411 -0.016548607 -0.016046971 -0.015571116 -0.0152428 -0.015073303 -0.015469697 -0.015579803 -0.016111914 -0.016734764 -0.017648816 -0.018610425 -0.019287353 -0.019428987][-0.019103596 -0.018636858 -0.018112874 -0.017563067 -0.017433356 -0.017066663 -0.016817857 -0.016734766 -0.016283236 -0.015923273 -0.015619161 -0.016147349 -0.016670119 -0.017299408 -0.017561721][-0.019964874 -0.019852791 -0.019611245 -0.019416414 -0.019272709 -0.019142684 -0.018863657 -0.018241214 -0.017314149 -0.016463438 -0.015695447 -0.015827628 -0.016231239 -0.017049761 -0.017662143][-0.019988211 -0.019840455 -0.01955674 -0.019355673 -0.01955056 -0.019517811 -0.019544238 -0.019395236 -0.018774914 -0.018309124 -0.017820122 -0.017823324 -0.017966807 -0.018497102 -0.019096117][-0.019866176 -0.019592697 -0.019271538 -0.018949214 -0.018978784 -0.019433783 -0.019865448 -0.02010159 -0.019847274 -0.019640096 -0.019396909 -0.019332163 -0.019322876 -0.019425586 -0.019630866][-0.020100199 -0.019549297 -0.019019315 -0.018604666 -0.018708035 -0.018932339 -0.019627482 -0.020228518 -0.02013048 -0.020037135 -0.019890836 -0.019842641 -0.019890947 -0.020007912 -0.020167494][-0.020589894 -0.020355592 -0.019700209 -0.01926443 -0.019361947 -0.019665634 -0.020086043 -0.020207988 -0.020193573 -0.020165063 -0.020164145 -0.020201329 -0.020228436 -0.02027102 -0.020310039][-0.020476682 -0.020516545 -0.02010389 -0.01961033 -0.019149 -0.01928764 -0.019737853 -0.020047626 -0.020200744 -0.020163877 -0.020137727 -0.020172676 -0.020178949 -0.020115877 -0.020116454][-0.019804727 -0.019888502 -0.019853432 -0.019653779 -0.019111242 -0.019182332 -0.01932391 -0.019824451 -0.01989653 -0.020000421 -0.020060176 -0.020133493 -0.020181773 -0.019949282 -0.01987903]]...]
INFO - root - 2017-12-09 06:39:41.850509: step 1910, loss = 0.91, batch loss = 0.69 (12.0 examples/sec; 0.668 sec/batch; 61h:21m:10s remains)
INFO - root - 2017-12-09 06:39:48.510990: step 1920, loss = 0.91, batch loss = 0.69 (13.9 examples/sec; 0.576 sec/batch; 52h:52m:16s remains)
