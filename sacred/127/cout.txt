INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "127"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-batch32
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv2/split:0", shape=(32, 29, 29, 48), dtype=float32)
Tensor("siamese_fc/conv2/def/transpose:0", shape=(32, 48, 29, 29), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b1/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc/conv2/def/transpose_1:0", shape=(32, 200, 29, 29), dtype=float32)
Tensor("siamese_fc/conv2/def/transpose_2:0", shape=(32, 48, 29, 29), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b2/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc/conv2/def/transpose_3:0", shape=(32, 200, 29, 29), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv2/split:0", shape=(32, 57, 57, 48), dtype=float32)
Tensor("siamese_fc_1/conv2/def/transpose:0", shape=(32, 48, 57, 57), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b1/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc_1/conv2/def/transpose_1:0", shape=(32, 200, 57, 57), dtype=float32)
Tensor("siamese_fc_1/conv2/def/transpose_2:0", shape=(32, 48, 57, 57), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b2/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc_1/conv2/def/transpose_3:0", shape=(32, 200, 57, 57), dtype=float32)
Tensor("detection/add:0", shape=(32, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-07 02:47:23.964311: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:47:23.964349: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:47:23.964355: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:47:23.964359: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:47:23.964363: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:47:24.405779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 6.38GiB
2017-12-07 02:47:24.405811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-07 02:47:24.405818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-07 02:47:24.405826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 83125 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-07 02:47:41.394440: step 0, loss = 0.74, batch loss = 0.67 (2.1 examples/sec; 14.911 sec/batch; 344h:17m:19s remains)
2017-12-07 02:47:43.736528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0092578 -2.0211437 -2.2040262 -2.479167 -2.6816058 -2.7766957 -2.8025069 -2.7916563 -2.854497 -2.8314209 -2.6992316 -2.6815634 -2.6004686 -2.5557396 -2.6863852][-3.1281123 -3.1682081 -3.3190417 -3.5271883 -3.6509 -3.7323856 -3.8057261 -3.7849426 -3.7997618 -3.7913187 -3.7098627 -3.6152558 -3.4173474 -3.2830036 -3.2843585][-3.5661814 -3.6631191 -3.7829478 -3.8589175 -3.8513906 -3.8602781 -3.9495134 -3.9708421 -4.0054846 -4.0224085 -4.0161538 -3.9871573 -3.8476255 -3.6767864 -3.5373092][-3.3493807 -3.4211898 -3.5108819 -3.5127542 -3.465163 -3.4140441 -3.4325366 -3.4751339 -3.588304 -3.7044625 -3.8393397 -4.0027027 -4.0537477 -3.9389119 -3.7547493][-3.1632218 -3.1092143 -3.143363 -3.1529408 -3.1897454 -3.194716 -3.1393371 -3.1048656 -3.2058458 -3.4031658 -3.7117381 -4.0474043 -4.1550021 -3.9865785 -3.7045977][-2.6379178 -2.489336 -2.4418309 -2.3795536 -2.3497837 -2.3086565 -2.176908 -2.0777683 -2.140204 -2.32858 -2.6623013 -3.027648 -3.0459452 -2.7687645 -2.4101624][-1.6257141 -1.4281983 -1.3352139 -1.1647317 -0.90226984 -0.60246062 -0.29318476 -0.16396713 -0.30202055 -0.56487274 -0.91882253 -1.3028982 -1.3415301 -1.16079 -0.95671105][-1.1208742 -0.92130351 -0.84978008 -0.69399095 -0.36369705 0.017656803 0.3311944 0.39727879 0.17154694 -0.16178513 -0.56075168 -0.98452282 -1.1280785 -1.0808916 -1.0134671][-1.6562381 -1.5946972 -1.6156242 -1.5572414 -1.3483381 -1.1478634 -1.0507181 -1.1231301 -1.3622546 -1.6237566 -1.8759475 -2.120662 -2.1701677 -2.031651 -1.8413115][-2.7761614 -2.8708134 -2.9315171 -2.8785939 -2.7030637 -2.5593255 -2.523658 -2.6144929 -2.7862134 -2.9643207 -3.0769525 -3.1498861 -3.0774546 -2.8189836 -2.4459395][-3.4308257 -3.5942471 -3.6530747 -3.5948367 -3.4552832 -3.3061819 -3.2025876 -3.1750152 -3.1949978 -3.2756026 -3.3300042 -3.3569663 -3.2881842 -3.0524046 -2.6267915][-3.2047048 -3.3274815 -3.3781569 -3.3812387 -3.3568904 -3.2764864 -3.1736109 -3.1047559 -3.0638041 -3.0886426 -3.1246362 -3.151099 -3.1534443 -3.0415826 -2.720386][-2.9131904 -2.9455771 -2.9641237 -3.0013981 -3.0432749 -3.026268 -2.9741592 -2.9401374 -2.9122469 -2.9065292 -2.9091909 -2.8944695 -2.9160709 -2.8919291 -2.6936011][-2.9196944 -2.8752532 -2.8333955 -2.8327556 -2.8457642 -2.8317108 -2.8076949 -2.782887 -2.74676 -2.6974077 -2.6176729 -2.5159016 -2.4920366 -2.4822574 -2.3580148][-2.9986949 -2.9043229 -2.8050761 -2.7350383 -2.6746345 -2.6281109 -2.6125298 -2.5862489 -2.5346909 -2.4616914 -2.3383152 -2.2116156 -2.1956432 -2.2463713 -2.2471628]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-batch32/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-batch32/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 02:48:19.985850: step 10, loss = 0.80, batch loss = 0.73 (9.1 examples/sec; 3.524 sec/batch; 81h:21m:31s remains)
INFO - root - 2017-12-07 02:48:55.593258: step 20, loss = 0.81, batch loss = 0.74 (8.9 examples/sec; 3.609 sec/batch; 83h:18m:56s remains)
INFO - root - 2017-12-07 02:49:34.135755: step 30, loss = 0.75, batch loss = 0.68 (9.0 examples/sec; 3.545 sec/batch; 81h:49m:18s remains)
INFO - root - 2017-12-07 02:50:09.855609: step 40, loss = 0.72, batch loss = 0.64 (9.1 examples/sec; 3.523 sec/batch; 81h:18m:32s remains)
INFO - root - 2017-12-07 02:50:45.347483: step 50, loss = 0.74, batch loss = 0.66 (8.9 examples/sec; 3.586 sec/batch; 82h:45m:42s remains)
INFO - root - 2017-12-07 02:51:20.952062: step 60, loss = 0.82, batch loss = 0.74 (9.0 examples/sec; 3.570 sec/batch; 82h:22m:56s remains)
INFO - root - 2017-12-07 02:51:56.736652: step 70, loss = 0.77, batch loss = 0.70 (8.9 examples/sec; 3.586 sec/batch; 82h:44m:17s remains)
INFO - root - 2017-12-07 02:52:32.404790: step 80, loss = 0.66, batch loss = 0.59 (8.9 examples/sec; 3.578 sec/batch; 82h:31m:41s remains)
INFO - root - 2017-12-07 02:53:08.053731: step 90, loss = 0.77, batch loss = 0.69 (8.8 examples/sec; 3.619 sec/batch; 83h:28m:58s remains)
INFO - root - 2017-12-07 02:53:44.027285: step 100, loss = 0.73, batch loss = 0.66 (8.9 examples/sec; 3.594 sec/batch; 82h:52m:30s remains)
2017-12-07 02:53:46.152869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3508768 -1.5658355 -1.8838789 -1.8858392 -1.1795251 -0.30207205 -0.12334633 -0.75547719 -1.504724 -1.8102224 -1.5881419 -1.0583594 -0.77011108 -1.0908053 -1.5699222][-1.4228866 -1.7002056 -2.0692713 -2.0014677 -1.2349052 -0.29652834 -0.026347637 -0.6599648 -1.5612783 -2.0450218 -1.9127109 -1.296633 -0.82337189 -1.0344613 -1.4789324][-1.2503631 -1.5379593 -1.8861721 -1.8168533 -1.2344315 -0.52707386 -0.34262991 -0.980047 -1.9276037 -2.4341278 -2.2673402 -1.5123188 -0.90007353 -1.0708339 -1.4826629][-1.1052837 -1.3932257 -1.7706077 -1.8683362 -1.6104441 -1.1252477 -0.90678835 -1.3429074 -2.1268332 -2.476501 -2.1259124 -1.2083056 -0.59500146 -0.91069794 -1.4304001][-1.2236583 -1.4511755 -1.927047 -2.2613966 -2.2245929 -1.7189567 -1.204483 -1.274467 -1.9187996 -2.2451379 -1.7501643 -0.72667646 -0.22002649 -0.74594688 -1.3621194][-1.2068081 -1.2321434 -1.7712412 -2.2400298 -2.206145 -1.5552454 -0.6917634 -0.50314665 -1.2705359 -1.8178914 -1.3460884 -0.35363674 -0.057806015 -0.74258471 -1.3860168][-1.0170813 -0.71123648 -1.1674569 -1.633028 -1.4971092 -0.61519265 0.70353985 1.0778799 -0.022863865 -0.947325 -0.63681793 0.21921539 0.25026464 -0.66513634 -1.4526427][-0.91475058 -0.41291666 -0.78126144 -1.2504308 -1.1248035 -0.20466328 1.4064579 1.9605594 0.68312979 -0.38507509 -0.092813969 0.78952503 0.77210236 -0.23989582 -1.1555717][-0.96743274 -0.45741606 -0.71649575 -1.2147424 -1.3580654 -0.79138684 0.55602646 1.1522002 0.14407396 -0.73440051 -0.35668659 0.7093668 0.98127031 0.22479868 -0.56561255][-1.0046124 -0.54617381 -0.72885323 -1.2382751 -1.6155343 -1.4619656 -0.59056711 -0.068698883 -0.70614219 -1.3944292 -1.1126099 -0.051422119 0.45355749 -0.0038223267 -0.6162169][-0.73375177 -0.45295095 -0.76092553 -1.3307683 -1.7607722 -1.8148041 -1.27982 -0.83728147 -1.1760719 -1.7115664 -1.641202 -0.77414417 -0.2178731 -0.5475018 -1.1553462][-0.1424284 -0.13337469 -0.62274647 -1.2192814 -1.6241877 -1.7568049 -1.4240022 -1.0387502 -1.1936979 -1.6702478 -1.7761152 -1.0707731 -0.44373298 -0.65727162 -1.3696632][0.36383343 0.096301556 -0.47334695 -0.94962525 -1.2448761 -1.4124577 -1.2085607 -0.8044045 -0.82331848 -1.3173952 -1.6320543 -1.1608117 -0.51520061 -0.56316447 -1.2369773][0.31650496 -0.095598221 -0.5958147 -0.84818077 -0.96402216 -1.1290236 -0.98626065 -0.5325017 -0.36655378 -0.80007744 -1.2905798 -1.1521134 -0.67453623 -0.628093 -1.1192908][-0.31101036 -0.62962031 -0.98079467 -1.0559697 -1.0667968 -1.2106626 -1.123888 -0.68536353 -0.35265398 -0.56727505 -1.0160148 -1.0587065 -0.77702188 -0.72257733 -1.0992019]]...]
INFO - root - 2017-12-07 02:54:21.808608: step 110, loss = 0.73, batch loss = 0.66 (8.9 examples/sec; 3.576 sec/batch; 82h:27m:44s remains)
INFO - root - 2017-12-07 02:54:57.255656: step 120, loss = 0.75, batch loss = 0.68 (8.9 examples/sec; 3.597 sec/batch; 82h:56m:43s remains)
INFO - root - 2017-12-07 02:55:32.891519: step 130, loss = 0.81, batch loss = 0.74 (9.0 examples/sec; 3.569 sec/batch; 82h:16m:46s remains)
INFO - root - 2017-12-07 02:56:08.595917: step 140, loss = 0.66, batch loss = 0.59 (8.9 examples/sec; 3.599 sec/batch; 82h:57m:13s remains)
INFO - root - 2017-12-07 02:56:47.166470: step 150, loss = 0.93, batch loss = 0.86 (9.0 examples/sec; 3.573 sec/batch; 82h:20m:40s remains)
INFO - root - 2017-12-07 02:57:22.766231: step 160, loss = 0.89, batch loss = 0.82 (8.9 examples/sec; 3.607 sec/batch; 83h:07m:18s remains)
INFO - root - 2017-12-07 02:57:58.476459: step 170, loss = 0.78, batch loss = 0.71 (8.8 examples/sec; 3.621 sec/batch; 83h:26m:36s remains)
INFO - root - 2017-12-07 02:58:34.136985: step 180, loss = 0.67, batch loss = 0.59 (9.1 examples/sec; 3.533 sec/batch; 81h:24m:23s remains)
INFO - root - 2017-12-07 02:59:09.890002: step 190, loss = 0.80, batch loss = 0.73 (9.0 examples/sec; 3.546 sec/batch; 81h:42m:06s remains)
INFO - root - 2017-12-07 02:59:45.526458: step 200, loss = 0.78, batch loss = 0.71 (8.9 examples/sec; 3.580 sec/batch; 82h:27m:51s remains)
2017-12-07 02:59:47.606585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4130538 -3.281137 -3.1726701 -3.052382 -2.9114628 -2.7661562 -2.5807443 -2.3647664 -2.1969337 -2.1193829 -2.0984094 -1.9704294 -1.7800486 -1.8075328 -2.0031826][-3.5375004 -3.5186589 -3.4961641 -3.38757 -3.2015431 -3.0094376 -2.7762847 -2.5013256 -2.2868688 -2.1462634 -2.0241411 -1.8448489 -1.7203081 -1.8450456 -2.0485027][-3.5010722 -3.6944542 -3.8380473 -3.8117611 -3.6462917 -3.4808047 -3.3072987 -3.1060824 -2.9907212 -2.9254351 -2.8309209 -2.730514 -2.763464 -2.964586 -3.0548482][-3.2078676 -3.557744 -3.8067164 -3.8128397 -3.6421461 -3.4867463 -3.3872776 -3.2894764 -3.300451 -3.3403208 -3.3193493 -3.3537769 -3.5560272 -3.8059626 -3.8414221][-2.92476 -3.2800736 -3.5273938 -3.5324862 -3.3401055 -3.153358 -3.0518532 -2.9619098 -2.9903469 -3.0905421 -3.1724441 -3.3295782 -3.6045394 -3.8346663 -3.8777063][-2.5624821 -2.795408 -2.956795 -2.9502769 -2.7734466 -2.5605664 -2.388834 -2.223284 -2.1902723 -2.3311679 -2.50969 -2.7715235 -3.0750294 -3.2494838 -3.3179545][-2.8082089 -2.8899543 -2.9292881 -2.9074998 -2.8092971 -2.6071837 -2.3162918 -2.0068092 -1.8479278 -1.9494262 -2.1425829 -2.4417648 -2.7735755 -2.9304049 -2.9825521][-3.5363483 -3.4196796 -3.1878467 -3.0173492 -2.9950075 -2.9473028 -2.718472 -2.4320135 -2.2674932 -2.3131511 -2.4251976 -2.6645916 -2.9721742 -3.1028752 -3.1098971][-3.4019148 -3.130682 -2.6037285 -2.1827753 -2.1295378 -2.2407751 -2.2175117 -2.1450741 -2.116447 -2.1289158 -2.1407077 -2.3142009 -2.6541138 -2.8911519 -3.035686][-2.6936946 -2.3936343 -1.7872074 -1.2282088 -1.0252216 -1.103256 -1.2103274 -1.3498776 -1.4811034 -1.5002937 -1.4624417 -1.6045623 -1.9716547 -2.3074946 -2.6024163][-2.413712 -2.2737491 -1.8925395 -1.4924674 -1.3010633 -1.3471086 -1.4864719 -1.6782525 -1.8440721 -1.8454564 -1.750036 -1.7659674 -1.9392574 -2.0849202 -2.24158][-2.5849371 -2.6393342 -2.5516684 -2.4293332 -2.3919785 -2.4872296 -2.6182637 -2.7310395 -2.7963595 -2.7389247 -2.627306 -2.5724583 -2.5731122 -2.4908741 -2.3835115][-2.7145815 -2.8311844 -2.8800552 -2.8996575 -2.9300051 -2.99911 -3.0555348 -3.0729704 -3.0628986 -3.0071793 -2.9563971 -2.9517341 -2.9580648 -2.8758578 -2.7151475][-2.6969714 -2.7962666 -2.8573871 -2.8964109 -2.9229605 -2.9506159 -2.964088 -2.95509 -2.9418011 -2.9302766 -2.9321508 -2.9683151 -3.0272057 -3.0483446 -2.9866161][-2.7511911 -2.8106618 -2.8453593 -2.8758259 -2.9051709 -2.9343557 -2.9588857 -2.9702573 -2.9834101 -3.0021572 -3.0234933 -3.0680828 -3.1512523 -3.2346606 -3.2354703]]...]
INFO - root - 2017-12-07 03:00:18.538506: step 210, loss = 0.77, batch loss = 0.70 (12.4 examples/sec; 2.571 sec/batch; 59h:12m:46s remains)
INFO - root - 2017-12-07 03:00:43.913569: step 220, loss = 0.85, batch loss = 0.78 (12.7 examples/sec; 2.525 sec/batch; 58h:08m:45s remains)
INFO - root - 2017-12-07 03:01:09.314384: step 230, loss = 0.77, batch loss = 0.69 (12.7 examples/sec; 2.527 sec/batch; 58h:11m:42s remains)
INFO - root - 2017-12-07 03:01:34.765847: step 240, loss = 0.79, batch loss = 0.72 (13.0 examples/sec; 2.461 sec/batch; 56h:40m:08s remains)
INFO - root - 2017-12-07 03:02:00.295051: step 250, loss = 0.75, batch loss = 0.67 (12.2 examples/sec; 2.614 sec/batch; 60h:09m:57s remains)
INFO - root - 2017-12-07 03:02:25.683244: step 260, loss = 0.79, batch loss = 0.72 (12.5 examples/sec; 2.560 sec/batch; 58h:55m:16s remains)
INFO - root - 2017-12-07 03:02:53.863992: step 270, loss = 0.76, batch loss = 0.69 (12.6 examples/sec; 2.548 sec/batch; 58h:38m:23s remains)
INFO - root - 2017-12-07 03:03:19.251909: step 280, loss = 0.78, batch loss = 0.71 (12.6 examples/sec; 2.537 sec/batch; 58h:23m:37s remains)
INFO - root - 2017-12-07 03:03:44.804009: step 290, loss = 0.77, batch loss = 0.70 (12.5 examples/sec; 2.563 sec/batch; 58h:58m:39s remains)
INFO - root - 2017-12-07 03:04:10.319691: step 300, loss = 0.68, batch loss = 0.61 (12.6 examples/sec; 2.533 sec/batch; 58h:16m:44s remains)
2017-12-07 03:04:11.836789: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3081551 -2.3204155 -1.982197 -1.8500636 -2.1843367 -2.0744836 -1.3866363 -0.96051049 -0.70498371 -0.42361355 -0.36787748 -0.60699129 -1.0863476 -1.7472174 -2.3897598][-2.308471 -2.3532615 -2.147948 -2.0642486 -2.3293192 -2.169961 -1.5227509 -1.1069391 -0.80257058 -0.43080926 -0.14691687 -0.1595912 -0.63137436 -1.467176 -2.2760775][-2.2473278 -2.5193071 -2.5233862 -2.4201825 -2.4296863 -2.0995445 -1.5052698 -1.172236 -0.91278768 -0.56087542 -0.1585784 -0.0041966438 -0.49291897 -1.4705148 -2.3840399][-2.0836048 -2.5098884 -2.6340065 -2.4196403 -2.1292644 -1.653964 -1.2334299 -1.1587877 -1.1240528 -0.87646651 -0.4082799 -0.11334085 -0.55228758 -1.596283 -2.587842][-1.6954892 -2.0485218 -2.2768729 -2.1046028 -1.7636268 -1.3134248 -1.0983765 -1.2570252 -1.4054689 -1.2583349 -0.69524479 -0.068006992 -0.18474197 -1.1944382 -2.4185872][-1.2553694 -1.3364251 -1.6679049 -1.7786195 -1.737721 -1.4743502 -1.3002207 -1.4447052 -1.6013558 -1.4959829 -0.785856 0.34925556 0.79579496 -0.11710501 -1.7215412][-1.2674811 -0.9565568 -1.1111028 -1.3998816 -1.7156098 -1.6208541 -1.3418624 -1.32532 -1.387675 -1.2659402 -0.49616671 0.93135166 1.8155727 1.0519714 -0.84399486][-1.5649827 -0.98096704 -0.92158771 -1.258671 -1.7029436 -1.594985 -1.1881764 -1.1536171 -1.2694674 -1.1549649 -0.47341251 0.91595221 1.94063 1.3949614 -0.40964842][-1.6923859 -1.0375187 -0.89855242 -1.2134778 -1.6513362 -1.6306379 -1.3705318 -1.6458611 -2.0570419 -1.9717038 -1.365201 -0.18730021 0.81539917 0.63687372 -0.64340496][-1.7856247 -1.1220169 -0.85496306 -1.1116183 -1.6568131 -1.9226484 -1.8976471 -2.3568859 -2.9151106 -2.835896 -2.2949436 -1.3691809 -0.4822917 -0.34466314 -1.092737][-1.7820201 -1.1272039 -0.75647807 -1.010339 -1.6972947 -2.0701985 -1.9513669 -2.145237 -2.5291619 -2.4094086 -1.9900451 -1.3764338 -0.80237937 -0.70230436 -1.25917][-1.6385157 -0.87939906 -0.44800806 -0.78100014 -1.5017941 -1.687752 -1.2427483 -1.0638192 -1.2982538 -1.3025773 -1.0299938 -0.61847782 -0.37759733 -0.62791967 -1.4065223][-1.5009489 -0.58732295 -0.13762331 -0.52564025 -1.1278214 -1.1147428 -0.6401155 -0.4438684 -0.72630548 -0.89156508 -0.63388491 -0.21695614 -0.17912245 -0.77018976 -1.8100941][-1.4335539 -0.42842627 0.041751862 -0.31076336 -0.85614729 -0.99487114 -0.94012284 -1.0307775 -1.3245201 -1.4052248 -0.95442438 -0.36902046 -0.28844738 -0.97596455 -2.1099238][-1.2489746 -0.38886595 -0.0026874542 -0.326468 -0.91410613 -1.3161218 -1.6025124 -1.8381994 -2.0128274 -1.8902898 -1.2088494 -0.3647604 -0.051284313 -0.72712755 -1.9734735]]...]
INFO - root - 2017-12-07 03:04:37.189040: step 310, loss = 0.80, batch loss = 0.72 (12.5 examples/sec; 2.567 sec/batch; 59h:02m:27s remains)
INFO - root - 2017-12-07 03:05:02.576509: step 320, loss = 0.89, batch loss = 0.82 (12.6 examples/sec; 2.539 sec/batch; 58h:23m:33s remains)
INFO - root - 2017-12-07 03:05:27.915725: step 330, loss = 0.73, batch loss = 0.65 (12.7 examples/sec; 2.518 sec/batch; 57h:54m:09s remains)
INFO - root - 2017-12-07 03:05:53.331336: step 340, loss = 0.78, batch loss = 0.71 (12.5 examples/sec; 2.560 sec/batch; 58h:52m:16s remains)
INFO - root - 2017-12-07 03:06:18.852415: step 350, loss = 0.67, batch loss = 0.60 (12.6 examples/sec; 2.542 sec/batch; 58h:26m:22s remains)
INFO - root - 2017-12-07 03:06:44.398635: step 360, loss = 0.77, batch loss = 0.69 (12.4 examples/sec; 2.587 sec/batch; 59h:27m:54s remains)
INFO - root - 2017-12-07 03:07:09.871427: step 370, loss = 0.73, batch loss = 0.65 (12.6 examples/sec; 2.537 sec/batch; 58h:18m:59s remains)
INFO - root - 2017-12-07 03:07:35.333206: step 380, loss = 0.73, batch loss = 0.66 (12.6 examples/sec; 2.542 sec/batch; 58h:25m:20s remains)
INFO - root - 2017-12-07 03:08:00.778833: step 390, loss = 0.82, batch loss = 0.75 (12.6 examples/sec; 2.542 sec/batch; 58h:25m:46s remains)
INFO - root - 2017-12-07 03:08:29.032444: step 400, loss = 0.70, batch loss = 0.63 (12.7 examples/sec; 2.527 sec/batch; 58h:04m:35s remains)
2017-12-07 03:08:30.563534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.66587567 -0.43087912 -0.058745384 0.28268528 0.38140297 0.094254494 -0.34760571 -0.49355388 -0.27124834 0.12734079 0.50313473 0.66074085 0.4241333 -0.27541065 -0.47578287][-0.6972785 -0.45630121 -0.068763733 0.26904869 0.41204882 0.17460489 -0.29895592 -0.49793148 -0.369565 -0.13297272 0.25765514 0.64071178 0.43937302 -0.26327133 -0.37052584][-0.30146027 -0.1990943 -0.060162067 0.080209732 0.23207998 0.13833475 -0.22157431 -0.39199829 -0.38264465 -0.32235479 0.051127911 0.50245285 0.3013196 -0.24872065 -0.19195461][0.11662817 -0.016977787 -0.26067495 -0.345006 -0.14929819 -0.036943436 -0.15035677 -0.16824627 -0.20348835 -0.25204849 0.043136597 0.36765242 0.14703321 -0.13787365 0.044773579][0.2355032 -0.079212189 -0.57366967 -0.75129747 -0.50374627 -0.22814655 -0.076328278 0.14880037 0.16844893 0.028571606 0.14030838 0.21696854 0.018120766 0.048355103 0.303267][0.19334173 -0.18362188 -0.71530724 -0.85569906 -0.55834436 -0.18039465 0.20711899 0.67692184 0.66621065 0.23538637 0.04373312 -0.069198608 -0.047863483 0.34914351 0.62816095][0.21321058 -0.1436429 -0.54407382 -0.58736134 -0.22353363 0.27161264 0.89805365 1.5423279 1.3138733 0.4499054 -0.08054781 -0.29487038 -0.011011124 0.66143131 0.89326191][0.18464708 0.054722786 -0.042737961 0.040248394 0.41127586 0.91365194 1.6487608 2.3345137 1.8134151 0.64049768 -0.069838047 -0.29970789 0.12797928 0.82532787 0.89335251][0.072510242 0.25787592 0.39266491 0.51794815 0.8295517 1.242126 1.9472022 2.4917393 1.7732325 0.6648407 0.061502457 -0.065522671 0.40825081 0.94888258 0.84943438][-0.051206112 0.2507143 0.37673616 0.49901724 0.79360676 1.0895538 1.6325994 1.9286847 1.212523 0.44846725 0.057066917 0.042402744 0.54075575 0.94575167 0.75666142][-0.32995272 -0.12443495 -0.11548805 0.042541504 0.355659 0.56366014 0.95433235 1.0647202 0.50948238 0.14485359 -0.070315361 0.013942719 0.51146221 0.77916384 0.48617983][-0.71997571 -0.71383882 -0.73256993 -0.46480393 -0.14787626 -0.0059280396 0.25522423 0.2546072 -0.11888504 -0.17947578 -0.21699047 -0.02273941 0.3831296 0.39601755 -0.021319389][-1.3887413 -1.4934354 -1.3813708 -0.97472286 -0.68890047 -0.55132222 -0.32075119 -0.32527733 -0.5197649 -0.46109819 -0.3743434 -0.094738483 0.14475489 -0.11711121 -0.60890603][-2.2592802 -2.3519099 -2.0685217 -1.5597575 -1.2850184 -1.0834663 -0.86691904 -0.88111758 -0.97589564 -0.87092948 -0.69836473 -0.41958761 -0.38659239 -0.82059979 -1.2910857][-2.9538321 -2.9920883 -2.6792665 -2.203033 -1.9274175 -1.6753995 -1.5179777 -1.5714188 -1.6108911 -1.4868925 -1.330348 -1.1814456 -1.2994509 -1.7064242 -2.0453973]]...]
INFO - root - 2017-12-07 03:08:56.041635: step 410, loss = 0.75, batch loss = 0.68 (12.5 examples/sec; 2.569 sec/batch; 59h:01m:11s remains)
INFO - root - 2017-12-07 03:09:21.614571: step 420, loss = 0.74, batch loss = 0.67 (12.6 examples/sec; 2.548 sec/batch; 58h:32m:45s remains)
INFO - root - 2017-12-07 03:09:46.934351: step 430, loss = 0.82, batch loss = 0.75 (13.5 examples/sec; 2.376 sec/batch; 54h:34m:02s remains)
INFO - root - 2017-12-07 03:10:12.452769: step 440, loss = 0.66, batch loss = 0.59 (12.6 examples/sec; 2.533 sec/batch; 58h:10m:15s remains)
INFO - root - 2017-12-07 03:10:40.791632: step 450, loss = 0.76, batch loss = 0.69 (12.5 examples/sec; 2.550 sec/batch; 58h:33m:36s remains)
INFO - root - 2017-12-07 03:11:06.342874: step 460, loss = 0.82, batch loss = 0.75 (12.2 examples/sec; 2.617 sec/batch; 60h:04m:58s remains)
INFO - root - 2017-12-07 03:11:31.534986: step 470, loss = 0.66, batch loss = 0.59 (12.5 examples/sec; 2.552 sec/batch; 58h:35m:33s remains)
INFO - root - 2017-12-07 03:12:06.052843: step 480, loss = 0.77, batch loss = 0.70 (8.4 examples/sec; 3.815 sec/batch; 87h:35m:30s remains)
INFO - root - 2017-12-07 03:12:53.971894: step 490, loss = 0.77, batch loss = 0.70 (6.6 examples/sec; 4.864 sec/batch; 111h:38m:53s remains)
INFO - root - 2017-12-07 03:13:49.997049: step 500, loss = 0.81, batch loss = 0.73 (6.6 examples/sec; 4.867 sec/batch; 111h:41m:42s remains)
2017-12-07 03:13:55.760192: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.77579927 0.23959875 -0.11736441 -0.39108038 -0.91752839 -1.8311324 -2.8581853 -3.069272 -2.5551744 -1.6569529 -0.45091534 0.33384752 0.49787951 0.26744413 -0.86094642][0.67434454 -0.070126057 -0.48658133 -0.57263827 -0.75018358 -1.2803452 -2.0446658 -2.2386911 -2.0128894 -1.5618024 -0.72735834 -0.0330925 0.4063139 0.34858608 -0.8587389][0.21230841 -0.6194768 -0.91729259 -0.70447326 -0.50279903 -0.72261095 -1.2711804 -1.4382448 -1.4484859 -1.4381647 -1.0599329 -0.49867868 0.10360336 0.20467758 -0.89821792][-0.27574587 -1.1418152 -1.34659 -0.95427918 -0.54276133 -0.57968473 -0.91067457 -0.93647742 -1.0215995 -1.323132 -1.3348122 -0.90055966 -0.26075077 -0.039580822 -0.86744952][-0.71683168 -1.5702193 -1.7327776 -1.2600467 -0.68810558 -0.51882052 -0.54983425 -0.33172941 -0.43241382 -1.0148933 -1.4279373 -1.188457 -0.6137526 -0.31752539 -0.75590563][-0.82716107 -1.6510794 -1.8010144 -1.3052316 -0.56459928 -0.12381268 0.20800543 0.7109189 0.59294415 -0.29383659 -1.1844733 -1.2875774 -0.88057685 -0.59858346 -0.65605688][-0.85935616 -1.5988839 -1.6977415 -1.1297867 -0.1681056 0.57783079 1.2919083 2.071403 1.9785509 0.85608864 -0.4922595 -1.0722229 -0.96146369 -0.8138504 -0.600214][-0.96856618 -1.5682328 -1.5804012 -0.90641809 0.29337311 1.324573 2.3092508 3.2373476 3.14891 1.9112854 0.26140785 -0.73020124 -0.98662615 -1.036221 -0.64839029][-1.1226292 -1.6191149 -1.5883088 -0.84313655 0.44071579 1.5414062 2.5234199 3.3937449 3.3674493 2.2709084 0.62632608 -0.53427649 -1.067589 -1.308172 -0.88769841][-1.3663733 -1.8061759 -1.7778487 -1.034189 0.14771318 1.1093488 1.897615 2.5869551 2.6641507 1.914423 0.55981731 -0.50313163 -1.0978687 -1.4481301 -1.1397057][-1.6162922 -1.9722667 -1.9575233 -1.2869871 -0.30432844 0.47024488 1.0448856 1.5274549 1.6443357 1.2077541 0.25977039 -0.52885628 -1.0102775 -1.3633547 -1.197787][-1.6918168 -1.9367342 -1.9565461 -1.4590485 -0.72976279 -0.10170174 0.37129784 0.72691107 0.82338667 0.56392717 -0.011898041 -0.48256969 -0.81146312 -1.1209059 -1.0827885][-1.5461028 -1.6153288 -1.5786467 -1.2145774 -0.70379591 -0.18131638 0.24084759 0.46675968 0.48112106 0.32094336 0.060705662 -0.13698673 -0.37018919 -0.65749383 -0.73782754][-1.3496399 -1.2511525 -1.0689254 -0.69700456 -0.25683212 0.22501659 0.57265806 0.593987 0.43983221 0.30835772 0.27100325 0.28459406 0.16556883 -0.058053017 -0.23156261][-1.1409435 -1.0137575 -0.77219462 -0.36504078 0.10307837 0.58904266 0.81496525 0.59015417 0.21772718 0.0408082 0.083531857 0.17860222 0.15423965 0.053216934 -0.1006155]]...]
INFO - root - 2017-12-07 03:19:21.114255: step 510, loss = 0.78, batch loss = 0.71 (6.8 examples/sec; 4.695 sec/batch; 107h:44m:05s remains)
INFO - root - 2017-12-07 03:20:15.197061: step 520, loss = 0.80, batch loss = 0.72 (6.6 examples/sec; 4.850 sec/batch; 111h:16m:49s remains)
INFO - root - 2017-12-07 03:20:56.161535: step 530, loss = 0.75, batch loss = 0.68 (8.7 examples/sec; 3.679 sec/batch; 84h:24m:34s remains)
INFO - root - 2017-12-07 03:21:33.053718: step 540, loss = 0.73, batch loss = 0.66 (8.7 examples/sec; 3.687 sec/batch; 84h:35m:25s remains)
INFO - root - 2017-12-07 03:22:09.814621: step 550, loss = 0.80, batch loss = 0.73 (8.7 examples/sec; 3.689 sec/batch; 84h:37m:25s remains)
INFO - root - 2017-12-07 03:22:40.632458: step 560, loss = 0.74, batch loss = 0.67 (12.3 examples/sec; 2.592 sec/batch; 59h:26m:34s remains)
INFO - root - 2017-12-07 03:23:06.500130: step 570, loss = 0.84, batch loss = 0.77 (12.3 examples/sec; 2.599 sec/batch; 59h:36m:13s remains)
INFO - root - 2017-12-07 03:23:32.303545: step 580, loss = 0.81, batch loss = 0.74 (12.3 examples/sec; 2.612 sec/batch; 59h:53m:18s remains)
INFO - root - 2017-12-07 03:23:57.975797: step 590, loss = 0.72, batch loss = 0.64 (13.3 examples/sec; 2.400 sec/batch; 55h:01m:44s remains)
INFO - root - 2017-12-07 03:24:24.151540: step 600, loss = 0.73, batch loss = 0.66 (12.2 examples/sec; 2.630 sec/batch; 60h:17m:41s remains)
2017-12-07 03:24:25.705465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.32632828 -0.13689995 0.067051888 0.08010006 -0.012068272 -0.13159323 -0.035779476 0.13662481 -0.015980721 -0.29087925 -0.60129428 -0.85403585 -1.0949168 -1.3279674 -1.4219453][0.081898212 0.30979347 0.41462994 0.23942852 0.020980835 -0.19497204 -0.12347507 0.11460733 0.074190617 -0.047470093 -0.2234807 -0.44482303 -0.70439863 -0.93958211 -1.0803058][0.18803406 0.46797991 0.56355095 0.366086 0.11531353 -0.16945839 -0.14956951 0.1173892 0.19482279 0.2186799 0.17156363 0.043665886 -0.11694765 -0.26528025 -0.38548422][-0.25507355 -0.0018978119 0.15911436 0.10640144 0.0089111328 -0.17206955 -0.15047264 0.1060195 0.24139643 0.3214674 0.28299475 0.16627264 0.080861092 0.075614929 0.10393476][-1.1610544 -1.0188229 -0.79140258 -0.59195495 -0.36763954 -0.24393559 -0.079969883 0.19586039 0.3366785 0.38630152 0.26805115 0.048802376 -0.11102152 -0.062520981 0.095523834][-1.7357073 -1.8291583 -1.7002723 -1.3657327 -0.84714317 -0.3409915 0.087649345 0.44824266 0.54266071 0.47351456 0.26291084 -0.098366261 -0.42297888 -0.41272736 -0.20440817][-1.8647764 -2.062978 -2.0706422 -1.8096268 -1.2238 -0.47799802 0.21398401 0.73299265 0.83299971 0.63180637 0.31041574 -0.19422865 -0.67558551 -0.70186472 -0.44394636][-1.9724307 -2.1787224 -2.24377 -2.082727 -1.5848927 -0.87451577 -0.13294458 0.51898289 0.76231909 0.60922146 0.30211878 -0.22799969 -0.75014806 -0.76671433 -0.45739412][-1.9693325 -2.182611 -2.2479157 -2.1190004 -1.7539308 -1.2529285 -0.69877934 -0.057179928 0.38258076 0.4110384 0.2153492 -0.23112106 -0.66152811 -0.61278772 -0.26498556][-1.7252593 -2.0281937 -2.0804148 -1.9178584 -1.6322303 -1.3042665 -0.93874288 -0.37148476 0.18288565 0.33561802 0.1491375 -0.2996583 -0.63870096 -0.49247169 -0.056828976][-1.2377245 -1.6731272 -1.7709653 -1.6273954 -1.4270294 -1.1846311 -0.89555383 -0.39109945 0.18153715 0.33888483 0.048921585 -0.50405741 -0.82558846 -0.64864779 -0.15217876][-0.97095656 -1.3626254 -1.4385169 -1.3825576 -1.3200965 -1.1481228 -0.85434246 -0.38997412 0.10672522 0.21677542 -0.11275482 -0.649086 -0.92735314 -0.82754278 -0.46647334][-0.93641591 -1.1619163 -1.1265166 -1.100471 -1.1440759 -1.0524919 -0.79130578 -0.43176365 -0.082536221 0.0090770721 -0.18595839 -0.4822681 -0.62489915 -0.69481063 -0.68196678][-0.80253625 -0.85066772 -0.69917107 -0.68409038 -0.82751679 -0.86871243 -0.71863937 -0.47567654 -0.24472189 -0.10789776 -0.083794117 -0.085636139 -0.066133022 -0.28629351 -0.6280899][-0.72148442 -0.62738895 -0.40190268 -0.41680193 -0.61814737 -0.74865961 -0.68555188 -0.50691485 -0.36762142 -0.23608828 -0.1037569 0.063774109 0.19389153 -0.094752789 -0.62017179]]...]
INFO - root - 2017-12-07 03:24:52.030368: step 610, loss = 0.75, batch loss = 0.68 (12.2 examples/sec; 2.614 sec/batch; 59h:55m:15s remains)
INFO - root - 2017-12-07 03:25:18.324303: step 620, loss = 0.69, batch loss = 0.62 (12.0 examples/sec; 2.656 sec/batch; 60h:51m:43s remains)
INFO - root - 2017-12-07 03:25:44.562369: step 630, loss = 0.76, batch loss = 0.69 (12.2 examples/sec; 2.628 sec/batch; 60h:13m:35s remains)
INFO - root - 2017-12-07 03:26:10.627827: step 640, loss = 0.78, batch loss = 0.71 (12.1 examples/sec; 2.655 sec/batch; 60h:50m:26s remains)
INFO - root - 2017-12-07 03:26:36.787758: step 650, loss = 0.77, batch loss = 0.70 (12.2 examples/sec; 2.614 sec/batch; 59h:52m:46s remains)
INFO - root - 2017-12-07 03:27:02.994730: step 660, loss = 0.81, batch loss = 0.73 (12.2 examples/sec; 2.623 sec/batch; 60h:05m:28s remains)
INFO - root - 2017-12-07 03:27:29.302365: step 670, loss = 0.83, batch loss = 0.76 (12.1 examples/sec; 2.646 sec/batch; 60h:36m:09s remains)
INFO - root - 2017-12-07 03:27:55.475936: step 680, loss = 0.77, batch loss = 0.70 (12.2 examples/sec; 2.617 sec/batch; 59h:55m:18s remains)
INFO - root - 2017-12-07 03:28:21.451604: step 690, loss = 0.72, batch loss = 0.65 (12.0 examples/sec; 2.672 sec/batch; 61h:11m:12s remains)
INFO - root - 2017-12-07 03:28:47.614783: step 700, loss = 0.72, batch loss = 0.65 (12.2 examples/sec; 2.620 sec/batch; 59h:59m:47s remains)
2017-12-07 03:28:49.212149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5197768 -2.1695392 -2.035949 -2.2566278 -2.672421 -3.104439 -3.3447227 -3.3883896 -3.4797082 -3.6134243 -3.6947465 -3.6538181 -3.4779465 -3.3330598 -3.3047323][-2.8741436 -2.2335157 -1.6579084 -1.6416862 -2.2562265 -3.1983957 -3.9463339 -4.2588935 -4.2837496 -4.1528955 -4.0315361 -3.9832931 -3.8685849 -3.704586 -3.5844865][-3.3056912 -2.5567179 -1.5341249 -0.93250704 -1.2833729 -2.4719849 -3.7689319 -4.6294751 -4.9162049 -4.7377863 -4.4714994 -4.3796506 -4.3146906 -4.163115 -3.972543][-3.7110062 -3.1303606 -1.8989909 -0.58562803 -0.10454893 -0.90850019 -2.4095073 -3.858285 -4.7898951 -5.0035181 -4.819397 -4.6714149 -4.5731449 -4.4357586 -4.2351141][-3.9603887 -3.7527215 -2.6987393 -1.0365334 0.40590191 0.65995216 -0.33744764 -2.009964 -3.6692405 -4.6978397 -4.9814095 -4.8868117 -4.6765275 -4.478076 -4.2466717][-3.9097712 -4.0975041 -3.5091214 -2.068965 -0.19937325 1.2101817 1.4416337 0.3302989 -1.6252487 -3.5030034 -4.5997658 -4.8459039 -4.6014543 -4.2965727 -4.0078945][-3.5029957 -3.9215744 -3.8799205 -3.0730257 -1.504719 0.38181782 1.8480477 2.0033693 0.56440926 -1.6594973 -3.5160489 -4.3511658 -4.2939868 -3.9312551 -3.5790164][-2.9515605 -3.3621371 -3.7519727 -3.6151068 -2.7042165 -1.110832 0.70446634 1.9385839 1.6338282 -0.12751722 -2.2230134 -3.5731478 -3.9078331 -3.6556983 -3.28487][-2.5321388 -2.7239413 -3.2641335 -3.5531888 -3.2226529 -2.2290134 -0.82630968 0.56043053 1.146873 0.33523178 -1.3518658 -2.8476424 -3.5552585 -3.5560639 -3.2527895][-2.4678583 -2.3543053 -2.7709699 -3.1987705 -3.2375336 -2.7370734 -1.881326 -0.8664217 -0.12505865 -0.21853447 -1.1864831 -2.4123917 -3.2738605 -3.5344534 -3.3762512][-2.7605844 -2.450917 -2.6097126 -2.933609 -3.1330662 -2.9600432 -2.4705138 -1.8432424 -1.3487444 -1.2840524 -1.7379453 -2.4920356 -3.1869273 -3.4938383 -3.400408][-3.207536 -2.8268938 -2.7264464 -2.8254714 -3.0298355 -3.0725205 -2.7947798 -2.3494256 -2.08224 -2.1425493 -2.4867086 -2.9649971 -3.3895979 -3.5346932 -3.3624079][-3.7862391 -3.368866 -3.0619495 -2.9164314 -3.0130916 -3.1589975 -3.0196276 -2.6175585 -2.3189294 -2.3751273 -2.7908154 -3.3287826 -3.7044232 -3.7305708 -3.466857][-4.4063039 -4.0003262 -3.54178 -3.1831579 -3.1017537 -3.2389226 -3.2304153 -2.8715968 -2.3760281 -2.1706672 -2.5362642 -3.294003 -3.8853326 -3.9843209 -3.7295673][-4.786643 -4.4570861 -3.9453413 -3.4498172 -3.1796784 -3.2143478 -3.3170495 -3.1226025 -2.5265117 -1.9653049 -2.0720127 -2.8928151 -3.7581692 -4.0866752 -3.9782088]]...]
INFO - root - 2017-12-07 03:29:15.363304: step 710, loss = 0.73, batch loss = 0.66 (12.4 examples/sec; 2.583 sec/batch; 59h:08m:36s remains)
INFO - root - 2017-12-07 03:29:41.453554: step 720, loss = 0.66, batch loss = 0.58 (12.3 examples/sec; 2.593 sec/batch; 59h:21m:01s remains)
INFO - root - 2017-12-07 03:30:07.623491: step 730, loss = 0.70, batch loss = 0.63 (12.3 examples/sec; 2.593 sec/batch; 59h:21m:24s remains)
INFO - root - 2017-12-07 03:30:33.653326: step 740, loss = 0.76, batch loss = 0.69 (12.3 examples/sec; 2.592 sec/batch; 59h:19m:24s remains)
INFO - root - 2017-12-07 03:30:59.810806: step 750, loss = 0.76, batch loss = 0.69 (12.2 examples/sec; 2.627 sec/batch; 60h:06m:32s remains)
INFO - root - 2017-12-07 03:31:26.070914: step 760, loss = 0.64, batch loss = 0.56 (12.2 examples/sec; 2.622 sec/batch; 59h:58m:51s remains)
INFO - root - 2017-12-07 03:31:52.457354: step 770, loss = 0.68, batch loss = 0.60 (12.1 examples/sec; 2.653 sec/batch; 60h:41m:00s remains)
INFO - root - 2017-12-07 03:32:18.849284: step 780, loss = 0.68, batch loss = 0.60 (12.2 examples/sec; 2.619 sec/batch; 59h:54m:56s remains)
INFO - root - 2017-12-07 03:32:44.914513: step 790, loss = 0.79, batch loss = 0.71 (12.3 examples/sec; 2.598 sec/batch; 59h:24m:36s remains)
INFO - root - 2017-12-07 03:33:11.484357: step 800, loss = 0.70, batch loss = 0.63 (12.2 examples/sec; 2.621 sec/batch; 59h:55m:45s remains)
2017-12-07 03:33:13.051872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.785665 -2.8025551 -2.8770127 -2.9075813 -2.833622 -2.8018832 -2.8493538 -2.9123008 -2.9878263 -3.0703714 -3.0689392 -2.9672871 -2.8963974 -2.8940039 -2.9610324][-2.788743 -2.8114572 -2.9043772 -2.9499147 -2.8957748 -2.9338193 -3.0707288 -3.1884046 -3.2689166 -3.37688 -3.3892341 -3.235518 -3.0947549 -3.0627775 -3.1151962][-2.7926362 -2.7386627 -2.7739756 -2.8025622 -2.7550273 -2.839536 -3.0249085 -3.137044 -3.172904 -3.2925484 -3.3441081 -3.163929 -2.9840271 -2.9475892 -2.9899077][-2.7681637 -2.5516121 -2.4684532 -2.4129868 -2.2772758 -2.2910013 -2.4405336 -2.4518318 -2.4181728 -2.6010807 -2.7917125 -2.6617372 -2.4917002 -2.4931488 -2.5824652][-2.6938515 -2.3352892 -2.1687944 -2.0373881 -1.7985892 -1.6891296 -1.6734357 -1.4249442 -1.2009182 -1.4416497 -1.8359504 -1.8351018 -1.7480092 -1.8111341 -1.9741902][-2.4989109 -2.0559402 -1.8531675 -1.6873586 -1.3625321 -1.0536995 -0.694365 -0.095788479 0.26561213 -0.11929178 -0.71219063 -0.82090688 -0.85714984 -1.018975 -1.3187678][-2.3223035 -1.8714201 -1.6637371 -1.4149656 -0.87826967 -0.21273041 0.60146379 1.5069723 1.7814293 1.043293 0.19370794 0.0031900406 -0.12938023 -0.44954228 -0.94923663][-2.2392132 -1.8182127 -1.573549 -1.1680431 -0.39420271 0.58293056 1.7194972 2.7779636 2.7912898 1.6379542 0.49552107 0.15935326 -0.034897804 -0.41596365 -0.96673632][-2.3191571 -2.0121238 -1.7828071 -1.3171029 -0.55569983 0.28853989 1.097044 1.6831856 1.4453011 0.39291286 -0.54081893 -0.66373944 -0.58912182 -0.73646903 -1.0895097][-2.4858789 -2.3037019 -2.1736922 -1.8420148 -1.3218863 -0.77626538 -0.34546328 -0.14902544 -0.42658424 -1.155303 -1.6587617 -1.4435515 -1.061321 -0.96436977 -1.1598237][-2.6264467 -2.5372899 -2.4976153 -2.2629032 -1.8568001 -1.4324374 -1.1490812 -1.0684226 -1.216574 -1.635694 -1.8795984 -1.5794451 -1.2022252 -1.1850162 -1.5005653][-2.7446918 -2.6769872 -2.6309936 -2.3516102 -1.8792522 -1.4172626 -1.1519871 -1.1172383 -1.223264 -1.5548403 -1.8218093 -1.7057552 -1.5429451 -1.7137971 -2.1749992][-2.825644 -2.7865539 -2.7818339 -2.5502198 -2.1120119 -1.6843865 -1.4616578 -1.4627171 -1.5905361 -1.955951 -2.3019934 -2.3322215 -2.2726183 -2.4685524 -2.8757858][-2.9284189 -2.9492967 -3.0105715 -2.9153519 -2.6554818 -2.4031088 -2.26546 -2.2527888 -2.3356929 -2.6167173 -2.8960381 -2.9357231 -2.8630052 -2.9488432 -3.166182][-2.9924297 -3.0577588 -3.1253953 -3.0873523 -2.94108 -2.802712 -2.7090063 -2.6631341 -2.655777 -2.7781587 -2.919672 -2.9327641 -2.8741517 -2.8953433 -2.9894166]]...]
INFO - root - 2017-12-07 03:33:39.510602: step 810, loss = 0.84, batch loss = 0.76 (12.2 examples/sec; 2.618 sec/batch; 59h:52m:06s remains)
INFO - root - 2017-12-07 03:34:05.683753: step 820, loss = 0.79, batch loss = 0.72 (12.2 examples/sec; 2.627 sec/batch; 60h:03m:48s remains)
INFO - root - 2017-12-07 03:34:31.982848: step 830, loss = 0.78, batch loss = 0.71 (12.0 examples/sec; 2.673 sec/batch; 61h:05m:47s remains)
INFO - root - 2017-12-07 03:34:58.326324: step 840, loss = 0.76, batch loss = 0.69 (12.2 examples/sec; 2.632 sec/batch; 60h:09m:56s remains)
INFO - root - 2017-12-07 03:35:24.469135: step 850, loss = 0.58, batch loss = 0.51 (12.1 examples/sec; 2.638 sec/batch; 60h:17m:46s remains)
INFO - root - 2017-12-07 03:35:50.617086: step 860, loss = 0.68, batch loss = 0.61 (12.3 examples/sec; 2.599 sec/batch; 59h:23m:32s remains)
INFO - root - 2017-12-07 03:36:16.707459: step 870, loss = 0.91, batch loss = 0.84 (12.3 examples/sec; 2.612 sec/batch; 59h:40m:45s remains)
INFO - root - 2017-12-07 03:36:42.719239: step 880, loss = 0.74, batch loss = 0.67 (12.5 examples/sec; 2.566 sec/batch; 58h:37m:25s remains)
INFO - root - 2017-12-07 03:37:08.841630: step 890, loss = 0.77, batch loss = 0.70 (12.2 examples/sec; 2.612 sec/batch; 59h:40m:29s remains)
INFO - root - 2017-12-07 03:37:34.898283: step 900, loss = 0.78, batch loss = 0.71 (12.2 examples/sec; 2.626 sec/batch; 59h:58m:57s remains)
2017-12-07 03:37:36.476852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.895762 -3.9089365 -3.9412267 -3.9682033 -3.968472 -3.9964175 -4.0127668 -3.9072902 -3.7893176 -3.7402673 -3.6282589 -3.5201168 -3.771441 -4.1486597 -4.1569176][-4.3798761 -4.4301715 -4.482513 -4.4936309 -4.4775219 -4.5388093 -4.5993104 -4.5039735 -4.3644834 -4.2943559 -4.2017236 -4.1174164 -4.3327932 -4.6268349 -4.5103197][-4.2841153 -4.3088708 -4.3384576 -4.362721 -4.3869314 -4.48487 -4.5498972 -4.4369879 -4.2741446 -4.222199 -4.2302585 -4.2412195 -4.4823966 -4.7564197 -4.6325755][-4.3790941 -4.4504743 -4.5886254 -4.7763147 -4.9425006 -5.0769844 -5.0916963 -4.88742 -4.6038 -4.4537606 -4.4255013 -4.4346638 -4.6497641 -4.8900123 -4.795517][-4.2386813 -4.2673216 -4.3404922 -4.4323645 -4.5343003 -4.6800814 -4.8009491 -4.7073932 -4.4687028 -4.3367949 -4.3432393 -4.4091749 -4.6655149 -4.9362788 -4.9094234][-4.023953 -3.8562658 -3.6948905 -3.505024 -3.3772042 -3.4605994 -3.7023168 -3.8078513 -3.7212267 -3.7211416 -3.8245578 -4.0034614 -4.3808765 -4.7555723 -4.8362756][-3.9313605 -3.68566 -3.4366283 -3.0525575 -2.6382625 -2.4810286 -2.6186423 -2.7865169 -2.8490515 -3.0067606 -3.2466483 -3.6061826 -4.1309047 -4.5548606 -4.6566086][-3.9270482 -3.6865056 -3.3489151 -2.7670226 -2.1101122 -1.784802 -1.8928216 -2.2140622 -2.5533834 -2.9416971 -3.3035181 -3.7492332 -4.2527957 -4.5198956 -4.4682622][-4.0477834 -3.9276958 -3.6974282 -3.1982222 -2.609942 -2.3565681 -2.4808221 -2.791749 -3.1552334 -3.5140772 -3.7748821 -4.1425362 -4.5219889 -4.575902 -4.3391566][-4.3100724 -4.4412217 -4.5400014 -4.34288 -3.9434397 -3.7117577 -3.6648369 -3.6720202 -3.7944489 -3.9487906 -4.0356359 -4.2854414 -4.5957847 -4.5950489 -4.3085895][-4.3419285 -4.554039 -4.9177709 -5.0269656 -4.8210382 -4.602087 -4.3552456 -4.0554342 -3.9478793 -3.9687703 -4.0109825 -4.2476096 -4.5861187 -4.6431236 -4.4115047][-3.8406165 -3.9234548 -4.4140735 -4.8100014 -4.8915133 -4.8339453 -4.58035 -4.144249 -3.8647819 -3.7992408 -3.8562155 -4.1394162 -4.5511651 -4.7380624 -4.6082587][-3.2789481 -3.153892 -3.5659621 -4.0382562 -4.2992387 -4.4091177 -4.2697 -3.8318741 -3.50365 -3.4570324 -3.6295533 -4.0479193 -4.5979104 -4.9373965 -4.8947825][-3.2600212 -3.0401485 -3.3188972 -3.6752295 -3.8455698 -3.8710713 -3.6680708 -3.1702607 -2.8276722 -2.8881955 -3.2627401 -3.8897924 -4.62233 -5.1046119 -5.12251][-3.6868544 -3.6939039 -3.9831684 -4.2072816 -4.1741614 -3.9520133 -3.5178454 -2.8462629 -2.3978236 -2.4543223 -2.8821597 -3.622309 -4.4872527 -5.0656924 -5.1041536]]...]
INFO - root - 2017-12-07 03:38:02.419027: step 910, loss = 0.89, batch loss = 0.81 (12.3 examples/sec; 2.596 sec/batch; 59h:17m:49s remains)
INFO - root - 2017-12-07 03:38:28.452120: step 920, loss = 0.83, batch loss = 0.75 (12.3 examples/sec; 2.605 sec/batch; 59h:29m:25s remains)
INFO - root - 2017-12-07 03:38:54.631607: step 930, loss = 0.76, batch loss = 0.69 (12.3 examples/sec; 2.607 sec/batch; 59h:31m:02s remains)
INFO - root - 2017-12-07 03:39:20.777493: step 940, loss = 0.74, batch loss = 0.67 (13.0 examples/sec; 2.464 sec/batch; 56h:15m:37s remains)
INFO - root - 2017-12-07 03:39:47.239033: step 950, loss = 0.80, batch loss = 0.72 (12.3 examples/sec; 2.598 sec/batch; 59h:17m:44s remains)
INFO - root - 2017-12-07 03:40:13.440648: step 960, loss = 0.80, batch loss = 0.73 (12.4 examples/sec; 2.577 sec/batch; 58h:48m:32s remains)
INFO - root - 2017-12-07 03:40:39.468753: step 970, loss = 0.66, batch loss = 0.59 (12.4 examples/sec; 2.572 sec/batch; 58h:41m:12s remains)
INFO - root - 2017-12-07 03:41:05.519241: step 980, loss = 0.81, batch loss = 0.74 (12.3 examples/sec; 2.600 sec/batch; 59h:19m:56s remains)
INFO - root - 2017-12-07 03:41:32.034158: step 990, loss = 0.87, batch loss = 0.80 (12.0 examples/sec; 2.667 sec/batch; 60h:50m:43s remains)
INFO - root - 2017-12-07 03:41:58.027491: step 1000, loss = 0.80, batch loss = 0.73 (12.4 examples/sec; 2.590 sec/batch; 59h:05m:30s remains)
2017-12-07 03:41:59.632897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5381043 -2.5310483 -2.4708436 -2.4151568 -2.3739 -2.3508368 -2.309917 -2.2369432 -2.1987605 -2.1985197 -2.2403436 -2.3317292 -2.42426 -2.5412102 -2.6750979][-2.03186 -2.1383789 -2.1957624 -2.2803674 -2.3752737 -2.4800723 -2.5256023 -2.4804742 -2.4311364 -2.3516734 -2.2517338 -2.1822402 -2.1209788 -2.1164284 -2.2115834][-1.8970692 -2.1027277 -2.3100436 -2.6074543 -2.9012146 -3.1672277 -3.3627119 -3.4440031 -3.4789677 -3.37815 -3.1688771 -2.9365726 -2.6599264 -2.4214194 -2.322794][-1.9102633 -2.0422912 -2.2978294 -2.7507849 -3.2001052 -3.578063 -3.8869562 -4.065568 -4.19844 -4.1858778 -4.04799 -3.8214355 -3.4610953 -3.0518162 -2.7613928][-2.0605896 -1.9365926 -2.0471814 -2.4727645 -2.9240532 -3.2830696 -3.5762627 -3.7567816 -3.958148 -4.1167622 -4.1918821 -4.1352544 -3.86119 -3.4526427 -3.1385942][-2.3065555 -1.8606982 -1.64462 -1.7752883 -1.9521248 -2.0043314 -1.9746361 -1.9404812 -2.1765707 -2.6457589 -3.1349292 -3.4845343 -3.5537021 -3.3766685 -3.2509024][-2.3684385 -1.7284548 -1.2200418 -0.97000289 -0.74771023 -0.35132074 0.18257093 0.60480404 0.41145277 -0.35843372 -1.2811198 -2.1145246 -2.672977 -2.8820438 -3.0855808][-2.4074326 -1.7595482 -1.1391628 -0.70007968 -0.29500008 0.31196451 1.1263623 1.8266954 1.7447762 0.90377712 -0.10785055 -1.1088665 -1.9268422 -2.375659 -2.7902241][-2.509779 -1.9665537 -1.3648121 -0.94075847 -0.66721606 -0.28374529 0.27567339 0.78533506 0.73178005 0.1267395 -0.55869794 -1.276202 -1.934489 -2.2524154 -2.5571198][-2.6035798 -2.2568822 -1.7936504 -1.4624724 -1.3433571 -1.2416785 -1.0499969 -0.90731764 -1.0490665 -1.4147732 -1.6933737 -1.9813538 -2.2842619 -2.3139009 -2.3661859][-2.6548562 -2.5350175 -2.3676867 -2.29436 -2.2960112 -2.2795663 -2.1864216 -2.151576 -2.2182133 -2.3315113 -2.3682611 -2.4225709 -2.4736214 -2.2809942 -2.1810365][-2.6772757 -2.6511521 -2.6924391 -2.8340774 -2.9185262 -2.9682381 -2.9430318 -2.9088693 -2.7913761 -2.6810892 -2.6884642 -2.7589431 -2.7313166 -2.438571 -2.2827616][-2.6412537 -2.6889834 -2.8250966 -3.0242045 -3.0987782 -3.2049735 -3.2548962 -3.2382379 -3.0628676 -2.9863563 -3.1857476 -3.3942909 -3.3413329 -3.0221443 -2.8578458][-2.3958728 -2.5365248 -2.7619414 -3.0230947 -3.1923609 -3.4604375 -3.6145103 -3.5846369 -3.4544249 -3.5465641 -3.9325032 -4.2028461 -4.1060686 -3.7993455 -3.6111093][-2.3496294 -2.4899569 -2.6426463 -2.8442574 -3.1187537 -3.6242814 -3.9841695 -4.0338364 -4.0250554 -4.2547221 -4.6693926 -4.8829141 -4.7292237 -4.4269924 -4.1767149]]...]
INFO - root - 2017-12-07 03:42:25.567533: step 1010, loss = 0.78, batch loss = 0.71 (12.3 examples/sec; 2.602 sec/batch; 59h:20m:30s remains)
INFO - root - 2017-12-07 03:42:51.731262: step 1020, loss = 0.76, batch loss = 0.69 (12.0 examples/sec; 2.658 sec/batch; 60h:36m:49s remains)
INFO - root - 2017-12-07 03:43:17.942939: step 1030, loss = 0.87, batch loss = 0.80 (12.3 examples/sec; 2.593 sec/batch; 59h:07m:25s remains)
INFO - root - 2017-12-07 03:43:43.931244: step 1040, loss = 0.77, batch loss = 0.70 (12.4 examples/sec; 2.575 sec/batch; 58h:42m:15s remains)
INFO - root - 2017-12-07 03:44:10.153196: step 1050, loss = 0.78, batch loss = 0.71 (12.1 examples/sec; 2.647 sec/batch; 60h:20m:16s remains)
INFO - root - 2017-12-07 03:44:36.165203: step 1060, loss = 0.80, batch loss = 0.73 (12.3 examples/sec; 2.601 sec/batch; 59h:17m:45s remains)
INFO - root - 2017-12-07 03:45:02.017558: step 1070, loss = 0.77, batch loss = 0.70 (12.3 examples/sec; 2.599 sec/batch; 59h:13m:43s remains)
INFO - root - 2017-12-07 03:45:28.128023: step 1080, loss = 0.73, batch loss = 0.66 (12.4 examples/sec; 2.578 sec/batch; 58h:44m:37s remains)
INFO - root - 2017-12-07 03:45:54.269146: step 1090, loss = 0.75, batch loss = 0.68 (12.5 examples/sec; 2.560 sec/batch; 58h:20m:45s remains)
INFO - root - 2017-12-07 03:46:20.451032: step 1100, loss = 0.86, batch loss = 0.79 (12.2 examples/sec; 2.617 sec/batch; 59h:37m:42s remains)
2017-12-07 03:46:21.825348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.8934567 -0.88119221 -0.65310717 -0.29844856 -0.240551 -0.30942869 -0.22344446 -0.30744839 -0.5045085 -0.29422235 0.13123703 0.29285145 0.07742548 0.077144146 0.15139484][-0.96304536 -0.8393662 -0.65087986 -0.38704157 -0.35275269 -0.38659048 -0.26757002 -0.2313323 -0.22367716 0.093079567 0.5030179 0.65916538 0.44811773 0.37618017 0.32366943][-0.87579465 -0.81303978 -0.78302884 -0.58839321 -0.51534247 -0.53128815 -0.37416506 -0.15219069 0.065195084 0.35442066 0.62292576 0.7605114 0.61489582 0.5017519 0.34554291][-0.56632829 -0.63150024 -0.77228689 -0.62830734 -0.5478785 -0.63266468 -0.44412661 -0.0093865395 0.32876921 0.44345093 0.4932909 0.61919212 0.54792213 0.40404558 0.24858713][-0.37206316 -0.51246357 -0.713439 -0.60368776 -0.5909574 -0.7730391 -0.47694778 0.2199564 0.6095376 0.44395256 0.24846983 0.37402773 0.38776016 0.25742292 0.25352907][-0.41840553 -0.59348011 -0.75707412 -0.6952374 -0.79962873 -0.96324921 -0.40752459 0.58866978 0.94228888 0.47612858 0.075809479 0.1760273 0.26660156 0.1984601 0.42323637][-0.56538057 -0.73133087 -0.78906822 -0.83245277 -1.080694 -1.0866861 -0.1902132 1.0357089 1.3056221 0.65189028 0.13129759 0.18352365 0.31246567 0.333354 0.75761175][-0.7254343 -0.81978583 -0.75632167 -0.89230394 -1.2246389 -1.0472269 0.013553619 1.2030764 1.3852811 0.77840805 0.28228521 0.27470064 0.42298651 0.53283596 1.0507417][-0.73577237 -0.75458264 -0.65088439 -0.87955523 -1.2409792 -0.96715903 -0.011372089 0.86441994 0.98321486 0.68752337 0.37960529 0.32566977 0.46643066 0.65103388 1.1692338][-0.49242163 -0.46237707 -0.40475845 -0.745651 -1.1487722 -0.91672873 -0.20122385 0.35568333 0.51903486 0.60263348 0.52191305 0.46549702 0.59752655 0.7999711 1.2104044][-0.18594599 -0.097670078 -0.054989338 -0.46622348 -0.92000031 -0.79001832 -0.30662727 0.011499405 0.21776152 0.57231236 0.6839323 0.66770172 0.81525469 1.0084562 1.2783933][0.0667119 0.26045656 0.37041187 -0.075806618 -0.59648895 -0.60277987 -0.33407545 -0.235003 -0.11317158 0.36179686 0.69178772 0.81284142 1.0430722 1.2368803 1.3824072][0.24239683 0.54684639 0.78097296 0.33405781 -0.22868347 -0.34230614 -0.24959469 -0.36262226 -0.38122368 0.16598225 0.7218399 0.98339844 1.2697606 1.4957581 1.5280704][0.47825623 0.79030466 1.0889697 0.62665367 0.035210133 -0.14389563 -0.15340948 -0.35553169 -0.44137573 0.13730335 0.82729673 1.1332407 1.4279294 1.6967053 1.6157718][0.75829172 0.95840883 1.2004681 0.68119812 0.043785095 -0.17036057 -0.18538618 -0.30715847 -0.34223127 0.21607018 0.89507246 1.1059184 1.335597 1.6434746 1.4908438]]...]
INFO - root - 2017-12-07 03:46:48.030796: step 1110, loss = 0.75, batch loss = 0.68 (12.3 examples/sec; 2.605 sec/batch; 59h:20m:14s remains)
INFO - root - 2017-12-07 03:47:14.118028: step 1120, loss = 0.84, batch loss = 0.76 (12.4 examples/sec; 2.582 sec/batch; 58h:48m:29s remains)
INFO - root - 2017-12-07 03:47:40.147365: step 1130, loss = 0.77, batch loss = 0.70 (12.3 examples/sec; 2.594 sec/batch; 59h:05m:10s remains)
INFO - root - 2017-12-07 03:48:06.013238: step 1140, loss = 0.82, batch loss = 0.75 (12.1 examples/sec; 2.642 sec/batch; 60h:09m:45s remains)
INFO - root - 2017-12-07 03:48:32.177030: step 1150, loss = 0.72, batch loss = 0.65 (12.3 examples/sec; 2.596 sec/batch; 59h:07m:06s remains)
INFO - root - 2017-12-07 03:48:58.195677: step 1160, loss = 0.77, batch loss = 0.70 (12.0 examples/sec; 2.658 sec/batch; 60h:31m:26s remains)
INFO - root - 2017-12-07 03:49:24.198924: step 1170, loss = 0.75, batch loss = 0.68 (12.1 examples/sec; 2.643 sec/batch; 60h:10m:45s remains)
INFO - root - 2017-12-07 03:49:50.353051: step 1180, loss = 0.91, batch loss = 0.84 (12.4 examples/sec; 2.573 sec/batch; 58h:34m:37s remains)
INFO - root - 2017-12-07 03:50:16.477997: step 1190, loss = 0.78, batch loss = 0.70 (12.2 examples/sec; 2.612 sec/batch; 59h:27m:31s remains)
INFO - root - 2017-12-07 03:50:42.300525: step 1200, loss = 0.83, batch loss = 0.76 (12.4 examples/sec; 2.581 sec/batch; 58h:44m:12s remains)
2017-12-07 03:50:43.849664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0165045 -0.90828037 -0.75889087 -0.44266272 -0.27791309 -0.28884745 -0.22855568 -0.23285055 -0.35819054 -0.38283825 -0.28522491 -0.070349693 0.17712259 0.28478813 0.27252102][-1.043849 -0.9254806 -0.78090215 -0.4889884 -0.37706661 -0.40227604 -0.3391633 -0.38544512 -0.53988051 -0.58206534 -0.52632046 -0.31929159 -0.065707684 0.072050571 0.098369122][-1.1200829 -1.0541689 -1.0001819 -0.8258698 -0.81302166 -0.86242437 -0.78408813 -0.78232121 -0.80750895 -0.74187732 -0.65954161 -0.49380708 -0.31701231 -0.21911669 -0.20833492][-1.2862682 -1.3270998 -1.4300699 -1.420505 -1.5108798 -1.5703485 -1.4638143 -1.3660386 -1.2068069 -1.0021894 -0.82700109 -0.66739368 -0.5713203 -0.54741764 -0.61084175][-1.5047557 -1.6412394 -1.8658497 -1.9549539 -2.0874972 -2.1186616 -1.9709864 -1.8145437 -1.5773275 -1.3566267 -1.1799583 -1.0616779 -1.017174 -0.98328114 -1.0293515][-1.7584841 -1.9384475 -2.1748645 -2.2219458 -2.289058 -2.2468791 -2.0446351 -1.8753352 -1.6748772 -1.5513513 -1.5302663 -1.6008978 -1.6652107 -1.5791717 -1.4830408][-1.9658761 -2.1147742 -2.2717118 -2.189913 -2.1285319 -2.0140929 -1.8026865 -1.7450457 -1.7322059 -1.8171287 -2.0284393 -2.2955706 -2.3975546 -2.2045739 -1.9498842][-1.995156 -2.0557313 -2.1162171 -1.9394162 -1.8000205 -1.680264 -1.5255914 -1.6228027 -1.8005528 -2.0639334 -2.4428473 -2.8010936 -2.8560674 -2.5758932 -2.252202][-1.9549906 -1.9875422 -2.0673277 -1.9413731 -1.8586745 -1.8201437 -1.7468114 -1.882479 -2.0462747 -2.2703726 -2.603163 -2.8720386 -2.8397534 -2.5921767 -2.3699911][-1.9689109 -2.011076 -2.1391737 -2.1129746 -2.1618145 -2.2725439 -2.3420887 -2.5022814 -2.5547333 -2.593492 -2.7127395 -2.7270067 -2.527102 -2.2890885 -2.1949582][-2.0877066 -2.1264067 -2.2139192 -2.1734824 -2.2689238 -2.4723835 -2.6864698 -2.9103568 -2.8913026 -2.7644358 -2.6872656 -2.4923983 -2.1997552 -1.9620798 -1.9101949][-2.2175174 -2.2534928 -2.2451777 -2.1082268 -2.1489418 -2.3434963 -2.6886709 -3.05345 -3.0831542 -2.9289794 -2.7803044 -2.5135689 -2.2417903 -2.0328557 -1.9538052][-2.2748611 -2.3276873 -2.2581611 -2.0641596 -2.0355051 -2.1584852 -2.572253 -3.0277886 -3.1102479 -3.0218427 -2.9144468 -2.6870642 -2.5084424 -2.3547049 -2.2041407][-2.3093264 -2.3973463 -2.3306046 -2.1845002 -2.1586423 -2.2039666 -2.6111302 -3.0087385 -3.0031264 -2.9156332 -2.7899601 -2.546015 -2.4093273 -2.2673335 -2.0105708][-2.3253953 -2.4159377 -2.3706765 -2.3505166 -2.4049606 -2.4247274 -2.8222084 -3.1003675 -2.9524899 -2.8290439 -2.6034684 -2.227674 -2.0518222 -1.8894277 -1.5836058]]...]
INFO - root - 2017-12-07 03:51:10.043866: step 1210, loss = 0.73, batch loss = 0.66 (12.2 examples/sec; 2.632 sec/batch; 59h:53m:20s remains)
INFO - root - 2017-12-07 03:51:36.148826: step 1220, loss = 0.77, batch loss = 0.70 (12.2 examples/sec; 2.622 sec/batch; 59h:39m:09s remains)
INFO - root - 2017-12-07 03:52:02.339052: step 1230, loss = 0.72, batch loss = 0.65 (12.3 examples/sec; 2.599 sec/batch; 59h:07m:10s remains)
INFO - root - 2017-12-07 03:52:28.534219: step 1240, loss = 0.84, batch loss = 0.77 (12.4 examples/sec; 2.583 sec/batch; 58h:45m:15s remains)
INFO - root - 2017-12-07 03:52:54.684803: step 1250, loss = 0.74, batch loss = 0.67 (12.0 examples/sec; 2.668 sec/batch; 60h:40m:17s remains)
INFO - root - 2017-12-07 03:53:20.874709: step 1260, loss = 0.76, batch loss = 0.69 (12.7 examples/sec; 2.516 sec/batch; 57h:13m:11s remains)
INFO - root - 2017-12-07 03:53:47.130893: step 1270, loss = 0.77, batch loss = 0.70 (12.3 examples/sec; 2.593 sec/batch; 58h:57m:45s remains)
INFO - root - 2017-12-07 03:54:13.402902: step 1280, loss = 0.74, batch loss = 0.66 (12.3 examples/sec; 2.600 sec/batch; 59h:06m:44s remains)
INFO - root - 2017-12-07 03:54:39.645386: step 1290, loss = 0.74, batch loss = 0.67 (12.2 examples/sec; 2.614 sec/batch; 59h:24m:59s remains)
INFO - root - 2017-12-07 03:55:05.795537: step 1300, loss = 0.66, batch loss = 0.59 (12.2 examples/sec; 2.626 sec/batch; 59h:41m:16s remains)
2017-12-07 03:55:07.399376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5425844 -2.6512189 -2.6677508 -2.6824965 -2.68913 -2.752388 -2.7594581 -2.5614882 -2.4312623 -2.4626346 -2.4163439 -2.4547367 -2.6929603 -2.8620443 -2.8523746][-2.3029268 -2.3692899 -2.3033938 -2.2001212 -2.1094711 -2.1850665 -2.1963069 -2.0059471 -1.9836309 -2.0716434 -1.9868546 -2.0173361 -2.3745432 -2.6965971 -2.7971745][-1.9407375 -1.9032478 -1.6821916 -1.456696 -1.4031758 -1.5749114 -1.5314162 -1.3163083 -1.4165964 -1.5724099 -1.4730101 -1.5480149 -2.0451672 -2.4827237 -2.6952209][-1.6067684 -1.4381144 -1.0910635 -0.82516956 -0.91110373 -1.1717861 -0.97155929 -0.60401869 -0.68548226 -0.85814643 -0.74661112 -0.90396953 -1.537133 -2.0515609 -2.4301295][-1.5249512 -1.2104468 -0.70901585 -0.32798815 -0.44466782 -0.67885208 -0.33907509 0.18401575 0.17797756 -0.0708375 -0.019702435 -0.19762802 -0.79956722 -1.2792647 -1.8735752][-1.6263001 -1.1824365 -0.48703194 0.1418643 0.17023325 0.0762558 0.48183775 1.1598306 1.2131014 0.75704479 0.63193893 0.49197769 0.049664974 -0.34932375 -1.1685534][-1.5013812 -0.97228193 -0.16132832 0.58150482 0.71787596 0.77134943 1.3245597 2.2714677 2.392714 1.6339259 1.2593727 1.1221762 0.78464651 0.33643675 -0.69345927][-1.3549426 -0.78845668 0.0062870979 0.56845284 0.59637833 0.6195507 1.2045808 2.187376 2.303822 1.5136085 1.0671258 0.88497639 0.54883575 0.029922962 -1.0030992][-1.6405909 -1.1800265 -0.55000353 -0.29211855 -0.46319366 -0.583982 -0.19914865 0.54697084 0.64967585 0.14280844 -0.17933464 -0.43006277 -0.78540111 -1.2549446 -2.016113][-2.2441502 -2.0386016 -1.7018881 -1.6622951 -1.90166 -2.1032269 -1.9750946 -1.4815433 -1.3112025 -1.489387 -1.7049503 -2.0170276 -2.3311961 -2.6443334 -3.0211873][-2.7928772 -2.7948508 -2.7483408 -2.8683929 -3.0832243 -3.2928858 -3.3148904 -2.9851294 -2.7962499 -2.84144 -2.9874868 -3.235404 -3.4375846 -3.5750773 -3.6339419][-3.1304765 -3.2023489 -3.319643 -3.5251245 -3.6990666 -3.8914175 -3.9693613 -3.7822475 -3.6987281 -3.7764277 -3.8589349 -3.9240208 -3.9045591 -3.8329208 -3.6985149][-3.3797841 -3.5147867 -3.6773727 -3.8558826 -3.931087 -4.0128007 -4.0765805 -4.0189772 -4.0388069 -4.1006756 -4.0589619 -3.9204321 -3.7083273 -3.4821444 -3.2936993][-3.4828076 -3.7110627 -3.9213753 -4.0995493 -4.1490121 -4.1240983 -4.0778122 -4.0046611 -3.9802933 -3.9334326 -3.7793527 -3.5502918 -3.2924142 -3.0590968 -2.9286003][-3.2752523 -3.49188 -3.6775706 -3.8298533 -3.8864121 -3.8485563 -3.7817078 -3.7193758 -3.6439455 -3.5234675 -3.353652 -3.1619949 -2.9807234 -2.8352723 -2.7693992]]...]
INFO - root - 2017-12-07 03:55:33.643740: step 1310, loss = 0.84, batch loss = 0.76 (12.1 examples/sec; 2.643 sec/batch; 60h:03m:38s remains)
INFO - root - 2017-12-07 03:55:59.907651: step 1320, loss = 0.72, batch loss = 0.64 (12.1 examples/sec; 2.641 sec/batch; 60h:00m:21s remains)
INFO - root - 2017-12-07 03:56:26.175872: step 1330, loss = 0.74, batch loss = 0.67 (12.1 examples/sec; 2.652 sec/batch; 60h:15m:31s remains)
INFO - root - 2017-12-07 03:56:52.574300: step 1340, loss = 0.74, batch loss = 0.66 (12.1 examples/sec; 2.634 sec/batch; 59h:50m:23s remains)
INFO - root - 2017-12-07 03:57:18.930817: step 1350, loss = 0.74, batch loss = 0.67 (12.2 examples/sec; 2.621 sec/batch; 59h:31m:54s remains)
INFO - root - 2017-12-07 03:57:45.000860: step 1360, loss = 0.78, batch loss = 0.71 (12.0 examples/sec; 2.670 sec/batch; 60h:38m:25s remains)
INFO - root - 2017-12-07 03:58:11.458888: step 1370, loss = 0.78, batch loss = 0.71 (11.9 examples/sec; 2.690 sec/batch; 61h:05m:25s remains)
INFO - root - 2017-12-07 03:58:37.745857: step 1380, loss = 0.84, batch loss = 0.77 (12.2 examples/sec; 2.624 sec/batch; 59h:34m:36s remains)
INFO - root - 2017-12-07 03:59:03.889199: step 1390, loss = 0.81, batch loss = 0.74 (11.9 examples/sec; 2.685 sec/batch; 60h:57m:41s remains)
INFO - root - 2017-12-07 03:59:30.231114: step 1400, loss = 0.74, batch loss = 0.66 (12.2 examples/sec; 2.628 sec/batch; 59h:38m:54s remains)
2017-12-07 03:59:31.856894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3405542 -3.4263985 -3.4767756 -3.5485625 -3.6790452 -3.7510209 -3.8329825 -3.7910638 -3.6068163 -3.3545241 -3.125149 -2.9783392 -2.8211966 -2.6850448 -2.6043334][-3.9121826 -3.9752495 -4.0379219 -4.1149855 -4.2270966 -4.2602248 -4.3067093 -4.2289872 -3.9889729 -3.7033472 -3.4533224 -3.3067513 -3.1691153 -3.077106 -3.0507326][-4.3234587 -4.3399258 -4.386436 -4.4297652 -4.433013 -4.356565 -4.3428254 -4.2507133 -4.0435658 -3.8465633 -3.7112556 -3.6751823 -3.6718447 -3.7002304 -3.7159569][-4.1470518 -4.1320462 -4.1341939 -4.09632 -3.952142 -3.7854922 -3.7453511 -3.6803315 -3.5841227 -3.5706387 -3.6189523 -3.7168872 -3.8871274 -4.0795913 -4.10714][-3.492713 -3.4567707 -3.3658803 -3.1793633 -2.8829679 -2.6759057 -2.6574223 -2.617789 -2.6251168 -2.8261304 -3.0664172 -3.2503567 -3.5142276 -3.8305354 -3.8933613][-2.5423722 -2.5897441 -2.4782495 -2.1840858 -1.7741151 -1.5057309 -1.4128332 -1.2720277 -1.2305396 -1.5651376 -1.9891448 -2.2599227 -2.5492473 -2.9507074 -3.1792197][-1.6158748 -1.7122309 -1.6379762 -1.3786564 -0.99674916 -0.73537564 -0.575783 -0.30531263 -0.1421876 -0.48911214 -1.0106235 -1.3031659 -1.5264764 -1.9213004 -2.2871482][-1.438766 -1.59776 -1.6741319 -1.6336603 -1.4500513 -1.3189752 -1.1662345 -0.80420828 -0.48787236 -0.60232711 -0.9726243 -1.1320212 -1.181705 -1.409513 -1.6900163][-1.9537809 -2.0826507 -2.2332029 -2.4335175 -2.549953 -2.7032461 -2.6736724 -2.3014071 -1.8900166 -1.6900554 -1.7073967 -1.6052477 -1.4682136 -1.4934742 -1.5715649][-2.4627805 -2.4433498 -2.453691 -2.6610613 -2.9625676 -3.4226248 -3.6654825 -3.4660425 -3.1289308 -2.7580004 -2.4284034 -2.0796 -1.8107998 -1.7021379 -1.65396][-2.7832475 -2.672998 -2.4785304 -2.4633608 -2.651485 -3.1537831 -3.6111455 -3.659137 -3.5106421 -3.1844697 -2.7545738 -2.3906691 -2.1535649 -2.0205665 -1.9741633][-2.9686465 -2.8823695 -2.6305528 -2.4433613 -2.4181287 -2.7103815 -3.1068449 -3.2195678 -3.1730504 -2.97862 -2.6907835 -2.5365558 -2.4635468 -2.4111104 -2.4570053][-3.1504884 -3.1806591 -3.0735068 -2.919894 -2.7521586 -2.7475502 -2.8680971 -2.8429823 -2.7231846 -2.5654736 -2.4607987 -2.5603323 -2.6677155 -2.7144403 -2.8340795][-3.1681631 -3.2955422 -3.3961768 -3.4184442 -3.2732434 -3.1381509 -3.0507462 -2.8912158 -2.6759355 -2.4820328 -2.4301343 -2.6122041 -2.7768724 -2.8344679 -2.9312027][-2.7113914 -2.7607703 -2.9360318 -3.1018476 -3.0872769 -3.073782 -3.0699723 -3.0136786 -2.9020247 -2.7791352 -2.7289572 -2.8480024 -2.952646 -2.9360545 -2.9248543]]...]
INFO - root - 2017-12-07 03:59:58.128037: step 1410, loss = 0.73, batch loss = 0.66 (12.2 examples/sec; 2.627 sec/batch; 59h:37m:43s remains)
INFO - root - 2017-12-07 04:00:24.129129: step 1420, loss = 0.84, batch loss = 0.77 (12.0 examples/sec; 2.667 sec/batch; 60h:31m:11s remains)
INFO - root - 2017-12-07 04:00:50.500681: step 1430, loss = 0.77, batch loss = 0.70 (12.0 examples/sec; 2.667 sec/batch; 60h:31m:30s remains)
INFO - root - 2017-12-07 04:01:16.705310: step 1440, loss = 0.82, batch loss = 0.74 (12.3 examples/sec; 2.599 sec/batch; 58h:58m:42s remains)
INFO - root - 2017-12-07 04:01:42.987129: step 1450, loss = 0.84, batch loss = 0.77 (11.8 examples/sec; 2.704 sec/batch; 61h:20m:55s remains)
INFO - root - 2017-12-07 04:02:08.971273: step 1460, loss = 0.84, batch loss = 0.77 (12.1 examples/sec; 2.636 sec/batch; 59h:47m:34s remains)
INFO - root - 2017-12-07 04:02:35.216450: step 1470, loss = 0.72, batch loss = 0.65 (12.3 examples/sec; 2.603 sec/batch; 59h:03m:02s remains)
INFO - root - 2017-12-07 04:03:01.481761: step 1480, loss = 0.81, batch loss = 0.74 (12.3 examples/sec; 2.610 sec/batch; 59h:11m:59s remains)
INFO - root - 2017-12-07 04:03:27.601187: step 1490, loss = 0.81, batch loss = 0.74 (12.1 examples/sec; 2.641 sec/batch; 59h:52m:46s remains)
INFO - root - 2017-12-07 04:03:53.907965: step 1500, loss = 0.71, batch loss = 0.64 (11.9 examples/sec; 2.686 sec/batch; 60h:53m:33s remains)
2017-12-07 04:03:55.481442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1395466 -2.0823455 -2.0859995 -2.0849988 -2.046864 -2.0380647 -2.0582683 -2.0569513 -2.0481634 -2.0587757 -2.0331609 -1.9723327 -1.9194379 -1.8998151 -1.9253139][-2.8288465 -2.7242131 -2.6869843 -2.6638546 -2.6401386 -2.6769829 -2.755558 -2.8132005 -2.8552735 -2.9043145 -2.9006822 -2.8426323 -2.7737193 -2.7193098 -2.7080188][-3.1520834 -3.06389 -3.0483942 -3.0498419 -3.0536053 -3.1092324 -3.2079515 -3.29163 -3.3680375 -3.4575324 -3.5020013 -3.4808941 -3.4307413 -3.3661964 -3.3229723][-2.7214143 -2.642909 -2.6556115 -2.7039475 -2.7551346 -2.8345907 -2.9385529 -3.0154653 -3.0981846 -3.2155731 -3.3080719 -3.3316216 -3.3234801 -3.2919078 -3.2544966][-2.1606977 -2.0252013 -2.0029788 -2.0539892 -2.1387079 -2.2546043 -2.3795719 -2.4698567 -2.57329 -2.7434835 -2.919647 -3.0192673 -3.0617192 -3.0495167 -3.0004284][-1.8190844 -1.6068528 -1.4955132 -1.4781775 -1.540489 -1.6861579 -1.863097 -2.0079026 -2.1609015 -2.4022384 -2.6923831 -2.9314857 -3.076463 -3.1016955 -3.0425794][-1.2762809 -1.0314929 -0.85320592 -0.75796366 -0.772377 -0.89693451 -1.0526249 -1.1840777 -1.3537428 -1.6496613 -2.065099 -2.4910371 -2.811347 -2.9624827 -3.0001211][-0.89105535 -0.53271651 -0.21619272 0.0064520836 0.062665939 -0.024726868 -0.15946913 -0.31270361 -0.55902362 -0.96779037 -1.5246606 -2.09698 -2.5347185 -2.7888658 -2.9376574][-1.1574469 -0.7184217 -0.32620239 -0.059920311 -0.012696266 -0.13025904 -0.33273125 -0.58387065 -0.91125965 -1.3709874 -1.9368134 -2.4702125 -2.8329949 -3.016006 -3.12259][-1.7676725 -1.4730101 -1.2056556 -1.0229123 -1.0018587 -1.1015716 -1.3058507 -1.575346 -1.8795958 -2.2729163 -2.7287207 -3.1286759 -3.354075 -3.4161773 -3.4302664][-1.9338608 -1.8875082 -1.852927 -1.8321681 -1.8642397 -1.94052 -2.0926325 -2.293673 -2.5049877 -2.7894943 -3.1247551 -3.4078646 -3.5465662 -3.5655513 -3.5720243][-1.6499288 -1.7311697 -1.8301105 -1.9100463 -1.9767456 -2.0370302 -2.1238389 -2.2432714 -2.3872194 -2.6147394 -2.8988991 -3.1501489 -3.3074436 -3.3883853 -3.4699974][-1.4076891 -1.4468458 -1.4936368 -1.5197165 -1.5274041 -1.5506129 -1.60853 -1.720571 -1.9038415 -2.1941814 -2.5431747 -2.8638721 -3.1116343 -3.2843249 -3.4190636][-1.3555999 -1.3184617 -1.2837961 -1.2363489 -1.1801422 -1.180032 -1.2480853 -1.3992689 -1.6595914 -2.0377395 -2.4565749 -2.8314118 -3.1374664 -3.3544371 -3.476568][-1.3673272 -1.3163352 -1.2735097 -1.2233825 -1.1605833 -1.165278 -1.2494891 -1.42326 -1.7277951 -2.1656861 -2.6207404 -3.0079007 -3.3239985 -3.5346313 -3.6016266]]...]
INFO - root - 2017-12-07 04:04:21.654694: step 1510, loss = 0.79, batch loss = 0.72 (12.3 examples/sec; 2.599 sec/batch; 58h:55m:17s remains)
INFO - root - 2017-12-07 04:04:47.598643: step 1520, loss = 0.80, batch loss = 0.73 (11.9 examples/sec; 2.683 sec/batch; 60h:48m:49s remains)
INFO - root - 2017-12-07 04:05:14.015238: step 1530, loss = 0.76, batch loss = 0.68 (12.1 examples/sec; 2.656 sec/batch; 60h:11m:20s remains)
INFO - root - 2017-12-07 04:05:40.344033: step 1540, loss = 0.69, batch loss = 0.62 (12.1 examples/sec; 2.645 sec/batch; 59h:57m:09s remains)
INFO - root - 2017-12-07 04:06:06.366122: step 1550, loss = 0.70, batch loss = 0.62 (12.2 examples/sec; 2.622 sec/batch; 59h:24m:57s remains)
INFO - root - 2017-12-07 04:06:32.576745: step 1560, loss = 0.79, batch loss = 0.72 (12.0 examples/sec; 2.669 sec/batch; 60h:28m:20s remains)
INFO - root - 2017-12-07 04:06:58.804285: step 1570, loss = 0.75, batch loss = 0.68 (12.2 examples/sec; 2.617 sec/batch; 59h:17m:06s remains)
INFO - root - 2017-12-07 04:07:24.832437: step 1580, loss = 0.77, batch loss = 0.70 (12.3 examples/sec; 2.610 sec/batch; 59h:07m:44s remains)
INFO - root - 2017-12-07 04:07:51.147868: step 1590, loss = 0.69, batch loss = 0.61 (12.1 examples/sec; 2.649 sec/batch; 59h:59m:43s remains)
INFO - root - 2017-12-07 04:08:17.318413: step 1600, loss = 0.75, batch loss = 0.67 (12.4 examples/sec; 2.589 sec/batch; 58h:37m:16s remains)
2017-12-07 04:08:18.900342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8139789 -1.8676035 -2.1344573 -2.3190434 -2.3003349 -2.3904574 -2.7282443 -2.8372717 -2.6192183 -2.3683236 -2.0892866 -2.1500065 -2.40258 -1.9868324 -1.2591698][-1.8480539 -1.7805831 -2.0847652 -2.2860224 -2.194006 -2.2965903 -2.5720286 -2.7060523 -2.6627181 -2.4079144 -2.0218797 -2.0845928 -2.3562014 -1.9953284 -1.4055688][-1.9047821 -1.7059138 -2.026911 -2.2321997 -2.124959 -2.2878401 -2.5763564 -2.8497252 -2.9893963 -2.7127476 -2.2460437 -2.2236292 -2.4335923 -2.1730952 -1.723398][-2.1126497 -1.7094259 -1.9528513 -2.0629952 -1.8950524 -2.0325198 -2.3095412 -2.788909 -3.1594815 -2.9497676 -2.5001655 -2.3478932 -2.4439831 -2.3235493 -2.045202][-2.2410746 -1.67993 -1.7645874 -1.6962342 -1.3945935 -1.4042361 -1.5921626 -2.2043684 -2.7798653 -2.7228932 -2.3500049 -2.0228913 -1.9828405 -2.0284147 -1.9861765][-2.3125417 -1.6957023 -1.594347 -1.3468833 -0.91215539 -0.78584433 -0.88748026 -1.5498245 -2.2609773 -2.3347836 -1.9690025 -1.3886652 -1.1211727 -1.1821897 -1.3680871][-2.4609528 -1.8316128 -1.5593693 -1.233803 -0.74350238 -0.53284645 -0.57809925 -1.2237995 -2.06623 -2.3015456 -1.9654729 -1.1872106 -0.65719724 -0.6245532 -0.98517656][-2.4658103 -1.8385932 -1.4921155 -1.2309899 -0.78445578 -0.54699874 -0.5471909 -1.1292133 -2.0738389 -2.5282297 -2.3745415 -1.6403022 -0.9443779 -0.77279162 -1.1872048][-2.3241165 -1.7306736 -1.4058313 -1.2442625 -0.883219 -0.68238068 -0.66508079 -1.180032 -2.1782644 -2.8470027 -3.029511 -2.6317587 -1.9909532 -1.691015 -1.9114373][-2.2981706 -1.7669542 -1.5570352 -1.5339746 -1.3172019 -1.167423 -1.1158292 -1.5225055 -2.4143755 -3.1515276 -3.5557134 -3.4991221 -3.0601335 -2.7255683 -2.709446][-2.6953158 -2.2887998 -2.1906514 -2.2532859 -2.1335576 -2.0060287 -1.9002092 -2.1276429 -2.7783179 -3.3799734 -3.7241907 -3.7763171 -3.49167 -3.2298083 -3.1843696][-3.2941589 -3.0055444 -2.935328 -3.0094481 -2.9694424 -2.917995 -2.836154 -2.8861847 -3.1975927 -3.4582341 -3.5315056 -3.5031173 -3.3158038 -3.2047498 -3.3565984][-3.7388382 -3.566385 -3.5147722 -3.6297886 -3.7315512 -3.8328159 -3.8176887 -3.7242267 -3.6785681 -3.5124438 -3.2202072 -3.0056477 -2.8826373 -2.974566 -3.3600862][-3.9827607 -3.9339714 -3.9532313 -4.1190457 -4.3359022 -4.5131125 -4.5100956 -4.3540058 -4.1472397 -3.7439218 -3.2498183 -2.9360666 -2.8493447 -3.0284789 -3.4526494][-4.1798534 -4.1286173 -4.1364355 -4.266614 -4.5207877 -4.7539926 -4.8010917 -4.7274265 -4.5732894 -4.1746292 -3.6999002 -3.4233861 -3.3532262 -3.478755 -3.7703555]]...]
INFO - root - 2017-12-07 04:08:44.943425: step 1610, loss = 0.78, batch loss = 0.71 (13.0 examples/sec; 2.463 sec/batch; 55h:45m:55s remains)
INFO - root - 2017-12-07 04:09:11.248026: step 1620, loss = 0.80, batch loss = 0.73 (12.4 examples/sec; 2.584 sec/batch; 58h:29m:28s remains)
INFO - root - 2017-12-07 04:09:37.590052: step 1630, loss = 0.76, batch loss = 0.69 (12.0 examples/sec; 2.661 sec/batch; 60h:14m:34s remains)
INFO - root - 2017-12-07 04:10:03.794197: step 1640, loss = 0.74, batch loss = 0.66 (12.2 examples/sec; 2.618 sec/batch; 59h:15m:54s remains)
INFO - root - 2017-12-07 04:10:29.991015: step 1650, loss = 0.66, batch loss = 0.58 (12.3 examples/sec; 2.608 sec/batch; 59h:01m:07s remains)
INFO - root - 2017-12-07 04:10:56.199161: step 1660, loss = 0.77, batch loss = 0.70 (12.2 examples/sec; 2.614 sec/batch; 59h:09m:02s remains)
INFO - root - 2017-12-07 04:11:22.394127: step 1670, loss = 0.75, batch loss = 0.68 (12.1 examples/sec; 2.655 sec/batch; 60h:03m:59s remains)
INFO - root - 2017-12-07 04:11:48.612484: step 1680, loss = 0.63, batch loss = 0.56 (12.1 examples/sec; 2.654 sec/batch; 60h:01m:57s remains)
INFO - root - 2017-12-07 04:12:14.732194: step 1690, loss = 0.81, batch loss = 0.74 (12.5 examples/sec; 2.563 sec/batch; 57h:58m:16s remains)
INFO - root - 2017-12-07 04:12:40.773266: step 1700, loss = 0.83, batch loss = 0.76 (12.2 examples/sec; 2.626 sec/batch; 59h:24m:22s remains)
2017-12-07 04:12:42.339953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.913434 -3.0886507 -3.1397166 -3.0472231 -3.0218339 -3.116951 -3.1653447 -2.9874496 -2.7474198 -2.6908638 -2.82652 -2.9694242 -3.0143013 -3.1497569 -3.3476615][-2.8156018 -3.0972047 -3.073452 -2.8081017 -2.7254143 -2.8327606 -2.8146155 -2.4797571 -2.1347861 -2.1343312 -2.4579487 -2.704144 -2.7878895 -3.032547 -3.3673615][-2.6299763 -2.9979367 -2.8183203 -2.3527894 -2.2570369 -2.4138682 -2.3392484 -1.9095895 -1.5554764 -1.6559792 -2.1277683 -2.4591312 -2.6556857 -3.0522742 -3.4745841][-2.3143289 -2.6900482 -2.3152053 -1.7321448 -1.6949136 -1.8606119 -1.6680007 -1.1364245 -0.83162069 -1.0789752 -1.6150603 -1.9861345 -2.3489876 -2.967329 -3.5069885][-1.7530329 -2.0063038 -1.5141833 -1.0028079 -1.0737014 -1.1521976 -0.660198 0.12116337 0.31807613 -0.20334291 -0.77509046 -1.1184645 -1.596756 -2.4528937 -3.2210627][-1.1106989 -1.2290313 -0.784595 -0.45125771 -0.5692718 -0.4330132 0.54480696 1.7234178 1.7095685 0.71697044 -0.019564152 -0.3096323 -0.80073094 -1.7960575 -2.7760458][-0.58960581 -0.70220518 -0.45942712 -0.27731037 -0.29755259 0.18795681 1.6461043 3.0506711 2.6484551 1.1951919 0.2490387 -0.075479507 -0.55418086 -1.5418911 -2.5716295][-0.29963017 -0.54567671 -0.50244546 -0.39976788 -0.28662109 0.47950745 2.0480819 3.2106562 2.4594169 0.92113256 -0.074269772 -0.47183895 -0.87103152 -1.6679034 -2.5524273][-0.3628602 -0.69769549 -0.70376182 -0.60653043 -0.42584944 0.34773207 1.5523067 2.1569366 1.3910937 0.16195631 -0.76112127 -1.1909196 -1.4269629 -1.9388502 -2.6224737][-0.61014795 -0.98310661 -0.93781614 -0.82047439 -0.67976117 -0.10229635 0.66843939 0.95750952 0.45694828 -0.39416122 -1.2071452 -1.5752923 -1.6677763 -2.049021 -2.6867075][-0.9033792 -1.3889639 -1.3761156 -1.2855923 -1.1789064 -0.70168877 -0.11619139 0.12052011 -0.11430597 -0.68167996 -1.3177037 -1.5531652 -1.629421 -2.0398417 -2.7068679][-1.2215042 -1.8730428 -1.9766693 -1.8940744 -1.7177601 -1.154074 -0.55198431 -0.3102355 -0.45083046 -0.88334394 -1.3742332 -1.5216634 -1.6338661 -2.0665278 -2.7161994][-1.3614402 -2.2005613 -2.4577537 -2.3202698 -2.0042269 -1.3459148 -0.76880646 -0.628211 -0.82039571 -1.1514533 -1.5730627 -1.7641923 -1.9084332 -2.2549748 -2.7717373][-1.4159391 -2.3284523 -2.657927 -2.4703431 -2.0988109 -1.4858534 -1.013886 -1.0066144 -1.193867 -1.3835602 -1.7520659 -2.0043614 -2.1411502 -2.3790226 -2.7545311][-1.4648902 -2.3204947 -2.6108141 -2.4290128 -2.1917849 -1.7808807 -1.4383736 -1.5138752 -1.6299572 -1.7120378 -2.0378759 -2.2898569 -2.355232 -2.5118146 -2.8020363]]...]
INFO - root - 2017-12-07 04:13:08.331240: step 1710, loss = 0.77, batch loss = 0.69 (12.3 examples/sec; 2.606 sec/batch; 58h:56m:40s remains)
INFO - root - 2017-12-07 04:13:34.536250: step 1720, loss = 0.74, batch loss = 0.67 (12.2 examples/sec; 2.627 sec/batch; 59h:24m:43s remains)
INFO - root - 2017-12-07 04:14:00.732332: step 1730, loss = 0.71, batch loss = 0.63 (12.4 examples/sec; 2.584 sec/batch; 58h:25m:07s remains)
INFO - root - 2017-12-07 04:14:26.783711: step 1740, loss = 0.82, batch loss = 0.75 (12.2 examples/sec; 2.615 sec/batch; 59h:07m:30s remains)
INFO - root - 2017-12-07 04:14:52.969559: step 1750, loss = 0.79, batch loss = 0.71 (12.2 examples/sec; 2.623 sec/batch; 59h:16m:51s remains)
INFO - root - 2017-12-07 04:15:19.186161: step 1760, loss = 0.74, batch loss = 0.67 (12.3 examples/sec; 2.607 sec/batch; 58h:55m:57s remains)
INFO - root - 2017-12-07 04:15:45.025337: step 1770, loss = 0.70, batch loss = 0.63 (13.5 examples/sec; 2.369 sec/batch; 53h:31m:31s remains)
INFO - root - 2017-12-07 04:16:11.235097: step 1780, loss = 0.80, batch loss = 0.73 (12.2 examples/sec; 2.621 sec/batch; 59h:12m:55s remains)
INFO - root - 2017-12-07 04:16:37.390571: step 1790, loss = 0.69, batch loss = 0.62 (12.2 examples/sec; 2.612 sec/batch; 59h:01m:27s remains)
INFO - root - 2017-12-07 04:17:03.515676: step 1800, loss = 0.68, batch loss = 0.61 (12.3 examples/sec; 2.610 sec/batch; 58h:57m:24s remains)
2017-12-07 04:17:05.079041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3506253 -2.5072713 -2.717356 -2.8972917 -3.0179658 -3.0531907 -2.9763901 -2.8508794 -2.7587781 -2.6942286 -2.650301 -2.6223726 -2.5948219 -2.5729549 -2.5778561][-2.543674 -2.7956526 -3.1322522 -3.4132671 -3.5858152 -3.6303396 -3.5263095 -3.3646936 -3.2477837 -3.1697845 -3.1028194 -3.051281 -3.0119214 -2.9945743 -3.0231957][-2.7096541 -3.0268445 -3.4806714 -3.8536706 -4.0627875 -4.1176705 -4.011591 -3.88223 -3.8251407 -3.8043485 -3.7572742 -3.6907332 -3.6275275 -3.5962877 -3.613112][-2.7656789 -3.119257 -3.6659424 -4.08618 -4.2538834 -4.2326488 -4.0990644 -4.0546513 -4.1493974 -4.2889109 -4.3550406 -4.3368874 -4.2946405 -4.2489161 -4.2131815][-2.7739372 -3.1384363 -3.7170944 -4.1018538 -4.1457324 -3.9634557 -3.7518897 -3.7749467 -4.0356855 -4.3638425 -4.5985565 -4.6591492 -4.6384616 -4.5806646 -4.5151033][-2.8403583 -3.2102079 -3.7640176 -4.0572448 -3.9272611 -3.5028985 -3.0907252 -3.041985 -3.3873534 -3.9427383 -4.4368896 -4.6589818 -4.6918907 -4.6250448 -4.5313892][-3.0572014 -3.4058428 -3.878124 -4.0927711 -3.8558714 -3.1960654 -2.4451249 -2.0883288 -2.3003006 -3.0100884 -3.7962432 -4.2403355 -4.3957138 -4.3975749 -4.360899][-3.3878818 -3.6458254 -4.0103431 -4.2276306 -4.0209885 -3.2132511 -2.1038029 -1.2899168 -1.1873112 -1.9361336 -2.9368935 -3.5666094 -3.8269453 -3.8788888 -3.9157088][-3.8074586 -3.9317195 -4.1567507 -4.3724546 -4.2401867 -3.3822641 -2.0021229 -0.7851932 -0.40184164 -1.2091832 -2.3964484 -3.2060947 -3.5861194 -3.6324708 -3.5979557][-4.1129947 -4.0999832 -4.1966529 -4.3955398 -4.3394585 -3.5274205 -2.0962572 -0.73897719 -0.2700696 -0.99789882 -2.1182849 -2.9847178 -3.5218806 -3.6519094 -3.5386276][-4.2396021 -4.1246619 -4.1082077 -4.2402182 -4.2149615 -3.5525866 -2.3530314 -1.2077951 -0.81545305 -1.3009479 -2.0756814 -2.81216 -3.4262791 -3.6408777 -3.4561644][-4.2444725 -4.0549636 -3.8992 -3.8839903 -3.8033876 -3.3260665 -2.5095415 -1.7593021 -1.4978933 -1.7288876 -2.1362734 -2.7224712 -3.3787112 -3.651772 -3.4292173][-4.1974568 -3.9638321 -3.6655674 -3.472403 -3.3163757 -3.0275877 -2.5830753 -2.1820428 -2.0158362 -2.114383 -2.3224852 -2.7884865 -3.415293 -3.6980662 -3.4926755][-4.09501 -3.921391 -3.5908749 -3.2795396 -3.0524273 -2.8910742 -2.7285466 -2.5588121 -2.4588423 -2.5487924 -2.73957 -3.1632462 -3.6781812 -3.8411403 -3.545058][-3.8591979 -3.8356745 -3.5832191 -3.2476425 -2.9775579 -2.875741 -2.8771591 -2.783854 -2.6621051 -2.7593837 -3.0260825 -3.4874024 -3.9156573 -3.9950938 -3.6664083]]...]
INFO - root - 2017-12-07 04:17:31.072884: step 1810, loss = 0.75, batch loss = 0.67 (12.4 examples/sec; 2.576 sec/batch; 58h:10m:45s remains)
INFO - root - 2017-12-07 04:17:57.422192: step 1820, loss = 0.80, batch loss = 0.73 (12.2 examples/sec; 2.624 sec/batch; 59h:15m:36s remains)
INFO - root - 2017-12-07 04:18:23.973755: step 1830, loss = 0.71, batch loss = 0.64 (12.3 examples/sec; 2.610 sec/batch; 58h:55m:54s remains)
INFO - root - 2017-12-07 04:18:50.087402: step 1840, loss = 0.80, batch loss = 0.73 (12.4 examples/sec; 2.583 sec/batch; 58h:19m:08s remains)
INFO - root - 2017-12-07 04:19:16.431362: step 1850, loss = 0.83, batch loss = 0.76 (12.1 examples/sec; 2.638 sec/batch; 59h:34m:01s remains)
INFO - root - 2017-12-07 04:19:42.610474: step 1860, loss = 0.73, batch loss = 0.66 (12.1 examples/sec; 2.641 sec/batch; 59h:36m:45s remains)
INFO - root - 2017-12-07 04:20:08.597672: step 1870, loss = 0.74, batch loss = 0.66 (12.2 examples/sec; 2.623 sec/batch; 59h:12m:19s remains)
INFO - root - 2017-12-07 04:20:34.908802: step 1880, loss = 0.74, batch loss = 0.67 (11.9 examples/sec; 2.684 sec/batch; 60h:34m:03s remains)
INFO - root - 2017-12-07 04:21:01.269896: step 1890, loss = 0.71, batch loss = 0.64 (12.2 examples/sec; 2.621 sec/batch; 59h:08m:51s remains)
INFO - root - 2017-12-07 04:21:27.269134: step 1900, loss = 0.90, batch loss = 0.83 (12.4 examples/sec; 2.578 sec/batch; 58h:10m:01s remains)
2017-12-07 04:21:28.899954: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5773771 -3.4744315 -3.2502213 -3.0114875 -2.8467004 -2.7476075 -2.7865405 -2.8797991 -2.9704249 -3.0294566 -2.9949775 -2.8263774 -2.5948362 -2.4332175 -2.3897839][-3.9983783 -3.8958993 -3.6327434 -3.3776836 -3.2190771 -3.1296785 -3.1783481 -3.2851048 -3.384099 -3.4585056 -3.3965833 -3.1821923 -2.9443657 -2.8483543 -2.8724232][-4.1500711 -4.0014849 -3.6493416 -3.3396196 -3.1746194 -3.118794 -3.207418 -3.3372269 -3.4297123 -3.4761565 -3.3263209 -2.9700737 -2.6523671 -2.5858328 -2.7305596][-3.8477688 -3.6607604 -3.3576407 -3.2269039 -3.3302937 -3.5546932 -3.8280835 -4.0423832 -4.1520853 -4.1692224 -3.9702039 -3.5392447 -3.1716084 -3.068948 -3.149642][-3.3449349 -3.0440192 -2.7485893 -2.7652531 -3.0897293 -3.5283308 -3.8484354 -4.0241594 -4.0588827 -3.9842606 -3.7371387 -3.3767095 -3.158462 -3.1752419 -3.3041091][-3.1952634 -2.8639131 -2.5437236 -2.4888103 -2.6651845 -2.9171333 -3.0244703 -3.1176085 -3.1916971 -3.1249452 -2.8452611 -2.4835119 -2.3258207 -2.41703 -2.6788182][-3.1869926 -2.8291776 -2.4423165 -2.1763127 -1.9968343 -1.8754461 -1.7478802 -1.8347394 -2.0692923 -2.1839201 -2.0445809 -1.7860539 -1.6467254 -1.695926 -2.0255194][-3.114027 -2.6849809 -2.1733918 -1.6761594 -1.1812136 -0.73218489 -0.3598485 -0.32204866 -0.56438017 -0.81771994 -0.95325065 -1.0374994 -1.0910368 -1.121552 -1.391181][-3.1271811 -2.7823119 -2.3559592 -1.8890238 -1.3313522 -0.77563119 -0.31616068 -0.17747402 -0.29996014 -0.55210042 -0.90092945 -1.2425857 -1.3754148 -1.2617781 -1.2612638][-3.1662641 -2.9484415 -2.7065437 -2.4132288 -2.0092096 -1.5931902 -1.277241 -1.1864226 -1.2631066 -1.4438071 -1.7975385 -2.1158597 -2.1696134 -1.9411325 -1.7518978][-3.2749355 -3.1088333 -2.9540873 -2.771194 -2.5505681 -2.3650637 -2.2462742 -2.2430012 -2.3257694 -2.4744344 -2.7334981 -2.9048395 -2.8386736 -2.5842237 -2.3967102][-3.2620869 -3.1906655 -3.1242485 -3.0320449 -2.9670975 -2.9966745 -3.0473421 -3.1124668 -3.1918988 -3.3139548 -3.446372 -3.4738832 -3.3372238 -3.1585963 -3.0604568][-2.9259629 -2.90373 -2.9291263 -2.9366736 -2.9610646 -3.0885754 -3.2372618 -3.3033361 -3.3031816 -3.3136029 -3.3119359 -3.2721875 -3.1998591 -3.2403121 -3.3539438][-2.7144175 -2.6571283 -2.6784658 -2.6964707 -2.7297864 -2.8662834 -3.0152764 -3.0506022 -2.994009 -2.9382794 -2.8908775 -2.8462825 -2.8657827 -3.0575705 -3.3056557][-2.9579062 -2.9211435 -2.921813 -2.9009478 -2.8754337 -2.9352121 -3.0197387 -3.0478704 -3.0291233 -3.0203519 -3.0123711 -3.0152757 -3.0856662 -3.2598221 -3.4229023]]...]
INFO - root - 2017-12-07 04:21:55.172245: step 1910, loss = 0.71, batch loss = 0.64 (12.3 examples/sec; 2.605 sec/batch; 58h:46m:11s remains)
INFO - root - 2017-12-07 04:22:21.352342: step 1920, loss = 0.72, batch loss = 0.65 (12.1 examples/sec; 2.643 sec/batch; 59h:37m:41s remains)
INFO - root - 2017-12-07 04:22:47.457673: step 1930, loss = 0.79, batch loss = 0.72 (12.1 examples/sec; 2.651 sec/batch; 59h:47m:02s remains)
INFO - root - 2017-12-07 04:23:13.588427: step 1940, loss = 0.82, batch loss = 0.75 (12.4 examples/sec; 2.573 sec/batch; 58h:01m:17s remains)
INFO - root - 2017-12-07 04:23:39.811299: step 1950, loss = 0.77, batch loss = 0.70 (12.2 examples/sec; 2.631 sec/batch; 59h:19m:26s remains)
INFO - root - 2017-12-07 04:24:06.013003: step 1960, loss = 0.83, batch loss = 0.76 (12.3 examples/sec; 2.595 sec/batch; 58h:30m:48s remains)
INFO - root - 2017-12-07 04:24:31.974157: step 1970, loss = 0.77, batch loss = 0.69 (12.2 examples/sec; 2.615 sec/batch; 58h:57m:22s remains)
INFO - root - 2017-12-07 04:24:58.296601: step 1980, loss = 0.79, batch loss = 0.72 (12.5 examples/sec; 2.560 sec/batch; 57h:42m:41s remains)
INFO - root - 2017-12-07 04:25:24.608550: step 1990, loss = 0.79, batch loss = 0.72 (11.9 examples/sec; 2.684 sec/batch; 60h:29m:13s remains)
INFO - root - 2017-12-07 04:25:50.704172: step 2000, loss = 0.74, batch loss = 0.67 (12.0 examples/sec; 2.671 sec/batch; 60h:10m:54s remains)
2017-12-07 04:25:52.265641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1754143 -2.1980443 -2.3196723 -2.293571 -2.2060981 -2.2877395 -2.4062018 -2.4632251 -2.2756064 -2.0504646 -2.0586224 -2.2427297 -2.358721 -2.4568748 -2.7162967][-1.7020223 -1.8247986 -2.0927246 -2.0189984 -1.8449163 -1.9025776 -1.9447429 -1.8900418 -1.5124626 -1.2170668 -1.3407388 -1.7026336 -1.8805699 -2.0242314 -2.4441533][-1.1992142 -1.5316868 -1.9896448 -1.8010657 -1.4788983 -1.4875309 -1.4303489 -1.2540886 -0.71850061 -0.454926 -0.76694059 -1.2830434 -1.5028744 -1.7550616 -2.3590417][-0.67146063 -1.1696305 -1.6983535 -1.3163645 -0.88804126 -0.93114734 -0.83322048 -0.4871695 0.19047356 0.35508585 -0.14792871 -0.742697 -0.98194337 -1.4019642 -2.2375774][-0.10687637 -0.57798195 -0.95715117 -0.4118681 -0.055551052 -0.2564497 -0.18272161 0.42027807 1.339294 1.4104939 0.74196863 0.19806862 0.051959991 -0.49496055 -1.6284733][0.37832022 0.04863739 -0.16841125 0.33020878 0.47571802 0.15498447 0.34934139 1.4019032 2.6032176 2.4668155 1.5037379 0.94826651 0.90954256 0.335927 -0.953012][0.87922382 0.6655283 0.42034054 0.625134 0.53459263 0.24735641 0.76144361 2.3159833 3.6721716 3.1522717 1.8454404 1.152987 1.0843368 0.53787565 -0.70586443][1.2528934 1.0154743 0.5652566 0.49518824 0.32140303 0.17445612 1.0018239 2.6939802 3.8062515 2.9938169 1.6528587 0.88530445 0.69040155 0.18167019 -0.89099288][1.2825818 0.91246843 0.32697868 0.22736645 0.11893702 0.054868221 0.88019133 2.1808095 2.7781315 1.9994335 0.93664217 0.17869473 -0.11075068 -0.48360586 -1.269738][1.2135739 0.72368145 0.18680859 0.25513744 0.18050337 0.067617416 0.64140892 1.4201035 1.6733932 1.0969586 0.32202053 -0.46486998 -0.79176188 -1.0119865 -1.6008484][1.017417 0.38550186 -0.13600111 0.024974346 -0.03694725 -0.083042145 0.388072 0.9048872 1.0714688 0.72902918 0.21243477 -0.4919951 -0.83170819 -1.080864 -1.6568015][0.72542381 -0.05681181 -0.70685363 -0.64776587 -0.68944669 -0.5460422 -0.010568619 0.43513823 0.60616207 0.39757252 0.062564373 -0.44618893 -0.69119835 -0.92062712 -1.488147][0.51966572 -0.308578 -1.1126616 -1.2291229 -1.1750159 -0.84833956 -0.25508118 0.1151433 0.17032194 -0.088349819 -0.31676292 -0.66492844 -0.85557556 -1.0235441 -1.4921236][0.24363661 -0.49487114 -1.3496511 -1.582238 -1.4477522 -1.0685074 -0.53397179 -0.24636745 -0.29382658 -0.54097819 -0.66698456 -0.95258331 -1.1662269 -1.3111205 -1.6824126][-0.10037661 -0.72306108 -1.5644724 -1.8486409 -1.72844 -1.4695752 -1.1030784 -0.89156556 -0.997885 -1.1523068 -1.1892121 -1.4701767 -1.6843946 -1.772105 -2.0497892]]...]
INFO - root - 2017-12-07 04:26:18.625669: step 2010, loss = 0.82, batch loss = 0.75 (12.1 examples/sec; 2.635 sec/batch; 59h:22m:03s remains)
INFO - root - 2017-12-07 04:26:44.736828: step 2020, loss = 0.70, batch loss = 0.63 (12.3 examples/sec; 2.609 sec/batch; 58h:46m:13s remains)
INFO - root - 2017-12-07 04:27:10.812747: step 2030, loss = 0.68, batch loss = 0.61 (12.2 examples/sec; 2.624 sec/batch; 59h:07m:10s remains)
INFO - root - 2017-12-07 04:27:36.948628: step 2040, loss = 0.75, batch loss = 0.68 (12.3 examples/sec; 2.605 sec/batch; 58h:40m:05s remains)
INFO - root - 2017-12-07 04:28:03.255711: step 2050, loss = 0.79, batch loss = 0.72 (12.3 examples/sec; 2.598 sec/batch; 58h:30m:52s remains)
INFO - root - 2017-12-07 04:28:29.398016: step 2060, loss = 0.76, batch loss = 0.69 (12.1 examples/sec; 2.640 sec/batch; 59h:27m:08s remains)
INFO - root - 2017-12-07 04:28:55.599364: step 2070, loss = 0.78, batch loss = 0.71 (12.3 examples/sec; 2.609 sec/batch; 58h:44m:53s remains)
INFO - root - 2017-12-07 04:29:21.708755: step 2080, loss = 0.78, batch loss = 0.70 (12.3 examples/sec; 2.593 sec/batch; 58h:22m:51s remains)
INFO - root - 2017-12-07 04:29:47.857330: step 2090, loss = 0.73, batch loss = 0.66 (12.1 examples/sec; 2.636 sec/batch; 59h:19m:58s remains)
INFO - root - 2017-12-07 04:30:14.174660: step 2100, loss = 0.82, batch loss = 0.74 (11.8 examples/sec; 2.704 sec/batch; 60h:51m:29s remains)
2017-12-07 04:30:15.716009: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2910218 -2.8550408 -2.2563138 -1.8049176 -1.5747161 -1.7012522 -2.2045643 -2.6048386 -2.8270197 -3.0971813 -3.4080553 -3.5146961 -3.2271743 -2.7897663 -2.4093347][-3.2878273 -2.7807779 -2.0890491 -1.5011575 -1.0976202 -1.2336409 -1.9706376 -2.5374672 -2.6965938 -2.8073573 -3.038806 -3.1578653 -2.8658586 -2.409487 -2.031749][-3.4239137 -2.9896739 -2.2851024 -1.506654 -0.75002885 -0.72747731 -1.6607654 -2.44992 -2.5974073 -2.527473 -2.6273813 -2.7500968 -2.5108595 -2.0923054 -1.7849934][-3.4469841 -3.2079451 -2.632988 -1.7962341 -0.76839495 -0.52798414 -1.5520051 -2.5351331 -2.6961174 -2.4204836 -2.3346219 -2.4336829 -2.29619 -1.9998674 -1.836555][-3.3007674 -3.2537522 -2.8568037 -2.0894406 -0.92641592 -0.45077848 -1.461344 -2.671144 -2.9882135 -2.6004877 -2.3111389 -2.2943933 -2.1718152 -1.9478652 -1.875571][-3.1041808 -3.2258234 -3.0261931 -2.4045162 -1.1420536 -0.29148531 -0.971992 -2.2567747 -2.8470559 -2.6122012 -2.3394074 -2.2919223 -2.1806116 -1.993943 -1.9860344][-2.8990688 -3.1642203 -3.1992021 -2.7886472 -1.5313869 -0.30822945 -0.4266119 -1.5648425 -2.4153714 -2.4821997 -2.3557668 -2.3236313 -2.1508923 -1.9140592 -1.9277823][-2.932132 -3.2973738 -3.4939573 -3.1813078 -1.9719007 -0.4973824 -0.0745244 -0.95875049 -1.9762597 -2.2862291 -2.275259 -2.2214947 -1.9487021 -1.6128314 -1.5926995][-3.0636396 -3.4315157 -3.640193 -3.3085184 -2.2125144 -0.69038987 0.11996078 -0.46202469 -1.477706 -1.8591452 -1.9138718 -1.9707396 -1.8766441 -1.7304237 -1.8081841][-3.0180132 -3.1611493 -3.2235312 -2.8981338 -2.1012478 -0.85785413 0.0056409836 -0.36931038 -1.2938843 -1.6382937 -1.7009079 -1.8751385 -2.0039468 -2.0732782 -2.2051735][-2.9425449 -2.842042 -2.7981029 -2.5996795 -2.1990733 -1.3773398 -0.64064336 -0.84030104 -1.5391717 -1.7419515 -1.7464161 -1.9460866 -2.1495123 -2.2545328 -2.31626][-2.7987013 -2.5682311 -2.6108599 -2.7302034 -2.7407336 -2.3086772 -1.708324 -1.7093947 -2.0555618 -2.0406096 -1.9591374 -2.0845382 -2.2552693 -2.3213744 -2.3145144][-2.5224919 -2.4042203 -2.7012515 -3.1539385 -3.4206283 -3.1832731 -2.6128807 -2.3921494 -2.3594413 -2.1064162 -1.8945124 -1.8563025 -1.919745 -1.9291947 -1.9269986][-2.19976 -2.3054965 -2.8206143 -3.4332857 -3.7643723 -3.6210744 -3.0689554 -2.6759987 -2.3412137 -1.8734202 -1.523865 -1.3773317 -1.4655838 -1.5456116 -1.5927181][-1.8080456 -1.9312253 -2.4263587 -2.9544621 -3.2242823 -3.1955767 -2.8550563 -2.5685468 -2.2428231 -1.7636232 -1.3746905 -1.2330787 -1.463279 -1.6786182 -1.7528913]]...]
INFO - root - 2017-12-07 04:30:41.855209: step 2110, loss = 0.76, batch loss = 0.68 (12.0 examples/sec; 2.656 sec/batch; 59h:46m:16s remains)
INFO - root - 2017-12-07 04:31:07.962509: step 2120, loss = 0.87, batch loss = 0.80 (12.5 examples/sec; 2.566 sec/batch; 57h:44m:05s remains)
INFO - root - 2017-12-07 04:31:34.164397: step 2130, loss = 0.78, batch loss = 0.71 (12.2 examples/sec; 2.615 sec/batch; 58h:50m:22s remains)
INFO - root - 2017-12-07 04:32:00.146853: step 2140, loss = 0.82, batch loss = 0.75 (12.3 examples/sec; 2.602 sec/batch; 58h:32m:08s remains)
INFO - root - 2017-12-07 04:32:26.525263: step 2150, loss = 0.69, batch loss = 0.62 (12.3 examples/sec; 2.605 sec/batch; 58h:36m:18s remains)
INFO - root - 2017-12-07 04:32:52.467792: step 2160, loss = 0.75, batch loss = 0.68 (12.1 examples/sec; 2.637 sec/batch; 59h:18m:02s remains)
INFO - root - 2017-12-07 04:33:18.797584: step 2170, loss = 0.74, batch loss = 0.67 (12.4 examples/sec; 2.591 sec/batch; 58h:15m:40s remains)
INFO - root - 2017-12-07 04:33:44.992898: step 2180, loss = 0.81, batch loss = 0.74 (12.2 examples/sec; 2.627 sec/batch; 59h:04m:35s remains)
INFO - root - 2017-12-07 04:34:11.037154: step 2190, loss = 0.81, batch loss = 0.73 (11.9 examples/sec; 2.693 sec/batch; 60h:32m:43s remains)
INFO - root - 2017-12-07 04:34:37.227682: step 2200, loss = 0.72, batch loss = 0.65 (12.3 examples/sec; 2.608 sec/batch; 58h:37m:30s remains)
2017-12-07 04:34:38.820576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4631295 -3.460515 -3.4290588 -3.3663158 -3.2840991 -3.2202122 -3.1854751 -3.1441743 -3.146878 -3.2485452 -3.3845496 -3.5748892 -3.7131884 -3.8105712 -3.9449418][-3.0019274 -3.1022029 -3.2105217 -3.3054216 -3.3835475 -3.4784842 -3.5296926 -3.4523973 -3.3149211 -3.2451003 -3.2184973 -3.2481434 -3.2571015 -3.2929142 -3.4878135][-2.8006787 -2.8878288 -3.0544248 -3.277287 -3.5054398 -3.7265515 -3.7866805 -3.6013153 -3.2453902 -2.9657278 -2.7419348 -2.5094833 -2.2841771 -2.19105 -2.4229429][-2.1626143 -2.0582364 -2.1150265 -2.354182 -2.7011123 -3.0674682 -3.2518783 -3.194663 -2.9030566 -2.6477554 -2.3001676 -1.7039382 -1.070437 -0.69123316 -0.8380022][-0.65845275 -0.3870182 -0.36268759 -0.65572524 -1.2171671 -1.8652616 -2.387774 -2.6952634 -2.6932158 -2.6081333 -2.2305639 -1.3900051 -0.45393658 0.17161274 0.19991398][0.25479412 0.60040092 0.77208328 0.60890818 0.088684082 -0.53994584 -1.0636842 -1.423491 -1.6090705 -1.8398147 -1.8480976 -1.3724802 -0.69115353 -0.14848614 0.0094447136][-0.033270836 0.37721157 0.814486 1.0877466 1.1177745 1.1455135 1.2246766 1.1734452 0.78528881 -0.037089825 -0.84791708 -1.206943 -1.160516 -0.94067073 -0.78555608][-0.86262655 -0.48753977 0.086369991 0.67208672 1.1769032 1.7290001 2.2136006 2.3019528 1.734128 0.67679834 -0.38345957 -1.0822217 -1.3555584 -1.3969221 -1.4073353][-2.094907 -1.9449294 -1.5203571 -1.0173166 -0.54857612 -0.075285435 0.40582848 0.66323137 0.4784646 0.032045364 -0.4722662 -0.92321944 -1.2387524 -1.5293925 -1.8367283][-2.8064785 -2.9213142 -2.8012013 -2.5235815 -2.1767864 -1.8134763 -1.3093152 -0.82816672 -0.62181377 -0.52427173 -0.59290242 -0.84440637 -1.1897972 -1.6575665 -2.1711624][-1.9196694 -2.2925856 -2.5438595 -2.5612028 -2.3743439 -2.1393805 -1.7311687 -1.3248274 -1.1794841 -1.0844336 -1.1289895 -1.3392093 -1.6728232 -2.14784 -2.6518414][-0.054361343 -0.7239356 -1.3639212 -1.7158494 -1.7614586 -1.7404969 -1.6419625 -1.5782595 -1.7496662 -1.8904011 -2.0471585 -2.2521372 -2.4919381 -2.7960417 -3.0686531][1.3393693 0.56755161 -0.27494335 -0.90712976 -1.2687583 -1.5355625 -1.7542317 -1.9637814 -2.3266237 -2.6117973 -2.8125303 -2.9742584 -3.0890684 -3.1831145 -3.2196667][1.512713 0.766757 -0.036492348 -0.67284703 -1.1606627 -1.5665247 -1.9438689 -2.2695026 -2.6716497 -2.9882777 -3.1811347 -3.2976546 -3.3313785 -3.3219962 -3.2692866][0.5519805 -0.0028147697 -0.53956866 -0.95249009 -1.3513358 -1.7834535 -2.2490637 -2.662591 -3.0316892 -3.2601 -3.3625445 -3.4069402 -3.4068003 -3.3820255 -3.3403623]]...]
INFO - root - 2017-12-07 04:35:04.970285: step 2210, loss = 0.78, batch loss = 0.70 (12.0 examples/sec; 2.664 sec/batch; 59h:53m:12s remains)
INFO - root - 2017-12-07 04:35:30.926134: step 2220, loss = 0.73, batch loss = 0.66 (12.0 examples/sec; 2.657 sec/batch; 59h:43m:17s remains)
INFO - root - 2017-12-07 04:35:57.225721: step 2230, loss = 0.71, batch loss = 0.63 (11.9 examples/sec; 2.690 sec/batch; 60h:26m:36s remains)
INFO - root - 2017-12-07 04:36:23.340353: step 2240, loss = 0.79, batch loss = 0.72 (12.4 examples/sec; 2.579 sec/batch; 57h:56m:31s remains)
INFO - root - 2017-12-07 04:36:49.235354: step 2250, loss = 0.68, batch loss = 0.61 (12.2 examples/sec; 2.619 sec/batch; 58h:50m:19s remains)
INFO - root - 2017-12-07 04:37:15.262179: step 2260, loss = 0.72, batch loss = 0.65 (12.3 examples/sec; 2.591 sec/batch; 58h:12m:31s remains)
INFO - root - 2017-12-07 04:37:41.360174: step 2270, loss = 0.71, batch loss = 0.64 (12.3 examples/sec; 2.600 sec/batch; 58h:23m:24s remains)
INFO - root - 2017-12-07 04:38:07.133847: step 2280, loss = 0.77, batch loss = 0.70 (13.4 examples/sec; 2.391 sec/batch; 53h:41m:11s remains)
INFO - root - 2017-12-07 04:38:33.346906: step 2290, loss = 0.89, batch loss = 0.82 (12.2 examples/sec; 2.627 sec/batch; 58h:59m:03s remains)
INFO - root - 2017-12-07 04:38:59.588544: step 2300, loss = 0.67, batch loss = 0.60 (12.0 examples/sec; 2.671 sec/batch; 59h:57m:53s remains)
2017-12-07 04:39:01.224048: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0427821 -1.1637208 -0.3473382 -0.010762691 -0.026080608 -0.30889082 -0.39113426 -0.090509892 -0.4270978 -1.4095886 -1.8605862 -1.7080648 -1.6289754 -1.4008019 -0.7767303][-1.6864512 -0.87916374 -0.41346741 -0.45778608 -0.63537025 -0.76337314 -0.53471446 0.065104485 -0.075680733 -1.0348351 -1.5979481 -1.5511525 -1.5683689 -1.6149323 -1.3727121][-1.6593323 -1.0323455 -0.98632693 -1.4276175 -1.8149958 -1.8552165 -1.4061882 -0.59216881 -0.50414753 -1.3049545 -1.8254604 -1.7483346 -1.73701 -1.9056971 -1.9055719][-1.777235 -1.3809204 -1.7173064 -2.4946938 -3.0549695 -3.0329256 -2.4763336 -1.6196609 -1.3466268 -1.9294529 -2.3112566 -2.0541291 -1.8071213 -1.8540082 -1.921056][-1.7589397 -1.5031359 -2.0436091 -2.9347486 -3.4164515 -3.1790876 -2.4980788 -1.6472516 -1.3153894 -1.8466468 -2.2139297 -1.8119402 -1.2761614 -1.1426063 -1.2869596][-1.9473724 -1.7587864 -2.3036675 -2.9630566 -2.9410548 -2.1614885 -1.1708593 -0.2966032 -0.10219193 -0.84306574 -1.3684511 -0.92546916 -0.11403227 0.16078091 -0.1674962][-2.501569 -2.4184275 -2.8257074 -2.9720519 -2.1591396 -0.66075134 0.67978716 1.4020104 1.0940018 -0.1226964 -0.8975687 -0.43565869 0.63578129 1.0133452 0.49631643][-2.951019 -2.9456253 -3.1492097 -2.795176 -1.3200359 0.56518316 1.8529143 2.0879483 1.1012697 -0.55986333 -1.4537995 -0.87350512 0.3801446 0.76422262 0.10358334][-2.9703674 -2.7948184 -2.7360883 -2.1353922 -0.62866282 0.84322071 1.3842678 0.92939043 -0.44230247 -2.0736787 -2.6995573 -1.8128972 -0.4453342 -0.16948223 -0.90502262][-2.6375732 -2.0008669 -1.5850141 -0.967515 0.055493832 0.58486223 0.15985012 -0.75244594 -2.066 -3.3193388 -3.4513397 -2.1219692 -0.69397664 -0.62866592 -1.4302902][-2.519361 -1.4670467 -0.77347541 -0.26503086 0.27116966 0.1208806 -0.76184845 -1.6388452 -2.5990834 -3.4702461 -3.2195511 -1.6045282 -0.34461212 -0.68280435 -1.6215718][-2.6595576 -1.4140806 -0.66706395 -0.41118193 -0.278749 -0.69945478 -1.4358516 -1.8504212 -2.3677425 -3.029212 -2.6725121 -1.0666044 -0.19699621 -0.98949623 -2.0298986][-2.9232755 -1.7272766 -1.0990255 -1.1249573 -1.2761819 -1.691849 -1.9980464 -1.8554358 -1.9954705 -2.5189142 -2.161376 -0.70493174 -0.27776909 -1.4299808 -2.4731665][-3.1982441 -2.0960879 -1.5879045 -1.7506914 -2.0076673 -2.3318191 -2.2771044 -1.836201 -1.8816614 -2.4092577 -2.0920143 -0.76096606 -0.58714414 -1.8348968 -2.7545891][-3.4822464 -2.5408621 -2.1342468 -2.2622366 -2.3827946 -2.4651823 -2.1315374 -1.6217732 -1.7732515 -2.3965077 -2.1343749 -0.86702681 -0.68608212 -1.824584 -2.6363513]]...]
INFO - root - 2017-12-07 04:39:27.401146: step 2310, loss = 0.71, batch loss = 0.64 (12.2 examples/sec; 2.619 sec/batch; 58h:46m:56s remains)
INFO - root - 2017-12-07 04:39:53.553015: step 2320, loss = 0.74, batch loss = 0.67 (12.2 examples/sec; 2.613 sec/batch; 58h:38m:24s remains)
INFO - root - 2017-12-07 04:40:19.719750: step 2330, loss = 0.69, batch loss = 0.61 (12.2 examples/sec; 2.616 sec/batch; 58h:42m:09s remains)
INFO - root - 2017-12-07 04:40:45.781424: step 2340, loss = 0.67, batch loss = 0.60 (12.3 examples/sec; 2.607 sec/batch; 58h:30m:19s remains)
INFO - root - 2017-12-07 04:41:11.813933: step 2350, loss = 0.85, batch loss = 0.77 (12.3 examples/sec; 2.592 sec/batch; 58h:09m:11s remains)
INFO - root - 2017-12-07 04:41:37.950442: step 2360, loss = 0.91, batch loss = 0.84 (12.2 examples/sec; 2.628 sec/batch; 58h:58m:03s remains)
INFO - root - 2017-12-07 04:42:04.081870: step 2370, loss = 0.78, batch loss = 0.71 (12.2 examples/sec; 2.629 sec/batch; 58h:58m:50s remains)
INFO - root - 2017-12-07 04:42:30.143848: step 2380, loss = 0.80, batch loss = 0.73 (11.9 examples/sec; 2.684 sec/batch; 60h:11m:24s remains)
INFO - root - 2017-12-07 04:42:56.244943: step 2390, loss = 0.73, batch loss = 0.66 (12.3 examples/sec; 2.604 sec/batch; 58h:23m:28s remains)
INFO - root - 2017-12-07 04:43:22.430596: step 2400, loss = 0.74, batch loss = 0.67 (12.1 examples/sec; 2.646 sec/batch; 59h:19m:30s remains)
2017-12-07 04:43:24.009127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.89334154 -0.11092901 1.6149092 2.4448495 1.4450665 -0.32283735 -2.2215176 -3.4544306 -3.7677412 -3.6380649 -3.3651111 -3.2142315 -3.0247374 -2.756562 -2.7753062][-0.88858438 -0.51378512 0.84328127 1.6296849 1.1564698 0.099250793 -1.4972725 -2.8764858 -3.6753826 -3.813309 -3.4485157 -3.2634478 -3.0917802 -2.9490323 -3.1022935][-0.10323477 0.297359 1.1847734 1.6526299 1.4297538 0.73350477 -0.65608764 -1.9678724 -2.9456787 -3.2667251 -2.9851646 -2.9695039 -2.9382906 -2.9783964 -3.2995625][-0.18728352 0.42993212 1.546237 2.6174922 2.8198214 1.8914022 0.051449776 -1.5337608 -2.7387245 -3.2496588 -3.1065435 -3.1556315 -3.0912197 -3.1647749 -3.570677][-1.5903814 -0.92250276 0.73199987 2.7838798 3.5870647 2.6744862 0.91950369 -0.48069811 -1.8546154 -2.8818698 -3.2784657 -3.6073503 -3.5885663 -3.596873 -3.9348078][-3.2284603 -2.6409755 -0.85733223 1.5103202 2.6822476 2.3968329 1.785748 1.4192781 0.23530531 -1.4188294 -2.7330995 -3.7760124 -4.1246486 -4.1697197 -4.3560209][-4.1839342 -3.7914169 -2.3582389 -0.49564815 0.643662 1.3116903 2.376658 3.449831 2.6775417 0.40611267 -1.869154 -3.5834775 -4.2847905 -4.4081554 -4.4941621][-3.8289776 -3.6146023 -2.7449439 -1.7361782 -0.90826583 0.36339617 2.5997782 4.8765688 4.8262463 2.552743 -0.14997768 -2.3585415 -3.5113549 -3.8635507 -3.9106271][-3.4296768 -3.2692008 -2.7345467 -2.3090696 -1.8639901 -0.98721385 0.79301834 3.2101121 4.1349964 2.9895029 0.87104654 -1.3518186 -2.8831382 -3.5827355 -3.6602178][-4.1165566 -3.8851032 -3.3454542 -3.0473981 -2.764493 -2.4848239 -1.6635156 0.24319506 1.7185574 1.5229411 0.022591591 -1.9920666 -3.586539 -4.4863176 -4.6761756][-4.6284256 -4.4646258 -3.885113 -3.4633303 -3.0568521 -2.8796182 -2.4929304 -1.0539424 0.36549997 0.3744483 -0.89732504 -2.6457515 -4.0454726 -4.9514914 -5.2738571][-4.8690228 -4.9156818 -4.4834604 -4.0184197 -3.4979284 -3.2072866 -2.9834356 -2.0008636 -0.973907 -1.0301452 -2.0117912 -3.2086654 -4.176363 -4.984971 -5.4592528][-4.7213788 -4.8547688 -4.6027374 -4.2555509 -3.79443 -3.4078116 -3.1378379 -2.4397449 -1.7696395 -1.8386073 -2.4082563 -3.02528 -3.5858648 -4.2488556 -4.739924][-4.3500667 -4.4562564 -4.3245187 -4.1114736 -3.8069229 -3.5210383 -3.3122525 -2.8031983 -2.2944994 -2.1825418 -2.3240764 -2.5518117 -2.8880901 -3.3655939 -3.6824808][-4.0012341 -3.9716222 -3.889477 -3.7929246 -3.629477 -3.4554331 -3.3407197 -3.1092563 -2.851567 -2.70749 -2.6025155 -2.6146874 -2.793386 -3.0566132 -3.1890218]]...]
INFO - root - 2017-12-07 04:43:49.795285: step 2410, loss = 0.80, batch loss = 0.73 (12.3 examples/sec; 2.604 sec/batch; 58h:22m:58s remains)
INFO - root - 2017-12-07 04:44:16.005590: step 2420, loss = 0.73, batch loss = 0.66 (12.3 examples/sec; 2.609 sec/batch; 58h:28m:54s remains)
INFO - root - 2017-12-07 04:44:42.047332: step 2430, loss = 0.73, batch loss = 0.65 (12.1 examples/sec; 2.635 sec/batch; 59h:03m:58s remains)
INFO - root - 2017-12-07 04:45:08.167244: step 2440, loss = 0.83, batch loss = 0.76 (12.0 examples/sec; 2.665 sec/batch; 59h:43m:51s remains)
INFO - root - 2017-12-07 04:45:34.719520: step 2450, loss = 0.75, batch loss = 0.68 (12.2 examples/sec; 2.616 sec/batch; 58h:37m:22s remains)
INFO - root - 2017-12-07 04:46:01.012263: step 2460, loss = 0.77, batch loss = 0.70 (12.3 examples/sec; 2.592 sec/batch; 58h:04m:34s remains)
INFO - root - 2017-12-07 04:46:27.421346: step 2470, loss = 0.76, batch loss = 0.68 (12.0 examples/sec; 2.656 sec/batch; 59h:30m:44s remains)
INFO - root - 2017-12-07 04:46:53.230956: step 2480, loss = 0.73, batch loss = 0.66 (12.1 examples/sec; 2.639 sec/batch; 59h:07m:08s remains)
INFO - root - 2017-12-07 04:47:19.642178: step 2490, loss = 0.75, batch loss = 0.68 (11.9 examples/sec; 2.686 sec/batch; 60h:10m:18s remains)
INFO - root - 2017-12-07 04:47:46.046795: step 2500, loss = 0.68, batch loss = 0.61 (12.1 examples/sec; 2.635 sec/batch; 59h:00m:36s remains)
2017-12-07 04:47:47.609056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6310484 -1.9859481 -1.6918683 -1.7290142 -1.9951398 -2.2336001 -2.33852 -2.3987696 -2.4339347 -2.4485083 -2.4569695 -2.4736819 -2.4963362 -2.5188875 -2.5474868][-2.9216518 -2.4836915 -2.4501858 -2.6984363 -3.0753593 -3.309994 -3.3520665 -3.3743725 -3.3929558 -3.3968692 -3.4064794 -3.4442306 -3.4847827 -3.5022373 -3.5200148][-3.3063269 -3.1267178 -3.349853 -3.7527857 -4.1514444 -4.298409 -4.213994 -4.1367464 -4.096806 -4.0581007 -4.0611768 -4.1299672 -4.1917448 -4.1812696 -4.1264191][-3.4716809 -3.4194932 -3.7187881 -4.1082273 -4.4086123 -4.4338784 -4.2663727 -4.1365948 -4.0843706 -4.0250835 -4.0307856 -4.1516461 -4.2783623 -4.2718887 -4.1124005][-3.4411609 -3.4263253 -3.6953902 -3.9765813 -4.1057324 -3.9983287 -3.7707288 -3.6200898 -3.5766735 -3.5242023 -3.5575972 -3.779428 -4.0653629 -4.1751366 -3.9558203][-3.4473686 -3.4847832 -3.7224088 -3.8694313 -3.7746603 -3.4326048 -2.997251 -2.6768045 -2.5480194 -2.517765 -2.6582544 -3.0462241 -3.5511096 -3.8528993 -3.6603417][-3.5067477 -3.5721953 -3.7163868 -3.6229467 -3.1615634 -2.3941441 -1.5792162 -1.0113623 -0.89571404 -1.125658 -1.6388693 -2.3785048 -3.1501875 -3.6512976 -3.5694396][-3.4572399 -3.4396434 -3.3635278 -2.9255664 -2.0617096 -0.89486885 0.20473671 0.81217861 0.60992908 -0.17748928 -1.2801509 -2.447859 -3.4093876 -3.9490438 -3.9050517][-3.3729155 -3.3094711 -3.185667 -2.7193871 -1.8802192 -0.83223176 0.0618639 0.41522741 0.023993969 -0.88855147 -2.0595119 -3.2061911 -4.027092 -4.3868809 -4.2826629][-3.4926844 -3.5643334 -3.7012873 -3.6100574 -3.2263327 -2.6277037 -2.0926774 -1.8843651 -2.0940354 -2.5931215 -3.2725158 -3.956908 -4.406199 -4.5767641 -4.5352478][-3.7151055 -3.966516 -4.3897738 -4.654706 -4.6206231 -4.2892962 -3.8761654 -3.6048172 -3.5166287 -3.5452466 -3.7048564 -3.9545465 -4.1225286 -4.2016225 -4.2635684][-3.7204452 -3.9998708 -4.4879804 -4.8565893 -4.9206395 -4.6426325 -4.2280293 -3.9025438 -3.6875708 -3.5272057 -3.4866924 -3.5850909 -3.6996017 -3.7818718 -3.876446][-3.556191 -3.7721269 -4.1998973 -4.5351548 -4.5949426 -4.3463478 -4.0028615 -3.7670417 -3.6350534 -3.5356841 -3.543879 -3.6710625 -3.8279183 -3.9467449 -4.0753536][-3.4639776 -3.6454935 -4.0285826 -4.34667 -4.4491863 -4.3025885 -4.0703478 -3.9196486 -3.8251843 -3.7347524 -3.7392511 -3.8465323 -4.0121584 -4.1718206 -4.3587179][-3.4599183 -3.5738347 -3.8380675 -4.0479021 -4.1282649 -4.0586381 -3.9406359 -3.8912508 -3.8576307 -3.7840919 -3.7630923 -3.8277071 -3.9819577 -4.1714082 -4.4090319]]...]
INFO - root - 2017-12-07 04:48:13.580142: step 2510, loss = 0.77, batch loss = 0.70 (12.3 examples/sec; 2.602 sec/batch; 58h:16m:20s remains)
INFO - root - 2017-12-07 04:48:39.992407: step 2520, loss = 0.77, batch loss = 0.69 (12.0 examples/sec; 2.658 sec/batch; 59h:31m:01s remains)
INFO - root - 2017-12-07 04:49:06.165269: step 2530, loss = 0.71, batch loss = 0.64 (11.8 examples/sec; 2.718 sec/batch; 60h:50m:56s remains)
INFO - root - 2017-12-07 04:49:32.459460: step 2540, loss = 0.75, batch loss = 0.68 (12.2 examples/sec; 2.624 sec/batch; 58h:44m:46s remains)
INFO - root - 2017-12-07 04:49:58.649274: step 2550, loss = 0.83, batch loss = 0.75 (12.3 examples/sec; 2.609 sec/batch; 58h:24m:09s remains)
INFO - root - 2017-12-07 04:50:24.903442: step 2560, loss = 0.74, batch loss = 0.67 (12.2 examples/sec; 2.620 sec/batch; 58h:38m:03s remains)
INFO - root - 2017-12-07 04:50:51.039085: step 2570, loss = 0.83, batch loss = 0.76 (12.2 examples/sec; 2.615 sec/batch; 58h:30m:33s remains)
INFO - root - 2017-12-07 04:51:17.293852: step 2580, loss = 0.77, batch loss = 0.70 (12.3 examples/sec; 2.595 sec/batch; 58h:04m:14s remains)
INFO - root - 2017-12-07 04:51:43.608033: step 2590, loss = 0.75, batch loss = 0.68 (12.1 examples/sec; 2.643 sec/batch; 59h:08m:08s remains)
INFO - root - 2017-12-07 04:52:09.515745: step 2600, loss = 0.82, batch loss = 0.75 (12.3 examples/sec; 2.595 sec/batch; 58h:03m:20s remains)
2017-12-07 04:52:11.177438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.19449854 -0.48314381 -0.89258742 -1.3176179 -1.708945 -2.1550822 -2.7711892 -2.9112782 -2.8238993 -3.0333333 -3.220705 -3.214241 -2.971252 -2.6229322 -2.5281982][-0.32678652 -0.55922532 -0.86169457 -1.2340672 -1.5839748 -1.9471996 -2.4627893 -2.624197 -2.6152153 -2.8680034 -3.0882277 -3.1944027 -3.0744061 -2.7235372 -2.5722868][-0.52023244 -0.7109921 -0.94536257 -1.2604611 -1.5296352 -1.8382664 -2.292428 -2.4705119 -2.4692829 -2.6581271 -2.8111882 -2.9984689 -3.0496042 -2.7753651 -2.6072855][-0.6656096 -0.81639242 -1.0407193 -1.3785579 -1.6362803 -1.9104395 -2.3000524 -2.4146519 -2.3440251 -2.4031684 -2.4411688 -2.6661067 -2.8982468 -2.7698269 -2.6421056][-0.94690943 -0.95376635 -1.0659523 -1.3813801 -1.6085112 -1.8204885 -2.1466093 -2.1670179 -2.0391994 -2.0212059 -2.0195286 -2.2919714 -2.6804781 -2.7440119 -2.7101555][-1.2795169 -1.1211066 -1.0325902 -1.1891782 -1.3312116 -1.539464 -1.9268906 -2.0084655 -1.9657218 -1.9654171 -1.9532046 -2.187582 -2.5725362 -2.7410145 -2.8024762][-1.4379754 -1.3208694 -1.1725545 -1.1598239 -1.2072141 -1.4735417 -1.9687977 -2.1873565 -2.2970042 -2.3425803 -2.3003457 -2.3979104 -2.6174655 -2.7701979 -2.8970022][-1.1418428 -1.2105145 -1.2243018 -1.2049744 -1.2338479 -1.5705309 -2.0601506 -2.3353086 -2.5596492 -2.6594896 -2.609798 -2.6078453 -2.6518359 -2.7655096 -2.9544921][-0.61543155 -0.78548074 -1.0031879 -1.0718153 -1.1531506 -1.5212131 -1.8982279 -2.1460152 -2.4079647 -2.5043035 -2.5116343 -2.5504441 -2.5402842 -2.6889923 -2.9618366][-0.16569471 -0.28768349 -0.63797879 -0.82158661 -1.0126534 -1.3999763 -1.654778 -1.8756683 -2.1200602 -2.1466892 -2.2120724 -2.3374393 -2.3522418 -2.5765824 -2.9363155][0.33673525 0.38368368 0.0083494186 -0.3474679 -0.74727225 -1.1677268 -1.3403938 -1.5710003 -1.7997518 -1.7858405 -1.9593489 -2.1908069 -2.2177069 -2.4893317 -2.9101343][0.843452 1.0765729 0.83242846 0.40636921 -0.19731188 -0.69140077 -0.88403368 -1.1779938 -1.421932 -1.4724193 -1.8342447 -2.1862786 -2.2216525 -2.5018487 -2.9280324][0.93250942 1.2244844 1.1907625 0.87456656 0.25951195 -0.25416517 -0.5415988 -0.94609094 -1.2325404 -1.3824086 -1.9092596 -2.3305047 -2.3553488 -2.5994356 -2.9719739][0.52249956 0.72790718 0.83444929 0.686069 0.22955847 -0.1882081 -0.51895595 -1.0001178 -1.3123553 -1.5203793 -2.1083763 -2.5380464 -2.5420337 -2.7157001 -3.0010951][-0.11161757 -0.056153297 0.0615263 0.040192127 -0.21164083 -0.45566368 -0.73749185 -1.2054582 -1.4991999 -1.7256365 -2.2849979 -2.6871247 -2.698009 -2.8147361 -3.0164459]]...]
INFO - root - 2017-12-07 04:52:37.362382: step 2610, loss = 0.84, batch loss = 0.76 (12.2 examples/sec; 2.630 sec/batch; 58h:48m:48s remains)
INFO - root - 2017-12-07 04:53:03.587180: step 2620, loss = 0.73, batch loss = 0.66 (12.3 examples/sec; 2.597 sec/batch; 58h:05m:07s remains)
INFO - root - 2017-12-07 04:53:29.746404: step 2630, loss = 0.61, batch loss = 0.54 (12.2 examples/sec; 2.630 sec/batch; 58h:48m:10s remains)
INFO - root - 2017-12-07 04:53:55.679479: step 2640, loss = 0.78, batch loss = 0.71 (12.4 examples/sec; 2.586 sec/batch; 57h:48m:28s remains)
INFO - root - 2017-12-07 04:54:21.957752: step 2650, loss = 0.79, batch loss = 0.72 (12.2 examples/sec; 2.620 sec/batch; 58h:34m:11s remains)
INFO - root - 2017-12-07 04:54:48.109707: step 2660, loss = 0.82, batch loss = 0.75 (12.2 examples/sec; 2.630 sec/batch; 58h:46m:22s remains)
INFO - root - 2017-12-07 04:55:14.049964: step 2670, loss = 0.80, batch loss = 0.72 (12.2 examples/sec; 2.624 sec/batch; 58h:39m:05s remains)
INFO - root - 2017-12-07 04:55:40.314234: step 2680, loss = 0.73, batch loss = 0.66 (12.1 examples/sec; 2.642 sec/batch; 59h:02m:50s remains)
INFO - root - 2017-12-07 04:56:06.444232: step 2690, loss = 0.73, batch loss = 0.66 (12.1 examples/sec; 2.642 sec/batch; 59h:01m:59s remains)
INFO - root - 2017-12-07 04:56:32.661293: step 2700, loss = 0.80, batch loss = 0.73 (12.1 examples/sec; 2.654 sec/batch; 59h:17m:49s remains)
2017-12-07 04:56:34.256286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0106783 -2.1647117 -2.2645121 -2.3119504 -2.3621852 -2.3207757 -2.2247028 -2.1965086 -2.0750651 -1.9191782 -1.8595078 -1.8533909 -1.9180152 -1.9390266 -1.9233704][-1.4659119 -1.6632845 -1.8136697 -1.8854337 -1.9566185 -1.9165306 -1.8710423 -1.8916624 -1.709089 -1.4655228 -1.371479 -1.3514047 -1.397928 -1.4401414 -1.4777319][-0.977216 -1.2065685 -1.4089954 -1.4987092 -1.5771794 -1.5590911 -1.5724959 -1.6091535 -1.3381965 -1.0128474 -0.91159296 -0.89172173 -0.93517351 -1.0210261 -1.1409464][-0.62883234 -0.819644 -1.013257 -1.1038947 -1.1944892 -1.1616604 -1.1606619 -1.1689918 -0.83874488 -0.51352572 -0.4815836 -0.51701164 -0.6066246 -0.74645543 -0.94763207][-0.45566869 -0.48421645 -0.57060647 -0.63153625 -0.68552017 -0.56319618 -0.4917264 -0.49978924 -0.22338104 -0.0046935081 -0.081990242 -0.1848774 -0.34678555 -0.55714965 -0.83657908][-0.31370449 -0.17446995 -0.12236452 -0.09632349 -0.03156662 0.2571826 0.4242301 0.35352802 0.47101212 0.52820015 0.36568975 0.25116014 0.040242672 -0.24966192 -0.61573935][-0.13867617 0.059262753 0.19327211 0.3186841 0.48510647 0.866374 1.0623856 0.89112759 0.88905478 0.87627077 0.72917986 0.68640804 0.49957991 0.17756176 -0.22618198][-0.00016117096 0.1600647 0.3205061 0.50983763 0.645596 0.94384718 1.1154227 0.90838051 0.87736511 0.8664403 0.7776227 0.83484793 0.7069664 0.40540218 0.061630726][-0.043408871 0.022842407 0.14398575 0.30903196 0.36967373 0.55776548 0.6966424 0.537004 0.54336119 0.58251333 0.55016088 0.62936592 0.4979434 0.22251558 0.0034747124][-0.27534294 -0.34924984 -0.34617662 -0.28380108 -0.27325392 -0.12790298 0.019143581 -0.037667751 0.02400589 0.088898182 0.095069885 0.16457748 0.0094294548 -0.21331167 -0.28606081][-0.49917579 -0.65823936 -0.69872189 -0.6999104 -0.74997377 -0.68011427 -0.57530808 -0.57161593 -0.5114181 -0.49010754 -0.50679493 -0.46762371 -0.5941422 -0.68489885 -0.5990448][-0.55672812 -0.68651724 -0.67617536 -0.65979314 -0.746269 -0.77928925 -0.77572584 -0.77797031 -0.77351689 -0.85584259 -0.94755411 -0.98579216 -1.0728543 -1.0046513 -0.78528833][-0.35421133 -0.34006691 -0.25485086 -0.25311661 -0.40883446 -0.54324865 -0.63487911 -0.66704226 -0.74067259 -0.91028547 -1.0641832 -1.2122023 -1.3114619 -1.1440234 -0.86595178][0.083868027 0.24664211 0.35581923 0.30365419 0.10168266 -0.10120678 -0.26902437 -0.36523008 -0.51661205 -0.73901439 -0.94048548 -1.1936326 -1.333509 -1.1522012 -0.91198277][0.33331013 0.55754185 0.63045788 0.52103662 0.32225275 0.12476254 -0.037308693 -0.16201925 -0.37416887 -0.6257062 -0.85082412 -1.1588748 -1.3246732 -1.1916373 -1.0418491]]...]
INFO - root - 2017-12-07 04:57:00.491694: step 2710, loss = 0.77, batch loss = 0.70 (12.1 examples/sec; 2.649 sec/batch; 59h:10m:15s remains)
INFO - root - 2017-12-07 04:57:26.536283: step 2720, loss = 0.89, batch loss = 0.82 (12.5 examples/sec; 2.569 sec/batch; 57h:22m:07s remains)
INFO - root - 2017-12-07 04:57:52.652219: step 2730, loss = 0.77, batch loss = 0.70 (12.2 examples/sec; 2.621 sec/batch; 58h:32m:00s remains)
INFO - root - 2017-12-07 04:58:18.881571: step 2740, loss = 0.80, batch loss = 0.72 (12.1 examples/sec; 2.635 sec/batch; 58h:50m:17s remains)
INFO - root - 2017-12-07 04:58:44.958269: step 2750, loss = 0.80, batch loss = 0.73 (12.3 examples/sec; 2.603 sec/batch; 58h:07m:05s remains)
INFO - root - 2017-12-07 04:59:11.042656: step 2760, loss = 0.67, batch loss = 0.60 (12.2 examples/sec; 2.629 sec/batch; 58h:41m:54s remains)
INFO - root - 2017-12-07 04:59:37.433623: step 2770, loss = 0.69, batch loss = 0.62 (12.1 examples/sec; 2.651 sec/batch; 59h:10m:55s remains)
INFO - root - 2017-12-07 05:00:03.486670: step 2780, loss = 0.85, batch loss = 0.78 (12.2 examples/sec; 2.630 sec/batch; 58h:42m:01s remains)
INFO - root - 2017-12-07 05:00:29.563219: step 2790, loss = 0.73, batch loss = 0.66 (12.9 examples/sec; 2.474 sec/batch; 55h:11m:54s remains)
INFO - root - 2017-12-07 05:00:55.795904: step 2800, loss = 0.71, batch loss = 0.63 (12.3 examples/sec; 2.605 sec/batch; 58h:06m:53s remains)
2017-12-07 05:00:57.355825: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4098077 -3.3026142 -3.2647181 -3.5116253 -3.8654819 -3.8677197 -3.6759412 -3.6102297 -3.5753021 -3.3938241 -3.3859448 -3.5360186 -3.6466794 -3.7685168 -3.8144317][-3.2991083 -3.096293 -2.9485292 -3.3245039 -3.8709862 -3.9283826 -3.7288227 -3.6177745 -3.4090009 -2.9724505 -2.8423195 -3.0481458 -3.3691077 -3.6831412 -3.7789807][-2.9895184 -2.8784714 -2.7912784 -3.3084421 -3.9012566 -3.8807521 -3.5778351 -3.379807 -3.0357752 -2.369657 -2.0723 -2.2572961 -2.7866387 -3.3978548 -3.6364098][-2.5482755 -2.6877682 -2.8170402 -3.3664606 -3.7310977 -3.3420506 -2.7076497 -2.3692951 -2.0281136 -1.3578031 -0.97453785 -1.1141994 -1.7527394 -2.6167188 -3.066493][-2.1678715 -2.5430126 -2.9260011 -3.415658 -3.4363456 -2.584034 -1.5308201 -1.1121609 -1.0448544 -0.7089777 -0.37930202 -0.33489704 -0.74024868 -1.5083463 -2.017349][-2.2458718 -2.7359357 -3.2693591 -3.6396422 -3.3361094 -2.0301697 -0.52975535 -0.065751076 -0.4541173 -0.66482782 -0.58455348 -0.37049913 -0.34572172 -0.70381713 -1.0346282][-2.5841568 -2.8869524 -3.1856534 -3.1840498 -2.5174732 -0.87426686 0.88372087 1.2375498 0.28905249 -0.55063319 -0.92678738 -0.84373 -0.611176 -0.592746 -0.60885119][-2.7388625 -2.5116115 -2.32514 -1.960959 -1.1204634 0.47723341 2.0788598 2.1090293 0.74096394 -0.44398928 -1.1290329 -1.2452693 -0.96548748 -0.71628 -0.47584558][-2.8430252 -2.1343305 -1.5945218 -1.1251864 -0.35761213 0.88454008 1.9964991 1.6801343 0.29945183 -0.72927356 -1.329546 -1.4748604 -1.1365211 -0.73159337 -0.34830713][-2.798774 -1.8607337 -1.2825232 -1.000227 -0.4577589 0.34545422 0.91321468 0.37515163 -0.75019646 -1.3816614 -1.6732502 -1.704772 -1.2646482 -0.73565817 -0.23975706][-2.259145 -1.2739921 -0.84036231 -0.85787225 -0.61730838 -0.19094086 -0.027231693 -0.69130611 -1.5941701 -1.9448102 -2.0355251 -1.9493635 -1.4367304 -0.8815012 -0.34421492][-1.7651122 -0.96383238 -0.80883861 -1.0742347 -1.007247 -0.72536731 -0.68114781 -1.2799091 -1.9716415 -2.1788392 -2.2139184 -2.0726275 -1.5815942 -1.1484485 -0.72802472][-1.6119459 -1.1419425 -1.2051382 -1.5030711 -1.4756818 -1.21714 -1.1410894 -1.5488489 -1.9611888 -1.9724364 -1.9072528 -1.7469134 -1.4251623 -1.2793908 -1.0865104][-1.5921721 -1.4494977 -1.6375706 -1.8901849 -1.8961594 -1.7443542 -1.6889083 -1.8733943 -1.9583793 -1.712553 -1.4911685 -1.3225515 -1.2315307 -1.3929193 -1.4388196][-1.4543357 -1.5036304 -1.6919091 -1.8464441 -1.8786681 -1.8348451 -1.8184266 -1.8807132 -1.8096688 -1.5069466 -1.2484224 -1.0886626 -1.1357312 -1.4485924 -1.6482666]]...]
INFO - root - 2017-12-07 05:01:23.642591: step 2810, loss = 0.78, batch loss = 0.71 (11.8 examples/sec; 2.712 sec/batch; 60h:30m:42s remains)
INFO - root - 2017-12-07 05:01:49.858873: step 2820, loss = 0.78, batch loss = 0.71 (12.2 examples/sec; 2.622 sec/batch; 58h:29m:44s remains)
INFO - root - 2017-12-07 05:02:15.957205: step 2830, loss = 0.78, batch loss = 0.70 (12.3 examples/sec; 2.612 sec/batch; 58h:15m:13s remains)
INFO - root - 2017-12-07 05:02:42.143606: step 2840, loss = 0.71, batch loss = 0.64 (12.2 examples/sec; 2.624 sec/batch; 58h:31m:20s remains)
INFO - root - 2017-12-07 05:03:08.314136: step 2850, loss = 0.81, batch loss = 0.74 (12.2 examples/sec; 2.630 sec/batch; 58h:38m:51s remains)
INFO - root - 2017-12-07 05:03:34.271447: step 2860, loss = 0.76, batch loss = 0.69 (12.2 examples/sec; 2.623 sec/batch; 58h:29m:28s remains)
INFO - root - 2017-12-07 05:04:00.587559: step 2870, loss = 0.71, batch loss = 0.64 (12.3 examples/sec; 2.600 sec/batch; 57h:58m:14s remains)
INFO - root - 2017-12-07 05:04:26.712558: step 2880, loss = 0.70, batch loss = 0.63 (12.0 examples/sec; 2.658 sec/batch; 59h:14m:54s remains)
INFO - root - 2017-12-07 05:04:52.714595: step 2890, loss = 0.80, batch loss = 0.73 (12.3 examples/sec; 2.601 sec/batch; 57h:58m:31s remains)
INFO - root - 2017-12-07 05:05:18.959380: step 2900, loss = 0.94, batch loss = 0.86 (12.4 examples/sec; 2.582 sec/batch; 57h:32m:32s remains)
2017-12-07 05:05:20.520463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.87733078 -0.96895123 -1.0294075 -1.0776372 -1.0744772 -1.034313 -0.99955678 -0.97124004 -0.92798567 -0.86768341 -0.81548929 -0.78101444 -0.78847289 -0.84525847 -0.8873][-0.64589572 -0.70415354 -0.72531629 -0.73093867 -0.69921017 -0.65991807 -0.64233208 -0.62996459 -0.59676576 -0.54785275 -0.5017004 -0.46030974 -0.45899391 -0.52061796 -0.5768249][-0.41600084 -0.45884633 -0.47507405 -0.48394084 -0.46889687 -0.4689157 -0.49755883 -0.52972293 -0.53679752 -0.52729344 -0.50162315 -0.4533875 -0.43452549 -0.47931027 -0.50778127][-0.27159452 -0.31984329 -0.35821915 -0.39685583 -0.40953207 -0.44445968 -0.52882934 -0.636426 -0.7171073 -0.75526476 -0.72974443 -0.651886 -0.60303092 -0.63000727 -0.625762][-0.046553612 -0.10071802 -0.16970968 -0.24811316 -0.27776289 -0.31379557 -0.43894148 -0.66654038 -0.91548538 -1.0768602 -1.0667717 -0.93510962 -0.82241035 -0.81266284 -0.76925611][0.14435339 0.086209774 -0.0066585541 -0.11018419 -0.11179781 -0.057302 -0.10588169 -0.3998313 -0.88398051 -1.3160491 -1.4650514 -1.3662634 -1.2080112 -1.1106403 -0.94882822][0.21419764 0.1559782 0.056931019 -0.046625614 -0.017201424 0.1227808 0.19516087 -0.02063036 -0.57507992 -1.2067547 -1.5630245 -1.6136668 -1.527194 -1.4003303 -1.1181571][0.31633568 0.26484632 0.18018818 0.098620892 0.14227915 0.29351473 0.39706612 0.27748537 -0.17107916 -0.78285885 -1.2104461 -1.3871324 -1.434443 -1.3681178 -1.0740745][0.49573231 0.44343233 0.36924553 0.31120396 0.35354948 0.44685698 0.43228245 0.24398136 -0.15044355 -0.6330564 -0.96070957 -1.1150095 -1.1984479 -1.1600983 -0.89717555][0.68396854 0.62896061 0.56523609 0.52608681 0.57670975 0.63216496 0.52177048 0.23495436 -0.16847229 -0.54357839 -0.73696136 -0.82497382 -0.9093678 -0.89360833 -0.69191813][0.7103591 0.65340328 0.60355473 0.56566477 0.61126375 0.67724705 0.60838175 0.35649395 -0.0096554756 -0.31831217 -0.43778443 -0.49990749 -0.59681463 -0.61420536 -0.47246075][0.7275753 0.69522429 0.67671776 0.63750172 0.63387966 0.66646338 0.64985752 0.49842739 0.23995876 0.032668591 -0.0028066635 -0.016917229 -0.11125469 -0.1880002 -0.14194536][0.93452072 0.92277527 0.92834282 0.89577961 0.85000229 0.82458115 0.80101061 0.70380545 0.54004145 0.42277908 0.4413414 0.46214724 0.36389971 0.21784353 0.16080952][1.1157451 1.0936112 1.092628 1.0645475 1.0092793 0.94736242 0.88961983 0.80451727 0.69046688 0.605041 0.61309433 0.64079285 0.55920267 0.39100552 0.29100657][1.1077251 1.05302 1.0236979 0.99039078 0.95974493 0.92709541 0.890275 0.84538937 0.7827487 0.71740866 0.69542456 0.69875717 0.61212063 0.42147493 0.28266954]]...]
INFO - root - 2017-12-07 05:05:46.897521: step 2910, loss = 0.79, batch loss = 0.71 (11.9 examples/sec; 2.680 sec/batch; 59h:43m:18s remains)
INFO - root - 2017-12-07 05:06:12.872448: step 2920, loss = 0.88, batch loss = 0.81 (12.3 examples/sec; 2.609 sec/batch; 58h:08m:08s remains)
INFO - root - 2017-12-07 05:06:39.143220: step 2930, loss = 0.77, batch loss = 0.70 (12.1 examples/sec; 2.637 sec/batch; 58h:44m:42s remains)
INFO - root - 2017-12-07 05:07:05.231040: step 2940, loss = 0.70, batch loss = 0.62 (12.1 examples/sec; 2.640 sec/batch; 58h:48m:19s remains)
INFO - root - 2017-12-07 05:07:31.240391: step 2950, loss = 0.75, batch loss = 0.68 (13.1 examples/sec; 2.449 sec/batch; 54h:32m:32s remains)
INFO - root - 2017-12-07 05:07:57.517737: step 2960, loss = 0.72, batch loss = 0.65 (12.3 examples/sec; 2.608 sec/batch; 58h:04m:57s remains)
INFO - root - 2017-12-07 05:08:23.630205: step 2970, loss = 0.80, batch loss = 0.72 (12.3 examples/sec; 2.593 sec/batch; 57h:44m:41s remains)
INFO - root - 2017-12-07 05:08:50.008715: step 2980, loss = 0.77, batch loss = 0.70 (12.2 examples/sec; 2.627 sec/batch; 58h:29m:15s remains)
INFO - root - 2017-12-07 05:09:16.045751: step 2990, loss = 0.67, batch loss = 0.60 (11.9 examples/sec; 2.686 sec/batch; 59h:47m:01s remains)
INFO - root - 2017-12-07 05:09:42.453324: step 3000, loss = 0.71, batch loss = 0.63 (12.2 examples/sec; 2.627 sec/batch; 58h:28m:45s remains)
2017-12-07 05:09:44.047703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3783371 -1.4756896 -1.7920573 -2.1978569 -2.5368402 -2.6949153 -2.6701095 -2.4958367 -2.2661619 -2.115473 -1.9469764 -1.7091029 -1.5578206 -1.6694095 -2.0143547][-1.435107 -1.5887918 -1.9138515 -2.3231351 -2.7362576 -3.0294113 -3.1709018 -3.1700444 -3.0377169 -2.8737888 -2.6307914 -2.2753761 -1.9924598 -1.9456785 -2.0982802][-1.8819673 -1.9668419 -2.1257985 -2.3579638 -2.6992147 -3.0303793 -3.2652323 -3.4094276 -3.4067111 -3.2917469 -3.0400395 -2.6465478 -2.2773633 -2.0833039 -2.0562677][-2.1707041 -2.1051381 -2.0364811 -2.0559552 -2.2907052 -2.6357679 -2.9295864 -3.1933467 -3.3330281 -3.281316 -3.0278785 -2.621501 -2.20384 -1.9386747 -1.8597658][-2.1065769 -1.8499529 -1.5778587 -1.4437134 -1.6270592 -2.0202079 -2.3617713 -2.6827416 -2.9009032 -2.8872569 -2.6428769 -2.2790236 -1.8965685 -1.6333528 -1.5536821][-1.9397891 -1.5025158 -1.0937088 -0.84336352 -0.92606211 -1.2650602 -1.5410857 -1.7938986 -1.9893589 -2.0062008 -1.8734035 -1.7559612 -1.6735237 -1.6191854 -1.6425579][-1.8876519 -1.3712173 -0.97472072 -0.70644307 -0.62360549 -0.70972276 -0.7032547 -0.69262075 -0.73083568 -0.7735374 -0.86320424 -1.1642649 -1.5629768 -1.8624651 -2.055002][-1.908329 -1.5535707 -1.3843348 -1.2419767 -1.0015059 -0.73574662 -0.32304811 0.079863071 0.28202343 0.22548294 -0.059780598 -0.68654752 -1.4286366 -1.9982638 -2.3515275][-1.7620549 -1.762373 -1.9214122 -1.9776268 -1.7051375 -1.269032 -0.6913588 -0.14021015 0.11603117 0.056529045 -0.2632308 -0.88975906 -1.5877681 -2.1095061 -2.4522457][-1.5540268 -1.8725548 -2.2786736 -2.4800162 -2.3012483 -1.9681985 -1.5889196 -1.2358131 -1.0824332 -1.0936253 -1.2749767 -1.6198156 -1.9476223 -2.1458678 -2.2757027][-1.5201735 -1.9217463 -2.3484423 -2.5352221 -2.4372392 -2.3103583 -2.27014 -2.2333498 -2.2032559 -2.1288934 -2.099582 -2.102648 -2.0398567 -1.9263811 -1.845789][-1.6840088 -1.9427445 -2.1918952 -2.2471242 -2.1876774 -2.2168195 -2.4029958 -2.584341 -2.6323242 -2.5217688 -2.3784082 -2.1874621 -1.9203491 -1.6623187 -1.4637721][-2.0948758 -2.1169591 -2.0955739 -1.9762735 -1.902488 -1.9788806 -2.2149031 -2.4695072 -2.5890269 -2.566803 -2.4671493 -2.2534595 -1.9630618 -1.7023487 -1.4585145][-2.4421883 -2.2833943 -2.0952084 -1.9329264 -1.90676 -2.0001931 -2.1943226 -2.428196 -2.5743885 -2.6377859 -2.5984621 -2.4215021 -2.2102365 -2.0475886 -1.8445747][-2.5648336 -2.3648224 -2.1815922 -2.0996785 -2.1591511 -2.2697885 -2.4212329 -2.6018944 -2.7079856 -2.7643645 -2.7384162 -2.6159151 -2.5296314 -2.5168729 -2.4013746]]...]
INFO - root - 2017-12-07 05:10:10.269704: step 3010, loss = 0.74, batch loss = 0.67 (12.2 examples/sec; 2.632 sec/batch; 58h:34m:32s remains)
INFO - root - 2017-12-07 05:10:36.424221: step 3020, loss = 0.80, batch loss = 0.73 (12.2 examples/sec; 2.629 sec/batch; 58h:29m:46s remains)
INFO - root - 2017-12-07 05:11:02.921764: step 3030, loss = 0.89, batch loss = 0.81 (12.2 examples/sec; 2.629 sec/batch; 58h:29m:38s remains)
INFO - root - 2017-12-07 05:11:29.103120: step 3040, loss = 0.72, batch loss = 0.65 (12.1 examples/sec; 2.650 sec/batch; 58h:56m:49s remains)
INFO - root - 2017-12-07 05:11:55.166471: step 3050, loss = 0.75, batch loss = 0.67 (12.2 examples/sec; 2.632 sec/batch; 58h:32m:09s remains)
INFO - root - 2017-12-07 05:12:21.460081: step 3060, loss = 0.78, batch loss = 0.71 (12.1 examples/sec; 2.639 sec/batch; 58h:40m:54s remains)
INFO - root - 2017-12-07 05:12:47.664195: step 3070, loss = 0.73, batch loss = 0.66 (12.2 examples/sec; 2.623 sec/batch; 58h:19m:15s remains)
INFO - root - 2017-12-07 05:13:13.641123: step 3080, loss = 0.79, batch loss = 0.71 (12.3 examples/sec; 2.606 sec/batch; 57h:55m:57s remains)
INFO - root - 2017-12-07 05:13:39.969670: step 3090, loss = 0.77, batch loss = 0.70 (12.1 examples/sec; 2.638 sec/batch; 58h:39m:18s remains)
INFO - root - 2017-12-07 05:14:06.096039: step 3100, loss = 0.80, batch loss = 0.73 (12.2 examples/sec; 2.618 sec/batch; 58h:12m:12s remains)
2017-12-07 05:14:07.683178: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9852896 -2.1034734 -1.9944367 -2.5107908 -3.1068454 -3.4156995 -3.2097769 -2.673913 -2.3123255 -1.9730175 -1.2840054 -1.061635 -1.8067853 -2.5235069 -2.7668366][-2.1588757 -1.4856763 -1.6251397 -2.2376375 -2.8956227 -3.2365079 -2.9516172 -2.3031406 -1.8164983 -1.4777398 -0.918946 -0.9141562 -1.8313758 -2.5832963 -2.7878385][-1.4148886 -1.166369 -1.5709724 -2.1700556 -2.7929831 -3.0883577 -2.6712785 -1.874058 -1.2392282 -0.89955831 -0.57030058 -0.86793089 -1.8675935 -2.5428038 -2.7053983][-1.0675328 -1.214298 -1.7949886 -2.4084454 -3.0373375 -3.1935468 -2.4956992 -1.4160109 -0.60902548 -0.32000542 -0.30803061 -0.94565463 -1.9681487 -2.5273118 -2.6366191][-1.0346265 -1.3488312 -1.9445279 -2.5948496 -3.2419829 -3.1682022 -2.1313953 -0.80762911 0.070036888 0.16358662 -0.25147724 -1.2034686 -2.1763709 -2.5803893 -2.5940056][-0.98502111 -1.3400207 -1.8657293 -2.440064 -2.8699827 -2.3329744 -1.0370347 0.15003443 0.66303873 0.31461716 -0.51505494 -1.5999572 -2.4055686 -2.6286225 -2.5376711][-0.70926809 -1.1761024 -1.6972747 -2.0388503 -1.9444087 -0.79351616 0.56914616 1.1214352 0.82019472 -0.12819529 -1.1921446 -2.1190329 -2.6375494 -2.6799803 -2.4916046][-0.37052202 -0.96024323 -1.4934847 -1.5206482 -0.852577 0.74516296 1.8866062 1.5178185 0.29120111 -1.0681183 -2.025038 -2.5566311 -2.7669601 -2.6775084 -2.4411547][-0.18015814 -0.77349734 -1.2035286 -0.90786052 0.10486507 1.6767282 2.2872052 1.0849266 -0.70208955 -2.0027497 -2.527262 -2.629632 -2.6790936 -2.5952506 -2.3997209][-0.13500786 -0.64093041 -0.89999175 -0.4028883 0.5596962 1.6555753 1.6508565 0.063508511 -1.6628754 -2.5181594 -2.494005 -2.2840965 -2.3462679 -2.4127479 -2.343349][-0.085650444 -0.50710106 -0.66074657 -0.16209555 0.4995389 0.99865294 0.56340265 -0.90532541 -2.1589544 -2.4964042 -2.0625508 -1.7323294 -1.9135852 -2.2029855 -2.2907357][0.078826904 -0.28432655 -0.47312069 -0.13120127 0.22601795 0.30366993 -0.26468563 -1.3610542 -2.1021781 -2.1126361 -1.5853829 -1.27953 -1.5652969 -2.0535531 -2.2644706][0.27588367 -0.060089111 -0.37446737 -0.1866107 0.033652306 -0.042017937 -0.55323124 -1.270638 -1.6717923 -1.5939665 -1.2026212 -0.97938943 -1.3029134 -1.9220612 -2.2358065][0.46815634 0.12536716 -0.31581736 -0.26690626 -0.086119652 -0.14934874 -0.49486446 -0.9240191 -1.1428859 -1.082056 -0.88356233 -0.75283384 -1.09109 -1.8132756 -2.2038815][0.52863646 0.14262486 -0.3151741 -0.3398881 -0.20118952 -0.24720383 -0.43708515 -0.64976621 -0.74086142 -0.66629076 -0.58954763 -0.527338 -0.88756824 -1.715986 -2.1744664]]...]
INFO - root - 2017-12-07 05:14:40.043037: step 3110, loss = 0.78, batch loss = 0.70 (8.7 examples/sec; 3.693 sec/batch; 82h:05m:11s remains)
INFO - root - 2017-12-07 05:15:16.886032: step 3120, loss = 0.81, batch loss = 0.74 (8.7 examples/sec; 3.687 sec/batch; 81h:55m:59s remains)
INFO - root - 2017-12-07 05:15:53.373206: step 3130, loss = 0.77, batch loss = 0.70 (8.8 examples/sec; 3.641 sec/batch; 80h:54m:56s remains)
INFO - root - 2017-12-07 05:16:30.126976: step 3140, loss = 0.80, batch loss = 0.73 (8.8 examples/sec; 3.649 sec/batch; 81h:04m:15s remains)
INFO - root - 2017-12-07 05:17:06.554715: step 3150, loss = 0.77, batch loss = 0.70 (8.7 examples/sec; 3.678 sec/batch; 81h:42m:01s remains)
INFO - root - 2017-12-07 05:17:43.111928: step 3160, loss = 0.77, batch loss = 0.70 (8.7 examples/sec; 3.659 sec/batch; 81h:16m:12s remains)
INFO - root - 2017-12-07 05:18:19.761174: step 3170, loss = 0.76, batch loss = 0.68 (8.8 examples/sec; 3.654 sec/batch; 81h:09m:53s remains)
INFO - root - 2017-12-07 05:18:56.573196: step 3180, loss = 0.67, batch loss = 0.60 (8.5 examples/sec; 3.773 sec/batch; 83h:47m:23s remains)
INFO - root - 2017-12-07 05:19:33.273847: step 3190, loss = 0.77, batch loss = 0.70 (8.6 examples/sec; 3.702 sec/batch; 82h:12m:07s remains)
INFO - root - 2017-12-07 05:20:10.217808: step 3200, loss = 0.77, batch loss = 0.70 (8.8 examples/sec; 3.646 sec/batch; 80h:56m:32s remains)
2017-12-07 05:20:12.316612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4268155 -2.4855995 -2.5593491 -2.6494031 -2.8019657 -2.9453468 -3.053823 -3.1303406 -3.1253543 -3.1043768 -3.066298 -3.003015 -2.9214077 -2.8339348 -2.7577219][-2.3297558 -2.3644042 -2.4178307 -2.5200305 -2.7143211 -2.8863678 -3.056504 -3.1825957 -3.2112536 -3.2556396 -3.2918861 -3.296469 -3.218034 -3.0752401 -2.9319191][-2.2071605 -2.139169 -2.1144216 -2.1886561 -2.3728569 -2.5613887 -2.8160939 -2.9867496 -3.0230105 -3.0976307 -3.1689465 -3.2449939 -3.2176254 -3.0894604 -2.9398584][-2.1463902 -1.9915955 -1.9168441 -1.9571483 -2.0648429 -2.1921418 -2.4363177 -2.5647779 -2.6053519 -2.7009091 -2.7831945 -2.8883455 -2.8920367 -2.7933896 -2.6823063][-2.0826461 -1.9567096 -1.9444921 -1.9532645 -1.8558888 -1.7070558 -1.6711833 -1.5826359 -1.5941706 -1.7407775 -1.93695 -2.1821666 -2.2931693 -2.2844925 -2.2731674][-1.9752731 -1.9099228 -1.955307 -1.8494952 -1.3885095 -0.77717304 -0.21662903 0.12173605 0.047258854 -0.19120407 -0.56688547 -0.997 -1.2968602 -1.550736 -1.8000085][-1.7426238 -1.5863655 -1.5166574 -1.1413448 -0.30725431 0.70957327 1.7222853 2.0378451 1.6123924 1.1160979 0.52199459 0.034427643 -0.30982733 -0.77723193 -1.3359299][-1.5535226 -1.1828389 -0.91682529 -0.34952879 0.50600004 1.548708 2.649992 2.7210512 2.0537882 1.3507962 0.63494205 0.27288866 0.048757553 -0.44337821 -1.1109202][-1.6213968 -1.1793258 -0.81144595 -0.26560259 0.254169 0.89923191 1.5872517 1.5293593 1.0818281 0.52820396 0.010998249 -0.1242981 -0.23233891 -0.66234612 -1.2594645][-2.0653112 -1.8768804 -1.6254299 -1.2206662 -0.95787978 -0.60175109 -0.22613573 -0.19717312 -0.32963419 -0.61338639 -0.84783578 -0.870281 -0.98896646 -1.3339722 -1.7080355][-2.5867009 -2.7296753 -2.6880322 -2.4692321 -2.3566935 -2.1787951 -1.9577751 -1.780215 -1.6649415 -1.7035348 -1.7306981 -1.7530396 -1.9286456 -2.1465774 -2.240977][-2.9392936 -3.2721152 -3.366993 -3.2477636 -3.1623883 -3.0840216 -2.9942241 -2.8524246 -2.7099051 -2.6156511 -2.5146062 -2.5119395 -2.6481218 -2.7343974 -2.6482339][-2.8847232 -3.2132204 -3.3722057 -3.3502073 -3.3057384 -3.2851515 -3.2725434 -3.1985457 -3.0970631 -2.997004 -2.8974628 -2.9015145 -2.9769764 -2.9778705 -2.8361087][-2.666688 -2.8842914 -3.0133624 -3.0350029 -3.0336823 -3.0567122 -3.0765052 -3.0399942 -2.9642935 -2.8876545 -2.84029 -2.8586659 -2.8972523 -2.8719153 -2.7570953][-2.4902692 -2.6098871 -2.6740036 -2.6751735 -2.66748 -2.675518 -2.6847937 -2.6627555 -2.6064858 -2.5629275 -2.5553005 -2.5714922 -2.5829582 -2.5646744 -2.5228434]]...]
INFO - root - 2017-12-07 05:20:48.708086: step 3210, loss = 0.72, batch loss = 0.64 (8.7 examples/sec; 3.678 sec/batch; 81h:38m:11s remains)
INFO - root - 2017-12-07 05:21:25.461033: step 3220, loss = 0.74, batch loss = 0.66 (8.7 examples/sec; 3.676 sec/batch; 81h:35m:17s remains)
INFO - root - 2017-12-07 05:22:01.859561: step 3230, loss = 0.77, batch loss = 0.69 (8.6 examples/sec; 3.701 sec/batch; 82h:07m:59s remains)
INFO - root - 2017-12-07 05:22:38.294232: step 3240, loss = 0.78, batch loss = 0.70 (8.7 examples/sec; 3.679 sec/batch; 81h:37m:43s remains)
INFO - root - 2017-12-07 05:23:14.824268: step 3250, loss = 0.82, batch loss = 0.75 (8.7 examples/sec; 3.689 sec/batch; 81h:51m:08s remains)
INFO - root - 2017-12-07 05:23:51.620913: step 3260, loss = 0.77, batch loss = 0.69 (8.4 examples/sec; 3.804 sec/batch; 84h:22m:47s remains)
INFO - root - 2017-12-07 05:24:28.189654: step 3270, loss = 0.85, batch loss = 0.78 (8.7 examples/sec; 3.664 sec/batch; 81h:17m:05s remains)
INFO - root - 2017-12-07 05:25:05.067887: step 3280, loss = 0.72, batch loss = 0.64 (8.8 examples/sec; 3.638 sec/batch; 80h:41m:30s remains)
INFO - root - 2017-12-07 05:25:41.738363: step 3290, loss = 0.84, batch loss = 0.77 (8.9 examples/sec; 3.606 sec/batch; 79h:58m:29s remains)
INFO - root - 2017-12-07 05:26:18.492605: step 3300, loss = 0.72, batch loss = 0.65 (9.0 examples/sec; 3.538 sec/batch; 78h:26m:21s remains)
2017-12-07 05:26:20.505280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3974936 -3.3610744 -3.3983915 -3.498431 -3.5653162 -3.553592 -3.5249443 -3.5223174 -3.5513077 -3.5947049 -3.618052 -3.5912957 -3.5075445 -3.4247901 -3.3752046][-2.9373662 -2.8373969 -2.8893578 -3.0605927 -3.1592319 -3.1083393 -3.0550859 -3.1169477 -3.2954795 -3.4610724 -3.5052443 -3.4500093 -3.3229408 -3.249939 -3.2482719][-2.7444112 -2.5899634 -2.63992 -2.8749738 -3.008481 -2.927541 -2.8574595 -2.9761176 -3.2725444 -3.4923561 -3.4873857 -3.3509493 -3.132093 -3.0397916 -3.0835235][-2.91569 -2.6784329 -2.6829867 -2.9263887 -3.0578637 -2.927027 -2.7867129 -2.8661652 -3.1540937 -3.3328214 -3.2494116 -3.0419946 -2.7430739 -2.618434 -2.7140329][-2.9306407 -2.5753481 -2.5023868 -2.69911 -2.7920175 -2.597496 -2.3584337 -2.3240154 -2.5304179 -2.6970229 -2.6546278 -2.4718978 -2.1209662 -1.9425199 -2.0490706][-2.4445076 -1.9404416 -1.8137107 -2.0066056 -2.1279652 -1.9853528 -1.7805212 -1.7050319 -1.8278286 -1.9707329 -2.0235195 -1.9600768 -1.6452775 -1.4687428 -1.5785251][-1.6600754 -1.0091527 -0.8647809 -1.0860355 -1.2709215 -1.2161086 -1.0750942 -0.95887971 -0.96064925 -1.0494943 -1.255024 -1.4685931 -1.4072847 -1.4076126 -1.5729446][-1.0977113 -0.43177581 -0.29841709 -0.49729729 -0.62962532 -0.56951213 -0.42203569 -0.24338865 -0.11639643 -0.15940332 -0.51285982 -0.99996018 -1.2387516 -1.4767048 -1.750185][-1.3405755 -0.76391721 -0.57030034 -0.6019578 -0.60642505 -0.56312752 -0.53877759 -0.5099473 -0.44652772 -0.45426583 -0.72748566 -1.1003606 -1.2693851 -1.4982653 -1.7967627][-2.0350819 -1.5156174 -1.1758494 -0.97969484 -0.8527441 -0.83614326 -0.96916986 -1.1446786 -1.2080979 -1.2425056 -1.3926165 -1.536912 -1.5195827 -1.651479 -1.9194047][-2.5147233 -2.0515645 -1.6336024 -1.3464227 -1.1911521 -1.1603367 -1.2676754 -1.4315906 -1.5210598 -1.6237664 -1.7692907 -1.8609908 -1.8258348 -1.9441004 -2.201153][-2.8439684 -2.4906135 -2.1466804 -1.9765706 -1.9302595 -1.9167674 -1.9532766 -2.0134103 -2.0606229 -2.1817815 -2.3414261 -2.4277189 -2.3708963 -2.3874612 -2.5125616][-3.0567887 -2.8555174 -2.6639807 -2.6470184 -2.7076409 -2.7371688 -2.7634342 -2.7669935 -2.7756245 -2.8565838 -2.9784184 -3.0422139 -2.9649844 -2.8896708 -2.8715754][-3.1628647 -3.0962148 -3.0145221 -3.0478063 -3.1109011 -3.124131 -3.1246929 -3.0872684 -3.0636024 -3.1046386 -3.1890564 -3.2522748 -3.2206397 -3.1561179 -3.1027851][-3.2774172 -3.2806358 -3.2496424 -3.2674923 -3.2882061 -3.27594 -3.2554684 -3.2065091 -3.1707492 -3.1728225 -3.2076645 -3.2544131 -3.2674134 -3.2504678 -3.2247944]]...]
INFO - root - 2017-12-07 05:26:57.328184: step 3310, loss = 0.77, batch loss = 0.69 (8.8 examples/sec; 3.646 sec/batch; 80h:49m:45s remains)
INFO - root - 2017-12-07 05:27:34.186793: step 3320, loss = 0.72, batch loss = 0.65 (8.8 examples/sec; 3.643 sec/batch; 80h:46m:00s remains)
INFO - root - 2017-12-07 05:28:11.033168: step 3330, loss = 0.78, batch loss = 0.71 (8.7 examples/sec; 3.695 sec/batch; 81h:54m:09s remains)
INFO - root - 2017-12-07 05:28:47.487624: step 3340, loss = 0.85, batch loss = 0.77 (8.8 examples/sec; 3.637 sec/batch; 80h:36m:00s remains)
INFO - root - 2017-12-07 05:29:24.157913: step 3350, loss = 0.77, batch loss = 0.70 (8.8 examples/sec; 3.643 sec/batch; 80h:43m:22s remains)
INFO - root - 2017-12-07 05:30:00.725784: step 3360, loss = 0.82, batch loss = 0.74 (8.6 examples/sec; 3.701 sec/batch; 82h:00m:33s remains)
INFO - root - 2017-12-07 05:30:37.455175: step 3370, loss = 0.77, batch loss = 0.70 (8.7 examples/sec; 3.660 sec/batch; 81h:04m:41s remains)
INFO - root - 2017-12-07 05:31:14.143780: step 3380, loss = 0.81, batch loss = 0.74 (8.7 examples/sec; 3.688 sec/batch; 81h:41m:37s remains)
INFO - root - 2017-12-07 05:31:50.772776: step 3390, loss = 0.70, batch loss = 0.63 (8.6 examples/sec; 3.712 sec/batch; 82h:12m:44s remains)
INFO - root - 2017-12-07 05:32:27.059153: step 3400, loss = 0.74, batch loss = 0.67 (8.6 examples/sec; 3.718 sec/batch; 82h:19m:44s remains)
2017-12-07 05:32:29.358737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2515695 -2.312572 -2.485497 -2.626153 -2.4078388 -1.8359458 -1.5368218 -1.4854987 -1.3651671 -1.5117106 -1.963455 -2.4589703 -3.0381756 -3.397645 -3.2248745][-1.7366879 -1.8704894 -2.067987 -2.1587977 -1.8424137 -1.1921666 -0.83341384 -0.62053704 -0.34360123 -0.61977744 -1.3917196 -2.2120636 -3.1416702 -3.6788 -3.4450617][-0.78830361 -1.018651 -1.2860289 -1.3702471 -1.0354416 -0.40257454 0.019637585 0.48134041 0.98660469 0.59485865 -0.47904062 -1.6216209 -2.9258883 -3.7156444 -3.5090616][-0.34346533 -0.54068232 -0.71054888 -0.70610166 -0.37865782 0.11560726 0.55255556 1.3493466 2.1270342 1.6325574 0.35480213 -0.97664189 -2.5959415 -3.6469822 -3.5098298][-0.27705002 -0.390769 -0.43804049 -0.355062 -0.0067362785 0.38783598 0.8302207 1.9310703 2.8834143 2.2403064 0.79195023 -0.59344625 -2.3550904 -3.6013041 -3.560955][-0.334486 -0.40046978 -0.427886 -0.3117466 0.17983866 0.67791462 1.2127938 2.4760141 3.3686824 2.5123744 0.99872732 -0.28647566 -2.0318363 -3.421433 -3.5211749][-0.32619715 -0.41338396 -0.527421 -0.37657022 0.35639238 1.0827394 1.7515507 2.9857435 3.5615158 2.496326 1.0421791 -0.10082769 -1.733052 -3.1876106 -3.4034061][0.08772707 -0.17706394 -0.53346896 -0.43632579 0.43305826 1.4113011 2.2350373 3.3282247 3.5782585 2.4193287 1.1001096 0.12079763 -1.3738699 -2.8296442 -3.193327][0.44582748 -0.045986652 -0.62212873 -0.60884833 0.19787073 1.2520509 2.2321348 3.2468848 3.3175478 2.1337361 0.913795 0.10322046 -1.0854564 -2.3867862 -2.9319587][0.50039387 -0.16654825 -0.8213985 -0.83825159 -0.18740559 0.76434135 1.7953162 2.7478995 2.7645097 1.6212792 0.42810535 -0.22377539 -1.0060327 -2.0356874 -2.6962323][0.1329751 -0.57026482 -1.1353519 -1.1088333 -0.70104837 -0.019166946 0.95519257 1.8595614 1.8855038 0.88481665 -0.19687128 -0.65882683 -1.0430143 -1.7392492 -2.3194432][-0.55330157 -1.2164881 -1.6455412 -1.5840833 -1.439909 -1.1389396 -0.34680367 0.50406027 0.65856934 0.013638496 -0.70372128 -0.91181517 -0.9900558 -1.4001863 -1.8376133][-1.4509127 -1.9953306 -2.237735 -2.1129692 -2.1061878 -2.1070368 -1.5790417 -0.8609345 -0.59014058 -0.84976387 -1.1828129 -1.2626843 -1.2068498 -1.4083064 -1.6681421][-2.5477123 -2.9823217 -2.9956746 -2.6922441 -2.6419871 -2.8221216 -2.6316128 -2.1647973 -1.8735158 -1.8227885 -1.9052973 -2.0114014 -1.9641063 -1.9757926 -1.959816][-3.5679109 -3.9022641 -3.7422943 -3.2800045 -3.1179595 -3.3681798 -3.4580233 -3.24726 -2.949826 -2.6624212 -2.6031904 -2.8037615 -2.8486295 -2.7343438 -2.4444489]]...]
INFO - root - 2017-12-07 05:33:06.152361: step 3410, loss = 0.74, batch loss = 0.67 (8.8 examples/sec; 3.650 sec/batch; 80h:48m:58s remains)
INFO - root - 2017-12-07 05:33:42.606018: step 3420, loss = 0.81, batch loss = 0.73 (8.7 examples/sec; 3.676 sec/batch; 81h:23m:39s remains)
INFO - root - 2017-12-07 05:34:19.056471: step 3430, loss = 0.73, batch loss = 0.66 (8.7 examples/sec; 3.690 sec/batch; 81h:41m:20s remains)
INFO - root - 2017-12-07 05:34:55.574543: step 3440, loss = 0.81, batch loss = 0.73 (8.7 examples/sec; 3.686 sec/batch; 81h:35m:08s remains)
INFO - root - 2017-12-07 05:35:32.226371: step 3450, loss = 0.76, batch loss = 0.68 (8.8 examples/sec; 3.640 sec/batch; 80h:33m:52s remains)
INFO - root - 2017-12-07 05:36:08.522284: step 3460, loss = 0.68, batch loss = 0.61 (9.0 examples/sec; 3.575 sec/batch; 79h:06m:43s remains)
INFO - root - 2017-12-07 05:36:45.105989: step 3470, loss = 0.71, batch loss = 0.64 (8.7 examples/sec; 3.673 sec/batch; 81h:16m:39s remains)
INFO - root - 2017-12-07 05:37:21.538732: step 3480, loss = 0.73, batch loss = 0.66 (8.8 examples/sec; 3.631 sec/batch; 80h:20m:26s remains)
INFO - root - 2017-12-07 05:37:58.379463: step 3490, loss = 0.73, batch loss = 0.66 (8.7 examples/sec; 3.666 sec/batch; 81h:05m:36s remains)
INFO - root - 2017-12-07 05:38:34.698434: step 3500, loss = 0.69, batch loss = 0.62 (8.8 examples/sec; 3.650 sec/batch; 80h:43m:55s remains)
2017-12-07 05:38:36.896140: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18403149 -0.0058116913 -0.17083883 -0.38358879 -0.78731179 -1.4657047 -2.4070072 -3.2372499 -3.6079025 -3.8884652 -3.9341319 -3.8666 -3.924329 -4.229033 -4.5503111][0.28752327 0.041913033 -0.10390759 -0.19325018 -0.36847925 -0.81256247 -1.6336646 -2.5202196 -3.1519568 -3.7338793 -3.8733416 -3.8010862 -3.7939291 -3.9549112 -4.129621][0.16824675 -0.10106468 -0.1796751 -0.12660789 -0.075716019 -0.30751085 -0.97943544 -1.8783739 -2.6938224 -3.4083593 -3.556597 -3.434248 -3.3516002 -3.4018478 -3.4698911][-0.072453022 -0.33635473 -0.35167122 -0.1689806 0.021103382 -0.097297668 -0.60535264 -1.408077 -2.2060857 -2.8340352 -2.8825283 -2.6254945 -2.4315119 -2.3785245 -2.3772945][-0.27775431 -0.49776506 -0.45705676 -0.18843317 0.031555653 -0.07268858 -0.4327662 -1.0591695 -1.6848011 -2.1132674 -2.0487864 -1.6788616 -1.3806782 -1.2186182 -1.1274719][-0.48111796 -0.62981486 -0.52824831 -0.19736195 0.034338474 -0.044371128 -0.23516989 -0.6390574 -0.99730206 -1.2066264 -1.114084 -0.76778293 -0.46758604 -0.23281813 -0.067795277][-0.6472106 -0.72203445 -0.566216 -0.18865156 0.094539642 0.081735134 0.035429 -0.20185184 -0.32340956 -0.34155226 -0.26243591 -0.033525467 0.14531136 0.36103678 0.5538888][-0.70354986 -0.75203919 -0.58527613 -0.18992901 0.15393543 0.20363855 0.23364162 0.1013279 0.16846228 0.29359579 0.35735512 0.46524239 0.46933603 0.583096 0.76899815][-0.575423 -0.67145777 -0.57272029 -0.22507524 0.15979052 0.25414228 0.32155514 0.28777647 0.48264265 0.7229805 0.81790733 0.855113 0.71412086 0.7169795 0.84074211][-0.38858032 -0.52210736 -0.48872852 -0.18436575 0.23156214 0.35355806 0.40637684 0.43712187 0.68995523 0.97518396 1.0856047 1.0944581 0.87180805 0.77953291 0.82055044][-0.34526539 -0.4712615 -0.43916845 -0.12268829 0.32681751 0.45924854 0.4963665 0.58275175 0.84961605 1.1091571 1.169251 1.1207108 0.84467077 0.71519804 0.74120378][-0.42214012 -0.53151345 -0.47137761 -0.14643669 0.28397655 0.42586327 0.50951052 0.70298433 0.98696041 1.1704378 1.139349 1.0199175 0.72800922 0.62279987 0.67603111][-0.68610954 -0.74825072 -0.63936925 -0.33452225 0.022043705 0.18385887 0.37820673 0.6749835 0.92890739 1.0293007 0.93492126 0.78831148 0.55076981 0.53139162 0.61641026][-1.1686273 -1.1480012 -0.97536731 -0.67472935 -0.37129402 -0.18460608 0.10061312 0.42536402 0.61585617 0.68141317 0.60683966 0.48611927 0.33979702 0.42127514 0.51951313][-1.6659288 -1.5411043 -1.3380911 -1.0491383 -0.76503611 -0.55170774 -0.22495031 0.078477383 0.21538639 0.30085087 0.29869604 0.2003603 0.11225891 0.22921371 0.32559776]]...]
INFO - root - 2017-12-07 05:39:13.811959: step 3510, loss = 0.77, batch loss = 0.70 (8.7 examples/sec; 3.697 sec/batch; 81h:45m:27s remains)
INFO - root - 2017-12-07 05:39:50.533675: step 3520, loss = 0.82, batch loss = 0.75 (8.7 examples/sec; 3.667 sec/batch; 81h:04m:45s remains)
INFO - root - 2017-12-07 05:40:27.390243: step 3530, loss = 0.82, batch loss = 0.75 (8.7 examples/sec; 3.697 sec/batch; 81h:44m:11s remains)
INFO - root - 2017-12-07 05:41:04.139030: step 3540, loss = 0.72, batch loss = 0.65 (8.7 examples/sec; 3.693 sec/batch; 81h:37m:49s remains)
INFO - root - 2017-12-07 05:41:40.987707: step 3550, loss = 0.85, batch loss = 0.77 (8.8 examples/sec; 3.646 sec/batch; 80h:35m:19s remains)
INFO - root - 2017-12-07 05:42:17.401846: step 3560, loss = 0.91, batch loss = 0.84 (8.7 examples/sec; 3.659 sec/batch; 80h:52m:37s remains)
INFO - root - 2017-12-07 05:42:54.061919: step 3570, loss = 0.78, batch loss = 0.71 (8.7 examples/sec; 3.659 sec/batch; 80h:51m:31s remains)
INFO - root - 2017-12-07 05:43:30.767796: step 3580, loss = 0.79, batch loss = 0.71 (8.6 examples/sec; 3.701 sec/batch; 81h:46m:03s remains)
INFO - root - 2017-12-07 05:44:07.284441: step 3590, loss = 0.70, batch loss = 0.63 (9.0 examples/sec; 3.549 sec/batch; 78h:23m:57s remains)
INFO - root - 2017-12-07 05:44:43.964545: step 3600, loss = 0.76, batch loss = 0.69 (8.7 examples/sec; 3.665 sec/batch; 80h:57m:40s remains)
2017-12-07 05:44:46.197403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.73770142 -0.70046115 -0.73664856 -0.823534 -0.987581 -1.1515436 -1.2510011 -1.2534969 -1.1652904 -1.0546951 -0.94971752 -0.917583 -0.972116 -1.0282247 -1.0765057][-0.75955486 -0.67756867 -0.64012241 -0.62593913 -0.73979568 -0.89455938 -1.0018709 -0.97775269 -0.843683 -0.6868329 -0.51049876 -0.44602847 -0.5419066 -0.65971804 -0.75862432][-0.77606606 -0.61342788 -0.45153451 -0.26535177 -0.25382948 -0.35711479 -0.43702149 -0.35378027 -0.14957762 0.089962959 0.37302494 0.47755861 0.29477358 0.043988228 -0.1576519][-0.73866057 -0.51975846 -0.29946661 -0.012312889 0.10424376 0.069078922 0.07088089 0.25218058 0.524425 0.84520388 1.2107687 1.3246179 1.0288987 0.66032362 0.40708494][-0.70292497 -0.44782567 -0.23909473 0.091413975 0.306705 0.37454176 0.48082113 0.65744734 0.85386229 1.1351986 1.5197272 1.6722193 1.4055243 1.0925164 0.91044092][-0.61891866 -0.26165533 0.016221046 0.4688611 0.8431921 1.0256991 1.1423006 1.1256227 1.0398073 1.1046524 1.3896151 1.5960183 1.5361824 1.4607272 1.4286609][-0.55788064 -0.039941788 0.41025972 1.0304441 1.5416627 1.7696137 1.7818022 1.5201521 1.165781 1.0077891 1.1374989 1.3813539 1.6230903 1.8752804 2.0000505][-0.74622226 -0.22145224 0.25074959 0.82642651 1.2602696 1.4337001 1.4341063 1.2665715 1.0021505 0.8089366 0.81892061 1.0736704 1.5827155 2.1270046 2.3992782][-1.1263206 -0.80168796 -0.50388837 -0.11412621 0.25080109 0.49178886 0.63118649 0.72706318 0.721972 0.64627314 0.6474309 0.90814734 1.5321522 2.2074804 2.6338434][-1.470403 -1.3129857 -1.1131041 -0.77868748 -0.3499403 -0.037491322 0.10694122 0.270514 0.44939137 0.54559088 0.63026857 0.87524366 1.4429598 2.1193295 2.7395582][-1.6464028 -1.5532231 -1.3880339 -1.1109793 -0.72798562 -0.48632646 -0.44617248 -0.32638073 -0.056650639 0.14621162 0.2993474 0.513999 0.95995808 1.5380011 2.2057657][-1.7141457 -1.733305 -1.6928349 -1.5653465 -1.3172808 -1.166944 -1.150532 -1.0446882 -0.79467773 -0.650506 -0.55944109 -0.4080863 -0.11872816 0.25449419 0.74307442][-1.7098997 -1.8117902 -1.8490534 -1.8201296 -1.6797287 -1.5766678 -1.5057878 -1.4179115 -1.3055432 -1.3550911 -1.4379878 -1.3735676 -1.2122126 -1.0338073 -0.77158022][-1.5761359 -1.6693847 -1.7071791 -1.7177501 -1.6722355 -1.6376734 -1.5754294 -1.5466413 -1.5579545 -1.716224 -1.8846409 -1.8590989 -1.7642927 -1.6925993 -1.5855892][-1.3943543 -1.4625056 -1.5178411 -1.5727408 -1.6102107 -1.6337898 -1.6062343 -1.6129699 -1.6575389 -1.7896521 -1.913306 -1.8892996 -1.866497 -1.8897412 -1.8943298]]...]
INFO - root - 2017-12-07 05:45:22.616481: step 3610, loss = 0.72, batch loss = 0.65 (8.7 examples/sec; 3.657 sec/batch; 80h:46m:53s remains)
INFO - root - 2017-12-07 05:45:59.251619: step 3620, loss = 0.80, batch loss = 0.73 (8.7 examples/sec; 3.685 sec/batch; 81h:23m:28s remains)
INFO - root - 2017-12-07 05:46:35.691750: step 3630, loss = 0.69, batch loss = 0.62 (8.7 examples/sec; 3.698 sec/batch; 81h:39m:13s remains)
INFO - root - 2017-12-07 05:47:12.207217: step 3640, loss = 0.72, batch loss = 0.65 (8.7 examples/sec; 3.678 sec/batch; 81h:12m:43s remains)
INFO - root - 2017-12-07 05:47:48.659071: step 3650, loss = 0.76, batch loss = 0.68 (8.8 examples/sec; 3.647 sec/batch; 80h:31m:10s remains)
INFO - root - 2017-12-07 05:48:25.079264: step 3660, loss = 0.74, batch loss = 0.67 (8.8 examples/sec; 3.639 sec/batch; 80h:19m:56s remains)
INFO - root - 2017-12-07 05:49:01.528248: step 3670, loss = 0.72, batch loss = 0.64 (8.8 examples/sec; 3.655 sec/batch; 80h:39m:53s remains)
INFO - root - 2017-12-07 05:49:38.133453: step 3680, loss = 0.76, batch loss = 0.68 (8.8 examples/sec; 3.648 sec/batch; 80h:29m:58s remains)
INFO - root - 2017-12-07 05:50:14.476107: step 3690, loss = 0.87, batch loss = 0.80 (8.8 examples/sec; 3.648 sec/batch; 80h:29m:43s remains)
INFO - root - 2017-12-07 05:50:51.242395: step 3700, loss = 0.75, batch loss = 0.68 (8.9 examples/sec; 3.611 sec/batch; 79h:40m:30s remains)
2017-12-07 05:50:53.417510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7168615 -2.7694566 -2.8444061 -3.0555518 -3.2996917 -3.4135904 -3.4712596 -3.514581 -3.4325602 -3.34404 -3.2035723 -2.9492011 -2.7143598 -2.5183144 -2.4285188][-2.7874243 -2.8661957 -2.995399 -3.3610075 -3.6741276 -3.7563591 -3.80733 -3.8436067 -3.6730337 -3.4966009 -3.2939746 -2.9496651 -2.63452 -2.4081039 -2.333823][-2.8380413 -2.9249442 -3.13625 -3.6794953 -4.0228481 -4.0048928 -4.0069594 -3.95534 -3.651104 -3.4073763 -3.1788125 -2.7727027 -2.4469583 -2.2310648 -2.1884444][-2.8764663 -2.9992461 -3.359457 -4.0805073 -4.383626 -4.1648173 -4.0036578 -3.7328517 -3.3021302 -3.089746 -2.889919 -2.476275 -2.2178586 -2.0456014 -2.0133216][-3.0244899 -3.2304261 -3.7579803 -4.555963 -4.7087765 -4.2046771 -3.7768776 -3.1750796 -2.6649485 -2.594758 -2.4677839 -2.0952206 -1.9554203 -1.8399961 -1.8078628][-3.3193345 -3.5453961 -4.1167612 -4.835268 -4.7882352 -4.0427995 -3.3135853 -2.3234982 -1.7755699 -1.9508543 -1.9718089 -1.7007463 -1.7040882 -1.656116 -1.6395569][-3.5740328 -3.6834235 -4.139854 -4.6313739 -4.3222151 -3.3361015 -2.2526758 -0.86606908 -0.33128023 -0.83660388 -1.1325235 -1.1249573 -1.3446496 -1.4125533 -1.4743586][-3.66316 -3.5965283 -3.911299 -4.1517119 -3.6131032 -2.4886632 -1.1573985 0.44447374 0.84262657 -0.011101723 -0.63553762 -0.9183948 -1.2553141 -1.3346117 -1.4293005][-3.5628884 -3.4069247 -3.6621029 -3.7086334 -3.1028233 -2.0899651 -0.789186 0.65496778 0.78316021 -0.24188995 -0.9820714 -1.3404164 -1.5701008 -1.4930072 -1.5327418][-3.5580487 -3.4089193 -3.6204844 -3.4689002 -2.8861008 -2.091867 -0.99641943 0.049731255 -0.033799648 -0.96153307 -1.602988 -1.8732326 -1.918829 -1.6752062 -1.6527338][-3.5732203 -3.4584548 -3.5999391 -3.3006525 -2.8217382 -2.2654881 -1.4442012 -0.78295469 -0.90687609 -1.576925 -2.0269465 -2.1603196 -2.087265 -1.7737365 -1.722115][-3.3951221 -3.2810273 -3.3232813 -3.0177574 -2.7867365 -2.5298262 -2.0686173 -1.728672 -1.7646222 -2.1296179 -2.3619444 -2.3247018 -2.1716967 -1.8472228 -1.7842305][-3.3110013 -3.1579676 -3.095407 -2.8567259 -2.8616576 -2.8514073 -2.689177 -2.5146222 -2.4140897 -2.5634923 -2.6436653 -2.4947095 -2.3115168 -1.9910104 -1.9032674][-3.4158463 -3.2617598 -3.1557422 -2.9951732 -3.1184897 -3.192373 -3.1440759 -2.9701037 -2.7112918 -2.7202704 -2.7083817 -2.5292611 -2.3716693 -2.0797298 -1.9843428][-3.6334364 -3.5332723 -3.4451103 -3.3571093 -3.5225713 -3.5849607 -3.5231962 -3.2845588 -2.9336996 -2.8515863 -2.7775664 -2.5712304 -2.4022272 -2.0982504 -1.9877937]]...]
INFO - root - 2017-12-07 05:51:29.917517: step 3710, loss = 0.81, batch loss = 0.74 (8.6 examples/sec; 3.729 sec/batch; 82h:16m:07s remains)
INFO - root - 2017-12-07 05:52:06.370213: step 3720, loss = 0.77, batch loss = 0.70 (8.8 examples/sec; 3.643 sec/batch; 80h:20m:54s remains)
INFO - root - 2017-12-07 05:52:42.869225: step 3730, loss = 0.78, batch loss = 0.71 (8.7 examples/sec; 3.691 sec/batch; 81h:23m:36s remains)
INFO - root - 2017-12-07 05:53:19.781169: step 3740, loss = 0.83, batch loss = 0.76 (8.6 examples/sec; 3.739 sec/batch; 82h:26m:44s remains)
INFO - root - 2017-12-07 05:53:57.518315: step 3750, loss = 0.72, batch loss = 0.65 (8.8 examples/sec; 3.620 sec/batch; 79h:48m:38s remains)
INFO - root - 2017-12-07 05:54:34.089164: step 3760, loss = 0.78, batch loss = 0.71 (8.6 examples/sec; 3.711 sec/batch; 81h:48m:32s remains)
INFO - root - 2017-12-07 05:55:10.666334: step 3770, loss = 0.68, batch loss = 0.61 (8.8 examples/sec; 3.643 sec/batch; 80h:18m:09s remains)
INFO - root - 2017-12-07 05:55:47.134301: step 3780, loss = 0.75, batch loss = 0.67 (8.6 examples/sec; 3.720 sec/batch; 82h:00m:00s remains)
INFO - root - 2017-12-07 05:56:23.461796: step 3790, loss = 0.77, batch loss = 0.70 (8.8 examples/sec; 3.634 sec/batch; 80h:05m:32s remains)
INFO - root - 2017-12-07 05:57:00.018220: step 3800, loss = 0.74, batch loss = 0.67 (8.8 examples/sec; 3.645 sec/batch; 80h:18m:44s remains)
2017-12-07 05:57:02.206067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.99931192 -1.6231017 -1.9232914 -1.8141491 -1.4466605 -0.85650873 -0.57822442 -0.88993573 -1.042352 -0.078498363 0.79818916 0.54766607 -0.079435825 -0.96310186 -1.8309095][-0.65332341 -1.1677086 -1.3536417 -1.2251844 -0.91463065 -0.3002162 -0.027485371 -0.42090178 -0.44221139 0.60126114 1.2238975 0.98842 0.59747553 -0.44471073 -1.6811199][-1.2073982 -1.5649767 -1.6864209 -1.5718889 -1.2567296 -0.61851025 -0.3201437 -0.64422846 -0.43717003 0.54666996 0.89444065 0.906188 0.86597157 -0.23520565 -1.5316417][-1.9298444 -2.0623665 -2.1462395 -2.0563831 -1.7912402 -1.2634509 -1.0759921 -1.343755 -1.0084488 -0.19994354 -0.0006480217 0.27499628 0.44904232 -0.47580004 -1.495599][-2.3599644 -2.3452399 -2.3920629 -2.3312013 -2.130847 -1.7102933 -1.5970414 -1.8479314 -1.6549509 -1.1857522 -1.0522044 -0.70786405 -0.530535 -1.1516125 -1.8143277][-2.0866952 -2.0798357 -2.2176747 -2.3091428 -2.2285879 -1.8768342 -1.7370934 -1.9975264 -2.1129134 -1.9754715 -1.9038942 -1.7069483 -1.6495891 -1.9659405 -2.3702426][-1.5682237 -1.6170709 -1.8522527 -2.0594246 -2.0930035 -1.8172309 -1.5786624 -1.8196161 -2.1920228 -2.2450449 -2.1751637 -2.1381993 -2.2344801 -2.4603548 -2.761534][-1.3825886 -1.5353372 -1.8022625 -1.960655 -1.9738939 -1.6762972 -1.2774117 -1.5633001 -2.1820092 -2.2855277 -2.0858471 -2.0062218 -2.2150877 -2.5410471 -2.8305256][-1.3915763 -1.7254567 -2.0008535 -2.0582068 -2.0218337 -1.663238 -1.1845617 -1.6239905 -2.4663129 -2.5970306 -2.2415471 -1.9801152 -2.1820986 -2.4906497 -2.6751962][-1.4129786 -1.8771524 -2.1202567 -2.0724847 -2.0099564 -1.6598864 -1.315114 -1.9889739 -2.8900747 -3.0711908 -2.7189651 -2.4024673 -2.5309067 -2.6158516 -2.5094652][-1.6813142 -2.0844162 -2.2003121 -2.0441041 -1.8833919 -1.510926 -1.4286115 -2.268841 -2.9705429 -3.0841532 -2.8779721 -2.766767 -2.9363279 -2.8756778 -2.5608749][-2.3465574 -2.4634809 -2.3373504 -2.0681119 -1.8196278 -1.4704452 -1.6302152 -2.4474645 -2.8440084 -2.8323195 -2.7460845 -2.7976389 -2.9974542 -2.9065621 -2.5992966][-3.1158118 -2.8836818 -2.4690723 -2.0660872 -1.7542546 -1.524158 -1.8876097 -2.5834742 -2.7781954 -2.7807388 -2.8173995 -2.9122472 -3.0018129 -2.870224 -2.645746][-3.7127695 -3.319705 -2.7578743 -2.2369051 -1.8040555 -1.5879798 -1.9356935 -2.4269247 -2.5838361 -2.7526269 -2.9735904 -3.1462073 -3.1496875 -2.9787865 -2.8096817][-4.0640054 -3.6964545 -3.1906815 -2.6808558 -2.1624906 -1.8183005 -1.8808537 -2.0316463 -2.1200838 -2.3897507 -2.7204847 -2.9890575 -3.0152237 -2.8904443 -2.7997355]]...]
INFO - root - 2017-12-07 05:57:38.484996: step 3810, loss = 0.74, batch loss = 0.67 (9.2 examples/sec; 3.484 sec/batch; 76h:46m:06s remains)
INFO - root - 2017-12-07 05:58:15.212493: step 3820, loss = 0.75, batch loss = 0.67 (8.8 examples/sec; 3.646 sec/batch; 80h:19m:13s remains)
INFO - root - 2017-12-07 05:58:51.501649: step 3830, loss = 0.73, batch loss = 0.66 (8.8 examples/sec; 3.642 sec/batch; 80h:13m:34s remains)
INFO - root - 2017-12-07 05:59:28.051205: step 3840, loss = 0.81, batch loss = 0.74 (8.7 examples/sec; 3.668 sec/batch; 80h:46m:53s remains)
INFO - root - 2017-12-07 06:00:04.720670: step 3850, loss = 0.69, batch loss = 0.62 (8.8 examples/sec; 3.638 sec/batch; 80h:06m:15s remains)
INFO - root - 2017-12-07 06:00:41.413767: step 3860, loss = 0.81, batch loss = 0.74 (8.9 examples/sec; 3.613 sec/batch; 79h:32m:34s remains)
INFO - root - 2017-12-07 06:01:18.275893: step 3870, loss = 0.70, batch loss = 0.63 (8.6 examples/sec; 3.739 sec/batch; 82h:19m:05s remains)
INFO - root - 2017-12-07 06:01:54.865385: step 3880, loss = 0.69, batch loss = 0.62 (8.9 examples/sec; 3.588 sec/batch; 78h:59m:00s remains)
INFO - root - 2017-12-07 06:02:31.722586: step 3890, loss = 0.78, batch loss = 0.70 (8.5 examples/sec; 3.773 sec/batch; 83h:03m:01s remains)
INFO - root - 2017-12-07 06:03:08.362453: step 3900, loss = 0.71, batch loss = 0.64 (8.7 examples/sec; 3.676 sec/batch; 80h:54m:23s remains)
2017-12-07 06:03:10.548204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0231411 -1.9397836 -2.0184822 -1.9980619 -1.9079018 -1.8706789 -1.8127663 -1.7478893 -1.7341232 -1.8176312 -1.9664402 -2.1253488 -2.1843376 -2.2065506 -2.2512391][-1.9931085 -1.7525654 -1.7397416 -1.6422117 -1.4916625 -1.4300568 -1.3062263 -1.1270185 -1.0384011 -1.1259665 -1.3080802 -1.5074277 -1.5739679 -1.6140165 -1.757165][-2.1034749 -2.0241365 -2.0849826 -1.9440475 -1.7567728 -1.7289441 -1.6716073 -1.5378361 -1.5085683 -1.6344843 -1.7258739 -1.7209256 -1.4763491 -1.2525313 -1.3134346][-2.3436341 -2.4578605 -2.6287527 -2.5085702 -2.3404872 -2.3004198 -2.1857016 -2.0102611 -2.0899808 -2.4305339 -2.6510379 -2.631083 -2.1801965 -1.6479752 -1.5137551][-2.5927429 -2.8121116 -2.9723148 -2.8182077 -2.6812909 -2.6517901 -2.4412355 -2.0469024 -1.910486 -2.2113416 -2.573051 -2.8789182 -2.7404757 -2.3059413 -2.1384649][-2.69881 -2.904892 -2.8983297 -2.5730014 -2.3894529 -2.401237 -2.3204551 -1.9623005 -1.6454289 -1.7355907 -2.0186958 -2.5158625 -2.7638483 -2.6312463 -2.6118875][-2.8520777 -3.0916648 -2.9465747 -2.3371482 -1.8334711 -1.606734 -1.5109255 -1.2996876 -1.0373704 -1.1521616 -1.5257144 -2.1677604 -2.6621668 -2.7501349 -2.8941019][-2.9179115 -3.3188627 -3.2290297 -2.5139475 -1.7619486 -1.3863683 -1.3004529 -1.149025 -0.78764939 -0.77396441 -1.1865518 -2.0179923 -2.8127351 -3.133275 -3.3582304][-2.7158909 -3.1896279 -3.2515793 -2.7003775 -1.9530187 -1.567652 -1.6054564 -1.7118981 -1.567498 -1.4914494 -1.7389119 -2.4572263 -3.2607017 -3.6323543 -3.8326907][-2.5956213 -2.9539933 -3.0348997 -2.7560472 -2.2720354 -1.9534714 -1.9648232 -2.2006493 -2.397305 -2.6079431 -2.9485683 -3.5315681 -4.0185332 -4.0731497 -4.0570827][-2.5428646 -2.5328929 -2.4052253 -2.3464282 -2.3766744 -2.480047 -2.6043711 -2.7573686 -2.8790274 -3.1365724 -3.6226676 -4.262517 -4.5542073 -4.3161707 -4.077981][-3.0336533 -2.7482028 -2.3559866 -2.2157791 -2.4093153 -2.8282592 -3.3196464 -3.7062984 -3.8173926 -3.8302631 -4.0130434 -4.3948622 -4.5370989 -4.3006711 -4.1465964][-3.7791648 -3.640465 -3.310292 -3.100045 -3.1028597 -3.3562498 -3.8340757 -4.3601789 -4.6576481 -4.6791682 -4.6044111 -4.5958738 -4.499619 -4.2955403 -4.3202753][-4.2047539 -4.2168283 -4.0127025 -3.8712134 -3.8532903 -3.96304 -4.210124 -4.518024 -4.7456536 -4.8238258 -4.7848825 -4.7054124 -4.5330677 -4.3454261 -4.379446][-4.3165612 -4.359313 -4.1694441 -4.0044508 -3.9730141 -4.0741539 -4.2714152 -4.4357333 -4.47679 -4.4473004 -4.4079227 -4.3742652 -4.2820077 -4.171001 -4.1643443]]...]
INFO - root - 2017-12-07 06:03:47.146128: step 3910, loss = 0.77, batch loss = 0.70 (8.7 examples/sec; 3.677 sec/batch; 80h:55m:11s remains)
INFO - root - 2017-12-07 06:04:23.782710: step 3920, loss = 0.69, batch loss = 0.62 (8.7 examples/sec; 3.676 sec/batch; 80h:52m:35s remains)
INFO - root - 2017-12-07 06:05:00.442974: step 3930, loss = 0.72, batch loss = 0.65 (8.6 examples/sec; 3.731 sec/batch; 82h:03m:59s remains)
INFO - root - 2017-12-07 06:05:36.897649: step 3940, loss = 0.77, batch loss = 0.70 (8.7 examples/sec; 3.678 sec/batch; 80h:53m:52s remains)
INFO - root - 2017-12-07 06:06:13.674469: step 3950, loss = 0.85, batch loss = 0.78 (8.8 examples/sec; 3.634 sec/batch; 79h:54m:54s remains)
INFO - root - 2017-12-07 06:06:50.167971: step 3960, loss = 0.77, batch loss = 0.70 (8.8 examples/sec; 3.617 sec/batch; 79h:31m:59s remains)
INFO - root - 2017-12-07 06:07:26.670379: step 3970, loss = 0.78, batch loss = 0.71 (8.9 examples/sec; 3.607 sec/batch; 79h:18m:52s remains)
INFO - root - 2017-12-07 06:08:03.241244: step 3980, loss = 0.75, batch loss = 0.67 (8.7 examples/sec; 3.675 sec/batch; 80h:48m:12s remains)
INFO - root - 2017-12-07 06:08:39.927239: step 3990, loss = 0.63, batch loss = 0.56 (8.7 examples/sec; 3.666 sec/batch; 80h:35m:31s remains)
INFO - root - 2017-12-07 06:09:16.453248: step 4000, loss = 0.85, batch loss = 0.78 (8.7 examples/sec; 3.683 sec/batch; 80h:56m:51s remains)
2017-12-07 06:09:18.658097: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0549648 -1.1840124 -1.3520198 -1.4440422 -1.4505534 -1.344883 -1.2003 -1.1871641 -1.1569948 -1.0407081 -0.96286106 -1.007694 -1.273761 -1.4849648 -1.3870299][-1.1244276 -1.2330754 -1.3827009 -1.4499342 -1.4603758 -1.3902974 -1.2849064 -1.2671151 -1.1149371 -0.86441827 -0.63588977 -0.65008664 -0.99591517 -1.2774646 -1.2652996][-1.141041 -1.3544595 -1.5473244 -1.613759 -1.6475682 -1.6252058 -1.5772405 -1.4881828 -1.1667521 -0.78320575 -0.41409302 -0.37164211 -0.70500469 -0.99353981 -1.0747111][-1.0734146 -1.4051008 -1.6626794 -1.7748623 -1.8333821 -1.8431084 -1.8973539 -1.7956741 -1.4468601 -1.0798278 -0.70578051 -0.62880254 -0.83834147 -0.96522593 -0.88978195][-1.476614 -1.8841674 -2.041115 -2.0262325 -1.9595425 -1.8664439 -1.9921994 -1.98999 -1.8342249 -1.684731 -1.5306041 -1.5740087 -1.7160733 -1.6295271 -1.1992168][-2.2175994 -2.7388649 -2.7234502 -2.372333 -1.9211662 -1.4713612 -1.4212224 -1.4690638 -1.5901206 -1.7337303 -1.926847 -2.2504272 -2.6072104 -2.6269817 -2.0885463][-2.0239928 -2.6240585 -2.5152149 -1.8704519 -1.0518274 -0.29154158 -0.038280487 -0.11360598 -0.40385151 -0.66922593 -1.0554373 -1.7091475 -2.5418439 -3.0270641 -2.774436][-1.1329241 -1.5236726 -1.2743514 -0.52118564 0.34917736 1.0848212 1.3141727 1.1853027 0.93739271 0.81960726 0.51692343 -0.32522392 -1.5652986 -2.5664673 -2.7838264][-1.2624431 -1.3454335 -0.96625257 -0.2465601 0.44427633 0.93748236 1.0075102 0.83365107 0.76305151 0.94863367 0.94923544 0.24411726 -0.95164442 -2.0110204 -2.4168153][-2.2881095 -2.2317915 -1.8893812 -1.3521225 -0.92822742 -0.69432831 -0.73820829 -0.89667821 -0.81560469 -0.39812183 -0.11258793 -0.47276044 -1.2497425 -1.8871009 -2.0442045][-3.4343128 -3.3848634 -3.1128235 -2.7387035 -2.5311396 -2.4836845 -2.6018291 -2.7336135 -2.600585 -2.1324027 -1.7398694 -1.8261099 -2.2122915 -2.4451022 -2.3155832][-4.0567284 -4.0906773 -3.9108882 -3.689641 -3.6481404 -3.7611966 -3.9568791 -4.0914435 -3.95854 -3.5393643 -3.1906285 -3.1911001 -3.4015253 -3.4505119 -3.207962][-3.8276131 -3.9255774 -3.8574178 -3.7619667 -3.7980931 -4.0028644 -4.2992449 -4.4976721 -4.4443216 -4.1599951 -3.9425857 -3.9874182 -4.1477365 -4.1423726 -3.901546][-3.2467024 -3.2687016 -3.22381 -3.1730137 -3.1902108 -3.3306739 -3.5708392 -3.756943 -3.8007715 -3.7313633 -3.7158918 -3.8545394 -4.0243492 -4.0585995 -3.9330449][-2.9239671 -2.9071412 -2.883173 -2.87452 -2.8958035 -2.9680672 -3.0793126 -3.1603112 -3.1937623 -3.2091055 -3.2694452 -3.3946223 -3.508121 -3.5592458 -3.5478277]]...]
INFO - root - 2017-12-07 06:09:54.961611: step 4010, loss = 0.73, batch loss = 0.65 (8.8 examples/sec; 3.644 sec/batch; 80h:04m:37s remains)
INFO - root - 2017-12-07 06:10:31.356013: step 4020, loss = 0.74, batch loss = 0.66 (8.6 examples/sec; 3.720 sec/batch; 81h:44m:40s remains)
INFO - root - 2017-12-07 06:11:08.018300: step 4030, loss = 0.70, batch loss = 0.63 (8.8 examples/sec; 3.645 sec/batch; 80h:05m:06s remains)
INFO - root - 2017-12-07 06:11:44.553390: step 4040, loss = 0.68, batch loss = 0.60 (8.7 examples/sec; 3.664 sec/batch; 80h:29m:42s remains)
INFO - root - 2017-12-07 06:12:21.361159: step 4050, loss = 0.88, batch loss = 0.80 (8.7 examples/sec; 3.659 sec/batch; 80h:21m:52s remains)
INFO - root - 2017-12-07 06:12:57.851352: step 4060, loss = 0.77, batch loss = 0.70 (8.6 examples/sec; 3.703 sec/batch; 81h:19m:21s remains)
INFO - root - 2017-12-07 06:13:34.504258: step 4070, loss = 0.78, batch loss = 0.71 (8.6 examples/sec; 3.711 sec/batch; 81h:28m:57s remains)
INFO - root - 2017-12-07 06:14:11.155321: step 4080, loss = 0.91, batch loss = 0.84 (8.7 examples/sec; 3.697 sec/batch; 81h:11m:02s remains)
INFO - root - 2017-12-07 06:14:48.094307: step 4090, loss = 0.61, batch loss = 0.54 (8.7 examples/sec; 3.695 sec/batch; 81h:06m:49s remains)
INFO - root - 2017-12-07 06:15:24.441675: step 4100, loss = 0.78, batch loss = 0.71 (8.6 examples/sec; 3.707 sec/batch; 81h:22m:14s remains)
2017-12-07 06:15:26.609585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.77043 -3.686305 -3.6869533 -3.6512485 -3.5863755 -3.5881982 -3.7167983 -3.8893712 -3.9947324 -4.0449767 -4.0107927 -3.9294398 -3.8723259 -3.8950531 -3.8731225][-4.0783696 -3.9631686 -3.9384851 -3.8704994 -3.7444425 -3.6823773 -3.735815 -3.8404942 -3.9264488 -4.0143352 -4.0289488 -3.9582675 -3.8835292 -3.8361239 -3.6808364][-4.2072916 -4.1259918 -4.1217623 -4.0558939 -3.928524 -3.8379838 -3.7780895 -3.7206781 -3.6882493 -3.7022235 -3.6116862 -3.4551637 -3.3566015 -3.23209 -2.954761][-4.2355919 -4.2378449 -4.265955 -4.146244 -3.9627595 -3.7991707 -3.5752292 -3.3275557 -3.1506121 -3.029707 -2.7989564 -2.6028516 -2.5659432 -2.4400864 -2.0951307][-4.014092 -4.0256476 -4.0261135 -3.8350158 -3.6281056 -3.4280052 -3.1065273 -2.7873256 -2.5936103 -2.4363127 -2.1806843 -2.031086 -2.050386 -1.8412697 -1.3431785][-3.2886109 -3.3261995 -3.3912115 -3.2619095 -3.103379 -2.8834839 -2.4987259 -2.149672 -1.9512146 -1.7843626 -1.5384886 -1.4517691 -1.5237448 -1.2904339 -0.72985721][-2.425616 -2.5389082 -2.7176623 -2.6767817 -2.5435317 -2.2776258 -1.8480234 -1.461693 -1.199573 -0.9325788 -0.61916852 -0.56834793 -0.72365737 -0.61723828 -0.25535965][-1.450099 -1.656234 -1.989485 -2.0888226 -2.0341556 -1.8103883 -1.4415188 -1.0469527 -0.65884209 -0.19386196 0.23721838 0.24904203 0.047410011 0.049679756 0.17837667][-0.63759279 -0.97904849 -1.5018597 -1.7738128 -1.8411782 -1.7101481 -1.4291945 -1.0322952 -0.49926281 0.14868736 0.62252712 0.586185 0.40520096 0.3829689 0.34702826][-0.3812995 -0.80162358 -1.3724601 -1.678515 -1.7721429 -1.7024674 -1.5218425 -1.2119932 -0.708822 -0.10419178 0.23143387 0.13851643 0.050778389 0.10635805 0.11262703][-0.79211593 -1.1808145 -1.6352408 -1.8376915 -1.8854504 -1.8725162 -1.8228493 -1.6772053 -1.3719578 -0.986928 -0.84023547 -0.93384743 -0.91363597 -0.7713654 -0.66205239][-1.6123328 -1.9250431 -2.2313788 -2.3325882 -2.3528788 -2.3737106 -2.3994181 -2.3650723 -2.2232118 -2.0426953 -2.0170782 -2.0565045 -1.9779079 -1.8308141 -1.6908886][-2.32449 -2.5647626 -2.7659171 -2.8097272 -2.7924619 -2.7835574 -2.789917 -2.7810664 -2.72685 -2.6656551 -2.6720195 -2.6619129 -2.5833168 -2.4952188 -2.4103215][-2.8418736 -3.0016346 -3.1425495 -3.1828692 -3.16343 -3.1241975 -3.0903771 -3.0626326 -3.0218344 -2.9873641 -2.9832702 -2.952302 -2.896492 -2.8619516 -2.8480368][-3.1214285 -3.2045712 -3.2924838 -3.3292165 -3.3166213 -3.2749734 -3.22991 -3.1998379 -3.177053 -3.1647313 -3.1630383 -3.1468754 -3.1263649 -3.1177893 -3.122067]]...]
INFO - root - 2017-12-07 06:16:03.435096: step 4110, loss = 0.81, batch loss = 0.74 (8.7 examples/sec; 3.669 sec/batch; 80h:31m:28s remains)
INFO - root - 2017-12-07 06:16:40.011468: step 4120, loss = 0.72, batch loss = 0.64 (8.6 examples/sec; 3.709 sec/batch; 81h:24m:28s remains)
INFO - root - 2017-12-07 06:17:16.776546: step 4130, loss = 0.77, batch loss = 0.70 (8.7 examples/sec; 3.678 sec/batch; 80h:41m:55s remains)
INFO - root - 2017-12-07 06:17:53.448548: step 4140, loss = 0.73, batch loss = 0.65 (8.6 examples/sec; 3.715 sec/batch; 81h:31m:06s remains)
INFO - root - 2017-12-07 06:18:30.241096: step 4150, loss = 0.82, batch loss = 0.75 (8.7 examples/sec; 3.680 sec/batch; 80h:43m:15s remains)
INFO - root - 2017-12-07 06:19:06.683244: step 4160, loss = 0.69, batch loss = 0.62 (8.6 examples/sec; 3.727 sec/batch; 81h:45m:33s remains)
INFO - root - 2017-12-07 06:19:43.118575: step 4170, loss = 0.73, batch loss = 0.66 (9.1 examples/sec; 3.511 sec/batch; 77h:00m:45s remains)
INFO - root - 2017-12-07 06:20:19.622376: step 4180, loss = 0.82, batch loss = 0.75 (8.8 examples/sec; 3.644 sec/batch; 79h:54m:02s remains)
INFO - root - 2017-12-07 06:20:56.319318: step 4190, loss = 0.74, batch loss = 0.67 (8.7 examples/sec; 3.660 sec/batch; 80h:15m:41s remains)
INFO - root - 2017-12-07 06:21:32.851843: step 4200, loss = 0.75, batch loss = 0.67 (8.8 examples/sec; 3.635 sec/batch; 79h:41m:10s remains)
2017-12-07 06:21:35.053585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8867786 -2.8234406 -2.7880402 -2.876904 -2.9634342 -3.045516 -3.1816835 -3.3139567 -3.3076706 -3.2920427 -3.3429518 -3.4132967 -3.518805 -3.6495683 -3.7218273][-2.8022134 -2.8076854 -2.8562109 -2.9948287 -3.1008542 -3.2129762 -3.4020014 -3.5603647 -3.5110505 -3.3871446 -3.3054504 -3.2833922 -3.3899231 -3.5816102 -3.6818266][-2.8275447 -2.9541931 -3.0604191 -3.1972942 -3.2456436 -3.320657 -3.5009711 -3.6223238 -3.5320532 -3.3176255 -3.0607409 -2.8663268 -2.8978605 -3.1040354 -3.2472644][-3.0297723 -3.1728725 -3.2158556 -3.3239131 -3.349937 -3.4013872 -3.5074468 -3.50376 -3.3330231 -3.0443196 -2.6261406 -2.2698164 -2.2093513 -2.4113617 -2.6595812][-3.236557 -3.2420278 -3.1336734 -3.2243795 -3.3067777 -3.369679 -3.3666124 -3.1540451 -2.7294309 -2.1857054 -1.5699418 -1.1745255 -1.1875396 -1.5027702 -1.9446197][-3.405926 -3.2019811 -2.9030428 -2.8282371 -2.7533541 -2.5813851 -2.2910717 -1.7998109 -1.1198738 -0.41878843 0.1618824 0.29132223 -0.073263645 -0.68950891 -1.3961878][-3.4486477 -2.9373963 -2.244122 -1.6467593 -1.0023546 -0.34482527 0.17812586 0.6368742 1.084281 1.3521943 1.352232 0.87115574 0.088203907 -0.71436882 -1.4340832][-2.9984889 -2.2322359 -1.1773319 -0.013827324 1.2264786 2.2676096 2.7346025 2.6758084 2.2526898 1.5290937 0.7419405 -0.13125372 -0.91738844 -1.4840586 -1.8781748][-2.2860229 -1.5338118 -0.591444 0.55816317 1.814909 2.7558961 2.8372765 2.14041 1.0233049 -0.15027666 -1.0571063 -1.7113631 -2.0923178 -2.2030134 -2.1978436][-1.523572 -1.0873821 -0.62523985 -0.051837921 0.5123744 0.7916913 0.53403425 -0.13526201 -1.0050793 -1.8141754 -2.3424914 -2.6235042 -2.6823485 -2.5326815 -2.3134251][-0.42500448 -0.17669773 -0.090765476 -0.12846518 -0.33774042 -0.74220514 -1.2227292 -1.6695848 -2.1135671 -2.5153732 -2.7446399 -2.841254 -2.7876303 -2.5886183 -2.3768802][0.12095881 0.17428589 0.041296005 -0.25872946 -0.7475841 -1.4030466 -1.9899831 -2.3389535 -2.5657222 -2.6973171 -2.7043021 -2.685626 -2.6111696 -2.4742937 -2.3867068][-0.78601241 -0.89972138 -1.0092463 -1.1588161 -1.3840723 -1.7754226 -2.2128491 -2.5201364 -2.7021649 -2.7688994 -2.7381709 -2.7165158 -2.623569 -2.4750886 -2.4104812][-2.0727546 -2.1605222 -2.1155977 -2.0249748 -1.9918797 -2.0976608 -2.3770556 -2.6888075 -2.906832 -2.9923778 -2.9555473 -2.9123974 -2.782907 -2.6102624 -2.5266371][-2.721909 -2.7237682 -2.5930352 -2.4408638 -2.3894911 -2.4356718 -2.6625681 -2.9635167 -3.1376977 -3.165128 -3.074214 -2.9838817 -2.8328552 -2.6727526 -2.5958233]]...]
INFO - root - 2017-12-07 06:22:11.722096: step 4210, loss = 0.74, batch loss = 0.67 (8.6 examples/sec; 3.703 sec/batch; 81h:10m:27s remains)
INFO - root - 2017-12-07 06:22:48.281529: step 4220, loss = 0.80, batch loss = 0.72 (8.7 examples/sec; 3.673 sec/batch; 80h:30m:42s remains)
INFO - root - 2017-12-07 06:23:24.602410: step 4230, loss = 0.69, batch loss = 0.62 (8.7 examples/sec; 3.698 sec/batch; 81h:03m:08s remains)
INFO - root - 2017-12-07 06:24:01.284098: step 4240, loss = 0.82, batch loss = 0.75 (8.7 examples/sec; 3.685 sec/batch; 80h:45m:17s remains)
INFO - root - 2017-12-07 06:24:37.937069: step 4250, loss = 0.66, batch loss = 0.59 (8.7 examples/sec; 3.661 sec/batch; 80h:12m:14s remains)
INFO - root - 2017-12-07 06:25:14.332230: step 4260, loss = 0.74, batch loss = 0.66 (8.8 examples/sec; 3.630 sec/batch; 79h:31m:25s remains)
INFO - root - 2017-12-07 06:25:50.755927: step 4270, loss = 0.75, batch loss = 0.68 (8.7 examples/sec; 3.668 sec/batch; 80h:20m:33s remains)
INFO - root - 2017-12-07 06:26:27.333839: step 4280, loss = 0.77, batch loss = 0.70 (8.7 examples/sec; 3.667 sec/batch; 80h:18m:32s remains)
INFO - root - 2017-12-07 06:27:03.803200: step 4290, loss = 0.85, batch loss = 0.78 (8.7 examples/sec; 3.679 sec/batch; 80h:33m:25s remains)
INFO - root - 2017-12-07 06:27:40.544988: step 4300, loss = 0.76, batch loss = 0.69 (8.5 examples/sec; 3.748 sec/batch; 82h:03m:56s remains)
2017-12-07 06:27:42.699723: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2858102 -2.0637007 -2.7748327 -3.0090261 -2.7243333 -2.323719 -2.6097536 -2.595583 -2.0495965 -1.0765178 -8.1062317e-06 0.31631851 0.045939445 -0.51009536 -1.1557403][-1.829011 -2.3738611 -2.6853576 -2.6832304 -2.4284198 -2.2307894 -2.5485396 -2.2625988 -1.3916368 -0.43967557 0.09621954 -0.10588884 -0.55004573 -1.024718 -1.3868258][-2.0180945 -2.5523489 -2.63882 -2.4260492 -2.1769342 -2.1609473 -2.433145 -1.8592486 -0.67886353 0.24522543 0.21494246 -0.57070088 -1.2204871 -1.5253527 -1.629854][-2.0801172 -2.5510044 -2.5390842 -2.2421422 -2.0735033 -2.2890286 -2.5796208 -1.9259455 -0.57908607 0.43057394 0.20331621 -0.83494306 -1.6566105 -1.8607965 -1.7143407][-2.35543 -2.6617582 -2.4909432 -2.102381 -2.0156326 -2.3354719 -2.4304912 -1.6008718 -0.25757647 0.53518677 0.12326527 -0.97148585 -1.900279 -2.1160378 -1.8123453][-2.6787088 -2.9003234 -2.5330048 -1.9803245 -1.8986866 -2.1796906 -1.8726063 -0.5934546 0.76861525 0.99772549 0.10240984 -1.0444229 -1.8684838 -2.0376947 -1.8197739][-2.9852047 -3.1080093 -2.5361211 -1.8001194 -1.7295225 -2.0122576 -1.4131773 0.29129076 1.7866192 1.6186562 0.23382807 -0.97655725 -1.5275357 -1.5965819 -1.6510859][-3.0649948 -3.067826 -2.4533412 -1.6837151 -1.590657 -1.9262645 -1.2937756 0.51821566 2.1236563 1.8827233 0.30005932 -0.85154605 -1.1538899 -1.174438 -1.471669][-2.936121 -2.9422846 -2.5494003 -1.8939505 -1.6804228 -1.8479762 -1.1730721 0.50470209 1.975379 1.709475 0.24806023 -0.66783118 -0.83669591 -0.99195075 -1.5332272][-2.6888156 -2.7593622 -2.6421711 -2.1453972 -1.6703987 -1.4335968 -0.74647689 0.45047855 1.4028654 1.0656085 -0.029784203 -0.53096223 -0.5887115 -1.0047915 -1.8352249][-2.5708578 -2.6233845 -2.7178304 -2.4407802 -1.8408499 -1.2072003 -0.53367996 0.01458168 0.30363274 -0.061462879 -0.65463257 -0.74258232 -0.74515843 -1.3724713 -2.3577793][-2.5956025 -2.5352237 -2.684464 -2.6238313 -2.1131849 -1.372757 -0.81825352 -0.7947588 -0.98923755 -1.2404842 -1.2843881 -0.95723438 -0.90604258 -1.6716692 -2.7292717][-2.5454841 -2.4224923 -2.5182393 -2.5587039 -2.2081478 -1.560554 -1.1328845 -1.3421521 -1.7618434 -1.8894215 -1.5550365 -0.97171259 -0.87959051 -1.6647913 -2.6515379][-2.669395 -2.5564241 -2.5737872 -2.592278 -2.3411636 -1.8445065 -1.4917729 -1.6479533 -2.0462341 -2.0924461 -1.6401508 -1.0198982 -0.91372681 -1.5717351 -2.3446283][-2.9352355 -2.8484173 -2.8069808 -2.7315364 -2.5048711 -2.1834297 -1.9241359 -1.8982539 -2.1030002 -2.1442685 -1.8263879 -1.3320789 -1.1460462 -1.4426408 -1.8747916]]...]
INFO - root - 2017-12-07 06:28:19.359166: step 4310, loss = 0.66, batch loss = 0.59 (8.8 examples/sec; 3.633 sec/batch; 79h:32m:05s remains)
INFO - root - 2017-12-07 06:28:55.972619: step 4320, loss = 0.79, batch loss = 0.71 (8.9 examples/sec; 3.593 sec/batch; 78h:38m:39s remains)
INFO - root - 2017-12-07 06:29:32.704415: step 4330, loss = 0.83, batch loss = 0.76 (8.7 examples/sec; 3.688 sec/batch; 80h:43m:04s remains)
INFO - root - 2017-12-07 06:30:09.727573: step 4340, loss = 0.70, batch loss = 0.63 (8.8 examples/sec; 3.654 sec/batch; 79h:57m:27s remains)
INFO - root - 2017-12-07 06:30:46.185442: step 4350, loss = 0.75, batch loss = 0.68 (8.7 examples/sec; 3.678 sec/batch; 80h:29m:07s remains)
INFO - root - 2017-12-07 06:31:22.810555: step 4360, loss = 0.70, batch loss = 0.63 (8.8 examples/sec; 3.638 sec/batch; 79h:35m:31s remains)
INFO - root - 2017-12-07 06:31:59.483301: step 4370, loss = 0.68, batch loss = 0.61 (8.6 examples/sec; 3.729 sec/batch; 81h:35m:08s remains)
INFO - root - 2017-12-07 06:32:36.024340: step 4380, loss = 0.79, batch loss = 0.71 (8.7 examples/sec; 3.674 sec/batch; 80h:21m:41s remains)
INFO - root - 2017-12-07 06:33:12.436875: step 4390, loss = 0.65, batch loss = 0.58 (8.6 examples/sec; 3.718 sec/batch; 81h:18m:25s remains)
INFO - root - 2017-12-07 06:33:49.235205: step 4400, loss = 0.72, batch loss = 0.65 (8.7 examples/sec; 3.667 sec/batch; 80h:11m:38s remains)
2017-12-07 06:33:51.468517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5052438 -3.5322151 -3.5714226 -3.553843 -3.4926062 -3.4604678 -3.4519422 -3.3976903 -3.3282895 -3.3479519 -3.5594683 -3.9228983 -4.0846405 -3.9816217 -3.8494103][-3.2558055 -3.2822423 -3.3191378 -3.2799125 -3.2019856 -3.1576443 -3.1414611 -3.0384617 -2.9043107 -2.881289 -3.1311433 -3.6403971 -3.9111221 -3.7696559 -3.5781145][-2.7045152 -2.6855137 -2.6598692 -2.5768533 -2.483736 -2.4166248 -2.4055071 -2.326489 -2.2187228 -2.1971 -2.4708424 -3.0611358 -3.3986881 -3.2247477 -3.0253997][-1.9129622 -1.7946134 -1.6985085 -1.6082516 -1.5387671 -1.4776828 -1.4815612 -1.4611881 -1.4169436 -1.4032595 -1.7021902 -2.3681185 -2.7944269 -2.6234837 -2.3975058][-1.654731 -1.3866165 -1.2747436 -1.2780905 -1.3479807 -1.4352088 -1.5626163 -1.6315079 -1.5916374 -1.4880753 -1.671185 -2.2635758 -2.6867027 -2.479259 -2.1846366][-1.9885349 -1.5571854 -1.4743979 -1.5838728 -1.7713606 -2.0136573 -2.281564 -2.4255195 -2.3700526 -2.2016144 -2.2701571 -2.7285054 -3.0894489 -2.7997479 -2.3829863][-1.7326214 -1.2322428 -1.1372106 -1.2528665 -1.4785542 -1.8069553 -2.0973957 -2.1580594 -2.0884683 -2.0138242 -2.1523361 -2.6482372 -3.0733285 -2.8236933 -2.4049945][-0.46661878 -0.021251202 0.10233974 0.070177078 -0.10364056 -0.40680504 -0.55566382 -0.48130155 -0.51992154 -0.68486547 -1.0462337 -1.7170033 -2.3417253 -2.2968166 -2.0414841][0.584167 0.85323858 0.98872089 1.0400529 0.98970985 0.87367678 0.98377037 1.180829 1.0219679 0.66011906 0.10192776 -0.75967145 -1.5909364 -1.7755108 -1.7227614][0.73112726 0.77041864 0.8090086 0.82092905 0.84803104 0.90996933 1.1289444 1.3040242 1.1792626 0.90968561 0.38794708 -0.46187258 -1.2873447 -1.5586183 -1.6233923][-0.31155014 -0.30024481 -0.28729582 -0.30664778 -0.20077085 -0.035630703 0.10450602 0.17628384 0.1372447 0.03280735 -0.3468194 -1.0268419 -1.6015546 -1.7210646 -1.7427917][-1.4218087 -1.2698138 -1.1689308 -1.0976458 -0.89988184 -0.73170877 -0.71283364 -0.70093632 -0.65466189 -0.70692492 -1.0529215 -1.6553934 -2.0559316 -2.041698 -1.9963787][-1.9405396 -1.7128649 -1.5624373 -1.4310319 -1.2300353 -1.1207693 -1.1460466 -1.1086452 -0.98506117 -0.97148013 -1.2423935 -1.7605474 -2.0819294 -2.0275066 -2.0144284][-1.9628456 -1.7615216 -1.6198978 -1.4847577 -1.3459353 -1.2966278 -1.3329895 -1.3059213 -1.1869075 -1.0859339 -1.1765003 -1.4938982 -1.6828215 -1.6197131 -1.7214167][-2.0867229 -1.9578919 -1.8574197 -1.7700827 -1.6883826 -1.6267796 -1.6131725 -1.6135738 -1.536736 -1.3704908 -1.2712727 -1.3434191 -1.3574994 -1.236773 -1.3907721]]...]
INFO - root - 2017-12-07 06:34:28.232973: step 4410, loss = 0.72, batch loss = 0.65 (8.7 examples/sec; 3.670 sec/batch; 80h:14m:11s remains)
INFO - root - 2017-12-07 06:35:04.882770: step 4420, loss = 0.76, batch loss = 0.68 (8.8 examples/sec; 3.644 sec/batch; 79h:40m:31s remains)
INFO - root - 2017-12-07 06:35:41.454423: step 4430, loss = 0.71, batch loss = 0.64 (8.8 examples/sec; 3.619 sec/batch; 79h:06m:47s remains)
INFO - root - 2017-12-07 06:36:18.061069: step 4440, loss = 0.79, batch loss = 0.71 (9.2 examples/sec; 3.491 sec/batch; 76h:18m:30s remains)
INFO - root - 2017-12-07 06:36:54.613569: step 4450, loss = 0.76, batch loss = 0.69 (8.7 examples/sec; 3.675 sec/batch; 80h:19m:26s remains)
INFO - root - 2017-12-07 06:37:31.107535: step 4460, loss = 0.71, batch loss = 0.63 (9.3 examples/sec; 3.429 sec/batch; 74h:55m:25s remains)
INFO - root - 2017-12-07 06:38:07.688311: step 4470, loss = 0.75, batch loss = 0.68 (8.7 examples/sec; 3.667 sec/batch; 80h:07m:20s remains)
INFO - root - 2017-12-07 06:38:43.940403: step 4480, loss = 0.74, batch loss = 0.67 (8.9 examples/sec; 3.580 sec/batch; 78h:12m:54s remains)
INFO - root - 2017-12-07 06:39:20.708948: step 4490, loss = 0.72, batch loss = 0.65 (8.8 examples/sec; 3.642 sec/batch; 79h:33m:27s remains)
INFO - root - 2017-12-07 06:39:57.150499: step 4500, loss = 0.78, batch loss = 0.71 (8.8 examples/sec; 3.655 sec/batch; 79h:49m:58s remains)
2017-12-07 06:39:59.378075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6750212 -1.698154 -1.4604154 -1.0167391 -0.72413325 -0.60323286 -0.60881448 -0.57842588 -0.7753973 -1.349308 -1.7639186 -2.0667219 -2.5424373 -3.0330665 -3.14937][-1.8327031 -1.8145409 -1.4566932 -0.99376774 -0.87381315 -0.88202333 -0.8271966 -0.58031416 -0.58148646 -0.99228692 -1.1778133 -1.3394954 -1.753319 -2.2385521 -2.6165285][-2.0077507 -2.0316379 -1.6465335 -1.293258 -1.3779492 -1.4098647 -1.2252777 -0.68897772 -0.38122654 -0.65471363 -0.76500297 -0.82827258 -1.0069299 -1.2047801 -1.5979235][-2.0914836 -2.1180482 -1.7270446 -1.5020716 -1.6613152 -1.5885308 -1.2965529 -0.52701139 0.0086388588 -0.25742102 -0.46771693 -0.619323 -0.67386246 -0.59279227 -0.81932735][-2.0526125 -2.0663981 -1.6714056 -1.5091867 -1.6061618 -1.3505218 -0.94501972 -0.059483528 0.55372763 0.24421501 -0.081428051 -0.3679142 -0.47405934 -0.34901 -0.49808741][-1.9163694 -1.8932116 -1.5193424 -1.4185765 -1.5072966 -1.1468596 -0.6260519 0.31863737 0.89785624 0.59043837 0.26308298 -0.073693752 -0.29177046 -0.2722559 -0.47056222][-1.905412 -1.77369 -1.3772895 -1.2916083 -1.4357595 -1.1524024 -0.60971165 0.3377862 0.84885263 0.6326251 0.43400669 0.14380932 -0.14881516 -0.23303747 -0.4985888][-1.9681859 -1.6742773 -1.2336628 -1.1140413 -1.2604661 -1.1087477 -0.62317586 0.26103973 0.72490215 0.68891 0.66794205 0.43574095 0.10062075 -0.014873981 -0.30460644][-1.9915283 -1.6307476 -1.2253733 -1.0838058 -1.1475735 -1.0196116 -0.56292963 0.21123362 0.60711908 0.75986624 0.88119125 0.6237216 0.21707678 0.10395288 -0.15051651][-1.952822 -1.6901994 -1.443028 -1.3740549 -1.3608952 -1.1660302 -0.69201922 -0.023591042 0.31453705 0.62100315 0.81365347 0.51243019 0.026151657 -0.15540314 -0.42624617][-1.8697658 -1.7971013 -1.7737355 -1.8435149 -1.8354211 -1.640882 -1.1947238 -0.63600945 -0.26096106 0.22012281 0.5111413 0.22904205 -0.30751562 -0.61033535 -0.92414737][-1.9225056 -1.9832594 -2.1060894 -2.2971892 -2.3446889 -2.2100213 -1.8781755 -1.4525454 -1.0135546 -0.35391903 0.11003876 -0.063481808 -0.613209 -1.0467272 -1.4022174][-2.2266495 -2.30287 -2.4450431 -2.6256962 -2.646908 -2.5423887 -2.3258514 -2.0281272 -1.5638285 -0.8291564 -0.28910971 -0.39103079 -0.89855289 -1.3431373 -1.6877127][-2.5985527 -2.6226978 -2.6579478 -2.6899524 -2.600332 -2.4582782 -2.3003712 -2.1022117 -1.7213073 -1.1136861 -0.69258523 -0.84274769 -1.2960851 -1.6666651 -1.9093649][-2.8351734 -2.8233948 -2.751677 -2.5935717 -2.3683109 -2.1392419 -1.9465916 -1.8079371 -1.5967755 -1.2492254 -1.0779049 -1.3619857 -1.7519917 -1.9673693 -2.0474632]]...]
INFO - root - 2017-12-07 06:40:35.874871: step 4510, loss = 0.77, batch loss = 0.70 (8.7 examples/sec; 3.691 sec/batch; 80h:35m:55s remains)
INFO - root - 2017-12-07 06:41:12.263482: step 4520, loss = 0.79, batch loss = 0.72 (8.7 examples/sec; 3.665 sec/batch; 80h:01m:46s remains)
INFO - root - 2017-12-07 06:41:49.265506: step 4530, loss = 0.71, batch loss = 0.64 (8.7 examples/sec; 3.672 sec/batch; 80h:09m:48s remains)
INFO - root - 2017-12-07 06:42:26.166970: step 4540, loss = 0.65, batch loss = 0.57 (8.6 examples/sec; 3.711 sec/batch; 81h:01m:07s remains)
INFO - root - 2017-12-07 06:43:02.803487: step 4550, loss = 0.75, batch loss = 0.68 (8.6 examples/sec; 3.720 sec/batch; 81h:11m:58s remains)
INFO - root - 2017-12-07 06:43:39.554009: step 4560, loss = 0.74, batch loss = 0.66 (8.5 examples/sec; 3.783 sec/batch; 82h:33m:04s remains)
INFO - root - 2017-12-07 06:44:16.187878: step 4570, loss = 0.75, batch loss = 0.68 (8.7 examples/sec; 3.674 sec/batch; 80h:10m:04s remains)
INFO - root - 2017-12-07 06:44:52.640309: step 4580, loss = 0.74, batch loss = 0.67 (8.6 examples/sec; 3.741 sec/batch; 81h:37m:14s remains)
INFO - root - 2017-12-07 06:45:29.235666: step 4590, loss = 0.73, batch loss = 0.65 (8.7 examples/sec; 3.667 sec/batch; 79h:59m:51s remains)
INFO - root - 2017-12-07 06:46:05.870538: step 4600, loss = 0.72, batch loss = 0.65 (8.6 examples/sec; 3.708 sec/batch; 80h:53m:27s remains)
2017-12-07 06:46:08.091725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1457152 -3.1501689 -3.1355872 -3.0997939 -3.007966 -2.9166551 -2.8920841 -2.9474559 -3.0823984 -3.2562723 -3.42057 -3.5749111 -3.7131665 -3.799397 -3.8772728][-3.1536655 -3.1883843 -3.1603241 -3.0702362 -2.9619033 -2.8865616 -2.8871031 -2.9659138 -3.1161098 -3.3074131 -3.5017467 -3.6484611 -3.7142177 -3.7077374 -3.7311907][-3.3361535 -3.3980918 -3.3260756 -3.1635351 -3.0318296 -2.94984 -2.9555364 -3.0490689 -3.1947823 -3.3827119 -3.5711064 -3.6673729 -3.6139662 -3.4803965 -3.4414539][-3.4610362 -3.5411687 -3.4432662 -3.2479758 -3.0903583 -2.956984 -2.9010587 -2.948627 -3.0411558 -3.1966624 -3.4001913 -3.5169997 -3.4555151 -3.3143051 -3.2955923][-3.3593953 -3.4180307 -3.3290491 -3.1859891 -3.0522044 -2.874718 -2.748301 -2.7380455 -2.7790885 -2.9094148 -3.1600993 -3.3660014 -3.3814785 -3.3114493 -3.3318462][-3.1958494 -3.2222195 -3.1826062 -3.1417255 -3.0449667 -2.8190384 -2.6168294 -2.5487413 -2.5532806 -2.6699419 -2.9718871 -3.2644954 -3.3652854 -3.3749337 -3.4371481][-3.057775 -3.0694971 -3.1042724 -3.1785498 -3.145937 -2.912565 -2.6653271 -2.5385919 -2.5015423 -2.588752 -2.8802354 -3.2000091 -3.3565764 -3.4177895 -3.4967623][-3.1017332 -3.1220279 -3.1938705 -3.3064165 -3.2959938 -3.0614529 -2.8071203 -2.6795449 -2.6627567 -2.7532728 -3.000973 -3.2834 -3.4362612 -3.4885061 -3.5322049][-3.2551904 -3.3000116 -3.3632696 -3.4301174 -3.3736405 -3.1093235 -2.8420463 -2.7214139 -2.7465284 -2.8772597 -3.0966191 -3.3234582 -3.4439373 -3.4708905 -3.4782295][-3.2652106 -3.3106863 -3.3426275 -3.3504562 -3.2488208 -2.9733524 -2.7152772 -2.6079512 -2.6655221 -2.8367953 -3.0510221 -3.2485662 -3.3566003 -3.3828633 -3.3816648][-3.2435403 -3.2742596 -3.2768574 -3.2463381 -3.1241832 -2.8787622 -2.6654096 -2.5840178 -2.65195 -2.8211038 -3.0161796 -3.1914163 -3.2987139 -3.3355837 -3.3378592][-3.2807579 -3.2928162 -3.2891023 -3.2681789 -3.1848803 -3.0256631 -2.8875182 -2.8267107 -2.8588219 -2.9560723 -3.0757227 -3.2024534 -3.3021317 -3.3577344 -3.3834338][-3.3872292 -3.390219 -3.4030495 -3.4224691 -3.4112487 -3.3538225 -3.2833881 -3.2265477 -3.2018142 -3.2042973 -3.2316687 -3.2991076 -3.3841352 -3.452307 -3.49904][-3.4799249 -3.4941368 -3.532861 -3.5841818 -3.6174278 -3.610723 -3.5620513 -3.48886 -3.4207153 -3.371871 -3.3586993 -3.3978224 -3.4673724 -3.5312955 -3.5751405][-3.4860814 -3.5101957 -3.5570142 -3.6109538 -3.6462324 -3.6433158 -3.5913248 -3.5078435 -3.4291916 -3.3784003 -3.3706543 -3.407393 -3.4632583 -3.5108893 -3.5338473]]...]
INFO - root - 2017-12-07 06:46:44.926668: step 4610, loss = 0.74, batch loss = 0.67 (8.6 examples/sec; 3.723 sec/batch; 81h:11m:24s remains)
INFO - root - 2017-12-07 06:47:21.498479: step 4620, loss = 0.81, batch loss = 0.74 (8.8 examples/sec; 3.647 sec/batch; 79h:31m:17s remains)
INFO - root - 2017-12-07 06:47:58.219579: step 4630, loss = 0.74, batch loss = 0.67 (8.8 examples/sec; 3.653 sec/batch; 79h:38m:39s remains)
INFO - root - 2017-12-07 06:48:34.607512: step 4640, loss = 0.66, batch loss = 0.58 (8.8 examples/sec; 3.616 sec/batch; 78h:49m:56s remains)
INFO - root - 2017-12-07 06:49:11.508950: step 4650, loss = 0.73, batch loss = 0.66 (8.6 examples/sec; 3.731 sec/batch; 81h:19m:52s remains)
INFO - root - 2017-12-07 06:49:47.958410: step 4660, loss = 0.74, batch loss = 0.66 (8.8 examples/sec; 3.642 sec/batch; 79h:22m:11s remains)
INFO - root - 2017-12-07 06:50:24.633242: step 4670, loss = 0.77, batch loss = 0.70 (8.9 examples/sec; 3.608 sec/batch; 78h:37m:47s remains)
INFO - root - 2017-12-07 06:51:00.898169: step 4680, loss = 0.72, batch loss = 0.65 (8.8 examples/sec; 3.631 sec/batch; 79h:06m:47s remains)
INFO - root - 2017-12-07 06:51:37.407040: step 4690, loss = 0.76, batch loss = 0.69 (8.8 examples/sec; 3.654 sec/batch; 79h:37m:07s remains)
INFO - root - 2017-12-07 06:52:13.795612: step 4700, loss = 0.81, batch loss = 0.73 (8.7 examples/sec; 3.667 sec/batch; 79h:53m:14s remains)
2017-12-07 06:52:16.010092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6753471 -2.7118373 -2.67909 -2.6107264 -2.51419 -2.3256679 -2.1946371 -2.2690074 -2.5349936 -2.8483739 -3.0222151 -2.94761 -2.7464867 -2.549809 -2.3869298][-2.7754583 -2.9586687 -3.0361731 -2.9610534 -2.665051 -2.1314251 -1.7105613 -1.7152834 -2.193053 -2.8441234 -3.2731609 -3.2615695 -2.9809694 -2.6607838 -2.4040775][-2.8986263 -3.3281908 -3.5965397 -3.5421765 -2.9796753 -1.9837923 -1.1387043 -0.96590447 -1.6006122 -2.6159458 -3.3534503 -3.4789517 -3.1968169 -2.7918115 -2.4672236][-3.072974 -3.7826538 -4.2727747 -4.257194 -3.4131365 -1.9371569 -0.635618 -0.21950245 -0.93290138 -2.1924381 -3.1606908 -3.4654968 -3.2759705 -2.8447173 -2.4769402][-3.5551486 -4.4399028 -5.0049624 -4.9255166 -3.7561057 -1.8467 -0.12317371 0.48069572 -0.34931421 -1.7900388 -2.8857527 -3.3198273 -3.2389002 -2.8334665 -2.4739704][-4.1446977 -5.1043777 -5.5690079 -5.2695742 -3.7800951 -1.5090148 0.64747429 1.4323988 0.4023571 -1.2729821 -2.5508528 -3.2081802 -3.2752655 -2.9095721 -2.5502627][-4.2572508 -5.2617912 -5.649013 -5.2216911 -3.6441138 -1.1727059 1.4972644 2.6609707 1.6150398 -0.16118002 -1.6102011 -2.5606518 -2.8519807 -2.6368475 -2.4198163][-4.0859723 -5.0766287 -5.363986 -4.8770528 -3.4340994 -1.0680931 1.8938594 3.50041 2.667532 0.98463535 -0.53771758 -1.6911657 -2.1755731 -2.1442137 -2.0825415][-3.8112502 -4.7144704 -4.8930092 -4.321362 -3.0134168 -0.91865921 1.8982029 3.6281509 3.0310216 1.4496551 -0.15566349 -1.4185696 -2.0665984 -2.2212257 -2.1873865][-3.3403387 -4.0844579 -4.1869788 -3.5947714 -2.4444299 -0.68783069 1.653933 3.080934 2.6979246 1.3216648 -0.26499367 -1.4494417 -2.1100364 -2.3517756 -2.2703884][-3.0622191 -3.6007383 -3.6242054 -3.0649176 -2.1218266 -0.77036381 0.92576265 1.8592334 1.489697 0.41286802 -0.83734775 -1.6040344 -1.9888959 -2.0495698 -1.8105783][-3.0380135 -3.5037746 -3.5436397 -3.0868249 -2.3230922 -1.2410192 -0.037807941 0.50420094 0.13008308 -0.72727323 -1.6256607 -2.0414386 -2.1931608 -2.0583909 -1.665488][-3.000092 -3.4454565 -3.5107388 -3.1164336 -2.4667547 -1.5941629 -0.75305057 -0.45872045 -0.79461789 -1.5392361 -2.2455335 -2.4763823 -2.4784739 -2.1773064 -1.6579368][-2.9637399 -3.4737439 -3.6832395 -3.3856199 -2.7761149 -1.9839816 -1.3332055 -1.1569226 -1.4426413 -2.1221473 -2.7657917 -2.9549074 -2.9081526 -2.5178716 -1.8999341][-2.8545818 -3.4105616 -3.8355098 -3.7531354 -3.2476754 -2.5142336 -1.9165828 -1.7197816 -1.8851111 -2.4043915 -2.912657 -3.0933373 -3.0791631 -2.71143 -2.1150134]]...]
INFO - root - 2017-12-07 06:52:52.575575: step 4710, loss = 0.73, batch loss = 0.66 (8.8 examples/sec; 3.636 sec/batch; 79h:11m:52s remains)
INFO - root - 2017-12-07 06:53:29.443661: step 4720, loss = 0.78, batch loss = 0.71 (8.7 examples/sec; 3.664 sec/batch; 79h:47m:59s remains)
INFO - root - 2017-12-07 06:54:06.031900: step 4730, loss = 0.81, batch loss = 0.74 (9.2 examples/sec; 3.485 sec/batch; 75h:53m:14s remains)
INFO - root - 2017-12-07 06:54:42.343234: step 4740, loss = 0.75, batch loss = 0.68 (8.8 examples/sec; 3.636 sec/batch; 79h:10m:12s remains)
INFO - root - 2017-12-07 06:55:18.990686: step 4750, loss = 0.66, batch loss = 0.59 (9.0 examples/sec; 3.558 sec/batch; 77h:27m:48s remains)
INFO - root - 2017-12-07 06:55:55.696268: step 4760, loss = 0.72, batch loss = 0.64 (8.7 examples/sec; 3.685 sec/batch; 80h:13m:31s remains)
INFO - root - 2017-12-07 06:56:32.246005: step 4770, loss = 0.82, batch loss = 0.75 (8.6 examples/sec; 3.717 sec/batch; 80h:53m:37s remains)
INFO - root - 2017-12-07 06:57:08.947739: step 4780, loss = 0.74, batch loss = 0.67 (8.8 examples/sec; 3.633 sec/batch; 79h:04m:16s remains)
INFO - root - 2017-12-07 06:57:45.646562: step 4790, loss = 0.88, batch loss = 0.81 (8.8 examples/sec; 3.649 sec/batch; 79h:24m:19s remains)
INFO - root - 2017-12-07 06:58:21.970514: step 4800, loss = 0.78, batch loss = 0.71 (8.8 examples/sec; 3.627 sec/batch; 78h:54m:36s remains)
2017-12-07 06:58:24.109469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9729209 -2.928659 -2.9555678 -3.0144553 -2.9244428 -2.8552594 -2.8896332 -2.9529819 -3.0577686 -3.2075562 -3.25883 -3.2061257 -3.2286618 -3.2983184 -3.2589827][-2.9682331 -2.8626623 -2.8168516 -2.8422832 -2.7654519 -2.704144 -2.7469673 -2.830657 -2.9604154 -3.1003516 -3.147655 -3.1216915 -3.1560051 -3.2145228 -3.19118][-2.9748342 -2.8127894 -2.6957779 -2.6677012 -2.5928941 -2.5373995 -2.5627508 -2.6147408 -2.7153592 -2.819643 -2.8647423 -2.9080982 -3.0176687 -3.1055961 -3.1288927][-2.8773947 -2.7383432 -2.6442819 -2.6313396 -2.6083016 -2.5900102 -2.6058633 -2.6076624 -2.6395352 -2.6705005 -2.6728618 -2.7405894 -2.8928156 -3.0025954 -3.0793457][-2.8556614 -2.7988381 -2.7862821 -2.8232312 -2.862704 -2.8768432 -2.884378 -2.8565421 -2.8451424 -2.8161492 -2.7672241 -2.8080654 -2.9518654 -3.044193 -3.1313944][-2.8816066 -2.8908539 -2.9152555 -2.9300885 -2.9319878 -2.904119 -2.8703601 -2.8150353 -2.7812376 -2.7513847 -2.728497 -2.806294 -3.0071321 -3.1362391 -3.2359633][-2.8833203 -2.8983271 -2.8850219 -2.813046 -2.6806359 -2.5308 -2.4068153 -2.2971008 -2.2436464 -2.2521062 -2.3271458 -2.537776 -2.8850026 -3.1188338 -3.2618186][-2.7868981 -2.7642221 -2.7112436 -2.5882158 -2.3277049 -2.0463591 -1.8185942 -1.6839435 -1.695384 -1.8146663 -2.0014133 -2.3223128 -2.7766857 -3.0766628 -3.228524][-2.6399059 -2.5697808 -2.5208807 -2.4311595 -2.1368372 -1.7937269 -1.520803 -1.4356875 -1.5601764 -1.7814591 -2.0127182 -2.3373187 -2.7834024 -3.0810862 -3.2210026][-2.6710057 -2.5874028 -2.5935433 -2.6132865 -2.4209304 -2.1162474 -1.8484778 -1.8113995 -1.9827211 -2.2103655 -2.3963368 -2.6342139 -2.9925342 -3.2470331 -3.3506012][-2.7174137 -2.6571045 -2.7131233 -2.8706608 -2.8690364 -2.6835346 -2.4384475 -2.3887258 -2.5494337 -2.7650745 -2.9309523 -3.1081429 -3.3827014 -3.5660274 -3.5810938][-2.6464407 -2.6405506 -2.7430158 -2.9965754 -3.1571913 -3.0886669 -2.8919764 -2.8401008 -2.9935942 -3.2027273 -3.3597116 -3.5192838 -3.7347291 -3.8275361 -3.7303722][-2.3633082 -2.3917518 -2.4958539 -2.736419 -2.974968 -2.9976966 -2.8988256 -2.9133177 -3.0968056 -3.2893167 -3.4077232 -3.5406873 -3.6884022 -3.7001173 -3.5563369][-2.0409484 -2.0726922 -2.1277566 -2.262141 -2.4729805 -2.500978 -2.4420938 -2.4932551 -2.6759033 -2.8338592 -2.9417872 -3.0986838 -3.2441454 -3.2693968 -3.1974072][-1.9634984 -2.0155191 -2.0560515 -2.0930555 -2.2121506 -2.1728969 -2.0875173 -2.1023262 -2.2139621 -2.3091848 -2.4219036 -2.616437 -2.8140178 -2.9345317 -2.9920909]]...]
INFO - root - 2017-12-07 06:59:00.659334: step 4810, loss = 0.81, batch loss = 0.74 (8.7 examples/sec; 3.664 sec/batch; 79h:42m:33s remains)
INFO - root - 2017-12-07 06:59:37.426966: step 4820, loss = 0.66, batch loss = 0.58 (8.7 examples/sec; 3.658 sec/batch; 79h:34m:02s remains)
INFO - root - 2017-12-07 07:00:13.901568: step 4830, loss = 0.84, batch loss = 0.76 (8.7 examples/sec; 3.690 sec/batch; 80h:15m:31s remains)
INFO - root - 2017-12-07 07:00:50.416904: step 4840, loss = 0.70, batch loss = 0.63 (8.9 examples/sec; 3.609 sec/batch; 78h:29m:03s remains)
INFO - root - 2017-12-07 07:01:27.004742: step 4850, loss = 0.71, batch loss = 0.64 (8.7 examples/sec; 3.696 sec/batch; 80h:21m:50s remains)
INFO - root - 2017-12-07 07:02:03.604788: step 4860, loss = 0.70, batch loss = 0.63 (8.9 examples/sec; 3.598 sec/batch; 78h:13m:55s remains)
INFO - root - 2017-12-07 07:02:39.904740: step 4870, loss = 0.74, batch loss = 0.67 (8.7 examples/sec; 3.699 sec/batch; 80h:23m:53s remains)
INFO - root - 2017-12-07 07:03:16.718839: step 4880, loss = 0.85, batch loss = 0.78 (8.8 examples/sec; 3.648 sec/batch; 79h:17m:02s remains)
INFO - root - 2017-12-07 07:03:53.245566: step 4890, loss = 0.73, batch loss = 0.66 (8.8 examples/sec; 3.650 sec/batch; 79h:19m:26s remains)
INFO - root - 2017-12-07 07:04:29.921857: step 4900, loss = 0.81, batch loss = 0.74 (8.8 examples/sec; 3.650 sec/batch; 79h:19m:11s remains)
2017-12-07 07:04:32.077509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4474015 -2.3150945 -2.1868725 -2.0147576 -1.8986492 -1.8215818 -1.8617773 -1.8943424 -1.6347115 -1.40204 -1.4756849 -1.6324511 -1.6487448 -1.4931984 -1.4243898][-2.1729488 -2.0339086 -1.8836765 -1.7507322 -1.6037707 -1.4397531 -1.4500008 -1.5102375 -1.309186 -1.1770658 -1.3430583 -1.5270369 -1.5455182 -1.4141107 -1.3089185][-1.7576663 -1.6790144 -1.6421428 -1.6714773 -1.6118619 -1.4578624 -1.4376271 -1.4543674 -1.2529762 -1.1058664 -1.2366233 -1.3369493 -1.323384 -1.2162542 -1.051722][-1.6085527 -1.6056371 -1.6793709 -1.7896338 -1.7661107 -1.6279685 -1.5760646 -1.5495183 -1.3650024 -1.1912575 -1.2278211 -1.2333424 -1.1816285 -1.0695131 -0.83358288][-1.5920992 -1.5965545 -1.7041478 -1.8194427 -1.8063087 -1.6761351 -1.539861 -1.3916974 -1.171639 -1.0188448 -1.0786946 -1.1131165 -1.0706398 -0.94252133 -0.67722559][-1.4578018 -1.4350169 -1.5468254 -1.6414335 -1.6825402 -1.6481507 -1.4725642 -1.1650367 -0.86713767 -0.72041821 -0.79826164 -0.89371848 -0.88445878 -0.7274127 -0.4877243][-1.1764407 -1.1897433 -1.3350265 -1.398103 -1.4605677 -1.4905252 -1.3261805 -0.96891785 -0.69003177 -0.56009531 -0.55261445 -0.62050843 -0.58923364 -0.42080164 -0.30752659][-0.777874 -0.93348074 -1.1821675 -1.2488146 -1.2717626 -1.214355 -0.98188376 -0.70268941 -0.66570973 -0.67388105 -0.54820967 -0.52206659 -0.43739104 -0.2833333 -0.3049984][-0.52098751 -0.74069023 -0.97663736 -0.98857665 -0.95929909 -0.86356425 -0.61716819 -0.52948666 -0.78751373 -0.94228721 -0.76983285 -0.65803576 -0.52363396 -0.39425707 -0.48821926][-0.59200144 -0.70879173 -0.79731488 -0.70894027 -0.66179371 -0.61956763 -0.46334743 -0.51699591 -0.80273724 -0.957783 -0.85711288 -0.78260469 -0.68713 -0.59610128 -0.65646195][-0.81953835 -0.82365441 -0.85461831 -0.78851652 -0.76293635 -0.8002069 -0.7543025 -0.75826216 -0.74844956 -0.73796368 -0.73734355 -0.74661875 -0.69751072 -0.63101673 -0.65998387][-0.97999358 -0.90541315 -0.98149037 -1.0039802 -0.98349237 -1.0363314 -1.0685587 -0.98347878 -0.73606086 -0.64419651 -0.73680139 -0.74494624 -0.67357159 -0.6472671 -0.72887659][-1.0123725 -0.90110707 -1.029803 -1.117852 -1.0668471 -1.0758953 -1.0814064 -0.93058133 -0.67587376 -0.70502663 -0.9476614 -0.99313879 -0.90004015 -0.88763475 -0.99525309][-1.0229814 -0.95612574 -1.1303167 -1.2484162 -1.2283423 -1.2499821 -1.1862638 -0.98751831 -0.77292037 -0.82744026 -1.0875623 -1.1863093 -1.1661043 -1.176002 -1.2491324][-1.1831958 -1.1301904 -1.2437141 -1.2988851 -1.3100495 -1.4221394 -1.3987663 -1.2810838 -1.1346149 -1.0820699 -1.1753137 -1.2509484 -1.2867072 -1.3123753 -1.3649173]]...]
INFO - root - 2017-12-07 07:05:08.763655: step 4910, loss = 0.76, batch loss = 0.69 (8.7 examples/sec; 3.673 sec/batch; 79h:48m:04s remains)
INFO - root - 2017-12-07 07:05:45.604777: step 4920, loss = 0.87, batch loss = 0.80 (8.7 examples/sec; 3.676 sec/batch; 79h:50m:50s remains)
INFO - root - 2017-12-07 07:06:22.214192: step 4930, loss = 0.76, batch loss = 0.69 (8.5 examples/sec; 3.757 sec/batch; 81h:36m:45s remains)
INFO - root - 2017-12-07 07:06:59.144516: step 4940, loss = 0.71, batch loss = 0.64 (8.6 examples/sec; 3.731 sec/batch; 81h:01m:09s remains)
INFO - root - 2017-12-07 07:07:35.538811: step 4950, loss = 0.72, batch loss = 0.64 (8.8 examples/sec; 3.627 sec/batch; 78h:45m:36s remains)
INFO - root - 2017-12-07 07:08:12.003087: step 4960, loss = 0.74, batch loss = 0.67 (8.8 examples/sec; 3.626 sec/batch; 78h:43m:48s remains)
INFO - root - 2017-12-07 07:08:48.749508: step 4970, loss = 0.67, batch loss = 0.60 (8.7 examples/sec; 3.695 sec/batch; 80h:12m:40s remains)
INFO - root - 2017-12-07 07:09:25.294351: step 4980, loss = 0.72, batch loss = 0.65 (8.8 examples/sec; 3.646 sec/batch; 79h:07m:58s remains)
INFO - root - 2017-12-07 07:10:01.703659: step 4990, loss = 0.80, batch loss = 0.73 (8.9 examples/sec; 3.601 sec/batch; 78h:09m:13s remains)
INFO - root - 2017-12-07 07:10:38.265489: step 5000, loss = 0.83, batch loss = 0.75 (8.7 examples/sec; 3.680 sec/batch; 79h:51m:31s remains)
2017-12-07 07:10:40.480348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.247112 -2.3842714 -2.4611402 -2.2855418 -2.0737112 -1.9793112 -1.7847259 -1.5053761 -1.34866 -1.3038051 -1.2390666 -1.1677551 -1.0494318 -1.1816103 -1.4766934][-1.6846485 -1.7702594 -1.8566899 -1.6754935 -1.4593859 -1.5030501 -1.4916995 -1.2637169 -1.0831814 -1.0455139 -0.96919942 -0.84839964 -0.74158621 -0.98926282 -1.4602711][-1.6768928 -1.8527453 -1.9432511 -1.7513742 -1.5569041 -1.7178309 -1.8685262 -1.7575359 -1.5978367 -1.5407443 -1.4372771 -1.2768836 -1.1829007 -1.3755584 -1.8117986][-2.4034412 -2.8244433 -2.922157 -2.6558762 -2.4348779 -2.5591743 -2.6723237 -2.5187435 -2.3451385 -2.317868 -2.2582171 -2.1314709 -1.9930103 -1.924109 -2.0376546][-2.4502583 -2.8651094 -2.7772217 -2.2586565 -1.9228368 -1.897629 -1.8906858 -1.7348192 -1.6489205 -1.7920246 -1.9008322 -1.9388127 -1.8624103 -1.6631835 -1.5673139][-1.685056 -1.8809934 -1.5606859 -0.77586126 -0.27911282 -0.11234665 -0.016931057 0.016301632 -0.1344676 -0.53558278 -0.9130547 -1.1988955 -1.2725515 -1.1300004 -1.028374][-1.3329077 -1.5421844 -1.3433993 -0.66044736 -0.28120422 -0.23317528 -0.19499731 -0.24399376 -0.46050262 -0.8500452 -1.1898918 -1.3733788 -1.3170354 -1.0864155 -0.94541192][-1.5303822 -1.820276 -1.8516717 -1.5174983 -1.4203558 -1.5551324 -1.5224729 -1.4877067 -1.584933 -1.7670822 -1.8608735 -1.80899 -1.6045384 -1.2893214 -1.109694][-1.5985487 -1.8359466 -1.9562807 -1.8952918 -1.9814351 -2.1286507 -2.010325 -1.8492668 -1.8497868 -1.9626381 -1.971487 -1.8603144 -1.6637793 -1.3856297 -1.2257116][-1.3141716 -1.5138781 -1.6764572 -1.7966273 -1.9568679 -2.101367 -2.0291533 -1.9301853 -2.0102112 -2.2229769 -2.3011692 -2.2118726 -2.0530584 -1.843787 -1.7319548][-1.1499791 -1.5051084 -1.8138638 -2.0856857 -2.3138864 -2.499135 -2.56461 -2.6103306 -2.76496 -3.0050082 -3.0819912 -3.0029244 -2.8788855 -2.6906486 -2.5602341][-1.5751643 -2.0298679 -2.3981678 -2.7105289 -2.9632702 -3.1328168 -3.216327 -3.2616255 -3.3347456 -3.4184706 -3.3756478 -3.2691412 -3.1865869 -3.05405 -2.9562578][-2.3377178 -2.6630967 -2.9020085 -3.1081336 -3.314157 -3.4507914 -3.52885 -3.6048369 -3.6617942 -3.648469 -3.5077684 -3.3489246 -3.2519655 -3.1360512 -3.0626876][-2.8997865 -3.0453153 -3.0974345 -3.1510386 -3.2877572 -3.4241371 -3.5380979 -3.6778963 -3.7940688 -3.8128045 -3.7144074 -3.589046 -3.5000646 -3.410955 -3.3665876][-3.0567033 -3.118381 -3.126756 -3.1470919 -3.261884 -3.3840108 -3.4740348 -3.5799887 -3.6730373 -3.7149763 -3.6978967 -3.6677616 -3.6619606 -3.6561463 -3.6648533]]...]
INFO - root - 2017-12-07 07:11:17.029286: step 5010, loss = 0.85, batch loss = 0.78 (8.8 examples/sec; 3.631 sec/batch; 78h:47m:49s remains)
INFO - root - 2017-12-07 07:11:53.542327: step 5020, loss = 0.79, batch loss = 0.72 (8.7 examples/sec; 3.689 sec/batch; 80h:01m:52s remains)
INFO - root - 2017-12-07 07:12:30.075601: step 5030, loss = 0.78, batch loss = 0.71 (8.8 examples/sec; 3.651 sec/batch; 79h:11m:32s remains)
INFO - root - 2017-12-07 07:13:06.175931: step 5040, loss = 0.74, batch loss = 0.66 (10.0 examples/sec; 3.184 sec/batch; 69h:04m:00s remains)
INFO - root - 2017-12-07 07:13:42.826445: step 5050, loss = 0.82, batch loss = 0.75 (8.7 examples/sec; 3.698 sec/batch; 80h:11m:56s remains)
INFO - root - 2017-12-07 07:14:19.229178: step 5060, loss = 0.74, batch loss = 0.67 (9.1 examples/sec; 3.523 sec/batch; 76h:23m:56s remains)
INFO - root - 2017-12-07 07:14:56.100283: step 5070, loss = 0.81, batch loss = 0.74 (8.8 examples/sec; 3.640 sec/batch; 78h:55m:29s remains)
INFO - root - 2017-12-07 07:15:32.610018: step 5080, loss = 0.87, batch loss = 0.80 (8.8 examples/sec; 3.654 sec/batch; 79h:13m:04s remains)
INFO - root - 2017-12-07 07:16:09.107632: step 5090, loss = 0.82, batch loss = 0.75 (8.8 examples/sec; 3.640 sec/batch; 78h:53m:29s remains)
INFO - root - 2017-12-07 07:16:41.751929: step 5100, loss = 0.70, batch loss = 0.63 (12.2 examples/sec; 2.625 sec/batch; 56h:53m:49s remains)
2017-12-07 07:16:43.413558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0104437 -2.5161567 -2.1785471 -1.8334606 -1.7925189 -1.898448 -1.9689357 -2.3364711 -2.6254053 -2.8833716 -3.2030628 -3.152879 -2.8825502 -2.7787883 -2.8280818][-3.021281 -2.2927551 -1.6526315 -1.084779 -1.0815148 -1.309675 -1.3886638 -1.6801624 -1.9077146 -2.2276871 -2.6193616 -2.5399098 -2.2741926 -2.1596692 -2.1217132][-3.2411504 -2.2830467 -1.2634964 -0.4644773 -0.52518225 -0.87774348 -0.91595626 -0.96107006 -0.956547 -1.2678308 -1.7260444 -1.7158101 -1.5452859 -1.4241509 -1.2783878][-3.4124892 -2.1521404 -0.7287674 0.19023895 0.0021119118 -0.5287044 -0.61466336 -0.44012618 -0.26745319 -0.66175652 -1.3454471 -1.5654187 -1.5338533 -1.4395406 -1.271986][-3.5043476 -2.0069087 -0.32760143 0.582345 0.30529165 -0.33570862 -0.39689922 0.051176548 0.41662788 -0.054569721 -1.0087788 -1.4309046 -1.4138162 -1.3057683 -1.2050104][-3.53285 -1.8553581 -0.14160681 0.56472254 0.054283142 -0.82069325 -0.84522343 -0.056233406 0.60502434 0.25044632 -0.72103786 -1.1159191 -0.96491075 -0.79500031 -0.76202464][-3.4817524 -1.7299888 -0.051350117 0.56452894 -0.06157732 -1.0453224 -0.94806004 0.12002611 0.85876274 0.45755291 -0.588259 -1.0009489 -0.81902766 -0.63566518 -0.65907907][-3.38152 -1.5540617 0.098802567 0.67801952 0.013110161 -0.95638847 -0.76150751 0.39139557 1.0604687 0.61352348 -0.42931175 -0.95977306 -0.96602559 -0.8500545 -0.90631557][-3.1815357 -1.3579309 0.15560341 0.534502 -0.20226192 -1.0934486 -0.91333461 0.055867195 0.6357975 0.35864878 -0.48631167 -1.1452472 -1.3765852 -1.3167918 -1.2825656][-3.0039072 -1.3120637 -0.053518295 0.040846348 -0.81099081 -1.6237011 -1.5342214 -0.80775142 -0.30592251 -0.29988003 -0.71634674 -1.2544327 -1.4687924 -1.376951 -1.2770433][-2.9354372 -1.358886 -0.28168058 -0.36645603 -1.212079 -1.8634763 -1.7381046 -1.0424383 -0.45076728 -0.19808435 -0.31320524 -0.78340983 -1.0754533 -1.136425 -1.170361][-2.9292922 -1.498992 -0.51792312 -0.551522 -1.2083321 -1.6655848 -1.5568738 -0.93646359 -0.37014675 -0.1345706 -0.17268038 -0.58280349 -0.96442342 -1.2430189 -1.4248405][-2.9074645 -1.6315596 -0.72054958 -0.6591301 -1.199707 -1.7017596 -1.854171 -1.4595997 -1.0058804 -0.77843475 -0.61627555 -0.82491493 -1.1641579 -1.5523725 -1.7637227][-2.8577747 -1.8059995 -1.0730999 -1.0002573 -1.4102058 -1.8839436 -2.1072695 -1.7859867 -1.4115109 -1.2253604 -0.93973804 -1.08547 -1.3938332 -1.7164619 -1.7811742][-2.7410407 -1.8936911 -1.357136 -1.2975354 -1.5746839 -1.9463866 -2.1676846 -1.8244123 -1.4833615 -1.3395724 -1.0553634 -1.2695367 -1.6465158 -1.9220998 -1.922549]]...]
INFO - root - 2017-12-07 07:17:09.642770: step 5110, loss = 0.78, batch loss = 0.71 (12.0 examples/sec; 2.670 sec/batch; 57h:51m:35s remains)
INFO - root - 2017-12-07 07:17:35.463771: step 5120, loss = 0.78, batch loss = 0.70 (12.2 examples/sec; 2.616 sec/batch; 56h:40m:44s remains)
INFO - root - 2017-12-07 07:18:01.474397: step 5130, loss = 0.79, batch loss = 0.72 (12.3 examples/sec; 2.600 sec/batch; 56h:20m:14s remains)
INFO - root - 2017-12-07 07:18:27.700866: step 5140, loss = 0.83, batch loss = 0.76 (12.2 examples/sec; 2.622 sec/batch; 56h:48m:24s remains)
INFO - root - 2017-12-07 07:18:53.529770: step 5150, loss = 0.72, batch loss = 0.65 (13.4 examples/sec; 2.380 sec/batch; 51h:33m:10s remains)
INFO - root - 2017-12-07 07:19:19.592463: step 5160, loss = 0.82, batch loss = 0.74 (12.5 examples/sec; 2.562 sec/batch; 55h:29m:39s remains)
INFO - root - 2017-12-07 07:19:45.830893: step 5170, loss = 0.76, batch loss = 0.69 (12.1 examples/sec; 2.634 sec/batch; 57h:02m:50s remains)
INFO - root - 2017-12-07 07:20:11.984866: step 5180, loss = 0.77, batch loss = 0.69 (12.3 examples/sec; 2.611 sec/batch; 56h:32m:18s remains)
INFO - root - 2017-12-07 07:20:37.863319: step 5190, loss = 0.79, batch loss = 0.72 (12.4 examples/sec; 2.590 sec/batch; 56h:04m:08s remains)
INFO - root - 2017-12-07 07:21:04.057016: step 5200, loss = 0.88, batch loss = 0.81 (12.2 examples/sec; 2.625 sec/batch; 56h:49m:26s remains)
2017-12-07 07:21:05.658186: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4327538 -2.3457887 -2.3645515 -2.4832778 -2.6752837 -2.8434901 -2.8204415 -2.7539237 -2.5868573 -2.3658 -2.1506977 -1.9418318 -1.7858717 -1.675317 -1.6102133][-2.4773023 -2.2175035 -2.1770394 -2.3422606 -2.6240938 -2.9108019 -2.9790778 -2.9854131 -2.8483319 -2.647933 -2.3986676 -2.1044416 -1.8677561 -1.6999481 -1.5857971][-2.4921765 -2.0578797 -1.8414176 -1.8997049 -2.1914036 -2.587131 -2.8499777 -3.0403552 -3.047749 -2.961262 -2.7535048 -2.4326158 -2.1365042 -1.9016662 -1.7137027][-2.45961 -2.0298877 -1.6552064 -1.4591908 -1.6090076 -2.0134821 -2.4046283 -2.7473605 -2.9320989 -3.0524173 -3.0112789 -2.7921178 -2.5407176 -2.2755153 -2.0005479][-2.2643616 -2.0594149 -1.6664846 -1.2750511 -1.2486603 -1.5368667 -1.8733127 -2.2239916 -2.5256286 -2.833457 -3.0241427 -3.0422597 -2.9759462 -2.7706819 -2.4127584][-2.0501924 -2.105303 -1.8138051 -1.2966297 -0.99401855 -0.98235822 -1.0990608 -1.3616178 -1.7157285 -2.1440451 -2.5712543 -2.8664458 -3.0746536 -3.079422 -2.7549653][-1.8654695 -1.9887323 -1.8569837 -1.4058206 -0.89412928 -0.48630548 -0.24912691 -0.26379585 -0.56805491 -1.1005111 -1.7871954 -2.3340626 -2.7689512 -3.0261388 -2.8497396][-2.0177152 -2.0501196 -1.9610815 -1.6245368 -1.0670202 -0.42508078 0.14869118 0.5135169 0.47356939 -0.00688076 -0.83424377 -1.5600128 -2.1600268 -2.6385589 -2.6549559][-2.3192847 -2.346632 -2.2914491 -2.0520971 -1.5575013 -0.88876057 -0.19263315 0.450984 0.78993273 0.60406256 -0.062935829 -0.72729373 -1.3935771 -2.052453 -2.2992065][-2.6233821 -2.6788011 -2.6413846 -2.4795914 -2.0600441 -1.4376879 -0.74375176 -0.044166565 0.4358902 0.49798393 0.13336992 -0.26443195 -0.79761863 -1.4630153 -1.8713589][-2.8974433 -2.9364529 -2.874264 -2.7200649 -2.3504436 -1.8226175 -1.1893253 -0.50899816 -0.0079865456 0.17052603 0.03098917 -0.1077466 -0.38748741 -0.90712571 -1.3391671][-3.0488133 -3.0889075 -3.0558045 -2.923717 -2.6066823 -2.22172 -1.7102141 -1.0699778 -0.53002381 -0.24015331 -0.21392202 -0.21079302 -0.29735565 -0.63697433 -0.9942503][-2.6934104 -2.8197474 -2.9111094 -2.8914371 -2.6955795 -2.4834938 -2.1188688 -1.5534265 -1.0213704 -0.67593026 -0.57953024 -0.56957984 -0.5933733 -0.78605342 -1.0172739][-1.8310003 -1.9769049 -2.1137469 -2.1889863 -2.1658018 -2.2033615 -2.0757682 -1.6751325 -1.2431448 -0.92068839 -0.83776641 -0.90643477 -0.98495889 -1.1257493 -1.2674992][-1.1880462 -1.2737331 -1.3557699 -1.4411922 -1.5018508 -1.688283 -1.7516654 -1.5178761 -1.1948972 -0.88318729 -0.7668972 -0.85544062 -1.0121574 -1.2102585 -1.3862169]]...]
INFO - root - 2017-12-07 07:21:31.831042: step 5210, loss = 0.81, batch loss = 0.74 (12.3 examples/sec; 2.603 sec/batch; 56h:20m:18s remains)
INFO - root - 2017-12-07 07:21:57.843141: step 5220, loss = 0.77, batch loss = 0.70 (12.3 examples/sec; 2.611 sec/batch; 56h:30m:48s remains)
INFO - root - 2017-12-07 07:22:23.972689: step 5230, loss = 0.70, batch loss = 0.63 (12.3 examples/sec; 2.611 sec/batch; 56h:30m:08s remains)
INFO - root - 2017-12-07 07:22:50.170784: step 5240, loss = 0.74, batch loss = 0.67 (12.3 examples/sec; 2.605 sec/batch; 56h:21m:13s remains)
INFO - root - 2017-12-07 07:23:16.191785: step 5250, loss = 0.73, batch loss = 0.65 (12.4 examples/sec; 2.584 sec/batch; 55h:54m:23s remains)
INFO - root - 2017-12-07 07:23:42.648063: step 5260, loss = 0.82, batch loss = 0.75 (12.4 examples/sec; 2.581 sec/batch; 55h:49m:53s remains)
INFO - root - 2017-12-07 07:24:08.724883: step 5270, loss = 0.80, batch loss = 0.72 (12.1 examples/sec; 2.648 sec/batch; 57h:15m:36s remains)
INFO - root - 2017-12-07 07:24:34.698512: step 5280, loss = 0.80, batch loss = 0.73 (12.2 examples/sec; 2.624 sec/batch; 56h:44m:28s remains)
INFO - root - 2017-12-07 07:25:00.964334: step 5290, loss = 0.68, batch loss = 0.60 (12.1 examples/sec; 2.643 sec/batch; 57h:09m:15s remains)
INFO - root - 2017-12-07 07:25:27.121980: step 5300, loss = 0.75, batch loss = 0.68 (12.4 examples/sec; 2.591 sec/batch; 56h:00m:16s remains)
2017-12-07 07:25:28.707733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.775651 -0.56269431 -0.41855335 -0.37609577 -0.41742039 -0.44297528 -0.37882566 -0.45637035 -0.80751896 -1.2319374 -1.4581008 -1.4996545 -1.531013 -1.4979315 -1.4677095][-0.56553507 -0.37176037 -0.36728573 -0.50371218 -0.64811397 -0.74651766 -0.75212812 -0.86234879 -1.1055686 -1.2869115 -1.3177164 -1.3169346 -1.2422862 -1.0151629 -0.75900507][-0.72255826 -0.6359303 -0.88979173 -1.3112044 -1.5673909 -1.6422913 -1.4930255 -1.2809117 -1.0723088 -0.80666876 -0.5949235 -0.57971644 -0.43296885 -0.10325193 0.22269917][-1.1988642 -1.3671653 -1.7605093 -2.0874412 -2.0482521 -1.7791281 -1.3527575 -0.89843845 -0.4891603 -0.14026737 0.0061335564 -0.087447166 -0.018857479 0.20361948 0.38631344][-1.4067423 -1.7373152 -1.9786956 -1.854465 -1.3788259 -0.91456842 -0.53686523 -0.26054525 -0.12790442 -0.12811852 -0.24430275 -0.51556277 -0.61934781 -0.54846978 -0.43649745][-1.1854112 -1.3787091 -1.3637822 -1.0205972 -0.618067 -0.49052715 -0.60796213 -0.77394271 -0.9753778 -1.1366458 -1.1534741 -1.2799375 -1.247663 -0.99050951 -0.7087965][-0.99171877 -1.0770946 -1.0175402 -0.78822374 -0.62307215 -0.73166418 -0.95805097 -1.0443561 -1.1075008 -1.0522254 -0.84975791 -0.85385084 -0.74315095 -0.37491989 -0.064303875][-1.207459 -1.3182824 -1.2258387 -0.871536 -0.40772057 -0.019661427 0.21626282 0.42985868 0.41559982 0.3496933 0.3873682 0.18793869 0.0959177 0.25084591 0.31808043][-1.2813246 -1.0890949 -0.69752312 -0.047384262 0.73064613 1.3636394 1.5822492 1.5593066 1.1874666 0.73961163 0.54191923 0.3352747 0.28795576 0.27167082 0.11399508][-0.8142221 -0.36371565 0.11882162 0.64474916 1.0757966 1.1822863 0.87245369 0.54617023 0.21674347 -0.077914715 -0.13753843 -0.099171162 0.07277441 0.09185648 -0.097612381][-0.60773587 -0.26945639 -0.037404537 0.090873718 0.013455868 -0.31265163 -0.66581273 -0.752038 -0.60266495 -0.5055964 -0.49376822 -0.42865133 -0.20300961 -0.12521315 -0.24967289][-1.1099532 -0.87052321 -0.65256739 -0.53016233 -0.62059474 -0.78785563 -0.79290605 -0.59954071 -0.34571695 -0.342124 -0.58207488 -0.74489522 -0.65259457 -0.57247734 -0.60883427][-1.3874843 -0.98108888 -0.6503973 -0.50271535 -0.59766889 -0.615041 -0.34266663 -0.09163475 -0.076682568 -0.3460269 -0.768383 -1.0451086 -1.0467024 -0.96513343 -0.92922187][-1.1016335 -0.78583264 -0.69325185 -0.81871557 -1.0129933 -0.9424715 -0.60974264 -0.4766202 -0.66197228 -0.94527173 -1.1936998 -1.2759745 -1.1477551 -1.0115702 -0.99519134][-1.042629 -1.1255746 -1.338923 -1.5492871 -1.6267269 -1.3963435 -1.1163416 -1.1907473 -1.4851995 -1.5793586 -1.5102265 -1.328656 -1.0381174 -0.87288237 -0.92710185]]...]
INFO - root - 2017-12-07 07:25:54.663011: step 5310, loss = 0.78, batch loss = 0.71 (12.0 examples/sec; 2.660 sec/batch; 57h:30m:04s remains)
INFO - root - 2017-12-07 07:26:20.803342: step 5320, loss = 0.79, batch loss = 0.72 (12.3 examples/sec; 2.593 sec/batch; 56h:02m:23s remains)
INFO - root - 2017-12-07 07:26:47.106708: step 5330, loss = 0.78, batch loss = 0.71 (11.9 examples/sec; 2.682 sec/batch; 57h:57m:35s remains)
INFO - root - 2017-12-07 07:27:13.417897: step 5340, loss = 0.81, batch loss = 0.73 (11.8 examples/sec; 2.709 sec/batch; 58h:32m:12s remains)
INFO - root - 2017-12-07 07:27:39.568499: step 5350, loss = 0.70, batch loss = 0.63 (12.0 examples/sec; 2.656 sec/batch; 57h:22m:30s remains)
INFO - root - 2017-12-07 07:28:05.764861: step 5360, loss = 0.77, batch loss = 0.70 (11.9 examples/sec; 2.687 sec/batch; 58h:02m:28s remains)
INFO - root - 2017-12-07 07:28:32.054485: step 5370, loss = 0.72, batch loss = 0.65 (12.2 examples/sec; 2.631 sec/batch; 56h:49m:42s remains)
INFO - root - 2017-12-07 07:28:58.079544: step 5380, loss = 0.74, batch loss = 0.67 (12.2 examples/sec; 2.616 sec/batch; 56h:29m:16s remains)
INFO - root - 2017-12-07 07:29:24.287953: step 5390, loss = 0.71, batch loss = 0.64 (12.2 examples/sec; 2.625 sec/batch; 56h:40m:16s remains)
INFO - root - 2017-12-07 07:29:50.392036: step 5400, loss = 0.82, batch loss = 0.75 (12.3 examples/sec; 2.609 sec/batch; 56h:19m:46s remains)
2017-12-07 07:29:51.996232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9616849 -4.0073009 -3.8831871 -3.659663 -3.4433894 -3.2923646 -3.2873244 -3.4509234 -3.6983697 -3.9331791 -4.0279322 -3.9662352 -3.80227 -3.5838642 -3.3860898][-3.8218677 -3.8679013 -3.7191982 -3.4555748 -3.215147 -3.0310183 -2.97486 -3.1190391 -3.4221311 -3.7783291 -4.0136771 -4.0279884 -3.8193493 -3.4472613 -3.1009943][-3.323101 -3.2925098 -3.1822298 -3.065906 -3.0302358 -2.993083 -2.9459372 -3.0048902 -3.2266073 -3.5724611 -3.9017692 -4.0367079 -3.8464575 -3.3959315 -2.9868181][-2.546638 -2.3158824 -2.2374191 -2.380929 -2.7079628 -2.9499006 -2.9680617 -2.9082098 -2.9457865 -3.139843 -3.4921665 -3.742233 -3.6377058 -3.2379646 -2.8956437][-1.7673988 -1.3059151 -1.2553308 -1.644855 -2.3008265 -2.7843089 -2.8259568 -2.5957115 -2.3908143 -2.3776433 -2.7383211 -3.1640246 -3.2833486 -3.1069655 -2.9688306][-1.4572434 -0.87070847 -0.8089416 -1.2564495 -1.9783809 -2.4418941 -2.3307471 -1.8749812 -1.4494524 -1.2831872 -1.7161086 -2.4230411 -2.9307797 -3.1485307 -3.3138905][-1.8502634 -1.3212702 -1.1763134 -1.3880682 -1.853869 -2.1125131 -1.8035803 -1.1449654 -0.57632518 -0.38501453 -0.99648118 -2.0648494 -2.9818108 -3.5539892 -3.9378657][-2.5736837 -2.2702808 -2.0733078 -1.9418418 -2.0102487 -2.0190046 -1.5780313 -0.79775167 -0.11571503 0.10472775 -0.56847048 -1.8572578 -3.0635929 -3.9403517 -4.4944682][-3.1046257 -3.0010026 -2.75425 -2.3741496 -2.1950212 -2.1241643 -1.7663808 -1.1290324 -0.56610084 -0.428751 -1.0181861 -2.1513739 -3.2788236 -4.1967382 -4.7944965][-2.9101305 -2.7818089 -2.5493965 -2.2722957 -2.2359056 -2.3455448 -2.2006776 -1.7930882 -1.4941523 -1.6059456 -2.1657195 -2.9403989 -3.6442528 -4.2537327 -4.6918454][-2.4562616 -2.1583195 -1.9914804 -2.0328016 -2.3340793 -2.6666098 -2.6659741 -2.4195366 -2.3591087 -2.6948352 -3.2017269 -3.588124 -3.8072395 -4.0170417 -4.2236547][-2.1781993 -1.761158 -1.7249782 -2.0994284 -2.6520939 -3.0325089 -3.0154645 -2.8142841 -2.8632073 -3.2793155 -3.6951385 -3.8186896 -3.7313082 -3.6804504 -3.7316997][-2.3715549 -1.9945192 -2.0420485 -2.5124378 -3.0569177 -3.3234801 -3.2217791 -3.0144076 -3.0819223 -3.4818609 -3.8510189 -3.91153 -3.7510626 -3.5971274 -3.5571249][-2.9771748 -2.7786593 -2.849864 -3.1879296 -3.5200293 -3.6345968 -3.5111661 -3.350358 -3.4137082 -3.7311416 -4.0150437 -4.0616341 -3.9522402 -3.8323483 -3.7703996][-3.509053 -3.4897478 -3.5719943 -3.7553012 -3.8871539 -3.9005904 -3.8120434 -3.7325108 -3.7952054 -4.002667 -4.1750546 -4.1941719 -4.1291947 -4.0510921 -3.9823542]]...]
INFO - root - 2017-12-07 07:30:17.929464: step 5410, loss = 0.86, batch loss = 0.79 (12.1 examples/sec; 2.635 sec/batch; 56h:53m:10s remains)
INFO - root - 2017-12-07 07:30:44.072548: step 5420, loss = 0.85, batch loss = 0.78 (12.4 examples/sec; 2.581 sec/batch; 55h:42m:21s remains)
INFO - root - 2017-12-07 07:31:10.168350: step 5430, loss = 0.81, batch loss = 0.73 (12.3 examples/sec; 2.611 sec/batch; 56h:20m:36s remains)
INFO - root - 2017-12-07 07:31:36.369305: step 5440, loss = 0.82, batch loss = 0.75 (11.9 examples/sec; 2.683 sec/batch; 57h:54m:24s remains)
INFO - root - 2017-12-07 07:32:02.665525: step 5450, loss = 0.80, batch loss = 0.72 (12.3 examples/sec; 2.606 sec/batch; 56h:14m:17s remains)
INFO - root - 2017-12-07 07:32:28.751990: step 5460, loss = 0.69, batch loss = 0.61 (12.4 examples/sec; 2.589 sec/batch; 55h:51m:49s remains)
INFO - root - 2017-12-07 07:32:54.731571: step 5470, loss = 0.81, batch loss = 0.74 (12.4 examples/sec; 2.573 sec/batch; 55h:30m:31s remains)
INFO - root - 2017-12-07 07:33:21.244151: step 5480, loss = 0.81, batch loss = 0.73 (12.0 examples/sec; 2.673 sec/batch; 57h:39m:35s remains)
INFO - root - 2017-12-07 07:33:47.811732: step 5490, loss = 0.77, batch loss = 0.70 (11.8 examples/sec; 2.702 sec/batch; 58h:15m:35s remains)
INFO - root - 2017-12-07 07:34:14.132726: step 5500, loss = 0.91, batch loss = 0.84 (12.2 examples/sec; 2.624 sec/batch; 56h:35m:04s remains)
2017-12-07 07:34:15.597915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3408895 -1.408534 -1.4395843 -1.124068 -0.64903283 -0.31750202 -0.039252281 0.09086132 -0.099904537 -0.4138875 -0.64160466 -0.69860673 -0.20441628 0.905879 1.8123274][-1.1418827 -1.2894635 -1.3878748 -1.0570476 -0.50912356 -0.12148285 0.18163919 0.33407402 0.09635973 -0.41610813 -0.97847486 -1.3342814 -0.93460822 0.28510809 1.5265217][-1.0377457 -1.1695826 -1.2822938 -0.92124605 -0.27768612 0.19657087 0.50059175 0.60302639 0.26090622 -0.45258427 -1.354311 -2.0221372 -1.7806227 -0.61708546 0.8319478][-1.1015725 -1.1960688 -1.2627878 -0.81456542 -0.035131454 0.5855422 0.957078 1.0073066 0.52898407 -0.4054184 -1.6172647 -2.5550199 -2.5197568 -1.5177429 -0.0077157021][-1.2524736 -1.3428547 -1.3572917 -0.84626484 0.08964777 0.94208 1.4862375 1.5581236 0.97805262 -0.15086126 -1.6472251 -2.8513713 -3.0863969 -2.3000557 -0.85528493][-1.41346 -1.5267134 -1.5019021 -0.99176478 0.037438393 1.1358237 1.9331374 2.1380858 1.5501404 0.26270294 -1.4963808 -2.9871597 -3.5597532 -3.0480483 -1.7341714][-1.5042236 -1.668946 -1.6138892 -1.1241443 -0.0505414 1.2405424 2.3182931 2.7894025 2.2646222 0.82164431 -1.1880836 -2.9716532 -3.9048438 -3.7513642 -2.6599913][-1.546917 -1.7249455 -1.6689548 -1.2269862 -0.17449665 1.2235379 2.5631738 3.3960319 2.9987173 1.449214 -0.73198557 -2.7566576 -4.025043 -4.2221 -3.3879089][-1.6056061 -1.7838857 -1.7725222 -1.4220853 -0.43974614 0.91772842 2.2959332 3.2915063 3.1858168 1.8451562 -0.2343092 -2.3347058 -3.8485632 -4.3376708 -3.7628896][-1.6032469 -1.8137574 -1.9351802 -1.7593906 -0.88808727 0.36144829 1.6835194 2.8114848 3.0440311 2.0652261 0.25351143 -1.7522962 -3.3283548 -3.9808881 -3.6643567][-1.5532792 -1.8020813 -2.0967968 -2.1751244 -1.5239725 -0.45662355 0.79428959 2.1170068 2.7094812 2.1307979 0.63444042 -1.2100768 -2.7404718 -3.4375739 -3.3063233][-1.3712552 -1.6593654 -2.1348083 -2.4988194 -2.1622941 -1.3761125 -0.27621984 1.1092458 1.9334345 1.7399707 0.64274406 -0.92022252 -2.2987053 -2.968051 -2.9503624][-1.2204585 -1.4721982 -2.0438046 -2.6254129 -2.6022303 -2.0872605 -1.1634743 0.13852453 0.99974203 1.051662 0.34728575 -0.86784291 -2.0167048 -2.5947967 -2.6206453][-1.104706 -1.1578095 -1.7028396 -2.4559016 -2.7818227 -2.6454642 -2.0037537 -0.8582468 0.047430992 0.34948587 0.011619091 -0.87456393 -1.7992718 -2.2875614 -2.3220954][-0.82191515 -0.57829547 -1.0268977 -1.9352727 -2.6479344 -2.9913125 -2.7614617 -1.8609557 -0.927752 -0.36873579 -0.3492136 -0.88931513 -1.5739808 -1.9945743 -2.074738]]...]
INFO - root - 2017-12-07 07:34:42.187508: step 5510, loss = 0.77, batch loss = 0.70 (12.0 examples/sec; 2.672 sec/batch; 57h:35m:54s remains)
INFO - root - 2017-12-07 07:35:08.577573: step 5520, loss = 0.76, batch loss = 0.69 (12.1 examples/sec; 2.639 sec/batch; 56h:53m:25s remains)
INFO - root - 2017-12-07 07:35:34.913533: step 5530, loss = 0.73, batch loss = 0.66 (11.9 examples/sec; 2.679 sec/batch; 57h:44m:07s remains)
INFO - root - 2017-12-07 07:36:01.229929: step 5540, loss = 0.76, batch loss = 0.69 (12.0 examples/sec; 2.665 sec/batch; 57h:26m:27s remains)
INFO - root - 2017-12-07 07:36:27.596259: step 5550, loss = 0.77, batch loss = 0.70 (12.2 examples/sec; 2.622 sec/batch; 56h:30m:10s remains)
INFO - root - 2017-12-07 07:36:53.969481: step 5560, loss = 0.78, batch loss = 0.71 (12.1 examples/sec; 2.641 sec/batch; 56h:54m:21s remains)
INFO - root - 2017-12-07 07:37:20.123896: step 5570, loss = 0.70, batch loss = 0.62 (12.2 examples/sec; 2.626 sec/batch; 56h:34m:34s remains)
INFO - root - 2017-12-07 07:37:46.452389: step 5580, loss = 0.74, batch loss = 0.66 (12.1 examples/sec; 2.636 sec/batch; 56h:46m:39s remains)
INFO - root - 2017-12-07 07:38:12.628968: step 5590, loss = 0.74, batch loss = 0.67 (12.0 examples/sec; 2.670 sec/batch; 57h:30m:36s remains)
INFO - root - 2017-12-07 07:38:38.767844: step 5600, loss = 0.70, batch loss = 0.62 (12.1 examples/sec; 2.653 sec/batch; 57h:08m:28s remains)
2017-12-07 07:38:40.395258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2913284 -2.8315141 -2.5982022 -2.5785422 -2.5745778 -2.5155296 -2.3465197 -2.1541233 -2.0659289 -2.1155539 -2.1866848 -2.1760261 -2.1812062 -2.1424241 -1.9275105][-3.630064 -3.0309823 -2.686157 -2.5707235 -2.46413 -2.3373821 -2.0946834 -1.7701595 -1.5378199 -1.4815686 -1.4888699 -1.4761224 -1.538682 -1.5788493 -1.4528012][-4.1143503 -3.5302346 -3.1396172 -2.8833275 -2.6120853 -2.3816445 -2.0266316 -1.5112834 -1.1065726 -0.9365201 -0.9471066 -1.0405543 -1.2214391 -1.3521688 -1.3236296][-4.4056239 -3.8741596 -3.4691033 -3.1104798 -2.7807026 -2.5604677 -2.1577234 -1.5201409 -0.98159838 -0.73389721 -0.81973219 -1.0635955 -1.3225861 -1.4495881 -1.4348967][-4.2911997 -3.6959682 -3.158025 -2.6931171 -2.3822491 -2.3165007 -2.0436509 -1.3817642 -0.70728731 -0.38561916 -0.58955288 -1.0270226 -1.400198 -1.5270834 -1.5031056][-4.004149 -3.3616357 -2.7448559 -2.2604351 -2.0167849 -2.1047041 -1.9375012 -1.1844597 -0.27545309 0.24790239 0.12792301 -0.34073782 -0.80266023 -1.0573483 -1.1901152][-3.7956038 -3.1557641 -2.4920931 -1.9809523 -1.789119 -1.9858022 -1.9136252 -1.1233528 -0.0011110306 0.86641884 1.1108155 0.80735111 0.27914906 -0.204638 -0.52973437][-3.8004591 -3.2493565 -2.5599856 -1.8871574 -1.552747 -1.6669533 -1.5931199 -0.855489 0.31651926 1.4446745 1.9968367 1.8361549 1.1568856 0.38000107 -0.08622694][-3.9098346 -3.624671 -3.1171556 -2.4000814 -1.8746645 -1.7486975 -1.5550697 -0.85545969 0.26429176 1.4606452 2.1587152 2.1090593 1.2680626 0.20860243 -0.33337307][-3.8734746 -3.9013889 -3.7129195 -3.1959033 -2.6157227 -2.2554216 -1.9787366 -1.4369309 -0.603801 0.36549044 1.0440917 1.144063 0.40782976 -0.62021947 -1.1413386][-3.8120823 -4.0280848 -4.104773 -3.8939807 -3.4210725 -2.9982939 -2.7306559 -2.3811111 -1.8767936 -1.1894064 -0.58216667 -0.3347621 -0.77474523 -1.540704 -1.9772332][-3.8949158 -4.0964432 -4.2928762 -4.37209 -4.15988 -3.875006 -3.6746869 -3.4021912 -3.0416696 -2.503567 -1.9482641 -1.5757051 -1.7047267 -2.1721461 -2.5258074][-4.0963669 -4.1644588 -4.307322 -4.4761 -4.4814496 -4.4140377 -4.3683939 -4.2235422 -3.9692397 -3.5526354 -3.0599275 -2.6504204 -2.5847054 -2.8197451 -3.052187][-4.2872639 -4.2658067 -4.2812753 -4.3321805 -4.3057742 -4.267148 -4.2742591 -4.245295 -4.1748443 -4.0259595 -3.7828774 -3.5401182 -3.4703648 -3.5632989 -3.6489568][-4.2865462 -4.236681 -4.1879587 -4.1348548 -4.0222716 -3.8897784 -3.7919047 -3.725255 -3.7032459 -3.707839 -3.6905637 -3.6686695 -3.7237115 -3.8340063 -3.9047985]]...]
INFO - root - 2017-12-07 07:39:06.723482: step 5610, loss = 0.70, batch loss = 0.63 (12.3 examples/sec; 2.612 sec/batch; 56h:13m:50s remains)
INFO - root - 2017-12-07 07:39:33.046723: step 5620, loss = 0.78, batch loss = 0.71 (12.3 examples/sec; 2.607 sec/batch; 56h:08m:09s remains)
INFO - root - 2017-12-07 07:39:59.098778: step 5630, loss = 0.71, batch loss = 0.64 (12.2 examples/sec; 2.621 sec/batch; 56h:25m:06s remains)
INFO - root - 2017-12-07 07:40:25.242817: step 5640, loss = 0.78, batch loss = 0.70 (12.1 examples/sec; 2.634 sec/batch; 56h:42m:02s remains)
INFO - root - 2017-12-07 07:40:51.580070: step 5650, loss = 0.80, batch loss = 0.73 (12.2 examples/sec; 2.623 sec/batch; 56h:26m:53s remains)
INFO - root - 2017-12-07 07:41:17.280092: step 5660, loss = 0.80, batch loss = 0.72 (15.5 examples/sec; 2.068 sec/batch; 44h:30m:04s remains)
INFO - root - 2017-12-07 07:41:43.823842: step 5670, loss = 0.79, batch loss = 0.72 (12.1 examples/sec; 2.648 sec/batch; 56h:58m:08s remains)
INFO - root - 2017-12-07 07:42:10.092373: step 5680, loss = 0.73, batch loss = 0.65 (12.4 examples/sec; 2.586 sec/batch; 55h:38m:15s remains)
INFO - root - 2017-12-07 07:42:36.320356: step 5690, loss = 0.82, batch loss = 0.75 (12.0 examples/sec; 2.665 sec/batch; 57h:19m:55s remains)
INFO - root - 2017-12-07 07:43:02.473936: step 5700, loss = 0.79, batch loss = 0.72 (12.1 examples/sec; 2.645 sec/batch; 56h:53m:13s remains)
2017-12-07 07:43:04.141309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1109042 -2.0096457 -1.9937975 -2.0212483 -2.1176131 -2.3161583 -2.5384035 -2.6665442 -2.7475736 -2.7306087 -2.6312594 -2.463119 -2.2239392 -2.0785077 -2.0645719][-2.5729256 -2.4845979 -2.4739895 -2.480618 -2.5383651 -2.774936 -3.0282493 -3.1847506 -3.3494892 -3.3660545 -3.145545 -2.7371583 -2.1783864 -1.8175094 -1.8052502][-3.2916119 -3.1651325 -3.0397243 -2.968585 -3.0170884 -3.2343984 -3.4901466 -3.729346 -4.0968118 -4.1818395 -3.7740898 -3.0556197 -2.1509809 -1.5703239 -1.5960212][-3.5624876 -3.2369452 -2.8496387 -2.6416209 -2.6437011 -2.8175893 -3.1201632 -3.5498083 -4.2220335 -4.496119 -4.051415 -3.1524298 -2.002784 -1.2723389 -1.3872633][-3.6393826 -3.0207877 -2.3275902 -1.9345667 -1.8145065 -1.8617995 -2.1616991 -2.6891234 -3.4973204 -3.9844217 -3.7128298 -2.8546467 -1.6259637 -0.85268521 -1.1278243][-3.7617726 -2.8326361 -1.8323865 -1.174284 -0.86056209 -0.77969933 -1.1008582 -1.6859846 -2.4886622 -3.1785479 -3.2678461 -2.669672 -1.5149243 -0.72791815 -1.0593171][-3.6000752 -2.6853895 -1.6331758 -0.8329134 -0.28724146 0.054987431 -0.084379196 -0.63897681 -1.4416726 -2.4112403 -2.9357088 -2.7123859 -1.7352335 -0.98705626 -1.2461345][-3.4623945 -2.893898 -2.0856833 -1.3861732 -0.62656808 0.24180079 0.79348373 0.65982533 -0.084195137 -1.3367453 -2.399652 -2.658088 -1.961303 -1.297852 -1.4570866][-3.18349 -2.9829569 -2.4554176 -1.9897356 -1.3208692 -0.25204659 0.83826017 1.1667781 0.5377655 -0.86319137 -2.2803593 -2.8331017 -2.2578413 -1.5781748 -1.6190002][-2.4680805 -2.5681489 -2.3757815 -2.2213206 -1.7948713 -0.91424894 0.11294079 0.46729612 -0.16222763 -1.5411096 -2.8892083 -3.3504481 -2.6708846 -1.8964508 -1.8488758][-1.9562049 -2.2236631 -2.3352468 -2.4120641 -2.2104812 -1.5435328 -0.74236178 -0.54614854 -1.1601505 -2.3208573 -3.3463998 -3.6240277 -2.9293871 -2.1791091 -2.1124988][-1.5564156 -1.6998892 -1.9169424 -2.2129281 -2.3751972 -2.0588713 -1.542743 -1.465153 -1.9642353 -2.7713923 -3.3631701 -3.4397635 -2.853899 -2.2618718 -2.2394209][-1.1587448 -1.142278 -1.3457398 -1.7728841 -2.2549028 -2.3732264 -2.1919169 -2.2165456 -2.5158124 -2.9305971 -3.1419649 -3.0797386 -2.6590395 -2.2611067 -2.2645974][-1.3458116 -1.2644329 -1.3561268 -1.6638367 -2.0918791 -2.3216088 -2.3057065 -2.402194 -2.6412449 -2.9093742 -3.0336897 -2.99336 -2.7355323 -2.4471619 -2.3979535][-1.7213993 -1.6226492 -1.6274796 -1.7864196 -2.0643308 -2.2623415 -2.3198464 -2.4473574 -2.6649823 -2.8825648 -3.0192537 -3.0230181 -2.8684926 -2.6472201 -2.5458546]]...]
INFO - root - 2017-12-07 07:43:30.497776: step 5710, loss = 0.84, batch loss = 0.76 (12.4 examples/sec; 2.576 sec/batch; 55h:23m:21s remains)
INFO - root - 2017-12-07 07:43:56.805161: step 5720, loss = 0.76, batch loss = 0.69 (12.0 examples/sec; 2.671 sec/batch; 57h:25m:16s remains)
INFO - root - 2017-12-07 07:44:22.866190: step 5730, loss = 0.76, batch loss = 0.69 (12.3 examples/sec; 2.611 sec/batch; 56h:07m:45s remains)
INFO - root - 2017-12-07 07:44:49.077857: step 5740, loss = 0.79, batch loss = 0.71 (12.3 examples/sec; 2.601 sec/batch; 55h:54m:27s remains)
INFO - root - 2017-12-07 07:45:15.422433: step 5750, loss = 0.82, batch loss = 0.74 (12.2 examples/sec; 2.618 sec/batch; 56h:15m:53s remains)
INFO - root - 2017-12-07 07:45:41.425501: step 5760, loss = 0.78, batch loss = 0.70 (12.1 examples/sec; 2.645 sec/batch; 56h:50m:39s remains)
INFO - root - 2017-12-07 07:46:07.667211: step 5770, loss = 0.76, batch loss = 0.69 (12.5 examples/sec; 2.569 sec/batch; 55h:11m:45s remains)
INFO - root - 2017-12-07 07:46:33.975188: step 5780, loss = 0.77, batch loss = 0.70 (12.3 examples/sec; 2.608 sec/batch; 56h:01m:58s remains)
INFO - root - 2017-12-07 07:47:00.016575: step 5790, loss = 0.76, batch loss = 0.69 (12.2 examples/sec; 2.622 sec/batch; 56h:19m:17s remains)
INFO - root - 2017-12-07 07:47:26.587539: step 5800, loss = 0.79, batch loss = 0.71 (12.3 examples/sec; 2.605 sec/batch; 55h:57m:22s remains)
2017-12-07 07:47:28.235631: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.58030581 -0.573323 -0.40699291 -0.25987387 -0.39447117 -0.5078702 -0.26452446 -0.063511848 -0.17799425 -0.3649869 -0.29894352 -0.38574934 -0.91200566 -1.4833636 -1.8043144][-0.16005039 0.071496964 0.40186596 0.53497839 0.29542685 0.10511017 0.34310532 0.52065706 0.3250761 0.080674648 0.13676929 0.063017368 -0.48823 -1.083308 -1.3690677][-0.10326481 0.33417034 0.82575226 0.89333916 0.52167273 0.23217249 0.43187237 0.56803131 0.27141571 0.0055413246 0.076967716 0.010984898 -0.48072529 -0.99824047 -1.175699][-0.0033793449 0.48596287 1.0205154 0.96675014 0.51140785 0.20587111 0.38568497 0.49810839 0.12737083 -0.16050625 -0.10300493 -0.13752556 -0.50155759 -0.92573929 -1.0268216][0.30628538 0.68984079 1.141758 1.0073147 0.59324074 0.43665695 0.74958658 0.90416 0.44965219 0.0059533119 -0.094999313 -0.21479511 -0.52543259 -0.90248322 -0.9355998][0.5511241 0.80321741 1.1374922 1.0131631 0.78056908 0.93119574 1.5543613 1.7211833 0.95594692 0.12726307 -0.23573685 -0.45743394 -0.73711133 -1.037715 -0.97293186][0.72966528 0.89346933 1.1274242 1.0884004 1.0851679 1.6267605 2.6203418 2.626574 1.2756052 -0.041752338 -0.62166977 -0.91577506 -1.1536779 -1.3370824 -1.1719191][0.8224473 0.97019863 1.1215925 1.1446552 1.3309636 2.1236062 3.2461834 2.9226599 1.0853996 -0.47548223 -1.0620213 -1.3048539 -1.416971 -1.4229338 -1.2001486][0.77568531 0.9871192 1.1705728 1.2367897 1.3334031 1.8154011 2.4146237 1.8249068 0.0877676 -1.2000501 -1.4770763 -1.4387403 -1.2913418 -1.0754938 -0.8480916][0.70610476 0.96264696 1.2148318 1.20116 0.96684074 0.85325766 0.87609625 0.24620724 -1.0414171 -1.8179903 -1.7045286 -1.3036442 -0.88992596 -0.4943943 -0.26366043][0.83036137 1.0367799 1.2507086 1.0163145 0.42232609 -0.12581444 -0.44004369 -0.93781996 -1.7309637 -2.0761638 -1.7082903 -1.0971165 -0.55974054 -0.098148346 0.16791916][1.1209402 1.1497283 1.1746697 0.72260904 -0.0089659691 -0.6502068 -1.0305877 -1.4090519 -1.916791 -2.0547798 -1.6463275 -1.0891447 -0.68446493 -0.33475971 -0.094718933][1.2127066 1.0303059 0.84363985 0.33508348 -0.29418278 -0.74697638 -1.0115621 -1.3188286 -1.6890798 -1.8039205 -1.5064676 -1.1221693 -0.89043045 -0.69166946 -0.56596088][0.52616692 0.27393341 0.094330311 -0.29052496 -0.72786736 -0.94166279 -1.0304992 -1.1834757 -1.3514936 -1.4068294 -1.2835591 -1.1244025 -1.0653718 -1.0167603 -1.0358899][-0.42792749 -0.63534594 -0.72606134 -1.0094619 -1.3212447 -1.4014754 -1.3122962 -1.233711 -1.0820143 -0.98264527 -1.0390885 -1.1650169 -1.2867606 -1.3973475 -1.5634868]]...]
INFO - root - 2017-12-07 07:47:54.640578: step 5810, loss = 0.82, batch loss = 0.74 (11.9 examples/sec; 2.688 sec/batch; 57h:43m:21s remains)
INFO - root - 2017-12-07 07:48:20.875960: step 5820, loss = 0.87, batch loss = 0.79 (13.4 examples/sec; 2.395 sec/batch; 51h:25m:13s remains)
INFO - root - 2017-12-07 07:48:47.058506: step 5830, loss = 0.82, batch loss = 0.75 (12.4 examples/sec; 2.581 sec/batch; 55h:25m:30s remains)
INFO - root - 2017-12-07 07:49:13.525491: step 5840, loss = 0.77, batch loss = 0.69 (12.1 examples/sec; 2.648 sec/batch; 56h:50m:58s remains)
INFO - root - 2017-12-07 07:49:39.975171: step 5850, loss = 0.74, batch loss = 0.67 (12.1 examples/sec; 2.651 sec/batch; 56h:54m:34s remains)
INFO - root - 2017-12-07 07:50:06.192818: step 5860, loss = 0.69, batch loss = 0.62 (11.9 examples/sec; 2.698 sec/batch; 57h:54m:01s remains)
INFO - root - 2017-12-07 07:50:32.429105: step 5870, loss = 0.85, batch loss = 0.77 (12.4 examples/sec; 2.589 sec/batch; 55h:33m:37s remains)
INFO - root - 2017-12-07 07:50:58.835800: step 5880, loss = 0.74, batch loss = 0.67 (12.2 examples/sec; 2.615 sec/batch; 56h:06m:07s remains)
INFO - root - 2017-12-07 07:51:24.978791: step 5890, loss = 0.73, batch loss = 0.66 (12.3 examples/sec; 2.611 sec/batch; 56h:01m:35s remains)
INFO - root - 2017-12-07 07:51:51.350429: step 5900, loss = 0.79, batch loss = 0.72 (12.2 examples/sec; 2.622 sec/batch; 56h:14m:12s remains)
2017-12-07 07:51:52.951384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5852051 -2.2902644 -1.9388943 -1.585613 -1.3289444 -1.1859431 -1.04936 -1.003824 -1.1176729 -1.1187029 -0.71756077 -0.16324043 -0.0063266754 -0.4890461 -1.2783768][-2.7808354 -2.6510499 -2.372716 -2.0256326 -1.7019858 -1.5372772 -1.4918444 -1.5154691 -1.6032567 -1.5060954 -1.0572853 -0.51779318 -0.44467664 -1.1224308 -2.0410376][-2.7604218 -2.7313261 -2.5816283 -2.3440003 -2.0799978 -1.9800436 -1.971158 -1.9702284 -2.053859 -2.0025828 -1.7034869 -1.3199747 -1.2758749 -1.8756342 -2.6181521][-2.5238895 -2.5334344 -2.5604057 -2.5282576 -2.4051332 -2.3727682 -2.2570794 -2.0552719 -2.0535691 -2.0559909 -1.9853718 -1.9084325 -1.9733925 -2.4039636 -2.8112526][-2.5260878 -2.6113639 -2.8652306 -2.9720013 -2.7858133 -2.6210446 -2.26661 -1.8432941 -1.7964821 -1.8788555 -1.9755285 -2.0739269 -2.197469 -2.4700572 -2.5261335][-2.5145073 -2.5655067 -2.9628935 -3.1162119 -2.7610908 -2.4203417 -1.8785272 -1.3005824 -1.2512524 -1.4334831 -1.6055396 -1.6816068 -1.7398806 -1.9034412 -1.7153416][-2.3694887 -2.2955635 -2.628582 -2.6686525 -2.1809638 -1.7281184 -1.0016243 -0.22392654 -0.1266427 -0.43009353 -0.6983211 -0.74673319 -0.82222509 -1.0643747 -0.8509469][-2.0044408 -1.9357185 -2.1701014 -2.0737717 -1.5789821 -1.0756717 -0.19513416 0.70591784 0.73492956 0.24736977 -0.13911533 -0.16320133 -0.24708033 -0.55128193 -0.35373449][-1.8484881 -1.680716 -1.6612945 -1.4201636 -1.0797908 -0.7711668 -0.070480824 0.58224297 0.48346186 -0.026230812 -0.36144829 -0.29983759 -0.31492329 -0.553437 -0.28871393][-1.5270922 -1.2096472 -0.95034719 -0.5920527 -0.50308037 -0.56876111 -0.24543858 0.0082287788 -0.19498014 -0.58915639 -0.82045364 -0.70914769 -0.70481038 -0.90530324 -0.65190649][-1.0996716 -0.75513744 -0.44428349 -0.077422142 -0.18314219 -0.52344322 -0.49243855 -0.45243239 -0.6612761 -0.94628477 -1.1346433 -1.0770652 -1.1347136 -1.3453829 -1.1797247][-1.1817014 -0.85925221 -0.5745194 -0.27267122 -0.4414258 -0.84299278 -0.95265365 -1.0057638 -1.1705551 -1.345902 -1.5043299 -1.5335965 -1.673156 -1.8621097 -1.7101619][-1.7438347 -1.4361506 -1.1268206 -0.89760089 -1.0800638 -1.450151 -1.6157181 -1.6849172 -1.7850695 -1.8797669 -2.0010746 -2.064491 -2.2157137 -2.3253782 -2.162288][-2.4580319 -2.1364067 -1.7220213 -1.5332043 -1.712853 -2.0479784 -2.2246821 -2.25884 -2.2532299 -2.2538757 -2.3296688 -2.4151275 -2.5508118 -2.5934792 -2.4382503][-2.887161 -2.6247222 -2.1692336 -1.9962063 -2.1058137 -2.3454809 -2.47629 -2.4696984 -2.4137762 -2.370358 -2.3907738 -2.440063 -2.522294 -2.546273 -2.4470925]]...]
INFO - root - 2017-12-07 07:52:19.271115: step 5910, loss = 0.75, batch loss = 0.68 (12.4 examples/sec; 2.581 sec/batch; 55h:21m:55s remains)
INFO - root - 2017-12-07 07:52:45.488010: step 5920, loss = 0.73, batch loss = 0.65 (12.2 examples/sec; 2.621 sec/batch; 56h:12m:33s remains)
INFO - root - 2017-12-07 07:53:11.483078: step 5930, loss = 0.77, batch loss = 0.70 (12.3 examples/sec; 2.610 sec/batch; 55h:58m:06s remains)
INFO - root - 2017-12-07 07:53:37.755726: step 5940, loss = 0.76, batch loss = 0.68 (12.3 examples/sec; 2.601 sec/batch; 55h:45m:21s remains)
INFO - root - 2017-12-07 07:54:03.720675: step 5950, loss = 0.76, batch loss = 0.69 (12.0 examples/sec; 2.659 sec/batch; 57h:00m:41s remains)
INFO - root - 2017-12-07 07:54:29.868720: step 5960, loss = 0.77, batch loss = 0.69 (12.2 examples/sec; 2.628 sec/batch; 56h:19m:14s remains)
INFO - root - 2017-12-07 07:54:56.177796: step 5970, loss = 0.82, batch loss = 0.75 (12.2 examples/sec; 2.633 sec/batch; 56h:26m:26s remains)
INFO - root - 2017-12-07 07:55:22.394774: step 5980, loss = 0.86, batch loss = 0.78 (12.5 examples/sec; 2.563 sec/batch; 54h:55m:59s remains)
INFO - root - 2017-12-07 07:55:48.564358: step 5990, loss = 0.77, batch loss = 0.69 (12.2 examples/sec; 2.633 sec/batch; 56h:24m:40s remains)
INFO - root - 2017-12-07 07:56:14.856227: step 6000, loss = 0.75, batch loss = 0.68 (12.0 examples/sec; 2.666 sec/batch; 57h:07m:03s remains)
2017-12-07 07:56:16.407640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2551785 -1.1767542 -1.0682399 -0.94463348 -0.62378931 -0.36258745 -0.56577945 -1.2941623 -2.2840772 -2.8502607 -2.7651811 -2.3793302 -1.8568032 -1.6114106 -1.9672055][-1.4510024 -1.3668325 -1.0759759 -0.68375206 -0.29133129 -0.20182037 -0.57133842 -1.3092389 -2.2329209 -2.8616467 -2.9538975 -2.5733085 -1.9394286 -1.6108212 -1.8866956][-1.5596347 -1.5358796 -1.088155 -0.48621488 -0.11015558 -0.19622278 -0.67765069 -1.3448381 -2.1232543 -2.854353 -3.1881771 -2.8364117 -2.1412375 -1.7785208 -1.9432147][-1.5325572 -1.5488088 -1.0177126 -0.36128283 -0.094338894 -0.33752918 -0.85075831 -1.400733 -1.9778085 -2.6690297 -3.1294904 -2.83328 -2.258486 -2.0073235 -2.1229055][-1.2667658 -1.3675075 -0.9350276 -0.38447952 -0.19854784 -0.43095779 -0.83042049 -1.2203972 -1.5809515 -2.1117268 -2.5449708 -2.3709481 -2.0709596 -2.0949652 -2.2854967][-0.89225793 -1.1595266 -1.0365522 -0.73909783 -0.5773468 -0.60445166 -0.63068008 -0.70733953 -0.80539751 -1.1546953 -1.5979617 -1.6598685 -1.7183785 -2.0524771 -2.3861918][-0.65289283 -0.9998343 -1.1870461 -1.1330466 -1.080925 -0.98272061 -0.66159058 -0.45206738 -0.39459276 -0.59617305 -1.011317 -1.2369807 -1.4851465 -1.9825602 -2.4339886][-0.79536343 -1.1146326 -1.4738183 -1.5822053 -1.7445092 -1.7422299 -1.3184831 -1.0399826 -0.96860337 -1.0439758 -1.3452108 -1.5316424 -1.6512184 -2.0505497 -2.5002007][-0.87854767 -1.1006589 -1.5116663 -1.8013742 -2.3065434 -2.5631592 -2.2552059 -1.9675989 -1.8214946 -1.7230482 -1.8997681 -1.9648938 -1.8846285 -2.1372492 -2.5521104][-0.60972857 -0.7162261 -1.0453975 -1.4987032 -2.3095939 -2.886313 -2.7707443 -2.4060376 -2.0604212 -1.7735147 -1.8857605 -1.94649 -1.8611379 -2.0975277 -2.5337186][-0.60035229 -0.66139507 -0.81933045 -1.2717228 -2.0840204 -2.687134 -2.6328278 -2.1385827 -1.601717 -1.2633712 -1.4510744 -1.6566429 -1.7340784 -2.0611093 -2.5290251][-1.08706 -1.1255803 -1.0602345 -1.3334777 -1.9384661 -2.3514261 -2.185786 -1.4839828 -0.798296 -0.51944804 -0.84963083 -1.3184042 -1.7019665 -2.1745868 -2.6550832][-1.5829096 -1.6299343 -1.3637714 -1.3896394 -1.6644588 -1.8286774 -1.6257062 -0.96203446 -0.27863455 -0.06882143 -0.41175222 -1.0029924 -1.6351333 -2.2380447 -2.7538671][-1.7020853 -1.7498837 -1.4333596 -1.2959621 -1.2540956 -1.1390779 -0.93861556 -0.59657669 -0.28720713 -0.37236929 -0.67777109 -1.1204803 -1.6672463 -2.202951 -2.7310462][-1.7441506 -1.595469 -1.304745 -1.220799 -1.1128669 -0.84179473 -0.62293625 -0.56509972 -0.72118807 -1.2077239 -1.5137098 -1.7000833 -1.9496963 -2.2648294 -2.705085]]...]
INFO - root - 2017-12-07 07:56:42.712545: step 6010, loss = 0.84, batch loss = 0.76 (12.0 examples/sec; 2.662 sec/batch; 57h:01m:41s remains)
INFO - root - 2017-12-07 07:57:08.819224: step 6020, loss = 0.74, batch loss = 0.67 (11.9 examples/sec; 2.687 sec/batch; 57h:33m:06s remains)
INFO - root - 2017-12-07 07:57:35.172817: step 6030, loss = 0.71, batch loss = 0.64 (12.0 examples/sec; 2.674 sec/batch; 57h:16m:07s remains)
INFO - root - 2017-12-07 07:58:01.362026: step 6040, loss = 0.80, batch loss = 0.73 (12.1 examples/sec; 2.636 sec/batch; 56h:26m:12s remains)
INFO - root - 2017-12-07 07:58:27.492080: step 6050, loss = 0.83, batch loss = 0.76 (12.2 examples/sec; 2.633 sec/batch; 56h:22m:00s remains)
INFO - root - 2017-12-07 07:58:53.766059: step 6060, loss = 0.78, batch loss = 0.71 (12.1 examples/sec; 2.653 sec/batch; 56h:47m:57s remains)
INFO - root - 2017-12-07 07:59:20.010935: step 6070, loss = 0.73, batch loss = 0.66 (12.0 examples/sec; 2.667 sec/batch; 57h:04m:50s remains)
INFO - root - 2017-12-07 07:59:46.082957: step 6080, loss = 0.76, batch loss = 0.69 (12.2 examples/sec; 2.621 sec/batch; 56h:06m:09s remains)
INFO - root - 2017-12-07 08:00:12.287462: step 6090, loss = 0.76, batch loss = 0.69 (12.3 examples/sec; 2.612 sec/batch; 55h:53m:13s remains)
INFO - root - 2017-12-07 08:00:38.588951: step 6100, loss = 0.82, batch loss = 0.75 (12.2 examples/sec; 2.633 sec/batch; 56h:20m:06s remains)
2017-12-07 08:00:40.288008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6587677 -3.6443083 -3.6572547 -3.7227504 -3.8285604 -3.9108844 -3.9165103 -3.9073324 -3.8910186 -3.85808 -3.824456 -4.0443683 -4.5352254 -4.9443064 -5.1373835][-3.53864 -3.4552233 -3.4018815 -3.4655375 -3.63534 -3.7760494 -3.7893918 -3.6830559 -3.4746242 -3.2577138 -3.0803976 -3.2633491 -3.8021104 -4.3283839 -4.6695576][-3.3656945 -3.2000837 -3.0172977 -3.0235062 -3.2248015 -3.4127676 -3.440249 -3.221101 -2.7930918 -2.3776271 -2.0674725 -2.1957581 -2.7415807 -3.38209 -3.9715378][-3.1411748 -2.7848649 -2.3818476 -2.3263118 -2.5637975 -2.7433193 -2.7767482 -2.5394759 -2.1501544 -1.8143573 -1.5523224 -1.6249831 -2.0477076 -2.6341996 -3.3585854][-2.8381181 -2.2627969 -1.7300613 -1.7851696 -2.1715295 -2.3032522 -2.146131 -1.7485847 -1.4909821 -1.5541394 -1.7154617 -1.9573691 -2.2302742 -2.5610008 -3.0861616][-2.6324072 -1.8455534 -1.1788294 -1.2015111 -1.5462601 -1.516742 -1.0583405 -0.41660833 -0.20705795 -0.62360644 -1.3904982 -2.1478171 -2.6475089 -2.9237871 -3.2160258][-2.7524962 -1.8547158 -1.0314693 -0.76149011 -0.78009343 -0.57447362 0.0070738792 0.79603958 1.1973152 0.96750259 0.053908825 -1.0415666 -1.8758368 -2.4441442 -2.8899112][-2.9814045 -2.1779873 -1.4498267 -1.0432434 -0.77813077 -0.40832996 0.24329424 1.1071916 1.6593022 1.6462345 0.96918249 0.091249466 -0.65905833 -1.3346508 -1.9582479][-3.1135821 -2.3590715 -1.7537148 -1.4096642 -1.0597577 -0.48210716 0.34653521 1.1521344 1.3835535 0.9953928 0.19676828 -0.48241663 -0.82299876 -1.0093009 -1.3673956][-3.2441707 -2.5797634 -2.1350822 -1.9283245 -1.5572646 -0.80116487 0.18939209 1.0434914 1.2572808 0.79778862 -0.1381526 -0.9512248 -1.3424439 -1.3885753 -1.6317384][-3.5315857 -3.1182594 -2.9047179 -2.822032 -2.497267 -1.8055613 -0.97597671 -0.28285503 0.045553684 -0.018871307 -0.49355769 -1.0789981 -1.455936 -1.5985949 -1.9299254][-3.8973191 -3.6815717 -3.5236449 -3.4157751 -3.1631069 -2.7263975 -2.2807937 -1.9999089 -1.895154 -1.9201343 -2.0860314 -2.3083642 -2.4304917 -2.4908128 -2.7307959][-3.9599595 -3.856261 -3.6858826 -3.512578 -3.2876544 -3.0047412 -2.7403574 -2.5738688 -2.5297947 -2.5922585 -2.7540374 -2.9910398 -3.2590902 -3.5212567 -3.7641652][-3.785784 -3.7360277 -3.6403534 -3.5424843 -3.4350362 -3.2848449 -3.0973425 -2.9106183 -2.8028796 -2.829267 -2.9720073 -3.2090769 -3.5215707 -3.8498197 -4.0732512][-3.6682553 -3.6765409 -3.6947608 -3.7416217 -3.7995968 -3.8078413 -3.7495241 -3.6393468 -3.5684114 -3.6025674 -3.6994877 -3.8054621 -3.886966 -3.9605355 -3.9933083]]...]
INFO - root - 2017-12-07 08:01:06.404118: step 6110, loss = 0.79, batch loss = 0.72 (12.0 examples/sec; 2.672 sec/batch; 57h:09m:53s remains)
INFO - root - 2017-12-07 08:01:32.566005: step 6120, loss = 0.79, batch loss = 0.72 (12.2 examples/sec; 2.629 sec/batch; 56h:14m:19s remains)
INFO - root - 2017-12-07 08:01:58.720572: step 6130, loss = 0.66, batch loss = 0.58 (12.4 examples/sec; 2.584 sec/batch; 55h:16m:22s remains)
INFO - root - 2017-12-07 08:02:24.947480: step 6140, loss = 0.75, batch loss = 0.68 (12.2 examples/sec; 2.628 sec/batch; 56h:11m:53s remains)
INFO - root - 2017-12-07 08:02:51.155328: step 6150, loss = 0.80, batch loss = 0.72 (12.4 examples/sec; 2.578 sec/batch; 55h:07m:36s remains)
INFO - root - 2017-12-07 08:03:17.263189: step 6160, loss = 0.77, batch loss = 0.70 (12.4 examples/sec; 2.573 sec/batch; 55h:00m:08s remains)
INFO - root - 2017-12-07 08:03:43.696600: step 6170, loss = 0.77, batch loss = 0.70 (12.0 examples/sec; 2.674 sec/batch; 57h:09m:23s remains)
INFO - root - 2017-12-07 08:04:09.627702: step 6180, loss = 0.77, batch loss = 0.70 (12.1 examples/sec; 2.654 sec/batch; 56h:43m:44s remains)
INFO - root - 2017-12-07 08:04:35.852887: step 6190, loss = 0.77, batch loss = 0.70 (12.3 examples/sec; 2.607 sec/batch; 55h:43m:00s remains)
INFO - root - 2017-12-07 08:05:02.131287: step 6200, loss = 0.79, batch loss = 0.72 (11.9 examples/sec; 2.684 sec/batch; 57h:21m:38s remains)
2017-12-07 08:05:03.797717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7181463 -1.9567063 -2.1796243 -2.2977576 -2.5643797 -1.9649091 -0.22690821 1.3273687 1.8457479 1.7735252 1.230505 0.52898693 -0.21451664 -1.0624394 -1.4001203][-2.0773742 -2.2986338 -2.4673655 -2.4407094 -2.495275 -1.6949396 0.32970238 2.0613842 2.293879 1.7133808 0.78428507 -0.0909276 -0.86968827 -1.626528 -1.7221794][-2.4195271 -2.6074276 -2.7418911 -2.6303532 -2.4743304 -1.439502 0.80151844 2.5416093 2.3646808 1.3633938 0.24798012 -0.73880982 -1.5383034 -2.1721957 -2.1229324][-2.4713259 -2.4929838 -2.5856757 -2.5622966 -2.4693503 -1.3624721 1.0362487 2.7308087 2.1593142 0.95960093 -0.085613251 -1.0708473 -1.8705921 -2.4462962 -2.4685116][-2.4048204 -2.11193 -1.9763937 -1.9921184 -2.0615575 -0.99067426 1.3675671 2.9187975 2.1006408 0.90317678 -0.065619469 -1.0374811 -1.796232 -2.2803323 -2.391067][-2.4809661 -1.8815715 -1.360652 -1.1833773 -1.2249775 -0.21272135 1.8507314 3.0556054 2.1350884 1.0379357 0.10976648 -0.93909 -1.6490555 -1.9755623 -2.0763488][-2.3941259 -1.6241117 -0.79379225 -0.24106741 -0.0047483444 0.95367622 2.4715347 3.145823 2.1713657 1.1764655 0.2936101 -0.88306165 -1.6132512 -1.8076658 -1.8595672][-1.8868947 -1.0250597 -0.17529392 0.62428093 1.1706085 2.1609664 3.2801533 3.6068192 2.5285625 1.3677783 0.356287 -1.0066471 -1.878288 -2.0495462 -2.1371281][-1.4073687 -0.43297863 0.19290495 0.84115553 1.3663421 2.2184319 3.1378841 3.4777541 2.4259067 1.0511651 -0.11725855 -1.4933743 -2.3052771 -2.3677249 -2.4363256][-1.7193131 -0.65393448 -0.12542772 0.3967104 0.74509907 1.1709547 1.7890244 2.2387233 1.4103532 0.11091805 -0.90986037 -2.0140698 -2.5684104 -2.4236245 -2.3813396][-2.3263729 -1.4566038 -0.96840453 -0.3753252 0.0084958076 0.15934563 0.45625353 0.78980637 0.038486958 -1.1552703 -1.9611304 -2.7507977 -3.0397658 -2.6725335 -2.4180169][-2.346981 -1.8858349 -1.617574 -1.0671692 -0.6159718 -0.52628493 -0.45891309 -0.4030652 -1.2076702 -2.2505672 -2.8691416 -3.3774188 -3.3844259 -2.8030372 -2.3235097][-1.8254411 -1.6202466 -1.5653903 -1.2237175 -0.90813136 -0.85455465 -0.92400455 -1.1017654 -1.8420541 -2.6618 -3.139492 -3.4513156 -3.2507989 -2.5707951 -1.9666452][-1.3780713 -1.275717 -1.3029902 -1.1947737 -1.1479578 -1.2436984 -1.4828362 -1.8293755 -2.3693352 -2.8333297 -3.0823493 -3.1603057 -2.8019559 -2.1409743 -1.5362794][-1.1424558 -1.0478313 -1.027494 -1.0677495 -1.2654889 -1.5351009 -1.9275897 -2.390996 -2.7494392 -2.8722959 -2.8260636 -2.6580393 -2.2415712 -1.7390106 -1.3121486]]...]
INFO - root - 2017-12-07 08:05:29.938917: step 6210, loss = 0.77, batch loss = 0.70 (11.8 examples/sec; 2.703 sec/batch; 57h:44m:48s remains)
INFO - root - 2017-12-07 08:05:56.108537: step 6220, loss = 0.69, batch loss = 0.62 (12.2 examples/sec; 2.624 sec/batch; 56h:03m:04s remains)
INFO - root - 2017-12-07 08:06:22.134615: step 6230, loss = 0.81, batch loss = 0.74 (12.0 examples/sec; 2.656 sec/batch; 56h:44m:30s remains)
INFO - root - 2017-12-07 08:06:48.205353: step 6240, loss = 0.88, batch loss = 0.81 (12.3 examples/sec; 2.608 sec/batch; 55h:41m:48s remains)
INFO - root - 2017-12-07 08:07:14.425441: step 6250, loss = 0.82, batch loss = 0.75 (11.9 examples/sec; 2.689 sec/batch; 57h:25m:10s remains)
INFO - root - 2017-12-07 08:07:40.594750: step 6260, loss = 0.75, batch loss = 0.67 (12.2 examples/sec; 2.618 sec/batch; 55h:53m:45s remains)
INFO - root - 2017-12-07 08:08:06.762078: step 6270, loss = 0.71, batch loss = 0.64 (12.2 examples/sec; 2.631 sec/batch; 56h:10m:35s remains)
INFO - root - 2017-12-07 08:08:33.110935: step 6280, loss = 0.93, batch loss = 0.86 (12.4 examples/sec; 2.588 sec/batch; 55h:14m:32s remains)
INFO - root - 2017-12-07 08:08:59.336108: step 6290, loss = 0.75, batch loss = 0.68 (12.3 examples/sec; 2.605 sec/batch; 55h:36m:22s remains)
INFO - root - 2017-12-07 08:09:25.198614: step 6300, loss = 0.71, batch loss = 0.63 (12.4 examples/sec; 2.581 sec/batch; 55h:04m:49s remains)
2017-12-07 08:09:26.832010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8138416 -3.006547 -3.035284 -2.9158504 -2.8335876 -2.8167434 -2.5724547 -1.7484388 -0.99716187 -1.2710471 -2.037411 -2.7112184 -3.0038624 -2.6461296 -2.1494312][-3.0972214 -3.1490448 -3.0638266 -2.8490319 -2.7094545 -2.648591 -2.368032 -1.5480523 -0.86987281 -1.3071566 -2.1608381 -2.8172531 -3.1133912 -2.7528496 -2.2295244][-3.1893435 -3.1331882 -2.9832754 -2.724668 -2.5345328 -2.4211752 -2.1277151 -1.3642399 -0.78256178 -1.3486724 -2.2470384 -2.8582072 -3.1422071 -2.7962589 -2.2674644][-3.1906042 -3.0876517 -2.9177079 -2.6411772 -2.4082465 -2.2344356 -1.9249718 -1.2167807 -0.72920537 -1.3837693 -2.3046706 -2.878046 -3.1363106 -2.7997861 -2.2767413][-3.2402804 -3.1085343 -2.8666558 -2.5346775 -2.2645018 -2.0491731 -1.7098367 -1.0522947 -0.68965125 -1.4233735 -2.3451843 -2.8829644 -3.0975111 -2.7576692 -2.2563138][-3.2841182 -3.1879156 -2.9194343 -2.5646181 -2.2385366 -1.9399734 -1.4919057 -0.82301259 -0.58555245 -1.4214573 -2.3827741 -2.8990226 -3.0509648 -2.6997294 -2.2268565][-3.3084674 -3.2684693 -3.0304914 -2.706964 -2.3266866 -1.8873858 -1.2733381 -0.56098461 -0.44708943 -1.3803537 -2.4054937 -2.9236779 -3.0208464 -2.6585088 -2.2071106][-3.3724229 -3.3270993 -3.1160927 -2.8435473 -2.4443746 -1.8959403 -1.1466863 -0.42289114 -0.39801311 -1.3555515 -2.4026828 -2.928117 -2.9958239 -2.6278763 -2.1916838][-3.2983327 -3.2861366 -3.1366127 -2.9430308 -2.6009541 -2.0362184 -1.2208343 -0.48238206 -0.44928026 -1.3341975 -2.3584163 -2.8982382 -2.9571896 -2.5931721 -2.173702][-3.2137656 -3.2579141 -3.1959639 -3.1110189 -2.8780932 -2.350178 -1.5123475 -0.72523165 -0.58674908 -1.3339012 -2.3058724 -2.8553019 -2.91031 -2.5492957 -2.1456106][-3.2893014 -3.33423 -3.29454 -3.2618628 -3.1190677 -2.6724424 -1.8889186 -1.0974596 -0.87173915 -1.4728541 -2.3556435 -2.8654308 -2.8907359 -2.5204268 -2.1245482][-3.2609081 -3.2684102 -3.2141614 -3.2187486 -3.1827993 -2.8577318 -2.1722407 -1.428596 -1.1797667 -1.6860344 -2.4774833 -2.92638 -2.9097915 -2.5171585 -2.1220765][-3.1718495 -3.1629033 -3.0895104 -3.1296344 -3.1934428 -2.9622822 -2.3315701 -1.6261461 -1.3730116 -1.8105683 -2.5236003 -2.9325545 -2.9056923 -2.5117841 -2.1249466][-3.0957503 -3.085083 -2.9980063 -3.0598578 -3.2019534 -3.0443313 -2.447485 -1.752526 -1.4603376 -1.8341658 -2.4786739 -2.8621268 -2.8514 -2.4842339 -2.1212447][-3.0545931 -3.0432346 -2.9408298 -2.9880033 -3.1660943 -3.0739241 -2.5317578 -1.8336623 -1.4648626 -1.7917223 -2.3895671 -2.7584586 -2.7742205 -2.4434998 -2.1087852]]...]
INFO - root - 2017-12-07 08:09:53.246464: step 6310, loss = 0.76, batch loss = 0.69 (12.1 examples/sec; 2.634 sec/batch; 56h:12m:04s remains)
INFO - root - 2017-12-07 08:10:19.525708: step 6320, loss = 0.73, batch loss = 0.66 (12.2 examples/sec; 2.630 sec/batch; 56h:07m:10s remains)
INFO - root - 2017-12-07 08:10:45.633276: step 6330, loss = 0.83, batch loss = 0.76 (13.5 examples/sec; 2.376 sec/batch; 50h:41m:14s remains)
INFO - root - 2017-12-07 08:11:11.858813: step 6340, loss = 0.85, batch loss = 0.77 (12.2 examples/sec; 2.614 sec/batch; 55h:45m:00s remains)
INFO - root - 2017-12-07 08:11:38.122517: step 6350, loss = 0.77, batch loss = 0.70 (12.2 examples/sec; 2.616 sec/batch; 55h:47m:44s remains)
INFO - root - 2017-12-07 08:12:04.381284: step 6360, loss = 0.76, batch loss = 0.68 (12.3 examples/sec; 2.600 sec/batch; 55h:25m:54s remains)
INFO - root - 2017-12-07 08:12:30.484751: step 6370, loss = 0.75, batch loss = 0.68 (12.1 examples/sec; 2.655 sec/batch; 56h:36m:26s remains)
INFO - root - 2017-12-07 08:12:56.769141: step 6380, loss = 0.73, batch loss = 0.66 (11.9 examples/sec; 2.697 sec/batch; 57h:30m:05s remains)
INFO - root - 2017-12-07 08:13:23.068832: step 6390, loss = 0.77, batch loss = 0.70 (12.2 examples/sec; 2.612 sec/batch; 55h:41m:00s remains)
INFO - root - 2017-12-07 08:13:49.077858: step 6400, loss = 0.73, batch loss = 0.65 (12.2 examples/sec; 2.624 sec/batch; 55h:55m:53s remains)
2017-12-07 08:13:50.753595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1568711 -1.640867 -2.1739266 -2.515846 -2.5088072 -2.3433702 -2.2574608 -2.3748353 -2.3630667 -2.1473081 -1.9602876 -1.8581433 -1.7247076 -1.4715352 -1.2509098][-1.0865984 -1.4833682 -1.8968675 -2.1948998 -2.2875569 -2.3188972 -2.4226925 -2.6790221 -2.7118366 -2.4817493 -2.2528031 -2.0691421 -1.8971972 -1.6770258 -1.4798155][-1.2102888 -1.5157015 -1.7697794 -1.9929206 -2.1523538 -2.2802596 -2.4818463 -2.7887328 -2.7740107 -2.4833186 -2.2815793 -2.1856225 -2.1227698 -1.9835565 -1.8117597][-1.2530909 -1.5778425 -1.7891119 -2.0213857 -2.2641182 -2.440105 -2.6309624 -2.8330331 -2.6101613 -2.1897953 -2.082155 -2.2246137 -2.3651118 -2.318687 -2.1617038][-1.2831457 -1.6827588 -1.8869019 -2.0466297 -2.2499866 -2.4064364 -2.5744996 -2.6690898 -2.290072 -1.7987812 -1.8183856 -2.2043605 -2.5198913 -2.506382 -2.3265722][-1.3490276 -1.7117863 -1.848536 -1.7973144 -1.7643323 -1.7814467 -1.8980386 -1.9636576 -1.6689627 -1.3268118 -1.4821475 -1.9963465 -2.4101808 -2.4603765 -2.2974288][-1.4606569 -1.6390419 -1.6516864 -1.4097998 -1.1298726 -0.94584918 -0.9169085 -0.89887547 -0.77874589 -0.69452071 -0.94582796 -1.4776406 -1.9210503 -2.1113634 -2.0822537][-1.844013 -1.8592021 -1.7054267 -1.2754958 -0.78758168 -0.46672392 -0.36185884 -0.26104736 -0.22519684 -0.25112724 -0.45492554 -0.9275682 -1.4016562 -1.7415097 -1.8670504][-2.3421807 -2.3117938 -2.0248539 -1.4498837 -0.84050727 -0.46662331 -0.37017488 -0.24686909 -0.20976543 -0.19861174 -0.28017712 -0.64936638 -1.1365919 -1.5772595 -1.7676117][-2.6881394 -2.7273321 -2.4250953 -1.8819907 -1.2934537 -0.91351628 -0.78407359 -0.63050961 -0.56632972 -0.5327599 -0.54226851 -0.79063153 -1.1729944 -1.5728748 -1.7375724][-2.8316398 -3.0095983 -2.8429954 -2.4707599 -1.9857867 -1.6008654 -1.3888915 -1.146136 -1.0138025 -0.9745307 -0.99982119 -1.1573217 -1.3886411 -1.6719248 -1.7978497][-2.6390462 -2.916434 -2.9323099 -2.7957046 -2.5071802 -2.194262 -1.9448934 -1.641782 -1.4273849 -1.3544261 -1.3843908 -1.469533 -1.5942445 -1.76705 -1.8593664][-2.3209569 -2.5562272 -2.6089234 -2.564363 -2.4365942 -2.2717581 -2.1100979 -1.8491962 -1.6095784 -1.5051684 -1.5177555 -1.5786731 -1.6726739 -1.7926385 -1.8461819][-2.1041696 -2.2718956 -2.2836719 -2.2166414 -2.1429636 -2.0926108 -2.0425735 -1.8931217 -1.7104359 -1.5983086 -1.5767801 -1.6157258 -1.6959913 -1.7749026 -1.7848446][-1.9928257 -2.0967906 -2.0958614 -2.0407097 -1.9933641 -1.9714742 -1.9473114 -1.8631222 -1.7389221 -1.6343565 -1.5894685 -1.612159 -1.6828899 -1.7413101 -1.7466161]]...]
INFO - root - 2017-12-07 08:14:17.102965: step 6410, loss = 0.82, batch loss = 0.75 (12.5 examples/sec; 2.557 sec/batch; 54h:29m:16s remains)
INFO - root - 2017-12-07 08:14:43.502265: step 6420, loss = 0.78, batch loss = 0.71 (12.0 examples/sec; 2.665 sec/batch; 56h:46m:31s remains)
INFO - root - 2017-12-07 08:15:09.678422: step 6430, loss = 0.72, batch loss = 0.65 (12.1 examples/sec; 2.641 sec/batch; 56h:15m:43s remains)
INFO - root - 2017-12-07 08:15:35.953962: step 6440, loss = 0.78, batch loss = 0.71 (12.2 examples/sec; 2.619 sec/batch; 55h:46m:48s remains)
INFO - root - 2017-12-07 08:16:02.185947: step 6450, loss = 0.95, batch loss = 0.88 (12.2 examples/sec; 2.624 sec/batch; 55h:53m:20s remains)
INFO - root - 2017-12-07 08:16:28.217678: step 6460, loss = 0.85, batch loss = 0.77 (12.1 examples/sec; 2.644 sec/batch; 56h:18m:54s remains)
INFO - root - 2017-12-07 08:16:54.436899: step 6470, loss = 0.81, batch loss = 0.74 (12.3 examples/sec; 2.608 sec/batch; 55h:32m:12s remains)
INFO - root - 2017-12-07 08:17:20.627921: step 6480, loss = 0.80, batch loss = 0.72 (12.2 examples/sec; 2.622 sec/batch; 55h:49m:20s remains)
INFO - root - 2017-12-07 08:17:46.633792: step 6490, loss = 0.77, batch loss = 0.70 (13.2 examples/sec; 2.429 sec/batch; 51h:43m:03s remains)
INFO - root - 2017-12-07 08:18:12.976876: step 6500, loss = 0.77, batch loss = 0.70 (12.1 examples/sec; 2.639 sec/batch; 56h:10m:21s remains)
2017-12-07 08:18:14.576035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1695445 -2.4953284 -3.1940413 -4.0251336 -4.1760354 -3.5193903 -2.8615103 -2.9399426 -3.5886607 -4.2386117 -4.6269336 -4.779222 -4.8941627 -5.0411038 -5.1024671][-2.0791829 -2.3574727 -3.0928059 -3.8671465 -3.8283553 -3.142714 -2.6884551 -2.9842505 -3.5480747 -3.8957562 -4.1364789 -4.2886033 -4.4012728 -4.5819144 -4.6918507][-1.9426355 -2.213217 -2.887 -3.4580939 -3.2790504 -2.7917788 -2.7370434 -3.2359984 -3.5732312 -3.5316453 -3.5784755 -3.7045059 -3.796742 -4.0066414 -4.2801709][-1.859525 -2.1432612 -2.7513862 -3.1214728 -2.8758569 -2.5906219 -2.8465691 -3.383997 -3.4695807 -3.1909075 -3.1216025 -3.1941342 -3.2094932 -3.2959251 -3.6109202][-1.891695 -2.1751978 -2.7080965 -2.9273305 -2.6524048 -2.4765697 -2.831893 -3.2255201 -3.0344846 -2.6529672 -2.5587573 -2.5748699 -2.5144382 -2.4622612 -2.7937965][-1.7518799 -1.9495697 -2.4346747 -2.661165 -2.4928656 -2.4503014 -2.8118091 -2.9961677 -2.5810695 -2.1896663 -2.1725848 -2.2401433 -2.1991723 -2.1006761 -2.4335008][-1.479229 -1.5222793 -1.9496903 -2.1925852 -2.1215045 -2.1955106 -2.6513135 -2.7882209 -2.2540634 -1.792501 -1.831948 -2.0492086 -2.1226776 -2.0377059 -2.3629651][-1.3183453 -1.2840679 -1.6623201 -1.8052487 -1.6656756 -1.7918761 -2.4093213 -2.6483741 -2.1249549 -1.6030591 -1.6468816 -1.9586406 -2.0068221 -1.8325958 -2.1016495][-1.3136556 -1.3162322 -1.6965017 -1.7457712 -1.480324 -1.5834851 -2.2376022 -2.4780571 -1.9771056 -1.4854999 -1.6280036 -2.0705392 -2.053493 -1.7826169 -1.9575803][-1.3189487 -1.4662273 -1.9447665 -2.0352249 -1.7895379 -1.8945613 -2.4705048 -2.5444617 -1.9686608 -1.5208812 -1.8006413 -2.3765218 -2.3698559 -2.1034446 -2.1487436][-1.1846344 -1.5559909 -2.2051194 -2.4203339 -2.2998707 -2.4504583 -2.9057651 -2.784838 -2.1258006 -1.697633 -1.9913416 -2.5216861 -2.5396206 -2.3707941 -2.3144839][-1.0848706 -1.6697414 -2.4849477 -2.7992308 -2.7735832 -2.9191294 -3.1807046 -2.9128404 -2.2923241 -1.9478185 -2.1503782 -2.4620428 -2.4702444 -2.4659266 -2.4432316][-1.4319901 -2.1289532 -2.9865932 -3.2914267 -3.2559485 -3.3026352 -3.3351221 -3.0199707 -2.5714903 -2.3579149 -2.4517276 -2.5214808 -2.4791174 -2.5792851 -2.6400704][-2.2176049 -2.7989211 -3.4899819 -3.6603339 -3.5417895 -3.4733183 -3.3699746 -3.1061211 -2.8625731 -2.7818763 -2.8510714 -2.8314829 -2.7902915 -2.8967423 -2.9337471][-2.9715869 -3.3611176 -3.7653539 -3.7750072 -3.6142554 -3.4922571 -3.3515725 -3.1494565 -3.0113959 -2.9931324 -3.0650818 -3.0769467 -3.0587494 -3.0831072 -3.005214]]...]
INFO - root - 2017-12-07 08:18:40.805561: step 6510, loss = 0.81, batch loss = 0.73 (11.9 examples/sec; 2.686 sec/batch; 57h:10m:00s remains)
INFO - root - 2017-12-07 08:19:07.023366: step 6520, loss = 0.82, batch loss = 0.75 (11.9 examples/sec; 2.682 sec/batch; 57h:04m:02s remains)
INFO - root - 2017-12-07 08:19:33.117167: step 6530, loss = 0.87, batch loss = 0.80 (12.2 examples/sec; 2.632 sec/batch; 56h:00m:12s remains)
INFO - root - 2017-12-07 08:19:59.410513: step 6540, loss = 0.69, batch loss = 0.62 (12.3 examples/sec; 2.601 sec/batch; 55h:19m:22s remains)
INFO - root - 2017-12-07 08:20:25.608971: step 6550, loss = 0.70, batch loss = 0.63 (12.2 examples/sec; 2.616 sec/batch; 55h:38m:49s remains)
INFO - root - 2017-12-07 08:20:51.632950: step 6560, loss = 0.71, batch loss = 0.64 (12.4 examples/sec; 2.590 sec/batch; 55h:04m:58s remains)
INFO - root - 2017-12-07 08:21:17.796378: step 6570, loss = 0.79, batch loss = 0.71 (12.4 examples/sec; 2.578 sec/batch; 54h:49m:29s remains)
INFO - root - 2017-12-07 08:21:43.986791: step 6580, loss = 0.77, batch loss = 0.70 (12.2 examples/sec; 2.613 sec/batch; 55h:33m:40s remains)
INFO - root - 2017-12-07 08:22:10.019081: step 6590, loss = 0.69, batch loss = 0.62 (12.1 examples/sec; 2.654 sec/batch; 56h:25m:46s remains)
INFO - root - 2017-12-07 08:22:36.275434: step 6600, loss = 0.81, batch loss = 0.73 (12.1 examples/sec; 2.641 sec/batch; 56h:07m:59s remains)
2017-12-07 08:22:37.826997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.977246 -1.981766 -1.9869816 -1.9965079 -2.0108132 -2.021076 -2.0212898 -2.0167165 -2.005594 -1.9906561 -1.9714754 -1.9489207 -1.9300387 -1.9235091 -1.9191742][-1.9465275 -1.9455791 -1.9485047 -1.9558856 -1.9657927 -1.9691212 -1.962091 -1.9541986 -1.9485178 -1.9435999 -1.9336805 -1.9173865 -1.900404 -1.8949006 -1.8933592][-1.8205888 -1.8116863 -1.8089945 -1.8119833 -1.8168254 -1.814409 -1.8054297 -1.8049908 -1.8207405 -1.8434227 -1.8616579 -1.8645744 -1.8532178 -1.8393557 -1.8179169][-1.7145343 -1.7032027 -1.6963983 -1.6941972 -1.6937869 -1.6891537 -1.6840818 -1.6969948 -1.7402112 -1.798382 -1.855593 -1.8864884 -1.882057 -1.8517439 -1.7954044][-1.6946101 -1.6809654 -1.6656382 -1.652838 -1.6445794 -1.6390078 -1.6384323 -1.6605985 -1.7195213 -1.8007219 -1.8926487 -1.9609916 -1.9809139 -1.9516795 -1.87149][-1.7008719 -1.6849866 -1.6610861 -1.6394553 -1.6258123 -1.6250627 -1.6385119 -1.6805563 -1.7581885 -1.8598704 -1.9801593 -2.0896218 -2.1489973 -2.1405554 -2.0519781][-1.6879404 -1.6697674 -1.6419399 -1.6183817 -1.6098015 -1.6277258 -1.6731184 -1.7561002 -1.8689914 -2.0014164 -2.1503594 -2.3010409 -2.405468 -2.4283745 -2.3425338][-1.6646411 -1.6453452 -1.6144371 -1.5898705 -1.5915413 -1.6350431 -1.7132666 -1.8311954 -1.9778776 -2.1430871 -2.3240252 -2.5165594 -2.676033 -2.746685 -2.6818552][-1.6424325 -1.6225693 -1.5854862 -1.5548611 -1.5590754 -1.616194 -1.709301 -1.8395882 -2.0016174 -2.1903033 -2.3993104 -2.6317432 -2.8523812 -2.9880354 -2.9682648][-1.6347208 -1.6122577 -1.5673444 -1.5277 -1.5286086 -1.5890272 -1.6930418 -1.8353407 -2.0188494 -2.2447147 -2.4945269 -2.765825 -3.0215697 -3.1911426 -3.20191][-1.6434534 -1.6114929 -1.5593441 -1.5146809 -1.522223 -1.6064565 -1.75652 -1.9512908 -2.1901603 -2.4769325 -2.7742405 -3.0589223 -3.2852111 -3.4259558 -3.4463155][-1.6687288 -1.6262 -1.5715861 -1.5340984 -1.5686953 -1.7051079 -1.9340832 -2.2036016 -2.5002275 -2.8239379 -3.1273582 -3.3773599 -3.5291264 -3.6105657 -3.6379633][-1.6935081 -1.6417444 -1.5886965 -1.5690193 -1.6380644 -1.8238361 -2.1048939 -2.4083421 -2.7134547 -3.0228145 -3.2909155 -3.4814923 -3.5605321 -3.5856171 -3.6142204][-1.7005622 -1.6416292 -1.5889113 -1.5810354 -1.6650333 -1.8584466 -2.1279364 -2.3980486 -2.6528025 -2.9021935 -3.1110618 -3.2396371 -3.2608171 -3.2376401 -3.2484684][-1.6802785 -1.6222835 -1.5725939 -1.5661917 -1.6404855 -1.7990606 -2.004678 -2.1967616 -2.3684487 -2.538357 -2.6805067 -2.7522359 -2.7357092 -2.6841416 -2.6738896]]...]
INFO - root - 2017-12-07 08:23:03.993883: step 6610, loss = 0.75, batch loss = 0.68 (12.2 examples/sec; 2.634 sec/batch; 55h:58m:32s remains)
INFO - root - 2017-12-07 08:23:29.821377: step 6620, loss = 0.74, batch loss = 0.66 (12.2 examples/sec; 2.613 sec/batch; 55h:32m:23s remains)
INFO - root - 2017-12-07 08:23:56.132625: step 6630, loss = 0.76, batch loss = 0.68 (11.9 examples/sec; 2.688 sec/batch; 57h:07m:25s remains)
INFO - root - 2017-12-07 08:24:22.158380: step 6640, loss = 0.74, batch loss = 0.66 (12.2 examples/sec; 2.633 sec/batch; 55h:56m:57s remains)
INFO - root - 2017-12-07 08:24:48.272130: step 6650, loss = 0.82, batch loss = 0.75 (12.1 examples/sec; 2.648 sec/batch; 56h:15m:35s remains)
INFO - root - 2017-12-07 08:25:14.704066: step 6660, loss = 0.75, batch loss = 0.68 (11.8 examples/sec; 2.705 sec/batch; 57h:27m:21s remains)
INFO - root - 2017-12-07 08:25:40.988922: step 6670, loss = 0.70, batch loss = 0.63 (12.2 examples/sec; 2.615 sec/batch; 55h:32m:35s remains)
INFO - root - 2017-12-07 08:26:07.234778: step 6680, loss = 0.79, batch loss = 0.72 (12.2 examples/sec; 2.616 sec/batch; 55h:33m:36s remains)
INFO - root - 2017-12-07 08:26:33.253234: step 6690, loss = 0.78, batch loss = 0.71 (12.0 examples/sec; 2.667 sec/batch; 56h:37m:08s remains)
INFO - root - 2017-12-07 08:26:59.468778: step 6700, loss = 0.72, batch loss = 0.64 (12.3 examples/sec; 2.597 sec/batch; 55h:08m:21s remains)
2017-12-07 08:27:01.036648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6707149 -2.4598699 -1.8550661 -1.6213739 -1.7396691 -2.357389 -3.145304 -3.519732 -3.3671608 -2.9114203 -2.770277 -3.13662 -3.8540277 -4.4393082 -4.5017376][-2.4366202 -2.1638567 -1.4972024 -1.3930652 -1.6745074 -2.4182608 -3.1527739 -3.3673029 -2.9729471 -2.2002146 -1.8462417 -2.2050591 -3.1138694 -3.9365191 -4.1003704][-2.0154557 -1.618782 -1.0176926 -1.1216168 -1.5447059 -2.3436608 -2.9578705 -3.0089474 -2.4552832 -1.4799137 -1.0092413 -1.318979 -2.2540514 -3.2700458 -3.5952704][-1.9443285 -1.3759787 -0.6455493 -0.5663712 -0.7827096 -1.493576 -2.0483274 -2.0950687 -1.6136765 -0.72745824 -0.48524737 -0.8671813 -1.6601324 -2.739819 -3.2086935][-2.0910604 -1.5670991 -0.77533364 -0.34176779 -0.071784019 -0.42107725 -0.69561863 -0.711303 -0.47255135 0.12187672 0.031493187 -0.45149994 -1.0911212 -2.3494534 -3.1385212][-2.26495 -1.963351 -1.2167337 -0.48140264 0.22176456 0.24909639 0.24895716 0.18501759 0.098958015 0.46366978 0.30982494 -0.0091671944 -0.4572947 -1.963217 -3.1029491][-2.1809673 -2.1434505 -1.3620603 -0.18346882 0.89998817 0.99995089 0.87353945 0.53023005 0.12743616 0.376544 0.3107481 0.19392204 -0.12510967 -1.6811366 -2.8138003][-1.7323501 -1.7702391 -0.90891838 0.63866806 1.9312773 1.8688107 1.5123405 0.95149422 0.34686375 0.41370821 0.3082633 0.24218082 0.0025401115 -1.4239435 -2.3601029][-1.647759 -1.6083066 -0.85594726 0.59220934 1.7242889 1.5647035 1.2910924 0.78354883 0.099932194 -0.095952511 -0.34476709 -0.38558769 -0.44660425 -1.5557482 -2.2790432][-1.905515 -1.84812 -1.3367233 -0.174016 0.68159151 0.61195993 0.49480247 -0.033588409 -0.78117466 -1.0796945 -1.3094079 -1.2742643 -1.1562545 -1.8702791 -2.3779211][-2.3004138 -2.5829473 -2.4395318 -1.4854474 -0.64609051 -0.39982319 -0.41625452 -1.0919313 -1.8396583 -2.0580013 -2.13408 -2.0157621 -1.9009387 -2.3607409 -2.6560555][-2.2950854 -3.0703425 -3.3342233 -2.7096875 -1.9991963 -1.4594607 -1.3605716 -2.0251031 -2.6405163 -2.7753172 -2.6599998 -2.415477 -2.3775408 -2.714993 -2.8651817][-1.9549046 -2.7592134 -3.0781398 -2.7075825 -2.1551208 -1.4622707 -1.3253224 -1.8663511 -2.3307152 -2.4384098 -2.1460721 -1.7625053 -1.8352308 -2.182029 -2.3434939][-1.8949161 -2.3908818 -2.4935217 -2.2328494 -1.6965365 -0.95409989 -0.8919909 -1.3780398 -1.8129969 -1.9839098 -1.5746205 -1.072679 -1.185111 -1.5104373 -1.7113659][-2.2811286 -2.4402013 -2.4116812 -2.3764732 -1.9484935 -1.3042562 -1.2420835 -1.5551214 -1.9437318 -2.2272713 -1.8079836 -1.3160369 -1.4499979 -1.7338219 -1.9095891]]...]
INFO - root - 2017-12-07 08:27:27.292666: step 6710, loss = 0.81, batch loss = 0.73 (12.1 examples/sec; 2.654 sec/batch; 56h:19m:45s remains)
INFO - root - 2017-12-07 08:27:53.475898: step 6720, loss = 0.78, batch loss = 0.71 (12.4 examples/sec; 2.589 sec/batch; 54h:57m:21s remains)
INFO - root - 2017-12-07 08:28:19.749541: step 6730, loss = 0.75, batch loss = 0.67 (12.1 examples/sec; 2.654 sec/batch; 56h:19m:00s remains)
INFO - root - 2017-12-07 08:28:46.040753: step 6740, loss = 0.81, batch loss = 0.74 (12.0 examples/sec; 2.662 sec/batch; 56h:29m:21s remains)
INFO - root - 2017-12-07 08:29:17.308825: step 6750, loss = 0.79, batch loss = 0.71 (8.8 examples/sec; 3.628 sec/batch; 76h:57m:36s remains)
INFO - root - 2017-12-07 08:29:53.756544: step 6760, loss = 0.76, batch loss = 0.69 (8.8 examples/sec; 3.641 sec/batch; 77h:14m:21s remains)
INFO - root - 2017-12-07 08:30:29.938109: step 6770, loss = 0.80, batch loss = 0.73 (8.7 examples/sec; 3.665 sec/batch; 77h:43m:50s remains)
INFO - root - 2017-12-07 08:31:06.027899: step 6780, loss = 0.74, batch loss = 0.67 (8.8 examples/sec; 3.624 sec/batch; 76h:51m:41s remains)
INFO - root - 2017-12-07 08:31:42.222533: step 6790, loss = 0.85, batch loss = 0.78 (8.8 examples/sec; 3.641 sec/batch; 77h:11m:49s remains)
INFO - root - 2017-12-07 08:32:18.448950: step 6800, loss = 0.84, batch loss = 0.77 (8.8 examples/sec; 3.652 sec/batch; 77h:25m:23s remains)
2017-12-07 08:32:20.567081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2326746 -3.4337389 -3.7569826 -3.8244207 -3.6748037 -3.4911435 -3.379385 -3.4492691 -3.709933 -4.1251445 -4.4428687 -4.4039831 -3.9950523 -3.6144385 -3.5109582][-3.2425337 -3.5454366 -4.06061 -4.19626 -3.9887185 -3.6834788 -3.361222 -3.2473714 -3.4755163 -4.0081587 -4.4603195 -4.42245 -3.9233944 -3.4856122 -3.3968811][-3.1898966 -3.2273755 -3.5448012 -3.5457821 -3.2735767 -2.9358594 -2.4964619 -2.2784948 -2.5432897 -3.2227972 -3.8309381 -3.8752797 -3.4893188 -3.2041764 -3.2096996][-3.2388091 -3.1597707 -3.3279252 -3.2703443 -3.0063696 -2.5059421 -1.7329676 -1.2903078 -1.628866 -2.4891427 -3.2710714 -3.5294609 -3.427305 -3.3545122 -3.306978][-2.823688 -2.5131125 -2.6412492 -2.9015417 -3.0079961 -2.4408562 -1.2872205 -0.56383634 -0.95122981 -1.9393849 -2.8390021 -3.2847729 -3.397099 -3.4042969 -3.1737285][-2.1124279 -1.2368088 -1.163821 -1.7423987 -2.1963124 -1.542383 -0.045915604 0.79869175 0.13686705 -1.2012038 -2.3837712 -3.0749016 -3.3993745 -3.492033 -3.1371531][-1.74645 -0.32079887 -0.031202316 -0.8327167 -1.4614108 -0.59224534 1.2922301 2.2934952 1.4540796 -0.16308689 -1.5751727 -2.4584069 -2.9838285 -3.284163 -2.9957304][-1.9880917 -0.13145733 0.42956734 -0.40497303 -1.1007311 -0.1418519 1.8542824 2.9057517 1.9991121 0.22883081 -1.2731411 -2.1736522 -2.7523315 -3.2427788 -3.1041164][-2.3464768 -0.31534672 0.45097208 -0.29099941 -0.98941469 -0.0758605 1.8305507 2.8901806 1.9895067 0.12105322 -1.404825 -2.2053893 -2.6777105 -3.2365789 -3.2223701][-2.5156822 -0.53591323 0.27114677 -0.41968012 -1.1823821 -0.5133512 1.064362 2.0430946 1.2150717 -0.62236404 -2.0798106 -2.6983471 -2.9439497 -3.3736346 -3.3655453][-2.5992222 -0.82190442 -0.19233656 -0.90467954 -1.7376533 -1.3964899 -0.25047255 0.5997262 -0.034331322 -1.6866803 -3.0124226 -3.4683809 -3.4806192 -3.6824608 -3.6051133][-2.2215781 -0.79257631 -0.56022453 -1.3459191 -2.1608925 -2.0195944 -1.1354713 -0.28464842 -0.53081083 -1.8342371 -3.0190179 -3.3919535 -3.247366 -3.2216485 -3.1021588][-1.6367893 -0.71578646 -0.93716574 -1.7005653 -2.21542 -1.938494 -1.1377733 -0.37949467 -0.41897869 -1.4754419 -2.6081076 -3.0345228 -2.8739254 -2.6209271 -2.34642][-1.3681598 -1.1873398 -1.8610311 -2.4767106 -2.4817882 -1.8448992 -0.99087763 -0.31166029 -0.22985125 -1.02228 -2.0699236 -2.6175296 -2.6321435 -2.3598213 -1.9427874][-1.6731541 -2.129848 -3.0996132 -3.5709577 -3.1579025 -2.2122777 -1.2809842 -0.62364578 -0.44775748 -0.98808122 -1.9141395 -2.5675788 -2.8264499 -2.7849979 -2.4396119]]...]
INFO - root - 2017-12-07 08:32:56.590735: step 6810, loss = 0.68, batch loss = 0.61 (8.8 examples/sec; 3.621 sec/batch; 76h:45m:53s remains)
INFO - root - 2017-12-07 08:33:32.769523: step 6820, loss = 0.71, batch loss = 0.63 (8.8 examples/sec; 3.643 sec/batch; 77h:12m:30s remains)
INFO - root - 2017-12-07 08:34:08.761701: step 6830, loss = 0.78, batch loss = 0.71 (8.8 examples/sec; 3.630 sec/batch; 76h:55m:38s remains)
INFO - root - 2017-12-07 08:34:45.087677: step 6840, loss = 0.74, batch loss = 0.67 (9.0 examples/sec; 3.556 sec/batch; 75h:20m:46s remains)
INFO - root - 2017-12-07 08:35:21.137887: step 6850, loss = 0.77, batch loss = 0.70 (8.7 examples/sec; 3.658 sec/batch; 77h:29m:57s remains)
INFO - root - 2017-12-07 08:35:57.645741: step 6860, loss = 0.81, batch loss = 0.74 (8.8 examples/sec; 3.657 sec/batch; 77h:28m:15s remains)
INFO - root - 2017-12-07 08:36:33.815819: step 6870, loss = 0.78, batch loss = 0.70 (8.8 examples/sec; 3.633 sec/batch; 76h:57m:34s remains)
INFO - root - 2017-12-07 08:37:09.883961: step 6880, loss = 0.81, batch loss = 0.74 (9.0 examples/sec; 3.548 sec/batch; 75h:09m:02s remains)
INFO - root - 2017-12-07 08:37:46.063973: step 6890, loss = 0.75, batch loss = 0.68 (9.3 examples/sec; 3.454 sec/batch; 73h:08m:00s remains)
INFO - root - 2017-12-07 08:38:22.400917: step 6900, loss = 0.72, batch loss = 0.65 (8.8 examples/sec; 3.619 sec/batch; 76h:37m:36s remains)
2017-12-07 08:38:24.515776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2354331 -3.1684294 -3.1692252 -3.2470875 -3.2963624 -3.3088264 -3.3125815 -3.3488805 -3.4033952 -3.4154 -3.3819563 -3.3219862 -3.27595 -3.257534 -3.2384219][-2.7249229 -2.6011496 -2.5417166 -2.6290703 -2.7161367 -2.7054822 -2.63793 -2.6275516 -2.7139564 -2.7918289 -2.8126097 -2.8032393 -2.8178573 -2.8590667 -2.872278][-1.7545846 -1.5521691 -1.4078348 -1.4614482 -1.6170735 -1.6527991 -1.5714204 -1.5279932 -1.6392481 -1.821902 -1.9470179 -2.0199828 -2.1148534 -2.2337935 -2.2940676][-0.8539865 -0.5736897 -0.40760612 -0.46793032 -0.73591542 -0.90514207 -0.92081118 -0.929759 -1.0438185 -1.2687941 -1.4165149 -1.43224 -1.4066207 -1.4272976 -1.4170473][-0.83372188 -0.56223226 -0.45579028 -0.55749869 -0.87659097 -1.1122355 -1.1793611 -1.1855671 -1.1937151 -1.3176887 -1.378927 -1.2163751 -0.91350555 -0.72377849 -0.62871647][-1.4462459 -1.2153964 -1.0851667 -1.0831652 -1.2082074 -1.2262974 -1.1430464 -1.0790386 -1.0546906 -1.1846194 -1.3096726 -1.1710634 -0.83644247 -0.68125987 -0.75114131][-2.0984249 -1.8545487 -1.6107602 -1.4370334 -1.3111253 -1.0463111 -0.69121194 -0.45577478 -0.41334629 -0.62666464 -0.87921858 -0.87370038 -0.713928 -0.78415108 -1.140044][-2.4133539 -2.3847349 -2.249469 -2.1454625 -2.0961301 -1.9061012 -1.5245843 -1.2139432 -1.1742492 -1.4457276 -1.737164 -1.7230489 -1.5441272 -1.5396855 -1.7871177][-2.6557364 -2.8514833 -2.8743496 -2.8886576 -3.0463006 -3.1484578 -3.0057025 -2.7469027 -2.6435745 -2.8048985 -2.9621239 -2.7607853 -2.3799024 -2.1624982 -2.1305034][-3.4998844 -3.7028363 -3.7603393 -3.7637305 -3.937479 -4.1443782 -4.1703758 -3.9512391 -3.6694937 -3.5744448 -3.5268979 -3.1823432 -2.7198169 -2.4657578 -2.3595653][-3.592468 -3.7743964 -3.8605137 -3.8852119 -4.0194945 -4.2033615 -4.3486495 -4.3117332 -4.0661764 -3.8691926 -3.7046018 -3.3159776 -2.8534055 -2.5768647 -2.428282][-3.4866891 -3.6123898 -3.6675088 -3.6511631 -3.6800888 -3.727133 -3.8163395 -3.8117959 -3.6079373 -3.4302335 -3.3097148 -3.0539451 -2.7608275 -2.5712194 -2.4451027][-4.1397462 -4.1823974 -4.1623325 -4.0728116 -3.9937129 -3.8854284 -3.8603678 -3.8199468 -3.6254098 -3.4904315 -3.4418273 -3.3281898 -3.1996193 -3.0724521 -2.9456551][-4.6224122 -4.5963297 -4.5121512 -4.4108496 -4.3234711 -4.2075949 -4.1865211 -4.1879582 -4.0816517 -4.0222926 -4.010417 -3.9639626 -3.9263551 -3.8397894 -3.7247255][-4.11682 -4.1088295 -4.0361333 -3.9521112 -3.875473 -3.7649589 -3.7340894 -3.7748518 -3.7990518 -3.8834193 -3.9811788 -4.0535064 -4.1327996 -4.1430726 -4.1048169]]...]
INFO - root - 2017-12-07 08:39:00.717176: step 6910, loss = 0.84, batch loss = 0.77 (8.8 examples/sec; 3.653 sec/batch; 77h:19m:37s remains)
INFO - root - 2017-12-07 08:39:37.045026: step 6920, loss = 0.73, batch loss = 0.66 (8.8 examples/sec; 3.645 sec/batch; 77h:09m:39s remains)
INFO - root - 2017-12-07 08:40:13.368345: step 6930, loss = 0.81, batch loss = 0.73 (9.2 examples/sec; 3.492 sec/batch; 73h:54m:07s remains)
INFO - root - 2017-12-07 08:40:49.431962: step 6940, loss = 0.73, batch loss = 0.65 (8.8 examples/sec; 3.630 sec/batch; 76h:48m:46s remains)
INFO - root - 2017-12-07 08:41:25.472908: step 6950, loss = 0.75, batch loss = 0.67 (9.1 examples/sec; 3.514 sec/batch; 74h:21m:36s remains)
INFO - root - 2017-12-07 08:42:01.680532: step 6960, loss = 0.76, batch loss = 0.69 (8.9 examples/sec; 3.583 sec/batch; 75h:48m:12s remains)
INFO - root - 2017-12-07 08:42:38.006297: step 6970, loss = 0.64, batch loss = 0.57 (9.0 examples/sec; 3.555 sec/batch; 75h:11m:42s remains)
INFO - root - 2017-12-07 08:43:14.188427: step 6980, loss = 0.72, batch loss = 0.65 (8.9 examples/sec; 3.603 sec/batch; 76h:12m:48s remains)
INFO - root - 2017-12-07 08:43:50.449797: step 6990, loss = 0.74, batch loss = 0.67 (8.8 examples/sec; 3.637 sec/batch; 76h:54m:42s remains)
INFO - root - 2017-12-07 08:44:26.628486: step 7000, loss = 0.78, batch loss = 0.71 (8.8 examples/sec; 3.638 sec/batch; 76h:55m:22s remains)
2017-12-07 08:44:28.760335: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.423667 -2.3774638 -2.4125257 -2.4559386 -2.4983315 -2.5210381 -2.4920309 -2.4297898 -2.3056777 -2.0207465 -1.7701325 -1.898912 -2.3150506 -2.7458239 -3.0186062][-2.3381147 -2.3770342 -2.4645557 -2.5248277 -2.5914569 -2.6627021 -2.6852794 -2.6412239 -2.4832129 -2.1330729 -1.8502696 -2.0031266 -2.4665885 -2.9050679 -3.1374023][-2.3638246 -2.4233863 -2.4240873 -2.3685086 -2.3695364 -2.4456062 -2.519002 -2.5407493 -2.429997 -2.1322398 -1.9582283 -2.2127132 -2.7039974 -3.0622129 -3.1455097][-2.4748125 -2.56486 -2.521863 -2.4174626 -2.3882492 -2.4504108 -2.5596848 -2.6753097 -2.6525142 -2.3909287 -2.2213614 -2.4154108 -2.8029037 -3.0828333 -3.1516585][-2.6236043 -2.8106883 -2.8894997 -2.9145942 -2.9388356 -2.931706 -2.9913764 -3.1584473 -3.2551832 -3.1296191 -3.0328343 -3.1234493 -3.259455 -3.2790565 -3.2682087][-2.4987526 -2.6951964 -2.9141407 -3.1419711 -3.2685714 -3.168469 -3.0753803 -3.1510892 -3.2818427 -3.3935976 -3.5693121 -3.7964532 -3.8754179 -3.74119 -3.6697578][-2.2599959 -2.3681047 -2.6184335 -2.9713228 -3.2416415 -3.2063475 -3.0532026 -2.9361014 -2.8292384 -2.8645096 -3.1023684 -3.4668155 -3.7340403 -3.8061457 -3.9179225][-2.2930713 -2.2904518 -2.4105344 -2.6733663 -2.9760594 -3.1106753 -3.0305676 -2.7691474 -2.4013054 -2.2481968 -2.3937545 -2.7257953 -3.0876036 -3.388248 -3.6841021][-2.4438329 -2.3942139 -2.3629208 -2.4222465 -2.6090145 -2.8829913 -3.0444837 -2.9425707 -2.6627355 -2.4886775 -2.4685941 -2.5261536 -2.674686 -2.9487529 -3.2716537][-2.6119967 -2.6645319 -2.6185408 -2.5007529 -2.4725688 -2.6533723 -2.8893037 -2.9475632 -2.9142079 -2.9115105 -2.9140921 -2.853951 -2.8055894 -2.9457922 -3.1578095][-2.8185139 -2.9979186 -3.0627482 -2.8873494 -2.6183629 -2.404954 -2.2950511 -2.2526789 -2.395977 -2.7203574 -3.1012173 -3.3494384 -3.417335 -3.5536361 -3.6172438][-3.1861005 -3.412107 -3.5586381 -3.4466305 -3.1155248 -2.6380243 -2.175869 -1.8815818 -1.8891776 -2.2037337 -2.7378278 -3.2632341 -3.6392968 -3.9796834 -4.0609226][-3.4867795 -3.6522298 -3.8252563 -3.858027 -3.6647263 -3.21166 -2.6371081 -2.1548462 -1.9004638 -1.8729119 -2.0925872 -2.4831362 -2.9523249 -3.4318726 -3.6735761][-3.443419 -3.4802005 -3.6022594 -3.702317 -3.6514907 -3.4000554 -3.0175362 -2.6584246 -2.4134333 -2.2386155 -2.197778 -2.3291464 -2.5752625 -2.8452673 -3.0523045][-3.511874 -3.5088387 -3.5611718 -3.6233144 -3.6432977 -3.6045344 -3.5404594 -3.5222666 -3.5114198 -3.4103177 -3.3210888 -3.3655992 -3.4723022 -3.5495698 -3.5839639]]...]
INFO - root - 2017-12-07 08:45:04.907359: step 7010, loss = 0.78, batch loss = 0.70 (8.9 examples/sec; 3.607 sec/batch; 76h:15m:09s remains)
INFO - root - 2017-12-07 08:45:41.512591: step 7020, loss = 0.68, batch loss = 0.61 (8.8 examples/sec; 3.643 sec/batch; 77h:00m:51s remains)
INFO - root - 2017-12-07 08:46:17.900904: step 7030, loss = 0.79, batch loss = 0.72 (8.7 examples/sec; 3.669 sec/batch; 77h:33m:17s remains)
INFO - root - 2017-12-07 08:46:53.943578: step 7040, loss = 0.81, batch loss = 0.73 (8.8 examples/sec; 3.626 sec/batch; 76h:38m:30s remains)
INFO - root - 2017-12-07 08:47:30.396145: step 7050, loss = 0.75, batch loss = 0.68 (8.8 examples/sec; 3.623 sec/batch; 76h:33m:06s remains)
INFO - root - 2017-12-07 08:48:06.651038: step 7060, loss = 0.73, batch loss = 0.66 (8.9 examples/sec; 3.609 sec/batch; 76h:14m:53s remains)
INFO - root - 2017-12-07 08:48:42.804086: step 7070, loss = 0.79, batch loss = 0.72 (8.7 examples/sec; 3.690 sec/batch; 77h:56m:46s remains)
INFO - root - 2017-12-07 08:49:19.132066: step 7080, loss = 0.76, batch loss = 0.69 (8.7 examples/sec; 3.660 sec/batch; 77h:18m:09s remains)
INFO - root - 2017-12-07 08:49:55.594609: step 7090, loss = 0.70, batch loss = 0.63 (8.9 examples/sec; 3.596 sec/batch; 75h:57m:17s remains)
INFO - root - 2017-12-07 08:50:31.579551: step 7100, loss = 0.75, batch loss = 0.68 (9.2 examples/sec; 3.494 sec/batch; 73h:47m:17s remains)
2017-12-07 08:50:33.749147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8999162 -2.2589076 -2.3254673 -2.1234078 -1.7503078 -1.2994733 -0.81639528 -0.66523933 -1.0855021 -1.5529387 -1.9198716 -2.3667622 -2.6176481 -2.2221887 -1.8608475][-1.6119211 -1.5672586 -1.4296656 -1.2633305 -1.0730162 -0.79782867 -0.44442558 -0.4975636 -1.2459772 -2.0410659 -2.5932691 -2.8583984 -2.6125402 -1.6986008 -1.1208334][-1.0113366 -0.63515425 -0.43728232 -0.54508376 -0.820987 -0.94285774 -0.77458549 -0.82776666 -1.5320973 -2.2644341 -2.6639719 -2.5987275 -1.9445393 -0.65231133 0.059917927][-0.64985824 -0.32954216 -0.40405846 -0.92535615 -1.6430101 -1.9944823 -1.7636042 -1.4774513 -1.7330565 -2.0790176 -2.1789169 -1.9964108 -1.3575666 -0.12222481 0.58981705][-0.6641264 -0.68430638 -1.0844879 -1.8044865 -2.6081202 -2.9345794 -2.6264479 -2.1798942 -2.1578054 -2.0952439 -1.8256454 -1.5956674 -1.232955 -0.38608027 0.1736207][-1.1411047 -1.4853177 -1.8876734 -2.357924 -2.8518698 -3.0257816 -2.8215542 -2.5713611 -2.5615041 -2.2410123 -1.6660223 -1.4108922 -1.3968105 -1.0404804 -0.6991725][-1.7287786 -2.1696727 -2.3310905 -2.3821337 -2.4797177 -2.482408 -2.3749275 -2.3185558 -2.3116667 -1.8129125 -1.1775153 -1.1595573 -1.6276448 -1.7319865 -1.5139704][-2.0125811 -2.3706505 -2.2533836 -1.9740458 -1.8403409 -1.7089021 -1.5391781 -1.4315603 -1.2543015 -0.59301233 -0.128438 -0.61901188 -1.6297204 -2.1040661 -1.8877342][-2.0361309 -2.2391577 -1.8250539 -1.3432102 -1.1634085 -1.0099149 -0.75243807 -0.5429635 -0.25448179 0.3941493 0.53086567 -0.39896345 -1.6253793 -2.0811458 -1.6971166][-1.8098176 -1.9178667 -1.3719022 -0.86392736 -0.76675558 -0.68759155 -0.51663756 -0.43987703 -0.30644512 0.08905983 -0.067928791 -1.0314445 -1.9152589 -1.927808 -1.2893977][-1.6649954 -1.7942722 -1.3061414 -0.89516759 -0.90313578 -0.93031812 -0.965508 -1.1475267 -1.271425 -1.1583455 -1.4318523 -2.0582669 -2.307409 -1.8587151 -1.1322441][-1.7229741 -1.9389124 -1.6325989 -1.4070091 -1.4751425 -1.5414331 -1.6806271 -1.9138339 -2.0769742 -2.1054909 -2.3815503 -2.6552873 -2.4787922 -1.9493525 -1.4466374][-1.9156051 -2.1595275 -2.0045671 -1.9369521 -2.015554 -2.0137644 -2.0301914 -2.0644011 -2.1258645 -2.2743614 -2.5820811 -2.6847773 -2.462996 -2.183888 -2.0036623][-2.2677109 -2.3869045 -2.2250886 -2.1773739 -2.1839402 -2.0729463 -1.9561021 -1.8308988 -1.8813527 -2.2161636 -2.5908806 -2.67103 -2.6329155 -2.6226106 -2.4684992][-2.3654418 -2.2757688 -2.0947204 -2.095259 -2.1158957 -2.0497291 -2.0562496 -2.033324 -2.1621041 -2.5822697 -2.9395514 -3.0154567 -3.1333489 -3.1029251 -2.6039968]]...]
INFO - root - 2017-12-07 08:51:10.123199: step 7110, loss = 0.76, batch loss = 0.69 (8.7 examples/sec; 3.669 sec/batch; 77h:28m:04s remains)
INFO - root - 2017-12-07 08:51:46.272975: step 7120, loss = 0.74, batch loss = 0.67 (8.8 examples/sec; 3.653 sec/batch; 77h:07m:41s remains)
INFO - root - 2017-12-07 08:52:22.545949: step 7130, loss = 0.72, batch loss = 0.65 (8.9 examples/sec; 3.613 sec/batch; 76h:16m:13s remains)
INFO - root - 2017-12-07 08:52:58.473207: step 7140, loss = 0.77, batch loss = 0.70 (8.9 examples/sec; 3.593 sec/batch; 75h:50m:25s remains)
INFO - root - 2017-12-07 08:53:34.905537: step 7150, loss = 0.83, batch loss = 0.76 (8.9 examples/sec; 3.597 sec/batch; 75h:54m:29s remains)
INFO - root - 2017-12-07 08:54:11.049608: step 7160, loss = 0.80, batch loss = 0.72 (8.8 examples/sec; 3.653 sec/batch; 77h:04m:30s remains)
INFO - root - 2017-12-07 08:54:47.213421: step 7170, loss = 0.77, batch loss = 0.70 (8.7 examples/sec; 3.677 sec/batch; 77h:35m:13s remains)
INFO - root - 2017-12-07 08:55:23.355314: step 7180, loss = 0.92, batch loss = 0.84 (8.7 examples/sec; 3.658 sec/batch; 77h:10m:32s remains)
INFO - root - 2017-12-07 08:55:59.504487: step 7190, loss = 0.80, batch loss = 0.73 (8.9 examples/sec; 3.579 sec/batch; 75h:29m:52s remains)
INFO - root - 2017-12-07 08:56:35.437857: step 7200, loss = 0.74, batch loss = 0.67 (8.8 examples/sec; 3.645 sec/batch; 76h:52m:11s remains)
2017-12-07 08:56:37.646510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.86836767 -0.68724132 -0.92143273 -1.3358471 -1.5094275 -1.4991024 -1.4837966 -1.4861126 -1.499923 -1.5121539 -1.4943035 -1.3623455 -1.2763624 -1.3351457 -1.5685031][-1.5236602 -1.2512522 -1.4466429 -1.8921156 -2.1193805 -2.0560584 -1.8479362 -1.6471205 -1.4950321 -1.4203372 -1.3096344 -1.0817218 -0.913491 -0.94238567 -1.0986974][-2.0091925 -1.761225 -1.9247932 -2.3214598 -2.5896344 -2.5450826 -2.2491703 -1.8828199 -1.576297 -1.4381647 -1.3023167 -1.0749063 -0.8797462 -0.88156509 -1.0225403][-2.0813148 -1.9364491 -2.0699098 -2.3793511 -2.6789474 -2.7508545 -2.5447898 -2.1656263 -1.791991 -1.6128838 -1.5007958 -1.3691046 -1.2371492 -1.2491174 -1.4693615][-1.6798284 -1.7084498 -1.8954358 -2.1981785 -2.5567446 -2.7567458 -2.7143283 -2.4457836 -2.1259882 -1.9557829 -1.8676074 -1.8403327 -1.8425155 -1.9106815 -2.2334795][-1.1216528 -1.3407843 -1.667259 -1.9825237 -2.3434637 -2.5819912 -2.6631246 -2.5900538 -2.3862798 -2.242317 -2.1727526 -2.2149413 -2.3256836 -2.4505222 -2.8185592][-0.71593904 -1.0359309 -1.4571106 -1.7912917 -2.1918418 -2.456177 -2.6245165 -2.678937 -2.5296965 -2.3717775 -2.2971551 -2.3619874 -2.4894423 -2.6060038 -2.9361157][-0.43763137 -0.82818484 -1.3069055 -1.6933615 -2.2041063 -2.5464959 -2.77723 -2.8666074 -2.6757963 -2.4025416 -2.2305088 -2.225538 -2.2603254 -2.2886825 -2.5094404][-0.2631917 -0.77761769 -1.2576301 -1.6994381 -2.3097157 -2.6878805 -2.9549813 -3.0496352 -2.80569 -2.3759758 -2.0273933 -1.8831341 -1.7860436 -1.7020359 -1.8053479][-0.31870985 -0.94211364 -1.3887198 -1.8776188 -2.5151143 -2.8377719 -3.0480285 -3.1220546 -2.8244777 -2.2676659 -1.8067229 -1.6150985 -1.4596789 -1.2874045 -1.3010886][-0.59347224 -1.1871302 -1.5857937 -2.1220295 -2.7193468 -2.9509563 -3.0962987 -3.1225429 -2.746726 -2.1456819 -1.7070932 -1.6081038 -1.4926775 -1.2789631 -1.2001593][-1.0498366 -1.4560921 -1.7699442 -2.2869117 -2.7694221 -2.8978426 -3.0248168 -3.0293827 -2.6443276 -2.09833 -1.7463176 -1.7287762 -1.6199358 -1.3488243 -1.1772568][-1.397666 -1.6059391 -1.8261311 -2.245451 -2.5905786 -2.6530128 -2.7681925 -2.746516 -2.4125154 -1.9830022 -1.7040148 -1.6359515 -1.4240227 -1.1150348 -0.9296][-1.3700125 -1.4363956 -1.5552316 -1.854435 -2.1319511 -2.2489929 -2.420145 -2.421452 -2.1731937 -1.8472786 -1.6528864 -1.521992 -1.2259271 -0.92513537 -0.78217316][-1.1246765 -1.2024827 -1.3112421 -1.5494463 -1.8143091 -1.9937093 -2.21924 -2.3070633 -2.1988053 -1.9935164 -1.9300623 -1.8164148 -1.4911973 -1.1481307 -0.94152308]]...]
INFO - root - 2017-12-07 08:57:13.684056: step 7210, loss = 0.83, batch loss = 0.76 (8.8 examples/sec; 3.627 sec/batch; 76h:29m:08s remains)
INFO - root - 2017-12-07 08:57:50.018337: step 7220, loss = 0.79, batch loss = 0.72 (8.8 examples/sec; 3.633 sec/batch; 76h:36m:37s remains)
INFO - root - 2017-12-07 08:58:26.300288: step 7230, loss = 0.71, batch loss = 0.64 (8.8 examples/sec; 3.649 sec/batch; 76h:55m:30s remains)
INFO - root - 2017-12-07 08:59:02.512907: step 7240, loss = 0.85, batch loss = 0.78 (8.8 examples/sec; 3.646 sec/batch; 76h:51m:12s remains)
INFO - root - 2017-12-07 08:59:38.924950: step 7250, loss = 0.79, batch loss = 0.72 (8.9 examples/sec; 3.593 sec/batch; 75h:43m:02s remains)
INFO - root - 2017-12-07 09:00:15.268927: step 7260, loss = 0.77, batch loss = 0.70 (8.7 examples/sec; 3.694 sec/batch; 77h:50m:48s remains)
INFO - root - 2017-12-07 09:00:51.444242: step 7270, loss = 0.83, batch loss = 0.75 (8.8 examples/sec; 3.628 sec/batch; 76h:27m:05s remains)
INFO - root - 2017-12-07 09:01:27.707806: step 7280, loss = 0.71, batch loss = 0.64 (8.9 examples/sec; 3.610 sec/batch; 76h:03m:56s remains)
INFO - root - 2017-12-07 09:02:04.092302: step 7290, loss = 0.82, batch loss = 0.75 (8.8 examples/sec; 3.629 sec/batch; 76h:26m:54s remains)
INFO - root - 2017-12-07 09:02:40.222008: step 7300, loss = 0.81, batch loss = 0.73 (8.6 examples/sec; 3.716 sec/batch; 78h:15m:57s remains)
2017-12-07 09:02:42.475096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.241261 -4.2063823 -4.1012869 -4.1574869 -4.2987952 -4.293313 -4.0547237 -3.5618439 -3.1643934 -3.2682972 -3.6543853 -4.0043917 -4.0758538 -3.8703814 -3.7254918][-4.2191286 -4.1851258 -4.0693707 -4.0700235 -3.902319 -3.3950226 -2.8395424 -2.2842181 -1.9759245 -2.2604353 -2.8325233 -3.2921474 -3.4409842 -3.1977391 -2.9874864][-4.3018365 -4.239079 -3.9581563 -3.6738484 -2.9853466 -1.9635949 -1.281889 -0.8648572 -0.73170328 -1.2397258 -1.920326 -2.3401532 -2.5727646 -2.4243159 -2.2305071][-4.4499083 -4.3816628 -3.9303353 -3.4189854 -2.4673536 -1.2754025 -0.58068681 -0.10919285 0.12659645 -0.44308424 -1.0680077 -1.2946408 -1.6014767 -1.7387199 -1.7087474][-4.67518 -4.6487379 -4.1072483 -3.5546799 -2.6581361 -1.525738 -0.68033814 0.28392935 0.98165178 0.49127865 -0.069367886 -0.18110561 -0.63863778 -1.0917571 -1.2487512][-4.8348694 -4.8066487 -4.2142563 -3.6518946 -2.7625971 -1.4745266 -0.2592063 1.2822051 2.3750091 1.774291 0.9673996 0.62604952 -0.0981555 -0.77198505 -0.97972894][-4.8211203 -4.7634993 -4.1533046 -3.5222561 -2.4967821 -0.96384454 0.57303762 2.4580779 3.6125526 2.6412578 1.4819765 0.80190992 -0.10182905 -0.77937675 -0.86717415][-4.6316915 -4.5675163 -3.9983695 -3.3110471 -2.20651 -0.69943047 0.66317463 2.1944437 2.8932014 1.7324042 0.47182322 -0.30333805 -1.0382628 -1.3463793 -1.1060052][-4.4616446 -4.3573222 -3.7954693 -3.0960143 -2.0673163 -0.796649 0.21935415 1.2127061 1.5347476 0.54502535 -0.46951437 -1.063549 -1.4449923 -1.3310962 -0.84450459][-4.3142433 -4.1904426 -3.6387835 -2.9785368 -2.1228092 -1.0606949 -0.26397276 0.40235519 0.52563238 -0.19455767 -0.86605 -1.2215626 -1.3102129 -0.92053056 -0.35314798][-4.2816143 -4.1705122 -3.6749485 -3.0989192 -2.4283645 -1.5919094 -0.94072294 -0.45746541 -0.41726494 -0.88852215 -1.3611822 -1.6550689 -1.7185383 -1.3729866 -0.91464543][-4.2304831 -4.0584159 -3.5233009 -2.927772 -2.3749456 -1.7883959 -1.3123069 -0.99440169 -1.0096016 -1.3287609 -1.6713576 -1.9030418 -1.9027736 -1.6230383 -1.3602405][-4.2526503 -4.1256256 -3.6469359 -3.1158128 -2.7242608 -2.4121256 -2.12695 -1.9876804 -2.1828072 -2.5283163 -2.8191009 -2.9412389 -2.8114529 -2.5180755 -2.2871735][-4.1966257 -4.1409273 -3.7964988 -3.4136591 -3.1706607 -3.0385902 -2.9226949 -2.9533715 -3.2330842 -3.5498767 -3.7609642 -3.8069549 -3.6342554 -3.3548589 -3.1293144][-3.9592838 -3.9059107 -3.6756146 -3.431582 -3.2863643 -3.2308645 -3.1851707 -3.2348752 -3.4430892 -3.632122 -3.6988049 -3.661624 -3.5134127 -3.3367147 -3.2293634]]...]
INFO - root - 2017-12-07 09:03:18.714199: step 7310, loss = 0.74, batch loss = 0.67 (8.9 examples/sec; 3.602 sec/batch; 75h:51m:11s remains)
INFO - root - 2017-12-07 09:03:55.018959: step 7320, loss = 0.80, batch loss = 0.73 (8.6 examples/sec; 3.741 sec/batch; 78h:46m:44s remains)
INFO - root - 2017-12-07 09:04:31.157602: step 7330, loss = 0.71, batch loss = 0.64 (8.9 examples/sec; 3.580 sec/batch; 75h:22m:28s remains)
INFO - root - 2017-12-07 09:05:07.262590: step 7340, loss = 0.70, batch loss = 0.63 (8.8 examples/sec; 3.634 sec/batch; 76h:30m:09s remains)
INFO - root - 2017-12-07 09:05:43.551866: step 7350, loss = 0.71, batch loss = 0.64 (8.9 examples/sec; 3.614 sec/batch; 76h:04m:25s remains)
INFO - root - 2017-12-07 09:06:19.654822: step 7360, loss = 0.83, batch loss = 0.76 (8.8 examples/sec; 3.629 sec/batch; 76h:22m:21s remains)
INFO - root - 2017-12-07 09:06:55.964314: step 7370, loss = 0.71, batch loss = 0.64 (8.8 examples/sec; 3.621 sec/batch; 76h:12m:16s remains)
INFO - root - 2017-12-07 09:07:32.006546: step 7380, loss = 0.83, batch loss = 0.76 (8.9 examples/sec; 3.594 sec/batch; 75h:37m:20s remains)
INFO - root - 2017-12-07 09:08:08.029768: step 7390, loss = 0.78, batch loss = 0.71 (9.2 examples/sec; 3.467 sec/batch; 72h:55m:48s remains)
INFO - root - 2017-12-07 09:08:44.045543: step 7400, loss = 0.77, batch loss = 0.70 (8.9 examples/sec; 3.592 sec/batch; 75h:33m:37s remains)
2017-12-07 09:08:46.278084: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6412666 -3.71627 -3.8052647 -3.9090812 -4.0045152 -4.0521016 -4.0211825 -3.9625666 -3.9519327 -3.9870803 -3.9569621 -3.8616486 -3.7798264 -3.741848 -3.7954187][-3.5945325 -3.7097893 -3.8443234 -3.9914873 -4.1239533 -4.1812367 -4.1338348 -4.0761948 -4.1172619 -4.211133 -4.1487513 -3.9443107 -3.7510164 -3.694222 -3.8210158][-3.5674222 -3.7272892 -3.8959544 -4.0384808 -4.1663795 -4.1836538 -4.10255 -4.0282655 -4.076684 -4.1866326 -4.0495577 -3.7033572 -3.4032691 -3.4015455 -3.7164969][-3.5164783 -3.7167411 -3.9070258 -4.0292606 -4.14449 -4.10555 -3.9506249 -3.763531 -3.6567378 -3.6344166 -3.3501887 -2.8725061 -2.5594189 -2.7369649 -3.3507822][-3.3940625 -3.5931079 -3.7460277 -3.7927539 -3.8356795 -3.684145 -3.3645363 -2.9408908 -2.5684729 -2.3763504 -2.0420625 -1.6314769 -1.5556543 -2.057132 -2.941329][-3.2119224 -3.4284067 -3.5625668 -3.5439649 -3.4501772 -3.0829971 -2.4953113 -1.7330315 -1.1407013 -0.95655632 -0.82764006 -0.79977226 -1.1296279 -1.8566287 -2.7021008][-3.0969024 -3.3054655 -3.3983245 -3.3068051 -3.0237985 -2.3704536 -1.4410522 -0.34709358 0.25265169 0.057365417 -0.29923725 -0.78468943 -1.407228 -1.9913034 -2.3647375][-3.0434842 -3.1287198 -3.0934014 -2.880096 -2.3765533 -1.4868762 -0.35252476 0.8768096 1.2488065 0.48351669 -0.37355375 -1.208977 -1.8306658 -1.9694841 -1.6942253][-2.99156 -2.8672194 -2.6721251 -2.3754656 -1.8013415 -0.943053 -0.015363216 0.83498573 0.79008913 -0.20251942 -1.0525441 -1.695915 -1.9675162 -1.6330142 -0.89714718][-2.951714 -2.5695243 -2.1691234 -1.7918675 -1.3192577 -0.78968716 -0.39544058 -0.21236181 -0.58284187 -1.3421664 -1.7386854 -1.8702776 -1.7616193 -1.2894583 -0.59189415][-3.0051064 -2.4989953 -1.991987 -1.6358221 -1.3851242 -1.2481468 -1.2786243 -1.4832516 -1.8636067 -2.2285755 -2.18637 -1.9595296 -1.7244325 -1.4551184 -1.1144819][-3.1702843 -2.6971459 -2.2026942 -1.9221087 -1.9160411 -2.0956252 -2.3131566 -2.5059774 -2.6338854 -2.6714852 -2.5133994 -2.3160961 -2.2271273 -2.230624 -2.1557443][-3.3845794 -3.0724576 -2.7147415 -2.5469995 -2.7217212 -3.0714993 -3.3029718 -3.3055182 -3.1297719 -2.9423141 -2.8558955 -2.8747935 -3.0095801 -3.1584215 -3.1353316][-3.6019895 -3.5138335 -3.3530245 -3.2848978 -3.4711823 -3.7654362 -3.9103651 -3.7945549 -3.5240996 -3.3099105 -3.3217535 -3.4828815 -3.7033086 -3.8522828 -3.7957718][-3.7539246 -3.79742 -3.7677062 -3.748363 -3.8560443 -4.0113626 -4.0647478 -3.9640081 -3.7932339 -3.6832447 -3.728704 -3.8607023 -4.0151644 -4.12105 -4.1044536]]...]
INFO - root - 2017-12-07 09:09:22.361490: step 7410, loss = 0.71, batch loss = 0.64 (8.9 examples/sec; 3.605 sec/batch; 75h:49m:46s remains)
INFO - root - 2017-12-07 09:09:58.355242: step 7420, loss = 0.69, batch loss = 0.62 (8.9 examples/sec; 3.613 sec/batch; 75h:58m:56s remains)
INFO - root - 2017-12-07 09:10:34.294031: step 7430, loss = 0.74, batch loss = 0.67 (9.0 examples/sec; 3.553 sec/batch; 74h:42m:47s remains)
INFO - root - 2017-12-07 09:11:10.531733: step 7440, loss = 0.75, batch loss = 0.68 (8.8 examples/sec; 3.626 sec/batch; 76h:14m:05s remains)
INFO - root - 2017-12-07 09:11:46.773992: step 7450, loss = 0.70, batch loss = 0.63 (8.8 examples/sec; 3.631 sec/batch; 76h:19m:12s remains)
INFO - root - 2017-12-07 09:12:22.788145: step 7460, loss = 0.84, batch loss = 0.77 (9.0 examples/sec; 3.571 sec/batch; 75h:03m:44s remains)
INFO - root - 2017-12-07 09:12:58.857399: step 7470, loss = 0.78, batch loss = 0.70 (8.9 examples/sec; 3.591 sec/batch; 75h:27m:27s remains)
INFO - root - 2017-12-07 09:13:35.312283: step 7480, loss = 0.77, batch loss = 0.70 (9.1 examples/sec; 3.513 sec/batch; 73h:48m:58s remains)
INFO - root - 2017-12-07 09:14:21.724373: step 7490, loss = 0.69, batch loss = 0.62 (6.8 examples/sec; 4.706 sec/batch; 98h:51m:44s remains)
INFO - root - 2017-12-07 09:15:08.121183: step 7500, loss = 0.84, batch loss = 0.76 (7.1 examples/sec; 4.491 sec/batch; 94h:20m:15s remains)
2017-12-07 09:15:10.892879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0396144 -0.55575919 -0.58955193 -1.0851247 -1.639487 -2.1213186 -2.5667148 -2.7194209 -2.7320261 -3.1714954 -3.8015518 -4.0024357 -3.7692583 -3.2507162 -2.5055876][-1.239022 -0.99069118 -1.1948342 -1.6505203 -1.979677 -2.2888927 -2.5891526 -2.5634861 -2.5391245 -3.0457616 -3.6891341 -3.9571838 -3.7881293 -3.1985254 -2.4467008][-1.4721351 -1.3997803 -1.5964332 -1.9798048 -2.2102265 -2.4574552 -2.6421542 -2.418854 -2.3305051 -2.8568344 -3.5387645 -3.9024205 -3.8143251 -3.2132959 -2.4844265][-1.4823809 -1.4536424 -1.5194578 -1.8406501 -2.1274295 -2.4661975 -2.6198151 -2.2681448 -2.1300447 -2.6731031 -3.4564679 -3.9396508 -3.9245572 -3.3608217 -2.6637983][-1.1083453 -0.85881424 -0.6182642 -0.83834648 -1.3220851 -1.9517269 -2.2998471 -2.026412 -1.8995419 -2.3730626 -3.1721711 -3.7595468 -3.8719087 -3.4392443 -2.802773][-0.51135182 -0.015181065 0.4876442 0.36493635 -0.30857515 -1.1957464 -1.7053609 -1.4524853 -1.19983 -1.4252529 -2.0717385 -2.7668166 -3.2052479 -3.1252875 -2.6902945][-0.28625965 0.21245146 0.77220917 0.69525862 -0.074812889 -0.95171833 -1.2953959 -0.81770015 -0.26665068 -0.14549541 -0.58286309 -1.4474573 -2.3205409 -2.6335177 -2.4219782][-0.61232424 -0.48671985 -0.13725471 -0.18381596 -0.77483344 -1.3784285 -1.4087503 -0.73798752 -0.036341667 0.2965374 0.036153316 -0.910347 -2.0670393 -2.5123959 -2.2926586][-1.0130482 -1.2272952 -1.1108918 -1.1667519 -1.5618925 -1.9260447 -1.8114967 -1.2060907 -0.64635754 -0.32933617 -0.49418616 -1.4213395 -2.569838 -2.8358526 -2.3623686][-1.3746128 -1.5138822 -1.3434095 -1.3368618 -1.6571877 -1.9482975 -1.879221 -1.500411 -1.245275 -1.1456611 -1.3919158 -2.2965295 -3.2511268 -3.1975856 -2.3928452][-1.7455218 -1.4930148 -1.0328176 -0.906739 -1.1716022 -1.398679 -1.3594222 -1.1411986 -1.2065363 -1.4488547 -1.9421012 -2.87844 -3.6450725 -3.3390117 -2.2912455][-1.8721521 -1.1963761 -0.44501877 -0.25430489 -0.50228381 -0.67648792 -0.6238606 -0.49818277 -0.82060456 -1.4306753 -2.2106907 -3.1540394 -3.7627883 -3.3096173 -2.1622078][-1.9082181 -1.0691345 -0.28033543 -0.16985178 -0.50581217 -0.6847682 -0.62988973 -0.50329471 -0.86169505 -1.6469328 -2.5812547 -3.4722319 -3.9390032 -3.4133902 -2.2231817][-2.0909207 -1.3823733 -0.78333426 -0.88386106 -1.3641386 -1.5665925 -1.4989665 -1.3295033 -1.5107801 -2.1936023 -3.1283965 -3.963798 -4.3418031 -3.749927 -2.5023828][-2.2456293 -1.854208 -1.5615594 -1.8659134 -2.401768 -2.5495605 -2.3880832 -2.211427 -2.279851 -2.8418016 -3.7193084 -4.4761405 -4.7623754 -4.0830297 -2.7480931]]...]
INFO - root - 2017-12-07 09:15:57.414977: step 7510, loss = 0.73, batch loss = 0.65 (6.8 examples/sec; 4.695 sec/batch; 98h:37m:07s remains)
INFO - root - 2017-12-07 09:16:43.683885: step 7520, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.523 sec/batch; 94h:59m:04s remains)
INFO - root - 2017-12-07 09:17:30.270466: step 7530, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.633 sec/batch; 97h:16m:37s remains)
INFO - root - 2017-12-07 09:18:16.853567: step 7540, loss = 0.76, batch loss = 0.69 (6.8 examples/sec; 4.685 sec/batch; 98h:21m:23s remains)
INFO - root - 2017-12-07 09:19:03.353961: step 7550, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.597 sec/batch; 96h:30m:18s remains)
INFO - root - 2017-12-07 09:19:49.927962: step 7560, loss = 0.77, batch loss = 0.69 (6.9 examples/sec; 4.658 sec/batch; 97h:46m:27s remains)
INFO - root - 2017-12-07 09:20:36.276299: step 7570, loss = 0.75, batch loss = 0.67 (6.9 examples/sec; 4.632 sec/batch; 97h:13m:18s remains)
INFO - root - 2017-12-07 09:21:22.765285: step 7580, loss = 0.72, batch loss = 0.65 (6.8 examples/sec; 4.678 sec/batch; 98h:10m:26s remains)
INFO - root - 2017-12-07 09:22:09.040623: step 7590, loss = 0.90, batch loss = 0.83 (6.8 examples/sec; 4.700 sec/batch; 98h:36m:44s remains)
INFO - root - 2017-12-07 09:22:55.929017: step 7600, loss = 0.86, batch loss = 0.79 (7.0 examples/sec; 4.589 sec/batch; 96h:15m:52s remains)
2017-12-07 09:22:58.646420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.60333562 -0.45825863 -0.39199209 -0.21307945 0.046731472 0.2951746 0.49614954 0.64595413 0.72685242 0.75006342 0.71170187 0.58038521 0.38050842 0.14464235 -0.13443327][-0.309505 -0.15555286 -0.042071342 0.19970846 0.51949883 0.81780672 1.0596194 1.2418771 1.343864 1.380949 1.3535428 1.2184191 1.0064821 0.76270294 0.47474337][-0.051831245 0.070866585 0.19651222 0.45787764 0.78262472 1.0744014 1.3077803 1.5005078 1.637536 1.7318277 1.7671003 1.6744208 1.4905972 1.2757096 1.020999][0.15943575 0.22632599 0.34113264 0.59082794 0.87846851 1.1175976 1.2946696 1.4526124 1.5778446 1.688458 1.7794213 1.7645068 1.6650763 1.5369725 1.3663592][0.27284241 0.35034752 0.50455236 0.77608919 1.0624671 1.2848768 1.4174852 1.5123439 1.5391521 1.544188 1.5742188 1.5525069 1.490602 1.4383368 1.3759727][0.406857 0.5559516 0.76998472 1.0571551 1.3316398 1.5335274 1.6205029 1.6388369 1.5382237 1.3937478 1.3005362 1.1845303 1.0687995 1.0292959 1.0603533][0.49874973 0.71903133 0.97442579 1.2561831 1.5103211 1.6973686 1.7691183 1.7577424 1.5946245 1.3565574 1.1567526 0.92076731 0.71297836 0.64458275 0.72686338][0.41537476 0.66565895 0.92611694 1.1867185 1.4151244 1.5818644 1.6389413 1.6354718 1.5042715 1.2986417 1.1071329 0.85660458 0.62940216 0.55010939 0.63912487][0.34985209 0.54275751 0.72916746 0.9211812 1.0942044 1.2021961 1.1838074 1.1212597 0.985322 0.80656481 0.66081285 0.49222946 0.3572731 0.36234474 0.50171661][0.36801195 0.48408937 0.58052969 0.69760847 0.82393932 0.90141964 0.85073328 0.75117588 0.57845449 0.35511684 0.19070864 0.0647254 0.0028400421 0.10413313 0.32404709][0.22262812 0.29671955 0.32852554 0.38031054 0.4433012 0.47976351 0.42768145 0.34775686 0.2008152 -0.019308567 -0.17309904 -0.28231144 -0.33684731 -0.20545626 0.05228138][-0.0017552376 0.036055565 0.019900322 0.046817303 0.096704006 0.13509798 0.11238766 0.083902359 0.0054178238 -0.1650095 -0.28074074 -0.36280632 -0.40562725 -0.27796555 -0.044291019][-0.12704611 -0.1124115 -0.15836096 -0.13140821 -0.060439587 0.015606403 0.04144907 0.072566032 0.084805489 -0.0021386147 -0.074725151 -0.12533522 -0.14027548 -0.015424252 0.1575613][-0.15321445 -0.16110659 -0.21080589 -0.17276049 -0.0930295 -0.00021409988 0.057884693 0.11722326 0.17567444 0.14079094 0.0928731 0.0587101 0.05794239 0.15742445 0.24631596][-0.10305357 -0.11483669 -0.12726974 -0.053662777 0.031601429 0.11778975 0.18694973 0.248981 0.30238771 0.27123165 0.23363066 0.22175789 0.23488045 0.30547047 0.32678366]]...]
INFO - root - 2017-12-07 09:23:45.210525: step 7610, loss = 0.86, batch loss = 0.78 (6.9 examples/sec; 4.632 sec/batch; 97h:09m:12s remains)
INFO - root - 2017-12-07 09:24:31.782666: step 7620, loss = 0.73, batch loss = 0.65 (6.8 examples/sec; 4.695 sec/batch; 98h:27m:58s remains)
INFO - root - 2017-12-07 09:25:18.235309: step 7630, loss = 0.87, batch loss = 0.80 (6.9 examples/sec; 4.626 sec/batch; 97h:00m:42s remains)
INFO - root - 2017-12-07 09:26:04.798512: step 7640, loss = 0.78, batch loss = 0.71 (6.8 examples/sec; 4.674 sec/batch; 98h:00m:47s remains)
INFO - root - 2017-12-07 09:26:51.380786: step 7650, loss = 0.75, batch loss = 0.67 (6.9 examples/sec; 4.664 sec/batch; 97h:46m:47s remains)
INFO - root - 2017-12-07 09:27:37.938970: step 7660, loss = 0.86, batch loss = 0.78 (7.0 examples/sec; 4.602 sec/batch; 96h:27m:50s remains)
INFO - root - 2017-12-07 09:28:24.397673: step 7670, loss = 0.71, batch loss = 0.63 (6.9 examples/sec; 4.671 sec/batch; 97h:53m:39s remains)
INFO - root - 2017-12-07 09:29:10.720833: step 7680, loss = 0.83, batch loss = 0.76 (6.8 examples/sec; 4.693 sec/batch; 98h:20m:56s remains)
INFO - root - 2017-12-07 09:29:57.129173: step 7690, loss = 0.71, batch loss = 0.64 (6.9 examples/sec; 4.609 sec/batch; 96h:34m:30s remains)
INFO - root - 2017-12-07 09:30:43.838606: step 7700, loss = 0.73, batch loss = 0.66 (6.8 examples/sec; 4.688 sec/batch; 98h:13m:42s remains)
2017-12-07 09:30:46.516941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1364896 -3.1661341 -3.2356515 -3.2401972 -3.1913719 -3.091363 -2.9401641 -2.9004865 -2.9782538 -3.1403768 -3.2584038 -3.2072284 -3.1195874 -3.1332161 -3.2475398][-3.4704037 -3.4811425 -3.4226136 -3.2676165 -3.1072793 -2.934114 -2.7405715 -2.7156448 -2.8277531 -2.9744086 -3.0034814 -2.8543079 -2.8021657 -2.9676223 -3.2789905][-3.9266202 -3.8347979 -3.6126373 -3.2842388 -3.0341349 -2.8349428 -2.63762 -2.5988257 -2.7066333 -2.7971411 -2.6623979 -2.3404915 -2.253077 -2.4638538 -2.8301945][-3.9832144 -3.8343439 -3.5578535 -3.1670127 -2.8688397 -2.6705155 -2.5678582 -2.6266003 -2.8110206 -2.8916762 -2.6664114 -2.2937772 -2.1801071 -2.331264 -2.6250262][-3.7240057 -3.6005709 -3.3957119 -3.0908523 -2.846067 -2.7130342 -2.7402353 -2.9363241 -3.1603661 -3.1767101 -2.8929727 -2.5193434 -2.3895538 -2.458045 -2.6188707][-3.292335 -3.2018716 -3.0730889 -2.8782611 -2.7237396 -2.6629076 -2.7704513 -3.0278883 -3.2066724 -3.1517506 -2.8732023 -2.5775411 -2.4975619 -2.5056112 -2.5051436][-2.8371356 -2.7280302 -2.590013 -2.4120994 -2.2652531 -2.1957161 -2.2602811 -2.4609122 -2.5309651 -2.4277956 -2.2542424 -2.1506457 -2.2606552 -2.3684509 -2.3501608][-2.6305556 -2.5369866 -2.3412416 -2.0784924 -1.852809 -1.7049391 -1.6363866 -1.6777263 -1.6115904 -1.4753451 -1.4225602 -1.5361478 -1.8801477 -2.2116458 -2.3842866][-2.5898805 -2.5897117 -2.4027832 -2.1028922 -1.8379099 -1.6571536 -1.5328107 -1.4738572 -1.3414979 -1.2029781 -1.2195876 -1.4361989 -1.8810334 -2.3285911 -2.5959239][-2.236222 -2.359051 -2.3066959 -2.1309841 -1.9870133 -1.8890862 -1.8268049 -1.7946165 -1.7279978 -1.6873147 -1.7713449 -1.9852107 -2.3062942 -2.5904944 -2.732779][-1.8191535 -2.0301206 -2.1207902 -2.1222708 -2.1448863 -2.1540768 -2.1492991 -2.1624417 -2.2092829 -2.30654 -2.4543254 -2.6122365 -2.7280135 -2.7733126 -2.7479029][-1.7607141 -1.9351249 -2.0629568 -2.1599929 -2.2744055 -2.3581061 -2.4199877 -2.4960971 -2.610445 -2.7434731 -2.8629446 -2.9281576 -2.9117751 -2.8611875 -2.8250432][-1.9978337 -2.0804226 -2.1872141 -2.3131604 -2.4487319 -2.566555 -2.6669464 -2.7701254 -2.8737965 -2.9570951 -3.0118752 -3.012239 -2.9657817 -2.9438119 -2.9665208][-2.294095 -2.3460095 -2.4255717 -2.5194025 -2.5994883 -2.6647892 -2.7178822 -2.776494 -2.8209493 -2.8362303 -2.8348083 -2.8061776 -2.7770097 -2.7998891 -2.8636851][-2.3724768 -2.4222713 -2.4765592 -2.529891 -2.5581751 -2.5733175 -2.5787241 -2.5858884 -2.5846725 -2.5639558 -2.52462 -2.4659686 -2.4331298 -2.4654779 -2.5275407]]...]
INFO - root - 2017-12-07 09:31:32.998125: step 7710, loss = 0.81, batch loss = 0.74 (6.9 examples/sec; 4.658 sec/batch; 97h:34m:25s remains)
INFO - root - 2017-12-07 09:32:19.326629: step 7720, loss = 0.81, batch loss = 0.73 (6.9 examples/sec; 4.637 sec/batch; 97h:08m:03s remains)
INFO - root - 2017-12-07 09:33:05.638394: step 7730, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.645 sec/batch; 97h:16m:17s remains)
INFO - root - 2017-12-07 09:33:52.356099: step 7740, loss = 0.86, batch loss = 0.79 (6.8 examples/sec; 4.679 sec/batch; 97h:59m:18s remains)
INFO - root - 2017-12-07 09:34:38.777395: step 7750, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 4.631 sec/batch; 96h:57m:50s remains)
INFO - root - 2017-12-07 09:35:25.269989: step 7760, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.647 sec/batch; 97h:16m:52s remains)
INFO - root - 2017-12-07 09:36:11.898678: step 7770, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.656 sec/batch; 97h:27m:56s remains)
INFO - root - 2017-12-07 09:36:58.547958: step 7780, loss = 0.71, batch loss = 0.64 (6.8 examples/sec; 4.688 sec/batch; 98h:07m:32s remains)
INFO - root - 2017-12-07 09:37:44.977154: step 7790, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.642 sec/batch; 97h:09m:00s remains)
INFO - root - 2017-12-07 09:38:31.386093: step 7800, loss = 0.78, batch loss = 0.70 (6.9 examples/sec; 4.665 sec/batch; 97h:36m:10s remains)
2017-12-07 09:38:34.118611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2164695 -2.7954535 -2.1984794 -1.8613913 -1.8974042 -1.9931452 -1.9687197 -2.012923 -1.9542272 -1.5901189 -1.2049942 -1.305325 -1.6054568 -1.6782191 -1.5402141][-3.0728421 -2.6379356 -2.0374491 -1.7220628 -1.8751857 -2.1872838 -2.2686379 -2.3506386 -2.3631451 -2.0843558 -1.7724652 -1.8970037 -2.166595 -2.1707075 -1.9770949][-2.6686869 -2.3796725 -1.9444735 -1.7575407 -2.0229025 -2.4510639 -2.5871191 -2.6356444 -2.67072 -2.5650222 -2.4611197 -2.5768771 -2.7051132 -2.6452518 -2.5022128][-2.3276939 -2.2109113 -1.9999795 -1.927779 -2.1576269 -2.5162737 -2.6513243 -2.6651878 -2.6704245 -2.7208529 -2.8417835 -2.8932869 -2.8340116 -2.7340169 -2.6993322][-2.2517936 -2.184222 -2.0940311 -2.0641873 -2.199553 -2.4228728 -2.49924 -2.4168177 -2.2923176 -2.4308805 -2.733063 -2.779042 -2.6293395 -2.5470223 -2.5994964][-2.5827193 -2.4183707 -2.2485173 -2.0791986 -1.9858317 -2.0370491 -2.0072424 -1.787323 -1.5483871 -1.7984271 -2.3232987 -2.4615183 -2.3119252 -2.2716711 -2.3916469][-2.9388325 -2.7228184 -2.4945493 -2.1811073 -1.8527825 -1.6830366 -1.4467857 -0.99625015 -0.6514957 -1.0841243 -1.8654494 -2.1819158 -2.136353 -2.1985695 -2.3914404][-2.9626737 -2.7822137 -2.60391 -2.2789326 -1.8532546 -1.5366964 -1.1477263 -0.52780437 -0.12501049 -0.672755 -1.5789964 -2.061295 -2.1952782 -2.3872485 -2.5976262][-2.9927707 -2.8577394 -2.7322147 -2.4572902 -2.0236316 -1.6720424 -1.3543956 -0.90774941 -0.6881032 -1.1673865 -1.8799787 -2.2814403 -2.5105975 -2.7523732 -2.8496528][-3.3381152 -3.226119 -3.1013091 -2.8331981 -2.3941243 -2.0379786 -1.9233439 -1.8369677 -1.8620994 -2.1971748 -2.5354548 -2.6789122 -2.8601785 -3.0541186 -3.0118334][-3.8140039 -3.6968663 -3.5521655 -3.2852287 -2.8724468 -2.5160496 -2.5291085 -2.6618681 -2.7948241 -2.9666483 -2.9578862 -2.8355846 -2.8945909 -3.0247915 -2.9492264][-3.8874674 -3.7725632 -3.6795292 -3.5170932 -3.2012172 -2.8685977 -2.863292 -2.9875455 -3.0784574 -3.1071918 -2.9065924 -2.6380658 -2.6115093 -2.7630839 -2.7832127][-3.5860937 -3.4450614 -3.431247 -3.4639361 -3.3369479 -3.0731673 -2.9686759 -2.9952502 -3.0703652 -3.0482635 -2.7870309 -2.4548237 -2.377002 -2.5701864 -2.7291226][-3.2432942 -3.0503569 -3.05447 -3.2070587 -3.2627373 -3.1107852 -2.9465232 -2.9041166 -2.9899888 -2.9784441 -2.7595978 -2.5009522 -2.4824154 -2.7451642 -2.999886][-2.8118286 -2.6061521 -2.575551 -2.7451324 -2.9112258 -2.8560073 -2.670198 -2.5793924 -2.6500278 -2.6758494 -2.6084423 -2.5790033 -2.7202811 -3.0411634 -3.3016295]]...]
INFO - root - 2017-12-07 09:39:20.456090: step 7810, loss = 0.72, batch loss = 0.65 (6.8 examples/sec; 4.707 sec/batch; 98h:28m:31s remains)
INFO - root - 2017-12-07 09:40:07.131631: step 7820, loss = 0.94, batch loss = 0.86 (7.0 examples/sec; 4.568 sec/batch; 95h:33m:34s remains)
INFO - root - 2017-12-07 09:40:53.886589: step 7830, loss = 0.75, batch loss = 0.67 (6.9 examples/sec; 4.662 sec/batch; 97h:30m:48s remains)
INFO - root - 2017-12-07 09:41:40.205505: step 7840, loss = 0.80, batch loss = 0.72 (6.9 examples/sec; 4.644 sec/batch; 97h:07m:14s remains)
INFO - root - 2017-12-07 09:42:26.513293: step 7850, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.641 sec/batch; 97h:02m:17s remains)
INFO - root - 2017-12-07 09:43:13.142732: step 7860, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.664 sec/batch; 97h:30m:02s remains)
INFO - root - 2017-12-07 09:43:59.389817: step 7870, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.620 sec/batch; 96h:35m:09s remains)
INFO - root - 2017-12-07 09:44:45.443477: step 7880, loss = 0.78, batch loss = 0.71 (6.9 examples/sec; 4.607 sec/batch; 96h:17m:54s remains)
INFO - root - 2017-12-07 09:45:31.427849: step 7890, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.623 sec/batch; 96h:36m:58s remains)
INFO - root - 2017-12-07 09:46:17.762349: step 7900, loss = 0.83, batch loss = 0.76 (6.9 examples/sec; 4.641 sec/batch; 96h:59m:09s remains)
2017-12-07 09:46:20.531356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2783267 -2.1504064 -2.8531537 -3.1743169 -3.1912446 -2.8534665 -2.4371071 -2.235868 -1.6782568 -1.3426309 -1.7701733 -2.2535539 -2.7153139 -3.2941155 -3.6806245][-1.1872785 -2.0343955 -2.6102014 -2.817873 -2.7572527 -2.4365377 -2.1607156 -2.093931 -1.6544662 -1.4458988 -2.028055 -2.6002064 -3.0033872 -3.5708017 -3.9386036][-1.0332167 -1.6914289 -2.0258269 -2.1172886 -2.0554709 -1.7745135 -1.5974483 -1.7265604 -1.5686321 -1.5137417 -2.1045728 -2.6720881 -3.0076222 -3.532795 -3.9153454][-0.86325693 -1.2769148 -1.3689418 -1.4106243 -1.4234769 -1.2198558 -1.2445364 -1.660182 -1.7660072 -1.7409697 -2.1936336 -2.6273317 -2.8370538 -3.2471082 -3.6199741][-0.81922126 -1.0655184 -1.0272989 -1.1255572 -1.2316966 -1.1378603 -1.415139 -1.9963791 -2.0627606 -1.8314769 -2.0348127 -2.2874963 -2.3738689 -2.6822875 -3.1114736][-0.99342155 -1.2198284 -1.2335079 -1.510222 -1.7112458 -1.7047133 -2.0530965 -2.4434454 -2.1111491 -1.5701313 -1.5073509 -1.5570819 -1.6144471 -1.982163 -2.5902069][-1.0892327 -1.3566816 -1.5069091 -1.9647808 -2.2684612 -2.3265021 -2.568687 -2.6339846 -1.9644918 -1.2512183 -0.97700262 -0.87004566 -1.0116062 -1.4995694 -2.2645769][-1.0858123 -1.4042878 -1.6341984 -2.1431231 -2.4811864 -2.5244312 -2.6322119 -2.5489302 -1.9047318 -1.290339 -0.98385525 -0.86061835 -1.1215935 -1.6297615 -2.3288386][-1.3883841 -1.6998041 -1.9307821 -2.3731132 -2.6592407 -2.6407409 -2.6593106 -2.6303315 -2.2533352 -1.9359674 -1.8059332 -1.8575714 -2.2217941 -2.6225624 -3.0749345][-1.4936702 -1.7593224 -1.9779367 -2.326298 -2.569778 -2.5285416 -2.5214458 -2.6125245 -2.4822264 -2.4158876 -2.5164428 -2.7840736 -3.2223058 -3.5589848 -3.8075812][-1.4483767 -1.6552835 -1.8349802 -2.0533957 -2.254493 -2.2667398 -2.2765341 -2.4002986 -2.4152527 -2.5351963 -2.8060737 -3.1877942 -3.6263709 -3.9362772 -4.0692186][-1.7360973 -1.8796899 -1.8904276 -1.9179473 -2.0966625 -2.208 -2.2488232 -2.3790109 -2.5174768 -2.7666483 -3.0898466 -3.4430704 -3.7673225 -3.9822388 -4.0305824][-2.6263976 -2.7395883 -2.5335274 -2.3973918 -2.5198898 -2.6081247 -2.5931516 -2.6870902 -2.8759758 -3.1359415 -3.3657455 -3.5699561 -3.7092247 -3.7804117 -3.7569466][-3.7354469 -3.8467424 -3.5702188 -3.4126256 -3.4861035 -3.4617839 -3.3463328 -3.3666675 -3.5110857 -3.6642671 -3.7266245 -3.752249 -3.7270029 -3.6890476 -3.650115][-4.0684037 -4.1884241 -3.9668441 -3.8672512 -3.9545097 -3.895596 -3.7646079 -3.7597244 -3.8482876 -3.9115975 -3.8900421 -3.8309178 -3.7404308 -3.6718833 -3.6531987]]...]
INFO - root - 2017-12-07 09:47:07.020767: step 7910, loss = 0.81, batch loss = 0.73 (6.8 examples/sec; 4.731 sec/batch; 98h:51m:17s remains)
INFO - root - 2017-12-07 09:47:53.815475: step 7920, loss = 0.71, batch loss = 0.64 (6.7 examples/sec; 4.762 sec/batch; 99h:29m:17s remains)
INFO - root - 2017-12-07 09:48:40.343761: step 7930, loss = 0.83, batch loss = 0.76 (6.9 examples/sec; 4.668 sec/batch; 97h:30m:38s remains)
INFO - root - 2017-12-07 09:49:27.267230: step 7940, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.639 sec/batch; 96h:52m:49s remains)
INFO - root - 2017-12-07 09:50:13.749285: step 7950, loss = 0.71, batch loss = 0.63 (6.8 examples/sec; 4.685 sec/batch; 97h:49m:19s remains)
INFO - root - 2017-12-07 09:51:00.469773: step 7960, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 4.630 sec/batch; 96h:40m:24s remains)
INFO - root - 2017-12-07 09:51:46.771720: step 7970, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 4.644 sec/batch; 96h:56m:43s remains)
INFO - root - 2017-12-07 09:52:33.412800: step 7980, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.637 sec/batch; 96h:47m:33s remains)
INFO - root - 2017-12-07 09:53:19.898602: step 7990, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 4.567 sec/batch; 95h:18m:32s remains)
INFO - root - 2017-12-07 09:54:06.452244: step 8000, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.603 sec/batch; 96h:02m:48s remains)
2017-12-07 09:54:09.125233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7318451 -2.7473905 -2.7974758 -2.8698995 -2.969789 -3.0055771 -3.0379133 -3.1027775 -3.1167707 -3.106797 -3.0729518 -2.9917467 -2.903121 -2.8039136 -2.7011909][-3.2673929 -3.2688498 -3.2859483 -3.2833438 -3.2766888 -3.2226934 -3.2137265 -3.2615209 -3.2370815 -3.2007952 -3.1809812 -3.1272044 -3.0399675 -2.9607887 -2.919606][-3.55716 -3.5053766 -3.4601979 -3.3565288 -3.2651768 -3.2116983 -3.2400491 -3.2936516 -3.2266 -3.2028742 -3.2313628 -3.1899366 -3.0480301 -2.9339252 -2.9262657][-3.6295695 -3.4117424 -3.2010891 -2.9328947 -2.7811017 -2.7735028 -2.8930619 -3.0131354 -2.9859972 -3.0572879 -3.1499243 -3.1104088 -2.9258566 -2.7996731 -2.8028584][-3.3438249 -3.0434694 -2.7172041 -2.3110521 -2.1073287 -2.0538235 -2.1160407 -2.1955018 -2.1785252 -2.3339267 -2.4436884 -2.4073694 -2.2739272 -2.2298026 -2.2911253][-2.9366465 -2.7623794 -2.4931629 -2.0922561 -1.8663843 -1.6269794 -1.4241412 -1.3158457 -1.2760561 -1.5495465 -1.7316463 -1.7816377 -1.7783542 -1.8261118 -1.9174035][-2.7301126 -2.6488152 -2.4101164 -2.0099344 -1.7004473 -1.2101462 -0.76442552 -0.52074981 -0.4343009 -0.80673027 -1.1310837 -1.4078927 -1.6489308 -1.801424 -1.9195995][-2.2901256 -2.2831955 -2.0755997 -1.6974969 -1.3073323 -0.68374228 -0.28203535 -0.13333368 -0.041021824 -0.43392396 -0.84465647 -1.2930186 -1.6997766 -1.8880744 -2.0084157][-2.3601756 -2.362355 -2.1859241 -1.8813572 -1.5005457 -0.93981647 -0.81106997 -0.90227079 -0.85767937 -1.231365 -1.648351 -2.0802433 -2.4294045 -2.5029581 -2.5403233][-3.0166097 -3.039552 -2.9326234 -2.7448864 -2.4387546 -1.9907894 -2.0047181 -2.1116035 -1.9870872 -2.2996471 -2.6775167 -2.9988928 -3.2177854 -3.1947131 -3.1823375][-3.6499062 -3.8012109 -3.8062296 -3.7173638 -3.480607 -3.1061339 -3.1139202 -3.0841739 -2.847353 -3.0535548 -3.2952127 -3.4525318 -3.5624483 -3.5220239 -3.5379834][-3.8119822 -4.170547 -4.3477154 -4.393394 -4.257905 -3.9658408 -3.9444175 -3.8427076 -3.6202915 -3.7597556 -3.8345826 -3.8089767 -3.8040323 -3.7197869 -3.7020125][-3.6464074 -4.1156349 -4.442524 -4.6787572 -4.7211046 -4.5669003 -4.5411911 -4.437253 -4.2876225 -4.3573103 -4.29972 -4.1422043 -4.034955 -3.8769577 -3.7535391][-3.5124843 -3.941714 -4.3101726 -4.6878009 -4.9066734 -4.9104571 -4.9056835 -4.8004417 -4.6641488 -4.6294103 -4.4945025 -4.2862196 -4.1147246 -3.8996952 -3.6778305][-3.5324128 -3.8178434 -4.1290841 -4.5181732 -4.80264 -4.9020495 -4.9184275 -4.8168416 -4.6836338 -4.5968485 -4.4820023 -4.3403678 -4.1983247 -4.0294757 -3.8050408]]...]
INFO - root - 2017-12-07 09:54:55.653460: step 8010, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.630 sec/batch; 96h:36m:04s remains)
INFO - root - 2017-12-07 09:55:42.093735: step 8020, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.642 sec/batch; 96h:50m:11s remains)
INFO - root - 2017-12-07 09:56:28.410560: step 8030, loss = 0.76, batch loss = 0.69 (6.8 examples/sec; 4.713 sec/batch; 98h:18m:20s remains)
INFO - root - 2017-12-07 09:57:14.669117: step 8040, loss = 0.74, batch loss = 0.66 (6.9 examples/sec; 4.620 sec/batch; 96h:22m:07s remains)
INFO - root - 2017-12-07 09:58:00.997960: step 8050, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.648 sec/batch; 96h:56m:18s remains)
INFO - root - 2017-12-07 09:58:47.396672: step 8060, loss = 0.78, batch loss = 0.71 (6.8 examples/sec; 4.680 sec/batch; 97h:34m:42s remains)
INFO - root - 2017-12-07 09:59:33.558214: step 8070, loss = 0.79, batch loss = 0.71 (6.9 examples/sec; 4.633 sec/batch; 96h:36m:00s remains)
INFO - root - 2017-12-07 10:00:19.775160: step 8080, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.648 sec/batch; 96h:53m:46s remains)
INFO - root - 2017-12-07 10:01:06.149288: step 8090, loss = 0.74, batch loss = 0.66 (6.9 examples/sec; 4.659 sec/batch; 97h:06m:44s remains)
INFO - root - 2017-12-07 10:01:52.205696: step 8100, loss = 0.76, batch loss = 0.69 (6.8 examples/sec; 4.683 sec/batch; 97h:35m:35s remains)
2017-12-07 10:01:55.003813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1093714 -2.9639082 -2.8766723 -2.8866916 -3.0187116 -3.1862164 -3.2722993 -3.2490039 -3.1514962 -3.1755106 -3.394176 -3.6685221 -3.8532894 -3.9291263 -4.0317712][-2.9866333 -2.8638985 -2.7115474 -2.6497407 -2.751349 -2.8952305 -2.9541445 -2.9175193 -2.8005836 -2.8163333 -3.0687523 -3.4231336 -3.7014055 -3.8659115 -4.0179057][-2.9295478 -2.8881636 -2.7347555 -2.6296484 -2.6722555 -2.7420413 -2.7376409 -2.6855493 -2.5632088 -2.5453329 -2.7585802 -3.1249671 -3.4491374 -3.6705859 -3.8427925][-2.788928 -2.861743 -2.7850075 -2.6962724 -2.6725888 -2.6264372 -2.52776 -2.4413376 -2.3312969 -2.271301 -2.3904681 -2.713963 -3.06715 -3.3318524 -3.5242546][-2.4180658 -2.5598011 -2.5882397 -2.5770402 -2.5283775 -2.3815382 -2.2012928 -2.0994217 -2.0388703 -1.9958675 -2.0678918 -2.3416493 -2.6940372 -2.9765124 -3.1914871][-1.8947144 -1.9852407 -2.0861819 -2.1778245 -2.1717157 -2.0104651 -1.8294628 -1.7677252 -1.7882991 -1.8286266 -1.9623876 -2.2564659 -2.6087615 -2.8776989 -3.0895224][-1.4996092 -1.4606276 -1.5446398 -1.6745734 -1.6985285 -1.5667386 -1.4350326 -1.4326253 -1.5277433 -1.6842012 -1.9664147 -2.3679295 -2.7616782 -3.0072703 -3.1623368][-1.4968772 -1.3138032 -1.2493439 -1.2611191 -1.2260568 -1.1053562 -1.0116198 -1.0359285 -1.1944442 -1.5048597 -1.9715548 -2.4919114 -2.9371419 -3.1741238 -3.252357][-1.8613791 -1.5970745 -1.3712695 -1.1923625 -1.0186033 -0.84529853 -0.719506 -0.68704796 -0.85675693 -1.2814324 -1.8650994 -2.4567142 -2.9862714 -3.3039203 -3.381362][-2.34781 -2.1033881 -1.8492665 -1.5759246 -1.2738893 -0.98730826 -0.73791265 -0.5734961 -0.6630733 -1.0579822 -1.5940287 -2.1442747 -2.7314515 -3.1869249 -3.3903503][-2.8250513 -2.6293569 -2.4702666 -2.2637358 -1.9507499 -1.5791893 -1.1941464 -0.87403321 -0.78508425 -0.98760986 -1.3045335 -1.6677365 -2.1806159 -2.6884952 -3.014581][-3.1908545 -3.0101929 -2.9169455 -2.8395486 -2.6669874 -2.3662112 -1.9876907 -1.614583 -1.3542418 -1.2729211 -1.26232 -1.3272095 -1.614099 -2.0058172 -2.3232706][-3.373781 -3.2303872 -3.1438527 -3.1345148 -3.1207261 -3.01691 -2.8368123 -2.5721543 -2.2436042 -1.9263425 -1.6260204 -1.393786 -1.3832614 -1.5411258 -1.7293346][-3.6134915 -3.510994 -3.3634708 -3.3023472 -3.3299844 -3.3818345 -3.4153953 -3.3219182 -3.0801094 -2.7302175 -2.3166492 -1.9080997 -1.663502 -1.5835602 -1.5847101][-3.7605672 -3.7890639 -3.6585231 -3.5128398 -3.4655509 -3.54402 -3.6773603 -3.7138824 -3.6035705 -3.351557 -3.0185628 -2.6328633 -2.3065622 -2.0677478 -1.9147544]]...]
INFO - root - 2017-12-07 10:02:41.250566: step 8110, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.574 sec/batch; 95h:18m:32s remains)
INFO - root - 2017-12-07 10:03:27.792269: step 8120, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.648 sec/batch; 96h:50m:56s remains)
INFO - root - 2017-12-07 10:04:14.390132: step 8130, loss = 0.86, batch loss = 0.79 (6.7 examples/sec; 4.808 sec/batch; 100h:09m:08s remains)
INFO - root - 2017-12-07 10:05:00.629936: step 8140, loss = 0.68, batch loss = 0.61 (6.8 examples/sec; 4.682 sec/batch; 97h:31m:49s remains)
INFO - root - 2017-12-07 10:05:47.158414: step 8150, loss = 0.89, batch loss = 0.82 (6.7 examples/sec; 4.783 sec/batch; 99h:37m:12s remains)
INFO - root - 2017-12-07 10:06:33.701849: step 8160, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.555 sec/batch; 94h:51m:32s remains)
INFO - root - 2017-12-07 10:07:20.335529: step 8170, loss = 0.83, batch loss = 0.76 (6.8 examples/sec; 4.674 sec/batch; 97h:19m:30s remains)
INFO - root - 2017-12-07 10:08:07.082999: step 8180, loss = 0.80, batch loss = 0.72 (6.8 examples/sec; 4.730 sec/batch; 98h:27m:54s remains)
INFO - root - 2017-12-07 10:08:45.150940: step 8190, loss = 0.75, batch loss = 0.68 (9.2 examples/sec; 3.489 sec/batch; 72h:37m:36s remains)
INFO - root - 2017-12-07 10:09:20.133019: step 8200, loss = 0.79, batch loss = 0.72 (9.0 examples/sec; 3.541 sec/batch; 73h:42m:15s remains)
2017-12-07 10:09:22.190781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0943527 -1.8501892 -1.672859 -1.64872 -1.6741662 -1.6471043 -1.6561155 -1.6446321 -1.6506443 -1.7811813 -1.943526 -2.1366756 -2.4576738 -2.6935878 -2.7975669][-2.1211619 -1.8947606 -1.765887 -1.8061838 -1.8681555 -1.839992 -1.7695472 -1.645596 -1.5635228 -1.5964754 -1.6874094 -1.9497292 -2.4146976 -2.8456225 -3.1549103][-2.2783093 -2.1171331 -2.0478239 -2.0437546 -1.9241345 -1.6908028 -1.4613011 -1.3233287 -1.3982868 -1.51547 -1.5920851 -1.8794856 -2.3602967 -2.7865534 -3.1107144][-2.4824762 -2.3215022 -2.1869402 -2.0085428 -1.6437109 -1.2322891 -0.86572146 -0.73999619 -1.0033972 -1.1921642 -1.2696698 -1.6530364 -2.1809337 -2.5812907 -2.852664][-2.5347879 -2.2800462 -2.0165544 -1.6842861 -1.1792223 -0.65690589 -0.096497059 0.1335907 -0.27158022 -0.60272121 -0.86008668 -1.4846511 -2.1290703 -2.5301161 -2.7089958][-2.3905411 -2.0745955 -1.7517307 -1.3615375 -0.71021175 0.11621475 1.1016054 1.4911776 0.77854824 0.017809391 -0.69599652 -1.6359744 -2.3063297 -2.6111255 -2.6466351][-2.2816393 -1.9377611 -1.5430505 -1.0249012 -0.0813694 1.2070775 2.6122031 2.9816198 1.7562952 0.40284872 -0.81437516 -1.963131 -2.5624192 -2.7689166 -2.6987109][-2.2138877 -1.7725439 -1.2471278 -0.59523845 0.49186134 1.8434429 3.1153097 3.0659618 1.3670106 -0.27454615 -1.5210433 -2.4388876 -2.7496476 -2.7930312 -2.6287675][-2.1168439 -1.5882058 -1.0125523 -0.42022467 0.43265057 1.2841754 1.8869696 1.4551244 -0.15149021 -1.5136695 -2.3623323 -2.8353148 -2.8424673 -2.7242854 -2.5026414][-2.0872455 -1.5558796 -1.0708323 -0.71406722 -0.29964018 -0.0018634796 0.19760227 -0.1583457 -1.2940784 -2.2025106 -2.701529 -2.9729919 -2.9224138 -2.7685738 -2.5734315][-2.1871784 -1.8102126 -1.5098712 -1.3751061 -1.2539752 -1.2184904 -1.1072116 -1.2736256 -1.9847872 -2.5384245 -2.8338442 -3.0332861 -2.9990981 -2.876883 -2.7516065][-2.2906735 -2.1047151 -1.9341929 -1.8981824 -1.919282 -1.9909356 -1.8950083 -1.911871 -2.3098552 -2.6063521 -2.7993374 -2.9947977 -3.0144565 -2.9756684 -2.9484034][-2.243017 -2.1727934 -2.0790374 -2.1164408 -2.2507615 -2.3970563 -2.3213277 -2.219914 -2.3681269 -2.4881845 -2.6248822 -2.8282964 -2.9156494 -2.9646595 -3.0099523][-2.1461532 -2.1446803 -2.1166465 -2.209367 -2.4091032 -2.5906734 -2.5633621 -2.4415202 -2.4715285 -2.5262847 -2.6308174 -2.7832499 -2.84055 -2.8544831 -2.8684559][-2.1432745 -2.1743243 -2.1959226 -2.2809191 -2.4117308 -2.5206637 -2.5096309 -2.4471529 -2.4777775 -2.5376065 -2.6070981 -2.6704669 -2.6613739 -2.6298549 -2.6313453]]...]
INFO - root - 2017-12-07 10:09:56.894205: step 8210, loss = 0.85, batch loss = 0.78 (9.1 examples/sec; 3.523 sec/batch; 73h:18m:26s remains)
INFO - root - 2017-12-07 10:10:31.707555: step 8220, loss = 0.71, batch loss = 0.64 (9.2 examples/sec; 3.462 sec/batch; 72h:02m:05s remains)
INFO - root - 2017-12-07 10:11:06.135850: step 8230, loss = 0.81, batch loss = 0.74 (9.6 examples/sec; 3.338 sec/batch; 69h:26m:19s remains)
INFO - root - 2017-12-07 10:11:41.055224: step 8240, loss = 0.73, batch loss = 0.66 (9.2 examples/sec; 3.463 sec/batch; 72h:02m:39s remains)
INFO - root - 2017-12-07 10:12:15.170729: step 8250, loss = 0.79, batch loss = 0.71 (9.8 examples/sec; 3.268 sec/batch; 67h:57m:36s remains)
INFO - root - 2017-12-07 10:12:49.381410: step 8260, loss = 0.79, batch loss = 0.72 (9.4 examples/sec; 3.393 sec/batch; 70h:33m:33s remains)
INFO - root - 2017-12-07 10:13:23.466799: step 8270, loss = 0.72, batch loss = 0.65 (9.4 examples/sec; 3.403 sec/batch; 70h:45m:04s remains)
INFO - root - 2017-12-07 10:13:57.648429: step 8280, loss = 0.80, batch loss = 0.73 (9.4 examples/sec; 3.412 sec/batch; 70h:56m:25s remains)
INFO - root - 2017-12-07 10:14:40.053431: step 8290, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.530 sec/batch; 94h:09m:38s remains)
INFO - root - 2017-12-07 10:15:24.542666: step 8300, loss = 0.83, batch loss = 0.76 (7.2 examples/sec; 4.439 sec/batch; 92h:16m:15s remains)
2017-12-07 10:15:27.146930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4275956 -3.3885684 -3.4460261 -3.6164362 -3.6246152 -3.2705779 -2.7717829 -2.6773634 -3.0645475 -3.5777204 -3.8304355 -3.788671 -3.6578112 -3.5432258 -3.4703538][-3.6224816 -3.5904016 -3.7404695 -4.09937 -4.1181383 -3.4727488 -2.5148129 -2.124449 -2.5960021 -3.4552374 -4.0125589 -4.0650215 -3.9144735 -3.7618313 -3.6782446][-3.5893693 -3.5126414 -3.6778817 -4.1309862 -4.1052151 -3.2247162 -1.9029093 -1.224781 -1.7996945 -3.0596642 -3.9866664 -4.1725044 -3.9860582 -3.752336 -3.6165819][-3.4863482 -3.2928925 -3.3489943 -3.7868617 -3.7697816 -2.8459826 -1.3287933 -0.36007547 -0.95235515 -2.5654485 -3.8928587 -4.3158236 -4.1168962 -3.7223227 -3.4488261][-3.3568301 -2.9152422 -2.6612854 -2.9840975 -3.142189 -2.4492626 -0.92411423 0.3437295 -0.098577976 -1.9653118 -3.7180805 -4.502192 -4.3754506 -3.8053579 -3.3544288][-3.2791309 -2.5691137 -1.8768063 -1.9176927 -2.1967549 -1.7840886 -0.40331316 0.99045372 0.645906 -1.4295413 -3.5404866 -4.6388679 -4.5961103 -3.9113984 -3.3172355][-3.3246083 -2.4980788 -1.4822569 -1.2091076 -1.4298532 -1.0932262 0.18977404 1.5834227 1.2249441 -1.0267241 -3.3720243 -4.6544204 -4.6966252 -4.005022 -3.3613129][-3.3905344 -2.6124372 -1.5663376 -1.2426224 -1.4951069 -1.2412536 -0.08950901 1.2511349 1.0213175 -1.0409636 -3.2376406 -4.4776325 -4.6023221 -4.0206413 -3.425518][-3.4505162 -2.8169875 -1.9373381 -1.727833 -2.0533466 -1.9567323 -1.111125 -0.019725323 -0.1204133 -1.6898546 -3.3650913 -4.3356137 -4.4689755 -3.9922314 -3.4428673][-3.5146732 -3.061996 -2.4094956 -2.2631676 -2.5152159 -2.4646244 -1.8704095 -1.0656378 -1.1158285 -2.2915442 -3.5232432 -4.2713666 -4.4221416 -4.0288181 -3.4847124][-3.6166034 -3.3864565 -2.9967728 -2.8929298 -2.9712274 -2.8138089 -2.273159 -1.5933971 -1.6074054 -2.558712 -3.5486698 -4.1798782 -4.3722916 -4.0682678 -3.5247085][-3.7191007 -3.7318873 -3.6030073 -3.551985 -3.5003374 -3.2510819 -2.7036338 -2.065407 -2.0246058 -2.807785 -3.6134009 -4.1347542 -4.3711586 -4.1204748 -3.5039968][-3.5599873 -3.749738 -3.7974808 -3.8018577 -3.7375696 -3.5226173 -3.0592494 -2.5171018 -2.4537511 -3.0406055 -3.6253204 -4.040998 -4.3512664 -4.15643 -3.4048352][-3.0939717 -3.4728694 -3.6998034 -3.7545917 -3.7099442 -3.5779109 -3.272336 -2.9195025 -2.8788919 -3.227366 -3.5547519 -3.8654346 -4.2390704 -4.110795 -3.2794085][-2.4752769 -3.10388 -3.5446429 -3.6630645 -3.6378245 -3.5787923 -3.4102774 -3.2609215 -3.2884357 -3.4261518 -3.5304692 -3.6916759 -4.0474772 -4.0066557 -3.2021551]]...]
INFO - root - 2017-12-07 10:16:11.906010: step 8310, loss = 0.69, batch loss = 0.62 (7.1 examples/sec; 4.500 sec/batch; 93h:30m:40s remains)
INFO - root - 2017-12-07 10:16:56.694293: step 8320, loss = 0.82, batch loss = 0.74 (7.1 examples/sec; 4.522 sec/batch; 93h:57m:12s remains)
INFO - root - 2017-12-07 10:17:42.314165: step 8330, loss = 0.86, batch loss = 0.79 (6.9 examples/sec; 4.645 sec/batch; 96h:30m:19s remains)
INFO - root - 2017-12-07 10:18:28.206951: step 8340, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.564 sec/batch; 94h:48m:49s remains)
INFO - root - 2017-12-07 10:19:14.198045: step 8350, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 4.485 sec/batch; 93h:09m:34s remains)
INFO - root - 2017-12-07 10:19:59.908105: step 8360, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.571 sec/batch; 94h:55m:34s remains)
INFO - root - 2017-12-07 10:20:45.682324: step 8370, loss = 0.74, batch loss = 0.66 (7.0 examples/sec; 4.551 sec/batch; 94h:29m:49s remains)
INFO - root - 2017-12-07 10:21:31.325962: step 8380, loss = 0.69, batch loss = 0.62 (7.1 examples/sec; 4.521 sec/batch; 93h:52m:12s remains)
INFO - root - 2017-12-07 10:22:16.788928: step 8390, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.601 sec/batch; 95h:31m:32s remains)
INFO - root - 2017-12-07 10:23:02.180389: step 8400, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.670 sec/batch; 96h:56m:03s remains)
2017-12-07 10:23:04.888958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3000073 -2.2387896 -2.2874684 -2.172924 -2.0850167 -2.2636659 -2.3861711 -2.4026439 -2.2628343 -1.9312961 -1.5425162 -1.2552414 -1.4950383 -2.0294561 -2.3997822][-2.0045912 -1.942945 -1.9351315 -1.6483889 -1.4174523 -1.7070203 -1.9543362 -2.1022131 -2.1067913 -1.8434134 -1.3918447 -0.925915 -1.055232 -1.6014175 -2.0003254][-2.0680721 -2.0283082 -1.9470441 -1.4844968 -1.0594008 -1.3419747 -1.5658722 -1.8481438 -2.0903997 -1.9518108 -1.4868255 -0.84775639 -0.72549582 -1.0768826 -1.3590729][-2.0940995 -2.0488698 -1.9438887 -1.4546795 -0.99507976 -1.2725184 -1.2438734 -1.4808996 -2.0615225 -2.2655184 -2.0025516 -1.3730664 -0.97936106 -1.0679357 -1.2328227][-2.1303856 -1.9140396 -1.74139 -1.3659894 -1.0918608 -1.3651075 -0.95393229 -0.90541482 -1.6697063 -2.1638045 -2.1225207 -1.5952864 -0.95271349 -0.68186903 -0.61569381][-1.7474632 -1.2361259 -0.98595881 -0.76479816 -0.72095919 -1.0220191 -0.38779306 -0.056780338 -0.80878711 -1.5035267 -1.7275608 -1.48715 -0.90139532 -0.53412223 -0.48558569][-1.8411095 -1.2866666 -1.03613 -0.89678288 -0.93085265 -1.2512155 -0.63429523 -0.15930605 -0.79628181 -1.6313329 -2.2164671 -2.4390025 -2.2189381 -1.9870102 -2.0428796][-2.6777439 -2.2016449 -1.896884 -1.7559445 -1.6995623 -1.8678896 -1.395165 -0.94914007 -1.3992028 -2.1298249 -2.7835798 -3.1599624 -3.1537626 -2.9394083 -2.9148417][-2.9854064 -2.5524979 -2.2633314 -2.1931849 -2.1475847 -2.1690383 -1.9515314 -1.7083251 -1.9017224 -2.3999298 -2.8896425 -3.2300615 -3.5157592 -3.5417175 -3.5590332][-3.357748 -2.8587852 -2.5710347 -2.5819235 -2.6260519 -2.5337944 -2.5001431 -2.4429429 -2.4138224 -2.6277125 -2.8841453 -3.1038172 -3.6220784 -3.9218342 -4.0431843][-3.5713546 -2.890089 -2.6112194 -2.6806626 -2.7525864 -2.5620191 -2.5310018 -2.5185385 -2.3767345 -2.4180169 -2.5109119 -2.5560079 -3.0888505 -3.4892483 -3.6503792][-3.3684902 -2.5859876 -2.3989835 -2.4943795 -2.6340551 -2.4595523 -2.3030756 -2.2157495 -2.044271 -2.0936048 -2.2582126 -2.3567958 -2.9399948 -3.3789945 -3.5294516][-3.5950837 -2.8889127 -2.6867318 -2.6361847 -2.7654305 -2.6940656 -2.5029378 -2.3819644 -2.2749243 -2.3047273 -2.5149348 -2.7065094 -3.2596703 -3.6108088 -3.580914][-3.8654084 -3.4151185 -3.2329473 -3.0472617 -3.1372223 -3.2037702 -3.1445975 -3.1172683 -3.0905106 -3.0246167 -3.1288595 -3.2650976 -3.59727 -3.6998773 -3.4372997][-4.4264145 -4.33738 -4.2311144 -4.0077696 -4.0139241 -4.0589075 -4.0215464 -4.0621176 -4.0749674 -3.9057388 -3.8285935 -3.8058195 -3.8715494 -3.8060741 -3.4747531]]...]
INFO - root - 2017-12-07 10:23:50.713208: step 8410, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.534 sec/batch; 94h:05m:39s remains)
INFO - root - 2017-12-07 10:24:36.367246: step 8420, loss = 0.77, batch loss = 0.69 (6.8 examples/sec; 4.708 sec/batch; 97h:42m:03s remains)
INFO - root - 2017-12-07 10:25:21.881340: step 8430, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.540 sec/batch; 94h:11m:22s remains)
INFO - root - 2017-12-07 10:26:07.274939: step 8440, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.545 sec/batch; 94h:17m:52s remains)
INFO - root - 2017-12-07 10:26:52.977413: step 8450, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.610 sec/batch; 95h:37m:40s remains)
INFO - root - 2017-12-07 10:27:38.646451: step 8460, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.586 sec/batch; 95h:07m:17s remains)
INFO - root - 2017-12-07 10:28:24.360434: step 8470, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.588 sec/batch; 95h:08m:24s remains)
INFO - root - 2017-12-07 10:29:09.897803: step 8480, loss = 0.63, batch loss = 0.56 (7.0 examples/sec; 4.578 sec/batch; 94h:55m:16s remains)
INFO - root - 2017-12-07 10:29:55.828086: step 8490, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.541 sec/batch; 94h:09m:09s remains)
INFO - root - 2017-12-07 10:30:41.290535: step 8500, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.503 sec/batch; 93h:20m:24s remains)
2017-12-07 10:30:43.925593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0705366 -3.006413 -2.8426533 -2.7714972 -2.8541169 -3.0949416 -3.2701788 -3.3809438 -3.3434618 -3.3055849 -3.3870935 -3.456625 -3.5283971 -3.5753064 -3.4936366][-2.6962824 -2.4996076 -2.2713571 -2.1917889 -2.2642188 -2.4522104 -2.578671 -2.6674418 -2.582761 -2.6088738 -2.8554683 -3.0078976 -3.0975652 -3.1721103 -3.1087036][-2.3051236 -2.0379772 -1.8838074 -1.9228127 -1.9546504 -1.9808879 -1.9389701 -1.9030442 -1.7716837 -1.9321654 -2.3505006 -2.54737 -2.6179652 -2.683393 -2.6945143][-1.7605007 -1.4769719 -1.4991794 -1.7244327 -1.7236917 -1.61532 -1.4843636 -1.358386 -1.2777455 -1.6380138 -2.1937284 -2.3932521 -2.360657 -2.3268201 -2.4030285][-1.1015432 -0.75768423 -0.9268465 -1.3178852 -1.288343 -1.0978823 -0.9552989 -0.84382296 -0.9302609 -1.4895995 -2.0575695 -2.1887798 -2.0470483 -1.9048193 -2.0507195][-0.4115243 0.00094509125 -0.279871 -0.758018 -0.60486579 -0.24519634 -0.020748615 -0.045240402 -0.49642515 -1.325135 -1.9345198 -2.0484285 -1.8995728 -1.7624426 -1.9788504][0.16114902 0.66618633 0.33106041 -0.11316538 0.28181171 0.9809947 1.5273857 1.5067501 0.70888281 -0.4476335 -1.2197697 -1.519356 -1.5766356 -1.604131 -1.9175241][0.27879095 0.66956282 0.15452003 -0.29126787 0.2766242 1.2892594 2.2457461 2.4106193 1.5361953 0.34989786 -0.42406273 -0.80964565 -0.98693061 -1.1675687 -1.5795524][-0.01103878 0.17544985 -0.59173226 -1.1623232 -0.75390697 0.16337967 1.1549501 1.4000583 0.69592667 -0.14276505 -0.61539626 -0.82078052 -0.90691447 -1.1140294 -1.5531683][-0.19707537 -0.10187578 -0.93885159 -1.533421 -1.3434074 -0.72950721 -0.016960621 0.069727421 -0.5421226 -1.0376616 -1.1890306 -1.1456616 -1.0252931 -1.1178918 -1.5013509][-0.32258987 -0.26308584 -0.94228005 -1.333353 -1.1780748 -0.76692605 -0.32603264 -0.41004562 -0.97267652 -1.3076282 -1.3324528 -1.1831651 -0.97057343 -0.9832809 -1.2893054][-0.70896816 -0.69297051 -1.1595948 -1.3197565 -1.1266217 -0.8041203 -0.5302825 -0.68730688 -1.1502934 -1.3980889 -1.3785083 -1.2074673 -0.99244642 -0.96632123 -1.1885257][-1.2351766 -1.2310324 -1.5104945 -1.537255 -1.3747916 -1.1739006 -1.0467482 -1.2238171 -1.5396824 -1.7364941 -1.7186003 -1.5765254 -1.4081488 -1.37358 -1.5090411][-1.7522068 -1.7089469 -1.8258586 -1.8136094 -1.7590296 -1.7191777 -1.732842 -1.9091799 -2.08928 -2.1920261 -2.1505487 -2.0057302 -1.8729839 -1.8461366 -1.9420249][-2.2350101 -2.1555536 -2.1664896 -2.1474674 -2.1675179 -2.2159967 -2.2969275 -2.4315674 -2.5094857 -2.5270729 -2.4557509 -2.2992814 -2.1562212 -2.122098 -2.1898074]]...]
INFO - root - 2017-12-07 10:31:29.681870: step 8510, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.564 sec/batch; 94h:35m:56s remains)
INFO - root - 2017-12-07 10:32:15.274433: step 8520, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.594 sec/batch; 95h:11m:52s remains)
INFO - root - 2017-12-07 10:33:01.009525: step 8530, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.672 sec/batch; 96h:47m:50s remains)
INFO - root - 2017-12-07 10:33:46.367643: step 8540, loss = 0.82, batch loss = 0.75 (7.2 examples/sec; 4.442 sec/batch; 92h:01m:22s remains)
INFO - root - 2017-12-07 10:34:31.987002: step 8550, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.534 sec/batch; 93h:55m:31s remains)
INFO - root - 2017-12-07 10:35:17.568006: step 8560, loss = 0.75, batch loss = 0.68 (7.2 examples/sec; 4.440 sec/batch; 91h:57m:17s remains)
INFO - root - 2017-12-07 10:36:03.092723: step 8570, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 4.499 sec/batch; 93h:10m:44s remains)
INFO - root - 2017-12-07 10:36:48.549804: step 8580, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 4.506 sec/batch; 93h:18m:20s remains)
INFO - root - 2017-12-07 10:37:33.875814: step 8590, loss = 0.81, batch loss = 0.73 (7.1 examples/sec; 4.533 sec/batch; 93h:51m:03s remains)
INFO - root - 2017-12-07 10:38:19.744643: step 8600, loss = 0.83, batch loss = 0.75 (7.1 examples/sec; 4.509 sec/batch; 93h:20m:31s remains)
2017-12-07 10:38:22.413699: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.65424347 0.67674732 0.5756135 0.40766764 0.12936592 -0.41158867 -0.95838928 -1.1605113 -0.95107841 -0.55343723 -0.23969173 -0.16200972 -0.29246521 -0.38021517 -0.42612648][0.66307211 0.66483212 0.55446386 0.33183765 -0.034404278 -0.63664031 -1.190671 -1.3724549 -1.1039081 -0.63639593 -0.26423073 -0.14303398 -0.27009964 -0.36728764 -0.42974782][0.6124897 0.64909077 0.57060671 0.31873798 -0.10944366 -0.75824118 -1.3215861 -1.4716024 -1.1271975 -0.60437417 -0.21752453 -0.09764719 -0.23198271 -0.33644724 -0.4170146][0.57857037 0.65721178 0.60857391 0.34227133 -0.10676861 -0.7344811 -1.2485456 -1.3184249 -0.89002156 -0.36167717 -0.042321682 0.0014567375 -0.16292906 -0.27827311 -0.38423014][0.57642984 0.68826389 0.66568661 0.41481161 0.010763645 -0.53943896 -0.98455238 -0.98982573 -0.53830242 -0.056210041 0.17012739 0.12137651 -0.085385323 -0.22950888 -0.35784531][0.56532907 0.68593311 0.69241381 0.50155592 0.18962383 -0.27511549 -0.69887543 -0.71923757 -0.33111954 0.072409153 0.24545336 0.1518259 -0.064919472 -0.22185564 -0.3526268][0.50689793 0.59875679 0.62467957 0.52042437 0.32077456 -0.073663235 -0.50559831 -0.58767962 -0.3094902 0.0055088997 0.14925671 0.057537556 -0.12430573 -0.25594044 -0.36386585][0.44501448 0.49924469 0.53363514 0.4986558 0.37562227 0.018190384 -0.42220116 -0.55271888 -0.3535347 -0.091436863 0.0461545 -0.039549351 -0.19391918 -0.29689646 -0.37253571][0.40931511 0.45771265 0.50682783 0.51273108 0.41029978 0.046549797 -0.40141535 -0.5535295 -0.39264584 -0.13849545 0.013703823 -0.073920727 -0.22873116 -0.3297472 -0.38215446][0.41256809 0.45896053 0.50618887 0.52021027 0.39008284 -0.025637627 -0.4915688 -0.64239311 -0.47451329 -0.189672 0.0048084259 -0.067791939 -0.23589182 -0.36053276 -0.40314531][0.43732738 0.465981 0.48936176 0.48031902 0.27681494 -0.23441124 -0.73247886 -0.85355878 -0.64066958 -0.30953836 -0.08197403 -0.12038755 -0.27407217 -0.40520954 -0.43692684][0.47611761 0.49513149 0.49283504 0.4294405 0.11285734 -0.5095129 -1.040463 -1.1144221 -0.84363413 -0.47543287 -0.23524141 -0.22589779 -0.33479929 -0.439955 -0.45773363][0.55464888 0.600636 0.6139102 0.51695108 0.12024975 -0.57040596 -1.1287196 -1.1989946 -0.92542052 -0.56656671 -0.32589626 -0.27761364 -0.34976006 -0.43059659 -0.44865394][0.59319973 0.68097496 0.73711586 0.63594151 0.22721863 -0.44870663 -1.0028322 -1.0981665 -0.85140634 -0.49574733 -0.24937534 -0.19936466 -0.27714634 -0.36673641 -0.41184187][0.54651785 0.66811085 0.76580477 0.68293142 0.30693913 -0.29477262 -0.78326774 -0.84881234 -0.58892846 -0.21192026 0.019159794 0.0031404495 -0.14392519 -0.27931023 -0.36919165]]...]
INFO - root - 2017-12-07 10:39:07.659609: step 8610, loss = 0.70, batch loss = 0.62 (6.9 examples/sec; 4.613 sec/batch; 95h:28m:37s remains)
INFO - root - 2017-12-07 10:39:53.246041: step 8620, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.566 sec/batch; 94h:29m:32s remains)
INFO - root - 2017-12-07 10:40:38.629699: step 8630, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.551 sec/batch; 94h:10m:39s remains)
INFO - root - 2017-12-07 10:41:24.064489: step 8640, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.543 sec/batch; 93h:59m:49s remains)
INFO - root - 2017-12-07 10:42:09.467913: step 8650, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.592 sec/batch; 95h:00m:02s remains)
INFO - root - 2017-12-07 10:42:55.387031: step 8660, loss = 0.71, batch loss = 0.63 (7.0 examples/sec; 4.551 sec/batch; 94h:07m:56s remains)
INFO - root - 2017-12-07 10:43:40.851359: step 8670, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.539 sec/batch; 93h:51m:54s remains)
INFO - root - 2017-12-07 10:44:26.278360: step 8680, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.498 sec/batch; 93h:00m:31s remains)
INFO - root - 2017-12-07 10:45:11.641670: step 8690, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.560 sec/batch; 94h:17m:26s remains)
INFO - root - 2017-12-07 10:45:57.313638: step 8700, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 4.620 sec/batch; 95h:30m:20s remains)
2017-12-07 10:46:00.036801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3105502 -1.7499933 -0.8482554 0.050034523 0.888535 0.76758575 0.54912758 0.70667028 0.68728733 0.33752012 -1.0578434 -2.3325453 -2.3746004 -1.5473602 -0.318542][-2.5108728 -1.7476742 -0.74164724 0.29141951 1.1252713 0.9763279 0.89319611 1.2551746 1.3184881 0.77566385 -1.0667477 -2.6137795 -2.5106368 -1.2340751 0.18732882][-2.5976772 -1.7602856 -0.77689719 0.28040886 1.0209155 0.86642551 0.97378206 1.5720792 1.7083054 0.97729874 -1.168539 -2.854552 -2.573544 -0.94627404 0.56815815][-2.8446641 -1.9502835 -0.9868722 -0.0014390945 0.5976634 0.43382215 0.74085331 1.540411 1.6383181 0.76078415 -1.4893255 -3.2547102 -2.8533506 -0.88850641 0.7748456][-2.945581 -2.042321 -1.0810144 -0.15353107 0.31903219 0.23341179 0.79978228 1.7644715 1.6270704 0.4882617 -1.7550125 -3.5651622 -3.1277952 -0.93580055 0.81653929][-2.6967573 -1.8412611 -0.94955397 -0.024501324 0.42144442 0.53827715 1.4313726 2.4613461 1.968123 0.52455759 -1.6693552 -3.4892602 -3.0777225 -0.84749818 0.84507179][-2.4076426 -1.6758537 -0.92460513 0.0048055649 0.45934343 0.84681749 2.0024314 2.9570713 2.1814404 0.63468838 -1.3897619 -3.1821012 -2.824348 -0.77632046 0.69584608][-2.0478418 -1.4851286 -0.89874625 -0.036140919 0.34659243 0.83292675 2.0108762 2.7560906 2.0267382 0.70970678 -1.0797858 -2.8693326 -2.643579 -0.9703064 0.20166063][-1.467864 -1.0591273 -0.6575737 0.080501556 0.3221035 0.81037951 1.8052521 2.3652925 1.9678826 1.0200906 -0.61107492 -2.4192011 -2.3020639 -1.0821612 -0.32330084][-0.9076426 -0.61447668 -0.34522486 0.26073313 0.3134737 0.79630613 1.7135081 2.299541 2.3342175 1.5805702 -0.13484001 -1.901242 -1.7791247 -1.000185 -0.69441032][-0.45769167 -0.23645067 -0.057031631 0.32699108 0.10306215 0.55815744 1.5786462 2.2777247 2.5619569 1.8171759 -0.0057415962 -1.5073495 -1.3367331 -1.0402057 -1.1785271][-0.13836193 -0.047877789 0.082503796 0.1984129 -0.35800552 -0.029281139 1.0133271 1.7453008 2.077157 1.3450947 -0.36077356 -1.4169805 -1.2590005 -1.4212606 -1.88286][0.045096397 -0.093243122 0.028485775 -0.12781715 -0.97716665 -0.79788303 0.16556025 0.84090471 1.0858622 0.47050476 -0.90069056 -1.5271273 -1.4118879 -1.777698 -2.2476654][-0.072576523 -0.50070262 -0.3765192 -0.6458981 -1.547627 -1.4215343 -0.59968829 -0.058330536 0.077194214 -0.339046 -1.3405864 -1.7041867 -1.6868162 -2.0891142 -2.4075248][-0.39836168 -1.1219471 -0.94797373 -1.1551328 -1.9662514 -1.9109256 -1.366565 -1.0502775 -0.991611 -1.2062674 -1.8945639 -2.1606972 -2.2839384 -2.6744609 -2.8411095]]...]
INFO - root - 2017-12-07 10:46:45.494708: step 8710, loss = 0.74, batch loss = 0.66 (7.2 examples/sec; 4.450 sec/batch; 91h:59m:30s remains)
INFO - root - 2017-12-07 10:47:30.893337: step 8720, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.553 sec/batch; 94h:05m:30s remains)
INFO - root - 2017-12-07 10:48:16.204679: step 8730, loss = 0.62, batch loss = 0.55 (7.1 examples/sec; 4.518 sec/batch; 93h:22m:04s remains)
INFO - root - 2017-12-07 10:49:01.880703: step 8740, loss = 0.79, batch loss = 0.71 (7.1 examples/sec; 4.526 sec/batch; 93h:31m:36s remains)
INFO - root - 2017-12-07 10:49:47.041601: step 8750, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.559 sec/batch; 94h:11m:43s remains)
INFO - root - 2017-12-07 10:50:32.533702: step 8760, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.583 sec/batch; 94h:40m:25s remains)
INFO - root - 2017-12-07 10:51:17.899068: step 8770, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 4.538 sec/batch; 93h:43m:46s remains)
INFO - root - 2017-12-07 10:52:03.422785: step 8780, loss = 0.88, batch loss = 0.81 (6.9 examples/sec; 4.605 sec/batch; 95h:06m:10s remains)
INFO - root - 2017-12-07 10:52:48.742527: step 8790, loss = 0.74, batch loss = 0.66 (7.1 examples/sec; 4.493 sec/batch; 92h:46m:42s remains)
INFO - root - 2017-12-07 10:53:34.166894: step 8800, loss = 0.85, batch loss = 0.77 (7.0 examples/sec; 4.579 sec/batch; 94h:32m:04s remains)
2017-12-07 10:53:37.059102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4919696 -3.4264941 -3.3928387 -3.4118552 -3.4659975 -3.560282 -3.6596537 -3.7111878 -3.7540417 -3.800127 -3.8109589 -3.7817097 -3.7466919 -3.7121427 -3.6873283][-3.6809785 -3.596338 -3.5598502 -3.592618 -3.6463065 -3.6937282 -3.6877296 -3.6105187 -3.5787683 -3.626318 -3.679296 -3.7014494 -3.7177091 -3.7253067 -3.7213664][-3.830421 -3.751853 -3.7450633 -3.8443449 -3.9361262 -3.9069865 -3.7294555 -3.4966233 -3.3892395 -3.4422731 -3.5518658 -3.6398029 -3.7090361 -3.7488742 -3.7621272][-3.7095454 -3.6695194 -3.7324736 -3.9163468 -4.0624795 -3.955061 -3.5830441 -3.1711659 -3.0081978 -3.1344123 -3.3442059 -3.5031652 -3.5971169 -3.6124916 -3.5997281][-3.8361895 -3.8220947 -3.9085319 -4.0958443 -4.181612 -3.8856466 -3.2015676 -2.5042696 -2.2703381 -2.5309982 -2.9252787 -3.224596 -3.3872874 -3.4016559 -3.3700204][-3.9981859 -3.9769144 -4.0469275 -4.1872916 -4.1431284 -3.6274295 -2.6407418 -1.6709735 -1.3850815 -1.7899806 -2.3603971 -2.7759371 -2.9668703 -2.9630542 -2.9323733][-4.0023522 -3.9324453 -3.9596868 -4.0268135 -3.8463988 -3.1609249 -2.0239484 -0.97566724 -0.7697711 -1.3283896 -2.0029728 -2.4022169 -2.4705815 -2.3457937 -2.2563345][-3.8211136 -3.8480649 -3.9188523 -3.9920905 -3.7835529 -3.0882688 -2.0318668 -1.193948 -1.2155685 -1.9103484 -2.6179476 -2.9384599 -2.886724 -2.6865437 -2.5652857][-3.1904831 -3.3936162 -3.5842047 -3.7470725 -3.6466727 -3.1022584 -2.2879856 -1.76634 -1.9544401 -2.6450155 -3.3010669 -3.5606022 -3.4773381 -3.2904291 -3.2101283][-3.1244287 -3.2529116 -3.3903534 -3.5315676 -3.4924817 -3.0641444 -2.4483128 -2.1381204 -2.3519373 -2.9067731 -3.43786 -3.6418517 -3.5640059 -3.4160857 -3.3950286][-3.296654 -3.3018785 -3.3825326 -3.5051067 -3.498805 -3.1606505 -2.6953416 -2.4955935 -2.6306472 -2.9900811 -3.3599887 -3.49042 -3.3953185 -3.2478552 -3.2540021][-3.08244 -3.0981045 -3.2255371 -3.4169111 -3.5085528 -3.3359756 -3.0398283 -2.8926029 -2.9081426 -3.0703566 -3.2510197 -3.2914305 -3.1761708 -3.0282273 -3.0502944][-2.9023142 -2.9862785 -3.15176 -3.353271 -3.4745774 -3.3996637 -3.2254927 -3.11685 -3.0861144 -3.1452022 -3.1938126 -3.1786165 -3.0831919 -2.9641638 -2.9847393][-3.0862432 -3.2222419 -3.3767724 -3.5138044 -3.5655632 -3.4869049 -3.3439875 -3.2485762 -3.2166655 -3.2393968 -3.2373147 -3.2048006 -3.1653836 -3.1063755 -3.1113532][-3.303081 -3.445828 -3.5739646 -3.6543746 -3.6507921 -3.5593352 -3.4286866 -3.3501616 -3.3322473 -3.3408191 -3.3111329 -3.2605581 -3.2478218 -3.2311063 -3.2134085]]...]
INFO - root - 2017-12-07 10:54:22.451610: step 8810, loss = 0.83, batch loss = 0.75 (7.1 examples/sec; 4.537 sec/batch; 93h:39m:51s remains)
INFO - root - 2017-12-07 10:55:07.909534: step 8820, loss = 0.75, batch loss = 0.68 (7.2 examples/sec; 4.434 sec/batch; 91h:30m:58s remains)
INFO - root - 2017-12-07 10:55:53.624527: step 8830, loss = 0.76, batch loss = 0.69 (6.7 examples/sec; 4.757 sec/batch; 98h:10m:43s remains)
INFO - root - 2017-12-07 10:56:39.277943: step 8840, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 4.526 sec/batch; 93h:23m:09s remains)
INFO - root - 2017-12-07 10:57:24.891066: step 8850, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.560 sec/batch; 94h:04m:19s remains)
INFO - root - 2017-12-07 10:58:10.339097: step 8860, loss = 0.85, batch loss = 0.78 (6.9 examples/sec; 4.625 sec/batch; 95h:24m:29s remains)
INFO - root - 2017-12-07 10:58:56.074402: step 8870, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.521 sec/batch; 93h:14m:34s remains)
INFO - root - 2017-12-07 10:59:41.779495: step 8880, loss = 0.74, batch loss = 0.67 (7.2 examples/sec; 4.434 sec/batch; 91h:26m:36s remains)
INFO - root - 2017-12-07 11:00:27.501495: step 8890, loss = 0.83, batch loss = 0.76 (6.9 examples/sec; 4.614 sec/batch; 95h:08m:22s remains)
INFO - root - 2017-12-07 11:01:12.900766: step 8900, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.590 sec/batch; 94h:38m:04s remains)
2017-12-07 11:01:15.619946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3675177 -2.2464852 -2.232779 -2.4050004 -2.6278572 -2.7792189 -2.8776853 -2.9528632 -2.983252 -2.9437716 -2.921411 -2.953691 -2.9571 -2.9508362 -2.9187436][-2.2507839 -2.1514747 -2.1631131 -2.3608909 -2.6235485 -2.8312593 -2.9608092 -3.0195935 -3.0080323 -2.9178786 -2.8346958 -2.8352656 -2.8643668 -2.8754814 -2.8349915][-2.2613411 -2.1752589 -2.2002149 -2.3888803 -2.6367354 -2.8612435 -3.0190611 -3.0712156 -3.0208549 -2.8951597 -2.7731037 -2.7624607 -2.822489 -2.8637328 -2.8223417][-2.4077477 -2.310493 -2.3250895 -2.478374 -2.674674 -2.8637414 -3.0154591 -3.0666528 -2.9939873 -2.8489733 -2.7117975 -2.702513 -2.7716928 -2.8235254 -2.7813][-2.5579987 -2.4462247 -2.4263444 -2.5369616 -2.6801896 -2.8208418 -2.9529383 -3.0105383 -2.9341402 -2.7721031 -2.6337631 -2.6376462 -2.7267652 -2.7957449 -2.7417331][-2.6552079 -2.5355773 -2.4791417 -2.5537915 -2.6587713 -2.7779703 -2.9301434 -3.0372288 -3.0084815 -2.8426228 -2.6919661 -2.6798038 -2.765492 -2.8407745 -2.7759848][-2.6527908 -2.5471206 -2.4967551 -2.5656478 -2.639523 -2.7368839 -2.9206548 -3.1121395 -3.1952703 -3.0829935 -2.9146085 -2.83916 -2.8614681 -2.8996866 -2.806715][-2.5684547 -2.4768429 -2.4720178 -2.5713606 -2.6140151 -2.6773391 -2.8489294 -3.0801666 -3.2388015 -3.1918087 -3.0382442 -2.9363372 -2.9113715 -2.9188843 -2.815084][-2.5479031 -2.479228 -2.499227 -2.5994022 -2.6208825 -2.6636677 -2.8007293 -2.9896135 -3.1288557 -3.1014018 -3.0040376 -2.9625609 -2.948957 -2.9719129 -2.8968096][-2.5979948 -2.5694845 -2.5899901 -2.6720328 -2.727165 -2.8070154 -2.9218612 -3.0205612 -3.0753708 -3.017282 -2.9674191 -3.0036936 -3.0125546 -3.0514693 -3.0043035][-2.6950226 -2.6704235 -2.6477041 -2.685879 -2.775167 -2.9015718 -3.0028667 -3.0309486 -3.0462861 -2.9736595 -2.933285 -2.9886253 -3.0137348 -3.0454741 -2.991117][-2.8262174 -2.8162656 -2.736378 -2.6834855 -2.7041326 -2.7886167 -2.8600504 -2.8868384 -2.9378986 -2.8974838 -2.8767979 -2.9468141 -3.0167193 -3.0546522 -2.9768887][-2.7992439 -2.8418174 -2.7764792 -2.6794968 -2.6138124 -2.626194 -2.6801267 -2.7525947 -2.871897 -2.8984847 -2.9357543 -3.0544798 -3.1579623 -3.2002749 -3.1141605][-2.6716375 -2.70874 -2.6679418 -2.5902052 -2.5149965 -2.500268 -2.5748091 -2.712904 -2.8929608 -2.9718232 -3.0243578 -3.1184106 -3.168427 -3.1875587 -3.1057696][-2.7422237 -2.7437317 -2.6912818 -2.6160955 -2.5487018 -2.5156312 -2.5944147 -2.7571492 -2.95899 -3.0651329 -3.1020942 -3.1401687 -3.1259484 -3.1071169 -3.0108848]]...]
INFO - root - 2017-12-07 11:02:01.322494: step 8910, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.587 sec/batch; 94h:33m:09s remains)
INFO - root - 2017-12-07 11:02:46.915071: step 8920, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.604 sec/batch; 94h:54m:00s remains)
INFO - root - 2017-12-07 11:03:32.492691: step 8930, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.576 sec/batch; 94h:18m:17s remains)
INFO - root - 2017-12-07 11:04:18.307211: step 8940, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.556 sec/batch; 93h:53m:04s remains)
INFO - root - 2017-12-07 11:05:04.291809: step 8950, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 4.556 sec/batch; 93h:51m:50s remains)
INFO - root - 2017-12-07 11:05:49.810312: step 8960, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.557 sec/batch; 93h:53m:23s remains)
INFO - root - 2017-12-07 11:06:35.266133: step 8970, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.592 sec/batch; 94h:35m:33s remains)
INFO - root - 2017-12-07 11:07:21.190511: step 8980, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.601 sec/batch; 94h:45m:52s remains)
INFO - root - 2017-12-07 11:08:06.861113: step 8990, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 4.563 sec/batch; 93h:58m:22s remains)
INFO - root - 2017-12-07 11:08:52.256104: step 9000, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.589 sec/batch; 94h:28m:55s remains)
2017-12-07 11:08:55.083542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7891016 -2.7812994 -2.7621012 -2.7663748 -2.7849526 -2.7951379 -2.8158829 -2.8299904 -2.8207345 -2.8255219 -2.8224247 -2.7921925 -2.7905178 -2.8126483 -2.8230772][-2.7029965 -2.6924977 -2.6688871 -2.6950431 -2.7478921 -2.7687578 -2.799087 -2.8298438 -2.8271348 -2.8431759 -2.8453989 -2.7989593 -2.7944534 -2.8284748 -2.8394759][-2.6477573 -2.6418467 -2.6115007 -2.6460757 -2.7077484 -2.704576 -2.7185349 -2.7592244 -2.772428 -2.8160815 -2.8549447 -2.8309269 -2.8455749 -2.8926947 -2.8830166][-2.6316409 -2.6392064 -2.6081643 -2.6511292 -2.7234759 -2.7002566 -2.6928608 -2.7307062 -2.7403607 -2.7875538 -2.8587079 -2.8779721 -2.9214537 -2.9793477 -2.9382181][-2.5622766 -2.5882263 -2.5625257 -2.6199849 -2.7260969 -2.7149854 -2.7013454 -2.7502317 -2.7624083 -2.785933 -2.8521926 -2.8981471 -2.964092 -3.0380292 -2.9804296][-2.5023472 -2.532352 -2.4689922 -2.4699068 -2.5441594 -2.5012379 -2.4649487 -2.5626149 -2.6455142 -2.6914897 -2.7666593 -2.8393538 -2.932796 -3.0359302 -2.9852078][-2.6035089 -2.6197429 -2.4662142 -2.3176262 -2.2451751 -2.0675862 -1.9455147 -2.0841243 -2.2830939 -2.418617 -2.5664382 -2.7078042 -2.8544986 -2.9989886 -2.9660959][-2.7581711 -2.8031421 -2.6259212 -2.3969626 -2.2163889 -1.9068053 -1.6824255 -1.810092 -2.0625031 -2.2551734 -2.4733505 -2.6822076 -2.8665028 -3.0252104 -2.9871855][-2.8627043 -2.9876409 -2.8993614 -2.7597132 -2.6547642 -2.3632603 -2.0954406 -2.1577477 -2.3285275 -2.4376624 -2.6090977 -2.8034976 -2.959384 -3.0956912 -3.0400887][-2.9037161 -3.0476599 -3.0073204 -2.9536676 -2.9609065 -2.7551568 -2.490797 -2.5074494 -2.6019521 -2.6172633 -2.7136993 -2.8649101 -2.9791317 -3.1026027 -3.0522833][-2.8421752 -2.915911 -2.8285155 -2.7640724 -2.8002267 -2.6525202 -2.4188035 -2.4592106 -2.5734434 -2.5886984 -2.6782429 -2.8276658 -2.928318 -3.0587459 -3.0385084][-2.7833362 -2.8055015 -2.6996918 -2.6243615 -2.6475744 -2.5009761 -2.2695959 -2.3163126 -2.452023 -2.5088921 -2.6532345 -2.8400598 -2.9393709 -3.0624452 -3.0589309][-2.6795673 -2.7180872 -2.6904483 -2.687912 -2.7368526 -2.5798998 -2.3258169 -2.3268898 -2.4219432 -2.4876099 -2.67814 -2.8925481 -2.9830618 -3.07249 -3.0620341][-2.5607173 -2.6092029 -2.6621349 -2.750417 -2.8508134 -2.7243006 -2.4833913 -2.4420238 -2.4565918 -2.4761086 -2.6497245 -2.8496742 -2.9407923 -3.0281446 -3.0404332][-2.6072855 -2.6432619 -2.683373 -2.7579823 -2.8252044 -2.715487 -2.5346155 -2.5168929 -2.5058346 -2.4917562 -2.6149681 -2.7707734 -2.8791401 -3.0000887 -3.0600269]]...]
INFO - root - 2017-12-07 11:09:40.657220: step 9010, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 4.501 sec/batch; 92h:39m:54s remains)
INFO - root - 2017-12-07 11:10:26.527331: step 9020, loss = 0.78, batch loss = 0.70 (6.9 examples/sec; 4.616 sec/batch; 95h:01m:34s remains)
INFO - root - 2017-12-07 11:11:12.440956: step 9030, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.632 sec/batch; 95h:20m:35s remains)
INFO - root - 2017-12-07 11:11:58.255196: step 9040, loss = 0.75, batch loss = 0.67 (6.9 examples/sec; 4.624 sec/batch; 95h:09m:47s remains)
INFO - root - 2017-12-07 11:12:43.740064: step 9050, loss = 0.71, batch loss = 0.64 (7.2 examples/sec; 4.469 sec/batch; 91h:57m:21s remains)
INFO - root - 2017-12-07 11:13:29.379062: step 9060, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.604 sec/batch; 94h:43m:39s remains)
INFO - root - 2017-12-07 11:14:14.897284: step 9070, loss = 0.81, batch loss = 0.74 (7.3 examples/sec; 4.399 sec/batch; 90h:28m:53s remains)
INFO - root - 2017-12-07 11:15:00.566428: step 9080, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 4.604 sec/batch; 94h:41m:15s remains)
INFO - root - 2017-12-07 11:15:46.135799: step 9090, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.597 sec/batch; 94h:32m:45s remains)
INFO - root - 2017-12-07 11:16:31.689290: step 9100, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.517 sec/batch; 92h:52m:38s remains)
2017-12-07 11:16:34.407752: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0922072 -3.0739403 -3.0178442 -2.9603181 -2.9736025 -3.0509238 -3.1472306 -3.233901 -3.298157 -3.3538747 -3.3708425 -3.3403449 -3.2665215 -3.2035728 -3.1807225][-2.92309 -2.91528 -2.8386807 -2.739943 -2.708899 -2.7714071 -2.8963585 -3.0294566 -3.1482372 -3.2451706 -3.2900357 -3.2793832 -3.2202039 -3.1767707 -3.1702743][-2.75482 -2.7765856 -2.7136197 -2.6164994 -2.5593362 -2.5923362 -2.7092996 -2.8427887 -2.992506 -3.1215415 -3.186379 -3.1889427 -3.1441255 -3.1246498 -3.1393983][-2.5469124 -2.5840106 -2.5884614 -2.5969481 -2.5881033 -2.6053891 -2.687387 -2.7793159 -2.9270608 -3.06568 -3.1325407 -3.1246448 -3.0769777 -3.0749376 -3.1086965][-2.3592885 -2.3730366 -2.4367318 -2.5653052 -2.6259069 -2.6458657 -2.7124875 -2.7800794 -2.9307694 -3.1027446 -3.185813 -3.1528187 -3.0779576 -3.0746851 -3.110673][-2.3179514 -2.2756984 -2.3318877 -2.4876661 -2.5637727 -2.5806282 -2.6354508 -2.699317 -2.8984179 -3.1798728 -3.3518443 -3.3206801 -3.1951916 -3.1490343 -3.1548953][-2.5559883 -2.4796124 -2.4727025 -2.5363426 -2.5266213 -2.4873946 -2.4877582 -2.5102429 -2.7436857 -3.1784117 -3.5266623 -3.568809 -3.4120765 -3.2948387 -3.2352984][-3.0988111 -3.0261254 -2.95053 -2.8786805 -2.7339678 -2.5538054 -2.4196026 -2.3337564 -2.5280664 -3.0720067 -3.6112957 -3.7962048 -3.6751285 -3.5045762 -3.3662329][-3.7772863 -3.7643845 -3.6639302 -3.5125475 -3.3023872 -3.026325 -2.7607112 -2.527709 -2.5440311 -3.0050766 -3.5987778 -3.9098842 -3.8796017 -3.7069168 -3.5195513][-4.2858143 -4.3596725 -4.33559 -4.23892 -4.0848355 -3.8143654 -3.5265949 -3.2379456 -3.0575161 -3.2862649 -3.7393959 -4.0358763 -4.0462294 -3.875783 -3.6654344][-4.4166307 -4.5411258 -4.6213245 -4.6407881 -4.6087875 -4.4191012 -4.1812873 -3.9405069 -3.7006903 -3.7402277 -3.995049 -4.1769323 -4.1547337 -3.965023 -3.7443678][-4.1501408 -4.2384343 -4.3591471 -4.48021 -4.5796356 -4.5145049 -4.358459 -4.208509 -4.0224414 -3.997982 -4.122467 -4.1951389 -4.1304946 -3.9329422 -3.7246878][-3.7931664 -3.7991633 -3.8912416 -4.0259604 -4.1681666 -4.1828723 -4.0992703 -4.0457273 -3.9772089 -3.9843569 -4.059803 -4.0747218 -3.9956751 -3.8213592 -3.6418831][-3.6122041 -3.5884717 -3.6312294 -3.7092917 -3.7983115 -3.8109014 -3.7357304 -3.7060106 -3.6971662 -3.7257314 -3.783462 -3.7893462 -3.7407057 -3.6271837 -3.5025516][-3.5081375 -3.5024529 -3.5212049 -3.5535777 -3.5955486 -3.60198 -3.5444853 -3.5196271 -3.5188675 -3.5245657 -3.5406449 -3.5326443 -3.5021181 -3.4385283 -3.3648319]]...]
INFO - root - 2017-12-07 11:17:20.025449: step 9110, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.579 sec/batch; 94h:08m:01s remains)
INFO - root - 2017-12-07 11:18:05.574813: step 9120, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.549 sec/batch; 93h:30m:32s remains)
INFO - root - 2017-12-07 11:18:51.512951: step 9130, loss = 0.68, batch loss = 0.60 (7.0 examples/sec; 4.565 sec/batch; 93h:49m:33s remains)
INFO - root - 2017-12-07 11:19:36.937489: step 9140, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 4.501 sec/batch; 92h:30m:07s remains)
INFO - root - 2017-12-07 11:20:22.450279: step 9150, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 4.531 sec/batch; 93h:06m:54s remains)
INFO - root - 2017-12-07 11:21:07.932136: step 9160, loss = 0.82, batch loss = 0.75 (7.1 examples/sec; 4.506 sec/batch; 92h:34m:39s remains)
INFO - root - 2017-12-07 11:21:53.649898: step 9170, loss = 0.83, batch loss = 0.75 (7.0 examples/sec; 4.590 sec/batch; 94h:17m:35s remains)
INFO - root - 2017-12-07 11:22:38.998352: step 9180, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.523 sec/batch; 92h:53m:39s remains)
INFO - root - 2017-12-07 11:23:24.755755: step 9190, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.578 sec/batch; 94h:01m:30s remains)
INFO - root - 2017-12-07 11:24:10.270943: step 9200, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.570 sec/batch; 93h:50m:08s remains)
2017-12-07 11:24:12.942398: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4049602 -1.1221259 -1.1646359 -1.205507 -1.1688464 -1.4001985 -1.7109139 -1.9615023 -2.2655032 -2.2770452 -2.1017113 -1.9862399 -1.7881565 -1.5653372 -1.4816682][-1.3376613 -1.0487537 -1.073298 -1.0512729 -0.97753215 -1.1275091 -1.2148354 -1.3020399 -1.6446326 -1.8128338 -1.7704275 -1.7471371 -1.689374 -1.5916159 -1.5883367][-1.4090493 -1.1895499 -1.2375476 -1.2057581 -1.125675 -1.1791043 -1.0566478 -0.97115254 -1.2686887 -1.4805496 -1.4861295 -1.5355115 -1.6230354 -1.6047859 -1.5921173][-1.5469022 -1.4227922 -1.5130594 -1.5031667 -1.4356592 -1.3852081 -1.073489 -0.84026384 -1.0922501 -1.3232429 -1.3432214 -1.456846 -1.6401796 -1.6376426 -1.5454419][-1.6325324 -1.5468934 -1.5971978 -1.567344 -1.5287888 -1.4248469 -0.96759129 -0.63696504 -0.92118478 -1.2368166 -1.3153493 -1.5158193 -1.793987 -1.8000274 -1.6400545][-1.5844269 -1.4278619 -1.3242579 -1.1857662 -1.1341746 -1.0022604 -0.50188589 -0.22203541 -0.69961834 -1.2310798 -1.4299781 -1.7103539 -2.0532546 -2.0604272 -1.866514][-1.4853361 -1.2154951 -0.94041371 -0.6781528 -0.57681656 -0.40834141 0.11158657 0.29257536 -0.4214325 -1.1859386 -1.520716 -1.8564515 -2.2259498 -2.2097323 -2.0025022][-1.4167404 -1.0871222 -0.74134541 -0.43148661 -0.32824898 -0.18549442 0.31148338 0.42510128 -0.38593674 -1.2122779 -1.5812604 -1.9358211 -2.2891035 -2.2066262 -1.987088][-1.3817012 -1.059634 -0.757318 -0.50542784 -0.46530724 -0.42170858 -0.037843704 0.0060424805 -0.75995469 -1.4646401 -1.7619159 -2.0807045 -2.340534 -2.1405489 -1.8835208][-1.385895 -1.1035304 -0.902647 -0.76758671 -0.8161478 -0.85501671 -0.56967115 -0.55576444 -1.1895516 -1.7008259 -1.9035804 -2.1584 -2.2802629 -1.9543107 -1.6721573][-1.4336748 -1.2113161 -1.1271677 -1.1042023 -1.2042189 -1.2788391 -1.0789988 -1.0854924 -1.5354002 -1.8140128 -1.8824191 -2.0469475 -2.0556257 -1.6765542 -1.4274158][-1.4668543 -1.280349 -1.2740574 -1.3229609 -1.4387233 -1.5181637 -1.4291785 -1.5138738 -1.8218884 -1.9064314 -1.847928 -1.9186502 -1.8520734 -1.5089147 -1.3760054][-1.457083 -1.2381396 -1.206089 -1.2427888 -1.3215284 -1.3541989 -1.3626611 -1.5592976 -1.8112268 -1.8227921 -1.753386 -1.8187864 -1.7429085 -1.5009601 -1.512784][-1.4296818 -1.1425307 -1.0263498 -0.99344444 -0.98273468 -0.917912 -0.97741127 -1.260644 -1.4777546 -1.4842885 -1.4858265 -1.6045153 -1.5774338 -1.4822156 -1.6176786][-1.3999045 -1.0354733 -0.83463407 -0.74460459 -0.66032934 -0.51562285 -0.6026113 -0.90707994 -1.0465496 -1.0325208 -1.0905552 -1.2226584 -1.22766 -1.2768972 -1.5117211]]...]
INFO - root - 2017-12-07 11:24:58.351093: step 9210, loss = 0.83, batch loss = 0.76 (7.2 examples/sec; 4.467 sec/batch; 91h:43m:22s remains)
INFO - root - 2017-12-07 11:25:43.805570: step 9220, loss = 0.75, batch loss = 0.68 (7.2 examples/sec; 4.454 sec/batch; 91h:26m:36s remains)
INFO - root - 2017-12-07 11:26:29.292472: step 9230, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.581 sec/batch; 94h:01m:38s remains)
INFO - root - 2017-12-07 11:27:15.146998: step 9240, loss = 0.86, batch loss = 0.79 (7.1 examples/sec; 4.512 sec/batch; 92h:35m:55s remains)
INFO - root - 2017-12-07 11:28:00.379457: step 9250, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.588 sec/batch; 94h:08m:37s remains)
INFO - root - 2017-12-07 11:28:45.947061: step 9260, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.529 sec/batch; 92h:55m:22s remains)
INFO - root - 2017-12-07 11:29:31.355170: step 9270, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.609 sec/batch; 94h:33m:02s remains)
INFO - root - 2017-12-07 11:30:17.198801: step 9280, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.590 sec/batch; 94h:09m:27s remains)
INFO - root - 2017-12-07 11:31:02.510761: step 9290, loss = 0.67, batch loss = 0.59 (7.1 examples/sec; 4.532 sec/batch; 92h:56m:30s remains)
INFO - root - 2017-12-07 11:31:48.318867: step 9300, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.611 sec/batch; 94h:33m:30s remains)
2017-12-07 11:31:51.040554: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.601531 -2.9184458 -3.2749844 -3.5176072 -3.6021364 -3.5538805 -3.6914775 -3.754427 -3.2713525 -2.6438212 -2.3782039 -2.34872 -2.1471345 -1.9133458 -2.2450047][-2.7714114 -2.9486914 -3.2423098 -3.4899244 -3.5478966 -3.3258209 -3.211544 -3.1579561 -2.8099618 -2.3121381 -2.1594696 -2.3162353 -2.2486634 -2.0842376 -2.4403586][-2.8811197 -2.9242587 -3.0750742 -3.2655296 -3.2945709 -2.9911509 -2.70774 -2.5489485 -2.3529329 -2.107403 -2.1425471 -2.4396029 -2.4226258 -2.3313334 -2.7429452][-2.7790446 -2.5984354 -2.5609028 -2.6785607 -2.7140775 -2.4867191 -2.2879446 -2.225363 -2.2469285 -2.2952507 -2.4107964 -2.5704942 -2.4076843 -2.3946862 -2.9352627][-2.384367 -2.0531485 -1.9720645 -2.1216805 -2.1857014 -2.0730209 -2.0244524 -2.0584738 -2.23475 -2.4929333 -2.5553555 -2.3818376 -1.9802444 -2.0833623 -2.8382602][-2.0046639 -1.7661262 -1.8485179 -2.0729675 -2.1153455 -2.0709856 -2.0697393 -2.0401883 -2.2917469 -2.7320027 -2.68788 -2.1260822 -1.4658539 -1.599165 -2.4718628][-1.6398647 -1.5868514 -1.8886638 -2.1652586 -2.1791122 -2.1268938 -1.9805198 -1.6984451 -1.964834 -2.6643085 -2.7415037 -2.058198 -1.2829137 -1.3325567 -2.148572][-1.6483953 -1.6472468 -1.9819636 -2.1321261 -2.0425274 -1.9012933 -1.4150994 -0.6648488 -0.81822872 -1.8500316 -2.4391189 -2.1923051 -1.670594 -1.625531 -2.1999533][-1.9297714 -1.8630741 -2.0909543 -2.0368171 -1.9187214 -1.8000481 -1.1515467 -0.073078632 0.051966667 -0.99183416 -1.9750061 -2.3685448 -2.3198831 -2.24068 -2.5226793][-2.0868025 -1.9542754 -2.0679598 -1.8811564 -1.8354554 -1.9358582 -1.6283636 -0.8620832 -0.58216715 -1.1747234 -2.0268855 -2.6817131 -2.9334249 -2.8176451 -2.8164949][-1.8517907 -1.7818093 -1.8624313 -1.6830368 -1.716398 -1.96209 -2.0661688 -1.825269 -1.5450096 -1.6982393 -2.2369516 -2.8602228 -3.1639478 -2.9822488 -2.8032913][-1.4543831 -1.494801 -1.5996475 -1.5572624 -1.7251194 -1.9830446 -2.2421918 -2.2818992 -1.9796984 -1.8456314 -2.1370246 -2.6071763 -2.8088474 -2.5293026 -2.2718673][-1.309942 -1.4092619 -1.5133202 -1.5738425 -1.7895477 -1.9385402 -2.1083508 -2.1320784 -1.7354419 -1.4236386 -1.5401921 -1.8612258 -1.9710183 -1.6701765 -1.4399519][-1.6579604 -1.7998958 -1.8322811 -1.8101859 -1.8488948 -1.7394574 -1.6983843 -1.6671481 -1.331881 -1.0666153 -1.1500382 -1.3769259 -1.4017918 -1.1104214 -0.952214][-2.2118471 -2.2969456 -2.2454386 -2.0764735 -1.9073098 -1.5948822 -1.3515768 -1.2749426 -1.1395016 -1.0874672 -1.3061116 -1.5591142 -1.5359614 -1.2999415 -1.2271574]]...]
INFO - root - 2017-12-07 11:32:36.492660: step 9310, loss = 0.73, batch loss = 0.65 (6.9 examples/sec; 4.620 sec/batch; 94h:44m:14s remains)
INFO - root - 2017-12-07 11:33:21.583021: step 9320, loss = 0.71, batch loss = 0.64 (7.3 examples/sec; 4.397 sec/batch; 90h:08m:55s remains)
INFO - root - 2017-12-07 11:34:06.909418: step 9330, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.559 sec/batch; 93h:27m:20s remains)
INFO - root - 2017-12-07 11:34:52.514373: step 9340, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.593 sec/batch; 94h:08m:11s remains)
INFO - root - 2017-12-07 11:35:38.117679: step 9350, loss = 0.85, batch loss = 0.78 (7.0 examples/sec; 4.576 sec/batch; 93h:46m:25s remains)
INFO - root - 2017-12-07 11:36:23.763229: step 9360, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.594 sec/batch; 94h:07m:27s remains)
INFO - root - 2017-12-07 11:37:09.320173: step 9370, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.568 sec/batch; 93h:35m:18s remains)
INFO - root - 2017-12-07 11:37:55.138737: step 9380, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.572 sec/batch; 93h:39m:51s remains)
INFO - root - 2017-12-07 11:38:40.801042: step 9390, loss = 0.74, batch loss = 0.67 (7.3 examples/sec; 4.412 sec/batch; 90h:22m:23s remains)
INFO - root - 2017-12-07 11:39:26.499038: step 9400, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.594 sec/batch; 94h:05m:26s remains)
2017-12-07 11:39:29.191772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3671012 -2.5246119 -2.506839 -2.3735857 -2.2848375 -2.2885046 -2.3890026 -2.4527531 -2.4058836 -2.4572031 -2.611412 -2.9225421 -3.3048778 -3.470717 -3.3484197][-2.3576558 -2.6553509 -2.7720308 -2.7416701 -2.7017713 -2.7247882 -2.8752728 -2.9528923 -2.8315716 -2.7504196 -2.775461 -3.0388994 -3.5173287 -3.7922926 -3.6874337][-2.3867145 -2.7787709 -2.9792213 -3.0386715 -3.0244703 -3.0463285 -3.204932 -3.2559257 -3.0471535 -2.8358412 -2.764596 -3.004204 -3.6108894 -4.0067091 -3.9148955][-2.4583883 -2.8179984 -2.9877372 -3.0628743 -3.0414653 -3.0142446 -3.114028 -3.1214142 -2.9368007 -2.7338035 -2.6652246 -2.9156225 -3.6184702 -4.0967731 -3.9864233][-2.5936599 -2.7261424 -2.703887 -2.695847 -2.6227095 -2.5392978 -2.5244358 -2.4134135 -2.2424324 -2.1920025 -2.2827747 -2.6513791 -3.4502647 -4.0023503 -3.9173741][-2.7689335 -2.583014 -2.3157089 -2.1232932 -1.8419681 -1.5631404 -1.3328571 -0.97934175 -0.76630497 -0.92896366 -1.343385 -2.0241551 -3.0378532 -3.7221136 -3.7234125][-2.9658062 -2.6182714 -2.2257559 -1.8017924 -1.1157801 -0.38223124 0.27646208 1.0170174 1.3039489 0.82469416 -0.11626148 -1.3226492 -2.674922 -3.5092659 -3.5624094][-3.1059051 -2.8657875 -2.5638235 -1.9632809 -0.82747912 0.4078064 1.557332 2.7163105 3.1150126 2.2960882 0.83649111 -0.86544704 -2.4785109 -3.345571 -3.3908396][-3.0514979 -3.0498552 -2.97058 -2.39686 -1.1468048 0.20827055 1.5717325 3.0031104 3.585166 2.7213287 1.1026068 -0.83292842 -2.507951 -3.2601328 -3.2137935][-2.8391137 -3.0579524 -3.2256024 -2.9128833 -2.0226142 -1.0014648 0.25477314 1.7764893 2.5541573 1.9593234 0.64824486 -1.112093 -2.632503 -3.1883461 -3.0481675][-2.7165077 -3.0654032 -3.4282858 -3.4552526 -3.0648556 -2.4462278 -1.3922513 0.046971798 0.83949423 0.52851486 -0.30332518 -1.631407 -2.8507895 -3.2123594 -3.0184441][-2.770936 -3.1326733 -3.5458596 -3.7750752 -3.7148838 -3.3609719 -2.522608 -1.3510549 -0.73136187 -0.89185667 -1.389925 -2.3210814 -3.2046447 -3.3951483 -3.1466203][-3.048759 -3.313854 -3.6375146 -3.8700738 -3.9123476 -3.7037883 -3.1502147 -2.4085331 -2.0864973 -2.2455628 -2.5674481 -3.1431599 -3.6364512 -3.6533613 -3.3863535][-3.4464886 -3.5660706 -3.7413719 -3.8976419 -3.9587488 -3.8720565 -3.6032267 -3.2532935 -3.1561878 -3.318254 -3.561599 -3.8694272 -4.0261769 -3.9075897 -3.670346][-4.0450382 -4.0689321 -4.119657 -4.1896558 -4.2299914 -4.212553 -4.1358929 -4.0303893 -4.0065465 -4.096262 -4.2640948 -4.4237466 -4.4217253 -4.2879171 -4.1401739]]...]
INFO - root - 2017-12-07 11:40:14.705658: step 9410, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.541 sec/batch; 92h:59m:14s remains)
INFO - root - 2017-12-07 11:41:00.399744: step 9420, loss = 0.85, batch loss = 0.78 (7.0 examples/sec; 4.590 sec/batch; 93h:58m:37s remains)
INFO - root - 2017-12-07 11:41:46.026265: step 9430, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.584 sec/batch; 93h:50m:17s remains)
INFO - root - 2017-12-07 11:42:31.767079: step 9440, loss = 0.68, batch loss = 0.61 (6.9 examples/sec; 4.641 sec/batch; 94h:59m:17s remains)
INFO - root - 2017-12-07 11:43:17.423929: step 9450, loss = 0.78, batch loss = 0.70 (7.0 examples/sec; 4.595 sec/batch; 94h:01m:41s remains)
INFO - root - 2017-12-07 11:44:03.171064: step 9460, loss = 0.67, batch loss = 0.59 (7.0 examples/sec; 4.587 sec/batch; 93h:52m:01s remains)
INFO - root - 2017-12-07 11:44:48.424551: step 9470, loss = 0.71, batch loss = 0.63 (7.1 examples/sec; 4.479 sec/batch; 91h:38m:41s remains)
INFO - root - 2017-12-07 11:45:33.908616: step 9480, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.515 sec/batch; 92h:21m:14s remains)
INFO - root - 2017-12-07 11:46:19.634199: step 9490, loss = 0.76, batch loss = 0.69 (7.2 examples/sec; 4.467 sec/batch; 91h:22m:44s remains)
INFO - root - 2017-12-07 11:47:05.283645: step 9500, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.620 sec/batch; 94h:29m:20s remains)
2017-12-07 11:47:08.026700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0628426 -1.9100912 -1.7366927 -1.801477 -1.9256067 -1.8217664 -1.620151 -1.2482879 -0.91343284 -0.93199778 -1.1906643 -1.558949 -1.9641092 -2.0589473 -2.0594506][-1.5137575 -1.4337661 -1.4676507 -1.7018733 -1.8614802 -1.7319674 -1.5205021 -1.1810603 -0.97755527 -1.1062505 -1.4275441 -1.8412287 -2.2253802 -2.2349946 -2.0930221][-1.2922442 -1.3422928 -1.5955594 -1.8913302 -1.9666703 -1.7749376 -1.5520594 -1.2418258 -1.093436 -1.1679113 -1.3378656 -1.6005373 -1.908217 -1.9397247 -1.8067789][-1.6056135 -1.6923816 -1.9556148 -2.1042352 -2.0349016 -1.799392 -1.5811136 -1.3405867 -1.2422593 -1.2577536 -1.1854529 -1.1219261 -1.208632 -1.2645633 -1.2952669][-1.8408947 -2.0069196 -2.2514572 -2.271116 -2.0968165 -1.7757356 -1.468946 -1.2824314 -1.3714797 -1.563134 -1.517401 -1.359416 -1.3512266 -1.4590597 -1.5897131][-1.7629712 -1.9079931 -2.03011 -1.9785316 -1.8249841 -1.5227199 -1.1259289 -0.91183996 -1.1817052 -1.6223874 -1.8078315 -1.8098345 -1.9112272 -2.1516252 -2.3638797][-1.5366278 -1.4466193 -1.243875 -1.0063019 -0.80806661 -0.53166223 -0.035624981 0.29586697 -0.049953461 -0.67380929 -1.115545 -1.359091 -1.6011028 -1.9075415 -2.2431095][-1.3646796 -1.1357234 -0.78746939 -0.52075553 -0.33013964 -0.012769699 0.6256609 1.1436315 0.91009283 0.36475849 -0.087041855 -0.35109568 -0.54607224 -0.77701306 -1.1754856][-1.3568304 -1.1877005 -0.956444 -0.863734 -0.84329104 -0.64953327 -0.1073103 0.35518646 0.28779459 0.11020184 -0.0094265938 -0.047472 -0.0774312 -0.16071415 -0.47921443][-1.6335647 -1.6556604 -1.5916224 -1.6271183 -1.6892893 -1.6077213 -1.3086698 -1.0891683 -1.1672537 -1.1556787 -1.0786061 -0.9869163 -0.96202016 -0.96473646 -1.119302][-2.2817793 -2.4862044 -2.5851994 -2.7115817 -2.7500994 -2.6425586 -2.4592993 -2.3970768 -2.5396173 -2.5593443 -2.5481136 -2.5560246 -2.6396444 -2.6742847 -2.6697817][-3.0047483 -3.1533031 -3.266047 -3.4284019 -3.4574754 -3.3361206 -3.2002139 -3.1835697 -3.3179221 -3.3597755 -3.4131625 -3.5091171 -3.6718397 -3.7337143 -3.6806149][-3.4761717 -3.4515879 -3.4274578 -3.4914575 -3.4923618 -3.3907735 -3.3013844 -3.2802544 -3.3677807 -3.3978987 -3.466912 -3.5666749 -3.6972671 -3.7434573 -3.7269258][-3.5682356 -3.5353084 -3.492568 -3.5275578 -3.5351679 -3.4687252 -3.3993187 -3.350678 -3.3763874 -3.375463 -3.4257255 -3.5012021 -3.5687792 -3.5640066 -3.5423849][-3.4172235 -3.4418213 -3.4647725 -3.5289671 -3.5767767 -3.5686598 -3.5345817 -3.4966302 -3.4934096 -3.4830105 -3.4949379 -3.5152726 -3.5158505 -3.4779022 -3.4376526]]...]
INFO - root - 2017-12-07 11:47:53.469066: step 9510, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.539 sec/batch; 92h:49m:21s remains)
INFO - root - 2017-12-07 11:48:39.160032: step 9520, loss = 0.84, batch loss = 0.76 (7.0 examples/sec; 4.577 sec/batch; 93h:34m:35s remains)
INFO - root - 2017-12-07 11:49:24.959315: step 9530, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.554 sec/batch; 93h:06m:07s remains)
INFO - root - 2017-12-07 11:50:10.245427: step 9540, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.593 sec/batch; 93h:52m:55s remains)
INFO - root - 2017-12-07 11:50:55.887576: step 9550, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.541 sec/batch; 92h:48m:58s remains)
INFO - root - 2017-12-07 11:51:41.213790: step 9560, loss = 0.80, batch loss = 0.73 (7.2 examples/sec; 4.452 sec/batch; 90h:59m:08s remains)
INFO - root - 2017-12-07 11:52:26.912870: step 9570, loss = 0.78, batch loss = 0.70 (7.2 examples/sec; 4.447 sec/batch; 90h:51m:32s remains)
INFO - root - 2017-12-07 11:53:12.389297: step 9580, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 4.499 sec/batch; 91h:54m:46s remains)
INFO - root - 2017-12-07 11:53:57.923927: step 9590, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.594 sec/batch; 93h:50m:25s remains)
INFO - root - 2017-12-07 11:54:43.437727: step 9600, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.558 sec/batch; 93h:04m:57s remains)
2017-12-07 11:54:46.162270: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8612123 -2.8832507 -2.8836322 -2.9158144 -3.0148587 -3.1289446 -3.1621706 -3.1622214 -3.2754378 -3.4690893 -3.715358 -3.8974879 -3.940433 -3.9172554 -3.8480196][-2.8455598 -2.9033437 -2.9224236 -2.9597864 -3.0458202 -3.1113412 -3.098021 -3.0945539 -3.2760539 -3.5942857 -3.9489217 -4.1677141 -4.1689005 -4.064611 -3.9212422][-2.7347798 -2.7802 -2.7972987 -2.8477972 -2.9376302 -2.9550068 -2.8715429 -2.8200016 -3.0112729 -3.4159794 -3.8973756 -4.2075982 -4.2467504 -4.1420517 -3.9776154][-2.4721251 -2.5161202 -2.5505173 -2.6170938 -2.7162974 -2.7091489 -2.574851 -2.487937 -2.683269 -3.131916 -3.71085 -4.1014662 -4.1844511 -4.1188965 -3.9837739][-2.4831476 -2.4886189 -2.5060358 -2.5327821 -2.5926538 -2.5546994 -2.4039257 -2.3500824 -2.627068 -3.1593235 -3.7845783 -4.1760821 -4.24732 -4.1885223 -4.0613923][-2.4408622 -2.3962786 -2.3406408 -2.2338898 -2.1620338 -2.0350258 -1.9031742 -1.9664629 -2.4018595 -3.0679321 -3.7307606 -4.1362081 -4.2409668 -4.2229943 -4.1207261][-2.2433951 -2.2084451 -2.1144049 -1.9022768 -1.691823 -1.4847844 -1.4141932 -1.6440771 -2.2239339 -2.9164743 -3.5119934 -3.8901234 -4.05707 -4.1080761 -4.0613208][-2.2663572 -2.2765615 -2.1701825 -1.9144719 -1.6612344 -1.4736564 -1.5205228 -1.8738601 -2.4502125 -2.9749153 -3.37235 -3.6646953 -3.845655 -3.9139338 -3.9018488][-2.4888783 -2.5533805 -2.4454982 -2.1701241 -1.9592414 -1.8847332 -2.0260482 -2.3673878 -2.7914493 -3.1134796 -3.3564136 -3.5774665 -3.7285526 -3.7591038 -3.7458627][-2.6316326 -2.7306442 -2.6654758 -2.4495163 -2.338093 -2.3670213 -2.5193405 -2.734724 -2.9614697 -3.1819029 -3.4023037 -3.6070595 -3.7345726 -3.7133832 -3.676774][-2.6961753 -2.8216441 -2.8590279 -2.7787294 -2.7654223 -2.8383365 -2.9398952 -2.9933805 -3.0468359 -3.2223825 -3.4742606 -3.6977043 -3.8240094 -3.7762921 -3.6985445][-2.7675102 -2.8789876 -2.9655757 -2.9677827 -2.9881775 -3.0449796 -3.0698595 -3.0249958 -3.0273957 -3.1873889 -3.4398804 -3.6755376 -3.8203588 -3.786428 -3.6964519][-2.7487915 -2.8273067 -2.9120297 -2.9539342 -3.0056176 -3.06789 -3.0243802 -2.9046741 -2.8635743 -2.9763677 -3.2001595 -3.448729 -3.6304781 -3.65728 -3.6138966][-2.6369457 -2.6581631 -2.7026577 -2.7895646 -2.9471555 -3.1018426 -3.0562806 -2.8819475 -2.7562525 -2.7676287 -2.9315057 -3.1680181 -3.3851702 -3.4843111 -3.510097][-2.5051522 -2.4539318 -2.4115162 -2.4896574 -2.7293105 -3.007751 -3.0637181 -2.921977 -2.772393 -2.7439685 -2.8735833 -3.0722516 -3.2882795 -3.4242043 -3.4815876]]...]
INFO - root - 2017-12-07 11:55:31.445317: step 9610, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.575 sec/batch; 93h:25m:21s remains)
INFO - root - 2017-12-07 11:56:16.815693: step 9620, loss = 0.78, batch loss = 0.70 (7.0 examples/sec; 4.596 sec/batch; 93h:50m:26s remains)
INFO - root - 2017-12-07 11:57:02.314141: step 9630, loss = 0.79, batch loss = 0.71 (7.0 examples/sec; 4.577 sec/batch; 93h:26m:32s remains)
INFO - root - 2017-12-07 11:57:47.787686: step 9640, loss = 0.85, batch loss = 0.78 (7.0 examples/sec; 4.571 sec/batch; 93h:18m:00s remains)
INFO - root - 2017-12-07 11:58:33.122849: step 9650, loss = 0.83, batch loss = 0.75 (6.9 examples/sec; 4.621 sec/batch; 94h:19m:14s remains)
INFO - root - 2017-12-07 11:59:18.727857: step 9660, loss = 0.67, batch loss = 0.60 (7.1 examples/sec; 4.534 sec/batch; 92h:31m:36s remains)
INFO - root - 2017-12-07 12:00:04.430288: step 9670, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.538 sec/batch; 92h:36m:02s remains)
INFO - root - 2017-12-07 12:00:49.730072: step 9680, loss = 0.69, batch loss = 0.62 (7.1 examples/sec; 4.521 sec/batch; 92h:14m:40s remains)
INFO - root - 2017-12-07 12:01:35.157332: step 9690, loss = 0.85, batch loss = 0.78 (7.0 examples/sec; 4.558 sec/batch; 92h:59m:05s remains)
INFO - root - 2017-12-07 12:02:20.441072: step 9700, loss = 0.79, batch loss = 0.71 (7.1 examples/sec; 4.507 sec/batch; 91h:55m:35s remains)
2017-12-07 12:02:23.134435: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7715201 -2.6212182 -2.4180543 -2.1414752 -1.8669798 -1.6648645 -1.4952297 -1.493921 -1.6624525 -1.9484844 -2.10148 -2.1883402 -2.1083615 -1.8780179 -1.6057155][-2.5087368 -2.4114182 -2.3033643 -1.9930964 -1.5411723 -1.0500598 -0.53055787 -0.37069368 -0.69760513 -1.2100437 -1.4091156 -1.4660623 -1.3263693 -1.0210261 -0.76458573][-2.0741024 -2.1758585 -2.3119595 -2.0745132 -1.4558485 -0.69876146 0.14703989 0.49715519 0.061430454 -0.60724163 -0.74022508 -0.6428864 -0.405787 -0.094390392 -0.038309574][-1.9395745 -2.2382495 -2.5133247 -2.2971518 -1.5122659 -0.58379722 0.42579174 0.88968325 0.37058544 -0.38077259 -0.38577652 -0.16602755 0.086493015 0.35030174 0.24920321][-1.8873742 -2.1633623 -2.3988094 -2.2034435 -1.3846931 -0.41794538 0.64445877 1.2065935 0.632957 -0.25488806 -0.33049297 -0.15861654 0.059317589 0.32962132 0.20688772][-1.6921957 -1.8348715 -2.0158839 -1.9373806 -1.1535568 -0.083176136 1.1491046 1.9197888 1.3784609 0.31648111 0.021224499 0.062515259 0.25054598 0.51590919 0.33737421][-1.6070731 -1.6830111 -1.8584113 -1.8580947 -1.0177765 0.27866077 1.7491908 2.7332377 2.3048172 1.1086192 0.52124357 0.32454491 0.42245007 0.64340591 0.4904952][-1.6394873 -1.7238011 -1.9059763 -1.8909278 -0.98323345 0.41856003 2.0091357 3.1087656 2.8272839 1.589653 0.73599195 0.32509279 0.38463402 0.58678818 0.50336552][-1.7539637 -1.8059976 -1.9115045 -1.8392401 -0.95391965 0.36725998 1.8247581 2.7766476 2.5685453 1.4491935 0.56941891 0.16298151 0.29304504 0.45268583 0.37360907][-1.844882 -1.7978265 -1.720479 -1.5035174 -0.65613151 0.51105452 1.6992092 2.3009849 1.9659991 0.93901634 0.13593435 -0.17960167 0.050923824 0.26764822 0.25911713][-1.8660281 -1.6681497 -1.383064 -1.0295491 -0.23886299 0.80870008 1.8274159 2.2253885 1.8330956 0.8921895 0.093910217 -0.247056 -0.045454502 0.15684462 0.1765337][-1.878804 -1.6464512 -1.232507 -0.71891952 0.09610033 1.0374317 1.9101548 2.2759023 1.997241 1.2400966 0.46386862 -0.0045018196 -0.029749393 0.085896492 0.13581038][-1.9306843 -1.8106806 -1.3751833 -0.70869231 0.18469477 1.0538054 1.7355037 2.050921 1.9376926 1.402287 0.7453289 0.23818588 0.012762547 0.052110195 0.11966038][-2.1687064 -2.2073865 -1.8023546 -1.0718005 -0.12870312 0.7019496 1.2345953 1.4639044 1.4093075 1.05442 0.59925556 0.18907976 -0.083928585 -0.10631371 -0.13272667][-2.3391092 -2.4794466 -2.1361406 -1.4710755 -0.66689086 -0.0071086884 0.36597824 0.50455523 0.45430183 0.19455719 -0.088689327 -0.311625 -0.459126 -0.4995451 -0.65180206]]...]
INFO - root - 2017-12-07 12:03:08.768344: step 9710, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 4.613 sec/batch; 94h:03m:58s remains)
INFO - root - 2017-12-07 12:03:54.164653: step 9720, loss = 0.80, batch loss = 0.72 (7.1 examples/sec; 4.539 sec/batch; 92h:32m:34s remains)
INFO - root - 2017-12-07 12:04:39.788415: step 9730, loss = 0.79, batch loss = 0.72 (7.2 examples/sec; 4.426 sec/batch; 90h:14m:04s remains)
INFO - root - 2017-12-07 12:05:25.469571: step 9740, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.584 sec/batch; 93h:26m:12s remains)
INFO - root - 2017-12-07 12:06:11.067351: step 9750, loss = 0.71, batch loss = 0.63 (7.0 examples/sec; 4.591 sec/batch; 93h:33m:56s remains)
INFO - root - 2017-12-07 12:06:56.728215: step 9760, loss = 0.85, batch loss = 0.78 (6.8 examples/sec; 4.679 sec/batch; 95h:20m:44s remains)
INFO - root - 2017-12-07 12:07:42.692785: step 9770, loss = 0.67, batch loss = 0.59 (7.0 examples/sec; 4.604 sec/batch; 93h:48m:24s remains)
INFO - root - 2017-12-07 12:08:28.164082: step 9780, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.562 sec/batch; 92h:56m:23s remains)
INFO - root - 2017-12-07 12:09:13.811700: step 9790, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.570 sec/batch; 93h:05m:47s remains)
INFO - root - 2017-12-07 12:09:59.331845: step 9800, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.504 sec/batch; 91h:44m:08s remains)
2017-12-07 12:10:01.982583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.782285 -3.7726965 -3.7789159 -3.7857614 -3.7784724 -3.7781157 -3.7952514 -3.8105156 -3.8097477 -3.7972131 -3.7905719 -3.7817957 -3.7622032 -3.7374716 -3.6784573][-3.6899877 -3.6317527 -3.6339331 -3.6567559 -3.6490667 -3.647645 -3.6717296 -3.6956198 -3.70693 -3.6970778 -3.6770084 -3.6371834 -3.5773783 -3.530499 -3.4749522][-3.3714316 -3.2324824 -3.2327876 -3.2997656 -3.3377836 -3.3762841 -3.4363728 -3.4936738 -3.5488682 -3.5684078 -3.5356231 -3.4375088 -3.2584362 -3.1122975 -3.0673423][-2.8960798 -2.6723034 -2.678972 -2.8114219 -2.9130592 -3.0037086 -3.1131763 -3.2468481 -3.4027748 -3.4881251 -3.468683 -3.3269672 -2.9894967 -2.6666694 -2.6063538][-2.4048963 -2.1011291 -2.1050541 -2.2823577 -2.4011507 -2.4935722 -2.6128702 -2.7786541 -2.9641094 -3.0200648 -2.9940257 -2.8708777 -2.4391353 -1.9699938 -1.9475183][-1.8165855 -1.4362297 -1.4491048 -1.6467049 -1.7687855 -1.8727205 -1.988193 -2.1071775 -2.1675348 -2.0496867 -2.0007229 -1.9951804 -1.5746458 -1.0553336 -1.153712][-1.262862 -0.86516714 -0.90583563 -1.0815592 -1.2026429 -1.3182991 -1.3434143 -1.264467 -1.0557473 -0.78784609 -0.90421939 -1.2258105 -0.98121619 -0.53517151 -0.77095342][-1.0625808 -0.71345234 -0.73663449 -0.82172966 -0.89355445 -0.976002 -0.85376692 -0.53425384 -0.08329916 0.12532234 -0.396338 -1.1547475 -1.1544738 -0.82267332 -1.0993133][-1.337327 -1.0554469 -0.9881537 -0.92335725 -0.94143081 -1.0567505 -0.9469142 -0.57275891 -0.056388855 -0.0063710213 -0.78028822 -1.7136233 -1.7605071 -1.3800714 -1.5190442][-1.7386203 -1.481075 -1.2928255 -1.1529589 -1.2415695 -1.5374703 -1.5976672 -1.3127723 -0.85743475 -0.83032107 -1.4988005 -2.2112381 -2.03759 -1.493803 -1.4467187][-2.0209842 -1.6805916 -1.3729446 -1.2725668 -1.5515652 -2.0953875 -2.358175 -2.164325 -1.7238472 -1.6134334 -2.0117626 -2.3457761 -1.9056609 -1.2531657 -1.1465571][-2.1604936 -1.8064661 -1.4784956 -1.4560337 -1.8325667 -2.4319677 -2.743469 -2.5914509 -2.1891837 -1.9999325 -2.1603162 -2.2146921 -1.6679938 -1.0527923 -1.0030537][-2.3594415 -2.0755568 -1.8381023 -1.8840773 -2.1801081 -2.627317 -2.8693931 -2.7570581 -2.4297972 -2.209198 -2.1856976 -2.0908461 -1.6277747 -1.2301624 -1.3026471][-2.5875716 -2.352644 -2.181066 -2.2069674 -2.3612292 -2.64293 -2.84122 -2.7875361 -2.5255342 -2.3129659 -2.2438686 -2.16496 -1.9174333 -1.7878041 -1.9615293][-2.9136515 -2.738915 -2.6095035 -2.56994 -2.5834167 -2.7054603 -2.847424 -2.8353672 -2.6461883 -2.5083079 -2.532335 -2.6062665 -2.5905867 -2.608459 -2.7672782]]...]
INFO - root - 2017-12-07 12:10:47.669792: step 9810, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.580 sec/batch; 93h:16m:02s remains)
INFO - root - 2017-12-07 12:11:33.019638: step 9820, loss = 0.81, batch loss = 0.73 (7.3 examples/sec; 4.400 sec/batch; 89h:35m:58s remains)
INFO - root - 2017-12-07 12:12:18.736779: step 9830, loss = 0.84, batch loss = 0.76 (6.9 examples/sec; 4.628 sec/batch; 94h:12m:58s remains)
INFO - root - 2017-12-07 12:13:04.422534: step 9840, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.600 sec/batch; 93h:39m:01s remains)
INFO - root - 2017-12-07 12:13:50.246352: step 9850, loss = 0.78, batch loss = 0.71 (6.9 examples/sec; 4.607 sec/batch; 93h:46m:44s remains)
INFO - root - 2017-12-07 12:14:35.704773: step 9860, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.574 sec/batch; 93h:05m:09s remains)
INFO - root - 2017-12-07 12:15:21.188989: step 9870, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.544 sec/batch; 92h:27m:18s remains)
INFO - root - 2017-12-07 12:16:06.780659: step 9880, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.581 sec/batch; 93h:11m:53s remains)
INFO - root - 2017-12-07 12:16:52.345253: step 9890, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.658 sec/batch; 94h:45m:55s remains)
INFO - root - 2017-12-07 12:17:37.980199: step 9900, loss = 0.68, batch loss = 0.60 (6.9 examples/sec; 4.628 sec/batch; 94h:08m:41s remains)
2017-12-07 12:17:40.596421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.473212 -3.7122285 -3.9169765 -3.7953002 -3.3331118 -2.8951941 -2.6080515 -2.5115423 -2.3048296 -1.595124 -0.78679752 -0.27304077 -0.57597089 -2.1277392 -3.7698226][-3.6584759 -3.6252782 -3.6134455 -3.4000421 -3.0097017 -2.7736351 -2.7045746 -2.6883078 -2.3866713 -1.5076952 -0.51781249 -0.010859966 -0.58869338 -2.3075781 -3.9076815][-3.7305799 -3.4840286 -3.338407 -3.0671701 -2.6978788 -2.5581617 -2.577282 -2.5279665 -2.1154027 -1.230593 -0.29752827 0.041642189 -0.80008531 -2.5536942 -4.0095663][-3.5331662 -3.1841135 -3.0370526 -2.783031 -2.3844855 -2.2416019 -2.2415891 -2.0505388 -1.5025573 -0.76399684 -0.15345144 -0.16248465 -1.2182355 -2.8596296 -4.0966449][-3.186717 -2.8032439 -2.7374086 -2.5404615 -2.0890441 -1.8903723 -1.7983763 -1.4101756 -0.78428483 -0.33177423 -0.19596195 -0.61289859 -1.7826526 -3.1848197 -4.1619039][-2.8532729 -2.4738441 -2.473062 -2.310533 -1.7787187 -1.4730871 -1.2237003 -0.64412189 -0.098698616 -0.080971241 -0.45919347 -1.2012866 -2.3417344 -3.4441504 -4.1877909][-2.5970728 -2.2708316 -2.2926154 -2.1217761 -1.4984281 -1.0091789 -0.49061489 0.27449989 0.56364727 0.018604279 -0.84397364 -1.776829 -2.7754579 -3.5939775 -4.1983461][-2.4014533 -2.167125 -2.1694989 -1.963361 -1.2858045 -0.60207367 0.22596073 1.1707335 1.1642003 0.11191511 -1.072087 -2.0864217 -2.9528863 -3.5939691 -4.185914][-2.3271689 -2.1600246 -2.103914 -1.8688073 -1.1993392 -0.4050765 0.61534071 1.6271391 1.4035778 0.11708784 -1.0850153 -2.0602057 -2.8553209 -3.4475052 -4.1399674][-2.4006927 -2.2682214 -2.1475427 -1.894403 -1.2692032 -0.48364496 0.49634075 1.3451543 0.9919529 -0.16434956 -1.0170357 -1.7913158 -2.5461745 -3.1690655 -4.0481119][-2.4949567 -2.4025679 -2.2529447 -2.0015819 -1.4385622 -0.72686052 0.071751595 0.61830473 0.18104458 -0.64072967 -0.94730139 -1.3967454 -2.0439894 -2.6776662 -3.7691345][-2.4892194 -2.4568582 -2.3453052 -2.1296082 -1.6469915 -1.0403292 -0.45910621 -0.21295023 -0.68877554 -1.1566217 -0.97596049 -1.0829117 -1.5050395 -1.9952331 -3.1435776][-2.4319835 -2.4758747 -2.4419694 -2.2682903 -1.8839645 -1.4085364 -1.0509837 -1.0439832 -1.490802 -1.6730733 -1.2044191 -1.0490599 -1.2009778 -1.4395776 -2.4089146][-2.4508269 -2.5538974 -2.5642829 -2.4191871 -2.1677961 -1.8558207 -1.690062 -1.8097842 -2.1246028 -2.0884519 -1.5581985 -1.2929904 -1.2756495 -1.3607934 -2.0432293][-2.478333 -2.6201503 -2.6392584 -2.524487 -2.4328506 -2.2878654 -2.2564597 -2.4007578 -2.5207698 -2.3135931 -1.8498597 -1.5641809 -1.5138202 -1.6917779 -2.2227495]]...]
INFO - root - 2017-12-07 12:18:26.297571: step 9910, loss = 0.80, batch loss = 0.72 (7.0 examples/sec; 4.592 sec/batch; 93h:23m:41s remains)
INFO - root - 2017-12-07 12:19:12.255971: step 9920, loss = 0.85, batch loss = 0.78 (7.1 examples/sec; 4.539 sec/batch; 92h:17m:55s remains)
INFO - root - 2017-12-07 12:19:57.736134: step 9930, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.603 sec/batch; 93h:35m:27s remains)
INFO - root - 2017-12-07 12:20:43.590146: step 9940, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.581 sec/batch; 93h:07m:13s remains)
INFO - root - 2017-12-07 12:21:29.233087: step 9950, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.584 sec/batch; 93h:11m:10s remains)
INFO - root - 2017-12-07 12:22:14.955442: step 9960, loss = 0.77, batch loss = 0.69 (7.0 examples/sec; 4.576 sec/batch; 93h:00m:02s remains)
INFO - root - 2017-12-07 12:23:00.353606: step 9970, loss = 0.67, batch loss = 0.59 (7.1 examples/sec; 4.514 sec/batch; 91h:43m:37s remains)
INFO - root - 2017-12-07 12:23:46.165093: step 9980, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 4.621 sec/batch; 93h:53m:17s remains)
INFO - root - 2017-12-07 12:24:31.788712: step 9990, loss = 0.77, batch loss = 0.69 (7.0 examples/sec; 4.588 sec/batch; 93h:12m:00s remains)
INFO - root - 2017-12-07 12:25:17.484961: step 10000, loss = 0.82, batch loss = 0.75 (6.8 examples/sec; 4.672 sec/batch; 94h:54m:35s remains)
2017-12-07 12:25:20.324346: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1319046 -2.3864887 -2.7033815 -2.7514567 -2.6208177 -2.4359775 -2.2567897 -2.1425402 -2.0150471 -1.9477146 -2.0277772 -2.144381 -2.2222643 -2.2919552 -2.3844979][-1.4522913 -1.8688347 -2.3660557 -2.4503784 -2.3088925 -2.0821843 -1.8648658 -1.7837923 -1.6864798 -1.6461306 -1.7324686 -1.8087487 -1.8335438 -1.8435414 -1.9184082][-0.92594147 -1.4695883 -2.1429749 -2.3246253 -2.2654078 -2.0648346 -1.8341684 -1.7808218 -1.7148836 -1.6952384 -1.694427 -1.6249423 -1.5424759 -1.4634616 -1.4921236][-0.84384775 -1.4116697 -2.1688871 -2.4620719 -2.5191922 -2.3731177 -2.1174731 -2.0160315 -1.9778037 -2.0412915 -1.9654069 -1.7219629 -1.5386431 -1.4052968 -1.3574624][-1.1750543 -1.7559779 -2.4875078 -2.8256009 -2.8960924 -2.6503081 -2.2139237 -1.8684108 -1.8195071 -2.0873904 -2.1089673 -1.8638074 -1.7275789 -1.6595755 -1.5837579][-1.4448287 -2.1009107 -2.7726123 -3.0907049 -3.0340419 -2.5355332 -1.794668 -1.1096725 -1.0609934 -1.6000309 -1.8374999 -1.7140539 -1.7041643 -1.7650056 -1.7194138][-1.473423 -2.2100222 -2.7963688 -2.9995983 -2.7187369 -1.9580481 -0.94533563 -0.0083389282 -0.065631866 -0.91489148 -1.4016349 -1.4491854 -1.5777323 -1.7573254 -1.7329688][-1.5155456 -2.2755475 -2.7426343 -2.741689 -2.2455742 -1.3921812 -0.30200624 0.70535851 0.49097443 -0.55780292 -1.2321966 -1.4628854 -1.7173979 -1.9121659 -1.8028035][-1.6154773 -2.3137352 -2.6978006 -2.5417428 -1.9782243 -1.2887347 -0.41755581 0.38549566 0.039285183 -0.94291973 -1.5603981 -1.8424959 -2.1319153 -2.2120423 -1.8983142][-1.8337181 -2.3736694 -2.7266798 -2.5122075 -1.9717889 -1.5335281 -1.0265918 -0.57944894 -0.99560857 -1.7412093 -2.1345916 -2.3743176 -2.6432776 -2.5929213 -2.0975437][-2.2694228 -2.6293783 -2.9440598 -2.72546 -2.2457047 -2.0060554 -1.8070257 -1.6552539 -2.0195785 -2.443377 -2.5803714 -2.7597384 -3.0297146 -2.9171252 -2.3510077][-2.9108791 -3.137846 -3.3957791 -3.1848483 -2.758575 -2.6212583 -2.6281159 -2.7022209 -2.9850965 -3.1430731 -3.0762413 -3.1839309 -3.4266429 -3.2928045 -2.7509432][-3.5621531 -3.6926699 -3.8767343 -3.6721635 -3.287663 -3.1723576 -3.2749281 -3.4926472 -3.7533453 -3.8178587 -3.7111616 -3.7834613 -3.9563618 -3.8160598 -3.3727083][-3.907619 -3.9653618 -4.0575013 -3.8617272 -3.536931 -3.4321024 -3.5637455 -3.8188729 -4.0505819 -4.1102743 -4.0735612 -4.1657329 -4.2865663 -4.1834207 -3.8883862][-3.7431297 -3.7595758 -3.7839313 -3.635453 -3.4238286 -3.3754435 -3.5148294 -3.7340007 -3.8950105 -3.9250064 -3.929142 -4.0256276 -4.11638 -4.0690932 -3.9250984]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-batch32/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-batch32/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 12:26:06.635958: step 10010, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.576 sec/batch; 92h:55m:53s remains)
INFO - root - 2017-12-07 12:26:52.416107: step 10020, loss = 0.92, batch loss = 0.84 (6.9 examples/sec; 4.607 sec/batch; 93h:33m:30s remains)
INFO - root - 2017-12-07 12:27:37.860883: step 10030, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.581 sec/batch; 93h:01m:13s remains)
INFO - root - 2017-12-07 12:28:23.730450: step 10040, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 4.627 sec/batch; 93h:56m:04s remains)
INFO - root - 2017-12-07 12:29:09.560385: step 10050, loss = 0.74, batch loss = 0.67 (6.8 examples/sec; 4.694 sec/batch; 95h:16m:26s remains)
INFO - root - 2017-12-07 12:29:55.304268: step 10060, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.604 sec/batch; 93h:27m:07s remains)
INFO - root - 2017-12-07 12:30:40.974494: step 10070, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 4.476 sec/batch; 90h:49m:56s remains)
INFO - root - 2017-12-07 12:31:26.613856: step 10080, loss = 0.88, batch loss = 0.81 (7.0 examples/sec; 4.566 sec/batch; 92h:38m:49s remains)
INFO - root - 2017-12-07 12:32:12.316781: step 10090, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.592 sec/batch; 93h:09m:59s remains)
INFO - root - 2017-12-07 12:32:58.000607: step 10100, loss = 0.76, batch loss = 0.69 (6.8 examples/sec; 4.695 sec/batch; 95h:13m:55s remains)
2017-12-07 12:33:00.738810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5070608 -1.2466784 -0.87014842 -0.77790213 -0.80068469 -0.39802265 -0.17717123 -0.34067631 -0.44204807 -0.36168003 -0.23431969 -0.010669708 -0.22436619 -1.0955133 -2.107013][-1.4532187 -1.2083704 -0.74693894 -0.4995935 -0.3725667 0.021670341 0.082400322 -0.27753925 -0.53099823 -0.54303503 -0.499542 -0.29596853 -0.3909893 -0.96684265 -1.7499573][-1.0043402 -0.84659863 -0.44656134 -0.15269136 0.057646275 0.41718626 0.34493923 -0.18022776 -0.51241231 -0.65357351 -0.83961105 -0.80010033 -0.83433437 -1.160059 -1.7158797][-1.09919 -0.990365 -0.65134406 -0.26597452 0.068713665 0.4515729 0.4088788 0.048367977 -0.1289916 -0.30429983 -0.72022343 -0.7833755 -0.79480004 -1.1119511 -1.648129][-1.2790613 -1.236732 -0.9551897 -0.4150753 0.10690784 0.56265211 0.65294218 0.49778605 0.45391273 0.2534709 -0.35120392 -0.52074194 -0.5776515 -0.94197869 -1.4541645][-0.67455339 -0.79343963 -0.72019958 -0.27870846 0.19738531 0.67088079 0.85013008 0.78432322 0.68991184 0.40442085 -0.3004775 -0.56194115 -0.64458942 -1.0180733 -1.4804952][-0.87307739 -0.94863892 -0.87580848 -0.43031812 0.1316452 0.73052406 1.0276814 1.006484 0.84015322 0.58580065 -0.08676672 -0.40381575 -0.47189498 -0.85624266 -1.4292264][-1.7887704 -1.6947386 -1.4617498 -0.85038066 -0.033649445 0.74706173 1.0938392 1.0425344 0.85664511 0.66371155 -0.00972271 -0.45397925 -0.54254174 -0.95951009 -1.6184626][-2.0390544 -2.0132961 -1.7995276 -1.200217 -0.42674875 0.12845278 0.32576227 0.38724947 0.37978458 0.30975771 -0.3455224 -0.91878486 -0.98885083 -1.3838835 -2.0385532][-1.8483467 -1.9593816 -1.923979 -1.5414684 -1.0567605 -0.8849833 -0.80018044 -0.61771011 -0.49841738 -0.41802502 -0.883847 -1.4793034 -1.5823038 -1.9310246 -2.5261731][-1.8493819 -1.9685624 -1.9749055 -1.6869099 -1.3571818 -1.3354261 -1.2889514 -1.1864271 -1.2346549 -1.178262 -1.5037663 -2.0923228 -2.2728231 -2.5595956 -2.9971123][-2.0934362 -2.1502678 -2.187088 -2.0186768 -1.8227887 -1.827491 -1.7749319 -1.72599 -1.8187294 -1.7484245 -1.9738297 -2.4827011 -2.6409039 -2.8235774 -3.1520448][-2.2621522 -2.3596041 -2.53965 -2.6115313 -2.6246364 -2.6383374 -2.5605178 -2.4517577 -2.4184628 -2.2574029 -2.356348 -2.6978984 -2.7534118 -2.8623652 -3.1412663][-2.8160138 -2.8925285 -3.0674381 -3.2200284 -3.3216395 -3.363719 -3.3478041 -3.3327065 -3.3137112 -3.2217057 -3.2560606 -3.355732 -3.2635882 -3.2660732 -3.4143598][-3.3183854 -3.4033179 -3.5271757 -3.6527836 -3.7430778 -3.7854981 -3.8418822 -3.9191566 -3.9694891 -3.9632211 -3.9643075 -3.9071534 -3.7746441 -3.7225137 -3.7322938]]...]
INFO - root - 2017-12-07 12:33:46.401422: step 10110, loss = 0.87, batch loss = 0.79 (7.0 examples/sec; 4.603 sec/batch; 93h:21m:57s remains)
INFO - root - 2017-12-07 12:34:32.167561: step 10120, loss = 0.82, batch loss = 0.74 (7.0 examples/sec; 4.593 sec/batch; 93h:08m:05s remains)
INFO - root - 2017-12-07 12:35:17.867726: step 10130, loss = 0.75, batch loss = 0.68 (7.2 examples/sec; 4.453 sec/batch; 90h:17m:57s remains)
INFO - root - 2017-12-07 12:36:03.371744: step 10140, loss = 0.81, batch loss = 0.74 (7.2 examples/sec; 4.461 sec/batch; 90h:26m:24s remains)
INFO - root - 2017-12-07 12:36:49.086173: step 10150, loss = 0.69, batch loss = 0.62 (6.9 examples/sec; 4.611 sec/batch; 93h:28m:28s remains)
INFO - root - 2017-12-07 12:37:34.498228: step 10160, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.538 sec/batch; 91h:58m:20s remains)
INFO - root - 2017-12-07 12:38:20.477189: step 10170, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.588 sec/batch; 92h:58m:59s remains)
INFO - root - 2017-12-07 12:39:05.912918: step 10180, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.552 sec/batch; 92h:14m:38s remains)
INFO - root - 2017-12-07 12:39:51.849415: step 10190, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.583 sec/batch; 92h:51m:00s remains)
INFO - root - 2017-12-07 12:40:37.390163: step 10200, loss = 0.73, batch loss = 0.65 (7.0 examples/sec; 4.551 sec/batch; 92h:10m:46s remains)
2017-12-07 12:40:40.105404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0639625 -1.661226 -1.5614674 -1.6068296 -1.643558 -1.8285646 -2.1699934 -2.4815228 -2.4948008 -2.2702191 -1.8496733 -1.5458684 -1.8402722 -2.6157763 -3.3572922][-2.1173637 -1.8267624 -1.788527 -1.6473167 -1.4416878 -1.520324 -1.8902605 -2.232527 -2.3312333 -2.0882323 -1.4792352 -1.0434005 -1.3718145 -2.1594872 -2.9080825][-1.9839292 -1.828779 -1.8220725 -1.6008554 -1.3470521 -1.3213787 -1.5497971 -1.8399365 -2.068408 -1.8643615 -1.1286936 -0.65508413 -0.95927429 -1.6246967 -2.2995358][-1.6140838 -1.619108 -1.6728408 -1.5121322 -1.2974596 -1.1328521 -1.0803151 -1.2121789 -1.4969466 -1.3563778 -0.64941621 -0.2632699 -0.44839764 -0.85131669 -1.4601805][-1.1876001 -1.4972513 -1.8033416 -1.9236605 -1.834846 -1.5263512 -1.1931217 -1.2173204 -1.5980809 -1.5709748 -1.012367 -0.66602063 -0.59706116 -0.58120275 -0.96041226][-0.45026731 -0.88768911 -1.4282601 -1.8000786 -1.8157663 -1.4297273 -1.022053 -1.19995 -1.8500252 -2.0816252 -1.7897551 -1.4981799 -1.2415414 -0.88597822 -0.93628287][-0.20376682 -0.35141134 -0.70766711 -0.97023058 -0.82016563 -0.22530842 0.23251057 -0.20532227 -1.1531188 -1.6995785 -1.7135763 -1.5843735 -1.3675716 -1.0135033 -1.0010192][-1.3378513 -1.1299706 -1.0808668 -0.97705293 -0.43712473 0.50497437 1.1705275 0.724576 -0.18632126 -0.79571104 -0.99621511 -1.017684 -0.81092262 -0.47950268 -0.54578662][-2.4112513 -2.241467 -2.1513693 -1.9915197 -1.410656 -0.54726458 0.0454154 -0.070998192 -0.47837138 -0.72419786 -0.80843663 -0.76150942 -0.38615274 0.074995518 0.017011642][-2.9857635 -2.883245 -2.8248935 -2.7395375 -2.3949194 -1.8911657 -1.5540569 -1.4680202 -1.5327883 -1.5774045 -1.6006153 -1.4890521 -1.0618477 -0.613055 -0.60710168][-3.7062342 -3.5870693 -3.4712296 -3.3503475 -3.12461 -2.7977815 -2.5281544 -2.3066535 -2.2352688 -2.3127048 -2.4314518 -2.4073191 -2.1179564 -1.8566165 -1.8630633][-4.0416288 -4.0007887 -3.9716985 -3.9540849 -3.9183993 -3.7761273 -3.5671072 -3.3140516 -3.1519728 -3.1509087 -3.2074435 -3.1752415 -2.969614 -2.7841685 -2.7227566][-4.043457 -3.9888263 -3.9950788 -4.0753031 -4.2010589 -4.2392979 -4.168879 -4.0289593 -3.922719 -3.9530046 -4.0319223 -4.0535131 -3.9431095 -3.7636454 -3.5577788][-4.2069693 -4.0926447 -4.0359 -4.0469089 -4.13029 -4.171844 -4.1544347 -4.0855255 -4.0494428 -4.1521626 -4.3011189 -4.4052439 -4.4240003 -4.312366 -4.0839076][-4.2139311 -4.1547265 -4.0919895 -4.0072894 -3.9655256 -3.9155562 -3.8952458 -3.877038 -3.909008 -4.0344577 -4.1263065 -4.1494651 -4.1351252 -4.0270939 -3.8587532]]...]
INFO - root - 2017-12-07 12:41:26.092773: step 10210, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.589 sec/batch; 92h:56m:25s remains)
INFO - root - 2017-12-07 12:42:11.465787: step 10220, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.552 sec/batch; 92h:11m:14s remains)
INFO - root - 2017-12-07 12:42:57.654423: step 10230, loss = 0.80, batch loss = 0.73 (6.9 examples/sec; 4.638 sec/batch; 93h:54m:43s remains)
INFO - root - 2017-12-07 12:43:43.445839: step 10240, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.505 sec/batch; 91h:13m:00s remains)
INFO - root - 2017-12-07 12:44:29.141413: step 10250, loss = 0.71, batch loss = 0.64 (6.9 examples/sec; 4.651 sec/batch; 94h:09m:35s remains)
INFO - root - 2017-12-07 12:45:14.998187: step 10260, loss = 0.69, batch loss = 0.62 (6.9 examples/sec; 4.617 sec/batch; 93h:27m:07s remains)
INFO - root - 2017-12-07 12:46:00.900669: step 10270, loss = 0.87, batch loss = 0.80 (6.9 examples/sec; 4.670 sec/batch; 94h:30m:12s remains)
INFO - root - 2017-12-07 12:46:46.839163: step 10280, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 4.638 sec/batch; 93h:50m:41s remains)
INFO - root - 2017-12-07 12:47:32.682102: step 10290, loss = 0.71, batch loss = 0.64 (6.9 examples/sec; 4.655 sec/batch; 94h:10m:25s remains)
INFO - root - 2017-12-07 12:48:18.466419: step 10300, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.610 sec/batch; 93h:15m:07s remains)
2017-12-07 12:48:21.218550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5786595 -3.2215414 -2.8989537 -2.6914573 -2.4997852 -2.2226987 -2.0094109 -2.0865979 -2.2704155 -2.3169889 -2.3056457 -2.3998213 -2.4717445 -2.3671687 -2.3695464][-3.6293807 -3.3408232 -3.0531366 -2.8374448 -2.56215 -2.2065487 -2.0770097 -2.2780945 -2.4262178 -2.2978573 -2.1754637 -2.3547099 -2.6370873 -2.6693907 -2.7839627][-3.7001615 -3.4587388 -3.1367407 -2.7869153 -2.3008332 -1.7938633 -1.7346971 -2.1349385 -2.4393888 -2.3151987 -2.0917697 -2.2542965 -2.6518221 -2.78716 -2.9816005][-3.7372787 -3.504993 -3.1581089 -2.716908 -2.0585496 -1.4479387 -1.471426 -2.1120284 -2.6979809 -2.6937737 -2.400089 -2.4938405 -2.8843904 -2.965452 -3.0197952][-3.7366204 -3.5367546 -3.2847109 -2.9066784 -2.1995139 -1.5245676 -1.4965074 -2.1428189 -2.8851447 -3.0812955 -2.9052513 -3.0711305 -3.4520831 -3.3794861 -3.1249824][-3.7620213 -3.6426194 -3.5296931 -3.2042706 -2.3743181 -1.4578605 -1.042485 -1.3664026 -2.1509905 -2.6609516 -2.8825421 -3.3831863 -3.8676927 -3.7279418 -3.2690318][-3.7979989 -3.6974373 -3.5569117 -3.0791535 -1.9916258 -0.71778107 0.21468639 0.25485945 -0.66531229 -1.6187477 -2.3497484 -3.17304 -3.6835964 -3.5445728 -3.1352949][-3.8062007 -3.6194835 -3.3170915 -2.623415 -1.3738468 0.10835552 1.3954144 1.6636915 0.52619743 -0.81053877 -1.8919709 -2.7864132 -3.0834508 -2.8468308 -2.5644875][-3.7869682 -3.5201375 -3.1597314 -2.510437 -1.4893601 -0.21868849 1.0022302 1.2494564 0.13309574 -1.0938165 -2.0067112 -2.5760417 -2.4016438 -1.8935618 -1.6029456][-3.7671781 -3.4901009 -3.1907177 -2.7559929 -2.1412165 -1.2935345 -0.39263535 -0.32988119 -1.2587621 -2.0085428 -2.4116042 -2.4798765 -1.8694906 -1.1246066 -0.67434835][-3.7037046 -3.4015312 -3.1071157 -2.8100705 -2.5310502 -2.1096332 -1.58601 -1.681829 -2.3341336 -2.5416284 -2.5185709 -2.3480937 -1.6800675 -1.000947 -0.4399929][-3.6367044 -3.2722595 -2.8277483 -2.4361281 -2.2385597 -2.0672772 -1.8491061 -2.0841069 -2.5651381 -2.5213346 -2.4253323 -2.3709416 -1.9998682 -1.6652811 -1.1293595][-3.6560218 -3.2709007 -2.6738834 -2.0931852 -1.7638886 -1.5578675 -1.3855898 -1.6353474 -2.0553322 -2.1150343 -2.2798393 -2.5306449 -2.5308666 -2.5793347 -2.209825][-3.7180166 -3.4029405 -2.803998 -2.1633811 -1.6733153 -1.1896465 -0.75445676 -0.81355929 -1.2164919 -1.5310266 -1.9864163 -2.4360046 -2.6892483 -3.0534427 -2.9468174][-3.7419076 -3.5450649 -3.0994482 -2.64366 -2.1956453 -1.4626539 -0.693238 -0.48822474 -0.81453824 -1.254065 -1.7125819 -1.9997489 -2.2154367 -2.6636639 -2.8031728]]...]
INFO - root - 2017-12-07 12:49:06.976267: step 10310, loss = 0.74, batch loss = 0.66 (7.0 examples/sec; 4.595 sec/batch; 92h:56m:04s remains)
INFO - root - 2017-12-07 12:49:52.556979: step 10320, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.564 sec/batch; 92h:17m:57s remains)
INFO - root - 2017-12-07 12:50:38.314095: step 10330, loss = 0.78, batch loss = 0.71 (6.9 examples/sec; 4.633 sec/batch; 93h:40m:23s remains)
INFO - root - 2017-12-07 12:51:24.230987: step 10340, loss = 0.72, batch loss = 0.64 (6.9 examples/sec; 4.629 sec/batch; 93h:35m:54s remains)
INFO - root - 2017-12-07 12:52:10.094361: step 10350, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.627 sec/batch; 93h:31m:38s remains)
INFO - root - 2017-12-07 12:52:55.682360: step 10360, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.579 sec/batch; 92h:33m:37s remains)
INFO - root - 2017-12-07 12:53:41.317238: step 10370, loss = 0.68, batch loss = 0.61 (6.9 examples/sec; 4.640 sec/batch; 93h:46m:30s remains)
INFO - root - 2017-12-07 12:54:27.162162: step 10380, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.604 sec/batch; 93h:01m:32s remains)
INFO - root - 2017-12-07 12:55:12.929058: step 10390, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.619 sec/batch; 93h:19m:28s remains)
INFO - root - 2017-12-07 12:55:58.459711: step 10400, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 4.541 sec/batch; 91h:44m:08s remains)
2017-12-07 12:56:01.262726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4739661 -4.3675184 -4.215724 -4.2179394 -4.2118597 -4.2121477 -3.8962684 -3.2595859 -3.1183627 -3.0820336 -2.3865347 -1.9847665 -2.4697337 -3.1111231 -3.8400404][-4.4876723 -4.1947994 -4.0025496 -4.1967378 -4.4104638 -4.4332981 -3.9504919 -3.2786479 -3.2498634 -2.9489155 -1.6718264 -1.1085432 -1.8463085 -2.8401017 -3.9439826][-4.349226 -3.8226309 -3.5384216 -3.9020629 -4.2544484 -4.1072936 -3.3791268 -2.7649794 -2.7256446 -1.9857328 -0.28875017 0.0059351921 -1.31408 -2.7800441 -4.1493664][-4.125247 -3.3649297 -2.9543116 -3.4346738 -3.7431848 -3.2054949 -2.2601979 -1.8000193 -1.6149311 -0.3589201 1.4513249 1.0427232 -1.0066228 -2.8806086 -4.3324265][-4.0021005 -3.1425233 -2.6230869 -3.1418529 -3.2753916 -2.2988112 -1.3072088 -1.0868735 -0.66288948 1.0954022 2.7615967 1.5677075 -0.90995836 -2.9453814 -4.3179903][-4.14185 -3.3201118 -2.6874027 -3.1200922 -2.9619198 -1.5553794 -0.67384505 -0.78080106 -0.17715788 1.8494477 3.0006242 1.199996 -1.2898076 -3.1597943 -4.31851][-4.3788538 -3.6000166 -2.8128111 -3.1151609 -2.6546435 -0.91010952 -0.25322962 -0.75416708 -0.12114286 1.8592 2.3659177 0.39309788 -1.7352879 -3.2909534 -4.1877317][-4.4965534 -3.6791921 -2.7040269 -2.9117005 -2.2196882 -0.24706554 0.11320591 -0.81147146 -0.22600651 1.6217003 1.7154026 -0.11256838 -1.7834935 -3.1736999 -3.9445167][-4.47541 -3.5903316 -2.4789028 -2.6675534 -1.8246939 0.30278111 0.3518095 -0.92562103 -0.34180355 1.3869538 1.2165928 -0.3302002 -1.6015441 -2.9889297 -3.7674549][-4.4151406 -3.5130706 -2.4012604 -2.6425395 -1.6992519 0.50813866 0.29037 -1.1392789 -0.41018867 1.1818943 0.75247574 -0.54177189 -1.5596716 -2.967042 -3.8122413][-4.4170523 -3.5578184 -2.5195856 -2.7825215 -1.753932 0.4024334 -0.036711216 -1.4030664 -0.44387245 0.99480104 0.37537432 -0.70451474 -1.6654243 -3.1240249 -4.0319095][-4.4924645 -3.6825833 -2.7112393 -2.9056897 -1.78881 0.18513536 -0.45954466 -1.648937 -0.53977418 0.68362188 0.0073642731 -0.86180806 -1.7901466 -3.2768626 -4.2099509][-4.6226063 -3.8975585 -3.0547361 -3.144269 -1.9897749 -0.26978731 -1.0531154 -2.0419137 -1.0144134 -0.16168213 -0.75279069 -1.3334963 -2.1317093 -3.4988453 -4.3343992][-4.7352014 -4.1682892 -3.5332081 -3.5243278 -2.423836 -1.0134344 -1.8435671 -2.700927 -1.9683607 -1.5361288 -1.9442048 -2.2137687 -2.7773108 -3.881629 -4.5303173][-4.6954374 -4.2950826 -3.8506508 -3.7529154 -2.8231807 -1.7767375 -2.5983446 -3.3482549 -2.9376218 -2.8098626 -3.0392184 -3.083549 -3.434087 -4.241847 -4.7000494]]...]
INFO - root - 2017-12-07 12:56:47.006200: step 10410, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.513 sec/batch; 91h:09m:15s remains)
INFO - root - 2017-12-07 12:57:32.726433: step 10420, loss = 0.78, batch loss = 0.71 (7.2 examples/sec; 4.456 sec/batch; 89h:59m:43s remains)
INFO - root - 2017-12-07 12:58:18.287862: step 10430, loss = 0.79, batch loss = 0.71 (7.1 examples/sec; 4.494 sec/batch; 90h:44m:58s remains)
INFO - root - 2017-12-07 12:59:03.750124: step 10440, loss = 0.84, batch loss = 0.77 (7.1 examples/sec; 4.507 sec/batch; 91h:00m:15s remains)
INFO - root - 2017-12-07 12:59:49.490560: step 10450, loss = 0.64, batch loss = 0.57 (7.0 examples/sec; 4.545 sec/batch; 91h:44m:35s remains)
INFO - root - 2017-12-07 13:00:34.831019: step 10460, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.492 sec/batch; 90h:39m:47s remains)
INFO - root - 2017-12-07 13:01:20.022436: step 10470, loss = 0.82, batch loss = 0.75 (7.1 examples/sec; 4.506 sec/batch; 90h:56m:01s remains)
INFO - root - 2017-12-07 13:02:05.776199: step 10480, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.594 sec/batch; 92h:41m:58s remains)
INFO - root - 2017-12-07 13:02:51.168984: step 10490, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.545 sec/batch; 91h:42m:40s remains)
INFO - root - 2017-12-07 13:03:36.300042: step 10500, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 4.502 sec/batch; 90h:48m:49s remains)
2017-12-07 13:03:39.087571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7046008 -3.716882 -3.7013023 -3.392694 -2.8123283 -2.3353171 -2.073987 -2.0652547 -2.439898 -2.9700801 -3.2255242 -3.2437341 -3.1307192 -3.1465929 -3.3945603][-3.5822768 -3.6242149 -3.6789961 -3.4033604 -2.7581377 -2.2457976 -2.0804546 -2.211 -2.5622561 -2.8732882 -2.9005518 -2.8301611 -2.8346977 -3.0810275 -3.4780056][-3.4037242 -3.4109812 -3.4255204 -3.1037025 -2.4406366 -2.0157766 -2.0344656 -2.313952 -2.6673346 -2.7924683 -2.5760889 -2.3149574 -2.3163788 -2.7576933 -3.3650322][-3.1555181 -3.119025 -3.0651255 -2.7436085 -2.2146628 -2.0492976 -2.2955658 -2.6819191 -3.0318351 -3.0162401 -2.5779743 -2.089911 -1.9113948 -2.3302391 -3.0137758][-2.9507275 -2.9250362 -2.8502872 -2.6014709 -2.2806859 -2.3903534 -2.7957051 -3.2212763 -3.5540793 -3.399565 -2.801733 -2.2439485 -1.9416559 -2.210448 -2.7524309][-2.7492239 -2.8042152 -2.7570133 -2.5829656 -2.3922458 -2.5961907 -3.0049367 -3.3830395 -3.6757412 -3.3810992 -2.7870188 -2.5070181 -2.3782945 -2.5845709 -2.8765597][-2.1040368 -2.3244939 -2.3664341 -2.2338958 -2.070766 -2.2116494 -2.4608684 -2.7047606 -2.9586716 -2.6135037 -2.1729875 -2.3773286 -2.6553793 -3.0215769 -3.2323658][-1.1029675 -1.4820819 -1.5802815 -1.4489148 -1.240036 -1.2137301 -1.1888108 -1.2685423 -1.5851774 -1.3547623 -1.0997856 -1.6950164 -2.3397312 -2.945703 -3.314594][-0.59120917 -0.99279213 -1.03668 -0.83951855 -0.56931853 -0.40237236 -0.11123371 -0.04681778 -0.48870397 -0.4408927 -0.24935961 -0.91532707 -1.624666 -2.2487953 -2.7465935][-1.112268 -1.3894358 -1.3121808 -1.0006919 -0.63365412 -0.37938833 0.041759491 0.13454771 -0.43914819 -0.53200626 -0.27152872 -0.68333077 -1.1609344 -1.5659108 -2.0256271][-2.1090271 -2.2272997 -2.1213195 -1.7745934 -1.340585 -1.0647917 -0.6535008 -0.62643194 -1.2901785 -1.4507473 -1.1055706 -1.2057238 -1.4095945 -1.565598 -1.9047141][-2.8018215 -2.856082 -2.8963206 -2.6642528 -2.2966361 -2.1237073 -1.8231134 -1.8294768 -2.4304428 -2.623323 -2.3072336 -2.2388363 -2.2987776 -2.3326857 -2.5460019][-3.0379624 -3.0701852 -3.2635031 -3.1744323 -2.9561682 -2.9542296 -2.8043776 -2.8080728 -3.2528281 -3.4831076 -3.3379414 -3.2779608 -3.3253515 -3.3413048 -3.4532261][-3.1296144 -3.1332562 -3.3483648 -3.303854 -3.1864567 -3.2617502 -3.1785426 -3.1295452 -3.3644788 -3.5826678 -3.6155453 -3.6378477 -3.7454457 -3.8238523 -3.9146521][-3.4327381 -3.4433002 -3.5983136 -3.5339642 -3.4591269 -3.5063195 -3.4074538 -3.2888081 -3.323036 -3.4314551 -3.4982297 -3.4999502 -3.5821819 -3.6832986 -3.7520733]]...]
INFO - root - 2017-12-07 13:04:24.632843: step 10510, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.562 sec/batch; 92h:00m:55s remains)
INFO - root - 2017-12-07 13:05:09.934820: step 10520, loss = 0.73, batch loss = 0.65 (7.1 examples/sec; 4.534 sec/batch; 91h:26m:38s remains)
INFO - root - 2017-12-07 13:05:55.459052: step 10530, loss = 0.74, batch loss = 0.67 (7.2 examples/sec; 4.474 sec/batch; 90h:13m:23s remains)
INFO - root - 2017-12-07 13:06:40.925372: step 10540, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.633 sec/batch; 93h:24m:24s remains)
INFO - root - 2017-12-07 13:07:26.784917: step 10550, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.552 sec/batch; 91h:46m:21s remains)
INFO - root - 2017-12-07 13:08:12.556697: step 10560, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.577 sec/batch; 92h:15m:08s remains)
INFO - root - 2017-12-07 13:08:57.755493: step 10570, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.614 sec/batch; 92h:59m:39s remains)
INFO - root - 2017-12-07 13:09:42.966650: step 10580, loss = 0.80, batch loss = 0.73 (7.2 examples/sec; 4.470 sec/batch; 90h:04m:39s remains)
INFO - root - 2017-12-07 13:10:28.381186: step 10590, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.572 sec/batch; 92h:06m:53s remains)
INFO - root - 2017-12-07 13:11:14.223069: step 10600, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.558 sec/batch; 91h:48m:52s remains)
2017-12-07 13:11:16.989962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9614964 -2.6685476 -2.3728766 -2.1691751 -2.1369932 -2.237771 -2.4550273 -2.7208905 -2.9685063 -3.1660504 -3.3058085 -3.3858757 -3.408807 -3.3562508 -3.2753925][-2.5770764 -2.0649049 -1.5874822 -1.2571883 -1.1646893 -1.2701688 -1.5483885 -1.8600149 -2.0796833 -2.2144837 -2.3050954 -2.378907 -2.4046524 -2.3789644 -2.3116057][-2.0823209 -1.5269294 -1.0835755 -0.84419394 -0.8511436 -0.99595213 -1.2332914 -1.4161777 -1.4246778 -1.3360732 -1.2027802 -1.0814748 -0.94774103 -0.85480571 -0.75050664][-1.9264469 -1.5155315 -1.2490494 -1.2125535 -1.3551457 -1.480279 -1.5680227 -1.594985 -1.4845407 -1.3462067 -1.1707313 -0.97185373 -0.73028731 -0.51538587 -0.31226397][-1.4241192 -1.137352 -1.0273707 -1.1208425 -1.2591956 -1.2412312 -1.132551 -1.0800178 -1.0670807 -1.150007 -1.1930842 -1.1385553 -0.96920204 -0.69258904 -0.39970636][-0.81757045 -0.41319609 -0.20837259 -0.21628761 -0.22237301 -0.080639362 0.037127972 -0.05228138 -0.32564354 -0.75276303 -1.121383 -1.354888 -1.430105 -1.306751 -1.1338151][-0.83907008 -0.42695808 -0.17808008 -0.17793655 -0.24555492 -0.2503624 -0.34671783 -0.59352708 -0.92400193 -1.3527405 -1.7301013 -2.0628552 -2.3575096 -2.5705843 -2.7637618][-1.213156 -0.98289323 -0.8685267 -0.95298028 -1.1345515 -1.2801147 -1.4949772 -1.7257948 -1.8602245 -2.0034871 -2.1443355 -2.3422391 -2.59406 -2.8666468 -3.1444983][-1.3875964 -1.2068124 -1.1300027 -1.190233 -1.3652191 -1.5604861 -1.8323827 -2.0910809 -2.1709704 -2.1894381 -2.2014468 -2.25047 -2.3242857 -2.4109223 -2.463865][-1.8990653 -1.784987 -1.7174902 -1.7758298 -1.9659472 -2.1834333 -2.4647808 -2.7270184 -2.7791603 -2.718147 -2.6133347 -2.5170732 -2.4850025 -2.5197952 -2.5104771][-2.3435073 -2.3417594 -2.345516 -2.4991391 -2.780488 -3.0320759 -3.2186184 -3.3473358 -3.3133574 -3.1784062 -2.9991388 -2.8539081 -2.8236098 -2.9363258 -3.0272031][-2.1776273 -2.1193185 -2.1105769 -2.3055575 -2.6196213 -2.8852777 -3.0769532 -3.2065907 -3.2412508 -3.2386389 -3.2141337 -3.1348581 -3.0132375 -2.9061193 -2.7044663][-1.8509026 -1.8437581 -1.9086661 -2.1473596 -2.4613166 -2.6941757 -2.8047156 -2.8147678 -2.8064818 -2.9009433 -3.0557079 -3.168798 -3.1416955 -2.9623585 -2.5557723][-1.9211671 -2.111455 -2.34593 -2.6594844 -2.9160709 -2.976809 -2.839843 -2.5546966 -2.3037 -2.2538877 -2.3823011 -2.6113725 -2.7533016 -2.6794882 -2.3456542][-1.9181564 -2.1671381 -2.4598453 -2.7896261 -3.0276253 -3.0760655 -2.9497104 -2.663743 -2.3889353 -2.2268796 -2.2180271 -2.3351197 -2.3817933 -2.2217982 -1.8173969]]...]
INFO - root - 2017-12-07 13:12:02.367163: step 10610, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.571 sec/batch; 92h:04m:53s remains)
INFO - root - 2017-12-07 13:12:47.920945: step 10620, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.564 sec/batch; 91h:55m:13s remains)
INFO - root - 2017-12-07 13:13:33.283062: step 10630, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.572 sec/batch; 92h:04m:06s remains)
INFO - root - 2017-12-07 13:14:18.845659: step 10640, loss = 0.75, batch loss = 0.68 (7.3 examples/sec; 4.404 sec/batch; 88h:40m:32s remains)
INFO - root - 2017-12-07 13:15:04.407884: step 10650, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.565 sec/batch; 91h:53m:43s remains)
INFO - root - 2017-12-07 13:15:49.967195: step 10660, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.576 sec/batch; 92h:06m:32s remains)
INFO - root - 2017-12-07 13:16:35.463257: step 10670, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.596 sec/batch; 92h:29m:55s remains)
INFO - root - 2017-12-07 13:17:21.162228: step 10680, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.574 sec/batch; 92h:02m:29s remains)
INFO - root - 2017-12-07 13:18:06.701279: step 10690, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 4.567 sec/batch; 91h:53m:09s remains)
INFO - root - 2017-12-07 13:18:52.501710: step 10700, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.600 sec/batch; 92h:32m:39s remains)
2017-12-07 13:18:55.178775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3170066 -4.2485805 -4.1686583 -4.1672907 -4.306016 -4.4133291 -4.3575439 -4.2150393 -4.0335336 -3.7239184 -3.4416542 -3.296464 -3.3811641 -3.6293588 -3.8957787][-3.7516708 -3.5627575 -3.4524837 -3.5103407 -3.7655716 -3.9628792 -3.8847449 -3.6217682 -3.3204904 -2.9052744 -2.5287089 -2.3385012 -2.399652 -2.6766124 -3.0387874][-3.0914109 -2.8488352 -2.7769828 -2.8742433 -3.1420665 -3.3257632 -3.1783943 -2.866303 -2.6058469 -2.2040579 -1.7835524 -1.5854044 -1.6628397 -1.9763753 -2.3899598][-2.4503348 -2.2720251 -2.3539276 -2.5417259 -2.8761744 -3.0393438 -2.779042 -2.392827 -2.1680653 -1.7776859 -1.350527 -1.2539186 -1.4887614 -1.9189646 -2.3852348][-2.0227642 -1.8901153 -2.0518625 -2.3938801 -2.9331145 -3.1709974 -2.8263597 -2.3535 -2.0611126 -1.5482421 -1.045064 -1.0370195 -1.4649074 -2.0779536 -2.6720743][-2.091562 -1.9100931 -1.9873741 -2.3574057 -3.0057244 -3.2383194 -2.8538177 -2.383445 -2.0937974 -1.5451264 -1.0179684 -1.0807941 -1.6015861 -2.3222773 -3.026083][-2.5023775 -2.3329494 -2.3107522 -2.6025336 -3.1546545 -3.2547832 -2.8164721 -2.3535237 -2.0948236 -1.6197047 -1.148186 -1.1742201 -1.5650172 -2.1885304 -2.9221315][-3.1271429 -2.9251153 -2.7494965 -2.8326716 -3.1313162 -3.0803607 -2.666986 -2.2723472 -2.0456052 -1.6264856 -1.2030723 -1.1759219 -1.428071 -1.9035261 -2.5546341][-3.6785052 -3.4443016 -3.0897248 -2.8454206 -2.8046224 -2.5716681 -2.1744621 -1.8623118 -1.6772239 -1.3026726 -0.92492223 -0.90917277 -1.1770637 -1.6353772 -2.2379692][-3.931916 -3.6622913 -3.1603336 -2.6713424 -2.3697267 -2.0008397 -1.5939515 -1.3471308 -1.2490637 -0.9901495 -0.71532273 -0.72485185 -0.99286604 -1.3951175 -1.8599136][-3.9752066 -3.5817876 -2.9840426 -2.4301062 -2.074074 -1.729028 -1.3737438 -1.2002683 -1.2097967 -1.1320364 -1.0267148 -1.110096 -1.3911994 -1.7307005 -1.9820969][-3.7859957 -3.2083783 -2.5722651 -2.1230974 -1.8730276 -1.6290066 -1.3960755 -1.3667643 -1.5276835 -1.6130407 -1.6442394 -1.7746077 -2.039139 -2.3253241 -2.3982239][-3.4719129 -2.7579002 -2.1447935 -1.8419523 -1.7076662 -1.5473883 -1.4227185 -1.5422225 -1.8610697 -2.1101704 -2.2840445 -2.4823065 -2.7451856 -3.0078177 -2.9864962][-3.334754 -2.6174681 -2.1009548 -1.945466 -1.8918316 -1.7498317 -1.630594 -1.8097064 -2.2051971 -2.5460162 -2.8221292 -3.0800676 -3.3498385 -3.6101274 -3.6049194][-3.585448 -2.9375291 -2.476768 -2.3771431 -2.3744459 -2.2296603 -2.0972323 -2.2929053 -2.6913061 -3.0200279 -3.2716277 -3.4841952 -3.6748409 -3.8566182 -3.8498828]]...]
INFO - root - 2017-12-07 13:19:40.622082: step 10710, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.572 sec/batch; 91h:58m:04s remains)
INFO - root - 2017-12-07 13:20:26.453458: step 10720, loss = 0.67, batch loss = 0.60 (6.9 examples/sec; 4.613 sec/batch; 92h:47m:13s remains)
INFO - root - 2017-12-07 13:21:11.807053: step 10730, loss = 0.84, batch loss = 0.77 (7.2 examples/sec; 4.429 sec/batch; 89h:04m:19s remains)
INFO - root - 2017-12-07 13:21:57.468925: step 10740, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.554 sec/batch; 91h:33m:48s remains)
INFO - root - 2017-12-07 13:22:42.750098: step 10750, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.480 sec/batch; 90h:03m:52s remains)
INFO - root - 2017-12-07 13:23:28.394476: step 10760, loss = 0.84, batch loss = 0.77 (6.9 examples/sec; 4.627 sec/batch; 93h:00m:29s remains)
INFO - root - 2017-12-07 13:24:14.123088: step 10770, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.576 sec/batch; 91h:58m:39s remains)
INFO - root - 2017-12-07 13:24:59.625788: step 10780, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.504 sec/batch; 90h:30m:11s remains)
INFO - root - 2017-12-07 13:25:45.377034: step 10790, loss = 0.67, batch loss = 0.59 (7.0 examples/sec; 4.578 sec/batch; 91h:58m:46s remains)
INFO - root - 2017-12-07 13:26:30.927417: step 10800, loss = 0.74, batch loss = 0.66 (7.0 examples/sec; 4.544 sec/batch; 91h:17m:32s remains)
2017-12-07 13:26:33.529018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7215853 -1.6312649 -1.664784 -1.6624053 -1.6898255 -1.7695217 -1.7478313 -1.5789909 -1.46243 -1.5051384 -1.4999294 -1.2754426 -0.9069066 -0.61560035 -0.70127606][-1.7582412 -1.7195733 -1.7408729 -1.66119 -1.6533048 -1.739511 -1.6617966 -1.3183126 -1.0518708 -1.1207941 -1.2575383 -1.127568 -0.78390408 -0.61277771 -0.80388165][-1.8581014 -1.8731091 -1.8553805 -1.6876135 -1.6659782 -1.7671399 -1.6569324 -1.2176454 -0.88997889 -1.0101819 -1.2509489 -1.2221906 -0.97587037 -0.99487472 -1.2675803][-1.6925111 -1.6707397 -1.5691869 -1.3176737 -1.2891574 -1.3823028 -1.2016106 -0.74216437 -0.532408 -0.78542113 -1.0885739 -1.1142428 -0.99032044 -1.187326 -1.5009544][-1.2748966 -1.3436136 -1.3232574 -1.1076682 -1.0807257 -1.036346 -0.62118316 -0.076583862 0.026074886 -0.24480629 -0.51163006 -0.59705162 -0.63359523 -1.0323651 -1.4417796][-0.67780542 -0.81006193 -0.78879786 -0.51126456 -0.3873682 -0.1367631 0.44050169 0.76934767 0.44698715 0.0348258 -0.18214273 -0.27024746 -0.40191317 -0.91909289 -1.4038584][-0.10031605 -0.076635361 0.17351341 0.56827211 0.72870016 1.0622449 1.5669904 1.3848581 0.45375681 -0.16625977 -0.34684229 -0.40884876 -0.56268954 -1.0661166 -1.46192][0.37913704 0.48227978 0.73893785 0.93955517 0.91710997 1.1679406 1.4411426 0.85384846 -0.271111 -0.81161332 -0.85688806 -0.85410523 -1.0729036 -1.613709 -1.9239807][0.39072943 0.45513344 0.50263262 0.40715218 0.26699781 0.40454769 0.36378384 -0.39939737 -1.3603849 -1.6482005 -1.5494406 -1.4532912 -1.6595757 -2.14645 -2.317275][-0.22100115 -0.19856215 -0.26922369 -0.39570236 -0.41996336 -0.32001972 -0.588619 -1.3019063 -1.922323 -1.9924808 -1.8863544 -1.8140419 -2.0149467 -2.3867421 -2.4478834][-0.76833749 -0.7874167 -0.93076825 -1.0463049 -1.0818751 -1.1857526 -1.5925732 -2.0717669 -2.2981832 -2.2212369 -2.1842365 -2.2343538 -2.4768476 -2.7283254 -2.7147779][-1.4035559 -1.435612 -1.5623212 -1.6351988 -1.7646866 -2.1075 -2.5582967 -2.799674 -2.7797315 -2.678947 -2.6830745 -2.743063 -2.8968775 -2.9613097 -2.8629434][-2.4740639 -2.4995553 -2.5201344 -2.445828 -2.5370498 -2.8895617 -3.2363725 -3.3379612 -3.2919154 -3.2995629 -3.3560462 -3.4102759 -3.4975252 -3.4218822 -3.2454097][-3.4352283 -3.4705153 -3.4571953 -3.3670313 -3.4713 -3.7456203 -3.9246955 -3.8918312 -3.7957983 -3.7996867 -3.8038642 -3.8348131 -3.94457 -3.8896546 -3.7226624][-3.6944234 -3.745908 -3.7478504 -3.6762156 -3.7309568 -3.8766775 -3.9283571 -3.8503957 -3.75886 -3.7447147 -3.7152562 -3.732168 -3.831897 -3.8068295 -3.6861737]]...]
INFO - root - 2017-12-07 13:27:19.260036: step 10810, loss = 0.82, batch loss = 0.74 (7.1 examples/sec; 4.538 sec/batch; 91h:09m:18s remains)
INFO - root - 2017-12-07 13:28:04.473356: step 10820, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.528 sec/batch; 90h:56m:52s remains)
INFO - root - 2017-12-07 13:28:50.272296: step 10830, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.597 sec/batch; 92h:19m:09s remains)
INFO - root - 2017-12-07 13:29:35.771524: step 10840, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 4.489 sec/batch; 90h:08m:27s remains)
INFO - root - 2017-12-07 13:30:21.664019: step 10850, loss = 0.71, batch loss = 0.63 (6.9 examples/sec; 4.636 sec/batch; 93h:04m:55s remains)
INFO - root - 2017-12-07 13:31:06.863586: step 10860, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.563 sec/batch; 91h:36m:12s remains)
INFO - root - 2017-12-07 13:31:52.840133: step 10870, loss = 0.78, batch loss = 0.70 (6.8 examples/sec; 4.695 sec/batch; 94h:13m:39s remains)
INFO - root - 2017-12-07 13:32:38.321320: step 10880, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 4.533 sec/batch; 90h:57m:58s remains)
INFO - root - 2017-12-07 13:33:23.946832: step 10890, loss = 0.75, batch loss = 0.68 (7.3 examples/sec; 4.395 sec/batch; 88h:11m:14s remains)
INFO - root - 2017-12-07 13:34:09.390188: step 10900, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.563 sec/batch; 91h:33m:13s remains)
2017-12-07 13:34:12.115736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4402251 -2.4801426 -2.5286582 -2.540915 -2.530087 -2.4934549 -2.3007865 -2.0152721 -1.9347117 -2.1288309 -2.4373899 -2.7112575 -2.8719244 -2.9089708 -2.7957277][-2.5311618 -2.6573861 -2.7843766 -2.8295007 -2.8036988 -2.6758368 -2.313072 -1.8975718 -1.945919 -2.4601073 -3.0754275 -3.4809935 -3.5103755 -3.3147719 -2.9871273][-2.7189279 -2.9765565 -3.2213411 -3.3216777 -3.2824886 -3.0328441 -2.5100021 -2.0615146 -2.3564837 -3.2241559 -4.0597434 -4.4298487 -4.1995778 -3.6980929 -3.1263504][-2.8231697 -3.1541541 -3.4859974 -3.6465561 -3.5821774 -3.1870089 -2.5276713 -2.1660042 -2.7600141 -3.9336004 -4.8690743 -5.0629711 -4.5992222 -3.9021387 -3.1839333][-2.6847076 -2.9878678 -3.3656347 -3.603698 -3.4960089 -2.8926244 -2.120033 -1.9598632 -2.8453488 -4.229352 -5.1671329 -5.1604 -4.579463 -3.8453665 -3.1232376][-2.4132133 -2.6348968 -3.036608 -3.3590891 -3.1404958 -2.1984575 -1.2888787 -1.3846242 -2.5290668 -4.0261588 -4.9002872 -4.7581644 -4.1877084 -3.5353804 -2.9293051][-2.1120074 -2.2385504 -2.63078 -2.948946 -2.4981625 -1.1465967 -0.15242767 -0.56588125 -1.9864821 -3.5718224 -4.3660316 -4.1872396 -3.74072 -3.2444806 -2.7764907][-1.9843509 -2.0746522 -2.4923599 -2.7623081 -2.0114496 -0.32140827 0.625545 -0.083429813 -1.7301848 -3.3420229 -4.0502009 -3.9027741 -3.6409411 -3.3178289 -2.8954511][-2.017621 -2.1418138 -2.6271732 -2.8831024 -1.9422481 -0.16814852 0.56256819 -0.34738159 -2.0370078 -3.5255044 -4.1200266 -4.0418282 -3.9166863 -3.6645212 -3.1968286][-2.1102703 -2.31623 -2.8814352 -3.1465516 -2.1519434 -0.54469514 -0.11706924 -1.0991976 -2.65234 -3.8917074 -4.3699956 -4.371161 -4.2866993 -3.9908869 -3.4462035][-2.2076087 -2.4910636 -3.0942147 -3.3877974 -2.5297146 -1.3251996 -1.225183 -2.1845334 -3.4757583 -4.3891034 -4.6864524 -4.66155 -4.48901 -4.0714116 -3.4840615][-2.1461384 -2.4410024 -2.9916673 -3.257021 -2.6035776 -1.8495679 -1.9553547 -2.7412424 -3.6719675 -4.2765412 -4.4419351 -4.3898277 -4.1754513 -3.7537069 -3.2649274][-2.0149648 -2.2246592 -2.6289816 -2.7738824 -2.2428586 -1.7471125 -1.8235064 -2.2769456 -2.8084478 -3.202306 -3.4034939 -3.4977124 -3.4418221 -3.2218697 -2.9548483][-1.9489045 -2.0440633 -2.2795179 -2.2928791 -1.8215759 -1.3783472 -1.2599416 -1.3610222 -1.5744298 -1.8422406 -2.154362 -2.4893355 -2.7138505 -2.780354 -2.7406325][-2.0186472 -2.030174 -2.1290369 -2.0666885 -1.6776366 -1.2493563 -0.99238205 -0.89731908 -0.93788409 -1.128979 -1.4890895 -1.9472322 -2.3272619 -2.5464849 -2.6228895]]...]
INFO - root - 2017-12-07 13:34:57.531549: step 10910, loss = 0.77, batch loss = 0.69 (7.1 examples/sec; 4.518 sec/batch; 90h:37m:27s remains)
INFO - root - 2017-12-07 13:35:43.159230: step 10920, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 4.567 sec/batch; 91h:36m:20s remains)
INFO - root - 2017-12-07 13:36:28.500076: step 10930, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.569 sec/batch; 91h:37m:49s remains)
INFO - root - 2017-12-07 13:37:13.973466: step 10940, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.520 sec/batch; 90h:37m:25s remains)
INFO - root - 2017-12-07 13:37:59.497233: step 10950, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.525 sec/batch; 90h:43m:27s remains)
INFO - root - 2017-12-07 13:38:45.065860: step 10960, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.537 sec/batch; 90h:56m:45s remains)
INFO - root - 2017-12-07 13:39:30.442184: step 10970, loss = 0.83, batch loss = 0.75 (7.1 examples/sec; 4.502 sec/batch; 90h:14m:04s remains)
INFO - root - 2017-12-07 13:40:16.463221: step 10980, loss = 0.88, batch loss = 0.81 (7.0 examples/sec; 4.546 sec/batch; 91h:05m:37s remains)
INFO - root - 2017-12-07 13:41:02.509203: step 10990, loss = 0.81, batch loss = 0.73 (7.0 examples/sec; 4.579 sec/batch; 91h:45m:36s remains)
INFO - root - 2017-12-07 13:41:47.727406: step 11000, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 4.518 sec/batch; 90h:30m:50s remains)
2017-12-07 13:41:50.436528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.942637 -2.7076592 -2.6171379 -2.665771 -2.7997029 -3.0466642 -3.345057 -3.5674982 -3.6769857 -3.7204068 -3.7059164 -3.6380408 -3.5425735 -3.5419898 -3.5251961][-2.6762869 -2.6735351 -2.7093339 -2.6589866 -2.5594037 -2.5783796 -2.7638922 -2.9817798 -3.1551268 -3.2737396 -3.3294082 -3.2635994 -3.1036878 -3.0546212 -3.0451429][-2.8790557 -3.119014 -3.2953963 -3.2513919 -3.0818686 -2.9755414 -3.0157447 -3.1041834 -3.2013564 -3.2664349 -3.3041317 -3.2093396 -2.9497035 -2.7712491 -2.6374416][-3.5511937 -3.8935974 -4.0709338 -4.0368066 -3.9020538 -3.7802491 -3.6970477 -3.5996439 -3.5343664 -3.4115376 -3.3042188 -3.1524286 -2.8521171 -2.6136842 -2.370641][-3.7969964 -4.0719151 -4.1640568 -4.1009297 -3.9990613 -3.8834407 -3.7438889 -3.5906706 -3.4767573 -3.2302227 -3.0060539 -2.8412476 -2.6187668 -2.4748974 -2.2261178][-3.5807958 -3.8048587 -3.8810186 -3.8422661 -3.7975235 -3.6636806 -3.4744029 -3.3006673 -3.1418998 -2.784333 -2.4833856 -2.3667147 -2.331291 -2.422873 -2.3226621][-3.3372431 -3.5680041 -3.7422421 -3.7861309 -3.8085227 -3.6961107 -3.4992032 -3.3009958 -3.0477269 -2.6148415 -2.3090947 -2.2724311 -2.4153061 -2.7333646 -2.8202615][-3.2467818 -3.4619029 -3.7017841 -3.746912 -3.7476716 -3.6302607 -3.4793496 -3.3520541 -3.1325762 -2.7857063 -2.5886145 -2.6169641 -2.7989211 -3.1752448 -3.3433323][-3.3135531 -3.4204726 -3.5756629 -3.5433023 -3.4843507 -3.3246331 -3.1747355 -3.087203 -2.960443 -2.8216996 -2.8110166 -2.8777742 -3.0133038 -3.3339477 -3.507894][-3.275965 -3.3100889 -3.4393597 -3.4266725 -3.3521423 -3.1214242 -2.895169 -2.7621255 -2.6851158 -2.7333364 -2.9025745 -3.0274243 -3.0996594 -3.3235064 -3.4609663][-3.0202022 -3.0363526 -3.2387538 -3.3504224 -3.2974753 -3.0227535 -2.7424178 -2.560092 -2.477879 -2.5752847 -2.7821617 -2.9309635 -2.9537108 -3.053757 -3.111151][-2.679038 -2.731895 -3.0220828 -3.2390215 -3.210741 -2.9836383 -2.7664056 -2.5870941 -2.4745173 -2.5272977 -2.6839948 -2.8496687 -2.8936958 -2.9214416 -2.9424787][-2.57522 -2.6548147 -2.9267912 -3.0906305 -3.0305066 -2.9129145 -2.8714435 -2.8019924 -2.7371726 -2.7962623 -2.9716077 -3.2040544 -3.3360567 -3.3756003 -3.4101267][-2.7756186 -2.8210783 -2.9820132 -3.0058897 -2.8871069 -2.8589053 -2.995502 -3.0789444 -3.0742002 -3.1361735 -3.31534 -3.5776062 -3.7543476 -3.7763855 -3.7588787][-3.124604 -3.072156 -3.1003327 -3.0463667 -2.9464364 -2.9686129 -3.1586223 -3.3276925 -3.3643146 -3.3950381 -3.5045786 -3.6822104 -3.7865937 -3.7040224 -3.5622292]]...]
INFO - root - 2017-12-07 13:42:36.171799: step 11010, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.599 sec/batch; 92h:07m:05s remains)
INFO - root - 2017-12-07 13:43:21.936895: step 11020, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.642 sec/batch; 92h:58m:58s remains)
INFO - root - 2017-12-07 13:44:07.702851: step 11030, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.565 sec/batch; 91h:25m:43s remains)
INFO - root - 2017-12-07 13:44:53.454319: step 11040, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.522 sec/batch; 90h:32m:56s remains)
INFO - root - 2017-12-07 13:45:39.144298: step 11050, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.570 sec/batch; 91h:29m:09s remains)
INFO - root - 2017-12-07 13:46:25.036025: step 11060, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.515 sec/batch; 90h:22m:29s remains)
INFO - root - 2017-12-07 13:47:10.492246: step 11070, loss = 0.76, batch loss = 0.68 (7.1 examples/sec; 4.520 sec/batch; 90h:27m:33s remains)
INFO - root - 2017-12-07 13:47:56.467725: step 11080, loss = 0.93, batch loss = 0.86 (6.9 examples/sec; 4.645 sec/batch; 92h:56m:59s remains)
INFO - root - 2017-12-07 13:48:41.974863: step 11090, loss = 0.85, batch loss = 0.77 (7.2 examples/sec; 4.452 sec/batch; 89h:05m:13s remains)
INFO - root - 2017-12-07 13:49:27.850455: step 11100, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.561 sec/batch; 91h:14m:42s remains)
2017-12-07 13:49:30.538776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4356036 -3.7919376 -4.0300493 -4.0805931 -4.1257277 -4.1932492 -4.1766615 -4.1075373 -4.1049223 -4.1921296 -4.1859303 -4.1201882 -4.1189313 -4.1175861 -4.12222][-2.9759934 -3.3510985 -3.5881221 -3.648937 -3.7681456 -3.9308698 -3.9745159 -3.9574974 -4.0012031 -4.0674677 -3.9873385 -3.8812904 -3.8891385 -3.8553338 -3.7882154][-2.8264418 -3.1959884 -3.3278465 -3.2684979 -3.3448505 -3.5198131 -3.56361 -3.5073755 -3.5308738 -3.5414917 -3.3769193 -3.240252 -3.2745445 -3.2225316 -3.0890427][-3.1003633 -3.5449238 -3.6024172 -3.3823757 -3.3171854 -3.4052219 -3.3396673 -3.1392798 -3.0704436 -2.9862337 -2.7266483 -2.5854287 -2.6617913 -2.601038 -2.4347668][-2.8384147 -3.3833601 -3.4621696 -3.2152138 -3.1267114 -3.2074783 -3.0830891 -2.8171721 -2.757159 -2.6519432 -2.3393719 -2.2174108 -2.3006802 -2.1507308 -1.9275975][-2.2139096 -2.6462712 -2.5951886 -2.3239498 -2.3476968 -2.5310752 -2.4725666 -2.3485198 -2.5301535 -2.5962749 -2.3735218 -2.3297644 -2.3787224 -2.0630863 -1.7674489][-2.4772606 -2.5747321 -2.0880411 -1.563854 -1.5015745 -1.553004 -1.4234805 -1.4537263 -1.9349899 -2.2844274 -2.3022175 -2.4661674 -2.5837522 -2.2264159 -1.988446][-3.359087 -3.2906613 -2.4815822 -1.7490008 -1.4343147 -1.0872116 -0.69563007 -0.71876097 -1.2327311 -1.6307878 -1.8169868 -2.203578 -2.4428639 -2.1853909 -2.1400898][-3.6922472 -3.7420609 -2.98311 -2.3424501 -2.0139256 -1.5210426 -1.054424 -1.0361307 -1.3210523 -1.4260533 -1.4927037 -1.8712838 -2.0879223 -1.9140735 -2.0565226][-3.4352167 -3.6461639 -3.1144686 -2.7323856 -2.6574588 -2.338392 -2.0185106 -2.0798161 -2.2211587 -2.0006125 -1.7770565 -1.9047503 -1.9084873 -1.7123461 -1.9447114][-2.8753991 -3.0909312 -2.6783547 -2.4770803 -2.6732426 -2.622164 -2.5296414 -2.754221 -2.9321957 -2.6201661 -2.2397323 -2.1458044 -1.9228971 -1.6006045 -1.7628708][-2.4884446 -2.4763482 -1.9563489 -1.7629011 -2.0649757 -2.1962025 -2.3131762 -2.6939206 -2.9653549 -2.7564254 -2.4544473 -2.341666 -2.0430982 -1.6114781 -1.6161156][-2.8443708 -2.4908395 -1.7243743 -1.3748393 -1.5567312 -1.6884274 -1.9394622 -2.4327226 -2.7405076 -2.6483989 -2.4965901 -2.4782248 -2.2428904 -1.8415754 -1.7773855][-3.5152085 -3.0068347 -2.1519992 -1.6945868 -1.6746483 -1.6699338 -1.908251 -2.3831742 -2.6366763 -2.5578189 -2.4855137 -2.5711462 -2.470001 -2.2327752 -2.2383652][-3.6215374 -3.2189865 -2.5609207 -2.2144442 -2.1160188 -1.9877274 -2.0775044 -2.3642733 -2.4588325 -2.2914846 -2.1848156 -2.3224812 -2.3509085 -2.2698538 -2.347455]]...]
INFO - root - 2017-12-07 13:50:16.095844: step 11110, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.528 sec/batch; 90h:34m:33s remains)
INFO - root - 2017-12-07 13:51:01.701528: step 11120, loss = 0.89, batch loss = 0.82 (7.0 examples/sec; 4.572 sec/batch; 91h:26m:33s remains)
INFO - root - 2017-12-07 13:51:47.176444: step 11130, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.608 sec/batch; 92h:08m:45s remains)
INFO - root - 2017-12-07 13:52:32.550653: step 11140, loss = 0.82, batch loss = 0.75 (7.3 examples/sec; 4.414 sec/batch; 88h:15m:06s remains)
INFO - root - 2017-12-07 13:53:18.208559: step 11150, loss = 0.81, batch loss = 0.73 (7.0 examples/sec; 4.560 sec/batch; 91h:10m:34s remains)
INFO - root - 2017-12-07 13:54:03.765801: step 11160, loss = 0.67, batch loss = 0.60 (6.9 examples/sec; 4.607 sec/batch; 92h:06m:07s remains)
INFO - root - 2017-12-07 13:54:49.501710: step 11170, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.641 sec/batch; 92h:46m:18s remains)
INFO - root - 2017-12-07 13:55:35.220425: step 11180, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.588 sec/batch; 91h:41m:39s remains)
INFO - root - 2017-12-07 13:56:21.205970: step 11190, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.589 sec/batch; 91h:41m:56s remains)
INFO - root - 2017-12-07 13:57:06.928910: step 11200, loss = 0.79, batch loss = 0.71 (7.0 examples/sec; 4.585 sec/batch; 91h:36m:08s remains)
2017-12-07 13:57:09.750380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7401202 -2.2949076 -2.2196722 -2.5880404 -3.0281997 -2.9960017 -2.7644887 -2.8729713 -3.0378923 -2.7505283 -2.4036636 -2.1933544 -1.6187596 -1.2420368 -1.6499555][-2.8567591 -2.4580207 -2.473897 -3.0053706 -3.42439 -3.2574468 -2.7734513 -2.6033249 -2.7346458 -2.585963 -2.2431281 -1.8880789 -1.4340789 -1.3482916 -1.8635354][-2.9735184 -2.5757198 -2.6543679 -3.2735064 -3.6775517 -3.4544852 -2.7443638 -2.3658776 -2.5789185 -2.7121844 -2.4743495 -2.0326614 -1.7380955 -1.9426041 -2.5168381][-3.0800259 -2.6029527 -2.6916256 -3.2745945 -3.6101623 -3.279609 -2.268055 -1.7767081 -2.2541831 -2.8011153 -2.7656035 -2.2830632 -2.1496453 -2.6896474 -3.4581804][-3.1520972 -2.5308065 -2.5474317 -2.9882398 -3.1941283 -2.6298947 -1.1683853 -0.56987882 -1.4214466 -2.4572668 -2.6799688 -2.219008 -2.22616 -3.0809069 -4.0696278][-3.1568546 -2.4557705 -2.4033744 -2.6796455 -2.7105632 -1.7967484 0.17544031 0.85311031 -0.42119026 -1.9239755 -2.4127798 -1.9481905 -1.9362903 -2.9199059 -4.0110116][-3.1895058 -2.4708838 -2.388773 -2.5784011 -2.4915643 -1.2812357 1.0195518 1.641736 -0.0049319267 -1.8594661 -2.5013351 -1.8895388 -1.6115713 -2.4535952 -3.5109696][-3.1413178 -2.4958787 -2.459065 -2.6540408 -2.5123358 -1.1797156 1.028029 1.3361082 -0.51579452 -2.4160516 -2.9633512 -2.0951009 -1.4356539 -1.9785695 -2.9019527][-3.0095031 -2.4396591 -2.4501762 -2.6245196 -2.3780954 -1.0513849 0.7438302 0.58852053 -1.3495266 -3.0995107 -3.4478297 -2.4520206 -1.568583 -1.8926165 -2.7424583][-2.7807355 -2.2858005 -2.2968736 -2.3991361 -2.0310168 -0.81497955 0.45068932 -0.08883667 -1.9176483 -3.3054965 -3.403887 -2.47361 -1.6510508 -1.9386511 -2.8465223][-2.5497139 -2.1083069 -2.1004694 -2.109988 -1.67382 -0.68588114 0.061694145 -0.67145157 -2.1805375 -3.0697992 -2.9715371 -2.3057835 -1.714658 -1.9561594 -2.7928867][-2.455302 -2.0538807 -1.9967318 -1.9039953 -1.4809151 -0.81438994 -0.51737022 -1.3038492 -2.426964 -2.8594098 -2.6756649 -2.3715641 -2.083508 -2.2527156 -2.8585167][-2.4005094 -2.1192532 -2.0669527 -1.9321198 -1.6024725 -1.2639475 -1.3110802 -2.0724144 -2.8858688 -3.0280285 -2.8319149 -2.7732031 -2.6109347 -2.5860548 -2.8856282][-2.2496774 -2.1590056 -2.2212079 -2.1850519 -2.0324607 -1.9636316 -2.1782324 -2.8105621 -3.3766713 -3.3917394 -3.2468662 -3.2921576 -3.1343572 -2.9616807 -3.0481896][-1.9845262 -2.0756922 -2.2995656 -2.4391446 -2.4894714 -2.5758476 -2.7469783 -3.0900517 -3.3987098 -3.4065447 -3.4550974 -3.703567 -3.730407 -3.6646404 -3.7301269]]...]
INFO - root - 2017-12-07 13:57:55.477208: step 11210, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.530 sec/batch; 90h:29m:16s remains)
INFO - root - 2017-12-07 13:58:41.068042: step 11220, loss = 0.86, batch loss = 0.79 (7.0 examples/sec; 4.562 sec/batch; 91h:06m:58s remains)
INFO - root - 2017-12-07 13:59:26.861822: step 11230, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.648 sec/batch; 92h:49m:27s remains)
INFO - root - 2017-12-07 14:00:12.467398: step 11240, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.554 sec/batch; 90h:55m:33s remains)
INFO - root - 2017-12-07 14:00:58.122613: step 11250, loss = 0.65, batch loss = 0.57 (7.0 examples/sec; 4.591 sec/batch; 91h:39m:28s remains)
INFO - root - 2017-12-07 14:01:43.745766: step 11260, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.554 sec/batch; 90h:54m:02s remains)
INFO - root - 2017-12-07 14:02:29.572477: step 11270, loss = 0.66, batch loss = 0.59 (7.0 examples/sec; 4.578 sec/batch; 91h:22m:42s remains)
INFO - root - 2017-12-07 14:03:15.444435: step 11280, loss = 0.76, batch loss = 0.68 (6.9 examples/sec; 4.607 sec/batch; 91h:55m:57s remains)
INFO - root - 2017-12-07 14:04:00.785190: step 11290, loss = 0.72, batch loss = 0.64 (6.9 examples/sec; 4.625 sec/batch; 92h:17m:42s remains)
INFO - root - 2017-12-07 14:04:46.548311: step 11300, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 4.587 sec/batch; 91h:30m:59s remains)
2017-12-07 14:04:49.206720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.108995 -4.1048765 -4.0783086 -4.040947 -4.0061579 -3.9564698 -3.8903182 -3.8203921 -3.7815251 -3.8120017 -3.8849196 -3.9367466 -3.9540317 -3.9498224 -3.9217458][-4.093081 -4.0802631 -4.0551267 -4.0274096 -3.9949679 -3.9218121 -3.7959111 -3.640717 -3.5564249 -3.6511278 -3.860553 -4.0622606 -4.1911726 -4.2383347 -4.1678662][-3.8013682 -3.8102913 -3.8321784 -3.8642633 -3.8890977 -3.8657343 -3.754113 -3.5684581 -3.4513652 -3.5796618 -3.8406434 -4.1029215 -4.2867961 -4.3651257 -4.2619514][-3.4075518 -3.4106236 -3.4558508 -3.5392742 -3.6340327 -3.70905 -3.6800275 -3.5067687 -3.3397322 -3.3892775 -3.5556543 -3.7892947 -4.0023503 -4.1331077 -4.06648][-3.1082387 -3.096684 -3.157177 -3.288393 -3.4358823 -3.5798995 -3.6373439 -3.5130129 -3.3177023 -3.2274702 -3.2245688 -3.3873267 -3.6138716 -3.8237588 -3.8609707][-2.9492917 -2.8948777 -2.9149086 -3.011939 -3.0966153 -3.2036414 -3.3093376 -3.2861247 -3.1808031 -3.1048198 -3.0484416 -3.1504734 -3.3302083 -3.5249877 -3.601572][-2.4271305 -2.3228052 -2.2966712 -2.3184128 -2.293721 -2.3230877 -2.4238842 -2.4732177 -2.5171485 -2.5840437 -2.643651 -2.8208907 -3.0226121 -3.1936555 -3.2240834][-1.5714545 -1.4413767 -1.4034956 -1.3676081 -1.2823987 -1.3280361 -1.5175331 -1.7413678 -2.03151 -2.291048 -2.4416909 -2.6173604 -2.7951095 -2.8819537 -2.8170519][-1.2785096 -1.1323004 -1.086755 -1.0126357 -0.94822717 -1.0696259 -1.3367369 -1.6754949 -2.090699 -2.3656626 -2.4575458 -2.5376039 -2.6165786 -2.6006823 -2.4604912][-1.6585221 -1.5803084 -1.5694366 -1.5332048 -1.5731082 -1.7039297 -1.8753972 -2.1202073 -2.3975434 -2.5154898 -2.5266995 -2.5742056 -2.6118908 -2.546958 -2.3931482][-2.3843858 -2.3682945 -2.3581467 -2.293083 -2.3084819 -2.3322489 -2.3277678 -2.4052382 -2.5203097 -2.5501866 -2.5767574 -2.6939068 -2.7872605 -2.7697897 -2.6732583][-2.9229102 -2.9357533 -2.9025993 -2.7776039 -2.7262 -2.6474166 -2.5090823 -2.4545565 -2.4733634 -2.5021791 -2.59649 -2.771409 -2.9117124 -2.9467487 -2.9052734][-3.0887425 -3.1077209 -3.0651784 -2.9384143 -2.881299 -2.8100019 -2.6901608 -2.6322901 -2.6420422 -2.6866698 -2.7931871 -2.9387422 -3.0311174 -3.0333638 -2.9922791][-3.0977721 -3.1247294 -3.1037068 -3.0432305 -3.0323086 -3.0107698 -2.9514332 -2.9209971 -2.9307365 -2.9565063 -3.0152004 -3.0895753 -3.1213872 -3.0823073 -3.0306625][-3.0513749 -3.1032205 -3.1163659 -3.116642 -3.1314266 -3.1277456 -3.1030998 -3.0931075 -3.0938654 -3.081841 -3.0825057 -3.1059771 -3.1118088 -3.070091 -3.0224485]]...]
INFO - root - 2017-12-07 14:05:34.406835: step 11310, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.525 sec/batch; 90h:16m:24s remains)
INFO - root - 2017-12-07 14:06:19.771279: step 11320, loss = 0.88, batch loss = 0.80 (7.1 examples/sec; 4.526 sec/batch; 90h:16m:26s remains)
INFO - root - 2017-12-07 14:07:05.431669: step 11330, loss = 0.80, batch loss = 0.72 (6.9 examples/sec; 4.615 sec/batch; 92h:01m:58s remains)
INFO - root - 2017-12-07 14:07:51.103932: step 11340, loss = 0.81, batch loss = 0.74 (6.9 examples/sec; 4.645 sec/batch; 92h:37m:32s remains)
INFO - root - 2017-12-07 14:08:36.657369: step 11350, loss = 0.82, batch loss = 0.75 (7.2 examples/sec; 4.466 sec/batch; 89h:02m:06s remains)
INFO - root - 2017-12-07 14:09:22.306824: step 11360, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.550 sec/batch; 90h:42m:39s remains)
INFO - root - 2017-12-07 14:10:08.061770: step 11370, loss = 0.82, batch loss = 0.74 (7.2 examples/sec; 4.472 sec/batch; 89h:07m:45s remains)
INFO - root - 2017-12-07 14:10:53.724713: step 11380, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.538 sec/batch; 90h:26m:30s remains)
INFO - root - 2017-12-07 14:11:39.506857: step 11390, loss = 0.82, batch loss = 0.74 (7.1 examples/sec; 4.492 sec/batch; 89h:30m:10s remains)
INFO - root - 2017-12-07 14:12:25.246008: step 11400, loss = 0.81, batch loss = 0.73 (7.0 examples/sec; 4.569 sec/batch; 91h:01m:45s remains)
2017-12-07 14:12:27.899902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2426312 -1.2889729 -1.2773521 -1.238209 -1.2168036 -1.23893 -1.2774954 -1.2858064 -1.273612 -1.2424569 -1.1823807 -1.1206608 -1.1026003 -1.2074695 -1.3772855][-1.1886184 -1.3088536 -1.3970342 -1.4453683 -1.5075672 -1.6081684 -1.683213 -1.6916356 -1.6461051 -1.561446 -1.4276667 -1.2493014 -1.117569 -1.1816802 -1.3775983][-1.2988265 -1.4736974 -1.6279411 -1.7244997 -1.8245087 -1.957691 -2.0375745 -2.0629029 -2.0469129 -1.9921646 -1.8625226 -1.5845599 -1.3174884 -1.2816451 -1.4342744][-1.444488 -1.5808957 -1.6748509 -1.7331223 -1.8282413 -1.9666324 -2.0270398 -2.0800972 -2.1554031 -2.2337308 -2.2244935 -1.951133 -1.6266007 -1.5116515 -1.6192496][-1.278909 -1.2383401 -1.1256344 -1.0533502 -1.1105266 -1.2582848 -1.3004606 -1.3840206 -1.5979078 -1.8910503 -2.087213 -1.900075 -1.5675228 -1.382221 -1.4425023][-0.74644947 -0.46861982 -0.11894989 0.075210571 0.029064178 -0.15442324 -0.18833303 -0.28011036 -0.66855645 -1.2912819 -1.8340862 -1.8841908 -1.6194031 -1.3286302 -1.2217963][-0.52314854 -0.13349724 0.34169054 0.636868 0.66954136 0.59935713 0.79077911 0.96956921 0.63843107 -0.19710255 -1.0841243 -1.4901781 -1.4872711 -1.2954147 -1.1261554][-0.9437747 -0.62796569 -0.19177675 0.1529994 0.35844517 0.56516695 1.0879922 1.6624479 1.6162071 0.83955765 -0.16217566 -0.80579662 -1.0901701 -1.1437607 -1.0989549][-1.6831663 -1.6027064 -1.3647311 -1.0797195 -0.78773808 -0.41213274 0.23109961 0.93763447 1.1822243 0.71429586 -0.1132822 -0.73541069 -1.1003375 -1.2756374 -1.3353546][-2.027787 -2.2152438 -2.2815549 -2.2490423 -2.1218343 -1.8653824 -1.3712354 -0.75621319 -0.37543297 -0.49284458 -0.89404297 -1.1380749 -1.2190263 -1.2559669 -1.3176639][-1.4202366 -1.7502155 -2.1092458 -2.4373159 -2.6603136 -2.7405324 -2.6206026 -2.3448813 -2.1032248 -2.0841947 -2.1419876 -1.9845138 -1.6866729 -1.4189386 -1.2876627][-0.34378576 -0.64642978 -1.1539972 -1.7541316 -2.3112431 -2.7356145 -2.9684477 -2.997803 -2.9110665 -2.8178074 -2.6670046 -2.2940443 -1.8281362 -1.4260864 -1.1983447][0.12407064 0.0048370361 -0.4032588 -0.9833324 -1.5974874 -2.1225853 -2.5014071 -2.6910276 -2.7039418 -2.6235681 -2.4441395 -2.0692248 -1.6330156 -1.2722199 -1.0815237][-0.303092 -0.23222923 -0.406703 -0.73980689 -1.1373923 -1.4845314 -1.7739282 -1.9761212 -2.0545485 -2.07637 -2.0370452 -1.8295577 -1.5314586 -1.2569118 -1.1071346][-1.2349432 -1.1116045 -1.1097693 -1.192692 -1.3109498 -1.3903477 -1.4676673 -1.5405784 -1.5729015 -1.6206923 -1.6761053 -1.6252277 -1.4793279 -1.3151271 -1.212513]]...]
INFO - root - 2017-12-07 14:13:13.675626: step 11410, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.575 sec/batch; 91h:08m:08s remains)
INFO - root - 2017-12-07 14:13:59.446103: step 11420, loss = 0.83, batch loss = 0.75 (6.9 examples/sec; 4.658 sec/batch; 92h:47m:06s remains)
INFO - root - 2017-12-07 14:14:44.945238: step 11430, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.571 sec/batch; 91h:02m:01s remains)
INFO - root - 2017-12-07 14:15:30.437729: step 11440, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.532 sec/batch; 90h:14m:23s remains)
INFO - root - 2017-12-07 14:16:16.008221: step 11450, loss = 0.76, batch loss = 0.69 (7.3 examples/sec; 4.405 sec/batch; 87h:42m:29s remains)
INFO - root - 2017-12-07 14:17:01.891071: step 11460, loss = 0.70, batch loss = 0.62 (6.9 examples/sec; 4.617 sec/batch; 91h:54m:12s remains)
INFO - root - 2017-12-07 14:17:47.529205: step 11470, loss = 0.83, batch loss = 0.76 (7.1 examples/sec; 4.516 sec/batch; 89h:52m:58s remains)
INFO - root - 2017-12-07 14:18:33.342082: step 11480, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.588 sec/batch; 91h:18m:50s remains)
INFO - root - 2017-12-07 14:19:19.042333: step 11490, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.558 sec/batch; 90h:41m:40s remains)
INFO - root - 2017-12-07 14:20:04.015493: step 11500, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.523 sec/batch; 89h:59m:48s remains)
2017-12-07 14:20:06.696283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.208081 -2.7651494 -2.6259117 -3.0307245 -3.4786339 -3.6993852 -3.8107066 -4.0385246 -4.2783842 -4.2859049 -4.1806264 -4.0699625 -3.8242342 -3.5220509 -3.4342051][-2.808857 -2.0828552 -2.0045295 -2.6728003 -3.1283395 -3.1732788 -3.2324891 -3.5663829 -3.8840983 -3.9750788 -4.038188 -4.1442051 -4.0354176 -3.665462 -3.4809499][-2.4503741 -1.5494082 -1.7004149 -2.6337614 -3.0600667 -3.009068 -3.0611939 -3.3310218 -3.4071853 -3.3675041 -3.5489202 -3.8723898 -4.0131574 -3.7534349 -3.5653586][-2.1297336 -1.2551105 -1.6986506 -2.8362956 -3.2718384 -3.2691417 -3.3814707 -3.4744008 -3.1113186 -2.824831 -3.0467334 -3.4295037 -3.734736 -3.6882133 -3.6205842][-1.9801521 -1.2453659 -1.8492806 -2.9837675 -3.3795249 -3.4053903 -3.5003915 -3.4059794 -2.6807647 -2.2942135 -2.6595631 -3.0286365 -3.35817 -3.4829545 -3.5656238][-1.9797938 -1.317745 -1.9160154 -2.9144888 -3.1371989 -2.9843707 -2.9250989 -2.8014419 -2.0456645 -1.7852111 -2.4876084 -2.9745426 -3.2426281 -3.3695273 -3.46602][-1.7284932 -1.0081074 -1.4615679 -2.2153137 -2.2396438 -1.9326468 -1.8414812 -1.8782382 -1.344862 -1.3810532 -2.5560672 -3.2801707 -3.4650123 -3.472774 -3.4840164][-1.5049953 -0.80477834 -1.0943661 -1.5590341 -1.5005677 -1.2419176 -1.266593 -1.4321461 -1.0024343 -1.152386 -2.5840251 -3.4621122 -3.5945802 -3.5334892 -3.4952772][-1.8945007 -1.3641839 -1.6167245 -1.8918986 -1.8117263 -1.6992736 -1.7740486 -1.7909172 -1.2097898 -1.2345872 -2.6021817 -3.4763682 -3.5480146 -3.4478097 -3.4017606][-2.6926966 -2.28451 -2.5060439 -2.652842 -2.5946574 -2.6414881 -2.6637554 -2.4938009 -1.8716559 -1.8316064 -2.9520054 -3.6255212 -3.5788169 -3.4130926 -3.3114738][-3.2401669 -2.7391357 -2.8442476 -2.9277308 -2.9541652 -3.1458812 -3.1414006 -2.9158421 -2.4671392 -2.5002503 -3.3637304 -3.7509937 -3.586549 -3.3653533 -3.2166958][-3.0894296 -2.5479121 -2.5958538 -2.7038143 -2.8052413 -3.0352178 -3.0204639 -2.8583448 -2.6759272 -2.8333488 -3.4900293 -3.6624167 -3.4517171 -3.234489 -3.1212146][-2.7962136 -2.4094021 -2.5082049 -2.631269 -2.6902394 -2.7903571 -2.7152233 -2.6335883 -2.6558738 -2.8954921 -3.3833172 -3.4744928 -3.3310785 -3.2041798 -3.1754959][-3.0132296 -2.8813286 -3.0168 -3.0839581 -3.0421081 -2.9812613 -2.8397903 -2.8017826 -2.8982706 -3.1121168 -3.4158626 -3.4737387 -3.4178214 -3.3605752 -3.3614368][-3.5091298 -3.5154176 -3.6289217 -3.661401 -3.6007576 -3.5067973 -3.3900206 -3.3800917 -3.4504969 -3.5556498 -3.6760585 -3.6764455 -3.625355 -3.5655777 -3.542253]]...]
INFO - root - 2017-12-07 14:20:52.419576: step 11510, loss = 0.83, batch loss = 0.76 (7.1 examples/sec; 4.495 sec/batch; 89h:25m:20s remains)
INFO - root - 2017-12-07 14:21:37.732174: step 11520, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.565 sec/batch; 90h:48m:29s remains)
INFO - root - 2017-12-07 14:22:23.266220: step 11530, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.495 sec/batch; 89h:23m:27s remains)
INFO - root - 2017-12-07 14:23:08.421492: step 11540, loss = 0.87, batch loss = 0.80 (7.1 examples/sec; 4.526 sec/batch; 89h:59m:59s remains)
INFO - root - 2017-12-07 14:23:54.108268: step 11550, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.556 sec/batch; 90h:34m:32s remains)
INFO - root - 2017-12-07 14:24:39.455630: step 11560, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.571 sec/batch; 90h:52m:28s remains)
INFO - root - 2017-12-07 14:25:24.962197: step 11570, loss = 0.88, batch loss = 0.81 (7.1 examples/sec; 4.476 sec/batch; 88h:57m:58s remains)
INFO - root - 2017-12-07 14:26:10.233589: step 11580, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 4.501 sec/batch; 89h:26m:28s remains)
INFO - root - 2017-12-07 14:26:55.498341: step 11590, loss = 0.80, batch loss = 0.73 (7.2 examples/sec; 4.449 sec/batch; 88h:24m:06s remains)
INFO - root - 2017-12-07 14:27:40.967401: step 11600, loss = 0.73, batch loss = 0.65 (7.2 examples/sec; 4.439 sec/batch; 88h:11m:58s remains)
2017-12-07 14:27:43.580892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3697162 -2.6997502 -3.075784 -3.2290077 -3.8094974 -4.4041677 -4.4225883 -4.2969031 -4.1280127 -3.8726349 -3.6107264 -3.5214298 -3.4811931 -3.4634204 -3.7798195][-3.1584353 -3.1859925 -3.2961569 -3.2392066 -3.5409002 -3.9280772 -3.8644137 -3.6940753 -3.6416924 -3.6071653 -3.4541724 -3.3848548 -3.3461866 -3.2610066 -3.42579][-3.7789192 -3.7196491 -3.7169704 -3.4630172 -3.43577 -3.5269668 -3.26747 -2.9436593 -2.8984811 -2.9826334 -2.9320836 -2.9780402 -3.11346 -3.1721525 -3.2925][-3.9796312 -3.9869564 -3.9811735 -3.5596492 -3.2155132 -2.9846575 -2.5138974 -2.0572121 -1.9967349 -2.1568456 -2.2675269 -2.5199938 -2.9419842 -3.2680082 -3.4655056][-3.7940652 -3.9636197 -4.0536933 -3.5572097 -2.9666495 -2.3697412 -1.608942 -1.0258729 -0.98450232 -1.2615378 -1.6124961 -2.1557627 -2.8972297 -3.5123334 -3.8047121][-3.3355761 -3.66838 -3.9070745 -3.4558172 -2.727778 -1.7586331 -0.63098645 0.11984634 0.083076954 -0.41441298 -1.0616038 -1.9040291 -2.9209328 -3.732666 -4.0344877][-2.9011979 -3.2815726 -3.5817709 -3.1977849 -2.4349144 -1.2237017 0.20127535 1.1413503 1.0472121 0.30528975 -0.65551233 -1.7798166 -2.9880154 -3.8862405 -4.1246204][-2.7618828 -3.0980554 -3.3074193 -2.8843083 -2.1028411 -0.850111 0.66057205 1.7143831 1.6309538 0.77914715 -0.36174726 -1.6683803 -2.9904788 -3.9237852 -4.1319442][-2.8090806 -3.0354347 -3.0694792 -2.589798 -1.8851585 -0.90673804 0.30843258 1.1847939 1.1654391 0.46434164 -0.52185249 -1.7584307 -3.0460191 -3.9602022 -4.1840324][-2.9279127 -2.9748938 -2.7724857 -2.2691121 -1.7729359 -1.2610734 -0.5206964 0.051855087 0.042418957 -0.43524861 -1.1335237 -2.1391082 -3.2498043 -4.0685816 -4.3137136][-3.1028585 -2.9301443 -2.5279598 -2.0747457 -1.8563142 -1.7930963 -1.5086291 -1.2523563 -1.2884007 -1.5717058 -2.0109046 -2.7508042 -3.5978651 -4.2277775 -4.431922][-3.2356157 -2.9292107 -2.4906809 -2.2022502 -2.253468 -2.4750271 -2.4937997 -2.5121238 -2.6183572 -2.778856 -2.9979637 -3.4595928 -4.0032253 -4.3982611 -4.5130548][-3.268137 -2.9388847 -2.6390738 -2.6341882 -2.9033008 -3.1851439 -3.2995791 -3.4703403 -3.6263523 -3.7099755 -3.740046 -3.9217582 -4.1846261 -4.3949003 -4.4583039][-3.2335968 -2.9860954 -2.9303474 -3.2434597 -3.632472 -3.8110836 -3.8690386 -4.0444913 -4.1741195 -4.2070994 -4.1315122 -4.0967574 -4.1398 -4.2027354 -4.2007403][-3.2010655 -3.1304955 -3.3490562 -3.8766785 -4.2618232 -4.305584 -4.3044987 -4.4162416 -4.4604096 -4.4443393 -4.3268266 -4.155726 -4.0375576 -3.9742703 -3.8772047]]...]
INFO - root - 2017-12-07 14:28:29.009769: step 11610, loss = 0.68, batch loss = 0.61 (7.1 examples/sec; 4.525 sec/batch; 89h:53m:55s remains)
INFO - root - 2017-12-07 14:29:14.757928: step 11620, loss = 0.86, batch loss = 0.79 (7.0 examples/sec; 4.552 sec/batch; 90h:24m:48s remains)
INFO - root - 2017-12-07 14:30:00.267430: step 11630, loss = 0.88, batch loss = 0.81 (6.9 examples/sec; 4.621 sec/batch; 91h:45m:48s remains)
INFO - root - 2017-12-07 14:30:45.582584: step 11640, loss = 0.85, batch loss = 0.77 (7.2 examples/sec; 4.447 sec/batch; 88h:18m:44s remains)
INFO - root - 2017-12-07 14:31:30.938226: step 11650, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.603 sec/batch; 91h:23m:44s remains)
INFO - root - 2017-12-07 14:32:16.355899: step 11660, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 4.508 sec/batch; 89h:29m:01s remains)
INFO - root - 2017-12-07 14:33:01.965366: step 11670, loss = 0.81, batch loss = 0.73 (7.0 examples/sec; 4.566 sec/batch; 90h:37m:21s remains)
INFO - root - 2017-12-07 14:33:47.445222: step 11680, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.581 sec/batch; 90h:55m:07s remains)
INFO - root - 2017-12-07 14:34:32.962247: step 11690, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.569 sec/batch; 90h:39m:36s remains)
INFO - root - 2017-12-07 14:35:18.665565: step 11700, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.582 sec/batch; 90h:54m:50s remains)
2017-12-07 14:35:21.317224: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.601862 -1.7246745 -1.8922606 -1.9929757 -2.0012712 -1.8830595 -1.7706859 -1.7434196 -1.7481916 -1.746407 -1.6963508 -1.7006862 -1.7425225 -1.8026969 -1.8913836][-1.7566843 -1.9603274 -2.2036357 -2.3991246 -2.4868767 -2.3729901 -2.2178717 -2.131562 -2.0617521 -2.0260451 -1.9813542 -1.9756832 -2.0042324 -2.0243037 -2.0774879][-2.5771093 -2.754674 -2.948401 -3.1272073 -3.2135811 -3.0978179 -2.9115915 -2.7867923 -2.7100701 -2.7218647 -2.7373838 -2.7052515 -2.6839657 -2.6353908 -2.637521][-3.1703959 -3.2962379 -3.4170358 -3.5434461 -3.5719695 -3.4459026 -3.2178898 -3.0118747 -2.9382627 -3.066021 -3.2222071 -3.2247372 -3.1680274 -3.0181847 -2.9268558][-2.8546839 -2.838057 -2.8349905 -2.8682494 -2.8392758 -2.7629025 -2.5140133 -2.1912537 -2.1226022 -2.4027462 -2.727803 -2.8079042 -2.7640641 -2.5862722 -2.4771771][-2.4059081 -2.2869623 -2.1489897 -2.0356848 -1.8816698 -1.7566478 -1.4148176 -0.93514466 -0.88131905 -1.2995594 -1.7410841 -1.8905675 -1.8649619 -1.7413199 -1.722528][-1.9096889 -1.8758531 -1.7869968 -1.6852181 -1.479507 -1.2097406 -0.62279606 0.084051132 0.12475777 -0.38845158 -0.85906124 -1.014395 -0.95332646 -0.85419059 -0.90406728][-0.89871955 -0.99655604 -1.1323259 -1.340095 -1.4090042 -1.1900632 -0.47779512 0.3393507 0.356771 -0.1421051 -0.51340938 -0.58069539 -0.42073727 -0.28864956 -0.33050728][-0.21752071 -0.33025455 -0.61191273 -1.0915759 -1.4456637 -1.3463199 -0.72245646 -0.10015631 -0.11289644 -0.41597176 -0.57700014 -0.51492691 -0.29084826 -0.14136696 -0.1600976][-0.35567904 -0.43215847 -0.72184873 -1.2589498 -1.6891453 -1.6318934 -1.152858 -0.76278448 -0.78040457 -0.91187239 -0.92706895 -0.79303193 -0.5424161 -0.36007071 -0.36013412][-1.0183954 -1.0468752 -1.2547948 -1.7116828 -2.0878754 -2.0676329 -1.7977216 -1.6359551 -1.6630194 -1.6885505 -1.6131439 -1.4428489 -1.2037263 -1.0081508 -0.96799517][-2.0537407 -2.0327854 -2.1192932 -2.4086671 -2.670223 -2.6699135 -2.5767314 -2.5592299 -2.5946209 -2.5914645 -2.4946694 -2.3350775 -2.1276081 -1.9477932 -1.8864708][-3.347245 -3.3372622 -3.3413587 -3.4700868 -3.5917666 -3.5662813 -3.5229111 -3.5205228 -3.5328975 -3.5171912 -3.4373183 -3.3222294 -3.1776485 -3.0474691 -2.9841251][-4.175427 -4.1846447 -4.1734815 -4.2254658 -4.2815638 -4.2558126 -4.2173362 -4.1884971 -4.1819267 -4.1759095 -4.1198382 -4.0443616 -3.9565873 -3.8895984 -3.8610291][-4.3751802 -4.3848982 -4.3766928 -4.4105635 -4.4462652 -4.4311705 -4.3921041 -4.3477421 -4.335176 -4.3299084 -4.2971048 -4.2637973 -4.2272811 -4.2053227 -4.2058339]]...]
INFO - root - 2017-12-07 14:36:06.778589: step 11710, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 4.535 sec/batch; 89h:57m:35s remains)
INFO - root - 2017-12-07 14:36:52.396007: step 11720, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.551 sec/batch; 90h:16m:23s remains)
INFO - root - 2017-12-07 14:37:37.472397: step 11730, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.576 sec/batch; 90h:45m:21s remains)
INFO - root - 2017-12-07 14:38:23.287666: step 11740, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.556 sec/batch; 90h:20m:46s remains)
INFO - root - 2017-12-07 14:39:08.709356: step 11750, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.528 sec/batch; 89h:46m:05s remains)
INFO - root - 2017-12-07 14:39:54.349602: step 11760, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.609 sec/batch; 91h:21m:58s remains)
INFO - root - 2017-12-07 14:40:39.835672: step 11770, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.478 sec/batch; 88h:45m:22s remains)
INFO - root - 2017-12-07 14:41:25.750696: step 11780, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.562 sec/batch; 90h:24m:55s remains)
INFO - root - 2017-12-07 14:42:11.265060: step 11790, loss = 0.66, batch loss = 0.59 (6.9 examples/sec; 4.608 sec/batch; 91h:17m:59s remains)
INFO - root - 2017-12-07 14:42:56.609131: step 11800, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.543 sec/batch; 90h:00m:48s remains)
2017-12-07 14:42:59.291670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8043423 -2.3875742 -1.9981902 -1.9429281 -2.2415354 -2.6552413 -2.9634588 -3.0493078 -2.9747608 -3.0066705 -3.0780945 -3.0782971 -3.0073261 -2.7643652 -2.6620286][-2.7075107 -2.1973906 -1.8361869 -1.9719822 -2.647871 -3.3883877 -3.697412 -3.5904071 -3.3099642 -3.3228421 -3.5268724 -3.591608 -3.3620477 -2.8747668 -2.7175014][-2.574862 -2.0000653 -1.7367308 -2.074533 -3.0728135 -4.062211 -4.3140812 -4.0573525 -3.6655664 -3.6314578 -3.8505437 -3.7646568 -3.2088065 -2.5813417 -2.5735919][-2.3226793 -1.6347654 -1.4566805 -1.9476101 -3.085912 -4.068491 -4.1614423 -3.8851318 -3.616204 -3.6741285 -3.8833079 -3.5472419 -2.575984 -1.9006984 -2.2576194][-1.923178 -1.0809031 -1.0058787 -1.6747146 -2.8634763 -3.6329298 -3.4513872 -3.1534429 -3.0647392 -3.211997 -3.3297224 -2.7466445 -1.5172284 -0.94987607 -1.7695453][-1.4729702 -0.48719788 -0.52337313 -1.3988044 -2.6060309 -3.102339 -2.6553516 -2.2384002 -2.1663611 -2.2942078 -2.2573767 -1.522244 -0.25514221 0.065471649 -1.1439645][-1.0693386 0.0095591545 -0.0532403 -1.0187936 -2.2292364 -2.5752401 -2.0730941 -1.6360421 -1.4869771 -1.5253487 -1.3704093 -0.63053632 0.45600462 0.47813082 -0.8525641][-1.0045555 0.02319622 -0.068125725 -1.0121715 -2.1401505 -2.448189 -2.1265378 -1.8131223 -1.5436664 -1.4265018 -1.2166624 -0.6066339 0.15069914 -0.061936855 -1.2143993][-1.3693817 -0.54278541 -0.70313835 -1.5666018 -2.5413027 -2.825073 -2.685323 -2.44061 -2.0307665 -1.8018887 -1.7142775 -1.3661571 -0.96310949 -1.2622607 -2.0486715][-1.7597487 -1.1374319 -1.3221071 -2.0630891 -2.8513675 -3.1324258 -3.1506562 -2.9445355 -2.4718652 -2.2801447 -2.457726 -2.4286849 -2.2783909 -2.5112281 -2.8821712][-2.062712 -1.6068177 -1.7640383 -2.3731067 -3.0315416 -3.3339481 -3.4278193 -3.1974452 -2.7403758 -2.7095156 -3.1388698 -3.3134756 -3.2677891 -3.3702164 -3.4121494][-2.354243 -2.0368235 -2.1712747 -2.698329 -3.3109078 -3.6338692 -3.7012329 -3.3932657 -2.9799919 -3.1476908 -3.789268 -4.0887003 -4.0825176 -4.0594449 -3.9007597][-2.5909774 -2.3788991 -2.505605 -2.9856963 -3.5601971 -3.852005 -3.8243847 -3.4600492 -3.152627 -3.514672 -4.2566309 -4.5724711 -4.5608149 -4.477468 -4.299346][-2.7345579 -2.6099167 -2.7515059 -3.18087 -3.6585512 -3.850966 -3.7338514 -3.3890262 -3.2211814 -3.6697943 -4.3552513 -4.6002808 -4.5420976 -4.4045219 -4.2632442][-2.8085227 -2.7545094 -2.8892541 -3.2366419 -3.5865695 -3.6765649 -3.5318372 -3.2882273 -3.2647 -3.6976476 -4.2232704 -4.3688445 -4.2586112 -4.0446272 -3.888988]]...]
INFO - root - 2017-12-07 14:43:44.902909: step 11810, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.579 sec/batch; 90h:42m:51s remains)
INFO - root - 2017-12-07 14:44:30.429068: step 11820, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.527 sec/batch; 89h:40m:26s remains)
INFO - root - 2017-12-07 14:45:16.124975: step 11830, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.598 sec/batch; 91h:03m:54s remains)
INFO - root - 2017-12-07 14:46:01.649424: step 11840, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.593 sec/batch; 90h:57m:12s remains)
INFO - root - 2017-12-07 14:46:47.364179: step 11850, loss = 0.71, batch loss = 0.64 (6.9 examples/sec; 4.612 sec/batch; 91h:18m:37s remains)
INFO - root - 2017-12-07 14:47:32.928188: step 11860, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.602 sec/batch; 91h:05m:28s remains)
INFO - root - 2017-12-07 14:48:18.694766: step 11870, loss = 0.80, batch loss = 0.73 (6.9 examples/sec; 4.652 sec/batch; 92h:04m:50s remains)
INFO - root - 2017-12-07 14:49:04.337343: step 11880, loss = 0.87, batch loss = 0.80 (6.9 examples/sec; 4.611 sec/batch; 91h:14m:38s remains)
INFO - root - 2017-12-07 14:49:50.047106: step 11890, loss = 0.66, batch loss = 0.59 (7.2 examples/sec; 4.422 sec/batch; 87h:29m:27s remains)
INFO - root - 2017-12-07 14:50:35.775166: step 11900, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.550 sec/batch; 90h:00m:40s remains)
2017-12-07 14:50:38.529556: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.0859861 3.1194062 3.801609 4.3128767 4.5223618 4.4490423 4.3797569 4.3128433 4.1959944 4.153594 4.1311874 4.2756681 4.7015715 5.2118282 5.3559532][2.7570257 3.6226768 4.1327438 4.4363956 4.432725 4.1606274 3.9364576 3.7871504 3.6376848 3.5904551 3.6042929 3.8444061 4.3588552 4.9424152 5.1287718][3.6003485 4.2482538 4.550539 4.6121521 4.3040628 3.7026882 3.2498503 2.9819441 2.778532 2.7519693 2.9115391 3.359467 4.072998 4.8150368 5.0590439][4.2234449 4.6619396 4.8179331 4.7344084 4.187171 3.271585 2.5855994 2.2245793 1.9617362 1.9181867 2.1440935 2.6959991 3.5776424 4.5217419 4.9047985][4.480279 4.7248383 4.8347378 4.7655687 4.1320553 2.9650102 1.9936252 1.4805312 1.0851798 0.90271854 1.0386667 1.5604968 2.5760498 3.7261276 4.2938652][4.331686 4.4972277 4.7118883 4.8321266 4.2818356 2.8923659 1.5518489 0.80402136 0.2640028 -0.039767742 -0.03922987 0.34591484 1.3986163 2.6858902 3.4061093][3.6657848 3.9702396 4.4795408 4.9640579 4.6509924 3.1979189 1.5850344 0.61168337 -0.059209347 -0.44203544 -0.60153246 -0.42993641 0.523098 1.80756 2.5322189][2.4756255 3.1022482 4.0331526 4.9284544 4.9036188 3.6347475 2.1151328 1.1521845 0.36558771 -0.2086153 -0.63641286 -0.71162772 0.14822245 1.3969421 2.0826011][1.0863657 2.0195651 3.3179111 4.4614315 4.5362072 3.431747 2.2053075 1.4836488 0.69171429 -0.084974766 -0.68569016 -0.775718 0.1820302 1.5625172 2.3224196][-0.2356329 0.8148675 2.3121405 3.4370918 3.3937759 2.4033346 1.6032887 1.2938204 0.61698151 -0.22470999 -0.77525115 -0.600971 0.58896923 2.1193814 2.9477901][-1.3844652 -0.49876046 0.90417194 1.8235784 1.6275401 0.84937525 0.55454445 0.65389204 0.11222982 -0.65478778 -1.0048215 -0.53275824 0.77624273 2.3182545 3.1512742][-2.1772287 -1.5617883 -0.47045541 0.20016193 -0.095166206 -0.67685366 -0.65625238 -0.45027137 -0.99997449 -1.6543474 -1.7760944 -1.1103582 0.22570848 1.7290511 2.5908995][-2.4801126 -2.1960344 -1.5305407 -1.0389979 -1.3200171 -1.7756128 -1.7209713 -1.6811287 -2.2963216 -2.8136911 -2.7942386 -2.1647048 -0.97923636 0.43741798 1.352572][-2.4427221 -2.4625998 -2.226366 -1.8912799 -2.1092453 -2.4795182 -2.5283275 -2.7379212 -3.3588734 -3.7560327 -3.793293 -3.4517758 -2.589118 -1.3144617 -0.31250095][-2.2315845 -2.3491824 -2.3827498 -2.2876892 -2.62742 -3.0785356 -3.3283477 -3.6983442 -4.2414994 -4.5173292 -4.6530366 -4.6073589 -3.996371 -2.8590877 -1.8853583]]...]
INFO - root - 2017-12-07 14:51:24.365858: step 11910, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.583 sec/batch; 90h:39m:51s remains)
INFO - root - 2017-12-07 14:52:10.076535: step 11920, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 4.524 sec/batch; 89h:28m:40s remains)
INFO - root - 2017-12-07 14:52:55.446522: step 11930, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.490 sec/batch; 88h:48m:07s remains)
INFO - root - 2017-12-07 14:53:41.145951: step 11940, loss = 0.83, batch loss = 0.76 (7.1 examples/sec; 4.529 sec/batch; 89h:33m:40s remains)
INFO - root - 2017-12-07 14:54:26.737188: step 11950, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.565 sec/batch; 90h:14m:47s remains)
INFO - root - 2017-12-07 14:55:12.630898: step 11960, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.563 sec/batch; 90h:12m:16s remains)
INFO - root - 2017-12-07 14:55:57.985571: step 11970, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.502 sec/batch; 88h:59m:08s remains)
INFO - root - 2017-12-07 14:56:43.480833: step 11980, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.518 sec/batch; 89h:17m:17s remains)
INFO - root - 2017-12-07 14:57:29.236210: step 11990, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.566 sec/batch; 90h:13m:19s remains)
INFO - root - 2017-12-07 14:58:14.500144: step 12000, loss = 0.69, batch loss = 0.62 (7.1 examples/sec; 4.502 sec/batch; 88h:56m:17s remains)
2017-12-07 14:58:17.244997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2399271 -2.2962761 -2.3170674 -2.3188 -2.2928891 -2.2718706 -2.2460024 -2.2098944 -2.2075491 -2.1702418 -2.1001668 -2.1305778 -2.1953583 -2.2305954 -2.2161653][-2.6076374 -2.6597512 -2.6986747 -2.73634 -2.7401421 -2.7138104 -2.6332586 -2.5293677 -2.4895468 -2.4936857 -2.5337226 -2.7107491 -2.9562666 -3.0621305 -2.9313173][-3.6371944 -3.7583878 -3.9187799 -4.1175938 -4.274766 -4.314187 -4.2002292 -3.9857235 -3.768029 -3.6109858 -3.5611629 -3.7102776 -4.0086141 -4.1430507 -3.9459996][-4.1094317 -4.2406569 -4.4639421 -4.7828226 -5.1197162 -5.3299842 -5.340097 -5.1453924 -4.7564716 -4.3018694 -3.9524395 -3.8783941 -4.0492496 -4.1297607 -3.9427962][-3.8184853 -3.9175684 -4.0792851 -4.3085213 -4.6005745 -4.8702726 -5.0353131 -5.0362749 -4.7475619 -4.217916 -3.672991 -3.3619537 -3.2703176 -3.1653371 -2.9774354][-2.3935773 -2.3292921 -2.3298867 -2.3550053 -2.401803 -2.5207572 -2.7092781 -2.9788027 -3.1402402 -3.0434237 -2.75716 -2.4394941 -2.091085 -1.6725533 -1.3650744][-0.98440576 -0.88773513 -0.81777549 -0.64618874 -0.26982641 0.12140226 0.3385191 0.094104767 -0.51592588 -1.0965724 -1.4227993 -1.4674065 -1.2361984 -0.85729289 -0.58613634][-0.23233175 -0.29959965 -0.39303637 -0.2619257 0.35030842 1.2788701 2.1924191 2.4340458 1.8617215 0.98168468 0.22683382 -0.1819458 -0.24195528 -0.14926958 -0.14730978][-0.47839618 -0.70776987 -1.0873444 -1.3664598 -1.2308581 -0.58177829 0.41106367 1.0979414 1.14958 0.75754404 0.19977045 -0.24780273 -0.50433517 -0.63305593 -0.76548243][-1.1445405 -1.3747931 -1.7957759 -2.3388011 -2.8144031 -2.9543192 -2.6105654 -2.0448265 -1.5325289 -1.1964581 -1.1292417 -1.2451954 -1.4937601 -1.7389977 -1.9201708][-1.1822193 -1.1958346 -1.3502717 -1.7535365 -2.4063826 -3.122261 -3.5451066 -3.5302069 -3.1273832 -2.536546 -2.0658991 -1.8687191 -1.9963696 -2.2185574 -2.3654852][-1.8279915 -1.7254748 -1.657064 -1.7386978 -2.1162868 -2.8185589 -3.5225792 -3.92446 -3.866015 -3.4423962 -2.9661732 -2.7202213 -2.8126764 -2.9731677 -2.9834058][-2.58245 -2.5492539 -2.4165998 -2.218405 -2.1477687 -2.3952475 -2.8101673 -3.1895218 -3.3301153 -3.1791892 -2.877573 -2.6766071 -2.7207434 -2.8502784 -2.8798871][-2.9921393 -3.0943992 -3.0463271 -2.8186746 -2.5389314 -2.4242702 -2.4612832 -2.595098 -2.7077785 -2.6962705 -2.5539553 -2.3890412 -2.3074052 -2.3024793 -2.3001435][-3.1082916 -3.2515144 -3.2673345 -3.1143143 -2.857182 -2.6520243 -2.5571346 -2.5726414 -2.632287 -2.6565421 -2.6117208 -2.4924631 -2.3408856 -2.2059484 -2.1072412]]...]
INFO - root - 2017-12-07 14:59:03.106763: step 12010, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 4.620 sec/batch; 91h:15m:17s remains)
INFO - root - 2017-12-07 14:59:48.677889: step 12020, loss = 0.81, batch loss = 0.74 (6.9 examples/sec; 4.622 sec/batch; 91h:17m:14s remains)
INFO - root - 2017-12-07 15:00:34.465091: step 12030, loss = 0.70, batch loss = 0.63 (6.8 examples/sec; 4.680 sec/batch; 92h:25m:52s remains)
INFO - root - 2017-12-07 15:01:19.907511: step 12040, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.628 sec/batch; 91h:22m:28s remains)
INFO - root - 2017-12-07 15:02:05.693288: step 12050, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.651 sec/batch; 91h:49m:56s remains)
INFO - root - 2017-12-07 15:02:51.412915: step 12060, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.568 sec/batch; 90h:10m:14s remains)
INFO - root - 2017-12-07 15:03:37.219698: step 12070, loss = 0.77, batch loss = 0.70 (6.8 examples/sec; 4.672 sec/batch; 92h:12m:36s remains)
INFO - root - 2017-12-07 15:04:22.860086: step 12080, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.510 sec/batch; 88h:59m:58s remains)
INFO - root - 2017-12-07 15:05:08.387046: step 12090, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 4.561 sec/batch; 89h:59m:37s remains)
INFO - root - 2017-12-07 15:05:54.169250: step 12100, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.608 sec/batch; 90h:55m:09s remains)
2017-12-07 15:05:56.897145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.162127 -2.2632616 -2.1669371 -2.0449803 -2.2712929 -2.8086486 -3.0306792 -2.900264 -2.7595205 -2.6410351 -2.4754114 -2.4661498 -2.6809843 -2.8511112 -3.0143356][-1.6935039 -1.7566237 -1.5482714 -1.2779336 -1.4002876 -1.8643088 -2.0705466 -2.0508516 -2.0830584 -2.0216255 -1.7963715 -1.8258185 -2.14144 -2.3141861 -2.4886031][-1.516259 -1.5795536 -1.2820725 -0.92754149 -0.93611693 -1.2651408 -1.3848419 -1.396256 -1.5683963 -1.6312187 -1.537816 -1.8096809 -2.3347998 -2.4795578 -2.4912844][-1.3660719 -1.5718558 -1.3889391 -1.2049134 -1.2684343 -1.4758775 -1.4667697 -1.4121199 -1.5523748 -1.5153728 -1.4080241 -1.9301546 -2.8095732 -3.09347 -2.9736748][-0.92153 -1.1170382 -1.1340425 -1.3730276 -1.7388704 -1.9005146 -1.783658 -1.6638634 -1.6983356 -1.4126036 -1.0314384 -1.5111756 -2.5430021 -3.0315845 -2.9391418][-0.71975827 -0.69995 -0.75428915 -1.272301 -1.8289216 -1.9006438 -1.7103412 -1.648824 -1.7788248 -1.440522 -0.85300183 -1.147511 -2.1028049 -2.7484546 -2.7581606][-0.89448404 -0.82731032 -0.8628242 -1.3714678 -1.7870076 -1.556258 -1.1602318 -1.1591198 -1.5591223 -1.470578 -0.93242645 -0.99575233 -1.7027137 -2.4179192 -2.625267][-0.82651281 -0.8858366 -1.034797 -1.6002326 -2.0315483 -1.6844501 -1.053364 -0.90758848 -1.3737855 -1.555115 -1.1781633 -1.0553503 -1.4821196 -2.1715348 -2.5584536][-0.68820858 -0.81510329 -1.0633943 -1.7920363 -2.5892136 -2.6479254 -2.2084489 -1.950624 -2.1897645 -2.3381293 -2.0437448 -1.8139207 -2.0010288 -2.5166545 -2.8907537][-0.83209538 -0.82446742 -0.95687819 -1.5966058 -2.557724 -3.0994396 -3.0912051 -2.9437284 -2.9405277 -2.8260636 -2.4251091 -2.0932434 -2.1049104 -2.4302721 -2.7523508][-1.395319 -1.1462274 -1.0101531 -1.24509 -1.9027395 -2.4922757 -2.7130609 -2.8047829 -2.8792145 -2.7669621 -2.364027 -1.9467192 -1.7268724 -1.7513387 -1.9350493][-2.094589 -1.8033514 -1.5793357 -1.4851105 -1.6870401 -1.9495041 -2.0271032 -2.138974 -2.3329508 -2.3519938 -2.0674729 -1.6561785 -1.2817044 -1.0844133 -1.2496538][-2.6653049 -2.5428813 -2.4353709 -2.2785497 -2.1720626 -2.0449371 -1.8348296 -1.8188171 -2.0396745 -2.1889396 -2.1230881 -1.9318821 -1.6299303 -1.3287971 -1.4216964][-2.8550909 -2.926702 -3.0521786 -3.0884132 -2.9576712 -2.6473653 -2.3027546 -2.3024325 -2.6219316 -2.9339156 -3.1353564 -3.2717552 -3.1195989 -2.7401605 -2.6377029][-2.7408788 -2.880703 -3.1403747 -3.3773546 -3.3261142 -2.9066381 -2.4277139 -2.3487043 -2.6049592 -2.887558 -3.2128439 -3.6543279 -3.7890418 -3.5508072 -3.4407711]]...]
INFO - root - 2017-12-07 15:06:42.230610: step 12110, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.511 sec/batch; 88h:58m:35s remains)
INFO - root - 2017-12-07 15:07:27.749176: step 12120, loss = 0.88, batch loss = 0.81 (7.0 examples/sec; 4.542 sec/batch; 89h:34m:37s remains)
INFO - root - 2017-12-07 15:08:13.638371: step 12130, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 4.600 sec/batch; 90h:43m:00s remains)
INFO - root - 2017-12-07 15:08:58.915467: step 12140, loss = 0.75, batch loss = 0.68 (7.2 examples/sec; 4.414 sec/batch; 87h:02m:14s remains)
INFO - root - 2017-12-07 15:09:44.807084: step 12150, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.548 sec/batch; 89h:39m:41s remains)
INFO - root - 2017-12-07 15:10:30.180651: step 12160, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.539 sec/batch; 89h:28m:24s remains)
INFO - root - 2017-12-07 15:11:16.255132: step 12170, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.547 sec/batch; 89h:36m:44s remains)
INFO - root - 2017-12-07 15:12:01.858793: step 12180, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 4.588 sec/batch; 90h:25m:09s remains)
INFO - root - 2017-12-07 15:12:47.647174: step 12190, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.594 sec/batch; 90h:30m:49s remains)
INFO - root - 2017-12-07 15:13:33.067506: step 12200, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.508 sec/batch; 88h:48m:38s remains)
2017-12-07 15:13:35.758284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8668313 -2.7904222 -2.8390682 -2.8107333 -2.7229605 -2.6636493 -2.6832733 -2.7461433 -2.7749786 -2.7631249 -2.8371682 -3.0615368 -3.184916 -3.0857787 -2.7893915][-2.875195 -2.850595 -2.9883747 -2.9895511 -2.8414171 -2.6872668 -2.6469994 -2.7123394 -2.7834842 -2.8259983 -3.0039887 -3.3783624 -3.6132822 -3.5765562 -3.2223067][-3.0662327 -3.0617223 -3.225122 -3.232187 -3.0164275 -2.7597284 -2.636168 -2.7091272 -2.9018836 -3.0821528 -3.3875465 -3.8727012 -4.1196084 -4.0224514 -3.4958155][-3.1457081 -3.125936 -3.2367418 -3.1794484 -2.8516488 -2.5122838 -2.3099523 -2.3450203 -2.6057482 -2.8945026 -3.2994075 -3.833147 -4.0140605 -3.823086 -3.1896198][-3.2545333 -3.2988868 -3.4142182 -3.3414111 -3.0084205 -2.6870627 -2.4023039 -2.2599807 -2.3847749 -2.6239648 -3.060533 -3.6365952 -3.839848 -3.7395082 -3.2181959][-3.1688836 -3.2916121 -3.4651613 -3.4819992 -3.3080087 -3.0217228 -2.5729322 -2.1688333 -2.0818014 -2.2315304 -2.7113256 -3.3611655 -3.6793551 -3.7550876 -3.402869][-2.3640535 -2.4519317 -2.7013361 -2.9083595 -2.9426694 -2.6617799 -2.0618107 -1.4991858 -1.2606654 -1.3660123 -1.8901603 -2.569905 -2.9338222 -3.1417735 -2.9822168][-1.6495509 -1.655679 -1.8951535 -2.1705179 -2.2475808 -1.9595869 -1.3958254 -0.9449861 -0.69556403 -0.76129031 -1.252425 -1.8428674 -2.179589 -2.4266999 -2.4151711][-1.660655 -1.6328797 -1.7937956 -1.9374864 -1.8578806 -1.5496194 -1.2161853 -1.051949 -0.86129212 -0.86041903 -1.2683046 -1.7111082 -1.9725494 -2.1754704 -2.2071857][-2.16859 -2.1186848 -2.1607502 -2.0943909 -1.8444109 -1.560667 -1.4389095 -1.4376068 -1.2464352 -1.1467638 -1.4558401 -1.7563658 -1.9113212 -2.0500262 -2.0556476][-2.8234515 -2.7949734 -2.7383306 -2.5292032 -2.1906617 -1.9269989 -1.8641648 -1.8741 -1.6856833 -1.5752537 -1.8521483 -2.0413182 -2.0519788 -2.0425818 -1.9289591][-3.3227482 -3.3385551 -3.3289654 -3.1987224 -2.9467077 -2.7370305 -2.6707783 -2.6349363 -2.4407127 -2.3600876 -2.5711617 -2.6195729 -2.4782071 -2.3467405 -2.1323695][-3.6060014 -3.6648707 -3.7873986 -3.8669794 -3.7920537 -3.6474731 -3.5711274 -3.4944792 -3.3076184 -3.2292 -3.3220325 -3.2603836 -3.0666203 -2.8931785 -2.5775027][-3.6040745 -3.6684127 -3.8433237 -4.0588837 -4.1011004 -4.0016451 -3.9091666 -3.8034573 -3.6471291 -3.61575 -3.6834905 -3.6346736 -3.5058949 -3.3077667 -2.8670235][-3.3892581 -3.4430711 -3.5931664 -3.8002911 -3.880347 -3.8505068 -3.8099008 -3.7436926 -3.6697254 -3.6996868 -3.7418547 -3.7099078 -3.6405394 -3.4122822 -2.9103556]]...]
INFO - root - 2017-12-07 15:14:21.618110: step 12210, loss = 0.80, batch loss = 0.72 (6.9 examples/sec; 4.620 sec/batch; 91h:00m:59s remains)
INFO - root - 2017-12-07 15:15:07.150033: step 12220, loss = 0.78, batch loss = 0.70 (6.9 examples/sec; 4.631 sec/batch; 91h:12m:12s remains)
INFO - root - 2017-12-07 15:15:53.021686: step 12230, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.552 sec/batch; 89h:38m:18s remains)
INFO - root - 2017-12-07 15:16:38.595828: step 12240, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.610 sec/batch; 90h:45m:44s remains)
INFO - root - 2017-12-07 15:17:24.442221: step 12250, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.564 sec/batch; 89h:51m:33s remains)
INFO - root - 2017-12-07 15:18:09.934709: step 12260, loss = 0.74, batch loss = 0.67 (7.2 examples/sec; 4.439 sec/batch; 87h:22m:55s remains)
INFO - root - 2017-12-07 15:18:55.906215: step 12270, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.558 sec/batch; 89h:42m:57s remains)
INFO - root - 2017-12-07 15:19:41.535339: step 12280, loss = 0.75, batch loss = 0.67 (7.2 examples/sec; 4.421 sec/batch; 87h:00m:13s remains)
INFO - root - 2017-12-07 15:20:26.983169: step 12290, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.558 sec/batch; 89h:41m:24s remains)
INFO - root - 2017-12-07 15:21:12.580567: step 12300, loss = 0.66, batch loss = 0.58 (7.1 examples/sec; 4.520 sec/batch; 88h:55m:36s remains)
2017-12-07 15:21:15.187149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0122464 -2.1123853 -2.2021461 -2.1506476 -1.9572487 -1.7932785 -1.7462645 -1.7597518 -1.68922 -1.705466 -1.8842659 -1.9027774 -1.6943486 -1.4800732 -1.2503245][-1.8441904 -1.9858954 -2.0700381 -1.95174 -1.7052782 -1.5327239 -1.4930921 -1.5282028 -1.4829392 -1.4966969 -1.7107058 -1.7986085 -1.6273055 -1.4293733 -1.1537576][-1.41856 -1.6354105 -1.7239566 -1.549171 -1.2573538 -1.068737 -1.0062463 -1.0179875 -0.92767167 -0.82700372 -0.90843487 -0.92895532 -0.76536345 -0.66842937 -0.53121948][-1.4226499 -1.656805 -1.6940241 -1.4743326 -1.1641593 -0.94078732 -0.78397655 -0.70920682 -0.56297827 -0.35039806 -0.22880077 -0.068127155 0.20124674 0.32565689 0.4317565][-1.7699373 -1.9924519 -1.9700887 -1.7471595 -1.4842513 -1.2857547 -1.0713837 -0.9546423 -0.86392736 -0.70783019 -0.5310843 -0.27304983 0.050285816 0.26009846 0.445467][-1.4364827 -1.6618476 -1.6183875 -1.3928797 -1.1433342 -0.90596151 -0.62532711 -0.51747584 -0.62143421 -0.69374466 -0.61149 -0.39005232 -0.14979696 -0.021807194 0.081150532][-0.46024871 -0.74986291 -0.77077556 -0.554209 -0.20072222 0.19917345 0.61039734 0.75243378 0.46507263 0.14755011 0.12037992 0.27358675 0.43159151 0.43909216 0.3880682][0.17756081 -0.2179904 -0.39423418 -0.27584839 0.12732983 0.64001322 1.1414485 1.3211579 0.9601326 0.54552412 0.47709608 0.59283924 0.71644974 0.69318438 0.60167265][-0.020322323 -0.44625497 -0.76483154 -0.78668857 -0.42998028 0.08795023 0.5825243 0.71187115 0.34894943 -0.04757452 -0.10695314 0.026249886 0.18591118 0.25537777 0.22710323][-0.39520836 -0.76386213 -1.1319985 -1.226078 -0.90749693 -0.37636089 0.043320179 0.086388588 -0.2349267 -0.56754708 -0.62096786 -0.52347231 -0.39917326 -0.30739975 -0.31751251][-0.32883835 -0.59330297 -0.92981315 -1.0636168 -0.84794211 -0.41358328 -0.13574648 -0.19671869 -0.47690535 -0.73182631 -0.75976777 -0.69305873 -0.63955212 -0.59926987 -0.63685226][-0.37175846 -0.45401835 -0.62771416 -0.69507384 -0.56489325 -0.30565834 -0.21048498 -0.38849211 -0.6582005 -0.85917354 -0.875823 -0.82668543 -0.81324744 -0.79991722 -0.85852313][-0.978045 -0.92571592 -0.88711214 -0.77658367 -0.60392952 -0.41962576 -0.41543484 -0.64720011 -0.9324348 -1.1330965 -1.1743779 -1.1255157 -1.0958655 -1.0775251 -1.159183][-1.45281 -1.3875415 -1.2633264 -1.0956366 -0.94040227 -0.8197937 -0.87173724 -1.1014984 -1.3568356 -1.521414 -1.5351999 -1.4458113 -1.3598821 -1.3121748 -1.412451][-1.5130115 -1.4565618 -1.3404729 -1.2240949 -1.1374919 -1.088587 -1.2133 -1.4842551 -1.7552526 -1.9117467 -1.8973508 -1.775357 -1.6559751 -1.5945673 -1.6890397]]...]
INFO - root - 2017-12-07 15:22:00.985098: step 12310, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.586 sec/batch; 90h:12m:02s remains)
INFO - root - 2017-12-07 15:22:46.712757: step 12320, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.551 sec/batch; 89h:30m:31s remains)
INFO - root - 2017-12-07 15:23:32.092559: step 12330, loss = 0.74, batch loss = 0.66 (7.1 examples/sec; 4.537 sec/batch; 89h:12m:57s remains)
INFO - root - 2017-12-07 15:24:17.896636: step 12340, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.549 sec/batch; 89h:26m:11s remains)
INFO - root - 2017-12-07 15:25:03.432509: step 12350, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 4.604 sec/batch; 90h:31m:13s remains)
INFO - root - 2017-12-07 15:25:49.303677: step 12360, loss = 0.82, batch loss = 0.74 (7.0 examples/sec; 4.593 sec/batch; 90h:16m:43s remains)
INFO - root - 2017-12-07 15:26:34.619785: step 12370, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 4.559 sec/batch; 89h:36m:36s remains)
INFO - root - 2017-12-07 15:27:20.343405: step 12380, loss = 0.72, batch loss = 0.64 (6.9 examples/sec; 4.620 sec/batch; 90h:47m:21s remains)
INFO - root - 2017-12-07 15:28:05.611426: step 12390, loss = 0.82, batch loss = 0.75 (7.3 examples/sec; 4.385 sec/batch; 86h:09m:32s remains)
INFO - root - 2017-12-07 15:28:51.299986: step 12400, loss = 0.77, batch loss = 0.70 (7.2 examples/sec; 4.468 sec/batch; 87h:46m:59s remains)
2017-12-07 15:28:54.062966: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.79740524 -0.80102372 -0.80366445 -0.80065513 -0.79332495 -0.7877233 -0.78270125 -0.77992654 -0.78683877 -0.7966311 -0.80854082 -0.81891513 -0.82300425 -0.83184528 -0.84838986][-0.82383323 -0.81754208 -0.82225084 -0.82048655 -0.81301427 -0.80677819 -0.80650687 -0.82680106 -0.88067031 -0.94842696 -1.0186369 -1.0566175 -1.0386746 -1.0061069 -0.980319][-0.68328214 -0.71075678 -0.76800013 -0.79942894 -0.79916954 -0.77251935 -0.7546289 -0.80327415 -0.94908452 -1.1625843 -1.375618 -1.4910862 -1.4247618 -1.2734783 -1.1341245][-0.49822378 -0.65319443 -0.86638737 -1.0099607 -1.0364909 -0.94450212 -0.8286221 -0.83649445 -1.0510104 -1.399158 -1.7465451 -1.9275725 -1.7989526 -1.5265026 -1.28531][-0.36987162 -0.66845322 -1.0615938 -1.3195322 -1.3333781 -1.0885348 -0.79927135 -0.7769978 -1.1199844 -1.6025577 -1.9931519 -2.1421986 -1.9021249 -1.5161796 -1.2253025][0.048802376 -0.36917925 -0.95013642 -1.3282313 -1.2804363 -0.80135965 -0.28483677 -0.3028264 -0.927526 -1.6896439 -2.209281 -2.3456981 -1.9580641 -1.3398478 -0.87124634][0.693244 0.11497736 -0.7550199 -1.3709719 -1.3295903 -0.6207478 0.179286 0.17960835 -0.74226451 -1.9032564 -2.7358775 -3.0019655 -2.4619844 -1.4246752 -0.52888918][1.3593087 0.73263645 -0.3506999 -1.2160013 -1.2844975 -0.41400766 0.70586967 0.87545586 -0.2287612 -1.7732759 -2.9731393 -3.4437597 -2.8391328 -1.4843271 -0.26625681][1.6893578 1.1871653 0.10913658 -0.88455057 -1.0996318 -0.20734215 1.1078477 1.4618664 0.31424618 -1.3860009 -2.6983671 -3.2038159 -2.5780478 -1.1714625 0.053040028][1.3864555 1.0498514 0.14292145 -0.82397771 -1.1639931 -0.45987725 0.74082708 1.1136971 0.10652447 -1.4203722 -2.5511146 -2.9142098 -2.2682467 -0.94012356 0.18955374][0.76269627 0.4937129 -0.23825788 -1.0982134 -1.507787 -1.0850015 -0.16915464 0.19411135 -0.4978826 -1.6575925 -2.5567491 -2.8168538 -2.2227376 -1.0138056 0.046123028][0.22357798 -0.018736839 -0.56965351 -1.2478614 -1.6574645 -1.5148072 -0.94638658 -0.629513 -1.013694 -1.8289001 -2.5370228 -2.7131958 -2.1136928 -0.95296097 0.045326233][-0.011912346 -0.16539431 -0.49356508 -0.91467547 -1.2191715 -1.2374871 -0.96653771 -0.73309588 -0.90482664 -1.4589839 -2.0162737 -2.130511 -1.5475361 -0.48169851 0.364573][-0.026989937 -0.10374832 -0.27964306 -0.50688052 -0.7027936 -0.79815006 -0.71254611 -0.52407336 -0.52595448 -0.85708809 -1.28001 -1.3831556 -0.90980268 -0.032655239 0.61030769][-0.059186459 -0.12565041 -0.23753691 -0.357687 -0.46248198 -0.55373788 -0.53687859 -0.3655901 -0.27363873 -0.45299673 -0.76789856 -0.89835215 -0.58552861 0.10669756 0.63009262]]...]
INFO - root - 2017-12-07 15:29:39.312035: step 12410, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.586 sec/batch; 90h:05m:14s remains)
INFO - root - 2017-12-07 15:30:24.848834: step 12420, loss = 0.86, batch loss = 0.79 (7.0 examples/sec; 4.585 sec/batch; 90h:02m:47s remains)
INFO - root - 2017-12-07 15:31:09.998905: step 12430, loss = 0.65, batch loss = 0.58 (7.1 examples/sec; 4.508 sec/batch; 88h:31m:26s remains)
INFO - root - 2017-12-07 15:31:55.524828: step 12440, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.566 sec/batch; 89h:39m:05s remains)
INFO - root - 2017-12-07 15:32:40.952999: step 12450, loss = 0.75, batch loss = 0.68 (7.2 examples/sec; 4.438 sec/batch; 87h:07m:55s remains)
INFO - root - 2017-12-07 15:33:26.420802: step 12460, loss = 0.70, batch loss = 0.62 (7.1 examples/sec; 4.522 sec/batch; 88h:45m:37s remains)
INFO - root - 2017-12-07 15:34:12.069377: step 12470, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 4.620 sec/batch; 90h:40m:05s remains)
INFO - root - 2017-12-07 15:34:57.809886: step 12480, loss = 0.82, batch loss = 0.74 (6.9 examples/sec; 4.612 sec/batch; 90h:30m:14s remains)
INFO - root - 2017-12-07 15:35:43.342061: step 12490, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 4.530 sec/batch; 88h:53m:15s remains)
INFO - root - 2017-12-07 15:36:28.720102: step 12500, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.547 sec/batch; 89h:12m:09s remains)
2017-12-07 15:36:31.400038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4846935 -2.4801407 -2.4993672 -2.5381346 -2.5816569 -2.6318827 -2.6943626 -2.7561355 -2.8046017 -2.8298519 -2.8190041 -2.7586422 -2.6718416 -2.5896909 -2.5335774][-2.4711943 -2.4604492 -2.4789453 -2.5187845 -2.5483222 -2.5801721 -2.6500037 -2.7565022 -2.8755512 -2.9687486 -2.9979625 -2.9530149 -2.8434939 -2.7010722 -2.5772982][-2.4800613 -2.4682417 -2.4788146 -2.4977303 -2.4797249 -2.4495912 -2.4808693 -2.5839553 -2.7522044 -2.9284372 -3.0468869 -3.0868473 -3.0257027 -2.867161 -2.6762621][-2.4943168 -2.491107 -2.4806182 -2.4252334 -2.2860715 -2.1143222 -2.0276966 -2.0568919 -2.2286859 -2.5132828 -2.8245435 -3.0762696 -3.1711111 -3.0607173 -2.8311646][-2.4422135 -2.422967 -2.3509541 -2.1694226 -1.8525653 -1.4732165 -1.1792605 -1.0014498 -1.0139575 -1.2815726 -1.7661381 -2.3318498 -2.7579331 -2.8877196 -2.7971725][-2.2611344 -2.2130637 -2.0862808 -1.8399138 -1.4770036 -1.0187502 -0.57587194 -0.1626873 0.082999706 0.0041184425 -0.47410131 -1.2641337 -2.0528622 -2.5285034 -2.6793256][-2.0117037 -1.9610128 -1.8390045 -1.6352892 -1.389734 -1.0531025 -0.66179848 -0.20841694 0.11346912 0.19038773 -0.10354996 -0.80672669 -1.6734071 -2.330828 -2.6373677][-1.8221412 -1.7819536 -1.6861579 -1.5345912 -1.3570211 -1.1312084 -0.85959625 -0.54602861 -0.36500692 -0.31247187 -0.47623396 -0.95968556 -1.6615975 -2.2858176 -2.6196322][-1.8285191 -1.7842488 -1.73258 -1.6517863 -1.5249543 -1.390105 -1.2454522 -1.0953255 -1.0634446 -1.1065388 -1.269418 -1.6002972 -2.0568378 -2.4817054 -2.6945586][-1.912971 -1.8670974 -1.8967896 -1.9422085 -1.9113619 -1.8697038 -1.822818 -1.7443504 -1.711596 -1.7407141 -1.8885925 -2.1185017 -2.3666804 -2.5804114 -2.6719484][-1.9686971 -1.9444852 -2.037024 -2.1586733 -2.1796272 -2.175575 -2.164798 -2.1111126 -2.0649409 -2.0793903 -2.20282 -2.351933 -2.4464216 -2.5124657 -2.5332742][-1.9298687 -1.9530606 -2.0833495 -2.2362156 -2.3074973 -2.3597941 -2.3859749 -2.3493798 -2.2867956 -2.2758172 -2.3607409 -2.4469743 -2.4597898 -2.4410062 -2.4106553][-1.9734468 -1.9871709 -2.0987537 -2.2516487 -2.3673625 -2.4601166 -2.4833853 -2.4200468 -2.3146029 -2.259558 -2.3013456 -2.3634009 -2.3772697 -2.354831 -2.3179533][-2.2928619 -2.2376947 -2.2543702 -2.3216918 -2.3896506 -2.4279094 -2.3994451 -2.320672 -2.2287064 -2.1839454 -2.2165217 -2.2763073 -2.3148775 -2.3202465 -2.2975729][-2.6451998 -2.5835524 -2.5407827 -2.5234785 -2.5090127 -2.4689293 -2.3892767 -2.2888353 -2.1896968 -2.1382356 -2.1524043 -2.1992443 -2.2476072 -2.2744834 -2.2755761]]...]
INFO - root - 2017-12-07 15:37:16.915667: step 12510, loss = 0.64, batch loss = 0.57 (7.0 examples/sec; 4.551 sec/batch; 89h:16m:09s remains)
INFO - root - 2017-12-07 15:38:02.502227: step 12520, loss = 0.83, batch loss = 0.76 (6.9 examples/sec; 4.641 sec/batch; 91h:01m:24s remains)
INFO - root - 2017-12-07 15:38:48.045548: step 12530, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.569 sec/batch; 89h:35m:47s remains)
INFO - root - 2017-12-07 15:39:33.409758: step 12540, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.565 sec/batch; 89h:30m:26s remains)
INFO - root - 2017-12-07 15:40:18.943555: step 12550, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.633 sec/batch; 90h:50m:05s remains)
INFO - root - 2017-12-07 15:41:04.576211: step 12560, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.608 sec/batch; 90h:18m:53s remains)
INFO - root - 2017-12-07 15:41:49.948655: step 12570, loss = 0.83, batch loss = 0.76 (7.2 examples/sec; 4.437 sec/batch; 86h:57m:28s remains)
INFO - root - 2017-12-07 15:42:35.896686: step 12580, loss = 0.79, batch loss = 0.71 (6.9 examples/sec; 4.623 sec/batch; 90h:35m:59s remains)
INFO - root - 2017-12-07 15:43:21.599524: step 12590, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.578 sec/batch; 89h:42m:09s remains)
INFO - root - 2017-12-07 15:44:07.189613: step 12600, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.622 sec/batch; 90h:32m:44s remains)
2017-12-07 15:44:09.952731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8453228 -3.0185387 -3.1766672 -3.3237772 -3.4418526 -3.4966016 -3.5021796 -3.5607417 -3.6763513 -3.7362461 -3.6464515 -3.4421163 -3.2746222 -3.1973171 -3.2663703][-2.7990923 -2.9307628 -2.935461 -2.899909 -2.926178 -2.9619589 -2.938168 -2.9665041 -3.0916936 -3.2513115 -3.2808907 -3.1214528 -2.9124658 -2.794189 -2.9152005][-2.6757255 -2.56987 -2.2378812 -2.0042951 -2.1292155 -2.4512386 -2.6102247 -2.5761666 -2.5002923 -2.5753181 -2.6613741 -2.5973427 -2.417166 -2.2953291 -2.4955721][-2.4287906 -2.0093431 -1.350754 -1.0583177 -1.4529552 -2.1336422 -2.4751165 -2.4113863 -2.1831677 -2.1371853 -2.1564579 -2.0124056 -1.7367151 -1.5889871 -1.8780744][-2.1918805 -1.6381395 -0.84152222 -0.5081389 -0.94887137 -1.6012878 -1.9081435 -1.9883792 -1.972388 -1.9877942 -1.8532946 -1.350275 -0.74506116 -0.52337956 -0.95879221][-2.0691795 -1.634836 -0.930187 -0.5379231 -0.58434439 -0.59866333 -0.5234127 -0.84107208 -1.3961415 -1.7337937 -1.4589219 -0.53164077 0.37788439 0.48607206 -0.32577324][-2.0287273 -1.7659311 -1.2468572 -0.78394461 -0.21558809 0.69822836 1.4282517 0.95661592 -0.30631638 -1.1922805 -1.0408063 -0.046991348 0.8274231 0.62332821 -0.53877354][-2.0943434 -1.8269563 -1.3102381 -0.74567652 0.21524906 1.6573472 2.772655 2.1518941 0.49745083 -0.66497731 -0.76539826 -0.15013933 0.32609081 -0.12092113 -1.2331076][-2.4020417 -1.941931 -1.2225266 -0.56704855 0.29099798 1.4083495 2.2071996 1.6338139 0.47610998 -0.22334862 -0.38032389 -0.33020258 -0.42464828 -0.93697262 -1.6293569][-2.7747965 -2.1852884 -1.3568528 -0.78573108 -0.44324279 -0.12464666 0.15964508 0.028052807 -0.055817604 0.15195084 0.20162296 -0.14386463 -0.66500092 -1.1597354 -1.5156174][-2.9701695 -2.4306045 -1.7580023 -1.4101717 -1.4360137 -1.4991457 -1.3404911 -1.0395544 -0.38773537 0.32516432 0.41058874 -0.10376072 -0.70675254 -1.0517724 -1.2041373][-3.0078583 -2.6303658 -2.2485349 -2.1083996 -2.1845546 -2.1888366 -1.9246383 -1.4319944 -0.63480139 0.016551495 -0.076345444 -0.59413171 -0.99181175 -1.0632222 -1.0264182][-2.9609928 -2.7229376 -2.5813479 -2.614805 -2.6739869 -2.6268 -2.3994677 -2.0161815 -1.4283497 -0.99115324 -1.1234865 -1.4477792 -1.5590835 -1.4195211 -1.2699075][-2.9198239 -2.781774 -2.8027139 -2.9702749 -3.1013522 -3.1408944 -3.0770988 -2.8770208 -2.483788 -2.1500623 -2.1462791 -2.212738 -2.1558571 -2.063267 -1.9882424][-2.8770552 -2.8206105 -2.9261289 -3.156899 -3.3506286 -3.4841297 -3.5212531 -3.439121 -3.1923528 -2.9332995 -2.8451848 -2.8080444 -2.7523968 -2.7620516 -2.7397971]]...]
INFO - root - 2017-12-07 15:44:55.792682: step 12610, loss = 0.80, batch loss = 0.73 (6.9 examples/sec; 4.609 sec/batch; 90h:16m:58s remains)
INFO - root - 2017-12-07 15:45:41.308500: step 12620, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.569 sec/batch; 89h:28m:25s remains)
INFO - root - 2017-12-07 15:46:27.041979: step 12630, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.550 sec/batch; 89h:05m:54s remains)
INFO - root - 2017-12-07 15:47:12.488073: step 12640, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 4.550 sec/batch; 89h:05m:40s remains)
INFO - root - 2017-12-07 15:47:58.313232: step 12650, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.596 sec/batch; 89h:58m:22s remains)
INFO - root - 2017-12-07 15:48:44.095710: step 12660, loss = 0.88, batch loss = 0.81 (7.0 examples/sec; 4.545 sec/batch; 88h:57m:55s remains)
INFO - root - 2017-12-07 15:49:29.722302: step 12670, loss = 0.66, batch loss = 0.59 (7.1 examples/sec; 4.505 sec/batch; 88h:09m:58s remains)
INFO - root - 2017-12-07 15:50:15.681675: step 12680, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.549 sec/batch; 89h:00m:39s remains)
INFO - root - 2017-12-07 15:51:01.522808: step 12690, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.584 sec/batch; 89h:41m:09s remains)
INFO - root - 2017-12-07 15:51:47.334289: step 12700, loss = 0.74, batch loss = 0.66 (7.0 examples/sec; 4.581 sec/batch; 89h:36m:23s remains)
2017-12-07 15:51:50.116115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7460647 -1.9959044 -2.1482866 -1.9623303 -1.6920242 -1.5414124 -1.2328274 -0.91250372 -0.80197763 -0.62714028 -0.37993193 -0.081297874 -0.1202569 -0.50249672 -1.0001223][-2.2540264 -2.5383539 -2.6108675 -2.2675393 -1.7972322 -1.4358478 -0.99582839 -0.61221528 -0.49409032 -0.3941927 -0.32295418 -0.18596029 -0.27420139 -0.63441133 -1.0812891][-2.8169444 -3.1682463 -3.1330166 -2.5525715 -1.813525 -1.2200227 -0.73700261 -0.46240973 -0.44612527 -0.46158767 -0.56340981 -0.56062484 -0.6625452 -0.957618 -1.2992203][-3.344728 -3.651696 -3.4215217 -2.58473 -1.6541526 -0.96111345 -0.5815897 -0.55888104 -0.72853661 -0.82016587 -0.95784307 -0.94584942 -0.9941051 -1.2155452 -1.465745][-3.4791493 -3.6625221 -3.24898 -2.3086298 -1.4393771 -0.87959504 -0.69034338 -0.86583447 -1.1169758 -1.1605403 -1.2015798 -1.0831695 -1.0492067 -1.2310123 -1.459337][-3.2347383 -3.2919178 -2.7922587 -1.9327788 -1.2803557 -0.85594654 -0.74156022 -1.003448 -1.2954068 -1.3061562 -1.2675838 -1.0809739 -0.99830127 -1.146862 -1.3685622][-2.6909378 -2.7117517 -2.2994308 -1.7083018 -1.3793337 -1.0146422 -0.78788066 -0.98427606 -1.2675457 -1.295413 -1.2509198 -1.0516467 -0.9571321 -1.0820696 -1.2979071][-2.0652096 -2.1897697 -1.9880228 -1.6785514 -1.5605321 -1.1568382 -0.73888707 -0.78120685 -1.0118647 -1.1165943 -1.1747119 -1.0565481 -1.0217998 -1.1498795 -1.3424921][-1.670527 -1.8541512 -1.7659812 -1.5529916 -1.4573309 -0.9953289 -0.4727366 -0.44539189 -0.7247057 -1.027622 -1.280839 -1.2694352 -1.2731812 -1.3808968 -1.5145409][-1.5164773 -1.6955454 -1.6384265 -1.3845837 -1.206615 -0.78216124 -0.37349749 -0.43245173 -0.80362129 -1.2382894 -1.5729082 -1.5423443 -1.4870687 -1.5473242 -1.6463757][-1.5920651 -1.8429806 -1.8427134 -1.5184915 -1.2300458 -0.91620755 -0.71923351 -0.87679362 -1.2272918 -1.5699646 -1.7808099 -1.5967131 -1.4203017 -1.4690504 -1.6225691][-1.4962559 -1.9351087 -2.1337538 -1.855469 -1.5196669 -1.3118126 -1.2441113 -1.3684075 -1.5642791 -1.7007141 -1.748055 -1.4467533 -1.1967666 -1.2859855 -1.5272007][-1.1634576 -1.8006623 -2.2179372 -2.0573997 -1.7564893 -1.6356316 -1.6023767 -1.6449177 -1.6864717 -1.6600685 -1.6255615 -1.3018689 -1.0434716 -1.1810701 -1.4741919][-0.7663269 -1.4921794 -2.0003855 -1.9261146 -1.7461867 -1.7462695 -1.752739 -1.7686701 -1.7186291 -1.6161907 -1.5864601 -1.3393502 -1.1213841 -1.261801 -1.5173407][-0.55072927 -1.2073863 -1.6972258 -1.7390184 -1.7648423 -1.8631363 -1.8457737 -1.8117027 -1.7148924 -1.586508 -1.5887518 -1.4234304 -1.2514942 -1.374733 -1.562372]]...]
INFO - root - 2017-12-07 15:52:35.593490: step 12710, loss = 0.74, batch loss = 0.66 (7.1 examples/sec; 4.509 sec/batch; 88h:11m:51s remains)
INFO - root - 2017-12-07 15:53:21.337003: step 12720, loss = 0.68, batch loss = 0.60 (6.9 examples/sec; 4.631 sec/batch; 90h:34m:20s remains)
INFO - root - 2017-12-07 15:54:07.070470: step 12730, loss = 0.65, batch loss = 0.58 (7.1 examples/sec; 4.526 sec/batch; 88h:30m:42s remains)
INFO - root - 2017-12-07 15:54:52.642628: step 12740, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.520 sec/batch; 88h:22m:16s remains)
INFO - root - 2017-12-07 15:55:38.069662: step 12750, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.617 sec/batch; 90h:15m:09s remains)
INFO - root - 2017-12-07 15:56:23.601594: step 12760, loss = 0.75, batch loss = 0.67 (7.1 examples/sec; 4.529 sec/batch; 88h:31m:39s remains)
INFO - root - 2017-12-07 15:57:09.295661: step 12770, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.563 sec/batch; 89h:09m:58s remains)
INFO - root - 2017-12-07 15:57:54.815028: step 12780, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.614 sec/batch; 90h:09m:42s remains)
INFO - root - 2017-12-07 15:58:40.382834: step 12790, loss = 0.80, batch loss = 0.73 (7.2 examples/sec; 4.444 sec/batch; 86h:49m:25s remains)
INFO - root - 2017-12-07 15:59:25.891537: step 12800, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.561 sec/batch; 89h:05m:30s remains)
2017-12-07 15:59:28.540106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.806073 -3.8551354 -3.9074759 -3.9776127 -4.0502968 -4.1020236 -4.135716 -4.141458 -4.1212316 -4.0755782 -4.0116181 -3.9619064 -3.9307041 -3.9150844 -3.9161906][-3.7525413 -3.7812755 -3.8057859 -3.844831 -3.8919282 -3.9330697 -3.9716456 -3.991785 -3.9750583 -3.9159288 -3.8361006 -3.778688 -3.7602782 -3.7715709 -3.8153448][-3.810415 -3.838819 -3.8573928 -3.8790135 -3.8971405 -3.9018433 -3.9131885 -3.9275119 -3.9098425 -3.8394468 -3.7520626 -3.6964471 -3.687494 -3.7081261 -3.7698953][-3.7625189 -3.813194 -3.8479562 -3.8612523 -3.8412192 -3.7880077 -3.755455 -3.7590671 -3.7452922 -3.6747808 -3.5968137 -3.5558369 -3.5673921 -3.6171579 -3.7125566][-3.7099755 -3.7508297 -3.7568898 -3.7086341 -3.5936804 -3.4450471 -3.3497391 -3.3341665 -3.3230929 -3.2630816 -3.2190778 -3.2083392 -3.2370555 -3.3132493 -3.436995][-3.4617262 -3.4988675 -3.4918873 -3.4068046 -3.2328653 -3.0415723 -2.9467738 -2.964777 -2.9833684 -2.9395814 -2.9406581 -2.9719737 -3.0066915 -3.079021 -3.1848807][-3.0732841 -3.081212 -3.0535588 -2.9300051 -2.7031703 -2.4804258 -2.4166293 -2.5337558 -2.6767135 -2.7399006 -2.825844 -2.8917818 -2.8946981 -2.9173374 -2.9683287][-2.9281521 -2.9046426 -2.8630624 -2.7179163 -2.4733756 -2.2491965 -2.2172532 -2.4164264 -2.6542878 -2.7967236 -2.903893 -2.9144487 -2.8156984 -2.748287 -2.7352939][-2.89858 -2.886888 -2.8873045 -2.8021469 -2.6410904 -2.4997473 -2.5088203 -2.6898751 -2.8848925 -2.991792 -3.059495 -3.0286927 -2.9012833 -2.8214822 -2.8073792][-2.9278836 -2.9625335 -3.0197318 -3.0037465 -2.9180853 -2.8351529 -2.8508635 -2.9720845 -3.0912185 -3.1525204 -3.1980038 -3.1799035 -3.0964077 -3.0456696 -3.0408754][-2.9694715 -3.0356212 -3.1174364 -3.1379266 -3.0877504 -3.0223064 -3.0275779 -3.1103296 -3.1968875 -3.2580028 -3.3044505 -3.3048964 -3.2605746 -3.2295399 -3.2295065][-3.0332828 -3.0855546 -3.1430016 -3.1611397 -3.1276124 -3.0767386 -3.0807357 -3.1412094 -3.2094579 -3.2644711 -3.3014278 -3.3073237 -3.2875662 -3.2705247 -3.2690034][-3.0984433 -3.12408 -3.150923 -3.1610563 -3.146668 -3.1191349 -3.1277251 -3.1705871 -3.2157042 -3.2488031 -3.2634242 -3.262424 -3.2493348 -3.2342062 -3.2252891][-3.1158619 -3.1403418 -3.1666265 -3.1849806 -3.1878884 -3.176307 -3.1788149 -3.1962976 -3.2118421 -3.2223158 -3.2225759 -3.2158833 -3.2049129 -3.1906776 -3.1794186][-3.1095891 -3.1331825 -3.1575203 -3.1757216 -3.1854529 -3.1853106 -3.1870365 -3.1903841 -3.1900997 -3.1896989 -3.1860185 -3.1796353 -3.1740155 -3.166003 -3.15676]]...]
INFO - root - 2017-12-07 16:00:14.056600: step 12810, loss = 0.78, batch loss = 0.70 (7.2 examples/sec; 4.475 sec/batch; 87h:24m:22s remains)
INFO - root - 2017-12-07 16:00:59.303281: step 12820, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.486 sec/batch; 87h:36m:35s remains)
INFO - root - 2017-12-07 16:01:44.845889: step 12830, loss = 0.82, batch loss = 0.75 (7.1 examples/sec; 4.538 sec/batch; 88h:36m:35s remains)
INFO - root - 2017-12-07 16:02:30.046605: step 12840, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.503 sec/batch; 87h:54m:25s remains)
INFO - root - 2017-12-07 16:03:15.659261: step 12850, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.563 sec/batch; 89h:04m:18s remains)
INFO - root - 2017-12-07 16:04:00.822427: step 12860, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.581 sec/batch; 89h:24m:19s remains)
INFO - root - 2017-12-07 16:04:46.688955: step 12870, loss = 0.65, batch loss = 0.58 (7.0 examples/sec; 4.557 sec/batch; 88h:55m:23s remains)
INFO - root - 2017-12-07 16:05:32.201670: step 12880, loss = 0.78, batch loss = 0.71 (7.2 examples/sec; 4.428 sec/batch; 86h:23m:30s remains)
INFO - root - 2017-12-07 16:06:17.602155: step 12890, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.582 sec/batch; 89h:23m:49s remains)
INFO - root - 2017-12-07 16:07:03.376167: step 12900, loss = 0.71, batch loss = 0.64 (7.2 examples/sec; 4.458 sec/batch; 86h:58m:08s remains)
2017-12-07 16:07:06.103459: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7058094 -2.4135849 -2.3134322 -2.3689122 -2.3748975 -2.3032718 -2.3889415 -2.6143832 -2.8082404 -2.9316592 -2.7929471 -2.7772605 -3.247124 -3.64206 -3.6923945][-2.9103923 -2.5533762 -2.2857134 -2.112824 -1.9305599 -1.7258499 -1.7578835 -1.9886482 -2.2627671 -2.568212 -2.6835015 -2.9599421 -3.6094801 -4.0219579 -3.9690475][-3.0752492 -2.5895028 -2.0174971 -1.5665622 -1.2792306 -1.1075861 -1.2227449 -1.5657158 -2.0408475 -2.6141317 -3.0272737 -3.4573133 -3.9860415 -4.118576 -3.8252373][-3.1943865 -2.4084096 -1.4443107 -0.85052085 -0.78966 -0.98738527 -1.3632679 -1.888217 -2.5423176 -3.2498271 -3.7290378 -4.0388775 -4.2105427 -3.9402294 -3.4256377][-3.4135058 -2.2338696 -0.8875823 -0.28933334 -0.67945981 -1.4413576 -2.1205125 -2.71605 -3.2849898 -3.7704957 -3.9792235 -4.0194616 -3.9425235 -3.5606122 -3.1232488][-3.5675015 -2.1071255 -0.48133111 0.15708017 -0.53073645 -1.7234972 -2.6902895 -3.2775671 -3.6063788 -3.7055569 -3.5955346 -3.4993813 -3.4601045 -3.2787845 -3.0704069][-3.4100449 -2.0178611 -0.44911265 0.13356924 -0.66239977 -2.040741 -3.1327217 -3.5717366 -3.6008415 -3.3527765 -3.0406671 -2.9796467 -3.125309 -3.2222786 -3.2083387][-3.245872 -2.2476106 -1.0848441 -0.71659446 -1.4078903 -2.5736268 -3.4472837 -3.6333389 -3.4936213 -3.0830002 -2.6222248 -2.5774403 -2.8802338 -3.2066293 -3.3312211][-3.2129211 -2.7630281 -2.0758996 -1.8118138 -2.1719661 -2.8735631 -3.346848 -3.3295145 -3.22842 -2.8682384 -2.3779819 -2.3050385 -2.7025266 -3.1572232 -3.3638511][-3.2211652 -3.163728 -2.7724023 -2.4909675 -2.5136552 -2.8220348 -2.960628 -2.8244653 -2.8419647 -2.6757343 -2.3364818 -2.2438304 -2.636415 -3.0556006 -3.2318258][-3.2802124 -3.2550774 -2.8891559 -2.5328853 -2.3738751 -2.5087438 -2.5350506 -2.4078784 -2.5452609 -2.5686789 -2.333627 -2.1583867 -2.4321775 -2.704128 -2.86076][-3.3753023 -3.1779008 -2.7174706 -2.3089817 -2.0855951 -2.2101355 -2.3022935 -2.2472835 -2.4141481 -2.5021276 -2.2253351 -1.9813576 -2.1649547 -2.3231032 -2.5706835][-3.4829545 -3.0423324 -2.4953721 -2.0738 -1.8404715 -2.073287 -2.3353939 -2.3100309 -2.3351128 -2.3188431 -1.9986906 -1.8673799 -2.1226218 -2.3192527 -2.7085519][-3.4349694 -2.7774305 -2.207417 -1.8341084 -1.6350403 -2.0444565 -2.532702 -2.5236235 -2.345623 -2.1751142 -1.8887951 -2.0006502 -2.3889322 -2.6911168 -3.183043][-3.1774426 -2.4363511 -1.8992069 -1.5786033 -1.4022696 -1.9811475 -2.6901836 -2.73385 -2.4525776 -2.1809864 -1.967006 -2.2917767 -2.7814047 -3.1657104 -3.664655]]...]
INFO - root - 2017-12-07 16:07:51.718873: step 12910, loss = 0.69, batch loss = 0.62 (6.9 examples/sec; 4.605 sec/batch; 89h:49m:24s remains)
INFO - root - 2017-12-07 16:08:37.694386: step 12920, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.586 sec/batch; 89h:26m:29s remains)
INFO - root - 2017-12-07 16:09:23.099525: step 12930, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.603 sec/batch; 89h:44m:46s remains)
INFO - root - 2017-12-07 16:10:08.759661: step 12940, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.622 sec/batch; 90h:07m:03s remains)
INFO - root - 2017-12-07 16:10:54.287450: step 12950, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.573 sec/batch; 89h:09m:02s remains)
INFO - root - 2017-12-07 16:11:39.754956: step 12960, loss = 0.86, batch loss = 0.79 (7.2 examples/sec; 4.445 sec/batch; 86h:38m:28s remains)
INFO - root - 2017-12-07 16:12:25.120252: step 12970, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 4.500 sec/batch; 87h:41m:51s remains)
INFO - root - 2017-12-07 16:13:10.874119: step 12980, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.567 sec/batch; 88h:59m:14s remains)
INFO - root - 2017-12-07 16:13:56.492639: step 12990, loss = 0.74, batch loss = 0.66 (7.1 examples/sec; 4.528 sec/batch; 88h:13m:01s remains)
INFO - root - 2017-12-07 16:14:42.147586: step 13000, loss = 0.78, batch loss = 0.70 (7.1 examples/sec; 4.535 sec/batch; 88h:20m:09s remains)
2017-12-07 16:14:44.809453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5419285 -2.6133018 -2.6047888 -2.4278386 -2.4304676 -2.6307545 -2.873353 -3.1029358 -3.1202726 -3.0430655 -2.9460354 -2.7223995 -2.4638135 -2.1755934 -1.8247182][-2.4302807 -2.5103445 -2.5187392 -2.3760302 -2.3588831 -2.5195341 -2.706181 -2.8389261 -2.8180695 -2.7683058 -2.6969373 -2.479521 -2.2251272 -1.8537409 -1.309736][-2.5416455 -2.6433165 -2.6508381 -2.5392869 -2.5269909 -2.6696634 -2.7979827 -2.8189826 -2.7004237 -2.5578926 -2.3897743 -2.1164591 -1.8649516 -1.4870343 -0.93370414][-2.76086 -2.9058285 -2.9550562 -2.9184639 -2.9573617 -3.1025071 -3.1885276 -3.1226516 -2.897963 -2.6339235 -2.3649318 -2.1174974 -2.0111144 -1.8102796 -1.4328904][-2.9824066 -3.153192 -3.1570184 -3.0028341 -2.8796577 -2.8434422 -2.7921748 -2.6885495 -2.5286136 -2.3858304 -2.2549837 -2.206532 -2.3087819 -2.332623 -2.1692836][-3.0380154 -3.1340294 -2.8939552 -2.3700747 -1.8778484 -1.5498159 -1.3497174 -1.2660453 -1.2428608 -1.3022268 -1.4084227 -1.5636628 -1.7780728 -1.9138744 -1.9107244][-2.6614847 -2.5404153 -2.0097821 -1.1907804 -0.46320438 0.0043783188 0.22531891 0.24333143 0.18475199 0.081757545 -0.059824467 -0.21537066 -0.35730362 -0.442868 -0.46267247][-2.0946808 -1.8196609 -1.2020428 -0.4884851 0.0014986992 0.2620635 0.32467365 0.25119829 0.17598009 0.14104319 0.1247468 0.12051249 0.15833473 0.22369719 0.29087877][-1.760716 -1.6172283 -1.2350149 -0.939404 -0.95902371 -1.0843537 -1.2264826 -1.357455 -1.4522622 -1.4705758 -1.4051499 -1.310571 -1.1725047 -1.0158925 -0.88529372][-1.8817854 -2.004169 -1.9122748 -1.8904617 -2.1347487 -2.3972151 -2.6012306 -2.7429905 -2.890955 -3.0067592 -3.048795 -3.0616095 -3.0055842 -2.8761458 -2.6936255][-2.2248826 -2.4710298 -2.4712529 -2.5036857 -2.7238679 -2.958652 -3.1559653 -3.3373513 -3.6002696 -3.8716784 -4.0431409 -4.1497259 -4.1568975 -4.0458236 -3.811471][-2.2997742 -2.5051494 -2.4716115 -2.5438437 -2.8173041 -3.1565204 -3.4587712 -3.7406473 -4.05914 -4.3188057 -4.4036288 -4.3776035 -4.2511344 -4.0155754 -3.6956499][-2.3154113 -2.4309621 -2.3378892 -2.4052145 -2.6949844 -3.0843997 -3.4092813 -3.6328082 -3.7888765 -3.847214 -3.7536874 -3.591532 -3.3746665 -3.1155748 -2.8443761][-2.7610459 -2.8293121 -2.6816509 -2.6411767 -2.755878 -2.9413126 -3.0371408 -2.9944844 -2.86208 -2.6935773 -2.5028963 -2.3735685 -2.2834704 -2.2133541 -2.1487429][-3.1914132 -3.2510691 -3.1142783 -3.0027471 -2.933176 -2.8810408 -2.7178049 -2.4169214 -2.0846047 -1.8579857 -1.7500961 -1.7499206 -1.7986476 -1.8970041 -1.9921348]]...]
INFO - root - 2017-12-07 16:15:30.260897: step 13010, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.521 sec/batch; 88h:03m:19s remains)
INFO - root - 2017-12-07 16:16:16.162291: step 13020, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.557 sec/batch; 88h:44m:09s remains)
INFO - root - 2017-12-07 16:17:01.535070: step 13030, loss = 0.79, batch loss = 0.71 (7.0 examples/sec; 4.549 sec/batch; 88h:34m:29s remains)
INFO - root - 2017-12-07 16:17:47.089228: step 13040, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.552 sec/batch; 88h:36m:31s remains)
INFO - root - 2017-12-07 16:18:32.621102: step 13050, loss = 0.80, batch loss = 0.73 (6.9 examples/sec; 4.606 sec/batch; 89h:39m:30s remains)
INFO - root - 2017-12-07 16:19:18.225566: step 13060, loss = 0.68, batch loss = 0.61 (7.1 examples/sec; 4.528 sec/batch; 88h:07m:28s remains)
INFO - root - 2017-12-07 16:20:03.547458: step 13070, loss = 0.79, batch loss = 0.71 (7.0 examples/sec; 4.542 sec/batch; 88h:22m:37s remains)
INFO - root - 2017-12-07 16:20:49.125462: step 13080, loss = 0.68, batch loss = 0.61 (6.9 examples/sec; 4.634 sec/batch; 90h:09m:18s remains)
INFO - root - 2017-12-07 16:21:34.382608: step 13090, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.556 sec/batch; 88h:38m:31s remains)
INFO - root - 2017-12-07 16:22:20.256052: step 13100, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.574 sec/batch; 88h:57m:57s remains)
2017-12-07 16:22:22.920116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2823238 -2.9230778 -2.2942753 -1.6560364 -1.0504129 -0.6455338 -0.69595766 -1.1981883 -1.8507688 -2.5437191 -3.1765881 -3.4274135 -3.3218093 -3.1279984 -3.0206137][-3.0347204 -2.3953383 -1.6061521 -1.0678372 -0.67661452 -0.517447 -0.75168395 -1.1972649 -1.6334167 -2.1615067 -2.7521648 -3.0363264 -2.9694986 -2.8540955 -2.8428946][-2.6200438 -1.9136546 -1.2967522 -1.1536961 -1.1561165 -1.2481306 -1.4682698 -1.6031845 -1.6415696 -1.8953137 -2.3114471 -2.4973512 -2.4045329 -2.3601346 -2.487884][-2.1980112 -1.8434269 -1.7267282 -2.0194497 -2.2735565 -2.3895218 -2.331351 -1.9411492 -1.5353343 -1.5634186 -1.8280995 -1.8966208 -1.7826853 -1.7931614 -2.0163898][-1.9968605 -2.1949255 -2.5668743 -3.0391979 -3.2382026 -3.1384368 -2.6651344 -1.7761962 -1.0601754 -1.0534401 -1.3191156 -1.356591 -1.2827911 -1.3802781 -1.6556683][-2.0380619 -2.6710491 -3.2374611 -3.6271198 -3.6517942 -3.3364921 -2.6080766 -1.5114603 -0.75931692 -0.87059355 -1.1704566 -1.1537292 -1.074033 -1.1933942 -1.4339433][-2.0642152 -2.9728069 -3.5545409 -3.8052931 -3.6501317 -3.1694183 -2.3811033 -1.3623204 -0.79666734 -1.0639334 -1.3335915 -1.2043784 -1.0491912 -1.1152146 -1.2615001][-1.8960156 -2.8361702 -3.3165381 -3.4437156 -3.149256 -2.5284567 -1.7748239 -0.96527219 -0.66825318 -1.0495842 -1.3100727 -1.1716032 -1.024719 -1.0697043 -1.1621623][-1.6780317 -2.4836633 -2.8055587 -2.8303871 -2.4769239 -1.8462694 -1.1940045 -0.6140244 -0.51859426 -0.8948586 -1.1650021 -1.1420915 -1.052098 -1.0421903 -1.1070769][-1.8770604 -2.5646882 -2.7591171 -2.7244697 -2.4129298 -1.8803937 -1.3180084 -0.86938858 -0.7675209 -0.95473695 -1.1991515 -1.3257303 -1.3245671 -1.297152 -1.4181755][-2.5042009 -3.0952687 -3.193799 -3.134912 -2.9256163 -2.5118079 -2.0381653 -1.6431892 -1.4255774 -1.4211457 -1.7107425 -2.0123651 -2.1035252 -2.05368 -2.1693203][-3.0943565 -3.5744402 -3.6169508 -3.5853918 -3.5093384 -3.2410021 -2.9115009 -2.6373754 -2.4173515 -2.3625765 -2.6742451 -2.9593253 -2.9764252 -2.825943 -2.8430963][-3.3157825 -3.683538 -3.7332239 -3.7655628 -3.8134022 -3.6937196 -3.5246429 -3.412945 -3.309278 -3.3006406 -3.5207257 -3.5913844 -3.4503069 -3.2524419 -3.2119598][-3.2592545 -3.5174086 -3.5775726 -3.6394851 -3.7437181 -3.7155209 -3.6311946 -3.6234131 -3.7012835 -3.8973069 -4.1518431 -4.1145988 -3.9023902 -3.67094 -3.5294309][-3.1958928 -3.3568683 -3.4110622 -3.4810472 -3.591033 -3.6152289 -3.5696137 -3.6428382 -3.9033158 -4.3040671 -4.6549315 -4.6724939 -4.5214596 -4.2833786 -4.0089421]]...]
INFO - root - 2017-12-07 16:23:08.398592: step 13110, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.597 sec/batch; 89h:23m:58s remains)
INFO - root - 2017-12-07 16:23:54.336293: step 13120, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.558 sec/batch; 88h:38m:25s remains)
INFO - root - 2017-12-07 16:24:39.671796: step 13130, loss = 0.81, batch loss = 0.74 (6.9 examples/sec; 4.639 sec/batch; 90h:12m:21s remains)
INFO - root - 2017-12-07 16:25:25.287546: step 13140, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 4.576 sec/batch; 88h:57m:17s remains)
INFO - root - 2017-12-07 16:26:10.799393: step 13150, loss = 0.79, batch loss = 0.71 (6.9 examples/sec; 4.612 sec/batch; 89h:38m:19s remains)
INFO - root - 2017-12-07 16:26:56.346046: step 13160, loss = 0.75, batch loss = 0.67 (7.1 examples/sec; 4.536 sec/batch; 88h:09m:37s remains)
INFO - root - 2017-12-07 16:27:42.020838: step 13170, loss = 0.77, batch loss = 0.70 (6.8 examples/sec; 4.677 sec/batch; 90h:52m:50s remains)
INFO - root - 2017-12-07 16:28:27.614946: step 13180, loss = 0.76, batch loss = 0.68 (6.9 examples/sec; 4.643 sec/batch; 90h:12m:42s remains)
INFO - root - 2017-12-07 16:29:13.143740: step 13190, loss = 0.74, batch loss = 0.67 (7.2 examples/sec; 4.475 sec/batch; 86h:55m:55s remains)
INFO - root - 2017-12-07 16:29:58.709995: step 13200, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.553 sec/batch; 88h:26m:05s remains)
2017-12-07 16:30:01.386251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.42876339 -0.69034791 -1.1580141 -1.6237659 -1.910635 -1.7940278 -1.5730083 -1.4985702 -1.3245707 -1.0584121 -0.82740641 -0.66236019 -0.74458122 -0.66379809 -0.16780376][-0.57086945 -0.90609336 -1.3733225 -1.8523128 -2.0728338 -1.8808525 -1.5826173 -1.4426091 -1.251461 -0.88907433 -0.47114134 -0.23189449 -0.34079123 -0.2635107 0.23631239][-0.94612169 -1.2865789 -1.7366369 -2.1791871 -2.2939806 -2.0326469 -1.6402571 -1.3684402 -1.1782634 -0.85959935 -0.42180347 -0.1340127 -0.17015028 -0.093484879 0.29592848][-1.2524028 -1.4750121 -1.8716018 -2.2524836 -2.2830982 -2.0370033 -1.6293542 -1.2874241 -1.1945727 -1.0339177 -0.59866977 -0.15949059 -0.055885792 -0.072566509 0.059258938][-1.3584204 -1.480381 -1.8206322 -2.0729334 -2.0103855 -1.8129053 -1.4096365 -1.0314178 -1.0830441 -1.0989387 -0.62821984 -0.0080285072 0.20591736 0.0034251213 -0.20946598][-1.2552977 -1.4543421 -1.7960067 -1.9083595 -1.7490516 -1.5456319 -1.0263538 -0.50863647 -0.67395616 -0.9134531 -0.5080626 0.19130039 0.47066641 0.12560511 -0.35270214][-1.0406871 -1.4587309 -1.8622415 -1.8773057 -1.6465065 -1.3142686 -0.47543955 0.28409672 -0.030299664 -0.65711045 -0.56380582 0.01287365 0.32723761 0.0080780983 -0.50832558][-0.7120018 -1.313978 -1.772155 -1.7928042 -1.5950887 -1.1116335 0.07428503 1.0628533 0.59822273 -0.45241523 -0.79951191 -0.48056412 -0.19420528 -0.40456104 -0.78750324][-0.52122569 -1.1292508 -1.5416076 -1.6432655 -1.6184988 -1.1444283 0.14240122 1.1942339 0.7501893 -0.43071985 -1.0418012 -0.93335223 -0.70253968 -0.7910068 -0.96881342][-0.5237174 -0.99252653 -1.2773774 -1.4804142 -1.6843104 -1.3777263 -0.24619436 0.75749874 0.55830669 -0.40192842 -1.0343561 -1.0502973 -0.8962245 -0.88917851 -0.870739][-0.59056711 -0.91100097 -1.1081998 -1.4001441 -1.7595129 -1.6342566 -0.75541854 0.15613556 0.23839617 -0.37511444 -0.91447973 -1.0114768 -0.94276905 -0.88739443 -0.76183391][-0.69402075 -0.90371656 -1.0624113 -1.3605163 -1.682955 -1.6209598 -1.0165882 -0.2968421 -0.086540222 -0.44762969 -0.90175438 -1.0540719 -1.0342479 -0.94816089 -0.76341605][-0.94642043 -1.130605 -1.2969084 -1.5262365 -1.6724398 -1.5458252 -1.1481047 -0.68866515 -0.52169394 -0.77423978 -1.146157 -1.3066237 -1.3057132 -1.19486 -0.96937966][-1.424418 -1.6295612 -1.8038781 -1.9433084 -1.9201005 -1.7255697 -1.4727147 -1.2500856 -1.1950858 -1.4069824 -1.6727459 -1.7770934 -1.7635999 -1.6468906 -1.4241662][-1.9797766 -2.16681 -2.3023918 -2.3636396 -2.2815213 -2.1086652 -1.9678454 -1.8936491 -1.9144113 -2.0617568 -2.1950412 -2.2108994 -2.1692259 -2.06832 -1.8936541]]...]
INFO - root - 2017-12-07 16:30:46.966038: step 13210, loss = 0.68, batch loss = 0.61 (7.4 examples/sec; 4.349 sec/batch; 84h:27m:23s remains)
INFO - root - 2017-12-07 16:31:32.588157: step 13220, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 4.615 sec/batch; 89h:36m:54s remains)
INFO - root - 2017-12-07 16:32:18.295349: step 13230, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.545 sec/batch; 88h:14m:20s remains)
INFO - root - 2017-12-07 16:33:03.791163: step 13240, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.555 sec/batch; 88h:25m:46s remains)
INFO - root - 2017-12-07 16:33:49.405161: step 13250, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.572 sec/batch; 88h:44m:58s remains)
INFO - root - 2017-12-07 16:34:35.083811: step 13260, loss = 0.67, batch loss = 0.59 (7.0 examples/sec; 4.600 sec/batch; 89h:16m:09s remains)
INFO - root - 2017-12-07 16:35:20.573199: step 13270, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.526 sec/batch; 87h:49m:35s remains)
INFO - root - 2017-12-07 16:36:06.143923: step 13280, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.573 sec/batch; 88h:43m:48s remains)
INFO - root - 2017-12-07 16:36:51.536371: step 13290, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.569 sec/batch; 88h:38m:25s remains)
INFO - root - 2017-12-07 16:37:36.868621: step 13300, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.570 sec/batch; 88h:38m:12s remains)
2017-12-07 16:37:39.655564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0954177 -2.0619662 -2.1117179 -2.2461307 -2.4319623 -2.5420926 -2.6488903 -2.7467747 -2.7335639 -2.8064156 -2.9141583 -3.0153408 -3.2148371 -3.3507695 -3.309916][-2.2373116 -2.2535946 -2.302187 -2.3844423 -2.4808066 -2.4963379 -2.5550609 -2.6348631 -2.5913801 -2.6310267 -2.7445507 -2.90226 -3.154619 -3.327631 -3.3123088][-2.3563182 -2.3920674 -2.4315929 -2.4737332 -2.5203774 -2.5082092 -2.5683374 -2.6568124 -2.6254721 -2.6576972 -2.7708695 -2.9408855 -3.1458988 -3.2543645 -3.2268176][-2.4153054 -2.4007869 -2.3676634 -2.3441691 -2.3671875 -2.3907945 -2.4888973 -2.5687447 -2.550076 -2.5779862 -2.6653514 -2.8140984 -2.9805002 -3.0652108 -3.0638466][-2.3285534 -2.4120004 -2.3935273 -2.2918499 -2.2027385 -2.1918731 -2.275275 -2.3301132 -2.3417776 -2.3811121 -2.4128723 -2.4892659 -2.6062589 -2.6939526 -2.7500863][-2.154845 -2.298085 -2.2828491 -2.0591588 -1.7806628 -1.6525536 -1.6689441 -1.7159927 -1.8369207 -1.9956291 -2.0808988 -2.1727748 -2.2801697 -2.3318379 -2.3729303][-2.5934782 -2.5997753 -2.44593 -2.0734675 -1.6738951 -1.4880989 -1.4198375 -1.3850808 -1.5201712 -1.7198429 -1.8710027 -2.0592673 -2.2463219 -2.2830679 -2.2404587][-3.4704552 -3.4551876 -3.243155 -2.8574462 -2.4945705 -2.3599982 -2.3009229 -2.2043035 -2.2216494 -2.2399211 -2.2274659 -2.32153 -2.5064306 -2.5151973 -2.38089][-3.9030323 -3.8762553 -3.6508839 -3.285748 -2.9592013 -2.8596673 -2.868475 -2.8686113 -2.9004993 -2.8337002 -2.7056677 -2.7056465 -2.843205 -2.7789311 -2.5534735][-3.8817532 -3.8249323 -3.6108766 -3.3011792 -3.0388122 -2.9657454 -3.0191689 -3.0836606 -3.095546 -2.9808664 -2.8512311 -2.9165268 -3.1213355 -3.0747674 -2.8248005][-3.6866341 -3.6024427 -3.4091873 -3.1687715 -2.9964252 -3.0157475 -3.1812112 -3.3339648 -3.3484116 -3.2004013 -3.0479796 -3.1423295 -3.3855863 -3.3822505 -3.1506104][-3.4409342 -3.2920547 -3.0900764 -2.8705258 -2.7140775 -2.76998 -3.028749 -3.3320272 -3.5058935 -3.4757743 -3.3722057 -3.4440727 -3.6038365 -3.5457964 -3.2843552][-3.2406094 -3.059011 -2.9058361 -2.766742 -2.6751583 -2.7657518 -3.015023 -3.3059912 -3.4925256 -3.4858139 -3.4122715 -3.4642582 -3.5493989 -3.4362087 -3.1533346][-3.0931618 -2.934854 -2.8222041 -2.7469857 -2.7872305 -2.9941416 -3.2160449 -3.4007742 -3.4984896 -3.475527 -3.4528363 -3.5474389 -3.6318419 -3.5132844 -3.2312448][-2.8962016 -2.7928841 -2.6809659 -2.5879459 -2.7056081 -2.9952846 -3.1673765 -3.2422824 -3.2799597 -3.2908096 -3.3569028 -3.5534942 -3.731369 -3.6842072 -3.4567959]]...]
INFO - root - 2017-12-07 16:38:25.250949: step 13310, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 4.518 sec/batch; 87h:37m:08s remains)
INFO - root - 2017-12-07 16:39:10.582199: step 13320, loss = 0.82, batch loss = 0.75 (7.2 examples/sec; 4.461 sec/batch; 86h:29m:44s remains)
INFO - root - 2017-12-07 16:39:56.236200: step 13330, loss = 0.87, batch loss = 0.80 (7.0 examples/sec; 4.546 sec/batch; 88h:08m:21s remains)
INFO - root - 2017-12-07 16:40:41.707748: step 13340, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.550 sec/batch; 88h:11m:42s remains)
INFO - root - 2017-12-07 16:41:27.363152: step 13350, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.565 sec/batch; 88h:29m:11s remains)
INFO - root - 2017-12-07 16:42:13.043797: step 13360, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.605 sec/batch; 89h:14m:36s remains)
INFO - root - 2017-12-07 16:42:58.877215: step 13370, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.572 sec/batch; 88h:34m:58s remains)
INFO - root - 2017-12-07 16:43:44.185864: step 13380, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 4.603 sec/batch; 89h:10m:29s remains)
INFO - root - 2017-12-07 16:44:29.416306: step 13390, loss = 0.71, batch loss = 0.63 (7.0 examples/sec; 4.600 sec/batch; 89h:06m:38s remains)
INFO - root - 2017-12-07 16:45:14.951747: step 13400, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.600 sec/batch; 89h:05m:37s remains)
2017-12-07 16:45:17.646518: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6404676 -1.6602154 -1.6910875 -1.7124598 -1.7756248 -1.8761892 -1.8764787 -1.87445 -1.9908519 -1.978909 -1.659965 -1.1583717 -0.77381372 -0.4660759 -0.10793734][-1.7242799 -1.7605753 -1.8054821 -1.825808 -1.8850667 -2.0040314 -2.0047889 -1.983969 -2.0849495 -2.0526936 -1.7527585 -1.2911069 -0.94091082 -0.64072132 -0.25642967][-1.8399119 -1.864444 -1.8947866 -1.8913765 -1.9323552 -2.0574045 -2.0590947 -2.034677 -2.1480291 -2.1346824 -1.872005 -1.4446745 -1.1186152 -0.8141613 -0.40440321][-2.1830208 -2.1617975 -2.1461387 -2.0881951 -2.0846076 -2.1902292 -2.1722777 -2.1296566 -2.2536833 -2.27787 -2.0466115 -1.6329823 -1.3113937 -0.99003482 -0.54491806][-2.4247055 -2.347909 -2.2887948 -2.1896205 -2.1449914 -2.2353177 -2.2228987 -2.1900389 -2.3398557 -2.4136822 -2.2196178 -1.8179328 -1.4893787 -1.1429763 -0.65570593][-2.7073228 -2.5792842 -2.4887061 -2.3696773 -2.2927647 -2.3513169 -2.3324778 -2.3087461 -2.4838314 -2.5953236 -2.4133382 -1.9967024 -1.6444278 -1.2655289 -0.72730136][-3.0066433 -2.8327179 -2.7267523 -2.6045723 -2.4948766 -2.5045552 -2.4665656 -2.4586833 -2.6647811 -2.8060856 -2.6193676 -2.1683578 -1.7752953 -1.3483198 -0.75840378][-3.1941547 -2.980823 -2.8627133 -2.7350545 -2.5816717 -2.5326304 -2.4717538 -2.4772308 -2.7117586 -2.8883882 -2.7142682 -2.2503524 -1.8410938 -1.3853576 -0.76410508][-3.3245659 -3.0928688 -2.9627066 -2.8079114 -2.6006422 -2.5073605 -2.4351354 -2.4409881 -2.6916285 -2.9039507 -2.7500787 -2.2900848 -1.8759081 -1.4059856 -0.76685023][-3.3467069 -3.1381927 -3.0237589 -2.8577614 -2.6211243 -2.5184741 -2.4529495 -2.462713 -2.7212362 -2.9494793 -2.7954819 -2.32168 -1.8936822 -1.4105601 -0.76640058][-3.3504379 -3.1779108 -3.0850897 -2.9159133 -2.6812067 -2.5989773 -2.5478573 -2.56179 -2.8101413 -3.026329 -2.8437705 -2.3379827 -1.8879979 -1.390197 -0.75961447][-3.3960335 -3.2326584 -3.1473222 -2.9861369 -2.7665017 -2.7145865 -2.6842463 -2.7015123 -2.922976 -3.0958524 -2.8777523 -2.350451 -1.8778291 -1.3602457 -0.75118995][-3.4593644 -3.272471 -3.1898918 -3.0515661 -2.8613739 -2.8283987 -2.8154435 -2.8267596 -3.0047083 -3.1196308 -2.8677382 -2.3373666 -1.8578322 -1.3322189 -0.74915361][-3.4362004 -3.2382085 -3.176146 -3.0774765 -2.9110112 -2.8717451 -2.874578 -2.8664989 -2.9933209 -3.0574036 -2.789325 -2.2746511 -1.79674 -1.2776208 -0.73170996][-3.3073115 -3.1003346 -3.0826035 -3.0535021 -2.9099474 -2.8458211 -2.8540111 -2.8259082 -2.9003627 -2.9313352 -2.677424 -2.1977341 -1.7264445 -1.2144024 -0.70383596]]...]
INFO - root - 2017-12-07 16:46:03.170213: step 13410, loss = 0.73, batch loss = 0.65 (7.0 examples/sec; 4.580 sec/batch; 88h:41m:03s remains)
INFO - root - 2017-12-07 16:46:48.665453: step 13420, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 4.520 sec/batch; 87h:30m:53s remains)
INFO - root - 2017-12-07 16:47:34.085230: step 13430, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.603 sec/batch; 89h:06m:32s remains)
INFO - root - 2017-12-07 16:48:19.604191: step 13440, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.538 sec/batch; 87h:50m:15s remains)
INFO - root - 2017-12-07 16:49:04.950770: step 13450, loss = 0.67, batch loss = 0.59 (7.0 examples/sec; 4.544 sec/batch; 87h:57m:01s remains)
INFO - root - 2017-12-07 16:49:50.262234: step 13460, loss = 0.71, batch loss = 0.64 (7.3 examples/sec; 4.366 sec/batch; 84h:28m:43s remains)
INFO - root - 2017-12-07 16:50:35.808646: step 13470, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.536 sec/batch; 87h:45m:48s remains)
INFO - root - 2017-12-07 16:51:21.274341: step 13480, loss = 0.90, batch loss = 0.83 (7.1 examples/sec; 4.529 sec/batch; 87h:36m:29s remains)
INFO - root - 2017-12-07 16:52:06.871810: step 13490, loss = 0.73, batch loss = 0.66 (7.2 examples/sec; 4.437 sec/batch; 85h:49m:20s remains)
INFO - root - 2017-12-07 16:52:52.279163: step 13500, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.502 sec/batch; 87h:03m:54s remains)
2017-12-07 16:52:55.046453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.707341 -3.8712816 -3.9243684 -3.9699922 -3.9063504 -3.5699046 -3.1906712 -3.1726975 -3.6742179 -4.3887429 -4.955142 -4.9711561 -4.361001 -3.6365454 -3.1302519][-3.6354823 -3.747251 -3.5054336 -3.2760272 -3.1555634 -2.8952823 -2.6176372 -2.7321327 -3.4549382 -4.3282232 -5.0694752 -5.1641569 -4.3959522 -3.4998994 -2.8047895][-3.5764818 -3.6330106 -3.0276787 -2.3725011 -2.0038576 -1.6433899 -1.3007658 -1.5017295 -2.4628365 -3.5705051 -4.6911535 -5.085072 -4.3591757 -3.3797631 -2.4669542][-3.430594 -3.4019084 -2.510941 -1.5622098 -0.93736553 -0.31415939 0.18159962 -0.13157606 -1.2195194 -2.4930634 -3.9780085 -4.7180538 -4.2213063 -3.3075175 -2.279007][-3.2616982 -3.1671767 -2.0834036 -0.98319745 -0.044881821 1.0095139 1.5716071 0.95221281 -0.33686972 -1.8665154 -3.6369388 -4.4782033 -4.0660467 -3.2040462 -2.1496694][-3.0891688 -3.0723538 -2.065181 -0.92677021 0.50771952 2.2736897 2.9266534 2.0015712 0.47767258 -1.5347004 -3.7198496 -4.5611973 -4.105309 -3.2165341 -2.0716412][-2.90797 -2.9583435 -2.2594147 -1.2911355 0.48481035 2.7108016 3.4120655 2.5540633 1.17412 -1.1612282 -3.7104998 -4.5720572 -4.1277695 -3.2180207 -1.877584][-2.533082 -2.7234964 -2.4207988 -1.6548676 0.24909925 2.4485421 3.0819521 2.5556602 1.5456963 -0.83326054 -3.4247355 -4.2281489 -3.8508878 -2.9753606 -1.4443424][-1.9690149 -2.2665715 -2.3815811 -1.8293307 0.0037431717 1.9712801 2.757731 2.7909627 1.991199 -0.50781775 -3.038959 -3.7961078 -3.4703302 -2.5859218 -0.94830441][-1.6047533 -1.8849497 -2.322825 -2.0347571 -0.52020574 1.1525311 2.3287 3.0839491 2.3739257 -0.22560644 -2.6938229 -3.5588248 -3.3669262 -2.4413705 -0.78289986][-1.2816591 -1.5092423 -2.1668649 -2.1728196 -1.0874763 0.21202278 1.6297851 2.9003582 2.2123094 -0.26659203 -2.4770882 -3.4823632 -3.5744843 -2.7193542 -1.1873076][-1.1032057 -1.2032156 -1.8672593 -2.1486344 -1.5818858 -0.66922975 0.723814 2.1672254 1.6537137 -0.38159752 -2.0880895 -3.1017385 -3.5222924 -2.96509 -1.8238249][-1.4259408 -1.4825213 -1.9897461 -2.3853369 -2.2196648 -1.6017084 -0.40842915 0.92970657 0.75919533 -0.62258124 -1.7422864 -2.6162996 -3.2764907 -3.0764024 -2.3219442][-1.9032619 -1.9997122 -2.3463776 -2.7911847 -2.9092777 -2.5232835 -1.6595042 -0.6775496 -0.54911685 -1.2509811 -1.8830311 -2.5862951 -3.3112066 -3.3377647 -2.81707][-2.2259004 -2.2954881 -2.4810915 -2.9160161 -3.2270579 -3.102422 -2.6579144 -2.1184216 -1.8818665 -2.0873184 -2.3807371 -2.8479443 -3.4180789 -3.5076635 -3.1522765]]...]
INFO - root - 2017-12-07 16:53:40.640974: step 13510, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.544 sec/batch; 87h:51m:54s remains)
INFO - root - 2017-12-07 16:54:26.159351: step 13520, loss = 0.75, batch loss = 0.67 (7.1 examples/sec; 4.485 sec/batch; 86h:43m:13s remains)
INFO - root - 2017-12-07 16:55:11.697563: step 13530, loss = 0.89, batch loss = 0.81 (7.0 examples/sec; 4.589 sec/batch; 88h:42m:35s remains)
INFO - root - 2017-12-07 16:55:56.908431: step 13540, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.579 sec/batch; 88h:30m:18s remains)
INFO - root - 2017-12-07 16:56:42.469135: step 13550, loss = 0.66, batch loss = 0.59 (6.9 examples/sec; 4.610 sec/batch; 89h:05m:37s remains)
INFO - root - 2017-12-07 16:57:27.671815: step 13560, loss = 0.67, batch loss = 0.59 (7.0 examples/sec; 4.576 sec/batch; 88h:26m:01s remains)
INFO - root - 2017-12-07 16:58:13.055329: step 13570, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 4.668 sec/batch; 90h:11m:05s remains)
INFO - root - 2017-12-07 16:58:58.367770: step 13580, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.506 sec/batch; 87h:03m:13s remains)
INFO - root - 2017-12-07 16:59:43.940098: step 13590, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.529 sec/batch; 87h:29m:08s remains)
INFO - root - 2017-12-07 17:00:29.462166: step 13600, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.563 sec/batch; 88h:07m:00s remains)
2017-12-07 17:00:32.165887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5818915 -2.5723491 -2.7557986 -2.7338271 -2.5587127 -2.5201206 -2.7075186 -2.7460284 -2.52011 -2.1989856 -2.0220354 -2.2657452 -2.6624558 -2.8568759 -2.5507398][-2.4099183 -2.3977644 -2.5925384 -2.5359278 -2.2609119 -2.0177977 -2.0174909 -1.9682083 -1.7361722 -1.5421183 -1.5491216 -1.9290555 -2.3686838 -2.4849827 -2.1647084][-2.1662827 -2.1699591 -2.3972695 -2.3912179 -2.1181965 -1.7129822 -1.3607225 -0.97532892 -0.61910033 -0.58907557 -0.83018327 -1.3467937 -1.8739543 -2.0718598 -1.8996634][-1.9611027 -2.00075 -2.2683969 -2.389473 -2.2499535 -1.7834964 -1.0988996 -0.3682971 0.040006638 -0.15567112 -0.60188222 -1.1100745 -1.5810592 -1.8348722 -1.7777495][-1.6400466 -1.6871123 -1.9808738 -2.2486877 -2.2147391 -1.6949918 -0.76471353 0.15121984 0.36524916 -0.18607235 -0.83775759 -1.2640536 -1.5788147 -1.7601433 -1.6301563][-1.440053 -1.3503747 -1.5819411 -1.8824344 -1.6730454 -0.81030059 0.4887104 1.4363532 1.1359572 -0.018410206 -1.0534189 -1.5666842 -1.8029289 -1.7854159 -1.3923497][-1.3297653 -1.0864382 -1.215239 -1.3696697 -0.72772169 0.71464539 2.5199628 3.4898591 2.5722489 0.75523567 -0.67184782 -1.3287284 -1.5680292 -1.4789493 -1.0026066][-0.80256844 -0.57234883 -0.69948173 -0.68382287 0.2395649 1.9162002 3.8620014 4.7300358 3.3992381 1.3436179 -0.12705517 -0.67888904 -0.80083704 -0.71636081 -0.32284403][-0.40040922 -0.3601141 -0.615911 -0.52977204 0.39365721 1.8005195 3.2309484 3.637208 2.4006143 0.71513796 -0.39858723 -0.66629171 -0.59873366 -0.47544456 -0.15244436][-0.33754206 -0.5454607 -1.0375252 -1.0512624 -0.3336544 0.58609962 1.3522582 1.3682575 0.47687626 -0.59814715 -1.3029299 -1.3934534 -1.2311699 -1.0302994 -0.72439337][-0.87979126 -1.1255627 -1.6712248 -1.8129659 -1.3688943 -0.83931446 -0.52088022 -0.69103932 -1.2511122 -1.7844007 -2.1798918 -2.2109339 -2.0452559 -1.8170974 -1.527844][-1.8087859 -1.9390883 -2.3924992 -2.6802411 -2.5503008 -2.2894504 -2.1547644 -2.2768366 -2.4994576 -2.6104829 -2.7470922 -2.779294 -2.6807666 -2.4789729 -2.1944821][-2.5026817 -2.5314305 -2.9115791 -3.3553381 -3.4870474 -3.3657317 -3.2206597 -3.1760855 -3.1177649 -2.9783039 -2.9839356 -3.0603411 -3.063225 -2.9275303 -2.6395702][-2.4830983 -2.5310798 -3.0616708 -3.7605443 -4.1150331 -4.0259562 -3.7054977 -3.3875346 -3.1403122 -2.9895639 -3.0650609 -3.2564306 -3.3589807 -3.3080475 -3.0557146][-2.1523554 -2.2393684 -2.8806977 -3.7614689 -4.2808514 -4.2281423 -3.7298946 -3.1877286 -2.8675511 -2.837878 -3.0459895 -3.3369551 -3.5267262 -3.5551887 -3.3182056]]...]
INFO - root - 2017-12-07 17:01:17.750815: step 13610, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.577 sec/batch; 88h:23m:18s remains)
INFO - root - 2017-12-07 17:02:03.710919: step 13620, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 4.633 sec/batch; 89h:26m:42s remains)
INFO - root - 2017-12-07 17:02:49.643165: step 13630, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.512 sec/batch; 87h:06m:33s remains)
INFO - root - 2017-12-07 17:03:35.113750: step 13640, loss = 0.80, batch loss = 0.72 (7.0 examples/sec; 4.540 sec/batch; 87h:37m:35s remains)
INFO - root - 2017-12-07 17:04:21.000180: step 13650, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.606 sec/batch; 88h:52m:49s remains)
INFO - root - 2017-12-07 17:05:06.745941: step 13660, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.533 sec/batch; 87h:28m:03s remains)
INFO - root - 2017-12-07 17:05:52.357569: step 13670, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.547 sec/batch; 87h:44m:04s remains)
INFO - root - 2017-12-07 17:06:37.874623: step 13680, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.583 sec/batch; 88h:24m:50s remains)
INFO - root - 2017-12-07 17:07:23.426588: step 13690, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.539 sec/batch; 87h:32m:28s remains)
INFO - root - 2017-12-07 17:08:09.359302: step 13700, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.580 sec/batch; 88h:19m:11s remains)
2017-12-07 17:08:12.246564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8283746 -1.8169575 -1.8086028 -1.8051405 -1.821013 -1.8403363 -1.8457489 -1.8392894 -1.8199944 -1.7904859 -1.7767749 -1.7887614 -1.8222547 -1.8538284 -1.8626206][-1.754667 -1.7265649 -1.7252986 -1.7425554 -1.7788274 -1.815613 -1.829205 -1.8278825 -1.8192575 -1.8026199 -1.7904727 -1.8075235 -1.8446662 -1.87601 -1.8780727][-1.6748586 -1.5860436 -1.5402164 -1.5423069 -1.5920453 -1.6720028 -1.7471838 -1.8135622 -1.8576672 -1.8495421 -1.7954249 -1.7759411 -1.8097425 -1.8745494 -1.9171889][-1.5891278 -1.415967 -1.2710028 -1.2037833 -1.2386155 -1.3549244 -1.5192323 -1.7138813 -1.869807 -1.8937902 -1.7891243 -1.6964395 -1.7002046 -1.7931306 -1.885781][-1.4410622 -1.2261732 -1.0001955 -0.83648157 -0.79386377 -0.8488853 -1.0103478 -1.3146548 -1.6228063 -1.764782 -1.705447 -1.5793874 -1.5202849 -1.5785272 -1.6680226][-1.246269 -1.069469 -0.80378103 -0.50757027 -0.28771019 -0.17814636 -0.26516914 -0.64974761 -1.1082766 -1.3972619 -1.4480116 -1.3530464 -1.2444522 -1.2292645 -1.2583587][-1.1103699 -1.0217493 -0.73960853 -0.28923321 0.17561913 0.52104425 0.56801367 0.13015604 -0.48421359 -0.92715931 -1.117552 -1.0888703 -0.92585468 -0.7837708 -0.69664764][-1.034431 -0.97579646 -0.63710904 -0.049660206 0.58956861 1.1324472 1.3468657 0.938293 0.20092344 -0.46833658 -0.9140892 -1.0540857 -0.86877131 -0.52983713 -0.20515203][-0.85242605 -0.66346717 -0.19803619 0.41028214 0.91741276 1.2760749 1.4503651 1.1682725 0.53027916 -0.19138479 -0.81102943 -1.1159251 -0.94529772 -0.450392 0.15087986][-0.38696241 -0.052582264 0.40023088 0.78264332 0.86922646 0.75710821 0.64723015 0.45117569 0.13470125 -0.29774523 -0.72440696 -0.90024519 -0.62778163 -0.078897476 0.53808022][0.1530385 0.37692165 0.5120616 0.44783497 0.12499142 -0.27269793 -0.49947333 -0.53526092 -0.48225307 -0.46751022 -0.4446857 -0.25173903 0.18398094 0.61099243 0.8745656][0.11271 0.084865093 -0.058914185 -0.3624053 -0.78481483 -1.1558912 -1.2634559 -1.0428188 -0.64116955 -0.27433109 0.050601006 0.39346504 0.73782015 0.84679079 0.64532423][-0.59797311 -0.72205925 -0.83290315 -1.0147147 -1.2192061 -1.365828 -1.3024178 -0.94350958 -0.45837164 -0.08204174 0.16877222 0.33145475 0.43092489 0.28783178 -0.12637091][-1.3864565 -1.42928 -1.3492217 -1.2590044 -1.1466515 -1.073844 -0.95877171 -0.68696737 -0.41036367 -0.27753448 -0.25587893 -0.27752113 -0.27138376 -0.38634968 -0.68707156][-1.764267 -1.6490612 -1.3834848 -1.1117873 -0.86610413 -0.7972374 -0.83832979 -0.80083728 -0.7560885 -0.74625373 -0.73802304 -0.72985411 -0.66881609 -0.66972971 -0.79347992]]...]
INFO - root - 2017-12-07 17:08:57.822925: step 13710, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.597 sec/batch; 88h:38m:02s remains)
INFO - root - 2017-12-07 17:09:43.570460: step 13720, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 4.530 sec/batch; 87h:20m:38s remains)
INFO - root - 2017-12-07 17:10:29.104224: step 13730, loss = 0.81, batch loss = 0.74 (6.9 examples/sec; 4.610 sec/batch; 88h:51m:23s remains)
INFO - root - 2017-12-07 17:11:14.872054: step 13740, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.580 sec/batch; 88h:16m:26s remains)
INFO - root - 2017-12-07 17:12:00.462691: step 13750, loss = 0.80, batch loss = 0.73 (6.9 examples/sec; 4.618 sec/batch; 88h:58m:59s remains)
INFO - root - 2017-12-07 17:12:46.184127: step 13760, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.520 sec/batch; 87h:05m:49s remains)
INFO - root - 2017-12-07 17:13:31.639810: step 13770, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.568 sec/batch; 88h:00m:41s remains)
INFO - root - 2017-12-07 17:14:17.391317: step 13780, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.549 sec/batch; 87h:37m:45s remains)
INFO - root - 2017-12-07 17:15:02.864796: step 13790, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.570 sec/batch; 88h:01m:16s remains)
INFO - root - 2017-12-07 17:15:48.570527: step 13800, loss = 0.73, batch loss = 0.65 (7.0 examples/sec; 4.553 sec/batch; 87h:40m:27s remains)
2017-12-07 17:15:51.259254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1615272 -4.0842614 -3.8792009 -3.6376424 -3.5077548 -3.4626472 -3.3494861 -3.1882579 -3.0936048 -2.9623044 -2.7988329 -2.6931109 -2.5410032 -2.2353232 -1.9223306][-4.2417765 -4.3111625 -4.3252807 -4.2858863 -4.2692041 -4.166163 -3.8576138 -3.5320213 -3.3867469 -3.214618 -3.0334713 -2.9979568 -2.9584718 -2.7483473 -2.4645414][-4.2911854 -4.431169 -4.5373464 -4.5439544 -4.4930644 -4.250237 -3.7067819 -3.177289 -2.984771 -2.8686297 -2.822257 -3.02635 -3.2316689 -3.2113922 -3.0151527][-4.4700222 -4.6358976 -4.7335491 -4.6883879 -4.5088534 -4.0415974 -3.1948166 -2.41683 -2.1513743 -2.109719 -2.2730918 -2.8216417 -3.3743207 -3.5995758 -3.4947457][-4.6054974 -4.8379865 -4.93022 -4.8308268 -4.5169396 -3.8204916 -2.7075775 -1.7454545 -1.4142375 -1.4013984 -1.6672728 -2.4420877 -3.280297 -3.7467842 -3.7565084][-4.5239716 -4.8585525 -4.9564424 -4.7728081 -4.3004656 -3.4318185 -2.1892087 -1.1820273 -0.8828547 -0.95834494 -1.2576847 -2.0803471 -3.0315075 -3.6422205 -3.7592406][-4.2016048 -4.5999608 -4.7192774 -4.4820719 -3.8921597 -2.9153605 -1.6228836 -0.60885549 -0.38941669 -0.7055347 -1.1701021 -2.0121405 -2.9624074 -3.6084137 -3.7926471][-3.8098984 -4.2616096 -4.4458 -4.1959634 -3.480478 -2.36324 -0.99045038 0.073817253 0.2286129 -0.36784697 -1.0877428 -1.9769669 -2.9277956 -3.6195979 -3.9070733][-3.5894957 -4.0002913 -4.2190423 -3.9799013 -3.1583571 -1.9747162 -0.65001822 0.38204765 0.52508545 -0.15586615 -0.93546128 -1.7401726 -2.6341472 -3.3817453 -3.8223047][-3.5844183 -3.8052325 -3.9914303 -3.8134346 -3.0936952 -2.0999193 -1.0471947 -0.10039139 0.24104595 -0.16178417 -0.70321536 -1.3017094 -2.0673909 -2.8124452 -3.363502][-3.7096596 -3.6800776 -3.750129 -3.5711863 -2.9774857 -2.2916653 -1.6219792 -0.87497187 -0.37850714 -0.41374588 -0.63915253 -1.0128284 -1.592468 -2.2478929 -2.8042374][-3.4748268 -3.2332838 -3.2000389 -2.9894385 -2.4879827 -2.0964768 -1.8536506 -1.5325518 -1.1907454 -1.0586493 -1.0188389 -1.1146226 -1.4135735 -1.9140968 -2.4134235][-2.7894187 -2.4194076 -2.3765886 -2.2598143 -1.9622121 -1.84108 -1.9102495 -1.9308479 -1.7718079 -1.5580819 -1.3443584 -1.230983 -1.2955813 -1.6530941 -2.148478][-2.035 -1.6016824 -1.5828083 -1.61885 -1.5530515 -1.5892196 -1.7065096 -1.7854831 -1.645596 -1.4161487 -1.2308395 -1.1122835 -1.0739355 -1.3116205 -1.8009818][-1.6328971 -1.1617041 -1.1502118 -1.3009038 -1.4019835 -1.4879115 -1.5418377 -1.5498509 -1.3879633 -1.2238898 -1.1020968 -0.95483017 -0.80605435 -0.9027288 -1.3581724]]...]
INFO - root - 2017-12-07 17:16:36.913374: step 13810, loss = 0.78, batch loss = 0.70 (7.0 examples/sec; 4.580 sec/batch; 88h:10m:44s remains)
INFO - root - 2017-12-07 17:17:22.654102: step 13820, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.548 sec/batch; 87h:33m:41s remains)
INFO - root - 2017-12-07 17:18:08.060780: step 13830, loss = 0.82, batch loss = 0.75 (7.4 examples/sec; 4.326 sec/batch; 83h:16m:32s remains)
INFO - root - 2017-12-07 17:18:53.730613: step 13840, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.546 sec/batch; 87h:29m:05s remains)
INFO - root - 2017-12-07 17:19:39.695937: step 13850, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.608 sec/batch; 88h:40m:39s remains)
INFO - root - 2017-12-07 17:20:24.719170: step 13860, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.545 sec/batch; 87h:26m:33s remains)
INFO - root - 2017-12-07 17:21:10.427989: step 13870, loss = 0.66, batch loss = 0.59 (6.9 examples/sec; 4.619 sec/batch; 88h:51m:46s remains)
INFO - root - 2017-12-07 17:21:56.040499: step 13880, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.588 sec/batch; 88h:15m:22s remains)
INFO - root - 2017-12-07 17:22:41.526170: step 13890, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.571 sec/batch; 87h:54m:41s remains)
INFO - root - 2017-12-07 17:23:27.468085: step 13900, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.624 sec/batch; 88h:55m:13s remains)
2017-12-07 17:23:30.113092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1977134 -5.1814508 -5.176888 -5.1741815 -5.1733007 -5.1522856 -5.0898867 -5.016839 -4.9843512 -5.0039396 -5.031899 -5.0437326 -5.0551534 -5.0644145 -5.087739][-5.1842766 -5.1372437 -5.087636 -5.0213389 -4.9579358 -4.8940439 -4.8006167 -4.7102356 -4.6803641 -4.7205195 -4.7899947 -4.8693595 -4.9529362 -5.010654 -5.0589638][-5.0913582 -4.9925165 -4.8846135 -4.7577152 -4.6448817 -4.5435166 -4.42639 -4.3174715 -4.2677393 -4.2821593 -4.3494406 -4.4657598 -4.6198735 -4.759573 -4.8919792][-4.8215823 -4.7378345 -4.6449184 -4.5417008 -4.4590297 -4.3605375 -4.2063656 -4.0186615 -3.8879611 -3.8587518 -3.9519849 -4.12073 -4.3223748 -4.4833217 -4.6217737][-4.227458 -4.1987281 -4.1923923 -4.1850796 -4.177834 -4.1440325 -4.0402284 -3.8680575 -3.7353783 -3.7272453 -3.8912024 -4.0841327 -4.2356291 -4.2952023 -4.321465][-3.7021024 -3.6759949 -3.7062438 -3.729737 -3.7108045 -3.6741159 -3.587158 -3.4417377 -3.3597832 -3.4486585 -3.7176967 -3.9290271 -4.0199347 -3.9909649 -3.9204373][-3.1606822 -3.1193175 -3.1957867 -3.2888741 -3.3305879 -3.3566942 -3.2583556 -2.9991136 -2.7915983 -2.829154 -3.1201227 -3.3609829 -3.4715853 -3.4807968 -3.4308386][-2.8603439 -2.7912364 -2.8758788 -3.0179236 -3.1638384 -3.3314662 -3.3278341 -3.09629 -2.882689 -2.8925281 -3.0869226 -3.1734667 -3.1032553 -2.9735608 -2.8414235][-3.1150331 -3.0449073 -3.0853114 -3.1574693 -3.251771 -3.3805737 -3.4003572 -3.3023458 -3.2959645 -3.4967079 -3.7809789 -3.8500035 -3.6240249 -3.2567065 -2.8721962][-3.2048478 -3.1292787 -3.0981455 -3.0822306 -3.1201491 -3.1880522 -3.1592686 -3.0736508 -3.1287935 -3.4208307 -3.8422036 -4.1279473 -4.1220703 -3.9259443 -3.6163402][-3.1668944 -3.0717871 -2.9809737 -2.9332581 -2.9669123 -3.0221648 -2.9686308 -2.8444042 -2.8263874 -2.9907341 -3.2937164 -3.5661998 -3.6609745 -3.6416862 -3.520803][-3.2637591 -3.1566684 -3.0862355 -3.1118767 -3.2214322 -3.3440971 -3.3709817 -3.3245811 -3.3437746 -3.4430332 -3.586493 -3.6539035 -3.5645156 -3.4156754 -3.2491336][-3.217669 -3.0694079 -2.9621415 -2.9650822 -3.0588338 -3.1896341 -3.2687054 -3.333312 -3.4951746 -3.714684 -3.9123173 -3.9993536 -3.9370348 -3.8214426 -3.7008467][-3.1006622 -2.9443212 -2.7919958 -2.7150426 -2.6959348 -2.7002542 -2.6891627 -2.7297971 -2.9133661 -3.1792331 -3.4271529 -3.5962713 -3.6698234 -3.6950173 -3.6870298][-3.0200253 -2.924237 -2.8539639 -2.858573 -2.8919373 -2.8925705 -2.8534906 -2.8516874 -2.984679 -3.2016335 -3.3899369 -3.4920568 -3.5072424 -3.483577 -3.4426079]]...]
INFO - root - 2017-12-07 17:24:15.903588: step 13910, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.540 sec/batch; 87h:17m:21s remains)
INFO - root - 2017-12-07 17:25:01.301974: step 13920, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.588 sec/batch; 88h:11m:58s remains)
INFO - root - 2017-12-07 17:25:46.951986: step 13930, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.513 sec/batch; 86h:45m:06s remains)
INFO - root - 2017-12-07 17:26:32.654624: step 13940, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.578 sec/batch; 87h:59m:01s remains)
INFO - root - 2017-12-07 17:27:18.590727: step 13950, loss = 0.78, batch loss = 0.70 (6.9 examples/sec; 4.634 sec/batch; 89h:02m:47s remains)
INFO - root - 2017-12-07 17:28:03.977890: step 13960, loss = 0.82, batch loss = 0.75 (7.1 examples/sec; 4.513 sec/batch; 86h:42m:20s remains)
INFO - root - 2017-12-07 17:28:50.023346: step 13970, loss = 0.85, batch loss = 0.78 (6.8 examples/sec; 4.677 sec/batch; 89h:50m:23s remains)
INFO - root - 2017-12-07 17:29:35.563138: step 13980, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.494 sec/batch; 86h:19m:01s remains)
INFO - root - 2017-12-07 17:30:21.219920: step 13990, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.548 sec/batch; 87h:19m:52s remains)
INFO - root - 2017-12-07 17:31:06.905343: step 14000, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.578 sec/batch; 87h:53m:42s remains)
2017-12-07 17:31:09.622200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3304873 -3.3253548 -3.0765905 -2.7986543 -2.6648598 -2.5250401 -2.56532 -2.7551541 -2.9242747 -2.9928112 -2.9299941 -2.7128644 -2.7050364 -2.7596712 -2.6234994][-3.432672 -3.4229307 -3.292145 -3.1655426 -3.1474788 -3.1176372 -3.1915443 -3.308857 -3.3667107 -3.30942 -3.2011724 -2.9789705 -2.9088297 -2.9376507 -2.8787658][-3.2679896 -3.2607751 -3.2365708 -3.2700796 -3.391511 -3.4759169 -3.527617 -3.464149 -3.2941418 -3.0527472 -2.9191327 -2.7805543 -2.7498465 -2.8230524 -2.8710165][-2.7760644 -2.7342987 -2.750694 -2.8776119 -3.1177192 -3.3527234 -3.44365 -3.27874 -2.9826875 -2.7162046 -2.672585 -2.6616354 -2.6761727 -2.7775211 -2.9232557][-2.600379 -2.5239062 -2.5233622 -2.6389689 -2.858521 -3.1039522 -3.1239753 -2.8550255 -2.5922606 -2.523284 -2.6563239 -2.7308066 -2.6923006 -2.7051144 -2.8301249][-2.1843517 -2.2271013 -2.3367324 -2.5257626 -2.7055151 -2.8100069 -2.540019 -2.0371208 -1.8902833 -2.142132 -2.4752154 -2.5700636 -2.4166653 -2.282738 -2.3035076][-1.429682 -1.5941818 -1.7999125 -2.0713041 -2.1738923 -2.0041173 -1.3347456 -0.58627367 -0.63328791 -1.2264576 -1.7908337 -1.9479125 -1.7493567 -1.5092649 -1.4367366][-0.93128777 -1.1378322 -1.3179259 -1.5445588 -1.4821606 -0.95324731 0.031319618 0.84998274 0.57687378 -0.26125813 -0.97669196 -1.2165973 -1.0588572 -0.79372382 -0.60461211][-0.66931272 -0.8843596 -1.0027523 -1.1920187 -1.1271238 -0.5446136 0.31579161 0.92064428 0.58885241 -0.12262201 -0.68611312 -0.84311223 -0.71353936 -0.55427909 -0.39452457][-1.0139976 -1.1547179 -1.1422439 -1.2436614 -1.2518189 -0.90306568 -0.49241042 -0.28234577 -0.595387 -1.0019066 -1.2081246 -1.1601593 -1.0493793 -1.0709915 -1.1124215][-1.9346492 -1.9510481 -1.7917297 -1.7489636 -1.7469802 -1.5742102 -1.4831128 -1.5300252 -1.8265979 -2.0444498 -2.0641482 -1.9573572 -1.8803988 -1.9608824 -2.0896676][-2.6292903 -2.57681 -2.4318848 -2.427321 -2.5544426 -2.5910974 -2.651449 -2.7400784 -2.9311717 -3.002018 -2.9488943 -2.8860738 -2.8391063 -2.8755565 -2.9827285][-3.239337 -3.1688747 -3.1080525 -3.1960797 -3.4337754 -3.5830255 -3.6650991 -3.6816213 -3.7387249 -3.7072906 -3.6303303 -3.595835 -3.5476756 -3.5594516 -3.6791055][-3.9602563 -3.9130387 -3.9012172 -3.9685464 -4.1411605 -4.2433915 -4.249712 -4.164196 -4.1262388 -4.069468 -3.9838312 -3.8905904 -3.8090177 -3.8821404 -4.109992][-4.0316372 -4.0885468 -4.116786 -4.1552024 -4.2821584 -4.3941412 -4.4454713 -4.4255619 -4.4378233 -4.3946033 -4.1761746 -3.8482528 -3.6419692 -3.7787604 -4.1484456]]...]
INFO - root - 2017-12-07 17:31:55.425790: step 14010, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.545 sec/batch; 87h:14m:54s remains)
INFO - root - 2017-12-07 17:32:40.887960: step 14020, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.593 sec/batch; 88h:09m:50s remains)
INFO - root - 2017-12-07 17:33:26.482817: step 14030, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.540 sec/batch; 87h:08m:28s remains)
INFO - root - 2017-12-07 17:34:11.999484: step 14040, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.604 sec/batch; 88h:20m:56s remains)
INFO - root - 2017-12-07 17:34:57.841715: step 14050, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.577 sec/batch; 87h:49m:15s remains)
INFO - root - 2017-12-07 17:35:43.445172: step 14060, loss = 0.77, batch loss = 0.69 (7.0 examples/sec; 4.566 sec/batch; 87h:35m:24s remains)
INFO - root - 2017-12-07 17:36:28.841517: step 14070, loss = 0.80, batch loss = 0.73 (6.9 examples/sec; 4.612 sec/batch; 88h:27m:49s remains)
INFO - root - 2017-12-07 17:37:14.538007: step 14080, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.555 sec/batch; 87h:21m:53s remains)
INFO - root - 2017-12-07 17:38:00.091479: step 14090, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.566 sec/batch; 87h:33m:55s remains)
INFO - root - 2017-12-07 17:38:45.779871: step 14100, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.479 sec/batch; 85h:53m:02s remains)
2017-12-07 17:38:48.476031: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2915807 -0.96743679 -1.1278532 -1.3773084 -1.626538 -1.9536772 -2.32503 -2.3815012 -2.2182939 -1.630569 -0.55438256 0.40823889 0.49559975 -0.10028124 -0.73519516][-1.4261696 -1.3235264 -1.4806576 -1.4833322 -1.4295545 -1.4682133 -1.648243 -1.6838009 -1.644388 -1.2359421 -0.32640934 0.61047173 0.82943916 0.36328125 -0.19854259][-1.7185593 -1.8251722 -1.9624803 -1.7626085 -1.3563328 -1.0645783 -1.0986218 -1.1319737 -1.1139348 -0.8420763 -0.26310253 0.34278297 0.54287338 0.21298027 -0.25600529][-1.6592498 -1.9302416 -2.1094368 -1.9388025 -1.4627082 -1.0383449 -1.0216346 -1.0030646 -0.88762951 -0.66742492 -0.43044853 -0.30734396 -0.21783638 -0.45132875 -0.85190821][-1.1199775 -1.5492976 -1.8129032 -1.8478489 -1.532841 -1.1557434 -1.0224428 -0.77538753 -0.56245112 -0.45554256 -0.59629679 -0.8894918 -0.82517791 -0.8412087 -1.0596356][-0.70352244 -1.2132659 -1.5151494 -1.6618962 -1.3626888 -0.81021309 -0.26174355 0.30306911 0.32354069 0.043963432 -0.51583362 -1.120913 -1.0557382 -0.7975409 -0.67998981][-0.8921032 -1.2934592 -1.4420414 -1.4644637 -0.9638288 -0.18206501 0.77834749 1.5268712 1.116652 0.44126034 -0.32096243 -1.0339913 -0.98030829 -0.55387044 -0.16782379][-1.3926868 -1.642452 -1.6234245 -1.4726684 -0.88273048 -0.16492033 0.79485512 1.4066062 0.79182816 0.20232439 -0.37480259 -0.96186233 -0.85442996 -0.36509562 0.096591473][-1.5210345 -1.7667623 -1.8201249 -1.7094214 -1.2927804 -0.9130764 -0.25398302 0.12899113 -0.31342459 -0.48088694 -0.71577096 -1.085794 -0.90837955 -0.528178 -0.22162676][-1.2973433 -1.5806332 -1.777467 -1.7741375 -1.5745738 -1.5283723 -1.1877773 -1.046577 -1.3083737 -1.145699 -1.1868303 -1.390914 -1.1493552 -0.9453361 -0.86163831][-1.1223016 -1.4216394 -1.6443508 -1.6266594 -1.5039661 -1.5746062 -1.4369392 -1.5060017 -1.7043905 -1.4962499 -1.5782628 -1.6786935 -1.3682497 -1.2483969 -1.2362068][-1.3357346 -1.6506312 -1.812968 -1.6961913 -1.5631874 -1.5886006 -1.521739 -1.6807609 -1.8175223 -1.6779294 -1.8379927 -1.8417301 -1.4698446 -1.3557963 -1.3102415][-1.7263958 -2.0137591 -2.132998 -2.0105803 -1.9246757 -1.8962865 -1.8663149 -2.021244 -2.06272 -1.9542844 -2.0662637 -1.9511468 -1.6298742 -1.5833881 -1.5276256][-1.9229486 -2.1019328 -2.2057171 -2.1620972 -2.1867981 -2.1859519 -2.1955705 -2.3262293 -2.2986958 -2.1911619 -2.2104461 -2.0410502 -1.8582411 -1.9176917 -1.8878374][-2.009907 -2.0411572 -2.0985813 -2.1436489 -2.265285 -2.3165398 -2.3602357 -2.4642997 -2.4060154 -2.2956138 -2.2627308 -2.1201818 -2.0915732 -2.221344 -2.1999874]]...]
INFO - root - 2017-12-07 17:39:33.923234: step 14110, loss = 0.81, batch loss = 0.73 (7.0 examples/sec; 4.580 sec/batch; 87h:48m:26s remains)
INFO - root - 2017-12-07 17:40:19.421055: step 14120, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.486 sec/batch; 85h:59m:21s remains)
INFO - root - 2017-12-07 17:41:04.714467: step 14130, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.581 sec/batch; 87h:48m:15s remains)
INFO - root - 2017-12-07 17:41:50.026407: step 14140, loss = 0.83, batch loss = 0.76 (6.9 examples/sec; 4.614 sec/batch; 88h:24m:26s remains)
INFO - root - 2017-12-07 17:42:35.544940: step 14150, loss = 0.66, batch loss = 0.58 (7.0 examples/sec; 4.549 sec/batch; 87h:09m:15s remains)
INFO - root - 2017-12-07 17:43:21.250935: step 14160, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.580 sec/batch; 87h:44m:16s remains)
INFO - root - 2017-12-07 17:44:06.601867: step 14170, loss = 0.77, batch loss = 0.70 (7.3 examples/sec; 4.394 sec/batch; 84h:09m:38s remains)
INFO - root - 2017-12-07 17:44:52.182071: step 14180, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.570 sec/batch; 87h:31m:50s remains)
INFO - root - 2017-12-07 17:45:37.880729: step 14190, loss = 0.69, batch loss = 0.61 (7.0 examples/sec; 4.600 sec/batch; 88h:05m:02s remains)
INFO - root - 2017-12-07 17:46:23.594105: step 14200, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.567 sec/batch; 87h:26m:09s remains)
2017-12-07 17:46:26.288669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0271821 -3.2662177 -3.492842 -3.4266188 -3.4444273 -3.564676 -3.36774 -3.0608244 -3.0425892 -3.1414719 -3.0025725 -2.7763903 -2.8141837 -2.9579997 -2.95855][-3.0063789 -3.3266025 -3.6006279 -3.6053002 -3.7192998 -3.8584931 -3.6488452 -3.3957295 -3.3446736 -3.3086514 -3.1233065 -2.982141 -3.1574898 -3.2930832 -3.1063366][-2.8078892 -3.1830678 -3.4798687 -3.5660155 -3.8429091 -4.0460091 -3.8745704 -3.7112041 -3.6871047 -3.581985 -3.41064 -3.3466978 -3.5468793 -3.614805 -3.3406563][-2.3971574 -2.7336912 -2.9967251 -3.13521 -3.5499427 -3.8249478 -3.7340586 -3.6657443 -3.7047515 -3.6505098 -3.5933352 -3.6260772 -3.8140128 -3.8476644 -3.6017234][-1.8948255 -2.133796 -2.3169851 -2.4472518 -2.8950446 -3.17409 -3.1126409 -3.0628974 -3.18391 -3.3234096 -3.4362245 -3.5549116 -3.7634103 -3.8283408 -3.6416469][-1.713058 -1.8878014 -1.976088 -1.9685824 -2.2098215 -2.2494121 -1.9355843 -1.6955135 -1.9328876 -2.4165041 -2.7369075 -2.9225378 -3.1962972 -3.3555012 -3.2261603][-1.6524639 -1.824049 -1.8902929 -1.8166642 -1.769129 -1.2884367 -0.37666273 0.17486095 -0.30629492 -1.2551007 -1.8097932 -2.06541 -2.4179587 -2.6471696 -2.5150537][-1.3634818 -1.4713941 -1.5433965 -1.6008773 -1.4503036 -0.54968905 0.81872845 1.4789095 0.70227814 -0.49837422 -1.080653 -1.3125436 -1.6367817 -1.7970271 -1.6063135][-1.290406 -1.2720282 -1.2779708 -1.4350667 -1.3619485 -0.48218083 0.71759462 1.0825214 0.30112171 -0.57725191 -0.8368063 -0.91164875 -1.1031528 -1.1266248 -0.95991421][-1.8029611 -1.6285334 -1.445955 -1.5710559 -1.6264699 -1.0874746 -0.40909576 -0.34356356 -0.86432028 -1.2526717 -1.2308547 -1.1955817 -1.2284524 -1.1288247 -1.0651882][-2.4203088 -2.1919692 -1.9196849 -1.9722424 -2.1184704 -1.9622576 -1.7798135 -1.8920677 -2.1770225 -2.2784925 -2.1677473 -2.0810707 -1.9602859 -1.7673199 -1.7798102][-2.9180031 -2.7649233 -2.599967 -2.6952634 -2.8880424 -2.9358513 -2.9890909 -3.1604548 -3.32157 -3.3156648 -3.177876 -3.038322 -2.836257 -2.6419616 -2.6897941][-3.4548583 -3.3874371 -3.3794513 -3.5587029 -3.7706511 -3.8596349 -3.9138305 -4.011632 -4.06946 -4.0153913 -3.8658011 -3.6932316 -3.501338 -3.3694849 -3.4453845][-3.9080453 -3.8443928 -3.8683119 -4.0261755 -4.1817541 -4.2512751 -4.2735991 -4.3090839 -4.3270073 -4.282433 -4.1688938 -4.0125723 -3.8493505 -3.7523983 -3.7991881][-4.0507894 -3.9722388 -3.944418 -3.9916105 -4.0487189 -4.0787516 -4.093163 -4.1214824 -4.1500497 -4.1423445 -4.0860119 -3.9881189 -3.875694 -3.7982881 -3.7938504]]...]
INFO - root - 2017-12-07 17:47:11.858324: step 14210, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.575 sec/batch; 87h:34m:52s remains)
INFO - root - 2017-12-07 17:47:57.941629: step 14220, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.548 sec/batch; 87h:02m:31s remains)
INFO - root - 2017-12-07 17:48:43.633980: step 14230, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.539 sec/batch; 86h:52m:00s remains)
INFO - root - 2017-12-07 17:49:29.254323: step 14240, loss = 0.79, batch loss = 0.71 (7.0 examples/sec; 4.539 sec/batch; 86h:51m:42s remains)
INFO - root - 2017-12-07 17:50:14.826773: step 14250, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.649 sec/batch; 88h:57m:07s remains)
INFO - root - 2017-12-07 17:51:00.467196: step 14260, loss = 0.69, batch loss = 0.62 (7.1 examples/sec; 4.524 sec/batch; 86h:32m:20s remains)
INFO - root - 2017-12-07 17:51:46.435984: step 14270, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.594 sec/batch; 87h:51m:58s remains)
INFO - root - 2017-12-07 17:52:32.021654: step 14280, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.670 sec/batch; 89h:18m:41s remains)
INFO - root - 2017-12-07 17:53:17.512885: step 14290, loss = 0.72, batch loss = 0.64 (7.0 examples/sec; 4.560 sec/batch; 87h:10m:58s remains)
INFO - root - 2017-12-07 17:54:02.931235: step 14300, loss = 0.85, batch loss = 0.77 (7.0 examples/sec; 4.555 sec/batch; 87h:04m:30s remains)
2017-12-07 17:54:05.665598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2935367 -3.2860632 -3.2947619 -3.3211012 -3.3559589 -3.3765848 -3.3789639 -3.3711958 -3.3587146 -3.3464117 -3.3333554 -3.3175964 -3.2959819 -3.2770834 -3.2700219][-3.3178854 -3.3204331 -3.3735871 -3.4776249 -3.5906684 -3.6448193 -3.6330009 -3.5882785 -3.5394394 -3.4989197 -3.4650545 -3.4295099 -3.3821416 -3.3303752 -3.2935274][-3.2721019 -3.279304 -3.4041815 -3.6234529 -3.8562775 -3.998702 -4.0424037 -4.0278068 -3.9777172 -3.8999112 -3.7928891 -3.6705325 -3.5476394 -3.4342771 -3.3497038][-3.0744591 -3.0912256 -3.3328848 -3.6923909 -4.02935 -4.2478514 -4.3903284 -4.4802322 -4.4900002 -4.4199815 -4.2553487 -4.02486 -3.7948737 -3.5931907 -3.438961][-2.7598467 -2.7289891 -3.0647683 -3.5593369 -3.9873509 -4.2334337 -4.4009314 -4.5355892 -4.5979524 -4.621026 -4.5602069 -4.3550024 -4.0773792 -3.789413 -3.5413175][-2.4231439 -2.1986268 -2.4174008 -2.8957767 -3.3824952 -3.706064 -3.952703 -4.1721115 -4.2836375 -4.3768754 -4.459219 -4.37362 -4.1552234 -3.8760548 -3.5847359][-2.200237 -1.7258322 -1.5883753 -1.7509577 -2.0655408 -2.3752244 -2.7149844 -3.1058226 -3.3727536 -3.5848997 -3.8083768 -3.8912868 -3.8548255 -3.7611432 -3.5572741][-2.2720659 -1.6533341 -1.1235092 -0.79537654 -0.73080659 -0.80575728 -1.0172341 -1.3794079 -1.7815115 -2.2415771 -2.7371104 -3.1059742 -3.3618059 -3.5433178 -3.5011234][-2.6401234 -2.0529392 -1.2579312 -0.49524426 -0.11436129 -0.031305313 -0.085492611 -0.19959593 -0.46379161 -0.95480728 -1.6087806 -2.2555223 -2.8230474 -3.2648506 -3.3852057][-3.07025 -2.7366562 -2.0281868 -1.1805756 -0.67990446 -0.52737331 -0.4398284 -0.30098534 -0.31462526 -0.58068514 -1.0939436 -1.7426527 -2.42058 -3.0023551 -3.2586145][-3.2893071 -3.2073324 -2.8091097 -2.2378418 -1.884201 -1.7627316 -1.5642693 -1.2409544 -1.0483723 -1.0781207 -1.3747215 -1.8374832 -2.3804944 -2.9005504 -3.2069473][-3.3686609 -3.389935 -3.2388906 -2.9967556 -2.886698 -2.8522801 -2.6501834 -2.2706511 -1.9380608 -1.7659416 -1.8468771 -2.1188123 -2.4978547 -2.9027522 -3.1942487][-3.4473767 -3.4808559 -3.4495504 -3.4083848 -3.4424477 -3.4937265 -3.3720942 -3.0941124 -2.7692034 -2.4825287 -2.3951671 -2.5106468 -2.7489738 -3.0280714 -3.2464128][-3.5352857 -3.5873952 -3.6250346 -3.6584773 -3.699101 -3.7343338 -3.668004 -3.5150957 -3.2996373 -3.0665903 -2.9572959 -2.9822044 -3.0844402 -3.2233911 -3.3390775][-3.5185392 -3.585381 -3.6552212 -3.7143948 -3.7531283 -3.7715461 -3.7257268 -3.62735 -3.4730453 -3.3090751 -3.2291026 -3.2255778 -3.2657461 -3.3293617 -3.3782437]]...]
INFO - root - 2017-12-07 17:54:51.643549: step 14310, loss = 0.81, batch loss = 0.74 (6.9 examples/sec; 4.618 sec/batch; 88h:16m:25s remains)
INFO - root - 2017-12-07 17:55:37.191485: step 14320, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.576 sec/batch; 87h:27m:16s remains)
INFO - root - 2017-12-07 17:56:23.115212: step 14330, loss = 0.69, batch loss = 0.62 (6.9 examples/sec; 4.661 sec/batch; 89h:03m:49s remains)
INFO - root - 2017-12-07 17:57:08.778961: step 14340, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.478 sec/batch; 85h:34m:09s remains)
INFO - root - 2017-12-07 17:57:54.616460: step 14350, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.589 sec/batch; 87h:39m:50s remains)
INFO - root - 2017-12-07 17:58:40.160807: step 14360, loss = 0.85, batch loss = 0.78 (7.0 examples/sec; 4.593 sec/batch; 87h:43m:49s remains)
INFO - root - 2017-12-07 17:59:25.516890: step 14370, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.573 sec/batch; 87h:19m:50s remains)
INFO - root - 2017-12-07 18:00:11.472581: step 14380, loss = 0.79, batch loss = 0.71 (6.9 examples/sec; 4.651 sec/batch; 88h:48m:55s remains)
INFO - root - 2017-12-07 18:00:56.997231: step 14390, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 4.512 sec/batch; 86h:08m:24s remains)
INFO - root - 2017-12-07 18:01:42.839096: step 14400, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.591 sec/batch; 87h:39m:00s remains)
2017-12-07 18:01:45.479612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9930711 -1.7755949 -1.7029192 -1.8094673 -1.8803444 -1.8760958 -1.9762659 -2.1642122 -2.2795858 -2.3011389 -2.4337082 -2.6581025 -2.7721756 -2.8493962 -3.0248966][-2.0777667 -1.8336444 -1.6731389 -1.6916492 -1.7446516 -1.6932316 -1.725769 -1.848089 -1.8764248 -1.8693678 -2.0473039 -2.3270023 -2.4821651 -2.6268172 -2.9038141][-2.2016158 -1.9580982 -1.716579 -1.653723 -1.6753302 -1.5965757 -1.6147189 -1.6641912 -1.627594 -1.6273661 -1.8162043 -2.1351652 -2.32245 -2.5128961 -2.9086304][-2.2600837 -2.0427794 -1.7720397 -1.6825764 -1.7153108 -1.6825407 -1.7496238 -1.7448225 -1.7143757 -1.770263 -1.8888147 -2.1197579 -2.222739 -2.3943641 -2.8815417][-2.2543957 -2.0710373 -1.812072 -1.7214363 -1.7688401 -1.7741859 -1.8837795 -1.8187773 -1.8202229 -1.9661479 -1.9964216 -2.0694785 -2.0377336 -2.1754265 -2.7253141][-2.2230477 -2.0357308 -1.7863171 -1.6861088 -1.7183895 -1.7163897 -1.814316 -1.6777909 -1.7085359 -1.97103 -1.9617512 -1.9092782 -1.8132839 -1.9821086 -2.5548878][-2.1679573 -1.9467709 -1.6836021 -1.5621941 -1.5421448 -1.5005813 -1.5662029 -1.3658946 -1.4167836 -1.8273714 -1.8462136 -1.74928 -1.7162669 -1.9908249 -2.5458989][-2.1598823 -1.908653 -1.6080651 -1.4324088 -1.3166385 -1.2271116 -1.2713392 -1.0265963 -1.090127 -1.60327 -1.6065745 -1.4839568 -1.5978866 -1.989135 -2.4813318][-2.2517631 -1.9807615 -1.6442301 -1.4117928 -1.1944687 -1.0898504 -1.1306546 -0.86722994 -0.96400261 -1.5024245 -1.4005451 -1.2265437 -1.4298162 -1.8076472 -2.1642542][-2.3631928 -2.0699809 -1.72595 -1.4417241 -1.1423581 -1.0323498 -1.0271053 -0.7390852 -0.84046793 -1.2947536 -1.0724406 -0.92215943 -1.2008302 -1.466063 -1.6734021][-2.4827838 -2.1759431 -1.8656616 -1.5473826 -1.1929805 -1.0449958 -0.90186024 -0.54185176 -0.58618927 -0.87289572 -0.60464954 -0.58493471 -0.92012095 -1.0660408 -1.230382][-2.6274662 -2.3399546 -2.0877426 -1.7650843 -1.391396 -1.1936498 -0.90621257 -0.50403309 -0.46618056 -0.56745219 -0.34990835 -0.47869539 -0.81472683 -0.88850117 -1.1081841][-2.7450943 -2.5108688 -2.3114278 -2.0160911 -1.676158 -1.4751718 -1.1401646 -0.78197885 -0.69080877 -0.65858912 -0.54350853 -0.71673012 -0.9389658 -0.94797873 -1.2045794][-2.83162 -2.6698451 -2.4942875 -2.238245 -1.9712014 -1.7917128 -1.4836247 -1.2144945 -1.0928552 -0.99772358 -0.99733472 -1.1244864 -1.1786232 -1.1236975 -1.3435178][-2.836796 -2.7324102 -2.5575233 -2.3551862 -2.1826432 -2.0252333 -1.7935741 -1.6022816 -1.4614656 -1.3789816 -1.4620745 -1.5166574 -1.4669831 -1.3988576 -1.5641444]]...]
INFO - root - 2017-12-07 18:02:31.167410: step 14410, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.536 sec/batch; 86h:35m:24s remains)
INFO - root - 2017-12-07 18:03:17.029082: step 14420, loss = 0.81, batch loss = 0.74 (6.9 examples/sec; 4.671 sec/batch; 89h:08m:27s remains)
INFO - root - 2017-12-07 18:04:02.325471: step 14430, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.489 sec/batch; 85h:40m:05s remains)
INFO - root - 2017-12-07 18:04:48.115533: step 14440, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.533 sec/batch; 86h:29m:08s remains)
INFO - root - 2017-12-07 18:05:33.755977: step 14450, loss = 0.67, batch loss = 0.60 (7.1 examples/sec; 4.524 sec/batch; 86h:18m:32s remains)
INFO - root - 2017-12-07 18:06:19.108804: step 14460, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.522 sec/batch; 86h:14m:59s remains)
INFO - root - 2017-12-07 18:07:04.740607: step 14470, loss = 0.92, batch loss = 0.85 (7.0 examples/sec; 4.541 sec/batch; 86h:35m:59s remains)
INFO - root - 2017-12-07 18:07:50.522538: step 14480, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.600 sec/batch; 87h:42m:21s remains)
INFO - root - 2017-12-07 18:08:36.218818: step 14490, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.625 sec/batch; 88h:10m:54s remains)
INFO - root - 2017-12-07 18:09:21.633572: step 14500, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.526 sec/batch; 86h:16m:44s remains)
2017-12-07 18:09:24.304924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0209465 -3.0460844 -3.0833683 -3.0945473 -3.0445595 -2.900012 -2.6893005 -2.4242253 -2.2079773 -2.1317434 -2.1917362 -2.3096159 -2.4682846 -2.6280489 -2.724973][-3.2559257 -3.2413964 -3.2157936 -3.1883013 -3.1360822 -3.0190277 -2.8444178 -2.5908177 -2.3366542 -2.1684577 -2.1217849 -2.1995759 -2.4101017 -2.636076 -2.7375326][-3.2220974 -3.1289167 -2.9955659 -2.9165292 -2.8966107 -2.8679438 -2.7880554 -2.6009436 -2.3641136 -2.1445069 -1.9957771 -2.0095959 -2.2324626 -2.5267196 -2.6831985][-2.8849316 -2.6904447 -2.4152453 -2.2225459 -2.1603863 -2.1734955 -2.1819332 -2.0975482 -1.966131 -1.828517 -1.7319257 -1.7682893 -1.9844513 -2.3074052 -2.5305939][-2.4346793 -2.1628397 -1.7801163 -1.4148304 -1.1614494 -1.0615022 -1.0773182 -1.1128182 -1.1644616 -1.2229614 -1.316118 -1.4766405 -1.7167244 -2.0340185 -2.2753453][-1.9656312 -1.653424 -1.2426574 -0.76062512 -0.28205585 0.056871891 0.13748312 -0.020361423 -0.34458256 -0.69514418 -1.034111 -1.3432345 -1.6088283 -1.8748939 -2.0594316][-1.5411012 -1.2706716 -0.94227409 -0.5075202 0.0437932 0.58055449 0.82733583 0.60486412 0.011767387 -0.62990069 -1.1338873 -1.4733183 -1.6756213 -1.8460202 -1.949806][-1.3748872 -1.270757 -1.1346545 -0.88297367 -0.45151377 0.083942413 0.41161489 0.20394897 -0.51723695 -1.3032629 -1.815486 -1.9983723 -1.9846632 -1.9743843 -1.9699466][-1.5968494 -1.675904 -1.7185938 -1.6691997 -1.479285 -1.1590977 -0.92698 -1.0996442 -1.7286832 -2.4191771 -2.7903333 -2.7322426 -2.4500923 -2.2118261 -2.0649981][-2.0633373 -2.2295003 -2.340493 -2.3992445 -2.413219 -2.353488 -2.3122709 -2.4894991 -2.922863 -3.35574 -3.511313 -3.2753725 -2.8274007 -2.4274838 -2.1557741][-2.7474058 -2.9075329 -2.9910731 -3.0405636 -3.0814042 -3.1010976 -3.1575947 -3.3806925 -3.7068627 -3.9235411 -3.8701308 -3.5227952 -3.0441325 -2.6297328 -2.321485][-3.4452853 -3.5687833 -3.620744 -3.6527209 -3.6552591 -3.6209795 -3.63137 -3.8313341 -4.0883417 -4.1870346 -4.0053682 -3.5947621 -3.1334863 -2.7591939 -2.4860611][-3.9235129 -4.0121317 -4.0355377 -4.0381813 -3.9917617 -3.889679 -3.8164434 -3.9487166 -4.1504469 -4.1934118 -3.9666862 -3.5295222 -3.0751467 -2.7187135 -2.4824533][-3.9897234 -4.045629 -4.0824761 -4.0874405 -4.0193758 -3.8616762 -3.7104177 -3.7621455 -3.9064689 -3.9260683 -3.7194524 -3.3234801 -2.90688 -2.5687647 -2.3561733][-3.7966852 -3.8298843 -3.8893158 -3.9133422 -3.8703675 -3.7147918 -3.5309548 -3.5192676 -3.5936127 -3.5802643 -3.4085035 -3.0915616 -2.7576058 -2.455162 -2.241241]]...]
INFO - root - 2017-12-07 18:10:09.984225: step 14510, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.482 sec/batch; 85h:25m:36s remains)
INFO - root - 2017-12-07 18:10:55.379530: step 14520, loss = 0.73, batch loss = 0.65 (6.9 examples/sec; 4.611 sec/batch; 87h:52m:08s remains)
INFO - root - 2017-12-07 18:11:41.095736: step 14530, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.550 sec/batch; 86h:41m:13s remains)
INFO - root - 2017-12-07 18:12:26.321876: step 14540, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.601 sec/batch; 87h:39m:12s remains)
INFO - root - 2017-12-07 18:13:12.048195: step 14550, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.566 sec/batch; 86h:58m:06s remains)
INFO - root - 2017-12-07 18:13:57.397278: step 14560, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.583 sec/batch; 87h:16m:41s remains)
INFO - root - 2017-12-07 18:14:42.788162: step 14570, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.552 sec/batch; 86h:41m:16s remains)
INFO - root - 2017-12-07 18:15:28.233404: step 14580, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 4.534 sec/batch; 86h:19m:45s remains)
INFO - root - 2017-12-07 18:16:14.026623: step 14590, loss = 0.69, batch loss = 0.62 (6.9 examples/sec; 4.608 sec/batch; 87h:43m:34s remains)
INFO - root - 2017-12-07 18:16:59.722213: step 14600, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.530 sec/batch; 86h:14m:08s remains)
2017-12-07 18:17:02.456703: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34749937 0.35045719 0.37343025 0.37898445 0.33196068 0.24863863 0.11268425 0.0090680122 0.00059604645 0.056576729 0.14448977 0.21688318 0.24587011 0.24999952 0.24114704][0.27517557 0.29383373 0.34391308 0.37033176 0.35634995 0.27329302 0.067669868 -0.1464591 -0.22690916 -0.15015078 -0.0018811226 0.13813591 0.20508146 0.2190485 0.21580553][0.19224739 0.28423738 0.44504642 0.56982136 0.63897848 0.55996561 0.24462175 -0.13725281 -0.38145638 -0.37332439 -0.21886539 -0.028862953 0.10250139 0.14972782 0.16872883][0.0802536 0.25320959 0.52217007 0.74837208 0.88723707 0.82437754 0.46264648 -0.041661263 -0.45408559 -0.58054852 -0.49639344 -0.30023003 -0.098356247 0.027423859 0.1090188][-0.12986517 -0.042869568 0.18174314 0.4322114 0.64775848 0.70409489 0.45631218 -0.082113266 -0.65803266 -0.95229554 -0.97688365 -0.79853964 -0.50900126 -0.24054098 -0.019415379][-0.31583261 -0.45520759 -0.38315392 -0.14888191 0.21407795 0.55006123 0.57895088 0.077356815 -0.71165586 -1.2411447 -1.447927 -1.3546312 -1.0143461 -0.60820889 -0.22362709][-0.29813766 -0.60639286 -0.68421412 -0.50001907 0.016878605 0.70492363 1.1704826 0.88590622 -0.021400452 -0.82888842 -1.3521519 -1.5120769 -1.2741394 -0.85420132 -0.39240932][-0.32466125 -0.7011385 -0.90413642 -0.803406 -0.24215698 0.65332556 1.4848819 1.5230017 0.68678093 -0.26773357 -1.0469129 -1.453001 -1.3519359 -0.97115684 -0.48916698][-0.56413555 -0.96589422 -1.2685909 -1.2611477 -0.80060983 -0.0042171478 0.83350468 1.0996237 0.56024361 -0.20927286 -0.96900034 -1.4378934 -1.3860281 -1.0266461 -0.54139972][-0.8559823 -1.2757301 -1.644707 -1.7488596 -1.4730973 -0.92355919 -0.24375153 0.17276192 0.008055687 -0.44154477 -1.0632262 -1.5190027 -1.4721017 -1.124743 -0.653564][-0.96101189 -1.3813205 -1.7931368 -2.0409639 -2.026552 -1.800781 -1.318917 -0.8081367 -0.6228478 -0.72992849 -1.1487355 -1.548198 -1.5123522 -1.1990268 -0.77891326][-0.76506472 -1.1344357 -1.5434394 -1.8991647 -2.1115129 -2.1838293 -1.9625008 -1.5255411 -1.1832273 -1.0066822 -1.1428821 -1.3790846 -1.3470232 -1.1181097 -0.79575944][-0.56845379 -0.83569789 -1.1831543 -1.5484846 -1.8311541 -2.0195069 -1.9663424 -1.681757 -1.3973615 -1.1533537 -1.102973 -1.1656411 -1.1181469 -0.98897743 -0.7881074][-0.6627624 -0.79078007 -1.0157454 -1.3029494 -1.5363913 -1.6790926 -1.62884 -1.4026248 -1.1669137 -0.9446609 -0.83777404 -0.82970715 -0.82361388 -0.83361006 -0.78255677][-0.9665792 -0.94689369 -1.0162771 -1.1758201 -1.3283014 -1.4224057 -1.3824489 -1.2253795 -1.0290534 -0.81912589 -0.65653443 -0.56229305 -0.55589652 -0.64599538 -0.69659233]]...]
INFO - root - 2017-12-07 18:17:47.747716: step 14610, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.561 sec/batch; 86h:48m:17s remains)
INFO - root - 2017-12-07 18:18:33.181825: step 14620, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.572 sec/batch; 86h:59m:48s remains)
INFO - root - 2017-12-07 18:19:18.912549: step 14630, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.610 sec/batch; 87h:42m:23s remains)
INFO - root - 2017-12-07 18:20:04.111576: step 14640, loss = 0.84, batch loss = 0.76 (7.0 examples/sec; 4.550 sec/batch; 86h:33m:07s remains)
INFO - root - 2017-12-07 18:20:49.777588: step 14650, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.571 sec/batch; 86h:57m:10s remains)
INFO - root - 2017-12-07 18:21:35.219839: step 14660, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.644 sec/batch; 88h:19m:35s remains)
INFO - root - 2017-12-07 18:22:20.851460: step 14670, loss = 0.77, batch loss = 0.69 (7.0 examples/sec; 4.582 sec/batch; 87h:07m:15s remains)
INFO - root - 2017-12-07 18:23:06.348430: step 14680, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.596 sec/batch; 87h:23m:03s remains)
INFO - root - 2017-12-07 18:23:52.112471: step 14690, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.588 sec/batch; 87h:13m:30s remains)
INFO - root - 2017-12-07 18:24:37.690285: step 14700, loss = 0.82, batch loss = 0.74 (7.0 examples/sec; 4.542 sec/batch; 86h:19m:14s remains)
2017-12-07 18:24:40.517886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7076483 -2.8884425 -2.8539734 -2.6225138 -2.3481886 -2.0097723 -1.8240764 -1.9243045 -2.1812165 -2.5386519 -2.779963 -2.8237319 -2.7434554 -2.5788307 -2.5231395][-3.3849587 -3.5595586 -3.463727 -3.0684929 -2.5473447 -1.9378138 -1.4812024 -1.5235515 -1.9443407 -2.6003745 -3.1592617 -3.4099522 -3.4469655 -3.3227594 -3.176532][-3.9076686 -4.0570636 -3.8931761 -3.3388243 -2.5703263 -1.7094846 -0.99100661 -0.89072084 -1.3837404 -2.2658646 -3.1455257 -3.7094464 -3.9950724 -4.0188513 -3.8221033][-4.1907287 -4.2984338 -4.0559931 -3.3858438 -2.4811642 -1.4749441 -0.53900051 -0.2053194 -0.66852832 -1.7063928 -2.8937526 -3.8275397 -4.3974214 -4.59307 -4.3717341][-4.2521091 -4.3153968 -3.9708781 -3.2419057 -2.3209002 -1.266187 -0.12704849 0.52913 0.16958332 -0.97125363 -2.4258206 -3.741359 -4.6114078 -4.9826937 -4.7423038][-4.1097207 -4.11304 -3.683156 -2.9695525 -2.1268256 -1.0752127 0.22572041 1.1867218 0.91149759 -0.31177568 -1.9181967 -3.4892464 -4.5697646 -5.0877247 -4.8868775][-3.8029418 -3.7538481 -3.3072374 -2.7115488 -2.0088596 -0.9686079 0.48874617 1.6879196 1.4949222 0.2322793 -1.4096746 -3.0925808 -4.3076682 -4.9436007 -4.7960811][-3.2362924 -3.1655064 -2.8182976 -2.4601789 -1.9855242 -1.044862 0.44219303 1.8177705 1.8347316 0.71660662 -0.84904814 -2.5692534 -3.9158776 -4.6832123 -4.5894847][-2.4091341 -2.3563786 -2.1534173 -2.0471048 -1.8500953 -1.1985283 0.0073599815 1.250916 1.5246124 0.744143 -0.63829279 -2.2713659 -3.6006327 -4.3646531 -4.2574711][-1.686326 -1.6917024 -1.5522087 -1.5445557 -1.5407727 -1.2374382 -0.459507 0.52393389 0.95315027 0.44601154 -0.84570527 -2.3913445 -3.5688214 -4.1598239 -3.9311841][-1.5398006 -1.6023102 -1.3993073 -1.3313544 -1.3718879 -1.2870083 -0.8122673 -0.017494202 0.49636364 0.12440491 -1.2141924 -2.7384443 -3.7375202 -4.0898042 -3.7036204][-1.8715272 -1.9002421 -1.5323527 -1.2937639 -1.2369208 -1.1813884 -0.88937807 -0.31286764 0.10900974 -0.28638792 -1.7008083 -3.223309 -4.0723419 -4.2052374 -3.674336][-2.2580152 -2.1550694 -1.5861211 -1.1933997 -1.008796 -0.91604495 -0.80256605 -0.49118233 -0.31821585 -0.83096027 -2.2517421 -3.7486906 -4.509738 -4.4835086 -3.8667917][-2.4002216 -2.2165232 -1.5672548 -1.1183906 -0.88576031 -0.85497189 -0.97372317 -0.894459 -0.91684031 -1.4885187 -2.7937665 -4.2009611 -4.9127522 -4.8139458 -4.2001848][-2.3825452 -2.1609759 -1.5813622 -1.2021534 -1.0007198 -1.0815713 -1.4641318 -1.6037135 -1.7458537 -2.2534578 -3.2985432 -4.5219293 -5.1857262 -5.1289034 -4.6570849]]...]
INFO - root - 2017-12-07 18:25:26.115933: step 14710, loss = 0.85, batch loss = 0.78 (7.1 examples/sec; 4.527 sec/batch; 86h:01m:22s remains)
INFO - root - 2017-12-07 18:26:11.768939: step 14720, loss = 0.76, batch loss = 0.69 (7.2 examples/sec; 4.447 sec/batch; 84h:29m:45s remains)
INFO - root - 2017-12-07 18:26:57.561520: step 14730, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.639 sec/batch; 88h:07m:41s remains)
INFO - root - 2017-12-07 18:27:43.313554: step 14740, loss = 0.75, batch loss = 0.67 (7.1 examples/sec; 4.529 sec/batch; 86h:02m:02s remains)
INFO - root - 2017-12-07 18:28:28.634653: step 14750, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.550 sec/batch; 86h:24m:43s remains)
INFO - root - 2017-12-07 18:29:14.208467: step 14760, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.597 sec/batch; 87h:17m:35s remains)
INFO - root - 2017-12-07 18:29:59.656983: step 14770, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.529 sec/batch; 86h:00m:00s remains)
INFO - root - 2017-12-07 18:30:45.500556: step 14780, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.553 sec/batch; 86h:25m:56s remains)
INFO - root - 2017-12-07 18:31:30.842544: step 14790, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.591 sec/batch; 87h:08m:31s remains)
INFO - root - 2017-12-07 18:32:16.485529: step 14800, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.576 sec/batch; 86h:50m:44s remains)
2017-12-07 18:32:19.257040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4279692 -3.4705963 -3.5308709 -3.5973132 -3.6575816 -3.7092061 -3.7411938 -3.7812028 -3.7991343 -3.7962203 -3.746892 -3.6379309 -3.5176334 -3.382247 -3.2624993][-3.5785875 -3.6513109 -3.7504854 -3.849088 -3.9143708 -3.95635 -3.9699366 -3.9899626 -3.9728003 -3.9315398 -3.8554451 -3.714946 -3.5513949 -3.3644876 -3.2044377][-3.6613634 -3.7265246 -3.8247352 -3.9222891 -3.9699893 -3.99589 -4.0084457 -4.0409226 -4.0390811 -4.0049825 -3.938669 -3.7759492 -3.5488288 -3.2911322 -3.09008][-3.6282396 -3.6344981 -3.6870685 -3.7739782 -3.8040953 -3.7933321 -3.7724681 -3.8053968 -3.8410239 -3.858655 -3.8556569 -3.7312255 -3.4773648 -3.1581147 -2.9312437][-3.4881747 -3.3940406 -3.365344 -3.4620683 -3.5545824 -3.5843766 -3.5567698 -3.5624356 -3.5886014 -3.5976455 -3.6151996 -3.5451994 -3.3166585 -2.9773219 -2.7558341][-3.2933717 -3.0515161 -2.8467259 -2.8492665 -2.9800029 -3.1154151 -3.1947982 -3.3073359 -3.4547884 -3.5017793 -3.5008461 -3.4286509 -3.1835623 -2.8087831 -2.5990233][-3.1205006 -2.7276747 -2.2864368 -2.0315113 -2.0056295 -2.0958991 -2.2381439 -2.5589592 -3.0440059 -3.3878853 -3.5183058 -3.4606502 -3.1667805 -2.7442894 -2.5306072][-3.0247869 -2.5538096 -1.951715 -1.4428627 -1.141778 -0.97360635 -0.93711782 -1.3048878 -2.0891545 -2.834372 -3.270925 -3.3585544 -3.1210251 -2.7258136 -2.5392969][-3.0841913 -2.6982925 -2.1291175 -1.5518816 -1.0843935 -0.63811588 -0.27867222 -0.40395594 -1.1308756 -2.0327675 -2.7047296 -3.0060277 -2.9586036 -2.7141161 -2.6004853][-3.0900393 -2.9320309 -2.6086698 -2.2272241 -1.8693051 -1.4177585 -0.94100714 -0.76322961 -1.1152084 -1.8121512 -2.4410558 -2.7970076 -2.8714387 -2.7579434 -2.7055857][-2.7592332 -2.7207618 -2.6301773 -2.5351033 -2.4440782 -2.2173216 -1.8583395 -1.5887766 -1.6642869 -2.0899992 -2.5440671 -2.8148875 -2.8933821 -2.8361244 -2.813725][-2.3342469 -2.1828275 -2.1020749 -2.1223297 -2.2115257 -2.2189891 -2.0912127 -1.9498219 -1.9778192 -2.2608843 -2.5930328 -2.8067832 -2.8948016 -2.8845749 -2.8920147][-2.3900037 -2.0952685 -1.8816755 -1.8224201 -1.8985989 -1.9704261 -1.9858923 -1.9794159 -2.0278707 -2.2312751 -2.4891324 -2.6917698 -2.8305588 -2.8868737 -2.9357066][-2.9011326 -2.6381125 -2.3999732 -2.2690525 -2.2529151 -2.2652347 -2.2849762 -2.3013587 -2.3339617 -2.4377563 -2.5821071 -2.7165122 -2.8434887 -2.9180279 -2.9820719][-3.3594832 -3.2482729 -3.1126485 -3.0051188 -2.9493971 -2.9125948 -2.8984547 -2.8951068 -2.8982415 -2.9201365 -2.9533482 -2.9834628 -3.0243375 -3.0470257 -3.0766516]]...]
INFO - root - 2017-12-07 18:33:05.037960: step 14810, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.655 sec/batch; 88h:19m:52s remains)
INFO - root - 2017-12-07 18:33:50.497505: step 14820, loss = 0.88, batch loss = 0.81 (7.1 examples/sec; 4.527 sec/batch; 85h:53m:03s remains)
INFO - root - 2017-12-07 18:34:35.788546: step 14830, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.564 sec/batch; 86h:35m:13s remains)
INFO - root - 2017-12-07 18:35:21.357646: step 14840, loss = 0.69, batch loss = 0.62 (6.8 examples/sec; 4.684 sec/batch; 88h:51m:01s remains)
INFO - root - 2017-12-07 18:36:06.813491: step 14850, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.560 sec/batch; 86h:28m:54s remains)
INFO - root - 2017-12-07 18:36:52.160583: step 14860, loss = 0.71, batch loss = 0.63 (7.1 examples/sec; 4.484 sec/batch; 85h:01m:58s remains)
INFO - root - 2017-12-07 18:37:37.854220: step 14870, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.511 sec/batch; 85h:31m:59s remains)
INFO - root - 2017-12-07 18:38:23.649946: step 14880, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.577 sec/batch; 86h:46m:11s remains)
INFO - root - 2017-12-07 18:39:09.150936: step 14890, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.525 sec/batch; 85h:45m:31s remains)
INFO - root - 2017-12-07 18:39:54.839620: step 14900, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.581 sec/batch; 86h:49m:28s remains)
2017-12-07 18:39:57.525385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1649442 -2.4913402 -2.8019967 -2.870183 -2.6012416 -2.2182693 -2.0131471 -2.1758552 -2.5657032 -2.7837424 -2.6051283 -2.4535313 -2.6097784 -2.8303061 -2.8063602][-2.1689222 -2.4979925 -2.8648345 -2.9089687 -2.5747719 -2.1449735 -1.9170003 -2.1261199 -2.4894462 -2.5748315 -2.2523055 -1.9728773 -2.0278597 -2.1325271 -2.0261769][-2.1851473 -2.6144748 -3.0653639 -3.037384 -2.5975401 -2.0875115 -1.7334993 -1.8599968 -2.1147871 -2.1147244 -1.8237512 -1.6073325 -1.6958981 -1.7672191 -1.5873349][-2.2822928 -2.8903408 -3.4447179 -3.3838332 -2.8752284 -2.270092 -1.709661 -1.6205218 -1.7414186 -1.7642064 -1.6390495 -1.6031227 -1.7916238 -1.8574107 -1.6422112][-2.2895792 -3.0185304 -3.6003516 -3.5344448 -3.0208712 -2.3723402 -1.6547432 -1.3722317 -1.4677501 -1.6122985 -1.6625957 -1.7424636 -1.921262 -1.9384511 -1.710726][-2.2245352 -2.9856381 -3.5248897 -3.4261377 -2.8312602 -2.0305634 -1.1652477 -0.78884387 -0.98647928 -1.321059 -1.5606863 -1.7573373 -1.9228706 -1.894594 -1.6832848][-2.1604915 -2.9427783 -3.4646473 -3.3057289 -2.4580877 -1.2748332 -0.11533689 0.31564236 -0.102458 -0.743088 -1.2793205 -1.7018135 -1.9813161 -2.0374675 -1.9074984][-2.1634007 -2.9170287 -3.3676915 -3.0464716 -1.8543069 -0.23606968 1.1846819 1.543932 0.80638742 -0.22082424 -1.0905671 -1.814795 -2.3339798 -2.5600634 -2.4482298][-2.2287138 -2.8616872 -3.1118088 -2.5472541 -1.1234577 0.61015129 1.9803514 2.1598177 1.2029848 -0.041724682 -1.0933309 -1.9856904 -2.620661 -2.8862333 -2.6487823][-2.3024733 -2.8002262 -2.85787 -2.2019217 -0.8739922 0.52740908 1.5143943 1.5522389 0.69172144 -0.41998196 -1.3545394 -2.1417336 -2.6775296 -2.8427796 -2.4021668][-2.3956332 -2.8472714 -2.88377 -2.3567522 -1.355469 -0.44818902 0.1225419 0.11492157 -0.476192 -1.2427945 -1.8688986 -2.3804448 -2.70326 -2.7464838 -2.2095778][-2.3934577 -2.8380775 -2.9923668 -2.7599118 -2.1613753 -1.6461878 -1.3573391 -1.3210015 -1.6232955 -2.0528772 -2.3683012 -2.5988264 -2.7184741 -2.7111979 -2.2850544][-2.163944 -2.5097494 -2.6958637 -2.683475 -2.4071176 -2.1419468 -2.016747 -1.9606354 -2.0824616 -2.2834036 -2.3758357 -2.4238496 -2.4362478 -2.4493098 -2.2066367][-1.7900083 -1.9202554 -1.9870234 -2.0115411 -1.9118814 -1.8080792 -1.8025067 -1.784256 -1.7911606 -1.8007488 -1.7034864 -1.63135 -1.6061635 -1.6159751 -1.4564636][-1.5211051 -1.4318841 -1.3427143 -1.2830443 -1.2364075 -1.2115982 -1.2204616 -1.1672139 -1.0571885 -0.91489029 -0.70740008 -0.56470394 -0.51066685 -0.50005674 -0.38057947]]...]
INFO - root - 2017-12-07 18:40:43.326756: step 14910, loss = 0.78, batch loss = 0.70 (6.8 examples/sec; 4.672 sec/batch; 88h:32m:02s remains)
INFO - root - 2017-12-07 18:41:29.070244: step 14920, loss = 0.86, batch loss = 0.78 (7.0 examples/sec; 4.584 sec/batch; 86h:50m:49s remains)
INFO - root - 2017-12-07 18:42:14.455486: step 14930, loss = 0.85, batch loss = 0.78 (7.1 examples/sec; 4.484 sec/batch; 84h:56m:56s remains)
INFO - root - 2017-12-07 18:43:00.018916: step 14940, loss = 0.77, batch loss = 0.69 (7.0 examples/sec; 4.546 sec/batch; 86h:05m:53s remains)
INFO - root - 2017-12-07 18:43:45.558747: step 14950, loss = 0.94, batch loss = 0.87 (7.0 examples/sec; 4.582 sec/batch; 86h:46m:39s remains)
INFO - root - 2017-12-07 18:44:31.193260: step 14960, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.570 sec/batch; 86h:32m:19s remains)
INFO - root - 2017-12-07 18:45:16.783999: step 14970, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.566 sec/batch; 86h:26m:04s remains)
INFO - root - 2017-12-07 18:46:02.650524: step 14980, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.577 sec/batch; 86h:38m:23s remains)
INFO - root - 2017-12-07 18:46:48.176224: step 14990, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.569 sec/batch; 86h:28m:33s remains)
INFO - root - 2017-12-07 18:47:33.764728: step 15000, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.477 sec/batch; 84h:43m:08s remains)
2017-12-07 18:47:36.444226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7796774 -2.8130562 -2.8234758 -2.8202925 -2.80788 -2.8165455 -2.8392844 -2.8735735 -2.8808742 -2.850791 -2.8118296 -2.7790105 -2.7575769 -2.729531 -2.7233033][-2.8358388 -2.8776383 -2.8991995 -2.9127212 -2.9161448 -2.9312494 -2.9469304 -2.9696584 -2.9431639 -2.8725243 -2.8040638 -2.7672741 -2.7712069 -2.759799 -2.7638879][-2.9195375 -2.9747183 -3.0153818 -3.043849 -3.0318098 -2.9899216 -2.9127069 -2.8452537 -2.7323506 -2.6190028 -2.5711477 -2.6044874 -2.7115462 -2.7820816 -2.8351078][-2.9791422 -3.0207412 -3.0557513 -3.0673366 -2.9742794 -2.7965357 -2.5529869 -2.354388 -2.1622336 -2.0647039 -2.1373935 -2.3224533 -2.5830541 -2.7582417 -2.8460283][-2.8272586 -2.7901084 -2.7541869 -2.7026343 -2.5132146 -2.2351341 -1.8866665 -1.634011 -1.4642386 -1.47438 -1.7197967 -2.0551677 -2.4256794 -2.6473393 -2.6986933][-2.3422508 -2.2146435 -2.0993307 -2.0150177 -1.818759 -1.579169 -1.2561445 -1.0405211 -0.97051382 -1.1111372 -1.4882135 -1.8788607 -2.2388318 -2.4184442 -2.4073899][-1.7142043 -1.5712817 -1.4500735 -1.3954761 -1.2573402 -1.1105797 -0.87775612 -0.73972917 -0.78480196 -1.0401218 -1.500535 -1.8820233 -2.1424632 -2.2181766 -2.1490574][-1.2726049 -1.1805794 -1.1024837 -1.0845509 -0.95000219 -0.79882383 -0.61858368 -0.57214332 -0.76367116 -1.1488869 -1.6353791 -1.9491663 -2.0651326 -2.0261037 -1.9416399][-1.2138367 -1.1654048 -1.0997872 -1.0553806 -0.85034013 -0.64290833 -0.51306248 -0.5685997 -0.85661411 -1.2598302 -1.6213758 -1.790489 -1.7758055 -1.7098191 -1.7079442][-1.4891703 -1.407861 -1.2912941 -1.2022438 -0.99615312 -0.86123991 -0.92361426 -1.1408806 -1.4296732 -1.6763694 -1.7266161 -1.6358283 -1.4935472 -1.4456916 -1.5674903][-1.9285676 -1.7300467 -1.5176568 -1.4109333 -1.3078146 -1.3339319 -1.625325 -2.0240984 -2.3032956 -2.3544538 -2.0796149 -1.7348335 -1.528002 -1.5496647 -1.7990649][-2.2883127 -2.0430686 -1.813904 -1.7119918 -1.6535554 -1.733186 -2.1202781 -2.6212363 -2.9049089 -2.8417072 -2.4058282 -2.0028191 -1.9161682 -2.1023767 -2.4608243][-2.4137213 -2.1986182 -2.0296478 -1.9740264 -1.9347937 -2.0047715 -2.3701029 -2.8349123 -3.0621467 -2.9216492 -2.5162177 -2.261425 -2.3822637 -2.7437549 -3.1521783][-2.476897 -2.3020651 -2.2086308 -2.2493415 -2.2925487 -2.3989244 -2.7081528 -3.0023556 -3.0644791 -2.8311081 -2.52384 -2.457736 -2.719233 -3.1388166 -3.4793758][-2.5856471 -2.4688306 -2.4524689 -2.6051145 -2.7521939 -2.8898 -3.1232469 -3.227267 -3.1346343 -2.8655367 -2.652565 -2.6892309 -2.9407067 -3.2600956 -3.4188137]]...]
INFO - root - 2017-12-07 18:48:22.283207: step 15010, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.565 sec/batch; 86h:22m:04s remains)
INFO - root - 2017-12-07 18:49:08.017496: step 15020, loss = 0.72, batch loss = 0.65 (7.2 examples/sec; 4.473 sec/batch; 84h:37m:39s remains)
INFO - root - 2017-12-07 18:49:53.755306: step 15030, loss = 0.83, batch loss = 0.76 (7.2 examples/sec; 4.463 sec/batch; 84h:25m:24s remains)
INFO - root - 2017-12-07 18:50:39.661727: step 15040, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.584 sec/batch; 86h:41m:56s remains)
INFO - root - 2017-12-07 18:51:25.254664: step 15050, loss = 0.79, batch loss = 0.71 (7.0 examples/sec; 4.577 sec/batch; 86h:33m:26s remains)
INFO - root - 2017-12-07 18:52:10.986881: step 15060, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.589 sec/batch; 86h:45m:34s remains)
INFO - root - 2017-12-07 18:52:56.849939: step 15070, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.556 sec/batch; 86h:08m:01s remains)
INFO - root - 2017-12-07 18:53:42.724305: step 15080, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.585 sec/batch; 86h:39m:20s remains)
INFO - root - 2017-12-07 18:54:28.327067: step 15090, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.568 sec/batch; 86h:19m:18s remains)
INFO - root - 2017-12-07 18:55:14.171018: step 15100, loss = 0.83, batch loss = 0.76 (6.9 examples/sec; 4.605 sec/batch; 87h:00m:47s remains)
2017-12-07 18:55:16.877822: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.97711706 -0.86509991 -0.790447 -0.73357034 -0.71701622 -0.73859835 -0.80283785 -0.85393119 -0.85905933 -0.8628664 -0.87594748 -0.88267541 -0.88035631 -0.87734556 -0.89064956][-0.89475393 -0.81001139 -0.78500223 -0.80287528 -0.83121061 -0.84867907 -0.86810613 -0.82369208 -0.72588587 -0.71199584 -0.76206064 -0.81671119 -0.82173443 -0.77590704 -0.72950506][-0.89585471 -0.9775691 -1.1339364 -1.2869577 -1.3017063 -1.177866 -0.98792291 -0.70542 -0.44015551 -0.44352889 -0.64767861 -0.85449433 -0.88617158 -0.72367454 -0.51620245][-1.1059682 -1.5353596 -1.9618151 -2.1668029 -1.9677658 -1.4884899 -0.93370485 -0.31879616 0.081259727 -0.080747128 -0.59398293 -0.98930144 -0.99352121 -0.61550665 -0.17033863][-1.4798338 -2.2703097 -2.871295 -3.028013 -2.5881681 -1.7962599 -0.87877822 0.12671185 0.69680262 0.36573792 -0.47308302 -1.0369453 -0.98131347 -0.39514589 0.22432709][-1.9262438 -3.0053554 -3.6882954 -3.765619 -3.1638243 -2.1570568 -0.85781217 0.66924 1.487906 0.98297358 -0.19650221 -0.96692753 -0.89079404 -0.136127 0.62304497][-2.3869572 -3.6025224 -4.2580676 -4.210258 -3.4316704 -2.1358044 -0.39908409 1.5537486 2.3714042 1.5160232 0.016989708 -0.8800106 -0.72962427 0.1916399 1.0896597][-2.7372565 -3.9072547 -4.461493 -4.301353 -3.3605804 -1.7968755 0.32210159 2.4837003 3.1002488 1.8602929 0.15271568 -0.78535247 -0.5888555 0.39168692 1.3648682][-2.9767756 -4.1080441 -4.6266661 -4.4072156 -3.37079 -1.7670598 0.31816578 2.2012625 2.4391484 1.1268926 -0.31224155 -0.94920468 -0.59134936 0.48092175 1.5112791][-3.1466255 -4.2735519 -4.7980461 -4.5299435 -3.5117221 -2.1005423 -0.40080833 0.93198824 0.926476 -0.0649519 -0.95215392 -1.155555 -0.59601378 0.58670139 1.6470876][-3.1454239 -4.2093563 -4.67417 -4.3523088 -3.4593582 -2.3971348 -1.2245927 -0.42192078 -0.53154159 -1.1823778 -1.622745 -1.4947059 -0.74979353 0.54929018 1.6243515][-3.0404813 -4.0474911 -4.4702349 -4.170475 -3.4618225 -2.7510023 -2.033649 -1.6108074 -1.7493279 -2.1267869 -2.2287982 -1.8398249 -0.94506073 0.39163446 1.3951769][-2.7742941 -3.6410518 -3.9940827 -3.7505655 -3.2409286 -2.7944884 -2.399348 -2.2283089 -2.416688 -2.6613998 -2.5671456 -2.0286317 -1.082031 0.17930746 1.0328865][-2.4381471 -3.1209807 -3.3973761 -3.2216575 -2.8838935 -2.6466222 -2.4933009 -2.497721 -2.7169158 -2.86199 -2.6239591 -1.9926565 -1.0667737 0.015761852 0.64721107][-2.0962389 -2.5711479 -2.7542026 -2.6043456 -2.3575974 -2.2228904 -2.1687195 -2.2241051 -2.4117637 -2.4866962 -2.2070432 -1.6455197 -0.91296768 -0.15299129 0.19231606]]...]
INFO - root - 2017-12-07 18:56:02.303586: step 15110, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.562 sec/batch; 86h:11m:18s remains)
INFO - root - 2017-12-07 18:56:48.153718: step 15120, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.559 sec/batch; 86h:07m:13s remains)
INFO - root - 2017-12-07 18:57:33.786631: step 15130, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.559 sec/batch; 86h:06m:22s remains)
INFO - root - 2017-12-07 18:58:19.036258: step 15140, loss = 0.80, batch loss = 0.72 (7.0 examples/sec; 4.549 sec/batch; 85h:53m:50s remains)
INFO - root - 2017-12-07 18:59:04.726034: step 15150, loss = 0.66, batch loss = 0.58 (6.9 examples/sec; 4.629 sec/batch; 87h:24m:35s remains)
INFO - root - 2017-12-07 18:59:50.410477: step 15160, loss = 0.75, batch loss = 0.68 (6.8 examples/sec; 4.703 sec/batch; 88h:46m:56s remains)
INFO - root - 2017-12-07 19:00:35.874204: step 15170, loss = 0.77, batch loss = 0.69 (7.2 examples/sec; 4.440 sec/batch; 83h:49m:00s remains)
INFO - root - 2017-12-07 19:01:21.293135: step 15180, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.570 sec/batch; 86h:14m:35s remains)
INFO - root - 2017-12-07 19:02:06.795641: step 15190, loss = 0.78, batch loss = 0.70 (7.1 examples/sec; 4.478 sec/batch; 84h:30m:04s remains)
INFO - root - 2017-12-07 19:02:52.124867: step 15200, loss = 0.79, batch loss = 0.71 (7.1 examples/sec; 4.536 sec/batch; 85h:34m:47s remains)
2017-12-07 19:02:54.851013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9861438 -1.753994 -1.6676204 -1.6511347 -1.6871705 -1.5499353 -1.026202 -0.58283114 -0.50587273 -0.6859374 -1.2620285 -1.9854743 -2.4455123 -2.6357317 -2.6371222][-2.0279357 -1.6385617 -1.4938145 -1.4002707 -1.3830733 -1.1702414 -0.45046544 0.097574234 0.013282299 -0.3218255 -0.92335367 -1.6622853 -2.1724474 -2.4635949 -2.5765512][-2.0469604 -1.5827551 -1.5117531 -1.4144568 -1.2789145 -0.89002061 0.078637123 0.75791121 0.52921343 0.055967808 -0.52594638 -1.2478001 -1.8261917 -2.2224128 -2.4580872][-1.9783587 -1.5329354 -1.6069868 -1.5594652 -1.3256078 -0.77214766 0.47709703 1.3830857 1.1310077 0.57737827 0.0026130676 -0.74802566 -1.4497757 -1.973809 -2.3301992][-1.9635582 -1.5621471 -1.712539 -1.6057239 -1.1638241 -0.38381672 1.1371336 2.2165909 1.9254513 1.2716951 0.63022041 -0.23160934 -1.0970685 -1.7441037 -2.2138395][-1.7492118 -1.349179 -1.4663358 -1.1990752 -0.50285172 0.41209841 1.9772253 2.9473944 2.4335117 1.6341238 0.94487667 0.0024576187 -0.94815993 -1.6267805 -2.1448948][-2.3358729 -1.8233457 -1.7474513 -1.2479169 -0.3913126 0.49791765 1.9296689 2.7673564 2.193666 1.4208336 0.80407667 -0.11966228 -1.0402093 -1.6638923 -2.1543865][-3.3442984 -2.9199805 -2.723227 -2.0581 -1.2242606 -0.49718618 0.8139677 1.813798 1.6763043 1.231369 0.73904991 -0.20898581 -1.1511512 -1.7588928 -2.215219][-3.6089022 -3.5193121 -3.4105208 -2.8156617 -2.194953 -1.6011875 -0.25606918 1.0485749 1.4440079 1.3915915 0.97398138 -0.056030273 -1.112201 -1.7981787 -2.2839093][-3.3961139 -3.6932955 -3.7853751 -3.3948932 -2.956778 -2.3598235 -0.85717034 0.71661568 1.4338202 1.5973663 1.2304454 0.14314318 -1.0069509 -1.7676535 -2.3113937][-2.639298 -3.2261109 -3.546102 -3.4612279 -3.2778659 -2.7502575 -1.2003224 0.4662447 1.2908115 1.5675769 1.3107829 0.31121063 -0.86391854 -1.6913657 -2.2866201][-1.8573937 -2.5421696 -3.0272245 -3.2535663 -3.3901782 -3.0476239 -1.652215 -0.15613222 0.60732031 0.98970795 1.0082421 0.36466217 -0.64984107 -1.5399158 -2.217927][-1.6798787 -2.3201497 -2.8532391 -3.2578523 -3.6078837 -3.4732804 -2.4178822 -1.3385715 -0.77928329 -0.313632 0.084479332 0.0079774857 -0.59434247 -1.3989022 -2.1190047][-1.8663595 -2.3798089 -2.8460176 -3.2695091 -3.6696529 -3.6798704 -3.0148988 -2.4115987 -2.1544602 -1.7412612 -1.1409314 -0.75199175 -0.8854816 -1.4303656 -2.0507853][-2.0082111 -2.3933449 -2.7485411 -3.0823812 -3.3812354 -3.4126902 -3.0369511 -2.7705483 -2.7558386 -2.5283666 -2.0021136 -1.4769239 -1.3521812 -1.6487503 -2.0819619]]...]
INFO - root - 2017-12-07 19:03:40.315160: step 15210, loss = 0.85, batch loss = 0.78 (7.0 examples/sec; 4.542 sec/batch; 85h:40m:52s remains)
INFO - root - 2017-12-07 19:04:25.667379: step 15220, loss = 0.79, batch loss = 0.71 (7.1 examples/sec; 4.520 sec/batch; 85h:15m:36s remains)
INFO - root - 2017-12-07 19:05:11.310856: step 15230, loss = 0.78, batch loss = 0.71 (6.9 examples/sec; 4.609 sec/batch; 86h:56m:01s remains)
INFO - root - 2017-12-07 19:05:56.582713: step 15240, loss = 0.71, batch loss = 0.63 (7.0 examples/sec; 4.564 sec/batch; 86h:04m:03s remains)
INFO - root - 2017-12-07 19:06:42.111496: step 15250, loss = 0.85, batch loss = 0.77 (7.0 examples/sec; 4.567 sec/batch; 86h:06m:01s remains)
INFO - root - 2017-12-07 19:07:27.553509: step 15260, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.568 sec/batch; 86h:07m:10s remains)
INFO - root - 2017-12-07 19:08:13.095816: step 15270, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.476 sec/batch; 84h:21m:53s remains)
INFO - root - 2017-12-07 19:08:58.657931: step 15280, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.587 sec/batch; 86h:27m:09s remains)
INFO - root - 2017-12-07 19:09:43.995184: step 15290, loss = 0.74, batch loss = 0.66 (7.1 examples/sec; 4.509 sec/batch; 84h:57m:14s remains)
INFO - root - 2017-12-07 19:10:29.536245: step 15300, loss = 0.81, batch loss = 0.73 (6.9 examples/sec; 4.630 sec/batch; 87h:14m:16s remains)
2017-12-07 19:10:32.294427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8038049 -2.7778831 -2.8225079 -2.8286667 -2.7737508 -2.7530615 -2.7888303 -2.7788062 -2.6938152 -2.6274953 -2.5928035 -2.5631537 -2.558455 -2.5589185 -2.5376143][-2.7624345 -2.7404885 -2.7798693 -2.7914968 -2.7516661 -2.7298203 -2.7413363 -2.7180336 -2.6383867 -2.5755548 -2.5467191 -2.5298254 -2.5313864 -2.5318861 -2.5155637][-2.7546561 -2.7627647 -2.8125739 -2.8390877 -2.8159034 -2.7837808 -2.7614865 -2.7168043 -2.6345572 -2.5640106 -2.5292325 -2.5172973 -2.5218148 -2.5222554 -2.5122738][-2.7503886 -2.8010552 -2.8725059 -2.9272523 -2.9365335 -2.9079151 -2.8601131 -2.7968416 -2.7081518 -2.6214695 -2.5666807 -2.5438409 -2.5399566 -2.5350358 -2.5290332][-2.7241831 -2.8023047 -2.8820629 -2.9591293 -3.0021536 -2.9860237 -2.9301915 -2.8705235 -2.7985854 -2.7129698 -2.6448107 -2.6104908 -2.5967402 -2.5835104 -2.5755558][-2.7411213 -2.8423805 -2.9297361 -3.0233517 -3.0871921 -3.0711241 -2.9990883 -2.9381461 -2.8841412 -2.8079839 -2.7363734 -2.6994081 -2.6858869 -2.6699591 -2.6586585][-2.7699265 -2.8923264 -2.9891126 -3.0959392 -3.1765919 -3.1625547 -3.0750458 -3.0053432 -2.9615088 -2.8939996 -2.8211493 -2.7859163 -2.7834971 -2.7746186 -2.7618115][-2.7915845 -2.9126482 -3.0060029 -3.115057 -3.20925 -3.2120194 -3.1353471 -3.0723834 -3.0398073 -2.9778948 -2.9022646 -2.8647842 -2.8703108 -2.867826 -2.8527076][-2.8341033 -2.9350076 -3.009958 -3.1001258 -3.1897168 -3.2073522 -3.1541443 -3.1107821 -3.0942869 -3.0439467 -2.9673262 -2.92023 -2.921639 -2.9187469 -2.9001484][-2.8958707 -2.967443 -3.0189795 -3.0828421 -3.15489 -3.1753979 -3.1371279 -3.10541 -3.0960259 -3.0563295 -2.9851599 -2.9334354 -2.929759 -2.925571 -2.9084938][-2.9489894 -2.9959288 -3.0335207 -3.0807891 -3.1333146 -3.1477909 -3.1146536 -3.0807557 -3.0583045 -3.0131674 -2.9446044 -2.8925142 -2.8862441 -2.8840151 -2.8764515][-2.9709482 -3.0051458 -3.0417888 -3.0814557 -3.1109219 -3.1088338 -3.07263 -3.0281398 -2.9818945 -2.9195051 -2.84947 -2.7972109 -2.7877338 -2.7881825 -2.7911637][-2.9852819 -3.0147161 -3.0539834 -3.0854826 -3.0850601 -3.0573912 -3.0159264 -2.9665103 -2.902972 -2.8261635 -2.7550869 -2.7009926 -2.6817091 -2.6765065 -2.6810775][-3.0204217 -3.0521073 -3.0929279 -3.1172786 -3.0879431 -3.0360174 -2.9903448 -2.9395604 -2.8642488 -2.7780631 -2.7080312 -2.651829 -2.6190376 -2.602042 -2.6010387][-3.0683298 -3.1003504 -3.1409564 -3.1611195 -3.1090372 -3.0391345 -2.9952421 -2.9543777 -2.8807607 -2.7963581 -2.7331758 -2.6765511 -2.6294775 -2.5928872 -2.5732696]]...]
INFO - root - 2017-12-07 19:11:18.265669: step 15310, loss = 0.68, batch loss = 0.61 (7.1 examples/sec; 4.534 sec/batch; 85h:23m:59s remains)
INFO - root - 2017-12-07 19:12:03.739737: step 15320, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.582 sec/batch; 86h:18m:17s remains)
INFO - root - 2017-12-07 19:12:49.521117: step 15330, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.557 sec/batch; 85h:48m:36s remains)
INFO - root - 2017-12-07 19:13:35.288730: step 15340, loss = 0.90, batch loss = 0.83 (7.1 examples/sec; 4.506 sec/batch; 84h:51m:03s remains)
INFO - root - 2017-12-07 19:14:21.151523: step 15350, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.628 sec/batch; 87h:07m:45s remains)
INFO - root - 2017-12-07 19:15:06.908379: step 15360, loss = 0.66, batch loss = 0.59 (7.0 examples/sec; 4.569 sec/batch; 86h:00m:46s remains)
INFO - root - 2017-12-07 19:15:52.295427: step 15370, loss = 0.74, batch loss = 0.66 (6.9 examples/sec; 4.642 sec/batch; 87h:21m:36s remains)
INFO - root - 2017-12-07 19:16:38.182863: step 15380, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.531 sec/batch; 85h:15m:22s remains)
INFO - root - 2017-12-07 19:17:23.569210: step 15390, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.579 sec/batch; 86h:09m:07s remains)
INFO - root - 2017-12-07 19:18:09.430126: step 15400, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.548 sec/batch; 85h:33m:26s remains)
2017-12-07 19:18:12.144417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1042688 -1.3291471 -1.1143582 -0.62250495 -0.27996492 -0.314723 -0.77226615 -1.3195188 -1.6117206 -1.560816 -1.3719893 -1.2918391 -1.2147992 -1.0389173 -0.9068737][-0.93320322 -0.93719435 -0.74411297 -0.46816969 -0.31681442 -0.45084357 -0.89253235 -1.3757415 -1.6959729 -1.7739096 -1.6915293 -1.5402718 -1.2330468 -0.86145258 -0.76407695][-0.98948956 -0.84858632 -0.72744966 -0.64514565 -0.6831758 -0.97462869 -1.4374104 -1.8817444 -2.1872056 -2.2559059 -2.0719323 -1.706243 -1.2257581 -0.8474133 -0.9621222][-1.0867026 -0.98652148 -1.050226 -1.1443431 -1.3120151 -1.6484582 -1.9758768 -2.2938309 -2.5910842 -2.7004538 -2.413358 -1.8584328 -1.3157165 -1.0762007 -1.4486282][-1.1385953 -1.182014 -1.48422 -1.7304204 -1.9003751 -1.9796622 -1.8428016 -1.8722816 -2.241092 -2.5621643 -2.3490734 -1.8152194 -1.3911791 -1.3349998 -1.8380454][-1.15435 -1.3093402 -1.7069221 -1.9259622 -1.860719 -1.4543395 -0.75463986 -0.61519 -1.3100417 -2.0814245 -2.112721 -1.7049496 -1.3761082 -1.3335638 -1.7453244][-0.90979767 -1.2500379 -1.7202985 -1.8219604 -1.3610334 -0.30221367 0.99148417 1.1614957 -0.12211657 -1.6242254 -2.1513777 -1.9935062 -1.60081 -1.2621326 -1.328675][-0.61715055 -1.1752846 -1.6974449 -1.6651013 -0.899364 0.63066912 2.2591658 2.3063788 0.52768087 -1.4571397 -2.33652 -2.4106567 -2.033776 -1.4704328 -1.1435821][-0.71264672 -1.2754049 -1.6069324 -1.4419582 -0.74927211 0.51844692 1.7172432 1.4434404 -0.25870609 -1.881665 -2.4943743 -2.5749445 -2.3366828 -1.8285885 -1.3622079][-1.2968049 -1.700026 -1.780925 -1.6231594 -1.3271449 -0.65717578 -0.064211845 -0.469918 -1.6834307 -2.5698555 -2.7035141 -2.7105436 -2.637383 -2.2808287 -1.8363652][-2.1326847 -2.3246338 -2.282511 -2.2597694 -2.3200462 -2.0102527 -1.6815133 -1.9955261 -2.7053111 -2.9793916 -2.7460575 -2.6721559 -2.7542796 -2.6478252 -2.4014249][-2.7377524 -2.7371595 -2.6163564 -2.6754475 -2.9062624 -2.8125644 -2.6436651 -2.8813143 -3.1943557 -3.01436 -2.4833865 -2.2668381 -2.4181373 -2.5898714 -2.666502][-2.8245473 -2.7723372 -2.6355836 -2.6965621 -2.9473591 -2.9672823 -2.8576255 -2.9978275 -3.0985532 -2.7870457 -2.2673702 -2.0479615 -2.2102468 -2.4988627 -2.7185717][-2.6943965 -2.6264811 -2.5223517 -2.5849292 -2.75925 -2.7418795 -2.5686128 -2.5789685 -2.6151006 -2.4259958 -2.1538875 -2.0958138 -2.2661393 -2.5312643 -2.7010162][-2.5868773 -2.5089273 -2.3898077 -2.3661044 -2.4428284 -2.4278848 -2.2798927 -2.229528 -2.2390296 -2.1646376 -2.0730238 -2.1039019 -2.2289824 -2.4157276 -2.5219195]]...]
INFO - root - 2017-12-07 19:18:57.755806: step 15410, loss = 0.76, batch loss = 0.68 (6.9 examples/sec; 4.609 sec/batch; 86h:41m:17s remains)
INFO - root - 2017-12-07 19:19:43.349089: step 15420, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.599 sec/batch; 86h:29m:54s remains)
INFO - root - 2017-12-07 19:20:28.622464: step 15430, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 4.630 sec/batch; 87h:04m:07s remains)
INFO - root - 2017-12-07 19:21:14.213121: step 15440, loss = 0.76, batch loss = 0.68 (6.9 examples/sec; 4.616 sec/batch; 86h:47m:13s remains)
INFO - root - 2017-12-07 19:22:00.210243: step 15450, loss = 0.73, batch loss = 0.65 (7.0 examples/sec; 4.555 sec/batch; 85h:38m:10s remains)
INFO - root - 2017-12-07 19:22:46.020101: step 15460, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.494 sec/batch; 84h:28m:04s remains)
INFO - root - 2017-12-07 19:23:31.639328: step 15470, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.576 sec/batch; 85h:59m:22s remains)
INFO - root - 2017-12-07 19:24:17.457141: step 15480, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 4.547 sec/batch; 85h:26m:26s remains)
INFO - root - 2017-12-07 19:25:03.024823: step 15490, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.630 sec/batch; 86h:58m:58s remains)
INFO - root - 2017-12-07 19:25:49.037722: step 15500, loss = 0.73, batch loss = 0.66 (6.8 examples/sec; 4.686 sec/batch; 88h:01m:25s remains)
2017-12-07 19:25:51.694542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.044065 -2.9316535 -2.991065 -3.2245848 -3.1826282 -2.9746878 -2.8958316 -2.9026518 -2.997565 -3.1194382 -3.2220035 -3.3159943 -3.3313403 -3.2585747 -3.2576051][-3.0321412 -2.9433937 -3.0216455 -3.2463164 -3.2055464 -2.9978762 -2.898911 -2.9045489 -3.0091567 -3.1256828 -3.2271605 -3.3479543 -3.3992014 -3.3539042 -3.3414922][-3.004961 -2.9397011 -3.0219393 -3.2112756 -3.1587083 -2.961307 -2.871129 -2.9007809 -3.0058298 -3.0986362 -3.1745019 -3.2912741 -3.3662477 -3.3443575 -3.3195634][-2.9612505 -2.9158523 -3.0038536 -3.136271 -3.0490284 -2.8476391 -2.7640624 -2.8265004 -2.9463687 -3.0341985 -3.0943584 -3.1813798 -3.249136 -3.2331414 -3.1964965][-2.9198818 -2.8845835 -2.9644489 -3.0195422 -2.8764973 -2.6472158 -2.5521328 -2.6561317 -2.8429124 -2.9720459 -3.0276909 -3.0521369 -3.0585477 -3.0205398 -2.9815931][-2.9012895 -2.8764164 -2.9398646 -2.8948145 -2.6368515 -2.2994504 -2.11647 -2.225925 -2.5215483 -2.7658749 -2.8832369 -2.8874993 -2.8397188 -2.7815092 -2.7429473][-2.9704723 -2.9862585 -3.0670846 -2.9686863 -2.595022 -2.0832887 -1.7173991 -1.7405169 -2.0696762 -2.4023371 -2.6257498 -2.6770434 -2.6047366 -2.5227752 -2.4805694][-3.0849218 -3.1610384 -3.2976427 -3.2437394 -2.8524163 -2.2312741 -1.6992204 -1.5724173 -1.7678287 -2.0130897 -2.2288671 -2.296236 -2.2204118 -2.1399944 -2.0967221][-3.1405458 -3.2617853 -3.4791181 -3.5671854 -3.3011658 -2.7260737 -2.1850374 -1.9658968 -1.9608068 -1.9639523 -2.0095849 -2.0065227 -1.9125159 -1.8540092 -1.8229516][-3.116518 -3.2458103 -3.5349975 -3.7864933 -3.7069261 -3.2905989 -2.9181371 -2.7792149 -2.7013731 -2.5156078 -2.3428879 -2.1669989 -1.9655969 -1.8889484 -1.8961825][-3.0503204 -3.150039 -3.4697926 -3.8241642 -3.888042 -3.6532378 -3.516952 -3.5657098 -3.5629601 -3.357631 -3.1059399 -2.8266015 -2.5266407 -2.3912303 -2.3968368][-3.0073466 -3.0528784 -3.3433537 -3.7222002 -3.856369 -3.7541194 -3.8110528 -4.0213652 -4.1131577 -4.004693 -3.8619535 -3.6739004 -3.414537 -3.2579021 -3.2029006][-3.0254357 -3.0267663 -3.2609017 -3.5963955 -3.7025187 -3.6237459 -3.7542615 -4.0161071 -4.1253228 -4.1066885 -4.1203151 -4.1158733 -4.0137053 -3.9271765 -3.8457303][-3.0839007 -3.0725276 -3.2621741 -3.546885 -3.5796492 -3.4397125 -3.5127888 -3.6893198 -3.7205505 -3.7197134 -3.8211915 -3.9493623 -4.0086503 -4.0550756 -4.0370369][-3.1317122 -3.1246288 -3.2996428 -3.5692244 -3.568867 -3.3586271 -3.3056633 -3.3418584 -3.2885056 -3.2747512 -3.377975 -3.5294282 -3.6758592 -3.8417418 -3.9197714]]...]
INFO - root - 2017-12-07 19:26:37.346209: step 15510, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.622 sec/batch; 86h:48m:20s remains)
INFO - root - 2017-12-07 19:27:23.157229: step 15520, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.560 sec/batch; 85h:37m:36s remains)
INFO - root - 2017-12-07 19:28:08.835173: step 15530, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.561 sec/batch; 85h:38m:14s remains)
INFO - root - 2017-12-07 19:28:54.603979: step 15540, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.581 sec/batch; 85h:59m:33s remains)
INFO - root - 2017-12-07 19:29:40.501359: step 15550, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 4.534 sec/batch; 85h:06m:24s remains)
INFO - root - 2017-12-07 19:30:26.310949: step 15560, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.560 sec/batch; 85h:35m:04s remains)
INFO - root - 2017-12-07 19:31:11.639948: step 15570, loss = 0.83, batch loss = 0.75 (7.1 examples/sec; 4.534 sec/batch; 85h:05m:17s remains)
INFO - root - 2017-12-07 19:31:56.980126: step 15580, loss = 0.88, batch loss = 0.81 (7.0 examples/sec; 4.586 sec/batch; 86h:02m:14s remains)
INFO - root - 2017-12-07 19:32:42.490660: step 15590, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.579 sec/batch; 85h:53m:44s remains)
INFO - root - 2017-12-07 19:33:28.019779: step 15600, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.543 sec/batch; 85h:12m:54s remains)
2017-12-07 19:33:30.744875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0114608 -2.0186234 -1.9409759 -1.8339124 -1.7184544 -1.8301125 -2.1519363 -2.2835863 -2.0209711 -1.5903015 -1.2461782 -1.1686428 -1.1502166 -1.2547257 -1.5039394][-1.8719506 -1.9104528 -1.8313835 -1.7196629 -1.6223004 -1.8035657 -2.1183279 -1.9967303 -1.5085506 -1.0435779 -0.85521364 -0.912544 -0.94679546 -1.1789231 -1.400641][-2.1730988 -2.0233254 -1.8183579 -1.6079853 -1.4347551 -1.5274751 -1.6262863 -1.2224059 -0.63134074 -0.28827572 -0.37397385 -0.64075112 -0.86868191 -1.2830646 -1.4616408][-2.2194202 -1.8499441 -1.5629549 -1.3377759 -1.1153643 -1.0358577 -0.79505348 -0.12735224 0.42976236 0.46025562 0.10254288 -0.32953024 -0.78571987 -1.3279138 -1.4746754][-1.9389637 -1.5358801 -1.3474436 -1.2178426 -1.0258224 -0.85426927 -0.40996885 0.37307787 0.68569851 0.34371567 -0.11974621 -0.52257156 -1.0011106 -1.4730024 -1.5295866][-1.7201345 -1.5134592 -1.5207341 -1.4389579 -1.2072995 -1.0157516 -0.53068614 0.23872232 0.26877832 -0.35479403 -0.74326539 -0.98247814 -1.3334608 -1.6266587 -1.6007628][-1.464576 -1.4935212 -1.6182098 -1.4857376 -1.1165714 -0.84813452 -0.28804016 0.5192976 0.45716858 -0.32949734 -0.7744782 -0.99346256 -1.3017275 -1.5335741 -1.5244994][-1.1236172 -1.1957984 -1.2731214 -1.0580969 -0.59560108 -0.26438046 0.37448835 1.2446871 1.2608075 0.48665619 -0.054587841 -0.35940123 -0.73180342 -1.0309319 -1.0721278][-0.99160171 -0.97134709 -0.8653419 -0.55772638 -0.06187439 0.25616503 0.79132223 1.5959539 1.738214 1.1114688 0.59236622 0.26356506 -0.18909979 -0.56352234 -0.62818503][-0.91762567 -0.79597735 -0.60223866 -0.32771873 0.071217537 0.26844311 0.58220482 1.1963506 1.3998237 1.0428944 0.73297262 0.45739365 0.0054531097 -0.29751778 -0.27935934][-0.84298658 -0.60705566 -0.36652184 -0.16358566 0.066771507 0.077850819 0.19300556 0.62529755 0.85706711 0.74532986 0.693871 0.47246456 0.029611588 -0.11944962 0.00097417831][-0.69830203 -0.43175554 -0.2317996 -0.14069939 -0.089468956 -0.21769857 -0.15621996 0.23586082 0.52204561 0.60949421 0.717999 0.49120283 0.042135239 0.015480042 0.21515703][-0.8876214 -0.65610862 -0.49619961 -0.45056725 -0.47126341 -0.59423447 -0.45444155 -0.028009415 0.25583696 0.38048506 0.47191668 0.19498873 -0.19363022 -0.12474537 0.13153076][-1.403502 -1.057596 -0.81720567 -0.78270364 -0.85302687 -0.927721 -0.72393084 -0.34345627 -0.13980103 -0.079623222 -0.078953266 -0.38707113 -0.68492174 -0.56724834 -0.29415894][-2.0918698 -1.6459687 -1.3981006 -1.452457 -1.5953009 -1.602757 -1.3600769 -1.0997324 -1.007962 -1.0121276 -1.0428808 -1.3133574 -1.5558727 -1.4630842 -1.2476106]]...]
INFO - root - 2017-12-07 19:34:16.300602: step 15610, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 4.515 sec/batch; 84h:40m:39s remains)
INFO - root - 2017-12-07 19:35:01.847530: step 15620, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.556 sec/batch; 85h:25m:28s remains)
INFO - root - 2017-12-07 19:35:47.593304: step 15630, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.499 sec/batch; 84h:20m:26s remains)
INFO - root - 2017-12-07 19:36:33.224725: step 15640, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.519 sec/batch; 84h:42m:41s remains)
INFO - root - 2017-12-07 19:37:19.088116: step 15650, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.577 sec/batch; 85h:47m:04s remains)
INFO - root - 2017-12-07 19:38:04.894213: step 15660, loss = 0.87, batch loss = 0.79 (7.1 examples/sec; 4.529 sec/batch; 84h:52m:54s remains)
INFO - root - 2017-12-07 19:38:50.691224: step 15670, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.576 sec/batch; 85h:44m:37s remains)
INFO - root - 2017-12-07 19:39:36.535421: step 15680, loss = 0.85, batch loss = 0.78 (7.1 examples/sec; 4.526 sec/batch; 84h:48m:06s remains)
INFO - root - 2017-12-07 19:40:22.563852: step 15690, loss = 0.81, batch loss = 0.73 (6.9 examples/sec; 4.624 sec/batch; 86h:37m:25s remains)
INFO - root - 2017-12-07 19:41:08.296297: step 15700, loss = 0.77, batch loss = 0.69 (7.0 examples/sec; 4.546 sec/batch; 85h:08m:00s remains)
2017-12-07 19:41:10.942542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4293373 -3.3351874 -3.3189039 -3.4146137 -3.5477488 -3.6871989 -3.7896137 -3.7462373 -3.5932264 -3.3570881 -3.0574565 -2.9137301 -3.1015339 -3.4853876 -3.7280383][-3.6455991 -3.33759 -3.1488252 -3.236165 -3.4498608 -3.7085714 -3.8491297 -3.7139421 -3.4519424 -3.08665 -2.6144342 -2.3309827 -2.5029709 -3.081471 -3.6303964][-3.8632281 -3.2206669 -2.740581 -2.7240624 -2.9608026 -3.307708 -3.5168166 -3.3560586 -3.0316851 -2.547873 -1.9073112 -1.5125401 -1.5911438 -2.2352757 -3.0895586][-4.0373917 -3.0954313 -2.3086779 -2.0811982 -2.2068298 -2.5273027 -2.7988751 -2.7485645 -2.4871929 -1.9586248 -1.2826412 -0.9369545 -0.9475944 -1.4911599 -2.4597642][-4.1865263 -3.1096957 -2.0693026 -1.5036979 -1.2853606 -1.3483317 -1.5940921 -1.8166854 -1.8689456 -1.4959743 -0.99316382 -0.88248658 -0.94369316 -1.3522794 -2.2010815][-4.0812855 -3.1384559 -2.0369353 -1.1417656 -0.45015478 0.0055699348 0.0050115585 -0.52050853 -1.0143955 -0.93868685 -0.74518013 -0.94923019 -1.1905026 -1.5776491 -2.2402103][-3.8464792 -3.2222273 -2.2459824 -1.1162982 0.094770908 1.3230596 1.8968496 1.14708 0.18405867 -0.12194633 -0.33508778 -0.85383749 -1.3511028 -1.8728945 -2.4383111][-3.6952913 -3.3042789 -2.4289713 -1.1651857 0.38530064 2.2028503 3.3385468 2.4748621 1.1794558 0.493402 -0.14716053 -0.92201781 -1.6141062 -2.2630496 -2.7575457][-3.5482001 -3.2963047 -2.5716796 -1.4769001 -0.098816872 1.6638145 2.8514352 2.2183976 1.0304508 0.22187471 -0.60680294 -1.3927791 -2.0691788 -2.7009411 -3.0823426][-3.3428874 -3.2064004 -2.7114391 -1.9920726 -1.0659363 0.28535748 1.2474151 0.9022522 0.058740139 -0.64150548 -1.3755057 -1.9599328 -2.4926631 -3.0118361 -3.2752519][-3.1449957 -3.0449862 -2.7116714 -2.2836111 -1.7954328 -0.96631074 -0.37188005 -0.53776288 -1.0061467 -1.4774396 -1.9818308 -2.3615863 -2.7695174 -3.1455109 -3.313221][-3.0324614 -2.8932168 -2.61153 -2.3542781 -2.219311 -1.872839 -1.6274278 -1.7211032 -1.9039834 -2.1672659 -2.4828444 -2.7412612 -3.0458066 -3.2770343 -3.3576369][-3.0171573 -2.8522141 -2.6057315 -2.4592237 -2.5471315 -2.5160465 -2.4746065 -2.5665441 -2.6524811 -2.8068662 -3.002007 -3.1808453 -3.3734741 -3.4721937 -3.4638782][-3.1336942 -3.0253215 -2.8682647 -2.785677 -2.9395485 -3.0415754 -3.1076689 -3.2263422 -3.2926717 -3.3735576 -3.4759412 -3.5683951 -3.6507087 -3.6433656 -3.5626066][-3.2738011 -3.2536826 -3.1966991 -3.1537805 -3.2834606 -3.4165163 -3.5204768 -3.628166 -3.6834054 -3.7225943 -3.7572398 -3.7628098 -3.7448397 -3.6707587 -3.560575]]...]
INFO - root - 2017-12-07 19:41:56.809561: step 15710, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 4.549 sec/batch; 85h:11m:24s remains)
INFO - root - 2017-12-07 19:42:42.705656: step 15720, loss = 0.80, batch loss = 0.72 (6.8 examples/sec; 4.708 sec/batch; 88h:09m:21s remains)
INFO - root - 2017-12-07 19:43:28.414302: step 15730, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.594 sec/batch; 86h:00m:32s remains)
INFO - root - 2017-12-07 19:44:14.290986: step 15740, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.583 sec/batch; 85h:46m:54s remains)
INFO - root - 2017-12-07 19:45:00.016587: step 15750, loss = 0.62, batch loss = 0.55 (6.9 examples/sec; 4.642 sec/batch; 86h:52m:43s remains)
INFO - root - 2017-12-07 19:45:45.715973: step 15760, loss = 0.73, batch loss = 0.65 (7.0 examples/sec; 4.601 sec/batch; 86h:05m:35s remains)
INFO - root - 2017-12-07 19:46:31.687292: step 15770, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.644 sec/batch; 86h:53m:37s remains)
INFO - root - 2017-12-07 19:47:17.547383: step 15780, loss = 0.76, batch loss = 0.68 (6.9 examples/sec; 4.628 sec/batch; 86h:34m:54s remains)
INFO - root - 2017-12-07 19:48:02.990115: step 15790, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.587 sec/batch; 85h:47m:26s remains)
INFO - root - 2017-12-07 19:48:49.079167: step 15800, loss = 0.72, batch loss = 0.64 (7.0 examples/sec; 4.581 sec/batch; 85h:40m:44s remains)
2017-12-07 19:48:51.876898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8705077 -2.8665709 -2.83355 -2.7796202 -2.7602286 -2.7900097 -2.8319056 -2.8467479 -2.8292267 -2.817169 -2.8017187 -2.752198 -2.6772847 -2.6155305 -2.5927703][-2.9073606 -2.8432174 -2.7483587 -2.6608105 -2.6536298 -2.7437453 -2.8564529 -2.916908 -2.9233198 -2.9237947 -2.9031496 -2.836982 -2.7590806 -2.7147756 -2.718297][-2.9636705 -2.8438501 -2.7152538 -2.6447186 -2.6962059 -2.8575771 -2.9910612 -3.0007184 -2.9620056 -2.9538665 -2.94772 -2.9000852 -2.8491333 -2.8169751 -2.8297658][-3.0493269 -2.9060125 -2.8129282 -2.809227 -2.907269 -3.0535369 -3.1030512 -2.994657 -2.8986459 -2.9108281 -2.9699874 -2.9890079 -2.9871812 -2.9419584 -2.9155431][-3.1195252 -2.998507 -2.9954338 -3.077518 -3.1481421 -3.1498146 -3.0304046 -2.8124409 -2.6947546 -2.7570236 -2.8852148 -2.9848676 -3.0412707 -3.0006332 -2.9410083][-3.2643809 -3.2526731 -3.3529618 -3.4113164 -3.2884209 -3.0129433 -2.7045872 -2.4395494 -2.3347425 -2.4190536 -2.5672402 -2.7020092 -2.7903378 -2.8023109 -2.8027678][-3.4509995 -3.547174 -3.6675644 -3.5627594 -3.136498 -2.550457 -2.1297772 -1.93714 -1.9312046 -2.0708978 -2.2472904 -2.382931 -2.4448743 -2.497591 -2.6069078][-3.4755301 -3.5469337 -3.5503256 -3.2354825 -2.5982704 -1.8919511 -1.5538664 -1.6087365 -1.8201935 -2.0878685 -2.3311896 -2.4466624 -2.412595 -2.4125123 -2.5483029][-3.3246598 -3.3511786 -3.2624168 -2.8489392 -2.1983328 -1.5915902 -1.4316165 -1.6993537 -2.0751061 -2.4365952 -2.7258706 -2.8126383 -2.6864357 -2.5769377 -2.6358888][-3.2242703 -3.2939038 -3.2526116 -2.9300919 -2.46386 -2.0769203 -2.0555818 -2.3672988 -2.7336884 -3.0592794 -3.30381 -3.3355546 -3.1491842 -2.9396238 -2.8477492][-3.2727313 -3.4602628 -3.5501683 -3.3936105 -3.0965395 -2.8428473 -2.8443918 -3.0780084 -3.3446794 -3.5564551 -3.6982248 -3.687036 -3.5061121 -3.27817 -3.0826373][-3.4105747 -3.7005403 -3.8801212 -3.8236012 -3.6196256 -3.4142008 -3.3923 -3.5164995 -3.6470048 -3.7139244 -3.7334957 -3.6873024 -3.5511758 -3.3781455 -3.1786768][-3.4572182 -3.7836075 -3.9932408 -4.0001349 -3.874866 -3.7030392 -3.6391883 -3.6394799 -3.6442924 -3.6190429 -3.5593796 -3.4864278 -3.3890519 -3.2722325 -3.1049633][-3.325841 -3.5982933 -3.7794411 -3.81798 -3.7646434 -3.6481924 -3.5610676 -3.4835315 -3.4313815 -3.3995466 -3.3360052 -3.2699494 -3.2008286 -3.1024165 -2.952929][-3.1103411 -3.3099165 -3.4586632 -3.5238693 -3.5338376 -3.4756985 -3.3974831 -3.3018332 -3.2417836 -3.2294841 -3.1774948 -3.1083655 -3.040452 -2.9417357 -2.805017]]...]
INFO - root - 2017-12-07 19:49:37.613708: step 15810, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.622 sec/batch; 86h:25m:37s remains)
INFO - root - 2017-12-07 19:50:23.131349: step 15820, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.539 sec/batch; 84h:51m:47s remains)
INFO - root - 2017-12-07 19:51:08.969818: step 15830, loss = 0.68, batch loss = 0.61 (6.9 examples/sec; 4.611 sec/batch; 86h:11m:42s remains)
INFO - root - 2017-12-07 19:51:54.703953: step 15840, loss = 0.78, batch loss = 0.71 (6.9 examples/sec; 4.606 sec/batch; 86h:05m:30s remains)
INFO - root - 2017-12-07 19:52:40.151121: step 15850, loss = 0.85, batch loss = 0.78 (7.3 examples/sec; 4.399 sec/batch; 82h:12m:24s remains)
INFO - root - 2017-12-07 19:53:25.894084: step 15860, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.598 sec/batch; 85h:54m:12s remains)
INFO - root - 2017-12-07 19:54:11.618372: step 15870, loss = 0.77, batch loss = 0.69 (7.0 examples/sec; 4.584 sec/batch; 85h:37m:48s remains)
INFO - root - 2017-12-07 19:54:57.419085: step 15880, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.625 sec/batch; 86h:23m:46s remains)
INFO - root - 2017-12-07 19:55:43.167748: step 15890, loss = 0.78, batch loss = 0.70 (6.9 examples/sec; 4.626 sec/batch; 86h:24m:21s remains)
INFO - root - 2017-12-07 19:56:28.999096: step 15900, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.625 sec/batch; 86h:21m:46s remains)
2017-12-07 19:56:31.748498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3987126 -1.4409997 -1.3755262 -1.2148688 -0.8561573 -0.77709532 -1.0672181 -0.81539631 -0.33454132 -0.46834373 -0.71928477 -0.85703015 -1.1859641 -1.5992205 -1.7372789][-1.4517858 -1.5788507 -1.5114224 -1.2136805 -0.71817517 -0.66654444 -1.0476303 -0.93124461 -0.6718781 -0.89660215 -1.061048 -1.1068203 -1.3981531 -1.8777435 -2.3084328][-1.4129832 -1.5319233 -1.4571605 -1.1606066 -0.69050884 -0.664763 -1.0181627 -0.99324894 -1.0087023 -1.4053855 -1.5975819 -1.5910418 -1.7525196 -2.1353476 -2.6815281][-1.2742934 -1.2643888 -1.1400852 -0.94791436 -0.68621373 -0.78525662 -1.1646252 -1.2623274 -1.49755 -2.0458264 -2.3367164 -2.307261 -2.2725625 -2.4229889 -2.877408][-1.0576055 -0.8934505 -0.73652029 -0.69097638 -0.6916604 -1.0014689 -1.5423615 -1.8221273 -2.1661718 -2.7317414 -3.0158834 -2.8728302 -2.6020207 -2.4922767 -2.7710218][-0.74576139 -0.50647926 -0.37854958 -0.49694395 -0.77618718 -1.366467 -2.1425281 -2.5481737 -2.8170764 -3.1953216 -3.2940488 -2.9585271 -2.4821856 -2.1796174 -2.3265555][-0.54060245 -0.3886714 -0.37813854 -0.62962413 -1.0931077 -1.8672879 -2.756393 -3.1248136 -3.1498013 -3.2181425 -3.0943582 -2.6682353 -2.154969 -1.8449354 -2.0145056][-0.61150861 -0.71314383 -0.87339306 -1.1757848 -1.6264918 -2.3464684 -3.136982 -3.3423386 -3.1117182 -2.9086034 -2.6862612 -2.3495255 -1.944185 -1.7576609 -2.057812][-0.85681677 -1.1979392 -1.4868152 -1.7300477 -1.9991684 -2.5301776 -3.1519704 -3.2326849 -2.852489 -2.4841263 -2.2636936 -2.1104445 -1.869453 -1.8303468 -2.2220635][-1.1387196 -1.5066369 -1.8087177 -1.9548087 -2.0092766 -2.3540611 -2.855814 -2.9068842 -2.50809 -2.0808992 -1.8906808 -1.8935723 -1.7913821 -1.8579869 -2.2374346][-1.3988817 -1.6082525 -1.8360112 -1.9132953 -1.8445163 -2.0621257 -2.4062788 -2.3818221 -2.0056946 -1.6322913 -1.5506949 -1.6971908 -1.705308 -1.8000834 -2.0510848][-1.5149648 -1.5655546 -1.708643 -1.7128277 -1.5713792 -1.6583939 -1.8085091 -1.7099855 -1.441983 -1.2349281 -1.3058097 -1.6113629 -1.7925873 -1.9556494 -2.1147022][-1.4151585 -1.4066966 -1.5059686 -1.4363751 -1.2506344 -1.1415062 -1.0299337 -0.981776 -1.0679595 -1.1634436 -1.4028196 -1.786113 -2.047189 -2.2835352 -2.4401221][-1.2257824 -1.2132244 -1.2689309 -1.1658905 -1.098197 -0.93676972 -0.65547061 -0.75396633 -1.2144608 -1.5483592 -1.8063979 -2.0587142 -2.2068343 -2.4371402 -2.6053381][-1.1539528 -1.147819 -1.1399038 -1.0316491 -1.1617146 -1.1617863 -0.96610188 -1.2626131 -1.8715136 -2.1310539 -2.1597731 -2.1007693 -2.0551848 -2.2787449 -2.4831135]]...]
INFO - root - 2017-12-07 19:57:17.701560: step 15910, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 4.621 sec/batch; 86h:17m:05s remains)
INFO - root - 2017-12-07 19:58:03.227884: step 15920, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.536 sec/batch; 84h:41m:03s remains)
INFO - root - 2017-12-07 19:58:48.813020: step 15930, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.577 sec/batch; 85h:26m:10s remains)
INFO - root - 2017-12-07 19:59:34.162648: step 15940, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 4.479 sec/batch; 83h:35m:25s remains)
INFO - root - 2017-12-07 20:00:19.649771: step 15950, loss = 0.75, batch loss = 0.68 (7.2 examples/sec; 4.467 sec/batch; 83h:20m:45s remains)
INFO - root - 2017-12-07 20:01:04.905443: step 15960, loss = 0.85, batch loss = 0.78 (7.1 examples/sec; 4.518 sec/batch; 84h:17m:39s remains)
INFO - root - 2017-12-07 20:01:50.444925: step 15970, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.558 sec/batch; 85h:01m:23s remains)
INFO - root - 2017-12-07 20:02:35.887713: step 15980, loss = 0.87, batch loss = 0.80 (7.1 examples/sec; 4.528 sec/batch; 84h:26m:49s remains)
INFO - root - 2017-12-07 20:03:21.708491: step 15990, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.622 sec/batch; 86h:12m:07s remains)
INFO - root - 2017-12-07 20:04:06.972178: step 16000, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.560 sec/batch; 85h:01m:19s remains)
2017-12-07 20:04:09.765571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4180889 -1.7808547 -1.1528649 -0.68318653 -0.22363186 -0.35199022 -0.242558 0.07505846 -0.27772427 -1.7687395 -3.1863165 -4.0636978 -3.8663955 -2.3240485 -0.55218554][-2.5977468 -2.0844071 -1.4582069 -0.92322636 -0.23023891 0.010326862 0.40446854 0.64901924 0.06157589 -1.5437975 -2.9633393 -3.77239 -3.3614459 -1.7121747 -0.18899059][-2.6087184 -2.3906455 -1.8813622 -1.3463981 -0.45646381 0.064926147 0.51744986 0.56116581 -0.16887283 -1.6642346 -2.9301214 -3.5360553 -2.8955107 -1.2197909 0.1517067][-2.4586029 -2.6784391 -2.3658974 -1.7457099 -0.53489065 0.30074692 0.66362572 0.42005062 -0.48604751 -1.9205928 -2.9957404 -3.3293762 -2.5225325 -0.88853526 0.44642639][-2.4583349 -3.1193583 -2.9434819 -2.1195922 -0.48160028 0.75547838 1.0972204 0.7124567 -0.41585493 -1.986902 -3.0972059 -3.3889751 -2.6232173 -1.130847 0.1211071][-2.7869291 -3.8175569 -3.753392 -2.7034581 -0.627965 1.069943 1.6176014 1.422121 0.18900299 -1.6283178 -2.8827739 -3.3049936 -2.7911901 -1.5986135 -0.66049647][-3.1058831 -4.2589684 -4.2445025 -2.9787176 -0.55235982 1.3895898 2.1528111 2.1896386 0.83596706 -1.1032214 -2.4188139 -3.054009 -2.8650799 -2.0458 -1.5215173][-3.4050655 -4.4241867 -4.35511 -2.8071308 -0.16970158 1.6786747 2.4269257 2.4832153 1.0736208 -0.6655426 -1.918678 -2.7259946 -2.7700489 -2.2691395 -2.1406512][-3.6526687 -4.4443111 -4.2362971 -2.4332695 0.08818531 1.4652586 1.9072351 1.8360457 0.74259806 -0.40562534 -1.3864179 -2.2399309 -2.3573382 -2.1150379 -2.3810627][-3.8546202 -4.4598641 -4.1104312 -2.2463384 -0.12229586 0.81881905 1.0868006 0.9855032 0.34432316 -0.12434626 -0.74552941 -1.4724419 -1.6239011 -1.6851916 -2.3682463][-4.2003856 -4.7366176 -4.2666879 -2.416693 -0.60245633 0.17172813 0.42977715 0.30517244 0.016084671 0.021904945 -0.22257423 -0.68387985 -0.85518026 -1.2651596 -2.322324][-4.5760732 -5.1145477 -4.45274 -2.5284662 -0.82019854 -0.13684464 -0.11199903 -0.41166592 -0.46805835 -0.088214874 0.074871063 -0.14035845 -0.3666048 -0.97389841 -2.1247175][-4.7721114 -5.2125912 -4.23056 -2.171196 -0.56181026 -0.17452145 -0.65666771 -1.2525444 -1.1549184 -0.46410036 0.024221897 -0.025567055 -0.27182293 -0.87907934 -1.9580727][-4.5542159 -4.8652472 -3.6923311 -1.7279358 -0.46326566 -0.47416663 -1.3226242 -2.0699489 -1.8335652 -1.0207009 -0.41725063 -0.32414675 -0.41263914 -0.82875323 -1.7571201][-4.0400968 -4.327219 -3.2056303 -1.5445347 -0.67671156 -0.84333396 -1.6897385 -2.3429449 -2.0470877 -1.3246214 -0.78800893 -0.52287436 -0.48155212 -0.82966661 -1.6929388]]...]
INFO - root - 2017-12-07 20:04:55.191295: step 16010, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.577 sec/batch; 85h:20m:04s remains)
INFO - root - 2017-12-07 20:05:40.909292: step 16020, loss = 0.78, batch loss = 0.70 (7.1 examples/sec; 4.491 sec/batch; 83h:42m:41s remains)
INFO - root - 2017-12-07 20:06:26.321727: step 16030, loss = 0.74, batch loss = 0.67 (7.2 examples/sec; 4.418 sec/batch; 82h:20m:14s remains)
INFO - root - 2017-12-07 20:07:11.855798: step 16040, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.533 sec/batch; 84h:28m:09s remains)
INFO - root - 2017-12-07 20:07:57.171441: step 16050, loss = 0.87, batch loss = 0.80 (7.1 examples/sec; 4.505 sec/batch; 83h:55m:55s remains)
INFO - root - 2017-12-07 20:08:42.922638: step 16060, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.523 sec/batch; 84h:16m:08s remains)
INFO - root - 2017-12-07 20:09:28.095616: step 16070, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.542 sec/batch; 84h:36m:03s remains)
INFO - root - 2017-12-07 20:10:13.579297: step 16080, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.551 sec/batch; 84h:45m:47s remains)
INFO - root - 2017-12-07 20:10:59.072819: step 16090, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.593 sec/batch; 85h:31m:57s remains)
INFO - root - 2017-12-07 20:11:44.569058: step 16100, loss = 0.75, batch loss = 0.67 (7.1 examples/sec; 4.508 sec/batch; 83h:55m:55s remains)
2017-12-07 20:11:47.421167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7656264 -3.1774211 -3.2504425 -2.9457345 -2.1066132 -1.2385006 -1.2417576 -1.6713269 -1.7550108 -1.9381042 -2.5722635 -2.7830124 -2.0624917 -1.1545677 -0.50839043][-2.6267354 -3.0629547 -3.0489807 -2.635746 -1.7892253 -1.0100188 -1.2054534 -1.7406442 -1.7743683 -1.9203126 -2.48637 -2.6680758 -1.9770887 -0.97739267 -0.29550076][-2.4397287 -2.9175971 -2.8058357 -2.2750475 -1.4722958 -0.80002165 -1.0173666 -1.3764291 -1.2495515 -1.508239 -2.1815956 -2.3768249 -1.8063734 -0.83140612 -0.1765871][-2.2557797 -2.7934251 -2.5739138 -1.9420595 -1.2730865 -0.77489281 -0.89474154 -0.87850523 -0.52249122 -0.98469257 -1.858866 -2.0320399 -1.5141711 -0.4766624 0.22629023][-2.0973139 -2.6759524 -2.3766296 -1.7342873 -1.2967908 -1.0601106 -1.0776432 -0.69071579 -0.13911819 -0.79759145 -1.8283238 -1.9157009 -1.36814 -0.20075226 0.604167][-1.9763262 -2.5528631 -2.2371235 -1.7035372 -1.4977508 -1.4106522 -1.270467 -0.5841217 0.14339209 -0.621758 -1.8066635 -1.9854198 -1.5767035 -0.4123497 0.4474597][-1.9759305 -2.4969194 -2.2189305 -1.8305261 -1.6982141 -1.5093265 -1.075866 -0.065688133 0.8715992 0.0085310936 -1.4858589 -2.0676808 -2.0650446 -1.141454 -0.31244946][-2.2440789 -2.6576927 -2.4102941 -2.082334 -1.7993431 -1.3120477 -0.56376934 0.67643023 1.7600098 0.80693626 -1.0016925 -2.0309367 -2.4591718 -1.8562889 -1.1073537][-2.6833606 -2.9235849 -2.6993449 -2.3865809 -1.9324782 -1.2514222 -0.40411329 0.83965826 2.0178361 1.2528396 -0.59561324 -1.8986022 -2.6491728 -2.3743992 -1.7887332][-3.0684402 -3.1626253 -3.0262733 -2.7876067 -2.305804 -1.6518493 -0.90698171 0.25306463 1.5785537 1.2550573 -0.377419 -1.8071327 -2.8355112 -2.9297218 -2.5835934][-3.3747263 -3.3964639 -3.3632631 -3.1935577 -2.737916 -2.2036297 -1.6171792 -0.54374003 0.84922981 0.88107109 -0.47418618 -1.8730941 -3.0152407 -3.3800492 -3.253408][-3.4382348 -3.4093347 -3.4325433 -3.29395 -2.8846149 -2.4911141 -2.0715866 -1.1428063 0.093387127 0.21005249 -0.89116573 -2.0592303 -3.0450518 -3.4578927 -3.4389949][-3.170212 -3.1492009 -3.2223029 -3.12778 -2.8307679 -2.6008952 -2.3524792 -1.6471722 -0.71720672 -0.65494394 -1.4647229 -2.2574346 -2.9241056 -3.2341013 -3.2369733][-2.8580825 -2.9109066 -3.0640955 -3.0652957 -2.9225323 -2.8268189 -2.6892405 -2.1875191 -1.5438449 -1.5058868 -1.987031 -2.4112353 -2.7761958 -2.973423 -3.0106564][-2.6694016 -2.7786169 -2.9672313 -3.0330994 -2.9806528 -2.9260924 -2.8070815 -2.4572437 -2.0583639 -2.0462701 -2.2921402 -2.4754705 -2.6517327 -2.7943454 -2.8806467]]...]
INFO - root - 2017-12-07 20:12:32.479907: step 16110, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.547 sec/batch; 84h:38m:27s remains)
INFO - root - 2017-12-07 20:13:17.920548: step 16120, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.498 sec/batch; 83h:43m:19s remains)
INFO - root - 2017-12-07 20:14:03.447987: step 16130, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.587 sec/batch; 85h:21m:51s remains)
INFO - root - 2017-12-07 20:14:49.236837: step 16140, loss = 0.86, batch loss = 0.79 (6.8 examples/sec; 4.694 sec/batch; 87h:20m:09s remains)
INFO - root - 2017-12-07 20:15:35.085959: step 16150, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.583 sec/batch; 85h:15m:30s remains)
INFO - root - 2017-12-07 20:16:21.074239: step 16160, loss = 0.79, batch loss = 0.71 (6.9 examples/sec; 4.643 sec/batch; 86h:22m:28s remains)
INFO - root - 2017-12-07 20:17:06.530398: step 16170, loss = 0.71, batch loss = 0.63 (7.0 examples/sec; 4.561 sec/batch; 84h:49m:51s remains)
INFO - root - 2017-12-07 20:17:52.550869: step 16180, loss = 0.84, batch loss = 0.77 (6.9 examples/sec; 4.611 sec/batch; 85h:45m:07s remains)
INFO - root - 2017-12-07 20:18:38.181287: step 16190, loss = 0.69, batch loss = 0.62 (7.2 examples/sec; 4.468 sec/batch; 83h:04m:18s remains)
INFO - root - 2017-12-07 20:19:24.477492: step 16200, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.599 sec/batch; 85h:29m:39s remains)
2017-12-07 20:19:27.205758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2295532 -2.6387486 -2.6095574 -2.2398944 -1.8918304 -1.597167 -1.5649524 -2.0804744 -2.8082271 -2.84847 -2.207535 -1.5051208 -1.3423686 -2.0555358 -3.1185384][-1.3207624 -1.8280907 -1.8724725 -1.5780237 -1.4718807 -1.2703192 -1.2908103 -1.9299514 -2.7091718 -2.7868643 -2.0753312 -1.073879 -0.60329032 -1.1475084 -2.3620353][-0.29977703 -0.67768526 -0.65866971 -0.43994451 -0.60648942 -0.64081573 -0.8479352 -1.7437346 -2.6777368 -2.9193773 -2.3404222 -1.2198567 -0.49911141 -0.76983905 -1.9342191][0.074392319 0.039941311 0.37737846 0.70091486 0.3688221 0.041856289 -0.41515636 -1.588027 -2.7761779 -3.3153851 -3.0524871 -2.0742204 -1.2032571 -1.1653671 -2.0712304][-0.59623647 -0.22591305 0.56012487 1.1853995 0.96330404 0.66846275 0.18125725 -1.1237702 -2.6057143 -3.5952888 -3.8295832 -3.2781882 -2.4664569 -2.1633918 -2.667594][-1.6241918 -0.9764421 0.14927006 1.110754 1.2052274 1.3314986 1.1348429 -0.20539856 -2.0575116 -3.6208465 -4.4348378 -4.3940573 -3.7601485 -3.2510123 -3.3439534][-2.5421214 -1.7412624 -0.40669918 0.78320026 1.2668085 2.0365319 2.4107709 1.1511121 -1.0423272 -3.1761088 -4.6084442 -5.11327 -4.7304091 -4.0889525 -3.8647671][-3.0197968 -2.1759691 -0.82652164 0.387187 1.2074246 2.6341977 3.7566051 2.7501636 0.37292957 -2.1863904 -4.2208595 -5.3025346 -5.2096047 -4.5607786 -4.1785116][-2.8836827 -2.1399827 -0.97105193 0.06205368 0.98532391 2.7688107 4.4945841 3.9239616 1.7189307 -0.86653209 -3.2794878 -4.9113288 -5.174 -4.7172141 -4.3660889][-2.6479213 -1.9767175 -0.96857929 -0.21865749 0.51938009 2.1740122 3.9573498 3.9541779 2.3900957 0.23204994 -2.1950531 -4.1772561 -4.826468 -4.6889234 -4.4910097][-2.8151209 -2.1283171 -1.1776311 -0.67278838 -0.29172516 0.83703709 2.1778936 2.6101151 1.9586172 0.58094549 -1.4446018 -3.4672298 -4.4659705 -4.6628194 -4.5999274][-3.2308483 -2.534008 -1.6606762 -1.3644111 -1.332875 -0.82764888 -0.062161446 0.53952169 0.68950653 0.14062929 -1.2310367 -2.9669559 -4.1111746 -4.5286345 -4.547183][-3.5755463 -2.9429564 -2.2539232 -2.105675 -2.2776504 -2.2432144 -1.988255 -1.5260506 -1.0214655 -1.0115211 -1.7116766 -2.869287 -3.8232739 -4.264564 -4.3198032][-3.78542 -3.2749922 -2.7725277 -2.6775637 -2.8776162 -3.045464 -3.0819376 -2.889442 -2.4699664 -2.2700882 -2.5176558 -3.0954163 -3.6470788 -3.9427211 -3.993166][-3.7980261 -3.470437 -3.1517642 -3.0771465 -3.1940205 -3.3484254 -3.4791284 -3.496829 -3.3308721 -3.1828215 -3.2118626 -3.3853574 -3.578666 -3.6938896 -3.716244]]...]
INFO - root - 2017-12-07 20:20:12.580748: step 16210, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.569 sec/batch; 84h:56m:03s remains)
INFO - root - 2017-12-07 20:20:58.282069: step 16220, loss = 0.91, batch loss = 0.84 (7.0 examples/sec; 4.600 sec/batch; 85h:29m:17s remains)
INFO - root - 2017-12-07 20:21:44.274357: step 16230, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.639 sec/batch; 86h:12m:16s remains)
INFO - root - 2017-12-07 20:22:30.053516: step 16240, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.575 sec/batch; 85h:00m:25s remains)
INFO - root - 2017-12-07 20:23:15.677749: step 16250, loss = 0.61, batch loss = 0.54 (7.3 examples/sec; 4.413 sec/batch; 81h:58m:33s remains)
INFO - root - 2017-12-07 20:24:01.131525: step 16260, loss = 0.70, batch loss = 0.63 (7.2 examples/sec; 4.468 sec/batch; 82h:59m:13s remains)
INFO - root - 2017-12-07 20:24:47.046753: step 16270, loss = 0.80, batch loss = 0.72 (7.1 examples/sec; 4.508 sec/batch; 83h:42m:55s remains)
INFO - root - 2017-12-07 20:25:32.534539: step 16280, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.594 sec/batch; 85h:18m:22s remains)
INFO - root - 2017-12-07 20:26:18.292512: step 16290, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.560 sec/batch; 84h:39m:39s remains)
INFO - root - 2017-12-07 20:27:03.914933: step 16300, loss = 0.78, batch loss = 0.71 (6.8 examples/sec; 4.672 sec/batch; 86h:43m:38s remains)
2017-12-07 20:27:06.724348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7937727 -2.5701058 -2.3356843 -2.2759402 -2.0152841 -1.5137141 -1.1897492 -1.2371457 -1.6531761 -2.2790022 -2.6777282 -2.7757249 -2.7948627 -2.7477093 -2.7311907][-2.953495 -2.8327684 -2.7218702 -2.8784127 -2.7341561 -2.2647312 -1.9729919 -1.9065506 -2.0919285 -2.4826441 -2.6919525 -2.6661956 -2.6587439 -2.6215072 -2.6150324][-3.0861549 -3.0564609 -3.0943832 -3.4094315 -3.2438197 -2.6733947 -2.4240885 -2.4291654 -2.5909843 -2.8961411 -3.0038314 -2.8549595 -2.7375574 -2.6234286 -2.5624559][-3.0645456 -3.0488555 -3.1853986 -3.5390716 -3.2427807 -2.4776487 -2.1810539 -2.271683 -2.5024123 -2.8677468 -3.0982256 -3.0821347 -2.9982991 -2.8315091 -2.6644251][-3.1104221 -3.0262465 -3.1286588 -3.3108749 -2.7553549 -1.8036523 -1.4713101 -1.6515784 -1.9216549 -2.2944589 -2.6379604 -2.8718634 -3.0098319 -2.9936953 -2.8522022][-3.2226729 -3.0982914 -3.1429713 -3.0680771 -2.1926975 -1.041343 -0.59844327 -0.75022292 -1.0462956 -1.4926291 -1.9900584 -2.4511957 -2.7811503 -2.9419436 -2.9252586][-3.2999942 -3.1388283 -3.1709766 -2.9894896 -2.0212352 -0.80367541 -0.11224937 0.11837387 0.061532021 -0.42167091 -1.1576643 -1.942488 -2.5301759 -2.8306355 -2.8922319][-3.3921125 -3.1523089 -3.1645296 -2.993372 -2.1997604 -1.2836661 -0.65239739 -0.17034531 0.20615816 0.16681623 -0.28408909 -1.09975 -1.9708996 -2.565 -2.8333006][-3.4070656 -3.0852199 -3.039434 -2.8816872 -2.304409 -1.8017356 -1.504344 -1.2230823 -0.88438272 -0.565964 -0.37227345 -0.66020465 -1.3748093 -2.0765848 -2.5717049][-3.3289404 -2.9121242 -2.7668691 -2.601233 -2.2471614 -2.1034515 -2.0580904 -2.0059397 -1.9833357 -1.7777982 -1.3978801 -1.2667489 -1.5301545 -1.876199 -2.2489674][-3.2386153 -2.7841997 -2.54818 -2.3549283 -2.184242 -2.3487463 -2.4961662 -2.5378175 -2.6806402 -2.6023228 -2.2732182 -2.0231905 -2.0278471 -2.0590672 -2.2084694][-3.1201801 -2.6664553 -2.4184027 -2.2400253 -2.2041683 -2.5422573 -2.8494668 -3.0073018 -3.2440357 -3.1896205 -2.8493967 -2.5141926 -2.3490808 -2.2251585 -2.2671218][-3.0359664 -2.5466738 -2.2904339 -2.1317632 -2.1782424 -2.5744171 -2.956404 -3.189702 -3.4281454 -3.3872004 -3.1099129 -2.8157396 -2.5629539 -2.3181949 -2.2583826][-3.060194 -2.5950255 -2.3779883 -2.2341421 -2.2261086 -2.5262945 -2.8659697 -3.0707736 -3.1614313 -3.0506034 -2.8885508 -2.7789016 -2.5940285 -2.3645821 -2.2793472][-3.0515659 -2.733124 -2.6578689 -2.5656538 -2.4294209 -2.5395586 -2.7952685 -2.9165511 -2.7824631 -2.5391221 -2.4609404 -2.4768324 -2.3449709 -2.1844778 -2.1717215]]...]
INFO - root - 2017-12-07 20:27:52.667051: step 16310, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.576 sec/batch; 84h:55m:57s remains)
INFO - root - 2017-12-07 20:28:38.053883: step 16320, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.609 sec/batch; 85h:31m:20s remains)
INFO - root - 2017-12-07 20:29:23.589840: step 16330, loss = 0.86, batch loss = 0.79 (7.0 examples/sec; 4.576 sec/batch; 84h:53m:54s remains)
INFO - root - 2017-12-07 20:30:09.073844: step 16340, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.560 sec/batch; 84h:36m:05s remains)
INFO - root - 2017-12-07 20:30:54.907024: step 16350, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.615 sec/batch; 85h:35m:49s remains)
INFO - root - 2017-12-07 20:31:40.426102: step 16360, loss = 0.70, batch loss = 0.62 (7.2 examples/sec; 4.464 sec/batch; 82h:47m:36s remains)
INFO - root - 2017-12-07 20:32:26.016933: step 16370, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.580 sec/batch; 84h:55m:23s remains)
INFO - root - 2017-12-07 20:33:11.570834: step 16380, loss = 0.87, batch loss = 0.80 (6.9 examples/sec; 4.620 sec/batch; 85h:39m:33s remains)
INFO - root - 2017-12-07 20:33:56.682991: step 16390, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 4.548 sec/batch; 84h:19m:01s remains)
INFO - root - 2017-12-07 20:34:42.695395: step 16400, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.588 sec/batch; 85h:02m:13s remains)
2017-12-07 20:34:45.407751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.86308074 -0.89937663 -0.96513009 -0.97685575 -0.95727038 -0.84375954 -0.79727268 -0.84709549 -0.80409193 -0.67613721 -0.44753885 -0.14629602 -0.070407867 -0.35741615 -0.5040741][-0.94842649 -1.0552878 -1.0996983 -1.0946307 -1.1153011 -1.0767186 -1.0903542 -1.1332228 -0.91340923 -0.51025105 -0.043627262 0.40910053 0.56174517 0.22845459 -0.1003952][-0.6946671 -0.835551 -0.90500259 -1.0208757 -1.2458866 -1.3480632 -1.3617399 -1.2809241 -0.90983415 -0.444376 -0.0075588226 0.40153122 0.60864735 0.49180984 0.31516266][-0.3555789 -0.487952 -0.60398364 -0.9008956 -1.3395882 -1.5391321 -1.5039802 -1.2851102 -0.89205623 -0.52161336 -0.23998165 0.0066623688 0.13011217 0.15017986 0.20539093][-0.16444492 -0.23403263 -0.31669044 -0.64341521 -1.1165578 -1.3916135 -1.4439247 -1.2905581 -0.93828821 -0.55370092 -0.24361849 -0.076001167 -0.12445307 -0.18949223 -0.042827606][-0.20860004 -0.20393753 -0.1568408 -0.34666443 -0.72216082 -1.0633051 -1.2718265 -1.1960363 -0.7925806 -0.2760787 0.090418339 0.11767578 -0.21597862 -0.43283796 -0.15662956][-0.35258913 -0.24934244 -0.060682297 -0.13009024 -0.42593288 -0.73340058 -0.8898735 -0.66676617 -0.12647009 0.42988634 0.65538454 0.39880657 -0.21421766 -0.54184747 -0.21915436][-0.72072339 -0.54990816 -0.28526306 -0.24877739 -0.38639164 -0.46338558 -0.39552736 0.010326862 0.575706 1.0445261 1.1328201 0.71319818 0.035079956 -0.34889221 -0.17700052][-1.1728597 -1.0831442 -0.85546374 -0.70593 -0.62657309 -0.48805404 -0.35093784 -0.071360111 0.29598427 0.66677141 0.82583523 0.54290581 0.038694859 -0.29806852 -0.27721071][-1.4428406 -1.5929646 -1.5415871 -1.3386457 -1.099735 -0.83850694 -0.72143221 -0.60475516 -0.41639471 -0.16331482 0.011799812 -0.06940937 -0.30946112 -0.53061867 -0.63271904][-1.5701234 -1.9469774 -2.110528 -1.9557819 -1.7230875 -1.4834659 -1.3337622 -1.0876551 -0.74902272 -0.48880935 -0.38044882 -0.44414568 -0.58448505 -0.74088 -0.97583485][-1.4961593 -1.9412224 -2.2479885 -2.2499139 -2.1799958 -2.1169682 -1.9941585 -1.5949509 -1.1070795 -0.80613184 -0.60439825 -0.53051925 -0.56670046 -0.72586012 -1.0934343][-1.3683662 -1.6818459 -1.9655464 -2.0886102 -2.1705315 -2.2656281 -2.2148745 -1.8424368 -1.4851501 -1.2845745 -0.9756906 -0.68656325 -0.56174183 -0.6468308 -0.97991967][-1.1833701 -1.3447337 -1.6125815 -1.81763 -1.9359546 -2.0365255 -1.96086 -1.6308088 -1.4535022 -1.3865881 -1.1320012 -0.88935685 -0.76846695 -0.75132775 -0.899369][-0.67006326 -0.68125987 -0.98774529 -1.3408043 -1.5347428 -1.6683185 -1.5957518 -1.3414371 -1.3016634 -1.2902565 -1.0993264 -0.98710513 -0.951473 -0.90881515 -0.96687007]]...]
INFO - root - 2017-12-07 20:35:31.026404: step 16410, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.611 sec/batch; 85h:27m:25s remains)
INFO - root - 2017-12-07 20:36:16.652003: step 16420, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 4.625 sec/batch; 85h:42m:03s remains)
INFO - root - 2017-12-07 20:37:02.053390: step 16430, loss = 0.66, batch loss = 0.59 (7.0 examples/sec; 4.571 sec/batch; 84h:40m:54s remains)
INFO - root - 2017-12-07 20:37:47.698196: step 16440, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.568 sec/batch; 84h:37m:21s remains)
INFO - root - 2017-12-07 20:38:33.270144: step 16450, loss = 0.72, batch loss = 0.64 (7.0 examples/sec; 4.553 sec/batch; 84h:19m:25s remains)
INFO - root - 2017-12-07 20:39:18.886533: step 16460, loss = 0.82, batch loss = 0.74 (7.0 examples/sec; 4.565 sec/batch; 84h:31m:36s remains)
INFO - root - 2017-12-07 20:40:04.507727: step 16470, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.583 sec/batch; 84h:50m:57s remains)
INFO - root - 2017-12-07 20:40:50.258561: step 16480, loss = 0.64, batch loss = 0.57 (6.9 examples/sec; 4.640 sec/batch; 85h:54m:04s remains)
INFO - root - 2017-12-07 20:41:35.806638: step 16490, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.592 sec/batch; 84h:59m:50s remains)
INFO - root - 2017-12-07 20:42:21.250665: step 16500, loss = 0.86, batch loss = 0.78 (7.1 examples/sec; 4.507 sec/batch; 83h:24m:24s remains)
2017-12-07 20:42:23.993636: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12136698 0.38005781 0.591383 0.58696461 0.49515963 0.53002882 0.73756313 0.95908403 1.0313506 0.92416382 0.82504034 0.94310808 1.1881499 1.3477507 1.142158][0.25407267 0.62076044 0.75242424 0.56168222 0.34371185 0.32159662 0.52370691 0.84289169 1.0831032 1.1433434 1.1340561 1.2226419 1.3356485 1.3761978 1.0948324][0.48712873 0.78032589 0.72438478 0.33635569 0.011609077 -0.054389 0.18651724 0.66550779 1.1284885 1.3831034 1.441587 1.4007411 1.2686052 1.117352 0.76327562][0.90646458 0.9467206 0.57712269 -0.067200184 -0.49341631 -0.51393676 -0.084609509 0.6791687 1.3848605 1.7808747 1.7994957 1.5340018 1.1164131 0.76476765 0.36008263][1.2538285 1.0331173 0.37115145 -0.47582889 -0.9315958 -0.79039454 -0.036305428 1.0578961 1.9346581 2.3071642 2.1342907 1.6056385 1.0115919 0.59774828 0.23917103][1.1672921 0.77256918 0.030785084 -0.70290351 -0.91449142 -0.47732639 0.58031034 1.8538566 2.6601005 2.7080889 2.1251645 1.3038516 0.64120817 0.32448673 0.13050508][0.81630754 0.32917547 -0.27260876 -0.62429643 -0.42105293 0.27969408 1.4018016 2.5419307 2.9924469 2.5836148 1.6573334 0.66048527 0.0069489479 -0.1525774 -0.13255978][0.41398335 0.012257099 -0.31857347 -0.28873348 0.18653202 0.87474394 1.6705065 2.2783504 2.3065562 1.7829275 0.9828968 0.19992828 -0.25169659 -0.25716496 -0.15175819][0.054986477 -0.050550938 -0.027280331 0.24278879 0.68279791 1.0324049 1.2561631 1.2788711 1.0680885 0.73175907 0.35737896 0.049939156 -0.028960228 0.12472391 0.2018199][0.028841972 0.18668318 0.45367575 0.76361942 0.9389782 0.8545928 0.62460423 0.33987856 0.11879778 0.012498856 -0.07060957 -0.05876112 0.1263938 0.37710762 0.42877054][0.15378571 0.30516005 0.54391575 0.74066877 0.69742107 0.40878868 0.06636 -0.18330622 -0.23349857 -0.16044569 -0.1285243 -0.0064921379 0.23604679 0.43716288 0.41789341][0.25588369 0.27274323 0.33220863 0.3524518 0.20836687 -0.066156864 -0.30787325 -0.40163708 -0.27309752 -0.070152283 0.066987514 0.29665422 0.54409981 0.63076735 0.4773159][0.49888325 0.3979311 0.2344923 0.063117981 -0.11200809 -0.25770426 -0.31404114 -0.25257635 -0.041815281 0.1913805 0.39786291 0.71408081 0.95680571 0.93538952 0.64224148][0.67615128 0.517992 0.25624037 0.040672302 -0.056668282 -0.028587341 0.085003853 0.22999334 0.39850378 0.51971769 0.6467061 0.90364647 1.0877376 1.0001163 0.64842272][0.83170128 0.63433981 0.39704752 0.28378963 0.30225372 0.4231143 0.60645008 0.76445341 0.83337879 0.7820878 0.72424316 0.81533861 0.92412519 0.86942387 0.58469915]]...]
INFO - root - 2017-12-07 20:43:09.724994: step 16510, loss = 0.76, batch loss = 0.68 (7.1 examples/sec; 4.514 sec/batch; 83h:31m:09s remains)
INFO - root - 2017-12-07 20:43:55.335533: step 16520, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.572 sec/batch; 84h:35m:50s remains)
INFO - root - 2017-12-07 20:44:40.880503: step 16530, loss = 0.75, batch loss = 0.68 (7.6 examples/sec; 4.227 sec/batch; 78h:11m:27s remains)
INFO - root - 2017-12-07 20:45:26.532377: step 16540, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.590 sec/batch; 84h:53m:13s remains)
INFO - root - 2017-12-07 20:46:12.209576: step 16550, loss = 0.80, batch loss = 0.73 (6.9 examples/sec; 4.617 sec/batch; 85h:22m:29s remains)
INFO - root - 2017-12-07 20:46:57.675151: step 16560, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.599 sec/batch; 85h:02m:07s remains)
INFO - root - 2017-12-07 20:47:43.286284: step 16570, loss = 0.88, batch loss = 0.81 (6.9 examples/sec; 4.640 sec/batch; 85h:47m:14s remains)
INFO - root - 2017-12-07 20:48:28.958726: step 16580, loss = 0.93, batch loss = 0.86 (7.3 examples/sec; 4.412 sec/batch; 81h:33m:45s remains)
INFO - root - 2017-12-07 20:49:14.638695: step 16590, loss = 0.74, batch loss = 0.66 (7.0 examples/sec; 4.573 sec/batch; 84h:31m:30s remains)
INFO - root - 2017-12-07 20:50:00.248273: step 16600, loss = 0.79, batch loss = 0.71 (7.1 examples/sec; 4.537 sec/batch; 83h:50m:12s remains)
2017-12-07 20:50:02.917155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.869513 -3.2856081 -3.4907515 -3.8869686 -4.0832005 -3.8800569 -3.7548914 -3.5780933 -3.3429551 -3.2576778 -3.3237512 -3.492023 -3.7250016 -3.9081509 -3.8911803][-2.9390748 -3.2710986 -3.5065963 -3.8961344 -4.0483375 -3.8829846 -3.758914 -3.5158005 -3.2573748 -3.1526213 -3.2419615 -3.5160904 -3.8418157 -4.0762644 -4.0812054][-2.9744282 -3.1903102 -3.44007 -3.8375614 -4.0149918 -3.9888926 -3.9122667 -3.6248937 -3.3343625 -3.1777205 -3.2542732 -3.6132543 -4.019321 -4.2963352 -4.3596454][-3.0164926 -3.1462595 -3.4274535 -3.8133163 -3.9619944 -3.9782553 -3.8918867 -3.6002817 -3.3715644 -3.2474213 -3.3464861 -3.7554464 -4.1705966 -4.4540944 -4.6080241][-3.033596 -3.1416097 -3.5367727 -3.9574404 -4.0307 -3.9748981 -3.8053598 -3.4804173 -3.3097818 -3.2659919 -3.43134 -3.8845344 -4.29056 -4.5406427 -4.7459764][-3.0057144 -3.1276948 -3.699115 -4.2261138 -4.2541246 -4.1606655 -3.9449906 -3.5564029 -3.3391101 -3.3217506 -3.4950824 -3.9388657 -4.3130589 -4.5285749 -4.7743711][-2.9081478 -2.9601371 -3.5497661 -4.0791922 -4.0771422 -4.0200777 -3.9013681 -3.5544777 -3.32227 -3.3232017 -3.4374266 -3.804873 -4.1473279 -4.33945 -4.5768452][-2.8110583 -2.7924943 -3.2912095 -3.7139876 -3.6647065 -3.6518865 -3.7142007 -3.5376427 -3.4061923 -3.4669337 -3.4669762 -3.6324377 -3.8525827 -3.9306297 -4.0683112][-2.8430996 -2.8495841 -3.271368 -3.5470769 -3.3978369 -3.3652723 -3.5461626 -3.5275669 -3.5264809 -3.6680927 -3.5932291 -3.5778577 -3.6643772 -3.61453 -3.6302309][-2.9842613 -3.0434747 -3.3953424 -3.5366426 -3.3320603 -3.3252621 -3.5722613 -3.5900078 -3.601409 -3.7786179 -3.6690719 -3.5595155 -3.5940845 -3.4907026 -3.4208815][-3.254215 -3.346899 -3.6163552 -3.6303601 -3.3989921 -3.4153829 -3.6491766 -3.6294889 -3.608084 -3.7747803 -3.6535234 -3.5304027 -3.5631852 -3.4475007 -3.3415704][-3.4861209 -3.5683298 -3.7353241 -3.6519263 -3.4528012 -3.4895551 -3.6442702 -3.5497117 -3.505096 -3.6700988 -3.5862002 -3.5115719 -3.5881083 -3.51084 -3.4332342][-3.5731158 -3.6124768 -3.6730926 -3.5450051 -3.409317 -3.4678025 -3.526073 -3.3716478 -3.3405337 -3.5005128 -3.4520535 -3.4180906 -3.5083258 -3.4630647 -3.4294314][-3.5836282 -3.5951922 -3.5956645 -3.462976 -3.3722041 -3.4299498 -3.4208736 -3.2539272 -3.2445486 -3.3636651 -3.3237858 -3.3009157 -3.3844376 -3.3799834 -3.3927298][-3.6215136 -3.6359329 -3.6339862 -3.5349126 -3.4697695 -3.501863 -3.4560881 -3.3199596 -3.3283648 -3.3980932 -3.3452134 -3.3166542 -3.3809571 -3.4039078 -3.4538271]]...]
INFO - root - 2017-12-07 20:50:48.384912: step 16610, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.554 sec/batch; 84h:08m:45s remains)
INFO - root - 2017-12-07 20:51:34.180264: step 16620, loss = 0.73, batch loss = 0.65 (7.1 examples/sec; 4.538 sec/batch; 83h:50m:10s remains)
INFO - root - 2017-12-07 20:52:19.971152: step 16630, loss = 0.85, batch loss = 0.78 (6.9 examples/sec; 4.662 sec/batch; 86h:06m:27s remains)
INFO - root - 2017-12-07 20:53:05.438120: step 16640, loss = 0.77, batch loss = 0.69 (7.1 examples/sec; 4.508 sec/batch; 83h:14m:44s remains)
INFO - root - 2017-12-07 20:53:51.054026: step 16650, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.609 sec/batch; 85h:06m:14s remains)
INFO - root - 2017-12-07 20:54:36.707025: step 16660, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.538 sec/batch; 83h:46m:50s remains)
INFO - root - 2017-12-07 20:55:22.462491: step 16670, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.599 sec/batch; 84h:53m:28s remains)
INFO - root - 2017-12-07 20:56:07.776187: step 16680, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.524 sec/batch; 83h:29m:36s remains)
INFO - root - 2017-12-07 20:56:53.514004: step 16690, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.559 sec/batch; 84h:07m:31s remains)
INFO - root - 2017-12-07 20:57:38.987335: step 16700, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.554 sec/batch; 84h:01m:39s remains)
2017-12-07 20:57:41.640113: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0064816 -4.0742183 -4.0424781 -3.9731562 -3.9307492 -3.9046974 -3.8639293 -3.8389182 -3.8336506 -3.8310785 -3.8242733 -3.8012819 -3.7517738 -3.6986065 -3.6376903][-4.2984457 -4.3457761 -4.2204385 -4.0912147 -4.0552621 -4.0360241 -3.9860117 -3.9624386 -3.9685798 -3.970773 -3.9744973 -3.9286444 -3.8252769 -3.7242405 -3.6121502][-4.4899044 -4.4486966 -4.1831884 -4.0011959 -3.9925504 -3.9763525 -3.8883667 -3.7672989 -3.650517 -3.5713468 -3.5305591 -3.4300227 -3.2898121 -3.210423 -3.140564][-4.5218587 -4.364594 -3.9981527 -3.83639 -3.8778679 -3.8456638 -3.6632285 -3.3858237 -3.1532397 -3.0524011 -3.0195062 -2.9053836 -2.8074694 -2.8071413 -2.7966056][-4.3613091 -4.09118 -3.7198527 -3.6501441 -3.7332358 -3.6724935 -3.4834728 -3.2411628 -3.1342654 -3.1887484 -3.2114046 -3.0813622 -2.9939287 -2.9690289 -2.8538575][-4.0852804 -3.7006273 -3.3409638 -3.2805448 -3.3142879 -3.273489 -3.2743888 -3.3340006 -3.5951509 -3.8885999 -3.920069 -3.6548696 -3.4104519 -3.1804867 -2.8365369][-3.7917447 -3.3262939 -2.8744092 -2.582865 -2.3946805 -2.3590729 -2.5757713 -2.9096096 -3.4114573 -3.7985258 -3.7577138 -3.3491695 -2.923367 -2.5230803 -2.1238372][-3.6519523 -3.1723776 -2.6156855 -2.1030569 -1.7562301 -1.7023475 -1.9361398 -2.3083055 -2.8118384 -3.1072803 -2.9585342 -2.4913597 -2.0273113 -1.6858196 -1.4690475][-3.6641545 -3.2378392 -2.6932962 -2.1600907 -1.8739719 -1.9015422 -2.1239245 -2.4588757 -2.8434424 -3.0152922 -2.8294492 -2.3384502 -1.8404922 -1.6078155 -1.6322267][-3.668015 -3.2520318 -2.7477241 -2.2934158 -2.1400452 -2.3126037 -2.6193616 -2.9682932 -3.254632 -3.3626432 -3.2254171 -2.7723086 -2.2143903 -1.9093204 -1.9208632][-3.6297951 -3.2370427 -2.8654556 -2.575844 -2.5111156 -2.7001147 -2.9531107 -3.197619 -3.3692377 -3.4351747 -3.3363781 -2.9382353 -2.4127548 -2.045882 -1.9485295][-3.6072612 -3.3361664 -3.1396918 -3.0088229 -2.9669809 -3.031105 -3.0619946 -3.120038 -3.2287893 -3.3162141 -3.2264798 -2.8881521 -2.4830475 -2.1806693 -2.0866036][-3.5773377 -3.4504092 -3.3372884 -3.2387595 -3.2039022 -3.2181592 -3.1805725 -3.2011733 -3.3720312 -3.5421572 -3.5294762 -3.3532581 -3.1484904 -2.9668393 -2.8744082][-3.4466598 -3.3427293 -3.208972 -3.1527243 -3.254636 -3.4234343 -3.5353229 -3.6575961 -3.8828166 -4.0499516 -4.0973043 -4.1215081 -4.1333265 -4.0769129 -3.9579303][-3.2563422 -3.1299262 -3.0268211 -3.0744419 -3.2821188 -3.5365109 -3.7344432 -3.898278 -4.0689116 -4.1659422 -4.2143731 -4.2931862 -4.3447394 -4.3063469 -4.1791883]]...]
INFO - root - 2017-12-07 20:58:27.146093: step 16710, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.539 sec/batch; 83h:44m:20s remains)
INFO - root - 2017-12-07 20:59:12.837308: step 16720, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.609 sec/batch; 85h:01m:15s remains)
INFO - root - 2017-12-07 20:59:58.446152: step 16730, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.577 sec/batch; 84h:24m:44s remains)
INFO - root - 2017-12-07 21:00:44.146971: step 16740, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.590 sec/batch; 84h:38m:55s remains)
INFO - root - 2017-12-07 21:01:29.564998: step 16750, loss = 0.68, batch loss = 0.61 (7.1 examples/sec; 4.517 sec/batch; 83h:17m:02s remains)
INFO - root - 2017-12-07 21:02:15.166431: step 16760, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 4.525 sec/batch; 83h:25m:30s remains)
INFO - root - 2017-12-07 21:03:00.739078: step 16770, loss = 0.85, batch loss = 0.77 (6.9 examples/sec; 4.657 sec/batch; 85h:50m:46s remains)
INFO - root - 2017-12-07 21:03:46.635280: step 16780, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.483 sec/batch; 82h:36m:42s remains)
INFO - root - 2017-12-07 21:04:32.060526: step 16790, loss = 0.79, batch loss = 0.71 (6.9 examples/sec; 4.633 sec/batch; 85h:22m:40s remains)
INFO - root - 2017-12-07 21:05:17.876737: step 16800, loss = 0.73, batch loss = 0.65 (7.0 examples/sec; 4.548 sec/batch; 83h:46m:54s remains)
2017-12-07 21:05:20.573270: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5246298 -1.6538308 -1.7618134 -1.8109758 -1.7881849 -1.6964993 -1.5692859 -1.4712505 -1.4308767 -1.4284437 -1.425828 -1.4344404 -1.4531026 -1.4679475 -1.4557629][-1.9306214 -2.1092594 -2.1848598 -2.1368473 -1.9959779 -1.7982664 -1.5929775 -1.5033154 -1.5489221 -1.6328533 -1.6835086 -1.7339616 -1.7663267 -1.7395189 -1.637049][-2.2911875 -2.4492395 -2.4486427 -2.2953441 -2.0426893 -1.7329023 -1.4647455 -1.4664466 -1.7051096 -1.9549813 -2.1076474 -2.2338207 -2.2833328 -2.1781404 -1.912693][-2.4568191 -2.6174695 -2.6052024 -2.4078851 -2.0378685 -1.5498543 -1.1565223 -1.2472136 -1.7278807 -2.1911261 -2.4691384 -2.6585274 -2.7061205 -2.5189674 -2.0770254][-2.7559001 -2.9703991 -2.9802094 -2.704484 -2.1390615 -1.3845291 -0.76969981 -0.87360263 -1.5650363 -2.2245324 -2.6272597 -2.8735616 -2.9394441 -2.7293143 -2.1453748][-3.4015398 -3.6385047 -3.6231778 -3.1987524 -2.4179974 -1.4400258 -0.62091184 -0.68459725 -1.4346302 -2.1260343 -2.5769978 -2.8961477 -3.0655756 -2.9492552 -2.2811906][-4.0602493 -4.2727995 -4.1816216 -3.5267525 -2.5445237 -1.4956739 -0.66027474 -0.69623446 -1.3357058 -1.8980823 -2.350687 -2.7695842 -3.0978737 -3.1433175 -2.4247525][-4.1616192 -4.3501959 -4.1915741 -3.3652468 -2.2932796 -1.3065374 -0.61196828 -0.70892453 -1.2598422 -1.69624 -2.159821 -2.6550448 -3.0605214 -3.1682429 -2.3646905][-3.5217342 -3.7235858 -3.6131628 -2.8735526 -1.9403634 -1.1239426 -0.636919 -0.841609 -1.3419425 -1.650702 -2.0321388 -2.4652214 -2.8151832 -2.8683605 -1.9743645][-2.7943549 -2.9825964 -3.0103269 -2.587115 -1.9371843 -1.2811463 -0.90463877 -1.1082501 -1.5018978 -1.6501663 -1.8783681 -2.2276545 -2.5584459 -2.5690041 -1.6404076][-2.3080707 -2.3966792 -2.5055187 -2.3586121 -1.9383819 -1.4075818 -1.0699775 -1.1981277 -1.4911265 -1.5818865 -1.780858 -2.2099392 -2.6598535 -2.64073 -1.6436126][-1.8616319 -1.9345214 -2.0921488 -2.0579054 -1.7511785 -1.3248112 -1.0230742 -1.073519 -1.3250613 -1.4686291 -1.7672451 -2.3844593 -2.9978981 -2.9219918 -1.7845252][-1.4853458 -1.5830045 -1.7199593 -1.681406 -1.4313583 -1.1029744 -0.84767222 -0.851485 -1.1006839 -1.3172681 -1.7221196 -2.5134311 -3.2552614 -3.120961 -1.8454213][-1.0665488 -1.1213231 -1.1816657 -1.129117 -0.96458173 -0.74091887 -0.52817631 -0.50086784 -0.75366569 -1.0363142 -1.5314445 -2.4611886 -3.3035831 -3.1469207 -1.8053493][-0.51327944 -0.55983424 -0.60548091 -0.59876442 -0.51691318 -0.35207987 -0.15053654 -0.11381721 -0.39325571 -0.72993541 -1.281095 -2.2921898 -3.209372 -3.0768633 -1.7414062]]...]
INFO - root - 2017-12-07 21:06:05.947117: step 16810, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.560 sec/batch; 83h:59m:33s remains)
INFO - root - 2017-12-07 21:06:51.491840: step 16820, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.555 sec/batch; 83h:54m:11s remains)
INFO - root - 2017-12-07 21:07:37.081687: step 16830, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.656 sec/batch; 85h:44m:08s remains)
INFO - root - 2017-12-07 21:08:22.544414: step 16840, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.573 sec/batch; 84h:11m:38s remains)
INFO - root - 2017-12-07 21:09:08.003416: step 16850, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.534 sec/batch; 83h:27m:59s remains)
INFO - root - 2017-12-07 21:09:53.653585: step 16860, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.564 sec/batch; 84h:00m:43s remains)
INFO - root - 2017-12-07 21:10:38.962978: step 16870, loss = 0.71, batch loss = 0.64 (7.3 examples/sec; 4.398 sec/batch; 80h:56m:44s remains)
INFO - root - 2017-12-07 21:11:24.593550: step 16880, loss = 0.68, batch loss = 0.60 (7.1 examples/sec; 4.536 sec/batch; 83h:28m:19s remains)
INFO - root - 2017-12-07 21:12:10.112921: step 16890, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.562 sec/batch; 83h:56m:09s remains)
INFO - root - 2017-12-07 21:12:55.726775: step 16900, loss = 0.69, batch loss = 0.61 (6.9 examples/sec; 4.617 sec/batch; 84h:56m:02s remains)
2017-12-07 21:12:58.435015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0510345 -3.9815888 -3.64907 -3.2062531 -2.7389374 -2.5245063 -2.4978266 -2.3511488 -2.6538796 -3.3646123 -3.5424657 -3.2558427 -3.1039584 -3.1122727 -3.040122][-3.6844106 -3.773736 -3.522305 -3.1562133 -2.7549891 -2.5789587 -2.548347 -2.398788 -2.7067039 -3.404798 -3.5594978 -3.2902849 -3.2013788 -3.2260373 -3.1254644][-3.4977674 -3.7100408 -3.5298243 -3.2453914 -2.9405382 -2.7946625 -2.7049685 -2.5230651 -2.7927756 -3.3776445 -3.4327664 -3.1658726 -3.1304293 -3.1709571 -3.0587113][-3.3476696 -3.5062454 -3.3270149 -3.1251626 -2.9413207 -2.8046615 -2.6453552 -2.4518819 -2.6851258 -3.1417065 -3.1253777 -2.8836813 -2.8891921 -2.9461002 -2.8404212][-3.1282737 -3.0529051 -2.8017821 -2.655195 -2.566196 -2.4377396 -2.2573023 -2.0837605 -2.2633715 -2.6229293 -2.6518314 -2.5227759 -2.5860105 -2.6566057 -2.5647864][-2.6964619 -2.336138 -1.9626226 -1.812053 -1.7691846 -1.6650491 -1.5098429 -1.3524928 -1.4697561 -1.8018606 -1.9576983 -1.9694574 -2.0821724 -2.2092478 -2.2080166][-2.1839557 -1.6897454 -1.2451689 -1.024317 -0.93121934 -0.81670094 -0.6869328 -0.57284737 -0.66957045 -1.0227206 -1.3203375 -1.4300258 -1.5686765 -1.7522786 -1.8678684][-1.4938672 -1.1546173 -0.82140732 -0.57665157 -0.4244628 -0.30190563 -0.22456455 -0.20689392 -0.34310913 -0.71731997 -1.0761585 -1.1972504 -1.303699 -1.4867733 -1.6732254][-0.69985723 -0.58459592 -0.45568132 -0.32900333 -0.24722672 -0.20426178 -0.19223595 -0.22640133 -0.36417675 -0.70466709 -1.0567262 -1.1484103 -1.2108753 -1.3858678 -1.627795][-0.34552765 -0.341156 -0.35656643 -0.33407736 -0.33314848 -0.38884211 -0.44462013 -0.49019623 -0.5910778 -0.86187363 -1.1514246 -1.2062366 -1.2346549 -1.412087 -1.6994042][-0.78020763 -0.77693629 -0.81881404 -0.80553055 -0.8303864 -0.95013165 -1.0573556 -1.0854313 -1.1307039 -1.306638 -1.4945033 -1.5025148 -1.4994469 -1.6591296 -1.9248445][-1.5919554 -1.5796921 -1.6017497 -1.5828235 -1.6129525 -1.7550254 -1.8916545 -1.9064543 -1.887033 -1.9499183 -2.0210516 -1.985208 -1.9581547 -2.0657654 -2.2469513][-2.2875886 -2.3034143 -2.349174 -2.3682282 -2.4192162 -2.5541003 -2.6754065 -2.6748564 -2.6119576 -2.5854154 -2.5596895 -2.4834681 -2.4223509 -2.460331 -2.5487878][-2.8364863 -2.8837507 -2.9360306 -2.9611835 -3.004266 -3.0990243 -3.1732662 -3.1506357 -3.0753875 -3.0147452 -2.9511352 -2.8662095 -2.7856855 -2.7677495 -2.7876084][-3.1150355 -3.1517911 -3.1684604 -3.1597633 -3.167151 -3.2155857 -3.2540817 -3.2330487 -3.17515 -3.1266632 -3.0788698 -3.0190413 -2.9550433 -2.9214611 -2.9100022]]...]
INFO - root - 2017-12-07 21:13:44.213603: step 16910, loss = 0.90, batch loss = 0.83 (7.0 examples/sec; 4.578 sec/batch; 84h:11m:59s remains)
INFO - root - 2017-12-07 21:14:30.077243: step 16920, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 4.606 sec/batch; 84h:42m:51s remains)
INFO - root - 2017-12-07 21:15:15.725674: step 16930, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.579 sec/batch; 84h:11m:16s remains)
INFO - root - 2017-12-07 21:16:01.359528: step 16940, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.568 sec/batch; 83h:58m:42s remains)
INFO - root - 2017-12-07 21:16:47.296514: step 16950, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.518 sec/batch; 83h:02m:32s remains)
INFO - root - 2017-12-07 21:17:32.781044: step 16960, loss = 0.66, batch loss = 0.59 (7.0 examples/sec; 4.579 sec/batch; 84h:09m:19s remains)
INFO - root - 2017-12-07 21:18:18.416283: step 16970, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.581 sec/batch; 84h:10m:25s remains)
INFO - root - 2017-12-07 21:19:03.913296: step 16980, loss = 0.65, batch loss = 0.58 (7.0 examples/sec; 4.541 sec/batch; 83h:26m:09s remains)
INFO - root - 2017-12-07 21:19:49.744614: step 16990, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.561 sec/batch; 83h:46m:48s remains)
INFO - root - 2017-12-07 21:20:35.220249: step 17000, loss = 0.65, batch loss = 0.58 (7.0 examples/sec; 4.582 sec/batch; 84h:09m:48s remains)
2017-12-07 21:20:37.968690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1968863 -3.1779995 -3.127368 -3.0726812 -3.0094066 -2.9786732 -2.9606118 -2.8846562 -2.8353021 -2.8446388 -2.8586235 -2.8556142 -2.8340697 -2.8067603 -2.7978632][-3.2059174 -3.1785281 -3.0952868 -3.0166466 -2.9403968 -2.9109926 -2.8802543 -2.7750878 -2.7233961 -2.7676082 -2.8482952 -2.8784387 -2.8269577 -2.7399166 -2.6491098][-3.3318636 -3.3545585 -3.299788 -3.2485931 -3.2041969 -3.205904 -3.1891844 -3.103178 -3.0785213 -3.178544 -3.3483534 -3.4215312 -3.3377857 -3.1476998 -2.8820608][-3.6546822 -3.7702837 -3.7734742 -3.7761164 -3.7814479 -3.797298 -3.8047769 -3.7974942 -3.8421068 -3.9890006 -4.1813345 -4.2281246 -4.0600305 -3.749934 -3.311821][-3.97986 -4.1206055 -4.1269693 -4.1546192 -4.1951642 -4.1818171 -4.1335421 -4.1052556 -4.1384878 -4.2585015 -4.3686023 -4.2906256 -3.9997053 -3.5923162 -3.1395509][-4.1464872 -4.2352586 -4.1526608 -4.0960078 -4.0666122 -3.9335489 -3.7072854 -3.5315511 -3.4690025 -3.489151 -3.4837461 -3.2974148 -2.9194517 -2.4965408 -2.1714377][-4.0913191 -4.0878887 -3.8670886 -3.602972 -3.3449371 -2.9771404 -2.4773848 -2.1069651 -1.9804065 -1.9819074 -1.9865172 -1.8575618 -1.5415754 -1.1942184 -1.060463][-3.9084408 -3.8112209 -3.4699163 -3.0041199 -2.5163772 -1.9424002 -1.2687809 -0.82562852 -0.79368544 -0.99427843 -1.2545307 -1.4361999 -1.3486061 -1.0972877 -0.99175811][-3.7825656 -3.6370974 -3.2908862 -2.8349543 -2.3742363 -1.8615997 -1.3154571 -1.0348747 -1.2116735 -1.62145 -2.0527775 -2.4086659 -2.4270642 -2.1140838 -1.7719371][-3.7667372 -3.6550679 -3.4120831 -3.0932703 -2.8463001 -2.6140795 -2.3414376 -2.2064469 -2.3848071 -2.7508717 -3.0415576 -3.1920435 -3.038403 -2.5301678 -1.9556406][-3.8864405 -3.843277 -3.6534557 -3.3772972 -3.2647698 -3.2871916 -3.2316632 -3.1193683 -3.1238258 -3.2713687 -3.292639 -3.120605 -2.7585206 -2.1541421 -1.550823][-4.0383663 -4.039763 -3.8403578 -3.5270748 -3.4125323 -3.5169582 -3.5512238 -3.4288745 -3.3099515 -3.2774129 -3.0666835 -2.67185 -2.26124 -1.7822168 -1.3399205][-4.10655 -4.0744448 -3.8454928 -3.4865253 -3.2910376 -3.3234844 -3.3656831 -3.2801912 -3.1495852 -3.0200343 -2.7194538 -2.3231611 -2.0450504 -1.7831256 -1.5401719][-4.03709 -3.9502854 -3.7225716 -3.3653502 -3.093029 -3.0174093 -3.0257382 -2.9692264 -2.8674848 -2.7260113 -2.4563751 -2.1928229 -2.0863214 -1.98634 -1.8639948][-3.8096898 -3.7113712 -3.5619514 -3.2776184 -3.00544 -2.8952191 -2.8915882 -2.8461936 -2.7476015 -2.5994391 -2.3646352 -2.1917131 -2.1919627 -2.1778586 -2.1026945]]...]
INFO - root - 2017-12-07 21:21:23.671466: step 17010, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 4.528 sec/batch; 83h:09m:01s remains)
INFO - root - 2017-12-07 21:22:09.427662: step 17020, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.569 sec/batch; 83h:54m:13s remains)
INFO - root - 2017-12-07 21:22:54.726425: step 17030, loss = 0.83, batch loss = 0.76 (7.2 examples/sec; 4.424 sec/batch; 81h:13m:38s remains)
INFO - root - 2017-12-07 21:23:40.071783: step 17040, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.516 sec/batch; 82h:53m:45s remains)
INFO - root - 2017-12-07 21:24:25.718039: step 17050, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.581 sec/batch; 84h:05m:00s remains)
INFO - root - 2017-12-07 21:25:11.012083: step 17060, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.571 sec/batch; 83h:52m:48s remains)
INFO - root - 2017-12-07 21:25:56.660689: step 17070, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.598 sec/batch; 84h:22m:09s remains)
INFO - root - 2017-12-07 21:26:42.253092: step 17080, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.574 sec/batch; 83h:55m:12s remains)
INFO - root - 2017-12-07 21:27:27.881846: step 17090, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.591 sec/batch; 84h:12m:18s remains)
INFO - root - 2017-12-07 21:28:13.547432: step 17100, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 4.548 sec/batch; 83h:24m:17s remains)
2017-12-07 21:28:16.214423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3658502 -3.4084404 -3.4825814 -3.55858 -3.6639438 -3.7894261 -3.8831882 -3.9063189 -3.8592386 -3.7942951 -3.714581 -3.6332002 -3.55302 -3.5173647 -3.5539565][-3.3248086 -3.311022 -3.3837237 -3.5299673 -3.706924 -3.8987384 -4.0311465 -4.0742345 -4.0642114 -4.0481873 -3.9959679 -3.89176 -3.7227941 -3.5651939 -3.5008857][-3.3660192 -3.2454476 -3.2838304 -3.47223 -3.6826243 -3.8996363 -4.0550346 -4.1228361 -4.1756668 -4.2469068 -4.255157 -4.1402488 -3.8867152 -3.5827892 -3.3884213][-3.3666615 -3.1686029 -3.2375393 -3.5379012 -3.8232934 -4.045321 -4.1617851 -4.1949058 -4.2944541 -4.4846978 -4.5904655 -4.4831047 -4.1299605 -3.6274657 -3.2579703][-3.2290564 -3.0333109 -3.2428198 -3.728143 -4.0687914 -4.1425261 -3.9633482 -3.747067 -3.8561749 -4.3136988 -4.7256851 -4.7744055 -4.3772368 -3.6963732 -3.1631947][-3.0753856 -2.9147897 -3.272608 -3.8546247 -4.0349312 -3.6250908 -2.8248625 -2.2181227 -2.4278951 -3.3648896 -4.2812767 -4.623065 -4.2994494 -3.5584288 -2.9427638][-2.8386066 -2.8130462 -3.3154912 -3.8112965 -3.5555854 -2.4504409 -1.0045724 -0.21638727 -0.79509664 -2.3076057 -3.6886008 -4.2921381 -4.1201358 -3.4613929 -2.8490157][-2.3761706 -2.5870943 -3.2230477 -3.5758944 -2.9089472 -1.311861 0.39637661 0.98964596 -0.056799412 -1.9010837 -3.4429467 -4.1641355 -4.1578922 -3.6542473 -3.1004333][-2.1380064 -2.4867475 -3.1385517 -3.4408851 -2.7554023 -1.2856927 0.071779251 0.26763105 -0.83573318 -2.4196599 -3.6754479 -4.2483835 -4.218461 -3.7446291 -3.2132626][-2.6481543 -2.991612 -3.5145192 -3.7700765 -3.267489 -2.2076514 -1.3390901 -1.3710144 -2.2002072 -3.2534776 -4.0550528 -4.3918581 -4.2917814 -3.8527 -3.4050786][-3.5214443 -3.7784488 -4.0903873 -4.2176781 -3.8213837 -3.0626781 -2.5228095 -2.6258566 -3.1437511 -3.7014191 -4.1090212 -4.2892466 -4.2479072 -3.9960346 -3.7448106][-4.0874496 -4.1433511 -4.2122703 -4.2126789 -3.9546213 -3.536139 -3.320195 -3.4620044 -3.6892309 -3.8085284 -3.8765125 -3.9194868 -3.9229491 -3.8288403 -3.7166359][-4.1817808 -4.0688744 -3.9954321 -3.9694381 -3.8848844 -3.7622921 -3.7442753 -3.8099067 -3.784029 -3.6601651 -3.6049182 -3.6362653 -3.689661 -3.6626327 -3.5875602][-3.8052034 -3.6927066 -3.6568255 -3.7043896 -3.7451673 -3.7424006 -3.718262 -3.6499412 -3.5108306 -3.390893 -3.4038568 -3.5105367 -3.6064816 -3.6040776 -3.5465672][-3.3415785 -3.2874002 -3.3233261 -3.4351916 -3.5357208 -3.5680597 -3.5268128 -3.4185581 -3.3086095 -3.3097529 -3.4324276 -3.5668285 -3.6259227 -3.5934286 -3.5405037]]...]
INFO - root - 2017-12-07 21:29:01.565259: step 17110, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.549 sec/batch; 83h:25m:18s remains)
INFO - root - 2017-12-07 21:29:47.446993: step 17120, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.609 sec/batch; 84h:29m:49s remains)
INFO - root - 2017-12-07 21:30:32.948776: step 17130, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.579 sec/batch; 83h:56m:08s remains)
INFO - root - 2017-12-07 21:31:18.440134: step 17140, loss = 0.65, batch loss = 0.58 (6.9 examples/sec; 4.614 sec/batch; 84h:34m:38s remains)
INFO - root - 2017-12-07 21:32:04.150610: step 17150, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.559 sec/batch; 83h:32m:48s remains)
INFO - root - 2017-12-07 21:32:49.844198: step 17160, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.580 sec/batch; 83h:55m:05s remains)
INFO - root - 2017-12-07 21:33:35.416382: step 17170, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.553 sec/batch; 83h:24m:28s remains)
INFO - root - 2017-12-07 21:34:20.965675: step 17180, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.534 sec/batch; 83h:03m:23s remains)
INFO - root - 2017-12-07 21:35:06.351606: step 17190, loss = 0.78, batch loss = 0.71 (7.2 examples/sec; 4.467 sec/batch; 81h:48m:47s remains)
INFO - root - 2017-12-07 21:35:52.338349: step 17200, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.554 sec/batch; 83h:23m:42s remains)
2017-12-07 21:35:55.096154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4638011 -3.1209598 -2.43498 -1.5773418 -0.71567869 -0.15397692 -0.10626411 -0.71771026 -1.5015802 -2.0438905 -2.3906298 -2.3957539 -2.2134545 -2.0789883 -2.0840714][-3.4877336 -3.0243855 -2.1484632 -1.0224521 0.1469593 0.87594032 0.85108328 0.019801617 -0.88686037 -1.3774548 -1.707597 -1.8338025 -1.8136008 -1.7524962 -1.7522855][-3.4990296 -2.9385331 -1.8924234 -0.56652308 0.76055479 1.52743 1.4548926 0.61072254 -0.22476864 -0.60495782 -0.94075537 -1.2448785 -1.4438205 -1.5152993 -1.5322661][-3.5051916 -2.8655179 -1.667968 -0.25030088 1.080719 1.8396201 1.7841582 1.0616846 0.33357954 -0.044600487 -0.45929289 -0.89958596 -1.2389007 -1.3902843 -1.3992503][-3.5221002 -2.8270159 -1.5457256 -0.13034582 1.1612573 1.9608006 2.0200524 1.4668407 0.76565266 0.22457695 -0.31825352 -0.8058269 -1.1690447 -1.3446033 -1.3688185][-3.5379162 -2.8448162 -1.5761321 -0.23419046 1.0248995 1.9365716 2.179873 1.7345605 0.90699148 0.09210062 -0.53154159 -0.90902567 -1.0962636 -1.2109447 -1.3337603][-3.5454054 -2.8959374 -1.6851873 -0.41423082 0.81109428 1.7803087 2.1667519 1.7944818 0.81314754 -0.26259089 -0.95804787 -1.1901546 -1.1121731 -1.0567698 -1.2426572][-3.5677214 -2.9925857 -1.8313661 -0.56556416 0.65739536 1.5829468 2.0124764 1.7081394 0.64100075 -0.57126808 -1.3402605 -1.5365577 -1.3618572 -1.2114196 -1.3740313][-3.5888119 -3.1238751 -2.0548723 -0.77649283 0.47935438 1.3632302 1.7860527 1.5610127 0.56440067 -0.62749267 -1.4333956 -1.7308073 -1.7000515 -1.6095946 -1.6909089][-3.5769792 -3.1979165 -2.2308931 -0.97711635 0.26593733 1.1136189 1.5064001 1.3385243 0.52173185 -0.52158904 -1.3180275 -1.7169218 -1.851069 -1.8149958 -1.7968073][-3.5606256 -3.2157197 -2.2947433 -1.0423958 0.17702532 0.99086475 1.2636619 1.0540605 0.38959265 -0.47462606 -1.1827302 -1.6042747 -1.8349752 -1.8177054 -1.7151775][-3.5501902 -3.2284114 -2.3288996 -1.0495653 0.20439625 1.052578 1.2325826 0.96553516 0.3839035 -0.35639572 -0.985466 -1.4369938 -1.757741 -1.8192852 -1.7010527][-3.5440106 -3.2697549 -2.4434524 -1.2150662 0.02073288 0.88167667 1.0641551 0.85495806 0.39628458 -0.21784115 -0.724844 -1.1636367 -1.5141263 -1.6673944 -1.6579094][-3.5245128 -3.3014159 -2.6024404 -1.5227966 -0.38664627 0.44448042 0.67573929 0.57318926 0.24900007 -0.20296383 -0.53696394 -0.86825609 -1.155695 -1.3653164 -1.5015955][-3.4636493 -3.2906199 -2.738348 -1.8319075 -0.82308173 -0.029515743 0.26884365 0.24047804 -0.0065550804 -0.34583521 -0.55529189 -0.79937911 -1.0482669 -1.2977545 -1.4963675]]...]
INFO - root - 2017-12-07 21:36:40.518924: step 17210, loss = 0.73, batch loss = 0.66 (7.2 examples/sec; 4.428 sec/batch; 81h:04m:56s remains)
INFO - root - 2017-12-07 21:37:26.459643: step 17220, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.621 sec/batch; 84h:35m:35s remains)
INFO - root - 2017-12-07 21:38:12.197202: step 17230, loss = 0.71, batch loss = 0.63 (6.9 examples/sec; 4.631 sec/batch; 84h:45m:27s remains)
INFO - root - 2017-12-07 21:38:57.546158: step 17240, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.528 sec/batch; 82h:52m:31s remains)
INFO - root - 2017-12-07 21:39:42.896091: step 17250, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.602 sec/batch; 84h:12m:08s remains)
INFO - root - 2017-12-07 21:40:28.354967: step 17260, loss = 0.81, batch loss = 0.73 (7.0 examples/sec; 4.577 sec/batch; 83h:44m:47s remains)
INFO - root - 2017-12-07 21:41:13.897144: step 17270, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.532 sec/batch; 82h:54m:21s remains)
INFO - root - 2017-12-07 21:41:59.736310: step 17280, loss = 0.68, batch loss = 0.61 (7.1 examples/sec; 4.506 sec/batch; 82h:25m:17s remains)
INFO - root - 2017-12-07 21:42:45.440871: step 17290, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.580 sec/batch; 83h:45m:08s remains)
INFO - root - 2017-12-07 21:43:31.242591: step 17300, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.583 sec/batch; 83h:48m:07s remains)
2017-12-07 21:43:34.005909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4248071 -4.4986477 -4.5041375 -4.4813118 -4.4790626 -4.4830041 -4.5077467 -4.5351944 -4.5852427 -4.6539674 -4.685308 -4.6327443 -4.463047 -4.2398715 -4.0585279][-4.6065464 -4.6411843 -4.6312923 -4.6358981 -4.6831703 -4.7441015 -4.8517218 -4.9626646 -5.0861607 -5.1573687 -5.124207 -4.9917526 -4.6758752 -4.2652497 -3.9427776][-4.5859661 -4.5628796 -4.5794549 -4.6193748 -4.6763921 -4.7410111 -4.8483543 -4.9533029 -5.0836897 -5.1261792 -5.0515132 -4.9087148 -4.5059738 -3.9190958 -3.4309716][-4.4925618 -4.5116673 -4.5962825 -4.6153851 -4.56337 -4.525732 -4.4901314 -4.4617114 -4.5595264 -4.6127224 -4.5104203 -4.311192 -3.7825673 -3.0499425 -2.4773898][-4.5639062 -4.671958 -4.7250276 -4.5667377 -4.2183504 -3.8937404 -3.5828757 -3.3783755 -3.4548302 -3.5798211 -3.5329783 -3.3193438 -2.6996999 -1.8702323 -1.3301051][-4.5931478 -4.5767984 -4.3081679 -3.8093331 -3.1202931 -2.4903984 -1.9453592 -1.547241 -1.5081306 -1.5966315 -1.6337903 -1.6316617 -1.2566924 -0.65441394 -0.37388849][-3.9348061 -3.5993221 -3.000304 -2.3460646 -1.5389011 -0.71238112 -0.019670963 0.50168037 0.59507418 0.49254656 0.30293751 -0.079576015 -0.18328571 -0.028441429 -0.088961124][-3.0399623 -2.4240112 -1.5914688 -0.86833692 -0.046111584 0.87885904 1.6708388 2.2443829 2.2299004 1.8741503 1.3759246 0.54396009 -0.031629086 -0.28834867 -0.63171148][-2.5283003 -1.7507198 -0.74502206 0.029926777 0.7449851 1.5100694 2.0862713 2.3961854 2.1529145 1.6224451 0.98112774 -0.0079894066 -0.72412062 -1.1096292 -1.5183966][-2.0539989 -1.2677834 -0.35864353 0.22034454 0.58368635 0.9098835 1.0063944 0.90723848 0.525105 -0.022965431 -0.70742917 -1.6131871 -2.1303079 -2.2880833 -2.4785259][-1.8993528 -1.3220315 -0.74670815 -0.49131584 -0.43929958 -0.41639185 -0.63935423 -0.98455787 -1.4116888 -1.878268 -2.4215002 -3.0458565 -3.2427557 -3.1309221 -3.1005614][-2.1704063 -1.8962259 -1.6924129 -1.7072144 -1.8165379 -1.9253235 -2.2455316 -2.637331 -2.9772935 -3.2740383 -3.5761745 -3.8806958 -3.837893 -3.5741346 -3.4123209][-2.6139703 -2.536191 -2.5848088 -2.7900217 -3.0212593 -3.1860402 -3.4280105 -3.6525025 -3.7736051 -3.8580043 -3.9622898 -4.0723267 -3.9517875 -3.6923442 -3.5214345][-3.3556495 -3.386575 -3.513659 -3.7119226 -3.8845947 -3.982852 -4.0719266 -4.1002994 -4.0612679 -4.0411696 -4.0586824 -4.0789227 -3.9777815 -3.7983847 -3.6689093][-3.9698071 -4.0340338 -4.118269 -4.2105675 -4.2694826 -4.296936 -4.3165703 -4.307168 -4.2617154 -4.2362871 -4.2486506 -4.2693295 -4.2189088 -4.1046567 -3.9779944]]...]
INFO - root - 2017-12-07 21:44:19.398023: step 17310, loss = 0.92, batch loss = 0.85 (7.0 examples/sec; 4.558 sec/batch; 83h:20m:16s remains)
INFO - root - 2017-12-07 21:45:05.134103: step 17320, loss = 0.66, batch loss = 0.59 (7.0 examples/sec; 4.598 sec/batch; 84h:02m:45s remains)
INFO - root - 2017-12-07 21:45:50.740082: step 17330, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 4.519 sec/batch; 82h:35m:21s remains)
INFO - root - 2017-12-07 21:46:36.465383: step 17340, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.553 sec/batch; 83h:11m:38s remains)
INFO - root - 2017-12-07 21:47:22.131153: step 17350, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.552 sec/batch; 83h:09m:41s remains)
INFO - root - 2017-12-07 21:48:07.740378: step 17360, loss = 0.71, batch loss = 0.63 (7.0 examples/sec; 4.581 sec/batch; 83h:40m:52s remains)
INFO - root - 2017-12-07 21:48:53.399169: step 17370, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 4.626 sec/batch; 84h:29m:40s remains)
INFO - root - 2017-12-07 21:49:39.187240: step 17380, loss = 0.67, batch loss = 0.60 (7.1 examples/sec; 4.519 sec/batch; 82h:31m:22s remains)
INFO - root - 2017-12-07 21:50:24.471567: step 17390, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.591 sec/batch; 83h:50m:09s remains)
INFO - root - 2017-12-07 21:51:10.381054: step 17400, loss = 0.78, batch loss = 0.70 (6.9 examples/sec; 4.609 sec/batch; 84h:08m:38s remains)
2017-12-07 21:51:13.189104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0538359 -2.5389631 -2.5344262 -2.3774753 -2.5228958 -2.3543265 -2.0304008 -1.8913996 -1.7394803 -1.9443533 -2.2275677 -2.3584621 -2.5128758 -2.4205163 -2.4192975][-1.5207751 -1.961364 -1.9582181 -1.8298976 -1.9692881 -1.770864 -1.3567238 -1.164923 -1.0369341 -1.2366471 -1.5103939 -1.7071128 -2.0881793 -2.2152655 -2.2845097][-0.90672827 -1.2777224 -1.3519201 -1.3253992 -1.4515722 -1.1848302 -0.67453623 -0.46038198 -0.40848255 -0.58684278 -0.83913946 -1.1387272 -1.7856886 -2.1498654 -2.3004627][-0.54207015 -0.84068179 -1.0023112 -0.98930645 -0.94014287 -0.57682395 -0.0754447 0.08993721 0.043203831 -0.18936014 -0.53388095 -0.95846558 -1.7467165 -2.2130845 -2.3944676][-0.086380959 -0.38763666 -0.62301493 -0.57664084 -0.34628153 0.020936966 0.35791588 0.42756557 0.34537077 0.010041714 -0.52577353 -1.0618064 -1.7691298 -2.1723766 -2.329926][0.34470606 0.029454708 -0.25822544 -0.23259258 0.049755573 0.39001083 0.57268238 0.5365839 0.38138771 -0.090973854 -0.78063488 -1.293365 -1.7484255 -2.0106919 -2.1470098][0.41309834 0.29070473 0.11704683 0.12866592 0.39116287 0.71752977 0.78309059 0.58693075 0.26370192 -0.34047747 -1.0466931 -1.4532423 -1.7096884 -1.9142389 -2.0368361][0.32672071 0.48422432 0.4761529 0.44692898 0.6342392 0.89471865 0.81348515 0.49536896 0.087917805 -0.51673579 -1.1293933 -1.481997 -1.6876786 -1.910296 -1.9928098][-0.013757229 0.37141132 0.48789835 0.40127134 0.49017 0.61978006 0.39713955 0.077685356 -0.27926683 -0.76701522 -1.2104871 -1.504966 -1.6764321 -1.8762619 -1.934756][-0.56736636 0.032062531 0.26890993 0.16745186 0.16377115 0.15169811 -0.14699221 -0.41541433 -0.72710967 -1.1331069 -1.4137127 -1.6765954 -1.8268938 -1.9844999 -2.0813589][-1.0719609 -0.35686016 -0.035503387 -0.135499 -0.21642065 -0.3277235 -0.60488486 -0.80344558 -1.0835774 -1.4017179 -1.5534725 -1.8250163 -2.0413134 -2.1802809 -2.3202496][-1.4660511 -0.77523565 -0.41829157 -0.48818493 -0.6213541 -0.79325461 -1.0430543 -1.194916 -1.4107108 -1.5798943 -1.577574 -1.8103824 -2.1106923 -2.3013754 -2.4633002][-1.6687751 -1.0606704 -0.70882535 -0.75154281 -0.92810369 -1.1582644 -1.4478796 -1.6479962 -1.8200026 -1.8646247 -1.7624626 -1.9493132 -2.2953866 -2.4872961 -2.5491567][-1.6578918 -1.1900256 -0.91272306 -0.94702935 -1.1304197 -1.4065516 -1.7716937 -2.0504031 -2.1972258 -2.1975136 -2.0647683 -2.1921558 -2.5072825 -2.6339617 -2.5552027][-1.6068587 -1.3184469 -1.1559763 -1.1978786 -1.354507 -1.6250379 -1.9780433 -2.2219608 -2.3087492 -2.2891946 -2.1802039 -2.2797639 -2.5631003 -2.6500969 -2.5145025]]...]
INFO - root - 2017-12-07 21:51:58.642331: step 17410, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.580 sec/batch; 83h:36m:26s remains)
INFO - root - 2017-12-07 21:52:44.407804: step 17420, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.543 sec/batch; 82h:55m:11s remains)
INFO - root - 2017-12-07 21:53:29.982320: step 17430, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.586 sec/batch; 83h:41m:49s remains)
INFO - root - 2017-12-07 21:54:15.828753: step 17440, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.563 sec/batch; 83h:15m:26s remains)
INFO - root - 2017-12-07 21:55:01.474625: step 17450, loss = 0.79, batch loss = 0.72 (7.3 examples/sec; 4.409 sec/batch; 80h:26m:07s remains)
INFO - root - 2017-12-07 21:55:47.040340: step 17460, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 4.529 sec/batch; 82h:36m:58s remains)
INFO - root - 2017-12-07 21:56:32.551068: step 17470, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.501 sec/batch; 82h:05m:20s remains)
INFO - root - 2017-12-07 21:57:18.035062: step 17480, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 4.512 sec/batch; 82h:16m:33s remains)
INFO - root - 2017-12-07 21:58:03.610626: step 17490, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.598 sec/batch; 83h:49m:56s remains)
INFO - root - 2017-12-07 21:58:48.922616: step 17500, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.536 sec/batch; 82h:40m:57s remains)
2017-12-07 21:58:51.727782: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.29883337 -0.21473837 -0.0787406 0.086739063 0.306787 0.39485645 0.25729084 -0.0097017288 -0.28896093 -0.46117568 -0.43521261 -0.17766905 0.070241928 0.25963593 0.50856924][-0.32501936 -0.22155714 -0.10540438 -0.0013613701 0.13661718 0.12674856 -0.081170082 -0.38560629 -0.6473 -0.66751647 -0.37521076 0.1130724 0.43019915 0.56637526 0.72912407][-0.36628771 -0.29593754 -0.26912212 -0.27834606 -0.24111748 -0.26772261 -0.32790661 -0.43083024 -0.54463005 -0.43424368 -0.044634819 0.44265223 0.64865828 0.66738653 0.75333405][-0.53780556 -0.57759738 -0.65641761 -0.71674061 -0.68560934 -0.58676457 -0.33171034 -0.12587166 -0.101789 -0.040179253 0.13891315 0.36184931 0.40421486 0.42144918 0.58166981][-0.87722635 -1.1093121 -1.2921031 -1.3327816 -1.1930208 -0.85024095 -0.25181484 0.19066525 0.23839903 0.11130333 -0.011597633 -0.00019931793 0.044593811 0.17761421 0.41225386][-1.3700206 -1.8078015 -2.0165184 -1.9015574 -1.5086715 -0.83747768 0.078345776 0.6961813 0.71099615 0.34091282 -0.070428848 -0.18893862 -0.10789967 0.089543343 0.30902719][-1.9512882 -2.4255426 -2.5107982 -2.1521628 -1.4863274 -0.5615046 0.57051086 1.3423505 1.367238 0.817029 0.15039444 -0.13380909 -0.10585785 0.10131216 0.33558178][-2.1160314 -2.500494 -2.5028422 -2.0630438 -1.3353615 -0.36387205 0.80665255 1.6235428 1.6329207 1.0156298 0.24495125 -0.15481234 -0.19787264 0.02612114 0.3028264][-1.6429935 -1.959033 -2.0275848 -1.7382932 -1.1827865 -0.3829093 0.58524704 1.2367315 1.1889882 0.70686913 0.096182346 -0.2419014 -0.29249907 -0.099445343 0.12020969][-1.1749027 -1.5394244 -1.7396619 -1.6470017 -1.3102193 -0.73661685 -0.024970055 0.45527983 0.46112585 0.25383759 -0.09147644 -0.29942989 -0.344337 -0.26157808 -0.18527174][-1.1184089 -1.5653732 -1.8486326 -1.8831472 -1.706744 -1.3285422 -0.81526041 -0.42752695 -0.30945015 -0.348423 -0.58421159 -0.68346214 -0.64817476 -0.57637334 -0.53593445][-1.2808006 -1.6983948 -1.9654241 -2.0182557 -1.8833079 -1.6584864 -1.3552039 -1.072279 -0.86847425 -0.7977879 -0.90827346 -0.8182478 -0.65203667 -0.56036735 -0.54150581][-1.4219208 -1.7449076 -1.9633234 -1.9737453 -1.79528 -1.6749213 -1.5549762 -1.3618548 -1.0896945 -0.91572833 -0.85329032 -0.59384155 -0.39035702 -0.36848545 -0.43869162][-1.5322196 -1.7939746 -1.9678869 -1.9385395 -1.7140086 -1.6427655 -1.617815 -1.4804089 -1.2267458 -1.0582039 -0.88463831 -0.521822 -0.26806831 -0.22936726 -0.31531191][-1.554481 -1.7504141 -1.9068866 -1.903177 -1.7369995 -1.7264552 -1.7359378 -1.6024117 -1.3910272 -1.2514791 -0.96346593 -0.47654462 -0.10334873 0.0450387 0.046826363]]...]
INFO - root - 2017-12-07 21:59:37.538437: step 17510, loss = 0.87, batch loss = 0.79 (7.0 examples/sec; 4.592 sec/batch; 83h:42m:01s remains)
INFO - root - 2017-12-07 22:00:23.156114: step 17520, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.500 sec/batch; 82h:00m:52s remains)
INFO - root - 2017-12-07 22:01:08.751420: step 17530, loss = 0.78, batch loss = 0.71 (7.2 examples/sec; 4.467 sec/batch; 81h:23m:37s remains)
INFO - root - 2017-12-07 22:01:54.295793: step 17540, loss = 0.82, batch loss = 0.75 (7.1 examples/sec; 4.539 sec/batch; 82h:41m:14s remains)
INFO - root - 2017-12-07 22:02:40.270232: step 17550, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.570 sec/batch; 83h:15m:03s remains)
INFO - root - 2017-12-07 22:03:25.717966: step 17560, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.522 sec/batch; 82h:20m:55s remains)
INFO - root - 2017-12-07 22:04:11.132494: step 17570, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.594 sec/batch; 83h:39m:38s remains)
INFO - root - 2017-12-07 22:04:56.743531: step 17580, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.556 sec/batch; 82h:57m:30s remains)
INFO - root - 2017-12-07 22:05:42.595034: step 17590, loss = 0.82, batch loss = 0.74 (7.0 examples/sec; 4.569 sec/batch; 83h:10m:11s remains)
INFO - root - 2017-12-07 22:06:28.022492: step 17600, loss = 0.78, batch loss = 0.70 (7.1 examples/sec; 4.530 sec/batch; 82h:27m:12s remains)
2017-12-07 22:06:30.717606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3687992 -2.3892174 -2.4306893 -2.2850552 -2.1691823 -2.2980926 -2.1138978 -1.7112844 -1.3937433 -1.1740484 -1.0712969 -0.9449265 -1.0343988 -0.78157496 -0.21140146][-2.6271381 -2.5861795 -2.5608139 -2.4351091 -2.3538289 -2.501667 -2.356708 -1.9515448 -1.5115135 -1.1634059 -0.94968104 -0.77448845 -0.70435333 -0.26944923 0.20548534][-2.9014578 -2.69987 -2.5182552 -2.4038727 -2.3840928 -2.5808549 -2.5256336 -2.1688621 -1.7553008 -1.4351637 -1.1848533 -0.986022 -0.80121708 -0.26814604 0.051976681][-2.8042724 -2.3718097 -2.0211174 -1.8953195 -1.8545225 -1.9797368 -1.9444797 -1.6989534 -1.5593145 -1.4761047 -1.2803986 -1.1660843 -1.0568678 -0.63235569 -0.49038506][-2.1065843 -1.6356237 -1.3127558 -1.2900467 -1.1750691 -1.0098484 -0.76016712 -0.57183862 -0.83084369 -1.0712371 -0.93043184 -0.92910242 -0.98460817 -0.74213243 -0.72747374][-1.0127869 -0.66145587 -0.46398902 -0.61055875 -0.44256759 0.097640038 0.72458649 0.95502043 0.31213284 -0.21007252 -0.12345505 -0.21957922 -0.41208839 -0.34261179 -0.37419271][0.10847807 0.35499287 0.46204853 0.12769127 0.27592325 1.1104836 2.1202445 2.4704747 1.5222197 0.81860733 0.89586687 0.73080444 0.46711206 0.42069912 0.4184351][0.57471561 0.80520105 0.90265226 0.43558264 0.47490597 1.3154745 2.419589 2.8062921 1.767961 1.1569467 1.3452539 1.2108016 1.0182624 0.91520786 0.9418726][0.28344631 0.58298826 0.72759867 0.24028111 0.17104244 0.759871 1.6342325 1.9473214 1.0682344 0.75041294 1.1083164 1.0943918 1.0417948 0.89912796 0.89689922][-0.46303391 -0.10443211 0.10801649 -0.25743866 -0.30344486 0.053617 0.6641016 0.90676641 0.224123 0.16728306 0.595778 0.65009832 0.72213554 0.56621122 0.52668428][-1.2174366 -0.84122658 -0.59631038 -0.7783494 -0.72211289 -0.52046657 -0.11690521 0.075306416 -0.40938711 -0.2951827 0.094047546 0.14281225 0.2964921 0.15621567 0.090146542][-1.5611305 -1.233192 -1.0598114 -1.1811175 -1.1061642 -1.0443234 -0.795861 -0.63589787 -0.89581871 -0.64431214 -0.30990505 -0.27202988 -0.085858822 -0.19688845 -0.29894257][-1.9135201 -1.6395733 -1.5553968 -1.6674254 -1.6481385 -1.6756108 -1.5116796 -1.3693314 -1.4631691 -1.1963956 -0.9749701 -0.99478006 -0.85175323 -0.95399833 -1.0748935][-2.6009398 -2.3675935 -2.3401413 -2.4474201 -2.4735396 -2.5203676 -2.4046633 -2.2846227 -2.2938612 -2.0925276 -1.9868851 -2.0409269 -1.9816179 -2.0926826 -2.2089145][-3.6118102 -3.4101346 -3.3596566 -3.3990939 -3.4099946 -3.435782 -3.3608298 -3.2975531 -3.3139884 -3.2172291 -3.1937327 -3.2501016 -3.2441387 -3.3161502 -3.3788092]]...]
INFO - root - 2017-12-07 22:07:16.160401: step 17610, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.595 sec/batch; 83h:37m:16s remains)
INFO - root - 2017-12-07 22:08:01.707219: step 17620, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.566 sec/batch; 83h:04m:53s remains)
INFO - root - 2017-12-07 22:08:47.562945: step 17630, loss = 0.80, batch loss = 0.73 (6.8 examples/sec; 4.692 sec/batch; 85h:22m:03s remains)
INFO - root - 2017-12-07 22:09:32.802693: step 17640, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.560 sec/batch; 82h:57m:22s remains)
INFO - root - 2017-12-07 22:10:18.517809: step 17650, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.563 sec/batch; 82h:59m:13s remains)
INFO - root - 2017-12-07 22:11:03.972160: step 17660, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.508 sec/batch; 81h:58m:12s remains)
INFO - root - 2017-12-07 22:11:49.686988: step 17670, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.526 sec/batch; 82h:17m:05s remains)
INFO - root - 2017-12-07 22:12:35.056546: step 17680, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.588 sec/batch; 83h:23m:53s remains)
INFO - root - 2017-12-07 22:13:20.737969: step 17690, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.604 sec/batch; 83h:40m:55s remains)
INFO - root - 2017-12-07 22:14:06.289671: step 17700, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.539 sec/batch; 82h:29m:44s remains)
2017-12-07 22:14:08.998633: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18266344 -0.099962234 -0.45767593 -0.734293 -0.83336186 -1.0427039 -1.269347 -1.4731238 -1.473357 -1.1667497 -0.99724269 -1.0058894 -0.84132576 -0.38888597 -0.12337828][0.13301945 -0.14517975 -0.38516855 -0.58732033 -0.68479633 -0.82398057 -0.86309886 -0.94768858 -0.98973465 -0.74714041 -0.61539674 -0.58725977 -0.40538311 -0.0068202019 0.15071726][-0.13064384 -0.47634864 -0.545279 -0.6145041 -0.77705479 -0.80938506 -0.57044411 -0.53586245 -0.68241572 -0.57474232 -0.52516079 -0.47151566 -0.24012089 0.038441181 0.034121513][-0.27609396 -0.72479606 -0.677691 -0.69013739 -0.98299527 -0.95287824 -0.59867549 -0.49509859 -0.56001234 -0.43607926 -0.37478352 -0.24336767 0.00630188 0.10824394 -0.061386108][-0.25100136 -0.76491022 -0.71581984 -0.83243537 -1.231775 -1.1844714 -0.87277317 -0.65930557 -0.50012636 -0.36299038 -0.31303024 -0.10294008 0.10319614 0.076107025 -0.10477686][-0.11127901 -0.57170892 -0.58766675 -0.82454157 -1.1802881 -1.1403244 -0.87526679 -0.39985466 0.0084795952 -0.11245632 -0.38961172 -0.3225503 -0.26386452 -0.30588579 -0.30332947][0.070049763 -0.2140708 -0.22657776 -0.4061985 -0.5321331 -0.43078661 -0.073518753 0.82414627 1.3993258 0.73995066 -0.07667017 -0.36986065 -0.60120249 -0.73426437 -0.62019992][0.052910805 -0.084824562 -0.0867548 -0.077240944 0.078178406 0.26383495 0.80637312 2.0612011 2.6346436 1.5009136 0.32132006 -0.26305294 -0.71018386 -0.92938781 -0.74850059][-0.084792137 -0.21903324 -0.24443388 -0.12062502 0.056536198 0.12207937 0.661819 1.7560248 2.0425448 1.0984674 0.17058754 -0.34908628 -0.77595234 -0.96039176 -0.719728][-0.16081715 -0.34725618 -0.37116861 -0.20057201 -0.20772076 -0.4201107 -0.19686651 0.33586884 0.42463827 0.092944145 -0.28049135 -0.542506 -0.78269911 -0.84145451 -0.60882616][-0.26148319 -0.42926097 -0.35898685 -0.19487858 -0.45785332 -0.92585349 -1.0151906 -0.9132483 -0.79385185 -0.57488656 -0.58012223 -0.75754762 -0.84263396 -0.80724883 -0.62693119][-0.7887311 -0.87092304 -0.66497588 -0.49819446 -0.8546536 -1.3341739 -1.4749987 -1.4574413 -1.150527 -0.62980604 -0.60605526 -0.93212819 -1.0374081 -1.0279229 -0.97954988][-1.2352562 -1.2307749 -1.0153434 -0.8816092 -1.1396995 -1.4120226 -1.4863482 -1.4981027 -1.1062145 -0.53414035 -0.56759048 -0.9890027 -1.1204557 -1.1373336 -1.1424036][-1.3498747 -1.2661564 -1.1349967 -1.0974255 -1.2710795 -1.3942015 -1.4895494 -1.5553532 -1.1713762 -0.68355274 -0.78549242 -1.2150934 -1.3834198 -1.4242663 -1.3611534][-1.5907171 -1.5324781 -1.4800026 -1.4742818 -1.588594 -1.6821647 -1.8684723 -1.9958341 -1.6397157 -1.2079344 -1.2762077 -1.6411819 -1.9176569 -2.0437589 -1.9569635]]...]
INFO - root - 2017-12-07 22:14:54.592906: step 17710, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.594 sec/batch; 83h:28m:38s remains)
INFO - root - 2017-12-07 22:15:40.243214: step 17720, loss = 0.75, batch loss = 0.68 (7.2 examples/sec; 4.451 sec/batch; 80h:52m:14s remains)
INFO - root - 2017-12-07 22:16:25.849425: step 17730, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.515 sec/batch; 82h:00m:27s remains)
INFO - root - 2017-12-07 22:17:11.665345: step 17740, loss = 0.71, batch loss = 0.63 (7.0 examples/sec; 4.581 sec/batch; 83h:11m:40s remains)
INFO - root - 2017-12-07 22:17:57.220482: step 17750, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 4.621 sec/batch; 83h:54m:30s remains)
INFO - root - 2017-12-07 22:18:42.696241: step 17760, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.558 sec/batch; 82h:45m:39s remains)
INFO - root - 2017-12-07 22:19:28.628215: step 17770, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.566 sec/batch; 82h:53m:00s remains)
INFO - root - 2017-12-07 22:20:13.986950: step 17780, loss = 0.68, batch loss = 0.61 (7.4 examples/sec; 4.316 sec/batch; 78h:20m:37s remains)
INFO - root - 2017-12-07 22:20:59.576342: step 17790, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.608 sec/batch; 83h:37m:29s remains)
INFO - root - 2017-12-07 22:21:45.447500: step 17800, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.567 sec/batch; 82h:52m:13s remains)
2017-12-07 22:21:48.034106: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11796999 0.64003229 1.0966935 0.65676928 -0.71085119 -1.8668752 -1.8034801 -1.1231492 -0.93459916 -0.84862661 -0.35641623 0.55758762 1.3419814 0.92040539 -1.4995673][0.38678598 0.93878031 1.2785525 0.46289682 -1.0977004 -1.949245 -1.5194061 -0.76238894 -0.86580229 -1.0208836 -0.57012129 0.22730112 0.78135538 0.23897934 -1.7755935][0.24026299 0.89500761 1.1544571 0.085289 -1.2988279 -1.46556 -0.59177423 -0.074148655 -0.85040903 -1.4707539 -1.1869044 -0.61199927 -0.28851128 -0.87128615 -2.22127][-0.45385981 0.28567457 0.53698349 -0.5154233 -1.3662596 -0.56061649 0.87092924 0.93518257 -0.60783553 -1.8051009 -1.8937178 -1.6160767 -1.430187 -1.9084949 -2.5632515][-0.97689962 -0.24810839 0.01746273 -0.8867209 -1.2301154 0.4468298 2.3106318 1.7868047 -0.37812996 -2.1079998 -2.7236857 -2.6988134 -2.4120281 -2.5436354 -2.6175127][-1.4027634 -0.653708 -0.28174448 -0.994555 -0.94392014 1.4111004 3.5402002 2.4602408 -0.091135025 -2.278049 -3.5114427 -3.6665928 -3.1244664 -2.7420475 -2.3300958][-2.0323112 -1.2416008 -0.56244731 -0.94997215 -0.51413584 2.2260213 4.3152008 2.8341074 0.15180206 -2.3545167 -4.1545568 -4.4491296 -3.5849113 -2.6118956 -1.651154][-2.6827068 -1.8429661 -0.7137537 -0.65890312 0.04039526 2.7101192 4.4599094 3.0086765 0.43699312 -2.3268492 -4.6021304 -5.0631256 -3.9438539 -2.4881573 -1.0074232][-3.10389 -2.2139177 -0.66405153 -0.2058053 0.55215883 2.7562971 4.1223879 3.1049542 0.81504583 -2.0733831 -4.6483788 -5.2844596 -4.0942779 -2.4471631 -0.5970242][-3.4956691 -2.5923238 -0.75309467 0.058415413 0.77546883 2.4204903 3.5284514 3.0138249 0.95794249 -1.9434669 -4.6138425 -5.376883 -4.2503271 -2.5611076 -0.35669994][-3.91784 -3.0686538 -1.1906755 -0.20074129 0.40275002 1.5875425 2.6269794 2.5330806 0.6991806 -2.051621 -4.5859218 -5.4098577 -4.4540062 -2.8510783 -0.45123076][-4.0126772 -3.3662229 -1.7338448 -0.77861977 -0.32403326 0.54255581 1.5405202 1.6943078 0.22883368 -2.0999751 -4.3013182 -5.1877365 -4.5457439 -3.1734271 -0.86937881][-3.88733 -3.4737914 -2.2492354 -1.4975934 -1.1656663 -0.47855186 0.41414785 0.65306711 -0.37876511 -2.1385686 -3.8880782 -4.8176408 -4.5410128 -3.4699996 -1.5042489][-3.5390635 -3.3571792 -2.5777388 -2.063153 -1.7896106 -1.2137225 -0.53146863 -0.34064341 -1.0132184 -2.2208567 -3.45892 -4.2759638 -4.2345843 -3.4654031 -1.9968333][-3.1612926 -3.1567025 -2.756443 -2.4563947 -2.2101533 -1.7033429 -1.2246544 -1.0843136 -1.4907262 -2.2654979 -3.0750191 -3.7010949 -3.7507892 -3.2347317 -2.2482946]]...]
INFO - root - 2017-12-07 22:22:33.630057: step 17810, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.501 sec/batch; 81h:39m:38s remains)
INFO - root - 2017-12-07 22:23:19.124973: step 17820, loss = 0.73, batch loss = 0.65 (7.0 examples/sec; 4.588 sec/batch; 83h:14m:03s remains)
INFO - root - 2017-12-07 22:24:04.622336: step 17830, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.607 sec/batch; 83h:33m:25s remains)
INFO - root - 2017-12-07 22:24:50.620794: step 17840, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.556 sec/batch; 82h:37m:14s remains)
INFO - root - 2017-12-07 22:25:36.252537: step 17850, loss = 0.69, batch loss = 0.61 (7.0 examples/sec; 4.569 sec/batch; 82h:51m:11s remains)
INFO - root - 2017-12-07 22:26:21.924927: step 17860, loss = 0.81, batch loss = 0.73 (7.0 examples/sec; 4.568 sec/batch; 82h:49m:11s remains)
INFO - root - 2017-12-07 22:27:07.554579: step 17870, loss = 0.79, batch loss = 0.71 (6.9 examples/sec; 4.631 sec/batch; 83h:56m:06s remains)
INFO - root - 2017-12-07 22:27:53.410118: step 17880, loss = 0.82, batch loss = 0.74 (6.9 examples/sec; 4.646 sec/batch; 84h:11m:45s remains)
INFO - root - 2017-12-07 22:28:38.780125: step 17890, loss = 0.81, batch loss = 0.73 (7.2 examples/sec; 4.458 sec/batch; 80h:46m:33s remains)
INFO - root - 2017-12-07 22:29:24.421415: step 17900, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.588 sec/batch; 83h:07m:56s remains)
2017-12-07 22:29:27.121859: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1604657 -2.1674802 -2.3364067 -2.3663788 -2.2194514 -2.3966219 -2.7288761 -2.7427564 -2.498126 -2.2295661 -2.1586771 -2.3815374 -2.5864928 -2.4128544 -2.1168103][-2.1297841 -2.2324049 -2.4433973 -2.4183509 -2.2359278 -2.3946681 -2.6310463 -2.5387654 -2.258487 -2.0051696 -1.9992673 -2.3065097 -2.4953895 -2.2092483 -1.8900931][-2.3560085 -2.5666375 -2.7889702 -2.7043681 -2.4903917 -2.5865266 -2.6569915 -2.4340408 -2.1452847 -1.9498365 -2.0301971 -2.4128993 -2.5755413 -2.1692946 -1.8468821][-2.4929795 -2.8623095 -3.1444516 -3.0484664 -2.8354912 -2.8671198 -2.7649827 -2.4398768 -2.1834512 -2.0682936 -2.2052374 -2.5976279 -2.6979165 -2.1586037 -1.8351524][-2.4176681 -2.9615355 -3.3337588 -3.2528269 -3.034091 -2.9894161 -2.7457819 -2.375654 -2.2033942 -2.1915057 -2.3606968 -2.7024713 -2.7151346 -2.0487328 -1.7409568][-2.1796951 -2.8159027 -3.2236414 -3.1449745 -2.908561 -2.8065906 -2.5086217 -2.1884139 -2.159884 -2.2620749 -2.434041 -2.69038 -2.5964131 -1.8316967 -1.5405006][-1.9448853 -2.6300559 -3.0113759 -2.8922787 -2.6038423 -2.422718 -2.0888972 -1.8562195 -1.9969261 -2.2101693 -2.3573995 -2.5165243 -2.3350196 -1.5328195 -1.2916071][-1.8318253 -2.5621967 -2.8963547 -2.7121103 -2.33849 -2.0495818 -1.6822107 -1.534754 -1.8179252 -2.1049106 -2.21533 -2.3228915 -2.1294563 -1.3683245 -1.2136447][-1.8662689 -2.6188323 -2.8817396 -2.6210847 -2.1951623 -1.8763454 -1.5724247 -1.5551555 -1.9311819 -2.2076802 -2.2425282 -2.3299758 -2.1867507 -1.5118737 -1.4095995][-2.1281154 -2.848568 -2.9882319 -2.6455157 -2.1881111 -1.8932073 -1.7100968 -1.8315685 -2.247051 -2.4674721 -2.4320412 -2.5158768 -2.4318392 -1.8380153 -1.7616389][-2.5220494 -3.136085 -3.1515884 -2.7507863 -2.2697132 -2.0004294 -1.9297214 -2.1535275 -2.5686567 -2.7550542 -2.698885 -2.79013 -2.7376537 -2.1930847 -2.1155987][-2.8456991 -3.3312807 -3.2717552 -2.8666177 -2.3765447 -2.11594 -2.1254582 -2.4155266 -2.8271618 -3.0157092 -2.9739752 -3.0509872 -2.9797769 -2.4539232 -2.3719199][-3.1058707 -3.4888244 -3.4119754 -3.0290921 -2.5272467 -2.2681332 -2.3389652 -2.6620975 -3.05744 -3.2680867 -3.2580864 -3.308444 -3.2020893 -2.7173829 -2.6362276][-3.2571487 -3.5419223 -3.4550722 -3.0864496 -2.585201 -2.3473074 -2.4738264 -2.7844646 -3.1198831 -3.3267031 -3.3506489 -3.384686 -3.2562733 -2.8520665 -2.7885218][-3.2065043 -3.4060249 -3.3100843 -2.9657118 -2.513247 -2.32884 -2.4777102 -2.7306497 -2.9774504 -3.1307251 -3.1433043 -3.1484494 -3.0311875 -2.7523134 -2.7227364]]...]
INFO - root - 2017-12-07 22:30:12.596375: step 17910, loss = 0.78, batch loss = 0.70 (7.0 examples/sec; 4.563 sec/batch; 82h:39m:36s remains)
INFO - root - 2017-12-07 22:30:57.820937: step 17920, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.513 sec/batch; 81h:44m:37s remains)
INFO - root - 2017-12-07 22:31:43.242366: step 17930, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.526 sec/batch; 81h:57m:27s remains)
INFO - root - 2017-12-07 22:32:28.607828: step 17940, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.556 sec/batch; 82h:29m:55s remains)
INFO - root - 2017-12-07 22:33:14.286655: step 17950, loss = 0.85, batch loss = 0.78 (7.0 examples/sec; 4.571 sec/batch; 82h:45m:19s remains)
INFO - root - 2017-12-07 22:33:59.522168: step 17960, loss = 0.73, batch loss = 0.65 (7.1 examples/sec; 4.487 sec/batch; 81h:13m:25s remains)
INFO - root - 2017-12-07 22:34:45.002032: step 17970, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.579 sec/batch; 82h:52m:34s remains)
INFO - root - 2017-12-07 22:35:30.616287: step 17980, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.577 sec/batch; 82h:50m:01s remains)
INFO - root - 2017-12-07 22:36:16.095559: step 17990, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.543 sec/batch; 82h:12m:06s remains)
INFO - root - 2017-12-07 22:37:01.663047: step 18000, loss = 0.73, batch loss = 0.65 (7.1 examples/sec; 4.513 sec/batch; 81h:37m:59s remains)
2017-12-07 22:37:04.408940: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7067659 -2.9308407 -3.2353158 -3.53054 -3.6261604 -3.5517378 -3.4654675 -3.4473929 -3.3746781 -3.2583888 -3.1617875 -3.1581535 -3.221664 -3.2988033 -3.3415217][-2.8835263 -3.1782384 -3.4760041 -3.7982464 -3.9937606 -4.0386515 -4.0690708 -4.1400609 -4.1121259 -3.9802568 -3.8277941 -3.745997 -3.7027431 -3.6297879 -3.4941878][-3.1224113 -3.4820876 -3.7281241 -4.0166588 -4.2785578 -4.4302154 -4.5959058 -4.7487888 -4.7522945 -4.6057611 -4.4194918 -4.366015 -4.4041767 -4.4361324 -4.3449869][-3.0530348 -3.2946959 -3.3119602 -3.3535881 -3.4515467 -3.5273426 -3.6574318 -3.7495482 -3.6848567 -3.4762342 -3.2434282 -3.2165198 -3.327311 -3.4274869 -3.3516557][-2.9559116 -3.1832862 -3.1937709 -3.2294457 -3.2908688 -3.3447645 -3.4101911 -3.3585985 -3.1996238 -3.0097167 -2.9252257 -3.0563698 -3.2180612 -3.2251983 -2.9604194][-2.6060338 -2.6920547 -2.7062969 -2.8354945 -2.9676723 -3.0866458 -3.1484475 -3.0231133 -2.8318176 -2.7270024 -2.8131919 -3.0510836 -3.240099 -3.2404423 -2.9532931][-2.296967 -2.1840911 -2.1404107 -2.2760651 -2.3577595 -2.4106917 -2.4108064 -2.2560105 -2.0565276 -1.9443376 -1.9500585 -1.929529 -1.8406665 -1.67961 -1.4014144][-2.0657878 -1.9236348 -2.0110385 -2.2663822 -2.3817642 -2.4086 -2.3332996 -2.1402097 -1.9643693 -1.9382515 -2.0234456 -1.9100358 -1.6670644 -1.3864672 -1.0903296][-1.9177856 -1.7464261 -1.932368 -2.1915114 -2.2504153 -2.2162688 -2.0508966 -1.7738152 -1.5134671 -1.5142448 -1.6798608 -1.5992088 -1.3800397 -1.1999459 -1.080972][-2.1162002 -2.0334539 -2.284529 -2.5289879 -2.5338678 -2.4493217 -2.275676 -2.0380321 -1.8124917 -1.8341539 -1.9697771 -1.8267453 -1.5823212 -1.4760439 -1.5455983][-2.543741 -2.5398402 -2.7482285 -2.9564843 -2.9558501 -2.8861325 -2.7875619 -2.7192771 -2.6897132 -2.8194261 -2.9860759 -2.8592963 -2.6338091 -2.5325694 -2.6115987][-3.0662544 -3.1346412 -3.2656083 -3.4422536 -3.4767802 -3.4262958 -3.3706334 -3.3582 -3.3877709 -3.4841809 -3.5667391 -3.4327583 -3.2317395 -3.1287837 -3.1788621][-3.2355719 -3.3822346 -3.5084491 -3.7090762 -3.8272662 -3.8639784 -3.8636043 -3.8348444 -3.8161368 -3.809948 -3.7958851 -3.6612539 -3.52563 -3.5015798 -3.5962434][-3.0930469 -3.2271791 -3.3379526 -3.5085571 -3.6361949 -3.7026749 -3.7196751 -3.6786492 -3.6062655 -3.5278876 -3.4603271 -3.3569789 -3.2824392 -3.2983568 -3.4027488][-2.9600122 -3.0303626 -3.0749786 -3.1385598 -3.1690059 -3.173008 -3.1506739 -3.1034355 -3.04921 -3.0001957 -2.9580221 -2.8983169 -2.8499813 -2.8331397 -2.8650146]]...]
INFO - root - 2017-12-07 22:37:49.902011: step 18010, loss = 0.65, batch loss = 0.57 (7.0 examples/sec; 4.549 sec/batch; 82h:16m:31s remains)
INFO - root - 2017-12-07 22:38:35.240520: step 18020, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 4.605 sec/batch; 83h:17m:18s remains)
INFO - root - 2017-12-07 22:39:20.673274: step 18030, loss = 0.77, batch loss = 0.70 (7.3 examples/sec; 4.369 sec/batch; 78h:59m:46s remains)
INFO - root - 2017-12-07 22:40:06.472604: step 18040, loss = 0.68, batch loss = 0.61 (6.9 examples/sec; 4.637 sec/batch; 83h:49m:48s remains)
INFO - root - 2017-12-07 22:40:51.922195: step 18050, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 4.496 sec/batch; 81h:15m:51s remains)
INFO - root - 2017-12-07 22:41:37.458876: step 18060, loss = 0.81, batch loss = 0.74 (7.2 examples/sec; 4.444 sec/batch; 80h:19m:34s remains)
INFO - root - 2017-12-07 22:42:23.152051: step 18070, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.583 sec/batch; 82h:49m:34s remains)
INFO - root - 2017-12-07 22:43:08.970707: step 18080, loss = 0.82, batch loss = 0.74 (6.9 examples/sec; 4.621 sec/batch; 83h:29m:12s remains)
INFO - root - 2017-12-07 22:43:54.486312: step 18090, loss = 0.73, batch loss = 0.66 (7.2 examples/sec; 4.464 sec/batch; 80h:38m:31s remains)
INFO - root - 2017-12-07 22:44:40.297465: step 18100, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 4.615 sec/batch; 83h:21m:49s remains)
2017-12-07 22:44:43.060900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7502608 -3.7688074 -3.8720202 -4.71471 -5.400012 -5.3692174 -4.4844751 -2.9506867 -1.8473303 -1.5919867 -1.6243794 -2.8698339 -5.0544696 -5.8082571 -4.7742939][-2.8756318 -2.7579648 -3.0326357 -4.1265922 -4.8899732 -4.7934561 -3.9036164 -2.4661579 -1.5230203 -1.2566278 -1.1766193 -2.4579878 -4.7302904 -5.50452 -4.4245238][-1.8886654 -1.7498152 -2.0794711 -3.0190048 -3.5433369 -3.4908731 -3.0167859 -2.0506961 -1.3531928 -1.0414314 -0.89819169 -2.1312838 -4.2796588 -4.949769 -3.8404098][-1.6407344 -1.625421 -1.9629321 -2.4692249 -2.5450287 -2.5105577 -2.3680906 -1.6089113 -0.9153862 -0.65641284 -0.7160387 -2.0111058 -3.969161 -4.4783955 -3.3544774][-1.8458836 -2.0116613 -2.4013693 -2.5559726 -2.2470212 -2.0158517 -1.5868518 -0.50057936 0.28879976 0.17802668 -0.5031774 -2.1407444 -3.9853354 -4.3095503 -3.1487231][-1.7734253 -2.0310805 -2.4821987 -2.5073223 -2.035156 -1.3276627 -0.042506695 1.6527371 2.296916 1.3509812 -0.28645611 -2.4357018 -4.2337594 -4.3791609 -3.1961823][-1.5671668 -1.8237009 -2.3278871 -2.4455943 -1.9755292 -0.6366086 1.6914315 3.8729162 4.0377378 2.094173 -0.33159876 -2.7423739 -4.4497685 -4.5007262 -3.3515704][-1.2311149 -1.3466966 -1.8592086 -2.2348249 -2.0233059 -0.50144887 2.2371941 4.4151745 4.1518517 1.8506145 -0.57290125 -2.7535987 -4.3260431 -4.42485 -3.4248774][-0.98934031 -0.92606044 -1.4155309 -2.1009276 -2.3550985 -1.192821 1.2568121 3.1311035 2.8975897 1.1269026 -0.57257152 -2.2706368 -3.8072519 -4.1359377 -3.4104578][-0.94257975 -0.77092004 -1.2493935 -2.1633704 -2.8167806 -2.1694381 -0.23653841 1.3933573 1.5465074 0.69160986 -0.14757347 -1.475379 -3.1143975 -3.7553067 -3.3663087][-0.90544534 -0.70130825 -1.1251569 -2.1103969 -2.9705782 -2.7595677 -1.4036453 -0.058795452 0.43263865 0.28716421 0.02527523 -1.0493069 -2.6369216 -3.4106927 -3.2818851][-1.0465593 -0.788882 -1.037255 -1.8936245 -2.7487791 -2.8328888 -2.0148273 -1.0721231 -0.59460092 -0.52899551 -0.60218477 -1.3968759 -2.636663 -3.27909 -3.2542491][-1.3667247 -1.0520842 -1.1332703 -1.8234451 -2.5921402 -2.83172 -2.4101815 -1.8280065 -1.5314593 -1.5124254 -1.539587 -2.0022483 -2.7870541 -3.2208138 -3.2318652][-1.51753 -1.29284 -1.4190125 -2.0456803 -2.7324314 -2.9930062 -2.7519689 -2.3685911 -2.2152941 -2.2465518 -2.2394724 -2.4510722 -2.9061782 -3.198245 -3.2283869][-1.8262882 -1.7906771 -2.0322013 -2.573956 -3.0660737 -3.2106414 -3.0100684 -2.7216415 -2.6231828 -2.6485929 -2.6297412 -2.748497 -3.0389335 -3.2251348 -3.2345014]]...]
INFO - root - 2017-12-07 22:45:28.131540: step 18110, loss = 0.74, batch loss = 0.66 (7.2 examples/sec; 4.422 sec/batch; 79h:51m:11s remains)
INFO - root - 2017-12-07 22:46:13.768446: step 18120, loss = 0.72, batch loss = 0.64 (6.9 examples/sec; 4.607 sec/batch; 83h:11m:37s remains)
INFO - root - 2017-12-07 22:46:59.356706: step 18130, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.605 sec/batch; 83h:08m:41s remains)
INFO - root - 2017-12-07 22:47:44.987270: step 18140, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.544 sec/batch; 82h:01m:28s remains)
INFO - root - 2017-12-07 22:48:30.723756: step 18150, loss = 0.81, batch loss = 0.73 (7.1 examples/sec; 4.524 sec/batch; 81h:38m:34s remains)
INFO - root - 2017-12-07 22:49:16.458843: step 18160, loss = 0.71, batch loss = 0.64 (7.2 examples/sec; 4.458 sec/batch; 80h:27m:06s remains)
INFO - root - 2017-12-07 22:50:02.322099: step 18170, loss = 0.73, batch loss = 0.65 (6.9 examples/sec; 4.634 sec/batch; 83h:36m:47s remains)
INFO - root - 2017-12-07 22:50:47.767796: step 18180, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.615 sec/batch; 83h:14m:58s remains)
INFO - root - 2017-12-07 22:51:33.338280: step 18190, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.596 sec/batch; 82h:53m:46s remains)
INFO - root - 2017-12-07 22:52:19.022378: step 18200, loss = 0.87, batch loss = 0.80 (6.9 examples/sec; 4.608 sec/batch; 83h:06m:27s remains)
2017-12-07 22:52:21.769043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7754626 -2.3541255 -2.0897679 -1.5782809 -0.84927654 -0.85588932 -1.0468016 -1.1121638 -1.4717987 -1.5190091 -1.3992035 -1.552927 -1.7365403 -1.5613751 -1.3273404][-2.6713986 -2.2667484 -2.0473847 -1.5273485 -0.81124592 -0.89603496 -1.1120563 -1.1985567 -1.6572049 -1.7803044 -1.6889119 -1.8144958 -1.9577854 -1.7996342 -1.4782903][-2.720058 -2.4457493 -2.3113825 -1.8023806 -1.0965126 -1.1892927 -1.3190866 -1.324235 -1.7789748 -1.8910267 -1.8032858 -1.8543522 -1.9103825 -1.8117685 -1.5050454][-2.7948513 -2.5928202 -2.5115314 -2.0748229 -1.4575553 -1.4969273 -1.3717918 -1.1441333 -1.44386 -1.4412813 -1.3651667 -1.4069841 -1.4553716 -1.5131166 -1.3595293][-2.7821338 -2.5657964 -2.5076158 -2.1619818 -1.6587923 -1.5848081 -1.1670027 -0.74369979 -0.9394927 -0.87331414 -0.84474516 -0.96123934 -1.0698302 -1.2990184 -1.3365169][-2.6393995 -2.3823292 -2.3869746 -2.1393294 -1.7017057 -1.4789495 -0.83957195 -0.38039827 -0.603143 -0.54198217 -0.49735379 -0.57180429 -0.66669345 -0.99820781 -1.1981535][-2.5118771 -2.2522347 -2.341681 -2.146029 -1.7134616 -1.3451867 -0.56642246 -0.0897336 -0.29372644 -0.2487092 -0.17496777 -0.16068459 -0.20323944 -0.55334616 -0.82564783][-2.4018545 -2.1540394 -2.2779224 -2.0794826 -1.6441331 -1.220494 -0.38584089 0.10609865 -0.02316761 -0.0035896301 0.044446945 0.097399712 0.092961788 -0.22089005 -0.48803282][-2.5161672 -2.3705127 -2.5359054 -2.351326 -1.9555848 -1.5959108 -0.84505439 -0.37023163 -0.37388515 -0.376729 -0.40193462 -0.40365267 -0.43077803 -0.66432285 -0.839087][-2.8350434 -2.8444796 -3.0573823 -2.9270549 -2.6188757 -2.388211 -1.8290865 -1.4125159 -1.2779202 -1.2394588 -1.326267 -1.4310641 -1.5444613 -1.7032301 -1.741344][-2.8939013 -2.9338737 -3.1133173 -3.0018554 -2.7471638 -2.6075563 -2.2646997 -1.9931023 -1.8257651 -1.7643275 -1.8843966 -2.0677621 -2.2679808 -2.3776367 -2.328064][-2.8543253 -2.8667467 -2.9941216 -2.8965609 -2.6543427 -2.5054011 -2.2785006 -2.1574409 -2.1013758 -2.1050448 -2.2187331 -2.3530788 -2.5096226 -2.5673709 -2.5198565][-3.0194976 -3.0634038 -3.1761329 -3.1010199 -2.8997591 -2.7471085 -2.5194988 -2.4031246 -2.4124629 -2.4918089 -2.5915442 -2.6344552 -2.6935368 -2.7111378 -2.6832337][-3.4719765 -3.6157215 -3.7624381 -3.7201405 -3.5717068 -3.4651089 -3.3006868 -3.1895885 -3.2038474 -3.2985554 -3.3746581 -3.3421612 -3.3196602 -3.3079586 -3.2893624][-3.7379582 -3.9473119 -4.1089177 -4.0959549 -4.0182285 -3.9928405 -3.9326711 -3.8784089 -3.8974886 -3.9700379 -4.029408 -3.9885836 -3.9450483 -3.9245727 -3.9090741]]...]
INFO - root - 2017-12-07 22:53:07.127794: step 18210, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.568 sec/batch; 82h:21m:53s remains)
INFO - root - 2017-12-07 22:53:52.678254: step 18220, loss = 0.67, batch loss = 0.59 (7.0 examples/sec; 4.588 sec/batch; 82h:42m:42s remains)
INFO - root - 2017-12-07 22:54:38.275213: step 18230, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.503 sec/batch; 81h:09m:50s remains)
INFO - root - 2017-12-07 22:55:23.693988: step 18240, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.541 sec/batch; 81h:50m:26s remains)
INFO - root - 2017-12-07 22:56:09.292396: step 18250, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 4.600 sec/batch; 82h:54m:12s remains)
INFO - root - 2017-12-07 22:56:54.620803: step 18260, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.574 sec/batch; 82h:24m:45s remains)
INFO - root - 2017-12-07 22:57:40.220451: step 18270, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.582 sec/batch; 82h:32m:17s remains)
INFO - root - 2017-12-07 22:58:25.711763: step 18280, loss = 0.85, batch loss = 0.78 (7.2 examples/sec; 4.448 sec/batch; 80h:07m:26s remains)
INFO - root - 2017-12-07 22:59:11.351454: step 18290, loss = 0.84, batch loss = 0.77 (6.9 examples/sec; 4.630 sec/batch; 83h:23m:30s remains)
INFO - root - 2017-12-07 22:59:56.723904: step 18300, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.531 sec/batch; 81h:34m:49s remains)
2017-12-07 22:59:59.440107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6319289 -3.7462616 -3.8565984 -3.9539304 -4.0413313 -4.1073165 -4.1283197 -4.106132 -4.0608497 -3.9988539 -3.9229667 -3.832911 -3.7228475 -3.6103148 -3.5268006][-3.7129364 -3.8697567 -3.9839952 -4.05701 -4.1350241 -4.2118788 -4.2317524 -4.1930151 -4.128221 -4.0505428 -3.9676273 -3.8675191 -3.7177649 -3.5506172 -3.4502728][-3.8079131 -3.9678841 -4.00271 -3.956851 -3.9450715 -3.9730346 -3.9532962 -3.9086227 -3.86052 -3.7922401 -3.7228606 -3.6570563 -3.5140662 -3.3320649 -3.2496395][-3.925456 -4.0061378 -3.8602426 -3.6133192 -3.4625397 -3.425735 -3.3867154 -3.4003844 -3.4492779 -3.4449539 -3.3886461 -3.3451607 -3.2309663 -3.08495 -3.0570946][-3.9919198 -3.8933046 -3.5335515 -3.1331487 -2.8995755 -2.8416948 -2.7948215 -2.8242316 -2.9291203 -2.9971685 -2.980238 -2.9644308 -2.9083185 -2.8718061 -2.9625099][-3.898226 -3.5608442 -3.05409 -2.6476436 -2.4304612 -2.3305273 -2.1032014 -1.9033484 -1.9534793 -2.1447029 -2.2720594 -2.3601379 -2.459672 -2.6410847 -2.9272361][-3.5482774 -2.97886 -2.401283 -2.0925467 -1.9463625 -1.7323489 -1.160197 -0.60216522 -0.65990925 -1.1269763 -1.4967616 -1.7439001 -2.0547781 -2.48182 -2.9380307][-3.0128794 -2.2412899 -1.6056185 -1.3831851 -1.312634 -1.0134261 -0.27562237 0.35535049 0.072309494 -0.70026875 -1.2284119 -1.5631051 -2.0068891 -2.561903 -3.0589082][-2.5913267 -1.7097003 -1.0528333 -0.8889358 -0.923409 -0.72876525 -0.24184942 0.025522232 -0.42918563 -1.166383 -1.5584118 -1.8136911 -2.2598586 -2.8170295 -3.2615094][-2.5386686 -1.6908686 -1.1027853 -1.0221531 -1.2099061 -1.2553947 -1.1405172 -1.1935353 -1.5802512 -1.9931297 -2.0951767 -2.1730266 -2.535408 -3.0390153 -3.4129949][-2.7929611 -2.1252322 -1.7049806 -1.7269998 -2.0336082 -2.2503381 -2.3196824 -2.4487147 -2.6512866 -2.7629895 -2.6693635 -2.6203766 -2.8642712 -3.256161 -3.5218105][-3.1619105 -2.7559838 -2.5515511 -2.6552238 -2.950418 -3.1678843 -3.2270355 -3.2879148 -3.36446 -3.3527246 -3.2300138 -3.1517406 -3.2758207 -3.5002949 -3.6207998][-3.4903965 -3.3359818 -3.2963181 -3.4098091 -3.5910711 -3.6927888 -3.6777494 -3.6668916 -3.6893489 -3.6623566 -3.5839965 -3.5265903 -3.5642092 -3.6437879 -3.6548371][-3.6549306 -3.6538227 -3.6852086 -3.7652662 -3.8372438 -3.8457904 -3.7947707 -3.758637 -3.7562275 -3.7300687 -3.6849728 -3.6482863 -3.6458189 -3.6519921 -3.6157751][-3.6388798 -3.6867943 -3.7329211 -3.7827773 -3.8028135 -3.7801838 -3.7339315 -3.701278 -3.68649 -3.6673396 -3.6397252 -3.6192214 -3.6078062 -3.5897965 -3.5500674]]...]
INFO - root - 2017-12-07 23:00:45.076689: step 18310, loss = 0.71, batch loss = 0.63 (7.1 examples/sec; 4.538 sec/batch; 81h:41m:54s remains)
INFO - root - 2017-12-07 23:01:30.710792: step 18320, loss = 0.89, batch loss = 0.81 (7.0 examples/sec; 4.586 sec/batch; 82h:33m:17s remains)
INFO - root - 2017-12-07 23:02:16.527410: step 18330, loss = 0.66, batch loss = 0.59 (7.1 examples/sec; 4.496 sec/batch; 80h:55m:35s remains)
INFO - root - 2017-12-07 23:03:02.231849: step 18340, loss = 0.68, batch loss = 0.61 (6.9 examples/sec; 4.619 sec/batch; 83h:07m:51s remains)
INFO - root - 2017-12-07 23:03:48.213476: step 18350, loss = 0.87, batch loss = 0.80 (6.9 examples/sec; 4.616 sec/batch; 83h:02m:58s remains)
INFO - root - 2017-12-07 23:04:33.799258: step 18360, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.549 sec/batch; 81h:50m:03s remains)
INFO - root - 2017-12-07 23:05:19.497811: step 18370, loss = 0.83, batch loss = 0.75 (7.0 examples/sec; 4.575 sec/batch; 82h:17m:26s remains)
INFO - root - 2017-12-07 23:06:05.234227: step 18380, loss = 0.76, batch loss = 0.68 (7.1 examples/sec; 4.482 sec/batch; 80h:36m:10s remains)
INFO - root - 2017-12-07 23:06:50.802514: step 18390, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 4.621 sec/batch; 83h:05m:44s remains)
INFO - root - 2017-12-07 23:07:36.565205: step 18400, loss = 0.82, batch loss = 0.74 (7.2 examples/sec; 4.455 sec/batch; 80h:06m:10s remains)
2017-12-07 23:07:39.256343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7076187 -1.4878948 -1.8619573 -2.4757226 -3.0966678 -3.613209 -4.0789247 -4.4424005 -4.419229 -3.9414515 -3.6062646 -3.7530155 -3.8279533 -3.8112445 -3.8582501][-1.5473232 -1.277344 -1.667047 -2.1151805 -2.5521195 -3.263011 -3.9776208 -4.2978806 -4.1589832 -3.7208409 -3.5896673 -3.9891365 -4.0718603 -3.8609109 -3.7870703][-2.3108363 -1.8973143 -2.0440147 -2.2116892 -2.4442873 -3.1648674 -3.7128396 -3.6501789 -3.395412 -3.2763276 -3.6085138 -4.3278394 -4.4784775 -4.1165075 -3.89535][-3.236089 -2.683497 -2.6935594 -2.9268603 -3.1803679 -3.5578604 -3.408257 -2.7587898 -2.4852226 -2.7389457 -3.3910465 -4.281743 -4.5745182 -4.2526679 -3.9347391][-3.6440952 -3.0190203 -3.1396165 -3.7518992 -4.097096 -3.9389591 -3.0016589 -1.9398215 -1.7452228 -2.2959044 -3.1470926 -4.1388321 -4.5696959 -4.3116407 -3.8702321][-3.2527881 -2.6055562 -2.9330657 -3.8062658 -4.1255527 -3.5398722 -2.0934269 -0.8620348 -0.85721517 -1.8618777 -3.121686 -4.2244039 -4.6095734 -4.2785039 -3.7885318][-2.3122995 -1.8353717 -2.433466 -3.3207881 -3.3717282 -2.3587761 -0.56220937 0.77411413 0.43959951 -1.1941159 -2.9419065 -3.9840708 -4.1006408 -3.71533 -3.4410713][-1.5045252 -1.266721 -1.9397831 -2.5495088 -2.1951807 -0.96424818 0.74621964 1.7472382 0.83478594 -1.2370677 -3.0285978 -3.6741736 -3.496171 -3.2762282 -3.4115644][-1.135782 -0.99314141 -1.4818859 -1.767324 -1.2931855 -0.39783382 0.57462692 0.8020649 -0.3743248 -2.1002336 -3.2630556 -3.5009656 -3.4413226 -3.61535 -3.9941459][-1.1036823 -1.0385983 -1.3947287 -1.6816213 -1.542619 -1.2550714 -0.98026085 -1.1238694 -1.9451704 -2.830796 -3.2866974 -3.468142 -3.8082514 -4.1809883 -4.2719769][-1.5451989 -1.6495159 -2.0468321 -2.4847178 -2.6981936 -2.7955642 -2.8335075 -2.9446583 -3.2164052 -3.4022062 -3.5512259 -3.8997433 -4.2904582 -4.2178965 -3.6463127][-2.3794003 -2.6688635 -3.0889397 -3.5778604 -3.9315975 -4.1513562 -4.2296152 -4.1360989 -4.0334778 -4.0134673 -4.2176819 -4.4956584 -4.3771558 -3.6125393 -2.7270832][-3.4441462 -3.7122784 -3.973871 -4.3099461 -4.5855231 -4.7597651 -4.77423 -4.4579906 -4.1248689 -4.0513964 -4.1835823 -4.0854936 -3.3733299 -2.3117712 -1.7408199][-4.2669296 -4.2981339 -4.3041239 -4.4429708 -4.5414681 -4.5744648 -4.5125194 -4.0725 -3.6569941 -3.4884386 -3.3685508 -2.9397435 -2.0872726 -1.3761628 -1.4528813][-4.1875367 -3.9872122 -3.8548563 -3.9401739 -3.9426839 -3.8499191 -3.7003341 -3.2467341 -2.9069214 -2.7503753 -2.5925937 -2.2701702 -1.8170552 -1.6986043 -2.0732355]]...]
INFO - root - 2017-12-07 23:08:25.056554: step 18410, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.563 sec/batch; 82h:01m:58s remains)
INFO - root - 2017-12-07 23:09:10.768748: step 18420, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.529 sec/batch; 81h:23m:44s remains)
INFO - root - 2017-12-07 23:09:56.150744: step 18430, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.577 sec/batch; 82h:15m:21s remains)
INFO - root - 2017-12-07 23:10:41.971111: step 18440, loss = 0.72, batch loss = 0.64 (6.9 examples/sec; 4.623 sec/batch; 83h:04m:20s remains)
INFO - root - 2017-12-07 23:11:27.605945: step 18450, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.608 sec/batch; 82h:46m:43s remains)
INFO - root - 2017-12-07 23:12:13.279906: step 18460, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.555 sec/batch; 81h:49m:26s remains)
INFO - root - 2017-12-07 23:12:59.008726: step 18470, loss = 0.89, batch loss = 0.81 (7.1 examples/sec; 4.515 sec/batch; 81h:05m:40s remains)
INFO - root - 2017-12-07 23:13:44.654734: step 18480, loss = 0.75, batch loss = 0.67 (7.1 examples/sec; 4.532 sec/batch; 81h:22m:39s remains)
INFO - root - 2017-12-07 23:14:30.326868: step 18490, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.609 sec/batch; 82h:44m:37s remains)
INFO - root - 2017-12-07 23:15:15.926294: step 18500, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.597 sec/batch; 82h:30m:53s remains)
2017-12-07 23:15:18.674991: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.338 -2.4881613 -2.6719108 -2.9023538 -2.9621649 -2.7951512 -2.474792 -2.1312456 -2.0423567 -2.2620256 -2.6562839 -3.110076 -3.4500027 -3.5891304 -3.6014214][-2.2058601 -2.4660139 -2.7910843 -3.0647674 -3.0599358 -2.8190169 -2.441597 -1.9835587 -1.6947725 -1.7230449 -2.120142 -2.7779076 -3.3352854 -3.565048 -3.5673444][-2.1088083 -2.592927 -3.0098405 -3.1964672 -3.1483872 -3.0606389 -2.8510375 -2.3373199 -1.7946925 -1.5188658 -1.7870238 -2.4754477 -3.0850344 -3.3379917 -3.3385596][-1.9085553 -2.7050269 -3.2148538 -3.2875707 -3.1968663 -3.2621055 -3.2033575 -2.7007332 -2.1001287 -1.7312334 -1.8804193 -2.4131303 -2.8336463 -2.9392776 -2.8720789][-1.5578821 -2.4586487 -2.9540904 -2.9530535 -2.8459079 -2.8978636 -2.7765882 -2.3537436 -2.1464334 -2.1749432 -2.3568351 -2.6131635 -2.6668072 -2.5185184 -2.3687875][-1.3842754 -2.0862312 -2.38947 -2.3205936 -2.152514 -1.915833 -1.3945613 -1.0307386 -1.5783083 -2.4069598 -2.8203554 -2.853092 -2.5198784 -2.0485497 -1.7681091][-1.5906024 -2.0696859 -2.189265 -2.0645773 -1.7327578 -0.96653271 0.20217228 0.6805439 -0.47642922 -2.0032606 -2.7717214 -2.8289158 -2.2987144 -1.5611496 -1.1119182][-1.9755409 -2.3546891 -2.450747 -2.4566808 -2.1891818 -1.2195768 0.29293919 1.0020313 -0.078305721 -1.5664797 -2.4224989 -2.6690803 -2.2539294 -1.5095992 -1.0126591][-2.0981886 -2.5388193 -2.7717092 -3.0289383 -3.07398 -2.4269867 -1.2274616 -0.50606322 -0.94475031 -1.6984918 -2.2354131 -2.6379704 -2.5170031 -1.9758482 -1.6011636][-1.800575 -2.3111892 -2.6202846 -2.9804211 -3.2271757 -3.0374088 -2.5091753 -2.1084065 -2.0920467 -2.1138158 -2.2244148 -2.6789246 -2.8096762 -2.4797916 -2.2724354][-1.5008931 -1.9756267 -2.1650882 -2.3306053 -2.4877996 -2.5894594 -2.747412 -2.8738108 -2.755841 -2.3068957 -2.0608585 -2.4550343 -2.7236841 -2.5216289 -2.3623283][-1.638258 -2.0493181 -2.013298 -1.7980926 -1.6210377 -1.7029965 -2.2122521 -2.7669394 -2.8186517 -2.2873948 -1.8909543 -2.1511207 -2.3644028 -2.1729367 -1.9784365][-2.160454 -2.4647288 -2.2728121 -1.8084164 -1.3701532 -1.2666056 -1.7031791 -2.3223295 -2.5036149 -2.183933 -1.9552479 -2.1689725 -2.2207112 -1.8899474 -1.6001375][-2.6343036 -2.7507906 -2.5124936 -2.1039464 -1.7078688 -1.5270998 -1.7578502 -2.1473138 -2.2318585 -2.0862746 -2.1006458 -2.3279605 -2.2632048 -1.8412347 -1.549176][-2.684176 -2.6237278 -2.4130254 -2.2201207 -2.0783031 -2.0062189 -2.1089044 -2.2391722 -2.1309519 -2.0195262 -2.133491 -2.3412251 -2.2635849 -1.9394648 -1.8372705]]...]
INFO - root - 2017-12-07 23:16:04.493452: step 18510, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.612 sec/batch; 82h:46m:36s remains)
INFO - root - 2017-12-07 23:16:50.421609: step 18520, loss = 0.80, batch loss = 0.72 (7.0 examples/sec; 4.597 sec/batch; 82h:29m:48s remains)
INFO - root - 2017-12-07 23:17:35.841668: step 18530, loss = 0.76, batch loss = 0.69 (7.3 examples/sec; 4.407 sec/batch; 79h:04m:47s remains)
INFO - root - 2017-12-07 23:18:21.584069: step 18540, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.576 sec/batch; 82h:05m:59s remains)
INFO - root - 2017-12-07 23:19:06.912147: step 18550, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.582 sec/batch; 82h:11m:10s remains)
INFO - root - 2017-12-07 23:19:52.751358: step 18560, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.572 sec/batch; 81h:59m:50s remains)
INFO - root - 2017-12-07 23:20:37.963577: step 18570, loss = 0.82, batch loss = 0.75 (7.2 examples/sec; 4.453 sec/batch; 79h:51m:16s remains)
INFO - root - 2017-12-07 23:21:23.781828: step 18580, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 4.617 sec/batch; 82h:46m:26s remains)
INFO - root - 2017-12-07 23:22:09.327100: step 18590, loss = 0.74, batch loss = 0.66 (7.0 examples/sec; 4.563 sec/batch; 81h:47m:51s remains)
INFO - root - 2017-12-07 23:22:54.951885: step 18600, loss = 0.83, batch loss = 0.76 (6.8 examples/sec; 4.689 sec/batch; 84h:02m:49s remains)
2017-12-07 23:22:57.723224: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2627637 -3.3143554 -3.2590218 -3.1400242 -3.0653396 -3.0626304 -3.2245512 -3.3762345 -3.5382767 -3.6440985 -3.6015606 -3.609046 -3.7426953 -3.8125744 -3.7250967][-3.5293345 -3.6458712 -3.6700165 -3.5344973 -3.3846912 -3.182476 -3.07301 -3.0784249 -3.2588568 -3.4143574 -3.4426787 -3.5844126 -3.8309531 -3.8760765 -3.6138809][-3.8974478 -4.0188689 -4.091701 -3.8729658 -3.5686669 -3.1102583 -2.6904905 -2.5543809 -2.7687488 -2.9918766 -3.1264167 -3.3770304 -3.6231711 -3.516562 -3.00471][-4.2213984 -4.2712884 -4.256793 -3.8396626 -3.3506074 -2.7415116 -2.2216039 -2.1797805 -2.511796 -2.7504897 -2.9686477 -3.2583036 -3.3867142 -3.0588436 -2.3538299][-4.1792068 -4.0594611 -3.867758 -3.2579966 -2.6294971 -1.8656631 -1.2619953 -1.4477868 -2.007931 -2.3008714 -2.5783932 -2.9024451 -2.9656553 -2.5535312 -1.8498006][-3.7751551 -3.515079 -3.193891 -2.4741459 -1.6997097 -0.61264014 0.33013964 -0.00356102 -0.96136808 -1.5616467 -2.023448 -2.4610806 -2.5240531 -2.09948 -1.4685979][-3.2411497 -2.901669 -2.5467758 -1.8304675 -0.911644 0.54409266 1.9002562 1.404582 -0.11329031 -1.1567848 -1.7807522 -2.2307708 -2.2627785 -1.852282 -1.3377068][-2.7645559 -2.3190646 -1.9223788 -1.310478 -0.43495917 1.0853243 2.5462179 1.952888 0.12636995 -1.1438932 -1.7754343 -2.0470407 -1.9117942 -1.5502019 -1.2803361][-2.3643098 -1.770808 -1.3935018 -1.0843213 -0.59890795 0.58128405 1.8602433 1.491416 -0.117558 -1.3275709 -1.8161411 -1.8179898 -1.419934 -1.0804305 -1.0861008][-2.13576 -1.4716604 -1.1796827 -1.2527604 -1.2919202 -0.525892 0.62331057 0.63820314 -0.5258 -1.4882646 -1.777719 -1.5550218 -0.98954797 -0.682529 -0.88129449][-2.2034543 -1.5946527 -1.3744636 -1.6497655 -1.9310589 -1.3671854 -0.25763988 0.065986156 -0.72682595 -1.4793024 -1.6590354 -1.3847375 -0.83332276 -0.60667372 -0.90459037][-2.4007773 -1.864017 -1.6178045 -1.87497 -2.2164774 -1.8261945 -0.8669343 -0.47461987 -1.0662374 -1.6916409 -1.7784231 -1.4114866 -0.81428933 -0.61500359 -0.97349095][-2.5802712 -2.0811217 -1.7884529 -1.9830494 -2.3625071 -2.2030287 -1.5222292 -1.1560411 -1.5280833 -1.9793382 -2.0002136 -1.6082878 -0.98349476 -0.753201 -1.1102679][-2.7705698 -2.3866141 -2.1501737 -2.3362381 -2.7375765 -2.7560472 -2.3070173 -1.9768322 -2.1536527 -2.4529624 -2.4563599 -2.1202557 -1.5694039 -1.318927 -1.5640025][-2.9369729 -2.7583311 -2.7012615 -2.8890271 -3.2085705 -3.2813203 -3.0116038 -2.7730815 -2.8053646 -2.9173112 -2.8408318 -2.5720327 -2.1768472 -1.9490623 -2.0721481]]...]
INFO - root - 2017-12-07 23:23:43.371664: step 18610, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 4.638 sec/batch; 83h:07m:27s remains)
INFO - root - 2017-12-07 23:24:29.105801: step 18620, loss = 0.87, batch loss = 0.79 (7.0 examples/sec; 4.594 sec/batch; 82h:18m:25s remains)
INFO - root - 2017-12-07 23:25:14.644735: step 18630, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.511 sec/batch; 80h:48m:57s remains)
INFO - root - 2017-12-07 23:26:00.148523: step 18640, loss = 0.74, batch loss = 0.66 (6.8 examples/sec; 4.673 sec/batch; 83h:42m:40s remains)
INFO - root - 2017-12-07 23:26:46.060278: step 18650, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 4.646 sec/batch; 83h:12m:46s remains)
INFO - root - 2017-12-07 23:27:31.943762: step 18660, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.611 sec/batch; 82h:34m:13s remains)
INFO - root - 2017-12-07 23:28:17.550014: step 18670, loss = 0.88, batch loss = 0.81 (7.1 examples/sec; 4.486 sec/batch; 80h:19m:21s remains)
INFO - root - 2017-12-07 23:29:03.061324: step 18680, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.578 sec/batch; 81h:56m:53s remains)
INFO - root - 2017-12-07 23:29:48.718029: step 18690, loss = 0.69, batch loss = 0.62 (7.2 examples/sec; 4.453 sec/batch; 79h:42m:10s remains)
INFO - root - 2017-12-07 23:30:34.472596: step 18700, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 4.554 sec/batch; 81h:29m:50s remains)
2017-12-07 23:30:37.225781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5141954 -2.5954916 -2.6468558 -2.5606833 -2.4210951 -2.425751 -2.6952014 -2.8401432 -2.716414 -2.4534473 -2.2585447 -2.2101147 -2.2712612 -2.2622859 -2.278075][-2.2008908 -2.282362 -2.3052604 -2.2570217 -2.094007 -2.0051599 -2.18809 -2.3506098 -2.4055476 -2.38593 -2.4149501 -2.585391 -2.7514005 -2.706243 -2.5781717][-1.9280708 -2.0303295 -2.1193521 -2.1471307 -1.9311461 -1.6635096 -1.6358776 -1.6967683 -1.7791791 -1.8818228 -2.0550265 -2.3339789 -2.4775276 -2.3198624 -2.0830407][-1.8397563 -1.899369 -2.075424 -2.2465596 -2.1696229 -1.9797492 -1.956646 -2.0006204 -2.0157292 -2.0578718 -2.1428492 -2.2452378 -2.159323 -1.8188233 -1.5144827][-2.27284 -2.2323947 -2.3897853 -2.6168687 -2.6755905 -2.6286948 -2.6584322 -2.6919394 -2.6096969 -2.5129843 -2.3996863 -2.2272639 -1.9331105 -1.5273762 -1.3029213][-2.7249241 -2.5374427 -2.5033569 -2.5660768 -2.5267563 -2.4223747 -2.3196702 -2.2136185 -2.0584393 -1.926616 -1.7573311 -1.5340393 -1.2969642 -1.0863104 -1.1454861][-2.4756155 -2.1586497 -1.8939331 -1.6925335 -1.3917122 -1.0411677 -0.666394 -0.40919113 -0.36202097 -0.48486161 -0.59858704 -0.65409565 -0.69378972 -0.78523111 -1.1353557][-1.6512311 -1.2975631 -0.99382734 -0.76358366 -0.44491482 -0.03179121 0.48777723 0.83712387 0.73203135 0.29231262 -0.21458864 -0.62273455 -0.91232562 -1.1744843 -1.6056349][-1.20735 -0.89181757 -0.73668313 -0.76294017 -0.75613165 -0.61484003 -0.2350626 0.10541201 0.022733212 -0.4042058 -0.96631694 -1.4235818 -1.664695 -1.800869 -2.0578234][-1.8942964 -1.6241696 -1.5701337 -1.7999785 -2.0548995 -2.1708493 -2.0104833 -1.7771108 -1.8166659 -2.0337219 -2.35551 -2.5611992 -2.4977708 -2.3431234 -2.3323641][-2.9744191 -2.7883339 -2.7580552 -2.9772921 -3.1783409 -3.2433171 -3.1303415 -2.9919691 -3.0655532 -3.1693909 -3.2773285 -3.2417293 -2.9566154 -2.649868 -2.5166259][-3.280901 -3.1718364 -3.1589074 -3.3259416 -3.4123349 -3.3648982 -3.233078 -3.1310763 -3.2225952 -3.3160732 -3.3764493 -3.3014727 -3.0460715 -2.8257298 -2.7332649][-2.8342757 -2.7434187 -2.7537723 -2.9171267 -3.009407 -2.9818504 -2.8949735 -2.8206818 -2.881978 -2.9816716 -3.0841088 -3.1006579 -3.010891 -2.9536815 -2.9078889][-2.3938713 -2.2974896 -2.2957897 -2.4017169 -2.4750752 -2.4523416 -2.391619 -2.3312714 -2.3530245 -2.4508233 -2.582696 -2.673625 -2.6953597 -2.7369568 -2.755852][-2.2807031 -2.1984317 -2.1842549 -2.2266638 -2.2682064 -2.2328763 -2.1636829 -2.0915329 -2.0703626 -2.1351936 -2.2400799 -2.336369 -2.3894479 -2.4630303 -2.5263667]]...]
INFO - root - 2017-12-07 23:31:22.841251: step 18710, loss = 0.86, batch loss = 0.79 (7.2 examples/sec; 4.469 sec/batch; 79h:57m:44s remains)
INFO - root - 2017-12-07 23:32:08.438673: step 18720, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.581 sec/batch; 81h:56m:56s remains)
INFO - root - 2017-12-07 23:32:54.109259: step 18730, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.601 sec/batch; 82h:18m:19s remains)
INFO - root - 2017-12-07 23:33:39.603987: step 18740, loss = 0.81, batch loss = 0.74 (7.3 examples/sec; 4.399 sec/batch; 78h:40m:10s remains)
INFO - root - 2017-12-07 23:34:25.238764: step 18750, loss = 0.75, batch loss = 0.67 (7.1 examples/sec; 4.506 sec/batch; 80h:34m:58s remains)
INFO - root - 2017-12-07 23:35:10.830523: step 18760, loss = 0.77, batch loss = 0.69 (7.0 examples/sec; 4.540 sec/batch; 81h:10m:06s remains)
INFO - root - 2017-12-07 23:35:56.761587: step 18770, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.602 sec/batch; 82h:15m:35s remains)
INFO - root - 2017-12-07 23:36:42.756920: step 18780, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 4.502 sec/batch; 80h:28m:16s remains)
INFO - root - 2017-12-07 23:37:28.537833: step 18790, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.579 sec/batch; 81h:50m:06s remains)
INFO - root - 2017-12-07 23:38:14.178862: step 18800, loss = 0.71, batch loss = 0.63 (7.0 examples/sec; 4.555 sec/batch; 81h:23m:11s remains)
2017-12-07 23:38:16.956541: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5599902 -1.5540142 -1.5503156 -1.5478845 -1.5498698 -1.554842 -1.5623558 -1.5724602 -1.5887353 -1.6076603 -1.6182675 -1.6214089 -1.6179597 -1.6109066 -1.5989923][-1.5316865 -1.5267181 -1.5266037 -1.5308723 -1.5402014 -1.5522883 -1.5675597 -1.5847459 -1.6055717 -1.6288185 -1.6423514 -1.6445272 -1.6374002 -1.626545 -1.6103294][-1.5141921 -1.513947 -1.5218713 -1.5381615 -1.5593836 -1.5799587 -1.5978942 -1.6137047 -1.628782 -1.6471188 -1.6581199 -1.6566861 -1.6456826 -1.6317725 -1.6133235][-1.5147605 -1.5247617 -1.5455949 -1.5775917 -1.6134953 -1.6409464 -1.6535635 -1.6587923 -1.659148 -1.6659622 -1.6709139 -1.6642981 -1.652066 -1.6383085 -1.6212463][-1.5204775 -1.5446451 -1.5806112 -1.629786 -1.6820621 -1.717387 -1.7248425 -1.7185729 -1.703229 -1.6987453 -1.6995442 -1.6857321 -1.6689992 -1.6527076 -1.63625][-1.5143962 -1.5494363 -1.5943797 -1.6526854 -1.7165861 -1.7595618 -1.7668653 -1.7563245 -1.7323527 -1.7274168 -1.7382479 -1.7257087 -1.7038674 -1.6774921 -1.6529043][-1.5480852 -1.5889547 -1.6365314 -1.6945565 -1.7588971 -1.8023446 -1.8096302 -1.7957723 -1.7603197 -1.7580886 -1.7953606 -1.804704 -1.7906199 -1.7582731 -1.720547][-1.6447356 -1.6825941 -1.7219758 -1.764365 -1.8095632 -1.8364074 -1.8334725 -1.8109117 -1.765729 -1.7753382 -1.8576472 -1.9174895 -1.9390998 -1.9200652 -1.8705208][-1.7590671 -1.7963982 -1.8341467 -1.8684223 -1.8993337 -1.9143083 -1.9085734 -1.8841102 -1.8383148 -1.8633144 -1.9831481 -2.087615 -2.1437986 -2.1362312 -2.0637689][-1.8034782 -1.8390868 -1.8768332 -1.9095008 -1.9370441 -1.9542904 -1.9609838 -1.9489675 -1.9176857 -1.958952 -2.0936873 -2.2179005 -2.2916529 -2.285358 -2.1865571][-1.7445879 -1.7685361 -1.793422 -1.8131576 -1.829118 -1.8435335 -1.8589847 -1.8627355 -1.8570802 -1.9151452 -2.0483484 -2.1734471 -2.2538407 -2.2498648 -2.1430039][-1.6092141 -1.6222191 -1.6348586 -1.6428137 -1.6468303 -1.6547976 -1.6709611 -1.684279 -1.6995158 -1.7608502 -1.8681784 -1.9680059 -2.0356619 -2.0327439 -1.9387279][-1.4583483 -1.4645476 -1.4702184 -1.4730988 -1.4722455 -1.4758434 -1.4886658 -1.5069642 -1.5398881 -1.6019089 -1.6808088 -1.7480857 -1.789053 -1.7768648 -1.6976047][-1.3488476 -1.3521724 -1.3546875 -1.3557203 -1.3534632 -1.35447 -1.3625505 -1.3800397 -1.4164858 -1.4694133 -1.5241725 -1.5644124 -1.5804181 -1.5585055 -1.4955833][-1.3276336 -1.3289607 -1.329915 -1.3306351 -1.3293231 -1.3288116 -1.3303621 -1.3388338 -1.3593967 -1.3881848 -1.4182253 -1.4411309 -1.4487121 -1.4336026 -1.3984084]]...]
INFO - root - 2017-12-07 23:39:02.780052: step 18810, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 4.616 sec/batch; 82h:27m:47s remains)
INFO - root - 2017-12-07 23:39:48.412875: step 18820, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.505 sec/batch; 80h:28m:13s remains)
INFO - root - 2017-12-07 23:40:34.139548: step 18830, loss = 0.80, batch loss = 0.72 (7.0 examples/sec; 4.543 sec/batch; 81h:08m:35s remains)
INFO - root - 2017-12-07 23:41:19.594612: step 18840, loss = 0.65, batch loss = 0.57 (6.9 examples/sec; 4.607 sec/batch; 82h:16m:23s remains)
INFO - root - 2017-12-07 23:42:05.187895: step 18850, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.526 sec/batch; 80h:48m:24s remains)
INFO - root - 2017-12-07 23:42:50.671264: step 18860, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 4.522 sec/batch; 80h:43m:37s remains)
INFO - root - 2017-12-07 23:43:36.979877: step 18870, loss = 0.81, batch loss = 0.74 (6.9 examples/sec; 4.634 sec/batch; 82h:42m:18s remains)
INFO - root - 2017-12-07 23:44:22.766172: step 18880, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.620 sec/batch; 82h:27m:22s remains)
INFO - root - 2017-12-07 23:45:08.458604: step 18890, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.627 sec/batch; 82h:33m:57s remains)
INFO - root - 2017-12-07 23:45:54.201700: step 18900, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.671 sec/batch; 83h:20m:25s remains)
2017-12-07 23:45:56.983815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4943495 -2.1348867 -2.0031803 -1.6569357 -1.454838 -1.2903669 -0.87646508 -0.77464581 -1.0560758 -1.4640265 -1.9673357 -2.3246131 -2.257232 -1.9795399 -1.796463][-2.2369444 -1.8795841 -1.618844 -1.2294211 -1.0448594 -0.91205955 -0.64740944 -0.6951983 -1.0980144 -1.6935856 -2.2491155 -2.5715551 -2.496664 -2.1174996 -1.794651][-1.9095964 -1.6345887 -1.3680897 -1.0354776 -0.85932708 -0.7708087 -0.73570228 -1.0004528 -1.5970559 -2.341223 -2.9073415 -3.2163901 -3.2061839 -2.7883129 -2.3149107][-1.7222118 -1.5212348 -1.3494349 -1.1634514 -1.0106914 -1.0061176 -1.1950581 -1.5777395 -2.1658249 -2.7853036 -3.153511 -3.3674626 -3.4929156 -3.242557 -2.8305938][-1.6304862 -1.4986477 -1.4767468 -1.4434154 -1.3220026 -1.3615842 -1.6400375 -1.9732487 -2.3516698 -2.6705632 -2.7248077 -2.7441936 -2.9813514 -3.0409939 -2.9160981][-1.4182677 -1.3948433 -1.5131261 -1.5246413 -1.3658109 -1.4065995 -1.6883142 -1.9524469 -2.1644969 -2.2748704 -2.0846424 -1.8553591 -2.0117249 -2.2292056 -2.3799744][-1.2929482 -1.4560676 -1.6400106 -1.5671213 -1.3114564 -1.2527781 -1.4253113 -1.5847721 -1.726881 -1.8471608 -1.6639917 -1.278079 -1.2019987 -1.292628 -1.5174408][-1.1890645 -1.5348394 -1.7448061 -1.5939269 -1.2654877 -1.053755 -1.0321219 -1.0544255 -1.1736228 -1.422941 -1.4189045 -1.0664377 -0.80453348 -0.71588326 -0.878206][-0.81294894 -1.1875403 -1.4083443 -1.2584743 -0.946347 -0.6847527 -0.55606866 -0.49678183 -0.58034873 -0.88069296 -0.98331046 -0.71843076 -0.45576096 -0.39309692 -0.5686264][-0.24742031 -0.42584276 -0.59570861 -0.50508189 -0.32011652 -0.17132473 -0.092441559 -0.018239498 0.014035702 -0.15656471 -0.21846676 -0.096774578 -0.031973839 -0.14105368 -0.41415834][0.16962194 0.28431416 0.17420816 0.19326687 0.23923683 0.17603922 0.093085289 0.16839886 0.35503674 0.3734355 0.36786604 0.30921555 0.15480947 -0.078763008 -0.39947367][0.37093067 0.69889927 0.6162529 0.55071688 0.45385933 0.15053463 -0.16532135 -0.17900133 0.061788082 0.18350792 0.1809268 0.022386551 -0.17339373 -0.38291025 -0.65692973][0.36190271 0.69998884 0.53687 0.34263849 0.16042233 -0.27192688 -0.77388358 -0.93167686 -0.78417349 -0.66056323 -0.60553575 -0.69129634 -0.7222724 -0.77758479 -0.9608798][-0.30195951 -0.050890446 -0.23011589 -0.37854671 -0.39998484 -0.68511295 -1.1687386 -1.388195 -1.352931 -1.2795846 -1.206804 -1.2243502 -1.1340938 -1.0856447 -1.2147417][-1.6102633 -1.4199424 -1.4368973 -1.2798843 -0.89474511 -0.78848433 -0.99381185 -1.0818818 -1.0919785 -1.0936785 -1.1171145 -1.2498181 -1.2812643 -1.304148 -1.4142776]]...]
INFO - root - 2017-12-07 23:46:42.860976: step 18910, loss = 0.68, batch loss = 0.61 (7.1 examples/sec; 4.484 sec/batch; 79h:59m:19s remains)
INFO - root - 2017-12-07 23:47:28.480041: step 18920, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.636 sec/batch; 82h:40m:51s remains)
INFO - root - 2017-12-07 23:48:14.316288: step 18930, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.633 sec/batch; 82h:36m:23s remains)
INFO - root - 2017-12-07 23:48:59.932458: step 18940, loss = 0.89, batch loss = 0.81 (6.9 examples/sec; 4.609 sec/batch; 82h:09m:57s remains)
INFO - root - 2017-12-07 23:49:45.808812: step 18950, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.544 sec/batch; 81h:00m:02s remains)
INFO - root - 2017-12-07 23:50:31.346245: step 18960, loss = 0.69, batch loss = 0.62 (6.9 examples/sec; 4.611 sec/batch; 82h:11m:12s remains)
INFO - root - 2017-12-07 23:51:17.386968: step 18970, loss = 0.93, batch loss = 0.86 (7.0 examples/sec; 4.596 sec/batch; 81h:53m:46s remains)
INFO - root - 2017-12-07 23:52:02.935055: step 18980, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.580 sec/batch; 81h:36m:50s remains)
INFO - root - 2017-12-07 23:52:48.679226: step 18990, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.541 sec/batch; 80h:53m:38s remains)
INFO - root - 2017-12-07 23:53:34.063319: step 19000, loss = 0.75, batch loss = 0.67 (7.1 examples/sec; 4.511 sec/batch; 80h:20m:53s remains)
2017-12-07 23:53:36.678390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1916809 -3.0879474 -2.6729879 -2.278681 -1.9911551 -1.914005 -1.8905158 -1.9515221 -2.1085835 -2.4579611 -2.78895 -2.7527738 -2.5060623 -2.3797796 -2.5566964][-3.2028303 -3.1091936 -2.7336063 -2.3392649 -1.8218088 -1.4325533 -1.2477999 -1.3165894 -1.6339965 -2.2626452 -2.8509903 -2.9976635 -2.7884746 -2.5085313 -2.4813571][-3.4384978 -3.4834335 -3.1799121 -2.7728579 -2.0855949 -1.5595863 -1.4306505 -1.6526458 -2.1202643 -2.8537745 -3.4872239 -3.6630735 -3.3856153 -2.8213367 -2.3880742][-3.8192921 -3.9737213 -3.6792917 -3.2805827 -2.6705832 -2.3066962 -2.4027889 -2.7948732 -3.3069673 -3.8531275 -4.140554 -4.0036893 -3.4871545 -2.7513638 -2.211148][-4.045557 -4.1634779 -3.8150492 -3.5014341 -3.1218781 -2.92709 -3.023458 -3.3448684 -3.78622 -4.1113396 -4.0573764 -3.6510329 -3.0853109 -2.6035776 -2.4561551][-4.0480633 -4.1332564 -3.754276 -3.4189272 -2.9746199 -2.5199609 -2.2069297 -2.1992919 -2.5828345 -2.9909492 -3.1422007 -3.075047 -2.9189346 -2.8936605 -3.0822721][-3.8291516 -3.9141984 -3.5027566 -2.9897943 -2.188308 -1.2415657 -0.41223335 -0.11610556 -0.63753057 -1.534405 -2.3803034 -2.9749074 -3.2029371 -3.2690105 -3.316854][-3.4684439 -3.5149505 -3.0431309 -2.3506556 -1.2895174 -0.15843964 0.717289 0.84430075 -0.0686388 -1.3678741 -2.4677978 -3.0894299 -3.1492095 -3.0245757 -2.9890351][-3.111064 -3.1432042 -2.7447939 -2.1544142 -1.3608792 -0.72904062 -0.44225311 -0.64597917 -1.4885705 -2.4455724 -3.0208619 -3.0829756 -2.7552564 -2.554481 -2.7400689][-3.1027155 -3.2386706 -3.1306691 -2.9683051 -2.7752752 -2.7341628 -2.7967434 -2.8816175 -3.1891246 -3.4728446 -3.4364486 -3.0702686 -2.5746384 -2.43378 -2.7696381][-3.6549139 -3.9786463 -4.13801 -4.2420363 -4.3012753 -4.3610392 -4.3548827 -4.2106242 -4.1715446 -4.1370478 -3.8675232 -3.3886828 -2.8836737 -2.7235756 -2.9617691][-4.2748251 -4.741931 -4.9874716 -5.0543866 -5.0093207 -4.9014187 -4.739316 -4.5376678 -4.5023875 -4.5153551 -4.2970457 -3.8743756 -3.3744507 -3.1061034 -3.1311736][-4.1781425 -4.6077356 -4.7733965 -4.720243 -4.5737586 -4.41473 -4.2593837 -4.118588 -4.1422367 -4.2388921 -4.1498947 -3.855217 -3.4234595 -3.1135097 -3.0169616][-3.4579892 -3.7622123 -3.8572502 -3.7917845 -3.6901553 -3.6117659 -3.5539131 -3.4821887 -3.5187774 -3.6260481 -3.6309626 -3.4932961 -3.2222981 -3.0009389 -2.9107432][-2.6444354 -2.8305838 -2.8945444 -2.8662643 -2.8686442 -2.9320993 -3.0262051 -3.0427625 -3.0488625 -3.0769835 -3.0583034 -2.981144 -2.8428588 -2.7668915 -2.7791867]]...]
INFO - root - 2017-12-07 23:54:22.528898: step 19010, loss = 0.72, batch loss = 0.64 (6.9 examples/sec; 4.665 sec/batch; 83h:04m:35s remains)
INFO - root - 2017-12-07 23:55:08.323635: step 19020, loss = 0.81, batch loss = 0.74 (6.9 examples/sec; 4.613 sec/batch; 82h:08m:25s remains)
INFO - root - 2017-12-07 23:55:54.180328: step 19030, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.507 sec/batch; 80h:14m:58s remains)
INFO - root - 2017-12-07 23:56:39.965624: step 19040, loss = 0.74, batch loss = 0.66 (7.0 examples/sec; 4.578 sec/batch; 81h:29m:10s remains)
INFO - root - 2017-12-07 23:57:25.710736: step 19050, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.592 sec/batch; 81h:44m:04s remains)
INFO - root - 2017-12-07 23:58:11.535768: step 19060, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 4.639 sec/batch; 82h:32m:50s remains)
INFO - root - 2017-12-07 23:58:57.090430: step 19070, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 4.561 sec/batch; 81h:08m:56s remains)
INFO - root - 2017-12-07 23:59:42.988689: step 19080, loss = 0.85, batch loss = 0.78 (7.1 examples/sec; 4.491 sec/batch; 79h:54m:10s remains)
INFO - root - 2017-12-08 00:00:28.649152: step 19090, loss = 0.73, batch loss = 0.65 (7.1 examples/sec; 4.522 sec/batch; 80h:26m:06s remains)
INFO - root - 2017-12-08 00:01:14.456342: step 19100, loss = 0.86, batch loss = 0.78 (7.0 examples/sec; 4.578 sec/batch; 81h:25m:15s remains)
2017-12-08 00:01:17.123577: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5384979 -2.5439758 -2.5471804 -2.5510118 -2.5479364 -2.5387211 -2.54248 -2.5709558 -2.6036673 -2.6227837 -2.6206117 -2.6108222 -2.6041834 -2.5855799 -2.5513868][-2.6312566 -2.6389914 -2.6300926 -2.6125102 -2.5986409 -2.588 -2.5911632 -2.6170259 -2.6413932 -2.6328874 -2.5963013 -2.5693772 -2.569633 -2.5769682 -2.5718071][-2.6499577 -2.6950827 -2.67268 -2.5996337 -2.5277383 -2.4870129 -2.4814906 -2.5030186 -2.5022447 -2.4469647 -2.3662298 -2.3186309 -2.3499851 -2.4399111 -2.537879][-2.4948285 -2.6189044 -2.5900578 -2.4386914 -2.2594712 -2.1503122 -2.1257732 -2.13645 -2.0886939 -1.9771669 -1.8766174 -1.8613214 -1.9906976 -2.2240136 -2.4454622][-2.2083836 -2.3879318 -2.3488235 -2.1452274 -1.8674307 -1.6700232 -1.619261 -1.6189039 -1.5065112 -1.3251383 -1.2412496 -1.3515534 -1.654861 -2.0249522 -2.3095868][-1.9628654 -2.1446631 -2.0870452 -1.8489273 -1.4779837 -1.1789241 -1.1038673 -1.1314831 -1.0110369 -0.84136891 -0.85502076 -1.1031325 -1.4768896 -1.808259 -2.0247478][-1.9036276 -2.0269561 -1.9592259 -1.7004592 -1.2632306 -0.89574862 -0.81508565 -0.89411688 -0.87532949 -0.86009407 -1.0094142 -1.2903929 -1.5261464 -1.6231856 -1.6451619][-1.918056 -1.9502087 -1.9156044 -1.6858141 -1.2949238 -1.0338457 -1.098958 -1.2948413 -1.3619246 -1.423219 -1.584657 -1.7562859 -1.7651787 -1.6050494 -1.4055984][-1.7767775 -1.7016578 -1.7075109 -1.555223 -1.3170705 -1.3366199 -1.7010183 -2.0382893 -2.09231 -2.0609891 -2.085026 -2.0899124 -1.9505975 -1.6873724 -1.4020517][-1.4522114 -1.2613101 -1.2817023 -1.2158864 -1.1318419 -1.3478742 -1.8634591 -2.2623806 -2.3348775 -2.2921884 -2.2654827 -2.181654 -1.9870934 -1.7331922 -1.4879785][-1.2618506 -1.0025241 -1.0247331 -1.0228186 -1.0239539 -1.2614217 -1.7050583 -2.0399284 -2.1360266 -2.1669235 -2.203907 -2.1421208 -1.9613314 -1.739311 -1.5303702][-1.4961252 -1.2793465 -1.272629 -1.2440298 -1.2414382 -1.4069717 -1.7056706 -1.9144032 -1.9829705 -2.0400138 -2.0985515 -2.0580723 -1.9071138 -1.7301133 -1.5655026][-2.0292547 -1.9313264 -1.8934619 -1.7934363 -1.7359138 -1.8113801 -1.9688935 -2.0524724 -2.0583344 -2.0752072 -2.0933313 -2.0409176 -1.9257157 -1.8267417 -1.7529702][-2.440258 -2.4599273 -2.4592986 -2.3958683 -2.3552885 -2.3700018 -2.4094021 -2.3858001 -2.3277948 -2.2922952 -2.264745 -2.2189686 -2.1777198 -2.1886098 -2.1991174][-2.4231827 -2.50086 -2.5382779 -2.5493388 -2.5539756 -2.5482528 -2.5261345 -2.4554095 -2.391885 -2.3653967 -2.3471494 -2.3353693 -2.368413 -2.461374 -2.518075]]...]
INFO - root - 2017-12-08 00:02:02.299438: step 19110, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.571 sec/batch; 81h:16m:32s remains)
INFO - root - 2017-12-08 00:02:47.941087: step 19120, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.586 sec/batch; 81h:32m:28s remains)
INFO - root - 2017-12-08 00:03:33.224418: step 19130, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.544 sec/batch; 80h:46m:11s remains)
INFO - root - 2017-12-08 00:04:18.913730: step 19140, loss = 0.85, batch loss = 0.78 (7.0 examples/sec; 4.549 sec/batch; 80h:50m:56s remains)
INFO - root - 2017-12-08 00:05:04.320826: step 19150, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.594 sec/batch; 81h:38m:22s remains)
INFO - root - 2017-12-08 00:05:49.976502: step 19160, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.574 sec/batch; 81h:16m:44s remains)
INFO - root - 2017-12-08 00:06:35.593795: step 19170, loss = 0.86, batch loss = 0.79 (7.1 examples/sec; 4.535 sec/batch; 80h:33m:37s remains)
INFO - root - 2017-12-08 00:07:21.445703: step 19180, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.595 sec/batch; 81h:37m:04s remains)
INFO - root - 2017-12-08 00:08:06.961887: step 19190, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.586 sec/batch; 81h:26m:38s remains)
INFO - root - 2017-12-08 00:08:53.037440: step 19200, loss = 0.74, batch loss = 0.67 (6.8 examples/sec; 4.682 sec/batch; 83h:08m:42s remains)
2017-12-08 00:08:55.699095: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1068621 -2.1232326 -2.3409529 -2.5019784 -2.4437094 -2.3674686 -2.2038813 -2.1453674 -2.2167046 -2.1973436 -2.08936 -1.994159 -1.9405675 -1.8561594 -1.7622657][-1.6148543 -1.6614442 -1.928906 -2.1623611 -2.1572173 -2.1486795 -1.9687624 -1.8285506 -1.8528845 -1.8358328 -1.8142996 -1.823724 -1.8009119 -1.6170814 -1.3761089][-1.739094 -1.7749972 -2.0537477 -2.3578451 -2.4537411 -2.5429058 -2.427587 -2.2666559 -2.2130046 -2.0792344 -1.9769533 -1.9384832 -1.8820429 -1.619051 -1.2639291][-1.833704 -1.920011 -2.2829306 -2.717566 -2.8476741 -2.8273993 -2.6235971 -2.3882558 -2.2635407 -2.0598743 -1.8747041 -1.7552376 -1.6809163 -1.4777374 -1.2249997][-1.6042902 -1.8202474 -2.3153088 -2.9105296 -2.9906578 -2.6771779 -2.2092218 -1.7972987 -1.6446857 -1.5732582 -1.4723046 -1.4046049 -1.3934646 -1.3595595 -1.3445752][-1.1315069 -1.3672872 -1.8952429 -2.5186276 -2.4803996 -1.825573 -1.0395019 -0.37023354 -0.22705269 -0.43585134 -0.59576082 -0.73885345 -0.85495472 -0.99030232 -1.2079909][-0.76051331 -0.84400415 -1.2060084 -1.6420217 -1.4950271 -0.65349722 0.34702921 1.2902851 1.3583078 0.83599091 0.30294609 -0.15001678 -0.38673496 -0.59670043 -0.918905][-1.0508766 -0.9370079 -1.0695856 -1.3124704 -1.1994364 -0.49036407 0.40623808 1.3766494 1.4815855 0.99389219 0.32906532 -0.31635714 -0.57067108 -0.66409278 -0.84612894][-1.8033381 -1.5479147 -1.581902 -1.8132412 -1.9547765 -1.6423159 -1.1064005 -0.37805033 -0.11948156 -0.28588772 -0.78174281 -1.3650081 -1.4840755 -1.3635573 -1.2835922][-2.6350026 -2.4146802 -2.4852638 -2.748137 -3.053252 -3.0019696 -2.7106857 -2.1986589 -1.9079328 -1.8849697 -2.2019434 -2.5783372 -2.4957952 -2.2126415 -1.9672544][-3.28011 -3.1934242 -3.3340318 -3.5573256 -3.7970717 -3.7458184 -3.4658878 -3.0396562 -2.8451076 -2.8554544 -3.0938544 -3.2577229 -3.0321946 -2.7081437 -2.4741621][-3.5533452 -3.6224198 -3.8380566 -3.9920721 -4.042841 -3.8591819 -3.4989157 -3.1286244 -3.0700624 -3.183248 -3.3975587 -3.4729733 -3.2409902 -2.9885545 -2.8546112][-3.478899 -3.6035666 -3.8530173 -3.9585583 -3.898232 -3.6442049 -3.2794261 -3.0222702 -3.104773 -3.3381221 -3.5846438 -3.6804755 -3.5187726 -3.3470464 -3.2785592][-3.3751092 -3.3438878 -3.4647353 -3.5020688 -3.4380364 -3.2545307 -3.0455136 -2.9965916 -3.2131422 -3.5133257 -3.758734 -3.8396513 -3.7324736 -3.6180451 -3.6014178][-3.555146 -3.4396009 -3.4622366 -3.4908504 -3.5135229 -3.4725831 -3.4318616 -3.4953241 -3.6944447 -3.8815446 -3.9540703 -3.8907456 -3.7433376 -3.6319318 -3.6189244]]...]
INFO - root - 2017-12-08 00:09:41.293208: step 19210, loss = 0.74, batch loss = 0.66 (7.0 examples/sec; 4.586 sec/batch; 81h:24m:48s remains)
INFO - root - 2017-12-08 00:10:27.282667: step 19220, loss = 0.82, batch loss = 0.74 (7.0 examples/sec; 4.596 sec/batch; 81h:35m:04s remains)
INFO - root - 2017-12-08 00:11:12.838119: step 19230, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 4.660 sec/batch; 82h:42m:30s remains)
INFO - root - 2017-12-08 00:11:58.849557: step 19240, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 4.508 sec/batch; 80h:00m:23s remains)
INFO - root - 2017-12-08 00:12:44.108641: step 19250, loss = 0.74, batch loss = 0.66 (7.1 examples/sec; 4.486 sec/batch; 79h:36m:00s remains)
INFO - root - 2017-12-08 00:13:29.691835: step 19260, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.586 sec/batch; 81h:21m:44s remains)
INFO - root - 2017-12-08 00:14:15.429248: step 19270, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.595 sec/batch; 81h:29m:41s remains)
INFO - root - 2017-12-08 00:15:00.795171: step 19280, loss = 0.75, batch loss = 0.68 (7.3 examples/sec; 4.382 sec/batch; 77h:43m:15s remains)
INFO - root - 2017-12-08 00:15:46.295880: step 19290, loss = 0.74, batch loss = 0.67 (7.2 examples/sec; 4.457 sec/batch; 79h:02m:16s remains)
INFO - root - 2017-12-08 00:16:31.902613: step 19300, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 4.497 sec/batch; 79h:44m:10s remains)
2017-12-08 00:16:34.643505: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6726944 -1.6612029 -1.6236577 -1.6899672 -1.8947487 -1.9689033 -2.012702 -2.1884506 -2.293745 -2.2420485 -2.2117424 -2.1976328 -2.1289136 -1.9536021 -1.7840126][-1.517082 -1.5552611 -1.4899373 -1.5656579 -1.8710732 -1.9778612 -2.0321462 -2.2695291 -2.4044414 -2.3251095 -2.2587888 -2.195605 -2.077436 -1.9056911 -1.8042932][-1.329025 -1.4183333 -1.3011205 -1.3569973 -1.6631496 -1.7106206 -1.7577698 -2.0490539 -2.194911 -2.0833685 -1.9906909 -1.9046209 -1.8103192 -1.757782 -1.835988][-1.1018343 -1.153182 -0.92657542 -0.94265914 -1.1378281 -1.0141022 -1.0014226 -1.27245 -1.397501 -1.3129256 -1.2990804 -1.2902701 -1.3231997 -1.43995 -1.6655114][-0.83557844 -0.75769544 -0.40690756 -0.41535282 -0.49912047 -0.22943497 -0.17410088 -0.3526001 -0.41904402 -0.42048407 -0.53975606 -0.64487958 -0.82971025 -1.0261321 -1.2757258][-0.58105731 -0.36015463 0.077003 0.049939156 0.0085711479 0.25373316 0.22847891 0.13717794 0.14510441 0.078717709 -0.068992138 -0.21289873 -0.49955344 -0.72471476 -1.0172758][-0.40444231 -0.058665276 0.45007896 0.42828321 0.36251545 0.40337038 0.1472497 0.07055378 0.12035227 0.052530766 0.0082468987 -0.065928459 -0.40159225 -0.6866715 -1.1640453][-0.28721905 0.14935589 0.70800877 0.72432327 0.60045004 0.3962636 -0.098477364 -0.21802855 -0.1791358 -0.23376322 -0.16787815 -0.17096376 -0.50487351 -0.86907935 -1.572634][-0.19129276 0.31771469 0.89571381 0.921154 0.69785357 0.32367468 -0.23633432 -0.3669734 -0.35349989 -0.45025563 -0.39854002 -0.37414503 -0.68047285 -1.104476 -1.9234943][-0.097197056 0.45362425 0.99157619 0.93583441 0.56888437 0.14998627 -0.27851534 -0.34213018 -0.36306238 -0.58728051 -0.64868355 -0.620733 -0.89104414 -1.3514302 -2.1728945][-0.03305769 0.5060358 0.95076323 0.74923515 0.23635483 -0.12422848 -0.34524298 -0.32817125 -0.38365364 -0.68397522 -0.79757452 -0.76109385 -1.0069633 -1.4742563 -2.2190051][-0.031184196 0.4707284 0.84856319 0.55629539 -0.046741962 -0.3498435 -0.44953012 -0.45414472 -0.57308173 -0.88834238 -1.0155499 -1.0017595 -1.2461586 -1.670572 -2.2667747][-0.16956425 0.30844069 0.66673422 0.36111116 -0.24958897 -0.52074003 -0.6149261 -0.719141 -0.87991238 -1.152302 -1.293731 -1.3416364 -1.5646648 -1.8735576 -2.32803][-0.48840404 0.02084446 0.38331413 0.074444294 -0.51175308 -0.80045891 -0.96199489 -1.1583536 -1.2911882 -1.4748104 -1.6231561 -1.7303472 -1.9326265 -2.1061513 -2.3892632][-0.88771367 -0.30271721 0.031680584 -0.32772779 -0.90987229 -1.217536 -1.4041836 -1.6090202 -1.6355164 -1.723798 -1.8343205 -1.9102969 -2.0646276 -2.1099718 -2.2373974]]...]
INFO - root - 2017-12-08 00:17:20.045073: step 19310, loss = 0.85, batch loss = 0.78 (7.2 examples/sec; 4.465 sec/batch; 79h:09m:11s remains)
INFO - root - 2017-12-08 00:18:05.193763: step 19320, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.567 sec/batch; 80h:56m:58s remains)
INFO - root - 2017-12-08 00:18:50.899101: step 19330, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.587 sec/batch; 81h:17m:30s remains)
INFO - root - 2017-12-08 00:19:36.264690: step 19340, loss = 0.66, batch loss = 0.59 (7.0 examples/sec; 4.554 sec/batch; 80h:40m:52s remains)
INFO - root - 2017-12-08 00:20:21.799346: step 19350, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.587 sec/batch; 81h:16m:06s remains)
INFO - root - 2017-12-08 00:21:06.990722: step 19360, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 4.543 sec/batch; 80h:27m:46s remains)
INFO - root - 2017-12-08 00:21:52.671965: step 19370, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.576 sec/batch; 81h:02m:43s remains)
INFO - root - 2017-12-08 00:22:38.246471: step 19380, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 4.526 sec/batch; 80h:08m:48s remains)
INFO - root - 2017-12-08 00:23:23.854275: step 19390, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.585 sec/batch; 81h:10m:44s remains)
INFO - root - 2017-12-08 00:24:09.255815: step 19400, loss = 0.86, batch loss = 0.79 (7.0 examples/sec; 4.541 sec/batch; 80h:22m:49s remains)
2017-12-08 00:24:12.069797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6810982 -3.8088863 -3.7890925 -3.7217507 -3.7049122 -3.7303729 -3.7948771 -3.8372972 -3.8271763 -3.770159 -3.7501915 -3.7053277 -3.5782404 -3.504905 -3.5804591][-3.6765962 -3.7506213 -3.6414342 -3.522948 -3.4951382 -3.5020876 -3.5732257 -3.6436243 -3.6253719 -3.5278318 -3.5154276 -3.483768 -3.2861676 -3.133816 -3.2633224][-3.5677178 -3.618485 -3.4389157 -3.2694521 -3.2194643 -3.1975935 -3.2505441 -3.3117294 -3.2412791 -3.073869 -3.0575585 -3.0452077 -2.789916 -2.5659142 -2.80244][-3.3694496 -3.380609 -3.1163425 -2.8880258 -2.8206608 -2.7694044 -2.7830739 -2.8157167 -2.7070577 -2.5108714 -2.5317321 -2.6053896 -2.3792048 -2.1615877 -2.5107398][-3.0360918 -2.960324 -2.6319485 -2.4105034 -2.3603694 -2.2847629 -2.2287066 -2.1734328 -2.0159128 -1.8296242 -1.9363172 -2.16682 -2.0677438 -1.9381986 -2.3880939][-2.6403227 -2.5386596 -2.2748299 -2.1558666 -2.115016 -1.9408681 -1.7473807 -1.5564971 -1.3248155 -1.1791308 -1.4182932 -1.8370895 -1.8999426 -1.896595 -2.4234273][-2.3618894 -2.3501644 -2.2297792 -2.1896923 -2.0825684 -1.7555804 -1.4174216 -1.1190886 -0.85179925 -0.79055786 -1.1669617 -1.7380745 -1.915133 -2.0120955 -2.5928712][-2.2602141 -2.3143251 -2.2714841 -2.2345293 -2.0462091 -1.6487663 -1.2409954 -0.861573 -0.61995959 -0.67968178 -1.157907 -1.7618465 -1.9575899 -2.0913665 -2.6733017][-2.2438743 -2.3369086 -2.3750596 -2.3553083 -2.1472585 -1.7656736 -1.3125331 -0.85406518 -0.67683077 -0.8491509 -1.3434539 -1.8474767 -1.9938869 -2.1128342 -2.6297741][-2.1640882 -2.2643356 -2.3651061 -2.3781104 -2.2108443 -1.8947601 -1.4283421 -0.97010732 -0.88068438 -1.1288655 -1.6005621 -2.0139174 -2.1520333 -2.2778509 -2.6924288][-2.011759 -2.0388956 -2.08699 -2.0393436 -1.8819466 -1.631346 -1.2431471 -0.93117285 -0.96585417 -1.2312312 -1.679213 -2.080343 -2.2942147 -2.4819205 -2.8032956][-2.1843665 -2.1363704 -2.0723553 -1.914089 -1.7149234 -1.4789231 -1.1998043 -1.0785422 -1.208112 -1.4726951 -1.8589849 -2.1935458 -2.387742 -2.5563052 -2.7943902][-2.650557 -2.6176052 -2.5167346 -2.3104827 -2.089426 -1.8706403 -1.6907237 -1.6851449 -1.826519 -2.0373635 -2.3016765 -2.4881802 -2.5720816 -2.6636705 -2.8367348][-3.176935 -3.1932225 -3.1208668 -2.9598966 -2.7936215 -2.6436863 -2.5449352 -2.5612645 -2.6438582 -2.7603621 -2.8877206 -2.9466457 -2.9425483 -2.9566751 -3.041594][-3.4946542 -3.5414572 -3.5208702 -3.4470878 -3.3708792 -3.3050733 -3.2680657 -3.2821829 -3.3246675 -3.3867726 -3.4477553 -3.4638879 -3.4354103 -3.3982787 -3.3886957]]...]
INFO - root - 2017-12-08 00:24:57.780840: step 19410, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 4.511 sec/batch; 79h:50m:26s remains)
INFO - root - 2017-12-08 00:25:43.293278: step 19420, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.480 sec/batch; 79h:16m:56s remains)
INFO - root - 2017-12-08 00:26:28.763119: step 19430, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.565 sec/batch; 80h:46m:18s remains)
INFO - root - 2017-12-08 00:27:14.651618: step 19440, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 4.528 sec/batch; 80h:05m:55s remains)
INFO - root - 2017-12-08 00:28:00.206145: step 19450, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.548 sec/batch; 80h:26m:20s remains)
INFO - root - 2017-12-08 00:28:45.730751: step 19460, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.546 sec/batch; 80h:23m:10s remains)
INFO - root - 2017-12-08 00:29:31.463607: step 19470, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 4.624 sec/batch; 81h:45m:16s remains)
INFO - root - 2017-12-08 00:30:16.996980: step 19480, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.601 sec/batch; 81h:20m:27s remains)
INFO - root - 2017-12-08 00:31:02.589664: step 19490, loss = 0.74, batch loss = 0.66 (7.0 examples/sec; 4.602 sec/batch; 81h:20m:43s remains)
INFO - root - 2017-12-08 00:31:48.068368: step 19500, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 4.618 sec/batch; 81h:37m:30s remains)
2017-12-08 00:31:50.744039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1567943 -3.185986 -3.2495351 -3.2109818 -2.9722049 -2.6650429 -2.5068302 -2.4269772 -2.3705897 -2.4366245 -2.5088842 -2.4583082 -2.2223685 -1.9974465 -2.0077138][-3.2984571 -3.1912193 -3.1901879 -3.2053506 -3.1149521 -2.9135525 -2.704705 -2.5114331 -2.3914518 -2.4941897 -2.643513 -2.6288676 -2.4174018 -2.2556434 -2.4155171][-3.0844107 -2.8925745 -2.9074676 -3.0206442 -3.0967746 -3.0213161 -2.7874117 -2.5151262 -2.3235505 -2.3409181 -2.38696 -2.3058498 -2.1119421 -2.0253348 -2.3383279][-2.8171883 -2.4859743 -2.5188136 -2.696352 -2.8542755 -2.879765 -2.6865659 -2.41512 -2.2373123 -2.1908445 -2.1564987 -2.0085597 -1.7570739 -1.6290202 -1.9456882][-2.6536109 -1.959166 -1.6980259 -1.6025991 -1.5804062 -1.6462729 -1.6444294 -1.6196933 -1.6695561 -1.7774153 -1.8941031 -1.900928 -1.7354171 -1.6068382 -1.8593688][-2.651248 -1.5874286 -0.86665297 -0.2835021 0.097776413 0.15807247 0.13698149 0.012351513 -0.31054592 -0.78260922 -1.3016632 -1.6648996 -1.7511249 -1.729491 -1.8819869][-2.5481436 -1.4399655 -0.59593391 0.2382226 0.93542004 1.403955 1.9101062 2.2365751 1.9830484 1.1325212 -0.0047397614 -1.0042098 -1.6087546 -1.9001186 -1.9719174][-2.5380533 -1.8740463 -1.4391418 -0.85693192 -0.10566902 0.77102852 1.9987783 3.07628 3.2133622 2.3507247 0.94418335 -0.46898723 -1.5076458 -2.1316473 -2.2579722][-2.8422308 -2.7621419 -2.7967305 -2.5655839 -1.9971268 -1.1706231 0.097505569 1.2332692 1.5850797 1.2269349 0.38104868 -0.62639713 -1.5742073 -2.3380952 -2.5895638][-2.7362726 -2.9692307 -3.3233821 -3.4429259 -3.2564068 -2.7941251 -1.8799603 -0.9856472 -0.47863698 -0.22140694 -0.32284975 -0.769011 -1.5105193 -2.3580608 -2.7979207][-2.4011087 -2.737787 -3.2605989 -3.6338451 -3.7797205 -3.5818186 -2.8980813 -2.1891468 -1.6745481 -1.242049 -1.0799167 -1.2258313 -1.704632 -2.4378614 -3.010479][-2.4628353 -2.7049162 -3.134655 -3.4883184 -3.7309117 -3.6737335 -3.1900854 -2.6810362 -2.3275301 -2.0532937 -1.9299486 -1.8945763 -2.0284173 -2.521832 -3.12858][-2.3717833 -2.3934135 -2.6206889 -2.9280481 -3.3365583 -3.524426 -3.2728353 -2.8775868 -2.6024945 -2.4705014 -2.398273 -2.224613 -2.0634592 -2.3020804 -2.7534976][-2.0505338 -1.9587467 -1.9362721 -1.9979489 -2.2435088 -2.4261997 -2.3574653 -2.2038417 -2.1928897 -2.3384242 -2.4609828 -2.3412414 -2.0416965 -1.9775522 -2.0378213][-2.2003117 -2.0354908 -1.7709582 -1.4854412 -1.3221836 -1.2386076 -1.22434 -1.3341832 -1.6300111 -2.0460246 -2.3916681 -2.3977611 -2.0593064 -1.7468219 -1.4446168]]...]
INFO - root - 2017-12-08 00:32:36.278676: step 19510, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 4.557 sec/batch; 80h:31m:27s remains)
INFO - root - 2017-12-08 00:33:22.078643: step 19520, loss = 0.67, batch loss = 0.59 (7.0 examples/sec; 4.549 sec/batch; 80h:22m:05s remains)
INFO - root - 2017-12-08 00:34:07.705252: step 19530, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.612 sec/batch; 81h:28m:05s remains)
INFO - root - 2017-12-08 00:34:53.196515: step 19540, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.563 sec/batch; 80h:35m:44s remains)
INFO - root - 2017-12-08 00:35:38.668734: step 19550, loss = 0.80, batch loss = 0.72 (7.0 examples/sec; 4.561 sec/batch; 80h:33m:14s remains)
INFO - root - 2017-12-08 00:36:24.495369: step 19560, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.588 sec/batch; 81h:00m:38s remains)
INFO - root - 2017-12-08 00:37:09.979826: step 19570, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 4.531 sec/batch; 79h:59m:56s remains)
INFO - root - 2017-12-08 00:37:55.699681: step 19580, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.609 sec/batch; 81h:21m:10s remains)
INFO - root - 2017-12-08 00:38:41.330241: step 19590, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.550 sec/batch; 80h:18m:27s remains)
INFO - root - 2017-12-08 00:39:26.772734: step 19600, loss = 0.73, batch loss = 0.66 (7.3 examples/sec; 4.374 sec/batch; 77h:10m:40s remains)
2017-12-08 00:39:29.361703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.034421921 0.3494482 0.34635353 0.10440683 0.014064789 0.28464031 0.52430344 0.50647545 0.28407192 -0.04228735 -0.38273096 -0.49301767 -0.32109833 -0.17338467 -0.17528677][-0.26785374 -0.013262749 -0.08407402 -0.34144878 -0.45620823 -0.23561954 -0.042963505 0.0011472702 -0.048452854 -0.1978302 -0.45116091 -0.48751378 -0.20979834 0.077361107 0.17874002][-0.38535023 -0.26697588 -0.37675667 -0.64089942 -0.81465292 -0.71119118 -0.58408308 -0.43793106 -0.3487606 -0.44604325 -0.738106 -0.76146889 -0.44833016 -0.12017775 0.031197071][-0.52425408 -0.4366169 -0.54957342 -0.8034246 -0.93825006 -0.83291221 -0.64941549 -0.38084984 -0.31373072 -0.64512277 -1.1994278 -1.3454549 -1.092921 -0.78834128 -0.61388254][-0.63045931 -0.65034556 -0.8172698 -1.0095944 -0.95997381 -0.66469526 -0.29670811 0.024233341 -0.059527397 -0.67770958 -1.4024649 -1.6165774 -1.4485774 -1.2045338 -1.0173228][-0.4591682 -0.6044569 -0.850719 -1.0159333 -0.88733077 -0.50169706 -0.048148632 0.15331936 -0.11998177 -0.81003642 -1.3525455 -1.4026303 -1.2581637 -1.0946727 -0.89877534][0.2489295 0.14867973 -0.046302795 -0.11272764 0.066274166 0.49710226 0.91710043 0.89937878 0.48291683 -0.088135719 -0.35734892 -0.33180618 -0.33321142 -0.38190603 -0.24178267][0.86742973 0.94304419 1.0105882 1.2782569 1.7203937 2.2917953 2.6357245 2.3514152 1.7708855 1.2392998 1.0002317 0.87817526 0.60531712 0.27801847 0.27827215][0.51800776 0.60239124 0.8284874 1.3237796 1.9298587 2.4557233 2.5781679 2.0883207 1.4558482 1.0039139 0.80508471 0.62146616 0.23164034 -0.23045254 -0.332088][-0.51885128 -0.66541767 -0.56933856 -0.18368435 0.25049162 0.4832077 0.42070675 0.0697937 -0.33965254 -0.52396059 -0.49085116 -0.53133535 -0.83603525 -1.2223182 -1.2967119][-1.5902395 -1.9502666 -1.9237213 -1.5782962 -1.248034 -1.1509686 -1.2027714 -1.3496795 -1.5902863 -1.5856271 -1.3445852 -1.1971509 -1.3238778 -1.5177996 -1.4771178][-1.8823018 -2.2793479 -2.2019033 -1.8509552 -1.6100996 -1.6122029 -1.6648085 -1.7586386 -1.9490538 -1.8451965 -1.4801033 -1.1978011 -1.2165556 -1.3491659 -1.341748][-1.2273362 -1.5724814 -1.4656205 -1.2838385 -1.375273 -1.60901 -1.7309546 -1.8446467 -1.9533904 -1.7012486 -1.2239354 -0.94070983 -1.0878317 -1.4066658 -1.6288757][-0.621737 -0.86803579 -0.78097653 -0.87518573 -1.3274059 -1.6834304 -1.796118 -1.8813598 -1.8619783 -1.50195 -1.0254836 -0.91909528 -1.2743289 -1.720361 -2.0273619][-0.5122509 -0.60332179 -0.46123266 -0.61081243 -1.1211071 -1.4314086 -1.514987 -1.6084871 -1.5744379 -1.2995906 -0.96981359 -1.0803945 -1.470691 -1.7725425 -1.9966021]]...]
INFO - root - 2017-12-08 00:40:14.907723: step 19610, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.565 sec/batch; 80h:32m:11s remains)
INFO - root - 2017-12-08 00:41:00.452255: step 19620, loss = 0.68, batch loss = 0.61 (7.2 examples/sec; 4.472 sec/batch; 78h:53m:02s remains)
INFO - root - 2017-12-08 00:41:46.238098: step 19630, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.548 sec/batch; 80h:12m:40s remains)
INFO - root - 2017-12-08 00:42:31.687929: step 19640, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 4.562 sec/batch; 80h:26m:59s remains)
INFO - root - 2017-12-08 00:43:17.436100: step 19650, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.582 sec/batch; 80h:47m:19s remains)
INFO - root - 2017-12-08 00:44:03.061566: step 19660, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.621 sec/batch; 81h:28m:16s remains)
INFO - root - 2017-12-08 00:44:48.635730: step 19670, loss = 0.97, batch loss = 0.90 (7.1 examples/sec; 4.533 sec/batch; 79h:54m:30s remains)
INFO - root - 2017-12-08 00:45:34.353176: step 19680, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.551 sec/batch; 80h:12m:03s remains)
INFO - root - 2017-12-08 00:46:20.128613: step 19690, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.601 sec/batch; 81h:04m:23s remains)
INFO - root - 2017-12-08 00:47:05.722664: step 19700, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.587 sec/batch; 80h:48m:43s remains)
2017-12-08 00:47:08.429281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4623976 -0.92133355 -0.46843553 -0.067566872 0.17896891 0.22956038 0.20595598 0.29494715 0.49579239 0.67026234 0.76097441 0.79079866 0.80778122 0.85173559 0.85192966][-1.7245858 -1.2735579 -0.84435868 -0.42582417 -0.12165165 0.0091080666 0.077565193 0.23817778 0.45056343 0.61907148 0.71106291 0.74261236 0.7713213 0.83595991 0.82661152][-1.8846958 -1.5330684 -1.1127369 -0.67466831 -0.33756161 -0.14604378 0.027956009 0.27280331 0.49805737 0.67016315 0.758533 0.76565218 0.79464388 0.87716007 0.85821962][-2.0338712 -1.7413313 -1.274698 -0.79184842 -0.47037673 -0.32842445 -0.17139864 0.075280666 0.2933917 0.48861217 0.61543036 0.65080976 0.75130653 0.90614557 0.91090727][-2.1775877 -1.895201 -1.3629913 -0.88062859 -0.64300609 -0.62623787 -0.53722787 -0.30136061 -0.08173418 0.17559242 0.38718843 0.51623249 0.73715353 0.9765687 1.0131588][-2.1671271 -1.8064339 -1.2023206 -0.75664568 -0.606359 -0.651639 -0.51633549 -0.22951508 -0.04542923 0.18394232 0.39531326 0.55661726 0.84002733 1.1106043 1.1592526][-1.9496698 -1.4768009 -0.85644174 -0.49082112 -0.40091133 -0.40983105 -0.11970043 0.23499489 0.29559851 0.34016705 0.41516495 0.53633642 0.840312 1.1240077 1.2007923][-1.5902836 -1.0451391 -0.48995018 -0.24922562 -0.22367764 -0.21703005 0.14250994 0.4682889 0.36269617 0.21635866 0.14662361 0.24405766 0.61262131 0.96556425 1.1113048][-1.1606653 -0.611516 -0.19996357 -0.13268757 -0.21878719 -0.28445053 -0.018021584 0.1967864 0.039064407 -0.11769342 -0.17622805 -0.013712883 0.4385829 0.84889364 1.0460191][-0.71901584 -0.21430683 0.01952219 -0.078197956 -0.23947191 -0.34729719 -0.18647766 -0.052185059 -0.18165302 -0.28082514 -0.27565575 -0.027567863 0.45492077 0.84308386 1.0330577][-0.32248163 0.12855291 0.22768259 0.0865674 -0.03700304 -0.099722862 0.0089011192 0.070304394 -0.090848923 -0.23065758 -0.24351597 0.0087008476 0.43514538 0.75093174 0.91296959][-0.0027179718 0.36517525 0.37572384 0.25386143 0.18889809 0.177001 0.26952314 0.29092073 0.091971874 -0.11641693 -0.20422173 -0.044163227 0.24514055 0.46954489 0.61680937][0.17232084 0.40812016 0.33646774 0.22891474 0.20522642 0.23647451 0.35039568 0.38696337 0.22147036 0.0043692589 -0.14794064 -0.11355877 0.020963669 0.15640116 0.28266][0.14666414 0.25443411 0.1394105 0.050584793 0.059343338 0.12874079 0.26357651 0.33251905 0.23355532 0.047221184 -0.12079716 -0.16888475 -0.12973356 -0.04446125 0.058771133][0.007121563 0.043786049 -0.053984642 -0.10955143 -0.081326485 -4.0531158e-05 0.12833881 0.204669 0.14218855 -0.013204098 -0.17005825 -0.24098635 -0.23765516 -0.16694593 -0.077718258]]...]
INFO - root - 2017-12-08 00:47:53.750299: step 19710, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.561 sec/batch; 80h:21m:07s remains)
INFO - root - 2017-12-08 00:48:39.186201: step 19720, loss = 0.89, batch loss = 0.82 (7.0 examples/sec; 4.544 sec/batch; 80h:01m:32s remains)
INFO - root - 2017-12-08 00:49:24.937109: step 19730, loss = 0.74, batch loss = 0.67 (6.8 examples/sec; 4.689 sec/batch; 82h:34m:49s remains)
INFO - root - 2017-12-08 00:50:10.519566: step 19740, loss = 0.82, batch loss = 0.74 (7.0 examples/sec; 4.545 sec/batch; 80h:01m:11s remains)
INFO - root - 2017-12-08 00:50:56.014824: step 19750, loss = 0.84, batch loss = 0.76 (6.9 examples/sec; 4.628 sec/batch; 81h:28m:43s remains)
INFO - root - 2017-12-08 00:51:41.567207: step 19760, loss = 0.77, batch loss = 0.69 (7.2 examples/sec; 4.421 sec/batch; 77h:48m:54s remains)
INFO - root - 2017-12-08 00:52:27.245212: step 19770, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.557 sec/batch; 80h:11m:54s remains)
INFO - root - 2017-12-08 00:53:13.013316: step 19780, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 4.562 sec/batch; 80h:16m:04s remains)
INFO - root - 2017-12-08 00:53:58.318728: step 19790, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.563 sec/batch; 80h:16m:57s remains)
INFO - root - 2017-12-08 00:54:44.182350: step 19800, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 4.511 sec/batch; 79h:20m:55s remains)
2017-12-08 00:54:46.839787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.20123243 -0.26877928 -0.29134226 -0.24384785 -0.12856913 -0.073654652 -0.12480974 -0.22871017 -0.29623175 -0.076615334 0.29426861 0.41709471 0.16152334 -0.30137348 -0.94517851][0.28994989 0.20657206 0.14542913 0.13286924 0.16017342 0.13491344 0.030365944 -0.18739653 -0.45298266 -0.38900518 -0.035164356 0.15360737 0.019083023 -0.28921127 -0.841254][0.61544561 0.56106997 0.48272371 0.41666698 0.38330698 0.31829166 0.20272636 -0.083718777 -0.54572439 -0.69230485 -0.43097997 -0.1725316 -0.13534498 -0.28984165 -0.74860835][0.61696672 0.68241835 0.67113256 0.67112827 0.73647118 0.76797819 0.68344593 0.30927324 -0.37027502 -0.77173066 -0.68111968 -0.42579842 -0.31318903 -0.38654804 -0.76418829][0.33683872 0.57311964 0.70852041 0.86768579 1.1053534 1.2892294 1.236196 0.77747631 -0.089670181 -0.74329662 -0.89003778 -0.71812153 -0.56375074 -0.513124 -0.74947429][-0.13837242 0.23051023 0.60869026 1.0250053 1.4364715 1.7437086 1.6999731 1.1686168 0.21257687 -0.6072681 -0.94834137 -0.85119057 -0.62530732 -0.40259886 -0.48021555][-0.78925323 -0.32177877 0.36408281 1.1255946 1.8089776 2.3176923 2.3428974 1.7658291 0.77380753 -0.18359375 -0.75707316 -0.86598015 -0.68232322 -0.36905956 -0.32386017][-1.2474422 -0.67823386 0.26522732 1.3151717 2.2569656 2.9430933 3.0150342 2.3781104 1.2996612 0.15695477 -0.71613812 -1.0350442 -0.8918097 -0.51180387 -0.36531687][-1.1509883 -0.44528008 0.6791873 1.829041 2.7486172 3.23213 3.0477543 2.2809582 1.1868358 0.015607834 -0.94126463 -1.2913589 -1.1177475 -0.73412657 -0.57201886][-0.55413485 0.092806816 1.0737262 1.9994783 2.5930967 2.6429706 2.1761932 1.4406443 0.56037617 -0.41236258 -1.2725892 -1.6267362 -1.500905 -1.1976969 -1.0518982][0.028022766 0.29589987 0.78705645 1.2911587 1.5332861 1.2809887 0.7632761 0.24852324 -0.32314968 -1.0172293 -1.7251437 -2.0950153 -2.0715892 -1.8766329 -1.7701681][-0.084418774 -0.17921734 -0.076634884 0.15659285 0.2100482 -0.14124441 -0.55882835 -0.86060166 -1.1894791 -1.6358011 -2.1772025 -2.5488706 -2.6344829 -2.5717287 -2.5238616][-0.77954865 -1.0327053 -1.0877855 -0.94735861 -0.91045237 -1.1773441 -1.4446261 -1.6056764 -1.7717462 -2.0038805 -2.3601809 -2.6770978 -2.8060577 -2.7993579 -2.7412753][-1.3480148 -1.6370013 -1.6915445 -1.5150733 -1.388195 -1.4916105 -1.6641877 -1.823534 -1.9503613 -2.0698016 -2.2869885 -2.5160131 -2.6392956 -2.659313 -2.5932672][-1.5881062 -1.7818842 -1.7632718 -1.5653098 -1.4077661 -1.4080594 -1.5396574 -1.7443454 -1.8898225 -1.9819908 -2.1326306 -2.3066695 -2.4142778 -2.4305308 -2.3481843]]...]
INFO - root - 2017-12-08 00:55:32.403888: step 19810, loss = 0.85, batch loss = 0.77 (7.0 examples/sec; 4.551 sec/batch; 80h:02m:50s remains)
INFO - root - 2017-12-08 00:56:17.713100: step 19820, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.525 sec/batch; 79h:34m:36s remains)
INFO - root - 2017-12-08 00:57:02.925639: step 19830, loss = 0.84, batch loss = 0.76 (7.0 examples/sec; 4.555 sec/batch; 80h:05m:40s remains)
INFO - root - 2017-12-08 00:57:48.667770: step 19840, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.571 sec/batch; 80h:20m:53s remains)
INFO - root - 2017-12-08 00:58:33.856009: step 19850, loss = 0.89, batch loss = 0.82 (7.2 examples/sec; 4.462 sec/batch; 78h:25m:19s remains)
INFO - root - 2017-12-08 00:59:19.665853: step 19860, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.593 sec/batch; 80h:42m:27s remains)
INFO - root - 2017-12-08 01:00:04.954772: step 19870, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.572 sec/batch; 80h:19m:54s remains)
INFO - root - 2017-12-08 01:00:50.610309: step 19880, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 4.535 sec/batch; 79h:40m:40s remains)
INFO - root - 2017-12-08 01:01:35.689948: step 19890, loss = 0.72, batch loss = 0.64 (7.1 examples/sec; 4.488 sec/batch; 78h:50m:20s remains)
INFO - root - 2017-12-08 01:02:21.446656: step 19900, loss = 0.76, batch loss = 0.68 (7.1 examples/sec; 4.527 sec/batch; 79h:29m:56s remains)
2017-12-08 01:02:24.107153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9472363 -1.9275169 -1.954536 -2.0131419 -2.0556374 -2.0687866 -2.0778909 -2.0687964 -2.0687597 -2.0906544 -2.1193533 -2.1387765 -2.1353583 -2.1151013 -2.0856423][-1.7783308 -1.7792449 -1.825778 -1.8908284 -1.9372439 -1.9676902 -2.0034947 -1.976517 -1.9213641 -1.8926291 -1.9145932 -1.9724011 -2.0123107 -2.0310977 -2.0155492][-1.5883791 -1.6510546 -1.7504241 -1.8162606 -1.8506165 -1.9252369 -2.0667782 -2.101243 -2.0139723 -1.8719349 -1.8119891 -1.8852544 -1.9808705 -2.0461383 -2.0103812][-1.3251839 -1.4203918 -1.5562887 -1.6286495 -1.622952 -1.7076986 -1.9525704 -2.1186013 -2.0870917 -1.8237078 -1.6221347 -1.6838534 -1.8645492 -2.017947 -1.9843068][-1.11696 -1.1844037 -1.3018537 -1.3223321 -1.1801066 -1.1657536 -1.3987563 -1.687964 -1.8036027 -1.5359902 -1.280993 -1.331907 -1.5765355 -1.8206077 -1.8435369][-1.0212598 -1.0135403 -1.052202 -0.97594237 -0.65404248 -0.42012548 -0.40524006 -0.63491631 -0.94740152 -0.94264936 -0.93680978 -1.0846429 -1.3250685 -1.587492 -1.6594279][-1.1571794 -1.071883 -0.99562716 -0.77487326 -0.22686148 0.35112572 0.85591459 0.92727518 0.46183872 0.031047821 -0.42675018 -0.76318622 -0.97687435 -1.2098975 -1.3089201][-1.5805795 -1.4625847 -1.2753882 -0.89568663 -0.17698574 0.58955383 1.380549 1.7115769 1.1934266 0.48390532 -0.29063702 -0.75347424 -0.92014551 -1.0408778 -1.0239284][-2.2718763 -2.20892 -1.985163 -1.5158968 -0.80926538 -0.17333221 0.50945425 0.88197041 0.58348513 0.081267834 -0.6372633 -1.1302023 -1.3128035 -1.3034005 -1.0290141][-2.9191113 -2.9626448 -2.8409617 -2.4933043 -2.0524061 -1.7702224 -1.4016871 -1.0833025 -1.0975347 -1.1810353 -1.5730515 -1.9525177 -2.1181505 -1.9648235 -1.4028738][-3.4131179 -3.6044991 -3.6734495 -3.5367227 -3.3578439 -3.2859349 -3.0576978 -2.7479126 -2.5696807 -2.3865433 -2.4910467 -2.7141039 -2.8394282 -2.6328313 -1.9623783][-3.5972383 -3.8904803 -4.1089344 -4.157176 -4.1772738 -4.2095547 -4.0547423 -3.8155937 -3.6089458 -3.3418083 -3.2441454 -3.2284274 -3.1653252 -2.8980846 -2.2861295][-3.3388057 -3.6378136 -3.8978512 -4.0444527 -4.1892438 -4.3142314 -4.3015881 -4.219461 -4.0722485 -3.7973866 -3.5728793 -3.3642073 -3.1256099 -2.8137527 -2.3330202][-2.911999 -3.160605 -3.4000049 -3.5723462 -3.7215962 -3.8245244 -3.8674767 -3.8927197 -3.804306 -3.5578508 -3.3216307 -3.1028805 -2.8693347 -2.6100154 -2.2707465][-2.4901185 -2.6551068 -2.8274167 -2.9824185 -3.1109681 -3.1878936 -3.25745 -3.3447134 -3.3384624 -3.180675 -2.9796612 -2.7847455 -2.5967622 -2.4166603 -2.1954441]]...]
INFO - root - 2017-12-08 01:03:09.427224: step 19910, loss = 0.85, batch loss = 0.78 (7.0 examples/sec; 4.567 sec/batch; 80h:11m:50s remains)
INFO - root - 2017-12-08 01:03:55.196489: step 19920, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 4.567 sec/batch; 80h:10m:40s remains)
INFO - root - 2017-12-08 01:04:40.265028: step 19930, loss = 0.87, batch loss = 0.80 (7.7 examples/sec; 4.162 sec/batch; 73h:04m:07s remains)
INFO - root - 2017-12-08 01:05:25.962666: step 19940, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.598 sec/batch; 80h:42m:05s remains)
INFO - root - 2017-12-08 01:06:11.788630: step 19950, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 4.550 sec/batch; 79h:50m:23s remains)
INFO - root - 2017-12-08 01:06:57.153032: step 19960, loss = 0.80, batch loss = 0.72 (7.1 examples/sec; 4.520 sec/batch; 79h:18m:43s remains)
INFO - root - 2017-12-08 01:07:42.866177: step 19970, loss = 0.82, batch loss = 0.74 (7.0 examples/sec; 4.539 sec/batch; 79h:37m:41s remains)
INFO - root - 2017-12-08 01:08:28.092050: step 19980, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 4.519 sec/batch; 79h:15m:57s remains)
INFO - root - 2017-12-08 01:09:13.775378: step 19990, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 4.504 sec/batch; 78h:59m:24s remains)
INFO - root - 2017-12-08 01:09:59.094203: step 20000, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.582 sec/batch; 80h:20m:50s remains)
2017-12-08 01:10:01.780875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1376865 -2.6117225 -2.1457076 -1.5213728 -0.89187956 -1.0674016 -1.6692324 -2.1688724 -2.2033589 -1.6091969 -0.947062 -1.1031966 -1.3782961 -1.5207818 -2.1352065][-3.1209855 -2.4110737 -1.8650348 -1.3316331 -0.92785072 -1.2623532 -1.8933392 -2.4806461 -2.5695567 -1.9162776 -1.0985246 -1.0262249 -1.1023133 -1.1534069 -1.8091528][-2.8182535 -1.9367967 -1.3155916 -0.9729259 -0.96363521 -1.6203685 -2.4925122 -3.3485034 -3.5610476 -2.8181615 -1.7597826 -1.3453183 -1.1966627 -1.189431 -1.8335738][-2.5896728 -1.6396971 -0.98176265 -0.90803432 -1.360316 -2.2331252 -3.0586982 -3.7429004 -3.7015991 -2.8294053 -1.8233836 -1.5098736 -1.543016 -1.7336013 -2.3946309][-2.7565093 -1.8727019 -1.2097859 -1.3039088 -2.0475745 -2.9006934 -3.2792439 -3.2460692 -2.6390481 -1.7688797 -1.1861014 -1.3270183 -1.7988935 -2.3375967 -3.10857][-3.0248847 -2.33047 -1.8152092 -1.9737635 -2.6299024 -2.9943357 -2.4830251 -1.3943746 -0.34984255 -0.093230247 -0.6040144 -1.4924669 -2.2806027 -2.8775051 -3.6216519][-3.3420444 -2.8861766 -2.6720521 -2.9559822 -3.3174927 -2.7880747 -0.91934347 1.5342579 2.8781571 1.9546108 -0.16438437 -2.0468664 -3.0325031 -3.3238254 -3.716078][-3.4943979 -3.2232814 -3.3508704 -3.87274 -4.0317993 -2.7841644 0.062571049 3.3252363 4.5911226 2.6286759 -0.46893883 -2.7312138 -3.5344446 -3.3758874 -3.3446569][-3.3102508 -3.1622505 -3.5276256 -4.1579185 -4.1385293 -2.6393342 0.17129803 2.8959875 3.3244715 0.93678808 -1.9795418 -3.6055155 -3.6778507 -2.9781828 -2.6204519][-3.0192389 -2.896837 -3.3633146 -3.9688215 -3.7609167 -2.3176281 -0.14996481 1.41541 0.90382719 -1.470506 -3.633112 -4.2704206 -3.5225446 -2.4280918 -1.8556604][-2.9049106 -2.61337 -2.9534731 -3.428721 -3.1642184 -2.1156073 -0.93418336 -0.56973982 -1.649317 -3.5366802 -4.6805215 -4.4283714 -3.1722436 -2.0084231 -1.4019942][-2.9624429 -2.3991883 -2.3361897 -2.4500594 -2.1716115 -1.760251 -1.7132998 -2.3879795 -3.701587 -4.9018936 -5.138607 -4.3499541 -2.9711022 -1.9478381 -1.366133][-3.2981696 -2.5619612 -2.0977576 -1.8322515 -1.5518787 -1.6381705 -2.3080623 -3.4469509 -4.6566291 -5.3150749 -5.1334877 -4.3176422 -3.1434176 -2.2704306 -1.5695343][-3.5823212 -2.9135113 -2.3879137 -2.0130517 -1.7704809 -1.9922557 -2.7253261 -3.71166 -4.558918 -4.8597121 -4.6064482 -4.040061 -3.2501018 -2.5953412 -1.8243878][-3.7155209 -3.292491 -3.0148978 -2.8011839 -2.644002 -2.7626152 -3.172627 -3.7248392 -4.1866817 -4.3054128 -4.1148615 -3.7585957 -3.249651 -2.7585411 -2.0231223]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-batch32/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-batch32/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 01:10:47.721953: step 20010, loss = 0.77, batch loss = 0.69 (7.1 examples/sec; 4.513 sec/batch; 79h:07m:10s remains)
INFO - root - 2017-12-08 01:11:33.058755: step 20020, loss = 0.71, batch loss = 0.64 (6.9 examples/sec; 4.617 sec/batch; 80h:56m:24s remains)
INFO - root - 2017-12-08 01:12:18.330014: step 20030, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 4.513 sec/batch; 79h:06m:11s remains)
INFO - root - 2017-12-08 01:13:03.850105: step 20040, loss = 0.81, batch loss = 0.73 (7.0 examples/sec; 4.561 sec/batch; 79h:55m:51s remains)
INFO - root - 2017-12-08 01:13:49.553230: step 20050, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 4.588 sec/batch; 80h:22m:46s remains)
INFO - root - 2017-12-08 01:14:34.972745: step 20060, loss = 0.84, batch loss = 0.77 (7.1 examples/sec; 4.520 sec/batch; 79h:10m:41s remains)
INFO - root - 2017-12-08 01:15:20.585155: step 20070, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 4.591 sec/batch; 80h:25m:16s remains)
INFO - root - 2017-12-08 01:16:05.892793: step 20080, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.630 sec/batch; 81h:04m:58s remains)
INFO - root - 2017-12-08 01:16:51.540534: step 20090, loss = 0.86, batch loss = 0.79 (6.9 examples/sec; 4.617 sec/batch; 80h:50m:06s remains)
INFO - root - 2017-12-08 01:17:36.866210: step 20100, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.506 sec/batch; 78h:53m:13s remains)
2017-12-08 01:17:39.383133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6467743 -3.6882715 -3.7100792 -3.7040157 -3.706738 -3.7397881 -3.7680018 -3.7658508 -3.7533979 -3.7185845 -3.6765845 -3.6790473 -3.6809444 -3.6385591 -3.5879226][-3.1378717 -3.1228528 -3.132247 -3.174793 -3.2473421 -3.299722 -3.2993562 -3.2680991 -3.2360291 -3.178638 -3.1349449 -3.2145691 -3.328948 -3.3625491 -3.3647356][-2.5736194 -2.4384019 -2.3831379 -2.4888694 -2.6760082 -2.7739248 -2.7563252 -2.7123504 -2.6603417 -2.5729213 -2.505579 -2.6375589 -2.8688409 -3.0043371 -3.0821447][-2.3768294 -2.1035783 -1.9292135 -2.0171092 -2.2136955 -2.3061297 -2.3074498 -2.3304558 -2.3307245 -2.2404153 -2.1199622 -2.2013156 -2.438818 -2.59278 -2.7360215][-2.5163682 -2.1508365 -1.8722606 -1.8566461 -1.929492 -1.8788536 -1.7805972 -1.8068664 -1.8969967 -1.9146879 -1.8294969 -1.8573408 -2.0152438 -2.1126716 -2.2955446][-2.4388158 -2.1645262 -1.920531 -1.842912 -1.7561383 -1.4176137 -0.96322584 -0.75498343 -0.89072728 -1.169893 -1.3493679 -1.5378895 -1.7523932 -1.8764102 -2.0960233][-2.1450317 -2.0554903 -1.9464242 -1.916625 -1.783824 -1.2468684 -0.44817948 0.0784111 -0.06145668 -0.60234904 -1.0899594 -1.4916611 -1.8244591 -2.0333724 -2.2857344][-2.071456 -2.1341023 -2.182327 -2.3204098 -2.3167732 -1.8713725 -1.0861528 -0.50643611 -0.54139137 -1.0305879 -1.5098157 -1.8606014 -2.087157 -2.2433231 -2.4721212][-2.0329328 -2.1172349 -2.2645433 -2.5138674 -2.6185648 -2.4172802 -1.9634511 -1.5880153 -1.5600305 -1.838057 -2.1239729 -2.3107567 -2.36192 -2.3710027 -2.5178289][-2.0034394 -1.9749706 -2.0351467 -2.1504505 -2.1691678 -2.1047611 -1.9778078 -1.9057248 -2.010365 -2.2252049 -2.4017882 -2.5544367 -2.5784585 -2.5229459 -2.586287][-2.248245 -2.1155853 -2.0341723 -1.9564235 -1.852092 -1.8069165 -1.8310544 -1.9294319 -2.1228743 -2.3225241 -2.4604692 -2.6284864 -2.6782935 -2.6235642 -2.6461041][-2.6859446 -2.6016028 -2.5283222 -2.4183822 -2.288619 -2.2556231 -2.2941916 -2.3700035 -2.4779363 -2.5315385 -2.5745957 -2.7127566 -2.78587 -2.7478976 -2.7546463][-3.1638832 -3.1288409 -3.1189842 -3.116461 -3.0854681 -3.0895643 -3.0611627 -3.0151546 -2.9688914 -2.8833847 -2.8385327 -2.9121304 -2.9591973 -2.9080939 -2.8874967][-3.5249538 -3.5589113 -3.6325121 -3.7481456 -3.861201 -3.9360738 -3.863658 -3.7047725 -3.566179 -3.4527414 -3.3781397 -3.3469877 -3.2569335 -3.1006894 -3.0125811][-3.7885945 -3.8992333 -4.0377326 -4.1843834 -4.2968607 -4.328393 -4.2020831 -4.0035148 -3.8652191 -3.808157 -3.7732031 -3.6957576 -3.5213714 -3.3099439 -3.1908805]]...]
INFO - root - 2017-12-08 01:18:25.100727: step 20110, loss = 0.75, batch loss = 0.67 (7.1 examples/sec; 4.512 sec/batch; 78h:58m:22s remains)
INFO - root - 2017-12-08 01:19:10.642494: step 20120, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.534 sec/batch; 79h:21m:00s remains)
INFO - root - 2017-12-08 01:19:56.328911: step 20130, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.549 sec/batch; 79h:36m:10s remains)
INFO - root - 2017-12-08 01:20:41.645256: step 20140, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 4.546 sec/batch; 79h:32m:34s remains)
INFO - root - 2017-12-08 01:21:27.202059: step 20150, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.596 sec/batch; 80h:23m:26s remains)
INFO - root - 2017-12-08 01:22:13.009371: step 20160, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.639 sec/batch; 81h:07m:49s remains)
INFO - root - 2017-12-08 01:22:58.612100: step 20170, loss = 0.88, batch loss = 0.80 (7.0 examples/sec; 4.585 sec/batch; 80h:11m:08s remains)
INFO - root - 2017-12-08 01:23:44.128289: step 20180, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.549 sec/batch; 79h:31m:57s remains)
INFO - root - 2017-12-08 01:24:29.728893: step 20190, loss = 0.81, batch loss = 0.74 (6.9 examples/sec; 4.628 sec/batch; 80h:54m:13s remains)
INFO - root - 2017-12-08 01:25:15.365088: step 20200, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.591 sec/batch; 80h:15m:10s remains)
2017-12-08 01:25:18.154854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1279416 -3.1296701 -3.3382335 -3.3459601 -2.6607518 -1.7079821 -1.3808892 -1.4540672 -1.05877 -0.25007915 0.84402609 1.8395033 1.0162244 -1.4884391 -3.0551124][-2.7977538 -2.4035125 -2.4324951 -2.552999 -2.0308645 -1.3080528 -1.3748372 -1.620173 -1.131907 -0.2227149 0.95619345 2.1371989 1.4492044 -1.1455851 -2.9038665][-2.3842804 -1.6010931 -1.4283156 -1.598145 -1.1644094 -0.71540213 -1.2817094 -1.7280238 -1.203573 -0.26259804 0.88017654 2.0827961 1.529366 -0.98880935 -2.8509173][-2.2816677 -1.2476292 -0.89417267 -0.90110707 -0.35330582 -0.028968811 -0.9351306 -1.4459338 -0.915843 -0.07367897 0.86308479 1.8983455 1.4043736 -0.93522 -2.7807117][-2.5311544 -1.4774094 -1.014776 -0.75590539 0.084715843 0.48006916 -0.52631807 -0.94504571 -0.39740515 0.21659136 0.76250315 1.4330959 0.92607641 -1.1471708 -2.7941294][-2.5892153 -1.7572024 -1.3875232 -0.91665196 0.30707788 0.96183968 0.11309195 -0.077008724 0.39359379 0.54790688 0.48345423 0.60678148 -0.016974449 -1.7493484 -3.0198491][-2.2699945 -1.7976346 -1.6667573 -1.116488 0.43589258 1.4551377 1.0442004 1.2711582 1.5840731 1.1296263 0.39178419 -0.08185339 -0.91115117 -2.3522849 -3.2589846][-1.9874465 -1.7397888 -1.7465115 -1.2269657 0.43912649 1.7711964 2.0070448 2.769413 2.888948 1.8162274 0.51088095 -0.46178341 -1.475153 -2.6722364 -3.3174174][-1.8852532 -1.6172717 -1.569973 -1.1890497 0.2383585 1.6245947 2.416327 3.57833 3.4785295 1.9806395 0.36038256 -0.882179 -1.9307835 -2.8750772 -3.3406658][-1.5276468 -1.116595 -0.91173577 -0.78894019 0.068131447 1.1746931 2.1051536 3.287302 3.0656257 1.5093818 -0.1673255 -1.5280805 -2.5353024 -3.2734694 -3.6145329][-1.1250441 -0.47419214 0.038455963 -0.063848972 0.20719624 0.89256763 1.6163096 2.5608253 2.342248 1.0102868 -0.55526829 -1.9265139 -2.9422312 -3.6188231 -3.9503167][-0.90727091 0.030121326 0.90844154 0.70176554 0.61214828 0.9493289 1.2671824 1.8393602 1.6729717 0.65547323 -0.63449478 -1.8509116 -2.8349805 -3.4993491 -3.8820295][-0.75580597 0.35909843 1.4422526 1.1985288 0.97694111 1.050364 0.85935545 0.95418262 0.76568747 0.076628685 -0.75042343 -1.5988767 -2.4129882 -3.0038872 -3.3979573][-0.34615993 0.71587992 1.7220078 1.470614 1.3113894 1.1585984 0.36682415 -0.1236124 -0.50833726 -0.95131969 -1.244503 -1.6269608 -2.1834021 -2.6239905 -2.9184144][0.22806931 0.98713446 1.7230239 1.5308552 1.5801845 1.2991719 0.036755562 -0.99206281 -1.7384992 -2.0851247 -1.9625204 -1.9546652 -2.2645786 -2.5494571 -2.7132792]]...]
INFO - root - 2017-12-08 01:26:03.651946: step 20210, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.591 sec/batch; 80h:13m:51s remains)
INFO - root - 2017-12-08 01:26:49.416367: step 20220, loss = 0.90, batch loss = 0.83 (7.0 examples/sec; 4.552 sec/batch; 79h:32m:07s remains)
INFO - root - 2017-12-08 01:27:34.951565: step 20230, loss = 0.87, batch loss = 0.80 (7.0 examples/sec; 4.550 sec/batch; 79h:29m:47s remains)
INFO - root - 2017-12-08 01:28:20.742257: step 20240, loss = 0.70, batch loss = 0.62 (7.2 examples/sec; 4.423 sec/batch; 77h:16m:09s remains)
INFO - root - 2017-12-08 01:29:06.418797: step 20250, loss = 0.67, batch loss = 0.60 (6.9 examples/sec; 4.623 sec/batch; 80h:44m:19s remains)
INFO - root - 2017-12-08 01:29:52.238467: step 20260, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.576 sec/batch; 79h:54m:30s remains)
INFO - root - 2017-12-08 01:30:37.848856: step 20270, loss = 0.65, batch loss = 0.58 (7.2 examples/sec; 4.434 sec/batch; 77h:24m:46s remains)
INFO - root - 2017-12-08 01:31:23.398842: step 20280, loss = 0.73, batch loss = 0.65 (7.0 examples/sec; 4.547 sec/batch; 79h:23m:01s remains)
INFO - root - 2017-12-08 01:32:09.065098: step 20290, loss = 0.88, batch loss = 0.81 (7.0 examples/sec; 4.577 sec/batch; 79h:52m:49s remains)
INFO - root - 2017-12-08 01:32:54.851168: step 20300, loss = 0.75, batch loss = 0.68 (6.8 examples/sec; 4.683 sec/batch; 81h:43m:52s remains)
2017-12-08 01:32:57.609512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8197045 -2.5398946 -2.1559229 -2.0650547 -2.035934 -2.0447781 -2.1219168 -1.8624682 -1.6207254 -1.6653929 -1.4680109 -1.2599406 -1.4619868 -1.6930113 -1.6441014][-3.2216835 -2.7371216 -2.2127819 -1.9641688 -1.7757697 -1.6346018 -1.6785257 -1.6109428 -1.5530789 -1.5934589 -1.2409692 -0.902987 -1.0911546 -1.4143641 -1.5020254][-3.4075279 -2.7915215 -2.1817284 -1.8040094 -1.5028148 -1.2204905 -1.1841681 -1.298629 -1.4855201 -1.6383436 -1.285553 -0.88729906 -1.0278795 -1.3476956 -1.4828806][-2.8171821 -2.2481537 -1.7342408 -1.4687722 -1.2729592 -1.0174885 -0.92120934 -1.1148491 -1.5008714 -1.8227167 -1.5892308 -1.1779206 -1.219167 -1.490175 -1.7186117][-2.2158535 -1.6783113 -1.2225056 -1.0791032 -1.0381906 -0.90675688 -0.81359744 -1.0647039 -1.652658 -2.1910594 -2.0592904 -1.5133746 -1.2104299 -1.2479753 -1.6176116][-1.826822 -1.4195275 -1.0726976 -0.97624731 -0.92911887 -0.74750352 -0.55683136 -0.822963 -1.6722493 -2.513855 -2.5572698 -1.9052329 -1.1322355 -0.73928666 -1.0797253][-0.82236695 -0.54315352 -0.37031126 -0.36283731 -0.29573584 0.04086113 0.43842459 0.16264629 -0.97863555 -2.1408553 -2.4384322 -1.8452272 -0.83976221 -0.20479441 -0.5251646][0.33803272 0.502337 0.50302458 0.40286016 0.50024986 1.0168729 1.558167 1.1545348 -0.31431007 -1.7267263 -2.243542 -1.7699711 -0.744529 -0.037505627 -0.33704567][0.42149019 0.45017195 0.26837206 0.016943455 0.040884495 0.62411213 1.2095332 0.85821581 -0.56577182 -2.0163016 -2.6763053 -2.2705233 -1.159395 -0.24805355 -0.3165884][0.11909962 0.13797808 -0.17122459 -0.6067028 -0.79258466 -0.38728428 0.066437721 -0.15478611 -1.2499759 -2.5676026 -3.3380289 -3.0729845 -1.9921548 -0.93100309 -0.73946905][0.013216496 0.21452618 0.10060215 -0.19844437 -0.42262602 -0.31118965 -0.208457 -0.55779934 -1.4896805 -2.6530323 -3.4383678 -3.3262196 -2.431695 -1.4552448 -1.1797543][-1.0151796 -0.72474718 -0.58301187 -0.54443765 -0.49441624 -0.29615021 -0.15510273 -0.38073874 -1.0350955 -1.8561432 -2.4169362 -2.3504837 -1.7383752 -1.0838969 -0.98526883][-2.2345195 -2.0471737 -1.8695657 -1.6978586 -1.4483995 -1.0887458 -0.75877976 -0.70930147 -1.0033355 -1.4550698 -1.7620485 -1.7088509 -1.3959208 -1.0875413 -1.1404905][-2.5100136 -2.3700106 -2.2745292 -2.229908 -2.1510198 -2.0124118 -1.8623962 -1.8211482 -1.9535668 -2.1857324 -2.3062835 -2.18537 -1.9426625 -1.7195973 -1.7003226][-2.5011806 -2.3008986 -2.2351477 -2.2801824 -2.3670046 -2.41781 -2.4170325 -2.4110854 -2.4258494 -2.4483347 -2.361299 -2.1109743 -1.8266211 -1.5932703 -1.5106614]]...]
INFO - root - 2017-12-08 01:33:43.562781: step 20310, loss = 0.80, batch loss = 0.72 (6.9 examples/sec; 4.666 sec/batch; 81h:25m:24s remains)
INFO - root - 2017-12-08 01:34:29.157459: step 20320, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 4.589 sec/batch; 80h:03m:11s remains)
INFO - root - 2017-12-08 01:35:14.807152: step 20330, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 4.512 sec/batch; 78h:41m:47s remains)
INFO - root - 2017-12-08 01:36:00.659676: step 20340, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 4.541 sec/batch; 79h:11m:57s remains)
INFO - root - 2017-12-08 01:36:46.187686: step 20350, loss = 0.69, batch loss = 0.62 (7.1 examples/sec; 4.492 sec/batch; 78h:19m:32s remains)
INFO - root - 2017-12-08 01:37:31.566735: step 20360, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 4.571 sec/batch; 79h:41m:33s remains)
INFO - root - 2017-12-08 01:38:17.259543: step 20370, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 4.626 sec/batch; 80h:37m:57s remains)
INFO - root - 2017-12-08 01:39:03.131757: step 20380, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 4.631 sec/batch; 80h:42m:37s remains)
INFO - root - 2017-12-08 01:39:48.857506: step 20390, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.620 sec/batch; 80h:30m:22s remains)
INFO - root - 2017-12-08 01:40:34.734142: step 20400, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 4.664 sec/batch; 81h:15m:42s remains)
2017-12-08 01:40:37.593088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3074026 -1.3565841 -1.3142819 -1.3499177 -1.5033391 -1.6546867 -1.6901262 -1.6524544 -1.5966012 -1.4988756 -1.4221141 -1.3983309 -1.299916 -1.2476451 -1.2451248][-1.027056 -1.047698 -1.0496571 -1.0992475 -1.2225275 -1.3970151 -1.4932506 -1.4696374 -1.3832333 -1.2900109 -1.2633059 -1.3256359 -1.2772896 -1.2349033 -1.3027389][-0.65622163 -0.66488886 -0.77378726 -0.87415051 -0.98425484 -1.1888859 -1.3322322 -1.2949739 -1.1543646 -1.0710375 -1.1193361 -1.2862561 -1.3183784 -1.3088906 -1.4073946][-0.3132596 -0.36532116 -0.57449341 -0.67244196 -0.71118236 -0.90388012 -1.0766041 -1.0513346 -0.89927864 -0.87559962 -1.01684 -1.2192268 -1.2742803 -1.2565939 -1.3023562][-0.10748434 -0.1755991 -0.39733791 -0.44079709 -0.39589167 -0.57267857 -0.80270386 -0.83751297 -0.72167754 -0.74799514 -0.8848989 -0.99954557 -1.039346 -1.0202849 -0.99603939][-0.0018596649 -0.0895977 -0.28027344 -0.27668953 -0.17124414 -0.31706381 -0.57262588 -0.64708567 -0.56130195 -0.59657717 -0.64831614 -0.6270175 -0.67672324 -0.69123983 -0.60341859][-0.061799049 -0.13730478 -0.21576738 -0.11286926 0.041017532 -0.063123226 -0.30563736 -0.42468596 -0.41918945 -0.48953891 -0.49323535 -0.39646435 -0.44051027 -0.45680213 -0.29865408][-0.0017418861 -0.07065773 -0.069661617 0.064026356 0.20145702 0.15322447 -0.040150642 -0.22009182 -0.34913254 -0.51517749 -0.55927944 -0.45645404 -0.45921063 -0.44121981 -0.25180864][0.24688864 0.14552784 0.17248917 0.28304672 0.37171078 0.3971386 0.27788305 0.057179451 -0.21299839 -0.49039769 -0.60638928 -0.54541707 -0.52632046 -0.51816225 -0.40041161][0.32535219 0.20907164 0.25574017 0.34185362 0.40081978 0.49957466 0.49074125 0.29747534 -0.0049738884 -0.30286074 -0.45541692 -0.46923113 -0.49488664 -0.53402996 -0.47898626][0.0090308189 -0.087930679 -0.0052051544 0.091456413 0.13368464 0.26712894 0.33274555 0.19321346 -0.039129257 -0.26168585 -0.41121054 -0.493356 -0.58680415 -0.67076492 -0.5933795][-0.59950829 -0.65265775 -0.50365138 -0.34832478 -0.27521896 -0.13394356 -0.071177959 -0.17633295 -0.30762291 -0.43407822 -0.58419013 -0.737839 -0.90005851 -1.0115969 -0.87999392][-1.3219769 -1.3337476 -1.1291873 -0.898782 -0.76351166 -0.62380719 -0.59818864 -0.69315696 -0.73616624 -0.75382328 -0.88633466 -1.0925758 -1.3199534 -1.4642875 -1.3077085][-1.8850389 -1.8667421 -1.6654208 -1.4278386 -1.2643404 -1.1217113 -1.1281922 -1.2380757 -1.2629871 -1.2511516 -1.3673563 -1.5479465 -1.7486706 -1.8896751 -1.7580874][-2.2428274 -2.2218242 -2.0740685 -1.8959191 -1.7514832 -1.6249509 -1.6331999 -1.7483592 -1.8046761 -1.8372135 -1.9283099 -2.0091777 -2.0855875 -2.1674836 -2.1127558]]...]
INFO - root - 2017-12-08 01:41:23.453535: step 20410, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 4.547 sec/batch; 79h:12m:16s remains)
INFO - root - 2017-12-08 01:42:09.148906: step 20420, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 4.495 sec/batch; 78h:18m:04s remains)
INFO - root - 2017-12-08 01:42:54.958380: step 20430, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.547 sec/batch; 79h:10m:47s remains)
INFO - root - 2017-12-08 01:43:40.607815: step 20440, loss = 0.74, batch loss = 0.67 (7.2 examples/sec; 4.469 sec/batch; 77h:49m:03s remains)
INFO - root - 2017-12-08 01:44:26.253567: step 20450, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 4.595 sec/batch; 79h:59m:26s remains)
INFO - root - 2017-12-08 01:45:12.063423: step 20460, loss = 0.85, batch loss = 0.78 (6.9 examples/sec; 4.611 sec/batch; 80h:16m:02s remains)
INFO - root - 2017-12-08 01:45:57.378784: step 20470, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 4.544 sec/batch; 79h:05m:20s remains)
INFO - root - 2017-12-08 01:46:42.929866: step 20480, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 4.515 sec/batch; 78h:33m:45s remains)
INFO - root - 2017-12-08 01:47:28.769851: step 20490, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.586 sec/batch; 79h:47m:31s remains)
INFO - root - 2017-12-08 01:48:14.434206: step 20500, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 4.546 sec/batch; 79h:05m:10s remains)
2017-12-08 01:48:17.180870: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.112102 -1.8176837 -1.681596 -1.7293358 -1.7785833 -1.736541 -1.7139313 -1.7632384 -1.8349602 -1.9982464 -2.3587196 -2.747436 -2.8754003 -2.8777204 -2.8525615][-2.18851 -1.9035521 -1.7248406 -1.6571038 -1.4920502 -1.2000277 -0.96874189 -0.93159294 -1.0528913 -1.4210637 -2.1194746 -2.7894654 -3.0090566 -2.9661398 -2.8262539][-2.4851985 -2.2484357 -2.0986936 -1.9817429 -1.6559408 -1.1349959 -0.61102223 -0.33810902 -0.45302272 -1.0759447 -2.1565344 -3.0613804 -3.3428869 -3.2531285 -2.9975431][-3.0203071 -2.7556624 -2.607192 -2.4771385 -1.9627476 -1.1319978 -0.23228836 0.28145409 0.11643696 -0.77207351 -2.138972 -3.1966033 -3.5891838 -3.5508285 -3.2581429][-3.4679577 -3.1950316 -3.0970168 -2.8922927 -2.1256595 -1.0335894 0.17289734 0.84994125 0.59286785 -0.4580133 -1.8948967 -2.9914489 -3.5246692 -3.6214421 -3.3889375][-3.7060421 -3.4352016 -3.1991892 -2.5924578 -1.45928 -0.29243755 0.84075356 1.3924537 0.90243149 -0.28805208 -1.6947591 -2.8045292 -3.4254236 -3.6033986 -3.4175582][-3.7381747 -3.2322564 -2.4536006 -1.1385348 0.30518818 1.2663193 1.9607906 2.0707102 1.1554365 -0.30393219 -1.780931 -2.928699 -3.4739308 -3.5416496 -3.3231473][-3.4659886 -2.6228669 -1.3027771 0.40374279 1.7527962 2.3471603 2.5438132 2.1722422 0.81207609 -0.94453883 -2.4686148 -3.473057 -3.7349637 -3.5742283 -3.3378651][-3.2581925 -2.207154 -0.63102841 1.0242333 2.0253668 2.3037462 2.0940037 1.2608676 -0.3906436 -2.1647079 -3.5061393 -4.1183219 -3.977186 -3.6596432 -3.5154042][-3.2186217 -2.0884171 -0.55865788 0.771863 1.4232454 1.4915409 0.92697668 -0.25420809 -1.8874993 -3.3972948 -4.3729324 -4.5108442 -4.0310936 -3.6851234 -3.6582172][-3.2969875 -2.2835853 -1.050432 -0.14461136 0.26601696 0.21770144 -0.53909063 -1.7661695 -3.0881338 -4.1610217 -4.7198768 -4.497211 -3.8817468 -3.5984788 -3.6373644][-3.4550068 -2.6599674 -1.7601683 -1.1421959 -0.87213683 -1.0131142 -1.8103938 -2.8528526 -3.729228 -4.3125324 -4.4846368 -4.0811367 -3.5551529 -3.4029756 -3.4778683][-3.6239169 -2.9861183 -2.317903 -1.8975329 -1.7904289 -2.045768 -2.750252 -3.4687681 -3.9269328 -4.1603632 -4.1268182 -3.7492635 -3.4034495 -3.3486784 -3.4169788][-3.8101032 -3.271033 -2.7271652 -2.4416478 -2.5170469 -2.8528914 -3.35142 -3.7592387 -3.9451795 -3.9822063 -3.873394 -3.6028688 -3.3994236 -3.3681295 -3.4017649][-3.7247028 -3.2696941 -2.8158336 -2.6382656 -2.8558021 -3.202816 -3.5090652 -3.7260573 -3.7905087 -3.7410228 -3.6121922 -3.4564922 -3.3569393 -3.3500924 -3.3660922]]...]
INFO - root - 2017-12-08 01:49:02.821250: step 20510, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.590 sec/batch; 79h:50m:20s remains)
INFO - root - 2017-12-08 01:49:48.461781: step 20520, loss = 0.72, batch loss = 0.65 (7.2 examples/sec; 4.474 sec/batch; 77h:47m:52s remains)
INFO - root - 2017-12-08 01:50:33.804084: step 20530, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 4.551 sec/batch; 79h:07m:32s remains)
INFO - root - 2017-12-08 01:51:19.573672: step 20540, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.583 sec/batch; 79h:40m:19s remains)
INFO - root - 2017-12-08 01:52:05.030528: step 20550, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 4.530 sec/batch; 78h:44m:30s remains)
INFO - root - 2017-12-08 01:52:50.977338: step 20560, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 4.615 sec/batch; 80h:12m:00s remains)
INFO - root - 2017-12-08 01:53:36.489033: step 20570, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 4.558 sec/batch; 79h:12m:24s remains)
INFO - root - 2017-12-08 01:54:21.976765: step 20580, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 4.607 sec/batch; 80h:02m:53s remains)
INFO - root - 2017-12-08 01:55:07.685809: step 20590, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 4.601 sec/batch; 79h:55m:25s remains)
INFO - root - 2017-12-08 01:55:53.361142: step 20600, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 4.540 sec/batch; 78h:51m:25s remains)
2017-12-08 01:55:55.981036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5512187 -1.6032133 -1.6675794 -1.6781902 -1.6770358 -1.6759241 -1.6912048 -1.6986959 -1.6997552 -1.6995707 -1.699492 -1.7002516 -1.7002664 -1.6999819 -1.7009017][-1.3094997 -1.4364536 -1.527667 -1.5433352 -1.5499885 -1.5681555 -1.6011648 -1.6125226 -1.6086853 -1.6048908 -1.6061361 -1.6097975 -1.6097434 -1.606231 -1.6051812][-1.7051041 -1.9506488 -2.0437744 -2.0163145 -1.9860759 -2.0047507 -2.0604897 -2.0937312 -2.1065388 -2.1205847 -2.1412575 -2.1616206 -2.1702523 -2.1661053 -2.1620691][-2.6539998 -2.9846606 -3.030551 -2.9274988 -2.85321 -2.8764887 -2.9461303 -2.9825842 -3.0038748 -3.0353355 -3.0793839 -3.1309228 -3.176429 -3.2052462 -3.2250357][-3.192656 -3.4842808 -3.4229684 -3.2288067 -3.122705 -3.1675787 -3.227684 -3.2288167 -3.2425299 -3.2973676 -3.3760209 -3.4728928 -3.5734742 -3.6497023 -3.7021189][-3.179848 -3.3890958 -3.2310445 -2.9388762 -2.7691393 -2.7791967 -2.7826276 -2.7286577 -2.7651024 -2.9207273 -3.0987253 -3.2775569 -3.467011 -3.6132216 -3.7042811][-2.9388559 -3.076241 -2.876833 -2.5405891 -2.3210306 -2.2796118 -2.1999373 -2.053556 -2.1164436 -2.4240968 -2.7424195 -2.9974823 -3.2486043 -3.4820819 -3.6486335][-2.2492971 -2.2648153 -1.9929206 -1.6104598 -1.3508132 -1.2469435 -1.1250811 -1.0072112 -1.2294381 -1.7150276 -2.0644853 -2.2483444 -2.4683702 -2.7762885 -3.030663][-1.422713 -1.239665 -0.855469 -0.459759 -0.22526503 -0.12425995 -0.04632473 -0.0708518 -0.44019532 -0.94075465 -1.1271164 -1.1128345 -1.2477431 -1.584403 -1.8576944][-1.0892262 -0.79395485 -0.38567066 -0.079348564 0.020896435 0.0020623207 -0.078933239 -0.26772976 -0.57360792 -0.79510427 -0.6866715 -0.49243164 -0.54419017 -0.82501841 -1.0057116][-1.5833473 -1.374794 -1.0598352 -0.81469655 -0.72002125 -0.731745 -0.884341 -1.2037063 -1.455153 -1.4648986 -1.2219229 -0.9941442 -0.99009538 -1.1251638 -1.1302562][-1.8435731 -1.8070235 -1.6944199 -1.5926678 -1.5643747 -1.5952766 -1.7572007 -2.0825467 -2.2591975 -2.1844442 -1.9940994 -1.8693037 -1.8512077 -1.833077 -1.6875813][-1.356338 -1.4003351 -1.3995442 -1.3781633 -1.446238 -1.6026042 -1.8778718 -2.2518563 -2.4451661 -2.4016376 -2.3092964 -2.2643857 -2.2161052 -2.0999022 -1.8868055][-0.84962177 -0.95576692 -1.0057306 -0.98968911 -1.0691075 -1.2631016 -1.5581875 -1.9028859 -2.1513324 -2.2657804 -2.3410149 -2.3669744 -2.2803473 -2.0898633 -1.8205242][-0.78949523 -0.97760367 -1.0796199 -1.1063566 -1.2106149 -1.3851373 -1.6002238 -1.8071609 -2.0002952 -2.1662171 -2.3050153 -2.3485215 -2.267571 -2.092021 -1.8391092]]...]
