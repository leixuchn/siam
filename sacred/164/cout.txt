INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "164"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01-clip-zeroinit-from-scratch
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-09 05:32:16.348024: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:32:16.348058: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:32:16.348066: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:32:16.348070: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:32:16.348075: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:32:16.677661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 6.38GiB
2017-12-09 05:32:16.677699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-09 05:32:16.677706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-09 05:32:16.677714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - 2017-12-09 05:32:24.188429: step 0, loss = 0.90, batch loss = 0.69 (1.4 examples/sec; 5.860 sec/batch; 541h:15m:31s remains)
2017-12-09 05:32:24.987048: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00013018308 0.00013281005 0.00013670952 0.00014201173 0.0001455412 0.00015017054 0.00015521399 0.00016105348 0.00016436566 0.00016291584 0.00015557639 0.0001522337 0.00015099943 0.00015157016 0.00014601472][0.00013503668 0.00013743983 0.00014218042 0.0001464114 0.00014816018 0.00015843846 0.00017515456 0.00019954918 0.00021454995 0.00020767843 0.00020064805 0.00018384992 0.00016927655 0.00016119004 0.00015003877][0.00013596038 0.00013949929 0.00014464986 0.00014803182 0.0001557458 0.00018586812 0.00024041398 0.00029496016 0.000308956 0.00030038008 0.00027812511 0.0002425439 0.00020859353 0.00017720649 0.00015513611][0.0001415679 0.00014249633 0.00015146611 0.00016910559 0.0002042955 0.00026832952 0.00035610941 0.00042464913 0.00043822292 0.00041797096 0.00037498106 0.00030774446 0.00025268 0.00020445051 0.00016708682][0.00014436027 0.00014537344 0.00016479893 0.00020221947 0.00027819027 0.00038083218 0.00050596258 0.000606841 0.00062199193 0.00055720942 0.00047344586 0.00037455955 0.00029039141 0.00022593273 0.00017764379][0.0001506633 0.0001531064 0.00018008906 0.0002432582 0.00034539934 0.00048840971 0.00068194297 0.00085274113 0.00081638445 0.00067862577 0.00055242877 0.00040942713 0.00030183405 0.00022949198 0.00017394531][0.00015460615 0.00016123871 0.00019274247 0.00027122983 0.0004015688 0.00056283496 0.0008139321 0.0010531658 0.000896068 0.00071222137 0.00054713385 0.00038984176 0.00027898038 0.00021557279 0.00016252205][0.00016164679 0.00017116943 0.00020717745 0.00028345335 0.0003832654 0.00050160295 0.000668085 0.00077507849 0.00067302835 0.00055728346 0.00044290733 0.00032164948 0.00023955331 0.00019321791 0.00015782494][0.00016703023 0.00017841031 0.0002090285 0.00027108577 0.00034584251 0.00042440652 0.00048562582 0.00049295306 0.00043870843 0.00039099841 0.00033478279 0.00026248975 0.00020546917 0.00017895269 0.00015793367][0.00016349602 0.00018217701 0.00021156041 0.00025403721 0.00029593 0.00033425484 0.00034134541 0.00032148976 0.00029407724 0.00028325128 0.00025441428 0.00022203525 0.00019296333 0.00018165495 0.00015929641][0.00015953457 0.0001789784 0.00021039869 0.00023957297 0.00025835665 0.0002625756 0.0002468515 0.00023754877 0.00022075842 0.00021918393 0.00020620192 0.00019606842 0.00018507276 0.00017802681 0.00015655579][0.00016399725 0.00017683549 0.00019069837 0.00020846097 0.00021537168 0.00020933992 0.00020341262 0.00020394624 0.00019607715 0.00019528196 0.00018832003 0.00018360795 0.00017522604 0.0001702038 0.000153456][0.00016265654 0.0001670324 0.00017066706 0.00017959651 0.00018690055 0.00018380616 0.00018159959 0.00019157745 0.00018878942 0.0001897781 0.00018285504 0.00017796268 0.0001705633 0.00016488574 0.00015131099][0.00016462043 0.00016680826 0.00016763795 0.00017008717 0.00017085019 0.00017139298 0.0001718658 0.00018793377 0.00018867682 0.00018293102 0.00017618106 0.00017250671 0.00016753834 0.00016145452 0.00014871848][0.00016403284 0.00017161421 0.00016928304 0.00016669805 0.00016372038 0.0001648802 0.00017068574 0.00018873721 0.00019096417 0.0001852167 0.00017985904 0.00017502106 0.00016876067 0.00016008026 0.00014881331]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01-clip-zeroinit-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01-clip-zeroinit-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 05:32:31.898644: step 10, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.650 sec/batch; 60h:02m:19s remains)
INFO - root - 2017-12-09 05:32:38.193538: step 20, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.634 sec/batch; 58h:33m:38s remains)
INFO - root - 2017-12-09 05:32:44.546545: step 30, loss = 0.90, batch loss = 0.69 (13.2 examples/sec; 0.604 sec/batch; 55h:49m:36s remains)
INFO - root - 2017-12-09 05:32:50.844414: step 40, loss = 0.91, batch loss = 0.69 (12.7 examples/sec; 0.629 sec/batch; 58h:06m:23s remains)
INFO - root - 2017-12-09 05:32:57.196819: step 50, loss = 0.91, batch loss = 0.69 (12.5 examples/sec; 0.642 sec/batch; 59h:15m:36s remains)
INFO - root - 2017-12-09 05:33:03.347279: step 60, loss = 0.91, batch loss = 0.69 (12.9 examples/sec; 0.619 sec/batch; 57h:11m:44s remains)
INFO - root - 2017-12-09 05:33:09.326936: step 70, loss = 0.91, batch loss = 0.69 (13.0 examples/sec; 0.613 sec/batch; 56h:39m:03s remains)
INFO - root - 2017-12-09 05:33:15.486939: step 80, loss = 0.92, batch loss = 0.70 (13.2 examples/sec; 0.608 sec/batch; 56h:10m:27s remains)
INFO - root - 2017-12-09 05:33:21.756261: step 90, loss = 1.02, batch loss = 0.80 (12.5 examples/sec; 0.642 sec/batch; 59h:14m:03s remains)
INFO - root - 2017-12-09 05:33:28.071395: step 100, loss = 1.18, batch loss = 0.95 (12.6 examples/sec; 0.637 sec/batch; 58h:47m:32s remains)
2017-12-09 05:33:28.716879: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.085623741 0.12738539 0.17202266 0.2085344 0.22734311 0.22454017 0.20453525 0.17387754 0.14201568 0.11806765 0.10913886 0.11911256 0.14205764 0.16715854 0.18548766][0.12979561 0.1807953 0.23071992 0.2665264 0.27735278 0.26079816 0.22382532 0.17677109 0.13156338 0.098307379 0.082754292 0.087806918 0.10727923 0.1302224 0.15187062][0.18420321 0.24073726 0.28897753 0.31588751 0.31178075 0.27771303 0.22310343 0.162306 0.1067303 0.066801138 0.045638442 0.044328541 0.057121515 0.074625544 0.094188444][0.24353293 0.30361968 0.34916815 0.36559972 0.34620008 0.29648906 0.22761944 0.15656713 0.094725996 0.050988857 0.027515085 0.019609269 0.024556216 0.033642095 0.047691543][0.305791 0.35963371 0.39725196 0.40616843 0.37909821 0.32361367 0.2488295 0.17288031 0.10571402 0.05510297 0.025588106 0.012480325 0.010086221 0.013026406 0.018539041][0.37742284 0.41742229 0.4413664 0.44194639 0.41154253 0.35797477 0.28660434 0.21232165 0.14157431 0.081972495 0.040151995 0.016149171 0.0064338772 0.0051100133 0.0089261485][0.44988933 0.47208232 0.47723094 0.46731764 0.43543676 0.38717619 0.3246437 0.25682274 0.18746227 0.12239733 0.070639133 0.035790835 0.016438577 0.0064841695 0.0036363688][0.52929908 0.52967757 0.50807172 0.47937217 0.441651 0.39685729 0.34420034 0.28612497 0.22463925 0.16197525 0.10740299 0.066139385 0.039272945 0.020967599 0.011562348][0.6082837 0.58424658 0.53171051 0.47539428 0.42072332 0.37177613 0.3279613 0.28489354 0.23836872 0.18883996 0.14282346 0.10353153 0.07278084 0.04814589 0.031639621][0.67657179 0.62786597 0.54353839 0.45369717 0.37320438 0.31016979 0.26781693 0.24025972 0.21702826 0.19303735 0.16880085 0.14375705 0.11753637 0.088772111 0.063627005][0.71940774 0.65143985 0.53943974 0.42016631 0.31429288 0.23419835 0.18764322 0.16881262 0.16585271 0.1695154 0.17336406 0.17103113 0.15806761 0.13298041 0.10596978][0.71357363 0.63412511 0.50909781 0.37617433 0.25728172 0.16685833 0.11680456 0.101305 0.1096198 0.13279879 0.16005419 0.17984763 0.18325981 0.16944478 0.15081459][0.66475928 0.58998865 0.47221166 0.34433648 0.2271007 0.13807799 0.087222993 0.071746878 0.084938571 0.11554583 0.1527319 0.18288131 0.19531642 0.19270122 0.19225208][0.565427 0.51135612 0.42447081 0.3282271 0.23526931 0.15893675 0.11064425 0.090879567 0.098776035 0.12494718 0.15882085 0.18695086 0.19753283 0.2019202 0.22443767][0.43348044 0.40816656 0.36434042 0.31259534 0.25663263 0.20465887 0.16499142 0.14192824 0.1396246 0.15243706 0.17082509 0.18431444 0.18688607 0.19281286 0.23567444]]...]
INFO - root - 2017-12-09 05:33:34.996178: step 110, loss = 1.47, batch loss = 1.24 (12.7 examples/sec; 0.629 sec/batch; 58h:03m:24s remains)
INFO - root - 2017-12-09 05:33:41.167211: step 120, loss = 11.40, batch loss = 11.17 (12.8 examples/sec; 0.626 sec/batch; 57h:47m:52s remains)
INFO - root - 2017-12-09 05:33:47.487317: step 130, loss = 0.96, batch loss = 0.73 (12.2 examples/sec; 0.654 sec/batch; 60h:21m:34s remains)
INFO - root - 2017-12-09 05:33:53.760789: step 140, loss = 0.91, batch loss = 0.68 (12.8 examples/sec; 0.627 sec/batch; 57h:55m:04s remains)
INFO - root - 2017-12-09 05:33:59.980493: step 150, loss = 0.93, batch loss = 0.70 (13.1 examples/sec; 0.610 sec/batch; 56h:19m:14s remains)
INFO - root - 2017-12-09 05:34:06.116319: step 160, loss = 0.93, batch loss = 0.69 (12.7 examples/sec; 0.630 sec/batch; 58h:10m:49s remains)
INFO - root - 2017-12-09 05:34:12.157855: step 170, loss = 0.93, batch loss = 0.69 (13.1 examples/sec; 0.611 sec/batch; 56h:24m:43s remains)
INFO - root - 2017-12-09 05:34:18.479636: step 180, loss = 0.93, batch loss = 0.69 (12.7 examples/sec; 0.630 sec/batch; 58h:09m:40s remains)
INFO - root - 2017-12-09 05:34:24.828683: step 190, loss = 0.92, batch loss = 0.68 (12.5 examples/sec; 0.642 sec/batch; 59h:13m:09s remains)
INFO - root - 2017-12-09 05:34:31.160054: step 200, loss = 0.93, batch loss = 0.69 (12.6 examples/sec; 0.633 sec/batch; 58h:25m:03s remains)
2017-12-09 05:34:31.833709: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19326484 0.19652483 0.19833061 0.20036042 0.20364267 0.20887402 0.21395582 0.21899489 0.21843818 0.2133106 0.21333477 0.21225199 0.20865324 0.20303911 0.20096603][0.19235837 0.20018429 0.20684391 0.2127153 0.22126171 0.23103809 0.23948976 0.24785647 0.25170502 0.24904096 0.24938351 0.24191824 0.23278514 0.22215691 0.21122506][0.1745981 0.18325394 0.19426155 0.208944 0.22687733 0.2486746 0.26673719 0.27925548 0.28700417 0.28911746 0.29094851 0.2834385 0.27333429 0.26220271 0.24935862][0.16232887 0.16947559 0.18679231 0.21162847 0.24112335 0.27534232 0.3074747 0.32906997 0.33924091 0.33774465 0.33400241 0.32189608 0.30437008 0.28965506 0.27272409][0.15379205 0.16397908 0.18283036 0.21748459 0.261813 0.30792925 0.3524521 0.38714552 0.40527511 0.40665537 0.39545017 0.37049985 0.33960885 0.31711909 0.29559335][0.14851993 0.16293114 0.19372591 0.23977643 0.29241645 0.34622762 0.39801925 0.4354932 0.45431325 0.45908678 0.44680309 0.41708767 0.37982082 0.34388211 0.31150511][0.14916554 0.16730627 0.20447648 0.25225502 0.31169954 0.37148875 0.42487663 0.46738082 0.49051946 0.49199522 0.47640079 0.44675967 0.40773207 0.36777249 0.33472452][0.15749249 0.17762467 0.21638584 0.27296504 0.33841786 0.39156199 0.43816406 0.47819227 0.49876654 0.49974674 0.48443311 0.45408365 0.416819 0.38172796 0.35010031][0.18420699 0.20264584 0.23384658 0.27927065 0.33289912 0.38881746 0.43782187 0.46856278 0.48664331 0.49199063 0.48047066 0.45187423 0.41483203 0.381881 0.353152][0.20998004 0.22560596 0.24868435 0.28386703 0.32321382 0.36302516 0.40049124 0.43006632 0.44721249 0.45303831 0.44643104 0.43060383 0.40974554 0.387234 0.36221403][0.22963834 0.24148852 0.25788692 0.27982241 0.3049202 0.32983637 0.35566866 0.38239244 0.40695727 0.41322702 0.40940034 0.4022629 0.38473096 0.37264818 0.36228806][0.23400685 0.24076769 0.24964291 0.26540917 0.28470069 0.29946867 0.31476897 0.33484995 0.3524816 0.36429682 0.3689681 0.36766347 0.36369407 0.35725555 0.34753582][0.23293293 0.236693 0.24008346 0.24669939 0.25710434 0.27056146 0.28302011 0.29464993 0.30833626 0.32544476 0.34064046 0.34624717 0.34464785 0.34448045 0.34492606][0.22189525 0.22513431 0.22845903 0.23231345 0.24019277 0.24889046 0.2619341 0.27206215 0.2807557 0.29121786 0.30251694 0.31873459 0.33127913 0.33537886 0.34002072][0.210863 0.21418831 0.21861994 0.22365099 0.22920695 0.23635513 0.24527124 0.25555611 0.26911208 0.2787706 0.28805476 0.29619488 0.30431274 0.31801003 0.32921988]]...]
INFO - root - 2017-12-09 05:34:38.068991: step 210, loss = 0.93, batch loss = 0.69 (12.6 examples/sec; 0.633 sec/batch; 58h:28m:19s remains)
INFO - root - 2017-12-09 05:34:44.365982: step 220, loss = 0.93, batch loss = 0.68 (12.4 examples/sec; 0.645 sec/batch; 59h:31m:00s remains)
INFO - root - 2017-12-09 05:34:50.682879: step 230, loss = 0.94, batch loss = 0.69 (12.5 examples/sec; 0.638 sec/batch; 58h:52m:46s remains)
INFO - root - 2017-12-09 05:34:56.975963: step 240, loss = 0.93, batch loss = 0.68 (12.6 examples/sec; 0.634 sec/batch; 58h:31m:33s remains)
INFO - root - 2017-12-09 05:35:03.306858: step 250, loss = 0.92, batch loss = 0.67 (12.9 examples/sec; 0.621 sec/batch; 57h:18m:58s remains)
INFO - root - 2017-12-09 05:35:09.432421: step 260, loss = 0.92, batch loss = 0.67 (13.1 examples/sec; 0.609 sec/batch; 56h:13m:25s remains)
INFO - root - 2017-12-09 05:35:15.349308: step 270, loss = 0.93, batch loss = 0.67 (13.0 examples/sec; 0.614 sec/batch; 56h:40m:33s remains)
INFO - root - 2017-12-09 05:35:21.611069: step 280, loss = 0.92, batch loss = 0.67 (12.5 examples/sec; 0.641 sec/batch; 59h:07m:45s remains)
INFO - root - 2017-12-09 05:35:27.958018: step 290, loss = 0.93, batch loss = 0.67 (13.2 examples/sec; 0.608 sec/batch; 56h:06m:51s remains)
INFO - root - 2017-12-09 05:35:34.258841: step 300, loss = 0.94, batch loss = 0.68 (12.5 examples/sec; 0.639 sec/batch; 58h:59m:52s remains)
2017-12-09 05:35:34.855846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.033938866 -0.033877615 -0.033859491 -0.03386708 -0.033907138 -0.033948503 -0.03397676 -0.033980723 -0.033966597 -0.033940386 -0.033907212 -0.033881783 -0.033869218 -0.033861239 -0.03385777][-0.0339196 -0.033858314 -0.033836111 -0.033837195 -0.033864461 -0.033892438 -0.033908933 -0.033904441 -0.033886082 -0.033862129 -0.033833586 -0.033811439 -0.0338009 -0.033792339 -0.033785846][-0.034006175 -0.033991233 -0.033995733 -0.034002114 -0.03401408 -0.034013644 -0.034000877 -0.03397486 -0.0339477 -0.033923544 -0.033895757 -0.033870019 -0.033849012 -0.033827983 -0.033806179][-0.034521379 -0.034697223 -0.034826256 -0.0348646 -0.034817632 -0.034700926 -0.034567926 -0.034456369 -0.034390066 -0.034352258 -0.034305014 -0.034236558 -0.034144357 -0.034038041 -0.033930205][-0.035986319 -0.036669865 -0.037156675 -0.037337966 -0.037200488 -0.0368436 -0.036450647 -0.036147092 -0.03597502 -0.035862412 -0.035704546 -0.035450418 -0.035105292 -0.034717515 -0.034341238][-0.038719345 -0.040362597 -0.041592147 -0.04214498 -0.041965388 -0.041295916 -0.040514875 -0.039880961 -0.03946707 -0.039143793 -0.038682815 -0.037965328 -0.037043728 -0.036072873 -0.035172727][-0.042090237 -0.044978604 -0.047268674 -0.048502944 -0.048556451 -0.047758371 -0.046677183 -0.045734134 -0.044991374 -0.044233888 -0.043143466 -0.041632418 -0.039816428 -0.037954208 -0.036292102][-0.0448045 -0.048789047 -0.05206402 -0.054057974 -0.054601643 -0.054001927 -0.05291348 -0.051842876 -0.050855055 -0.049642194 -0.04780839 -0.045348115 -0.042539142 -0.039808333 -0.037464328][-0.0460628 -0.050652988 -0.054580517 -0.057163559 -0.058175161 -0.057932913 -0.057061046 -0.05602894 -0.054884486 -0.053337067 -0.051014051 -0.047918756 -0.044461846 -0.041163493 -0.038389295][-0.045891587 -0.050552242 -0.054695942 -0.057609335 -0.05901029 -0.059106331 -0.058535028 -0.057672918 -0.056516323 -0.054836616 -0.052348033 -0.049104437 -0.045475803 -0.041964561 -0.039010875][-0.044497568 -0.04872172 -0.052651305 -0.055650935 -0.057347693 -0.057827536 -0.057623096 -0.057060957 -0.056024946 -0.054366026 -0.051943488 -0.048857234 -0.045414988 -0.042034142 -0.039180689][-0.041825168 -0.0451362 -0.048407305 -0.05114609 -0.052998155 -0.053912602 -0.054206774 -0.054046378 -0.053276725 -0.051828098 -0.049686294 -0.047001719 -0.044052411 -0.041190173 -0.038778633][-0.038530495 -0.040568653 -0.042752288 -0.044822663 -0.046481725 -0.047599215 -0.048279807 -0.048509583 -0.048149474 -0.047195539 -0.045720667 -0.043841284 -0.041789316 -0.039801642 -0.038103782][-0.035890449 -0.03678485 -0.037898976 -0.039118942 -0.040234234 -0.041161235 -0.041902635 -0.04233877 -0.042309843 -0.041864861 -0.041179277 -0.040256575 -0.039178062 -0.038061477 -0.037081685][-0.034644425 -0.034889907 -0.035301141 -0.035880581 -0.036501832 -0.037080936 -0.037642412 -0.0380796 -0.038265698 -0.038224235 -0.038100451 -0.037886634 -0.037502371 -0.036985591 -0.036467724]]...]
INFO - root - 2017-12-09 05:35:41.160636: step 310, loss = 0.93, batch loss = 0.66 (12.8 examples/sec; 0.623 sec/batch; 57h:29m:44s remains)
INFO - root - 2017-12-09 05:35:47.468047: step 320, loss = 0.95, batch loss = 0.68 (12.8 examples/sec; 0.623 sec/batch; 57h:30m:49s remains)
INFO - root - 2017-12-09 05:35:53.778186: step 330, loss = 0.94, batch loss = 0.67 (12.9 examples/sec; 0.619 sec/batch; 57h:06m:05s remains)
INFO - root - 2017-12-09 05:36:00.081969: step 340, loss = 0.93, batch loss = 0.66 (12.7 examples/sec; 0.631 sec/batch; 58h:15m:43s remains)
INFO - root - 2017-12-09 05:36:06.449735: step 350, loss = 0.95, batch loss = 0.67 (12.4 examples/sec; 0.646 sec/batch; 59h:37m:52s remains)
INFO - root - 2017-12-09 05:36:12.684211: step 360, loss = 0.92, batch loss = 0.64 (12.4 examples/sec; 0.643 sec/batch; 59h:18m:48s remains)
INFO - root - 2017-12-09 05:36:18.801166: step 370, loss = 0.93, batch loss = 0.65 (12.9 examples/sec; 0.622 sec/batch; 57h:24m:19s remains)
INFO - root - 2017-12-09 05:36:24.927479: step 380, loss = 0.89, batch loss = 0.61 (13.1 examples/sec; 0.612 sec/batch; 56h:26m:14s remains)
INFO - root - 2017-12-09 05:36:31.101856: step 390, loss = 0.96, batch loss = 0.67 (12.8 examples/sec; 0.625 sec/batch; 57h:37m:11s remains)
INFO - root - 2017-12-09 05:36:37.416575: step 400, loss = 0.96, batch loss = 0.67 (12.6 examples/sec; 0.635 sec/batch; 58h:35m:42s remains)
2017-12-09 05:36:38.117696: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.067259453 -0.067062154 -0.066995442 -0.066936523 -0.066902839 -0.066860236 -0.066856734 -0.066824019 -0.066804312 -0.066794559 -0.066762954 -0.06674888 -0.0667215 -0.066716231 -0.066722825][-0.067158617 -0.066946626 -0.066861838 -0.0667865 -0.066735357 -0.066694438 -0.066671357 -0.066651128 -0.066623561 -0.066612534 -0.066574827 -0.066556029 -0.066526636 -0.066509955 -0.066513866][-0.067177638 -0.066962451 -0.066867232 -0.066786855 -0.066732042 -0.0666961 -0.066674009 -0.066649012 -0.066620171 -0.066579983 -0.066525973 -0.066495046 -0.066454753 -0.066436388 -0.066455148][-0.067240909 -0.067035258 -0.066950336 -0.0668936 -0.066872254 -0.066865981 -0.066837683 -0.066773973 -0.066531479 -0.066577137 -0.066474743 -0.066455141 -0.06642089 -0.066394411 -0.066431776][-0.067298733 -0.067098476 -0.067038655 -0.067136712 -0.067723647 -0.067390949 -0.067470782 -0.067152232 -0.066513494 -0.066713132 -0.066572145 -0.066470653 -0.06639947 -0.066361234 -0.06642551][-0.067361705 -0.067183308 -0.067180783 -0.06743589 -0.068030342 -0.067845106 -0.067565106 -0.067428052 -0.066774465 -0.067260534 -0.06691438 -0.066703163 -0.066509537 -0.066423334 -0.066478141][-0.06746646 -0.067245752 -0.067301564 -0.068092354 -0.06853123 -0.068039358 -0.0687986 -0.06802965 -0.067170776 -0.067751385 -0.067141108 -0.066807784 -0.066682726 -0.066529542 -0.066548124][-0.067478433 -0.067522392 -0.067591265 -0.068242915 -0.069510072 -0.069199942 -0.069179244 -0.069107816 -0.067123838 -0.06793648 -0.067701645 -0.066927053 -0.066819616 -0.066652246 -0.0666045][-0.067504041 -0.067435496 -0.067693219 -0.067806542 -0.069139443 -0.069330439 -0.068873905 -0.068821505 -0.067962587 -0.068218648 -0.068306759 -0.067275509 -0.066891067 -0.066742823 -0.066637181][-0.0674886 -0.067232527 -0.067471087 -0.067908235 -0.068624146 -0.068709187 -0.068314411 -0.06884896 -0.068500176 -0.068858616 -0.068892814 -0.06760551 -0.067029342 -0.066806689 -0.066646159][-0.067499921 -0.067438163 -0.067414194 -0.067721315 -0.068295658 -0.068101615 -0.0679916 -0.0684768 -0.068675712 -0.069326036 -0.069605976 -0.068084091 -0.0671745 -0.0668772 -0.06665504][-0.067470334 -0.06734737 -0.067456208 -0.0675349 -0.068046369 -0.0682515 -0.067970358 -0.068084329 -0.068238072 -0.069058977 -0.069804281 -0.0685291 -0.067261621 -0.066882364 -0.066651091][-0.067526586 -0.067333259 -0.067387618 -0.067479089 -0.067693345 -0.067776822 -0.067772627 -0.067786619 -0.067901239 -0.068412483 -0.068908513 -0.068296373 -0.067347273 -0.0668838 -0.066653386][-0.067610495 -0.06740053 -0.067371756 -0.067389876 -0.067416251 -0.067406587 -0.067405112 -0.067475088 -0.067470565 -0.067664146 -0.067959949 -0.067725644 -0.06722983 -0.066866554 -0.06666822][-0.067686878 -0.067478225 -0.067411385 -0.067361392 -0.067308813 -0.0672371 -0.067194164 -0.067238845 -0.06728854 -0.067337677 -0.067339435 -0.067212388 -0.067018822 -0.066834979 -0.066687107]]...]
INFO - root - 2017-12-09 05:36:44.357837: step 410, loss = 0.97, batch loss = 0.68 (12.6 examples/sec; 0.635 sec/batch; 58h:33m:36s remains)
INFO - root - 2017-12-09 05:36:50.712023: step 420, loss = 0.97, batch loss = 0.67 (12.5 examples/sec; 0.641 sec/batch; 59h:09m:28s remains)
INFO - root - 2017-12-09 05:36:57.037410: step 430, loss = 0.98, batch loss = 0.68 (12.4 examples/sec; 0.645 sec/batch; 59h:28m:35s remains)
INFO - root - 2017-12-09 05:37:03.313784: step 440, loss = 0.95, batch loss = 0.65 (12.7 examples/sec; 0.629 sec/batch; 58h:02m:19s remains)
INFO - root - 2017-12-09 05:37:09.458525: step 450, loss = 0.98, batch loss = 0.68 (13.2 examples/sec; 0.606 sec/batch; 55h:53m:34s remains)
INFO - root - 2017-12-09 05:37:15.560832: step 460, loss = 0.93, batch loss = 0.63 (13.4 examples/sec; 0.596 sec/batch; 54h:56m:46s remains)
INFO - root - 2017-12-09 05:37:21.611351: step 470, loss = 0.97, batch loss = 0.66 (12.7 examples/sec; 0.629 sec/batch; 57h:58m:09s remains)
INFO - root - 2017-12-09 05:37:27.683256: step 480, loss = 1.00, batch loss = 0.70 (13.2 examples/sec; 0.608 sec/batch; 56h:05m:58s remains)
INFO - root - 2017-12-09 05:37:33.878526: step 490, loss = 0.99, batch loss = 0.68 (12.7 examples/sec; 0.630 sec/batch; 58h:07m:58s remains)
INFO - root - 2017-12-09 05:37:40.165583: step 500, loss = 0.98, batch loss = 0.67 (12.8 examples/sec; 0.627 sec/batch; 57h:47m:23s remains)
2017-12-09 05:37:40.773655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.068178445 -0.067877091 -0.067849211 -0.06782119 -0.067797348 -0.06774503 -0.067673847 -0.06758938 -0.06747508 -0.06730929 -0.0671379 -0.067009956 -0.0668777 -0.06674923 -0.0666439][-0.067971595 -0.067677669 -0.067626558 -0.067587614 -0.067548372 -0.067493431 -0.067403056 -0.067295671 -0.0671612 -0.066994317 -0.066828743 -0.066695742 -0.066578738 -0.0664656 -0.066379607][-0.067891337 -0.067620307 -0.067552179 -0.067512505 -0.067484349 -0.067429051 -0.067330256 -0.06722147 -0.067087896 -0.066934824 -0.0667877 -0.0666674 -0.066563122 -0.066458583 -0.066378735][-0.067762636 -0.067484587 -0.067401484 -0.067369267 -0.067361228 -0.067326359 -0.067247674 -0.067158058 -0.067047477 -0.066924974 -0.066800423 -0.06669163 -0.066588968 -0.066481531 -0.066392355][-0.067567751 -0.06726063 -0.067161508 -0.067127183 -0.067133993 -0.067126989 -0.067084625 -0.067038871 -0.066973984 -0.066900983 -0.066808753 -0.066713959 -0.066609547 -0.066494882 -0.066391446][-0.067365557 -0.06700474 -0.066887505 -0.066841304 -0.066847689 -0.066871271 -0.066879459 -0.066885889 -0.066879921 -0.066854119 -0.06679146 -0.066700384 -0.066582106 -0.066451117 -0.066327296][-0.067140676 -0.066745959 -0.066619843 -0.066565618 -0.066569492 -0.066608831 -0.06665004 -0.066695534 -0.066729695 -0.066738874 -0.066696577 -0.06660898 -0.0664891 -0.066348016 -0.066212445][-0.066941082 -0.066509135 -0.06638249 -0.066309094 -0.066317253 -0.066355169 -0.066412538 -0.066472217 -0.066523068 -0.066556044 -0.066525921 -0.066449709 -0.066336893 -0.066206194 -0.066080138][-0.066779435 -0.066300131 -0.066166423 -0.0660669 -0.066051558 -0.066071436 -0.066119894 -0.066175267 -0.066227019 -0.066268966 -0.066245839 -0.066184185 -0.066089071 -0.0659751 -0.065865628][-0.066667378 -0.06615527 -0.0660136 -0.065878481 -0.065828219 -0.065816432 -0.065826617 -0.065855712 -0.065885961 -0.06592989 -0.065918922 -0.065870866 -0.065808147 -0.065726444 -0.065648511][-0.066617981 -0.066105783 -0.065971628 -0.065811545 -0.065731294 -0.065673769 -0.065631986 -0.065604493 -0.065585881 -0.065607548 -0.065593489 -0.065563604 -0.065531455 -0.06548927 -0.065449707][-0.066607326 -0.066125393 -0.066009268 -0.065842979 -0.065753214 -0.065651484 -0.065552607 -0.065467089 -0.065394141 -0.065383695 -0.065355867 -0.0653405 -0.065332547 -0.065322667 -0.065313973][-0.066698536 -0.06623745 -0.0661322 -0.065964788 -0.065856844 -0.065728933 -0.065586746 -0.0654557 -0.065344043 -0.065304033 -0.065260649 -0.065242209 -0.065245867 -0.0652498 -0.06525448][-0.066863552 -0.066429242 -0.066326477 -0.066165589 -0.06603428 -0.065892041 -0.065722361 -0.065569267 -0.065436393 -0.065369852 -0.06531319 -0.065283462 -0.065286592 -0.06528952 -0.065294534][-0.067094944 -0.066678628 -0.066579923 -0.066434249 -0.066301018 -0.066153362 -0.065977573 -0.065808088 -0.06566336 -0.065577835 -0.065506421 -0.065454759 -0.065445915 -0.065444559 -0.065447971]]...]
INFO - root - 2017-12-09 05:37:47.012507: step 510, loss = 1.02, batch loss = 0.70 (12.6 examples/sec; 0.637 sec/batch; 58h:45m:48s remains)
INFO - root - 2017-12-09 05:37:53.257696: step 520, loss = 0.98, batch loss = 0.66 (12.8 examples/sec; 0.627 sec/batch; 57h:47m:31s remains)
INFO - root - 2017-12-09 05:37:59.504132: step 530, loss = 0.95, batch loss = 0.63 (12.6 examples/sec; 0.634 sec/batch; 58h:27m:52s remains)
INFO - root - 2017-12-09 05:38:05.738783: step 540, loss = 1.41, batch loss = 1.09 (12.6 examples/sec; 0.637 sec/batch; 58h:44m:16s remains)
INFO - root - 2017-12-09 05:38:11.934472: step 550, loss = 0.99, batch loss = 0.67 (13.4 examples/sec; 0.595 sec/batch; 54h:51m:08s remains)
INFO - root - 2017-12-09 05:38:18.200289: step 560, loss = 1.02, batch loss = 0.69 (12.3 examples/sec; 0.651 sec/batch; 60h:00m:20s remains)
INFO - root - 2017-12-09 05:38:24.050789: step 570, loss = 1.02, batch loss = 0.69 (13.1 examples/sec; 0.612 sec/batch; 56h:25m:25s remains)
INFO - root - 2017-12-09 05:38:30.315237: step 580, loss = 1.03, batch loss = 0.70 (12.4 examples/sec; 0.644 sec/batch; 59h:20m:48s remains)
INFO - root - 2017-12-09 05:38:36.929897: step 590, loss = 1.03, batch loss = 0.70 (11.6 examples/sec; 0.688 sec/batch; 63h:25m:37s remains)
INFO - root - 2017-12-09 05:38:43.417790: step 600, loss = 1.02, batch loss = 0.69 (13.0 examples/sec; 0.615 sec/batch; 56h:44m:42s remains)
2017-12-09 05:38:44.078632: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31620646 0.27964306 0.22175409 0.18510915 0.12315853 0.10319377 0.087621957 0.019282579 -0.032516729 -0.091166221 -0.12776163 -0.14222404 -0.13037811 -0.11195418 -0.088055804][0.12900566 0.13624631 0.14648323 0.16026925 0.14439818 0.13562529 0.088433936 0.053804278 0.019215077 0.0036343783 -0.015740417 -0.051922038 -0.075704619 -0.09474197 -0.093246773][-0.028151207 -0.033096161 -0.051683325 -0.050193604 -0.025081463 -0.015439972 -0.0022367388 -0.021039829 -0.091858529 -0.14779904 -0.14109257 -0.15061446 -0.12351739 -0.11218151 -0.088091217][-0.0715685 -0.048217855 -0.012498967 -0.0054335 -0.0074504241 0.020328172 -0.00032620877 -0.047780868 -0.074034914 -0.080725417 -0.083737165 -0.11633734 -0.128554 -0.11408183 -0.094275527][-0.010829717 -0.032873068 0.022841081 0.022373311 -0.023544103 -0.063960232 -0.07084471 -0.064861037 -0.037005231 -0.05948377 -0.0637437 -0.073318772 -0.064856142 -0.06950663 -0.080546789][-0.073461562 -0.060000312 -0.033379339 -0.015480012 0.01189281 -0.021181896 -0.025564924 -0.045950443 -0.067134216 -0.076752819 -0.064469039 -0.040303718 -0.033212021 -0.048111506 -0.064155951][0.052340493 -0.034367979 -0.10024933 -0.082311824 -0.076689087 -0.039205007 -0.021706983 0.0029746741 0.016241737 0.01691547 0.018571101 -0.0069985613 -0.014360629 -0.02964513 -0.040565707][-0.0042145327 0.023019232 0.049350083 0.032887235 -0.0062454939 -0.045549512 -0.086586922 -0.0844929 -0.064915076 -0.055053253 -0.041596349 -0.044122107 -0.039525304 -0.043800991 -0.052675195][0.01969409 0.03674531 0.034738928 0.030597068 0.060298398 0.040903926 0.0027265176 -0.036544241 -0.064089812 -0.066530153 -0.059715535 -0.055390809 -0.055133976 -0.057052687 -0.063033417][0.050468951 0.06959258 0.055455148 0.022502348 -0.036862075 -0.067155495 -0.074581429 -0.085220493 -0.084345371 -0.084553927 -0.080477372 -0.074482813 -0.074578688 -0.075505674 -0.075206019][0.040193334 0.057666853 0.069757462 0.049357846 0.016571835 -0.028511032 -0.0601315 -0.080161817 -0.089367777 -0.093276843 -0.08960972 -0.083469719 -0.08000648 -0.077939339 -0.076363415][-0.03622226 -0.018280648 0.011774167 0.042165518 0.037452787 0.010081969 -0.0085608512 -0.019574702 -0.041558385 -0.066901565 -0.081325404 -0.081846461 -0.080762796 -0.079957068 -0.077689283][-0.047090203 -0.054926127 -0.054149993 -0.025809713 -0.0022545382 0.0097458363 -0.003725037 -0.017028943 -0.024998948 -0.0445169 -0.064062029 -0.06954959 -0.07082361 -0.072792619 -0.09013354][-0.11240596 -0.086574458 -0.052134994 -0.036257315 -0.022472925 -0.014266171 -0.02172903 -0.025688902 -0.038920153 -0.046675917 -0.059048574 -0.07304655 -0.077881828 -0.077868842 -0.08263][-0.079093583 -0.087609611 -0.099076666 -0.10352916 -0.080761179 -0.054170296 -0.0365325 -0.035973385 -0.048505023 -0.058542646 -0.060772117 -0.065273829 -0.073254541 -0.078070588 -0.084578648]]...]
INFO - root - 2017-12-09 05:38:50.562404: step 610, loss = 1.02, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:51m:39s remains)
INFO - root - 2017-12-09 05:38:57.044039: step 620, loss = 1.01, batch loss = 0.68 (12.6 examples/sec; 0.634 sec/batch; 58h:25m:50s remains)
INFO - root - 2017-12-09 05:39:03.441440: step 630, loss = 1.01, batch loss = 0.68 (12.7 examples/sec; 0.631 sec/batch; 58h:12m:07s remains)
INFO - root - 2017-12-09 05:39:09.805965: step 640, loss = 1.02, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 60h:04m:58s remains)
INFO - root - 2017-12-09 05:39:16.180291: step 650, loss = 1.01, batch loss = 0.67 (12.5 examples/sec; 0.640 sec/batch; 58h:57m:26s remains)
INFO - root - 2017-12-09 05:39:22.436676: step 660, loss = 1.02, batch loss = 0.68 (12.6 examples/sec; 0.634 sec/batch; 58h:25m:54s remains)
INFO - root - 2017-12-09 05:39:28.446552: step 670, loss = 1.04, batch loss = 0.69 (17.1 examples/sec; 0.469 sec/batch; 43h:14m:09s remains)
INFO - root - 2017-12-09 05:39:34.706184: step 680, loss = 1.07, batch loss = 0.73 (12.8 examples/sec; 0.625 sec/batch; 57h:33m:44s remains)
INFO - root - 2017-12-09 05:39:40.969829: step 690, loss = 1.03, batch loss = 0.69 (12.7 examples/sec; 0.628 sec/batch; 57h:51m:59s remains)
INFO - root - 2017-12-09 05:39:47.328074: step 700, loss = 1.01, batch loss = 0.66 (12.9 examples/sec; 0.621 sec/batch; 57h:12m:09s remains)
2017-12-09 05:39:47.995927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.094423719 -0.09444233 -0.094499856 -0.094587386 -0.094694108 -0.094800778 -0.094882123 -0.0948989 -0.094814934 -0.094633274 -0.094398364 -0.094160192 -0.09399651 -0.093921937 -0.093905941][-0.093324058 -0.093245 -0.093229227 -0.093229681 -0.093260027 -0.093324259 -0.0934058 -0.093455486 -0.093412064 -0.093249165 -0.092992768 -0.092708074 -0.092474028 -0.092338152 -0.092293747][-0.093328327 -0.09299881 -0.092800274 -0.092597142 -0.092447139 -0.092396915 -0.092448466 -0.092545286 -0.092596382 -0.092530929 -0.092333652 -0.092049584 -0.091766417 -0.091559239 -0.091459818][-0.093672641 -0.093071975 -0.092675723 -0.092231773 -0.09184204 -0.091604814 -0.091560118 -0.091665417 -0.09181267 -0.091887265 -0.091816723 -0.091600917 -0.091308668 -0.091030635 -0.090838671][-0.094035447 -0.093084335 -0.092485227 -0.091794655 -0.091148779 -0.090693951 -0.090510949 -0.0905837 -0.090810791 -0.091048218 -0.091162041 -0.091088235 -0.0908549 -0.090549558 -0.090274505][-0.094451904 -0.093085095 -0.092298262 -0.091383509 -0.090496778 -0.089821443 -0.089479566 -0.089494362 -0.089785054 -0.090190291 -0.090520218 -0.090637505 -0.090511546 -0.090213813 -0.089873254][-0.094988339 -0.093202129 -0.092280783 -0.091207281 -0.090141751 -0.089287877 -0.088801116 -0.088745944 -0.089072771 -0.0896161 -0.090140477 -0.0904413 -0.090429619 -0.090154931 -0.089766964][-0.095554769 -0.093437344 -0.092466608 -0.091336943 -0.090195857 -0.08924713 -0.088661596 -0.088536181 -0.0888555 -0.089464031 -0.090102866 -0.090521783 -0.090590104 -0.090338975 -0.089929767][-0.096156783 -0.093864895 -0.092938937 -0.091867343 -0.090772033 -0.089835666 -0.089223176 -0.089044534 -0.089310378 -0.089887381 -0.090521418 -0.090951726 -0.0910323 -0.090786271 -0.090376392][-0.096911661 -0.094529949 -0.0937226 -0.09279602 -0.091846071 -0.091022216 -0.090461016 -0.090259552 -0.090438768 -0.090890348 -0.091393284 -0.091716379 -0.09172909 -0.091459274 -0.091060892][-0.097777456 -0.095430166 -0.094798036 -0.094070733 -0.093322605 -0.0926673 -0.092201896 -0.091991916 -0.092051581 -0.092302121 -0.092577592 -0.0927055 -0.09259475 -0.092284285 -0.091911644][-0.098634258 -0.096477583 -0.096069239 -0.095568836 -0.095029846 -0.09453591 -0.094150543 -0.0939019 -0.093799904 -0.093805231 -0.093818471 -0.093734518 -0.093502536 -0.093169741 -0.092847109][-0.099404536 -0.097448625 -0.097288407 -0.09702336 -0.096678011 -0.096312612 -0.095969185 -0.095648393 -0.095364481 -0.095127977 -0.094911359 -0.09466587 -0.094366059 -0.09405414 -0.093807071][-0.099851668 -0.09815868 -0.098229475 -0.098179296 -0.097998887 -0.097729765 -0.097404182 -0.097007826 -0.09656509 -0.096137069 -0.095761605 -0.09543211 -0.0951316 -0.094882995 -0.094726175][-0.099978492 -0.09847305 -0.098722674 -0.098846592 -0.098803207 -0.098617285 -0.0983176 -0.09788081 -0.0973383 -0.096793428 -0.096336246 -0.095990404 -0.095737383 -0.095573813 -0.095504157]]...]
INFO - root - 2017-12-09 05:39:54.313779: step 710, loss = 1.03, batch loss = 0.68 (12.6 examples/sec; 0.633 sec/batch; 58h:20m:17s remains)
INFO - root - 2017-12-09 05:40:00.839844: step 720, loss = 1.05, batch loss = 0.69 (12.3 examples/sec; 0.653 sec/batch; 60h:10m:23s remains)
INFO - root - 2017-12-09 05:40:07.405764: step 730, loss = 1.03, batch loss = 0.68 (12.4 examples/sec; 0.643 sec/batch; 59h:15m:28s remains)
INFO - root - 2017-12-09 05:40:13.912267: step 740, loss = 1.02, batch loss = 0.67 (12.5 examples/sec; 0.641 sec/batch; 59h:04m:31s remains)
INFO - root - 2017-12-09 05:40:20.399322: step 750, loss = 1.03, batch loss = 0.67 (12.6 examples/sec; 0.635 sec/batch; 58h:28m:21s remains)
INFO - root - 2017-12-09 05:40:26.869957: step 760, loss = 0.99, batch loss = 0.63 (12.3 examples/sec; 0.652 sec/batch; 60h:03m:17s remains)
INFO - root - 2017-12-09 05:40:33.080419: step 770, loss = 1.04, batch loss = 0.68 (12.8 examples/sec; 0.627 sec/batch; 57h:48m:28s remains)
INFO - root - 2017-12-09 05:40:39.308894: step 780, loss = 1.04, batch loss = 0.68 (12.4 examples/sec; 0.647 sec/batch; 59h:39m:34s remains)
INFO - root - 2017-12-09 05:40:45.717338: step 790, loss = 1.03, batch loss = 0.67 (12.3 examples/sec; 0.651 sec/batch; 60h:01m:41s remains)
INFO - root - 2017-12-09 05:40:52.127580: step 800, loss = 1.04, batch loss = 0.67 (12.3 examples/sec; 0.650 sec/batch; 59h:52m:32s remains)
2017-12-09 05:40:52.812730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.19833557 -0.33193469 -0.1134827 -0.04217165 0.11384763 0.39178777 0.7397542 0.64790618 0.73220563 0.61800385 0.52368373 0.38206977 0.2191052 0.11518262 0.096934661][-0.16740285 -0.1502104 -0.0104606 0.03682068 0.41707742 0.53794831 0.96302933 1.0172546 1.1924022 0.97189373 0.82374787 0.74024069 0.80486006 0.83733946 0.92703635][-0.38337535 -0.39080232 -0.10682882 0.064480916 0.54175091 0.50417143 0.81339669 0.75713354 0.80359495 0.60972166 0.67682564 0.59138608 0.35414863 0.2663846 0.29472888][-0.12396592 0.12413095 0.10424702 0.063212827 0.033694953 0.05116576 0.34721726 0.32779545 0.56792319 0.50553375 0.35017985 0.33384675 0.35612428 0.23824747 0.11135398][0.15964653 0.32777047 0.42681503 0.51848722 0.46666366 0.30517662 0.082942426 0.11909617 0.26513219 0.13327481 0.27392977 0.21348588 0.24472906 0.13327231 0.14157839][0.66265339 0.8071866 0.61920381 0.69209671 0.59989 0.42531365 0.23845617 0.0041641891 -0.093122572 -0.079424322 0.20527251 0.041968241 0.078749686 -0.008321166 -0.0087167323][0.9195717 1.1908835 1.06506 0.87687486 0.48511034 0.43237942 0.2910074 0.15118383 0.15250193 -0.01569543 -0.25429329 -0.42496943 -0.29429042 -0.26607043 -0.2640816][1.0290977 1.0235362 1.0577903 1.2097636 1.1655129 1.0523443 0.62035722 0.35818791 0.052962258 -0.19813663 -0.32847095 -0.47193563 -0.55165809 -0.52185047 -0.56081736][0.88589185 1.0417845 1.1075354 1.1682472 1.1238993 1.3052405 1.1294438 0.75655657 0.31688035 0.020265549 -0.24705842 -0.34549 -0.40528286 -0.28521538 -0.39522707][0.52811092 0.77480757 1.0233563 1.2070341 1.2415329 1.2282146 0.90463072 0.72449571 0.47162187 0.083339795 -0.22336417 -0.2735244 -0.41479993 -0.24600857 -0.17961475][0.0078637153 0.18306445 0.36684233 0.56689924 0.62610137 0.57389426 0.5603742 0.32218248 0.034910664 0.029749736 -0.073395237 -0.073880754 -0.18654528 -0.10357854 -0.089335375][-0.483406 -0.44291431 -0.24553962 -0.13145457 0.028694674 0.084631875 0.15405063 0.22045873 0.23949589 0.15384047 -0.01136905 -0.1137508 -0.12711906 -0.098590828 -0.14143944][-0.68867993 -0.61172104 -0.55285996 -0.48182404 -0.28376651 -0.15157327 -0.0038714856 -0.028304629 0.0096717775 -0.0085323453 -0.016706124 -0.047886655 -0.14043626 -0.16979671 -0.24297071][-0.62637007 -0.57633775 -0.48442918 -0.41088927 -0.46383566 -0.44395524 -0.32744911 -0.19735953 -0.10872024 -0.12033013 -0.13677764 -0.17446575 -0.22986561 -0.19846132 -0.17648484][-0.47927004 -0.56206232 -0.66046619 -0.53840166 -0.38903749 -0.38578054 -0.42295951 -0.39741075 -0.31710777 -0.23604241 -0.21111551 -0.21084657 -0.26596016 -0.26432288 -0.26649719]]...]
INFO - root - 2017-12-09 05:40:59.179362: step 810, loss = 1.02, batch loss = 0.65 (12.4 examples/sec; 0.643 sec/batch; 59h:17m:09s remains)
INFO - root - 2017-12-09 05:41:05.643815: step 820, loss = 1.04, batch loss = 0.67 (12.1 examples/sec; 0.662 sec/batch; 60h:59m:07s remains)
INFO - root - 2017-12-09 05:41:12.131787: step 830, loss = 1.03, batch loss = 0.66 (12.7 examples/sec; 0.631 sec/batch; 58h:07m:58s remains)
INFO - root - 2017-12-09 05:41:18.538047: step 840, loss = 1.01, batch loss = 0.63 (12.7 examples/sec; 0.629 sec/batch; 57h:57m:19s remains)
INFO - root - 2017-12-09 05:41:24.764813: step 850, loss = 1.06, batch loss = 0.68 (12.4 examples/sec; 0.646 sec/batch; 59h:30m:42s remains)
INFO - root - 2017-12-09 05:41:31.371112: step 860, loss = 1.05, batch loss = 0.67 (12.5 examples/sec; 0.642 sec/batch; 59h:09m:30s remains)
INFO - root - 2017-12-09 05:41:37.730867: step 870, loss = 1.05, batch loss = 0.67 (12.3 examples/sec; 0.652 sec/batch; 60h:01m:21s remains)
INFO - root - 2017-12-09 05:41:43.874578: step 880, loss = 1.06, batch loss = 0.68 (12.8 examples/sec; 0.626 sec/batch; 57h:41m:10s remains)
INFO - root - 2017-12-09 05:41:50.205308: step 890, loss = 1.08, batch loss = 0.69 (12.6 examples/sec; 0.633 sec/batch; 58h:16m:18s remains)
INFO - root - 2017-12-09 05:41:56.629091: step 900, loss = 1.06, batch loss = 0.68 (12.5 examples/sec; 0.640 sec/batch; 58h:55m:52s remains)
2017-12-09 05:41:57.362045: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0091585219 0.010652453 0.011305749 0.011037007 0.010602415 0.009720549 0.009188965 0.0091083348 0.0092442781 0.0090438426 0.0087924749 0.00890401 0.0088751912 0.0088464171 0.008216083][0.0087508559 0.010739177 0.011734456 0.011718944 0.011520863 0.011067629 0.010770589 0.010866836 0.011323839 0.01181972 0.012091801 0.012153551 0.01178807 0.011075079 0.010105938][0.0068277717 0.0088109076 0.0096559525 0.0095642358 0.0093936622 0.0090915412 0.0090870857 0.0096257925 0.010603756 0.011615023 0.012055546 0.012567461 0.012465984 0.011495426 0.010214239][0.0021143258 0.004319936 0.0052718371 0.0055469275 0.0059245229 0.0062783659 0.0069587976 0.0081719458 0.0097688437 0.011285692 0.011898249 0.01233016 0.011958852 0.010719046 0.0092774928][-0.0097155869 -0.0065016598 -0.0042534024 -0.0021505952 -0.00028769672 0.0018452853 0.0038882643 0.0060812533 0.0083449483 0.010329649 0.011447281 0.011735409 0.011136219 0.0096177906 0.0080204606][-0.03540571 -0.029313147 -0.023180321 -0.016408816 -0.0099989325 -0.0046269298 -0.0005401969 0.0030540973 0.0059977323 0.0082584023 0.009469986 0.010084376 0.0097506642 0.0081318766 0.006624043][-0.074328177 -0.064423859 -0.052668557 -0.038884133 -0.025413409 -0.014086738 -0.0058590174 -0.0005274713 0.0028145015 0.0052325279 0.0063472539 0.0063923 0.0056852251 0.0042105168 0.0033869594][-0.11944935 -0.10670844 -0.0895096 -0.068505995 -0.047251157 -0.028843373 -0.015266985 -0.0067680478 -0.0020923465 0.0001707226 0.00084877014 0.0010610521 0.00078997016 -4.2483211e-05 -0.00064116716][-0.16527191 -0.1516033 -0.13082464 -0.10472865 -0.077415206 -0.052742928 -0.033787884 -0.021642774 -0.015170664 -0.012240618 -0.011030436 -0.010114849 -0.0090503991 -0.007831648 -0.0067302287][-0.20666911 -0.19460183 -0.17327575 -0.14562868 -0.11576352 -0.087775506 -0.065373242 -0.050480247 -0.042547829 -0.038798131 -0.03633891 -0.032902978 -0.028017461 -0.022534311 -0.018073916][-0.23370239 -0.22556846 -0.20749739 -0.18290564 -0.1553302 -0.12857434 -0.10636114 -0.091101192 -0.08288037 -0.078738727 -0.074935332 -0.068185717 -0.058025569 -0.046756163 -0.038011938][-0.236852 -0.23369756 -0.22244847 -0.20579705 -0.18585595 -0.16533993 -0.14734891 -0.13433971 -0.1271624 -0.12338751 -0.11883952 -0.10914727 -0.093699306 -0.076522157 -0.063754842][-0.22303548 -0.22278008 -0.21815413 -0.21077663 -0.20097288 -0.18994671 -0.17942879 -0.1715574 -0.16749755 -0.16534747 -0.16081612 -0.14860623 -0.12816419 -0.1054761 -0.089265846][-0.20645946 -0.20544314 -0.20363307 -0.2020202 -0.19972628 -0.19689074 -0.19402066 -0.19262598 -0.19353066 -0.19484217 -0.191808 -0.17838974 -0.15436998 -0.12753956 -0.10920272][-0.19332421 -0.1889509 -0.18582648 -0.18488316 -0.18489863 -0.18563998 -0.18709441 -0.19021678 -0.19531448 -0.20029923 -0.19982809 -0.18764327 -0.16357531 -0.13624769 -0.11875656]]...]
INFO - root - 2017-12-09 05:42:03.709601: step 910, loss = 1.08, batch loss = 0.69 (12.4 examples/sec; 0.645 sec/batch; 59h:26m:00s remains)
INFO - root - 2017-12-09 05:42:10.174247: step 920, loss = 1.07, batch loss = 0.69 (12.4 examples/sec; 0.643 sec/batch; 59h:15m:52s remains)
INFO - root - 2017-12-09 05:42:16.625480: step 930, loss = 1.07, batch loss = 0.68 (12.6 examples/sec; 0.633 sec/batch; 58h:17m:55s remains)
INFO - root - 2017-12-09 05:42:22.875752: step 940, loss = 1.07, batch loss = 0.68 (12.8 examples/sec; 0.624 sec/batch; 57h:29m:36s remains)
INFO - root - 2017-12-09 05:42:29.300847: step 950, loss = 1.06, batch loss = 0.67 (12.6 examples/sec; 0.637 sec/batch; 58h:38m:47s remains)
INFO - root - 2017-12-09 05:42:35.892795: step 960, loss = 1.05, batch loss = 0.65 (12.2 examples/sec; 0.654 sec/batch; 60h:16m:27s remains)
INFO - root - 2017-12-09 05:42:42.235339: step 970, loss = 1.09, batch loss = 0.69 (14.7 examples/sec; 0.543 sec/batch; 50h:02m:50s remains)
INFO - root - 2017-12-09 05:42:48.100068: step 980, loss = 1.06, batch loss = 0.66 (12.4 examples/sec; 0.646 sec/batch; 59h:30m:04s remains)
INFO - root - 2017-12-09 05:42:54.564570: step 990, loss = 1.08, batch loss = 0.68 (12.6 examples/sec; 0.633 sec/batch; 58h:15m:46s remains)
INFO - root - 2017-12-09 05:43:00.970237: step 1000, loss = 1.09, batch loss = 0.69 (12.4 examples/sec; 0.645 sec/batch; 59h:21m:36s remains)
2017-12-09 05:43:01.657742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.34259105 -0.3329463 -0.32963547 -0.33068091 -0.33373952 -0.33682913 -0.33849826 -0.33837622 -0.33719385 -0.33697087 -0.33940279 -0.34279788 -0.34436554 -0.34279889 -0.3404257][-0.30367163 -0.29198045 -0.28851676 -0.28998673 -0.29347152 -0.29693294 -0.298861 -0.29865295 -0.2972239 -0.29703143 -0.29964602 -0.30330762 -0.30524379 -0.30394986 -0.30198288][-0.25385475 -0.24071839 -0.23717204 -0.23871429 -0.24233454 -0.24610668 -0.24855432 -0.24900782 -0.24807069 -0.24792649 -0.25019583 -0.25331756 -0.25476134 -0.25304312 -0.25140965][-0.20892927 -0.1946415 -0.19057029 -0.19142957 -0.19443467 -0.19799073 -0.20091979 -0.20253256 -0.20288399 -0.20346217 -0.20564136 -0.20821793 -0.2091569 -0.20712139 -0.20598185][-0.17806689 -0.16416372 -0.1599824 -0.15999357 -0.16171025 -0.1640764 -0.16669679 -0.16918726 -0.17090048 -0.17223489 -0.17414011 -0.17605241 -0.17669173 -0.17488539 -0.17455149][-0.16659448 -0.15373373 -0.14946787 -0.14838387 -0.14826958 -0.14874579 -0.15048932 -0.15345669 -0.1561396 -0.15774675 -0.15885408 -0.15971933 -0.15998819 -0.15860738 -0.159063][-0.17030919 -0.15748914 -0.15227653 -0.1495162 -0.14733061 -0.14609471 -0.14718299 -0.150423 -0.15357853 -0.15505564 -0.15526453 -0.15514126 -0.15508011 -0.15409726 -0.15494642][-0.17807736 -0.16391599 -0.15658174 -0.15173925 -0.14785998 -0.14567631 -0.14655699 -0.14993021 -0.15328914 -0.15481541 -0.15482402 -0.15452851 -0.15464164 -0.15419069 -0.15508096][-0.18509372 -0.1688381 -0.15913245 -0.1525892 -0.14776661 -0.14519905 -0.1457731 -0.14868672 -0.15166187 -0.15311895 -0.15332302 -0.15343797 -0.15422745 -0.15462217 -0.15548518][-0.19301951 -0.17450412 -0.16241695 -0.1544062 -0.14885095 -0.14581551 -0.14572965 -0.14771521 -0.14981432 -0.15083158 -0.15112728 -0.15175293 -0.15333524 -0.15458961 -0.1556572][-0.20460698 -0.18401901 -0.16931395 -0.15952802 -0.15304528 -0.14957777 -0.14899701 -0.15017039 -0.15143052 -0.15200132 -0.15235633 -0.15337649 -0.15569425 -0.15795255 -0.15996307][-0.22269931 -0.20169502 -0.18537909 -0.17467715 -0.16809539 -0.16501847 -0.16459447 -0.1654093 -0.16622187 -0.16670015 -0.16731119 -0.16864452 -0.1712736 -0.17406298 -0.17707965][-0.2498166 -0.23110829 -0.2152537 -0.20522316 -0.19975673 -0.19768295 -0.19753246 -0.19803259 -0.19854179 -0.19912158 -0.20009854 -0.20172597 -0.20438741 -0.20730233 -0.21089472][-0.28578117 -0.27143428 -0.2576372 -0.24929433 -0.24518773 -0.24385092 -0.24374789 -0.24392793 -0.24418971 -0.24477842 -0.24588197 -0.24747291 -0.24973941 -0.25223219 -0.25581616][-0.32581192 -0.31665075 -0.30527106 -0.29839617 -0.29501653 -0.2937904 -0.29350963 -0.29354835 -0.2938557 -0.29466075 -0.2960048 -0.29760817 -0.2994701 -0.30141628 -0.30477214]]...]
INFO - root - 2017-12-09 05:43:07.933476: step 1010, loss = 1.12, batch loss = 0.72 (12.0 examples/sec; 0.664 sec/batch; 61h:09m:40s remains)
INFO - root - 2017-12-09 05:43:14.517978: step 1020, loss = 1.09, batch loss = 0.68 (12.4 examples/sec; 0.648 sec/batch; 59h:37m:35s remains)
INFO - root - 2017-12-09 05:43:21.037246: step 1030, loss = 1.12, batch loss = 0.71 (12.8 examples/sec; 0.624 sec/batch; 57h:30m:02s remains)
INFO - root - 2017-12-09 05:43:27.500372: step 1040, loss = 1.09, batch loss = 0.68 (12.4 examples/sec; 0.644 sec/batch; 59h:17m:33s remains)
INFO - root - 2017-12-09 05:43:33.781835: step 1050, loss = 1.14, batch loss = 0.72 (13.4 examples/sec; 0.595 sec/batch; 54h:46m:08s remains)
INFO - root - 2017-12-09 05:43:40.088689: step 1060, loss = 1.20, batch loss = 0.79 (12.2 examples/sec; 0.653 sec/batch; 60h:09m:55s remains)
INFO - root - 2017-12-09 05:43:46.471000: step 1070, loss = 1.15, batch loss = 0.74 (12.7 examples/sec; 0.632 sec/batch; 58h:08m:44s remains)
INFO - root - 2017-12-09 05:43:52.450939: step 1080, loss = 1.04, batch loss = 0.63 (12.9 examples/sec; 0.620 sec/batch; 57h:05m:58s remains)
INFO - root - 2017-12-09 05:43:58.798384: step 1090, loss = 1.09, batch loss = 0.67 (12.4 examples/sec; 0.644 sec/batch; 59h:15m:45s remains)
INFO - root - 2017-12-09 05:44:05.352668: step 1100, loss = 1.08, batch loss = 0.66 (12.7 examples/sec; 0.631 sec/batch; 58h:05m:25s remains)
2017-12-09 05:44:05.998142: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.2624784 1.2593317 1.2500589 1.2421008 1.2363455 1.2379429 1.2418381 1.2458255 1.2478381 1.2450403 1.238567 1.2266809 1.2155639 1.2069491 1.2016541][1.2896649 1.2960087 1.291221 1.2839097 1.2777481 1.2783242 1.2814186 1.2853907 1.2883846 1.2888435 1.2859851 1.2782059 1.2702028 1.2635722 1.2591573][1.3111833 1.3239346 1.3210973 1.3148985 1.3090611 1.3057733 1.3052733 1.3063345 1.3072349 1.3063433 1.302816 1.2969749 1.2898521 1.2829671 1.2773582][1.3067259 1.3281145 1.3272027 1.3224142 1.3175447 1.3151243 1.3156906 1.3182871 1.321119 1.322317 1.3206258 1.3159509 1.3091329 1.3016927 1.2948155][1.2955366 1.3303916 1.3315119 1.3286372 1.325586 1.3248419 1.3269265 1.3309263 1.335104 1.3375385 1.3368099 1.3327112 1.3260728 1.3183993 1.310817][1.2823492 1.3323613 1.3358867 1.3352544 1.3341607 1.3348987 1.3379335 1.3425171 1.3470167 1.3496728 1.3492078 1.3454088 1.3390547 1.3315003 1.3238248][1.2653141 1.3329662 1.338874 1.340161 1.3404815 1.3419639 1.3451614 1.3496133 1.3538697 1.3562868 1.3557701 1.3520904 1.3460057 1.3387213 1.3313386][1.2419317 1.3257681 1.3348784 1.3374286 1.3384713 1.3401581 1.3431587 1.3472486 1.3510708 1.3531874 1.3525839 1.3490058 1.3432618 1.3364471 1.329834][1.2089342 1.3058542 1.3183869 1.3219767 1.3233411 1.3250147 1.3277547 1.3315643 1.3351712 1.3373085 1.3369548 1.3338296 1.3286407 1.322503 1.3169178][1.1566 1.2622908 1.2810224 1.2854116 1.2861038 1.2866001 1.2882681 1.2917298 1.2956439 1.2988471 1.2997189 1.2977203 1.2930875 1.2875315 1.282616][1.0733399 1.1800748 1.2035371 1.2074788 1.2056605 1.2032428 1.2029778 1.2062088 1.2114071 1.2171736 1.2202129 1.2194784 1.2144443 1.2079233 1.2020514][0.93447316 1.0352178 1.0599879 1.0612787 1.0557168 1.0504655 1.0491551 1.0531934 1.0606 1.068894 1.073176 1.0721335 1.0648071 1.0560228 1.0483479][0.73761654 0.82386243 0.84684646 0.84496737 0.83638692 0.82929778 0.82795417 0.83371115 0.84345257 0.853629 0.85816526 0.85565925 0.84567308 0.83452 0.82515728][0.50581485 0.57193613 0.59115392 0.58739412 0.57793647 0.57083631 0.57019162 0.57695609 0.58758879 0.59816033 0.60231221 0.59883845 0.587738 0.57570785 0.56580335][0.26565325 0.31361121 0.32890332 0.32422459 0.31495476 0.30832171 0.30764389 0.31379145 0.32350594 0.33299106 0.33612603 0.33204907 0.32118183 0.3097524 0.30042255]]...]
INFO - root - 2017-12-09 05:44:12.418461: step 1110, loss = 1.14, batch loss = 0.72 (12.0 examples/sec; 0.666 sec/batch; 61h:16m:55s remains)
INFO - root - 2017-12-09 05:44:18.824512: step 1120, loss = 1.13, batch loss = 0.70 (12.7 examples/sec; 0.630 sec/batch; 57h:57m:30s remains)
INFO - root - 2017-12-09 05:44:25.222681: step 1130, loss = 1.10, batch loss = 0.67 (12.6 examples/sec; 0.636 sec/batch; 58h:31m:57s remains)
INFO - root - 2017-12-09 05:44:31.625695: step 1140, loss = 1.09, batch loss = 0.66 (12.7 examples/sec; 0.630 sec/batch; 57h:57m:24s remains)
INFO - root - 2017-12-09 05:44:37.896053: step 1150, loss = 1.09, batch loss = 0.67 (12.7 examples/sec; 0.631 sec/batch; 58h:05m:17s remains)
INFO - root - 2017-12-09 05:44:44.123798: step 1160, loss = 1.09, batch loss = 0.66 (12.9 examples/sec; 0.619 sec/batch; 57h:00m:42s remains)
INFO - root - 2017-12-09 05:44:50.606584: step 1170, loss = 1.09, batch loss = 0.66 (12.5 examples/sec; 0.638 sec/batch; 58h:43m:00s remains)
INFO - root - 2017-12-09 05:44:56.638835: step 1180, loss = 1.09, batch loss = 0.66 (13.7 examples/sec; 0.583 sec/batch; 53h:36m:42s remains)
INFO - root - 2017-12-09 05:45:02.916511: step 1190, loss = 1.07, batch loss = 0.64 (12.3 examples/sec; 0.652 sec/batch; 60h:00m:18s remains)
INFO - root - 2017-12-09 05:45:09.224213: step 1200, loss = 1.10, batch loss = 0.66 (12.3 examples/sec; 0.649 sec/batch; 59h:45m:35s remains)
2017-12-09 05:45:09.892490: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11396444 0.11968276 0.13285422 0.1475715 0.16188145 0.17386758 0.17843851 0.17901784 0.17878202 0.1795707 0.18230447 0.18332717 0.18520388 0.18605965 0.18372506][0.087466687 0.09276554 0.10368758 0.11895794 0.13485307 0.14964733 0.15917668 0.16323587 0.16443485 0.16385618 0.16498026 0.16560653 0.16793385 0.17128187 0.17282346][0.097317904 0.099317044 0.10488033 0.11660671 0.13158 0.14629927 0.15779623 0.16388246 0.16511977 0.16366324 0.16198921 0.16173935 0.16407493 0.16932347 0.17442563][0.099947363 0.099191338 0.098476946 0.10329819 0.11469245 0.12955347 0.14467207 0.15569267 0.1610747 0.16130045 0.15847889 0.15502328 0.15365759 0.15619805 0.16061977][0.10635155 0.10593957 0.097870409 0.093415469 0.09902975 0.11260116 0.13091877 0.147856 0.159058 0.16258773 0.15957406 0.15297031 0.14690337 0.14486563 0.14635095][0.10996661 0.11273557 0.098958343 0.085836142 0.0845696 0.095054865 0.11531833 0.13804305 0.15609711 0.16533419 0.16524786 0.15821701 0.14863375 0.14140457 0.13798496][0.11567807 0.12590924 0.10884631 0.089876652 0.082510412 0.088596672 0.10746646 0.13218918 0.15462899 0.16909763 0.17325687 0.16854158 0.15879354 0.1487543 0.14090675][0.12117049 0.14148495 0.12595424 0.10504496 0.093391985 0.094341666 0.10947913 0.13325101 0.15744367 0.17557573 0.18419474 0.18366838 0.17626089 0.1657334 0.15491086][0.12678307 0.15852523 0.14761966 0.12789997 0.11448494 0.11160672 0.12229976 0.14241439 0.16495714 0.18376151 0.19510502 0.19865039 0.19529331 0.18721756 0.17796487][0.12718624 0.17049447 0.16636932 0.15072617 0.13792837 0.13298231 0.13939676 0.15487611 0.17378131 0.19102472 0.20321074 0.20983657 0.210693 0.20641798 0.19953048][0.10753345 0.16031417 0.16609615 0.15981388 0.15265974 0.14924121 0.15322629 0.16401225 0.1780895 0.19230229 0.20398673 0.21249887 0.21704507 0.21728322 0.21434709][0.050616324 0.11383697 0.13399351 0.14102158 0.14333248 0.14497906 0.14874852 0.15560076 0.16439438 0.17420906 0.1832366 0.19149709 0.19794351 0.20181224 0.20286068][-0.053799942 0.015985772 0.052070752 0.074041575 0.088123381 0.096615821 0.10194272 0.10613394 0.11007226 0.11480922 0.11997342 0.12607577 0.13179219 0.13672599 0.13938412][-0.1928183 -0.12708485 -0.082033053 -0.049937338 -0.026617929 -0.011574358 -0.0035563409 -6.5296888e-05 0.0012200475 0.0023982823 0.0042743087 0.0076159686 0.011287972 0.014509335 0.016456231][-0.33934855 -0.28555584 -0.2397722 -0.20388119 -0.1754598 -0.15579978 -0.14506535 -0.14103673 -0.1407059 -0.14114141 -0.14056021 -0.13786189 -0.13493405 -0.13257731 -0.13216281]]...]
INFO - root - 2017-12-09 05:45:16.171022: step 1210, loss = 1.10, batch loss = 0.66 (12.7 examples/sec; 0.629 sec/batch; 57h:50m:27s remains)
INFO - root - 2017-12-09 05:45:22.539783: step 1220, loss = 1.11, batch loss = 0.67 (12.3 examples/sec; 0.649 sec/batch; 59h:45m:07s remains)
INFO - root - 2017-12-09 05:45:29.053649: step 1230, loss = 1.11, batch loss = 0.67 (12.2 examples/sec; 0.653 sec/batch; 60h:06m:36s remains)
INFO - root - 2017-12-09 05:45:35.560904: step 1240, loss = 1.11, batch loss = 0.66 (12.1 examples/sec; 0.662 sec/batch; 60h:55m:27s remains)
INFO - root - 2017-12-09 05:45:42.128507: step 1250, loss = 1.12, batch loss = 0.67 (12.3 examples/sec; 0.648 sec/batch; 59h:36m:25s remains)
INFO - root - 2017-12-09 05:45:49.880940: step 1260, loss = 1.12, batch loss = 0.67 (13.1 examples/sec; 0.613 sec/batch; 56h:23m:21s remains)
INFO - root - 2017-12-09 05:45:56.763573: step 1270, loss = 1.11, batch loss = 0.66 (12.7 examples/sec; 0.630 sec/batch; 57h:57m:01s remains)
INFO - root - 2017-12-09 05:46:02.943641: step 1280, loss = 1.10, batch loss = 0.65 (12.6 examples/sec; 0.637 sec/batch; 58h:34m:55s remains)
INFO - root - 2017-12-09 05:46:08.927271: step 1290, loss = 1.07, batch loss = 0.62 (13.0 examples/sec; 0.616 sec/batch; 56h:42m:22s remains)
INFO - root - 2017-12-09 05:46:15.364048: step 1300, loss = 1.17, batch loss = 0.72 (12.3 examples/sec; 0.652 sec/batch; 59h:58m:37s remains)
2017-12-09 05:46:16.037129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.21361923 -0.15019341 -0.13479021 -0.19560388 -0.33453092 -0.20307063 -0.29929602 -0.3091321 -0.22915557 -0.32735237 -0.30464718 -0.29489553 -0.37207705 -0.29924685 -0.19401813][-0.12197119 -0.14677966 -0.065023594 -0.052792177 -0.12048384 -0.116603 -0.25560987 -0.37674975 -0.31355715 -0.39939618 -0.34884113 -0.32681224 -0.38282818 -0.27717176 -0.098237053][-0.144491 -0.025339365 0.024326339 -0.023001641 -0.12714061 -0.10654246 -0.20061381 -0.32245219 -0.33648762 -0.52763331 -0.430849 -0.4334667 -0.41906446 -0.34483159 -0.1477512][-0.14334846 0.025877446 0.12017962 0.07114765 0.093918115 0.077525169 -0.11805544 -0.20714951 -0.30008852 -0.40376446 -0.45413128 -0.42304611 -0.38665229 -0.39305609 -0.25261509][0.039602965 0.18120277 0.2830576 0.16096586 0.25480798 0.26902837 0.10387796 0.011184961 -0.13310105 -0.27602372 -0.36775386 -0.45086256 -0.51269841 -0.486293 -0.37662688][0.11188796 0.2996788 0.445621 0.5121063 0.56219375 0.48653448 0.31340691 0.33638793 0.17516893 -0.0013224781 -0.13210411 -0.26286811 -0.37305844 -0.34707361 -0.34389564][0.17961875 0.405325 0.53719389 0.60237592 0.750592 0.87556463 0.71179193 0.79712832 0.6113174 0.42163897 0.25130376 0.042111933 -0.075985894 -0.17530489 -0.10711265][0.20491076 0.35137075 0.51427776 0.57744777 0.76335734 0.92077023 1.0258281 1.2087643 1.0357122 0.87909931 0.66379064 0.34063041 0.16494742 -0.06726978 -0.16648856][0.043244496 0.1351029 0.17774722 0.28003383 0.52288556 0.62816858 0.72611618 1.0841663 1.1301005 0.97187835 0.84797531 0.52248931 0.22035012 -0.054885849 -0.013950929][-0.15098849 -0.010190606 0.10333848 0.092790008 0.080858111 0.15462711 0.26335865 0.53261137 0.67104 0.76912254 0.58836615 0.38965839 0.23222458 0.025414824 -0.10240998][-0.053796977 -0.14657052 -0.22194445 -0.29266888 -0.2879822 -0.31276256 -0.2622492 -0.11880164 -0.03731738 0.077214092 0.062387452 -0.11043461 -0.26299867 -0.286888 -0.325604][-0.13567264 -0.17823672 -0.36525571 -0.32992858 -0.30250946 -0.31747615 -0.29328185 -0.29840142 -0.4282684 -0.3639726 -0.32143676 -0.36686504 -0.38693351 -0.4209429 -0.43913406][-0.10536508 -0.087296635 -0.22762553 -0.46152878 -0.536927 -0.63089329 -0.54552627 -0.58129787 -0.57651359 -0.43856382 -0.40754911 -0.36170334 -0.35788792 -0.33979091 -0.23753731][-0.098332711 -0.083735794 -0.19198547 -0.24069341 -0.56780457 -0.79805619 -0.83110976 -0.85887897 -0.78777713 -0.75499076 -0.64770025 -0.47476488 -0.38993075 -0.2975316 -0.35596085][-0.070066914 -0.034541577 -0.0040013045 -0.17667542 -0.32339635 -0.61983013 -0.85659474 -0.96019232 -0.96821028 -0.94677877 -0.90975457 -0.754186 -0.49901992 -0.31858671 -0.22959816]]...]
INFO - root - 2017-12-09 05:46:22.336831: step 1310, loss = 1.14, batch loss = 0.69 (12.7 examples/sec; 0.628 sec/batch; 57h:49m:02s remains)
INFO - root - 2017-12-09 05:46:28.750010: step 1320, loss = 1.14, batch loss = 0.68 (12.5 examples/sec; 0.642 sec/batch; 59h:01m:25s remains)
INFO - root - 2017-12-09 05:46:35.161367: step 1330, loss = 1.14, batch loss = 0.68 (12.4 examples/sec; 0.644 sec/batch; 59h:14m:51s remains)
INFO - root - 2017-12-09 05:46:41.520368: step 1340, loss = 1.17, batch loss = 0.71 (12.2 examples/sec; 0.658 sec/batch; 60h:31m:43s remains)
INFO - root - 2017-12-09 05:46:47.831018: step 1350, loss = 1.33, batch loss = 0.86 (13.0 examples/sec; 0.616 sec/batch; 56h:39m:43s remains)
INFO - root - 2017-12-09 05:46:54.038765: step 1360, loss = 1.14, batch loss = 0.68 (12.4 examples/sec; 0.647 sec/batch; 59h:30m:41s remains)
INFO - root - 2017-12-09 05:47:00.389109: step 1370, loss = 1.15, batch loss = 0.68 (12.9 examples/sec; 0.621 sec/batch; 57h:05m:03s remains)
INFO - root - 2017-12-09 05:47:06.722949: step 1380, loss = 1.16, batch loss = 0.69 (13.3 examples/sec; 0.603 sec/batch; 55h:25m:01s remains)
INFO - root - 2017-12-09 05:47:12.784796: step 1390, loss = 1.20, batch loss = 0.73 (12.8 examples/sec; 0.626 sec/batch; 57h:34m:47s remains)
INFO - root - 2017-12-09 05:47:19.137597: step 1400, loss = 1.15, batch loss = 0.67 (12.2 examples/sec; 0.656 sec/batch; 60h:18m:10s remains)
2017-12-09 05:47:19.817882: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34488645 0.3507733 0.34209594 0.33874634 0.3436794 0.34050664 0.40959147 0.37102357 0.37945446 0.40776 0.32676294 0.35708037 0.35648152 0.39339462 0.48236373][0.43435594 0.42396638 0.43006632 0.43534461 0.4419916 0.45061216 0.46926668 0.46267328 0.45842078 0.45758322 0.47626576 0.47819605 0.43445876 0.44781747 0.45260838][0.53036976 0.51766849 0.46576217 0.39671966 0.40691212 0.45738527 0.49048242 0.5154593 0.47889844 0.44622561 0.48376343 0.52430439 0.56449258 0.53531229 0.55877423][0.61338425 0.55811489 0.48526332 0.48010179 0.58636141 0.6015656 0.58118474 0.68965685 0.91414857 0.95629144 0.72323465 0.54355562 0.55474818 0.49651173 0.58709335][0.582155 0.58232439 0.63281071 0.80195034 0.75215387 0.70439124 0.67691028 0.72793615 0.81013012 0.83876479 0.757187 0.7111026 0.82980728 0.92254877 0.78012824][0.62848508 0.70927608 0.85540318 0.80096591 0.88000941 0.8248359 0.81307352 0.84492314 0.87630236 0.97115278 1.0445342 0.93354535 0.92652357 0.95683682 0.8704896][0.5806694 0.82049716 0.86974895 1.0228461 1.087358 0.93899262 0.82593679 0.795779 0.84804082 0.87174284 0.76337087 0.79917109 0.76770484 0.82984543 0.8958025][0.47648606 0.52185714 0.7076627 0.77355337 0.777792 0.70964515 0.73982644 0.75568831 0.74664176 0.79434586 0.80749559 0.72559893 0.63397038 0.69929385 0.64542067][0.46534118 0.59022176 0.78418255 0.68794537 0.84139454 0.86130846 0.91762328 0.940256 0.89556026 0.94178987 1.0591292 1.0485582 0.83353758 0.66983509 0.63798773][0.48356971 0.62735856 0.75832283 0.83337188 1.0393548 0.96357715 0.91602349 0.79920065 0.74679804 0.72384131 0.74299872 0.84809875 0.80221343 0.71003008 0.70898354][0.45802203 0.47141263 0.59101224 0.70781791 0.73167384 0.77948904 0.84087169 0.77080953 0.72717845 0.73226404 0.71405625 0.65454304 0.615631 0.55332363 0.56948292][0.3736423 0.46978679 0.578285 0.48762158 0.38123718 0.33276024 0.38315013 0.55953288 0.69872618 0.69261324 0.64032471 0.60869944 0.53785014 0.41372362 0.3496677][0.37690416 0.44074783 0.39108673 0.43444666 0.33915111 0.28719315 0.31824359 0.5613631 0.54536784 0.37496951 0.14649484 -0.010259181 -0.0021313429 0.20292515 0.32251015][0.47087047 0.474331 0.36080542 0.34395054 0.19092342 0.21041232 0.20377383 0.28074405 0.41752341 0.44871661 0.28000358 0.10001174 0.041188926 0.32893762 0.38528392][0.50393152 0.49604705 0.44862315 0.46398535 0.32169095 0.46186104 0.33077136 0.43266246 0.36149207 0.37023035 0.16349325 0.017796442 -0.11135192 -0.082446344 0.049868807]]...]
INFO - root - 2017-12-09 05:47:26.183428: step 1410, loss = 1.19, batch loss = 0.71 (12.7 examples/sec; 0.630 sec/batch; 57h:58m:59s remains)
INFO - root - 2017-12-09 05:47:32.693187: step 1420, loss = 1.16, batch loss = 0.69 (12.2 examples/sec; 0.655 sec/batch; 60h:15m:08s remains)
INFO - root - 2017-12-09 05:47:39.005273: step 1430, loss = 1.23, batch loss = 0.75 (12.8 examples/sec; 0.627 sec/batch; 57h:38m:48s remains)
INFO - root - 2017-12-09 05:47:45.425269: step 1440, loss = 1.19, batch loss = 0.71 (12.5 examples/sec; 0.640 sec/batch; 58h:51m:14s remains)
INFO - root - 2017-12-09 05:47:51.660011: step 1450, loss = 1.17, batch loss = 0.69 (12.9 examples/sec; 0.619 sec/batch; 56h:57m:39s remains)
INFO - root - 2017-12-09 05:47:57.939925: step 1460, loss = 1.17, batch loss = 0.69 (12.6 examples/sec; 0.633 sec/batch; 58h:14m:05s remains)
INFO - root - 2017-12-09 05:48:04.209569: step 1470, loss = 1.18, batch loss = 0.70 (12.8 examples/sec; 0.626 sec/batch; 57h:31m:13s remains)
INFO - root - 2017-12-09 05:48:10.514950: step 1480, loss = 1.15, batch loss = 0.66 (12.2 examples/sec; 0.653 sec/batch; 60h:02m:56s remains)
INFO - root - 2017-12-09 05:48:16.446048: step 1490, loss = 1.17, batch loss = 0.68 (12.5 examples/sec; 0.641 sec/batch; 58h:55m:45s remains)
INFO - root - 2017-12-09 05:48:22.862028: step 1500, loss = 1.18, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:42m:21s remains)
2017-12-09 05:48:23.501170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10514145 -0.014224783 0.21582192 -0.20947047 -0.43095821 -0.35524848 -0.48626915 0.060941949 0.16019511 0.10710874 0.079060107 -0.067728966 -0.12052306 -0.026737899 -0.011335015][-0.18309778 0.0043029636 -0.24375531 -0.54347265 -0.93446946 -0.81644619 0.069015741 0.26482373 0.70288813 0.42911211 0.292506 0.03766188 0.040179297 0.34859905 0.065214008][-0.098533817 -0.034353778 -0.13861631 -0.51139033 -1.13709 -0.23783273 0.11434039 0.63769519 0.7096349 0.43031868 0.33513787 0.53017545 -0.026354745 -0.15599129 0.29730773][-0.32412124 -0.21657923 -0.5792833 -0.91355062 -0.702757 -0.34215808 -0.99830067 -0.97157538 0.033659965 0.78958893 0.88391066 0.42935494 -0.0796949 0.20934251 0.034036517][0.21706024 -0.22696352 -0.55508906 -1.040805 -2.188256 -0.61073351 0.52496326 0.86726749 0.20126775 1.4370596 1.2263198 0.51058471 -0.22434777 -0.3374919 0.20880979][0.11239526 -0.0083798021 -0.39401805 -0.55471325 -1.481191 -2.31191 -1.5953908 0.62422347 1.1360422 1.1341909 1.6589956 1.418074 0.510151 0.20063776 0.20496812][-0.12284674 0.11646131 -0.48634708 -0.41145065 -0.80647504 -0.67981267 -0.2702305 0.27494067 1.9894111 2.4815781 1.9402227 1.7351365 0.75070989 0.25381592 0.028902054][0.12287542 0.27717593 0.35281429 0.061206028 -0.48683891 -0.020004883 -0.19135614 0.81058037 0.77129388 2.3701186 1.7635596 1.4617647 0.8689189 -0.26476592 -0.26378754][0.10423926 0.18230858 0.53154671 1.0395962 1.5332309 1.3650128 1.271001 0.97227776 1.0728636 1.6666238 1.6673467 1.3143359 0.78145719 0.2199955 -0.040106371][-0.30587167 0.44659391 0.5838114 0.54506791 1.1230758 1.7337626 1.6119764 1.6129586 1.1845914 0.57437527 0.65508831 0.78740597 0.718683 0.64729035 0.11199021][0.055942237 0.31890681 0.59551203 0.73651421 1.0600419 1.5842676 1.7456185 1.6226492 1.0423352 0.1785965 -0.18052433 -0.16531153 0.32306948 0.20260489 0.24062863][-0.055058494 0.0484211 0.25294268 0.28906542 0.75805616 1.3372866 1.1860746 1.0937517 1.1379952 1.0228533 0.22868297 0.051973835 -0.21804501 0.027569994 0.16222659][-0.027163357 0.01152651 0.10143322 -0.016426876 0.033649504 0.37224713 0.79266953 0.69851089 0.49771056 0.22196588 -0.11751468 -0.42506742 -0.38787681 -0.046255991 0.15844795][-0.011723831 -0.028386995 -0.055908382 -0.1445681 -0.14508279 0.306478 0.44564363 0.75698972 0.83715868 0.56378436 0.24053648 -0.0035053343 0.030365258 0.0027147532 0.046924919][0.18064246 0.11506587 0.00327228 -0.067653343 -0.053848103 0.44140241 0.56048608 0.50526094 0.26753125 0.0093647987 0.0056259632 0.094005167 0.044629812 -0.01472944 0.055485979]]...]
INFO - root - 2017-12-09 05:48:29.918970: step 1510, loss = 1.16, batch loss = 0.67 (12.6 examples/sec; 0.633 sec/batch; 58h:10m:25s remains)
INFO - root - 2017-12-09 05:48:36.407301: step 1520, loss = 1.16, batch loss = 0.67 (12.1 examples/sec; 0.659 sec/batch; 60h:37m:35s remains)
INFO - root - 2017-12-09 05:48:42.861929: step 1530, loss = 1.16, batch loss = 0.66 (12.4 examples/sec; 0.646 sec/batch; 59h:24m:19s remains)
INFO - root - 2017-12-09 05:48:49.272520: step 1540, loss = 1.18, batch loss = 0.68 (12.7 examples/sec; 0.632 sec/batch; 58h:05m:30s remains)
INFO - root - 2017-12-09 05:48:55.766423: step 1550, loss = 1.21, batch loss = 0.71 (12.5 examples/sec; 0.642 sec/batch; 58h:59m:43s remains)
INFO - root - 2017-12-09 05:49:02.250886: step 1560, loss = 1.19, batch loss = 0.69 (12.5 examples/sec; 0.640 sec/batch; 58h:48m:29s remains)
INFO - root - 2017-12-09 05:49:08.728026: step 1570, loss = 1.21, batch loss = 0.71 (12.4 examples/sec; 0.643 sec/batch; 59h:06m:12s remains)
INFO - root - 2017-12-09 05:49:15.122851: step 1580, loss = 1.18, batch loss = 0.68 (12.6 examples/sec; 0.636 sec/batch; 58h:25m:36s remains)
INFO - root - 2017-12-09 05:49:21.108056: step 1590, loss = 1.19, batch loss = 0.68 (12.3 examples/sec; 0.649 sec/batch; 59h:41m:01s remains)
INFO - root - 2017-12-09 05:49:27.520591: step 1600, loss = 1.16, batch loss = 0.65 (12.8 examples/sec; 0.623 sec/batch; 57h:14m:03s remains)
2017-12-09 05:49:28.164875: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.49159914 0.5972448 0.6115886 0.63088232 0.62288761 0.59009165 0.35495788 0.24636742 0.24567887 0.24508345 0.24562195 0.24774638 0.25124419 0.25522211 0.24651986][0.26048893 0.25303137 0.2441138 0.23576641 0.22931817 0.22471163 0.22118706 0.21820354 0.21566129 0.21416342 0.21451521 0.21750456 0.22349182 0.23178312 0.22881344][0.25056976 0.24099272 0.22867331 0.21735254 0.20866522 0.20194671 0.19611791 0.19056979 0.1854915 0.18181211 0.18088377 0.18440387 0.19323486 0.20645517 0.21123645][0.24197838 0.23134214 0.21686518 0.2032927 0.19268557 0.18399188 0.17624187 0.16856951 0.16104859 0.15483966 0.15194896 0.1551609 0.16632158 0.18429086 0.19625983][0.2338244 0.22499511 0.20940179 0.19456798 0.18314674 0.17369017 0.16511205 0.15676385 0.1482071 0.13973504 0.13412961 0.13569584 0.14771545 0.16877392 0.18465024][0.22639045 0.22252288 0.20695931 0.19191334 0.18072361 0.17174613 0.16397119 0.1564779 0.14821517 0.13808817 0.12931967 0.12780026 0.13899803 0.1613822 0.18001509][0.22145227 0.22323948 0.20856404 0.19369763 0.18279156 0.4198603 0.28720108 0.16159108 0.15394613 0.14296868 0.13193798 0.12806064 0.13786939 0.16003883 0.18052673][0.21637371 0.22354004 0.21148655 0.19685966 0.18569005 0.17687303 0.17015055 0.16477659 0.15845588 0.14876303 0.13811696 0.1338447 0.14279389 0.16375411 0.18314171][0.21117601 0.22313538 0.21353048 0.19879833 0.18689734 0.17702281 0.16941637 0.16433641 0.16021559 0.38775015 0.2601836 0.1435754 0.15144646 0.16968834 0.18553472][0.20698863 0.22256267 0.21504977 0.20012015 0.18785465 0.17756888 0.16988456 0.16557249 0.16395739 0.16196725 0.15868652 0.15775219 0.16384622 0.17772967 0.18830547][0.20473206 0.22265038 0.21795565 0.20345709 0.1917299 0.18254817 0.1763508 0.3946324 0.50284016 0.28313002 0.17573014 0.17618772 0.18020386 0.18943644 0.19464666][0.2054134 0.22505808 0.22433239 0.21118754 0.200979 0.193685 0.1894232 0.18801042 0.18908209 0.19081384 0.19192302 0.19304913 0.19605553 0.20241943 0.20356435][0.20873889 0.23008507 0.23451757 0.22392309 0.21582842 0.21057934 0.2079328 0.20707384 0.20763531 0.20852983 0.20939514 0.21056023 0.21313444 0.21798316 0.21643546][0.21454722 0.23851117 0.24588725 0.23835874 0.232721 0.22885579 0.22694305 0.22601902 0.22570214 0.22583225 0.22636771 0.22745481 0.22983333 0.23366633 0.23068443][0.2021029 0.22482458 0.23459312 0.22861388 0.22364593 0.21988949 0.21694592 0.21642593 0.21687517 0.21757665 0.21993676 0.22186005 0.22520068 0.22955909 0.22540021]]...]
INFO - root - 2017-12-09 05:49:34.522257: step 1610, loss = 1.18, batch loss = 0.67 (12.4 examples/sec; 0.645 sec/batch; 59h:16m:35s remains)
INFO - root - 2017-12-09 05:49:40.920511: step 1620, loss = 1.18, batch loss = 0.67 (12.5 examples/sec; 0.639 sec/batch; 58h:45m:04s remains)
INFO - root - 2017-12-09 05:49:47.324498: step 1630, loss = 1.21, batch loss = 0.70 (12.7 examples/sec; 0.632 sec/batch; 58h:07m:25s remains)
INFO - root - 2017-12-09 05:49:53.509514: step 1640, loss = 1.20, batch loss = 0.68 (12.8 examples/sec; 0.626 sec/batch; 57h:32m:56s remains)
INFO - root - 2017-12-09 05:49:59.829707: step 1650, loss = 1.23, batch loss = 0.71 (12.2 examples/sec; 0.655 sec/batch; 60h:11m:48s remains)
INFO - root - 2017-12-09 05:50:06.289007: step 1660, loss = 1.22, batch loss = 0.70 (13.0 examples/sec; 0.614 sec/batch; 56h:26m:59s remains)
INFO - root - 2017-12-09 05:50:12.709011: step 1670, loss = 1.25, batch loss = 0.72 (12.5 examples/sec; 0.643 sec/batch; 59h:02m:48s remains)
INFO - root - 2017-12-09 05:50:19.093081: step 1680, loss = 1.21, batch loss = 0.68 (12.0 examples/sec; 0.667 sec/batch; 61h:18m:49s remains)
INFO - root - 2017-12-09 05:50:25.088434: step 1690, loss = 1.22, batch loss = 0.69 (12.8 examples/sec; 0.627 sec/batch; 57h:34m:34s remains)
INFO - root - 2017-12-09 05:50:31.469126: step 1700, loss = 1.21, batch loss = 0.68 (12.6 examples/sec; 0.633 sec/batch; 58h:12m:31s remains)
2017-12-09 05:50:32.172726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.204527 -0.45428675 -0.39972556 0.023339435 0.03464222 0.28928679 0.6020636 0.76857638 1.0059334 1.2557538 0.42801166 0.25091946 0.49992889 0.81226289 0.58636969][0.41113979 0.40731961 -0.542995 -0.643364 -0.05727756 0.70648724 0.78190929 0.76230347 1.141487 0.86500454 1.2396929 0.7842328 0.43772334 0.10458036 -0.2735424][-0.068883881 -0.288216 -0.23933217 0.070581362 0.072982505 0.46845227 0.88619268 1.38301 1.7066153 0.674946 0.10366626 0.11928745 -0.20929453 -0.44855452 -0.89024508][-0.36897659 -0.98238724 -0.85180587 -0.48664808 0.14400797 1.0264586 1.5085826 1.0312083 0.54309219 0.16427927 0.3617844 0.13790689 0.4076525 -0.14507458 -0.41568148][-0.71870744 -0.58384418 -1.0337787 -0.95602214 -0.48369128 -0.013261601 0.77650875 1.5899806 0.30364984 0.46766067 0.15686698 0.036969855 -0.14665222 0.25079542 -0.86230129][0.49373573 0.254367 -0.20590322 0.18808667 -0.41674584 0.63053292 0.13743813 0.27288479 0.75908273 0.25931513 -0.508888 -0.53812873 -0.58011276 -0.94947493 0.21665077][0.12272824 0.32508969 0.62092727 0.26058757 0.080034778 -0.12500051 -0.28359538 -0.37966073 -0.28897536 -0.13375713 0.40103054 0.25466371 -0.099732533 0.034949362 -1.204398][-0.056923136 -0.8788588 -1.3123746 -1.1610196 -0.4684965 -0.1417231 -0.56204355 0.33353364 0.26995271 -0.38494655 -0.21960676 -0.26707244 -0.93167889 -0.75176495 -0.4264541][0.26830679 0.086068079 -0.24390969 0.08788006 0.25024849 -0.4003478 0.95611036 0.569264 0.86355925 0.897833 0.96012771 -0.49993271 -0.41510436 -0.015856519 -0.35578325][-0.3948009 0.041613102 -0.36885244 -0.51978332 -0.86822826 0.14469518 0.2330382 1.3360209 1.0341135 1.7389708 1.0239009 0.82180083 0.53131777 -0.37787816 -0.52243006][0.13810839 -0.66591817 -0.83895284 -0.53091431 -0.75639582 -0.92325187 0.20692979 1.3323942 1.3727802 1.3127599 1.1303931 1.0108253 0.77139145 0.35388267 0.45089418][-0.073405504 0.21697323 -0.50840265 -0.24742526 -1.336869 -1.0175409 -0.085678838 0.60876137 1.1804705 1.5615726 1.3691669 1.0340394 0.62435722 -0.17297946 0.26247734][-0.3277539 -0.54027754 -0.35411659 -0.42102402 -0.94804895 0.48753613 0.20980783 0.56157744 1.1190537 1.1947999 1.8246026 0.62734365 0.36556917 0.5715481 0.27418876][-0.11743392 -0.14280331 -0.7002933 -0.261074 -0.022334591 0.30491811 0.58960813 1.2537878 0.36017895 -0.29929975 -0.069297507 -0.50340575 0.49294335 -0.11931852 -0.12655361][0.19338213 -0.036178261 -0.52149761 -0.8411932 -0.8146323 0.16305251 0.085522488 -0.6656332 -1.7479239 -1.5609353 -2.301393 -1.2975121 -1.1485744 -0.93616587 -0.45070034]]...]
INFO - root - 2017-12-09 05:50:38.499749: step 1710, loss = 1.22, batch loss = 0.69 (12.0 examples/sec; 0.664 sec/batch; 61h:01m:19s remains)
INFO - root - 2017-12-09 05:50:44.765751: step 1720, loss = 1.21, batch loss = 0.68 (13.1 examples/sec; 0.610 sec/batch; 56h:02m:44s remains)
INFO - root - 2017-12-09 05:50:51.127976: step 1730, loss = 1.22, batch loss = 0.68 (12.6 examples/sec; 0.637 sec/batch; 58h:29m:49s remains)
INFO - root - 2017-12-09 05:50:57.550188: step 1740, loss = 1.21, batch loss = 0.67 (12.9 examples/sec; 0.618 sec/batch; 56h:47m:15s remains)
INFO - root - 2017-12-09 05:51:04.212474: step 1750, loss = 1.21, batch loss = 0.67 (12.5 examples/sec; 0.639 sec/batch; 58h:43m:03s remains)
INFO - root - 2017-12-09 05:51:10.619853: step 1760, loss = 1.21, batch loss = 0.67 (12.5 examples/sec; 0.642 sec/batch; 59h:01m:28s remains)
INFO - root - 2017-12-09 05:51:16.934939: step 1770, loss = 1.23, batch loss = 0.69 (12.7 examples/sec; 0.628 sec/batch; 57h:39m:49s remains)
INFO - root - 2017-12-09 05:51:23.287766: step 1780, loss = 1.22, batch loss = 0.67 (12.6 examples/sec; 0.636 sec/batch; 58h:28m:13s remains)
INFO - root - 2017-12-09 05:51:29.463682: step 1790, loss = 1.21, batch loss = 0.66 (16.2 examples/sec; 0.493 sec/batch; 45h:19m:18s remains)
INFO - root - 2017-12-09 05:51:35.756667: step 1800, loss = 1.22, batch loss = 0.67 (13.0 examples/sec; 0.618 sec/batch; 56h:44m:04s remains)
2017-12-09 05:51:36.476268: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22059751 0.22667092 0.23168474 0.23751786 0.24312827 0.24778512 0.25071242 0.25252324 0.25348896 0.25479382 0.2559154 0.258412 0.26087227 0.26273781 0.23413348][0.21900848 0.22831658 0.23663804 0.24541405 0.25404027 0.26126012 0.26584709 0.26857615 0.2696524 0.26994792 0.26980236 0.27017808 0.27163059 0.274205 0.24405757][0.1951592 0.20669821 0.21721849 0.22815746 0.23884585 0.24776816 0.25366151 0.25676218 0.25701645 0.2560465 0.2548494 0.25515756 0.25795218 0.26294839 0.23836029][0.17796171 0.18969628 0.20042306 0.21088627 0.22112226 0.23003194 0.23634234 0.23881155 0.23703805 0.23340723 0.2302509 0.23048234 0.23559776 0.24435246 0.22745579][0.16440898 0.17381969 0.18147472 0.18814 0.19526976 0.20294949 0.20920968 0.21050858 0.20589367 0.19843131 0.19275409 0.19312942 0.20158067 0.2157951 0.208651][0.14876813 0.15314853 0.15464446 0.155655 0.1585103 0.16452324 0.17008647 0.16979644 0.16203672 0.15075123 0.14322218 0.14482555 0.15788358 0.17836058 0.18244547][0.11965355 0.11799142 0.11196023 0.10619712 0.10495582 0.10964236 0.11481231 0.11349502 0.10380143 0.090329558 0.082474709 0.086027056 0.10383844 0.13048023 0.14561865][0.074176371 0.066366225 0.05314441 0.041087881 0.03587231 0.039719239 0.044980466 0.044178709 0.034162417 0.020864591 0.014364019 0.020383403 0.043003678 0.076177746 0.10207391][0.018175915 0.0069835484 -0.0094071031 -0.025063232 -0.032673255 -0.02978678 -0.024016067 -0.023626208 -0.032430112 -0.043645874 -0.048245952 -0.039409176 -0.013736814 0.022554919 0.054783478][-0.039317086 -0.051304713 -0.067484878 -0.082466349 -0.090150349 -0.088625826 -0.083627872 -0.082601249 -0.088843234 -0.097048007 -0.099876121 -0.090616383 -0.066108659 -0.030508503 0.0044847578][-0.086840793 -0.096628726 -0.10975927 -0.12149943 -0.12759171 -0.12703604 -0.12328205 -0.12207353 -0.12556157 -0.13131423 -0.13308676 -0.12476079 -0.10373725 -0.072715633 -0.041266128][-0.11270823 -0.1182461 -0.12647705 -0.13296269 -0.13672087 -0.13637076 -0.13334368 -0.13254221 -0.13432242 -0.13860023 -0.13994461 -0.13412659 -0.11894776 -0.094829507 -0.0710491][-0.1054725 -0.10639168 -0.10977152 -0.11037247 -0.11183606 -0.11173416 -0.1101238 -0.11034109 -0.11175966 -0.11571538 -0.11757086 -0.11486302 -0.10566799 -0.090303324 -0.076404646][-0.06700886 -0.06334734 -0.062699512 -0.058357 -0.05835104 -0.058639631 -0.058502048 -0.059128731 -0.059910536 -0.063077778 -0.065142244 -0.065224543 -0.061139941 -0.053442821 -0.048980132][-0.0048556477 0.0022615641 0.0054002404 0.011156499 0.011274531 0.011161536 0.010916695 0.010638967 0.010397255 0.0088133365 0.0073208958 0.007081598 0.0086796731 0.011044279 0.0081940591]]...]
INFO - root - 2017-12-09 05:51:42.699846: step 1810, loss = 1.25, batch loss = 0.69 (12.3 examples/sec; 0.651 sec/batch; 59h:47m:34s remains)
INFO - root - 2017-12-09 05:51:49.185691: step 1820, loss = 1.28, batch loss = 0.73 (12.3 examples/sec; 0.653 sec/batch; 59h:57m:22s remains)
INFO - root - 2017-12-09 05:51:55.649204: step 1830, loss = 1.26, batch loss = 0.70 (12.3 examples/sec; 0.653 sec/batch; 59h:56m:26s remains)
INFO - root - 2017-12-09 05:52:02.093285: step 1840, loss = 1.31, batch loss = 0.74 (12.3 examples/sec; 0.650 sec/batch; 59h:40m:26s remains)
INFO - root - 2017-12-09 05:52:08.614572: step 1850, loss = 1.26, batch loss = 0.69 (12.5 examples/sec; 0.639 sec/batch; 58h:41m:01s remains)
INFO - root - 2017-12-09 05:52:14.957220: step 1860, loss = 1.29, batch loss = 0.71 (12.6 examples/sec; 0.637 sec/batch; 58h:30m:03s remains)
INFO - root - 2017-12-09 05:52:21.176652: step 1870, loss = 1.25, batch loss = 0.66 (13.1 examples/sec; 0.610 sec/batch; 56h:02m:23s remains)
INFO - root - 2017-12-09 05:52:27.475110: step 1880, loss = 1.29, batch loss = 0.70 (12.3 examples/sec; 0.649 sec/batch; 59h:37m:42s remains)
INFO - root - 2017-12-09 05:52:33.749977: step 1890, loss = 1.29, batch loss = 0.69 (14.0 examples/sec; 0.572 sec/batch; 52h:32m:49s remains)
INFO - root - 2017-12-09 05:52:39.800619: step 1900, loss = 1.28, batch loss = 0.68 (12.6 examples/sec; 0.635 sec/batch; 58h:16m:27s remains)
2017-12-09 05:52:40.454743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0015369058 0.00022275746 -0.019735962 -0.038366944 -0.049910158 -0.051528051 -0.0085726678 -0.023813695 0.019119665 0.046127021 0.045739025 0.00018629432 -0.050627783 -0.0070889592 0.023027673][0.022569552 0.0077069551 -0.044871897 -0.035437316 -0.020815343 -0.024860442 -0.044818327 -0.028678343 -0.0063121915 0.016308755 0.032987237 -0.0021898448 -0.048823446 -0.015175581 -0.049298376][-0.029998213 -0.02210559 -0.026534915 -0.047380909 -0.018581793 -0.017578781 -0.058090448 -0.045507327 -0.028080657 -0.010321721 0.0030314177 0.009717226 -0.0097527653 0.001817137 -0.060432687][-0.036538675 -0.052991539 -0.04756698 -0.066063829 -0.078230575 -0.081960462 -0.0779242 -0.068526573 -0.056149557 -0.043465972 -0.033279762 -0.024655953 -0.023529351 -0.091567211 -0.1503848][-0.04892002 -0.059743628 -0.0751749 -0.090116717 -0.099961832 -0.10278118 -0.099587955 -0.092924312 -0.084798686 -0.076308616 -0.068433143 -0.061947986 -0.08420179 -0.078672223 -0.057414308][-0.083230719 -0.090640776 -0.10135924 -0.11176951 -0.11847184 -0.11998745 -0.11747301 -0.11319531 -0.10835959 -0.10282315 -0.0962733 -0.088404641 -0.10900073 -0.19480886 -0.26194873][-0.1106738 -0.11455416 -0.12073507 -0.12671554 -0.13041726 -0.13087538 -0.1290372 -0.12638903 -0.12333498 -0.11909437 -0.11275338 -0.10555973 -0.098509751 -0.18222533 -0.29491162][-0.12794936 -0.12926564 -0.13173044 -0.13416258 -0.13547899 -0.13539334 -0.13430211 -0.13274758 -0.13052228 -0.12663621 -0.12008314 -0.11139492 -0.10258015 -0.095705718 -0.10824481][-0.13669324 -0.13505077 -0.13531086 -0.13598502 -0.13622797 -0.13598689 -0.13534638 -0.1343234 -0.13243848 -0.1287438 -0.12235246 -0.11400186 -0.105784 -0.09985581 -0.096609771][-0.13842824 -0.1354173 -0.13488658 -0.13506378 -0.1351179 -0.13500242 -0.13466541 -0.12893228 -0.12756355 -0.1293629 -0.12395962 -0.11702684 -0.11047185 -0.10629448 -0.12799205][-0.13798749 -0.13449016 -0.13355747 -0.13369663 -0.13376936 -0.13373436 -0.13354243 -0.1330865 -0.13201562 -0.11916935 -0.11557218 -0.12520358 -0.12526859 -0.11860845 -0.13858859][-0.13744795 -0.13385984 -0.13263339 -0.13274503 -0.13280106 -0.13277736 -0.13264525 -0.13315381 -0.13323805 -0.13101295 -0.12790269 -0.12506588 -0.12267366 -0.12165801 -0.12206151][-0.11336655 -0.1339325 -0.1324397 -0.13247719 -0.1324738 -0.1324141 -0.13228494 -0.1355792 -0.13902159 -0.13470961 -0.12966299 -0.12830383 -0.12729393 -0.12709932 -0.1276882][-0.13779724 -0.088712774 -0.073251866 -0.11446059 -0.086292334 -0.1037961 -0.13252084 -0.13233162 -0.13205528 -0.13165089 -0.1311108 -0.13055108 -0.13019907 -0.13026789 -0.13072234][-0.06088759 -0.089765929 -0.13347837 -0.10606678 -0.10931243 -0.1218932 -0.11104082 -0.12244684 -0.13286805 -0.13264708 -0.13238874 -0.13214748 -0.13202265 -0.13210937 -0.13238759]]...]
INFO - root - 2017-12-09 05:52:46.939949: step 1910, loss = 1.29, batch loss = 0.68 (12.1 examples/sec; 0.663 sec/batch; 60h:51m:50s remains)
INFO - root - 2017-12-09 05:52:53.363398: step 1920, loss = 1.27, batch loss = 0.66 (12.7 examples/sec; 0.632 sec/batch; 58h:01m:50s remains)
INFO - root - 2017-12-09 05:52:59.706676: step 1930, loss = 1.30, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:07m:52s remains)
INFO - root - 2017-12-09 05:53:06.106403: step 1940, loss = 1.29, batch loss = 0.67 (12.1 examples/sec; 0.662 sec/batch; 60h:45m:03s remains)
INFO - root - 2017-12-09 05:53:12.639791: step 1950, loss = 1.30, batch loss = 0.68 (11.9 examples/sec; 0.671 sec/batch; 61h:35m:26s remains)
INFO - root - 2017-12-09 05:53:19.103256: step 1960, loss = 1.35, batch loss = 0.73 (12.4 examples/sec; 0.645 sec/batch; 59h:11m:42s remains)
INFO - root - 2017-12-09 05:53:25.523350: step 1970, loss = 1.32, batch loss = 0.69 (12.3 examples/sec; 0.648 sec/batch; 59h:29m:19s remains)
INFO - root - 2017-12-09 05:53:31.967864: step 1980, loss = 1.31, batch loss = 0.69 (12.8 examples/sec; 0.627 sec/batch; 57h:34m:40s remains)
INFO - root - 2017-12-09 05:53:38.533812: step 1990, loss = 1.31, batch loss = 0.68 (12.0 examples/sec; 0.666 sec/batch; 61h:09m:15s remains)
INFO - root - 2017-12-09 05:53:44.516252: step 2000, loss = 1.43, batch loss = 0.80 (12.8 examples/sec; 0.627 sec/batch; 57h:34m:16s remains)
2017-12-09 05:53:45.211879: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.9812175 0.98516166 0.98871541 0.99365771 0.99738741 1.0002109 1.0018539 1.0021005 1.0014516 0.99822748 0.99323833 0.98808694 0.98296511 0.97844708 0.96707249][1.0381804 1.0497146 1.0599178 1.0680023 1.0730675 1.0756296 1.0755581 1.0732603 1.0692368 1.0627961 1.054684 1.0448345 1.0348912 1.025916 1.0118746][1.0684614 1.0903032 1.1089538 1.1230751 1.1312287 1.1338406 1.1320305 1.1269354 1.1192324 1.1084751 1.0946631 1.0783601 1.0612959 1.0452838 1.0229983][1.1060982 1.1424334 1.1722322 1.1938689 1.2054012 1.2078706 1.203805 1.1956459 1.184243 1.1684746 1.1476963 1.1224247 1.0951189 1.0686743 1.0378709][1.1506338 1.2058206 1.2487466 1.278996 1.2938889 1.2956021 1.2883857 1.2763078 1.2604413 1.2391953 1.2108867 1.1757132 1.1366649 1.0978912 1.0569172][1.1954607 1.2725323 1.3287668 1.3671272 1.3844914 1.3842777 1.3728623 1.3561711 1.3356514 1.3090664 1.2735952 1.2289636 1.1783849 1.127383 1.0747873][1.2293925 1.3275086 1.394932 1.440012 1.4591117 1.456825 1.4413843 1.4202757 1.3956181 1.3647095 1.3237537 1.2717701 1.2121484 1.1514441 1.0913286][1.2426745 1.356644 1.4326856 1.4813888 1.500908 1.4965236 1.4777256 1.4533632 1.4261528 1.3930119 1.3494722 1.2939482 1.2297266 1.1639106 1.0999572][1.2295779 1.3520094 1.4301788 1.4784776 1.4972405 1.4917789 1.4713717 1.4456912 1.4177165 1.3848037 1.3422477 1.288016 1.2252178 1.1608806 1.1014493][1.189958 1.3122109 1.3872976 1.4315987 1.4491416 1.4444128 1.4253641 1.4009322 1.3743891 1.3440276 1.3055403 1.2569904 1.2011136 1.1440214 1.0937109][1.1350462 1.2481081 1.314895 1.3509668 1.3655677 1.3618064 1.3458815 1.3252966 1.3030905 1.2778679 1.2462239 1.2066846 1.1615849 1.1159089 1.0759771][1.0732646 1.1709976 1.2268825 1.2527918 1.2635752 1.2613038 1.2500935 1.2355111 1.2195581 1.2012398 1.1782874 1.1496465 1.1173304 1.0848303 1.0565834][1.0160896 1.0981421 1.1443859 1.1603383 1.1670259 1.1654739 1.1583272 1.1494 1.1395909 1.1284196 1.1141126 1.0961111 1.0759338 1.0558083 1.0385249][0.97762549 1.0465266 1.0812075 1.0895127 1.0928016 1.0914348 1.0868961 1.0815957 1.0758423 1.069539 1.0618244 1.0525798 1.0422959 1.0317845 1.0229019][0.93418026 0.99236429 1.0211936 1.0226974 1.0213611 1.017651 1.0123792 1.0084876 1.0042659 0.99962616 0.99552417 0.9898268 0.98386621 0.9778738 0.97240281]]...]
INFO - root - 2017-12-09 05:53:51.551273: step 2010, loss = 1.44, batch loss = 0.81 (12.2 examples/sec; 0.653 sec/batch; 59h:57m:24s remains)
INFO - root - 2017-12-09 05:53:57.936325: step 2020, loss = 1.56, batch loss = 0.92 (13.1 examples/sec; 0.612 sec/batch; 56h:08m:54s remains)
INFO - root - 2017-12-09 05:54:04.181616: step 2030, loss = 1.48, batch loss = 0.84 (12.9 examples/sec; 0.620 sec/batch; 56h:54m:11s remains)
INFO - root - 2017-12-09 05:54:10.648649: step 2040, loss = 1.47, batch loss = 0.83 (12.0 examples/sec; 0.668 sec/batch; 61h:16m:26s remains)
INFO - root - 2017-12-09 05:54:17.071993: step 2050, loss = 1.36, batch loss = 0.72 (12.3 examples/sec; 0.652 sec/batch; 59h:52m:37s remains)
INFO - root - 2017-12-09 05:54:23.627054: step 2060, loss = 1.41, batch loss = 0.76 (12.4 examples/sec; 0.647 sec/batch; 59h:25m:26s remains)
INFO - root - 2017-12-09 05:54:30.124110: step 2070, loss = 1.41, batch loss = 0.76 (12.7 examples/sec; 0.630 sec/batch; 57h:46m:47s remains)
INFO - root - 2017-12-09 05:54:36.523683: step 2080, loss = 1.40, batch loss = 0.75 (12.8 examples/sec; 0.624 sec/batch; 57h:14m:25s remains)
INFO - root - 2017-12-09 05:54:42.778141: step 2090, loss = 1.39, batch loss = 0.74 (12.9 examples/sec; 0.619 sec/batch; 56h:48m:15s remains)
INFO - root - 2017-12-09 05:54:48.844711: step 2100, loss = 1.38, batch loss = 0.72 (11.6 examples/sec; 0.692 sec/batch; 63h:33m:14s remains)
2017-12-09 05:54:49.573226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.65410006 -0.67861855 -0.69830376 -0.71042949 -0.71553189 -0.71759957 -0.71786124 -0.71719742 -0.71117568 -0.694256 -0.66395968 -0.6233831 -0.58630818 -0.55525976 -0.52313787][-0.69168705 -0.72545439 -0.75319231 -0.77077079 -0.7789948 -0.78241897 -0.78334934 -0.78120548 -0.77253187 -0.75029337 -0.71194035 -0.66061813 -0.60893017 -0.5672701 -0.52831376][-0.67230707 -0.71189493 -0.74409449 -0.76452792 -0.77410966 -0.77777117 -0.77880377 -0.77646136 -0.76608336 -0.74041909 -0.69542176 -0.63496751 -0.57249886 -0.5228495 -0.48050612][-0.665381 -0.70789981 -0.74241722 -0.76445955 -0.77495646 -0.77832735 -0.77858657 -0.77583349 -0.76415044 -0.73501211 -0.68511379 -0.62010664 -0.55282104 -0.49876344 -0.45152891][-0.67271423 -0.71588731 -0.74882859 -0.76895106 -0.77797121 -0.78061879 -0.78028166 -0.77676868 -0.7643593 -0.73385614 -0.68273103 -0.61670864 -0.54952776 -0.49641818 -0.44854516][-0.69039559 -0.72975612 -0.75782996 -0.77472121 -0.78186041 -0.78331214 -0.7823568 -0.77821219 -0.76553893 -0.7363897 -0.68817961 -0.6268248 -0.56542253 -0.51646149 -0.47134393][-0.7129572 -0.745609 -0.76706052 -0.77974796 -0.78433275 -0.78479886 -0.78340757 -0.77895218 -0.76627189 -0.73969984 -0.69699013 -0.643669 -0.59132409 -0.551283 -0.51133639][-0.73240381 -0.75713927 -0.77280694 -0.78233242 -0.78557253 -0.78555423 -0.78398937 -0.7794463 -0.76774383 -0.74454826 -0.7089262 -0.66578835 -0.62451166 -0.59547 -0.56139636][-0.74843609 -0.76364446 -0.77346355 -0.78049493 -0.78325981 -0.78376156 -0.78308821 -0.77944052 -0.76974928 -0.75118047 -0.72391617 -0.69233561 -0.66382933 -0.64524519 -0.61839545][-0.75688905 -0.76131207 -0.766583 -0.7720629 -0.77546316 -0.77789652 -0.77955669 -0.77847904 -0.77242446 -0.759613 -0.74061358 -0.71938014 -0.70160395 -0.69112939 -0.66891509][-0.7563324 -0.74932015 -0.74961293 -0.753352 -0.75753993 -0.76273125 -0.76851785 -0.77248651 -0.77227396 -0.76648271 -0.75605738 -0.74440062 -0.7350347 -0.73020095 -0.7096194][-0.74428964 -0.72634083 -0.72003937 -0.72089112 -0.72627872 -0.73518425 -0.74638003 -0.75693578 -0.76463157 -0.7672736 -0.76564461 -0.76198721 -0.75919473 -0.75850934 -0.74039495][-0.72376877 -0.69401366 -0.67765981 -0.67234111 -0.6759901 -0.68790996 -0.70609021 -0.72578347 -0.74321514 -0.75517666 -0.76202369 -0.76594728 -0.76943219 -0.77295655 -0.7597602][-0.70060015 -0.65739477 -0.62659794 -0.610214 -0.60883892 -0.62002003 -0.64269251 -0.6704182 -0.69790787 -0.719984 -0.73667949 -0.7485562 -0.75829381 -0.76834148 -0.76151192][-0.6800307 -0.62463814 -0.58046007 -0.55046719 -0.53850609 -0.54444021 -0.56694251 -0.59880072 -0.63291615 -0.6625089 -0.6883111 -0.71201211 -0.73317426 -0.75172693 -0.75427264]]...]
INFO - root - 2017-12-09 05:54:55.921870: step 2110, loss = 1.38, batch loss = 0.73 (13.0 examples/sec; 0.616 sec/batch; 56h:31m:07s remains)
INFO - root - 2017-12-09 05:55:02.323348: step 2120, loss = 1.40, batch loss = 0.74 (12.5 examples/sec; 0.638 sec/batch; 58h:34m:04s remains)
INFO - root - 2017-12-09 05:55:08.805641: step 2130, loss = 1.36, batch loss = 0.70 (12.2 examples/sec; 0.655 sec/batch; 60h:06m:20s remains)
INFO - root - 2017-12-09 05:55:15.232246: step 2140, loss = 1.36, batch loss = 0.69 (12.4 examples/sec; 0.646 sec/batch; 59h:15m:59s remains)
INFO - root - 2017-12-09 05:55:21.583948: step 2150, loss = 1.56, batch loss = 0.90 (12.6 examples/sec; 0.634 sec/batch; 58h:12m:59s remains)
INFO - root - 2017-12-09 05:55:27.805723: step 2160, loss = 1.36, batch loss = 0.70 (13.4 examples/sec; 0.596 sec/batch; 54h:43m:07s remains)
INFO - root - 2017-12-09 05:55:34.164195: step 2170, loss = 1.36, batch loss = 0.69 (12.4 examples/sec; 0.646 sec/batch; 59h:14m:02s remains)
INFO - root - 2017-12-09 05:55:40.517122: step 2180, loss = 1.36, batch loss = 0.69 (12.2 examples/sec; 0.655 sec/batch; 60h:03m:23s remains)
INFO - root - 2017-12-09 05:55:46.963903: step 2190, loss = 1.42, batch loss = 0.75 (12.8 examples/sec; 0.626 sec/batch; 57h:23m:39s remains)
INFO - root - 2017-12-09 05:55:52.923442: step 2200, loss = 1.48, batch loss = 0.80 (12.7 examples/sec; 0.632 sec/batch; 58h:01m:18s remains)
2017-12-09 05:55:53.606700: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.47400635 0.40996373 0.39857084 0.49308121 0.60290122 0.86725706 1.092666 1.1946533 1.1807336 1.1383507 1.1750627 0.72186619 0.27855194 0.24661653 0.16593979][0.22108866 0.17585941 0.14952837 0.28470588 0.59771937 0.82637328 1.1670568 1.2165966 1.3065234 1.126477 0.61489332 0.91245538 0.39867789 0.43022645 0.051309913][0.042568952 0.01362817 -0.020493776 0.17596851 0.42096168 0.67736661 0.93850332 1.2952007 1.4382983 1.2322448 0.94328982 0.69989443 0.26227725 0.25142467 0.028372824][-0.067159079 -0.081589863 -0.017851844 0.15536858 0.43397897 0.7678954 1.068276 1.2981998 1.3667713 1.3412958 1.1056113 0.65505028 0.41543174 0.30738777 -0.038951069][-0.090317279 -0.080959521 -0.010450557 0.15557362 0.431342 0.77183145 1.0873593 1.3401767 1.4175389 1.303228 1.0135632 0.57479578 0.19567423 -0.049094379 -0.122525][-0.050677136 -0.0032446235 0.10504146 0.10892002 0.38784349 0.73361742 1.0374564 1.2025161 1.1738499 0.97086948 0.71121007 0.4829762 0.25971586 -0.62323022 -0.0787881][-0.11847724 -0.1421435 -0.036809191 0.073061749 0.34599459 0.6795935 1.131561 0.54943913 1.1659443 0.950151 0.60748827 0.29122716 0.14414705 -0.62610769 -0.13566746][-0.12127401 -0.15292774 -0.070482209 0.018930405 0.26172179 0.56801194 0.85216254 1.4135659 0.93519109 0.77627861 0.59707129 0.33473271 0.063463345 -0.30900469 -0.34383082][-0.13291471 -0.16952908 -0.11215383 -0.046744183 0.14957739 0.41099191 0.79290342 0.8223936 0.80842263 0.45748526 -0.0781025 -0.20192702 -0.28046206 -0.067231424 -0.0944254][-0.22924767 -0.39714798 -0.14274357 -0.10999056 0.030636668 0.23582275 0.44323462 0.5877533 0.61172229 0.50730312 0.07574828 -0.22932786 -0.41304457 -0.42975253 -0.34643644][-0.31360218 -0.22741172 -0.39773464 -0.20934187 -0.18923378 0.066802546 0.22316183 0.35204768 0.38680929 0.69501561 0.50573242 -0.088268414 -0.12398642 -0.14551684 -0.18280448][-0.19753797 -0.40046522 -0.27486593 -0.69623023 -0.77824032 -0.1667408 -0.089371279 0.093471214 0.112693 0.14577971 0.072049305 0.24490936 -0.10190506 -0.15470964 -0.18814304][-0.18293402 -0.19149229 -0.2021614 -0.92315573 -0.89435762 -0.81482369 -0.62207156 -0.11767145 -0.076187372 -0.070137605 -0.12300609 -0.14102605 -0.14881368 -0.18327573 -0.20695554][-0.1939266 -0.19578892 -0.21038058 -0.20582838 -0.11189527 -0.48038167 -0.37137866 -0.20103002 -0.1938968 -0.23829828 -0.14082877 -0.15657242 -0.17381811 -0.18287095 -0.19474527][-0.18920989 -0.1909394 -0.19445148 -0.20474108 -0.20264313 -0.18131007 -0.17446408 -0.19055378 -0.18397808 -0.20844381 -0.17171441 -0.17754871 -0.18474543 -0.18819961 -0.19208394]]...]
INFO - root - 2017-12-09 05:55:59.877566: step 2210, loss = 1.46, batch loss = 0.79 (12.4 examples/sec; 0.647 sec/batch; 59h:21m:05s remains)
INFO - root - 2017-12-09 05:56:06.178550: step 2220, loss = 1.36, batch loss = 0.68 (12.7 examples/sec; 0.628 sec/batch; 57h:35m:33s remains)
INFO - root - 2017-12-09 05:56:12.421005: step 2230, loss = 1.34, batch loss = 0.66 (13.0 examples/sec; 0.614 sec/batch; 56h:18m:51s remains)
INFO - root - 2017-12-09 05:56:18.889735: step 2240, loss = 1.32, batch loss = 0.64 (12.1 examples/sec; 0.659 sec/batch; 60h:26m:03s remains)
INFO - root - 2017-12-09 05:56:25.338604: step 2250, loss = 1.34, batch loss = 0.66 (11.8 examples/sec; 0.675 sec/batch; 61h:56m:26s remains)
INFO - root - 2017-12-09 05:56:31.901074: step 2260, loss = 1.37, batch loss = 0.68 (12.5 examples/sec; 0.642 sec/batch; 58h:52m:52s remains)
INFO - root - 2017-12-09 05:56:38.408556: step 2270, loss = 1.37, batch loss = 0.68 (12.7 examples/sec; 0.630 sec/batch; 57h:44m:48s remains)
INFO - root - 2017-12-09 05:56:44.730771: step 2280, loss = 1.40, batch loss = 0.71 (12.9 examples/sec; 0.623 sec/batch; 57h:06m:09s remains)
INFO - root - 2017-12-09 05:56:50.984889: step 2290, loss = 1.38, batch loss = 0.69 (13.2 examples/sec; 0.608 sec/batch; 55h:44m:47s remains)
INFO - root - 2017-12-09 05:56:56.577153: step 2300, loss = 1.35, batch loss = 0.66 (13.4 examples/sec; 0.597 sec/batch; 54h:47m:49s remains)
2017-12-09 05:56:57.294199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243][-0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243 -0.18761243]]...]
INFO - root - 2017-12-09 05:57:03.651406: step 2310, loss = 1.38, batch loss = 0.69 (12.3 examples/sec; 0.651 sec/batch; 59h:43m:53s remains)
INFO - root - 2017-12-09 05:57:10.085529: step 2320, loss = 1.38, batch loss = 0.68 (12.7 examples/sec; 0.631 sec/batch; 57h:50m:46s remains)
INFO - root - 2017-12-09 05:57:16.499789: step 2330, loss = 1.38, batch loss = 0.68 (12.2 examples/sec; 0.654 sec/batch; 59h:58m:13s remains)
INFO - root - 2017-12-09 05:57:22.927896: step 2340, loss = 1.39, batch loss = 0.69 (12.6 examples/sec; 0.636 sec/batch; 58h:19m:32s remains)
INFO - root - 2017-12-09 05:57:29.256785: step 2350, loss = 1.40, batch loss = 0.70 (13.2 examples/sec; 0.604 sec/batch; 55h:24m:36s remains)
INFO - root - 2017-12-09 05:57:35.624538: step 2360, loss = 1.38, batch loss = 0.68 (12.1 examples/sec; 0.664 sec/batch; 60h:51m:43s remains)
INFO - root - 2017-12-09 05:57:41.900151: step 2370, loss = 1.40, batch loss = 0.69 (12.5 examples/sec; 0.641 sec/batch; 58h:49m:05s remains)
INFO - root - 2017-12-09 05:57:48.235299: step 2380, loss = 1.37, batch loss = 0.66 (12.7 examples/sec; 0.629 sec/batch; 57h:42m:22s remains)
INFO - root - 2017-12-09 05:57:54.642155: step 2390, loss = 1.38, batch loss = 0.67 (12.4 examples/sec; 0.646 sec/batch; 59h:13m:24s remains)
INFO - root - 2017-12-09 05:58:00.792568: step 2400, loss = 1.39, batch loss = 0.68 (19.1 examples/sec; 0.420 sec/batch; 38h:29m:47s remains)
2017-12-09 05:58:01.420231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.18806732 -0.1880673 -0.18806718 -0.18806706 -0.18806697 -0.18806697 -0.18806705 -0.18806712 -0.18806717 -0.18806726 -0.18806735 -0.18806742 -0.18806741 -0.18806756 -0.18806757][-0.18806703 -0.18806696 -0.18806675 -0.1880665 -0.18806629 -0.18806626 -0.18806633 -0.18806642 -0.18806654 -0.18806678 -0.18806703 -0.18806727 -0.18806747 -0.18806767 -0.18806756][-0.18806688 -0.18806672 -0.18806645 -0.18806621 -0.18806608 -0.18806605 -0.18806605 -0.18806618 -0.18806629 -0.18806653 -0.18806696 -0.18806744 -0.18806775 -0.18806797 -0.18806793][-0.18806662 -0.18806635 -0.18806615 -0.18806592 -0.18806574 -0.18806577 -0.18806587 -0.18806605 -0.18806624 -0.18806653 -0.18806686 -0.18806738 -0.18806791 -0.18806808 -0.18806802][-0.18806648 -0.18806617 -0.18806596 -0.18806578 -0.18806571 -0.1880658 -0.18806586 -0.18806612 -0.18806633 -0.1880666 -0.18806702 -0.18806759 -0.18806794 -0.18806799 -0.18806793][-0.18806633 -0.18806611 -0.18806598 -0.18806589 -0.18806589 -0.18806615 -0.18806642 -0.18806672 -0.18806686 -0.18806709 -0.18806733 -0.18806766 -0.18806785 -0.18806785 -0.18806776][-0.1880662 -0.1880662 -0.18806611 -0.18806623 -0.18806636 -0.18806671 -0.18806697 -0.18806724 -0.1880673 -0.18806738 -0.18806747 -0.18806759 -0.1880676 -0.18806757 -0.18806754][-0.18806604 -0.18806632 -0.18806654 -0.1880669 -0.18806715 -0.18806747 -0.18806751 -0.18806759 -0.18806748 -0.18806742 -0.18806741 -0.18806742 -0.18806745 -0.18806745 -0.18806745][-0.1880662 -0.18806665 -0.18806709 -0.18806764 -0.18806803 -0.1880682 -0.188068 -0.18806782 -0.18806754 -0.18806744 -0.18806738 -0.18806736 -0.18806739 -0.18806747 -0.18806747][-0.18806632 -0.18806703 -0.18806764 -0.18806821 -0.18806843 -0.18806843 -0.18806811 -0.18806779 -0.18806756 -0.18806744 -0.18806736 -0.18806735 -0.18806735 -0.18806738 -0.18806742][-0.18806665 -0.18806714 -0.18806764 -0.1880682 -0.1880683 -0.1880682 -0.18806788 -0.18806762 -0.18806747 -0.18806739 -0.18806735 -0.18806733 -0.18806735 -0.18806739 -0.18806744][-0.18806703 -0.18806739 -0.18806776 -0.18806835 -0.1880683 -0.18806806 -0.18806788 -0.18806767 -0.18806745 -0.18806736 -0.18806733 -0.18806733 -0.18806736 -0.18806745 -0.18806757][-0.18806736 -0.18806756 -0.18806775 -0.18806799 -0.18806812 -0.18806782 -0.18806757 -0.18806745 -0.18806739 -0.18806738 -0.18806735 -0.18806735 -0.18806738 -0.18806741 -0.18806744][-0.1880675 -0.18806763 -0.18806764 -0.18806779 -0.18806781 -0.18806775 -0.18806754 -0.18806745 -0.18806741 -0.18806739 -0.18806738 -0.18806738 -0.18806739 -0.18806742 -0.18806744][-0.1880675 -0.18806756 -0.18806757 -0.18806764 -0.18806781 -0.18806775 -0.18806762 -0.1880675 -0.18806745 -0.18806744 -0.18806742 -0.18806742 -0.18806742 -0.18806742 -0.18806744]]...]
INFO - root - 2017-12-09 05:58:07.836969: step 2410, loss = 1.39, batch loss = 0.68 (12.5 examples/sec; 0.639 sec/batch; 58h:33m:53s remains)
INFO - root - 2017-12-09 05:58:14.235797: step 2420, loss = 1.39, batch loss = 0.68 (12.3 examples/sec; 0.648 sec/batch; 59h:23m:53s remains)
INFO - root - 2017-12-09 05:58:20.647733: step 2430, loss = 1.40, batch loss = 0.69 (12.5 examples/sec; 0.639 sec/batch; 58h:36m:13s remains)
INFO - root - 2017-12-09 05:58:26.990981: step 2440, loss = 1.39, batch loss = 0.67 (12.7 examples/sec; 0.628 sec/batch; 57h:35m:11s remains)
INFO - root - 2017-12-09 05:58:33.318567: step 2450, loss = 1.36, batch loss = 0.64 (12.8 examples/sec; 0.627 sec/batch; 57h:31m:08s remains)
INFO - root - 2017-12-09 05:58:39.710778: step 2460, loss = 1.39, batch loss = 0.67 (12.7 examples/sec; 0.629 sec/batch; 57h:39m:02s remains)
INFO - root - 2017-12-09 05:58:45.978414: step 2470, loss = 1.40, batch loss = 0.68 (12.9 examples/sec; 0.620 sec/batch; 56h:47m:57s remains)
INFO - root - 2017-12-09 05:58:52.266633: step 2480, loss = 1.43, batch loss = 0.71 (12.5 examples/sec; 0.642 sec/batch; 58h:48m:55s remains)
INFO - root - 2017-12-09 05:58:58.766503: step 2490, loss = 1.40, batch loss = 0.68 (12.3 examples/sec; 0.651 sec/batch; 59h:40m:35s remains)
INFO - root - 2017-12-09 05:59:05.195007: step 2500, loss = 1.41, batch loss = 0.68 (12.4 examples/sec; 0.647 sec/batch; 59h:16m:14s remains)
2017-12-09 05:59:05.775370: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.13978326 -0.11458937 -0.096777409 -0.0932398 -0.10305832 -0.1229368 -0.14587288 -0.16759548 -0.1829565 -0.18894906 -0.18800274 -0.18532464 -0.1826615 -0.18253678 -0.18376862][-0.042720407 -0.00021386147 0.024572521 0.022739097 0.00016027689 -0.039129481 -0.08297696 -0.12176561 -0.1487008 -0.16096486 -0.16346616 -0.16106281 -0.16089097 -0.16430795 -0.17141955][0.084678769 0.14839765 0.18267038 0.17699349 0.1421248 0.084476858 0.024310648 -0.032275841 -0.074497737 -0.094155617 -0.10055082 -0.10444496 -0.11293689 -0.12729956 -0.1453355][0.18722367 0.27108371 0.31840962 0.31827283 0.28393954 0.22266117 0.15711126 0.095333219 0.046816409 0.017665103 0.00019173324 -0.012429193 -0.035631523 -0.067761458 -0.10277925][0.22009671 0.31869 0.37843543 0.39249712 0.37454933 0.33133858 0.28307971 0.23172358 0.18837649 0.15583414 0.12876377 0.10302004 0.061725989 0.0059259683 -0.05031018][0.17866504 0.28778175 0.35997468 0.39190364 0.39438802 0.37809241 0.35550761 0.32830244 0.30093014 0.26867861 0.23728591 0.20056972 0.14287299 0.07094571 -0.0018016249][0.10700768 0.22380024 0.30460826 0.34622163 0.36228788 0.36285406 0.35888249 0.34990335 0.33624786 0.30866614 0.275383 0.23526147 0.18088171 0.10414189 0.023897797][0.037899241 0.15662146 0.24137387 0.28616551 0.30553558 0.30836803 0.30524498 0.29743266 0.2852436 0.25686064 0.22739378 0.1912888 0.14660445 0.085620522 0.017646059][-0.030735195 0.08034578 0.16338834 0.2094869 0.22912896 0.22614536 0.21166086 0.19174218 0.17154354 0.13878495 0.1104061 0.08570537 0.06380716 0.028317675 -0.018596277][-0.10775899 -0.012247622 0.067515612 0.11131272 0.12554666 0.11518076 0.093169481 0.061542317 0.031420991 0.0012658238 -0.019527003 -0.030134514 -0.03412725 -0.042671934 -0.065171659][-0.18801533 -0.11576357 -0.049764454 -0.01095438 0.0018607825 -0.011357695 -0.037184671 -0.072576508 -0.10376278 -0.12669428 -0.1387227 -0.1367676 -0.12753278 -0.11804506 -0.11356027][-0.26722723 -0.22524226 -0.17463858 -0.1421055 -0.12707931 -0.13058379 -0.14745651 -0.16845147 -0.18661679 -0.19808885 -0.201347 -0.19736907 -0.18250652 -0.16940951 -0.15780117][-0.31583697 -0.30270883 -0.27497998 -0.25111592 -0.23340116 -0.22872177 -0.23112345 -0.23437451 -0.23749289 -0.23514442 -0.22814788 -0.21939935 -0.20308472 -0.18967545 -0.17624928][-0.33357942 -0.3407771 -0.33188948 -0.31667835 -0.29495841 -0.27933645 -0.26794249 -0.25343841 -0.24138466 -0.22923236 -0.22042796 -0.21452852 -0.20208412 -0.19365808 -0.18279479][-0.30251628 -0.31934959 -0.32426763 -0.31884646 -0.30182236 -0.28565016 -0.26527113 -0.24222744 -0.22584146 -0.21153826 -0.20548086 -0.2029378 -0.19402857 -0.1903865 -0.18489882]]...]
INFO - root - 2017-12-09 05:59:12.094110: step 2510, loss = 1.39, batch loss = 0.66 (12.4 examples/sec; 0.643 sec/batch; 58h:57m:33s remains)
INFO - root - 2017-12-09 05:59:18.452280: step 2520, loss = 1.39, batch loss = 0.66 (12.6 examples/sec; 0.637 sec/batch; 58h:21m:50s remains)
INFO - root - 2017-12-09 05:59:24.851609: step 2530, loss = 1.42, batch loss = 0.68 (12.4 examples/sec; 0.647 sec/batch; 59h:17m:58s remains)
INFO - root - 2017-12-09 05:59:31.357108: step 2540, loss = 1.42, batch loss = 0.69 (12.5 examples/sec; 0.642 sec/batch; 58h:47m:51s remains)
INFO - root - 2017-12-09 05:59:37.771077: step 2550, loss = 1.42, batch loss = 0.69 (13.0 examples/sec; 0.616 sec/batch; 56h:25m:44s remains)
INFO - root - 2017-12-09 05:59:44.171713: step 2560, loss = 1.43, batch loss = 0.69 (12.1 examples/sec; 0.662 sec/batch; 60h:40m:39s remains)
INFO - root - 2017-12-09 05:59:50.571156: step 2570, loss = 1.41, batch loss = 0.67 (12.3 examples/sec; 0.653 sec/batch; 59h:48m:45s remains)
INFO - root - 2017-12-09 05:59:56.874133: step 2580, loss = 1.40, batch loss = 0.66 (12.7 examples/sec; 0.631 sec/batch; 57h:48m:25s remains)
INFO - root - 2017-12-09 06:00:03.146693: step 2590, loss = 1.42, batch loss = 0.68 (12.4 examples/sec; 0.648 sec/batch; 59h:20m:24s remains)
INFO - root - 2017-12-09 06:00:09.520559: step 2600, loss = 1.42, batch loss = 0.68 (12.7 examples/sec; 0.628 sec/batch; 57h:31m:33s remains)
2017-12-09 06:00:10.157839: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34977445 0.34396175 0.33944348 0.3439413 0.34910813 0.35958102 0.36956164 0.37456688 0.38116392 0.38516197 0.38908002 0.38479045 0.36316839 0.31918487 0.25732589][0.33241722 0.32863835 0.32476196 0.33040914 0.33656797 0.34431127 0.35098347 0.35797778 0.36554572 0.370608 0.37618294 0.37324724 0.35641989 0.31002727 0.24338204][0.31605378 0.31627485 0.31417605 0.31467685 0.31567702 0.3189567 0.32120183 0.32829049 0.33603731 0.34460995 0.35311845 0.35696068 0.35317245 0.31076631 0.23757905][0.29908338 0.30646035 0.30771628 0.30783892 0.30802682 0.30824527 0.30845582 0.30860621 0.308771 0.30892253 0.30899489 0.30903691 0.3056969 0.27744934 0.21417516][0.28701696 0.30095512 0.30495313 0.30504614 0.30518836 0.30528483 0.30540216 0.30547321 0.3055236 0.30561396 0.30564335 0.3056837 0.30233261 0.28421894 0.22548461][0.26806715 0.29263163 0.3019796 0.3019658 0.30200022 0.30204639 0.30211741 0.30214998 0.30219534 0.30222976 0.30220425 0.302256 0.30246896 0.29308006 0.24615201][0.24094528 0.28133705 0.29889885 0.2987898 0.29876351 0.29877904 0.2988821 0.29897028 0.29902315 0.29910541 0.29914826 0.29927054 0.30163613 0.30021176 0.26750413][0.20782679 0.26613328 0.29522297 0.29706717 0.296978 0.297045 0.29716769 0.29731014 0.29743779 0.29754367 0.29763767 0.29780504 0.30009043 0.30319589 0.27846664][0.16722304 0.24429241 0.28995961 0.29677704 0.29668751 0.29671383 0.2968021 0.29692933 0.29700536 0.29711252 0.29722348 0.29737824 0.30813441 0.31729409 0.30104911][0.12510201 0.21924251 0.28317049 0.29686087 0.29669175 0.29668987 0.29673442 0.29679182 0.29680783 0.29685125 0.29693785 0.29710105 0.31117776 0.32209077 0.3148416][0.08997196 0.20017803 0.27439174 0.29721364 0.29703674 0.29699394 0.29697862 0.29695919 0.29693145 0.29694569 0.29700029 0.29711094 0.31113693 0.32450268 0.32751456][0.067035824 0.17893147 0.26202139 0.29311675 0.29086927 0.29084292 0.29081985 0.29074785 0.29068047 0.29062024 0.29061392 0.2906653 0.30459434 0.32030168 0.33286217][0.038559958 0.14781651 0.23373786 0.26420382 0.25996694 0.2591134 0.25698864 0.25699136 0.25699437 0.25692469 0.25688314 0.25687435 0.27070597 0.28651273 0.30536973][0.0063266456 0.10357073 0.17271838 0.19916123 0.18598905 0.18163377 0.17815778 0.17828202 0.17851108 0.17729905 0.17641127 0.17540002 0.18903196 0.20666265 0.22708315][-0.057136118 0.015616208 0.06931603 0.078673363 0.062774539 0.054291129 0.046451047 0.048389912 0.050149187 0.048283353 0.045442358 0.041835994 0.052400336 0.067412794 0.088507444]]...]
INFO - root - 2017-12-09 06:00:16.369099: step 2610, loss = 1.41, batch loss = 0.67 (12.4 examples/sec; 0.647 sec/batch; 59h:15m:27s remains)
INFO - root - 2017-12-09 06:00:22.781718: step 2620, loss = 1.42, batch loss = 0.67 (12.7 examples/sec; 0.630 sec/batch; 57h:45m:08s remains)
INFO - root - 2017-12-09 06:00:29.063360: step 2630, loss = 1.45, batch loss = 0.70 (12.8 examples/sec; 0.623 sec/batch; 57h:03m:38s remains)
INFO - root - 2017-12-09 06:00:35.394468: step 2640, loss = 1.40, batch loss = 0.65 (12.4 examples/sec; 0.645 sec/batch; 59h:03m:27s remains)
INFO - root - 2017-12-09 06:00:41.705870: step 2650, loss = 1.44, batch loss = 0.69 (12.9 examples/sec; 0.619 sec/batch; 56h:44m:55s remains)
INFO - root - 2017-12-09 06:00:48.014684: step 2660, loss = 1.45, batch loss = 0.69 (12.7 examples/sec; 0.632 sec/batch; 57h:55m:50s remains)
INFO - root - 2017-12-09 06:00:54.243677: step 2670, loss = 1.44, batch loss = 0.68 (12.7 examples/sec; 0.632 sec/batch; 57h:55m:50s remains)
INFO - root - 2017-12-09 06:01:00.628067: step 2680, loss = 1.43, batch loss = 0.68 (12.4 examples/sec; 0.644 sec/batch; 59h:01m:10s remains)
INFO - root - 2017-12-09 06:01:06.976656: step 2690, loss = 1.52, batch loss = 0.76 (12.6 examples/sec; 0.637 sec/batch; 58h:19m:29s remains)
INFO - root - 2017-12-09 06:01:13.188208: step 2700, loss = 1.46, batch loss = 0.70 (12.6 examples/sec; 0.636 sec/batch; 58h:17m:00s remains)
2017-12-09 06:01:13.927833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.18794957 -0.16169059 -0.16645561 -0.22279644 -0.14265607 -0.28481826 -0.30544829 -0.66579515 -0.74849975 -0.47025 -0.61559343 -0.48769307 -0.38890564 -0.30939013 -0.27285677][-0.41009077 -0.28245237 -0.10810176 -0.13012184 -0.11047585 -0.28496185 -0.34334052 -0.43456042 -0.5662806 -0.47304642 -0.476588 -0.679882 -0.094189778 0.23448895 -0.256226][-0.21905328 -0.14337274 -0.21786864 -0.16896512 -0.42363828 -0.14961886 -0.43210781 -0.34891611 -0.41573948 -0.40970159 -0.17534861 -0.17878634 -0.25551879 -0.086142056 -0.050558776][-0.092204548 0.098726794 -0.0061873049 -0.15753898 -0.041064397 -0.21020249 -0.39988682 -0.29165956 -0.19025286 -0.43129665 -0.33493817 -0.48699325 -0.27175787 -0.27794355 -0.51261][0.06709154 -0.022345379 0.21575333 0.12445728 0.10935093 0.021765366 0.091349289 0.080571339 -0.34807163 -0.59776896 -0.1214088 -0.049194887 -0.64185619 -0.17958237 0.095238879][-0.15693045 -0.14194761 -0.058023095 0.002730906 0.15426634 -0.23987302 -0.090030834 0.07568951 0.053359911 0.0044682622 -0.14859448 -0.099295087 0.009316355 -0.062564582 -0.17057027][-0.21266317 -0.046614781 -0.10153586 -0.3146835 0.071526334 0.015204385 -0.046052784 -0.010765433 -0.17314343 -0.08703839 0.26397264 0.21215738 0.14678781 -0.04317525 -0.23799786][0.076789215 -0.0058809668 -0.045367941 0.056747377 0.14900224 0.060375288 -0.11399709 0.21168004 0.25789297 0.24221964 0.063811973 -0.018202722 0.23820244 0.12962602 0.11961593][-0.042049527 -0.06104064 0.010941774 -0.17399031 -0.18806258 0.10647561 -0.18232745 -0.058284357 0.070049629 0.17038159 0.18991475 0.31453246 0.30497968 0.016734317 0.67342675][-0.17114252 -0.20323548 -0.21537906 -0.25254086 -0.48069167 -0.033350274 -0.13096447 0.013703778 0.15152495 -0.14688329 -0.2365213 0.081491426 0.30584276 0.29240549 0.46408081][-0.0083923638 0.22336607 -0.044983596 -0.15724248 -0.36708182 -0.33215576 -0.28613013 -0.28850511 -0.41433769 -0.26101863 -0.14764717 -0.11074478 -0.43006894 0.22203393 0.46215886][-0.16451712 0.22959416 -0.00949657 -0.022877514 0.078256354 -0.1510106 -0.31894666 -0.16688311 -0.0736903 -0.30831891 -0.29249865 -0.072654046 0.35255337 0.27424884 0.32529837][0.093001083 0.064072832 -0.090327755 0.06499593 0.037888557 -0.290259 0.28142744 0.065472379 -0.22609779 0.14769308 -0.50954384 -0.35843956 -0.756396 -0.028523758 -0.24181563][0.059257329 -0.062623426 -0.25907329 -0.5580371 -0.59849125 -0.67965889 -0.67517251 0.060184464 0.1814317 -0.0276521 -0.055812806 -0.17131619 -0.59648967 -0.66469324 -0.70178294][0.10933788 -0.30313236 -0.098990045 -0.34227675 -0.70625466 -0.7428745 -0.84422892 -0.72010386 -0.39540982 -0.26343191 0.20082544 0.0053344816 0.25557512 0.16353117 -0.038490653]]...]
INFO - root - 2017-12-09 06:01:19.787359: step 2710, loss = 1.45, batch loss = 0.68 (12.4 examples/sec; 0.644 sec/batch; 58h:57m:41s remains)
INFO - root - 2017-12-09 06:01:26.212155: step 2720, loss = 1.44, batch loss = 0.68 (12.7 examples/sec; 0.630 sec/batch; 57h:41m:04s remains)
INFO - root - 2017-12-09 06:01:32.580368: step 2730, loss = 1.46, batch loss = 0.69 (12.6 examples/sec; 0.635 sec/batch; 58h:10m:19s remains)
INFO - root - 2017-12-09 06:01:39.054106: step 2740, loss = 1.46, batch loss = 0.69 (12.3 examples/sec; 0.651 sec/batch; 59h:36m:12s remains)
INFO - root - 2017-12-09 06:01:45.308644: step 2750, loss = 1.47, batch loss = 0.69 (12.7 examples/sec; 0.632 sec/batch; 57h:54m:00s remains)
INFO - root - 2017-12-09 06:01:51.684366: step 2760, loss = 1.44, batch loss = 0.67 (12.5 examples/sec; 0.641 sec/batch; 58h:43m:27s remains)
INFO - root - 2017-12-09 06:01:58.119707: step 2770, loss = 1.44, batch loss = 0.66 (12.2 examples/sec; 0.656 sec/batch; 60h:02m:39s remains)
INFO - root - 2017-12-09 06:02:04.630817: step 2780, loss = 1.49, batch loss = 0.71 (12.2 examples/sec; 0.656 sec/batch; 60h:02m:28s remains)
INFO - root - 2017-12-09 06:02:11.114643: step 2790, loss = 1.51, batch loss = 0.73 (12.4 examples/sec; 0.643 sec/batch; 58h:51m:56s remains)
INFO - root - 2017-12-09 06:02:17.500884: step 2800, loss = 1.46, batch loss = 0.68 (12.5 examples/sec; 0.642 sec/batch; 58h:47m:15s remains)
2017-12-09 06:02:18.194203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913][-0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913 -0.1891913]]...]
INFO - root - 2017-12-09 06:02:24.125830: step 2810, loss = 1.48, batch loss = 0.69 (12.6 examples/sec; 0.636 sec/batch; 58h:13m:33s remains)
INFO - root - 2017-12-09 06:02:30.551761: step 2820, loss = 1.47, batch loss = 0.68 (12.5 examples/sec; 0.641 sec/batch; 58h:43m:54s remains)
INFO - root - 2017-12-09 06:02:37.007105: step 2830, loss = 1.49, batch loss = 0.70 (12.0 examples/sec; 0.664 sec/batch; 60h:49m:59s remains)
INFO - root - 2017-12-09 06:02:43.417427: step 2840, loss = 1.50, batch loss = 0.71 (12.4 examples/sec; 0.643 sec/batch; 58h:53m:17s remains)
INFO - root - 2017-12-09 06:02:49.968555: step 2850, loss = 1.50, batch loss = 0.70 (12.0 examples/sec; 0.666 sec/batch; 60h:58m:43s remains)
INFO - root - 2017-12-09 06:02:56.281988: step 2860, loss = 1.48, batch loss = 0.68 (12.5 examples/sec; 0.638 sec/batch; 58h:27m:20s remains)
INFO - root - 2017-12-09 06:03:02.578700: step 2870, loss = 1.50, batch loss = 0.69 (13.0 examples/sec; 0.615 sec/batch; 56h:18m:19s remains)
INFO - root - 2017-12-09 06:03:08.944646: step 2880, loss = 1.48, batch loss = 0.67 (12.2 examples/sec; 0.658 sec/batch; 60h:13m:39s remains)
INFO - root - 2017-12-09 06:03:15.419432: step 2890, loss = 1.50, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 59h:40m:22s remains)
INFO - root - 2017-12-09 06:03:21.946072: step 2900, loss = 1.49, batch loss = 0.68 (12.5 examples/sec; 0.641 sec/batch; 58h:43m:54s remains)
2017-12-09 06:03:22.639224: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.18957371 -0.18963741 -0.18993811 -0.19074565 -0.1926011 -0.19539887 -0.1968306 -0.19657576 -0.19302031 -0.18417312 -0.17246562 -0.16256958 -0.15933892 -0.15960106 -0.16492389][-0.18955745 -0.18959312 -0.18985534 -0.19069168 -0.19282287 -0.19607058 -0.19960178 -0.20276038 -0.20327047 -0.19909562 -0.19122915 -0.182827 -0.17808902 -0.17519882 -0.1756039][-0.18955557 -0.18957289 -0.18978463 -0.19050545 -0.19192053 -0.19356462 -0.19496843 -0.19614303 -0.19514385 -0.18995835 -0.18353458 -0.1773753 -0.17527866 -0.17426912 -0.17613074][-0.18955557 -0.1895621 -0.18971491 -0.19030927 -0.19144432 -0.19250108 -0.19269444 -0.19298148 -0.19209602 -0.18820223 -0.18307881 -0.17801993 -0.17729759 -0.17730209 -0.17934293][-0.18955557 -0.18955718 -0.18966003 -0.19010043 -0.19108306 -0.19213751 -0.19200131 -0.19232856 -0.19138379 -0.188917 -0.18543102 -0.18128318 -0.1812607 -0.18158257 -0.18294622][-0.1895557 -0.1895557 -0.18962355 -0.18994613 -0.19079302 -0.19189991 -0.19167106 -0.19209887 -0.19174281 -0.19032735 -0.18856058 -0.18524331 -0.18543294 -0.18519178 -0.18610278][-0.1895598 -0.18955885 -0.189598 -0.18983755 -0.19058083 -0.19176209 -0.19172604 -0.19271034 -0.19353254 -0.19333532 -0.19162644 -0.18879189 -0.18795696 -0.18750994 -0.18768002][-0.18959168 -0.18958633 -0.18961468 -0.18986905 -0.19069996 -0.19191198 -0.19212794 -0.19226158 -0.19152994 -0.18956457 -0.18624401 -0.18159653 -0.17958209 -0.17832154 -0.17941496][-0.18968236 -0.18968092 -0.18980521 -0.19025326 -0.19112219 -0.19144273 -0.18989895 -0.18569694 -0.17961195 -0.17028481 -0.16073044 -0.15148401 -0.14574976 -0.14438881 -0.1485932][-0.18985692 -0.18985441 -0.19002979 -0.19043516 -0.19067878 -0.188799 -0.18313147 -0.17214495 -0.15627922 -0.1337935 -0.11227386 -0.095087394 -0.086068667 -0.087094478 -0.097341761][-0.19004355 -0.19003353 -0.1902485 -0.19056113 -0.19018301 -0.18685171 -0.17868361 -0.16230422 -0.13739926 -0.10097309 -0.064401448 -0.034368247 -0.016257048 -0.016208991 -0.032217205][-0.19026995 -0.19043297 -0.19117777 -0.19262001 -0.193293 -0.1902968 -0.18136564 -0.16291785 -0.13256696 -0.086477473 -0.035906866 0.0090565085 0.041105524 0.049222022 0.032873437][-0.1905394 -0.19107744 -0.19270027 -0.19574012 -0.19849667 -0.19803427 -0.19210926 -0.17648648 -0.14722686 -0.098612361 -0.039978728 0.0172351 0.065063477 0.089698851 0.084034055][-0.1908628 -0.19139248 -0.19329819 -0.19722851 -0.20197724 -0.20560454 -0.20647343 -0.20018797 -0.1805746 -0.14149667 -0.08633393 -0.0243451 0.037850112 0.0848116 0.10121825][-0.19099303 -0.19110034 -0.19286644 -0.19728577 -0.20363568 -0.21116506 -0.21956423 -0.22575182 -0.22162616 -0.19962141 -0.1567857 -0.10011066 -0.0324717 0.030365676 0.070846379]]...]
INFO - root - 2017-12-09 06:03:28.670553: step 2910, loss = 1.49, batch loss = 0.68 (12.8 examples/sec; 0.626 sec/batch; 57h:16m:21s remains)
INFO - root - 2017-12-09 06:03:35.028918: step 2920, loss = 1.47, batch loss = 0.65 (11.9 examples/sec; 0.671 sec/batch; 61h:26m:59s remains)
INFO - root - 2017-12-09 06:03:41.521332: step 2930, loss = 1.51, batch loss = 0.69 (12.2 examples/sec; 0.653 sec/batch; 59h:48m:52s remains)
INFO - root - 2017-12-09 06:03:47.929624: step 2940, loss = 1.48, batch loss = 0.66 (12.2 examples/sec; 0.655 sec/batch; 59h:56m:43s remains)
INFO - root - 2017-12-09 06:03:54.401824: step 2950, loss = 1.52, batch loss = 0.70 (12.3 examples/sec; 0.649 sec/batch; 59h:23m:11s remains)
INFO - root - 2017-12-09 06:04:00.685132: step 2960, loss = 1.50, batch loss = 0.68 (12.7 examples/sec; 0.631 sec/batch; 57h:47m:02s remains)
INFO - root - 2017-12-09 06:04:06.957969: step 2970, loss = 1.47, batch loss = 0.65 (12.3 examples/sec; 0.648 sec/batch; 59h:18m:08s remains)
INFO - root - 2017-12-09 06:04:13.471535: step 2980, loss = 1.48, batch loss = 0.66 (12.4 examples/sec; 0.644 sec/batch; 58h:58m:16s remains)
INFO - root - 2017-12-09 06:04:19.864661: step 2990, loss = 1.46, batch loss = 0.63 (12.7 examples/sec; 0.631 sec/batch; 57h:43m:52s remains)
INFO - root - 2017-12-09 06:04:26.330898: step 3000, loss = 1.52, batch loss = 0.69 (12.3 examples/sec; 0.650 sec/batch; 59h:30m:47s remains)
2017-12-09 06:04:27.015076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.21278989 0.3520112 0.62327158 0.11358418 0.29469019 0.29618627 -0.0971941 -0.4468807 -0.55758673 -0.28174844 0.042959392 0.10541035 -0.22269054 -0.46092868 -0.31596553][-0.28658241 -0.24904248 -0.18432587 -0.27325922 -0.20147756 -0.27069393 0.10530271 0.1359507 -0.37659478 -0.58324617 -0.61575061 -0.54157484 -0.32596603 -0.19192407 -0.13796997][-0.17266671 -0.38925302 -0.24554051 -0.12799937 -0.10526288 -0.12667303 -0.10178287 0.28092134 0.21232741 -0.016145661 -0.044725746 -0.18937753 -0.20757145 -0.079103678 0.039491817][-0.19383387 -0.073167935 0.088215277 0.021078587 -0.11166532 -0.017124087 -0.1190209 -0.090699442 0.080912486 0.12807526 0.14238776 0.11285104 -0.17239057 -0.1625334 -0.18388078][-0.18255812 -0.11994067 0.033550262 0.089299157 0.13607387 0.10976507 0.058946863 0.005733043 -0.03090091 -0.10053667 -0.10144911 -0.20510796 -0.30362868 -0.37272614 -0.56856257][-0.0306492 0.19804029 0.42789966 0.52601868 0.4998737 0.30247045 0.10827918 0.090897873 0.10433044 0.080811962 0.056380555 0.018656611 -0.0023164749 0.060425237 -0.10263763][-0.23825723 -0.24219164 0.015761957 0.19446258 0.52701288 0.52067941 0.45384049 0.27649814 0.040907338 0.098257259 0.12443952 0.17311372 0.13595526 0.20165165 0.31207025][-0.21184777 -0.20516053 -0.14804313 0.18993275 0.46411824 0.9082213 1.3996941 1.0059862 0.24920647 0.0929658 -0.20478977 -0.258477 0.0036266148 0.0078052878 0.17226566][-0.26675692 -0.30054277 -0.20498216 -0.16651666 -0.15457508 -0.016030312 0.17743523 0.45435309 0.609497 0.66946238 0.33239263 0.28390145 0.36260688 0.023175746 0.085288987][-0.21456073 0.088853464 0.43506485 0.40568173 0.20538421 -0.1445744 -0.10747952 0.14646129 0.20550542 0.33001649 0.87040907 0.56146157 0.18801172 0.3747564 0.38865817][-0.31674626 -0.39055648 -0.40663821 0.34608823 0.95017737 0.44887704 -0.23558 -0.2230676 -0.1294442 0.27788597 0.17031519 0.11270897 -0.12173185 0.36320168 0.41459686][-0.27524802 -0.39379591 -0.588578 -0.70981419 -0.22902322 0.72059542 0.37842304 -0.11452343 -0.073467389 0.4539355 0.42966938 0.13219146 1.1077573 0.60508043 -0.1228542][-0.25419372 -0.27750728 -0.27995574 -0.49917454 -0.75336391 -0.319297 -0.16624823 0.70159262 0.51799184 0.00078997016 0.41756892 0.60683596 1.0747521 0.46498221 -0.015919074][-0.22217144 -0.22777878 -0.23663211 -0.26251522 -0.39228904 -0.73544282 -0.59184325 -0.27248359 -0.26686481 0.14441712 0.89253491 0.44486946 0.24323009 0.73084235 0.6979571][-0.19826907 -0.08680097 0.026683718 0.2477776 0.13217779 -0.20270649 -0.31166133 -0.708207 -0.71762049 -0.69898325 -0.342306 0.36271489 0.93132085 0.64290494 0.50176984]]...]
INFO - root - 2017-12-09 06:04:33.111805: step 3010, loss = 1.54, batch loss = 0.71 (15.6 examples/sec; 0.513 sec/batch; 46h:59m:43s remains)
INFO - root - 2017-12-09 06:04:39.310960: step 3020, loss = 1.50, batch loss = 0.67 (12.7 examples/sec; 0.629 sec/batch; 57h:36m:25s remains)
INFO - root - 2017-12-09 06:04:45.851426: step 3030, loss = 1.54, batch loss = 0.71 (11.7 examples/sec; 0.683 sec/batch; 62h:32m:32s remains)
INFO - root - 2017-12-09 06:04:52.299541: step 3040, loss = 1.52, batch loss = 0.68 (13.0 examples/sec; 0.617 sec/batch; 56h:29m:24s remains)
INFO - root - 2017-12-09 06:04:58.653194: step 3050, loss = 1.92, batch loss = 1.08 (13.2 examples/sec; 0.608 sec/batch; 55h:38m:30s remains)
INFO - root - 2017-12-09 06:05:04.994521: step 3060, loss = 2.21, batch loss = 1.37 (13.3 examples/sec; 0.603 sec/batch; 55h:10m:13s remains)
INFO - root - 2017-12-09 06:05:11.364869: step 3070, loss = 2.15, batch loss = 1.31 (12.6 examples/sec; 0.637 sec/batch; 58h:17m:06s remains)
INFO - root - 2017-12-09 06:05:17.773974: step 3080, loss = 1.53, batch loss = 0.69 (12.4 examples/sec; 0.646 sec/batch; 59h:07m:40s remains)
INFO - root - 2017-12-09 06:05:24.373108: step 3090, loss = 1.52, batch loss = 0.67 (12.4 examples/sec; 0.646 sec/batch; 59h:08m:49s remains)
INFO - root - 2017-12-09 06:05:30.864994: step 3100, loss = 1.61, batch loss = 0.77 (12.2 examples/sec; 0.655 sec/batch; 59h:55m:09s remains)
2017-12-09 06:05:31.550076: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.050181866 0.053051546 0.062802047 0.0906961 0.10192496 0.12792256 0.13463509 0.12787515 0.12178501 0.10093492 0.088726908 0.056315795 0.022032142 -0.01824519 -0.058141485][0.4833335 0.51042414 0.53361213 0.58732891 0.62292564 0.67057419 0.68748784 0.68399847 0.66886532 0.6418072 0.62383842 0.57650435 0.52811956 0.4667159 0.40611491][1.0071533 1.0664253 1.1123759 1.1916188 1.2477626 1.319241 1.3475341 1.3461668 1.3197687 1.2820606 1.2541499 1.1915362 1.133281 1.0635225 1.0015838][1.3790447 1.4760392 1.5555803 1.6749587 1.7730479 1.8712366 1.9160178 1.9145846 1.8704512 1.8281906 1.7835928 1.7156481 1.656075 1.5949515 1.5472057][1.5439645 1.6820095 1.798485 1.9471326 2.0743995 2.2038059 2.2629755 2.2673419 2.2044289 2.1620035 2.1036117 2.0340447 1.9757779 1.9089341 1.8761592][1.4590704 1.62749 1.7769326 1.9609606 2.1231427 2.2811499 2.3615577 2.3756461 2.3001473 2.2324679 2.1237164 2.0165648 1.9371955 1.8647175 1.8476136][1.1312537 1.3168489 1.4996051 1.7056252 1.9054673 2.0786853 2.1842706 2.2123706 2.1384659 2.0574148 1.9103026 1.7606254 1.6113876 1.5044781 1.4589926][0.56965864 0.74083185 0.94129229 1.1460934 1.3543446 1.530508 1.6567224 1.7066191 1.6485008 1.5663855 1.4230353 1.2638841 1.0947579 0.95933592 0.87757325][0.070130408 0.18217346 0.33056524 0.49306366 0.66441584 0.81002164 0.91688025 0.96381378 0.93772411 0.85941279 0.74429309 0.61060309 0.49116972 0.37625238 0.31473538][-0.19441359 -0.15477416 -0.096430644 -0.0029636025 0.10237992 0.19444782 0.26279479 0.29014811 0.27836344 0.230759 0.17319489 0.089959085 0.038661212 -0.035306379 -0.046545997][-0.25146875 -0.26032537 -0.2737844 -0.25346944 -0.22733022 -0.19756883 -0.17160811 -0.16108108 -0.16627118 -0.18040326 -0.19606127 -0.21584629 -0.20726754 -0.22156255 -0.17595629][-0.19777507 -0.21735756 -0.25419778 -0.26969138 -0.28958249 -0.2966525 -0.29996762 -0.29796767 -0.29419178 -0.28430676 -0.27435115 -0.26246095 -0.23882645 -0.22685194 -0.16086][-0.1580337 -0.16790548 -0.1918359 -0.20831174 -0.23804936 -0.25130239 -0.26473093 -0.26731363 -0.26728773 -0.25374493 -0.23905554 -0.22334726 -0.20113163 -0.1904832 -0.13399783][-0.13455568 -0.13807926 -0.14838463 -0.15114431 -0.17497215 -0.1836362 -0.19969609 -0.20293455 -0.21256588 -0.20897058 -0.20270023 -0.1975421 -0.1909101 -0.18763362 -0.15150934][-0.12858424 -0.12908661 -0.11535007 -0.11185212 -0.13694641 -0.14200573 -0.1598641 -0.16690232 -0.18815713 -0.19355208 -0.1933341 -0.1931318 -0.19228439 -0.19213726 -0.1730562]]...]
INFO - root - 2017-12-09 06:05:37.828308: step 3110, loss = 1.54, batch loss = 0.69 (15.9 examples/sec; 0.504 sec/batch; 46h:09m:06s remains)
INFO - root - 2017-12-09 06:05:43.939531: step 3120, loss = 1.48, batch loss = 0.63 (12.2 examples/sec; 0.654 sec/batch; 59h:50m:28s remains)
INFO - root - 2017-12-09 06:05:50.461987: step 3130, loss = 1.50, batch loss = 0.65 (12.3 examples/sec; 0.653 sec/batch; 59h:43m:51s remains)
INFO - root - 2017-12-09 06:05:56.910612: step 3140, loss = 1.55, batch loss = 0.70 (12.2 examples/sec; 0.656 sec/batch; 60h:02m:39s remains)
INFO - root - 2017-12-09 06:06:03.469165: step 3150, loss = 1.56, batch loss = 0.71 (12.1 examples/sec; 0.663 sec/batch; 60h:38m:57s remains)
INFO - root - 2017-12-09 06:06:10.004363: step 3160, loss = 1.53, batch loss = 0.67 (12.4 examples/sec; 0.647 sec/batch; 59h:13m:31s remains)
INFO - root - 2017-12-09 06:06:16.532764: step 3170, loss = 1.54, batch loss = 0.68 (12.2 examples/sec; 0.654 sec/batch; 59h:50m:39s remains)
INFO - root - 2017-12-09 06:06:22.930154: step 3180, loss = 1.54, batch loss = 0.68 (12.9 examples/sec; 0.621 sec/batch; 56h:47m:28s remains)
INFO - root - 2017-12-09 06:06:29.234688: step 3190, loss = 1.57, batch loss = 0.71 (12.5 examples/sec; 0.638 sec/batch; 58h:18m:58s remains)
INFO - root - 2017-12-09 06:06:35.620369: step 3200, loss = 1.58, batch loss = 0.72 (12.4 examples/sec; 0.647 sec/batch; 59h:09m:47s remains)
2017-12-09 06:06:36.333463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.27826712 -0.28768986 -0.29492331 -0.30384296 -0.3124195 -0.32314497 -0.33384854 -0.34320605 -0.34801513 -0.34074253 -0.32791498 -0.3101446 -0.28685004 -0.25949079 -0.23403603][-0.33039948 -0.34383315 -0.3517676 -0.35919356 -0.36055338 -0.36327636 -0.36405912 -0.36312681 -0.35882258 -0.34359425 -0.32775772 -0.31084085 -0.29159427 -0.26703358 -0.24139348][-0.3633112 -0.37629136 -0.37971729 -0.38040978 -0.3658658 -0.35437989 -0.33973622 -0.32052064 -0.30232096 -0.27480781 -0.255211 -0.24501494 -0.24087547 -0.23278643 -0.22113082][-0.34100345 -0.346007 -0.34157428 -0.33239692 -0.30089608 -0.27413902 -0.24225202 -0.20772491 -0.18170513 -0.14644249 -0.12483995 -0.12416269 -0.14180246 -0.1617237 -0.1753809][-0.24598607 -0.23869409 -0.23184311 -0.21878943 -0.18346481 -0.15720794 -0.12426316 -0.083494373 -0.053025931 -0.021261752 -0.0047159791 -0.011846617 -0.044848934 -0.086291552 -0.12098312][-0.12430058 -0.10484967 -0.095971681 -0.085254341 -0.060897708 -0.043071985 -0.019753695 0.018092737 0.043423936 0.07484822 0.094865873 0.0798132 0.030717418 -0.030382052 -0.08205165][-0.0026458651 0.027081326 0.033379048 0.039839461 0.057575658 0.056778729 0.064516976 0.090208575 0.11067827 0.13696559 0.1517861 0.135251 0.083320692 0.015339956 -0.046749577][0.091159686 0.13901557 0.15549119 0.15955202 0.16415174 0.15696399 0.15348195 0.17228396 0.19337697 0.21065523 0.21332674 0.17885761 0.11809094 0.036784604 -0.035527706][0.16282935 0.23789199 0.27347022 0.28503335 0.27961183 0.26654798 0.25074792 0.251765 0.26695096 0.27304411 0.27036822 0.22620137 0.15511258 0.068276152 -0.014466062][0.15089191 0.24660183 0.2965014 0.31115896 0.30594105 0.29265749 0.258142 0.24445553 0.24839766 0.24635918 0.23601161 0.19784956 0.13731684 0.0569012 -0.021897092][0.069464907 0.17019813 0.219712 0.23398726 0.22632287 0.20956792 0.16713335 0.13420962 0.12490974 0.1187162 0.10608308 0.0841551 0.044557318 -0.004557699 -0.058553159][-0.059576258 0.021099746 0.053648785 0.058268502 0.040616542 0.021309763 -0.016677439 -0.045667484 -0.053427637 -0.047907904 -0.044271812 -0.045082897 -0.064473942 -0.089763679 -0.11904398][-0.19210124 -0.14528915 -0.1360625 -0.13482799 -0.14326534 -0.15017726 -0.17828134 -0.20121625 -0.19942929 -0.18704432 -0.17346758 -0.15673552 -0.15433031 -0.15416509 -0.16210859][-0.25059709 -0.23088044 -0.23462528 -0.22973186 -0.23014879 -0.23101233 -0.2398541 -0.24900961 -0.24613073 -0.24261327 -0.22709459 -0.20897673 -0.1983556 -0.18702343 -0.18419383][-0.2440895 -0.23372152 -0.23613629 -0.23432778 -0.23302084 -0.22618471 -0.22535576 -0.23439392 -0.23582962 -0.23086697 -0.22403449 -0.21900494 -0.21088466 -0.1998798 -0.19359849]]...]
INFO - root - 2017-12-09 06:06:42.633029: step 3210, loss = 1.55, batch loss = 0.68 (12.7 examples/sec; 0.631 sec/batch; 57h:40m:28s remains)
INFO - root - 2017-12-09 06:06:48.537158: step 3220, loss = 1.55, batch loss = 0.68 (12.7 examples/sec; 0.629 sec/batch; 57h:29m:32s remains)
INFO - root - 2017-12-09 06:06:54.907243: step 3230, loss = 1.53, batch loss = 0.66 (12.7 examples/sec; 0.628 sec/batch; 57h:28m:23s remains)
INFO - root - 2017-12-09 06:07:01.386736: step 3240, loss = 1.56, batch loss = 0.69 (12.4 examples/sec; 0.645 sec/batch; 59h:01m:51s remains)
INFO - root - 2017-12-09 06:07:07.858513: step 3250, loss = 1.62, batch loss = 0.75 (12.6 examples/sec; 0.633 sec/batch; 57h:53m:09s remains)
INFO - root - 2017-12-09 06:07:14.316610: step 3260, loss = 1.64, batch loss = 0.77 (12.3 examples/sec; 0.651 sec/batch; 59h:31m:32s remains)
INFO - root - 2017-12-09 06:07:20.661353: step 3270, loss = 1.66, batch loss = 0.79 (12.5 examples/sec; 0.642 sec/batch; 58h:40m:14s remains)
INFO - root - 2017-12-09 06:07:26.931999: step 3280, loss = 1.67, batch loss = 0.80 (13.0 examples/sec; 0.617 sec/batch; 56h:25m:12s remains)
INFO - root - 2017-12-09 06:07:33.246739: step 3290, loss = 1.65, batch loss = 0.78 (13.1 examples/sec; 0.611 sec/batch; 55h:53m:26s remains)
INFO - root - 2017-12-09 06:07:39.608419: step 3300, loss = 1.67, batch loss = 0.79 (12.5 examples/sec; 0.639 sec/batch; 58h:25m:05s remains)
2017-12-09 06:07:40.311318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.19673654 -0.19123687 -0.15675956 -0.088025622 -0.018798783 -0.04333353 0.025421858 0.1464384 -0.087310605 -0.0994562 0.039597824 -0.17635812 -0.17855017 -0.15162583 -0.059353814][-0.16755456 -0.15828764 -0.092119157 -0.083123274 0.043045551 -0.013177693 -0.084645726 -0.026049867 -0.053183973 0.02055946 -0.12956715 -0.08631023 0.031363487 -0.085717805 -0.040061966][-0.22266071 -0.20023091 -0.092026815 0.0070037395 0.1637453 0.11883117 0.083055481 0.069377556 -0.23438013 -0.24546799 -0.32268214 -0.24887133 -0.37264341 -0.27494666 0.10660209][-0.24227042 -0.16255963 -0.088268667 0.0098391771 0.020019144 0.21416889 0.259521 0.15146323 -0.028454021 -0.011511102 -0.4136402 -0.44921523 -0.30296636 -0.34218627 -0.68101764][-0.22587584 -0.20362918 -0.16991824 -0.066347077 0.22175299 0.30195671 0.42050135 0.18590416 0.26580119 0.048906296 -0.19097513 -0.38324365 -0.49901575 -0.5234524 -0.22994843][-0.28242594 -0.2214008 -0.24503425 -0.13276365 0.24594991 0.29416102 0.63798815 0.71114343 0.70588142 0.502945 0.3107025 0.11483108 -0.65328485 -0.63456541 -0.74842197][-0.22496732 -0.24095684 -0.36031485 -0.15058248 0.31840855 0.43318468 0.55930507 0.83667171 0.88427985 0.88849938 0.78555137 0.605559 0.053304121 0.0067988932 -0.37180072][-0.21722928 -0.33472893 -0.33257273 -0.046208337 0.13380174 0.25484258 0.7474125 0.89157546 1.1339271 0.99716103 0.96913695 0.74501348 0.63680595 0.14373289 -0.61510891][-0.19846405 -0.34887224 -0.32529187 -0.26233032 -0.080845878 0.13507132 0.48582482 0.49354762 0.88042569 1.0073115 1.2880802 1.3487589 0.99903846 0.31012827 -0.17732668][-0.22224967 -0.36480117 -0.3750394 -0.4096981 -0.32224762 -0.22391252 0.11096369 0.15190168 0.46895629 0.41921073 0.69877559 0.70359343 0.6884405 0.68373632 0.33319211][-0.19184962 -0.21289785 -0.27109563 -0.45415771 -0.42790961 -0.43599975 -0.24521422 -0.30265993 -0.10622828 0.03024669 0.45219761 0.33620346 -0.025660753 -0.23625161 -0.275752][-0.16627175 -0.19268693 -0.20828612 -0.2973119 -0.37925431 -0.54221624 -0.45152658 -0.37920672 -0.24679323 -0.23106577 0.016084433 -0.1294255 0.17361467 0.016050875 -0.24083951][-0.17741475 -0.1469871 -0.15007074 -0.21627054 -0.34809914 -0.57526839 -0.71183574 -0.83975887 -0.69940752 -0.54341406 -0.15479368 -0.31074464 -0.16258678 -0.33162531 -0.27220517][-0.14721021 -0.16779597 -0.20780323 -0.22503993 -0.31207672 -0.54665256 -0.72354668 -0.87965178 -0.89471495 -0.82751191 -0.65594578 -0.46190679 -0.32240012 -0.28343776 -0.27175081][-0.16832909 -0.069673076 -0.085192636 -0.27983674 -0.34651762 -0.46933502 -0.59483123 -0.75079381 -0.80370593 -0.80101949 -0.84302825 -0.69457185 -0.41903096 -0.33172166 -0.32598135]]...]
INFO - root - 2017-12-09 06:07:46.686934: step 3310, loss = 1.65, batch loss = 0.77 (12.6 examples/sec; 0.634 sec/batch; 58h:00m:43s remains)
INFO - root - 2017-12-09 06:07:52.683182: step 3320, loss = 1.73, batch loss = 0.85 (13.0 examples/sec; 0.615 sec/batch; 56h:16m:32s remains)
INFO - root - 2017-12-09 06:07:59.085755: step 3330, loss = 1.69, batch loss = 0.80 (11.9 examples/sec; 0.670 sec/batch; 61h:16m:13s remains)
INFO - root - 2017-12-09 06:08:05.472272: step 3340, loss = 1.70, batch loss = 0.81 (12.6 examples/sec; 0.635 sec/batch; 58h:04m:25s remains)
INFO - root - 2017-12-09 06:08:11.919226: step 3350, loss = 1.71, batch loss = 0.82 (12.4 examples/sec; 0.643 sec/batch; 58h:47m:29s remains)
INFO - root - 2017-12-09 06:08:18.162768: step 3360, loss = 1.62, batch loss = 0.73 (12.7 examples/sec; 0.631 sec/batch; 57h:40m:57s remains)
INFO - root - 2017-12-09 06:08:24.550584: step 3370, loss = 1.68, batch loss = 0.78 (12.3 examples/sec; 0.648 sec/batch; 59h:16m:08s remains)
INFO - root - 2017-12-09 06:08:30.848563: step 3380, loss = 1.70, batch loss = 0.80 (13.1 examples/sec; 0.613 sec/batch; 56h:01m:09s remains)
INFO - root - 2017-12-09 06:08:37.097319: step 3390, loss = 1.55, batch loss = 0.65 (12.2 examples/sec; 0.654 sec/batch; 59h:48m:16s remains)
INFO - root - 2017-12-09 06:08:43.662522: step 3400, loss = 1.64, batch loss = 0.74 (12.1 examples/sec; 0.660 sec/batch; 60h:18m:25s remains)
2017-12-09 06:08:44.346730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.59700561 -0.56698966 -0.56878036 -0.59769672 -0.62246668 -0.63545156 -0.62341207 -0.58829391 -0.56491554 -0.5604952 -0.56838262 -0.583616 -0.60361713 -0.63718122 -0.66580164][-0.62783182 -0.60535222 -0.61202443 -0.63935876 -0.66241419 -0.66335773 -0.63543117 -0.59059751 -0.55947018 -0.55380964 -0.56963658 -0.5836916 -0.60829878 -0.648875 -0.68018818][-0.67092764 -0.65403318 -0.65670407 -0.67678785 -0.69056964 -0.68159342 -0.64798164 -0.59384048 -0.55898654 -0.55497718 -0.57476026 -0.59744418 -0.62745011 -0.66304219 -0.68938792][-0.71959674 -0.72220194 -0.72651041 -0.73238122 -0.72507834 -0.70153069 -0.65664607 -0.60646641 -0.5786351 -0.57941884 -0.60927773 -0.63519657 -0.65966916 -0.69335967 -0.7142396][-0.76226389 -0.77589333 -0.77315617 -0.76768267 -0.74825847 -0.71030533 -0.65997732 -0.61840832 -0.60061967 -0.61513078 -0.65514737 -0.68946797 -0.71914303 -0.74696136 -0.754884][-0.77798009 -0.79126418 -0.79622579 -0.7916677 -0.76929486 -0.72976756 -0.68193984 -0.64282155 -0.63202369 -0.65433908 -0.69806671 -0.7352308 -0.76824534 -0.78477263 -0.78255737][-0.80054891 -0.80708611 -0.80913746 -0.80488575 -0.78379631 -0.74727404 -0.70443428 -0.67193055 -0.66612536 -0.69018352 -0.7376008 -0.76986897 -0.7894783 -0.7953999 -0.78343308][-0.82583773 -0.81396461 -0.81402695 -0.81002462 -0.79519939 -0.76401234 -0.72572088 -0.70012105 -0.6977818 -0.72160113 -0.76338506 -0.79303312 -0.80226982 -0.79810309 -0.77305555][-0.88046849 -0.84143496 -0.82440269 -0.81532681 -0.80153894 -0.77837837 -0.74788356 -0.72580314 -0.72805882 -0.75340891 -0.78788114 -0.81298113 -0.81102657 -0.79763114 -0.75241256][-0.95194173 -0.88529325 -0.82742333 -0.81074333 -0.79530752 -0.77584636 -0.75463688 -0.74245751 -0.75285661 -0.78316653 -0.82136881 -0.83752644 -0.82579184 -0.80569768 -0.74864304][-1.0118027 -0.93504524 -0.84420109 -0.81209385 -0.78827322 -0.76832092 -0.75554383 -0.75500917 -0.77510571 -0.816465 -0.86222112 -0.87923777 -0.86197484 -0.82788146 -0.75767279][-1.0549185 -0.98429155 -0.88702726 -0.82356572 -0.78408074 -0.758392 -0.74853957 -0.76324272 -0.79931092 -0.85207736 -0.90695953 -0.92927158 -0.90341032 -0.85640275 -0.77083778][-1.0701205 -1.0171369 -0.92343056 -0.86146641 -0.822194 -0.7815063 -0.7620343 -0.78245008 -0.82882071 -0.89543915 -0.95055389 -0.96723449 -0.94164228 -0.88672292 -0.79026186][-1.0769364 -1.0442179 -0.98182 -0.93044543 -0.88281131 -0.84293485 -0.82671976 -0.835794 -0.8736279 -0.93079615 -0.97523332 -1.0048056 -1.0039541 -0.96611762 -0.88554454][-1.0626969 -1.0763011 -1.0624299 -1.0264363 -0.98453736 -0.94373906 -0.92900038 -0.93093479 -0.9574759 -1.0065163 -1.0510449 -1.0796738 -1.0856198 -1.0758332 -1.0168405]]...]
INFO - root - 2017-12-09 06:08:50.693214: step 3410, loss = 1.65, batch loss = 0.75 (12.4 examples/sec; 0.643 sec/batch; 58h:45m:42s remains)
INFO - root - 2017-12-09 06:08:56.773811: step 3420, loss = 1.59, batch loss = 0.69 (12.7 examples/sec; 0.628 sec/batch; 57h:25m:15s remains)
INFO - root - 2017-12-09 06:09:03.068989: step 3430, loss = 1.60, batch loss = 0.69 (12.5 examples/sec; 0.642 sec/batch; 58h:40m:17s remains)
INFO - root - 2017-12-09 06:09:09.433804: step 3440, loss = 1.63, batch loss = 0.72 (12.4 examples/sec; 0.643 sec/batch; 58h:47m:58s remains)
INFO - root - 2017-12-09 06:09:15.837565: step 3450, loss = 1.60, batch loss = 0.69 (12.6 examples/sec; 0.637 sec/batch; 58h:13m:32s remains)
INFO - root - 2017-12-09 06:09:22.305170: step 3460, loss = 1.60, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:21m:09s remains)
INFO - root - 2017-12-09 06:09:28.662824: step 3470, loss = 1.56, batch loss = 0.65 (12.7 examples/sec; 0.632 sec/batch; 57h:45m:38s remains)
INFO - root - 2017-12-09 06:09:34.980818: step 3480, loss = 1.55, batch loss = 0.63 (12.4 examples/sec; 0.643 sec/batch; 58h:47m:10s remains)
INFO - root - 2017-12-09 06:09:41.214202: step 3490, loss = 1.59, batch loss = 0.68 (12.6 examples/sec; 0.634 sec/batch; 57h:54m:03s remains)
INFO - root - 2017-12-09 06:09:47.616814: step 3500, loss = 1.57, batch loss = 0.65 (12.7 examples/sec; 0.631 sec/batch; 57h:37m:58s remains)
2017-12-09 06:09:48.317657: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14390145 0.15343566 0.1547886 0.17702059 0.19924681 0.22233348 0.24587779 0.26588207 0.27495784 0.26160026 0.25039476 0.15549673 0.0211166 -0.12168042 -0.25403517][0.41154003 0.41568071 0.41789556 0.42029411 0.42297095 0.443559 0.46742737 0.48227918 0.49808806 0.48426384 0.47005093 0.40758562 0.28379756 0.12978928 -0.06005843][0.62906086 0.63194251 0.63927591 0.63716751 0.637449 0.63288051 0.63743156 0.65909606 0.68267679 0.69193655 0.69747615 0.65163314 0.53783184 0.39410585 0.17332314][0.80764246 0.78576148 0.78547174 0.77872306 0.77513158 0.76864141 0.76390922 0.748205 0.7314623 0.74012858 0.74550229 0.7365045 0.6672703 0.55317831 0.33523524][0.77764714 0.74550116 0.74586338 0.72009921 0.69864315 0.68606871 0.67186868 0.65791273 0.64312112 0.66143864 0.66831261 0.65518069 0.59890747 0.52209896 0.33705264][0.71999127 0.68678081 0.67881334 0.65811324 0.64050591 0.619789 0.59847182 0.58961421 0.58188856 0.59232122 0.59832227 0.60094166 0.56465453 0.518006 0.35702765][0.6797772 0.65489346 0.64253336 0.62734514 0.61682415 0.60261732 0.58747286 0.58081776 0.57364786 0.56269568 0.55510223 0.55333859 0.51470685 0.49219555 0.37888277][0.59676355 0.59432089 0.60345823 0.60436738 0.60348618 0.59979981 0.59499747 0.58881789 0.58178455 0.563231 0.54979581 0.53278553 0.48814017 0.4664138 0.37731558][0.52015173 0.53641331 0.58757323 0.60660255 0.603236 0.60026306 0.59630203 0.59098268 0.58479017 0.56584 0.55601376 0.54846889 0.52221352 0.52136332 0.46971989][0.42732263 0.48976129 0.56588787 0.58642042 0.58943665 0.59416729 0.59027958 0.58608997 0.58135074 0.56699783 0.55858034 0.55149907 0.53223532 0.53953654 0.53430659][0.27876705 0.39899212 0.51495552 0.56532389 0.58967745 0.60434461 0.60480154 0.601398 0.59799635 0.59093755 0.58425862 0.57854444 0.57060957 0.58800316 0.59787273][0.18330909 0.31474942 0.45396161 0.54849637 0.60201311 0.64160007 0.65295196 0.65633547 0.65418327 0.64034915 0.62847817 0.61832452 0.610328 0.641059 0.65438294][0.11540969 0.26246667 0.42455047 0.53231931 0.591398 0.62863845 0.63914984 0.64790541 0.64348245 0.63527584 0.63037896 0.6220324 0.630089 0.68783581 0.72894853][0.10264467 0.23960312 0.39011103 0.46678662 0.51699257 0.56731951 0.58639979 0.59495533 0.59636313 0.6063689 0.6135208 0.59950256 0.60300577 0.63316864 0.67165577][0.044987872 0.15140103 0.25395226 0.32101625 0.36635876 0.39981604 0.41124928 0.41552514 0.41532803 0.43290442 0.45064294 0.45593977 0.48182988 0.53243053 0.58527994]]...]
INFO - root - 2017-12-09 06:09:54.662990: step 3510, loss = 1.61, batch loss = 0.69 (12.4 examples/sec; 0.643 sec/batch; 58h:47m:55s remains)
INFO - root - 2017-12-09 06:10:00.759926: step 3520, loss = 1.61, batch loss = 0.69 (13.9 examples/sec; 0.575 sec/batch; 52h:32m:19s remains)
INFO - root - 2017-12-09 06:10:07.017148: step 3530, loss = 1.60, batch loss = 0.67 (12.6 examples/sec; 0.634 sec/batch; 57h:54m:17s remains)
INFO - root - 2017-12-09 06:10:13.426851: step 3540, loss = 1.63, batch loss = 0.70 (12.8 examples/sec; 0.625 sec/batch; 57h:04m:45s remains)
INFO - root - 2017-12-09 06:10:19.956125: step 3550, loss = 1.59, batch loss = 0.66 (12.3 examples/sec; 0.652 sec/batch; 59h:32m:22s remains)
INFO - root - 2017-12-09 06:10:26.339046: step 3560, loss = 1.61, batch loss = 0.68 (12.7 examples/sec; 0.629 sec/batch; 57h:25m:44s remains)
INFO - root - 2017-12-09 06:10:32.633944: step 3570, loss = 1.60, batch loss = 0.67 (12.7 examples/sec; 0.631 sec/batch; 57h:37m:34s remains)
INFO - root - 2017-12-09 06:10:38.951180: step 3580, loss = 1.60, batch loss = 0.67 (12.4 examples/sec; 0.643 sec/batch; 58h:44m:48s remains)
INFO - root - 2017-12-09 06:10:45.466935: step 3590, loss = 1.62, batch loss = 0.69 (12.4 examples/sec; 0.643 sec/batch; 58h:47m:01s remains)
INFO - root - 2017-12-09 06:10:51.856775: step 3600, loss = 1.62, batch loss = 0.68 (12.7 examples/sec; 0.629 sec/batch; 57h:28m:48s remains)
2017-12-09 06:10:52.572344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.25775087 -0.28587252 -0.34400094 -0.30047923 -0.23651785 -0.23116742 -0.2253875 -0.21289197 -0.2014657 -0.18659498 -0.17996848 -0.18770215 -0.20619832 -0.34084964 -0.25494647][-0.25569654 -0.21455374 -0.22846912 -0.33063775 -0.32075664 -0.30006063 -0.28257209 -0.17525618 -0.13041702 -0.072349936 -0.044370353 -0.027500123 -0.033251241 -0.048397139 -0.059500247][-0.39358217 -0.40450639 -0.6826489 -0.33429825 -0.37591934 -0.43029007 -0.3488875 -0.10597314 -0.022396043 0.065145314 0.11487147 0.16263211 0.16979879 0.1829986 0.1846033][0.041502431 -0.2471832 -0.40863305 -0.086943924 -0.10255919 -0.08804258 0.056229413 -0.8233285 -0.25309908 0.1685918 0.263535 0.34648642 0.36805084 0.40432784 0.41061482][0.42674449 0.14544746 0.057620436 0.30037162 -0.070802063 -0.036425248 0.11105984 0.23563045 0.33533874 0.30769184 0.11538553 0.11710799 0.47278878 0.60185862 0.61000383][0.22228 0.15756452 0.096100718 0.092588156 0.12626338 0.16544282 0.19396329 0.33101138 0.43141147 0.57230544 0.67908263 0.79421246 0.71580088 0.81630671 0.7486881][0.09971419 0.067663968 0.048830584 0.36376485 0.13684493 0.28225353 0.49670383 0.42733172 0.50966322 0.58658063 0.68578434 0.93763435 1.1851197 1.3673381 0.96821833][-0.090114094 -0.097781807 -0.087895162 -0.011027992 0.099956363 0.39339784 0.53353357 0.56506014 0.65661788 0.56831849 0.83226335 0.66158116 0.70221591 0.76690066 0.8271488][-0.26884055 -0.25307196 -0.22962068 -0.17299822 -0.053616792 0.12788662 0.30555508 0.58611131 0.7095108 0.42284653 0.48115191 0.71658516 0.94158387 0.73615253 0.9058466][-0.38666356 -0.37714404 -0.36541808 -0.3292141 -0.23200811 -0.16290568 -0.054342762 0.11718318 0.16967323 0.11497939 0.13787311 0.24054277 0.33048883 0.42966393 0.52041054][-0.42349249 -0.43106809 -1.0726182 -0.61323285 -0.57696486 -0.86224473 -0.59109205 -0.31454414 -0.28965241 -0.19635573 -0.16429169 0.00069569051 -0.024418145 0.088916868 0.21967754][-0.4052 -0.42517489 -0.46209064 -1.3221226 -1.5327675 -2.5071552 -2.1495583 -2.0483546 -1.9265673 -1.2222556 -0.9905194 -0.44464952 -0.36280414 -0.26437643 -0.11458285][-0.35666591 -0.38679266 -0.42865121 -0.82934821 -0.87629879 -2.3485267 -2.8462741 -2.3237154 -2.753931 -1.1198592 -2.637953 -1.2853503 -1.6399251 -2.0770869 -2.5245445][-0.29511517 -0.32311034 -0.36225012 -0.4453412 -0.48630354 -0.97757828 -1.1749105 -1.5861137 -1.726409 -1.9579237 -2.2818694 -0.77036917 -1.0250597 -1.3919476 -1.9270787][-0.24258693 -0.26210475 -0.29022565 -0.31632242 -0.3489641 -0.4231708 -0.47050843 -0.54255652 -0.57661784 -1.2116582 -0.970677 -0.6115976 -1.0636621 -1.4654491 -1.0701722]]...]
INFO - root - 2017-12-09 06:10:59.026403: step 3610, loss = 1.59, batch loss = 0.65 (12.0 examples/sec; 0.667 sec/batch; 60h:56m:11s remains)
INFO - root - 2017-12-09 06:11:05.233540: step 3620, loss = 1.61, batch loss = 0.67 (12.5 examples/sec; 0.641 sec/batch; 58h:33m:11s remains)
INFO - root - 2017-12-09 06:11:11.433358: step 3630, loss = 1.59, batch loss = 0.65 (12.4 examples/sec; 0.648 sec/batch; 59h:09m:21s remains)
INFO - root - 2017-12-09 06:11:17.951304: step 3640, loss = 1.60, batch loss = 0.66 (12.4 examples/sec; 0.645 sec/batch; 58h:56m:09s remains)
INFO - root - 2017-12-09 06:11:24.426525: step 3650, loss = 1.64, batch loss = 0.70 (12.8 examples/sec; 0.625 sec/batch; 57h:06m:24s remains)
INFO - root - 2017-12-09 06:11:30.770091: step 3660, loss = 1.70, batch loss = 0.76 (13.5 examples/sec; 0.594 sec/batch; 54h:17m:16s remains)
INFO - root - 2017-12-09 06:11:37.116877: step 3670, loss = 1.63, batch loss = 0.68 (12.7 examples/sec; 0.628 sec/batch; 57h:23m:33s remains)
INFO - root - 2017-12-09 06:11:43.361013: step 3680, loss = 1.62, batch loss = 0.67 (12.5 examples/sec; 0.641 sec/batch; 58h:32m:49s remains)
INFO - root - 2017-12-09 06:11:49.692545: step 3690, loss = 1.67, batch loss = 0.71 (13.0 examples/sec; 0.615 sec/batch; 56h:07m:36s remains)
INFO - root - 2017-12-09 06:11:56.100446: step 3700, loss = 1.69, batch loss = 0.73 (12.4 examples/sec; 0.646 sec/batch; 58h:57m:22s remains)
2017-12-09 06:11:56.716533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.19441102 -0.19446489 -0.19448964 -0.19451229 -0.19463027 -0.19477248 -0.19494322 -0.19504902 -0.19509339 -0.19509004 -0.19506095 -0.19506969 -0.19504057 -0.19503349 -0.19506939][-0.19434544 -0.19437909 -0.19435494 -0.194348 -0.19442254 -0.19452111 -0.19469564 -0.19484985 -0.19491665 -0.19488187 -0.19494507 -0.19495161 -0.19493577 -0.19498169 -0.1950717][-0.19414815 -0.19411132 -0.19404779 -0.19409317 -0.19410077 -0.19409391 -0.19418566 -0.19430959 -0.19444884 -0.19449605 -0.19460106 -0.19457057 -0.19467947 -0.19480777 -0.19491398][-0.19396353 -0.19390698 -0.19379784 -0.1937506 -0.19366613 -0.19370061 -0.19373468 -0.19375445 -0.19384933 -0.19384557 -0.19405507 -0.19423628 -0.19436428 -0.19467485 -0.19482963][-0.19373272 -0.19373591 -0.19354995 -0.19353105 -0.1934005 -0.19333382 -0.19325878 -0.19327907 -0.19331717 -0.19342633 -0.19357923 -0.19374669 -0.19404019 -0.19440109 -0.19465037][-0.19363624 -0.19352956 -0.19347374 -0.19350038 -0.19333844 -0.19328476 -0.19317536 -0.19314228 -0.19312811 -0.1931527 -0.19319542 -0.19334687 -0.19362587 -0.19400389 -0.19432697][-0.19370526 -0.19373281 -0.1936879 -0.19362457 -0.19355889 -0.19348171 -0.19339204 -0.19338112 -0.19325911 -0.19315681 -0.19296966 -0.19296919 -0.19314753 -0.19350734 -0.19386128][-0.19392249 -0.19393136 -0.19393086 -0.19396584 -0.19395989 -0.19389346 -0.19384797 -0.19378303 -0.19362912 -0.19350675 -0.19316749 -0.19301343 -0.1929944 -0.19321883 -0.1935192][-0.19417143 -0.19415931 -0.19415002 -0.19416191 -0.19413999 -0.19412842 -0.19410507 -0.19407439 -0.19399761 -0.19381036 -0.1935337 -0.19332078 -0.19314243 -0.19324343 -0.1934097][-0.19423096 -0.19423133 -0.19420654 -0.19421498 -0.19417179 -0.19422407 -0.19420844 -0.19414391 -0.19412333 -0.19407597 -0.19393487 -0.19374296 -0.19354612 -0.19351688 -0.19359596][-0.19416818 -0.19420223 -0.1941738 -0.19419283 -0.19418724 -0.19418292 -0.19414207 -0.19416536 -0.19415919 -0.19413245 -0.19406259 -0.19400543 -0.19395071 -0.19392742 -0.19397396][-0.19416913 -0.19418322 -0.19416365 -0.1941516 -0.19414739 -0.19415694 -0.194164 -0.19418702 -0.19417827 -0.19411269 -0.19406961 -0.19400188 -0.19400394 -0.19411936 -0.19415043][-0.19416888 -0.19416894 -0.1941693 -0.19417058 -0.19417058 -0.19417064 -0.19415377 -0.19414628 -0.19415055 -0.19416274 -0.19413821 -0.19407323 -0.19409126 -0.19407931 -0.19410639][-0.19417058 -0.19416209 -0.1941552 -0.19416721 -0.1941701 -0.19416991 -0.19417699 -0.19419219 -0.19419491 -0.19417551 -0.19415772 -0.1941416 -0.19412191 -0.1941621 -0.19420634][-0.19417058 -0.19417058 -0.19417058 -0.19417058 -0.19417058 -0.19416341 -0.19416213 -0.19421917 -0.19422019 -0.19418024 -0.19417079 -0.19415413 -0.19415015 -0.19419138 -0.19421686]]...]
INFO - root - 2017-12-09 06:12:03.090546: step 3710, loss = 1.67, batch loss = 0.71 (12.3 examples/sec; 0.648 sec/batch; 59h:10m:42s remains)
INFO - root - 2017-12-09 06:12:09.438647: step 3720, loss = 1.65, batch loss = 0.69 (15.7 examples/sec; 0.509 sec/batch; 46h:29m:56s remains)
INFO - root - 2017-12-09 06:12:15.504864: step 3730, loss = 1.67, batch loss = 0.70 (12.5 examples/sec; 0.641 sec/batch; 58h:33m:13s remains)
INFO - root - 2017-12-09 06:12:21.814505: step 3740, loss = 1.65, batch loss = 0.69 (12.7 examples/sec; 0.631 sec/batch; 57h:38m:28s remains)
INFO - root - 2017-12-09 06:12:28.110500: step 3750, loss = 1.64, batch loss = 0.67 (12.3 examples/sec; 0.649 sec/batch; 59h:15m:00s remains)
INFO - root - 2017-12-09 06:12:34.526949: step 3760, loss = 1.66, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:13m:46s remains)
INFO - root - 2017-12-09 06:12:40.905259: step 3770, loss = 1.65, batch loss = 0.68 (12.5 examples/sec; 0.637 sec/batch; 58h:12m:42s remains)
INFO - root - 2017-12-09 06:12:47.295796: step 3780, loss = 1.66, batch loss = 0.69 (13.0 examples/sec; 0.615 sec/batch; 56h:09m:54s remains)
INFO - root - 2017-12-09 06:12:53.625130: step 3790, loss = 1.62, batch loss = 0.65 (12.6 examples/sec; 0.637 sec/batch; 58h:11m:53s remains)
INFO - root - 2017-12-09 06:12:59.820434: step 3800, loss = 1.65, batch loss = 0.67 (12.6 examples/sec; 0.636 sec/batch; 58h:03m:24s remains)
2017-12-09 06:13:00.514486: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.39486 -0.44078583 -0.47545391 -0.49887478 -0.51742119 -0.53560311 -0.57386249 -0.83862579 -0.65509331 -0.64752549 -0.66051203 -0.69188011 -0.62888193 -0.5674994 -0.19794491][-0.57685953 -0.55136234 -0.56772882 -0.5635646 -0.56744593 -0.56755155 -0.57507503 -0.21941891 -0.61108679 -0.70422578 -0.71968669 -0.36969978 -0.47376353 -0.88861191 -0.65960127][-0.52363545 -0.59666383 -0.62805939 -0.69082183 -0.54160112 -0.51170069 -0.49337524 -0.23352689 -0.54436886 -0.5678283 -0.64255685 -0.44326502 -0.18487504 -0.015872821 -0.0282944][-0.901354 -0.65436155 -0.64594704 -0.54847312 -0.47334737 -0.5012266 -0.43896043 -0.50290614 -0.89108855 -0.93385625 -0.834769 -0.61177474 -0.81072378 -0.15994218 0.40824848][-0.83697891 -0.83643508 -1.031678 -1.0634805 -0.70551664 -0.78735286 -0.84990108 -0.80037487 -0.65629929 -0.47790581 -0.24348961 0.20399253 -0.049541771 -0.38456678 0.41184157][-0.026438475 -0.0042772442 0.11474143 -0.083330475 -0.071020447 -0.314766 -0.075411029 -0.23359242 -0.27685046 0.16517182 0.63254809 0.71096545 0.81493878 0.87767184 0.80020243][0.043576717 0.10197486 0.45383435 0.50167447 1.3669202 0.76876736 0.78571576 0.56535888 0.4104538 0.40040636 0.95560193 1.0595858 1.0709885 1.0311582 0.98699832][0.50065583 0.95325458 0.64768165 0.6796847 1.1437222 1.9112144 1.1610942 0.82064092 0.53578174 0.66636837 1.0590563 1.0965196 0.90294993 1.0257695 0.91570008][0.080443516 0.3421585 0.64908272 0.9723959 0.54054862 0.63184291 0.99089289 1.8402178 1.4755397 1.0532113 1.077327 1.0567269 0.73764664 0.76246339 0.81568277][0.12809725 0.26343244 0.41282433 0.56950432 0.63805437 0.83426917 0.85426545 0.8133949 0.69838285 1.1202134 1.198725 1.2039754 1.1335253 0.63435256 0.535777][0.12533037 0.29829186 0.32912117 0.35733223 0.4013744 0.39800918 0.41039425 0.44323498 0.53607869 0.51022375 0.49743968 0.5404048 0.8915205 1.1784323 0.40515029][-0.010567069 -0.10836735 0.26950288 0.34066522 0.35257787 0.35692108 0.30776924 0.19041629 0.068135247 0.077902541 0.20151506 0.28782541 0.43279904 -0.47093451 -0.534227][-0.12016553 -0.33887571 -0.52888018 -0.61672193 -0.511664 -0.51820946 -0.15613936 0.15495653 0.028378963 -0.088546783 -0.02933225 0.079331234 -0.0737093 -0.49435067 -1.1715665][-0.75910586 -0.822768 -1.0356628 -1.3512552 -1.6761115 -2.2200017 -1.6680654 -0.97613162 -0.66036046 -0.16355512 -0.016111419 -0.33720148 -0.53423327 -0.94421542 -1.7092509][-0.27629173 -0.23935686 -0.25851169 -0.41683954 -0.55932993 -0.82017666 -0.74444288 -1.56573 -1.5379883 -1.1488502 -0.62356472 -0.44010454 -0.59751719 -0.82983518 -1.2056165]]...]
INFO - root - 2017-12-09 06:13:06.817966: step 3810, loss = 1.64, batch loss = 0.66 (13.0 examples/sec; 0.616 sec/batch; 56h:16m:32s remains)
INFO - root - 2017-12-09 06:13:13.140138: step 3820, loss = 1.62, batch loss = 0.64 (12.5 examples/sec; 0.639 sec/batch; 58h:19m:23s remains)
INFO - root - 2017-12-09 06:13:19.100816: step 3830, loss = 1.64, batch loss = 0.66 (12.6 examples/sec; 0.636 sec/batch; 58h:04m:42s remains)
INFO - root - 2017-12-09 06:13:25.612451: step 3840, loss = 1.62, batch loss = 0.64 (12.4 examples/sec; 0.647 sec/batch; 59h:04m:57s remains)
INFO - root - 2017-12-09 06:13:31.928825: step 3850, loss = 1.67, batch loss = 0.69 (12.5 examples/sec; 0.640 sec/batch; 58h:24m:07s remains)
INFO - root - 2017-12-09 06:13:38.254302: step 3860, loss = 1.63, batch loss = 0.64 (13.0 examples/sec; 0.614 sec/batch; 56h:02m:53s remains)
INFO - root - 2017-12-09 06:13:44.514959: step 3870, loss = 1.66, batch loss = 0.68 (13.0 examples/sec; 0.615 sec/batch; 56h:06m:41s remains)
INFO - root - 2017-12-09 06:13:50.900252: step 3880, loss = 1.64, batch loss = 0.65 (12.0 examples/sec; 0.668 sec/batch; 61h:01m:07s remains)
INFO - root - 2017-12-09 06:13:57.323673: step 3890, loss = 1.67, batch loss = 0.68 (12.7 examples/sec; 0.630 sec/batch; 57h:28m:29s remains)
INFO - root - 2017-12-09 06:14:03.730516: step 3900, loss = 1.66, batch loss = 0.67 (12.4 examples/sec; 0.645 sec/batch; 58h:50m:12s remains)
2017-12-09 06:14:04.402445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.42505312 -0.40597686 -0.41178882 -0.41083431 -0.40564251 -0.37659156 -0.33658922 -0.29174474 -0.23623854 -0.2280952 -0.22861664 -0.29744917 -0.41860104 -0.59211814 -0.7660833][-0.24775046 -0.23512781 -0.24572691 -0.26050597 -0.26054609 -0.23266456 -0.1911467 -0.12999807 -0.063715979 -0.043378264 -0.030796126 -0.089522563 -0.21389151 -0.41561002 -0.64199233][-0.017177835 -0.032276973 -0.048115477 -0.064641967 -0.0750301 -0.057640314 -0.016577601 0.042081088 0.11466847 0.16242592 0.19750454 0.14576833 0.028684109 -0.18368463 -0.45510894][0.23853074 0.20445131 0.18560801 0.1640275 0.1470909 0.14219247 0.15549932 0.18139453 0.22116913 0.26262957 0.29211897 0.27440077 0.1929559 0.012445018 -0.25261754][0.27576286 0.23428015 0.21781524 0.19297589 0.17107977 0.14922945 0.14523132 0.16286881 0.19330896 0.23749398 0.26234531 0.25487173 0.188094 0.045094058 -0.1768852][0.30028188 0.25073344 0.23417957 0.21255861 0.19308008 0.16752793 0.15417905 0.16365765 0.18724729 0.22275607 0.23631568 0.24180646 0.20004897 0.089152887 -0.10222071][0.31659949 0.29169673 0.27336228 0.257217 0.24429362 0.22424723 0.20923595 0.20488499 0.21020724 0.21842714 0.2185704 0.22835653 0.19202103 0.11288206 -0.042624265][0.27532572 0.27538365 0.26511538 0.25633961 0.25284344 0.2447959 0.24004476 0.24288256 0.25214589 0.24793161 0.2406802 0.23797123 0.19242688 0.12609152 0.011794835][0.25983447 0.29036558 0.31425762 0.31553781 0.32179558 0.32966042 0.33655757 0.34271109 0.34411597 0.32724106 0.31291783 0.2980243 0.25968689 0.20729558 0.11144124][0.25514185 0.32607079 0.3842178 0.37908167 0.38267845 0.39453018 0.40763003 0.43057883 0.44591993 0.43069458 0.41991812 0.40293562 0.3635965 0.32887661 0.26167351][0.23447444 0.34658241 0.45659405 0.49137431 0.52076179 0.52951694 0.543854 0.56485462 0.57620263 0.56456143 0.55645245 0.53892821 0.50772119 0.48337311 0.43564552][0.22464408 0.36943704 0.51082158 0.56353819 0.59990555 0.62535 0.65771157 0.67911881 0.69237876 0.68653083 0.68076861 0.6644308 0.6430819 0.62388283 0.59596252][0.21280532 0.393987 0.56396586 0.63479877 0.67639112 0.69740939 0.72099245 0.743416 0.76125127 0.75636464 0.75247282 0.72560382 0.71560425 0.69262964 0.66658914][0.18287881 0.3767429 0.53651148 0.60386682 0.64574122 0.65771425 0.671714 0.68721259 0.702027 0.72046936 0.74484533 0.727999 0.71223 0.67039084 0.64483106][0.017169923 0.15832169 0.27182138 0.3434664 0.3660028 0.3748036 0.39558154 0.40245938 0.41689944 0.44636661 0.49310613 0.49026418 0.48018777 0.45065624 0.43304569]]...]
INFO - root - 2017-12-09 06:14:10.755415: step 3910, loss = 1.69, batch loss = 0.70 (12.8 examples/sec; 0.626 sec/batch; 57h:08m:55s remains)
INFO - root - 2017-12-09 06:14:17.021545: step 3920, loss = 1.63, batch loss = 0.63 (12.7 examples/sec; 0.630 sec/batch; 57h:30m:43s remains)
INFO - root - 2017-12-09 06:14:22.903139: step 3930, loss = 1.67, batch loss = 0.67 (13.3 examples/sec; 0.603 sec/batch; 55h:01m:23s remains)
INFO - root - 2017-12-09 06:14:29.140991: step 3940, loss = 1.65, batch loss = 0.65 (13.1 examples/sec; 0.613 sec/batch; 55h:56m:33s remains)
