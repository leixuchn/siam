INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "43"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-03 08:45:16.783066: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 08:45:16.783106: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 08:45:16.783112: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 08:45:16.783117: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 08:45:16.783121: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 08:45:17.368487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-03 08:45:17.368528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-03 08:45:17.368536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-03 08:45:17.368544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-03 08:45:20.334741: step 0, loss = 0.65, batch loss = 0.58 (3.6 examples/sec; 2.253 sec/batch; 208h:07m:37s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-03 08:45:22.851443: step 10, loss = 0.80, batch loss = 0.73 (42.0 examples/sec; 0.190 sec/batch; 17h:34m:31s remains)
INFO - root - 2017-12-03 08:45:24.717876: step 20, loss = 1.00, batch loss = 0.93 (43.9 examples/sec; 0.182 sec/batch; 16h:50m:56s remains)
INFO - root - 2017-12-03 08:45:26.582981: step 30, loss = 0.86, batch loss = 0.79 (43.8 examples/sec; 0.183 sec/batch; 16h:51m:52s remains)
INFO - root - 2017-12-03 08:45:28.450948: step 40, loss = 0.98, batch loss = 0.91 (41.8 examples/sec; 0.191 sec/batch; 17h:40m:36s remains)
INFO - root - 2017-12-03 08:45:30.301173: step 50, loss = 0.82, batch loss = 0.75 (41.9 examples/sec; 0.191 sec/batch; 17h:37m:19s remains)
INFO - root - 2017-12-03 08:45:32.184728: step 60, loss = 0.77, batch loss = 0.70 (42.3 examples/sec; 0.189 sec/batch; 17h:27m:37s remains)
INFO - root - 2017-12-03 08:45:34.047975: step 70, loss = 0.94, batch loss = 0.87 (43.1 examples/sec; 0.186 sec/batch; 17h:08m:40s remains)
INFO - root - 2017-12-03 08:45:35.907636: step 80, loss = 0.91, batch loss = 0.84 (43.7 examples/sec; 0.183 sec/batch; 16h:55m:06s remains)
INFO - root - 2017-12-03 08:45:37.769969: step 90, loss = 0.77, batch loss = 0.70 (43.0 examples/sec; 0.186 sec/batch; 17h:11m:26s remains)
INFO - root - 2017-12-03 08:45:39.657609: step 100, loss = 0.94, batch loss = 0.87 (42.9 examples/sec; 0.186 sec/batch; 17h:12m:40s remains)
INFO - root - 2017-12-03 08:45:41.600508: step 110, loss = 0.94, batch loss = 0.87 (42.1 examples/sec; 0.190 sec/batch; 17h:33m:06s remains)
INFO - root - 2017-12-03 08:45:43.479869: step 120, loss = 0.91, batch loss = 0.84 (42.1 examples/sec; 0.190 sec/batch; 17h:32m:18s remains)
INFO - root - 2017-12-03 08:45:45.335866: step 130, loss = 0.86, batch loss = 0.79 (41.6 examples/sec; 0.192 sec/batch; 17h:44m:05s remains)
INFO - root - 2017-12-03 08:45:47.220853: step 140, loss = 0.81, batch loss = 0.74 (43.1 examples/sec; 0.186 sec/batch; 17h:07m:40s remains)
INFO - root - 2017-12-03 08:45:49.105932: step 150, loss = 0.91, batch loss = 0.84 (42.4 examples/sec; 0.189 sec/batch; 17h:25m:58s remains)
INFO - root - 2017-12-03 08:45:50.982424: step 160, loss = 1.01, batch loss = 0.94 (41.7 examples/sec; 0.192 sec/batch; 17h:42m:51s remains)
INFO - root - 2017-12-03 08:45:52.864227: step 170, loss = 0.89, batch loss = 0.82 (41.1 examples/sec; 0.194 sec/batch; 17h:57m:05s remains)
INFO - root - 2017-12-03 08:45:54.730319: step 180, loss = 0.84, batch loss = 0.77 (41.9 examples/sec; 0.191 sec/batch; 17h:37m:43s remains)
INFO - root - 2017-12-03 08:45:56.631886: step 190, loss = 0.94, batch loss = 0.87 (40.6 examples/sec; 0.197 sec/batch; 18h:12m:18s remains)
INFO - root - 2017-12-03 08:45:58.520591: step 200, loss = 0.74, batch loss = 0.67 (42.4 examples/sec; 0.189 sec/batch; 17h:25m:04s remains)
INFO - root - 2017-12-03 08:46:00.441070: step 210, loss = 0.90, batch loss = 0.83 (43.0 examples/sec; 0.186 sec/batch; 17h:09m:50s remains)
INFO - root - 2017-12-03 08:46:02.341710: step 220, loss = 0.68, batch loss = 0.61 (42.3 examples/sec; 0.189 sec/batch; 17h:27m:54s remains)
INFO - root - 2017-12-03 08:46:04.223818: step 230, loss = 0.57, batch loss = 0.50 (41.7 examples/sec; 0.192 sec/batch; 17h:41m:38s remains)
INFO - root - 2017-12-03 08:46:06.114069: step 240, loss = 1.09, batch loss = 1.02 (42.8 examples/sec; 0.187 sec/batch; 17h:16m:04s remains)
INFO - root - 2017-12-03 08:46:08.008574: step 250, loss = 0.85, batch loss = 0.78 (42.7 examples/sec; 0.187 sec/batch; 17h:17m:51s remains)
INFO - root - 2017-12-03 08:46:09.894201: step 260, loss = 1.08, batch loss = 1.01 (42.2 examples/sec; 0.190 sec/batch; 17h:29m:26s remains)
INFO - root - 2017-12-03 08:46:11.795448: step 270, loss = 0.92, batch loss = 0.85 (43.9 examples/sec; 0.182 sec/batch; 16h:48m:57s remains)
INFO - root - 2017-12-03 08:46:13.677003: step 280, loss = 0.83, batch loss = 0.76 (43.4 examples/sec; 0.184 sec/batch; 17h:01m:08s remains)
INFO - root - 2017-12-03 08:46:15.552702: step 290, loss = 0.83, batch loss = 0.76 (42.4 examples/sec; 0.189 sec/batch; 17h:23m:48s remains)
INFO - root - 2017-12-03 08:46:17.451178: step 300, loss = 1.04, batch loss = 0.97 (41.7 examples/sec; 0.192 sec/batch; 17h:43m:10s remains)
INFO - root - 2017-12-03 08:46:19.402836: step 310, loss = 0.83, batch loss = 0.76 (42.7 examples/sec; 0.187 sec/batch; 17h:18m:04s remains)
INFO - root - 2017-12-03 08:46:21.304422: step 320, loss = 1.08, batch loss = 1.01 (41.6 examples/sec; 0.192 sec/batch; 17h:43m:32s remains)
INFO - root - 2017-12-03 08:46:23.188089: step 330, loss = 0.87, batch loss = 0.80 (42.6 examples/sec; 0.188 sec/batch; 17h:19m:46s remains)
INFO - root - 2017-12-03 08:46:25.097072: step 340, loss = 1.07, batch loss = 1.00 (42.8 examples/sec; 0.187 sec/batch; 17h:15m:36s remains)
INFO - root - 2017-12-03 08:46:26.980334: step 350, loss = 0.99, batch loss = 0.92 (42.0 examples/sec; 0.191 sec/batch; 17h:35m:39s remains)
INFO - root - 2017-12-03 08:46:28.869080: step 360, loss = 0.90, batch loss = 0.83 (41.1 examples/sec; 0.195 sec/batch; 17h:56m:56s remains)
INFO - root - 2017-12-03 08:46:30.779954: step 370, loss = 0.81, batch loss = 0.74 (41.8 examples/sec; 0.191 sec/batch; 17h:39m:37s remains)
INFO - root - 2017-12-03 08:46:32.655984: step 380, loss = 1.03, batch loss = 0.96 (43.6 examples/sec; 0.183 sec/batch; 16h:55m:30s remains)
INFO - root - 2017-12-03 08:46:34.537188: step 390, loss = 0.95, batch loss = 0.88 (42.8 examples/sec; 0.187 sec/batch; 17h:15m:04s remains)
INFO - root - 2017-12-03 08:46:36.436657: step 400, loss = 1.07, batch loss = 1.00 (43.6 examples/sec; 0.184 sec/batch; 16h:56m:34s remains)
INFO - root - 2017-12-03 08:46:38.400516: step 410, loss = 0.68, batch loss = 0.61 (41.5 examples/sec; 0.193 sec/batch; 17h:47m:02s remains)
INFO - root - 2017-12-03 08:46:40.310921: step 420, loss = 1.13, batch loss = 1.06 (41.6 examples/sec; 0.192 sec/batch; 17h:44m:40s remains)
INFO - root - 2017-12-03 08:46:42.196866: step 430, loss = 1.01, batch loss = 0.94 (42.8 examples/sec; 0.187 sec/batch; 17h:14m:31s remains)
INFO - root - 2017-12-03 08:46:44.108713: step 440, loss = 0.82, batch loss = 0.75 (42.2 examples/sec; 0.190 sec/batch; 17h:29m:10s remains)
INFO - root - 2017-12-03 08:46:46.015936: step 450, loss = 0.80, batch loss = 0.73 (41.2 examples/sec; 0.194 sec/batch; 17h:54m:32s remains)
INFO - root - 2017-12-03 08:46:47.907109: step 460, loss = 0.83, batch loss = 0.76 (41.8 examples/sec; 0.191 sec/batch; 17h:39m:03s remains)
INFO - root - 2017-12-03 08:46:49.807505: step 470, loss = 0.82, batch loss = 0.75 (41.6 examples/sec; 0.192 sec/batch; 17h:45m:13s remains)
INFO - root - 2017-12-03 08:46:51.714352: step 480, loss = 1.08, batch loss = 1.01 (41.0 examples/sec; 0.195 sec/batch; 17h:59m:54s remains)
INFO - root - 2017-12-03 08:46:53.608434: step 490, loss = 0.84, batch loss = 0.77 (42.5 examples/sec; 0.188 sec/batch; 17h:21m:25s remains)
INFO - root - 2017-12-03 08:46:55.521845: step 500, loss = 0.99, batch loss = 0.92 (41.6 examples/sec; 0.192 sec/batch; 17h:44m:00s remains)
INFO - root - 2017-12-03 08:46:57.492946: step 510, loss = 0.79, batch loss = 0.72 (42.7 examples/sec; 0.187 sec/batch; 17h:17m:03s remains)
INFO - root - 2017-12-03 08:46:59.411382: step 520, loss = 1.15, batch loss = 1.08 (41.5 examples/sec; 0.193 sec/batch; 17h:45m:41s remains)
INFO - root - 2017-12-03 08:47:01.315835: step 530, loss = 0.87, batch loss = 0.80 (42.7 examples/sec; 0.187 sec/batch; 17h:15m:24s remains)
INFO - root - 2017-12-03 08:47:03.211613: step 540, loss = 0.90, batch loss = 0.83 (39.8 examples/sec; 0.201 sec/batch; 18h:30m:55s remains)
INFO - root - 2017-12-03 08:47:05.111222: step 550, loss = 0.91, batch loss = 0.84 (41.3 examples/sec; 0.194 sec/batch; 17h:52m:08s remains)
INFO - root - 2017-12-03 08:47:07.004337: step 560, loss = 1.03, batch loss = 0.96 (42.1 examples/sec; 0.190 sec/batch; 17h:31m:12s remains)
INFO - root - 2017-12-03 08:47:08.921293: step 570, loss = 0.73, batch loss = 0.66 (40.4 examples/sec; 0.198 sec/batch; 18h:16m:33s remains)
INFO - root - 2017-12-03 08:47:10.845988: step 580, loss = 1.03, batch loss = 0.96 (42.0 examples/sec; 0.190 sec/batch; 17h:33m:06s remains)
INFO - root - 2017-12-03 08:47:12.744325: step 590, loss = 0.77, batch loss = 0.70 (41.6 examples/sec; 0.192 sec/batch; 17h:44m:48s remains)
INFO - root - 2017-12-03 08:47:14.658492: step 600, loss = 1.15, batch loss = 1.08 (42.5 examples/sec; 0.188 sec/batch; 17h:22m:22s remains)
INFO - root - 2017-12-03 08:47:16.608678: step 610, loss = 1.05, batch loss = 0.98 (41.8 examples/sec; 0.192 sec/batch; 17h:39m:19s remains)
INFO - root - 2017-12-03 08:47:18.535181: step 620, loss = 0.97, batch loss = 0.90 (40.9 examples/sec; 0.196 sec/batch; 18h:03m:05s remains)
INFO - root - 2017-12-03 08:47:20.436810: step 630, loss = 0.90, batch loss = 0.83 (43.2 examples/sec; 0.185 sec/batch; 17h:04m:08s remains)
INFO - root - 2017-12-03 08:47:22.334158: step 640, loss = 0.79, batch loss = 0.72 (42.9 examples/sec; 0.187 sec/batch; 17h:12m:03s remains)
INFO - root - 2017-12-03 08:47:24.235237: step 650, loss = 1.03, batch loss = 0.96 (42.3 examples/sec; 0.189 sec/batch; 17h:25m:10s remains)
INFO - root - 2017-12-03 08:47:26.127394: step 660, loss = 0.75, batch loss = 0.68 (43.5 examples/sec; 0.184 sec/batch; 16h:56m:01s remains)
INFO - root - 2017-12-03 08:47:28.065486: step 670, loss = 0.69, batch loss = 0.62 (39.8 examples/sec; 0.201 sec/batch; 18h:32m:41s remains)
INFO - root - 2017-12-03 08:47:29.969732: step 680, loss = 1.12, batch loss = 1.05 (43.0 examples/sec; 0.186 sec/batch; 17h:08m:21s remains)
INFO - root - 2017-12-03 08:47:31.884645: step 690, loss = 0.80, batch loss = 0.72 (41.7 examples/sec; 0.192 sec/batch; 17h:41m:19s remains)
INFO - root - 2017-12-03 08:47:33.794835: step 700, loss = 0.93, batch loss = 0.86 (40.2 examples/sec; 0.199 sec/batch; 18h:20m:52s remains)
INFO - root - 2017-12-03 08:47:35.774995: step 710, loss = 0.91, batch loss = 0.84 (43.3 examples/sec; 0.185 sec/batch; 17h:00m:46s remains)
INFO - root - 2017-12-03 08:47:37.676288: step 720, loss = 0.72, batch loss = 0.65 (41.8 examples/sec; 0.191 sec/batch; 17h:37m:16s remains)
INFO - root - 2017-12-03 08:47:39.610688: step 730, loss = 0.76, batch loss = 0.69 (42.4 examples/sec; 0.189 sec/batch; 17h:22m:25s remains)
INFO - root - 2017-12-03 08:47:41.544358: step 740, loss = 0.79, batch loss = 0.72 (42.1 examples/sec; 0.190 sec/batch; 17h:30m:40s remains)
INFO - root - 2017-12-03 08:47:43.449212: step 750, loss = 0.88, batch loss = 0.81 (41.6 examples/sec; 0.192 sec/batch; 17h:43m:08s remains)
INFO - root - 2017-12-03 08:47:45.357885: step 760, loss = 1.04, batch loss = 0.97 (41.5 examples/sec; 0.193 sec/batch; 17h:45m:36s remains)
INFO - root - 2017-12-03 08:47:47.267756: step 770, loss = 1.01, batch loss = 0.94 (41.7 examples/sec; 0.192 sec/batch; 17h:41m:00s remains)
INFO - root - 2017-12-03 08:47:49.172919: step 780, loss = 0.81, batch loss = 0.74 (42.7 examples/sec; 0.187 sec/batch; 17h:15m:49s remains)
INFO - root - 2017-12-03 08:47:51.093755: step 790, loss = 1.00, batch loss = 0.93 (41.2 examples/sec; 0.194 sec/batch; 17h:54m:37s remains)
INFO - root - 2017-12-03 08:47:53.017917: step 800, loss = 0.95, batch loss = 0.88 (42.3 examples/sec; 0.189 sec/batch; 17h:26m:22s remains)
INFO - root - 2017-12-03 08:47:54.988252: step 810, loss = 0.88, batch loss = 0.81 (43.1 examples/sec; 0.186 sec/batch; 17h:05m:36s remains)
INFO - root - 2017-12-03 08:47:56.893857: step 820, loss = 0.79, batch loss = 0.72 (42.0 examples/sec; 0.190 sec/batch; 17h:33m:02s remains)
INFO - root - 2017-12-03 08:47:58.818096: step 830, loss = 0.97, batch loss = 0.90 (40.8 examples/sec; 0.196 sec/batch; 18h:03m:03s remains)
INFO - root - 2017-12-03 08:48:00.717571: step 840, loss = 0.91, batch loss = 0.84 (42.8 examples/sec; 0.187 sec/batch; 17h:13m:02s remains)
INFO - root - 2017-12-03 08:48:02.638470: step 850, loss = 0.73, batch loss = 0.66 (42.1 examples/sec; 0.190 sec/batch; 17h:30m:49s remains)
INFO - root - 2017-12-03 08:48:04.535613: step 860, loss = 0.93, batch loss = 0.86 (42.4 examples/sec; 0.189 sec/batch; 17h:23m:32s remains)
INFO - root - 2017-12-03 08:48:06.446326: step 870, loss = 1.15, batch loss = 1.08 (39.8 examples/sec; 0.201 sec/batch; 18h:30m:35s remains)
INFO - root - 2017-12-03 08:48:08.350225: step 880, loss = 0.93, batch loss = 0.86 (41.9 examples/sec; 0.191 sec/batch; 17h:35m:38s remains)
INFO - root - 2017-12-03 08:48:10.300161: step 890, loss = 0.71, batch loss = 0.64 (39.7 examples/sec; 0.202 sec/batch; 18h:34m:07s remains)
INFO - root - 2017-12-03 08:48:12.216123: step 900, loss = 1.06, batch loss = 0.99 (40.4 examples/sec; 0.198 sec/batch; 18h:13m:23s remains)
INFO - root - 2017-12-03 08:48:14.219714: step 910, loss = 1.29, batch loss = 1.22 (42.0 examples/sec; 0.190 sec/batch; 17h:32m:10s remains)
INFO - root - 2017-12-03 08:48:16.159269: step 920, loss = 1.08, batch loss = 1.01 (40.1 examples/sec; 0.200 sec/batch; 18h:23m:45s remains)
INFO - root - 2017-12-03 08:48:18.074668: step 930, loss = 1.18, batch loss = 1.11 (41.6 examples/sec; 0.192 sec/batch; 17h:43m:18s remains)
INFO - root - 2017-12-03 08:48:19.998273: step 940, loss = 0.99, batch loss = 0.92 (42.3 examples/sec; 0.189 sec/batch; 17h:24m:03s remains)
INFO - root - 2017-12-03 08:48:21.916392: step 950, loss = 0.87, batch loss = 0.80 (42.2 examples/sec; 0.189 sec/batch; 17h:26m:50s remains)
INFO - root - 2017-12-03 08:48:23.825938: step 960, loss = 0.94, batch loss = 0.87 (42.0 examples/sec; 0.191 sec/batch; 17h:33m:19s remains)
INFO - root - 2017-12-03 08:48:25.760163: step 970, loss = 1.09, batch loss = 1.02 (41.8 examples/sec; 0.191 sec/batch; 17h:36m:33s remains)
INFO - root - 2017-12-03 08:48:27.658576: step 980, loss = 0.97, batch loss = 0.90 (40.6 examples/sec; 0.197 sec/batch; 18h:09m:09s remains)
INFO - root - 2017-12-03 08:48:29.584846: step 990, loss = 0.84, batch loss = 0.77 (43.7 examples/sec; 0.183 sec/batch; 16h:51m:55s remains)
INFO - root - 2017-12-03 08:48:31.506700: step 1000, loss = 0.77, batch loss = 0.70 (41.7 examples/sec; 0.192 sec/batch; 17h:40m:25s remains)
INFO - root - 2017-12-03 08:48:33.489422: step 1010, loss = 0.86, batch loss = 0.79 (42.4 examples/sec; 0.189 sec/batch; 17h:22m:28s remains)
INFO - root - 2017-12-03 08:48:35.393303: step 1020, loss = 0.96, batch loss = 0.89 (40.8 examples/sec; 0.196 sec/batch; 18h:03m:01s remains)
INFO - root - 2017-12-03 08:48:37.320966: step 1030, loss = 0.69, batch loss = 0.62 (41.5 examples/sec; 0.193 sec/batch; 17h:44m:38s remains)
INFO - root - 2017-12-03 08:48:39.256211: step 1040, loss = 1.03, batch loss = 0.96 (41.1 examples/sec; 0.194 sec/batch; 17h:54m:01s remains)
INFO - root - 2017-12-03 08:48:41.156022: step 1050, loss = 0.90, batch loss = 0.83 (42.1 examples/sec; 0.190 sec/batch; 17h:28m:51s remains)
INFO - root - 2017-12-03 08:48:43.053705: step 1060, loss = 1.11, batch loss = 1.04 (42.7 examples/sec; 0.188 sec/batch; 17h:16m:08s remains)
INFO - root - 2017-12-03 08:48:44.971202: step 1070, loss = 0.94, batch loss = 0.87 (42.4 examples/sec; 0.189 sec/batch; 17h:21m:54s remains)
INFO - root - 2017-12-03 08:48:46.880695: step 1080, loss = 0.81, batch loss = 0.74 (41.2 examples/sec; 0.194 sec/batch; 17h:51m:53s remains)
INFO - root - 2017-12-03 08:48:48.817244: step 1090, loss = 0.75, batch loss = 0.68 (41.1 examples/sec; 0.195 sec/batch; 17h:56m:12s remains)
INFO - root - 2017-12-03 08:48:50.713453: step 1100, loss = 0.98, batch loss = 0.91 (42.2 examples/sec; 0.189 sec/batch; 17h:26m:34s remains)
INFO - root - 2017-12-03 08:48:52.702878: step 1110, loss = 0.86, batch loss = 0.79 (40.9 examples/sec; 0.196 sec/batch; 17h:59m:59s remains)
INFO - root - 2017-12-03 08:48:54.650721: step 1120, loss = 0.92, batch loss = 0.85 (41.6 examples/sec; 0.192 sec/batch; 17h:41m:25s remains)
INFO - root - 2017-12-03 08:48:56.580494: step 1130, loss = 1.05, batch loss = 0.98 (41.5 examples/sec; 0.193 sec/batch; 17h:45m:02s remains)
INFO - root - 2017-12-03 08:48:58.479029: step 1140, loss = 1.02, batch loss = 0.95 (42.4 examples/sec; 0.189 sec/batch; 17h:23m:08s remains)
INFO - root - 2017-12-03 08:49:00.405822: step 1150, loss = 0.92, batch loss = 0.85 (41.2 examples/sec; 0.194 sec/batch; 17h:52m:25s remains)
INFO - root - 2017-12-03 08:49:02.331249: step 1160, loss = 1.07, batch loss = 1.00 (41.2 examples/sec; 0.194 sec/batch; 17h:52m:47s remains)
INFO - root - 2017-12-03 08:49:04.282100: step 1170, loss = 0.89, batch loss = 0.82 (40.2 examples/sec; 0.199 sec/batch; 18h:19m:36s remains)
INFO - root - 2017-12-03 08:49:06.199939: step 1180, loss = 1.20, batch loss = 1.13 (42.0 examples/sec; 0.191 sec/batch; 17h:32m:04s remains)
INFO - root - 2017-12-03 08:49:08.122642: step 1190, loss = 0.91, batch loss = 0.84 (41.3 examples/sec; 0.194 sec/batch; 17h:50m:11s remains)
INFO - root - 2017-12-03 08:49:10.057337: step 1200, loss = 1.07, batch loss = 1.00 (42.0 examples/sec; 0.191 sec/batch; 17h:32m:32s remains)
INFO - root - 2017-12-03 08:49:12.071022: step 1210, loss = 0.96, batch loss = 0.89 (40.5 examples/sec; 0.198 sec/batch; 18h:12m:00s remains)
INFO - root - 2017-12-03 08:49:13.979445: step 1220, loss = 0.94, batch loss = 0.87 (42.6 examples/sec; 0.188 sec/batch; 17h:15m:59s remains)
INFO - root - 2017-12-03 08:49:15.892924: step 1230, loss = 1.01, batch loss = 0.94 (43.2 examples/sec; 0.185 sec/batch; 17h:01m:55s remains)
INFO - root - 2017-12-03 08:49:17.791160: step 1240, loss = 1.16, batch loss = 1.09 (41.5 examples/sec; 0.193 sec/batch; 17h:44m:00s remains)
INFO - root - 2017-12-03 08:49:19.702528: step 1250, loss = 1.14, batch loss = 1.07 (42.5 examples/sec; 0.188 sec/batch; 17h:18m:03s remains)
INFO - root - 2017-12-03 08:49:21.646813: step 1260, loss = 0.78, batch loss = 0.71 (41.2 examples/sec; 0.194 sec/batch; 17h:51m:31s remains)
INFO - root - 2017-12-03 08:49:23.571291: step 1270, loss = 0.72, batch loss = 0.65 (43.7 examples/sec; 0.183 sec/batch; 16h:50m:04s remains)
INFO - root - 2017-12-03 08:49:25.489420: step 1280, loss = 1.03, batch loss = 0.96 (41.9 examples/sec; 0.191 sec/batch; 17h:34m:58s remains)
INFO - root - 2017-12-03 08:49:27.397003: step 1290, loss = 0.98, batch loss = 0.91 (42.3 examples/sec; 0.189 sec/batch; 17h:24m:06s remains)
INFO - root - 2017-12-03 08:49:29.309574: step 1300, loss = 1.03, batch loss = 0.96 (43.4 examples/sec; 0.184 sec/batch; 16h:56m:44s remains)
INFO - root - 2017-12-03 08:49:31.338595: step 1310, loss = 0.81, batch loss = 0.74 (42.2 examples/sec; 0.190 sec/batch; 17h:26m:31s remains)
INFO - root - 2017-12-03 08:49:33.256722: step 1320, loss = 1.08, batch loss = 1.01 (41.8 examples/sec; 0.191 sec/batch; 17h:36m:04s remains)
INFO - root - 2017-12-03 08:49:35.193879: step 1330, loss = 0.92, batch loss = 0.85 (40.7 examples/sec; 0.196 sec/batch; 18h:04m:22s remains)
INFO - root - 2017-12-03 08:49:37.099509: step 1340, loss = 0.76, batch loss = 0.69 (42.6 examples/sec; 0.188 sec/batch; 17h:15m:46s remains)
INFO - root - 2017-12-03 08:49:39.040842: step 1350, loss = 0.84, batch loss = 0.77 (39.7 examples/sec; 0.202 sec/batch; 18h:33m:04s remains)
INFO - root - 2017-12-03 08:49:40.953652: step 1360, loss = 1.28, batch loss = 1.21 (42.6 examples/sec; 0.188 sec/batch; 17h:15m:22s remains)
INFO - root - 2017-12-03 08:49:42.858299: step 1370, loss = 0.89, batch loss = 0.82 (42.8 examples/sec; 0.187 sec/batch; 17h:12m:29s remains)
INFO - root - 2017-12-03 08:49:44.799915: step 1380, loss = 0.82, batch loss = 0.75 (42.3 examples/sec; 0.189 sec/batch; 17h:24m:17s remains)
INFO - root - 2017-12-03 08:49:46.732763: step 1390, loss = 0.85, batch loss = 0.78 (41.5 examples/sec; 0.193 sec/batch; 17h:44m:27s remains)
INFO - root - 2017-12-03 08:49:48.642046: step 1400, loss = 0.86, batch loss = 0.79 (42.4 examples/sec; 0.189 sec/batch; 17h:21m:27s remains)
INFO - root - 2017-12-03 08:49:50.620941: step 1410, loss = 0.87, batch loss = 0.80 (40.8 examples/sec; 0.196 sec/batch; 18h:01m:34s remains)
INFO - root - 2017-12-03 08:49:52.526536: step 1420, loss = 1.04, batch loss = 0.97 (41.6 examples/sec; 0.192 sec/batch; 17h:40m:32s remains)
INFO - root - 2017-12-03 08:49:54.437197: step 1430, loss = 0.93, batch loss = 0.86 (42.1 examples/sec; 0.190 sec/batch; 17h:28m:36s remains)
INFO - root - 2017-12-03 08:49:56.363301: step 1440, loss = 0.97, batch loss = 0.90 (42.4 examples/sec; 0.188 sec/batch; 17h:20m:03s remains)
INFO - root - 2017-12-03 08:49:58.278886: step 1450, loss = 0.80, batch loss = 0.73 (43.0 examples/sec; 0.186 sec/batch; 17h:06m:56s remains)
INFO - root - 2017-12-03 08:50:00.192795: step 1460, loss = 0.79, batch loss = 0.72 (42.6 examples/sec; 0.188 sec/batch; 17h:16m:03s remains)
INFO - root - 2017-12-03 08:50:02.117809: step 1470, loss = 0.70, batch loss = 0.63 (42.3 examples/sec; 0.189 sec/batch; 17h:24m:31s remains)
INFO - root - 2017-12-03 08:50:04.074715: step 1480, loss = 1.06, batch loss = 0.99 (38.8 examples/sec; 0.206 sec/batch; 18h:57m:58s remains)
INFO - root - 2017-12-03 08:50:05.988838: step 1490, loss = 1.13, batch loss = 1.06 (41.9 examples/sec; 0.191 sec/batch; 17h:32m:07s remains)
INFO - root - 2017-12-03 08:50:07.902333: step 1500, loss = 0.90, batch loss = 0.83 (41.5 examples/sec; 0.193 sec/batch; 17h:43m:31s remains)
INFO - root - 2017-12-03 08:50:09.923885: step 1510, loss = 0.99, batch loss = 0.92 (42.4 examples/sec; 0.189 sec/batch; 17h:21m:43s remains)
INFO - root - 2017-12-03 08:50:11.847224: step 1520, loss = 1.02, batch loss = 0.95 (41.7 examples/sec; 0.192 sec/batch; 17h:37m:16s remains)
INFO - root - 2017-12-03 08:50:13.767491: step 1530, loss = 0.90, batch loss = 0.83 (41.3 examples/sec; 0.194 sec/batch; 17h:48m:56s remains)
INFO - root - 2017-12-03 08:50:15.671918: step 1540, loss = 1.01, batch loss = 0.94 (42.6 examples/sec; 0.188 sec/batch; 17h:15m:18s remains)
INFO - root - 2017-12-03 08:50:17.601158: step 1550, loss = 0.97, batch loss = 0.90 (42.7 examples/sec; 0.187 sec/batch; 17h:13m:01s remains)
INFO - root - 2017-12-03 08:50:19.535099: step 1560, loss = 1.27, batch loss = 1.20 (42.6 examples/sec; 0.188 sec/batch; 17h:16m:54s remains)
INFO - root - 2017-12-03 08:50:21.490028: step 1570, loss = 0.95, batch loss = 0.88 (40.9 examples/sec; 0.196 sec/batch; 17h:58m:44s remains)
INFO - root - 2017-12-03 08:50:23.388211: step 1580, loss = 0.85, batch loss = 0.78 (41.8 examples/sec; 0.192 sec/batch; 17h:36m:24s remains)
INFO - root - 2017-12-03 08:50:25.332384: step 1590, loss = 0.94, batch loss = 0.87 (40.3 examples/sec; 0.199 sec/batch; 18h:15m:21s remains)
INFO - root - 2017-12-03 08:50:27.241946: step 1600, loss = 0.97, batch loss = 0.90 (42.1 examples/sec; 0.190 sec/batch; 17h:29m:12s remains)
INFO - root - 2017-12-03 08:50:29.214980: step 1610, loss = 1.00, batch loss = 0.93 (41.5 examples/sec; 0.193 sec/batch; 17h:42m:19s remains)
INFO - root - 2017-12-03 08:50:31.118155: step 1620, loss = 0.86, batch loss = 0.79 (42.1 examples/sec; 0.190 sec/batch; 17h:28m:23s remains)
INFO - root - 2017-12-03 08:50:33.020195: step 1630, loss = 1.08, batch loss = 1.01 (42.5 examples/sec; 0.188 sec/batch; 17h:19m:06s remains)
INFO - root - 2017-12-03 08:50:34.929233: step 1640, loss = 0.96, batch loss = 0.89 (42.7 examples/sec; 0.188 sec/batch; 17h:14m:06s remains)
INFO - root - 2017-12-03 08:50:36.843530: step 1650, loss = 0.88, batch loss = 0.81 (41.6 examples/sec; 0.192 sec/batch; 17h:39m:25s remains)
INFO - root - 2017-12-03 08:50:38.759717: step 1660, loss = 0.78, batch loss = 0.71 (41.4 examples/sec; 0.193 sec/batch; 17h:44m:40s remains)
INFO - root - 2017-12-03 08:50:40.747781: step 1670, loss = 1.02, batch loss = 0.95 (40.2 examples/sec; 0.199 sec/batch; 18h:17m:34s remains)
INFO - root - 2017-12-03 08:50:42.680675: step 1680, loss = 0.94, batch loss = 0.87 (41.6 examples/sec; 0.192 sec/batch; 17h:40m:55s remains)
INFO - root - 2017-12-03 08:50:44.611866: step 1690, loss = 0.84, batch loss = 0.77 (42.5 examples/sec; 0.188 sec/batch; 17h:18m:22s remains)
INFO - root - 2017-12-03 08:50:46.536157: step 1700, loss = 0.87, batch loss = 0.80 (40.2 examples/sec; 0.199 sec/batch; 18h:16m:34s remains)
INFO - root - 2017-12-03 08:50:48.519455: step 1710, loss = 0.81, batch loss = 0.74 (43.9 examples/sec; 0.182 sec/batch; 16h:44m:07s remains)
INFO - root - 2017-12-03 08:50:50.410555: step 1720, loss = 0.84, batch loss = 0.77 (41.7 examples/sec; 0.192 sec/batch; 17h:38m:53s remains)
INFO - root - 2017-12-03 08:50:52.346750: step 1730, loss = 0.77, batch loss = 0.70 (43.1 examples/sec; 0.186 sec/batch; 17h:02m:50s remains)
INFO - root - 2017-12-03 08:50:54.268067: step 1740, loss = 1.12, batch loss = 1.05 (42.5 examples/sec; 0.188 sec/batch; 17h:17m:35s remains)
INFO - root - 2017-12-03 08:50:56.177142: step 1750, loss = 1.27, batch loss = 1.20 (43.1 examples/sec; 0.186 sec/batch; 17h:04m:17s remains)
INFO - root - 2017-12-03 08:50:58.101699: step 1760, loss = 1.08, batch loss = 1.01 (41.8 examples/sec; 0.191 sec/batch; 17h:35m:24s remains)
INFO - root - 2017-12-03 08:51:00.017860: step 1770, loss = 1.15, batch loss = 1.08 (41.4 examples/sec; 0.193 sec/batch; 17h:44m:18s remains)
INFO - root - 2017-12-03 08:51:01.963372: step 1780, loss = 1.04, batch loss = 0.97 (41.8 examples/sec; 0.191 sec/batch; 17h:35m:13s remains)
INFO - root - 2017-12-03 08:51:03.888832: step 1790, loss = 1.11, batch loss = 1.04 (40.7 examples/sec; 0.197 sec/batch; 18h:03m:13s remains)
INFO - root - 2017-12-03 08:51:05.811034: step 1800, loss = 1.06, batch loss = 0.99 (43.2 examples/sec; 0.185 sec/batch; 17h:00m:50s remains)
INFO - root - 2017-12-03 08:51:07.789366: step 1810, loss = 1.00, batch loss = 0.93 (42.7 examples/sec; 0.187 sec/batch; 17h:11m:46s remains)
INFO - root - 2017-12-03 08:51:09.701072: step 1820, loss = 0.99, batch loss = 0.92 (41.2 examples/sec; 0.194 sec/batch; 17h:49m:57s remains)
INFO - root - 2017-12-03 08:51:11.608508: step 1830, loss = 0.93, batch loss = 0.86 (41.5 examples/sec; 0.193 sec/batch; 17h:43m:20s remains)
INFO - root - 2017-12-03 08:51:13.524177: step 1840, loss = 0.86, batch loss = 0.79 (42.7 examples/sec; 0.187 sec/batch; 17h:12m:48s remains)
INFO - root - 2017-12-03 08:51:15.433165: step 1850, loss = 0.96, batch loss = 0.89 (42.3 examples/sec; 0.189 sec/batch; 17h:22m:43s remains)
INFO - root - 2017-12-03 08:51:17.334667: step 1860, loss = 1.07, batch loss = 1.00 (42.0 examples/sec; 0.191 sec/batch; 17h:30m:33s remains)
INFO - root - 2017-12-03 08:51:19.261048: step 1870, loss = 0.85, batch loss = 0.78 (42.2 examples/sec; 0.189 sec/batch; 17h:24m:05s remains)
INFO - root - 2017-12-03 08:51:21.209971: step 1880, loss = 0.91, batch loss = 0.84 (39.9 examples/sec; 0.201 sec/batch; 18h:25m:50s remains)
INFO - root - 2017-12-03 08:51:23.136559: step 1890, loss = 1.03, batch loss = 0.95 (42.0 examples/sec; 0.191 sec/batch; 17h:30m:14s remains)
INFO - root - 2017-12-03 08:51:25.043429: step 1900, loss = 0.87, batch loss = 0.80 (40.1 examples/sec; 0.200 sec/batch; 18h:19m:47s remains)
INFO - root - 2017-12-03 08:51:27.054051: step 1910, loss = 1.11, batch loss = 1.04 (41.7 examples/sec; 0.192 sec/batch; 17h:37m:58s remains)
INFO - root - 2017-12-03 08:51:28.997808: step 1920, loss = 0.86, batch loss = 0.79 (41.6 examples/sec; 0.193 sec/batch; 17h:40m:40s remains)
INFO - root - 2017-12-03 08:51:30.933093: step 1930, loss = 0.80, batch loss = 0.73 (42.1 examples/sec; 0.190 sec/batch; 17h:28m:09s remains)
INFO - root - 2017-12-03 08:51:32.860341: step 1940, loss = 0.83, batch loss = 0.76 (41.9 examples/sec; 0.191 sec/batch; 17h:31m:21s remains)
INFO - root - 2017-12-03 08:51:34.766735: step 1950, loss = 1.21, batch loss = 1.14 (42.4 examples/sec; 0.189 sec/batch; 17h:19m:27s remains)
INFO - root - 2017-12-03 08:51:36.685884: step 1960, loss = 0.79, batch loss = 0.72 (42.1 examples/sec; 0.190 sec/batch; 17h:26m:06s remains)
INFO - root - 2017-12-03 08:51:38.590478: step 1970, loss = 0.83, batch loss = 0.76 (43.9 examples/sec; 0.182 sec/batch; 16h:44m:48s remains)
INFO - root - 2017-12-03 08:51:40.497304: step 1980, loss = 0.75, batch loss = 0.68 (42.2 examples/sec; 0.190 sec/batch; 17h:25m:24s remains)
INFO - root - 2017-12-03 08:51:42.410079: step 1990, loss = 1.04, batch loss = 0.97 (41.2 examples/sec; 0.194 sec/batch; 17h:50m:12s remains)
INFO - root - 2017-12-03 08:51:44.322099: step 2000, loss = 0.88, batch loss = 0.81 (41.5 examples/sec; 0.193 sec/batch; 17h:42m:45s remains)
INFO - root - 2017-12-03 08:51:46.293942: step 2010, loss = 1.10, batch loss = 1.03 (40.7 examples/sec; 0.197 sec/batch; 18h:03m:51s remains)
INFO - root - 2017-12-03 08:51:48.189663: step 2020, loss = 1.02, batch loss = 0.95 (42.5 examples/sec; 0.188 sec/batch; 17h:15m:59s remains)
INFO - root - 2017-12-03 08:51:50.111544: step 2030, loss = 0.83, batch loss = 0.76 (41.9 examples/sec; 0.191 sec/batch; 17h:32m:16s remains)
INFO - root - 2017-12-03 08:51:52.046406: step 2040, loss = 1.02, batch loss = 0.95 (41.9 examples/sec; 0.191 sec/batch; 17h:31m:30s remains)
INFO - root - 2017-12-03 08:51:53.986293: step 2050, loss = 0.78, batch loss = 0.71 (40.8 examples/sec; 0.196 sec/batch; 17h:58m:46s remains)
INFO - root - 2017-12-03 08:51:55.915291: step 2060, loss = 1.09, batch loss = 1.02 (39.9 examples/sec; 0.201 sec/batch; 18h:25m:25s remains)
INFO - root - 2017-12-03 08:51:57.831963: step 2070, loss = 0.94, batch loss = 0.86 (42.2 examples/sec; 0.190 sec/batch; 17h:24m:01s remains)
INFO - root - 2017-12-03 08:51:59.740349: step 2080, loss = 0.95, batch loss = 0.88 (42.0 examples/sec; 0.190 sec/batch; 17h:28m:20s remains)
INFO - root - 2017-12-03 08:52:01.657914: step 2090, loss = 1.19, batch loss = 1.12 (42.2 examples/sec; 0.190 sec/batch; 17h:24m:37s remains)
INFO - root - 2017-12-03 08:52:03.586474: step 2100, loss = 1.08, batch loss = 1.01 (42.9 examples/sec; 0.186 sec/batch; 17h:06m:35s remains)
INFO - root - 2017-12-03 08:52:05.599566: step 2110, loss = 0.95, batch loss = 0.88 (42.3 examples/sec; 0.189 sec/batch; 17h:22m:02s remains)
INFO - root - 2017-12-03 08:52:07.496529: step 2120, loss = 0.92, batch loss = 0.85 (41.9 examples/sec; 0.191 sec/batch; 17h:31m:26s remains)
INFO - root - 2017-12-03 08:52:09.403676: step 2130, loss = 0.81, batch loss = 0.74 (42.1 examples/sec; 0.190 sec/batch; 17h:26m:24s remains)
INFO - root - 2017-12-03 08:52:11.302301: step 2140, loss = 1.05, batch loss = 0.98 (42.1 examples/sec; 0.190 sec/batch; 17h:25m:41s remains)
INFO - root - 2017-12-03 08:52:13.198451: step 2150, loss = 1.12, batch loss = 1.05 (42.4 examples/sec; 0.189 sec/batch; 17h:18m:27s remains)
INFO - root - 2017-12-03 08:52:15.120058: step 2160, loss = 0.85, batch loss = 0.78 (42.6 examples/sec; 0.188 sec/batch; 17h:13m:02s remains)
INFO - root - 2017-12-03 08:52:17.035217: step 2170, loss = 0.86, batch loss = 0.79 (40.1 examples/sec; 0.200 sec/batch; 18h:19m:21s remains)
INFO - root - 2017-12-03 08:52:18.951741: step 2180, loss = 0.96, batch loss = 0.89 (41.0 examples/sec; 0.195 sec/batch; 17h:52m:59s remains)
INFO - root - 2017-12-03 08:52:20.887156: step 2190, loss = 1.06, batch loss = 0.99 (41.5 examples/sec; 0.193 sec/batch; 17h:39m:59s remains)
INFO - root - 2017-12-03 08:52:22.792702: step 2200, loss = 1.22, batch loss = 1.15 (42.5 examples/sec; 0.188 sec/batch; 17h:15m:17s remains)
INFO - root - 2017-12-03 08:52:24.800973: step 2210, loss = 0.93, batch loss = 0.86 (38.9 examples/sec; 0.206 sec/batch; 18h:53m:15s remains)
INFO - root - 2017-12-03 08:52:26.704438: step 2220, loss = 1.02, batch loss = 0.95 (43.1 examples/sec; 0.186 sec/batch; 17h:01m:25s remains)
INFO - root - 2017-12-03 08:52:28.620608: step 2230, loss = 1.08, batch loss = 1.01 (42.8 examples/sec; 0.187 sec/batch; 17h:09m:23s remains)
INFO - root - 2017-12-03 08:52:30.537940: step 2240, loss = 0.94, batch loss = 0.87 (41.4 examples/sec; 0.193 sec/batch; 17h:42m:32s remains)
INFO - root - 2017-12-03 08:52:32.432861: step 2250, loss = 1.06, batch loss = 0.99 (42.6 examples/sec; 0.188 sec/batch; 17h:14m:46s remains)
INFO - root - 2017-12-03 08:52:34.342442: step 2260, loss = 0.87, batch loss = 0.80 (42.2 examples/sec; 0.190 sec/batch; 17h:24m:30s remains)
INFO - root - 2017-12-03 08:52:36.250675: step 2270, loss = 1.09, batch loss = 1.02 (42.2 examples/sec; 0.190 sec/batch; 17h:24m:22s remains)
INFO - root - 2017-12-03 08:52:38.165618: step 2280, loss = 1.00, batch loss = 0.93 (41.7 examples/sec; 0.192 sec/batch; 17h:34m:52s remains)
INFO - root - 2017-12-03 08:52:40.080017: step 2290, loss = 0.98, batch loss = 0.91 (42.2 examples/sec; 0.190 sec/batch; 17h:23m:54s remains)
INFO - root - 2017-12-03 08:52:42.008404: step 2300, loss = 1.10, batch loss = 1.03 (41.3 examples/sec; 0.194 sec/batch; 17h:47m:18s remains)
INFO - root - 2017-12-03 08:52:43.997022: step 2310, loss = 1.37, batch loss = 1.30 (39.9 examples/sec; 0.201 sec/batch; 18h:24m:09s remains)
INFO - root - 2017-12-03 08:52:45.922492: step 2320, loss = 1.00, batch loss = 0.93 (42.2 examples/sec; 0.190 sec/batch; 17h:23m:41s remains)
INFO - root - 2017-12-03 08:52:47.833209: step 2330, loss = 0.97, batch loss = 0.90 (41.7 examples/sec; 0.192 sec/batch; 17h:35m:20s remains)
INFO - root - 2017-12-03 08:52:49.745719: step 2340, loss = 1.10, batch loss = 1.03 (42.6 examples/sec; 0.188 sec/batch; 17h:13m:09s remains)
INFO - root - 2017-12-03 08:52:51.652375: step 2350, loss = 0.93, batch loss = 0.86 (41.9 examples/sec; 0.191 sec/batch; 17h:30m:07s remains)
INFO - root - 2017-12-03 08:52:53.558579: step 2360, loss = 1.07, batch loss = 1.00 (43.2 examples/sec; 0.185 sec/batch; 16h:58m:37s remains)
INFO - root - 2017-12-03 08:52:55.481726: step 2370, loss = 1.17, batch loss = 1.10 (41.6 examples/sec; 0.192 sec/batch; 17h:39m:00s remains)
INFO - root - 2017-12-03 08:52:57.401131: step 2380, loss = 0.90, batch loss = 0.83 (43.5 examples/sec; 0.184 sec/batch; 16h:51m:05s remains)
INFO - root - 2017-12-03 08:52:59.312577: step 2390, loss = 0.82, batch loss = 0.75 (41.7 examples/sec; 0.192 sec/batch; 17h:35m:29s remains)
INFO - root - 2017-12-03 08:53:01.213887: step 2400, loss = 0.76, batch loss = 0.69 (42.7 examples/sec; 0.187 sec/batch; 17h:10m:19s remains)
INFO - root - 2017-12-03 08:53:03.224479: step 2410, loss = 0.82, batch loss = 0.75 (41.3 examples/sec; 0.194 sec/batch; 17h:46m:22s remains)
INFO - root - 2017-12-03 08:53:05.127454: step 2420, loss = 0.93, batch loss = 0.86 (42.5 examples/sec; 0.188 sec/batch; 17h:15m:35s remains)
INFO - root - 2017-12-03 08:53:07.053445: step 2430, loss = 1.26, batch loss = 1.19 (40.1 examples/sec; 0.199 sec/batch; 18h:16m:48s remains)
INFO - root - 2017-12-03 08:53:08.973264: step 2440, loss = 1.01, batch loss = 0.94 (42.1 examples/sec; 0.190 sec/batch; 17h:25m:49s remains)
INFO - root - 2017-12-03 08:53:10.895252: step 2450, loss = 1.02, batch loss = 0.95 (41.7 examples/sec; 0.192 sec/batch; 17h:35m:22s remains)
INFO - root - 2017-12-03 08:53:12.865407: step 2460, loss = 1.02, batch loss = 0.95 (41.9 examples/sec; 0.191 sec/batch; 17h:31m:17s remains)
INFO - root - 2017-12-03 08:53:14.797276: step 2470, loss = 1.10, batch loss = 1.03 (42.9 examples/sec; 0.186 sec/batch; 17h:05m:25s remains)
INFO - root - 2017-12-03 08:53:16.714125: step 2480, loss = 1.00, batch loss = 0.93 (43.4 examples/sec; 0.184 sec/batch; 16h:54m:05s remains)
INFO - root - 2017-12-03 08:53:18.608425: step 2490, loss = 0.76, batch loss = 0.69 (41.3 examples/sec; 0.194 sec/batch; 17h:45m:28s remains)
INFO - root - 2017-12-03 08:53:20.531386: step 2500, loss = 0.83, batch loss = 0.76 (41.8 examples/sec; 0.191 sec/batch; 17h:31m:27s remains)
INFO - root - 2017-12-03 08:53:22.531394: step 2510, loss = 0.92, batch loss = 0.85 (41.1 examples/sec; 0.195 sec/batch; 17h:51m:34s remains)
INFO - root - 2017-12-03 08:53:24.461349: step 2520, loss = 0.98, batch loss = 0.91 (38.2 examples/sec; 0.209 sec/batch; 19h:11m:37s remains)
INFO - root - 2017-12-03 08:53:26.408015: step 2530, loss = 1.21, batch loss = 1.14 (42.2 examples/sec; 0.189 sec/batch; 17h:21m:47s remains)
INFO - root - 2017-12-03 08:53:28.344319: step 2540, loss = 0.71, batch loss = 0.64 (41.5 examples/sec; 0.193 sec/batch; 17h:40m:06s remains)
INFO - root - 2017-12-03 08:53:30.259714: step 2550, loss = 1.06, batch loss = 0.99 (41.6 examples/sec; 0.192 sec/batch; 17h:38m:29s remains)
INFO - root - 2017-12-03 08:53:32.148088: step 2560, loss = 0.93, batch loss = 0.86 (42.5 examples/sec; 0.188 sec/batch; 17h:16m:17s remains)
INFO - root - 2017-12-03 08:53:34.061256: step 2570, loss = 1.07, batch loss = 1.00 (40.8 examples/sec; 0.196 sec/batch; 17h:57m:01s remains)
INFO - root - 2017-12-03 08:53:35.988783: step 2580, loss = 1.00, batch loss = 0.93 (40.2 examples/sec; 0.199 sec/batch; 18h:13m:31s remains)
INFO - root - 2017-12-03 08:53:37.883965: step 2590, loss = 1.11, batch loss = 1.04 (43.0 examples/sec; 0.186 sec/batch; 17h:01m:53s remains)
INFO - root - 2017-12-03 08:53:39.819550: step 2600, loss = 1.06, batch loss = 0.99 (41.0 examples/sec; 0.195 sec/batch; 17h:52m:19s remains)
INFO - root - 2017-12-03 08:53:41.821297: step 2610, loss = 0.95, batch loss = 0.88 (42.4 examples/sec; 0.189 sec/batch; 17h:18m:11s remains)
INFO - root - 2017-12-03 08:53:43.728218: step 2620, loss = 0.77, batch loss = 0.70 (40.7 examples/sec; 0.197 sec/batch; 18h:01m:32s remains)
INFO - root - 2017-12-03 08:53:45.642235: step 2630, loss = 1.12, batch loss = 1.05 (41.4 examples/sec; 0.193 sec/batch; 17h:41m:50s remains)
INFO - root - 2017-12-03 08:53:47.544657: step 2640, loss = 1.11, batch loss = 1.04 (43.0 examples/sec; 0.186 sec/batch; 17h:02m:04s remains)
INFO - root - 2017-12-03 08:53:49.460592: step 2650, loss = 1.12, batch loss = 1.05 (40.9 examples/sec; 0.196 sec/batch; 17h:55m:29s remains)
INFO - root - 2017-12-03 08:53:51.362748: step 2660, loss = 1.02, batch loss = 0.95 (41.8 examples/sec; 0.191 sec/batch; 17h:31m:42s remains)
INFO - root - 2017-12-03 08:53:53.314355: step 2670, loss = 1.12, batch loss = 1.05 (40.1 examples/sec; 0.199 sec/batch; 18h:15m:33s remains)
INFO - root - 2017-12-03 08:53:55.237875: step 2680, loss = 1.19, batch loss = 1.12 (43.7 examples/sec; 0.183 sec/batch; 16h:46m:26s remains)
INFO - root - 2017-12-03 08:53:57.144315: step 2690, loss = 1.40, batch loss = 1.33 (42.2 examples/sec; 0.190 sec/batch; 17h:22m:04s remains)
INFO - root - 2017-12-03 08:53:59.077101: step 2700, loss = 1.02, batch loss = 0.95 (41.9 examples/sec; 0.191 sec/batch; 17h:29m:38s remains)
INFO - root - 2017-12-03 08:54:01.075690: step 2710, loss = 1.42, batch loss = 1.35 (42.1 examples/sec; 0.190 sec/batch; 17h:23m:52s remains)
INFO - root - 2017-12-03 08:54:02.983292: step 2720, loss = 0.84, batch loss = 0.77 (41.4 examples/sec; 0.193 sec/batch; 17h:41m:31s remains)
INFO - root - 2017-12-03 08:54:04.930428: step 2730, loss = 1.27, batch loss = 1.20 (41.3 examples/sec; 0.194 sec/batch; 17h:43m:44s remains)
INFO - root - 2017-12-03 08:54:06.843781: step 2740, loss = 0.94, batch loss = 0.87 (42.5 examples/sec; 0.188 sec/batch; 17h:15m:44s remains)
INFO - root - 2017-12-03 08:54:08.774931: step 2750, loss = 1.01, batch loss = 0.94 (40.4 examples/sec; 0.198 sec/batch; 18h:08m:42s remains)
INFO - root - 2017-12-03 08:54:10.747748: step 2760, loss = 1.14, batch loss = 1.07 (40.6 examples/sec; 0.197 sec/batch; 18h:03m:34s remains)
INFO - root - 2017-12-03 08:54:12.674279: step 2770, loss = 1.18, batch loss = 1.11 (40.2 examples/sec; 0.199 sec/batch; 18h:12m:48s remains)
INFO - root - 2017-12-03 08:54:14.598547: step 2780, loss = 1.10, batch loss = 1.03 (42.4 examples/sec; 0.189 sec/batch; 17h:17m:22s remains)
INFO - root - 2017-12-03 08:54:16.498066: step 2790, loss = 0.92, batch loss = 0.85 (42.8 examples/sec; 0.187 sec/batch; 17h:06m:54s remains)
INFO - root - 2017-12-03 08:54:18.417322: step 2800, loss = 1.27, batch loss = 1.20 (41.9 examples/sec; 0.191 sec/batch; 17h:30m:21s remains)
INFO - root - 2017-12-03 08:54:20.421143: step 2810, loss = 0.95, batch loss = 0.88 (41.8 examples/sec; 0.192 sec/batch; 17h:32m:40s remains)
INFO - root - 2017-12-03 08:54:22.363211: step 2820, loss = 1.10, batch loss = 1.03 (40.2 examples/sec; 0.199 sec/batch; 18h:13m:54s remains)
INFO - root - 2017-12-03 08:54:24.269850: step 2830, loss = 1.39, batch loss = 1.32 (40.7 examples/sec; 0.197 sec/batch; 18h:00m:46s remains)
INFO - root - 2017-12-03 08:54:26.181084: step 2840, loss = 1.09, batch loss = 1.02 (42.2 examples/sec; 0.190 sec/batch; 17h:22m:05s remains)
INFO - root - 2017-12-03 08:54:28.117295: step 2850, loss = 1.16, batch loss = 1.09 (42.0 examples/sec; 0.191 sec/batch; 17h:26m:46s remains)
INFO - root - 2017-12-03 08:54:30.044761: step 2860, loss = 0.88, batch loss = 0.81 (41.9 examples/sec; 0.191 sec/batch; 17h:28m:29s remains)
INFO - root - 2017-12-03 08:54:31.962862: step 2870, loss = 0.95, batch loss = 0.88 (42.0 examples/sec; 0.191 sec/batch; 17h:26m:59s remains)
INFO - root - 2017-12-03 08:54:33.863847: step 2880, loss = 1.07, batch loss = 1.00 (42.1 examples/sec; 0.190 sec/batch; 17h:24m:50s remains)
INFO - root - 2017-12-03 08:54:35.767070: step 2890, loss = 1.20, batch loss = 1.13 (42.2 examples/sec; 0.190 sec/batch; 17h:21m:03s remains)
INFO - root - 2017-12-03 08:54:37.700957: step 2900, loss = 1.09, batch loss = 1.02 (42.8 examples/sec; 0.187 sec/batch; 17h:07m:36s remains)
INFO - root - 2017-12-03 08:54:39.715812: step 2910, loss = 1.09, batch loss = 1.02 (40.3 examples/sec; 0.198 sec/batch; 18h:09m:32s remains)
INFO - root - 2017-12-03 08:54:41.624086: step 2920, loss = 0.93, batch loss = 0.86 (43.1 examples/sec; 0.186 sec/batch; 16h:59m:24s remains)
INFO - root - 2017-12-03 08:54:43.533864: step 2930, loss = 1.22, batch loss = 1.15 (41.2 examples/sec; 0.194 sec/batch; 17h:47m:02s remains)
INFO - root - 2017-12-03 08:54:45.457726: step 2940, loss = 1.03, batch loss = 0.96 (41.6 examples/sec; 0.192 sec/batch; 17h:37m:19s remains)
INFO - root - 2017-12-03 08:54:47.383042: step 2950, loss = 1.34, batch loss = 1.27 (41.8 examples/sec; 0.191 sec/batch; 17h:31m:28s remains)
INFO - root - 2017-12-03 08:54:49.290856: step 2960, loss = 1.14, batch loss = 1.07 (42.9 examples/sec; 0.187 sec/batch; 17h:04m:42s remains)
INFO - root - 2017-12-03 08:54:51.205701: step 2970, loss = 0.92, batch loss = 0.85 (41.9 examples/sec; 0.191 sec/batch; 17h:29m:04s remains)
INFO - root - 2017-12-03 08:54:53.134071: step 2980, loss = 1.39, batch loss = 1.32 (39.5 examples/sec; 0.203 sec/batch; 18h:32m:56s remains)
INFO - root - 2017-12-03 08:54:55.057901: step 2990, loss = 1.23, batch loss = 1.16 (40.7 examples/sec; 0.197 sec/batch; 17h:59m:36s remains)
INFO - root - 2017-12-03 08:54:56.959084: step 3000, loss = 1.28, batch loss = 1.21 (42.2 examples/sec; 0.189 sec/batch; 17h:20m:33s remains)
INFO - root - 2017-12-03 08:54:58.937884: step 3010, loss = 1.16, batch loss = 1.09 (41.7 examples/sec; 0.192 sec/batch; 17h:32m:57s remains)
INFO - root - 2017-12-03 08:55:00.841285: step 3020, loss = 1.19, batch loss = 1.12 (41.5 examples/sec; 0.193 sec/batch; 17h:39m:45s remains)
INFO - root - 2017-12-03 08:55:02.778777: step 3030, loss = 1.14, batch loss = 1.07 (40.7 examples/sec; 0.197 sec/batch; 18h:00m:32s remains)
INFO - root - 2017-12-03 08:55:04.692721: step 3040, loss = 1.16, batch loss = 1.09 (42.9 examples/sec; 0.186 sec/batch; 17h:02m:59s remains)
INFO - root - 2017-12-03 08:55:06.605674: step 3050, loss = 1.13, batch loss = 1.06 (42.5 examples/sec; 0.188 sec/batch; 17h:14m:20s remains)
INFO - root - 2017-12-03 08:55:08.510809: step 3060, loss = 1.14, batch loss = 1.07 (42.1 examples/sec; 0.190 sec/batch; 17h:23m:50s remains)
INFO - root - 2017-12-03 08:55:10.454918: step 3070, loss = 0.89, batch loss = 0.82 (41.3 examples/sec; 0.194 sec/batch; 17h:43m:41s remains)
INFO - root - 2017-12-03 08:55:12.361252: step 3080, loss = 1.00, batch loss = 0.93 (42.3 examples/sec; 0.189 sec/batch; 17h:19m:00s remains)
INFO - root - 2017-12-03 08:55:14.302491: step 3090, loss = 1.22, batch loss = 1.15 (41.6 examples/sec; 0.192 sec/batch; 17h:35m:11s remains)
INFO - root - 2017-12-03 08:55:16.206136: step 3100, loss = 0.96, batch loss = 0.89 (42.2 examples/sec; 0.190 sec/batch; 17h:20m:58s remains)
INFO - root - 2017-12-03 08:55:18.175144: step 3110, loss = 1.09, batch loss = 1.02 (43.0 examples/sec; 0.186 sec/batch; 17h:00m:30s remains)
INFO - root - 2017-12-03 08:55:20.086787: step 3120, loss = 1.48, batch loss = 1.41 (41.8 examples/sec; 0.191 sec/batch; 17h:30m:03s remains)
INFO - root - 2017-12-03 08:55:22.006437: step 3130, loss = 1.19, batch loss = 1.12 (41.3 examples/sec; 0.194 sec/batch; 17h:42m:40s remains)
INFO - root - 2017-12-03 08:55:23.930193: step 3140, loss = 0.99, batch loss = 0.92 (41.8 examples/sec; 0.191 sec/batch; 17h:29m:59s remains)
INFO - root - 2017-12-03 08:55:25.855486: step 3150, loss = 1.01, batch loss = 0.94 (42.2 examples/sec; 0.189 sec/batch; 17h:19m:44s remains)
INFO - root - 2017-12-03 08:55:27.783756: step 3160, loss = 0.91, batch loss = 0.84 (41.2 examples/sec; 0.194 sec/batch; 17h:45m:29s remains)
INFO - root - 2017-12-03 08:55:29.694597: step 3170, loss = 1.05, batch loss = 0.97 (42.5 examples/sec; 0.188 sec/batch; 17h:12m:26s remains)
INFO - root - 2017-12-03 08:55:31.610969: step 3180, loss = 1.05, batch loss = 0.98 (41.8 examples/sec; 0.191 sec/batch; 17h:29m:45s remains)
INFO - root - 2017-12-03 08:55:33.501680: step 3190, loss = 1.10, batch loss = 1.03 (42.4 examples/sec; 0.189 sec/batch; 17h:15m:20s remains)
INFO - root - 2017-12-03 08:55:35.405570: step 3200, loss = 0.93, batch loss = 0.86 (41.3 examples/sec; 0.194 sec/batch; 17h:42m:05s remains)
INFO - root - 2017-12-03 08:55:37.372041: step 3210, loss = 1.30, batch loss = 1.23 (43.0 examples/sec; 0.186 sec/batch; 17h:01m:52s remains)
INFO - root - 2017-12-03 08:55:39.298943: step 3220, loss = 1.12, batch loss = 1.05 (41.6 examples/sec; 0.192 sec/batch; 17h:35m:05s remains)
INFO - root - 2017-12-03 08:55:41.239071: step 3230, loss = 0.85, batch loss = 0.78 (40.2 examples/sec; 0.199 sec/batch; 18h:12m:27s remains)
INFO - root - 2017-12-03 08:55:43.147826: step 3240, loss = 1.27, batch loss = 1.20 (40.9 examples/sec; 0.195 sec/batch; 17h:52m:50s remains)
INFO - root - 2017-12-03 08:55:45.070982: step 3250, loss = 0.89, batch loss = 0.82 (40.8 examples/sec; 0.196 sec/batch; 17h:55m:44s remains)
INFO - root - 2017-12-03 08:55:46.964066: step 3260, loss = 0.96, batch loss = 0.89 (42.5 examples/sec; 0.188 sec/batch; 17h:13m:05s remains)
INFO - root - 2017-12-03 08:55:48.874231: step 3270, loss = 1.04, batch loss = 0.97 (41.6 examples/sec; 0.192 sec/batch; 17h:36m:08s remains)
INFO - root - 2017-12-03 08:55:50.785858: step 3280, loss = 1.05, batch loss = 0.98 (42.6 examples/sec; 0.188 sec/batch; 17h:09m:26s remains)
INFO - root - 2017-12-03 08:55:52.714805: step 3290, loss = 0.87, batch loss = 0.80 (41.2 examples/sec; 0.194 sec/batch; 17h:44m:40s remains)
INFO - root - 2017-12-03 08:55:54.651018: step 3300, loss = 1.13, batch loss = 1.06 (41.7 examples/sec; 0.192 sec/batch; 17h:33m:49s remains)
INFO - root - 2017-12-03 08:55:56.628964: step 3310, loss = 0.86, batch loss = 0.79 (41.3 examples/sec; 0.194 sec/batch; 17h:42m:47s remains)
INFO - root - 2017-12-03 08:55:58.537806: step 3320, loss = 1.19, batch loss = 1.12 (41.7 examples/sec; 0.192 sec/batch; 17h:33m:28s remains)
INFO - root - 2017-12-03 08:56:00.464468: step 3330, loss = 1.04, batch loss = 0.97 (39.8 examples/sec; 0.201 sec/batch; 18h:22m:23s remains)
INFO - root - 2017-12-03 08:56:02.370979: step 3340, loss = 0.74, batch loss = 0.67 (44.9 examples/sec; 0.178 sec/batch; 16h:17m:41s remains)
INFO - root - 2017-12-03 08:56:04.287398: step 3350, loss = 1.04, batch loss = 0.97 (43.1 examples/sec; 0.185 sec/batch; 16h:57m:25s remains)
INFO - root - 2017-12-03 08:56:06.192275: step 3360, loss = 0.96, batch loss = 0.89 (42.1 examples/sec; 0.190 sec/batch; 17h:22m:44s remains)
INFO - root - 2017-12-03 08:56:08.113228: step 3370, loss = 0.94, batch loss = 0.87 (41.8 examples/sec; 0.191 sec/batch; 17h:30m:15s remains)
INFO - root - 2017-12-03 08:56:10.049836: step 3380, loss = 1.00, batch loss = 0.93 (41.4 examples/sec; 0.193 sec/batch; 17h:40m:45s remains)
INFO - root - 2017-12-03 08:56:11.990711: step 3390, loss = 1.06, batch loss = 0.99 (42.1 examples/sec; 0.190 sec/batch; 17h:21m:34s remains)
INFO - root - 2017-12-03 08:56:13.927443: step 3400, loss = 0.90, batch loss = 0.83 (43.0 examples/sec; 0.186 sec/batch; 16h:59m:40s remains)
INFO - root - 2017-12-03 08:56:15.911086: step 3410, loss = 1.03, batch loss = 0.96 (40.6 examples/sec; 0.197 sec/batch; 18h:00m:29s remains)
INFO - root - 2017-12-03 08:56:17.835675: step 3420, loss = 1.03, batch loss = 0.96 (43.0 examples/sec; 0.186 sec/batch; 17h:01m:03s remains)
INFO - root - 2017-12-03 08:56:19.748241: step 3430, loss = 0.88, batch loss = 0.81 (41.4 examples/sec; 0.193 sec/batch; 17h:39m:15s remains)
INFO - root - 2017-12-03 08:56:21.659038: step 3440, loss = 0.96, batch loss = 0.89 (41.9 examples/sec; 0.191 sec/batch; 17h:26m:24s remains)
INFO - root - 2017-12-03 08:56:23.572425: step 3450, loss = 1.07, batch loss = 1.00 (41.6 examples/sec; 0.192 sec/batch; 17h:34m:40s remains)
INFO - root - 2017-12-03 08:56:25.499432: step 3460, loss = 1.28, batch loss = 1.21 (42.2 examples/sec; 0.190 sec/batch; 17h:20m:24s remains)
INFO - root - 2017-12-03 08:56:27.396922: step 3470, loss = 1.08, batch loss = 1.01 (41.7 examples/sec; 0.192 sec/batch; 17h:33m:07s remains)
INFO - root - 2017-12-03 08:56:29.313973: step 3480, loss = 1.35, batch loss = 1.28 (41.1 examples/sec; 0.195 sec/batch; 17h:46m:56s remains)
INFO - root - 2017-12-03 08:56:31.256009: step 3490, loss = 1.21, batch loss = 1.14 (41.8 examples/sec; 0.191 sec/batch; 17h:28m:40s remains)
INFO - root - 2017-12-03 08:56:33.181520: step 3500, loss = 0.96, batch loss = 0.89 (42.3 examples/sec; 0.189 sec/batch; 17h:17m:11s remains)
INFO - root - 2017-12-03 08:56:35.147615: step 3510, loss = 1.02, batch loss = 0.95 (42.4 examples/sec; 0.189 sec/batch; 17h:14m:14s remains)
INFO - root - 2017-12-03 08:56:37.064162: step 3520, loss = 1.15, batch loss = 1.08 (41.1 examples/sec; 0.194 sec/batch; 17h:46m:23s remains)
INFO - root - 2017-12-03 08:56:38.984072: step 3530, loss = 1.07, batch loss = 1.00 (41.7 examples/sec; 0.192 sec/batch; 17h:31m:15s remains)
INFO - root - 2017-12-03 08:56:40.887278: step 3540, loss = 1.06, batch loss = 0.99 (41.5 examples/sec; 0.193 sec/batch; 17h:35m:46s remains)
INFO - root - 2017-12-03 08:56:42.794732: step 3550, loss = 1.02, batch loss = 0.95 (42.2 examples/sec; 0.190 sec/batch; 17h:20m:12s remains)
INFO - root - 2017-12-03 08:56:44.710948: step 3560, loss = 1.13, batch loss = 1.05 (41.8 examples/sec; 0.191 sec/batch; 17h:28m:38s remains)
INFO - root - 2017-12-03 08:56:46.645086: step 3570, loss = 1.21, batch loss = 1.14 (41.3 examples/sec; 0.194 sec/batch; 17h:41m:24s remains)
INFO - root - 2017-12-03 08:56:48.587558: step 3580, loss = 0.87, batch loss = 0.80 (41.7 examples/sec; 0.192 sec/batch; 17h:31m:24s remains)
INFO - root - 2017-12-03 08:56:50.487490: step 3590, loss = 0.94, batch loss = 0.87 (42.3 examples/sec; 0.189 sec/batch; 17h:17m:35s remains)
INFO - root - 2017-12-03 08:56:52.413135: step 3600, loss = 1.01, batch loss = 0.94 (41.9 examples/sec; 0.191 sec/batch; 17h:27m:08s remains)
INFO - root - 2017-12-03 08:56:54.411116: step 3610, loss = 1.13, batch loss = 1.06 (41.7 examples/sec; 0.192 sec/batch; 17h:30m:43s remains)
INFO - root - 2017-12-03 08:56:56.339788: step 3620, loss = 1.45, batch loss = 1.38 (41.3 examples/sec; 0.194 sec/batch; 17h:42m:18s remains)
INFO - root - 2017-12-03 08:56:58.258519: step 3630, loss = 1.22, batch loss = 1.15 (41.6 examples/sec; 0.192 sec/batch; 17h:33m:13s remains)
INFO - root - 2017-12-03 08:57:00.174875: step 3640, loss = 1.22, batch loss = 1.15 (42.0 examples/sec; 0.191 sec/batch; 17h:25m:08s remains)
INFO - root - 2017-12-03 08:57:02.076259: step 3650, loss = 1.38, batch loss = 1.31 (42.3 examples/sec; 0.189 sec/batch; 17h:16m:01s remains)
INFO - root - 2017-12-03 08:57:03.972867: step 3660, loss = 1.25, batch loss = 1.18 (40.9 examples/sec; 0.196 sec/batch; 17h:52m:44s remains)
INFO - root - 2017-12-03 08:57:05.905119: step 3670, loss = 1.53, batch loss = 1.46 (43.6 examples/sec; 0.184 sec/batch; 16h:46m:34s remains)
INFO - root - 2017-12-03 08:57:07.792680: step 3680, loss = 1.11, batch loss = 1.04 (41.7 examples/sec; 0.192 sec/batch; 17h:32m:19s remains)
INFO - root - 2017-12-03 08:57:09.697780: step 3690, loss = 1.57, batch loss = 1.50 (43.0 examples/sec; 0.186 sec/batch; 16h:59m:33s remains)
INFO - root - 2017-12-03 08:57:11.611184: step 3700, loss = 1.15, batch loss = 1.07 (42.1 examples/sec; 0.190 sec/batch; 17h:20m:30s remains)
INFO - root - 2017-12-03 08:57:13.609877: step 3710, loss = 1.09, batch loss = 1.02 (40.6 examples/sec; 0.197 sec/batch; 17h:59m:02s remains)
INFO - root - 2017-12-03 08:57:15.577936: step 3720, loss = 1.48, batch loss = 1.41 (41.5 examples/sec; 0.193 sec/batch; 17h:37m:26s remains)
INFO - root - 2017-12-03 08:57:17.508776: step 3730, loss = 1.34, batch loss = 1.27 (42.4 examples/sec; 0.189 sec/batch; 17h:13m:34s remains)
INFO - root - 2017-12-03 08:57:19.433172: step 3740, loss = 1.39, batch loss = 1.32 (41.9 examples/sec; 0.191 sec/batch; 17h:25m:16s remains)
INFO - root - 2017-12-03 08:57:21.387078: step 3750, loss = 1.23, batch loss = 1.15 (40.1 examples/sec; 0.199 sec/batch; 18h:12m:03s remains)
INFO - root - 2017-12-03 08:57:23.309037: step 3760, loss = 1.13, batch loss = 1.06 (42.0 examples/sec; 0.190 sec/batch; 17h:22m:41s remains)
INFO - root - 2017-12-03 08:57:25.230751: step 3770, loss = 1.32, batch loss = 1.25 (41.7 examples/sec; 0.192 sec/batch; 17h:32m:13s remains)
INFO - root - 2017-12-03 08:57:27.134751: step 3780, loss = 1.26, batch loss = 1.18 (39.6 examples/sec; 0.202 sec/batch; 18h:25m:25s remains)
INFO - root - 2017-12-03 08:57:29.046999: step 3790, loss = 1.19, batch loss = 1.12 (40.3 examples/sec; 0.199 sec/batch; 18h:07m:35s remains)
INFO - root - 2017-12-03 08:57:30.984167: step 3800, loss = 1.05, batch loss = 0.98 (41.6 examples/sec; 0.192 sec/batch; 17h:33m:31s remains)
INFO - root - 2017-12-03 08:57:32.974194: step 3810, loss = 1.39, batch loss = 1.32 (41.4 examples/sec; 0.193 sec/batch; 17h:39m:21s remains)
INFO - root - 2017-12-03 08:57:34.887626: step 3820, loss = 1.37, batch loss = 1.30 (41.3 examples/sec; 0.194 sec/batch; 17h:42m:04s remains)
INFO - root - 2017-12-03 08:57:36.811249: step 3830, loss = 1.29, batch loss = 1.21 (40.7 examples/sec; 0.197 sec/batch; 17h:56m:47s remains)
INFO - root - 2017-12-03 08:57:38.770716: step 3840, loss = 1.21, batch loss = 1.14 (41.5 examples/sec; 0.193 sec/batch; 17h:35m:03s remains)
INFO - root - 2017-12-03 08:57:40.717117: step 3850, loss = 1.15, batch loss = 1.08 (41.6 examples/sec; 0.192 sec/batch; 17h:32m:42s remains)
INFO - root - 2017-12-03 08:57:42.645589: step 3860, loss = 1.34, batch loss = 1.26 (40.8 examples/sec; 0.196 sec/batch; 17h:52m:51s remains)
INFO - root - 2017-12-03 08:57:44.532737: step 3870, loss = 1.51, batch loss = 1.43 (42.2 examples/sec; 0.190 sec/batch; 17h:19m:05s remains)
INFO - root - 2017-12-03 08:57:46.430579: step 3880, loss = 1.00, batch loss = 0.93 (41.5 examples/sec; 0.193 sec/batch; 17h:34m:35s remains)
INFO - root - 2017-12-03 08:57:48.377974: step 3890, loss = 1.36, batch loss = 1.29 (39.8 examples/sec; 0.201 sec/batch; 18h:21m:20s remains)
INFO - root - 2017-12-03 08:57:50.297424: step 3900, loss = 1.31, batch loss = 1.23 (42.2 examples/sec; 0.190 sec/batch; 17h:18m:32s remains)
INFO - root - 2017-12-03 08:57:52.258242: step 3910, loss = 1.25, batch loss = 1.18 (41.5 examples/sec; 0.193 sec/batch; 17h:34m:37s remains)
INFO - root - 2017-12-03 08:57:54.199891: step 3920, loss = 1.02, batch loss = 0.94 (41.3 examples/sec; 0.194 sec/batch; 17h:40m:41s remains)
INFO - root - 2017-12-03 08:57:56.140955: step 3930, loss = 1.18, batch loss = 1.11 (42.6 examples/sec; 0.188 sec/batch; 17h:07m:31s remains)
INFO - root - 2017-12-03 08:57:58.071578: step 3940, loss = 1.00, batch loss = 0.92 (42.4 examples/sec; 0.189 sec/batch; 17h:13m:34s remains)
INFO - root - 2017-12-03 08:57:59.980240: step 3950, loss = 1.38, batch loss = 1.31 (42.2 examples/sec; 0.189 sec/batch; 17h:16m:59s remains)
INFO - root - 2017-12-03 08:58:01.900340: step 3960, loss = 1.21, batch loss = 1.14 (40.7 examples/sec; 0.197 sec/batch; 17h:56m:20s remains)
INFO - root - 2017-12-03 08:58:03.808817: step 3970, loss = 1.33, batch loss = 1.26 (41.8 examples/sec; 0.191 sec/batch; 17h:26m:56s remains)
INFO - root - 2017-12-03 08:58:05.718267: step 3980, loss = 1.14, batch loss = 1.07 (42.6 examples/sec; 0.188 sec/batch; 17h:08m:37s remains)
INFO - root - 2017-12-03 08:58:07.619620: step 3990, loss = 1.53, batch loss = 1.46 (43.2 examples/sec; 0.185 sec/batch; 16h:53m:42s remains)
INFO - root - 2017-12-03 08:58:09.523342: step 4000, loss = 1.04, batch loss = 0.97 (41.7 examples/sec; 0.192 sec/batch; 17h:30m:02s remains)
INFO - root - 2017-12-03 08:58:11.530141: step 4010, loss = 1.26, batch loss = 1.19 (41.6 examples/sec; 0.193 sec/batch; 17h:34m:03s remains)
INFO - root - 2017-12-03 08:58:13.442438: step 4020, loss = 1.20, batch loss = 1.12 (42.8 examples/sec; 0.187 sec/batch; 17h:02m:17s remains)
INFO - root - 2017-12-03 08:58:15.381613: step 4030, loss = 1.42, batch loss = 1.35 (42.1 examples/sec; 0.190 sec/batch; 17h:20m:41s remains)
INFO - root - 2017-12-03 08:58:17.267045: step 4040, loss = 1.46, batch loss = 1.39 (41.4 examples/sec; 0.193 sec/batch; 17h:38m:55s remains)
INFO - root - 2017-12-03 08:58:19.158260: step 4050, loss = 1.25, batch loss = 1.17 (42.2 examples/sec; 0.190 sec/batch; 17h:17m:22s remains)
INFO - root - 2017-12-03 08:58:21.086182: step 4060, loss = 1.42, batch loss = 1.35 (40.6 examples/sec; 0.197 sec/batch; 17h:57m:26s remains)
INFO - root - 2017-12-03 08:58:23.009656: step 4070, loss = 1.22, batch loss = 1.14 (41.6 examples/sec; 0.192 sec/batch; 17h:32m:39s remains)
INFO - root - 2017-12-03 08:58:24.953924: step 4080, loss = 0.94, batch loss = 0.86 (41.1 examples/sec; 0.195 sec/batch; 17h:46m:12s remains)
INFO - root - 2017-12-03 08:58:26.884188: step 4090, loss = 1.40, batch loss = 1.33 (41.5 examples/sec; 0.193 sec/batch; 17h:35m:17s remains)
INFO - root - 2017-12-03 08:58:28.795352: step 4100, loss = 1.49, batch loss = 1.41 (41.0 examples/sec; 0.195 sec/batch; 17h:48m:26s remains)
INFO - root - 2017-12-03 08:58:30.770124: step 4110, loss = 1.53, batch loss = 1.46 (42.0 examples/sec; 0.191 sec/batch; 17h:22m:48s remains)
INFO - root - 2017-12-03 08:58:32.679636: step 4120, loss = 1.27, batch loss = 1.20 (41.7 examples/sec; 0.192 sec/batch; 17h:30m:16s remains)
INFO - root - 2017-12-03 08:58:34.621111: step 4130, loss = 1.22, batch loss = 1.14 (41.4 examples/sec; 0.193 sec/batch; 17h:37m:25s remains)
INFO - root - 2017-12-03 08:58:36.522089: step 4140, loss = 1.32, batch loss = 1.25 (43.3 examples/sec; 0.185 sec/batch; 16h:49m:57s remains)
INFO - root - 2017-12-03 08:58:38.439467: step 4150, loss = 1.32, batch loss = 1.24 (42.4 examples/sec; 0.189 sec/batch; 17h:12m:51s remains)
INFO - root - 2017-12-03 08:58:40.369505: step 4160, loss = 1.00, batch loss = 0.93 (40.2 examples/sec; 0.199 sec/batch; 18h:08m:59s remains)
INFO - root - 2017-12-03 08:58:42.280418: step 4170, loss = 1.44, batch loss = 1.36 (43.7 examples/sec; 0.183 sec/batch; 16h:41m:23s remains)
INFO - root - 2017-12-03 08:58:44.203905: step 4180, loss = 1.23, batch loss = 1.15 (40.2 examples/sec; 0.199 sec/batch; 18h:08m:45s remains)
INFO - root - 2017-12-03 08:58:46.132803: step 4190, loss = 1.15, batch loss = 1.08 (40.9 examples/sec; 0.196 sec/batch; 17h:51m:10s remains)
INFO - root - 2017-12-03 08:58:48.042499: step 4200, loss = 1.47, batch loss = 1.40 (41.5 examples/sec; 0.193 sec/batch; 17h:33m:43s remains)
INFO - root - 2017-12-03 08:58:50.009238: step 4210, loss = 1.21, batch loss = 1.13 (41.5 examples/sec; 0.193 sec/batch; 17h:35m:51s remains)
INFO - root - 2017-12-03 08:58:51.920559: step 4220, loss = 1.07, batch loss = 1.00 (41.9 examples/sec; 0.191 sec/batch; 17h:24m:14s remains)
INFO - root - 2017-12-03 08:58:53.836010: step 4230, loss = 1.30, batch loss = 1.23 (42.6 examples/sec; 0.188 sec/batch; 17h:06m:15s remains)
INFO - root - 2017-12-03 08:58:55.785820: step 4240, loss = 1.50, batch loss = 1.43 (41.7 examples/sec; 0.192 sec/batch; 17h:28m:22s remains)
INFO - root - 2017-12-03 08:58:57.713403: step 4250, loss = 1.15, batch loss = 1.08 (41.7 examples/sec; 0.192 sec/batch; 17h:30m:12s remains)
INFO - root - 2017-12-03 08:58:59.624127: step 4260, loss = 1.23, batch loss = 1.15 (42.1 examples/sec; 0.190 sec/batch; 17h:19m:45s remains)
INFO - root - 2017-12-03 08:59:01.540770: step 4270, loss = 1.19, batch loss = 1.12 (41.9 examples/sec; 0.191 sec/batch; 17h:25m:33s remains)
INFO - root - 2017-12-03 08:59:03.457204: step 4280, loss = 1.21, batch loss = 1.14 (40.4 examples/sec; 0.198 sec/batch; 18h:02m:44s remains)
INFO - root - 2017-12-03 08:59:05.374075: step 4290, loss = 1.18, batch loss = 1.11 (43.5 examples/sec; 0.184 sec/batch; 16h:45m:03s remains)
INFO - root - 2017-12-03 08:59:07.277978: step 4300, loss = 1.12, batch loss = 1.04 (42.9 examples/sec; 0.187 sec/batch; 17h:00m:25s remains)
INFO - root - 2017-12-03 08:59:09.265987: step 4310, loss = 1.01, batch loss = 0.93 (40.0 examples/sec; 0.200 sec/batch; 18h:14m:36s remains)
INFO - root - 2017-12-03 08:59:11.192451: step 4320, loss = 1.08, batch loss = 1.01 (43.0 examples/sec; 0.186 sec/batch; 16h:57m:07s remains)
INFO - root - 2017-12-03 08:59:13.106622: step 4330, loss = 1.08, batch loss = 1.00 (42.0 examples/sec; 0.190 sec/batch; 17h:21m:33s remains)
INFO - root - 2017-12-03 08:59:15.035186: step 4340, loss = 1.28, batch loss = 1.21 (39.5 examples/sec; 0.202 sec/batch; 18h:27m:09s remains)
INFO - root - 2017-12-03 08:59:16.945247: step 4350, loss = 1.17, batch loss = 1.09 (42.8 examples/sec; 0.187 sec/batch; 17h:03m:08s remains)
INFO - root - 2017-12-03 08:59:18.856713: step 4360, loss = 1.04, batch loss = 0.96 (41.2 examples/sec; 0.194 sec/batch; 17h:42m:01s remains)
INFO - root - 2017-12-03 08:59:20.778046: step 4370, loss = 1.15, batch loss = 1.07 (41.6 examples/sec; 0.192 sec/batch; 17h:31m:13s remains)
INFO - root - 2017-12-03 08:59:22.696317: step 4380, loss = 1.00, batch loss = 0.92 (44.6 examples/sec; 0.179 sec/batch; 16h:21m:10s remains)
INFO - root - 2017-12-03 08:59:24.629775: step 4390, loss = 1.30, batch loss = 1.22 (42.9 examples/sec; 0.187 sec/batch; 17h:00m:38s remains)
INFO - root - 2017-12-03 08:59:26.532392: step 4400, loss = 1.19, batch loss = 1.11 (42.1 examples/sec; 0.190 sec/batch; 17h:19m:38s remains)
INFO - root - 2017-12-03 08:59:28.516086: step 4410, loss = 1.22, batch loss = 1.15 (43.0 examples/sec; 0.186 sec/batch; 16h:57m:15s remains)
INFO - root - 2017-12-03 08:59:30.430583: step 4420, loss = 1.19, batch loss = 1.11 (42.1 examples/sec; 0.190 sec/batch; 17h:18m:32s remains)
INFO - root - 2017-12-03 08:59:32.335015: step 4430, loss = 1.03, batch loss = 0.95 (42.0 examples/sec; 0.190 sec/batch; 17h:21m:08s remains)
INFO - root - 2017-12-03 08:59:34.252046: step 4440, loss = 1.20, batch loss = 1.13 (38.5 examples/sec; 0.208 sec/batch; 18h:57m:01s remains)
INFO - root - 2017-12-03 08:59:36.170382: step 4450, loss = 1.39, batch loss = 1.32 (41.1 examples/sec; 0.195 sec/batch; 17h:44m:15s remains)
INFO - root - 2017-12-03 08:59:38.077824: step 4460, loss = 1.12, batch loss = 1.04 (41.7 examples/sec; 0.192 sec/batch; 17h:27m:38s remains)
INFO - root - 2017-12-03 08:59:40.031109: step 4470, loss = 1.13, batch loss = 1.06 (38.7 examples/sec; 0.207 sec/batch; 18h:50m:31s remains)
INFO - root - 2017-12-03 08:59:41.960083: step 4480, loss = 1.15, batch loss = 1.07 (40.6 examples/sec; 0.197 sec/batch; 17h:56m:05s remains)
INFO - root - 2017-12-03 08:59:43.855313: step 4490, loss = 1.26, batch loss = 1.19 (42.0 examples/sec; 0.191 sec/batch; 17h:22m:05s remains)
INFO - root - 2017-12-03 08:59:45.764531: step 4500, loss = 1.27, batch loss = 1.20 (42.3 examples/sec; 0.189 sec/batch; 17h:13m:54s remains)
INFO - root - 2017-12-03 08:59:47.726946: step 4510, loss = 1.06, batch loss = 0.98 (42.1 examples/sec; 0.190 sec/batch; 17h:18m:07s remains)
INFO - root - 2017-12-03 08:59:49.633202: step 4520, loss = 0.96, batch loss = 0.88 (42.6 examples/sec; 0.188 sec/batch; 17h:06m:45s remains)
INFO - root - 2017-12-03 08:59:51.549280: step 4530, loss = 1.36, batch loss = 1.28 (41.2 examples/sec; 0.194 sec/batch; 17h:41m:45s remains)
INFO - root - 2017-12-03 08:59:53.455523: step 4540, loss = 1.02, batch loss = 0.95 (41.5 examples/sec; 0.193 sec/batch; 17h:32m:47s remains)
INFO - root - 2017-12-03 08:59:55.372584: step 4550, loss = 1.37, batch loss = 1.29 (41.7 examples/sec; 0.192 sec/batch; 17h:27m:28s remains)
INFO - root - 2017-12-03 08:59:57.286250: step 4560, loss = 1.14, batch loss = 1.06 (41.8 examples/sec; 0.192 sec/batch; 17h:26m:46s remains)
INFO - root - 2017-12-03 08:59:59.218795: step 4570, loss = 1.21, batch loss = 1.14 (41.0 examples/sec; 0.195 sec/batch; 17h:45m:43s remains)
INFO - root - 2017-12-03 09:00:01.140363: step 4580, loss = 1.47, batch loss = 1.40 (42.5 examples/sec; 0.188 sec/batch; 17h:08m:12s remains)
INFO - root - 2017-12-03 09:00:03.051797: step 4590, loss = 1.03, batch loss = 0.95 (42.0 examples/sec; 0.191 sec/batch; 17h:21m:22s remains)
INFO - root - 2017-12-03 09:00:04.964555: step 4600, loss = 0.95, batch loss = 0.87 (40.8 examples/sec; 0.196 sec/batch; 17h:51m:50s remains)
INFO - root - 2017-12-03 09:00:06.972476: step 4610, loss = 1.38, batch loss = 1.30 (42.5 examples/sec; 0.188 sec/batch; 17h:07m:33s remains)
INFO - root - 2017-12-03 09:00:08.917106: step 4620, loss = 1.35, batch loss = 1.27 (39.3 examples/sec; 0.204 sec/batch; 18h:32m:52s remains)
INFO - root - 2017-12-03 09:00:10.879830: step 4630, loss = 1.36, batch loss = 1.28 (41.1 examples/sec; 0.195 sec/batch; 17h:44m:32s remains)
INFO - root - 2017-12-03 09:00:12.794255: step 4640, loss = 1.37, batch loss = 1.30 (42.4 examples/sec; 0.189 sec/batch; 17h:11m:17s remains)
INFO - root - 2017-12-03 09:00:14.711536: step 4650, loss = 1.35, batch loss = 1.28 (40.8 examples/sec; 0.196 sec/batch; 17h:50m:48s remains)
INFO - root - 2017-12-03 09:00:16.629046: step 4660, loss = 1.11, batch loss = 1.04 (42.2 examples/sec; 0.190 sec/batch; 17h:16m:42s remains)
INFO - root - 2017-12-03 09:00:18.544190: step 4670, loss = 1.40, batch loss = 1.32 (42.1 examples/sec; 0.190 sec/batch; 17h:17m:56s remains)
INFO - root - 2017-12-03 09:00:20.471744: step 4680, loss = 1.31, batch loss = 1.24 (41.9 examples/sec; 0.191 sec/batch; 17h:23m:22s remains)
INFO - root - 2017-12-03 09:00:22.392426: step 4690, loss = 1.26, batch loss = 1.19 (42.6 examples/sec; 0.188 sec/batch; 17h:06m:44s remains)
INFO - root - 2017-12-03 09:00:24.329785: step 4700, loss = 1.16, batch loss = 1.09 (39.2 examples/sec; 0.204 sec/batch; 18h:34m:06s remains)
INFO - root - 2017-12-03 09:00:26.302354: step 4710, loss = 1.34, batch loss = 1.26 (41.9 examples/sec; 0.191 sec/batch; 17h:21m:59s remains)
INFO - root - 2017-12-03 09:00:28.210549: step 4720, loss = 1.23, batch loss = 1.15 (43.2 examples/sec; 0.185 sec/batch; 16h:52m:00s remains)
INFO - root - 2017-12-03 09:00:30.123980: step 4730, loss = 1.44, batch loss = 1.37 (40.6 examples/sec; 0.197 sec/batch; 17h:56m:07s remains)
INFO - root - 2017-12-03 09:00:32.040740: step 4740, loss = 1.27, batch loss = 1.19 (41.9 examples/sec; 0.191 sec/batch; 17h:23m:25s remains)
INFO - root - 2017-12-03 09:00:33.952473: step 4750, loss = 1.13, batch loss = 1.05 (41.4 examples/sec; 0.193 sec/batch; 17h:36m:21s remains)
INFO - root - 2017-12-03 09:00:35.872529: step 4760, loss = 1.03, batch loss = 0.96 (40.9 examples/sec; 0.196 sec/batch; 17h:49m:40s remains)
INFO - root - 2017-12-03 09:00:37.794307: step 4770, loss = 1.21, batch loss = 1.14 (41.4 examples/sec; 0.193 sec/batch; 17h:36m:03s remains)
INFO - root - 2017-12-03 09:00:39.702077: step 4780, loss = 1.20, batch loss = 1.12 (43.9 examples/sec; 0.182 sec/batch; 16h:35m:10s remains)
INFO - root - 2017-12-03 09:00:41.610114: step 4790, loss = 1.11, batch loss = 1.04 (41.8 examples/sec; 0.191 sec/batch; 17h:25m:24s remains)
INFO - root - 2017-12-03 09:00:43.529105: step 4800, loss = 1.08, batch loss = 1.01 (40.8 examples/sec; 0.196 sec/batch; 17h:50m:12s remains)
INFO - root - 2017-12-03 09:00:45.518586: step 4810, loss = 1.24, batch loss = 1.17 (41.3 examples/sec; 0.193 sec/batch; 17h:36m:39s remains)
INFO - root - 2017-12-03 09:00:47.461290: step 4820, loss = 0.99, batch loss = 0.92 (41.6 examples/sec; 0.192 sec/batch; 17h:29m:08s remains)
INFO - root - 2017-12-03 09:00:49.370911: step 4830, loss = 1.13, batch loss = 1.05 (41.1 examples/sec; 0.195 sec/batch; 17h:43m:22s remains)
INFO - root - 2017-12-03 09:00:51.271944: step 4840, loss = 0.94, batch loss = 0.87 (43.2 examples/sec; 0.185 sec/batch; 16h:50m:24s remains)
INFO - root - 2017-12-03 09:00:53.210165: step 4850, loss = 1.11, batch loss = 1.03 (42.9 examples/sec; 0.187 sec/batch; 16h:59m:09s remains)
INFO - root - 2017-12-03 09:00:55.130680: step 4860, loss = 1.16, batch loss = 1.08 (40.9 examples/sec; 0.196 sec/batch; 17h:48m:36s remains)
INFO - root - 2017-12-03 09:00:57.065143: step 4870, loss = 1.58, batch loss = 1.51 (41.4 examples/sec; 0.193 sec/batch; 17h:35m:04s remains)
INFO - root - 2017-12-03 09:00:58.975367: step 4880, loss = 1.07, batch loss = 0.99 (41.9 examples/sec; 0.191 sec/batch; 17h:23m:43s remains)
INFO - root - 2017-12-03 09:01:00.884619: step 4890, loss = 0.86, batch loss = 0.78 (40.9 examples/sec; 0.195 sec/batch; 17h:47m:13s remains)
INFO - root - 2017-12-03 09:01:02.795043: step 4900, loss = 1.19, batch loss = 1.11 (42.6 examples/sec; 0.188 sec/batch; 17h:05m:27s remains)
INFO - root - 2017-12-03 09:01:04.756608: step 4910, loss = 0.90, batch loss = 0.82 (42.1 examples/sec; 0.190 sec/batch; 17h:17m:43s remains)
INFO - root - 2017-12-03 09:01:06.674259: step 4920, loss = 1.22, batch loss = 1.15 (41.0 examples/sec; 0.195 sec/batch; 17h:46m:18s remains)
INFO - root - 2017-12-03 09:01:08.569203: step 4930, loss = 1.11, batch loss = 1.04 (43.7 examples/sec; 0.183 sec/batch; 16h:40m:10s remains)
INFO - root - 2017-12-03 09:01:10.510443: step 4940, loss = 1.04, batch loss = 0.96 (40.8 examples/sec; 0.196 sec/batch; 17h:51m:30s remains)
INFO - root - 2017-12-03 09:01:12.450475: step 4950, loss = 0.98, batch loss = 0.91 (40.1 examples/sec; 0.199 sec/batch; 18h:07m:55s remains)
INFO - root - 2017-12-03 09:01:14.372322: step 4960, loss = 1.25, batch loss = 1.18 (41.9 examples/sec; 0.191 sec/batch; 17h:22m:02s remains)
INFO - root - 2017-12-03 09:01:16.268806: step 4970, loss = 1.17, batch loss = 1.09 (43.0 examples/sec; 0.186 sec/batch; 16h:55m:50s remains)
INFO - root - 2017-12-03 09:01:18.181894: step 4980, loss = 1.11, batch loss = 1.04 (42.2 examples/sec; 0.189 sec/batch; 17h:14m:05s remains)
INFO - root - 2017-12-03 09:01:20.105202: step 4990, loss = 1.42, batch loss = 1.35 (39.6 examples/sec; 0.202 sec/batch; 18h:22m:57s remains)
INFO - root - 2017-12-03 09:01:22.003728: step 5000, loss = 1.11, batch loss = 1.03 (42.6 examples/sec; 0.188 sec/batch; 17h:03m:55s remains)
INFO - root - 2017-12-03 09:01:24.010770: step 5010, loss = 1.47, batch loss = 1.39 (42.0 examples/sec; 0.190 sec/batch; 17h:19m:13s remains)
INFO - root - 2017-12-03 09:01:25.935530: step 5020, loss = 1.43, batch loss = 1.35 (42.3 examples/sec; 0.189 sec/batch; 17h:13m:14s remains)
INFO - root - 2017-12-03 09:01:27.853860: step 5030, loss = 1.16, batch loss = 1.08 (41.6 examples/sec; 0.192 sec/batch; 17h:30m:36s remains)
INFO - root - 2017-12-03 09:01:29.768586: step 5040, loss = 1.51, batch loss = 1.44 (41.6 examples/sec; 0.192 sec/batch; 17h:28m:34s remains)
INFO - root - 2017-12-03 09:01:31.676102: step 5050, loss = 1.14, batch loss = 1.07 (42.2 examples/sec; 0.190 sec/batch; 17h:15m:24s remains)
INFO - root - 2017-12-03 09:01:33.585472: step 5060, loss = 1.17, batch loss = 1.10 (41.9 examples/sec; 0.191 sec/batch; 17h:23m:00s remains)
INFO - root - 2017-12-03 09:01:35.498653: step 5070, loss = 1.48, batch loss = 1.41 (40.6 examples/sec; 0.197 sec/batch; 17h:55m:08s remains)
INFO - root - 2017-12-03 09:01:37.400993: step 5080, loss = 1.39, batch loss = 1.32 (41.4 examples/sec; 0.193 sec/batch; 17h:35m:16s remains)
INFO - root - 2017-12-03 09:01:39.315900: step 5090, loss = 1.23, batch loss = 1.16 (40.6 examples/sec; 0.197 sec/batch; 17h:55m:56s remains)
INFO - root - 2017-12-03 09:01:41.221861: step 5100, loss = 1.32, batch loss = 1.24 (42.1 examples/sec; 0.190 sec/batch; 17h:17m:33s remains)
INFO - root - 2017-12-03 09:01:43.187442: step 5110, loss = 1.01, batch loss = 0.94 (43.4 examples/sec; 0.185 sec/batch; 16h:46m:45s remains)
INFO - root - 2017-12-03 09:01:45.100966: step 5120, loss = 1.50, batch loss = 1.43 (41.9 examples/sec; 0.191 sec/batch; 17h:21m:57s remains)
INFO - root - 2017-12-03 09:01:46.996883: step 5130, loss = 1.40, batch loss = 1.33 (43.6 examples/sec; 0.184 sec/batch; 16h:42m:13s remains)
INFO - root - 2017-12-03 09:01:48.900999: step 5140, loss = 1.34, batch loss = 1.27 (42.5 examples/sec; 0.188 sec/batch; 17h:06m:52s remains)
INFO - root - 2017-12-03 09:01:50.819307: step 5150, loss = 1.06, batch loss = 0.98 (41.2 examples/sec; 0.194 sec/batch; 17h:40m:17s remains)
INFO - root - 2017-12-03 09:01:52.728418: step 5160, loss = 1.24, batch loss = 1.16 (42.0 examples/sec; 0.191 sec/batch; 17h:19m:29s remains)
INFO - root - 2017-12-03 09:01:54.640556: step 5170, loss = 1.15, batch loss = 1.07 (42.9 examples/sec; 0.187 sec/batch; 16h:57m:48s remains)
INFO - root - 2017-12-03 09:01:56.535761: step 5180, loss = 1.44, batch loss = 1.37 (42.3 examples/sec; 0.189 sec/batch; 17h:12m:22s remains)
INFO - root - 2017-12-03 09:01:58.454491: step 5190, loss = 1.42, batch loss = 1.35 (41.9 examples/sec; 0.191 sec/batch; 17h:21m:05s remains)
INFO - root - 2017-12-03 09:02:00.370624: step 5200, loss = 0.90, batch loss = 0.83 (41.9 examples/sec; 0.191 sec/batch; 17h:21m:48s remains)
INFO - root - 2017-12-03 09:02:02.363895: step 5210, loss = 1.22, batch loss = 1.14 (42.2 examples/sec; 0.190 sec/batch; 17h:14m:39s remains)
INFO - root - 2017-12-03 09:02:04.292040: step 5220, loss = 1.29, batch loss = 1.22 (42.3 examples/sec; 0.189 sec/batch; 17h:11m:56s remains)
INFO - root - 2017-12-03 09:02:06.206309: step 5230, loss = 1.46, batch loss = 1.38 (43.3 examples/sec; 0.185 sec/batch; 16h:46m:58s remains)
INFO - root - 2017-12-03 09:02:08.118647: step 5240, loss = 1.40, batch loss = 1.33 (40.9 examples/sec; 0.196 sec/batch; 17h:46m:47s remains)
INFO - root - 2017-12-03 09:02:10.029887: step 5250, loss = 0.93, batch loss = 0.86 (41.3 examples/sec; 0.194 sec/batch; 17h:36m:22s remains)
INFO - root - 2017-12-03 09:02:11.954837: step 5260, loss = 1.18, batch loss = 1.10 (42.5 examples/sec; 0.188 sec/batch; 17h:06m:58s remains)
INFO - root - 2017-12-03 09:02:13.899649: step 5270, loss = 1.24, batch loss = 1.16 (41.5 examples/sec; 0.193 sec/batch; 17h:31m:21s remains)
INFO - root - 2017-12-03 09:02:15.840342: step 5280, loss = 1.09, batch loss = 1.02 (42.1 examples/sec; 0.190 sec/batch; 17h:16m:25s remains)
INFO - root - 2017-12-03 09:02:17.767769: step 5290, loss = 1.03, batch loss = 0.96 (41.8 examples/sec; 0.191 sec/batch; 17h:22m:41s remains)
INFO - root - 2017-12-03 09:02:19.687917: step 5300, loss = 1.41, batch loss = 1.34 (42.7 examples/sec; 0.188 sec/batch; 17h:02m:53s remains)
INFO - root - 2017-12-03 09:02:21.697891: step 5310, loss = 1.13, batch loss = 1.05 (41.3 examples/sec; 0.194 sec/batch; 17h:36m:25s remains)
INFO - root - 2017-12-03 09:02:23.617082: step 5320, loss = 1.05, batch loss = 0.98 (43.2 examples/sec; 0.185 sec/batch; 16h:49m:14s remains)
INFO - root - 2017-12-03 09:02:25.548009: step 5330, loss = 1.05, batch loss = 0.97 (42.4 examples/sec; 0.188 sec/batch; 17h:07m:48s remains)
INFO - root - 2017-12-03 09:02:27.438276: step 5340, loss = 0.94, batch loss = 0.87 (42.6 examples/sec; 0.188 sec/batch; 17h:04m:36s remains)
INFO - root - 2017-12-03 09:02:29.356504: step 5350, loss = 1.16, batch loss = 1.09 (40.9 examples/sec; 0.196 sec/batch; 17h:47m:06s remains)
INFO - root - 2017-12-03 09:02:31.268827: step 5360, loss = 0.90, batch loss = 0.83 (40.7 examples/sec; 0.196 sec/batch; 17h:50m:29s remains)
INFO - root - 2017-12-03 09:02:33.177881: step 5370, loss = 1.24, batch loss = 1.16 (41.5 examples/sec; 0.193 sec/batch; 17h:32m:02s remains)
INFO - root - 2017-12-03 09:02:35.084905: step 5380, loss = 1.38, batch loss = 1.30 (41.9 examples/sec; 0.191 sec/batch; 17h:20m:49s remains)
INFO - root - 2017-12-03 09:02:36.993208: step 5390, loss = 1.14, batch loss = 1.06 (42.1 examples/sec; 0.190 sec/batch; 17h:16m:15s remains)
INFO - root - 2017-12-03 09:02:38.893881: step 5400, loss = 1.21, batch loss = 1.13 (42.4 examples/sec; 0.189 sec/batch; 17h:08m:42s remains)
INFO - root - 2017-12-03 09:02:40.903471: step 5410, loss = 1.33, batch loss = 1.26 (41.4 examples/sec; 0.193 sec/batch; 17h:33m:58s remains)
INFO - root - 2017-12-03 09:02:42.822377: step 5420, loss = 1.01, batch loss = 0.94 (41.5 examples/sec; 0.193 sec/batch; 17h:31m:51s remains)
INFO - root - 2017-12-03 09:02:44.725286: step 5430, loss = 1.06, batch loss = 0.98 (42.2 examples/sec; 0.190 sec/batch; 17h:14m:04s remains)
INFO - root - 2017-12-03 09:02:46.631065: step 5440, loss = 1.24, batch loss = 1.16 (42.3 examples/sec; 0.189 sec/batch; 17h:10m:31s remains)
INFO - root - 2017-12-03 09:02:48.533017: step 5450, loss = 1.09, batch loss = 1.02 (42.4 examples/sec; 0.189 sec/batch; 17h:08m:03s remains)
INFO - root - 2017-12-03 09:02:50.446739: step 5460, loss = 0.93, batch loss = 0.86 (41.2 examples/sec; 0.194 sec/batch; 17h:38m:35s remains)
INFO - root - 2017-12-03 09:02:52.366842: step 5470, loss = 1.19, batch loss = 1.11 (41.9 examples/sec; 0.191 sec/batch; 17h:20m:19s remains)
INFO - root - 2017-12-03 09:02:54.309667: step 5480, loss = 0.95, batch loss = 0.88 (39.4 examples/sec; 0.203 sec/batch; 18h:26m:55s remains)
INFO - root - 2017-12-03 09:02:56.234339: step 5490, loss = 1.27, batch loss = 1.20 (41.1 examples/sec; 0.195 sec/batch; 17h:40m:40s remains)
INFO - root - 2017-12-03 09:02:58.148658: step 5500, loss = 1.02, batch loss = 0.94 (40.9 examples/sec; 0.196 sec/batch; 17h:47m:12s remains)
INFO - root - 2017-12-03 09:03:00.176834: step 5510, loss = 1.03, batch loss = 0.95 (40.6 examples/sec; 0.197 sec/batch; 17h:54m:34s remains)
INFO - root - 2017-12-03 09:03:02.106275: step 5520, loss = 0.93, batch loss = 0.86 (38.8 examples/sec; 0.206 sec/batch; 18h:44m:09s remains)
INFO - root - 2017-12-03 09:03:04.033819: step 5530, loss = 1.08, batch loss = 1.00 (42.4 examples/sec; 0.188 sec/batch; 17h:07m:13s remains)
INFO - root - 2017-12-03 09:03:05.927302: step 5540, loss = 0.97, batch loss = 0.89 (42.7 examples/sec; 0.187 sec/batch; 17h:01m:24s remains)
INFO - root - 2017-12-03 09:03:07.849339: step 5550, loss = 1.00, batch loss = 0.92 (40.2 examples/sec; 0.199 sec/batch; 18h:03m:50s remains)
INFO - root - 2017-12-03 09:03:09.794939: step 5560, loss = 1.04, batch loss = 0.96 (40.0 examples/sec; 0.200 sec/batch; 18h:08m:46s remains)
INFO - root - 2017-12-03 09:03:11.695799: step 5570, loss = 0.98, batch loss = 0.90 (43.6 examples/sec; 0.183 sec/batch; 16h:39m:34s remains)
INFO - root - 2017-12-03 09:03:13.598863: step 5580, loss = 1.13, batch loss = 1.06 (40.6 examples/sec; 0.197 sec/batch; 17h:54m:42s remains)
INFO - root - 2017-12-03 09:03:15.515629: step 5590, loss = 1.01, batch loss = 0.93 (41.9 examples/sec; 0.191 sec/batch; 17h:19m:38s remains)
INFO - root - 2017-12-03 09:03:17.410852: step 5600, loss = 1.04, batch loss = 0.97 (43.6 examples/sec; 0.183 sec/batch; 16h:39m:14s remains)
INFO - root - 2017-12-03 09:03:19.392693: step 5610, loss = 0.98, batch loss = 0.91 (40.5 examples/sec; 0.198 sec/batch; 17h:56m:35s remains)
INFO - root - 2017-12-03 09:03:21.307837: step 5620, loss = 1.10, batch loss = 1.03 (41.1 examples/sec; 0.195 sec/batch; 17h:41m:17s remains)
INFO - root - 2017-12-03 09:03:23.227950: step 5630, loss = 1.03, batch loss = 0.96 (42.8 examples/sec; 0.187 sec/batch; 16h:57m:33s remains)
INFO - root - 2017-12-03 09:03:25.136579: step 5640, loss = 1.04, batch loss = 0.96 (41.7 examples/sec; 0.192 sec/batch; 17h:24m:11s remains)
INFO - root - 2017-12-03 09:03:27.065235: step 5650, loss = 0.99, batch loss = 0.92 (40.8 examples/sec; 0.196 sec/batch; 17h:49m:06s remains)
INFO - root - 2017-12-03 09:03:28.975282: step 5660, loss = 0.97, batch loss = 0.89 (41.1 examples/sec; 0.195 sec/batch; 17h:41m:21s remains)
INFO - root - 2017-12-03 09:03:30.912249: step 5670, loss = 1.15, batch loss = 1.08 (42.0 examples/sec; 0.190 sec/batch; 17h:16m:23s remains)
INFO - root - 2017-12-03 09:03:32.848500: step 5680, loss = 0.99, batch loss = 0.91 (41.0 examples/sec; 0.195 sec/batch; 17h:43m:27s remains)
INFO - root - 2017-12-03 09:03:34.774411: step 5690, loss = 1.10, batch loss = 1.03 (42.9 examples/sec; 0.187 sec/batch; 16h:56m:27s remains)
INFO - root - 2017-12-03 09:03:36.689783: step 5700, loss = 0.99, batch loss = 0.91 (40.8 examples/sec; 0.196 sec/batch; 17h:47m:14s remains)
INFO - root - 2017-12-03 09:03:38.668719: step 5710, loss = 0.95, batch loss = 0.88 (43.1 examples/sec; 0.186 sec/batch; 16h:50m:34s remains)
INFO - root - 2017-12-03 09:03:40.581598: step 5720, loss = 0.92, batch loss = 0.85 (41.5 examples/sec; 0.193 sec/batch; 17h:29m:34s remains)
INFO - root - 2017-12-03 09:03:42.474214: step 5730, loss = 1.11, batch loss = 1.03 (41.5 examples/sec; 0.193 sec/batch; 17h:29m:31s remains)
INFO - root - 2017-12-03 09:03:44.421605: step 5740, loss = 1.36, batch loss = 1.29 (41.7 examples/sec; 0.192 sec/batch; 17h:25m:11s remains)
INFO - root - 2017-12-03 09:03:46.329561: step 5750, loss = 1.15, batch loss = 1.07 (42.7 examples/sec; 0.187 sec/batch; 16h:59m:44s remains)
INFO - root - 2017-12-03 09:03:48.242941: step 5760, loss = 1.11, batch loss = 1.03 (42.3 examples/sec; 0.189 sec/batch; 17h:09m:50s remains)
INFO - root - 2017-12-03 09:03:50.173751: step 5770, loss = 0.91, batch loss = 0.84 (42.5 examples/sec; 0.188 sec/batch; 17h:05m:16s remains)
INFO - root - 2017-12-03 09:03:52.083814: step 5780, loss = 1.25, batch loss = 1.18 (40.6 examples/sec; 0.197 sec/batch; 17h:52m:41s remains)
INFO - root - 2017-12-03 09:03:54.017282: step 5790, loss = 1.18, batch loss = 1.10 (41.4 examples/sec; 0.193 sec/batch; 17h:31m:01s remains)
INFO - root - 2017-12-03 09:03:55.948463: step 5800, loss = 1.32, batch loss = 1.25 (39.1 examples/sec; 0.205 sec/batch; 18h:33m:44s remains)
INFO - root - 2017-12-03 09:03:57.932688: step 5810, loss = 1.15, batch loss = 1.07 (42.2 examples/sec; 0.190 sec/batch; 17h:11m:56s remains)
INFO - root - 2017-12-03 09:03:59.875358: step 5820, loss = 0.83, batch loss = 0.75 (40.5 examples/sec; 0.197 sec/batch; 17h:54m:59s remains)
INFO - root - 2017-12-03 09:04:01.784328: step 5830, loss = 0.96, batch loss = 0.88 (41.7 examples/sec; 0.192 sec/batch; 17h:23m:37s remains)
INFO - root - 2017-12-03 09:04:03.678470: step 5840, loss = 1.01, batch loss = 0.93 (40.6 examples/sec; 0.197 sec/batch; 17h:51m:42s remains)
INFO - root - 2017-12-03 09:04:05.590018: step 5850, loss = 1.04, batch loss = 0.96 (42.7 examples/sec; 0.188 sec/batch; 17h:01m:08s remains)
INFO - root - 2017-12-03 09:04:07.518193: step 5860, loss = 1.10, batch loss = 1.03 (40.7 examples/sec; 0.197 sec/batch; 17h:50m:11s remains)
INFO - root - 2017-12-03 09:04:09.437455: step 5870, loss = 1.04, batch loss = 0.96 (40.9 examples/sec; 0.196 sec/batch; 17h:44m:31s remains)
INFO - root - 2017-12-03 09:04:11.375733: step 5880, loss = 1.15, batch loss = 1.08 (41.7 examples/sec; 0.192 sec/batch; 17h:24m:56s remains)
INFO - root - 2017-12-03 09:04:13.296713: step 5890, loss = 1.14, batch loss = 1.07 (41.0 examples/sec; 0.195 sec/batch; 17h:43m:13s remains)
INFO - root - 2017-12-03 09:04:15.202281: step 5900, loss = 1.08, batch loss = 1.00 (41.7 examples/sec; 0.192 sec/batch; 17h:25m:27s remains)
INFO - root - 2017-12-03 09:04:17.164147: step 5910, loss = 1.29, batch loss = 1.21 (41.6 examples/sec; 0.192 sec/batch; 17h:26m:08s remains)
INFO - root - 2017-12-03 09:04:19.090630: step 5920, loss = 1.41, batch loss = 1.33 (43.0 examples/sec; 0.186 sec/batch; 16h:52m:37s remains)
INFO - root - 2017-12-03 09:04:21.021384: step 5930, loss = 1.05, batch loss = 0.97 (42.9 examples/sec; 0.187 sec/batch; 16h:55m:53s remains)
INFO - root - 2017-12-03 09:04:22.914994: step 5940, loss = 0.91, batch loss = 0.84 (43.1 examples/sec; 0.186 sec/batch; 16h:50m:43s remains)
INFO - root - 2017-12-03 09:04:24.839852: step 5950, loss = 1.15, batch loss = 1.07 (41.7 examples/sec; 0.192 sec/batch; 17h:24m:49s remains)
INFO - root - 2017-12-03 09:04:26.745290: step 5960, loss = 1.34, batch loss = 1.26 (43.1 examples/sec; 0.186 sec/batch; 16h:50m:15s remains)
INFO - root - 2017-12-03 09:04:28.646887: step 5970, loss = 1.09, batch loss = 1.01 (42.6 examples/sec; 0.188 sec/batch; 17h:02m:44s remains)
INFO - root - 2017-12-03 09:04:30.593827: step 5980, loss = 0.86, batch loss = 0.78 (39.9 examples/sec; 0.200 sec/batch; 18h:10m:50s remains)
INFO - root - 2017-12-03 09:04:32.506707: step 5990, loss = 1.10, batch loss = 1.02 (42.3 examples/sec; 0.189 sec/batch; 17h:09m:05s remains)
INFO - root - 2017-12-03 09:04:34.410771: step 6000, loss = 1.08, batch loss = 1.01 (42.2 examples/sec; 0.190 sec/batch; 17h:12m:12s remains)
INFO - root - 2017-12-03 09:04:36.392645: step 6010, loss = 0.95, batch loss = 0.87 (40.2 examples/sec; 0.199 sec/batch; 18h:03m:02s remains)
INFO - root - 2017-12-03 09:04:38.322987: step 6020, loss = 1.01, batch loss = 0.94 (42.3 examples/sec; 0.189 sec/batch; 17h:09m:55s remains)
INFO - root - 2017-12-03 09:04:40.275457: step 6030, loss = 1.18, batch loss = 1.10 (40.5 examples/sec; 0.197 sec/batch; 17h:53m:31s remains)
INFO - root - 2017-12-03 09:04:42.202707: step 6040, loss = 1.07, batch loss = 1.00 (39.7 examples/sec; 0.202 sec/batch; 18h:16m:39s remains)
INFO - root - 2017-12-03 09:04:44.105487: step 6050, loss = 0.94, batch loss = 0.87 (42.1 examples/sec; 0.190 sec/batch; 17h:13m:33s remains)
INFO - root - 2017-12-03 09:04:46.026738: step 6060, loss = 1.04, batch loss = 0.96 (42.1 examples/sec; 0.190 sec/batch; 17h:13m:56s remains)
INFO - root - 2017-12-03 09:04:47.939601: step 6070, loss = 1.07, batch loss = 0.99 (41.8 examples/sec; 0.192 sec/batch; 17h:22m:17s remains)
INFO - root - 2017-12-03 09:04:49.875414: step 6080, loss = 1.17, batch loss = 1.10 (41.6 examples/sec; 0.192 sec/batch; 17h:25m:51s remains)
INFO - root - 2017-12-03 09:04:51.777048: step 6090, loss = 0.99, batch loss = 0.92 (41.9 examples/sec; 0.191 sec/batch; 17h:17m:45s remains)
INFO - root - 2017-12-03 09:04:53.677212: step 6100, loss = 1.05, batch loss = 0.97 (42.6 examples/sec; 0.188 sec/batch; 17h:01m:30s remains)
INFO - root - 2017-12-03 09:04:55.680658: step 6110, loss = 1.02, batch loss = 0.94 (42.9 examples/sec; 0.186 sec/batch; 16h:54m:03s remains)
INFO - root - 2017-12-03 09:04:57.632767: step 6120, loss = 1.12, batch loss = 1.04 (39.2 examples/sec; 0.204 sec/batch; 18h:30m:15s remains)
INFO - root - 2017-12-03 09:04:59.558721: step 6130, loss = 0.96, batch loss = 0.88 (40.1 examples/sec; 0.199 sec/batch; 18h:04m:30s remains)
INFO - root - 2017-12-03 09:05:01.475477: step 6140, loss = 1.05, batch loss = 0.97 (41.9 examples/sec; 0.191 sec/batch; 17h:18m:21s remains)
INFO - root - 2017-12-03 09:05:03.421146: step 6150, loss = 1.12, batch loss = 1.04 (41.0 examples/sec; 0.195 sec/batch; 17h:41m:59s remains)
INFO - root - 2017-12-03 09:05:05.331549: step 6160, loss = 1.19, batch loss = 1.11 (42.5 examples/sec; 0.188 sec/batch; 17h:04m:02s remains)
INFO - root - 2017-12-03 09:05:07.235756: step 6170, loss = 1.29, batch loss = 1.21 (41.7 examples/sec; 0.192 sec/batch; 17h:23m:33s remains)
INFO - root - 2017-12-03 09:05:09.161007: step 6180, loss = 1.00, batch loss = 0.92 (41.5 examples/sec; 0.193 sec/batch; 17h:27m:41s remains)
INFO - root - 2017-12-03 09:05:11.114894: step 6190, loss = 0.91, batch loss = 0.83 (41.2 examples/sec; 0.194 sec/batch; 17h:34m:50s remains)
INFO - root - 2017-12-03 09:05:13.025955: step 6200, loss = 0.84, batch loss = 0.76 (41.5 examples/sec; 0.193 sec/batch; 17h:27m:18s remains)
INFO - root - 2017-12-03 09:05:14.996726: step 6210, loss = 0.82, batch loss = 0.74 (42.3 examples/sec; 0.189 sec/batch; 17h:07m:24s remains)
INFO - root - 2017-12-03 09:05:16.902259: step 6220, loss = 1.25, batch loss = 1.18 (42.3 examples/sec; 0.189 sec/batch; 17h:08m:52s remains)
INFO - root - 2017-12-03 09:05:18.822184: step 6230, loss = 0.87, batch loss = 0.79 (41.3 examples/sec; 0.194 sec/batch; 17h:33m:18s remains)
INFO - root - 2017-12-03 09:05:20.735550: step 6240, loss = 1.10, batch loss = 1.02 (40.3 examples/sec; 0.199 sec/batch; 18h:00m:37s remains)
INFO - root - 2017-12-03 09:05:22.637945: step 6250, loss = 0.99, batch loss = 0.91 (41.9 examples/sec; 0.191 sec/batch; 17h:19m:04s remains)
INFO - root - 2017-12-03 09:05:24.589501: step 6260, loss = 1.20, batch loss = 1.12 (40.7 examples/sec; 0.197 sec/batch; 17h:48m:42s remains)
INFO - root - 2017-12-03 09:05:26.542258: step 6270, loss = 0.98, batch loss = 0.90 (42.1 examples/sec; 0.190 sec/batch; 17h:13m:54s remains)
INFO - root - 2017-12-03 09:05:28.432989: step 6280, loss = 1.24, batch loss = 1.16 (42.2 examples/sec; 0.190 sec/batch; 17h:11m:13s remains)
INFO - root - 2017-12-03 09:05:30.352131: step 6290, loss = 1.45, batch loss = 1.38 (42.1 examples/sec; 0.190 sec/batch; 17h:13m:29s remains)
INFO - root - 2017-12-03 09:05:32.277894: step 6300, loss = 1.34, batch loss = 1.26 (42.0 examples/sec; 0.191 sec/batch; 17h:16m:13s remains)
INFO - root - 2017-12-03 09:05:34.279309: step 6310, loss = 1.37, batch loss = 1.29 (42.6 examples/sec; 0.188 sec/batch; 17h:00m:02s remains)
INFO - root - 2017-12-03 09:05:36.191269: step 6320, loss = 1.10, batch loss = 1.02 (41.7 examples/sec; 0.192 sec/batch; 17h:23m:47s remains)
INFO - root - 2017-12-03 09:05:38.119187: step 6330, loss = 1.55, batch loss = 1.47 (42.4 examples/sec; 0.189 sec/batch; 17h:06m:46s remains)
INFO - root - 2017-12-03 09:05:40.029851: step 6340, loss = 1.34, batch loss = 1.26 (41.7 examples/sec; 0.192 sec/batch; 17h:23m:23s remains)
INFO - root - 2017-12-03 09:05:41.934666: step 6350, loss = 1.45, batch loss = 1.37 (42.5 examples/sec; 0.188 sec/batch; 17h:04m:24s remains)
INFO - root - 2017-12-03 09:05:43.883550: step 6360, loss = 1.19, batch loss = 1.11 (40.4 examples/sec; 0.198 sec/batch; 17h:56m:37s remains)
INFO - root - 2017-12-03 09:05:45.815200: step 6370, loss = 0.99, batch loss = 0.91 (42.4 examples/sec; 0.189 sec/batch; 17h:05m:33s remains)
INFO - root - 2017-12-03 09:05:47.734769: step 6380, loss = 1.15, batch loss = 1.07 (41.5 examples/sec; 0.193 sec/batch; 17h:27m:36s remains)
INFO - root - 2017-12-03 09:05:49.671929: step 6390, loss = 1.15, batch loss = 1.07 (42.5 examples/sec; 0.188 sec/batch; 17h:01m:59s remains)
INFO - root - 2017-12-03 09:05:51.603870: step 6400, loss = 1.21, batch loss = 1.13 (41.4 examples/sec; 0.193 sec/batch; 17h:30m:02s remains)
INFO - root - 2017-12-03 09:05:53.568825: step 6410, loss = 0.95, batch loss = 0.87 (43.3 examples/sec; 0.185 sec/batch; 16h:43m:42s remains)
INFO - root - 2017-12-03 09:05:55.507300: step 6420, loss = 1.11, batch loss = 1.04 (42.1 examples/sec; 0.190 sec/batch; 17h:12m:31s remains)
INFO - root - 2017-12-03 09:05:57.427927: step 6430, loss = 1.36, batch loss = 1.28 (41.3 examples/sec; 0.194 sec/batch; 17h:33m:08s remains)
INFO - root - 2017-12-03 09:05:59.340701: step 6440, loss = 1.25, batch loss = 1.17 (41.7 examples/sec; 0.192 sec/batch; 17h:22m:45s remains)
INFO - root - 2017-12-03 09:06:01.261596: step 6450, loss = 1.17, batch loss = 1.09 (42.5 examples/sec; 0.188 sec/batch; 17h:04m:01s remains)
INFO - root - 2017-12-03 09:06:03.178520: step 6460, loss = 0.82, batch loss = 0.74 (41.0 examples/sec; 0.195 sec/batch; 17h:39m:31s remains)
INFO - root - 2017-12-03 09:06:05.125103: step 6470, loss = 1.13, batch loss = 1.05 (40.7 examples/sec; 0.197 sec/batch; 17h:49m:04s remains)
INFO - root - 2017-12-03 09:06:07.023695: step 6480, loss = 1.07, batch loss = 0.99 (43.5 examples/sec; 0.184 sec/batch; 16h:38m:29s remains)
INFO - root - 2017-12-03 09:06:08.965240: step 6490, loss = 1.77, batch loss = 1.69 (38.5 examples/sec; 0.208 sec/batch; 18h:49m:08s remains)
INFO - root - 2017-12-03 09:06:10.883278: step 6500, loss = 1.27, batch loss = 1.19 (41.3 examples/sec; 0.194 sec/batch; 17h:31m:45s remains)
INFO - root - 2017-12-03 09:06:12.891095: step 6510, loss = 1.04, batch loss = 0.96 (43.8 examples/sec; 0.183 sec/batch; 16h:32m:24s remains)
INFO - root - 2017-12-03 09:06:14.798206: step 6520, loss = 1.39, batch loss = 1.31 (40.2 examples/sec; 0.199 sec/batch; 18h:00m:40s remains)
INFO - root - 2017-12-03 09:06:16.730860: step 6530, loss = 1.00, batch loss = 0.92 (42.0 examples/sec; 0.191 sec/batch; 17h:15m:35s remains)
INFO - root - 2017-12-03 09:06:18.648248: step 6540, loss = 1.27, batch loss = 1.19 (41.9 examples/sec; 0.191 sec/batch; 17h:18m:21s remains)
INFO - root - 2017-12-03 09:06:20.590982: step 6550, loss = 0.77, batch loss = 0.69 (40.3 examples/sec; 0.198 sec/batch; 17h:57m:54s remains)
INFO - root - 2017-12-03 09:06:22.521119: step 6560, loss = 1.11, batch loss = 1.03 (41.6 examples/sec; 0.192 sec/batch; 17h:25m:06s remains)
INFO - root - 2017-12-03 09:06:24.428240: step 6570, loss = 0.93, batch loss = 0.85 (42.6 examples/sec; 0.188 sec/batch; 16h:59m:30s remains)
INFO - root - 2017-12-03 09:06:26.368584: step 6580, loss = 1.12, batch loss = 1.04 (40.3 examples/sec; 0.199 sec/batch; 17h:58m:19s remains)
INFO - root - 2017-12-03 09:06:28.278286: step 6590, loss = 1.12, batch loss = 1.03 (41.6 examples/sec; 0.192 sec/batch; 17h:23m:52s remains)
INFO - root - 2017-12-03 09:06:30.182870: step 6600, loss = 1.47, batch loss = 1.39 (41.5 examples/sec; 0.193 sec/batch; 17h:27m:48s remains)
INFO - root - 2017-12-03 09:06:32.183066: step 6610, loss = 1.22, batch loss = 1.14 (42.0 examples/sec; 0.190 sec/batch; 17h:13m:20s remains)
INFO - root - 2017-12-03 09:06:34.109992: step 6620, loss = 0.93, batch loss = 0.85 (41.1 examples/sec; 0.195 sec/batch; 17h:37m:15s remains)
INFO - root - 2017-12-03 09:06:36.045963: step 6630, loss = 1.27, batch loss = 1.19 (41.7 examples/sec; 0.192 sec/batch; 17h:22m:40s remains)
INFO - root - 2017-12-03 09:06:37.961153: step 6640, loss = 0.90, batch loss = 0.82 (41.2 examples/sec; 0.194 sec/batch; 17h:35m:47s remains)
INFO - root - 2017-12-03 09:06:39.870759: step 6650, loss = 1.29, batch loss = 1.21 (43.9 examples/sec; 0.182 sec/batch; 16h:28m:45s remains)
INFO - root - 2017-12-03 09:06:41.790929: step 6660, loss = 1.57, batch loss = 1.49 (42.0 examples/sec; 0.190 sec/batch; 17h:13m:37s remains)
INFO - root - 2017-12-03 09:06:43.708205: step 6670, loss = 0.94, batch loss = 0.86 (42.9 examples/sec; 0.186 sec/batch; 16h:52m:35s remains)
INFO - root - 2017-12-03 09:06:45.631399: step 6680, loss = 1.26, batch loss = 1.18 (42.0 examples/sec; 0.190 sec/batch; 17h:14m:09s remains)
INFO - root - 2017-12-03 09:06:47.566380: step 6690, loss = 1.34, batch loss = 1.26 (41.1 examples/sec; 0.195 sec/batch; 17h:37m:08s remains)
INFO - root - 2017-12-03 09:06:49.465118: step 6700, loss = 1.15, batch loss = 1.07 (41.2 examples/sec; 0.194 sec/batch; 17h:34m:48s remains)
INFO - root - 2017-12-03 09:06:51.448274: step 6710, loss = 1.43, batch loss = 1.35 (42.0 examples/sec; 0.191 sec/batch; 17h:14m:37s remains)
INFO - root - 2017-12-03 09:06:53.371951: step 6720, loss = 1.50, batch loss = 1.42 (41.0 examples/sec; 0.195 sec/batch; 17h:38m:44s remains)
INFO - root - 2017-12-03 09:06:55.292868: step 6730, loss = 1.40, batch loss = 1.31 (40.8 examples/sec; 0.196 sec/batch; 17h:44m:37s remains)
INFO - root - 2017-12-03 09:06:57.193842: step 6740, loss = 1.30, batch loss = 1.22 (41.5 examples/sec; 0.193 sec/batch; 17h:27m:09s remains)
INFO - root - 2017-12-03 09:06:59.091302: step 6750, loss = 1.33, batch loss = 1.25 (41.8 examples/sec; 0.191 sec/batch; 17h:17m:57s remains)
INFO - root - 2017-12-03 09:07:01.005379: step 6760, loss = 0.99, batch loss = 0.91 (41.8 examples/sec; 0.192 sec/batch; 17h:19m:40s remains)
INFO - root - 2017-12-03 09:07:02.920374: step 6770, loss = 1.26, batch loss = 1.18 (41.6 examples/sec; 0.192 sec/batch; 17h:23m:34s remains)
INFO - root - 2017-12-03 09:07:04.850583: step 6780, loss = 1.59, batch loss = 1.51 (40.5 examples/sec; 0.197 sec/batch; 17h:51m:18s remains)
INFO - root - 2017-12-03 09:07:06.747294: step 6790, loss = 1.29, batch loss = 1.21 (42.0 examples/sec; 0.190 sec/batch; 17h:13m:09s remains)
INFO - root - 2017-12-03 09:07:08.660993: step 6800, loss = 0.91, batch loss = 0.82 (42.5 examples/sec; 0.188 sec/batch; 17h:01m:13s remains)
INFO - root - 2017-12-03 09:07:10.638098: step 6810, loss = 1.09, batch loss = 1.01 (42.5 examples/sec; 0.188 sec/batch; 17h:02m:25s remains)
INFO - root - 2017-12-03 09:07:12.552952: step 6820, loss = 1.33, batch loss = 1.24 (42.4 examples/sec; 0.189 sec/batch; 17h:04m:41s remains)
INFO - root - 2017-12-03 09:07:14.497935: step 6830, loss = 1.15, batch loss = 1.07 (38.7 examples/sec; 0.206 sec/batch; 18h:40m:47s remains)
INFO - root - 2017-12-03 09:07:16.415962: step 6840, loss = 0.98, batch loss = 0.89 (41.7 examples/sec; 0.192 sec/batch; 17h:22m:09s remains)
INFO - root - 2017-12-03 09:07:18.337099: step 6850, loss = 1.30, batch loss = 1.22 (40.7 examples/sec; 0.197 sec/batch; 17h:47m:49s remains)
INFO - root - 2017-12-03 09:07:20.290275: step 6860, loss = 1.03, batch loss = 0.95 (39.3 examples/sec; 0.203 sec/batch; 18h:24m:08s remains)
INFO - root - 2017-12-03 09:07:22.229949: step 6870, loss = 1.50, batch loss = 1.42 (41.0 examples/sec; 0.195 sec/batch; 17h:40m:06s remains)
INFO - root - 2017-12-03 09:07:24.146766: step 6880, loss = 1.03, batch loss = 0.95 (42.4 examples/sec; 0.189 sec/batch; 17h:03m:26s remains)
INFO - root - 2017-12-03 09:07:26.053188: step 6890, loss = 1.35, batch loss = 1.27 (42.1 examples/sec; 0.190 sec/batch; 17h:12m:23s remains)
INFO - root - 2017-12-03 09:07:27.984148: step 6900, loss = 0.95, batch loss = 0.86 (41.4 examples/sec; 0.193 sec/batch; 17h:27m:55s remains)
INFO - root - 2017-12-03 09:07:29.965280: step 6910, loss = 1.28, batch loss = 1.20 (42.2 examples/sec; 0.190 sec/batch; 17h:08m:46s remains)
INFO - root - 2017-12-03 09:07:31.871594: step 6920, loss = 1.20, batch loss = 1.11 (42.7 examples/sec; 0.187 sec/batch; 16h:57m:16s remains)
INFO - root - 2017-12-03 09:07:33.815756: step 6930, loss = 1.00, batch loss = 0.92 (40.8 examples/sec; 0.196 sec/batch; 17h:42m:45s remains)
INFO - root - 2017-12-03 09:07:35.725312: step 6940, loss = 1.29, batch loss = 1.20 (42.4 examples/sec; 0.189 sec/batch; 17h:03m:15s remains)
INFO - root - 2017-12-03 09:07:37.631928: step 6950, loss = 1.17, batch loss = 1.08 (41.7 examples/sec; 0.192 sec/batch; 17h:21m:26s remains)
INFO - root - 2017-12-03 09:07:39.568339: step 6960, loss = 1.01, batch loss = 0.92 (41.7 examples/sec; 0.192 sec/batch; 17h:21m:02s remains)
INFO - root - 2017-12-03 09:07:41.506226: step 6970, loss = 0.77, batch loss = 0.69 (41.6 examples/sec; 0.192 sec/batch; 17h:23m:07s remains)
INFO - root - 2017-12-03 09:07:43.405463: step 6980, loss = 0.99, batch loss = 0.91 (43.1 examples/sec; 0.186 sec/batch; 16h:47m:14s remains)
INFO - root - 2017-12-03 09:07:45.327519: step 6990, loss = 1.04, batch loss = 0.96 (41.8 examples/sec; 0.191 sec/batch; 17h:18m:08s remains)
INFO - root - 2017-12-03 09:07:47.233549: step 7000, loss = 1.43, batch loss = 1.35 (42.0 examples/sec; 0.190 sec/batch; 17h:12m:16s remains)
INFO - root - 2017-12-03 09:07:49.192460: step 7010, loss = 0.97, batch loss = 0.89 (42.9 examples/sec; 0.186 sec/batch; 16h:51m:37s remains)
INFO - root - 2017-12-03 09:07:51.089559: step 7020, loss = 1.00, batch loss = 0.91 (42.2 examples/sec; 0.190 sec/batch; 17h:09m:31s remains)
INFO - root - 2017-12-03 09:07:53.008644: step 7030, loss = 1.02, batch loss = 0.94 (42.5 examples/sec; 0.188 sec/batch; 17h:01m:19s remains)
INFO - root - 2017-12-03 09:07:54.920430: step 7040, loss = 0.91, batch loss = 0.83 (42.3 examples/sec; 0.189 sec/batch; 17h:06m:11s remains)
INFO - root - 2017-12-03 09:07:56.852456: step 7050, loss = 1.28, batch loss = 1.19 (40.9 examples/sec; 0.196 sec/batch; 17h:41m:15s remains)
INFO - root - 2017-12-03 09:07:58.785207: step 7060, loss = 1.39, batch loss = 1.31 (40.0 examples/sec; 0.200 sec/batch; 18h:05m:34s remains)
INFO - root - 2017-12-03 09:08:00.697062: step 7070, loss = 1.17, batch loss = 1.09 (41.6 examples/sec; 0.192 sec/batch; 17h:23m:04s remains)
INFO - root - 2017-12-03 09:08:02.606635: step 7080, loss = 1.04, batch loss = 0.96 (42.8 examples/sec; 0.187 sec/batch; 16h:53m:53s remains)
INFO - root - 2017-12-03 09:08:04.511524: step 7090, loss = 1.14, batch loss = 1.06 (41.9 examples/sec; 0.191 sec/batch; 17h:15m:19s remains)
INFO - root - 2017-12-03 09:08:06.401511: step 7100, loss = 1.17, batch loss = 1.08 (41.9 examples/sec; 0.191 sec/batch; 17h:15m:33s remains)
INFO - root - 2017-12-03 09:08:08.385034: step 7110, loss = 1.14, batch loss = 1.06 (42.3 examples/sec; 0.189 sec/batch; 17h:04m:28s remains)
INFO - root - 2017-12-03 09:08:10.301195: step 7120, loss = 1.05, batch loss = 0.97 (42.1 examples/sec; 0.190 sec/batch; 17h:09m:50s remains)
INFO - root - 2017-12-03 09:08:12.226667: step 7130, loss = 0.82, batch loss = 0.73 (40.9 examples/sec; 0.196 sec/batch; 17h:41m:23s remains)
INFO - root - 2017-12-03 09:08:14.121647: step 7140, loss = 1.04, batch loss = 0.96 (41.0 examples/sec; 0.195 sec/batch; 17h:36m:49s remains)
INFO - root - 2017-12-03 09:08:16.031093: step 7150, loss = 0.92, batch loss = 0.84 (41.4 examples/sec; 0.193 sec/batch; 17h:27m:44s remains)
INFO - root - 2017-12-03 09:08:17.948253: step 7160, loss = 1.11, batch loss = 1.03 (41.9 examples/sec; 0.191 sec/batch; 17h:14m:34s remains)
INFO - root - 2017-12-03 09:08:19.861073: step 7170, loss = 1.34, batch loss = 1.26 (42.0 examples/sec; 0.191 sec/batch; 17h:12m:57s remains)
INFO - root - 2017-12-03 09:08:21.782561: step 7180, loss = 0.88, batch loss = 0.80 (42.5 examples/sec; 0.188 sec/batch; 17h:01m:24s remains)
INFO - root - 2017-12-03 09:08:23.719824: step 7190, loss = 1.01, batch loss = 0.92 (42.0 examples/sec; 0.190 sec/batch; 17h:12m:11s remains)
INFO - root - 2017-12-03 09:08:25.665978: step 7200, loss = 1.11, batch loss = 1.02 (40.2 examples/sec; 0.199 sec/batch; 17h:59m:45s remains)
INFO - root - 2017-12-03 09:08:27.665193: step 7210, loss = 1.15, batch loss = 1.06 (40.2 examples/sec; 0.199 sec/batch; 17h:58m:33s remains)
INFO - root - 2017-12-03 09:08:29.581280: step 7220, loss = 1.04, batch loss = 0.96 (42.1 examples/sec; 0.190 sec/batch; 17h:09m:20s remains)
INFO - root - 2017-12-03 09:08:31.512519: step 7230, loss = 1.18, batch loss = 1.09 (41.8 examples/sec; 0.191 sec/batch; 17h:17m:41s remains)
INFO - root - 2017-12-03 09:08:33.440028: step 7240, loss = 0.94, batch loss = 0.86 (42.7 examples/sec; 0.187 sec/batch; 16h:55m:47s remains)
INFO - root - 2017-12-03 09:08:35.356773: step 7250, loss = 0.91, batch loss = 0.82 (42.2 examples/sec; 0.190 sec/batch; 17h:08m:14s remains)
INFO - root - 2017-12-03 09:08:37.283214: step 7260, loss = 0.91, batch loss = 0.82 (41.5 examples/sec; 0.193 sec/batch; 17h:25m:04s remains)
INFO - root - 2017-12-03 09:08:39.239268: step 7270, loss = 1.14, batch loss = 1.06 (40.1 examples/sec; 0.200 sec/batch; 18h:02m:22s remains)
INFO - root - 2017-12-03 09:08:41.161990: step 7280, loss = 1.08, batch loss = 0.99 (43.2 examples/sec; 0.185 sec/batch; 16h:44m:40s remains)
INFO - root - 2017-12-03 09:08:43.091956: step 7290, loss = 1.05, batch loss = 0.97 (41.5 examples/sec; 0.193 sec/batch; 17h:23m:59s remains)
INFO - root - 2017-12-03 09:08:45.045547: step 7300, loss = 0.97, batch loss = 0.89 (42.6 examples/sec; 0.188 sec/batch; 16h:58m:58s remains)
INFO - root - 2017-12-03 09:08:47.068882: step 7310, loss = 1.05, batch loss = 0.96 (39.0 examples/sec; 0.205 sec/batch; 18h:30m:28s remains)
INFO - root - 2017-12-03 09:08:48.995539: step 7320, loss = 0.85, batch loss = 0.76 (41.5 examples/sec; 0.193 sec/batch; 17h:23m:35s remains)
INFO - root - 2017-12-03 09:08:50.896676: step 7330, loss = 1.20, batch loss = 1.11 (41.8 examples/sec; 0.191 sec/batch; 17h:17m:05s remains)
INFO - root - 2017-12-03 09:08:52.805333: step 7340, loss = 0.91, batch loss = 0.83 (42.4 examples/sec; 0.189 sec/batch; 17h:01m:38s remains)
INFO - root - 2017-12-03 09:08:54.722850: step 7350, loss = 1.02, batch loss = 0.93 (41.1 examples/sec; 0.195 sec/batch; 17h:35m:08s remains)
INFO - root - 2017-12-03 09:08:56.653020: step 7360, loss = 1.04, batch loss = 0.95 (41.5 examples/sec; 0.193 sec/batch; 17h:25m:07s remains)
INFO - root - 2017-12-03 09:08:58.578170: step 7370, loss = 0.91, batch loss = 0.83 (41.5 examples/sec; 0.193 sec/batch; 17h:24m:52s remains)
INFO - root - 2017-12-03 09:09:00.494851: step 7380, loss = 1.18, batch loss = 1.09 (41.2 examples/sec; 0.194 sec/batch; 17h:33m:18s remains)
INFO - root - 2017-12-03 09:09:02.418175: step 7390, loss = 1.04, batch loss = 0.95 (41.2 examples/sec; 0.194 sec/batch; 17h:33m:13s remains)
INFO - root - 2017-12-03 09:09:04.334580: step 7400, loss = 1.03, batch loss = 0.94 (42.7 examples/sec; 0.187 sec/batch; 16h:54m:35s remains)
INFO - root - 2017-12-03 09:09:06.315723: step 7410, loss = 1.07, batch loss = 0.99 (41.8 examples/sec; 0.191 sec/batch; 17h:16m:03s remains)
INFO - root - 2017-12-03 09:09:08.264648: step 7420, loss = 1.05, batch loss = 0.97 (39.0 examples/sec; 0.205 sec/batch; 18h:30m:18s remains)
INFO - root - 2017-12-03 09:09:10.183199: step 7430, loss = 1.16, batch loss = 1.07 (39.3 examples/sec; 0.203 sec/batch; 18h:21m:38s remains)
INFO - root - 2017-12-03 09:09:12.095303: step 7440, loss = 1.13, batch loss = 1.05 (41.0 examples/sec; 0.195 sec/batch; 17h:35m:49s remains)
INFO - root - 2017-12-03 09:09:13.993211: step 7450, loss = 1.13, batch loss = 1.04 (42.1 examples/sec; 0.190 sec/batch; 17h:09m:59s remains)
INFO - root - 2017-12-03 09:09:15.892599: step 7460, loss = 1.16, batch loss = 1.07 (42.2 examples/sec; 0.190 sec/batch; 17h:07m:26s remains)
INFO - root - 2017-12-03 09:09:17.792083: step 7470, loss = 0.94, batch loss = 0.85 (41.5 examples/sec; 0.193 sec/batch; 17h:24m:18s remains)
INFO - root - 2017-12-03 09:09:19.737860: step 7480, loss = 1.17, batch loss = 1.08 (39.2 examples/sec; 0.204 sec/batch; 18h:24m:24s remains)
INFO - root - 2017-12-03 09:09:21.659306: step 7490, loss = 1.28, batch loss = 1.19 (42.1 examples/sec; 0.190 sec/batch; 17h:09m:42s remains)
INFO - root - 2017-12-03 09:09:23.572744: step 7500, loss = 1.11, batch loss = 1.02 (40.6 examples/sec; 0.197 sec/batch; 17h:47m:29s remains)
INFO - root - 2017-12-03 09:09:25.586694: step 7510, loss = 0.98, batch loss = 0.89 (42.2 examples/sec; 0.190 sec/batch; 17h:07m:10s remains)
INFO - root - 2017-12-03 09:09:27.495298: step 7520, loss = 1.20, batch loss = 1.11 (41.5 examples/sec; 0.193 sec/batch; 17h:23m:30s remains)
INFO - root - 2017-12-03 09:09:29.402200: step 7530, loss = 1.20, batch loss = 1.11 (42.4 examples/sec; 0.189 sec/batch; 17h:02m:26s remains)
INFO - root - 2017-12-03 09:09:31.317387: step 7540, loss = 0.87, batch loss = 0.78 (41.0 examples/sec; 0.195 sec/batch; 17h:38m:02s remains)
INFO - root - 2017-12-03 09:09:33.209019: step 7550, loss = 1.08, batch loss = 0.99 (42.5 examples/sec; 0.188 sec/batch; 17h:00m:12s remains)
INFO - root - 2017-12-03 09:09:35.146886: step 7560, loss = 0.89, batch loss = 0.80 (40.7 examples/sec; 0.196 sec/batch; 17h:43m:28s remains)
INFO - root - 2017-12-03 09:09:37.037912: step 7570, loss = 0.99, batch loss = 0.90 (41.1 examples/sec; 0.195 sec/batch; 17h:34m:02s remains)
INFO - root - 2017-12-03 09:09:38.939798: step 7580, loss = 1.10, batch loss = 1.01 (42.8 examples/sec; 0.187 sec/batch; 16h:53m:20s remains)
INFO - root - 2017-12-03 09:09:40.875752: step 7590, loss = 1.20, batch loss = 1.11 (43.4 examples/sec; 0.184 sec/batch; 16h:38m:30s remains)
INFO - root - 2017-12-03 09:09:42.798473: step 7600, loss = 1.36, batch loss = 1.27 (41.3 examples/sec; 0.194 sec/batch; 17h:29m:07s remains)
INFO - root - 2017-12-03 09:09:44.806037: step 7610, loss = 1.12, batch loss = 1.03 (40.4 examples/sec; 0.198 sec/batch; 17h:52m:44s remains)
INFO - root - 2017-12-03 09:09:46.708484: step 7620, loss = 1.00, batch loss = 0.91 (41.3 examples/sec; 0.194 sec/batch; 17h:28m:41s remains)
INFO - root - 2017-12-03 09:09:48.609587: step 7630, loss = 1.14, batch loss = 1.05 (42.9 examples/sec; 0.187 sec/batch; 16h:50m:47s remains)
INFO - root - 2017-12-03 09:09:50.508513: step 7640, loss = 1.12, batch loss = 1.03 (42.6 examples/sec; 0.188 sec/batch; 16h:56m:10s remains)
INFO - root - 2017-12-03 09:09:52.401221: step 7650, loss = 1.12, batch loss = 1.03 (42.8 examples/sec; 0.187 sec/batch; 16h:51m:57s remains)
INFO - root - 2017-12-03 09:09:54.317884: step 7660, loss = 1.01, batch loss = 0.93 (41.3 examples/sec; 0.194 sec/batch; 17h:28m:15s remains)
INFO - root - 2017-12-03 09:09:56.230038: step 7670, loss = 1.22, batch loss = 1.13 (42.6 examples/sec; 0.188 sec/batch; 16h:56m:37s remains)
INFO - root - 2017-12-03 09:09:58.131774: step 7680, loss = 1.02, batch loss = 0.93 (42.2 examples/sec; 0.189 sec/batch; 17h:05m:50s remains)
INFO - root - 2017-12-03 09:10:00.031299: step 7690, loss = 0.96, batch loss = 0.87 (42.1 examples/sec; 0.190 sec/batch; 17h:08m:36s remains)
INFO - root - 2017-12-03 09:10:01.918718: step 7700, loss = 0.91, batch loss = 0.82 (42.1 examples/sec; 0.190 sec/batch; 17h:08m:44s remains)
INFO - root - 2017-12-03 09:10:03.905612: step 7710, loss = 0.91, batch loss = 0.82 (41.7 examples/sec; 0.192 sec/batch; 17h:17m:56s remains)
INFO - root - 2017-12-03 09:10:05.807043: step 7720, loss = 0.94, batch loss = 0.85 (43.3 examples/sec; 0.185 sec/batch; 16h:40m:04s remains)
INFO - root - 2017-12-03 09:10:07.729850: step 7730, loss = 1.18, batch loss = 1.09 (41.3 examples/sec; 0.193 sec/batch; 17h:27m:20s remains)
INFO - root - 2017-12-03 09:10:09.653062: step 7740, loss = 0.93, batch loss = 0.84 (39.9 examples/sec; 0.200 sec/batch; 18h:05m:13s remains)
INFO - root - 2017-12-03 09:10:11.577471: step 7750, loss = 0.86, batch loss = 0.77 (42.2 examples/sec; 0.190 sec/batch; 17h:06m:05s remains)
INFO - root - 2017-12-03 09:10:13.491720: step 7760, loss = 0.95, batch loss = 0.85 (42.9 examples/sec; 0.187 sec/batch; 16h:49m:28s remains)
INFO - root - 2017-12-03 09:10:15.394905: step 7770, loss = 0.81, batch loss = 0.72 (40.0 examples/sec; 0.200 sec/batch; 18h:01m:28s remains)
INFO - root - 2017-12-03 09:10:17.306654: step 7780, loss = 1.18, batch loss = 1.09 (42.7 examples/sec; 0.187 sec/batch; 16h:54m:07s remains)
INFO - root - 2017-12-03 09:10:19.206333: step 7790, loss = 1.06, batch loss = 0.97 (42.6 examples/sec; 0.188 sec/batch; 16h:57m:29s remains)
INFO - root - 2017-12-03 09:10:21.150215: step 7800, loss = 0.85, batch loss = 0.76 (41.3 examples/sec; 0.194 sec/batch; 17h:28m:41s remains)
INFO - root - 2017-12-03 09:10:23.146100: step 7810, loss = 1.02, batch loss = 0.93 (41.4 examples/sec; 0.193 sec/batch; 17h:26m:10s remains)
INFO - root - 2017-12-03 09:10:25.068713: step 7820, loss = 1.09, batch loss = 1.00 (39.9 examples/sec; 0.200 sec/batch; 18h:04m:24s remains)
INFO - root - 2017-12-03 09:10:26.982157: step 7830, loss = 1.06, batch loss = 0.97 (42.8 examples/sec; 0.187 sec/batch; 16h:51m:26s remains)
INFO - root - 2017-12-03 09:10:28.887135: step 7840, loss = 1.02, batch loss = 0.93 (42.6 examples/sec; 0.188 sec/batch; 16h:56m:37s remains)
INFO - root - 2017-12-03 09:10:30.804416: step 7850, loss = 1.28, batch loss = 1.19 (41.0 examples/sec; 0.195 sec/batch; 17h:36m:16s remains)
INFO - root - 2017-12-03 09:10:32.702652: step 7860, loss = 1.34, batch loss = 1.25 (42.1 examples/sec; 0.190 sec/batch; 17h:09m:13s remains)
INFO - root - 2017-12-03 09:10:34.618977: step 7870, loss = 1.16, batch loss = 1.07 (41.3 examples/sec; 0.194 sec/batch; 17h:27m:48s remains)
INFO - root - 2017-12-03 09:10:36.523560: step 7880, loss = 1.41, batch loss = 1.30 (41.0 examples/sec; 0.195 sec/batch; 17h:36m:55s remains)
INFO - root - 2017-12-03 09:10:38.420669: step 7890, loss = 1.00, batch loss = 0.90 (42.8 examples/sec; 0.187 sec/batch; 16h:52m:25s remains)
INFO - root - 2017-12-03 09:10:40.360144: step 7900, loss = 1.28, batch loss = 1.18 (40.2 examples/sec; 0.199 sec/batch; 17h:55m:28s remains)
INFO - root - 2017-12-03 09:10:42.402962: step 7910, loss = 1.59, batch loss = 1.48 (40.7 examples/sec; 0.196 sec/batch; 17h:42m:59s remains)
INFO - root - 2017-12-03 09:10:44.311451: step 7920, loss = 1.28, batch loss = 1.18 (41.0 examples/sec; 0.195 sec/batch; 17h:35m:58s remains)
INFO - root - 2017-12-03 09:10:46.217513: step 7930, loss = 1.22, batch loss = 1.11 (42.6 examples/sec; 0.188 sec/batch; 16h:56m:58s remains)
INFO - root - 2017-12-03 09:10:48.131692: step 7940, loss = 1.25, batch loss = 1.14 (39.9 examples/sec; 0.200 sec/batch; 18h:03m:45s remains)
INFO - root - 2017-12-03 09:10:50.040809: step 7950, loss = 1.64, batch loss = 1.53 (41.0 examples/sec; 0.195 sec/batch; 17h:35m:19s remains)
INFO - root - 2017-12-03 09:10:51.936685: step 7960, loss = 1.18, batch loss = 1.07 (40.7 examples/sec; 0.196 sec/batch; 17h:42m:20s remains)
INFO - root - 2017-12-03 09:10:53.853039: step 7970, loss = 1.17, batch loss = 1.07 (42.8 examples/sec; 0.187 sec/batch; 16h:50m:17s remains)
INFO - root - 2017-12-03 09:10:55.765919: step 7980, loss = 1.23, batch loss = 1.12 (41.5 examples/sec; 0.193 sec/batch; 17h:23m:11s remains)
INFO - root - 2017-12-03 09:10:57.659905: step 7990, loss = 1.20, batch loss = 1.09 (41.4 examples/sec; 0.193 sec/batch; 17h:24m:13s remains)
INFO - root - 2017-12-03 09:10:59.577000: step 8000, loss = 1.15, batch loss = 1.03 (40.7 examples/sec; 0.196 sec/batch; 17h:42m:00s remains)
INFO - root - 2017-12-03 09:11:01.555142: step 8010, loss = 0.99, batch loss = 0.88 (41.1 examples/sec; 0.195 sec/batch; 17h:33m:14s remains)
INFO - root - 2017-12-03 09:11:03.468474: step 8020, loss = 0.81, batch loss = 0.69 (41.0 examples/sec; 0.195 sec/batch; 17h:34m:09s remains)
INFO - root - 2017-12-03 09:11:05.371621: step 8030, loss = 0.79, batch loss = 0.68 (42.1 examples/sec; 0.190 sec/batch; 17h:08m:14s remains)
INFO - root - 2017-12-03 09:11:07.270631: step 8040, loss = 0.73, batch loss = 0.61 (40.9 examples/sec; 0.196 sec/batch; 17h:38m:35s remains)
INFO - root - 2017-12-03 09:11:09.178363: step 8050, loss = 1.00, batch loss = 0.88 (42.9 examples/sec; 0.186 sec/batch; 16h:47m:27s remains)
INFO - root - 2017-12-03 09:11:11.093694: step 8060, loss = 1.62, batch loss = 1.33 (42.3 examples/sec; 0.189 sec/batch; 17h:03m:04s remains)
INFO - root - 2017-12-03 09:11:13.010648: step 8070, loss = 1.66, batch loss = 1.36 (42.1 examples/sec; 0.190 sec/batch; 17h:08m:00s remains)
INFO - root - 2017-12-03 09:11:14.879934: step 8080, loss = 1.85, batch loss = 1.55 (40.4 examples/sec; 0.198 sec/batch; 17h:51m:41s remains)
INFO - root - 2017-12-03 09:11:16.779045: step 8090, loss = 1.53, batch loss = 1.24 (39.4 examples/sec; 0.203 sec/batch; 18h:18m:05s remains)
INFO - root - 2017-12-03 09:11:18.695965: step 8100, loss = 1.94, batch loss = 1.64 (41.9 examples/sec; 0.191 sec/batch; 17h:11m:06s remains)
INFO - root - 2017-12-03 09:11:20.667793: step 8110, loss = 1.35, batch loss = 1.05 (42.4 examples/sec; 0.189 sec/batch; 17h:00m:37s remains)
INFO - root - 2017-12-03 09:11:22.573285: step 8120, loss = 1.62, batch loss = 1.33 (43.0 examples/sec; 0.186 sec/batch; 16h:44m:42s remains)
INFO - root - 2017-12-03 09:11:24.490156: step 8130, loss = 1.63, batch loss = 1.33 (42.6 examples/sec; 0.188 sec/batch; 16h:54m:39s remains)
INFO - root - 2017-12-03 09:11:26.381819: step 8140, loss = 1.61, batch loss = 1.31 (41.0 examples/sec; 0.195 sec/batch; 17h:35m:34s remains)
INFO - root - 2017-12-03 09:11:28.297938: step 8150, loss = 1.09, batch loss = 0.80 (41.4 examples/sec; 0.193 sec/batch; 17h:24m:30s remains)
INFO - root - 2017-12-03 09:11:30.238651: step 8160, loss = 1.15, batch loss = 0.85 (41.9 examples/sec; 0.191 sec/batch; 17h:12m:14s remains)
INFO - root - 2017-12-03 09:11:32.121901: step 8170, loss = 1.21, batch loss = 0.92 (42.0 examples/sec; 0.190 sec/batch; 17h:09m:00s remains)
INFO - root - 2017-12-03 09:11:34.014732: step 8180, loss = 1.63, batch loss = 1.33 (42.4 examples/sec; 0.189 sec/batch; 16h:59m:40s remains)
INFO - root - 2017-12-03 09:11:35.917901: step 8190, loss = 1.01, batch loss = 0.71 (42.4 examples/sec; 0.189 sec/batch; 16h:59m:51s remains)
INFO - root - 2017-12-03 09:11:37.820014: step 8200, loss = 1.69, batch loss = 1.39 (41.5 examples/sec; 0.193 sec/batch; 17h:22m:36s remains)
INFO - root - 2017-12-03 09:11:39.823433: step 8210, loss = 1.51, batch loss = 1.21 (41.2 examples/sec; 0.194 sec/batch; 17h:29m:09s remains)
INFO - root - 2017-12-03 09:11:41.726915: step 8220, loss = 1.03, batch loss = 0.73 (41.2 examples/sec; 0.194 sec/batch; 17h:28m:23s remains)
INFO - root - 2017-12-03 09:11:43.615426: step 8230, loss = 1.38, batch loss = 1.08 (40.8 examples/sec; 0.196 sec/batch; 17h:39m:27s remains)
INFO - root - 2017-12-03 09:11:45.481844: step 8240, loss = 1.27, batch loss = 0.97 (42.3 examples/sec; 0.189 sec/batch; 17h:02m:28s remains)
INFO - root - 2017-12-03 09:11:47.407502: step 8250, loss = 1.44, batch loss = 1.14 (41.9 examples/sec; 0.191 sec/batch; 17h:13m:01s remains)
INFO - root - 2017-12-03 09:11:49.317359: step 8260, loss = 1.36, batch loss = 1.06 (42.5 examples/sec; 0.188 sec/batch; 16h:56m:43s remains)
INFO - root - 2017-12-03 09:11:51.235659: step 8270, loss = 1.34, batch loss = 1.03 (41.5 examples/sec; 0.193 sec/batch; 17h:22m:24s remains)
INFO - root - 2017-12-03 09:11:53.153235: step 8280, loss = 1.13, batch loss = 0.83 (41.7 examples/sec; 0.192 sec/batch; 17h:16m:07s remains)
INFO - root - 2017-12-03 09:11:55.041947: step 8290, loss = 1.56, batch loss = 1.26 (40.8 examples/sec; 0.196 sec/batch; 17h:40m:14s remains)
INFO - root - 2017-12-03 09:11:56.949234: step 8300, loss = 1.53, batch loss = 1.23 (42.7 examples/sec; 0.187 sec/batch; 16h:51m:29s remains)
INFO - root - 2017-12-03 09:11:58.910050: step 8310, loss = 1.53, batch loss = 1.23 (42.5 examples/sec; 0.188 sec/batch; 16h:57m:58s remains)
INFO - root - 2017-12-03 09:12:00.799445: step 8320, loss = 1.73, batch loss = 1.43 (42.3 examples/sec; 0.189 sec/batch; 17h:01m:06s remains)
INFO - root - 2017-12-03 09:12:02.693920: step 8330, loss = 1.13, batch loss = 0.83 (40.5 examples/sec; 0.198 sec/batch; 17h:47m:37s remains)
INFO - root - 2017-12-03 09:12:04.589649: step 8340, loss = 1.15, batch loss = 0.84 (42.3 examples/sec; 0.189 sec/batch; 17h:01m:16s remains)
INFO - root - 2017-12-03 09:12:06.462378: step 8350, loss = 1.59, batch loss = 1.29 (43.4 examples/sec; 0.184 sec/batch; 16h:35m:52s remains)
INFO - root - 2017-12-03 09:12:08.333960: step 8360, loss = 1.81, batch loss = 1.51 (43.2 examples/sec; 0.185 sec/batch; 16h:40m:43s remains)
INFO - root - 2017-12-03 09:12:10.249071: step 8370, loss = 1.65, batch loss = 1.34 (40.1 examples/sec; 0.199 sec/batch; 17h:57m:36s remains)
INFO - root - 2017-12-03 09:12:12.144942: step 8380, loss = 1.96, batch loss = 1.64 (41.2 examples/sec; 0.194 sec/batch; 17h:28m:22s remains)
INFO - root - 2017-12-03 09:12:14.050870: step 8390, loss = 1.65, batch loss = 1.34 (40.7 examples/sec; 0.196 sec/batch; 17h:40m:37s remains)
INFO - root - 2017-12-03 09:12:15.971606: step 8400, loss = 2.17, batch loss = 1.86 (42.7 examples/sec; 0.187 sec/batch; 16h:52m:40s remains)
INFO - root - 2017-12-03 09:12:17.962969: step 8410, loss = 1.55, batch loss = 1.23 (39.8 examples/sec; 0.201 sec/batch; 18h:04m:24s remains)
INFO - root - 2017-12-03 09:12:19.867232: step 8420, loss = 1.90, batch loss = 1.59 (40.7 examples/sec; 0.196 sec/batch; 17h:41m:13s remains)
INFO - root - 2017-12-03 09:12:21.767718: step 8430, loss = 1.89, batch loss = 1.57 (42.5 examples/sec; 0.188 sec/batch; 16h:56m:00s remains)
INFO - root - 2017-12-03 09:12:23.654667: step 8440, loss = 2.04, batch loss = 1.72 (41.3 examples/sec; 0.194 sec/batch; 17h:25m:29s remains)
INFO - root - 2017-12-03 09:12:25.588476: step 8450, loss = 1.83, batch loss = 1.52 (40.6 examples/sec; 0.197 sec/batch; 17h:45m:16s remains)
INFO - root - 2017-12-03 09:12:27.524183: step 8460, loss = 2.12, batch loss = 1.80 (42.4 examples/sec; 0.189 sec/batch; 16h:59m:38s remains)
INFO - root - 2017-12-03 09:12:29.406311: step 8470, loss = 2.05, batch loss = 1.74 (42.0 examples/sec; 0.190 sec/batch; 17h:07m:38s remains)
INFO - root - 2017-12-03 09:12:31.318642: step 8480, loss = 1.92, batch loss = 1.61 (42.3 examples/sec; 0.189 sec/batch; 17h:01m:10s remains)
INFO - root - 2017-12-03 09:12:33.189509: step 8490, loss = 2.11, batch loss = 1.79 (43.4 examples/sec; 0.184 sec/batch; 16h:35m:05s remains)
INFO - root - 2017-12-03 09:12:35.079642: step 8500, loss = 2.49, batch loss = 2.18 (42.4 examples/sec; 0.189 sec/batch; 16h:58m:27s remains)
INFO - root - 2017-12-03 09:12:37.056934: step 8510, loss = 1.85, batch loss = 1.53 (42.8 examples/sec; 0.187 sec/batch; 16h:48m:26s remains)
INFO - root - 2017-12-03 09:12:38.932991: step 8520, loss = 1.81, batch loss = 1.49 (43.1 examples/sec; 0.185 sec/batch; 16h:41m:15s remains)
INFO - root - 2017-12-03 09:12:40.810596: step 8530, loss = 1.82, batch loss = 1.50 (42.7 examples/sec; 0.187 sec/batch; 16h:50m:34s remains)
INFO - root - 2017-12-03 09:12:42.715459: step 8540, loss = 2.24, batch loss = 1.78 (41.5 examples/sec; 0.193 sec/batch; 17h:20m:32s remains)
INFO - root - 2017-12-03 09:12:44.639750: step 8550, loss = 2.04, batch loss = 1.58 (40.4 examples/sec; 0.198 sec/batch; 17h:49m:03s remains)
INFO - root - 2017-12-03 09:12:46.534849: step 8560, loss = 2.19, batch loss = 1.73 (43.2 examples/sec; 0.185 sec/batch; 16h:38m:51s remains)
INFO - root - 2017-12-03 09:12:48.446405: step 8570, loss = 2.35, batch loss = 1.89 (41.4 examples/sec; 0.193 sec/batch; 17h:24m:09s remains)
INFO - root - 2017-12-03 09:12:50.361520: step 8580, loss = 2.60, batch loss = 2.14 (43.1 examples/sec; 0.186 sec/batch; 16h:42m:22s remains)
INFO - root - 2017-12-03 09:12:52.288071: step 8590, loss = 2.47, batch loss = 2.00 (40.7 examples/sec; 0.197 sec/batch; 17h:41m:46s remains)
INFO - root - 2017-12-03 09:12:54.156523: step 8600, loss = 2.71, batch loss = 2.23 (42.8 examples/sec; 0.187 sec/batch; 16h:48m:55s remains)
INFO - root - 2017-12-03 09:12:56.128701: step 8610, loss = 2.50, batch loss = 2.02 (42.8 examples/sec; 0.187 sec/batch; 16h:49m:22s remains)
INFO - root - 2017-12-03 09:12:58.037273: step 8620, loss = 2.56, batch loss = 2.07 (41.0 examples/sec; 0.195 sec/batch; 17h:32m:10s remains)
INFO - root - 2017-12-03 09:12:59.921441: step 8630, loss = 2.39, batch loss = 1.90 (41.0 examples/sec; 0.195 sec/batch; 17h:33m:01s remains)
INFO - root - 2017-12-03 09:13:01.821481: step 8640, loss = 2.44, batch loss = 1.95 (42.0 examples/sec; 0.191 sec/batch; 17h:08m:38s remains)
INFO - root - 2017-12-03 09:13:03.691951: step 8650, loss = 2.58, batch loss = 2.08 (42.2 examples/sec; 0.190 sec/batch; 17h:03m:17s remains)
INFO - root - 2017-12-03 09:13:05.596949: step 8660, loss = 3.02, batch loss = 2.53 (43.4 examples/sec; 0.184 sec/batch; 16h:35m:25s remains)
INFO - root - 2017-12-03 09:13:07.477505: step 8670, loss = 3.02, batch loss = 2.52 (42.9 examples/sec; 0.187 sec/batch; 16h:47m:16s remains)
INFO - root - 2017-12-03 09:13:09.366712: step 8680, loss = 3.35, batch loss = 2.86 (42.6 examples/sec; 0.188 sec/batch; 16h:53m:40s remains)
INFO - root - 2017-12-03 09:13:11.262702: step 8690, loss = 3.23, batch loss = 2.74 (43.4 examples/sec; 0.184 sec/batch; 16h:34m:01s remains)
INFO - root - 2017-12-03 09:13:13.157492: step 8700, loss = 3.62, batch loss = 3.13 (40.9 examples/sec; 0.196 sec/batch; 17h:36m:25s remains)
INFO - root - 2017-12-03 09:13:15.155531: step 8710, loss = 4.03, batch loss = 3.54 (43.5 examples/sec; 0.184 sec/batch; 16h:32m:21s remains)
INFO - root - 2017-12-03 09:13:17.046779: step 8720, loss = 4.12, batch loss = 3.63 (41.0 examples/sec; 0.195 sec/batch; 17h:32m:32s remains)
INFO - root - 2017-12-03 09:13:18.947519: step 8730, loss = 4.39, batch loss = 3.90 (42.5 examples/sec; 0.188 sec/batch; 16h:56m:00s remains)
INFO - root - 2017-12-03 09:13:20.837657: step 8740, loss = 3.83, batch loss = 3.34 (43.6 examples/sec; 0.184 sec/batch; 16h:31m:13s remains)
INFO - root - 2017-12-03 09:13:22.737299: step 8750, loss = 4.21, batch loss = 3.72 (42.4 examples/sec; 0.189 sec/batch; 16h:59m:01s remains)
INFO - root - 2017-12-03 09:13:24.646404: step 8760, loss = 4.71, batch loss = 4.22 (42.1 examples/sec; 0.190 sec/batch; 17h:04m:59s remains)
INFO - root - 2017-12-03 09:13:26.537771: step 8770, loss = 4.77, batch loss = 4.28 (41.9 examples/sec; 0.191 sec/batch; 17h:09m:32s remains)
INFO - root - 2017-12-03 09:13:28.440608: step 8780, loss = 5.05, batch loss = 4.56 (42.2 examples/sec; 0.190 sec/batch; 17h:03m:42s remains)
INFO - root - 2017-12-03 09:13:30.326753: step 8790, loss = 4.87, batch loss = 4.38 (41.5 examples/sec; 0.193 sec/batch; 17h:20m:54s remains)
INFO - root - 2017-12-03 09:13:32.211805: step 8800, loss = 5.04, batch loss = 4.55 (41.5 examples/sec; 0.193 sec/batch; 17h:19m:35s remains)
INFO - root - 2017-12-03 09:13:34.152969: step 8810, loss = 5.53, batch loss = 5.03 (42.8 examples/sec; 0.187 sec/batch; 16h:49m:03s remains)
INFO - root - 2017-12-03 09:13:36.058010: step 8820, loss = 4.96, batch loss = 4.47 (40.5 examples/sec; 0.198 sec/batch; 17h:45m:38s remains)
INFO - root - 2017-12-03 09:13:37.938581: step 8830, loss = 5.40, batch loss = 4.91 (42.7 examples/sec; 0.187 sec/batch; 16h:51m:08s remains)
INFO - root - 2017-12-03 09:13:39.827508: step 8840, loss = 6.20, batch loss = 5.71 (42.5 examples/sec; 0.188 sec/batch; 16h:54m:56s remains)
INFO - root - 2017-12-03 09:13:41.749938: step 8850, loss = 6.99, batch loss = 6.49 (44.3 examples/sec; 0.181 sec/batch; 16h:14m:49s remains)
INFO - root - 2017-12-03 09:13:43.644095: step 8860, loss = 7.58, batch loss = 7.08 (41.7 examples/sec; 0.192 sec/batch; 17h:15m:10s remains)
INFO - root - 2017-12-03 09:13:45.531592: step 8870, loss = 7.04, batch loss = 6.54 (42.6 examples/sec; 0.188 sec/batch; 16h:53m:44s remains)
INFO - root - 2017-12-03 09:13:47.413572: step 8880, loss = 8.68, batch loss = 8.18 (43.1 examples/sec; 0.185 sec/batch; 16h:40m:06s remains)
INFO - root - 2017-12-03 09:13:49.308175: step 8890, loss = 8.81, batch loss = 8.31 (42.8 examples/sec; 0.187 sec/batch; 16h:48m:30s remains)
INFO - root - 2017-12-03 09:13:51.207003: step 8900, loss = 9.09, batch loss = 8.59 (42.0 examples/sec; 0.191 sec/batch; 17h:07m:43s remains)
INFO - root - 2017-12-03 09:13:53.177006: step 8910, loss = 10.79, batch loss = 10.30 (42.6 examples/sec; 0.188 sec/batch; 16h:53m:12s remains)
INFO - root - 2017-12-03 09:13:55.098366: step 8920, loss = 11.46, batch loss = 10.96 (42.6 examples/sec; 0.188 sec/batch; 16h:52m:54s remains)
INFO - root - 2017-12-03 09:13:56.974093: step 8930, loss = 11.99, batch loss = 11.49 (42.5 examples/sec; 0.188 sec/batch; 16h:54m:57s remains)
INFO - root - 2017-12-03 09:13:58.880378: step 8940, loss = 13.39, batch loss = 12.89 (42.9 examples/sec; 0.187 sec/batch; 16h:46m:13s remains)
INFO - root - 2017-12-03 09:14:00.784340: step 8950, loss = 13.14, batch loss = 12.64 (42.6 examples/sec; 0.188 sec/batch; 16h:51m:57s remains)
INFO - root - 2017-12-03 09:14:02.684416: step 8960, loss = 15.74, batch loss = 15.23 (45.0 examples/sec; 0.178 sec/batch; 15h:58m:08s remains)
INFO - root - 2017-12-03 09:14:04.564274: step 8970, loss = 16.29, batch loss = 15.78 (43.1 examples/sec; 0.186 sec/batch; 16h:40m:45s remains)
INFO - root - 2017-12-03 09:14:06.456864: step 8980, loss = 19.20, batch loss = 18.69 (43.0 examples/sec; 0.186 sec/batch; 16h:43m:28s remains)
INFO - root - 2017-12-03 09:14:08.323157: step 8990, loss = 18.75, batch loss = 18.23 (44.1 examples/sec; 0.181 sec/batch; 16h:17m:30s remains)
INFO - root - 2017-12-03 09:14:10.224778: step 9000, loss = 23.09, batch loss = 22.54 (43.8 examples/sec; 0.183 sec/batch; 16h:25m:09s remains)
INFO - root - 2017-12-03 09:14:12.165861: step 9010, loss = 27.45, batch loss = 26.89 (41.5 examples/sec; 0.193 sec/batch; 17h:18m:09s remains)
INFO - root - 2017-12-03 09:14:14.040099: step 9020, loss = 29.27, batch loss = 28.70 (43.7 examples/sec; 0.183 sec/batch; 16h:27m:17s remains)
INFO - root - 2017-12-03 09:14:15.932000: step 9030, loss = 31.55, batch loss = 30.99 (44.2 examples/sec; 0.181 sec/batch; 16h:16m:30s remains)
INFO - root - 2017-12-03 09:14:17.811301: step 9040, loss = 33.62, batch loss = 33.04 (43.9 examples/sec; 0.182 sec/batch; 16h:23m:31s remains)
INFO - root - 2017-12-03 09:14:19.698578: step 9050, loss = 34.27, batch loss = 33.57 (42.9 examples/sec; 0.186 sec/batch; 16h:44m:23s remains)
INFO - root - 2017-12-03 09:14:21.574696: step 9060, loss = 39.23, batch loss = 38.49 (42.6 examples/sec; 0.188 sec/batch; 16h:51m:10s remains)
INFO - root - 2017-12-03 09:14:23.455871: step 9070, loss = 45.47, batch loss = 44.73 (43.2 examples/sec; 0.185 sec/batch; 16h:38m:06s remains)
INFO - root - 2017-12-03 09:14:25.337299: step 9080, loss = 51.49, batch loss = 50.65 (41.9 examples/sec; 0.191 sec/batch; 17h:07m:57s remains)
INFO - root - 2017-12-03 09:14:27.217953: step 9090, loss = 54.59, batch loss = 53.51 (42.6 examples/sec; 0.188 sec/batch; 16h:52m:25s remains)
INFO - root - 2017-12-03 09:14:29.100333: step 9100, loss = 3.41, batch loss = 2.06 (42.4 examples/sec; 0.189 sec/batch; 16h:56m:33s remains)
INFO - root - 2017-12-03 09:14:31.039267: step 9110, loss = 3.40, batch loss = 2.04 (45.4 examples/sec; 0.176 sec/batch; 15h:50m:25s remains)
INFO - root - 2017-12-03 09:14:32.922461: step 9120, loss = 4.00, batch loss = 2.65 (42.5 examples/sec; 0.188 sec/batch; 16h:53m:50s remains)
INFO - root - 2017-12-03 09:14:34.806325: step 9130, loss = 3.29, batch loss = 1.92 (42.4 examples/sec; 0.189 sec/batch; 16h:56m:44s remains)
INFO - root - 2017-12-03 09:14:36.695636: step 9140, loss = 3.25, batch loss = 1.88 (43.0 examples/sec; 0.186 sec/batch; 16h:43m:21s remains)
INFO - root - 2017-12-03 09:14:38.565891: step 9150, loss = 3.67, batch loss = 2.30 (43.0 examples/sec; 0.186 sec/batch; 16h:43m:35s remains)
INFO - root - 2017-12-03 09:14:40.457462: step 9160, loss = 3.17, batch loss = 1.80 (42.5 examples/sec; 0.188 sec/batch; 16h:54m:11s remains)
INFO - root - 2017-12-03 09:14:42.340386: step 9170, loss = 3.31, batch loss = 1.94 (42.5 examples/sec; 0.188 sec/batch; 16h:54m:24s remains)
INFO - root - 2017-12-03 09:14:44.209808: step 9180, loss = 3.71, batch loss = 2.34 (42.5 examples/sec; 0.188 sec/batch; 16h:55m:19s remains)
INFO - root - 2017-12-03 09:14:46.094687: step 9190, loss = 3.81, batch loss = 2.45 (43.1 examples/sec; 0.186 sec/batch; 16h:41m:03s remains)
INFO - root - 2017-12-03 09:14:47.981253: step 9200, loss = 3.54, batch loss = 2.17 (42.5 examples/sec; 0.188 sec/batch; 16h:55m:19s remains)
INFO - root - 2017-12-03 09:14:49.926182: step 9210, loss = 4.15, batch loss = 2.78 (43.1 examples/sec; 0.186 sec/batch; 16h:39m:45s remains)
INFO - root - 2017-12-03 09:14:51.831769: step 9220, loss = 4.28, batch loss = 2.91 (41.9 examples/sec; 0.191 sec/batch; 17h:09m:28s remains)
INFO - root - 2017-12-03 09:14:53.733879: step 9230, loss = 4.33, batch loss = 2.96 (41.9 examples/sec; 0.191 sec/batch; 17h:09m:01s remains)
INFO - root - 2017-12-03 09:14:55.619380: step 9240, loss = 4.41, batch loss = 3.04 (41.5 examples/sec; 0.193 sec/batch; 17h:17m:59s remains)
INFO - root - 2017-12-03 09:14:57.498801: step 9250, loss = 5.03, batch loss = 3.66 (42.0 examples/sec; 0.190 sec/batch; 17h:06m:16s remains)
INFO - root - 2017-12-03 09:14:59.382364: step 9260, loss = 4.77, batch loss = 3.40 (40.6 examples/sec; 0.197 sec/batch; 17h:42m:16s remains)
INFO - root - 2017-12-03 09:15:01.261568: step 9270, loss = 5.15, batch loss = 3.78 (43.7 examples/sec; 0.183 sec/batch; 16h:26m:23s remains)
INFO - root - 2017-12-03 09:15:03.120646: step 9280, loss = 5.47, batch loss = 4.10 (42.8 examples/sec; 0.187 sec/batch; 16h:45m:50s remains)
INFO - root - 2017-12-03 09:15:04.967306: step 9290, loss = 5.80, batch loss = 4.43 (44.2 examples/sec; 0.181 sec/batch; 16h:14m:37s remains)
INFO - root - 2017-12-03 09:15:06.841632: step 9300, loss = 5.85, batch loss = 4.48 (42.7 examples/sec; 0.187 sec/batch; 16h:49m:06s remains)
INFO - root - 2017-12-03 09:15:08.794068: step 9310, loss = 6.44, batch loss = 5.07 (42.9 examples/sec; 0.186 sec/batch; 16h:43m:35s remains)
INFO - root - 2017-12-03 09:15:10.675860: step 9320, loss = 6.60, batch loss = 5.23 (43.1 examples/sec; 0.185 sec/batch; 16h:39m:03s remains)
INFO - root - 2017-12-03 09:15:12.546295: step 9330, loss = 7.96, batch loss = 6.59 (43.3 examples/sec; 0.185 sec/batch; 16h:36m:13s remains)
INFO - root - 2017-12-03 09:15:14.413815: step 9340, loss = 8.23, batch loss = 6.86 (41.5 examples/sec; 0.193 sec/batch; 17h:17m:24s remains)
INFO - root - 2017-12-03 09:15:16.301248: step 9350, loss = 8.57, batch loss = 7.20 (43.0 examples/sec; 0.186 sec/batch; 16h:41m:54s remains)
INFO - root - 2017-12-03 09:15:18.160206: step 9360, loss = 8.73, batch loss = 7.36 (43.4 examples/sec; 0.184 sec/batch; 16h:32m:15s remains)
INFO - root - 2017-12-03 09:15:20.061679: step 9370, loss = 10.31, batch loss = 8.94 (41.8 examples/sec; 0.192 sec/batch; 17h:11m:19s remains)
INFO - root - 2017-12-03 09:15:21.944335: step 9380, loss = 9.99, batch loss = 8.58 (42.0 examples/sec; 0.190 sec/batch; 17h:05m:24s remains)
INFO - root - 2017-12-03 09:15:23.832362: step 9390, loss = 10.76, batch loss = 9.35 (42.9 examples/sec; 0.187 sec/batch; 16h:44m:47s remains)
INFO - root - 2017-12-03 09:15:25.688983: step 9400, loss = 11.38, batch loss = 9.98 (43.5 examples/sec; 0.184 sec/batch; 16h:29m:45s remains)
INFO - root - 2017-12-03 09:15:27.616855: step 9410, loss = 12.16, batch loss = 10.75 (42.8 examples/sec; 0.187 sec/batch; 16h:47m:04s remains)
INFO - root - 2017-12-03 09:15:29.492417: step 9420, loss = 12.46, batch loss = 11.05 (42.5 examples/sec; 0.188 sec/batch; 16h:54m:18s remains)
INFO - root - 2017-12-03 09:15:31.366083: step 9430, loss = 13.38, batch loss = 11.97 (41.4 examples/sec; 0.193 sec/batch; 17h:19m:18s remains)
INFO - root - 2017-12-03 09:15:33.247025: step 9440, loss = 13.19, batch loss = 11.78 (42.4 examples/sec; 0.189 sec/batch; 16h:55m:08s remains)
INFO - root - 2017-12-03 09:15:35.108441: step 9450, loss = 14.04, batch loss = 12.62 (43.6 examples/sec; 0.183 sec/batch; 16h:27m:54s remains)
INFO - root - 2017-12-03 09:15:36.991595: step 9460, loss = 12.32, batch loss = 10.91 (42.4 examples/sec; 0.189 sec/batch; 16h:54m:55s remains)
INFO - root - 2017-12-03 09:15:38.846359: step 9470, loss = 13.34, batch loss = 11.92 (43.9 examples/sec; 0.182 sec/batch; 16h:21m:35s remains)
INFO - root - 2017-12-03 09:15:40.746040: step 9480, loss = 14.53, batch loss = 13.12 (42.0 examples/sec; 0.191 sec/batch; 17h:06m:17s remains)
INFO - root - 2017-12-03 09:15:42.619757: step 9490, loss = 16.70, batch loss = 15.23 (43.6 examples/sec; 0.183 sec/batch; 16h:27m:41s remains)
INFO - root - 2017-12-03 09:15:44.491366: step 9500, loss = 19.63, batch loss = 18.15 (43.7 examples/sec; 0.183 sec/batch; 16h:26m:07s remains)
INFO - root - 2017-12-03 09:15:46.419790: step 9510, loss = 20.22, batch loss = 18.74 (43.4 examples/sec; 0.184 sec/batch; 16h:33m:11s remains)
INFO - root - 2017-12-03 09:15:48.294328: step 9520, loss = 20.67, batch loss = 19.18 (42.3 examples/sec; 0.189 sec/batch; 16h:57m:10s remains)
INFO - root - 2017-12-03 09:15:50.164337: step 9530, loss = 19.68, batch loss = 18.19 (43.6 examples/sec; 0.184 sec/batch; 16h:28m:40s remains)
INFO - root - 2017-12-03 09:15:52.043279: step 9540, loss = 21.38, batch loss = 19.28 (42.5 examples/sec; 0.188 sec/batch; 16h:53m:21s remains)
INFO - root - 2017-12-03 09:15:53.922600: step 9550, loss = 23.35, batch loss = 21.24 (42.4 examples/sec; 0.189 sec/batch; 16h:54m:38s remains)
INFO - root - 2017-12-03 09:15:55.802885: step 9560, loss = 25.47, batch loss = 23.35 (42.9 examples/sec; 0.186 sec/batch; 16h:42m:45s remains)
INFO - root - 2017-12-03 09:15:57.663079: step 9570, loss = 29.77, batch loss = 27.62 (42.0 examples/sec; 0.191 sec/batch; 17h:05m:48s remains)
INFO - root - 2017-12-03 09:15:59.534237: step 9580, loss = 30.89, batch loss = 28.74 (42.7 examples/sec; 0.187 sec/batch; 16h:48m:46s remains)
INFO - root - 2017-12-03 09:16:01.389535: step 9590, loss = 33.85, batch loss = 31.70 (42.9 examples/sec; 0.186 sec/batch; 16h:42m:59s remains)
INFO - root - 2017-12-03 09:16:03.238085: step 9600, loss = 34.28, batch loss = 32.13 (43.0 examples/sec; 0.186 sec/batch; 16h:41m:03s remains)
INFO - root - 2017-12-03 09:16:05.251971: step 9610, loss = 42.21, batch loss = 40.06 (43.2 examples/sec; 0.185 sec/batch; 16h:36m:25s remains)
INFO - root - 2017-12-03 09:16:07.131237: step 9620, loss = 40.36, batch loss = 38.20 (42.5 examples/sec; 0.188 sec/batch; 16h:52m:26s remains)
INFO - root - 2017-12-03 09:16:08.973308: step 9630, loss = 49.88, batch loss = 47.72 (42.1 examples/sec; 0.190 sec/batch; 17h:02m:23s remains)
INFO - root - 2017-12-03 09:16:10.866763: step 9640, loss = 56.36, batch loss = 54.13 (42.1 examples/sec; 0.190 sec/batch; 17h:03m:05s remains)
INFO - root - 2017-12-03 09:16:12.719963: step 9650, loss = 65.49, batch loss = 63.26 (44.2 examples/sec; 0.181 sec/batch; 16h:14m:23s remains)
INFO - root - 2017-12-03 09:16:14.574701: step 9660, loss = 84.54, batch loss = 82.22 (42.9 examples/sec; 0.186 sec/batch; 16h:42m:15s remains)
INFO - root - 2017-12-03 09:16:16.425747: step 9670, loss = 73.57, batch loss = 70.42 (43.3 examples/sec; 0.185 sec/batch; 16h:33m:31s remains)
INFO - root - 2017-12-03 09:16:18.265582: step 9680, loss = 85.21, batch loss = 82.04 (42.9 examples/sec; 0.187 sec/batch; 16h:44m:25s remains)
INFO - root - 2017-12-03 09:16:20.133585: step 9690, loss = 543.67, batch loss = 517.33 (43.7 examples/sec; 0.183 sec/batch; 16h:25m:53s remains)
INFO - root - 2017-12-03 09:16:21.996540: step 9700, loss = 651.06, batch loss = 617.92 (43.0 examples/sec; 0.186 sec/batch; 16h:40m:06s remains)
INFO - root - 2017-12-03 09:16:23.941821: step 9710, loss = 869.61, batch loss = 812.87 (43.1 examples/sec; 0.186 sec/batch; 16h:38m:51s remains)
INFO - root - 2017-12-03 09:16:25.831807: step 9720, loss = 4028.30, batch loss = 74.80 (40.3 examples/sec; 0.199 sec/batch; 17h:48m:08s remains)
INFO - root - 2017-12-03 09:16:27.663525: step 9730, loss = 4054.13, batch loss = 97.67 (43.5 examples/sec; 0.184 sec/batch; 16h:29m:14s remains)
INFO - root - 2017-12-03 09:16:29.523979: step 9740, loss = 3990.76, batch loss = 30.85 (42.3 examples/sec; 0.189 sec/batch; 16h:57m:57s remains)
INFO - root - 2017-12-03 09:16:31.353097: step 9750, loss = 3974.28, batch loss = 13.82 (43.6 examples/sec; 0.184 sec/batch; 16h:27m:25s remains)
INFO - root - 2017-12-03 09:16:33.194207: step 9760, loss = 3975.64, batch loss = 15.18 (43.3 examples/sec; 0.185 sec/batch; 16h:33m:16s remains)
INFO - root - 2017-12-03 09:16:35.040066: step 9770, loss = 3973.40, batch loss = 12.92 (43.6 examples/sec; 0.183 sec/batch; 16h:26m:15s remains)
INFO - root - 2017-12-03 09:16:36.872673: step 9780, loss = 4281.86, batch loss = 1.94 (43.1 examples/sec; 0.186 sec/batch; 16h:38m:52s remains)
INFO - root - 2017-12-03 09:16:38.703764: step 9790, loss = 4292.29, batch loss = 12.36 (43.5 examples/sec; 0.184 sec/batch; 16h:29m:35s remains)
INFO - root - 2017-12-03 09:16:40.558825: step 9800, loss = 4283.78, batch loss = 3.86 (43.0 examples/sec; 0.186 sec/batch; 16h:41m:15s remains)
INFO - root - 2017-12-03 09:16:42.450615: step 9810, loss = 4287.17, batch loss = 7.24 (44.1 examples/sec; 0.182 sec/batch; 16h:16m:32s remains)
INFO - root - 2017-12-03 09:16:44.305025: step 9820, loss = 4308.89, batch loss = 22.24 (42.9 examples/sec; 0.186 sec/batch; 16h:42m:16s remains)
INFO - root - 2017-12-03 09:16:46.132699: step 9830, loss = 4332.56, batch loss = 45.91 (44.5 examples/sec; 0.180 sec/batch; 16h:05m:59s remains)
INFO - root - 2017-12-03 09:16:47.966982: step 9840, loss = 4289.92, batch loss = 3.27 (43.3 examples/sec; 0.185 sec/batch; 16h:34m:19s remains)
INFO - root - 2017-12-03 09:16:49.805913: step 9850, loss = 4326.86, batch loss = 40.21 (42.9 examples/sec; 0.186 sec/batch; 16h:42m:40s remains)
INFO - root - 2017-12-03 09:16:51.657164: step 9860, loss = 4305.42, batch loss = 18.78 (43.1 examples/sec; 0.186 sec/batch; 16h:38m:09s remains)
INFO - root - 2017-12-03 09:16:53.484515: step 9870, loss = 4296.20, batch loss = 9.55 (43.9 examples/sec; 0.182 sec/batch; 16h:20m:37s remains)
INFO - root - 2017-12-03 09:16:55.338090: step 9880, loss = 4303.34, batch loss = 16.69 (44.3 examples/sec; 0.180 sec/batch; 16h:10m:23s remains)
INFO - root - 2017-12-03 09:16:57.169782: step 9890, loss = 4305.39, batch loss = 18.75 (43.6 examples/sec; 0.183 sec/batch; 16h:26m:23s remains)
INFO - root - 2017-12-03 09:16:59.002907: step 9900, loss = 4313.54, batch loss = 26.90 (43.4 examples/sec; 0.184 sec/batch; 16h:30m:38s remains)
INFO - root - 2017-12-03 09:17:00.896433: step 9910, loss = 4293.58, batch loss = 1.82 (43.4 examples/sec; 0.184 sec/batch; 16h:30m:28s remains)
INFO - root - 2017-12-03 09:17:02.748951: step 9920, loss = 17789480960.00, batch loss = 3513222.00 (44.5 examples/sec; 0.180 sec/batch; 16h:05m:56s remains)
INFO - root - 2017-12-03 09:17:04.580157: step 9930, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.183 sec/batch; 16h:26m:19s remains)
INFO - root - 2017-12-03 09:17:06.410930: step 9940, loss = nan, batch loss = 2.22 (44.7 examples/sec; 0.179 sec/batch; 16h:03m:11s remains)
INFO - root - 2017-12-03 09:17:08.245617: step 9950, loss = nan, batch loss = 2.22 (42.9 examples/sec; 0.186 sec/batch; 16h:41m:39s remains)
INFO - root - 2017-12-03 09:17:10.061663: step 9960, loss = nan, batch loss = 2.22 (42.5 examples/sec; 0.188 sec/batch; 16h:50m:42s remains)
INFO - root - 2017-12-03 09:17:11.892245: step 9970, loss = nan, batch loss = 2.22 (43.4 examples/sec; 0.184 sec/batch; 16h:30m:40s remains)
INFO - root - 2017-12-03 09:17:13.748896: step 9980, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.183 sec/batch; 16h:25m:12s remains)
INFO - root - 2017-12-03 09:17:15.600885: step 9990, loss = nan, batch loss = 2.22 (43.5 examples/sec; 0.184 sec/batch; 16h:27m:36s remains)
INFO - root - 2017-12-03 09:17:17.434817: step 10000, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.183 sec/batch; 16h:25m:13s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-03 09:17:19.612530: step 10010, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.184 sec/batch; 16h:26m:20s remains)
INFO - root - 2017-12-03 09:17:21.450261: step 10020, loss = nan, batch loss = 2.22 (44.1 examples/sec; 0.181 sec/batch; 16h:14m:29s remains)
INFO - root - 2017-12-03 09:17:23.277420: step 10030, loss = nan, batch loss = 2.22 (44.2 examples/sec; 0.181 sec/batch; 16h:13m:35s remains)
INFO - root - 2017-12-03 09:17:25.111154: step 10040, loss = nan, batch loss = 2.22 (42.8 examples/sec; 0.187 sec/batch; 16h:45m:13s remains)
INFO - root - 2017-12-03 09:17:26.935384: step 10050, loss = nan, batch loss = 2.22 (44.4 examples/sec; 0.180 sec/batch; 16h:09m:18s remains)
INFO - root - 2017-12-03 09:17:28.752680: step 10060, loss = nan, batch loss = 2.22 (44.4 examples/sec; 0.180 sec/batch; 16h:08m:18s remains)
INFO - root - 2017-12-03 09:17:30.572491: step 10070, loss = nan, batch loss = 2.22 (44.9 examples/sec; 0.178 sec/batch; 15h:57m:16s remains)
INFO - root - 2017-12-03 09:17:32.390439: step 10080, loss = nan, batch loss = 2.22 (44.3 examples/sec; 0.181 sec/batch; 16h:11m:03s remains)
INFO - root - 2017-12-03 09:17:34.215549: step 10090, loss = nan, batch loss = 2.22 (44.4 examples/sec; 0.180 sec/batch; 16h:09m:12s remains)
INFO - root - 2017-12-03 09:17:36.051849: step 10100, loss = nan, batch loss = 2.22 (44.6 examples/sec; 0.179 sec/batch; 16h:04m:18s remains)
INFO - root - 2017-12-03 09:17:37.939488: step 10110, loss = nan, batch loss = 2.22 (45.0 examples/sec; 0.178 sec/batch; 15h:55m:19s remains)
INFO - root - 2017-12-03 09:17:39.770478: step 10120, loss = nan, batch loss = 2.22 (45.1 examples/sec; 0.177 sec/batch; 15h:53m:34s remains)
INFO - root - 2017-12-03 09:17:41.583262: step 10130, loss = nan, batch loss = 2.22 (44.2 examples/sec; 0.181 sec/batch; 16h:12m:36s remains)
INFO - root - 2017-12-03 09:17:43.415999: step 10140, loss = nan, batch loss = 2.22 (43.7 examples/sec; 0.183 sec/batch; 16h:23m:48s remains)
INFO - root - 2017-12-03 09:17:45.253666: step 10150, loss = nan, batch loss = 2.22 (43.4 examples/sec; 0.184 sec/batch; 16h:29m:58s remains)
INFO - root - 2017-12-03 09:17:47.086359: step 10160, loss = nan, batch loss = 2.22 (44.2 examples/sec; 0.181 sec/batch; 16h:12m:12s remains)
INFO - root - 2017-12-03 09:17:48.913111: step 10170, loss = nan, batch loss = 2.22 (42.7 examples/sec; 0.187 sec/batch; 16h:46m:05s remains)
INFO - root - 2017-12-03 09:17:50.746889: step 10180, loss = nan, batch loss = 2.22 (43.7 examples/sec; 0.183 sec/batch; 16h:24m:20s remains)
INFO - root - 2017-12-03 09:17:52.580117: step 10190, loss = nan, batch loss = 2.22 (43.9 examples/sec; 0.182 sec/batch; 16h:19m:51s remains)
INFO - root - 2017-12-03 09:17:54.418379: step 10200, loss = nan, batch loss = 2.22 (42.9 examples/sec; 0.186 sec/batch; 16h:41m:07s remains)
INFO - root - 2017-12-03 09:17:56.294679: step 10210, loss = nan, batch loss = 2.22 (44.0 examples/sec; 0.182 sec/batch; 16h:16m:46s remains)
INFO - root - 2017-12-03 09:17:58.112548: step 10220, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.184 sec/batch; 16h:25m:57s remains)
INFO - root - 2017-12-03 09:17:59.941257: step 10230, loss = nan, batch loss = 2.22 (44.3 examples/sec; 0.181 sec/batch; 16h:10m:24s remains)
INFO - root - 2017-12-03 09:18:01.769411: step 10240, loss = nan, batch loss = 2.22 (43.1 examples/sec; 0.186 sec/batch; 16h:37m:44s remains)
INFO - root - 2017-12-03 09:18:03.600562: step 10250, loss = nan, batch loss = 2.22 (43.5 examples/sec; 0.184 sec/batch; 16h:28m:35s remains)
INFO - root - 2017-12-03 09:18:05.434202: step 10260, loss = nan, batch loss = 2.22 (45.0 examples/sec; 0.178 sec/batch; 15h:55m:07s remains)
INFO - root - 2017-12-03 09:18:07.248441: step 10270, loss = nan, batch loss = 2.22 (43.9 examples/sec; 0.182 sec/batch; 16h:19m:37s remains)
INFO - root - 2017-12-03 09:18:09.077845: step 10280, loss = nan, batch loss = 2.22 (43.3 examples/sec; 0.185 sec/batch; 16h:31m:30s remains)
INFO - root - 2017-12-03 09:18:10.865003: step 10290, loss = nan, batch loss = 2.22 (45.9 examples/sec; 0.174 sec/batch; 15h:36m:31s remains)
INFO - root - 2017-12-03 09:18:12.674044: step 10300, loss = nan, batch loss = 2.22 (45.2 examples/sec; 0.177 sec/batch; 15h:50m:54s remains)
INFO - root - 2017-12-03 09:18:14.550092: step 10310, loss = nan, batch loss = 2.22 (44.2 examples/sec; 0.181 sec/batch; 16h:11m:45s remains)
INFO - root - 2017-12-03 09:18:16.378744: step 10320, loss = nan, batch loss = 2.22 (44.3 examples/sec; 0.181 sec/batch; 16h:10m:25s remains)
INFO - root - 2017-12-03 09:18:18.203477: step 10330, loss = nan, batch loss = 2.22 (43.1 examples/sec; 0.185 sec/batch; 16h:35m:39s remains)
INFO - root - 2017-12-03 09:18:20.035805: step 10340, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.183 sec/batch; 16h:25m:11s remains)
INFO - root - 2017-12-03 09:18:21.873723: step 10350, loss = nan, batch loss = 2.22 (43.1 examples/sec; 0.186 sec/batch; 16h:36m:57s remains)
INFO - root - 2017-12-03 09:18:23.690909: step 10360, loss = nan, batch loss = 2.22 (44.4 examples/sec; 0.180 sec/batch; 16h:08m:15s remains)
INFO - root - 2017-12-03 09:18:25.538260: step 10370, loss = nan, batch loss = 2.22 (44.1 examples/sec; 0.181 sec/batch; 16h:12m:53s remains)
INFO - root - 2017-12-03 09:18:27.371440: step 10380, loss = nan, batch loss = 2.22 (43.9 examples/sec; 0.182 sec/batch; 16h:19m:22s remains)
INFO - root - 2017-12-03 09:18:29.204088: step 10390, loss = nan, batch loss = 2.22 (44.2 examples/sec; 0.181 sec/batch; 16h:11m:43s remains)
INFO - root - 2017-12-03 09:18:31.035931: step 10400, loss = nan, batch loss = 2.22 (43.9 examples/sec; 0.182 sec/batch; 16h:18m:09s remains)
INFO - root - 2017-12-03 09:18:32.942630: step 10410, loss = nan, batch loss = 2.22 (44.1 examples/sec; 0.181 sec/batch; 16h:13m:37s remains)
INFO - root - 2017-12-03 09:18:34.761712: step 10420, loss = nan, batch loss = 2.22 (43.4 examples/sec; 0.184 sec/batch; 16h:30m:05s remains)
INFO - root - 2017-12-03 09:18:36.595643: step 10430, loss = nan, batch loss = 2.22 (43.7 examples/sec; 0.183 sec/batch; 16h:21m:51s remains)
INFO - root - 2017-12-03 09:18:38.445152: step 10440, loss = nan, batch loss = 2.22 (44.0 examples/sec; 0.182 sec/batch; 16h:16m:30s remains)
INFO - root - 2017-12-03 09:18:40.268238: step 10450, loss = nan, batch loss = 2.22 (44.0 examples/sec; 0.182 sec/batch; 16h:16m:51s remains)
INFO - root - 2017-12-03 09:18:42.103866: step 10460, loss = nan, batch loss = 2.22 (43.5 examples/sec; 0.184 sec/batch; 16h:27m:40s remains)
INFO - root - 2017-12-03 09:18:43.902685: step 10470, loss = nan, batch loss = 2.22 (44.2 examples/sec; 0.181 sec/batch; 16h:12m:28s remains)
INFO - root - 2017-12-03 09:18:45.741766: step 10480, loss = nan, batch loss = 2.22 (44.0 examples/sec; 0.182 sec/batch; 16h:16m:10s remains)
INFO - root - 2017-12-03 09:18:47.576588: step 10490, loss = nan, batch loss = 2.22 (44.9 examples/sec; 0.178 sec/batch; 15h:56m:12s remains)
INFO - root - 2017-12-03 09:18:49.410591: step 10500, loss = nan, batch loss = 2.22 (42.9 examples/sec; 0.186 sec/batch; 16h:39m:56s remains)
INFO - root - 2017-12-03 09:18:51.282258: step 10510, loss = nan, batch loss = 2.22 (44.6 examples/sec; 0.179 sec/batch; 16h:01m:57s remains)
INFO - root - 2017-12-03 09:18:53.098183: step 10520, loss = nan, batch loss = 2.22 (44.9 examples/sec; 0.178 sec/batch; 15h:56m:18s remains)
INFO - root - 2017-12-03 09:18:54.906268: step 10530, loss = nan, batch loss = 2.22 (43.5 examples/sec; 0.184 sec/batch; 16h:27m:40s remains)
INFO - root - 2017-12-03 09:18:56.744049: step 10540, loss = nan, batch loss = 2.22 (44.1 examples/sec; 0.181 sec/batch; 16h:13m:29s remains)
INFO - root - 2017-12-03 09:18:58.579211: step 10550, loss = nan, batch loss = 2.22 (43.2 examples/sec; 0.185 sec/batch; 16h:33m:50s remains)
INFO - root - 2017-12-03 09:19:00.390154: step 10560, loss = nan, batch loss = 2.22 (43.4 examples/sec; 0.184 sec/batch; 16h:28m:28s remains)
INFO - root - 2017-12-03 09:19:02.224465: step 10570, loss = nan, batch loss = 2.22 (42.8 examples/sec; 0.187 sec/batch; 16h:42m:31s remains)
INFO - root - 2017-12-03 09:19:04.050582: step 10580, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.184 sec/batch; 16h:25m:17s remains)
INFO - root - 2017-12-03 09:19:05.892885: step 10590, loss = nan, batch loss = 2.22 (42.3 examples/sec; 0.189 sec/batch; 16h:54m:22s remains)
INFO - root - 2017-12-03 09:19:07.710706: step 10600, loss = nan, batch loss = 2.22 (44.5 examples/sec; 0.180 sec/batch; 16h:03m:44s remains)
INFO - root - 2017-12-03 09:19:09.610885: step 10610, loss = nan, batch loss = 2.22 (43.4 examples/sec; 0.184 sec/batch; 16h:27m:49s remains)
INFO - root - 2017-12-03 09:19:11.446145: step 10620, loss = nan, batch loss = 2.22 (43.2 examples/sec; 0.185 sec/batch; 16h:33m:00s remains)
INFO - root - 2017-12-03 09:19:13.274181: step 10630, loss = nan, batch loss = 2.22 (44.7 examples/sec; 0.179 sec/batch; 16h:00m:49s remains)
INFO - root - 2017-12-03 09:19:15.108543: step 10640, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.184 sec/batch; 16h:25m:22s remains)
INFO - root - 2017-12-03 09:19:16.919811: step 10650, loss = nan, batch loss = 2.22 (43.9 examples/sec; 0.182 sec/batch; 16h:17m:33s remains)
INFO - root - 2017-12-03 09:19:18.738786: step 10660, loss = nan, batch loss = 2.22 (43.0 examples/sec; 0.186 sec/batch; 16h:37m:17s remains)
INFO - root - 2017-12-03 09:19:20.573394: step 10670, loss = nan, batch loss = 2.22 (44.4 examples/sec; 0.180 sec/batch; 16h:06m:11s remains)
INFO - root - 2017-12-03 09:19:22.404457: step 10680, loss = nan, batch loss = 2.22 (44.3 examples/sec; 0.181 sec/batch; 16h:09m:14s remains)
INFO - root - 2017-12-03 09:19:24.218254: step 10690, loss = nan, batch loss = 2.22 (45.0 examples/sec; 0.178 sec/batch; 15h:53m:07s remains)
INFO - root - 2017-12-03 09:19:26.070049: step 10700, loss = nan, batch loss = 2.22 (43.3 examples/sec; 0.185 sec/batch; 16h:30m:07s remains)
INFO - root - 2017-12-03 09:19:28.029255: step 10710, loss = nan, batch loss = 2.22 (45.0 examples/sec; 0.178 sec/batch; 15h:54m:21s remains)
INFO - root - 2017-12-03 09:19:29.833844: step 10720, loss = nan, batch loss = 2.22 (43.3 examples/sec; 0.185 sec/batch; 16h:31m:48s remains)
INFO - root - 2017-12-03 09:19:31.678297: step 10730, loss = nan, batch loss = 2.22 (41.5 examples/sec; 0.193 sec/batch; 17h:13m:37s remains)
INFO - root - 2017-12-03 09:19:33.501657: step 10740, loss = nan, batch loss = 2.22 (43.2 examples/sec; 0.185 sec/batch; 16h:33m:28s remains)
INFO - root - 2017-12-03 09:19:35.335921: step 10750, loss = nan, batch loss = 2.22 (44.7 examples/sec; 0.179 sec/batch; 15h:59m:20s remains)
INFO - root - 2017-12-03 09:19:37.169482: step 10760, loss = nan, batch loss = 2.22 (43.9 examples/sec; 0.182 sec/batch; 16h:17m:31s remains)
INFO - root - 2017-12-03 09:19:38.975478: step 10770, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.183 sec/batch; 16h:23m:00s remains)
INFO - root - 2017-12-03 09:19:40.811769: step 10780, loss = nan, batch loss = 2.22 (45.1 examples/sec; 0.178 sec/batch; 15h:51m:59s remains)
INFO - root - 2017-12-03 09:19:42.643407: step 10790, loss = nan, batch loss = 2.22 (43.9 examples/sec; 0.182 sec/batch; 16h:16m:17s remains)
INFO - root - 2017-12-03 09:19:44.479955: step 10800, loss = nan, batch loss = 2.22 (43.3 examples/sec; 0.185 sec/batch; 16h:30m:37s remains)
INFO - root - 2017-12-03 09:19:46.352475: step 10810, loss = nan, batch loss = 2.22 (43.5 examples/sec; 0.184 sec/batch; 16h:25m:41s remains)
INFO - root - 2017-12-03 09:19:48.169018: step 10820, loss = nan, batch loss = 2.22 (43.1 examples/sec; 0.186 sec/batch; 16h:34m:31s remains)
INFO - root - 2017-12-03 09:19:49.989381: step 10830, loss = nan, batch loss = 2.22 (44.4 examples/sec; 0.180 sec/batch; 16h:05m:05s remains)
INFO - root - 2017-12-03 09:19:51.809358: step 10840, loss = nan, batch loss = 2.22 (44.6 examples/sec; 0.179 sec/batch; 16h:01m:43s remains)
INFO - root - 2017-12-03 09:19:53.643937: step 10850, loss = nan, batch loss = 2.22 (43.0 examples/sec; 0.186 sec/batch; 16h:37m:04s remains)
INFO - root - 2017-12-03 09:19:55.479371: step 10860, loss = nan, batch loss = 2.22 (43.4 examples/sec; 0.184 sec/batch; 16h:27m:24s remains)
INFO - root - 2017-12-03 09:19:57.306440: step 10870, loss = nan, batch loss = 2.22 (44.1 examples/sec; 0.181 sec/batch; 16h:12m:36s remains)
INFO - root - 2017-12-03 09:19:59.142379: step 10880, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.183 sec/batch; 16h:22m:54s remains)
INFO - root - 2017-12-03 09:20:00.979821: step 10890, loss = nan, batch loss = 2.22 (43.6 examples/sec; 0.183 sec/batch; 16h:23m:21s remains)
INFO - root - 2017-12-03 09:20:02.818256: step 10900, loss = nan, batch loss = 2.22 (42.2 examples/sec; 0.190 sec/batch; 16h:56m:53s remains)
INFO - root - 2017-12-03 09:20:04.722896: step 10910, loss = nan, batch loss = 2.22 (44.4 examples/sec; 0.180 sec/batch; 16h:06m:28s remains)
INFO - root - 2017-12-03 09:20:06.556038: step 10920, loss = nan, batch loss = 2.22 (43.3 examples/sec; 0.185 sec/batch; 16h:31m:13s remains)
INFO - root - 2017-12-03 09:20:08.386326: step 10930, loss = nan, batch loss = 2.22 (45.7 examples/sec; 0.175 sec/batch; 15h:38m:54s remains)
INFO - root - 2017-12-03 09:20:10.225419: step 10940, loss = nan, batch loss = 2.22 (43.2 examples/sec; 0.185 sec/batch; 16h:31m:38s remains)
INFO - root - 2017-12-03 09:20:12.061573: step 10950, loss = nan, batch loss = 2.22 (43.5 examples/sec; 0.184 sec/batch; 16h:26m:05s remains)
INFO - root - 2017-12-03 09:20:13.894701: step 10960, loss = nan, batch loss = 2.22 (43.5 examples/sec; 0.184 sec/batch; 16h:25m:32s remains)
INFO - root - 2017-12-03 09:20:15.701000: step 10970, loss = nan, batch loss = 2.22 (44.2 examples/sec; 0.181 sec/batch; 16h:10m:23s remains)
INFO - root - 2017-12-03 09:20:17.539741: step 10980, loss = nan, batch loss = 2.22 (42.3 examples/sec; 0.189 sec/batch; 16h:54m:11s remains)
INFO - root - 2017-12-03 09:20:19.347724: step 10990, loss = nan, batch loss = 2.22 (44.0 examples/sec; 0.182 sec/batch; 16h:14m:35s remains)
INFO - root - 2017-12-03 09:20:21.185333: step 11000, loss = nan, batch loss = 2.22 (43.0 examples/sec; 0.186 sec/batch; 16h:35m:52s remains)
INFO - root - 2017-12-03 09:20:23.118223: step 11010, loss = nan, batch loss = 2.22 (43.5 examples/sec; 0.184 sec/batch; 16h:25m:33s remains)
INFO - root - 2017-12-03 09:20:24.937566: step 11020, loss = nan, batch loss = 2.22 (44.2 examples/sec; 0.181 sec/batch; 16h:09m:39s remains)
INFO - root - 2017-12-03 09:20:26.759789: step 11030, loss = nan, batch loss = 2.22 (43.3 examples/sec; 0.185 sec/batch; 16h:30m:43s remains)
INFO - root - 2017-12-03 09:20:28.589981: step 11040, loss = nan, batch loss = 2.22 (44.0 examples/sec; 0.182 sec/batch; 16h:13m:19s remains)
INFO - root - 2017-12-03 09:20:30.412155: step 11050, loss = nan, batch loss = 2.22 (43.3 examples/sec; 0.185 sec/batch; 16h:29m:57s remains)
INFO - root - 2017-12-03 09:20:32.242185: step 11060, loss = nan, batch loss = 2.22 (44.0 examples/sec; 0.182 sec/batch; 16h:13m:07s remains)
INFO - root - 2017-12-03 09:20:34.059327: step 11070, loss = nan, batch loss = 2.22 (45.0 examples/sec; 0.178 sec/batch; 15h:53m:02s remains)
INFO - root - 2017-12-03 09:20:35.884134: step 11080, loss = nan, batch loss = 2.22 (43.8 examples/sec; 0.183 sec/batch; 16h:19m:06s remains)
INFO - root - 2017-12-03 09:20:37.719252: step 11090, loss = nan, batch loss = 2.22 (43.3 examples/sec; 0.185 sec/batch; 16h:29m:47s remains)
INFO - root - 2017-12-03 09:20:39.538326: step 11100, loss = nan, batch loss = 2.22 (42.7 examples/sec; 0.187 sec/batch; 16h:43m:19s remains)
INFO - root - 2017-12-03 09:20:41.436340: step 11110, loss = nan, batch loss = 2.22 (42.6 examples/sec; 0.188 sec/batch; 16h:44m:58s remains)
INFO - root - 2017-12-03 09:20:43.263813: step 11120, loss = nan, batch loss = 2.22 (42.7 examples/sec; 0.187 sec/batch; 16h:43m:35s remains)
INFO - root - 2017-12-03 09:20:45.114147: step 11130, loss = nan, batch loss = 2.22 (43.4 examples/sec; 0.184 sec/batch; 16h:26m:22s remains)
INFO - root - 2017-12-03 09:20:46.950793: step 11140, loss = nan, batch loss = 2.22 (42.8 examples/sec; 0.187 sec/batch; 16h:40m:46s remains)
INFO - root - 2017-12-03 09:20:48.784036: step 11150, loss = nan, batch loss = 2.22 (43.7 examples/sec; 0.183 sec/batch; 16h:20m:28s remains)
INFO - root - 2017-12-03 09:20:50.595928: step 11160, loss = nan, batch loss = 2.22 (43.8 examples/sec; 0.182 sec/batch; 16h:17m:22s remains)
INFO - root - 2017-12-03 09:20:52.430155: step 11170, loss = nan, batch loss = 2.22 (44.1 examples/sec; 0.181 sec/batch; 16h:10m:58s remains)
INFO - root - 2017-12-03 09:20:54.242743: step 11180, loss = nan, batch loss = 2.22 (44.9 examples/sec; 0.178 sec/batch; 15h:55m:10s remains)
INFO - root - 2017-12-03 09:20:56.058682: step 11190, loss = nan, batch loss = 2.22 (44.5 examples/sec; 0.180 sec/batch; 16h:01m:57s remains)
INFO - root - 2017-12-03 09:20:57.864719: step 11200, loss = nan, batch loss = 2.22 (43.4 examples/sec; 0.184 sec/batch; 16h:26m:27s remains)
INFO - root - 2017-12-03 09:20:59.744442: step 11210, loss = nan, batch loss = 2.22 (43.4 examples/sec; 0.184 sec/batch; 16h:27m:39s remains)
INFO - root - 2017-12-03 09:21:01.573216: step 11220, loss = nan, batch loss = 2.22 (43.8 examples/sec; 0.183 sec/batch; 16h:18m:19s remains)
INFO - root - 2017-12-03 09:21:03.376411: step 11230, loss = nan, batch loss = 2.22 (44.3 examples/sec; 0.181 sec/batch; 16h:06m:29s remains)
INFO - root - 2017-12-03 09:21:05.183987: step 11240, loss = nan, batch loss = 2.22 (42.5 examples/sec; 0.188 sec/batch; 16h:47m:04s remains)
INFO - root - 2017-12-03 09:21:07.010589: step 11250, loss = nan, batch loss = 2.22 (43.8 examples/sec; 0.183 sec/batch; 16h:18m:44s remains)
INFO - root - 2017-12-03 09:21:08.844966: step 11260, loss = nan, batch loss = 2.22 (43.7 examples/sec; 0.183 sec/batch; 16h:19m:41s remains)
INFO - root - 2017-12-03 09:21:10.661134: step 11270, loss = nan, batch loss = 2.22 (44.0 examples/sec; 0.182 sec/batch; 16h:12m:40s remains)
INFO - root - 2017-12-03 09:21:12.498084: step 11280, loss = nan, batch loss = 2.22 (42.4 examples/sec; 0.188 sec/batch; 16h:49m:07s remains)
INFO - root - 2017-12-03 09:21:14.287883: step 11290, loss = nan, batch loss = 2.22 (45.0 examples/sec; 0.178 sec/batch; 15h:50m:56s remains)
INFO - root - 2017-12-03 09:21:16.103486: step 11300, loss = nan, batch loss = 2.22 (43.7 examples/sec; 0.183 sec/batch; 16h:20m:29s remains)
INFO - root - 2017-12-03 09:21:17.992906: step 11310, loss = nan, batch loss = 2.22 (43.9 examples/sec; 0.182 sec/batch; 16h:14m:31s remains)
INFO - root - 2017-12-03 09:21:19.809184: step 11320, loss = nan, batch loss = 2.22 (43.7 examples/sec; 0.183 sec/batch; 16h:19m:39s remains)
INFO - root - 2017-12-03 09:21:21.643021: step 11330, loss = nan, batch loss = 2.22 (43.2 examples/sec; 0.185 sec/batch; 16h:30m:48s remains)
INFO - root - 2017-12-03 09:21:23.473576: step 11340, loss = nan, batch loss = 2.22 (44.2 examples/sec; 0.181 sec/batch; 16h:09m:32s remains)
INFO - root - 2017-12-03 09:21:25.307990: step 11350, loss = nan, batch loss = 2.22 (43.3 examples/sec; 0.185 sec/batch; 16h:27m:51s remains)
INFO - root - 2017-12-03 09:21:27.111445: step 11360, loss = nan, batch loss = 2.22 (44.4 examples/sec; 0.180 sec/batch; 16h:04m:11s remains)
