INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "215"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
111111111 Tensor("siamese_fc/conv5/concat:0", shape=(8, 6, 6, 256), dtype=float32) Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 6, 6), dtype=float32)
sdiufhasudf Tensor("siamese_fc/conv5/b1/BiasAdd:0", shape=(8, 6, 6, 128), dtype=float32)
111111111 Tensor("siamese_fc_1/conv5/concat:0", shape=(8, 20, 20, 256), dtype=float32) Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 20, 20), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/b1/BiasAdd:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-11 10:07:35.601995: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:07:35.602066: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:07:35.602092: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:07:35.602114: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:07:35.602135: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:07:36.454260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-11 10:07:36.454335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-11 10:07:36.454363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-11 10:07:36.454389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-11 10:07:42.043652: step 0, loss = 0.24, batch loss = 0.15 (1.9 examples/sec; 4.117 sec/batch; 380h:12m:19s remains)
2017-12-11 10:07:42.660783: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5733385 -5.3236794 -6.2197361 -6.8588591 -7.0540133 -6.8472733 -6.4906836 -6.2476764 -6.3301573 -6.6584435 -6.968183 -7.1232047 -6.8286457 -6.1057181 -5.2601027][-5.0806136 -6.0906887 -7.1418653 -7.6432924 -7.4906545 -6.9687119 -6.5139027 -6.3219066 -6.5583572 -7.1034422 -7.6037951 -7.9196634 -7.6743183 -6.8463349 -5.8072596][-5.3993788 -6.5582523 -7.5050507 -7.5295029 -6.7284374 -5.6728897 -4.9755025 -4.7436533 -5.1616106 -6.0601678 -6.9685946 -7.7298527 -7.8344131 -7.1417418 -6.0750322][-5.3719182 -6.5467372 -7.199811 -6.6137414 -5.1087627 -3.4289923 -2.3060818 -1.8997223 -2.5340712 -3.9370852 -5.45224 -6.8642516 -7.51038 -7.1078548 -6.1232538][-4.9776673 -6.1385131 -6.4939308 -5.3755932 -3.2737374 -0.92808819 0.83349514 1.535953 0.62624073 -1.4148529 -3.6528678 -5.786643 -7.0340347 -6.9801664 -6.1165195][-4.3382378 -5.4564648 -5.4975762 -3.9220052 -1.2831736 1.8133311 4.3852777 5.4014196 4.0889578 1.2780304 -1.7544267 -4.5627036 -6.3511181 -6.6484661 -5.9691839][-3.6233048 -4.5899696 -4.318337 -2.51687 0.36042452 3.9203043 7.0241089 8.1131725 6.3489685 2.9392471 -0.60397959 -3.7226336 -5.7447076 -6.2355628 -5.7240953][-3.1396971 -3.9556749 -3.5986538 -2.0583839 0.44292736 3.7392807 6.6243095 7.3991528 5.4084921 1.9470229 -1.436945 -4.1860518 -5.8538017 -6.148344 -5.6099253][-3.151834 -3.8604131 -3.6826818 -2.7500215 -0.936748 1.6929011 3.9327412 4.3637819 2.5698924 -0.40199184 -3.1358461 -5.1547832 -6.2214422 -6.1950612 -5.5871797][-3.456512 -4.0391178 -4.1312318 -3.8046741 -2.5817273 -0.56191158 1.0535722 1.2835078 -0.10202122 -2.3816729 -4.3682814 -5.71129 -6.2862525 -6.0484095 -5.4648638][-3.8692153 -4.3300304 -4.6433682 -4.7174778 -3.8372552 -2.2988167 -1.2484679 -1.1954913 -2.1732304 -3.7649922 -5.100842 -5.9348 -6.1515732 -5.7766294 -5.2614503][-4.27998 -4.697217 -5.1664333 -5.4158993 -4.6900535 -3.4817982 -2.8147814 -2.8415136 -3.4361556 -4.4619732 -5.36632 -5.9535093 -6.0294056 -5.6147089 -5.166419][-4.3361216 -4.8349953 -5.4565434 -5.7814379 -5.1471982 -4.1520486 -3.6568093 -3.6283169 -3.8368251 -4.4336762 -5.1225586 -5.6924353 -5.8272557 -5.4761515 -5.1219182][-4.1832142 -4.74588 -5.4373946 -5.7469454 -5.1269231 -4.1688361 -3.6192672 -3.4010265 -3.2594314 -3.5846605 -4.247786 -4.99596 -5.3862238 -5.2345023 -5.0216117][-3.8082097 -4.4084578 -5.1264639 -5.4038444 -4.70491 -3.5886424 -2.8639951 -2.474402 -2.151912 -2.4061627 -3.1996918 -4.22323 -4.9080505 -4.9640193 -4.8757734]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-11 10:07:47.954937: step 10, loss = 0.26, batch loss = 0.17 (17.9 examples/sec; 0.446 sec/batch; 41h:12m:08s remains)
INFO - root - 2017-12-11 10:07:52.397587: step 20, loss = 0.24, batch loss = 0.15 (17.4 examples/sec; 0.460 sec/batch; 42h:29m:49s remains)
INFO - root - 2017-12-11 10:07:56.919971: step 30, loss = 0.28, batch loss = 0.19 (17.9 examples/sec; 0.447 sec/batch; 41h:14m:16s remains)
INFO - root - 2017-12-11 10:08:01.410911: step 40, loss = 0.32, batch loss = 0.24 (18.3 examples/sec; 0.438 sec/batch; 40h:26m:36s remains)
INFO - root - 2017-12-11 10:08:05.933495: step 50, loss = 0.28, batch loss = 0.20 (18.7 examples/sec; 0.427 sec/batch; 39h:28m:41s remains)
INFO - root - 2017-12-11 10:08:10.387186: step 60, loss = 0.50, batch loss = 0.42 (18.7 examples/sec; 0.429 sec/batch; 39h:36m:12s remains)
INFO - root - 2017-12-11 10:08:14.580404: step 70, loss = 0.24, batch loss = 0.15 (16.9 examples/sec; 0.475 sec/batch; 43h:50m:12s remains)
INFO - root - 2017-12-11 10:08:19.029444: step 80, loss = 0.28, batch loss = 0.20 (18.6 examples/sec; 0.430 sec/batch; 39h:43m:44s remains)
INFO - root - 2017-12-11 10:08:23.504828: step 90, loss = 0.29, batch loss = 0.21 (18.0 examples/sec; 0.444 sec/batch; 41h:01m:51s remains)
INFO - root - 2017-12-11 10:08:28.028813: step 100, loss = 0.24, batch loss = 0.16 (18.1 examples/sec; 0.441 sec/batch; 40h:44m:46s remains)
2017-12-11 10:08:28.481528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0654488 -3.5977962 -2.7778075 -1.5077181 -0.52897024 -0.3455472 -0.32029819 -0.57541323 -1.576283 -2.7582889 -3.3983779 -3.4205098 -3.3109632 -2.8441768 -2.2935951][-3.7327456 -3.0714703 -2.0100961 -0.54114366 0.57352924 0.82487488 0.77740669 0.41041374 -0.67353439 -1.9432547 -2.7019956 -2.8361144 -2.8241663 -2.5051448 -2.1668823][-3.6767113 -2.9960384 -1.8847413 -0.42657614 0.73592281 1.1502929 1.2125263 0.92699575 -0.10115528 -1.4035769 -2.2796102 -2.489542 -2.4601426 -2.1366761 -1.8489857][-3.8109851 -3.2494457 -2.2382646 -0.91660118 0.2471962 0.8701849 1.1492686 1.0632138 0.2113018 -1.0277057 -1.9776268 -2.2549641 -2.1862867 -1.8400776 -1.5474629][-3.9726977 -3.5900977 -2.7347806 -1.5622654 -0.36643314 0.521698 1.1228747 1.3736873 0.81532335 -0.25949907 -1.2286837 -1.5725844 -1.5034039 -1.1959465 -0.94589782][-3.9932055 -3.6400504 -2.7545118 -1.57272 -0.27714586 0.85288143 1.7112012 2.2342529 1.9298244 0.98643541 -0.062231064 -0.58571982 -0.66604376 -0.57483196 -0.50362659][-3.8808184 -3.4404056 -2.4216511 -1.1891057 0.13850451 1.3724627 2.363616 3.0622182 2.9773517 2.123373 0.94457006 0.14498711 -0.22167015 -0.45948219 -0.60433841][-3.7811241 -3.274101 -2.1949089 -1.0041852 0.24745083 1.4741406 2.5060945 3.286653 3.3534613 2.5892496 1.3830247 0.45398235 -0.064177513 -0.50400233 -0.77647471][-3.7642372 -3.2680492 -2.2622113 -1.2231247 -0.14558268 0.9745574 1.9673824 2.7576184 2.90301 2.2454171 1.1326656 0.21768951 -0.29799557 -0.75738835 -1.032814][-3.787776 -3.3573928 -2.5280871 -1.7296796 -0.88138747 0.090226173 1.0181675 1.7754951 1.9440436 1.4043241 0.43935537 -0.41981077 -0.87449026 -1.2382073 -1.436969][-3.8815935 -3.5763674 -3.0021045 -2.4937768 -1.9301112 -1.196028 -0.43847823 0.18569279 0.32968092 -0.061898232 -0.79840684 -1.5324318 -1.9129701 -2.1773298 -2.3029583][-4.0183988 -3.8725286 -3.5697346 -3.3337207 -3.037096 -2.5792174 -2.0546839 -1.6269314 -1.5664675 -1.8494089 -2.3495033 -2.8776832 -3.1156814 -3.2351091 -3.2405572][-4.0827837 -4.05208 -3.920023 -3.8331437 -3.6922493 -3.4451079 -3.1600609 -2.965344 -3.0311608 -3.2387569 -3.5232611 -3.8164475 -3.8716764 -3.8511477 -3.7927413][-4.0801067 -4.1522183 -4.1664782 -4.2251353 -4.2158961 -4.1218624 -4.038559 -4.0290103 -4.134531 -4.2193923 -4.3240738 -4.439939 -4.3659382 -4.2624993 -4.2190475][-3.9222653 -4.048315 -4.1423488 -4.2784023 -4.3403335 -4.3208427 -4.3491459 -4.4468331 -4.576776 -4.6409626 -4.703773 -4.7451515 -4.6011138 -4.4236031 -4.3430424]]...]
INFO - root - 2017-12-11 10:08:32.911668: step 110, loss = 0.37, batch loss = 0.28 (17.8 examples/sec; 0.450 sec/batch; 41h:33m:58s remains)
INFO - root - 2017-12-11 10:08:37.339690: step 120, loss = 0.34, batch loss = 0.26 (17.8 examples/sec; 0.449 sec/batch; 41h:25m:18s remains)
INFO - root - 2017-12-11 10:08:41.829554: step 130, loss = 0.32, batch loss = 0.24 (17.2 examples/sec; 0.464 sec/batch; 42h:49m:35s remains)
INFO - root - 2017-12-11 10:08:46.322147: step 140, loss = 0.28, batch loss = 0.20 (18.8 examples/sec; 0.426 sec/batch; 39h:18m:42s remains)
INFO - root - 2017-12-11 10:08:50.756331: step 150, loss = 0.37, batch loss = 0.28 (18.6 examples/sec; 0.431 sec/batch; 39h:48m:24s remains)
INFO - root - 2017-12-11 10:08:55.230909: step 160, loss = 0.30, batch loss = 0.22 (17.4 examples/sec; 0.460 sec/batch; 42h:26m:46s remains)
INFO - root - 2017-12-11 10:08:59.743883: step 170, loss = 0.31, batch loss = 0.23 (17.6 examples/sec; 0.454 sec/batch; 41h:52m:46s remains)
INFO - root - 2017-12-11 10:09:04.277416: step 180, loss = 0.28, batch loss = 0.20 (17.8 examples/sec; 0.450 sec/batch; 41h:32m:47s remains)
INFO - root - 2017-12-11 10:09:08.375025: step 190, loss = 0.29, batch loss = 0.20 (17.0 examples/sec; 0.471 sec/batch; 43h:28m:52s remains)
INFO - root - 2017-12-11 10:09:12.889789: step 200, loss = 0.28, batch loss = 0.20 (17.6 examples/sec; 0.453 sec/batch; 41h:51m:06s remains)
2017-12-11 10:09:13.405054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2785771 -3.1971307 -3.2563915 -3.5363007 -3.8849564 -4.0928946 -4.0479589 -3.9324663 -3.9922123 -4.3319163 -4.7675009 -4.9893942 -4.8616614 -4.3967938 -3.7802913][-3.677433 -3.5612659 -3.7827926 -4.3471441 -4.908442 -5.0633078 -4.7287688 -4.368782 -4.4204803 -5.0027609 -5.7396865 -6.1455908 -6.0413246 -5.3887587 -4.4395008][-4.1387606 -4.0497994 -4.5156803 -5.3559265 -5.9952087 -5.8775377 -5.056252 -4.3207431 -4.3159409 -5.1560817 -6.249011 -6.9342031 -6.9945045 -6.2921743 -5.101665][-4.444169 -4.4446321 -5.0337505 -5.8130393 -6.0964155 -5.3504286 -3.8754256 -2.7271357 -2.7482183 -4.007802 -5.6505289 -6.8304076 -7.2675638 -6.7429795 -5.5194397][-4.3956509 -4.3933792 -4.8275142 -5.19861 -4.8601503 -3.35426 -1.242553 0.22027111 0.0077400208 -1.8156009 -4.0824032 -5.8169661 -6.7227125 -6.5648289 -5.5437403][-3.7221327 -3.4692166 -3.4384251 -3.2083688 -2.1967175 -0.0074462891 2.5886106 4.1052465 3.4062343 0.89754105 -1.9722626 -4.2334023 -5.6358938 -5.9280167 -5.2496195][-2.9713745 -2.4409242 -1.9191353 -1.1318629 0.44764948 3.1667314 6.111455 7.4986982 6.2071524 3.1161971 -0.17987919 -2.8401628 -4.6489515 -5.300797 -4.9170361][-2.6532259 -2.0593386 -1.2672102 -0.2083478 1.58178 4.3940916 7.301959 8.3978977 6.737874 3.5096145 0.19183922 -2.5423203 -4.4417825 -5.1504226 -4.834765][-2.5998096 -2.1166546 -1.3682535 -0.47494006 1.0015426 3.3696003 5.7926912 6.520092 4.8445024 1.962121 -0.9680419 -3.3951597 -4.9998908 -5.4565754 -5.0022125][-2.8168778 -2.5880318 -2.1974235 -1.8185449 -0.98787093 0.65092087 2.375731 2.7369132 1.2949519 -0.93743825 -3.1803608 -4.9669514 -5.9743109 -5.9926462 -5.2989411][-3.082037 -3.0925295 -3.1614556 -3.3413229 -3.121017 -2.1927979 -1.2060623 -1.2110903 -2.3484864 -3.8963072 -5.3810835 -6.4303608 -6.7916937 -6.3953905 -5.5025258][-3.5547831 -3.7442305 -4.1460385 -4.6353269 -4.7389569 -4.2791815 -3.8591962 -4.1084142 -4.939136 -5.9029765 -6.7099924 -7.1106734 -6.9781704 -6.3393755 -5.3993707][-4.1746712 -4.442872 -4.9568543 -5.4732776 -5.6184349 -5.3477974 -5.1859822 -5.4627066 -5.9952664 -6.5239024 -6.8464651 -6.845027 -6.4867177 -5.8284941 -5.0018582][-4.4248013 -4.6321926 -5.0549965 -5.4236155 -5.4783454 -5.2415195 -5.1314969 -5.324954 -5.6426239 -5.9072914 -5.9712014 -5.8219032 -5.47563 -4.975215 -4.383122][-4.4207683 -4.5712829 -4.8305397 -4.9927483 -4.9297247 -4.6875587 -4.5511518 -4.6155763 -4.7582545 -4.8463211 -4.77913 -4.6001158 -4.3488412 -4.0463734 -3.7085464]]...]
INFO - root - 2017-12-11 10:09:17.870284: step 210, loss = 0.32, batch loss = 0.24 (17.8 examples/sec; 0.450 sec/batch; 41h:34m:28s remains)
INFO - root - 2017-12-11 10:09:22.379428: step 220, loss = 0.29, batch loss = 0.20 (18.2 examples/sec; 0.440 sec/batch; 40h:34m:59s remains)
INFO - root - 2017-12-11 10:09:26.860721: step 230, loss = 0.28, batch loss = 0.20 (17.9 examples/sec; 0.447 sec/batch; 41h:13m:42s remains)
INFO - root - 2017-12-11 10:09:31.362447: step 240, loss = 0.27, batch loss = 0.18 (18.2 examples/sec; 0.440 sec/batch; 40h:34m:52s remains)
INFO - root - 2017-12-11 10:09:35.833128: step 250, loss = 0.32, batch loss = 0.24 (18.7 examples/sec; 0.427 sec/batch; 39h:23m:47s remains)
INFO - root - 2017-12-11 10:09:40.298197: step 260, loss = 0.29, batch loss = 0.21 (18.7 examples/sec; 0.427 sec/batch; 39h:24m:10s remains)
INFO - root - 2017-12-11 10:09:44.793195: step 270, loss = 0.36, batch loss = 0.27 (18.0 examples/sec; 0.443 sec/batch; 40h:55m:00s remains)
INFO - root - 2017-12-11 10:09:49.322925: step 280, loss = 0.27, batch loss = 0.18 (17.7 examples/sec; 0.453 sec/batch; 41h:46m:23s remains)
INFO - root - 2017-12-11 10:09:53.816703: step 290, loss = 0.27, batch loss = 0.18 (17.7 examples/sec; 0.453 sec/batch; 41h:47m:10s remains)
INFO - root - 2017-12-11 10:09:58.267776: step 300, loss = 0.40, batch loss = 0.32 (17.9 examples/sec; 0.448 sec/batch; 41h:19m:09s remains)
2017-12-11 10:09:58.741837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.52905631 -1.076221 -1.3310976 -1.3280792 -1.7368033 -2.2948263 -2.4118793 -1.8894994 -0.9944427 -0.32105446 -0.25257587 -0.964875 -1.9984577 -2.4586279 -2.6733258][-2.2126043 -2.1871181 -1.7914152 -1.3014772 -1.5481634 -2.2961321 -2.7652631 -2.5831556 -2.0009651 -1.5974398 -1.6434674 -2.0868917 -2.6070528 -2.6727164 -2.6663871][-3.6382451 -3.112406 -2.0703814 -1.068078 -1.035639 -1.7579782 -2.4056506 -2.56651 -2.4881852 -2.4963069 -2.6615283 -2.8265257 -2.9054823 -2.6675129 -2.4740024][-4.0572267 -3.2153652 -1.724221 -0.36541367 -0.040167332 -0.53028584 -1.074976 -1.4082696 -1.8153694 -2.1516535 -2.372736 -2.4559646 -2.471765 -2.2056015 -1.9510128][-3.213222 -2.1309543 -0.4039712 1.1162548 1.6610293 1.4406538 1.1559443 0.81299686 0.10291815 -0.31482124 -0.482404 -0.65256119 -0.86703992 -0.7475481 -0.607574][-2.3614202 -0.97990513 0.8771472 2.4276404 3.1958447 3.2968273 3.3164506 3.0023589 2.1641011 1.8817821 1.8191905 1.5430441 1.1358519 1.0671358 0.95391893][-1.9498143 -0.36511946 1.5030847 3.0226288 3.98495 4.3642969 4.6001263 4.3369474 3.5908241 3.5790138 3.6123209 3.2552371 2.7626495 2.5039763 2.0793719][-1.8307347 -0.36278486 1.3319755 2.6909728 3.6861706 4.1717386 4.4748554 4.2918653 3.7804632 4.004055 4.1270552 3.8196125 3.3430996 2.8684115 2.1348953][-2.1843395 -1.1517429 0.2485404 1.3789988 2.2713809 2.7668839 3.1352668 3.1273069 2.8620067 3.1470461 3.275331 3.025702 2.581614 1.9359007 1.0253696][-2.3623827 -2.0824261 -1.2452474 -0.60392022 -0.021623135 0.46954107 1.0310068 1.384161 1.4284382 1.7104411 1.7460651 1.4057803 0.94446564 0.24219179 -0.65735149][-2.1125445 -2.6873298 -2.5825415 -2.7005665 -2.7314239 -2.4314768 -1.7992792 -1.2002585 -0.90176773 -0.51037431 -0.44692397 -0.82163811 -1.1813967 -1.7272167 -2.4603903][-1.7171814 -2.7408636 -3.0942221 -3.7519391 -4.26445 -4.1638861 -3.6202817 -3.0271082 -2.619256 -2.1006165 -1.9238601 -2.268568 -2.515703 -2.9081583 -3.4862158][-1.2460368 -2.3588448 -2.9349465 -3.8416479 -4.5339093 -4.510591 -4.0909758 -3.60249 -3.1430469 -2.5168138 -2.2164071 -2.525902 -2.7731633 -3.1487989 -3.6875818][-1.4050241 -2.3443139 -2.9447722 -3.8674383 -4.5608759 -4.6078711 -4.34839 -3.9558773 -3.4331691 -2.7340157 -2.3170176 -2.5097015 -2.6860256 -2.9317908 -3.3151319][-1.5765073 -2.0450945 -2.425905 -3.1688962 -3.8242798 -4.053587 -4.0396991 -3.7434654 -3.1863813 -2.4933295 -2.0844839 -2.2655735 -2.3920586 -2.412009 -2.5397382]]...]
INFO - root - 2017-12-11 10:10:03.026913: step 310, loss = 0.34, batch loss = 0.25 (16.8 examples/sec; 0.477 sec/batch; 44h:01m:39s remains)
INFO - root - 2017-12-11 10:10:07.557355: step 320, loss = 0.28, batch loss = 0.19 (17.2 examples/sec; 0.464 sec/batch; 42h:48m:21s remains)
INFO - root - 2017-12-11 10:10:12.006605: step 330, loss = 0.28, batch loss = 0.20 (17.1 examples/sec; 0.469 sec/batch; 43h:15m:55s remains)
INFO - root - 2017-12-11 10:10:16.459403: step 340, loss = 0.28, batch loss = 0.19 (17.6 examples/sec; 0.454 sec/batch; 41h:51m:50s remains)
INFO - root - 2017-12-11 10:10:20.930867: step 350, loss = 0.28, batch loss = 0.19 (17.5 examples/sec; 0.457 sec/batch; 42h:08m:40s remains)
INFO - root - 2017-12-11 10:10:25.435947: step 360, loss = 0.36, batch loss = 0.27 (18.1 examples/sec; 0.442 sec/batch; 40h:46m:23s remains)
INFO - root - 2017-12-11 10:10:29.934931: step 370, loss = 0.33, batch loss = 0.24 (18.2 examples/sec; 0.440 sec/batch; 40h:34m:53s remains)
INFO - root - 2017-12-11 10:10:34.461750: step 380, loss = 0.30, batch loss = 0.22 (17.7 examples/sec; 0.453 sec/batch; 41h:45m:11s remains)
INFO - root - 2017-12-11 10:10:38.952831: step 390, loss = 0.29, batch loss = 0.21 (18.0 examples/sec; 0.445 sec/batch; 41h:03m:04s remains)
INFO - root - 2017-12-11 10:10:43.423197: step 400, loss = 0.22, batch loss = 0.14 (18.2 examples/sec; 0.439 sec/batch; 40h:30m:00s remains)
2017-12-11 10:10:43.869525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8400469 -5.4092541 -5.5742407 -5.630362 -5.8456879 -6.26846 -6.3349872 -5.9299493 -5.4061365 -5.4242282 -5.862586 -6.0005412 -5.7869058 -5.9438605 -6.6161661][-5.1252289 -5.7145071 -5.7343974 -5.5630665 -5.5955534 -5.8581991 -5.7507057 -5.1776123 -4.6316204 -4.7882442 -5.3709378 -5.5885487 -5.30556 -5.3146629 -5.8988581][-4.8093128 -5.3494773 -5.16615 -4.7008653 -4.4579816 -4.4694042 -4.1470151 -3.383697 -2.8294382 -3.1662681 -3.9719532 -4.4315395 -4.2901382 -4.3258 -4.9073849][-3.8753257 -4.2327948 -3.791615 -3.0229416 -2.539753 -2.4195669 -2.1029966 -1.3443639 -0.81102991 -1.2306926 -2.1508904 -2.7514813 -2.7267952 -2.8710601 -3.5958993][-3.1593411 -3.3496032 -2.7378342 -1.6768465 -0.85175371 -0.46299291 -0.057802677 0.71701765 1.2622266 0.79836273 -0.22266912 -0.94837976 -1.0536745 -1.3666413 -2.3082144][-2.6904769 -2.8760457 -2.4543173 -1.4799304 -0.55745769 0.032140255 0.65545654 1.7227936 2.6043968 2.3337345 1.2948475 0.3959589 0.030108452 -0.51768494 -1.636601][-1.9172277 -2.0861487 -1.9981475 -1.4693666 -0.97407055 -0.77192783 -0.33344507 0.98126507 2.4322042 2.6817694 1.8971906 0.94455481 0.27480125 -0.54021072 -1.7784021][-1.0596969 -0.99968791 -0.98901796 -0.8225801 -0.92444658 -1.4833391 -1.6681786 -0.50428867 1.2597556 2.0098271 1.6462264 0.79837179 -0.11094332 -1.1877375 -2.5312567][-0.23958302 0.17083597 0.34357691 0.33159924 -0.34712744 -1.8139248 -2.8966618 -2.263814 -0.56793737 0.48450184 0.55255842 -0.0806694 -1.0694392 -2.2787309 -3.641808][0.17810202 0.91776562 1.3877583 1.4202099 0.39247131 -1.8138998 -3.7529907 -3.8181076 -2.4926209 -1.3994222 -1.0776143 -1.5510201 -2.5388005 -3.7390139 -4.9567223][-0.36010408 0.570488 1.3549881 1.6701093 0.71020365 -1.7300358 -4.1231542 -4.7813087 -3.9891257 -3.1394103 -2.8234568 -3.252743 -4.1966643 -5.2876658 -6.2424397][-1.5941777 -0.84444761 -0.050168037 0.4621501 -0.1780057 -2.2751081 -4.4769282 -5.3203435 -4.9515743 -4.4106426 -4.195272 -4.5747662 -5.3983669 -6.3181572 -7.0112581][-3.2699239 -2.8100815 -2.1036134 -1.4707608 -1.7273457 -3.1658959 -4.7400379 -5.393795 -5.2515292 -4.9976835 -4.9224644 -5.2416205 -5.8879724 -6.5922537 -7.050529][-4.7182927 -4.4339581 -3.7851052 -3.0972061 -3.0622573 -3.9054015 -4.8654556 -5.2583437 -5.2320061 -5.1778822 -5.1917834 -5.404706 -5.8204346 -6.2738562 -6.5280905][-5.3361297 -5.2238731 -4.7404847 -4.1652503 -4.0110016 -4.4233589 -4.8966446 -5.0407419 -5.0157905 -5.0193586 -5.0432935 -5.1485596 -5.369267 -5.6163049 -5.7255983]]...]
INFO - root - 2017-12-11 10:10:48.325326: step 410, loss = 0.31, batch loss = 0.22 (17.9 examples/sec; 0.448 sec/batch; 41h:19m:27s remains)
INFO - root - 2017-12-11 10:10:52.823046: step 420, loss = 0.28, batch loss = 0.20 (18.0 examples/sec; 0.445 sec/batch; 41h:04m:10s remains)
INFO - root - 2017-12-11 10:10:57.306057: step 430, loss = 0.31, batch loss = 0.23 (17.1 examples/sec; 0.467 sec/batch; 43h:02m:32s remains)
INFO - root - 2017-12-11 10:11:01.504127: step 440, loss = 0.26, batch loss = 0.18 (17.6 examples/sec; 0.454 sec/batch; 41h:50m:36s remains)
INFO - root - 2017-12-11 10:11:06.094806: step 450, loss = 0.29, batch loss = 0.21 (17.7 examples/sec; 0.453 sec/batch; 41h:44m:38s remains)
INFO - root - 2017-12-11 10:11:10.499863: step 460, loss = 0.27, batch loss = 0.18 (18.9 examples/sec; 0.424 sec/batch; 39h:06m:06s remains)
INFO - root - 2017-12-11 10:11:15.008341: step 470, loss = 0.21, batch loss = 0.13 (17.5 examples/sec; 0.456 sec/batch; 42h:03m:18s remains)
INFO - root - 2017-12-11 10:11:19.582122: step 480, loss = 0.23, batch loss = 0.15 (17.5 examples/sec; 0.456 sec/batch; 42h:05m:25s remains)
INFO - root - 2017-12-11 10:11:24.115540: step 490, loss = 0.27, batch loss = 0.18 (18.2 examples/sec; 0.438 sec/batch; 40h:25m:50s remains)
INFO - root - 2017-12-11 10:11:28.620807: step 500, loss = 0.31, batch loss = 0.23 (18.3 examples/sec; 0.438 sec/batch; 40h:21m:50s remains)
2017-12-11 10:11:29.067949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6275406 -3.694339 -5.4577317 -7.085639 -7.8204 -7.4156981 -6.3250513 -5.2596269 -4.5609026 -4.2104945 -4.0627413 -3.5508966 -2.8001406 -2.2876823 -2.0829771][-2.3975215 -3.9343159 -6.0087166 -7.4580874 -7.7444534 -6.9739075 -5.7348976 -4.691133 -4.2061939 -4.2458577 -4.3216052 -3.7965436 -3.0354626 -2.6131196 -2.46593][-2.8148153 -4.55348 -6.5320177 -7.4244442 -7.034729 -5.8049612 -4.4004412 -3.56253 -3.6394138 -4.47118 -5.103364 -4.7052541 -3.8053787 -3.2090316 -2.9327948][-3.0155845 -4.5522733 -6.1275258 -6.346489 -5.3081446 -3.6783903 -2.2017448 -1.8375993 -2.8296242 -4.68756 -6.0070844 -5.8761787 -4.8177338 -3.8440375 -3.2229288][-3.2656837 -4.5434551 -5.6088886 -5.078753 -3.2951009 -1.0450568 0.80404615 0.9260788 -0.85285425 -3.6399987 -5.7436194 -6.2426534 -5.4266052 -4.2883129 -3.3563051][-3.9263473 -4.9836311 -5.4248171 -3.9905844 -1.2588947 1.9264355 4.6205454 4.9757853 2.7109604 -0.87207627 -3.9091578 -5.4268408 -5.3283248 -4.3573308 -3.3048902][-4.77594 -5.5370078 -5.333806 -3.0902128 0.52090406 4.6118708 8.1797533 8.946599 6.3973761 2.1877599 -1.704618 -4.2300849 -4.9068208 -4.1758056 -3.090713][-5.3488197 -5.7904091 -5.2144141 -2.6992416 1.2188907 5.6152563 9.4744024 10.546961 8.0887756 3.7712088 -0.42682838 -3.4492667 -4.5725365 -3.9885788 -2.9497986][-5.54025 -5.854599 -5.40194 -3.410131 -0.07218647 3.7345276 7.0745611 8.2568235 6.4561577 2.8408189 -0.86568141 -3.6502955 -4.7149038 -4.1399479 -3.2308497][-5.406425 -5.82685 -5.8641205 -4.8907032 -2.6960959 0.045698166 2.5279188 3.7287664 2.8588762 0.40482998 -2.3389578 -4.4142323 -5.0485964 -4.3649077 -3.5759053][-4.9232183 -5.4931483 -5.9772525 -5.9219103 -4.8373122 -3.0680785 -1.3123035 -0.17338133 -0.30540943 -1.7015491 -3.4855201 -4.7506447 -4.8397555 -4.031765 -3.3489747][-4.3774219 -5.1286736 -5.9027853 -6.4031982 -6.049561 -4.8691635 -3.5176885 -2.4994426 -2.2809942 -3.057559 -4.1913176 -4.8328266 -4.5279 -3.6932225 -3.0922093][-3.5134141 -4.4958544 -5.4852581 -6.2886038 -6.3963337 -5.612658 -4.5319309 -3.6858027 -3.4011259 -3.9159698 -4.6798959 -4.9173465 -4.3987765 -3.5803163 -3.0130978][-2.5322194 -3.5676646 -4.5504127 -5.3960929 -5.7891417 -5.4141369 -4.6674027 -4.0922222 -3.97509 -4.4610691 -5.030158 -5.0144653 -4.3792357 -3.5628338 -3.0295596][-1.7195168 -2.5558543 -3.31426 -4.0051279 -4.5164247 -4.4995279 -4.1327538 -3.9271512 -4.14252 -4.75155 -5.278513 -5.24245 -4.6847811 -3.96411 -3.5435095]]...]
INFO - root - 2017-12-11 10:11:33.604152: step 510, loss = 0.30, batch loss = 0.22 (17.3 examples/sec; 0.461 sec/batch; 42h:32m:42s remains)
INFO - root - 2017-12-11 10:11:38.104379: step 520, loss = 0.28, batch loss = 0.20 (18.4 examples/sec; 0.436 sec/batch; 40h:10m:27s remains)
INFO - root - 2017-12-11 10:11:42.625997: step 530, loss = 0.35, batch loss = 0.27 (17.9 examples/sec; 0.447 sec/batch; 41h:13m:26s remains)
INFO - root - 2017-12-11 10:11:47.091227: step 540, loss = 0.37, batch loss = 0.28 (18.0 examples/sec; 0.444 sec/batch; 40h:55m:53s remains)
INFO - root - 2017-12-11 10:11:51.561441: step 550, loss = 0.28, batch loss = 0.20 (17.4 examples/sec; 0.460 sec/batch; 42h:24m:01s remains)
INFO - root - 2017-12-11 10:11:55.710700: step 560, loss = 0.25, batch loss = 0.17 (18.2 examples/sec; 0.440 sec/batch; 40h:36m:41s remains)
INFO - root - 2017-12-11 10:12:00.204048: step 570, loss = 0.27, batch loss = 0.19 (18.2 examples/sec; 0.440 sec/batch; 40h:31m:26s remains)
INFO - root - 2017-12-11 10:12:04.740352: step 580, loss = 0.26, batch loss = 0.18 (16.9 examples/sec; 0.474 sec/batch; 43h:41m:01s remains)
INFO - root - 2017-12-11 10:12:09.170898: step 590, loss = 0.31, batch loss = 0.22 (18.2 examples/sec; 0.441 sec/batch; 40h:38m:08s remains)
INFO - root - 2017-12-11 10:12:13.657949: step 600, loss = 0.29, batch loss = 0.21 (18.2 examples/sec; 0.439 sec/batch; 40h:29m:27s remains)
2017-12-11 10:12:14.104100: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.33116817 -0.063241005 -0.73635411 -1.6166604 -1.9813397 -2.4768288 -2.5658875 -2.7369289 -3.0374179 -3.1985712 -3.7209163 -4.3987937 -5.0097022 -5.1885357 -4.722929][0.30288458 0.32252645 -0.57000995 -1.5509264 -1.7483168 -1.9976335 -1.9131532 -1.9081144 -2.1346684 -2.2603269 -2.9528491 -3.9863689 -4.9325757 -5.3390484 -4.9621072][-0.067848682 -0.129385 -1.0611155 -2.0687282 -2.1685965 -2.1673186 -1.8892438 -1.7455537 -1.9461408 -2.0686131 -2.7712011 -3.8356545 -4.7779837 -5.1711574 -4.7850227][-1.1604729 -1.2056975 -2.10333 -3.0612741 -3.0755839 -2.7845938 -2.2643888 -2.0088651 -2.2359641 -2.3946819 -2.9636126 -3.7473223 -4.3739743 -4.5492377 -4.1060128][-2.3898017 -2.2979431 -2.8882718 -3.4717343 -3.3697288 -2.9896488 -2.4685302 -2.2939057 -2.6118827 -2.8666339 -3.3433716 -3.8101721 -4.0437846 -3.9155855 -3.362381][-3.1777592 -2.8018801 -2.8475423 -2.85606 -2.6747594 -2.4450469 -2.1165833 -2.1541376 -2.605104 -3.0336685 -3.5498753 -3.8527598 -3.8651986 -3.5868905 -3.0047126][-3.1558352 -2.4198637 -1.893975 -1.3411746 -1.0980203 -1.0587804 -0.96657896 -1.3222845 -2.0436394 -2.7474155 -3.3985209 -3.6809437 -3.6877151 -3.4915967 -3.0381184][-2.8296773 -1.6745317 -0.58700514 0.46480989 0.75575876 0.5892868 0.36362028 -0.41805696 -1.491132 -2.433248 -3.1696546 -3.4934227 -3.6138949 -3.6132498 -3.3447456][-2.7093186 -1.3953352 -0.046912193 1.2105007 1.4869738 1.2081099 0.81743145 -0.21154928 -1.4995887 -2.5414286 -3.2337561 -3.5110548 -3.6618924 -3.7815275 -3.659894][-2.8762732 -1.7486429 -0.56587505 0.49480534 0.60373926 0.24301147 -0.15946913 -1.1405208 -2.3512132 -3.2698154 -3.762558 -3.8546641 -3.8694541 -3.9322989 -3.8246498][-3.3460982 -2.6164453 -1.8769784 -1.2389815 -1.3476005 -1.7451968 -2.0515559 -2.7569005 -3.6513584 -4.264595 -4.4537597 -4.2818952 -4.0845728 -3.9824951 -3.7782381][-3.5680676 -3.2082696 -2.9615254 -2.8165841 -3.157239 -3.5779433 -3.7561817 -4.1200171 -4.6651978 -4.9806633 -4.9083958 -4.5276322 -4.1494346 -3.8848717 -3.5600708][-3.3504598 -3.1345234 -3.203809 -3.4418411 -3.9358535 -4.3527627 -4.4575086 -4.6207771 -4.9624152 -5.0941963 -4.89841 -4.4764094 -4.060389 -3.7355666 -3.3479037][-3.0302732 -2.8420868 -3.0413158 -3.4227881 -3.8520117 -4.1149859 -4.1074162 -4.1788611 -4.4421043 -4.4801812 -4.2887735 -4.015574 -3.751128 -3.5363338 -3.2366829][-2.9832382 -2.8093138 -3.0410218 -3.4086852 -3.6243262 -3.6255434 -3.4781682 -3.5277169 -3.7832341 -3.7506666 -3.5673213 -3.4698465 -3.3909268 -3.3317981 -3.1838202]]...]
INFO - root - 2017-12-11 10:12:18.608570: step 610, loss = 0.30, batch loss = 0.21 (18.1 examples/sec; 0.443 sec/batch; 40h:51m:10s remains)
INFO - root - 2017-12-11 10:12:23.067704: step 620, loss = 0.23, batch loss = 0.15 (18.1 examples/sec; 0.443 sec/batch; 40h:48m:30s remains)
INFO - root - 2017-12-11 10:12:27.552527: step 630, loss = 0.23, batch loss = 0.15 (17.3 examples/sec; 0.462 sec/batch; 42h:37m:54s remains)
INFO - root - 2017-12-11 10:12:31.994847: step 640, loss = 0.30, batch loss = 0.22 (17.5 examples/sec; 0.457 sec/batch; 42h:06m:52s remains)
INFO - root - 2017-12-11 10:12:36.470487: step 650, loss = 0.30, batch loss = 0.22 (18.1 examples/sec; 0.442 sec/batch; 40h:44m:03s remains)
INFO - root - 2017-12-11 10:12:40.936009: step 660, loss = 0.26, batch loss = 0.18 (18.0 examples/sec; 0.444 sec/batch; 40h:55m:39s remains)
INFO - root - 2017-12-11 10:12:45.427058: step 670, loss = 0.31, batch loss = 0.23 (18.2 examples/sec; 0.439 sec/batch; 40h:26m:30s remains)
INFO - root - 2017-12-11 10:12:49.710778: step 680, loss = 0.33, batch loss = 0.25 (16.0 examples/sec; 0.500 sec/batch; 46h:03m:37s remains)
INFO - root - 2017-12-11 10:12:54.172536: step 690, loss = 0.28, batch loss = 0.20 (17.8 examples/sec; 0.449 sec/batch; 41h:23m:24s remains)
INFO - root - 2017-12-11 10:12:58.656664: step 700, loss = 0.34, batch loss = 0.26 (18.2 examples/sec; 0.440 sec/batch; 40h:35m:24s remains)
2017-12-11 10:12:59.131308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.303647 -2.2235246 -2.1377802 -2.1125376 -2.0711536 -1.8694487 -1.5293512 -1.1971865 -0.99478054 -0.920789 -0.85361505 -0.590611 0.045071602 0.80181694 1.1774964][-3.3113303 -3.18554 -3.0855272 -3.0912075 -3.1012096 -2.8860636 -2.4343565 -1.9340804 -1.578655 -1.4010315 -1.2516868 -0.96493912 -0.40988731 0.27694035 0.74058771][-3.5816386 -3.4125643 -3.3570709 -3.4963448 -3.688632 -3.5953379 -3.1453578 -2.5787287 -2.1852796 -2.0188763 -1.9044802 -1.7271614 -1.354404 -0.774534 -0.23501778][-2.7769279 -2.4859347 -2.4262438 -2.6877246 -3.0525684 -3.1003675 -2.731307 -2.239871 -2.0069435 -2.0878811 -2.2888486 -2.4726067 -2.4177744 -1.9884007 -1.3863745][-1.2558219 -0.79094195 -0.67843628 -0.99916148 -1.4013584 -1.42519 -1.035074 -0.62798262 -0.6395123 -1.1149795 -1.8748 -2.6702514 -3.0814357 -2.864594 -2.2449622][0.1859808 0.74935246 0.86603785 0.50849676 0.22397137 0.4610486 1.0705261 1.5053954 1.3012557 0.44146872 -0.9237895 -2.3841228 -3.2910492 -3.3155136 -2.7128344][0.74176264 1.2414036 1.2880263 0.91543293 0.83693933 1.4587359 2.4138865 3.0183973 2.7845163 1.7369246 0.0024681091 -1.9038801 -3.1510735 -3.3594158 -2.794384][0.23350382 0.52057981 0.44982719 0.088701248 0.2190423 1.1578732 2.3666964 3.1391554 2.9939814 2.0012579 0.27772856 -1.6622508 -2.9314308 -3.151855 -2.5885038][-0.73440623 -0.5862937 -0.68256426 -0.96984053 -0.7125361 0.27713156 1.3980584 2.0902305 1.9559755 1.1106772 -0.34627962 -1.9604764 -2.9104362 -2.9107099 -2.2502229][-1.5872805 -1.43484 -1.4439936 -1.5934179 -1.3034616 -0.50333047 0.26533604 0.67788935 0.45372438 -0.27331877 -1.4094186 -2.5705154 -3.0439968 -2.70011 -1.887053][-1.9389138 -1.6817667 -1.5130403 -1.4489992 -1.1347911 -0.58943629 -0.24899149 -0.19168806 -0.566988 -1.25505 -2.1528177 -2.9478328 -2.9906912 -2.3590598 -1.4863][-1.6958754 -1.3384914 -1.0376682 -0.84157348 -0.59578085 -0.31949806 -0.30714321 -0.46294117 -0.85603046 -1.4367981 -2.1125827 -2.6409411 -2.3895352 -1.6403947 -0.90867352][-1.1557026 -0.79685235 -0.52634048 -0.37718296 -0.31281424 -0.28950596 -0.48136425 -0.70242977 -0.98262906 -1.3746858 -1.8277586 -2.1535914 -1.7094538 -0.95551825 -0.48016286][-0.674525 -0.34851742 -0.16036463 -0.09848547 -0.18108702 -0.31116295 -0.5936954 -0.80975223 -0.96115971 -1.1756291 -1.4533963 -1.6585538 -1.134773 -0.44633961 -0.2199378][-0.72136784 -0.46482754 -0.37169313 -0.38847446 -0.5574913 -0.7508595 -1.0149493 -1.154321 -1.1748211 -1.2338619 -1.3662798 -1.4878907 -0.95611048 -0.36565876 -0.32273149]]...]
INFO - root - 2017-12-11 10:13:03.666915: step 710, loss = 0.32, batch loss = 0.23 (17.5 examples/sec; 0.456 sec/batch; 42h:03m:56s remains)
INFO - root - 2017-12-11 10:13:08.117761: step 720, loss = 0.29, batch loss = 0.21 (17.8 examples/sec; 0.449 sec/batch; 41h:23m:12s remains)
INFO - root - 2017-12-11 10:13:12.605397: step 730, loss = 0.28, batch loss = 0.20 (18.1 examples/sec; 0.443 sec/batch; 40h:47m:22s remains)
INFO - root - 2017-12-11 10:13:17.101144: step 740, loss = 0.37, batch loss = 0.29 (17.9 examples/sec; 0.446 sec/batch; 41h:08m:35s remains)
INFO - root - 2017-12-11 10:13:21.598199: step 750, loss = 0.27, batch loss = 0.18 (17.9 examples/sec; 0.447 sec/batch; 41h:12m:23s remains)
INFO - root - 2017-12-11 10:13:26.044865: step 760, loss = 0.27, batch loss = 0.19 (18.5 examples/sec; 0.431 sec/batch; 39h:45m:32s remains)
INFO - root - 2017-12-11 10:13:30.481216: step 770, loss = 0.26, batch loss = 0.18 (17.7 examples/sec; 0.453 sec/batch; 41h:45m:19s remains)
INFO - root - 2017-12-11 10:13:34.975122: step 780, loss = 0.36, batch loss = 0.27 (17.6 examples/sec; 0.455 sec/batch; 41h:53m:54s remains)
INFO - root - 2017-12-11 10:13:39.436701: step 790, loss = 0.36, batch loss = 0.27 (17.5 examples/sec; 0.458 sec/batch; 42h:14m:32s remains)
INFO - root - 2017-12-11 10:13:43.808947: step 800, loss = 0.25, batch loss = 0.17 (25.1 examples/sec; 0.319 sec/batch; 29h:22m:15s remains)
2017-12-11 10:13:44.193589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4658103 -4.116622 -3.7216449 -3.0628366 -1.9138377 -0.69314337 0.021505356 -0.00404644 -0.43726873 -0.95804811 -1.7068961 -2.5489631 -3.3370237 -3.585011 -2.8807461][-4.8817978 -4.5052047 -4.0995593 -3.5035748 -2.4488487 -1.2885554 -0.51236629 -0.34623861 -0.6043 -1.0001657 -1.5758998 -2.1173737 -2.6175938 -2.6814218 -1.7905965][-4.3344607 -3.7667258 -3.2517452 -2.7079241 -1.873595 -0.99401784 -0.42208576 -0.42003441 -0.914382 -1.5510554 -2.1183314 -2.44556 -2.7491498 -2.6757154 -1.6617405][-3.20222 -2.4711628 -1.8093185 -1.2066286 -0.45426583 0.28531456 0.72153378 0.3985858 -0.61645818 -1.7492571 -2.4405265 -2.6709793 -2.8841343 -2.6973071 -1.6615133][-2.1275051 -1.1701996 -0.25606918 0.5643754 1.4673495 2.3297725 2.8240008 2.1655831 0.49730015 -1.2858136 -2.3020575 -2.7038794 -3.0343947 -2.8160918 -1.8114314][-1.398999 -0.2948041 0.83884382 1.9244328 3.0901146 4.2151213 4.9220076 4.0759611 1.8732257 -0.49081993 -1.8513582 -2.4995558 -2.9717019 -2.6737285 -1.5881274][-1.3642292 -0.29175758 0.943203 2.2659311 3.7423792 5.1902065 6.2218828 5.4996462 3.2100048 0.69399548 -0.71177411 -1.4043922 -1.8798378 -1.4547777 -0.29076052][-1.722446 -0.72656679 0.43861294 1.70861 3.1173692 4.4726572 5.4716129 4.8920021 2.8920565 0.68126822 -0.46961236 -1.0134623 -1.3421886 -0.7075789 0.6021657][-2.5337758 -1.7431242 -0.80838442 0.23702192 1.4113526 2.5142531 3.337491 2.9648933 1.5036678 -0.12976074 -0.90767455 -1.2367969 -1.3588967 -0.54008174 0.86402512][-3.8719881 -3.3207827 -2.6359072 -1.7828021 -0.758199 0.21533442 0.96597242 0.90410233 0.049264431 -1.0427547 -1.5834587 -1.7909782 -1.7340345 -0.812691 0.62669134][-5.4878354 -5.152987 -4.7478018 -4.1555762 -3.369812 -2.5881195 -1.9660232 -1.8426003 -2.2822003 -3.0127008 -3.4497197 -3.6047678 -3.4088449 -2.4390283 -0.97859359][-6.5782719 -6.4186845 -6.2415628 -5.8842506 -5.3342047 -4.7440891 -4.2858796 -4.1358685 -4.3744831 -4.8906746 -5.2713642 -5.4087772 -5.1511607 -4.2360311 -2.877223][-6.5619621 -6.5080395 -6.4858584 -6.3211155 -5.9958224 -5.6155329 -5.3479147 -5.2681255 -5.4236155 -5.792758 -6.1212683 -6.2906547 -6.1275768 -5.4665017 -4.431283][-5.806241 -5.8029122 -5.8491125 -5.8064451 -5.6706219 -5.4985003 -5.4016542 -5.3960614 -5.5019369 -5.750217 -6.0142479 -6.21826 -6.227603 -5.9085321 -5.2973714][-4.79972 -4.8020329 -4.8419957 -4.8369536 -4.7956514 -4.7408595 -4.7300239 -4.7480512 -4.8164721 -4.9848614 -5.1928949 -5.41201 -5.5465374 -5.4697089 -5.15743]]...]
INFO - root - 2017-12-11 10:13:48.642639: step 810, loss = 0.32, batch loss = 0.24 (17.5 examples/sec; 0.457 sec/batch; 42h:06m:36s remains)
INFO - root - 2017-12-11 10:13:53.155759: step 820, loss = 0.27, batch loss = 0.18 (17.7 examples/sec; 0.451 sec/batch; 41h:32m:07s remains)
INFO - root - 2017-12-11 10:13:57.703219: step 830, loss = 0.37, batch loss = 0.29 (17.2 examples/sec; 0.465 sec/batch; 42h:49m:24s remains)
INFO - root - 2017-12-11 10:14:02.226467: step 840, loss = 0.23, batch loss = 0.15 (18.3 examples/sec; 0.438 sec/batch; 40h:22m:22s remains)
INFO - root - 2017-12-11 10:14:06.724695: step 850, loss = 0.27, batch loss = 0.18 (17.8 examples/sec; 0.449 sec/batch; 41h:20m:12s remains)
INFO - root - 2017-12-11 10:14:11.227561: step 860, loss = 0.39, batch loss = 0.30 (17.9 examples/sec; 0.446 sec/batch; 41h:06m:04s remains)
INFO - root - 2017-12-11 10:14:15.701206: step 870, loss = 0.30, batch loss = 0.22 (18.2 examples/sec; 0.439 sec/batch; 40h:26m:49s remains)
INFO - root - 2017-12-11 10:14:20.190401: step 880, loss = 0.37, batch loss = 0.29 (17.2 examples/sec; 0.464 sec/batch; 42h:43m:25s remains)
INFO - root - 2017-12-11 10:14:24.687111: step 890, loss = 0.31, batch loss = 0.23 (17.7 examples/sec; 0.453 sec/batch; 41h:41m:28s remains)
INFO - root - 2017-12-11 10:14:29.176402: step 900, loss = 0.25, batch loss = 0.16 (18.2 examples/sec; 0.439 sec/batch; 40h:27m:26s remains)
2017-12-11 10:14:29.614080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6160994 -4.8332825 -5.0075374 -5.1019425 -5.1211524 -5.127923 -5.1327615 -5.1679072 -5.2656527 -5.3278728 -5.3201351 -5.162302 -4.848609 -4.4260497 -3.985261][-5.0572147 -5.403172 -5.5779414 -5.6092863 -5.5739126 -5.5124183 -5.4737053 -5.5885773 -5.9216504 -6.2491155 -6.4177027 -6.3141503 -5.9279137 -5.2797 -4.5225105][-5.3561873 -5.7362156 -5.8064547 -5.7071047 -5.5681086 -5.3451266 -5.1917973 -5.4166255 -6.09848 -6.8391438 -7.29438 -7.3634996 -7.048234 -6.269578 -5.205483][-5.3940554 -5.6374159 -5.4064293 -4.9865594 -4.5403843 -3.9328568 -3.5095494 -3.8353462 -4.9512668 -6.2371669 -7.1155734 -7.509922 -7.474637 -6.805263 -5.6487341][-5.3147368 -5.3226647 -4.641274 -3.6712332 -2.6216249 -1.3390901 -0.46856523 -0.8777566 -2.5075662 -4.4534974 -5.8895378 -6.733686 -7.1077003 -6.7303982 -5.7082839][-5.3565841 -5.161212 -4.060739 -2.4765387 -0.6376965 1.5909433 3.1450558 2.7567143 0.62921047 -1.9465921 -3.9684796 -5.3139296 -6.1324978 -6.1206169 -5.3837147][-5.54556 -5.2899714 -4.0012112 -2.022784 0.44666052 3.5902557 5.98435 5.8497677 3.4418707 0.43348217 -2.0652971 -3.8806958 -5.113348 -5.4312973 -4.9580173][-5.8715458 -5.780551 -4.6699996 -2.7654858 -0.2011261 3.3324747 6.3536892 6.7688246 4.6547289 1.7266431 -0.93029261 -3.0556011 -4.5951734 -5.1328955 -4.7910957][-6.0607009 -6.2251081 -5.5125608 -4.0597739 -1.9589319 1.2051115 4.2078943 5.0839996 3.7230644 1.4661603 -0.84236217 -2.915638 -4.5292726 -5.1595964 -4.8585482][-5.9105277 -6.2756133 -5.9956074 -5.17653 -3.8549128 -1.5480824 0.85748816 1.8007646 1.1341763 -0.2810626 -1.9226506 -3.5901344 -4.9784236 -5.5112734 -5.1642256][-5.5392704 -5.9926705 -6.0360732 -5.8107533 -5.2956882 -3.9573004 -2.356636 -1.6457663 -1.9487743 -2.7318645 -3.6866441 -4.7515769 -5.680522 -5.952785 -5.5114436][-5.1272249 -5.5388217 -5.68071 -5.7598619 -5.776001 -5.25209 -4.4340186 -4.0823116 -4.3071446 -4.7787313 -5.263948 -5.7729731 -6.1955905 -6.1764469 -5.6749773][-4.7788854 -5.1018534 -5.2228351 -5.35043 -5.5429144 -5.4995232 -5.25052 -5.1846905 -5.3892326 -5.7113137 -5.9872756 -6.1976476 -6.2954183 -6.0874605 -5.5872908][-4.4659138 -4.7114878 -4.8206425 -4.9344244 -5.1259246 -5.2579203 -5.2837954 -5.3409271 -5.4806805 -5.66931 -5.8253651 -5.9086628 -5.8790503 -5.6364503 -5.2116785][-4.1664877 -4.34009 -4.4579329 -4.5790577 -4.7355075 -4.8741593 -4.9620543 -5.0323091 -5.105031 -5.185039 -5.2438445 -5.2505584 -5.1746492 -4.9686289 -4.6573248]]...]
INFO - root - 2017-12-11 10:14:34.092730: step 910, loss = 0.35, batch loss = 0.26 (18.2 examples/sec; 0.439 sec/batch; 40h:25m:47s remains)
INFO - root - 2017-12-11 10:14:38.571101: step 920, loss = 0.47, batch loss = 0.39 (17.8 examples/sec; 0.449 sec/batch; 41h:21m:50s remains)
INFO - root - 2017-12-11 10:14:42.788506: step 930, loss = 0.25, batch loss = 0.17 (18.0 examples/sec; 0.444 sec/batch; 40h:53m:47s remains)
INFO - root - 2017-12-11 10:14:47.257585: step 940, loss = 0.25, batch loss = 0.17 (17.7 examples/sec; 0.452 sec/batch; 41h:38m:03s remains)
INFO - root - 2017-12-11 10:14:51.851709: step 950, loss = 0.27, batch loss = 0.19 (17.6 examples/sec; 0.455 sec/batch; 41h:56m:19s remains)
INFO - root - 2017-12-11 10:14:56.369036: step 960, loss = 0.28, batch loss = 0.20 (17.0 examples/sec; 0.469 sec/batch; 43h:13m:18s remains)
INFO - root - 2017-12-11 10:15:00.890185: step 970, loss = 0.28, batch loss = 0.20 (17.3 examples/sec; 0.463 sec/batch; 42h:39m:29s remains)
INFO - root - 2017-12-11 10:15:05.332975: step 980, loss = 0.26, batch loss = 0.18 (17.6 examples/sec; 0.455 sec/batch; 41h:54m:01s remains)
INFO - root - 2017-12-11 10:15:09.795796: step 990, loss = 0.27, batch loss = 0.19 (18.2 examples/sec; 0.440 sec/batch; 40h:31m:37s remains)
INFO - root - 2017-12-11 10:15:14.312072: step 1000, loss = 0.27, batch loss = 0.19 (17.1 examples/sec; 0.469 sec/batch; 43h:11m:14s remains)
2017-12-11 10:15:14.751104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.967299 -2.7034576 -3.3569655 -4.2030807 -4.710887 -3.977047 -3.1659319 -3.1470513 -3.7363589 -5.1051612 -6.4071994 -7.7811885 -8.5312748 -7.8169823 -6.5752268][-0.72163391 -1.6817238 -2.5365951 -3.560605 -4.3426652 -3.7501249 -2.9149733 -2.7547476 -3.2856412 -4.7967987 -6.3035703 -7.8132591 -8.6399689 -8.0381155 -6.9125853][-0.20021343 -1.3751142 -2.4316998 -3.5098193 -4.3741822 -3.7976146 -2.7840989 -2.2190783 -2.4151545 -3.9029281 -5.6271505 -7.3676558 -8.3575993 -7.9617524 -7.0182743][-0.028978825 -1.3727264 -2.5255232 -3.4416795 -4.0628977 -3.1949043 -1.7409191 -0.59425712 -0.44996357 -2.0624695 -4.2074423 -6.3363771 -7.6110168 -7.535604 -6.8529491][-0.53945422 -1.8629191 -2.87604 -3.396987 -3.508956 -2.0773003 0.046487331 1.9177022 2.3789544 0.46001863 -2.3088264 -4.9545507 -6.6150069 -6.9607558 -6.6179948][-1.732929 -2.8260891 -3.4214158 -3.2781532 -2.5944581 -0.37879038 2.5001249 5.0388517 5.6798382 3.2652698 -0.28858757 -3.5592632 -5.6931667 -6.5206823 -6.5348673][-3.0667253 -3.7704182 -3.8691788 -3.0824988 -1.6851282 1.1346169 4.4626846 7.3443279 8.0463552 5.2835293 1.1850772 -2.5566311 -5.08642 -6.3083715 -6.5836515][-4.1107421 -4.3290291 -4.0230856 -2.910748 -1.2804079 1.5532718 4.7381096 7.5307369 8.238348 5.5607367 1.5345926 -2.2001526 -4.8230672 -6.202034 -6.5714674][-4.5288711 -4.3454475 -3.8224058 -2.7582364 -1.4629846 0.7004981 3.1033235 5.3257389 5.92772 3.7496691 0.43466949 -2.7128174 -4.9926448 -6.19947 -6.4628286][-4.0810251 -3.7169256 -3.1630516 -2.3887589 -1.7815504 -0.65841126 0.66212368 2.073791 2.4840236 0.930192 -1.434495 -3.7526102 -5.4634223 -6.2948093 -6.3310022][-3.3606706 -3.0502729 -2.6135216 -2.1743729 -2.2063665 -2.0018215 -1.6196935 -0.93474531 -0.70742321 -1.7119334 -3.2583895 -4.8727336 -6.0660658 -6.5357494 -6.3526516][-2.8844907 -2.7219527 -2.3949523 -2.1878953 -2.6177542 -3.0154014 -3.2687955 -3.1276879 -3.1190548 -3.8435454 -4.9236512 -6.0861292 -6.8927503 -7.0689216 -6.6781807][-2.8856876 -2.9106526 -2.7242327 -2.6859674 -3.26618 -3.8978784 -4.4171028 -4.5778289 -4.7950788 -5.5163407 -6.4653525 -7.4052305 -7.918684 -7.8206077 -7.1974134][-3.6429234 -3.8495574 -3.8070369 -3.8492522 -4.3453293 -4.8775406 -5.3239603 -5.5340152 -5.8819938 -6.6906853 -7.6639419 -8.5141373 -8.8089819 -8.4376774 -7.5698147][-4.7489243 -5.0159545 -5.052722 -5.1001854 -5.3961954 -5.6734 -5.876132 -5.9725533 -6.2980976 -7.0848083 -8.0327034 -8.8140869 -8.9854488 -8.4524136 -7.4562664]]...]
INFO - root - 2017-12-11 10:15:19.315037: step 1010, loss = 0.27, batch loss = 0.18 (17.0 examples/sec; 0.472 sec/batch; 43h:26m:53s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50-shortcut
INFO - root - 2017-12-11 10:15:23.788538: step 1020, loss = 0.26, batch loss = 0.18 (18.2 examples/sec; 0.440 sec/batch; 40h:28m:16s remains)
INFO - root - 2017-12-11 10:15:28.315807: step 1030, loss = 0.31, batch loss = 0.22 (17.7 examples/sec; 0.452 sec/batch; 41h:35m:05s remains)
INFO - root - 2017-12-11 10:15:32.828958: step 1040, loss = 0.30, batch loss = 0.21 (17.6 examples/sec; 0.455 sec/batch; 41h:52m:39s remains)
INFO - root - 2017-12-11 10:15:37.076717: step 1050, loss = 0.34, batch loss = 0.26 (17.0 examples/sec; 0.469 sec/batch; 43h:13m:30s remains)
INFO - root - 2017-12-11 10:15:41.562248: step 1060, loss = 0.23, batch loss = 0.15 (17.7 examples/sec; 0.451 sec/batch; 41h:31m:42s remains)
INFO - root - 2017-12-11 10:15:46.016136: step 1070, loss = 0.27, batch loss = 0.18 (17.4 examples/sec; 0.459 sec/batch; 42h:14m:51s remains)
INFO - root - 2017-12-11 10:15:50.482793: step 1080, loss = 0.28, batch loss = 0.20 (18.2 examples/sec; 0.439 sec/batch; 40h:25m:50s remains)
INFO - root - 2017-12-11 10:15:54.923569: step 1090, loss = 0.34, batch loss = 0.26 (18.0 examples/sec; 0.444 sec/batch; 40h:54m:44s remains)
INFO - root - 2017-12-11 10:15:59.506432: step 1100, loss = 0.25, batch loss = 0.17 (16.5 examples/sec; 0.484 sec/batch; 44h:33m:16s remains)
2017-12-11 10:15:59.976667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6627157 -1.7745967 -0.36756992 0.30564833 0.64795685 0.39490414 -0.89060283 -2.6987491 -4.1544485 -4.9437923 -5.5220351 -5.99748 -5.9211121 -5.5170865 -4.7246795][-4.157846 -2.3554196 -0.68686152 0.7400012 1.8725753 2.0748873 0.85041475 -1.0314283 -2.6592569 -3.8556814 -4.8367138 -5.5796795 -5.6550441 -5.3988008 -4.7783322][-4.5029564 -2.7544088 -0.72337627 1.501781 3.3135858 3.6981897 2.3407373 0.35010958 -1.4758997 -3.0419874 -4.3689871 -5.295454 -5.4866977 -5.3708611 -4.8935509][-4.9595532 -3.5386422 -1.4479928 1.1969714 3.437283 4.0303841 2.8323231 1.0063891 -0.85525942 -2.581989 -4.0382218 -5.0414724 -5.3465943 -5.3908849 -5.02448][-5.2853155 -4.3615761 -2.5219069 0.062409878 2.4547043 3.4342031 2.9482236 1.7900462 0.14424706 -1.5774798 -3.0716686 -4.1989098 -4.758913 -5.0939574 -4.9169064][-5.05801 -4.5671644 -3.0608525 -0.7904799 1.5184274 2.8692398 3.278492 2.879703 1.3881931 -0.40488911 -1.9745686 -3.2650056 -4.144568 -4.7915545 -4.7820525][-4.7817492 -4.659359 -3.4768012 -1.5785863 0.55172539 2.2618551 3.5808458 3.9385014 2.5843105 0.66308403 -1.0538385 -2.5705786 -3.7655518 -4.656095 -4.7324991][-4.8486519 -5.1051874 -4.2567096 -2.6855388 -0.72555375 1.3273301 3.4427838 4.4706612 3.3011732 1.2553964 -0.63559556 -2.3679752 -3.8242292 -4.8689132 -4.9702716][-5.0092049 -5.5615392 -5.1189532 -3.972074 -2.3269465 -0.22901583 2.2317586 3.7091331 2.9007888 1.0061345 -0.84774852 -2.6023715 -4.1542726 -5.2606711 -5.3787336][-5.1602378 -5.8895135 -5.8615537 -5.1712432 -3.9076221 -2.0103528 0.47712421 2.3089471 2.1043234 0.7222743 -0.85146141 -2.5033331 -4.0993872 -5.3046227 -5.5294294][-4.7582793 -5.4947252 -5.7785769 -5.50955 -4.663795 -3.2101402 -1.0740104 0.78641891 1.0794134 0.310822 -0.8915441 -2.3656042 -3.900696 -5.1067262 -5.3774524][-4.1372037 -4.7039232 -5.173553 -5.26925 -4.86063 -3.9931777 -2.427511 -0.74534106 -0.05978632 -0.17322779 -0.90550423 -2.1272137 -3.5285754 -4.6827912 -4.9870462][-3.90558 -4.2088833 -4.6567926 -4.8795357 -4.7775383 -4.4804745 -3.5521328 -2.1900384 -1.2875109 -0.83915091 -1.0793884 -1.9706497 -3.1481187 -4.1947384 -4.5245094][-3.8679457 -3.9665179 -4.3010592 -4.4713707 -4.5351419 -4.7043662 -4.3459239 -3.3488851 -2.4043334 -1.5786586 -1.4098506 -1.9557354 -2.839869 -3.717423 -4.0475492][-3.7135358 -3.6926615 -3.9105957 -3.9916289 -4.1361384 -4.5788774 -4.5560174 -3.8075948 -2.9000082 -1.9093482 -1.5390594 -1.885392 -2.5775998 -3.3520646 -3.6918502]]...]
INFO - root - 2017-12-11 10:16:04.543207: step 1110, loss = 0.26, batch loss = 0.18 (16.4 examples/sec; 0.487 sec/batch; 44h:47m:33s remains)
INFO - root - 2017-12-11 10:16:09.080591: step 1120, loss = 0.50, batch loss = 0.42 (17.9 examples/sec; 0.447 sec/batch; 41h:10m:20s remains)
