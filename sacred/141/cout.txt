INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "141"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch32
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(32, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(32, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(32, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(32, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(32, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(32, 72, 8, 8), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-07 10:16:50.695509: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:16:50.695549: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:16:50.695555: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:16:50.695560: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:16:50.695564: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:16:51.361670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.81GiB
2017-12-07 10:16:51.361709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-07 10:16:51.361715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-07 10:16:51.361723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 83125 steps
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(32, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(32, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(32, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(32, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(32, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(32, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(32, 15, 15), dtype=float32)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-07 10:17:00.996662: step 0, loss = 2.06, batch loss = 2.00 (3.9 examples/sec; 8.255 sec/batch; 190h:36m:52s remains)
2017-12-07 10:17:01.875986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2059236 -4.2036219 -4.1977935 -4.1913037 -4.1922307 -4.2085128 -4.2286363 -4.2408395 -4.2402191 -4.2307906 -4.2124987 -4.2022614 -4.2095604 -4.2126508 -4.2066755][-4.2358141 -4.2349381 -4.2265477 -4.2169433 -4.216486 -4.2305794 -4.2445555 -4.2500925 -4.2502122 -4.2510252 -4.2422948 -4.2367725 -4.2411146 -4.2410469 -4.2303104][-4.265511 -4.2619095 -4.2493529 -4.2375331 -4.2349029 -4.2453923 -4.2551713 -4.2584205 -4.262989 -4.2719259 -4.2714448 -4.2687631 -4.2694097 -4.2657161 -4.2507029][-4.2775078 -4.2691426 -4.2522545 -4.2347293 -4.2239814 -4.2271419 -4.2332139 -4.2387519 -4.2499685 -4.2652845 -4.2711077 -4.2693405 -4.2664852 -4.2594051 -4.2413611][-4.2569971 -4.249537 -4.22903 -4.2004852 -4.1727686 -4.1565013 -4.1529202 -4.1639705 -4.1903315 -4.2177353 -4.2294359 -4.2288337 -4.2240024 -4.2125726 -4.1901197][-4.1972613 -4.1922874 -4.170464 -4.1315379 -4.0802917 -4.0328293 -4.011148 -4.033186 -4.08315 -4.1267505 -4.1471043 -4.14648 -4.141408 -4.1288195 -4.1061511][-4.1194596 -4.1134868 -4.0922561 -4.0487161 -3.9790959 -3.8989935 -3.8552096 -3.8904939 -3.9656765 -4.0291104 -4.061018 -4.0693188 -4.0791526 -4.0794473 -4.0665379][-4.0814295 -4.0753188 -4.0586805 -4.0223775 -3.958111 -3.8781581 -3.8362677 -3.8753574 -3.9533849 -4.0204434 -4.0571465 -4.0767031 -4.1022758 -4.1133938 -4.106637][-4.1236649 -4.11956 -4.1076016 -4.0813575 -4.0350885 -3.9786525 -3.9531102 -3.9855924 -4.0455918 -4.0981922 -4.12547 -4.1438074 -4.1695132 -4.177906 -4.166513][-4.1923008 -4.1911497 -4.1841612 -4.1679597 -4.1382947 -4.1051135 -4.0933967 -4.1169467 -4.157886 -4.19678 -4.2144413 -4.2263494 -4.24032 -4.2377815 -4.2185645][-4.2500262 -4.2487621 -4.2449651 -4.2359824 -4.2191796 -4.203352 -4.2015047 -4.2183161 -4.2439861 -4.2684197 -4.2764592 -4.2791209 -4.28054 -4.2686481 -4.243259][-4.2887235 -4.2851777 -4.2815404 -4.2765121 -4.2682328 -4.2628942 -4.2664695 -4.2786174 -4.2923713 -4.3016748 -4.2990069 -4.2933531 -4.2863255 -4.2682214 -4.2374778][-4.3114038 -4.3052883 -4.299242 -4.2955532 -4.2922449 -4.2933526 -4.3004546 -4.3082862 -4.3120732 -4.3081927 -4.2956514 -4.2818193 -4.2686515 -4.24536 -4.2103825][-4.3257189 -4.3174834 -4.3083625 -4.3034706 -4.3014736 -4.3051276 -4.3135686 -4.3183346 -4.3144183 -4.30081 -4.2817049 -4.2643313 -4.2504182 -4.2274303 -4.1922688][-4.3386421 -4.3296709 -4.3190446 -4.3122463 -4.3092432 -4.312469 -4.3207531 -4.3242555 -4.3177137 -4.3018966 -4.2828517 -4.2684708 -4.2601376 -4.245214 -4.2183728]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch32/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch32/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 10:17:10.471069: step 10, loss = 2.08, batch loss = 2.02 (42.3 examples/sec; 0.757 sec/batch; 17h:28m:02s remains)
INFO - root - 2017-12-07 10:17:18.213908: step 20, loss = 2.07, batch loss = 2.01 (42.3 examples/sec; 0.757 sec/batch; 17h:28m:02s remains)
INFO - root - 2017-12-07 10:17:25.830110: step 30, loss = 2.08, batch loss = 2.02 (42.7 examples/sec; 0.749 sec/batch; 17h:17m:17s remains)
INFO - root - 2017-12-07 10:17:33.565542: step 40, loss = 2.07, batch loss = 2.01 (41.9 examples/sec; 0.764 sec/batch; 17h:38m:19s remains)
INFO - root - 2017-12-07 10:17:41.283423: step 50, loss = 2.08, batch loss = 2.03 (41.4 examples/sec; 0.773 sec/batch; 17h:50m:13s remains)
INFO - root - 2017-12-07 10:17:48.980640: step 60, loss = 2.08, batch loss = 2.02 (42.5 examples/sec; 0.752 sec/batch; 17h:21m:27s remains)
INFO - root - 2017-12-07 10:17:56.713092: step 70, loss = 2.07, batch loss = 2.01 (41.2 examples/sec; 0.776 sec/batch; 17h:54m:47s remains)
INFO - root - 2017-12-07 10:18:04.406941: step 80, loss = 2.07, batch loss = 2.01 (40.6 examples/sec; 0.788 sec/batch; 18h:10m:28s remains)
INFO - root - 2017-12-07 10:18:12.134957: step 90, loss = 2.07, batch loss = 2.02 (41.1 examples/sec; 0.778 sec/batch; 17h:56m:54s remains)
INFO - root - 2017-12-07 10:18:19.848096: step 100, loss = 2.08, batch loss = 2.02 (41.9 examples/sec; 0.764 sec/batch; 17h:37m:27s remains)
2017-12-07 10:18:20.556864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3048739 -4.3063412 -4.3090734 -4.3109064 -4.3121948 -4.3133135 -4.31531 -4.3173785 -4.3193521 -4.3191123 -4.3168163 -4.313818 -4.3113489 -4.308434 -4.3042812][-4.2889109 -4.2933316 -4.2982893 -4.3014221 -4.3023777 -4.3015857 -4.3005404 -4.3001366 -4.3019023 -4.3034873 -4.3032441 -4.3023114 -4.3001385 -4.2955561 -4.2887106][-4.265799 -4.2741 -4.2807312 -4.2841234 -4.2832012 -4.2779975 -4.2716851 -4.2674932 -4.2699852 -4.2759976 -4.2797933 -4.2821865 -4.280952 -4.2738361 -4.2636981][-4.266799 -4.2748275 -4.2784963 -4.27866 -4.2723904 -4.261673 -4.2527714 -4.2497044 -4.2572532 -4.2685614 -4.2764864 -4.283596 -4.2856703 -4.2794681 -4.2649021][-4.2944231 -4.2925706 -4.2828808 -4.2685881 -4.2493086 -4.2281404 -4.2136064 -4.2087483 -4.2199993 -4.2389088 -4.2562656 -4.2759371 -4.290051 -4.2916975 -4.2805061][-4.3210378 -4.3056006 -4.2755928 -4.2399087 -4.2035022 -4.1705317 -4.1496429 -4.1393661 -4.1510468 -4.177886 -4.2042217 -4.2359319 -4.261662 -4.2764592 -4.2756157][-4.31226 -4.2853808 -4.2366214 -4.1803665 -4.1273546 -4.087378 -4.0648861 -4.0545187 -4.0663157 -4.096468 -4.1319413 -4.1748161 -4.2111235 -4.2369595 -4.2483597][-4.2907114 -4.2556214 -4.1967793 -4.1308928 -4.0745773 -4.0408468 -4.0266132 -4.0215173 -4.0288272 -4.0548034 -4.0895557 -4.1293259 -4.1675916 -4.2000604 -4.2175732][-4.29592 -4.2601914 -4.2020807 -4.1396117 -4.0911012 -4.06889 -4.0634871 -4.0611382 -4.0618262 -4.08021 -4.1092787 -4.138124 -4.1698136 -4.199667 -4.2138543][-4.3105688 -4.2791014 -4.2270975 -4.1725612 -4.1335707 -4.1181531 -4.1146712 -4.1125073 -4.1143875 -4.1347351 -4.1636572 -4.1844 -4.2046423 -4.2231922 -4.2291179][-4.3122444 -4.28509 -4.239532 -4.1944251 -4.1669397 -4.159656 -4.1588888 -4.161046 -4.171484 -4.1996555 -4.2314878 -4.2513943 -4.2646627 -4.2723203 -4.2669907][-4.3084111 -4.2840495 -4.2466545 -4.2141409 -4.1993213 -4.19831 -4.1989541 -4.2040825 -4.2195215 -4.2513165 -4.2868619 -4.3099408 -4.3209209 -4.3213706 -4.3073807][-4.297 -4.2723136 -4.2405405 -4.2191958 -4.215497 -4.2200594 -4.2218771 -4.2263069 -4.2400608 -4.2689958 -4.3019609 -4.3246861 -4.3349695 -4.3344955 -4.3191581][-4.2622347 -4.2357607 -4.2085328 -4.1963363 -4.1990929 -4.2064919 -4.2116971 -4.2179451 -4.2275143 -4.2466063 -4.2717266 -4.2927823 -4.3028464 -4.3038163 -4.2902985][-4.2323713 -4.2058735 -4.1842561 -4.1781287 -4.1833439 -4.1934047 -4.2030268 -4.2115884 -4.2176647 -4.2262974 -4.2415042 -4.2568345 -4.2660322 -4.2684317 -4.25679]]...]
INFO - root - 2017-12-07 10:18:28.409919: step 110, loss = 2.08, batch loss = 2.02 (39.8 examples/sec; 0.805 sec/batch; 18h:33m:26s remains)
INFO - root - 2017-12-07 10:18:36.169133: step 120, loss = 2.08, batch loss = 2.02 (42.4 examples/sec; 0.755 sec/batch; 17h:24m:49s remains)
INFO - root - 2017-12-07 10:18:44.105302: step 130, loss = 2.08, batch loss = 2.02 (41.1 examples/sec; 0.778 sec/batch; 17h:56m:12s remains)
INFO - root - 2017-12-07 10:18:51.849414: step 140, loss = 2.07, batch loss = 2.01 (41.6 examples/sec; 0.770 sec/batch; 17h:44m:40s remains)
INFO - root - 2017-12-07 10:18:59.776916: step 150, loss = 2.08, batch loss = 2.02 (37.1 examples/sec; 0.863 sec/batch; 19h:53m:11s remains)
INFO - root - 2017-12-07 10:19:07.780010: step 160, loss = 2.09, batch loss = 2.04 (40.4 examples/sec; 0.792 sec/batch; 18h:15m:16s remains)
INFO - root - 2017-12-07 10:19:15.551488: step 170, loss = 2.08, batch loss = 2.02 (42.6 examples/sec; 0.751 sec/batch; 17h:18m:24s remains)
INFO - root - 2017-12-07 10:19:23.315634: step 180, loss = 2.07, batch loss = 2.02 (40.9 examples/sec; 0.783 sec/batch; 18h:02m:05s remains)
INFO - root - 2017-12-07 10:19:31.129136: step 190, loss = 2.08, batch loss = 2.02 (41.2 examples/sec; 0.777 sec/batch; 17h:53m:59s remains)
INFO - root - 2017-12-07 10:19:38.881829: step 200, loss = 2.07, batch loss = 2.01 (42.7 examples/sec; 0.750 sec/batch; 17h:16m:19s remains)
2017-12-07 10:19:39.588800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2355947 -4.2373171 -4.2302237 -4.2163353 -4.2045259 -4.199687 -4.197751 -4.178689 -4.1462283 -4.1414313 -4.1696262 -4.2113361 -4.2466788 -4.2735119 -4.2934241][-4.2366562 -4.2345691 -4.2228489 -4.2061319 -4.1911726 -4.1832085 -4.1789131 -4.1574893 -4.1219339 -4.1128783 -4.1383476 -4.1794448 -4.219871 -4.2516718 -4.2742982][-4.2336826 -4.2279835 -4.2148666 -4.2006392 -4.1885338 -4.18071 -4.1749349 -4.1579485 -4.1292748 -4.1229529 -4.1473103 -4.1866784 -4.2269568 -4.2577686 -4.2777777][-4.2248139 -4.2153182 -4.2023568 -4.1920004 -4.1825762 -4.1736269 -4.16618 -4.1568332 -4.1435928 -4.1445003 -4.1706667 -4.2121491 -4.2531261 -4.2813344 -4.2948532][-4.2053652 -4.1937361 -4.18134 -4.1738181 -4.1634183 -4.1464252 -4.1328707 -4.1290431 -4.1356421 -4.1513147 -4.1843333 -4.2309165 -4.274127 -4.3018661 -4.3101444][-4.1806159 -4.1675344 -4.1516304 -4.1405692 -4.1270056 -4.1034994 -4.0838122 -4.0790353 -4.0997372 -4.1328225 -4.177228 -4.2310233 -4.2787642 -4.3087182 -4.3160625][-4.154006 -4.1346226 -4.1145983 -4.1001821 -4.0854411 -4.0600243 -4.0376482 -4.0300627 -4.0616 -4.1101007 -4.1626282 -4.2188644 -4.2665043 -4.298605 -4.3089309][-4.1376257 -4.1077881 -4.0861368 -4.0717082 -4.0582213 -4.0378895 -4.0203981 -4.0114975 -4.0439072 -4.0954123 -4.1488533 -4.2038264 -4.2486315 -4.2810516 -4.2941837][-4.1184244 -4.0849814 -4.0627136 -4.0510159 -4.0433598 -4.0314279 -4.0205536 -4.0086746 -4.0385 -4.0887885 -4.14073 -4.1914353 -4.2345829 -4.2669287 -4.2808962][-4.0992746 -4.0710745 -4.0486846 -4.0347028 -4.0292249 -4.0241904 -4.0158305 -3.9971664 -4.0171537 -4.0629582 -4.1168761 -4.1672382 -4.2115173 -4.2482057 -4.2673168][-4.0947843 -4.0798841 -4.0624518 -4.0470619 -4.0432463 -4.0427032 -4.0348382 -4.0087357 -4.0132341 -4.0492296 -4.1024303 -4.152576 -4.1967406 -4.2340817 -4.2577319][-4.1047149 -4.1070414 -4.1000924 -4.0883927 -4.0819573 -4.0802021 -4.0728536 -4.046207 -4.038249 -4.0599003 -4.104466 -4.1512513 -4.1928692 -4.2272606 -4.2520671][-4.12749 -4.1393795 -4.1400366 -4.1307359 -4.1205726 -4.1141663 -4.1064997 -4.0841594 -4.0676422 -4.074399 -4.1082168 -4.1488886 -4.1880193 -4.2223725 -4.249898][-4.1517134 -4.1647878 -4.1684623 -4.1625714 -4.1539927 -4.1471844 -4.1397529 -4.1229897 -4.1036139 -4.0991158 -4.1200824 -4.1504726 -4.1841879 -4.2173562 -4.2478523][-4.1863527 -4.1961317 -4.20032 -4.1982193 -4.193357 -4.1890206 -4.1836524 -4.1716242 -4.1546903 -4.14651 -4.1581779 -4.1787395 -4.2045774 -4.2322383 -4.2598081]]...]
INFO - root - 2017-12-07 10:19:47.379117: step 210, loss = 2.08, batch loss = 2.02 (42.3 examples/sec; 0.757 sec/batch; 17h:26m:10s remains)
INFO - root - 2017-12-07 10:19:55.195508: step 220, loss = 2.09, batch loss = 2.03 (40.5 examples/sec; 0.791 sec/batch; 18h:13m:02s remains)
INFO - root - 2017-12-07 10:20:03.016564: step 230, loss = 2.07, batch loss = 2.01 (40.4 examples/sec; 0.792 sec/batch; 18h:14m:16s remains)
INFO - root - 2017-12-07 10:20:10.834336: step 240, loss = 2.08, batch loss = 2.02 (41.7 examples/sec; 0.767 sec/batch; 17h:39m:40s remains)
INFO - root - 2017-12-07 10:20:18.619902: step 250, loss = 2.08, batch loss = 2.02 (41.2 examples/sec; 0.777 sec/batch; 17h:53m:06s remains)
INFO - root - 2017-12-07 10:20:26.517175: step 260, loss = 2.08, batch loss = 2.02 (39.9 examples/sec; 0.801 sec/batch; 18h:26m:52s remains)
INFO - root - 2017-12-07 10:20:34.329613: step 270, loss = 2.07, batch loss = 2.01 (41.1 examples/sec; 0.778 sec/batch; 17h:53m:57s remains)
INFO - root - 2017-12-07 10:20:42.118170: step 280, loss = 2.08, batch loss = 2.02 (41.4 examples/sec; 0.772 sec/batch; 17h:46m:26s remains)
INFO - root - 2017-12-07 10:20:49.893751: step 290, loss = 2.07, batch loss = 2.01 (41.8 examples/sec; 0.765 sec/batch; 17h:36m:19s remains)
INFO - root - 2017-12-07 10:20:57.632744: step 300, loss = 2.07, batch loss = 2.01 (41.4 examples/sec; 0.773 sec/batch; 17h:47m:13s remains)
2017-12-07 10:20:58.338028: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2936344 -4.2887864 -4.2789044 -4.273952 -4.2723651 -4.264924 -4.2495842 -4.2288656 -4.19197 -4.1558433 -4.1424222 -4.153882 -4.1756434 -4.2165956 -4.2657447][-4.3194418 -4.3078761 -4.2952633 -4.2896729 -4.2875342 -4.2795157 -4.2642 -4.2437634 -4.2116146 -4.1783829 -4.1599441 -4.1625504 -4.1791334 -4.2188058 -4.2657847][-4.3018308 -4.2935114 -4.2839646 -4.2806478 -4.2761703 -4.266923 -4.2569361 -4.244771 -4.2205086 -4.1944747 -4.1751246 -4.174582 -4.1884174 -4.2232757 -4.2668757][-4.2671371 -4.2673817 -4.26289 -4.2646885 -4.2612906 -4.2500148 -4.2468224 -4.2447925 -4.2287226 -4.2075076 -4.1857352 -4.1823883 -4.1956086 -4.2262154 -4.267839][-4.2407918 -4.2452106 -4.2431383 -4.2450657 -4.2455626 -4.2386513 -4.2395158 -4.2435613 -4.231391 -4.2109551 -4.1898932 -4.1830893 -4.1957297 -4.2248368 -4.2643981][-4.2329526 -4.2256665 -4.21355 -4.2100029 -4.2127676 -4.2129769 -4.2170339 -4.2256045 -4.2185912 -4.2023287 -4.1820073 -4.1654243 -4.1712914 -4.2024665 -4.2476306][-4.2390885 -4.2130938 -4.1858969 -4.17394 -4.17476 -4.175015 -4.1771755 -4.184042 -4.1813464 -4.1754775 -4.1627235 -4.1400733 -4.1401863 -4.1723318 -4.2236252][-4.2540951 -4.2162166 -4.1804972 -4.1619439 -4.1572633 -4.1541638 -4.1491079 -4.149786 -4.1516271 -4.1569152 -4.1530371 -4.136178 -4.1386828 -4.167026 -4.218091][-4.2666183 -4.2259555 -4.1909523 -4.1742015 -4.1732969 -4.1739736 -4.1671038 -4.1667738 -4.1746287 -4.1820173 -4.1804404 -4.1724715 -4.1797161 -4.2015915 -4.2420406][-4.2669883 -4.230494 -4.2026539 -4.1955962 -4.2059178 -4.217535 -4.2169476 -4.2173791 -4.2243094 -4.2286015 -4.2254133 -4.2209826 -4.2281909 -4.2443 -4.272016][-4.2557936 -4.2258306 -4.2078791 -4.2120266 -4.2357478 -4.2577629 -4.2655387 -4.2636204 -4.2626162 -4.2630839 -4.2590823 -4.2568855 -4.2638307 -4.2725806 -4.2914877][-4.2482862 -4.2192349 -4.2070613 -4.2225289 -4.2568526 -4.2834744 -4.2958736 -4.2935581 -4.2864718 -4.28134 -4.275403 -4.2728939 -4.2776065 -4.2845755 -4.3011246][-4.2495165 -4.2209935 -4.2102942 -4.2339191 -4.2747159 -4.3023295 -4.3163252 -4.3124108 -4.299613 -4.289053 -4.2843189 -4.2814794 -4.2815742 -4.2886753 -4.3049212][-4.2584162 -4.2308764 -4.2200756 -4.2448263 -4.2854252 -4.3114681 -4.32349 -4.3163805 -4.3001513 -4.286952 -4.2834735 -4.2785974 -4.2761583 -4.2831283 -4.3010731][-4.2669024 -4.2467089 -4.2355452 -4.2506962 -4.2799373 -4.2959194 -4.2965617 -4.2848926 -4.2688994 -4.257452 -4.2553778 -4.2497253 -4.2466516 -4.25934 -4.2857928]]...]
INFO - root - 2017-12-07 10:21:06.240924: step 310, loss = 2.07, batch loss = 2.02 (37.7 examples/sec; 0.848 sec/batch; 19h:30m:38s remains)
INFO - root - 2017-12-07 10:21:14.190234: step 320, loss = 2.09, batch loss = 2.03 (37.3 examples/sec; 0.858 sec/batch; 19h:43m:29s remains)
INFO - root - 2017-12-07 10:21:22.125162: step 330, loss = 2.06, batch loss = 2.01 (39.7 examples/sec; 0.807 sec/batch; 18h:33m:23s remains)
INFO - root - 2017-12-07 10:21:29.931801: step 340, loss = 2.09, batch loss = 2.03 (40.7 examples/sec; 0.787 sec/batch; 18h:05m:45s remains)
INFO - root - 2017-12-07 10:21:37.689823: step 350, loss = 2.06, batch loss = 2.01 (42.0 examples/sec; 0.763 sec/batch; 17h:32m:18s remains)
INFO - root - 2017-12-07 10:21:45.391810: step 360, loss = 2.07, batch loss = 2.01 (41.1 examples/sec; 0.779 sec/batch; 17h:54m:31s remains)
INFO - root - 2017-12-07 10:21:53.188618: step 370, loss = 2.07, batch loss = 2.01 (41.1 examples/sec; 0.778 sec/batch; 17h:53m:41s remains)
INFO - root - 2017-12-07 10:22:00.988355: step 380, loss = 2.07, batch loss = 2.01 (40.4 examples/sec; 0.792 sec/batch; 18h:12m:13s remains)
INFO - root - 2017-12-07 10:22:08.725945: step 390, loss = 2.08, batch loss = 2.02 (40.7 examples/sec; 0.785 sec/batch; 18h:03m:01s remains)
INFO - root - 2017-12-07 10:22:16.613723: step 400, loss = 2.07, batch loss = 2.01 (39.9 examples/sec; 0.801 sec/batch; 18h:24m:28s remains)
2017-12-07 10:22:17.312007: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2595196 -4.2380285 -4.2170539 -4.2048578 -4.2120476 -4.2355208 -4.2612886 -4.2755237 -4.2824554 -4.2862005 -4.2825437 -4.2818956 -4.287889 -4.29277 -4.2843719][-4.2368927 -4.2076669 -4.184782 -4.175992 -4.1932416 -4.2300115 -4.26242 -4.2761364 -4.2809162 -4.2831073 -4.2814198 -4.2849154 -4.2950683 -4.3006678 -4.2914929][-4.2243662 -4.1964011 -4.1731205 -4.1670313 -4.1860361 -4.2233672 -4.2519712 -4.2603087 -4.261694 -4.2656035 -4.2716885 -4.28126 -4.2945981 -4.3046718 -4.3043814][-4.2199755 -4.2016783 -4.183517 -4.18124 -4.1958337 -4.2183642 -4.230619 -4.2276282 -4.2237597 -4.2294173 -4.2455244 -4.2626243 -4.2811241 -4.2987132 -4.3103108][-4.2170625 -4.2101636 -4.2021141 -4.2033439 -4.2079067 -4.2108669 -4.2007484 -4.1794043 -4.1660967 -4.1729937 -4.1995878 -4.2290163 -4.2565126 -4.281034 -4.2989845][-4.2207203 -4.2210951 -4.2208891 -4.219101 -4.2078385 -4.1838064 -4.1382117 -4.087616 -4.0729957 -4.1005864 -4.1473942 -4.1944942 -4.233839 -4.2609 -4.276691][-4.23269 -4.2337012 -4.2293882 -4.2151423 -4.1833863 -4.1308703 -4.0523119 -3.9811056 -3.9811246 -4.0434909 -4.1159062 -4.1806412 -4.2268343 -4.2519159 -4.261148][-4.2374754 -4.2325234 -4.2165656 -4.1905165 -4.1504269 -4.0951061 -4.0280118 -3.977874 -3.9933167 -4.0589142 -4.1316609 -4.1982923 -4.2447252 -4.26657 -4.2677307][-4.2283688 -4.2152719 -4.1929946 -4.1673074 -4.1340084 -4.0996757 -4.0787816 -4.0725026 -4.0899568 -4.127213 -4.1720195 -4.219286 -4.258213 -4.2789431 -4.27759][-4.2123551 -4.1923609 -4.172297 -4.1551833 -4.1301532 -4.1129065 -4.1244664 -4.1405106 -4.1514077 -4.1613073 -4.1775289 -4.2083526 -4.2424555 -4.2679024 -4.2706847][-4.2041597 -4.184484 -4.1712055 -4.1578116 -4.1373143 -4.1262417 -4.1444049 -4.1590219 -4.160192 -4.1534338 -4.1512241 -4.1728172 -4.2083778 -4.2395892 -4.2452879][-4.21335 -4.19741 -4.1905961 -4.1788039 -4.1601152 -4.1455679 -4.1512613 -4.1576595 -4.1570787 -4.1445947 -4.1271167 -4.137176 -4.1717386 -4.2043538 -4.2091565][-4.2289195 -4.2171135 -4.2138553 -4.2018085 -4.1818013 -4.1622343 -4.1540961 -4.156311 -4.1600776 -4.1454115 -4.1172886 -4.1171741 -4.1508436 -4.1822104 -4.1859384][-4.2356811 -4.2241035 -4.2209468 -4.2066593 -4.184763 -4.1619825 -4.1511531 -4.1582212 -4.1672463 -4.1554575 -4.1293283 -4.1273117 -4.155427 -4.1839914 -4.1941648][-4.2268252 -4.2058864 -4.195519 -4.180819 -4.1600714 -4.1418519 -4.1424708 -4.1593032 -4.1708641 -4.1664519 -4.1567626 -4.1599894 -4.1835036 -4.2085009 -4.2251787]]...]
INFO - root - 2017-12-07 10:22:25.161977: step 410, loss = 2.07, batch loss = 2.02 (40.0 examples/sec; 0.799 sec/batch; 18h:21m:49s remains)
INFO - root - 2017-12-07 10:22:33.072836: step 420, loss = 2.08, batch loss = 2.02 (39.3 examples/sec; 0.815 sec/batch; 18h:43m:12s remains)
INFO - root - 2017-12-07 10:22:40.967845: step 430, loss = 2.08, batch loss = 2.02 (41.5 examples/sec; 0.772 sec/batch; 17h:43m:37s remains)
INFO - root - 2017-12-07 10:22:48.755840: step 440, loss = 2.07, batch loss = 2.01 (43.0 examples/sec; 0.744 sec/batch; 17h:05m:16s remains)
INFO - root - 2017-12-07 10:22:56.684855: step 450, loss = 2.07, batch loss = 2.01 (34.5 examples/sec; 0.927 sec/batch; 21h:17m:52s remains)
INFO - root - 2017-12-07 10:23:04.427527: step 460, loss = 2.09, batch loss = 2.03 (41.1 examples/sec; 0.778 sec/batch; 17h:51m:39s remains)
INFO - root - 2017-12-07 10:23:12.299732: step 470, loss = 2.06, batch loss = 2.00 (40.8 examples/sec; 0.785 sec/batch; 18h:01m:38s remains)
INFO - root - 2017-12-07 10:23:20.004428: step 480, loss = 2.07, batch loss = 2.01 (42.1 examples/sec; 0.760 sec/batch; 17h:27m:00s remains)
INFO - root - 2017-12-07 10:23:27.730815: step 490, loss = 2.07, batch loss = 2.01 (41.9 examples/sec; 0.763 sec/batch; 17h:30m:58s remains)
INFO - root - 2017-12-07 10:23:35.589119: step 500, loss = 2.09, batch loss = 2.04 (40.0 examples/sec; 0.800 sec/batch; 18h:21m:27s remains)
2017-12-07 10:23:36.322321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.093894 -4.1619086 -4.2175307 -4.2432294 -4.2672315 -4.2899384 -4.3061013 -4.31599 -4.3095679 -4.2635341 -4.1738639 -4.0775671 -4.0501904 -4.1018257 -4.1954718][-4.1379418 -4.1984515 -4.2529478 -4.2692018 -4.2704716 -4.267477 -4.260664 -4.2622662 -4.2759824 -4.2712932 -4.2167435 -4.1359649 -4.106101 -4.1397872 -4.2112246][-4.1939783 -4.24058 -4.2826653 -4.2840438 -4.2574573 -4.2191529 -4.1782875 -4.1712933 -4.2141528 -4.2614756 -4.2578864 -4.203846 -4.1735821 -4.1854439 -4.2282352][-4.2432361 -4.2823124 -4.3144279 -4.2995224 -4.2382112 -4.1474538 -4.0564089 -4.0371914 -4.1192646 -4.2277188 -4.2809849 -4.2609625 -4.2342224 -4.2247429 -4.236743][-4.2714958 -4.3114419 -4.3369756 -4.3075361 -4.2094469 -4.0579228 -3.9013984 -3.8536305 -3.9707294 -4.1476793 -4.2609015 -4.2819772 -4.2718916 -4.2519021 -4.2345982][-4.2912178 -4.3316298 -4.3506317 -4.3081188 -4.1829109 -3.9808507 -3.7481079 -3.6391444 -3.7686749 -4.0169687 -4.2001839 -4.2703633 -4.286366 -4.267612 -4.2288313][-4.303453 -4.3405261 -4.3515096 -4.3028092 -4.1737032 -3.9582443 -3.6797462 -3.4945531 -3.58874 -3.881335 -4.1262693 -4.2444453 -4.2888923 -4.2775426 -4.2282257][-4.3155165 -4.3430529 -4.347796 -4.3024282 -4.1907516 -4.0045233 -3.751142 -3.5472598 -3.5751278 -3.8340819 -4.094409 -4.2371006 -4.2975936 -4.2914743 -4.2390757][-4.3218975 -4.336102 -4.3340392 -4.294539 -4.2088051 -4.0729947 -3.8887668 -3.730257 -3.7262056 -3.9074476 -4.1267071 -4.2589331 -4.3165317 -4.3135109 -4.2665739][-4.3200855 -4.3208656 -4.3132219 -4.281178 -4.2214894 -4.1327004 -4.0149632 -3.9147277 -3.9075732 -4.0221314 -4.182106 -4.2880116 -4.332756 -4.3320117 -4.2957959][-4.3195291 -4.3160858 -4.3055787 -4.2758856 -4.2325406 -4.1798196 -4.1109157 -4.0549269 -4.0507207 -4.1198297 -4.2244873 -4.2978868 -4.3280163 -4.326952 -4.30316][-4.3199577 -4.31833 -4.3091569 -4.280107 -4.2419448 -4.2091441 -4.1770773 -4.1544766 -4.1533551 -4.1896262 -4.2511487 -4.2951646 -4.3108258 -4.308733 -4.295269][-4.3185821 -4.3212366 -4.3151436 -4.2874832 -4.2470245 -4.2169805 -4.2028856 -4.2043328 -4.21292 -4.2349057 -4.2658162 -4.2823853 -4.2831221 -4.2820635 -4.2826214][-4.3194981 -4.326005 -4.3229547 -4.2983012 -4.2573376 -4.2290893 -4.2264328 -4.2435894 -4.2610345 -4.2767096 -4.2892151 -4.286767 -4.2729959 -4.2706757 -4.2804546][-4.3313541 -4.3359876 -4.33392 -4.3119946 -4.2718821 -4.2448158 -4.2484632 -4.2735996 -4.2958121 -4.3101177 -4.3166709 -4.3043532 -4.2824678 -4.2738862 -4.2799478]]...]
INFO - root - 2017-12-07 10:23:44.117855: step 510, loss = 2.07, batch loss = 2.02 (41.8 examples/sec; 0.766 sec/batch; 17h:34m:53s remains)
INFO - root - 2017-12-07 10:23:51.988731: step 520, loss = 2.07, batch loss = 2.01 (41.2 examples/sec; 0.776 sec/batch; 17h:48m:37s remains)
INFO - root - 2017-12-07 10:23:59.817962: step 530, loss = 2.07, batch loss = 2.01 (40.1 examples/sec; 0.798 sec/batch; 18h:18m:50s remains)
INFO - root - 2017-12-07 10:24:07.621203: step 540, loss = 2.06, batch loss = 2.00 (41.6 examples/sec; 0.770 sec/batch; 17h:39m:30s remains)
INFO - root - 2017-12-07 10:24:15.414079: step 550, loss = 2.08, batch loss = 2.02 (40.8 examples/sec; 0.784 sec/batch; 17h:59m:28s remains)
INFO - root - 2017-12-07 10:24:23.265511: step 560, loss = 2.07, batch loss = 2.01 (40.0 examples/sec; 0.800 sec/batch; 18h:20m:22s remains)
INFO - root - 2017-12-07 10:24:31.203911: step 570, loss = 2.08, batch loss = 2.03 (40.4 examples/sec; 0.791 sec/batch; 18h:08m:39s remains)
INFO - root - 2017-12-07 10:24:39.074028: step 580, loss = 2.08, batch loss = 2.02 (40.9 examples/sec; 0.783 sec/batch; 17h:56m:54s remains)
INFO - root - 2017-12-07 10:24:46.829720: step 590, loss = 2.08, batch loss = 2.02 (40.4 examples/sec; 0.793 sec/batch; 18h:10m:35s remains)
INFO - root - 2017-12-07 10:24:54.726759: step 600, loss = 2.07, batch loss = 2.01 (38.9 examples/sec; 0.824 sec/batch; 18h:52m:53s remains)
2017-12-07 10:24:55.440876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2190647 -4.1961975 -4.1909423 -4.1973886 -4.2051616 -4.206984 -4.20265 -4.2055397 -4.2095661 -4.2065678 -4.2051959 -4.2083921 -4.2173762 -4.2205663 -4.2162209][-4.2047291 -4.1741557 -4.1647029 -4.1713262 -4.1813054 -4.1877279 -4.1831756 -4.177494 -4.1746407 -4.1721206 -4.179368 -4.1916814 -4.1997547 -4.2011447 -4.1982412][-4.1996684 -4.16316 -4.1505866 -4.1608744 -4.1738882 -4.1838331 -4.1777296 -4.1650438 -4.15919 -4.1568265 -4.1671834 -4.1832385 -4.1860471 -4.17686 -4.1707635][-4.2210841 -4.1873713 -4.1689858 -4.1742358 -4.1847825 -4.1907115 -4.179162 -4.1634345 -4.1616883 -4.1653843 -4.1781445 -4.1923594 -4.1895041 -4.1685562 -4.1531658][-4.2299452 -4.2062116 -4.1914425 -4.1924338 -4.190618 -4.1849952 -4.1613894 -4.1438808 -4.1461167 -4.1632004 -4.1864476 -4.2017622 -4.1970959 -4.1670647 -4.1332779][-4.2310061 -4.2142949 -4.2094779 -4.2080703 -4.195189 -4.17159 -4.1309409 -4.0991731 -4.1057959 -4.1458549 -4.185421 -4.2053685 -4.2029619 -4.1713295 -4.1233287][-4.2077069 -4.1997714 -4.2021914 -4.2021732 -4.1873055 -4.1482739 -4.0859036 -4.0295348 -4.0349736 -4.1002502 -4.1578417 -4.1884732 -4.189393 -4.1628447 -4.1154747][-4.1805673 -4.1795807 -4.1826019 -4.1831713 -4.1720142 -4.1346006 -4.0659809 -3.9982049 -4.0028524 -4.07969 -4.1461625 -4.1808605 -4.1830559 -4.1625948 -4.1249852][-4.1807156 -4.1832271 -4.1849661 -4.1862807 -4.1799874 -4.1569037 -4.1077843 -4.0626168 -4.0734949 -4.1368818 -4.1869283 -4.2101541 -4.2074752 -4.1892738 -4.1616607][-4.2138033 -4.2200365 -4.2214603 -4.2212772 -4.2120414 -4.1945329 -4.1628942 -4.1397519 -4.152832 -4.1982756 -4.2326093 -4.2480006 -4.2423425 -4.2264128 -4.2096171][-4.2375216 -4.2427535 -4.243773 -4.2449307 -4.2383456 -4.2238669 -4.1991024 -4.181376 -4.1871357 -4.2204242 -4.2508078 -4.263979 -4.2594452 -4.2460837 -4.2374091][-4.2378788 -4.2394533 -4.2360873 -4.2347374 -4.2311573 -4.2201891 -4.2012105 -4.1835246 -4.1867404 -4.2152205 -4.2447371 -4.2589188 -4.2558303 -4.2447572 -4.2375226][-4.2412434 -4.2465444 -4.2424407 -4.2338548 -4.2268682 -4.2158222 -4.1938424 -4.1718044 -4.1767678 -4.2075219 -4.2402773 -4.2559929 -4.2522478 -4.2404146 -4.2300992][-4.23228 -4.2405233 -4.2374673 -4.2261515 -4.2186155 -4.20895 -4.1859822 -4.1609578 -4.165195 -4.1952729 -4.2325506 -4.2500792 -4.2469535 -4.2380333 -4.2271037][-4.2177091 -4.2255592 -4.2306786 -4.2246695 -4.21552 -4.2058034 -4.1857228 -4.1647282 -4.1649914 -4.1875124 -4.2208309 -4.2400055 -4.2425785 -4.2396469 -4.2303958]]...]
INFO - root - 2017-12-07 10:25:03.263259: step 610, loss = 2.07, batch loss = 2.01 (39.9 examples/sec; 0.803 sec/batch; 18h:24m:05s remains)
INFO - root - 2017-12-07 10:25:11.008735: step 620, loss = 2.07, batch loss = 2.02 (40.7 examples/sec; 0.787 sec/batch; 18h:01m:47s remains)
INFO - root - 2017-12-07 10:25:18.794239: step 630, loss = 2.08, batch loss = 2.02 (40.5 examples/sec; 0.790 sec/batch; 18h:06m:41s remains)
INFO - root - 2017-12-07 10:25:26.729048: step 640, loss = 2.09, batch loss = 2.03 (41.3 examples/sec; 0.775 sec/batch; 17h:46m:05s remains)
INFO - root - 2017-12-07 10:25:36.187391: step 650, loss = 2.08, batch loss = 2.02 (37.5 examples/sec; 0.853 sec/batch; 19h:32m:50s remains)
INFO - root - 2017-12-07 10:25:50.069707: step 660, loss = 2.08, batch loss = 2.03 (22.2 examples/sec; 1.442 sec/batch; 33h:02m:15s remains)
INFO - root - 2017-12-07 10:26:04.617960: step 670, loss = 2.09, batch loss = 2.03 (22.0 examples/sec; 1.456 sec/batch; 33h:20m:34s remains)
INFO - root - 2017-12-07 10:26:19.488337: step 680, loss = 2.07, batch loss = 2.01 (22.0 examples/sec; 1.452 sec/batch; 33h:15m:31s remains)
INFO - root - 2017-12-07 10:26:38.363812: step 690, loss = 2.07, batch loss = 2.01 (14.4 examples/sec; 2.228 sec/batch; 51h:00m:59s remains)
INFO - root - 2017-12-07 10:26:59.708569: step 700, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.155 sec/batch; 49h:20m:33s remains)
2017-12-07 10:27:01.366069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2372928 -4.2328424 -4.2286639 -4.2261109 -4.2298136 -4.2356653 -4.2389555 -4.242229 -4.2437124 -4.2421093 -4.2312946 -4.222137 -4.2197928 -4.2267971 -4.2313104][-4.2555728 -4.2506366 -4.2427645 -4.2370095 -4.2414656 -4.254128 -4.26656 -4.2774658 -4.2836518 -4.2848606 -4.2740746 -4.2615256 -4.254622 -4.2572308 -4.2591686][-4.2673645 -4.2597413 -4.2424011 -4.2263632 -4.2252259 -4.2465668 -4.2691021 -4.2915192 -4.3096247 -4.3213344 -4.3197265 -4.3099637 -4.3000717 -4.2959132 -4.2934442][-4.2684555 -4.2580514 -4.2292924 -4.1927214 -4.176631 -4.2039652 -4.2421427 -4.2755575 -4.3041 -4.3305817 -4.3460011 -4.3498449 -4.3431554 -4.3363547 -4.3288164][-4.2786756 -4.26481 -4.2265539 -4.1659913 -4.1143355 -4.1247859 -4.1725636 -4.2237787 -4.2628775 -4.3006697 -4.33196 -4.3506255 -4.3546062 -4.3512611 -4.342278][-4.2937851 -4.281146 -4.2446957 -4.1764383 -4.0935121 -4.0528069 -4.0778866 -4.1328535 -4.1896763 -4.2392335 -4.2849736 -4.3185763 -4.3335714 -4.3340268 -4.324101][-4.3010983 -4.29696 -4.2751756 -4.2211704 -4.1346879 -4.04833 -3.998369 -4.02074 -4.0869455 -4.1594629 -4.2192769 -4.2641354 -4.2888446 -4.2936649 -4.2844987][-4.3093524 -4.3129177 -4.3102117 -4.2771792 -4.2058 -4.1051826 -3.9938493 -3.9309127 -3.981344 -4.0722079 -4.1502113 -4.202991 -4.2368836 -4.2480235 -4.2433677][-4.3138709 -4.3181248 -4.3280687 -4.3171425 -4.2698808 -4.1841569 -4.0644903 -3.9530973 -3.9369509 -4.0171919 -4.1021562 -4.1614003 -4.2004881 -4.2196903 -4.2234683][-4.3143573 -4.3119092 -4.324542 -4.329556 -4.3081317 -4.251514 -4.1617136 -4.0590382 -4.0064073 -4.0307169 -4.0948086 -4.1481214 -4.1858544 -4.2084932 -4.2198591][-4.315114 -4.3102126 -4.3187895 -4.3276038 -4.3216777 -4.2910404 -4.2369838 -4.1659818 -4.1134315 -4.1046181 -4.1279268 -4.1608696 -4.1887107 -4.2062669 -4.2201357][-4.3166909 -4.3116646 -4.31831 -4.3237844 -4.3224888 -4.3079805 -4.2763915 -4.2318559 -4.1932178 -4.1772814 -4.1791582 -4.190619 -4.2070565 -4.2196269 -4.2299562][-4.3247733 -4.3173938 -4.3210607 -4.3224254 -4.319695 -4.3113675 -4.2916903 -4.25888 -4.2289615 -4.2157445 -4.21212 -4.2163959 -4.2231112 -4.2339005 -4.242095][-4.3346 -4.3266034 -4.3244014 -4.3218508 -4.318274 -4.311451 -4.2954564 -4.2640538 -4.2320304 -4.2187495 -4.2164311 -4.2227287 -4.2294216 -4.2374096 -4.2447023][-4.3422122 -4.33739 -4.3305526 -4.3238707 -4.320003 -4.3152037 -4.301888 -4.2685776 -4.2271061 -4.205503 -4.2034197 -4.2126102 -4.2237492 -4.2319565 -4.2363706]]...]
INFO - root - 2017-12-07 10:27:22.725238: step 710, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.140 sec/batch; 48h:59m:46s remains)
INFO - root - 2017-12-07 10:27:43.099203: step 720, loss = 2.06, batch loss = 2.00 (21.2 examples/sec; 1.508 sec/batch; 34h:31m:30s remains)
INFO - root - 2017-12-07 10:27:52.207688: step 730, loss = 2.07, batch loss = 2.02 (40.6 examples/sec; 0.789 sec/batch; 18h:02m:51s remains)
INFO - root - 2017-12-07 10:28:00.056045: step 740, loss = 2.08, batch loss = 2.02 (40.5 examples/sec; 0.790 sec/batch; 18h:04m:47s remains)
INFO - root - 2017-12-07 10:28:07.763196: step 750, loss = 2.08, batch loss = 2.02 (42.0 examples/sec; 0.762 sec/batch; 17h:25m:37s remains)
INFO - root - 2017-12-07 10:28:15.467512: step 760, loss = 2.06, batch loss = 2.00 (41.8 examples/sec; 0.766 sec/batch; 17h:31m:41s remains)
INFO - root - 2017-12-07 10:28:23.180446: step 770, loss = 2.07, batch loss = 2.01 (41.2 examples/sec; 0.776 sec/batch; 17h:45m:02s remains)
INFO - root - 2017-12-07 10:28:31.060443: step 780, loss = 2.07, batch loss = 2.02 (40.4 examples/sec; 0.792 sec/batch; 18h:06m:24s remains)
INFO - root - 2017-12-07 10:28:38.894059: step 790, loss = 2.07, batch loss = 2.02 (39.7 examples/sec; 0.806 sec/batch; 18h:26m:16s remains)
INFO - root - 2017-12-07 10:28:46.750657: step 800, loss = 2.07, batch loss = 2.01 (41.7 examples/sec; 0.767 sec/batch; 17h:32m:44s remains)
2017-12-07 10:28:47.403574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2409568 -4.2253256 -4.2182631 -4.2174115 -4.2179036 -4.223022 -4.2281036 -4.221293 -4.2043142 -4.1916628 -4.1912737 -4.2090445 -4.2349157 -4.2559547 -4.2718477][-4.2074394 -4.1887932 -4.1830196 -4.1863036 -4.190001 -4.1965461 -4.2018795 -4.1954079 -4.1789551 -4.1682358 -4.1727619 -4.1953058 -4.2229047 -4.2420206 -4.2566962][-4.17815 -4.1590052 -4.1569963 -4.1663361 -4.1737947 -4.180697 -4.1869583 -4.1819863 -4.1695 -4.1645203 -4.1766262 -4.1994677 -4.2221279 -4.2342358 -4.244977][-4.1529112 -4.1326923 -4.1315045 -4.1419106 -4.1480851 -4.1543512 -4.1585188 -4.1514583 -4.1446981 -4.1506667 -4.174438 -4.2014503 -4.2220688 -4.2308311 -4.2407527][-4.1277027 -4.1025834 -4.1001616 -4.1056533 -4.1051803 -4.1052113 -4.1015768 -4.0857339 -4.0824356 -4.1053 -4.1459103 -4.1826372 -4.2077589 -4.2188554 -4.2327032][-4.0989089 -4.0702982 -4.0632315 -4.0590611 -4.0484552 -4.0360522 -4.013936 -3.9783235 -3.980958 -4.0316162 -4.0940566 -4.144455 -4.1773887 -4.1935 -4.2154579][-4.0683603 -4.0419312 -4.0256958 -4.0079045 -3.986089 -3.9563723 -3.910444 -3.8556705 -3.8718591 -3.9548805 -4.0340204 -4.0929451 -4.1377807 -4.1656179 -4.1981988][-4.0762281 -4.0614638 -4.0430374 -4.0184169 -3.9917316 -3.9539309 -3.902246 -3.854286 -3.8787127 -3.9621909 -4.029192 -4.0774555 -4.1245322 -4.158473 -4.1919932][-4.1159563 -4.1158819 -4.1040816 -4.0848379 -4.0652657 -4.0332766 -3.9934235 -3.962404 -3.9814897 -4.03806 -4.0796757 -4.1086626 -4.1467671 -4.1764464 -4.2021461][-4.1705003 -4.180707 -4.1780386 -4.166821 -4.1583314 -4.1388946 -4.1115789 -4.0913076 -4.1006045 -4.1321383 -4.1519361 -4.1655445 -4.1926103 -4.2133775 -4.2273088][-4.2260013 -4.2424288 -4.246171 -4.2409277 -4.2399988 -4.2316995 -4.2139854 -4.2003312 -4.2030268 -4.2184572 -4.227932 -4.23298 -4.2482171 -4.257803 -4.2613311][-4.2642465 -4.2779794 -4.2819691 -4.2794747 -4.2810841 -4.2793593 -4.2694669 -4.2617044 -4.2625313 -4.271296 -4.2788439 -4.2806892 -4.2867866 -4.2910757 -4.2908664][-4.2733316 -4.2803931 -4.2827268 -4.2820415 -4.2849584 -4.2858148 -4.2803082 -4.276197 -4.2766128 -4.2844276 -4.2940555 -4.2979436 -4.3015676 -4.3065233 -4.3080297][-4.270453 -4.2733059 -4.2750263 -4.2755313 -4.2772512 -4.2777457 -4.2741404 -4.270885 -4.2703505 -4.2766495 -4.286509 -4.2928095 -4.2985487 -4.3060107 -4.3116269][-4.283742 -4.2855487 -4.2873683 -4.2883835 -4.2891812 -4.2887015 -4.285531 -4.2824574 -4.2809978 -4.2843528 -4.2911777 -4.296679 -4.3016763 -4.3087368 -4.3153424]]...]
INFO - root - 2017-12-07 10:28:55.247203: step 810, loss = 2.08, batch loss = 2.02 (41.1 examples/sec; 0.779 sec/batch; 17h:49m:16s remains)
INFO - root - 2017-12-07 10:29:03.079484: step 820, loss = 2.08, batch loss = 2.02 (40.0 examples/sec; 0.800 sec/batch; 18h:17m:44s remains)
INFO - root - 2017-12-07 10:29:10.927858: step 830, loss = 2.07, batch loss = 2.01 (41.0 examples/sec; 0.780 sec/batch; 17h:49m:30s remains)
INFO - root - 2017-12-07 10:29:18.721103: step 840, loss = 2.08, batch loss = 2.02 (40.9 examples/sec; 0.782 sec/batch; 17h:52m:14s remains)
INFO - root - 2017-12-07 10:29:26.576255: step 850, loss = 2.06, batch loss = 2.01 (42.4 examples/sec; 0.755 sec/batch; 17h:15m:55s remains)
INFO - root - 2017-12-07 10:29:34.465574: step 860, loss = 2.07, batch loss = 2.01 (41.8 examples/sec; 0.766 sec/batch; 17h:30m:41s remains)
INFO - root - 2017-12-07 10:29:42.200801: step 870, loss = 2.08, batch loss = 2.02 (41.3 examples/sec; 0.775 sec/batch; 17h:43m:04s remains)
INFO - root - 2017-12-07 10:29:50.003389: step 880, loss = 2.08, batch loss = 2.02 (42.0 examples/sec; 0.761 sec/batch; 17h:23m:29s remains)
INFO - root - 2017-12-07 10:29:57.915393: step 890, loss = 2.08, batch loss = 2.02 (41.7 examples/sec; 0.768 sec/batch; 17h:32m:02s remains)
INFO - root - 2017-12-07 10:30:05.778796: step 900, loss = 2.07, batch loss = 2.01 (40.6 examples/sec; 0.788 sec/batch; 18h:00m:34s remains)
2017-12-07 10:30:06.487711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2864094 -4.2844524 -4.2864351 -4.2865391 -4.2860656 -4.2857842 -4.2797427 -4.2589889 -4.2205329 -4.1901588 -4.187201 -4.2006345 -4.219336 -4.2495084 -4.2881956][-4.3159938 -4.3139324 -4.31572 -4.3149948 -4.3135014 -4.3117838 -4.304112 -4.2836032 -4.2478333 -4.2182884 -4.2182302 -4.236783 -4.2541747 -4.2759886 -4.3040214][-4.3295345 -4.3266249 -4.3256888 -4.3236456 -4.322299 -4.3202362 -4.3106585 -4.2889724 -4.2540946 -4.227746 -4.2400818 -4.270648 -4.2899351 -4.3031359 -4.3204236][-4.3136067 -4.3096557 -4.3078833 -4.3069897 -4.3053064 -4.3005466 -4.2864776 -4.2590861 -4.2210474 -4.1959777 -4.2219591 -4.269145 -4.2967305 -4.3096223 -4.3228073][-4.2843795 -4.2808447 -4.2800384 -4.279264 -4.2740335 -4.2619286 -4.2368951 -4.1963682 -4.1480742 -4.1207538 -4.1604109 -4.2245741 -4.2683487 -4.2927837 -4.3118472][-4.241497 -4.2410979 -4.2436638 -4.24034 -4.224431 -4.1971912 -4.1580629 -4.11052 -4.0587592 -4.0306726 -4.0772691 -4.1554303 -4.2183161 -4.2595191 -4.2930217][-4.1936488 -4.1992521 -4.2057471 -4.1996679 -4.168838 -4.1196952 -4.0673923 -4.0266681 -3.9903016 -3.9753711 -4.0280175 -4.1125617 -4.1856031 -4.2363558 -4.2799807][-4.1682324 -4.177496 -4.1875386 -4.1841369 -4.1500835 -4.0936761 -4.0436521 -4.0225906 -4.00749 -4.0059657 -4.0568976 -4.1342068 -4.2005844 -4.2458591 -4.2865176][-4.1663785 -4.17622 -4.1888118 -4.1976018 -4.1780767 -4.1350036 -4.0980611 -4.0881925 -4.0848322 -4.0912848 -4.1339145 -4.1930561 -4.2431092 -4.2779226 -4.3096175][-4.1567893 -4.1667967 -4.1846337 -4.2062516 -4.2054529 -4.1838479 -4.1626554 -4.1593256 -4.1655073 -4.177454 -4.2113895 -4.2548342 -4.2876816 -4.3108606 -4.3346567][-4.1419692 -4.1494069 -4.1716928 -4.2010174 -4.215198 -4.2095337 -4.1995072 -4.2007527 -4.2136817 -4.2275586 -4.2564349 -4.2906742 -4.3157711 -4.333786 -4.3497491][-4.1455445 -4.147994 -4.166759 -4.1945696 -4.212482 -4.2153621 -4.213644 -4.2170463 -4.2299714 -4.2416234 -4.2652373 -4.2931495 -4.3179269 -4.3354511 -4.3488889][-4.1657033 -4.1647191 -4.176609 -4.197185 -4.2125473 -4.2165642 -4.2175431 -4.2245646 -4.2368336 -4.2421675 -4.2557373 -4.2766266 -4.2995052 -4.3187323 -4.3362865][-4.2003713 -4.2026968 -4.2150922 -4.2303009 -4.2384949 -4.2352824 -4.2307353 -4.2314677 -4.2341237 -4.227592 -4.2301574 -4.2405109 -4.2588243 -4.2837524 -4.3091254][-4.2364016 -4.2446337 -4.2587056 -4.2674203 -4.2658143 -4.2555566 -4.2417507 -4.2270966 -4.21184 -4.1856575 -4.1756439 -4.1793566 -4.1989961 -4.2359266 -4.2740269]]...]
INFO - root - 2017-12-07 10:30:14.332406: step 910, loss = 2.07, batch loss = 2.02 (41.1 examples/sec; 0.778 sec/batch; 17h:45m:37s remains)
INFO - root - 2017-12-07 10:30:22.173458: step 920, loss = 2.08, batch loss = 2.02 (40.8 examples/sec; 0.785 sec/batch; 17h:55m:38s remains)
INFO - root - 2017-12-07 10:30:29.964907: step 930, loss = 2.07, batch loss = 2.01 (41.0 examples/sec; 0.780 sec/batch; 17h:48m:41s remains)
INFO - root - 2017-12-07 10:30:37.725991: step 940, loss = 2.07, batch loss = 2.01 (40.8 examples/sec; 0.784 sec/batch; 17h:53m:17s remains)
INFO - root - 2017-12-07 10:30:45.479688: step 950, loss = 2.08, batch loss = 2.02 (41.2 examples/sec; 0.777 sec/batch; 17h:43m:38s remains)
INFO - root - 2017-12-07 10:30:53.232517: step 960, loss = 2.07, batch loss = 2.01 (40.0 examples/sec; 0.800 sec/batch; 18h:14m:54s remains)
INFO - root - 2017-12-07 10:31:00.999920: step 970, loss = 2.06, batch loss = 2.01 (42.4 examples/sec; 0.754 sec/batch; 17h:12m:45s remains)
INFO - root - 2017-12-07 10:31:08.854541: step 980, loss = 2.07, batch loss = 2.01 (40.9 examples/sec; 0.783 sec/batch; 17h:52m:01s remains)
INFO - root - 2017-12-07 10:31:16.614423: step 990, loss = 2.07, batch loss = 2.01 (40.3 examples/sec; 0.794 sec/batch; 18h:06m:56s remains)
INFO - root - 2017-12-07 10:31:24.442825: step 1000, loss = 2.08, batch loss = 2.02 (40.5 examples/sec; 0.791 sec/batch; 18h:02m:22s remains)
2017-12-07 10:31:25.159940: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2985792 -4.2987261 -4.2983503 -4.2975 -4.2925196 -4.2875695 -4.282815 -4.2770371 -4.2753634 -4.2836847 -4.2962332 -4.3079162 -4.3175635 -4.32673 -4.3344631][-4.2889524 -4.2899504 -4.2887664 -4.284019 -4.2733097 -4.2642665 -4.2563577 -4.24632 -4.2442617 -4.2580061 -4.2791266 -4.2965145 -4.30787 -4.3177567 -4.3275805][-4.2810364 -4.2818861 -4.2791524 -4.2684669 -4.2475657 -4.2314391 -4.2176323 -4.2013397 -4.1977448 -4.2209449 -4.2566142 -4.28232 -4.298089 -4.3093977 -4.32068][-4.2729478 -4.27635 -4.2747064 -4.2580462 -4.2253327 -4.1976056 -4.1701074 -4.1414104 -4.1341004 -4.168932 -4.2211595 -4.2590904 -4.2846775 -4.3015766 -4.3149347][-4.2626672 -4.2681475 -4.2689595 -4.247993 -4.2076292 -4.1667962 -4.114819 -4.0624065 -4.05199 -4.1005039 -4.1721611 -4.2262034 -4.2663813 -4.2942042 -4.311656][-4.25753 -4.2619772 -4.2599187 -4.2328811 -4.1825776 -4.1230469 -4.0353923 -3.9506271 -3.9453959 -4.0188003 -4.1148424 -4.1895957 -4.24684 -4.2888188 -4.3108273][-4.2586379 -4.2593827 -4.2540536 -4.2180252 -4.1540666 -4.0711956 -3.9383323 -3.8143504 -3.8239009 -3.938966 -4.0609732 -4.1557074 -4.2292271 -4.2835083 -4.3091445][-4.264298 -4.259191 -4.2494168 -4.2057171 -4.1299005 -4.0264144 -3.8601618 -3.7045879 -3.7398491 -3.8963068 -4.0365644 -4.1438222 -4.22187 -4.2793794 -4.3074203][-4.2700429 -4.262836 -4.2509847 -4.2050524 -4.1306024 -4.0366654 -3.8892372 -3.754555 -3.8003993 -3.9474 -4.0675526 -4.1621614 -4.2289243 -4.2792172 -4.3054905][-4.2704487 -4.2647133 -4.2559576 -4.2165923 -4.1614447 -4.101162 -4.0043445 -3.9161391 -3.9489012 -4.0485754 -4.1286416 -4.1966734 -4.2466359 -4.2857904 -4.3074803][-4.2617011 -4.2572565 -4.2528009 -4.2267265 -4.1951303 -4.1670508 -4.1084023 -4.0453396 -4.0615106 -4.1240377 -4.1753359 -4.22499 -4.2653027 -4.2966852 -4.3140788][-4.2531548 -4.2477989 -4.2469926 -4.232832 -4.2211041 -4.2145586 -4.1776323 -4.1265779 -4.1294179 -4.1689072 -4.2039466 -4.2465968 -4.2833514 -4.3086395 -4.321589][-4.2472477 -4.2433429 -4.2485285 -4.2467804 -4.2508974 -4.2589025 -4.238554 -4.196557 -4.1860285 -4.206069 -4.2300673 -4.2663565 -4.2979927 -4.3186064 -4.328229][-4.2532849 -4.2505131 -4.2581758 -4.2647195 -4.2789288 -4.2924495 -4.2801661 -4.2452359 -4.2274923 -4.2382016 -4.2577906 -4.287652 -4.31344 -4.330575 -4.3363023][-4.2753515 -4.271452 -4.27754 -4.2858644 -4.3000317 -4.3097773 -4.2976856 -4.2698956 -4.2547827 -4.2654338 -4.284349 -4.3077464 -4.3271532 -4.3406162 -4.3431668]]...]
INFO - root - 2017-12-07 10:31:32.931400: step 1010, loss = 2.07, batch loss = 2.01 (42.6 examples/sec; 0.751 sec/batch; 17h:07m:35s remains)
INFO - root - 2017-12-07 10:31:40.701301: step 1020, loss = 2.08, batch loss = 2.02 (41.0 examples/sec; 0.780 sec/batch; 17h:47m:31s remains)
INFO - root - 2017-12-07 10:31:48.511479: step 1030, loss = 2.08, batch loss = 2.02 (41.4 examples/sec; 0.773 sec/batch; 17h:37m:46s remains)
INFO - root - 2017-12-07 10:31:56.264700: step 1040, loss = 2.08, batch loss = 2.02 (41.8 examples/sec; 0.765 sec/batch; 17h:27m:07s remains)
INFO - root - 2017-12-07 10:32:04.205977: step 1050, loss = 2.08, batch loss = 2.02 (39.3 examples/sec; 0.814 sec/batch; 18h:33m:54s remains)
INFO - root - 2017-12-07 10:32:11.988349: step 1060, loss = 2.08, batch loss = 2.02 (40.1 examples/sec; 0.798 sec/batch; 18h:11m:09s remains)
INFO - root - 2017-12-07 10:32:19.777462: step 1070, loss = 2.08, batch loss = 2.02 (41.8 examples/sec; 0.766 sec/batch; 17h:27m:37s remains)
INFO - root - 2017-12-07 10:32:27.526520: step 1080, loss = 2.07, batch loss = 2.02 (40.6 examples/sec; 0.788 sec/batch; 17h:57m:12s remains)
INFO - root - 2017-12-07 10:32:35.323960: step 1090, loss = 2.08, batch loss = 2.02 (41.7 examples/sec; 0.768 sec/batch; 17h:29m:54s remains)
INFO - root - 2017-12-07 10:32:43.111169: step 1100, loss = 2.08, batch loss = 2.02 (41.6 examples/sec; 0.770 sec/batch; 17h:32m:42s remains)
2017-12-07 10:32:43.819810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2591133 -4.2761617 -4.2841806 -4.2785378 -4.2685609 -4.2607741 -4.2580538 -4.2580843 -4.2591214 -4.2643557 -4.2726059 -4.2774487 -4.2818165 -4.2909145 -4.2951283][-4.2568021 -4.2872314 -4.3033857 -4.2969518 -4.2802286 -4.2617884 -4.2465796 -4.2371712 -4.2363763 -4.2417746 -4.2522349 -4.2617764 -4.2719221 -4.2857838 -4.2962713][-4.2611041 -4.30211 -4.3227644 -4.3139367 -4.2887883 -4.25458 -4.2209625 -4.1990528 -4.1976676 -4.2089696 -4.2250957 -4.2383347 -4.2546821 -4.2743506 -4.292191][-4.2747545 -4.3153882 -4.3344107 -4.3221488 -4.2904496 -4.2422614 -4.1897845 -4.1517372 -4.14841 -4.1722984 -4.1994987 -4.2184491 -4.2390537 -4.2625222 -4.2854633][-4.2923694 -4.329443 -4.3434734 -4.3272161 -4.2897105 -4.2288785 -4.1552434 -4.094883 -4.0863023 -4.12811 -4.1723022 -4.2062569 -4.2344856 -4.2594824 -4.2811809][-4.3058615 -4.3408976 -4.3500848 -4.3288956 -4.2837682 -4.2098513 -4.1136255 -4.0250726 -4.0060215 -4.0695162 -4.1373482 -4.1921587 -4.2319226 -4.2632122 -4.281321][-4.3157396 -4.3463907 -4.3486571 -4.3199897 -4.2669973 -4.1803708 -4.0643826 -3.949625 -3.9208689 -4.0085998 -4.1037269 -4.1784 -4.2290783 -4.2629576 -4.275517][-4.3242965 -4.3469086 -4.3386049 -4.3017812 -4.2450924 -4.1586595 -4.0463939 -3.932435 -3.9039416 -3.9966898 -4.1030116 -4.1846094 -4.2332134 -4.2587514 -4.2614703][-4.3269892 -4.3441386 -4.3319454 -4.2965188 -4.2455306 -4.1755872 -4.0892844 -4.0073547 -3.990339 -4.0567484 -4.1442895 -4.2139573 -4.2506866 -4.2641506 -4.2575755][-4.3270221 -4.3403726 -4.3302965 -4.302927 -4.2617669 -4.2072964 -4.1461463 -4.094389 -4.0870008 -4.1261034 -4.1870141 -4.2344432 -4.2572966 -4.2625389 -4.2528172][-4.32366 -4.3343182 -4.3291454 -4.3104305 -4.274817 -4.2277026 -4.1805286 -4.144887 -4.1395774 -4.1618586 -4.2032309 -4.2300963 -4.2386541 -4.2403789 -4.2345247][-4.3170295 -4.3265643 -4.325316 -4.3103819 -4.2775264 -4.2382388 -4.2030497 -4.176425 -4.1690106 -4.1824641 -4.2087331 -4.215776 -4.20938 -4.2087064 -4.2114897][-4.3015852 -4.3124828 -4.3132067 -4.3005605 -4.2731094 -4.2452269 -4.222425 -4.203773 -4.1953259 -4.2021484 -4.2141118 -4.2046752 -4.1887031 -4.1888981 -4.1989641][-4.2814035 -4.2956133 -4.3021665 -4.2969131 -4.2784023 -4.26199 -4.2516284 -4.2414126 -4.2307253 -4.2273979 -4.2250366 -4.20391 -4.1813812 -4.1843295 -4.1995316][-4.2636142 -4.2793927 -4.2927928 -4.2969222 -4.2866178 -4.2774587 -4.2748013 -4.2695889 -4.2581258 -4.2500806 -4.2378254 -4.209228 -4.1828232 -4.1862159 -4.203011]]...]
INFO - root - 2017-12-07 10:32:51.654926: step 1110, loss = 2.07, batch loss = 2.01 (39.7 examples/sec; 0.807 sec/batch; 18h:22m:34s remains)
INFO - root - 2017-12-07 10:32:59.542896: step 1120, loss = 2.08, batch loss = 2.03 (41.0 examples/sec; 0.780 sec/batch; 17h:46m:19s remains)
INFO - root - 2017-12-07 10:33:07.398327: step 1130, loss = 2.07, batch loss = 2.02 (40.7 examples/sec; 0.785 sec/batch; 17h:53m:12s remains)
INFO - root - 2017-12-07 10:33:15.226444: step 1140, loss = 2.09, batch loss = 2.03 (41.2 examples/sec; 0.777 sec/batch; 17h:41m:59s remains)
INFO - root - 2017-12-07 10:33:23.037871: step 1150, loss = 2.07, batch loss = 2.01 (42.0 examples/sec; 0.762 sec/batch; 17h:21m:18s remains)
INFO - root - 2017-12-07 10:33:30.884730: step 1160, loss = 2.08, batch loss = 2.02 (41.0 examples/sec; 0.780 sec/batch; 17h:45m:47s remains)
INFO - root - 2017-12-07 10:33:38.659417: step 1170, loss = 2.07, batch loss = 2.02 (40.3 examples/sec; 0.793 sec/batch; 18h:03m:44s remains)
INFO - root - 2017-12-07 10:33:46.492652: step 1180, loss = 2.08, batch loss = 2.02 (42.5 examples/sec; 0.753 sec/batch; 17h:08m:22s remains)
INFO - root - 2017-12-07 10:33:54.254308: step 1190, loss = 2.07, batch loss = 2.01 (39.2 examples/sec; 0.817 sec/batch; 18h:35m:00s remains)
INFO - root - 2017-12-07 10:34:02.027211: step 1200, loss = 2.08, batch loss = 2.02 (40.9 examples/sec; 0.783 sec/batch; 17h:48m:53s remains)
2017-12-07 10:34:02.744629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2261395 -4.2218332 -4.2230053 -4.2421832 -4.2585325 -4.2549186 -4.2510824 -4.2534461 -4.2612009 -4.2675276 -4.2757483 -4.2788334 -4.273128 -4.2655849 -4.2631145][-4.2084746 -4.2045054 -4.2067246 -4.2280164 -4.25345 -4.2553129 -4.2480779 -4.2456489 -4.2543578 -4.2631197 -4.2665076 -4.2598572 -4.24717 -4.2350359 -4.2392378][-4.191484 -4.1938052 -4.201242 -4.2219348 -4.2488708 -4.2531872 -4.2442656 -4.2385316 -4.2456889 -4.2540574 -4.2510943 -4.2336073 -4.2107224 -4.1944571 -4.2025433][-4.1812773 -4.1916656 -4.2057323 -4.2263355 -4.2507725 -4.2553234 -4.2480464 -4.2407775 -4.2439303 -4.2474818 -4.2345872 -4.2062054 -4.1746259 -4.1560588 -4.1655788][-4.18535 -4.2024574 -4.2178783 -4.2330456 -4.25153 -4.2529874 -4.2457294 -4.2349157 -4.2313161 -4.2280993 -4.2089725 -4.1748242 -4.1405072 -4.1224966 -4.1335316][-4.1948576 -4.2157512 -4.228868 -4.2374949 -4.2476068 -4.2419467 -4.2291689 -4.2117114 -4.1978831 -4.1883912 -4.1728096 -4.1458836 -4.1157589 -4.0994658 -4.1099486][-4.1983776 -4.220551 -4.2293634 -4.2329874 -4.2353635 -4.2239413 -4.20534 -4.1822295 -4.1602149 -4.1474943 -4.1392531 -4.1252789 -4.1019363 -4.0880971 -4.1002474][-4.1976857 -4.2156935 -4.2203851 -4.2185216 -4.2125816 -4.1979756 -4.181509 -4.1615033 -4.1407008 -4.1285119 -4.1256356 -4.1209884 -4.104846 -4.0934796 -4.1064391][-4.1894321 -4.1985331 -4.1996555 -4.1939712 -4.1837378 -4.1681376 -4.1561069 -4.1409225 -4.1272316 -4.1259794 -4.1327662 -4.1356516 -4.1251416 -4.1167369 -4.1277328][-4.1794372 -4.178041 -4.1721129 -4.1624 -4.1495776 -4.1341686 -4.1242661 -4.1173844 -4.1149583 -4.1262937 -4.1430631 -4.1558571 -4.1526895 -4.1448517 -4.1498628][-4.1692181 -4.1599579 -4.1461205 -4.1345382 -4.12046 -4.1048627 -4.0962329 -4.0964141 -4.1023984 -4.1206708 -4.1454849 -4.165338 -4.167439 -4.1624427 -4.1627712][-4.1586418 -4.1454673 -4.1295085 -4.1222596 -4.1144404 -4.1004171 -4.0910282 -4.0918355 -4.1011515 -4.1230226 -4.152771 -4.1746025 -4.17816 -4.1726842 -4.1683469][-4.1610856 -4.150229 -4.1431875 -4.14645 -4.1465034 -4.1352344 -4.1218777 -4.1174955 -4.12578 -4.1482058 -4.1768203 -4.1959934 -4.1980734 -4.1915016 -4.1855774][-4.1745381 -4.1692157 -4.17337 -4.1856074 -4.1935573 -4.1866975 -4.1725464 -4.1669979 -4.1757665 -4.1944275 -4.2146368 -4.2262135 -4.2254267 -4.2186704 -4.2131362][-4.1787057 -4.1794648 -4.1900158 -4.2089648 -4.2230034 -4.2229562 -4.2148919 -4.2119193 -4.22112 -4.2335024 -4.2435226 -4.2469187 -4.2437634 -4.2380552 -4.233634]]...]
INFO - root - 2017-12-07 10:34:10.598243: step 1210, loss = 2.08, batch loss = 2.02 (41.1 examples/sec; 0.778 sec/batch; 17h:42m:12s remains)
INFO - root - 2017-12-07 10:34:18.414957: step 1220, loss = 2.07, batch loss = 2.01 (40.7 examples/sec; 0.786 sec/batch; 17h:53m:24s remains)
INFO - root - 2017-12-07 10:34:26.249496: step 1230, loss = 2.07, batch loss = 2.01 (41.0 examples/sec; 0.780 sec/batch; 17h:45m:15s remains)
INFO - root - 2017-12-07 10:34:34.044784: step 1240, loss = 2.09, batch loss = 2.03 (41.1 examples/sec; 0.778 sec/batch; 17h:41m:54s remains)
INFO - root - 2017-12-07 10:34:41.933021: step 1250, loss = 2.07, batch loss = 2.01 (40.4 examples/sec; 0.792 sec/batch; 18h:00m:59s remains)
INFO - root - 2017-12-07 10:34:49.742448: step 1260, loss = 2.08, batch loss = 2.02 (41.5 examples/sec; 0.770 sec/batch; 17h:31m:16s remains)
INFO - root - 2017-12-07 10:34:57.544256: step 1270, loss = 2.08, batch loss = 2.02 (40.8 examples/sec; 0.784 sec/batch; 17h:49m:16s remains)
INFO - root - 2017-12-07 10:35:05.315784: step 1280, loss = 2.08, batch loss = 2.02 (41.7 examples/sec; 0.768 sec/batch; 17h:27m:54s remains)
INFO - root - 2017-12-07 10:35:13.162294: step 1290, loss = 2.06, batch loss = 2.01 (40.4 examples/sec; 0.792 sec/batch; 18h:00m:06s remains)
INFO - root - 2017-12-07 10:35:20.966382: step 1300, loss = 2.07, batch loss = 2.01 (39.7 examples/sec; 0.806 sec/batch; 18h:19m:18s remains)
2017-12-07 10:35:21.665292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3163753 -4.2877049 -4.2612319 -4.2527137 -4.2512441 -4.2384958 -4.2229314 -4.2150645 -4.2233253 -4.228003 -4.2261071 -4.2133894 -4.1856313 -4.160697 -4.1527328][-4.318152 -4.2949276 -4.2734528 -4.265924 -4.2628531 -4.2531667 -4.2409668 -4.2333956 -4.2479715 -4.260323 -4.2559705 -4.232585 -4.1904588 -4.1597247 -4.1541014][-4.3177567 -4.2986312 -4.2817478 -4.2747626 -4.2696009 -4.2545166 -4.2378135 -4.2297225 -4.2503166 -4.2708168 -4.2649508 -4.2334971 -4.183341 -4.1480188 -4.1455712][-4.31469 -4.2978659 -4.2827559 -4.2748256 -4.2655354 -4.2441845 -4.2215877 -4.2119894 -4.2354989 -4.2641621 -4.2628331 -4.2257609 -4.167974 -4.1309566 -4.1350174][-4.3094096 -4.2918053 -4.2746339 -4.2587471 -4.2415905 -4.2100849 -4.1792865 -4.1710997 -4.1996493 -4.2343836 -4.2401752 -4.2049294 -4.1532621 -4.1211805 -4.1302881][-4.2981329 -4.2747436 -4.2506576 -4.2230277 -4.192121 -4.14318 -4.0993719 -4.0997033 -4.144208 -4.1896591 -4.2059932 -4.1777873 -4.1393576 -4.1169219 -4.1313777][-4.2910233 -4.2638259 -4.2322073 -4.1939588 -4.1440682 -4.0670047 -3.9943264 -3.9972084 -4.0690203 -4.1391845 -4.1737828 -4.1540647 -4.1322489 -4.1248226 -4.1455965][-4.2820072 -4.2489853 -4.2127109 -4.1655779 -4.0937395 -3.9798245 -3.8516965 -3.8392513 -3.9543986 -4.0665045 -4.12579 -4.1284747 -4.1271095 -4.1418982 -4.1707144][-4.2785397 -4.2415066 -4.210238 -4.1757631 -4.1131182 -3.9945297 -3.8437555 -3.8001196 -3.9111731 -4.0278111 -4.0906153 -4.1085896 -4.1219082 -4.1506934 -4.1824107][-4.2869463 -4.25364 -4.2335777 -4.22308 -4.1955304 -4.116796 -4.0092545 -3.9554429 -4.0133162 -4.0912237 -4.1318617 -4.1402173 -4.1447735 -4.1661096 -4.1894593][-4.2935419 -4.2667756 -4.2534752 -4.2538977 -4.2451658 -4.1977358 -4.1269941 -4.083992 -4.1108975 -4.1605659 -4.1872692 -4.1884255 -4.1818895 -4.1897397 -4.2010384][-4.292603 -4.2707829 -4.2598548 -4.2627254 -4.2615719 -4.2334785 -4.1866512 -4.1531143 -4.1650119 -4.1979594 -4.215826 -4.211967 -4.20349 -4.2066031 -4.20711][-4.2924614 -4.2772279 -4.2676258 -4.2715588 -4.2735176 -4.2529478 -4.2220569 -4.2012362 -4.209414 -4.2306738 -4.2428522 -4.23706 -4.2332206 -4.2331467 -4.2266226][-4.293097 -4.2831759 -4.2749472 -4.2761874 -4.2764478 -4.2597294 -4.2401881 -4.2295241 -4.2391419 -4.2551103 -4.2643986 -4.2575345 -4.2531018 -4.2498884 -4.240747][-4.2918668 -4.28535 -4.281507 -4.2822919 -4.2832556 -4.2731004 -4.2604203 -4.2536225 -4.2619252 -4.27622 -4.284142 -4.2766485 -4.2709985 -4.2661796 -4.2577448]]...]
INFO - root - 2017-12-07 10:35:29.423309: step 1310, loss = 2.07, batch loss = 2.01 (40.5 examples/sec; 0.791 sec/batch; 17h:58m:40s remains)
INFO - root - 2017-12-07 10:35:37.231886: step 1320, loss = 2.08, batch loss = 2.02 (39.6 examples/sec; 0.809 sec/batch; 18h:22m:51s remains)
INFO - root - 2017-12-07 10:35:45.076237: step 1330, loss = 2.07, batch loss = 2.01 (41.2 examples/sec; 0.777 sec/batch; 17h:39m:34s remains)
INFO - root - 2017-12-07 10:35:52.867220: step 1340, loss = 2.08, batch loss = 2.02 (40.2 examples/sec; 0.796 sec/batch; 18h:04m:46s remains)
INFO - root - 2017-12-07 10:36:00.718892: step 1350, loss = 2.07, batch loss = 2.01 (41.1 examples/sec; 0.779 sec/batch; 17h:41m:53s remains)
INFO - root - 2017-12-07 10:36:08.541555: step 1360, loss = 2.07, batch loss = 2.01 (41.2 examples/sec; 0.776 sec/batch; 17h:37m:42s remains)
INFO - root - 2017-12-07 10:36:16.373751: step 1370, loss = 2.08, batch loss = 2.02 (41.1 examples/sec; 0.779 sec/batch; 17h:40m:52s remains)
INFO - root - 2017-12-07 10:36:24.231607: step 1380, loss = 2.08, batch loss = 2.02 (39.7 examples/sec; 0.806 sec/batch; 18h:17m:49s remains)
INFO - root - 2017-12-07 10:36:31.984381: step 1390, loss = 2.08, batch loss = 2.02 (41.7 examples/sec; 0.768 sec/batch; 17h:26m:15s remains)
INFO - root - 2017-12-07 10:36:39.868223: step 1400, loss = 2.06, batch loss = 2.01 (40.8 examples/sec; 0.785 sec/batch; 17h:49m:01s remains)
2017-12-07 10:36:40.578230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3300505 -4.323761 -4.3117604 -4.3048787 -4.3016477 -4.3021359 -4.306571 -4.3114471 -4.3146057 -4.31908 -4.3214879 -4.3177309 -4.3115253 -4.3064418 -4.300652][-4.3290329 -4.3195014 -4.306417 -4.2985349 -4.2948275 -4.2969246 -4.3037691 -4.3120508 -4.3192196 -4.3250122 -4.3258934 -4.3238149 -4.3194718 -4.3138967 -4.307056][-4.2973342 -4.2884774 -4.2797432 -4.2736878 -4.2665324 -4.2633929 -4.2662172 -4.2719817 -4.281692 -4.2907724 -4.2949495 -4.2991509 -4.3020372 -4.3031335 -4.3016357][-4.2364068 -4.233211 -4.2350793 -4.2356071 -4.2265286 -4.2154951 -4.2079954 -4.2025952 -4.2079577 -4.2197104 -4.2301097 -4.2420406 -4.2533493 -4.2648573 -4.2745461][-4.1716661 -4.176415 -4.1891141 -4.1950097 -4.1806145 -4.16106 -4.1425047 -4.1183224 -4.1104269 -4.1242676 -4.14543 -4.1668644 -4.1857133 -4.2084689 -4.230598][-4.122148 -4.1321731 -4.1508589 -4.1561131 -4.1317215 -4.0965643 -4.0583634 -4.0062666 -3.9795346 -4.004209 -4.048727 -4.0895615 -4.122035 -4.1552539 -4.184371][-4.0920548 -4.0994339 -4.1178093 -4.1236629 -4.0974884 -4.0530624 -3.9982526 -3.91731 -3.8606749 -3.8900099 -3.9564288 -4.0182996 -4.0697794 -4.1143932 -4.1468053][-4.0978184 -4.095386 -4.1044631 -4.1094337 -4.0962014 -4.0700889 -4.03213 -3.9606898 -3.8921304 -3.9003606 -3.9536295 -4.0093374 -4.0628018 -4.1072149 -4.1337037][-4.1248069 -4.11861 -4.1176276 -4.1204085 -4.1210017 -4.1183939 -4.1080956 -4.0721135 -4.0256305 -4.0206585 -4.0458174 -4.0795937 -4.118557 -4.1492014 -4.162097][-4.1535954 -4.1509743 -4.1488094 -4.1502318 -4.1567965 -4.1673512 -4.1758904 -4.1669917 -4.1444659 -4.1380649 -4.1437125 -4.1598487 -4.1865315 -4.2075148 -4.2151356][-4.1758528 -4.18015 -4.180974 -4.1811976 -4.1880903 -4.2023578 -4.2202907 -4.2281079 -4.2216191 -4.2153349 -4.2114167 -4.2189703 -4.239665 -4.2572765 -4.2639322][-4.1828132 -4.1888747 -4.1922517 -4.1940908 -4.2021575 -4.2219329 -4.2466154 -4.2633967 -4.2687321 -4.2678022 -4.2638693 -4.2691474 -4.2859 -4.2999206 -4.3042579][-4.1785917 -4.1769657 -4.1802287 -4.1892519 -4.2062511 -4.2352495 -4.2668519 -4.289772 -4.3021164 -4.3070917 -4.30746 -4.3140841 -4.3264089 -4.3342643 -4.3341174][-4.1802135 -4.1615734 -4.1551752 -4.166317 -4.1913924 -4.2277966 -4.2644105 -4.2942424 -4.314023 -4.3243575 -4.3302546 -4.3392982 -4.34856 -4.3508768 -4.3454719][-4.192503 -4.1539822 -4.1326385 -4.138382 -4.1639338 -4.2026119 -4.2447267 -4.2825232 -4.3079877 -4.3209062 -4.3290691 -4.3377404 -4.3446627 -4.3438931 -4.3349447]]...]
INFO - root - 2017-12-07 10:36:48.349826: step 1410, loss = 2.07, batch loss = 2.01 (41.4 examples/sec; 0.774 sec/batch; 17h:33m:37s remains)
INFO - root - 2017-12-07 10:36:56.128309: step 1420, loss = 2.08, batch loss = 2.02 (41.6 examples/sec; 0.769 sec/batch; 17h:27m:42s remains)
INFO - root - 2017-12-07 10:37:03.925250: step 1430, loss = 2.06, batch loss = 2.00 (41.5 examples/sec; 0.772 sec/batch; 17h:31m:07s remains)
INFO - root - 2017-12-07 10:37:11.756441: step 1440, loss = 2.08, batch loss = 2.02 (41.7 examples/sec; 0.767 sec/batch; 17h:24m:10s remains)
INFO - root - 2017-12-07 10:37:19.588294: step 1450, loss = 2.08, batch loss = 2.02 (39.6 examples/sec; 0.807 sec/batch; 18h:18m:51s remains)
INFO - root - 2017-12-07 10:37:27.427491: step 1460, loss = 2.08, batch loss = 2.02 (40.3 examples/sec; 0.793 sec/batch; 17h:59m:54s remains)
INFO - root - 2017-12-07 10:37:35.222548: step 1470, loss = 2.07, batch loss = 2.01 (40.4 examples/sec; 0.793 sec/batch; 17h:58m:32s remains)
INFO - root - 2017-12-07 10:37:43.168297: step 1480, loss = 2.07, batch loss = 2.01 (40.9 examples/sec; 0.782 sec/batch; 17h:44m:17s remains)
INFO - root - 2017-12-07 10:37:50.956080: step 1490, loss = 2.07, batch loss = 2.01 (41.5 examples/sec; 0.770 sec/batch; 17h:28m:04s remains)
INFO - root - 2017-12-07 10:37:58.755579: step 1500, loss = 2.09, batch loss = 2.03 (40.4 examples/sec; 0.792 sec/batch; 17h:57m:19s remains)
2017-12-07 10:37:59.510952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2302179 -4.2452979 -4.2568612 -4.261724 -4.2582059 -4.2504044 -4.2427492 -4.235889 -4.2319613 -4.2318096 -4.232563 -4.2308178 -4.2248106 -4.2206383 -4.2250195][-4.2623444 -4.2734771 -4.2816019 -4.2842765 -4.2804637 -4.2731972 -4.267015 -4.2622409 -4.2606425 -4.2625585 -4.2649903 -4.2638345 -4.2569323 -4.251101 -4.2514648][-4.2966132 -4.3056841 -4.3132482 -4.315825 -4.3131618 -4.3074169 -4.3023262 -4.298481 -4.2984314 -4.3024449 -4.3071351 -4.3073254 -4.3010788 -4.2938967 -4.2889118][-4.3081255 -4.3160968 -4.3239722 -4.3282218 -4.3284454 -4.3257442 -4.3226762 -4.3192496 -4.3201151 -4.3260379 -4.3334937 -4.3356538 -4.3308253 -4.3227262 -4.31371][-4.2868447 -4.2909989 -4.2967806 -4.301939 -4.3049726 -4.3062863 -4.3074393 -4.308908 -4.3142524 -4.3231139 -4.3330684 -4.3380585 -4.3359327 -4.3296771 -4.3202353][-4.241437 -4.2357903 -4.2339978 -4.2355986 -4.2394772 -4.2446036 -4.2515178 -4.2612429 -4.2746878 -4.2892017 -4.303134 -4.3125844 -4.3158584 -4.3141708 -4.3087397][-4.1867843 -4.1660624 -4.1506834 -4.1424642 -4.1426458 -4.1497407 -4.1628065 -4.1817069 -4.20492 -4.229003 -4.2528658 -4.272047 -4.2854786 -4.2928247 -4.2947636][-4.1556592 -4.1189718 -4.0852928 -4.0601792 -4.049809 -4.0554748 -4.073801 -4.1017561 -4.1353951 -4.1708956 -4.2082167 -4.24067 -4.2667089 -4.2837944 -4.2919211][-4.1773348 -4.1365604 -4.0937247 -4.0588212 -4.0404515 -4.0446591 -4.0670729 -4.0994306 -4.1354103 -4.1728835 -4.2130589 -4.2474594 -4.2747359 -4.2929807 -4.3009648][-4.2304831 -4.199481 -4.1644487 -4.136271 -4.1224885 -4.1280203 -4.148243 -4.1745443 -4.2028332 -4.23201 -4.2617173 -4.2852335 -4.3036304 -4.3153553 -4.3177834][-4.2897215 -4.272119 -4.2514262 -4.2356687 -4.230022 -4.2362337 -4.2491431 -4.2631183 -4.27863 -4.2957544 -4.3120985 -4.3233771 -4.3314619 -4.3347487 -4.330636][-4.3350868 -4.3291807 -4.3219242 -4.31753 -4.3169627 -4.3194118 -4.3219476 -4.3235297 -4.3269758 -4.3328919 -4.3378072 -4.3397064 -4.3408194 -4.3392529 -4.3330669][-4.3587813 -4.3592143 -4.3587151 -4.3592415 -4.35949 -4.3574243 -4.3523016 -4.3459263 -4.341393 -4.33934 -4.3369203 -4.3343396 -4.3344421 -4.3336086 -4.3296371][-4.3654809 -4.367969 -4.3695602 -4.3711233 -4.3700819 -4.3653107 -4.3563089 -4.3454018 -4.3351979 -4.3271985 -4.32029 -4.3173642 -4.3211102 -4.3250165 -4.3250732][-4.3646364 -4.3674822 -4.3692317 -4.3700862 -4.3673348 -4.3610182 -4.3509297 -4.3388586 -4.3257484 -4.3144417 -4.3058114 -4.3043876 -4.3119307 -4.31981 -4.3227758]]...]
INFO - root - 2017-12-07 10:38:07.339497: step 1510, loss = 2.08, batch loss = 2.02 (41.4 examples/sec; 0.774 sec/batch; 17h:32m:31s remains)
INFO - root - 2017-12-07 10:38:15.122474: step 1520, loss = 2.08, batch loss = 2.03 (41.6 examples/sec; 0.769 sec/batch; 17h:26m:25s remains)
INFO - root - 2017-12-07 10:38:22.962901: step 1530, loss = 2.08, batch loss = 2.02 (40.2 examples/sec; 0.797 sec/batch; 18h:03m:37s remains)
INFO - root - 2017-12-07 10:38:30.753056: step 1540, loss = 2.07, batch loss = 2.01 (41.4 examples/sec; 0.773 sec/batch; 17h:30m:52s remains)
INFO - root - 2017-12-07 10:38:38.552725: step 1550, loss = 2.07, batch loss = 2.01 (40.9 examples/sec; 0.782 sec/batch; 17h:43m:24s remains)
INFO - root - 2017-12-07 10:38:46.307862: step 1560, loss = 2.07, batch loss = 2.02 (41.1 examples/sec; 0.779 sec/batch; 17h:39m:05s remains)
INFO - root - 2017-12-07 10:38:54.192049: step 1570, loss = 2.08, batch loss = 2.02 (40.4 examples/sec; 0.792 sec/batch; 17h:56m:27s remains)
INFO - root - 2017-12-07 10:39:02.080683: step 1580, loss = 2.08, batch loss = 2.02 (40.6 examples/sec; 0.789 sec/batch; 17h:52m:31s remains)
INFO - root - 2017-12-07 10:39:09.970536: step 1590, loss = 2.07, batch loss = 2.01 (40.3 examples/sec; 0.793 sec/batch; 17h:57m:55s remains)
INFO - root - 2017-12-07 10:39:17.799866: step 1600, loss = 2.07, batch loss = 2.01 (40.1 examples/sec; 0.798 sec/batch; 18h:04m:44s remains)
2017-12-07 10:39:18.443233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2239962 -4.2276392 -4.2247262 -4.2123661 -4.2019725 -4.191298 -4.1874671 -4.1885076 -4.1944609 -4.202415 -4.2102828 -4.2192888 -4.2190919 -4.203824 -4.1873727][-4.2304821 -4.2238607 -4.2142434 -4.1975284 -4.1849766 -4.1774039 -4.177134 -4.1821113 -4.1901374 -4.2024012 -4.2134094 -4.2223773 -4.2247982 -4.2127218 -4.1982284][-4.2233524 -4.2146826 -4.204246 -4.1854725 -4.1684985 -4.1551604 -4.1495318 -4.1559238 -4.17045 -4.191689 -4.205369 -4.2096968 -4.2107072 -4.202548 -4.1882095][-4.2120509 -4.2044153 -4.1976714 -4.1793666 -4.154747 -4.1297355 -4.1151233 -4.1246119 -4.1504383 -4.1801143 -4.1934209 -4.1873126 -4.1770096 -4.1700883 -4.160604][-4.2019343 -4.193717 -4.1900892 -4.17427 -4.1452117 -4.1110311 -4.0891743 -4.10074 -4.1326308 -4.1643863 -4.1729684 -4.1568227 -4.1349177 -4.1262546 -4.1221948][-4.1816154 -4.1743 -4.173224 -4.1605749 -4.1302447 -4.0925627 -4.0660906 -4.0789986 -4.1131721 -4.1418076 -4.1479096 -4.128449 -4.1034145 -4.0971446 -4.1058154][-4.1576262 -4.1490164 -4.1468649 -4.1374426 -4.1091204 -4.0689025 -4.0363946 -4.0471725 -4.082057 -4.11018 -4.1231818 -4.1211438 -4.1150064 -4.1215615 -4.1388903][-4.1379194 -4.1255341 -4.1209526 -4.1162434 -4.0905213 -4.0512891 -4.0161371 -4.0216274 -4.0556893 -4.0855246 -4.1076822 -4.1268063 -4.147747 -4.1729145 -4.1951718][-4.1368794 -4.12073 -4.1111507 -4.1092319 -4.090991 -4.0567961 -4.024507 -4.0242376 -4.0518637 -4.0781589 -4.1040864 -4.1362653 -4.177351 -4.2166657 -4.23905][-4.1549792 -4.1329875 -4.1187625 -4.1198711 -4.1154208 -4.0954723 -4.0716786 -4.0659089 -4.0820918 -4.1006408 -4.1264629 -4.1615667 -4.203815 -4.2406006 -4.2538748][-4.1804833 -4.1548405 -4.137846 -4.138176 -4.1435595 -4.1398396 -4.127058 -4.1197495 -4.1247954 -4.1363058 -4.1568937 -4.1810007 -4.2081041 -4.2320046 -4.2344642][-4.211988 -4.1874723 -4.1685786 -4.1622486 -4.1650872 -4.1699967 -4.1678944 -4.1644964 -4.1682749 -4.1767859 -4.1869955 -4.193903 -4.1990852 -4.207324 -4.1988978][-4.2377577 -4.2167645 -4.1993995 -4.18873 -4.1863055 -4.1939116 -4.1981945 -4.20025 -4.2065215 -4.21309 -4.210897 -4.2013702 -4.1880932 -4.1820025 -4.1683488][-4.2586088 -4.2396474 -4.2239547 -4.2109914 -4.2050724 -4.2105575 -4.2165875 -4.2240124 -4.2335649 -4.2388034 -4.228477 -4.2087994 -4.1896019 -4.1820211 -4.169292][-4.2780333 -4.2607 -4.247036 -4.2351561 -4.2295389 -4.23361 -4.2380271 -4.2440777 -4.251565 -4.2555304 -4.2428904 -4.2228436 -4.2106314 -4.2096038 -4.2029705]]...]
INFO - root - 2017-12-07 10:39:26.213519: step 1610, loss = 2.09, batch loss = 2.03 (42.1 examples/sec; 0.761 sec/batch; 17h:13m:35s remains)
INFO - root - 2017-12-07 10:39:33.901774: step 1620, loss = 2.07, batch loss = 2.01 (40.8 examples/sec; 0.785 sec/batch; 17h:46m:11s remains)
INFO - root - 2017-12-07 10:39:41.758182: step 1630, loss = 2.09, batch loss = 2.03 (39.9 examples/sec; 0.801 sec/batch; 18h:08m:35s remains)
INFO - root - 2017-12-07 10:39:49.568199: step 1640, loss = 2.07, batch loss = 2.02 (41.0 examples/sec; 0.780 sec/batch; 17h:39m:30s remains)
INFO - root - 2017-12-07 10:39:57.430429: step 1650, loss = 2.07, batch loss = 2.01 (40.0 examples/sec; 0.800 sec/batch; 18h:06m:53s remains)
INFO - root - 2017-12-07 10:40:05.204612: step 1660, loss = 2.07, batch loss = 2.01 (41.7 examples/sec; 0.768 sec/batch; 17h:22m:44s remains)
INFO - root - 2017-12-07 10:40:13.059041: step 1670, loss = 2.07, batch loss = 2.01 (40.5 examples/sec; 0.790 sec/batch; 17h:51m:49s remains)
INFO - root - 2017-12-07 10:40:20.888698: step 1680, loss = 2.07, batch loss = 2.01 (40.2 examples/sec; 0.796 sec/batch; 18h:00m:47s remains)
INFO - root - 2017-12-07 10:40:28.679337: step 1690, loss = 2.07, batch loss = 2.01 (41.5 examples/sec; 0.771 sec/batch; 17h:26m:51s remains)
INFO - root - 2017-12-07 10:40:36.481130: step 1700, loss = 2.08, batch loss = 2.02 (41.3 examples/sec; 0.775 sec/batch; 17h:31m:25s remains)
2017-12-07 10:40:37.211791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3169966 -4.3138356 -4.3099275 -4.304709 -4.2978816 -4.2910924 -4.2865787 -4.2870154 -4.2930956 -4.3032446 -4.3119326 -4.315722 -4.3186407 -4.3228793 -4.3270698][-4.3127918 -4.309031 -4.3048511 -4.2972407 -4.286325 -4.2733274 -4.2630625 -4.2630734 -4.2727432 -4.2873592 -4.2980185 -4.301795 -4.3053937 -4.3118091 -4.3181734][-4.3060064 -4.2994571 -4.2910914 -4.276907 -4.2561088 -4.2330165 -4.2154231 -4.2152081 -4.2302608 -4.253047 -4.2707872 -4.2803178 -4.2874355 -4.2965021 -4.30611][-4.2982116 -4.2864757 -4.2722259 -4.24883 -4.2114224 -4.1704912 -4.1415272 -4.1446576 -4.1711373 -4.2082286 -4.2371063 -4.2531681 -4.264545 -4.2758927 -4.2897177][-4.2875843 -4.273663 -4.2571106 -4.2248774 -4.166821 -4.1005578 -4.0548353 -4.0641737 -4.1095734 -4.1647725 -4.2026529 -4.2244544 -4.2409315 -4.2552071 -4.2718229][-4.2747836 -4.2623348 -4.2463813 -4.2031283 -4.1192951 -4.0194135 -3.949049 -3.9714105 -4.0531268 -4.1330647 -4.1776323 -4.2008109 -4.2205033 -4.2382345 -4.2549529][-4.2662106 -4.2599807 -4.2495804 -4.1994729 -4.0903783 -3.9488072 -3.8409526 -3.8728626 -4.0000715 -4.1102705 -4.1636262 -4.1898074 -4.2127352 -4.2320104 -4.2469735][-4.2669244 -4.2738976 -4.2749095 -4.2269745 -4.1053867 -3.9311144 -3.7883425 -3.8202088 -3.9762979 -4.1052046 -4.1645207 -4.1925154 -4.2162066 -4.2346392 -4.2471466][-4.2603526 -4.2826014 -4.3021584 -4.2737088 -4.1671185 -4.0054545 -3.8791032 -3.9042375 -4.0361052 -4.1459627 -4.1944833 -4.2158761 -4.2327862 -4.247004 -4.2576232][-4.2261691 -4.2611547 -4.3005953 -4.2945795 -4.212472 -4.0857162 -3.9972229 -4.0188637 -4.117671 -4.2005911 -4.232954 -4.2400875 -4.247345 -4.2590795 -4.2735348][-4.176332 -4.2228394 -4.2789822 -4.2924843 -4.2383447 -4.1487923 -4.0920019 -4.1108928 -4.1821661 -4.2384329 -4.2509451 -4.2418308 -4.2422142 -4.2565842 -4.2780571][-4.1438847 -4.1967177 -4.2627587 -4.2936792 -4.2664957 -4.2104888 -4.1738062 -4.1872373 -4.2353878 -4.2688336 -4.2620935 -4.2418952 -4.2389536 -4.2547154 -4.2797551][-4.139935 -4.1920042 -4.2561293 -4.2944193 -4.2858858 -4.2539549 -4.2329521 -4.2430682 -4.272881 -4.2889452 -4.2722249 -4.2471085 -4.2428193 -4.260169 -4.2879057][-4.1685405 -4.2156219 -4.2687168 -4.3032289 -4.304512 -4.2873659 -4.276495 -4.2834153 -4.3012204 -4.306139 -4.2879519 -4.265543 -4.2612739 -4.2790737 -4.3060975][-4.2252965 -4.2598839 -4.29846 -4.3248153 -4.3295197 -4.3204808 -4.3114986 -4.3119192 -4.3206296 -4.3210306 -4.3085794 -4.2935581 -4.2908473 -4.3053303 -4.326354]]...]
INFO - root - 2017-12-07 10:40:45.077163: step 1710, loss = 2.09, batch loss = 2.03 (41.3 examples/sec; 0.774 sec/batch; 17h:30m:09s remains)
INFO - root - 2017-12-07 10:40:52.841556: step 1720, loss = 2.06, batch loss = 2.01 (40.0 examples/sec; 0.800 sec/batch; 18h:04m:57s remains)
INFO - root - 2017-12-07 10:41:00.665565: step 1730, loss = 2.07, batch loss = 2.01 (42.0 examples/sec; 0.761 sec/batch; 17h:12m:39s remains)
INFO - root - 2017-12-07 10:41:08.483446: step 1740, loss = 2.08, batch loss = 2.02 (40.4 examples/sec; 0.792 sec/batch; 17h:53m:52s remains)
INFO - root - 2017-12-07 10:41:16.285462: step 1750, loss = 2.07, batch loss = 2.01 (41.4 examples/sec; 0.772 sec/batch; 17h:27m:15s remains)
INFO - root - 2017-12-07 10:41:24.071465: step 1760, loss = 2.08, batch loss = 2.02 (40.5 examples/sec; 0.791 sec/batch; 17h:52m:19s remains)
INFO - root - 2017-12-07 10:41:31.878045: step 1770, loss = 2.07, batch loss = 2.01 (41.1 examples/sec; 0.779 sec/batch; 17h:36m:22s remains)
INFO - root - 2017-12-07 10:41:39.664134: step 1780, loss = 2.08, batch loss = 2.02 (42.3 examples/sec; 0.757 sec/batch; 17h:06m:25s remains)
INFO - root - 2017-12-07 10:41:47.465889: step 1790, loss = 2.06, batch loss = 2.00 (42.0 examples/sec; 0.763 sec/batch; 17h:13m:46s remains)
INFO - root - 2017-12-07 10:41:55.263457: step 1800, loss = 2.07, batch loss = 2.01 (41.2 examples/sec; 0.777 sec/batch; 17h:33m:05s remains)
2017-12-07 10:41:55.925132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3323441 -4.33828 -4.3409076 -4.3419361 -4.3418827 -4.3342857 -4.3295784 -4.3367939 -4.3475046 -4.35372 -4.355598 -4.3568678 -4.352591 -4.34306 -4.3339071][-4.3083563 -4.3121858 -4.3119311 -4.3098273 -4.3072724 -4.2946854 -4.2844782 -4.2924571 -4.3120208 -4.3281364 -4.3360934 -4.3400855 -4.3364611 -4.3252931 -4.3138208][-4.277051 -4.2766929 -4.2718859 -4.2632937 -4.2564826 -4.2396274 -4.22164 -4.2272463 -4.2549295 -4.2803984 -4.296144 -4.3061585 -4.3053951 -4.2945824 -4.2835097][-4.2424645 -4.2341642 -4.2236362 -4.2092476 -4.2004094 -4.1832323 -4.1591506 -4.1606617 -4.1913028 -4.224875 -4.2493215 -4.2630992 -4.259563 -4.2495484 -4.2407913][-4.2175603 -4.1991878 -4.1780972 -4.1535196 -4.1389894 -4.1210818 -4.0924582 -4.0898976 -4.1248107 -4.1704235 -4.2084241 -4.2236805 -4.2154746 -4.2052655 -4.200336][-4.2142158 -4.1873488 -4.1548252 -4.1172242 -4.0870185 -4.0534391 -4.0081968 -3.9950936 -4.0381212 -4.1016078 -4.1602926 -4.1843266 -4.1780977 -4.1712642 -4.171751][-4.2255588 -4.1989465 -4.1614962 -4.115315 -4.0674872 -4.0064468 -3.9312663 -3.9017031 -3.9566505 -4.0348892 -4.1040154 -4.1348171 -4.1316376 -4.130095 -4.1395092][-4.2336297 -4.2131376 -4.1829495 -4.1406641 -4.0870652 -4.0096784 -3.9205339 -3.8857689 -3.9428177 -4.0130095 -4.0667548 -4.093555 -4.0895004 -4.0855746 -4.0975471][-4.2296515 -4.2131596 -4.1941767 -4.1633267 -4.1148925 -4.0455413 -3.9738889 -3.9529679 -3.9962475 -4.0390925 -4.0624933 -4.0766296 -4.0713816 -4.0612373 -4.0640178][-4.2160435 -4.2025027 -4.1918354 -4.1753674 -4.1415009 -4.092556 -4.0484219 -4.0379581 -4.0577283 -4.0701361 -4.0698276 -4.0744743 -4.0691137 -4.0559096 -4.050755][-4.19553 -4.1845675 -4.1804123 -4.1791596 -4.1664443 -4.140449 -4.1158476 -4.1085677 -4.1100774 -4.1011076 -4.0871882 -4.0838504 -4.0745878 -4.0621295 -4.0549917][-4.1631985 -4.1486034 -4.1486082 -4.16406 -4.1758575 -4.171627 -4.1589975 -4.1504183 -4.1419911 -4.1243267 -4.1046739 -4.09485 -4.0835371 -4.0730863 -4.0653524][-4.1356897 -4.11445 -4.1185069 -4.1500387 -4.1793523 -4.1902447 -4.1831307 -4.16859 -4.1546235 -4.1366696 -4.1175356 -4.1060681 -4.0946603 -4.0838113 -4.075201][-4.1338511 -4.1125164 -4.1229138 -4.1611753 -4.1953883 -4.2094011 -4.2028227 -4.1835856 -4.1649604 -4.1477981 -4.1314678 -4.1180043 -4.1037235 -4.0875964 -4.0786524][-4.1603003 -4.1445208 -4.1575274 -4.1899614 -4.2149658 -4.2236257 -4.2183924 -4.1998892 -4.1792793 -4.1634903 -4.150454 -4.1364722 -4.1171961 -4.0996647 -4.0948081]]...]
INFO - root - 2017-12-07 10:42:03.685179: step 1810, loss = 2.06, batch loss = 2.00 (41.2 examples/sec; 0.776 sec/batch; 17h:31m:26s remains)
INFO - root - 2017-12-07 10:42:11.501060: step 1820, loss = 2.08, batch loss = 2.02 (41.5 examples/sec; 0.770 sec/batch; 17h:24m:03s remains)
INFO - root - 2017-12-07 10:42:19.248640: step 1830, loss = 2.07, batch loss = 2.01 (40.7 examples/sec; 0.786 sec/batch; 17h:45m:19s remains)
INFO - root - 2017-12-07 10:42:27.077550: step 1840, loss = 2.08, batch loss = 2.02 (40.4 examples/sec; 0.793 sec/batch; 17h:54m:20s remains)
INFO - root - 2017-12-07 10:42:34.807685: step 1850, loss = 2.08, batch loss = 2.02 (41.0 examples/sec; 0.780 sec/batch; 17h:36m:29s remains)
INFO - root - 2017-12-07 10:42:42.582057: step 1860, loss = 2.07, batch loss = 2.01 (41.4 examples/sec; 0.773 sec/batch; 17h:26m:27s remains)
INFO - root - 2017-12-07 10:42:50.344864: step 1870, loss = 2.08, batch loss = 2.02 (41.4 examples/sec; 0.773 sec/batch; 17h:27m:19s remains)
INFO - root - 2017-12-07 10:42:58.225896: step 1880, loss = 2.07, batch loss = 2.01 (40.5 examples/sec; 0.791 sec/batch; 17h:51m:05s remains)
INFO - root - 2017-12-07 10:43:06.036245: step 1890, loss = 2.07, batch loss = 2.01 (42.5 examples/sec; 0.753 sec/batch; 16h:59m:39s remains)
INFO - root - 2017-12-07 10:43:13.829244: step 1900, loss = 2.09, batch loss = 2.03 (41.3 examples/sec; 0.775 sec/batch; 17h:28m:58s remains)
2017-12-07 10:43:14.492699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2379842 -4.2327986 -4.24092 -4.2533474 -4.2676077 -4.2802367 -4.2831855 -4.2821417 -4.27601 -4.266695 -4.2746749 -4.2987533 -4.3118811 -4.3121777 -4.2926221][-4.2524447 -4.24569 -4.2490993 -4.258709 -4.2758842 -4.2906008 -4.2936339 -4.2901087 -4.2850289 -4.2761297 -4.2805696 -4.3043737 -4.323523 -4.3307385 -4.3158689][-4.2599072 -4.2521524 -4.2518787 -4.2565393 -4.2710471 -4.285408 -4.2926607 -4.2920074 -4.2919636 -4.2887168 -4.2892256 -4.3012419 -4.3171144 -4.3268766 -4.3180895][-4.2510967 -4.2507052 -4.2458854 -4.2370405 -4.2389421 -4.2461777 -4.2538323 -4.2615266 -4.2734914 -4.2826643 -4.2886152 -4.2943125 -4.303328 -4.3131332 -4.31105][-4.2064428 -4.2238607 -4.2267971 -4.2084289 -4.1897764 -4.1786747 -4.1822429 -4.2000275 -4.2226152 -4.2447228 -4.2611985 -4.2725472 -4.283886 -4.3018951 -4.3083086][-4.1380472 -4.1747742 -4.1959867 -4.1825638 -4.1406603 -4.0994592 -4.0942545 -4.1277952 -4.1612687 -4.1910229 -4.2199335 -4.2422342 -4.259634 -4.2830606 -4.2984653][-4.0969052 -4.1362152 -4.16835 -4.1586242 -4.1083384 -4.0440836 -4.0311832 -4.0784564 -4.1203513 -4.1527457 -4.1910858 -4.2228518 -4.2494607 -4.274662 -4.2924652][-4.1018629 -4.1256633 -4.1625223 -4.1650443 -4.1306973 -4.0659957 -4.0419264 -4.0848303 -4.1217184 -4.1459222 -4.1820827 -4.2154207 -4.2458992 -4.2721009 -4.2881069][-4.1419616 -4.1394897 -4.1693587 -4.1905389 -4.1881614 -4.1488709 -4.122766 -4.1462126 -4.1616416 -4.1667719 -4.1918483 -4.2189603 -4.244401 -4.2667885 -4.2792974][-4.1831822 -4.1621222 -4.1764588 -4.2017293 -4.2312727 -4.22918 -4.2139244 -4.2237368 -4.2231803 -4.2109165 -4.2181244 -4.2339816 -4.252202 -4.2668214 -4.2731309][-4.1873136 -4.1626048 -4.172081 -4.202713 -4.2502513 -4.2762337 -4.2750387 -4.2821589 -4.2798004 -4.2657404 -4.2607851 -4.2623749 -4.267313 -4.2718663 -4.2701488][-4.1737757 -4.1454082 -4.1492286 -4.1811671 -4.2385063 -4.2808447 -4.2936258 -4.3059173 -4.3117886 -4.3075137 -4.3022804 -4.2999716 -4.296773 -4.2894988 -4.2759819][-4.1719494 -4.1415377 -4.1380315 -4.16592 -4.2210178 -4.2722178 -4.2960963 -4.31207 -4.3250294 -4.3300028 -4.3276038 -4.3237524 -4.3157969 -4.3017788 -4.2826271][-4.1867285 -4.1580186 -4.1446838 -4.1592789 -4.1996136 -4.2464781 -4.272562 -4.2913976 -4.3122759 -4.3270125 -4.3307981 -4.3291287 -4.3223948 -4.3093476 -4.2892146][-4.2162395 -4.1935353 -4.1725855 -4.1647158 -4.1813192 -4.2108526 -4.2330704 -4.2551374 -4.2818017 -4.3038783 -4.3158932 -4.3217926 -4.322577 -4.3165064 -4.298903]]...]
INFO - root - 2017-12-07 10:43:22.283503: step 1910, loss = 2.08, batch loss = 2.02 (41.0 examples/sec; 0.780 sec/batch; 17h:35m:34s remains)
INFO - root - 2017-12-07 10:43:30.047636: step 1920, loss = 2.06, batch loss = 2.00 (40.9 examples/sec; 0.783 sec/batch; 17h:39m:48s remains)
INFO - root - 2017-12-07 10:43:37.930917: step 1930, loss = 2.09, batch loss = 2.03 (40.7 examples/sec; 0.786 sec/batch; 17h:44m:01s remains)
INFO - root - 2017-12-07 10:43:45.753393: step 1940, loss = 2.08, batch loss = 2.02 (40.8 examples/sec; 0.783 sec/batch; 17h:40m:07s remains)
INFO - root - 2017-12-07 10:43:53.625144: step 1950, loss = 2.06, batch loss = 2.00 (40.5 examples/sec; 0.790 sec/batch; 17h:48m:37s remains)
INFO - root - 2017-12-07 10:44:01.322255: step 1960, loss = 2.08, batch loss = 2.02 (41.6 examples/sec; 0.770 sec/batch; 17h:21m:47s remains)
INFO - root - 2017-12-07 10:44:09.193921: step 1970, loss = 2.07, batch loss = 2.01 (40.9 examples/sec; 0.782 sec/batch; 17h:37m:27s remains)
INFO - root - 2017-12-07 10:44:16.926694: step 1980, loss = 2.08, batch loss = 2.02 (42.6 examples/sec; 0.752 sec/batch; 16h:57m:05s remains)
INFO - root - 2017-12-07 10:44:24.739275: step 1990, loss = 2.07, batch loss = 2.01 (41.7 examples/sec; 0.767 sec/batch; 17h:16m:38s remains)
INFO - root - 2017-12-07 10:44:32.516761: step 2000, loss = 2.07, batch loss = 2.01 (42.3 examples/sec; 0.757 sec/batch; 17h:03m:10s remains)
2017-12-07 10:44:33.260766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3099689 -4.3078136 -4.3042936 -4.3006258 -4.2980423 -4.2940764 -4.2903476 -4.2885556 -4.2922692 -4.299964 -4.3074713 -4.3120055 -4.3168674 -4.32201 -4.3268523][-4.3058186 -4.3040133 -4.3009138 -4.2960396 -4.2912955 -4.2818093 -4.2721634 -4.2677436 -4.2728 -4.2828112 -4.2925563 -4.2978444 -4.30485 -4.3111868 -4.3177433][-4.2989311 -4.2937779 -4.28581 -4.2752409 -4.2622356 -4.241869 -4.2241454 -4.2201257 -4.230731 -4.24921 -4.2666526 -4.2773004 -4.287746 -4.2962012 -4.3049417][-4.2865806 -4.2742491 -4.2587171 -4.2376385 -4.2088871 -4.1732826 -4.1442809 -4.1421456 -4.1629548 -4.1971254 -4.2265005 -4.244494 -4.2578773 -4.2690711 -4.283][-4.2770452 -4.2612271 -4.2400374 -4.2051921 -4.1554995 -4.0980182 -4.0544243 -4.0573072 -4.0949783 -4.145648 -4.1829538 -4.2058911 -4.2215376 -4.2370667 -4.2567711][-4.2667165 -4.2510972 -4.2262816 -4.1762452 -4.1012793 -4.0145683 -3.9514904 -3.9689932 -4.0395303 -4.1090446 -4.1504593 -4.1746626 -4.1931109 -4.2122455 -4.2337184][-4.2614508 -4.251472 -4.2283473 -4.16548 -4.0624533 -3.9367464 -3.8440781 -3.8818207 -3.9974589 -4.0915089 -4.1397767 -4.1648812 -4.1844163 -4.2037153 -4.2216673][-4.2661176 -4.2717195 -4.2577567 -4.19277 -4.0715528 -3.9138329 -3.7914641 -3.8370812 -3.9816492 -4.0935831 -4.1487179 -4.1767039 -4.19863 -4.2165885 -4.2295609][-4.2720742 -4.2939563 -4.2977147 -4.2477026 -4.135891 -3.9874218 -3.8806257 -3.9163117 -4.0401492 -4.1389151 -4.1879683 -4.2109232 -4.229073 -4.2417932 -4.2501674][-4.2546916 -4.2906966 -4.3155718 -4.2886848 -4.2004895 -4.0834141 -4.0089741 -4.0345907 -4.1226315 -4.1967587 -4.2301445 -4.2409372 -4.2510729 -4.2606468 -4.2686548][-4.2190475 -4.26521 -4.308465 -4.3017178 -4.2401986 -4.1567688 -4.1077056 -4.12602 -4.1855469 -4.2352777 -4.2483764 -4.2427297 -4.2465081 -4.2597189 -4.27346][-4.1877389 -4.2381706 -4.29474 -4.3090315 -4.2762766 -4.2245111 -4.1923923 -4.2002711 -4.2338672 -4.2585983 -4.2507119 -4.2331767 -4.2349176 -4.2525597 -4.2723823][-4.1747861 -4.2227321 -4.2791362 -4.3076124 -4.2962136 -4.2670155 -4.2466345 -4.2496157 -4.2698941 -4.278275 -4.2578382 -4.2345405 -4.234446 -4.2532563 -4.2770457][-4.1868997 -4.2274351 -4.2731895 -4.3039765 -4.3045764 -4.2899566 -4.2792621 -4.2806706 -4.2915907 -4.2895074 -4.2659836 -4.2448497 -4.2465339 -4.2672868 -4.2932248][-4.2266464 -4.2582865 -4.2904382 -4.3140831 -4.3187766 -4.3114939 -4.3052359 -4.30409 -4.3077807 -4.3031821 -4.2857885 -4.270607 -4.272687 -4.2903838 -4.3122821]]...]
INFO - root - 2017-12-07 10:44:41.168508: step 2010, loss = 2.08, batch loss = 2.02 (40.7 examples/sec; 0.786 sec/batch; 17h:42m:42s remains)
INFO - root - 2017-12-07 10:44:48.994326: step 2020, loss = 2.07, batch loss = 2.02 (40.8 examples/sec; 0.784 sec/batch; 17h:40m:16s remains)
INFO - root - 2017-12-07 10:44:56.960123: step 2030, loss = 2.07, batch loss = 2.01 (39.3 examples/sec; 0.813 sec/batch; 18h:19m:20s remains)
INFO - root - 2017-12-07 10:45:04.808182: step 2040, loss = 2.08, batch loss = 2.02 (40.8 examples/sec; 0.784 sec/batch; 17h:40m:00s remains)
INFO - root - 2017-12-07 10:45:12.596719: step 2050, loss = 2.08, batch loss = 2.02 (40.9 examples/sec; 0.781 sec/batch; 17h:35m:57s remains)
INFO - root - 2017-12-07 10:45:20.336408: step 2060, loss = 2.08, batch loss = 2.02 (43.0 examples/sec; 0.744 sec/batch; 16h:44m:41s remains)
INFO - root - 2017-12-07 10:45:28.149279: step 2070, loss = 2.07, batch loss = 2.01 (41.4 examples/sec; 0.773 sec/batch; 17h:24m:22s remains)
INFO - root - 2017-12-07 10:45:36.022132: step 2080, loss = 2.07, batch loss = 2.01 (40.5 examples/sec; 0.790 sec/batch; 17h:47m:09s remains)
INFO - root - 2017-12-07 10:45:43.844769: step 2090, loss = 2.08, batch loss = 2.02 (41.8 examples/sec; 0.765 sec/batch; 17h:12m:55s remains)
INFO - root - 2017-12-07 10:45:51.632541: step 2100, loss = 2.08, batch loss = 2.02 (41.0 examples/sec; 0.780 sec/batch; 17h:33m:20s remains)
2017-12-07 10:45:52.327703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3283882 -4.3320012 -4.3358631 -4.3391004 -4.3416905 -4.3422608 -4.3411384 -4.3381114 -4.333673 -4.3257642 -4.312604 -4.2979665 -4.2788243 -4.2397842 -4.1927772][-4.3300095 -4.3329449 -4.336607 -4.3411603 -4.3458862 -4.3482547 -4.3484745 -4.346489 -4.341135 -4.3303366 -4.3161654 -4.3023324 -4.2812338 -4.2348 -4.1779923][-4.3211522 -4.3231411 -4.3259912 -4.3305278 -4.3373947 -4.3429656 -4.3469162 -4.3491578 -4.348186 -4.3398046 -4.3290558 -4.3199368 -4.3007951 -4.2575746 -4.2012296][-4.2975407 -4.2956791 -4.2952743 -4.2967582 -4.3031697 -4.3108449 -4.31869 -4.3274517 -4.334631 -4.3346939 -4.3320136 -4.3295221 -4.3165755 -4.2852421 -4.2400036][-4.2633867 -4.255249 -4.2488775 -4.2454467 -4.2485571 -4.2559104 -4.2659388 -4.2799892 -4.2949934 -4.3039875 -4.3092585 -4.3130865 -4.3107162 -4.2951055 -4.2662182][-4.229876 -4.2133927 -4.1986403 -4.187573 -4.1836925 -4.18688 -4.1964211 -4.2132745 -4.2344913 -4.2518444 -4.2657671 -4.2782331 -4.2877722 -4.2859435 -4.2721405][-4.2087231 -4.1803417 -4.1498971 -4.121151 -4.1001883 -4.0884972 -4.0870514 -4.101069 -4.1291428 -4.1601319 -4.1909528 -4.22026 -4.2464461 -4.2603149 -4.261497][-4.2062454 -4.1665249 -4.1193886 -4.0677056 -4.0188751 -3.9804127 -3.954401 -3.9533777 -3.9853325 -4.0368586 -4.0944176 -4.1479836 -4.1927805 -4.2220721 -4.2365956][-4.2240252 -4.1794267 -4.123734 -4.0586815 -3.9931529 -3.9368417 -3.8918519 -3.8738656 -3.901418 -3.9634509 -4.0387 -4.1085262 -4.1628866 -4.1987557 -4.219768][-4.2545977 -4.2145858 -4.1660914 -4.1100974 -4.0563841 -4.0113926 -3.9755106 -3.9619017 -3.9833529 -4.032095 -4.0910645 -4.1453242 -4.1868773 -4.2133832 -4.2283068][-4.2798591 -4.2527733 -4.2234416 -4.1911097 -4.1603894 -4.1331191 -4.1116257 -4.1067071 -4.1219378 -4.1492434 -4.1824193 -4.2117281 -4.2319603 -4.2422895 -4.2461977][-4.2871513 -4.2747297 -4.2638474 -4.25102 -4.2388182 -4.2244363 -4.2115297 -4.2092886 -4.2160134 -4.2264481 -4.2414212 -4.2544785 -4.2603707 -4.2598104 -4.2569065][-4.2864995 -4.2817192 -4.2793989 -4.2766743 -4.2737918 -4.2673712 -4.25977 -4.259129 -4.2595506 -4.2601633 -4.2657018 -4.2725968 -4.2735085 -4.2685938 -4.2640262][-4.2952342 -4.2930503 -4.2939854 -4.2945633 -4.2939873 -4.2898293 -4.2851524 -4.2853265 -4.28245 -4.278779 -4.2787671 -4.2828155 -4.2809968 -4.270741 -4.2603955][-4.3068776 -4.3060765 -4.3084493 -4.3095174 -4.3087845 -4.3068051 -4.3057313 -4.3065839 -4.3007112 -4.2931166 -4.287993 -4.2874155 -4.2796278 -4.2596817 -4.2375641]]...]
INFO - root - 2017-12-07 10:46:00.166098: step 2110, loss = 2.08, batch loss = 2.02 (39.8 examples/sec; 0.804 sec/batch; 18h:06m:04s remains)
INFO - root - 2017-12-07 10:46:08.052988: step 2120, loss = 2.09, batch loss = 2.03 (39.4 examples/sec; 0.811 sec/batch; 18h:15m:14s remains)
INFO - root - 2017-12-07 10:46:15.949442: step 2130, loss = 2.08, batch loss = 2.02 (40.8 examples/sec; 0.784 sec/batch; 17h:37m:53s remains)
INFO - root - 2017-12-07 10:46:23.752689: step 2140, loss = 2.08, batch loss = 2.02 (41.2 examples/sec; 0.776 sec/batch; 17h:27m:54s remains)
INFO - root - 2017-12-07 10:46:31.495001: step 2150, loss = 2.07, batch loss = 2.01 (42.8 examples/sec; 0.748 sec/batch; 16h:49m:09s remains)
INFO - root - 2017-12-07 10:46:39.322672: step 2160, loss = 2.07, batch loss = 2.01 (41.4 examples/sec; 0.772 sec/batch; 17h:22m:04s remains)
INFO - root - 2017-12-07 10:46:47.139862: step 2170, loss = 2.08, batch loss = 2.02 (39.6 examples/sec; 0.809 sec/batch; 18h:10m:55s remains)
INFO - root - 2017-12-07 10:46:54.987731: step 2180, loss = 2.08, batch loss = 2.02 (41.7 examples/sec; 0.768 sec/batch; 17h:15m:50s remains)
INFO - root - 2017-12-07 10:47:02.713525: step 2190, loss = 2.09, batch loss = 2.03 (42.4 examples/sec; 0.755 sec/batch; 16h:58m:41s remains)
INFO - root - 2017-12-07 10:47:10.491215: step 2200, loss = 2.07, batch loss = 2.01 (41.0 examples/sec; 0.781 sec/batch; 17h:32m:49s remains)
2017-12-07 10:47:11.150933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3390193 -4.3404541 -4.339004 -4.3388195 -4.3390832 -4.3352542 -4.3306952 -4.3264384 -4.322031 -4.3159666 -4.304039 -4.2914572 -4.2819891 -4.2736726 -4.2707758][-4.3369384 -4.3359008 -4.3318872 -4.3281245 -4.3261528 -4.3199487 -4.3138585 -4.3075495 -4.3001261 -4.2902331 -4.2700295 -4.2463565 -4.2328863 -4.225709 -4.2267752][-4.3264356 -4.3226323 -4.3171425 -4.311574 -4.3075509 -4.3001442 -4.2908826 -4.2789817 -4.2641878 -4.2469683 -4.218441 -4.1856122 -4.1660171 -4.1609955 -4.1713777][-4.3058228 -4.3006377 -4.2960873 -4.2915621 -4.2880006 -4.278501 -4.260982 -4.240871 -4.2224126 -4.2031393 -4.1746016 -4.1408782 -4.1170282 -4.1145916 -4.1344433][-4.280169 -4.2717638 -4.2653165 -4.2555251 -4.241961 -4.2201996 -4.1895466 -4.1669688 -4.1560259 -4.1530414 -4.1467581 -4.1332512 -4.1195321 -4.1236629 -4.1448746][-4.2521229 -4.2352381 -4.2183986 -4.194891 -4.1616473 -4.1144576 -4.0600238 -4.0341072 -4.043797 -4.0752172 -4.1105919 -4.1317844 -4.137044 -4.1493998 -4.1712046][-4.2135553 -4.1871033 -4.158638 -4.1234117 -4.0728927 -4.0015354 -3.9261122 -3.90606 -3.9573684 -4.0316324 -4.0978961 -4.1399689 -4.1589503 -4.1802683 -4.2061124][-4.1875515 -4.1626182 -4.1373739 -4.1065235 -4.0621471 -4.003089 -3.9491203 -3.9538438 -4.01962 -4.0905242 -4.1434226 -4.1751022 -4.1894536 -4.2104392 -4.2347212][-4.1953597 -4.1780424 -4.16644 -4.1487136 -4.1235166 -4.0965896 -4.0723586 -4.0816193 -4.12352 -4.1650877 -4.193212 -4.2086163 -4.2174659 -4.2367377 -4.2594771][-4.2134662 -4.2068596 -4.2060623 -4.1960511 -4.1804738 -4.1729207 -4.1643023 -4.1666846 -4.1878471 -4.2103887 -4.225275 -4.2338486 -4.2422423 -4.2620988 -4.2833481][-4.2085018 -4.2112556 -4.2175593 -4.215867 -4.2098985 -4.2098827 -4.2115822 -4.2143412 -4.2284684 -4.2432442 -4.2557125 -4.2664657 -4.277307 -4.2920313 -4.3053322][-4.1790895 -4.1929245 -4.2074256 -4.2174931 -4.2199135 -4.2222424 -4.2277503 -4.2336316 -4.2489247 -4.2658052 -4.281044 -4.295073 -4.3064618 -4.316659 -4.32256][-4.1464243 -4.1701617 -4.1914444 -4.2120395 -4.224308 -4.2344127 -4.2447529 -4.253531 -4.2700462 -4.288146 -4.3033171 -4.3167448 -4.3247175 -4.3276024 -4.3256469][-4.130322 -4.1542072 -4.1759248 -4.2049069 -4.2290096 -4.2504482 -4.2669883 -4.2785053 -4.2942758 -4.3099604 -4.3213449 -4.3302073 -4.3316588 -4.3270521 -4.3198795][-4.1548157 -4.1752543 -4.19222 -4.2198086 -4.2469187 -4.271215 -4.2894487 -4.2996831 -4.3120027 -4.3225107 -4.3282337 -4.331089 -4.3277745 -4.3200512 -4.3130741]]...]
INFO - root - 2017-12-07 10:47:18.969526: step 2210, loss = 2.09, batch loss = 2.03 (40.5 examples/sec; 0.789 sec/batch; 17h:44m:25s remains)
INFO - root - 2017-12-07 10:47:26.827970: step 2220, loss = 2.07, batch loss = 2.02 (40.4 examples/sec; 0.792 sec/batch; 17h:48m:15s remains)
INFO - root - 2017-12-07 10:47:34.676465: step 2230, loss = 2.08, batch loss = 2.02 (41.0 examples/sec; 0.781 sec/batch; 17h:33m:33s remains)
INFO - root - 2017-12-07 10:47:42.489628: step 2240, loss = 2.08, batch loss = 2.02 (41.4 examples/sec; 0.773 sec/batch; 17h:22m:29s remains)
INFO - root - 2017-12-07 10:47:50.330268: step 2250, loss = 2.07, batch loss = 2.01 (40.6 examples/sec; 0.788 sec/batch; 17h:41m:59s remains)
INFO - root - 2017-12-07 10:47:58.179982: step 2260, loss = 2.07, batch loss = 2.02 (41.3 examples/sec; 0.775 sec/batch; 17h:24m:43s remains)
INFO - root - 2017-12-07 10:48:06.058097: step 2270, loss = 2.07, batch loss = 2.01 (40.1 examples/sec; 0.797 sec/batch; 17h:54m:06s remains)
INFO - root - 2017-12-07 10:48:13.891730: step 2280, loss = 2.07, batch loss = 2.01 (40.0 examples/sec; 0.801 sec/batch; 17h:59m:11s remains)
INFO - root - 2017-12-07 10:48:21.651686: step 2290, loss = 2.09, batch loss = 2.03 (40.7 examples/sec; 0.787 sec/batch; 17h:40m:23s remains)
INFO - root - 2017-12-07 10:48:29.614855: step 2300, loss = 2.06, batch loss = 2.00 (41.0 examples/sec; 0.781 sec/batch; 17h:32m:12s remains)
2017-12-07 10:48:30.329131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.222753 -4.1975675 -4.1824946 -4.1838293 -4.1823449 -4.180172 -4.1753411 -4.172472 -4.1796112 -4.2008262 -4.2206244 -4.2365274 -4.2470984 -4.242043 -4.2194815][-4.2225757 -4.1972036 -4.1806593 -4.18103 -4.1787214 -4.1723623 -4.1600938 -4.1503353 -4.1538572 -4.1784391 -4.20367 -4.2248745 -4.2400408 -4.2420473 -4.2292442][-4.2245111 -4.1999922 -4.182107 -4.177278 -4.1707506 -4.160758 -4.14535 -4.1346617 -4.1367259 -4.158318 -4.179338 -4.1952386 -4.2081585 -4.215497 -4.2109489][-4.2239118 -4.1982837 -4.1757522 -4.162519 -4.1465344 -4.129354 -4.1176596 -4.1139355 -4.1201749 -4.140192 -4.1535468 -4.1538286 -4.1564674 -4.1658816 -4.16936][-4.2157106 -4.1885509 -4.1592283 -4.1352282 -4.1023035 -4.0696635 -4.0585346 -4.0725374 -4.0965791 -4.1216974 -4.1276407 -4.110414 -4.099472 -4.1122179 -4.1285629][-4.19659 -4.1704698 -4.1385579 -4.1046538 -4.0478411 -3.9872217 -3.9676301 -4.00195 -4.0513463 -4.0908084 -4.0976214 -4.0689821 -4.051126 -4.0756588 -4.1098995][-4.1868362 -4.1646376 -4.1338954 -4.0953622 -4.0248528 -3.942934 -3.9185791 -3.9673235 -4.0305386 -4.0742993 -4.0799456 -4.0452685 -4.0317044 -4.0741272 -4.1279254][-4.2004552 -4.185298 -4.1629548 -4.1317573 -4.0726457 -4.0059609 -3.9976482 -4.0434966 -4.0893259 -4.1154208 -4.1091352 -4.0716157 -4.0599489 -4.1062665 -4.1699486][-4.2109213 -4.2022185 -4.1922793 -4.1768942 -4.1401944 -4.1027579 -4.1139054 -4.1511073 -4.1729112 -4.1768236 -4.1548724 -4.1172404 -4.1056371 -4.1485405 -4.2104154][-4.2014832 -4.198071 -4.2042975 -4.2097917 -4.1962771 -4.1786633 -4.1969085 -4.2190485 -4.2179151 -4.201787 -4.1706305 -4.1373148 -4.13025 -4.1672249 -4.2210526][-4.193243 -4.1853895 -4.201251 -4.2268038 -4.2325969 -4.2248325 -4.2367697 -4.2420483 -4.2256751 -4.1956019 -4.1619425 -4.1349421 -4.1324792 -4.1677394 -4.2157073][-4.1938691 -4.1760192 -4.1890745 -4.224102 -4.2430363 -4.240098 -4.2403502 -4.2341361 -4.2113218 -4.1797481 -4.1512127 -4.1325788 -4.1361647 -4.1727347 -4.2157512][-4.192184 -4.1604538 -4.1623173 -4.1927686 -4.2218795 -4.2277632 -4.2250414 -4.2152262 -4.1939836 -4.1742349 -4.1563945 -4.1422744 -4.1441622 -4.1766605 -4.2152553][-4.1770773 -4.1336851 -4.1230588 -4.1470947 -4.1815743 -4.198463 -4.1996117 -4.1909761 -4.1783624 -4.17181 -4.1639266 -4.1530724 -4.1488409 -4.1771669 -4.2152014][-4.1783438 -4.1291308 -4.1095505 -4.1204863 -4.1501064 -4.1721764 -4.1778026 -4.1731315 -4.16799 -4.1695967 -4.1686573 -4.16221 -4.1552134 -4.1792774 -4.2157178]]...]
INFO - root - 2017-12-07 10:48:38.243950: step 2310, loss = 2.08, batch loss = 2.02 (40.2 examples/sec; 0.796 sec/batch; 17h:52m:31s remains)
INFO - root - 2017-12-07 10:48:46.314519: step 2320, loss = 2.08, batch loss = 2.02 (41.9 examples/sec; 0.763 sec/batch; 17h:07m:37s remains)
INFO - root - 2017-12-07 10:48:59.703238: step 2330, loss = 2.07, batch loss = 2.01 (22.5 examples/sec; 1.421 sec/batch; 31h:53m:36s remains)
INFO - root - 2017-12-07 10:49:14.166314: step 2340, loss = 2.07, batch loss = 2.01 (21.8 examples/sec; 1.470 sec/batch; 32h:59m:00s remains)
INFO - root - 2017-12-07 10:49:28.805614: step 2350, loss = 2.07, batch loss = 2.01 (22.0 examples/sec; 1.456 sec/batch; 32h:40m:36s remains)
INFO - root - 2017-12-07 10:49:43.008032: step 2360, loss = 2.07, batch loss = 2.02 (22.0 examples/sec; 1.456 sec/batch; 32h:39m:54s remains)
INFO - root - 2017-12-07 10:49:57.444760: step 2370, loss = 2.07, batch loss = 2.01 (21.6 examples/sec; 1.482 sec/batch; 33h:13m:59s remains)
INFO - root - 2017-12-07 10:50:11.928547: step 2380, loss = 2.08, batch loss = 2.02 (22.4 examples/sec; 1.427 sec/batch; 32h:00m:55s remains)
INFO - root - 2017-12-07 10:50:26.208815: step 2390, loss = 2.06, batch loss = 2.01 (21.9 examples/sec; 1.458 sec/batch; 32h:41m:48s remains)
INFO - root - 2017-12-07 10:50:40.696688: step 2400, loss = 2.09, batch loss = 2.03 (22.8 examples/sec; 1.402 sec/batch; 31h:25m:39s remains)
2017-12-07 10:50:41.888603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28757 -4.2760234 -4.2769589 -4.2823348 -4.2829084 -4.2775917 -4.2776275 -4.2778769 -4.2784519 -4.27604 -4.2737379 -4.2769694 -4.2902193 -4.3111286 -4.3260241][-4.2696028 -4.2537575 -4.2557225 -4.2676597 -4.2674065 -4.25737 -4.2559524 -4.2589917 -4.2651 -4.2638712 -4.2595563 -4.26169 -4.2731843 -4.2966957 -4.31693][-4.2445235 -4.225852 -4.2258186 -4.2403193 -4.2303367 -4.2047768 -4.1915574 -4.1958327 -4.2173409 -4.2287555 -4.227849 -4.2335563 -4.248024 -4.27522 -4.3000793][-4.205018 -4.1820369 -4.1811585 -4.1963716 -4.1786709 -4.1374865 -4.1085181 -4.117888 -4.1598625 -4.1872196 -4.19609 -4.2087736 -4.2303824 -4.2608113 -4.2865057][-4.1713581 -4.1418624 -4.1393394 -4.1527181 -4.1261916 -4.0637188 -4.0098667 -4.0308056 -4.1118374 -4.158916 -4.177494 -4.1977882 -4.2279663 -4.2620354 -4.2847281][-4.157815 -4.122046 -4.1158066 -4.1187682 -4.0752192 -3.9815559 -3.8901181 -3.9271779 -4.0488553 -4.1134167 -4.1446714 -4.1771908 -4.2189732 -4.2605724 -4.2838612][-4.1757989 -4.1397209 -4.1218247 -4.0937977 -4.0128956 -3.8610244 -3.7039702 -3.7568812 -3.9132676 -4.0044894 -4.0647316 -4.1214185 -4.1823869 -4.2382474 -4.2724366][-4.206686 -4.1689816 -4.1280694 -4.0615721 -3.9379873 -3.7236309 -3.491951 -3.5701947 -3.7720203 -3.8979371 -3.9912677 -4.0730944 -4.1464968 -4.2094364 -4.2548094][-4.2482557 -4.2174921 -4.1712747 -4.1014862 -3.9872968 -3.7970529 -3.5975666 -3.6624413 -3.8284538 -3.9381254 -4.0202823 -4.0929942 -4.1568522 -4.2120585 -4.2553968][-4.2981467 -4.2775869 -4.2368484 -4.179913 -4.0903816 -3.9536676 -3.8168802 -3.840625 -3.9416032 -4.021101 -4.0801525 -4.1385827 -4.1890688 -4.231616 -4.2689943][-4.3299704 -4.3147035 -4.2801957 -4.2344804 -4.1639647 -4.0647526 -3.9747117 -3.9849749 -4.0488391 -4.106462 -4.1513767 -4.199306 -4.236896 -4.2683449 -4.2950916][-4.3400965 -4.3292074 -4.3029981 -4.2733107 -4.2277527 -4.16231 -4.1106067 -4.1218786 -4.1616526 -4.1985683 -4.2310977 -4.2660904 -4.2880511 -4.3096623 -4.3281741][-4.3491473 -4.34249 -4.325346 -4.309597 -4.2834506 -4.2420187 -4.2113991 -4.2203655 -4.2464914 -4.2731318 -4.2950573 -4.3166423 -4.3258123 -4.3380046 -4.3469315][-4.3454418 -4.3427453 -4.3348002 -4.3325086 -4.3203025 -4.2957749 -4.2766509 -4.2793112 -4.2978287 -4.3185439 -4.3311048 -4.3398705 -4.3420372 -4.3475866 -4.3512406][-4.3337855 -4.3308845 -4.3261695 -4.3283238 -4.3236704 -4.311914 -4.3008342 -4.3027024 -4.3165417 -4.3325424 -4.3402214 -4.3438025 -4.3455658 -4.3480453 -4.3496919]]...]
INFO - root - 2017-12-07 10:50:56.385357: step 2410, loss = 2.08, batch loss = 2.02 (22.6 examples/sec; 1.416 sec/batch; 31h:44m:12s remains)
INFO - root - 2017-12-07 10:51:10.922530: step 2420, loss = 2.07, batch loss = 2.01 (23.2 examples/sec; 1.377 sec/batch; 30h:51m:42s remains)
INFO - root - 2017-12-07 10:51:30.769066: step 2430, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.107 sec/batch; 47h:14m:14s remains)
INFO - root - 2017-12-07 10:51:51.921403: step 2440, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 47h:29m:55s remains)
INFO - root - 2017-12-07 10:52:13.123410: step 2450, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.105 sec/batch; 47h:09m:45s remains)
INFO - root - 2017-12-07 10:52:34.082030: step 2460, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.140 sec/batch; 47h:56m:25s remains)
INFO - root - 2017-12-07 10:52:55.213300: step 2470, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.146 sec/batch; 48h:04m:22s remains)
INFO - root - 2017-12-07 10:53:16.513440: step 2480, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.138 sec/batch; 47h:53m:52s remains)
INFO - root - 2017-12-07 10:53:37.464478: step 2490, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 47h:41m:41s remains)
INFO - root - 2017-12-07 10:53:58.626866: step 2500, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.104 sec/batch; 47h:06m:59s remains)
2017-12-07 10:54:00.228973: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2319179 -4.2156391 -4.2145519 -4.2235661 -4.2253542 -4.2273226 -4.2353606 -4.2428937 -4.2550449 -4.260726 -4.2625232 -4.2649903 -4.2676063 -4.2684126 -4.2666774][-4.2297935 -4.2194881 -4.2237678 -4.2376246 -4.2441683 -4.2460704 -4.2515707 -4.2572908 -4.2696266 -4.2745171 -4.2759156 -4.2795095 -4.283401 -4.2844052 -4.2810044][-4.2320285 -4.2276378 -4.2366982 -4.254972 -4.2660003 -4.2683311 -4.2695613 -4.2720323 -4.2833967 -4.2861309 -4.2859716 -4.2890749 -4.2932873 -4.2949142 -4.2908454][-4.2400231 -4.2396035 -4.2515516 -4.2683649 -4.2759004 -4.2738752 -4.2687516 -4.2666817 -4.2750039 -4.2751808 -4.27397 -4.2756214 -4.278522 -4.28093 -4.2774568][-4.2471681 -4.2483835 -4.2617426 -4.2723727 -4.2658935 -4.2476754 -4.2234621 -4.20673 -4.212316 -4.2139053 -4.2177153 -4.2218885 -4.2259293 -4.2305269 -4.2275882][-4.2460408 -4.2431526 -4.2500358 -4.2447877 -4.2131014 -4.1656618 -4.1067853 -4.0674982 -4.0810666 -4.1030607 -4.1286097 -4.149704 -4.1639977 -4.1765513 -4.17815][-4.2336841 -4.2174172 -4.2047677 -4.1731744 -4.1086283 -4.0242624 -3.9252055 -3.8693841 -3.9140716 -3.9796481 -4.0419888 -4.0883236 -4.1167254 -4.1402621 -4.1503339][-4.2269497 -4.205441 -4.1863627 -4.1481609 -4.0767851 -3.9878578 -3.8921709 -3.8467317 -3.9094932 -3.9912808 -4.0610533 -4.1082616 -4.1340437 -4.1548705 -4.1607223][-4.2334375 -4.2180848 -4.2103333 -4.193121 -4.1494675 -4.0948324 -4.0407062 -4.0170307 -4.0582132 -4.1097174 -4.1491013 -4.1755443 -4.1886063 -4.2015009 -4.2013845][-4.2383947 -4.2279277 -4.2291269 -4.2285972 -4.2090397 -4.181026 -4.1570907 -4.1470728 -4.1695223 -4.1932111 -4.2076678 -4.2147837 -4.2155347 -4.2179713 -4.213048][-4.2337375 -4.2212319 -4.2236671 -4.23132 -4.2268982 -4.2159123 -4.2127657 -4.2149439 -4.2280912 -4.234777 -4.231792 -4.2271781 -4.2189016 -4.2120795 -4.1993537][-4.2311654 -4.21764 -4.220933 -4.2321076 -4.2344155 -4.231916 -4.2395258 -4.247952 -4.2586641 -4.2597222 -4.2516932 -4.2428031 -4.2307415 -4.2173843 -4.2004733][-4.2344875 -4.2230577 -4.2283926 -4.2410336 -4.2482052 -4.24909 -4.2561588 -4.262526 -4.2718787 -4.2717023 -4.2643547 -4.2563796 -4.2472868 -4.2378097 -4.226037][-4.2391181 -4.2274461 -4.2319 -4.2438488 -4.251595 -4.2536578 -4.2614918 -4.2692528 -4.2804642 -4.2821045 -4.2772026 -4.2721515 -4.2681828 -4.2644587 -4.2587676][-4.2376895 -4.2223263 -4.2254033 -4.2380934 -4.24723 -4.2522969 -4.2622766 -4.2734561 -4.2887111 -4.2937489 -4.2931213 -4.2923617 -4.2928429 -4.2934608 -4.2910933]]...]
INFO - root - 2017-12-07 10:54:21.339559: step 2510, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.074 sec/batch; 46h:26m:02s remains)
INFO - root - 2017-12-07 10:54:42.408064: step 2520, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.072 sec/batch; 46h:23m:44s remains)
INFO - root - 2017-12-07 10:55:03.502593: step 2530, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 47h:35m:30s remains)
INFO - root - 2017-12-07 10:55:24.854744: step 2540, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 47h:47m:59s remains)
INFO - root - 2017-12-07 10:55:45.979316: step 2550, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.146 sec/batch; 48h:01m:47s remains)
INFO - root - 2017-12-07 10:56:06.988112: step 2560, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.154 sec/batch; 48h:12m:55s remains)
INFO - root - 2017-12-07 10:56:28.170919: step 2570, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 47h:33m:59s remains)
INFO - root - 2017-12-07 10:56:49.559374: step 2580, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.104 sec/batch; 47h:04m:46s remains)
INFO - root - 2017-12-07 10:57:10.337323: step 2590, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.147 sec/batch; 48h:01m:34s remains)
INFO - root - 2017-12-07 10:57:31.658793: step 2600, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 47h:36m:59s remains)
2017-12-07 10:57:33.132311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3458734 -4.3367171 -4.3230329 -4.3097143 -4.2965655 -4.2987084 -4.3117523 -4.3275657 -4.3332772 -4.3276782 -4.3170362 -4.3098788 -4.3073511 -4.3117485 -4.3230705][-4.3426361 -4.3299756 -4.3151441 -4.2969227 -4.2757425 -4.26932 -4.2742553 -4.2863245 -4.2908969 -4.2811365 -4.2656546 -4.2572279 -4.2542243 -4.2667565 -4.2920723][-4.3371053 -4.321332 -4.3042936 -4.2812333 -4.2525029 -4.236361 -4.2279773 -4.2314444 -4.23509 -4.2230039 -4.2077289 -4.2034264 -4.2004037 -4.2194848 -4.2609372][-4.3309755 -4.3093762 -4.2856812 -4.2547421 -4.2164745 -4.1891971 -4.1675425 -4.1633539 -4.1733885 -4.1697593 -4.1610074 -4.1659913 -4.1628661 -4.1841063 -4.2376065][-4.3239484 -4.2959251 -4.2603245 -4.2152824 -4.1596723 -4.1146774 -4.0730381 -4.058917 -4.088263 -4.1113615 -4.1186032 -4.1343427 -4.1342468 -4.156074 -4.2163391][-4.3180804 -4.2835116 -4.232408 -4.1659756 -4.08531 -4.0089912 -3.9348722 -3.9072449 -3.9682112 -4.0347729 -4.07118 -4.1059208 -4.1162844 -4.138371 -4.1964021][-4.3116527 -4.2687492 -4.2003722 -4.1088119 -3.9948277 -3.8802121 -3.7674088 -3.7265396 -3.8306777 -3.9506383 -4.0203404 -4.0733433 -4.0941987 -4.1167173 -4.1721992][-4.3026557 -4.2526674 -4.1730723 -4.0672717 -3.9408298 -3.8144581 -3.6935446 -3.6583338 -3.7918818 -3.9348009 -4.0138273 -4.0662169 -4.083859 -4.1038289 -4.15841][-4.2979722 -4.2472639 -4.1677113 -4.0730562 -3.9769971 -3.8930216 -3.8265424 -3.8216245 -3.9290037 -4.0339994 -4.0885429 -4.1227121 -4.1279955 -4.1432562 -4.1891184][-4.3030958 -4.2579975 -4.1877837 -4.1156263 -4.0626392 -4.0282588 -4.0079856 -4.0188684 -4.0895195 -4.1531353 -4.1860914 -4.202477 -4.1987734 -4.2118125 -4.2481856][-4.3126478 -4.2770257 -4.2219467 -4.1728783 -4.1480927 -4.1401763 -4.1438489 -4.1583056 -4.2016234 -4.2391958 -4.259306 -4.2649 -4.2617044 -4.2757516 -4.3047233][-4.3179727 -4.2925696 -4.2557249 -4.2252564 -4.2153511 -4.2189732 -4.2307215 -4.24622 -4.2733645 -4.29788 -4.3095517 -4.3074026 -4.3040805 -4.3170495 -4.3401513][-4.3207664 -4.3036003 -4.280962 -4.2639384 -4.2617083 -4.2706437 -4.2850075 -4.2991824 -4.3156772 -4.3300419 -4.3361583 -4.3337712 -4.3320079 -4.3417959 -4.357317][-4.328187 -4.3152337 -4.3005738 -4.292563 -4.2951221 -4.3045444 -4.3150053 -4.3242259 -4.3345089 -4.34239 -4.3468366 -4.3466411 -4.3471112 -4.3528714 -4.3624053][-4.3413739 -4.3340168 -4.3258624 -4.321372 -4.3226748 -4.3272018 -4.33168 -4.3357735 -4.3400683 -4.3445377 -4.3481045 -4.3496208 -4.3519845 -4.3570533 -4.36381]]...]
INFO - root - 2017-12-07 10:57:54.416885: step 2610, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.165 sec/batch; 48h:24m:55s remains)
INFO - root - 2017-12-07 10:58:15.288306: step 2620, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.079 sec/batch; 46h:29m:45s remains)
INFO - root - 2017-12-07 10:58:36.521335: step 2630, loss = 2.05, batch loss = 1.99 (15.0 examples/sec; 2.132 sec/batch; 47h:40m:46s remains)
INFO - root - 2017-12-07 10:58:57.725069: step 2640, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 47h:35m:06s remains)
INFO - root - 2017-12-07 10:59:18.449217: step 2650, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 1.854 sec/batch; 41h:26m:38s remains)
INFO - root - 2017-12-07 10:59:39.585228: step 2660, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 47h:38m:33s remains)
INFO - root - 2017-12-07 11:00:00.932321: step 2670, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.150 sec/batch; 48h:03m:26s remains)
INFO - root - 2017-12-07 11:00:22.168975: step 2680, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.080 sec/batch; 46h:28m:47s remains)
INFO - root - 2017-12-07 11:00:43.062764: step 2690, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.093 sec/batch; 46h:46m:13s remains)
INFO - root - 2017-12-07 11:01:04.260232: step 2700, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.152 sec/batch; 48h:04m:58s remains)
2017-12-07 11:01:05.810802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3155336 -4.3114939 -4.3030491 -4.2941613 -4.2895036 -4.2862644 -4.2809548 -4.2757916 -4.2735524 -4.2698927 -4.2671957 -4.2527194 -4.2245345 -4.1960278 -4.204423][-4.315557 -4.3095956 -4.3027978 -4.3002429 -4.3004737 -4.2936893 -4.2802887 -4.2652617 -4.2546082 -4.2546425 -4.2602305 -4.2570853 -4.239254 -4.2178221 -4.2223825][-4.314508 -4.3074894 -4.3039117 -4.3065906 -4.3090944 -4.2981734 -4.2778625 -4.2521787 -4.2318416 -4.2332482 -4.2422309 -4.2483535 -4.2463222 -4.2401824 -4.2452431][-4.31247 -4.3052273 -4.3014064 -4.3040714 -4.3039184 -4.2891483 -4.2598205 -4.220521 -4.1972861 -4.2091293 -4.227191 -4.2399311 -4.251163 -4.25955 -4.2664495][-4.3061285 -4.298131 -4.2918358 -4.2909636 -4.2848482 -4.2638807 -4.2166438 -4.1547041 -4.1356578 -4.1729455 -4.2086077 -4.2296286 -4.2473631 -4.265841 -4.273519][-4.2972336 -4.286664 -4.2781696 -4.2704935 -4.2527661 -4.2160368 -4.13801 -4.0446606 -4.0423751 -4.124526 -4.1901603 -4.2224922 -4.2437506 -4.2662864 -4.2722483][-4.2886429 -4.2750025 -4.2656736 -4.2488685 -4.2150216 -4.15416 -4.0332527 -3.8999979 -3.9227602 -4.0570464 -4.1577411 -4.2078948 -4.2348061 -4.258707 -4.2643762][-4.2787304 -4.2625074 -4.2523065 -4.2306166 -4.1873302 -4.1041937 -3.9439206 -3.7790465 -3.8306623 -4.0016012 -4.1278105 -4.194335 -4.2294354 -4.2534027 -4.25645][-4.2639766 -4.2443657 -4.2343564 -4.2188473 -4.1824751 -4.1034045 -3.9507639 -3.8113155 -3.8645902 -4.017333 -4.131937 -4.1974888 -4.230813 -4.2514067 -4.2496862][-4.2486067 -4.2243581 -4.2134318 -4.2057533 -4.1863942 -4.1305141 -4.0186658 -3.9305034 -3.970567 -4.0783691 -4.163126 -4.211926 -4.2361431 -4.2505922 -4.2448835][-4.2280712 -4.1990814 -4.1842093 -4.1804824 -4.1769185 -4.146605 -4.0773182 -4.0368018 -4.0733972 -4.1497383 -4.2084575 -4.2378726 -4.2495551 -4.257031 -4.247499][-4.215385 -4.185843 -4.1659966 -4.1612287 -4.1701522 -4.162488 -4.1292992 -4.1215091 -4.1586289 -4.2168779 -4.2581892 -4.2715769 -4.2725649 -4.2727771 -4.2585764][-4.2298393 -4.2076039 -4.1895509 -4.1829343 -4.1938891 -4.1996932 -4.1895628 -4.1973147 -4.2307506 -4.2739954 -4.3008442 -4.3055277 -4.3022923 -4.2997961 -4.2819643][-4.2697926 -4.2560263 -4.2430453 -4.23648 -4.2467694 -4.2579269 -4.2568626 -4.2670283 -4.2927155 -4.3212771 -4.3341336 -4.3328571 -4.328135 -4.3244705 -4.3080235][-4.3121309 -4.306078 -4.2998691 -4.2958636 -4.3041534 -4.3142991 -4.3155613 -4.3224964 -4.3379292 -4.351244 -4.3527355 -4.3475842 -4.3434005 -4.3403058 -4.3305264]]...]
INFO - root - 2017-12-07 11:01:26.714678: step 2710, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 47h:00m:21s remains)
INFO - root - 2017-12-07 11:01:47.606612: step 2720, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.125 sec/batch; 47h:27m:02s remains)
INFO - root - 2017-12-07 11:02:08.853143: step 2730, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.182 sec/batch; 48h:43m:11s remains)
INFO - root - 2017-12-07 11:02:30.079952: step 2740, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 47h:38m:08s remains)
INFO - root - 2017-12-07 11:02:50.794771: step 2750, loss = 2.10, batch loss = 2.04 (15.2 examples/sec; 2.106 sec/batch; 47h:01m:06s remains)
INFO - root - 2017-12-07 11:03:11.981448: step 2760, loss = 2.06, batch loss = 2.01 (15.3 examples/sec; 2.094 sec/batch; 46h:44m:32s remains)
INFO - root - 2017-12-07 11:03:33.123984: step 2770, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.152 sec/batch; 48h:01m:39s remains)
INFO - root - 2017-12-07 11:03:54.036047: step 2780, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.183 sec/batch; 48h:42m:34s remains)
INFO - root - 2017-12-07 11:04:15.296944: step 2790, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.146 sec/batch; 47h:53m:56s remains)
INFO - root - 2017-12-07 11:04:36.463580: step 2800, loss = 2.07, batch loss = 2.02 (14.7 examples/sec; 2.172 sec/batch; 48h:27m:47s remains)
2017-12-07 11:04:38.083117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2367053 -4.2115149 -4.2121286 -4.2333326 -4.2518773 -4.2514935 -4.2389908 -4.2134933 -4.1911144 -4.1911325 -4.196949 -4.2003074 -4.202333 -4.2066383 -4.2134933][-4.2227836 -4.1939573 -4.1955667 -4.2211175 -4.2465353 -4.2463446 -4.226265 -4.1892791 -4.1582003 -4.1587172 -4.1725049 -4.185081 -4.1942348 -4.2017226 -4.2123809][-4.2077913 -4.1863737 -4.1902022 -4.215961 -4.240129 -4.2359686 -4.2088838 -4.16003 -4.1190639 -4.1200938 -4.1408181 -4.16431 -4.1846266 -4.1999679 -4.2174873][-4.2026587 -4.2015719 -4.2125978 -4.2326012 -4.2462749 -4.2349825 -4.1962848 -4.1380548 -4.0928187 -4.09487 -4.1143441 -4.1404123 -4.1702929 -4.1958427 -4.2210078][-4.2055578 -4.2235413 -4.2405839 -4.2517319 -4.249414 -4.2266893 -4.1744328 -4.1135712 -4.0825911 -4.0968256 -4.113349 -4.133461 -4.1652408 -4.1953387 -4.224669][-4.2037573 -4.2276645 -4.2477312 -4.2483644 -4.2236929 -4.1771712 -4.1066818 -4.0555449 -4.0624905 -4.1064172 -4.1364384 -4.1602983 -4.1856608 -4.2091374 -4.2340255][-4.1999326 -4.2161059 -4.2300596 -4.217483 -4.1756315 -4.0985394 -4.0018797 -3.9658284 -4.0243716 -4.1066756 -4.1601992 -4.1930242 -4.2117491 -4.2269154 -4.2370958][-4.1933789 -4.1982646 -4.2002187 -4.1764355 -4.1221576 -4.0178194 -3.8884065 -3.8628223 -3.9756756 -4.095715 -4.1710544 -4.2116566 -4.2258859 -4.2291327 -4.2230396][-4.1861358 -4.1836629 -4.1793242 -4.1570635 -4.1044645 -3.9970975 -3.8625329 -3.8481312 -3.9751272 -4.10134 -4.1783814 -4.2131448 -4.2126985 -4.2007046 -4.1803384][-4.1985455 -4.18839 -4.1857848 -4.1792316 -4.1484523 -4.0735011 -3.9827058 -3.9777491 -4.0685387 -4.1607733 -4.2158513 -4.2310085 -4.2098756 -4.1765213 -4.1410971][-4.2174654 -4.1891875 -4.1877728 -4.2005558 -4.1919689 -4.1486688 -4.1017623 -4.1096849 -4.1683488 -4.2272806 -4.2581844 -4.2554359 -4.2196112 -4.1705432 -4.1283212][-4.23952 -4.1958704 -4.1969028 -4.2202964 -4.2238941 -4.2084684 -4.1912365 -4.2058191 -4.2434282 -4.2775364 -4.2904773 -4.2781763 -4.243113 -4.1971064 -4.1601639][-4.2794929 -4.2408514 -4.2385049 -4.2532029 -4.2547145 -4.2498069 -4.2464304 -4.2620583 -4.2842808 -4.3041973 -4.3104539 -4.2968268 -4.2730627 -4.2448144 -4.2202039][-4.3068023 -4.28791 -4.2859392 -4.2872767 -4.2800007 -4.2712569 -4.2675862 -4.2808704 -4.2965326 -4.3072643 -4.3079863 -4.2978487 -4.287168 -4.27838 -4.2697544][-4.3023272 -4.3020115 -4.3067436 -4.3047495 -4.292841 -4.2821746 -4.2783704 -4.2879114 -4.2981782 -4.30219 -4.2996435 -4.2923059 -4.2901225 -4.2943606 -4.2967796]]...]
INFO - root - 2017-12-07 11:04:58.970511: step 2810, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.120 sec/batch; 47h:17m:28s remains)
INFO - root - 2017-12-07 11:05:20.034209: step 2820, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.110 sec/batch; 47h:03m:24s remains)
INFO - root - 2017-12-07 11:05:41.253836: step 2830, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.150 sec/batch; 47h:56m:47s remains)
INFO - root - 2017-12-07 11:06:02.224110: step 2840, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.134 sec/batch; 47h:35m:26s remains)
INFO - root - 2017-12-07 11:06:23.030389: step 2850, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.151 sec/batch; 47h:58m:08s remains)
INFO - root - 2017-12-07 11:06:44.430650: step 2860, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 47h:17m:51s remains)
INFO - root - 2017-12-07 11:07:05.557186: step 2870, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.096 sec/batch; 46h:43m:46s remains)
INFO - root - 2017-12-07 11:07:26.515404: step 2880, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.141 sec/batch; 47h:42m:47s remains)
INFO - root - 2017-12-07 11:07:47.744131: step 2890, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.191 sec/batch; 48h:49m:16s remains)
INFO - root - 2017-12-07 11:08:09.037920: step 2900, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 47h:22m:25s remains)
2017-12-07 11:08:10.575064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3372126 -4.3362832 -4.3354459 -4.3351603 -4.3343692 -4.3329787 -4.33301 -4.33326 -4.3334136 -4.3350878 -4.3376718 -4.3408418 -4.3434858 -4.3439379 -4.3378892][-4.3367906 -4.3381267 -4.3399029 -4.340014 -4.3373542 -4.3333206 -4.3312917 -4.3315253 -4.3341827 -4.3381982 -4.3412728 -4.3434687 -4.3446274 -4.3433204 -4.3347378][-4.3271537 -4.3288713 -4.3305788 -4.3288703 -4.321537 -4.3100281 -4.3019805 -4.3010993 -4.308682 -4.3206387 -4.3309374 -4.3376312 -4.3405132 -4.3385262 -4.3277774][-4.3155122 -4.316915 -4.3169641 -4.3098192 -4.2907753 -4.2624779 -4.2395692 -4.2329216 -4.2461753 -4.2725797 -4.2988424 -4.3188343 -4.3305435 -4.33247 -4.3218365][-4.3046775 -4.3044882 -4.3005724 -4.2840729 -4.2466269 -4.190361 -4.1398072 -4.1196203 -4.1394134 -4.1864371 -4.2367926 -4.2778263 -4.3054075 -4.3169103 -4.3098536][-4.2851367 -4.2829189 -4.2757225 -4.2503366 -4.194087 -4.1049314 -4.0163031 -3.9733181 -3.9986727 -4.0714588 -4.1496911 -4.2148447 -4.2626605 -4.2874427 -4.2862954][-4.2513447 -4.2521567 -4.2470627 -4.2212839 -4.1570187 -4.046433 -3.9252741 -3.8559971 -3.8799226 -3.9705012 -4.0697589 -4.152935 -4.218935 -4.2585955 -4.2667489][-4.221808 -4.230371 -4.2331076 -4.2168169 -4.1615028 -4.0559664 -3.9313693 -3.851891 -3.8651857 -3.9490767 -4.045898 -4.1306295 -4.2006226 -4.2462726 -4.2603874][-4.2206759 -4.2355938 -4.2444544 -4.2387352 -4.2003932 -4.1181207 -4.0161986 -3.9459333 -3.9458551 -4.0047226 -4.0786629 -4.1503968 -4.2132797 -4.2550254 -4.268559][-4.2399168 -4.2569585 -4.267416 -4.2677236 -4.2451549 -4.1908402 -4.1191931 -4.0645614 -4.0565333 -4.0907831 -4.1375976 -4.1893888 -4.2383046 -4.2704873 -4.2778416][-4.261271 -4.2752914 -4.2829828 -4.2850943 -4.2745848 -4.2447557 -4.2018161 -4.1649017 -4.155663 -4.1732049 -4.197372 -4.2276616 -4.2588592 -4.2780657 -4.2763963][-4.277739 -4.2862511 -4.289577 -4.2909617 -4.2879229 -4.2758331 -4.2576203 -4.2396092 -4.2339826 -4.2408366 -4.2480965 -4.2584438 -4.2716107 -4.2769952 -4.265698][-4.2926607 -4.29485 -4.2935982 -4.29182 -4.2908525 -4.2883048 -4.2853394 -4.2817359 -4.2811122 -4.2814531 -4.2776566 -4.27499 -4.275322 -4.2713027 -4.2532706][-4.30143 -4.3003507 -4.2970581 -4.2931561 -4.2906961 -4.2896852 -4.2919 -4.2954931 -4.299108 -4.2983751 -4.2906961 -4.2826209 -4.2756491 -4.2663097 -4.2463608][-4.3012471 -4.3005953 -4.2990704 -4.2960472 -4.2915249 -4.287518 -4.2877626 -4.2919121 -4.2969651 -4.2986765 -4.2946362 -4.2881665 -4.2804918 -4.2710342 -4.2534723]]...]
INFO - root - 2017-12-07 11:08:31.249566: step 2910, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.159 sec/batch; 48h:06m:14s remains)
INFO - root - 2017-12-07 11:08:52.575238: step 2920, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.107 sec/batch; 46h:56m:37s remains)
INFO - root - 2017-12-07 11:09:13.593828: step 2930, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 47h:16m:35s remains)
INFO - root - 2017-12-07 11:09:34.412941: step 2940, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 1.894 sec/batch; 42h:10m:47s remains)
INFO - root - 2017-12-07 11:09:55.417443: step 2950, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 47h:09m:14s remains)
INFO - root - 2017-12-07 11:10:16.558083: step 2960, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.168 sec/batch; 48h:17m:07s remains)
INFO - root - 2017-12-07 11:10:37.606260: step 2970, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.108 sec/batch; 46h:56m:46s remains)
INFO - root - 2017-12-07 11:10:58.741102: step 2980, loss = 2.06, batch loss = 2.01 (15.3 examples/sec; 2.097 sec/batch; 46h:40m:38s remains)
INFO - root - 2017-12-07 11:11:19.819071: step 2990, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.148 sec/batch; 47h:48m:13s remains)
INFO - root - 2017-12-07 11:11:40.954707: step 3000, loss = 2.08, batch loss = 2.02 (15.8 examples/sec; 2.028 sec/batch; 45h:08m:36s remains)
2017-12-07 11:11:42.503830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.266645 -4.2823939 -4.300787 -4.3101077 -4.303987 -4.2839603 -4.252388 -4.2132344 -4.1722622 -4.1464562 -4.1478319 -4.1733303 -4.2121105 -4.2472277 -4.2710619][-4.2551675 -4.2691 -4.2883945 -4.3034372 -4.3106742 -4.3098578 -4.29992 -4.2821746 -4.258307 -4.2358932 -4.229495 -4.2408867 -4.2648816 -4.285306 -4.2973309][-4.2610312 -4.263885 -4.2738552 -4.2873797 -4.3008027 -4.3121195 -4.3194375 -4.3194995 -4.31311 -4.3015265 -4.2939353 -4.2952437 -4.3059225 -4.3139791 -4.3146462][-4.2653689 -4.2524691 -4.2465611 -4.252646 -4.265574 -4.2801943 -4.2933855 -4.30089 -4.3063369 -4.3078423 -4.30692 -4.3087254 -4.3150597 -4.3172164 -4.3123593][-4.2557564 -4.224762 -4.2000852 -4.1933928 -4.2007842 -4.2117176 -4.2221713 -4.2305 -4.243609 -4.2588425 -4.2726092 -4.2867532 -4.2988358 -4.3016238 -4.2962871][-4.2436695 -4.1947317 -4.1504259 -4.1263924 -4.1224275 -4.1209722 -4.1192522 -4.1211638 -4.1398354 -4.1723332 -4.2074819 -4.2434478 -4.2714038 -4.2825875 -4.281363][-4.2414374 -4.1842723 -4.1256986 -4.0824037 -4.0580211 -4.0343089 -4.0096874 -3.9958014 -4.01987 -4.0741315 -4.136929 -4.1990218 -4.2471819 -4.2714872 -4.2795882][-4.2460637 -4.1981368 -4.1396289 -4.0803375 -4.0285025 -3.970603 -3.9122362 -3.880862 -3.9175723 -3.9974482 -4.08473 -4.1661272 -4.22808 -4.2648339 -4.2840052][-4.251966 -4.2265735 -4.1828265 -4.1214933 -4.0517879 -3.9671764 -3.8780394 -3.8297627 -3.8743658 -3.9672024 -4.063417 -4.1503878 -4.2129693 -4.2516112 -4.2777934][-4.249671 -4.24518 -4.2221851 -4.1777029 -4.1162343 -4.0372233 -3.95572 -3.9081643 -3.9359767 -4.0085893 -4.0881438 -4.1576719 -4.1991339 -4.2225108 -4.2441473][-4.2347484 -4.2424159 -4.2373796 -4.2168417 -4.1810412 -4.132194 -4.0816722 -4.0474072 -4.0570288 -4.1002426 -4.1494403 -4.1852012 -4.1927562 -4.1891694 -4.194366][-4.2184424 -4.2258539 -4.2288795 -4.2248497 -4.2125359 -4.1939912 -4.171103 -4.1523743 -4.1573424 -4.1845851 -4.2100239 -4.2175183 -4.1959782 -4.1678753 -4.1535568][-4.2105217 -4.2072878 -4.2075286 -4.2089763 -4.2120771 -4.2155838 -4.2117834 -4.20629 -4.2128291 -4.2334275 -4.2505755 -4.2486091 -4.2194376 -4.1833935 -4.1565843][-4.2244105 -4.2088556 -4.2014003 -4.2027979 -4.2144289 -4.2305036 -4.2398262 -4.2421775 -4.2491055 -4.26585 -4.2806091 -4.2791138 -4.2575207 -4.227459 -4.2002506][-4.257205 -4.2402482 -4.2324004 -4.2342157 -4.2463384 -4.2628469 -4.2746491 -4.2798738 -4.2852821 -4.295733 -4.305377 -4.3054576 -4.2934632 -4.2730126 -4.2488]]...]
INFO - root - 2017-12-07 11:12:03.557934: step 3010, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.152 sec/batch; 47h:52m:58s remains)
INFO - root - 2017-12-07 11:12:24.833763: step 3020, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 46h:47m:12s remains)
INFO - root - 2017-12-07 11:12:46.101468: step 3030, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 47h:00m:14s remains)
INFO - root - 2017-12-07 11:13:06.960837: step 3040, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.088 sec/batch; 46h:26m:45s remains)
INFO - root - 2017-12-07 11:13:28.252485: step 3050, loss = 2.09, batch loss = 2.03 (15.5 examples/sec; 2.069 sec/batch; 46h:01m:16s remains)
INFO - root - 2017-12-07 11:13:49.456640: step 3060, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.103 sec/batch; 46h:46m:06s remains)
INFO - root - 2017-12-07 11:14:10.534338: step 3070, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 46h:40m:23s remains)
INFO - root - 2017-12-07 11:14:31.711498: step 3080, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.086 sec/batch; 46h:22m:52s remains)
INFO - root - 2017-12-07 11:14:52.764098: step 3090, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.066 sec/batch; 45h:56m:22s remains)
INFO - root - 2017-12-07 11:15:13.791911: step 3100, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.092 sec/batch; 46h:30m:18s remains)
2017-12-07 11:15:15.344916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0525093 -4.0558066 -4.1581435 -4.2458124 -4.2812123 -4.252615 -4.2077312 -4.1902146 -4.2053494 -4.2355919 -4.272203 -4.305923 -4.3309131 -4.3392057 -4.3381748][-4.0169964 -4.0411997 -4.1627917 -4.2594757 -4.2923574 -4.2570114 -4.198853 -4.1725335 -4.1836224 -4.21239 -4.2580619 -4.30017 -4.328999 -4.3387642 -4.3380404][-4.0391951 -4.0838447 -4.202076 -4.2847719 -4.3015275 -4.2524004 -4.1784282 -4.1410112 -4.1454668 -4.1777949 -4.2381587 -4.2918653 -4.3260765 -4.3372812 -4.3379989][-4.1168113 -4.1672492 -4.2580404 -4.3106475 -4.303432 -4.2383251 -4.1478286 -4.1004019 -4.1042929 -4.1499095 -4.2265606 -4.2910333 -4.3268647 -4.3392091 -4.3406053][-4.21518 -4.2578816 -4.3115492 -4.3222685 -4.2806959 -4.190124 -4.0841689 -4.040175 -4.0677495 -4.1418219 -4.2357178 -4.3026052 -4.3335733 -4.3442454 -4.3446312][-4.3028789 -4.3288884 -4.3447695 -4.3132176 -4.2322598 -4.1150885 -4.0056343 -3.9861176 -4.0582762 -4.1619954 -4.2613769 -4.320435 -4.34369 -4.3490529 -4.3472996][-4.3641043 -4.3735118 -4.3549237 -4.2789917 -4.1548257 -4.015873 -3.9230566 -3.9541702 -4.0733128 -4.1928735 -4.2839074 -4.3328724 -4.3505907 -4.35237 -4.348876][-4.3972654 -4.3944535 -4.3436751 -4.2252283 -4.05745 -3.9013407 -3.8494344 -3.949677 -4.1059418 -4.2252259 -4.3021145 -4.3421941 -4.3573976 -4.3562965 -4.3500724][-4.4111404 -4.398634 -4.3223457 -4.1679277 -3.9634962 -3.8006196 -3.8098476 -3.9738152 -4.1419468 -4.2517514 -4.3150282 -4.34988 -4.3635545 -4.3597541 -4.3509641][-4.4089866 -4.3884721 -4.2999344 -4.1316929 -3.9164512 -3.7720642 -3.8340712 -4.0181751 -4.1714153 -4.269259 -4.3236141 -4.3549328 -4.3661695 -4.3602686 -4.3510633][-4.3978257 -4.3742929 -4.288959 -4.132061 -3.941627 -3.8426254 -3.9226367 -4.0829029 -4.208662 -4.288239 -4.3319869 -4.354239 -4.3619127 -4.3564172 -4.3492479][-4.3819633 -4.3595452 -4.2876921 -4.1596575 -4.0185246 -3.9649713 -4.0364242 -4.158565 -4.2546992 -4.311151 -4.338099 -4.34931 -4.3533077 -4.3496537 -4.3458862][-4.3577843 -4.3432355 -4.2914395 -4.1979318 -4.1064978 -4.0857177 -4.1424794 -4.2271605 -4.29529 -4.3290892 -4.3391728 -4.3424249 -4.3439035 -4.3431435 -4.34295][-4.326776 -4.3286877 -4.3009319 -4.2436047 -4.1897368 -4.1845336 -4.2220173 -4.2756219 -4.3204794 -4.3358655 -4.3328962 -4.3323894 -4.335259 -4.3378916 -4.340662][-4.2986755 -4.317203 -4.3120666 -4.2839208 -4.2553039 -4.2539177 -4.2735944 -4.3048987 -4.3306651 -4.333436 -4.3234344 -4.3213463 -4.3267155 -4.3323092 -4.3378038]]...]
INFO - root - 2017-12-07 11:15:36.551155: step 3110, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.095 sec/batch; 46h:34m:03s remains)
INFO - root - 2017-12-07 11:15:57.860784: step 3120, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.181 sec/batch; 48h:27m:57s remains)
INFO - root - 2017-12-07 11:16:18.961407: step 3130, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.141 sec/batch; 47h:34m:04s remains)
INFO - root - 2017-12-07 11:16:39.980718: step 3140, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.132 sec/batch; 47h:21m:58s remains)
INFO - root - 2017-12-07 11:17:01.115805: step 3150, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.138 sec/batch; 47h:29m:43s remains)
INFO - root - 2017-12-07 11:17:22.499449: step 3160, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.110 sec/batch; 46h:51m:53s remains)
INFO - root - 2017-12-07 11:17:43.393475: step 3170, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.116 sec/batch; 47h:00m:20s remains)
INFO - root - 2017-12-07 11:18:04.631315: step 3180, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 47h:14m:44s remains)
INFO - root - 2017-12-07 11:18:25.811482: step 3190, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 47h:00m:37s remains)
INFO - root - 2017-12-07 11:18:46.675660: step 3200, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.153 sec/batch; 47h:47m:23s remains)
2017-12-07 11:18:48.324730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3122044 -4.2993259 -4.2809544 -4.2705007 -4.2733231 -4.2840376 -4.2899919 -4.2819662 -4.2628918 -4.2532763 -4.264482 -4.2836666 -4.3015938 -4.3172541 -4.3254642][-4.3187284 -4.2960958 -4.2682943 -4.2520189 -4.2534113 -4.2629557 -4.262568 -4.2487345 -4.22753 -4.2219114 -4.2373176 -4.2634134 -4.2858558 -4.3044043 -4.3129373][-4.3212862 -4.2936721 -4.2589645 -4.2362895 -4.2327747 -4.2404456 -4.2396703 -4.2321978 -4.2173748 -4.21141 -4.2150517 -4.2321777 -4.2527671 -4.2741866 -4.287499][-4.3174477 -4.2944074 -4.2616053 -4.2341938 -4.2171273 -4.2052526 -4.196939 -4.2012744 -4.2088475 -4.2139306 -4.2124934 -4.2175779 -4.2282844 -4.2450147 -4.2632418][-4.3040519 -4.290309 -4.2662044 -4.2385049 -4.2030916 -4.149437 -4.1051159 -4.1111665 -4.1545811 -4.1883264 -4.2007451 -4.202702 -4.2038374 -4.2182517 -4.2445159][-4.2799287 -4.2722335 -4.2590365 -4.2389083 -4.1959734 -4.1063151 -4.0157871 -4.0073481 -4.08018 -4.14259 -4.1740928 -4.182519 -4.1798668 -4.1960692 -4.23282][-4.2494936 -4.2454457 -4.2404094 -4.2307305 -4.1963286 -4.1116586 -4.0152049 -3.9867716 -4.0411873 -4.0965538 -4.1352668 -4.1588197 -4.1729994 -4.1998229 -4.237175][-4.2291412 -4.2270279 -4.2273469 -4.2310634 -4.2197704 -4.1697817 -4.1052284 -4.0703039 -4.0801129 -4.0978909 -4.1263928 -4.1600986 -4.1897321 -4.2186794 -4.2477832][-4.2287483 -4.2237487 -4.2264314 -4.2374325 -4.2455997 -4.2235518 -4.1869316 -4.1582503 -4.1424356 -4.1297717 -4.1387181 -4.1668019 -4.2053885 -4.2347188 -4.2563024][-4.2436142 -4.2346344 -4.2361 -4.2483048 -4.2681994 -4.2658725 -4.2472553 -4.2225442 -4.197648 -4.1669779 -4.1570029 -4.1744385 -4.2123585 -4.2399964 -4.2586594][-4.2724848 -4.2612615 -4.2583122 -4.2667756 -4.2863989 -4.2955856 -4.2892985 -4.2694173 -4.2414026 -4.2034426 -4.1819839 -4.1903014 -4.2209744 -4.2455945 -4.2616491][-4.2990918 -4.2905226 -4.2874365 -4.2933841 -4.3069282 -4.319221 -4.3218694 -4.3110394 -4.2883444 -4.2546282 -4.2285433 -4.2299852 -4.247457 -4.2618484 -4.2727394][-4.3169961 -4.3153582 -4.3163137 -4.3210764 -4.3301907 -4.3421564 -4.3491449 -4.34657 -4.3338218 -4.3098512 -4.284164 -4.278872 -4.2849293 -4.2876625 -4.2901645][-4.3264413 -4.32925 -4.33103 -4.3342571 -4.3405142 -4.3482738 -4.3543787 -4.3549953 -4.349411 -4.3357677 -4.3173 -4.3104939 -4.3114533 -4.3095589 -4.3078966][-4.3247881 -4.3295522 -4.3325543 -4.33504 -4.338923 -4.3415275 -4.3435278 -4.3435469 -4.3411646 -4.3350806 -4.3261838 -4.3221955 -4.3221178 -4.3211975 -4.3206968]]...]
INFO - root - 2017-12-07 11:19:09.693472: step 3210, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 47h:10m:33s remains)
INFO - root - 2017-12-07 11:19:30.929792: step 3220, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.188 sec/batch; 48h:33m:41s remains)
INFO - root - 2017-12-07 11:19:51.852735: step 3230, loss = 2.09, batch loss = 2.03 (16.7 examples/sec; 1.920 sec/batch; 42h:36m:55s remains)
INFO - root - 2017-12-07 11:20:12.977742: step 3240, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.124 sec/batch; 47h:07m:55s remains)
INFO - root - 2017-12-07 11:20:34.455121: step 3250, loss = 2.08, batch loss = 2.03 (14.7 examples/sec; 2.175 sec/batch; 48h:14m:55s remains)
INFO - root - 2017-12-07 11:20:55.387873: step 3260, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 47h:00m:41s remains)
INFO - root - 2017-12-07 11:21:16.530258: step 3270, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.082 sec/batch; 46h:10m:31s remains)
INFO - root - 2017-12-07 11:21:37.731201: step 3280, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.129 sec/batch; 47h:13m:14s remains)
INFO - root - 2017-12-07 11:21:58.858495: step 3290, loss = 2.09, batch loss = 2.03 (15.9 examples/sec; 2.018 sec/batch; 44h:45m:13s remains)
INFO - root - 2017-12-07 11:22:19.650687: step 3300, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.077 sec/batch; 46h:03m:36s remains)
2017-12-07 11:22:21.224549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.281477 -4.2682977 -4.258532 -4.2574682 -4.2635365 -4.2707229 -4.2776923 -4.2788234 -4.2765203 -4.2744884 -4.2759786 -4.279851 -4.2884326 -4.29951 -4.3100419][-4.237391 -4.2197194 -4.2117853 -4.2173262 -4.2314715 -4.2420254 -4.2518244 -4.2534609 -4.2506247 -4.2490363 -4.2501802 -4.2521644 -4.2610817 -4.2736821 -4.2852888][-4.2175574 -4.2012186 -4.1994267 -4.2134914 -4.2344708 -4.2465615 -4.25311 -4.2476683 -4.2395182 -4.2383633 -4.2392154 -4.2422013 -4.2541361 -4.2670708 -4.2756748][-4.2114129 -4.1915 -4.1872416 -4.1988883 -4.2160754 -4.2232833 -4.2206755 -4.2037597 -4.19047 -4.1948409 -4.2034783 -4.2161059 -4.2386003 -4.2603755 -4.2698793][-4.1987219 -4.16661 -4.1493897 -4.1476345 -4.1502028 -4.1461272 -4.1315165 -4.1035933 -4.0847812 -4.0959344 -4.1175866 -4.1457438 -4.1855679 -4.2258806 -4.2479048][-4.1731277 -4.1312013 -4.1018782 -4.0882096 -4.0756593 -4.0619836 -4.0383472 -4.0004287 -3.9708948 -3.9824314 -4.0135427 -4.0531826 -4.1073203 -4.1659031 -4.2061214][-4.141829 -4.0954256 -4.0585241 -4.0314617 -4.0056362 -3.9905252 -3.9716663 -3.931494 -3.8943605 -3.9014082 -3.9355726 -3.9824741 -4.0450797 -4.1150107 -4.1692939][-4.1317186 -4.0881042 -4.0513415 -4.01712 -3.9866061 -3.9783258 -3.9743192 -3.9440074 -3.9079456 -3.9085944 -3.9376667 -3.9799597 -4.04213 -4.1121349 -4.16706][-4.1456218 -4.11002 -4.0797644 -4.0504704 -4.0293851 -4.0337324 -4.0466762 -4.0351887 -4.0072861 -3.9944963 -4.0061531 -4.03688 -4.0910392 -4.1518044 -4.1980891][-4.178062 -4.1516075 -4.1326604 -4.11327 -4.10616 -4.1201668 -4.1408958 -4.1402831 -4.1163068 -4.0943813 -4.0925283 -4.1136742 -4.1585298 -4.2051988 -4.2371664][-4.224174 -4.2104468 -4.2050261 -4.1967998 -4.194458 -4.2064719 -4.22777 -4.2323976 -4.2143264 -4.1966162 -4.1915665 -4.2027717 -4.2306809 -4.2608862 -4.2808414][-4.2694273 -4.2670712 -4.2681074 -4.262764 -4.2591281 -4.2654819 -4.280035 -4.2835441 -4.2745142 -4.2669058 -4.2649941 -4.2696781 -4.2835732 -4.3011689 -4.3115187][-4.30166 -4.3023777 -4.3035197 -4.3000607 -4.2972817 -4.2996459 -4.3056459 -4.307332 -4.3039751 -4.3031435 -4.3051763 -4.3083315 -4.3144093 -4.3228245 -4.3269868][-4.3177156 -4.3166275 -4.3154125 -4.31381 -4.313273 -4.3146138 -4.3168244 -4.3175344 -4.3170547 -4.3189559 -4.32164 -4.3235264 -4.3261566 -4.3306317 -4.3334255][-4.3278227 -4.3253975 -4.3226895 -4.32157 -4.3225822 -4.324523 -4.3254166 -4.3252478 -4.3252487 -4.3262286 -4.3277192 -4.3292413 -4.3310146 -4.3339162 -4.3363957]]...]
INFO - root - 2017-12-07 11:22:42.409234: step 3310, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 47h:12m:58s remains)
INFO - root - 2017-12-07 11:23:03.579818: step 3320, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.084 sec/batch; 46h:12m:24s remains)
INFO - root - 2017-12-07 11:23:24.478676: step 3330, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 47h:05m:25s remains)
INFO - root - 2017-12-07 11:23:45.579252: step 3340, loss = 2.08, batch loss = 2.03 (15.6 examples/sec; 2.057 sec/batch; 45h:35m:19s remains)
INFO - root - 2017-12-07 11:24:06.999836: step 3350, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.081 sec/batch; 46h:06m:52s remains)
INFO - root - 2017-12-07 11:24:27.779957: step 3360, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 1.949 sec/batch; 43h:10m:54s remains)
INFO - root - 2017-12-07 11:24:48.835232: step 3370, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 47h:06m:17s remains)
INFO - root - 2017-12-07 11:25:10.036691: step 3380, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.141 sec/batch; 47h:26m:13s remains)
INFO - root - 2017-12-07 11:25:31.245836: step 3390, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 46h:35m:54s remains)
INFO - root - 2017-12-07 11:25:52.343631: step 3400, loss = 2.08, batch loss = 2.03 (14.8 examples/sec; 2.168 sec/batch; 48h:00m:28s remains)
2017-12-07 11:25:54.022788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3389764 -4.3489547 -4.341743 -4.3274713 -4.3129754 -4.3061409 -4.3065562 -4.3060861 -4.3017888 -4.3023829 -4.3104587 -4.3104057 -4.2988839 -4.2818947 -4.2538881][-4.34134 -4.3510971 -4.3436232 -4.3293071 -4.3057947 -4.2900152 -4.2895241 -4.2939997 -4.2965193 -4.3041539 -4.3187852 -4.3238173 -4.3148327 -4.2969031 -4.2705612][-4.3338304 -4.3426485 -4.3399792 -4.3277669 -4.2963257 -4.2652645 -4.2527537 -4.2553754 -4.2650056 -4.2829719 -4.3083487 -4.3250217 -4.3260388 -4.3139424 -4.2935367][-4.3287163 -4.3336611 -4.3320189 -4.3180094 -4.2746325 -4.2184072 -4.1822414 -4.1747336 -4.1966658 -4.2375145 -4.2830029 -4.314074 -4.3280115 -4.3273649 -4.3132071][-4.3222251 -4.3172817 -4.3032732 -4.2723832 -4.2094207 -4.1245184 -4.0535536 -4.0261993 -4.0623093 -4.138567 -4.2164803 -4.2712078 -4.3038707 -4.32283 -4.3229771][-4.313014 -4.2975736 -4.2640491 -4.2073889 -4.1228027 -4.0116353 -3.8915768 -3.8192642 -3.8633833 -3.9849548 -4.1065383 -4.1921883 -4.2506809 -4.2930045 -4.3137302][-4.3114104 -4.289362 -4.2403789 -4.1591983 -4.0604105 -3.9365559 -3.7738855 -3.6456838 -3.6902263 -3.8477511 -4.0018892 -4.1151786 -4.1962752 -4.2554526 -4.2929511][-4.3176217 -4.2968936 -4.2466655 -4.1603069 -4.063467 -3.94944 -3.7897477 -3.6516337 -3.6777732 -3.8200495 -3.9686365 -4.0854211 -4.1724157 -4.2350411 -4.2779551][-4.3240733 -4.307827 -4.2677226 -4.1971292 -4.1223865 -4.0456944 -3.9417391 -3.8489051 -3.8444293 -3.9218163 -4.0234694 -4.1150289 -4.1849685 -4.2355347 -4.2721124][-4.3246155 -4.3131151 -4.2849417 -4.2365279 -4.191957 -4.1585574 -4.1113296 -4.0597639 -4.037889 -4.0632906 -4.116384 -4.1739721 -4.2182493 -4.2500992 -4.2739024][-4.3195434 -4.3081188 -4.2874432 -4.2574244 -4.2379022 -4.2314587 -4.2176638 -4.1949048 -4.1732664 -4.1738505 -4.1948948 -4.2244992 -4.2468987 -4.263587 -4.276906][-4.3123255 -4.3047214 -4.2956505 -4.282742 -4.2749634 -4.2747602 -4.2701678 -4.2627254 -4.2499971 -4.2430687 -4.2435422 -4.2539845 -4.2644587 -4.2740741 -4.2820368][-4.2984371 -4.2962551 -4.29916 -4.3000488 -4.30231 -4.3043165 -4.3003106 -4.2986331 -4.2917604 -4.282311 -4.2713337 -4.2694726 -4.27449 -4.2809944 -4.2837105][-4.2705975 -4.270494 -4.280818 -4.2931418 -4.30783 -4.3190155 -4.3181353 -4.3160219 -4.3087492 -4.296916 -4.28162 -4.2736187 -4.2728748 -4.2724528 -4.2662034][-4.2250385 -4.2233715 -4.2389526 -4.2617168 -4.2913203 -4.3145285 -4.3190041 -4.3157554 -4.3096805 -4.2993379 -4.2844315 -4.2705178 -4.2606068 -4.2508836 -4.2360992]]...]
INFO - root - 2017-12-07 11:26:15.222258: step 3410, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 46h:36m:59s remains)
INFO - root - 2017-12-07 11:26:36.410573: step 3420, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 46h:36m:54s remains)
INFO - root - 2017-12-07 11:26:57.244652: step 3430, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.077 sec/batch; 45h:58m:08s remains)
INFO - root - 2017-12-07 11:27:18.316752: step 3440, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.125 sec/batch; 47h:02m:19s remains)
INFO - root - 2017-12-07 11:27:39.349362: step 3450, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.081 sec/batch; 46h:02m:54s remains)
INFO - root - 2017-12-07 11:28:00.228476: step 3460, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.129 sec/batch; 47h:06m:56s remains)
INFO - root - 2017-12-07 11:28:21.538295: step 3470, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.070 sec/batch; 45h:47m:35s remains)
INFO - root - 2017-12-07 11:28:42.797623: step 3480, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 46h:40m:00s remains)
INFO - root - 2017-12-07 11:29:03.377512: step 3490, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.107 sec/batch; 46h:36m:23s remains)
INFO - root - 2017-12-07 11:29:24.530963: step 3500, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.100 sec/batch; 46h:27m:26s remains)
2017-12-07 11:29:26.091786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2060003 -4.2364683 -4.2671919 -4.2922587 -4.3045945 -4.30385 -4.2863388 -4.2635937 -4.2459364 -4.2458739 -4.2566113 -4.2775531 -4.2838869 -4.2638149 -4.2336488][-4.2211723 -4.2539849 -4.28086 -4.2940965 -4.2939696 -4.2848077 -4.2658777 -4.2430882 -4.2297335 -4.2341623 -4.25443 -4.280591 -4.28766 -4.264473 -4.2262559][-4.2301555 -4.262989 -4.2850041 -4.2885022 -4.2765522 -4.2616754 -4.2435384 -4.220922 -4.2076459 -4.21425 -4.241147 -4.2715464 -4.2806034 -4.2585478 -4.2203979][-4.2351956 -4.264317 -4.2799678 -4.2765317 -4.2568612 -4.2365575 -4.2178345 -4.1959281 -4.181953 -4.1885791 -4.2153149 -4.2466197 -4.2601318 -4.2439084 -4.2105742][-4.244719 -4.2642775 -4.2729139 -4.2643137 -4.2403712 -4.2121468 -4.1848311 -4.1606226 -4.1510086 -4.1592026 -4.1829419 -4.2138433 -4.2337956 -4.2248869 -4.1957669][-4.2614307 -4.2706285 -4.2727051 -4.2610941 -4.2304993 -4.1880145 -4.1436949 -4.1102853 -4.10202 -4.1181951 -4.1488 -4.1872392 -4.2153206 -4.2114034 -4.18509][-4.2777553 -4.280261 -4.2755251 -4.2574906 -4.2179346 -4.1613035 -4.0989904 -4.0509377 -4.0407944 -4.0704441 -4.1178861 -4.171401 -4.2108641 -4.2159882 -4.1986804][-4.2858419 -4.2870903 -4.2784505 -4.2527728 -4.2042623 -4.13751 -4.0640492 -4.0041971 -3.9936295 -4.03546 -4.0965686 -4.1613746 -4.2110443 -4.2297707 -4.228548][-4.2859888 -4.2885051 -4.2770572 -4.2450652 -4.1911359 -4.12142 -4.04906 -3.9900246 -3.9838107 -4.0286322 -4.0920444 -4.1600432 -4.2128062 -4.2396207 -4.2487454][-4.2817459 -4.2832375 -4.2697887 -4.2359533 -4.1822577 -4.1189966 -4.0576196 -4.0108891 -4.01146 -4.0524349 -4.1122465 -4.1764832 -4.2251687 -4.249867 -4.2562671][-4.2772164 -4.2771134 -4.2613106 -4.2297139 -4.1811943 -4.1295066 -4.0814681 -4.0492764 -4.0581579 -4.0958843 -4.1526089 -4.2112312 -4.2523866 -4.2679014 -4.2634997][-4.2786112 -4.2763257 -4.2587914 -4.2307754 -4.1904826 -4.1502085 -4.1147027 -4.0960031 -4.1108332 -4.1458793 -4.2000051 -4.2518692 -4.2823114 -4.2863269 -4.2717638][-4.28819 -4.2853146 -4.2656479 -4.2397876 -4.2075672 -4.1748919 -4.1502609 -4.1417384 -4.1583419 -4.1899424 -4.2381787 -4.2831249 -4.3038292 -4.296051 -4.2727218][-4.3052468 -4.3006988 -4.278079 -4.2512741 -4.2216125 -4.1909838 -4.1703181 -4.16693 -4.1819553 -4.2101426 -4.2543507 -4.2958293 -4.3137822 -4.2980862 -4.265029][-4.319808 -4.30731 -4.2803831 -4.2519069 -4.2227068 -4.1931419 -4.1705627 -4.163012 -4.1749249 -4.2031345 -4.2478008 -4.2909575 -4.3114533 -4.2968845 -4.2592096]]...]
INFO - root - 2017-12-07 11:29:47.340500: step 3510, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.070 sec/batch; 45h:46m:06s remains)
INFO - root - 2017-12-07 11:30:08.304615: step 3520, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.146 sec/batch; 47h:27m:43s remains)
INFO - root - 2017-12-07 11:30:29.398808: step 3530, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 47h:09m:51s remains)
INFO - root - 2017-12-07 11:30:50.652440: step 3540, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.140 sec/batch; 47h:18m:06s remains)
INFO - root - 2017-12-07 11:31:11.811542: step 3550, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.151 sec/batch; 47h:32m:36s remains)
INFO - root - 2017-12-07 11:31:32.988027: step 3560, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.131 sec/batch; 47h:06m:09s remains)
INFO - root - 2017-12-07 11:31:53.985800: step 3570, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.091 sec/batch; 46h:12m:52s remains)
INFO - root - 2017-12-07 11:32:15.138654: step 3580, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.116 sec/batch; 46h:45m:50s remains)
INFO - root - 2017-12-07 11:32:35.895704: step 3590, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.095 sec/batch; 46h:17m:09s remains)
INFO - root - 2017-12-07 11:32:56.911448: step 3600, loss = 2.06, batch loss = 2.00 (15.4 examples/sec; 2.081 sec/batch; 45h:58m:24s remains)
2017-12-07 11:32:58.459919: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3236055 -4.324367 -4.3269486 -4.3208857 -4.3120952 -4.3018312 -4.2935438 -4.2866254 -4.2911096 -4.3079286 -4.3257985 -4.3368497 -4.336556 -4.3286247 -4.3216982][-4.3065596 -4.3073974 -4.3098879 -4.3043137 -4.2948246 -4.2816739 -4.2681813 -4.2556791 -4.2557845 -4.2712789 -4.2926221 -4.3129172 -4.3176861 -4.3071165 -4.29505][-4.2800837 -4.2872186 -4.2919216 -4.2836971 -4.2678061 -4.2447991 -4.2226639 -4.2081728 -4.2088003 -4.2281356 -4.2581706 -4.2883363 -4.2966762 -4.2847667 -4.2726345][-4.2495294 -4.2675638 -4.2744079 -4.25976 -4.2260685 -4.1800284 -4.1406622 -4.1270595 -4.1372805 -4.1685419 -4.2119327 -4.2548041 -4.2694216 -4.2622395 -4.2569618][-4.2082233 -4.2362623 -4.2484965 -4.2282619 -4.1736474 -4.0936642 -4.0269265 -4.0157456 -4.0462503 -4.103075 -4.1635451 -4.2165327 -4.2415152 -4.246788 -4.2513][-4.1789546 -4.2077503 -4.2205071 -4.1992087 -4.1328244 -4.0241556 -3.9251826 -3.9126873 -3.9702995 -4.0546465 -4.1303735 -4.1886358 -4.2226725 -4.2370906 -4.2460666][-4.1850905 -4.2044816 -4.2129712 -4.19707 -4.1383286 -4.0339646 -3.9256287 -3.9053657 -3.9713471 -4.0594463 -4.1336632 -4.1900392 -4.2287908 -4.2459221 -4.2526641][-4.2072792 -4.2166967 -4.219079 -4.2150145 -4.1877222 -4.1253781 -4.0480356 -4.0253487 -4.0690227 -4.1272798 -4.1800094 -4.2213359 -4.2569795 -4.2754087 -4.2779565][-4.21489 -4.21208 -4.2058496 -4.2117181 -4.2154679 -4.2032275 -4.1743937 -4.1646638 -4.1856408 -4.2111754 -4.2384953 -4.261044 -4.2874818 -4.3058085 -4.3075857][-4.1740122 -4.1622286 -4.1501679 -4.1627922 -4.1897383 -4.2176056 -4.2355762 -4.2472315 -4.2620273 -4.2731042 -4.2833371 -4.2905483 -4.3065934 -4.3231125 -4.3281388][-4.1032162 -4.0922513 -4.0860105 -4.1067748 -4.1503487 -4.2044368 -4.2541747 -4.2871761 -4.3058372 -4.3157263 -4.3191528 -4.3183827 -4.3255849 -4.3359451 -4.3404832][-4.0659275 -4.0624585 -4.0692654 -4.0950418 -4.1440368 -4.2071815 -4.2702365 -4.3131914 -4.3344646 -4.3456821 -4.3500867 -4.3480659 -4.3471951 -4.3463893 -4.3456798][-4.0875983 -4.0943723 -4.1119189 -4.1396065 -4.1824212 -4.234055 -4.2881746 -4.3275833 -4.3461232 -4.35527 -4.3598857 -4.3598285 -4.3550577 -4.3471012 -4.3430848][-4.1350808 -4.1537395 -4.1810389 -4.2086515 -4.2399254 -4.271915 -4.3063788 -4.3314896 -4.3422527 -4.3483505 -4.3529906 -4.3519955 -4.3439317 -4.3335423 -4.3296881][-4.1930251 -4.2172918 -4.249651 -4.27313 -4.2909718 -4.3042459 -4.3205824 -4.3339286 -4.3394604 -4.3405323 -4.3425756 -4.340498 -4.3297529 -4.3146229 -4.3069506]]...]
INFO - root - 2017-12-07 11:33:19.969064: step 3610, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 47h:42m:12s remains)
INFO - root - 2017-12-07 11:33:40.859538: step 3620, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 46h:52m:19s remains)
INFO - root - 2017-12-07 11:34:02.158450: step 3630, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.138 sec/batch; 47h:12m:17s remains)
INFO - root - 2017-12-07 11:34:23.186380: step 3640, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.077 sec/batch; 45h:51m:19s remains)
INFO - root - 2017-12-07 11:34:44.049113: step 3650, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.090 sec/batch; 46h:07m:45s remains)
INFO - root - 2017-12-07 11:35:05.334438: step 3660, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.177 sec/batch; 48h:02m:41s remains)
INFO - root - 2017-12-07 11:35:26.547574: step 3670, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.094 sec/batch; 46h:12m:26s remains)
INFO - root - 2017-12-07 11:35:47.485259: step 3680, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.068 sec/batch; 45h:37m:53s remains)
INFO - root - 2017-12-07 11:36:08.412383: step 3690, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.150 sec/batch; 47h:25m:51s remains)
INFO - root - 2017-12-07 11:36:29.505991: step 3700, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.090 sec/batch; 46h:06m:39s remains)
2017-12-07 11:36:31.051806: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3633509 -4.3665819 -4.3679695 -4.3683348 -4.3626976 -4.3472118 -4.3230658 -4.296926 -4.2767272 -4.2714324 -4.2810788 -4.2963638 -4.3160753 -4.335896 -4.3516712][-4.3698072 -4.3731122 -4.3752155 -4.372673 -4.3595147 -4.3297315 -4.2895474 -4.2498569 -4.2228146 -4.2212362 -4.2415891 -4.2672849 -4.29514 -4.3226056 -4.3429384][-4.3717279 -4.3756533 -4.3775516 -4.3686028 -4.3384695 -4.2868757 -4.2245555 -4.16741 -4.1342978 -4.1405764 -4.1764922 -4.2201052 -4.2644119 -4.3039069 -4.3317423][-4.3681083 -4.3731966 -4.3732395 -4.35123 -4.2977991 -4.222013 -4.1329241 -4.052762 -4.0119267 -4.030858 -4.0873241 -4.1548886 -4.2245612 -4.2807198 -4.3185453][-4.3573995 -4.3639126 -4.361177 -4.3283157 -4.2575765 -4.1582556 -4.036109 -3.9214685 -3.8681893 -3.9086754 -3.9918666 -4.0866919 -4.182508 -4.2553225 -4.3044357][-4.3446093 -4.35194 -4.3433762 -4.3003879 -4.2126222 -4.0825028 -3.9182858 -3.7584891 -3.6980467 -3.7793937 -3.897954 -4.0220308 -4.1407576 -4.2302303 -4.2922959][-4.3344054 -4.3387885 -4.3203874 -4.2643943 -4.1604991 -4.001678 -3.79665 -3.5909314 -3.5530865 -3.6965594 -3.8543177 -3.9974928 -4.1230822 -4.2187657 -4.2867274][-4.3237724 -4.318162 -4.2859235 -4.2186694 -4.1074214 -3.9365933 -3.7155662 -3.5093255 -3.5357347 -3.7275014 -3.8938611 -4.0275269 -4.1395793 -4.2283516 -4.2931314][-4.3107715 -4.2937818 -4.2527003 -4.1868277 -4.0891318 -3.9469001 -3.7731619 -3.6442969 -3.713917 -3.8747602 -3.9983163 -4.09082 -4.1749229 -4.24799 -4.3038163][-4.2936435 -4.2699265 -4.2311754 -4.1777668 -4.1053691 -4.0083146 -3.9004037 -3.842087 -3.9097993 -4.0164709 -4.0918384 -4.1442876 -4.2054372 -4.2657032 -4.3143778][-4.2757659 -4.2537246 -4.2231417 -4.18296 -4.1324887 -4.0724449 -4.0113797 -3.9921832 -4.0490565 -4.115591 -4.1540809 -4.1777792 -4.2256789 -4.2788363 -4.3226237][-4.2654171 -4.2469063 -4.2247562 -4.1965361 -4.1621323 -4.1291356 -4.1010461 -4.102694 -4.144238 -4.1793017 -4.1942225 -4.2052641 -4.245357 -4.2927575 -4.3304853][-4.269115 -4.2566428 -4.2433519 -4.2269087 -4.208127 -4.1956754 -4.1848731 -4.1873727 -4.2060871 -4.2150264 -4.2178297 -4.2254219 -4.2610826 -4.3046293 -4.3370333][-4.2884173 -4.2827587 -4.2772427 -4.2705946 -4.2630405 -4.2602229 -4.248723 -4.2378578 -4.2328115 -4.2238679 -4.220294 -4.229794 -4.2671819 -4.3093767 -4.3395815][-4.2956343 -4.2960386 -4.2983084 -4.2992706 -4.2989607 -4.2965364 -4.2758126 -4.2476058 -4.2230277 -4.2054491 -4.2022882 -4.2186441 -4.2628202 -4.3072572 -4.3379006]]...]
INFO - root - 2017-12-07 11:36:52.113480: step 3710, loss = 2.08, batch loss = 2.02 (16.5 examples/sec; 1.944 sec/batch; 42h:53m:09s remains)
INFO - root - 2017-12-07 11:37:13.222187: step 3720, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 46h:11m:02s remains)
INFO - root - 2017-12-07 11:37:34.500991: step 3730, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.152 sec/batch; 47h:27m:12s remains)
INFO - root - 2017-12-07 11:37:55.743039: step 3740, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.138 sec/batch; 47h:08m:57s remains)
INFO - root - 2017-12-07 11:38:16.776672: step 3750, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.091 sec/batch; 46h:06m:21s remains)
INFO - root - 2017-12-07 11:38:37.970297: step 3760, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.113 sec/batch; 46h:35m:04s remains)
INFO - root - 2017-12-07 11:38:59.124732: step 3770, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 46h:36m:45s remains)
INFO - root - 2017-12-07 11:39:20.195489: step 3780, loss = 2.07, batch loss = 2.02 (15.7 examples/sec; 2.033 sec/batch; 44h:48m:36s remains)
INFO - root - 2017-12-07 11:39:41.391901: step 3790, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 46h:14m:49s remains)
INFO - root - 2017-12-07 11:40:02.715060: step 3800, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.143 sec/batch; 47h:13m:18s remains)
2017-12-07 11:40:04.261652: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3290687 -4.3206887 -4.3084259 -4.2966623 -4.2855644 -4.2780261 -4.2766294 -4.278789 -4.282124 -4.28791 -4.2941222 -4.2898049 -4.2787876 -4.2682753 -4.2513809][-4.3342776 -4.3288331 -4.3202968 -4.3126559 -4.3022027 -4.2918258 -4.2877531 -4.2874904 -4.28782 -4.2906523 -4.2955933 -4.290247 -4.27519 -4.2579713 -4.2356772][-4.3330169 -4.3296785 -4.3259144 -4.32266 -4.3119621 -4.2988162 -4.2914891 -4.2881823 -4.2863369 -4.2881184 -4.2909808 -4.2811427 -4.2615504 -4.2386537 -4.2123079][-4.3271322 -4.3246813 -4.3237576 -4.3222241 -4.3112593 -4.2970695 -4.2886114 -4.2829461 -4.2783561 -4.2795849 -4.2810488 -4.2694378 -4.2463646 -4.217659 -4.1878695][-4.3170371 -4.3128891 -4.3107367 -4.3068538 -4.2938552 -4.2788291 -4.2715421 -4.2675457 -4.2632856 -4.2644043 -4.2649508 -4.2560425 -4.235414 -4.2050185 -4.174655][-4.306077 -4.2983804 -4.2925735 -4.284359 -4.2670474 -4.2502575 -4.2446628 -4.2456689 -4.2440715 -4.246623 -4.2492747 -4.246973 -4.23441 -4.2111363 -4.1863136][-4.2996225 -4.2885451 -4.2788191 -4.2664385 -4.2461538 -4.227035 -4.2192435 -4.2219996 -4.2224727 -4.2277269 -4.234817 -4.2412372 -4.2409077 -4.2327518 -4.2204456][-4.3024178 -4.291698 -4.2807617 -4.2673216 -4.2466736 -4.2254343 -4.2139459 -4.2138014 -4.21377 -4.2199864 -4.2299953 -4.2405534 -4.2496185 -4.2559505 -4.2541027][-4.3125167 -4.3060584 -4.2981358 -4.2886415 -4.2731581 -4.2529507 -4.2387857 -4.2324471 -4.2281051 -4.2291389 -4.233695 -4.2412567 -4.2510166 -4.2626452 -4.2659216][-4.3224354 -4.320374 -4.3168192 -4.3130851 -4.3029995 -4.287631 -4.2761688 -4.2677546 -4.2585435 -4.2504516 -4.2444215 -4.2444525 -4.2494349 -4.2601218 -4.2641416][-4.3261356 -4.3254995 -4.3234611 -4.3226886 -4.3172874 -4.3072596 -4.2994685 -4.2925258 -4.2826996 -4.2703524 -4.257853 -4.2525558 -4.2531328 -4.2599397 -4.2614231][-4.3246961 -4.3222923 -4.3183627 -4.3170376 -4.3138027 -4.3088446 -4.3051424 -4.301981 -4.2940021 -4.2824836 -4.2699447 -4.263237 -4.2610865 -4.2635813 -4.2625909][-4.3187003 -4.3134279 -4.307466 -4.3043575 -4.3025765 -4.3013291 -4.300766 -4.2997246 -4.2945771 -4.2858648 -4.2762551 -4.2709212 -4.2679563 -4.2678752 -4.2667589][-4.3128028 -4.3047986 -4.2984662 -4.2954588 -4.2947416 -4.2943959 -4.2937932 -4.292367 -4.2885733 -4.282373 -4.2754478 -4.2712183 -4.2678204 -4.2674308 -4.267735][-4.3143468 -4.3048573 -4.2980418 -4.2954254 -4.29527 -4.2945542 -4.2928057 -4.2913766 -4.2896032 -4.2865682 -4.2824216 -4.2782555 -4.2738452 -4.2730689 -4.274096]]...]
INFO - root - 2017-12-07 11:40:25.410661: step 3810, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.185 sec/batch; 48h:08m:44s remains)
INFO - root - 2017-12-07 11:40:46.335460: step 3820, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.093 sec/batch; 46h:06m:58s remains)
INFO - root - 2017-12-07 11:41:07.502626: step 3830, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.124 sec/batch; 46h:46m:34s remains)
INFO - root - 2017-12-07 11:41:28.613541: step 3840, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.093 sec/batch; 46h:06m:20s remains)
INFO - root - 2017-12-07 11:41:49.314019: step 3850, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 46h:26m:41s remains)
INFO - root - 2017-12-07 11:42:10.460047: step 3860, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 2.077 sec/batch; 45h:43m:58s remains)
INFO - root - 2017-12-07 11:42:31.656027: step 3870, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 46h:48m:12s remains)
INFO - root - 2017-12-07 11:42:52.719501: step 3880, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.190 sec/batch; 48h:12m:17s remains)
INFO - root - 2017-12-07 11:43:13.881143: step 3890, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.094 sec/batch; 46h:04m:43s remains)
INFO - root - 2017-12-07 11:43:34.941350: step 3900, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.132 sec/batch; 46h:55m:46s remains)
2017-12-07 11:43:36.526583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2943068 -4.2844911 -4.2747521 -4.2684264 -4.2721405 -4.2773652 -4.280488 -4.2781363 -4.2775 -4.2774649 -4.2819839 -4.2901416 -4.297895 -4.2998624 -4.29808][-4.2805843 -4.2716479 -4.2642303 -4.2629123 -4.2686481 -4.2732949 -4.2718072 -4.2621827 -4.2597752 -4.2613029 -4.2667327 -4.2758131 -4.2868891 -4.2935109 -4.2941279][-4.2709222 -4.2652049 -4.2642961 -4.271801 -4.2808523 -4.283742 -4.2757149 -4.2594485 -4.256526 -4.2641621 -4.2740993 -4.2819934 -4.2903557 -4.2964926 -4.2960477][-4.2667308 -4.2651005 -4.2713237 -4.2875743 -4.2960238 -4.2917347 -4.2713327 -4.2462807 -4.2433405 -4.2585516 -4.2754097 -4.2851753 -4.2911782 -4.2909389 -4.2823253][-4.26643 -4.2676649 -4.2761197 -4.2948933 -4.2983575 -4.2826037 -4.2443614 -4.2038531 -4.2000532 -4.227262 -4.2564964 -4.2732258 -4.2802167 -4.2734556 -4.2560177][-4.2656565 -4.266613 -4.2710705 -4.2818947 -4.2733169 -4.2402515 -4.1762772 -4.1160564 -4.1104746 -4.1514115 -4.1974645 -4.2254105 -4.2354341 -4.2255082 -4.1987329][-4.2605267 -4.2564692 -4.2533894 -4.2525587 -4.2273283 -4.1739597 -4.084764 -4.0034275 -3.9940395 -4.0460029 -4.1061935 -4.143466 -4.1562481 -4.140523 -4.1077709][-4.25204 -4.2402997 -4.2275419 -4.2181077 -4.1809807 -4.1142378 -4.0109115 -3.9136682 -3.9018376 -3.9558988 -4.0198712 -4.0591393 -4.0773916 -4.0638146 -4.0359254][-4.2491817 -4.2323236 -4.2163978 -4.2079773 -4.1743159 -4.1147623 -4.022397 -3.9328916 -3.9213 -3.9664905 -4.01428 -4.0412683 -4.0633616 -4.0641484 -4.05387][-4.2585583 -4.2470946 -4.2369809 -4.233067 -4.2115355 -4.1738844 -4.115159 -4.0536814 -4.0408096 -4.0669713 -4.0932789 -4.1061664 -4.1248236 -4.1278367 -4.1241164][-4.2702203 -4.2676139 -4.2633204 -4.2626047 -4.2465129 -4.220355 -4.1854548 -4.1483154 -4.1392932 -4.1561427 -4.17241 -4.178997 -4.1916261 -4.1926131 -4.1884923][-4.2862616 -4.2912679 -4.2906532 -4.2916756 -4.2768478 -4.2520366 -4.2275591 -4.2075958 -4.2061687 -4.220335 -4.231318 -4.2362742 -4.2420115 -4.2367711 -4.2278495][-4.3028564 -4.3108411 -4.3131289 -4.3162413 -4.3010712 -4.2766013 -4.2592297 -4.2493629 -4.2502384 -4.2601371 -4.2657347 -4.2693024 -4.2723308 -4.2645054 -4.2529039][-4.3084831 -4.3115196 -4.3111997 -4.3128777 -4.2980027 -4.2775311 -4.266993 -4.2648983 -4.2680097 -4.275908 -4.280261 -4.2849097 -4.2889953 -4.2851729 -4.2785411][-4.3111873 -4.3100739 -4.3057947 -4.3054276 -4.2953634 -4.2821455 -4.2788715 -4.2807932 -4.2833881 -4.2905679 -4.2966413 -4.3020258 -4.3051353 -4.3044529 -4.3020568]]...]
INFO - root - 2017-12-07 11:43:57.344972: step 3910, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.109 sec/batch; 46h:24m:06s remains)
INFO - root - 2017-12-07 11:44:18.428602: step 3920, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 46h:51m:55s remains)
INFO - root - 2017-12-07 11:44:39.721260: step 3930, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.192 sec/batch; 48h:12m:42s remains)
INFO - root - 2017-12-07 11:45:00.772094: step 3940, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.099 sec/batch; 46h:10m:10s remains)
INFO - root - 2017-12-07 11:45:21.812938: step 3950, loss = 2.08, batch loss = 2.03 (14.9 examples/sec; 2.154 sec/batch; 47h:22m:42s remains)
INFO - root - 2017-12-07 11:45:43.133274: step 3960, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.104 sec/batch; 46h:16m:33s remains)
INFO - root - 2017-12-07 11:46:04.322820: step 3970, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 46h:08m:48s remains)
INFO - root - 2017-12-07 11:46:25.351268: step 3980, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.062 sec/batch; 45h:19m:54s remains)
INFO - root - 2017-12-07 11:46:46.591912: step 3990, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 46h:52m:40s remains)
INFO - root - 2017-12-07 11:47:07.854629: step 4000, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.176 sec/batch; 47h:49m:31s remains)
2017-12-07 11:47:09.438993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0870991 -4.1102576 -4.1420727 -4.1698174 -4.1944184 -4.2057252 -4.2124224 -4.2077565 -4.1881361 -4.1553187 -4.1180048 -4.097785 -4.1083617 -4.13495 -4.1646719][-4.0144162 -4.0497627 -4.0925064 -4.1281781 -4.1543326 -4.1634727 -4.1697073 -4.1649017 -4.1417432 -4.104023 -4.0635948 -4.0434933 -4.0583305 -4.0925717 -4.1298103][-3.9469287 -3.9962382 -4.0516968 -4.09826 -4.1322341 -4.144063 -4.146822 -4.1385727 -4.111649 -4.0685616 -4.0184789 -3.9909532 -4.0060239 -4.0474381 -4.095448][-3.9185705 -3.9729764 -4.0330234 -4.0814562 -4.11687 -4.1292605 -4.1279411 -4.120419 -4.1015253 -4.0607076 -4.0039535 -3.9697337 -3.9810097 -4.02396 -4.072525][-3.997035 -4.0367041 -4.0746078 -4.1014404 -4.1177773 -4.1169534 -4.106163 -4.1033182 -4.1012864 -4.0785284 -4.0376725 -4.0144668 -4.0255728 -4.0598187 -4.0955973][-4.1029758 -4.1178412 -4.125381 -4.1248 -4.1139135 -4.0918427 -4.0732317 -4.0763826 -4.0890217 -4.0953779 -4.0888162 -4.0883546 -4.1045136 -4.1304107 -4.14548][-4.1549244 -4.1498575 -4.1361089 -4.11739 -4.085875 -4.0497589 -4.028718 -4.0379543 -4.0625091 -4.0971212 -4.125927 -4.1469145 -4.1740046 -4.1997242 -4.2043805][-4.1740441 -4.1588697 -4.1346807 -4.1056228 -4.0649891 -4.0280948 -4.0090013 -4.0181704 -4.0434961 -4.0856805 -4.1297517 -4.1632905 -4.2027617 -4.2363734 -4.2473931][-4.2053843 -4.1892676 -4.1645846 -4.1377234 -4.1086884 -4.0837097 -4.0642371 -4.066339 -4.0783458 -4.1055956 -4.1396356 -4.1696763 -4.2095318 -4.2426705 -4.2582941][-4.2583604 -4.2412558 -4.2166948 -4.1929307 -4.1746869 -4.1587796 -4.1412539 -4.1379867 -4.139111 -4.1516538 -4.16966 -4.1869936 -4.217443 -4.2434216 -4.2569623][-4.3063531 -4.2897711 -4.26472 -4.2434568 -4.2302995 -4.219399 -4.2075291 -4.2044907 -4.2017508 -4.2041879 -4.2103581 -4.2172122 -4.2369246 -4.2564449 -4.2684231][-4.3314261 -4.3213749 -4.3040357 -4.2883205 -4.2792859 -4.2728333 -4.2665825 -4.265718 -4.2627268 -4.25844 -4.2547674 -4.253994 -4.2656012 -4.2803793 -4.291605][-4.33899 -4.33513 -4.3255191 -4.3166728 -4.3112721 -4.3088346 -4.3089523 -4.3121414 -4.3108711 -4.306344 -4.3001528 -4.2970209 -4.3028479 -4.3128862 -4.3204684][-4.3375626 -4.3370185 -4.3326321 -4.3280416 -4.3248429 -4.3248391 -4.32738 -4.331984 -4.33393 -4.3332496 -4.3309364 -4.3308334 -4.3352637 -4.34075 -4.3427386][-4.3374867 -4.3389626 -4.3374543 -4.3356171 -4.3341103 -4.3351493 -4.3374548 -4.3412347 -4.3443 -4.345376 -4.3451109 -4.3455544 -4.347259 -4.3483272 -4.34735]]...]
INFO - root - 2017-12-07 11:47:30.332774: step 4010, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.083 sec/batch; 45h:46m:40s remains)
INFO - root - 2017-12-07 11:47:51.516651: step 4020, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.107 sec/batch; 46h:17m:35s remains)
INFO - root - 2017-12-07 11:48:12.848614: step 4030, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 46h:42m:48s remains)
INFO - root - 2017-12-07 11:48:33.784651: step 4040, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.140 sec/batch; 47h:00m:04s remains)
INFO - root - 2017-12-07 11:48:54.997629: step 4050, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 46h:40m:43s remains)
INFO - root - 2017-12-07 11:49:16.143524: step 4060, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.101 sec/batch; 46h:09m:12s remains)
INFO - root - 2017-12-07 11:49:37.067828: step 4070, loss = 2.08, batch loss = 2.02 (16.1 examples/sec; 1.985 sec/batch; 43h:35m:41s remains)
INFO - root - 2017-12-07 11:49:58.240184: step 4080, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.117 sec/batch; 46h:29m:16s remains)
INFO - root - 2017-12-07 11:50:19.270066: step 4090, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.106 sec/batch; 46h:14m:22s remains)
INFO - root - 2017-12-07 11:50:40.265553: step 4100, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 46h:33m:46s remains)
2017-12-07 11:50:41.869346: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2161622 -4.2210078 -4.2224116 -4.2298336 -4.2414613 -4.2563195 -4.2727795 -4.2849975 -4.2923946 -4.297533 -4.3057389 -4.3156562 -4.3215256 -4.3211622 -4.31646][-4.2796435 -4.278441 -4.2756267 -4.2778387 -4.2842035 -4.294188 -4.3041019 -4.3101726 -4.3142147 -4.3173194 -4.3221846 -4.3240676 -4.322453 -4.3176169 -4.31211][-4.3183212 -4.3084846 -4.299787 -4.2962713 -4.2964683 -4.300334 -4.3036385 -4.3066182 -4.30839 -4.3104963 -4.3129492 -4.3099775 -4.3032537 -4.2937303 -4.2862859][-4.3220406 -4.3042045 -4.2902427 -4.2812023 -4.2772126 -4.2808676 -4.285831 -4.2916121 -4.2952495 -4.2975087 -4.2956352 -4.2842846 -4.268199 -4.2497873 -4.236526][-4.3044391 -4.2866163 -4.2723851 -4.2615433 -4.25566 -4.2607923 -4.2663746 -4.2697735 -4.2691274 -4.2653379 -4.2567029 -4.2388391 -4.2165093 -4.1902637 -4.1720257][-4.2873092 -4.2666979 -4.2481165 -4.2296748 -4.2167993 -4.2177334 -4.2210674 -4.2246876 -4.2257876 -4.2218409 -4.2135239 -4.1959162 -4.176084 -4.1498551 -4.1293192][-4.2495747 -4.2185545 -4.1921048 -4.1672778 -4.148222 -4.1460533 -4.1517773 -4.1615715 -4.1720834 -4.1762266 -4.1744852 -4.1647587 -4.1513743 -4.1271362 -4.1043506][-4.182539 -4.1487427 -4.1209607 -4.0940437 -4.072803 -4.0742025 -4.0896211 -4.1101503 -4.13004 -4.1409364 -4.143199 -4.1347947 -4.1205659 -4.0940261 -4.073544][-4.1223187 -4.1037254 -4.0883274 -4.0673618 -4.0475016 -4.0547638 -4.0750227 -4.0948381 -4.1090522 -4.1137743 -4.1128068 -4.1022091 -4.0844669 -4.0577779 -4.0467257][-4.11954 -4.1204171 -4.1203513 -4.1056848 -4.0872035 -4.0909243 -4.1015615 -4.1113329 -4.1176777 -4.116713 -4.1159182 -4.1094103 -4.0953732 -4.07672 -4.0772853][-4.1697578 -4.171804 -4.1746573 -4.1626277 -4.1462536 -4.1442609 -4.1454759 -4.1489253 -4.1524258 -4.1525021 -4.1561127 -4.1574593 -4.1516881 -4.1451659 -4.1501532][-4.2372661 -4.2360959 -4.2359223 -4.2265668 -4.2147026 -4.2113662 -4.2101073 -4.2111192 -4.2143264 -4.2183251 -4.2237945 -4.2271285 -4.2247214 -4.2229595 -4.224247][-4.289063 -4.2870536 -4.2845426 -4.2771859 -4.2694974 -4.2678261 -4.2679768 -4.2692337 -4.2728672 -4.2777486 -4.2825637 -4.2849555 -4.2830987 -4.2810493 -4.2795873][-4.3163071 -4.3163338 -4.315485 -4.3118877 -4.3086863 -4.3086452 -4.3104239 -4.3121319 -4.31375 -4.3156891 -4.3178692 -4.3184481 -4.3171649 -4.3170567 -4.3170695][-4.3346386 -4.33549 -4.335134 -4.3332148 -4.3322463 -4.3335342 -4.3357115 -4.3373876 -4.338264 -4.3388109 -4.3393664 -4.3391509 -4.3384371 -4.3390441 -4.3399043]]...]
INFO - root - 2017-12-07 11:51:02.805175: step 4110, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.106 sec/batch; 46h:13m:38s remains)
INFO - root - 2017-12-07 11:51:24.104045: step 4120, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.163 sec/batch; 47h:27m:53s remains)
INFO - root - 2017-12-07 11:51:45.078398: step 4130, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 1.902 sec/batch; 41h:44m:36s remains)
INFO - root - 2017-12-07 11:52:06.307011: step 4140, loss = 2.06, batch loss = 2.00 (14.7 examples/sec; 2.184 sec/batch; 47h:55m:18s remains)
INFO - root - 2017-12-07 11:52:27.579735: step 4150, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.146 sec/batch; 47h:04m:58s remains)
INFO - root - 2017-12-07 11:52:48.793769: step 4160, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 46h:40m:50s remains)
INFO - root - 2017-12-07 11:53:09.780851: step 4170, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.142 sec/batch; 46h:58m:14s remains)
INFO - root - 2017-12-07 11:53:31.151173: step 4180, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.091 sec/batch; 45h:50m:55s remains)
INFO - root - 2017-12-07 11:53:52.384055: step 4190, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.107 sec/batch; 46h:12m:15s remains)
INFO - root - 2017-12-07 11:54:13.112839: step 4200, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.100 sec/batch; 46h:01m:56s remains)
2017-12-07 11:54:14.755488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3413486 -4.3391705 -4.3381019 -4.3372993 -4.33552 -4.3325753 -4.3288312 -4.3236995 -4.3183556 -4.3141551 -4.3119254 -4.3110452 -4.3117814 -4.3137822 -4.3198376][-4.3245549 -4.3226147 -4.3226733 -4.3230796 -4.3214855 -4.3157339 -4.3064227 -4.2949052 -4.2864356 -4.28114 -4.2764487 -4.2748337 -4.279561 -4.2865844 -4.2981462][-4.3015175 -4.2992 -4.300456 -4.3035769 -4.3022919 -4.2921062 -4.2746067 -4.2529407 -4.2391438 -4.2344441 -4.23104 -4.2319908 -4.2426972 -4.256361 -4.2735076][-4.2745109 -4.2733936 -4.2750559 -4.2801089 -4.2798486 -4.2658095 -4.236033 -4.1985941 -4.1794147 -4.181603 -4.1854882 -4.1911845 -4.2085557 -4.2273459 -4.2491789][-4.2400327 -4.2405677 -4.2415323 -4.2473087 -4.24802 -4.2297015 -4.1848364 -4.1272669 -4.1088433 -4.1287265 -4.1470871 -4.16 -4.1820254 -4.2047195 -4.22956][-4.195075 -4.1970034 -4.1972313 -4.2021732 -4.199049 -4.1698384 -4.09981 -4.016366 -4.0131111 -4.0666246 -4.1070075 -4.1279206 -4.1531878 -4.1805954 -4.2095227][-4.135335 -4.1385865 -4.1381741 -4.1414714 -4.13036 -4.0839992 -3.9794817 -3.8700325 -3.9003232 -3.9989154 -4.0624003 -4.0916162 -4.121686 -4.1585097 -4.1922035][-4.0914636 -4.0973577 -4.0943 -4.093482 -4.0766387 -4.02046 -3.8963225 -3.783946 -3.8536634 -3.9835167 -4.0570769 -4.0901551 -4.1224513 -4.1620665 -4.1940393][-4.1007624 -4.1080008 -4.1048193 -4.103035 -4.0898876 -4.0456853 -3.9509339 -3.8764544 -3.9472094 -4.0564413 -4.1153545 -4.1392083 -4.1610155 -4.1891727 -4.209909][-4.1407242 -4.1431828 -4.1399493 -4.1398888 -4.1343284 -4.1080728 -4.0560861 -4.0185719 -4.0686703 -4.1402111 -4.1774669 -4.1892071 -4.19692 -4.208611 -4.215857][-4.1594014 -4.1549492 -4.1515479 -4.1540036 -4.1545339 -4.1432734 -4.1224217 -4.1082435 -4.1361566 -4.1763816 -4.1970892 -4.19874 -4.1942525 -4.1925268 -4.1906285][-4.1610508 -4.1514778 -4.1456118 -4.1499619 -4.1552792 -4.1552453 -4.1526031 -4.1485915 -4.1605239 -4.180058 -4.1908979 -4.1866655 -4.1758542 -4.1692796 -4.1653814][-4.1877742 -4.1771021 -4.1696405 -4.1735821 -4.1814394 -4.1875386 -4.1911187 -4.1904478 -4.1957 -4.2046762 -4.2103114 -4.2043881 -4.1932311 -4.1873279 -4.1856146][-4.2416821 -4.2345085 -4.2301006 -4.2334027 -4.2394776 -4.2444806 -4.2482915 -4.2493105 -4.2508793 -4.2537923 -4.2558565 -4.2518077 -4.2454357 -4.2429824 -4.2424135][-4.2875381 -4.2833781 -4.2817903 -4.284019 -4.2878079 -4.2907386 -4.2925549 -4.2932181 -4.2938881 -4.2948456 -4.2946873 -4.2921333 -4.2894449 -4.2886238 -4.2881327]]...]
INFO - root - 2017-12-07 11:54:35.879797: step 4210, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.147 sec/batch; 47h:03m:28s remains)
INFO - root - 2017-12-07 11:54:57.001693: step 4220, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 46h:26m:16s remains)
INFO - root - 2017-12-07 11:55:18.052663: step 4230, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 46h:24m:27s remains)
INFO - root - 2017-12-07 11:55:39.211940: step 4240, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 47h:19m:43s remains)
INFO - root - 2017-12-07 11:56:00.434840: step 4250, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.096 sec/batch; 45h:54m:44s remains)
INFO - root - 2017-12-07 11:56:21.336914: step 4260, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.103 sec/batch; 46h:03m:44s remains)
INFO - root - 2017-12-07 11:56:42.312526: step 4270, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.116 sec/batch; 46h:20m:30s remains)
INFO - root - 2017-12-07 11:57:03.393272: step 4280, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.143 sec/batch; 46h:55m:48s remains)
INFO - root - 2017-12-07 11:57:24.780991: step 4290, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 46h:11m:06s remains)
INFO - root - 2017-12-07 11:57:45.740040: step 4300, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.171 sec/batch; 47h:31m:57s remains)
2017-12-07 11:57:47.286385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3042374 -4.3003292 -4.2981114 -4.2968874 -4.2978606 -4.2977462 -4.2959695 -4.2957649 -4.2991238 -4.3020453 -4.3045778 -4.302248 -4.2991595 -4.2997942 -4.3055792][-4.297925 -4.2951117 -4.2924128 -4.2887926 -4.2878094 -4.28852 -4.2831211 -4.2782483 -4.282867 -4.2876592 -4.2883534 -4.2831488 -4.2799621 -4.2803454 -4.2869592][-4.278255 -4.2738519 -4.2707109 -4.26523 -4.267117 -4.2720628 -4.2653503 -4.2575622 -4.2640867 -4.2726035 -4.2713284 -4.2621045 -4.2586207 -4.2576923 -4.2661195][-4.2443128 -4.2327795 -4.227293 -4.2195163 -4.2260246 -4.2372465 -4.2305193 -4.2185478 -4.2289557 -4.2451615 -4.2452493 -4.2355523 -4.2326951 -4.2306161 -4.2422094][-4.2016296 -4.1801329 -4.1706867 -4.1565742 -4.1609163 -4.177084 -4.1684937 -4.1460066 -4.1580338 -4.191112 -4.2007093 -4.1951394 -4.1975245 -4.1993394 -4.2168469][-4.1654625 -4.1349859 -4.116663 -4.0902505 -4.0853696 -4.1026406 -4.0857086 -4.0369239 -4.0453477 -4.1088514 -4.138937 -4.143713 -4.1577625 -4.1712136 -4.1972451][-4.1384468 -4.1025772 -4.0723629 -4.0252194 -4.0036387 -4.0147653 -3.9817927 -3.8980374 -3.9011974 -4.0107574 -4.0721664 -4.0892949 -4.11814 -4.15016 -4.1870208][-4.1401682 -4.1029816 -4.064187 -4.00328 -3.96696 -3.9677024 -3.9222212 -3.8114612 -3.8046532 -3.9442592 -4.026834 -4.0507269 -4.0889406 -4.1390648 -4.185194][-4.1685977 -4.1360483 -4.1021667 -4.0474749 -4.0126934 -4.0104909 -3.9742706 -3.8875153 -3.8744974 -3.9737346 -4.0373464 -4.0564065 -4.0918827 -4.1452231 -4.1934795][-4.2058992 -4.1819248 -4.1583986 -4.11774 -4.0928135 -4.091609 -4.0712495 -4.0225806 -4.0051312 -4.0519795 -4.0861592 -4.0919437 -4.11436 -4.1566229 -4.2021236][-4.2509427 -4.234189 -4.21732 -4.1868792 -4.1662145 -4.1647162 -4.155869 -4.1303563 -4.1138835 -4.1326623 -4.150526 -4.1505914 -4.1573315 -4.1820626 -4.2199082][-4.2891121 -4.2787628 -4.2659359 -4.245358 -4.233808 -4.2363491 -4.2344656 -4.2200947 -4.2057629 -4.2138028 -4.2243624 -4.2232709 -4.2180953 -4.2269831 -4.2521148][-4.3138804 -4.307374 -4.2973685 -4.2854719 -4.2818508 -4.285387 -4.2840209 -4.2745724 -4.2649364 -4.2704177 -4.2767611 -4.2757273 -4.2672977 -4.2676315 -4.2819252][-4.3246174 -4.3204684 -4.312541 -4.3047218 -4.3040657 -4.3087816 -4.307097 -4.2995081 -4.2935677 -4.2968984 -4.3014889 -4.3011146 -4.2954993 -4.295805 -4.3050737][-4.328753 -4.3254213 -4.3197374 -4.3149452 -4.3146548 -4.3180733 -4.3173838 -4.3124 -4.3081532 -4.3097491 -4.3137612 -4.3136964 -4.3104858 -4.3133764 -4.3216205]]...]
INFO - root - 2017-12-07 11:58:08.397264: step 4310, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.138 sec/batch; 46h:47m:47s remains)
INFO - root - 2017-12-07 11:58:29.642381: step 4320, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.111 sec/batch; 46h:12m:04s remains)
INFO - root - 2017-12-07 11:58:50.463873: step 4330, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.113 sec/batch; 46h:15m:05s remains)
INFO - root - 2017-12-07 11:59:11.547246: step 4340, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.104 sec/batch; 46h:03m:14s remains)
INFO - root - 2017-12-07 11:59:32.543517: step 4350, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.083 sec/batch; 45h:34m:44s remains)
INFO - root - 2017-12-07 11:59:53.448528: step 4360, loss = 2.07, batch loss = 2.01 (15.8 examples/sec; 2.028 sec/batch; 44h:22m:12s remains)
INFO - root - 2017-12-07 12:00:14.437579: step 4370, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 2.078 sec/batch; 45h:28m:11s remains)
INFO - root - 2017-12-07 12:00:35.429249: step 4380, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.112 sec/batch; 46h:11m:47s remains)
INFO - root - 2017-12-07 12:00:56.378962: step 4390, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.103 sec/batch; 45h:59m:23s remains)
INFO - root - 2017-12-07 12:01:17.394736: step 4400, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.108 sec/batch; 46h:05m:39s remains)
2017-12-07 12:01:19.032844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1228089 -4.169517 -4.2108116 -4.23422 -4.2380891 -4.2401443 -4.242713 -4.2327104 -4.2115736 -4.1807976 -4.1363807 -4.0982914 -4.0949492 -4.1257915 -4.1671004][-4.14688 -4.1896267 -4.2256851 -4.2439094 -4.2484336 -4.2552404 -4.2595077 -4.2472324 -4.2185035 -4.1808658 -4.1330481 -4.0943279 -4.0935426 -4.1254258 -4.1644325][-4.1995859 -4.2280111 -4.2518959 -4.2577791 -4.2557917 -4.2608204 -4.2650733 -4.2546377 -4.2247796 -4.1883631 -4.1473212 -4.1150842 -4.1174831 -4.1502881 -4.1872644][-4.235301 -4.2522693 -4.265873 -4.2575793 -4.2472305 -4.24481 -4.2435308 -4.2358246 -4.2113485 -4.185041 -4.1579604 -4.1356082 -4.1412883 -4.1742687 -4.2117896][-4.2583075 -4.2667923 -4.2727613 -4.2541642 -4.2306218 -4.2129521 -4.2009244 -4.1953974 -4.1831713 -4.1740541 -4.1657882 -4.156507 -4.1609578 -4.1868358 -4.221508][-4.2784848 -4.2820916 -4.279994 -4.2525616 -4.2119131 -4.1713109 -4.1481595 -4.1469097 -4.152123 -4.1673546 -4.1771574 -4.1730151 -4.1674991 -4.1801758 -4.2120008][-4.2942362 -4.2948408 -4.2832322 -4.2465262 -4.1890607 -4.1302876 -4.0967436 -4.0994143 -4.1245275 -4.1617885 -4.1821623 -4.1761179 -4.1603007 -4.1622205 -4.1865625][-4.2986341 -4.2934728 -4.268528 -4.2184372 -4.1471996 -4.073029 -4.0247154 -4.03518 -4.0849128 -4.141706 -4.169136 -4.1667738 -4.1575422 -4.159379 -4.1739683][-4.2956252 -4.2792974 -4.24012 -4.176024 -4.0919003 -4.0010638 -3.943547 -3.9680979 -4.0463858 -4.1232686 -4.1621089 -4.1716537 -4.17109 -4.1706872 -4.1742511][-4.2833657 -4.2521434 -4.2026043 -4.130455 -4.0435343 -3.9545462 -3.9105849 -3.9623842 -4.0632377 -4.1450686 -4.1868629 -4.1970167 -4.1917238 -4.185812 -4.180109][-4.2588744 -4.2126389 -4.1569152 -4.0931149 -4.0330114 -3.9878073 -3.9884524 -4.0511794 -4.1330743 -4.1927714 -4.22046 -4.2190223 -4.206346 -4.1982808 -4.1913242][-4.2246237 -4.1656342 -4.1073627 -4.0599461 -4.0403976 -4.053751 -4.091198 -4.1452513 -4.1947217 -4.2278318 -4.2383957 -4.2319098 -4.2188187 -4.2113976 -4.2076693][-4.1839042 -4.1176734 -4.0588694 -4.0265732 -4.0445561 -4.0992002 -4.1562071 -4.199254 -4.2250919 -4.2401142 -4.2419667 -4.2357244 -4.2284918 -4.2283053 -4.2312169][-4.1514053 -4.0845509 -4.0294409 -4.0136142 -4.0552063 -4.1253548 -4.1860285 -4.2221823 -4.2374439 -4.2438536 -4.2396941 -4.2362533 -4.2384529 -4.247642 -4.2562156][-4.1518807 -4.1003537 -4.0597548 -4.0588512 -4.1061411 -4.16818 -4.2160563 -4.2422824 -4.2505183 -4.249136 -4.2421784 -4.2432022 -4.2547536 -4.2695708 -4.2780547]]...]
INFO - root - 2017-12-07 12:01:40.172807: step 4410, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.115 sec/batch; 46h:14m:17s remains)
INFO - root - 2017-12-07 12:02:01.338645: step 4420, loss = 2.08, batch loss = 2.02 (15.9 examples/sec; 2.017 sec/batch; 44h:05m:37s remains)
INFO - root - 2017-12-07 12:02:22.522006: step 4430, loss = 2.07, batch loss = 2.02 (14.7 examples/sec; 2.183 sec/batch; 47h:43m:28s remains)
INFO - root - 2017-12-07 12:02:43.907087: step 4440, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.171 sec/batch; 47h:27m:37s remains)
INFO - root - 2017-12-07 12:03:05.093167: step 4450, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 46h:29m:46s remains)
INFO - root - 2017-12-07 12:03:25.903536: step 4460, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.175 sec/batch; 47h:31m:37s remains)
INFO - root - 2017-12-07 12:03:47.043219: step 4470, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 46h:30m:31s remains)
INFO - root - 2017-12-07 12:04:08.330948: step 4480, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.157 sec/batch; 47h:07m:04s remains)
INFO - root - 2017-12-07 12:04:29.272252: step 4490, loss = 2.07, batch loss = 2.01 (15.7 examples/sec; 2.041 sec/batch; 44h:35m:26s remains)
INFO - root - 2017-12-07 12:04:50.504830: step 4500, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.072 sec/batch; 45h:15m:11s remains)
2017-12-07 12:04:52.011580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2715831 -4.267097 -4.2637334 -4.2452621 -4.2145996 -4.1750536 -4.128613 -4.1203713 -4.1626153 -4.2111187 -4.2415252 -4.2557683 -4.2636366 -4.2708859 -4.2711182][-4.2859073 -4.2786522 -4.2746105 -4.2596774 -4.2295709 -4.191741 -4.143178 -4.1194754 -4.1498327 -4.1969733 -4.2321239 -4.2517266 -4.26038 -4.2672276 -4.26937][-4.2991967 -4.297987 -4.2962861 -4.2822113 -4.2556405 -4.2235985 -4.1753645 -4.1371961 -4.1544876 -4.1965189 -4.2316351 -4.253583 -4.260674 -4.2628646 -4.265449][-4.31069 -4.3150787 -4.3167419 -4.3004951 -4.2704968 -4.2371945 -4.1861753 -4.13758 -4.1448574 -4.1901517 -4.2310414 -4.2566147 -4.2611518 -4.2581635 -4.2611451][-4.3095407 -4.3177881 -4.3237767 -4.3055758 -4.2712522 -4.2304387 -4.1704469 -4.1103096 -4.1122975 -4.1654215 -4.2135506 -4.2444782 -4.2537565 -4.2553277 -4.2604709][-4.3035197 -4.3110013 -4.3177876 -4.3016953 -4.2677679 -4.2208719 -4.1522331 -4.0856762 -4.0835114 -4.1364937 -4.1840258 -4.2164054 -4.2367368 -4.2501535 -4.2624378][-4.3036819 -4.3064566 -4.3116264 -4.3003049 -4.2711997 -4.220788 -4.1443362 -4.0720611 -4.0600171 -4.1029239 -4.14296 -4.1763439 -4.2096066 -4.2374611 -4.2601876][-4.2993741 -4.2985272 -4.3023038 -4.2972269 -4.2777576 -4.23445 -4.1600766 -4.0812111 -4.0453267 -4.063201 -4.0916748 -4.1234827 -4.1718493 -4.2159004 -4.2475605][-4.2877531 -4.2845387 -4.287746 -4.2872071 -4.2815294 -4.2522287 -4.1903949 -4.1164179 -4.0642738 -4.0507021 -4.0637031 -4.0947242 -4.1525006 -4.2044783 -4.2364922][-4.2810984 -4.278789 -4.282475 -4.2854061 -4.28729 -4.2673464 -4.2187853 -4.1613584 -4.1136441 -4.0849385 -4.0821934 -4.1067581 -4.1646023 -4.2169156 -4.2450256][-4.2797089 -4.279047 -4.2829661 -4.288343 -4.2961354 -4.2855124 -4.2496843 -4.2076468 -4.1682348 -4.1375318 -4.1286116 -4.1446352 -4.1911211 -4.2394004 -4.2672243][-4.2797289 -4.2775583 -4.2784266 -4.2826481 -4.293911 -4.2929225 -4.2729564 -4.2459865 -4.2137852 -4.1831841 -4.1716561 -4.1824327 -4.2202277 -4.2654943 -4.2959328][-4.279819 -4.2746854 -4.2689357 -4.2678266 -4.2780061 -4.285152 -4.2802539 -4.2654257 -4.2390943 -4.2095485 -4.1992874 -4.21173 -4.2456822 -4.2879658 -4.318265][-4.2784233 -4.2676797 -4.2526064 -4.2429085 -4.249639 -4.2621894 -4.26784 -4.2621155 -4.2426543 -4.2182727 -4.2132692 -4.2334146 -4.264812 -4.2987518 -4.3235345][-4.278893 -4.264081 -4.242404 -4.2234883 -4.2230382 -4.2337151 -4.2420263 -4.2395782 -4.2262144 -4.2105041 -4.2140608 -4.24415 -4.2793083 -4.3094058 -4.3272262]]...]
INFO - root - 2017-12-07 12:05:13.349470: step 4510, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.096 sec/batch; 45h:45m:56s remains)
INFO - root - 2017-12-07 12:05:34.375173: step 4520, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.123 sec/batch; 46h:21m:04s remains)
INFO - root - 2017-12-07 12:05:55.357666: step 4530, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 45h:42m:35s remains)
INFO - root - 2017-12-07 12:06:16.662632: step 4540, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.106 sec/batch; 45h:58m:49s remains)
INFO - root - 2017-12-07 12:06:37.573919: step 4550, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 2.054 sec/batch; 44h:49m:37s remains)
INFO - root - 2017-12-07 12:06:58.521440: step 4560, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.145 sec/batch; 46h:48m:56s remains)
INFO - root - 2017-12-07 12:07:19.733193: step 4570, loss = 2.06, batch loss = 2.01 (14.8 examples/sec; 2.155 sec/batch; 47h:01m:40s remains)
INFO - root - 2017-12-07 12:07:40.943853: step 4580, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.148 sec/batch; 46h:52m:03s remains)
INFO - root - 2017-12-07 12:08:01.889384: step 4590, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.140 sec/batch; 46h:41m:21s remains)
INFO - root - 2017-12-07 12:08:23.079110: step 4600, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 45h:53m:41s remains)
2017-12-07 12:08:24.704562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.214952 -4.2121387 -4.2027612 -4.1986909 -4.2074838 -4.2175279 -4.2111096 -4.1708083 -4.1295767 -4.134027 -4.1484909 -4.1588421 -4.1812963 -4.2243776 -4.273859][-4.2198367 -4.2282577 -4.230526 -4.2328997 -4.2386522 -4.2399611 -4.2228131 -4.1769953 -4.1374555 -4.149456 -4.1703887 -4.1854229 -4.207232 -4.242414 -4.2835784][-4.2228551 -4.2366219 -4.24429 -4.2461939 -4.2426014 -4.2343082 -4.2116385 -4.1612911 -4.1216812 -4.1422338 -4.1746078 -4.1984234 -4.2239923 -4.2560797 -4.2922678][-4.2257581 -4.2392187 -4.2478118 -4.2440791 -4.2288151 -4.2111173 -4.1843743 -4.1325383 -4.0975275 -4.1290264 -4.1738148 -4.2018561 -4.2272615 -4.258976 -4.2938027][-4.2203331 -4.2307186 -4.2360516 -4.2283125 -4.2056746 -4.1831961 -4.1513634 -4.0919266 -4.0584722 -4.1019893 -4.1565852 -4.1874027 -4.2165236 -4.25191 -4.290216][-4.2047567 -4.2120223 -4.2158933 -4.2061782 -4.1758871 -4.1435084 -4.1002326 -4.0327005 -4.002336 -4.0608349 -4.1296597 -4.1695418 -4.2052245 -4.245153 -4.2872467][-4.1854749 -4.1944194 -4.1985669 -4.1811819 -4.1363692 -4.0849452 -4.0216641 -3.9368412 -3.9062324 -3.9847641 -4.0768228 -4.137043 -4.1865568 -4.233644 -4.2812762][-4.173183 -4.1864743 -4.1889644 -4.1622357 -4.1009245 -4.0250835 -3.9320166 -3.8224683 -3.7962964 -3.904037 -4.0225086 -4.103004 -4.165906 -4.2210951 -4.2722864][-4.1705627 -4.1806674 -4.178854 -4.1504211 -4.0903249 -4.0087028 -3.9048035 -3.7867236 -3.7607656 -3.8729169 -3.9944727 -4.0806479 -4.1499867 -4.2096934 -4.2622705][-4.1837049 -4.187839 -4.1814241 -4.1564889 -4.1111555 -4.0473495 -3.9688592 -3.8774564 -3.8485861 -3.9283824 -4.0217004 -4.0934844 -4.1544328 -4.2084455 -4.2566705][-4.2088013 -4.2098241 -4.2018523 -4.183125 -4.1563396 -4.1170459 -4.0670338 -3.9982409 -3.9640861 -4.0126667 -4.0767784 -4.1312094 -4.1806016 -4.2219477 -4.2604671][-4.2394381 -4.239707 -4.2333307 -4.2212706 -4.2076955 -4.1872191 -4.1565676 -4.104949 -4.0715504 -4.0992308 -4.1430073 -4.1848722 -4.2243052 -4.2519722 -4.2768745][-4.2718849 -4.268034 -4.2624125 -4.2568989 -4.25356 -4.2461586 -4.2289653 -4.1928911 -4.1666675 -4.1842523 -4.2148352 -4.2447948 -4.2721639 -4.286531 -4.2981935][-4.2865257 -4.2807016 -4.2753792 -4.2751274 -4.2800255 -4.2831984 -4.2757416 -4.2509041 -4.2300444 -4.2420378 -4.2644334 -4.2856541 -4.3049808 -4.313201 -4.31729][-4.2815394 -4.2761393 -4.2741323 -4.2782292 -4.2864342 -4.292892 -4.2885494 -4.2666283 -4.2450094 -4.253469 -4.2734737 -4.292469 -4.3114505 -4.3228483 -4.3285494]]...]
INFO - root - 2017-12-07 12:08:46.217640: step 4610, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.158 sec/batch; 47h:04m:03s remains)
INFO - root - 2017-12-07 12:09:07.150977: step 4620, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.148 sec/batch; 46h:50m:22s remains)
INFO - root - 2017-12-07 12:09:28.424545: step 4630, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.166 sec/batch; 47h:14m:13s remains)
INFO - root - 2017-12-07 12:09:49.615410: step 4640, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.135 sec/batch; 46h:32m:28s remains)
INFO - root - 2017-12-07 12:10:10.636355: step 4650, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.067 sec/batch; 45h:03m:21s remains)
INFO - root - 2017-12-07 12:10:31.726404: step 4660, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.078 sec/batch; 45h:16m:56s remains)
INFO - root - 2017-12-07 12:10:52.878839: step 4670, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.083 sec/batch; 45h:24m:12s remains)
INFO - root - 2017-12-07 12:11:13.760831: step 4680, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.137 sec/batch; 46h:34m:17s remains)
INFO - root - 2017-12-07 12:11:34.604503: step 4690, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.101 sec/batch; 45h:46m:29s remains)
INFO - root - 2017-12-07 12:11:55.936422: step 4700, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.100 sec/batch; 45h:44m:50s remains)
2017-12-07 12:11:57.443730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1735182 -4.1911659 -4.2017665 -4.2030735 -4.1909695 -4.1639891 -4.1389236 -4.1333227 -4.1486588 -4.1897969 -4.2296538 -4.2441626 -4.2290449 -4.1979408 -4.1758223][-4.2073193 -4.2198591 -4.2232437 -4.2173891 -4.2042508 -4.1833572 -4.1583633 -4.1429105 -4.1489229 -4.18324 -4.2221656 -4.2403293 -4.2346578 -4.2117915 -4.1911607][-4.2161207 -4.2244363 -4.2227364 -4.2119722 -4.1981339 -4.1820273 -4.1599169 -4.1395211 -4.1392183 -4.1679311 -4.2079468 -4.233222 -4.2397103 -4.2243924 -4.204145][-4.2062626 -4.2096648 -4.2022629 -4.1837096 -4.1663637 -4.15369 -4.135726 -4.110918 -4.1027555 -4.1208887 -4.1621671 -4.2020025 -4.2244096 -4.2184358 -4.2034621][-4.1949611 -4.1937509 -4.181293 -4.1601624 -4.1431508 -4.1352177 -4.1187534 -4.0865459 -4.0647326 -4.0646667 -4.0953708 -4.14597 -4.1890044 -4.1977344 -4.1887574][-4.1994834 -4.1995444 -4.1894135 -4.1760216 -4.1660538 -4.159883 -4.13895 -4.0983024 -4.0640087 -4.0405483 -4.0466661 -4.0943909 -4.15389 -4.1791115 -4.1758389][-4.2195306 -4.2244043 -4.2202206 -4.2137332 -4.2108226 -4.205792 -4.1827369 -4.1418152 -4.1022768 -4.0627918 -4.0452766 -4.0772219 -4.137804 -4.1717453 -4.1715021][-4.2322493 -4.2401509 -4.2416406 -4.2393103 -4.2413673 -4.2390118 -4.2181096 -4.1831703 -4.1484694 -4.111455 -4.0893025 -4.1070833 -4.1550078 -4.1899166 -4.1934657][-4.228477 -4.2330275 -4.2377367 -4.2414956 -4.2506509 -4.2546673 -4.2401123 -4.2148533 -4.1893587 -4.165936 -4.1511521 -4.1602845 -4.1918225 -4.2229676 -4.2329965][-4.2162066 -4.2114449 -4.2192011 -4.2306523 -4.245821 -4.2557845 -4.250309 -4.2354765 -4.2186656 -4.2059813 -4.1980834 -4.2025695 -4.2211533 -4.244246 -4.2571006][-4.2085223 -4.1969576 -4.205616 -4.2253613 -4.2452617 -4.2555704 -4.2552433 -4.2507577 -4.2419586 -4.2307115 -4.2237649 -4.2243681 -4.229269 -4.2391944 -4.2503586][-4.209022 -4.1932826 -4.2003317 -4.223866 -4.2450318 -4.2518039 -4.2530427 -4.2581425 -4.257616 -4.2461677 -4.2375822 -4.2314973 -4.2208948 -4.2144313 -4.21916][-4.2047 -4.1830654 -4.1856794 -4.20635 -4.2254367 -4.2307997 -4.23554 -4.2464385 -4.2509418 -4.2422862 -4.2353873 -4.2244124 -4.1984739 -4.1750994 -4.17249][-4.18701 -4.1633282 -4.15996 -4.174027 -4.1885858 -4.1959362 -4.2061348 -4.2173572 -4.2214694 -4.2181473 -4.2189636 -4.2102404 -4.1786666 -4.1466789 -4.1408544][-4.162632 -4.1443429 -4.1400614 -4.1495466 -4.1582603 -4.1651812 -4.1750336 -4.1786885 -4.17759 -4.179584 -4.1898413 -4.1908522 -4.1684489 -4.1408019 -4.1348553]]...]
INFO - root - 2017-12-07 12:12:18.590188: step 4710, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.103 sec/batch; 45h:48m:08s remains)
INFO - root - 2017-12-07 12:12:39.661775: step 4720, loss = 2.07, batch loss = 2.01 (14.4 examples/sec; 2.215 sec/batch; 48h:14m:49s remains)
INFO - root - 2017-12-07 12:13:00.784717: step 4730, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.129 sec/batch; 46h:22m:09s remains)
INFO - root - 2017-12-07 12:13:21.894810: step 4740, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.107 sec/batch; 45h:52m:18s remains)
INFO - root - 2017-12-07 12:13:42.815720: step 4750, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.162 sec/batch; 47h:04m:31s remains)
INFO - root - 2017-12-07 12:14:03.967555: step 4760, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 46h:01m:18s remains)
INFO - root - 2017-12-07 12:14:25.178220: step 4770, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.108 sec/batch; 45h:53m:08s remains)
INFO - root - 2017-12-07 12:14:46.072551: step 4780, loss = 2.09, batch loss = 2.03 (16.7 examples/sec; 1.914 sec/batch; 41h:38m:44s remains)
INFO - root - 2017-12-07 12:15:07.373929: step 4790, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 46h:22m:53s remains)
INFO - root - 2017-12-07 12:15:28.644880: step 4800, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 46h:18m:13s remains)
2017-12-07 12:15:30.239587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.332365 -4.3396549 -4.3425221 -4.3381872 -4.3341541 -4.3286033 -4.317 -4.3084893 -4.3095269 -4.3069191 -4.3051639 -4.3097405 -4.3156586 -4.3176012 -4.3184271][-4.3402014 -4.3422928 -4.3418078 -4.3346291 -4.3277807 -4.3172355 -4.2982006 -4.2838616 -4.28429 -4.2800331 -4.2738519 -4.2795215 -4.2927322 -4.3020034 -4.3085337][-4.3303871 -4.3306141 -4.3281307 -4.3202453 -4.3114748 -4.2930436 -4.263082 -4.2399812 -4.2401824 -4.2366929 -4.2301941 -4.2411408 -4.2640758 -4.2823648 -4.297884][-4.3021669 -4.30206 -4.2989407 -4.2907457 -4.2790785 -4.2523975 -4.2096033 -4.1786227 -4.1841927 -4.1909094 -4.1930747 -4.21144 -4.2418714 -4.2678356 -4.2902727][-4.2613759 -4.2595391 -4.2580824 -4.2508812 -4.2379251 -4.203795 -4.1448369 -4.1027074 -4.1150689 -4.1392274 -4.1568637 -4.1870413 -4.2256575 -4.2577095 -4.2857413][-4.2069845 -4.2007022 -4.202435 -4.1993685 -4.1882424 -4.150444 -4.0793991 -4.0268145 -4.042882 -4.0802145 -4.1127267 -4.1580143 -4.2070265 -4.2457204 -4.2793136][-4.1614151 -4.15013 -4.1539311 -4.1502581 -4.1365962 -4.0970864 -4.0255785 -3.9720175 -3.9905295 -4.0348978 -4.07841 -4.1363611 -4.1949244 -4.2392197 -4.2760072][-4.1576457 -4.1517825 -4.1579061 -4.1518993 -4.13502 -4.0950813 -4.0274448 -3.9770744 -3.9954338 -4.040906 -4.0856533 -4.1452174 -4.2037239 -4.247086 -4.2825289][-4.2049451 -4.2027583 -4.2062178 -4.1987929 -4.180963 -4.1436777 -4.0861969 -4.0442052 -4.0606794 -4.1012735 -4.1418443 -4.1935787 -4.2422404 -4.2754178 -4.3012447][-4.2595735 -4.2594609 -4.2614903 -4.2533917 -4.2350249 -4.2024951 -4.1582847 -4.1283937 -4.1426363 -4.1763039 -4.2102895 -4.2495804 -4.2836556 -4.3051105 -4.3195219][-4.2888341 -4.2916322 -4.2956161 -4.288712 -4.27364 -4.2478666 -4.214447 -4.1934047 -4.2057891 -4.2322445 -4.2591009 -4.2861876 -4.3070126 -4.3191767 -4.3267775][-4.2786789 -4.2906237 -4.3018436 -4.3025165 -4.2967954 -4.2793045 -4.2520013 -4.233613 -4.2405906 -4.259933 -4.2798924 -4.2978368 -4.3086452 -4.317143 -4.3233337][-4.2443633 -4.2661233 -4.2863884 -4.29855 -4.3034115 -4.2932334 -4.26592 -4.242034 -4.2420206 -4.2566557 -4.2725835 -4.2869053 -4.2950921 -4.3051443 -4.31314][-4.2177563 -4.2453275 -4.2702074 -4.2883883 -4.2966022 -4.2862697 -4.2553306 -4.2243161 -4.2198925 -4.2365637 -4.2549562 -4.2708011 -4.2804337 -4.2922482 -4.3025603][-4.2139387 -4.2409658 -4.2633333 -4.2795982 -4.2855706 -4.2706695 -4.2348638 -4.2003503 -4.1962547 -4.2187638 -4.2425623 -4.2613454 -4.2729688 -4.2871585 -4.2988811]]...]
INFO - root - 2017-12-07 12:15:51.417804: step 4810, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.104 sec/batch; 45h:46m:06s remains)
INFO - root - 2017-12-07 12:16:12.724403: step 4820, loss = 2.06, batch loss = 2.00 (14.8 examples/sec; 2.167 sec/batch; 47h:08m:16s remains)
INFO - root - 2017-12-07 12:16:33.912419: step 4830, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.141 sec/batch; 46h:33m:10s remains)
INFO - root - 2017-12-07 12:16:54.947232: step 4840, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 1.918 sec/batch; 41h:42m:26s remains)
INFO - root - 2017-12-07 12:17:16.017852: step 4850, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.063 sec/batch; 44h:51m:56s remains)
INFO - root - 2017-12-07 12:17:37.164380: step 4860, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 46h:03m:18s remains)
INFO - root - 2017-12-07 12:17:58.350735: step 4870, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.173 sec/batch; 47h:14m:30s remains)
INFO - root - 2017-12-07 12:18:19.102520: step 4880, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.101 sec/batch; 45h:39m:43s remains)
INFO - root - 2017-12-07 12:18:40.262352: step 4890, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.077 sec/batch; 45h:08m:45s remains)
INFO - root - 2017-12-07 12:19:01.541994: step 4900, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 46h:32m:40s remains)
2017-12-07 12:19:03.103546: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2217417 -4.2245836 -4.2264237 -4.2363086 -4.2483759 -4.2549806 -4.2542124 -4.2444835 -4.2242737 -4.2068682 -4.2062287 -4.2157407 -4.2225156 -4.2219949 -4.2192993][-4.20453 -4.2144613 -4.22876 -4.2481794 -4.2599883 -4.2610569 -4.253602 -4.2401567 -4.2182403 -4.2020454 -4.2032785 -4.2120314 -4.2191219 -4.2195988 -4.218709][-4.1789308 -4.1971712 -4.21616 -4.2361455 -4.2475471 -4.2502275 -4.2489028 -4.2443476 -4.2343831 -4.2280664 -4.225945 -4.2261505 -4.223906 -4.2183146 -4.2161446][-4.1680708 -4.1846123 -4.1972532 -4.2122169 -4.2268152 -4.2366261 -4.2440844 -4.2484441 -4.2493815 -4.2484527 -4.2429547 -4.2369876 -4.2277527 -4.2179804 -4.2148657][-4.1752162 -4.1857562 -4.1927543 -4.2036643 -4.2175069 -4.2267785 -4.2339044 -4.2347131 -4.2317581 -4.2321205 -4.2301173 -4.2298636 -4.2252069 -4.2181158 -4.2143235][-4.1887131 -4.1967778 -4.2008142 -4.2080054 -4.2147822 -4.217021 -4.2146239 -4.2016864 -4.1867266 -4.1873918 -4.198421 -4.2149768 -4.2231112 -4.2230673 -4.2232809][-4.2040439 -4.2108703 -4.2142715 -4.2119408 -4.2053657 -4.1944361 -4.1782331 -4.1466703 -4.1197658 -4.1280808 -4.1593018 -4.1930976 -4.2147083 -4.2259655 -4.2364731][-4.2118354 -4.2128634 -4.2105408 -4.19856 -4.1795893 -4.1550894 -4.1267462 -4.0789747 -4.0483618 -4.0693531 -4.1219654 -4.1738286 -4.2060237 -4.2256842 -4.2445087][-4.2159762 -4.2122664 -4.2028351 -4.1817641 -4.1519418 -4.1183105 -4.0857882 -4.0400338 -4.0186157 -4.0557475 -4.1220403 -4.1809926 -4.214335 -4.2316966 -4.2468843][-4.2157555 -4.2088366 -4.1945562 -4.1730089 -4.1438541 -4.1166639 -4.1017261 -4.0789537 -4.0732379 -4.1114011 -4.167974 -4.2148156 -4.23711 -4.2417355 -4.2480311][-4.2086945 -4.1984444 -4.1855974 -4.1740327 -4.1623635 -4.1542735 -4.1616759 -4.1608906 -4.1647968 -4.1919508 -4.2283173 -4.2562447 -4.2601409 -4.2490005 -4.2449284][-4.2124267 -4.1982541 -4.185576 -4.1823907 -4.1869678 -4.1964426 -4.2170844 -4.229876 -4.2394543 -4.2557955 -4.2741976 -4.2843695 -4.2718124 -4.2491407 -4.2370253][-4.2350945 -4.2201881 -4.2078176 -4.2058024 -4.2103944 -4.2246723 -4.2512031 -4.2718663 -4.2858329 -4.2969255 -4.30158 -4.2967587 -4.2794137 -4.2625971 -4.2521605][-4.2675595 -4.2567797 -4.2440157 -4.2373176 -4.2370758 -4.2484317 -4.2745562 -4.29846 -4.3139734 -4.3216438 -4.3181829 -4.3089576 -4.2959647 -4.2880559 -4.280736][-4.2998195 -4.2936797 -4.2805338 -4.2710791 -4.2683029 -4.2758093 -4.294765 -4.3118839 -4.3203197 -4.3239632 -4.3226228 -4.3158817 -4.3065877 -4.3020687 -4.2951441]]...]
INFO - root - 2017-12-07 12:19:23.948780: step 4910, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.151 sec/batch; 46h:44m:10s remains)
INFO - root - 2017-12-07 12:19:45.166928: step 4920, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.080 sec/batch; 45h:10m:43s remains)
INFO - root - 2017-12-07 12:20:06.372046: step 4930, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.090 sec/batch; 45h:23m:12s remains)
INFO - root - 2017-12-07 12:20:27.365394: step 4940, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 46h:12m:26s remains)
INFO - root - 2017-12-07 12:20:48.445983: step 4950, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.140 sec/batch; 46h:28m:03s remains)
INFO - root - 2017-12-07 12:21:09.965083: step 4960, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.183 sec/batch; 47h:23m:44s remains)
INFO - root - 2017-12-07 12:21:31.013266: step 4970, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.179 sec/batch; 47h:18m:47s remains)
INFO - root - 2017-12-07 12:21:51.914044: step 4980, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.084 sec/batch; 45h:14m:16s remains)
INFO - root - 2017-12-07 12:22:13.224441: step 4990, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 46h:02m:55s remains)
INFO - root - 2017-12-07 12:22:34.495773: step 5000, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.142 sec/batch; 46h:29m:24s remains)
2017-12-07 12:22:36.067735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1599979 -4.1483049 -4.130444 -4.1369443 -4.1642814 -4.17805 -4.1743979 -4.1766043 -4.1778774 -4.1699424 -4.1522355 -4.1390314 -4.1489449 -4.1680841 -4.1792569][-4.1446309 -4.139708 -4.1219244 -4.1270676 -4.1519809 -4.1679435 -4.1704531 -4.1703963 -4.1681461 -4.1576719 -4.1396661 -4.1275244 -4.13851 -4.1573272 -4.1692243][-4.1376085 -4.1401215 -4.1291337 -4.136734 -4.1565752 -4.1683059 -4.1699829 -4.1675797 -4.1640954 -4.1518903 -4.1265731 -4.1100888 -4.121314 -4.1421876 -4.1566248][-4.1452804 -4.1559591 -4.1508636 -4.15878 -4.1704416 -4.1696916 -4.16109 -4.1532092 -4.1515808 -4.1462021 -4.1235743 -4.1047387 -4.1157269 -4.14089 -4.1556253][-4.1450534 -4.1608272 -4.1634445 -4.1669092 -4.1638803 -4.1431341 -4.1170783 -4.10439 -4.1111374 -4.1225877 -4.1187949 -4.1090031 -4.1241469 -4.1540194 -4.1730418][-4.1319571 -4.1472616 -4.1531167 -4.1479664 -4.120285 -4.0712214 -4.0254736 -4.0175424 -4.0472322 -4.08344 -4.1065755 -4.1127381 -4.1370959 -4.178359 -4.2092524][-4.1295848 -4.1430678 -4.1495967 -4.1397405 -4.0954123 -4.022501 -3.9620569 -3.9604962 -4.0100932 -4.0674949 -4.1108437 -4.1340818 -4.1632032 -4.2063565 -4.2386866][-4.1400585 -4.1517963 -4.1583676 -4.152791 -4.1180172 -4.0579295 -4.0058541 -4.0045485 -4.0506468 -4.1072965 -4.1534324 -4.181468 -4.2084274 -4.2408075 -4.2647285][-4.1542606 -4.1648331 -4.1753812 -4.1780834 -4.1595078 -4.1190147 -4.079473 -4.0751343 -4.1107078 -4.1596537 -4.1991277 -4.2280493 -4.2555656 -4.2798996 -4.2905579][-4.1837535 -4.1921744 -4.2055817 -4.2143922 -4.207737 -4.1819663 -4.152276 -4.1440692 -4.1685672 -4.207058 -4.23737 -4.2637382 -4.2882237 -4.3048797 -4.3078265][-4.213244 -4.2196546 -4.2385674 -4.2543545 -4.2588015 -4.2468834 -4.2314839 -4.2259026 -4.2406507 -4.2651587 -4.284039 -4.3016853 -4.3151727 -4.3222394 -4.317378][-4.2383389 -4.2436881 -4.2659416 -4.2881308 -4.2995343 -4.2969108 -4.2935781 -4.2945151 -4.3026857 -4.3144522 -4.3228865 -4.33067 -4.3349504 -4.3333397 -4.32358][-4.2617636 -4.2669187 -4.2867389 -4.3066659 -4.31901 -4.3209639 -4.3208308 -4.3210726 -4.32436 -4.3298626 -4.3333845 -4.336369 -4.3363605 -4.3309569 -4.3194485][-4.2769938 -4.2791433 -4.292881 -4.3084517 -4.3181729 -4.3208418 -4.3198071 -4.3174243 -4.3164735 -4.3168149 -4.316813 -4.3172178 -4.3170767 -4.3149281 -4.308507][-4.2895865 -4.2857952 -4.2906818 -4.2979317 -4.3016858 -4.3010731 -4.2997427 -4.2989044 -4.298696 -4.299191 -4.3003631 -4.30147 -4.3019547 -4.3020983 -4.2999806]]...]
INFO - root - 2017-12-07 12:22:56.885327: step 5010, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.109 sec/batch; 45h:46m:09s remains)
INFO - root - 2017-12-07 12:23:18.029833: step 5020, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.131 sec/batch; 46h:13m:35s remains)
INFO - root - 2017-12-07 12:23:39.361997: step 5030, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.138 sec/batch; 46h:22m:55s remains)
INFO - root - 2017-12-07 12:24:00.258471: step 5040, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.098 sec/batch; 45h:30m:23s remains)
INFO - root - 2017-12-07 12:24:21.351773: step 5050, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.122 sec/batch; 46h:00m:48s remains)
INFO - root - 2017-12-07 12:24:42.604011: step 5060, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 45h:37m:26s remains)
INFO - root - 2017-12-07 12:25:03.660884: step 5070, loss = 2.07, batch loss = 2.01 (15.7 examples/sec; 2.040 sec/batch; 44h:13m:41s remains)
INFO - root - 2017-12-07 12:25:24.868866: step 5080, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.182 sec/batch; 47h:18m:00s remains)
INFO - root - 2017-12-07 12:25:46.032823: step 5090, loss = 2.09, batch loss = 2.03 (14.7 examples/sec; 2.172 sec/batch; 47h:05m:22s remains)
INFO - root - 2017-12-07 12:26:07.220100: step 5100, loss = 2.06, batch loss = 2.01 (14.9 examples/sec; 2.142 sec/batch; 46h:25m:17s remains)
2017-12-07 12:26:08.821801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3456712 -4.3420177 -4.3388357 -4.3360343 -4.3338442 -4.3314619 -4.3292346 -4.3275375 -4.3242598 -4.322958 -4.3271809 -4.3339405 -4.3425488 -4.3496447 -4.3519092][-4.3343267 -4.3283033 -4.3231778 -4.3162727 -4.310257 -4.303299 -4.2947187 -4.2860036 -4.2785473 -4.2784443 -4.2865453 -4.2993493 -4.3154325 -4.328506 -4.3367286][-4.3198271 -4.3129578 -4.3050017 -4.29101 -4.2759271 -4.262085 -4.2442107 -4.2265024 -4.2167296 -4.2220325 -4.2391748 -4.2615685 -4.2860303 -4.3046169 -4.3183417][-4.3049059 -4.2980876 -4.2837687 -4.25704 -4.2276492 -4.20518 -4.1755342 -4.1491156 -4.1386704 -4.1492143 -4.1756668 -4.2075377 -4.2400293 -4.2665224 -4.2895627][-4.2852383 -4.2739372 -4.2498107 -4.2096038 -4.1658678 -4.1317081 -4.0899053 -4.057806 -4.0502515 -4.0666113 -4.1007524 -4.1430726 -4.1821384 -4.2162385 -4.2505741][-4.2616072 -4.23767 -4.201139 -4.1465864 -4.0839968 -4.0273261 -3.9686725 -3.938941 -3.9486787 -3.9808047 -4.0263505 -4.0803523 -4.1277261 -4.17081 -4.2157574][-4.238925 -4.1961904 -4.1429381 -4.0737052 -3.9897316 -3.8999166 -3.8154373 -3.7944503 -3.8406088 -3.9024732 -3.966109 -4.0327373 -4.0889921 -4.1417427 -4.1968131][-4.2183328 -4.1591229 -4.09096 -4.0130329 -3.9101877 -3.785305 -3.6748357 -3.6694489 -3.7552321 -3.8472943 -3.9266787 -3.9982622 -4.057353 -4.1163597 -4.1811318][-4.2063541 -4.1435256 -4.073287 -3.9987149 -3.8975272 -3.7715921 -3.6718898 -3.6817467 -3.7712195 -3.8599572 -3.9332042 -3.9960036 -4.0525246 -4.1131511 -4.1819654][-4.2123079 -4.1581278 -4.1022849 -4.046433 -3.9760084 -3.8943148 -3.8340762 -3.837652 -3.8909836 -3.9495137 -4.006361 -4.0567341 -4.1055522 -4.1577153 -4.2159715][-4.2333417 -4.1911278 -4.1534019 -4.1206508 -4.0845642 -4.0438223 -4.0119638 -4.0091643 -4.0323753 -4.06532 -4.1030812 -4.1416173 -4.1791215 -4.2161484 -4.2553997][-4.2625256 -4.2308197 -4.2079592 -4.1931362 -4.1789255 -4.1608963 -4.144887 -4.138823 -4.1475215 -4.1647835 -4.18592 -4.2139158 -4.242888 -4.2689991 -4.2919345][-4.2914009 -4.2687049 -4.254395 -4.2490611 -4.2458153 -4.2390518 -4.23001 -4.2265944 -4.2328367 -4.2419362 -4.2519774 -4.2691121 -4.2890949 -4.3070726 -4.3202338][-4.3140564 -4.2977133 -4.2870994 -4.2844677 -4.2851529 -4.2847257 -4.2815638 -4.282258 -4.2879658 -4.2923269 -4.29618 -4.3048139 -4.3164268 -4.3269234 -4.3346443][-4.3316135 -4.3220034 -4.31466 -4.3115811 -4.3124495 -4.3149252 -4.3158336 -4.3173189 -4.3204913 -4.322298 -4.3238363 -4.3272176 -4.3318295 -4.336359 -4.340302]]...]
INFO - root - 2017-12-07 12:26:29.976970: step 5110, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 45h:33m:11s remains)
INFO - root - 2017-12-07 12:26:50.952209: step 5120, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 45h:32m:27s remains)
INFO - root - 2017-12-07 12:27:12.002045: step 5130, loss = 2.08, batch loss = 2.02 (16.3 examples/sec; 1.959 sec/batch; 42h:26m:32s remains)
INFO - root - 2017-12-07 12:27:33.117041: step 5140, loss = 2.08, batch loss = 2.03 (14.6 examples/sec; 2.192 sec/batch; 47h:29m:28s remains)
INFO - root - 2017-12-07 12:27:54.273074: step 5150, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.112 sec/batch; 45h:44m:15s remains)
INFO - root - 2017-12-07 12:28:15.476500: step 5160, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.089 sec/batch; 45h:14m:26s remains)
INFO - root - 2017-12-07 12:28:36.522239: step 5170, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.111 sec/batch; 45h:42m:27s remains)
INFO - root - 2017-12-07 12:28:57.699447: step 5180, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.147 sec/batch; 46h:29m:37s remains)
INFO - root - 2017-12-07 12:29:18.729027: step 5190, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 45h:47m:43s remains)
INFO - root - 2017-12-07 12:29:39.465794: step 5200, loss = 2.09, batch loss = 2.03 (14.8 examples/sec; 2.157 sec/batch; 46h:41m:31s remains)
2017-12-07 12:29:41.019876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2133245 -4.2017531 -4.21378 -4.2415442 -4.2718191 -4.299427 -4.3203464 -4.3376465 -4.3483968 -4.3521476 -4.3548961 -4.3547125 -4.3514004 -4.3470674 -4.3420906][-4.1586533 -4.1413951 -4.1486592 -4.17665 -4.2090435 -4.2405972 -4.270432 -4.3007073 -4.3253961 -4.3409872 -4.3539081 -4.36136 -4.3612785 -4.3570786 -4.350605][-4.1231885 -4.0972013 -4.0990467 -4.1220584 -4.152216 -4.1796103 -4.2071152 -4.2418485 -4.2763882 -4.3055043 -4.33222 -4.3518414 -4.3615956 -4.3623667 -4.3571854][-4.097856 -4.06492 -4.0604415 -4.0774956 -4.1056128 -4.1265411 -4.1467028 -4.179431 -4.2149487 -4.2502608 -4.2889929 -4.3225179 -4.347661 -4.3593941 -4.3594012][-4.1020608 -4.0670424 -4.0523605 -4.0508394 -4.0622358 -4.0664225 -4.0773096 -4.1136909 -4.1527376 -4.1910148 -4.2374177 -4.2822652 -4.3218689 -4.3465419 -4.3550105][-4.1416612 -4.1094656 -4.0850153 -4.0576611 -4.0302248 -3.9961545 -3.9896243 -4.0331812 -4.0865607 -4.1350255 -4.1896296 -4.2444715 -4.2930927 -4.3275785 -4.3446374][-4.2071185 -4.1780338 -4.1451812 -4.0987568 -4.0383306 -3.9614267 -3.9200597 -3.9565816 -4.0196543 -4.0791583 -4.1446486 -4.2105818 -4.2654324 -4.3046031 -4.3272562][-4.2794242 -4.2552156 -4.2179947 -4.1622758 -4.0917988 -4.0008855 -3.9320064 -3.931313 -3.9720585 -4.02633 -4.0977211 -4.1745949 -4.2355647 -4.2772765 -4.3042569][-4.3357515 -4.3165379 -4.279995 -4.2230673 -4.1551518 -4.0725656 -4.0009332 -3.970027 -3.9730067 -3.9999022 -4.0604119 -4.1393023 -4.2038364 -4.2483521 -4.279325][-4.3625331 -4.3509426 -4.3224297 -4.2740045 -4.215107 -4.1455927 -4.0826926 -4.0397167 -4.0199862 -4.0220432 -4.0573936 -4.1208491 -4.1817312 -4.226634 -4.2602429][-4.3598418 -4.3541079 -4.3362436 -4.3053684 -4.2635083 -4.213407 -4.1619363 -4.1204214 -4.0942469 -4.0829053 -4.0935535 -4.132319 -4.1814079 -4.2227788 -4.2556119][-4.3374691 -4.3395567 -4.3332338 -4.3183889 -4.2938828 -4.261322 -4.2236528 -4.1896625 -4.1660562 -4.1516228 -4.1490393 -4.1688886 -4.2041669 -4.238812 -4.26756][-4.3054538 -4.3156724 -4.3196878 -4.3164926 -4.3033414 -4.2803478 -4.2504768 -4.2221084 -4.2020555 -4.1881075 -4.1827497 -4.1951995 -4.22292 -4.2529421 -4.2773342][-4.2819443 -4.2956882 -4.304523 -4.305995 -4.2968497 -4.27627 -4.2478256 -4.2220416 -4.2045307 -4.1925039 -4.1886039 -4.1982522 -4.2210932 -4.2473359 -4.2688417][-4.2737103 -4.2885585 -4.2990079 -4.3014894 -4.2923646 -4.2709022 -4.2410169 -4.2122617 -4.1920781 -4.1790209 -4.1748805 -4.1819072 -4.19979 -4.22147 -4.2396917]]...]
INFO - root - 2017-12-07 12:30:02.275649: step 5210, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.140 sec/batch; 46h:19m:15s remains)
INFO - root - 2017-12-07 12:30:23.615010: step 5220, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.180 sec/batch; 47h:11m:06s remains)
INFO - root - 2017-12-07 12:30:44.576810: step 5230, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.097 sec/batch; 45h:22m:07s remains)
INFO - root - 2017-12-07 12:31:05.830182: step 5240, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.108 sec/batch; 45h:36m:49s remains)
INFO - root - 2017-12-07 12:31:27.045072: step 5250, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.164 sec/batch; 46h:48m:24s remains)
INFO - root - 2017-12-07 12:31:48.029250: step 5260, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.089 sec/batch; 45h:10m:34s remains)
INFO - root - 2017-12-07 12:32:08.966968: step 5270, loss = 2.08, batch loss = 2.03 (14.8 examples/sec; 2.159 sec/batch; 46h:40m:55s remains)
INFO - root - 2017-12-07 12:32:30.348019: step 5280, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.130 sec/batch; 46h:02m:52s remains)
INFO - root - 2017-12-07 12:32:51.742651: step 5290, loss = 2.07, batch loss = 2.02 (14.7 examples/sec; 2.170 sec/batch; 46h:55m:30s remains)
INFO - root - 2017-12-07 12:33:12.550812: step 5300, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.081 sec/batch; 44h:58m:49s remains)
2017-12-07 12:33:14.129649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1376514 -4.1529255 -4.1732869 -4.1902089 -4.1992531 -4.2038059 -4.2090216 -4.2138448 -4.2227902 -4.2310348 -4.2378349 -4.239078 -4.2324448 -4.2279305 -4.2214141][-4.1823983 -4.1966228 -4.2113943 -4.222971 -4.2261057 -4.2243266 -4.224781 -4.2275891 -4.2298965 -4.2312021 -4.229969 -4.2218146 -4.2118936 -4.2049303 -4.200489][-4.2143621 -4.2193952 -4.2244163 -4.227191 -4.22559 -4.2209406 -4.219523 -4.2214351 -4.2203803 -4.2187991 -4.2136378 -4.2035875 -4.1942554 -4.1863809 -4.1804643][-4.2079253 -4.2100935 -4.2192621 -4.2219567 -4.2212 -4.2163 -4.2143664 -4.2145338 -4.2103343 -4.2041488 -4.1960158 -4.1873107 -4.1780591 -4.1704187 -4.1688485][-4.2009687 -4.2057371 -4.2174716 -4.22104 -4.2197995 -4.213892 -4.2079339 -4.2028389 -4.1922493 -4.1794667 -4.1704874 -4.16771 -4.1668224 -4.1741743 -4.1829643][-4.1877089 -4.1928148 -4.2016878 -4.2084761 -4.2110686 -4.2059751 -4.1960435 -4.1850982 -4.1727796 -4.1627641 -4.1629066 -4.1752105 -4.1860585 -4.1994348 -4.2094603][-4.1642118 -4.1694193 -4.1814361 -4.1962113 -4.2023463 -4.1954689 -4.1780024 -4.1592646 -4.1495595 -4.14451 -4.1551852 -4.1847143 -4.2036614 -4.2147369 -4.2234321][-4.1431031 -4.1572556 -4.1736188 -4.1819587 -4.1726122 -4.1488347 -4.1138172 -4.0810885 -4.07292 -4.0837359 -4.1100259 -4.15849 -4.1912546 -4.2051663 -4.2161236][-4.14561 -4.1566143 -4.1589985 -4.1370635 -4.094429 -4.0485854 -4.0048141 -3.9809871 -3.999321 -4.040606 -4.0852919 -4.1482425 -4.1926413 -4.2117491 -4.2180591][-4.1514568 -4.1439748 -4.1262932 -4.0727363 -4.0021358 -3.9654436 -3.9704919 -3.9987471 -4.0483742 -4.0981097 -4.1397181 -4.1861258 -4.2177019 -4.2245431 -4.2157454][-4.1401892 -4.124083 -4.1095939 -4.0693183 -4.0266523 -4.0351577 -4.0814276 -4.1251583 -4.158114 -4.1842465 -4.2028313 -4.221108 -4.230227 -4.2259955 -4.2084265][-4.142396 -4.1393895 -4.1480622 -4.1463852 -4.1371717 -4.150733 -4.1843815 -4.211638 -4.2232823 -4.2320623 -4.2424593 -4.2500353 -4.2462316 -4.2376056 -4.2184768][-4.163528 -4.1715884 -4.1914582 -4.2064 -4.2064176 -4.2094741 -4.2218294 -4.2346621 -4.2429786 -4.2522078 -4.2660022 -4.2745862 -4.2727308 -4.2644758 -4.2447619][-4.1721187 -4.1843953 -4.2080994 -4.2286797 -4.2307019 -4.2250166 -4.226522 -4.2343264 -4.2475557 -4.2637897 -4.2794075 -4.2886333 -4.2886086 -4.2797055 -4.2582359][-4.1758604 -4.1891842 -4.2120752 -4.2342067 -4.2301631 -4.2178922 -4.2170177 -4.2230763 -4.2447062 -4.2696452 -4.2882519 -4.2948437 -4.2935877 -4.2826114 -4.2637234]]...]
INFO - root - 2017-12-07 12:33:35.428195: step 5310, loss = 2.09, batch loss = 2.04 (14.9 examples/sec; 2.153 sec/batch; 46h:31m:58s remains)
INFO - root - 2017-12-07 12:33:56.516042: step 5320, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.108 sec/batch; 45h:33m:25s remains)
INFO - root - 2017-12-07 12:34:17.123884: step 5330, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.111 sec/batch; 45h:37m:40s remains)
INFO - root - 2017-12-07 12:34:38.177141: step 5340, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.133 sec/batch; 46h:05m:28s remains)
INFO - root - 2017-12-07 12:34:59.317404: step 5350, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 46h:01m:31s remains)
INFO - root - 2017-12-07 12:35:20.177860: step 5360, loss = 2.07, batch loss = 2.02 (16.7 examples/sec; 1.913 sec/batch; 41h:19m:00s remains)
INFO - root - 2017-12-07 12:35:41.446904: step 5370, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.155 sec/batch; 46h:32m:58s remains)
INFO - root - 2017-12-07 12:36:02.699110: step 5380, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.076 sec/batch; 44h:49m:26s remains)
INFO - root - 2017-12-07 12:36:23.687687: step 5390, loss = 2.08, batch loss = 2.02 (15.6 examples/sec; 2.047 sec/batch; 44h:11m:26s remains)
INFO - root - 2017-12-07 12:36:44.609895: step 5400, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.150 sec/batch; 46h:24m:57s remains)
2017-12-07 12:36:46.118524: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2041621 -4.1898546 -4.1850996 -4.182261 -4.1854215 -4.1974869 -4.2125626 -4.2230358 -4.2308674 -4.2332492 -4.2350225 -4.2432466 -4.2530251 -4.2476559 -4.2338557][-4.2276082 -4.2158408 -4.2081585 -4.1989746 -4.1973357 -4.2074571 -4.2165756 -4.2187767 -4.2183042 -4.2200637 -4.2280307 -4.2440782 -4.2575827 -4.2590251 -4.2502995][-4.2586555 -4.2514477 -4.2455997 -4.2348418 -4.2290344 -4.2308702 -4.221674 -4.206738 -4.1966777 -4.2001557 -4.221499 -4.253026 -4.2718959 -4.2752047 -4.2677031][-4.2855 -4.2791686 -4.2774892 -4.2687535 -4.2579551 -4.2462039 -4.2129602 -4.1774049 -4.1600547 -4.1727781 -4.2194147 -4.2734933 -4.3003392 -4.3022313 -4.2911816][-4.2996173 -4.2896705 -4.2862086 -4.2743278 -4.2557769 -4.2272038 -4.169641 -4.1184759 -4.1069627 -4.1434064 -4.2150416 -4.2897348 -4.3253603 -4.3251162 -4.3078661][-4.3079028 -4.2932124 -4.2828703 -4.2615614 -4.2311945 -4.182795 -4.1008277 -4.0338788 -4.0347629 -4.10492 -4.19977 -4.2879415 -4.3301773 -4.3320684 -4.3149619][-4.3091764 -4.2937632 -4.275784 -4.2458682 -4.2012844 -4.1321197 -4.0215383 -3.924346 -3.9329305 -4.0454636 -4.1700268 -4.2658892 -4.3092737 -4.3167272 -4.3053727][-4.3052483 -4.290688 -4.2691221 -4.2342224 -4.181 -4.0987129 -3.9689674 -3.8474803 -3.8584709 -3.9933743 -4.1338639 -4.2302985 -4.2719254 -4.2823095 -4.2798467][-4.3015981 -4.2851224 -4.2608743 -4.2288508 -4.1784463 -4.1020317 -3.9881303 -3.889833 -3.9114754 -4.0291309 -4.1453185 -4.2218518 -4.2516 -4.2594624 -4.260828][-4.29586 -4.2737541 -4.2499413 -4.2276063 -4.1894975 -4.1263485 -4.0421214 -3.9888546 -4.026094 -4.1134758 -4.1931868 -4.2397885 -4.2514491 -4.2539592 -4.2580814][-4.3031163 -4.2750735 -4.2509718 -4.2341709 -4.2062263 -4.1559267 -4.0954223 -4.0704551 -4.1140227 -4.1823559 -4.238122 -4.2671432 -4.267036 -4.2613549 -4.263958][-4.3140254 -4.2880259 -4.2679873 -4.2527776 -4.2250881 -4.1849265 -4.1438031 -4.139607 -4.1869063 -4.2403984 -4.2749505 -4.2911625 -4.2845654 -4.2770615 -4.281064][-4.3267579 -4.3090682 -4.2930155 -4.2752995 -4.2471342 -4.2139978 -4.1889553 -4.1966491 -4.2419081 -4.2832656 -4.2990832 -4.3000846 -4.2904878 -4.2889156 -4.2981277][-4.32833 -4.3187766 -4.3058305 -4.2893019 -4.2662106 -4.2435436 -4.23303 -4.2448225 -4.2811942 -4.3105159 -4.3189225 -4.3120494 -4.2983103 -4.2951894 -4.3033514][-4.3120923 -4.3086543 -4.2989659 -4.284492 -4.2728968 -4.2661314 -4.2667561 -4.2799978 -4.3023844 -4.322042 -4.32947 -4.3235726 -4.3072143 -4.2960587 -4.2944775]]...]
INFO - root - 2017-12-07 12:37:07.242167: step 5410, loss = 2.10, batch loss = 2.04 (15.2 examples/sec; 2.107 sec/batch; 45h:29m:02s remains)
INFO - root - 2017-12-07 12:37:28.380514: step 5420, loss = 2.07, batch loss = 2.01 (16.1 examples/sec; 1.992 sec/batch; 42h:59m:12s remains)
INFO - root - 2017-12-07 12:37:49.282494: step 5430, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 45h:45m:17s remains)
INFO - root - 2017-12-07 12:38:10.550222: step 5440, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.134 sec/batch; 46h:02m:55s remains)
INFO - root - 2017-12-07 12:38:31.823005: step 5450, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.101 sec/batch; 45h:19m:30s remains)
INFO - root - 2017-12-07 12:38:52.854698: step 5460, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.147 sec/batch; 46h:18m:36s remains)
INFO - root - 2017-12-07 12:39:13.990419: step 5470, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.116 sec/batch; 45h:38m:33s remains)
INFO - root - 2017-12-07 12:39:35.042520: step 5480, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.149 sec/batch; 46h:21m:34s remains)
INFO - root - 2017-12-07 12:39:55.926895: step 5490, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.063 sec/batch; 44h:29m:52s remains)
INFO - root - 2017-12-07 12:40:17.236681: step 5500, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 46h:01m:48s remains)
2017-12-07 12:40:18.798924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2869897 -4.2685261 -4.2453742 -4.232553 -4.2264452 -4.2243743 -4.2183971 -4.2066283 -4.1971426 -4.2015214 -4.2078004 -4.1999731 -4.1947494 -4.1891823 -4.1652775][-4.2852468 -4.2658443 -4.2404346 -4.2264781 -4.2174869 -4.2124038 -4.2066097 -4.1967854 -4.1870656 -4.1948986 -4.2078881 -4.2026281 -4.1988964 -4.1903977 -4.1649175][-4.2891436 -4.2712631 -4.2448773 -4.2258625 -4.206049 -4.1886835 -4.1755581 -4.16584 -4.1579652 -4.1690159 -4.190897 -4.194397 -4.1894369 -4.178915 -4.1585035][-4.2917213 -4.2737064 -4.2462053 -4.2230334 -4.1923537 -4.1613283 -4.1409769 -4.1301084 -4.1239791 -4.1386003 -4.1672273 -4.1810007 -4.1772356 -4.1637754 -4.1471419][-4.2893567 -4.2691727 -4.2399683 -4.2154341 -4.180584 -4.1377645 -4.1090927 -4.09518 -4.0866966 -4.1018786 -4.1336656 -4.1547132 -4.1497979 -4.1241665 -4.104177][-4.2836652 -4.2605653 -4.22933 -4.2049427 -4.164722 -4.1084723 -4.0680761 -4.0517759 -4.0433278 -4.063128 -4.0967164 -4.12326 -4.1167097 -4.0806117 -4.0587115][-4.2776041 -4.2509637 -4.2166429 -4.1875105 -4.1427269 -4.0763092 -4.025538 -4.0137138 -4.0220695 -4.0543551 -4.0934582 -4.1247892 -4.1234484 -4.0907955 -4.0669513][-4.2694054 -4.2383881 -4.1980968 -4.1662397 -4.1250634 -4.0629606 -4.0219398 -4.0340152 -4.0730815 -4.1189904 -4.1570354 -4.1808138 -4.17976 -4.1518421 -4.1236973][-4.2600594 -4.2283659 -4.1927915 -4.1728978 -4.1480055 -4.1075058 -4.0840635 -4.1026855 -4.1443172 -4.1893573 -4.2239218 -4.2419291 -4.2410803 -4.217679 -4.1910534][-4.2552228 -4.2287312 -4.2047577 -4.1990719 -4.1910572 -4.1722736 -4.1586671 -4.1712666 -4.2020788 -4.2374563 -4.2614975 -4.270823 -4.2701788 -4.2551193 -4.2381654][-4.2548923 -4.2365661 -4.223928 -4.22986 -4.2313471 -4.2266455 -4.2197919 -4.2251611 -4.245996 -4.2678428 -4.2745533 -4.2743444 -4.2717853 -4.2607741 -4.2481465][-4.2595439 -4.2461181 -4.2405443 -4.248713 -4.2528849 -4.2564111 -4.2545543 -4.2546186 -4.265008 -4.2727175 -4.2646 -4.2557039 -4.2509727 -4.24227 -4.2298555][-4.2661071 -4.25447 -4.248384 -4.2521038 -4.2546945 -4.2621417 -4.2621536 -4.2573037 -4.2589045 -4.2572422 -4.2422681 -4.2310619 -4.2278957 -4.221364 -4.2063594][-4.2762566 -4.2645879 -4.2542696 -4.2507396 -4.2493753 -4.2550044 -4.2517681 -4.2442913 -4.2422438 -4.2374034 -4.221715 -4.2117338 -4.21254 -4.210453 -4.199666][-4.2852869 -4.2737904 -4.2593269 -4.2484112 -4.2410073 -4.2389474 -4.2315879 -4.2255039 -4.2258897 -4.2266121 -4.2183185 -4.2135458 -4.2176552 -4.2208948 -4.2175441]]...]
INFO - root - 2017-12-07 12:40:39.929494: step 5510, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 45h:39m:31s remains)
INFO - root - 2017-12-07 12:41:00.967164: step 5520, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.079 sec/batch; 44h:49m:00s remains)
INFO - root - 2017-12-07 12:41:22.030181: step 5530, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.088 sec/batch; 44h:59m:49s remains)
INFO - root - 2017-12-07 12:41:43.061368: step 5540, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.063 sec/batch; 44h:27m:03s remains)
INFO - root - 2017-12-07 12:42:03.730417: step 5550, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 1.782 sec/batch; 38h:23m:56s remains)
INFO - root - 2017-12-07 12:42:24.794695: step 5560, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.138 sec/batch; 46h:03m:17s remains)
INFO - root - 2017-12-07 12:42:46.070401: step 5570, loss = 2.06, batch loss = 2.01 (15.2 examples/sec; 2.107 sec/batch; 45h:23m:23s remains)
INFO - root - 2017-12-07 12:43:07.176818: step 5580, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.120 sec/batch; 45h:39m:58s remains)
INFO - root - 2017-12-07 12:43:28.085941: step 5590, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.080 sec/batch; 44h:48m:15s remains)
INFO - root - 2017-12-07 12:43:49.281054: step 5600, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 45h:40m:24s remains)
2017-12-07 12:43:50.835574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3164768 -4.3147326 -4.3021731 -4.2857013 -4.268867 -4.2543459 -4.249186 -4.2535791 -4.265974 -4.2817307 -4.2948456 -4.2991648 -4.2957478 -4.289608 -4.2891054][-4.305233 -4.3064947 -4.2934666 -4.2732067 -4.2515216 -4.2333941 -4.2234616 -4.2272439 -4.2424893 -4.2601089 -4.2775145 -4.283145 -4.2756743 -4.2634106 -4.2598495][-4.2893791 -4.2961397 -4.2886643 -4.2705317 -4.2470465 -4.2219071 -4.2065105 -4.2083631 -4.222497 -4.2400351 -4.2602816 -4.2687435 -4.261138 -4.2452993 -4.2377696][-4.26456 -4.2775421 -4.2773018 -4.2667875 -4.246922 -4.2183204 -4.1950903 -4.19072 -4.2001143 -4.2149458 -4.2398939 -4.2545295 -4.2498126 -4.233254 -4.2226062][-4.2243795 -4.2378669 -4.2417326 -4.2443256 -4.2360258 -4.2119813 -4.184207 -4.1691179 -4.1754775 -4.19004 -4.2191648 -4.2422581 -4.2410922 -4.2253947 -4.2123413][-4.1743608 -4.1809835 -4.1857667 -4.1969247 -4.1961927 -4.1734414 -4.1408181 -4.1198468 -4.1289058 -4.1494384 -4.1852937 -4.2210784 -4.2274065 -4.2176738 -4.2070866][-4.1294417 -4.1238914 -4.1243925 -4.1340909 -4.1282806 -4.0986495 -4.0558662 -4.0269122 -4.0407858 -4.0767221 -4.1226726 -4.1691332 -4.1914172 -4.198257 -4.1976662][-4.1100917 -4.0906682 -4.07661 -4.0719795 -4.0556722 -4.0151987 -3.9546659 -3.9063897 -3.9171386 -3.9714136 -4.0316448 -4.0896535 -4.1312041 -4.156817 -4.1711736][-4.1299105 -4.10367 -4.0755329 -4.0508475 -4.0172176 -3.9635663 -3.8931351 -3.8310478 -3.8301649 -3.8856404 -3.9515185 -4.0140352 -4.0663118 -4.10296 -4.1299543][-4.1670322 -4.1477237 -4.1210704 -4.0896139 -4.045043 -3.9829507 -3.9157457 -3.8594656 -3.8495674 -3.8883982 -3.9447193 -4.0003924 -4.0469546 -4.0808177 -4.1122761][-4.195694 -4.1850119 -4.1699047 -4.1455512 -4.1062827 -4.048789 -3.9921927 -3.9512761 -3.9407024 -3.9647503 -4.0063477 -4.0482655 -4.0826421 -4.107336 -4.1350722][-4.2332287 -4.2291827 -4.222033 -4.206007 -4.1767607 -4.1358132 -4.0988159 -4.0743418 -4.0638814 -4.07402 -4.1018515 -4.1312642 -4.1543112 -4.1707554 -4.1908031][-4.2789607 -4.2775435 -4.2727489 -4.2631364 -4.246161 -4.2251139 -4.2057376 -4.1895733 -4.1784911 -4.1784911 -4.1940427 -4.2137814 -4.2288189 -4.2411656 -4.2537003][-4.3134203 -4.31462 -4.3124084 -4.3068423 -4.297739 -4.2871151 -4.2766938 -4.2656674 -4.2568784 -4.2542224 -4.2608638 -4.2708673 -4.2803874 -4.2897186 -4.2970529][-4.3278875 -4.3300056 -4.330049 -4.3275943 -4.3220882 -4.3158431 -4.3092184 -4.3015532 -4.2968154 -4.2955656 -4.2977104 -4.3018818 -4.3079591 -4.314219 -4.3183737]]...]
INFO - root - 2017-12-07 12:44:11.947859: step 5610, loss = 2.06, batch loss = 2.01 (15.4 examples/sec; 2.083 sec/batch; 44h:51m:01s remains)
INFO - root - 2017-12-07 12:44:32.680322: step 5620, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 2.078 sec/batch; 44h:43m:52s remains)
INFO - root - 2017-12-07 12:44:53.953620: step 5630, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.143 sec/batch; 46h:08m:29s remains)
INFO - root - 2017-12-07 12:45:14.873976: step 5640, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.110 sec/batch; 45h:25m:01s remains)
INFO - root - 2017-12-07 12:45:35.746807: step 5650, loss = 2.08, batch loss = 2.03 (15.7 examples/sec; 2.034 sec/batch; 43h:46m:56s remains)
INFO - root - 2017-12-07 12:45:56.636595: step 5660, loss = 2.09, batch loss = 2.03 (15.5 examples/sec; 2.062 sec/batch; 44h:22m:15s remains)
INFO - root - 2017-12-07 12:46:17.850402: step 5670, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.131 sec/batch; 45h:50m:31s remains)
INFO - root - 2017-12-07 12:46:38.741819: step 5680, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.132 sec/batch; 45h:52m:30s remains)
INFO - root - 2017-12-07 12:46:59.678630: step 5690, loss = 2.09, batch loss = 2.04 (15.0 examples/sec; 2.130 sec/batch; 45h:49m:07s remains)
INFO - root - 2017-12-07 12:47:20.967256: step 5700, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.136 sec/batch; 45h:55m:45s remains)
2017-12-07 12:47:22.528900: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3201928 -4.3124142 -4.3051763 -4.2991495 -4.2863708 -4.2659211 -4.2564564 -4.2637854 -4.2758307 -4.28686 -4.2884908 -4.2819347 -4.2776752 -4.2725644 -4.2796412][-4.3138838 -4.297215 -4.2812066 -4.2703223 -4.25425 -4.2309532 -4.2214656 -4.2366233 -4.2559905 -4.2675505 -4.2688775 -4.2635279 -4.2639766 -4.2592931 -4.2637382][-4.3117409 -4.2885923 -4.2655478 -4.2523541 -4.2365675 -4.2132821 -4.2016673 -4.2240834 -4.2517042 -4.2681665 -4.270247 -4.2636862 -4.2632594 -4.2598553 -4.2593904][-4.3018775 -4.2756886 -4.2532349 -4.2432046 -4.2286539 -4.2037921 -4.1880221 -4.2128682 -4.2459249 -4.2694 -4.2725472 -4.2614241 -4.2592554 -4.2595725 -4.2571831][-4.2788916 -4.2479105 -4.2213426 -4.2105784 -4.1929021 -4.1651936 -4.1495714 -4.1825995 -4.2279668 -4.258316 -4.2564673 -4.2381406 -4.2358108 -4.2440972 -4.2451787][-4.2527208 -4.2139931 -4.1663523 -4.1349387 -4.1034284 -4.0724039 -4.0641851 -4.11355 -4.1845059 -4.2281718 -4.22496 -4.2031069 -4.2062306 -4.2261171 -4.2345929][-4.2422433 -4.1966724 -4.1280465 -4.0669222 -4.0223117 -3.9938385 -3.9928563 -4.0463433 -4.125433 -4.175725 -4.1762362 -4.15877 -4.1665616 -4.1991634 -4.2205567][-4.24607 -4.2074537 -4.1419568 -4.0742526 -4.0232234 -3.9921322 -3.9712877 -3.9929681 -4.0494447 -4.0886769 -4.0950675 -4.094636 -4.1162791 -4.1619821 -4.1995564][-4.2506533 -4.2250214 -4.1812959 -4.1260681 -4.0765138 -4.0302444 -3.9707851 -3.9410737 -3.9677212 -3.9956241 -4.0151377 -4.0422578 -4.08326 -4.1404181 -4.1868114][-4.2670279 -4.2498593 -4.223567 -4.1893749 -4.1551208 -4.1057076 -4.0288 -3.9696147 -3.977134 -4.00188 -4.0300083 -4.0699182 -4.1126132 -4.1637769 -4.2043028][-4.2986679 -4.283195 -4.2639565 -4.2425489 -4.2241607 -4.1882906 -4.1241436 -4.0678258 -4.0707903 -4.0970225 -4.1231413 -4.1547375 -4.18462 -4.2199273 -4.2488937][-4.3268523 -4.3133583 -4.2983174 -4.282918 -4.2714629 -4.252203 -4.2146425 -4.1808314 -4.1842332 -4.2059565 -4.2230744 -4.2425094 -4.2609229 -4.2819366 -4.2994905][-4.3371696 -4.3277297 -4.3181925 -4.3112187 -4.305687 -4.2972531 -4.2819786 -4.2718911 -4.2808542 -4.2954316 -4.3048773 -4.3148108 -4.3231459 -4.3318791 -4.3377118][-4.3335962 -4.3274646 -4.3213897 -4.3202891 -4.3193693 -4.3192387 -4.3182383 -4.3195491 -4.3285375 -4.3392615 -4.347486 -4.3531017 -4.3553476 -4.357688 -4.3580317][-4.3343549 -4.3318305 -4.3284626 -4.32819 -4.3281817 -4.3297095 -4.3319592 -4.3353567 -4.340898 -4.3477573 -4.3540692 -4.3573108 -4.35993 -4.3627844 -4.3641658]]...]
INFO - root - 2017-12-07 12:47:43.698774: step 5710, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.088 sec/batch; 44h:53m:28s remains)
INFO - root - 2017-12-07 12:48:04.613770: step 5720, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 45h:32m:01s remains)
INFO - root - 2017-12-07 12:48:25.782228: step 5730, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.095 sec/batch; 45h:02m:20s remains)
INFO - root - 2017-12-07 12:48:46.998168: step 5740, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 45h:20m:03s remains)
INFO - root - 2017-12-07 12:49:07.825145: step 5750, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.105 sec/batch; 45h:14m:27s remains)
INFO - root - 2017-12-07 12:49:29.227724: step 5760, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.126 sec/batch; 45h:41m:48s remains)
INFO - root - 2017-12-07 12:49:50.317844: step 5770, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.123 sec/batch; 45h:36m:34s remains)
INFO - root - 2017-12-07 12:50:11.201053: step 5780, loss = 2.07, batch loss = 2.01 (15.7 examples/sec; 2.034 sec/batch; 43h:42m:08s remains)
INFO - root - 2017-12-07 12:50:32.195960: step 5790, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 45h:33m:06s remains)
INFO - root - 2017-12-07 12:50:53.404856: step 5800, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.092 sec/batch; 44h:55m:38s remains)
2017-12-07 12:50:54.984324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3349786 -4.3364129 -4.3349628 -4.3297262 -4.3208365 -4.3125505 -4.3072405 -4.3043337 -4.303247 -4.3082619 -4.3141041 -4.3177023 -4.3209085 -4.3210993 -4.3192782][-4.3407679 -4.3414226 -4.337563 -4.3268313 -4.3093767 -4.2904458 -4.2775073 -4.2731261 -4.2735343 -4.28601 -4.301023 -4.3104448 -4.3158484 -4.316906 -4.3152933][-4.3449316 -4.3435311 -4.335207 -4.3147883 -4.2851377 -4.2529111 -4.2302532 -4.2234793 -4.2268214 -4.2468758 -4.2743092 -4.2939715 -4.3057227 -4.3099451 -4.3094][-4.3490715 -4.3432121 -4.3261147 -4.2928071 -4.2502255 -4.2034698 -4.1647792 -4.1567712 -4.1697612 -4.1969771 -4.2324476 -4.2617331 -4.2828188 -4.2974844 -4.301918][-4.3502026 -4.336729 -4.3107352 -4.2677622 -4.2127409 -4.1469092 -4.0821095 -4.0714049 -4.1005588 -4.1348658 -4.1747894 -4.2143111 -4.2465024 -4.2747569 -4.2868567][-4.3441162 -4.323307 -4.2909613 -4.2412014 -4.173111 -4.0804672 -3.9803414 -3.9604039 -4.0128217 -4.0621409 -4.1130385 -4.1658611 -4.2100253 -4.2527227 -4.2740045][-4.33512 -4.309164 -4.2713227 -4.2143521 -4.1301723 -4.0091467 -3.8666399 -3.8324451 -3.9235306 -4.0068693 -4.0766015 -4.143434 -4.1946015 -4.2429824 -4.2686815][-4.3261428 -4.2951689 -4.254447 -4.1934781 -4.0970621 -3.9578896 -3.7938058 -3.7616844 -3.8991318 -4.021781 -4.1059022 -4.1741519 -4.215878 -4.2499766 -4.26824][-4.3170648 -4.2822132 -4.2405534 -4.18248 -4.0904374 -3.972882 -3.8545084 -3.8571768 -3.9898729 -4.1066713 -4.1781182 -4.2310548 -4.2594609 -4.2736335 -4.2751679][-4.3107824 -4.2767959 -4.2369456 -4.1887746 -4.119772 -4.0502453 -4.0015635 -4.0291896 -4.1207914 -4.1979294 -4.2408314 -4.2683549 -4.2847481 -4.288435 -4.2774997][-4.3107076 -4.2832308 -4.2519732 -4.2189078 -4.1751051 -4.1388326 -4.1250339 -4.1524086 -4.2074056 -4.2531314 -4.2738919 -4.2811761 -4.2865233 -4.2853131 -4.2710686][-4.314971 -4.2989669 -4.2808862 -4.2622051 -4.233077 -4.20781 -4.2001181 -4.2169237 -4.2474165 -4.2778821 -4.28836 -4.279665 -4.273345 -4.2719116 -4.266098][-4.3197527 -4.3137851 -4.3050284 -4.2952714 -4.2726278 -4.2503839 -4.2411666 -4.2466679 -4.2620029 -4.2782555 -4.2819996 -4.2662673 -4.2552667 -4.2570057 -4.2569408][-4.3194818 -4.3169684 -4.3133984 -4.3069677 -4.2886019 -4.2653179 -4.2497344 -4.2464976 -4.2473779 -4.2529278 -4.2543855 -4.2402697 -4.2297473 -4.2319617 -4.2315021][-4.317678 -4.316186 -4.3150897 -4.3087044 -4.2903347 -4.2641664 -4.2399764 -4.2320905 -4.229526 -4.2349615 -4.2384686 -4.2279415 -4.2157769 -4.2127137 -4.2094421]]...]
INFO - root - 2017-12-07 12:51:15.919843: step 5810, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.078 sec/batch; 44h:38m:16s remains)
INFO - root - 2017-12-07 12:51:36.644593: step 5820, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 45h:21m:48s remains)
INFO - root - 2017-12-07 12:51:57.799650: step 5830, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 45h:09m:29s remains)
INFO - root - 2017-12-07 12:52:18.810084: step 5840, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 1.948 sec/batch; 41h:49m:38s remains)
INFO - root - 2017-12-07 12:52:39.694339: step 5850, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.084 sec/batch; 44h:44m:04s remains)
INFO - root - 2017-12-07 12:53:00.811645: step 5860, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.099 sec/batch; 45h:02m:52s remains)
INFO - root - 2017-12-07 12:53:22.002975: step 5870, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 45h:39m:27s remains)
INFO - root - 2017-12-07 12:53:42.723572: step 5880, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.070 sec/batch; 44h:24m:23s remains)
INFO - root - 2017-12-07 12:54:03.755378: step 5890, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 45h:07m:12s remains)
INFO - root - 2017-12-07 12:54:24.994034: step 5900, loss = 2.09, batch loss = 2.03 (14.8 examples/sec; 2.165 sec/batch; 46h:26m:18s remains)
2017-12-07 12:54:26.504869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3098373 -4.3062329 -4.3024473 -4.2977285 -4.2935567 -4.2888455 -4.2837319 -4.2787561 -4.2759428 -4.2795935 -4.2873559 -4.2915883 -4.2919731 -4.2971277 -4.3043][-4.3072963 -4.3034377 -4.2992659 -4.2932372 -4.2866988 -4.2774687 -4.265388 -4.2542229 -4.2490945 -4.2545223 -4.267168 -4.2740741 -4.2755289 -4.2821851 -4.2902789][-4.299706 -4.2936139 -4.2860508 -4.2746797 -4.2608967 -4.2426372 -4.2194419 -4.20088 -4.1944652 -4.2054496 -4.2266812 -4.2430229 -4.2516952 -4.2613769 -4.2710061][-4.2926478 -4.2837381 -4.271111 -4.2532883 -4.2281127 -4.19449 -4.1557274 -4.1297927 -4.1244164 -4.1438541 -4.1777568 -4.2045946 -4.2203188 -4.2324634 -4.2440886][-4.2852688 -4.2766957 -4.2629557 -4.2400503 -4.202096 -4.1490383 -4.0932488 -4.0574484 -4.0570474 -4.0916677 -4.1381855 -4.1702552 -4.1885571 -4.2030244 -4.2174406][-4.2741575 -4.2678356 -4.256938 -4.2309446 -4.1767869 -4.1000252 -4.0208278 -3.9696598 -3.9861329 -4.0528417 -4.1147556 -4.1492591 -4.166358 -4.1823373 -4.1982131][-4.2661152 -4.2645907 -4.2610369 -4.2366018 -4.1674318 -4.0624733 -3.949034 -3.870595 -3.9074616 -4.014864 -4.0967097 -4.1374264 -4.1563859 -4.1746054 -4.1922059][-4.266274 -4.2762194 -4.2855396 -4.2681808 -4.193254 -4.0668297 -3.9222512 -3.8171473 -3.8669329 -4.0024529 -4.0997939 -4.1453309 -4.164022 -4.1834168 -4.2003646][-4.2652731 -4.2851338 -4.3104038 -4.3060727 -4.2420011 -4.1225209 -3.9866648 -3.8957913 -3.9412048 -4.0571613 -4.1422081 -4.1796842 -4.1909533 -4.2052145 -4.2182431][-4.2373552 -4.2628069 -4.3033576 -4.3160329 -4.2695251 -4.1717587 -4.0660572 -4.0056272 -4.043653 -4.1314979 -4.1948018 -4.2164507 -4.2137218 -4.21865 -4.2301927][-4.1970048 -4.2262506 -4.2832131 -4.3100467 -4.2815909 -4.20828 -4.1336784 -4.0970945 -4.1281943 -4.1902337 -4.2295008 -4.2303696 -4.210865 -4.2071867 -4.2227392][-4.1669917 -4.1990609 -4.2656512 -4.3052111 -4.2937574 -4.2453089 -4.1981192 -4.1746459 -4.1934853 -4.2317295 -4.2472277 -4.228332 -4.1993437 -4.193131 -4.215086][-4.1555715 -4.1872931 -4.2504773 -4.2954 -4.2961922 -4.26666 -4.2376647 -4.2234387 -4.2370429 -4.2617021 -4.2611923 -4.2286353 -4.1954393 -4.1912508 -4.2179351][-4.1668496 -4.1972976 -4.2489552 -4.290463 -4.2980595 -4.2811861 -4.262455 -4.25566 -4.2653008 -4.2797451 -4.2733536 -4.2393374 -4.2095432 -4.2082028 -4.2357397][-4.2082777 -4.2326169 -4.2708335 -4.3025761 -4.3109179 -4.3016586 -4.28966 -4.285553 -4.2902126 -4.2963738 -4.2892451 -4.2626896 -4.2421074 -4.2427564 -4.2656069]]...]
INFO - root - 2017-12-07 12:54:47.491742: step 5910, loss = 2.06, batch loss = 2.01 (15.2 examples/sec; 2.112 sec/batch; 45h:17m:44s remains)
INFO - root - 2017-12-07 12:55:08.675756: step 5920, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.110 sec/batch; 45h:15m:36s remains)
INFO - root - 2017-12-07 12:55:29.890673: step 5930, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.092 sec/batch; 44h:52m:03s remains)
INFO - root - 2017-12-07 12:55:51.032842: step 5940, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.101 sec/batch; 45h:03m:10s remains)
INFO - root - 2017-12-07 12:56:11.779815: step 5950, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 45h:37m:44s remains)
INFO - root - 2017-12-07 12:56:32.945128: step 5960, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.099 sec/batch; 44h:59m:33s remains)
INFO - root - 2017-12-07 12:56:53.919777: step 5970, loss = 2.09, batch loss = 2.03 (16.2 examples/sec; 1.975 sec/batch; 42h:19m:29s remains)
INFO - root - 2017-12-07 12:57:14.892282: step 5980, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.161 sec/batch; 46h:18m:32s remains)
INFO - root - 2017-12-07 12:57:36.064616: step 5990, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.181 sec/batch; 46h:43m:21s remains)
INFO - root - 2017-12-07 12:57:57.322582: step 6000, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.108 sec/batch; 45h:10m:05s remains)
2017-12-07 12:57:58.852573: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2393236 -4.2381325 -4.2387252 -4.239511 -4.2409286 -4.2434778 -4.2476315 -4.2529936 -4.2577558 -4.2620411 -4.265564 -4.2678418 -4.2677727 -4.2657962 -4.2637053][-4.2263165 -4.2240849 -4.2232857 -4.2214866 -4.2197075 -4.2201519 -4.2244449 -4.232667 -4.2409978 -4.2484608 -4.2550325 -4.2596602 -4.2608786 -4.2585697 -4.2553916][-4.2119365 -4.2093768 -4.2072659 -4.2015553 -4.1955042 -4.1933904 -4.1974363 -4.2090154 -4.222362 -4.2339635 -4.2423034 -4.2471409 -4.248621 -4.2456856 -4.2402406][-4.1834826 -4.1802983 -4.1771736 -4.1670012 -4.1537843 -4.1447721 -4.1446724 -4.1582541 -4.179585 -4.1996355 -4.2136216 -4.2212253 -4.2251444 -4.2235117 -4.2174921][-4.1418858 -4.1409216 -4.1380968 -4.1251693 -4.107007 -4.0905027 -4.0832567 -4.0972853 -4.1282954 -4.1601639 -4.1822586 -4.1930156 -4.1997237 -4.200181 -4.1938553][-4.0853572 -4.0864415 -4.0881581 -4.0788469 -4.0609131 -4.0386529 -4.024591 -4.0361114 -4.0747228 -4.1177859 -4.1472087 -4.1596212 -4.1665964 -4.1686316 -4.1628022][-4.0630922 -4.0619283 -4.0634584 -4.0543246 -4.0341334 -4.0084743 -3.991271 -3.9988689 -4.0374207 -4.08365 -4.1137333 -4.1231413 -4.1263442 -4.1281762 -4.1220341][-4.0909758 -4.0889115 -4.0869455 -4.0731354 -4.0491128 -4.0232363 -4.0074534 -4.0107903 -4.0406971 -4.0779247 -4.099679 -4.100534 -4.0966625 -4.0939531 -4.0836005][-4.1182127 -4.1178465 -4.1149216 -4.1013718 -4.0815372 -4.064992 -4.0564146 -4.0572958 -4.0750408 -4.0996647 -4.1128054 -4.1071944 -4.099299 -4.0938792 -4.0815268][-4.1353793 -4.1384816 -4.1400638 -4.1344686 -4.1262407 -4.1221156 -4.1208863 -4.1165438 -4.1166124 -4.1248856 -4.1312857 -4.12552 -4.1187611 -4.1163535 -4.1099954][-4.14465 -4.1508117 -4.1596589 -4.1631322 -4.1655364 -4.1730533 -4.1775775 -4.169292 -4.1558967 -4.1516833 -4.1518097 -4.147099 -4.14223 -4.1429157 -4.1425228][-4.1457849 -4.1522284 -4.1661787 -4.1756735 -4.1827006 -4.1935334 -4.19899 -4.1901336 -4.1740088 -4.1672297 -4.1687031 -4.1698017 -4.1696873 -4.1726146 -4.1740265][-4.1538072 -4.1569929 -4.170002 -4.1793323 -4.1853828 -4.19333 -4.19743 -4.1910877 -4.1801362 -4.1754265 -4.1788454 -4.1843433 -4.1877546 -4.1914015 -4.1932006][-4.1653194 -4.1641216 -4.1736846 -4.179348 -4.1801429 -4.1828179 -4.1850328 -4.1821656 -4.1805391 -4.1825032 -4.1883116 -4.1954689 -4.2005563 -4.202002 -4.2002773][-4.1762061 -4.1706624 -4.1748338 -4.1757326 -4.1729274 -4.1723027 -4.1737456 -4.1739507 -4.1795712 -4.1875978 -4.1954789 -4.2044997 -4.2110939 -4.2114663 -4.2068386]]...]
INFO - root - 2017-12-07 12:58:19.605484: step 6010, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.112 sec/batch; 45h:13m:54s remains)
INFO - root - 2017-12-07 12:58:41.006198: step 6020, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.139 sec/batch; 45h:48m:13s remains)
INFO - root - 2017-12-07 12:59:02.198855: step 6030, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.093 sec/batch; 44h:48m:59s remains)
INFO - root - 2017-12-07 12:59:23.126759: step 6040, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.080 sec/batch; 44h:32m:40s remains)
INFO - root - 2017-12-07 12:59:44.476057: step 6050, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.162 sec/batch; 46h:16m:46s remains)
INFO - root - 2017-12-07 13:00:05.874903: step 6060, loss = 2.08, batch loss = 2.03 (15.3 examples/sec; 2.095 sec/batch; 44h:50m:18s remains)
INFO - root - 2017-12-07 13:00:26.720011: step 6070, loss = 2.08, batch loss = 2.02 (15.9 examples/sec; 2.010 sec/batch; 43h:00m:44s remains)
INFO - root - 2017-12-07 13:00:48.194288: step 6080, loss = 2.08, batch loss = 2.03 (15.3 examples/sec; 2.096 sec/batch; 44h:50m:59s remains)
INFO - root - 2017-12-07 13:01:09.342769: step 6090, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.168 sec/batch; 46h:23m:06s remains)
INFO - root - 2017-12-07 13:01:30.447345: step 6100, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 45h:12m:56s remains)
2017-12-07 13:01:32.083205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.283042 -4.2395234 -4.1911297 -4.1578836 -4.1395316 -4.1197195 -4.1018462 -4.0940919 -4.1178246 -4.1512694 -4.156219 -4.1502357 -4.1617589 -4.1719327 -4.1777864][-4.2717128 -4.2240534 -4.1827664 -4.1637182 -4.15717 -4.1436472 -4.124207 -4.1096249 -4.1287994 -4.1602616 -4.1602726 -4.1475182 -4.1529841 -4.1640429 -4.1729746][-4.2581167 -4.2092252 -4.1761847 -4.1736426 -4.18126 -4.1758642 -4.1580133 -4.1363068 -4.1450028 -4.1655664 -4.1585789 -4.1415763 -4.142849 -4.1544189 -4.163662][-4.2452722 -4.197475 -4.1713037 -4.1778197 -4.1898117 -4.1867728 -4.1697373 -4.1447573 -4.143539 -4.1533957 -4.1453567 -4.1333661 -4.1340985 -4.1443486 -4.154254][-4.2397089 -4.1915545 -4.1651778 -4.1704988 -4.1769428 -4.169117 -4.1490269 -4.1168313 -4.1098037 -4.1174679 -4.1154823 -4.1102042 -4.1095543 -4.1181593 -4.1283989][-4.2380342 -4.1880274 -4.1569552 -4.1553907 -4.1516933 -4.1286144 -4.0866752 -4.0321689 -4.01925 -4.0393329 -4.0534029 -4.0581675 -4.0580535 -4.0697603 -4.0898237][-4.2396359 -4.190537 -4.1563945 -4.1410046 -4.1126103 -4.0585775 -3.9759684 -3.8757455 -3.85193 -3.8997447 -3.9500601 -3.9848669 -4.0106039 -4.0450768 -4.0770483][-4.2412519 -4.1956272 -4.1623864 -4.1375546 -4.0888548 -4.0053525 -3.8816359 -3.7333312 -3.6943228 -3.7800941 -3.8766003 -3.9501615 -4.0048227 -4.0528789 -4.0876131][-4.2405605 -4.2006011 -4.1734519 -4.1535535 -4.1093721 -4.029676 -3.9138525 -3.7816157 -3.7536764 -3.8390574 -3.9267519 -3.9953039 -4.0460067 -4.0847797 -4.1101542][-4.2389174 -4.201663 -4.17791 -4.1681037 -4.1436877 -4.091753 -4.01967 -3.9386156 -3.9240489 -3.9777021 -4.0229039 -4.0598941 -4.0906897 -4.1169167 -4.1339951][-4.2344041 -4.1959381 -4.1718869 -4.1716738 -4.1696544 -4.1459227 -4.1105685 -4.0631442 -4.0502181 -4.0782967 -4.094439 -4.1077108 -4.1260581 -4.150692 -4.1677284][-4.2270164 -4.1877894 -4.165381 -4.1757989 -4.193974 -4.1905217 -4.1776185 -4.1488032 -4.1307459 -4.1390791 -4.143733 -4.1483178 -4.1638136 -4.1863837 -4.2027988][-4.2289004 -4.195189 -4.1797328 -4.1988196 -4.2318435 -4.2441616 -4.2448812 -4.2217855 -4.1938224 -4.1840715 -4.1817575 -4.1863461 -4.2038274 -4.2261629 -4.2404027][-4.242568 -4.2176647 -4.20976 -4.2328858 -4.2709785 -4.2950668 -4.3017354 -4.2771931 -4.2397327 -4.2197676 -4.2156563 -4.2201538 -4.2368517 -4.2556005 -4.2651286][-4.2627525 -4.2418714 -4.2353907 -4.2516274 -4.280426 -4.3008213 -4.3034286 -4.2846541 -4.25658 -4.2395668 -4.2343478 -4.2368975 -4.2484522 -4.2619948 -4.2690582]]...]
INFO - root - 2017-12-07 13:01:53.004655: step 6110, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 45h:14m:21s remains)
INFO - root - 2017-12-07 13:02:14.360794: step 6120, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.106 sec/batch; 45h:03m:25s remains)
INFO - root - 2017-12-07 13:02:35.261101: step 6130, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 1.928 sec/batch; 41h:14m:31s remains)
INFO - root - 2017-12-07 13:02:56.321166: step 6140, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.110 sec/batch; 45h:06m:40s remains)
INFO - root - 2017-12-07 13:03:17.610736: step 6150, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 45h:22m:03s remains)
INFO - root - 2017-12-07 13:03:38.808136: step 6160, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 45h:20m:20s remains)
INFO - root - 2017-12-07 13:03:59.806292: step 6170, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 45h:30m:59s remains)
INFO - root - 2017-12-07 13:04:21.106267: step 6180, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.189 sec/batch; 46h:47m:09s remains)
INFO - root - 2017-12-07 13:04:42.429303: step 6190, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.161 sec/batch; 46h:10m:30s remains)
INFO - root - 2017-12-07 13:05:03.281208: step 6200, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.113 sec/batch; 45h:08m:48s remains)
2017-12-07 13:05:04.828617: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2057257 -4.2123165 -4.2195034 -4.2221651 -4.2173395 -4.215013 -4.2103763 -4.2046103 -4.1935525 -4.1869011 -4.198936 -4.2251816 -4.2449269 -4.2456784 -4.2495584][-4.2091842 -4.2134948 -4.2197671 -4.2277827 -4.2341352 -4.2416363 -4.245574 -4.2449355 -4.236268 -4.2309632 -4.23739 -4.2525873 -4.2583179 -4.2459192 -4.2391629][-4.2192488 -4.220325 -4.2234468 -4.2337084 -4.2457919 -4.2587843 -4.2672958 -4.2680345 -4.2609539 -4.2551241 -4.25368 -4.2559462 -4.2522068 -4.2340417 -4.2213464][-4.2323213 -4.2321367 -4.2338853 -4.2453461 -4.2599168 -4.2731957 -4.2779322 -4.2739749 -4.26426 -4.2568269 -4.2495432 -4.2420335 -4.2351685 -4.219069 -4.2069945][-4.2425132 -4.241776 -4.2396393 -4.2485175 -4.263237 -4.2730465 -4.269701 -4.258255 -4.2447691 -4.2369933 -4.2313933 -4.2222214 -4.216568 -4.2055445 -4.2004251][-4.2462206 -4.2429414 -4.2359338 -4.237237 -4.247242 -4.2552543 -4.2456207 -4.2251377 -4.2061009 -4.2031841 -4.2087579 -4.2046142 -4.1989422 -4.1918597 -4.1929679][-4.2367187 -4.2350507 -4.2285848 -4.2224269 -4.2245088 -4.2291608 -4.2164726 -4.1856251 -4.1534562 -4.1520452 -4.1744876 -4.186758 -4.1855826 -4.1816821 -4.188467][-4.2105665 -4.2132959 -4.2117667 -4.2032475 -4.2000003 -4.2021127 -4.1875443 -4.149961 -4.1013613 -4.0884671 -4.1259289 -4.1651869 -4.1798759 -4.1864562 -4.19945][-4.1822009 -4.1896315 -4.1932859 -4.1862597 -4.1799088 -4.1760182 -4.1593342 -4.1177287 -4.052989 -4.0197377 -4.0698748 -4.1374016 -4.1732221 -4.1918321 -4.2086263][-4.1718178 -4.1800566 -4.1836057 -4.1760526 -4.1666222 -4.1572409 -4.1410789 -4.1050887 -4.0386953 -3.9935522 -4.0405483 -4.11592 -4.1604905 -4.1886649 -4.2107058][-4.1954474 -4.2036219 -4.2050338 -4.194334 -4.1794772 -4.1654577 -4.1507187 -4.1267848 -4.07976 -4.0405188 -4.0623288 -4.1144028 -4.151444 -4.1800718 -4.206511][-4.236 -4.2458768 -4.2421174 -4.2236824 -4.1985769 -4.1755929 -4.1520872 -4.1290565 -4.0979624 -4.0684476 -4.0752006 -4.1091352 -4.1420832 -4.1725831 -4.2012587][-4.2724023 -4.2790289 -4.2705612 -4.2452526 -4.2118015 -4.1829314 -4.1540709 -4.1311092 -4.1106067 -4.0891728 -4.0894384 -4.1159477 -4.1464257 -4.1780381 -4.2079062][-4.2842145 -4.2832565 -4.2728209 -4.2467828 -4.2150168 -4.1870265 -4.1631866 -4.1496005 -4.1356483 -4.1149597 -4.1109676 -4.1318922 -4.1615286 -4.1952529 -4.2255249][-4.2601485 -4.2522745 -4.2399673 -4.218236 -4.1980586 -4.1827464 -4.1724706 -4.1697445 -4.1615067 -4.1454487 -4.1427217 -4.1584892 -4.1846991 -4.2179861 -4.2427406]]...]
INFO - root - 2017-12-07 13:05:26.102210: step 6210, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.095 sec/batch; 44h:46m:03s remains)
INFO - root - 2017-12-07 13:05:47.449757: step 6220, loss = 2.06, batch loss = 2.01 (15.3 examples/sec; 2.098 sec/batch; 44h:48m:45s remains)
INFO - root - 2017-12-07 13:06:08.479407: step 6230, loss = 2.09, batch loss = 2.03 (15.4 examples/sec; 2.072 sec/batch; 44h:15m:18s remains)
INFO - root - 2017-12-07 13:06:29.428819: step 6240, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 45h:09m:44s remains)
INFO - root - 2017-12-07 13:06:50.615715: step 6250, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.090 sec/batch; 44h:37m:16s remains)
INFO - root - 2017-12-07 13:07:11.759019: step 6260, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.085 sec/batch; 44h:30m:50s remains)
INFO - root - 2017-12-07 13:07:32.801429: step 6270, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 45h:32m:23s remains)
INFO - root - 2017-12-07 13:07:53.860840: step 6280, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 2.080 sec/batch; 44h:24m:30s remains)
INFO - root - 2017-12-07 13:08:15.132761: step 6290, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.187 sec/batch; 46h:40m:28s remains)
INFO - root - 2017-12-07 13:08:35.934698: step 6300, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.062 sec/batch; 43h:59m:51s remains)
2017-12-07 13:08:37.430972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2887068 -4.2858906 -4.293169 -4.3038778 -4.3083272 -4.3034954 -4.2977667 -4.2949724 -4.2903738 -4.2867045 -4.2866082 -4.286581 -4.2902813 -4.2915158 -4.28417][-4.2492371 -4.2467365 -4.2616143 -4.277688 -4.285686 -4.2840576 -4.2822208 -4.2796574 -4.2740245 -4.2673326 -4.2667656 -4.2670293 -4.2732553 -4.2728572 -4.2615843][-4.2106185 -4.2049651 -4.2289777 -4.2504797 -4.2615671 -4.26464 -4.2675266 -4.2651339 -4.255754 -4.2458072 -4.2447772 -4.2479634 -4.2588844 -4.2563615 -4.2371693][-4.162035 -4.1488256 -4.1784081 -4.2043071 -4.216414 -4.2256575 -4.235405 -4.2416272 -4.2349977 -4.2223516 -4.2194462 -4.222404 -4.2344351 -4.2303543 -4.2117238][-4.1364045 -4.1119447 -4.1317062 -4.1496038 -4.1581349 -4.1662717 -4.1795039 -4.2025928 -4.2173758 -4.2116823 -4.2037416 -4.203536 -4.21614 -4.2150111 -4.2022376][-4.1526189 -4.1178756 -4.1151967 -4.1088343 -4.0978136 -4.0882878 -4.0838094 -4.1101093 -4.1541162 -4.1720371 -4.1756263 -4.1832685 -4.2064605 -4.2243509 -4.2247324][-4.1820679 -4.1483779 -4.1229458 -4.0844016 -4.0412712 -3.9940999 -3.944833 -3.955409 -4.0302763 -4.0866857 -4.1176143 -4.1437616 -4.178411 -4.2128105 -4.2271676][-4.2068071 -4.1777787 -4.1402507 -4.0832782 -4.0179534 -3.9399793 -3.8469 -3.8386309 -3.9422286 -4.0335593 -4.0854549 -4.1109524 -4.1346869 -4.161098 -4.1762204][-4.2229109 -4.204071 -4.1721029 -4.124629 -4.0688434 -4.000793 -3.9218943 -3.9091992 -3.9874923 -4.0625839 -4.0997663 -4.1027393 -4.1016607 -4.1080346 -4.1066318][-4.2428083 -4.2382278 -4.2207546 -4.1938462 -4.1577749 -4.1141963 -4.0635958 -4.0484667 -4.0902157 -4.1310248 -4.1429749 -4.1318254 -4.1193776 -4.1091247 -4.0834494][-4.2704978 -4.2775807 -4.2722154 -4.2599759 -4.2427468 -4.2157192 -4.1823468 -4.1671839 -4.1856 -4.2007561 -4.1953673 -4.1796684 -4.1728721 -4.1620736 -4.1288319][-4.289526 -4.3010197 -4.3019218 -4.2988973 -4.2940874 -4.2795372 -4.2562861 -4.241818 -4.2517195 -4.2566576 -4.243228 -4.2257028 -4.2253232 -4.2283363 -4.2066627][-4.2996883 -4.3122005 -4.3143811 -4.3143358 -4.313098 -4.3041215 -4.2869425 -4.2720675 -4.27975 -4.2866859 -4.2757931 -4.2668 -4.2756672 -4.2862864 -4.2791915][-4.3053937 -4.3142095 -4.3161783 -4.3173933 -4.3176446 -4.3123922 -4.2996006 -4.2866411 -4.2909203 -4.2966733 -4.2918124 -4.291378 -4.3036671 -4.3139682 -4.3153915][-4.30858 -4.3136783 -4.3147397 -4.3146782 -4.31569 -4.3129945 -4.3033857 -4.2906656 -4.288321 -4.293117 -4.2947431 -4.2994885 -4.3116193 -4.321137 -4.3247452]]...]
INFO - root - 2017-12-07 13:08:58.586005: step 6310, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.087 sec/batch; 44h:31m:56s remains)
INFO - root - 2017-12-07 13:09:19.874700: step 6320, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.151 sec/batch; 45h:53m:53s remains)
INFO - root - 2017-12-07 13:09:40.608868: step 6330, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.101 sec/batch; 44h:48m:52s remains)
INFO - root - 2017-12-07 13:10:01.976779: step 6340, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.153 sec/batch; 45h:55m:17s remains)
INFO - root - 2017-12-07 13:10:23.262116: step 6350, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 45h:19m:39s remains)
INFO - root - 2017-12-07 13:10:44.108200: step 6360, loss = 2.07, batch loss = 2.01 (15.7 examples/sec; 2.040 sec/batch; 43h:29m:38s remains)
INFO - root - 2017-12-07 13:11:05.310050: step 6370, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.082 sec/batch; 44h:23m:30s remains)
INFO - root - 2017-12-07 13:11:26.502874: step 6380, loss = 2.06, batch loss = 2.01 (15.4 examples/sec; 2.077 sec/batch; 44h:16m:40s remains)
INFO - root - 2017-12-07 13:11:47.406453: step 6390, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.110 sec/batch; 44h:58m:28s remains)
INFO - root - 2017-12-07 13:12:08.445686: step 6400, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.071 sec/batch; 44h:08m:28s remains)
2017-12-07 13:12:10.066207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2740259 -4.3103228 -4.3389382 -4.354692 -4.35674 -4.3453183 -4.3299375 -4.3230724 -4.3274808 -4.3285995 -4.3150692 -4.2847767 -4.2392354 -4.1844306 -4.1506577][-4.2511673 -4.2843552 -4.3094697 -4.3235545 -4.32607 -4.3184481 -4.3094883 -4.3081594 -4.3154907 -4.3108106 -4.2887039 -4.2547708 -4.2134237 -4.1699023 -4.1473203][-4.2329421 -4.2560749 -4.2718043 -4.279561 -4.2769527 -4.2689161 -4.2645049 -4.2714067 -4.2800126 -4.271193 -4.2477484 -4.2208433 -4.1954079 -4.1697979 -4.16003][-4.2246327 -4.232029 -4.2328043 -4.2298756 -4.2186217 -4.202251 -4.1964245 -4.205698 -4.2135186 -4.2057524 -4.1917419 -4.1828508 -4.1819782 -4.1761584 -4.1756816][-4.2173476 -4.2049923 -4.1918044 -4.1785483 -4.1527958 -4.1188555 -4.1024761 -4.1034651 -4.1060157 -4.1071386 -4.1159244 -4.1337471 -4.1584649 -4.1702843 -4.1783028][-4.2203536 -4.1963491 -4.172936 -4.1474156 -4.1053481 -4.0488067 -4.0163388 -4.0012512 -3.9926941 -3.9997818 -4.0348125 -4.082242 -4.1286097 -4.1554041 -4.1706357][-4.2414556 -4.2162504 -4.1886196 -4.1546726 -4.1004486 -4.0278745 -3.9847116 -3.9566495 -3.9343863 -3.9356191 -3.9823697 -4.0493321 -4.1107073 -4.151897 -4.1750193][-4.281713 -4.2597108 -4.2309747 -4.1903949 -4.131485 -4.0590191 -4.0161357 -3.9820783 -3.9505925 -3.9387264 -3.9821317 -4.0545478 -4.12183 -4.1697893 -4.1981654][-4.3272305 -4.3091073 -4.2814507 -4.2406311 -4.1892943 -4.1322308 -4.100409 -4.0728731 -4.0427914 -4.0251741 -4.0552545 -4.1147542 -4.172677 -4.2155232 -4.2412224][-4.3570518 -4.3435111 -4.3205018 -4.2897806 -4.2551808 -4.2194362 -4.2020512 -4.1833391 -4.1585445 -4.1408997 -4.1571841 -4.193572 -4.2319679 -4.2622356 -4.2808819][-4.3669372 -4.3599949 -4.3461666 -4.3289771 -4.3105702 -4.29303 -4.2868571 -4.2744422 -4.2548323 -4.2408056 -4.2467661 -4.2634449 -4.2818494 -4.2963014 -4.3046951][-4.3618317 -4.3601866 -4.3550887 -4.3494172 -4.3419247 -4.3344908 -4.334209 -4.3299856 -4.3191881 -4.3093972 -4.3074503 -4.310071 -4.3147864 -4.31857 -4.3201704][-4.3542237 -4.3563771 -4.3565197 -4.355958 -4.3534966 -4.3507133 -4.3514948 -4.3509865 -4.3465886 -4.3405032 -4.3350835 -4.3319488 -4.3323255 -4.3334312 -4.3336329][-4.3533134 -4.3557706 -4.3558064 -4.3553305 -4.3541055 -4.3528795 -4.3536367 -4.3549957 -4.3549027 -4.3520203 -4.3477731 -4.34479 -4.34407 -4.3447886 -4.3451028][-4.3544745 -4.3562346 -4.3557863 -4.3547797 -4.3534222 -4.3522019 -4.3520331 -4.3533792 -4.3545485 -4.3543391 -4.3527617 -4.3516746 -4.35127 -4.3518577 -4.3522739]]...]
INFO - root - 2017-12-07 13:12:31.390613: step 6410, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.114 sec/batch; 45h:02m:49s remains)
INFO - root - 2017-12-07 13:12:52.532939: step 6420, loss = 2.08, batch loss = 2.02 (15.9 examples/sec; 2.015 sec/batch; 42h:56m:29s remains)
INFO - root - 2017-12-07 13:13:13.660072: step 6430, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.153 sec/batch; 45h:51m:56s remains)
INFO - root - 2017-12-07 13:13:34.841063: step 6440, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.147 sec/batch; 45h:44m:38s remains)
INFO - root - 2017-12-07 13:13:55.953846: step 6450, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.112 sec/batch; 44h:58m:26s remains)
INFO - root - 2017-12-07 13:14:16.873111: step 6460, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.109 sec/batch; 44h:55m:19s remains)
INFO - root - 2017-12-07 13:14:37.991998: step 6470, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.110 sec/batch; 44h:55m:46s remains)
INFO - root - 2017-12-07 13:14:59.053588: step 6480, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.096 sec/batch; 44h:37m:05s remains)
INFO - root - 2017-12-07 13:15:20.050316: step 6490, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.090 sec/batch; 44h:29m:54s remains)
INFO - root - 2017-12-07 13:15:41.213055: step 6500, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 45h:05m:12s remains)
2017-12-07 13:15:42.843610: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.357522 -4.3673573 -4.3640089 -4.3539724 -4.3446321 -4.3293 -4.3088121 -4.2912893 -4.2877126 -4.2896972 -4.2840843 -4.2756157 -4.2757316 -4.2882519 -4.3010068][-4.3652396 -4.3762226 -4.3713717 -4.3574581 -4.3452959 -4.3242512 -4.2969627 -4.2755766 -4.2695594 -4.2722697 -4.267714 -4.2612233 -4.2631221 -4.2836814 -4.3013749][-4.3700857 -4.3790941 -4.3708644 -4.3522544 -4.3321295 -4.3010087 -4.2667727 -4.2465706 -4.2444396 -4.2477536 -4.2452664 -4.2427449 -4.2495618 -4.2775211 -4.3001633][-4.3715849 -4.37713 -4.3633661 -4.3378158 -4.3078156 -4.2642736 -4.2175479 -4.1996613 -4.2089434 -4.217133 -4.2174177 -4.2198491 -4.233007 -4.2664247 -4.2934856][-4.3700085 -4.3723025 -4.3520565 -4.3191814 -4.2791505 -4.2218413 -4.1559834 -4.1329393 -4.15109 -4.1713829 -4.1810207 -4.1909647 -4.2107334 -4.2502642 -4.283174][-4.3664517 -4.3656926 -4.34072 -4.3026319 -4.2553854 -4.1895118 -4.1122274 -4.0772114 -4.1020522 -4.1365442 -4.1561193 -4.1695523 -4.1936464 -4.2383003 -4.2765584][-4.3620138 -4.3589263 -4.3329182 -4.2940679 -4.2416339 -4.1716151 -4.092485 -4.0593863 -4.0927734 -4.1330862 -4.1526666 -4.16616 -4.19264 -4.2374344 -4.2760825][-4.3545923 -4.3532696 -4.3303037 -4.2922583 -4.2382169 -4.1694517 -4.098155 -4.0759993 -4.1149979 -4.1541195 -4.1693988 -4.1830239 -4.2066622 -4.2456441 -4.2802253][-4.3473763 -4.3517871 -4.3356957 -4.3010483 -4.2515631 -4.1933322 -4.1358428 -4.1160583 -4.1471672 -4.1781931 -4.1912489 -4.1998568 -4.2198524 -4.2528739 -4.2817984][-4.3420024 -4.3523345 -4.3454628 -4.3206215 -4.2809172 -4.2392769 -4.1949649 -4.1697626 -4.177784 -4.1953306 -4.2012138 -4.201663 -4.2178345 -4.2473946 -4.276412][-4.3388581 -4.350718 -4.3499107 -4.3317142 -4.3016238 -4.27124 -4.2374449 -4.2114668 -4.2038746 -4.2069917 -4.199522 -4.1933532 -4.2083993 -4.2394376 -4.2706494][-4.3343663 -4.3458757 -4.3486204 -4.3346653 -4.3093882 -4.2848043 -4.2590084 -4.232522 -4.2209558 -4.2160778 -4.2012539 -4.1952257 -4.211904 -4.2412028 -4.2710419][-4.3319445 -4.3438778 -4.3500571 -4.3411779 -4.3210149 -4.3023553 -4.2778158 -4.2448764 -4.2201014 -4.2075014 -4.1996722 -4.2075334 -4.229898 -4.2557497 -4.2797451][-4.3324962 -4.3448005 -4.3551092 -4.35315 -4.3383164 -4.320446 -4.2951174 -4.2546239 -4.217526 -4.1992211 -4.2013397 -4.2219524 -4.2484436 -4.2714753 -4.2926879][-4.332984 -4.3455572 -4.3586845 -4.3623166 -4.3531179 -4.3383484 -4.3139191 -4.2722006 -4.2342148 -4.2156792 -4.2247829 -4.2451677 -4.2657313 -4.2858381 -4.3053]]...]
INFO - root - 2017-12-07 13:16:04.130761: step 6510, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.141 sec/batch; 45h:33m:26s remains)
INFO - root - 2017-12-07 13:16:25.204757: step 6520, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.129 sec/batch; 45h:18m:11s remains)
INFO - root - 2017-12-07 13:16:46.167342: step 6530, loss = 2.09, batch loss = 2.03 (15.4 examples/sec; 2.074 sec/batch; 44h:08m:15s remains)
INFO - root - 2017-12-07 13:17:07.304108: step 6540, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.129 sec/batch; 45h:16m:52s remains)
INFO - root - 2017-12-07 13:17:28.310290: step 6550, loss = 2.06, batch loss = 2.01 (15.4 examples/sec; 2.080 sec/batch; 44h:14m:17s remains)
INFO - root - 2017-12-07 13:17:49.263189: step 6560, loss = 2.06, batch loss = 2.00 (15.4 examples/sec; 2.083 sec/batch; 44h:18m:40s remains)
INFO - root - 2017-12-07 13:18:10.640084: step 6570, loss = 2.09, batch loss = 2.03 (15.4 examples/sec; 2.083 sec/batch; 44h:17m:59s remains)
INFO - root - 2017-12-07 13:18:31.935446: step 6580, loss = 2.09, batch loss = 2.03 (14.8 examples/sec; 2.164 sec/batch; 46h:00m:19s remains)
INFO - root - 2017-12-07 13:18:52.977539: step 6590, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.118 sec/batch; 45h:01m:39s remains)
2017-12-07 13:19:10.966115: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 921.38MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
INFO - root - 2017-12-07 13:19:14.165809: step 6600, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.101 sec/batch; 44h:39m:34s remains)
2017-12-07 13:19:15.678498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1258578 -4.1383247 -4.1560612 -4.1671605 -4.166039 -4.1610689 -4.1658759 -4.1874056 -4.2172675 -4.2419114 -4.2534266 -4.2515631 -4.2492952 -4.2522902 -4.2552543][-4.1254463 -4.1397662 -4.1546593 -4.1595531 -4.1498294 -4.1379037 -4.1417937 -4.1677566 -4.2040219 -4.2338982 -4.2497425 -4.2517805 -4.2509861 -4.2520094 -4.2512927][-4.1257048 -4.14372 -4.1573572 -4.1566424 -4.1400886 -4.1200294 -4.1190081 -4.1458635 -4.1871114 -4.223228 -4.2444005 -4.2510309 -4.2520847 -4.2516212 -4.2484722][-4.1252289 -4.1468267 -4.1602554 -4.1541414 -4.1296024 -4.0974035 -4.0842924 -4.1089439 -4.1585932 -4.205842 -4.2370052 -4.2510085 -4.255383 -4.2554049 -4.25192][-4.1272106 -4.1488304 -4.1630917 -4.153039 -4.118968 -4.0693703 -4.0368519 -4.0540023 -4.1134458 -4.1774163 -4.2234097 -4.2489486 -4.2586231 -4.261622 -4.2605591][-4.1340284 -4.1498179 -4.1642981 -4.1540079 -4.1138725 -4.0464678 -3.9890225 -3.9925826 -4.0612497 -4.1434517 -4.204988 -4.2423105 -4.25897 -4.2672372 -4.2705011][-4.148674 -4.1567664 -4.1696033 -4.1622982 -4.1227093 -4.047658 -3.9711063 -3.9542711 -4.0229516 -4.1160893 -4.1877756 -4.2338686 -4.2573004 -4.2707763 -4.2791843][-4.1723976 -4.1741252 -4.1823454 -4.1775675 -4.1440167 -4.0756712 -3.9985328 -3.9664927 -4.015996 -4.1019592 -4.1750655 -4.2261257 -4.25451 -4.2724109 -4.2858305][-4.2014346 -4.1984267 -4.198535 -4.1919355 -4.1647887 -4.1103983 -4.0457678 -4.0108089 -4.036428 -4.1016278 -4.16773 -4.2194958 -4.2515774 -4.2735872 -4.2911534][-4.2275519 -4.2200613 -4.2103343 -4.1996307 -4.178751 -4.1376705 -4.0875912 -4.0545826 -4.0634804 -4.1081738 -4.1653304 -4.2153263 -4.24955 -4.2750387 -4.2954092][-4.2462687 -4.2349839 -4.2163663 -4.2003379 -4.1835475 -4.1535645 -4.1150403 -4.0848227 -4.0856104 -4.1171532 -4.1665874 -4.2133169 -4.247961 -4.2753572 -4.2974415][-4.2579088 -4.2452135 -4.2210693 -4.1988454 -4.1816568 -4.1581936 -4.1271 -4.0983524 -4.09544 -4.1207633 -4.1656976 -4.2101474 -4.2447124 -4.2738538 -4.297586][-4.2628059 -4.2504625 -4.2249203 -4.1985073 -4.1777706 -4.1568875 -4.1318431 -4.1062951 -4.100668 -4.1204281 -4.1620088 -4.204668 -4.2395911 -4.2714291 -4.2975769][-4.2630739 -4.2535858 -4.2298927 -4.2010975 -4.1760931 -4.1564875 -4.1390367 -4.1206412 -4.1137304 -4.1258535 -4.1598349 -4.1983318 -4.2327929 -4.266716 -4.2962079][-4.2576742 -4.2529335 -4.2336583 -4.2049031 -4.1769824 -4.1576509 -4.1475258 -4.1377773 -4.1309772 -4.1356354 -4.1590428 -4.1895928 -4.2201295 -4.2540956 -4.2867708]]...]
INFO - root - 2017-12-07 13:19:36.819312: step 6610, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.107 sec/batch; 44h:46m:55s remains)
INFO - root - 2017-12-07 13:19:57.542803: step 6620, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.097 sec/batch; 44h:34m:15s remains)
INFO - root - 2017-12-07 13:20:18.578435: step 6630, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 45h:15m:08s remains)
INFO - root - 2017-12-07 13:20:39.919440: step 6640, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.175 sec/batch; 46h:12m:23s remains)
INFO - root - 2017-12-07 13:21:00.603715: step 6650, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 1.944 sec/batch; 41h:17m:17s remains)
INFO - root - 2017-12-07 13:21:21.775878: step 6660, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.127 sec/batch; 45h:10m:18s remains)
INFO - root - 2017-12-07 13:21:42.984031: step 6670, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.096 sec/batch; 44h:30m:24s remains)
INFO - root - 2017-12-07 13:22:03.960578: step 6680, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 44h:27m:33s remains)
INFO - root - 2017-12-07 13:22:24.963308: step 6690, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.189 sec/batch; 46h:28m:31s remains)
INFO - root - 2017-12-07 13:22:46.182567: step 6700, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.086 sec/batch; 44h:17m:01s remains)
2017-12-07 13:22:47.757824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2845893 -4.246377 -4.1931434 -4.12175 -4.0190148 -3.9530106 -4.0221453 -4.1125393 -4.1625948 -4.1797681 -4.1671104 -4.1096416 -4.0456309 -4.0659471 -4.1352758][-4.287189 -4.2637782 -4.2249169 -4.1735187 -4.1060205 -4.0708551 -4.1107016 -4.1716547 -4.2132268 -4.2292237 -4.2272711 -4.2037253 -4.1726542 -4.1761069 -4.2072949][-4.278646 -4.2704468 -4.2511735 -4.21766 -4.1786566 -4.1582913 -4.1758251 -4.2085562 -4.2372527 -4.2528596 -4.255362 -4.2449317 -4.2246118 -4.2156482 -4.2228041][-4.2644215 -4.2724237 -4.2684307 -4.2469044 -4.2116909 -4.188405 -4.1939759 -4.212791 -4.2354503 -4.2521839 -4.258203 -4.2519755 -4.2301483 -4.2048755 -4.1968904][-4.2370706 -4.2634606 -4.2710576 -4.2495131 -4.2050276 -4.1660986 -4.1545682 -4.1713548 -4.2015543 -4.2229495 -4.2329974 -4.22805 -4.1984906 -4.1505346 -4.1243258][-4.1989694 -4.2413669 -4.2573624 -4.2282543 -4.1629391 -4.0859919 -4.0404181 -4.0684614 -4.1283913 -4.166111 -4.1828194 -4.1760669 -4.1293459 -4.0418696 -3.9854596][-4.1665754 -4.2173748 -4.234056 -4.192822 -4.0995545 -3.9676771 -3.8643 -3.9326065 -4.0554905 -4.124681 -4.1519561 -4.1429524 -4.0775852 -3.9538634 -3.8795598][-4.1538453 -4.2048321 -4.2277918 -4.1879716 -4.0932579 -3.9450898 -3.8245659 -3.9152994 -4.0585494 -4.1384258 -4.1662688 -4.1657887 -4.11976 -4.0272694 -3.9792998][-4.1768155 -4.2125444 -4.2359791 -4.2134242 -4.1516461 -4.0599737 -3.9973583 -4.0492821 -4.13914 -4.1938567 -4.2156072 -4.225071 -4.20738 -4.1618333 -4.1340456][-4.2174325 -4.2364092 -4.2578692 -4.2486162 -4.2080393 -4.1567507 -4.1284561 -4.1541162 -4.2007174 -4.2306976 -4.2490511 -4.2712731 -4.2705469 -4.2440348 -4.221045][-4.2276988 -4.2405281 -4.2625122 -4.2585025 -4.2183366 -4.1703515 -4.1516595 -4.1768451 -4.20956 -4.2304678 -4.2555208 -4.2892466 -4.3003421 -4.2825155 -4.2600431][-4.2044444 -4.2243528 -4.2480049 -4.2391806 -4.1883974 -4.1271415 -4.1079273 -4.1471725 -4.1851525 -4.2090468 -4.2443209 -4.2852583 -4.3029709 -4.2933517 -4.2742329][-4.1734729 -4.2023039 -4.2278986 -4.2080088 -4.1372914 -4.0450039 -4.0141039 -4.0771475 -4.1411872 -4.1811252 -4.2248206 -4.2661004 -4.2857361 -4.2818346 -4.2663302][-4.1479549 -4.1937146 -4.2197685 -4.1925788 -4.10798 -3.9932289 -3.951304 -4.0231519 -4.1066313 -4.1626654 -4.2100205 -4.2485857 -4.2645707 -4.2579646 -4.2394652][-4.1520433 -4.2081528 -4.2368989 -4.2132587 -4.14493 -4.0570521 -4.0213675 -4.0723472 -4.1437583 -4.1858969 -4.2145658 -4.2375646 -4.2478132 -4.2417831 -4.2262287]]...]
INFO - root - 2017-12-07 13:23:08.815866: step 6710, loss = 2.07, batch loss = 2.01 (15.9 examples/sec; 2.014 sec/batch; 42h:44m:37s remains)
INFO - root - 2017-12-07 13:23:29.956458: step 6720, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 44h:38m:19s remains)
INFO - root - 2017-12-07 13:23:51.028141: step 6730, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.149 sec/batch; 45h:36m:49s remains)
INFO - root - 2017-12-07 13:24:12.026336: step 6740, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.112 sec/batch; 44h:48m:41s remains)
INFO - root - 2017-12-07 13:24:33.036324: step 6750, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.182 sec/batch; 46h:17m:04s remains)
INFO - root - 2017-12-07 13:24:54.143571: step 6760, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.106 sec/batch; 44h:40m:35s remains)
INFO - root - 2017-12-07 13:25:15.356386: step 6770, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.073 sec/batch; 43h:57m:56s remains)
INFO - root - 2017-12-07 13:25:36.470441: step 6780, loss = 2.06, batch loss = 2.01 (15.7 examples/sec; 2.043 sec/batch; 43h:19m:59s remains)
INFO - root - 2017-12-07 13:25:57.594089: step 6790, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.174 sec/batch; 46h:06m:05s remains)
INFO - root - 2017-12-07 13:26:18.749287: step 6800, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.145 sec/batch; 45h:28m:26s remains)
2017-12-07 13:26:20.285204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2457814 -4.2685871 -4.2886081 -4.29746 -4.2996035 -4.3027849 -4.30603 -4.3079185 -4.3086624 -4.3083787 -4.3107519 -4.3126574 -4.30004 -4.2632337 -4.2228608][-4.2486567 -4.2696218 -4.2882972 -4.299993 -4.3106384 -4.3195047 -4.3238692 -4.3255634 -4.3215742 -4.3181162 -4.3210373 -4.3242726 -4.3052197 -4.2575421 -4.2121954][-4.2406034 -4.2555032 -4.2678218 -4.2749796 -4.2869954 -4.299057 -4.3047814 -4.3078237 -4.3024426 -4.3007822 -4.3091421 -4.3164749 -4.2972536 -4.2489586 -4.2035851][-4.2289619 -4.2361155 -4.2387052 -4.2369165 -4.242053 -4.2503629 -4.255806 -4.2593393 -4.2540836 -4.2561436 -4.2696815 -4.2817516 -4.2726312 -4.2396188 -4.2015905][-4.2251081 -4.2229924 -4.21461 -4.2021632 -4.1971846 -4.1976008 -4.1992278 -4.197866 -4.1914949 -4.1969733 -4.2163615 -4.2351327 -4.2417564 -4.2323346 -4.2096334][-4.2210536 -4.206131 -4.1837196 -4.1600308 -4.1425467 -4.1325383 -4.1260376 -4.114161 -4.107367 -4.1213593 -4.1529613 -4.1836438 -4.2107134 -4.2262273 -4.2212424][-4.1940331 -4.1595626 -4.1178226 -4.0757031 -4.0410762 -4.0275393 -4.019556 -4.0046816 -4.004849 -4.0365348 -4.0877323 -4.1350851 -4.180882 -4.2180338 -4.2304945][-4.1551666 -4.0973186 -4.02949 -3.9610822 -3.906502 -3.9034493 -3.9122236 -3.9146821 -3.9370916 -3.9932044 -4.0622835 -4.1190767 -4.1723523 -4.2156825 -4.2344513][-4.1478539 -4.0842 -4.009634 -3.9363885 -3.8817272 -3.8984797 -3.9328954 -3.9650214 -4.0066538 -4.065937 -4.1272726 -4.170084 -4.2036772 -4.2266178 -4.231843][-4.1732097 -4.1372056 -4.0886083 -4.0432639 -4.0151324 -4.041811 -4.078105 -4.1112423 -4.1435132 -4.1788559 -4.2115097 -4.2279539 -4.2339344 -4.2278852 -4.2113695][-4.202404 -4.20448 -4.18998 -4.173315 -4.1649933 -4.1843257 -4.2038479 -4.2175274 -4.2263641 -4.2348833 -4.2442179 -4.2432451 -4.2294979 -4.2024493 -4.1684761][-4.2023363 -4.2331529 -4.2472734 -4.2485738 -4.24592 -4.2509251 -4.2536826 -4.2513452 -4.2431111 -4.2350659 -4.233933 -4.2286882 -4.2061453 -4.1661162 -4.11814][-4.1941667 -4.2385225 -4.267251 -4.2799854 -4.2812147 -4.2805443 -4.2747741 -4.2629647 -4.2446985 -4.2271314 -4.21788 -4.2107863 -4.1853647 -4.1351776 -4.0782843][-4.21927 -4.2580514 -4.2844176 -4.2965937 -4.2984915 -4.2974744 -4.2906656 -4.2760577 -4.2556896 -4.2359676 -4.2219172 -4.2120705 -4.1872525 -4.136313 -4.0800209][-4.2758956 -4.2989597 -4.3151007 -4.3206854 -4.3208675 -4.3199735 -4.3149123 -4.3015666 -4.2831788 -4.2660875 -4.252955 -4.2458992 -4.2271514 -4.1879578 -4.1464491]]...]
INFO - root - 2017-12-07 13:26:41.319178: step 6810, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 44h:56m:53s remains)
INFO - root - 2017-12-07 13:27:02.502870: step 6820, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 44h:55m:03s remains)
INFO - root - 2017-12-07 13:27:23.770092: step 6830, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.072 sec/batch; 43h:54m:31s remains)
INFO - root - 2017-12-07 13:27:44.925113: step 6840, loss = 2.09, batch loss = 2.03 (15.7 examples/sec; 2.037 sec/batch; 43h:09m:54s remains)
INFO - root - 2017-12-07 13:28:05.954793: step 6850, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.084 sec/batch; 44h:09m:31s remains)
INFO - root - 2017-12-07 13:28:27.222368: step 6860, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.107 sec/batch; 44h:38m:00s remains)
INFO - root - 2017-12-07 13:28:48.448890: step 6870, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.184 sec/batch; 46h:15m:14s remains)
INFO - root - 2017-12-07 13:29:09.326257: step 6880, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.161 sec/batch; 45h:46m:01s remains)
INFO - root - 2017-12-07 13:29:30.715984: step 6890, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 2.050 sec/batch; 43h:24m:53s remains)
INFO - root - 2017-12-07 13:29:51.857594: step 6900, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.089 sec/batch; 44h:14m:24s remains)
2017-12-07 13:29:53.385241: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2861872 -4.283637 -4.28275 -4.2826467 -4.2839556 -4.2857409 -4.2869277 -4.28766 -4.2883363 -4.2892485 -4.2892895 -4.2888875 -4.2880855 -4.2876935 -4.2875652][-4.2821474 -4.2777786 -4.2737579 -4.2724938 -4.2747884 -4.2781157 -4.2802815 -4.2832 -4.2866397 -4.2892513 -4.2896929 -4.2886081 -4.2873516 -4.2875314 -4.2886081][-4.2583075 -4.2500715 -4.2423959 -4.2430859 -4.2496414 -4.2556472 -4.2600169 -4.26569 -4.273119 -4.2802444 -4.2834883 -4.2826939 -4.2809672 -4.2819767 -4.2848577][-4.1922183 -4.1772065 -4.164248 -4.1678696 -4.1815472 -4.1905437 -4.19564 -4.2042656 -4.2178111 -4.2329559 -4.2422204 -4.2432847 -4.2415166 -4.24428 -4.2484255][-4.11727 -4.0976562 -4.0813837 -4.0839992 -4.0982208 -4.0991249 -4.0936165 -4.1014585 -4.1196218 -4.1406531 -4.1539764 -4.1551771 -4.1540976 -4.1616826 -4.1702127][-4.0992908 -4.0812039 -4.0607605 -4.0546813 -4.0538979 -4.0335217 -4.0073695 -4.010159 -4.0333462 -4.0604367 -4.0758395 -4.0751295 -4.072547 -4.0827265 -4.09422][-4.115592 -4.0998273 -4.0768771 -4.0623507 -4.04299 -3.99746 -3.9454758 -3.9409747 -3.9754248 -4.0144634 -4.0374928 -4.0374269 -4.0358391 -4.0513215 -4.0678878][-4.1352444 -4.1213837 -4.1010003 -4.0856152 -4.0622225 -4.0085387 -3.9440606 -3.9331834 -3.9765368 -4.0245457 -4.0535917 -4.0598373 -4.064465 -4.0831618 -4.1004348][-4.1598282 -4.148654 -4.1344304 -4.1272559 -4.1169634 -4.0831661 -4.0428734 -4.0399528 -4.0767379 -4.112155 -4.1304789 -4.1327906 -4.1331568 -4.1440334 -4.1547289][-4.2001929 -4.195569 -4.1856771 -4.1843686 -4.1829042 -4.1674876 -4.1518517 -4.1549764 -4.1781859 -4.1954374 -4.2019987 -4.1988931 -4.1932216 -4.1942992 -4.1977091][-4.249177 -4.2493606 -4.2411265 -4.2378817 -4.2357526 -4.227778 -4.2232118 -4.22689 -4.2407327 -4.2485957 -4.2516046 -4.2500048 -4.2438784 -4.2408733 -4.23908][-4.2780094 -4.2781434 -4.2717795 -4.2670579 -4.2651863 -4.262044 -4.2623405 -4.2665405 -4.2752428 -4.2787185 -4.2796345 -4.2799053 -4.2766609 -4.2742677 -4.2707014][-4.2879639 -4.2883158 -4.28424 -4.279355 -4.2780318 -4.2773075 -4.2816768 -4.2871466 -4.2928882 -4.2949286 -4.2947955 -4.2950139 -4.2936492 -4.2928953 -4.2914205][-4.3007402 -4.302248 -4.3026986 -4.2998376 -4.296433 -4.2931776 -4.2951188 -4.2991114 -4.3025494 -4.3046064 -4.305995 -4.3064246 -4.3049107 -4.3036771 -4.3026443][-4.3076253 -4.3087316 -4.3108988 -4.3089471 -4.3039222 -4.2991548 -4.3000512 -4.3038349 -4.3077469 -4.3112555 -4.3147869 -4.3156528 -4.3141012 -4.3116441 -4.3084974]]...]
INFO - root - 2017-12-07 13:30:14.443671: step 6910, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.149 sec/batch; 45h:29m:09s remains)
INFO - root - 2017-12-07 13:30:35.429675: step 6920, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.112 sec/batch; 44h:42m:23s remains)
INFO - root - 2017-12-07 13:30:56.633833: step 6930, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.188 sec/batch; 46h:18m:37s remains)
INFO - root - 2017-12-07 13:31:17.523108: step 6940, loss = 2.07, batch loss = 2.02 (16.3 examples/sec; 1.964 sec/batch; 41h:33m:42s remains)
INFO - root - 2017-12-07 13:31:38.736083: step 6950, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.109 sec/batch; 44h:37m:42s remains)
INFO - root - 2017-12-07 13:31:59.838883: step 6960, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.114 sec/batch; 44h:43m:07s remains)
INFO - root - 2017-12-07 13:32:20.860989: step 6970, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.106 sec/batch; 44h:33m:35s remains)
INFO - root - 2017-12-07 13:32:41.753574: step 6980, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.150 sec/batch; 45h:28m:09s remains)
INFO - root - 2017-12-07 13:33:03.250824: step 6990, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.189 sec/batch; 46h:17m:38s remains)
INFO - root - 2017-12-07 13:33:24.581345: step 7000, loss = 2.07, batch loss = 2.02 (14.7 examples/sec; 2.178 sec/batch; 46h:03m:51s remains)
2017-12-07 13:33:26.079562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1743321 -4.1870079 -4.2077947 -4.2215886 -4.2191892 -4.2181845 -4.2268376 -4.2367897 -4.2423048 -4.2460461 -4.2496471 -4.2476225 -4.2311592 -4.2054319 -4.1815996][-4.1593604 -4.1771536 -4.2029562 -4.2197847 -4.2249966 -4.2269511 -4.2314119 -4.2382779 -4.245419 -4.2521348 -4.2564783 -4.2553892 -4.2415957 -4.221899 -4.2039652][-4.1339269 -4.1502991 -4.1734548 -4.193511 -4.2071385 -4.2150888 -4.2187085 -4.226687 -4.2398858 -4.2532811 -4.2608175 -4.2591057 -4.2477193 -4.2339249 -4.2213016][-4.0981688 -4.0992713 -4.111084 -4.1292996 -4.1506448 -4.1629295 -4.1655593 -4.1764274 -4.1979589 -4.2191882 -4.2299075 -4.227026 -4.2145934 -4.2011123 -4.19369][-4.0672131 -4.0511131 -4.0469136 -4.0516453 -4.06571 -4.0707355 -4.0653729 -4.0767384 -4.1105971 -4.1474133 -4.1665726 -4.1621175 -4.1452808 -4.1288161 -4.1232185][-4.0740013 -4.0466247 -4.0274529 -4.0079565 -3.9921708 -3.9683967 -3.9374595 -3.9408884 -3.9896679 -4.0491624 -4.085722 -4.0874333 -4.0697393 -4.0515547 -4.0471892][-4.127574 -4.0997114 -4.0718012 -4.0297985 -3.9788568 -3.9156547 -3.8455632 -3.8297677 -3.8915038 -3.9708648 -4.02518 -4.0404716 -4.0302167 -4.0209913 -4.02133][-4.2038817 -4.1826267 -4.1552658 -4.1041021 -4.0361586 -3.9571757 -3.8733401 -3.83828 -3.884294 -3.9564681 -4.0140862 -4.0418878 -4.0473037 -4.0537329 -4.0620031][-4.2740788 -4.2602191 -4.2394772 -4.1956091 -4.1298008 -4.0529132 -3.9724233 -3.9240313 -3.937773 -3.9840643 -4.0307817 -4.0638905 -4.0814662 -4.0987072 -4.1124825][-4.320507 -4.3127747 -4.2994928 -4.2682357 -4.2141008 -4.1476007 -4.0767531 -4.0234294 -4.0133295 -4.0350871 -4.0674438 -4.0990419 -4.12225 -4.14468 -4.1610579][-4.3431544 -4.3393407 -4.3325486 -4.3134155 -4.2752271 -4.2255478 -4.1701732 -4.1191492 -4.0935354 -4.0952539 -4.1130614 -4.1402245 -4.1671772 -4.1892943 -4.2027164][-4.3458223 -4.3483462 -4.3486519 -4.3391767 -4.314465 -4.281105 -4.2435503 -4.2029467 -4.1734447 -4.1629891 -4.1663618 -4.1850495 -4.2109909 -4.2322607 -4.2433944][-4.3417645 -4.3502975 -4.3567858 -4.3541741 -4.3391104 -4.3191462 -4.2985616 -4.2734332 -4.2510977 -4.2388535 -4.2344584 -4.2431865 -4.2629 -4.2810936 -4.2900763][-4.3370333 -4.3474488 -4.3574367 -4.3590069 -4.3507762 -4.3405805 -4.3317447 -4.3197041 -4.3080111 -4.3027387 -4.2991371 -4.3001986 -4.3104129 -4.3209434 -4.32275][-4.3340535 -4.3437634 -4.3545361 -4.3584285 -4.3553071 -4.3516703 -4.3475904 -4.341157 -4.3365474 -4.3367009 -4.3363037 -4.3362513 -4.3399692 -4.3425121 -4.3379345]]...]
INFO - root - 2017-12-07 13:33:46.866866: step 7010, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.093 sec/batch; 44h:15m:19s remains)
INFO - root - 2017-12-07 13:34:08.091291: step 7020, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 44h:55m:58s remains)
INFO - root - 2017-12-07 13:34:29.329447: step 7030, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 44h:40m:42s remains)
INFO - root - 2017-12-07 13:34:50.313565: step 7040, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.125 sec/batch; 44h:54m:54s remains)
INFO - root - 2017-12-07 13:35:11.448189: step 7050, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.064 sec/batch; 43h:36m:56s remains)
INFO - root - 2017-12-07 13:35:32.884957: step 7060, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.155 sec/batch; 45h:32m:05s remains)
INFO - root - 2017-12-07 13:35:53.772479: step 7070, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.124 sec/batch; 44h:51m:52s remains)
INFO - root - 2017-12-07 13:36:14.911145: step 7080, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.163 sec/batch; 45h:41m:21s remains)
INFO - root - 2017-12-07 13:36:36.012896: step 7090, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.117 sec/batch; 44h:42m:16s remains)
INFO - root - 2017-12-07 13:36:57.053901: step 7100, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.162 sec/batch; 45h:38m:58s remains)
2017-12-07 13:36:58.591400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1927643 -4.1848431 -4.1913629 -4.2009916 -4.2239108 -4.2538948 -4.28195 -4.3044333 -4.3166389 -4.3138881 -4.2968593 -4.2725124 -4.2562242 -4.2458143 -4.24369][-4.2027607 -4.1844249 -4.176239 -4.176475 -4.1987433 -4.2365737 -4.2717838 -4.2953138 -4.3067656 -4.3069687 -4.2941232 -4.2725539 -4.2553973 -4.2426372 -4.239243][-4.2286992 -4.200151 -4.1799922 -4.1704149 -4.1876183 -4.2253413 -4.2595649 -4.2765245 -4.2789826 -4.2751551 -4.2652121 -4.252089 -4.241168 -4.2337852 -4.2339988][-4.2583261 -4.2244678 -4.1990733 -4.1816587 -4.1842833 -4.2067513 -4.2271128 -4.2339005 -4.2322249 -4.2318239 -4.2315335 -4.2301865 -4.2276964 -4.2270103 -4.2317657][-4.2849631 -4.250381 -4.2246203 -4.20049 -4.1785688 -4.16635 -4.1544766 -4.1470256 -4.1513062 -4.1705394 -4.1909876 -4.20291 -4.2059884 -4.2109175 -4.2220678][-4.2917652 -4.257462 -4.23298 -4.203321 -4.1588521 -4.1062474 -4.0517292 -4.0293994 -4.0505538 -4.0982804 -4.1387587 -4.1603723 -4.1665606 -4.177597 -4.1989222][-4.2801361 -4.2454 -4.2218542 -4.1928935 -4.1421919 -4.064322 -3.9762387 -3.9409614 -3.9722829 -4.0409293 -4.0915961 -4.1111388 -4.1163774 -4.1311765 -4.164422][-4.274271 -4.2423711 -4.2247558 -4.2027578 -4.1584778 -4.081358 -3.9944379 -3.9563041 -3.9772537 -4.035778 -4.0767374 -4.0838757 -4.0805426 -4.0937786 -4.1361322][-4.2686033 -4.2435279 -4.2400804 -4.2341871 -4.2086525 -4.1540685 -4.0942683 -4.065237 -4.069335 -4.0956283 -4.1071725 -4.0970297 -4.0863271 -4.0941286 -4.1323075][-4.25507 -4.2363348 -4.2472582 -4.2627597 -4.2572575 -4.2270913 -4.1954989 -4.1817565 -4.1784348 -4.1832833 -4.1734 -4.1506157 -4.1306019 -4.1263118 -4.1487556][-4.2459469 -4.2330546 -4.2517104 -4.2771554 -4.2860379 -4.2762175 -4.2640772 -4.2609634 -4.2566137 -4.2498097 -4.2283707 -4.2007079 -4.1772885 -4.166358 -4.1746178][-4.2530074 -4.243906 -4.2633681 -4.2872648 -4.3015351 -4.3038273 -4.3018537 -4.3020172 -4.2945552 -4.2818685 -4.2581539 -4.2317524 -4.2109036 -4.2008715 -4.2035589][-4.2696695 -4.2638803 -4.2800388 -4.2981315 -4.3109984 -4.3173394 -4.318912 -4.3173852 -4.3083262 -4.2950015 -4.27459 -4.25287 -4.2377338 -4.2307878 -4.2343135][-4.2939162 -4.2902532 -4.3009815 -4.3122678 -4.3207293 -4.3272061 -4.3304019 -4.3286138 -4.3220468 -4.3124843 -4.2985239 -4.2837734 -4.27383 -4.2695632 -4.2712421][-4.3163271 -4.3134084 -4.3175564 -4.3209295 -4.3245816 -4.3303256 -4.3350344 -4.3364024 -4.3347116 -4.3302908 -4.323194 -4.314508 -4.3081489 -4.3046317 -4.3029947]]...]
INFO - root - 2017-12-07 13:37:19.641184: step 7110, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 2.213 sec/batch; 46h:43m:30s remains)
INFO - root - 2017-12-07 13:37:40.975992: step 7120, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 44h:53m:14s remains)
INFO - root - 2017-12-07 13:38:02.115429: step 7130, loss = 2.09, batch loss = 2.03 (16.8 examples/sec; 1.904 sec/batch; 40h:11m:38s remains)
INFO - root - 2017-12-07 13:38:23.125171: step 7140, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.144 sec/batch; 45h:15m:11s remains)
INFO - root - 2017-12-07 13:38:44.392960: step 7150, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.131 sec/batch; 44h:58m:13s remains)
INFO - root - 2017-12-07 13:39:05.613561: step 7160, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.076 sec/batch; 43h:48m:16s remains)
INFO - root - 2017-12-07 13:39:26.699871: step 7170, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 45h:34m:57s remains)
INFO - root - 2017-12-07 13:39:47.843162: step 7180, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.094 sec/batch; 44h:10m:48s remains)
INFO - root - 2017-12-07 13:40:08.911843: step 7190, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.092 sec/batch; 44h:07m:17s remains)
INFO - root - 2017-12-07 13:40:29.556090: step 7200, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 44h:09m:23s remains)
2017-12-07 13:40:31.205634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3616295 -4.3602629 -4.3563371 -4.3510952 -4.3493352 -4.3472733 -4.3408432 -4.3309655 -4.3242059 -4.3268318 -4.3405681 -4.3560152 -4.3645768 -4.3677139 -4.3624306][-4.3647847 -4.3607774 -4.3516755 -4.3385305 -4.3320017 -4.3283091 -4.3216438 -4.31212 -4.3043013 -4.3066053 -4.3233194 -4.3439608 -4.3584485 -4.3665662 -4.363256][-4.3672776 -4.3602409 -4.34411 -4.3199024 -4.3012509 -4.2891355 -4.2804294 -4.2756758 -4.275763 -4.2861676 -4.30835 -4.3328748 -4.3519239 -4.3648963 -4.3636646][-4.3669972 -4.3553104 -4.3312397 -4.2936125 -4.2563829 -4.2290206 -4.2141709 -4.21716 -4.2312713 -4.2558107 -4.2879567 -4.3219137 -4.3487306 -4.3658552 -4.3653593][-4.361609 -4.3436589 -4.3091497 -4.2559505 -4.1988821 -4.153429 -4.1312904 -4.1451263 -4.1775131 -4.2155986 -4.2560773 -4.3005309 -4.3387132 -4.3618298 -4.3632154][-4.35362 -4.3266745 -4.2755485 -4.2020369 -4.1207075 -4.0482082 -4.0132575 -4.0433922 -4.1032372 -4.1623778 -4.2168775 -4.2742157 -4.3234019 -4.3522573 -4.3579674][-4.3478422 -4.3106532 -4.2417283 -4.1467409 -4.0379057 -3.9247925 -3.8535345 -3.8986368 -4.0027347 -4.0955658 -4.1725326 -4.2460093 -4.3064613 -4.3418684 -4.3521895][-4.3458529 -4.303462 -4.2229805 -4.1136103 -3.9776573 -3.820569 -3.6986411 -3.7557809 -3.9139326 -4.0462065 -4.1475081 -4.2343616 -4.3001928 -4.3376169 -4.34949][-4.3479242 -4.3062649 -4.2248182 -4.1137509 -3.9781749 -3.8261852 -3.7125065 -3.770637 -3.9271142 -4.0611053 -4.1601787 -4.2421808 -4.3038039 -4.3397741 -4.350307][-4.3520827 -4.3173766 -4.2462978 -4.1475377 -4.0392709 -3.9369183 -3.8763628 -3.9251642 -4.0386114 -4.1390862 -4.2139635 -4.27484 -4.3207655 -4.3466163 -4.3527985][-4.3580451 -4.3322 -4.2769785 -4.1995749 -4.1208048 -4.059999 -4.0389662 -4.081883 -4.1616096 -4.2292018 -4.2783155 -4.3162708 -4.3434367 -4.355485 -4.35441][-4.36491 -4.3480535 -4.3120441 -4.2592669 -4.2067471 -4.1713977 -4.1679387 -4.2022867 -4.2544727 -4.2965083 -4.32512 -4.3458376 -4.3587837 -4.360754 -4.3550954][-4.3698206 -4.3620534 -4.3428912 -4.3125806 -4.2802529 -4.2595949 -4.259305 -4.2808537 -4.3102279 -4.3336864 -4.3488369 -4.3585296 -4.3630729 -4.3607907 -4.3541746][-4.3731079 -4.3700318 -4.3588495 -4.3397679 -4.3193946 -4.3066711 -4.3077617 -4.3219967 -4.3388753 -4.3513322 -4.3596129 -4.3640132 -4.3646317 -4.3602648 -4.353313][-4.3728366 -4.3700185 -4.362287 -4.3503704 -4.3377461 -4.330472 -4.333179 -4.343102 -4.3545723 -4.3632231 -4.3676591 -4.3682604 -4.3654456 -4.3590441 -4.3516707]]...]
INFO - root - 2017-12-07 13:40:52.458739: step 7210, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.134 sec/batch; 45h:00m:26s remains)
INFO - root - 2017-12-07 13:41:13.683929: step 7220, loss = 2.08, batch loss = 2.03 (14.7 examples/sec; 2.171 sec/batch; 45h:46m:24s remains)
INFO - root - 2017-12-07 13:41:34.755085: step 7230, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 2.048 sec/batch; 43h:10m:52s remains)
INFO - root - 2017-12-07 13:41:56.018145: step 7240, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.145 sec/batch; 45h:12m:42s remains)
INFO - root - 2017-12-07 13:42:17.134574: step 7250, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.082 sec/batch; 43h:52m:38s remains)
INFO - root - 2017-12-07 13:42:38.232652: step 7260, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 44h:14m:14s remains)
INFO - root - 2017-12-07 13:42:59.002239: step 7270, loss = 2.08, batch loss = 2.02 (15.7 examples/sec; 2.035 sec/batch; 42h:53m:10s remains)
INFO - root - 2017-12-07 13:43:20.044920: step 7280, loss = 2.06, batch loss = 2.01 (15.4 examples/sec; 2.073 sec/batch; 43h:40m:37s remains)
INFO - root - 2017-12-07 13:43:41.098030: step 7290, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.150 sec/batch; 45h:17m:20s remains)
INFO - root - 2017-12-07 13:44:01.935175: step 7300, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.094 sec/batch; 44h:05m:47s remains)
2017-12-07 13:44:03.505444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3356586 -4.3307614 -4.32422 -4.3156009 -4.3005233 -4.2907577 -4.2899303 -4.2912064 -4.2948761 -4.2984819 -4.3040328 -4.3122849 -4.3169389 -4.3174887 -4.3143129][-4.3313084 -4.3270154 -4.317153 -4.3005528 -4.2774391 -4.2638264 -4.2612314 -4.2646194 -4.2699876 -4.2737274 -4.2800021 -4.2921257 -4.2992287 -4.2979889 -4.2914143][-4.3269835 -4.3207231 -4.3030548 -4.2770085 -4.2447977 -4.2233677 -4.2135224 -4.2181058 -4.2284975 -4.2382255 -4.2494669 -4.2671566 -4.2762909 -4.2735963 -4.2641106][-4.3240943 -4.3149204 -4.2877483 -4.2486315 -4.2064667 -4.1759262 -4.1538548 -4.1583824 -4.178071 -4.2011461 -4.222661 -4.244523 -4.2529688 -4.2466674 -4.2328906][-4.3250175 -4.3162131 -4.2833524 -4.2329059 -4.1820078 -4.1322613 -4.0823288 -4.0758567 -4.1032357 -4.1442246 -4.1823454 -4.2093468 -4.2183137 -4.2109661 -4.1960187][-4.3290048 -4.3241429 -4.2884235 -4.22735 -4.1669645 -4.0910883 -3.9962192 -3.9677632 -4.010498 -4.0797291 -4.140162 -4.1722097 -4.1823425 -4.1757326 -4.167685][-4.330483 -4.3293853 -4.2882423 -4.2168469 -4.1406775 -4.0324235 -3.8834739 -3.8227234 -3.9026349 -4.0124664 -4.0956659 -4.1354504 -4.1493278 -4.1537127 -4.1555967][-4.3255792 -4.3237457 -4.28008 -4.2001209 -4.1016293 -3.9593227 -3.7549596 -3.6665633 -3.8113484 -3.971055 -4.0723853 -4.1173782 -4.1358218 -4.1485319 -4.156785][-4.31613 -4.3108196 -4.2656164 -4.1855536 -4.0798197 -3.9281301 -3.7267427 -3.6592951 -3.8376069 -4.0055027 -4.0919371 -4.1275496 -4.14198 -4.1550512 -4.165122][-4.3065681 -4.3015833 -4.2651143 -4.2032433 -4.1198182 -4.0050168 -3.8738103 -3.8439946 -3.9728034 -4.0888495 -4.1389031 -4.1586022 -4.1642475 -4.1709237 -4.1801567][-4.3049555 -4.3044176 -4.2783537 -4.238606 -4.1809893 -4.0990286 -4.0178933 -4.0036283 -4.07688 -4.1391282 -4.1648173 -4.1795797 -4.1870661 -4.1926255 -4.2009206][-4.3132706 -4.3172283 -4.3031683 -4.2782936 -4.2335987 -4.168232 -4.1132617 -4.1142931 -4.1538119 -4.1770167 -4.1845565 -4.1971021 -4.2129979 -4.2232542 -4.2309732][-4.3269696 -4.3318777 -4.322969 -4.3042207 -4.2697744 -4.2211857 -4.186554 -4.1987877 -4.2218351 -4.2233658 -4.2173624 -4.2285414 -4.2492714 -4.2605829 -4.2675619][-4.3416619 -4.3453727 -4.3387971 -4.3269281 -4.3060193 -4.2743082 -4.2519636 -4.2627845 -4.2775717 -4.2732334 -4.2656336 -4.27686 -4.2968802 -4.302403 -4.3016114][-4.3523879 -4.3545933 -4.3510795 -4.3452358 -4.3324742 -4.3136835 -4.302073 -4.3117561 -4.3242807 -4.3202968 -4.3144188 -4.3206434 -4.3334084 -4.3318572 -4.3222713]]...]
INFO - root - 2017-12-07 13:44:24.609466: step 7310, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.080 sec/batch; 43h:47m:58s remains)
INFO - root - 2017-12-07 13:44:45.829570: step 7320, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.159 sec/batch; 45h:27m:25s remains)
INFO - root - 2017-12-07 13:45:06.638767: step 7330, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.105 sec/batch; 44h:18m:50s remains)
INFO - root - 2017-12-07 13:45:28.008863: step 7340, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.192 sec/batch; 46h:08m:56s remains)
INFO - root - 2017-12-07 13:45:49.247885: step 7350, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.138 sec/batch; 44h:59m:30s remains)
INFO - root - 2017-12-07 13:46:10.282996: step 7360, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 1.950 sec/batch; 41h:02m:36s remains)
INFO - root - 2017-12-07 13:46:31.283080: step 7370, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 44h:43m:03s remains)
INFO - root - 2017-12-07 13:46:52.302690: step 7380, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.136 sec/batch; 44h:57m:00s remains)
INFO - root - 2017-12-07 13:47:13.260251: step 7390, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.094 sec/batch; 44h:03m:46s remains)
INFO - root - 2017-12-07 13:47:34.133699: step 7400, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.138 sec/batch; 44h:58m:47s remains)
2017-12-07 13:47:35.703770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3078632 -4.3007665 -4.2949567 -4.2926 -4.29273 -4.2942214 -4.2915449 -4.2909145 -4.3002615 -4.3111806 -4.3162322 -4.3114271 -4.3022742 -4.2974229 -4.2963967][-4.295104 -4.2887068 -4.2849059 -4.2833662 -4.2836709 -4.2838035 -4.2779527 -4.2757959 -4.2902379 -4.3075075 -4.3138542 -4.3069258 -4.2953482 -4.2888103 -4.2860756][-4.2821565 -4.2734432 -4.2644157 -4.2573962 -4.2551312 -4.2518873 -4.2414441 -4.239284 -4.2610931 -4.2860079 -4.2927375 -4.2860756 -4.2740226 -4.2684965 -4.2650571][-4.2685614 -4.2511106 -4.2293344 -4.2100873 -4.2029552 -4.198195 -4.1817904 -4.1775227 -4.2056756 -4.238596 -4.2470603 -4.2396483 -4.2291822 -4.2271647 -4.2241855][-4.2517881 -4.2239132 -4.1902575 -4.160192 -4.1495581 -4.1424336 -4.111445 -4.0880337 -4.111505 -4.156497 -4.169817 -4.165575 -4.1613021 -4.1644244 -4.1611056][-4.2357254 -4.2000608 -4.1559448 -4.115911 -4.0983829 -4.0850964 -4.0274796 -3.967227 -3.9795728 -4.0499916 -4.0815997 -4.0882287 -4.0912318 -4.0991464 -4.0935955][-4.2235556 -4.1791244 -4.12526 -4.0712719 -4.0396705 -4.0163083 -3.9263561 -3.8137555 -3.8143787 -3.9285393 -3.9988871 -4.0292244 -4.0449824 -4.0543146 -4.0476332][-4.2268391 -4.1829891 -4.1328859 -4.0797067 -4.0407968 -4.0036 -3.8971071 -3.7500842 -3.746613 -3.8916845 -3.9867558 -4.0319219 -4.0559983 -4.05977 -4.0470424][-4.2432723 -4.2126713 -4.1812372 -4.1447115 -4.1127782 -4.0789175 -3.9933853 -3.8776996 -3.8768806 -3.9876571 -4.0625172 -4.0981326 -4.1156626 -4.1111951 -4.0877337][-4.2601151 -4.2433038 -4.2262335 -4.199204 -4.172811 -4.14809 -4.09729 -4.0318189 -4.0376868 -4.1108232 -4.1570721 -4.1785755 -4.1883163 -4.1788859 -4.14828][-4.273623 -4.2623706 -4.2549081 -4.2383161 -4.223259 -4.2111826 -4.1872706 -4.151648 -4.1550217 -4.1998262 -4.2275205 -4.2401328 -4.245575 -4.2373886 -4.2122149][-4.2830529 -4.2714419 -4.2649565 -4.2582917 -4.2568316 -4.2580843 -4.2536631 -4.2341452 -4.2295394 -4.2511744 -4.2675557 -4.2751455 -4.2784138 -4.27421 -4.2588525][-4.2900634 -4.2755623 -4.26718 -4.2664385 -4.2746797 -4.2833972 -4.2870197 -4.2764149 -4.2675695 -4.2737617 -4.2818666 -4.2865214 -4.2886934 -4.2865329 -4.2772365][-4.300992 -4.2884431 -4.2818017 -4.2847705 -4.2935014 -4.2992697 -4.30161 -4.2958274 -4.2885275 -4.2891607 -4.2925892 -4.2949409 -4.2959442 -4.2939367 -4.2866478][-4.3143272 -4.3061271 -4.3017735 -4.3042989 -4.3073273 -4.3081412 -4.3090725 -4.3075752 -4.3056741 -4.3067589 -4.3091755 -4.3109961 -4.3109026 -4.3071575 -4.3017993]]...]
INFO - root - 2017-12-07 13:47:57.003952: step 7410, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 44h:34m:12s remains)
INFO - root - 2017-12-07 13:48:18.038787: step 7420, loss = 2.07, batch loss = 2.01 (16.2 examples/sec; 1.971 sec/batch; 41h:27m:19s remains)
INFO - root - 2017-12-07 13:48:39.050950: step 7430, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.130 sec/batch; 44h:47m:39s remains)
INFO - root - 2017-12-07 13:49:00.227648: step 7440, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 44h:55m:37s remains)
INFO - root - 2017-12-07 13:49:21.428517: step 7450, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.155 sec/batch; 45h:17m:36s remains)
INFO - root - 2017-12-07 13:49:42.208608: step 7460, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 44h:49m:28s remains)
INFO - root - 2017-12-07 13:50:03.411116: step 7470, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 44h:12m:46s remains)
INFO - root - 2017-12-07 13:50:24.657178: step 7480, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.171 sec/batch; 45h:37m:33s remains)
INFO - root - 2017-12-07 13:50:45.548265: step 7490, loss = 2.06, batch loss = 2.01 (15.1 examples/sec; 2.113 sec/batch; 44h:23m:36s remains)
INFO - root - 2017-12-07 13:51:06.697912: step 7500, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.123 sec/batch; 44h:36m:12s remains)
2017-12-07 13:51:08.308092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.18003 -4.20648 -4.2250619 -4.2349029 -4.244689 -4.2480741 -4.2429018 -4.235415 -4.2365351 -4.2491856 -4.2540417 -4.2466378 -4.231041 -4.2129869 -4.2125449][-4.1811576 -4.2030492 -4.2213378 -4.2295771 -4.2371 -4.2415967 -4.2414889 -4.2383552 -4.2444315 -4.262785 -4.2690358 -4.2584238 -4.242487 -4.2260389 -4.223556][-4.1773524 -4.1967254 -4.2156596 -4.2261248 -4.2319241 -4.2360859 -4.2358594 -4.2315583 -4.2360435 -4.2535667 -4.2567129 -4.2436347 -4.2305517 -4.2195625 -4.218297][-4.1740294 -4.1937857 -4.2111425 -4.2204113 -4.2209129 -4.2183933 -4.2117229 -4.20355 -4.2074661 -4.2263231 -4.2295475 -4.2203317 -4.2141476 -4.206943 -4.2057338][-4.1818752 -4.1987677 -4.2099538 -4.2142167 -4.205966 -4.1894288 -4.1726465 -4.1642013 -4.1758566 -4.2053647 -4.216753 -4.2152667 -4.2165866 -4.2117109 -4.2068844][-4.1960235 -4.2016172 -4.20022 -4.1932626 -4.1729879 -4.1393466 -4.1109605 -4.1040092 -4.1322966 -4.180326 -4.20253 -4.2064285 -4.2130308 -4.2130923 -4.2094917][-4.1860213 -4.1754136 -4.1593871 -4.1412244 -4.1104417 -4.0586405 -4.0162024 -4.0146632 -4.0630455 -4.1335068 -4.16961 -4.1772766 -4.1850314 -4.1898584 -4.1917615][-4.1462259 -4.1243896 -4.10323 -4.0867915 -4.0594673 -4.0050254 -3.9627063 -3.9715674 -4.0304108 -4.1097846 -4.1558633 -4.1674504 -4.1730628 -4.1749086 -4.1791248][-4.1242094 -4.1052523 -4.0936894 -4.0930929 -4.08543 -4.0547814 -4.0307021 -4.037312 -4.075706 -4.1341934 -4.1722808 -4.1855912 -4.19181 -4.1912642 -4.19368][-4.1436806 -4.1298947 -4.1230035 -4.1283207 -4.1320252 -4.1191807 -4.1066813 -4.1059308 -4.120719 -4.1568084 -4.186779 -4.2044287 -4.2147403 -4.2138143 -4.2131295][-4.1723719 -4.1594415 -4.1476955 -4.1498909 -4.1568971 -4.1518588 -4.1451612 -4.1458349 -4.1524467 -4.1719728 -4.1941071 -4.2138886 -4.2270875 -4.225317 -4.2178903][-4.1927624 -4.1802554 -4.1655135 -4.1674118 -4.1785746 -4.181953 -4.1817374 -4.1853333 -4.1896744 -4.1976752 -4.209764 -4.2257662 -4.2347136 -4.2276473 -4.2139015][-4.2245188 -4.2159991 -4.20365 -4.20349 -4.2143936 -4.2222276 -4.2264352 -4.2312441 -4.2334328 -4.2327604 -4.23626 -4.2449708 -4.2454643 -4.2307043 -4.2139916][-4.261426 -4.2573967 -4.2484422 -4.2461405 -4.2527237 -4.2589784 -4.2612047 -4.2607346 -4.2568011 -4.2493725 -4.2457395 -4.2450681 -4.2369914 -4.2206421 -4.2096167][-4.2857046 -4.2850666 -4.2792435 -4.2754459 -4.27718 -4.2784014 -4.2757373 -4.2700315 -4.261723 -4.2515335 -4.2436256 -4.2359319 -4.2235765 -4.2094421 -4.2046857]]...]
INFO - root - 2017-12-07 13:51:29.558941: step 7510, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.121 sec/batch; 44h:33m:29s remains)
INFO - root - 2017-12-07 13:51:50.514377: step 7520, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.106 sec/batch; 44h:14m:18s remains)
INFO - root - 2017-12-07 13:52:11.469586: step 7530, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 44h:33m:06s remains)
INFO - root - 2017-12-07 13:52:32.899056: step 7540, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 44h:29m:46s remains)
INFO - root - 2017-12-07 13:52:54.090772: step 7550, loss = 2.07, batch loss = 2.01 (16.2 examples/sec; 1.981 sec/batch; 41h:35m:04s remains)
INFO - root - 2017-12-07 13:53:15.413882: step 7560, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 44h:30m:17s remains)
INFO - root - 2017-12-07 13:53:36.685631: step 7570, loss = 2.07, batch loss = 2.02 (14.4 examples/sec; 2.219 sec/batch; 46h:34m:29s remains)
INFO - root - 2017-12-07 13:53:57.966881: step 7580, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.108 sec/batch; 44h:13m:56s remains)
INFO - root - 2017-12-07 13:54:18.815659: step 7590, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.143 sec/batch; 44h:58m:02s remains)
INFO - root - 2017-12-07 13:54:40.003765: step 7600, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.085 sec/batch; 43h:44m:28s remains)
2017-12-07 13:54:41.581014: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1749997 -4.1679626 -4.1966758 -4.2296548 -4.2566123 -4.2718921 -4.2852716 -4.3025985 -4.3286195 -4.347187 -4.3477745 -4.333909 -4.2873807 -4.2146549 -4.1596904][-4.1209555 -4.1086292 -4.1405044 -4.1792655 -4.21441 -4.2419167 -4.267168 -4.2971435 -4.3311234 -4.3524456 -4.3528872 -4.3331504 -4.2664084 -4.1588612 -4.0728292][-4.0575771 -4.0452213 -4.0837622 -4.1229582 -4.1618319 -4.2034278 -4.2491932 -4.29355 -4.3336506 -4.3545623 -4.3500519 -4.3180766 -4.2266355 -4.0741186 -3.9441416][-3.9897408 -3.9997244 -4.0591455 -4.0983696 -4.1280375 -4.1706843 -4.2301335 -4.2835326 -4.3244319 -4.3457861 -4.3359737 -4.2880783 -4.1672044 -3.9652176 -3.792351][-3.9593148 -4.0011191 -4.0785451 -4.1105108 -4.1170897 -4.1421113 -4.2020855 -4.2620845 -4.3055091 -4.3319287 -4.3251367 -4.2691307 -4.1330757 -3.9116812 -3.7212248][-3.9559081 -4.0055223 -4.0810809 -4.1022115 -4.0870681 -4.0950589 -4.1550746 -4.223175 -4.2732983 -4.3099771 -4.3183146 -4.2719975 -4.1462259 -3.942625 -3.7604754][-3.9699435 -3.9902763 -4.0342588 -4.0348115 -4.0023251 -4.0055947 -4.0751629 -4.1584496 -4.22122 -4.2739048 -4.304914 -4.2830606 -4.1901703 -4.0332527 -3.884167][-3.9932601 -3.9616172 -3.9580109 -3.9358394 -3.8932128 -3.9013586 -3.9907277 -4.0921488 -4.1674604 -4.23196 -4.2818627 -4.2874489 -4.2336817 -4.1329474 -4.0291815][-4.0340672 -3.9761255 -3.945282 -3.9100657 -3.8630276 -3.8668797 -3.9571073 -4.0666943 -4.15081 -4.217886 -4.27258 -4.293654 -4.2691317 -4.2091084 -4.1441774][-4.1040382 -4.0441718 -4.00667 -3.9716673 -3.9293272 -3.9220998 -3.9942608 -4.0953226 -4.1780763 -4.2371788 -4.28544 -4.3084469 -4.2997093 -4.2653952 -4.2300234][-4.1857615 -4.1301165 -4.0903988 -4.0599313 -4.0287919 -4.0221658 -4.0766058 -4.1594834 -4.2282867 -4.2735553 -4.3105083 -4.3252878 -4.3199086 -4.2984934 -4.2801766][-4.2571 -4.2110376 -4.1718144 -4.1443148 -4.1241975 -4.1233287 -4.164331 -4.2251887 -4.2745886 -4.3069482 -4.331656 -4.337923 -4.3277884 -4.3123479 -4.3050475][-4.3034372 -4.269978 -4.2376595 -4.2160983 -4.2053776 -4.2081332 -4.2352371 -4.2740889 -4.3055673 -4.3274021 -4.3429313 -4.3438921 -4.3298454 -4.3180246 -4.316143][-4.3278255 -4.3044295 -4.2806425 -4.266305 -4.2604532 -4.261343 -4.27332 -4.291389 -4.3086567 -4.3234973 -4.3329983 -4.3329816 -4.3203306 -4.3120418 -4.3169007][-4.3285189 -4.3107314 -4.2924414 -4.2820392 -4.2786503 -4.2763257 -4.2755332 -4.2767134 -4.280571 -4.2895122 -4.2958684 -4.295351 -4.2846174 -4.2811108 -4.293426]]...]
INFO - root - 2017-12-07 13:55:02.651878: step 7610, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.098 sec/batch; 44h:00m:45s remains)
INFO - root - 2017-12-07 13:55:23.436661: step 7620, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 44h:30m:35s remains)
INFO - root - 2017-12-07 13:55:44.498214: step 7630, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 44h:29m:01s remains)
INFO - root - 2017-12-07 13:56:05.610647: step 7640, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 44h:20m:23s remains)
INFO - root - 2017-12-07 13:56:26.400315: step 7650, loss = 2.08, batch loss = 2.02 (16.2 examples/sec; 1.975 sec/batch; 41h:24m:03s remains)
INFO - root - 2017-12-07 13:56:47.373893: step 7660, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.090 sec/batch; 43h:48m:47s remains)
INFO - root - 2017-12-07 13:57:08.522559: step 7670, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 44h:44m:22s remains)
INFO - root - 2017-12-07 13:57:29.580039: step 7680, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.111 sec/batch; 44h:13m:46s remains)
INFO - root - 2017-12-07 13:57:50.593019: step 7690, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.134 sec/batch; 44h:43m:07s remains)
INFO - root - 2017-12-07 13:58:11.656033: step 7700, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.095 sec/batch; 43h:53m:20s remains)
2017-12-07 13:58:13.132510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0753169 -4.1005578 -4.1362152 -4.1679525 -4.1868138 -4.1915407 -4.1830606 -4.1691756 -4.1533709 -4.1357374 -4.1116791 -4.1011953 -4.1079659 -4.1226521 -4.1494808][-4.0327663 -4.0529518 -4.0901542 -4.1259184 -4.1541071 -4.1735463 -4.1812825 -4.1841779 -4.1860843 -4.1800733 -4.1563368 -4.1420541 -4.142837 -4.1462631 -4.1593418][-4.0508428 -4.0621309 -4.0960169 -4.1325417 -4.164886 -4.1902223 -4.2055712 -4.2170434 -4.2260923 -4.2293358 -4.2148852 -4.2006907 -4.1883836 -4.1737719 -4.1639605][-4.1214018 -4.1236515 -4.1477957 -4.1765261 -4.202117 -4.2208991 -4.2299552 -4.2383437 -4.242363 -4.2457056 -4.2401156 -4.2293267 -4.2098441 -4.1831145 -4.1548357][-4.1666207 -4.1635642 -4.1793375 -4.1986485 -4.2139177 -4.2213187 -4.222589 -4.2266865 -4.2256923 -4.227417 -4.2301559 -4.22716 -4.2078309 -4.1774745 -4.1405816][-4.1778932 -4.1663795 -4.1695633 -4.1776133 -4.1842237 -4.1866269 -4.1878214 -4.1939359 -4.1937957 -4.1967297 -4.2040834 -4.2037339 -4.1865492 -4.1557431 -4.1146231][-4.1584148 -4.1386318 -4.131474 -4.1299706 -4.1311693 -4.1334748 -4.1374617 -4.1466455 -4.1478405 -4.1528997 -4.1657872 -4.16929 -4.1584077 -4.132987 -4.0936704][-4.1327844 -4.110795 -4.1001039 -4.095499 -4.099762 -4.1047091 -4.1094618 -4.1118851 -4.1090117 -4.11481 -4.1344852 -4.1505537 -4.1570449 -4.1498351 -4.1242852][-4.1064472 -4.0807204 -4.0705724 -4.0724955 -4.0825405 -4.0898719 -4.0935626 -4.0930119 -4.0896463 -4.0943317 -4.1180305 -4.1427746 -4.1659341 -4.1769209 -4.1689281][-4.0835533 -4.0530429 -4.0420179 -4.0441575 -4.0493407 -4.0481009 -4.0442748 -4.0414405 -4.0434241 -4.0566859 -4.0910339 -4.1256065 -4.1616154 -4.189364 -4.1973734][-4.0922084 -4.0627656 -4.0513387 -4.0495496 -4.0418019 -4.0235753 -4.0027909 -3.9893651 -3.9920831 -4.01312 -4.0578461 -4.1016793 -4.1445127 -4.182951 -4.2046471][-4.1513062 -4.1354008 -4.1326094 -4.131988 -4.1188226 -4.0940561 -4.0639973 -4.0372505 -4.0263939 -4.0359445 -4.0714822 -4.1063037 -4.1424084 -4.1781483 -4.2017174][-4.2229385 -4.225491 -4.2335386 -4.2371426 -4.2282248 -4.2105546 -4.1881824 -4.1639638 -4.1458621 -4.1403594 -4.152741 -4.1644559 -4.1783028 -4.1961184 -4.2092853][-4.2634563 -4.2799225 -4.2969551 -4.3052216 -4.30176 -4.292429 -4.2796316 -4.2640996 -4.2480869 -4.2379994 -4.2369604 -4.2340322 -4.2308445 -4.2317333 -4.2342153][-4.2623281 -4.2889528 -4.3120179 -4.3236036 -4.3239732 -4.3196979 -4.3118935 -4.3038907 -4.2956409 -4.2889295 -4.2844391 -4.2779183 -4.2700782 -4.2642717 -4.2615285]]...]
INFO - root - 2017-12-07 13:58:34.255013: step 7710, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 44h:03m:06s remains)
INFO - root - 2017-12-07 13:58:55.055036: step 7720, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.085 sec/batch; 43h:40m:39s remains)
INFO - root - 2017-12-07 13:59:16.077729: step 7730, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.098 sec/batch; 43h:56m:16s remains)
INFO - root - 2017-12-07 13:59:37.349361: step 7740, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 44h:17m:54s remains)
INFO - root - 2017-12-07 13:59:58.323390: step 7750, loss = 2.07, batch loss = 2.01 (14.0 examples/sec; 2.284 sec/batch; 47h:49m:52s remains)
INFO - root - 2017-12-07 14:00:19.549405: step 7760, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.124 sec/batch; 44h:27m:47s remains)
INFO - root - 2017-12-07 14:00:40.622399: step 7770, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.106 sec/batch; 44h:05m:23s remains)
INFO - root - 2017-12-07 14:01:01.280738: step 7780, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 1.930 sec/batch; 40h:23m:37s remains)
INFO - root - 2017-12-07 14:01:22.324298: step 7790, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 44h:19m:40s remains)
INFO - root - 2017-12-07 14:01:43.339231: step 7800, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.084 sec/batch; 43h:36m:40s remains)
2017-12-07 14:01:44.883771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2514005 -4.2461767 -4.2521782 -4.2727866 -4.2808185 -4.2656965 -4.2337461 -4.1979795 -4.1755924 -4.1800303 -4.2140565 -4.259191 -4.2959275 -4.3102188 -4.3015356][-4.2659831 -4.2595787 -4.2644811 -4.2797809 -4.27412 -4.2444577 -4.2030039 -4.1612349 -4.1388264 -4.1508117 -4.1961436 -4.2499933 -4.2873645 -4.2961135 -4.2791481][-4.2672381 -4.2665811 -4.2757292 -4.2848673 -4.2672958 -4.2268162 -4.17945 -4.1393905 -4.1281729 -4.15404 -4.2050886 -4.2556753 -4.2850637 -4.2864528 -4.261178][-4.2640109 -4.2705688 -4.2818356 -4.2837982 -4.255332 -4.2056766 -4.1508923 -4.1222138 -4.1354876 -4.1788435 -4.2309093 -4.2688313 -4.2867694 -4.2814879 -4.2524304][-4.2582803 -4.2672868 -4.2739472 -4.2648182 -4.2257133 -4.1637487 -4.0980363 -4.0868888 -4.1374664 -4.1994591 -4.2508097 -4.2778616 -4.2863359 -4.2777772 -4.2506466][-4.2557335 -4.2622914 -4.2577152 -4.2318149 -4.1800175 -4.0997906 -4.0154586 -4.024652 -4.1188979 -4.2038579 -4.2579274 -4.2823181 -4.2835703 -4.2737312 -4.2536006][-4.2609282 -4.2599196 -4.236412 -4.1866484 -4.1171079 -4.0169196 -3.9141016 -3.9516702 -4.0906439 -4.196661 -4.2521515 -4.2730808 -4.2692575 -4.2574406 -4.2464252][-4.2576804 -4.2488909 -4.2095742 -4.14235 -4.0684681 -3.9736118 -3.884403 -3.9428291 -4.0913506 -4.1930761 -4.2390852 -4.2525148 -4.2449737 -4.2334557 -4.2330489][-4.2396245 -4.225337 -4.1794376 -4.1115561 -4.0548329 -4.0022984 -3.9657204 -4.0248013 -4.1342945 -4.2047305 -4.2263932 -4.2277036 -4.216011 -4.2083807 -4.2222819][-4.2230992 -4.2017517 -4.1547136 -4.094789 -4.063056 -4.0570984 -4.0650525 -4.1137824 -4.1798425 -4.2162223 -4.2121816 -4.1971307 -4.1821914 -4.1848149 -4.2123365][-4.2160664 -4.1907969 -4.1439872 -4.0954504 -4.0887852 -4.1170654 -4.1465778 -4.1798191 -4.2113886 -4.2212706 -4.2034426 -4.175766 -4.1594458 -4.1720786 -4.207993][-4.2151756 -4.1872044 -4.1405649 -4.1043139 -4.1196365 -4.166213 -4.2063193 -4.2317548 -4.2391768 -4.2298365 -4.2053452 -4.1744947 -4.1593623 -4.174684 -4.2078419][-4.21379 -4.1871653 -4.1458254 -4.1240969 -4.1510692 -4.2010303 -4.2410169 -4.2584062 -4.2511525 -4.2299109 -4.2027159 -4.1758704 -4.1680908 -4.1831746 -4.2091293][-4.2154522 -4.1984706 -4.1703663 -4.1592388 -4.1816578 -4.2200007 -4.2495866 -4.2585068 -4.243259 -4.2140112 -4.1826162 -4.1600394 -4.1643605 -4.1868982 -4.2119546][-4.2246284 -4.2189355 -4.2064981 -4.2007613 -4.2080255 -4.222631 -4.2335763 -4.2337575 -4.2194529 -4.1933737 -4.1617575 -4.1394024 -4.1470523 -4.1730094 -4.2018437]]...]
INFO - root - 2017-12-07 14:02:05.874395: step 7810, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.098 sec/batch; 43h:53m:28s remains)
INFO - root - 2017-12-07 14:02:27.122548: step 7820, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.129 sec/batch; 44h:32m:11s remains)
INFO - root - 2017-12-07 14:02:48.405398: step 7830, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.159 sec/batch; 45h:09m:01s remains)
INFO - root - 2017-12-07 14:03:09.581618: step 7840, loss = 2.09, batch loss = 2.03 (16.2 examples/sec; 1.975 sec/batch; 41h:18m:27s remains)
INFO - root - 2017-12-07 14:03:30.446894: step 7850, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.111 sec/batch; 44h:08m:00s remains)
INFO - root - 2017-12-07 14:03:51.666568: step 7860, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.085 sec/batch; 43h:35m:16s remains)
INFO - root - 2017-12-07 14:04:12.853438: step 7870, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.115 sec/batch; 44h:12m:10s remains)
INFO - root - 2017-12-07 14:04:33.635857: step 7880, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.123 sec/batch; 44h:22m:04s remains)
INFO - root - 2017-12-07 14:04:54.963283: step 7890, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 44h:28m:42s remains)
INFO - root - 2017-12-07 14:05:16.018155: step 7900, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.102 sec/batch; 43h:55m:46s remains)
2017-12-07 14:05:17.616182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.160748 -4.1538372 -4.1711612 -4.1994619 -4.2116208 -4.213129 -4.2206135 -4.2257648 -4.2261424 -4.2241783 -4.2251277 -4.2179728 -4.2067881 -4.1993933 -4.1938915][-4.1225781 -4.12121 -4.1458125 -4.1815882 -4.2013879 -4.2098718 -4.2185259 -4.2214165 -4.2173839 -4.2122693 -4.2166824 -4.2098861 -4.1950054 -4.1850739 -4.1847434][-4.1277037 -4.1238861 -4.1379566 -4.1698074 -4.1990061 -4.2152286 -4.2209048 -4.2178173 -4.2077346 -4.203052 -4.2142215 -4.2104764 -4.1953406 -4.1855783 -4.18433][-4.1557541 -4.1463151 -4.1416607 -4.1589 -4.1851311 -4.199789 -4.2066751 -4.2062683 -4.1980343 -4.2051458 -4.2249703 -4.228375 -4.2173061 -4.2073207 -4.2038555][-4.1791639 -4.1696982 -4.1559224 -4.1562414 -4.1621451 -4.1578836 -4.15001 -4.1436324 -4.1427069 -4.1775355 -4.2222471 -4.2424765 -4.2377138 -4.2307148 -4.2283874][-4.197053 -4.1959429 -4.1835785 -4.16662 -4.1399212 -4.099093 -4.0540562 -4.0124345 -4.00426 -4.082026 -4.167974 -4.2115922 -4.2250166 -4.2321453 -4.2434115][-4.1914582 -4.1973357 -4.192647 -4.1661091 -4.1153631 -4.0392575 -3.9409013 -3.8254414 -3.7772779 -3.9073491 -4.0461984 -4.1255221 -4.1730165 -4.2091908 -4.2363877][-4.1470275 -4.1688666 -4.1821642 -4.16271 -4.1191025 -4.0528359 -3.9559886 -3.822928 -3.7505999 -3.8525496 -3.9725692 -4.0556517 -4.1259074 -4.1823592 -4.2151766][-4.0645075 -4.1158376 -4.1577349 -4.1645551 -4.1514497 -4.1196861 -4.0659394 -3.9949288 -3.960362 -4.0017085 -4.0466733 -4.0895658 -4.1389103 -4.1774378 -4.1968274][-4.0308747 -4.0999675 -4.1571417 -4.1818547 -4.1927309 -4.1866097 -4.16453 -4.14355 -4.1372218 -4.1502776 -4.1561284 -4.1690826 -4.1912379 -4.1986756 -4.1930485][-4.0867209 -4.1484284 -4.1947861 -4.2184272 -4.2274828 -4.22497 -4.2196035 -4.2199116 -4.2240725 -4.2274508 -4.225636 -4.2314777 -4.2443371 -4.2380404 -4.2161536][-4.1687431 -4.205544 -4.2362514 -4.2512836 -4.2548823 -4.2538795 -4.2541604 -4.2636447 -4.2696137 -4.2673445 -4.2627835 -4.2692342 -4.27883 -4.2654233 -4.2333212][-4.2309103 -4.24301 -4.2629447 -4.2711391 -4.2696004 -4.2678032 -4.2665639 -4.2759151 -4.2828441 -4.283905 -4.2780809 -4.27807 -4.2778692 -4.2615461 -4.2293539][-4.2808957 -4.2771454 -4.2840438 -4.2799416 -4.2697644 -4.2621436 -4.2557707 -4.2616892 -4.2708898 -4.2780685 -4.2746682 -4.2677336 -4.2546358 -4.2346163 -4.2091627][-4.3046837 -4.2952557 -4.2922883 -4.2796516 -4.2609205 -4.24589 -4.2318339 -4.2296643 -4.2408123 -4.252625 -4.2493196 -4.237412 -4.2192683 -4.2000494 -4.1850638]]...]
INFO - root - 2017-12-07 14:05:38.496855: step 7910, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.103 sec/batch; 43h:56m:31s remains)
INFO - root - 2017-12-07 14:05:59.772766: step 7920, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.085 sec/batch; 43h:33m:59s remains)
INFO - root - 2017-12-07 14:06:20.987780: step 7930, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 44h:28m:25s remains)
INFO - root - 2017-12-07 14:06:41.992592: step 7940, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 44h:16m:29s remains)
INFO - root - 2017-12-07 14:07:02.907927: step 7950, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.145 sec/batch; 44h:47m:19s remains)
INFO - root - 2017-12-07 14:07:24.138822: step 7960, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.172 sec/batch; 45h:20m:34s remains)
INFO - root - 2017-12-07 14:07:45.070546: step 7970, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 1.907 sec/batch; 39h:48m:39s remains)
INFO - root - 2017-12-07 14:08:06.173806: step 7980, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.129 sec/batch; 44h:26m:40s remains)
INFO - root - 2017-12-07 14:08:27.422669: step 7990, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.110 sec/batch; 44h:02m:14s remains)
INFO - root - 2017-12-07 14:08:48.432267: step 8000, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.080 sec/batch; 43h:24m:47s remains)
2017-12-07 14:08:50.026086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3109951 -4.3084054 -4.3090529 -4.3125129 -4.316854 -4.3183589 -4.3142505 -4.313436 -4.3185763 -4.3202972 -4.3166833 -4.3083854 -4.3012547 -4.2965283 -4.2924104][-4.3007383 -4.2954917 -4.2936625 -4.2961407 -4.301826 -4.3069654 -4.3062816 -4.3121777 -4.3252811 -4.3314071 -4.3271432 -4.3150682 -4.3043122 -4.2964339 -4.2895961][-4.2900572 -4.2791533 -4.268795 -4.263896 -4.2640285 -4.2670527 -4.2672472 -4.2801166 -4.3016968 -4.31417 -4.3138514 -4.3013453 -4.289794 -4.2835212 -4.2812548][-4.2829514 -4.2665477 -4.2507176 -4.2396054 -4.22991 -4.22645 -4.2269826 -4.2431264 -4.2675323 -4.2833323 -4.2887244 -4.2799072 -4.2699127 -4.2681189 -4.2723584][-4.2801585 -4.261375 -4.2415018 -4.2201138 -4.1955905 -4.1830239 -4.1834836 -4.198823 -4.2203059 -4.2345181 -4.2439914 -4.240416 -4.2367077 -4.2441311 -4.25789][-4.2776341 -4.2551956 -4.2300138 -4.192944 -4.1441607 -4.1150918 -4.1144605 -4.1289053 -4.14579 -4.158947 -4.1718526 -4.1762223 -4.1806726 -4.1960773 -4.2197528][-4.2722936 -4.2470827 -4.215662 -4.16381 -4.0933838 -4.0426826 -4.0310874 -4.0407629 -4.0574183 -4.0732503 -4.0875936 -4.0978975 -4.1116838 -4.1345887 -4.1630225][-4.2677031 -4.2414045 -4.2058153 -4.14764 -4.0669594 -3.9981775 -3.966495 -3.9648583 -3.9840143 -4.0025382 -4.0072608 -4.012917 -4.0325575 -4.062336 -4.0908637][-4.2651339 -4.2442088 -4.2150941 -4.16589 -4.097291 -4.034534 -3.9918714 -3.9799416 -3.9968369 -4.0074749 -3.9975584 -3.9949834 -4.0155511 -4.0518246 -4.0767775][-4.26951 -4.2579708 -4.2383671 -4.2084551 -4.1647429 -4.1211414 -4.0837154 -4.0676413 -4.0776505 -4.076 -4.0590177 -4.0515909 -4.063961 -4.0965514 -4.1196074][-4.2769194 -4.2724848 -4.2593293 -4.2446141 -4.2217522 -4.1979065 -4.1732769 -4.1585779 -4.1611929 -4.1534028 -4.1371684 -4.12772 -4.1306977 -4.1541362 -4.1746588][-4.2850285 -4.2841353 -4.2750192 -4.2682457 -4.2577195 -4.2473378 -4.2316456 -4.2193093 -4.2182145 -4.2126794 -4.2044449 -4.1957622 -4.1922884 -4.2072082 -4.2253823][-4.2915597 -4.2904439 -4.2832174 -4.2786131 -4.2731996 -4.27025 -4.2604952 -4.251451 -4.2490234 -4.248199 -4.2484307 -4.2438631 -4.2401752 -4.2497826 -4.2630491][-4.2979174 -4.2946291 -4.2882257 -4.2844543 -4.2806587 -4.2776141 -4.2705088 -4.2653136 -4.2657309 -4.2678609 -4.2743573 -4.2752905 -4.2740321 -4.28076 -4.2906613][-4.3031054 -4.2957048 -4.2874436 -4.2817545 -4.2763448 -4.2706819 -4.2656746 -4.2648153 -4.2667284 -4.2686529 -4.2751255 -4.2779059 -4.2793951 -4.2865195 -4.2980356]]...]
INFO - root - 2017-12-07 14:09:10.817853: step 8010, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.108 sec/batch; 43h:59m:08s remains)
INFO - root - 2017-12-07 14:09:31.917553: step 8020, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.074 sec/batch; 43h:16m:05s remains)
INFO - root - 2017-12-07 14:09:53.115181: step 8030, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.099 sec/batch; 43h:47m:11s remains)
INFO - root - 2017-12-07 14:10:13.855973: step 8040, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.062 sec/batch; 42h:59m:53s remains)
INFO - root - 2017-12-07 14:10:35.059253: step 8050, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.092 sec/batch; 43h:37m:48s remains)
INFO - root - 2017-12-07 14:10:55.912779: step 8060, loss = 2.07, batch loss = 2.02 (15.5 examples/sec; 2.058 sec/batch; 42h:54m:37s remains)
INFO - root - 2017-12-07 14:11:16.740516: step 8070, loss = 2.08, batch loss = 2.02 (16.2 examples/sec; 1.978 sec/batch; 41h:14m:13s remains)
INFO - root - 2017-12-07 14:11:37.983866: step 8080, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.144 sec/batch; 44h:41m:01s remains)
INFO - root - 2017-12-07 14:11:59.323156: step 8090, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 44h:09m:50s remains)
INFO - root - 2017-12-07 14:12:20.395892: step 8100, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.126 sec/batch; 44h:18m:15s remains)
2017-12-07 14:12:22.052871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25741 -4.2479692 -4.2375045 -4.2266107 -4.2151284 -4.2040434 -4.1941504 -4.1865926 -4.1875443 -4.2026887 -4.2309246 -4.2680783 -4.2934361 -4.2955546 -4.2738438][-4.253428 -4.2416277 -4.2276077 -4.2126665 -4.1955352 -4.1743011 -4.1528435 -4.13624 -4.1349745 -4.1605043 -4.2028017 -4.2485981 -4.2788363 -4.2817488 -4.256074][-4.252769 -4.2392063 -4.2196937 -4.1972671 -4.17397 -4.14595 -4.1134119 -4.085001 -4.0799551 -4.1151338 -4.1715927 -4.225204 -4.2574 -4.2589812 -4.22871][-4.2549834 -4.2402754 -4.2160697 -4.1901402 -4.1645164 -4.129817 -4.0858746 -4.0520535 -4.0492916 -4.0895624 -4.1509595 -4.2036138 -4.2313628 -4.2282643 -4.1954188][-4.2561045 -4.2409873 -4.2164264 -4.189703 -4.1582327 -4.1102262 -4.0506544 -4.01431 -4.0222869 -4.0757623 -4.1402555 -4.1897707 -4.2102361 -4.2043614 -4.1735172][-4.2499146 -4.2346735 -4.2138476 -4.1874838 -4.1414895 -4.0628452 -3.973655 -3.9329262 -3.9695482 -4.0518794 -4.1259093 -4.1747231 -4.1982069 -4.1954384 -4.1691322][-4.2393818 -4.2251 -4.2093868 -4.1835475 -4.1187234 -3.9947081 -3.8518357 -3.8055196 -3.8966036 -4.0270691 -4.116281 -4.168 -4.1981483 -4.2032919 -4.183002][-4.2284565 -4.2156434 -4.2027674 -4.1747589 -4.0924234 -3.9306633 -3.7306623 -3.684459 -3.8374226 -4.0127258 -4.11833 -4.1740704 -4.2051749 -4.2146478 -4.2010608][-4.2220664 -4.211762 -4.2001233 -4.173749 -4.0937958 -3.9452207 -3.7677543 -3.73983 -3.8935471 -4.0542083 -4.1511564 -4.2001057 -4.2223358 -4.2265086 -4.2141638][-4.2208405 -4.2171907 -4.2095232 -4.1888952 -4.1285672 -4.0330048 -3.9364851 -3.9354095 -4.0433483 -4.1459394 -4.2114649 -4.2440577 -4.2497368 -4.24435 -4.2309747][-4.2305655 -4.2338176 -4.2291312 -4.2127476 -4.1737862 -4.1192079 -4.0748563 -4.0918632 -4.1619825 -4.2205186 -4.2608585 -4.2792616 -4.2763381 -4.2664337 -4.2568874][-4.2332916 -4.239306 -4.2365885 -4.2205868 -4.1903872 -4.1512628 -4.127511 -4.15657 -4.2163296 -4.2599592 -4.2908421 -4.3030872 -4.2981024 -4.2877212 -4.2818437][-4.2303047 -4.2390194 -4.2366719 -4.2150869 -4.1818576 -4.148232 -4.1405573 -4.1848755 -4.2429833 -4.2784004 -4.3039222 -4.3130279 -4.3094587 -4.3003988 -4.294003][-4.2369428 -4.2474875 -4.2442808 -4.2241211 -4.1927934 -4.165391 -4.1678262 -4.2182951 -4.2708797 -4.2994289 -4.3180451 -4.3241353 -4.3197393 -4.309731 -4.2997603][-4.2505927 -4.2652063 -4.26287 -4.2460608 -4.2231479 -4.2075505 -4.2170835 -4.2635403 -4.3059964 -4.3260369 -4.3367224 -4.3379655 -4.3297749 -4.316431 -4.3023877]]...]
INFO - root - 2017-12-07 14:12:42.837585: step 8110, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.127 sec/batch; 44h:19m:10s remains)
INFO - root - 2017-12-07 14:13:03.921000: step 8120, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.198 sec/batch; 45h:47m:59s remains)
INFO - root - 2017-12-07 14:13:25.123377: step 8130, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 44h:36m:55s remains)
INFO - root - 2017-12-07 14:13:45.836267: step 8140, loss = 2.06, batch loss = 2.00 (15.5 examples/sec; 2.059 sec/batch; 42h:53m:51s remains)
INFO - root - 2017-12-07 14:14:06.983053: step 8150, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.135 sec/batch; 44h:27m:46s remains)
INFO - root - 2017-12-07 14:14:27.960686: step 8160, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.121 sec/batch; 44h:09m:23s remains)
INFO - root - 2017-12-07 14:14:48.819984: step 8170, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 44h:14m:34s remains)
INFO - root - 2017-12-07 14:15:09.931447: step 8180, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.163 sec/batch; 45h:01m:33s remains)
INFO - root - 2017-12-07 14:15:31.179382: step 8190, loss = 2.08, batch loss = 2.02 (14.5 examples/sec; 2.201 sec/batch; 45h:49m:16s remains)
INFO - root - 2017-12-07 14:15:52.030761: step 8200, loss = 2.07, batch loss = 2.01 (16.3 examples/sec; 1.969 sec/batch; 40h:59m:01s remains)
2017-12-07 14:15:53.627075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3224926 -4.3096213 -4.2945113 -4.278883 -4.2704096 -4.2680378 -4.2704659 -4.269722 -4.2699189 -4.2744055 -4.2853322 -4.2985954 -4.3149242 -4.3291297 -4.3375444][-4.3335004 -4.3231516 -4.3091583 -4.2947755 -4.2877474 -4.2843313 -4.2851553 -4.2784829 -4.2680073 -4.2635117 -4.27012 -4.2859206 -4.3096747 -4.3313951 -4.3475461][-4.3438764 -4.33552 -4.3198862 -4.302969 -4.2942 -4.28644 -4.2823195 -4.2679296 -4.2519164 -4.2420855 -4.2452474 -4.261055 -4.2892504 -4.3174152 -4.3393087][-4.3508782 -4.3410006 -4.3221192 -4.29967 -4.2851195 -4.2685113 -4.2518654 -4.2277489 -4.2091627 -4.1974945 -4.2011418 -4.2213807 -4.2561631 -4.2935 -4.32206][-4.3544464 -4.3406458 -4.3131647 -4.2773504 -4.2485394 -4.2138672 -4.1715441 -4.1290755 -4.1114879 -4.1175628 -4.1381431 -4.1733537 -4.2184639 -4.2666364 -4.3037395][-4.3479323 -4.3241668 -4.2839823 -4.2322383 -4.1848936 -4.1243467 -4.0401726 -3.962045 -3.9546225 -4.0058355 -4.0690494 -4.1334772 -4.1965036 -4.2542272 -4.294106][-4.3320804 -4.2975254 -4.2432084 -4.1763692 -4.1073518 -4.0159922 -3.8842647 -3.7586639 -3.7782812 -3.9051514 -4.026216 -4.1203389 -4.1953745 -4.2533035 -4.2870154][-4.3038082 -4.2586212 -4.1929164 -4.1138611 -4.0306745 -3.9265442 -3.7875774 -3.6661334 -3.7339568 -3.913084 -4.050159 -4.1423397 -4.2083569 -4.2554488 -4.2785335][-4.2774119 -4.2271252 -4.1609149 -4.0864496 -4.0158782 -3.9453015 -3.8711438 -3.826411 -3.8982594 -4.0372686 -4.1380439 -4.2023268 -4.2437496 -4.2727256 -4.285223][-4.2693086 -4.2229757 -4.1665239 -4.1103611 -4.0688071 -4.0431204 -4.0269728 -4.0275679 -4.07791 -4.167037 -4.2321596 -4.2710152 -4.2920494 -4.3053694 -4.3100863][-4.2766218 -4.240757 -4.20167 -4.1685352 -4.1510239 -4.1525321 -4.1638689 -4.1748986 -4.205133 -4.2605958 -4.3020253 -4.3251457 -4.3358808 -4.3409905 -4.3394833][-4.29754 -4.2746711 -4.2528262 -4.237802 -4.2352734 -4.2469907 -4.26526 -4.2744813 -4.2886257 -4.3174486 -4.3416543 -4.3561635 -4.36276 -4.3645349 -4.3594761][-4.316855 -4.3037472 -4.2939076 -4.288506 -4.2901492 -4.3001022 -4.3158426 -4.3262234 -4.3326836 -4.3434267 -4.3555608 -4.3640857 -4.3664522 -4.3645897 -4.3584795][-4.3315053 -4.3234096 -4.3175864 -4.314383 -4.3153071 -4.3227673 -4.3367333 -4.3460531 -4.3502774 -4.3542457 -4.3588138 -4.3600359 -4.3577065 -4.3541017 -4.3489432][-4.3446631 -4.3395309 -4.3347859 -4.3319659 -4.3320785 -4.3380957 -4.3490462 -4.3565316 -4.3599367 -4.3606877 -4.3601828 -4.3578091 -4.3544812 -4.3516273 -4.34757]]...]
INFO - root - 2017-12-07 14:16:14.975800: step 8210, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.113 sec/batch; 43h:58m:19s remains)
INFO - root - 2017-12-07 14:16:36.234666: step 8220, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.081 sec/batch; 43h:18m:21s remains)
INFO - root - 2017-12-07 14:16:57.148648: step 8230, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.085 sec/batch; 43h:22m:57s remains)
INFO - root - 2017-12-07 14:17:18.132249: step 8240, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.113 sec/batch; 43h:56m:35s remains)
INFO - root - 2017-12-07 14:17:39.581643: step 8250, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.185 sec/batch; 45h:27m:07s remains)
2017-12-07 14:17:40.646568: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 921.38MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2017-12-07 14:17:40.646843: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 843.00MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
INFO - root - 2017-12-07 14:18:00.794709: step 8260, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.138 sec/batch; 44h:27m:20s remains)
INFO - root - 2017-12-07 14:18:21.585432: step 8270, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.093 sec/batch; 43h:31m:24s remains)
INFO - root - 2017-12-07 14:18:42.572743: step 8280, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.063 sec/batch; 42h:53m:06s remains)
INFO - root - 2017-12-07 14:19:03.648210: step 8290, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 43h:31m:17s remains)
INFO - root - 2017-12-07 14:19:24.594279: step 8300, loss = 2.06, batch loss = 2.00 (14.5 examples/sec; 2.205 sec/batch; 45h:49m:39s remains)
2017-12-07 14:19:26.223380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2585669 -4.2606297 -4.2586732 -4.2499948 -4.2388115 -4.2275696 -4.2263942 -4.2314973 -4.2398567 -4.2521353 -4.2688227 -4.2775459 -4.2828679 -4.2930584 -4.3023038][-4.2597542 -4.2643051 -4.2608252 -4.2493205 -4.23407 -4.2182884 -4.2125359 -4.2156963 -4.2230291 -4.2341342 -4.2506862 -4.2592058 -4.2683764 -4.2827754 -4.2949071][-4.2600393 -4.2678256 -4.2663178 -4.2556825 -4.2388344 -4.2187438 -4.2082715 -4.2089968 -4.216239 -4.2279377 -4.2424545 -4.2498293 -4.2597332 -4.2734432 -4.2845831][-4.245903 -4.2588019 -4.2641158 -4.2579837 -4.240097 -4.21467 -4.1982994 -4.1911831 -4.1937623 -4.2057085 -4.2226324 -4.2349477 -4.2484241 -4.2627826 -4.27414][-4.2227149 -4.2379651 -4.2457504 -4.2418222 -4.2244973 -4.1972756 -4.1707377 -4.146081 -4.1419182 -4.1608324 -4.1876392 -4.2064209 -4.2232394 -4.2421985 -4.2594643][-4.1947966 -4.20406 -4.2023845 -4.1905966 -4.1725883 -4.1414442 -4.0981112 -4.04513 -4.0349746 -4.0787635 -4.1271467 -4.158741 -4.1830883 -4.2104249 -4.2371726][-4.1652012 -4.1634045 -4.1460905 -4.1223211 -4.0985141 -4.05985 -3.9913564 -3.8983698 -3.8812065 -3.9601283 -4.0386147 -4.0906315 -4.1329594 -4.1765218 -4.2159095][-4.1591258 -4.1504784 -4.1286263 -4.1027174 -4.0787363 -4.0364056 -3.9535747 -3.83986 -3.8145535 -3.8994212 -3.9831939 -4.0443087 -4.0989451 -4.15326 -4.2022796][-4.18261 -4.1752887 -4.1640816 -4.1532588 -4.1427789 -4.1146269 -4.0516376 -3.9661064 -3.9371324 -3.9804797 -4.0296893 -4.0733709 -4.1193094 -4.1664567 -4.209796][-4.2230573 -4.2200689 -4.2210126 -4.2250929 -4.2257018 -4.2092919 -4.1682882 -4.1131825 -4.0822468 -4.0898623 -4.1082497 -4.1353111 -4.1675854 -4.19986 -4.2304125][-4.2626562 -4.2630687 -4.2691631 -4.27745 -4.2808404 -4.269311 -4.2434263 -4.2098722 -4.1789441 -4.1653867 -4.1645088 -4.1772962 -4.1987081 -4.2200904 -4.2438779][-4.2880878 -4.2892227 -4.2950206 -4.3019872 -4.3052211 -4.2991037 -4.286252 -4.2667565 -4.24009 -4.217792 -4.2030563 -4.2013683 -4.2129097 -4.2280455 -4.25004][-4.3000879 -4.3003135 -4.3033628 -4.306694 -4.308496 -4.3070793 -4.3024182 -4.2911472 -4.2711711 -4.2494764 -4.2302575 -4.2219911 -4.2293825 -4.2426615 -4.2640128][-4.311718 -4.3111372 -4.3113313 -4.3127732 -4.3149734 -4.3158135 -4.3146534 -4.3085856 -4.2954159 -4.2785397 -4.2623467 -4.2539482 -4.2580204 -4.2699518 -4.2878222][-4.3218122 -4.3208866 -4.3200512 -4.3199773 -4.3209863 -4.3215842 -4.3208175 -4.3166881 -4.3086796 -4.2978029 -4.2864604 -4.2795987 -4.2827992 -4.293489 -4.3073912]]...]
INFO - root - 2017-12-07 14:19:47.523912: step 8310, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.135 sec/batch; 44h:22m:18s remains)
INFO - root - 2017-12-07 14:20:08.956465: step 8320, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.140 sec/batch; 44h:27m:51s remains)
INFO - root - 2017-12-07 14:20:29.477182: step 8330, loss = 2.08, batch loss = 2.03 (15.6 examples/sec; 2.053 sec/batch; 42h:39m:35s remains)
INFO - root - 2017-12-07 14:20:50.639240: step 8340, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 44h:18m:11s remains)
INFO - root - 2017-12-07 14:21:11.920412: step 8350, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.138 sec/batch; 44h:24m:21s remains)
INFO - root - 2017-12-07 14:21:33.207201: step 8360, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 44h:17m:39s remains)
INFO - root - 2017-12-07 14:21:54.112439: step 8370, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.100 sec/batch; 43h:36m:16s remains)
INFO - root - 2017-12-07 14:22:15.360657: step 8380, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.103 sec/batch; 43h:39m:13s remains)
INFO - root - 2017-12-07 14:22:36.542602: step 8390, loss = 2.08, batch loss = 2.02 (16.6 examples/sec; 1.923 sec/batch; 39h:54m:46s remains)
INFO - root - 2017-12-07 14:22:57.270877: step 8400, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 43h:42m:03s remains)
2017-12-07 14:22:58.800725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1632481 -4.2277918 -4.2851176 -4.317853 -4.3245354 -4.3202338 -4.3162537 -4.3169847 -4.3213825 -4.3229523 -4.3193927 -4.3142829 -4.3104906 -4.3114552 -4.3159046][-4.2518072 -4.2945743 -4.3282032 -4.3373408 -4.3261566 -4.3081422 -4.2968831 -4.300148 -4.31333 -4.3236928 -4.3253689 -4.3207684 -4.3144159 -4.3117003 -4.3134313][-4.3136158 -4.3361082 -4.3485546 -4.3382969 -4.3135242 -4.2828908 -4.2636638 -4.2662745 -4.2864203 -4.3080482 -4.3200703 -4.3199739 -4.3118687 -4.3042274 -4.301127][-4.34351 -4.3518262 -4.347909 -4.3213758 -4.2837286 -4.2449069 -4.2240162 -4.2302794 -4.2562943 -4.2844391 -4.3072219 -4.3158126 -4.3091297 -4.2986469 -4.290225][-4.3475304 -4.345325 -4.3275361 -4.2866173 -4.2350655 -4.1888 -4.1690006 -4.1842971 -4.2226567 -4.2604079 -4.2924247 -4.309442 -4.3094926 -4.302536 -4.2923961][-4.3414674 -4.3268814 -4.2908115 -4.2302585 -4.1610417 -4.1001511 -4.0723338 -4.0986857 -4.1616817 -4.2219648 -4.2673397 -4.294241 -4.3061824 -4.3106017 -4.3068652][-4.3418474 -4.318131 -4.2657957 -4.1843791 -4.0898032 -3.9966507 -3.9388475 -3.9681816 -4.0621829 -4.1530714 -4.2197161 -4.2634916 -4.2915897 -4.3077273 -4.3138142][-4.3508282 -4.3285809 -4.276166 -4.187861 -4.0735917 -3.9443107 -3.8413072 -3.8516619 -3.9605026 -4.0732627 -4.1588268 -4.2190642 -4.2623034 -4.2879462 -4.3027744][-4.3604193 -4.34744 -4.3094854 -4.23552 -4.1286769 -4.0002856 -3.891541 -3.876596 -3.9573596 -4.0539474 -4.133347 -4.1937542 -4.2375813 -4.2627859 -4.2800317][-4.3656888 -4.3602958 -4.3392038 -4.2924557 -4.2152877 -4.1164494 -4.0327516 -4.0138946 -4.0600328 -4.1251888 -4.1783476 -4.2136292 -4.2338657 -4.2409515 -4.2491517][-4.366879 -4.3660316 -4.3588328 -4.3372421 -4.2946653 -4.2322907 -4.1744776 -4.1546721 -4.1764989 -4.2161202 -4.2466049 -4.258204 -4.2516608 -4.2355247 -4.2260623][-4.3658071 -4.3675308 -4.3702426 -4.3686676 -4.3530278 -4.3187122 -4.2800012 -4.2611246 -4.2674079 -4.28673 -4.2973323 -4.2904134 -4.268681 -4.2415085 -4.2211504][-4.3643937 -4.3658652 -4.3715911 -4.3784218 -4.3764648 -4.3597207 -4.3370347 -4.3246188 -4.325 -4.3314104 -4.32508 -4.3013339 -4.2682552 -4.2357841 -4.2157326][-4.3635888 -4.3649282 -4.3692613 -4.3764105 -4.3795042 -4.3707204 -4.3573418 -4.3483105 -4.3446937 -4.3414507 -4.3219295 -4.2836256 -4.2359395 -4.1959548 -4.1810532][-4.3638039 -4.3648176 -4.3676887 -4.371232 -4.3734751 -4.3681583 -4.3585696 -4.3494196 -4.3423872 -4.3282 -4.2905116 -4.2308359 -4.1625242 -4.1073723 -4.089016]]...]
INFO - root - 2017-12-07 14:23:19.906796: step 8410, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 43h:50m:54s remains)
INFO - root - 2017-12-07 14:23:41.129794: step 8420, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 44h:18m:36s remains)
INFO - root - 2017-12-07 14:24:01.967989: step 8430, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.153 sec/batch; 44h:40m:34s remains)
INFO - root - 2017-12-07 14:24:23.179001: step 8440, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.164 sec/batch; 44h:53m:17s remains)
INFO - root - 2017-12-07 14:24:44.414660: step 8450, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.136 sec/batch; 44h:18m:15s remains)
INFO - root - 2017-12-07 14:25:05.380079: step 8460, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.156 sec/batch; 44h:42m:37s remains)
INFO - root - 2017-12-07 14:25:26.404876: step 8470, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.084 sec/batch; 43h:13m:04s remains)
INFO - root - 2017-12-07 14:25:47.441191: step 8480, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.100 sec/batch; 43h:32m:36s remains)
INFO - root - 2017-12-07 14:26:08.510748: step 8490, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 43h:55m:32s remains)
INFO - root - 2017-12-07 14:26:29.513329: step 8500, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.128 sec/batch; 44h:07m:01s remains)
2017-12-07 14:26:31.019209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.269012 -4.2744327 -4.2779346 -4.2814064 -4.2846265 -4.2811975 -4.2746086 -4.27418 -4.2729511 -4.2673292 -4.2645178 -4.2618465 -4.2593241 -4.2601724 -4.2615547][-4.2759957 -4.2712121 -4.2665009 -4.2656837 -4.2672496 -4.2626657 -4.2546906 -4.2564816 -4.2562537 -4.2478595 -4.2435551 -4.2408047 -4.2367506 -4.2373748 -4.2418718][-4.2824659 -4.2650046 -4.2467155 -4.2361269 -4.2345328 -4.2306209 -4.2271972 -4.2315454 -4.23317 -4.2271862 -4.2273769 -4.2218747 -4.2150035 -4.2127595 -4.2196741][-4.2805142 -4.255909 -4.2264528 -4.2029176 -4.1930189 -4.1909623 -4.1958275 -4.2001905 -4.2068114 -4.209764 -4.2162275 -4.2080903 -4.19615 -4.1936646 -4.2074761][-4.2720118 -4.2497082 -4.2163796 -4.1804414 -4.1550117 -4.1464782 -4.1460814 -4.1443458 -4.162158 -4.1881375 -4.2074795 -4.2011251 -4.1858416 -4.1824145 -4.1998019][-4.2683792 -4.2517471 -4.2161927 -4.1640143 -4.1120934 -4.0804176 -4.051497 -4.0319691 -4.0710654 -4.1361542 -4.1799426 -4.1797204 -4.1618981 -4.1494904 -4.160542][-4.2560692 -4.2396464 -4.2005105 -4.1347728 -4.0568566 -3.9861152 -3.9011054 -3.8373787 -3.9006605 -4.0225782 -4.1064758 -4.1240287 -4.1052103 -4.0807123 -4.0834303][-4.205564 -4.1958961 -4.1589727 -4.0908318 -4.0041771 -3.9106622 -3.7875934 -3.6829431 -3.7666764 -3.9360735 -4.0472865 -4.0719175 -4.0463943 -4.0137568 -4.0143933][-4.151454 -4.1512489 -4.1340976 -4.0954137 -4.0427647 -3.9775469 -3.8869278 -3.8114467 -3.8743961 -4.0060883 -4.0887108 -4.0937791 -4.0579977 -4.0228581 -4.0206327][-4.1378975 -4.1350183 -4.1320543 -4.1254387 -4.111259 -4.0846868 -4.0408659 -4.0073419 -4.0480189 -4.1226015 -4.1687937 -4.162128 -4.1236944 -4.0823975 -4.0693393][-4.1535492 -4.1370068 -4.1336904 -4.1434221 -4.1557035 -4.1584511 -4.1464834 -4.1410794 -4.1707215 -4.210578 -4.2350368 -4.2289944 -4.1934485 -4.1455722 -4.1208477][-4.2037311 -4.1786361 -4.1709442 -4.1860371 -4.2081189 -4.2234864 -4.2263618 -4.2298284 -4.2491388 -4.2661071 -4.2738352 -4.2675977 -4.2391691 -4.1940651 -4.1665711][-4.2658157 -4.2428951 -4.2325134 -4.2437286 -4.2609076 -4.2729487 -4.2744765 -4.2745357 -4.2799921 -4.2808814 -4.2791805 -4.2774734 -4.2615452 -4.2278023 -4.2054539][-4.3005285 -4.2846217 -4.2744374 -4.2800078 -4.2926669 -4.3011336 -4.2995133 -4.28894 -4.2803144 -4.2733622 -4.2751904 -4.28356 -4.2786112 -4.2568836 -4.2399092][-4.3076468 -4.2986665 -4.2922163 -4.2944846 -4.3027606 -4.3074527 -4.3013382 -4.2866192 -4.2739511 -4.268146 -4.27609 -4.2902594 -4.2930136 -4.280076 -4.2694049]]...]
INFO - root - 2017-12-07 14:26:52.299114: step 8510, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.125 sec/batch; 44h:02m:32s remains)
INFO - root - 2017-12-07 14:27:13.449687: step 8520, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.108 sec/batch; 43h:41m:14s remains)
INFO - root - 2017-12-07 14:27:34.613943: step 8530, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 43h:41m:48s remains)
INFO - root - 2017-12-07 14:27:55.758403: step 8540, loss = 2.09, batch loss = 2.04 (15.3 examples/sec; 2.087 sec/batch; 43h:14m:11s remains)
INFO - root - 2017-12-07 14:28:16.967566: step 8550, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.136 sec/batch; 44h:14m:40s remains)
INFO - root - 2017-12-07 14:28:37.730946: step 8560, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.093 sec/batch; 43h:20m:39s remains)
INFO - root - 2017-12-07 14:28:58.882585: step 8570, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.142 sec/batch; 44h:21m:26s remains)
INFO - root - 2017-12-07 14:29:19.884722: step 8580, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.151 sec/batch; 44h:32m:22s remains)
INFO - root - 2017-12-07 14:29:40.818002: step 8590, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.097 sec/batch; 43h:25m:19s remains)
INFO - root - 2017-12-07 14:30:01.924048: step 8600, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 44h:13m:59s remains)
2017-12-07 14:30:03.513705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2711468 -4.2631946 -4.239769 -4.2159743 -4.200459 -4.1988173 -4.2082381 -4.2264671 -4.2460384 -4.262332 -4.2788539 -4.2753782 -4.270575 -4.2606673 -4.2446685][-4.2549891 -4.2495508 -4.2296944 -4.1968012 -4.1733284 -4.173738 -4.1919332 -4.2195287 -4.2406707 -4.2503977 -4.2650928 -4.2644162 -4.2631388 -4.2536135 -4.2303796][-4.2432227 -4.2361326 -4.2178674 -4.1835494 -4.153266 -4.1485529 -4.1710181 -4.2080326 -4.2354765 -4.2440042 -4.2559323 -4.2590852 -4.2600493 -4.2517419 -4.2243738][-4.2394133 -4.2307487 -4.2146373 -4.1869216 -4.1513519 -4.1353 -4.1537142 -4.1962032 -4.2331524 -4.2442293 -4.25324 -4.2562671 -4.2573671 -4.2521558 -4.2314739][-4.2359653 -4.2275844 -4.2141843 -4.1934886 -4.1547966 -4.1296525 -4.1422882 -4.1848874 -4.2311773 -4.2503991 -4.2576828 -4.2567325 -4.2559266 -4.2550354 -4.2478728][-4.2271652 -4.2144337 -4.2007627 -4.1846313 -4.146349 -4.1181059 -4.1259694 -4.1704574 -4.2241645 -4.2539024 -4.2600422 -4.2547007 -4.250977 -4.2522416 -4.2561674][-4.2145076 -4.191803 -4.1723223 -4.1589751 -4.1262007 -4.0933137 -4.0973396 -4.1460366 -4.2068877 -4.2465658 -4.254962 -4.2481356 -4.2434435 -4.2451887 -4.2513795][-4.2062378 -4.1761427 -4.1488013 -4.1339531 -4.1103196 -4.0788555 -4.0774755 -4.1256771 -4.1888556 -4.2324262 -4.2432456 -4.2379794 -4.2316327 -4.2325044 -4.2374048][-4.2009182 -4.1648221 -4.132699 -4.1174183 -4.1034441 -4.0848503 -4.0852304 -4.1275487 -4.182425 -4.2198606 -4.2325644 -4.2311287 -4.2237039 -4.2230511 -4.2244983][-4.1972442 -4.1552095 -4.1222091 -4.1083636 -4.1033297 -4.1040492 -4.1156268 -4.152916 -4.1940303 -4.2201691 -4.2349062 -4.2386646 -4.2330289 -4.2306695 -4.2284889][-4.2025294 -4.15997 -4.1247787 -4.1090636 -4.1063819 -4.118782 -4.1441264 -4.1814375 -4.2127919 -4.2313986 -4.2460661 -4.25282 -4.2520509 -4.2505183 -4.2441192][-4.2162814 -4.1829 -4.1473756 -4.12502 -4.1194162 -4.1325536 -4.1625795 -4.1990242 -4.2255964 -4.2414694 -4.25229 -4.2592087 -4.2642789 -4.2645121 -4.2555628][-4.2330446 -4.2129555 -4.180871 -4.1547089 -4.145205 -4.157258 -4.1860094 -4.2168913 -4.2380872 -4.2484326 -4.251842 -4.2562218 -4.2648711 -4.2690988 -4.2610545][-4.2460213 -4.2352428 -4.2085137 -4.1826892 -4.1754045 -4.1897273 -4.2141962 -4.2362924 -4.2474313 -4.2481174 -4.2456646 -4.2487326 -4.2601423 -4.2686563 -4.2640252][-4.248106 -4.2445822 -4.2264333 -4.2066722 -4.2065587 -4.2230344 -4.2413392 -4.2539363 -4.2559538 -4.2487893 -4.2439113 -4.2465544 -4.25696 -4.2648616 -4.2616024]]...]
INFO - root - 2017-12-07 14:30:24.845017: step 8610, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.163 sec/batch; 44h:46m:12s remains)
INFO - root - 2017-12-07 14:30:45.791908: step 8620, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.123 sec/batch; 43h:56m:41s remains)
INFO - root - 2017-12-07 14:31:06.985033: step 8630, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 43h:48m:44s remains)
INFO - root - 2017-12-07 14:31:28.100357: step 8640, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 43h:57m:25s remains)
INFO - root - 2017-12-07 14:31:49.148090: step 8650, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.106 sec/batch; 43h:33m:59s remains)
INFO - root - 2017-12-07 14:32:10.092775: step 8660, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.089 sec/batch; 43h:12m:03s remains)
INFO - root - 2017-12-07 14:32:31.147180: step 8670, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 43h:32m:06s remains)
INFO - root - 2017-12-07 14:32:52.382972: step 8680, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.097 sec/batch; 43h:21m:52s remains)
INFO - root - 2017-12-07 14:33:13.396997: step 8690, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.100 sec/batch; 43h:24m:59s remains)
INFO - root - 2017-12-07 14:33:34.481578: step 8700, loss = 2.06, batch loss = 2.01 (15.2 examples/sec; 2.107 sec/batch; 43h:33m:57s remains)
2017-12-07 14:33:36.054034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2520561 -4.2287297 -4.2235112 -4.225935 -4.2337904 -4.240253 -4.2442465 -4.2387781 -4.2307281 -4.2274904 -4.229938 -4.2351284 -4.2498837 -4.2640767 -4.2732267][-4.1917877 -4.1703768 -4.1779194 -4.1992722 -4.2244544 -4.2395554 -4.2452216 -4.2433887 -4.2363272 -4.2310381 -4.2307272 -4.2375669 -4.2549968 -4.268662 -4.2768693][-4.1357737 -4.1198349 -4.1443009 -4.1842394 -4.2195063 -4.2379937 -4.2421689 -4.2392049 -4.2305942 -4.2246871 -4.2244334 -4.2324929 -4.250329 -4.2624764 -4.2647276][-4.0982914 -4.0851321 -4.1209979 -4.1753516 -4.2176828 -4.2374 -4.237442 -4.2280297 -4.2166424 -4.2106333 -4.2121854 -4.2229466 -4.2391653 -4.2469811 -4.2392049][-4.09788 -4.0877862 -4.1254425 -4.1799779 -4.2194328 -4.2358456 -4.2337794 -4.21967 -4.2055554 -4.1981158 -4.2002125 -4.2116227 -4.2232656 -4.2216868 -4.2050185][-4.1203995 -4.1119986 -4.1435065 -4.1888266 -4.2197824 -4.2307372 -4.2274246 -4.2136497 -4.197228 -4.1871014 -4.1868167 -4.1944962 -4.2012992 -4.1924257 -4.17115][-4.1425362 -4.1283288 -4.1457 -4.1775603 -4.2008457 -4.2103496 -4.2110424 -4.2042389 -4.191298 -4.1800041 -4.1731682 -4.1741233 -4.1759691 -4.1657729 -4.1480479][-4.1539345 -4.1297932 -4.1315389 -4.1510034 -4.1698494 -4.1803255 -4.185945 -4.186697 -4.1806273 -4.1707082 -4.16034 -4.155056 -4.1525445 -4.1466646 -4.1399946][-4.1570749 -4.12349 -4.11231 -4.1228027 -4.136776 -4.1485009 -4.1572909 -4.163743 -4.1659942 -4.1641803 -4.1592078 -4.1556478 -4.15442 -4.1549754 -4.1579618][-4.1671753 -4.1315169 -4.1140308 -4.11818 -4.1283569 -4.13798 -4.1451044 -4.1517887 -4.1605082 -4.1687827 -4.1737537 -4.1773987 -4.18256 -4.1908717 -4.1987286][-4.1928968 -4.1646714 -4.1495857 -4.1529465 -4.16095 -4.1655979 -4.1662064 -4.1688533 -4.1795697 -4.1919322 -4.2012496 -4.2093692 -4.2195816 -4.2318358 -4.2403493][-4.2209973 -4.2024965 -4.1946611 -4.2035847 -4.2144771 -4.2182484 -4.2168226 -4.2188926 -4.2282319 -4.2379894 -4.2450542 -4.250699 -4.2594786 -4.2689638 -4.27512][-4.2429523 -4.2301741 -4.2274041 -4.2404976 -4.2556887 -4.2632442 -4.2655048 -4.2697883 -4.27707 -4.2830462 -4.2858977 -4.2868981 -4.290401 -4.2949281 -4.2980323][-4.2544775 -4.2435741 -4.2426119 -4.2576103 -4.2768412 -4.2899241 -4.2968359 -4.3026972 -4.3079066 -4.311039 -4.3114395 -4.308919 -4.3071847 -4.30758 -4.3095427][-4.2560215 -4.24599 -4.2460537 -4.2632055 -4.2868762 -4.3048806 -4.3147783 -4.3210726 -4.3242011 -4.324657 -4.3228459 -4.3178515 -4.3135333 -4.312386 -4.3144445]]...]
INFO - root - 2017-12-07 14:33:57.064778: step 8710, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.100 sec/batch; 43h:24m:28s remains)
INFO - root - 2017-12-07 14:34:17.690390: step 8720, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.091 sec/batch; 43h:13m:13s remains)
INFO - root - 2017-12-07 14:34:38.862191: step 8730, loss = 2.05, batch loss = 2.00 (15.2 examples/sec; 2.107 sec/batch; 43h:32m:50s remains)
INFO - root - 2017-12-07 14:35:00.030045: step 8740, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.067 sec/batch; 42h:42m:44s remains)
INFO - root - 2017-12-07 14:35:21.152903: step 8750, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 43h:44m:46s remains)
INFO - root - 2017-12-07 14:35:42.504529: step 8760, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.116 sec/batch; 43h:42m:29s remains)
INFO - root - 2017-12-07 14:36:03.725203: step 8770, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.149 sec/batch; 44h:22m:34s remains)
INFO - root - 2017-12-07 14:36:24.537882: step 8780, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.127 sec/batch; 43h:54m:56s remains)
INFO - root - 2017-12-07 14:36:45.580746: step 8790, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.148 sec/batch; 44h:21m:27s remains)
INFO - root - 2017-12-07 14:37:06.786778: step 8800, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.184 sec/batch; 45h:04m:55s remains)
2017-12-07 14:37:08.443221: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.290731 -4.28668 -4.2889419 -4.2982459 -4.3086648 -4.3179731 -4.323946 -4.3257132 -4.3215628 -4.3151116 -4.3085861 -4.3020525 -4.2978663 -4.2968931 -4.2939758][-4.2742691 -4.2655764 -4.2660704 -4.2754555 -4.28662 -4.2970552 -4.3040562 -4.3078184 -4.3054881 -4.3007283 -4.29709 -4.2931533 -4.292532 -4.2949119 -4.2936726][-4.2674408 -4.25006 -4.2420273 -4.2439942 -4.2503872 -4.2591481 -4.2679076 -4.2756834 -4.2797093 -4.2827806 -4.2847228 -4.2832952 -4.284462 -4.2881813 -4.2885308][-4.262342 -4.2414527 -4.2283626 -4.2244744 -4.2254615 -4.2302794 -4.2382197 -4.2459636 -4.2554965 -4.2663927 -4.2718863 -4.2699804 -4.2701097 -4.2719073 -4.2707176][-4.2615461 -4.241518 -4.2250719 -4.2150817 -4.2096539 -4.2062087 -4.2046142 -4.2021308 -4.2121887 -4.232182 -4.2445421 -4.2450905 -4.243227 -4.2394309 -4.2328811][-4.262733 -4.2508636 -4.2338767 -4.2187643 -4.203917 -4.1860266 -4.1630316 -4.1400623 -4.1479435 -4.1769896 -4.2014194 -4.2108059 -4.2100291 -4.200666 -4.1877832][-4.2523618 -4.250073 -4.235991 -4.2150793 -4.18902 -4.1546812 -4.1065445 -4.0622449 -4.0718012 -4.111958 -4.1478605 -4.1647367 -4.1656909 -4.1562543 -4.1408577][-4.237597 -4.2388268 -4.2248034 -4.2003193 -4.1661315 -4.1155481 -4.0433745 -3.9892581 -4.0148215 -4.0711617 -4.1146889 -4.1366229 -4.1404853 -4.1357317 -4.1237774][-4.2123742 -4.2131233 -4.2003026 -4.1788564 -4.149241 -4.1005311 -4.0316267 -3.9945519 -4.0369868 -4.0946836 -4.1322436 -4.1519361 -4.1587234 -4.1609654 -4.1578269][-4.1933584 -4.1936183 -4.186491 -4.1738009 -4.1541634 -4.1168218 -4.0667758 -4.0514307 -4.0938282 -4.1376348 -4.1598887 -4.1699376 -4.17738 -4.1884427 -4.1965][-4.1948934 -4.1951909 -4.196363 -4.192245 -4.1790633 -4.14955 -4.1137505 -4.1096468 -4.14108 -4.1691737 -4.1776681 -4.1787038 -4.1838322 -4.1983938 -4.2134142][-4.1983347 -4.1987553 -4.2049847 -4.2073464 -4.2014217 -4.1809387 -4.1577468 -4.158082 -4.1782722 -4.19475 -4.1980147 -4.1973538 -4.2021742 -4.2169728 -4.233243][-4.2008533 -4.2037821 -4.210752 -4.215457 -4.2148552 -4.2045741 -4.1940308 -4.1974039 -4.2094779 -4.2191634 -4.2213364 -4.2215791 -4.2260389 -4.2382064 -4.2524471][-4.2205958 -4.2281823 -4.2348518 -4.2370734 -4.2350178 -4.2282033 -4.2228637 -4.2248468 -4.2288303 -4.232543 -4.2334218 -4.2354088 -4.2419896 -4.2524028 -4.263639][-4.2419157 -4.2519732 -4.2583184 -4.2579603 -4.2536 -4.2473207 -4.2429986 -4.2409658 -4.2388678 -4.2389536 -4.24038 -4.2442894 -4.2513385 -4.2607312 -4.2709231]]...]
INFO - root - 2017-12-07 14:37:29.489464: step 8810, loss = 2.06, batch loss = 2.01 (15.2 examples/sec; 2.112 sec/batch; 43h:35m:52s remains)
INFO - root - 2017-12-07 14:37:50.356975: step 8820, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.138 sec/batch; 44h:07m:54s remains)
INFO - root - 2017-12-07 14:38:11.488588: step 8830, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 43h:54m:59s remains)
INFO - root - 2017-12-07 14:38:32.709545: step 8840, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 43h:18m:18s remains)
INFO - root - 2017-12-07 14:38:53.565442: step 8850, loss = 2.06, batch loss = 2.01 (14.9 examples/sec; 2.142 sec/batch; 44h:12m:01s remains)
INFO - root - 2017-12-07 14:39:14.911273: step 8860, loss = 2.08, batch loss = 2.03 (14.9 examples/sec; 2.148 sec/batch; 44h:19m:02s remains)
INFO - root - 2017-12-07 14:39:36.232433: step 8870, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.143 sec/batch; 44h:12m:34s remains)
INFO - root - 2017-12-07 14:39:56.691632: step 8880, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.139 sec/batch; 44h:06m:53s remains)
INFO - root - 2017-12-07 14:40:18.067891: step 8890, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 43h:17m:09s remains)
INFO - root - 2017-12-07 14:40:39.036295: step 8900, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.106 sec/batch; 43h:25m:00s remains)
2017-12-07 14:40:40.551371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2446456 -4.2429123 -4.2484446 -4.2457008 -4.2197628 -4.1823559 -4.1501918 -4.1421342 -4.1446695 -4.1531692 -4.1714668 -4.1854868 -4.1750731 -4.1496305 -4.1276951][-4.2439823 -4.2371445 -4.241498 -4.2392349 -4.2163982 -4.1828947 -4.1502795 -4.1352625 -4.1294384 -4.1303768 -4.1437945 -4.1570339 -4.142467 -4.1130223 -4.0923796][-4.2385516 -4.22684 -4.2238784 -4.2164969 -4.2011824 -4.1832447 -4.1604009 -4.1382556 -4.1197481 -4.1043224 -4.1065006 -4.1200418 -4.1049366 -4.0796824 -4.0701284][-4.23146 -4.2151151 -4.2057357 -4.1905646 -4.1834707 -4.1885123 -4.187881 -4.1664553 -4.1344385 -4.0944619 -4.0790472 -4.0872231 -4.0747442 -4.0597234 -4.0663028][-4.2210164 -4.1911411 -4.1671987 -4.1418591 -4.1371708 -4.1570106 -4.1736126 -4.1645889 -4.1376963 -4.0898972 -4.0614696 -4.058991 -4.0463834 -4.0414605 -4.065414][-4.1940031 -4.1510921 -4.1057615 -4.0667744 -4.0613174 -4.0889392 -4.1181245 -4.1282458 -4.1107993 -4.0698032 -4.0417953 -4.0274792 -4.0125008 -4.0184174 -4.0583777][-4.1378727 -4.083488 -4.0193048 -3.9723239 -3.9758282 -4.0173359 -4.0564194 -4.0725894 -4.0613775 -4.0369596 -4.0227847 -4.0042787 -3.9818151 -3.9922233 -4.0500441][-4.0745988 -4.0183105 -3.9521995 -3.9102898 -3.9259932 -3.9791625 -4.0167556 -4.0256205 -4.0184574 -4.014461 -4.0183649 -4.0007067 -3.9684565 -3.9776652 -4.0381351][-4.04349 -4.0080109 -3.9585307 -3.9263451 -3.9381216 -3.9794545 -4.0021286 -3.9949772 -3.984719 -3.9907398 -4.0079451 -4.0023551 -3.9820111 -3.9945018 -4.0416107][-4.0497847 -4.0426526 -4.0129991 -3.9860005 -3.9798746 -3.9881079 -3.9911115 -3.9781086 -3.9685473 -3.97784 -3.9971993 -4.0002704 -3.99346 -4.0029049 -4.0302219][-4.08169 -4.0851192 -4.0672455 -4.0492496 -4.04085 -4.0368843 -4.0307446 -4.0196352 -4.0133104 -4.0225034 -4.0330453 -4.0303388 -4.0223188 -4.0192642 -4.019979][-4.1182342 -4.1159091 -4.1050234 -4.098165 -4.1015739 -4.1039996 -4.1009054 -4.0918031 -4.0844736 -4.0953045 -4.0983014 -4.0874205 -4.0741448 -4.0569987 -4.0365071][-4.1593618 -4.1523466 -4.1456337 -4.1443577 -4.1560373 -4.165904 -4.1705275 -4.1659269 -4.1607389 -4.1725678 -4.1777406 -4.1656103 -4.141808 -4.1102495 -4.07574][-4.2031312 -4.197248 -4.1910276 -4.18807 -4.1993108 -4.2116909 -4.2214212 -4.2217464 -4.2196131 -4.2317877 -4.23757 -4.2264304 -4.2011633 -4.1652179 -4.1266942][-4.2410126 -4.2378983 -4.2314272 -4.225359 -4.2295117 -4.2388606 -4.2490854 -4.2542591 -4.2561588 -4.2661381 -4.2678981 -4.2589021 -4.2403097 -4.20995 -4.1764975]]...]
INFO - root - 2017-12-07 14:41:01.250511: step 8910, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 1.940 sec/batch; 39h:59m:49s remains)
INFO - root - 2017-12-07 14:41:22.381238: step 8920, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.172 sec/batch; 44h:45m:54s remains)
INFO - root - 2017-12-07 14:41:43.799569: step 8930, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.092 sec/batch; 43h:07m:29s remains)
INFO - root - 2017-12-07 14:42:04.934444: step 8940, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.105 sec/batch; 43h:22m:45s remains)
INFO - root - 2017-12-07 14:42:26.155042: step 8950, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.087 sec/batch; 42h:59m:26s remains)
INFO - root - 2017-12-07 14:42:47.456437: step 8960, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.145 sec/batch; 44h:11m:36s remains)
INFO - root - 2017-12-07 14:43:08.499754: step 8970, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.069 sec/batch; 42h:37m:35s remains)
INFO - root - 2017-12-07 14:43:29.270771: step 8980, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 44h:07m:30s remains)
INFO - root - 2017-12-07 14:43:50.566479: step 8990, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.109 sec/batch; 43h:26m:06s remains)
INFO - root - 2017-12-07 14:44:11.789624: step 9000, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 43h:45m:18s remains)
2017-12-07 14:44:13.393721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3520608 -4.355598 -4.3583789 -4.36016 -4.3599439 -4.3593364 -4.3578959 -4.354784 -4.350996 -4.347734 -4.346271 -4.34667 -4.3493881 -4.3518815 -4.3533306][-4.35093 -4.356678 -4.3619761 -4.3652148 -4.364049 -4.3602118 -4.3536363 -4.3453317 -4.3380184 -4.3322611 -4.3291984 -4.3300347 -4.3346353 -4.3392067 -4.34283][-4.34486 -4.3522472 -4.3589883 -4.3607688 -4.3540554 -4.3416343 -4.3251061 -4.3081689 -4.2974358 -4.2925196 -4.2914486 -4.2957215 -4.3051276 -4.3139653 -4.3217177][-4.3345389 -4.3417392 -4.3460803 -4.3403358 -4.3208666 -4.2931557 -4.2613263 -4.2337804 -4.2236562 -4.2264977 -4.2331247 -4.2449617 -4.2621059 -4.2773581 -4.29121][-4.3238387 -4.3294487 -4.3275585 -4.3083329 -4.2684484 -4.2175632 -4.1654096 -4.1280894 -4.1258073 -4.1451597 -4.1668377 -4.1914024 -4.21771 -4.2387443 -4.2580237][-4.3181896 -4.32232 -4.3131609 -4.276834 -4.2109017 -4.1311622 -4.0572267 -4.014317 -4.0282283 -4.071269 -4.1143122 -4.1553984 -4.1899271 -4.2131867 -4.2326927][-4.3226757 -4.3259478 -4.3115554 -4.2615223 -4.1738396 -4.0696592 -3.9749556 -3.9280977 -3.9616516 -4.02929 -4.09422 -4.1509047 -4.1897984 -4.2085819 -4.2200341][-4.32947 -4.3332887 -4.3185439 -4.2646523 -4.1706166 -4.0593696 -3.9570079 -3.9118311 -3.9571002 -4.0366735 -4.1133385 -4.1769204 -4.2126856 -4.2208295 -4.2184153][-4.3376441 -4.341917 -4.3297892 -4.2807164 -4.19736 -4.1026039 -4.0142508 -3.977608 -4.0185804 -4.0881214 -4.1589055 -4.21682 -4.2435341 -4.2383642 -4.2217617][-4.3442369 -4.3471308 -4.3380117 -4.3012991 -4.24059 -4.1736312 -4.1065364 -4.0746813 -4.0987821 -4.1448021 -4.1984286 -4.2451553 -4.2628665 -4.2484245 -4.2234855][-4.3426 -4.3440156 -4.3376136 -4.31407 -4.2774386 -4.2364273 -4.18881 -4.1597209 -4.1666675 -4.1906352 -4.225215 -4.2603416 -4.27055 -4.2525444 -4.2266088][-4.3192325 -4.3232684 -4.3231525 -4.313293 -4.296032 -4.2734766 -4.2396975 -4.2135439 -4.2097473 -4.2186666 -4.238677 -4.2639422 -4.2695866 -4.2543817 -4.2328038][-4.2698722 -4.27946 -4.2889791 -4.2938871 -4.2928686 -4.2828064 -4.2581525 -4.2350612 -4.2244959 -4.2248845 -4.2366481 -4.2559261 -4.2606273 -4.2515459 -4.2376246][-4.2023783 -4.2202177 -4.2414083 -4.25916 -4.2673721 -4.2624364 -4.2417583 -4.2199154 -4.2062893 -4.2055478 -4.2162886 -4.2330918 -4.238842 -4.2364087 -4.2314615][-4.1319942 -4.157011 -4.1900291 -4.216558 -4.226264 -4.2206631 -4.2012215 -4.1807537 -4.1680274 -4.1710072 -4.184691 -4.1998076 -4.2043338 -4.20523 -4.2071977]]...]
INFO - root - 2017-12-07 14:44:34.112176: step 9010, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.114 sec/batch; 43h:31m:46s remains)
INFO - root - 2017-12-07 14:44:55.204887: step 9020, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.079 sec/batch; 42h:47m:43s remains)
INFO - root - 2017-12-07 14:45:16.689938: step 9030, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.157 sec/batch; 44h:23m:29s remains)
INFO - root - 2017-12-07 14:45:37.676695: step 9040, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 43h:35m:51s remains)
INFO - root - 2017-12-07 14:45:59.000406: step 9050, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.111 sec/batch; 43h:25m:49s remains)
INFO - root - 2017-12-07 14:46:20.413981: step 9060, loss = 2.09, batch loss = 2.03 (14.6 examples/sec; 2.187 sec/batch; 45h:00m:11s remains)
INFO - root - 2017-12-07 14:46:41.316883: step 9070, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.115 sec/batch; 43h:30m:56s remains)
INFO - root - 2017-12-07 14:47:02.427801: step 9080, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.118 sec/batch; 43h:34m:06s remains)
INFO - root - 2017-12-07 14:47:23.576540: step 9090, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.076 sec/batch; 42h:41m:38s remains)
INFO - root - 2017-12-07 14:47:44.603683: step 9100, loss = 2.08, batch loss = 2.02 (15.7 examples/sec; 2.042 sec/batch; 41h:59m:29s remains)
2017-12-07 14:47:46.181143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2315331 -4.24153 -4.256022 -4.2688775 -4.2801471 -4.2831469 -4.2808323 -4.270381 -4.251368 -4.2335706 -4.2405405 -4.2668447 -4.2904096 -4.3131 -4.3335757][-4.2013626 -4.21093 -4.227159 -4.2418804 -4.253377 -4.2533431 -4.2489209 -4.2351007 -4.2108111 -4.189055 -4.2010155 -4.2365713 -4.2650776 -4.2908416 -4.3170648][-4.1931047 -4.2014585 -4.2150221 -4.2266059 -4.2352481 -4.2319674 -4.2239089 -4.2058873 -4.1760092 -4.1522436 -4.1686506 -4.212441 -4.2453127 -4.273438 -4.3045115][-4.1918063 -4.2006311 -4.2126527 -4.2234578 -4.23011 -4.2229776 -4.2114749 -4.187911 -4.1511 -4.1259174 -4.1474967 -4.19763 -4.2332253 -4.2620711 -4.29705][-4.1892695 -4.1996269 -4.2127852 -4.2250328 -4.2316294 -4.2207918 -4.2029843 -4.170259 -4.1265721 -4.0985436 -4.1236029 -4.1819973 -4.2248688 -4.257813 -4.2956772][-4.1991611 -4.2085071 -4.2157874 -4.2250657 -4.2273989 -4.2086515 -4.1843362 -4.1455374 -4.0942883 -4.0589919 -4.084579 -4.1555777 -4.2123075 -4.2539172 -4.2960749][-4.2211 -4.2216458 -4.2190642 -4.219913 -4.2127209 -4.1840868 -4.157455 -4.1178312 -4.0618353 -4.01503 -4.0375824 -4.1217561 -4.1945877 -4.2444534 -4.2904663][-4.2428756 -4.2323494 -4.2185197 -4.2132397 -4.2020149 -4.1706109 -4.1451735 -4.1046429 -4.0461559 -3.9909489 -4.0060716 -4.09449 -4.1749883 -4.2305846 -4.2803326][-4.2607951 -4.2471547 -4.2349181 -4.23571 -4.2272959 -4.2020569 -4.1792893 -4.1383047 -4.0773563 -4.0157452 -4.0165749 -4.0921478 -4.1664634 -4.2211185 -4.2719183][-4.272727 -4.2597785 -4.2582417 -4.27039 -4.2660122 -4.2442837 -4.2280583 -4.1949806 -4.1368904 -4.0739307 -4.0603404 -4.1153564 -4.1774969 -4.2258973 -4.2751126][-4.2748661 -4.2616572 -4.264607 -4.2834749 -4.2801609 -4.260057 -4.2517238 -4.2323017 -4.1851873 -4.1283 -4.1083212 -4.1462975 -4.1962829 -4.2397504 -4.2863331][-4.2681971 -4.255651 -4.254776 -4.2691298 -4.2628379 -4.2447581 -4.2403612 -4.2349281 -4.207057 -4.1653748 -4.1484838 -4.1789565 -4.2214322 -4.2609577 -4.303][-4.2631955 -4.2524366 -4.2425709 -4.24545 -4.236311 -4.2216578 -4.2224016 -4.229805 -4.2209268 -4.1988263 -4.1914816 -4.2187123 -4.2547054 -4.2886963 -4.3243227][-4.266048 -4.2561765 -4.2443147 -4.2430391 -4.2360692 -4.2270546 -4.2300072 -4.2444668 -4.246335 -4.2371063 -4.2379618 -4.2608724 -4.2901087 -4.3172026 -4.3446193][-4.2766433 -4.267138 -4.2577553 -4.2576303 -4.2560558 -4.2517676 -4.2559156 -4.2697878 -4.277092 -4.276669 -4.2837868 -4.3021049 -4.3227644 -4.3417621 -4.3595591]]...]
INFO - root - 2017-12-07 14:48:07.228285: step 9110, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 43h:41m:16s remains)
INFO - root - 2017-12-07 14:48:28.390373: step 9120, loss = 2.09, batch loss = 2.03 (15.5 examples/sec; 2.061 sec/batch; 42h:21m:57s remains)
INFO - root - 2017-12-07 14:48:49.490088: step 9130, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.084 sec/batch; 42h:49m:55s remains)
INFO - root - 2017-12-07 14:49:10.214080: step 9140, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.098 sec/batch; 43h:07m:24s remains)
INFO - root - 2017-12-07 14:49:31.366099: step 9150, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.098 sec/batch; 43h:07m:08s remains)
INFO - root - 2017-12-07 14:49:52.644435: step 9160, loss = 2.08, batch loss = 2.02 (14.5 examples/sec; 2.208 sec/batch; 45h:21m:50s remains)
INFO - root - 2017-12-07 14:50:13.444440: step 9170, loss = 2.08, batch loss = 2.02 (15.6 examples/sec; 2.057 sec/batch; 42h:15m:57s remains)
INFO - root - 2017-12-07 14:50:34.644577: step 9180, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.131 sec/batch; 43h:45m:49s remains)
INFO - root - 2017-12-07 14:50:55.878591: step 9190, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.112 sec/batch; 43h:22m:54s remains)
INFO - root - 2017-12-07 14:51:16.802633: step 9200, loss = 2.06, batch loss = 2.01 (16.3 examples/sec; 1.967 sec/batch; 40h:24m:07s remains)
2017-12-07 14:51:18.275823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3049407 -4.2870083 -4.2762938 -4.2760806 -4.2770348 -4.2746234 -4.2713084 -4.280582 -4.3025193 -4.3226919 -4.3280416 -4.3219347 -4.3086367 -4.2874331 -4.2579904][-4.3074913 -4.2911038 -4.27963 -4.2740941 -4.2659736 -4.256032 -4.2481289 -4.25261 -4.2772727 -4.306529 -4.3187094 -4.3177629 -4.3038344 -4.2764482 -4.2446795][-4.3150949 -4.303081 -4.2911582 -4.27933 -4.25914 -4.2355919 -4.2153168 -4.2055678 -4.2271848 -4.2658119 -4.2911081 -4.3012757 -4.2917466 -4.2610373 -4.2207985][-4.3248215 -4.3166618 -4.303576 -4.2849407 -4.2526331 -4.2102695 -4.1653428 -4.1274247 -4.1395555 -4.1913328 -4.2369552 -4.2660103 -4.269547 -4.2447786 -4.2027082][-4.3327808 -4.3274088 -4.3133955 -4.2885008 -4.2434425 -4.1821985 -4.1100116 -4.0359859 -4.0319924 -4.1021986 -4.1716218 -4.2226048 -4.2443733 -4.233005 -4.2015634][-4.3348989 -4.3303037 -4.3139653 -4.2807631 -4.2236042 -4.1478491 -4.0524616 -3.9388838 -3.9146469 -4.0106087 -4.1109748 -4.1864495 -4.2258425 -4.2310181 -4.21841][-4.3315315 -4.3255644 -4.306426 -4.266048 -4.2024403 -4.1239886 -4.0203114 -3.8788562 -3.8298383 -3.9456162 -4.0734544 -4.16883 -4.2195253 -4.2361512 -4.2374263][-4.3245821 -4.3171859 -4.2971764 -4.2562804 -4.1962676 -4.1311893 -4.0473757 -3.9160461 -3.8578122 -3.96411 -4.0884275 -4.18098 -4.2297721 -4.2490268 -4.2521038][-4.3177433 -4.3098469 -4.2916265 -4.2564306 -4.208529 -4.1670294 -4.1196742 -4.0284772 -3.9786391 -4.0527997 -4.147161 -4.218596 -4.25558 -4.26875 -4.2659745][-4.3145604 -4.3081889 -4.2949438 -4.2700233 -4.2363458 -4.2155147 -4.1994405 -4.1496868 -4.1162281 -4.1591258 -4.216507 -4.260282 -4.2824068 -4.2855406 -4.2733278][-4.31492 -4.3110352 -4.3043008 -4.2909455 -4.2699428 -4.2606153 -4.2624488 -4.244966 -4.2303181 -4.2518992 -4.2784071 -4.29569 -4.3020911 -4.2947226 -4.2752423][-4.3158159 -4.3138084 -4.3123345 -4.3065329 -4.291378 -4.2841711 -4.2903781 -4.2913885 -4.2929559 -4.3053603 -4.3137264 -4.3123565 -4.3065095 -4.2910409 -4.2699432][-4.31551 -4.3128457 -4.3126626 -4.3088384 -4.2939734 -4.2843657 -4.2871742 -4.2941093 -4.3046479 -4.3131409 -4.3121195 -4.3008456 -4.2856026 -4.2631664 -4.2435284][-4.314744 -4.3092403 -4.3068695 -4.2998047 -4.28258 -4.2677493 -4.2627869 -4.2691331 -4.2853079 -4.29417 -4.2896886 -4.2740359 -4.2522521 -4.2261858 -4.2098203][-4.3140154 -4.3049092 -4.297296 -4.2852497 -4.2650094 -4.2451077 -4.2323394 -4.2350678 -4.2538981 -4.2663021 -4.2622862 -4.2444992 -4.21952 -4.1959562 -4.1861625]]...]
INFO - root - 2017-12-07 14:51:39.444432: step 9210, loss = 2.09, batch loss = 2.03 (14.8 examples/sec; 2.165 sec/batch; 44h:26m:29s remains)
INFO - root - 2017-12-07 14:52:00.568917: step 9220, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.131 sec/batch; 43h:44m:56s remains)
INFO - root - 2017-12-07 14:52:21.562739: step 9230, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.101 sec/batch; 43h:08m:01s remains)
INFO - root - 2017-12-07 14:52:42.435213: step 9240, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 2.081 sec/batch; 42h:42m:37s remains)
INFO - root - 2017-12-07 14:53:03.637327: step 9250, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 43h:28m:12s remains)
INFO - root - 2017-12-07 14:53:24.597665: step 9260, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.112 sec/batch; 43h:20m:36s remains)
INFO - root - 2017-12-07 14:53:45.278049: step 9270, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.135 sec/batch; 43h:48m:32s remains)
INFO - root - 2017-12-07 14:54:06.422969: step 9280, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.130 sec/batch; 43h:41m:57s remains)
INFO - root - 2017-12-07 14:54:27.637777: step 9290, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.085 sec/batch; 42h:45m:12s remains)
INFO - root - 2017-12-07 14:54:48.638487: step 9300, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.134 sec/batch; 43h:46m:04s remains)
2017-12-07 14:54:50.152372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2334404 -4.2276344 -4.2295585 -4.22226 -4.2090092 -4.2180595 -4.2372079 -4.224216 -4.1908665 -4.1587076 -4.1380119 -4.135211 -4.1542544 -4.1673174 -4.1851845][-4.2307234 -4.2224975 -4.2237182 -4.2139287 -4.191196 -4.1909184 -4.2039828 -4.1930804 -4.16923 -4.1422668 -4.1212773 -4.1168094 -4.1411586 -4.1623988 -4.1839848][-4.218544 -4.2090483 -4.21289 -4.2028856 -4.1709757 -4.1558261 -4.1561441 -4.1539106 -4.1510277 -4.1392512 -4.118329 -4.1025877 -4.1223106 -4.1494646 -4.17451][-4.1946244 -4.186461 -4.1976089 -4.1926432 -4.1600332 -4.1262145 -4.1059108 -4.1090164 -4.1379776 -4.1567717 -4.1405783 -4.1163187 -4.1268992 -4.1505885 -4.1719704][-4.1590543 -4.1615753 -4.1859035 -4.1869497 -4.1542239 -4.0993834 -4.05039 -4.054122 -4.1138425 -4.1700711 -4.1747055 -4.1496677 -4.1476474 -4.1549211 -4.1666851][-4.1262097 -4.1344252 -4.1648908 -4.1714158 -4.1375275 -4.0584631 -3.9693635 -3.9582419 -4.0478029 -4.1480327 -4.1870995 -4.17454 -4.1664214 -4.1592321 -4.1582952][-4.1169429 -4.1217971 -4.1487737 -4.1567521 -4.1189842 -4.0177579 -3.8819716 -3.8335786 -3.9491487 -4.0964689 -4.1737728 -4.184864 -4.1860366 -4.1764488 -4.163527][-4.1367068 -4.1366348 -4.1529074 -4.1574926 -4.1238732 -4.0250778 -3.8823075 -3.8099098 -3.9138789 -4.0667167 -4.1618972 -4.19506 -4.2057881 -4.1976275 -4.1773367][-4.1570392 -4.1552391 -4.1663117 -4.1678867 -4.14753 -4.0789018 -3.9754727 -3.9156942 -3.9709473 -4.077219 -4.1623387 -4.2053976 -4.2154465 -4.2056561 -4.1883726][-4.1732197 -4.1704121 -4.17646 -4.1782703 -4.1732287 -4.1385174 -4.074791 -4.0315003 -4.0538478 -4.1150694 -4.1775956 -4.2196097 -4.2236433 -4.2087073 -4.1957531][-4.1962152 -4.1909876 -4.1875157 -4.1856647 -4.1938553 -4.188869 -4.1569147 -4.1210427 -4.120791 -4.1549439 -4.198483 -4.2287755 -4.224607 -4.2057848 -4.1961923][-4.2283373 -4.2198443 -4.2045636 -4.19199 -4.1999493 -4.2091217 -4.1950774 -4.16751 -4.1591272 -4.176558 -4.2038789 -4.2222953 -4.212894 -4.1969404 -4.189538][-4.2601857 -4.2483373 -4.2244844 -4.2019906 -4.2024364 -4.2125306 -4.2041078 -4.185297 -4.178936 -4.1887012 -4.2062759 -4.2178531 -4.2084208 -4.1989231 -4.1950183][-4.2858276 -4.26929 -4.242137 -4.2180147 -4.2136092 -4.2183852 -4.2102857 -4.1978297 -4.1970582 -4.2078729 -4.2221828 -4.2279148 -4.2226181 -4.2227163 -4.2212625][-4.285697 -4.26892 -4.2482209 -4.2350698 -4.2352848 -4.2357574 -4.2264233 -4.2160959 -4.2168403 -4.222146 -4.230248 -4.2329006 -4.2345767 -4.24214 -4.24273]]...]
INFO - root - 2017-12-07 14:55:11.620442: step 9310, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.153 sec/batch; 44h:09m:19s remains)
INFO - root - 2017-12-07 14:55:32.996661: step 9320, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.112 sec/batch; 43h:17m:24s remains)
INFO - root - 2017-12-07 14:55:53.843577: step 9330, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.078 sec/batch; 42h:35m:34s remains)
INFO - root - 2017-12-07 14:56:15.099756: step 9340, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.130 sec/batch; 43h:39m:47s remains)
INFO - root - 2017-12-07 14:56:36.193452: step 9350, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.078 sec/batch; 42h:34m:56s remains)
INFO - root - 2017-12-07 14:56:57.372143: step 9360, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 43h:42m:12s remains)
INFO - root - 2017-12-07 14:57:18.426805: step 9370, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.086 sec/batch; 42h:44m:38s remains)
INFO - root - 2017-12-07 14:57:39.811797: step 9380, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.178 sec/batch; 44h:36m:35s remains)
INFO - root - 2017-12-07 14:58:00.776141: step 9390, loss = 2.08, batch loss = 2.02 (16.2 examples/sec; 1.976 sec/batch; 40h:28m:29s remains)
INFO - root - 2017-12-07 14:58:21.905155: step 9400, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.162 sec/batch; 44h:16m:06s remains)
2017-12-07 14:58:23.417140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2543583 -4.2610106 -4.2439213 -4.2122092 -4.1896663 -4.1807508 -4.1724834 -4.1589422 -4.1404362 -4.1235614 -4.1075473 -4.0913897 -4.0825143 -4.0877719 -4.0969048][-4.2756681 -4.2748213 -4.253787 -4.2192068 -4.1895695 -4.1715283 -4.1547956 -4.1460114 -4.1466103 -4.1489186 -4.1458273 -4.133945 -4.1200733 -4.1146955 -4.1196995][-4.2743769 -4.2656384 -4.2396641 -4.2012095 -4.1659584 -4.1385517 -4.1158781 -4.1082134 -4.1287766 -4.1534472 -4.1639309 -4.1593666 -4.1464005 -4.1364594 -4.1360641][-4.2520385 -4.2409353 -4.215836 -4.175992 -4.1351871 -4.1039567 -4.0792527 -4.0708542 -4.1014409 -4.1433139 -4.1613731 -4.1615167 -4.1524267 -4.1443934 -4.1426163][-4.2289958 -4.2249227 -4.204618 -4.1634803 -4.1131268 -4.073009 -4.0480938 -4.0407887 -4.0709543 -4.1149483 -4.132009 -4.1321292 -4.1277533 -4.13234 -4.1381116][-4.2234468 -4.22572 -4.2100191 -4.165988 -4.1036687 -4.0504079 -4.0267582 -4.0271354 -4.0574284 -4.0957403 -4.1100378 -4.1110191 -4.1095729 -4.122479 -4.1339679][-4.2347322 -4.2324328 -4.2067385 -4.1557603 -4.0908623 -4.0347075 -4.0150065 -4.0288839 -4.0684209 -4.1087537 -4.1239915 -4.1222615 -4.1190753 -4.128335 -4.1345992][-4.2533665 -4.2382231 -4.1961732 -4.1438942 -4.088448 -4.0368123 -4.0205555 -4.0426593 -4.0937481 -4.138463 -4.1512194 -4.1430488 -4.1358504 -4.1381311 -4.133996][-4.2615218 -4.2430186 -4.1955767 -4.1456747 -4.09998 -4.05906 -4.0461655 -4.0674095 -4.119483 -4.161581 -4.168602 -4.1561227 -4.145298 -4.13955 -4.1282954][-4.2576675 -4.2423434 -4.1991162 -4.1573358 -4.1212769 -4.0898438 -4.0781827 -4.0916982 -4.1317191 -4.1649594 -4.1694427 -4.155982 -4.1430559 -4.1328964 -4.1224833][-4.2507987 -4.2423124 -4.2061634 -4.1703916 -4.1437054 -4.1201243 -4.1076064 -4.1134291 -4.1396408 -4.1614742 -4.1622524 -4.14947 -4.1308846 -4.117487 -4.1154213][-4.2430491 -4.240911 -4.213625 -4.1851311 -4.1686292 -4.15059 -4.1357913 -4.1336575 -4.1448689 -4.1565828 -4.1569066 -4.1444073 -4.1212678 -4.1075783 -4.1118751][-4.2431035 -4.2515764 -4.236165 -4.2135758 -4.1986985 -4.1784673 -4.161654 -4.1588955 -4.1649122 -4.1691318 -4.1646128 -4.1479783 -4.1236038 -4.1140847 -4.1253247][-4.2481756 -4.2664375 -4.2624111 -4.2449522 -4.2252455 -4.2052488 -4.1953149 -4.1980419 -4.2059937 -4.2066875 -4.1955466 -4.1711178 -4.1467552 -4.1401892 -4.155983][-4.257443 -4.281878 -4.2872758 -4.2768197 -4.2564263 -4.2382212 -4.234302 -4.2434244 -4.2549033 -4.2533317 -4.2366805 -4.2094855 -4.1877418 -4.1829638 -4.195488]]...]
INFO - root - 2017-12-07 14:58:44.592550: step 9410, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.111 sec/batch; 43h:13m:40s remains)
INFO - root - 2017-12-07 14:59:05.701897: step 9420, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 2.082 sec/batch; 42h:38m:10s remains)
INFO - root - 2017-12-07 14:59:26.624084: step 9430, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 43h:21m:06s remains)
INFO - root - 2017-12-07 14:59:47.980846: step 9440, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.110 sec/batch; 43h:10m:54s remains)
INFO - root - 2017-12-07 15:00:09.240157: step 9450, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 43h:19m:22s remains)
INFO - root - 2017-12-07 15:00:30.177018: step 9460, loss = 2.06, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 43h:15m:52s remains)
INFO - root - 2017-12-07 15:00:51.382622: step 9470, loss = 2.06, batch loss = 2.01 (14.9 examples/sec; 2.149 sec/batch; 43h:58m:04s remains)
INFO - root - 2017-12-07 15:01:12.488259: step 9480, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.139 sec/batch; 43h:46m:00s remains)
INFO - root - 2017-12-07 15:01:33.452383: step 9490, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 43h:20m:46s remains)
INFO - root - 2017-12-07 15:01:54.473712: step 9500, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.131 sec/batch; 43h:35m:02s remains)
2017-12-07 15:01:56.060490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1751962 -4.1708817 -4.1534729 -4.1325693 -4.1201696 -4.1149015 -4.10533 -4.0945253 -4.1153083 -4.16323 -4.2082529 -4.2265463 -4.2106867 -4.1884274 -4.1741447][-4.1629596 -4.1662865 -4.1572871 -4.1457086 -4.1336336 -4.1200995 -4.1007485 -4.0758438 -4.0867329 -4.1309495 -4.174099 -4.1916361 -4.1786704 -4.1590061 -4.1519432][-4.1934505 -4.2073593 -4.2053733 -4.2006359 -4.1919827 -4.1817036 -4.1644421 -4.1360841 -4.1331167 -4.1568904 -4.1785841 -4.1831994 -4.1685805 -4.1465983 -4.1388683][-4.2201715 -4.2460074 -4.2507124 -4.2457576 -4.2364268 -4.2263703 -4.2106938 -4.182168 -4.1723633 -4.1805148 -4.185009 -4.176765 -4.160399 -4.144237 -4.1398993][-4.2221241 -4.2483888 -4.2477274 -4.2362604 -4.2238369 -4.2055974 -4.1785169 -4.14362 -4.1326008 -4.1430316 -4.1529617 -4.1539755 -4.1418314 -4.1286888 -4.133214][-4.1987586 -4.2107992 -4.1976681 -4.1763821 -4.1476135 -4.104702 -4.0500255 -4.0010042 -4.0037575 -4.0422359 -4.0716271 -4.0866504 -4.0778942 -4.0688791 -4.0832644][-4.149868 -4.1371121 -4.1099267 -4.0821657 -4.0358386 -3.9652271 -3.8785 -3.8144996 -3.8376482 -3.9123929 -3.9737792 -4.012682 -4.012074 -3.997576 -4.01244][-4.1108985 -4.0903883 -4.0677094 -4.044908 -3.9946711 -3.9220984 -3.833257 -3.7721169 -3.8053827 -3.8811686 -3.9416533 -3.9793482 -3.9714804 -3.9451025 -3.9513292][-4.1084805 -4.0966024 -4.0869007 -4.067595 -4.0271692 -3.9786706 -3.9247479 -3.8909097 -3.9162972 -3.9617186 -3.9970622 -4.0193172 -4.0022874 -3.9696662 -3.9698949][-4.1644011 -4.1669321 -4.1720405 -4.1617761 -4.1347747 -4.1056061 -4.076951 -4.0587921 -4.0721984 -4.0959511 -4.1147461 -4.1270175 -4.1120148 -4.0915265 -4.0920334][-4.240396 -4.2480531 -4.2602482 -4.2611327 -4.2502804 -4.2358637 -4.2195339 -4.207799 -4.2102404 -4.2208319 -4.2311878 -4.2398157 -4.2323885 -4.2241917 -4.224112][-4.2882972 -4.2915111 -4.3027611 -4.3113818 -4.3121924 -4.3073416 -4.2967496 -4.2853775 -4.2817879 -4.2853451 -4.2924109 -4.2992263 -4.2971334 -4.2962127 -4.295681][-4.31573 -4.3154926 -4.3220611 -4.3306866 -4.3356171 -4.3339806 -4.3249354 -4.3132639 -4.3079071 -4.3085265 -4.3128815 -4.3172588 -4.3183222 -4.3220835 -4.324451][-4.3273082 -4.3253121 -4.3276529 -4.3321738 -4.3347926 -4.3337483 -4.3281422 -4.3214684 -4.3181171 -4.3174748 -4.318574 -4.3202062 -4.3226194 -4.3278747 -4.3329077][-4.33905 -4.3382874 -4.3397145 -4.3417459 -4.342742 -4.3419895 -4.3398228 -4.3373108 -4.3356342 -4.3348031 -4.3344073 -4.3343992 -4.3356352 -4.33801 -4.3404188]]...]
INFO - root - 2017-12-07 15:02:17.272053: step 9510, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.102 sec/batch; 42h:58m:55s remains)
INFO - root - 2017-12-07 15:02:38.139359: step 9520, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.075 sec/batch; 42h:25m:28s remains)
INFO - root - 2017-12-07 15:02:59.282227: step 9530, loss = 2.06, batch loss = 2.01 (15.3 examples/sec; 2.087 sec/batch; 42h:40m:02s remains)
INFO - root - 2017-12-07 15:03:20.499528: step 9540, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 43h:26m:00s remains)
INFO - root - 2017-12-07 15:03:41.845440: step 9550, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.154 sec/batch; 44h:01m:07s remains)
INFO - root - 2017-12-07 15:04:02.695856: step 9560, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.126 sec/batch; 43h:26m:23s remains)
INFO - root - 2017-12-07 15:04:24.010734: step 9570, loss = 2.08, batch loss = 2.02 (14.5 examples/sec; 2.205 sec/batch; 45h:02m:40s remains)
INFO - root - 2017-12-07 15:04:45.114325: step 9580, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.076 sec/batch; 42h:24m:25s remains)
INFO - root - 2017-12-07 15:05:05.976437: step 9590, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.120 sec/batch; 43h:18m:16s remains)
INFO - root - 2017-12-07 15:05:27.027499: step 9600, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.064 sec/batch; 42h:09m:18s remains)
2017-12-07 15:05:28.588413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2242217 -4.2183318 -4.2182374 -4.2162509 -4.2155128 -4.219286 -4.224124 -4.2240767 -4.227809 -4.2399673 -4.2573938 -4.2743592 -4.2858109 -4.2973595 -4.3071423][-4.1761794 -4.171361 -4.1718264 -4.1694789 -4.1673636 -4.1724505 -4.1790719 -4.1820784 -4.1927862 -4.2134643 -4.2370133 -4.2589593 -4.276062 -4.2920122 -4.3037591][-4.1578388 -4.1565623 -4.1580739 -4.1561103 -4.1509476 -4.1523433 -4.1561246 -4.1604152 -4.1761193 -4.2023678 -4.2309017 -4.2578459 -4.2798724 -4.2978463 -4.308619][-4.1697373 -4.1694703 -4.1698737 -4.1670027 -4.15783 -4.1500363 -4.1438007 -4.1458364 -4.1631937 -4.1932969 -4.2276115 -4.2621207 -4.2883739 -4.3068738 -4.3144231][-4.1939468 -4.1904349 -4.1852055 -4.1759257 -4.1578064 -4.1360159 -4.1164274 -4.1132607 -4.129703 -4.1664648 -4.2104664 -4.2537522 -4.2864289 -4.3081241 -4.315434][-4.2100668 -4.1975269 -4.18053 -4.1598706 -4.1285353 -4.0888629 -4.0538316 -4.0448713 -4.0657563 -4.1177473 -4.1765323 -4.2298651 -4.2703829 -4.299345 -4.3113208][-4.2236309 -4.195106 -4.158565 -4.12107 -4.072464 -4.0112872 -3.9592943 -3.9464931 -3.9802632 -4.0555739 -4.1315823 -4.1965771 -4.2478709 -4.2858071 -4.3036184][-4.2341046 -4.1909227 -4.1373873 -4.0837855 -4.0226727 -3.9551022 -3.9052842 -3.8981683 -3.9394732 -4.023447 -4.1029472 -4.17336 -4.2333121 -4.2772923 -4.2969203][-4.2506275 -4.2002206 -4.1405773 -4.0851603 -4.0354171 -3.9985101 -3.9813085 -3.9832783 -4.0098753 -4.0691977 -4.1260719 -4.1857519 -4.2419271 -4.2825079 -4.2996769][-4.2705812 -4.2212229 -4.1680593 -4.1247683 -4.0984583 -4.09445 -4.0999937 -4.1027079 -4.10861 -4.139636 -4.1762824 -4.2214346 -4.266942 -4.2986703 -4.3115368][-4.2778363 -4.2371516 -4.2012477 -4.1783109 -4.174274 -4.1861987 -4.1941671 -4.18867 -4.1772828 -4.1892004 -4.2134552 -4.2478414 -4.285007 -4.3114815 -4.3230324][-4.2728977 -4.2447972 -4.2295952 -4.228158 -4.238637 -4.2526932 -4.2535772 -4.2366352 -4.21393 -4.2128863 -4.227953 -4.254652 -4.2888722 -4.314486 -4.3276367][-4.2622786 -4.24468 -4.2416697 -4.25102 -4.2687087 -4.283999 -4.2833929 -4.2622972 -4.2359304 -4.2276068 -4.2367072 -4.2601728 -4.2921972 -4.3170304 -4.3294325][-4.2454672 -4.2320027 -4.232996 -4.2476826 -4.2695427 -4.2876282 -4.288568 -4.2652969 -4.2365446 -4.2229424 -4.2309403 -4.2562656 -4.288743 -4.3146677 -4.3292994][-4.2197018 -4.209815 -4.2156982 -4.2345433 -4.2572479 -4.2739749 -4.27332 -4.2469974 -4.2159643 -4.1999993 -4.2095551 -4.2396979 -4.2763863 -4.3078861 -4.3276396]]...]
INFO - root - 2017-12-07 15:05:49.845268: step 9610, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.096 sec/batch; 42h:47m:33s remains)
INFO - root - 2017-12-07 15:06:10.692291: step 9620, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.107 sec/batch; 43h:00m:54s remains)
INFO - root - 2017-12-07 15:06:31.797958: step 9630, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 43h:44m:03s remains)
INFO - root - 2017-12-07 15:06:53.023398: step 9640, loss = 2.09, batch loss = 2.03 (14.7 examples/sec; 2.179 sec/batch; 44h:29m:02s remains)
INFO - root - 2017-12-07 15:07:14.043008: step 9650, loss = 2.09, batch loss = 2.03 (14.7 examples/sec; 2.172 sec/batch; 44h:20m:06s remains)
INFO - root - 2017-12-07 15:07:35.131824: step 9660, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.169 sec/batch; 44h:15m:39s remains)
INFO - root - 2017-12-07 15:07:56.508378: step 9670, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.160 sec/batch; 44h:04m:42s remains)
INFO - root - 2017-12-07 15:08:17.568911: step 9680, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 1.945 sec/batch; 39h:40m:54s remains)
INFO - root - 2017-12-07 15:08:38.471506: step 9690, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 43h:13m:44s remains)
INFO - root - 2017-12-07 15:08:59.708942: step 9700, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 43h:00m:41s remains)
2017-12-07 15:09:01.294199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2976255 -4.2908859 -4.2677517 -4.2573371 -4.2575607 -4.2454448 -4.2343316 -4.2465491 -4.2626867 -4.2600861 -4.252902 -4.2470865 -4.2364025 -4.2278066 -4.2185326][-4.2898755 -4.283298 -4.2597275 -4.2500377 -4.2456846 -4.2234912 -4.2012763 -4.2158084 -4.2444649 -4.2485929 -4.2411261 -4.2296262 -4.2166309 -4.2066994 -4.1961374][-4.2919803 -4.2826114 -4.2529392 -4.2380857 -4.2281337 -4.193645 -4.1558533 -4.16878 -4.2116838 -4.2289186 -4.229641 -4.2189703 -4.2081213 -4.1996441 -4.1886911][-4.2931066 -4.2800317 -4.2438235 -4.2241459 -4.2066875 -4.1605768 -4.10828 -4.1176896 -4.1692433 -4.199626 -4.2142711 -4.2125158 -4.2065334 -4.1994157 -4.1907554][-4.2906179 -4.2721791 -4.2289925 -4.2025757 -4.1775112 -4.1228714 -4.0596094 -4.0672479 -4.1249905 -4.1644912 -4.190661 -4.2036457 -4.2078347 -4.2003331 -4.1880808][-4.2904086 -4.268043 -4.2202 -4.1860423 -4.1494713 -4.0785027 -4.0029073 -4.0184803 -4.0882282 -4.1381769 -4.1724758 -4.1951284 -4.2062926 -4.1996713 -4.1847348][-4.2937403 -4.2722816 -4.2237353 -4.1839542 -4.134078 -4.0438747 -3.9511247 -3.9690311 -4.0525064 -4.1161108 -4.1572685 -4.1873879 -4.2038574 -4.2033644 -4.1892533][-4.2904692 -4.2717752 -4.2289834 -4.1953387 -4.1451144 -4.0476565 -3.9409347 -3.9483624 -4.0311127 -4.1009097 -4.1474519 -4.1818762 -4.2009964 -4.2055893 -4.1939273][-4.283102 -4.2651892 -4.2263069 -4.2006073 -4.1628408 -4.0782356 -3.9757218 -3.9656906 -4.0335011 -4.1004276 -4.1497221 -4.1817164 -4.1994681 -4.2090082 -4.2006888][-4.282176 -4.2616968 -4.2204351 -4.1964679 -4.1713576 -4.1067858 -4.0203185 -4.0013008 -4.0525427 -4.1130376 -4.1601796 -4.186285 -4.2032719 -4.2187104 -4.213][-4.2828031 -4.2629933 -4.2178731 -4.1924119 -4.1788111 -4.1310668 -4.0609875 -4.0435905 -4.0833077 -4.1365581 -4.1753478 -4.194942 -4.214303 -4.2310753 -4.226408][-4.2779245 -4.2568951 -4.2103224 -4.1861587 -4.1856227 -4.1531067 -4.09851 -4.0861716 -4.1194606 -4.1666942 -4.20012 -4.2141643 -4.2340808 -4.248539 -4.2417288][-4.2804108 -4.2556672 -4.2077613 -4.1837063 -4.1919489 -4.1745543 -4.1364489 -4.1310911 -4.1584339 -4.2020683 -4.2308683 -4.2413659 -4.2584887 -4.2652254 -4.2564683][-4.2884955 -4.2614131 -4.2141581 -4.1910729 -4.201921 -4.1971951 -4.1781526 -4.1791725 -4.2009196 -4.2354622 -4.2564549 -4.2647872 -4.2765903 -4.2767816 -4.2696362][-4.2936573 -4.2690806 -4.2281523 -4.210741 -4.2239308 -4.2285151 -4.2217903 -4.2277961 -4.2450237 -4.2673469 -4.2786317 -4.2819586 -4.2879715 -4.2853293 -4.2796426]]...]
INFO - root - 2017-12-07 15:09:22.648247: step 9710, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.130 sec/batch; 43h:26m:48s remains)
INFO - root - 2017-12-07 15:09:43.572495: step 9720, loss = 2.09, batch loss = 2.03 (14.8 examples/sec; 2.165 sec/batch; 44h:08m:11s remains)
INFO - root - 2017-12-07 15:10:04.650843: step 9730, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.143 sec/batch; 43h:40m:49s remains)
INFO - root - 2017-12-07 15:10:25.870338: step 9740, loss = 2.06, batch loss = 2.01 (14.8 examples/sec; 2.164 sec/batch; 44h:07m:15s remains)
INFO - root - 2017-12-07 15:10:46.667446: step 9750, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.179 sec/batch; 44h:24m:33s remains)
INFO - root - 2017-12-07 15:11:07.971965: step 9760, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.199 sec/batch; 44h:49m:10s remains)
INFO - root - 2017-12-07 15:11:29.154416: step 9770, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.121 sec/batch; 43h:13m:07s remains)
INFO - root - 2017-12-07 15:11:50.199247: step 9780, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.080 sec/batch; 42h:22m:22s remains)
INFO - root - 2017-12-07 15:12:11.258384: step 9790, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 43h:06m:57s remains)
INFO - root - 2017-12-07 15:12:32.459682: step 9800, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.153 sec/batch; 43h:51m:10s remains)
2017-12-07 15:12:34.061974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2661028 -4.2511544 -4.2440524 -4.2535505 -4.2676992 -4.2794023 -4.2884669 -4.2894688 -4.2795 -4.2663894 -4.2531929 -4.2391481 -4.2317276 -4.2402229 -4.2611508][-4.2056403 -4.1862144 -4.1791506 -4.1954274 -4.2202377 -4.2430439 -4.259901 -4.2606907 -4.2434583 -4.2150669 -4.1893029 -4.1712294 -4.1681709 -4.1857247 -4.2158732][-4.1584687 -4.1456285 -4.1488724 -4.1751938 -4.2066808 -4.2326775 -4.25002 -4.251543 -4.2321734 -4.1970148 -4.1670833 -4.1527462 -4.1583261 -4.1819134 -4.2142329][-4.1337056 -4.1360888 -4.1543369 -4.1893749 -4.2181067 -4.233686 -4.2435422 -4.2456856 -4.2314978 -4.2047 -4.1840277 -4.1797853 -4.1926832 -4.2174077 -4.247457][-4.1384673 -4.15402 -4.1814451 -4.2148743 -4.2324419 -4.2297225 -4.2241511 -4.2227635 -4.2176518 -4.2095904 -4.2073064 -4.2139363 -4.2304492 -4.2523222 -4.279768][-4.1662645 -4.1828432 -4.2043419 -4.2209125 -4.2183962 -4.1918344 -4.165535 -4.1605768 -4.1684113 -4.1818695 -4.2019711 -4.2217832 -4.2439022 -4.2685537 -4.2959185][-4.1980405 -4.2078676 -4.2156157 -4.2124877 -4.1850228 -4.1305122 -4.0784526 -4.0652757 -4.0878873 -4.1263056 -4.1726136 -4.2101793 -4.2436056 -4.2714009 -4.2949214][-4.2229652 -4.2222071 -4.2196112 -4.2014842 -4.1501989 -4.0665541 -3.9886861 -3.9740536 -4.0157 -4.08004 -4.1517959 -4.2065792 -4.2415113 -4.2624607 -4.2802296][-4.2200413 -4.2040772 -4.1880484 -4.1611495 -4.0986838 -4.0047874 -3.9241741 -3.9190814 -3.9759495 -4.0569735 -4.1334953 -4.1885257 -4.2169943 -4.2323208 -4.2489614][-4.2024407 -4.1689925 -4.14026 -4.1158528 -4.0677981 -3.9939148 -3.9385905 -3.9491432 -4.0015764 -4.0695243 -4.1255112 -4.1601176 -4.1756797 -4.1885624 -4.2089911][-4.1679497 -4.1202455 -4.0814009 -4.065589 -4.0437536 -4.0091734 -3.9924769 -4.013175 -4.0438347 -4.0769887 -4.1027441 -4.1148014 -4.1182852 -4.1331224 -4.1619368][-4.12287 -4.072011 -4.0321164 -4.0296044 -4.0358067 -4.0339808 -4.0420065 -4.0603657 -4.0672851 -4.0678792 -4.0684285 -4.070025 -4.0751162 -4.0976663 -4.1370044][-4.1179695 -4.085011 -4.0603848 -4.0672622 -4.08595 -4.0981703 -4.1149082 -4.1273785 -4.12004 -4.1031885 -4.0919566 -4.0920324 -4.1027279 -4.1261177 -4.16327][-4.1492081 -4.1379972 -4.1310139 -4.14226 -4.1593652 -4.1723881 -4.1852889 -4.1910763 -4.1801534 -4.1637397 -4.1533856 -4.1565566 -4.1688242 -4.1890926 -4.2213097][-4.2125778 -4.2134204 -4.2169356 -4.2273955 -4.2366104 -4.2421746 -4.2469645 -4.2471414 -4.2384162 -4.2293711 -4.2251239 -4.2297969 -4.2407527 -4.2564869 -4.2801619]]...]
INFO - root - 2017-12-07 15:12:55.093433: step 9810, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 43h:02m:04s remains)
INFO - root - 2017-12-07 15:13:15.916902: step 9820, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.089 sec/batch; 42h:32m:04s remains)
INFO - root - 2017-12-07 15:13:37.236005: step 9830, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 43h:26m:11s remains)
INFO - root - 2017-12-07 15:13:58.510730: step 9840, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.100 sec/batch; 42h:45m:03s remains)
INFO - root - 2017-12-07 15:14:19.343285: step 9850, loss = 2.08, batch loss = 2.03 (14.9 examples/sec; 2.148 sec/batch; 43h:43m:48s remains)
INFO - root - 2017-12-07 15:14:40.635291: step 9860, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.137 sec/batch; 43h:30m:01s remains)
INFO - root - 2017-12-07 15:15:02.041444: step 9870, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.155 sec/batch; 43h:51m:13s remains)
INFO - root - 2017-12-07 15:15:23.164609: step 9880, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.121 sec/batch; 43h:09m:44s remains)
INFO - root - 2017-12-07 15:15:44.416622: step 9890, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.116 sec/batch; 43h:02m:34s remains)
INFO - root - 2017-12-07 15:16:05.707315: step 9900, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.111 sec/batch; 42h:56m:51s remains)
2017-12-07 15:16:07.263116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2592254 -4.246871 -4.2412634 -4.2482247 -4.2539644 -4.2525291 -4.2517214 -4.2579465 -4.2607403 -4.2547584 -4.2492552 -4.2523351 -4.2633886 -4.2783947 -4.290658][-4.2796021 -4.2710519 -4.2646394 -4.2666421 -4.2706032 -4.2717357 -4.2745838 -4.2817984 -4.2858043 -4.2821417 -4.2787089 -4.2818384 -4.29056 -4.3022385 -4.3061118][-4.3013835 -4.2973123 -4.2913866 -4.2889776 -4.2895703 -4.2910461 -4.2955513 -4.3029613 -4.3056827 -4.3021946 -4.29972 -4.3022075 -4.3083963 -4.3165293 -4.3160572][-4.306654 -4.3040652 -4.2958255 -4.2871928 -4.2830033 -4.2820835 -4.2849045 -4.289916 -4.2907906 -4.2881017 -4.2852263 -4.2891459 -4.2981129 -4.3093667 -4.3121905][-4.28621 -4.284369 -4.2721324 -4.2579956 -4.2501683 -4.2469163 -4.2479734 -4.2499766 -4.2478814 -4.2417469 -4.2361979 -4.2419715 -4.2571673 -4.2758427 -4.2848554][-4.2554231 -4.247365 -4.2257156 -4.1998305 -4.18312 -4.1755738 -4.1752195 -4.1765294 -4.1717911 -4.1646643 -4.1580939 -4.1657848 -4.1884317 -4.2165575 -4.2375431][-4.2257142 -4.202971 -4.166575 -4.1245604 -4.0927367 -4.0759168 -4.0744772 -4.0796161 -4.0773726 -4.0746822 -4.0727859 -4.0873384 -4.1175051 -4.1542892 -4.1885881][-4.2172871 -4.1823883 -4.1383018 -4.0895309 -4.0500259 -4.0310812 -4.0354671 -4.0491076 -4.0507331 -4.0513272 -4.0553126 -4.0746379 -4.1050019 -4.143126 -4.1843748][-4.2453866 -4.2149687 -4.17962 -4.1436696 -4.1175385 -4.1098065 -4.12131 -4.1351604 -4.1326742 -4.1279693 -4.12835 -4.1428862 -4.1677132 -4.1963706 -4.2246861][-4.276268 -4.2606564 -4.2408953 -4.2208838 -4.2057738 -4.1992793 -4.2045641 -4.2100716 -4.2022667 -4.1934533 -4.1893229 -4.1980262 -4.2180152 -4.237864 -4.2549624][-4.2828045 -4.2833648 -4.2741675 -4.2592354 -4.246367 -4.2391257 -4.2389679 -4.238853 -4.2302904 -4.220952 -4.2120457 -4.2108784 -4.2212715 -4.2314591 -4.2414556][-4.2695017 -4.2788115 -4.2754941 -4.2609472 -4.2466192 -4.2407537 -4.2431283 -4.2472715 -4.2438817 -4.2350836 -4.2215261 -4.2101097 -4.2095118 -4.2093511 -4.2123275][-4.2626948 -4.2742538 -4.273613 -4.2606487 -4.24767 -4.2442079 -4.2502232 -4.2591047 -4.2606125 -4.2535515 -4.2394042 -4.2255921 -4.2209773 -4.2172894 -4.2177672][-4.2722168 -4.2770348 -4.2743249 -4.2647281 -4.2561722 -4.2573118 -4.2677722 -4.2787881 -4.2826166 -4.2778492 -4.2684255 -4.2601409 -4.2573938 -4.25456 -4.2544637][-4.2872229 -4.2830205 -4.276463 -4.2694945 -4.2633166 -4.2643919 -4.2752609 -4.2863231 -4.2891617 -4.2842388 -4.2791219 -4.27416 -4.271863 -4.2695241 -4.2704749]]...]
INFO - root - 2017-12-07 15:16:28.260794: step 9910, loss = 2.08, batch loss = 2.03 (16.1 examples/sec; 1.992 sec/batch; 40h:31m:19s remains)
2017-12-07 15:16:46.269057: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 921.38MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
INFO - root - 2017-12-07 15:16:49.496264: step 9920, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 43h:22m:20s remains)
INFO - root - 2017-12-07 15:17:10.803253: step 9930, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.131 sec/batch; 43h:20m:03s remains)
INFO - root - 2017-12-07 15:17:32.049815: step 9940, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.140 sec/batch; 43h:30m:05s remains)
INFO - root - 2017-12-07 15:17:53.295842: step 9950, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 43h:20m:02s remains)
INFO - root - 2017-12-07 15:18:14.441472: step 9960, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.189 sec/batch; 44h:29m:49s remains)
INFO - root - 2017-12-07 15:18:35.832533: step 9970, loss = 2.07, batch loss = 2.01 (16.0 examples/sec; 2.006 sec/batch; 40h:45m:53s remains)
INFO - root - 2017-12-07 15:18:56.747927: step 9980, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.076 sec/batch; 42h:10m:13s remains)
INFO - root - 2017-12-07 15:19:17.901072: step 9990, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 42h:45m:47s remains)
INFO - root - 2017-12-07 15:19:39.163999: step 10000, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.156 sec/batch; 43h:48m:01s remains)
2017-12-07 15:19:40.677897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2521281 -4.2573447 -4.2724485 -4.28622 -4.2931242 -4.2890854 -4.2798433 -4.270371 -4.2608285 -4.2566 -4.2561936 -4.2536359 -4.2545094 -4.2643223 -4.2803345][-4.2237349 -4.2349505 -4.2551045 -4.2698522 -4.2737775 -4.263998 -4.2519736 -4.2432113 -4.2359591 -4.2371254 -4.2410469 -4.2363729 -4.234201 -4.2442603 -4.2634597][-4.1854377 -4.2102833 -4.23644 -4.2515554 -4.2486429 -4.2293205 -4.2106247 -4.1990514 -4.1962147 -4.2075129 -4.2191133 -4.2146096 -4.2088408 -4.2183003 -4.2418904][-4.1440806 -4.1847854 -4.2174172 -4.2317052 -4.2209487 -4.1915436 -4.16374 -4.1450591 -4.146121 -4.17563 -4.2007532 -4.1975346 -4.1853471 -4.1900668 -4.2152572][-4.1099691 -4.1669388 -4.2032895 -4.2142243 -4.1944051 -4.1538172 -4.1142206 -4.0818281 -4.0880442 -4.1441183 -4.1865754 -4.1867428 -4.1709433 -4.1704454 -4.1938448][-4.0711074 -4.1439447 -4.182384 -4.1882081 -4.1567903 -4.099803 -4.0388389 -3.9862635 -4.0038147 -4.0941668 -4.1546564 -4.1610866 -4.1474385 -4.1501374 -4.1767225][-4.0156951 -4.1091185 -4.1545477 -4.1560879 -4.1060572 -4.018764 -3.9215312 -3.8394759 -3.8755586 -3.9998085 -4.0780044 -4.0927558 -4.08814 -4.1052828 -4.1447687][-3.9764364 -4.0831428 -4.1346278 -4.1345491 -4.0713549 -3.9572911 -3.8196154 -3.7068877 -3.760915 -3.9030781 -3.9897692 -4.0138288 -4.02219 -4.0541129 -4.10737][-4.0268087 -4.1195726 -4.1653275 -4.16669 -4.1116347 -4.003757 -3.8623354 -3.7536664 -3.8021276 -3.9141877 -3.9833961 -4.0078511 -4.0253239 -4.0599179 -4.1125436][-4.1304026 -4.1888967 -4.2184081 -4.221982 -4.1881251 -4.1117263 -4.00435 -3.9295573 -3.959054 -4.0196342 -4.0598426 -4.0796962 -4.0969963 -4.1235423 -4.1640229][-4.2112231 -4.2413564 -4.257288 -4.263051 -4.2485108 -4.2028422 -4.1334047 -4.0906076 -4.1025181 -4.1240916 -4.1412039 -4.1561294 -4.1698084 -4.1872768 -4.2165651][-4.2626204 -4.2776256 -4.282999 -4.289916 -4.2878695 -4.2647862 -4.2253027 -4.20416 -4.2053127 -4.2075586 -4.2136211 -4.2247405 -4.2324018 -4.2411256 -4.25926][-4.2892046 -4.2978044 -4.2965107 -4.3023849 -4.3079009 -4.2998919 -4.2797956 -4.2707281 -4.2694678 -4.2652936 -4.2680039 -4.2759004 -4.2794724 -4.2819018 -4.291141][-4.2927737 -4.2972221 -4.2938318 -4.2993937 -4.3088374 -4.3095322 -4.3021531 -4.2979994 -4.2946968 -4.2888131 -4.2869797 -4.2884383 -4.28814 -4.2868943 -4.2897243][-4.2760525 -4.2795053 -4.2778592 -4.2825847 -4.2898946 -4.2924247 -4.2909346 -4.2875471 -4.28059 -4.2713404 -4.2666574 -4.2668028 -4.2656083 -4.2653904 -4.2670693]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch32/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch32/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 15:20:02.083750: step 10010, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.146 sec/batch; 43h:35m:04s remains)
INFO - root - 2017-12-07 15:20:23.240265: step 10020, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.151 sec/batch; 43h:40m:19s remains)
INFO - root - 2017-12-07 15:20:44.303505: step 10030, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.113 sec/batch; 42h:54m:28s remains)
INFO - root - 2017-12-07 15:21:05.084128: step 10040, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.156 sec/batch; 43h:45m:42s remains)
INFO - root - 2017-12-07 15:21:26.322220: step 10050, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.097 sec/batch; 42h:33m:26s remains)
INFO - root - 2017-12-07 15:21:47.563699: step 10060, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.191 sec/batch; 44h:27m:57s remains)
INFO - root - 2017-12-07 15:22:08.524216: step 10070, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.160 sec/batch; 43h:50m:26s remains)
INFO - root - 2017-12-07 15:22:29.508610: step 10080, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.117 sec/batch; 42h:57m:14s remains)
INFO - root - 2017-12-07 15:22:50.679172: step 10090, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 42h:42m:08s remains)
INFO - root - 2017-12-07 15:23:11.797774: step 10100, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.096 sec/batch; 42h:31m:04s remains)
2017-12-07 15:23:13.510285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1507273 -4.1650839 -4.152935 -4.1301727 -4.1110773 -4.09134 -4.0890317 -4.1145821 -4.1559544 -4.1841292 -4.1785612 -4.1317124 -4.0845518 -4.0924029 -4.1429338][-4.153038 -4.16843 -4.1527152 -4.1321268 -4.1197739 -4.1022177 -4.1025658 -4.1284785 -4.1779928 -4.2150884 -4.2113976 -4.1652269 -4.1227121 -4.1291447 -4.16892][-4.1726656 -4.1849427 -4.1648445 -4.1404376 -4.13051 -4.1165485 -4.1180739 -4.1455483 -4.1994915 -4.2462382 -4.2497888 -4.2026844 -4.1617064 -4.1615996 -4.1926785][-4.1970434 -4.1976514 -4.1776094 -4.1537981 -4.1362247 -4.11498 -4.1143923 -4.1399407 -4.19456 -4.25165 -4.2676845 -4.2285867 -4.1884437 -4.18293 -4.21111][-4.2360067 -4.222939 -4.1990767 -4.1660733 -4.1239934 -4.0724559 -4.0477123 -4.0715723 -4.1351967 -4.206749 -4.2374673 -4.2150493 -4.1828089 -4.1792169 -4.2110133][-4.2635503 -4.2452626 -4.2102861 -4.1552949 -4.0820127 -3.9996257 -3.9425509 -3.9543619 -4.0329542 -4.1244445 -4.1728892 -4.172483 -4.1573782 -4.1641927 -4.2036285][-4.2625151 -4.2419362 -4.1989851 -4.1195631 -4.021296 -3.9217818 -3.851227 -3.8570976 -3.9386964 -4.0443168 -4.1098371 -4.1272888 -4.1265469 -4.1473527 -4.195044][-4.26549 -4.24282 -4.195188 -4.1034541 -3.9993567 -3.9117208 -3.8567924 -3.8640583 -3.9257021 -4.0124874 -4.0837083 -4.11578 -4.1262131 -4.1538115 -4.2036557][-4.2841506 -4.2643051 -4.2219272 -4.1407456 -4.0525622 -3.989707 -3.9578438 -3.9593744 -3.9834912 -4.0329151 -4.0949907 -4.136899 -4.1620378 -4.1935949 -4.2353625][-4.319716 -4.3079047 -4.2770743 -4.2169552 -4.1546364 -4.1161118 -4.0939283 -4.0868359 -4.0835357 -4.1021757 -4.1516156 -4.1915116 -4.2142205 -4.2405233 -4.2712736][-4.3431377 -4.3380947 -4.3218479 -4.2863345 -4.2500024 -4.2277646 -4.2102685 -4.1997476 -4.188457 -4.1932936 -4.2279453 -4.2602625 -4.2767658 -4.2935276 -4.3128805][-4.342134 -4.3458233 -4.3401074 -4.3198352 -4.2976165 -4.2862697 -4.2785983 -4.2744155 -4.2633495 -4.2637596 -4.2861805 -4.311605 -4.324264 -4.3342094 -4.3426514][-4.31478 -4.3274951 -4.3302283 -4.3180757 -4.3069286 -4.3047495 -4.3031893 -4.3001027 -4.2928786 -4.2957582 -4.3096261 -4.3285689 -4.3371692 -4.34289 -4.3443747][-4.2942638 -4.3070197 -4.3127522 -4.3057313 -4.2996078 -4.2995787 -4.3006067 -4.2981682 -4.2939081 -4.295176 -4.3050561 -4.3197613 -4.3281436 -4.3303132 -4.3279228][-4.2863197 -4.2939034 -4.2976923 -4.2964096 -4.295886 -4.2970266 -4.2980337 -4.295979 -4.2912431 -4.2900176 -4.295352 -4.3033609 -4.3103271 -4.3128295 -4.3111448]]...]
INFO - root - 2017-12-07 15:23:34.532575: step 10110, loss = 2.08, batch loss = 2.03 (14.8 examples/sec; 2.166 sec/batch; 43h:55m:20s remains)
INFO - root - 2017-12-07 15:23:55.698526: step 10120, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.157 sec/batch; 43h:44m:23s remains)
INFO - root - 2017-12-07 15:24:16.930993: step 10130, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 43h:08m:29s remains)
INFO - root - 2017-12-07 15:24:37.883698: step 10140, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 43h:25m:24s remains)
INFO - root - 2017-12-07 15:24:59.155784: step 10150, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.171 sec/batch; 44h:00m:17s remains)
INFO - root - 2017-12-07 15:25:20.456946: step 10160, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 43h:08m:32s remains)
INFO - root - 2017-12-07 15:25:41.560716: step 10170, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 2.207 sec/batch; 44h:43m:29s remains)
INFO - root - 2017-12-07 15:26:02.936099: step 10180, loss = 2.09, batch loss = 2.03 (14.8 examples/sec; 2.168 sec/batch; 43h:55m:09s remains)
INFO - root - 2017-12-07 15:26:24.178229: step 10190, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 43h:03m:45s remains)
INFO - root - 2017-12-07 15:26:44.897174: step 10200, loss = 2.06, batch loss = 2.00 (16.2 examples/sec; 1.971 sec/batch; 39h:55m:58s remains)
2017-12-07 15:26:46.445904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2639661 -4.2545571 -4.2323995 -4.21325 -4.1968665 -4.1859903 -4.1930723 -4.2109237 -4.2213798 -4.2252235 -4.2220664 -4.210681 -4.2093492 -4.2230458 -4.23844][-4.2583961 -4.24478 -4.2242603 -4.2032714 -4.1815882 -4.166748 -4.1784911 -4.2058988 -4.2247372 -4.2346578 -4.2351351 -4.2222419 -4.2123275 -4.21576 -4.2237406][-4.2435341 -4.2363248 -4.2226181 -4.2018738 -4.1805825 -4.161171 -4.1720715 -4.2025676 -4.2241926 -4.2357712 -4.242979 -4.2361336 -4.217927 -4.2080035 -4.20522][-4.22482 -4.2238612 -4.2185388 -4.2029176 -4.1791215 -4.1557255 -4.162221 -4.1951947 -4.2233195 -4.2380462 -4.2493286 -4.2470455 -4.2257166 -4.2047167 -4.1900735][-4.1998239 -4.2042103 -4.2036257 -4.1913753 -4.1655512 -4.1317573 -4.1278257 -4.1637053 -4.2055893 -4.2307229 -4.2422295 -4.2380919 -4.2207456 -4.1981506 -4.1740494][-4.1517811 -4.1579332 -4.1576576 -4.149199 -4.1230702 -4.0751576 -4.0496936 -4.0843019 -4.1524453 -4.2014747 -4.21715 -4.208179 -4.1930161 -4.1739521 -4.15215][-4.1355319 -4.1379347 -4.1296158 -4.1176944 -4.0912867 -4.02496 -3.9585779 -3.9704149 -4.0573759 -4.1335979 -4.1602879 -4.153111 -4.1441298 -4.12954 -4.1140165][-4.1726685 -4.1691666 -4.155488 -4.1424747 -4.1174731 -4.0493746 -3.9601068 -3.9367738 -4.00463 -4.085063 -4.1200566 -4.1134043 -4.0996666 -4.0842924 -4.0737367][-4.2171946 -4.2189569 -4.21421 -4.209353 -4.193881 -4.1468453 -4.0790124 -4.0320873 -4.0462189 -4.0975313 -4.1272039 -4.1163673 -4.0930991 -4.0698 -4.0598702][-4.2362928 -4.2417479 -4.2465329 -4.2508364 -4.244802 -4.216188 -4.1710234 -4.1255779 -4.1082468 -4.1325917 -4.156527 -4.1543174 -4.1377 -4.1186562 -4.1032233][-4.2489347 -4.2564368 -4.2637606 -4.2697067 -4.267127 -4.2495956 -4.2205553 -4.1843591 -4.1556015 -4.1601777 -4.1818447 -4.1896868 -4.184566 -4.1743355 -4.1584105][-4.2629013 -4.2740135 -4.2840323 -4.2894998 -4.2876296 -4.2749767 -4.2533154 -4.2266173 -4.1977086 -4.1874928 -4.197402 -4.2042727 -4.2024407 -4.195117 -4.1810217][-4.2622709 -4.27521 -4.2886329 -4.2941313 -4.2935519 -4.2857924 -4.272954 -4.2585559 -4.242991 -4.2349234 -4.2387266 -4.2397327 -4.2287765 -4.2107024 -4.190166][-4.2684808 -4.27458 -4.2834339 -4.2857366 -4.2836585 -4.2780766 -4.2700596 -4.2661061 -4.263927 -4.2653241 -4.272985 -4.2765493 -4.2657604 -4.2426152 -4.2188187][-4.2876835 -4.2908354 -4.2944732 -4.2898941 -4.2824845 -4.2736173 -4.2654915 -4.264421 -4.2678733 -4.2742095 -4.2839251 -4.2883697 -4.2785168 -4.2581043 -4.2403121]]...]
INFO - root - 2017-12-07 15:27:07.844496: step 10210, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.178 sec/batch; 44h:06m:42s remains)
INFO - root - 2017-12-07 15:27:29.071250: step 10220, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.162 sec/batch; 43h:47m:31s remains)
INFO - root - 2017-12-07 15:27:50.086979: step 10230, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.092 sec/batch; 42h:21m:45s remains)
INFO - root - 2017-12-07 15:28:11.146889: step 10240, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.072 sec/batch; 41h:57m:03s remains)
INFO - root - 2017-12-07 15:28:32.355996: step 10250, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.157 sec/batch; 43h:39m:57s remains)
INFO - root - 2017-12-07 15:28:53.379478: step 10260, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 1.941 sec/batch; 39h:17m:36s remains)
INFO - root - 2017-12-07 15:29:14.348567: step 10270, loss = 2.08, batch loss = 2.03 (14.8 examples/sec; 2.157 sec/batch; 43h:38m:35s remains)
INFO - root - 2017-12-07 15:29:35.421746: step 10280, loss = 2.09, batch loss = 2.03 (15.5 examples/sec; 2.068 sec/batch; 41h:50m:19s remains)
INFO - root - 2017-12-07 15:29:56.761832: step 10290, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 42h:34m:51s remains)
INFO - root - 2017-12-07 15:30:17.793455: step 10300, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.094 sec/batch; 42h:21m:20s remains)
2017-12-07 15:30:19.399588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3442125 -4.353651 -4.3400378 -4.3026285 -4.2590342 -4.20537 -4.1377439 -4.075501 -4.0696507 -4.089839 -4.12859 -4.1893911 -4.2356987 -4.2604084 -4.279007][-4.3431168 -4.3492231 -4.3290486 -4.2829127 -4.2344036 -4.181046 -4.1204009 -4.0724058 -4.0724444 -4.0907393 -4.1276298 -4.1859822 -4.227819 -4.2525167 -4.2678967][-4.3437591 -4.3464785 -4.3193 -4.2661858 -4.2159848 -4.16512 -4.1134257 -4.0853128 -4.1033716 -4.1328325 -4.1699686 -4.2178659 -4.2436013 -4.2578797 -4.2610683][-4.3484898 -4.3484879 -4.3172064 -4.260118 -4.2086048 -4.155736 -4.1031876 -4.0895386 -4.1299672 -4.1817656 -4.2273345 -4.2644186 -4.2712145 -4.2707782 -4.2573633][-4.355299 -4.35228 -4.3183475 -4.2602625 -4.2054448 -4.1413131 -4.0697145 -4.0529933 -4.1183386 -4.2051873 -4.2691751 -4.2954783 -4.2874331 -4.2778382 -4.2509589][-4.358912 -4.3527613 -4.3149648 -4.2514992 -4.1864071 -4.0980239 -3.9818833 -3.9335189 -4.0361085 -4.1760578 -4.2684011 -4.2993712 -4.2921648 -4.2795982 -4.2438126][-4.3576365 -4.3470311 -4.3031473 -4.2307553 -4.1503935 -4.0279007 -3.8528428 -3.7649441 -3.9229429 -4.1242118 -4.2455473 -4.2929549 -4.2995696 -4.28662 -4.2436647][-4.3534832 -4.3399482 -4.2959609 -4.2248993 -4.14552 -4.0151119 -3.8221169 -3.7246587 -3.901037 -4.1158066 -4.2386522 -4.2919483 -4.307467 -4.2879634 -4.2359428][-4.3497343 -4.3370166 -4.3009405 -4.244689 -4.1864734 -4.0881777 -3.9414396 -3.8799016 -4.0210218 -4.1853852 -4.2735825 -4.3068871 -4.3071222 -4.2751236 -4.21378][-4.3450346 -4.335485 -4.308239 -4.2660666 -4.2251282 -4.1627932 -4.0749893 -4.0567713 -4.1609006 -4.2676406 -4.3135791 -4.3207183 -4.2997389 -4.2546706 -4.1892567][-4.3374419 -4.3283582 -4.3015985 -4.2642913 -4.2298412 -4.1901655 -4.1509509 -4.1661839 -4.247324 -4.3183913 -4.3372045 -4.3243823 -4.2901392 -4.2433443 -4.1886668][-4.3296242 -4.31971 -4.2914095 -4.2558484 -4.22153 -4.1918035 -4.181848 -4.2168202 -4.2870274 -4.3397622 -4.3434744 -4.3186712 -4.2821927 -4.24222 -4.2029095][-4.3233891 -4.3126993 -4.2843161 -4.2478371 -4.2115498 -4.1853342 -4.1918068 -4.2358065 -4.3023863 -4.3446655 -4.3379436 -4.3003154 -4.2627864 -4.2256122 -4.1929979][-4.3226132 -4.3101397 -4.2826724 -4.244175 -4.2055459 -4.1832976 -4.1969748 -4.24175 -4.3029833 -4.333878 -4.3182807 -4.2724276 -4.2341728 -4.19935 -4.1717033][-4.3275986 -4.3139691 -4.2880287 -4.2501197 -4.2125244 -4.1938839 -4.2076797 -4.2443957 -4.2901678 -4.3062911 -4.2882504 -4.2444191 -4.2052822 -4.176403 -4.1537018]]...]
INFO - root - 2017-12-07 15:30:40.552219: step 10310, loss = 2.06, batch loss = 2.00 (14.8 examples/sec; 2.162 sec/batch; 43h:44m:07s remains)
INFO - root - 2017-12-07 15:31:01.721638: step 10320, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.118 sec/batch; 42h:49m:58s remains)
INFO - root - 2017-12-07 15:31:22.709754: step 10330, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.111 sec/batch; 42h:41m:00s remains)
INFO - root - 2017-12-07 15:31:43.981260: step 10340, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.115 sec/batch; 42h:46m:08s remains)
INFO - root - 2017-12-07 15:32:05.017395: step 10350, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.111 sec/batch; 42h:40m:53s remains)
INFO - root - 2017-12-07 15:32:26.034527: step 10360, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.159 sec/batch; 43h:38m:15s remains)
INFO - root - 2017-12-07 15:32:47.111938: step 10370, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.148 sec/batch; 43h:24m:37s remains)
INFO - root - 2017-12-07 15:33:08.394929: step 10380, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.146 sec/batch; 43h:21m:44s remains)
INFO - root - 2017-12-07 15:33:29.474884: step 10390, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.087 sec/batch; 42h:09m:33s remains)
INFO - root - 2017-12-07 15:33:50.507723: step 10400, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.108 sec/batch; 42h:35m:34s remains)
2017-12-07 15:33:52.064041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3174019 -4.3100672 -4.2975163 -4.2878447 -4.2931662 -4.3138914 -4.3322353 -4.3406024 -4.3409324 -4.3423243 -4.3397937 -4.3312831 -4.3246675 -4.3181973 -4.3148317][-4.3228583 -4.31307 -4.2923937 -4.2731209 -4.2720847 -4.2896223 -4.3085589 -4.321209 -4.3304472 -4.3417945 -4.3400893 -4.326138 -4.3131666 -4.2992148 -4.29097][-4.3206105 -4.3051877 -4.2729254 -4.2412434 -4.2309709 -4.2420006 -4.2598538 -4.2782726 -4.299819 -4.3222289 -4.3264451 -4.3092852 -4.2869282 -4.2635465 -4.245153][-4.3186336 -4.2969275 -4.25427 -4.2094507 -4.185349 -4.18442 -4.1987653 -4.2229013 -4.25797 -4.2907944 -4.3016324 -4.2845607 -4.2526331 -4.2168941 -4.1856761][-4.3176289 -4.2934904 -4.2471728 -4.19316 -4.15155 -4.1313848 -4.139276 -4.1684861 -4.21654 -4.2603111 -4.2777719 -4.2612658 -4.2210474 -4.1702533 -4.1220961][-4.3164644 -4.2943859 -4.2527423 -4.1979384 -4.1386614 -4.0918317 -4.08389 -4.1124215 -4.1717849 -4.2270451 -4.255682 -4.2484007 -4.2063174 -4.142076 -4.0753231][-4.3170629 -4.2999063 -4.2675529 -4.2190318 -4.1499243 -4.0792346 -4.0473661 -4.0677447 -4.1304226 -4.1952758 -4.2384558 -4.2502232 -4.2203937 -4.153234 -4.073719][-4.3214307 -4.3085051 -4.2841763 -4.2463551 -4.1797228 -4.0992174 -4.0453062 -4.0515079 -4.1061397 -4.1682382 -4.2206907 -4.2528167 -4.2430763 -4.191371 -4.1187205][-4.3279462 -4.3169994 -4.2983475 -4.2709637 -4.2182832 -4.1457677 -4.0818768 -4.0689926 -4.1039524 -4.1513805 -4.200325 -4.2424951 -4.2511845 -4.2236562 -4.1741133][-4.3303351 -4.3199992 -4.3048511 -4.28424 -4.2472544 -4.1953154 -4.1351686 -4.1092186 -4.1228304 -4.1524286 -4.1901093 -4.2295589 -4.2487736 -4.2422037 -4.219305][-4.33315 -4.322578 -4.3099222 -4.2944293 -4.2684555 -4.2374706 -4.1940956 -4.1669679 -4.1656346 -4.1792865 -4.2024465 -4.2302647 -4.2487245 -4.2531781 -4.2504697][-4.3357835 -4.3264146 -4.3163252 -4.3041611 -4.2840281 -4.2685275 -4.2442322 -4.2259674 -4.218811 -4.2190371 -4.22927 -4.246099 -4.2580776 -4.2658644 -4.2725086][-4.3329091 -4.3266606 -4.3194704 -4.3098836 -4.2935648 -4.2868876 -4.277853 -4.2710276 -4.2662163 -4.260148 -4.2594104 -4.2658281 -4.2690411 -4.2729678 -4.2771254][-4.3235903 -4.3155837 -4.3114104 -4.30313 -4.2886252 -4.2850509 -4.2876987 -4.2949276 -4.2947326 -4.2855406 -4.2772231 -4.2735314 -4.2644348 -4.2567372 -4.2484813][-4.305583 -4.2928352 -4.2878609 -4.2776146 -4.2609181 -4.2582564 -4.2695374 -4.2859211 -4.2882533 -4.2772226 -4.2648468 -4.2561927 -4.2418509 -4.221662 -4.2003183]]...]
INFO - root - 2017-12-07 15:34:13.248071: step 10410, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.163 sec/batch; 43h:40m:46s remains)
INFO - root - 2017-12-07 15:34:34.513766: step 10420, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 42h:29m:33s remains)
INFO - root - 2017-12-07 15:34:55.404535: step 10430, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.145 sec/batch; 43h:19m:01s remains)
2017-12-07 15:34:58.582283: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 921.38MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
INFO - root - 2017-12-07 15:35:16.550201: step 10440, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.152 sec/batch; 43h:27m:11s remains)
INFO - root - 2017-12-07 15:35:37.769925: step 10450, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.111 sec/batch; 42h:37m:31s remains)
INFO - root - 2017-12-07 15:35:58.647505: step 10460, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.097 sec/batch; 42h:19m:59s remains)
INFO - root - 2017-12-07 15:36:19.974994: step 10470, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 42h:58m:09s remains)
INFO - root - 2017-12-07 15:36:41.225160: step 10480, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.107 sec/batch; 42h:31m:10s remains)
INFO - root - 2017-12-07 15:37:02.186167: step 10490, loss = 2.09, batch loss = 2.03 (15.9 examples/sec; 2.014 sec/batch; 40h:37m:31s remains)
INFO - root - 2017-12-07 15:37:23.250919: step 10500, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 42h:55m:58s remains)
2017-12-07 15:37:24.906284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2563272 -4.2558169 -4.2492304 -4.2336783 -4.2094121 -4.1819696 -4.1640792 -4.1598377 -4.1654844 -4.17755 -4.1869874 -4.2007194 -4.2241139 -4.24937 -4.2724791][-4.2613063 -4.2598147 -4.2504525 -4.228879 -4.1954412 -4.1608405 -4.144906 -4.1506276 -4.1615777 -4.17009 -4.1767116 -4.1854658 -4.2064919 -4.2342434 -4.2575378][-4.2565174 -4.2501345 -4.2389503 -4.2166214 -4.1858039 -4.1568394 -4.1520247 -4.1654549 -4.1761236 -4.1794634 -4.1824775 -4.1845264 -4.193718 -4.2132645 -4.2317872][-4.218164 -4.2078552 -4.1998005 -4.1872506 -4.1718988 -4.1610322 -4.1667638 -4.1827354 -4.1914783 -4.1902733 -4.1865973 -4.1867304 -4.1871986 -4.1946378 -4.2068605][-4.1570196 -4.1503224 -4.1508126 -4.1506839 -4.1484571 -4.1509409 -4.1577196 -4.1712527 -4.1756516 -4.1642079 -4.1535997 -4.1612864 -4.1673751 -4.1705265 -4.176641][-4.0853081 -4.08699 -4.0966325 -4.101254 -4.1035614 -4.1070371 -4.1064792 -4.1106529 -4.1030936 -4.0767837 -4.0621295 -4.085712 -4.1120534 -4.1256895 -4.1339498][-4.0495872 -4.0478134 -4.0528331 -4.0477061 -4.0414853 -4.0338335 -4.0164795 -4.0083623 -3.9939175 -3.9571803 -3.9434555 -3.9861581 -4.0345893 -4.060946 -4.0766172][-4.0868964 -4.0705848 -4.0557151 -4.0296855 -4.0044856 -3.9758039 -3.9375994 -3.9157512 -3.9070039 -3.8757327 -3.8585391 -3.90481 -3.9560289 -3.9838564 -4.0057364][-4.1499867 -4.1272388 -4.1052661 -4.0754895 -4.0439806 -4.0069194 -3.9651673 -3.9442523 -3.9459605 -3.9335606 -3.915483 -3.9427259 -3.9693394 -3.9772913 -3.9892159][-4.2025123 -4.1805706 -4.1624322 -4.1397753 -4.1106048 -4.0778322 -4.0481372 -4.0380487 -4.0425787 -4.0388494 -4.0229111 -4.0337944 -4.0415587 -4.0359259 -4.0363712][-4.2361488 -4.2188263 -4.2089863 -4.1943235 -4.1697626 -4.1428967 -4.12466 -4.12081 -4.1224341 -4.1199789 -4.108614 -4.1142383 -4.1184468 -4.1112366 -4.1065655][-4.2472744 -4.2376246 -4.2364731 -4.2314467 -4.2179689 -4.2020888 -4.1933727 -4.1919742 -4.1913447 -4.1892562 -4.1813884 -4.1841669 -4.1879005 -4.1835141 -4.1808004][-4.2502356 -4.2463932 -4.2504992 -4.2516484 -4.2467146 -4.2397509 -4.2389956 -4.2399344 -4.2393 -4.2385144 -4.2334294 -4.2339139 -4.2391787 -4.2393112 -4.2401247][-4.2610326 -4.2584853 -4.2618794 -4.2621431 -4.2589393 -4.2558408 -4.258491 -4.2605252 -4.2607193 -4.2623281 -4.2606263 -4.2584743 -4.2639375 -4.2697186 -4.27518][-4.2749128 -4.2739663 -4.2757235 -4.2738562 -4.2691207 -4.2656097 -4.2665157 -4.2660174 -4.2631369 -4.2627296 -4.2615223 -4.2591448 -4.2630596 -4.2701139 -4.2748089]]...]
INFO - root - 2017-12-07 15:37:46.075796: step 10510, loss = 2.06, batch loss = 2.00 (15.4 examples/sec; 2.083 sec/batch; 42h:00m:29s remains)
INFO - root - 2017-12-07 15:38:07.294792: step 10520, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.112 sec/batch; 42h:35m:22s remains)
INFO - root - 2017-12-07 15:38:28.554740: step 10530, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.180 sec/batch; 43h:57m:21s remains)
INFO - root - 2017-12-07 15:38:49.804413: step 10540, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.198 sec/batch; 44h:19m:28s remains)
INFO - root - 2017-12-07 15:39:10.816590: step 10550, loss = 2.08, batch loss = 2.03 (16.5 examples/sec; 1.945 sec/batch; 39h:12m:25s remains)
INFO - root - 2017-12-07 15:39:32.140211: step 10560, loss = 2.08, batch loss = 2.02 (14.3 examples/sec; 2.233 sec/batch; 45h:01m:01s remains)
INFO - root - 2017-12-07 15:39:53.331724: step 10570, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 42h:43m:21s remains)
INFO - root - 2017-12-07 15:40:14.455957: step 10580, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.059 sec/batch; 41h:30m:01s remains)
INFO - root - 2017-12-07 15:40:35.516446: step 10590, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 2.200 sec/batch; 44h:19m:41s remains)
INFO - root - 2017-12-07 15:40:56.851712: step 10600, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.087 sec/batch; 42h:02m:17s remains)
2017-12-07 15:40:58.488737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.185389 -4.1858368 -4.1905227 -4.1942887 -4.1982446 -4.2031565 -4.2134218 -4.2270885 -4.2403231 -4.2512622 -4.2603273 -4.2665415 -4.2709479 -4.2720137 -4.2706337][-4.162816 -4.1607409 -4.1631861 -4.1609907 -4.1567764 -4.1580205 -4.1690183 -4.1804919 -4.1871281 -4.1905217 -4.1953321 -4.1992826 -4.2016406 -4.1997581 -4.1965427][-4.1346388 -4.1285477 -4.1287608 -4.1273408 -4.1229911 -4.1233444 -4.1313796 -4.1348391 -4.12927 -4.1223087 -4.1204658 -4.1196284 -4.1185131 -4.1139936 -4.1083679][-4.108459 -4.0959044 -4.0945029 -4.09873 -4.1027513 -4.1075454 -4.1141992 -4.111104 -4.0990133 -4.0890145 -4.0898452 -4.0905409 -4.0891218 -4.0844913 -4.07835][-4.0882077 -4.066083 -4.06018 -4.0693955 -4.0801253 -4.0858846 -4.0850496 -4.07359 -4.0615215 -4.063108 -4.0779905 -4.0895777 -4.0930028 -4.0893183 -4.0816841][-4.0826859 -4.0448856 -4.0281415 -4.0369725 -4.0500684 -4.0525517 -4.0415297 -4.0215497 -4.0148158 -4.0364218 -4.0735636 -4.1000786 -4.1119862 -4.1131282 -4.1033635][-4.1052589 -4.060832 -4.0348549 -4.0355544 -4.0469604 -4.0478997 -4.035656 -4.0163674 -4.0132246 -4.0408287 -4.0864644 -4.120779 -4.1361823 -4.1375093 -4.12929][-4.1503167 -4.1097083 -4.0827789 -4.0772996 -4.0834546 -4.0836515 -4.0769715 -4.0597124 -4.0468278 -4.0539455 -4.0838256 -4.1142292 -4.1283484 -4.1301308 -4.1281552][-4.1945248 -4.1598268 -4.1384544 -4.1292353 -4.12996 -4.1285882 -4.1276875 -4.1138558 -4.0907278 -4.0732231 -4.0754871 -4.0891 -4.0974803 -4.1004629 -4.1071758][-4.2207751 -4.1882725 -4.1717887 -4.1651268 -4.1626463 -4.1596713 -4.1648974 -4.1588497 -4.1374941 -4.1115656 -4.0957479 -4.088068 -4.0829673 -4.0819831 -4.0933847][-4.2267308 -4.1962571 -4.1819372 -4.1767859 -4.1705585 -4.1620631 -4.1654205 -4.1650414 -4.1549659 -4.1397057 -4.1239424 -4.1095881 -4.0981803 -4.0910068 -4.0973144][-4.2184305 -4.1931634 -4.1820078 -4.1769128 -4.1678495 -4.1550465 -4.153161 -4.1592674 -4.167244 -4.1686044 -4.1614404 -4.1513095 -4.142839 -4.133955 -4.1290169][-4.2034731 -4.1804938 -4.1742582 -4.1711407 -4.1637597 -4.1506343 -4.1468396 -4.1583652 -4.1781282 -4.1908488 -4.1911545 -4.1850362 -4.1824121 -4.1777411 -4.1674671][-4.2077522 -4.18638 -4.181601 -4.1778736 -4.1687965 -4.15517 -4.1508784 -4.1647515 -4.1856136 -4.1972876 -4.1951847 -4.1861639 -4.1844134 -4.1883774 -4.1862922][-4.227109 -4.2079697 -4.2032084 -4.1995749 -4.1910272 -4.1800418 -4.177907 -4.1903033 -4.2048326 -4.206358 -4.1933932 -4.1773872 -4.1754966 -4.1865559 -4.1947432]]...]
INFO - root - 2017-12-07 15:41:19.577275: step 10610, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.120 sec/batch; 42h:42m:41s remains)
INFO - root - 2017-12-07 15:41:40.456448: step 10620, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.100 sec/batch; 42h:17m:26s remains)
INFO - root - 2017-12-07 15:42:01.567048: step 10630, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.068 sec/batch; 41h:38m:20s remains)
INFO - root - 2017-12-07 15:42:22.727807: step 10640, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.167 sec/batch; 43h:37m:20s remains)
INFO - root - 2017-12-07 15:42:43.852242: step 10650, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.107 sec/batch; 42h:24m:45s remains)
INFO - root - 2017-12-07 15:43:04.975740: step 10660, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.154 sec/batch; 43h:21m:20s remains)
INFO - root - 2017-12-07 15:43:26.150407: step 10670, loss = 2.09, batch loss = 2.03 (15.7 examples/sec; 2.042 sec/batch; 41h:05m:16s remains)
INFO - root - 2017-12-07 15:43:47.103837: step 10680, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.162 sec/batch; 43h:30m:42s remains)
INFO - root - 2017-12-07 15:44:07.898987: step 10690, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.139 sec/batch; 43h:02m:08s remains)
INFO - root - 2017-12-07 15:44:29.148875: step 10700, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 42h:38m:06s remains)
2017-12-07 15:44:30.690806: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2787008 -4.2969403 -4.3006287 -4.2884455 -4.258522 -4.23945 -4.2442551 -4.2548108 -4.2614326 -4.2619338 -4.2623239 -4.2656169 -4.2750177 -4.283957 -4.2906876][-4.2548308 -4.2757487 -4.2816586 -4.2742276 -4.2403336 -4.2113786 -4.2130985 -4.2295208 -4.2400804 -4.2475739 -4.2537942 -4.2590117 -4.2697964 -4.2781014 -4.2804279][-4.2202954 -4.2460136 -4.2592807 -4.2577906 -4.2272148 -4.1912985 -4.1830115 -4.1954718 -4.2029009 -4.2132015 -4.2294483 -4.2452683 -4.2635021 -4.2748928 -4.2748914][-4.1874866 -4.217185 -4.2379994 -4.2394071 -4.2164259 -4.181447 -4.1651645 -4.1638913 -4.1640668 -4.1767678 -4.2043 -4.228385 -4.2528739 -4.2678633 -4.2706928][-4.1888475 -4.2092848 -4.2255907 -4.2257533 -4.2148089 -4.1870174 -4.1628671 -4.1425343 -4.1406059 -4.1635184 -4.1996894 -4.2275739 -4.2513623 -4.2661471 -4.2679577][-4.2006187 -4.208744 -4.214694 -4.2143903 -4.211586 -4.190309 -4.1526394 -4.1000557 -4.0888677 -4.1326051 -4.1841569 -4.2184525 -4.2463613 -4.2654529 -4.2657046][-4.201405 -4.2011333 -4.2009864 -4.1977763 -4.1927686 -4.1688209 -4.1155772 -4.0207992 -3.9845514 -4.0541415 -4.13289 -4.1834478 -4.2221661 -4.2511907 -4.2558417][-4.1989784 -4.1918607 -4.1901755 -4.1839385 -4.1665859 -4.1314511 -4.0577831 -3.9189894 -3.8484583 -3.9390554 -4.056632 -4.1370707 -4.1925988 -4.2265759 -4.2367983][-4.1840239 -4.1797447 -4.1786194 -4.1774936 -4.1650729 -4.13126 -4.0586209 -3.9231033 -3.8460836 -3.9185271 -4.0339785 -4.1251955 -4.1879244 -4.2177839 -4.22729][-4.1607141 -4.1676383 -4.1739573 -4.1819525 -4.1822839 -4.1629086 -4.1116486 -4.0187993 -3.9678235 -4.0065846 -4.0866289 -4.1623535 -4.2152424 -4.2360711 -4.2419167][-4.1630888 -4.1787438 -4.1925139 -4.2003808 -4.2039027 -4.1977787 -4.1643052 -4.1007071 -4.0668626 -4.0886984 -4.1442456 -4.20372 -4.249958 -4.2671819 -4.2704768][-4.20446 -4.2176533 -4.2274427 -4.2292218 -4.228054 -4.225719 -4.2042141 -4.1535931 -4.1211519 -4.1333089 -4.1755586 -4.2286892 -4.2711549 -4.2890186 -4.291502][-4.2474346 -4.2571917 -4.2603121 -4.2555985 -4.248157 -4.2424841 -4.2289982 -4.1937232 -4.1658678 -4.170835 -4.2014766 -4.2445164 -4.2803664 -4.2969317 -4.3002853][-4.260241 -4.2692628 -4.2715983 -4.2677374 -4.2609715 -4.2542815 -4.247818 -4.2308154 -4.2157764 -4.219964 -4.2380738 -4.2641482 -4.2889223 -4.3038349 -4.3094988][-4.2684355 -4.2744818 -4.2775655 -4.2755351 -4.2713661 -4.2679853 -4.2665024 -4.2617493 -4.2578831 -4.2628636 -4.2726326 -4.2857704 -4.3003869 -4.3110404 -4.3162875]]...]
INFO - root - 2017-12-07 15:44:52.010223: step 10710, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.106 sec/batch; 42h:21m:18s remains)
INFO - root - 2017-12-07 15:45:12.788680: step 10720, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.075 sec/batch; 41h:43m:37s remains)
INFO - root - 2017-12-07 15:45:33.921329: step 10730, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.134 sec/batch; 42h:54m:27s remains)
INFO - root - 2017-12-07 15:45:55.323846: step 10740, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 42h:44m:12s remains)
INFO - root - 2017-12-07 15:46:16.351361: step 10750, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.120 sec/batch; 42h:36m:49s remains)
INFO - root - 2017-12-07 15:46:37.458042: step 10760, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 42h:28m:12s remains)
INFO - root - 2017-12-07 15:46:58.762103: step 10770, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 42h:43m:05s remains)
INFO - root - 2017-12-07 15:47:19.574067: step 10780, loss = 2.07, batch loss = 2.01 (16.2 examples/sec; 1.978 sec/batch; 39h:44m:28s remains)
INFO - root - 2017-12-07 15:47:40.590865: step 10790, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.110 sec/batch; 42h:24m:00s remains)
INFO - root - 2017-12-07 15:48:01.890146: step 10800, loss = 2.06, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 42h:28m:40s remains)
2017-12-07 15:48:03.510013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29585 -4.2637715 -4.2351723 -4.2256608 -4.2296643 -4.2377977 -4.2446322 -4.2466812 -4.2463226 -4.2527714 -4.2705336 -4.2838135 -4.2872572 -4.2871919 -4.2878814][-4.2854328 -4.2433963 -4.2052383 -4.1905193 -4.1936355 -4.2066841 -4.2181683 -4.2251592 -4.2264419 -4.2358637 -4.257689 -4.2728672 -4.2757807 -4.2710228 -4.2655826][-4.2834373 -4.2358871 -4.1908503 -4.1725383 -4.1730981 -4.1832 -4.1908221 -4.1984076 -4.2066026 -4.2239323 -4.2518353 -4.2668114 -4.2672205 -4.26064 -4.2522449][-4.2884436 -4.2383766 -4.1878343 -4.1626391 -4.1558204 -4.1607909 -4.1646824 -4.1713982 -4.188375 -4.2139926 -4.24508 -4.2617993 -4.2579217 -4.2495832 -4.2427964][-4.2943168 -4.2468166 -4.1930585 -4.1566558 -4.1372638 -4.1354351 -4.1352577 -4.1368332 -4.1502838 -4.17997 -4.215693 -4.2367535 -4.2333665 -4.2294807 -4.2253823][-4.2994132 -4.2561932 -4.2010903 -4.150022 -4.1121821 -4.0970311 -4.0874262 -4.0776329 -4.0843253 -4.1214895 -4.1675053 -4.1961975 -4.2006397 -4.2073789 -4.2121677][-4.304493 -4.2605114 -4.20377 -4.1405621 -4.0861726 -4.0531249 -4.03554 -4.0154428 -4.016037 -4.0648761 -4.1221943 -4.15502 -4.165688 -4.1854386 -4.2089605][-4.3114719 -4.2649217 -4.2052817 -4.1374459 -4.07385 -4.0335026 -4.0216136 -4.0083151 -4.0098586 -4.0601926 -4.1164389 -4.1410084 -4.1465034 -4.1661325 -4.1964326][-4.319809 -4.2720118 -4.2099233 -4.1411781 -4.0768771 -4.0362816 -4.0359621 -4.0421686 -4.0566053 -4.1018662 -4.1456666 -4.1560807 -4.148222 -4.156961 -4.1789389][-4.3303795 -4.2885904 -4.2332358 -4.1696539 -4.1109052 -4.0700774 -4.0677452 -4.08434 -4.111598 -4.1464915 -4.17245 -4.1709156 -4.1564355 -4.1558523 -4.1680861][-4.3409953 -4.3112659 -4.2693024 -4.2191157 -4.1714921 -4.1357961 -4.1299868 -4.146419 -4.1740122 -4.1975713 -4.2052946 -4.1911526 -4.17077 -4.1683416 -4.1803713][-4.34986 -4.3312454 -4.3025532 -4.2685137 -4.2365088 -4.2143564 -4.2106891 -4.22501 -4.2466078 -4.2589216 -4.2581596 -4.238317 -4.2139049 -4.2077718 -4.2177882][-4.3543339 -4.3447394 -4.3270407 -4.3046651 -4.2856269 -4.2758832 -4.2767353 -4.2887983 -4.3036046 -4.3110495 -4.3090849 -4.291553 -4.2706385 -4.2600937 -4.2623243][-4.3538857 -4.349978 -4.3412304 -4.3276238 -4.3168573 -4.313025 -4.3144379 -4.3206258 -4.3295841 -4.336482 -4.3346858 -4.322731 -4.3074369 -4.2961545 -4.2921505][-4.3534908 -4.3518963 -4.3477631 -4.3413916 -4.3348017 -4.3324952 -4.332696 -4.3342643 -4.3382134 -4.343308 -4.3434114 -4.336041 -4.3245978 -4.315908 -4.3114233]]...]
INFO - root - 2017-12-07 15:48:24.631377: step 10810, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.096 sec/batch; 42h:06m:37s remains)
INFO - root - 2017-12-07 15:48:45.651871: step 10820, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 42h:39m:19s remains)
INFO - root - 2017-12-07 15:49:06.645966: step 10830, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.076 sec/batch; 41h:41m:56s remains)
INFO - root - 2017-12-07 15:49:27.767135: step 10840, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 2.046 sec/batch; 41h:05m:04s remains)
INFO - root - 2017-12-07 15:49:48.541488: step 10850, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.100 sec/batch; 42h:09m:15s remains)
INFO - root - 2017-12-07 15:50:09.506699: step 10860, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 42h:53m:29s remains)
INFO - root - 2017-12-07 15:50:30.593902: step 10870, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.098 sec/batch; 42h:06m:20s remains)
INFO - root - 2017-12-07 15:50:51.599421: step 10880, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 2.199 sec/batch; 44h:08m:20s remains)
INFO - root - 2017-12-07 15:51:12.509516: step 10890, loss = 2.07, batch loss = 2.01 (15.7 examples/sec; 2.040 sec/batch; 40h:55m:44s remains)
INFO - root - 2017-12-07 15:51:33.709367: step 10900, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.117 sec/batch; 42h:28m:03s remains)
2017-12-07 15:51:35.228710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3213372 -4.3275495 -4.3351893 -4.3351316 -4.3308926 -4.3157706 -4.2998939 -4.2911057 -4.28653 -4.2858887 -4.2900882 -4.29963 -4.3094249 -4.3140559 -4.3115473][-4.3302727 -4.3351078 -4.3408556 -4.3385324 -4.3294129 -4.3064165 -4.2833157 -4.2735286 -4.2707858 -4.2720108 -4.2783132 -4.2907181 -4.3051357 -4.3141613 -4.3151][-4.3383279 -4.3433595 -4.34669 -4.3391924 -4.3222566 -4.291667 -4.2613049 -4.2490683 -4.2451439 -4.2450352 -4.254231 -4.2717962 -4.2904124 -4.303473 -4.3075714][-4.3415713 -4.3483925 -4.3497996 -4.3378687 -4.313828 -4.2794108 -4.245522 -4.2302017 -4.2240405 -4.2208476 -4.2329869 -4.2549181 -4.2768316 -4.2941809 -4.3026819][-4.33582 -4.344346 -4.34441 -4.3286657 -4.30089 -4.2623787 -4.2263856 -4.2132335 -4.2113748 -4.2118139 -4.2279 -4.2514248 -4.2760372 -4.2957463 -4.3062162][-4.3282814 -4.3357549 -4.3321161 -4.3104987 -4.2770185 -4.2331624 -4.1964235 -4.186378 -4.1909657 -4.2023644 -4.2272782 -4.2564697 -4.2851734 -4.3061481 -4.3170261][-4.3263121 -4.3317795 -4.3200693 -4.2875371 -4.2431517 -4.19372 -4.1592789 -4.1545362 -4.167305 -4.1900039 -4.2246504 -4.2598906 -4.2900467 -4.3098803 -4.3217535][-4.327034 -4.3304887 -4.3114877 -4.2684865 -4.2122817 -4.1547914 -4.1218147 -4.12194 -4.144659 -4.1798468 -4.2225919 -4.2594037 -4.2865095 -4.3021512 -4.3129387][-4.3243861 -4.3279953 -4.3063931 -4.2590575 -4.1988144 -4.1381721 -4.105525 -4.104744 -4.1270266 -4.165482 -4.2127614 -4.2505522 -4.2733569 -4.2870793 -4.2960634][-4.3214769 -4.3280454 -4.3084769 -4.2637858 -4.2076354 -4.1509719 -4.1193466 -4.113708 -4.1227875 -4.1496654 -4.1910582 -4.2287607 -4.2539921 -4.269042 -4.2754526][-4.3211203 -4.3294363 -4.3150716 -4.2784505 -4.2318878 -4.1826959 -4.154563 -4.1471643 -4.1463103 -4.1550207 -4.1767526 -4.2002492 -4.2226605 -4.2393341 -4.2457023][-4.3251514 -4.3320141 -4.3217564 -4.2953496 -4.2597122 -4.2194061 -4.1917663 -4.18285 -4.1795444 -4.17908 -4.1848078 -4.1901846 -4.1981416 -4.2063603 -4.2069454][-4.3308277 -4.3351645 -4.3284078 -4.3100381 -4.2843609 -4.254086 -4.2288208 -4.21764 -4.214817 -4.21367 -4.2125096 -4.2094827 -4.2048388 -4.1984353 -4.1860271][-4.3361545 -4.3392315 -4.3354177 -4.3228049 -4.3053827 -4.2847419 -4.2656417 -4.25348 -4.2483258 -4.24666 -4.2435789 -4.236619 -4.2258496 -4.2124486 -4.1938305][-4.3405447 -4.3433433 -4.34226 -4.3342757 -4.322536 -4.3079987 -4.2932529 -4.2826719 -4.2767506 -4.2747011 -4.2722497 -4.2635098 -4.2501698 -4.2334332 -4.2145414]]...]
INFO - root - 2017-12-07 15:51:55.984590: step 10910, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.074 sec/batch; 41h:36m:23s remains)
INFO - root - 2017-12-07 15:52:17.081366: step 10920, loss = 2.07, batch loss = 2.02 (14.7 examples/sec; 2.180 sec/batch; 43h:43m:43s remains)
INFO - root - 2017-12-07 15:52:38.193894: step 10930, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 42h:12m:57s remains)
INFO - root - 2017-12-07 15:52:59.159948: step 10940, loss = 2.08, batch loss = 2.02 (15.6 examples/sec; 2.056 sec/batch; 41h:13m:32s remains)
INFO - root - 2017-12-07 15:53:20.300515: step 10950, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.078 sec/batch; 41h:39m:17s remains)
INFO - root - 2017-12-07 15:53:41.332660: step 10960, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.116 sec/batch; 42h:25m:21s remains)
INFO - root - 2017-12-07 15:54:02.438242: step 10970, loss = 2.09, batch loss = 2.03 (14.8 examples/sec; 2.160 sec/batch; 43h:17m:13s remains)
INFO - root - 2017-12-07 15:54:23.372827: step 10980, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.096 sec/batch; 41h:59m:53s remains)
INFO - root - 2017-12-07 15:54:44.555767: step 10990, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.145 sec/batch; 42h:59m:10s remains)
INFO - root - 2017-12-07 15:55:05.765783: step 11000, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 42h:55m:11s remains)
2017-12-07 15:55:07.305560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1420808 -4.15013 -4.1762 -4.1956844 -4.199338 -4.2011323 -4.2066402 -4.2132964 -4.2146325 -4.2170963 -4.2180834 -4.2200541 -4.230906 -4.241889 -4.2529297][-4.1184845 -4.1191797 -4.1477752 -4.1710014 -4.1763525 -4.1778836 -4.1803803 -4.1817374 -4.1775436 -4.177115 -4.174273 -4.1706982 -4.1817427 -4.2015495 -4.2232938][-4.1372719 -4.1306648 -4.147963 -4.1637516 -4.1672578 -4.1642876 -4.1581945 -4.151948 -4.1410866 -4.1354728 -4.1272764 -4.11617 -4.1259971 -4.1522217 -4.1844411][-4.1796288 -4.1688557 -4.1725211 -4.1793756 -4.1798263 -4.17067 -4.1562791 -4.1443739 -4.1285386 -4.1171446 -4.1041889 -4.0871744 -4.0901785 -4.1131611 -4.1480227][-4.2045331 -4.195395 -4.1981721 -4.2031426 -4.1979289 -4.1785417 -4.1562462 -4.1409454 -4.1236959 -4.1077452 -4.0946474 -4.0819621 -4.0837159 -4.1007724 -4.1303182][-4.1998377 -4.1977763 -4.2027049 -4.207191 -4.195601 -4.1649537 -4.1346512 -4.1199737 -4.1052933 -4.0873165 -4.0777922 -4.0798516 -4.0919261 -4.1099906 -4.1341825][-4.1798553 -4.1855445 -4.1910181 -4.1946073 -4.1772304 -4.1363811 -4.1011486 -4.091074 -4.083106 -4.0660214 -4.0607247 -4.0776138 -4.1061506 -4.1311665 -4.1528754][-4.167161 -4.1748319 -4.1755071 -4.1746278 -4.1515851 -4.1066761 -4.0726318 -4.0661116 -4.0618391 -4.0485663 -4.0502543 -4.0766678 -4.1153636 -4.1479549 -4.171124][-4.1641836 -4.1706295 -4.1669083 -4.1613312 -4.1374226 -4.0953288 -4.0646219 -4.0554338 -4.0532246 -4.0501552 -4.0606208 -4.0907197 -4.1295347 -4.1619806 -4.1820493][-4.1593714 -4.1659708 -4.1627488 -4.1565228 -4.1335812 -4.095818 -4.0682912 -4.0577617 -4.0627327 -4.0737815 -4.09 -4.1173 -4.149899 -4.1752291 -4.1855717][-4.1495681 -4.1574154 -4.1586328 -4.1540151 -4.1319165 -4.0980897 -4.0723238 -4.0624466 -4.0777864 -4.101963 -4.1190758 -4.1379075 -4.160831 -4.1768761 -4.1750522][-4.1319871 -4.1366062 -4.1431937 -4.1424665 -4.1291628 -4.1054821 -4.0862274 -4.0783677 -4.0980225 -4.1307249 -4.1476092 -4.1605597 -4.175354 -4.1804638 -4.1677713][-4.1195307 -4.1250014 -4.1372585 -4.14386 -4.137691 -4.12483 -4.1151071 -4.1122513 -4.1341133 -4.1698003 -4.1845956 -4.1923485 -4.2001519 -4.1963367 -4.1761608][-4.1377358 -4.1437097 -4.1552482 -4.1604762 -4.1539569 -4.1465073 -4.1440511 -4.1473179 -4.1688757 -4.2039089 -4.2204061 -4.2263927 -4.23108 -4.2249203 -4.2016525][-4.1739821 -4.174768 -4.1777825 -4.1779647 -4.1702762 -4.1675954 -4.1700287 -4.1764684 -4.1940508 -4.2233958 -4.2420788 -4.2530489 -4.2586827 -4.2512269 -4.2267861]]...]
INFO - root - 2017-12-07 15:55:28.433607: step 11010, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.130 sec/batch; 42h:40m:02s remains)
INFO - root - 2017-12-07 15:55:49.765983: step 11020, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.145 sec/batch; 42h:57m:45s remains)
INFO - root - 2017-12-07 15:56:10.944018: step 11030, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 42h:08m:16s remains)
INFO - root - 2017-12-07 15:56:31.923904: step 11040, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.161 sec/batch; 43h:15m:58s remains)
INFO - root - 2017-12-07 15:56:53.037532: step 11050, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.088 sec/batch; 41h:47m:49s remains)
INFO - root - 2017-12-07 15:57:14.203400: step 11060, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 42h:28m:18s remains)
INFO - root - 2017-12-07 15:57:35.086283: step 11070, loss = 2.08, batch loss = 2.02 (16.3 examples/sec; 1.963 sec/batch; 39h:17m:53s remains)
INFO - root - 2017-12-07 15:57:56.479667: step 11080, loss = 2.09, batch loss = 2.03 (14.5 examples/sec; 2.203 sec/batch; 44h:04m:59s remains)
INFO - root - 2017-12-07 15:58:17.824420: step 11090, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.157 sec/batch; 43h:10m:06s remains)
INFO - root - 2017-12-07 15:58:38.898658: step 11100, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.094 sec/batch; 41h:53m:15s remains)
2017-12-07 15:58:40.454881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.250176 -4.25924 -4.2753739 -4.2916856 -4.3053036 -4.3134613 -4.3124042 -4.3007684 -4.2840285 -4.26839 -4.2562532 -4.2511859 -4.2543426 -4.2647843 -4.2692118][-4.1867466 -4.2030506 -4.2254467 -4.2468119 -4.2650251 -4.2777905 -4.279232 -4.267508 -4.24909 -4.2336888 -4.2247643 -4.2243137 -4.2321186 -4.2451992 -4.2485051][-4.118783 -4.1477785 -4.179904 -4.2046375 -4.2203135 -4.2311425 -4.2335424 -4.2271724 -4.2157521 -4.2108984 -4.212975 -4.2203703 -4.23071 -4.2396226 -4.2341809][-4.0604768 -4.0996823 -4.1443477 -4.1706462 -4.177866 -4.1767569 -4.1719413 -4.1683278 -4.1665277 -4.180316 -4.1997766 -4.2201972 -4.2347927 -4.2407331 -4.22692][-4.0482163 -4.0824213 -4.1228104 -4.1384668 -4.1322794 -4.11639 -4.094717 -4.0815964 -4.087636 -4.1223278 -4.1592813 -4.1950488 -4.2220321 -4.2352338 -4.2259641][-4.09473 -4.1116872 -4.1368542 -4.1336827 -4.1047387 -4.0665975 -4.01786 -3.9823763 -3.9906502 -4.0435476 -4.0982428 -4.1482663 -4.1901441 -4.2170763 -4.2213082][-4.166204 -4.1709089 -4.1850753 -4.1686521 -4.1240411 -4.067523 -3.9996467 -3.9430561 -3.9443426 -4.0017166 -4.0659418 -4.1227927 -4.1713495 -4.2061834 -4.2215204][-4.2160454 -4.2185903 -4.2246261 -4.2039013 -4.1622386 -4.1142812 -4.0627165 -4.015028 -4.008256 -4.0437512 -4.09438 -4.1431394 -4.1836448 -4.21228 -4.2273803][-4.2318897 -4.2363796 -4.2411118 -4.2242327 -4.1959572 -4.1707807 -4.14822 -4.1235313 -4.1214623 -4.1397705 -4.16622 -4.1931095 -4.2143078 -4.2294726 -4.2358208][-4.235177 -4.2437735 -4.2497916 -4.2407718 -4.2270861 -4.2190652 -4.2130966 -4.20526 -4.2066331 -4.2156782 -4.2261806 -4.2367754 -4.2449303 -4.248858 -4.2468457][-4.235898 -4.241365 -4.249249 -4.2528744 -4.2549663 -4.2594719 -4.2626877 -4.2617607 -4.2643108 -4.2674203 -4.2692819 -4.271637 -4.2741008 -4.2719455 -4.2647333][-4.2460632 -4.2436709 -4.24487 -4.2508297 -4.2637696 -4.2788839 -4.2911763 -4.2969384 -4.2993026 -4.2970676 -4.2920556 -4.2904482 -4.2901244 -4.2850752 -4.2771459][-4.2518754 -4.242382 -4.2340412 -4.2345891 -4.2505832 -4.2717466 -4.2870717 -4.2948895 -4.2926478 -4.2819839 -4.270607 -4.2678137 -4.2687554 -4.26764 -4.2659321][-4.2469249 -4.2351208 -4.2211409 -4.2135139 -4.2258372 -4.2473955 -4.2576938 -4.2542453 -4.2406974 -4.2244182 -4.2101932 -4.2073507 -4.21429 -4.2255735 -4.2370234][-4.2364883 -4.2267461 -4.2146597 -4.2054543 -4.2135153 -4.2293143 -4.2302094 -4.2135191 -4.1899996 -4.1741762 -4.1638422 -4.161572 -4.1747947 -4.1958408 -4.2168813]]...]
INFO - root - 2017-12-07 15:59:01.708205: step 11110, loss = 2.06, batch loss = 2.00 (14.4 examples/sec; 2.218 sec/batch; 44h:22m:05s remains)
INFO - root - 2017-12-07 15:59:22.958986: step 11120, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 42h:44m:47s remains)
INFO - root - 2017-12-07 15:59:43.978792: step 11130, loss = 2.08, batch loss = 2.02 (16.3 examples/sec; 1.961 sec/batch; 39h:13m:13s remains)
INFO - root - 2017-12-07 16:00:04.820394: step 11140, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.132 sec/batch; 42h:37m:46s remains)
INFO - root - 2017-12-07 16:00:25.986781: step 11150, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 42h:38m:02s remains)
INFO - root - 2017-12-07 16:00:47.103438: step 11160, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.097 sec/batch; 41h:54m:52s remains)
INFO - root - 2017-12-07 16:01:08.054730: step 11170, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.130 sec/batch; 42h:34m:14s remains)
INFO - root - 2017-12-07 16:01:29.290522: step 11180, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 42h:48m:33s remains)
INFO - root - 2017-12-07 16:01:50.551163: step 11190, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 42h:01m:31s remains)
INFO - root - 2017-12-07 16:02:11.626124: step 11200, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 42h:39m:16s remains)
2017-12-07 16:02:13.152864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3100967 -4.311861 -4.3099179 -4.3030314 -4.2974887 -4.2911487 -4.2814212 -4.2707553 -4.267066 -4.2695465 -4.2674031 -4.25784 -4.2434454 -4.237103 -4.2399693][-4.2979045 -4.3026209 -4.30506 -4.2985282 -4.2909784 -4.2783504 -4.2614336 -4.2479696 -4.2415657 -4.2398081 -4.2286844 -4.2122312 -4.2003179 -4.2034316 -4.2169232][-4.2810268 -4.2865963 -4.2891793 -4.2830644 -4.2735887 -4.2567306 -4.2368813 -4.2230406 -4.2173753 -4.2120986 -4.1893916 -4.1574059 -4.142662 -4.161057 -4.1924667][-4.2572007 -4.2610178 -4.2589374 -4.2524924 -4.243855 -4.2269926 -4.2059736 -4.1942177 -4.1954927 -4.1906996 -4.1587553 -4.1163135 -4.1006565 -4.1314521 -4.1796694][-4.230371 -4.2252703 -4.2130818 -4.2051148 -4.1969347 -4.1769285 -4.1549363 -4.1464682 -4.1591854 -4.1614661 -4.1322293 -4.0941224 -4.0884523 -4.1290312 -4.1875973][-4.1930132 -4.1715875 -4.1465092 -4.1342149 -4.1237831 -4.0973139 -4.0711718 -4.070591 -4.100172 -4.11847 -4.1044278 -4.0847535 -4.0965319 -4.14354 -4.2017779][-4.1476231 -4.1078706 -4.0711074 -4.0576458 -4.0460043 -4.0028105 -3.96442 -3.9778581 -4.0291772 -4.0677361 -4.0744162 -4.0714111 -4.0974541 -4.1494832 -4.2071609][-4.0929713 -4.0332723 -3.9882483 -3.9878459 -3.9822969 -3.918941 -3.8553267 -3.8792117 -3.9597261 -4.0158606 -4.0353618 -4.0432777 -4.082273 -4.1422529 -4.2040296][-4.028007 -3.958704 -3.9227142 -3.9439943 -3.9549189 -3.8863225 -3.8102362 -3.833406 -3.9209192 -3.9758768 -3.9891064 -3.9994907 -4.0524564 -4.1243854 -4.1915126][-3.9891686 -3.9278269 -3.9111164 -3.9492898 -3.9732568 -3.92894 -3.8742144 -3.889493 -3.9471693 -3.9762449 -3.9744165 -3.9814146 -4.0397587 -4.1177788 -4.1880503][-4.0065441 -3.9623425 -3.9572959 -3.9971437 -4.0277047 -4.008863 -3.9778013 -3.9844766 -4.0155249 -4.025135 -4.0159383 -4.020402 -4.0740032 -4.1473379 -4.212379][-4.0616937 -4.0397735 -4.0422974 -4.0725284 -4.0994163 -4.0918622 -4.0688868 -4.0676222 -4.0879307 -4.0983276 -4.0953321 -4.100184 -4.1446109 -4.2034221 -4.2542782][-4.1437545 -4.1394696 -4.1459203 -4.1645756 -4.182951 -4.1780329 -4.1588941 -4.1542292 -4.1699147 -4.1845632 -4.1893849 -4.195282 -4.2269092 -4.2657423 -4.2987189][-4.2223911 -4.2274523 -4.2350173 -4.2457705 -4.2536645 -4.2492385 -4.2360268 -4.2317414 -4.2416921 -4.255127 -4.2624912 -4.2676806 -4.2877192 -4.3101349 -4.3285027][-4.2778449 -4.286375 -4.29176 -4.2959132 -4.29589 -4.291872 -4.28566 -4.2827916 -4.2864342 -4.2955813 -4.3028512 -4.3079634 -4.3200121 -4.3324943 -4.3424339]]...]
INFO - root - 2017-12-07 16:02:34.196704: step 11210, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.125 sec/batch; 42h:27m:24s remains)
INFO - root - 2017-12-07 16:02:55.483749: step 11220, loss = 2.08, batch loss = 2.02 (15.7 examples/sec; 2.043 sec/batch; 40h:48m:12s remains)
INFO - root - 2017-12-07 16:03:16.400217: step 11230, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.131 sec/batch; 42h:32m:56s remains)
INFO - root - 2017-12-07 16:03:37.637225: step 11240, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 42h:15m:56s remains)
INFO - root - 2017-12-07 16:03:58.957479: step 11250, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.126 sec/batch; 42h:26m:23s remains)
INFO - root - 2017-12-07 16:04:20.139612: step 11260, loss = 2.09, batch loss = 2.03 (14.8 examples/sec; 2.156 sec/batch; 43h:02m:20s remains)
INFO - root - 2017-12-07 16:04:41.566149: step 11270, loss = 2.06, batch loss = 2.00 (14.4 examples/sec; 2.215 sec/batch; 44h:12m:21s remains)
INFO - root - 2017-12-07 16:05:02.903135: step 11280, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.077 sec/batch; 41h:27m:26s remains)
INFO - root - 2017-12-07 16:05:24.107366: step 11290, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.113 sec/batch; 42h:09m:55s remains)
INFO - root - 2017-12-07 16:05:44.973865: step 11300, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.125 sec/batch; 42h:24m:08s remains)
2017-12-07 16:05:46.553087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3381348 -4.336525 -4.3398018 -4.34102 -4.3333287 -4.3202744 -4.3080821 -4.3008885 -4.3011217 -4.3119802 -4.3187943 -4.3217978 -4.3211179 -4.317596 -4.3072591][-4.33543 -4.3315997 -4.3329449 -4.3315392 -4.3202744 -4.3016996 -4.284914 -4.2726274 -4.2737107 -4.2854452 -4.2925172 -4.2982354 -4.2985687 -4.2929425 -4.2811346][-4.3299947 -4.3266611 -4.3280687 -4.32106 -4.3023119 -4.2725835 -4.2434206 -4.2273588 -4.2351503 -4.2524624 -4.2575483 -4.2590947 -4.2563515 -4.2515612 -4.2449217][-4.3258672 -4.3244114 -4.3252149 -4.3090334 -4.2796464 -4.2343392 -4.1910243 -4.1720695 -4.1923733 -4.2161388 -4.2157493 -4.2067924 -4.198833 -4.1958714 -4.1969767][-4.3238153 -4.3230858 -4.3213506 -4.2959394 -4.2518535 -4.1888132 -4.12576 -4.0959969 -4.1291471 -4.1688843 -4.1719704 -4.1638103 -4.1591454 -4.1568685 -4.1582608][-4.3203874 -4.3172078 -4.3116913 -4.2795043 -4.2198648 -4.1318417 -4.0380154 -3.97049 -4.0069385 -4.0860052 -4.1183805 -4.1365523 -4.148365 -4.1470656 -4.1408639][-4.3159947 -4.3106556 -4.3019676 -4.26515 -4.1926379 -4.080379 -3.9450791 -3.8068049 -3.8232191 -3.9680836 -4.0570383 -4.1149321 -4.15064 -4.15065 -4.1364226][-4.3103828 -4.3042426 -4.2940931 -4.257051 -4.1789665 -4.0581756 -3.902931 -3.7120624 -3.6961412 -3.8856916 -4.0224619 -4.1112165 -4.1624413 -4.1674781 -4.1500807][-4.3013682 -4.2956829 -4.2889042 -4.2566233 -4.1907487 -4.0927844 -3.9695961 -3.8195622 -3.7979915 -3.9381289 -4.0580268 -4.139719 -4.18803 -4.1939163 -4.1779933][-4.2935505 -4.2877932 -4.2870626 -4.2667394 -4.2241631 -4.1633658 -4.0876575 -3.9973755 -3.9788296 -4.0571432 -4.1333632 -4.1881318 -4.2194157 -4.2245717 -4.2142906][-4.2923083 -4.2853513 -4.2889709 -4.28208 -4.2604303 -4.2292728 -4.188139 -4.1406479 -4.1295767 -4.1710477 -4.2114854 -4.2400265 -4.2553105 -4.2586074 -4.2572002][-4.2974238 -4.28889 -4.2923031 -4.2942185 -4.2870555 -4.2736983 -4.2543774 -4.2306266 -4.2265487 -4.2490687 -4.2648335 -4.2716541 -4.2742991 -4.2780366 -4.2829022][-4.3048282 -4.2951641 -4.2969179 -4.3013053 -4.3027382 -4.3002448 -4.2943521 -4.2857008 -4.2826676 -4.2945142 -4.2986794 -4.2935452 -4.2874179 -4.2857289 -4.2870126][-4.31331 -4.3037715 -4.303247 -4.3066206 -4.3114924 -4.314815 -4.3167076 -4.3138156 -4.3119369 -4.3185444 -4.3182254 -4.3091531 -4.2989144 -4.2877049 -4.28054][-4.3227291 -4.3134465 -4.3099 -4.3097935 -4.3134823 -4.3179054 -4.3211164 -4.3211889 -4.3211589 -4.3252988 -4.3249993 -4.3167038 -4.3054514 -4.2928524 -4.2823091]]...]
INFO - root - 2017-12-07 16:06:07.582709: step 11310, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.156 sec/batch; 43h:00m:19s remains)
INFO - root - 2017-12-07 16:06:28.930403: step 11320, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 42h:27m:10s remains)
INFO - root - 2017-12-07 16:06:49.677383: step 11330, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 2.079 sec/batch; 41h:28m:05s remains)
INFO - root - 2017-12-07 16:07:10.856937: step 11340, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.080 sec/batch; 41h:28m:03s remains)
INFO - root - 2017-12-07 16:07:32.048638: step 11350, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 41h:51m:26s remains)
INFO - root - 2017-12-07 16:07:52.872996: step 11360, loss = 2.08, batch loss = 2.02 (16.2 examples/sec; 1.976 sec/batch; 39h:23m:24s remains)
INFO - root - 2017-12-07 16:08:14.135287: step 11370, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.126 sec/batch; 42h:23m:04s remains)
INFO - root - 2017-12-07 16:08:35.476103: step 11380, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 41h:55m:59s remains)
INFO - root - 2017-12-07 16:08:56.416419: step 11390, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.111 sec/batch; 42h:03m:31s remains)
INFO - root - 2017-12-07 16:09:17.331498: step 11400, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.144 sec/batch; 42h:42m:57s remains)
2017-12-07 16:09:18.848451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3286028 -4.3367009 -4.337429 -4.336761 -4.3360682 -4.3350973 -4.3348918 -4.3345947 -4.3337593 -4.3325562 -4.3304734 -4.32719 -4.3261595 -4.3306756 -4.3396072][-4.3141112 -4.3241696 -4.3269329 -4.3294425 -4.3331461 -4.3364849 -4.3389492 -4.33985 -4.3395786 -4.3374166 -4.3337612 -4.3276472 -4.3224597 -4.3241439 -4.3329062][-4.2945228 -4.3019705 -4.3022156 -4.3036008 -4.3087597 -4.3149977 -4.3183975 -4.3209443 -4.3242059 -4.3262205 -4.3258581 -4.3211021 -4.3123727 -4.3092518 -4.315968][-4.263938 -4.2624087 -4.2538509 -4.2481561 -4.2505741 -4.255661 -4.2564778 -4.2597942 -4.2704272 -4.282073 -4.2898765 -4.2908316 -4.2814932 -4.2744927 -4.2805986][-4.2202587 -4.204977 -4.1841927 -4.1681252 -4.1626883 -4.1590257 -4.1480227 -4.1497269 -4.1737156 -4.2027755 -4.2252088 -4.2365222 -4.2312207 -4.2242846 -4.2344146][-4.1714997 -4.1408591 -4.1066642 -4.0791097 -4.0637445 -4.0453615 -4.0117154 -4.0048208 -4.0449314 -4.0958772 -4.1362677 -4.1634245 -4.1705141 -4.1710434 -4.188468][-4.1406145 -4.0953054 -4.0453959 -4.0016394 -3.9712958 -3.9311969 -3.8671131 -3.8494072 -3.9148896 -3.9922211 -4.04724 -4.0874252 -4.1101708 -4.1235251 -4.1502876][-4.1518497 -4.1038671 -4.0493832 -3.9951887 -3.9461403 -3.8786969 -3.7815681 -3.7460845 -3.82915 -3.9248614 -3.9877684 -4.0361104 -4.076685 -4.10657 -4.1409531][-4.1891336 -4.15756 -4.1206617 -4.0785694 -4.0304379 -3.9617617 -3.8705366 -3.8294415 -3.8829346 -3.9530964 -3.9999046 -4.0445766 -4.0923 -4.1299434 -4.1653566][-4.21681 -4.2061224 -4.1956382 -4.1791253 -4.1524806 -4.1075339 -4.0491114 -4.0180812 -4.0344276 -4.0604062 -4.0751324 -4.1009307 -4.1385865 -4.1719327 -4.2025552][-4.2115855 -4.2180648 -4.22969 -4.2377672 -4.2372966 -4.2209005 -4.194191 -4.1766429 -4.1744132 -4.172801 -4.1649442 -4.1687589 -4.1856403 -4.2055426 -4.2278209][-4.1780005 -4.1962576 -4.2234774 -4.2490206 -4.2702508 -4.2762918 -4.27127 -4.2648454 -4.2578058 -4.2451658 -4.2263741 -4.2141027 -4.2124023 -4.2181444 -4.2329507][-4.156117 -4.1760826 -4.2061257 -4.238297 -4.2739291 -4.295166 -4.30475 -4.3063426 -4.300405 -4.2854142 -4.2626367 -4.2410464 -4.2290287 -4.2279572 -4.2389536][-4.1924868 -4.2039423 -4.2246041 -4.2508693 -4.2867093 -4.3113294 -4.3247976 -4.3295946 -4.3269386 -4.3161478 -4.2983193 -4.2781997 -4.2642274 -4.2603559 -4.2668924][-4.2628732 -4.2681146 -4.2783318 -4.2947221 -4.3207731 -4.3389053 -4.3482103 -4.3508859 -4.3500943 -4.3443966 -4.3337469 -4.3204384 -4.310533 -4.3063412 -4.3084226]]...]
INFO - root - 2017-12-07 16:09:40.062679: step 11410, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.121 sec/batch; 42h:15m:34s remains)
INFO - root - 2017-12-07 16:10:00.999965: step 11420, loss = 2.08, batch loss = 2.03 (16.1 examples/sec; 1.992 sec/batch; 39h:40m:33s remains)
INFO - root - 2017-12-07 16:10:22.002429: step 11430, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.072 sec/batch; 41h:16m:06s remains)
INFO - root - 2017-12-07 16:10:43.200232: step 11440, loss = 2.08, batch loss = 2.03 (15.3 examples/sec; 2.096 sec/batch; 41h:43m:40s remains)
INFO - root - 2017-12-07 16:11:04.513304: step 11450, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.174 sec/batch; 43h:17m:09s remains)
INFO - root - 2017-12-07 16:11:25.379148: step 11460, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.121 sec/batch; 42h:13m:38s remains)
INFO - root - 2017-12-07 16:11:46.685781: step 11470, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.095 sec/batch; 41h:41m:51s remains)
INFO - root - 2017-12-07 16:12:08.107425: step 11480, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.164 sec/batch; 43h:03m:31s remains)
INFO - root - 2017-12-07 16:12:28.764410: step 11490, loss = 2.06, batch loss = 2.01 (15.9 examples/sec; 2.012 sec/batch; 40h:01m:49s remains)
INFO - root - 2017-12-07 16:12:49.961610: step 11500, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 42h:18m:01s remains)
2017-12-07 16:12:51.560173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3273854 -4.3338008 -4.3403945 -4.339354 -4.3263969 -4.3083234 -4.2931046 -4.2846642 -4.288569 -4.3020768 -4.316762 -4.3240461 -4.3217249 -4.3098783 -4.2866654][-4.3304868 -4.3364711 -4.3418217 -4.3389068 -4.323195 -4.2995377 -4.2759027 -4.2600446 -4.2632375 -4.2806048 -4.3009238 -4.3108964 -4.3094563 -4.2952461 -4.2699862][-4.3330903 -4.3370948 -4.3381271 -4.3299303 -4.3086028 -4.2775121 -4.2436314 -4.2244244 -4.2298803 -4.2499619 -4.2711635 -4.2814631 -4.2789373 -4.2616611 -4.2362947][-4.335979 -4.3371787 -4.3319087 -4.3168511 -4.2879958 -4.2457843 -4.2024021 -4.1832781 -4.192522 -4.2142048 -4.2376084 -4.2475653 -4.2411451 -4.218277 -4.1920214][-4.3381481 -4.335866 -4.32364 -4.2995539 -4.2587929 -4.2031426 -4.154954 -4.1406431 -4.1582003 -4.1883569 -4.2169423 -4.2257433 -4.2106533 -4.1773872 -4.1488752][-4.3374004 -4.3306236 -4.3096528 -4.27353 -4.2181215 -4.1565456 -4.116642 -4.1151195 -4.1463833 -4.1860166 -4.2139316 -4.2157292 -4.1883497 -4.1475363 -4.123198][-4.3336086 -4.3212872 -4.2918143 -4.2431674 -4.1772733 -4.1204414 -4.0980878 -4.1124225 -4.1559153 -4.1980071 -4.2225189 -4.2163248 -4.1822047 -4.1417022 -4.1244631][-4.3282514 -4.3103642 -4.273632 -4.2154875 -4.1489291 -4.1046777 -4.0990467 -4.1278415 -4.1755533 -4.2141519 -4.234067 -4.2237315 -4.1939392 -4.162425 -4.1562653][-4.3256273 -4.3046589 -4.263176 -4.2034688 -4.1454554 -4.1155047 -4.122735 -4.1596041 -4.2038321 -4.2330461 -4.2474985 -4.2392683 -4.2192049 -4.2004104 -4.2049007][-4.3287644 -4.30872 -4.2707744 -4.2194519 -4.1768594 -4.159924 -4.173624 -4.2062812 -4.2374067 -4.2554026 -4.2657604 -4.2642407 -4.253047 -4.2422109 -4.2487612][-4.3352518 -4.3199062 -4.29108 -4.2540641 -4.227716 -4.2196226 -4.2310467 -4.252347 -4.27021 -4.2799859 -4.2889228 -4.2927365 -4.2872539 -4.2790327 -4.2811975][-4.3397942 -4.327424 -4.3046756 -4.2782216 -4.2610388 -4.2538004 -4.2585492 -4.2712927 -4.2824612 -4.2909222 -4.3016744 -4.3107667 -4.3109875 -4.3043766 -4.3020954][-4.3402205 -4.3264356 -4.3049593 -4.2805948 -4.2624412 -4.248167 -4.244009 -4.2532783 -4.2672734 -4.2801604 -4.2952852 -4.3101282 -4.3167224 -4.3145943 -4.3122535][-4.3359838 -4.3170552 -4.2913232 -4.2625513 -4.237247 -4.2137389 -4.2035246 -4.2146544 -4.2387271 -4.2617359 -4.2831955 -4.3043766 -4.3160691 -4.3184252 -4.3167725][-4.325377 -4.2987747 -4.2659192 -4.2307429 -4.1966033 -4.1649218 -4.1542573 -4.1728096 -4.2094059 -4.2445993 -4.275434 -4.3030934 -4.3183765 -4.320137 -4.3157072]]...]
INFO - root - 2017-12-07 16:13:12.770233: step 11510, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.127 sec/batch; 42h:19m:17s remains)
INFO - root - 2017-12-07 16:13:33.540829: step 11520, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.150 sec/batch; 42h:45m:19s remains)
INFO - root - 2017-12-07 16:13:54.708091: step 11530, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.113 sec/batch; 42h:01m:00s remains)
INFO - root - 2017-12-07 16:14:15.869234: step 11540, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 42h:20m:20s remains)
INFO - root - 2017-12-07 16:14:37.074567: step 11550, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.103 sec/batch; 41h:48m:45s remains)
INFO - root - 2017-12-07 16:14:58.070724: step 11560, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 42h:14m:18s remains)
INFO - root - 2017-12-07 16:15:19.129603: step 11570, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.077 sec/batch; 41h:17m:16s remains)
2017-12-07 16:15:22.234662: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 921.38MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
INFO - root - 2017-12-07 16:15:40.402244: step 11580, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.117 sec/batch; 42h:04m:41s remains)
INFO - root - 2017-12-07 16:16:01.349466: step 11590, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.116 sec/batch; 42h:02m:39s remains)
INFO - root - 2017-12-07 16:16:22.642408: step 11600, loss = 2.06, batch loss = 2.00 (14.6 examples/sec; 2.189 sec/batch; 43h:29m:44s remains)
2017-12-07 16:16:24.294123: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.267786 -4.2647247 -4.27977 -4.2948337 -4.2998586 -4.2912893 -4.2738004 -4.2505207 -4.2323937 -4.2311416 -4.2520118 -4.2773 -4.2884059 -4.2809348 -4.2604046][-4.2986774 -4.2972069 -4.2998347 -4.3009667 -4.2942553 -4.277215 -4.2481666 -4.2135553 -4.1960211 -4.2062955 -4.2356052 -4.2604165 -4.2707763 -4.2577929 -4.2281866][-4.2996278 -4.3024368 -4.2992554 -4.2931914 -4.2816119 -4.2573838 -4.2158146 -4.1676521 -4.1500793 -4.1760297 -4.2146244 -4.2376204 -4.2449412 -4.2294912 -4.1989717][-4.2795191 -4.2871184 -4.2835488 -4.2751055 -4.2617073 -4.2306972 -4.17433 -4.1105723 -4.0919466 -4.1382818 -4.191967 -4.2155166 -4.218699 -4.2033143 -4.1768513][-4.2565928 -4.270247 -4.267261 -4.2544212 -4.238049 -4.2005477 -4.1327834 -4.0557747 -4.0369835 -4.1071353 -4.1787338 -4.2035465 -4.202127 -4.1844769 -4.159512][-4.22304 -4.2407541 -4.2403078 -4.227437 -4.2128325 -4.1734657 -4.0963354 -3.9986665 -3.9743295 -4.0713806 -4.1603165 -4.1908965 -4.1882329 -4.1681914 -4.1435404][-4.1881957 -4.2107086 -4.2111764 -4.2016382 -4.1897922 -4.1500807 -4.0647039 -3.9453466 -3.9052808 -4.0268164 -4.132781 -4.1736116 -4.1774516 -4.1620045 -4.1399193][-4.1832457 -4.2052212 -4.2036748 -4.1966944 -4.1885967 -4.1562095 -4.0809646 -3.9729197 -3.9329553 -4.041389 -4.1430511 -4.1876044 -4.1953378 -4.1841311 -4.1659083][-4.2159848 -4.2331471 -4.22477 -4.2146211 -4.2046938 -4.1811523 -4.1280689 -4.0501232 -4.0202971 -4.0973949 -4.1818056 -4.2232232 -4.2337265 -4.2278895 -4.2152047][-4.2516618 -4.2661147 -4.2496672 -4.2330813 -4.2211761 -4.2060637 -4.1748295 -4.123703 -4.1004252 -4.1515079 -4.218266 -4.2550869 -4.2654 -4.2630744 -4.2558556][-4.2658162 -4.2786627 -4.2591009 -4.2408233 -4.2317543 -4.2281785 -4.2167888 -4.18935 -4.1756296 -4.2079268 -4.2517352 -4.276536 -4.2840052 -4.2814641 -4.2767][-4.2613215 -4.2692332 -4.2491217 -4.2329092 -4.23108 -4.2394934 -4.2437921 -4.2367029 -4.2312155 -4.2489948 -4.2724576 -4.2863379 -4.2902865 -4.2882833 -4.2867026][-4.2617774 -4.2581758 -4.2380419 -4.2276697 -4.234591 -4.2523861 -4.2655683 -4.2702341 -4.2682624 -4.2732325 -4.2817535 -4.2868495 -4.2893758 -4.2920356 -4.2928925][-4.2643881 -4.2508512 -4.2314038 -4.2276497 -4.2408409 -4.2615833 -4.275919 -4.2845469 -4.2850947 -4.28267 -4.2791929 -4.2777638 -4.2800136 -4.2860312 -4.2875824][-4.2607279 -4.2424936 -4.2258124 -4.2264419 -4.2443748 -4.2665186 -4.2818317 -4.2908483 -4.2893405 -4.2797718 -4.2644663 -4.2553639 -4.2561541 -4.2635827 -4.2674017]]...]
INFO - root - 2017-12-07 16:16:45.559985: step 11610, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.075 sec/batch; 41h:13m:38s remains)
INFO - root - 2017-12-07 16:17:06.521628: step 11620, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 42h:24m:47s remains)
INFO - root - 2017-12-07 16:17:27.924879: step 11630, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.182 sec/batch; 43h:20m:13s remains)
INFO - root - 2017-12-07 16:17:49.056132: step 11640, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.177 sec/batch; 43h:14m:12s remains)
INFO - root - 2017-12-07 16:18:09.847452: step 11650, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 1.913 sec/batch; 37h:59m:21s remains)
INFO - root - 2017-12-07 16:18:30.924070: step 11660, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.090 sec/batch; 41h:29m:33s remains)
INFO - root - 2017-12-07 16:18:51.991541: step 11670, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.140 sec/batch; 42h:28m:42s remains)
INFO - root - 2017-12-07 16:19:13.158285: step 11680, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 42h:05m:46s remains)
INFO - root - 2017-12-07 16:19:34.250138: step 11690, loss = 2.08, batch loss = 2.03 (15.3 examples/sec; 2.096 sec/batch; 41h:35m:26s remains)
INFO - root - 2017-12-07 16:19:55.443789: step 11700, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 2.053 sec/batch; 40h:44m:07s remains)
2017-12-07 16:19:57.055788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2180457 -4.2280693 -4.2377205 -4.2469025 -4.2564573 -4.2636042 -4.2661858 -4.2637143 -4.2596498 -4.2572732 -4.2549748 -4.2498021 -4.2412491 -4.2279296 -4.200562][-4.2098961 -4.2220383 -4.2302446 -4.2375975 -4.24609 -4.2522249 -4.2547455 -4.2519522 -4.244998 -4.2399879 -4.2372236 -4.2325368 -4.22422 -4.2089987 -4.1767321][-4.2271748 -4.240304 -4.24697 -4.2527165 -4.257647 -4.2589841 -4.2560906 -4.2485614 -4.2372389 -4.2304068 -4.2278209 -4.2237587 -4.2176867 -4.2044191 -4.1736507][-4.2802463 -4.2884879 -4.2879829 -4.2856903 -4.2811656 -4.275629 -4.2670808 -4.25288 -4.2383652 -4.2311153 -4.2300229 -4.228024 -4.2254333 -4.2208409 -4.2047157][-4.3250265 -4.32122 -4.3093042 -4.2955685 -4.2815194 -4.2684011 -4.252068 -4.2312579 -4.2184119 -4.2151232 -4.21697 -4.2205396 -4.225903 -4.2352142 -4.2406425][-4.3346739 -4.3151326 -4.2903032 -4.2673359 -4.2467418 -4.2225814 -4.1878343 -4.1549 -4.1506419 -4.1614733 -4.1732531 -4.1856337 -4.2027669 -4.2304907 -4.2538385][-4.3072662 -4.2727194 -4.2328959 -4.2010012 -4.1763611 -4.139235 -4.0783238 -4.028245 -4.0525374 -4.0949407 -4.1192479 -4.1369362 -4.1630192 -4.2067494 -4.239635][-4.2398047 -4.1987739 -4.1446137 -4.1004729 -4.0697293 -4.0171528 -3.9226737 -3.8531711 -3.924099 -4.0153818 -4.0571556 -4.0822172 -4.11781 -4.1670275 -4.1933575][-4.1641383 -4.129622 -4.0750136 -4.0287905 -4.0032263 -3.9533346 -3.8591576 -3.802052 -3.8946285 -3.9974122 -4.0396357 -4.0641313 -4.0998921 -4.1371422 -4.1470647][-4.1433067 -4.1240473 -4.0844889 -4.0477581 -4.0356374 -4.0093665 -3.954838 -3.9308617 -4.0017056 -4.0737295 -4.0975409 -4.1088834 -4.1314864 -4.147491 -4.1412592][-4.1798573 -4.1684766 -4.1426978 -4.1193371 -4.1145616 -4.1061025 -4.0853624 -4.0826812 -4.1264195 -4.1659279 -4.1735697 -4.1737428 -4.1835585 -4.1818085 -4.1665058][-4.2323027 -4.224421 -4.2118464 -4.2012391 -4.1997943 -4.1980534 -4.1949334 -4.200716 -4.2255855 -4.2441034 -4.2447948 -4.2415538 -4.2419486 -4.2311678 -4.2132711][-4.2819033 -4.2779117 -4.2737169 -4.2735214 -4.2752542 -4.2751665 -4.2773142 -4.2825117 -4.29246 -4.2975607 -4.2956557 -4.2937613 -4.2905707 -4.2796483 -4.2667379][-4.3169079 -4.317327 -4.3173528 -4.3205495 -4.3238058 -4.3274679 -4.332324 -4.3330822 -4.3306713 -4.3274188 -4.3241491 -4.3215785 -4.318644 -4.3130541 -4.3082404][-4.3301535 -4.3314347 -4.3315554 -4.3353076 -4.3395085 -4.3448181 -4.3496647 -4.3485136 -4.343071 -4.3372374 -4.3327522 -4.3292212 -4.3258734 -4.3238306 -4.3244991]]...]
INFO - root - 2017-12-07 16:20:18.134261: step 11710, loss = 2.07, batch loss = 2.02 (16.3 examples/sec; 1.968 sec/batch; 39h:01m:58s remains)
INFO - root - 2017-12-07 16:20:39.198066: step 11720, loss = 2.06, batch loss = 2.01 (15.3 examples/sec; 2.090 sec/batch; 41h:26m:58s remains)
2017-12-07 16:20:46.631931: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 921.38MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
INFO - root - 2017-12-07 16:21:00.441832: step 11730, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 2.213 sec/batch; 43h:52m:42s remains)
INFO - root - 2017-12-07 16:21:21.708136: step 11740, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 42h:49m:18s remains)
INFO - root - 2017-12-07 16:21:42.709466: step 11750, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 42h:03m:44s remains)
INFO - root - 2017-12-07 16:22:04.084813: step 11760, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.149 sec/batch; 42h:35m:37s remains)
INFO - root - 2017-12-07 16:22:25.126576: step 11770, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.116 sec/batch; 41h:56m:21s remains)
INFO - root - 2017-12-07 16:22:46.079504: step 11780, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.100 sec/batch; 41h:37m:32s remains)
INFO - root - 2017-12-07 16:23:07.219363: step 11790, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.139 sec/batch; 42h:22m:50s remains)
INFO - root - 2017-12-07 16:23:28.531222: step 11800, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.166 sec/batch; 42h:54m:19s remains)
2017-12-07 16:23:30.121532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2841353 -4.2494664 -4.223299 -4.2115483 -4.2139816 -4.2189689 -4.2249594 -4.2271366 -4.2310839 -4.227078 -4.2241421 -4.218833 -4.2122765 -4.2068586 -4.2118273][-4.2681518 -4.2203689 -4.1865745 -4.1734896 -4.1774373 -4.1869221 -4.199079 -4.2052541 -4.2100253 -4.209856 -4.207273 -4.1995378 -4.1868978 -4.1780086 -4.1868544][-4.2460356 -4.1872334 -4.1454482 -4.1278462 -4.13894 -4.1593819 -4.1802607 -4.1893859 -4.1954293 -4.1967664 -4.186202 -4.1671033 -4.1503758 -4.14807 -4.1649895][-4.2200055 -4.1574364 -4.1074066 -4.0841126 -4.1022468 -4.135623 -4.1657348 -4.1775331 -4.1837673 -4.1847639 -4.1574173 -4.1177034 -4.0961852 -4.1111016 -4.1412115][-4.1957183 -4.1309757 -4.0758929 -4.0502338 -4.0756612 -4.1150894 -4.1496267 -4.1661892 -4.1730695 -4.1692467 -4.1245832 -4.0613327 -4.0347514 -4.070507 -4.1173792][-4.1738124 -4.1075921 -4.0547767 -4.0314646 -4.0578504 -4.0964918 -4.1284351 -4.1476526 -4.1582518 -4.1541328 -4.0950937 -4.0106306 -3.9768462 -4.0293517 -4.0947313][-4.1561322 -4.0870161 -4.0440092 -4.0359793 -4.0675859 -4.1043172 -4.1282172 -4.1429996 -4.1541095 -4.1535192 -4.0937924 -4.0054388 -3.9670541 -4.0216961 -4.094409][-4.1635184 -4.0972147 -4.0617905 -4.0660167 -4.1053023 -4.1416059 -4.1555133 -4.1562924 -4.167078 -4.1746707 -4.1381836 -4.0694151 -4.0282135 -4.0606251 -4.12028][-4.1910992 -4.1320896 -4.0996137 -4.1029077 -4.1440048 -4.1795545 -4.1895661 -4.1858211 -4.1950979 -4.208869 -4.2010016 -4.1642613 -4.1250834 -4.1257529 -4.1590323][-4.220808 -4.1646919 -4.1300421 -4.1277 -4.1650429 -4.1969204 -4.2107978 -4.2124944 -4.2210579 -4.2389135 -4.249156 -4.2379475 -4.206965 -4.1879506 -4.1993489][-4.2434306 -4.1862063 -4.15047 -4.1489854 -4.1846862 -4.2158456 -4.23074 -4.239265 -4.2497134 -4.2679925 -4.2840667 -4.285789 -4.2627254 -4.2383285 -4.2350378][-4.2669454 -4.2126827 -4.1781044 -4.1761971 -4.2082853 -4.2386827 -4.2536678 -4.264904 -4.2756181 -4.2925158 -4.3105083 -4.3187819 -4.3009076 -4.2696104 -4.2556453][-4.29488 -4.250824 -4.2194505 -4.2146311 -4.2360764 -4.2599492 -4.2715912 -4.2804089 -4.2912536 -4.3081741 -4.3280382 -4.3404927 -4.3269157 -4.2922058 -4.270802][-4.3191195 -4.2883186 -4.2625475 -4.2528534 -4.2628841 -4.2789559 -4.286243 -4.2902923 -4.3009906 -4.3181486 -4.3384953 -4.3503113 -4.3371696 -4.3045335 -4.2843375][-4.331151 -4.311574 -4.2931519 -4.28134 -4.2817831 -4.2889366 -4.2904358 -4.2912855 -4.3018875 -4.3182755 -4.3369823 -4.346118 -4.3336997 -4.3085656 -4.2934427]]...]
INFO - root - 2017-12-07 16:23:51.021739: step 11810, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.118 sec/batch; 41h:57m:10s remains)
INFO - root - 2017-12-07 16:24:11.915449: step 11820, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.085 sec/batch; 41h:17m:25s remains)
INFO - root - 2017-12-07 16:24:32.991694: step 11830, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.106 sec/batch; 41h:42m:09s remains)
INFO - root - 2017-12-07 16:24:53.753124: step 11840, loss = 2.07, batch loss = 2.01 (17.1 examples/sec; 1.872 sec/batch; 37h:04m:22s remains)
INFO - root - 2017-12-07 16:25:14.795889: step 11850, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.079 sec/batch; 41h:10m:09s remains)
INFO - root - 2017-12-07 16:25:35.936902: step 11860, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 41h:59m:00s remains)
INFO - root - 2017-12-07 16:25:57.274128: step 11870, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.129 sec/batch; 42h:08m:21s remains)
INFO - root - 2017-12-07 16:26:18.272739: step 11880, loss = 2.08, batch loss = 2.03 (14.7 examples/sec; 2.176 sec/batch; 43h:04m:16s remains)
INFO - root - 2017-12-07 16:26:39.177829: step 11890, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.082 sec/batch; 41h:12m:01s remains)
INFO - root - 2017-12-07 16:27:00.453208: step 11900, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.108 sec/batch; 41h:42m:26s remains)
2017-12-07 16:27:02.078759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2897391 -4.29681 -4.2973247 -4.2995548 -4.3004847 -4.2965965 -4.2926345 -4.2886648 -4.2833796 -4.2763424 -4.2687335 -4.2533684 -4.2425847 -4.2287989 -4.206296][-4.2891889 -4.2902522 -4.2875566 -4.2875862 -4.2866569 -4.2801776 -4.2720923 -4.2653189 -4.2618723 -4.2569146 -4.2518744 -4.2382755 -4.2302318 -4.2231555 -4.2051568][-4.2895527 -4.2867293 -4.2799449 -4.2747293 -4.2662535 -4.2526765 -4.2386661 -4.2317905 -4.2334018 -4.2343726 -4.2350469 -4.2273 -4.2248721 -4.2228074 -4.2059331][-4.2845654 -4.2783051 -4.2688289 -4.2571321 -4.2377505 -4.211904 -4.1900945 -4.1848435 -4.1943979 -4.2049613 -4.2136512 -4.215085 -4.2204695 -4.2230191 -4.2066946][-4.2686915 -4.2615914 -4.2510943 -4.2325959 -4.2039628 -4.168704 -4.1419883 -4.1393123 -4.1556954 -4.1760025 -4.194479 -4.2062559 -4.2180185 -4.2233086 -4.2088671][-4.2516561 -4.2436676 -4.2318525 -4.2063847 -4.1732554 -4.1362252 -4.11007 -4.107861 -4.1282086 -4.1563773 -4.1832671 -4.2030864 -4.2190175 -4.2269859 -4.2144523][-4.2385473 -4.2300496 -4.2138896 -4.1831613 -4.151577 -4.1207519 -4.0959959 -4.0898538 -4.1070495 -4.1384544 -4.1720061 -4.1969166 -4.2190967 -4.2311416 -4.2195807][-4.224864 -4.2139716 -4.1966271 -4.1662574 -4.1423941 -4.1203113 -4.0989289 -4.0898271 -4.1030445 -4.1300778 -4.1624908 -4.1880083 -4.2130384 -4.2262497 -4.2124891][-4.2094769 -4.2023149 -4.1880531 -4.16158 -4.14635 -4.1364617 -4.1251082 -4.1197867 -4.1257839 -4.1408648 -4.1627107 -4.1784334 -4.1969991 -4.2054229 -4.1868858][-4.199048 -4.1997867 -4.1933918 -4.1731005 -4.1655774 -4.1664505 -4.1652231 -4.1655054 -4.1680055 -4.1774392 -4.1922803 -4.19718 -4.1980419 -4.188261 -4.1572304][-4.1977253 -4.20295 -4.2038651 -4.1890106 -4.1840234 -4.192205 -4.19768 -4.2021408 -4.2051535 -4.2112494 -4.2236886 -4.2258062 -4.2173057 -4.1921115 -4.153811][-4.2000356 -4.2090259 -4.2124267 -4.2013688 -4.1958427 -4.2043262 -4.2140355 -4.2220855 -4.2270465 -4.2316489 -4.2437606 -4.2444086 -4.2327495 -4.2063203 -4.1747575][-4.2092929 -4.2230186 -4.2330222 -4.2230716 -4.2105217 -4.2116513 -4.2184916 -4.227406 -4.2331896 -4.240325 -4.25205 -4.2517462 -4.23877 -4.2191577 -4.1974735][-4.2092094 -4.2207756 -4.2347245 -4.2318096 -4.2212477 -4.2203016 -4.2235613 -4.2298331 -4.2379384 -4.2429276 -4.2479744 -4.2468729 -4.2384706 -4.2234917 -4.2092204][-4.191987 -4.202692 -4.2203908 -4.2270284 -4.22651 -4.2310309 -4.2354259 -4.2409792 -4.2473059 -4.2477021 -4.2438164 -4.2401547 -4.235435 -4.2239285 -4.2145996]]...]
INFO - root - 2017-12-07 16:27:23.039817: step 11910, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.147 sec/batch; 42h:27m:47s remains)
INFO - root - 2017-12-07 16:27:44.170740: step 11920, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.135 sec/batch; 42h:13m:16s remains)
INFO - root - 2017-12-07 16:28:05.524002: step 11930, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 42h:07m:11s remains)
INFO - root - 2017-12-07 16:28:26.655250: step 11940, loss = 2.09, batch loss = 2.03 (15.4 examples/sec; 2.081 sec/batch; 41h:09m:05s remains)
INFO - root - 2017-12-07 16:28:47.781070: step 11950, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.164 sec/batch; 42h:47m:31s remains)
INFO - root - 2017-12-07 16:29:08.939391: step 11960, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 42h:09m:09s remains)
INFO - root - 2017-12-07 16:29:30.029274: step 11970, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 41h:34m:37s remains)
INFO - root - 2017-12-07 16:29:51.030888: step 11980, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.082 sec/batch; 41h:08m:13s remains)
INFO - root - 2017-12-07 16:30:12.285596: step 11990, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.159 sec/batch; 42h:39m:17s remains)
INFO - root - 2017-12-07 16:30:33.484620: step 12000, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.106 sec/batch; 41h:36m:22s remains)
2017-12-07 16:30:35.057477: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2660203 -4.2784023 -4.2942176 -4.2963204 -4.293622 -4.29187 -4.2916818 -4.2928562 -4.2923431 -4.2845554 -4.2675524 -4.2431135 -4.2221313 -4.2075095 -4.1950655][-4.2603021 -4.275948 -4.2953129 -4.2960129 -4.2930222 -4.2919407 -4.2921014 -4.293498 -4.2927756 -4.2844458 -4.26868 -4.2470131 -4.2274036 -4.2102604 -4.1924744][-4.2719169 -4.2861404 -4.3003559 -4.2935228 -4.2860579 -4.282949 -4.2823758 -4.2834883 -4.2840691 -4.2793603 -4.2689028 -4.2524071 -4.2363338 -4.2218518 -4.2060928][-4.2815285 -4.2904325 -4.294208 -4.277442 -4.2595458 -4.247416 -4.241189 -4.2415118 -4.2457762 -4.2525382 -4.2586875 -4.2564869 -4.2477417 -4.2359114 -4.222434][-4.2728958 -4.2759528 -4.2706909 -4.2449565 -4.2142053 -4.1853552 -4.16234 -4.1492705 -4.1544704 -4.182241 -4.2164636 -4.2394247 -4.2476296 -4.2458649 -4.2376118][-4.2471337 -4.2471704 -4.2368951 -4.2072339 -4.1668878 -4.1180682 -4.063395 -4.0144606 -4.0055838 -4.0543532 -4.1259432 -4.1816564 -4.2121515 -4.2254019 -4.2283483][-4.2064042 -4.2097316 -4.2034416 -4.1811681 -4.1446013 -4.0881109 -4.0108285 -3.9212656 -3.8777485 -3.9285047 -4.0207672 -4.0984864 -4.1468115 -4.176065 -4.1937809][-4.1975064 -4.2068024 -4.2125764 -4.205956 -4.1828609 -4.13853 -4.0725923 -3.9895225 -3.9346879 -3.9501595 -4.0097075 -4.0697947 -4.1140108 -4.1494679 -4.1776233][-4.2087107 -4.2198172 -4.2340722 -4.2386427 -4.2286105 -4.2047796 -4.1652112 -4.1127124 -4.0710092 -4.0588336 -4.0702939 -4.0883675 -4.1094766 -4.1375103 -4.1629663][-4.2151079 -4.2277732 -4.2473483 -4.2554092 -4.25044 -4.2374253 -4.216682 -4.1898532 -4.1664886 -4.1456537 -4.1282763 -4.1138911 -4.1123409 -4.1254997 -4.1403933][-4.2068186 -4.22264 -4.2463017 -4.2550049 -4.2529807 -4.2462254 -4.2364411 -4.2247891 -4.216538 -4.2003365 -4.1752028 -4.1467228 -4.127501 -4.12492 -4.1285305][-4.2094378 -4.225203 -4.2501011 -4.2569671 -4.2549415 -4.2507424 -4.2444258 -4.23775 -4.2363753 -4.2289987 -4.2103696 -4.1818519 -4.153986 -4.1371865 -4.1300392][-4.2266164 -4.2384562 -4.2572403 -4.2578363 -4.2512054 -4.2452397 -4.2385306 -4.2330475 -4.2334743 -4.2325253 -4.22443 -4.2037215 -4.178071 -4.15699 -4.1447253][-4.2438149 -4.2528563 -4.2648921 -4.2616382 -4.2521029 -4.2446151 -4.2369676 -4.2307916 -4.2304368 -4.2323403 -4.2341146 -4.2256956 -4.2098126 -4.1933813 -4.18102][-4.2421775 -4.2477279 -4.2543373 -4.251646 -4.2442703 -4.2369566 -4.2299819 -4.2240758 -4.2234545 -4.2276034 -4.2360697 -4.2387824 -4.2348661 -4.228168 -4.2195673]]...]
INFO - root - 2017-12-07 16:30:55.514923: step 12010, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 42h:18m:26s remains)
INFO - root - 2017-12-07 16:31:16.623854: step 12020, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.100 sec/batch; 41h:28m:11s remains)
INFO - root - 2017-12-07 16:31:38.035878: step 12030, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.153 sec/batch; 42h:31m:08s remains)
INFO - root - 2017-12-07 16:31:58.978179: step 12040, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.081 sec/batch; 41h:05m:35s remains)
INFO - root - 2017-12-07 16:32:20.135580: step 12050, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.096 sec/batch; 41h:22m:57s remains)
INFO - root - 2017-12-07 16:32:41.353182: step 12060, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.127 sec/batch; 41h:58m:41s remains)
INFO - root - 2017-12-07 16:33:02.289817: step 12070, loss = 2.08, batch loss = 2.02 (17.2 examples/sec; 1.859 sec/batch; 36h:41m:49s remains)
INFO - root - 2017-12-07 16:33:23.385008: step 12080, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.092 sec/batch; 41h:17m:02s remains)
INFO - root - 2017-12-07 16:33:44.594157: step 12090, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.148 sec/batch; 42h:22m:44s remains)
INFO - root - 2017-12-07 16:34:05.740927: step 12100, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.100 sec/batch; 41h:26m:16s remains)
2017-12-07 16:34:07.372113: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2562528 -4.2639403 -4.2704773 -4.2594204 -4.247005 -4.2494254 -4.2448525 -4.2351646 -4.2208385 -4.2089591 -4.2002254 -4.1978431 -4.2022429 -4.2141514 -4.2185][-4.235568 -4.2525496 -4.2678833 -4.2593155 -4.2411489 -4.2389336 -4.2297378 -4.2161593 -4.1995373 -4.1880031 -4.180387 -4.1754227 -4.1774664 -4.195425 -4.2064638][-4.2416582 -4.2657924 -4.2839122 -4.2759728 -4.2577982 -4.2500873 -4.2346168 -4.2202878 -4.2033119 -4.1906796 -4.1819372 -4.1720028 -4.1703115 -4.1863909 -4.2011957][-4.2402825 -4.2688766 -4.2850418 -4.2783527 -4.2637858 -4.2515283 -4.2313437 -4.2217836 -4.2114305 -4.2044454 -4.1980371 -4.1827397 -4.170722 -4.1798468 -4.1932845][-4.20983 -4.2368631 -4.2499809 -4.2428589 -4.2307768 -4.2149758 -4.1925354 -4.1829915 -4.1831174 -4.1882329 -4.1955924 -4.1888247 -4.1792903 -4.1821 -4.1908031][-4.178051 -4.200181 -4.2084436 -4.1994314 -4.183516 -4.1626797 -4.1362228 -4.1169386 -4.119566 -4.1392384 -4.1678915 -4.1833024 -4.1902986 -4.1977196 -4.2089438][-4.1599588 -4.1752696 -4.1775475 -4.16545 -4.1462564 -4.121408 -4.0869164 -4.0581226 -4.0616241 -4.096108 -4.1416254 -4.1724634 -4.1909723 -4.2043452 -4.221539][-4.1612511 -4.1685581 -4.1699729 -4.161448 -4.147059 -4.1246185 -4.09063 -4.0584474 -4.0581422 -4.089407 -4.1287246 -4.1551781 -4.1738391 -4.1914973 -4.214416][-4.1890931 -4.1888165 -4.1903377 -4.1896853 -4.1847529 -4.1731062 -4.151588 -4.1310763 -4.1307111 -4.1467285 -4.1556616 -4.1599584 -4.1653128 -4.1783719 -4.1985373][-4.2211671 -4.2197766 -4.2213621 -4.2234168 -4.2245412 -4.2191505 -4.2057595 -4.19272 -4.1920977 -4.195787 -4.181746 -4.1670232 -4.1574745 -4.1592474 -4.1750255][-4.2522264 -4.2537556 -4.2589498 -4.2619128 -4.2621832 -4.2575636 -4.2481232 -4.239748 -4.2390337 -4.2374978 -4.2173438 -4.1941948 -4.1710072 -4.1609678 -4.1682625][-4.2793088 -4.2828808 -4.2922058 -4.3004441 -4.3034048 -4.2958546 -4.2823477 -4.273469 -4.2703052 -4.2650585 -4.2486734 -4.2301464 -4.2041054 -4.1883259 -4.1885209][-4.2938485 -4.3008957 -4.3139081 -4.3269396 -4.3338976 -4.3250008 -4.3068652 -4.2937384 -4.2812605 -4.2704344 -4.2591529 -4.2477551 -4.2273664 -4.21165 -4.2108727][-4.2981424 -4.3057761 -4.3185143 -4.33254 -4.3425136 -4.335619 -4.3181868 -4.3029227 -4.2802057 -4.2577462 -4.2423811 -4.2334461 -4.22159 -4.2162919 -4.2227049][-4.3004355 -4.3070436 -4.3170605 -4.3254652 -4.3307695 -4.3223763 -4.3079505 -4.2925396 -4.2653728 -4.2352042 -4.21192 -4.2031579 -4.2002115 -4.2085061 -4.2257609]]...]
INFO - root - 2017-12-07 16:34:28.144274: step 12110, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.137 sec/batch; 42h:08m:51s remains)
INFO - root - 2017-12-07 16:34:49.567834: step 12120, loss = 2.08, batch loss = 2.02 (14.4 examples/sec; 2.215 sec/batch; 43h:41m:51s remains)
INFO - root - 2017-12-07 16:35:10.589096: step 12130, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 1.944 sec/batch; 38h:19m:54s remains)
INFO - root - 2017-12-07 16:35:31.436669: step 12140, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.079 sec/batch; 40h:59m:28s remains)
INFO - root - 2017-12-07 16:35:52.819147: step 12150, loss = 2.07, batch loss = 2.01 (14.3 examples/sec; 2.232 sec/batch; 43h:59m:46s remains)
INFO - root - 2017-12-07 16:36:13.989729: step 12160, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.081 sec/batch; 41h:00m:50s remains)
INFO - root - 2017-12-07 16:36:34.859281: step 12170, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 42h:05m:20s remains)
INFO - root - 2017-12-07 16:36:56.016003: step 12180, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.092 sec/batch; 41h:13m:58s remains)
INFO - root - 2017-12-07 16:37:17.324519: step 12190, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.112 sec/batch; 41h:37m:19s remains)
INFO - root - 2017-12-07 16:37:38.077953: step 12200, loss = 2.09, batch loss = 2.03 (15.6 examples/sec; 2.048 sec/batch; 40h:20m:24s remains)
2017-12-07 16:37:39.725446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2289882 -4.203907 -4.1825571 -4.1814871 -4.2067013 -4.2433958 -4.2756472 -4.2881074 -4.2813697 -4.2697477 -4.2559156 -4.2454562 -4.2406659 -4.2415085 -4.24305][-4.212657 -4.181159 -4.1573982 -4.1600728 -4.1913886 -4.2303576 -4.2623296 -4.2720566 -4.2615876 -4.247561 -4.2387819 -4.23743 -4.2362542 -4.2373762 -4.2345047][-4.1939578 -4.1590528 -4.1329246 -4.1407285 -4.1762824 -4.2109146 -4.2323351 -4.2337227 -4.2208204 -4.2118311 -4.2198691 -4.2365217 -4.2415719 -4.2409043 -4.2311258][-4.184907 -4.1459894 -4.1149597 -4.12567 -4.169189 -4.1991138 -4.2032504 -4.1931572 -4.177289 -4.1751351 -4.202745 -4.2379141 -4.2502527 -4.2478352 -4.2313547][-4.1973062 -4.153863 -4.1218905 -4.1313267 -4.1716151 -4.1881804 -4.1685715 -4.1351657 -4.1055865 -4.1130033 -4.1653128 -4.2180443 -4.2443686 -4.2515411 -4.2374783][-4.2334552 -4.1938562 -4.1645379 -4.1639085 -4.1811061 -4.1688747 -4.1129732 -4.0404911 -3.9829855 -4.0034709 -4.0975509 -4.17789 -4.2181587 -4.2383108 -4.2390146][-4.2644577 -4.2368021 -4.2131615 -4.2021909 -4.1945715 -4.1462469 -4.0416942 -3.9026871 -3.7903657 -3.8415475 -4.0069394 -4.1289859 -4.18661 -4.2194438 -4.2393007][-4.2745275 -4.2581921 -4.2429681 -4.2300348 -4.2095346 -4.1411428 -4.0123768 -3.8231828 -3.6605906 -3.7415676 -3.9562085 -4.0993528 -4.1665859 -4.2012281 -4.2310243][-4.2783508 -4.2711334 -4.2650933 -4.2522926 -4.2207785 -4.1511703 -4.040473 -3.8846059 -3.7620671 -3.8338196 -4.0003705 -4.1100569 -4.1560483 -4.1765065 -4.2065134][-4.2897062 -4.291779 -4.2929177 -4.2785153 -4.2379518 -4.1761065 -4.104023 -4.0203896 -3.9715617 -4.0178881 -4.09914 -4.1414075 -4.1408005 -4.1412258 -4.1667819][-4.3019567 -4.3117423 -4.3173418 -4.3035879 -4.2636685 -4.2094984 -4.1626949 -4.1277905 -4.121244 -4.1561756 -4.1862507 -4.1769209 -4.1439748 -4.1283579 -4.1448259][-4.3060961 -4.3218102 -4.3308029 -4.3195825 -4.2854033 -4.2429671 -4.2136226 -4.2031951 -4.2123461 -4.2374549 -4.2476249 -4.2167888 -4.1674814 -4.1417804 -4.1435022][-4.301352 -4.3204436 -4.3332391 -4.3249707 -4.2991161 -4.2709875 -4.2532778 -4.2509503 -4.2651844 -4.2870712 -4.2909408 -4.2582855 -4.2134395 -4.1870804 -4.1783352][-4.2927771 -4.3103743 -4.3231373 -4.3182492 -4.3035803 -4.28977 -4.2796841 -4.2792459 -4.2930255 -4.313859 -4.321362 -4.3011284 -4.2695522 -4.2468972 -4.2339044][-4.2873936 -4.29944 -4.3102098 -4.3108892 -4.30724 -4.3040218 -4.3008804 -4.3021646 -4.3108182 -4.32525 -4.3315239 -4.3230305 -4.3078079 -4.2973895 -4.2906065]]...]
INFO - root - 2017-12-07 16:38:00.884570: step 12210, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.186 sec/batch; 43h:03m:47s remains)
INFO - root - 2017-12-07 16:38:22.082697: step 12220, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 41h:14m:54s remains)
INFO - root - 2017-12-07 16:38:43.068147: step 12230, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.074 sec/batch; 40h:50m:21s remains)
INFO - root - 2017-12-07 16:39:03.871699: step 12240, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.131 sec/batch; 41h:58m:00s remains)
INFO - root - 2017-12-07 16:39:25.163091: step 12250, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 41h:23m:46s remains)
INFO - root - 2017-12-07 16:39:46.281892: step 12260, loss = 2.07, batch loss = 2.01 (15.9 examples/sec; 2.013 sec/batch; 39h:37m:04s remains)
INFO - root - 2017-12-07 16:40:07.481367: step 12270, loss = 2.06, batch loss = 2.00 (14.8 examples/sec; 2.164 sec/batch; 42h:35m:29s remains)
INFO - root - 2017-12-07 16:40:28.763770: step 12280, loss = 2.08, batch loss = 2.03 (14.9 examples/sec; 2.149 sec/batch; 42h:16m:58s remains)
INFO - root - 2017-12-07 16:40:50.059430: step 12290, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.095 sec/batch; 41h:13m:08s remains)
INFO - root - 2017-12-07 16:41:10.889575: step 12300, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.091 sec/batch; 41h:08m:15s remains)
2017-12-07 16:41:12.493858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.278316 -4.2592158 -4.2484612 -4.2464004 -4.2354031 -4.2116036 -4.194325 -4.213429 -4.2398229 -4.2599983 -4.2756438 -4.2879095 -4.3038492 -4.3130164 -4.3141174][-4.2522554 -4.2362514 -4.2261291 -4.2263889 -4.2165985 -4.1882315 -4.1660314 -4.179924 -4.2127924 -4.2406697 -4.2597446 -4.27358 -4.2845511 -4.29082 -4.2924838][-4.2201476 -4.2158751 -4.2136927 -4.2190304 -4.2167983 -4.1942759 -4.1690273 -4.1689153 -4.1989141 -4.2293096 -4.2506194 -4.2644715 -4.2699723 -4.270184 -4.2687497][-4.1792932 -4.1873012 -4.1959224 -4.2105055 -4.2177548 -4.20521 -4.1786637 -4.1642418 -4.1904054 -4.2215285 -4.2460828 -4.2618012 -4.2633057 -4.25771 -4.2518816][-4.1268692 -4.1455388 -4.1681247 -4.1934052 -4.2032065 -4.1883154 -4.1518474 -4.1228986 -4.15588 -4.1956782 -4.2275085 -4.2477932 -4.2505512 -4.2453222 -4.2416143][-4.0836935 -4.1128983 -4.1509156 -4.1811466 -4.1792941 -4.1424046 -4.0782766 -4.0309596 -4.0868516 -4.1518908 -4.197412 -4.2239313 -4.2333174 -4.2366452 -4.2415748][-4.0769191 -4.1154275 -4.1561022 -4.1798034 -4.1586494 -4.0929918 -3.9936466 -3.9225435 -4.00229 -4.1000705 -4.1658411 -4.2030139 -4.2207551 -4.2343407 -4.2487249][-4.0975628 -4.1388469 -4.1716557 -4.1837435 -4.149147 -4.0696154 -3.9529779 -3.8690434 -3.958637 -4.076725 -4.1570578 -4.2018161 -4.2242093 -4.240983 -4.25838][-4.1246638 -4.1639948 -4.189352 -4.18931 -4.1468563 -4.0729995 -3.9774563 -3.9131193 -3.989639 -4.0979533 -4.1769285 -4.2223053 -4.2440443 -4.2609272 -4.2776995][-4.1571412 -4.196013 -4.21514 -4.2044644 -4.1567521 -4.09497 -4.0318551 -3.9948709 -4.0528741 -4.1406407 -4.2102294 -4.2517018 -4.2717514 -4.2906013 -4.3091116][-4.1988149 -4.2355714 -4.2482567 -4.2316895 -4.1831179 -4.1292915 -4.0853248 -4.0620613 -4.1056242 -4.1752963 -4.2348132 -4.2725754 -4.2930884 -4.31559 -4.3365312][-4.2421503 -4.2712622 -4.2788815 -4.2607894 -4.21644 -4.1674023 -4.1294465 -4.1111512 -4.1454749 -4.2003255 -4.249836 -4.2827029 -4.3023887 -4.327189 -4.3491292][-4.2704325 -4.291625 -4.2979388 -4.2809877 -4.2444139 -4.2030611 -4.1699977 -4.1566391 -4.1831241 -4.22521 -4.2633438 -4.2894783 -4.3070307 -4.3299818 -4.3485093][-4.2836485 -4.2986073 -4.3050518 -4.2913761 -4.2640214 -4.2323647 -4.2064476 -4.1973524 -4.216495 -4.249084 -4.2794266 -4.3019085 -4.3177614 -4.3362446 -4.3490868][-4.2837176 -4.2956657 -4.3019 -4.2910457 -4.2703719 -4.2469468 -4.2293777 -4.224206 -4.23812 -4.2640004 -4.2901349 -4.3108459 -4.3253255 -4.3392887 -4.3482437]]...]
INFO - root - 2017-12-07 16:41:33.612592: step 12310, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.135 sec/batch; 41h:59m:36s remains)
INFO - root - 2017-12-07 16:41:54.675184: step 12320, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.107 sec/batch; 41h:26m:30s remains)
INFO - root - 2017-12-07 16:42:15.575115: step 12330, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.117 sec/batch; 41h:37m:39s remains)
INFO - root - 2017-12-07 16:42:36.713196: step 12340, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 41h:51m:04s remains)
INFO - root - 2017-12-07 16:42:57.909988: step 12350, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 41h:23m:02s remains)
INFO - root - 2017-12-07 16:43:18.922658: step 12360, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.067 sec/batch; 40h:37m:42s remains)
INFO - root - 2017-12-07 16:43:39.981280: step 12370, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.147 sec/batch; 42h:11m:41s remains)
INFO - root - 2017-12-07 16:44:01.334808: step 12380, loss = 2.07, batch loss = 2.02 (15.5 examples/sec; 2.058 sec/batch; 40h:26m:59s remains)
INFO - root - 2017-12-07 16:44:22.364406: step 12390, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.120 sec/batch; 41h:39m:51s remains)
INFO - root - 2017-12-07 16:44:43.439259: step 12400, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 41h:52m:54s remains)
2017-12-07 16:44:45.099040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2138 -4.1723738 -4.1268144 -4.1049037 -4.0958257 -4.1098843 -4.1642704 -4.2084455 -4.2107477 -4.2002525 -4.194705 -4.1915812 -4.1842604 -4.1749687 -4.1771283][-4.20542 -4.1612458 -4.1133156 -4.0873923 -4.0740352 -4.0880885 -4.1431446 -4.1929669 -4.1985168 -4.1862168 -4.1735659 -4.1632252 -4.1496058 -4.138629 -4.1430192][-4.1942945 -4.1469979 -4.0965719 -4.06876 -4.0582662 -4.076139 -4.1246438 -4.1645527 -4.1708169 -4.1598353 -4.1429687 -4.1261377 -4.1015649 -4.0856552 -4.08993][-4.1660571 -4.1073103 -4.0485668 -4.0204959 -4.0140209 -4.040112 -4.0883284 -4.1236892 -4.1339374 -4.123877 -4.1013584 -4.0831242 -4.0516534 -4.0348163 -4.0349069][-4.142849 -4.0791969 -4.0172014 -3.9859309 -3.9717796 -3.9946275 -4.0453405 -4.0839658 -4.1006651 -4.097352 -4.0844488 -4.0817113 -4.0598397 -4.0407734 -4.0331931][-4.1429429 -4.0865617 -4.0280371 -3.9898469 -3.9650943 -3.9770465 -4.0205507 -4.0511637 -4.0660129 -4.0692663 -4.07531 -4.1004777 -4.1067247 -4.099113 -4.0893726][-4.1506214 -4.0989146 -4.0338006 -3.9775882 -3.9340923 -3.9227948 -3.9461017 -3.9650126 -3.9751809 -3.9813108 -4.0091834 -4.063354 -4.0937819 -4.1074533 -4.1048656][-4.1521378 -4.1000514 -4.024601 -3.9371843 -3.8500583 -3.7999902 -3.8012059 -3.808619 -3.8151944 -3.8299253 -3.8839736 -3.9711413 -4.02389 -4.0607772 -4.0699744][-4.1530008 -4.1042194 -4.0308261 -3.9303284 -3.8170538 -3.7386007 -3.7289288 -3.7408857 -3.7545919 -3.7770777 -3.8385139 -3.93302 -3.9877255 -4.0279388 -4.048079][-4.1719203 -4.1366744 -4.0825877 -3.9989252 -3.89417 -3.819993 -3.8181624 -3.8405597 -3.8592155 -3.879149 -3.9211421 -3.9863217 -4.0125003 -4.0355582 -4.0533667][-4.21752 -4.1960449 -4.1653132 -4.1153722 -4.0446806 -3.992408 -3.9922643 -4.00814 -4.0231385 -4.0388193 -4.0601363 -4.0877461 -4.0901508 -4.0991774 -4.1153927][-4.2526884 -4.2404947 -4.2285275 -4.205986 -4.1668391 -4.1398621 -4.1418581 -4.14886 -4.155777 -4.1628947 -4.1734233 -4.1846576 -4.1783929 -4.1783 -4.1913095][-4.2706642 -4.264801 -4.2638955 -4.2570553 -4.2385392 -4.227828 -4.2321072 -4.2312827 -4.2295842 -4.2303991 -4.2357864 -4.2417917 -4.2358909 -4.2351747 -4.2449651][-4.2970638 -4.2926016 -4.2944036 -4.2914772 -4.2825127 -4.2817912 -4.2870789 -4.2857947 -4.2843909 -4.2835855 -4.2846713 -4.2872257 -4.2821107 -4.2819986 -4.2902389][-4.3274822 -4.3261876 -4.3269544 -4.3266053 -4.3245134 -4.3263597 -4.3297977 -4.3279333 -4.3254681 -4.3233776 -4.3225203 -4.3234992 -4.3214369 -4.3222718 -4.3282471]]...]
INFO - root - 2017-12-07 16:45:06.406229: step 12410, loss = 2.08, batch loss = 2.03 (14.8 examples/sec; 2.160 sec/batch; 42h:25m:38s remains)
INFO - root - 2017-12-07 16:45:27.530872: step 12420, loss = 2.09, batch loss = 2.03 (16.2 examples/sec; 1.977 sec/batch; 38h:50m:00s remains)
INFO - root - 2017-12-07 16:45:48.567209: step 12430, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.131 sec/batch; 41h:50m:27s remains)
INFO - root - 2017-12-07 16:46:09.773431: step 12440, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.140 sec/batch; 42h:00m:41s remains)
INFO - root - 2017-12-07 16:46:31.000844: step 12450, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.121 sec/batch; 41h:38m:48s remains)
INFO - root - 2017-12-07 16:46:51.806777: step 12460, loss = 2.05, batch loss = 1.99 (15.2 examples/sec; 2.106 sec/batch; 41h:20m:53s remains)
INFO - root - 2017-12-07 16:47:13.293410: step 12470, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 41h:11m:45s remains)
INFO - root - 2017-12-07 16:47:34.644306: step 12480, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.151 sec/batch; 42h:13m:07s remains)
INFO - root - 2017-12-07 16:47:55.632269: step 12490, loss = 2.07, batch loss = 2.01 (15.7 examples/sec; 2.035 sec/batch; 39h:56m:12s remains)
INFO - root - 2017-12-07 16:48:16.766996: step 12500, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.128 sec/batch; 41h:44m:19s remains)
2017-12-07 16:48:18.339219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3645182 -4.366096 -4.3669109 -4.3661427 -4.3633871 -4.3576536 -4.3503251 -4.345891 -4.3481474 -4.3563395 -4.3659244 -4.3729486 -4.3758078 -4.3751945 -4.3723416][-4.3623857 -4.3644385 -4.3655629 -4.3644733 -4.3591862 -4.3470025 -4.3318315 -4.3208189 -4.32064 -4.3320208 -4.3493361 -4.3664045 -4.377048 -4.3808427 -4.3788166][-4.3622522 -4.3647318 -4.3656898 -4.3626008 -4.3516312 -4.329102 -4.3018489 -4.2798848 -4.2721767 -4.2826872 -4.3051081 -4.33399 -4.3579621 -4.3725548 -4.377284][-4.3632236 -4.3651342 -4.3650618 -4.3580842 -4.3376679 -4.2997069 -4.2517042 -4.2123623 -4.1952348 -4.2064376 -4.2352533 -4.2765541 -4.3184161 -4.3501797 -4.3666072][-4.3579712 -4.360188 -4.3601527 -4.3491969 -4.3178244 -4.2628703 -4.1894054 -4.1264834 -4.0955625 -4.1059728 -4.1426821 -4.2006955 -4.2659197 -4.31954 -4.3503761][-4.346086 -4.3491464 -4.3494081 -4.3352356 -4.2945004 -4.2248454 -4.131618 -4.0488067 -4.0021343 -4.0031385 -4.0405116 -4.1133089 -4.2043729 -4.282661 -4.3302364][-4.3301687 -4.3343258 -4.3342023 -4.3164935 -4.2697606 -4.1939764 -4.0967145 -4.0128765 -3.9616535 -3.9570806 -3.9928672 -4.0711975 -4.1719737 -4.2600641 -4.315196][-4.31883 -4.325038 -4.32283 -4.3014765 -4.2531 -4.1813264 -4.0974975 -4.0319934 -3.9967482 -4.0026193 -4.0433474 -4.1161566 -4.2010093 -4.2717562 -4.3172712][-4.3139286 -4.3241382 -4.3219252 -4.2999105 -4.2548409 -4.1928282 -4.1247869 -4.0774612 -4.05974 -4.0773983 -4.1259022 -4.19298 -4.2566061 -4.3022885 -4.3308434][-4.3083315 -4.3228688 -4.324842 -4.3104444 -4.2780638 -4.2338352 -4.184525 -4.1521835 -4.1420064 -4.1589012 -4.2004619 -4.2541914 -4.2988782 -4.3253474 -4.3402991][-4.293016 -4.310411 -4.3189421 -4.3169055 -4.3028154 -4.2797661 -4.2524815 -4.2354422 -4.2299418 -4.2392578 -4.2644224 -4.297009 -4.3233056 -4.3365932 -4.3427429][-4.2828283 -4.2972479 -4.3079271 -4.31439 -4.3141637 -4.3075409 -4.2986722 -4.2952242 -4.295136 -4.2988176 -4.310216 -4.3250642 -4.3368917 -4.3413377 -4.3424096][-4.2909541 -4.2990327 -4.3053446 -4.3105488 -4.3150263 -4.3171334 -4.3169923 -4.3178244 -4.31939 -4.3212538 -4.3272939 -4.3349218 -4.3412538 -4.3427472 -4.3420415][-4.3146462 -4.3149767 -4.3146715 -4.3152647 -4.31891 -4.3239393 -4.3271823 -4.32778 -4.3261666 -4.3243451 -4.3261418 -4.3303952 -4.3361707 -4.3396392 -4.3409052][-4.3388977 -4.3356237 -4.3318624 -4.32951 -4.3305154 -4.3330307 -4.334619 -4.3340077 -4.3302522 -4.3257937 -4.3240819 -4.3251066 -4.3295565 -4.3338442 -4.3362274]]...]
INFO - root - 2017-12-07 16:48:39.597986: step 12510, loss = 2.06, batch loss = 2.00 (14.7 examples/sec; 2.184 sec/batch; 42h:50m:21s remains)
INFO - root - 2017-12-07 16:49:00.628082: step 12520, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.082 sec/batch; 40h:49m:49s remains)
INFO - root - 2017-12-07 16:49:21.518680: step 12530, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.076 sec/batch; 40h:43m:06s remains)
INFO - root - 2017-12-07 16:49:42.656377: step 12540, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 41h:12m:44s remains)
INFO - root - 2017-12-07 16:50:03.616960: step 12550, loss = 2.07, batch loss = 2.01 (15.8 examples/sec; 2.025 sec/batch; 39h:41m:36s remains)
INFO - root - 2017-12-07 16:50:24.624949: step 12560, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.096 sec/batch; 41h:05m:13s remains)
INFO - root - 2017-12-07 16:50:45.665000: step 12570, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.109 sec/batch; 41h:20m:18s remains)
INFO - root - 2017-12-07 16:51:06.901271: step 12580, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.130 sec/batch; 41h:43m:51s remains)
INFO - root - 2017-12-07 16:51:27.719133: step 12590, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.100 sec/batch; 41h:08m:18s remains)
INFO - root - 2017-12-07 16:51:49.011976: step 12600, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.092 sec/batch; 40h:58m:35s remains)
2017-12-07 16:51:50.466321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1887412 -4.202342 -4.2090473 -4.2049 -4.1951375 -4.1933351 -4.209095 -4.2179646 -4.22519 -4.217639 -4.1905694 -4.1681147 -4.1531491 -4.1573386 -4.17422][-4.1733561 -4.1913409 -4.1958389 -4.1845174 -4.1669617 -4.1604791 -4.1812143 -4.1880417 -4.1919265 -4.18925 -4.1732135 -4.1602745 -4.1498432 -4.1626492 -4.1853309][-4.1809883 -4.1980076 -4.2001009 -4.1830349 -4.156786 -4.1451173 -4.1689367 -4.1716795 -4.1631041 -4.1612067 -4.1595211 -4.159306 -4.1507621 -4.1671376 -4.1918674][-4.193625 -4.2072487 -4.2126884 -4.1939154 -4.1632309 -4.1524878 -4.1661887 -4.1535416 -4.1245317 -4.12386 -4.1419368 -4.1608543 -4.16104 -4.175889 -4.1990533][-4.1681676 -4.1851664 -4.2075319 -4.1930661 -4.162869 -4.1464562 -4.127912 -4.0858088 -4.0452366 -4.0525093 -4.0954671 -4.1397719 -4.1533861 -4.1688533 -4.196866][-4.0955791 -4.1292763 -4.1753354 -4.1715913 -4.1383286 -4.1044617 -4.0491219 -3.9804511 -3.9476125 -3.9753304 -4.0450196 -4.1153269 -4.146719 -4.1668053 -4.1965442][-4.0116167 -4.0661955 -4.1402369 -4.1551909 -4.1218443 -4.0702457 -3.9869983 -3.910574 -3.8943486 -3.9386795 -4.0202274 -4.0984244 -4.1456504 -4.1728683 -4.1991959][-3.9798646 -4.039958 -4.1245995 -4.1561708 -4.1335959 -4.0926766 -4.0234165 -3.9677138 -3.9572284 -3.9880877 -4.0470724 -4.1059456 -4.153338 -4.1819754 -4.2043743][-4.0370007 -4.0785294 -4.1485047 -4.1829891 -4.1825485 -4.1677222 -4.1289015 -4.10144 -4.0908947 -4.096694 -4.1205831 -4.150106 -4.1796184 -4.2037168 -4.2206459][-4.1358957 -4.155529 -4.1913805 -4.2159543 -4.229991 -4.2328773 -4.2103763 -4.1975789 -4.1920009 -4.18679 -4.1837535 -4.1859574 -4.1966381 -4.2133346 -4.2273555][-4.2290058 -4.2316079 -4.2404327 -4.2505546 -4.2593226 -4.2625332 -4.2486582 -4.2435856 -4.2416711 -4.235043 -4.2230773 -4.2140255 -4.2155275 -4.2268686 -4.23953][-4.2915697 -4.2872276 -4.2827711 -4.2805834 -4.2811465 -4.2815142 -4.2717972 -4.2660422 -4.2642074 -4.2556567 -4.2436132 -4.2348313 -4.2379861 -4.2485447 -4.2587509][-4.3222136 -4.3189845 -4.3128877 -4.3072033 -4.3035388 -4.2997651 -4.2905488 -4.2865629 -4.2848034 -4.2772446 -4.2693124 -4.264462 -4.2688646 -4.2770433 -4.2819462][-4.3334565 -4.3310928 -4.3274536 -4.3237128 -4.3185549 -4.31412 -4.3070469 -4.3046942 -4.3049269 -4.3011379 -4.2967181 -4.2962089 -4.2998481 -4.3054113 -4.3071241][-4.3274708 -4.3287821 -4.3282676 -4.3258681 -4.3236427 -4.3219528 -4.3191795 -4.3176522 -4.3177857 -4.3165293 -4.3150339 -4.3156738 -4.3182197 -4.3215756 -4.3218126]]...]
INFO - root - 2017-12-07 16:52:11.568734: step 12610, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.110 sec/batch; 41h:20m:21s remains)
INFO - root - 2017-12-07 16:52:32.512722: step 12620, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.146 sec/batch; 42h:01m:55s remains)
INFO - root - 2017-12-07 16:52:53.734121: step 12630, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.180 sec/batch; 42h:41m:11s remains)
INFO - root - 2017-12-07 16:53:14.921712: step 12640, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.155 sec/batch; 42h:11m:29s remains)
INFO - root - 2017-12-07 16:53:35.805531: step 12650, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 1.945 sec/batch; 38h:05m:08s remains)
INFO - root - 2017-12-07 16:53:57.025649: step 12660, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 41h:35m:01s remains)
INFO - root - 2017-12-07 16:54:18.303140: step 12670, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.127 sec/batch; 41h:37m:36s remains)
INFO - root - 2017-12-07 16:54:39.484180: step 12680, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.109 sec/batch; 41h:15m:48s remains)
INFO - root - 2017-12-07 16:55:00.429270: step 12690, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 41h:34m:44s remains)
INFO - root - 2017-12-07 16:55:21.737810: step 12700, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.158 sec/batch; 42h:12m:40s remains)
2017-12-07 16:55:23.333693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1447411 -4.2225404 -4.2730207 -4.2921205 -4.2905884 -4.275774 -4.2464967 -4.2159595 -4.205061 -4.2067862 -4.2071662 -4.2021012 -4.1832161 -4.1479974 -4.1241088][-4.1646895 -4.2328739 -4.2698097 -4.2807031 -4.2779822 -4.271522 -4.2545276 -4.2306061 -4.2192984 -4.2137289 -4.1968327 -4.1643796 -4.1281629 -4.0923138 -4.0731997][-4.2009358 -4.2468982 -4.2661853 -4.265008 -4.2562613 -4.2474856 -4.2322106 -4.2054939 -4.1849222 -4.1705666 -4.1428061 -4.1008658 -4.0643649 -4.0391545 -4.0355597][-4.2221575 -4.2509961 -4.2590322 -4.2460489 -4.2252064 -4.2066755 -4.1856856 -4.1525412 -4.1249418 -4.1061964 -4.0826216 -4.0503225 -4.0234728 -4.0142293 -4.030149][-4.2203584 -4.24226 -4.2440705 -4.2199707 -4.1849475 -4.1508808 -4.1228609 -4.0938497 -4.0753407 -4.0623503 -4.0553079 -4.0481315 -4.0410337 -4.0448952 -4.0696068][-4.1901641 -4.2166209 -4.217772 -4.1864209 -4.1386571 -4.0904946 -4.0574875 -4.04481 -4.0516624 -4.0583038 -4.0684986 -4.0819831 -4.0979471 -4.1185913 -4.1434374][-4.1478324 -4.1808105 -4.1871233 -4.1557245 -4.0994787 -4.0401812 -4.005919 -4.0148244 -4.0505519 -4.0824275 -4.1048026 -4.1255736 -4.1539388 -4.1851339 -4.2061157][-4.1189055 -4.1548376 -4.1628008 -4.132822 -4.0754161 -4.0108476 -3.9798186 -4.0061007 -4.0590773 -4.1066589 -4.1399431 -4.164206 -4.1924767 -4.2211981 -4.2345252][-4.13031 -4.1650305 -4.1664243 -4.1301155 -4.072113 -4.0109744 -3.9860406 -4.0216646 -4.0791283 -4.13061 -4.1705117 -4.1948891 -4.2154031 -4.2338943 -4.2357836][-4.1780715 -4.2077312 -4.2018642 -4.1618619 -4.1048155 -4.051403 -4.0300694 -4.0615029 -4.1140442 -4.1616082 -4.2003322 -4.2221975 -4.2285857 -4.2291193 -4.2203441][-4.2335811 -4.2519164 -4.2406378 -4.2032137 -4.1530991 -4.1121216 -4.0990834 -4.1230359 -4.15981 -4.1924253 -4.2226624 -4.2356296 -4.2264013 -4.2122626 -4.1969581][-4.2687683 -4.2745357 -4.2591863 -4.2280736 -4.1881657 -4.1623073 -4.160955 -4.17766 -4.1982164 -4.2129278 -4.2281075 -4.2270169 -4.206758 -4.1887035 -4.1747131][-4.2743187 -4.2705045 -4.2587547 -4.242291 -4.2181892 -4.2052755 -4.2112532 -4.2232451 -4.2291636 -4.2231379 -4.2137165 -4.1981382 -4.1752386 -4.1600533 -4.1516337][-4.250802 -4.2404556 -4.2379971 -4.2432127 -4.2389145 -4.2375059 -4.2462258 -4.2533131 -4.2464881 -4.2204041 -4.1910844 -4.1660705 -4.1438127 -4.1330042 -4.1328869][-4.2174139 -4.2069678 -4.2178545 -4.2408776 -4.2532177 -4.2588196 -4.2661157 -4.2660851 -4.24899 -4.2116222 -4.1721311 -4.1429548 -4.121707 -4.1158371 -4.1245151]]...]
INFO - root - 2017-12-07 16:55:44.449788: step 12710, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 1.947 sec/batch; 38h:05m:09s remains)
INFO - root - 2017-12-07 16:56:05.468507: step 12720, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.144 sec/batch; 41h:55m:21s remains)
INFO - root - 2017-12-07 16:56:26.853961: step 12730, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 41h:36m:55s remains)
INFO - root - 2017-12-07 16:56:48.049392: step 12740, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.104 sec/batch; 41h:07m:42s remains)
INFO - root - 2017-12-07 16:57:08.954732: step 12750, loss = 2.07, batch loss = 2.02 (14.5 examples/sec; 2.212 sec/batch; 43h:14m:53s remains)
INFO - root - 2017-12-07 16:57:30.142533: step 12760, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.087 sec/batch; 40h:46m:58s remains)
INFO - root - 2017-12-07 16:57:51.618950: step 12770, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.170 sec/batch; 42h:23m:55s remains)
INFO - root - 2017-12-07 16:58:12.628417: step 12780, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 41h:43m:35s remains)
INFO - root - 2017-12-07 16:58:33.945410: step 12790, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 2.202 sec/batch; 43h:00m:45s remains)
INFO - root - 2017-12-07 16:58:55.165592: step 12800, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.185 sec/batch; 42h:41m:23s remains)
2017-12-07 16:58:56.697580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2483158 -4.2500629 -4.2539396 -4.2587342 -4.2636919 -4.2686472 -4.2725692 -4.2742405 -4.2749133 -4.2768497 -4.2791176 -4.2797923 -4.2796788 -4.2784286 -4.2758965][-4.2453585 -4.2469082 -4.2501326 -4.2527261 -4.2539506 -4.2537923 -4.252214 -4.2490253 -4.2463112 -4.2480545 -4.2517 -4.2552891 -4.2589836 -4.2591486 -4.2565165][-4.2517934 -4.2521281 -4.2532234 -4.2517886 -4.2464485 -4.2374291 -4.2271256 -4.2187395 -4.2140326 -4.2157478 -4.2196679 -4.2255979 -4.2329116 -4.23418 -4.2311339][-4.2555075 -4.2556438 -4.2555232 -4.2509031 -4.2384067 -4.2192245 -4.1981015 -4.1863337 -4.1852856 -4.1910214 -4.1952715 -4.2018156 -4.2127991 -4.2166758 -4.21491][-4.2481127 -4.2474046 -4.2451229 -4.2361012 -4.2183971 -4.1909242 -4.1578736 -4.1453853 -4.1574912 -4.1737986 -4.1794949 -4.18333 -4.1943841 -4.2005782 -4.2009754][-4.2115755 -4.2071896 -4.2014003 -4.1882396 -4.1680098 -4.1337843 -4.0863914 -4.0711269 -4.1033669 -4.1398664 -4.1536903 -4.1585345 -4.1712136 -4.1800332 -4.1805673][-4.1633596 -4.1495867 -4.1363478 -4.1183472 -4.0939121 -4.048048 -3.9812405 -3.9588907 -4.0143042 -4.0793085 -4.1130567 -4.1303806 -4.1488204 -4.15753 -4.1542253][-4.1310081 -4.105413 -4.0831442 -4.0609474 -4.035737 -3.9849696 -3.9070766 -3.8788373 -3.952975 -4.0433273 -4.0989814 -4.1301494 -4.1500325 -4.1510363 -4.1393037][-4.1215682 -4.0864506 -4.0598555 -4.0430937 -4.0321169 -4.0018663 -3.9452415 -3.9243445 -3.9900949 -4.0723143 -4.1269221 -4.1599464 -4.1755428 -4.1708393 -4.1562562][-4.1288285 -4.098516 -4.0804386 -4.07846 -4.0876646 -4.0835028 -4.0556817 -4.0448527 -4.0877805 -4.1436214 -4.1844807 -4.2102985 -4.2197576 -4.2126522 -4.2017741][-4.1505485 -4.1328044 -4.1265635 -4.1351633 -4.1545849 -4.1637282 -4.1525626 -4.1482244 -4.1727495 -4.2056336 -4.2324109 -4.2514071 -4.259438 -4.2555251 -4.2492938][-4.1872563 -4.180233 -4.1816363 -4.1928291 -4.2119293 -4.2228413 -4.2190533 -4.2165408 -4.22889 -4.2465773 -4.262774 -4.2755766 -4.282351 -4.2819753 -4.2789087][-4.2320991 -4.229866 -4.2329612 -4.2414765 -4.2548304 -4.2638807 -4.2638326 -4.2613893 -4.2660284 -4.2738376 -4.2829671 -4.2904959 -4.2940125 -4.2935529 -4.2906203][-4.2655177 -4.2656627 -4.2686048 -4.2743487 -4.2827568 -4.2886105 -4.2888641 -4.2858605 -4.2858148 -4.288475 -4.2935934 -4.2977133 -4.2992043 -4.2980895 -4.2944808][-4.282896 -4.2837763 -4.2861834 -4.2897959 -4.2943811 -4.297152 -4.2966509 -4.2935333 -4.2925816 -4.2940769 -4.2978168 -4.3001776 -4.3008571 -4.2991037 -4.2953544]]...]
INFO - root - 2017-12-07 16:59:17.571465: step 12810, loss = 2.08, batch loss = 2.03 (16.1 examples/sec; 1.988 sec/batch; 38h:49m:44s remains)
INFO - root - 2017-12-07 16:59:38.816744: step 12820, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 41h:05m:31s remains)
INFO - root - 2017-12-07 17:00:00.056789: step 12830, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.101 sec/batch; 41h:01m:31s remains)
INFO - root - 2017-12-07 17:00:21.249731: step 12840, loss = 2.08, batch loss = 2.02 (15.6 examples/sec; 2.046 sec/batch; 39h:57m:00s remains)
INFO - root - 2017-12-07 17:00:42.304739: step 12850, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.087 sec/batch; 40h:44m:25s remains)
INFO - root - 2017-12-07 17:01:03.456054: step 12860, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.095 sec/batch; 40h:53m:58s remains)
INFO - root - 2017-12-07 17:01:24.715941: step 12870, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.152 sec/batch; 41h:59m:25s remains)
INFO - root - 2017-12-07 17:01:45.549643: step 12880, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.175 sec/batch; 42h:26m:16s remains)
INFO - root - 2017-12-07 17:02:06.833607: step 12890, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.084 sec/batch; 40h:39m:11s remains)
INFO - root - 2017-12-07 17:02:28.128680: step 12900, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.157 sec/batch; 42h:04m:18s remains)
2017-12-07 17:02:29.671394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3241882 -4.3304334 -4.3284431 -4.3098087 -4.2795424 -4.2408895 -4.2084141 -4.1935616 -4.1963825 -4.2229133 -4.2619543 -4.2844486 -4.29432 -4.2989688 -4.301723][-4.3176451 -4.3234773 -4.312376 -4.28043 -4.2390208 -4.1920977 -4.1483922 -4.1236467 -4.1357174 -4.1831479 -4.2364345 -4.2712688 -4.2853985 -4.2875357 -4.2882423][-4.3072782 -4.3079553 -4.2843833 -4.2370429 -4.1826315 -4.1338534 -4.08815 -4.0649638 -4.0933766 -4.1641068 -4.2304678 -4.2734876 -4.2861857 -4.2803168 -4.2739077][-4.2967987 -4.2867804 -4.2474084 -4.182189 -4.1220284 -4.0853086 -4.06225 -4.064476 -4.1104245 -4.186851 -4.2499108 -4.2861977 -4.2896409 -4.273191 -4.2562494][-4.2875223 -4.2663531 -4.208478 -4.1201105 -4.0569305 -4.05372 -4.0813832 -4.1161962 -4.1643424 -4.222981 -4.2644615 -4.2823629 -4.2692118 -4.2398028 -4.2162571][-4.27274 -4.2434187 -4.1661644 -4.0516396 -3.9828 -4.0211411 -4.1036673 -4.16224 -4.1974669 -4.2281084 -4.2432513 -4.2449226 -4.222672 -4.1855288 -4.1618586][-4.2552085 -4.2249727 -4.1348457 -4.0001035 -3.926764 -4.0003829 -4.1209693 -4.1916933 -4.2090611 -4.2109327 -4.1976209 -4.1851907 -4.1690292 -4.1454673 -4.1350527][-4.24393 -4.2213497 -4.1401525 -4.0215955 -3.9641623 -4.036509 -4.1490769 -4.2096982 -4.2091703 -4.1851096 -4.1456494 -4.1199675 -4.1197071 -4.1303215 -4.1438575][-4.2517476 -4.2381492 -4.181066 -4.1022468 -4.0660906 -4.1102133 -4.1826415 -4.2156248 -4.2002749 -4.1600342 -4.1014624 -4.0660348 -4.0792203 -4.1195354 -4.1499114][-4.2758956 -4.26318 -4.2288947 -4.1849065 -4.1597905 -4.1723495 -4.2055283 -4.21551 -4.1953468 -4.1527987 -4.0922384 -4.0595527 -4.0783448 -4.1253443 -4.1590118][-4.3093162 -4.2977691 -4.2764244 -4.2503595 -4.2266092 -4.2158875 -4.2219076 -4.2188153 -4.1997895 -4.1652155 -4.1225667 -4.1058292 -4.126699 -4.1652822 -4.1945453][-4.3417792 -4.3315859 -4.3141832 -4.2933817 -4.2683105 -4.24749 -4.2410903 -4.229425 -4.210392 -4.1873012 -4.1651034 -4.165565 -4.1895528 -4.217279 -4.2382364][-4.36238 -4.3513842 -4.3323655 -4.3121696 -4.2864337 -4.2633939 -4.2528896 -4.2406564 -4.2273769 -4.216764 -4.211586 -4.2215872 -4.2461834 -4.2666254 -4.2816663][-4.3686948 -4.3540235 -4.3301029 -4.3097467 -4.2823091 -4.2590518 -4.2514429 -4.2483459 -4.2464991 -4.2493258 -4.2559724 -4.2710862 -4.2928176 -4.3071961 -4.31575][-4.3650427 -4.3460717 -4.3203773 -4.2997808 -4.27362 -4.2553411 -4.2551403 -4.2598715 -4.2666388 -4.2771068 -4.2909479 -4.3071437 -4.3255677 -4.3337679 -4.3341141]]...]
INFO - root - 2017-12-07 17:02:50.630322: step 12910, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.134 sec/batch; 41h:36m:46s remains)
INFO - root - 2017-12-07 17:03:11.843198: step 12920, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.137 sec/batch; 41h:40m:58s remains)
INFO - root - 2017-12-07 17:03:32.974519: step 12930, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.174 sec/batch; 42h:23m:40s remains)
INFO - root - 2017-12-07 17:03:53.651295: step 12940, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.074 sec/batch; 40h:26m:04s remains)
INFO - root - 2017-12-07 17:04:14.822836: step 12950, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 41h:36m:52s remains)
INFO - root - 2017-12-07 17:04:35.915502: step 12960, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 41h:11m:46s remains)
INFO - root - 2017-12-07 17:04:56.814172: step 12970, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.107 sec/batch; 41h:04m:04s remains)
INFO - root - 2017-12-07 17:05:17.814819: step 12980, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.095 sec/batch; 40h:48m:39s remains)
INFO - root - 2017-12-07 17:05:39.178479: step 12990, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.153 sec/batch; 41h:56m:13s remains)
INFO - root - 2017-12-07 17:06:00.339287: step 13000, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.102 sec/batch; 40h:56m:30s remains)
2017-12-07 17:06:01.858804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2527251 -4.24743 -4.23294 -4.210999 -4.1849117 -4.1629033 -4.157845 -4.1677403 -4.1846776 -4.2032976 -4.2126226 -4.2124152 -4.2067752 -4.1934781 -4.1726742][-4.22792 -4.2129178 -4.1888008 -4.1599007 -4.1304655 -4.1092792 -4.1106992 -4.1270466 -4.1502361 -4.1729355 -4.1837034 -4.1868525 -4.1832843 -4.1711168 -4.1520844][-4.2056365 -4.185019 -4.1546235 -4.119236 -4.0878959 -4.0702286 -4.0766716 -4.0983 -4.1272731 -4.1541882 -4.1659503 -4.1737947 -4.1778793 -4.171226 -4.1549153][-4.1820703 -4.1620016 -4.1313853 -4.0960565 -4.0657287 -4.0508223 -4.0587335 -4.0826387 -4.1129565 -4.1379418 -4.1485319 -4.1591797 -4.1724854 -4.1742263 -4.1633739][-4.1650343 -4.1478214 -4.1217642 -4.0899715 -4.0595522 -4.0438929 -4.0461564 -4.0678825 -4.096386 -4.1204696 -4.1311245 -4.1462483 -4.1659231 -4.1701841 -4.1615286][-4.1629486 -4.1513891 -4.1304889 -4.10049 -4.0665312 -4.0433683 -4.0323181 -4.0490365 -4.0748081 -4.0968046 -4.1103253 -4.1321516 -4.1559639 -4.1609364 -4.1569076][-4.1733356 -4.1649156 -4.14814 -4.1206913 -4.0840836 -4.0489874 -4.0228052 -4.0316439 -4.0544052 -4.0715957 -4.085618 -4.1136241 -4.1413064 -4.1499791 -4.1514249][-4.1795306 -4.1691575 -4.1546946 -4.1308084 -4.0950389 -4.0527496 -4.0212793 -4.0251045 -4.0421371 -4.0511 -4.0656347 -4.0972867 -4.1271157 -4.1379051 -4.14298][-4.1744642 -4.1586151 -4.145092 -4.1279187 -4.1002378 -4.0614114 -4.0350804 -4.0398936 -4.05231 -4.0568652 -4.0740037 -4.1050854 -4.1257758 -4.1282411 -4.1304307][-4.1651545 -4.1461453 -4.1351404 -4.1268749 -4.1118007 -4.0834079 -4.0656395 -4.0726619 -4.0827641 -4.0848589 -4.0993319 -4.1173654 -4.1223555 -4.1134849 -4.1069746][-4.1597538 -4.1430902 -4.1360435 -4.1381874 -4.1388783 -4.1249676 -4.113174 -4.1192317 -4.1254034 -4.1202226 -4.1250029 -4.1264143 -4.1170063 -4.098897 -4.0856595][-4.1625924 -4.1510668 -4.1514068 -4.1662188 -4.1797094 -4.1775732 -4.1729774 -4.1815548 -4.18874 -4.1788883 -4.17146 -4.1610379 -4.139535 -4.1153259 -4.0979471][-4.1787004 -4.1668415 -4.1720395 -4.1974859 -4.2233219 -4.2323256 -4.2338781 -4.2451968 -4.255794 -4.2454791 -4.235599 -4.2226315 -4.1947951 -4.1657639 -4.1428933][-4.206408 -4.1931086 -4.1981144 -4.2259274 -4.2549467 -4.2676926 -4.2733507 -4.2852235 -4.2983046 -4.292819 -4.2813473 -4.2696137 -4.2446394 -4.2183747 -4.1938224][-4.2325091 -4.2213125 -4.2265897 -4.2488794 -4.269424 -4.2784591 -4.2838459 -4.2959938 -4.3087082 -4.3065338 -4.29752 -4.2896409 -4.2740602 -4.2571268 -4.2393003]]...]
INFO - root - 2017-12-07 17:06:22.942295: step 13010, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.097 sec/batch; 40h:51m:03s remains)
INFO - root - 2017-12-07 17:06:43.957798: step 13020, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.089 sec/batch; 40h:40m:36s remains)
INFO - root - 2017-12-07 17:07:05.221362: step 13030, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 40h:56m:43s remains)
INFO - root - 2017-12-07 17:07:26.158975: step 13040, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.143 sec/batch; 41h:43m:10s remains)
INFO - root - 2017-12-07 17:07:47.402336: step 13050, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.124 sec/batch; 41h:21m:07s remains)
INFO - root - 2017-12-07 17:08:08.478382: step 13060, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.103 sec/batch; 40h:55m:25s remains)
INFO - root - 2017-12-07 17:08:29.240719: step 13070, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.105 sec/batch; 40h:58m:17s remains)
INFO - root - 2017-12-07 17:08:50.673843: step 13080, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.154 sec/batch; 41h:54m:21s remains)
INFO - root - 2017-12-07 17:09:11.989393: step 13090, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.152 sec/batch; 41h:51m:50s remains)
INFO - root - 2017-12-07 17:09:33.008764: step 13100, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.135 sec/batch; 41h:31m:23s remains)
2017-12-07 17:09:34.369490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1482692 -4.1529269 -4.1620874 -4.1708245 -4.1741924 -4.1715307 -4.1634068 -4.15454 -4.1518111 -4.1488662 -4.1482348 -4.1481552 -4.1562986 -4.1685119 -4.1737432][-4.138576 -4.1527677 -4.1657152 -4.1722527 -4.1721659 -4.1687303 -4.1638546 -4.1630077 -4.1663556 -4.1682367 -4.16924 -4.1673222 -4.1746869 -4.1851826 -4.18807][-4.1573839 -4.1738338 -4.1825361 -4.1812496 -4.1714783 -4.160131 -4.1555395 -4.1649508 -4.1791697 -4.1918149 -4.201757 -4.2056193 -4.2124634 -4.2172513 -4.2130842][-4.1799397 -4.1924443 -4.1953468 -4.1882153 -4.1700811 -4.1506462 -4.1413021 -4.1542253 -4.177392 -4.2007871 -4.2205539 -4.2313433 -4.238481 -4.2404943 -4.2309089][-4.1822028 -4.1853781 -4.1846991 -4.1756248 -4.1510606 -4.1227174 -4.1100178 -4.1271253 -4.1645288 -4.2038293 -4.2364178 -4.2514458 -4.2530022 -4.2469244 -4.2300391][-4.1565228 -4.1495714 -4.1454997 -4.1355529 -4.1078029 -4.0712719 -4.0484328 -4.069541 -4.1302013 -4.195981 -4.2429523 -4.261435 -4.2574258 -4.241621 -4.2175746][-4.1243176 -4.1110678 -4.1001081 -4.0880108 -4.06258 -4.0204406 -3.98234 -3.997647 -4.0816627 -4.1754293 -4.2389345 -4.2629442 -4.2564483 -4.2374568 -4.2108426][-4.1193352 -4.1080351 -4.0959468 -4.0811009 -4.0585027 -4.0197682 -3.9772282 -3.9810877 -4.0623503 -4.1608615 -4.2281857 -4.2556553 -4.2510905 -4.2324023 -4.2094994][-4.1398182 -4.1276689 -4.1160512 -4.1016712 -4.08574 -4.0604892 -4.0307193 -4.0315838 -4.0891814 -4.1646428 -4.2198477 -4.2431078 -4.2395582 -4.2232795 -4.2054429][-4.1624937 -4.1481152 -4.1362729 -4.1265812 -4.1167274 -4.1025276 -4.0873718 -4.0920582 -4.1322541 -4.1868629 -4.2249961 -4.2400475 -4.2375879 -4.2237382 -4.208178][-4.17881 -4.1658559 -4.158092 -4.153842 -4.1471357 -4.1374259 -4.1314025 -4.1421685 -4.1720028 -4.2090731 -4.2313991 -4.2386489 -4.2367043 -4.2257133 -4.2120028][-4.1812587 -4.1729736 -4.1708713 -4.1707273 -4.1659422 -4.1621213 -4.1648512 -4.1777606 -4.1976156 -4.2162781 -4.2215247 -4.2171817 -4.210103 -4.1974 -4.1840224][-4.1830397 -4.1751785 -4.1737313 -4.1726623 -4.1705976 -4.1737261 -4.1826172 -4.1936245 -4.2022653 -4.2068677 -4.200479 -4.1906562 -4.180532 -4.1670704 -4.1554322][-4.1915388 -4.1789508 -4.1682024 -4.1573925 -4.1525574 -4.1573291 -4.1697116 -4.1814904 -4.1895418 -4.1904473 -4.18223 -4.1746936 -4.1687155 -4.1609817 -4.15285][-4.1832819 -4.1689973 -4.1557131 -4.1412358 -4.1337838 -4.1384449 -4.152648 -4.167778 -4.1782193 -4.1785545 -4.1712947 -4.1662555 -4.1615162 -4.1542058 -4.1464648]]...]
INFO - root - 2017-12-07 17:09:55.665554: step 13110, loss = 2.10, batch loss = 2.04 (14.8 examples/sec; 2.156 sec/batch; 41h:55m:53s remains)
INFO - root - 2017-12-07 17:10:16.925134: step 13120, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.142 sec/batch; 41h:39m:17s remains)
INFO - root - 2017-12-07 17:10:37.825478: step 13130, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.124 sec/batch; 41h:18m:06s remains)
INFO - root - 2017-12-07 17:10:58.737980: step 13140, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.166 sec/batch; 42h:06m:01s remains)
INFO - root - 2017-12-07 17:11:20.089405: step 13150, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.143 sec/batch; 41h:39m:34s remains)
INFO - root - 2017-12-07 17:11:41.196481: step 13160, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 41h:22m:56s remains)
INFO - root - 2017-12-07 17:12:01.919867: step 13170, loss = 2.09, batch loss = 2.03 (15.6 examples/sec; 2.058 sec/batch; 39h:58m:52s remains)
INFO - root - 2017-12-07 17:12:22.996479: step 13180, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 41h:09m:24s remains)
INFO - root - 2017-12-07 17:12:44.165148: step 13190, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.090 sec/batch; 40h:35m:52s remains)
INFO - root - 2017-12-07 17:13:05.064934: step 13200, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.123 sec/batch; 41h:14m:32s remains)
2017-12-07 17:13:06.631202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2979541 -4.3043871 -4.3191814 -4.3281631 -4.3294692 -4.3249793 -4.3171105 -4.3101888 -4.3072548 -4.3082256 -4.3125305 -4.3172488 -4.318943 -4.3185506 -4.3188729][-4.2946167 -4.29957 -4.3145156 -4.3244367 -4.3261905 -4.3213625 -4.3118162 -4.3041592 -4.3013921 -4.3032045 -4.311305 -4.3197112 -4.3223386 -4.32082 -4.3196769][-4.297225 -4.297646 -4.3091216 -4.3163786 -4.3158083 -4.3101397 -4.2991118 -4.2901711 -4.2871804 -4.2904997 -4.303092 -4.3166618 -4.3228874 -4.3211069 -4.3172283][-4.2935638 -4.2880406 -4.2935009 -4.2941079 -4.2871485 -4.2762933 -4.2618771 -4.2538037 -4.2527261 -4.2605152 -4.2783966 -4.2969108 -4.3087168 -4.3101535 -4.3028617][-4.2771254 -4.2667112 -4.2685232 -4.2631588 -4.2455673 -4.2247977 -4.2073145 -4.2054057 -4.2134032 -4.2289333 -4.2494469 -4.2687078 -4.2819376 -4.2858028 -4.2754073][-4.2551079 -4.2435012 -4.2454939 -4.2342172 -4.2032151 -4.1683826 -4.15037 -4.1632981 -4.1870823 -4.2100096 -4.2277517 -4.2424006 -4.2530904 -4.2570939 -4.2459474][-4.2298646 -4.2162867 -4.2131143 -4.1903358 -4.1443138 -4.093637 -4.07307 -4.1030564 -4.1482525 -4.1838155 -4.203805 -4.216279 -4.2245145 -4.226409 -4.2173915][-4.2044973 -4.187305 -4.1747227 -4.1385512 -4.0775075 -4.006382 -3.9781084 -4.0296721 -4.1041703 -4.1616669 -4.1901703 -4.2045889 -4.2130108 -4.2111311 -4.2031369][-4.1889033 -4.1709847 -4.1521578 -4.1092014 -4.0401812 -3.9600084 -3.9296582 -3.9896142 -4.0774903 -4.1466036 -4.1813459 -4.1975703 -4.2063794 -4.2036848 -4.1976786][-4.1826968 -4.1690397 -4.1553159 -4.1260419 -4.0778584 -4.0211844 -4.0036144 -4.0377412 -4.096839 -4.1511989 -4.1800041 -4.1917858 -4.1968808 -4.1940875 -4.1903687][-4.1809936 -4.17064 -4.165307 -4.157311 -4.1376634 -4.1092949 -4.10478 -4.118072 -4.1457596 -4.1783705 -4.1915779 -4.1896968 -4.1841025 -4.1811423 -4.1816912][-4.1898203 -4.1795239 -4.1793571 -4.1808524 -4.1758285 -4.164566 -4.1662068 -4.1719279 -4.1849546 -4.2020392 -4.2031832 -4.1919837 -4.1792684 -4.1748734 -4.1784081][-4.2031379 -4.1935134 -4.1964722 -4.20139 -4.2026014 -4.2000074 -4.2031517 -4.2068067 -4.2158437 -4.2237082 -4.2176776 -4.2044272 -4.1878023 -4.1758595 -4.1753607][-4.2104559 -4.202487 -4.20898 -4.2175856 -4.221714 -4.2221255 -4.222744 -4.224885 -4.2313662 -4.2348232 -4.2276134 -4.2153878 -4.1962438 -4.1780019 -4.1738648][-4.2096434 -4.2056065 -4.2169 -4.2281208 -4.2306542 -4.2296319 -4.2268457 -4.2299843 -4.2358541 -4.2364783 -4.229651 -4.2218866 -4.2056723 -4.187324 -4.1817641]]...]
INFO - root - 2017-12-07 17:13:27.853002: step 13210, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.122 sec/batch; 41h:12m:31s remains)
INFO - root - 2017-12-07 17:13:48.892037: step 13220, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.116 sec/batch; 41h:05m:23s remains)
INFO - root - 2017-12-07 17:14:09.823544: step 13230, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.164 sec/batch; 42h:01m:20s remains)
INFO - root - 2017-12-07 17:14:30.896415: step 13240, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.075 sec/batch; 40h:16m:37s remains)
INFO - root - 2017-12-07 17:14:51.878282: step 13250, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.112 sec/batch; 40h:59m:09s remains)
INFO - root - 2017-12-07 17:15:12.717798: step 13260, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.108 sec/batch; 40h:54m:17s remains)
INFO - root - 2017-12-07 17:15:33.552751: step 13270, loss = 2.09, batch loss = 2.03 (15.4 examples/sec; 2.078 sec/batch; 40h:19m:27s remains)
INFO - root - 2017-12-07 17:15:54.575574: step 13280, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 40h:47m:34s remains)
INFO - root - 2017-12-07 17:16:15.711499: step 13290, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.083 sec/batch; 40h:24m:47s remains)
INFO - root - 2017-12-07 17:16:36.490740: step 13300, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 2.049 sec/batch; 39h:45m:05s remains)
2017-12-07 17:16:38.047049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33333 -4.3346848 -4.3347011 -4.3340597 -4.3329787 -4.3333888 -4.3347511 -4.3373084 -4.3425612 -4.3457112 -4.3457327 -4.3423743 -4.3384285 -4.3345366 -4.3307376][-4.2989082 -4.2982903 -4.300519 -4.3026915 -4.3040838 -4.306767 -4.3087716 -4.3119669 -4.3204889 -4.3295622 -4.3339562 -4.3312612 -4.3250256 -4.3167367 -4.307611][-4.2409968 -4.2372808 -4.2419033 -4.2487831 -4.2550216 -4.261065 -4.2624617 -4.2627168 -4.2726097 -4.2887273 -4.2989044 -4.2982936 -4.2924795 -4.2819519 -4.2666955][-4.1623745 -4.1562853 -4.16538 -4.1785865 -4.1907625 -4.1982985 -4.19467 -4.1891646 -4.2000985 -4.2237411 -4.238574 -4.2394366 -4.233202 -4.2200189 -4.1986942][-4.0978065 -4.0859346 -4.0930352 -4.1065383 -4.1205654 -4.1247921 -4.1091976 -4.0928655 -4.1087031 -4.1496596 -4.1782289 -4.1856847 -4.1768765 -4.1579633 -4.1271834][-4.0891056 -4.062458 -4.0531836 -4.0513368 -4.05344 -4.0411553 -4.0011535 -3.9679866 -3.9985831 -4.0728226 -4.1303511 -4.1559873 -4.1500707 -4.1256976 -4.0852079][-4.1435394 -4.1023812 -4.067935 -4.0389585 -4.0159044 -3.9748816 -3.8982911 -3.8411577 -3.891674 -4.0042839 -4.0937591 -4.1433616 -4.1510582 -4.1324034 -4.0926943][-4.2266459 -4.1858387 -4.1417432 -4.0986547 -4.0601368 -4.0027423 -3.9102643 -3.844316 -3.8901486 -3.9963331 -4.0852284 -4.1432242 -4.1619892 -4.154439 -4.1277146][-4.2980847 -4.2721734 -4.2375851 -4.2019477 -4.1698327 -4.12279 -4.0523324 -4.0029082 -4.0196314 -4.0750074 -4.1277847 -4.1706748 -4.1896338 -4.1904278 -4.1773496][-4.3221216 -4.312655 -4.2958837 -4.2757821 -4.2579708 -4.2294869 -4.1866164 -4.1532559 -4.1493731 -4.1648512 -4.1870079 -4.2133684 -4.2318368 -4.2412372 -4.2387619][-4.2949872 -4.297009 -4.2963471 -4.2939935 -4.2927055 -4.284122 -4.2636957 -4.242147 -4.2293119 -4.2258821 -4.2340322 -4.2530923 -4.2728853 -4.2901478 -4.2953949][-4.24658 -4.2506661 -4.2588687 -4.2707863 -4.2862248 -4.2957191 -4.2931724 -4.281949 -4.2674494 -4.2560716 -4.2584476 -4.2756319 -4.2971239 -4.3161578 -4.3242316][-4.1981826 -4.1986885 -4.2082243 -4.2250872 -4.2489204 -4.2671833 -4.2761436 -4.2729259 -4.2589073 -4.2450714 -4.2449193 -4.2635074 -4.286921 -4.3062253 -4.31487][-4.1591635 -4.1496868 -4.1556616 -4.1738205 -4.20157 -4.225492 -4.2398038 -4.238205 -4.2199917 -4.2022629 -4.1998763 -4.2201467 -4.246933 -4.2686982 -4.2812161][-4.14397 -4.1242447 -4.1223383 -4.1383877 -4.1675677 -4.1938567 -4.2064996 -4.1991796 -4.1745949 -4.1523871 -4.1513824 -4.17448 -4.2014461 -4.22287 -4.2368169]]...]
INFO - root - 2017-12-07 17:16:59.308481: step 13310, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 41h:00m:26s remains)
INFO - root - 2017-12-07 17:17:20.515657: step 13320, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 40h:46m:45s remains)
INFO - root - 2017-12-07 17:17:41.495789: step 13330, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 40h:48m:38s remains)
INFO - root - 2017-12-07 17:18:02.768163: step 13340, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 41h:17m:23s remains)
INFO - root - 2017-12-07 17:18:23.805159: step 13350, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 41h:31m:29s remains)
INFO - root - 2017-12-07 17:18:44.701520: step 13360, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.132 sec/batch; 41h:19m:12s remains)
INFO - root - 2017-12-07 17:19:06.049953: step 13370, loss = 2.06, batch loss = 2.00 (14.7 examples/sec; 2.173 sec/batch; 42h:06m:47s remains)
INFO - root - 2017-12-07 17:19:27.222097: step 13380, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.136 sec/batch; 41h:22m:31s remains)
INFO - root - 2017-12-07 17:19:48.132216: step 13390, loss = 2.07, batch loss = 2.01 (16.3 examples/sec; 1.965 sec/batch; 38h:03m:19s remains)
INFO - root - 2017-12-07 17:20:09.443984: step 13400, loss = 2.08, batch loss = 2.03 (14.8 examples/sec; 2.157 sec/batch; 41h:46m:12s remains)
2017-12-07 17:20:11.180272: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1921272 -4.185266 -4.1757321 -4.170795 -4.1842365 -4.2099309 -4.2356586 -4.2505288 -4.25739 -4.2641883 -4.2728548 -4.2822475 -4.2843728 -4.2738557 -4.2541289][-4.2246704 -4.2157283 -4.202744 -4.1945548 -4.2096214 -4.2429638 -4.2778568 -4.2999458 -4.3072071 -4.3066463 -4.303546 -4.3011379 -4.2953658 -4.2811532 -4.2606654][-4.2439332 -4.2268295 -4.2105227 -4.2051277 -4.2246509 -4.2621026 -4.2988429 -4.3227572 -4.3293052 -4.3236465 -4.3102827 -4.2980933 -4.2882547 -4.2743926 -4.2570515][-4.2450714 -4.21672 -4.19652 -4.1962032 -4.2206054 -4.2571244 -4.289567 -4.3113794 -4.3185596 -4.311904 -4.2921972 -4.2728062 -4.2609825 -4.2512155 -4.2396283][-4.2497034 -4.2149992 -4.1889019 -4.1867576 -4.2083774 -4.2353868 -4.258604 -4.2770109 -4.2869935 -4.2833376 -4.2619896 -4.2382817 -4.2274628 -4.2226148 -4.2157259][-4.2600574 -4.2224383 -4.1893749 -4.1799173 -4.1896729 -4.2021661 -4.215426 -4.2316856 -4.2464204 -4.2484741 -4.2301707 -4.2069559 -4.1991458 -4.19897 -4.193727][-4.2658157 -4.2252665 -4.1874995 -4.1694117 -4.1654978 -4.1647582 -4.1696987 -4.1853619 -4.2063642 -4.2142453 -4.1993389 -4.1795607 -4.1759882 -4.1789093 -4.1733327][-4.2559323 -4.2150731 -4.1779051 -4.157413 -4.1453552 -4.1370773 -4.1383958 -4.1546507 -4.1800246 -4.192111 -4.1829162 -4.1701107 -4.1682143 -4.1685739 -4.1580768][-4.2327576 -4.1947441 -4.16239 -4.1470046 -4.1365461 -4.129653 -4.1345768 -4.1528726 -4.1781487 -4.18872 -4.1808076 -4.1721768 -4.1712432 -4.1681447 -4.1521497][-4.2072458 -4.1757274 -4.1530771 -4.1422343 -4.1362844 -4.1370468 -4.148344 -4.1678352 -4.1913362 -4.1994977 -4.1928196 -4.1856532 -4.1823421 -4.1763759 -4.1561284][-4.1891084 -4.1693482 -4.159276 -4.1503668 -4.1457562 -4.1554594 -4.1710796 -4.1887932 -4.2089496 -4.2185297 -4.2195158 -4.2182341 -4.2156372 -4.2076826 -4.1840544][-4.1865497 -4.1764646 -4.1722951 -4.1587148 -4.1501408 -4.1648417 -4.1836262 -4.1997614 -4.2193561 -4.2337017 -4.2447152 -4.2517967 -4.2508984 -4.2423534 -4.2192068][-4.1948967 -4.19153 -4.1881447 -4.1701069 -4.1566348 -4.1700969 -4.1897135 -4.2056603 -4.2257452 -4.24131 -4.2557378 -4.2667255 -4.26668 -4.2574687 -4.2390528][-4.2111826 -4.2143989 -4.2096872 -4.1887431 -4.1711297 -4.1780615 -4.1931458 -4.2078614 -4.2253504 -4.238081 -4.2489414 -4.2588339 -4.2602553 -4.252903 -4.2411761][-4.2395668 -4.2459373 -4.2389431 -4.2166719 -4.196219 -4.1962638 -4.2041016 -4.2148728 -4.2288971 -4.2368007 -4.2429986 -4.2495217 -4.2513537 -4.2480955 -4.2437167]]...]
INFO - root - 2017-12-07 17:20:32.538466: step 13410, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 41h:08m:42s remains)
INFO - root - 2017-12-07 17:20:53.432507: step 13420, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.084 sec/batch; 40h:21m:29s remains)
INFO - root - 2017-12-07 17:21:14.442692: step 13430, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.197 sec/batch; 42h:31m:26s remains)
INFO - root - 2017-12-07 17:21:35.722966: step 13440, loss = 2.08, batch loss = 2.03 (14.6 examples/sec; 2.198 sec/batch; 42h:33m:18s remains)
INFO - root - 2017-12-07 17:21:56.859218: step 13450, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.084 sec/batch; 40h:19m:32s remains)
INFO - root - 2017-12-07 17:22:17.658524: step 13460, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.066 sec/batch; 39h:59m:08s remains)
INFO - root - 2017-12-07 17:22:38.999025: step 13470, loss = 2.08, batch loss = 2.02 (14.5 examples/sec; 2.212 sec/batch; 42h:48m:23s remains)
INFO - root - 2017-12-07 17:23:00.189273: step 13480, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.107 sec/batch; 40h:46m:14s remains)
INFO - root - 2017-12-07 17:23:20.943541: step 13490, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 41h:05m:46s remains)
INFO - root - 2017-12-07 17:23:42.339075: step 13500, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.167 sec/batch; 41h:54m:42s remains)
2017-12-07 17:23:43.975183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1945348 -4.206707 -4.2310281 -4.2428417 -4.2421389 -4.2377882 -4.2197232 -4.1873713 -4.1757278 -4.1960182 -4.2183228 -4.2229586 -4.2240038 -4.2221627 -4.2081542][-4.1338196 -4.149014 -4.1895404 -4.2119737 -4.21458 -4.2046318 -4.1657739 -4.1095505 -4.0904417 -4.1228642 -4.1601915 -4.1810312 -4.1997929 -4.2023358 -4.1780562][-4.0691614 -4.0864658 -4.142746 -4.1778111 -4.1862483 -4.1677651 -4.1001968 -4.0167747 -3.9929955 -4.0370398 -4.0843668 -4.1246772 -4.1695652 -4.1866717 -4.1586823][-4.0372162 -4.057703 -4.1259637 -4.163321 -4.1726036 -4.1349454 -4.0361462 -3.9352319 -3.9157579 -3.9669187 -4.0162253 -4.0757761 -4.1468034 -4.1828542 -4.1577563][-4.0730329 -4.0959954 -4.1550283 -4.1761661 -4.16928 -4.1010146 -3.9780493 -3.8802524 -3.8801923 -3.9426873 -3.9986145 -4.0772467 -4.1614242 -4.2036848 -4.1826873][-4.1578732 -4.1709461 -4.2070608 -4.2062235 -4.1754322 -4.0776925 -3.9467847 -3.8720632 -3.8914022 -3.9567432 -4.014091 -4.1030803 -4.1880965 -4.2258468 -4.2131648][-4.2361679 -4.2390351 -4.2506428 -4.2348819 -4.1904416 -4.0852513 -3.9624567 -3.9117584 -3.9348927 -3.9827452 -4.0290608 -4.1162906 -4.1946244 -4.2311363 -4.2287397][-4.2681732 -4.2613182 -4.2560315 -4.2388611 -4.1943974 -4.10221 -3.9979525 -3.9480042 -3.9549356 -3.9769466 -4.0138221 -4.1092987 -4.1870685 -4.2217789 -4.2261024][-4.25316 -4.2323723 -4.2207265 -4.2092533 -4.1779008 -4.1150112 -4.0377555 -3.9798727 -3.9671621 -3.9739084 -4.0100522 -4.1074715 -4.1814065 -4.2108135 -4.2150421][-4.2247529 -4.193347 -4.1822143 -4.1740241 -4.1542387 -4.1185055 -4.0742607 -4.0281887 -4.0113 -4.0180488 -4.0533962 -4.1333466 -4.1945124 -4.218658 -4.2207184][-4.2160344 -4.1870551 -4.1791511 -4.1690154 -4.1568928 -4.14317 -4.1247559 -4.0995173 -4.0825105 -4.0908036 -4.1230216 -4.1756029 -4.2179308 -4.2379646 -4.2371273][-4.226603 -4.2100806 -4.2049189 -4.19648 -4.195127 -4.1934385 -4.1866331 -4.1771502 -4.1648569 -4.1712523 -4.1948314 -4.2205257 -4.2438588 -4.2583652 -4.2567358][-4.2446876 -4.243926 -4.24103 -4.2366681 -4.2438755 -4.2466903 -4.2417746 -4.2440987 -4.24329 -4.2477942 -4.2593174 -4.2677584 -4.2792125 -4.2899609 -4.2874355][-4.2622705 -4.2716975 -4.2699618 -4.2675943 -4.2800236 -4.2848926 -4.2790356 -4.2820406 -4.2874079 -4.2943306 -4.3007789 -4.3017316 -4.3076525 -4.3112049 -4.3084145][-4.2728977 -4.28257 -4.2820239 -4.2795672 -4.2898064 -4.2974863 -4.2946157 -4.29486 -4.298912 -4.3082733 -4.3135653 -4.3120584 -4.3123794 -4.3124514 -4.3102608]]...]
INFO - root - 2017-12-07 17:24:05.032127: step 13510, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.113 sec/batch; 40h:51m:16s remains)
INFO - root - 2017-12-07 17:24:25.793017: step 13520, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 40h:50m:51s remains)
INFO - root - 2017-12-07 17:24:46.967795: step 13530, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.140 sec/batch; 41h:21m:49s remains)
INFO - root - 2017-12-07 17:25:08.391861: step 13540, loss = 2.06, batch loss = 2.01 (14.8 examples/sec; 2.161 sec/batch; 41h:46m:07s remains)
INFO - root - 2017-12-07 17:25:29.298881: step 13550, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 40h:28m:10s remains)
INFO - root - 2017-12-07 17:25:50.124195: step 13560, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.082 sec/batch; 40h:13m:55s remains)
INFO - root - 2017-12-07 17:26:11.327201: step 13570, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.116 sec/batch; 40h:52m:45s remains)
INFO - root - 2017-12-07 17:26:32.468506: step 13580, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.071 sec/batch; 40h:00m:24s remains)
INFO - root - 2017-12-07 17:26:53.450396: step 13590, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.135 sec/batch; 41h:14m:15s remains)
INFO - root - 2017-12-07 17:27:14.627432: step 13600, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.176 sec/batch; 42h:02m:00s remains)
2017-12-07 17:27:16.225763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2544951 -4.2525291 -4.2601714 -4.2772145 -4.2714481 -4.2565484 -4.2521358 -4.2390223 -4.2146487 -4.1996493 -4.2032075 -4.2195225 -4.2383218 -4.2470083 -4.2314038][-4.2427368 -4.2397904 -4.2500658 -4.2630014 -4.2439513 -4.2138643 -4.1984076 -4.1804132 -4.165441 -4.1619616 -4.1725693 -4.1924739 -4.2113318 -4.2190561 -4.2072468][-4.2248673 -4.2218556 -4.2299514 -4.2386341 -4.2126541 -4.1741652 -4.1503129 -4.1280665 -4.12818 -4.1397181 -4.1565361 -4.1784072 -4.1909003 -4.1969876 -4.1905875][-4.1983509 -4.1951847 -4.2055621 -4.2156296 -4.1878028 -4.1458678 -4.11416 -4.093524 -4.110497 -4.1391358 -4.1623669 -4.1823053 -4.1853666 -4.1839867 -4.1772838][-4.1549945 -4.156034 -4.1732531 -4.1872034 -4.1578007 -4.1084862 -4.0586686 -4.0357046 -4.0781302 -4.1320152 -4.1684303 -4.185874 -4.1783786 -4.1643209 -4.1528029][-4.0949893 -4.0992656 -4.1259389 -4.1427412 -4.1054354 -4.034894 -3.9491839 -3.9233196 -4.0074515 -4.0999179 -4.162262 -4.1866097 -4.1763558 -4.155014 -4.1339436][-4.0427103 -4.0497127 -4.0849962 -4.1029449 -4.0542684 -3.9511225 -3.8204961 -3.7920704 -3.9162052 -4.0417213 -4.1278887 -4.1679468 -4.1672339 -4.1461172 -4.1245389][-4.0432162 -4.057651 -4.1023617 -4.1259885 -4.0830688 -3.9837646 -3.8618922 -3.8310068 -3.925967 -4.0291023 -4.1062264 -4.1523643 -4.1665316 -4.1577716 -4.1414351][-4.0917873 -4.1160769 -4.1707168 -4.2016973 -4.17855 -4.1123815 -4.027698 -3.992888 -4.0312943 -4.083189 -4.1311216 -4.1698513 -4.193275 -4.1937833 -4.18228][-4.1386137 -4.1656222 -4.2215619 -4.2556844 -4.2499294 -4.2118011 -4.1522551 -4.1179643 -4.1214738 -4.1372662 -4.1650105 -4.1980939 -4.2202439 -4.2212629 -4.2096567][-4.173686 -4.1936502 -4.2370596 -4.2659531 -4.2704353 -4.2505174 -4.2131042 -4.1860762 -4.1746283 -4.1650324 -4.1758513 -4.201323 -4.2185216 -4.2184219 -4.20731][-4.180521 -4.1917372 -4.2238612 -4.245441 -4.2563262 -4.2477288 -4.225153 -4.2037339 -4.1848903 -4.1649919 -4.1616325 -4.1771221 -4.1886959 -4.1928463 -4.1894164][-4.1656494 -4.1701193 -4.1970754 -4.2168388 -4.2261229 -4.2226496 -4.207655 -4.1910124 -4.1747179 -4.1562905 -4.1478014 -4.1552057 -4.1647573 -4.173974 -4.1801066][-4.1466241 -4.1521926 -4.1787658 -4.2008915 -4.20724 -4.2036505 -4.1931715 -4.1821842 -4.1757178 -4.1626205 -4.1518426 -4.1537905 -4.1628637 -4.1764555 -4.1883931][-4.1515961 -4.164454 -4.19386 -4.2120152 -4.2139907 -4.2115083 -4.2040462 -4.1955361 -4.1949778 -4.1845913 -4.171402 -4.1687312 -4.1777735 -4.1941862 -4.2079926]]...]
INFO - root - 2017-12-07 17:27:37.345702: step 13610, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.124 sec/batch; 41h:00m:45s remains)
INFO - root - 2017-12-07 17:27:58.079145: step 13620, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.083 sec/batch; 40h:12m:44s remains)
INFO - root - 2017-12-07 17:28:19.299819: step 13630, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.096 sec/batch; 40h:28m:02s remains)
INFO - root - 2017-12-07 17:28:40.545068: step 13640, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.064 sec/batch; 39h:49m:59s remains)
INFO - root - 2017-12-07 17:29:01.397870: step 13650, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.146 sec/batch; 41h:24m:48s remains)
INFO - root - 2017-12-07 17:29:22.524870: step 13660, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.120 sec/batch; 40h:54m:27s remains)
INFO - root - 2017-12-07 17:29:43.821829: step 13670, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.084 sec/batch; 40h:12m:05s remains)
INFO - root - 2017-12-07 17:30:04.728941: step 13680, loss = 2.08, batch loss = 2.02 (16.6 examples/sec; 1.930 sec/batch; 37h:14m:06s remains)
2017-12-07 17:30:22.751439: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 921.38MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
INFO - root - 2017-12-07 17:30:26.033164: step 13690, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.198 sec/batch; 42h:23m:08s remains)
INFO - root - 2017-12-07 17:30:47.290555: step 13700, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 41h:10m:27s remains)
2017-12-07 17:30:48.838023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2094617 -4.2092156 -4.22184 -4.2409554 -4.2581487 -4.2695861 -4.2746429 -4.2751913 -4.2718472 -4.2669911 -4.2629237 -4.2602224 -4.2569976 -4.2538276 -4.2529478][-4.232337 -4.2324 -4.2422252 -4.2574878 -4.27163 -4.2810946 -4.2847047 -4.2832627 -4.2786245 -4.2734013 -4.2700424 -4.268549 -4.266778 -4.2641168 -4.2619944][-4.2332516 -4.2344418 -4.2415285 -4.2513695 -4.2600904 -4.26707 -4.2711735 -4.2724209 -4.2721558 -4.2719669 -4.27382 -4.27656 -4.2772546 -4.2744937 -4.2691612][-4.207242 -4.2083216 -4.2126021 -4.2159047 -4.2156897 -4.2166696 -4.220871 -4.2284164 -4.2380438 -4.2487168 -4.2615461 -4.2737508 -4.2806773 -4.2795129 -4.2708983][-4.1644678 -4.1616764 -4.1607122 -4.1559176 -4.1454153 -4.1365056 -4.1376982 -4.15178 -4.1738243 -4.1991286 -4.2261477 -4.2512221 -4.2680316 -4.2720714 -4.2636647][-4.1213579 -4.1111979 -4.1016154 -4.086834 -4.06529 -4.0433426 -4.0361562 -4.0528259 -4.0854712 -4.1258721 -4.1676679 -4.2068162 -4.2356038 -4.2489767 -4.2469072][-4.0921335 -4.072957 -4.0528846 -4.0283546 -3.9973421 -3.9613078 -3.9369178 -3.9454553 -3.9828124 -4.0353041 -4.09096 -4.1435342 -4.1845264 -4.2097006 -4.21928][-4.0939331 -4.0684938 -4.0406618 -4.0092158 -3.9720063 -3.9240174 -3.8798435 -3.8709412 -3.9017951 -3.954819 -4.0143309 -4.0726447 -4.122231 -4.1592927 -4.1829138][-4.1239867 -4.0949407 -4.0627189 -4.0279365 -3.9910104 -3.9440808 -3.8948703 -3.8716002 -3.8839011 -3.9193459 -3.9650424 -4.0168777 -4.0686288 -4.1138563 -4.1489391][-4.1522164 -4.1227913 -4.0912166 -4.0583639 -4.0283804 -3.99201 -3.9533567 -3.9298029 -3.9277666 -3.9414654 -3.9670444 -4.0048561 -4.0508375 -4.0948911 -4.1311355][-4.1525078 -4.1261621 -4.1009188 -4.07678 -4.0572386 -4.0330391 -4.0076103 -3.9921181 -3.9892795 -3.9966879 -4.0144434 -4.0434737 -4.0798168 -4.1134834 -4.1395526][-4.1252613 -4.1040316 -4.0887837 -4.0772843 -4.0698805 -4.0575843 -4.0433345 -4.037559 -4.0403528 -4.0502286 -4.0683203 -4.0950475 -4.1245885 -4.1485476 -4.1640234][-4.0884 -4.0728369 -4.0684161 -4.0720487 -4.0786352 -4.0777369 -4.0718403 -4.0730162 -4.07856 -4.088182 -4.1040764 -4.1276155 -4.1527767 -4.1718187 -4.1828451][-4.0715055 -4.0596557 -4.0653319 -4.082324 -4.0998211 -4.1079812 -4.1085873 -4.1113605 -4.113276 -4.1152539 -4.1220937 -4.1379995 -4.15798 -4.1740823 -4.1860919][-4.0994482 -4.0917182 -4.1022415 -4.1228423 -4.1423192 -4.1529064 -4.1546612 -4.1537418 -4.1477613 -4.1390305 -4.1333613 -4.137526 -4.1494594 -4.1619844 -4.1762605]]...]
INFO - root - 2017-12-07 17:31:09.876719: step 13710, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 41h:17m:42s remains)
INFO - root - 2017-12-07 17:31:30.832834: step 13720, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.122 sec/batch; 40h:54m:50s remains)
INFO - root - 2017-12-07 17:31:52.109528: step 13730, loss = 2.06, batch loss = 2.01 (14.8 examples/sec; 2.155 sec/batch; 41h:33m:00s remains)
INFO - root - 2017-12-07 17:32:13.349285: step 13740, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.096 sec/batch; 40h:23m:32s remains)
INFO - root - 2017-12-07 17:32:34.204086: step 13750, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.093 sec/batch; 40h:19m:57s remains)
INFO - root - 2017-12-07 17:32:55.434359: step 13760, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.082 sec/batch; 40h:07m:09s remains)
INFO - root - 2017-12-07 17:33:16.711865: step 13770, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 40h:54m:38s remains)
INFO - root - 2017-12-07 17:33:37.491546: step 13780, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.131 sec/batch; 41h:02m:59s remains)
INFO - root - 2017-12-07 17:33:58.683683: step 13790, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.132 sec/batch; 41h:04m:12s remains)
INFO - root - 2017-12-07 17:34:19.869479: step 13800, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 40h:55m:48s remains)
2017-12-07 17:34:21.433342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2900414 -4.2913651 -4.2912421 -4.2878404 -4.2811003 -4.2720065 -4.2700825 -4.2820215 -4.2948861 -4.3057547 -4.3164353 -4.3168278 -4.3017306 -4.2843575 -4.2806029][-4.3043842 -4.3123307 -4.3126855 -4.3028326 -4.2831459 -4.2574725 -4.2382059 -4.2424188 -4.2555208 -4.2709656 -4.2881422 -4.2987957 -4.2951612 -4.284668 -4.2816696][-4.3178673 -4.3321838 -4.3327389 -4.3146338 -4.2834563 -4.2411003 -4.2043986 -4.2000308 -4.2148447 -4.2348347 -4.2570591 -4.2739835 -4.2779183 -4.2733297 -4.268847][-4.3248291 -4.3457956 -4.3491483 -4.3250823 -4.2858524 -4.2302494 -4.1811681 -4.1713171 -4.1900053 -4.2201977 -4.2489972 -4.26748 -4.2711267 -4.2641644 -4.2536135][-4.3239508 -4.3493595 -4.3542008 -4.3254018 -4.2782793 -4.2155242 -4.1597242 -4.147439 -4.1703148 -4.2114849 -4.2492867 -4.2704706 -4.2724042 -4.2608624 -4.246253][-4.3112793 -4.340519 -4.3480453 -4.3199096 -4.2692456 -4.2046533 -4.1467557 -4.1325331 -4.1576843 -4.20309 -4.2456627 -4.2694874 -4.2721572 -4.260819 -4.2495966][-4.2817907 -4.318872 -4.3308353 -4.3063164 -4.2527213 -4.1844945 -4.1243691 -4.1097226 -4.1378813 -4.1861644 -4.2330837 -4.2602553 -4.2674232 -4.2629957 -4.2605958][-4.2404361 -4.2855649 -4.3025255 -4.2823992 -4.2307363 -4.1602955 -4.0934658 -4.0735908 -4.1045547 -4.160603 -4.2143183 -4.2492094 -4.2623072 -4.2636805 -4.26813][-4.2074609 -4.2554841 -4.27642 -4.2618752 -4.2181177 -4.1540866 -4.0856233 -4.0575428 -4.083817 -4.1406507 -4.1982026 -4.2372279 -4.2536993 -4.2572284 -4.26473][-4.1870089 -4.2308469 -4.2598338 -4.2547412 -4.2228894 -4.1735659 -4.1160164 -4.0858603 -4.0987034 -4.1429715 -4.194272 -4.2273941 -4.2410321 -4.2435446 -4.251967][-4.1716104 -4.209188 -4.2450795 -4.2520556 -4.2340584 -4.2041121 -4.1645894 -4.1410928 -4.1429534 -4.1674681 -4.1986942 -4.2157941 -4.2201939 -4.2185764 -4.2270441][-4.1536407 -4.189949 -4.2320175 -4.2514319 -4.2470384 -4.2313089 -4.2064261 -4.18911 -4.1856341 -4.1986666 -4.2110748 -4.211237 -4.2038169 -4.1935844 -4.1968889][-4.12911 -4.1653924 -4.2134953 -4.2437816 -4.2543011 -4.250567 -4.2378535 -4.22782 -4.2230616 -4.2263517 -4.2257285 -4.2120371 -4.1945648 -4.1747446 -4.1689143][-4.1128631 -4.1490979 -4.2006521 -4.2372026 -4.2625093 -4.2713804 -4.2676668 -4.2622294 -4.2526731 -4.2420912 -4.22835 -4.208005 -4.1857805 -4.1606607 -4.1481142][-4.1168976 -4.1589355 -4.2107973 -4.2457552 -4.2731233 -4.2867136 -4.2869744 -4.2808423 -4.2631016 -4.237772 -4.2141933 -4.1974826 -4.1825266 -4.1593394 -4.1435065]]...]
INFO - root - 2017-12-07 17:34:42.526292: step 13810, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 40h:18m:34s remains)
INFO - root - 2017-12-07 17:35:03.725579: step 13820, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.093 sec/batch; 40h:17m:38s remains)
INFO - root - 2017-12-07 17:35:25.250934: step 13830, loss = 2.07, batch loss = 2.01 (14.4 examples/sec; 2.215 sec/batch; 42h:37m:44s remains)
INFO - root - 2017-12-07 17:35:46.528338: step 13840, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 40h:18m:24s remains)
INFO - root - 2017-12-07 17:36:07.759265: step 13850, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.107 sec/batch; 40h:33m:02s remains)
INFO - root - 2017-12-07 17:36:29.008589: step 13860, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.145 sec/batch; 41h:16m:17s remains)
INFO - root - 2017-12-07 17:36:50.062980: step 13870, loss = 2.07, batch loss = 2.01 (17.3 examples/sec; 1.851 sec/batch; 35h:36m:58s remains)
INFO - root - 2017-12-07 17:37:11.179253: step 13880, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.086 sec/batch; 40h:07m:10s remains)
INFO - root - 2017-12-07 17:37:32.433342: step 13890, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.089 sec/batch; 40h:10m:22s remains)
INFO - root - 2017-12-07 17:37:53.500731: step 13900, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.139 sec/batch; 41h:08m:03s remains)
2017-12-07 17:37:55.013726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2084465 -4.1971264 -4.1926928 -4.2043462 -4.2188287 -4.2198405 -4.2041154 -4.1926255 -4.1850905 -4.1679716 -4.1506 -4.1636472 -4.2029376 -4.2377272 -4.259089][-4.2251115 -4.2106223 -4.1987958 -4.2009788 -4.2092457 -4.2019973 -4.1740856 -4.1544266 -4.1484966 -4.1362438 -4.1212788 -4.1371746 -4.1850805 -4.2239823 -4.2422872][-4.2160263 -4.2000685 -4.1850872 -4.1844506 -4.1879492 -4.175313 -4.1448216 -4.1249576 -4.1221118 -4.1125512 -4.1032271 -4.1173606 -4.1654892 -4.2009506 -4.2131472][-4.1864495 -4.174407 -4.1628542 -4.1584425 -4.1574907 -4.1417255 -4.1104326 -4.0875711 -4.0864034 -4.083178 -4.0830383 -4.1039143 -4.1574063 -4.1934443 -4.1946044][-4.1612525 -4.1461849 -4.1341419 -4.1221247 -4.1181531 -4.0962381 -4.0505123 -4.0111814 -4.0203567 -4.0398808 -4.0587091 -4.0914063 -4.148437 -4.1869378 -4.1814728][-4.1328793 -4.1162491 -4.1054654 -4.0898905 -4.0774446 -4.0363188 -3.9487448 -3.877058 -3.9114594 -3.9831924 -4.0410404 -4.0923166 -4.14741 -4.1780219 -4.171392][-4.1115861 -4.0972586 -4.0914283 -4.0725522 -4.0402164 -3.96668 -3.8270602 -3.7208068 -3.8056259 -3.9440243 -4.0403552 -4.1082888 -4.1608038 -4.1815567 -4.1766891][-4.0986853 -4.0936241 -4.0964179 -4.0825014 -4.0458875 -3.9741437 -3.8450241 -3.759614 -3.8568904 -3.9887309 -4.076437 -4.1310248 -4.1721678 -4.1826162 -4.1810584][-4.107141 -4.1143003 -4.124465 -4.11743 -4.0908904 -4.044539 -3.9752657 -3.9420757 -4.000392 -4.0724635 -4.1197844 -4.15031 -4.173408 -4.180522 -4.18288][-4.1245036 -4.1380992 -4.15365 -4.1587839 -4.1470222 -4.1207852 -4.095489 -4.0846448 -4.1011057 -4.1253476 -4.1419578 -4.1525035 -4.163631 -4.1746068 -4.1804104][-4.1357327 -4.15787 -4.1813426 -4.1980586 -4.1935992 -4.1738443 -4.1653976 -4.1599569 -4.1556792 -4.1599689 -4.1592031 -4.1573477 -4.1587877 -4.1676784 -4.1719565][-4.1422696 -4.1738043 -4.19779 -4.2126975 -4.2077446 -4.1867332 -4.1847124 -4.1873946 -4.181695 -4.1799059 -4.1754861 -4.173305 -4.1735048 -4.1820273 -4.1810193][-4.1324778 -4.166944 -4.1902738 -4.2019043 -4.1921458 -4.1688976 -4.1692219 -4.1839514 -4.1908364 -4.1891685 -4.1836 -4.1815991 -4.1831918 -4.1936417 -4.1931415][-4.1255541 -4.1562 -4.1762838 -4.1797457 -4.16114 -4.1344371 -4.139802 -4.1708155 -4.195693 -4.1992278 -4.1878839 -4.1760206 -4.1684942 -4.1762147 -4.1806836][-4.14491 -4.1660171 -4.1733651 -4.1582265 -4.1247392 -4.0951462 -4.1098728 -4.1559753 -4.1936235 -4.200201 -4.1820021 -4.1549778 -4.1402693 -4.1480169 -4.1647205]]...]
INFO - root - 2017-12-07 17:38:15.715783: step 13910, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 40h:45m:33s remains)
INFO - root - 2017-12-07 17:38:37.081103: step 13920, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.091 sec/batch; 40h:11m:22s remains)
INFO - root - 2017-12-07 17:38:58.435620: step 13930, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.143 sec/batch; 41h:11m:16s remains)
INFO - root - 2017-12-07 17:39:19.369672: step 13940, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.095 sec/batch; 40h:16m:10s remains)
INFO - root - 2017-12-07 17:39:40.682388: step 13950, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 41h:09m:56s remains)
INFO - root - 2017-12-07 17:40:01.954965: step 13960, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.105 sec/batch; 40h:26m:55s remains)
INFO - root - 2017-12-07 17:40:22.851820: step 13970, loss = 2.09, batch loss = 2.03 (16.4 examples/sec; 1.949 sec/batch; 37h:26m:11s remains)
INFO - root - 2017-12-07 17:40:44.054701: step 13980, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.168 sec/batch; 41h:37m:59s remains)
INFO - root - 2017-12-07 17:41:05.416106: step 13990, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.163 sec/batch; 41h:32m:07s remains)
INFO - root - 2017-12-07 17:41:26.554587: step 14000, loss = 2.07, batch loss = 2.01 (16.8 examples/sec; 1.905 sec/batch; 36h:35m:15s remains)
2017-12-07 17:41:28.110850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2906523 -4.2826166 -4.2782373 -4.2712908 -4.2686644 -4.2759414 -4.2883086 -4.2982535 -4.30349 -4.3043275 -4.2996783 -4.2909102 -4.2820082 -4.2736511 -4.2689795][-4.2808161 -4.2750235 -4.2700014 -4.2599592 -4.2537394 -4.2591219 -4.2714448 -4.2850647 -4.2980542 -4.308013 -4.3122306 -4.3114295 -4.3062234 -4.2966514 -4.2890272][-4.2788715 -4.27384 -4.2653704 -4.2507014 -4.2382522 -4.2401404 -4.2499676 -4.2624264 -4.2788153 -4.2976737 -4.3130474 -4.32255 -4.3245149 -4.3181543 -4.3103838][-4.2871609 -4.2785921 -4.2607331 -4.2379293 -4.2175078 -4.2118163 -4.2135859 -4.222785 -4.2409945 -4.2667875 -4.2946897 -4.3168945 -4.3285284 -4.3280945 -4.32318][-4.2998233 -4.2858787 -4.2532368 -4.2142758 -4.1793647 -4.1607537 -4.1502404 -4.1591229 -4.1859207 -4.2212152 -4.2614322 -4.2983236 -4.3210654 -4.3282142 -4.3273034][-4.3022742 -4.2818122 -4.2368541 -4.1798034 -4.1289897 -4.09352 -4.0733781 -4.0832524 -4.1225386 -4.1705174 -4.2216954 -4.2703848 -4.304347 -4.32002 -4.3241997][-4.2890325 -4.2641277 -4.2134175 -4.144681 -4.0805774 -4.0325704 -4.0126944 -4.0305896 -4.0804443 -4.1404595 -4.19844 -4.25099 -4.2902203 -4.3112965 -4.3185263][-4.2732611 -4.2475338 -4.19667 -4.1235466 -4.0524797 -3.9993682 -3.983403 -4.010129 -4.0618458 -4.1236897 -4.1839614 -4.2383308 -4.2790403 -4.3008494 -4.3081989][-4.2685804 -4.2481508 -4.20468 -4.136301 -4.065022 -4.01297 -4.0013533 -4.0249596 -4.0624166 -4.1109967 -4.1660123 -4.2221713 -4.2635517 -4.2820654 -4.288435][-4.2684951 -4.2607365 -4.2314777 -4.1750584 -4.1074042 -4.0581684 -4.0462589 -4.0609202 -4.0827641 -4.1148 -4.1592989 -4.2102413 -4.248208 -4.259223 -4.2607932][-4.2634907 -4.2704244 -4.2596607 -4.2220793 -4.1669545 -4.1218042 -4.1024423 -4.1085739 -4.1234012 -4.1456242 -4.1782885 -4.2162061 -4.2420726 -4.2390914 -4.230289][-4.2573624 -4.2757926 -4.279068 -4.2612085 -4.2274046 -4.193356 -4.1732688 -4.1727157 -4.1815543 -4.1945796 -4.2123528 -4.2318387 -4.2401543 -4.2214603 -4.201869][-4.24376 -4.2697859 -4.2807655 -4.2793069 -4.2662578 -4.2482476 -4.2355318 -4.2350321 -4.239922 -4.2449474 -4.2485385 -4.2517934 -4.2461023 -4.2200642 -4.1967][-4.2197976 -4.2538943 -4.2734728 -4.2838173 -4.2865148 -4.2809529 -4.2774305 -4.2816358 -4.2868109 -4.2872906 -4.2845607 -4.2816758 -4.2723041 -4.2475061 -4.2267156][-4.2123432 -4.2505488 -4.2767549 -4.2944922 -4.305006 -4.3067837 -4.307322 -4.3116617 -4.3167791 -4.3161559 -4.3126593 -4.3098755 -4.3031149 -4.2836394 -4.26691]]...]
INFO - root - 2017-12-07 17:41:49.356095: step 14010, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.135 sec/batch; 40h:59m:42s remains)
INFO - root - 2017-12-07 17:42:10.817607: step 14020, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.121 sec/batch; 40h:42m:38s remains)
INFO - root - 2017-12-07 17:42:31.999714: step 14030, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.115 sec/batch; 40h:35m:58s remains)
INFO - root - 2017-12-07 17:42:52.653123: step 14040, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 40h:20m:13s remains)
INFO - root - 2017-12-07 17:43:14.042114: step 14050, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.127 sec/batch; 40h:48m:11s remains)
INFO - root - 2017-12-07 17:43:35.201590: step 14060, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 40h:40m:55s remains)
INFO - root - 2017-12-07 17:43:56.346171: step 14070, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.136 sec/batch; 40h:57m:54s remains)
INFO - root - 2017-12-07 17:44:17.498743: step 14080, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.157 sec/batch; 41h:21m:38s remains)
INFO - root - 2017-12-07 17:44:38.545515: step 14090, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.061 sec/batch; 39h:30m:51s remains)
INFO - root - 2017-12-07 17:44:59.450207: step 14100, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.143 sec/batch; 41h:05m:27s remains)
2017-12-07 17:45:00.992702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2284241 -4.218399 -4.2205334 -4.2372 -4.2748985 -4.2994576 -4.3117352 -4.3049793 -4.2649636 -4.2011533 -4.1464167 -4.141489 -4.1604891 -4.1970258 -4.2374449][-4.2066479 -4.2045636 -4.2038283 -4.2084889 -4.242053 -4.265718 -4.2770844 -4.2735186 -4.2395382 -4.1781836 -4.1239586 -4.1158223 -4.1320724 -4.1676073 -4.2055306][-4.2001462 -4.2086053 -4.2049546 -4.1955876 -4.2178931 -4.2358618 -4.239069 -4.2351913 -4.218977 -4.1766233 -4.1349154 -4.1298046 -4.1455803 -4.1768827 -4.2092094][-4.2046108 -4.2204132 -4.2150984 -4.1958389 -4.2057714 -4.2158918 -4.2059469 -4.1942968 -4.1967573 -4.187191 -4.1651855 -4.1694732 -4.1902652 -4.217515 -4.2398696][-4.2026334 -4.2225404 -4.2163172 -4.1931252 -4.1884422 -4.1858091 -4.1645184 -4.1395707 -4.1524014 -4.1795197 -4.1832724 -4.2008667 -4.231163 -4.2550621 -4.2634206][-4.1996288 -4.2164893 -4.2093606 -4.1856112 -4.1612196 -4.1375074 -4.0981212 -4.0453424 -4.0592241 -4.1281514 -4.1706877 -4.203589 -4.2407632 -4.2650466 -4.2632065][-4.2089362 -4.218493 -4.2085772 -4.1783385 -4.1311145 -4.0803232 -4.0094523 -3.9128537 -3.9297268 -4.0504007 -4.1307659 -4.1793857 -4.223052 -4.2542844 -4.2493396][-4.2124085 -4.2229342 -4.216938 -4.1893406 -4.1385541 -4.0697446 -3.9775186 -3.8590846 -3.8812513 -4.0167012 -4.10779 -4.1589427 -4.201798 -4.2373962 -4.2323604][-4.1848245 -4.2029214 -4.2158704 -4.2088938 -4.1800861 -4.1240277 -4.0540533 -3.970489 -3.9896536 -4.0842447 -4.1522493 -4.1894069 -4.2178307 -4.242661 -4.2316389][-4.1568704 -4.1789951 -4.2056146 -4.2194204 -4.2158918 -4.1868916 -4.1481252 -4.0980577 -4.111692 -4.1685824 -4.2112813 -4.2343087 -4.2458615 -4.2543211 -4.237977][-4.1478567 -4.1685691 -4.1982222 -4.2248392 -4.2404385 -4.2342587 -4.2163515 -4.1871462 -4.1965518 -4.2304597 -4.2558184 -4.2691774 -4.267035 -4.2613664 -4.2448869][-4.1749 -4.1903806 -4.2133265 -4.2411184 -4.2657895 -4.2732682 -4.2671843 -4.2496543 -4.2556968 -4.273747 -4.2871232 -4.2919011 -4.2829618 -4.2716146 -4.2575126][-4.2302837 -4.239233 -4.2517872 -4.27345 -4.2951365 -4.3076549 -4.3096743 -4.2995086 -4.30148 -4.3070974 -4.3105674 -4.3098574 -4.2992382 -4.2870049 -4.2760539][-4.2871571 -4.2939634 -4.2981706 -4.3084369 -4.3197365 -4.3282623 -4.3316865 -4.3246918 -4.3254089 -4.3270512 -4.3254704 -4.3241572 -4.3153281 -4.307508 -4.3026791][-4.3250217 -4.3328071 -4.3330836 -4.3354483 -4.3379602 -4.3397536 -4.342649 -4.3416729 -4.3440466 -4.3448839 -4.34153 -4.3396 -4.3333311 -4.3290534 -4.3303485]]...]
INFO - root - 2017-12-07 17:45:22.149967: step 14110, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 40h:58m:12s remains)
INFO - root - 2017-12-07 17:45:43.080354: step 14120, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 40h:13m:55s remains)
INFO - root - 2017-12-07 17:46:04.042972: step 14130, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.123 sec/batch; 40h:40m:48s remains)
INFO - root - 2017-12-07 17:46:24.983768: step 14140, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.109 sec/batch; 40h:24m:43s remains)
INFO - root - 2017-12-07 17:46:46.063557: step 14150, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.084 sec/batch; 39h:56m:16s remains)
INFO - root - 2017-12-07 17:47:07.191350: step 14160, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.104 sec/batch; 40h:17m:53s remains)
INFO - root - 2017-12-07 17:47:28.236676: step 14170, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.112 sec/batch; 40h:27m:03s remains)
INFO - root - 2017-12-07 17:47:49.425265: step 14180, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.142 sec/batch; 41h:01m:47s remains)
INFO - root - 2017-12-07 17:48:10.638472: step 14190, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.139 sec/batch; 40h:57m:03s remains)
INFO - root - 2017-12-07 17:48:31.484103: step 14200, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 40h:16m:23s remains)
2017-12-07 17:48:33.054708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.226141 -4.233201 -4.2385592 -4.2395024 -4.2378659 -4.2388654 -4.2396851 -4.2430525 -4.2433748 -4.2405491 -4.2343159 -4.2314949 -4.2292123 -4.2205858 -4.2010689][-4.2313766 -4.2424555 -4.2497754 -4.2496643 -4.2466455 -4.2450414 -4.2408881 -4.2387652 -4.2343907 -4.230432 -4.220758 -4.2141633 -4.2118592 -4.1989965 -4.1720982][-4.2260933 -4.233007 -4.2370815 -4.2342682 -4.2336836 -4.2345166 -4.2294827 -4.2225461 -4.2150006 -4.2110448 -4.2004886 -4.1908116 -4.1852908 -4.1694269 -4.1390271][-4.1972656 -4.2054925 -4.2123728 -4.207233 -4.2051024 -4.2054572 -4.197041 -4.1842365 -4.1770244 -4.1827335 -4.1839252 -4.1759634 -4.1620517 -4.14008 -4.1076975][-4.1349754 -4.1619196 -4.1858044 -4.1820455 -4.172132 -4.1619587 -4.1424828 -4.1218462 -4.124227 -4.1538911 -4.1780691 -4.1730785 -4.147583 -4.119267 -4.0880733][-4.0551081 -4.098124 -4.1383238 -4.1391015 -4.1244497 -4.09729 -4.0511928 -4.0150471 -4.038928 -4.1085472 -4.1612473 -4.1642408 -4.136035 -4.1049027 -4.0745687][-4.0043888 -4.0484629 -4.0929351 -4.0956936 -4.0719142 -4.0170841 -3.9265261 -3.8666117 -3.921453 -4.0383554 -4.1230793 -4.1393204 -4.1182885 -4.091342 -4.0640764][-4.0121694 -4.040875 -4.0757828 -4.0762062 -4.0433888 -3.9679236 -3.8532233 -3.7855515 -3.8632488 -3.9985709 -4.0933619 -4.1195536 -4.1076527 -4.0873127 -4.0678892][-4.0542846 -4.0643587 -4.0887923 -4.09111 -4.0655994 -4.0084438 -3.928525 -3.8877327 -3.9402566 -4.0361753 -4.1075587 -4.1293807 -4.1203928 -4.107604 -4.1005549][-4.1155086 -4.1121187 -4.1291137 -4.1382589 -4.1295023 -4.101809 -4.06103 -4.0384064 -4.0601139 -4.1109371 -4.1560488 -4.1703167 -4.1629395 -4.1551929 -4.1584282][-4.1843963 -4.1745682 -4.1881571 -4.2050643 -4.2105989 -4.2028513 -4.186132 -4.1750417 -4.1812291 -4.2044115 -4.2288108 -4.2337837 -4.2269545 -4.2229462 -4.2304368][-4.2547193 -4.24758 -4.2570662 -4.2725153 -4.2848892 -4.2873306 -4.282712 -4.276866 -4.2781405 -4.2882304 -4.2987218 -4.2990756 -4.2931628 -4.290308 -4.2956142][-4.309164 -4.3053885 -4.3104591 -4.3209405 -4.331646 -4.3365493 -4.33366 -4.328299 -4.3257284 -4.3289189 -4.3324866 -4.3338027 -4.3309298 -4.330708 -4.33439][-4.3376484 -4.3379579 -4.3415995 -4.347105 -4.3513408 -4.3529367 -4.3489547 -4.3435621 -4.3416 -4.3431292 -4.3453846 -4.3463845 -4.3448453 -4.3454986 -4.3482037][-4.3513651 -4.353724 -4.3562274 -4.3580103 -4.3582697 -4.3569045 -4.3525805 -4.3486466 -4.3478341 -4.3493505 -4.3519111 -4.3535333 -4.352447 -4.3519254 -4.3530045]]...]
INFO - root - 2017-12-07 17:48:54.227490: step 14210, loss = 2.09, batch loss = 2.03 (14.8 examples/sec; 2.168 sec/batch; 41h:29m:53s remains)
INFO - root - 2017-12-07 17:49:15.444916: step 14220, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.149 sec/batch; 41h:08m:01s remains)
INFO - root - 2017-12-07 17:49:36.381327: step 14230, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.090 sec/batch; 39h:59m:48s remains)
INFO - root - 2017-12-07 17:49:57.773501: step 14240, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 40h:30m:15s remains)
INFO - root - 2017-12-07 17:50:19.097325: step 14250, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.164 sec/batch; 41h:23m:53s remains)
INFO - root - 2017-12-07 17:50:39.861049: step 14260, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 1.877 sec/batch; 35h:53m:47s remains)
INFO - root - 2017-12-07 17:51:01.041501: step 14270, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.120 sec/batch; 40h:33m:04s remains)
INFO - root - 2017-12-07 17:51:22.215763: step 14280, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.131 sec/batch; 40h:45m:32s remains)
INFO - root - 2017-12-07 17:51:43.271235: step 14290, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.076 sec/batch; 39h:41m:51s remains)
INFO - root - 2017-12-07 17:52:04.370070: step 14300, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.124 sec/batch; 40h:36m:52s remains)
2017-12-07 17:52:06.095082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.353374 -4.3537388 -4.3518214 -4.3509169 -4.3513503 -4.352695 -4.3549471 -4.3578115 -4.3599648 -4.3615012 -4.3622708 -4.3618541 -4.3607011 -4.3593154 -4.3581185][-4.3529949 -4.349308 -4.3440475 -4.3415995 -4.3418803 -4.34407 -4.3480892 -4.3529954 -4.3566918 -4.3600206 -4.3628669 -4.3640876 -4.3637419 -4.361928 -4.3600454][-4.345099 -4.3365035 -4.3277721 -4.32265 -4.3206797 -4.3220973 -4.3287168 -4.3365865 -4.34382 -4.3515663 -4.358397 -4.362308 -4.3632607 -4.3617997 -4.3594069][-4.3269777 -4.3126993 -4.2981033 -4.28596 -4.2778749 -4.2789607 -4.2896204 -4.3027482 -4.3167248 -4.3317652 -4.344604 -4.3529062 -4.3573151 -4.3575611 -4.3559766][-4.3006754 -4.2741323 -4.2473831 -4.224668 -4.2083731 -4.2062244 -4.2200694 -4.2418623 -4.2678971 -4.2930169 -4.3150406 -4.3307781 -4.3420367 -4.3461542 -4.3473759][-4.2725573 -4.2299976 -4.1875091 -4.1493969 -4.1199241 -4.1051731 -4.1142554 -4.1450071 -4.1893115 -4.23155 -4.2668257 -4.2938876 -4.31481 -4.3261824 -4.3334379][-4.2529664 -4.196496 -4.13428 -4.073741 -4.0211058 -3.9848495 -3.9852214 -4.0266013 -4.0927415 -4.1547494 -4.2046428 -4.2443337 -4.2764368 -4.2975011 -4.3144903][-4.2526751 -4.1890435 -4.10872 -4.0230336 -3.9409592 -3.8754647 -3.861093 -3.9049258 -3.9837463 -4.0620937 -4.1283784 -4.1843033 -4.2323904 -4.2677341 -4.2973375][-4.274322 -4.2159104 -4.1302924 -4.0277033 -3.9180532 -3.8256817 -3.7942467 -3.8228946 -3.8930418 -3.976187 -4.0560007 -4.1293192 -4.195456 -4.2481542 -4.2895751][-4.3082576 -4.2660761 -4.1949563 -4.0986581 -3.9902508 -3.8969536 -3.8513851 -3.8506718 -3.8858 -3.9492707 -4.02753 -4.1089249 -4.1862779 -4.2491651 -4.2946048][-4.3397264 -4.317852 -4.2740583 -4.2065048 -4.1262603 -4.0552516 -4.0112033 -3.9883463 -3.987699 -4.0186119 -4.0745616 -4.143364 -4.2115126 -4.26841 -4.307508][-4.3577409 -4.349215 -4.3271432 -4.2897534 -4.2432351 -4.2019529 -4.170361 -4.1442418 -4.12976 -4.1378779 -4.1679368 -4.2112489 -4.2572269 -4.2968283 -4.3235612][-4.36294 -4.3618803 -4.3522458 -4.3347907 -4.3127141 -4.2929831 -4.2735457 -4.2517557 -4.2363119 -4.2375011 -4.2526588 -4.2755666 -4.3011365 -4.3231869 -4.337328][-4.361043 -4.3631825 -4.359355 -4.3508115 -4.3408976 -4.3320184 -4.3217382 -4.3074951 -4.295805 -4.2935724 -4.3001542 -4.3123569 -4.3260365 -4.3371944 -4.3430123][-4.3567171 -4.3592062 -4.3569131 -4.3519096 -4.3472276 -4.3434944 -4.3383393 -4.3307605 -4.3243217 -4.3221941 -4.3249168 -4.3313861 -4.3375425 -4.3421712 -4.3442707]]...]
INFO - root - 2017-12-07 17:52:27.125182: step 14310, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.112 sec/batch; 40h:21m:46s remains)
INFO - root - 2017-12-07 17:52:48.171740: step 14320, loss = 2.06, batch loss = 2.00 (15.4 examples/sec; 2.084 sec/batch; 39h:49m:36s remains)
INFO - root - 2017-12-07 17:53:09.079797: step 14330, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 40h:39m:22s remains)
INFO - root - 2017-12-07 17:53:30.188446: step 14340, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.100 sec/batch; 40h:07m:15s remains)
INFO - root - 2017-12-07 17:53:51.416248: step 14350, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.106 sec/batch; 40h:14m:25s remains)
INFO - root - 2017-12-07 17:54:12.163152: step 14360, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.123 sec/batch; 40h:32m:44s remains)
INFO - root - 2017-12-07 17:54:33.314779: step 14370, loss = 2.06, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 40h:35m:04s remains)
INFO - root - 2017-12-07 17:54:54.496266: step 14380, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.124 sec/batch; 40h:33m:10s remains)
INFO - root - 2017-12-07 17:55:15.278432: step 14390, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.115 sec/batch; 40h:23m:27s remains)
INFO - root - 2017-12-07 17:55:36.403343: step 14400, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.119 sec/batch; 40h:26m:37s remains)
2017-12-07 17:55:37.952308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2415276 -4.2187705 -4.2151361 -4.2057123 -4.181921 -4.1546431 -4.1284094 -4.104867 -4.1236238 -4.1744251 -4.2057037 -4.2057528 -4.1838226 -4.1445866 -4.1164761][-4.2392082 -4.2181568 -4.2085037 -4.18897 -4.1613121 -4.1341934 -4.1031203 -4.0770454 -4.1048584 -4.1666193 -4.2030449 -4.2060261 -4.1835113 -4.1419578 -4.1120415][-4.2323618 -4.212306 -4.19278 -4.162673 -4.1378231 -4.1174631 -4.0857544 -4.0662308 -4.1067748 -4.1740184 -4.2118773 -4.2180238 -4.198709 -4.1591845 -4.1269][-4.2287445 -4.2065983 -4.1749492 -4.136683 -4.1157389 -4.0992579 -4.0679221 -4.0591884 -4.1107593 -4.1767054 -4.2122149 -4.2225628 -4.2135005 -4.1878581 -4.1645751][-4.22965 -4.2033877 -4.1593018 -4.1120667 -4.0878124 -4.0664897 -4.0310125 -4.0298967 -4.0905867 -4.154264 -4.1840115 -4.1965518 -4.2023339 -4.1957316 -4.1892653][-4.2269735 -4.1944165 -4.1388979 -4.0770292 -4.0329666 -3.9910235 -3.9437702 -3.9501004 -4.0263548 -4.0966692 -4.1258254 -4.1422968 -4.1627874 -4.1763721 -4.1901755][-4.2225785 -4.1804504 -4.109899 -4.028008 -3.9565332 -3.887732 -3.825119 -3.8409252 -3.9370763 -4.0193381 -4.059062 -4.0865679 -4.1209035 -4.1532474 -4.1852984][-4.2241068 -4.1749115 -4.0951085 -4.0042338 -3.9166608 -3.8280401 -3.7526913 -3.7662354 -3.8607218 -3.9435308 -3.9965255 -4.0412431 -4.0904474 -4.1377344 -4.1805053][-4.2331972 -4.1853876 -4.111526 -4.0290742 -3.9450808 -3.8563063 -3.7809079 -3.7786021 -3.839148 -3.8996687 -3.9573 -4.0127134 -4.0681763 -4.122261 -4.1703978][-4.2511973 -4.2104588 -4.1531596 -4.0877905 -4.0172162 -3.9358802 -3.8630867 -3.8439691 -3.8641677 -3.893749 -3.9434309 -3.9961457 -4.0490041 -4.1077437 -4.1634769][-4.2717271 -4.2408724 -4.2025819 -4.1567121 -4.1026077 -4.0318389 -3.9643254 -3.9328775 -3.9224129 -3.9256122 -3.9604805 -4.0022125 -4.0485511 -4.111032 -4.1713662][-4.2858777 -4.2628641 -4.23645 -4.2064896 -4.1691003 -4.1180239 -4.0678473 -4.0404248 -4.019701 -4.0096312 -4.0285678 -4.0557618 -4.0915747 -4.1459484 -4.1969838][-4.284925 -4.2692909 -4.25076 -4.2305207 -4.20756 -4.177454 -4.1527414 -4.1422572 -4.1284657 -4.1188536 -4.1276417 -4.141716 -4.1649008 -4.2057762 -4.2408109][-4.2720127 -4.2616892 -4.2489767 -4.2342048 -4.2196193 -4.2045307 -4.2000031 -4.2055745 -4.2053714 -4.2046723 -4.2107005 -4.2176681 -4.2323804 -4.26099 -4.2820191][-4.2445226 -4.2360668 -4.22663 -4.2162666 -4.20625 -4.200748 -4.211473 -4.2310786 -4.2457151 -4.2563186 -4.2623587 -4.2656507 -4.2742133 -4.2933369 -4.3044515]]...]
INFO - root - 2017-12-07 17:55:59.056774: step 14410, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.138 sec/batch; 40h:48m:47s remains)
INFO - root - 2017-12-07 17:56:20.243541: step 14420, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.107 sec/batch; 40h:13m:10s remains)
INFO - root - 2017-12-07 17:56:41.312445: step 14430, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.167 sec/batch; 41h:21m:34s remains)
INFO - root - 2017-12-07 17:57:02.434348: step 14440, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 40h:06m:58s remains)
INFO - root - 2017-12-07 17:57:23.692494: step 14450, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 1.944 sec/batch; 37h:05m:23s remains)
INFO - root - 2017-12-07 17:57:44.665121: step 14460, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.145 sec/batch; 40h:54m:50s remains)
INFO - root - 2017-12-07 17:58:05.916530: step 14470, loss = 2.10, batch loss = 2.04 (15.2 examples/sec; 2.101 sec/batch; 40h:03m:47s remains)
INFO - root - 2017-12-07 17:58:27.197744: step 14480, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.149 sec/batch; 40h:58m:56s remains)
INFO - root - 2017-12-07 17:58:48.335379: step 14490, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.155 sec/batch; 41h:04m:50s remains)
INFO - root - 2017-12-07 17:59:09.590017: step 14500, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 40h:21m:58s remains)
2017-12-07 17:59:11.194714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2858133 -4.2707815 -4.2586102 -4.2579689 -4.2687321 -4.2861967 -4.3011332 -4.3059745 -4.3022938 -4.2978668 -4.2958593 -4.2974324 -4.3048072 -4.311902 -4.3090215][-4.2651429 -4.2552505 -4.2440629 -4.23974 -4.2477932 -4.2650104 -4.2810321 -4.2865624 -4.2835846 -4.2785916 -4.2749195 -4.2779188 -4.2909575 -4.3046269 -4.3088808][-4.2321515 -4.2264042 -4.218008 -4.2135077 -4.2189589 -4.23394 -4.2503572 -4.2576494 -4.2547984 -4.2472205 -4.2394114 -4.2402043 -4.2577872 -4.2814207 -4.2995911][-4.2013149 -4.197052 -4.1890626 -4.1818218 -4.182703 -4.1924057 -4.2073374 -4.2172375 -4.216825 -4.2072425 -4.1949105 -4.1910858 -4.210721 -4.2454481 -4.2808895][-4.1778708 -4.1702733 -4.1600122 -4.1496663 -4.1447949 -4.1458297 -4.1547408 -4.1657634 -4.1706276 -4.1639991 -4.1513076 -4.1460919 -4.1648364 -4.2056041 -4.253509][-4.167861 -4.1552305 -4.1431284 -4.1302443 -4.1184168 -4.1048946 -4.1003366 -4.1100955 -4.1237297 -4.126792 -4.1213684 -4.12084 -4.1379795 -4.176403 -4.2273827][-4.1703081 -4.1527328 -4.1376529 -4.1205997 -4.0980964 -4.0649047 -4.0403061 -4.0473604 -4.0762138 -4.0991235 -4.1102729 -4.1191936 -4.1365209 -4.165853 -4.2081003][-4.1803226 -4.1633968 -4.1491485 -4.1292052 -4.096034 -4.0438089 -3.9959879 -3.9934969 -4.0369611 -4.0852365 -4.1185741 -4.1386905 -4.155159 -4.1732793 -4.2008438][-4.2000194 -4.19291 -4.18868 -4.1744289 -4.1406178 -4.0838308 -4.0250673 -4.0059128 -4.0418286 -4.0980921 -4.1457777 -4.1721125 -4.186583 -4.1959624 -4.2076454][-4.217864 -4.2201858 -4.2273803 -4.2243795 -4.19952 -4.15403 -4.1045413 -4.07999 -4.0977087 -4.1420646 -4.1880531 -4.2130947 -4.2213068 -4.2212648 -4.2183518][-4.22915 -4.2352557 -4.2494574 -4.2576504 -4.245667 -4.2172828 -4.1845317 -4.1637297 -4.1680422 -4.1927543 -4.226078 -4.2448583 -4.2470784 -4.2404156 -4.2283492][-4.2356634 -4.2407155 -4.2560253 -4.2723908 -4.2733941 -4.2614241 -4.2435603 -4.2290554 -4.2263274 -4.2360778 -4.25511 -4.2647133 -4.2612858 -4.2514005 -4.2363443][-4.2370243 -4.2431006 -4.2609329 -4.2836504 -4.29339 -4.2909656 -4.2804852 -4.2703424 -4.2655711 -4.266016 -4.2719116 -4.2723646 -4.2638488 -4.2515473 -4.23762][-4.238152 -4.2498708 -4.269053 -4.2921553 -4.3032079 -4.3016963 -4.2919269 -4.2836862 -4.2801852 -4.2791343 -4.277544 -4.2710328 -4.2590933 -4.2471008 -4.2357903][-4.2457786 -4.2608232 -4.2795868 -4.2994409 -4.3087206 -4.30531 -4.2935119 -4.2850752 -4.2843103 -4.2855515 -4.2829309 -4.2739639 -4.2596712 -4.2481856 -4.2375927]]...]
INFO - root - 2017-12-07 17:59:32.324162: step 14510, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.132 sec/batch; 40h:37m:50s remains)
INFO - root - 2017-12-07 17:59:53.203561: step 14520, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.139 sec/batch; 40h:45m:38s remains)
INFO - root - 2017-12-07 18:00:14.721568: step 14530, loss = 2.08, batch loss = 2.02 (14.5 examples/sec; 2.200 sec/batch; 41h:54m:57s remains)
INFO - root - 2017-12-07 18:00:35.818458: step 14540, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.079 sec/batch; 39h:36m:27s remains)
INFO - root - 2017-12-07 18:00:56.742331: step 14550, loss = 2.08, batch loss = 2.02 (15.8 examples/sec; 2.030 sec/batch; 38h:40m:31s remains)
INFO - root - 2017-12-07 18:01:17.954739: step 14560, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.112 sec/batch; 40h:13m:29s remains)
INFO - root - 2017-12-07 18:01:39.111267: step 14570, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.181 sec/batch; 41h:32m:16s remains)
INFO - root - 2017-12-07 18:02:00.122547: step 14580, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.084 sec/batch; 39h:40m:24s remains)
INFO - root - 2017-12-07 18:02:21.143664: step 14590, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.080 sec/batch; 39h:35m:24s remains)
INFO - root - 2017-12-07 18:02:42.242513: step 14600, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.140 sec/batch; 40h:44m:09s remains)
2017-12-07 18:02:43.762770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3576536 -4.3598056 -4.3555179 -4.340045 -4.3180118 -4.3030429 -4.3039441 -4.3199077 -4.3379126 -4.350522 -4.3554535 -4.3541703 -4.351882 -4.3508463 -4.3511219][-4.3640728 -4.363667 -4.3532462 -4.3257942 -4.2870173 -4.2561517 -4.251749 -4.2765813 -4.3108788 -4.3399439 -4.355309 -4.3582687 -4.3563313 -4.3530803 -4.3514357][-4.3674746 -4.3647957 -4.346806 -4.3035736 -4.2394247 -4.181776 -4.1627903 -4.1949148 -4.2516527 -4.3051934 -4.3399849 -4.3546662 -4.3579054 -4.3556056 -4.3528972][-4.3673034 -4.3655386 -4.3437128 -4.2860346 -4.1925759 -4.1008158 -4.0588884 -4.0942469 -4.1735773 -4.2538552 -4.3112264 -4.342154 -4.3548017 -4.3556633 -4.35368][-4.36206 -4.3655062 -4.3454247 -4.2777681 -4.1572623 -4.0299063 -3.957041 -3.9917617 -4.0942726 -4.1998405 -4.2756209 -4.3209505 -4.3446 -4.3521037 -4.3533959][-4.3524537 -4.3619175 -4.3460116 -4.2746911 -4.1355238 -3.9754183 -3.8659093 -3.8887792 -4.0119052 -4.1439552 -4.2361336 -4.2951937 -4.3309369 -4.3465929 -4.351923][-4.3471675 -4.3622689 -4.3518877 -4.2856593 -4.1436906 -3.9646428 -3.823694 -3.8229737 -3.949229 -4.0946469 -4.1960545 -4.2641726 -4.311307 -4.3358607 -4.3458247][-4.3502197 -4.3696384 -4.3658609 -4.3139119 -4.1921921 -4.028327 -3.8922486 -3.8729854 -3.9696491 -4.0972605 -4.1903811 -4.2564607 -4.3048377 -4.3312106 -4.3418069][-4.3546772 -4.3772745 -4.3802814 -4.3455634 -4.2547235 -4.1250129 -4.01416 -3.986392 -4.0465121 -4.1412544 -4.2158961 -4.2711244 -4.3128076 -4.3353243 -4.3428588][-4.35096 -4.3749318 -4.3845758 -4.3663793 -4.3073964 -4.2148471 -4.1272559 -4.0936666 -4.126646 -4.19393 -4.2528691 -4.2980876 -4.3311749 -4.3474879 -4.3504829][-4.3360157 -4.3614807 -4.3783917 -4.3772907 -4.3480096 -4.2910061 -4.2258873 -4.1900134 -4.2031136 -4.2483759 -4.2924767 -4.3264871 -4.3494582 -4.3592429 -4.3587246][-4.3163719 -4.3417864 -4.3633122 -4.3762836 -4.3718796 -4.3435831 -4.3007693 -4.2686253 -4.2680635 -4.2937741 -4.3239331 -4.3479905 -4.3613191 -4.3641105 -4.3607841][-4.3038464 -4.3240747 -4.3453426 -4.3645411 -4.37371 -4.3641777 -4.3398485 -4.3179488 -4.3121333 -4.3227463 -4.3379974 -4.3516688 -4.3575568 -4.3554358 -4.3495383][-4.3014317 -4.3121333 -4.3277922 -4.3449597 -4.356236 -4.3548703 -4.3436842 -4.3320956 -4.3276515 -4.3302803 -4.3349128 -4.3376875 -4.3363271 -4.3319221 -4.3253851][-4.3097014 -4.3115315 -4.3183856 -4.32816 -4.3350468 -4.3358746 -4.3326011 -4.3288164 -4.3270059 -4.3268161 -4.325819 -4.3213024 -4.3150859 -4.3082542 -4.299047]]...]
INFO - root - 2017-12-07 18:03:05.011982: step 14610, loss = 2.06, batch loss = 2.01 (15.2 examples/sec; 2.102 sec/batch; 40h:00m:03s remains)
INFO - root - 2017-12-07 18:03:25.983947: step 14620, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.072 sec/batch; 39h:26m:08s remains)
INFO - root - 2017-12-07 18:03:47.459630: step 14630, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 40h:33m:31s remains)
INFO - root - 2017-12-07 18:04:08.734196: step 14640, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.139 sec/batch; 40h:41m:30s remains)
INFO - root - 2017-12-07 18:04:29.527166: step 14650, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.090 sec/batch; 39h:45m:32s remains)
INFO - root - 2017-12-07 18:04:50.981242: step 14660, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.179 sec/batch; 41h:25m:54s remains)
INFO - root - 2017-12-07 18:05:12.210328: step 14670, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.077 sec/batch; 39h:30m:07s remains)
INFO - root - 2017-12-07 18:05:33.022147: step 14680, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 40h:22m:26s remains)
INFO - root - 2017-12-07 18:05:54.083911: step 14690, loss = 2.06, batch loss = 2.00 (15.6 examples/sec; 2.047 sec/batch; 38h:54m:29s remains)
INFO - root - 2017-12-07 18:06:15.415947: step 14700, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.146 sec/batch; 40h:47m:11s remains)
2017-12-07 18:06:16.920864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3235445 -4.3183641 -4.3107829 -4.3019242 -4.297843 -4.2978411 -4.2970915 -4.3011708 -4.3081455 -4.3077722 -4.3056831 -4.3060727 -4.3083458 -4.3148041 -4.3201032][-4.3362679 -4.33224 -4.3219833 -4.3072848 -4.2956495 -4.2894754 -4.2836275 -4.286386 -4.2990966 -4.3021669 -4.2994194 -4.2981734 -4.3023663 -4.3120027 -4.3188734][-4.3494244 -4.3456469 -4.333456 -4.3126936 -4.2899547 -4.2692366 -4.2501607 -4.2450824 -4.2632232 -4.2752471 -4.2746129 -4.2730842 -4.2827096 -4.2971082 -4.3075709][-4.3563151 -4.3498979 -4.3315792 -4.3034186 -4.2700424 -4.2365561 -4.2053576 -4.1919475 -4.212142 -4.2341828 -4.2392964 -4.2365212 -4.2468758 -4.260994 -4.2742767][-4.3510137 -4.3353357 -4.3036628 -4.2610612 -4.2101731 -4.1571903 -4.112896 -4.0999532 -4.1361895 -4.1822686 -4.199769 -4.1970978 -4.2027106 -4.2113466 -4.2219048][-4.3384452 -4.3075781 -4.2561502 -4.1888146 -4.1074419 -4.0200429 -3.9504063 -3.9407623 -4.0105696 -4.0967269 -4.1410222 -4.1494207 -4.1529355 -4.159101 -4.1699419][-4.3283153 -4.2832065 -4.2138085 -4.1231713 -4.0074697 -3.8703849 -3.7461855 -3.7265499 -3.845346 -3.9902945 -4.0761805 -4.1054139 -4.1170807 -4.1270685 -4.1449981][-4.3244057 -4.2734704 -4.1938539 -4.0928583 -3.9655914 -3.8094022 -3.6487498 -3.6074045 -3.7523909 -3.9335432 -4.043829 -4.0902247 -4.1140089 -4.1330957 -4.1595755][-4.3269234 -4.2837939 -4.2106514 -4.1185851 -4.0142841 -3.8975127 -3.7768531 -3.7368345 -3.8388314 -3.9840443 -4.079752 -4.123991 -4.1527305 -4.1790586 -4.2091341][-4.33501 -4.3048582 -4.2501531 -4.1783223 -4.108233 -4.0433373 -3.9782939 -3.9527307 -4.0105677 -4.10194 -4.16102 -4.1882219 -4.2149167 -4.2461114 -4.273109][-4.3417497 -4.3237185 -4.2901378 -4.2456231 -4.2049565 -4.1729388 -4.1419234 -4.1306725 -4.1651378 -4.2194142 -4.2511816 -4.2639351 -4.28072 -4.3077297 -4.3295736][-4.3400893 -4.3275151 -4.3072352 -4.28514 -4.2652969 -4.2537694 -4.243721 -4.2407527 -4.2627473 -4.2963214 -4.313241 -4.3170147 -4.3229723 -4.3404384 -4.3559542][-4.3359461 -4.3248711 -4.308692 -4.2958593 -4.2881308 -4.2894197 -4.2910995 -4.2938046 -4.3093419 -4.3305235 -4.338335 -4.3375564 -4.3382535 -4.3471079 -4.3566966][-4.3347158 -4.324584 -4.3108659 -4.3009095 -4.2968893 -4.3002119 -4.3048811 -4.309196 -4.3180375 -4.32975 -4.3355284 -4.3356233 -4.3352251 -4.3400207 -4.3456936][-4.3388448 -4.3315439 -4.3214788 -4.3132424 -4.30967 -4.3107858 -4.3130684 -4.3147573 -4.317275 -4.3212762 -4.3252077 -4.3273997 -4.328505 -4.331635 -4.3353815]]...]
INFO - root - 2017-12-07 18:06:38.045320: step 14710, loss = 2.07, batch loss = 2.01 (16.2 examples/sec; 1.978 sec/batch; 37h:35m:27s remains)
INFO - root - 2017-12-07 18:06:59.131728: step 14720, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.095 sec/batch; 39h:48m:45s remains)
INFO - root - 2017-12-07 18:07:20.340811: step 14730, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.116 sec/batch; 40h:11m:56s remains)
INFO - root - 2017-12-07 18:07:41.283687: step 14740, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 1.871 sec/batch; 35h:32m:55s remains)
INFO - root - 2017-12-07 18:08:02.240448: step 14750, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.173 sec/batch; 41h:16m:39s remains)
INFO - root - 2017-12-07 18:08:23.465004: step 14760, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 39h:56m:03s remains)
INFO - root - 2017-12-07 18:08:44.851973: step 14770, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.111 sec/batch; 40h:04m:25s remains)
INFO - root - 2017-12-07 18:09:05.630621: step 14780, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.071 sec/batch; 39h:18m:53s remains)
INFO - root - 2017-12-07 18:09:26.572298: step 14790, loss = 2.07, batch loss = 2.02 (15.5 examples/sec; 2.069 sec/batch; 39h:15m:54s remains)
INFO - root - 2017-12-07 18:09:47.849281: step 14800, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.085 sec/batch; 39h:33m:56s remains)
2017-12-07 18:09:49.449824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2646847 -4.2581258 -4.2570786 -4.2560134 -4.2516475 -4.2463326 -4.2411227 -4.238 -4.2333822 -4.2243319 -4.2164884 -4.2206354 -4.2298503 -4.2392964 -4.2572312][-4.23371 -4.2247839 -4.2209992 -4.21743 -4.2119284 -4.2074151 -4.2030444 -4.1995935 -4.1964293 -4.1905141 -4.1855092 -4.1923823 -4.2079892 -4.2255354 -4.250114][-4.205833 -4.1921167 -4.1847548 -4.1786757 -4.17589 -4.1769462 -4.1804767 -4.185709 -4.1916676 -4.1923418 -4.1870904 -4.189662 -4.2060161 -4.228785 -4.2571473][-4.1759872 -4.1564164 -4.1425905 -4.1343012 -4.1337285 -4.1400857 -4.1537428 -4.1716876 -4.19003 -4.1988096 -4.1919436 -4.1877489 -4.2017679 -4.2282486 -4.260591][-4.1398735 -4.1180396 -4.100215 -4.0923748 -4.0926566 -4.1004148 -4.1172919 -4.1404924 -4.1635065 -4.1745768 -4.1681867 -4.1636186 -4.1773834 -4.2055588 -4.2416434][-4.1053939 -4.0925803 -4.082314 -4.0794082 -4.0824265 -4.0888562 -4.0967941 -4.1078143 -4.1207662 -4.1249509 -4.1190219 -4.1189818 -4.1389632 -4.1717997 -4.2131739][-4.0686979 -4.0682673 -4.0732422 -4.0816894 -4.0903215 -4.0946951 -4.0906105 -4.0841889 -4.0835662 -4.0817118 -4.078804 -4.0859146 -4.1150546 -4.1558857 -4.20287][-4.04981 -4.0529089 -4.0633249 -4.0764656 -4.0838547 -4.0799026 -4.064662 -4.0511808 -4.0528059 -4.0580649 -4.0637012 -4.0793285 -4.1157031 -4.1597214 -4.2084846][-4.0585856 -4.0592809 -4.0678906 -4.0808983 -4.0867839 -4.0795 -4.0599513 -4.0472155 -4.0554929 -4.0690494 -4.0770426 -4.0915017 -4.1288056 -4.1725411 -4.2181029][-4.0824952 -4.0829864 -4.0896544 -4.1024737 -4.1102428 -4.1063023 -4.0874906 -4.070962 -4.0746942 -4.0870957 -4.0921755 -4.1023846 -4.1378794 -4.1820827 -4.2259421][-4.1006041 -4.1071153 -4.1143785 -4.1261134 -4.1337404 -4.1309385 -4.1125522 -4.0909047 -4.0841665 -4.0903583 -4.09627 -4.1082063 -4.1425638 -4.1860118 -4.2289662][-4.1321464 -4.1448874 -4.152657 -4.1590743 -4.1616573 -4.1579947 -4.1445084 -4.1271434 -4.11734 -4.1169205 -4.1216187 -4.1354003 -4.1638818 -4.1979961 -4.234448][-4.1722188 -4.18417 -4.1879287 -4.187355 -4.1847391 -4.17971 -4.1726518 -4.1655722 -4.1598463 -4.1550879 -4.1553841 -4.1673851 -4.1889095 -4.2137108 -4.2434058][-4.1994328 -4.2063208 -4.2072372 -4.2044482 -4.1995797 -4.19429 -4.1904669 -4.1875596 -4.1822562 -4.1725245 -4.1671834 -4.17717 -4.1968155 -4.2187266 -4.2473865][-4.209805 -4.212513 -4.2132711 -4.2117987 -4.2085552 -4.2045331 -4.2019091 -4.199337 -4.1924877 -4.179337 -4.1702161 -4.178299 -4.1988325 -4.2220449 -4.2502117]]...]
INFO - root - 2017-12-07 18:10:10.461554: step 14810, loss = 2.06, batch loss = 2.01 (14.5 examples/sec; 2.214 sec/batch; 42h:00m:18s remains)
INFO - root - 2017-12-07 18:10:31.756382: step 14820, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.149 sec/batch; 40h:46m:28s remains)
INFO - root - 2017-12-07 18:10:52.971526: step 14830, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.082 sec/batch; 39h:30m:05s remains)
INFO - root - 2017-12-07 18:11:13.558769: step 14840, loss = 2.07, batch loss = 2.02 (16.7 examples/sec; 1.919 sec/batch; 36h:24m:12s remains)
INFO - root - 2017-12-07 18:11:34.765937: step 14850, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.123 sec/batch; 40h:16m:02s remains)
INFO - root - 2017-12-07 18:11:56.054022: step 14860, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.137 sec/batch; 40h:31m:01s remains)
INFO - root - 2017-12-07 18:12:17.223399: step 14870, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.194 sec/batch; 41h:36m:03s remains)
INFO - root - 2017-12-07 18:12:38.205267: step 14880, loss = 2.06, batch loss = 2.01 (15.4 examples/sec; 2.079 sec/batch; 39h:24m:19s remains)
INFO - root - 2017-12-07 18:12:59.396160: step 14890, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.098 sec/batch; 39h:46m:30s remains)
INFO - root - 2017-12-07 18:13:20.841484: step 14900, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.163 sec/batch; 40h:59m:35s remains)
2017-12-07 18:13:22.388047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2299948 -4.2544646 -4.2819304 -4.3044853 -4.317585 -4.3166556 -4.2954969 -4.2740583 -4.2676344 -4.2662935 -4.26535 -4.2573638 -4.2570605 -4.2740812 -4.2958632][-4.2129226 -4.2349358 -4.2701674 -4.2961583 -4.3043509 -4.2925143 -4.2591171 -4.2284713 -4.222568 -4.2339768 -4.2461886 -4.2519031 -4.2614326 -4.2804384 -4.2988577][-4.2154989 -4.2275677 -4.2566614 -4.2799911 -4.2775145 -4.2484522 -4.2007728 -4.164391 -4.1623707 -4.1915536 -4.2243462 -4.2507029 -4.2699823 -4.2864528 -4.2993507][-4.2362018 -4.2395797 -4.2516351 -4.261631 -4.2446971 -4.1921034 -4.1203055 -4.0798583 -4.0965414 -4.1525249 -4.2097297 -4.2470307 -4.2650118 -4.2752643 -4.2846661][-4.2671385 -4.2629023 -4.2600112 -4.2513132 -4.2173123 -4.1353436 -4.0301194 -3.9894416 -4.0445538 -4.1376023 -4.2117529 -4.2493825 -4.2586937 -4.2601004 -4.2644486][-4.2996454 -4.290349 -4.276979 -4.2512741 -4.2047582 -4.0980921 -3.9434001 -3.880342 -3.9781857 -4.1174512 -4.2088728 -4.2479811 -4.24785 -4.2394218 -4.2405033][-4.3190961 -4.3128777 -4.2967715 -4.2661395 -4.214747 -4.0972834 -3.8947868 -3.7710836 -3.8906574 -4.0815363 -4.1951151 -4.2371521 -4.2256775 -4.2062411 -4.2053328][-4.3258481 -4.324502 -4.3141513 -4.2868404 -4.2384057 -4.1333737 -3.9290762 -3.7545233 -3.8601117 -4.0728436 -4.1895537 -4.2177548 -4.1922951 -4.168129 -4.1689272][-4.32901 -4.3269625 -4.3214426 -4.3058772 -4.2715783 -4.1924686 -4.0384173 -3.8905072 -3.955528 -4.1203761 -4.2034144 -4.2035084 -4.16042 -4.1304975 -4.1314821][-4.3356504 -4.3328977 -4.3280187 -4.3188992 -4.3009753 -4.2497797 -4.1463895 -4.0501719 -4.0861998 -4.1823087 -4.221941 -4.1946363 -4.13538 -4.0953078 -4.0991721][-4.3339295 -4.3275657 -4.3251524 -4.3225384 -4.316195 -4.2874 -4.224052 -4.1669588 -4.1887989 -4.2385244 -4.2519407 -4.2152944 -4.150949 -4.1029811 -4.0993633][-4.3196244 -4.3079877 -4.3050041 -4.3085589 -4.31079 -4.3031673 -4.2715826 -4.2414312 -4.2559476 -4.2802358 -4.2824574 -4.244195 -4.1720495 -4.1204119 -4.1091185][-4.30209 -4.2889042 -4.2823606 -4.2849832 -4.2899671 -4.2971334 -4.2908497 -4.2779789 -4.2863054 -4.2993045 -4.2981648 -4.262814 -4.1914048 -4.1428838 -4.1343527][-4.2850785 -4.2744045 -4.2685714 -4.2664313 -4.2681265 -4.2825356 -4.2916703 -4.289494 -4.2959671 -4.3066769 -4.3091092 -4.2858524 -4.2303305 -4.18753 -4.1794991][-4.26852 -4.2650943 -4.2650161 -4.2629857 -4.2636948 -4.2759309 -4.28861 -4.2906222 -4.2946644 -4.3042917 -4.3150949 -4.3080187 -4.272347 -4.2337093 -4.2236657]]...]
INFO - root - 2017-12-07 18:13:43.021825: step 14910, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.090 sec/batch; 39h:36m:14s remains)
INFO - root - 2017-12-07 18:14:04.386506: step 14920, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.126 sec/batch; 40h:16m:59s remains)
INFO - root - 2017-12-07 18:14:25.699297: step 14930, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 40h:08m:42s remains)
INFO - root - 2017-12-07 18:14:46.605956: step 14940, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 40h:01m:13s remains)
INFO - root - 2017-12-07 18:15:07.725893: step 14950, loss = 2.10, batch loss = 2.04 (15.2 examples/sec; 2.111 sec/batch; 39h:58m:32s remains)
INFO - root - 2017-12-07 18:15:28.964503: step 14960, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.173 sec/batch; 41h:08m:38s remains)
INFO - root - 2017-12-07 18:15:49.736352: step 14970, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.096 sec/batch; 39h:40m:31s remains)
INFO - root - 2017-12-07 18:16:10.881525: step 14980, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.098 sec/batch; 39h:42m:53s remains)
INFO - root - 2017-12-07 18:16:32.217899: step 14990, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 40h:13m:42s remains)
INFO - root - 2017-12-07 18:16:53.243528: step 15000, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.143 sec/batch; 40h:33m:00s remains)
2017-12-07 18:16:54.889091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3254671 -4.3284593 -4.3304305 -4.3310747 -4.3297267 -4.3272014 -4.3246217 -4.3210063 -4.3157244 -4.3118958 -4.3117442 -4.3140097 -4.3168192 -4.320272 -4.3245716][-4.3198233 -4.322124 -4.3237557 -4.3235755 -4.321104 -4.3173304 -4.313673 -4.3089247 -4.3016005 -4.2958255 -4.2951412 -4.299366 -4.3051205 -4.3111849 -4.3172894][-4.3156886 -4.313839 -4.3115768 -4.308176 -4.3018546 -4.2950072 -4.2867794 -4.2776237 -4.267179 -4.2587147 -4.2574134 -4.2649221 -4.2780476 -4.2905254 -4.3003831][-4.3033252 -4.2963786 -4.2890196 -4.2811904 -4.2696738 -4.2568841 -4.2392273 -4.2208362 -4.2046719 -4.1952381 -4.1984162 -4.2141614 -4.2384553 -4.2615957 -4.2763634][-4.2724762 -4.2620683 -4.2515678 -4.236866 -4.2148123 -4.1908107 -4.1632905 -4.1368771 -4.1191368 -4.1162906 -4.1290383 -4.1543808 -4.1906586 -4.2238665 -4.2403994][-4.2157745 -4.2031651 -4.191606 -4.17282 -4.1407 -4.1043983 -4.0706964 -4.03807 -4.0201621 -4.0291982 -4.0555029 -4.092165 -4.1390271 -4.177928 -4.1912684][-4.14355 -4.1264477 -4.1180911 -4.1005573 -4.0635872 -4.0168767 -3.979367 -3.9396923 -3.9232838 -3.9546831 -4.0050225 -4.0553904 -4.1070218 -4.1450205 -4.1491742][-4.0669541 -4.0401788 -4.0369182 -4.0276752 -3.9932094 -3.9406247 -3.9032128 -3.8619292 -3.8562756 -3.9248087 -4.0082979 -4.0709991 -4.1174707 -4.1365409 -4.1110849][-3.9947605 -3.9606738 -3.961771 -3.963896 -3.9439704 -3.9064932 -3.8870378 -3.8670478 -3.8826714 -3.9716606 -4.05972 -4.1074262 -4.1270285 -4.1149316 -4.0602679][-3.9511013 -3.9200177 -3.9243331 -3.9376032 -3.9415021 -3.9387035 -3.9521935 -3.9620667 -3.9945798 -4.069015 -4.1250391 -4.1392508 -4.1275496 -4.0938516 -4.032094][-3.978477 -3.9599371 -3.9649603 -3.9815016 -4.0023541 -4.0241408 -4.0504117 -4.0691128 -4.1031828 -4.1546512 -4.175766 -4.1608505 -4.1257081 -4.089211 -4.0496349][-4.0679784 -4.0568213 -4.0556374 -4.0649996 -4.0831623 -4.1072035 -4.1289911 -4.1433787 -4.1745095 -4.211051 -4.2109575 -4.1784348 -4.134655 -4.10971 -4.1027513][-4.1586552 -4.1457648 -4.1330061 -4.130095 -4.1379123 -4.1595116 -4.1785665 -4.1900263 -4.2171617 -4.2450333 -4.237422 -4.2030892 -4.1636128 -4.1538067 -4.1699476][-4.2145662 -4.1973796 -4.1756887 -4.1635823 -4.1662626 -4.1920314 -4.2170968 -4.2275982 -4.2469482 -4.26389 -4.2542577 -4.2246442 -4.1957064 -4.1984057 -4.2243338][-4.2366896 -4.2216434 -4.2021976 -4.1893506 -4.1950941 -4.2272043 -4.2521749 -4.2521415 -4.253231 -4.2590575 -4.2536278 -4.2386751 -4.2264366 -4.2380695 -4.2614942]]...]
INFO - root - 2017-12-07 18:17:15.999059: step 15010, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.110 sec/batch; 39h:55m:03s remains)
INFO - root - 2017-12-07 18:17:37.144717: step 15020, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.073 sec/batch; 39h:12m:33s remains)
INFO - root - 2017-12-07 18:17:58.323316: step 15030, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.089 sec/batch; 39h:30m:27s remains)
INFO - root - 2017-12-07 18:18:19.058353: step 15040, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.099 sec/batch; 39h:42m:00s remains)
INFO - root - 2017-12-07 18:18:40.403843: step 15050, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.139 sec/batch; 40h:26m:22s remains)
INFO - root - 2017-12-07 18:19:01.627674: step 15060, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 40h:13m:56s remains)
INFO - root - 2017-12-07 18:19:22.269663: step 15070, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.152 sec/batch; 40h:41m:15s remains)
INFO - root - 2017-12-07 18:19:43.484421: step 15080, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.138 sec/batch; 40h:25m:04s remains)
INFO - root - 2017-12-07 18:20:04.727381: step 15090, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 39h:57m:02s remains)
INFO - root - 2017-12-07 18:20:25.525539: step 15100, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.147 sec/batch; 40h:33m:50s remains)
2017-12-07 18:20:27.099281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.336926 -4.3409057 -4.3406687 -4.3373666 -4.3333173 -4.3301859 -4.3272161 -4.3264341 -4.3287277 -4.3326688 -4.3364186 -4.3400283 -4.3431344 -4.3439679 -4.3435946][-4.3343291 -4.3402767 -4.3402638 -4.3331738 -4.3245225 -4.316895 -4.3090544 -4.3027596 -4.3054047 -4.3156767 -4.3257155 -4.3334465 -4.3388009 -4.3392844 -4.3398376][-4.3318868 -4.3407912 -4.3387976 -4.3237143 -4.30446 -4.2860746 -4.2641897 -4.2433977 -4.2467732 -4.2703857 -4.2920036 -4.3072972 -4.3170242 -4.3193469 -4.3233581][-4.3284659 -4.3363905 -4.3292661 -4.3025322 -4.2699623 -4.2375436 -4.1967664 -4.1561389 -4.1607332 -4.2003536 -4.2360129 -4.2604985 -4.2777796 -4.284564 -4.2951832][-4.3253059 -4.3318439 -4.3200536 -4.2832437 -4.2353745 -4.1824727 -4.1184959 -4.0552239 -4.0643344 -4.1231818 -4.1704326 -4.2041378 -4.2300415 -4.2410622 -4.26049][-4.3231254 -4.3271933 -4.3108487 -4.2654476 -4.2038336 -4.131422 -4.0402694 -3.9544268 -3.9748259 -4.0541449 -4.111834 -4.150929 -4.1821113 -4.1985946 -4.2283068][-4.3207774 -4.3238635 -4.3007216 -4.2457247 -4.1705661 -4.0745425 -3.948184 -3.8412273 -3.8847179 -3.9911394 -4.059567 -4.1038828 -4.1407452 -4.1670175 -4.2064066][-4.3176513 -4.3196797 -4.2903743 -4.2326803 -4.14864 -4.0288939 -3.8677158 -3.7489729 -3.8244829 -3.9540591 -4.0305605 -4.0779023 -4.1213856 -4.1548071 -4.199194][-4.3179679 -4.3190274 -4.2882981 -4.23806 -4.158958 -4.0395508 -3.8886895 -3.7974558 -3.8726625 -3.9884119 -4.0534706 -4.0944366 -4.1344628 -4.1646428 -4.205265][-4.3214417 -4.323113 -4.2960038 -4.2583528 -4.1934538 -4.0963979 -3.9909306 -3.9371829 -3.9846108 -4.0623455 -4.1067634 -4.1395078 -4.1709266 -4.1909919 -4.2207618][-4.3236842 -4.3261747 -4.3066349 -4.2816553 -4.2357063 -4.1646938 -4.0986805 -4.0681672 -4.0890841 -4.1341062 -4.1636882 -4.189744 -4.2100554 -4.2196631 -4.23759][-4.3261061 -4.3281469 -4.3151264 -4.2993255 -4.2688136 -4.2187166 -4.1764264 -4.1581411 -4.1652083 -4.1904168 -4.212122 -4.2296386 -4.2427139 -4.2454433 -4.2545328][-4.3287621 -4.3325787 -4.3260059 -4.3181863 -4.2996149 -4.266624 -4.2394233 -4.2270265 -4.2286391 -4.241693 -4.252244 -4.2588267 -4.2657228 -4.2668414 -4.2733431][-4.3302612 -4.3361449 -4.3340306 -4.3303142 -4.3185644 -4.29803 -4.28187 -4.2761307 -4.2768321 -4.2825403 -4.2823138 -4.278904 -4.279119 -4.28142 -4.2897558][-4.3294945 -4.3352876 -4.3353481 -4.3330379 -4.3239965 -4.3114886 -4.3033767 -4.3028851 -4.3057642 -4.3079109 -4.3037076 -4.2962027 -4.2933631 -4.2956104 -4.3045387]]...]
INFO - root - 2017-12-07 18:20:48.197271: step 15110, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 40h:19m:53s remains)
INFO - root - 2017-12-07 18:21:09.488022: step 15120, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 40h:04m:32s remains)
INFO - root - 2017-12-07 18:21:30.553080: step 15130, loss = 2.09, batch loss = 2.03 (15.8 examples/sec; 2.021 sec/batch; 38h:10m:34s remains)
INFO - root - 2017-12-07 18:21:51.735035: step 15140, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 39h:38m:37s remains)
INFO - root - 2017-12-07 18:22:12.934768: step 15150, loss = 2.06, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 40h:06m:11s remains)
INFO - root - 2017-12-07 18:22:33.832977: step 15160, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 1.925 sec/batch; 36h:20m:06s remains)
INFO - root - 2017-12-07 18:22:54.853066: step 15170, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.171 sec/batch; 40h:58m:20s remains)
INFO - root - 2017-12-07 18:23:16.121379: step 15180, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 40h:06m:19s remains)
INFO - root - 2017-12-07 18:23:37.389263: step 15190, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.173 sec/batch; 41h:00m:14s remains)
INFO - root - 2017-12-07 18:23:58.169462: step 15200, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.092 sec/batch; 39h:27m:52s remains)
2017-12-07 18:23:59.702848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1533895 -4.1525149 -4.1641417 -4.17976 -4.1842923 -4.1607504 -4.1130657 -4.0878968 -4.1145539 -4.1805382 -4.2494326 -4.2966118 -4.3282928 -4.3476782 -4.3594966][-4.138298 -4.13818 -4.1544518 -4.1749225 -4.1817193 -4.1518488 -4.0898943 -4.0543246 -4.0875082 -4.1610122 -4.2347031 -4.2844958 -4.3200822 -4.3425426 -4.3564696][-4.1341529 -4.1375 -4.1567173 -4.1777534 -4.1820874 -4.1442513 -4.0666628 -4.0253077 -4.0666018 -4.1436672 -4.2193937 -4.2727365 -4.3126392 -4.3371348 -4.3526363][-4.1312652 -4.1416597 -4.1676087 -4.1845655 -4.1733985 -4.11364 -4.0146332 -3.9745264 -4.0354338 -4.1259828 -4.2086291 -4.2673554 -4.3093839 -4.3347583 -4.3507905][-4.136075 -4.1467838 -4.1727095 -4.1775317 -4.1413217 -4.0499468 -3.9241467 -3.8959961 -3.9903166 -4.1044679 -4.2033215 -4.2695293 -4.3114061 -4.3356252 -4.3510766][-4.1460161 -4.1536016 -4.1758661 -4.1670065 -4.1103458 -3.9922194 -3.8445556 -3.8282046 -3.9496439 -4.0876656 -4.2025933 -4.2768521 -4.3182793 -4.3396983 -4.3526893][-4.179172 -4.1776075 -4.1892476 -4.1726933 -4.1181 -4.0034847 -3.8574352 -3.8289568 -3.9397726 -4.0817394 -4.2037792 -4.283843 -4.3244243 -4.3444943 -4.3553123][-4.2317772 -4.2195163 -4.2195482 -4.201046 -4.1585822 -4.065907 -3.9451365 -3.8984654 -3.9645188 -4.0863848 -4.2063737 -4.2884536 -4.3296337 -4.3490453 -4.3576045][-4.2704644 -4.259654 -4.2591968 -4.2420087 -4.2036772 -4.1250434 -4.0283394 -3.9798403 -4.0074248 -4.0991631 -4.2064834 -4.2866383 -4.3294449 -4.3497143 -4.3580055][-4.2970634 -4.2969904 -4.2992296 -4.2850704 -4.2521515 -4.1797729 -4.0916042 -4.0461626 -4.0557065 -4.1217222 -4.2106419 -4.2806163 -4.3232274 -4.3460116 -4.3563232][-4.3066816 -4.3118992 -4.3162518 -4.3100896 -4.2862034 -4.2213306 -4.1324158 -4.087121 -4.0918922 -4.1468382 -4.2220287 -4.2806306 -4.3189125 -4.3418493 -4.3532848][-4.3120317 -4.3157306 -4.3180437 -4.3168793 -4.3032751 -4.2508841 -4.1650443 -4.1146259 -4.1167555 -4.1679263 -4.2354913 -4.286448 -4.317039 -4.336237 -4.3485856][-4.323441 -4.3257709 -4.3238096 -4.3197823 -4.3094678 -4.2697148 -4.1929631 -4.1396027 -4.1372366 -4.1810808 -4.2420182 -4.2908983 -4.3172669 -4.3315024 -4.3443694][-4.3206906 -4.3213496 -4.3180594 -4.3147573 -4.3069348 -4.2789512 -4.2193155 -4.1744223 -4.1698775 -4.2018166 -4.2540317 -4.2988453 -4.3230743 -4.3342919 -4.3445535][-4.31336 -4.3142791 -4.3129392 -4.3119464 -4.31033 -4.2909384 -4.2478161 -4.215354 -4.2136312 -4.2380891 -4.2821345 -4.3200431 -4.3389072 -4.3457708 -4.350585]]...]
INFO - root - 2017-12-07 18:24:21.040176: step 15210, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.186 sec/batch; 41h:14m:32s remains)
INFO - root - 2017-12-07 18:24:42.260656: step 15220, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.110 sec/batch; 39h:47m:26s remains)
INFO - root - 2017-12-07 18:25:02.742558: step 15230, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 39h:59m:28s remains)
INFO - root - 2017-12-07 18:25:24.054852: step 15240, loss = 2.06, batch loss = 2.00 (14.8 examples/sec; 2.165 sec/batch; 40h:49m:49s remains)
INFO - root - 2017-12-07 18:25:45.359080: step 15250, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.106 sec/batch; 39h:42m:12s remains)
INFO - root - 2017-12-07 18:26:06.355962: step 15260, loss = 2.08, batch loss = 2.02 (16.0 examples/sec; 1.999 sec/batch; 37h:41m:28s remains)
INFO - root - 2017-12-07 18:26:27.523236: step 15270, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.102 sec/batch; 39h:37m:05s remains)
INFO - root - 2017-12-07 18:26:48.762248: step 15280, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.085 sec/batch; 39h:18m:02s remains)
INFO - root - 2017-12-07 18:27:09.800141: step 15290, loss = 2.08, batch loss = 2.02 (16.0 examples/sec; 2.003 sec/batch; 37h:44m:39s remains)
INFO - root - 2017-12-07 18:27:30.661690: step 15300, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.135 sec/batch; 40h:13m:45s remains)
2017-12-07 18:27:32.230934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2561045 -4.2582774 -4.2596188 -4.2623816 -4.2668614 -4.2740574 -4.2814789 -4.2887745 -4.2876267 -4.275641 -4.2634954 -4.2528496 -4.245903 -4.2395592 -4.2395949][-4.269896 -4.2705755 -4.2729721 -4.2787414 -4.28351 -4.2879958 -4.2904072 -4.2910762 -4.2826571 -4.2639093 -4.2461953 -4.2312703 -4.2215314 -4.2158885 -4.2164879][-4.2740126 -4.2762766 -4.2794933 -4.2854466 -4.2860794 -4.2796016 -4.2717843 -4.2659273 -4.2569594 -4.2423606 -4.2295952 -4.2201781 -4.2151933 -4.2128892 -4.2123885][-4.2646127 -4.2686434 -4.2696204 -4.2724643 -4.2663913 -4.2434306 -4.2208 -4.2117958 -4.20792 -4.2062817 -4.2102737 -4.2175026 -4.2256966 -4.2313337 -4.2311959][-4.2518439 -4.2553072 -4.2482815 -4.2418194 -4.224977 -4.1836662 -4.1464639 -4.1382365 -4.1453485 -4.160048 -4.1891794 -4.2183547 -4.2401443 -4.2535772 -4.2553215][-4.2393823 -4.2378488 -4.2206721 -4.2005243 -4.1668525 -4.10376 -4.0490317 -4.0472531 -4.079309 -4.1177058 -4.1675072 -4.2081332 -4.236331 -4.2536449 -4.2599192][-4.2284431 -4.2163835 -4.1876726 -4.1533737 -4.1038384 -4.0185146 -3.9449325 -3.9558814 -4.0257559 -4.0973835 -4.1607742 -4.2002397 -4.2230062 -4.2358894 -4.2457][-4.209167 -4.189271 -4.1576662 -4.1212907 -4.074172 -3.9916861 -3.9153571 -3.9324236 -4.02093 -4.104723 -4.1652551 -4.1970315 -4.2102733 -4.2172 -4.2295337][-4.188427 -4.1666203 -4.1427631 -4.1190391 -4.095099 -4.0526423 -4.0127187 -4.0287285 -4.091352 -4.1480885 -4.1872935 -4.2069483 -4.212379 -4.2153111 -4.2274804][-4.184402 -4.1632142 -4.1493988 -4.1422343 -4.1433182 -4.1398239 -4.1347342 -4.14842 -4.1800575 -4.2048941 -4.2196321 -4.2263722 -4.2269006 -4.2259312 -4.2343206][-4.1861329 -4.1653032 -4.157485 -4.1606011 -4.1760812 -4.1960988 -4.211494 -4.2241244 -4.2387218 -4.2483325 -4.2505751 -4.2497606 -4.2471819 -4.2430072 -4.2465687][-4.2008929 -4.1832914 -4.1789961 -4.1871443 -4.2085147 -4.2372875 -4.2570443 -4.2646985 -4.2680869 -4.2694883 -4.2683816 -4.2656765 -4.2616768 -4.2561469 -4.2565117][-4.2128711 -4.2024155 -4.2041359 -4.2148924 -4.2337766 -4.2569194 -4.2689919 -4.2673244 -4.2611032 -4.2606406 -4.2654209 -4.2679014 -4.2663078 -4.2626138 -4.2621613][-4.2258658 -4.2214117 -4.2267 -4.2380466 -4.2502294 -4.2614875 -4.264184 -4.2565446 -4.2452588 -4.245327 -4.2574468 -4.2671933 -4.2691894 -4.2685971 -4.2709126][-4.2499952 -4.25016 -4.255722 -4.2644958 -4.2693224 -4.2697821 -4.2671375 -4.2620525 -4.2545395 -4.2563987 -4.2698879 -4.2797389 -4.2811384 -4.28387 -4.2907968]]...]
INFO - root - 2017-12-07 18:27:53.496920: step 15310, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.108 sec/batch; 39h:42m:36s remains)
INFO - root - 2017-12-07 18:28:14.678029: step 15320, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.152 sec/batch; 40h:32m:05s remains)
INFO - root - 2017-12-07 18:28:35.298043: step 15330, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 39h:32m:07s remains)
INFO - root - 2017-12-07 18:28:56.761588: step 15340, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 40h:04m:25s remains)
INFO - root - 2017-12-07 18:29:17.895822: step 15350, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.145 sec/batch; 40h:22m:24s remains)
INFO - root - 2017-12-07 18:29:38.668249: step 15360, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.106 sec/batch; 39h:38m:55s remains)
INFO - root - 2017-12-07 18:29:59.834051: step 15370, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.147 sec/batch; 40h:24m:36s remains)
INFO - root - 2017-12-07 18:30:21.051894: step 15380, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.185 sec/batch; 41h:07m:25s remains)
INFO - root - 2017-12-07 18:30:42.027848: step 15390, loss = 2.07, batch loss = 2.01 (16.4 examples/sec; 1.951 sec/batch; 36h:42m:50s remains)
INFO - root - 2017-12-07 18:31:03.430550: step 15400, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 40h:04m:27s remains)
2017-12-07 18:31:04.990733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2363529 -4.2189345 -4.1965246 -4.1914654 -4.2057652 -4.2215147 -4.2286105 -4.2285657 -4.2235537 -4.2200813 -4.2178073 -4.2147255 -4.2193646 -4.2198339 -4.2140951][-4.2462549 -4.234437 -4.2195258 -4.223906 -4.2429175 -4.2556715 -4.2539086 -4.2432642 -4.2304521 -4.2209029 -4.2133188 -4.2086716 -4.2140627 -4.216383 -4.2148886][-4.2349572 -4.2268262 -4.2193179 -4.2297983 -4.2513409 -4.2613821 -4.2522235 -4.2334714 -4.2149429 -4.2045145 -4.1971192 -4.195179 -4.202126 -4.2046533 -4.2020235][-4.1985841 -4.1849089 -4.180696 -4.1959672 -4.220706 -4.2306266 -4.2183795 -4.1920862 -4.1630754 -4.151309 -4.149004 -4.1560254 -4.170135 -4.176477 -4.1753621][-4.1433587 -4.1151285 -4.1038914 -4.1202517 -4.148355 -4.1612182 -4.1467466 -4.1085958 -4.0680809 -4.0568638 -4.0676227 -4.0915318 -4.1211381 -4.1368203 -4.1410422][-4.0723877 -4.0322142 -4.0168762 -4.0331392 -4.0586658 -4.0662074 -4.0421138 -3.9878464 -3.9402058 -3.9425964 -3.9733322 -4.0206351 -4.075038 -4.1072373 -4.1202183][-4.0338039 -3.9976079 -3.9870872 -4.0014563 -4.015059 -4.0093226 -3.977113 -3.9164505 -3.870672 -3.8791485 -3.913619 -3.9720476 -4.0428572 -4.0876675 -4.1078591][-4.0408068 -4.0120564 -4.0089192 -4.024241 -4.0325856 -4.025423 -4.0043821 -3.9665976 -3.9383855 -3.9387758 -3.9543 -3.9979172 -4.0553136 -4.0892248 -4.1005068][-4.0877328 -4.065937 -4.0678906 -4.0832148 -4.0894985 -4.0837374 -4.0702796 -4.0496178 -4.0353541 -4.0303664 -4.0326657 -4.0576773 -4.0916376 -4.1032271 -4.0972013][-4.1580048 -4.1407652 -4.1427097 -4.1534491 -4.1584959 -4.1534462 -4.142415 -4.1282787 -4.1186938 -4.1103134 -4.1054559 -4.1175985 -4.1343989 -4.1296015 -4.1080737][-4.2154531 -4.1996083 -4.1983461 -4.2043862 -4.2086291 -4.2070494 -4.1997004 -4.1889634 -4.1792841 -4.1677852 -4.1588249 -4.1630979 -4.1682258 -4.1558785 -4.1334286][-4.2546625 -4.2409306 -4.2378845 -4.2409382 -4.2440419 -4.24468 -4.2402086 -4.2288966 -4.2140231 -4.198132 -4.1881895 -4.1866741 -4.1813245 -4.1634011 -4.143569][-4.2836523 -4.2721505 -4.269249 -4.2704463 -4.2721844 -4.2731504 -4.2693925 -4.2572441 -4.2396 -4.2197118 -4.2049313 -4.1951056 -4.1822281 -4.16504 -4.1515174][-4.2989888 -4.2910128 -4.2904668 -4.292347 -4.2936096 -4.2940664 -4.2905965 -4.2796025 -4.2622509 -4.2399197 -4.2202935 -4.2038012 -4.1856918 -4.1722608 -4.1687684][-4.3041191 -4.2998209 -4.3017673 -4.3048248 -4.3052926 -4.3038225 -4.2996039 -4.2906947 -4.2765522 -4.257576 -4.2394929 -4.2243648 -4.2078032 -4.1973009 -4.19764]]...]
INFO - root - 2017-12-07 18:31:26.187168: step 15410, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 40h:03m:30s remains)
INFO - root - 2017-12-07 18:31:47.394830: step 15420, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.106 sec/batch; 39h:35m:57s remains)
INFO - root - 2017-12-07 18:32:08.447084: step 15430, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 39h:45m:21s remains)
INFO - root - 2017-12-07 18:32:29.725809: step 15440, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.154 sec/batch; 40h:29m:30s remains)
INFO - root - 2017-12-07 18:32:51.069164: step 15450, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.151 sec/batch; 40h:26m:38s remains)
INFO - root - 2017-12-07 18:33:12.027501: step 15460, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 39h:59m:36s remains)
INFO - root - 2017-12-07 18:33:33.324579: step 15470, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.112 sec/batch; 39h:41m:47s remains)
INFO - root - 2017-12-07 18:33:54.785134: step 15480, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.191 sec/batch; 41h:09m:59s remains)
INFO - root - 2017-12-07 18:34:15.685352: step 15490, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.100 sec/batch; 39h:27m:08s remains)
INFO - root - 2017-12-07 18:34:36.932343: step 15500, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.110 sec/batch; 39h:38m:08s remains)
2017-12-07 18:34:38.527826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3275256 -4.3163371 -4.2981191 -4.2809677 -4.2626262 -4.2600565 -4.2831316 -4.3129215 -4.3306117 -4.3391495 -4.345067 -4.3452749 -4.3401642 -4.334362 -4.3324232][-4.3253174 -4.3113308 -4.2872934 -4.2658629 -4.2426009 -4.23491 -4.2594337 -4.2935724 -4.315383 -4.326231 -4.3343334 -4.336885 -4.3339458 -4.3298187 -4.327498][-4.3237114 -4.3058996 -4.2736554 -4.2445827 -4.2173748 -4.2091818 -4.2366495 -4.2715859 -4.2942586 -4.3076954 -4.3167868 -4.3190446 -4.3148503 -4.3082929 -4.3065639][-4.320281 -4.2970185 -4.2557859 -4.2173243 -4.1831136 -4.174242 -4.209362 -4.250319 -4.2766485 -4.2924037 -4.3010106 -4.3025155 -4.2963104 -4.286612 -4.2842417][-4.3128438 -4.2821803 -4.2298236 -4.1775212 -4.1310406 -4.1157079 -4.1580243 -4.2122397 -4.250392 -4.2734356 -4.284524 -4.2885857 -4.2833805 -4.2711606 -4.2678986][-4.3083763 -4.2717023 -4.206851 -4.1365209 -4.071528 -4.0412354 -4.0856209 -4.1559868 -4.2079124 -4.2395582 -4.2575908 -4.2667193 -4.2656097 -4.2546177 -4.2515154][-4.3127608 -4.2750611 -4.2016668 -4.1152778 -4.0269742 -3.9699683 -4.0009561 -4.0793567 -4.1427221 -4.1854362 -4.2132668 -4.229845 -4.2335868 -4.2261038 -4.2260509][-4.3228621 -4.2891345 -4.2165422 -4.1295214 -4.0319214 -3.9523644 -3.9565818 -4.0193162 -4.0765367 -4.1219592 -4.1578422 -4.1820788 -4.1927042 -4.191236 -4.1958265][-4.3338923 -4.3077059 -4.2451673 -4.1734095 -4.0896311 -4.0136228 -4.0011296 -4.0345011 -4.0643559 -4.0930138 -4.1207833 -4.1405659 -4.1513109 -4.1559453 -4.1677837][-4.3432479 -4.3245754 -4.2746439 -4.22067 -4.1574903 -4.1013622 -4.0909438 -4.1073074 -4.1153831 -4.1245294 -4.1339979 -4.1358442 -4.1356955 -4.1407666 -4.1565924][-4.3490992 -4.3358264 -4.2968726 -4.2546186 -4.2074351 -4.1706023 -4.1676641 -4.1777248 -4.1759138 -4.1752143 -4.1750827 -4.1640654 -4.1532907 -4.1565657 -4.1735191][-4.3498812 -4.3380179 -4.3058586 -4.2709227 -4.2353625 -4.2125545 -4.2167053 -4.2269297 -4.2229586 -4.2194266 -4.2175317 -4.203589 -4.1877637 -4.1853876 -4.1988673][-4.3488545 -4.3374352 -4.310462 -4.280633 -4.2525129 -4.2397218 -4.2496133 -4.2634749 -4.2623982 -4.2588606 -4.2534742 -4.2411361 -4.2255712 -4.2180924 -4.2242165][-4.3494473 -4.3383384 -4.3158069 -4.2899804 -4.2666965 -4.2611551 -4.2744627 -4.2895856 -4.2924032 -4.2902379 -4.2848144 -4.2758012 -4.2657471 -4.2570977 -4.2533536][-4.3518019 -4.339045 -4.317131 -4.2915678 -4.2695403 -4.2683144 -4.2854786 -4.3025646 -4.309257 -4.311244 -4.3106208 -4.3057609 -4.2999907 -4.2920742 -4.2827492]]...]
INFO - root - 2017-12-07 18:34:59.652135: step 15510, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 39h:58m:18s remains)
INFO - root - 2017-12-07 18:35:20.621083: step 15520, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.155 sec/batch; 40h:28m:28s remains)
INFO - root - 2017-12-07 18:35:41.732566: step 15530, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.123 sec/batch; 39h:51m:13s remains)
INFO - root - 2017-12-07 18:36:02.945357: step 15540, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.103 sec/batch; 39h:29m:02s remains)
INFO - root - 2017-12-07 18:36:24.048127: step 15550, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.144 sec/batch; 40h:14m:57s remains)
INFO - root - 2017-12-07 18:36:45.280166: step 15560, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.121 sec/batch; 39h:48m:19s remains)
INFO - root - 2017-12-07 18:37:06.463407: step 15570, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.184 sec/batch; 40h:58m:41s remains)
INFO - root - 2017-12-07 18:37:27.506392: step 15580, loss = 2.09, batch loss = 2.03 (16.2 examples/sec; 1.971 sec/batch; 36h:58m:29s remains)
INFO - root - 2017-12-07 18:37:48.348976: step 15590, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.098 sec/batch; 39h:21m:14s remains)
INFO - root - 2017-12-07 18:38:09.622585: step 15600, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 39h:46m:15s remains)
2017-12-07 18:38:11.247678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2805672 -4.2819533 -4.2860537 -4.2897573 -4.292119 -4.2934995 -4.2942433 -4.292572 -4.29026 -4.290134 -4.2925029 -4.2946649 -4.2966638 -4.2975755 -4.29661][-4.2763295 -4.2729473 -4.2740159 -4.2773695 -4.2813368 -4.2856641 -4.2897158 -4.2888417 -4.2855592 -4.28579 -4.2903948 -4.2948065 -4.2987075 -4.3009973 -4.300643][-4.2740831 -4.2615724 -4.2565932 -4.2579217 -4.2612414 -4.2663655 -4.2717271 -4.2718215 -4.2706952 -4.2729187 -4.2793961 -4.2863379 -4.29322 -4.2975326 -4.29949][-4.2793393 -4.2614765 -4.2496014 -4.2446847 -4.245316 -4.2502322 -4.2518735 -4.2481136 -4.2486782 -4.252327 -4.258678 -4.2667704 -4.2759719 -4.2844472 -4.2909565][-4.2876496 -4.2695475 -4.2524 -4.2392688 -4.2328744 -4.2334385 -4.2319956 -4.2238035 -4.2252994 -4.2325859 -4.2399421 -4.2488914 -4.2592392 -4.2698588 -4.279644][-4.293468 -4.2726927 -4.24861 -4.2255864 -4.20494 -4.1897092 -4.1732183 -4.1567459 -4.1687603 -4.1928525 -4.2093811 -4.2256007 -4.24132 -4.2530246 -4.2639861][-4.2865648 -4.2617378 -4.2246947 -4.1846933 -4.1419325 -4.0969791 -4.0476418 -4.0126495 -4.04356 -4.1057076 -4.1492934 -4.1800871 -4.2053041 -4.2223735 -4.2362108][-4.2684746 -4.24331 -4.1985588 -4.143064 -4.0797286 -4.0025635 -3.9070523 -3.8486092 -3.9039497 -4.0128813 -4.0920029 -4.1419539 -4.1805243 -4.2042093 -4.2199121][-4.273459 -4.2555075 -4.2167616 -4.1635947 -4.1001697 -4.0216293 -3.9196548 -3.8586431 -3.9131761 -4.0223165 -4.1040273 -4.15702 -4.2010565 -4.2292852 -4.2442174][-4.2969828 -4.2877393 -4.264544 -4.2288241 -4.1869926 -4.1393056 -4.0805297 -4.0418458 -4.0673008 -4.1308732 -4.1844277 -4.2204962 -4.2519479 -4.2722545 -4.2812839][-4.3183107 -4.3158407 -4.3056583 -4.2847066 -4.2594624 -4.2365737 -4.2133965 -4.1967154 -4.207088 -4.2356462 -4.2594476 -4.2725382 -4.2828579 -4.2880797 -4.2884631][-4.3064132 -4.3068242 -4.3055787 -4.2953439 -4.2813621 -4.2722006 -4.2637734 -4.2535677 -4.2546287 -4.2665062 -4.2747192 -4.2744656 -4.2727675 -4.2681031 -4.2642393][-4.253202 -4.2523332 -4.2531381 -4.2509842 -4.248138 -4.2469387 -4.2432685 -4.2370653 -4.23593 -4.2423859 -4.2461796 -4.2417512 -4.2364068 -4.2309318 -4.2305522][-4.1983581 -4.19455 -4.1940417 -4.1979342 -4.20318 -4.2091618 -4.2120657 -4.215055 -4.2193918 -4.2283549 -4.2342906 -4.2322707 -4.2281961 -4.2247453 -4.2262111][-4.1886415 -4.1828427 -4.1819396 -4.1897349 -4.1981039 -4.2075477 -4.2155643 -4.2245607 -4.2348695 -4.2476492 -4.2583547 -4.2612567 -4.25767 -4.2507448 -4.2456703]]...]
INFO - root - 2017-12-07 18:38:32.525338: step 15610, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.158 sec/batch; 40h:28m:08s remains)
INFO - root - 2017-12-07 18:38:53.397044: step 15620, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.137 sec/batch; 40h:04m:43s remains)
INFO - root - 2017-12-07 18:39:14.529537: step 15630, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 39h:15m:40s remains)
INFO - root - 2017-12-07 18:39:35.845986: step 15640, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.133 sec/batch; 39h:59m:27s remains)
INFO - root - 2017-12-07 18:39:56.793379: step 15650, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.129 sec/batch; 39h:54m:03s remains)
INFO - root - 2017-12-07 18:40:17.900745: step 15660, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 39h:27m:22s remains)
INFO - root - 2017-12-07 18:40:39.108061: step 15670, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.167 sec/batch; 40h:35m:52s remains)
INFO - root - 2017-12-07 18:41:00.261706: step 15680, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 1.939 sec/batch; 36h:20m:01s remains)
INFO - root - 2017-12-07 18:41:21.323401: step 15690, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.122 sec/batch; 39h:45m:05s remains)
INFO - root - 2017-12-07 18:41:42.866003: step 15700, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.109 sec/batch; 39h:29m:28s remains)
2017-12-07 18:41:44.507282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3207412 -4.3131862 -4.3065352 -4.3052549 -4.3039403 -4.3018994 -4.2979822 -4.2946548 -4.2925038 -4.2945108 -4.2999172 -4.3073835 -4.3164105 -4.324748 -4.3307309][-4.3258967 -4.31451 -4.3027916 -4.2954435 -4.2857022 -4.2788367 -4.275558 -4.2742758 -4.2704468 -4.2738495 -4.2857952 -4.301024 -4.3150687 -4.3257904 -4.331789][-4.3310747 -4.3170323 -4.298687 -4.2822447 -4.2608752 -4.2452059 -4.24032 -4.2387619 -4.2309265 -4.2334242 -4.2534618 -4.2813625 -4.3069954 -4.3215904 -4.3280091][-4.3306403 -4.3114028 -4.2828083 -4.2572832 -4.2236242 -4.1963344 -4.1908941 -4.1938853 -4.1873426 -4.1889009 -4.2156591 -4.2552509 -4.294208 -4.3170328 -4.3243732][-4.31679 -4.2899914 -4.24618 -4.2084193 -4.1600561 -4.1177955 -4.1114359 -4.1299624 -4.1341424 -4.1368647 -4.1704483 -4.221549 -4.2761135 -4.3084111 -4.3186049][-4.294929 -4.2566051 -4.1927633 -4.1326489 -4.0606594 -3.9803457 -3.9659793 -4.0213456 -4.0555673 -4.0740991 -4.1134357 -4.1759372 -4.24466 -4.2908411 -4.308363][-4.2736135 -4.2206521 -4.1316829 -4.0407124 -3.9310956 -3.7805254 -3.7321534 -3.8446915 -3.932184 -3.9830661 -4.0408616 -4.1192212 -4.2056565 -4.2727103 -4.3010192][-4.2627988 -4.2017074 -4.0917468 -3.9690094 -3.8173842 -3.5916452 -3.4850492 -3.6547613 -3.8107867 -3.9053664 -3.9887271 -4.089479 -4.1933103 -4.2742214 -4.3073282][-4.2705321 -4.2127452 -4.1060982 -3.9786875 -3.8235888 -3.6004405 -3.487433 -3.6407139 -3.8080676 -3.9170313 -4.007309 -4.1159015 -4.2183709 -4.2950397 -4.3221965][-4.295126 -4.252924 -4.1750817 -4.0792985 -3.9657903 -3.8170822 -3.7414689 -3.8197787 -3.9375863 -4.0273523 -4.10268 -4.1921539 -4.268631 -4.32216 -4.3367877][-4.3232059 -4.2984219 -4.2539988 -4.2021241 -4.1412029 -4.06096 -4.0161695 -4.0357 -4.0943508 -4.1549211 -4.2094932 -4.2706542 -4.3168406 -4.3441219 -4.3462033][-4.3372836 -4.3270912 -4.3053489 -4.2854137 -4.2606773 -4.2203221 -4.191031 -4.1828203 -4.2087021 -4.2518411 -4.2886858 -4.3250933 -4.3502431 -4.3596616 -4.3527737][-4.3367844 -4.3336959 -4.3219061 -4.3161426 -4.3107605 -4.295 -4.2789245 -4.268836 -4.2838578 -4.3126812 -4.3357191 -4.3547378 -4.3658342 -4.3641768 -4.35251][-4.3311391 -4.3281016 -4.3199244 -4.3165517 -4.3194375 -4.318306 -4.3144293 -4.3123755 -4.324307 -4.3419924 -4.3543978 -4.3631506 -4.3642297 -4.3569384 -4.3451452][-4.3282518 -4.3254824 -4.3193064 -4.315505 -4.3187079 -4.322823 -4.3257 -4.3278933 -4.336359 -4.3459983 -4.3504143 -4.3518863 -4.3487139 -4.3430066 -4.3375473]]...]
INFO - root - 2017-12-07 18:42:05.684261: step 15710, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 39h:51m:00s remains)
INFO - root - 2017-12-07 18:42:26.854439: step 15720, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.108 sec/batch; 39h:27m:56s remains)
INFO - root - 2017-12-07 18:42:48.022010: step 15730, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 39h:34m:13s remains)
INFO - root - 2017-12-07 18:43:09.238088: step 15740, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 39h:55m:54s remains)
INFO - root - 2017-12-07 18:43:30.261476: step 15750, loss = 2.05, batch loss = 2.00 (15.0 examples/sec; 2.129 sec/batch; 39h:50m:44s remains)
INFO - root - 2017-12-07 18:43:51.459316: step 15760, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.102 sec/batch; 39h:19m:40s remains)
INFO - root - 2017-12-07 18:44:12.657682: step 15770, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.097 sec/batch; 39h:14m:13s remains)
INFO - root - 2017-12-07 18:44:33.607924: step 15780, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 39h:37m:26s remains)
INFO - root - 2017-12-07 18:44:54.777398: step 15790, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.148 sec/batch; 40h:10m:25s remains)
INFO - root - 2017-12-07 18:45:16.104785: step 15800, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.149 sec/batch; 40h:11m:24s remains)
2017-12-07 18:45:17.661484: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3188119 -4.3310938 -4.34581 -4.3596964 -4.3658829 -4.3550019 -4.3280058 -4.2912054 -4.2619948 -4.2494822 -4.2589469 -4.2783251 -4.299705 -4.3146539 -4.320497][-4.3227444 -4.33992 -4.3605123 -4.3758631 -4.378787 -4.3563676 -4.3108463 -4.2533212 -4.2112904 -4.1995363 -4.2215905 -4.2568374 -4.2921643 -4.3188319 -4.3310876][-4.321167 -4.3430233 -4.3685789 -4.385653 -4.3830633 -4.348887 -4.28335 -4.2006755 -4.142839 -4.1354985 -4.177237 -4.2326207 -4.2798171 -4.3132281 -4.32934][-4.3227234 -4.3449802 -4.3741984 -4.3911414 -4.3790536 -4.3315859 -4.2428746 -4.1326294 -4.0606341 -4.0662403 -4.1340041 -4.2069969 -4.2607675 -4.2955308 -4.30976][-4.337317 -4.358036 -4.3845649 -4.3913774 -4.3615618 -4.2912469 -4.1716166 -4.025177 -3.9453616 -3.9811502 -4.0839815 -4.1765261 -4.2347465 -4.2673936 -4.2747498][-4.3598876 -4.3754792 -4.3925424 -4.380455 -4.3277922 -4.2306914 -4.0759592 -3.88726 -3.8035009 -3.8903332 -4.0381036 -4.1474891 -4.2057915 -4.2310061 -4.2293329][-4.3735585 -4.381032 -4.3848643 -4.3548741 -4.2812514 -4.1636858 -3.9804544 -3.7564962 -3.6758571 -3.8281133 -4.017365 -4.1358418 -4.1871738 -4.2030005 -4.197597][-4.3753276 -4.3727088 -4.3613081 -4.3219829 -4.2421131 -4.1199384 -3.9369276 -3.7239439 -3.6703503 -3.8581617 -4.0539856 -4.1607237 -4.192131 -4.1949487 -4.1909056][-4.3648138 -4.3514409 -4.3293438 -4.2889357 -4.2139549 -4.1064348 -3.9629638 -3.8174324 -3.8079953 -3.9640734 -4.1198616 -4.1976581 -4.205502 -4.1990123 -4.1998873][-4.348464 -4.3281713 -4.3048806 -4.2652793 -4.1971884 -4.1161437 -4.0289474 -3.9541457 -3.966677 -4.0729804 -4.1794291 -4.2324657 -4.2319632 -4.2286887 -4.2401361][-4.3385596 -4.3226123 -4.3048806 -4.2714076 -4.2162995 -4.1662774 -4.1292744 -4.0981083 -4.1118584 -4.1695695 -4.23455 -4.2719145 -4.2751369 -4.2797136 -4.2955165][-4.334486 -4.3291445 -4.322556 -4.3023319 -4.2632246 -4.2370763 -4.2263346 -4.2138267 -4.2222376 -4.2476692 -4.2831535 -4.3070903 -4.3135233 -4.3221231 -4.3335671][-4.3345323 -4.3398724 -4.3412251 -4.3281784 -4.2999458 -4.2841835 -4.2801266 -4.2746935 -4.2805119 -4.2894144 -4.3068786 -4.3223934 -4.3320584 -4.3413754 -4.346982][-4.3309417 -4.3404722 -4.3440185 -4.3369479 -4.3185029 -4.3070188 -4.303216 -4.2993784 -4.3016958 -4.3042684 -4.3129334 -4.3234873 -4.3334541 -4.3406563 -4.3410892][-4.3233175 -4.3315349 -4.333457 -4.3302627 -4.319519 -4.3108292 -4.3068557 -4.3043852 -4.3055582 -4.30657 -4.311398 -4.3180909 -4.3251853 -4.3297658 -4.3297448]]...]
INFO - root - 2017-12-07 18:45:38.613181: step 15810, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.146 sec/batch; 40h:07m:36s remains)
INFO - root - 2017-12-07 18:45:59.826568: step 15820, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.107 sec/batch; 39h:23m:20s remains)
INFO - root - 2017-12-07 18:46:20.935096: step 15830, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.149 sec/batch; 40h:10m:40s remains)
INFO - root - 2017-12-07 18:46:41.823381: step 15840, loss = 2.08, batch loss = 2.02 (15.6 examples/sec; 2.053 sec/batch; 38h:21m:59s remains)
INFO - root - 2017-12-07 18:47:02.932482: step 15850, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.153 sec/batch; 40h:13m:31s remains)
INFO - root - 2017-12-07 18:47:24.397125: step 15860, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.167 sec/batch; 40h:29m:03s remains)
INFO - root - 2017-12-07 18:47:45.543760: step 15870, loss = 2.08, batch loss = 2.02 (16.1 examples/sec; 1.985 sec/batch; 37h:05m:14s remains)
INFO - root - 2017-12-07 18:48:06.647704: step 15880, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.088 sec/batch; 39h:00m:06s remains)
INFO - root - 2017-12-07 18:48:27.725188: step 15890, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.120 sec/batch; 39h:35m:07s remains)
INFO - root - 2017-12-07 18:48:49.127314: step 15900, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.145 sec/batch; 40h:03m:38s remains)
2017-12-07 18:48:50.685754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2861013 -4.2766242 -4.2696991 -4.2809572 -4.3057055 -4.3264503 -4.3263497 -4.3127432 -4.3050809 -4.3056993 -4.3160634 -4.3275743 -4.3378177 -4.343554 -4.3431706][-4.2906308 -4.28482 -4.2831497 -4.2926435 -4.3174338 -4.3380232 -4.3344297 -4.3134632 -4.2929993 -4.2876959 -4.2974176 -4.3095245 -4.3258352 -4.3388476 -4.3398876][-4.2881374 -4.2877808 -4.2927146 -4.3031082 -4.3271055 -4.3476281 -4.3359461 -4.2914586 -4.2392287 -4.22546 -4.2522912 -4.2767615 -4.3086185 -4.3314724 -4.3331885][-4.2914691 -4.2959146 -4.3033433 -4.312717 -4.3332152 -4.3461571 -4.3097548 -4.2253103 -4.1384492 -4.1346679 -4.1914816 -4.2374735 -4.2865829 -4.32123 -4.32816][-4.2987809 -4.3052535 -4.3107886 -4.3170719 -4.327199 -4.315588 -4.241425 -4.1120796 -4.0026278 -4.0334935 -4.1288924 -4.197052 -4.2622156 -4.3070617 -4.3179088][-4.2985315 -4.3037386 -4.3066039 -4.3057346 -4.29718 -4.252809 -4.1370292 -3.9587936 -3.8428295 -3.9401457 -4.0857763 -4.1765714 -4.2516317 -4.2980976 -4.3076859][-4.2884092 -4.2900734 -4.2854242 -4.2725272 -4.2508221 -4.1851621 -4.0389538 -3.8247969 -3.7350376 -3.9129853 -4.0890779 -4.189549 -4.2605124 -4.2939887 -4.2966213][-4.2781615 -4.2739968 -4.2598004 -4.24097 -4.2198296 -4.15482 -4.0075283 -3.8249259 -3.8062549 -3.9953566 -4.1456161 -4.2312269 -4.2800875 -4.29128 -4.2834406][-4.2745466 -4.2669859 -4.2494736 -4.2354393 -4.2249222 -4.1748056 -4.05862 -3.9502656 -3.9828737 -4.1271744 -4.2234812 -4.2790236 -4.3011441 -4.2937436 -4.2793713][-4.284596 -4.276185 -4.2598443 -4.2533627 -4.2542095 -4.22212 -4.1427684 -4.0875835 -4.13303 -4.2307591 -4.2867923 -4.3167181 -4.3176208 -4.2994971 -4.2816024][-4.3061662 -4.2994208 -4.2859211 -4.2826576 -4.2885084 -4.2671189 -4.2154508 -4.1908932 -4.231565 -4.29297 -4.3253188 -4.3377528 -4.3289633 -4.3093281 -4.29054][-4.3264894 -4.3230772 -4.3129663 -4.30793 -4.3120546 -4.2938275 -4.2607012 -4.2513676 -4.2799845 -4.321033 -4.3426347 -4.3481922 -4.3365827 -4.3171434 -4.3003][-4.3322043 -4.3346086 -4.3313932 -4.3277311 -4.3284435 -4.3119006 -4.2905593 -4.2867522 -4.30677 -4.336381 -4.3526821 -4.3544488 -4.3408623 -4.3229442 -4.3101764][-4.3342967 -4.342155 -4.3445945 -4.3427644 -4.3412509 -4.329845 -4.3191109 -4.3179178 -4.3309116 -4.3492165 -4.3584852 -4.3560653 -4.3444362 -4.3328218 -4.3273544][-4.3394809 -4.3499622 -4.3543582 -4.3540707 -4.3521719 -4.34461 -4.3406057 -4.3413148 -4.3490534 -4.3584728 -4.3608203 -4.3558922 -4.3484507 -4.343821 -4.34316]]...]
INFO - root - 2017-12-07 18:49:11.604413: step 15910, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.146 sec/batch; 40h:03m:32s remains)
INFO - root - 2017-12-07 18:49:32.781882: step 15920, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.162 sec/batch; 40h:21m:42s remains)
INFO - root - 2017-12-07 18:49:53.973577: step 15930, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.107 sec/batch; 39h:19m:18s remains)
INFO - root - 2017-12-07 18:50:14.708076: step 15940, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.086 sec/batch; 38h:55m:24s remains)
INFO - root - 2017-12-07 18:50:35.679747: step 15950, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 39h:52m:36s remains)
INFO - root - 2017-12-07 18:50:56.984362: step 15960, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.155 sec/batch; 40h:11m:52s remains)
INFO - root - 2017-12-07 18:51:17.930523: step 15970, loss = 2.08, batch loss = 2.02 (16.6 examples/sec; 1.926 sec/batch; 35h:55m:45s remains)
INFO - root - 2017-12-07 18:51:39.188930: step 15980, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.145 sec/batch; 40h:00m:37s remains)
INFO - root - 2017-12-07 18:52:00.522785: step 15990, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 39h:45m:40s remains)
INFO - root - 2017-12-07 18:52:21.425962: step 16000, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 39h:29m:03s remains)
2017-12-07 18:52:23.019449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2739315 -4.2840295 -4.2810464 -4.2627916 -4.2344389 -4.2131066 -4.2092957 -4.2009335 -4.1990428 -4.2140965 -4.2381988 -4.2505555 -4.2518134 -4.22686 -4.1671038][-4.2609415 -4.2799044 -4.2815971 -4.2619734 -4.21999 -4.1791792 -4.1592159 -4.150876 -4.1617827 -4.1931386 -4.2222357 -4.2337594 -4.23481 -4.1981325 -4.1228957][-4.2256961 -4.2595167 -4.2687593 -4.2510791 -4.2039886 -4.1440845 -4.103878 -4.0907726 -4.1159658 -4.16483 -4.2039652 -4.2205434 -4.22145 -4.1769013 -4.09248][-4.1959572 -4.2502956 -4.2681217 -4.2509532 -4.19897 -4.1182694 -4.0488963 -4.02761 -4.0696454 -4.1390433 -4.1931038 -4.2176604 -4.22043 -4.174684 -4.0848775][-4.1909704 -4.2588825 -4.2810063 -4.2614908 -4.19757 -4.0899043 -3.9866357 -3.9572682 -4.0195003 -4.1120033 -4.1853623 -4.2261586 -4.23924 -4.1994133 -4.1119061][-4.189837 -4.2628126 -4.2847834 -4.2621808 -4.1843176 -4.0511723 -3.9105675 -3.8571568 -3.9340489 -4.0628338 -4.1677589 -4.2324414 -4.2603984 -4.2361641 -4.1653838][-4.2008915 -4.2700658 -4.2851257 -4.2562652 -4.1697373 -4.0144348 -3.8263445 -3.7290435 -3.8181605 -3.9962459 -4.1386847 -4.2239614 -4.2662077 -4.2617388 -4.2187214][-4.230104 -4.2864056 -4.2943897 -4.2601566 -4.1677642 -4.0074291 -3.7947202 -3.6662445 -3.7645197 -3.966109 -4.1191769 -4.20642 -4.2550759 -4.2644458 -4.2454524][-4.2473397 -4.2913651 -4.2960815 -4.2602181 -4.1663246 -4.0234518 -3.8413959 -3.7395477 -3.8235207 -3.9941268 -4.1211329 -4.1932478 -4.2365355 -4.25073 -4.2459531][-4.2536259 -4.2887249 -4.2924109 -4.2538128 -4.164856 -4.0477962 -3.915508 -3.855422 -3.9235961 -4.0489631 -4.1410513 -4.1894369 -4.214798 -4.2246604 -4.2266412][-4.2598176 -4.2839494 -4.2805467 -4.2385769 -4.1561341 -4.0649629 -3.9826109 -3.9648321 -4.0276728 -4.1140819 -4.176734 -4.1977592 -4.2000828 -4.1957712 -4.1944141][-4.2571034 -4.2683291 -4.255692 -4.2092905 -4.13055 -4.0604634 -4.0182323 -4.0391769 -4.1068764 -4.1686387 -4.2005696 -4.1971011 -4.1855326 -4.1786222 -4.1743603][-4.2442493 -4.2456846 -4.230072 -4.1752772 -4.0935922 -4.0267978 -4.0034857 -4.0540023 -4.1325021 -4.1852446 -4.2009373 -4.1878967 -4.1751137 -4.1744804 -4.1726065][-4.2258329 -4.22354 -4.207552 -4.1460991 -4.061861 -3.9902067 -3.9759398 -4.037961 -4.1256523 -4.1833119 -4.1963878 -4.1812377 -4.16807 -4.1729426 -4.1753082][-4.2014332 -4.2031932 -4.1867723 -4.1323829 -4.06352 -4.0026383 -3.9853356 -4.0358748 -4.1220946 -4.1854024 -4.2002535 -4.1824532 -4.172965 -4.1769075 -4.1774578]]...]
INFO - root - 2017-12-07 18:52:44.073074: step 16010, loss = 2.09, batch loss = 2.04 (15.3 examples/sec; 2.094 sec/batch; 39h:02m:44s remains)
INFO - root - 2017-12-07 18:53:05.461290: step 16020, loss = 2.08, batch loss = 2.03 (14.9 examples/sec; 2.146 sec/batch; 39h:59m:37s remains)
INFO - root - 2017-12-07 18:53:26.639243: step 16030, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 39h:35m:47s remains)
INFO - root - 2017-12-07 18:53:47.545866: step 16040, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 39h:43m:25s remains)
INFO - root - 2017-12-07 18:54:08.741275: step 16050, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.100 sec/batch; 39h:07m:39s remains)
INFO - root - 2017-12-07 18:54:29.823468: step 16060, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.063 sec/batch; 38h:25m:22s remains)
INFO - root - 2017-12-07 18:54:50.624052: step 16070, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.165 sec/batch; 40h:19m:10s remains)
INFO - root - 2017-12-07 18:55:11.974672: step 16080, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.127 sec/batch; 39h:37m:04s remains)
INFO - root - 2017-12-07 18:55:33.237034: step 16090, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.094 sec/batch; 38h:59m:56s remains)
INFO - root - 2017-12-07 18:55:54.250134: step 16100, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.086 sec/batch; 38h:49m:59s remains)
2017-12-07 18:55:55.804279: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3281159 -4.3203382 -4.2728415 -4.1993027 -4.1268582 -4.0730572 -4.0549927 -4.1080151 -4.20438 -4.2590289 -4.2533979 -4.2112856 -4.1635652 -4.1427622 -4.1715422][-4.3109741 -4.314014 -4.2758894 -4.209178 -4.1367135 -4.073411 -4.0428171 -4.089828 -4.1903248 -4.2486258 -4.2406197 -4.18222 -4.10598 -4.0633111 -4.1038365][-4.2916651 -4.3075104 -4.284019 -4.2283492 -4.15444 -4.0786686 -4.0342336 -4.0743122 -4.1767931 -4.2417293 -4.2367082 -4.1663589 -4.0644836 -3.9891989 -4.0239711][-4.2764158 -4.30166 -4.2915463 -4.2442465 -4.1605482 -4.0660243 -4.0077834 -4.0451126 -4.1572466 -4.2303472 -4.2337832 -4.1642995 -4.0480652 -3.942647 -3.9595675][-4.2602921 -4.2927327 -4.2901454 -4.2475004 -4.1526175 -4.0340066 -3.9535713 -3.9846544 -4.1157632 -4.2093596 -4.2322965 -4.1798587 -4.0657134 -3.9457006 -3.9374387][-4.2512465 -4.285531 -4.2850046 -4.2431397 -4.1380658 -3.9877598 -3.867341 -3.8853095 -4.0501943 -4.17783 -4.2254577 -4.2046804 -4.1175218 -4.0066285 -3.9817979][-4.2550364 -4.2844458 -4.2817965 -4.2365923 -4.1220284 -3.9416561 -3.7744412 -3.7702265 -3.9701705 -4.1347275 -4.2074275 -4.2188559 -4.1687255 -4.0847759 -4.0587668][-4.2678556 -4.2893782 -4.2821569 -4.2361908 -4.1216455 -3.9371037 -3.7506857 -3.7215667 -3.9196405 -4.0972223 -4.1858411 -4.2233829 -4.2061906 -4.1546211 -4.1369276][-4.2781115 -4.2915821 -4.2814054 -4.2374659 -4.1347332 -3.9718688 -3.7971876 -3.7464318 -3.8986068 -4.0592537 -4.1568246 -4.2133212 -4.2242494 -4.2026954 -4.194284][-4.2861462 -4.2907195 -4.2778287 -4.2356253 -4.1523533 -4.0232139 -3.8789923 -3.8200443 -3.9219956 -4.0527391 -4.147572 -4.2120395 -4.2411208 -4.2424068 -4.2423244][-4.2897692 -4.2834406 -4.2712383 -4.2362218 -4.1741066 -4.0797472 -3.9718986 -3.9232938 -3.9937773 -4.0965981 -4.1747251 -4.23139 -4.2651849 -4.2750745 -4.2786369][-4.283432 -4.2691951 -4.2598405 -4.2377625 -4.1986394 -4.1352491 -4.059175 -4.0227103 -4.0716891 -4.1514692 -4.2137432 -4.2588048 -4.2873616 -4.2940884 -4.2924523][-4.2816534 -4.265564 -4.2591786 -4.2491055 -4.2277732 -4.1874366 -4.1329856 -4.1041536 -4.134006 -4.1943336 -4.245317 -4.2832932 -4.3055487 -4.3076396 -4.3012323][-4.2948766 -4.2811403 -4.2767773 -4.2737885 -4.2651954 -4.2413635 -4.2032247 -4.1800289 -4.1966619 -4.2376928 -4.2770882 -4.3069048 -4.324327 -4.3250594 -4.3167248][-4.3120275 -4.3043413 -4.30377 -4.3038731 -4.3005147 -4.2878695 -4.2642341 -4.2450118 -4.2495861 -4.2731056 -4.299531 -4.320426 -4.3328238 -4.3344021 -4.3294692]]...]
INFO - root - 2017-12-07 18:56:16.948319: step 16110, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.134 sec/batch; 39h:43m:43s remains)
INFO - root - 2017-12-07 18:56:38.073794: step 16120, loss = 2.08, batch loss = 2.02 (15.8 examples/sec; 2.030 sec/batch; 37h:47m:03s remains)
INFO - root - 2017-12-07 18:56:59.012208: step 16130, loss = 2.07, batch loss = 2.01 (16.1 examples/sec; 1.982 sec/batch; 36h:52m:44s remains)
INFO - root - 2017-12-07 18:57:20.184571: step 16140, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 39h:24m:51s remains)
INFO - root - 2017-12-07 18:57:41.535048: step 16150, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.101 sec/batch; 39h:05m:33s remains)
INFO - root - 2017-12-07 18:58:02.557839: step 16160, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 1.893 sec/batch; 35h:13m:10s remains)
INFO - root - 2017-12-07 18:58:23.444234: step 16170, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.070 sec/batch; 38h:30m:29s remains)
INFO - root - 2017-12-07 18:58:44.426921: step 16180, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 39h:36m:53s remains)
INFO - root - 2017-12-07 18:59:05.705247: step 16190, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.129 sec/batch; 39h:35m:29s remains)
INFO - root - 2017-12-07 18:59:26.647420: step 16200, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.098 sec/batch; 39h:00m:40s remains)
2017-12-07 18:59:28.189226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1171041 -4.1162395 -4.1240816 -4.134757 -4.1323357 -4.1135678 -4.0970726 -4.09052 -4.0961103 -4.1096215 -4.1188297 -4.1158228 -4.1091909 -4.1096234 -4.128593][-4.1313105 -4.1382189 -4.1474037 -4.1485696 -4.132266 -4.0995927 -4.0706654 -4.0566559 -4.0622926 -4.0777478 -4.0819893 -4.0723386 -4.0642972 -4.06814 -4.0916104][-4.1203122 -4.1342592 -4.1465969 -4.14261 -4.1256137 -4.0949903 -4.06068 -4.0383425 -4.0401115 -4.0560575 -4.05586 -4.0433874 -4.040216 -4.0475516 -4.0688643][-4.0819235 -4.101203 -4.1235156 -4.1251631 -4.1193008 -4.1024108 -4.0648284 -4.03099 -4.0224042 -4.0358748 -4.0369878 -4.0303144 -4.0380726 -4.0467048 -4.0592837][-4.0245137 -4.0500402 -4.0910239 -4.1149573 -4.1217613 -4.1090355 -4.0579791 -4.0037756 -3.9849188 -3.9992716 -4.0113811 -4.0174541 -4.0368481 -4.0502872 -4.0584226][-3.966424 -4.0071855 -4.0687652 -4.109313 -4.1137166 -4.0835567 -4.0041986 -3.9273314 -3.9153669 -3.9434469 -3.9668436 -3.9857564 -4.019042 -4.0423775 -4.0575004][-3.923007 -3.9821064 -4.0495367 -4.0850492 -4.0653563 -4.0006981 -3.8914719 -3.796176 -3.810565 -3.868768 -3.9062061 -3.9351895 -3.9826057 -4.0229459 -4.0524492][-3.8843427 -3.9425721 -3.9961612 -4.0094995 -3.9696381 -3.8841357 -3.7590516 -3.6610312 -3.7065444 -3.7992382 -3.8568888 -3.903672 -3.964813 -4.0195045 -4.0577602][-3.90359 -3.9431357 -3.9755356 -3.9690428 -3.9281602 -3.8469586 -3.7362084 -3.6694448 -3.7278345 -3.8236997 -3.8925691 -3.9529619 -4.0124555 -4.0627623 -4.0961523][-3.9831588 -4.0024371 -4.0214496 -4.01016 -3.9821987 -3.9199917 -3.8422287 -3.8134749 -3.8688183 -3.9422791 -4.0073223 -4.0656171 -4.1093583 -4.1426234 -4.1644187][-4.0849466 -4.0907207 -4.1018729 -4.0959649 -4.0808806 -4.0425024 -3.99676 -3.9893861 -4.0305705 -4.0789394 -4.1306119 -4.17402 -4.2026639 -4.2207642 -4.230083][-4.1757851 -4.1734433 -4.1781497 -4.174808 -4.1638017 -4.1451769 -4.1251159 -4.1280985 -4.1548719 -4.1850495 -4.2181382 -4.2455735 -4.2646465 -4.2741261 -4.2747622][-4.2359524 -4.2322488 -4.23196 -4.229351 -4.2245293 -4.2201219 -4.2132893 -4.2159796 -4.2319732 -4.2505627 -4.2673345 -4.28372 -4.2965775 -4.3012829 -4.3004689][-4.2663198 -4.2654724 -4.266511 -4.2657075 -4.26392 -4.2661548 -4.2653546 -4.2653432 -4.2705784 -4.2792554 -4.2879858 -4.2972536 -4.3053651 -4.3093934 -4.3099709][-4.2813616 -4.2842803 -4.2873592 -4.290679 -4.2931008 -4.2976933 -4.2974896 -4.2964039 -4.2959046 -4.2966881 -4.2997923 -4.3045287 -4.3084273 -4.3114328 -4.312602]]...]
INFO - root - 2017-12-07 18:59:49.473329: step 16210, loss = 2.08, batch loss = 2.03 (14.9 examples/sec; 2.141 sec/batch; 39h:48m:17s remains)
INFO - root - 2017-12-07 19:00:10.857140: step 16220, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.153 sec/batch; 40h:00m:45s remains)
INFO - root - 2017-12-07 19:00:31.570200: step 16230, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.110 sec/batch; 39h:12m:26s remains)
INFO - root - 2017-12-07 19:00:52.713380: step 16240, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.119 sec/batch; 39h:22m:17s remains)
INFO - root - 2017-12-07 19:01:14.101619: step 16250, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 39h:26m:56s remains)
INFO - root - 2017-12-07 19:01:35.127480: step 16260, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.058 sec/batch; 38h:14m:00s remains)
INFO - root - 2017-12-07 19:01:56.291028: step 16270, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 39h:09m:33s remains)
INFO - root - 2017-12-07 19:02:17.574733: step 16280, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.114 sec/batch; 39h:15m:13s remains)
INFO - root - 2017-12-07 19:02:38.647855: step 16290, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.112 sec/batch; 39h:12m:20s remains)
INFO - root - 2017-12-07 19:02:59.776808: step 16300, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 39h:30m:56s remains)
2017-12-07 19:03:01.403402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2212214 -4.1894073 -4.1692982 -4.1509809 -4.1432762 -4.1574368 -4.1918221 -4.2359009 -4.2731357 -4.2949104 -4.2981973 -4.2955575 -4.2960224 -4.3019986 -4.3092046][-4.2353539 -4.230433 -4.23021 -4.22069 -4.2080221 -4.2044897 -4.2135849 -4.2375803 -4.2648845 -4.2859545 -4.2918577 -4.2899251 -4.2892652 -4.2943487 -4.3016105][-4.2442007 -4.2549863 -4.2649503 -4.2600384 -4.2450809 -4.23396 -4.2327771 -4.2456079 -4.2648082 -4.2826147 -4.2889166 -4.2871995 -4.2847724 -4.2878904 -4.2944484][-4.2491121 -4.2680354 -4.2814555 -4.2748384 -4.2534275 -4.2352285 -4.2282677 -4.2377315 -4.2572718 -4.2770238 -4.2867851 -4.2879772 -4.2852783 -4.2851062 -4.2889128][-4.2611346 -4.2796359 -4.2878208 -4.2728157 -4.2373743 -4.20344 -4.18954 -4.2037568 -4.2331238 -4.2615409 -4.2815914 -4.289979 -4.2898989 -4.2882719 -4.2879758][-4.2784562 -4.2891212 -4.2837787 -4.2508755 -4.193079 -4.1393352 -4.1178236 -4.1412592 -4.1877875 -4.233088 -4.2709556 -4.2921863 -4.2992973 -4.2981834 -4.2944293][-4.2943478 -4.2960491 -4.2784882 -4.2296553 -4.1484566 -4.0706992 -4.0363684 -4.0627308 -4.1230621 -4.1866245 -4.2455974 -4.2872958 -4.308157 -4.3113585 -4.3049374][-4.3070006 -4.3027778 -4.2805519 -4.2271152 -4.1407118 -4.0583458 -4.0146408 -4.0265431 -4.0818257 -4.1485176 -4.2180882 -4.2724032 -4.3074617 -4.31988 -4.3153691][-4.3147774 -4.3034105 -4.2784543 -4.2310562 -4.1638541 -4.1094489 -4.0824208 -4.0792961 -4.1013732 -4.1393843 -4.1949162 -4.2486014 -4.2911625 -4.3155494 -4.3193517][-4.3165045 -4.2970614 -4.2667851 -4.2263808 -4.1865921 -4.1699138 -4.1682138 -4.1657948 -4.169383 -4.1745281 -4.1965451 -4.232553 -4.2730255 -4.3039522 -4.3155451][-4.3153276 -4.2903957 -4.2548909 -4.2172608 -4.1983781 -4.2084885 -4.2268438 -4.2327709 -4.2327409 -4.2240758 -4.2242489 -4.2391467 -4.2669673 -4.294662 -4.3088083][-4.3119283 -4.2825 -4.2437792 -4.2102551 -4.2048807 -4.2299137 -4.258718 -4.2726545 -4.2728872 -4.2618704 -4.2545352 -4.2572584 -4.2724705 -4.2908072 -4.3013363][-4.3057885 -4.275157 -4.2394681 -4.2166471 -4.22387 -4.2536759 -4.2836747 -4.301384 -4.302423 -4.2921205 -4.280746 -4.2757306 -4.2799816 -4.2885194 -4.2939024][-4.2980084 -4.2719183 -4.2471404 -4.2373257 -4.2507234 -4.2749853 -4.2990389 -4.318574 -4.3237348 -4.312861 -4.2976213 -4.2872915 -4.2846127 -4.2866135 -4.2907877][-4.2896156 -4.2743678 -4.2640028 -4.2625256 -4.2747765 -4.2894497 -4.3063354 -4.32542 -4.3318844 -4.3216343 -4.306776 -4.2956285 -4.2894139 -4.2879906 -4.2914767]]...]
INFO - root - 2017-12-07 19:03:22.634920: step 16310, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 39h:39m:46s remains)
INFO - root - 2017-12-07 19:03:43.820138: step 16320, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 2.083 sec/batch; 38h:38m:52s remains)
INFO - root - 2017-12-07 19:04:04.950947: step 16330, loss = 2.09, batch loss = 2.03 (14.7 examples/sec; 2.172 sec/batch; 40h:18m:13s remains)
INFO - root - 2017-12-07 19:04:26.345751: step 16340, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.162 sec/batch; 40h:05m:59s remains)
INFO - root - 2017-12-07 19:04:47.599617: step 16350, loss = 2.08, batch loss = 2.02 (14.3 examples/sec; 2.232 sec/batch; 41h:24m:21s remains)
INFO - root - 2017-12-07 19:05:08.458992: step 16360, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 2.056 sec/batch; 38h:07m:20s remains)
INFO - root - 2017-12-07 19:05:29.979109: step 16370, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.149 sec/batch; 39h:51m:27s remains)
INFO - root - 2017-12-07 19:05:51.112792: step 16380, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.082 sec/batch; 38h:35m:47s remains)
INFO - root - 2017-12-07 19:06:11.923729: step 16390, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.145 sec/batch; 39h:46m:17s remains)
INFO - root - 2017-12-07 19:06:33.108023: step 16400, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 39h:31m:44s remains)
2017-12-07 19:06:34.777907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2254796 -4.2276015 -4.2400804 -4.2611852 -4.2750134 -4.2764874 -4.2613239 -4.236083 -4.2091093 -4.192585 -4.1890659 -4.195601 -4.2039661 -4.2174792 -4.2353444][-4.240006 -4.2490096 -4.2675991 -4.2879443 -4.2951107 -4.2888727 -4.2665224 -4.2337847 -4.1966834 -4.1696835 -4.1657767 -4.1826224 -4.2040334 -4.2256122 -4.2464485][-4.2523084 -4.2686033 -4.2916861 -4.3101177 -4.3099523 -4.293654 -4.2641373 -4.2241635 -4.1785526 -4.1446776 -4.1465917 -4.1769347 -4.2089033 -4.2355089 -4.2563157][-4.25211 -4.2739692 -4.3013315 -4.3188443 -4.3120322 -4.2869387 -4.2509217 -4.2045288 -4.152205 -4.11802 -4.1324215 -4.1765747 -4.2140288 -4.2407975 -4.2581005][-4.2475514 -4.2697182 -4.298604 -4.3153038 -4.3040924 -4.2717681 -4.2270756 -4.1707344 -4.1129756 -4.0903172 -4.1268044 -4.182467 -4.2214551 -4.2456656 -4.2568135][-4.2451639 -4.2613859 -4.2867732 -4.3004532 -4.2844505 -4.2453218 -4.1902962 -4.1256685 -4.0690713 -4.072464 -4.13731 -4.2037768 -4.24349 -4.2612338 -4.2629967][-4.2399583 -4.2484288 -4.2677231 -4.2789664 -4.2590923 -4.2132378 -4.1481557 -4.0744762 -4.0239887 -4.0638051 -4.1569963 -4.2322583 -4.271698 -4.2838049 -4.2768269][-4.2288256 -4.2297397 -4.2431407 -4.2515254 -4.2286282 -4.1771193 -4.1028404 -4.0192423 -3.9795139 -4.0558333 -4.1698494 -4.2499948 -4.2879744 -4.2942491 -4.28167][-4.2130942 -4.2074075 -4.2165565 -4.2230721 -4.2010694 -4.1499567 -4.0733695 -3.9855824 -3.9610889 -4.0609 -4.1799946 -4.2577548 -4.2915072 -4.2911777 -4.27561][-4.2083726 -4.1904125 -4.1925397 -4.1994934 -4.1869826 -4.1481519 -4.0813055 -4.0023308 -3.9927649 -4.0899148 -4.1956172 -4.2627993 -4.2866182 -4.2765632 -4.2581453][-4.2168522 -4.1860304 -4.1776786 -4.1843143 -4.1830831 -4.1612053 -4.1102018 -4.0464597 -4.0483418 -4.1321311 -4.2195487 -4.2716403 -4.2840319 -4.2661052 -4.2452688][-4.2295775 -4.1903915 -4.1758842 -4.182126 -4.1893749 -4.1818 -4.1462679 -4.0983291 -4.1063695 -4.1730356 -4.2403955 -4.2769351 -4.2789721 -4.257153 -4.2366524][-4.2427626 -4.2006617 -4.183167 -4.1874642 -4.2006407 -4.2037005 -4.1805496 -4.1456766 -4.154552 -4.2035823 -4.2533469 -4.2764149 -4.2697673 -4.2460961 -4.2262363][-4.2541227 -4.2141323 -4.1945086 -4.193202 -4.2082314 -4.21798 -4.2037873 -4.1801138 -4.1857114 -4.2184463 -4.2544413 -4.2688971 -4.2591372 -4.2357354 -4.2177238][-4.2634168 -4.2252517 -4.2020884 -4.1946645 -4.2073483 -4.2189937 -4.2113547 -4.1973147 -4.1988878 -4.2159357 -4.2397614 -4.2504134 -4.2421117 -4.2214079 -4.2043462]]...]
INFO - root - 2017-12-07 19:06:56.084619: step 16410, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 2.079 sec/batch; 38h:31m:41s remains)
INFO - root - 2017-12-07 19:07:17.179138: step 16420, loss = 2.07, batch loss = 2.02 (15.8 examples/sec; 2.030 sec/batch; 37h:36m:26s remains)
INFO - root - 2017-12-07 19:07:38.257825: step 16430, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 39h:00m:22s remains)
INFO - root - 2017-12-07 19:07:59.590143: step 16440, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 38h:52m:58s remains)
INFO - root - 2017-12-07 19:08:20.700011: step 16450, loss = 2.08, batch loss = 2.02 (16.3 examples/sec; 1.968 sec/batch; 36h:26m:36s remains)
INFO - root - 2017-12-07 19:08:41.698240: step 16460, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.143 sec/batch; 39h:41m:32s remains)
INFO - root - 2017-12-07 19:09:02.825799: step 16470, loss = 2.06, batch loss = 2.00 (14.8 examples/sec; 2.158 sec/batch; 39h:57m:11s remains)
INFO - root - 2017-12-07 19:09:23.987435: step 16480, loss = 2.06, batch loss = 2.00 (15.4 examples/sec; 2.082 sec/batch; 38h:32m:43s remains)
INFO - root - 2017-12-07 19:09:45.056857: step 16490, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.175 sec/batch; 40h:15m:42s remains)
INFO - root - 2017-12-07 19:10:06.404646: step 16500, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.168 sec/batch; 40h:07m:20s remains)
2017-12-07 19:10:08.037853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3176837 -4.2883778 -4.2582736 -4.2185855 -4.1749358 -4.1430717 -4.1241937 -4.1443324 -4.1818495 -4.194015 -4.1796494 -4.1579118 -4.144691 -4.1311541 -4.1151776][-4.3170896 -4.2825723 -4.2457366 -4.2019734 -4.1508474 -4.1091666 -4.0892038 -4.1187034 -4.1674619 -4.1957207 -4.1915207 -4.1658192 -4.144906 -4.1236582 -4.0974965][-4.3207049 -4.2828445 -4.2419062 -4.1971679 -4.1411428 -4.0879025 -4.0673041 -4.1027956 -4.152617 -4.1861277 -4.1836085 -4.153676 -4.1214695 -4.0899487 -4.0631266][-4.3247004 -4.2819705 -4.2389035 -4.19861 -4.1448121 -4.0929341 -4.0723376 -4.1066027 -4.1576676 -4.1866512 -4.1777072 -4.1463518 -4.10161 -4.057889 -4.0341916][-4.3244171 -4.2788219 -4.2341151 -4.1954136 -4.1452065 -4.1006012 -4.0747342 -4.1024075 -4.1650624 -4.1977167 -4.190608 -4.1665072 -4.11874 -4.0713677 -4.0562453][-4.3184433 -4.2718163 -4.2206025 -4.1722016 -4.1164846 -4.0696745 -4.0218763 -4.0302062 -4.1189365 -4.178647 -4.1853528 -4.1757469 -4.1472287 -4.111299 -4.1039991][-4.3069272 -4.25816 -4.193222 -4.1185474 -4.0412741 -3.978071 -3.880872 -3.8520975 -3.9871876 -4.0849366 -4.1165161 -4.13978 -4.1524773 -4.147881 -4.1574788][-4.289886 -4.238668 -4.1677361 -4.07247 -3.9729652 -3.8887875 -3.7369008 -3.6613503 -3.8459918 -3.9774561 -4.03313 -4.0918889 -4.1406016 -4.168654 -4.2000003][-4.2755895 -4.2318549 -4.1761341 -4.0930357 -4.0048251 -3.9353 -3.8104749 -3.7547555 -3.9027171 -4.0074258 -4.0602622 -4.1220779 -4.1718621 -4.2011895 -4.2339964][-4.2732806 -4.2424736 -4.202733 -4.1400414 -4.0799174 -4.0370226 -3.9656887 -3.94398 -4.0345736 -4.095448 -4.1351118 -4.1827264 -4.2212706 -4.2407293 -4.2610598][-4.2748175 -4.2500339 -4.2222724 -4.1795759 -4.1438622 -4.1215744 -4.0876341 -4.0799131 -4.1326265 -4.1677856 -4.1926155 -4.22181 -4.2491026 -4.2677712 -4.2814178][-4.2792048 -4.2598567 -4.2438526 -4.2228909 -4.2029014 -4.1944747 -4.1857524 -4.1856804 -4.2098827 -4.2264881 -4.2401118 -4.255918 -4.2721229 -4.2905765 -4.3027205][-4.28995 -4.2776375 -4.2707129 -4.2624636 -4.2495584 -4.2496572 -4.2535048 -4.2568336 -4.2657924 -4.2703447 -4.2723327 -4.2757349 -4.2832966 -4.2951612 -4.3047848][-4.3034115 -4.2967825 -4.2988911 -4.3003411 -4.2930832 -4.2948303 -4.2963209 -4.2952275 -4.2971716 -4.2937894 -4.2892108 -4.2887139 -4.2919216 -4.2988935 -4.3043237][-4.3126225 -4.3078475 -4.3109574 -4.3180342 -4.3182821 -4.3217878 -4.3206472 -4.3155279 -4.3141809 -4.3079462 -4.3015585 -4.2998729 -4.3018384 -4.3065081 -4.3098989]]...]
INFO - root - 2017-12-07 19:10:29.231879: step 16510, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.111 sec/batch; 39h:03m:36s remains)
INFO - root - 2017-12-07 19:10:50.088713: step 16520, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.097 sec/batch; 38h:47m:18s remains)
INFO - root - 2017-12-07 19:11:11.257331: step 16530, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.079 sec/batch; 38h:27m:57s remains)
INFO - root - 2017-12-07 19:11:32.577505: step 16540, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 38h:49m:46s remains)
INFO - root - 2017-12-07 19:11:53.475284: step 16550, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 1.937 sec/batch; 35h:48m:57s remains)
INFO - root - 2017-12-07 19:12:14.803907: step 16560, loss = 2.08, batch loss = 2.02 (14.4 examples/sec; 2.222 sec/batch; 41h:04m:54s remains)
INFO - root - 2017-12-07 19:12:36.043215: step 16570, loss = 2.10, batch loss = 2.04 (15.1 examples/sec; 2.119 sec/batch; 39h:10m:30s remains)
INFO - root - 2017-12-07 19:12:56.954674: step 16580, loss = 2.09, batch loss = 2.03 (15.4 examples/sec; 2.075 sec/batch; 38h:20m:54s remains)
INFO - root - 2017-12-07 19:13:17.809402: step 16590, loss = 2.06, batch loss = 2.01 (15.4 examples/sec; 2.075 sec/batch; 38h:21m:24s remains)
INFO - root - 2017-12-07 19:13:38.928969: step 16600, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.096 sec/batch; 38h:43m:40s remains)
2017-12-07 19:13:40.545030: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2762585 -4.2515941 -4.2351809 -4.2360611 -4.2517085 -4.2670922 -4.2817845 -4.2855196 -4.2790089 -4.2640119 -4.2515793 -4.2443166 -4.2417116 -4.2571449 -4.2831573][-4.2813959 -4.2574859 -4.2411494 -4.2419271 -4.2544742 -4.2642841 -4.2727537 -4.2726011 -4.2635784 -4.2476573 -4.2361541 -4.2291207 -4.2245488 -4.2437115 -4.2732034][-4.2871304 -4.2650571 -4.2485471 -4.2469916 -4.2532616 -4.2577524 -4.2641916 -4.2642584 -4.2553034 -4.2366195 -4.2220936 -4.2151 -4.2137289 -4.2365956 -4.2669883][-4.3000031 -4.279232 -4.259459 -4.2521834 -4.2493191 -4.2488952 -4.2579141 -4.2640638 -4.2596807 -4.2400308 -4.2244816 -4.2171631 -4.2215352 -4.2461534 -4.273181][-4.3129096 -4.2921162 -4.2673106 -4.2541795 -4.2416425 -4.2327094 -4.238461 -4.2484512 -4.2499833 -4.2313428 -4.2192221 -4.2177134 -4.2300806 -4.258388 -4.2823319][-4.31964 -4.2973142 -4.2658215 -4.24345 -4.2183952 -4.2008724 -4.2024617 -4.2147465 -4.22089 -4.207509 -4.1994491 -4.201952 -4.220921 -4.256114 -4.282218][-4.3188777 -4.2924991 -4.2535763 -4.2218208 -4.1889486 -4.1671138 -4.1676178 -4.1837115 -4.1933208 -4.1822224 -4.1751461 -4.1799579 -4.2051849 -4.2489047 -4.2801151][-4.3163471 -4.2864966 -4.2414446 -4.2020612 -4.1695428 -4.1513515 -4.156199 -4.1743965 -4.1846972 -4.1761174 -4.1699233 -4.1751275 -4.2021718 -4.2442417 -4.2736282][-4.3167348 -4.2833991 -4.2338266 -4.1923532 -4.1655703 -4.1544933 -4.1655703 -4.1866074 -4.1971121 -4.1919379 -4.1866155 -4.1885524 -4.2087121 -4.2392392 -4.2604632][-4.3215823 -4.2888007 -4.2393966 -4.1963239 -4.17099 -4.1649246 -4.1824837 -4.2051296 -4.2154078 -4.2170825 -4.2180972 -4.2203164 -4.2309713 -4.2465467 -4.2566876][-4.3322325 -4.3059011 -4.26303 -4.2216277 -4.1966305 -4.1925116 -4.2105427 -4.2292137 -4.2354755 -4.238266 -4.2449508 -4.2504735 -4.2572007 -4.2650023 -4.267395][-4.3353615 -4.3125939 -4.27565 -4.2395411 -4.2179689 -4.2169023 -4.2353282 -4.2508497 -4.2536454 -4.2546978 -4.2602983 -4.2658043 -4.2719316 -4.2778845 -4.2796836][-4.3359122 -4.3141437 -4.2816834 -4.2501554 -4.2311091 -4.2326946 -4.2500443 -4.262291 -4.2646718 -4.2640295 -4.2662544 -4.2717543 -4.2796965 -4.2859755 -4.2881227][-4.3404145 -4.3204374 -4.2927408 -4.265707 -4.2492552 -4.2496142 -4.2609267 -4.269176 -4.2711186 -4.2702594 -4.272234 -4.278789 -4.2878881 -4.2943587 -4.2964344][-4.3505173 -4.3368311 -4.3170023 -4.295393 -4.282444 -4.2818551 -4.2877817 -4.2911563 -4.2913828 -4.2910805 -4.2939358 -4.3002715 -4.3074427 -4.3132386 -4.3154483]]...]
INFO - root - 2017-12-07 19:14:01.705593: step 16610, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 39h:24m:18s remains)
INFO - root - 2017-12-07 19:14:22.543045: step 16620, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.123 sec/batch; 39h:13m:31s remains)
INFO - root - 2017-12-07 19:14:43.719447: step 16630, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 38h:52m:58s remains)
INFO - root - 2017-12-07 19:15:04.655669: step 16640, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 39h:23m:05s remains)
INFO - root - 2017-12-07 19:15:25.503862: step 16650, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.088 sec/batch; 38h:33m:28s remains)
INFO - root - 2017-12-07 19:15:46.727606: step 16660, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.117 sec/batch; 39h:05m:21s remains)
INFO - root - 2017-12-07 19:16:08.014740: step 16670, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 39h:26m:21s remains)
INFO - root - 2017-12-07 19:16:28.882512: step 16680, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 38h:59m:56s remains)
INFO - root - 2017-12-07 19:16:49.869291: step 16690, loss = 2.06, batch loss = 2.01 (15.5 examples/sec; 2.066 sec/batch; 38h:07m:37s remains)
INFO - root - 2017-12-07 19:17:10.882736: step 16700, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.093 sec/batch; 38h:37m:16s remains)
2017-12-07 19:17:12.436644: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3537111 -4.3535895 -4.3492928 -4.3444371 -4.3383741 -4.3321109 -4.3245091 -4.3150816 -4.3049841 -4.2981944 -4.2951264 -4.2961206 -4.2991381 -4.3010168 -4.3008285][-4.3592753 -4.35689 -4.3476639 -4.3393989 -4.3305392 -4.3194766 -4.305614 -4.2897925 -4.2747374 -4.2654037 -4.2634511 -4.2687621 -4.2777481 -4.2833033 -4.2823424][-4.3600674 -4.3544044 -4.3422675 -4.3320751 -4.3184524 -4.2997422 -4.27855 -4.2567024 -4.2349491 -4.2206216 -4.2192616 -4.2300429 -4.245769 -4.256094 -4.256629][-4.358964 -4.3512979 -4.3390608 -4.3285322 -4.3099575 -4.2838769 -4.2536674 -4.2232585 -4.1960506 -4.180553 -4.1823721 -4.1983256 -4.2176757 -4.2318082 -4.2352529][-4.358871 -4.3493505 -4.3365688 -4.3251886 -4.3044767 -4.2725992 -4.2336569 -4.1965203 -4.1683879 -4.1592979 -4.16937 -4.1913042 -4.21334 -4.2283373 -4.2308884][-4.3559923 -4.3423991 -4.32568 -4.309401 -4.2836003 -4.2468929 -4.201539 -4.1650209 -4.14462 -4.1504297 -4.1716108 -4.1949162 -4.2116284 -4.2186475 -4.2150259][-4.3515019 -4.3323708 -4.3092818 -4.2855339 -4.2497487 -4.2034054 -4.1551871 -4.1293597 -4.1279221 -4.1475215 -4.1747994 -4.1940055 -4.2004824 -4.2000327 -4.19365][-4.3476367 -4.3255458 -4.2975769 -4.2646923 -4.2169013 -4.1603818 -4.1139412 -4.101861 -4.1176329 -4.1487012 -4.1789675 -4.1913657 -4.1902347 -4.1828375 -4.1726418][-4.3448105 -4.3216281 -4.2914395 -4.2557735 -4.2033019 -4.1469054 -4.1099834 -4.1068444 -4.1301985 -4.1687965 -4.1987691 -4.2057996 -4.196104 -4.1794829 -4.1642485][-4.3401527 -4.3165841 -4.2883477 -4.2556643 -4.2101474 -4.166007 -4.1400738 -4.1416435 -4.1697636 -4.2097955 -4.233942 -4.2326827 -4.2140679 -4.1876178 -4.1678648][-4.3313184 -4.3082175 -4.283267 -4.2569432 -4.2207642 -4.1852412 -4.1668534 -4.17242 -4.2046919 -4.2393193 -4.2522526 -4.2414818 -4.2156868 -4.1828232 -4.1597481][-4.3200812 -4.299624 -4.2792258 -4.2578697 -4.2277203 -4.1978579 -4.1819205 -4.1890597 -4.2169476 -4.2416744 -4.246479 -4.2327824 -4.2052493 -4.1706276 -4.1418033][-4.3160815 -4.300868 -4.28692 -4.2681856 -4.241641 -4.2130022 -4.1929126 -4.1931562 -4.2109928 -4.2238793 -4.2233057 -4.2130442 -4.1951957 -4.1693783 -4.142211][-4.3184867 -4.3109293 -4.3016763 -4.2813692 -4.25299 -4.2249551 -4.2048512 -4.201623 -4.2109489 -4.2141957 -4.2105007 -4.2047987 -4.1966877 -4.1833739 -4.1618581][-4.3240943 -4.3225636 -4.3178649 -4.3007097 -4.2752318 -4.2509785 -4.232513 -4.2270169 -4.2313304 -4.2322145 -4.2308946 -4.2333679 -4.232832 -4.2246156 -4.2095733]]...]
INFO - root - 2017-12-07 19:17:33.383955: step 16710, loss = 2.06, batch loss = 2.01 (16.3 examples/sec; 1.967 sec/batch; 36h:16m:51s remains)
INFO - root - 2017-12-07 19:17:54.328548: step 16720, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 39h:11m:55s remains)
INFO - root - 2017-12-07 19:18:15.411377: step 16730, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.078 sec/batch; 38h:19m:02s remains)
INFO - root - 2017-12-07 19:18:36.450898: step 16740, loss = 2.06, batch loss = 2.01 (15.9 examples/sec; 2.017 sec/batch; 37h:11m:43s remains)
INFO - root - 2017-12-07 19:18:57.390054: step 16750, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.086 sec/batch; 38h:28m:09s remains)
INFO - root - 2017-12-07 19:19:18.574280: step 16760, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.095 sec/batch; 38h:37m:07s remains)
INFO - root - 2017-12-07 19:19:39.815408: step 16770, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.149 sec/batch; 39h:36m:40s remains)
INFO - root - 2017-12-07 19:20:00.677468: step 16780, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.082 sec/batch; 38h:22m:04s remains)
INFO - root - 2017-12-07 19:20:21.851716: step 16790, loss = 2.08, batch loss = 2.03 (15.3 examples/sec; 2.087 sec/batch; 38h:27m:46s remains)
INFO - root - 2017-12-07 19:20:42.925851: step 16800, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.120 sec/batch; 39h:03m:52s remains)
2017-12-07 19:20:44.624818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3426356 -4.3477616 -4.3480105 -4.3438854 -4.338006 -4.3344178 -4.3337049 -4.3351789 -4.3377113 -4.3398795 -4.3426881 -4.344523 -4.3455372 -4.3465672 -4.347146][-4.3322887 -4.3384576 -4.3373675 -4.3293972 -4.3205857 -4.3164892 -4.3154778 -4.3174105 -4.3226156 -4.3272996 -4.3324904 -4.3355227 -4.3361154 -4.3360929 -4.3341303][-4.3126678 -4.3145843 -4.3089871 -4.2981825 -4.2879963 -4.2836323 -4.2822261 -4.2850409 -4.2938561 -4.3027959 -4.31023 -4.3137231 -4.3133187 -4.3113508 -4.3046618][-4.2811384 -4.2719374 -4.2561355 -4.2393942 -4.2268548 -4.2218628 -4.2215466 -4.2300406 -4.2476892 -4.2636719 -4.2735476 -4.2768478 -4.2748108 -4.2689123 -4.2569909][-4.2465606 -4.2279415 -4.200398 -4.1746488 -4.1555758 -4.1459408 -4.1423979 -4.1577754 -4.1870756 -4.2118139 -4.224937 -4.2276931 -4.223752 -4.2129369 -4.1978307][-4.2148504 -4.190876 -4.155407 -4.1221161 -4.0977936 -4.0815129 -4.0700703 -4.0874977 -4.1258836 -4.157927 -4.17397 -4.1768088 -4.1737037 -4.1616383 -4.1459241][-4.1794963 -4.1549978 -4.1185102 -4.0839491 -4.06207 -4.0446692 -4.029377 -4.0476117 -4.0867791 -4.1168137 -4.1300225 -4.1344552 -4.1348209 -4.1270475 -4.1146832][-4.1469526 -4.125752 -4.095818 -4.0698657 -4.0567508 -4.0410786 -4.0266871 -4.0393987 -4.0692582 -4.0904608 -4.1005492 -4.1085205 -4.1146665 -4.1140275 -4.1078768][-4.1337032 -4.1219425 -4.1047831 -4.0906081 -4.0850263 -4.0704088 -4.0570555 -4.0606036 -4.0758228 -4.0875835 -4.0955129 -4.1045418 -4.1139755 -4.1205516 -4.1228323][-4.1328044 -4.13253 -4.1273427 -4.1225309 -4.1211233 -4.1096525 -4.0993237 -4.099833 -4.1045327 -4.1114759 -4.1182041 -4.1265759 -4.1370468 -4.1492128 -4.1573858][-4.1412807 -4.1488895 -4.1541872 -4.1597805 -4.1638823 -4.1575217 -4.1498327 -4.1475644 -4.1465778 -4.14867 -4.1555586 -4.1649075 -4.1753588 -4.19024 -4.1988878][-4.1593437 -4.1699028 -4.1810718 -4.1937904 -4.2007861 -4.1974444 -4.1923161 -4.189085 -4.186429 -4.1862326 -4.1929803 -4.2013659 -4.2107773 -4.2247152 -4.2300057][-4.1884508 -4.1997657 -4.2145948 -4.2262 -4.2281919 -4.2229362 -4.2176704 -4.2140303 -4.20967 -4.2069678 -4.2113 -4.2179041 -4.2269611 -4.2396231 -4.243125][-4.217907 -4.2297745 -4.243062 -4.2482729 -4.2430224 -4.2339935 -4.2263184 -4.2199068 -4.2130475 -4.2096181 -4.2134833 -4.2190585 -4.2282348 -4.2406693 -4.2449846][-4.2316179 -4.242826 -4.2531862 -4.2523084 -4.2404838 -4.2259 -4.2148495 -4.2078404 -4.2037249 -4.2041321 -4.2103586 -4.216311 -4.2261539 -4.2376418 -4.2422228]]...]
INFO - root - 2017-12-07 19:21:05.438896: step 16810, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 39h:08m:08s remains)
INFO - root - 2017-12-07 19:21:26.630379: step 16820, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.079 sec/batch; 38h:17m:00s remains)
INFO - root - 2017-12-07 19:21:47.687293: step 16830, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.114 sec/batch; 38h:55m:25s remains)
INFO - root - 2017-12-07 19:22:08.568314: step 16840, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.063 sec/batch; 37h:58m:51s remains)
INFO - root - 2017-12-07 19:22:29.927636: step 16850, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.155 sec/batch; 39h:40m:52s remains)
INFO - root - 2017-12-07 19:22:51.136654: step 16860, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.149 sec/batch; 39h:33m:06s remains)
INFO - root - 2017-12-07 19:23:12.313419: step 16870, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.122 sec/batch; 39h:02m:52s remains)
INFO - root - 2017-12-07 19:23:33.354464: step 16880, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.113 sec/batch; 38h:52m:53s remains)
INFO - root - 2017-12-07 19:23:54.669315: step 16890, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 39h:09m:33s remains)
INFO - root - 2017-12-07 19:24:15.701258: step 16900, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.162 sec/batch; 39h:46m:43s remains)
2017-12-07 19:24:17.201335: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3540735 -4.3503008 -4.3503642 -4.350543 -4.3517866 -4.3510227 -4.3468609 -4.3400869 -4.3330917 -4.3291407 -4.3277783 -4.3268123 -4.32885 -4.3354015 -4.3441114][-4.3414574 -4.3341327 -4.3348613 -4.3380442 -4.3430085 -4.3448005 -4.3397756 -4.3281407 -4.3159652 -4.310039 -4.308032 -4.3060341 -4.3091559 -4.320601 -4.3353271][-4.3186779 -4.3073897 -4.3077784 -4.3152986 -4.327456 -4.3341012 -4.3276644 -4.3094831 -4.2889581 -4.2800231 -4.2782788 -4.2761483 -4.2830234 -4.3011737 -4.32277][-4.2913933 -4.2767653 -4.2717395 -4.2781606 -4.293931 -4.30275 -4.2926702 -4.2691121 -4.241693 -4.2290797 -4.2269692 -4.2284102 -4.2425904 -4.2706909 -4.3007922][-4.2621064 -4.2429929 -4.2330055 -4.2379761 -4.2584085 -4.2680621 -4.2536445 -4.2261581 -4.193151 -4.1755428 -4.172698 -4.1801276 -4.2060318 -4.2461605 -4.2849669][-4.2379375 -4.2123823 -4.197319 -4.2039685 -4.2320957 -4.2443533 -4.2278314 -4.1988053 -4.1614494 -4.1386042 -4.1345444 -4.1498308 -4.1904 -4.2416906 -4.2856832][-4.222856 -4.19289 -4.1741495 -4.1826468 -4.2200322 -4.2415137 -4.2270007 -4.1937189 -4.1514335 -4.1247606 -4.1189032 -4.1370091 -4.1860552 -4.2485008 -4.2981863][-4.210248 -4.1801443 -4.1616898 -4.1752677 -4.2205186 -4.2525282 -4.24353 -4.2081866 -4.1642876 -4.1379991 -4.1288772 -4.1421046 -4.1912117 -4.2599659 -4.3135586][-4.1939874 -4.1656032 -4.1476831 -4.1613064 -4.2089381 -4.24608 -4.2424955 -4.2062511 -4.1669993 -4.1491995 -4.1447291 -4.1506977 -4.1930485 -4.261476 -4.3158092][-4.1689034 -4.1460118 -4.1318054 -4.144762 -4.1887846 -4.2203617 -4.2141619 -4.1713386 -4.133472 -4.125073 -4.1312981 -4.1402435 -4.1811485 -4.2468224 -4.300209][-4.1348276 -4.1236148 -4.1232562 -4.1374836 -4.1754341 -4.1931577 -4.1752729 -4.1190963 -4.0684738 -4.0583282 -4.0733833 -4.0948291 -4.1439838 -4.2115011 -4.2660413][-4.09459 -4.0881915 -4.0955758 -4.1108265 -4.1426692 -4.14936 -4.1220813 -4.0587878 -3.9945285 -3.9724479 -3.980176 -4.0058713 -4.0679369 -4.1456909 -4.2109027][-4.0672584 -4.0599494 -4.0629535 -4.0694809 -4.0906811 -4.0912232 -4.0620513 -4.0066938 -3.9484189 -3.9201636 -3.9090648 -3.9260333 -3.994499 -4.083365 -4.1635537][-4.1055121 -4.1023517 -4.0978017 -4.0933981 -4.096684 -4.0896211 -4.0653305 -4.0300136 -3.9919937 -3.96914 -3.9446275 -3.9424133 -3.9980528 -4.0793862 -4.16178][-4.1740479 -4.1747541 -4.1708279 -4.1641736 -4.1594539 -4.1461892 -4.1260109 -4.1049576 -4.0875049 -4.0763741 -4.0552983 -4.046268 -4.0811586 -4.1443467 -4.2143922]]...]
INFO - root - 2017-12-07 19:24:38.142921: step 16910, loss = 2.09, batch loss = 2.03 (14.6 examples/sec; 2.196 sec/batch; 40h:23m:06s remains)
INFO - root - 2017-12-07 19:24:59.247026: step 16920, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.085 sec/batch; 38h:21m:08s remains)
INFO - root - 2017-12-07 19:25:20.449641: step 16930, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.156 sec/batch; 39h:39m:06s remains)
INFO - root - 2017-12-07 19:25:41.295860: step 16940, loss = 2.09, batch loss = 2.03 (14.6 examples/sec; 2.190 sec/batch; 40h:15m:24s remains)
INFO - root - 2017-12-07 19:26:02.664373: step 16950, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.079 sec/batch; 38h:12m:36s remains)
INFO - root - 2017-12-07 19:26:23.765193: step 16960, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.093 sec/batch; 38h:27m:33s remains)
INFO - root - 2017-12-07 19:26:44.575151: step 16970, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.076 sec/batch; 38h:09m:04s remains)
INFO - root - 2017-12-07 19:27:05.792460: step 16980, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.109 sec/batch; 38h:45m:02s remains)
INFO - root - 2017-12-07 19:27:26.897557: step 16990, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.123 sec/batch; 38h:59m:56s remains)
INFO - root - 2017-12-07 19:27:47.917938: step 17000, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.074 sec/batch; 38h:06m:03s remains)
2017-12-07 19:27:49.398993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2892537 -4.2832294 -4.2823563 -4.2872033 -4.2929726 -4.2961521 -4.3001208 -4.3063951 -4.311554 -4.3173308 -4.3255887 -4.3341284 -4.3387346 -4.3380203 -4.3323417][-4.2503347 -4.2408609 -4.2399564 -4.2492442 -4.260694 -4.26812 -4.2772875 -4.2901912 -4.2984614 -4.3073044 -4.3183408 -4.3268566 -4.3299017 -4.3284535 -4.3230538][-4.2239132 -4.2127175 -4.2108397 -4.2220817 -4.2335849 -4.2389703 -4.2480216 -4.2620888 -4.2712536 -4.28203 -4.2975545 -4.3090892 -4.314302 -4.3153024 -4.3117976][-4.2223206 -4.2101269 -4.20499 -4.2122941 -4.2182345 -4.2170057 -4.2231708 -4.2346063 -4.2425933 -4.2561507 -4.277585 -4.2924333 -4.2956018 -4.2957058 -4.2935271][-4.23578 -4.2184095 -4.2046762 -4.2007947 -4.1987386 -4.1908984 -4.1916561 -4.1995373 -4.2076392 -4.2261047 -4.2521157 -4.2669387 -4.2675438 -4.2669983 -4.2698212][-4.2456574 -4.2246733 -4.201046 -4.1819878 -4.1669941 -4.1508422 -4.1440964 -4.1451969 -4.1497922 -4.1691737 -4.1984224 -4.2151742 -4.2171984 -4.221034 -4.2363806][-4.2336535 -4.2089491 -4.1744065 -4.1396666 -4.1118193 -4.0840974 -4.0630541 -4.0494566 -4.0484829 -4.0684824 -4.1032767 -4.1261244 -4.1380849 -4.1577082 -4.1944928][-4.2131071 -4.1849332 -4.1451964 -4.0990715 -4.0592871 -4.0208573 -3.9873333 -3.9644792 -3.9638863 -3.9893529 -4.0239215 -4.048337 -4.0771923 -4.1223993 -4.1759534][-4.211657 -4.1888862 -4.1593742 -4.12305 -4.0903273 -4.0580392 -4.0295892 -4.0156994 -4.0244641 -4.0462232 -4.0695071 -4.0885544 -4.1215754 -4.1680384 -4.2102437][-4.226244 -4.2119331 -4.2008533 -4.187088 -4.1759882 -4.1644549 -4.1554923 -4.1554084 -4.16825 -4.1791935 -4.1826329 -4.1868715 -4.2067857 -4.2337279 -4.2498455][-4.2425084 -4.2369995 -4.2397437 -4.2414985 -4.2433395 -4.2440968 -4.2434654 -4.2443786 -4.2529106 -4.2519264 -4.2404933 -4.2328176 -4.239933 -4.2528033 -4.257587][-4.2486806 -4.2488313 -4.2555008 -4.25986 -4.2614303 -4.26268 -4.261477 -4.2598839 -4.264359 -4.2599821 -4.2464848 -4.2373443 -4.2406955 -4.2515755 -4.2578039][-4.2524486 -4.255415 -4.26175 -4.2658138 -4.2628512 -4.2569084 -4.2516656 -4.2502022 -4.2528734 -4.2496915 -4.2443228 -4.2435875 -4.2493472 -4.2605295 -4.2698741][-4.2522478 -4.2576237 -4.2635241 -4.2653351 -4.2540503 -4.23577 -4.2242165 -4.2249584 -4.2255392 -4.2221613 -4.2274075 -4.2398186 -4.2499361 -4.2608714 -4.2712817][-4.2494836 -4.2527533 -4.2565446 -4.2538376 -4.2365131 -4.2130413 -4.19846 -4.1960177 -4.1927528 -4.1886005 -4.1981621 -4.21492 -4.2282763 -4.2423215 -4.256865]]...]
INFO - root - 2017-12-07 19:28:10.323013: step 17010, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 39h:14m:16s remains)
INFO - root - 2017-12-07 19:28:31.669588: step 17020, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.116 sec/batch; 38h:51m:04s remains)
INFO - root - 2017-12-07 19:28:52.661665: step 17030, loss = 2.08, batch loss = 2.02 (16.2 examples/sec; 1.974 sec/batch; 36h:14m:51s remains)
INFO - root - 2017-12-07 19:29:13.733137: step 17040, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.131 sec/batch; 39h:06m:58s remains)
INFO - root - 2017-12-07 19:29:34.973953: step 17050, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.151 sec/batch; 39h:28m:40s remains)
INFO - root - 2017-12-07 19:29:56.190166: step 17060, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.112 sec/batch; 38h:45m:20s remains)
INFO - root - 2017-12-07 19:30:16.982190: step 17070, loss = 2.07, batch loss = 2.02 (14.4 examples/sec; 2.216 sec/batch; 40h:40m:01s remains)
INFO - root - 2017-12-07 19:30:38.085412: step 17080, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.107 sec/batch; 38h:39m:06s remains)
INFO - root - 2017-12-07 19:30:59.286389: step 17090, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.095 sec/batch; 38h:25m:57s remains)
INFO - root - 2017-12-07 19:31:20.105489: step 17100, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 39h:06m:56s remains)
2017-12-07 19:31:21.767885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2516909 -4.2365808 -4.2271852 -4.2189331 -4.2047863 -4.1897216 -4.1741228 -4.1642222 -4.1753864 -4.19502 -4.2002044 -4.1844344 -4.1807303 -4.1927905 -4.2130547][-4.2485809 -4.2354236 -4.2278261 -4.2198372 -4.2048774 -4.1920977 -4.181828 -4.1740561 -4.18721 -4.2104869 -4.21873 -4.2021241 -4.1908298 -4.1999383 -4.2159843][-4.2707691 -4.2593322 -4.2496333 -4.2392507 -4.2246418 -4.2160616 -4.2095923 -4.202395 -4.2136822 -4.2353959 -4.2446117 -4.2302628 -4.2167473 -4.2192607 -4.2297492][-4.2865291 -4.2754726 -4.2624826 -4.2453566 -4.2281027 -4.2199512 -4.2145929 -4.2076044 -4.2198086 -4.2423296 -4.2538581 -4.2446284 -4.2307677 -4.2307267 -4.23858][-4.2780795 -4.2648835 -4.2454414 -4.2206116 -4.2020946 -4.1991425 -4.1945024 -4.1884422 -4.2076173 -4.2361298 -4.253233 -4.2500625 -4.2380948 -4.2376552 -4.2421169][-4.2598743 -4.2353225 -4.2049894 -4.172276 -4.1540122 -4.1544828 -4.1493917 -4.1415863 -4.1635122 -4.2011194 -4.2305822 -4.2402372 -4.2384424 -4.244555 -4.2480516][-4.2387466 -4.205359 -4.1690011 -4.1309896 -4.1059561 -4.0943637 -4.0745926 -4.05918 -4.0898504 -4.1410484 -4.1821723 -4.2071266 -4.2209668 -4.2368178 -4.2433119][-4.2165117 -4.1815214 -4.1522083 -4.1206341 -4.0867305 -4.0550666 -4.0136681 -3.9878974 -4.0297384 -4.0943327 -4.1404424 -4.1703973 -4.1938138 -4.2138453 -4.2198396][-4.1927152 -4.1661 -4.1525288 -4.1370606 -4.1068454 -4.069838 -4.0283775 -4.0120549 -4.0547171 -4.1100249 -4.1417704 -4.1611528 -4.1795692 -4.1906452 -4.1892433][-4.1773534 -4.1660748 -4.1721907 -4.1756592 -4.1541963 -4.1250615 -4.1049891 -4.1081457 -4.141221 -4.1706715 -4.1808147 -4.1820259 -4.1826077 -4.1751595 -4.16123][-4.1808605 -4.1788492 -4.1944113 -4.2090178 -4.195406 -4.1771708 -4.1752567 -4.1882792 -4.2082033 -4.2199163 -4.2178726 -4.208859 -4.1950722 -4.170938 -4.1449575][-4.199347 -4.1984076 -4.2146616 -4.2288737 -4.2182717 -4.2063651 -4.2105389 -4.2256355 -4.2329369 -4.2368717 -4.2341571 -4.2259469 -4.2119265 -4.1907096 -4.1646109][-4.2361031 -4.2323065 -4.2421818 -4.2504172 -4.2372332 -4.2212977 -4.2193508 -4.2301545 -4.2331877 -4.2399349 -4.2450557 -4.242991 -4.2369337 -4.22544 -4.2079315][-4.2785125 -4.2735124 -4.276216 -4.276031 -4.25832 -4.2354183 -4.2193351 -4.2216797 -4.2253108 -4.2358027 -4.2498317 -4.2546625 -4.2541409 -4.2500453 -4.2394338][-4.3073 -4.303628 -4.3031306 -4.2979627 -4.2780051 -4.2479577 -4.2179375 -4.2085567 -4.2121978 -4.227911 -4.2479391 -4.2547245 -4.252738 -4.2525091 -4.2472577]]...]
INFO - root - 2017-12-07 19:31:42.865617: step 17110, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.086 sec/batch; 38h:15m:34s remains)
INFO - root - 2017-12-07 19:32:04.142865: step 17120, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 38h:32m:53s remains)
INFO - root - 2017-12-07 19:32:24.919901: step 17130, loss = 2.09, batch loss = 2.03 (16.4 examples/sec; 1.957 sec/batch; 35h:52m:09s remains)
INFO - root - 2017-12-07 19:32:45.792735: step 17140, loss = 2.06, batch loss = 2.01 (15.4 examples/sec; 2.074 sec/batch; 38h:01m:24s remains)
INFO - root - 2017-12-07 19:33:07.023721: step 17150, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.107 sec/batch; 38h:36m:28s remains)
INFO - root - 2017-12-07 19:33:28.083625: step 17160, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 38h:42m:53s remains)
INFO - root - 2017-12-07 19:33:49.105418: step 17170, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.145 sec/batch; 39h:18m:21s remains)
INFO - root - 2017-12-07 19:34:10.128292: step 17180, loss = 2.08, batch loss = 2.02 (15.6 examples/sec; 2.048 sec/batch; 37h:30m:47s remains)
INFO - root - 2017-12-07 19:34:31.522982: step 17190, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 39h:00m:46s remains)
INFO - root - 2017-12-07 19:34:52.588624: step 17200, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 38h:29m:33s remains)
2017-12-07 19:34:54.212209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3534765 -4.3474255 -4.3359609 -4.3251991 -4.3121176 -4.2956667 -4.289 -4.2912917 -4.2959914 -4.3022223 -4.312058 -4.3159471 -4.3159966 -4.3167186 -4.317759][-4.3581152 -4.351778 -4.3371606 -4.3232307 -4.3020763 -4.2746515 -4.2625985 -4.2651305 -4.2730145 -4.2871895 -4.3081646 -4.3213181 -4.3225422 -4.3220019 -4.3224421][-4.3626842 -4.3548179 -4.3349514 -4.3149748 -4.2862964 -4.251111 -4.2370934 -4.2359033 -4.2369995 -4.2543015 -4.2876439 -4.315062 -4.3236241 -4.3239942 -4.323267][-4.3653173 -4.3542447 -4.3253641 -4.2960463 -4.257544 -4.2187991 -4.21087 -4.2072687 -4.1973534 -4.21239 -4.2566566 -4.2965436 -4.3154349 -4.3203249 -4.3228354][-4.359745 -4.3432479 -4.3046365 -4.2608047 -4.2088456 -4.1661625 -4.1644959 -4.1585112 -4.1354322 -4.14952 -4.2099981 -4.2654181 -4.2925367 -4.2989807 -4.3030014][-4.3484864 -4.3220091 -4.2705569 -4.2093821 -4.1410065 -4.0960517 -4.099822 -4.0897665 -4.0516458 -4.0643258 -4.1444144 -4.2178721 -4.25232 -4.2584677 -4.2596812][-4.3388896 -4.304162 -4.2411766 -4.164485 -4.0850282 -4.0432105 -4.0581193 -4.0495992 -4.0027976 -4.0126429 -4.1008043 -4.183826 -4.2207079 -4.2209506 -4.2170715][-4.3376102 -4.3014851 -4.234076 -4.14961 -4.062273 -4.023591 -4.0496893 -4.0508771 -4.0127645 -4.0203185 -4.0941057 -4.165801 -4.1916871 -4.1843204 -4.180541][-4.3428464 -4.31204 -4.2501626 -4.1684985 -4.0852666 -4.052774 -4.0844913 -4.0965767 -4.0786605 -4.0856156 -4.1315885 -4.176053 -4.1840529 -4.1663775 -4.1621518][-4.3496504 -4.3254933 -4.2757292 -4.2087278 -4.1354027 -4.1066961 -4.1394782 -4.1598344 -4.1598849 -4.1692762 -4.193017 -4.2106361 -4.2014709 -4.1727228 -4.1639271][-4.3553233 -4.3345308 -4.29313 -4.2410789 -4.1810327 -4.1544662 -4.1831541 -4.2087436 -4.2218323 -4.2341752 -4.2443185 -4.2427998 -4.2241344 -4.1927657 -4.1832547][-4.3550739 -4.3339305 -4.2967114 -4.2538233 -4.2077303 -4.186986 -4.2134433 -4.2457428 -4.2712111 -4.2881832 -4.2934361 -4.2811852 -4.2574658 -4.2297888 -4.2232065][-4.3569851 -4.33809 -4.3058953 -4.2734261 -4.2420235 -4.2292132 -4.2536016 -4.2868814 -4.3153996 -4.3301897 -4.3324337 -4.3194771 -4.2977586 -4.2760515 -4.2709761][-4.3604355 -4.3451867 -4.3183389 -4.2934284 -4.272522 -4.2662325 -4.2851682 -4.312439 -4.3341742 -4.3422861 -4.3446946 -4.3368759 -4.3244033 -4.3119497 -4.304338][-4.3631268 -4.3535223 -4.3329945 -4.3127894 -4.2989945 -4.2978382 -4.315979 -4.3351188 -4.3451242 -4.3428545 -4.3403282 -4.3323407 -4.3238425 -4.31779 -4.3138275]]...]
INFO - root - 2017-12-07 19:35:15.396925: step 17210, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.148 sec/batch; 39h:19m:56s remains)
INFO - root - 2017-12-07 19:35:36.514631: step 17220, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.080 sec/batch; 38h:04m:51s remains)
INFO - root - 2017-12-07 19:35:57.309842: step 17230, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 38h:48m:26s remains)
INFO - root - 2017-12-07 19:36:18.415060: step 17240, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.155 sec/batch; 39h:26m:04s remains)
INFO - root - 2017-12-07 19:36:39.756568: step 17250, loss = 2.06, batch loss = 2.01 (14.8 examples/sec; 2.169 sec/batch; 39h:41m:50s remains)
INFO - root - 2017-12-07 19:37:00.455073: step 17260, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.127 sec/batch; 38h:54m:44s remains)
INFO - root - 2017-12-07 19:37:21.277870: step 17270, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.099 sec/batch; 38h:24m:02s remains)
INFO - root - 2017-12-07 19:37:42.458156: step 17280, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.123 sec/batch; 38h:49m:26s remains)
INFO - root - 2017-12-07 19:38:03.431524: step 17290, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 38h:40m:02s remains)
INFO - root - 2017-12-07 19:38:24.507838: step 17300, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.062 sec/batch; 37h:41m:54s remains)
2017-12-07 19:38:26.093419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3326612 -4.3240204 -4.3198833 -4.3202934 -4.3228149 -4.3255844 -4.3288517 -4.3288383 -4.3245707 -4.3225675 -4.3229275 -4.3257666 -4.3328066 -4.3400936 -4.3455429][-4.3119669 -4.2986989 -4.2932544 -4.290545 -4.2904248 -4.2925286 -4.2929821 -4.2908297 -4.2833023 -4.2836056 -4.2897315 -4.2963758 -4.3088841 -4.3183928 -4.3265939][-4.2939248 -4.2769613 -4.2669621 -4.2569957 -4.247334 -4.2449336 -4.2450838 -4.2414827 -4.2328987 -4.2394915 -4.2537045 -4.2658658 -4.2808356 -4.2920337 -4.3029771][-4.2837915 -4.2702723 -4.259336 -4.2388043 -4.2154317 -4.2035437 -4.1964812 -4.18626 -4.1807652 -4.19772 -4.2222953 -4.2416663 -4.2569709 -4.2678041 -4.2796879][-4.2800908 -4.2731628 -4.26278 -4.2323937 -4.1955562 -4.1660886 -4.1409645 -4.1152954 -4.1148453 -4.1485815 -4.1867251 -4.2190509 -4.2408562 -4.2542171 -4.2661238][-4.2686572 -4.2621036 -4.2439156 -4.2012177 -4.1485605 -4.0957961 -4.0471139 -4.0015106 -4.0060363 -4.0610723 -4.1190543 -4.1700072 -4.2066817 -4.2306056 -4.2494483][-4.2403216 -4.2224283 -4.1852384 -4.1207848 -4.045351 -3.9671278 -3.8900149 -3.8244047 -3.844166 -3.9432051 -4.0412073 -4.1159282 -4.1668968 -4.2034082 -4.232461][-4.2051568 -4.1740532 -4.1255507 -4.049521 -3.965306 -3.8810663 -3.7920334 -3.7240343 -3.7676172 -3.8973737 -4.0156584 -4.1007352 -4.1567073 -4.1990538 -4.2325068][-4.1861949 -4.1531048 -4.1098852 -4.0458865 -3.9844527 -3.9355547 -3.8845029 -3.8570223 -3.9020967 -3.9968002 -4.0853677 -4.14942 -4.1934042 -4.2294445 -4.257216][-4.19258 -4.1677589 -4.1399264 -4.1019111 -4.0698524 -4.0475693 -4.0213838 -4.013515 -4.04564 -4.1061378 -4.16835 -4.214787 -4.2487411 -4.2770996 -4.2959633][-4.2144036 -4.1997414 -4.187016 -4.1724977 -4.1609278 -4.1518645 -4.1377974 -4.1356521 -4.154428 -4.193862 -4.2401628 -4.2757149 -4.2999864 -4.3162813 -4.3253555][-4.2457976 -4.2388616 -4.2359505 -4.2371364 -4.237124 -4.2354054 -4.2310438 -4.2306895 -4.2357759 -4.2545528 -4.28332 -4.3097019 -4.3274493 -4.3373704 -4.34197][-4.2696981 -4.2702351 -4.27449 -4.2818809 -4.2871366 -4.2888741 -4.2877369 -4.2847614 -4.2823076 -4.290659 -4.3095174 -4.3287044 -4.3409653 -4.3460989 -4.3473372][-4.2927303 -4.2961807 -4.3040953 -4.3132553 -4.3194623 -4.3216944 -4.3204389 -4.3184395 -4.315887 -4.3202558 -4.3309503 -4.3421063 -4.3482871 -4.3505955 -4.3500638][-4.3172131 -4.321547 -4.3293886 -4.3375144 -4.342473 -4.34274 -4.3403387 -4.3373637 -4.335031 -4.3373981 -4.3433123 -4.349813 -4.3545275 -4.3576145 -4.3578315]]...]
INFO - root - 2017-12-07 19:38:47.302964: step 17310, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 39h:28m:48s remains)
INFO - root - 2017-12-07 19:39:08.219417: step 17320, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 1.886 sec/batch; 34h:28m:55s remains)
INFO - root - 2017-12-07 19:39:29.169818: step 17330, loss = 2.06, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 38h:44m:39s remains)
INFO - root - 2017-12-07 19:39:50.249014: step 17340, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 38h:24m:07s remains)
INFO - root - 2017-12-07 19:40:11.328534: step 17350, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.128 sec/batch; 38h:52m:46s remains)
INFO - root - 2017-12-07 19:40:32.067296: step 17360, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.082 sec/batch; 38h:02m:03s remains)
INFO - root - 2017-12-07 19:40:53.243800: step 17370, loss = 2.08, batch loss = 2.02 (14.4 examples/sec; 2.229 sec/batch; 40h:42m:32s remains)
INFO - root - 2017-12-07 19:41:14.510629: step 17380, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 38h:40m:15s remains)
INFO - root - 2017-12-07 19:41:35.313685: step 17390, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.170 sec/batch; 39h:37m:19s remains)
INFO - root - 2017-12-07 19:41:56.452133: step 17400, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.131 sec/batch; 38h:54m:39s remains)
2017-12-07 19:41:58.053667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2308693 -4.2206531 -4.2175283 -4.220705 -4.2115092 -4.1861324 -4.1612697 -4.1563759 -4.1829114 -4.2180552 -4.2380509 -4.2474327 -4.2391596 -4.22855 -4.2299247][-4.2118392 -4.2032976 -4.2012205 -4.2079573 -4.2004805 -4.1665926 -4.1257935 -4.1131353 -4.1400704 -4.1816964 -4.2094975 -4.2245245 -4.2174878 -4.2046981 -4.2075415][-4.198997 -4.1915112 -4.1912408 -4.1996546 -4.1909466 -4.1490464 -4.0954337 -4.0781121 -4.1046953 -4.1469712 -4.1793609 -4.1980705 -4.1944771 -4.1823077 -4.1881418][-4.1841521 -4.1816473 -4.1857562 -4.1925707 -4.180438 -4.1320796 -4.0732975 -4.0554957 -4.0869551 -4.1331496 -4.1634927 -4.1756568 -4.1715641 -4.1636171 -4.175653][-4.1670012 -4.1772137 -4.1879177 -4.1886916 -4.1658154 -4.109252 -4.0494781 -4.0396991 -4.0837631 -4.1349187 -4.1579185 -4.161334 -4.1569338 -4.1573086 -4.1791487][-4.1379523 -4.166647 -4.1888127 -4.1869612 -4.1497426 -4.0815706 -4.0229654 -4.0307159 -4.0946379 -4.1563435 -4.1740427 -4.1652679 -4.1524248 -4.1532784 -4.185698][-4.1114469 -4.1491094 -4.1784248 -4.1728325 -4.1189833 -4.0365224 -3.9827363 -4.0103674 -4.0968747 -4.1709313 -4.1888523 -4.171288 -4.1535487 -4.1527748 -4.1879373][-4.1166949 -4.1480446 -4.1723614 -4.16156 -4.098331 -4.0049076 -3.9575148 -4.0030141 -4.1005235 -4.1773443 -4.1970038 -4.179091 -4.1610694 -4.1615009 -4.1937723][-4.1478825 -4.1728525 -4.1882744 -4.1790342 -4.1279192 -4.0520635 -4.0128946 -4.0534148 -4.1299829 -4.191298 -4.2066112 -4.1907635 -4.1719689 -4.1699357 -4.195272][-4.1799884 -4.1994128 -4.2084808 -4.203135 -4.1695232 -4.1236119 -4.0984931 -4.1244216 -4.1677923 -4.2050738 -4.2141571 -4.2013006 -4.1869607 -4.1847911 -4.2036796][-4.2169528 -4.2298779 -4.2346444 -4.2305136 -4.2101035 -4.1864047 -4.1700864 -4.1837711 -4.2064891 -4.2261491 -4.2266135 -4.212934 -4.2059345 -4.2091236 -4.2248344][-4.258533 -4.2660246 -4.2656145 -4.2595553 -4.247251 -4.233098 -4.2186875 -4.2223005 -4.2336082 -4.245223 -4.2424726 -4.231986 -4.2347574 -4.2439404 -4.2553282][-4.2922268 -4.2938776 -4.2871671 -4.27762 -4.2718968 -4.2625732 -4.2506633 -4.2481709 -4.25379 -4.2643666 -4.2623682 -4.2583427 -4.2647705 -4.272604 -4.278069][-4.3076458 -4.3066311 -4.2981124 -4.2883816 -4.2865481 -4.2821712 -4.2721682 -4.267478 -4.2724662 -4.2848482 -4.2870746 -4.2871385 -4.2894316 -4.288321 -4.2885809][-4.3084331 -4.3052416 -4.2977247 -4.2896423 -4.2881665 -4.2880392 -4.2824316 -4.2793283 -4.2851872 -4.2972579 -4.30397 -4.3057537 -4.3036551 -4.2969327 -4.2957268]]...]
INFO - root - 2017-12-07 19:42:19.239156: step 17410, loss = 2.06, batch loss = 2.00 (14.7 examples/sec; 2.181 sec/batch; 39h:49m:17s remains)
INFO - root - 2017-12-07 19:42:39.934565: step 17420, loss = 2.07, batch loss = 2.01 (17.2 examples/sec; 1.864 sec/batch; 34h:00m:49s remains)
INFO - root - 2017-12-07 19:43:01.063404: step 17430, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 38h:22m:00s remains)
INFO - root - 2017-12-07 19:43:22.371929: step 17440, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.149 sec/batch; 39h:12m:33s remains)
INFO - root - 2017-12-07 19:43:43.305454: step 17450, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.072 sec/batch; 37h:48m:29s remains)
INFO - root - 2017-12-07 19:44:04.393786: step 17460, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.117 sec/batch; 38h:36m:51s remains)
INFO - root - 2017-12-07 19:44:25.480880: step 17470, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.089 sec/batch; 38h:06m:06s remains)
INFO - root - 2017-12-07 19:44:46.507926: step 17480, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 38h:38m:03s remains)
INFO - root - 2017-12-07 19:45:07.287742: step 17490, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.096 sec/batch; 38h:13m:14s remains)
INFO - root - 2017-12-07 19:45:28.386409: step 17500, loss = 2.08, batch loss = 2.02 (15.6 examples/sec; 2.058 sec/batch; 37h:30m:26s remains)
2017-12-07 19:45:29.919558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1842484 -4.1992307 -4.2080712 -4.2072048 -4.2114024 -4.2223873 -4.2302504 -4.2375216 -4.2498631 -4.2666712 -4.273932 -4.2617245 -4.2388015 -4.209619 -4.183394][-4.1842375 -4.1997948 -4.2142353 -4.2181306 -4.2247906 -4.2367177 -4.2403378 -4.2388878 -4.2502489 -4.2719736 -4.2874045 -4.280252 -4.2599344 -4.2304482 -4.205616][-4.194325 -4.2083335 -4.2246137 -4.2324347 -4.2407041 -4.2485495 -4.24363 -4.229095 -4.2351532 -4.260807 -4.2832665 -4.283555 -4.2672091 -4.2388706 -4.2164268][-4.2028494 -4.2153783 -4.228847 -4.2390337 -4.2488055 -4.2510395 -4.2309089 -4.1999331 -4.1941948 -4.2220306 -4.2551317 -4.2684546 -4.258647 -4.2322388 -4.2129679][-4.2186542 -4.2288642 -4.2385716 -4.2474818 -4.2546973 -4.24532 -4.1997972 -4.1432195 -4.1256094 -4.1607165 -4.2087369 -4.235333 -4.2363625 -4.220449 -4.2039495][-4.2385592 -4.2452364 -4.2525744 -4.259707 -4.259903 -4.2317133 -4.1506629 -4.0591836 -4.0250053 -4.0758805 -4.1467547 -4.1868548 -4.1977434 -4.1929584 -4.1781969][-4.24584 -4.2531476 -4.2608666 -4.2660546 -4.2595158 -4.2138224 -4.09986 -3.9637494 -3.8997717 -3.9754875 -4.0862303 -4.1531491 -4.1813049 -4.1837063 -4.167491][-4.2417107 -4.2575884 -4.2701163 -4.2804012 -4.275331 -4.2210693 -4.0922022 -3.9327395 -3.8553383 -3.9430363 -4.0665607 -4.1452446 -4.1871676 -4.1967382 -4.1848483][-4.2332435 -4.2596745 -4.2825007 -4.3015728 -4.3031 -4.253994 -4.139008 -4.0056973 -3.9509549 -4.0178275 -4.1094885 -4.1705933 -4.2080812 -4.2180181 -4.2112012][-4.219635 -4.2532587 -4.2816448 -4.3061786 -4.3185582 -4.2837448 -4.19365 -4.0969024 -4.0666165 -4.1073089 -4.1612749 -4.1958227 -4.217 -4.2193336 -4.2158871][-4.2084 -4.2446246 -4.2740107 -4.2952814 -4.3119655 -4.2863131 -4.213397 -4.1387253 -4.1228051 -4.1518378 -4.187017 -4.2012 -4.2053685 -4.1998615 -4.1945319][-4.2016511 -4.2301369 -4.2574058 -4.2791629 -4.2957711 -4.2786374 -4.2205515 -4.1643324 -4.1547551 -4.1771359 -4.2003083 -4.2013187 -4.1898212 -4.1751328 -4.1654463][-4.21729 -4.2310014 -4.2504973 -4.2702794 -4.2856312 -4.276999 -4.2338986 -4.1903811 -4.1786566 -4.1902962 -4.2016621 -4.1945481 -4.1725173 -4.1538343 -4.1418214][-4.2305913 -4.2345939 -4.244648 -4.2585254 -4.2710328 -4.268178 -4.2363658 -4.2035508 -4.1938987 -4.2017627 -4.2094674 -4.1989493 -4.1778769 -4.1625128 -4.150115][-4.2277145 -4.228096 -4.23483 -4.2404189 -4.2453551 -4.2439222 -4.2230186 -4.2014928 -4.198205 -4.206028 -4.211472 -4.2017078 -4.1869049 -4.1788754 -4.1692705]]...]
INFO - root - 2017-12-07 19:45:51.094288: step 17510, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.139 sec/batch; 38h:59m:31s remains)
INFO - root - 2017-12-07 19:46:12.120290: step 17520, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.158 sec/batch; 39h:19m:51s remains)
INFO - root - 2017-12-07 19:46:33.347285: step 17530, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 38h:34m:57s remains)
INFO - root - 2017-12-07 19:46:54.561774: step 17540, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.105 sec/batch; 38h:21m:00s remains)
INFO - root - 2017-12-07 19:47:15.415809: step 17550, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.178 sec/batch; 39h:39m:58s remains)
INFO - root - 2017-12-07 19:47:36.794726: step 17560, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.107 sec/batch; 38h:22m:39s remains)
INFO - root - 2017-12-07 19:47:58.127771: step 17570, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.110 sec/batch; 38h:25m:04s remains)
INFO - root - 2017-12-07 19:48:19.107687: step 17580, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 38h:30m:04s remains)
INFO - root - 2017-12-07 19:48:40.013650: step 17590, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.122 sec/batch; 38h:37m:50s remains)
INFO - root - 2017-12-07 19:49:01.169489: step 17600, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.074 sec/batch; 37h:45m:10s remains)
2017-12-07 19:49:02.723767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2231226 -4.2243505 -4.2139511 -4.2105756 -4.2158933 -4.2108312 -4.2031136 -4.2000341 -4.1918178 -4.1852188 -4.1841893 -4.1844797 -4.1824512 -4.1709638 -4.1379333][-4.2152028 -4.2125683 -4.2019725 -4.1947551 -4.1956863 -4.1908751 -4.1850491 -4.1844521 -4.1849833 -4.1910958 -4.1969824 -4.1950493 -4.1843195 -4.167872 -4.1372652][-4.2019992 -4.1927047 -4.1812959 -4.1704974 -4.1700993 -4.1729655 -4.1724353 -4.1769094 -4.18824 -4.2034836 -4.2128768 -4.2073264 -4.1886468 -4.1700406 -4.1452942][-4.1826782 -4.1697245 -4.1643748 -4.1592088 -4.159049 -4.1624765 -4.159698 -4.1656532 -4.1863875 -4.206934 -4.2157173 -4.2124748 -4.1950836 -4.1802192 -4.1614513][-4.1553254 -4.1465669 -4.1544037 -4.1554494 -4.1522913 -4.1438727 -4.1265249 -4.134973 -4.1701555 -4.1996908 -4.2093725 -4.2134233 -4.2032351 -4.1912618 -4.1725106][-4.1263638 -4.1205869 -4.1418204 -4.1499505 -4.1431251 -4.1126127 -4.0636721 -4.072731 -4.1303372 -4.1666422 -4.1778612 -4.1867213 -4.1871557 -4.1803265 -4.1648779][-4.0942945 -4.0853806 -4.1101232 -4.1224432 -4.1069937 -4.04328 -3.9436858 -3.9530284 -4.0431933 -4.094882 -4.113843 -4.1273794 -4.1414013 -4.1436639 -4.1361175][-4.0786943 -4.0569553 -4.072032 -4.0833392 -4.0588889 -3.9692569 -3.8352 -3.8544805 -3.9761057 -4.0415688 -4.0704231 -4.0895553 -4.111115 -4.1181207 -4.1179218][-4.09766 -4.0722666 -4.0785871 -4.0904737 -4.0702205 -4.0005832 -3.9056013 -3.9235885 -4.0111527 -4.0590439 -4.0859852 -4.1032171 -4.1222334 -4.1280813 -4.128552][-4.140111 -4.125731 -4.1311827 -4.1375093 -4.1222653 -4.0813842 -4.031013 -4.0410733 -4.0844154 -4.106019 -4.1257172 -4.1384048 -4.1513805 -4.15337 -4.1516876][-4.1826377 -4.1778088 -4.1805997 -4.1783137 -4.1662025 -4.1441298 -4.1224914 -4.1288471 -4.1427097 -4.1506119 -4.1651115 -4.1745925 -4.1826258 -4.1827569 -4.1821704][-4.2106009 -4.208653 -4.2077684 -4.2012115 -4.1921873 -4.1793995 -4.1719666 -4.1758013 -4.1782732 -4.1795983 -4.1916156 -4.2005153 -4.2051005 -4.2062082 -4.2097926][-4.2377777 -4.23617 -4.2322474 -4.2251511 -4.2190104 -4.2118821 -4.2092757 -4.2118487 -4.2087803 -4.206789 -4.2162051 -4.2269034 -4.2316132 -4.2335091 -4.2387757][-4.2659221 -4.26462 -4.2601166 -4.2552834 -4.252223 -4.2496805 -4.2500319 -4.2499137 -4.2443404 -4.2410383 -4.2484679 -4.2593546 -4.2647076 -4.2684054 -4.2740269][-4.3003712 -4.3004618 -4.2972517 -4.2951984 -4.2941446 -4.2946892 -4.2967381 -4.2952561 -4.2890863 -4.2850184 -4.2883306 -4.2943563 -4.2986994 -4.3044195 -4.3111358]]...]
INFO - root - 2017-12-07 19:49:23.788681: step 17610, loss = 2.07, batch loss = 2.01 (15.8 examples/sec; 2.021 sec/batch; 36h:47m:06s remains)
INFO - root - 2017-12-07 19:49:44.689303: step 17620, loss = 2.06, batch loss = 2.01 (15.4 examples/sec; 2.081 sec/batch; 37h:51m:23s remains)
INFO - root - 2017-12-07 19:50:05.869354: step 17630, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.103 sec/batch; 38h:15m:42s remains)
INFO - root - 2017-12-07 19:50:27.060174: step 17640, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.136 sec/batch; 38h:50m:59s remains)
INFO - root - 2017-12-07 19:50:48.000771: step 17650, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.181 sec/batch; 39h:39m:41s remains)
INFO - root - 2017-12-07 19:51:09.160570: step 17660, loss = 2.08, batch loss = 2.02 (14.5 examples/sec; 2.209 sec/batch; 40h:10m:08s remains)
INFO - root - 2017-12-07 19:51:30.315137: step 17670, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.174 sec/batch; 39h:31m:08s remains)
INFO - root - 2017-12-07 19:51:51.193157: step 17680, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.090 sec/batch; 37h:59m:31s remains)
INFO - root - 2017-12-07 19:52:12.433920: step 17690, loss = 2.08, batch loss = 2.03 (15.5 examples/sec; 2.066 sec/batch; 37h:33m:02s remains)
INFO - root - 2017-12-07 19:52:33.518872: step 17700, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.073 sec/batch; 37h:40m:26s remains)
2017-12-07 19:52:35.125820: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1246142 -4.1275635 -4.1452928 -4.1718016 -4.1899881 -4.1996975 -4.1930571 -4.170404 -4.163384 -4.1704507 -4.1722369 -4.1557369 -4.1081796 -4.0538692 -4.0243425][-4.1237435 -4.1312675 -4.1522779 -4.1710329 -4.1835618 -4.1992445 -4.2081237 -4.2063966 -4.2078714 -4.2063508 -4.1946363 -4.1707511 -4.134275 -4.0983315 -4.0751624][-4.1349273 -4.146915 -4.1658955 -4.1744289 -4.1777377 -4.1917896 -4.205461 -4.2170177 -4.2294693 -4.2284718 -4.2160039 -4.20059 -4.1872287 -4.1719027 -4.1545215][-4.1470313 -4.1570621 -4.1711831 -4.1702251 -4.1583962 -4.1552086 -4.1601558 -4.1712337 -4.1896081 -4.201025 -4.2074127 -4.21482 -4.225316 -4.2247543 -4.2095709][-4.152545 -4.1576858 -4.1598334 -4.1422086 -4.1126695 -4.0840454 -4.0694766 -4.0818677 -4.1129179 -4.1499867 -4.1864963 -4.2203307 -4.2493072 -4.2555861 -4.24151][-4.1619172 -4.15838 -4.1476221 -4.1123543 -4.0546222 -3.9852753 -3.9369783 -3.9565136 -4.0199623 -4.094841 -4.1673346 -4.22606 -4.2659268 -4.2765808 -4.2662864][-4.174159 -4.1567 -4.1310248 -4.07704 -3.9818678 -3.8469434 -3.7359891 -3.780777 -3.9166553 -4.0474005 -4.1507721 -4.2224479 -4.2677145 -4.283524 -4.2785945][-4.1783814 -4.1494241 -4.1198492 -4.0643916 -3.9531286 -3.7792139 -3.6220224 -3.6805518 -3.8674941 -4.0292649 -4.1411214 -4.2094922 -4.2490077 -4.2668042 -4.2692404][-4.1641021 -4.1307487 -4.1075058 -4.0746469 -3.9928164 -3.8649673 -3.7560315 -3.7932267 -3.9379945 -4.0771055 -4.1684737 -4.211062 -4.2315712 -4.246623 -4.2575622][-4.1465311 -4.1168633 -4.1073766 -4.0961976 -4.0539923 -3.9925656 -3.9445586 -3.9674273 -4.0534863 -4.1520166 -4.2158527 -4.2336712 -4.2338696 -4.2420969 -4.255106][-4.1398554 -4.1209087 -4.1255865 -4.1308637 -4.1129789 -4.0830641 -4.0658736 -4.0814104 -4.1302705 -4.1961389 -4.2392912 -4.2458005 -4.2369304 -4.2411861 -4.2529931][-4.1401787 -4.1369419 -4.160172 -4.179327 -4.1752605 -4.1571808 -4.1412048 -4.1391139 -4.1598673 -4.2006931 -4.231389 -4.2372212 -4.2307048 -4.2378578 -4.2477527][-4.151762 -4.1623111 -4.1945462 -4.2220469 -4.2271914 -4.2155652 -4.1929145 -4.1712441 -4.1722646 -4.195014 -4.2128959 -4.2180133 -4.2193394 -4.23119 -4.2351441][-4.1765065 -4.1845431 -4.211678 -4.2406263 -4.2498012 -4.2424469 -4.2226138 -4.1955357 -4.1863675 -4.1989212 -4.20977 -4.2127218 -4.2185578 -4.2328649 -4.2345443][-4.2133222 -4.2098775 -4.2221828 -4.2429514 -4.2504025 -4.2507162 -4.2455788 -4.2278748 -4.2165709 -4.2238207 -4.2314243 -4.2352481 -4.2427921 -4.25512 -4.2576656]]...]
INFO - root - 2017-12-07 19:52:56.007158: step 17710, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 1.894 sec/batch; 34h:24m:54s remains)
INFO - root - 2017-12-07 19:53:17.186368: step 17720, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 38h:30m:19s remains)
INFO - root - 2017-12-07 19:53:38.379686: step 17730, loss = 2.07, batch loss = 2.02 (14.5 examples/sec; 2.201 sec/batch; 39h:58m:40s remains)
INFO - root - 2017-12-07 19:53:59.524561: step 17740, loss = 2.06, batch loss = 2.00 (15.8 examples/sec; 2.025 sec/batch; 36h:46m:33s remains)
INFO - root - 2017-12-07 19:54:20.626256: step 17750, loss = 2.08, batch loss = 2.03 (14.9 examples/sec; 2.142 sec/batch; 38h:53m:24s remains)
INFO - root - 2017-12-07 19:54:41.853184: step 17760, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 38h:23m:59s remains)
INFO - root - 2017-12-07 19:55:02.855759: step 17770, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.088 sec/batch; 37h:54m:29s remains)
INFO - root - 2017-12-07 19:55:23.891022: step 17780, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.135 sec/batch; 38h:45m:10s remains)
INFO - root - 2017-12-07 19:55:44.929676: step 17790, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 38h:30m:26s remains)
INFO - root - 2017-12-07 19:56:06.178668: step 17800, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.101 sec/batch; 38h:07m:57s remains)
2017-12-07 19:56:07.701321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2200518 -4.2163391 -4.204092 -4.1940346 -4.1917782 -4.1997094 -4.2105803 -4.2215419 -4.2268982 -4.2304149 -4.2323279 -4.2301645 -4.2222633 -4.2118592 -4.2140956][-4.2325797 -4.2224021 -4.2084155 -4.1987629 -4.19535 -4.2012043 -4.214314 -4.2277689 -4.2384415 -4.2537603 -4.2670727 -4.2672696 -4.2538838 -4.2377472 -4.2362065][-4.23554 -4.2197447 -4.206912 -4.2015886 -4.1998129 -4.2019424 -4.2094364 -4.2182608 -4.231636 -4.2609472 -4.2866621 -4.29157 -4.2778339 -4.2587948 -4.253768][-4.2331276 -4.2162271 -4.2071853 -4.2069359 -4.2049618 -4.1973729 -4.1893182 -4.1871114 -4.1992879 -4.2401032 -4.2779026 -4.2872791 -4.2739124 -4.2536216 -4.2470732][-4.2186122 -4.2033811 -4.1975327 -4.2034121 -4.2010465 -4.1778488 -4.1476359 -4.1327949 -4.1518621 -4.2082863 -4.2560196 -4.2700548 -4.2572932 -4.2366204 -4.2308469][-4.1984148 -4.1813169 -4.1775255 -4.1879549 -4.1855774 -4.1450152 -4.0867147 -4.0595374 -4.0988469 -4.1773958 -4.2345252 -4.2539787 -4.2439179 -4.2221704 -4.213347][-4.1739392 -4.1546073 -4.1523676 -4.1642962 -4.1608319 -4.1011729 -4.0136137 -3.9699383 -4.0285978 -4.1299858 -4.1997504 -4.2301826 -4.2295227 -4.2110009 -4.1995649][-4.1706352 -4.1422024 -4.1305037 -4.1359835 -4.1282563 -4.0552807 -3.9476705 -3.88856 -3.9594603 -4.0785985 -4.1601806 -4.2040715 -4.21632 -4.2048163 -4.1931295][-4.1844063 -4.1483893 -4.1250887 -4.1189809 -4.1037912 -4.0273991 -3.915885 -3.8490202 -3.9220729 -4.0521331 -4.1433282 -4.197897 -4.2202115 -4.2108645 -4.1957421][-4.210022 -4.1755362 -4.1513605 -4.1422291 -4.128603 -4.0680933 -3.9800613 -3.9237485 -3.9766908 -4.08671 -4.1688123 -4.2201147 -4.2418628 -4.228126 -4.2069082][-4.2312608 -4.2044477 -4.1869788 -4.1808128 -4.171864 -4.13005 -4.0685153 -4.0254111 -4.0550871 -4.136353 -4.205955 -4.2508163 -4.2685905 -4.2509184 -4.2258615][-4.2386165 -4.2199669 -4.2070942 -4.2043595 -4.2019968 -4.1755743 -4.1355882 -4.1047125 -4.1181188 -4.1759973 -4.2351456 -4.2754288 -4.2892737 -4.2705245 -4.2470126][-4.2387691 -4.2285638 -4.2201872 -4.2215991 -4.2244935 -4.2112384 -4.1899819 -4.1723356 -4.1752553 -4.2129035 -4.2589221 -4.29195 -4.29936 -4.2823811 -4.2645988][-4.2506089 -4.2482753 -4.2431941 -4.246397 -4.2533689 -4.2508035 -4.2420211 -4.2326365 -4.2299767 -4.2491913 -4.2780457 -4.2994423 -4.3026967 -4.2917714 -4.2829018][-4.2746339 -4.2756171 -4.2715545 -4.2732663 -4.280117 -4.2831988 -4.281724 -4.2772136 -4.2738042 -4.279808 -4.2938023 -4.3049507 -4.3061833 -4.30167 -4.3006864]]...]
INFO - root - 2017-12-07 19:56:28.517130: step 17810, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.108 sec/batch; 38h:14m:24s remains)
INFO - root - 2017-12-07 19:56:49.743322: step 17820, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.140 sec/batch; 38h:48m:50s remains)
INFO - root - 2017-12-07 19:57:11.007111: step 17830, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 39h:10m:37s remains)
INFO - root - 2017-12-07 19:57:31.940871: step 17840, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.092 sec/batch; 37h:56m:48s remains)
INFO - root - 2017-12-07 19:57:53.138373: step 17850, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.083 sec/batch; 37h:45m:43s remains)
INFO - root - 2017-12-07 19:58:14.449998: step 17860, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.078 sec/batch; 37h:40m:32s remains)
INFO - root - 2017-12-07 19:58:35.542244: step 17870, loss = 2.09, batch loss = 2.03 (14.6 examples/sec; 2.197 sec/batch; 39h:49m:02s remains)
INFO - root - 2017-12-07 19:58:56.602905: step 17880, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 38h:07m:05s remains)
INFO - root - 2017-12-07 19:59:17.980033: step 17890, loss = 2.09, batch loss = 2.04 (15.0 examples/sec; 2.133 sec/batch; 38h:39m:09s remains)
INFO - root - 2017-12-07 19:59:39.284125: step 17900, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 38h:25m:31s remains)
2017-12-07 19:59:40.726692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.184669 -4.1816287 -4.1751146 -4.1716771 -4.1761236 -4.1927938 -4.2122889 -4.2203422 -4.2150607 -4.2048888 -4.2042971 -4.217948 -4.2301111 -4.2202506 -4.1990094][-4.1829634 -4.18319 -4.17755 -4.1744661 -4.1776881 -4.1900339 -4.2036228 -4.2032409 -4.1941614 -4.1854992 -4.1895089 -4.2093625 -4.2261271 -4.2160769 -4.1924644][-4.1888771 -4.1962819 -4.1938863 -4.1905036 -4.1899605 -4.195303 -4.2018995 -4.1930966 -4.1821742 -4.1773915 -4.1880436 -4.2141705 -4.2324133 -4.2206812 -4.1934195][-4.1959014 -4.2117438 -4.2135339 -4.208148 -4.2008471 -4.19647 -4.1960335 -4.1828609 -4.177362 -4.1816435 -4.1993322 -4.2278204 -4.2434335 -4.2279887 -4.1970448][-4.1934404 -4.2159972 -4.2210407 -4.2134271 -4.1993165 -4.1876831 -4.1820297 -4.1695695 -4.1755028 -4.1918206 -4.2151289 -4.2427158 -4.2516565 -4.2293277 -4.1927872][-4.183063 -4.2080822 -4.2152934 -4.2051067 -4.1857028 -4.1695428 -4.1594706 -4.1486764 -4.1663547 -4.1944814 -4.2227664 -4.2482119 -4.25068 -4.2218566 -4.1798706][-4.167541 -4.1936154 -4.203227 -4.1898293 -4.165966 -4.1480541 -4.1348677 -4.1247911 -4.1507545 -4.187438 -4.2191014 -4.2431903 -4.2427316 -4.2087126 -4.1626945][-4.1581869 -4.185256 -4.1976666 -4.1818147 -4.1559777 -4.1395497 -4.1257863 -4.11541 -4.1405072 -4.1772156 -4.208693 -4.2311349 -4.2298441 -4.1938696 -4.1476097][-4.1674175 -4.1930346 -4.2049003 -4.1865048 -4.161726 -4.1483817 -4.1375027 -4.1281929 -4.1477957 -4.1776 -4.2046924 -4.2238126 -4.2238016 -4.1919656 -4.1522546][-4.1953983 -4.2163138 -4.2209511 -4.1971741 -4.1716475 -4.1589851 -4.152298 -4.1504464 -4.167716 -4.1885729 -4.2096524 -4.2262697 -4.2276783 -4.2057247 -4.1777391][-4.2290678 -4.2442365 -4.23865 -4.2063775 -4.1749296 -4.1586714 -4.155911 -4.1635013 -4.1814528 -4.1969385 -4.2148023 -4.231256 -4.2345681 -4.2231011 -4.2078919][-4.2519794 -4.2628412 -4.2502275 -4.2127585 -4.1769214 -4.154623 -4.1525674 -4.1669931 -4.1860614 -4.2001405 -4.21855 -4.2349987 -4.2373972 -4.2327371 -4.2280068][-4.2577591 -4.2662339 -4.2528715 -4.2180085 -4.183156 -4.1590242 -4.1585331 -4.1740971 -4.1895952 -4.2006516 -4.21574 -4.2293267 -4.2305884 -4.2302322 -4.2325854][-4.2475762 -4.25478 -4.2455921 -4.2186165 -4.1900244 -4.1708951 -4.1753907 -4.1884813 -4.1939421 -4.1936059 -4.1985025 -4.2071481 -4.2099094 -4.2136903 -4.2200136][-4.2265835 -4.23219 -4.227273 -4.2091856 -4.1912031 -4.1828947 -4.1913157 -4.1989455 -4.1936269 -4.1799278 -4.1712046 -4.1731768 -4.177917 -4.1857886 -4.1940579]]...]
INFO - root - 2017-12-07 20:00:01.788099: step 17910, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.069 sec/batch; 37h:29m:03s remains)
INFO - root - 2017-12-07 20:00:23.027196: step 17920, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.165 sec/batch; 39h:12m:31s remains)
INFO - root - 2017-12-07 20:00:44.290782: step 17930, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.090 sec/batch; 37h:50m:25s remains)
INFO - root - 2017-12-07 20:01:05.334112: step 17940, loss = 2.06, batch loss = 2.00 (14.6 examples/sec; 2.196 sec/batch; 39h:45m:35s remains)
INFO - root - 2017-12-07 20:01:26.531381: step 17950, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.093 sec/batch; 37h:53m:28s remains)
INFO - root - 2017-12-07 20:01:47.717669: step 17960, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.120 sec/batch; 38h:22m:14s remains)
INFO - root - 2017-12-07 20:02:08.432494: step 17970, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 38h:02m:09s remains)
INFO - root - 2017-12-07 20:02:29.741867: step 17980, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.144 sec/batch; 38h:48m:13s remains)
INFO - root - 2017-12-07 20:02:50.941947: step 17990, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.136 sec/batch; 38h:39m:14s remains)
INFO - root - 2017-12-07 20:03:12.089871: step 18000, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.149 sec/batch; 38h:52m:35s remains)
2017-12-07 20:03:13.569032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2021394 -4.187181 -4.16783 -4.1520915 -4.1305513 -4.1130066 -4.1143818 -4.1351 -4.1469836 -4.149354 -4.1611409 -4.1765532 -4.1929817 -4.2047963 -4.2124591][-4.197464 -4.1873055 -4.1772242 -4.1698256 -4.1524773 -4.1350675 -4.1362915 -4.1540852 -4.1670237 -4.1711721 -4.1822271 -4.1924648 -4.2040577 -4.2119012 -4.2154922][-4.2098732 -4.2019525 -4.1941791 -4.1862807 -4.16662 -4.1501646 -4.1543884 -4.1714911 -4.1891642 -4.1974697 -4.2082496 -4.217042 -4.2266254 -4.2321315 -4.2310519][-4.2242107 -4.2141094 -4.2021222 -4.1875119 -4.1613111 -4.1396875 -4.143321 -4.1597786 -4.1794491 -4.1930213 -4.2077527 -4.2198391 -4.2320862 -4.2369838 -4.231647][-4.2272449 -4.205934 -4.180315 -4.1548047 -4.1225977 -4.0950427 -4.0936737 -4.1091213 -4.1351223 -4.1602039 -4.1857419 -4.2047958 -4.2196565 -4.224143 -4.2187805][-4.2153983 -4.1756406 -4.1327839 -4.0928793 -4.052248 -4.013186 -3.9999261 -4.0111747 -4.0463128 -4.0875616 -4.1260028 -4.1544104 -4.1724982 -4.1774321 -4.1728039][-4.2070608 -4.152061 -4.0932441 -4.0417447 -3.9933357 -3.9439442 -3.9159169 -3.9175079 -3.9552252 -4.0047903 -4.0511374 -4.0868139 -4.1063437 -4.1123371 -4.1122618][-4.2014742 -4.13934 -4.0754547 -4.0218596 -3.9738007 -3.9244552 -3.8901906 -3.882086 -3.9128683 -3.9561017 -4.00038 -4.0362158 -4.0512104 -4.05668 -4.0643811][-4.20163 -4.1414413 -4.0842109 -4.0382423 -3.9997494 -3.9593737 -3.9272668 -3.9115448 -3.9279075 -3.959516 -3.9950652 -4.0240755 -4.03436 -4.0392356 -4.0575457][-4.2199574 -4.1716661 -4.1291237 -4.0966372 -4.0677738 -4.0364242 -4.0100422 -3.9924767 -3.9983671 -4.0216007 -4.0481238 -4.0694723 -4.0757785 -4.0799971 -4.1014252][-4.2548203 -4.222846 -4.1969557 -4.1777329 -4.1596255 -4.1406221 -4.1243582 -4.1106739 -4.1116261 -4.1287189 -4.1469579 -4.1589437 -4.1592894 -4.1600022 -4.1733284][-4.2920732 -4.2740049 -4.2617974 -4.2544613 -4.2469249 -4.24068 -4.2351818 -4.2263288 -4.2234898 -4.2336612 -4.2437444 -4.2459397 -4.2411165 -4.2384381 -4.244112][-4.31945 -4.3105803 -4.3063526 -4.305192 -4.303668 -4.3032641 -4.3018723 -4.2961817 -4.292738 -4.2979689 -4.3007154 -4.2973404 -4.2935333 -4.2918839 -4.2933369][-4.3369918 -4.3321209 -4.3290119 -4.3275981 -4.3255196 -4.3244772 -4.3236742 -4.3206077 -4.3196015 -4.3231754 -4.3237338 -4.3213415 -4.3206196 -4.3210011 -4.3210797][-4.3479829 -4.3440022 -4.3391256 -4.3353038 -4.3313665 -4.3290887 -4.3285851 -4.3276548 -4.3281336 -4.3311625 -4.3330464 -4.3337817 -4.3350248 -4.3361168 -4.335649]]...]
INFO - root - 2017-12-07 20:03:34.834174: step 18010, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.171 sec/batch; 39h:16m:27s remains)
INFO - root - 2017-12-07 20:03:56.238307: step 18020, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.137 sec/batch; 38h:38m:53s remains)
INFO - root - 2017-12-07 20:04:17.259997: step 18030, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.111 sec/batch; 38h:09m:47s remains)
INFO - root - 2017-12-07 20:04:38.299176: step 18040, loss = 2.06, batch loss = 2.01 (15.3 examples/sec; 2.092 sec/batch; 37h:49m:01s remains)
INFO - root - 2017-12-07 20:04:59.453550: step 18050, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.108 sec/batch; 38h:06m:25s remains)
INFO - root - 2017-12-07 20:05:20.429124: step 18060, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.077 sec/batch; 37h:32m:22s remains)
INFO - root - 2017-12-07 20:05:41.218416: step 18070, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.111 sec/batch; 38h:09m:12s remains)
INFO - root - 2017-12-07 20:06:02.447077: step 18080, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 38h:32m:50s remains)
INFO - root - 2017-12-07 20:06:23.672451: step 18090, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 38h:22m:39s remains)
INFO - root - 2017-12-07 20:06:44.518189: step 18100, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.098 sec/batch; 37h:53m:18s remains)
2017-12-07 20:06:46.036179: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2746348 -4.2750063 -4.271317 -4.2622991 -4.2524166 -4.2493258 -4.2519979 -4.2591648 -4.258688 -4.254149 -4.2491345 -4.2518382 -4.2665148 -4.2862544 -4.3030939][-4.2823529 -4.2824016 -4.2750363 -4.2629595 -4.2541814 -4.2545648 -4.2624168 -4.2721248 -4.2704525 -4.2625995 -4.2523541 -4.2449846 -4.2489977 -4.2640882 -4.2824788][-4.279242 -4.2784538 -4.2717371 -4.2587528 -4.2526097 -4.2559795 -4.2658734 -4.2774706 -4.2765064 -4.2697372 -4.2616324 -4.249486 -4.2411876 -4.2443061 -4.2591844][-4.2757721 -4.2750249 -4.2679553 -4.2516279 -4.2410655 -4.2401409 -4.247437 -4.2583013 -4.2593117 -4.2579517 -4.25858 -4.24804 -4.2334838 -4.2292075 -4.2413082][-4.2800918 -4.2779822 -4.2662444 -4.2395873 -4.2133427 -4.2004247 -4.1985264 -4.2055759 -4.2102823 -4.2172513 -4.229454 -4.2307734 -4.219121 -4.2137909 -4.2268][-4.2819066 -4.2739429 -4.2507486 -4.2087884 -4.162509 -4.1304145 -4.1137705 -4.1188564 -4.1348267 -4.1587782 -4.1898589 -4.2126303 -4.2133603 -4.2094016 -4.2179427][-4.2754192 -4.2573452 -4.2211819 -4.16644 -4.1029572 -4.0480042 -4.0126572 -4.02194 -4.0558681 -4.1018128 -4.15419 -4.2002997 -4.2176356 -4.2152863 -4.2160807][-4.2755151 -4.2522097 -4.209784 -4.1512709 -4.0837846 -4.0217638 -3.9806905 -4.0004816 -4.0469465 -4.0995378 -4.1548738 -4.2067409 -4.2304425 -4.2314558 -4.225698][-4.2909417 -4.2697153 -4.2334719 -4.184638 -4.1330118 -4.0899496 -4.0683837 -4.0956531 -4.1348844 -4.1713114 -4.2037063 -4.2355552 -4.2504992 -4.2492046 -4.2372713][-4.3085647 -4.2931185 -4.266439 -4.2327 -4.2000885 -4.1755047 -4.1704683 -4.19769 -4.2236514 -4.2378674 -4.2435994 -4.24875 -4.2503986 -4.2475262 -4.2357655][-4.320869 -4.3120117 -4.2963023 -4.2785025 -4.2602167 -4.2474856 -4.2471795 -4.2628207 -4.2678251 -4.2571788 -4.2390485 -4.2248197 -4.2205777 -4.2233291 -4.2236409][-4.3276267 -4.3239923 -4.31766 -4.309309 -4.299274 -4.292172 -4.2894559 -4.2916226 -4.279417 -4.2491207 -4.2143579 -4.1857085 -4.1765471 -4.1863241 -4.2008224][-4.3354506 -4.333777 -4.3317623 -4.3271966 -4.3216181 -4.3174496 -4.3136516 -4.3081245 -4.2871881 -4.249938 -4.2103305 -4.1733975 -4.1582646 -4.1674066 -4.1885138][-4.3396182 -4.3391147 -4.339673 -4.3379593 -4.3339357 -4.3302732 -4.3257341 -4.3182893 -4.2995133 -4.2675605 -4.234591 -4.2051454 -4.1908097 -4.194706 -4.2132688][-4.3415551 -4.340713 -4.3424296 -4.3430381 -4.3396487 -4.3346124 -4.3300481 -4.3235211 -4.3079753 -4.2791739 -4.2495694 -4.2261095 -4.214344 -4.2186375 -4.2342978]]...]
INFO - root - 2017-12-07 20:07:07.506987: step 18110, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.175 sec/batch; 39h:17m:14s remains)
INFO - root - 2017-12-07 20:07:28.923917: step 18120, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.105 sec/batch; 38h:01m:07s remains)
INFO - root - 2017-12-07 20:07:49.982185: step 18130, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.152 sec/batch; 38h:50m:54s remains)
INFO - root - 2017-12-07 20:08:11.193891: step 18140, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.114 sec/batch; 38h:09m:40s remains)
INFO - root - 2017-12-07 20:08:32.427191: step 18150, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.134 sec/batch; 38h:31m:28s remains)
INFO - root - 2017-12-07 20:08:53.485665: step 18160, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.105 sec/batch; 37h:59m:42s remains)
INFO - root - 2017-12-07 20:09:14.671887: step 18170, loss = 2.06, batch loss = 2.01 (15.1 examples/sec; 2.115 sec/batch; 38h:09m:56s remains)
INFO - root - 2017-12-07 20:09:36.183415: step 18180, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.168 sec/batch; 39h:07m:01s remains)
INFO - root - 2017-12-07 20:09:57.293900: step 18190, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 37h:46m:45s remains)
INFO - root - 2017-12-07 20:10:18.116908: step 18200, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.117 sec/batch; 38h:10m:33s remains)
2017-12-07 20:10:19.677147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2533903 -4.25678 -4.2608619 -4.23915 -4.2076797 -4.1969132 -4.2116566 -4.2435408 -4.2739687 -4.2930493 -4.3024693 -4.3061776 -4.2994719 -4.2923083 -4.2890964][-4.2678771 -4.2715964 -4.2741961 -4.25362 -4.2193837 -4.1979365 -4.1989021 -4.2208896 -4.2479353 -4.2674627 -4.2770891 -4.2845769 -4.2808204 -4.2754459 -4.2723951][-4.2883506 -4.29533 -4.2971864 -4.2769165 -4.2383685 -4.2069831 -4.1952548 -4.2038403 -4.2227054 -4.2422457 -4.2547565 -4.2659922 -4.2635937 -4.2568035 -4.2517715][-4.302494 -4.3125072 -4.3128204 -4.2904673 -4.249001 -4.214438 -4.1964478 -4.1963472 -4.20992 -4.230341 -4.2455816 -4.2574968 -4.25366 -4.2432637 -4.2345424][-4.3064809 -4.3143706 -4.3106847 -4.2846718 -4.2428541 -4.2080646 -4.1892071 -4.1886096 -4.2052016 -4.22811 -4.2413621 -4.2514148 -4.250648 -4.2419338 -4.2369556][-4.298584 -4.298666 -4.2878194 -4.2572017 -4.2156534 -4.1799874 -4.157712 -4.1585417 -4.1776781 -4.2058506 -4.2209468 -4.2316146 -4.23706 -4.2351584 -4.2358317][-4.2802696 -4.2711992 -4.256536 -4.2227225 -4.1743627 -4.12653 -4.0962548 -4.1006947 -4.1248503 -4.1623349 -4.1855855 -4.2022309 -4.2152357 -4.2219372 -4.2266579][-4.2586637 -4.2443824 -4.2285972 -4.1929488 -4.1310353 -4.0597548 -4.00958 -4.0176172 -4.0565529 -4.1125951 -4.1535406 -4.1811366 -4.2006507 -4.2138548 -4.2213044][-4.2473946 -4.2330842 -4.2165146 -4.1839876 -4.1243873 -4.0417848 -3.9690373 -3.9695122 -4.0232344 -4.1017413 -4.1616287 -4.1981897 -4.2217822 -4.2357883 -4.2430139][-4.2486048 -4.240345 -4.2265368 -4.1980991 -4.1510954 -4.0817971 -4.0074797 -3.9961588 -4.0451059 -4.1264725 -4.192101 -4.2302666 -4.2524323 -4.2635436 -4.2675443][-4.2585673 -4.2581673 -4.250093 -4.227015 -4.1968803 -4.1556759 -4.1056242 -4.0938196 -4.12566 -4.1885066 -4.240777 -4.2696624 -4.283237 -4.2894568 -4.291893][-4.263721 -4.2667294 -4.2650914 -4.2483535 -4.2320843 -4.216176 -4.1950321 -4.1910706 -4.2068553 -4.2443414 -4.277915 -4.2953005 -4.3003964 -4.3025312 -4.3050313][-4.2718267 -4.2738323 -4.2764382 -4.2662139 -4.2571173 -4.2534204 -4.2513738 -4.2574348 -4.2670646 -4.2847052 -4.30239 -4.3112273 -4.310739 -4.3103552 -4.312912][-4.2947297 -4.2979503 -4.3028746 -4.2959819 -4.2890015 -4.2878218 -4.291522 -4.2998333 -4.3090172 -4.31916 -4.3278737 -4.3302283 -4.3259149 -4.3227477 -4.3240995][-4.3225021 -4.3289008 -4.3345532 -4.3298063 -4.3236933 -4.3222466 -4.3260317 -4.3309364 -4.3358097 -4.3417954 -4.3463564 -4.3457675 -4.3420339 -4.3389144 -4.3380632]]...]
INFO - root - 2017-12-07 20:10:41.027614: step 18210, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 38h:15m:41s remains)
INFO - root - 2017-12-07 20:11:02.511839: step 18220, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.142 sec/batch; 38h:36m:49s remains)
INFO - root - 2017-12-07 20:11:23.587933: step 18230, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.123 sec/batch; 38h:15m:59s remains)
INFO - root - 2017-12-07 20:11:44.743090: step 18240, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 38h:17m:42s remains)
INFO - root - 2017-12-07 20:12:06.263120: step 18250, loss = 2.07, batch loss = 2.01 (14.2 examples/sec; 2.246 sec/batch; 40h:28m:12s remains)
INFO - root - 2017-12-07 20:12:27.034408: step 18260, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.062 sec/batch; 37h:09m:05s remains)
INFO - root - 2017-12-07 20:12:48.093556: step 18270, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 38h:04m:28s remains)
INFO - root - 2017-12-07 20:13:09.318885: step 18280, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.094 sec/batch; 37h:43m:08s remains)
INFO - root - 2017-12-07 20:13:30.392671: step 18290, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 37h:51m:30s remains)
INFO - root - 2017-12-07 20:13:51.537570: step 18300, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.190 sec/batch; 39h:26m:16s remains)
2017-12-07 20:13:53.201577: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3473535 -4.3413606 -4.3360124 -4.3327413 -4.33011 -4.3291607 -4.3322277 -4.3366189 -4.3401842 -4.3433809 -4.3475747 -4.3520255 -4.355792 -4.3583856 -4.3575168][-4.3395624 -4.32796 -4.315414 -4.3051438 -4.2958393 -4.29049 -4.2916617 -4.299315 -4.3080163 -4.31567 -4.3246207 -4.333931 -4.3417253 -4.3476038 -4.3481045][-4.3293967 -4.3080711 -4.2837734 -4.2624354 -4.2432961 -4.2293286 -4.2249913 -4.2349563 -4.2507362 -4.2651277 -4.2804585 -4.296865 -4.3121915 -4.3235526 -4.328167][-4.3083253 -4.2733145 -4.2352729 -4.2016649 -4.1717687 -4.1481733 -4.1377926 -4.1518865 -4.1773672 -4.2008009 -4.2204447 -4.2430372 -4.2703133 -4.2922044 -4.3053][-4.267693 -4.2170792 -4.1641111 -4.1172709 -4.0731282 -4.0354013 -4.0174551 -4.0432158 -4.0874033 -4.1258245 -4.1525893 -4.1821489 -4.222261 -4.2588153 -4.2840891][-4.204608 -4.1410284 -4.0798097 -4.0272908 -3.9674489 -3.9043446 -3.8636646 -3.906297 -3.9878836 -4.0516734 -4.0928879 -4.13256 -4.1820474 -4.2308054 -4.2680154][-4.1324844 -4.0619035 -4.0056705 -3.962904 -3.8931251 -3.7942631 -3.7102427 -3.7702842 -3.8965671 -3.9916375 -4.0525646 -4.1068516 -4.1652465 -4.2208447 -4.2634892][-4.0793962 -4.0124111 -3.9742203 -3.9509664 -3.890466 -3.7802372 -3.6746655 -3.7363739 -3.8753579 -3.982338 -4.0550385 -4.1164751 -4.1767006 -4.2324624 -4.2742643][-4.063787 -4.0103612 -3.996068 -3.9970884 -3.9649363 -3.8873661 -3.8056149 -3.8406522 -3.9416497 -4.0293074 -4.09912 -4.1577358 -4.2113252 -4.260181 -4.2962618][-4.0856962 -4.04261 -4.0438604 -4.0625 -4.058042 -4.0215983 -3.9729795 -3.9870205 -4.0445209 -4.1027675 -4.1606894 -4.2122211 -4.254766 -4.29289 -4.3211131][-4.1371789 -4.1001296 -4.105423 -4.1320906 -4.147037 -4.1413126 -4.1198597 -4.1259723 -4.1538725 -4.1861629 -4.2269025 -4.2685709 -4.3003869 -4.3262906 -4.3444424][-4.2040567 -4.1742945 -4.1773844 -4.2007284 -4.2205234 -4.2291164 -4.2258806 -4.2316046 -4.2429748 -4.2584181 -4.2835765 -4.3131285 -4.3350525 -4.3501463 -4.3592129][-4.262043 -4.2428994 -4.2444692 -4.259366 -4.2752047 -4.2855029 -4.2892227 -4.2946315 -4.2997584 -4.3070168 -4.3216476 -4.3410897 -4.35519 -4.363452 -4.3672357][-4.3076572 -4.2983561 -4.2996244 -4.3081355 -4.3171415 -4.323689 -4.3269157 -4.3292747 -4.3316445 -4.3357339 -4.344317 -4.3550954 -4.362762 -4.3677092 -4.3689775][-4.3410788 -4.3373756 -4.3378978 -4.341444 -4.3455667 -4.3488951 -4.3496561 -4.3493333 -4.3497667 -4.351357 -4.35481 -4.3594127 -4.3631639 -4.3661437 -4.3666797]]...]
INFO - root - 2017-12-07 20:14:14.429674: step 18310, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.065 sec/batch; 37h:10m:51s remains)
INFO - root - 2017-12-07 20:14:35.598700: step 18320, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.147 sec/batch; 38h:39m:11s remains)
INFO - root - 2017-12-07 20:14:56.636457: step 18330, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.136 sec/batch; 38h:26m:46s remains)
INFO - root - 2017-12-07 20:15:17.832799: step 18340, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.116 sec/batch; 38h:04m:28s remains)
INFO - root - 2017-12-07 20:15:39.155065: step 18350, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.111 sec/batch; 37h:58m:35s remains)
INFO - root - 2017-12-07 20:15:59.701424: step 18360, loss = 2.08, batch loss = 2.03 (15.3 examples/sec; 2.089 sec/batch; 37h:35m:12s remains)
INFO - root - 2017-12-07 20:16:20.738635: step 18370, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 38h:09m:04s remains)
INFO - root - 2017-12-07 20:16:41.981261: step 18380, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 38h:04m:25s remains)
INFO - root - 2017-12-07 20:17:02.977786: step 18390, loss = 2.05, batch loss = 1.99 (14.8 examples/sec; 2.162 sec/batch; 38h:53m:06s remains)
INFO - root - 2017-12-07 20:17:24.374287: step 18400, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 38h:05m:10s remains)
2017-12-07 20:17:25.934413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.01946 -4.0270362 -4.0676494 -4.1161814 -4.1563125 -4.1911979 -4.2208843 -4.235075 -4.2464504 -4.2581234 -4.2669954 -4.2837152 -4.3034005 -4.3135748 -4.3187432][-3.9837728 -3.9907813 -4.028955 -4.0754371 -4.1241546 -4.1745915 -4.2134409 -4.2255683 -4.2310486 -4.2465849 -4.2600551 -4.2806363 -4.3024139 -4.3139925 -4.3173633][-4.0227027 -4.0200148 -4.0287166 -4.0460596 -4.0961919 -4.1668906 -4.2117572 -4.2194614 -4.2226849 -4.242578 -4.2613182 -4.280035 -4.2982831 -4.30577 -4.3061094][-4.0960269 -4.0832896 -4.0619397 -4.0466838 -4.0908947 -4.1667724 -4.1993971 -4.1910534 -4.1929693 -4.21991 -4.2475109 -4.2663364 -4.27926 -4.2842779 -4.2876148][-4.1617212 -4.146029 -4.1240659 -4.1074529 -4.1306629 -4.16943 -4.1641431 -4.1297388 -4.12934 -4.1699724 -4.2126193 -4.2386351 -4.2504973 -4.2579322 -4.2674775][-4.2005711 -4.1849208 -4.1800246 -4.1766419 -4.1781864 -4.1684532 -4.1166887 -4.0430608 -4.0271225 -4.0905266 -4.16141 -4.2038236 -4.2245398 -4.2395277 -4.2555141][-4.2111707 -4.1990428 -4.2113161 -4.2141066 -4.1972475 -4.1537056 -4.0642314 -3.9352987 -3.8823283 -3.9757302 -4.0895576 -4.1593843 -4.1931572 -4.21479 -4.2394056][-4.2100611 -4.2094512 -4.2281461 -4.2240653 -4.1915822 -4.1291862 -4.0182981 -3.8615563 -3.791759 -3.8987324 -4.0354071 -4.1151705 -4.150198 -4.1794128 -4.2134495][-4.2075257 -4.2194886 -4.23818 -4.2262821 -4.1845913 -4.1187038 -4.0236282 -3.9078038 -3.8634548 -3.9345045 -4.0266056 -4.0870228 -4.1182 -4.1537738 -4.1941872][-4.2110219 -4.2286892 -4.2425404 -4.2325134 -4.1983943 -4.1502881 -4.1001978 -4.0477791 -4.022471 -4.0353527 -4.0546136 -4.0792375 -4.1108565 -4.1564074 -4.1977139][-4.221283 -4.2404237 -4.2537575 -4.2542162 -4.2384019 -4.2162728 -4.2009521 -4.1823034 -4.1590533 -4.130414 -4.0919552 -4.0914965 -4.1304932 -4.1756029 -4.2000966][-4.2257543 -4.251493 -4.2678585 -4.2755237 -4.273531 -4.2692432 -4.2698674 -4.2627115 -4.2396364 -4.19843 -4.1463838 -4.134129 -4.1598539 -4.1773677 -4.1719308][-4.2286839 -4.2557979 -4.2711306 -4.278995 -4.2821331 -4.2860789 -4.2914762 -4.2858095 -4.2647195 -4.2306314 -4.1948957 -4.1791654 -4.1727648 -4.1461 -4.10288][-4.229672 -4.2515364 -4.2633872 -4.265903 -4.268703 -4.275001 -4.2814407 -4.2790861 -4.2619247 -4.2390537 -4.2202377 -4.2012753 -4.16696 -4.1023884 -4.0264211][-4.2284484 -4.2421465 -4.2484484 -4.2449412 -4.2464585 -4.2536879 -4.2628279 -4.2650776 -4.2543058 -4.2387328 -4.2219386 -4.1985388 -4.15814 -4.0925117 -4.0310965]]...]
INFO - root - 2017-12-07 20:17:46.985283: step 18410, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.081 sec/batch; 37h:24m:30s remains)
INFO - root - 2017-12-07 20:18:08.066766: step 18420, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.190 sec/batch; 39h:22m:12s remains)
INFO - root - 2017-12-07 20:18:29.110036: step 18430, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 37h:47m:20s remains)
INFO - root - 2017-12-07 20:18:50.361748: step 18440, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.095 sec/batch; 37h:39m:00s remains)
INFO - root - 2017-12-07 20:19:11.046465: step 18450, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.142 sec/batch; 38h:28m:48s remains)
INFO - root - 2017-12-07 20:19:32.243774: step 18460, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.151 sec/batch; 38h:37m:51s remains)
INFO - root - 2017-12-07 20:19:53.501729: step 18470, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.073 sec/batch; 37h:13m:19s remains)
INFO - root - 2017-12-07 20:20:14.808437: step 18480, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.110 sec/batch; 37h:53m:27s remains)
INFO - root - 2017-12-07 20:20:35.676107: step 18490, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.107 sec/batch; 37h:50m:10s remains)
INFO - root - 2017-12-07 20:20:56.833615: step 18500, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.081 sec/batch; 37h:21m:02s remains)
2017-12-07 20:20:58.341135: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2420797 -4.2592821 -4.2704191 -4.2633734 -4.2420011 -4.2205091 -4.2072697 -4.1932659 -4.1925707 -4.2152662 -4.2450953 -4.2626357 -4.2647467 -4.2634583 -4.2620125][-4.2523527 -4.2702746 -4.2845354 -4.2777395 -4.2525916 -4.2247238 -4.2083273 -4.1974792 -4.2032027 -4.2278662 -4.2582293 -4.2755694 -4.274982 -4.2701921 -4.2663879][-4.2531142 -4.2745762 -4.2936735 -4.28351 -4.2468028 -4.2082877 -4.1887841 -4.1894736 -4.2076616 -4.2353282 -4.2667155 -4.28604 -4.2838311 -4.2767496 -4.2730746][-4.2446265 -4.2734513 -4.2959294 -4.2802935 -4.2293367 -4.1816835 -4.1633415 -4.17844 -4.2084675 -4.2365212 -4.2661753 -4.28317 -4.275847 -4.2656589 -4.262145][-4.2333393 -4.2635312 -4.2837257 -4.2579474 -4.1917553 -4.13064 -4.1100564 -4.1424341 -4.1931076 -4.2289896 -4.2542515 -4.2642198 -4.2496562 -4.23221 -4.2237611][-4.2304144 -4.2534151 -4.2652965 -4.2258439 -4.1344252 -4.0391426 -3.998529 -4.0550237 -4.1502094 -4.2115636 -4.2358794 -4.2336946 -4.2111187 -4.1873469 -4.1741667][-4.2317233 -4.2470841 -4.2529106 -4.2035866 -4.0857644 -3.9371834 -3.8435948 -3.9224687 -4.0854564 -4.1925297 -4.224545 -4.2084727 -4.1732912 -4.1406355 -4.1244154][-4.223453 -4.2349625 -4.2452269 -4.204669 -4.0836887 -3.8971691 -3.7334712 -3.7997127 -4.0181808 -4.1720085 -4.2168579 -4.18928 -4.13649 -4.0926881 -4.0783906][-4.1939244 -4.21428 -4.2444377 -4.2337871 -4.1521797 -3.9999049 -3.8403273 -3.8557916 -4.0289669 -4.1699772 -4.2119646 -4.1811385 -4.1164827 -4.0639091 -4.055151][-4.1418324 -4.176424 -4.2290616 -4.2490149 -4.2138376 -4.1251583 -4.02049 -4.004993 -4.09437 -4.1848159 -4.2134509 -4.1838865 -4.1215854 -4.0730553 -4.0718622][-4.0916791 -4.136476 -4.2004933 -4.2334685 -4.2260184 -4.184185 -4.129765 -4.1131654 -4.1535587 -4.204452 -4.2203693 -4.1922927 -4.1400952 -4.1041164 -4.1077256][-4.0668044 -4.1073332 -4.1680264 -4.2011485 -4.20629 -4.1943369 -4.1753397 -4.1699162 -4.1933355 -4.2256866 -4.2318921 -4.20695 -4.1670537 -4.1415219 -4.1448178][-4.0831714 -4.1111879 -4.157073 -4.1806951 -4.1899118 -4.1947074 -4.1945834 -4.1975145 -4.2119837 -4.2299843 -4.2344456 -4.2165866 -4.1887269 -4.1706219 -4.1736145][-4.1183424 -4.1363339 -4.1694417 -4.1861687 -4.1975241 -4.2087359 -4.2111197 -4.2067289 -4.2035336 -4.2083836 -4.2137794 -4.2085881 -4.1963325 -4.1851282 -4.1874571][-4.1427479 -4.1541066 -4.1799645 -4.1955624 -4.2115593 -4.2246161 -4.2234869 -4.2040162 -4.1798754 -4.1717486 -4.1770473 -4.1834831 -4.1856532 -4.1838465 -4.1832294]]...]
INFO - root - 2017-12-07 20:21:19.488885: step 18510, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.139 sec/batch; 38h:23m:22s remains)
INFO - root - 2017-12-07 20:21:40.503197: step 18520, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.127 sec/batch; 38h:10m:27s remains)
INFO - root - 2017-12-07 20:22:01.694887: step 18530, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.083 sec/batch; 37h:22m:20s remains)
INFO - root - 2017-12-07 20:22:22.924073: step 18540, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 37h:39m:21s remains)
INFO - root - 2017-12-07 20:22:43.562767: step 18550, loss = 2.06, batch loss = 2.00 (15.4 examples/sec; 2.077 sec/batch; 37h:15m:45s remains)
INFO - root - 2017-12-07 20:23:04.633915: step 18560, loss = 2.06, batch loss = 2.00 (15.5 examples/sec; 2.069 sec/batch; 37h:05m:56s remains)
INFO - root - 2017-12-07 20:23:25.726916: step 18570, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.112 sec/batch; 37h:51m:50s remains)
INFO - root - 2017-12-07 20:23:46.639090: step 18580, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 2.049 sec/batch; 36h:43m:54s remains)
INFO - root - 2017-12-07 20:24:07.699627: step 18590, loss = 2.06, batch loss = 2.01 (15.2 examples/sec; 2.109 sec/batch; 37h:48m:44s remains)
INFO - root - 2017-12-07 20:24:28.913273: step 18600, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.154 sec/batch; 38h:36m:08s remains)
2017-12-07 20:24:30.499216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3225732 -4.3125505 -4.2994857 -4.2902255 -4.2854166 -4.272737 -4.2517133 -4.2480712 -4.2683587 -4.2944722 -4.307395 -4.3104849 -4.2996278 -4.2822471 -4.2736096][-4.3082075 -4.2968125 -4.282701 -4.2683115 -4.2538314 -4.2226167 -4.1828823 -4.1773367 -4.214478 -4.2599478 -4.2860603 -4.2990332 -4.2877555 -4.2635989 -4.2516][-4.2885427 -4.2773552 -4.2632937 -4.2411013 -4.2089434 -4.1521726 -4.0897627 -4.0845757 -4.147953 -4.2198744 -4.2622828 -4.2865534 -4.271678 -4.237339 -4.2200813][-4.2774758 -4.2660275 -4.2505302 -4.2214317 -4.1649427 -4.0731664 -3.9823012 -3.9870048 -4.0893941 -4.1909304 -4.24709 -4.2769809 -4.255703 -4.2106581 -4.1890268][-4.2760472 -4.2593083 -4.2407622 -4.2090321 -4.1321959 -3.9980063 -3.858422 -3.8742158 -4.0343752 -4.1702719 -4.2384753 -4.2742753 -4.2509608 -4.1953831 -4.1627746][-4.272809 -4.2493196 -4.2239242 -4.1969824 -4.1155028 -3.939676 -3.731004 -3.7569964 -3.9875307 -4.1598248 -4.2385349 -4.2821312 -4.2617278 -4.1982803 -4.1525092][-4.2595491 -4.2252269 -4.1915641 -4.1737165 -4.1065655 -3.9145594 -3.6496048 -3.6750495 -3.9616952 -4.1604342 -4.2456894 -4.2884068 -4.2669768 -4.1980085 -4.1489978][-4.2345428 -4.1842208 -4.1479893 -4.1488442 -4.1092858 -3.9463456 -3.705189 -3.721278 -3.9840631 -4.1671729 -4.2469029 -4.2804551 -4.2529655 -4.1775146 -4.1293793][-4.2325435 -4.1657743 -4.1277466 -4.1448016 -4.1403327 -4.034656 -3.8699436 -3.8762116 -4.0554562 -4.1933923 -4.2586169 -4.2802987 -4.2399087 -4.1536512 -4.1089687][-4.2571154 -4.1791153 -4.1347151 -4.1607208 -4.1849189 -4.1275368 -4.0293269 -4.0325007 -4.1387653 -4.2311459 -4.2768397 -4.2822104 -4.2302961 -4.1431804 -4.1124821][-4.294838 -4.2165279 -4.1605616 -4.1799679 -4.2170472 -4.1954789 -4.1428504 -4.1503305 -4.2105951 -4.264348 -4.2920523 -4.2813382 -4.2246394 -4.14803 -4.1387396][-4.3299379 -4.2597513 -4.1969028 -4.2043748 -4.24727 -4.2567749 -4.2376242 -4.2464976 -4.2778921 -4.3028111 -4.309474 -4.2847176 -4.2255287 -4.1653891 -4.1774974][-4.3464794 -4.2957406 -4.2433481 -4.2435746 -4.2836285 -4.3124495 -4.3126578 -4.316205 -4.3275576 -4.3308797 -4.3182573 -4.2878847 -4.2320161 -4.1871233 -4.2089972][-4.3378372 -4.3049049 -4.2708683 -4.2713814 -4.3089218 -4.3426685 -4.3472495 -4.3431878 -4.3387566 -4.328732 -4.3083649 -4.2824626 -4.2442617 -4.21581 -4.2361417][-4.3191638 -4.2945914 -4.2728229 -4.27695 -4.3115396 -4.3390684 -4.3407917 -4.3374796 -4.331655 -4.3202281 -4.3016825 -4.2840123 -4.2652874 -4.2542262 -4.2691469]]...]
INFO - root - 2017-12-07 20:24:51.807022: step 18610, loss = 2.07, batch loss = 2.01 (16.0 examples/sec; 1.996 sec/batch; 35h:46m:34s remains)
INFO - root - 2017-12-07 20:25:12.725202: step 18620, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.124 sec/batch; 38h:02m:58s remains)
INFO - root - 2017-12-07 20:25:33.887432: step 18630, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.118 sec/batch; 37h:56m:30s remains)
INFO - root - 2017-12-07 20:25:55.220770: step 18640, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.149 sec/batch; 38h:29m:34s remains)
INFO - root - 2017-12-07 20:26:16.009384: step 18650, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.097 sec/batch; 37h:33m:27s remains)
INFO - root - 2017-12-07 20:26:37.329904: step 18660, loss = 2.06, batch loss = 2.00 (14.7 examples/sec; 2.170 sec/batch; 38h:51m:20s remains)
INFO - root - 2017-12-07 20:26:58.696324: step 18670, loss = 2.08, batch loss = 2.03 (14.7 examples/sec; 2.171 sec/batch; 38h:51m:41s remains)
INFO - root - 2017-12-07 20:27:19.550323: step 18680, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.076 sec/batch; 37h:09m:15s remains)
INFO - root - 2017-12-07 20:27:40.591425: step 18690, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.148 sec/batch; 38h:26m:26s remains)
INFO - root - 2017-12-07 20:28:01.720292: step 18700, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.130 sec/batch; 38h:06m:48s remains)
2017-12-07 20:28:03.400536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3142939 -4.3181925 -4.3202925 -4.321909 -4.3231225 -4.3242483 -4.3261766 -4.3263912 -4.3253512 -4.323092 -4.3209305 -4.3197455 -4.3190274 -4.3180914 -4.3177161][-4.3002481 -4.3057804 -4.3099775 -4.3123918 -4.3128743 -4.3127384 -4.3138452 -4.3131714 -4.3108549 -4.3076248 -4.3055277 -4.3060522 -4.3081861 -4.3087468 -4.3091869][-4.2787652 -4.2843137 -4.2872663 -4.28616 -4.2826519 -4.2804723 -4.2817011 -4.280787 -4.2771916 -4.2724152 -4.27057 -4.2736883 -4.2803912 -4.284925 -4.288835][-4.2484016 -4.2480197 -4.2429113 -4.2342 -4.2256718 -4.2227077 -4.2271366 -4.2295432 -4.2267327 -4.222518 -4.2236509 -4.2327762 -4.2457328 -4.2547493 -4.2629662][-4.2055259 -4.1928124 -4.1747317 -4.1578054 -4.1481228 -4.1513138 -4.1657057 -4.1760631 -4.1769681 -4.1754227 -4.1815352 -4.1975932 -4.2163754 -4.2290611 -4.2400508][-4.1642537 -4.1399112 -4.1111016 -4.088737 -4.0812492 -4.094429 -4.1204591 -4.1387167 -4.1424351 -4.1448874 -4.1572008 -4.1782694 -4.1997833 -4.2132759 -4.2242804][-4.1542196 -4.1269321 -4.0963874 -4.0726094 -4.0659275 -4.0834823 -4.1145506 -4.1351132 -4.138732 -4.1435523 -4.1597929 -4.1816354 -4.201282 -4.2128878 -4.2216024][-4.1734037 -4.1539392 -4.13235 -4.1133928 -4.1066418 -4.1192951 -4.1432533 -4.1577754 -4.1572332 -4.1603 -4.1763134 -4.1966028 -4.2135 -4.2244487 -4.2320781][-4.2084589 -4.1981835 -4.1886435 -4.1778135 -4.1711206 -4.1759863 -4.1886759 -4.1946931 -4.1902542 -4.1905508 -4.2032981 -4.2198739 -4.2340875 -4.2446513 -4.251596][-4.2427888 -4.2396278 -4.2377582 -4.2323413 -4.2270837 -4.2278476 -4.2329712 -4.2336321 -4.2285113 -4.2278147 -4.2366486 -4.2483544 -4.258914 -4.2681041 -4.2732863][-4.2695303 -4.27 -4.2703772 -4.2676105 -4.2642665 -4.2630181 -4.2628279 -4.260396 -4.2558322 -4.2556515 -4.2630315 -4.2727804 -4.2812743 -4.2879167 -4.2900786][-4.276886 -4.2759595 -4.2754693 -4.2740812 -4.2730603 -4.2726383 -4.2721019 -4.2703381 -4.2682757 -4.26956 -4.2772508 -4.285975 -4.2925038 -4.296649 -4.2969928][-4.2724953 -4.2692871 -4.2681756 -4.268393 -4.2700744 -4.2720084 -4.2725525 -4.2716541 -4.2713442 -4.2736225 -4.2805243 -4.2866 -4.2899642 -4.2921071 -4.2928867][-4.2641153 -4.2593894 -4.2598476 -4.2630658 -4.2673545 -4.2710505 -4.2717791 -4.2704067 -4.2702827 -4.2722311 -4.2773342 -4.2806239 -4.2815933 -4.2827458 -4.2854419][-4.2517552 -4.2456131 -4.2482414 -4.2550974 -4.2624731 -4.2669683 -4.2668166 -4.2646952 -4.264287 -4.2658749 -4.2700987 -4.2719655 -4.2719512 -4.2731142 -4.2773051]]...]
INFO - root - 2017-12-07 20:28:24.238376: step 18710, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.105 sec/batch; 37h:39m:24s remains)
INFO - root - 2017-12-07 20:28:45.406243: step 18720, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 38h:08m:05s remains)
INFO - root - 2017-12-07 20:29:06.686265: step 18730, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.139 sec/batch; 38h:15m:56s remains)
INFO - root - 2017-12-07 20:29:27.713257: step 18740, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.145 sec/batch; 38h:21m:55s remains)
INFO - root - 2017-12-07 20:29:48.916391: step 18750, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.158 sec/batch; 38h:35m:25s remains)
INFO - root - 2017-12-07 20:30:10.104604: step 18760, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 37h:36m:53s remains)
INFO - root - 2017-12-07 20:30:31.386196: step 18770, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.067 sec/batch; 36h:56m:33s remains)
INFO - root - 2017-12-07 20:30:52.398713: step 18780, loss = 2.07, batch loss = 2.02 (14.7 examples/sec; 2.184 sec/batch; 39h:01m:40s remains)
INFO - root - 2017-12-07 20:31:13.867775: step 18790, loss = 2.10, batch loss = 2.04 (15.2 examples/sec; 2.106 sec/batch; 37h:37m:53s remains)
INFO - root - 2017-12-07 20:31:35.062592: step 18800, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.120 sec/batch; 37h:52m:19s remains)
2017-12-07 20:31:36.604637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2893257 -4.293314 -4.294106 -4.2939458 -4.2935286 -4.2935314 -4.294138 -4.2949739 -4.2956519 -4.2964697 -4.2976665 -4.2992864 -4.3008909 -4.3018126 -4.3020205][-4.2930226 -4.2974238 -4.2984324 -4.2983565 -4.2976775 -4.2965851 -4.2954693 -4.2946458 -4.2945313 -4.2956514 -4.2977567 -4.3003798 -4.30266 -4.3036046 -4.3032265][-4.2935019 -4.2974315 -4.2979417 -4.2975917 -4.2964272 -4.2942061 -4.2913079 -4.2885613 -4.2874017 -4.2889681 -4.29247 -4.2969236 -4.300931 -4.3030682 -4.3030615][-4.2925768 -4.2952485 -4.29437 -4.2926 -4.2895956 -4.2850246 -4.2793145 -4.2738533 -4.2712951 -4.2736807 -4.279614 -4.2872491 -4.2944846 -4.2994032 -4.3011618][-4.2898831 -4.291316 -4.2888646 -4.2849007 -4.2789721 -4.2709017 -4.26152 -4.2528563 -4.2485566 -4.2513628 -4.2601848 -4.2715096 -4.2826843 -4.2912469 -4.2953897][-4.2850208 -4.2856383 -4.282105 -4.276206 -4.2674069 -4.2559929 -4.2435627 -4.2317915 -4.2244463 -4.2254224 -4.234972 -4.2487655 -4.2631254 -4.2751932 -4.2824144][-4.2791572 -4.279408 -4.2753081 -4.2682681 -4.2575297 -4.2435741 -4.2281318 -4.2126436 -4.1998711 -4.1951604 -4.2027526 -4.2179818 -4.2350941 -4.250967 -4.2623868][-4.2775488 -4.2769876 -4.2717018 -4.2630634 -4.2503963 -4.2338834 -4.2157245 -4.1961884 -4.1774149 -4.1658139 -4.1703916 -4.186583 -4.20666 -4.226758 -4.2426748][-4.278667 -4.2773328 -4.2713375 -4.2622991 -4.249681 -4.232862 -4.214292 -4.1931396 -4.1707149 -4.1539221 -4.1559005 -4.172905 -4.1948781 -4.2165966 -4.2342205][-4.2791963 -4.2778578 -4.2729545 -4.2661719 -4.2569561 -4.2438974 -4.2291903 -4.2112641 -4.1900082 -4.1721978 -4.1723485 -4.1877913 -4.2071662 -4.2252316 -4.2397151][-4.2811732 -4.2809043 -4.2784667 -4.2751966 -4.2705507 -4.2630162 -4.2541466 -4.2418489 -4.2250733 -4.2100239 -4.2090716 -4.220789 -4.2343669 -4.2454858 -4.254365][-4.2875566 -4.2880049 -4.2870836 -4.2858891 -4.2840967 -4.2804151 -4.2761207 -4.2693529 -4.2580681 -4.2472095 -4.2460332 -4.2534242 -4.2612028 -4.2664714 -4.2707334][-4.2920918 -4.2923884 -4.2913222 -4.2903814 -4.2897315 -4.2882895 -4.287147 -4.2847953 -4.2789607 -4.272603 -4.2713709 -4.2745695 -4.2775731 -4.2795238 -4.28094][-4.2900553 -4.2903566 -4.2892194 -4.2883372 -4.2881136 -4.2880406 -4.288559 -4.28859 -4.2863088 -4.2828617 -4.2810268 -4.2810259 -4.2809949 -4.2811718 -4.2817173][-4.2920742 -4.2930651 -4.2922382 -4.2914147 -4.2911673 -4.2913876 -4.2920995 -4.2925029 -4.2916756 -4.2895961 -4.2872715 -4.2853885 -4.2839723 -4.2839704 -4.2851233]]...]
INFO - root - 2017-12-07 20:31:57.645704: step 18810, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.147 sec/batch; 38h:21m:47s remains)
INFO - root - 2017-12-07 20:32:18.972196: step 18820, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.172 sec/batch; 38h:47m:54s remains)
INFO - root - 2017-12-07 20:32:40.181508: step 18830, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.114 sec/batch; 37h:45m:28s remains)
INFO - root - 2017-12-07 20:33:00.974384: step 18840, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.166 sec/batch; 38h:40m:36s remains)
INFO - root - 2017-12-07 20:33:22.165876: step 18850, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.136 sec/batch; 38h:07m:52s remains)
INFO - root - 2017-12-07 20:33:43.484823: step 18860, loss = 2.06, batch loss = 2.00 (15.5 examples/sec; 2.068 sec/batch; 36h:54m:41s remains)
INFO - root - 2017-12-07 20:34:04.654355: step 18870, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.176 sec/batch; 38h:50m:45s remains)
INFO - root - 2017-12-07 20:34:25.798989: step 18880, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 38h:07m:53s remains)
INFO - root - 2017-12-07 20:34:47.096737: step 18890, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.177 sec/batch; 38h:50m:18s remains)
INFO - root - 2017-12-07 20:35:08.202383: step 18900, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.071 sec/batch; 36h:57m:12s remains)
2017-12-07 20:35:09.821053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2528796 -4.2437797 -4.2305613 -4.2160215 -4.2111588 -4.2044067 -4.1916866 -4.2008033 -4.22403 -4.2475123 -4.2608743 -4.2557497 -4.2314386 -4.2049203 -4.1868815][-4.2514653 -4.2429585 -4.2299132 -4.2228141 -4.2303782 -4.2317786 -4.2226577 -4.2305121 -4.2495461 -4.26562 -4.2693381 -4.2654324 -4.2487297 -4.2300029 -4.2158093][-4.2552342 -4.2480187 -4.2379808 -4.2363691 -4.246726 -4.2524543 -4.2507153 -4.2554255 -4.2620783 -4.2634907 -4.2589197 -4.2585292 -4.2549634 -4.2497911 -4.2408547][-4.2740679 -4.2688465 -4.256701 -4.2498837 -4.2513318 -4.2512321 -4.2514229 -4.2489457 -4.236392 -4.2197347 -4.2073817 -4.2149463 -4.2346869 -4.2488642 -4.2489052][-4.28974 -4.2857409 -4.2657533 -4.2461972 -4.2340207 -4.2249708 -4.2206225 -4.2106228 -4.1842995 -4.1585 -4.1452255 -4.1638265 -4.2027493 -4.2346888 -4.2474632][-4.2890534 -4.2841272 -4.2585778 -4.2285852 -4.1986327 -4.1754336 -4.1672087 -4.155066 -4.1298757 -4.1067066 -4.0983291 -4.1243243 -4.174643 -4.2224135 -4.2449546][-4.271853 -4.2642255 -4.2330365 -4.1917787 -4.148 -4.1176338 -4.1119461 -4.1081266 -4.0941916 -4.0760846 -4.0631518 -4.0890307 -4.1480412 -4.2048583 -4.2338157][-4.2442193 -4.2339478 -4.1965103 -4.1479392 -4.1038485 -4.0778618 -4.0792975 -4.0864372 -4.0819764 -4.0629549 -4.0400949 -4.061882 -4.1289182 -4.1919794 -4.2198176][-4.2175364 -4.2056541 -4.16698 -4.1218729 -4.0914307 -4.0784407 -4.0881987 -4.101007 -4.0984316 -4.075212 -4.0438566 -4.0552959 -4.1138763 -4.1760497 -4.2054][-4.1979842 -4.1878333 -4.1568708 -4.126368 -4.1139412 -4.1126928 -4.1224627 -4.1327662 -4.131855 -4.1110287 -4.0770216 -4.0765777 -4.1164627 -4.1667633 -4.1967258][-4.1956811 -4.1895809 -4.1715674 -4.1584554 -4.1605487 -4.1641865 -4.1663795 -4.1685934 -4.1693563 -4.156497 -4.1308494 -4.1257372 -4.1465988 -4.1754556 -4.1963391][-4.2095232 -4.2106509 -4.2070031 -4.2041492 -4.2072668 -4.2063932 -4.1984372 -4.1895227 -4.1885462 -4.1875896 -4.1791391 -4.1794662 -4.189116 -4.1996074 -4.2049961][-4.2387066 -4.2455335 -4.2479696 -4.2428675 -4.2351971 -4.2238622 -4.2067995 -4.1925454 -4.1943088 -4.2061028 -4.2146096 -4.2269764 -4.23623 -4.2376428 -4.2313104][-4.2706137 -4.2794724 -4.280972 -4.267715 -4.2445974 -4.2212696 -4.1985111 -4.1855483 -4.1969194 -4.2205763 -4.2416062 -4.2652931 -4.2794166 -4.2783966 -4.2653379][-4.28675 -4.2923126 -4.2883391 -4.267477 -4.2372389 -4.2082181 -4.1830697 -4.1747885 -4.19549 -4.22645 -4.2517376 -4.2794271 -4.2999563 -4.3022285 -4.2905049]]...]
INFO - root - 2017-12-07 20:35:30.818596: step 18910, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.135 sec/batch; 38h:04m:36s remains)
INFO - root - 2017-12-07 20:35:51.958152: step 18920, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.179 sec/batch; 38h:51m:57s remains)
INFO - root - 2017-12-07 20:36:13.101078: step 18930, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.076 sec/batch; 37h:01m:26s remains)
INFO - root - 2017-12-07 20:36:34.059019: step 18940, loss = 2.09, batch loss = 2.03 (15.4 examples/sec; 2.080 sec/batch; 37h:04m:51s remains)
INFO - root - 2017-12-07 20:36:55.202065: step 18950, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 37h:42m:22s remains)
INFO - root - 2017-12-07 20:37:16.405467: step 18960, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.120 sec/batch; 37h:46m:41s remains)
INFO - root - 2017-12-07 20:37:37.263476: step 18970, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.124 sec/batch; 37h:50m:40s remains)
INFO - root - 2017-12-07 20:37:58.260256: step 18980, loss = 2.06, batch loss = 2.01 (15.3 examples/sec; 2.091 sec/batch; 37h:15m:45s remains)
INFO - root - 2017-12-07 20:38:19.388390: step 18990, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 37h:44m:36s remains)
INFO - root - 2017-12-07 20:38:40.358271: step 19000, loss = 2.07, batch loss = 2.02 (15.5 examples/sec; 2.062 sec/batch; 36h:44m:04s remains)
2017-12-07 20:38:41.922037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.35064 -4.3349738 -4.3106403 -4.28671 -4.2684193 -4.2587271 -4.2640309 -4.2789335 -4.2934418 -4.3079982 -4.3230538 -4.3313212 -4.3271694 -4.3195233 -4.3153667][-4.3527136 -4.330668 -4.2977881 -4.2658615 -4.2392058 -4.2211885 -4.2222514 -4.2416377 -4.2667041 -4.2929721 -4.3180089 -4.3319535 -4.3288951 -4.3199368 -4.3148446][-4.3542385 -4.3236523 -4.2773767 -4.231019 -4.1897769 -4.1590514 -4.1548443 -4.1817145 -4.2231112 -4.26587 -4.3010697 -4.3196149 -4.3181524 -4.3063774 -4.2973833][-4.3443174 -4.3018088 -4.2396879 -4.1766195 -4.1181145 -4.07312 -4.0663862 -4.1052952 -4.1675711 -4.2290564 -4.2736816 -4.293828 -4.2919688 -4.2766504 -4.2647123][-4.3284874 -4.2772417 -4.2063813 -4.1345625 -4.0632162 -4.0056438 -3.9971025 -4.0466409 -4.1283927 -4.2043767 -4.2508879 -4.2655873 -4.2602367 -4.2394423 -4.2197218][-4.315484 -4.2612195 -4.1936641 -4.1254716 -4.0519791 -3.9865954 -3.9693048 -4.0169172 -4.1049442 -4.1871023 -4.2287588 -4.2341666 -4.22239 -4.1984978 -4.1739693][-4.3095641 -4.2566762 -4.1996479 -4.1445503 -4.0815544 -4.0166407 -3.9886405 -4.0217376 -4.0960097 -4.1680765 -4.2030787 -4.2035308 -4.1874404 -4.1624317 -4.1383138][-4.3086748 -4.2620087 -4.217134 -4.1787524 -4.1329489 -4.0803947 -4.05064 -4.0639586 -4.1076827 -4.1552076 -4.1772361 -4.1710529 -4.14878 -4.1179762 -4.0933247][-4.3002381 -4.2593484 -4.2223606 -4.193769 -4.1608944 -4.1227117 -4.0990252 -4.0995221 -4.1173306 -4.1404672 -4.1487684 -4.1362271 -4.1063342 -4.0685658 -4.0434346][-4.2856116 -4.2476196 -4.2153444 -4.19038 -4.163887 -4.1357551 -4.1202064 -4.1192803 -4.1251111 -4.1353846 -4.137219 -4.1266122 -4.1006551 -4.0662093 -4.0438924][-4.2742023 -4.2364006 -4.2048078 -4.1800351 -4.1564417 -4.1352673 -4.1268997 -4.1290469 -4.1333227 -4.13976 -4.1466951 -4.148037 -4.136867 -4.1159053 -4.1005626][-4.2869258 -4.2542934 -4.22478 -4.1994467 -4.1774673 -4.1622553 -4.159914 -4.1660895 -4.1717563 -4.1784315 -4.1906071 -4.2044168 -4.2108521 -4.2049284 -4.1984715][-4.3169332 -4.2940588 -4.2712116 -4.25329 -4.2418036 -4.2373362 -4.2392483 -4.2447557 -4.2469845 -4.2478929 -4.2555304 -4.2682514 -4.2786341 -4.2794361 -4.2766328][-4.3386326 -4.3224721 -4.30491 -4.2930145 -4.2896023 -4.292696 -4.297019 -4.3006668 -4.3014679 -4.3013444 -4.304812 -4.3110518 -4.3170614 -4.3187771 -4.3183212][-4.3486 -4.3368659 -4.3226542 -4.3118587 -4.3082428 -4.3106451 -4.3135881 -4.3154378 -4.3164635 -4.3175321 -4.3212543 -4.3263497 -4.3302712 -4.332314 -4.3337541]]...]
INFO - root - 2017-12-07 20:39:03.088949: step 19010, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.113 sec/batch; 37h:37m:26s remains)
INFO - root - 2017-12-07 20:39:24.297621: step 19020, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 37h:50m:58s remains)
INFO - root - 2017-12-07 20:39:45.376976: step 19030, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.066 sec/batch; 36h:46m:42s remains)
INFO - root - 2017-12-07 20:40:06.285660: step 19040, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.096 sec/batch; 37h:18m:24s remains)
INFO - root - 2017-12-07 20:40:27.607745: step 19050, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.085 sec/batch; 37h:06m:34s remains)
INFO - root - 2017-12-07 20:40:49.012732: step 19060, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.120 sec/batch; 37h:43m:49s remains)
INFO - root - 2017-12-07 20:41:09.750939: step 19070, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 37h:44m:25s remains)
INFO - root - 2017-12-07 20:41:31.138677: step 19080, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.093 sec/batch; 37h:14m:32s remains)
INFO - root - 2017-12-07 20:41:52.373538: step 19090, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.141 sec/batch; 38h:04m:47s remains)
INFO - root - 2017-12-07 20:42:13.138120: step 19100, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 37h:43m:27s remains)
2017-12-07 20:42:14.829546: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3342605 -4.3383312 -4.3396239 -4.3360286 -4.3268976 -4.3182535 -4.3118372 -4.3110957 -4.3159475 -4.3223743 -4.3293066 -4.3359637 -4.3396568 -4.3421388 -4.3421531][-4.3297129 -4.3343511 -4.3350029 -4.3277683 -4.3106885 -4.2957091 -4.2823367 -4.2760196 -4.2828417 -4.2938886 -4.3047595 -4.3149137 -4.3211784 -4.3278446 -4.3306274][-4.31974 -4.32287 -4.3209929 -4.3102832 -4.2855515 -4.2618504 -4.2363052 -4.2194343 -4.2340388 -4.255466 -4.2710962 -4.28273 -4.29024 -4.304533 -4.3128076][-4.3084736 -4.303689 -4.2940836 -4.2812223 -4.2530227 -4.2215104 -4.178124 -4.1460233 -4.1728382 -4.2111278 -4.2337942 -4.2491646 -4.2614121 -4.2859855 -4.3013887][-4.2992411 -4.2859054 -4.267127 -4.2491331 -4.2171354 -4.17656 -4.1161165 -4.0691328 -4.10968 -4.1654277 -4.1941032 -4.2103839 -4.227417 -4.2615466 -4.2851372][-4.2930074 -4.2737303 -4.2444382 -4.212492 -4.1650381 -4.1072955 -4.0234814 -3.9576316 -4.0151758 -4.0982943 -4.1427174 -4.1665568 -4.1870041 -4.2294917 -4.2602577][-4.2804174 -4.2546144 -4.216085 -4.1699758 -4.1004467 -4.0197659 -3.9041774 -3.8080616 -3.8802588 -3.995635 -4.0599003 -4.0984411 -4.1279912 -4.1831903 -4.2258363][-4.2538276 -4.216794 -4.1619334 -4.0968533 -4.003881 -3.894557 -3.7443275 -3.6176448 -3.7068491 -3.8585522 -3.9542632 -4.0239367 -4.0758853 -4.1450329 -4.1997638][-4.2324109 -4.1954279 -4.146028 -4.0892491 -4.0110731 -3.9168644 -3.7844558 -3.6691039 -3.7460136 -3.8862634 -3.9768171 -4.0446391 -4.0936775 -4.1550822 -4.2034149][-4.2265935 -4.2012796 -4.1731267 -4.1426797 -4.0997033 -4.0426149 -3.9542925 -3.8715262 -3.9196737 -4.0189128 -4.0851507 -4.1318445 -4.1612368 -4.2004032 -4.2312179][-4.2350955 -4.2194891 -4.206862 -4.1926236 -4.1709394 -4.1389422 -4.0783858 -4.0170135 -4.04701 -4.115119 -4.1631646 -4.19683 -4.2147527 -4.2379379 -4.2581329][-4.2538261 -4.2415133 -4.2349381 -4.2288857 -4.2189679 -4.2012706 -4.1635342 -4.12589 -4.1507821 -4.20107 -4.2366385 -4.2590222 -4.2681541 -4.2791286 -4.2884817][-4.2761126 -4.2650242 -4.2598252 -4.2574682 -4.2561946 -4.2483058 -4.2286034 -4.2104158 -4.2309875 -4.2661104 -4.2909751 -4.30528 -4.309042 -4.3126779 -4.3138742][-4.2960291 -4.2869244 -4.2827396 -4.2828789 -4.284759 -4.2816448 -4.2730594 -4.2655573 -4.2751479 -4.2928615 -4.3065243 -4.3152246 -4.3186169 -4.3207045 -4.3222728][-4.3125443 -4.303885 -4.2986259 -4.2973552 -4.2975516 -4.2948141 -4.2900186 -4.2871985 -4.2911906 -4.2987318 -4.3056121 -4.3102984 -4.3145218 -4.3191767 -4.3239784]]...]
INFO - root - 2017-12-07 20:42:36.058272: step 19110, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 37h:39m:57s remains)
INFO - root - 2017-12-07 20:42:57.152139: step 19120, loss = 2.06, batch loss = 2.00 (15.4 examples/sec; 2.074 sec/batch; 36h:52m:27s remains)
INFO - root - 2017-12-07 20:43:17.797363: step 19130, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.092 sec/batch; 37h:11m:10s remains)
INFO - root - 2017-12-07 20:43:39.079740: step 19140, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 37h:58m:38s remains)
INFO - root - 2017-12-07 20:44:00.178857: step 19150, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.144 sec/batch; 38h:05m:41s remains)
INFO - root - 2017-12-07 20:44:21.043377: step 19160, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.059 sec/batch; 36h:34m:53s remains)
INFO - root - 2017-12-07 20:44:42.222713: step 19170, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 37h:41m:31s remains)
INFO - root - 2017-12-07 20:45:03.449099: step 19180, loss = 2.06, batch loss = 2.01 (14.9 examples/sec; 2.141 sec/batch; 38h:01m:57s remains)
INFO - root - 2017-12-07 20:45:24.593243: step 19190, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.093 sec/batch; 37h:10m:25s remains)
INFO - root - 2017-12-07 20:45:45.444394: step 19200, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 37h:47m:40s remains)
2017-12-07 20:45:47.002038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2096043 -4.2090378 -4.2311788 -4.2583542 -4.2768326 -4.2847533 -4.2874069 -4.2905774 -4.2928987 -4.2903056 -4.2815533 -4.2702446 -4.2602973 -4.2494359 -4.2368813][-4.1966085 -4.1976485 -4.2247305 -4.2522178 -4.2630816 -4.2580853 -4.2517571 -4.2540531 -4.2608027 -4.2645807 -4.2608433 -4.2539606 -4.2462296 -4.2340927 -4.2187004][-4.2021575 -4.20527 -4.2289648 -4.2492323 -4.24675 -4.2234411 -4.2065406 -4.2072997 -4.2215214 -4.2356052 -4.2425003 -4.2456684 -4.2448959 -4.2379823 -4.2271147][-4.21083 -4.2112103 -4.2227526 -4.2287354 -4.2094274 -4.1696606 -4.1405811 -4.1369634 -4.1562285 -4.1807971 -4.1989503 -4.2143621 -4.2232366 -4.2299361 -4.2300391][-4.1957903 -4.1926937 -4.1913848 -4.1797843 -4.1419048 -4.0838089 -4.0350165 -4.0225616 -4.0549335 -4.1049991 -4.1444392 -4.1758337 -4.1986036 -4.2195249 -4.2305074][-4.170733 -4.1611886 -4.1507978 -4.1242881 -4.0685062 -3.984777 -3.8999102 -3.8601823 -3.9133682 -4.0078788 -4.079494 -4.1323133 -4.170217 -4.2043037 -4.227345][-4.1388941 -4.1219406 -4.1039095 -4.0721068 -4.0050106 -3.8945909 -3.7591505 -3.6680388 -3.7387681 -3.8941526 -4.0138488 -4.0957823 -4.1511245 -4.1981549 -4.2324524][-4.1361804 -4.1130347 -4.0833354 -4.0416574 -3.969048 -3.8446944 -3.6802437 -3.5464883 -3.6272407 -3.8238297 -3.976572 -4.0798206 -4.1462226 -4.1981668 -4.2343483][-4.1479096 -4.1310358 -4.1052465 -4.0680494 -4.009511 -3.9139843 -3.7898259 -3.6938045 -3.7603467 -3.9181492 -4.0428948 -4.1302662 -4.1853089 -4.2271276 -4.2514129][-4.1797228 -4.1749821 -4.16136 -4.1329818 -4.0874262 -4.0211191 -3.9448168 -3.898103 -3.9480853 -4.0502005 -4.1336269 -4.1907468 -4.2287621 -4.258934 -4.273375][-4.2139935 -4.2174911 -4.2176671 -4.2022367 -4.16516 -4.1169205 -4.0753174 -4.0643158 -4.1029449 -4.1647234 -4.2177186 -4.2525625 -4.274941 -4.2920556 -4.2986264][-4.2520733 -4.25933 -4.2677813 -4.263485 -4.2349014 -4.1979141 -4.1783776 -4.1835208 -4.2120671 -4.246582 -4.2752891 -4.2941241 -4.3054376 -4.314733 -4.3174305][-4.2844224 -4.2907758 -4.3025527 -4.3064494 -4.287487 -4.2616153 -4.2512422 -4.2570958 -4.2761531 -4.2955155 -4.3095365 -4.3185968 -4.3237729 -4.3271089 -4.3278475][-4.3022184 -4.3041444 -4.3116384 -4.317718 -4.3101478 -4.299078 -4.2961836 -4.2998872 -4.3101416 -4.320776 -4.3271675 -4.3324003 -4.3362479 -4.3356438 -4.33631][-4.3104386 -4.3082256 -4.3109345 -4.316206 -4.3168912 -4.3166676 -4.318222 -4.3212409 -4.3257833 -4.3308506 -4.3326926 -4.3353882 -4.3380713 -4.3377919 -4.3393703]]...]
INFO - root - 2017-12-07 20:46:08.387419: step 19210, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.162 sec/batch; 38h:23m:22s remains)
INFO - root - 2017-12-07 20:46:29.795429: step 19220, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.106 sec/batch; 37h:23m:29s remains)
INFO - root - 2017-12-07 20:46:50.671751: step 19230, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.075 sec/batch; 36h:49m:55s remains)
INFO - root - 2017-12-07 20:47:11.860657: step 19240, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.126 sec/batch; 37h:44m:03s remains)
INFO - root - 2017-12-07 20:47:33.084400: step 19250, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.132 sec/batch; 37h:49m:45s remains)
INFO - root - 2017-12-07 20:47:53.934929: step 19260, loss = 2.07, batch loss = 2.01 (14.4 examples/sec; 2.222 sec/batch; 39h:24m:46s remains)
INFO - root - 2017-12-07 20:48:15.000086: step 19270, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.135 sec/batch; 37h:51m:42s remains)
INFO - root - 2017-12-07 20:48:36.209347: step 19280, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.097 sec/batch; 37h:10m:56s remains)
INFO - root - 2017-12-07 20:48:57.214305: step 19290, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.116 sec/batch; 37h:31m:03s remains)
INFO - root - 2017-12-07 20:49:18.282207: step 19300, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.129 sec/batch; 37h:45m:14s remains)
2017-12-07 20:49:19.818913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3243103 -4.3306007 -4.3313866 -4.3308487 -4.3300867 -4.3296089 -4.3268495 -4.3251085 -4.3263803 -4.327158 -4.3260593 -4.3227654 -4.3192286 -4.3168368 -4.3176303][-4.3210235 -4.3276343 -4.3265314 -4.3232803 -4.3198767 -4.31697 -4.3136778 -4.3154387 -4.321044 -4.3249645 -4.325479 -4.3219709 -4.31864 -4.3158107 -4.3162937][-4.3126297 -4.319725 -4.3156271 -4.3055086 -4.2917709 -4.2796793 -4.2730908 -4.2781563 -4.2908878 -4.3016934 -4.3102551 -4.3146544 -4.3174896 -4.3162341 -4.3136997][-4.3041821 -4.311141 -4.3023515 -4.2815084 -4.2503943 -4.2232857 -4.2110062 -4.2209458 -4.2452803 -4.2683268 -4.2918558 -4.3093495 -4.3185625 -4.3164258 -4.3044782][-4.2955818 -4.2995958 -4.2807603 -4.2444625 -4.19358 -4.148613 -4.1307983 -4.1504774 -4.1910734 -4.23313 -4.2734466 -4.301187 -4.3116627 -4.3037624 -4.2775922][-4.2917662 -4.289525 -4.2587128 -4.2079468 -4.140872 -4.0764017 -4.0534015 -4.0855932 -4.14542 -4.2081294 -4.2633452 -4.2937207 -4.2977982 -4.2766705 -4.236814][-4.2963495 -4.2861071 -4.2458973 -4.1893826 -4.1182313 -4.0454245 -4.0214629 -4.0634727 -4.13564 -4.2083621 -4.2663293 -4.28865 -4.2769489 -4.2397852 -4.1943388][-4.3035274 -4.2858295 -4.2432384 -4.1936011 -4.1345448 -4.0748644 -4.0601454 -4.1035419 -4.1691842 -4.231122 -4.2735739 -4.2778635 -4.2494154 -4.2036719 -4.1692171][-4.3028245 -4.2805395 -4.2437348 -4.2098718 -4.1737332 -4.1365929 -4.1303425 -4.1651468 -4.2158284 -4.2602081 -4.2804947 -4.2649441 -4.2241359 -4.1826324 -4.1717873][-4.2908921 -4.2667542 -4.2420664 -4.2289805 -4.2185931 -4.2009535 -4.1954446 -4.2174335 -4.2523808 -4.2785945 -4.278635 -4.247736 -4.2019396 -4.1735406 -4.1832204][-4.2763638 -4.2522082 -4.2391572 -4.2428055 -4.2499804 -4.2440376 -4.2366743 -4.2491193 -4.2704277 -4.2779331 -4.2616396 -4.2233629 -4.1826425 -4.1675391 -4.1862421][-4.2644444 -4.2405539 -4.2335486 -4.2437096 -4.2557087 -4.25212 -4.2457547 -4.2562752 -4.2710414 -4.2643828 -4.2359371 -4.1991243 -4.1714983 -4.1701765 -4.1902809][-4.2576342 -4.2332721 -4.2267675 -4.2313385 -4.2334018 -4.2269397 -4.2285542 -4.2492566 -4.2663708 -4.2525563 -4.21906 -4.191154 -4.1790366 -4.1858697 -4.2028947][-4.2574925 -4.2340903 -4.2216196 -4.2099466 -4.192584 -4.183445 -4.1996264 -4.2355952 -4.2561345 -4.2417054 -4.2119021 -4.19548 -4.197371 -4.210094 -4.2253561][-4.2564459 -4.23536 -4.2183414 -4.1942649 -4.1639996 -4.1532073 -4.1783724 -4.2194662 -4.2372775 -4.2253809 -4.2047448 -4.2002625 -4.2106738 -4.2255235 -4.2383351]]...]
INFO - root - 2017-12-07 20:49:40.946249: step 19310, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.109 sec/batch; 37h:23m:33s remains)
INFO - root - 2017-12-07 20:50:02.033288: step 19320, loss = 2.07, batch loss = 2.01 (16.0 examples/sec; 1.998 sec/batch; 35h:25m:06s remains)
INFO - root - 2017-12-07 20:50:23.048741: step 19330, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.181 sec/batch; 38h:38m:53s remains)
INFO - root - 2017-12-07 20:50:44.489298: step 19340, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.123 sec/batch; 37h:36m:47s remains)
INFO - root - 2017-12-07 20:51:05.634529: step 19350, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.143 sec/batch; 37h:57m:35s remains)
INFO - root - 2017-12-07 20:51:26.439831: step 19360, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.098 sec/batch; 37h:09m:26s remains)
INFO - root - 2017-12-07 20:51:47.729038: step 19370, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.142 sec/batch; 37h:56m:22s remains)
INFO - root - 2017-12-07 20:52:08.975829: step 19380, loss = 2.06, batch loss = 2.01 (15.3 examples/sec; 2.098 sec/batch; 37h:08m:53s remains)
INFO - root - 2017-12-07 20:52:29.919107: step 19390, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.132 sec/batch; 37h:45m:12s remains)
INFO - root - 2017-12-07 20:52:51.256688: step 19400, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.170 sec/batch; 38h:25m:01s remains)
2017-12-07 20:52:52.822228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.315484 -4.3104849 -4.3059864 -4.3028622 -4.2986488 -4.2950873 -4.295702 -4.300653 -4.3056955 -4.3069863 -4.3055758 -4.3059421 -4.3044796 -4.2986507 -4.2956829][-4.2933369 -4.2874794 -4.2848387 -4.28323 -4.2756391 -4.2684402 -4.2676167 -4.272222 -4.2769527 -4.2783151 -4.2763991 -4.2787824 -4.2786617 -4.2700996 -4.2655458][-4.26545 -4.2585568 -4.2553363 -4.25269 -4.2408218 -4.2289276 -4.2254281 -4.2294331 -4.2329011 -4.2327213 -4.2295132 -4.2363691 -4.2405515 -4.2311368 -4.2272563][-4.2379513 -4.2263875 -4.2180753 -4.212697 -4.1994805 -4.1810203 -4.1726613 -4.1751151 -4.1762762 -4.1742148 -4.1750088 -4.1900945 -4.2035952 -4.1998568 -4.2007031][-4.2093754 -4.1929154 -4.1792231 -4.1714272 -4.1601648 -4.1427941 -4.13625 -4.1372266 -4.1297884 -4.1206946 -4.1226883 -4.1440797 -4.1698627 -4.1832304 -4.195334][-4.1796284 -4.1615577 -4.145514 -4.1405749 -4.1342907 -4.1172981 -4.1119819 -4.1116829 -4.0913696 -4.0697389 -4.0679884 -4.0918379 -4.1284733 -4.1626854 -4.1915812][-4.1593733 -4.1482754 -4.1379642 -4.135221 -4.124547 -4.100769 -4.0936522 -4.090138 -4.059679 -4.025239 -4.014668 -4.0399489 -4.0876584 -4.1354795 -4.1750731][-4.1521506 -4.1491313 -4.1458488 -4.1458473 -4.1297 -4.0998688 -4.0907187 -4.0851393 -4.0558462 -4.0218377 -4.00802 -4.03308 -4.0846796 -4.1347747 -4.1739244][-4.1572666 -4.159132 -4.1630564 -4.1698809 -4.1590195 -4.1346226 -4.1235361 -4.1143255 -4.091116 -4.0667396 -4.0547709 -4.0717926 -4.1145082 -4.1570559 -4.1872253][-4.1821504 -4.1923332 -4.2043109 -4.2168951 -4.2160225 -4.2040939 -4.1945329 -4.1810923 -4.1628056 -4.1481209 -4.1372042 -4.1449361 -4.1723166 -4.2032962 -4.2245264][-4.2217193 -4.2375903 -4.2520204 -4.2647357 -4.2677197 -4.2643604 -4.258904 -4.2477088 -4.2373466 -4.2321458 -4.2258711 -4.229 -4.2430263 -4.2598863 -4.2717085][-4.2550349 -4.2690215 -4.2828412 -4.292047 -4.2918358 -4.2887616 -4.2868042 -4.2824206 -4.28018 -4.2819452 -4.2815142 -4.2838044 -4.2902284 -4.2964387 -4.3011675][-4.2807913 -4.2913356 -4.3027 -4.307445 -4.3039002 -4.2991114 -4.2970343 -4.2950125 -4.295711 -4.2991266 -4.30108 -4.3027081 -4.30528 -4.3075619 -4.3101974][-4.2967515 -4.3010693 -4.3062267 -4.3070321 -4.3031225 -4.2997594 -4.2993822 -4.2994432 -4.3000641 -4.3022289 -4.3044124 -4.3066287 -4.3092237 -4.3123097 -4.3147364][-4.3056374 -4.3053608 -4.3057055 -4.3045697 -4.3020482 -4.3008757 -4.3021655 -4.30341 -4.3041978 -4.3047113 -4.3050127 -4.3058639 -4.3078742 -4.3116775 -4.3148904]]...]
INFO - root - 2017-12-07 20:53:14.023077: step 19410, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.136 sec/batch; 37h:48m:01s remains)
INFO - root - 2017-12-07 20:53:35.090943: step 19420, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.167 sec/batch; 38h:20m:32s remains)
INFO - root - 2017-12-07 20:53:56.137911: step 19430, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.072 sec/batch; 36h:39m:49s remains)
INFO - root - 2017-12-07 20:54:17.108931: step 19440, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.087 sec/batch; 36h:55m:04s remains)
INFO - root - 2017-12-07 20:54:38.161148: step 19450, loss = 2.07, batch loss = 2.01 (16.2 examples/sec; 1.978 sec/batch; 34h:59m:31s remains)
INFO - root - 2017-12-07 20:54:59.417428: step 19460, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.123 sec/batch; 37h:32m:31s remains)
INFO - root - 2017-12-07 20:55:20.710326: step 19470, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.137 sec/batch; 37h:47m:22s remains)
INFO - root - 2017-12-07 20:55:42.083046: step 19480, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 37h:26m:14s remains)
INFO - root - 2017-12-07 20:56:02.979958: step 19490, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.193 sec/batch; 38h:45m:48s remains)
INFO - root - 2017-12-07 20:56:24.358311: step 19500, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.185 sec/batch; 38h:37m:21s remains)
2017-12-07 20:56:25.811903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2637291 -4.2477036 -4.2379937 -4.2416611 -4.2590671 -4.2757192 -4.2857456 -4.28437 -4.2779903 -4.2721086 -4.2682838 -4.2638135 -4.2576342 -4.2573829 -4.2670569][-4.2367959 -4.2145214 -4.1977215 -4.2023435 -4.2240896 -4.2433977 -4.2528358 -4.2492247 -4.2405043 -4.2336321 -4.2286034 -4.220613 -4.2116232 -4.2140169 -4.2301106][-4.2134628 -4.1897516 -4.1691332 -4.1716013 -4.1912885 -4.2065773 -4.2086372 -4.1975622 -4.18415 -4.1772447 -4.1757503 -4.1690826 -4.1617255 -4.1678638 -4.18922][-4.2115965 -4.1926818 -4.173727 -4.1729941 -4.1815772 -4.1815004 -4.166254 -4.1394711 -4.119091 -4.118784 -4.1317868 -4.1362576 -4.1349783 -4.1433439 -4.1670818][-4.2336783 -4.2203851 -4.20452 -4.198935 -4.1930871 -4.1718025 -4.1328669 -4.0860229 -4.0603342 -4.0742364 -4.111867 -4.1341963 -4.139555 -4.146224 -4.1682253][-4.2449265 -4.2359638 -4.2235932 -4.2144079 -4.1912518 -4.1412621 -4.0735455 -4.0100422 -3.9922628 -4.0339479 -4.1026621 -4.1494203 -4.164649 -4.1681461 -4.1826425][-4.22932 -4.2196827 -4.2103629 -4.2011914 -4.1696262 -4.1005287 -4.0118604 -3.9440544 -3.9448714 -4.0131736 -4.1018281 -4.1632376 -4.1841 -4.1851425 -4.1916556][-4.2118049 -4.2030516 -4.2002606 -4.1989589 -4.1759663 -4.1112518 -4.026711 -3.9706624 -3.9831913 -4.0489216 -4.1258678 -4.1780324 -4.1916242 -4.18799 -4.1895552][-4.2124081 -4.2053142 -4.2084813 -4.2180028 -4.2100587 -4.1649284 -4.0995545 -4.0583115 -4.0642776 -4.1036749 -4.1529074 -4.1829104 -4.1846838 -4.1783428 -4.1841946][-4.2363691 -4.2333279 -4.2399035 -4.2523704 -4.2485943 -4.2134843 -4.1608958 -4.1297073 -4.1333032 -4.1560793 -4.1871886 -4.1994781 -4.1917424 -4.1858053 -4.1971221][-4.264286 -4.2620606 -4.2684255 -4.2780652 -4.2693071 -4.2362318 -4.192482 -4.1734433 -4.1814432 -4.2004638 -4.2277641 -4.2350397 -4.2242904 -4.2171307 -4.2282453][-4.291183 -4.2908592 -4.2945433 -4.2980437 -4.28469 -4.2559257 -4.2246323 -4.21679 -4.2312293 -4.2506371 -4.2754188 -4.2827668 -4.273416 -4.2647309 -4.2708864][-4.3008881 -4.300868 -4.3020525 -4.3048968 -4.2951617 -4.2781587 -4.2628803 -4.2656994 -4.2806144 -4.2938309 -4.3086815 -4.3127146 -4.3047528 -4.2956724 -4.2988119][-4.2971177 -4.2939653 -4.2938552 -4.29844 -4.2962542 -4.2916765 -4.2885542 -4.2946539 -4.303834 -4.3075767 -4.3092203 -4.3074489 -4.3012357 -4.29607 -4.3018627][-4.3030157 -4.2986593 -4.2976356 -4.3022404 -4.3050227 -4.3060346 -4.3067079 -4.310751 -4.3138919 -4.3121128 -4.307713 -4.3033938 -4.3001232 -4.2996407 -4.3057718]]...]
INFO - root - 2017-12-07 20:56:47.200043: step 19510, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.088 sec/batch; 36h:54m:09s remains)
INFO - root - 2017-12-07 20:57:08.087519: step 19520, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.098 sec/batch; 37h:04m:34s remains)
INFO - root - 2017-12-07 20:57:29.248109: step 19530, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 37h:15m:37s remains)
INFO - root - 2017-12-07 20:57:50.488614: step 19540, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.158 sec/batch; 38h:06m:32s remains)
INFO - root - 2017-12-07 20:58:11.550721: step 19550, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 37h:30m:04s remains)
INFO - root - 2017-12-07 20:58:32.911304: step 19560, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.167 sec/batch; 38h:15m:20s remains)
INFO - root - 2017-12-07 20:58:54.175030: step 19570, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.073 sec/batch; 36h:36m:00s remains)
INFO - root - 2017-12-07 20:59:15.035483: step 19580, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 37h:39m:25s remains)
INFO - root - 2017-12-07 20:59:36.392017: step 19590, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.123 sec/batch; 37h:28m:06s remains)
INFO - root - 2017-12-07 20:59:57.677872: step 19600, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.105 sec/batch; 37h:08m:44s remains)
2017-12-07 20:59:59.178769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20675 -4.196588 -4.1888652 -4.1899958 -4.1997023 -4.1994228 -4.2067175 -4.2216954 -4.2321243 -4.2303796 -4.2237415 -4.2217288 -4.2150221 -4.197238 -4.1655207][-4.2175808 -4.2077136 -4.19698 -4.1882992 -4.1830277 -4.1757174 -4.1847396 -4.2066569 -4.2259583 -4.2270284 -4.2209625 -4.2209549 -4.2173934 -4.2004285 -4.172513][-4.2291679 -4.2173572 -4.2057214 -4.1917834 -4.1745453 -4.1605792 -4.1664815 -4.1868486 -4.2079892 -4.2158689 -4.2154174 -4.2211666 -4.2229242 -4.2099566 -4.1885362][-4.2346687 -4.2237916 -4.21591 -4.2029538 -4.1765084 -4.1523561 -4.1435204 -4.15387 -4.1768675 -4.1912565 -4.1958365 -4.2056127 -4.2110338 -4.2039604 -4.1943622][-4.2267637 -4.2192259 -4.217876 -4.204587 -4.1697087 -4.1277442 -4.0906591 -4.0837984 -4.1107249 -4.1379237 -4.1542625 -4.1691461 -4.1740785 -4.1707363 -4.1701427][-4.1871204 -4.1800065 -4.1807847 -4.1642084 -4.1165171 -4.0472035 -3.9718466 -3.9500186 -3.9926691 -4.0431647 -4.0819883 -4.1056852 -4.1109405 -4.1114316 -4.1178861][-4.1312146 -4.1221409 -4.1147442 -4.0897303 -4.0260839 -3.9290819 -3.8226051 -3.804059 -3.8758225 -3.9545755 -4.0193391 -4.0590005 -4.0694823 -4.0737486 -4.0871][-4.1052823 -4.0995817 -4.0924144 -4.0717955 -4.0198712 -3.9410431 -3.8625541 -3.8661942 -3.9384031 -4.0082865 -4.0650578 -4.0985184 -4.1020203 -4.1007533 -4.1153946][-4.1390185 -4.1417909 -4.1448493 -4.1383071 -4.1122928 -4.0714922 -4.03655 -4.0487275 -4.0910659 -4.12639 -4.1542597 -4.1688356 -4.1656218 -4.1628051 -4.1716995][-4.1955991 -4.2018456 -4.2086835 -4.2112193 -4.2051468 -4.1907907 -4.1774669 -4.1855783 -4.2033954 -4.2095485 -4.2148252 -4.2185187 -4.2134275 -4.20848 -4.2105594][-4.2474623 -4.25301 -4.2580953 -4.2627649 -4.2651911 -4.2629671 -4.258234 -4.2609239 -4.2637315 -4.2537231 -4.2449446 -4.2395234 -4.23033 -4.2239385 -4.2274275][-4.2644572 -4.2684226 -4.2677584 -4.2679458 -4.2727389 -4.2789326 -4.2834725 -4.2844281 -4.2779894 -4.261323 -4.243885 -4.2323394 -4.2222376 -4.2225075 -4.2293344][-4.2504616 -4.2529321 -4.2480259 -4.24413 -4.250751 -4.2647667 -4.2762518 -4.2793922 -4.2689886 -4.2485008 -4.2265229 -4.2119536 -4.2032461 -4.2096829 -4.2178507][-4.2358241 -4.2335653 -4.2216144 -4.211884 -4.2170219 -4.2330236 -4.2470527 -4.2518759 -4.2434096 -4.2261224 -4.206955 -4.1943474 -4.1908121 -4.2047911 -4.2138119][-4.2280526 -4.2186222 -4.1992183 -4.1822124 -4.1832156 -4.1969752 -4.2088695 -4.2141962 -4.2107821 -4.1980848 -4.1860566 -4.1809578 -4.187561 -4.2086911 -4.2182236]]...]
INFO - root - 2017-12-07 21:00:20.348342: step 19610, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 37h:26m:42s remains)
INFO - root - 2017-12-07 21:00:41.532196: step 19620, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.088 sec/batch; 36h:49m:28s remains)
INFO - root - 2017-12-07 21:01:02.828630: step 19630, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.157 sec/batch; 38h:02m:15s remains)
INFO - root - 2017-12-07 21:01:24.002241: step 19640, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.094 sec/batch; 36h:55m:31s remains)
INFO - root - 2017-12-07 21:01:44.900258: step 19650, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 37h:24m:44s remains)
INFO - root - 2017-12-07 21:02:06.104471: step 19660, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.090 sec/batch; 36h:51m:12s remains)
INFO - root - 2017-12-07 21:02:27.213108: step 19670, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.106 sec/batch; 37h:07m:35s remains)
INFO - root - 2017-12-07 21:02:48.044082: step 19680, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.108 sec/batch; 37h:08m:46s remains)
INFO - root - 2017-12-07 21:03:09.207084: step 19690, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.140 sec/batch; 37h:42m:49s remains)
INFO - root - 2017-12-07 21:03:30.384967: step 19700, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.102 sec/batch; 37h:01m:50s remains)
2017-12-07 21:03:31.967869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2038093 -4.1913686 -4.1811242 -4.18151 -4.1867752 -4.1867857 -4.1774545 -4.1644807 -4.1552086 -4.1470671 -4.1428661 -4.1460505 -4.1533222 -4.1592836 -4.1608438][-4.1987581 -4.1890469 -4.1822591 -4.1862946 -4.1983542 -4.2031169 -4.1979256 -4.1892548 -4.1839247 -4.1798205 -4.1794572 -4.1856995 -4.1920404 -4.1941061 -4.1898861][-4.2000718 -4.1899891 -4.1803594 -4.1819248 -4.1960759 -4.2077823 -4.2123928 -4.2154651 -4.221066 -4.2265143 -4.2337642 -4.2444048 -4.2503686 -4.2484174 -4.238234][-4.2141743 -4.1941662 -4.1718287 -4.1604662 -4.166256 -4.1756749 -4.1846051 -4.1977272 -4.2176456 -4.2406683 -4.2628341 -4.2832718 -4.2943611 -4.2941713 -4.2829003][-4.232276 -4.1946197 -4.1521749 -4.1190891 -4.1068192 -4.1023221 -4.1025043 -4.1169157 -4.1519637 -4.19972 -4.2430344 -4.2773256 -4.2969089 -4.303411 -4.296021][-4.2487841 -4.1982527 -4.1381688 -4.0836263 -4.0447416 -4.0115433 -3.9842627 -3.9836247 -4.0329213 -4.1107655 -4.1793232 -4.2289124 -4.2609015 -4.2777386 -4.2796373][-4.2483969 -4.20137 -4.1387944 -4.0722809 -4.0097609 -3.9424603 -3.8706107 -3.8353014 -3.8958616 -4.0049262 -4.0987043 -4.1624379 -4.2089133 -4.2389407 -4.253746][-4.2429357 -4.2150855 -4.1697421 -4.1113195 -4.0419025 -3.9531651 -3.8439572 -3.7694526 -3.8213592 -3.9385855 -4.038065 -4.103703 -4.1579061 -4.2012572 -4.2312016][-4.2376013 -4.2316279 -4.2121882 -4.1760526 -4.1212411 -4.04463 -3.9471655 -3.8737488 -3.8923807 -3.9702032 -4.042222 -4.0910716 -4.1379547 -4.1854544 -4.2245574][-4.2278323 -4.2346992 -4.2335753 -4.218998 -4.1876974 -4.1404157 -4.07918 -4.03106 -4.0295219 -4.0631189 -4.0988946 -4.1214094 -4.1491961 -4.187192 -4.2256527][-4.2235808 -4.2291975 -4.23102 -4.2264733 -4.211072 -4.1862955 -4.1558371 -4.1320233 -4.1282039 -4.1396875 -4.1523166 -4.1572905 -4.1690512 -4.19416 -4.2225671][-4.2284532 -4.2282577 -4.225491 -4.22061 -4.209538 -4.1951928 -4.1795158 -4.1675549 -4.1662211 -4.1703272 -4.1729054 -4.1712117 -4.1741724 -4.1893992 -4.2073774][-4.2559462 -4.2533035 -4.2483487 -4.2416368 -4.2300215 -4.2153854 -4.2012949 -4.1888671 -4.1841507 -4.1843395 -4.1837626 -4.1798291 -4.1784759 -4.1838136 -4.1902122][-4.2681022 -4.271275 -4.2720432 -4.2698526 -4.2628522 -4.2505755 -4.2370892 -4.2236719 -4.2159333 -4.2127967 -4.2114177 -4.208508 -4.2047658 -4.2011781 -4.1952529][-4.2424436 -4.2552633 -4.2676187 -4.27511 -4.277041 -4.2703133 -4.2597575 -4.2483139 -4.2403808 -4.2372684 -4.2370706 -4.2377338 -4.2357802 -4.2288575 -4.2159505]]...]
INFO - root - 2017-12-07 21:03:52.722672: step 19710, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.100 sec/batch; 36h:59m:33s remains)
INFO - root - 2017-12-07 21:04:14.003555: step 19720, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.136 sec/batch; 37h:37m:02s remains)
INFO - root - 2017-12-07 21:04:35.181652: step 19730, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.107 sec/batch; 37h:06m:34s remains)
INFO - root - 2017-12-07 21:04:56.164043: step 19740, loss = 2.07, batch loss = 2.01 (16.1 examples/sec; 1.992 sec/batch; 35h:04m:27s remains)
INFO - root - 2017-12-07 21:05:17.420858: step 19750, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.140 sec/batch; 37h:40m:02s remains)
INFO - root - 2017-12-07 21:05:38.878922: step 19760, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.199 sec/batch; 38h:42m:08s remains)
INFO - root - 2017-12-07 21:06:00.016709: step 19770, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.172 sec/batch; 38h:13m:30s remains)
INFO - root - 2017-12-07 21:06:20.870204: step 19780, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.096 sec/batch; 36h:52m:39s remains)
INFO - root - 2017-12-07 21:06:42.014663: step 19790, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 37h:26m:10s remains)
INFO - root - 2017-12-07 21:07:03.312046: step 19800, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.123 sec/batch; 37h:20m:26s remains)
2017-12-07 21:07:04.870096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2671084 -4.270978 -4.2800946 -4.2836714 -4.2724066 -4.2538142 -4.239522 -4.2371845 -4.23823 -4.2281828 -4.193203 -4.1440592 -4.1098475 -4.1117749 -4.1479788][-4.2636604 -4.256144 -4.25403 -4.2503443 -4.2349496 -4.2136121 -4.199451 -4.2056937 -4.2230082 -4.2330089 -4.2237334 -4.2004447 -4.1757336 -4.1730027 -4.1927981][-4.2591305 -4.2373271 -4.2156863 -4.1947131 -4.1708131 -4.1479025 -4.1385603 -4.1549687 -4.1883049 -4.2181659 -4.2313786 -4.2328076 -4.2200508 -4.2153697 -4.221313][-4.258256 -4.2256913 -4.184958 -4.1388354 -4.0992112 -4.07477 -4.0712233 -4.0987959 -4.147819 -4.1923227 -4.2247252 -4.2473497 -4.2465086 -4.2359281 -4.2246666][-4.2608228 -4.2268267 -4.1736231 -4.1013141 -4.0336637 -3.9963055 -3.9960489 -4.0362854 -4.0986419 -4.1556573 -4.1991034 -4.23359 -4.2408834 -4.2268972 -4.204][-4.2666359 -4.2386146 -4.1771779 -4.0792437 -3.9769843 -3.9148998 -3.9143231 -3.9678886 -4.0405164 -4.1071277 -4.1564503 -4.1924977 -4.2050595 -4.1896243 -4.1680417][-4.2606864 -4.2434282 -4.184298 -4.0696015 -3.9347765 -3.84404 -3.8342273 -3.8983243 -3.9878061 -4.0687561 -4.1245713 -4.1568413 -4.1650028 -4.1482306 -4.1330967][-4.2421913 -4.2350945 -4.1857605 -4.06985 -3.9192424 -3.8112934 -3.7937169 -3.8648672 -3.9734194 -4.0717678 -4.1349649 -4.1580882 -4.1509037 -4.1261525 -4.1144023][-4.223907 -4.2233334 -4.1852546 -4.0808129 -3.9410715 -3.84565 -3.8407061 -3.916748 -4.0249538 -4.12037 -4.1761293 -4.1857367 -4.1617117 -4.1236844 -4.1080604][-4.2138047 -4.2133188 -4.1857529 -4.1071305 -4.0031462 -3.941138 -3.9527879 -4.0191903 -4.102674 -4.1718655 -4.20718 -4.2037244 -4.1721959 -4.1294789 -4.112402][-4.2186604 -4.2154493 -4.1963749 -4.1472583 -4.0855417 -4.0534644 -4.0670729 -4.1097221 -4.1585336 -4.1950231 -4.207726 -4.1944556 -4.1679788 -4.1355343 -4.1227384][-4.2370558 -4.2361355 -4.2233267 -4.1918354 -4.1544313 -4.1369872 -4.1451163 -4.1652646 -4.1857381 -4.1953168 -4.1909375 -4.1775856 -4.1654835 -4.1513896 -4.1460018][-4.2697325 -4.2710109 -4.2583232 -4.2309823 -4.19937 -4.1831083 -4.1828895 -4.1892395 -4.1929808 -4.1885848 -4.1780834 -4.1707191 -4.1731062 -4.1747613 -4.174417][-4.3047352 -4.30592 -4.2889261 -4.2548709 -4.2173209 -4.1934896 -4.1849413 -4.1856818 -4.18695 -4.1829076 -4.177352 -4.1787572 -4.1897888 -4.2000365 -4.2019567][-4.3322034 -4.3312135 -4.3075171 -4.2623358 -4.2138667 -4.1830831 -4.172308 -4.1751604 -4.1817 -4.1859746 -4.1902595 -4.2000365 -4.2155504 -4.2278037 -4.2295585]]...]
INFO - root - 2017-12-07 21:07:25.749965: step 19810, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.117 sec/batch; 37h:13m:43s remains)
INFO - root - 2017-12-07 21:07:47.109497: step 19820, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.145 sec/batch; 37h:43m:18s remains)
INFO - root - 2017-12-07 21:08:08.520440: step 19830, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.139 sec/batch; 37h:36m:57s remains)
INFO - root - 2017-12-07 21:08:29.450994: step 19840, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.134 sec/batch; 37h:30m:21s remains)
INFO - root - 2017-12-07 21:08:50.716983: step 19850, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.127 sec/batch; 37h:22m:50s remains)
INFO - root - 2017-12-07 21:09:11.997710: step 19860, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.073 sec/batch; 36h:26m:00s remains)
INFO - root - 2017-12-07 21:09:33.013069: step 19870, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.166 sec/batch; 38h:03m:07s remains)
INFO - root - 2017-12-07 21:09:54.425678: step 19880, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.116 sec/batch; 37h:10m:33s remains)
INFO - root - 2017-12-07 21:10:15.596622: step 19890, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.132 sec/batch; 37h:26m:41s remains)
INFO - root - 2017-12-07 21:10:36.511207: step 19900, loss = 2.06, batch loss = 2.00 (16.5 examples/sec; 1.942 sec/batch; 34h:06m:21s remains)
2017-12-07 21:10:38.130997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3430147 -4.3321395 -4.3237314 -4.3162961 -4.3111753 -4.3113279 -4.3113995 -4.3093338 -4.3026667 -4.296628 -4.2901125 -4.2803793 -4.2641516 -4.2492576 -4.2441006][-4.3385968 -4.3259931 -4.31755 -4.3090324 -4.3015566 -4.2985153 -4.2947712 -4.2853389 -4.2769489 -4.2759395 -4.27495 -4.2661371 -4.2460537 -4.2256632 -4.2139645][-4.3374457 -4.3236609 -4.3148966 -4.3054695 -4.2949228 -4.2887406 -4.2808847 -4.26329 -4.2509813 -4.251935 -4.2530041 -4.2460613 -4.2265606 -4.2038851 -4.1908531][-4.3365626 -4.3199534 -4.3090296 -4.2955265 -4.278348 -4.2662811 -4.2564344 -4.2338047 -4.2154684 -4.2178621 -4.2255878 -4.2217097 -4.2085562 -4.1916795 -4.18324][-4.3316545 -4.3080344 -4.2891083 -4.2699404 -4.244905 -4.2245817 -4.2125511 -4.1858978 -4.1638818 -4.1654019 -4.1823878 -4.188962 -4.1846094 -4.1787324 -4.1813364][-4.3194947 -4.2845287 -4.2549982 -4.229898 -4.1978359 -4.1701784 -4.1543212 -4.1203861 -4.0940852 -4.1005697 -4.1296968 -4.1521153 -4.1616659 -4.1648941 -4.1725035][-4.3040113 -4.2632413 -4.2315316 -4.2095566 -4.182539 -4.1539474 -4.1286588 -4.0818219 -4.0541353 -4.0683751 -4.1098518 -4.148797 -4.1720624 -4.1801281 -4.1860924][-4.2959557 -4.2563534 -4.2282038 -4.2137461 -4.1956892 -4.172513 -4.143219 -4.08913 -4.056571 -4.0762362 -4.1222482 -4.1644359 -4.1900878 -4.2050557 -4.2145114][-4.2964063 -4.26242 -4.2394423 -4.2319078 -4.2235241 -4.2060232 -4.1755786 -4.1213665 -4.0796523 -4.091258 -4.1303754 -4.1660829 -4.1928692 -4.21754 -4.2376943][-4.302259 -4.272737 -4.2518964 -4.2488837 -4.2473521 -4.2360668 -4.2151871 -4.1720734 -4.1279 -4.1235905 -4.1494222 -4.1766567 -4.2067466 -4.2390175 -4.2644973][-4.3112655 -4.2862992 -4.2683158 -4.2717957 -4.2780662 -4.2740049 -4.2643762 -4.2382617 -4.2033563 -4.1892881 -4.1974254 -4.2108488 -4.2346358 -4.2691965 -4.2974081][-4.3197646 -4.2982569 -4.2831979 -4.2910419 -4.3019075 -4.3020082 -4.2989888 -4.2873883 -4.2670126 -4.2521586 -4.2511544 -4.2546859 -4.2698855 -4.2990055 -4.3250942][-4.3239384 -4.3044481 -4.2898231 -4.296699 -4.3093557 -4.3111362 -4.3102865 -4.3060317 -4.2948751 -4.2821803 -4.2802339 -4.2824855 -4.2938776 -4.316824 -4.3387475][-4.3293948 -4.3122892 -4.2989669 -4.3030548 -4.3137636 -4.3170109 -4.3174367 -4.3164158 -4.3109365 -4.3027816 -4.3039818 -4.3066077 -4.3130827 -4.3291211 -4.3454142][-4.3405714 -4.3275118 -4.31664 -4.31718 -4.3235397 -4.3280706 -4.3314872 -4.3332343 -4.3303061 -4.3250718 -4.3270159 -4.3289542 -4.33252 -4.341928 -4.3519411]]...]
INFO - root - 2017-12-07 21:10:59.255457: step 19910, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.161 sec/batch; 37h:56m:55s remains)
INFO - root - 2017-12-07 21:11:20.684089: step 19920, loss = 2.07, batch loss = 2.02 (14.4 examples/sec; 2.219 sec/batch; 38h:57m:11s remains)
INFO - root - 2017-12-07 21:11:41.962932: step 19930, loss = 2.09, batch loss = 2.03 (15.4 examples/sec; 2.084 sec/batch; 36h:35m:10s remains)
INFO - root - 2017-12-07 21:12:02.927733: step 19940, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.177 sec/batch; 38h:12m:44s remains)
INFO - root - 2017-12-07 21:12:24.094734: step 19950, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 37h:54m:14s remains)
INFO - root - 2017-12-07 21:12:45.249413: step 19960, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 37h:27m:46s remains)
INFO - root - 2017-12-07 21:13:06.252896: step 19970, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 37h:21m:27s remains)
INFO - root - 2017-12-07 21:13:27.610665: step 19980, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 2.204 sec/batch; 38h:39m:04s remains)
INFO - root - 2017-12-07 21:13:48.832894: step 19990, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 37h:24m:55s remains)
INFO - root - 2017-12-07 21:14:09.549532: step 20000, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.088 sec/batch; 36h:36m:17s remains)
2017-12-07 21:14:11.161397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3358841 -4.3276935 -4.3000426 -4.2772222 -4.2630525 -4.271307 -4.2875962 -4.2934575 -4.2929034 -4.2809834 -4.2655435 -4.2627587 -4.2647867 -4.2609715 -4.2455373][-4.3284788 -4.321198 -4.2963119 -4.2820044 -4.2787833 -4.2936354 -4.3129587 -4.3139052 -4.3054094 -4.2852273 -4.2634568 -4.2562571 -4.2504339 -4.240634 -4.2215505][-4.3119669 -4.3053803 -4.2823963 -4.2762384 -4.2840571 -4.3016882 -4.3194337 -4.3141932 -4.2949929 -4.2698684 -4.2496004 -4.2498732 -4.2509055 -4.2438459 -4.2250853][-4.3024826 -4.295826 -4.2710309 -4.2668362 -4.2775621 -4.2911758 -4.3027806 -4.2891293 -4.2618623 -4.2380538 -4.2234139 -4.2352214 -4.2507958 -4.2540784 -4.2412596][-4.3003497 -4.2950974 -4.2653279 -4.2513733 -4.2509494 -4.2488837 -4.2386026 -4.2067962 -4.1823425 -4.1801848 -4.1882706 -4.2189984 -4.25076 -4.2684393 -4.261241][-4.3076205 -4.308147 -4.2728624 -4.2444339 -4.2230086 -4.1889477 -4.135653 -4.0603819 -4.0393233 -4.0920105 -4.1527624 -4.2124834 -4.2623281 -4.2869029 -4.2780037][-4.3196387 -4.326798 -4.2922606 -4.251102 -4.2055068 -4.12718 -4.0088515 -3.8648705 -3.8432026 -3.9785919 -4.1135902 -4.2089019 -4.2750592 -4.2965469 -4.2783146][-4.3306603 -4.3410349 -4.3078575 -4.2572441 -4.1937094 -4.0935955 -3.9470863 -3.777451 -3.7594781 -3.9361594 -4.1047273 -4.2176557 -4.2870994 -4.301424 -4.2726736][-4.3370776 -4.3501716 -4.3200479 -4.26828 -4.2015438 -4.1153097 -4.0089965 -3.9004397 -3.8950865 -4.0245919 -4.1599035 -4.2516642 -4.3055882 -4.304873 -4.2590947][-4.3377328 -4.3518128 -4.3259974 -4.2802515 -4.2223744 -4.1613011 -4.1076336 -4.0636649 -4.0675611 -4.1445136 -4.2339754 -4.2939262 -4.32099 -4.2983003 -4.2344656][-4.3361621 -4.34819 -4.3277817 -4.2975936 -4.2576804 -4.2184577 -4.19508 -4.1810522 -4.1901135 -4.2416325 -4.3014379 -4.3330212 -4.3342147 -4.29155 -4.2117252][-4.3344555 -4.3396635 -4.3209772 -4.304297 -4.2898192 -4.2743998 -4.26783 -4.2686028 -4.2796183 -4.314383 -4.3509116 -4.358273 -4.3425393 -4.2902775 -4.2090268][-4.3327217 -4.3341641 -4.317071 -4.307827 -4.3113875 -4.3177919 -4.3254795 -4.3333364 -4.3413353 -4.3607383 -4.3742895 -4.3662319 -4.3450894 -4.2965074 -4.22643][-4.3357043 -4.3379526 -4.3240652 -4.3161678 -4.3220944 -4.3333735 -4.3449779 -4.3557668 -4.3609886 -4.3706431 -4.3711853 -4.3584056 -4.3393083 -4.2988248 -4.2444396][-4.3402748 -4.3436222 -4.3350453 -4.3296547 -4.3301339 -4.3354197 -4.3429704 -4.3507962 -4.3539791 -4.3594503 -4.3552651 -4.3455787 -4.3310995 -4.300396 -4.2634206]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch32/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-adm-0.01-batch32/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 21:14:32.834533: step 20010, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.084 sec/batch; 36h:32m:01s remains)
INFO - root - 2017-12-07 21:14:54.043976: step 20020, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.096 sec/batch; 36h:44m:27s remains)
INFO - root - 2017-12-07 21:15:14.941535: step 20030, loss = 2.08, batch loss = 2.02 (16.5 examples/sec; 1.941 sec/batch; 34h:00m:40s remains)
INFO - root - 2017-12-07 21:15:36.150637: step 20040, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.147 sec/batch; 37h:37m:02s remains)
INFO - root - 2017-12-07 21:15:57.313786: step 20050, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.140 sec/batch; 37h:30m:07s remains)
INFO - root - 2017-12-07 21:16:18.550004: step 20060, loss = 2.08, batch loss = 2.03 (15.3 examples/sec; 2.097 sec/batch; 36h:43m:49s remains)
INFO - root - 2017-12-07 21:16:39.623742: step 20070, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 37h:49m:57s remains)
INFO - root - 2017-12-07 21:17:00.721095: step 20080, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.079 sec/batch; 36h:24m:35s remains)
INFO - root - 2017-12-07 21:17:21.917026: step 20090, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.120 sec/batch; 37h:07m:32s remains)
INFO - root - 2017-12-07 21:17:42.909587: step 20100, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.153 sec/batch; 37h:41m:08s remains)
2017-12-07 21:17:44.511916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3494687 -4.323071 -4.3052444 -4.294785 -4.2707081 -4.2387495 -4.2159219 -4.2032194 -4.212759 -4.24537 -4.279213 -4.3013587 -4.3233576 -4.3449655 -4.3589196][-4.3635139 -4.3414545 -4.323338 -4.3090391 -4.2836542 -4.2522087 -4.2270012 -4.212172 -4.2172952 -4.2448435 -4.2764087 -4.3000135 -4.3229451 -4.3451447 -4.360146][-4.3611078 -4.3398881 -4.3158679 -4.2927723 -4.2646532 -4.2355824 -4.2115831 -4.1957254 -4.2025027 -4.2325745 -4.2678733 -4.2982526 -4.3246078 -4.3466916 -4.3604097][-4.3522825 -4.3301845 -4.303226 -4.2729063 -4.2398663 -4.2064023 -4.1780372 -4.1589112 -4.1656208 -4.2010803 -4.2451034 -4.2860088 -4.3194966 -4.3448815 -4.3589015][-4.3395123 -4.3151793 -4.2854972 -4.2501707 -4.2122707 -4.1697035 -4.1309786 -4.1016107 -4.1038094 -4.1445284 -4.1996346 -4.2532921 -4.2972736 -4.3316646 -4.349967][-4.3272543 -4.2980685 -4.2645397 -4.2261367 -4.181592 -4.1210575 -4.0582924 -4.01464 -4.012352 -4.065094 -4.1394796 -4.2097387 -4.267015 -4.3115144 -4.3368297][-4.3178835 -4.2823081 -4.248487 -4.2058787 -4.1483445 -4.065115 -3.9720082 -3.9092298 -3.9053695 -3.9800935 -4.0760818 -4.1632328 -4.2348933 -4.2909937 -4.3248844][-4.31033 -4.2729144 -4.2390432 -4.1905713 -4.1239886 -4.0299573 -3.9200437 -3.8406062 -3.8423567 -3.9367318 -4.04602 -4.1417532 -4.2178173 -4.2762256 -4.3155456][-4.3099914 -4.27283 -4.2402043 -4.1937423 -4.134707 -4.0594339 -3.9715436 -3.9086514 -3.9175043 -3.9977686 -4.0887742 -4.17072 -4.2357092 -4.2803464 -4.3132181][-4.3180809 -4.2845345 -4.2564869 -4.2222733 -4.1813765 -4.1359415 -4.08576 -4.0528703 -4.0590029 -4.1031256 -4.1572795 -4.2152686 -4.2657957 -4.2959538 -4.3193245][-4.3291698 -4.300014 -4.2777781 -4.2565284 -4.2343869 -4.2076368 -4.1774631 -4.1580362 -4.1578422 -4.1741962 -4.20063 -4.2380981 -4.2802048 -4.3062277 -4.3261786][-4.338346 -4.3145514 -4.293777 -4.2778473 -4.2653575 -4.2442822 -4.2238369 -4.2116494 -4.2090139 -4.2113624 -4.2219491 -4.2447233 -4.2762794 -4.3013153 -4.322763][-4.3401103 -4.3211842 -4.2991061 -4.2822409 -4.2699876 -4.2489262 -4.2327485 -4.2244959 -4.2214737 -4.2180376 -4.2232385 -4.2382097 -4.2640266 -4.2903934 -4.3157759][-4.3249354 -4.3088737 -4.2867727 -4.2689857 -4.25482 -4.2333226 -4.2177892 -4.2105618 -4.2052565 -4.1963463 -4.199379 -4.2142072 -4.2426114 -4.2766805 -4.3101711][-4.3113294 -4.297112 -4.27314 -4.251123 -4.2287946 -4.2024822 -4.1820021 -4.1712313 -4.157639 -4.1426353 -4.1468978 -4.1659226 -4.1998773 -4.2456322 -4.2920232]]...]
INFO - root - 2017-12-07 21:18:05.660606: step 20110, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 37h:20m:08s remains)
INFO - root - 2017-12-07 21:18:26.746253: step 20120, loss = 2.09, batch loss = 2.03 (15.4 examples/sec; 2.080 sec/batch; 36h:24m:08s remains)
INFO - root - 2017-12-07 21:18:47.528223: step 20130, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.098 sec/batch; 36h:42m:14s remains)
INFO - root - 2017-12-07 21:19:08.835076: step 20140, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.179 sec/batch; 38h:07m:39s remains)
INFO - root - 2017-12-07 21:19:30.248623: step 20150, loss = 2.08, batch loss = 2.02 (14.4 examples/sec; 2.217 sec/batch; 38h:47m:06s remains)
INFO - root - 2017-12-07 21:19:51.218646: step 20160, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.101 sec/batch; 36h:44m:50s remains)
INFO - root - 2017-12-07 21:20:12.293417: step 20170, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 37h:13m:09s remains)
INFO - root - 2017-12-07 21:20:33.728957: step 20180, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.197 sec/batch; 38h:25m:02s remains)
INFO - root - 2017-12-07 21:20:54.720393: step 20190, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.060 sec/batch; 36h:00m:44s remains)
INFO - root - 2017-12-07 21:21:15.584740: step 20200, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.115 sec/batch; 36h:58m:04s remains)
2017-12-07 21:21:17.120462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2601562 -4.2467175 -4.2329917 -4.2265258 -4.217762 -4.1882639 -4.1541052 -4.1309109 -4.1160269 -4.1139846 -4.1229019 -4.1552987 -4.1982279 -4.2331986 -4.2616043][-4.2619538 -4.247879 -4.2311749 -4.2229857 -4.2210827 -4.203485 -4.179523 -4.1663542 -4.1547003 -4.1438446 -4.1395383 -4.1638575 -4.2002072 -4.2300353 -4.2577505][-4.2685575 -4.2548351 -4.2299523 -4.2116938 -4.211844 -4.2023525 -4.1863017 -4.1793981 -4.1707439 -4.15895 -4.1553221 -4.1789417 -4.2106214 -4.2364135 -4.260601][-4.2724342 -4.260685 -4.2281151 -4.1995525 -4.195116 -4.1849933 -4.1662731 -4.1592145 -4.1536956 -4.1529031 -4.1653361 -4.1994886 -4.23286 -4.2581716 -4.2779489][-4.2528915 -4.2464213 -4.2159872 -4.18306 -4.1682119 -4.1469364 -4.115252 -4.0971746 -4.0947995 -4.1120844 -4.153151 -4.2075577 -4.2506986 -4.2788172 -4.2957678][-4.2275138 -4.217207 -4.1894364 -4.1567883 -4.1357017 -4.0988631 -4.0459857 -4.00364 -4.00087 -4.0454888 -4.123827 -4.2027979 -4.2557063 -4.2849455 -4.3001728][-4.20347 -4.1878004 -4.1640582 -4.1345057 -4.1085396 -4.0596962 -3.9846792 -3.911006 -3.9005976 -3.9720826 -4.0856957 -4.1862016 -4.24715 -4.2779865 -4.2959514][-4.1758032 -4.1613779 -4.1458559 -4.1225538 -4.0989261 -4.0498695 -3.9705577 -3.8822684 -3.8575873 -3.9381766 -4.0652571 -4.1738925 -4.2387671 -4.2712917 -4.2944436][-4.1484385 -4.1320529 -4.12146 -4.1058855 -4.0970097 -4.0667238 -4.0072446 -3.9373102 -3.9104924 -3.9760773 -4.0858631 -4.1836238 -4.2442636 -4.2762647 -4.3043733][-4.1357727 -4.1147542 -4.1023712 -4.0889025 -4.0953984 -4.0879469 -4.0551968 -4.01511 -3.9997985 -4.0504966 -4.136168 -4.2174296 -4.269949 -4.2966952 -4.3251719][-4.1482983 -4.1268864 -4.1075191 -4.0860891 -4.09667 -4.1058083 -4.0962925 -4.0842633 -4.0823574 -4.1222978 -4.1884866 -4.2532439 -4.2980585 -4.3223691 -4.3496618][-4.1619525 -4.1472898 -4.1244869 -4.095942 -4.0999503 -4.1163864 -4.125051 -4.1360488 -4.1460795 -4.1755514 -4.2230248 -4.2703881 -4.306881 -4.3298163 -4.3575058][-4.1603751 -4.1566405 -4.1377759 -4.1100807 -4.1069827 -4.1223879 -4.1431465 -4.1712508 -4.1905766 -4.2141376 -4.2463884 -4.2752924 -4.2996321 -4.3174396 -4.3442974][-4.1453524 -4.1523662 -4.1426463 -4.1208177 -4.11291 -4.1279206 -4.1560783 -4.1947236 -4.2214932 -4.2420216 -4.2656703 -4.2805715 -4.2900372 -4.2987757 -4.3200283][-4.1287856 -4.1491361 -4.1537495 -4.1399832 -4.1291285 -4.1449251 -4.177896 -4.2190723 -4.2445168 -4.2603369 -4.2764864 -4.2805977 -4.2778077 -4.2781506 -4.2922072]]...]
INFO - root - 2017-12-07 21:21:38.143199: step 20210, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.095 sec/batch; 36h:37m:15s remains)
INFO - root - 2017-12-07 21:21:59.321696: step 20220, loss = 2.11, batch loss = 2.05 (15.1 examples/sec; 2.118 sec/batch; 37h:00m:55s remains)
INFO - root - 2017-12-07 21:22:20.177824: step 20230, loss = 2.09, batch loss = 2.04 (14.8 examples/sec; 2.157 sec/batch; 37h:40m:35s remains)
INFO - root - 2017-12-07 21:22:41.537642: step 20240, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.116 sec/batch; 36h:57m:13s remains)
INFO - root - 2017-12-07 21:23:02.872219: step 20250, loss = 2.06, batch loss = 2.01 (15.3 examples/sec; 2.095 sec/batch; 36h:35m:25s remains)
INFO - root - 2017-12-07 21:23:23.826979: step 20260, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.145 sec/batch; 37h:27m:28s remains)
INFO - root - 2017-12-07 21:23:45.143432: step 20270, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.077 sec/batch; 36h:15m:31s remains)
INFO - root - 2017-12-07 21:24:06.384136: step 20280, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.100 sec/batch; 36h:40m:01s remains)
INFO - root - 2017-12-07 21:24:27.404973: step 20290, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.149 sec/batch; 37h:30m:15s remains)
INFO - root - 2017-12-07 21:24:48.500013: step 20300, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.083 sec/batch; 36h:20m:49s remains)
2017-12-07 21:24:50.083143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3448038 -4.3412204 -4.3351116 -4.3272104 -4.3212395 -4.3197756 -4.3243055 -4.3299227 -4.33448 -4.3358965 -4.3325372 -4.3283939 -4.3253188 -4.3244276 -4.3281589][-4.3360677 -4.328063 -4.3150759 -4.3001418 -4.2916026 -4.2918806 -4.2996721 -4.3071241 -4.3115697 -4.311482 -4.3068104 -4.3023882 -4.30107 -4.3025908 -4.31171][-4.3242135 -4.3091397 -4.2862377 -4.2621746 -4.24795 -4.24598 -4.2528696 -4.2609396 -4.2660789 -4.267509 -4.2648058 -4.2620354 -4.2636342 -4.2674122 -4.2838731][-4.3045163 -4.2806225 -4.2469087 -4.2098804 -4.1833496 -4.1749887 -4.1766849 -4.1832905 -4.1863942 -4.1910992 -4.1976414 -4.2008624 -4.2034187 -4.2085786 -4.2318373][-4.2821555 -4.2487421 -4.2059226 -4.1565804 -4.1138206 -4.0938973 -4.0880804 -4.10103 -4.1107717 -4.1181035 -4.1306453 -4.1403055 -4.1468492 -4.1551714 -4.1820517][-4.2552238 -4.2131205 -4.1648407 -4.1098728 -4.0525322 -4.0173216 -4.0030785 -4.0267243 -4.0493593 -4.0593123 -4.0733404 -4.0923681 -4.1128049 -4.1314292 -4.1645765][-4.2351313 -4.1878881 -4.135148 -4.0758986 -4.013236 -3.9683807 -3.9498808 -3.9759045 -4.0020185 -4.0052204 -4.011488 -4.033957 -4.0694113 -4.1071134 -4.1556907][-4.2317152 -4.1792464 -4.1213541 -4.0579481 -3.9926553 -3.9463172 -3.9250057 -3.9474096 -3.9627128 -3.9503417 -3.9467375 -3.9646533 -4.0100379 -4.0750742 -4.1425252][-4.2344 -4.1783643 -4.113524 -4.044796 -3.9751575 -3.9267879 -3.9045393 -3.9250951 -3.9258175 -3.901552 -3.8976455 -3.9227221 -3.9845836 -4.0704489 -4.14183][-4.2359481 -4.17995 -4.1125369 -4.0456295 -3.982235 -3.9341202 -3.9100685 -3.9302816 -3.9342878 -3.9236956 -3.9430306 -3.983561 -4.0505285 -4.1284537 -4.1794333][-4.2388473 -4.1910315 -4.1377616 -4.08867 -4.0440979 -4.0028639 -3.9756057 -3.9909031 -4.005815 -4.0151772 -4.0465851 -4.0888734 -4.1469307 -4.2079277 -4.2414055][-4.2520385 -4.2200451 -4.1892428 -4.1614213 -4.136044 -4.1014171 -4.0699048 -4.0746756 -4.0909371 -4.1077933 -4.1385083 -4.1699123 -4.2153096 -4.2654467 -4.2933159][-4.2662191 -4.2474537 -4.2310357 -4.2167168 -4.2024856 -4.1759863 -4.1479511 -4.1471939 -4.1633267 -4.1826019 -4.2045736 -4.2210646 -4.2578807 -4.30069 -4.32239][-4.2762556 -4.2654309 -4.2562785 -4.2484021 -4.2402 -4.2227144 -4.2040191 -4.2029538 -4.2209663 -4.2449412 -4.2640758 -4.2730122 -4.2972708 -4.3266106 -4.3399873][-4.2909508 -4.282114 -4.276792 -4.2727427 -4.27019 -4.2619209 -4.2538333 -4.259438 -4.2803588 -4.305522 -4.3202047 -4.32354 -4.3340006 -4.349195 -4.3555388]]...]
INFO - root - 2017-12-07 21:25:11.391755: step 20310, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 36h:37m:13s remains)
INFO - root - 2017-12-07 21:25:32.309598: step 20320, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.110 sec/batch; 36h:48m:20s remains)
INFO - root - 2017-12-07 21:25:53.528259: step 20330, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.132 sec/batch; 37h:11m:44s remains)
INFO - root - 2017-12-07 21:26:14.788313: step 20340, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.184 sec/batch; 38h:05m:09s remains)
INFO - root - 2017-12-07 21:26:36.075782: step 20350, loss = 2.06, batch loss = 2.01 (16.2 examples/sec; 1.970 sec/batch; 34h:20m:48s remains)
INFO - root - 2017-12-07 21:26:57.129349: step 20360, loss = 2.06, batch loss = 2.00 (14.6 examples/sec; 2.194 sec/batch; 38h:14m:40s remains)
INFO - root - 2017-12-07 21:27:18.191358: step 20370, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.101 sec/batch; 36h:37m:11s remains)
INFO - root - 2017-12-07 21:27:39.517111: step 20380, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 36h:56m:49s remains)
INFO - root - 2017-12-07 21:28:00.443624: step 20390, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.131 sec/batch; 37h:08m:01s remains)
INFO - root - 2017-12-07 21:28:21.674800: step 20400, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.081 sec/batch; 36h:15m:05s remains)
2017-12-07 21:28:23.421220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1928926 -4.2026048 -4.2091713 -4.2068658 -4.2010674 -4.2034264 -4.2151122 -4.2261887 -4.2339439 -4.2382817 -4.2435818 -4.2433319 -4.230443 -4.20758 -4.1929708][-4.1848712 -4.1924491 -4.2003427 -4.2016377 -4.1985526 -4.2017789 -4.2152328 -4.2289176 -4.2392511 -4.2455373 -4.2517953 -4.2512794 -4.235744 -4.2087727 -4.1895814][-4.1868935 -4.1896806 -4.1947494 -4.1917291 -4.1838608 -4.1820912 -4.1949377 -4.2129221 -4.2276392 -4.2383828 -4.2470579 -4.2479753 -4.2300658 -4.2006807 -4.1763473][-4.1898985 -4.1874352 -4.1871567 -4.1762319 -4.1565533 -4.1465778 -4.1555581 -4.1734371 -4.1897674 -4.2031503 -4.2135663 -4.2179103 -4.20313 -4.1752162 -4.1505308][-4.1822777 -4.1755543 -4.171577 -4.1533937 -4.125185 -4.1048894 -4.1046667 -4.1153765 -4.1274142 -4.1394577 -4.1537223 -4.1690969 -4.1656232 -4.1473942 -4.1293058][-4.1741877 -4.1614909 -4.1493177 -4.1218753 -4.0863247 -4.0596662 -4.0550356 -4.0624914 -4.0738931 -4.0867777 -4.1061668 -4.1303015 -4.1358142 -4.1286597 -4.1193705][-4.1782055 -4.1590924 -4.1366906 -4.1003428 -4.0548358 -4.0222135 -4.0188036 -4.0321302 -4.0528779 -4.0742588 -4.0983591 -4.1223626 -4.1265869 -4.1263542 -4.1243362][-4.1787586 -4.1565766 -4.1315594 -4.0908346 -4.0388842 -4.0071731 -4.0101914 -4.0314364 -4.0603733 -4.0906558 -4.1172009 -4.1392837 -4.1461487 -4.1533737 -4.158989][-4.1611772 -4.1425514 -4.1230555 -4.0891562 -4.0447392 -4.0233107 -4.0365515 -4.060833 -4.0883307 -4.117137 -4.1415329 -4.1609588 -4.1724644 -4.1867805 -4.1967769][-4.143743 -4.1318192 -4.1204257 -4.098628 -4.0677152 -4.0586443 -4.0775704 -4.0975213 -4.1163292 -4.138742 -4.158102 -4.172545 -4.1823373 -4.1953015 -4.2023854][-4.1481276 -4.1425838 -4.1326656 -4.1164813 -4.0981789 -4.0975885 -4.1139388 -4.1242428 -4.1297331 -4.1417689 -4.1556463 -4.1654272 -4.1702313 -4.1754541 -4.1768022][-4.1825542 -4.1808109 -4.1672645 -4.1513991 -4.1411395 -4.1421261 -4.1517458 -4.1533728 -4.1481819 -4.1485963 -4.1551571 -4.1600823 -4.1609435 -4.1618261 -4.1622024][-4.2288 -4.2269316 -4.2124829 -4.19851 -4.1930156 -4.1915765 -4.1935167 -4.192327 -4.1847563 -4.1782928 -4.1762991 -4.1766043 -4.1767583 -4.177464 -4.1780572][-4.2613726 -4.25914 -4.2477374 -4.2382593 -4.2365766 -4.2334571 -4.2300091 -4.2285333 -4.22573 -4.2192183 -4.2135854 -4.2127585 -4.2133169 -4.2144704 -4.2157736][-4.2787342 -4.2760348 -4.2678089 -4.2627797 -4.2643628 -4.2624297 -4.2587233 -4.2597189 -4.2632408 -4.261889 -4.2592034 -4.2604375 -4.2602873 -4.2611961 -4.2630472]]...]
INFO - root - 2017-12-07 21:28:44.633816: step 20410, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.131 sec/batch; 37h:07m:36s remains)
INFO - root - 2017-12-07 21:29:05.381112: step 20420, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.093 sec/batch; 36h:27m:36s remains)
INFO - root - 2017-12-07 21:29:26.729506: step 20430, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.117 sec/batch; 36h:52m:27s remains)
INFO - root - 2017-12-07 21:29:48.081723: step 20440, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 36h:53m:55s remains)
INFO - root - 2017-12-07 21:30:08.977017: step 20450, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.069 sec/batch; 36h:01m:18s remains)
INFO - root - 2017-12-07 21:30:30.214225: step 20460, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.086 sec/batch; 36h:18m:38s remains)
INFO - root - 2017-12-07 21:30:51.512959: step 20470, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.187 sec/batch; 38h:03m:49s remains)
INFO - root - 2017-12-07 21:31:12.370156: step 20480, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.161 sec/batch; 37h:35m:54s remains)
INFO - root - 2017-12-07 21:31:33.428130: step 20490, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.124 sec/batch; 36h:57m:24s remains)
INFO - root - 2017-12-07 21:31:54.812768: step 20500, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 2.200 sec/batch; 38h:16m:40s remains)
2017-12-07 21:31:56.439895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.34609 -4.3495884 -4.3505259 -4.3483071 -4.3459883 -4.3465929 -4.348937 -4.3504491 -4.3510771 -4.3505673 -4.3494773 -4.3462744 -4.3434057 -4.3427458 -4.3441267][-4.3291693 -4.3350754 -4.3358455 -4.33239 -4.3307595 -4.3331776 -4.33747 -4.33955 -4.3394737 -4.3364639 -4.3323236 -4.3272276 -4.3257337 -4.3301826 -4.3357587][-4.2822123 -4.2900729 -4.2902122 -4.2867475 -4.2864785 -4.2924471 -4.2987995 -4.30078 -4.3007007 -4.2965579 -4.2891712 -4.2812347 -4.2843986 -4.29719 -4.3099041][-4.1956062 -4.2061372 -4.2068357 -4.2058229 -4.2070065 -4.2135878 -4.2193203 -4.2206011 -4.2232661 -4.221591 -4.2140565 -4.2053742 -4.2171359 -4.2413411 -4.264091][-4.075788 -4.087616 -4.0899391 -4.0918746 -4.0923285 -4.0993 -4.1035118 -4.1036124 -4.1093307 -4.109458 -4.1016574 -4.0911169 -4.111927 -4.155241 -4.19495][-3.9636207 -3.9743967 -3.977144 -3.9800534 -3.9764006 -3.9823253 -3.9856355 -3.983613 -3.9910889 -3.99243 -3.9853435 -3.9719918 -4.0021448 -4.0673094 -4.1295915][-3.9398184 -3.9462597 -3.9482086 -3.9477568 -3.9400744 -3.9413536 -3.9393625 -3.929358 -3.9335237 -3.934433 -3.9292688 -3.9195223 -3.9559083 -4.030757 -4.1017237][-4.0025706 -4.004283 -4.0069966 -4.0091105 -4.001318 -3.9938557 -3.9853063 -3.9675984 -3.9609134 -3.95731 -3.9546108 -3.9554868 -3.9927542 -4.05911 -4.1209416][-4.0749388 -4.0755725 -4.0841045 -4.09328 -4.0899205 -4.0754952 -4.0594544 -4.0354605 -4.0196681 -4.0128531 -4.0115609 -4.0159121 -4.0464249 -4.0998015 -4.1513925][-4.1412663 -4.1389618 -4.1476827 -4.1633768 -4.167171 -4.1507974 -4.1281328 -4.098804 -4.079093 -4.0653481 -4.0551081 -4.058434 -4.0890632 -4.1342773 -4.1786575][-4.1868062 -4.1791306 -4.1852384 -4.2011013 -4.2112894 -4.1996903 -4.1751709 -4.1448188 -4.1206479 -4.0946794 -4.0668159 -4.064446 -4.1000791 -4.1497912 -4.1911955][-4.2080741 -4.1944957 -4.1955051 -4.20915 -4.2228265 -4.2187438 -4.1934295 -4.1574688 -4.1289163 -4.0977216 -4.0554438 -4.0445 -4.0858116 -4.1455045 -4.19271][-4.1947722 -4.1751671 -4.177505 -4.1961741 -4.2158117 -4.2209034 -4.1995792 -4.1646767 -4.139637 -4.1097813 -4.0633373 -4.045166 -4.0829573 -4.1452565 -4.1964555][-4.179616 -4.1544681 -4.1537509 -4.1742787 -4.2003441 -4.2150846 -4.2020683 -4.1815009 -4.1689062 -4.1519527 -4.1172915 -4.0962505 -4.1224227 -4.1704607 -4.2141886][-4.1780114 -4.1615829 -4.1616907 -4.1770039 -4.1978807 -4.2103024 -4.1990652 -4.186379 -4.1854467 -4.184062 -4.1674809 -4.1495247 -4.1645255 -4.200211 -4.2334266]]...]
INFO - root - 2017-12-07 21:32:17.700932: step 20510, loss = 2.06, batch loss = 2.01 (15.4 examples/sec; 2.078 sec/batch; 36h:08m:41s remains)
INFO - root - 2017-12-07 21:32:38.810315: step 20520, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.166 sec/batch; 37h:39m:45s remains)
INFO - root - 2017-12-07 21:33:00.096569: step 20530, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.091 sec/batch; 36h:20m:57s remains)
INFO - root - 2017-12-07 21:33:21.557105: step 20540, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 36h:32m:57s remains)
INFO - root - 2017-12-07 21:33:42.400403: step 20550, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.099 sec/batch; 36h:29m:23s remains)
INFO - root - 2017-12-07 21:34:03.777184: step 20560, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 37h:08m:49s remains)
INFO - root - 2017-12-07 21:34:24.990817: step 20570, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.170 sec/batch; 37h:42m:22s remains)
INFO - root - 2017-12-07 21:34:46.028148: step 20580, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.121 sec/batch; 36h:50m:50s remains)
INFO - root - 2017-12-07 21:35:07.287308: step 20590, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 36h:27m:39s remains)
INFO - root - 2017-12-07 21:35:28.450275: step 20600, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.140 sec/batch; 37h:10m:29s remains)
2017-12-07 21:35:30.048508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2219014 -4.2058105 -4.1761 -4.1466289 -4.1328306 -4.140172 -4.1613183 -4.1885457 -4.2133965 -4.2301769 -4.2374611 -4.2407513 -4.2438731 -4.2394743 -4.2338095][-4.2149954 -4.2087369 -4.19227 -4.1768756 -4.1725607 -4.1874013 -4.2051768 -4.2195044 -4.2264075 -4.2266297 -4.2202373 -4.21762 -4.2283063 -4.2388706 -4.247395][-4.2083888 -4.2170572 -4.2127767 -4.204412 -4.2055678 -4.2237124 -4.2345266 -4.2322927 -4.2167697 -4.1930823 -4.1728458 -4.1710486 -4.1980653 -4.2297287 -4.2525325][-4.1998935 -4.21761 -4.2135844 -4.1985922 -4.1920643 -4.2044239 -4.2071557 -4.1944833 -4.1631227 -4.1212745 -4.0965438 -4.1055913 -4.1507592 -4.2022915 -4.2365193][-4.2049313 -4.2153964 -4.1924582 -4.1548882 -4.1264153 -4.1239638 -4.1235895 -4.1141915 -4.0858035 -4.0424795 -4.0253534 -4.0541267 -4.116117 -4.1761503 -4.2091303][-4.2210693 -4.2061286 -4.147593 -4.07803 -4.0225372 -4.0007067 -4.003653 -4.0125728 -4.0038548 -3.9843283 -3.9964397 -4.0552287 -4.127914 -4.1749821 -4.1831017][-4.2338 -4.1954622 -4.1159558 -4.0263753 -3.9538224 -3.9214084 -3.9311998 -3.9545841 -3.9703259 -3.9837649 -4.0256143 -4.1075215 -4.176827 -4.1948633 -4.165801][-4.24895 -4.205091 -4.1337581 -4.0594497 -4.007103 -3.9905944 -4.0035205 -4.022686 -4.0415978 -4.0639262 -4.1093731 -4.1816721 -4.2273989 -4.2186866 -4.1680675][-4.2561078 -4.2219505 -4.1748395 -4.1315451 -4.1087704 -4.1078768 -4.1160717 -4.1264749 -4.1431341 -4.1634779 -4.1973057 -4.24297 -4.2645025 -4.2441783 -4.1960034][-4.2615542 -4.2412963 -4.2123718 -4.186677 -4.1783524 -4.1818132 -4.18647 -4.1969962 -4.2176204 -4.2384329 -4.2609711 -4.2823052 -4.2834206 -4.2569132 -4.2208123][-4.2704935 -4.2600322 -4.240921 -4.2219405 -4.2172737 -4.2196512 -4.2223959 -4.2345047 -4.2574315 -4.27872 -4.2909212 -4.292737 -4.2788868 -4.249897 -4.228858][-4.2704163 -4.2653141 -4.249342 -4.2322369 -4.2308087 -4.2386942 -4.2470746 -4.2617388 -4.2848759 -4.3013425 -4.2990961 -4.2840734 -4.2599564 -4.2332525 -4.22779][-4.2520185 -4.2486796 -4.2340765 -4.2180009 -4.2216697 -4.2404909 -4.2615585 -4.2832103 -4.3047752 -4.3133626 -4.2964115 -4.2674456 -4.2378974 -4.2176595 -4.2264733][-4.2244658 -4.2202358 -4.2053246 -4.1931562 -4.2042947 -4.2338529 -4.266521 -4.292459 -4.309299 -4.3094711 -4.2872748 -4.2581153 -4.2370515 -4.2286563 -4.2451458][-4.2058158 -4.1983376 -4.1850734 -4.1785932 -4.1938834 -4.2260385 -4.2638884 -4.2894573 -4.29857 -4.292449 -4.2725282 -4.255095 -4.2504511 -4.2546644 -4.2719059]]...]
INFO - root - 2017-12-07 21:35:50.930544: step 20610, loss = 2.06, batch loss = 2.01 (15.2 examples/sec; 2.101 sec/batch; 36h:28m:56s remains)
INFO - root - 2017-12-07 21:36:12.091725: step 20620, loss = 2.06, batch loss = 2.01 (15.2 examples/sec; 2.101 sec/batch; 36h:28m:25s remains)
INFO - root - 2017-12-07 21:36:33.204915: step 20630, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.107 sec/batch; 36h:34m:55s remains)
INFO - root - 2017-12-07 21:36:54.048593: step 20640, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.104 sec/batch; 36h:30m:44s remains)
INFO - root - 2017-12-07 21:37:14.992731: step 20650, loss = 2.06, batch loss = 2.00 (15.7 examples/sec; 2.045 sec/batch; 35h:28m:59s remains)
INFO - root - 2017-12-07 21:37:36.227690: step 20660, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.111 sec/batch; 36h:37m:28s remains)
INFO - root - 2017-12-07 21:37:57.470192: step 20670, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 36h:51m:40s remains)
INFO - root - 2017-12-07 21:38:18.554071: step 20680, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 36h:19m:24s remains)
INFO - root - 2017-12-07 21:38:39.822398: step 20690, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.092 sec/batch; 36h:16m:56s remains)
INFO - root - 2017-12-07 21:39:00.979449: step 20700, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.139 sec/batch; 37h:05m:52s remains)
2017-12-07 21:39:02.558338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2998466 -4.3033533 -4.3054414 -4.3065481 -4.3042488 -4.303534 -4.3043318 -4.3075867 -4.3136725 -4.3202314 -4.3229394 -4.3204007 -4.3166223 -4.3137045 -4.3097372][-4.3101873 -4.3120556 -4.3113127 -4.3083925 -4.2994256 -4.2910624 -4.285913 -4.2872167 -4.2970877 -4.3101816 -4.3175263 -4.318634 -4.3187361 -4.3191137 -4.3175321][-4.3162446 -4.3154225 -4.3088708 -4.2994919 -4.2825623 -4.2644043 -4.2504392 -4.25093 -4.26524 -4.2865276 -4.3005543 -4.306572 -4.3125296 -4.3177261 -4.3198147][-4.3072486 -4.3054295 -4.292357 -4.2730393 -4.2451849 -4.2159863 -4.1907225 -4.1873584 -4.2053318 -4.2347565 -4.258707 -4.2741857 -4.2912745 -4.3052597 -4.3121][-4.2804551 -4.2758274 -4.2550621 -4.2228622 -4.1834846 -4.1416249 -4.1054931 -4.0918617 -4.1095023 -4.1497064 -4.1880851 -4.218421 -4.2505465 -4.2744045 -4.2883453][-4.2497253 -4.2363687 -4.2056146 -4.1607261 -4.1118383 -4.0549583 -3.9954889 -3.9539187 -3.9638968 -4.026443 -4.0912819 -4.144515 -4.1952324 -4.2288208 -4.2530417][-4.2269659 -4.196362 -4.1441364 -4.0793958 -4.0185103 -3.9421806 -3.8525362 -3.761003 -3.7557883 -3.858882 -3.9638197 -4.0452995 -4.1169872 -4.1656966 -4.2065883][-4.2241006 -4.17225 -4.0962257 -4.0130911 -3.9422519 -3.8573408 -3.7511342 -3.6237745 -3.6072426 -3.7375884 -3.8584075 -3.9496553 -4.0303612 -4.0933485 -4.1534739][-4.2390609 -4.1767616 -4.0906549 -4.006968 -3.9466522 -3.8825743 -3.8097756 -3.720983 -3.6977103 -3.7766395 -3.8533964 -3.9226441 -3.9958065 -4.0685205 -4.1406093][-4.2523766 -4.1958375 -4.1235523 -4.0619783 -4.0267339 -3.9967644 -3.9674437 -3.9259722 -3.895025 -3.911783 -3.9396672 -3.9825315 -4.0382228 -4.1011424 -4.1642647][-4.2525463 -4.214251 -4.1659331 -4.1310697 -4.1195922 -4.11341 -4.1061826 -4.0888 -4.0579753 -4.0444026 -4.0478134 -4.0706987 -4.1048341 -4.143826 -4.1836286][-4.2351694 -4.2171206 -4.1899261 -4.1737127 -4.175859 -4.1802187 -4.1794729 -4.1724968 -4.1512022 -4.1328068 -4.1242967 -4.1309071 -4.1434011 -4.1581469 -4.1776495][-4.2183223 -4.2140789 -4.2014122 -4.196228 -4.20328 -4.210968 -4.2123504 -4.2105913 -4.1988635 -4.1826391 -4.1701374 -4.1666775 -4.1671791 -4.1694541 -4.1769013][-4.2286925 -4.2284985 -4.2212176 -4.2190509 -4.2247496 -4.2309756 -4.2341905 -4.2357025 -4.2320442 -4.222609 -4.2121534 -4.205492 -4.2017179 -4.1995974 -4.2019157][-4.271111 -4.271224 -4.2664027 -4.2637749 -4.2656879 -4.2688293 -4.2719784 -4.2748718 -4.2757306 -4.2723579 -4.2669892 -4.2618318 -4.2575083 -4.2548366 -4.2551584]]...]
INFO - root - 2017-12-07 21:39:23.341795: step 20710, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 36h:26m:41s remains)
INFO - root - 2017-12-07 21:39:44.414312: step 20720, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.114 sec/batch; 36h:39m:00s remains)
INFO - root - 2017-12-07 21:40:05.455547: step 20730, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 37h:00m:15s remains)
INFO - root - 2017-12-07 21:40:26.395334: step 20740, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 36h:54m:54s remains)
INFO - root - 2017-12-07 21:40:47.619873: step 20750, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.115 sec/batch; 36h:38m:37s remains)
INFO - root - 2017-12-07 21:41:08.834595: step 20760, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.086 sec/batch; 36h:08m:29s remains)
INFO - root - 2017-12-07 21:41:29.420886: step 20770, loss = 2.09, batch loss = 2.03 (16.6 examples/sec; 1.925 sec/batch; 33h:20m:52s remains)
INFO - root - 2017-12-07 21:41:50.630955: step 20780, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.152 sec/batch; 37h:16m:15s remains)
INFO - root - 2017-12-07 21:42:12.048564: step 20790, loss = 2.07, batch loss = 2.02 (14.6 examples/sec; 2.195 sec/batch; 38h:00m:08s remains)
INFO - root - 2017-12-07 21:42:33.241149: step 20800, loss = 2.08, batch loss = 2.02 (16.0 examples/sec; 2.000 sec/batch; 34h:37m:45s remains)
2017-12-07 21:42:34.783789: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2604837 -4.2656126 -4.2757115 -4.2846837 -4.2915025 -4.2934613 -4.2905917 -4.27939 -4.2725415 -4.2704945 -4.2683835 -4.2594271 -4.249352 -4.258678 -4.2797956][-4.2492442 -4.252028 -4.263793 -4.277698 -4.2889795 -4.29227 -4.28694 -4.2713838 -4.2615032 -4.2581196 -4.2578468 -4.2534809 -4.2463694 -4.2573442 -4.2784829][-4.2541285 -4.2580576 -4.2686024 -4.2793317 -4.2864423 -4.284317 -4.2735453 -4.252439 -4.2401915 -4.2387123 -4.2431226 -4.2479153 -4.2468357 -4.2586164 -4.2785525][-4.2597275 -4.2651668 -4.2729516 -4.2767816 -4.2740817 -4.2616906 -4.2420111 -4.2190905 -4.2105422 -4.2143488 -4.2273841 -4.2422843 -4.2491341 -4.2614961 -4.2784061][-4.2509146 -4.2567368 -4.2612486 -4.2575274 -4.2431602 -4.2180009 -4.18741 -4.1643662 -4.1647038 -4.1775589 -4.2001123 -4.227046 -4.245316 -4.2601094 -4.274097][-4.2337294 -4.2384 -4.2363868 -4.2226114 -4.1975555 -4.1574006 -4.1121087 -4.0859041 -4.1010113 -4.12834 -4.16115 -4.1982193 -4.2294064 -4.2495713 -4.2639971][-4.2221169 -4.2166142 -4.2011981 -4.1750336 -4.1373782 -4.0811949 -4.0130143 -3.9764252 -4.0083766 -4.0584488 -4.1086397 -4.1565552 -4.1982436 -4.2286296 -4.2509909][-4.2050166 -4.18801 -4.1672611 -4.1386461 -4.09739 -4.0299568 -3.9381151 -3.884681 -3.9287004 -3.9982588 -4.06117 -4.119246 -4.1703548 -4.2117271 -4.244051][-4.1873622 -4.171659 -4.1602292 -4.1505809 -4.1287427 -4.0835581 -4.013072 -3.967063 -3.9944649 -4.0440097 -4.0914297 -4.1389894 -4.18213 -4.21915 -4.2520781][-4.1753764 -4.1744442 -4.1815844 -4.1905193 -4.1913037 -4.1758447 -4.1399617 -4.1077304 -4.1177273 -4.1462364 -4.1733437 -4.1987171 -4.2206469 -4.2434697 -4.26765][-4.1824951 -4.1932077 -4.2091789 -4.2246995 -4.2353225 -4.238349 -4.2233214 -4.2021914 -4.2041931 -4.2237959 -4.2392116 -4.2482796 -4.2547569 -4.26822 -4.2856784][-4.2099795 -4.2220697 -4.2333388 -4.2460389 -4.2585564 -4.2660308 -4.2588854 -4.243576 -4.2441592 -4.2590518 -4.2657428 -4.2637014 -4.2628179 -4.2752395 -4.2917209][-4.242352 -4.2549324 -4.2630582 -4.2721729 -4.2821207 -4.2869377 -4.2785869 -4.2650762 -4.2644677 -4.2754197 -4.2752166 -4.2680874 -4.2648287 -4.27632 -4.29225][-4.2586 -4.2718024 -4.2807469 -4.288064 -4.2953234 -4.2984996 -4.289578 -4.2773461 -4.2732 -4.27935 -4.2767043 -4.2688236 -4.2662606 -4.27848 -4.2949567][-4.2473607 -4.2618771 -4.2715731 -4.2777224 -4.2876096 -4.2954121 -4.2910147 -4.2832456 -4.2799592 -4.2839017 -4.2818642 -4.2743387 -4.2706985 -4.2823267 -4.299727]]...]
INFO - root - 2017-12-07 21:42:55.821369: step 20810, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.136 sec/batch; 36h:58m:11s remains)
INFO - root - 2017-12-07 21:43:16.951649: step 20820, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.096 sec/batch; 36h:16m:40s remains)
INFO - root - 2017-12-07 21:43:38.340572: step 20830, loss = 2.08, batch loss = 2.02 (14.5 examples/sec; 2.206 sec/batch; 38h:10m:42s remains)
INFO - root - 2017-12-07 21:43:59.159749: step 20840, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.127 sec/batch; 36h:47m:46s remains)
INFO - root - 2017-12-07 21:44:20.348466: step 20850, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 36h:38m:57s remains)
INFO - root - 2017-12-07 21:44:41.573710: step 20860, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.142 sec/batch; 37h:02m:27s remains)
INFO - root - 2017-12-07 21:45:02.170716: step 20870, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.102 sec/batch; 36h:20m:57s remains)
INFO - root - 2017-12-07 21:45:23.512312: step 20880, loss = 2.07, batch loss = 2.01 (14.4 examples/sec; 2.220 sec/batch; 38h:22m:59s remains)
INFO - root - 2017-12-07 21:45:44.696165: step 20890, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.186 sec/batch; 37h:47m:39s remains)
INFO - root - 2017-12-07 21:46:05.803942: step 20900, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.158 sec/batch; 37h:18m:28s remains)
2017-12-07 21:46:07.340855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3388529 -4.3478069 -4.351624 -4.3527083 -4.3499794 -4.3325067 -4.2822814 -4.2030015 -4.1101294 -4.0640306 -4.091404 -4.1583467 -4.2314029 -4.2964168 -4.3458834][-4.324151 -4.3416271 -4.3527751 -4.3573918 -4.3504791 -4.3264575 -4.2677469 -4.177865 -4.0792007 -4.0374279 -4.0715113 -4.1418834 -4.2208428 -4.2939377 -4.3539319][-4.2892933 -4.3169703 -4.3384666 -4.3475924 -4.3417621 -4.31577 -4.25294 -4.1540627 -4.0524793 -4.0170379 -4.0580254 -4.1320429 -4.2143216 -4.2946038 -4.3619947][-4.24684 -4.2873249 -4.3232889 -4.339076 -4.3304176 -4.2965064 -4.2233095 -4.1141577 -4.015244 -3.9971609 -4.0519376 -4.1312919 -4.2163563 -4.3021755 -4.3713751][-4.2157936 -4.268754 -4.3177996 -4.3373518 -4.319222 -4.2667093 -4.1768651 -4.0597463 -3.9716942 -3.978225 -4.0489922 -4.1351895 -4.2257028 -4.3142643 -4.3814888][-4.1973858 -4.2638636 -4.3236251 -4.343492 -4.3138237 -4.241045 -4.1339293 -4.008759 -3.9311857 -3.9606447 -4.0461082 -4.1400852 -4.2357349 -4.3243656 -4.38744][-4.1903024 -4.2656512 -4.3293915 -4.3477168 -4.3110294 -4.2260075 -4.1094503 -3.9810109 -3.9100454 -3.9470336 -4.0366468 -4.1381435 -4.2413621 -4.3302131 -4.3890834][-4.1866035 -4.2643547 -4.3255982 -4.3422141 -4.3064208 -4.224195 -4.1123447 -3.9914548 -3.9222269 -3.9443734 -4.0224595 -4.12766 -4.239789 -4.3301444 -4.3834672][-4.1848578 -4.2615438 -4.3189077 -4.3367562 -4.3085928 -4.2391891 -4.1427493 -4.0364285 -3.9650264 -3.962678 -4.0193572 -4.1213555 -4.23871 -4.329392 -4.3760409][-4.191834 -4.2655783 -4.3179083 -4.3365383 -4.3150835 -4.2598257 -4.1805224 -4.0897603 -4.0203695 -4.0031519 -4.0433826 -4.1381083 -4.2507944 -4.3349867 -4.3713307][-4.2005711 -4.2722239 -4.321847 -4.3412371 -4.3263583 -4.2829857 -4.2173405 -4.1429429 -4.0844269 -4.0670829 -4.1007056 -4.1842542 -4.2800884 -4.3458204 -4.3650513][-4.2008319 -4.2743735 -4.3264408 -4.3486714 -4.3406553 -4.3070807 -4.254725 -4.1980834 -4.1545906 -4.1446376 -4.1757607 -4.2446761 -4.3154035 -4.3561945 -4.3545027][-4.1959319 -4.2707887 -4.3263488 -4.3506722 -4.3474526 -4.3190813 -4.2762547 -4.233696 -4.2051229 -4.2042422 -4.2361007 -4.2918587 -4.338428 -4.3568869 -4.3392372][-4.1882663 -4.2616229 -4.3183117 -4.3430719 -4.33924 -4.31177 -4.274991 -4.2416496 -4.223012 -4.2281723 -4.2631321 -4.3109965 -4.3416018 -4.34752 -4.3237357][-4.1834412 -4.2518368 -4.304276 -4.326489 -4.3195477 -4.29528 -4.2662659 -4.2395239 -4.2244811 -4.232347 -4.267539 -4.3093429 -4.3324533 -4.3345261 -4.3110785]]...]
INFO - root - 2017-12-07 21:46:28.576132: step 20910, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.155 sec/batch; 37h:15m:02s remains)
INFO - root - 2017-12-07 21:46:49.787147: step 20920, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.177 sec/batch; 37h:37m:09s remains)
INFO - root - 2017-12-07 21:47:10.505467: step 20930, loss = 2.07, batch loss = 2.01 (16.4 examples/sec; 1.950 sec/batch; 33h:41m:07s remains)
INFO - root - 2017-12-07 21:47:31.502082: step 20940, loss = 2.08, batch loss = 2.03 (15.3 examples/sec; 2.097 sec/batch; 36h:13m:08s remains)
INFO - root - 2017-12-07 21:47:52.710342: step 20950, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.156 sec/batch; 37h:13m:51s remains)
INFO - root - 2017-12-07 21:48:13.827614: step 20960, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.134 sec/batch; 36h:51m:00s remains)
INFO - root - 2017-12-07 21:48:34.746723: step 20970, loss = 2.07, batch loss = 2.02 (15.6 examples/sec; 2.053 sec/batch; 35h:26m:25s remains)
INFO - root - 2017-12-07 21:48:56.001159: step 20980, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.095 sec/batch; 36h:10m:05s remains)
INFO - root - 2017-12-07 21:49:17.137215: step 20990, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 36h:51m:22s remains)
INFO - root - 2017-12-07 21:49:37.993346: step 21000, loss = 2.08, batch loss = 2.03 (15.5 examples/sec; 2.071 sec/batch; 35h:44m:28s remains)
2017-12-07 21:49:39.528012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3133759 -4.3087015 -4.3044434 -4.3041344 -4.308074 -4.3164778 -4.3240519 -4.3281641 -4.32878 -4.3267183 -4.3246832 -4.3203945 -4.3175945 -4.3213754 -4.328557][-4.2834744 -4.2741752 -4.2667408 -4.2670603 -4.2743673 -4.2854276 -4.2922893 -4.2977304 -4.3005614 -4.3006616 -4.3026605 -4.3025508 -4.3008561 -4.3033547 -4.311131][-4.2508974 -4.2359524 -4.22117 -4.2163897 -4.2228761 -4.234838 -4.2432218 -4.2528996 -4.2605767 -4.26535 -4.2718487 -4.2733197 -4.274395 -4.2790127 -4.2882895][-4.2383213 -4.2170153 -4.191371 -4.1711464 -4.1630583 -4.1679926 -4.1799397 -4.2007122 -4.2221332 -4.2416019 -4.2540259 -4.2555346 -4.2601295 -4.2685332 -4.2753282][-4.2373719 -4.2102246 -4.1754117 -4.1360188 -4.0993433 -4.0810828 -4.0912943 -4.1261921 -4.1634016 -4.2016077 -4.2267652 -4.2337155 -4.2442513 -4.2583675 -4.2674694][-4.2311339 -4.20306 -4.1650977 -4.1149721 -4.0536103 -4.0019722 -3.9957612 -4.0340319 -4.0816216 -4.1334052 -4.1702485 -4.1889958 -4.2118406 -4.2364888 -4.254416][-4.2210689 -4.2002611 -4.1680584 -4.1187677 -4.0504761 -3.9787681 -3.9462662 -3.9575663 -3.9987178 -4.059402 -4.1089697 -4.1432014 -4.1808758 -4.2184768 -4.2460651][-4.2054944 -4.1933308 -4.1729321 -4.1351018 -4.0749612 -4.0060453 -3.9575 -3.9384894 -3.9557257 -4.0107656 -4.0710378 -4.1221237 -4.1679311 -4.2131209 -4.2470231][-4.1989121 -4.1884294 -4.1798177 -4.1593266 -4.1172361 -4.0657768 -4.0212212 -3.9889128 -3.9823849 -4.0119843 -4.0639377 -4.1196437 -4.1667142 -4.2133207 -4.2491021][-4.1991339 -4.1897182 -4.1893215 -4.1843567 -4.1642232 -4.1371036 -4.1079273 -4.0725851 -4.0491142 -4.0530705 -4.0899096 -4.1418486 -4.1880875 -4.2308149 -4.2627959][-4.2074409 -4.1971464 -4.1954808 -4.1963925 -4.1907439 -4.1828427 -4.170495 -4.1431308 -4.114151 -4.105001 -4.13051 -4.1748419 -4.2159128 -4.2534261 -4.2813392][-4.2252145 -4.2144608 -4.2085891 -4.2061453 -4.2044153 -4.2077427 -4.206903 -4.1899366 -4.1640635 -4.149518 -4.1634359 -4.1974006 -4.2329769 -4.2668939 -4.2920527][-4.2404776 -4.2292271 -4.2207894 -4.217855 -4.2179461 -4.2254481 -4.2299609 -4.2214313 -4.20406 -4.19171 -4.1986275 -4.2215457 -4.2495632 -4.2791047 -4.3015575][-4.2635207 -4.2529106 -4.2448936 -4.2418885 -4.2423639 -4.249505 -4.2555742 -4.253284 -4.2434139 -4.2346258 -4.2370543 -4.2493978 -4.2686887 -4.2903376 -4.3087921][-4.2926807 -4.2874489 -4.2821283 -4.2770615 -4.2741246 -4.2756104 -4.2785015 -4.2776337 -4.2735257 -4.2713037 -4.2723851 -4.276803 -4.28717 -4.3010631 -4.3139338]]...]
INFO - root - 2017-12-07 21:50:00.676710: step 21010, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 36h:39m:51s remains)
INFO - root - 2017-12-07 21:50:21.869363: step 21020, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.151 sec/batch; 37h:06m:11s remains)
INFO - root - 2017-12-07 21:50:42.398672: step 21030, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.078 sec/batch; 35h:50m:30s remains)
INFO - root - 2017-12-07 21:51:03.505965: step 21040, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.111 sec/batch; 36h:24m:34s remains)
INFO - root - 2017-12-07 21:51:24.754410: step 21050, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.151 sec/batch; 37h:04m:56s remains)
INFO - root - 2017-12-07 21:51:45.728143: step 21060, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.091 sec/batch; 36h:02m:38s remains)
INFO - root - 2017-12-07 21:52:06.915538: step 21070, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 36h:15m:06s remains)
INFO - root - 2017-12-07 21:52:28.203404: step 21080, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.106 sec/batch; 36h:18m:05s remains)
INFO - root - 2017-12-07 21:52:49.299230: step 21090, loss = 2.09, batch loss = 2.03 (16.0 examples/sec; 1.997 sec/batch; 34h:24m:49s remains)
INFO - root - 2017-12-07 21:53:10.152250: step 21100, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.152 sec/batch; 37h:04m:07s remains)
2017-12-07 21:53:11.721015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2750893 -4.2582664 -4.2355576 -4.212729 -4.2010345 -4.2056308 -4.2225294 -4.2320647 -4.2336264 -4.2331586 -4.2333107 -4.2314367 -4.2327962 -4.2398295 -4.2492566][-4.2562804 -4.2295418 -4.1984615 -4.1662841 -4.1469936 -4.151886 -4.1796465 -4.2019567 -4.2106543 -4.2123623 -4.2070546 -4.2024245 -4.203795 -4.21129 -4.2226548][-4.2389421 -4.1978803 -4.1547937 -4.1118417 -4.0828037 -4.0876164 -4.1261091 -4.1610432 -4.1794229 -4.1846685 -4.1772075 -4.1681972 -4.1676025 -4.172256 -4.1810122][-4.2268081 -4.1707172 -4.1129308 -4.0626783 -4.0267868 -4.0301142 -4.0782285 -4.1231747 -4.1420465 -4.1459808 -4.138576 -4.1274028 -4.1271 -4.1285267 -4.1332917][-4.2218423 -4.1535783 -4.0823731 -4.0222836 -3.9759998 -3.9795809 -4.0378447 -4.0903745 -4.1016889 -4.1017065 -4.0997868 -4.095253 -4.0993881 -4.0994945 -4.103847][-4.2177162 -4.1403227 -4.0550857 -3.9793909 -3.9203591 -3.9260828 -3.9922571 -4.046174 -4.0495653 -4.051558 -4.0654793 -4.0737181 -4.0845928 -4.0858536 -4.0895433][-4.2174215 -4.1345034 -4.0358663 -3.9412766 -3.8726149 -3.8754404 -3.9417322 -3.9911623 -3.9842646 -3.9915888 -4.0270762 -4.0547595 -4.0736284 -4.0768557 -4.0844479][-4.2244244 -4.1454854 -4.0494843 -3.94483 -3.8693082 -3.86378 -3.9224124 -3.9676294 -3.9615188 -3.973335 -4.0171142 -4.0516481 -4.0721741 -4.0750704 -4.0786915][-4.2312131 -4.1620073 -4.0793457 -3.9824696 -3.9028473 -3.8897936 -3.9476206 -4.0026169 -4.0161395 -4.0221672 -4.0495143 -4.0749063 -4.0822315 -4.0732312 -4.069428][-4.2424183 -4.1843209 -4.1114583 -4.0246119 -3.9446623 -3.9257579 -3.9879649 -4.050561 -4.0720897 -4.0732927 -4.08578 -4.1006 -4.0925374 -4.0683856 -4.0540028][-4.259974 -4.2124648 -4.1437993 -4.0616422 -3.9873416 -3.9576938 -4.0135164 -4.0693011 -4.0935912 -4.0979872 -4.1058617 -4.1127071 -4.0995822 -4.0701108 -4.0499058][-4.2734418 -4.2328091 -4.1696706 -4.0951796 -4.0269451 -3.9817863 -4.0170493 -4.0573859 -4.0839148 -4.0973997 -4.1103492 -4.118351 -4.1150279 -4.0957489 -4.0744872][-4.2786617 -4.23785 -4.1788335 -4.1194415 -4.0637894 -4.0156045 -4.02834 -4.0551715 -4.0804062 -4.098475 -4.1197071 -4.1362538 -4.1489167 -4.144206 -4.1245246][-4.2777829 -4.2337742 -4.1745911 -4.1276326 -4.0841002 -4.0470247 -4.0618329 -4.0838747 -4.1028852 -4.1183443 -4.1425195 -4.1638546 -4.1848788 -4.1830869 -4.1679511][-4.2810054 -4.2332697 -4.1675296 -4.1255059 -4.091629 -4.0683923 -4.0943069 -4.121778 -4.1387591 -4.1536427 -4.1756229 -4.196034 -4.2174697 -4.2129917 -4.201221]]...]
INFO - root - 2017-12-07 21:53:32.929140: step 21110, loss = 2.06, batch loss = 2.01 (15.2 examples/sec; 2.106 sec/batch; 36h:16m:58s remains)
INFO - root - 2017-12-07 21:53:54.237625: step 21120, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.098 sec/batch; 36h:08m:22s remains)
INFO - root - 2017-12-07 21:54:14.948553: step 21130, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.099 sec/batch; 36h:08m:33s remains)
INFO - root - 2017-12-07 21:54:36.182141: step 21140, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.155 sec/batch; 37h:06m:21s remains)
INFO - root - 2017-12-07 21:54:57.462827: step 21150, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.134 sec/batch; 36h:44m:03s remains)
INFO - root - 2017-12-07 21:55:18.356040: step 21160, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.090 sec/batch; 35h:58m:47s remains)
INFO - root - 2017-12-07 21:55:39.655797: step 21170, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.171 sec/batch; 37h:21m:15s remains)
INFO - root - 2017-12-07 21:56:00.951100: step 21180, loss = 2.07, batch loss = 2.02 (14.6 examples/sec; 2.191 sec/batch; 37h:42m:05s remains)
INFO - root - 2017-12-07 21:56:21.741344: step 21190, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.109 sec/batch; 36h:17m:03s remains)
INFO - root - 2017-12-07 21:56:43.016679: step 21200, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.150 sec/batch; 36h:58m:38s remains)
2017-12-07 21:56:44.604190: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3402996 -4.3348618 -4.3220549 -4.30582 -4.2769184 -4.2319074 -4.1796632 -4.1233549 -4.1115336 -4.13615 -4.1815634 -4.2304921 -4.274951 -4.3061385 -4.3215237][-4.3457804 -4.3445487 -4.3289156 -4.3057661 -4.2683744 -4.2112846 -4.1406703 -4.0769544 -4.0735412 -4.1125336 -4.1684628 -4.223228 -4.2672057 -4.2984414 -4.3164649][-4.3301153 -4.3293819 -4.313962 -4.2911253 -4.2523818 -4.1923008 -4.1104579 -4.0485363 -4.0556908 -4.1063833 -4.1664586 -4.2182226 -4.260148 -4.2904372 -4.3089385][-4.2999835 -4.3052497 -4.2966967 -4.2788782 -4.2464156 -4.193984 -4.1126938 -4.0546532 -4.0702629 -4.1214805 -4.1747317 -4.218853 -4.2578216 -4.2891784 -4.3065724][-4.2921219 -4.2988148 -4.2928891 -4.2719965 -4.2389255 -4.1865425 -4.100843 -4.0395336 -4.0706491 -4.1334004 -4.1860552 -4.2237015 -4.263104 -4.2958484 -4.3120265][-4.2851658 -4.287816 -4.2753572 -4.2423687 -4.1973553 -4.1265182 -4.015656 -3.9396131 -3.9981663 -4.0916538 -4.1592989 -4.2040758 -4.252811 -4.2934608 -4.3134346][-4.2526951 -4.2495365 -4.2287569 -4.18511 -4.1228366 -4.0197468 -3.8655276 -3.760905 -3.855911 -3.9946611 -4.0919409 -4.1539412 -4.2182975 -4.2739716 -4.3032465][-4.2236853 -4.2167335 -4.1934524 -4.1468573 -4.0815959 -3.9708693 -3.8066273 -3.684927 -3.7859745 -3.94613 -4.0590072 -4.1300592 -4.1994195 -4.2623949 -4.2949524][-4.2290363 -4.2213058 -4.2084994 -4.1808705 -4.1454086 -4.0751715 -3.9644969 -3.8632081 -3.913362 -4.0307474 -4.1211333 -4.1776772 -4.2288074 -4.2776833 -4.3027458][-4.2485662 -4.2346015 -4.2279449 -4.2187085 -4.2054329 -4.1641555 -4.0978012 -4.022305 -4.0343871 -4.1127648 -4.1830678 -4.22921 -4.2654781 -4.3003449 -4.3160596][-4.2605391 -4.2463894 -4.2459378 -4.2479181 -4.2418318 -4.2087307 -4.157299 -4.0926366 -4.0865149 -4.1474042 -4.210505 -4.2542973 -4.2855558 -4.313344 -4.3254032][-4.2772231 -4.2715106 -4.2767854 -4.2862554 -4.2874646 -4.2607942 -4.2157326 -4.1578879 -4.143024 -4.1850553 -4.2338634 -4.2724009 -4.3001633 -4.3234096 -4.3326554][-4.3059464 -4.3048868 -4.3097873 -4.3198032 -4.3250914 -4.3099952 -4.2728505 -4.2211609 -4.1990676 -4.2227612 -4.2591362 -4.2902217 -4.3154678 -4.336782 -4.3420877][-4.3269095 -4.3263955 -4.3285971 -4.3350639 -4.33917 -4.3301277 -4.3028626 -4.262753 -4.239934 -4.24981 -4.2768183 -4.3037248 -4.3251476 -4.3421674 -4.3446116][-4.336369 -4.3336191 -4.3334703 -4.3365793 -4.3381324 -4.3324308 -4.3147373 -4.2848811 -4.2644191 -4.2669396 -4.2880197 -4.3096957 -4.3249326 -4.3379779 -4.3405323]]...]
INFO - root - 2017-12-07 21:57:05.872850: step 21210, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.089 sec/batch; 35h:55m:39s remains)
INFO - root - 2017-12-07 21:57:26.712641: step 21220, loss = 2.07, batch loss = 2.02 (16.7 examples/sec; 1.921 sec/batch; 33h:02m:25s remains)
INFO - root - 2017-12-07 21:57:47.918362: step 21230, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.085 sec/batch; 35h:51m:19s remains)
INFO - root - 2017-12-07 21:58:09.077682: step 21240, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.084 sec/batch; 35h:49m:12s remains)
INFO - root - 2017-12-07 21:58:30.340732: step 21250, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.137 sec/batch; 36h:43m:41s remains)
INFO - root - 2017-12-07 21:58:50.964519: step 21260, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.084 sec/batch; 35h:49m:08s remains)
INFO - root - 2017-12-07 21:59:12.229284: step 21270, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 36h:29m:43s remains)
INFO - root - 2017-12-07 21:59:33.359958: step 21280, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 36h:30m:26s remains)
INFO - root - 2017-12-07 21:59:54.254861: step 21290, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 36h:32m:35s remains)
INFO - root - 2017-12-07 22:00:15.448411: step 21300, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.144 sec/batch; 36h:48m:57s remains)
2017-12-07 22:00:17.020813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2524853 -4.2398629 -4.2332559 -4.2366738 -4.2452536 -4.2578692 -4.273747 -4.2859316 -4.2872729 -4.2764463 -4.2580643 -4.2410359 -4.2341151 -4.2328453 -4.2367978][-4.2509513 -4.2344537 -4.2229738 -4.2260294 -4.2322454 -4.2389784 -4.2535539 -4.2704368 -4.2792945 -4.2747912 -4.2557955 -4.2313638 -4.2230811 -4.2239928 -4.2226706][-4.2618604 -4.2415428 -4.2238283 -4.2226663 -4.2238941 -4.2221746 -4.2291622 -4.2475085 -4.2635641 -4.2631516 -4.2408624 -4.2089825 -4.1937075 -4.1979237 -4.1960216][-4.2742982 -4.2529874 -4.2297211 -4.2216735 -4.2172484 -4.2072678 -4.2006016 -4.2153473 -4.2324963 -4.2330751 -4.2118669 -4.1778688 -4.1591463 -4.1650414 -4.1634][-4.2753124 -4.2542157 -4.2259555 -4.2083993 -4.2004118 -4.1868057 -4.1715059 -4.1782131 -4.1941972 -4.1959839 -4.1814528 -4.1540728 -4.13833 -4.1486959 -4.1480818][-4.2707181 -4.2448869 -4.2116294 -4.1863647 -4.177145 -4.1690812 -4.1535578 -4.1506162 -4.1612997 -4.1694512 -4.1642451 -4.1505661 -4.1392388 -4.1490712 -4.1565533][-4.2578692 -4.227169 -4.1901932 -4.1605539 -4.1569896 -4.1634188 -4.154664 -4.1408353 -4.14405 -4.157855 -4.1581664 -4.1537962 -4.1497622 -4.1591029 -4.1733742][-4.2530761 -4.2230368 -4.1900578 -4.1646962 -4.1700706 -4.189116 -4.1894932 -4.1755409 -4.1701 -4.1785588 -4.17162 -4.1642351 -4.1649475 -4.1727071 -4.1888227][-4.2632022 -4.2396832 -4.2194409 -4.2074165 -4.2183027 -4.2362838 -4.2389283 -4.2328038 -4.2224689 -4.2179542 -4.196836 -4.1777315 -4.1731825 -4.1749754 -4.1905303][-4.2739315 -4.2585917 -4.2505736 -4.2502694 -4.25857 -4.2684922 -4.27073 -4.2662821 -4.2532964 -4.2408333 -4.2166762 -4.1919937 -4.1814966 -4.176291 -4.1862812][-4.2865796 -4.2747588 -4.2671514 -4.265532 -4.2660828 -4.2721467 -4.2754226 -4.2733011 -4.2645316 -4.2525487 -4.2349067 -4.2160697 -4.2058921 -4.1981378 -4.2033534][-4.2963367 -4.2840085 -4.2698164 -4.2616014 -4.258738 -4.2631626 -4.2716613 -4.2786379 -4.2791991 -4.271842 -4.2614875 -4.2519674 -4.2432814 -4.2362318 -4.240715][-4.2957554 -4.2888436 -4.2766795 -4.2685971 -4.2673845 -4.2685366 -4.2752957 -4.2864008 -4.29065 -4.2862763 -4.2813458 -4.2771664 -4.2718258 -4.2676764 -4.2722731][-4.287662 -4.2867861 -4.2843475 -4.2866597 -4.2879257 -4.2866163 -4.2862372 -4.29227 -4.2938972 -4.2919059 -4.2895007 -4.2874126 -4.2851453 -4.2823629 -4.2858539][-4.2864513 -4.2906742 -4.2952013 -4.3032746 -4.3069863 -4.3040309 -4.3011026 -4.3016419 -4.3002086 -4.2977128 -4.2955165 -4.2929339 -4.2918773 -4.2899933 -4.2923131]]...]
INFO - root - 2017-12-07 22:00:38.157643: step 21310, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.171 sec/batch; 37h:16m:29s remains)
INFO - root - 2017-12-07 22:00:59.099354: step 21320, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.105 sec/batch; 36h:08m:38s remains)
INFO - root - 2017-12-07 22:01:20.388577: step 21330, loss = 2.06, batch loss = 2.01 (14.6 examples/sec; 2.197 sec/batch; 37h:42m:38s remains)
INFO - root - 2017-12-07 22:01:41.687412: step 21340, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.157 sec/batch; 37h:01m:28s remains)
INFO - root - 2017-12-07 22:02:02.500228: step 21350, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.164 sec/batch; 37h:08m:11s remains)
INFO - root - 2017-12-07 22:02:23.717429: step 21360, loss = 2.09, batch loss = 2.03 (14.8 examples/sec; 2.168 sec/batch; 37h:12m:17s remains)
INFO - root - 2017-12-07 22:02:44.804423: step 21370, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.090 sec/batch; 35h:51m:24s remains)
INFO - root - 2017-12-07 22:03:05.955013: step 21380, loss = 2.07, batch loss = 2.01 (16.3 examples/sec; 1.961 sec/batch; 33h:38m:21s remains)
INFO - root - 2017-12-07 22:03:26.982686: step 21390, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.118 sec/batch; 36h:19m:29s remains)
INFO - root - 2017-12-07 22:03:48.052390: step 21400, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.103 sec/batch; 36h:03m:06s remains)
2017-12-07 22:03:49.683441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2955136 -4.280304 -4.2749553 -4.2706308 -4.26143 -4.2503443 -4.2419376 -4.2255158 -4.2009392 -4.1826148 -4.1849885 -4.2019844 -4.2156796 -4.2266116 -4.2394805][-4.2697062 -4.2462888 -4.23805 -4.2383666 -4.2317772 -4.2186875 -4.2096691 -4.189424 -4.1662183 -4.1593537 -4.1764841 -4.1997013 -4.2102118 -4.2171197 -4.2270288][-4.2466626 -4.2159648 -4.1996913 -4.1997309 -4.1944523 -4.1816397 -4.1701055 -4.1494875 -4.128437 -4.1301889 -4.1552391 -4.18449 -4.195715 -4.1979709 -4.2052822][-4.2437906 -4.2144656 -4.1946306 -4.1915584 -4.1846018 -4.1670461 -4.1452866 -4.1169591 -4.0957723 -4.1021128 -4.1260386 -4.1497478 -4.160212 -4.1655669 -4.1735668][-4.2498393 -4.2247672 -4.2042885 -4.1958036 -4.1889648 -4.1708078 -4.1375003 -4.1019578 -4.0804229 -4.0863667 -4.100482 -4.1133795 -4.124485 -4.1332412 -4.1412463][-4.2569232 -4.2330432 -4.2137775 -4.2014494 -4.1988525 -4.1879282 -4.1535645 -4.1154447 -4.0911922 -4.0918927 -4.0932922 -4.0922127 -4.0973687 -4.1032777 -4.1072445][-4.2600379 -4.2372632 -4.2195811 -4.2050071 -4.2036114 -4.2032638 -4.179769 -4.1474314 -4.1230116 -4.1177969 -4.1159968 -4.1056924 -4.1020765 -4.1042347 -4.1030216][-4.2614493 -4.2431989 -4.2297568 -4.2149563 -4.210052 -4.2106967 -4.1980543 -4.1760039 -4.1553516 -4.1491156 -4.1510267 -4.1412549 -4.1315069 -4.1317177 -4.1277347][-4.267848 -4.2593727 -4.2535009 -4.2394748 -4.22728 -4.2178235 -4.20555 -4.1903148 -4.1773858 -4.1777277 -4.1843114 -4.1757298 -4.1637297 -4.1646233 -4.171771][-4.2834125 -4.281826 -4.2816925 -4.2676315 -4.2493868 -4.2314873 -4.2113967 -4.1956916 -4.1915126 -4.1979132 -4.208168 -4.2027082 -4.1892285 -4.1902995 -4.2093387][-4.2995009 -4.30046 -4.2996068 -4.2845292 -4.262044 -4.2391973 -4.2123632 -4.1942143 -4.19706 -4.2085423 -4.2221289 -4.2146587 -4.2003808 -4.2005796 -4.2224436][-4.3123326 -4.3163772 -4.3150163 -4.3005028 -4.2771139 -4.2482715 -4.2153468 -4.196012 -4.2016473 -4.215188 -4.2300134 -4.2233763 -4.2098279 -4.2085528 -4.2313271][-4.3316712 -4.3334856 -4.3328767 -4.3213172 -4.3008976 -4.2702169 -4.2350254 -4.2155509 -4.2211347 -4.2330585 -4.24872 -4.2506604 -4.2415876 -4.2395649 -4.2560463][-4.3451777 -4.347775 -4.3491807 -4.3412094 -4.3272691 -4.3041844 -4.2730451 -4.2551208 -4.255434 -4.2623315 -4.277874 -4.2872143 -4.2842331 -4.2838082 -4.2934813][-4.3407993 -4.343143 -4.3444376 -4.3419714 -4.3375769 -4.325768 -4.3056827 -4.2903113 -4.2864509 -4.2874756 -4.2961478 -4.303988 -4.3066773 -4.3086329 -4.3127756]]...]
INFO - root - 2017-12-07 22:04:10.769639: step 21410, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.092 sec/batch; 35h:51m:51s remains)
INFO - root - 2017-12-07 22:04:31.571344: step 21420, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 2.209 sec/batch; 37h:51m:19s remains)
INFO - root - 2017-12-07 22:04:52.621396: step 21430, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.143 sec/batch; 36h:43m:52s remains)
INFO - root - 2017-12-07 22:05:13.904647: step 21440, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.140 sec/batch; 36h:40m:29s remains)
INFO - root - 2017-12-07 22:05:34.963565: step 21450, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.103 sec/batch; 36h:01m:16s remains)
INFO - root - 2017-12-07 22:05:56.282001: step 21460, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.105 sec/batch; 36h:03m:16s remains)
INFO - root - 2017-12-07 22:06:17.436746: step 21470, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.127 sec/batch; 36h:25m:40s remains)
INFO - root - 2017-12-07 22:06:38.146771: step 21480, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.112 sec/batch; 36h:09m:29s remains)
INFO - root - 2017-12-07 22:06:59.251859: step 21490, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.164 sec/batch; 37h:03m:28s remains)
INFO - root - 2017-12-07 22:07:20.410902: step 21500, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.088 sec/batch; 35h:44m:21s remains)
2017-12-07 22:07:22.047757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2505322 -4.2224727 -4.2111273 -4.2213778 -4.2335196 -4.2367334 -4.2400246 -4.2507253 -4.2628245 -4.2692385 -4.2709489 -4.2696061 -4.2664113 -4.2633238 -4.2604928][-4.2499595 -4.2227221 -4.2132206 -4.224997 -4.2380462 -4.2399721 -4.2398334 -4.2475567 -4.2595749 -4.2683768 -4.2738256 -4.2757916 -4.2741671 -4.2712631 -4.2674804][-4.251123 -4.2242818 -4.2158527 -4.2279291 -4.2396469 -4.2372179 -4.2306795 -4.2335033 -4.2457676 -4.2588363 -4.2690878 -4.2741303 -4.2728839 -4.2694383 -4.2648025][-4.251626 -4.2246375 -4.2167745 -4.2279992 -4.2361569 -4.2266569 -4.2112422 -4.2082181 -4.2217569 -4.2401752 -4.2546911 -4.2620096 -4.2596493 -4.2536221 -4.2474875][-4.2505441 -4.2229891 -4.215188 -4.2250252 -4.2286148 -4.2101822 -4.1838751 -4.1749024 -4.1918268 -4.2171268 -4.2352724 -4.24313 -4.2372308 -4.2269549 -4.2180972][-4.2484021 -4.2203431 -4.2125125 -4.2212443 -4.22066 -4.1934514 -4.1560326 -4.1421223 -4.1642833 -4.1969218 -4.2176366 -4.2230945 -4.2113438 -4.1948733 -4.1822648][-4.2468643 -4.2189136 -4.2114749 -4.219862 -4.2167354 -4.1835337 -4.1377311 -4.1212988 -4.1483207 -4.1855912 -4.2055955 -4.2051096 -4.1856689 -4.1630368 -4.1476645][-4.2473173 -4.2202721 -4.2134337 -4.221776 -4.2182441 -4.1835337 -4.1347513 -4.118926 -4.1472726 -4.1836658 -4.198669 -4.1896181 -4.1622248 -4.1350102 -4.1195526][-4.2495465 -4.2238111 -4.2173834 -4.2258382 -4.2234969 -4.1914258 -4.14515 -4.1319175 -4.1577587 -4.1881256 -4.1949835 -4.1776361 -4.1446247 -4.1152244 -4.102407][-4.2514906 -4.2264166 -4.2191958 -4.2268424 -4.2258353 -4.1984792 -4.1581197 -4.1482029 -4.1699395 -4.1925526 -4.1918607 -4.1694822 -4.1355543 -4.1090493 -4.1027279][-4.2512026 -4.225647 -4.2164845 -4.2225 -4.222465 -4.20002 -4.1667504 -4.1599717 -4.1785064 -4.1939149 -4.187438 -4.1626472 -4.1314855 -4.1109529 -4.1119056][-4.2487779 -4.2222824 -4.2113128 -4.2157397 -4.2161393 -4.1977692 -4.1713071 -4.1677756 -4.18406 -4.1937318 -4.1822 -4.155324 -4.1277375 -4.11458 -4.1223826][-4.2460251 -4.2187867 -4.2072749 -4.2112775 -4.2118316 -4.1963291 -4.1756811 -4.1751013 -4.1897569 -4.1952386 -4.1808639 -4.1532726 -4.12793 -4.1194458 -4.1307297][-4.2451382 -4.2182512 -4.2075353 -4.2123384 -4.2137151 -4.2005939 -4.1851115 -4.1867876 -4.2004728 -4.2041121 -4.19031 -4.1648512 -4.1414495 -4.1333389 -4.1424527][-4.2466984 -4.2209806 -4.2114716 -4.2174835 -4.2202511 -4.2096925 -4.1986756 -4.2019353 -4.2152843 -4.2196717 -4.2102346 -4.1905932 -4.1704383 -4.1607332 -4.1638064]]...]
INFO - root - 2017-12-07 22:07:42.805020: step 21510, loss = 2.08, batch loss = 2.02 (16.9 examples/sec; 1.896 sec/batch; 32h:27m:16s remains)
INFO - root - 2017-12-07 22:08:04.081430: step 21520, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.152 sec/batch; 36h:49m:12s remains)
INFO - root - 2017-12-07 22:08:25.340722: step 21530, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.096 sec/batch; 35h:51m:17s remains)
INFO - root - 2017-12-07 22:08:46.459986: step 21540, loss = 2.07, batch loss = 2.01 (16.0 examples/sec; 2.005 sec/batch; 34h:18m:17s remains)
INFO - root - 2017-12-07 22:09:07.585467: step 21550, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.145 sec/batch; 36h:41m:40s remains)
INFO - root - 2017-12-07 22:09:29.037995: step 21560, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.169 sec/batch; 37h:05m:34s remains)
INFO - root - 2017-12-07 22:09:50.285078: step 21570, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.154 sec/batch; 36h:49m:56s remains)
INFO - root - 2017-12-07 22:10:11.267157: step 21580, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 36h:36m:43s remains)
INFO - root - 2017-12-07 22:10:32.691230: step 21590, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.130 sec/batch; 36h:24m:33s remains)
INFO - root - 2017-12-07 22:10:53.986615: step 21600, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.102 sec/batch; 35h:55m:33s remains)
2017-12-07 22:10:55.526252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3459835 -4.3478885 -4.3454595 -4.3438988 -4.3365636 -4.3174748 -4.3020344 -4.2949743 -4.2869449 -4.2831068 -4.2943678 -4.3019543 -4.3045039 -4.3038526 -4.2982087][-4.3482428 -4.3482785 -4.3408523 -4.3297672 -4.3122354 -4.2859669 -4.270474 -4.2713337 -4.27406 -4.2799363 -4.3013945 -4.3178148 -4.3246346 -4.3248463 -4.3191791][-4.3480129 -4.3455825 -4.3308568 -4.3060694 -4.2740493 -4.2355666 -4.2141128 -4.218133 -4.2322564 -4.2493019 -4.2813125 -4.3069711 -4.3213668 -4.32821 -4.3288579][-4.3459926 -4.3413887 -4.3199434 -4.2823873 -4.2357516 -4.1844587 -4.1548357 -4.1590195 -4.1829805 -4.2122679 -4.2551174 -4.2865915 -4.3018165 -4.3118911 -4.3167429][-4.3442369 -4.3393445 -4.3151484 -4.2710214 -4.2143106 -4.1521506 -4.1110144 -4.1109333 -4.1394877 -4.1751232 -4.2260523 -4.2617717 -4.2753606 -4.2843628 -4.2892051][-4.3424973 -4.338654 -4.3156548 -4.2708597 -4.2100887 -4.139987 -4.0863452 -4.0777555 -4.1048594 -4.142848 -4.2017446 -4.2438769 -4.2564888 -4.2615824 -4.2634435][-4.3414307 -4.3391476 -4.3194437 -4.2776814 -4.2174263 -4.1433558 -4.0814919 -4.0623274 -4.0800252 -4.1159816 -4.1814895 -4.2325897 -4.24837 -4.2508631 -4.2473607][-4.3417678 -4.34218 -4.3266635 -4.2877407 -4.230217 -4.157712 -4.090404 -4.0579824 -4.058331 -4.08536 -4.1535587 -4.2159457 -4.240859 -4.2459474 -4.2383909][-4.3438287 -4.346797 -4.3354855 -4.2991619 -4.243011 -4.1741643 -4.1051064 -4.0571423 -4.0384059 -4.0546427 -4.1204805 -4.18855 -4.222589 -4.2373147 -4.2358718][-4.3470316 -4.3510084 -4.3431296 -4.3120217 -4.2594614 -4.1963873 -4.1296725 -4.0738177 -4.041348 -4.0473218 -4.1054697 -4.1703563 -4.2091756 -4.2338061 -4.2399325][-4.3501105 -4.354002 -4.3485622 -4.3249903 -4.2796788 -4.224781 -4.1640854 -4.1097918 -4.0715904 -4.0693431 -4.1137891 -4.1695781 -4.2091827 -4.2392612 -4.2501135][-4.3510122 -4.3537145 -4.35009 -4.3360133 -4.3033161 -4.2609415 -4.2122946 -4.1680694 -4.1310534 -4.114974 -4.13861 -4.1786695 -4.214036 -4.2453556 -4.2586269][-4.3484254 -4.3496256 -4.3455906 -4.3365045 -4.3122697 -4.2825365 -4.24914 -4.2187262 -4.1896968 -4.1639781 -4.1703539 -4.1950617 -4.2204351 -4.2474871 -4.2617836][-4.3446212 -4.3428683 -4.3325958 -4.3189821 -4.2933269 -4.2674332 -4.2465897 -4.2323461 -4.2152638 -4.1945162 -4.2006369 -4.2134771 -4.2232718 -4.2388244 -4.252707][-4.3433709 -4.33967 -4.3200126 -4.2932763 -4.2575374 -4.2309413 -4.2222605 -4.2241788 -4.2162218 -4.2036858 -4.2169623 -4.2258692 -4.2264004 -4.2339416 -4.2465014]]...]
INFO - root - 2017-12-07 22:11:16.432346: step 21610, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.139 sec/batch; 36h:32m:31s remains)
INFO - root - 2017-12-07 22:11:37.632025: step 21620, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.162 sec/batch; 36h:55m:53s remains)
INFO - root - 2017-12-07 22:11:59.106227: step 21630, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.156 sec/batch; 36h:49m:28s remains)
INFO - root - 2017-12-07 22:12:19.955856: step 21640, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.108 sec/batch; 36h:00m:07s remains)
INFO - root - 2017-12-07 22:12:41.360320: step 21650, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.136 sec/batch; 36h:28m:35s remains)
INFO - root - 2017-12-07 22:13:02.653349: step 21660, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.083 sec/batch; 35h:34m:04s remains)
INFO - root - 2017-12-07 22:13:23.200488: step 21670, loss = 2.08, batch loss = 2.02 (19.7 examples/sec; 1.628 sec/batch; 27h:47m:42s remains)
INFO - root - 2017-12-07 22:13:44.326746: step 21680, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.166 sec/batch; 36h:58m:37s remains)
INFO - root - 2017-12-07 22:14:05.457232: step 21690, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.099 sec/batch; 35h:49m:32s remains)
INFO - root - 2017-12-07 22:14:26.635556: step 21700, loss = 2.06, batch loss = 2.01 (14.8 examples/sec; 2.161 sec/batch; 36h:51m:54s remains)
2017-12-07 22:14:28.217978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2150278 -4.2186394 -4.2333865 -4.2436004 -4.2400732 -4.2311015 -4.2344308 -4.2495232 -4.2584591 -4.2552495 -4.2468677 -4.2403936 -4.2442389 -4.2514296 -4.2585454][-4.2435637 -4.2463083 -4.2584929 -4.2650747 -4.2608662 -4.2539639 -4.2583127 -4.2720733 -4.2789655 -4.27348 -4.2646308 -4.2632356 -4.2727122 -4.2825456 -4.2883511][-4.2665758 -4.2656507 -4.2722397 -4.2731133 -4.2675948 -4.2609453 -4.2621703 -4.2703881 -4.2714362 -4.2642179 -4.2596498 -4.2626042 -4.2721877 -4.2804661 -4.2835464][-4.2864375 -4.2820683 -4.2800956 -4.2701254 -4.2586622 -4.24732 -4.2418537 -4.2428212 -4.240303 -4.2346063 -4.2351155 -4.2394691 -4.2459879 -4.2514215 -4.2535024][-4.2903161 -4.2765021 -4.2578611 -4.2297707 -4.2067285 -4.186635 -4.1716714 -4.1651855 -4.1615763 -4.1625252 -4.1743803 -4.1865535 -4.1948562 -4.2035394 -4.2107139][-4.2545428 -4.2276697 -4.1906819 -4.1438904 -4.1110258 -4.0817432 -4.05058 -4.0327353 -4.031599 -4.0485015 -4.0802259 -4.1079507 -4.1274157 -4.148057 -4.1685033][-4.1884346 -4.1539664 -4.1101375 -4.0590258 -4.0238819 -3.9843109 -3.9334807 -3.900691 -3.9040685 -3.9411077 -3.9925113 -4.0375004 -4.0769882 -4.1180768 -4.1569953][-4.1284089 -4.1015429 -4.0735574 -4.0433788 -4.0201635 -3.9827287 -3.9264767 -3.8839519 -3.8861132 -3.9324076 -3.9909334 -4.0451784 -4.0979586 -4.1491647 -4.1923895][-4.1319809 -4.1212463 -4.1155782 -4.1099939 -4.10157 -4.07826 -4.0409956 -4.0088339 -4.0034328 -4.0366988 -4.080802 -4.12306 -4.1673837 -4.2083035 -4.2385297][-4.193409 -4.1889257 -4.1901321 -4.19121 -4.185915 -4.1715508 -4.1529922 -4.1323557 -4.1233954 -4.1422882 -4.1680737 -4.1926322 -4.218915 -4.2438679 -4.2588768][-4.2390943 -4.2313671 -4.2264533 -4.2213888 -4.2127042 -4.2033348 -4.1974149 -4.1859851 -4.1770954 -4.1901116 -4.2056704 -4.2171292 -4.2303 -4.2446961 -4.2514334][-4.2411423 -4.22639 -4.2096772 -4.1935115 -4.1816773 -4.174931 -4.1741848 -4.17368 -4.17388 -4.186244 -4.1967125 -4.2001061 -4.2088809 -4.22286 -4.2307358][-4.2300911 -4.2140069 -4.1935825 -4.1733894 -4.1612916 -4.1560621 -4.1578131 -4.1648149 -4.17196 -4.1838322 -4.1915474 -4.1935072 -4.203805 -4.2214026 -4.2333984][-4.25451 -4.2441335 -4.2276726 -4.2097988 -4.1981344 -4.1952739 -4.1991963 -4.2097306 -4.2209697 -4.229702 -4.2318254 -4.2324543 -4.24275 -4.2591629 -4.2707548][-4.3051529 -4.3000622 -4.2896156 -4.2769647 -4.2667642 -4.2649093 -4.2684045 -4.2776437 -4.2877131 -4.2912984 -4.2888236 -4.2878027 -4.29482 -4.3055863 -4.3135433]]...]
INFO - root - 2017-12-07 22:14:49.137862: step 21710, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 36h:20m:14s remains)
INFO - root - 2017-12-07 22:15:10.567202: step 21720, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.116 sec/batch; 36h:05m:15s remains)
INFO - root - 2017-12-07 22:15:31.817118: step 21730, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.145 sec/batch; 36h:35m:05s remains)
INFO - root - 2017-12-07 22:15:52.648872: step 21740, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.111 sec/batch; 36h:00m:11s remains)
INFO - root - 2017-12-07 22:16:13.924442: step 21750, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.178 sec/batch; 37h:07m:29s remains)
INFO - root - 2017-12-07 22:16:35.117510: step 21760, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.134 sec/batch; 36h:22m:18s remains)
INFO - root - 2017-12-07 22:16:55.673261: step 21770, loss = 2.10, batch loss = 2.04 (15.3 examples/sec; 2.088 sec/batch; 35h:35m:29s remains)
INFO - root - 2017-12-07 22:17:16.926137: step 21780, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.100 sec/batch; 35h:47m:33s remains)
INFO - root - 2017-12-07 22:17:38.085194: step 21790, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.115 sec/batch; 36h:01m:34s remains)
INFO - root - 2017-12-07 22:17:59.190026: step 21800, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 36h:19m:05s remains)
2017-12-07 22:18:00.628431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2942481 -4.303462 -4.3079147 -4.3058929 -4.2975006 -4.2884846 -4.2804966 -4.2669621 -4.263443 -4.27151 -4.2772818 -4.2769032 -4.2694178 -4.2615066 -4.2500038][-4.2884464 -4.2963743 -4.2990284 -4.2944856 -4.2838717 -4.2725716 -4.2624164 -4.2492371 -4.2480879 -4.2614322 -4.2647724 -4.2567983 -4.2388954 -4.2255983 -4.2109866][-4.2885804 -4.2874269 -4.2831798 -4.2765479 -4.2648807 -4.2449322 -4.2254944 -4.2133174 -4.2183228 -4.2388554 -4.2449636 -4.2345085 -4.2105021 -4.1990213 -4.189785][-4.2875466 -4.2735033 -4.2566009 -4.2384057 -4.2164454 -4.1826982 -4.1509485 -4.141336 -4.1550083 -4.184485 -4.2035913 -4.203794 -4.1867075 -4.1877794 -4.194849][-4.2868505 -4.25972 -4.2242136 -4.1873708 -4.1493578 -4.1007614 -4.0612011 -4.0565214 -4.0823879 -4.1255488 -4.165998 -4.1830568 -4.1775374 -4.192112 -4.2154374][-4.2871714 -4.2457876 -4.1938996 -4.1434727 -4.0901289 -4.0217934 -3.9769433 -3.9851511 -4.0289392 -4.084754 -4.14104 -4.174469 -4.1835227 -4.20887 -4.2386284][-4.2768564 -4.2277918 -4.1681261 -4.1108766 -4.045568 -3.957901 -3.9087307 -3.9329975 -3.9989972 -4.0706258 -4.1376028 -4.1817007 -4.2003841 -4.2281337 -4.253861][-4.2487187 -4.2088184 -4.1559048 -4.1065216 -4.0488596 -3.9687648 -3.9336715 -3.9747069 -4.0504947 -4.1242237 -4.184834 -4.2191138 -4.2279291 -4.2420821 -4.2556534][-4.2053781 -4.1933527 -4.1647239 -4.1369243 -4.1040559 -4.0562534 -4.0475039 -4.0935378 -4.1607904 -4.2199264 -4.2605338 -4.2702427 -4.2564473 -4.2512684 -4.2531047][-4.1744671 -4.1942496 -4.1968031 -4.1886148 -4.17639 -4.160316 -4.1710548 -4.2101231 -4.2579546 -4.2950044 -4.3120489 -4.2982659 -4.2662978 -4.2463965 -4.2396512][-4.1780958 -4.2129107 -4.232985 -4.2384458 -4.24155 -4.2427683 -4.2591176 -4.2858038 -4.312499 -4.3273406 -4.32305 -4.2907133 -4.2468805 -4.220593 -4.2117267][-4.2031536 -4.2377615 -4.261508 -4.2711263 -4.2761235 -4.2789884 -4.2904854 -4.3046088 -4.3163714 -4.3176851 -4.3011289 -4.2625117 -4.2173338 -4.1925149 -4.1831484][-4.221365 -4.2487683 -4.2711382 -4.2798285 -4.282115 -4.2835126 -4.2886143 -4.2927756 -4.2944174 -4.2880883 -4.2691655 -4.2327967 -4.1919394 -4.1728249 -4.1648135][-4.2374125 -4.2553697 -4.271677 -4.2752466 -4.2733765 -4.2717991 -4.2707076 -4.2683535 -4.26434 -4.2565207 -4.2410851 -4.2134895 -4.1828561 -4.1707206 -4.1640091][-4.2467957 -4.2535276 -4.2617745 -4.2615929 -4.2581415 -4.2544408 -4.2518473 -4.2490683 -4.2461963 -4.2410316 -4.2319064 -4.214478 -4.1943951 -4.1869321 -4.1802649]]...]
INFO - root - 2017-12-07 22:18:21.770629: step 21810, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.168 sec/batch; 36h:55m:11s remains)
INFO - root - 2017-12-07 22:18:43.047777: step 21820, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 2.201 sec/batch; 37h:29m:19s remains)
INFO - root - 2017-12-07 22:19:04.258979: step 21830, loss = 2.09, batch loss = 2.03 (14.6 examples/sec; 2.193 sec/batch; 37h:19m:58s remains)
INFO - root - 2017-12-07 22:19:25.397459: step 21840, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.104 sec/batch; 35h:48m:33s remains)
INFO - root - 2017-12-07 22:19:46.552852: step 21850, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.143 sec/batch; 36h:28m:32s remains)
INFO - root - 2017-12-07 22:20:07.754457: step 21860, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.109 sec/batch; 35h:53m:25s remains)
INFO - root - 2017-12-07 22:20:28.455379: step 21870, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.096 sec/batch; 35h:39m:57s remains)
INFO - root - 2017-12-07 22:20:49.837522: step 21880, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 36h:00m:29s remains)
INFO - root - 2017-12-07 22:21:11.324956: step 21890, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.167 sec/batch; 36h:51m:30s remains)
INFO - root - 2017-12-07 22:21:32.430996: step 21900, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.130 sec/batch; 36h:13m:58s remains)
2017-12-07 22:21:34.032743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.317461 -4.3061166 -4.2915597 -4.27161 -4.2442241 -4.207314 -4.1695333 -4.1420207 -4.1404338 -4.17144 -4.1891465 -4.1846881 -4.1825924 -4.1896396 -4.2033043][-4.317452 -4.3059778 -4.2916017 -4.2727838 -4.2456985 -4.2079077 -4.1685224 -4.1368976 -4.1305804 -4.155293 -4.1718187 -4.1676655 -4.1676245 -4.1779332 -4.1914287][-4.3184218 -4.3052931 -4.2876425 -4.265213 -4.2342272 -4.1938195 -4.1528654 -4.1193771 -4.1103992 -4.13586 -4.1563039 -4.1540413 -4.1549129 -4.1680908 -4.1824584][-4.3169479 -4.3013387 -4.2794285 -4.2500577 -4.2119455 -4.1661806 -4.1203184 -4.0840034 -4.0757704 -4.1050944 -4.1322207 -4.1328583 -4.1362839 -4.1536646 -4.1685085][-4.3127708 -4.2953563 -4.2718763 -4.240046 -4.1993103 -4.151123 -4.0999632 -4.0569878 -4.0456409 -4.0741272 -4.104373 -4.1074977 -4.1157269 -4.13577 -4.1487145][-4.3074284 -4.2904692 -4.2674036 -4.2373328 -4.2022924 -4.1586618 -4.1065083 -4.0586348 -4.04432 -4.0674343 -4.0942559 -4.0964785 -4.1041522 -4.11892 -4.1272664][-4.3027859 -4.2870851 -4.2661366 -4.2396417 -4.2114654 -4.1719346 -4.1203256 -4.07149 -4.0562892 -4.077291 -4.1056108 -4.1075268 -4.1079721 -4.1139617 -4.1191783][-4.29695 -4.2815967 -4.2610106 -4.2379117 -4.2127304 -4.1724334 -4.1160755 -4.0633144 -4.0470347 -4.0734868 -4.1076889 -4.1117468 -4.10803 -4.1097422 -4.1164207][-4.2882509 -4.268178 -4.244523 -4.2175121 -4.1891375 -4.1415367 -4.0757427 -4.0140996 -3.9976358 -4.0403109 -4.0889297 -4.0983658 -4.09441 -4.1019034 -4.1162014][-4.2784767 -4.252542 -4.2220216 -4.1871548 -4.1509824 -4.094418 -4.0189834 -3.9470959 -3.9294202 -3.9889483 -4.0519633 -4.0630956 -4.0569172 -4.0721054 -4.0944123][-4.2768974 -4.2478704 -4.2138433 -4.1738262 -4.1319213 -4.0648718 -3.9796586 -3.8963909 -3.873939 -3.9394107 -4.0074368 -4.0175986 -4.0089927 -4.0260363 -4.0493197][-4.2870417 -4.2601261 -4.2329636 -4.2038913 -4.1745524 -4.1196632 -4.0461555 -3.968689 -3.934521 -3.9750066 -4.021596 -4.0162477 -3.9964781 -4.0014048 -4.0156941][-4.3037624 -4.2794538 -4.2584772 -4.2396054 -4.2242026 -4.1932478 -4.1477036 -4.0935082 -4.0650434 -4.0869107 -4.1130476 -4.1026011 -4.0844746 -4.0817337 -4.0834713][-4.3250709 -4.3076406 -4.2915492 -4.276937 -4.2660556 -4.2514358 -4.22841 -4.1965904 -4.1791186 -4.1913481 -4.2047234 -4.1986313 -4.191977 -4.191443 -4.1900573][-4.3444929 -4.3342247 -4.3225861 -4.3108511 -4.3013544 -4.2937903 -4.2841792 -4.270503 -4.2643275 -4.2724442 -4.2781897 -4.2750778 -4.2734041 -4.2763004 -4.2753921]]...]
INFO - root - 2017-12-07 22:21:55.171745: step 21910, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 35h:56m:01s remains)
INFO - root - 2017-12-07 22:22:16.261209: step 21920, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.122 sec/batch; 36h:04m:41s remains)
INFO - root - 2017-12-07 22:22:37.057915: step 21930, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.163 sec/batch; 36h:46m:34s remains)
INFO - root - 2017-12-07 22:22:58.163816: step 21940, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 2.057 sec/batch; 34h:57m:53s remains)
INFO - root - 2017-12-07 22:23:19.277746: step 21950, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.123 sec/batch; 36h:04m:48s remains)
INFO - root - 2017-12-07 22:23:40.215228: step 21960, loss = 2.07, batch loss = 2.01 (16.6 examples/sec; 1.933 sec/batch; 32h:50m:58s remains)
INFO - root - 2017-12-07 22:24:01.279185: step 21970, loss = 2.08, batch loss = 2.03 (14.9 examples/sec; 2.153 sec/batch; 36h:34m:29s remains)
INFO - root - 2017-12-07 22:24:22.377481: step 21980, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.101 sec/batch; 35h:41m:09s remains)
INFO - root - 2017-12-07 22:24:43.681267: step 21990, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.132 sec/batch; 36h:12m:36s remains)
INFO - root - 2017-12-07 22:25:04.355887: step 22000, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.099 sec/batch; 35h:38m:41s remains)
2017-12-07 22:25:06.025352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3166986 -4.3149462 -4.3126206 -4.3097653 -4.3073864 -4.3051596 -4.3042526 -4.3041592 -4.3035965 -4.3017626 -4.29963 -4.2977281 -4.2954736 -4.2921429 -4.2852449][-4.3103652 -4.308743 -4.3064575 -4.3038054 -4.3024135 -4.2998586 -4.2972469 -4.2958231 -4.2951012 -4.2947521 -4.2938256 -4.2928786 -4.29071 -4.286283 -4.2754264][-4.302834 -4.300561 -4.2970052 -4.2920961 -4.2878122 -4.2805662 -4.2738109 -4.2696528 -4.2705045 -4.2755246 -4.2788916 -4.2765374 -4.2695451 -4.2599616 -4.2419581][-4.2924852 -4.2866054 -4.277092 -4.2653065 -4.2539883 -4.2382846 -4.2237811 -4.216764 -4.22366 -4.2400832 -4.2518482 -4.2468696 -4.2313685 -4.2149181 -4.1904][-4.2748442 -4.2637372 -4.2447319 -4.2190943 -4.1920533 -4.1607628 -4.1293406 -4.113996 -4.1313705 -4.1719856 -4.2071333 -4.2127976 -4.1959014 -4.1779904 -4.1500936][-4.2605023 -4.2444839 -4.2135019 -4.1709843 -4.1223378 -4.0621347 -3.9952378 -3.9581938 -3.991617 -4.0723071 -4.1452031 -4.1724949 -4.1629615 -4.15096 -4.1248622][-4.2428513 -4.2214289 -4.1778197 -4.1151676 -4.0400534 -3.9405441 -3.8239188 -3.752141 -3.8081341 -3.9442804 -4.0678134 -4.1278353 -4.139214 -4.14221 -4.127316][-4.2382307 -4.2107596 -4.1591253 -4.0830007 -3.9925237 -3.8755221 -3.732151 -3.6255865 -3.6811521 -3.8457527 -4.0010214 -4.0865636 -4.1225538 -4.1436977 -4.1458416][-4.2624207 -4.2362576 -4.1953297 -4.1349158 -4.0671682 -3.9838967 -3.8863215 -3.8091495 -3.8294036 -3.9314263 -4.0456705 -4.120204 -4.1584125 -4.17834 -4.1819162][-4.278367 -4.2555609 -4.2277451 -4.1905637 -4.1532111 -4.1172514 -4.0772228 -4.0441527 -4.0467329 -4.0889845 -4.1466393 -4.1899767 -4.2121 -4.2237263 -4.2243795][-4.2858963 -4.2733736 -4.2552657 -4.23329 -4.2150273 -4.2036376 -4.1915011 -4.1852641 -4.1857262 -4.1950784 -4.2089911 -4.2239714 -4.2333889 -4.2420726 -4.2437868][-4.2639279 -4.2582989 -4.2461061 -4.2324052 -4.2253027 -4.2306938 -4.2343698 -4.2369618 -4.2401013 -4.2370505 -4.2269764 -4.2214866 -4.2231121 -4.2294688 -4.2309132][-4.2250209 -4.213078 -4.1916471 -4.1728559 -4.1696353 -4.1855574 -4.1991506 -4.2050085 -4.2088447 -4.20597 -4.19199 -4.181447 -4.179245 -4.1831908 -4.1841116][-4.2008691 -4.169106 -4.1283135 -4.0962806 -4.0941248 -4.1181889 -4.1377888 -4.1446929 -4.1475873 -4.1481323 -4.1407666 -4.1307373 -4.1239333 -4.1273742 -4.129899][-4.1894093 -4.1426487 -4.0904946 -4.05472 -4.0567451 -4.0854688 -4.1043682 -4.1066947 -4.10574 -4.1060967 -4.1020856 -4.0900474 -4.0800824 -4.0821085 -4.0876594]]...]
INFO - root - 2017-12-07 22:25:27.266882: step 22010, loss = 2.08, batch loss = 2.03 (14.9 examples/sec; 2.153 sec/batch; 36h:33m:24s remains)
INFO - root - 2017-12-07 22:25:48.718759: step 22020, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.195 sec/batch; 37h:15m:03s remains)
INFO - root - 2017-12-07 22:26:09.838134: step 22030, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.122 sec/batch; 36h:01m:04s remains)
INFO - root - 2017-12-07 22:26:31.221769: step 22040, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 35h:59m:27s remains)
INFO - root - 2017-12-07 22:26:52.319869: step 22050, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.127 sec/batch; 36h:04m:40s remains)
INFO - root - 2017-12-07 22:27:13.060473: step 22060, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.094 sec/batch; 35h:30m:54s remains)
INFO - root - 2017-12-07 22:27:34.443132: step 22070, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.170 sec/batch; 36h:48m:19s remains)
INFO - root - 2017-12-07 22:27:55.495513: step 22080, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.091 sec/batch; 35h:27m:24s remains)
INFO - root - 2017-12-07 22:28:16.446914: step 22090, loss = 2.08, batch loss = 2.02 (16.0 examples/sec; 2.005 sec/batch; 33h:59m:50s remains)
INFO - root - 2017-12-07 22:28:37.694733: step 22100, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.161 sec/batch; 36h:37m:26s remains)
2017-12-07 22:28:39.336352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2825933 -4.2912683 -4.2985134 -4.3046217 -4.307837 -4.3086786 -4.301693 -4.2926955 -4.2842689 -4.2779922 -4.2785497 -4.2814417 -4.2832627 -4.2828565 -4.2788839][-4.3091435 -4.3202934 -4.3292003 -4.3353162 -4.3367515 -4.3344259 -4.3267136 -4.3160167 -4.3067703 -4.3023386 -4.307076 -4.315876 -4.3224559 -4.3242264 -4.3193684][-4.3223262 -4.3347173 -4.3427148 -4.3446569 -4.339509 -4.329607 -4.3157597 -4.3001318 -4.2899494 -4.2883096 -4.2995539 -4.3164229 -4.3310528 -4.3385143 -4.3367438][-4.3202348 -4.3346725 -4.3419414 -4.3387327 -4.3248444 -4.30267 -4.2765956 -4.2515125 -4.2368717 -4.2368827 -4.2562547 -4.2850981 -4.3124352 -4.3298893 -4.3360181][-4.2983418 -4.3158736 -4.3227253 -4.3140941 -4.29065 -4.2553139 -4.2146988 -4.176362 -4.152493 -4.1510544 -4.1798429 -4.2237325 -4.26563 -4.295248 -4.3103261][-4.2694893 -4.2893276 -4.2954078 -4.282299 -4.2504296 -4.2026815 -4.1480045 -4.0962968 -4.0616975 -4.0577965 -4.095777 -4.1519227 -4.2030182 -4.2407975 -4.2615871][-4.2396111 -4.2591848 -4.263432 -4.2468109 -4.2086878 -4.1528268 -4.0902562 -4.0308414 -3.9902656 -3.9871535 -4.0298057 -4.0874672 -4.1373544 -4.1722827 -4.1915817][-4.2174888 -4.2350206 -4.2376766 -4.2186928 -4.1784592 -4.1233177 -4.0623503 -4.0063133 -3.9711967 -3.9736512 -4.0093427 -4.0525155 -4.0882211 -4.1090665 -4.1174741][-4.2152743 -4.2274327 -4.2277908 -4.2097955 -4.1772985 -4.1334229 -4.0856547 -4.0446205 -4.0238433 -4.0289774 -4.0477691 -4.0667419 -4.0816641 -4.0852513 -4.0800533][-4.2381744 -4.2444797 -4.2424498 -4.2279291 -4.2060633 -4.1783819 -4.1497741 -4.1277275 -4.1189418 -4.1196809 -4.1207829 -4.1194596 -4.1177487 -4.1087646 -4.0931768][-4.2574105 -4.2592072 -4.256063 -4.24768 -4.2374115 -4.2247186 -4.2110033 -4.200881 -4.1976094 -4.1945958 -4.1872058 -4.1779318 -4.1697974 -4.1563983 -4.1376843][-4.2688403 -4.2673507 -4.2641273 -4.2610378 -4.2588639 -4.2547441 -4.2473421 -4.2405047 -4.2378054 -4.2350445 -4.2300649 -4.2238345 -4.2177267 -4.2059727 -4.1906786][-4.2782593 -4.2733994 -4.2693439 -4.2691321 -4.2716246 -4.2721553 -4.2669153 -4.2598143 -4.2549624 -4.2527132 -4.25303 -4.2537069 -4.2531967 -4.24668 -4.2369418][-4.2922177 -4.2846336 -4.2790871 -4.2803035 -4.286005 -4.2902908 -4.2875495 -4.2811589 -4.2747335 -4.2715526 -4.2743349 -4.2792816 -4.2825108 -4.2797232 -4.2734275][-4.3019366 -4.2922268 -4.2853575 -4.2875509 -4.2954321 -4.3023744 -4.3022 -4.2964048 -4.2885656 -4.2846675 -4.2880869 -4.2945986 -4.3001289 -4.2997108 -4.2954388]]...]
INFO - root - 2017-12-07 22:29:00.403766: step 22110, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.137 sec/batch; 36h:12m:44s remains)
INFO - root - 2017-12-07 22:29:21.552184: step 22120, loss = 2.08, batch loss = 2.02 (16.0 examples/sec; 2.000 sec/batch; 33h:53m:31s remains)
INFO - root - 2017-12-07 22:29:42.667871: step 22130, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.090 sec/batch; 35h:24m:41s remains)
INFO - root - 2017-12-07 22:30:03.840482: step 22140, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.126 sec/batch; 36h:00m:37s remains)
INFO - root - 2017-12-07 22:30:25.160449: step 22150, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.191 sec/batch; 37h:06m:47s remains)
INFO - root - 2017-12-07 22:30:46.147204: step 22160, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.161 sec/batch; 36h:35m:26s remains)
INFO - root - 2017-12-07 22:31:07.564140: step 22170, loss = 2.06, batch loss = 2.00 (14.8 examples/sec; 2.167 sec/batch; 36h:41m:36s remains)
INFO - root - 2017-12-07 22:31:28.688663: step 22180, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.105 sec/batch; 35h:38m:04s remains)
INFO - root - 2017-12-07 22:31:49.528056: step 22190, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.072 sec/batch; 35h:04m:41s remains)
INFO - root - 2017-12-07 22:32:10.884464: step 22200, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.138 sec/batch; 36h:10m:58s remains)
2017-12-07 22:32:12.452560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3008738 -4.298142 -4.2900014 -4.2787943 -4.2686954 -4.2630672 -4.2602706 -4.2598863 -4.2633204 -4.2701631 -4.2791438 -4.2893662 -4.2972655 -4.3030252 -4.3059788][-4.2984905 -4.3018274 -4.2987351 -4.2906141 -4.2818789 -4.2768979 -4.271956 -4.2689776 -4.271091 -4.2772121 -4.2855024 -4.2927732 -4.2940807 -4.2917662 -4.2883935][-4.3062367 -4.315484 -4.3142195 -4.3023663 -4.2868981 -4.2725487 -4.2565022 -4.2456851 -4.2443476 -4.2512012 -4.261704 -4.2700043 -4.2690682 -4.261086 -4.2515373][-4.3204036 -4.3291693 -4.3207431 -4.2951641 -4.2639613 -4.2320194 -4.1968703 -4.16807 -4.160192 -4.1724257 -4.1946688 -4.2148762 -4.2210355 -4.2154012 -4.20495][-4.3308721 -4.33381 -4.3119841 -4.2641363 -4.2072453 -4.146472 -4.0765724 -4.0192223 -4.006928 -4.0326853 -4.0794296 -4.1265593 -4.1520853 -4.1615124 -4.1643209][-4.3296642 -4.3169341 -4.2676811 -4.1829643 -4.089087 -3.9913421 -3.8831198 -3.8082817 -3.810503 -3.8696594 -3.9555833 -4.0362053 -4.0877771 -4.1209321 -4.14448][-4.313117 -4.2785258 -4.1946073 -4.0702119 -3.9447253 -3.8220668 -3.6967604 -3.6246257 -3.65261 -3.7450771 -3.8628857 -3.967746 -4.0420995 -4.0978088 -4.1405687][-4.2925243 -4.2413735 -4.1358771 -4.0029483 -3.8881183 -3.793144 -3.7044353 -3.6568468 -3.683635 -3.7660499 -3.8743644 -3.9716804 -4.0485439 -4.1083059 -4.1536207][-4.2793388 -4.2282214 -4.1343369 -4.0335841 -3.9624753 -3.916538 -3.8751898 -3.8509078 -3.8626254 -3.9125173 -3.9820321 -4.0476723 -4.1028104 -4.1442246 -4.1768126][-4.278636 -4.24395 -4.18308 -4.1262474 -4.0927415 -4.0779724 -4.067863 -4.06267 -4.06768 -4.0896664 -4.1203089 -4.1482391 -4.1724095 -4.1881886 -4.2018895][-4.2811823 -4.2642531 -4.2356434 -4.2115264 -4.1999459 -4.1988134 -4.2031112 -4.2112627 -4.218739 -4.2288127 -4.2352443 -4.2347074 -4.231513 -4.2253675 -4.2245779][-4.2808795 -4.2761235 -4.2687168 -4.2633562 -4.2613554 -4.2617412 -4.2668986 -4.2812195 -4.2949748 -4.301621 -4.2980132 -4.2842054 -4.2657886 -4.2482247 -4.2393436][-4.2774992 -4.2789669 -4.2828693 -4.2859631 -4.2857256 -4.2830343 -4.2848754 -4.2974954 -4.3092842 -4.3121614 -4.3053827 -4.287961 -4.2679577 -4.2502451 -4.23961][-4.2732487 -4.2755151 -4.2816095 -4.2857332 -4.2853541 -4.2824836 -4.2829614 -4.2905827 -4.2937531 -4.2898407 -4.2789979 -4.2612491 -4.2453079 -4.2329321 -4.2228012][-4.2694831 -4.2714376 -4.2759271 -4.277873 -4.2771726 -4.2755303 -4.2752414 -4.2752604 -4.267602 -4.2556686 -4.24007 -4.2233105 -4.2144537 -4.2089977 -4.2004962]]...]
INFO - root - 2017-12-07 22:32:33.776123: step 22210, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.178 sec/batch; 36h:51m:15s remains)
INFO - root - 2017-12-07 22:32:54.865953: step 22220, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.095 sec/batch; 35h:26m:18s remains)
INFO - root - 2017-12-07 22:33:16.254535: step 22230, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.115 sec/batch; 35h:46m:04s remains)
INFO - root - 2017-12-07 22:33:37.501072: step 22240, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.111 sec/batch; 35h:42m:00s remains)
INFO - root - 2017-12-07 22:33:58.551240: step 22250, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.098 sec/batch; 35h:28m:58s remains)
INFO - root - 2017-12-07 22:34:19.536242: step 22260, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.069 sec/batch; 34h:58m:38s remains)
INFO - root - 2017-12-07 22:34:40.976680: step 22270, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.123 sec/batch; 35h:53m:33s remains)
INFO - root - 2017-12-07 22:35:02.192112: step 22280, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.101 sec/batch; 35h:30m:26s remains)
INFO - root - 2017-12-07 22:35:23.179185: step 22290, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 35h:31m:00s remains)
INFO - root - 2017-12-07 22:35:44.320733: step 22300, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.134 sec/batch; 36h:02m:52s remains)
2017-12-07 22:35:45.882659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2584348 -4.2651758 -4.2668433 -4.2599945 -4.2470317 -4.2342005 -4.2289286 -4.2323875 -4.2423239 -4.2389636 -4.2342515 -4.2344851 -4.2370315 -4.2499952 -4.2680612][-4.26376 -4.2726145 -4.2751083 -4.2652287 -4.2448997 -4.222405 -4.207871 -4.2053657 -4.2143016 -4.2110705 -4.2051959 -4.2057981 -4.2098303 -4.2243309 -4.2466769][-4.2711763 -4.2807422 -4.2842221 -4.2697654 -4.2407007 -4.2075529 -4.1801443 -4.16524 -4.1734858 -4.17354 -4.1700583 -4.1734304 -4.1797571 -4.1963053 -4.2206717][-4.2841158 -4.293644 -4.2940283 -4.2743211 -4.235805 -4.1889672 -4.1470132 -4.1193428 -4.1267071 -4.1318941 -4.1344543 -4.1412568 -4.1500139 -4.1674414 -4.1914096][-4.2955093 -4.3048415 -4.3022113 -4.27507 -4.2294831 -4.1731591 -4.1186514 -4.0826144 -4.0907145 -4.1042123 -4.1161466 -4.1263642 -4.134346 -4.1483169 -4.1674604][-4.3003597 -4.3088069 -4.3024735 -4.2708306 -4.2191768 -4.15314 -4.0859504 -4.041708 -4.0567994 -4.0900488 -4.1170692 -4.1316195 -4.136045 -4.1404891 -4.150363][-4.2987046 -4.29928 -4.2869544 -4.2526979 -4.1980438 -4.1234531 -4.0378118 -3.9754195 -3.9997082 -4.0644641 -4.11351 -4.1320724 -4.1327877 -4.1272044 -4.1272922][-4.2921405 -4.285718 -4.2683363 -4.2337608 -4.1810246 -4.1046338 -4.0096169 -3.9330132 -3.9640837 -4.0509396 -4.1156974 -4.1410894 -4.1421127 -4.1306386 -4.12104][-4.2814007 -4.27114 -4.2518463 -4.2213278 -4.17716 -4.1174212 -4.0459538 -3.9920368 -4.0265756 -4.09848 -4.1544237 -4.1777787 -4.1758175 -4.1609926 -4.1453576][-4.2724876 -4.2617178 -4.240881 -4.2141638 -4.1806493 -4.1418114 -4.1009851 -4.0753236 -4.106493 -4.1552639 -4.1952915 -4.2116971 -4.2052374 -4.1882744 -4.1738153][-4.2621903 -4.2532454 -4.234374 -4.2130432 -4.191649 -4.172545 -4.1548977 -4.144249 -4.1670346 -4.1967535 -4.2208652 -4.229619 -4.2215014 -4.2049417 -4.1936007][-4.2530475 -4.2487478 -4.238451 -4.2265568 -4.214891 -4.2067766 -4.201335 -4.1977978 -4.21207 -4.2268138 -4.2369432 -4.2406917 -4.2330904 -4.2199697 -4.2114177][-4.248734 -4.2493649 -4.2482147 -4.2457776 -4.2410803 -4.2371154 -4.2343392 -4.2311511 -4.2370172 -4.242631 -4.2458215 -4.2485256 -4.2427835 -4.2308836 -4.2236552][-4.2474256 -4.2492685 -4.2515025 -4.2531657 -4.2535152 -4.2530603 -4.2496381 -4.2454944 -4.2468753 -4.2487297 -4.2508316 -4.2539964 -4.251081 -4.241322 -4.2350736][-4.2455244 -4.2461705 -4.248105 -4.2519984 -4.2575183 -4.2608576 -4.2598672 -4.2573681 -4.2571392 -4.2548442 -4.2537713 -4.2541456 -4.2520247 -4.2452965 -4.2404141]]...]
INFO - root - 2017-12-07 22:36:07.031773: step 22310, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 35h:37m:34s remains)
INFO - root - 2017-12-07 22:36:27.799684: step 22320, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.136 sec/batch; 36h:05m:08s remains)
INFO - root - 2017-12-07 22:36:49.127473: step 22330, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.104 sec/batch; 35h:32m:10s remains)
INFO - root - 2017-12-07 22:37:10.472479: step 22340, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 35h:36m:09s remains)
INFO - root - 2017-12-07 22:37:31.501669: step 22350, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.082 sec/batch; 35h:08m:23s remains)
INFO - root - 2017-12-07 22:37:52.830861: step 22360, loss = 2.07, batch loss = 2.02 (15.5 examples/sec; 2.064 sec/batch; 34h:50m:05s remains)
INFO - root - 2017-12-07 22:38:14.003152: step 22370, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 35h:55m:25s remains)
INFO - root - 2017-12-07 22:38:34.948044: step 22380, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 1.916 sec/batch; 32h:19m:49s remains)
INFO - root - 2017-12-07 22:38:56.264734: step 22390, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 35h:51m:48s remains)
INFO - root - 2017-12-07 22:39:17.410319: step 22400, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.118 sec/batch; 35h:43m:22s remains)
2017-12-07 22:39:19.085194: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3173213 -4.3164425 -4.3108196 -4.3017879 -4.2906704 -4.2798634 -4.2733951 -4.2703595 -4.2698045 -4.2763624 -4.2890577 -4.3027425 -4.31103 -4.310297 -4.3031521][-4.2997813 -4.3026385 -4.30112 -4.2941422 -4.2831311 -4.2717204 -4.2610245 -4.2484183 -4.2394571 -4.2474627 -4.2660093 -4.2871065 -4.3032489 -4.3070283 -4.3017669][-4.2820396 -4.2898989 -4.2948251 -4.290216 -4.2772112 -4.2579131 -4.2313542 -4.1981812 -4.1755829 -4.191196 -4.2251725 -4.2586284 -4.2866478 -4.2991953 -4.3001723][-4.2756267 -4.2854095 -4.2954612 -4.2940264 -4.2759261 -4.2443962 -4.1967835 -4.1372933 -4.0989194 -4.1278977 -4.1802168 -4.2233772 -4.2601633 -4.2833171 -4.294198][-4.2755084 -4.2845159 -4.29607 -4.2969794 -4.2715492 -4.228972 -4.1585617 -4.0667281 -4.008256 -4.0556965 -4.129343 -4.1825738 -4.2256727 -4.259902 -4.2828474][-4.2771673 -4.2835612 -4.2927823 -4.2923832 -4.2579885 -4.2044 -4.1088886 -3.9751644 -3.8932271 -3.9728425 -4.0774441 -4.1476488 -4.19948 -4.2440319 -4.2759919][-4.2857027 -4.2863178 -4.2890444 -4.2822771 -4.2359591 -4.1670055 -4.0402894 -3.8563657 -3.7582233 -3.8868461 -4.0325212 -4.1271009 -4.1919966 -4.2442341 -4.2803278][-4.3022261 -4.2941928 -4.2872539 -4.2710056 -4.2132335 -4.1260552 -3.9712632 -3.7491746 -3.6512182 -3.8255761 -4.00516 -4.1215515 -4.1968122 -4.2518005 -4.2896419][-4.321424 -4.3039837 -4.2873368 -4.2650051 -4.209466 -4.1256824 -3.9840572 -3.7814915 -3.7080402 -3.8729906 -4.0325918 -4.1402612 -4.2102447 -4.262495 -4.300878][-4.3367982 -4.3147817 -4.2962828 -4.2760863 -4.2330832 -4.16631 -4.0598345 -3.9153149 -3.8724818 -3.9875791 -4.09837 -4.1766553 -4.2339191 -4.28009 -4.3168521][-4.3438911 -4.3213329 -4.3060074 -4.2931323 -4.2632585 -4.2061357 -4.127089 -4.0351691 -4.0143809 -4.0860567 -4.1593242 -4.2131248 -4.2568088 -4.2957373 -4.3288617][-4.3475451 -4.326961 -4.3130393 -4.3076186 -4.2881765 -4.240273 -4.1827126 -4.1302795 -4.1247621 -4.1661196 -4.2155457 -4.2491407 -4.2773957 -4.3071189 -4.335649][-4.349802 -4.3326774 -4.3179579 -4.31563 -4.3060622 -4.2705469 -4.2328734 -4.208828 -4.2117805 -4.2341084 -4.2685223 -4.2872443 -4.3011436 -4.3212638 -4.3419752][-4.3527021 -4.3368392 -4.3203382 -4.3194919 -4.3188643 -4.3002863 -4.2808914 -4.2760978 -4.2823925 -4.293807 -4.3139353 -4.3211026 -4.3237791 -4.3346438 -4.3466763][-4.3582163 -4.3425884 -4.3269696 -4.3273768 -4.3323131 -4.3286476 -4.3219485 -4.3256183 -4.3298783 -4.3348913 -4.3447022 -4.3447914 -4.3401141 -4.3448629 -4.3511739]]...]
INFO - root - 2017-12-07 22:39:40.089306: step 22410, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 1.906 sec/batch; 32h:08m:51s remains)
INFO - root - 2017-12-07 22:40:01.129466: step 22420, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 35h:23m:20s remains)
INFO - root - 2017-12-07 22:40:22.472072: step 22430, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.155 sec/batch; 36h:19m:37s remains)
INFO - root - 2017-12-07 22:40:43.693849: step 22440, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.083 sec/batch; 35h:06m:23s remains)
INFO - root - 2017-12-07 22:41:04.543346: step 22450, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 35h:33m:06s remains)
INFO - root - 2017-12-07 22:41:25.918343: step 22460, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.141 sec/batch; 36h:05m:09s remains)
INFO - root - 2017-12-07 22:41:47.273648: step 22470, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 35h:21m:48s remains)
INFO - root - 2017-12-07 22:42:08.026970: step 22480, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.127 sec/batch; 35h:49m:28s remains)
INFO - root - 2017-12-07 22:42:29.397036: step 22490, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.162 sec/batch; 36h:24m:47s remains)
INFO - root - 2017-12-07 22:42:50.585618: step 22500, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.074 sec/batch; 34h:55m:34s remains)
2017-12-07 22:42:52.101783: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3375759 -4.3303638 -4.3235912 -4.3183279 -4.3044071 -4.2805877 -4.2572379 -4.2441778 -4.2520418 -4.2728119 -4.2956982 -4.3116069 -4.3232651 -4.33295 -4.3409905][-4.34434 -4.3437233 -4.3429556 -4.3343244 -4.3076706 -4.2646146 -4.2207561 -4.1944547 -4.2050972 -4.2398448 -4.2774887 -4.3061967 -4.3259912 -4.3379622 -4.3451257][-4.350101 -4.3564925 -4.3612003 -4.3484797 -4.3079166 -4.2414141 -4.1703281 -4.1259446 -4.1406803 -4.19387 -4.2499118 -4.2937613 -4.3234339 -4.3383369 -4.3446841][-4.3564057 -4.3684754 -4.3776488 -4.3607664 -4.3056278 -4.2134027 -4.1109066 -4.0458503 -4.0691051 -4.1469612 -4.2224913 -4.2803245 -4.3203921 -4.34008 -4.3456445][-4.3615432 -4.3769989 -4.3888168 -4.3672342 -4.297843 -4.1802559 -4.0464783 -3.9606743 -3.9972005 -4.1036749 -4.1991625 -4.2692127 -4.3183603 -4.3432922 -4.3489][-4.3646331 -4.3818088 -4.3953733 -4.370266 -4.2896514 -4.1513267 -3.9903255 -3.885118 -3.9368711 -4.0698533 -4.1817 -4.261816 -4.317194 -4.34582 -4.352489][-4.3663993 -4.3845334 -4.3991508 -4.3732767 -4.2860165 -4.1345353 -3.9566951 -3.8417845 -3.9116218 -4.0652843 -4.1846008 -4.2656851 -4.3203368 -4.3500118 -4.35655][-4.3667107 -4.3845387 -4.3993559 -4.375576 -4.2891278 -4.1374817 -3.9633424 -3.8559222 -3.9375465 -4.0951743 -4.207727 -4.2786622 -4.3262153 -4.3547182 -4.360374][-4.3661971 -4.3824115 -4.39627 -4.3763785 -4.2987256 -4.1633725 -4.0157557 -3.9327056 -4.0099645 -4.1480737 -4.2408614 -4.2949052 -4.3330417 -4.35744 -4.3620038][-4.3645916 -4.3784981 -4.39125 -4.3746862 -4.3074107 -4.1924629 -4.0775275 -4.0220022 -4.0911293 -4.2040911 -4.2760015 -4.31448 -4.3417406 -4.3593612 -4.3611474][-4.3617964 -4.3732324 -4.3844886 -4.3696861 -4.3108025 -4.2137976 -4.1291795 -4.0972824 -4.1580477 -4.2488527 -4.3039432 -4.3318605 -4.3497739 -4.3602691 -4.3586607][-4.3588371 -4.3679757 -4.3772221 -4.363153 -4.3098326 -4.2282577 -4.1698647 -4.1563835 -4.2084522 -4.2807913 -4.3243489 -4.3440943 -4.3552966 -4.3604965 -4.355947][-4.3558521 -4.36208 -4.3687644 -4.35691 -4.3111262 -4.2467666 -4.2103562 -4.2086449 -4.25024 -4.3053226 -4.3381987 -4.3507686 -4.3572168 -4.3587441 -4.3523889][-4.3528786 -4.3553038 -4.3586459 -4.3494596 -4.3145618 -4.2679987 -4.2466135 -4.2504611 -4.2826962 -4.3223958 -4.345572 -4.3527608 -4.3553715 -4.3538227 -4.3469377][-4.3496861 -4.347775 -4.3474393 -4.3417988 -4.3199697 -4.2895451 -4.276226 -4.2805758 -4.3043885 -4.3324823 -4.3481526 -4.3507748 -4.350493 -4.3480043 -4.3423734]]...]
INFO - root - 2017-12-07 22:43:12.935523: step 22510, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 35h:25m:56s remains)
INFO - root - 2017-12-07 22:43:33.933399: step 22520, loss = 2.09, batch loss = 2.04 (15.5 examples/sec; 2.070 sec/batch; 34h:50m:29s remains)
INFO - root - 2017-12-07 22:43:55.115986: step 22530, loss = 2.09, batch loss = 2.03 (14.3 examples/sec; 2.235 sec/batch; 37h:37m:18s remains)
INFO - root - 2017-12-07 22:44:16.347287: step 22540, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.131 sec/batch; 35h:51m:43s remains)
INFO - root - 2017-12-07 22:44:37.295698: step 22550, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.116 sec/batch; 35h:36m:28s remains)
INFO - root - 2017-12-07 22:44:58.536626: step 22560, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.132 sec/batch; 35h:51m:54s remains)
INFO - root - 2017-12-07 22:45:19.675903: step 22570, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.138 sec/batch; 35h:58m:04s remains)
INFO - root - 2017-12-07 22:45:40.512661: step 22580, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.082 sec/batch; 35h:01m:01s remains)
INFO - root - 2017-12-07 22:46:01.639223: step 22590, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.148 sec/batch; 36h:06m:56s remains)
INFO - root - 2017-12-07 22:46:22.806116: step 22600, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 35h:12m:39s remains)
2017-12-07 22:46:24.380552: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2952852 -4.2835312 -4.2789173 -4.261909 -4.2398534 -4.2259293 -4.2262349 -4.2269125 -4.2282071 -4.2369843 -4.243073 -4.2431397 -4.245544 -4.2518892 -4.2454481][-4.2908015 -4.2793188 -4.2763157 -4.2582846 -4.2297759 -4.2099171 -4.2120447 -4.2181673 -4.2169375 -4.22292 -4.2269158 -4.2203455 -4.2141418 -4.214766 -4.2087111][-4.29298 -4.2835131 -4.280447 -4.262887 -4.2337117 -4.2089109 -4.2114086 -4.2217779 -4.2195888 -4.2230234 -4.2255464 -4.2154193 -4.204483 -4.1998224 -4.193603][-4.3001328 -4.2904243 -4.2826724 -4.2607007 -4.2270823 -4.1956048 -4.1975532 -4.2148991 -4.2184424 -4.22786 -4.2355695 -4.2300782 -4.2202687 -4.2149348 -4.2082772][-4.3036237 -4.2892427 -4.2718596 -4.2406669 -4.1966047 -4.1518507 -4.1463642 -4.1728411 -4.1944475 -4.2157378 -4.2332206 -4.2430506 -4.2434692 -4.2396541 -4.2332292][-4.3002715 -4.2765293 -4.2459917 -4.2054563 -4.1507692 -4.0858808 -4.060463 -4.0898209 -4.1333814 -4.1716623 -4.2062826 -4.2373791 -4.2502084 -4.2519155 -4.2476497][-4.2932978 -4.2610474 -4.2174468 -4.1703386 -4.1100388 -4.0291777 -3.9795878 -4.0018382 -4.0624819 -4.1204224 -4.1661267 -4.2075081 -4.2302942 -4.2384052 -4.2393961][-4.2801104 -4.2413325 -4.1885047 -4.1383891 -4.0834894 -4.0093994 -3.9518132 -3.9611042 -4.02512 -4.0912352 -4.1369224 -4.1736884 -4.1963754 -4.2035322 -4.20582][-4.2640853 -4.2227526 -4.1684132 -4.1195812 -4.0775619 -4.0271988 -3.9831529 -3.9841835 -4.036406 -4.097465 -4.1353331 -4.1596575 -4.1717281 -4.1708679 -4.1702118][-4.2556977 -4.2193828 -4.1721072 -4.1302681 -4.1011968 -4.0759964 -4.0496025 -4.0448771 -4.081562 -4.1307664 -4.1590095 -4.1715727 -4.1715975 -4.1582203 -4.1521182][-4.2535529 -4.2252688 -4.1913867 -4.1597075 -4.1412797 -4.1366744 -4.1287355 -4.123807 -4.1473351 -4.1845126 -4.2035661 -4.2043219 -4.191751 -4.1672087 -4.1537795][-4.2550273 -4.2356682 -4.2171669 -4.1998463 -4.1921306 -4.1987019 -4.2033691 -4.2011828 -4.2156119 -4.2407131 -4.2516742 -4.241478 -4.219141 -4.1935558 -4.1793528][-4.2650757 -4.2528133 -4.2462444 -4.2438641 -4.2467031 -4.2587337 -4.2688756 -4.267149 -4.2734571 -4.2868724 -4.2891312 -4.2726955 -4.2479315 -4.2266417 -4.2159858][-4.2783637 -4.2704525 -4.2716103 -4.2792826 -4.2888942 -4.303144 -4.3120589 -4.3078747 -4.3067012 -4.3094587 -4.3062658 -4.290977 -4.2713447 -4.2557445 -4.246376][-4.2919006 -4.2840118 -4.2865009 -4.2967839 -4.3078761 -4.3215046 -4.3295407 -4.325201 -4.319067 -4.3154087 -4.31006 -4.2981544 -4.283185 -4.2714038 -4.2631521]]...]
INFO - root - 2017-12-07 22:46:45.236643: step 22610, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.097 sec/batch; 35h:15m:08s remains)
INFO - root - 2017-12-07 22:47:06.453421: step 22620, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.118 sec/batch; 35h:35m:27s remains)
INFO - root - 2017-12-07 22:47:27.535321: step 22630, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.111 sec/batch; 35h:28m:23s remains)
INFO - root - 2017-12-07 22:47:48.353435: step 22640, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.069 sec/batch; 34h:45m:13s remains)
INFO - root - 2017-12-07 22:48:09.493555: step 22650, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.140 sec/batch; 35h:57m:13s remains)
INFO - root - 2017-12-07 22:48:30.874681: step 22660, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.062 sec/batch; 34h:38m:13s remains)
INFO - root - 2017-12-07 22:48:51.684914: step 22670, loss = 2.07, batch loss = 2.01 (17.0 examples/sec; 1.886 sec/batch; 31h:40m:35s remains)
INFO - root - 2017-12-07 22:49:12.909747: step 22680, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.169 sec/batch; 36h:25m:27s remains)
INFO - root - 2017-12-07 22:49:34.093886: step 22690, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.089 sec/batch; 35h:04m:03s remains)
INFO - root - 2017-12-07 22:49:55.390622: step 22700, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.143 sec/batch; 35h:58m:03s remains)
2017-12-07 22:49:56.944376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3089671 -4.31033 -4.3214726 -4.3310785 -4.3330317 -4.325604 -4.3123903 -4.2974453 -4.2798114 -4.2591782 -4.2416739 -4.2281284 -4.2195678 -4.2120833 -4.1938019][-4.2943225 -4.296423 -4.3101339 -4.3210087 -4.3222094 -4.3151608 -4.3015666 -4.2854357 -4.2641983 -4.235631 -4.208405 -4.1842294 -4.1713619 -4.1667261 -4.14811][-4.2813892 -4.2826586 -4.2953186 -4.3030667 -4.3004966 -4.2899027 -4.2739887 -4.2575693 -4.2370782 -4.2085943 -4.18016 -4.1513915 -4.1338387 -4.1266322 -4.1094346][-4.2806454 -4.2790623 -4.2837987 -4.2819552 -4.2722616 -4.2555456 -4.2349062 -4.217658 -4.2014375 -4.1807218 -4.1592903 -4.1371374 -4.1253681 -4.12101 -4.104229][-4.2888203 -4.2834225 -4.2752261 -4.2555728 -4.2321515 -4.2063589 -4.1831326 -4.1711578 -4.1638727 -4.1549215 -4.1461906 -4.1394434 -4.143178 -4.1512861 -4.1419239][-4.2941761 -4.2816262 -4.2573705 -4.217711 -4.1778216 -4.1437578 -4.1220927 -4.1178064 -4.12255 -4.1303658 -4.138308 -4.1488733 -4.166595 -4.1868253 -4.1889033][-4.2932129 -4.2717576 -4.2328539 -4.1812654 -4.1324267 -4.0948672 -4.0779181 -4.0776944 -4.0912361 -4.1135707 -4.1328249 -4.1483655 -4.1690168 -4.1954155 -4.2088885][-4.287147 -4.2595892 -4.2132678 -4.1594095 -4.1108818 -4.0754738 -4.0625215 -4.0611286 -4.0773978 -4.1039548 -4.1203332 -4.1256037 -4.142343 -4.1718307 -4.1941085][-4.2823114 -4.2516479 -4.2041554 -4.1530809 -4.1073408 -4.0720553 -4.0607657 -4.0582504 -4.0748386 -4.0952916 -4.0930719 -4.0779295 -4.0874381 -4.1249528 -4.1627154][-4.2815518 -4.2496228 -4.2028537 -4.1553116 -4.1087174 -4.071146 -4.0594196 -4.0564919 -4.0740919 -4.0886545 -4.0725546 -4.0396857 -4.0389447 -4.0848131 -4.1372337][-4.2834711 -4.2538977 -4.2117009 -4.1673722 -4.1193948 -4.082994 -4.0740709 -4.07708 -4.0933104 -4.1011362 -4.0810447 -4.0404954 -4.0266237 -4.0705681 -4.1280365][-4.2892365 -4.2651544 -4.2274008 -4.1840534 -4.1368237 -4.1054449 -4.102437 -4.112484 -4.1284056 -4.1328297 -4.1146288 -4.0724516 -4.0471973 -4.078115 -4.1289196][-4.299952 -4.2790813 -4.244422 -4.2019482 -4.1570816 -4.1334486 -4.1363111 -4.1493592 -4.1642861 -4.1704741 -4.1593533 -4.1233773 -4.0935836 -4.1073022 -4.142302][-4.3162961 -4.2992082 -4.2668071 -4.2236557 -4.182 -4.1640267 -4.1688843 -4.1818504 -4.1951294 -4.2023621 -4.1990643 -4.17725 -4.1527438 -4.1525211 -4.168335][-4.3314843 -4.31786 -4.28888 -4.2506847 -4.2172818 -4.2040009 -4.2065945 -4.2136068 -4.2211995 -4.2256145 -4.2289462 -4.2259703 -4.2170496 -4.2135468 -4.2142057]]...]
INFO - root - 2017-12-07 22:50:17.872178: step 22710, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.100 sec/batch; 35h:14m:36s remains)
INFO - root - 2017-12-07 22:50:39.208772: step 22720, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.114 sec/batch; 35h:28m:14s remains)
INFO - root - 2017-12-07 22:51:00.433795: step 22730, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.148 sec/batch; 36h:02m:22s remains)
INFO - root - 2017-12-07 22:51:21.554027: step 22740, loss = 2.08, batch loss = 2.02 (14.5 examples/sec; 2.208 sec/batch; 37h:01m:44s remains)
INFO - root - 2017-12-07 22:51:42.759350: step 22750, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.118 sec/batch; 35h:31m:39s remains)
INFO - root - 2017-12-07 22:52:03.991279: step 22760, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.107 sec/batch; 35h:19m:54s remains)
INFO - root - 2017-12-07 22:52:24.841451: step 22770, loss = 2.08, batch loss = 2.02 (14.4 examples/sec; 2.227 sec/batch; 37h:19m:50s remains)
INFO - root - 2017-12-07 22:52:46.215134: step 22780, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 35h:26m:31s remains)
INFO - root - 2017-12-07 22:53:07.505590: step 22790, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.126 sec/batch; 35h:37m:39s remains)
INFO - root - 2017-12-07 22:53:28.319664: step 22800, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.110 sec/batch; 35h:21m:25s remains)
2017-12-07 22:53:29.947434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1893129 -4.1815543 -4.1647048 -4.1273127 -4.0896831 -4.0839262 -4.1044092 -4.141645 -4.1798468 -4.1937366 -4.1780839 -4.1397004 -4.0863557 -4.065484 -4.0834188][-4.1832981 -4.1796412 -4.1688523 -4.1406755 -4.1169577 -4.123219 -4.1455917 -4.17442 -4.1982303 -4.1982603 -4.1737938 -4.129272 -4.0703917 -4.045331 -4.0669875][-4.1850271 -4.1894383 -4.18521 -4.16632 -4.1559944 -4.1674318 -4.1821585 -4.1926017 -4.1971068 -4.1865005 -4.1647086 -4.132237 -4.0869327 -4.0650768 -4.0816455][-4.2033415 -4.210979 -4.2097907 -4.19393 -4.1840391 -4.1869011 -4.1854186 -4.1746449 -4.1630888 -4.1484923 -4.138422 -4.1298685 -4.1136451 -4.1037049 -4.113615][-4.2148175 -4.2153215 -4.2094469 -4.187655 -4.1661329 -4.1526856 -4.1359019 -4.1123347 -4.0962191 -4.0893307 -4.0950708 -4.1092262 -4.1191878 -4.123126 -4.131793][-4.1846294 -4.1739869 -4.1611023 -4.1300178 -4.091073 -4.0600419 -4.0346079 -4.0125976 -4.0088787 -4.023809 -4.0487313 -4.0798273 -4.109684 -4.1244984 -4.1328135][-4.12523 -4.104166 -4.0880017 -4.0521235 -3.9988828 -3.9497836 -3.9160004 -3.9038372 -3.924572 -3.9732609 -4.0190921 -4.0571904 -4.0924816 -4.11425 -4.1224203][-4.0615244 -4.0338264 -4.0231752 -3.9995337 -3.951648 -3.8985968 -3.8667378 -3.8636243 -3.900831 -3.9659586 -4.0158772 -4.0472889 -4.0801787 -4.1059327 -4.1140375][-4.0241585 -3.9994745 -4.0060143 -4.0115337 -3.9912653 -3.9588854 -3.9454622 -3.94988 -3.9762149 -4.0220556 -4.0553608 -4.0704727 -4.0919752 -4.1148586 -4.1236138][-4.043385 -4.0340858 -4.0570283 -4.0837011 -4.0860238 -4.0721574 -4.0686312 -4.0707884 -4.0779252 -4.0972185 -4.1152744 -4.1193104 -4.1270304 -4.1389232 -4.1423025][-4.1038485 -4.1106744 -4.1421776 -4.1735187 -4.1815729 -4.1706548 -4.16007 -4.1504054 -4.1422033 -4.1444235 -4.1535416 -4.1558657 -4.1533103 -4.1498938 -4.1454434][-4.1664143 -4.1814628 -4.2104826 -4.2360716 -4.2420816 -4.228826 -4.2062712 -4.1813025 -4.1603212 -4.1545119 -4.1636305 -4.16874 -4.1637483 -4.1481175 -4.1354032][-4.2211924 -4.2343 -4.250792 -4.2638307 -4.2650933 -4.2502613 -4.2213335 -4.1888962 -4.1620541 -4.1536264 -4.1655908 -4.1752362 -4.1702161 -4.1467843 -4.1263485][-4.2659831 -4.2720308 -4.2759151 -4.2771497 -4.2736554 -4.2578392 -4.2288504 -4.1983614 -4.1740727 -4.1670089 -4.18028 -4.1911545 -4.1846962 -4.1507244 -4.1173897][-4.2945743 -4.2969275 -4.2932916 -4.2870617 -4.2785912 -4.2617483 -4.2349634 -4.2099509 -4.1932859 -4.1903353 -4.2020621 -4.2097073 -4.1959624 -4.1538219 -4.1094918]]...]
INFO - root - 2017-12-07 22:53:51.101613: step 22810, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 35h:20m:01s remains)
INFO - root - 2017-12-07 22:54:12.415371: step 22820, loss = 2.06, batch loss = 2.01 (14.8 examples/sec; 2.156 sec/batch; 36h:07m:16s remains)
INFO - root - 2017-12-07 22:54:33.205235: step 22830, loss = 2.07, batch loss = 2.01 (16.9 examples/sec; 1.898 sec/batch; 31h:47m:40s remains)
INFO - root - 2017-12-07 22:54:54.463788: step 22840, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.164 sec/batch; 36h:14m:03s remains)
INFO - root - 2017-12-07 22:55:15.687082: step 22850, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 35h:51m:50s remains)
INFO - root - 2017-12-07 22:55:36.968589: step 22860, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.123 sec/batch; 35h:32m:25s remains)
INFO - root - 2017-12-07 22:55:57.704970: step 22870, loss = 2.08, batch loss = 2.02 (15.6 examples/sec; 2.058 sec/batch; 34h:26m:25s remains)
INFO - root - 2017-12-07 22:56:18.860131: step 22880, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.087 sec/batch; 34h:55m:30s remains)
INFO - root - 2017-12-07 22:56:39.996149: step 22890, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.094 sec/batch; 35h:02m:09s remains)
INFO - root - 2017-12-07 22:57:00.785056: step 22900, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.073 sec/batch; 34h:40m:48s remains)
2017-12-07 22:57:02.311669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2411208 -4.2390585 -4.2438664 -4.2486463 -4.25047 -4.2480159 -4.2453113 -4.2465229 -4.2468767 -4.2461939 -4.2475004 -4.2547913 -4.270227 -4.2870455 -4.2942953][-4.2711682 -4.2753067 -4.2814312 -4.2847009 -4.2825146 -4.2754374 -4.2686915 -4.266881 -4.2659 -4.2651491 -4.2661324 -4.27147 -4.2843442 -4.2995038 -4.3054681][-4.29192 -4.2973537 -4.302763 -4.3039708 -4.2986956 -4.2899466 -4.2827439 -4.2817993 -4.2835021 -4.2851257 -4.2864323 -4.2896156 -4.3000531 -4.3124981 -4.3143644][-4.2957678 -4.2930069 -4.2916489 -4.289094 -4.2825022 -4.2763886 -4.275198 -4.2815056 -4.2901287 -4.2965 -4.2988873 -4.3010859 -4.3095417 -4.3193398 -4.3182211][-4.27358 -4.2637172 -4.2573256 -4.2523808 -4.245554 -4.2417336 -4.2488503 -4.2644029 -4.28001 -4.2898154 -4.2922959 -4.294662 -4.3024125 -4.3093534 -4.3059969][-4.2246618 -4.2088265 -4.2015553 -4.1998878 -4.1922989 -4.1829352 -4.1893816 -4.209115 -4.2314343 -4.24625 -4.2521544 -4.2582903 -4.2728081 -4.2861552 -4.2856426][-4.1563768 -4.1382871 -4.1338096 -4.13813 -4.1311579 -4.1137433 -4.1077833 -4.120018 -4.145472 -4.1691465 -4.1790257 -4.1874704 -4.2118144 -4.2402072 -4.2503476][-4.1125436 -4.0960193 -4.0927963 -4.0966039 -4.0880575 -4.0625181 -4.0379639 -4.0324373 -4.0549078 -4.0861883 -4.0979605 -4.1042142 -4.1337256 -4.1731658 -4.1936469][-4.1173778 -4.1029067 -4.1001186 -4.0985651 -4.0833955 -4.0475931 -4.0036321 -3.9786243 -3.9920008 -4.0271673 -4.0432043 -4.04624 -4.073009 -4.1145244 -4.1405783][-4.1630058 -4.1516876 -4.1486998 -4.1408725 -4.1201811 -4.0815339 -4.0307908 -3.9939792 -3.993103 -4.0231476 -4.0422459 -4.0438571 -4.0614662 -4.0963717 -4.12371][-4.2303495 -4.2231 -4.2206683 -4.2113113 -4.1922984 -4.16168 -4.1197472 -4.0848875 -4.0748534 -4.0947757 -4.1122766 -4.114264 -4.1214585 -4.1419086 -4.1636486][-4.299058 -4.2950444 -4.2956157 -4.2912407 -4.2816577 -4.2650285 -4.2403169 -4.217063 -4.2060728 -4.2134838 -4.2225804 -4.2242713 -4.2247381 -4.2323203 -4.2468662][-4.3470621 -4.345449 -4.3492069 -4.3517303 -4.352344 -4.3483882 -4.3402786 -4.3297973 -4.320715 -4.3192534 -4.3197207 -4.3182745 -4.3146095 -4.3151493 -4.3237014][-4.3743811 -4.3743715 -4.3781919 -4.3829904 -4.3878722 -4.3900476 -4.3892961 -4.3860812 -4.3800573 -4.3753066 -4.3710456 -4.3671188 -4.3631253 -4.3621116 -4.367084][-4.390563 -4.3924279 -4.394938 -4.3981142 -4.4011631 -4.4034629 -4.4038911 -4.4021206 -4.3980885 -4.3935995 -4.3896108 -4.3867073 -4.3846045 -4.3841267 -4.3870873]]...]
INFO - root - 2017-12-07 22:57:23.486813: step 22910, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 35h:31m:33s remains)
INFO - root - 2017-12-07 22:57:44.632459: step 22920, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.108 sec/batch; 35h:15m:13s remains)
INFO - root - 2017-12-07 22:58:05.582024: step 22930, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.136 sec/batch; 35h:42m:39s remains)
INFO - root - 2017-12-07 22:58:26.948553: step 22940, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.112 sec/batch; 35h:18m:06s remains)
INFO - root - 2017-12-07 22:58:48.112757: step 22950, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 35h:08m:47s remains)
INFO - root - 2017-12-07 22:59:09.012924: step 22960, loss = 2.06, batch loss = 2.01 (16.3 examples/sec; 1.963 sec/batch; 32h:48m:04s remains)
INFO - root - 2017-12-07 22:59:30.191292: step 22970, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.135 sec/batch; 35h:40m:11s remains)
INFO - root - 2017-12-07 22:59:51.378565: step 22980, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 35h:38m:34s remains)
INFO - root - 2017-12-07 23:00:12.571021: step 22990, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.140 sec/batch; 35h:44m:59s remains)
INFO - root - 2017-12-07 23:00:33.441645: step 23000, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.172 sec/batch; 36h:16m:10s remains)
2017-12-07 23:00:35.065726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2792687 -4.2722254 -4.2793202 -4.2886863 -4.2839231 -4.2697706 -4.2615762 -4.2559381 -4.2574358 -4.2674103 -4.278739 -4.2873106 -4.2917233 -4.2994285 -4.3038454][-4.24217 -4.2297735 -4.2356515 -4.2500873 -4.2458758 -4.229023 -4.2174859 -4.2067447 -4.2023654 -4.217485 -4.2396612 -4.2588511 -4.26925 -4.2787452 -4.2854528][-4.204051 -4.1840534 -4.1881323 -4.2062263 -4.2047548 -4.1881738 -4.1778474 -4.1601372 -4.1427846 -4.1586585 -4.1910434 -4.223299 -4.2437153 -4.2544951 -4.261816][-4.1841769 -4.15686 -4.15823 -4.1759267 -4.1742854 -4.1602035 -4.1492929 -4.1236577 -4.0973749 -4.1090131 -4.1428413 -4.1829653 -4.2112675 -4.2222519 -4.2331429][-4.1917272 -4.1597633 -4.1582718 -4.1741714 -4.1734929 -4.1627154 -4.1518254 -4.1226263 -4.0836725 -4.0765576 -4.0955844 -4.1370821 -4.1732731 -4.18946 -4.2080469][-4.2184539 -4.1884913 -4.1843314 -4.1942205 -4.1920962 -4.1827707 -4.175302 -4.1450424 -4.0877385 -4.0443807 -4.0352473 -4.0749936 -4.1277237 -4.1619349 -4.1939344][-4.2461019 -4.230844 -4.2241683 -4.2211127 -4.2120795 -4.2008424 -4.1947231 -4.1663857 -4.098 -4.02507 -3.9899526 -4.02413 -4.0853114 -4.1326656 -4.1780596][-4.2644777 -4.2623043 -4.2530608 -4.24056 -4.2199311 -4.2008233 -4.1978183 -4.1851068 -4.128603 -4.05839 -4.020956 -4.0437098 -4.0865088 -4.1230803 -4.1662388][-4.2791967 -4.2844925 -4.275085 -4.2550778 -4.2234874 -4.19604 -4.1957278 -4.1988015 -4.1685719 -4.1171727 -4.0856829 -4.0933671 -4.1085439 -4.124887 -4.1519804][-4.2938495 -4.299583 -4.2888174 -4.2636681 -4.2246881 -4.1936207 -4.1936626 -4.2045355 -4.19679 -4.1685762 -4.1451879 -4.1428432 -4.1361303 -4.1279283 -4.1359434][-4.2922683 -4.2938733 -4.2774673 -4.2512751 -4.2102427 -4.17897 -4.1794591 -4.1957154 -4.2007704 -4.1871905 -4.1743631 -4.1664367 -4.1370831 -4.0989342 -4.0885615][-4.2830143 -4.2757277 -4.253756 -4.2262983 -4.1855125 -4.1597171 -4.1622334 -4.1818552 -4.190455 -4.1817689 -4.1765766 -4.1688256 -4.1309261 -4.0775871 -4.0554266][-4.261188 -4.2508993 -4.228591 -4.2042513 -4.1693807 -4.15271 -4.1621847 -4.1869521 -4.1992364 -4.1885977 -4.1780705 -4.1676116 -4.1389828 -4.097702 -4.0747895][-4.2359409 -4.23218 -4.21619 -4.1960945 -4.17015 -4.1625443 -4.1773281 -4.2049561 -4.2192254 -4.2064414 -4.1863141 -4.1759763 -4.1603661 -4.1299057 -4.1018963][-4.2462544 -4.2516923 -4.2420692 -4.22295 -4.2000976 -4.1938677 -4.2021093 -4.2210937 -4.2314038 -4.2189126 -4.1908126 -4.1731286 -4.1578422 -4.1269054 -4.0881634]]...]
INFO - root - 2017-12-07 23:00:56.325724: step 23010, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.134 sec/batch; 35h:38m:21s remains)
INFO - root - 2017-12-07 23:01:17.347746: step 23020, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.070 sec/batch; 34h:33m:42s remains)
INFO - root - 2017-12-07 23:01:38.118483: step 23030, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.069 sec/batch; 34h:32m:36s remains)
INFO - root - 2017-12-07 23:01:59.254152: step 23040, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 35h:11m:56s remains)
INFO - root - 2017-12-07 23:02:20.343254: step 23050, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.062 sec/batch; 34h:24m:52s remains)
INFO - root - 2017-12-07 23:02:41.252035: step 23060, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.163 sec/batch; 36h:05m:13s remains)
INFO - root - 2017-12-07 23:03:02.314633: step 23070, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.132 sec/batch; 35h:34m:09s remains)
INFO - root - 2017-12-07 23:03:23.654217: step 23080, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 35h:27m:35s remains)
INFO - root - 2017-12-07 23:03:44.328288: step 23090, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.092 sec/batch; 34h:53m:01s remains)
INFO - root - 2017-12-07 23:04:05.288461: step 23100, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 35h:20m:18s remains)
2017-12-07 23:04:06.812229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2860465 -4.2741904 -4.2590356 -4.2465096 -4.2361255 -4.2319317 -4.2341948 -4.2467551 -4.258121 -4.26429 -4.2658854 -4.2686067 -4.2696304 -4.2680278 -4.2617474][-4.257216 -4.2386675 -4.2172036 -4.200738 -4.1896276 -4.1866226 -4.1907392 -4.2072606 -4.2190704 -4.22384 -4.2242322 -4.2303152 -4.2342749 -4.231051 -4.21779][-4.2318716 -4.21051 -4.1844616 -4.165061 -4.1553526 -4.1530266 -4.1570048 -4.1748214 -4.1883779 -4.1929893 -4.1890249 -4.194355 -4.200356 -4.1977162 -4.1802707][-4.2094669 -4.1854372 -4.1564364 -4.1339412 -4.1253438 -4.1235356 -4.1204205 -4.1307583 -4.1460171 -4.1547508 -4.1530252 -4.1581731 -4.1680088 -4.1707406 -4.1528482][-4.1948833 -4.1690059 -4.1381683 -4.1134024 -4.102829 -4.0921211 -4.068141 -4.0620756 -4.0832415 -4.1075168 -4.1175022 -4.129283 -4.1476169 -4.1546721 -4.137516][-4.1839962 -4.1564169 -4.1197772 -4.0890985 -4.0668173 -4.0312719 -3.9701505 -3.9447124 -3.9900081 -4.0481443 -4.0805454 -4.103559 -4.1291742 -4.1441112 -4.1322675][-4.1735516 -4.1419091 -4.0970993 -4.0561452 -4.013226 -3.9380383 -3.8260779 -3.7847078 -3.8785977 -3.9859259 -4.0489969 -4.0867667 -4.1160054 -4.1334453 -4.1280279][-4.1614604 -4.13139 -4.0848584 -4.0418081 -3.9889109 -3.8943131 -3.7706933 -3.7445157 -3.8676903 -3.9890668 -4.0575724 -4.0944967 -4.1200809 -4.1363149 -4.1330023][-4.1510572 -4.129961 -4.0984054 -4.0723586 -4.0344105 -3.9661458 -3.8885746 -3.887032 -3.972904 -4.05426 -4.0976572 -4.1189013 -4.1367741 -4.1519971 -4.1484923][-4.1546679 -4.1445622 -4.12876 -4.1161933 -4.092814 -4.0533485 -4.0129738 -4.0218754 -4.0701451 -4.113483 -4.1296744 -4.1296859 -4.1379714 -4.1515355 -4.1555343][-4.1742396 -4.16657 -4.1537771 -4.1477518 -4.136816 -4.118423 -4.1041665 -4.1171908 -4.1433287 -4.1621976 -4.1613274 -4.1509871 -4.1573968 -4.1692834 -4.1758294][-4.1990356 -4.188601 -4.1737375 -4.1719317 -4.1730614 -4.1697817 -4.1682711 -4.1811237 -4.192142 -4.1983957 -4.195 -4.1890335 -4.1969962 -4.2070231 -4.2080832][-4.2272992 -4.2128692 -4.1983242 -4.1982751 -4.2076492 -4.2119427 -4.2130427 -4.220521 -4.2194786 -4.2213612 -4.2230144 -4.2214127 -4.2293715 -4.2394633 -4.2366743][-4.2582994 -4.2461872 -4.2377949 -4.2410712 -4.2513356 -4.2544374 -4.2544045 -4.2562985 -4.2514844 -4.25454 -4.2595797 -4.2574668 -4.2612171 -4.2678552 -4.2618647][-4.2903304 -4.2817345 -4.2772079 -4.2817984 -4.2913365 -4.2945733 -4.2916713 -4.2900834 -4.288888 -4.29622 -4.3046646 -4.3048372 -4.3055592 -4.3059988 -4.2985587]]...]
INFO - root - 2017-12-07 23:04:27.844216: step 23110, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.161 sec/batch; 36h:01m:15s remains)
INFO - root - 2017-12-07 23:04:48.983896: step 23120, loss = 2.09, batch loss = 2.03 (16.4 examples/sec; 1.950 sec/batch; 32h:29m:58s remains)
INFO - root - 2017-12-07 23:05:10.015247: step 23130, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 34h:53m:29s remains)
INFO - root - 2017-12-07 23:05:31.147413: step 23140, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.068 sec/batch; 34h:27m:05s remains)
INFO - root - 2017-12-07 23:05:52.425417: step 23150, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.097 sec/batch; 34h:55m:48s remains)
INFO - root - 2017-12-07 23:06:13.296978: step 23160, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 35h:31m:57s remains)
INFO - root - 2017-12-07 23:06:34.626877: step 23170, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.136 sec/batch; 35h:34m:46s remains)
INFO - root - 2017-12-07 23:06:55.721290: step 23180, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.151 sec/batch; 35h:48m:59s remains)
INFO - root - 2017-12-07 23:07:16.665750: step 23190, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 35h:12m:39s remains)
INFO - root - 2017-12-07 23:07:37.834133: step 23200, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.097 sec/batch; 34h:54m:20s remains)
2017-12-07 23:07:39.458032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2021341 -4.2056179 -4.2095718 -4.2170267 -4.2337937 -4.2524204 -4.2674022 -4.2704062 -4.2641182 -4.2503519 -4.2313304 -4.2130842 -4.20042 -4.1919632 -4.1857214][-4.1847095 -4.1934628 -4.2031727 -4.2129726 -4.2320628 -4.2488723 -4.2608061 -4.262444 -4.2594347 -4.2498536 -4.2325191 -4.2154741 -4.2020597 -4.1901507 -4.180522][-4.21204 -4.223773 -4.2345362 -4.2415662 -4.2549624 -4.2609668 -4.2657833 -4.2673311 -4.2693009 -4.2669892 -4.2553263 -4.2448111 -4.23575 -4.2266054 -4.2171793][-4.2682014 -4.2790141 -4.2888083 -4.29007 -4.2918358 -4.2811308 -4.2799134 -4.2819538 -4.2846746 -4.2886758 -4.28428 -4.2834005 -4.2836814 -4.2833319 -4.2806869][-4.326149 -4.3314004 -4.3354516 -4.3239894 -4.3067851 -4.2753439 -4.2639484 -4.2648473 -4.272491 -4.2860947 -4.2900858 -4.3016882 -4.3137922 -4.3238707 -4.3308716][-4.3612232 -4.3569512 -4.348218 -4.3161755 -4.2768197 -4.222662 -4.1917706 -4.1881762 -4.2150192 -4.2515707 -4.2695866 -4.2882872 -4.307539 -4.3271732 -4.3437433][-4.3591132 -4.3427072 -4.31154 -4.2511821 -4.1864662 -4.1118507 -4.062746 -4.06554 -4.1241403 -4.194797 -4.2310286 -4.2545996 -4.2751517 -4.2995076 -4.325285][-4.3246131 -4.2907381 -4.2348285 -4.1493855 -4.0647588 -3.9754889 -3.9160223 -3.9343011 -4.0240736 -4.1256051 -4.1788607 -4.2090168 -4.2318468 -4.2571607 -4.2895117][-4.2701139 -4.21947 -4.1461425 -4.0501781 -3.9610381 -3.8699424 -3.8211336 -3.8665609 -3.9671841 -4.07489 -4.1371374 -4.1730466 -4.2006111 -4.2246475 -4.2570195][-4.2170277 -4.1555405 -4.0800962 -3.9947529 -3.9212599 -3.8537381 -3.8328807 -3.8965549 -3.9844613 -4.0765624 -4.13789 -4.1748347 -4.2035713 -4.2215524 -4.2458563][-4.1797056 -4.1212139 -4.0589356 -4.0004778 -3.9625731 -3.9305727 -3.9268897 -3.9807158 -4.0420461 -4.1122904 -4.169239 -4.2053103 -4.2274747 -4.2354083 -4.2480235][-4.1626353 -4.1189322 -4.0804229 -4.05104 -4.0458355 -4.0454412 -4.0483809 -4.085907 -4.1200895 -4.16722 -4.2122245 -4.2360253 -4.2434621 -4.2403655 -4.24473][-4.174119 -4.152288 -4.1387453 -4.133431 -4.1446157 -4.157855 -4.165236 -4.1884527 -4.2034273 -4.227191 -4.2531095 -4.2616458 -4.2563047 -4.2476606 -4.2476382][-4.2054977 -4.199007 -4.2033625 -4.2136817 -4.2322788 -4.2479811 -4.2531343 -4.2609696 -4.2628813 -4.2716451 -4.2838264 -4.2830939 -4.2745285 -4.2651505 -4.2642436][-4.2455487 -4.2453256 -4.2537465 -4.2687225 -4.2876768 -4.3004732 -4.3014812 -4.300684 -4.2992253 -4.3041539 -4.3103385 -4.3096581 -4.3060589 -4.3012652 -4.3008842]]...]
INFO - root - 2017-12-07 23:08:00.594232: step 23210, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.122 sec/batch; 35h:18m:31s remains)
INFO - root - 2017-12-07 23:08:21.539251: step 23220, loss = 2.06, batch loss = 2.00 (15.5 examples/sec; 2.066 sec/batch; 34h:22m:19s remains)
INFO - root - 2017-12-07 23:08:42.674643: step 23230, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.142 sec/batch; 35h:38m:00s remains)
INFO - root - 2017-12-07 23:09:03.763189: step 23240, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 35h:08m:54s remains)
INFO - root - 2017-12-07 23:09:24.670735: step 23250, loss = 2.08, batch loss = 2.02 (15.8 examples/sec; 2.027 sec/batch; 33h:42m:43s remains)
INFO - root - 2017-12-07 23:09:45.855180: step 23260, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.086 sec/batch; 34h:41m:46s remains)
INFO - root - 2017-12-07 23:10:07.050580: step 23270, loss = 2.06, batch loss = 2.00 (14.8 examples/sec; 2.161 sec/batch; 35h:55m:56s remains)
INFO - root - 2017-12-07 23:10:28.239839: step 23280, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 34h:53m:59s remains)
INFO - root - 2017-12-07 23:10:49.145706: step 23290, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.081 sec/batch; 34h:35m:05s remains)
INFO - root - 2017-12-07 23:11:10.477760: step 23300, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.127 sec/batch; 35h:20m:26s remains)
2017-12-07 23:11:12.104246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3082304 -4.3167448 -4.3191481 -4.3198824 -4.3140383 -4.3078279 -4.3027525 -4.2972436 -4.2987251 -4.3027673 -4.3095307 -4.3147368 -4.3075924 -4.3071265 -4.3223205][-4.3204479 -4.3294258 -4.3324814 -4.3338251 -4.3288693 -4.3241787 -4.3193879 -4.3143444 -4.3158097 -4.3171949 -4.3187442 -4.3191075 -4.3092332 -4.30698 -4.3198752][-4.32407 -4.33236 -4.3355751 -4.3356857 -4.3300219 -4.3244853 -4.3215981 -4.3200955 -4.3238297 -4.3236566 -4.3218131 -4.3196664 -4.3081646 -4.3027964 -4.3122792][-4.3154173 -4.3232203 -4.3244619 -4.3199339 -4.3079047 -4.2980213 -4.2960339 -4.2992859 -4.3094616 -4.313 -4.3127694 -4.3131256 -4.3050938 -4.298305 -4.3051453][-4.2912917 -4.29631 -4.2920742 -4.2788329 -4.2559867 -4.2390032 -4.2380004 -4.2500696 -4.2714472 -4.2828979 -4.2856579 -4.2894692 -4.2889738 -4.2856388 -4.2923536][-4.2500191 -4.2510185 -4.2375188 -4.213007 -4.175365 -4.1459146 -4.1428576 -4.1712089 -4.2152705 -4.2401223 -4.247972 -4.25564 -4.2640338 -4.26545 -4.2727289][-4.1973758 -4.1923895 -4.1706972 -4.1305418 -4.0711627 -4.0189843 -4.0103855 -4.0599303 -4.1317506 -4.1737294 -4.1892219 -4.2032623 -4.22186 -4.2321005 -4.2465439][-4.1443181 -4.1371427 -4.1145444 -4.06386 -3.9825332 -3.9025769 -3.8815339 -3.944942 -4.0378079 -4.0948005 -4.119997 -4.1452694 -4.1757088 -4.1970491 -4.2238936][-4.1216249 -4.1256351 -4.1193905 -4.0868878 -4.0217471 -3.9507613 -3.9270315 -3.976501 -4.054111 -4.106142 -4.1332326 -4.1586504 -4.1853061 -4.2057643 -4.2334418][-4.1469502 -4.1575713 -4.16891 -4.1664181 -4.1356549 -4.0923223 -4.0728068 -4.1005445 -4.1518931 -4.1903944 -4.212451 -4.2286453 -4.2398477 -4.2466087 -4.263618][-4.1929631 -4.1982813 -4.2117462 -4.2237339 -4.217854 -4.1957006 -4.1810164 -4.1941457 -4.2265906 -4.2543159 -4.272727 -4.282218 -4.2805758 -4.2765713 -4.287426][-4.2396574 -4.2388988 -4.2477136 -4.2594981 -4.2601843 -4.2494907 -4.2390676 -4.2434258 -4.2641292 -4.2856817 -4.3004918 -4.306263 -4.2982321 -4.2899828 -4.2991662][-4.2653861 -4.264617 -4.2711945 -4.2823825 -4.2841721 -4.2798328 -4.27553 -4.2760105 -4.2875948 -4.3011484 -4.3110476 -4.314817 -4.305687 -4.2984128 -4.308414][-4.2666721 -4.2714128 -4.2811852 -4.294734 -4.2990723 -4.29709 -4.296535 -4.29358 -4.2979732 -4.3061333 -4.3139224 -4.3190603 -4.3122435 -4.3084435 -4.3195052][-4.231751 -4.2449679 -4.2642293 -4.2843294 -4.2950239 -4.2978711 -4.29943 -4.295331 -4.2965608 -4.3030982 -4.3115754 -4.320632 -4.31721 -4.316606 -4.327795]]...]
INFO - root - 2017-12-07 23:11:33.170152: step 23310, loss = 2.09, batch loss = 2.03 (15.5 examples/sec; 2.063 sec/batch; 34h:16m:20s remains)
INFO - root - 2017-12-07 23:11:54.190414: step 23320, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.175 sec/batch; 36h:07m:33s remains)
INFO - root - 2017-12-07 23:12:15.402461: step 23330, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.198 sec/batch; 36h:30m:55s remains)
INFO - root - 2017-12-07 23:12:36.720451: step 23340, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.147 sec/batch; 35h:39m:15s remains)
INFO - root - 2017-12-07 23:12:57.566193: step 23350, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.102 sec/batch; 34h:54m:16s remains)
INFO - root - 2017-12-07 23:13:18.910243: step 23360, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.093 sec/batch; 34h:45m:16s remains)
INFO - root - 2017-12-07 23:13:40.115883: step 23370, loss = 2.06, batch loss = 2.01 (15.1 examples/sec; 2.117 sec/batch; 35h:08m:40s remains)
INFO - root - 2017-12-07 23:14:00.877195: step 23380, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.108 sec/batch; 34h:58m:59s remains)
INFO - root - 2017-12-07 23:14:22.097534: step 23390, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.108 sec/batch; 34h:58m:54s remains)
INFO - root - 2017-12-07 23:14:43.150716: step 23400, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 35h:49m:55s remains)
2017-12-07 23:14:44.776177: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2645383 -4.2625117 -4.2591085 -4.2519608 -4.24725 -4.2466497 -4.2500329 -4.2505169 -4.2495437 -4.2529869 -4.2609758 -4.2683806 -4.2760234 -4.2829742 -4.2892513][-4.2487392 -4.2470503 -4.2427526 -4.2344451 -4.2282271 -4.2264361 -4.2307258 -4.2343364 -4.2336049 -4.2361927 -4.2443452 -4.2512827 -4.2559056 -4.2590752 -4.2647886][-4.2318563 -4.2300682 -4.2262206 -4.2171211 -4.2069626 -4.2003088 -4.2064557 -4.2182908 -4.2239423 -4.2301445 -4.2415042 -4.2466826 -4.246285 -4.242136 -4.2430019][-4.2235193 -4.2197776 -4.2138438 -4.2014251 -4.1837029 -4.1668124 -4.1713295 -4.1920462 -4.206027 -4.2145562 -4.2263565 -4.2304811 -4.2287073 -4.2214131 -4.2211108][-4.2152548 -4.2084842 -4.1994314 -4.1806808 -4.15167 -4.1224322 -4.1210589 -4.1452732 -4.1646643 -4.17527 -4.1872516 -4.1938477 -4.1979647 -4.1967335 -4.2012839][-4.2070866 -4.1961894 -4.1816406 -4.1547151 -4.11908 -4.0839815 -4.0788074 -4.1006231 -4.1188579 -4.1342278 -4.1475234 -4.1562734 -4.1698246 -4.1784954 -4.1890759][-4.1854725 -4.1669836 -4.144383 -4.1089306 -4.0749288 -4.0481582 -4.0479445 -4.065855 -4.0836377 -4.1074367 -4.1262217 -4.1379476 -4.1607504 -4.1774149 -4.1904583][-4.1562133 -4.1301832 -4.1016316 -4.0618715 -4.0314789 -4.0159216 -4.0227561 -4.0385289 -4.054265 -4.0836883 -4.1093073 -4.1239533 -4.15077 -4.1732192 -4.1896114][-4.1369734 -4.107338 -4.0768948 -4.0368128 -4.0087094 -3.9947376 -4.0060983 -4.02346 -4.0379086 -4.0686197 -4.0973544 -4.1132984 -4.136519 -4.1595855 -4.1795807][-4.1355457 -4.1100464 -4.0858035 -4.0537424 -4.0303321 -4.0174475 -4.0313148 -4.053793 -4.0704613 -4.0984 -4.1249828 -4.1364675 -4.1481085 -4.1627927 -4.1810555][-4.1573462 -4.1412282 -4.1272454 -4.1054735 -4.0888729 -4.0784612 -4.09028 -4.1123443 -4.1291666 -4.1523356 -4.1765819 -4.183856 -4.1835804 -4.1879034 -4.201417][-4.19286 -4.1877532 -4.1829925 -4.1711783 -4.161417 -4.15361 -4.160089 -4.1754265 -4.18978 -4.20696 -4.2245321 -4.2270927 -4.2200851 -4.2190146 -4.2278156][-4.2363734 -4.2391362 -4.2406359 -4.2361612 -4.2300925 -4.22329 -4.2250171 -4.2333264 -4.2443175 -4.2559085 -4.264442 -4.2623034 -4.2539182 -4.2494297 -4.2522988][-4.2691331 -4.274128 -4.2776189 -4.2761097 -4.271595 -4.2652535 -4.2643051 -4.2677464 -4.2747416 -4.2822318 -4.2854128 -4.2814412 -4.274776 -4.2702918 -4.2695479][-4.28298 -4.2865438 -4.2889318 -4.2884064 -4.2857265 -4.2817755 -4.2804461 -4.2818284 -4.2857776 -4.2903924 -4.2917805 -4.2883344 -4.2841406 -4.2811489 -4.2802134]]...]
INFO - root - 2017-12-07 23:15:05.746110: step 23410, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 1.915 sec/batch; 31h:45m:31s remains)
INFO - root - 2017-12-07 23:15:26.642554: step 23420, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.136 sec/batch; 35h:25m:48s remains)
INFO - root - 2017-12-07 23:15:47.841625: step 23430, loss = 2.08, batch loss = 2.02 (15.6 examples/sec; 2.053 sec/batch; 34h:02m:30s remains)
INFO - root - 2017-12-07 23:16:09.007363: step 23440, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.131 sec/batch; 35h:20m:12s remains)
INFO - root - 2017-12-07 23:16:29.834113: step 23450, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.106 sec/batch; 34h:54m:31s remains)
INFO - root - 2017-12-07 23:16:51.109240: step 23460, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.135 sec/batch; 35h:22m:37s remains)
INFO - root - 2017-12-07 23:17:12.190991: step 23470, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.126 sec/batch; 35h:13m:52s remains)
INFO - root - 2017-12-07 23:17:33.039528: step 23480, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.117 sec/batch; 35h:04m:17s remains)
INFO - root - 2017-12-07 23:17:54.179714: step 23490, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.129 sec/batch; 35h:16m:29s remains)
INFO - root - 2017-12-07 23:18:15.207435: step 23500, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.062 sec/batch; 34h:09m:20s remains)
2017-12-07 23:18:16.766083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2825665 -4.2964249 -4.3173437 -4.3261948 -4.3075938 -4.2612834 -4.2129927 -4.182271 -4.1819363 -4.214323 -4.2633171 -4.29962 -4.3107476 -4.3037081 -4.2868991][-4.27085 -4.2763605 -4.2932477 -4.3071237 -4.2976193 -4.2602377 -4.21707 -4.1893177 -4.18723 -4.2078767 -4.2492666 -4.2809191 -4.2876225 -4.280323 -4.2694197][-4.2598906 -4.2585578 -4.2700472 -4.286016 -4.28683 -4.2595367 -4.2179656 -4.1877432 -4.1829715 -4.1989188 -4.2367935 -4.2685556 -4.2782464 -4.2728071 -4.2680368][-4.2335768 -4.231987 -4.2483058 -4.2705379 -4.2831774 -4.2618251 -4.2179842 -4.1833172 -4.1783133 -4.200326 -4.2391167 -4.267993 -4.27563 -4.2705359 -4.2688165][-4.2076526 -4.2105241 -4.2344909 -4.2629766 -4.2797518 -4.2551208 -4.1997247 -4.1515884 -4.1472631 -4.1809258 -4.2250476 -4.2552533 -4.2701159 -4.2714009 -4.2753978][-4.1906843 -4.1955457 -4.22045 -4.2476749 -4.2607126 -4.2259536 -4.147325 -4.073606 -4.059268 -4.1067524 -4.1654229 -4.2101164 -4.2378855 -4.2503757 -4.2641354][-4.1855364 -4.1858406 -4.2040634 -4.2244167 -4.2258744 -4.1742516 -4.0675526 -3.9593706 -3.9278917 -3.9941807 -4.078764 -4.1456447 -4.1881385 -4.211678 -4.232626][-4.2229156 -4.220633 -4.2251668 -4.2264009 -4.2055426 -4.1341887 -4.0079446 -3.8742232 -3.8216226 -3.9037237 -4.0164089 -4.1052489 -4.1648707 -4.2008715 -4.2279782][-4.2660036 -4.2660809 -4.2649312 -4.2519183 -4.2177544 -4.1469436 -4.0342755 -3.9109471 -3.8567057 -3.93976 -4.0592709 -4.1537962 -4.2168846 -4.249898 -4.2657981][-4.297883 -4.3032579 -4.3017459 -4.2882681 -4.2596807 -4.2067003 -4.1278648 -4.0408659 -3.9990003 -4.0590029 -4.152657 -4.2305217 -4.282115 -4.3046937 -4.3059464][-4.3182859 -4.3243933 -4.3229122 -4.3131785 -4.2948728 -4.2622724 -4.2145257 -4.1640472 -4.1386976 -4.1725297 -4.2282376 -4.2768044 -4.3073621 -4.3150258 -4.3044562][-4.3337636 -4.3368273 -4.333107 -4.3287787 -4.3215451 -4.3057151 -4.2807322 -4.25487 -4.2390857 -4.2529635 -4.2770729 -4.29696 -4.3068686 -4.3021574 -4.2892475][-4.3429627 -4.3425879 -4.3391047 -4.3393536 -4.3390336 -4.3309665 -4.3166008 -4.3033814 -4.29154 -4.2968397 -4.3045921 -4.3059096 -4.3012857 -4.2879548 -4.2774124][-4.3366928 -4.3331766 -4.3288074 -4.3289313 -4.3284726 -4.3226795 -4.3139787 -4.30783 -4.295599 -4.2926283 -4.29181 -4.28592 -4.2763777 -4.2639027 -4.2595429][-4.3414645 -4.3362474 -4.3292627 -4.3250489 -4.32009 -4.3121042 -4.3039203 -4.2994785 -4.2903342 -4.2833371 -4.2779579 -4.2743206 -4.2700195 -4.2634525 -4.2649055]]...]
INFO - root - 2017-12-07 23:18:37.665288: step 23510, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 35h:05m:03s remains)
INFO - root - 2017-12-07 23:18:58.720832: step 23520, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 35h:01m:16s remains)
INFO - root - 2017-12-07 23:19:19.893656: step 23530, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.095 sec/batch; 34h:40m:56s remains)
INFO - root - 2017-12-07 23:19:40.781010: step 23540, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 1.904 sec/batch; 31h:30m:26s remains)
INFO - root - 2017-12-07 23:20:01.956164: step 23550, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.132 sec/batch; 35h:17m:13s remains)
INFO - root - 2017-12-07 23:20:23.341659: step 23560, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 34h:39m:14s remains)
INFO - root - 2017-12-07 23:20:44.368939: step 23570, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.073 sec/batch; 34h:17m:39s remains)
INFO - root - 2017-12-07 23:21:05.251371: step 23580, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 35h:20m:44s remains)
INFO - root - 2017-12-07 23:21:26.599692: step 23590, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.098 sec/batch; 34h:42m:06s remains)
INFO - root - 2017-12-07 23:21:47.704766: step 23600, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.154 sec/batch; 35h:37m:21s remains)
2017-12-07 23:21:49.287926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2397327 -4.2316055 -4.226985 -4.2205229 -4.210155 -4.1934996 -4.1746135 -4.1606145 -4.1577697 -4.1635561 -4.1775765 -4.1898031 -4.2005672 -4.2188935 -4.239635][-4.2098155 -4.2029381 -4.1988955 -4.1970644 -4.1938281 -4.1875849 -4.1781025 -4.1652417 -4.1568384 -4.1552267 -4.164825 -4.1761732 -4.18653 -4.2012925 -4.2222929][-4.1770215 -4.1772251 -4.1792684 -4.1837363 -4.187336 -4.1878967 -4.1817207 -4.1657839 -4.153801 -4.1489816 -4.1567011 -4.1703167 -4.1825223 -4.1947894 -4.2147112][-4.1560693 -4.1684861 -4.1788683 -4.1869922 -4.1932187 -4.1917477 -4.1822991 -4.1621704 -4.144845 -4.1396093 -4.1504803 -4.1667838 -4.1799431 -4.189774 -4.2087674][-4.1491013 -4.1666975 -4.1793971 -4.187027 -4.1927938 -4.1890416 -4.1771021 -4.1525722 -4.1298676 -4.127615 -4.1453853 -4.1651397 -4.1746793 -4.1762342 -4.1889405][-4.1501927 -4.1650281 -4.1752033 -4.1787767 -4.1809182 -4.1798425 -4.1698947 -4.1423883 -4.1174111 -4.1199727 -4.1456761 -4.1678848 -4.1712685 -4.1609979 -4.1654696][-4.1620378 -4.1695571 -4.1733713 -4.1746368 -4.1789455 -4.1866612 -4.1846766 -4.1609793 -4.1370211 -4.1393361 -4.1638947 -4.1818953 -4.1760893 -4.1532845 -4.1514978][-4.1852803 -4.1834388 -4.1797385 -4.1816125 -4.19287 -4.2094321 -4.21277 -4.1923637 -4.1701751 -4.16714 -4.1821284 -4.1944594 -4.1868043 -4.1653891 -4.1656775][-4.1803613 -4.1720867 -4.1656356 -4.1733189 -4.1926475 -4.2147279 -4.2224431 -4.2101073 -4.1938643 -4.1888156 -4.196835 -4.2068872 -4.2053957 -4.1958356 -4.2011642][-4.1459622 -4.1387463 -4.1367955 -4.1502542 -4.1759968 -4.2024322 -4.2157803 -4.2127013 -4.2059546 -4.2044997 -4.2091284 -4.2174907 -4.2216115 -4.2211094 -4.2281237][-4.1310277 -4.1267757 -4.126492 -4.1392217 -4.1618757 -4.184042 -4.1950917 -4.1952019 -4.1948047 -4.1964378 -4.1997681 -4.2070723 -4.2139964 -4.2174735 -4.2234521][-4.1646123 -4.1605172 -4.1570058 -4.1617246 -4.17547 -4.1897931 -4.196681 -4.1973629 -4.1990843 -4.2009211 -4.2022471 -4.2057557 -4.2095628 -4.2095013 -4.2091107][-4.2271233 -4.2210746 -4.214509 -4.2119713 -4.2174568 -4.2257471 -4.2289095 -4.2291265 -4.2306333 -4.231956 -4.2326546 -4.234138 -4.2345567 -4.2297082 -4.22228][-4.2736864 -4.2659893 -4.257967 -4.2526703 -4.254148 -4.2584 -4.2601371 -4.2604318 -4.2609649 -4.2612181 -4.2608714 -4.2598948 -4.2567992 -4.2486186 -4.2376685][-4.2962613 -4.2885656 -4.281558 -4.2766695 -4.2761474 -4.2777662 -4.2789993 -4.2796884 -4.2800994 -4.27965 -4.2784362 -4.2759767 -4.270359 -4.2602057 -4.2492275]]...]
INFO - root - 2017-12-07 23:22:10.117181: step 23610, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 35h:06m:44s remains)
INFO - root - 2017-12-07 23:22:31.346119: step 23620, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 35h:19m:41s remains)
INFO - root - 2017-12-07 23:22:52.587703: step 23630, loss = 2.08, batch loss = 2.03 (14.1 examples/sec; 2.264 sec/batch; 37h:24m:36s remains)
INFO - root - 2017-12-07 23:23:13.707517: step 23640, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.143 sec/batch; 35h:24m:46s remains)
INFO - root - 2017-12-07 23:23:34.796460: step 23650, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.138 sec/batch; 35h:19m:02s remains)
INFO - root - 2017-12-07 23:23:56.074313: step 23660, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.146 sec/batch; 35h:27m:10s remains)
INFO - root - 2017-12-07 23:24:16.714029: step 23670, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 2.058 sec/batch; 33h:59m:01s remains)
INFO - root - 2017-12-07 23:24:37.877569: step 23680, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.182 sec/batch; 36h:01m:26s remains)
INFO - root - 2017-12-07 23:24:59.079569: step 23690, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.075 sec/batch; 34h:15m:11s remains)
INFO - root - 2017-12-07 23:25:20.295407: step 23700, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.079 sec/batch; 34h:18m:45s remains)
2017-12-07 23:25:21.706726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2381353 -4.2348695 -4.2151384 -4.1980319 -4.1928926 -4.1861162 -4.1806269 -4.175231 -4.170104 -4.185637 -4.2076507 -4.2063351 -4.1882582 -4.1699128 -4.1571779][-4.2338519 -4.2314725 -4.2116342 -4.192009 -4.1843796 -4.1795712 -4.1800675 -4.1789012 -4.1767216 -4.1917562 -4.209506 -4.207253 -4.1921506 -4.1772332 -4.166306][-4.2347388 -4.2320104 -4.2068863 -4.1841321 -4.1753154 -4.1740251 -4.1793871 -4.1819415 -4.182374 -4.1970034 -4.2099328 -4.2068014 -4.1961408 -4.1865706 -4.1809573][-4.241673 -4.2368312 -4.2042117 -4.1734948 -4.1589413 -4.1596141 -4.1703444 -4.1780586 -4.1850719 -4.20253 -4.2116942 -4.2073011 -4.1994481 -4.1946507 -4.1944575][-4.2559738 -4.2489996 -4.2099109 -4.1693006 -4.1464558 -4.1427956 -4.1519637 -4.1607566 -4.1744528 -4.1972666 -4.2039852 -4.198977 -4.1937175 -4.1922255 -4.1948147][-4.259387 -4.252038 -4.2111635 -4.1680379 -4.1400938 -4.1277065 -4.1243625 -4.1253324 -4.1402349 -4.1644835 -4.1711707 -4.1691132 -4.1692996 -4.1744523 -4.1798115][-4.2412357 -4.2354631 -4.1988492 -4.1560493 -4.1251807 -4.1038175 -4.0846791 -4.0704641 -4.0809779 -4.1097612 -4.1241961 -4.1336646 -4.1452527 -4.1600413 -4.1715164][-4.2112966 -4.2067766 -4.1762605 -4.136198 -4.0995355 -4.0625777 -4.0189891 -3.9858105 -3.9928167 -4.0353293 -4.0719376 -4.1049948 -4.1338248 -4.15965 -4.1777716][-4.1780148 -4.1714458 -4.146008 -4.1161194 -4.0827312 -4.0356526 -3.9710097 -3.9235237 -3.9272733 -3.9815748 -4.0384059 -4.0943084 -4.13911 -4.1725659 -4.19234][-4.1767039 -4.1709871 -4.1516805 -4.1298819 -4.1092148 -4.0691438 -4.006454 -3.9634748 -3.9640031 -4.007277 -4.0594077 -4.1166153 -4.1642027 -4.1944747 -4.2085633][-4.2036476 -4.2018423 -4.1890917 -4.1764207 -4.1653686 -4.1355467 -4.0851583 -4.0495911 -4.0432644 -4.0684628 -4.1066236 -4.15649 -4.2013083 -4.2236967 -4.232316][-4.2325625 -4.2319164 -4.2250462 -4.2172427 -4.2067351 -4.1828184 -4.1451783 -4.1151214 -4.1008778 -4.1116982 -4.1419811 -4.1861339 -4.2284102 -4.2454214 -4.2516708][-4.2422166 -4.2388048 -4.2317171 -4.2219987 -4.2068062 -4.1874666 -4.1631045 -4.1407852 -4.1251965 -4.1296363 -4.156527 -4.1962996 -4.2337112 -4.247386 -4.250526][-4.2249312 -4.2188816 -4.2118917 -4.1996441 -4.1836972 -4.1678505 -4.1522217 -4.1378794 -4.1261549 -4.1292429 -4.1525397 -4.1857147 -4.2153864 -4.2263026 -4.2309165][-4.1893048 -4.1855855 -4.1816859 -4.1693363 -4.1544075 -4.1416264 -4.13136 -4.1239257 -4.1179967 -4.12112 -4.1371508 -4.1590223 -4.1780438 -4.1860476 -4.1969047]]...]
INFO - root - 2017-12-07 23:25:42.717441: step 23710, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.086 sec/batch; 34h:25m:10s remains)
INFO - root - 2017-12-07 23:26:03.814734: step 23720, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.110 sec/batch; 34h:48m:44s remains)
INFO - root - 2017-12-07 23:26:24.984927: step 23730, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.169 sec/batch; 35h:46m:53s remains)
INFO - root - 2017-12-07 23:26:45.955808: step 23740, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.187 sec/batch; 36h:04m:57s remains)
INFO - root - 2017-12-07 23:27:07.167819: step 23750, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.076 sec/batch; 34h:14m:40s remains)
INFO - root - 2017-12-07 23:27:28.384677: step 23760, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.103 sec/batch; 34h:40m:36s remains)
INFO - root - 2017-12-07 23:27:49.012584: step 23770, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.088 sec/batch; 34h:25m:36s remains)
INFO - root - 2017-12-07 23:28:10.190781: step 23780, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 35h:11m:54s remains)
INFO - root - 2017-12-07 23:28:31.504695: step 23790, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 34h:53m:59s remains)
INFO - root - 2017-12-07 23:28:52.429060: step 23800, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 34h:39m:12s remains)
2017-12-07 23:28:54.073850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2002487 -4.1981826 -4.2278519 -4.2646875 -4.285378 -4.2931662 -4.2955675 -4.2959156 -4.2963586 -4.2970462 -4.2984104 -4.3033352 -4.3064828 -4.3065476 -4.3074589][-4.1493053 -4.1385903 -4.1709609 -4.216743 -4.2426553 -4.2523832 -4.259068 -4.2625732 -4.2648396 -4.2671695 -4.2705369 -4.2786994 -4.2861691 -4.2887692 -4.2911534][-4.1226988 -4.1053171 -4.1345716 -4.1790676 -4.202281 -4.2124591 -4.2260532 -4.2341914 -4.2354865 -4.2345619 -4.2343926 -4.2412157 -4.2516356 -4.2595057 -4.2670994][-4.1150579 -4.0920634 -4.1112733 -4.1438756 -4.1594977 -4.1706033 -4.1953106 -4.210072 -4.2098351 -4.2053576 -4.1997151 -4.199007 -4.2100134 -4.2285132 -4.2480307][-4.1144185 -4.0795879 -4.0854025 -4.106771 -4.1161389 -4.127759 -4.1641927 -4.1851125 -4.1818457 -4.1778455 -4.1704588 -4.1604862 -4.1710558 -4.202219 -4.2376852][-4.102716 -4.0533843 -4.045269 -4.0624595 -4.0715256 -4.0827069 -4.1218119 -4.1359439 -4.1261325 -4.1294188 -4.1314964 -4.1219387 -4.1330867 -4.1736112 -4.2226696][-4.0783114 -4.0186729 -3.995815 -4.0033846 -4.0051088 -4.0066957 -4.0327158 -4.028594 -4.0098643 -4.032567 -4.0707192 -4.0881572 -4.1103892 -4.1539903 -4.2035971][-4.0656848 -4.0081921 -3.9773393 -3.9715061 -3.9639714 -3.9510667 -3.9536865 -3.9289155 -3.9063394 -3.9490688 -4.0248985 -4.0754857 -4.1107984 -4.1526036 -4.1974225][-4.1045585 -4.0639954 -4.0379295 -4.0299277 -4.0190468 -4.0018635 -3.9942298 -3.971509 -3.9566681 -3.9947779 -4.0630517 -4.1115 -4.146172 -4.1824379 -4.2199244][-4.1686659 -4.1502905 -4.1369519 -4.1346054 -4.1317511 -4.1201191 -4.1145139 -4.0974383 -4.0857258 -4.107862 -4.1500988 -4.1816888 -4.2062707 -4.2341089 -4.2628493][-4.22993 -4.2267418 -4.223711 -4.2295508 -4.235393 -4.2303395 -4.2264471 -4.21275 -4.19818 -4.2071328 -4.23267 -4.25375 -4.2683396 -4.2867332 -4.3069658][-4.26995 -4.2727571 -4.2764139 -4.2858343 -4.2949767 -4.2975903 -4.2957249 -4.2836905 -4.2702842 -4.2735462 -4.2883353 -4.3029613 -4.3126 -4.323772 -4.3376584][-4.3024197 -4.3074441 -4.3124814 -4.3200951 -4.3270788 -4.3313551 -4.3321228 -4.3258052 -4.31826 -4.3216591 -4.3310342 -4.3393207 -4.3447194 -4.35047 -4.35698][-4.3325295 -4.3394523 -4.3427916 -4.3454137 -4.3482718 -4.3506646 -4.3516855 -4.3502483 -4.3483996 -4.3526211 -4.359128 -4.363184 -4.3656292 -4.3665571 -4.3665338][-4.3480635 -4.3541837 -4.3555808 -4.3546352 -4.3534136 -4.3539047 -4.3548555 -4.355989 -4.3575292 -4.3611584 -4.3649349 -4.3668971 -4.3678713 -4.3672867 -4.3660784]]...]
INFO - root - 2017-12-07 23:29:15.216854: step 23810, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.131 sec/batch; 35h:06m:54s remains)
INFO - root - 2017-12-07 23:29:36.240844: step 23820, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 35h:08m:42s remains)
INFO - root - 2017-12-07 23:29:57.178768: step 23830, loss = 2.07, batch loss = 2.01 (16.4 examples/sec; 1.946 sec/batch; 32h:03m:07s remains)
INFO - root - 2017-12-07 23:30:18.362774: step 23840, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.064 sec/batch; 33h:59m:29s remains)
INFO - root - 2017-12-07 23:30:39.637209: step 23850, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.169 sec/batch; 35h:43m:09s remains)
INFO - root - 2017-12-07 23:31:01.017068: step 23860, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.087 sec/batch; 34h:21m:11s remains)
INFO - root - 2017-12-07 23:31:21.625320: step 23870, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.090 sec/batch; 34h:24m:21s remains)
INFO - root - 2017-12-07 23:31:42.811400: step 23880, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.130 sec/batch; 35h:03m:39s remains)
INFO - root - 2017-12-07 23:32:03.980264: step 23890, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 34h:52m:01s remains)
INFO - root - 2017-12-07 23:32:24.948414: step 23900, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.142 sec/batch; 35h:14m:19s remains)
2017-12-07 23:32:26.517050: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3411865 -4.3362222 -4.33122 -4.3258591 -4.3180528 -4.3094573 -4.2984567 -4.2846293 -4.2744217 -4.273437 -4.2812304 -4.2861342 -4.2900968 -4.2957697 -4.2908368][-4.3480086 -4.3408542 -4.3321352 -4.3212972 -4.307622 -4.2920604 -4.2735033 -4.2525029 -4.2411656 -4.2432842 -4.2504869 -4.24854 -4.2505088 -4.2557831 -4.2484169][-4.3549886 -4.3473387 -4.3374743 -4.3243022 -4.3093743 -4.291399 -4.2711844 -4.249568 -4.239429 -4.244863 -4.249465 -4.2417045 -4.2440996 -4.253921 -4.2472162][-4.3514795 -4.3431692 -4.3317418 -4.3158822 -4.3027921 -4.2849894 -4.2626262 -4.2399278 -4.23501 -4.24948 -4.2597318 -4.2497282 -4.2527552 -4.2652082 -4.2618141][-4.3282967 -4.3159494 -4.3021431 -4.2836294 -4.2708616 -4.2501507 -4.22156 -4.1926241 -4.1954422 -4.2238 -4.246583 -4.24026 -4.2366967 -4.247901 -4.2489924][-4.2968049 -4.2790937 -4.2609024 -4.2386293 -4.2234607 -4.1924567 -4.1487093 -4.1051316 -4.1134715 -4.1612267 -4.20068 -4.2024937 -4.1929812 -4.1976352 -4.1996408][-4.259316 -4.2300916 -4.201952 -4.1684327 -4.141953 -4.0976896 -4.0336037 -3.9672134 -3.9824066 -4.0563126 -4.1177015 -4.139071 -4.13773 -4.1412597 -4.1409864][-4.2233176 -4.1816082 -4.1419024 -4.0921183 -4.049139 -3.9915242 -3.9086242 -3.8204226 -3.844826 -3.9443345 -4.0223541 -4.0595026 -4.0665541 -4.0748577 -4.0794811][-4.2124262 -4.1711426 -4.1328173 -4.0840845 -4.0421748 -3.9915473 -3.9213378 -3.8504009 -3.8725638 -3.9479468 -3.9968426 -4.012146 -4.0033336 -3.9994237 -4.0003963][-4.2166638 -4.1814408 -4.1537333 -4.1230836 -4.1007261 -4.075006 -4.0405736 -4.0092959 -4.0194988 -4.0474625 -4.055769 -4.0389204 -4.0069356 -3.9803298 -3.9647336][-4.2160168 -4.185461 -4.169332 -4.1555877 -4.1514382 -4.147891 -4.1400123 -4.1304531 -4.1311107 -4.1317511 -4.1213779 -4.0942521 -4.0605493 -4.0308528 -4.0091076][-4.2203865 -4.1958055 -4.1885581 -4.1867247 -4.193398 -4.2004919 -4.2037492 -4.202024 -4.1993175 -4.1905165 -4.1782265 -4.1549683 -4.1285262 -4.1048851 -4.084415][-4.2411733 -4.2266903 -4.2265635 -4.232708 -4.2445178 -4.255332 -4.260087 -4.2593818 -4.2554283 -4.2458105 -4.235774 -4.22036 -4.202487 -4.1872215 -4.1724958][-4.2748003 -4.2700753 -4.2761245 -4.2863965 -4.2967763 -4.30439 -4.3076439 -4.3053036 -4.3002567 -4.2925196 -4.2854514 -4.2762537 -4.2659831 -4.2572117 -4.2482529][-4.3124371 -4.3139892 -4.3214307 -4.3295512 -4.3347898 -4.3359561 -4.335535 -4.3323812 -4.3285456 -4.3240819 -4.3192739 -4.3135533 -4.3081665 -4.3039451 -4.3007851]]...]
INFO - root - 2017-12-07 23:32:47.628449: step 23910, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 35h:05m:09s remains)
INFO - root - 2017-12-07 23:33:08.962275: step 23920, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.140 sec/batch; 35h:11m:33s remains)
INFO - root - 2017-12-07 23:33:29.569424: step 23930, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.062 sec/batch; 33h:54m:17s remains)
INFO - root - 2017-12-07 23:33:50.796143: step 23940, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.163 sec/batch; 35h:33m:29s remains)
INFO - root - 2017-12-07 23:34:11.860073: step 23950, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.163 sec/batch; 35h:32m:52s remains)
INFO - root - 2017-12-07 23:34:32.817076: step 23960, loss = 2.07, batch loss = 2.02 (14.7 examples/sec; 2.176 sec/batch; 35h:45m:13s remains)
INFO - root - 2017-12-07 23:34:53.955789: step 23970, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.146 sec/batch; 35h:15m:26s remains)
INFO - root - 2017-12-07 23:35:15.063077: step 23980, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.157 sec/batch; 35h:26m:37s remains)
INFO - root - 2017-12-07 23:35:36.195725: step 23990, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.117 sec/batch; 34h:46m:48s remains)
INFO - root - 2017-12-07 23:35:57.060999: step 24000, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 35h:01m:38s remains)
2017-12-07 23:35:58.709966: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31061 -4.2973881 -4.2635427 -4.2185965 -4.1734905 -4.154747 -4.1488152 -4.1514134 -4.1594396 -4.1749992 -4.171309 -4.1416836 -4.1116657 -4.0722318 -4.017374][-4.3072248 -4.2894468 -4.2511897 -4.2037811 -4.1606417 -4.1388364 -4.1242237 -4.1274538 -4.1402769 -4.158608 -4.1523991 -4.1289372 -4.119576 -4.1112523 -4.0839853][-4.3008218 -4.2778158 -4.2341943 -4.1860447 -4.1410236 -4.1140776 -4.0948882 -4.1079264 -4.1295967 -4.150743 -4.145577 -4.126698 -4.1264296 -4.1327772 -4.1154304][-4.2977581 -4.2730184 -4.22502 -4.1786246 -4.1315584 -4.0946884 -4.0769672 -4.0974112 -4.1274395 -4.1468692 -4.1347427 -4.1119742 -4.1073685 -4.1062489 -4.0843906][-4.2912292 -4.2627878 -4.2043233 -4.1469769 -4.0932603 -4.0506568 -4.0387034 -4.0697746 -4.1095004 -4.1286831 -4.1060128 -4.0672336 -4.0382934 -4.0164404 -4.0082607][-4.2718449 -4.2319927 -4.1504717 -4.0598855 -3.9770505 -3.9264243 -3.9259002 -3.9829624 -4.0536804 -4.0928593 -4.0662107 -4.006124 -3.952791 -3.9344447 -3.9605269][-4.2508745 -4.1999378 -4.096323 -3.9639692 -3.8379149 -3.7788672 -3.7952662 -3.8843586 -3.9988093 -4.0736885 -4.0572114 -3.9907756 -3.925878 -3.913811 -3.9687667][-4.2499933 -4.20511 -4.1072564 -3.9800215 -3.8552508 -3.7940283 -3.8111873 -3.9051805 -4.0274048 -4.1110935 -4.1018438 -4.0383992 -3.9737163 -3.9616582 -4.0214944][-4.2636776 -4.2278295 -4.1512079 -4.0553994 -3.9677823 -3.9253016 -3.9359572 -4.0087128 -4.1051469 -4.1706562 -4.1577797 -4.0992146 -4.038868 -4.0241461 -4.07236][-4.2797418 -4.2507806 -4.1921921 -4.1214266 -4.0607262 -4.0361061 -4.0516071 -4.1099758 -4.1751871 -4.2142372 -4.2051039 -4.1569748 -4.0988846 -4.0669236 -4.0901942][-4.2956138 -4.2710156 -4.2251577 -4.1689153 -4.1219187 -4.104578 -4.1236386 -4.174191 -4.2172451 -4.2346606 -4.2337432 -4.2034225 -4.1509333 -4.1045637 -4.1007862][-4.3077245 -4.2900105 -4.2569261 -4.2136674 -4.179615 -4.1677542 -4.1779075 -4.213232 -4.2381372 -4.242373 -4.246501 -4.228004 -4.1845551 -4.1420178 -4.133018][-4.3089671 -4.2944231 -4.2708836 -4.2403355 -4.2184782 -4.2142138 -4.215517 -4.2394047 -4.25342 -4.2537861 -4.2568974 -4.2424817 -4.2138181 -4.1883841 -4.1906123][-4.3094206 -4.2966919 -4.2812977 -4.2603164 -4.2467513 -4.2478514 -4.2510786 -4.2648954 -4.2718229 -4.2732043 -4.2788291 -4.2727208 -4.2545047 -4.2439466 -4.2520995][-4.3189144 -4.3101954 -4.3018289 -4.2883315 -4.2801242 -4.2820296 -4.286561 -4.2928696 -4.2958975 -4.2995014 -4.3028193 -4.2970667 -4.2871647 -4.2840705 -4.2902575]]...]
INFO - root - 2017-12-07 23:36:19.839016: step 24010, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.134 sec/batch; 35h:02m:14s remains)
INFO - root - 2017-12-07 23:36:41.070390: step 24020, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.145 sec/batch; 35h:12m:40s remains)
INFO - root - 2017-12-07 23:37:01.821985: step 24030, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.098 sec/batch; 34h:26m:45s remains)
INFO - root - 2017-12-07 23:37:23.074250: step 24040, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.128 sec/batch; 34h:55m:44s remains)
INFO - root - 2017-12-07 23:37:44.212812: step 24050, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.159 sec/batch; 35h:25m:46s remains)
INFO - root - 2017-12-07 23:38:04.957663: step 24060, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.111 sec/batch; 34h:37m:59s remains)
INFO - root - 2017-12-07 23:38:26.116528: step 24070, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.117 sec/batch; 34h:43m:13s remains)
INFO - root - 2017-12-07 23:38:47.416546: step 24080, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.153 sec/batch; 35h:18m:33s remains)
INFO - root - 2017-12-07 23:39:08.136834: step 24090, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.089 sec/batch; 34h:15m:14s remains)
INFO - root - 2017-12-07 23:39:29.390031: step 24100, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.107 sec/batch; 34h:33m:06s remains)
2017-12-07 23:39:30.972892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2256107 -4.2263966 -4.2232847 -4.2222381 -4.2266464 -4.2355494 -4.2427993 -4.2297082 -4.1995311 -4.1836114 -4.1919689 -4.2089438 -4.232 -4.264214 -4.2967963][-4.1886935 -4.1910648 -4.1835589 -4.1791058 -4.1878066 -4.2028432 -4.2134013 -4.1959658 -4.153007 -4.132875 -4.146914 -4.1690793 -4.1946163 -4.2321262 -4.271224][-4.1738882 -4.1746531 -4.1588144 -4.1452641 -4.1485386 -4.1648941 -4.1811428 -4.1669626 -4.1227202 -4.1022854 -4.1212215 -4.1471596 -4.1770787 -4.2178774 -4.258841][-4.1624475 -4.1580005 -4.1318774 -4.1068573 -4.1050882 -4.1230392 -4.1514473 -4.1479964 -4.105792 -4.0903139 -4.1133757 -4.1429558 -4.1796708 -4.221024 -4.2593055][-4.1542835 -4.1472487 -4.1151147 -4.0872197 -4.0838981 -4.0964956 -4.1255527 -4.1318054 -4.1023941 -4.0984826 -4.1226826 -4.1494966 -4.1881843 -4.2254925 -4.2589054][-4.1603756 -4.1496024 -4.1164937 -4.0953918 -4.0949917 -4.0897098 -4.0989709 -4.1035433 -4.0902205 -4.1021252 -4.1264119 -4.1528397 -4.187613 -4.2192483 -4.2496886][-4.1900525 -4.1714973 -4.1357036 -4.1135149 -4.10658 -4.0786581 -4.0594831 -4.05365 -4.0583153 -4.0929189 -4.1292267 -4.1641083 -4.1950445 -4.2223654 -4.249855][-4.2169361 -4.2006197 -4.1647573 -4.1363292 -4.1110077 -4.0580945 -4.0143065 -4.00292 -4.03051 -4.089787 -4.1408343 -4.1847897 -4.211843 -4.2353396 -4.2568173][-4.243948 -4.2374573 -4.2114892 -4.1826835 -4.1426797 -4.0739036 -4.0158262 -4.0050898 -4.045764 -4.10713 -4.1562357 -4.1991138 -4.2268729 -4.2470546 -4.2625365][-4.2641077 -4.2629142 -4.24836 -4.2266088 -4.1884055 -4.1211491 -4.0608578 -4.0529943 -4.0914822 -4.1380086 -4.1777124 -4.2104836 -4.2362347 -4.2551346 -4.2669573][-4.2630639 -4.2624187 -4.2541904 -4.2384105 -4.2087355 -4.1549568 -4.1015568 -4.0949636 -4.1225448 -4.1564364 -4.1884713 -4.2109261 -4.2337503 -4.2529855 -4.2649617][-4.2557688 -4.2545543 -4.2509985 -4.2375841 -4.2154918 -4.17426 -4.133399 -4.1266775 -4.1403632 -4.1616478 -4.1888719 -4.2067356 -4.2272644 -4.2467237 -4.2590723][-4.2570839 -4.2594428 -4.2602139 -4.2497835 -4.2308741 -4.1961575 -4.1669207 -4.1582232 -4.1581 -4.1660914 -4.1862526 -4.2031226 -4.2213488 -4.2406087 -4.2530031][-4.2698827 -4.2757554 -4.2781506 -4.2706285 -4.2564907 -4.2285676 -4.2059774 -4.1971436 -4.1911707 -4.1894636 -4.202837 -4.21707 -4.2306957 -4.2456236 -4.25601][-4.2858305 -4.2925 -4.2937894 -4.287693 -4.2768264 -4.257257 -4.2411981 -4.2350168 -4.2332664 -4.2325964 -4.2430725 -4.2519693 -4.2596416 -4.2685523 -4.2736812]]...]
INFO - root - 2017-12-07 23:39:52.254561: step 24110, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.143 sec/batch; 35h:07m:42s remains)
INFO - root - 2017-12-07 23:40:13.434437: step 24120, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.114 sec/batch; 34h:38m:43s remains)
INFO - root - 2017-12-07 23:40:34.360810: step 24130, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.141 sec/batch; 35h:05m:02s remains)
INFO - root - 2017-12-07 23:40:55.513187: step 24140, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.095 sec/batch; 34h:20m:00s remains)
INFO - root - 2017-12-07 23:41:16.554664: step 24150, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.108 sec/batch; 34h:32m:05s remains)
INFO - root - 2017-12-07 23:41:37.371196: step 24160, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.122 sec/batch; 34h:45m:00s remains)
INFO - root - 2017-12-07 23:41:58.586306: step 24170, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.154 sec/batch; 35h:16m:34s remains)
INFO - root - 2017-12-07 23:42:19.867459: step 24180, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.159 sec/batch; 35h:21m:28s remains)
INFO - root - 2017-12-07 23:42:40.683918: step 24190, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.100 sec/batch; 34h:22m:41s remains)
INFO - root - 2017-12-07 23:43:01.814800: step 24200, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.136 sec/batch; 34h:57m:57s remains)
2017-12-07 23:43:03.444835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2513866 -4.2561178 -4.2407112 -4.2103782 -4.16693 -4.1386242 -4.1350646 -4.1589942 -4.1933966 -4.2165189 -4.2352328 -4.2612028 -4.2919912 -4.3199806 -4.3364615][-4.2165256 -4.2268391 -4.2156191 -4.1794744 -4.1354485 -4.1128292 -4.1256304 -4.1695776 -4.2180638 -4.2413 -4.2626066 -4.290606 -4.3201938 -4.3411365 -4.349514][-4.1844692 -4.2002587 -4.1913462 -4.1486297 -4.099515 -4.0821767 -4.105082 -4.1607122 -4.2196455 -4.2458167 -4.2676687 -4.2944565 -4.3205061 -4.3390636 -4.3454156][-4.1721382 -4.186707 -4.1729326 -4.1258955 -4.0687909 -4.0493469 -4.0744057 -4.134738 -4.1987696 -4.2325158 -4.2537031 -4.2751961 -4.2945905 -4.3130407 -4.3228054][-4.1799178 -4.1842084 -4.1586323 -4.1042976 -4.0437202 -4.0239749 -4.0530972 -4.1137443 -4.1744952 -4.2093749 -4.2252889 -4.2400508 -4.2575684 -4.2773085 -4.2890058][-4.1864243 -4.1765084 -4.1380363 -4.0767088 -4.0193238 -4.0087752 -4.0471268 -4.10031 -4.1456432 -4.1753893 -4.1910138 -4.2079086 -4.2267556 -4.245204 -4.2582726][-4.1839552 -4.1563077 -4.10577 -4.0426593 -3.9973226 -4.0049515 -4.0491691 -4.0849 -4.1120768 -4.1420054 -4.1716504 -4.193522 -4.2086978 -4.2196951 -4.2322292][-4.1866379 -4.1387916 -4.0819364 -4.0249219 -3.9976196 -4.0218639 -4.0584121 -4.0768352 -4.0956812 -4.1357746 -4.1783662 -4.1976314 -4.2019153 -4.2030845 -4.207305][-4.2036824 -4.1402802 -4.0814443 -4.0347157 -4.0234551 -4.0535259 -4.0809703 -4.0912957 -4.1131668 -4.1589441 -4.1998258 -4.2098708 -4.2008715 -4.1900816 -4.1839056][-4.2182717 -4.15411 -4.1006713 -4.0639663 -4.0587854 -4.08321 -4.1035333 -4.1149225 -4.140038 -4.1822238 -4.21398 -4.2164593 -4.197401 -4.178091 -4.1680679][-4.2258267 -4.170012 -4.1253881 -4.096786 -4.092135 -4.1070189 -4.11943 -4.1303988 -4.1533113 -4.1885839 -4.2179966 -4.2189474 -4.1942935 -4.1671839 -4.1599922][-4.2375865 -4.19235 -4.1561732 -4.134685 -4.130415 -4.1372113 -4.14454 -4.1521888 -4.16706 -4.1945319 -4.2217536 -4.2224216 -4.195262 -4.1673574 -4.1700311][-4.2613578 -4.2261038 -4.196032 -4.1793075 -4.1766567 -4.1813951 -4.1858549 -4.1891541 -4.19671 -4.2185354 -4.2426438 -4.2438545 -4.2226191 -4.2012539 -4.2053175][-4.2913423 -4.2673445 -4.24545 -4.2328897 -4.2304406 -4.2331915 -4.2367945 -4.2375364 -4.2406244 -4.2559023 -4.2743545 -4.2771306 -4.2657857 -4.2534866 -4.2588315][-4.3193216 -4.3054056 -4.2918067 -4.2835784 -4.2812843 -4.2833352 -4.2866096 -4.288312 -4.291347 -4.3010139 -4.3121648 -4.3136129 -4.308917 -4.3043904 -4.3085022]]...]
INFO - root - 2017-12-07 23:43:24.647072: step 24210, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.134 sec/batch; 34h:55m:32s remains)
INFO - root - 2017-12-07 23:43:45.350346: step 24220, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.070 sec/batch; 33h:52m:12s remains)
INFO - root - 2017-12-07 23:44:06.228532: step 24230, loss = 2.09, batch loss = 2.03 (15.5 examples/sec; 2.069 sec/batch; 33h:50m:37s remains)
INFO - root - 2017-12-07 23:44:27.517467: step 24240, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.077 sec/batch; 33h:58m:14s remains)
INFO - root - 2017-12-07 23:44:48.538354: step 24250, loss = 2.07, batch loss = 2.01 (16.4 examples/sec; 1.956 sec/batch; 31h:59m:18s remains)
INFO - root - 2017-12-07 23:45:09.630587: step 24260, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.154 sec/batch; 35h:12m:58s remains)
INFO - root - 2017-12-07 23:45:30.944414: step 24270, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 34h:22m:33s remains)
INFO - root - 2017-12-07 23:45:52.228880: step 24280, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.107 sec/batch; 34h:26m:29s remains)
INFO - root - 2017-12-07 23:46:13.020996: step 24290, loss = 2.06, batch loss = 2.00 (14.7 examples/sec; 2.177 sec/batch; 35h:34m:58s remains)
INFO - root - 2017-12-07 23:46:34.359309: step 24300, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.141 sec/batch; 34h:59m:22s remains)
2017-12-07 23:46:35.941599: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0924177 -4.1023879 -4.148273 -4.1872759 -4.2064924 -4.2042327 -4.1882038 -4.1632371 -4.1471481 -4.1542888 -4.1498494 -4.1190519 -4.1142244 -4.1523223 -4.1986351][-4.0684376 -4.0985785 -4.1497025 -4.1850533 -4.2022862 -4.2032657 -4.19162 -4.1688094 -4.1541715 -4.1607924 -4.1511912 -4.1130953 -4.1056714 -4.1442528 -4.1924548][-4.0781474 -4.1129389 -4.1615119 -4.19036 -4.200973 -4.2052803 -4.1982493 -4.177083 -4.1620913 -4.1627507 -4.1514707 -4.121376 -4.1186523 -4.1551857 -4.1987977][-4.1043754 -4.1201143 -4.1529088 -4.1738229 -4.18253 -4.1931949 -4.1920609 -4.1738086 -4.1584091 -4.1523361 -4.1418562 -4.1341491 -4.1467905 -4.1787133 -4.210288][-4.1367965 -4.1243076 -4.1335897 -4.1448603 -4.1509748 -4.1642394 -4.1697578 -4.1583142 -4.1472712 -4.1425834 -4.140481 -4.1500716 -4.1764331 -4.2025719 -4.218545][-4.1713142 -4.1450076 -4.1363297 -4.1341782 -4.1300049 -4.1308517 -4.1284637 -4.1196566 -4.1247029 -4.1382332 -4.1486683 -4.1603804 -4.1819892 -4.2002668 -4.2092524][-4.1970263 -4.169239 -4.1513839 -4.1350927 -4.11617 -4.0959 -4.075273 -4.0656137 -4.0912313 -4.1283507 -4.1517019 -4.1563182 -4.1674476 -4.1797714 -4.1862955][-4.2162371 -4.1938381 -4.1751008 -4.152586 -4.1248946 -4.087863 -4.0432062 -4.0210433 -4.0570993 -4.1092043 -4.1403036 -4.1408143 -4.1464891 -4.1550245 -4.1665597][-4.2404957 -4.2272496 -4.2173305 -4.198885 -4.1752772 -4.1383119 -4.0857048 -4.0484371 -4.0739183 -4.1203189 -4.146771 -4.1433034 -4.1477389 -4.157948 -4.1738348][-4.2688632 -4.2594523 -4.2597084 -4.253592 -4.2404957 -4.2213936 -4.189353 -4.1542449 -4.1610203 -4.1839476 -4.191504 -4.1791024 -4.1783433 -4.186625 -4.206265][-4.2911935 -4.2854438 -4.289587 -4.2922893 -4.289113 -4.2828135 -4.2671685 -4.2423697 -4.238627 -4.2448049 -4.2393694 -4.2249336 -4.22122 -4.22701 -4.2466969][-4.2972865 -4.2960014 -4.2993655 -4.3056226 -4.3102479 -4.310895 -4.3034024 -4.2871971 -4.2781091 -4.2751169 -4.2654748 -4.2548671 -4.2564783 -4.2649336 -4.2836781][-4.2953835 -4.2967677 -4.3006644 -4.3066144 -4.3132048 -4.3169384 -4.312686 -4.3020816 -4.2919488 -4.2846727 -4.2746544 -4.2688684 -4.2745538 -4.2831769 -4.29936][-4.3091717 -4.3096213 -4.3121443 -4.316741 -4.3219361 -4.3251319 -4.3232865 -4.3169527 -4.3090611 -4.3010774 -4.2919335 -4.2873144 -4.2915521 -4.2988005 -4.3115196][-4.3223481 -4.3224173 -4.3233485 -4.3256617 -4.3293362 -4.3328667 -4.3327651 -4.3294048 -4.3235278 -4.3167286 -4.3097124 -4.3060956 -4.30815 -4.3132968 -4.3211045]]...]
INFO - root - 2017-12-07 23:46:57.038405: step 24310, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.096 sec/batch; 34h:14m:40s remains)
INFO - root - 2017-12-07 23:47:17.761655: step 24320, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 34h:37m:09s remains)
INFO - root - 2017-12-07 23:47:38.802453: step 24330, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.059 sec/batch; 33h:37m:28s remains)
INFO - root - 2017-12-07 23:47:59.879557: step 24340, loss = 2.06, batch loss = 2.00 (14.8 examples/sec; 2.157 sec/batch; 35h:13m:47s remains)
INFO - root - 2017-12-07 23:48:20.975297: step 24350, loss = 2.08, batch loss = 2.02 (14.5 examples/sec; 2.208 sec/batch; 36h:02m:45s remains)
INFO - root - 2017-12-07 23:48:42.141787: step 24360, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.164 sec/batch; 35h:19m:42s remains)
INFO - root - 2017-12-07 23:49:03.271323: step 24370, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.089 sec/batch; 34h:05m:40s remains)
INFO - root - 2017-12-07 23:49:23.996844: step 24380, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.086 sec/batch; 34h:02m:04s remains)
INFO - root - 2017-12-07 23:49:45.019780: step 24390, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.116 sec/batch; 34h:31m:47s remains)
INFO - root - 2017-12-07 23:50:06.187820: step 24400, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.154 sec/batch; 35h:07m:56s remains)
2017-12-07 23:50:07.749628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2264123 -4.2333908 -4.2469826 -4.2610712 -4.2681465 -4.2643638 -4.2589111 -4.2533269 -4.2500658 -4.2444181 -4.2289896 -4.2158861 -4.2151279 -4.2298641 -4.2543945][-4.2158279 -4.2241187 -4.2339435 -4.2444692 -4.2491422 -4.2457423 -4.24166 -4.2364407 -4.2370949 -4.2331982 -4.215878 -4.1982303 -4.1936622 -4.209425 -4.2407684][-4.233737 -4.2396774 -4.2382216 -4.2379742 -4.2308226 -4.2209797 -4.2136941 -4.2068973 -4.2072296 -4.2047496 -4.1923857 -4.1766572 -4.1712003 -4.186573 -4.2230725][-4.2776494 -4.2775469 -4.2586184 -4.2386231 -4.2137785 -4.1946568 -4.1815844 -4.1703682 -4.1677217 -4.1706767 -4.1717963 -4.1651616 -4.1617608 -4.1781859 -4.2169762][-4.3075404 -4.2982945 -4.2624097 -4.2213645 -4.1823759 -4.1534152 -4.13214 -4.1116948 -4.1074376 -4.1267176 -4.1463542 -4.1491323 -4.153089 -4.1724453 -4.2123203][-4.2970018 -4.2782631 -4.232553 -4.1813745 -4.1365476 -4.0987635 -4.0602479 -4.0145354 -3.9915895 -4.0289493 -4.0802951 -4.1038694 -4.1248631 -4.1559753 -4.1988893][-4.2542896 -4.2286325 -4.179122 -4.1264815 -4.0814729 -4.0384512 -3.9783623 -3.8821669 -3.8032417 -3.8495171 -3.9507046 -4.0131969 -4.0569687 -4.1042223 -4.1588297][-4.2008181 -4.1614838 -4.1109509 -4.0626116 -4.0228381 -3.9786258 -3.9004877 -3.7482061 -3.5997555 -3.6513567 -3.8069212 -3.9142358 -3.9782958 -4.0348063 -4.1005282][-4.1679335 -4.1200089 -4.0744419 -4.0391254 -4.0153074 -3.9803538 -3.9081233 -3.758558 -3.6052787 -3.6443107 -3.7904723 -3.8981135 -3.9613664 -4.0095959 -4.0711269][-4.1799893 -4.1351552 -4.100522 -4.080883 -4.0755949 -4.0583038 -4.0147581 -3.9259675 -3.8331389 -3.8433938 -3.9193802 -3.9898911 -4.0359378 -4.069942 -4.11341][-4.2235432 -4.1918588 -4.1686721 -4.1575341 -4.1652541 -4.16857 -4.152739 -4.1084452 -4.056427 -4.0470786 -4.0696259 -4.1021028 -4.13169 -4.1602445 -4.1952124][-4.2750936 -4.2580557 -4.2492371 -4.2472415 -4.2609949 -4.2712541 -4.264431 -4.238121 -4.20944 -4.1982541 -4.19584 -4.2016692 -4.2160068 -4.2383566 -4.2674823][-4.3081803 -4.300209 -4.2991447 -4.3027468 -4.3157864 -4.3238549 -4.3223133 -4.3088765 -4.295537 -4.2886405 -4.2804942 -4.2744408 -4.2775793 -4.2906055 -4.3077927][-4.3227515 -4.3199105 -4.3215652 -4.3261247 -4.3338227 -4.3387332 -4.338541 -4.3312292 -4.3240285 -4.321456 -4.3179955 -4.3132997 -4.3121305 -4.3180571 -4.32586][-4.332418 -4.3304939 -4.3313594 -4.3323517 -4.3345265 -4.3361187 -4.3356633 -4.3315058 -4.3282151 -4.327733 -4.3271823 -4.3257394 -4.3255978 -4.3289766 -4.3328357]]...]
INFO - root - 2017-12-07 23:50:28.736465: step 24410, loss = 2.08, batch loss = 2.02 (16.1 examples/sec; 1.988 sec/batch; 32h:25m:21s remains)
INFO - root - 2017-12-07 23:50:49.618520: step 24420, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.138 sec/batch; 34h:51m:57s remains)
INFO - root - 2017-12-07 23:51:10.693941: step 24430, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.090 sec/batch; 34h:04m:59s remains)
INFO - root - 2017-12-07 23:51:31.929996: step 24440, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.156 sec/batch; 35h:08m:54s remains)
INFO - root - 2017-12-07 23:51:52.655103: step 24450, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.166 sec/batch; 35h:17m:57s remains)
INFO - root - 2017-12-07 23:52:14.086101: step 24460, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.147 sec/batch; 34h:58m:54s remains)
INFO - root - 2017-12-07 23:52:35.414107: step 24470, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.111 sec/batch; 34h:24m:02s remains)
INFO - root - 2017-12-07 23:52:56.172456: step 24480, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.152 sec/batch; 35h:03m:44s remains)
INFO - root - 2017-12-07 23:53:17.309628: step 24490, loss = 2.08, batch loss = 2.03 (14.9 examples/sec; 2.148 sec/batch; 34h:59m:14s remains)
INFO - root - 2017-12-07 23:53:38.409460: step 24500, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.092 sec/batch; 34h:03m:43s remains)
2017-12-07 23:53:39.932971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2804241 -4.2796931 -4.2945027 -4.3060241 -4.30671 -4.2900753 -4.2619982 -4.2353592 -4.2219229 -4.2131081 -4.1889272 -4.1336136 -4.0788455 -4.0714021 -4.1087508][-4.2661862 -4.2680106 -4.28899 -4.3084817 -4.312099 -4.2925115 -4.2532511 -4.2133403 -4.2006807 -4.2005563 -4.1874604 -4.1457171 -4.0996523 -4.0916166 -4.1242104][-4.2417197 -4.248991 -4.2807317 -4.3118939 -4.3194361 -4.2948503 -4.2448406 -4.1903462 -4.1800017 -4.1901021 -4.1855221 -4.1534867 -4.1130724 -4.1050339 -4.1353641][-4.2062197 -4.2273464 -4.2746415 -4.313858 -4.3233128 -4.2921462 -4.2265649 -4.1555533 -4.1510425 -4.180088 -4.18811 -4.1620479 -4.1231356 -4.1161819 -4.1462383][-4.1423626 -4.1892366 -4.260982 -4.3076324 -4.313549 -4.2681408 -4.1725683 -4.0753927 -4.0786791 -4.1417179 -4.17833 -4.1694551 -4.1383128 -4.132956 -4.1641273][-4.0671091 -4.1386986 -4.2323704 -4.2835393 -4.2837667 -4.2225919 -4.0953712 -3.9599338 -3.9678531 -4.0763321 -4.1575747 -4.179606 -4.1682911 -4.1690512 -4.1967516][-3.9943686 -4.0799618 -4.1950374 -4.2518134 -4.2425833 -4.1636443 -4.0012341 -3.813256 -3.8185406 -3.986114 -4.1266527 -4.1888003 -4.2028389 -4.210948 -4.2323418][-3.9693477 -4.0504994 -4.1679497 -4.224925 -4.2053847 -4.1132526 -3.9312403 -3.7072673 -3.7022877 -3.9114985 -4.0925055 -4.1852331 -4.2203112 -4.235466 -4.2539062][-4.0219059 -4.0839443 -4.1813078 -4.2275963 -4.2060061 -4.1251736 -3.9706583 -3.7799659 -3.7640843 -3.9440925 -4.1062694 -4.1922545 -4.2247777 -4.2366328 -4.2524371][-4.1082377 -4.1474342 -4.2148261 -4.2443743 -4.2267966 -4.1702261 -4.0677915 -3.9398842 -3.9200234 -4.0350161 -4.1443548 -4.1998749 -4.2147837 -4.2177525 -4.2307172][-4.1941991 -4.2163062 -4.2545013 -4.2653775 -4.2525682 -4.219058 -4.1595945 -4.0809484 -4.0592012 -4.1200037 -4.1768632 -4.1960082 -4.1916566 -4.1884742 -4.2007995][-4.2505212 -4.265666 -4.2826705 -4.2806611 -4.2689261 -4.2482591 -4.2144194 -4.168509 -4.1514206 -4.1800895 -4.2009583 -4.1904449 -4.167304 -4.1592512 -4.1719875][-4.2690182 -4.283134 -4.2875624 -4.2775364 -4.264401 -4.2520933 -4.2331996 -4.2090855 -4.2031364 -4.220098 -4.2190976 -4.185935 -4.1449771 -4.1333923 -4.1529012][-4.268012 -4.28127 -4.2820997 -4.2706084 -4.2590504 -4.25284 -4.2442732 -4.232688 -4.2354517 -4.2462463 -4.2368159 -4.1927142 -4.1445079 -4.1339793 -4.1585617][-4.2748842 -4.285255 -4.2853436 -4.2771988 -4.2691712 -4.2666845 -4.2653894 -4.2614694 -4.2656941 -4.2720265 -4.2625041 -4.2245164 -4.1845803 -4.1791086 -4.2044644]]...]
INFO - root - 2017-12-07 23:54:00.661359: step 24510, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.088 sec/batch; 34h:00m:13s remains)
INFO - root - 2017-12-07 23:54:21.730236: step 24520, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.088 sec/batch; 33h:59m:10s remains)
INFO - root - 2017-12-07 23:54:42.860894: step 24530, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.132 sec/batch; 34h:41m:52s remains)
INFO - root - 2017-12-07 23:55:03.704107: step 24540, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 1.936 sec/batch; 31h:30m:07s remains)
INFO - root - 2017-12-07 23:55:24.844523: step 24550, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.144 sec/batch; 34h:53m:05s remains)
INFO - root - 2017-12-07 23:55:46.142022: step 24560, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.146 sec/batch; 34h:55m:02s remains)
INFO - root - 2017-12-07 23:56:07.404827: step 24570, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.127 sec/batch; 34h:36m:10s remains)
INFO - root - 2017-12-07 23:56:28.212817: step 24580, loss = 2.09, batch loss = 2.03 (15.6 examples/sec; 2.046 sec/batch; 33h:16m:40s remains)
INFO - root - 2017-12-07 23:56:49.340844: step 24590, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.122 sec/batch; 34h:30m:25s remains)
INFO - root - 2017-12-07 23:57:10.615837: step 24600, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.083 sec/batch; 33h:51m:55s remains)
2017-12-07 23:57:12.224510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2837725 -4.2685704 -4.2462306 -4.2246695 -4.2217174 -4.245389 -4.27946 -4.3066983 -4.3264461 -4.3395233 -4.3466654 -4.3456469 -4.3405762 -4.3325958 -4.3188043][-4.2796516 -4.2764277 -4.2693357 -4.2590427 -4.2554889 -4.2647462 -4.2808704 -4.2946415 -4.3105435 -4.3233633 -4.3320317 -4.3319268 -4.3278413 -4.3233724 -4.31317][-4.2759671 -4.2834406 -4.2872834 -4.2844467 -4.2795258 -4.2756062 -4.2727308 -4.2701807 -4.2780013 -4.2880878 -4.297688 -4.3012881 -4.3022552 -4.303987 -4.2990413][-4.2788639 -4.2901516 -4.2972322 -4.2934251 -4.2836466 -4.2713723 -4.25341 -4.2339373 -4.2317972 -4.2414684 -4.25664 -4.2682238 -4.27756 -4.2865233 -4.287087][-4.2813492 -4.2866249 -4.2899947 -4.2814708 -4.2646804 -4.2471728 -4.2185707 -4.1813083 -4.166162 -4.1812077 -4.2114811 -4.2361145 -4.2544065 -4.2713952 -4.2789335][-4.2757297 -4.269486 -4.266726 -4.2563419 -4.2366018 -4.2139387 -4.1734219 -4.113625 -4.08038 -4.104836 -4.1595984 -4.2060184 -4.2350411 -4.2566962 -4.2688122][-4.2635818 -4.2445154 -4.2351375 -4.2253733 -4.2053266 -4.178453 -4.1239438 -4.0329061 -3.9744446 -4.0128093 -4.1035457 -4.1784749 -4.2197132 -4.2438636 -4.2572827][-4.2496328 -4.2199411 -4.2024689 -4.1935954 -4.1785946 -4.1524205 -4.0872254 -3.9740968 -3.8961911 -3.9404006 -4.0559392 -4.1513195 -4.200242 -4.2245 -4.2365751][-4.2381687 -4.1997271 -4.1752338 -4.1692376 -4.1649809 -4.1483326 -4.0903764 -3.9863193 -3.9109869 -3.94071 -4.0369658 -4.1245804 -4.1714425 -4.1941376 -4.2055659][-4.2346234 -4.1919532 -4.1651387 -4.1635842 -4.1695561 -4.1658111 -4.1263919 -4.0467505 -3.9845083 -3.9889472 -4.0419636 -4.101779 -4.139514 -4.1597509 -4.1742988][-4.23375 -4.1949568 -4.1734123 -4.1754217 -4.1840048 -4.1870961 -4.1648993 -4.110106 -4.0600548 -4.041748 -4.0565615 -4.0919476 -4.1221352 -4.1395731 -4.1549325][-4.2153583 -4.1921887 -4.1868095 -4.194797 -4.2023191 -4.2062306 -4.1955957 -4.1610627 -4.1215029 -4.08911 -4.08055 -4.1015878 -4.1290407 -4.1466084 -4.1601343][-4.1934824 -4.1937022 -4.2063904 -4.2182493 -4.2219005 -4.2213206 -4.2145581 -4.1944828 -4.167892 -4.1365986 -4.1184068 -4.129281 -4.1522846 -4.1683459 -4.1783009][-4.1902447 -4.2154732 -4.237824 -4.2464476 -4.242835 -4.2343612 -4.2267647 -4.2158656 -4.2026134 -4.182755 -4.1665726 -4.1673231 -4.1799326 -4.19058 -4.1977086][-4.20218 -4.2417812 -4.2640595 -4.2674417 -4.2554717 -4.24048 -4.2334261 -4.227488 -4.2255011 -4.2223282 -4.2151747 -4.2112985 -4.2128835 -4.2159157 -4.2196741]]...]
INFO - root - 2017-12-07 23:57:32.900627: step 24610, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.066 sec/batch; 33h:35m:19s remains)
INFO - root - 2017-12-07 23:57:53.847905: step 24620, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 34h:21m:59s remains)
INFO - root - 2017-12-07 23:58:14.951214: step 24630, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 34h:10m:01s remains)
INFO - root - 2017-12-07 23:58:35.884993: step 24640, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 34h:25m:37s remains)
INFO - root - 2017-12-07 23:58:57.003644: step 24650, loss = 2.09, batch loss = 2.03 (15.4 examples/sec; 2.078 sec/batch; 33h:44m:57s remains)
INFO - root - 2017-12-07 23:59:18.121297: step 24660, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.140 sec/batch; 34h:45m:10s remains)
INFO - root - 2017-12-07 23:59:39.058682: step 24670, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.132 sec/batch; 34h:37m:22s remains)
INFO - root - 2017-12-08 00:00:00.384582: step 24680, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.131 sec/batch; 34h:35m:23s remains)
INFO - root - 2017-12-08 00:00:21.609640: step 24690, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 34h:37m:12s remains)
INFO - root - 2017-12-08 00:00:42.851260: step 24700, loss = 2.08, batch loss = 2.02 (15.7 examples/sec; 2.033 sec/batch; 32h:59m:17s remains)
2017-12-08 00:00:44.505610: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3421936 -4.3444438 -4.3431811 -4.3347688 -4.321516 -4.307539 -4.2992296 -4.2993951 -4.3083253 -4.3224564 -4.3314934 -4.3293934 -4.3234711 -4.3209453 -4.3208232][-4.33994 -4.3418045 -4.3392072 -4.3280153 -4.3110752 -4.2944369 -4.2846184 -4.2823896 -4.2878003 -4.30473 -4.3200932 -4.3231506 -4.3186393 -4.315062 -4.3117757][-4.3337517 -4.3321095 -4.325604 -4.3095837 -4.2875352 -4.2662134 -4.2538109 -4.24981 -4.2530217 -4.2742648 -4.2968216 -4.3079596 -4.3097606 -4.3078294 -4.30289][-4.324965 -4.3170004 -4.30541 -4.2850547 -4.2550025 -4.2225852 -4.1969643 -4.1828823 -4.1865735 -4.2194629 -4.2585206 -4.2837348 -4.296782 -4.3015785 -4.2985787][-4.305697 -4.2914505 -4.2775307 -4.256278 -4.2188544 -4.1731453 -4.1269636 -4.0923004 -4.0911732 -4.1408477 -4.2058291 -4.2526088 -4.2793412 -4.2952862 -4.2991018][-4.2676592 -4.2437005 -4.2281055 -4.2100616 -4.1754875 -4.1224666 -4.052958 -3.9889474 -3.9759488 -4.0474224 -4.1452627 -4.2177105 -4.2581525 -4.2835627 -4.297389][-4.2261157 -4.1916933 -4.171165 -4.1583509 -4.1358705 -4.0837421 -3.9905865 -3.8864291 -3.8449225 -3.9348097 -4.0710564 -4.1753631 -4.2322206 -4.2665243 -4.2922297][-4.2056532 -4.1658673 -4.1392198 -4.1309977 -4.1239362 -4.0865183 -3.9937837 -3.8721998 -3.8037803 -3.8843384 -4.0309973 -4.1470251 -4.212276 -4.2523956 -4.2871284][-4.1997223 -4.1576204 -4.1254778 -4.1216617 -4.1266665 -4.1102438 -4.042551 -3.9425929 -3.8786013 -3.9293342 -4.051002 -4.1510406 -4.2107482 -4.2504468 -4.2863517][-4.2165136 -4.1803379 -4.1492109 -4.144021 -4.1558838 -4.1582336 -4.1207156 -4.0545425 -4.0115447 -4.0405884 -4.1191297 -4.1884542 -4.2315917 -4.2626295 -4.293148][-4.252924 -4.2267604 -4.2007527 -4.1939549 -4.2070589 -4.2211418 -4.2091408 -4.175457 -4.1471233 -4.1579247 -4.1998863 -4.2394171 -4.2621617 -4.28024 -4.3019271][-4.297617 -4.2845297 -4.2684851 -4.2606688 -4.2663846 -4.2766514 -4.2732162 -4.2571974 -4.2380428 -4.2390418 -4.2612586 -4.2816429 -4.2903538 -4.2978067 -4.3097305][-4.3232527 -4.322197 -4.3165069 -4.31068 -4.3123875 -4.3163991 -4.31382 -4.30193 -4.2858691 -4.283617 -4.2969851 -4.3092775 -4.3121991 -4.3142109 -4.3194928][-4.333488 -4.3362708 -4.3352432 -4.3300934 -4.3293576 -4.3318248 -4.3310647 -4.3207421 -4.306859 -4.3034472 -4.3124175 -4.3216405 -4.3246584 -4.325633 -4.3285685][-4.3367767 -4.3386683 -4.3379364 -4.3342366 -4.3339157 -4.3365684 -4.3363042 -4.3280983 -4.3175192 -4.314846 -4.3205876 -4.32794 -4.331985 -4.3335462 -4.3354578]]...]
INFO - root - 2017-12-08 00:01:05.698733: step 24710, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.145 sec/batch; 34h:47m:57s remains)
INFO - root - 2017-12-08 00:01:26.864616: step 24720, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.159 sec/batch; 35h:01m:39s remains)
INFO - root - 2017-12-08 00:01:48.180174: step 24730, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 34h:16m:06s remains)
INFO - root - 2017-12-08 00:02:09.310866: step 24740, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.157 sec/batch; 34h:58m:58s remains)
INFO - root - 2017-12-08 00:02:30.562957: step 24750, loss = 2.09, batch loss = 2.03 (14.8 examples/sec; 2.158 sec/batch; 34h:59m:58s remains)
INFO - root - 2017-12-08 00:02:51.641408: step 24760, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.100 sec/batch; 34h:02m:31s remains)
INFO - root - 2017-12-08 00:03:12.621627: step 24770, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 34h:34m:07s remains)
INFO - root - 2017-12-08 00:03:33.891863: step 24780, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 34h:34m:11s remains)
INFO - root - 2017-12-08 00:03:55.096122: step 24790, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.105 sec/batch; 34h:06m:26s remains)
INFO - root - 2017-12-08 00:04:15.761499: step 24800, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.164 sec/batch; 35h:03m:47s remains)
2017-12-08 00:04:17.365825: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3208284 -4.3273177 -4.3297725 -4.3303609 -4.33053 -4.3305736 -4.3304844 -4.3310513 -4.3318372 -4.3317356 -4.3305097 -4.3288307 -4.3267441 -4.3245 -4.3222036][-4.3218 -4.3277073 -4.3290277 -4.3283572 -4.3277626 -4.3273973 -4.326581 -4.3266649 -4.3275547 -4.3272305 -4.3252153 -4.3226032 -4.31993 -4.3180728 -4.316772][-4.3244143 -4.3287845 -4.3274622 -4.3234143 -4.3193746 -4.3149657 -4.3098836 -4.3069983 -4.3062348 -4.3052826 -4.3033633 -4.3021665 -4.3023763 -4.3047042 -4.3084555][-4.3265386 -4.3265705 -4.317462 -4.3035245 -4.2887936 -4.2734218 -4.2573867 -4.2457829 -4.2408218 -4.2394776 -4.240344 -4.2466693 -4.2574196 -4.2705994 -4.2857857][-4.3167486 -4.3059006 -4.2809539 -4.2479043 -4.2132006 -4.1787038 -4.145647 -4.1221404 -4.11406 -4.1163182 -4.125649 -4.14575 -4.1754146 -4.2066727 -4.2396169][-4.2818933 -4.2513747 -4.2004042 -4.1411066 -4.0831003 -4.0288906 -3.9814811 -3.9527569 -3.9548965 -3.9721797 -3.9955862 -4.0336838 -4.0842228 -4.1335421 -4.1819644][-4.2203808 -4.1658888 -4.0899358 -4.0135107 -3.9473126 -3.8925562 -3.8500326 -3.8318675 -3.8586102 -3.8986864 -3.9330368 -3.9785964 -4.0337596 -4.0855818 -4.1363049][-4.1630092 -4.0995522 -4.0231605 -3.9581985 -3.913835 -3.8888433 -3.877285 -3.8772616 -3.9117196 -3.9510736 -3.9749575 -4.0079861 -4.0515013 -4.0930223 -4.1356044][-4.1529961 -4.1025476 -4.0490203 -4.0113845 -3.9936736 -3.9922712 -4.000958 -4.0096197 -4.0324283 -4.0539203 -4.065033 -4.0867858 -4.120563 -4.1535592 -4.1862226][-4.1843514 -4.1543078 -4.1252408 -4.1081834 -4.1042552 -4.1087365 -4.1180553 -4.1243968 -4.1356978 -4.145833 -4.1529531 -4.169498 -4.1966038 -4.2231708 -4.2480645][-4.2298994 -4.2197657 -4.21007 -4.206543 -4.2093916 -4.214375 -4.2202845 -4.2241669 -4.230741 -4.2364931 -4.2424774 -4.2547674 -4.2718606 -4.2875333 -4.301559][-4.273952 -4.2781658 -4.2813196 -4.2855196 -4.2915969 -4.2963061 -4.2997136 -4.302125 -4.3065386 -4.3105116 -4.3139257 -4.3209839 -4.3288441 -4.3361197 -4.3408012][-4.3066173 -4.3174138 -4.3257852 -4.3317184 -4.3377 -4.3421879 -4.3451982 -4.3477669 -4.3517027 -4.3552661 -4.3577294 -4.3613806 -4.3645864 -4.366611 -4.3650665][-4.325089 -4.3399096 -4.3499746 -4.3561378 -4.3616257 -4.365303 -4.3669024 -4.3681235 -4.3699384 -4.3717322 -4.3732653 -4.3751216 -4.3758249 -4.3744364 -4.3693542][-4.3273134 -4.3429389 -4.3522329 -4.3564191 -4.3593955 -4.3607221 -4.3600359 -4.3589358 -4.35843 -4.3582978 -4.3584051 -4.3587513 -4.3585963 -4.3569627 -4.3529916]]...]
INFO - root - 2017-12-08 00:04:38.807518: step 24810, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.183 sec/batch; 35h:21m:15s remains)
INFO - root - 2017-12-08 00:04:59.982594: step 24820, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.100 sec/batch; 34h:00m:48s remains)
INFO - root - 2017-12-08 00:05:20.752389: step 24830, loss = 2.09, batch loss = 2.03 (15.7 examples/sec; 2.037 sec/batch; 32h:59m:21s remains)
INFO - root - 2017-12-08 00:05:42.061572: step 24840, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.137 sec/batch; 34h:36m:19s remains)
INFO - root - 2017-12-08 00:06:03.283198: step 24850, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.174 sec/batch; 35h:11m:35s remains)
INFO - root - 2017-12-08 00:06:24.418685: step 24860, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 34h:15m:45s remains)
INFO - root - 2017-12-08 00:06:45.431865: step 24870, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 34h:19m:17s remains)
INFO - root - 2017-12-08 00:07:06.482131: step 24880, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 2.081 sec/batch; 33h:39m:56s remains)
INFO - root - 2017-12-08 00:07:27.925979: step 24890, loss = 2.07, batch loss = 2.02 (14.6 examples/sec; 2.194 sec/batch; 35h:29m:13s remains)
INFO - root - 2017-12-08 00:07:48.397199: step 24900, loss = 2.06, batch loss = 2.00 (14.8 examples/sec; 2.164 sec/batch; 35h:00m:11s remains)
2017-12-08 00:07:50.093087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2933469 -4.2903509 -4.289916 -4.2909164 -4.2928782 -4.2948718 -4.2956514 -4.2956181 -4.2958817 -4.2943535 -4.2894392 -4.2844605 -4.2822733 -4.2828469 -4.2869239][-4.2697611 -4.2673454 -4.2698822 -4.2737188 -4.2761817 -4.27674 -4.2746763 -4.2728643 -4.2724295 -4.270216 -4.26438 -4.259696 -4.2584496 -4.2596 -4.2646179][-4.2328162 -4.2326055 -4.2383103 -4.2437992 -4.245821 -4.2426805 -4.2362933 -4.2336507 -4.2351189 -4.2349725 -4.2304645 -4.22653 -4.2264051 -4.2269845 -4.2293873][-4.1912394 -4.1939378 -4.2029352 -4.2088685 -4.208065 -4.1978641 -4.1826382 -4.1774154 -4.184854 -4.1932335 -4.1943841 -4.1928492 -4.1949329 -4.1941447 -4.18969][-4.1544838 -4.1572447 -4.1669559 -4.1702547 -4.1644745 -4.1443887 -4.1130919 -4.1013532 -4.1188126 -4.14289 -4.1535521 -4.1558771 -4.1619263 -4.1598969 -4.1446176][-4.1305585 -4.1275268 -4.1329374 -4.1300812 -4.117857 -4.0873771 -4.0382938 -4.0175061 -4.0464983 -4.0869718 -4.1095843 -4.1199121 -4.1307335 -4.1273355 -4.1002512][-4.1353469 -4.1242023 -4.1224837 -4.1138186 -4.0949421 -4.0578074 -4.0007944 -3.9760127 -4.01145 -4.0607667 -4.0899215 -4.1077528 -4.122808 -4.1196203 -4.0893831][-4.1741819 -4.1565533 -4.1489859 -4.1394687 -4.1200461 -4.0884132 -4.0423942 -4.0223665 -4.0524688 -4.0942154 -4.1156816 -4.1300559 -4.1435862 -4.1443191 -4.1256924][-4.2218289 -4.198143 -4.1872067 -4.1829767 -4.1729884 -4.1536655 -4.1249361 -4.1111364 -4.1282458 -4.1541753 -4.1642723 -4.1715264 -4.1812534 -4.1867981 -4.1824293][-4.2562294 -4.2292008 -4.2155685 -4.2149758 -4.2145147 -4.2082248 -4.1951008 -4.1867924 -4.1951642 -4.2113233 -4.2160077 -4.2185388 -4.2250376 -4.2334037 -4.2381558][-4.2702618 -4.2441287 -4.2296023 -4.230206 -4.2380786 -4.2450914 -4.24489 -4.2400413 -4.2429748 -4.2541986 -4.2587171 -4.2591767 -4.2620711 -4.2691479 -4.276639][-4.2761312 -4.2565536 -4.2434282 -4.2439013 -4.2555161 -4.2714334 -4.2821274 -4.2822819 -4.282536 -4.2888207 -4.2915249 -4.2903919 -4.2914181 -4.2959638 -4.3030128][-4.2832441 -4.2717056 -4.263196 -4.264709 -4.276526 -4.2935452 -4.3080292 -4.3124485 -4.31234 -4.3145447 -4.3144836 -4.312705 -4.3138165 -4.3171487 -4.322731][-4.2957263 -4.2904062 -4.2862158 -4.2884769 -4.2979889 -4.311779 -4.325037 -4.3305764 -4.3306179 -4.3301358 -4.3276772 -4.32515 -4.3264232 -4.3300467 -4.3347769][-4.3093805 -4.3077393 -4.3054409 -4.30791 -4.3149309 -4.3245411 -4.3342829 -4.3383451 -4.3374743 -4.3341289 -4.3292212 -4.3251786 -4.3257957 -4.3298473 -4.3343248]]...]
INFO - root - 2017-12-08 00:08:11.359238: step 24910, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.174 sec/batch; 35h:08m:57s remains)
INFO - root - 2017-12-08 00:08:32.629236: step 24920, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.089 sec/batch; 33h:46m:41s remains)
INFO - root - 2017-12-08 00:08:53.245576: step 24930, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 34h:13m:41s remains)
INFO - root - 2017-12-08 00:09:14.389237: step 24940, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 2.052 sec/batch; 33h:09m:27s remains)
INFO - root - 2017-12-08 00:09:35.484366: step 24950, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.077 sec/batch; 33h:33m:24s remains)
INFO - root - 2017-12-08 00:09:56.297419: step 24960, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 1.946 sec/batch; 31h:26m:30s remains)
INFO - root - 2017-12-08 00:10:17.486979: step 24970, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 34h:08m:53s remains)
INFO - root - 2017-12-08 00:10:38.738125: step 24980, loss = 2.08, batch loss = 2.02 (14.4 examples/sec; 2.225 sec/batch; 35h:55m:55s remains)
INFO - root - 2017-12-08 00:10:59.964592: step 24990, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.112 sec/batch; 34h:06m:47s remains)
INFO - root - 2017-12-08 00:11:20.547945: step 25000, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.089 sec/batch; 33h:44m:09s remains)
2017-12-08 00:11:22.133147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2464023 -4.2336111 -4.2262478 -4.2197018 -4.2081566 -4.1983271 -4.1984515 -4.2060671 -4.2149124 -4.2171593 -4.2126451 -4.2097106 -4.2093873 -4.2137675 -4.2251153][-4.2499504 -4.2318897 -4.2210431 -4.2099504 -4.1946468 -4.1823254 -4.1832314 -4.1943684 -4.2086883 -4.2130613 -4.2098002 -4.20902 -4.2102432 -4.2135539 -4.2188168][-4.2326164 -4.2102194 -4.1960468 -4.1811361 -4.1636515 -4.1478944 -4.1459084 -4.1564312 -4.1743374 -4.18081 -4.181325 -4.1857352 -4.1930938 -4.200501 -4.2056589][-4.2089486 -4.1840606 -4.1672773 -4.146811 -4.1209459 -4.0948362 -4.0809126 -4.0864992 -4.1101785 -4.1261387 -4.1352196 -4.1454716 -4.1596055 -4.172689 -4.1828246][-4.1986737 -4.1734657 -4.1529465 -4.1246638 -4.085876 -4.0431871 -4.0093603 -4.008441 -4.04668 -4.0788517 -4.097856 -4.1105847 -4.1242456 -4.1395965 -4.1513004][-4.2017088 -4.1747656 -4.1435528 -4.1021981 -4.0452695 -3.9793773 -3.9186985 -3.9155488 -3.9758544 -4.0259037 -4.0512218 -4.0634279 -4.0750132 -4.0917573 -4.1017303][-4.2047739 -4.17417 -4.1349936 -4.0848837 -4.0150476 -3.9246564 -3.8325241 -3.8255792 -3.9093966 -3.9809792 -4.0116959 -4.0274615 -4.0369353 -4.0530462 -4.058773][-4.2164321 -4.1883488 -4.1542935 -4.1106772 -4.0459862 -3.9568808 -3.8619642 -3.8508615 -3.9328566 -4.003583 -4.034791 -4.0538278 -4.0555577 -4.0619979 -4.0606952][-4.2336345 -4.2089052 -4.1830344 -4.1546125 -4.1136141 -4.0598192 -3.9941616 -3.9811459 -4.0316195 -4.0790296 -4.105444 -4.1167688 -4.10193 -4.0927076 -4.0867534][-4.2455654 -4.2215943 -4.2010894 -4.1875353 -4.1714234 -4.150434 -4.1131773 -4.0989566 -4.1211209 -4.1462827 -4.1655006 -4.166996 -4.1420059 -4.1276479 -4.12747][-4.2557263 -4.230464 -4.2124748 -4.2075987 -4.2083764 -4.205893 -4.1880436 -4.1788034 -4.1875134 -4.2061152 -4.220614 -4.2149239 -4.186708 -4.17485 -4.1845489][-4.2698441 -4.2441883 -4.2268476 -4.2247839 -4.232048 -4.2357774 -4.2276826 -4.2264504 -4.2370677 -4.2548838 -4.26806 -4.2610064 -4.2393684 -4.2308655 -4.2403793][-4.2843175 -4.2608714 -4.2443209 -4.241468 -4.2477818 -4.2530847 -4.2495642 -4.2512431 -4.2612166 -4.2782249 -4.2906485 -4.2869215 -4.2751107 -4.2688775 -4.2722754][-4.3003287 -4.28337 -4.2697945 -4.264976 -4.2663283 -4.2689247 -4.2683306 -4.2704139 -4.2775831 -4.2906036 -4.3014832 -4.2998643 -4.2919426 -4.2874537 -4.2865367][-4.3167777 -4.307271 -4.300231 -4.2966809 -4.2965555 -4.2985592 -4.2996478 -4.3017812 -4.3036628 -4.3097334 -4.3162456 -4.3139834 -4.3082266 -4.30555 -4.3037925]]...]
INFO - root - 2017-12-08 00:11:43.524486: step 25010, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.155 sec/batch; 34h:47m:13s remains)
INFO - root - 2017-12-08 00:12:04.539762: step 25020, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.066 sec/batch; 33h:20m:18s remains)
INFO - root - 2017-12-08 00:12:25.534866: step 25030, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.098 sec/batch; 33h:51m:26s remains)
INFO - root - 2017-12-08 00:12:46.703060: step 25040, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.123 sec/batch; 34h:14m:52s remains)
INFO - root - 2017-12-08 00:13:08.064634: step 25050, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.079 sec/batch; 33h:32m:30s remains)
INFO - root - 2017-12-08 00:13:28.802219: step 25060, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.134 sec/batch; 34h:25m:27s remains)
INFO - root - 2017-12-08 00:13:49.832269: step 25070, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.086 sec/batch; 33h:38m:27s remains)
INFO - root - 2017-12-08 00:14:11.038702: step 25080, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.161 sec/batch; 34h:50m:06s remains)
INFO - root - 2017-12-08 00:14:32.092739: step 25090, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.150 sec/batch; 34h:40m:02s remains)
INFO - root - 2017-12-08 00:14:53.352507: step 25100, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.174 sec/batch; 35h:02m:31s remains)
2017-12-08 00:14:54.900755: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2557683 -4.2268796 -4.2106023 -4.2127519 -4.2275996 -4.2399144 -4.2386093 -4.2291584 -4.2179713 -4.2139382 -4.215497 -4.224473 -4.2374034 -4.2365675 -4.2180281][-4.2412105 -4.210638 -4.1884646 -4.1895404 -4.2124071 -4.2338395 -4.2374673 -4.2293148 -4.2201915 -4.2209477 -4.2234354 -4.2272439 -4.2344837 -4.2302108 -4.2088923][-4.225194 -4.2025642 -4.180212 -4.1754708 -4.1995006 -4.2267809 -4.236011 -4.2319565 -4.2280903 -4.2354331 -4.239759 -4.2386 -4.2377815 -4.2247996 -4.200964][-4.1933131 -4.1863508 -4.1743879 -4.1663442 -4.1840296 -4.2123332 -4.2279282 -4.2280087 -4.228477 -4.2395892 -4.247653 -4.2478657 -4.2442455 -4.2275124 -4.2042289][-4.1653872 -4.1705985 -4.1708217 -4.1680412 -4.1800804 -4.201057 -4.2151446 -4.2113032 -4.2069225 -4.2148461 -4.227283 -4.2374377 -4.2415323 -4.2318821 -4.2138009][-4.1579762 -4.1641726 -4.1686916 -4.1700583 -4.1796188 -4.1912451 -4.196579 -4.1845021 -4.1693158 -4.1709809 -4.1891031 -4.2125058 -4.2278371 -4.2271276 -4.2127161][-4.1780648 -4.1788158 -4.1783977 -4.1778755 -4.1832175 -4.1874871 -4.18361 -4.1654778 -4.1411119 -4.1338215 -4.1523566 -4.184473 -4.2052593 -4.2075348 -4.1964946][-4.2174816 -4.2153578 -4.2111607 -4.2058043 -4.200839 -4.1966548 -4.1870756 -4.1673369 -4.1414242 -4.1301274 -4.1450806 -4.1781673 -4.2012949 -4.2048974 -4.1979108][-4.2390084 -4.2366996 -4.2334809 -4.229382 -4.2234054 -4.2177677 -4.207943 -4.1914058 -4.1715875 -4.161099 -4.1691351 -4.1976914 -4.2212205 -4.224227 -4.2183685][-4.23757 -4.2364655 -4.2375727 -4.2384968 -4.2390571 -4.2370205 -4.2305412 -4.2214785 -4.2112522 -4.2017965 -4.2012911 -4.2230496 -4.2420254 -4.2402477 -4.23207][-4.2205687 -4.2243476 -4.2302318 -4.2368488 -4.2449808 -4.2499962 -4.24847 -4.2459846 -4.2430162 -4.2349348 -4.2286997 -4.243114 -4.2552414 -4.2496343 -4.2410398][-4.1855307 -4.188695 -4.1981592 -4.2120228 -4.2278767 -4.24087 -4.2455382 -4.2469215 -4.2475443 -4.2408428 -4.23434 -4.2430515 -4.2509136 -4.2497034 -4.2464547][-4.1448278 -4.1462722 -4.1579189 -4.1769595 -4.1957803 -4.2107992 -4.2182178 -4.2194433 -4.2196159 -4.214283 -4.2098832 -4.2205033 -4.2351046 -4.2440848 -4.2471633][-4.1162891 -4.1211529 -4.1367459 -4.1559381 -4.1704578 -4.1821208 -4.1872492 -4.1847739 -4.1809473 -4.1743684 -4.1706052 -4.1833997 -4.2052665 -4.221993 -4.23072][-4.1305571 -4.1382155 -4.1526737 -4.1639819 -4.166677 -4.1668544 -4.164166 -4.158534 -4.1540437 -4.1474638 -4.1429658 -4.1540675 -4.1751475 -4.1920967 -4.2028136]]...]
INFO - root - 2017-12-08 00:15:16.195665: step 25110, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.125 sec/batch; 34h:14m:43s remains)
INFO - root - 2017-12-08 00:15:37.199817: step 25120, loss = 2.08, batch loss = 2.02 (16.2 examples/sec; 1.977 sec/batch; 31h:51m:40s remains)
INFO - root - 2017-12-08 00:15:58.337335: step 25130, loss = 2.06, batch loss = 2.00 (14.6 examples/sec; 2.186 sec/batch; 35h:13m:10s remains)
INFO - root - 2017-12-08 00:16:19.652459: step 25140, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.106 sec/batch; 33h:55m:43s remains)
INFO - root - 2017-12-08 00:16:41.047992: step 25150, loss = 2.07, batch loss = 2.02 (14.4 examples/sec; 2.227 sec/batch; 35h:52m:17s remains)
INFO - root - 2017-12-08 00:17:02.183986: step 25160, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.138 sec/batch; 34h:25m:21s remains)
INFO - root - 2017-12-08 00:17:23.442687: step 25170, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.099 sec/batch; 33h:47m:45s remains)
INFO - root - 2017-12-08 00:17:44.847071: step 25180, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.158 sec/batch; 34h:44m:11s remains)
INFO - root - 2017-12-08 00:18:05.786717: step 25190, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 2.078 sec/batch; 33h:26m:13s remains)
INFO - root - 2017-12-08 00:18:27.206230: step 25200, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.127 sec/batch; 34h:13m:23s remains)
2017-12-08 00:18:28.856170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2114067 -4.2278218 -4.2503395 -4.2636914 -4.2686305 -4.2706718 -4.27293 -4.2778592 -4.2785983 -4.2649226 -4.2457728 -4.2297411 -4.2224836 -4.2347016 -4.265234][-4.1588006 -4.1878104 -4.2238474 -4.2457857 -4.2515349 -4.253902 -4.2594514 -4.2700996 -4.2775855 -4.2645411 -4.237545 -4.209516 -4.1920319 -4.2003717 -4.2345948][-4.1172466 -4.1556358 -4.2008905 -4.2264748 -4.2297621 -4.229053 -4.2359419 -4.2527814 -4.2694073 -4.2619777 -4.2294683 -4.1912632 -4.1630917 -4.1664395 -4.2045054][-4.0874686 -4.1274738 -4.174933 -4.1963377 -4.1925049 -4.1828766 -4.1869564 -4.2083244 -4.2365742 -4.2400336 -4.2082667 -4.1639385 -4.1261663 -4.1254144 -4.1698103][-4.0761304 -4.1070786 -4.14662 -4.1576376 -4.1402326 -4.1128139 -4.1040292 -4.1287079 -4.1766744 -4.2017951 -4.1817851 -4.1384149 -4.0966468 -4.094512 -4.1453986][-4.1086926 -4.1204987 -4.139204 -4.1308784 -4.093039 -4.0392051 -4.0014477 -4.0202003 -4.0929852 -4.1514969 -4.1564789 -4.1247959 -4.0876603 -4.0862188 -4.1408691][-4.1594129 -4.1534085 -4.1499434 -4.1226053 -4.0632577 -3.97654 -3.8940563 -3.888638 -3.9863806 -4.086216 -4.1251707 -4.1156273 -4.0902548 -4.0913672 -4.146647][-4.2138133 -4.1993613 -4.1799269 -4.1331024 -4.0528774 -3.9386873 -3.8144917 -3.7755713 -3.8859184 -4.0183029 -4.0893455 -4.1047072 -4.0953217 -4.1033154 -4.1587529][-4.2458544 -4.2328711 -4.2095766 -4.1606259 -4.083683 -3.9830632 -3.8743339 -3.8282592 -3.905057 -4.0126367 -4.0816021 -4.107583 -4.1089778 -4.1237092 -4.1769667][-4.242557 -4.2362461 -4.219347 -4.1826777 -4.1255188 -4.0554829 -3.98343 -3.9465446 -3.9783819 -4.0391712 -4.0876555 -4.1127911 -4.1223488 -4.1445112 -4.1971221][-4.2334614 -4.2346783 -4.2282438 -4.2085767 -4.1732955 -4.1303344 -4.084765 -4.0539508 -4.0554323 -4.0801854 -4.1101103 -4.1311622 -4.1470065 -4.1755276 -4.2256889][-4.2506895 -4.2544808 -4.2559013 -4.2506423 -4.2367582 -4.2167649 -4.1911631 -4.1666808 -4.1539197 -4.156405 -4.1696491 -4.1827726 -4.1978283 -4.2241507 -4.2652087][-4.2853479 -4.28635 -4.2888994 -4.2915182 -4.2914495 -4.2867208 -4.2750115 -4.2577376 -4.242197 -4.2356668 -4.2394505 -4.2468338 -4.2582846 -4.2779636 -4.3064137][-4.3170953 -4.3156104 -4.3160515 -4.321846 -4.3289514 -4.331882 -4.326571 -4.3137336 -4.3006945 -4.2947106 -4.2971892 -4.302721 -4.309617 -4.3212752 -4.3383274][-4.337512 -4.3354387 -4.33503 -4.3407106 -4.3482165 -4.3530889 -4.3512735 -4.3435936 -4.3359776 -4.3343925 -4.3376141 -4.3413367 -4.3443427 -4.3496742 -4.3585191]]...]
INFO - root - 2017-12-08 00:18:50.171845: step 25210, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.065 sec/batch; 33h:12m:57s remains)
INFO - root - 2017-12-08 00:19:11.167206: step 25220, loss = 2.08, batch loss = 2.03 (15.3 examples/sec; 2.085 sec/batch; 33h:32m:17s remains)
INFO - root - 2017-12-08 00:19:32.375653: step 25230, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.115 sec/batch; 34h:00m:26s remains)
INFO - root - 2017-12-08 00:19:53.541790: step 25240, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.107 sec/batch; 33h:52m:29s remains)
INFO - root - 2017-12-08 00:20:14.505756: step 25250, loss = 2.08, batch loss = 2.03 (16.9 examples/sec; 1.890 sec/batch; 30h:22m:39s remains)
INFO - root - 2017-12-08 00:20:35.745473: step 25260, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.158 sec/batch; 34h:40m:47s remains)
INFO - root - 2017-12-08 00:20:56.863806: step 25270, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 33h:47m:20s remains)
INFO - root - 2017-12-08 00:21:18.235019: step 25280, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.171 sec/batch; 34h:53m:06s remains)
INFO - root - 2017-12-08 00:21:38.948201: step 25290, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.099 sec/batch; 33h:43m:20s remains)
INFO - root - 2017-12-08 00:22:00.457640: step 25300, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.150 sec/batch; 34h:32m:04s remains)
2017-12-08 00:22:01.980521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.349329 -4.3486123 -4.3472281 -4.346211 -4.3449659 -4.3449574 -4.345809 -4.3470254 -4.347466 -4.3466182 -4.3454275 -4.3457117 -4.347661 -4.3508768 -4.354744][-4.3421307 -4.3382235 -4.3338637 -4.3282409 -4.3236613 -4.3244467 -4.3285389 -4.3337421 -4.3382177 -4.3410649 -4.3422375 -4.343473 -4.3449931 -4.348022 -4.3530941][-4.328023 -4.3193555 -4.3088927 -4.297019 -4.2870493 -4.2857075 -4.2920752 -4.3012695 -4.3118534 -4.3209138 -4.3270292 -4.3322334 -4.3365688 -4.3425465 -4.3505497][-4.3068509 -4.2889256 -4.2689247 -4.250783 -4.2336287 -4.2254314 -4.2295527 -4.2429218 -4.2638617 -4.28385 -4.2989674 -4.3101945 -4.3195505 -4.3307962 -4.3439012][-4.2801437 -4.2533169 -4.219954 -4.1885653 -4.1567554 -4.1369214 -4.1345468 -4.1453185 -4.1735716 -4.2063203 -4.2324815 -4.2520046 -4.2706442 -4.2928114 -4.3162127][-4.2464442 -4.2078419 -4.1591825 -4.1064763 -4.0520144 -4.0129466 -3.9983592 -3.9982955 -4.0334663 -4.0832119 -4.12191 -4.1496029 -4.1804729 -4.2181039 -4.2578807][-4.2147279 -4.1643152 -4.0994277 -4.0191283 -3.9295793 -3.8585379 -3.8174703 -3.7920454 -3.8269584 -3.8921459 -3.9468455 -3.9890597 -4.0371995 -4.0971231 -4.1612225][-4.1918364 -4.1348939 -4.059413 -3.9551649 -3.8306379 -3.7234576 -3.6392446 -3.5670588 -3.5925789 -3.6809573 -3.7587719 -3.8154616 -3.8811007 -3.9663959 -4.0592484][-4.18894 -4.1396194 -4.0729733 -3.9784832 -3.8590536 -3.7431014 -3.6254783 -3.5074339 -3.4999938 -3.57552 -3.6445258 -3.7013774 -3.7796686 -3.8877668 -4.0000572][-4.215817 -4.1842442 -4.1409445 -4.0815487 -4.0069084 -3.9272592 -3.8368483 -3.7422211 -3.7114499 -3.72713 -3.7480488 -3.779649 -3.8418841 -3.9332223 -4.0286379][-4.2653995 -4.2532511 -4.2329249 -4.2053943 -4.1702652 -4.128458 -4.0753174 -4.0170183 -3.9859262 -3.9708824 -3.9646485 -3.9743168 -4.0080686 -4.0647607 -4.1247182][-4.3041639 -4.3010049 -4.2934194 -4.2831798 -4.2699833 -4.2514176 -4.2251897 -4.196682 -4.175952 -4.1557755 -4.1445775 -4.1445656 -4.1576109 -4.1855145 -4.2164989][-4.3285441 -4.325953 -4.3237023 -4.3193941 -4.3138914 -4.3055887 -4.2911983 -4.2774353 -4.2658343 -4.2529893 -4.2461419 -4.2447615 -4.2495518 -4.2603755 -4.2755327][-4.3458443 -4.3442683 -4.3423629 -4.3395514 -4.3365388 -4.3322859 -4.325243 -4.3182778 -4.311573 -4.3042831 -4.299921 -4.2992024 -4.301209 -4.3056879 -4.3133249][-4.35621 -4.3556995 -4.3535237 -4.3499813 -4.3457842 -4.3423958 -4.3397446 -4.3369403 -4.3341236 -4.3316431 -4.3299537 -4.3301177 -4.3318849 -4.3336277 -4.3374667]]...]
INFO - root - 2017-12-08 00:22:23.260694: step 25310, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.148 sec/batch; 34h:29m:19s remains)
INFO - root - 2017-12-08 00:22:44.306054: step 25320, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.116 sec/batch; 33h:59m:01s remains)
INFO - root - 2017-12-08 00:23:05.507296: step 25330, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.129 sec/batch; 34h:10m:43s remains)
INFO - root - 2017-12-08 00:23:26.939625: step 25340, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.136 sec/batch; 34h:16m:52s remains)
INFO - root - 2017-12-08 00:23:47.943487: step 25350, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.078 sec/batch; 33h:20m:55s remains)
INFO - root - 2017-12-08 00:24:09.197020: step 25360, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.133 sec/batch; 34h:13m:50s remains)
INFO - root - 2017-12-08 00:24:30.462325: step 25370, loss = 2.09, batch loss = 2.03 (14.7 examples/sec; 2.181 sec/batch; 34h:59m:48s remains)
INFO - root - 2017-12-08 00:24:51.323806: step 25380, loss = 2.08, batch loss = 2.03 (14.8 examples/sec; 2.157 sec/batch; 34h:35m:58s remains)
INFO - root - 2017-12-08 00:25:12.469239: step 25390, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.090 sec/batch; 33h:30m:50s remains)
INFO - root - 2017-12-08 00:25:33.571166: step 25400, loss = 2.08, batch loss = 2.03 (14.9 examples/sec; 2.153 sec/batch; 34h:31m:45s remains)
2017-12-08 00:25:35.165840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3453469 -4.3443303 -4.337677 -4.3237391 -4.3103604 -4.302835 -4.3015494 -4.3071051 -4.3180261 -4.3268685 -4.3351374 -4.3403759 -4.3428912 -4.3452663 -4.3469529][-4.3377013 -4.3341608 -4.324697 -4.3054895 -4.2882414 -4.2795711 -4.2819796 -4.291214 -4.3060117 -4.3202186 -4.3332767 -4.3421135 -4.344048 -4.3449316 -4.3469148][-4.3282089 -4.3215666 -4.3071542 -4.2819681 -4.2644696 -4.2606845 -4.2704334 -4.2833719 -4.2989912 -4.3166871 -4.3345652 -4.3473125 -4.3491321 -4.3485527 -4.3515749][-4.3177476 -4.3091645 -4.2884965 -4.2567673 -4.2392745 -4.241353 -4.2593651 -4.2788305 -4.2968016 -4.3161139 -4.3378725 -4.3527832 -4.3539176 -4.3523378 -4.3564467][-4.30444 -4.2947578 -4.2658396 -4.2214279 -4.1916103 -4.1885166 -4.2118964 -4.2421389 -4.2706523 -4.2987676 -4.3264761 -4.3457112 -4.3499975 -4.3493304 -4.3538256][-4.2883148 -4.2773142 -4.2409048 -4.1797562 -4.1242533 -4.0981159 -4.1149025 -4.1575723 -4.2045784 -4.2502756 -4.2911139 -4.3219743 -4.3373108 -4.3411632 -4.3454804][-4.2719359 -4.2634687 -4.2237616 -4.1496754 -4.0665216 -4.0069366 -4.0033474 -4.0531683 -4.1220059 -4.185194 -4.2378163 -4.2831974 -4.3140826 -4.3278384 -4.3335924][-4.251183 -4.2514477 -4.2210407 -4.1514463 -4.0584493 -3.9731874 -3.9423356 -3.9844093 -4.0608282 -4.1306462 -4.1855931 -4.2384992 -4.2810259 -4.3049974 -4.3137288][-4.22496 -4.235323 -4.2223248 -4.1725287 -4.0897574 -3.9965684 -3.9422491 -3.9611778 -4.0250049 -4.0887523 -4.1402817 -4.192625 -4.2385359 -4.2694926 -4.2817683][-4.1925683 -4.2083764 -4.2096953 -4.18187 -4.1181068 -4.0323873 -3.9662321 -3.9606233 -4.0058055 -4.0630455 -4.1109571 -4.1574697 -4.1976357 -4.2282391 -4.2415791][-4.1619072 -4.179317 -4.1903176 -4.1825967 -4.1450272 -4.0791907 -4.01438 -3.9925864 -4.0210018 -4.0715408 -4.1144948 -4.1499267 -4.1762633 -4.1973948 -4.2058129][-4.1569614 -4.1720433 -4.1883397 -4.1962566 -4.1836228 -4.1434555 -4.0933828 -4.0672159 -4.080255 -4.11797 -4.1524205 -4.173986 -4.183444 -4.1901875 -4.1906219][-4.1829996 -4.1892667 -4.2036538 -4.21775 -4.2233992 -4.2107415 -4.1833291 -4.1634173 -4.1679859 -4.189826 -4.2109833 -4.2187 -4.215734 -4.2120624 -4.2084236][-4.2162619 -4.2087555 -4.2141314 -4.2279387 -4.2439618 -4.2488127 -4.2402954 -4.230092 -4.2306018 -4.2385879 -4.2489834 -4.253067 -4.251843 -4.2502213 -4.2494631][-4.2412291 -4.2173367 -4.2082281 -4.2144351 -4.2300472 -4.2393508 -4.2391024 -4.2388811 -4.2401748 -4.2414947 -4.249979 -4.258225 -4.2670588 -4.2741213 -4.2795186]]...]
INFO - root - 2017-12-08 00:25:56.269660: step 25410, loss = 2.08, batch loss = 2.02 (16.2 examples/sec; 1.978 sec/batch; 31h:42m:30s remains)
INFO - root - 2017-12-08 00:26:17.502901: step 25420, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.116 sec/batch; 33h:55m:28s remains)
INFO - root - 2017-12-08 00:26:38.650802: step 25430, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.088 sec/batch; 33h:28m:04s remains)
INFO - root - 2017-12-08 00:26:59.975106: step 25440, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.078 sec/batch; 33h:18m:09s remains)
INFO - root - 2017-12-08 00:27:20.857179: step 25450, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 33h:58m:47s remains)
INFO - root - 2017-12-08 00:27:42.051468: step 25460, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.097 sec/batch; 33h:34m:59s remains)
INFO - root - 2017-12-08 00:28:03.562182: step 25470, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.155 sec/batch; 34h:30m:33s remains)
INFO - root - 2017-12-08 00:28:24.415927: step 25480, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.153 sec/batch; 34h:28m:27s remains)
INFO - root - 2017-12-08 00:28:45.645899: step 25490, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.140 sec/batch; 34h:15m:42s remains)
INFO - root - 2017-12-08 00:29:06.794067: step 25500, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.098 sec/batch; 33h:35m:04s remains)
2017-12-08 00:29:08.378634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3268433 -4.317759 -4.313467 -4.3142257 -4.318028 -4.3202004 -4.3186493 -4.3169494 -4.3173556 -4.3172 -4.3134317 -4.3055973 -4.2888556 -4.2687659 -4.2503996][-4.338438 -4.3279405 -4.3224707 -4.3230395 -4.3271418 -4.32983 -4.3280144 -4.3240681 -4.3229008 -4.3220282 -4.3155861 -4.3044991 -4.2842784 -4.2604494 -4.2352324][-4.3433318 -4.329792 -4.3223252 -4.3215256 -4.3255692 -4.3279834 -4.323431 -4.3143458 -4.3102078 -4.3082428 -4.30077 -4.2879963 -4.2689066 -4.248857 -4.2255254][-4.3370833 -4.3185716 -4.3039951 -4.2985067 -4.3000278 -4.2988172 -4.2878056 -4.2728524 -4.264873 -4.2656889 -4.2623968 -4.25328 -4.2409253 -4.2300296 -4.2157459][-4.3284731 -4.3005795 -4.2747068 -4.2608304 -4.2584991 -4.2526035 -4.2322311 -4.2065811 -4.1984596 -4.2110033 -4.2208576 -4.2216978 -4.22242 -4.2242889 -4.2225089][-4.3141947 -4.2757435 -4.2360358 -4.20638 -4.1931629 -4.17632 -4.1428232 -4.1029062 -4.106575 -4.1522522 -4.1913142 -4.208097 -4.221508 -4.2364054 -4.246294][-4.2873945 -4.2396131 -4.18262 -4.1282048 -4.0900307 -4.0482292 -3.9838264 -3.9222989 -3.9510322 -4.0493345 -4.1268649 -4.1675339 -4.1984706 -4.2261796 -4.2446966][-4.247623 -4.2007217 -4.1363869 -4.0681047 -4.0091605 -3.9426188 -3.8348637 -3.7394083 -3.7984166 -3.9535666 -4.069087 -4.1315169 -4.170239 -4.1987233 -4.2123752][-4.2085285 -4.1700158 -4.1149735 -4.0566792 -4.004199 -3.9426262 -3.8350282 -3.7454481 -3.8093135 -3.9538767 -4.0640287 -4.1244698 -4.1525831 -4.1670537 -4.1676979][-4.1807785 -4.1467395 -4.1006756 -4.0607362 -4.0357924 -4.011395 -3.9575634 -3.9167442 -3.9593122 -4.0409808 -4.1043253 -4.136003 -4.1444645 -4.1367149 -4.1155515][-4.1744885 -4.1394053 -4.0963888 -4.0699625 -4.0706468 -4.0821247 -4.0750594 -4.0664024 -4.0897579 -4.1253638 -4.1454186 -4.14682 -4.1399341 -4.113966 -4.0749989][-4.1966486 -4.1605177 -4.1190171 -4.097753 -4.1108856 -4.1415973 -4.1587243 -4.1648755 -4.1772194 -4.1880627 -4.1791263 -4.1589093 -4.1400609 -4.1051283 -4.06547][-4.2427139 -4.210937 -4.1762757 -4.15744 -4.172194 -4.2062564 -4.22988 -4.2415977 -4.2495823 -4.2476239 -4.2295532 -4.2071033 -4.1905851 -4.1583633 -4.1262312][-4.2887616 -4.2679167 -4.2441797 -4.2287197 -4.2378926 -4.2636757 -4.2838817 -4.2953715 -4.3008633 -4.2961469 -4.28357 -4.2715816 -4.2638936 -4.2414241 -4.21996][-4.3177972 -4.3073306 -4.2935777 -4.2832131 -4.2877283 -4.3036022 -4.314352 -4.3195667 -4.3214684 -4.3191581 -4.3145962 -4.3112764 -4.3093009 -4.2973533 -4.2876115]]...]
INFO - root - 2017-12-08 00:29:29.119065: step 25510, loss = 2.08, batch loss = 2.03 (15.0 examples/sec; 2.132 sec/batch; 34h:06m:57s remains)
INFO - root - 2017-12-08 00:29:50.140851: step 25520, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.098 sec/batch; 33h:34m:31s remains)
INFO - root - 2017-12-08 00:30:11.434601: step 25530, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.137 sec/batch; 34h:11m:46s remains)
INFO - root - 2017-12-08 00:30:32.364932: step 25540, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.102 sec/batch; 33h:37m:12s remains)
INFO - root - 2017-12-08 00:30:53.441216: step 25550, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.129 sec/batch; 34h:02m:39s remains)
INFO - root - 2017-12-08 00:31:14.810288: step 25560, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 33h:53m:08s remains)
INFO - root - 2017-12-08 00:31:35.996398: step 25570, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.077 sec/batch; 33h:12m:41s remains)
INFO - root - 2017-12-08 00:31:56.804375: step 25580, loss = 2.08, batch loss = 2.03 (15.1 examples/sec; 2.123 sec/batch; 33h:56m:21s remains)
INFO - root - 2017-12-08 00:32:17.984892: step 25590, loss = 2.06, batch loss = 2.00 (15.3 examples/sec; 2.096 sec/batch; 33h:30m:21s remains)
INFO - root - 2017-12-08 00:32:39.217119: step 25600, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.117 sec/batch; 33h:50m:05s remains)
2017-12-08 00:32:40.816620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2817411 -4.25791 -4.2426047 -4.2517924 -4.2709718 -4.2850142 -4.2894411 -4.2993183 -4.3125105 -4.3165913 -4.3164911 -4.3183293 -4.3123608 -4.3035398 -4.2964945][-4.2867312 -4.2585206 -4.2407289 -4.249939 -4.2731457 -4.2896791 -4.2926397 -4.2993417 -4.3083043 -4.3118396 -4.3128309 -4.3195152 -4.3208008 -4.3172674 -4.3122764][-4.2925544 -4.2665162 -4.251286 -4.2592206 -4.2773528 -4.2891164 -4.2854662 -4.2860918 -4.292356 -4.2968082 -4.2983284 -4.3076563 -4.3160062 -4.3197637 -4.3195286][-4.2922697 -4.2731924 -4.2645493 -4.2692719 -4.276557 -4.276237 -4.262084 -4.2533231 -4.25606 -4.2647095 -4.2699747 -4.2829566 -4.2988586 -4.3120217 -4.3190265][-4.2906656 -4.2813597 -4.2780604 -4.2763495 -4.27038 -4.2516365 -4.2185011 -4.1974115 -4.2014647 -4.2213416 -4.2365308 -4.2551661 -4.2793541 -4.3017321 -4.3150706][-4.2956438 -4.293911 -4.2913828 -4.2815289 -4.2591095 -4.2132163 -4.1500039 -4.1112947 -4.1259055 -4.1681738 -4.2005177 -4.2309575 -4.2653913 -4.2950444 -4.3118854][-4.2853146 -4.2827249 -4.2782545 -4.2609243 -4.2226152 -4.1492686 -4.0521097 -4.0039358 -4.0457783 -4.1190724 -4.1738405 -4.2190857 -4.259872 -4.2923913 -4.3107023][-4.2341385 -4.228507 -4.2247324 -4.2067394 -4.1647758 -4.0823383 -3.9720249 -3.9351106 -4.0117774 -4.11134 -4.1823783 -4.2382517 -4.2762318 -4.3027887 -4.3164859][-4.1642251 -4.1601343 -4.1642308 -4.1588874 -4.1345406 -4.078732 -4.0016484 -3.9901862 -4.0710015 -4.1641 -4.2314672 -4.2801604 -4.3038788 -4.3158331 -4.3204904][-4.1470079 -4.1502652 -4.1582165 -4.1621127 -4.1586022 -4.1376991 -4.1046877 -4.1074753 -4.1641817 -4.2282391 -4.27607 -4.3098226 -4.3192635 -4.3187904 -4.3160071][-4.1906343 -4.1911025 -4.1881628 -4.187521 -4.191123 -4.1898074 -4.18113 -4.183845 -4.2133756 -4.2517586 -4.28432 -4.3099504 -4.3171659 -4.3150339 -4.3102751][-4.2329082 -4.2198167 -4.2014265 -4.1926332 -4.1973004 -4.201931 -4.2004776 -4.2009144 -4.216157 -4.2451787 -4.275774 -4.302856 -4.3142405 -4.3142695 -4.3106689][-4.2506623 -4.2232704 -4.1942616 -4.1826067 -4.1874833 -4.1919231 -4.1895714 -4.1876922 -4.2019835 -4.2353482 -4.2720528 -4.300972 -4.3149285 -4.3178539 -4.317688][-4.2513266 -4.220191 -4.191009 -4.182364 -4.1865764 -4.1867943 -4.1799035 -4.1762085 -4.1948137 -4.2349639 -4.2746134 -4.3001623 -4.3113604 -4.3151031 -4.3176103][-4.2410316 -4.2210226 -4.2062378 -4.2054715 -4.2092309 -4.205524 -4.1955242 -4.1902061 -4.208478 -4.2461352 -4.2801056 -4.2988605 -4.3066025 -4.3110781 -4.3176913]]...]
INFO - root - 2017-12-08 00:33:01.836084: step 25610, loss = 2.09, batch loss = 2.03 (14.5 examples/sec; 2.209 sec/batch; 35h:17m:07s remains)
INFO - root - 2017-12-08 00:33:23.238500: step 25620, loss = 2.07, batch loss = 2.02 (15.0 examples/sec; 2.137 sec/batch; 34h:08m:07s remains)
INFO - root - 2017-12-08 00:33:44.240304: step 25630, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.075 sec/batch; 33h:08m:23s remains)
INFO - root - 2017-12-08 00:34:05.050056: step 25640, loss = 2.06, batch loss = 2.01 (15.2 examples/sec; 2.101 sec/batch; 33h:32m:31s remains)
INFO - root - 2017-12-08 00:34:26.151719: step 25650, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.116 sec/batch; 33h:46m:44s remains)
INFO - root - 2017-12-08 00:34:47.381191: step 25660, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.149 sec/batch; 34h:18m:31s remains)
INFO - root - 2017-12-08 00:35:08.367118: step 25670, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.113 sec/batch; 33h:43m:09s remains)
INFO - root - 2017-12-08 00:35:29.623571: step 25680, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.188 sec/batch; 34h:54m:22s remains)
INFO - root - 2017-12-08 00:35:50.805213: step 25690, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.110 sec/batch; 33h:39m:27s remains)
INFO - root - 2017-12-08 00:36:11.837180: step 25700, loss = 2.07, batch loss = 2.02 (16.4 examples/sec; 1.956 sec/batch; 31h:12m:28s remains)
2017-12-08 00:36:13.259743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3268266 -4.327033 -4.3192182 -4.3051014 -4.2907043 -4.2844105 -4.2934957 -4.310442 -4.3244205 -4.3323441 -4.3366427 -4.3363934 -4.3315091 -4.3241267 -4.317347][-4.314919 -4.3046389 -4.2841334 -4.2542143 -4.2258759 -4.2106118 -4.2225037 -4.2496805 -4.2748413 -4.295135 -4.3143086 -4.3268013 -4.3301167 -4.3270526 -4.3219895][-4.2899766 -4.2665806 -4.2282548 -4.1826138 -4.1455441 -4.1255713 -4.1413188 -4.1790409 -4.2103629 -4.2370958 -4.2700248 -4.2952552 -4.3075876 -4.3126616 -4.313591][-4.2592382 -4.2221107 -4.1636643 -4.0987959 -4.0450773 -4.0184183 -4.0433412 -4.0942416 -4.13129 -4.1648922 -4.2145677 -4.2488728 -4.2679172 -4.2828054 -4.29409][-4.241024 -4.1910248 -4.1106663 -4.0205011 -3.9386568 -3.8947489 -3.9269204 -3.9974072 -4.0440903 -4.0909505 -4.1578512 -4.1976266 -4.2245917 -4.2499189 -4.2719541][-4.2397885 -4.18255 -4.0936074 -3.9955902 -3.8988059 -3.8438423 -3.8698494 -3.9295521 -3.9652903 -4.0136929 -4.0902696 -4.1337495 -4.1698604 -4.2077193 -4.2433071][-4.2606745 -4.2106996 -4.1338363 -4.0552478 -3.9772654 -3.9301591 -3.9357991 -3.9514451 -3.9433446 -3.9623981 -4.024673 -4.0663724 -4.1123104 -4.1641512 -4.2126684][-4.2905512 -4.2579865 -4.2061524 -4.1576452 -4.1044621 -4.0669665 -4.0463991 -4.0156617 -3.9668314 -3.9543171 -3.9917862 -4.0259666 -4.0765648 -4.1396546 -4.2007632][-4.3139849 -4.2973604 -4.2689977 -4.2441087 -4.2103863 -4.17568 -4.1317687 -4.0680904 -3.9896734 -3.9573874 -3.9838924 -4.02333 -4.085042 -4.1561589 -4.220438][-4.3258362 -4.3205242 -4.3070583 -4.2953811 -4.2718511 -4.2354522 -4.1813092 -4.10547 -4.0161977 -3.9775953 -4.0062881 -4.0574627 -4.1283684 -4.1992784 -4.2559891][-4.3285384 -4.3270912 -4.3203259 -4.3156872 -4.3003974 -4.2680025 -4.2177978 -4.1527529 -4.0774608 -4.0486612 -4.0794439 -4.1321378 -4.1976094 -4.2550573 -4.2945657][-4.325707 -4.3262186 -4.3244724 -4.3258133 -4.319315 -4.297092 -4.2593412 -4.2131848 -4.1675129 -4.157124 -4.1881509 -4.2344408 -4.2818689 -4.3135228 -4.3278904][-4.3200359 -4.3219595 -4.3226619 -4.3280182 -4.3298869 -4.3189259 -4.294311 -4.2661595 -4.2469525 -4.2506614 -4.27608 -4.3097816 -4.3378978 -4.3473158 -4.3440042][-4.3184438 -4.3209596 -4.3223834 -4.3277025 -4.333 -4.3307958 -4.3181438 -4.3031292 -4.2965007 -4.3030787 -4.3186 -4.3383145 -4.3524647 -4.3525248 -4.3441443][-4.319881 -4.3225141 -4.3246078 -4.3289819 -4.3341742 -4.3354588 -4.3300886 -4.3225455 -4.318892 -4.3232918 -4.3316092 -4.3416271 -4.3473492 -4.3449631 -4.3382254]]...]
INFO - root - 2017-12-08 00:36:34.429658: step 25710, loss = 2.06, batch loss = 2.01 (15.1 examples/sec; 2.121 sec/batch; 33h:50m:03s remains)
INFO - root - 2017-12-08 00:36:55.632843: step 25720, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 34h:26m:27s remains)
INFO - root - 2017-12-08 00:37:17.069990: step 25730, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 34h:26m:27s remains)
INFO - root - 2017-12-08 00:37:37.914357: step 25740, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.093 sec/batch; 33h:21m:59s remains)
INFO - root - 2017-12-08 00:37:59.069336: step 25750, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 33h:31m:52s remains)
INFO - root - 2017-12-08 00:38:20.446342: step 25760, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 33h:41m:17s remains)
INFO - root - 2017-12-08 00:38:41.533388: step 25770, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.162 sec/batch; 34h:27m:05s remains)
INFO - root - 2017-12-08 00:39:02.771246: step 25780, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 33h:50m:11s remains)
INFO - root - 2017-12-08 00:39:23.944166: step 25790, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.134 sec/batch; 33h:58m:56s remains)
INFO - root - 2017-12-08 00:39:44.777606: step 25800, loss = 2.08, batch loss = 2.03 (15.3 examples/sec; 2.086 sec/batch; 33h:13m:22s remains)
2017-12-08 00:39:46.352850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2792869 -4.2652683 -4.2511282 -4.2510796 -4.2593532 -4.263721 -4.264431 -4.2697639 -4.2820773 -4.29144 -4.288518 -4.2648392 -4.2286077 -4.2021866 -4.2031455][-4.2950277 -4.2787433 -4.2607803 -4.2591524 -4.2680392 -4.2698059 -4.2619562 -4.2550907 -4.25893 -4.2665291 -4.258533 -4.2238946 -4.1744347 -4.1441622 -4.1494017][-4.300796 -4.2836266 -4.2696085 -4.2731438 -4.2859125 -4.2861471 -4.2678423 -4.2466149 -4.2409778 -4.2415042 -4.2264795 -4.184093 -4.1279941 -4.0940509 -4.0995326][-4.2832203 -4.2637162 -4.2566552 -4.2703409 -4.2912192 -4.294096 -4.2689366 -4.2389665 -4.2283359 -4.2260213 -4.2108526 -4.1714373 -4.1191173 -4.0856614 -4.087131][-4.2429528 -4.2190528 -4.2190084 -4.2412539 -4.2691464 -4.2751193 -4.2495046 -4.2169123 -4.2078509 -4.2109036 -4.2022543 -4.1727753 -4.1282949 -4.0957842 -4.0949364][-4.1876249 -4.1598997 -4.168211 -4.1999049 -4.2296114 -4.2312522 -4.1999741 -4.1658883 -4.1639462 -4.1804786 -4.1892881 -4.1742477 -4.1358738 -4.1046395 -4.1023254][-4.1506853 -4.1223536 -4.1350608 -4.1680403 -4.1879821 -4.1700811 -4.1142468 -4.0665417 -4.0721493 -4.1139522 -4.14839 -4.1547065 -4.1295118 -4.1013894 -4.0939565][-4.1588755 -4.1355405 -4.1472931 -4.17016 -4.1666813 -4.113306 -4.0189333 -3.9485536 -3.9627171 -4.0344648 -4.0998516 -4.1327505 -4.1270771 -4.1018119 -4.0856314][-4.1942449 -4.1790915 -4.1889405 -4.2034264 -4.1838908 -4.1091833 -3.995645 -3.9140162 -3.9303782 -4.0128303 -4.0895271 -4.1381593 -4.1473322 -4.1210761 -4.0980353][-4.2345276 -4.2289476 -4.2388358 -4.2483273 -4.2251806 -4.1564727 -4.0576735 -3.9859896 -3.9976645 -4.0681758 -4.136673 -4.1818829 -4.1895447 -4.1565351 -4.1272421][-4.2731743 -4.2736659 -4.2818117 -4.2868576 -4.2660618 -4.2140212 -4.1404357 -4.0854282 -4.0914927 -4.1434736 -4.1931424 -4.23026 -4.2366657 -4.2015104 -4.1669855][-4.3044982 -4.3094988 -4.3138928 -4.3121738 -4.2931356 -4.2546668 -4.2000966 -4.1580887 -4.1614223 -4.1979251 -4.2318063 -4.2598557 -4.2673173 -4.2408972 -4.2109718][-4.312571 -4.3238759 -4.3287172 -4.3248782 -4.3080955 -4.2772241 -4.2334714 -4.1989007 -4.1996775 -4.2285094 -4.2571597 -4.2780852 -4.2853413 -4.2692065 -4.2476411][-4.2903471 -4.3110538 -4.3237667 -4.3271646 -4.3175292 -4.2911825 -4.2501135 -4.2171769 -4.2150655 -4.2390828 -4.262383 -4.2777109 -4.28403 -4.2754736 -4.2608819][-4.2422667 -4.2736731 -4.2981935 -4.3140726 -4.3166943 -4.3000412 -4.2679071 -4.24148 -4.2362156 -4.2503791 -4.2646437 -4.2732525 -4.2743869 -4.2660332 -4.2545495]]...]
INFO - root - 2017-12-08 00:40:07.544801: step 25810, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.101 sec/batch; 33h:26m:45s remains)
INFO - root - 2017-12-08 00:40:28.970305: step 25820, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 33h:43m:22s remains)
INFO - root - 2017-12-08 00:40:49.870730: step 25830, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.074 sec/batch; 33h:00m:50s remains)
INFO - root - 2017-12-08 00:41:11.255797: step 25840, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.106 sec/batch; 33h:30m:57s remains)
INFO - root - 2017-12-08 00:41:32.373097: step 25850, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.106 sec/batch; 33h:30m:37s remains)
INFO - root - 2017-12-08 00:41:53.849829: step 25860, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.153 sec/batch; 34h:15m:05s remains)
INFO - root - 2017-12-08 00:42:14.707078: step 25870, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.115 sec/batch; 33h:37m:58s remains)
INFO - root - 2017-12-08 00:42:35.873626: step 25880, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.121 sec/batch; 33h:43m:14s remains)
INFO - root - 2017-12-08 00:42:57.454083: step 25890, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.146 sec/batch; 34h:07m:06s remains)
INFO - root - 2017-12-08 00:43:18.410156: step 25900, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.159 sec/batch; 34h:19m:29s remains)
2017-12-08 00:43:20.037921: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2706513 -4.2733955 -4.2741308 -4.2762775 -4.2811522 -4.2827921 -4.277667 -4.2765851 -4.2888193 -4.304338 -4.3129 -4.3136358 -4.3086824 -4.3053389 -4.3115859][-4.230721 -4.2338905 -4.2313485 -4.2284813 -4.2329464 -4.2364693 -4.2309551 -4.2289476 -4.2457747 -4.2664809 -4.2784457 -4.2831059 -4.2831216 -4.2843122 -4.2947206][-4.1805091 -4.182035 -4.17853 -4.1736236 -4.1785235 -4.1846547 -4.1787276 -4.1771894 -4.1986766 -4.2227216 -4.2384949 -4.2477589 -4.2524924 -4.2579937 -4.2718644][-4.128963 -4.1262517 -4.1277919 -4.1257739 -4.1284866 -4.1323624 -4.1241331 -4.1215563 -4.1458578 -4.1724725 -4.1920033 -4.2045183 -4.2111459 -4.2200413 -4.2379503][-4.0812907 -4.0720172 -4.0809841 -4.0868559 -4.0851941 -4.078135 -4.0587134 -4.0488009 -4.0726962 -4.1054664 -4.1344557 -4.1540847 -4.1637511 -4.1768217 -4.2020183][-4.0488415 -4.0325761 -4.044642 -4.0530319 -4.0444493 -4.0232482 -3.9862771 -3.9641621 -3.9854183 -4.028079 -4.0730948 -4.1046534 -4.1232147 -4.1451893 -4.1798596][-4.0413427 -4.0181031 -4.0257454 -4.0289917 -4.0122476 -3.9753683 -3.9173181 -3.8778787 -3.8969946 -3.9514849 -4.0164781 -4.0662994 -4.0991325 -4.1318946 -4.17315][-4.0660033 -4.0442567 -4.0428972 -4.0355587 -4.0080948 -3.9540429 -3.8805296 -3.8283553 -3.8431044 -3.9045577 -3.9827833 -4.045918 -4.0891738 -4.1289096 -4.1698141][-4.1001496 -4.0856624 -4.0778832 -4.0600138 -4.0241756 -3.9658608 -3.8956113 -3.850668 -3.8619447 -3.9129581 -3.9859669 -4.049315 -4.09413 -4.1330919 -4.1673918][-4.1266036 -4.1153636 -4.1057549 -4.0866818 -4.052217 -4.0002789 -3.945375 -3.9138398 -3.9181571 -3.949209 -4.0060205 -4.0648947 -4.1077657 -4.1423731 -4.1694031][-4.1471934 -4.1325283 -4.1218286 -4.1068363 -4.0814543 -4.0442271 -4.00869 -3.9879129 -3.9823062 -3.9932339 -4.029973 -4.0802922 -4.1197567 -4.1506896 -4.1733479][-4.1681371 -4.1494241 -4.1394777 -4.1311326 -4.1166887 -4.0959067 -4.076591 -4.0606565 -4.0456643 -4.0405245 -4.0609655 -4.1008644 -4.1354146 -4.1625552 -4.1831369][-4.1999083 -4.182652 -4.1757627 -4.17269 -4.166995 -4.1580806 -4.1481762 -4.1371942 -4.1212516 -4.1109328 -4.1199989 -4.1467137 -4.1729474 -4.1941071 -4.2107668][-4.2408528 -4.2288322 -4.2254338 -4.2254033 -4.2243037 -4.2215581 -4.2164145 -4.2107282 -4.2014894 -4.1951251 -4.2001734 -4.2173619 -4.2361584 -4.2490039 -4.2584157][-4.2844176 -4.2776933 -4.2755623 -4.2755275 -4.2752156 -4.274684 -4.2735386 -4.27274 -4.2698331 -4.2687683 -4.2727728 -4.282846 -4.2939334 -4.3000345 -4.3043976]]...]
INFO - root - 2017-12-08 00:43:41.367318: step 25910, loss = 2.06, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 33h:41m:41s remains)
INFO - root - 2017-12-08 00:44:02.647202: step 25920, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.107 sec/batch; 33h:28m:30s remains)
INFO - root - 2017-12-08 00:44:23.499720: step 25930, loss = 2.06, batch loss = 2.01 (15.4 examples/sec; 2.082 sec/batch; 33h:05m:08s remains)
INFO - root - 2017-12-08 00:44:44.770344: step 25940, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.104 sec/batch; 33h:25m:02s remains)
INFO - root - 2017-12-08 00:45:06.265618: step 25950, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.175 sec/batch; 34h:32m:24s remains)
INFO - root - 2017-12-08 00:45:27.150686: step 25960, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.131 sec/batch; 33h:50m:05s remains)
INFO - root - 2017-12-08 00:45:48.528920: step 25970, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.085 sec/batch; 33h:06m:29s remains)
INFO - root - 2017-12-08 00:46:09.772722: step 25980, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.071 sec/batch; 32h:52m:43s remains)
INFO - root - 2017-12-08 00:46:30.823692: step 25990, loss = 2.09, batch loss = 2.04 (16.9 examples/sec; 1.892 sec/batch; 30h:01m:19s remains)
INFO - root - 2017-12-08 00:46:52.036200: step 26000, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 34h:16m:06s remains)
2017-12-08 00:46:53.601610: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2468371 -4.2341084 -4.2186852 -4.2364855 -4.2755136 -4.2858052 -4.2677426 -4.2531509 -4.2375646 -4.224216 -4.2185698 -4.2139125 -4.1962423 -4.148386 -4.0842423][-4.2194872 -4.199748 -4.1843004 -4.2170525 -4.2684259 -4.2802238 -4.2590513 -4.2359786 -4.2069311 -4.1799669 -4.1670847 -4.1630464 -4.1463075 -4.0928397 -4.0197239][-4.1962376 -4.169899 -4.1591668 -4.2050986 -4.2640185 -4.2757678 -4.2537556 -4.2299008 -4.1944466 -4.1594248 -4.1451793 -4.1450882 -4.1271186 -4.0673165 -3.9873567][-4.1967669 -4.1714673 -4.1689548 -4.2176495 -4.2701344 -4.2748089 -4.2479939 -4.2239227 -4.1948295 -4.1697435 -4.1677647 -4.177691 -4.1618152 -4.102529 -4.0243449][-4.2013068 -4.1760068 -4.180172 -4.223453 -4.2622328 -4.2572045 -4.2227173 -4.19912 -4.1863909 -4.1861782 -4.2018919 -4.2187095 -4.2034373 -4.1564207 -4.0947461][-4.1869535 -4.1620097 -4.16788 -4.2016768 -4.2242584 -4.1974411 -4.1349773 -4.1040068 -4.1201215 -4.1620245 -4.20593 -4.2358341 -4.2282777 -4.1980872 -4.1499596][-4.1675549 -4.1463647 -4.1482911 -4.1675863 -4.1713648 -4.1100225 -4.0024104 -3.9522092 -3.999475 -4.0934892 -4.1763916 -4.2312078 -4.238461 -4.21602 -4.1699271][-4.1369681 -4.1169825 -4.1162024 -4.1228909 -4.1063008 -4.0122185 -3.8643765 -3.8016686 -3.8932681 -4.0390229 -4.1585932 -4.2297111 -4.2454858 -4.2222896 -4.1730757][-4.1364307 -4.1171579 -4.1175327 -4.1128635 -4.0742493 -3.9604075 -3.80281 -3.7533119 -3.8814249 -4.0494461 -4.178576 -4.2491117 -4.2591624 -4.2255392 -4.1667385][-4.1785049 -4.1623044 -4.1628351 -4.1562066 -4.1119943 -4.0095682 -3.8854556 -3.8649652 -3.9839876 -4.1257205 -4.2312675 -4.2823143 -4.2781467 -4.2311363 -4.1620054][-4.2178411 -4.2033243 -4.2039719 -4.200181 -4.1650796 -4.0892868 -4.0039377 -3.9997694 -4.0904341 -4.1947088 -4.2659564 -4.2994413 -4.2872753 -4.2349663 -4.16322][-4.2515006 -4.2332444 -4.2304668 -4.2267914 -4.2009664 -4.149334 -4.0947762 -4.1017623 -4.16728 -4.2393174 -4.2862215 -4.3102283 -4.2977948 -4.2504506 -4.1857128][-4.2776661 -4.259213 -4.2529125 -4.2477326 -4.2313519 -4.2026515 -4.1718235 -4.1812882 -4.2249331 -4.2715659 -4.3044524 -4.3240337 -4.3143058 -4.2761197 -4.2198396][-4.2850027 -4.2683945 -4.2605882 -4.2582626 -4.2534227 -4.2439747 -4.2323713 -4.2411814 -4.2706094 -4.3010726 -4.3255467 -4.3445692 -4.3388424 -4.3079991 -4.2570982][-4.2921238 -4.2821369 -4.2774334 -4.2774534 -4.2801638 -4.2805028 -4.2773252 -4.2851 -4.3055344 -4.325242 -4.3422093 -4.3577962 -4.3550806 -4.3291068 -4.2844028]]...]
INFO - root - 2017-12-08 00:47:14.775263: step 26010, loss = 2.07, batch loss = 2.01 (15.6 examples/sec; 2.057 sec/batch; 32h:38m:16s remains)
INFO - root - 2017-12-08 00:47:36.015903: step 26020, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.146 sec/batch; 34h:01m:59s remains)
INFO - root - 2017-12-08 00:47:57.018637: step 26030, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.088 sec/batch; 33h:06m:36s remains)
INFO - root - 2017-12-08 00:48:18.438428: step 26040, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.091 sec/batch; 33h:09m:32s remains)
INFO - root - 2017-12-08 00:48:39.913899: step 26050, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.197 sec/batch; 34h:49m:45s remains)
INFO - root - 2017-12-08 00:49:00.527206: step 26060, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.124 sec/batch; 33h:40m:04s remains)
INFO - root - 2017-12-08 00:49:21.883571: step 26070, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.100 sec/batch; 33h:17m:20s remains)
INFO - root - 2017-12-08 00:49:43.136628: step 26080, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.148 sec/batch; 34h:02m:24s remains)
INFO - root - 2017-12-08 00:50:03.998364: step 26090, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.091 sec/batch; 33h:07m:21s remains)
INFO - root - 2017-12-08 00:50:25.318085: step 26100, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 33h:45m:53s remains)
2017-12-08 00:50:26.894300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2949243 -4.3070951 -4.3117733 -4.3122621 -4.2984891 -4.2732272 -4.2537684 -4.2274981 -4.2217631 -4.2331471 -4.2413688 -4.2473836 -4.2522888 -4.2528234 -4.2573915][-4.2915854 -4.30015 -4.2990942 -4.2971697 -4.2842388 -4.2617321 -4.2375655 -4.2080622 -4.2205133 -4.2408485 -4.2429457 -4.2460265 -4.2549872 -4.2568908 -4.2541361][-4.2874908 -4.2873116 -4.2758241 -4.2672815 -4.2538161 -4.2296739 -4.19627 -4.1670337 -4.1994405 -4.2292762 -4.2241168 -4.2169619 -4.2261353 -4.2332449 -4.2323427][-4.2864718 -4.2763333 -4.2543254 -4.240653 -4.225544 -4.199667 -4.1583142 -4.1291933 -4.1753321 -4.2094107 -4.1981187 -4.1833653 -4.187665 -4.2016568 -4.2099891][-4.2927532 -4.2802758 -4.25576 -4.2381544 -4.2138681 -4.1798034 -4.1348691 -4.1076622 -4.15215 -4.1813774 -4.1695061 -4.1504488 -4.1508427 -4.1695914 -4.1873717][-4.3069358 -4.3003874 -4.2777605 -4.2516775 -4.2122226 -4.16081 -4.1099529 -4.0827217 -4.1169009 -4.1457825 -4.1359692 -4.1182866 -4.1187272 -4.138907 -4.1597676][-4.3175578 -4.3135438 -4.2905416 -4.2566819 -4.2050557 -4.1422825 -4.0885715 -4.0639567 -4.0949941 -4.1311255 -4.1222072 -4.1005716 -4.0977697 -4.1156144 -4.1325078][-4.3216181 -4.3190894 -4.2959328 -4.256083 -4.1960554 -4.1285696 -4.0746441 -4.0446858 -4.0781522 -4.1331086 -4.1337709 -4.113205 -4.1069126 -4.1126337 -4.1204505][-4.3174672 -4.3143625 -4.2880363 -4.2408466 -4.1760545 -4.1083164 -4.0484037 -4.0097957 -4.0489764 -4.1320262 -4.1546612 -4.1482086 -4.1456504 -4.1359386 -4.1285014][-4.309731 -4.3046017 -4.2757282 -4.2278304 -4.1694 -4.1080656 -4.0408692 -3.9870143 -4.0285645 -4.1359277 -4.1791029 -4.186285 -4.1892834 -4.1689792 -4.1464238][-4.3077126 -4.30503 -4.2839236 -4.252388 -4.2135954 -4.1641145 -4.0936327 -4.022666 -4.0479527 -4.1455927 -4.1913338 -4.2082286 -4.2128572 -4.1869087 -4.159235][-4.3106437 -4.3084869 -4.2958088 -4.2833686 -4.2652164 -4.2294674 -4.1631346 -4.0837522 -4.0891008 -4.1564574 -4.192533 -4.2179379 -4.2217212 -4.1976881 -4.1753044][-4.3101745 -4.3056936 -4.2972636 -4.2949419 -4.2863464 -4.2605195 -4.2001948 -4.1221523 -4.11569 -4.1563387 -4.1784258 -4.20471 -4.2088947 -4.1916718 -4.1775594][-4.3076706 -4.3010769 -4.293788 -4.2916517 -4.2840366 -4.2637358 -4.2108135 -4.1385818 -4.1257405 -4.1500955 -4.1617422 -4.182292 -4.1876283 -4.179708 -4.1804848][-4.3062434 -4.2966318 -4.2877889 -4.2839241 -4.2749443 -4.2583508 -4.2155972 -4.1494322 -4.1334572 -4.1523137 -4.1575665 -4.1750345 -4.1833153 -4.1848817 -4.196116]]...]
INFO - root - 2017-12-08 00:50:48.168989: step 26110, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.126 sec/batch; 33h:40m:34s remains)
INFO - root - 2017-12-08 00:51:08.917435: step 26120, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 33h:09m:13s remains)
INFO - root - 2017-12-08 00:51:30.264836: step 26130, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.112 sec/batch; 33h:26m:35s remains)
INFO - root - 2017-12-08 00:51:51.711814: step 26140, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.164 sec/batch; 34h:15m:19s remains)
INFO - root - 2017-12-08 00:52:12.744098: step 26150, loss = 2.08, batch loss = 2.02 (16.1 examples/sec; 1.989 sec/batch; 31h:28m:43s remains)
INFO - root - 2017-12-08 00:52:33.913448: step 26160, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.169 sec/batch; 34h:19m:06s remains)
INFO - root - 2017-12-08 00:52:55.517533: step 26170, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.122 sec/batch; 33h:34m:02s remains)
INFO - root - 2017-12-08 00:53:16.761097: step 26180, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.113 sec/batch; 33h:25m:35s remains)
INFO - root - 2017-12-08 00:53:37.821417: step 26190, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.148 sec/batch; 33h:57m:49s remains)
INFO - root - 2017-12-08 00:53:59.115763: step 26200, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.143 sec/batch; 33h:52m:51s remains)
2017-12-08 00:54:00.690174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2834315 -4.2904372 -4.289732 -4.2797608 -4.2637124 -4.2427258 -4.2183356 -4.1946125 -4.176743 -4.1716685 -4.1806021 -4.199481 -4.2233572 -4.2466826 -4.2657447][-4.2850275 -4.289712 -4.2835555 -4.2667718 -4.2435532 -4.2152667 -4.1839848 -4.1561832 -4.1427326 -4.1513324 -4.17699 -4.2093921 -4.2380934 -4.2580366 -4.2701507][-4.277185 -4.2814751 -4.2735629 -4.2534952 -4.226193 -4.1931677 -4.1573596 -4.1281757 -4.1223631 -4.1460209 -4.1876974 -4.2303839 -4.2608676 -4.276258 -4.2823706][-4.254755 -4.2555571 -4.2432318 -4.2173281 -4.1843634 -4.1467757 -4.10811 -4.0802274 -4.0815921 -4.1146545 -4.164701 -4.2133121 -4.2461948 -4.261394 -4.2681918][-4.2165103 -4.2074356 -4.1865106 -4.1523924 -4.1124091 -4.0698357 -4.0289497 -4.0035377 -4.0110612 -4.0485916 -4.104228 -4.1609826 -4.2036891 -4.2283645 -4.2426653][-4.1808562 -4.16718 -4.1450677 -4.1107717 -4.0694661 -4.0251422 -3.9831316 -3.9579527 -3.9658954 -4.001215 -4.0583916 -4.1220841 -4.1739521 -4.2060671 -4.2253084][-4.1747456 -4.1743441 -4.169868 -4.1543608 -4.1284657 -4.0939412 -4.0549536 -4.0266161 -4.0222325 -4.0380621 -4.0770092 -4.1281123 -4.17403 -4.2055039 -4.224072][-4.220264 -4.2311482 -4.2406135 -4.2408419 -4.2305179 -4.2105179 -4.1826544 -4.1581383 -4.1446824 -4.1396661 -4.150301 -4.1732049 -4.1981592 -4.2178884 -4.2282486][-4.2749205 -4.2879262 -4.3008938 -4.3075967 -4.3071594 -4.2997112 -4.2859235 -4.2708778 -4.2573261 -4.2438903 -4.2372079 -4.2388649 -4.2439227 -4.2481027 -4.2478113][-4.2956004 -4.3046956 -4.3139019 -4.3213611 -4.3271995 -4.3301125 -4.3294783 -4.32587 -4.3171644 -4.3033395 -4.2919588 -4.2857337 -4.2828774 -4.2813869 -4.278306][-4.2771654 -4.2806187 -4.2843442 -4.2916036 -4.3012366 -4.3106785 -4.3189225 -4.3243637 -4.3237686 -4.3162069 -4.3077126 -4.3028245 -4.3010588 -4.2996769 -4.2967057][-4.2496786 -4.2465649 -4.2441173 -4.2485261 -4.2581177 -4.269557 -4.28227 -4.2924333 -4.2974191 -4.2967672 -4.2945333 -4.2947903 -4.297576 -4.2995849 -4.2986875][-4.2368636 -4.2313194 -4.226059 -4.2268214 -4.2329631 -4.2421737 -4.2545161 -4.2649708 -4.2709675 -4.2742891 -4.2780151 -4.28334 -4.2891207 -4.2933111 -4.29513][-4.2423573 -4.235239 -4.2275395 -4.2250104 -4.2274461 -4.2331395 -4.2427106 -4.2502937 -4.2554941 -4.2616439 -4.2711935 -4.2823834 -4.2914042 -4.2968554 -4.2999134][-4.2674284 -4.2612333 -4.2536697 -4.2500443 -4.2508378 -4.2538791 -4.2591882 -4.2618046 -4.2623258 -4.2652659 -4.2739129 -4.286314 -4.2980189 -4.3055487 -4.3098574]]...]
INFO - root - 2017-12-08 00:54:21.898057: step 26210, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.098 sec/batch; 33h:09m:51s remains)
INFO - root - 2017-12-08 00:54:42.679550: step 26220, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.149 sec/batch; 33h:58m:08s remains)
INFO - root - 2017-12-08 00:55:03.799157: step 26230, loss = 2.09, batch loss = 2.04 (15.4 examples/sec; 2.083 sec/batch; 32h:54m:53s remains)
INFO - root - 2017-12-08 00:55:25.099648: step 26240, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.152 sec/batch; 34h:00m:15s remains)
INFO - root - 2017-12-08 00:55:46.002011: step 26250, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.069 sec/batch; 32h:41m:01s remains)
INFO - root - 2017-12-08 00:56:07.136874: step 26260, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 33h:33m:49s remains)
INFO - root - 2017-12-08 00:56:28.600330: step 26270, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 33h:22m:10s remains)
INFO - root - 2017-12-08 00:56:49.596794: step 26280, loss = 2.08, batch loss = 2.02 (16.6 examples/sec; 1.927 sec/batch; 30h:25m:26s remains)
INFO - root - 2017-12-08 00:57:10.922930: step 26290, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.140 sec/batch; 33h:47m:07s remains)
INFO - root - 2017-12-08 00:57:32.058545: step 26300, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 33h:22m:27s remains)
2017-12-08 00:57:33.654408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3223662 -4.3181124 -4.3126616 -4.3022127 -4.287643 -4.2687311 -4.2496638 -4.2367377 -4.2191129 -4.2018027 -4.1885195 -4.187 -4.1995649 -4.2252979 -4.2468381][-4.3282046 -4.3244805 -4.3213305 -4.3167186 -4.3097296 -4.2987518 -4.2848868 -4.2764196 -4.262445 -4.2459817 -4.2311406 -4.2272744 -4.2318773 -4.2478757 -4.2660213][-4.3366523 -4.3310342 -4.3251643 -4.3190122 -4.3136048 -4.3039989 -4.2912407 -4.2821665 -4.2693758 -4.255228 -4.2408686 -4.2361865 -4.2356172 -4.2457757 -4.2613297][-4.3401413 -4.3284917 -4.3146362 -4.3017459 -4.2909279 -4.2750812 -4.257452 -4.2461996 -4.2370572 -4.228755 -4.2198224 -4.2145219 -4.2125816 -4.2225423 -4.2344623][-4.33601 -4.3156395 -4.2909718 -4.2687068 -4.2492762 -4.2240067 -4.2005835 -4.1865468 -4.1792693 -4.1720185 -4.1652784 -4.1609073 -4.1619129 -4.1739783 -4.1837578][-4.3253107 -4.2974753 -4.2621474 -4.227715 -4.1965847 -4.1610718 -4.1287718 -4.1093063 -4.1025634 -4.0961351 -4.0937109 -4.0942421 -4.0993156 -4.1127086 -4.1243329][-4.3108492 -4.2749414 -4.2253995 -4.1725545 -4.12628 -4.0763745 -4.0316129 -4.0096726 -4.0143266 -4.0250487 -4.0409703 -4.0573134 -4.0725641 -4.091363 -4.107151][-4.2971015 -4.2509651 -4.1848516 -4.1123724 -4.0466075 -3.9805186 -3.9281356 -3.9169652 -3.9501209 -3.9930906 -4.0333266 -4.071003 -4.1012096 -4.12825 -4.1445808][-4.2920618 -4.24223 -4.1727319 -4.0979018 -4.0243182 -3.9504323 -3.9020081 -3.9082146 -3.9633877 -4.0244761 -4.0739608 -4.1208854 -4.1581078 -4.1821423 -4.1862016][-4.303019 -4.2586126 -4.2020607 -4.1448379 -4.085042 -4.0186849 -3.9788666 -3.9882436 -4.0355296 -4.0900259 -4.1324992 -4.1745338 -4.2048416 -4.2139015 -4.1980581][-4.3203292 -4.2857108 -4.244987 -4.2094922 -4.1746206 -4.1298704 -4.1014004 -4.1048474 -4.1320643 -4.1659088 -4.1918468 -4.2182508 -4.2337875 -4.2244153 -4.1885829][-4.3254805 -4.2976894 -4.2690024 -4.2490592 -4.2354093 -4.2159419 -4.2066174 -4.2146826 -4.22959 -4.2422028 -4.2471824 -4.2536364 -4.2506356 -4.2233157 -4.1736426][-4.3125796 -4.282176 -4.2547054 -4.2420149 -4.2439322 -4.2509909 -4.2674837 -4.2882943 -4.2995906 -4.2987342 -4.2889323 -4.2798419 -4.2629147 -4.2255206 -4.1714115][-4.2795811 -4.2402205 -4.2098341 -4.2010932 -4.213882 -4.2405233 -4.2761035 -4.3041687 -4.31173 -4.3055811 -4.2950273 -4.2866659 -4.2709923 -4.23845 -4.19334][-4.2383089 -4.1914139 -4.1591482 -4.1542277 -4.1741524 -4.2106419 -4.2519317 -4.2773552 -4.2792592 -4.2710652 -4.2652206 -4.2666712 -4.26576 -4.2540841 -4.2290525]]...]
INFO - root - 2017-12-08 00:57:54.639203: step 26310, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.109 sec/batch; 33h:16m:50s remains)
INFO - root - 2017-12-08 00:58:15.332155: step 26320, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.119 sec/batch; 33h:26m:15s remains)
INFO - root - 2017-12-08 00:58:36.406086: step 26330, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.084 sec/batch; 32h:52m:13s remains)
INFO - root - 2017-12-08 00:58:57.685628: step 26340, loss = 2.09, batch loss = 2.03 (15.1 examples/sec; 2.120 sec/batch; 33h:26m:01s remains)
INFO - root - 2017-12-08 00:59:18.707645: step 26350, loss = 2.08, batch loss = 2.03 (15.3 examples/sec; 2.094 sec/batch; 33h:01m:45s remains)
INFO - root - 2017-12-08 00:59:39.936852: step 26360, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.108 sec/batch; 33h:14m:35s remains)
INFO - root - 2017-12-08 01:00:01.201435: step 26370, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.138 sec/batch; 33h:42m:46s remains)
INFO - root - 2017-12-08 01:00:22.169354: step 26380, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.143 sec/batch; 33h:47m:03s remains)
INFO - root - 2017-12-08 01:00:43.435997: step 26390, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.154 sec/batch; 33h:57m:09s remains)
INFO - root - 2017-12-08 01:01:04.426214: step 26400, loss = 2.09, batch loss = 2.03 (15.6 examples/sec; 2.054 sec/batch; 32h:22m:05s remains)
2017-12-08 01:01:06.014363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3736992 -4.3710089 -4.3517938 -4.31455 -4.2495546 -4.1700087 -4.1121678 -4.1004243 -4.1144595 -4.1544933 -4.211699 -4.2558918 -4.2808247 -4.2829623 -4.27742][-4.3758926 -4.3755393 -4.3579307 -4.3195257 -4.2515764 -4.1639204 -4.0922689 -4.0753803 -4.0978222 -4.1450853 -4.2034822 -4.2474027 -4.2758522 -4.280477 -4.27613][-4.3771391 -4.377491 -4.3602557 -4.3192616 -4.250298 -4.1569867 -4.0712118 -4.04607 -4.0777779 -4.1349106 -4.19243 -4.233634 -4.265512 -4.2789431 -4.2780256][-4.3792758 -4.3789172 -4.3603172 -4.3148293 -4.2438388 -4.1444516 -4.0454826 -4.0082154 -4.0460434 -4.1098261 -4.1711369 -4.2155409 -4.2523837 -4.2750807 -4.2806683][-4.3843875 -4.3804684 -4.3547554 -4.2992043 -4.2151456 -4.1026716 -3.9837854 -3.9270918 -3.9720926 -4.0526738 -4.1242585 -4.1784668 -4.2267594 -4.2590375 -4.2736316][-4.3881736 -4.3809686 -4.3435979 -4.2700658 -4.1675024 -4.0434842 -3.9057729 -3.8173616 -3.8671389 -3.970921 -4.061698 -4.1358604 -4.2010164 -4.2452755 -4.2699938][-4.3881364 -4.3755207 -4.325367 -4.2310476 -4.1118913 -3.9742219 -3.8025622 -3.6746802 -3.7436347 -3.889322 -4.0134377 -4.1130056 -4.1931143 -4.2440929 -4.2698221][-4.382606 -4.3646255 -4.3061171 -4.2013683 -4.0764065 -3.9379728 -3.7660296 -3.6402161 -3.7218428 -3.8779416 -4.009634 -4.1171756 -4.1989231 -4.2477064 -4.266819][-4.3753085 -4.3534455 -4.2934122 -4.191153 -4.0810819 -3.973218 -3.8517032 -3.7759867 -3.8395197 -3.9570596 -4.0578918 -4.1443462 -4.2119026 -4.2516069 -4.2601156][-4.3709598 -4.3469877 -4.2880092 -4.1923032 -4.0995212 -4.0219126 -3.9428551 -3.9026866 -3.9564593 -4.0488915 -4.125145 -4.1888256 -4.236867 -4.2636576 -4.2606263][-4.368485 -4.3450541 -4.287456 -4.1991386 -4.1236954 -4.0725975 -4.0266213 -4.0099721 -4.0566115 -4.1271048 -4.1831512 -4.2273264 -4.2576594 -4.27127 -4.2605467][-4.3665571 -4.3441305 -4.2901525 -4.210741 -4.1491423 -4.1170759 -4.0959868 -4.0971546 -4.1389766 -4.1928778 -4.232687 -4.2610035 -4.2776875 -4.2812996 -4.2662439][-4.3674874 -4.3475337 -4.3009582 -4.2346144 -4.184597 -4.164865 -4.16376 -4.1805949 -4.2189326 -4.2578845 -4.2823071 -4.29744 -4.3062615 -4.3057594 -4.2912741][-4.3723073 -4.3567753 -4.3212395 -4.2716856 -4.2349453 -4.2256126 -4.2353749 -4.2573352 -4.2859497 -4.3101192 -4.3240137 -4.33122 -4.335598 -4.3331442 -4.321322][-4.3771091 -4.3666992 -4.3418469 -4.3081112 -4.2828074 -4.2797365 -4.2930045 -4.312263 -4.3307548 -4.3434687 -4.3505716 -4.3542342 -4.3553128 -4.3519731 -4.3433971]]...]
INFO - root - 2017-12-08 01:01:27.017543: step 26410, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.097 sec/batch; 33h:02m:32s remains)
INFO - root - 2017-12-08 01:01:48.384789: step 26420, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.136 sec/batch; 33h:38m:15s remains)
INFO - root - 2017-12-08 01:02:09.663343: step 26430, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 2.193 sec/batch; 34h:32m:27s remains)
INFO - root - 2017-12-08 01:02:30.937398: step 26440, loss = 2.07, batch loss = 2.02 (15.4 examples/sec; 2.073 sec/batch; 32h:38m:47s remains)
INFO - root - 2017-12-08 01:02:51.954271: step 26450, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.125 sec/batch; 33h:27m:05s remains)
INFO - root - 2017-12-08 01:03:13.131918: step 26460, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.142 sec/batch; 33h:43m:04s remains)
INFO - root - 2017-12-08 01:03:34.371169: step 26470, loss = 2.07, batch loss = 2.01 (14.3 examples/sec; 2.234 sec/batch; 35h:09m:16s remains)
INFO - root - 2017-12-08 01:03:55.140286: step 26480, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.110 sec/batch; 33h:11m:40s remains)
INFO - root - 2017-12-08 01:04:16.345640: step 26490, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.092 sec/batch; 32h:54m:25s remains)
INFO - root - 2017-12-08 01:04:37.798010: step 26500, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.081 sec/batch; 32h:43m:56s remains)
2017-12-08 01:04:39.346496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2979851 -4.2896123 -4.283884 -4.2825823 -4.28528 -4.2915587 -4.302609 -4.3152328 -4.3213992 -4.3172975 -4.3067017 -4.2960725 -4.2944155 -4.2980413 -4.3026409][-4.2543902 -4.2445364 -4.2447948 -4.252634 -4.261085 -4.2701235 -4.283627 -4.2975254 -4.3040071 -4.2995858 -4.2865262 -4.2745295 -4.2735295 -4.2785282 -4.2824321][-4.2197051 -4.2135563 -4.2227721 -4.2400208 -4.2531152 -4.2605157 -4.2675953 -4.2737536 -4.2764215 -4.2717371 -4.2589374 -4.2502823 -4.2493472 -4.2558169 -4.2616863][-4.1974335 -4.1967483 -4.2137904 -4.2381806 -4.2528911 -4.25136 -4.2398934 -4.2290635 -4.224442 -4.22 -4.21755 -4.2204685 -4.2246833 -4.235517 -4.2444029][-4.184411 -4.1886578 -4.2105265 -4.2332015 -4.2378907 -4.2148595 -4.1718483 -4.1354451 -4.1258783 -4.1338053 -4.153698 -4.1784062 -4.1969862 -4.2169905 -4.23285][-4.18 -4.1854563 -4.2024865 -4.2119184 -4.1954112 -4.139225 -4.0554867 -3.9893448 -3.985538 -4.0252819 -4.0863643 -4.1381621 -4.1712027 -4.2023368 -4.2287235][-4.1800051 -4.1804237 -4.1842046 -4.1725073 -4.1255741 -4.0276723 -3.9003787 -3.8089695 -3.831567 -3.9276066 -4.0302215 -4.1031818 -4.1483021 -4.1906457 -4.2243805][-4.1734118 -4.16168 -4.1456919 -4.1089911 -4.0331721 -3.9080765 -3.7570527 -3.6719728 -3.7461076 -3.8826931 -3.9960473 -4.0701909 -4.1157427 -4.1623859 -4.2006817][-4.1627707 -4.1357126 -4.1008339 -4.0460982 -3.9638186 -3.8531137 -3.7492666 -3.7232745 -3.8023498 -3.916775 -4.003859 -4.0588365 -4.0998335 -4.1428618 -4.1832213][-4.1609135 -4.125967 -4.0802174 -4.02259 -3.9583435 -3.901319 -3.8745224 -3.8810773 -3.9279447 -3.9955013 -4.0477667 -4.0815177 -4.1129737 -4.1461911 -4.1829209][-4.1720157 -4.1371851 -4.0914803 -4.0465503 -4.0172729 -4.0098882 -4.0167208 -4.0243559 -4.0409527 -4.0721664 -4.0997982 -4.1191936 -4.1409659 -4.1649823 -4.1951118][-4.1950788 -4.1659465 -4.1337447 -4.11397 -4.1138549 -4.1254826 -4.135253 -4.1323533 -4.1269889 -4.134594 -4.1456113 -4.1553082 -4.1714296 -4.1912737 -4.2148514][-4.225224 -4.2066693 -4.1934981 -4.1938815 -4.2057548 -4.2175822 -4.2198029 -4.2085271 -4.1933546 -4.1886778 -4.1903768 -4.1951962 -4.2066741 -4.2218547 -4.2403994][-4.2610574 -4.251718 -4.2504697 -4.2583866 -4.2681322 -4.2730703 -4.2707391 -4.2599716 -4.2453 -4.2372379 -4.2346272 -4.2357526 -4.2428942 -4.25392 -4.2695408][-4.2935424 -4.2898965 -4.2918591 -4.2978611 -4.3011637 -4.3002281 -4.2963233 -4.2889504 -4.2796254 -4.2738953 -4.2714462 -4.2718921 -4.2773962 -4.285852 -4.2977037]]...]
INFO - root - 2017-12-08 01:05:00.366720: step 26510, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.158 sec/batch; 33h:56m:16s remains)
INFO - root - 2017-12-08 01:05:21.539756: step 26520, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.074 sec/batch; 32h:36m:44s remains)
INFO - root - 2017-12-08 01:05:42.688350: step 26530, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.134 sec/batch; 33h:32m:46s remains)
INFO - root - 2017-12-08 01:06:03.442728: step 26540, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.130 sec/batch; 33h:28m:46s remains)
INFO - root - 2017-12-08 01:06:24.586564: step 26550, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.115 sec/batch; 33h:13m:57s remains)
INFO - root - 2017-12-08 01:06:45.635078: step 26560, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.098 sec/batch; 32h:57m:53s remains)
INFO - root - 2017-12-08 01:07:06.711965: step 26570, loss = 2.08, batch loss = 2.02 (15.7 examples/sec; 2.032 sec/batch; 31h:55m:10s remains)
INFO - root - 2017-12-08 01:07:27.812219: step 26580, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 33h:56m:00s remains)
INFO - root - 2017-12-08 01:07:49.168037: step 26590, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.130 sec/batch; 33h:27m:10s remains)
INFO - root - 2017-12-08 01:08:10.335625: step 26600, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.097 sec/batch; 32h:55m:45s remains)
2017-12-08 01:08:11.914465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2220364 -4.1979575 -4.1769257 -4.1579704 -4.1489596 -4.1443224 -4.1366205 -4.1388826 -4.1504526 -4.1669931 -4.1772375 -4.1771979 -4.162138 -4.1540375 -4.1523204][-4.2156515 -4.1848216 -4.1587133 -4.1376457 -4.1267495 -4.1218266 -4.1141715 -4.1165195 -4.1277075 -4.1456795 -4.162333 -4.1695867 -4.1607223 -4.1551089 -4.1587329][-4.2292552 -4.1987467 -4.1710706 -4.1496339 -4.1375461 -4.1334295 -4.129591 -4.1321535 -4.142983 -4.1609535 -4.179287 -4.1868048 -4.1818948 -4.178081 -4.1808915][-4.2462125 -4.2198133 -4.1933064 -4.1732659 -4.1623821 -4.1615129 -4.1657591 -4.1744318 -4.18437 -4.1990843 -4.2118249 -4.2152948 -4.2090473 -4.2042756 -4.2045059][-4.2397633 -4.22148 -4.2000737 -4.1823707 -4.1730723 -4.1773686 -4.1907616 -4.2067261 -4.216207 -4.2238426 -4.2272563 -4.2265115 -4.2205296 -4.21454 -4.2169886][-4.2110343 -4.1928391 -4.170836 -4.1505113 -4.1378436 -4.1402988 -4.158112 -4.1790137 -4.1887822 -4.1928673 -4.1924019 -4.1923814 -4.1884251 -4.1844621 -4.19075][-4.1309114 -4.1127887 -4.0898824 -4.0641208 -4.0450149 -4.0413346 -4.0616579 -4.0905609 -4.1091704 -4.11849 -4.1230974 -4.1300673 -4.1352105 -4.1387196 -4.1510358][-4.0461826 -4.0179172 -3.9796844 -3.9336433 -3.895227 -3.8779864 -3.8951669 -3.9321573 -3.962842 -3.9832573 -4.0004416 -4.0182028 -4.0330429 -4.0489206 -4.0758276][-4.0292516 -3.9930787 -3.9448698 -3.8832774 -3.8272204 -3.7900436 -3.7917414 -3.8229337 -3.8495944 -3.8644445 -3.880944 -3.9012902 -3.9173341 -3.9388986 -3.9785922][-4.0636611 -4.041729 -4.0074153 -3.9587088 -3.9077725 -3.8663583 -3.8567941 -3.8728518 -3.8839266 -3.8838809 -3.8862257 -3.8936346 -3.8990006 -3.9128096 -3.9456797][-4.0973535 -4.099874 -4.0916533 -4.0681806 -4.036984 -4.00502 -3.9881558 -3.9891508 -3.9877973 -3.978858 -3.9740887 -3.9740751 -3.9713955 -3.9764059 -3.9964974][-4.1572618 -4.173799 -4.183239 -4.1791296 -4.1643443 -4.1435761 -4.1280761 -4.1232915 -4.1174645 -4.1079731 -4.1024942 -4.0985746 -4.0882769 -4.0810862 -4.0857453][-4.2377172 -4.2493062 -4.2574544 -4.2578754 -4.2505894 -4.2390184 -4.2302618 -4.2271805 -4.2242708 -4.2181187 -4.2135754 -4.2086906 -4.1939216 -4.1749992 -4.1633539][-4.2829652 -4.2906775 -4.2955561 -4.2946963 -4.2896409 -4.2833796 -4.2786684 -4.2786827 -4.2805195 -4.2797771 -4.2772241 -4.2724276 -4.2584691 -4.2368832 -4.2169266][-4.2886996 -4.2940569 -4.2971959 -4.2970219 -4.2950621 -4.2928767 -4.2914777 -4.2931066 -4.2971077 -4.2995977 -4.299262 -4.2957044 -4.2853227 -4.2685385 -4.25041]]...]
INFO - root - 2017-12-08 01:08:32.867996: step 26610, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.124 sec/batch; 33h:20m:42s remains)
INFO - root - 2017-12-08 01:08:54.107996: step 26620, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.154 sec/batch; 33h:48m:17s remains)
INFO - root - 2017-12-08 01:09:15.320019: step 26630, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 32h:56m:03s remains)
INFO - root - 2017-12-08 01:09:36.276779: step 26640, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.099 sec/batch; 32h:56m:02s remains)
INFO - root - 2017-12-08 01:09:57.597719: step 26650, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.139 sec/batch; 33h:33m:39s remains)
INFO - root - 2017-12-08 01:10:19.059740: step 26660, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.127 sec/batch; 33h:21m:41s remains)
INFO - root - 2017-12-08 01:10:39.778305: step 26670, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.087 sec/batch; 32h:44m:02s remains)
INFO - root - 2017-12-08 01:11:01.173136: step 26680, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.099 sec/batch; 32h:54m:54s remains)
INFO - root - 2017-12-08 01:11:22.309483: step 26690, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 33h:12m:56s remains)
INFO - root - 2017-12-08 01:11:43.261727: step 26700, loss = 2.09, batch loss = 2.03 (16.2 examples/sec; 1.972 sec/batch; 30h:54m:39s remains)
2017-12-08 01:11:44.806633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2206116 -4.1777511 -4.1420622 -4.1242385 -4.1187758 -4.1192322 -4.1078496 -4.10396 -4.1214895 -4.1393557 -4.1300225 -4.12109 -4.1508989 -4.1835294 -4.2064734][-4.220448 -4.173058 -4.1311293 -4.108346 -4.0957317 -4.0931764 -4.0815115 -4.0849485 -4.1249771 -4.1591859 -4.1542344 -4.1329789 -4.148324 -4.181179 -4.2108469][-4.2092094 -4.1698232 -4.1318808 -4.1051459 -4.0827394 -4.0746922 -4.0615191 -4.0680485 -4.1182456 -4.159483 -4.1577187 -4.1336622 -4.143527 -4.1764774 -4.21284][-4.1967192 -4.1736884 -4.1457114 -4.1166959 -4.0885234 -4.0763221 -4.0628505 -4.0626154 -4.1078482 -4.1445189 -4.14855 -4.1399188 -4.1540251 -4.182991 -4.2133021][-4.19348 -4.1897893 -4.1727037 -4.1426058 -4.1045885 -4.0753293 -4.0523739 -4.0410624 -4.0817757 -4.1236448 -4.1420212 -4.1572571 -4.1767616 -4.1958475 -4.2157226][-4.198535 -4.2045813 -4.1947985 -4.1603789 -4.1049857 -4.0463343 -3.9944565 -3.9685669 -4.0307465 -4.1072817 -4.1545343 -4.1865368 -4.2041788 -4.2116542 -4.2192745][-4.2015276 -4.210978 -4.2061572 -4.1675129 -4.0901361 -3.994715 -3.9053235 -3.8681405 -3.9755483 -4.0989928 -4.1672578 -4.1950245 -4.2057896 -4.2099695 -4.2157331][-4.2018132 -4.2179389 -4.2182007 -4.1771426 -4.0900364 -3.9855382 -3.8968775 -3.8724174 -3.9907155 -4.1174545 -4.1714888 -4.1795788 -4.180161 -4.1887116 -4.2023273][-4.2041407 -4.2250147 -4.2266307 -4.1921535 -4.122354 -4.0477128 -3.9967601 -3.9847069 -4.0606017 -4.1458988 -4.1699791 -4.1611609 -4.1591887 -4.1747823 -4.1990666][-4.2104826 -4.2323875 -4.2380657 -4.21638 -4.165473 -4.1135092 -4.0798855 -4.0617776 -4.0946817 -4.1429696 -4.1552472 -4.1525035 -4.1593881 -4.181807 -4.2120457][-4.22571 -4.2443891 -4.2500815 -4.2369404 -4.1928911 -4.1435938 -4.1049328 -4.0767555 -4.0853772 -4.1204195 -4.1429524 -4.159102 -4.1784124 -4.2033381 -4.2332187][-4.2341771 -4.2531028 -4.2610292 -4.2485547 -4.200922 -4.1460605 -4.1003237 -4.0692444 -4.073184 -4.1129503 -4.1502323 -4.1792254 -4.2008886 -4.222816 -4.2509427][-4.2404747 -4.2643542 -4.2770185 -4.2548428 -4.1962 -4.1382642 -4.1000695 -4.081821 -4.0899334 -4.1354589 -4.174994 -4.2017932 -4.2170258 -4.2349205 -4.2624817][-4.2509995 -4.2768345 -4.2854829 -4.2525563 -4.188684 -4.1351914 -4.1097088 -4.1030407 -4.1126289 -4.1574802 -4.1949329 -4.2112694 -4.2232618 -4.2453256 -4.2727308][-4.2724447 -4.2919874 -4.289598 -4.2514672 -4.1939325 -4.1481009 -4.1276731 -4.123126 -4.1286421 -4.1656351 -4.1989288 -4.2082958 -4.2180181 -4.2440138 -4.2731833]]...]
INFO - root - 2017-12-08 01:12:06.157148: step 26710, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 33h:07m:42s remains)
INFO - root - 2017-12-08 01:12:27.113779: step 26720, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.129 sec/batch; 33h:21m:01s remains)
INFO - root - 2017-12-08 01:12:48.294810: step 26730, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.151 sec/batch; 33h:41m:19s remains)
INFO - root - 2017-12-08 01:13:09.371175: step 26740, loss = 2.07, batch loss = 2.02 (15.1 examples/sec; 2.114 sec/batch; 33h:06m:54s remains)
INFO - root - 2017-12-08 01:13:30.735372: step 26750, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 33h:24m:00s remains)
INFO - root - 2017-12-08 01:13:51.924467: step 26760, loss = 2.08, batch loss = 2.02 (14.6 examples/sec; 2.193 sec/batch; 34h:19m:55s remains)
INFO - root - 2017-12-08 01:14:12.752521: step 26770, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.095 sec/batch; 32h:47m:27s remains)
INFO - root - 2017-12-08 01:14:33.895322: step 26780, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.108 sec/batch; 32h:59m:20s remains)
INFO - root - 2017-12-08 01:14:55.186493: step 26790, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.157 sec/batch; 33h:45m:07s remains)
INFO - root - 2017-12-08 01:15:16.041519: step 26800, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 33h:04m:44s remains)
2017-12-08 01:15:17.657571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3584428 -4.3545723 -4.3422556 -4.33214 -4.3293023 -4.3327055 -4.3421445 -4.3472438 -4.3462133 -4.3402104 -4.3329797 -4.3277884 -4.3221054 -4.31944 -4.32312][-4.3634868 -4.3582878 -4.3447618 -4.3322759 -4.3274779 -4.3314667 -4.3426218 -4.3510141 -4.3541093 -4.3513494 -4.3450766 -4.3382974 -4.329987 -4.321178 -4.3178792][-4.3666143 -4.3586535 -4.3402634 -4.3204322 -4.3070836 -4.3052168 -4.3138008 -4.3281412 -4.3412366 -4.3470273 -4.3440056 -4.3350687 -4.3237824 -4.309535 -4.302495][-4.3624969 -4.3533411 -4.3286448 -4.3002796 -4.2696924 -4.2473216 -4.2434778 -4.2623186 -4.2886004 -4.3051953 -4.3101225 -4.3014917 -4.2895212 -4.2762275 -4.271337][-4.3475494 -4.3301077 -4.2926693 -4.2446513 -4.1866474 -4.1347151 -4.11598 -4.142345 -4.1895432 -4.2215548 -4.2366772 -4.2371931 -4.2355204 -4.2320619 -4.2347584][-4.3221393 -4.2935843 -4.2376118 -4.1563478 -4.0552435 -3.9591081 -3.9225097 -3.9718223 -4.0559163 -4.1139665 -4.1453614 -4.1628871 -4.1807928 -4.188899 -4.1968651][-4.2877045 -4.24353 -4.1664867 -4.0494709 -3.8992445 -3.7520065 -3.7026348 -3.7895207 -3.9205027 -4.0101624 -4.0607414 -4.0973272 -4.1361847 -4.159616 -4.1732774][-4.2530003 -4.1972494 -4.1121049 -3.9922278 -3.8438339 -3.7002313 -3.66167 -3.7572618 -3.8878732 -3.9780881 -4.0320172 -4.0770311 -4.1262984 -4.16046 -4.1829634][-4.2492085 -4.19516 -4.1294532 -4.047533 -3.9493442 -3.856406 -3.8424242 -3.9107344 -3.9947028 -4.0507278 -4.0882597 -4.1281223 -4.1714554 -4.2019854 -4.2230759][-4.2669673 -4.2259684 -4.1814432 -4.13673 -4.0838175 -4.0362387 -4.0343189 -4.0721397 -4.11296 -4.1399713 -4.1618447 -4.1910977 -4.22325 -4.246911 -4.2637072][-4.2894197 -4.2639656 -4.2371249 -4.2155423 -4.1895514 -4.16693 -4.1727576 -4.1904755 -4.2040157 -4.2121439 -4.2227612 -4.2447639 -4.2710667 -4.2928834 -4.3021145][-4.3049469 -4.2874374 -4.2749748 -4.2679687 -4.2580447 -4.247335 -4.2565389 -4.2647319 -4.2652664 -4.2629867 -4.2648573 -4.2793646 -4.3010588 -4.3204365 -4.3250751][-4.3189268 -4.3041792 -4.2990584 -4.2961831 -4.2930989 -4.2903852 -4.2992907 -4.3065743 -4.3060379 -4.3003111 -4.295053 -4.299582 -4.3141513 -4.32804 -4.3334627][-4.3397527 -4.3314023 -4.3303924 -4.3297133 -4.3275871 -4.3265481 -4.3305283 -4.3343487 -4.3322811 -4.3241234 -4.3153353 -4.3133693 -4.3209476 -4.3316464 -4.3387275][-4.3489108 -4.3489914 -4.3506722 -4.3508325 -4.3491898 -4.3475943 -4.347682 -4.3471713 -4.3436327 -4.3371782 -4.3303223 -4.32706 -4.3307414 -4.3379912 -4.3445673]]...]
INFO - root - 2017-12-08 01:15:38.892678: step 26810, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.172 sec/batch; 33h:59m:02s remains)
INFO - root - 2017-12-08 01:16:00.200981: step 26820, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.097 sec/batch; 32h:48m:13s remains)
INFO - root - 2017-12-08 01:16:21.050060: step 26830, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.140 sec/batch; 33h:27m:28s remains)
INFO - root - 2017-12-08 01:16:42.342294: step 26840, loss = 2.07, batch loss = 2.02 (14.7 examples/sec; 2.181 sec/batch; 34h:05m:30s remains)
INFO - root - 2017-12-08 01:17:03.567827: step 26850, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.108 sec/batch; 32h:56m:55s remains)
INFO - root - 2017-12-08 01:17:24.499578: step 26860, loss = 2.08, batch loss = 2.02 (17.0 examples/sec; 1.886 sec/batch; 29h:28m:43s remains)
INFO - root - 2017-12-08 01:17:45.566183: step 26870, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.101 sec/batch; 32h:49m:45s remains)
INFO - root - 2017-12-08 01:18:06.768822: step 26880, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.133 sec/batch; 33h:19m:15s remains)
INFO - root - 2017-12-08 01:18:28.196406: step 26890, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.126 sec/batch; 33h:12m:08s remains)
INFO - root - 2017-12-08 01:18:49.108100: step 26900, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.111 sec/batch; 32h:58m:24s remains)
2017-12-08 01:18:50.691357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1849008 -4.2097797 -4.2110395 -4.1817803 -4.1376996 -4.1034102 -4.0822721 -4.0879893 -4.1110234 -4.1433864 -4.1614041 -4.163403 -4.157721 -4.1453609 -4.1421309][-4.21645 -4.2335386 -4.2342377 -4.2120132 -4.1775026 -4.1461377 -4.121531 -4.1165285 -4.1324582 -4.1577897 -4.1681423 -4.1691318 -4.1677527 -4.1580486 -4.1542315][-4.2646441 -4.2648268 -4.2534914 -4.2284226 -4.1963263 -4.1645188 -4.1386132 -4.1333227 -4.1534915 -4.1803174 -4.1890159 -4.1875358 -4.1858678 -4.1756568 -4.1669145][-4.3007989 -4.2832389 -4.2596092 -4.230124 -4.1950812 -4.1565638 -4.1299629 -4.1360111 -4.1716042 -4.2083316 -4.2217207 -4.2222009 -4.216928 -4.2044973 -4.19046][-4.3094807 -4.2760353 -4.2414083 -4.2068777 -4.1673136 -4.1212716 -4.0973816 -4.1176233 -4.169569 -4.2188325 -4.2410064 -4.2449079 -4.23525 -4.2197013 -4.202744][-4.2943377 -4.2511144 -4.2086191 -4.1652613 -4.1141005 -4.0602226 -4.0409336 -4.07557 -4.1449084 -4.2121038 -4.2488279 -4.2558646 -4.2392573 -4.2136126 -4.1875095][-4.2693157 -4.2190065 -4.1694412 -4.1151915 -4.0532045 -3.9931002 -3.9772015 -4.0221033 -4.1069779 -4.1942453 -4.2467613 -4.2592549 -4.2393084 -4.2041354 -4.162869][-4.2565932 -4.209249 -4.1592426 -4.0974216 -4.0292463 -3.9696569 -3.957372 -4.0008936 -4.0839024 -4.1750236 -4.2391782 -4.258944 -4.2384982 -4.1936235 -4.1420746][-4.2618876 -4.2300129 -4.1870508 -4.1231003 -4.0562491 -4.0051527 -3.9983954 -4.0345058 -4.0985885 -4.17316 -4.2342367 -4.2585607 -4.2429829 -4.1953487 -4.1372333][-4.2704911 -4.2638054 -4.2354212 -4.1777053 -4.1147494 -4.070241 -4.066246 -4.0969906 -4.1447682 -4.1973557 -4.2431374 -4.2651539 -4.2543635 -4.208838 -4.1460304][-4.2681723 -4.2849751 -4.27454 -4.2348485 -4.182487 -4.1407118 -4.133718 -4.1576271 -4.1929307 -4.2289119 -4.2578287 -4.2723141 -4.2604518 -4.2156324 -4.152791][-4.2496476 -4.2842937 -4.2928281 -4.2796364 -4.2497134 -4.2133856 -4.1967983 -4.2052822 -4.2278442 -4.2527509 -4.2753038 -4.2873859 -4.2746196 -4.2337842 -4.1742778][-4.2242308 -4.2727218 -4.301012 -4.3101077 -4.2980471 -4.2694354 -4.2462163 -4.2415829 -4.2521977 -4.2695246 -4.2903781 -4.3034573 -4.2938533 -4.2597537 -4.20792][-4.1911836 -4.2558451 -4.303648 -4.3295064 -4.3276348 -4.3052359 -4.2820277 -4.2721758 -4.2754569 -4.2892814 -4.3083639 -4.3203197 -4.311316 -4.2821951 -4.2392063][-4.1443505 -4.2259617 -4.2946944 -4.3359494 -4.3409247 -4.3225636 -4.3012948 -4.287272 -4.2851958 -4.30007 -4.3227081 -4.3360143 -4.3277588 -4.3037372 -4.2697234]]...]
INFO - root - 2017-12-08 01:19:11.941422: step 26910, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.096 sec/batch; 32h:43m:47s remains)
INFO - root - 2017-12-08 01:19:33.437122: step 26920, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 2.149 sec/batch; 33h:32m:54s remains)
INFO - root - 2017-12-08 01:19:54.230528: step 26930, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.115 sec/batch; 33h:00m:27s remains)
INFO - root - 2017-12-08 01:20:15.508012: step 26940, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 2.139 sec/batch; 33h:23m:21s remains)
INFO - root - 2017-12-08 01:20:36.758409: step 26950, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.133 sec/batch; 33h:16m:39s remains)
INFO - root - 2017-12-08 01:20:57.762472: step 26960, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.110 sec/batch; 32h:54m:55s remains)
INFO - root - 2017-12-08 01:21:19.102143: step 26970, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.182 sec/batch; 34h:01m:48s remains)
INFO - root - 2017-12-08 01:21:40.296249: step 26980, loss = 2.06, batch loss = 2.00 (15.0 examples/sec; 2.130 sec/batch; 33h:13m:25s remains)
INFO - root - 2017-12-08 01:22:01.124160: step 26990, loss = 2.07, batch loss = 2.01 (16.3 examples/sec; 1.962 sec/batch; 30h:35m:42s remains)
INFO - root - 2017-12-08 01:22:22.447749: step 27000, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 2.171 sec/batch; 33h:51m:09s remains)
2017-12-08 01:22:24.053289: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2631311 -4.2638373 -4.2651763 -4.2675066 -4.2753329 -4.28405 -4.29005 -4.2918611 -4.2890511 -4.2765193 -4.2523804 -4.2232966 -4.1965752 -4.18482 -4.1988554][-4.2239885 -4.22977 -4.2364349 -4.2416081 -4.2486687 -4.2522569 -4.2511907 -4.2465334 -4.2394104 -4.225698 -4.2050128 -4.1860762 -4.1759334 -4.1810541 -4.2050738][-4.177784 -4.1917973 -4.2071271 -4.2154417 -4.2172704 -4.2103682 -4.1979613 -4.18586 -4.1800208 -4.1744881 -4.1654892 -4.1608906 -4.1669397 -4.1861291 -4.2147322][-4.1465583 -4.1659946 -4.1868038 -4.1955829 -4.1896505 -4.1676807 -4.1370559 -4.1125364 -4.1100535 -4.1228423 -4.1342335 -4.1475391 -4.1671228 -4.1952929 -4.2236695][-4.148797 -4.1614747 -4.1763291 -4.1806388 -4.1661458 -4.12713 -4.073184 -4.03025 -4.0307188 -4.0677958 -4.1062164 -4.1396961 -4.1715288 -4.2051525 -4.2292891][-4.1673574 -4.166379 -4.1683865 -4.1654081 -4.1420317 -4.0887804 -4.0140362 -3.9523027 -3.9537396 -4.0195818 -4.0896006 -4.1410103 -4.1769567 -4.209403 -4.229013][-4.181726 -4.1718025 -4.165132 -4.1565213 -4.1263227 -4.064095 -3.9778719 -3.9094164 -3.9166942 -4.0019946 -4.0926695 -4.1525145 -4.1849928 -4.2084575 -4.2216187][-4.1977334 -4.184339 -4.1748056 -4.1638112 -4.133399 -4.075098 -3.9978006 -3.9431763 -3.9599681 -4.0427294 -4.1282763 -4.1806121 -4.2015195 -4.2096419 -4.2137742][-4.2112145 -4.1979785 -4.1864181 -4.1740408 -4.1494594 -4.1061006 -4.0511751 -4.0181227 -4.0402451 -4.1050386 -4.1707716 -4.2087512 -4.2179637 -4.2151561 -4.2130618][-4.2291789 -4.2188892 -4.2076983 -4.1948366 -4.1751642 -4.14411 -4.1090617 -4.092937 -4.1118712 -4.1558514 -4.2034063 -4.2288489 -4.2299943 -4.2220087 -4.2179904][-4.2567649 -4.2484121 -4.2391405 -4.227036 -4.2091165 -4.1826358 -4.1622939 -4.15969 -4.1731682 -4.1995792 -4.230351 -4.2435555 -4.2392993 -4.2292795 -4.2254653][-4.2736411 -4.2666764 -4.2600589 -4.2497711 -4.2348485 -4.2159247 -4.20963 -4.2155952 -4.2259045 -4.2413716 -4.2572513 -4.2592955 -4.2508254 -4.2418008 -4.2378912][-4.2662573 -4.2610536 -4.2546368 -4.2446012 -4.2331667 -4.2266026 -4.2345543 -4.248445 -4.2603421 -4.2727857 -4.2799234 -4.2754507 -4.2635579 -4.2526045 -4.2473965][-4.245059 -4.2425056 -4.2372861 -4.226872 -4.2189264 -4.224649 -4.2413387 -4.2605214 -4.2775655 -4.2909384 -4.2949247 -4.2899852 -4.2781329 -4.2657576 -4.26007][-4.225215 -4.2229977 -4.2187567 -4.2150979 -4.2162237 -4.2313213 -4.2528267 -4.2754669 -4.2964573 -4.3091083 -4.3095369 -4.3049073 -4.2947626 -4.2839966 -4.279511]]...]
INFO - root - 2017-12-08 01:22:45.397906: step 27010, loss = 2.07, batch loss = 2.01 (15.7 examples/sec; 2.033 sec/batch; 31h:41m:42s remains)
INFO - root - 2017-12-08 01:23:06.551650: step 27020, loss = 2.06, batch loss = 2.01 (15.2 examples/sec; 2.110 sec/batch; 32h:52m:53s remains)
INFO - root - 2017-12-08 01:23:27.585702: step 27030, loss = 2.09, batch loss = 2.03 (14.7 examples/sec; 2.179 sec/batch; 33h:57m:05s remains)
INFO - root - 2017-12-08 01:23:48.629510: step 27040, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.126 sec/batch; 33h:07m:31s remains)
INFO - root - 2017-12-08 01:24:09.813398: step 27050, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.119 sec/batch; 33h:00m:25s remains)
INFO - root - 2017-12-08 01:24:30.641819: step 27060, loss = 2.07, batch loss = 2.02 (14.8 examples/sec; 2.162 sec/batch; 33h:40m:38s remains)
INFO - root - 2017-12-08 01:24:51.866701: step 27070, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.147 sec/batch; 33h:26m:12s remains)
INFO - root - 2017-12-08 01:25:13.088413: step 27080, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.163 sec/batch; 33h:40m:09s remains)
INFO - root - 2017-12-08 01:25:33.986332: step 27090, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.142 sec/batch; 33h:20m:14s remains)
INFO - root - 2017-12-08 01:25:55.269463: step 27100, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.110 sec/batch; 32h:50m:35s remains)
2017-12-08 01:25:56.914382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.333107 -4.3257766 -4.297153 -4.2550859 -4.2200279 -4.1853375 -4.1601868 -4.1762419 -4.2140908 -4.2381124 -4.2590795 -4.284039 -4.2959213 -4.2894349 -4.2691283][-4.3341818 -4.3260393 -4.2967091 -4.2493291 -4.2063913 -4.161633 -4.1308489 -4.151051 -4.1878729 -4.2068543 -4.2220273 -4.2489262 -4.2637286 -4.2602558 -4.2414069][-4.3370924 -4.3279142 -4.297483 -4.2446008 -4.1885953 -4.13053 -4.0930557 -4.1246424 -4.1675572 -4.1879492 -4.2009487 -4.2225928 -4.234623 -4.230937 -4.2128348][-4.3404546 -4.3286791 -4.2955956 -4.2379932 -4.1718717 -4.0988607 -4.0568504 -4.1084757 -4.1685643 -4.1975546 -4.2145319 -4.230248 -4.2326832 -4.2254577 -4.2094426][-4.3447442 -4.3305011 -4.2907815 -4.2255793 -4.1502476 -4.0639577 -4.0170751 -4.091764 -4.1753139 -4.2196913 -4.2483606 -4.2599711 -4.2517781 -4.2420306 -4.2311211][-4.346478 -4.3294625 -4.2824125 -4.2084594 -4.1273293 -4.0259686 -3.9595487 -4.0412316 -4.1521044 -4.2162313 -4.261179 -4.2722263 -4.2571816 -4.2456622 -4.2408957][-4.3455582 -4.3264732 -4.2741451 -4.19435 -4.105092 -3.9825583 -3.8739595 -3.9414773 -4.087821 -4.181612 -4.2472863 -4.264863 -4.2511244 -4.2403269 -4.2388611][-4.3428378 -4.3238659 -4.2693243 -4.1854367 -4.0880404 -3.9528823 -3.8164196 -3.869287 -4.0418377 -4.1605577 -4.2415552 -4.2685852 -4.2585282 -4.245523 -4.2406316][-4.3402357 -4.3259511 -4.2760968 -4.1955032 -4.1064186 -3.9926283 -3.8828282 -3.9311166 -4.0847645 -4.1922421 -4.2689414 -4.2973075 -4.2841749 -4.2616215 -4.243361][-4.3350182 -4.3241158 -4.281878 -4.2149768 -4.1492605 -4.079092 -4.0250249 -4.0729375 -4.1831369 -4.2560539 -4.3101797 -4.325458 -4.3001862 -4.2677193 -4.2382846][-4.3291721 -4.3186984 -4.2797618 -4.2240748 -4.1773386 -4.1394119 -4.1259313 -4.1755605 -4.2552214 -4.3027339 -4.3364844 -4.3371019 -4.3041582 -4.2693071 -4.2419176][-4.322288 -4.3090949 -4.2713971 -4.2203617 -4.1795297 -4.1566634 -4.1605787 -4.2143316 -4.2818203 -4.3182425 -4.3364611 -4.3267417 -4.2953825 -4.268096 -4.2498865][-4.3177209 -4.3010788 -4.26432 -4.2172904 -4.179285 -4.1620426 -4.1722069 -4.227643 -4.2873478 -4.314446 -4.3209319 -4.3047061 -4.2796221 -4.2590261 -4.2467475][-4.3175 -4.3003674 -4.2667441 -4.2230883 -4.1880717 -4.1716528 -4.1823659 -4.2324963 -4.2795238 -4.2974496 -4.2987185 -4.2852707 -4.2674947 -4.2506018 -4.2405615][-4.3203158 -4.3067713 -4.2800374 -4.2434855 -4.2123742 -4.1936293 -4.1991839 -4.2396145 -4.273119 -4.279767 -4.2768388 -4.2653022 -4.252985 -4.2396846 -4.2336421]]...]
INFO - root - 2017-12-08 01:26:18.288641: step 27110, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.135 sec/batch; 33h:12m:53s remains)
INFO - root - 2017-12-08 01:26:39.089229: step 27120, loss = 2.07, batch loss = 2.02 (15.2 examples/sec; 2.103 sec/batch; 32h:43m:24s remains)
INFO - root - 2017-12-08 01:27:00.341224: step 27130, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.142 sec/batch; 33h:19m:12s remains)
INFO - root - 2017-12-08 01:27:21.582785: step 27140, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.089 sec/batch; 32h:29m:15s remains)
INFO - root - 2017-12-08 01:27:42.720341: step 27150, loss = 2.07, batch loss = 2.02 (15.7 examples/sec; 2.040 sec/batch; 31h:43m:06s remains)
INFO - root - 2017-12-08 01:28:03.820043: step 27160, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 2.174 sec/batch; 33h:47m:35s remains)
INFO - root - 2017-12-08 01:28:24.971732: step 27170, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 2.098 sec/batch; 32h:36m:26s remains)
INFO - root - 2017-12-08 01:28:46.171901: step 27180, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.159 sec/batch; 33h:33m:18s remains)
INFO - root - 2017-12-08 01:29:06.925182: step 27190, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.085 sec/batch; 32h:23m:24s remains)
INFO - root - 2017-12-08 01:29:28.091749: step 27200, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.156 sec/batch; 33h:29m:32s remains)
2017-12-08 01:29:29.658199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25071 -4.2518983 -4.250803 -4.2485247 -4.2443771 -4.2463489 -4.2537127 -4.261961 -4.2694793 -4.2709236 -4.2691264 -4.26586 -4.2631454 -4.2609963 -4.2592206][-4.246788 -4.2410622 -4.2334933 -4.2273107 -4.2205033 -4.2206721 -4.2297025 -4.24107 -4.2517576 -4.253757 -4.2509847 -4.2460246 -4.2424097 -4.2388663 -4.2346][-4.2186084 -4.2091994 -4.2014914 -4.1963363 -4.1906209 -4.1896772 -4.198555 -4.2110233 -4.2235026 -4.2263231 -4.22308 -4.216392 -4.210433 -4.2044916 -4.198009][-4.1821876 -4.1762543 -4.1739 -4.1717472 -4.166687 -4.1610394 -4.164794 -4.1749597 -4.1874647 -4.1916242 -4.1882172 -4.1789651 -4.1673846 -4.1544433 -4.1435232][-4.1641817 -4.1692572 -4.173481 -4.1700015 -4.1583004 -4.1440063 -4.13885 -4.1456189 -4.1568789 -4.1581283 -4.150238 -4.1366634 -4.1205263 -4.1037893 -4.0904641][-4.1618047 -4.1734858 -4.1787891 -4.1689496 -4.1474991 -4.1250181 -4.1134706 -4.1169147 -4.1243153 -4.1194792 -4.1048627 -4.08825 -4.0749273 -4.06293 -4.0547714][-4.1580448 -4.1669145 -4.1681037 -4.1530452 -4.1271377 -4.1016674 -4.0884647 -4.0895605 -4.0944357 -4.08586 -4.0698814 -4.0554104 -4.0497556 -4.0463352 -4.0435176][-4.1583662 -4.1600475 -4.1566668 -4.1414495 -4.1193056 -4.0989051 -4.0908794 -4.0947514 -4.0999184 -4.0956583 -4.0850167 -4.0728636 -4.0694561 -4.0696359 -4.0678635][-4.1453857 -4.1382561 -4.1312141 -4.1187606 -4.1042275 -4.0930028 -4.0930057 -4.1011996 -4.1077476 -4.1097956 -4.1078691 -4.1030288 -4.104393 -4.1079698 -4.1069293][-4.1213245 -4.1062727 -4.0973253 -4.0896668 -4.0820203 -4.0773377 -4.0839982 -4.0954218 -4.1021132 -4.1083121 -4.1153417 -4.1211443 -4.1312218 -4.1408863 -4.1450686][-4.1039143 -4.0853839 -4.0784168 -4.0749364 -4.0748463 -4.0785561 -4.0907688 -4.1026511 -4.107717 -4.1168227 -4.1297393 -4.1436214 -4.1573095 -4.1701488 -4.1809][-4.1213875 -4.109396 -4.10816 -4.1078362 -4.1144271 -4.1253614 -4.141921 -4.1520419 -4.1536117 -4.1604528 -4.1721506 -4.1850395 -4.1948543 -4.2050476 -4.21559][-4.1692553 -4.1683121 -4.173336 -4.1768742 -4.1854191 -4.1979623 -4.2128162 -4.2175522 -4.215836 -4.2206864 -4.2288637 -4.23934 -4.24599 -4.2520056 -4.2593789][-4.223176 -4.2323031 -4.2414613 -4.2478132 -4.2550063 -4.26497 -4.2758269 -4.2767315 -4.2738185 -4.2762761 -4.28133 -4.2902603 -4.2978663 -4.3039937 -4.3100233][-4.2793107 -4.2926626 -4.3022509 -4.3078012 -4.3119955 -4.316443 -4.3216448 -4.3186665 -4.3142762 -4.3141494 -4.3173037 -4.3251839 -4.3328595 -4.3386145 -4.3434339]]...]
INFO - root - 2017-12-08 01:29:50.768492: step 27210, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.112 sec/batch; 32h:48m:38s remains)
INFO - root - 2017-12-08 01:30:11.444062: step 27220, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.109 sec/batch; 32h:45m:16s remains)
INFO - root - 2017-12-08 01:30:32.664405: step 27230, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.128 sec/batch; 33h:02m:19s remains)
INFO - root - 2017-12-08 01:30:54.015722: step 27240, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.083 sec/batch; 32h:20m:16s remains)
INFO - root - 2017-12-08 01:31:14.804775: step 27250, loss = 2.08, batch loss = 2.03 (15.2 examples/sec; 2.101 sec/batch; 32h:36m:49s remains)
INFO - root - 2017-12-08 01:31:35.955371: step 27260, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.113 sec/batch; 32h:47m:43s remains)
INFO - root - 2017-12-08 01:31:57.151943: step 27270, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.101 sec/batch; 32h:35m:50s remains)
INFO - root - 2017-12-08 01:32:18.134412: step 27280, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 1.917 sec/batch; 29h:44m:04s remains)
INFO - root - 2017-12-08 01:32:39.626118: step 27290, loss = 2.07, batch loss = 2.01 (14.5 examples/sec; 2.210 sec/batch; 34h:16m:12s remains)
INFO - root - 2017-12-08 01:33:00.960436: step 27300, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.160 sec/batch; 33h:29m:46s remains)
2017-12-08 01:33:02.529888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3148966 -4.3075156 -4.2946439 -4.2780352 -4.2700086 -4.2744265 -4.2841721 -4.29137 -4.294755 -4.2945633 -4.2931113 -4.2972393 -4.3028655 -4.3067064 -4.3073525][-4.298511 -4.2858181 -4.266078 -4.2443314 -4.2356677 -4.2406855 -4.2523293 -4.2607307 -4.2620988 -4.2617378 -4.2614937 -4.2705412 -4.2799134 -4.2869072 -4.289784][-4.2674761 -4.246027 -4.2210479 -4.1997123 -4.19382 -4.1979289 -4.2080989 -4.2158418 -4.2123451 -4.2128654 -4.2185149 -4.2358179 -4.2516971 -4.2647338 -4.2716885][-4.2260685 -4.1960378 -4.1703949 -4.1537123 -4.1484528 -4.14966 -4.1585011 -4.1644254 -4.1573944 -4.1611891 -4.1727633 -4.1922822 -4.210866 -4.2308593 -4.2466373][-4.1955309 -4.1651697 -4.1407242 -4.1255784 -4.1158991 -4.1103764 -4.1088772 -4.1088204 -4.0995865 -4.103828 -4.1162276 -4.1298251 -4.1485457 -4.1785479 -4.209403][-4.1831536 -4.1543288 -4.1300745 -4.1147394 -4.10133 -4.0808558 -4.0508566 -4.0258517 -4.0071192 -4.0146375 -4.0288491 -4.0469341 -4.0800619 -4.1297488 -4.1777287][-4.1856189 -4.1616549 -4.1489711 -4.1459713 -4.1311727 -4.0843887 -4.0052023 -3.9394581 -3.9163389 -3.9433403 -3.9801397 -4.0214453 -4.0747914 -4.1329408 -4.1801281][-4.1994472 -4.1833572 -4.18432 -4.1908612 -4.1706514 -4.0980577 -3.9873013 -3.9090521 -3.9077625 -3.9677482 -4.0286751 -4.0815492 -4.13425 -4.177228 -4.2057385][-4.2149868 -4.2027497 -4.2069144 -4.2106037 -4.1859922 -4.1044359 -4.0009041 -3.9504619 -3.9831247 -4.0569582 -4.1136479 -4.1571612 -4.1947865 -4.2202888 -4.2313838][-4.2146163 -4.2042818 -4.2008591 -4.1977558 -4.1741538 -4.10447 -4.0287733 -4.0173264 -4.0723238 -4.13225 -4.1624751 -4.1938519 -4.2257929 -4.2447958 -4.2473159][-4.2010303 -4.1891208 -4.1821785 -4.1794891 -4.1636705 -4.1155944 -4.065155 -4.0704904 -4.1240225 -4.1626892 -4.1785159 -4.2074709 -4.2377224 -4.2544794 -4.256856][-4.196877 -4.1806684 -4.1706886 -4.1684718 -4.1573935 -4.1290078 -4.0988584 -4.1095204 -4.1513858 -4.173388 -4.1864204 -4.2163982 -4.2474208 -4.2633519 -4.267849][-4.20555 -4.17869 -4.1616211 -4.1592746 -4.1553416 -4.1455383 -4.1338377 -4.1494246 -4.177177 -4.1856356 -4.1956625 -4.223886 -4.254703 -4.27119 -4.2799492][-4.238893 -4.2096872 -4.19123 -4.1882219 -4.1862512 -4.182775 -4.1770258 -4.1897397 -4.2059803 -4.2095652 -4.2169805 -4.2386751 -4.2658262 -4.2847338 -4.2974777][-4.284349 -4.2615395 -4.2463942 -4.2426705 -4.2401071 -4.2370186 -4.2340436 -4.2409997 -4.2490578 -4.2509356 -4.2545161 -4.2693977 -4.2909579 -4.3081894 -4.3198915]]...]
INFO - root - 2017-12-08 01:33:23.737785: step 27310, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.132 sec/batch; 33h:03m:25s remains)
INFO - root - 2017-12-08 01:33:44.610284: step 27320, loss = 2.06, batch loss = 2.00 (15.1 examples/sec; 2.124 sec/batch; 32h:55m:50s remains)
INFO - root - 2017-12-08 01:34:05.845814: step 27330, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.126 sec/batch; 32h:56m:51s remains)
INFO - root - 2017-12-08 01:34:26.875182: step 27340, loss = 2.06, batch loss = 2.00 (15.5 examples/sec; 2.069 sec/batch; 32h:03m:33s remains)
INFO - root - 2017-12-08 01:34:48.006083: step 27350, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.098 sec/batch; 32h:30m:43s remains)
INFO - root - 2017-12-08 01:35:09.390631: step 27360, loss = 2.07, batch loss = 2.02 (14.9 examples/sec; 2.148 sec/batch; 33h:16m:27s remains)
INFO - root - 2017-12-08 01:35:30.625009: step 27370, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.145 sec/batch; 33h:13m:14s remains)
INFO - root - 2017-12-08 01:35:51.439867: step 27380, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.122 sec/batch; 32h:51m:12s remains)
INFO - root - 2017-12-08 01:36:12.754240: step 27390, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.090 sec/batch; 32h:21m:11s remains)
INFO - root - 2017-12-08 01:36:33.884343: step 27400, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.089 sec/batch; 32h:19m:44s remains)
2017-12-08 01:36:35.395321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3135467 -4.3089771 -4.3061175 -4.2930789 -4.2833886 -4.276391 -4.2742348 -4.2766261 -4.2808266 -4.2934203 -4.309361 -4.3232541 -4.330718 -4.3350658 -4.3360052][-4.3119779 -4.3025856 -4.2913065 -4.2703772 -4.25741 -4.2476578 -4.2390509 -4.2377014 -4.24544 -4.2669816 -4.293313 -4.3152266 -4.3306293 -4.3370247 -4.3360682][-4.3014526 -4.2905703 -4.2700047 -4.24047 -4.2286687 -4.2193127 -4.2049694 -4.196312 -4.2049184 -4.2331934 -4.2677307 -4.2993417 -4.3240786 -4.3355846 -4.3354077][-4.2724767 -4.2633815 -4.2378993 -4.2020774 -4.192451 -4.1813865 -4.1608071 -4.14669 -4.1598749 -4.1924081 -4.2332249 -4.2775259 -4.3128223 -4.3312445 -4.3334007][-4.24571 -4.2386451 -4.20671 -4.1633711 -4.147563 -4.1303053 -4.097877 -4.0811477 -4.1037498 -4.1457591 -4.1953692 -4.2532244 -4.2993631 -4.3259845 -4.3319964][-4.2292275 -4.2286944 -4.1951151 -4.1450005 -4.1116629 -4.0703549 -4.014677 -3.9929209 -4.0380611 -4.1031952 -4.1656303 -4.2339621 -4.2874722 -4.321609 -4.331646][-4.2255292 -4.2377534 -4.2106161 -4.1566911 -4.1020708 -4.022716 -3.9184289 -3.8773568 -3.9586515 -4.057889 -4.1385574 -4.216785 -4.2794862 -4.3192196 -4.3311648][-4.2414851 -4.269258 -4.2531209 -4.2005148 -4.1307888 -4.0227265 -3.8710938 -3.7957251 -3.8969383 -4.0197439 -4.1130204 -4.198761 -4.2700062 -4.3137407 -4.3276982][-4.257854 -4.2971363 -4.2974672 -4.2577457 -4.1921387 -4.0891571 -3.9405525 -3.8477631 -3.9063756 -4.0077672 -4.0949011 -4.180573 -4.2555847 -4.3032126 -4.3223319][-4.2703247 -4.3132806 -4.3257651 -4.301044 -4.2497754 -4.1692419 -4.0481806 -3.9554315 -3.9581532 -4.014607 -4.083879 -4.166224 -4.2436433 -4.2950044 -4.3173285][-4.2799048 -4.321496 -4.3428884 -4.3344293 -4.3013153 -4.2456856 -4.1565695 -4.0719719 -4.0360756 -4.0515337 -4.100143 -4.1714582 -4.2448049 -4.2956071 -4.3169661][-4.2870841 -4.3237271 -4.3472195 -4.3481154 -4.3325372 -4.3010674 -4.2418284 -4.1747689 -4.1281457 -4.1242738 -4.155447 -4.2068076 -4.2657957 -4.3078747 -4.3230014][-4.3009672 -4.3286991 -4.3462806 -4.3489695 -4.3449225 -4.3309193 -4.2943864 -4.2440987 -4.2015314 -4.1911817 -4.2105513 -4.2491741 -4.2948561 -4.3251476 -4.3320065][-4.3183136 -4.3349452 -4.3404622 -4.3383465 -4.3392 -4.3358812 -4.3152623 -4.2809286 -4.2499523 -4.2403655 -4.2533183 -4.28466 -4.3198419 -4.3405838 -4.3416924][-4.3305759 -4.3354616 -4.3318486 -4.325027 -4.3236685 -4.3249655 -4.3174667 -4.2994614 -4.2819104 -4.2773957 -4.2871871 -4.3101597 -4.3358583 -4.3491273 -4.34835]]...]
INFO - root - 2017-12-08 01:36:56.274273: step 27410, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.110 sec/batch; 32h:39m:35s remains)
INFO - root - 2017-12-08 01:37:17.589520: step 27420, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 32h:48m:27s remains)
INFO - root - 2017-12-08 01:37:38.810147: step 27430, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 2.111 sec/batch; 32h:39m:53s remains)
INFO - root - 2017-12-08 01:37:59.860444: step 27440, loss = 2.08, batch loss = 2.02 (16.6 examples/sec; 1.923 sec/batch; 29h:44m:39s remains)
INFO - root - 2017-12-08 01:38:20.737652: step 27450, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.072 sec/batch; 32h:02m:20s remains)
INFO - root - 2017-12-08 01:38:41.798485: step 27460, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.102 sec/batch; 32h:29m:50s remains)
INFO - root - 2017-12-08 01:39:02.893566: step 27470, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 2.063 sec/batch; 31h:53m:33s remains)
INFO - root - 2017-12-08 01:39:23.423572: step 27480, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.081 sec/batch; 32h:10m:01s remains)
INFO - root - 2017-12-08 01:39:44.752854: step 27490, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.108 sec/batch; 32h:34m:38s remains)
INFO - root - 2017-12-08 01:40:06.015982: step 27500, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.120 sec/batch; 32h:45m:37s remains)
2017-12-08 01:40:07.514426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1931448 -4.211513 -4.2361546 -4.2628913 -4.2772918 -4.2769461 -4.2641983 -4.2517128 -4.2517776 -4.263032 -4.2686977 -4.269784 -4.2669029 -4.2650032 -4.2596889][-4.2378726 -4.2282305 -4.2362852 -4.2499433 -4.2486134 -4.228651 -4.2052131 -4.1981883 -4.210546 -4.2336078 -4.2542415 -4.2647328 -4.2677722 -4.2683139 -4.2686734][-4.2349782 -4.2038484 -4.20699 -4.2148938 -4.2022877 -4.1616664 -4.12149 -4.116569 -4.1410046 -4.1793814 -4.2177329 -4.2416596 -4.2544374 -4.2627668 -4.2723265][-4.2055268 -4.1605158 -4.1651931 -4.1753135 -4.1570015 -4.0945477 -4.0303698 -4.01758 -4.0481796 -4.1015363 -4.1597691 -4.1995049 -4.2257829 -4.2456684 -4.2641478][-4.1373644 -4.0955067 -4.1177621 -4.1434112 -4.1208887 -4.0370793 -3.94145 -3.9092896 -3.9426949 -4.0109463 -4.0918884 -4.1506133 -4.1931973 -4.2223015 -4.2469287][-4.0770173 -4.0628185 -4.1063604 -4.1351833 -4.0965943 -3.9840884 -3.8591697 -3.8183787 -3.8615208 -3.9488516 -4.0571723 -4.1375561 -4.1937718 -4.2259264 -4.2486544][-4.072897 -4.0829449 -4.1260958 -4.1336355 -4.0700159 -3.9369013 -3.8089149 -3.7838013 -3.8464823 -3.9519656 -4.0840888 -4.1794915 -4.2376094 -4.2655706 -4.2827368][-4.1480427 -4.1651936 -4.1886959 -4.1682496 -4.0927343 -3.9626694 -3.8516161 -3.8368332 -3.8980529 -3.9978504 -4.12693 -4.2236834 -4.2803707 -4.3105183 -4.3270564][-4.2632885 -4.2761345 -4.2809792 -4.2451382 -4.1697 -4.0564504 -3.9598815 -3.932441 -3.9710763 -4.0492616 -4.1593342 -4.2516837 -4.3095894 -4.3445044 -4.3622217][-4.3427777 -4.3498149 -4.3485565 -4.3125072 -4.2457724 -4.1539297 -4.0747418 -4.03782 -4.0521259 -4.1066241 -4.1938844 -4.2734318 -4.3276138 -4.3619881 -4.3814898][-4.3575163 -4.3625455 -4.3640742 -4.33883 -4.2920146 -4.2291627 -4.1733184 -4.1418567 -4.1436324 -4.1802511 -4.2420731 -4.3006716 -4.3419681 -4.3683228 -4.384181][-4.3341732 -4.339529 -4.3445783 -4.3342438 -4.3138909 -4.2838387 -4.2513075 -4.2311721 -4.2331991 -4.2592068 -4.297369 -4.3356524 -4.360734 -4.374011 -4.3775425][-4.2811289 -4.2859926 -4.2940321 -4.303493 -4.3148689 -4.3169231 -4.3052258 -4.29819 -4.3029718 -4.3221626 -4.3414488 -4.3608937 -4.3705883 -4.3702888 -4.3613214][-4.1734104 -4.1770911 -4.2012572 -4.2395496 -4.2842722 -4.3145857 -4.3236232 -4.327168 -4.3310971 -4.3413072 -4.346427 -4.3536091 -4.3557577 -4.3487024 -4.3307648][-4.0280089 -4.0436578 -4.0990462 -4.1723042 -4.2427492 -4.2888956 -4.3088131 -4.3142309 -4.3124971 -4.3118768 -4.307735 -4.3084941 -4.3077989 -4.298296 -4.2755108]]...]
INFO - root - 2017-12-08 01:40:28.403115: step 27510, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.124 sec/batch; 32h:48m:29s remains)
INFO - root - 2017-12-08 01:40:49.545239: step 27520, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.108 sec/batch; 32h:33m:18s remains)
INFO - root - 2017-12-08 01:41:10.540269: step 27530, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.065 sec/batch; 31h:53m:48s remains)
INFO - root - 2017-12-08 01:41:31.417540: step 27540, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.119 sec/batch; 32h:43m:07s remains)
INFO - root - 2017-12-08 01:41:52.616329: step 27550, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.103 sec/batch; 32h:27m:43s remains)
INFO - root - 2017-12-08 01:42:13.686708: step 27560, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.072 sec/batch; 31h:58m:49s remains)
INFO - root - 2017-12-08 01:42:34.644037: step 27570, loss = 2.07, batch loss = 2.01 (16.7 examples/sec; 1.916 sec/batch; 29h:34m:03s remains)
INFO - root - 2017-12-08 01:42:55.741856: step 27580, loss = 2.06, batch loss = 2.00 (14.8 examples/sec; 2.167 sec/batch; 33h:25m:50s remains)
INFO - root - 2017-12-08 01:43:17.105359: step 27590, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.130 sec/batch; 32h:51m:56s remains)
INFO - root - 2017-12-08 01:43:38.208806: step 27600, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.145 sec/batch; 33h:04m:46s remains)
2017-12-08 01:43:39.785000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1226683 -4.1455574 -4.1936584 -4.2261314 -4.225379 -4.198823 -4.1445928 -4.0864525 -4.0347872 -4.0147095 -4.0437202 -4.0931258 -4.1384425 -4.1864157 -4.2385373][-4.1818194 -4.2124228 -4.2617683 -4.2894635 -4.2864509 -4.2593818 -4.2118149 -4.1717606 -4.1415057 -4.1351762 -4.1559639 -4.1822634 -4.2071757 -4.2426653 -4.2780056][-4.2432261 -4.263422 -4.2962441 -4.3116093 -4.2991734 -4.2655005 -4.2206721 -4.1934543 -4.1779838 -4.1856947 -4.2098436 -4.2275629 -4.2429585 -4.2728662 -4.2998543][-4.2726746 -4.2816758 -4.29428 -4.2903275 -4.2658968 -4.2301 -4.1921883 -4.1664662 -4.1572919 -4.177228 -4.2135587 -4.2364659 -4.2548652 -4.2820024 -4.3035831][-4.2532048 -4.2535639 -4.2468066 -4.2245913 -4.1887808 -4.1471052 -4.1059318 -4.0715046 -4.0694 -4.1067667 -4.1596432 -4.1996446 -4.2315731 -4.2648873 -4.291018][-4.1967072 -4.1872926 -4.1697493 -4.1388087 -4.0885787 -4.0237904 -3.9502883 -3.8814802 -3.8846288 -3.9632373 -4.0551028 -4.1312728 -4.1870284 -4.230876 -4.2653713][-4.1512647 -4.1348009 -4.1145816 -4.0781355 -4.0106287 -3.9095719 -3.7733781 -3.6446116 -3.6602213 -3.7996309 -3.9466684 -4.0638366 -4.1442933 -4.2014661 -4.2446856][-4.1637864 -4.1477194 -4.1274009 -4.0907226 -4.022655 -3.9184387 -3.7783773 -3.6515112 -3.662019 -3.7899528 -3.937583 -4.0594869 -4.1442847 -4.2038426 -4.2466812][-4.2243633 -4.21298 -4.1973095 -4.1666489 -4.1134281 -4.03823 -3.9492843 -3.8766737 -3.8736184 -3.9437389 -4.04447 -4.1361861 -4.2000275 -4.2409949 -4.2707877][-4.2846594 -4.2756605 -4.2622051 -4.2389045 -4.2046514 -4.1609678 -4.1123452 -4.076787 -4.0704737 -4.1010332 -4.1573782 -4.2175736 -4.2632227 -4.2873273 -4.300405][-4.3196044 -4.3153348 -4.3070192 -4.2917337 -4.2719736 -4.24896 -4.22015 -4.1972961 -4.1895 -4.2047176 -4.2352123 -4.2735724 -4.3035421 -4.316514 -4.3203378][-4.3335781 -4.3334937 -4.3309927 -4.3254104 -4.3160276 -4.3021617 -4.2818246 -4.2637682 -4.2565522 -4.2647786 -4.2818422 -4.3042502 -4.3214636 -4.32703 -4.3259239][-4.3361549 -4.3404431 -4.34362 -4.3454261 -4.3422747 -4.3319798 -4.3107738 -4.2877779 -4.2718611 -4.2679863 -4.2760539 -4.2906675 -4.3035088 -4.3073444 -4.3097029][-4.3298168 -4.3358212 -4.3413658 -4.3444781 -4.3393836 -4.3246293 -4.2984185 -4.2649 -4.2346215 -4.2174344 -4.2216964 -4.2383451 -4.2574339 -4.2712636 -4.28283][-4.3210278 -4.3258586 -4.328197 -4.3237519 -4.3079605 -4.2813969 -4.2426481 -4.1969514 -4.155251 -4.1321592 -4.140666 -4.1682849 -4.2056832 -4.2348995 -4.2556057]]...]
INFO - root - 2017-12-08 01:44:00.784440: step 27610, loss = 2.08, batch loss = 2.03 (14.8 examples/sec; 2.157 sec/batch; 33h:15m:25s remains)
INFO - root - 2017-12-08 01:44:22.155888: step 27620, loss = 2.06, batch loss = 2.01 (14.7 examples/sec; 2.177 sec/batch; 33h:33m:29s remains)
INFO - root - 2017-12-08 01:44:43.367648: step 27630, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.157 sec/batch; 33h:15m:00s remains)
INFO - root - 2017-12-08 01:45:04.245812: step 27640, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 2.081 sec/batch; 32h:04m:07s remains)
INFO - root - 2017-12-08 01:45:25.523222: step 27650, loss = 2.07, batch loss = 2.01 (15.3 examples/sec; 2.091 sec/batch; 32h:13m:27s remains)
INFO - root - 2017-12-08 01:45:46.941581: step 27660, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.113 sec/batch; 32h:32m:57s remains)
INFO - root - 2017-12-08 01:46:08.108896: step 27670, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.121 sec/batch; 32h:40m:39s remains)
INFO - root - 2017-12-08 01:46:29.368208: step 27680, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.120 sec/batch; 32h:39m:04s remains)
INFO - root - 2017-12-08 01:46:50.415222: step 27690, loss = 2.08, batch loss = 2.02 (15.4 examples/sec; 2.074 sec/batch; 31h:56m:21s remains)
INFO - root - 2017-12-08 01:47:11.228190: step 27700, loss = 2.07, batch loss = 2.02 (16.7 examples/sec; 1.919 sec/batch; 29h:32m:38s remains)
2017-12-08 01:47:12.829854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2475343 -4.2490849 -4.2495756 -4.2500005 -4.2494822 -4.2479086 -4.2469783 -4.247602 -4.2484579 -4.2473702 -4.246489 -4.2466397 -4.2461147 -4.243083 -4.2387371][-4.2507596 -4.2534175 -4.2553196 -4.255805 -4.2533126 -4.2488956 -4.2466593 -4.2488952 -4.2523227 -4.2534866 -4.2534828 -4.2529411 -4.250803 -4.2462654 -4.24086][-4.2550659 -4.2584281 -4.2602496 -4.2579956 -4.2502 -4.2393756 -4.2328773 -4.2364321 -4.2464161 -4.2552838 -4.2603221 -4.2620292 -4.2596884 -4.2532954 -4.2457705][-4.25592 -4.2580571 -4.2563009 -4.2470345 -4.2298207 -4.20757 -4.1931214 -4.1974673 -4.2177196 -4.2409091 -4.2572994 -4.2661695 -4.2673221 -4.2611737 -4.2521648][-4.2511158 -4.2482595 -4.2381883 -4.2176385 -4.187552 -4.1495767 -4.1207218 -4.1227822 -4.1569805 -4.2013392 -4.2349739 -4.2555504 -4.2643065 -4.2620864 -4.2542343][-4.2391524 -4.2269969 -4.2058754 -4.1718683 -4.1264405 -4.0686073 -4.0172415 -4.0104437 -4.0612035 -4.1316423 -4.1876826 -4.2246294 -4.2449822 -4.2502518 -4.2471085][-4.2272916 -4.205647 -4.1745753 -4.1291442 -4.0700569 -3.9931662 -3.9144711 -3.8883379 -3.9490647 -4.0406213 -4.1170416 -4.1725874 -4.2087259 -4.2260532 -4.23194][-4.2251139 -4.199789 -4.1642456 -4.1149392 -4.0520859 -3.9675167 -3.8715019 -3.8219094 -3.8751545 -3.969322 -4.0544786 -4.123342 -4.1732888 -4.2018747 -4.21601][-4.2352748 -4.2159643 -4.1875782 -4.1474886 -4.0962768 -4.0254726 -3.9420323 -3.89304 -3.9259248 -3.9964314 -4.0653529 -4.1259766 -4.1723866 -4.1995935 -4.2137289][-4.2411213 -4.2359767 -4.22471 -4.2035117 -4.1718836 -4.123291 -4.0662818 -4.0329685 -4.05282 -4.0972137 -4.1399641 -4.1779718 -4.205677 -4.2190962 -4.2250366][-4.2309241 -4.2392492 -4.2464824 -4.246109 -4.2348194 -4.2091274 -4.1774716 -4.1598663 -4.1713171 -4.1957488 -4.2171092 -4.2336221 -4.2417607 -4.2387848 -4.2332444][-4.2069988 -4.2213697 -4.2411413 -4.2566295 -4.2601776 -4.25086 -4.2360778 -4.227097 -4.2318735 -4.2439723 -4.25235 -4.2556496 -4.2514257 -4.2378154 -4.2240295][-4.1766162 -4.18842 -4.2108121 -4.2329125 -4.2448392 -4.2445951 -4.2377458 -4.2312474 -4.2302356 -4.2332587 -4.2345452 -4.2311759 -4.2206726 -4.2021317 -4.1851406][-4.1538267 -4.1580625 -4.173336 -4.1911821 -4.2027988 -4.205164 -4.2012734 -4.1950006 -4.1896296 -4.1877251 -4.1858473 -4.1801324 -4.1677227 -4.1482697 -4.1325574][-4.1609564 -4.160068 -4.1658311 -4.1737604 -4.1790447 -4.1787128 -4.1742172 -4.1670671 -4.1595745 -4.1556931 -4.1534085 -4.1495132 -4.1398067 -4.1224575 -4.1090407]]...]
INFO - root - 2017-12-08 01:47:34.007117: step 27710, loss = 2.08, batch loss = 2.02 (15.5 examples/sec; 2.065 sec/batch; 31h:47m:06s remains)
INFO - root - 2017-12-08 01:47:55.199781: step 27720, loss = 2.09, batch loss = 2.03 (15.2 examples/sec; 2.104 sec/batch; 32h:22m:42s remains)
INFO - root - 2017-12-08 01:48:16.439731: step 27730, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.150 sec/batch; 33h:05m:21s remains)
INFO - root - 2017-12-08 01:48:37.256379: step 27740, loss = 2.08, batch loss = 2.02 (15.0 examples/sec; 2.132 sec/batch; 32h:48m:18s remains)
INFO - root - 2017-12-08 01:48:58.430796: step 27750, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.106 sec/batch; 32h:23m:18s remains)
INFO - root - 2017-12-08 01:49:19.798401: step 27760, loss = 2.09, batch loss = 2.03 (14.9 examples/sec; 2.148 sec/batch; 33h:01m:37s remains)
INFO - root - 2017-12-08 01:49:40.812028: step 27770, loss = 2.08, batch loss = 2.03 (15.4 examples/sec; 2.083 sec/batch; 32h:01m:45s remains)
INFO - root - 2017-12-08 01:50:02.211083: step 27780, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.102 sec/batch; 32h:18m:55s remains)
INFO - root - 2017-12-08 01:50:23.352269: step 27790, loss = 2.09, batch loss = 2.03 (15.0 examples/sec; 2.134 sec/batch; 32h:47m:48s remains)
INFO - root - 2017-12-08 01:50:44.259506: step 27800, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.146 sec/batch; 32h:58m:28s remains)
2017-12-08 01:50:45.799395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2237659 -4.238996 -4.2459888 -4.2682562 -4.281796 -4.2890229 -4.2972412 -4.2984529 -4.3007731 -4.3025823 -4.2996044 -4.2941546 -4.2866721 -4.2734852 -4.2519069][-4.2131596 -4.2334328 -4.2429991 -4.2657437 -4.27818 -4.28462 -4.2910395 -4.2889872 -4.2867646 -4.2861876 -4.2846956 -4.2817721 -4.2745 -4.2597752 -4.2382808][-4.1842537 -4.211132 -4.2253137 -4.2497358 -4.2614331 -4.2653894 -4.2698593 -4.265852 -4.2608156 -4.2624893 -4.2681489 -4.27021 -4.2654791 -4.2533073 -4.2363][-4.15578 -4.187695 -4.2065129 -4.2296286 -4.2383771 -4.2378788 -4.2412872 -4.2392049 -4.2375274 -4.2442946 -4.2568297 -4.2615004 -4.2587934 -4.2494831 -4.2369056][-4.1565719 -4.1845832 -4.2053108 -4.2265205 -4.2295284 -4.2246971 -4.2270741 -4.2294979 -4.2324781 -4.2417059 -4.2549348 -4.259129 -4.2566257 -4.2494426 -4.2397313][-4.1658955 -4.1865921 -4.2083006 -4.22857 -4.2307591 -4.2249141 -4.2248669 -4.2254295 -4.2288852 -4.2365842 -4.2472453 -4.2514195 -4.2496004 -4.24374 -4.240499][-4.1512508 -4.1720166 -4.1988683 -4.2212596 -4.22529 -4.2169223 -4.2078838 -4.2007403 -4.2039719 -4.2119946 -4.228972 -4.2376132 -4.2343383 -4.2294488 -4.2349486][-4.1294961 -4.153017 -4.1859984 -4.2087269 -4.2104206 -4.1902261 -4.1623449 -4.1436887 -4.1478395 -4.1647649 -4.1944513 -4.2165518 -4.2177587 -4.2164121 -4.2252831][-4.1230559 -4.139472 -4.1696181 -4.1899514 -4.185267 -4.1560183 -4.1176562 -4.0925822 -4.100276 -4.1303792 -4.1740928 -4.2068696 -4.2118816 -4.2085562 -4.2168279][-4.1397524 -4.1450443 -4.1666303 -4.1839433 -4.1809845 -4.1580744 -4.127284 -4.1102576 -4.124465 -4.1557603 -4.19376 -4.21826 -4.2169619 -4.2090249 -4.2144389][-4.1650505 -4.1656466 -4.178978 -4.1961412 -4.2028208 -4.1965189 -4.1807122 -4.1734991 -4.1890349 -4.2151594 -4.2395787 -4.2473693 -4.2318549 -4.2132955 -4.2136354][-4.1638103 -4.1666203 -4.1742177 -4.1890669 -4.2036133 -4.2122126 -4.2105746 -4.2126884 -4.2291064 -4.253716 -4.2702508 -4.2658739 -4.2389803 -4.2120886 -4.2085614][-4.145927 -4.1534486 -4.1593671 -4.1715736 -4.1912332 -4.2143073 -4.2285986 -4.2341552 -4.2457385 -4.2697954 -4.2837977 -4.2760859 -4.2471442 -4.2207255 -4.2169995][-4.1480508 -4.1598058 -4.1631184 -4.1695523 -4.1858025 -4.2087483 -4.22554 -4.2293081 -4.2361383 -4.2579288 -4.2770667 -4.2770319 -4.2564669 -4.237308 -4.2383795][-4.1673074 -4.1820574 -4.1824174 -4.1822233 -4.1901793 -4.2032042 -4.2128706 -4.2141743 -4.2176833 -4.234971 -4.2578712 -4.2666497 -4.2561555 -4.246542 -4.2521763]]...]
INFO - root - 2017-12-08 01:51:07.153231: step 27810, loss = 2.08, batch loss = 2.02 (14.9 examples/sec; 2.147 sec/batch; 32h:59m:22s remains)
INFO - root - 2017-12-08 01:51:28.245758: step 27820, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.094 sec/batch; 32h:10m:08s remains)
INFO - root - 2017-12-08 01:51:49.215855: step 27830, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.106 sec/batch; 32h:20m:27s remains)
INFO - root - 2017-12-08 01:52:10.345529: step 27840, loss = 2.07, batch loss = 2.01 (15.2 examples/sec; 2.101 sec/batch; 32h:15m:31s remains)
INFO - root - 2017-12-08 01:52:31.728366: step 27850, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 2.163 sec/batch; 33h:12m:21s remains)
INFO - root - 2017-12-08 01:52:52.782619: step 27860, loss = 2.08, batch loss = 2.02 (16.4 examples/sec; 1.955 sec/batch; 30h:00m:59s remains)
INFO - root - 2017-12-08 01:53:14.012132: step 27870, loss = 2.07, batch loss = 2.01 (14.4 examples/sec; 2.226 sec/batch; 34h:10m:03s remains)
INFO - root - 2017-12-08 01:53:35.346400: step 27880, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 2.109 sec/batch; 32h:21m:40s remains)
INFO - root - 2017-12-08 01:53:56.571985: step 27890, loss = 2.08, batch loss = 2.02 (15.1 examples/sec; 2.118 sec/batch; 32h:29m:20s remains)
INFO - root - 2017-12-08 01:54:17.488730: step 27900, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 2.129 sec/batch; 32h:39m:21s remains)
2017-12-08 01:54:19.098856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3334985 -4.33342 -4.3317256 -4.3307171 -4.3341689 -4.3389072 -4.3421059 -4.34267 -4.341466 -4.3403378 -4.3398361 -4.34052 -4.340302 -4.3390651 -4.3378797][-4.3253994 -4.3238435 -4.3221536 -4.3233438 -4.3305588 -4.3374281 -4.3383665 -4.3356085 -4.3314519 -4.3287983 -4.3287153 -4.3304048 -4.33033 -4.3288417 -4.3287377][-4.3158627 -4.3121986 -4.31163 -4.3148494 -4.3230486 -4.329668 -4.3254476 -4.318532 -4.3127332 -4.3106441 -4.3117971 -4.3164878 -4.3183537 -4.31638 -4.3169441][-4.3003669 -4.2960992 -4.2972646 -4.3013277 -4.3077989 -4.3091168 -4.295836 -4.283843 -4.2813425 -4.2869344 -4.2962809 -4.3072367 -4.3116693 -4.3087811 -4.3084064][-4.2822742 -4.2775383 -4.2772861 -4.2779846 -4.2754245 -4.2620349 -4.2327065 -4.2125988 -4.2209606 -4.2464137 -4.2735329 -4.2939968 -4.3033738 -4.3011007 -4.298285][-4.2618151 -4.2560835 -4.2509985 -4.243897 -4.2280746 -4.1937408 -4.1384106 -4.1030149 -4.1274929 -4.1856775 -4.2412519 -4.27559 -4.2901688 -4.287848 -4.2816296][-4.2522783 -4.241374 -4.2279463 -4.2069755 -4.1732764 -4.1089191 -4.0122681 -3.9521627 -3.9974654 -4.0994811 -4.1881795 -4.2426286 -4.2685347 -4.2711735 -4.2636046][-4.2561359 -4.2415366 -4.2218909 -4.1902919 -4.1373835 -4.0388422 -3.9000335 -3.8156095 -3.8882995 -4.0259614 -4.1319971 -4.2003121 -4.240355 -4.2542167 -4.2516975][-4.2654762 -4.2536397 -4.2408876 -4.2151589 -4.1611104 -4.0565534 -3.9162221 -3.8339243 -3.9067974 -4.0339403 -4.1250372 -4.1864605 -4.2305884 -4.2502313 -4.2487612][-4.2606049 -4.2563047 -4.2581525 -4.2526951 -4.2208419 -4.1434445 -4.041399 -3.9863758 -4.0329452 -4.1146178 -4.1732492 -4.2118325 -4.2434893 -4.2586694 -4.2526054][-4.2442961 -4.2477288 -4.2633872 -4.2744846 -4.2653809 -4.223834 -4.1658564 -4.136786 -4.15808 -4.1963849 -4.225091 -4.2458687 -4.2658205 -4.2744579 -4.2635274][-4.2145171 -4.229054 -4.2559404 -4.2788124 -4.2869225 -4.2722077 -4.2440968 -4.2314243 -4.2406297 -4.2542458 -4.26581 -4.27761 -4.2903543 -4.2939558 -4.2816973][-4.1643543 -4.193512 -4.2353139 -4.269794 -4.2934232 -4.2967038 -4.2886558 -4.2879486 -4.295475 -4.297039 -4.2996888 -4.3052874 -4.3108439 -4.3100605 -4.2976532][-4.10367 -4.1442194 -4.1988873 -4.2472382 -4.2831559 -4.2998171 -4.3045769 -4.3092437 -4.314889 -4.3134451 -4.3166151 -4.3204908 -4.32044 -4.3160686 -4.3058138][-4.0697017 -4.109683 -4.1637936 -4.2174606 -4.2627068 -4.2906842 -4.3035874 -4.3070879 -4.3084435 -4.307426 -4.3129535 -4.3175516 -4.3161845 -4.3129735 -4.3078122]]...]
INFO - root - 2017-12-08 01:54:40.370394: step 27910, loss = 2.07, batch loss = 2.02 (15.3 examples/sec; 2.089 sec/batch; 32h:02m:50s remains)
INFO - root - 2017-12-08 01:55:01.762349: step 27920, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.162 sec/batch; 33h:09m:37s remains)
INFO - root - 2017-12-08 01:55:22.686836: step 27930, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 2.149 sec/batch; 32h:56m:48s remains)
INFO - root - 2017-12-08 01:55:43.771968: step 27940, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 2.168 sec/batch; 33h:14m:01s remains)
INFO - root - 2017-12-08 01:56:04.999499: step 27950, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 2.091 sec/batch; 32h:02m:57s remains)
INFO - root - 2017-12-08 01:56:26.011006: step 27960, loss = 2.07, batch loss = 2.01 (15.1 examples/sec; 2.114 sec/batch; 32h:23m:31s remains)
INFO - root - 2017-12-08 01:56:43.388585: step 27970, loss = 2.07, batch loss = 2.01 (34.7 examples/sec; 0.922 sec/batch; 14h:07m:05s remains)
