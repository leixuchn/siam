INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "132"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.0001
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv2/split:0", shape=(8, 29, 29, 48), dtype=float32)
Tensor("siamese_fc/conv2/def/transpose:0", shape=(8, 48, 29, 29), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b1/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc/conv2/def/transpose_1:0", shape=(8, 200, 29, 29), dtype=float32)
Tensor("siamese_fc/conv2/def/transpose_2:0", shape=(8, 48, 29, 29), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b2/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc/conv2/def/transpose_3:0", shape=(8, 200, 29, 29), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv2/split:0", shape=(8, 57, 57, 48), dtype=float32)
Tensor("siamese_fc_1/conv2/def/transpose:0", shape=(8, 48, 57, 57), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b1/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc_1/conv2/def/transpose_1:0", shape=(8, 200, 57, 57), dtype=float32)
Tensor("siamese_fc_1/conv2/def/transpose_2:0", shape=(8, 48, 57, 57), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b2/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc_1/conv2/def/transpose_3:0", shape=(8, 200, 57, 57), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-07 03:12:14.846668: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:12:14.846816: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:12:14.846823: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:12:14.846827: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:12:14.846830: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 03:12:15.593999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 4.02GiB
2017-12-07 03:12:15.594034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-07 03:12:15.594041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-07 03:12:15.594049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-07 03:12:27.094948: step 0, loss = 0.65, batch loss = 0.58 (0.9 examples/sec; 9.266 sec/batch; 855h:47m:06s remains)
2017-12-07 03:12:28.441707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0016584 -2.9386351 -2.8971477 -2.9404211 -3.0580115 -3.2084427 -3.3501406 -3.4393036 -3.4728632 -3.4036298 -3.2248185 -3.1093943 -3.0207772 -2.8988693 -2.8312135][-2.9768736 -2.9291968 -2.9399981 -3.0675478 -3.2268531 -3.3786266 -3.5539918 -3.685766 -3.6882062 -3.564611 -3.3467724 -3.1868813 -3.073823 -2.9377351 -2.8528509][-3.0031016 -2.9609051 -3.037298 -3.2426639 -3.3787813 -3.4738595 -3.6685061 -3.7908206 -3.6865311 -3.4815826 -3.210367 -3.0008421 -2.9124012 -2.8219593 -2.7774975][-3.0112383 -2.9789758 -3.1562576 -3.4302456 -3.4660807 -3.4387133 -3.5837016 -3.5939856 -3.3549757 -3.1459312 -2.8930402 -2.6839428 -2.6758623 -2.6398568 -2.6436839][-2.9624033 -3.0045578 -3.3627677 -3.7326925 -3.6656852 -3.4785185 -3.446383 -3.2073965 -2.8991036 -2.8259678 -2.6640377 -2.4869261 -2.5449371 -2.5179017 -2.5286903][-3.0057411 -3.1696796 -3.7069709 -4.1203518 -3.9346275 -3.5549722 -3.188026 -2.5822878 -2.3379622 -2.5979037 -2.6313069 -2.5220628 -2.599123 -2.5260189 -2.5090413][-3.1796865 -3.4085073 -4.0048847 -4.3493805 -4.0177116 -3.407938 -2.5756845 -1.5091217 -1.3565111 -2.0164285 -2.3208768 -2.3833518 -2.5307937 -2.4638991 -2.4826608][-3.3638391 -3.564158 -4.0975161 -4.2920723 -3.767138 -2.8899097 -1.599546 -0.16886902 -0.15305424 -1.1778386 -1.7908473 -2.1340854 -2.3869874 -2.3523736 -2.4300551][-3.4391222 -3.5469022 -3.9732327 -4.047811 -3.4427218 -2.488723 -1.0565016 0.33783102 0.058959484 -1.2021668 -1.9767411 -2.474184 -2.6617296 -2.5037694 -2.5310609][-3.4827528 -3.552948 -3.9115133 -3.881542 -3.3054957 -2.4566855 -1.1708395 -0.17609882 -0.72715878 -1.901479 -2.5634427 -3.0272207 -3.0457983 -2.7015333 -2.6546645][-3.5482297 -3.6602449 -3.9629109 -3.8155534 -3.3039699 -2.595973 -1.5429819 -1.0007551 -1.6721287 -2.5696349 -2.9792824 -3.2915082 -3.1780872 -2.7383776 -2.6853428][-3.5731826 -3.6795275 -3.8621681 -3.6610124 -3.3328035 -2.8660617 -2.1454096 -1.9756291 -2.5866461 -3.1360083 -3.2729039 -3.3875272 -3.1922226 -2.7438607 -2.7087607][-3.5927944 -3.5835519 -3.5544033 -3.3445778 -3.2548223 -3.0496192 -2.677367 -2.7444122 -3.2057872 -3.4796982 -3.451211 -3.4408855 -3.21608 -2.7988956 -2.7665944][-3.5942855 -3.4999249 -3.3514495 -3.2314374 -3.3204598 -3.2439911 -3.0666127 -3.1931758 -3.4434872 -3.5189726 -3.4466281 -3.43407 -3.2496333 -2.8849292 -2.843647][-3.7404103 -3.6487536 -3.4861779 -3.4693487 -3.6285667 -3.5702491 -3.4610636 -3.5621986 -3.6511393 -3.6004303 -3.51738 -3.5080171 -3.3233259 -2.9601555 -2.88657]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.0001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2-adm-0.0001/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 03:12:44.236375: step 10, loss = 0.80, batch loss = 0.73 (5.2 examples/sec; 1.545 sec/batch; 142h:40m:01s remains)
INFO - root - 2017-12-07 03:12:59.324831: step 20, loss = 0.66, batch loss = 0.59 (5.2 examples/sec; 1.549 sec/batch; 143h:01m:20s remains)
INFO - root - 2017-12-07 03:13:17.366910: step 30, loss = 0.67, batch loss = 0.60 (5.0 examples/sec; 1.608 sec/batch; 148h:29m:54s remains)
INFO - root - 2017-12-07 03:13:32.846636: step 40, loss = 0.79, batch loss = 0.72 (5.3 examples/sec; 1.514 sec/batch; 139h:50m:49s remains)
INFO - root - 2017-12-07 03:13:55.549022: step 50, loss = 0.87, batch loss = 0.79 (5.5 examples/sec; 1.450 sec/batch; 133h:56m:29s remains)
INFO - root - 2017-12-07 03:14:10.709445: step 60, loss = 0.99, batch loss = 0.92 (5.3 examples/sec; 1.518 sec/batch; 140h:08m:16s remains)
INFO - root - 2017-12-07 03:19:01.324354: step 70, loss = 0.80, batch loss = 0.73 (5.2 examples/sec; 1.528 sec/batch; 141h:08m:28s remains)
INFO - root - 2017-12-07 03:19:17.762742: step 80, loss = 0.74, batch loss = 0.67 (5.5 examples/sec; 1.454 sec/batch; 134h:17m:31s remains)
INFO - root - 2017-12-07 03:19:35.483333: step 90, loss = 0.87, batch loss = 0.80 (1.9 examples/sec; 4.320 sec/batch; 398h:54m:23s remains)
INFO - root - 2017-12-07 03:19:50.571514: step 100, loss = 0.85, batch loss = 0.78 (5.5 examples/sec; 1.445 sec/batch; 133h:27m:37s remains)
2017-12-07 03:19:51.539836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4742079 -2.5025921 -2.6656601 -2.8236542 -2.9500418 -2.9654779 -2.831634 -2.7511 -2.6284716 -2.4376745 -2.4037864 -2.4330108 -2.43784 -2.5679393 -2.6046467][-2.491396 -2.5457597 -2.6478629 -2.6833844 -2.7060938 -2.6786561 -2.5503483 -2.5123355 -2.4406929 -2.3443723 -2.4138818 -2.5075362 -2.5694084 -2.7305381 -2.76716][-2.5322692 -2.6173472 -2.6888394 -2.6605239 -2.6535916 -2.6380737 -2.5422289 -2.5511253 -2.5095842 -2.4798243 -2.5858455 -2.667872 -2.7434111 -2.905827 -2.9321988][-2.5989866 -2.7116041 -2.7833993 -2.7193298 -2.681581 -2.6497786 -2.5495634 -2.5779204 -2.5491953 -2.5692723 -2.6820259 -2.7294788 -2.8191557 -2.9859285 -3.0062914][-2.6477129 -2.7636309 -2.8332615 -2.7498448 -2.6891589 -2.6325388 -2.5322948 -2.5625174 -2.525382 -2.5703523 -2.6736181 -2.6977925 -2.8247633 -3.0243433 -3.0644803][-2.6492424 -2.7455983 -2.8007684 -2.7306325 -2.6865025 -2.6341891 -2.5436072 -2.5360761 -2.4386849 -2.4571552 -2.5420966 -2.5832109 -2.7828221 -3.0551906 -3.1576252][-2.6041944 -2.6887517 -2.7471714 -2.7071941 -2.6869216 -2.6369326 -2.540307 -2.466907 -2.2921729 -2.2910295 -2.4155521 -2.5254521 -2.7661142 -3.0439792 -3.1725273][-2.483777 -2.5569334 -2.6370349 -2.6484556 -2.66545 -2.6165924 -2.5016513 -2.3724313 -2.1327233 -2.1108177 -2.2803938 -2.4757829 -2.7171578 -2.9401684 -3.0605693][-2.3986785 -2.45874 -2.5627279 -2.6194265 -2.6722145 -2.6361618 -2.5379486 -2.4188821 -2.1619012 -2.0760827 -2.19563 -2.3892512 -2.5696139 -2.7270145 -2.8510928][-2.4288082 -2.5075059 -2.6501493 -2.7459431 -2.8218298 -2.7964141 -2.7152543 -2.6305351 -2.4088676 -2.2635605 -2.2671268 -2.3592033 -2.4222069 -2.5276775 -2.6787798][-2.5294771 -2.6493909 -2.8430414 -2.9767199 -3.0635324 -3.0242009 -2.9024978 -2.7923698 -2.6001265 -2.4355602 -2.3798814 -2.4281402 -2.4410915 -2.5107136 -2.6448369][-2.6069112 -2.7299531 -2.9221148 -3.0563989 -3.1361008 -3.0726144 -2.89997 -2.7535009 -2.5977492 -2.4734683 -2.4372692 -2.4927444 -2.4894474 -2.515749 -2.5820589][-2.6347523 -2.7224989 -2.8463678 -2.9193561 -2.9626966 -2.8990192 -2.7384191 -2.6235995 -2.537467 -2.4642067 -2.4374382 -2.4618318 -2.411742 -2.3948925 -2.4213514][-2.6318679 -2.682447 -2.734163 -2.7343793 -2.7325728 -2.6917508 -2.6047482 -2.5638461 -2.5422444 -2.4964209 -2.4525166 -2.420054 -2.3341861 -2.3148053 -2.3505836][-2.5867028 -2.6149738 -2.6297021 -2.6134067 -2.6069961 -2.6047978 -2.5910141 -2.5918708 -2.5808349 -2.5303142 -2.4703431 -2.4277751 -2.3687267 -2.3751242 -2.4275658]]...]
INFO - root - 2017-12-07 03:20:09.830324: step 110, loss = 0.95, batch loss = 0.88 (5.5 examples/sec; 1.467 sec/batch; 135h:27m:35s remains)
INFO - root - 2017-12-07 03:20:27.047710: step 120, loss = 0.77, batch loss = 0.69 (6.7 examples/sec; 1.197 sec/batch; 110h:31m:58s remains)
INFO - root - 2017-12-07 03:20:38.375355: step 130, loss = 0.85, batch loss = 0.77 (7.7 examples/sec; 1.037 sec/batch; 95h:45m:23s remains)
INFO - root - 2017-12-07 03:20:49.893097: step 140, loss = 0.84, batch loss = 0.77 (7.5 examples/sec; 1.073 sec/batch; 99h:03m:07s remains)
INFO - root - 2017-12-07 03:21:01.502461: step 150, loss = 1.09, batch loss = 1.01 (6.6 examples/sec; 1.210 sec/batch; 111h:41m:54s remains)
INFO - root - 2017-12-07 03:21:12.996103: step 160, loss = 0.77, batch loss = 0.69 (7.1 examples/sec; 1.131 sec/batch; 104h:25m:09s remains)
INFO - root - 2017-12-07 03:21:24.488332: step 170, loss = 0.87, batch loss = 0.79 (6.6 examples/sec; 1.215 sec/batch; 112h:10m:32s remains)
INFO - root - 2017-12-07 03:21:35.962109: step 180, loss = 0.71, batch loss = 0.63 (7.1 examples/sec; 1.127 sec/batch; 104h:00m:39s remains)
INFO - root - 2017-12-07 03:21:47.491580: step 190, loss = 1.02, batch loss = 0.95 (6.6 examples/sec; 1.215 sec/batch; 112h:10m:11s remains)
INFO - root - 2017-12-07 03:21:59.334539: step 200, loss = 0.91, batch loss = 0.84 (6.3 examples/sec; 1.280 sec/batch; 118h:08m:05s remains)
2017-12-07 03:22:00.179535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5443304 -3.9551775 -4.2847953 -4.5287905 -4.2933574 -3.647845 -3.1385961 -2.9299693 -3.0102258 -3.232336 -3.5720744 -3.9765239 -4.3263507 -4.4639373 -4.31996][-3.7673419 -4.2276678 -4.620307 -4.923511 -4.6567178 -3.8562813 -3.2660785 -3.0954418 -3.2060707 -3.4191854 -3.7507579 -4.2248683 -4.7158833 -4.9424815 -4.7295227][-3.3563786 -3.7965012 -4.20717 -4.6029658 -4.4283352 -3.6374218 -3.0941482 -2.9820251 -3.1270905 -3.3289254 -3.6122587 -4.0595441 -4.6182423 -4.9117403 -4.5523186][-2.6769881 -3.0734091 -3.5057108 -4.042984 -3.9901788 -3.17376 -2.5169215 -2.2471714 -2.3017659 -2.4942427 -2.748548 -3.1281614 -3.7377553 -4.1637211 -3.7450092][-2.2210829 -2.5272164 -3.0240433 -3.7446451 -3.7711575 -2.807693 -1.8168879 -1.2254822 -1.1838951 -1.495111 -1.8561292 -2.2537527 -2.9558368 -3.5914881 -3.2320483][-1.639636 -1.7868595 -2.3288326 -3.2155933 -3.3728051 -2.3343253 -1.0229204 -0.12699938 -0.090594769 -0.64751911 -1.2040341 -1.703238 -2.4792385 -3.2045965 -2.8474751][-0.59454513 -0.50864697 -1.0680857 -2.0842931 -2.4284124 -1.4280982 0.17190313 1.4480486 1.5234423 0.67124891 -0.15169287 -0.79279208 -1.544354 -2.2031155 -1.892509][0.0107584 0.19595289 -0.40109396 -1.4310544 -1.9060702 -1.0233624 0.675529 2.110497 2.1374483 1.0192814 -0.045379639 -0.80616117 -1.4913092 -2.0361545 -1.8424783][-0.30994081 -0.2848897 -1.0117068 -1.9661136 -2.5104141 -1.9757214 -0.6246798 0.53867435 0.579659 -0.32057047 -1.2881277 -2.0558529 -2.6637597 -3.0349464 -2.8132815][-1.017535 -1.2224972 -2.0493021 -2.904942 -3.5363898 -3.421149 -2.5153246 -1.6217673 -1.4760821 -1.9991224 -2.719609 -3.3880067 -3.8230286 -3.8819668 -3.5148787][-2.0363972 -2.38172 -3.133069 -3.7202837 -4.164228 -4.1675925 -3.5868673 -3.002378 -2.9133334 -3.1927724 -3.6921942 -4.2211804 -4.5908847 -4.5949488 -4.3129764][-3.0005507 -3.4192131 -4.0123749 -4.2470875 -4.3157864 -4.1646643 -3.69802 -3.3140969 -3.3064013 -3.4890783 -3.8388376 -4.261167 -4.7205644 -4.9058418 -4.8030453][-3.2617631 -3.6678665 -4.1030602 -4.1642971 -4.0882406 -3.9236207 -3.5484529 -3.2222085 -3.1779695 -3.3413756 -3.6090627 -3.9429896 -4.437696 -4.6921554 -4.6509614][-3.0335944 -3.3843675 -3.7532668 -3.8450558 -3.8235178 -3.7748241 -3.5680208 -3.3361211 -3.3191636 -3.5309546 -3.7300134 -3.9249203 -4.2986712 -4.4521456 -4.3511772][-2.7463112 -2.9988778 -3.3032532 -3.4512734 -3.4983478 -3.5235279 -3.4363422 -3.2676365 -3.2405295 -3.4411783 -3.5441527 -3.6305711 -3.88236 -3.9365275 -3.8030782]]...]
INFO - root - 2017-12-07 03:22:11.877648: step 210, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 1.150 sec/batch; 106h:06m:27s remains)
